Published in Transactions on Machine Learning Research (01/2024)
FederatedClassificationinHyperbolicSpacesviaSecureAg-
gregation of Convex Hulls
Saurav Prakash∗sauravp2@illinois.edu
Department of Electrical and Computer Engineering
University of Illinois Urbana Champaign
Jin Sima∗jsima@illinois.edu
Department of Electrical and Computer Engineering
University of Illinois Urbana Champaign
Chao Pan∗chaopan2@illinois.edu
Department of Electrical and Computer Engineering
University of Illinois Urbana Champaign
Eli Chien ichien3@illinois.edu
Department of Electrical and Computer Engineering
University of Illinois Urbana Champaign
Olgica Milenkovic milenkov@illinois.edu
Department of Electrical and Computer Engineering
University of Illinois Urbana Champaign
Reviewed on OpenReview: https: // openreview. net/ forum? id= umggDfMHha
Abstract
Hierarchical and tree-like data sets arise in many relevant applications, including language
processing, graph data mining, phylogeny and genomics. It is known that tree-like data
cannot be embedded into Euclidean spaces of finite dimension with small distortion, and
that this problem can be mitigated through the use of hyperbolic spaces. When such data
also has to be processed in a distributed and privatized setting, it becomes necessary to
work with new federated learning methods tailored to hyperbolic spaces. As an initial
step towards the development of the field of federated learning in hyperbolic spaces, we
propose the first known approach to federated classification in hyperbolic spaces. Our
contributions are as follows. First, we develop distributed versions of convex SVM classifiers
for Poincaré discs. In this setting, the information conveyed from clients to the global
classifier are convex hulls of clusters present in individual client data. Second, to avoid
label switching issues, we introduce a number-theoretic approach for label recovery based
on the so-called integer Bhsequences. Third, we compute the complexity of the convex
hulls in hyperbolic spaces to assess the extent of data leakage; at the same time, in order
to limit the communication cost for the hulls, we propose a new quantization method for
the Poincaré disc coupled with Reed-Solomon-like encoding. Fourth, at the server level, we
introduceanewapproachforaggregatingconvexhullsoftheclientsbasedonbalancedgraph
partitioning. We test our method on a collection of diverse data sets, including hierarchical
single-cell RNA-seq data from different patients distributed across different repositories that
have stringent privacy constraints. The classification accuracy of our method is up to ∼11%
better than its Euclidean counterpart, demonstrating the importance of privacy-preserving
learning in hyperbolic spaces. Our implementation for the proposed method is available at
https://github.com/sauravpr/hyperbolic_federated_classification .
∗Saurav, Jin, and Chao contributed equally to this work.
1Published in Transactions on Machine Learning Research (01/2024)
1 Introduction
Learninginhyperbolicspaces,whichunlikeflatEuclideanspacesarenegativelycurved,isatopicofsignificant
interest in many application domains (Ungar, 2001; Vermeer, 2005; Hitchman, 2009; Nickel & Kiela, 2017;
Tifrea et al., 2019; Ganea et al., 2018; Sala et al., 2018; Cho et al., 2019; Liu et al., 2019; Chami et al.,
2020b; Tabaghi & Dokmanić, 2021; Chien et al., 2021; Pan et al., 2023a). In particular, data embedding and
learning in hyperbolic spaces has been investigated in the context of natural language processing (Dhingra
et al., 2018), graph mining (Chen et al., 2022), phylogeny (Hughes et al., 2004; Jiang et al., 2022), and
genomic data studies (Raimundo et al., 2021; Pan et al., 2023a). The fundamental reason behind the use
of negatively curved spaces is that hierarchical data sets and tree-like metrics can be embedded into small-
dimensional hyperbolic spaces with small distortion. This is a property not shared by Euclidean spaces, for
which the distortion of embedding Npoints sampled from a tree-metric is known to be O(logN)(Bourgain,
1985; Linial et al., 1995).
In the context of genomic data learning and visualization, of special importance are methods for hyper-
bolic single-cell (sc) RNA-seq (scRNA-seq) data analysis (Luecken & Theis, 2019; Ding & Regev, 2021;
Klimovskaia et al., 2020; Tian et al., 2023). Since individual cells give rise to progeny cells of similar prop-
erties, scRNA-seq data tends to be hierarchical; this implies that for accurate and scalable analysis, it is
desirable to embed it into hyperbolic spaces (Ding & Regev, 2021; Klimovskaia et al., 2020; Tian et al.,
2023). Furthermore, since scRNA-seq data reports gene activity levels of individual cells that are used in
medical research, it is the subject of stringent privacy constraints – sharing data distributed across different
institutions is challenging and often outright prohibited. This application domain makes a compelling case
for designing novel federated learning techniques in hyperbolic spaces.
Federated learning (FL) is a class of approaches that combine distributed machine learning methods with
data privacy mechanisms (McMahan et al., 2017; Bonawitz et al., 2017; Sheller et al., 2020; Kairouz et al.,
2021;Pichai,2019;Lietal.,2020b). FLmodelsaretrainedacrossmultipledecentralizededgedevices(clients)
that hold local data samples that are not allowed to be exchanged with other communicating entities. The
task of interest is to construct a global model using single- or multi-round exchanges of secured information
with a centralized server.
We take steps towards establishing the field of FL in hyperbolic spaces by developing the first known mech-
anism for federated classification on the Poincaré disc, a two-dimensional hyperbolic space model (Ungar,
2001; Sarkar, 2012). The goal of our approach is to securely transmit low-complexity information from the
clients to the server in one round to construct a global classifier at the server side. In addition to privacy
concerns, “label switching” is another major bottleneck in federated settings. Switching arises when local
labels are not aligned across clients. Our FL solution combines, for the first time, learning methods with
specialized number-theoretic approaches to address label switching issues. It also describes novel quantiza-
tion methods and sparse coding protocols for hyperbolic spaces that enable low-complexity communication
based on convex hulls of data clusters. More specifically, our main contributions are as follows.
•We describe a distributed version of centralized convex SVM classifiers for Poincaré discs (Chien et al.,
2021) which relies on the use of convex hulls of data classes for selection of biases of the classifiers. In
our solution, the convex hulls are transmitted in a communication-efficient manner to the server that
aggregates the convex hulls and computes an estimate of the bias to be used on the aggregated (global)
data.
•We perform the first known analysis of the expected complexity of random convex hulls of data sets in
hyperbolic spaces. The results allow us to determine the communication complexity of the scheme and to
assess its privacy features which depend on the size of the transmitted convex hulls. Although convex hull
complexity has been studied in-depth for Euclidean spaces (see (Har-Peled, 2011) and references therein),
no counterparts are currently known for negatively curved spaces.
•We introduce a new quantization method for the Poincaré disc that can be coupled with Reed-Solomon-
like centroid encoding (Pan et al., 2023b) to exploit data sparsity in the private communication setting.
This component of our FL scheme is required because the extreme points of the convex hulls have to be
quantized before transmission due to privacy considerations.
2Published in Transactions on Machine Learning Research (01/2024)
Convex Hull
Quan�za�on
Labeling of
Nonempty Bins
via Bh Codes
Convex Hull
Quan�za�on
Labeling of
Nonempty Bins
via Bh Codes
Sparse Secure
Reed-Solomon-
Like Coding
&
Transmission
Aggrega�on + Label Resolu�on via Bh decoding
Graph-Based Grouping of Pointsets &
Computa�on of Global Hyperbolic Classiﬁer
Clients
ServerHierarchical
Data
Hierarchical
Data
Figure 1: Diagram of the hyperbolic federated classification framework in the Poincaré model of a hyperbolic
space. For simplicity, only binary classifiers for two clients are considered. (a) The clients embed their
hierarchical data sets into the Poincaré disc. (b) The clients compute the convex hulls of their data to
convey the extreme points to the server. (c) To efficiently communicate the extreme points, the Poincaré
disc is uniformly quantized (due to distance skewing on the disc, the regions do not appear to be of the
same size). (d) As part of the secure transmission module, only the information about the corresponding
quantization bins containing extreme points is transmitted via Reed-Solomon coding, along with the unique
labels of clusters held by the clients, selected from integer Bhsequences (in this case, h= 2since there
are two classes). (e) The server securely resolves the label switching issue via B2-decoding. (f) Upon label
disambiguation, the server constructs a complete weighted graph in which the convex hulls represent the
nodes while the edge weights equal w(·,·) = 1/d(·,·), whered(·,·)denotes the average pairwise hyperbolic
distance between points in the two hulls. The server then performs balanced graph partitioning to aggregate
the convex hulls and arrive at “proxies” for the original, global clusters. (g) Once the global clusters are
reconstructed, a reference point (i.e., “bias” of the hyperbolic classifier), p, is computed as the midpoint of
the shortest geodesic between the convex hulls, and subsequently used for learning the “normal” vector w
of the hyperbolic classifier.
•Another important contribution pertains to a number-theoretic scheme for dealing with the challenging
label switching problem. Label switching occurs due to inconsistent class labeling at the clients and
prevents global reconciliation of the client data sets at the server side. We address this problem through
the use ofBhsequences (Halberstam & Roth, 2012), with integer parameter h≥2. Each client is assigned
labels selected from the sequence of Bhintegers which ensures that any possible collection of hconfusable
labels can be resolved due to the unique h-sum property of Bhintegers. The proposed approach is not
restricted to hyperbolic spaces and can be applied in other settings as well.
•To facilitate classification at the server level after label disambiguation, we propose a new approach for
(secure) aggregation of convex hulls of the clients based on balanced graph partitioning (Kernighan &
Lin, 1970).
An illustration of the various components of our model is shown in Figure 1.
The performance of the new FL classification method is tested on a collection of diverse data sets, including
scRNA-seq data for which we report the classification accuracy of both the FL and global (centralized)
models. The results reveal that our proposed framework offers excellent classification accuracy, consistently
exceeding that of Euclidean FL learners.
It is also relevant to point out that our results constitute the first known solution for FL of hierarchical
biological data sets in hyperbolic spaces and only one of a handful of solutions for FL of biomedical data
(with almost all prior work focusing on imaging data (Dayan et al., 2021; Rieke et al., 2020; Mushtaq et al.,
2022; Gazula et al., 2022; Saha et al., 2022)).
3Published in Transactions on Machine Learning Research (01/2024)
2 Related Works
Learning in Hyperbolic Spaces. There has been significant interest in developing theoretical and algo-
rithmic methods for data analysis in negatively curved spaces, with a representative (but not exhaustive)
sampling of results pertaining to hyperbolic neural networks (Ganea et al., 2018; Peng et al., 2021; Liu
et al., 2019; Zhang et al., 2021); embedding and visualization in hyperbolic spaces (Nickel & Kiela, 2017);
learning hierarchical models in hyperbolic spaces (Nickel & Kiela, 2018; Chami et al., 2020a); tree-metric
denoising (Chien et al., 2022); computer vision (Khrulkov et al., 2020; Klimovskaia et al., 2020; Fang et al.,
2021); Procrustes analysis (Tabaghi & Dokmanić, 2021); and most relevant to this work, single-cell expres-
sion data analysis (Klimovskaia et al., 2020; Ding & Regev, 2021), hyperbolic clustering (Xavier, 2010) and
classification (Cho et al., 2019; Chen et al., 2020; Chien et al., 2021; Tabaghi et al., 2021; Pan et al., 2023a).
With regards to classification in hyperbolic spaces, the work (Cho et al., 2019) initiated the study of large-
margin classifiers, but resulted in a nonconvex formulation for SVMs. This issue was addressed in (Chien
et al., 2021) by leveraging tangent spaces; however, the method necessitates a careful selection of the bias
of classifiers, which is a nontrivial issue in the federated setting. In our work we also introduce a novel
quantization scheme that is specially designed for Poincaré disc, which is distinct from the “tiling-based
models” proposed in (Yu & De Sa, 2019).
Federated Classification. Classification has emerged as an important problem for FL. Several strategies
were reported in (McMahan et al., 2017; Zhao et al., 2018; Li et al., 2020c; Karimireddy et al., 2020; Kairouz
et al., 2021; Niu et al., 2023; Elkordy et al., 2022; Prakash et al., 2020b;a; Babakniya et al., 2023; Prakash
et al., 2020c), with the aim to address the pressing concerns of privacy protection, non-identically distributed
data, and system heterogeneity inherent in FL. In order to further alleviate the communication complexity
of the training procedure, one-shot federated classification (Zhou et al., 2020; Li et al., 2020a; Salehkaleybar
et al., 2021) was introduced to enable the central server to learn a global model over a network of federated
devices within a single round of communication, thus streamlining the process. Despite these advances in
federated classification, the intricate nature of hyperbolic spaces presents a unique challenge for the direct
application of off-the-shelf FL methods. To the best of our knowledge, our work is the first one to explore
specialized federated classification techniques for learning within this specific embedding domain. Notably,
there are two recent papers on Riemannian manifold learning in FL setting (Li & Ma, 2022; Wu et al.,
2023). While hyperbolic spaces are Riemannian manifolds, our work aims to formulate the hyperbolic SVM
classification problem as a convex optimization problem that is solvable through Euclidean optimization
methods. In contrast, Li & Ma (2022) and Wu et al. (2023) emphasize the extension of traditional Euclidean
optimization algorithms, such as SVRG, to Riemannian manifolds within the FL framework. Thus, our
focus is more on a new problem formulation, while Li & Ma (2022) and Wu et al. (2023) primarily address
the optimization procedures. Furthermore, due to label switching, these optimization procedures are not
directly applicable to our problems.
FL for Biomedical Data Analysis. Biomedical data is subject to stringent privacy constraints and it is
imperative to mitigate user information leakage and data breaches (Cheng & Hung, 2006; Avancha et al.,
2012). FL has emerged as a promising solution to address these challenges (Dayan et al., 2021; Mushtaq
et al., 2022; Gazula et al., 2022; Saha et al., 2022), especially in the context of genomic (e.g., cancer genomic
and transcriptomic) data analysis (Rieke et al., 2020; Chowdhury et al., 2022), and it is an essential step
towards realizing a more secure and efficient approach for biological data processing. For FL techniques in
computational biology, instead of the classical notions of (local) differential privacy (Dwork et al., 2014),
more adequate notions of privacy constraints are needed to not compromise utility. We describe one such
new notion of data privacy based on quantized minimal convex hulls in Section 4.
In what follows, we first provide a review of basic notions pertaining to hyperbolic spaces and the corre-
sponding SVM classification task.
3 Classification in Hyperbolic Spaces
The Poincaré ball model (Ratcliffe et al., 1994) of hyperbolic spaces is widely used in machine learning
and data mining research. In particular, the Poincaré ball model has been extensively used to address
classification problems in the hyperbolic space (Ganea et al., 2018; Pan et al., 2023a).
4Published in Transactions on Machine Learning Research (01/2024)
We start by providing the relevant mathematical background for hyperbolic classification in the Poincaré ball
model. Formally, the Poincaré ball Bn
kis a Riemannian manifold of curvature −k(wherek>0), defined as
Bn
k={x∈Rn:k∥x∥2<1}. Here and elsewhere, ∥·∥and⟨·,·⟩stand for the ℓ2norm and the standard inner
product, respectively. For simplicity, we consider linear binary classification tasks in the two-dimensional
Poincaré disc model B2
k, where the input data set consists of Ndata points{(xj,yj)}N
j=1with features
xj∈B2
kand labels yj∈{− 1,+1},j= 1,...,N. Our analysis can be easily extended to the multi-class
classification problem and for the general Poincaré ball model in a straightforward manner.
To enable linear classification on the Poincaré disc, one needs to first define a Poincaré hyperplane, which
generalizes the notion of a hyperplane in Euclidean space. It has been shown in (Ganea et al., 2018) that
such a generalization can be obtained via the tangent space TpB2
k, which is the first order approximation of
the Poincaré disc at a point p∈B2
k. The point pis commonly termed the “reference point” for the tangent
spaceTpB2
k. Any point x∈B2
kcan be mapped to the tangent space TpB2
kas a tangent vector v= logpxvia
the logarithmic map logp(·) :B2
k→T pB2
k. Conversely, the inverse mapping is given by the exponential map
expp(·) :TpB2
k→B2
k. These two mappings are formally defined as follows:
v= logp(x) =1−k∥p∥2
√
ktanh−1(√
k∥(−p)⊕kx∥)(−p)⊕kx
∥(−p)⊕kx∥,p∈B2
k,x∈B2
k; (1)
x= expp(v) =p⊕k/parenleftigg
tanh/parenleftigg√
k∥v∥
1−k∥p∥2/parenrightigg
v√
k∥v∥/parenrightigg
,p∈B2
k,v∈TpB2
k. (2)
Here,⊕kstands for Möbius addition in the Poincaré ball model, defined as
x⊕ky=(1 + 2k⟨x,y⟩+k∥y∥2)x+ (1−k∥x∥2)y
1 + 2k⟨x,y⟩+k2∥x∥2∥y∥2,∀x,y∈B2
k. (3)
Furthermore, the distance between x∈B2
kandy∈B2
kis given by
dk(x,y) =2√
ktanh−1(√
k∥(−x)⊕ky∥). (4)
The notions of reference point pand tangent space TpB2
kallow one to define a Poincaré hyperplane as
Hw,p∆={x∈B2
k:⟨(−p⊕kx),w⟩= 0}={x∈B2
k:⟨logp(x),w⟩= 0}, (5)
where w∈TpB2
kdenotes the normal vector of this hyperplane, resembling the normal vector for Euclidean
hyperplane. Furthermore, as evident from the first of the two equivalent definitions of a Poincaré hyperplane,
the reference point p∈Hw,presembles the “bias term” for Euclidean hyperplane.
SVM in Hyperbolic Spaces : Given a Poincaré hyperplane as defined in (5), one can formulate linear
classification problems in the Poincaré disc model analogous to their counterparts in Euclidean spaces. Our
focusisonhyperbolicSVMs, sincetheyhaveconvexformulationsandbroadapplicability (Panetal.,2023a).
As shown in (Pan et al., 2023a), solving the SVM problem over B2
kis equivalent to solving the following
convex problem:
min
w∈R21
2∥w∥2s.t.yj⟨logp(xj),w⟩≥1∀j∈[N]. (6)
The hyperbolic SVM formulation in (6) is well-suited to two well-separated clusters of data points. However,
in practice, clusters may not be well-separated, in which case one needs to solve the convex soft-margin SVM
problem
min
w∈R21
2∥w∥2+λN/summationdisplay
j=1max/parenleftbig
0,1−yj⟨logp(xj),w⟩/parenrightbig
. (7)
For solving the optimization problems (6) and (7), one requires the reference point p. However, the reference
point is not known beforehand. Hence, one needs to estimate a “good” pfor a given data set before solving
5Published in Transactions on Machine Learning Research (01/2024)
the optimization problem that leads to the normal vector w. To this end, for linearly separable data sets
with binary labels, it can be shown that the optimal Poincaré hyperplane that can correctly classifier all
data points must correspond to a reference point pthat does not fall within either of the two convex hulls
of data classes, since the hyperplane always passes through p. Convex hulls in Poincaré disc can be defined
by replacing lines with geodesics in the definition of Euclidean convex hulls (Ratcliffe et al., 2006).
To estimate the reference point pin practice, a heuristic adaptation of the Graham scan algorithm (Graham,
1972) for finding convex hulls of points in the Poincaré disc has been proposed in (Pan et al., 2023a).
Specifically, the reference point is generated by first constructing the Poincaré convex hulls of the classes
labeled by +1and−1, and then choosing as the geodesic midpoint of the closest pair of extreme points (wrt
the hyperbolic distance) in the two convex hulls. The normal vector wis then determined using (6) or (7).
Next, we proceed to introduce the problem of federated hyperbolic SVM classification, and describe an
end-to-end solution for the problem.
4 Federated Classification in Hyperbolic Spaces: Problem Formulations and
Solutions
As described in Section 1, our proposed approach is motivated by genomic/multiomic data analysis, since
such data often exhibits a hierarchical structure and is traditionally stored in a distributed manner. Genomic
data repositories are subject to stringent patient privacy constraints and due to the sheer volume of the data,
they also face problems due to significant communication and computational complexity overheads.
Therefore, in the federated learning setting corresponding to such scenarios, one has to devise a distributed
classification method for hyperbolic spaces that is privacy-preserving and allows for efficient client-server
communication protocols. Moreover, due to certain limitations of (local) differential privacy (DP), particu-
larly with regards to biological data, new privacy constraints are required. DP is standardly ensured through
addition of privacy noise, which is problematic because biological data is already highly noisy and adding
noise significantly reduces the utility of the inference pipelines. Furthermore, the proposed privacy method
needs to be able to accommodate potential “label switching” problems, which arise due to inconsistent class
labeling at the clients, preventing global reconciliation of the clients’ data sets at the server side.
More formally, for the federated model at hand, we assume that there are L≥2clients in possession of
private hierarchical data sets. The problem of interest is to perform hyperbolic SVM classification over the
unionoftheclients’datasetsattheFLserverviaselectionofasuitablereferencepointandthecorresponding
normal vector. For communication efficiency, we also require that the classifier be learned in a single round
of communication from the clients to the server. We enforce this requirement since one-round transmission
ensures a good trade-off between communication complexity and accuracy, given that the SVM classifiers
only use quantized convex hulls.
In what follows, we present the main challenges in addressing the aforementioned problem of interest, along-
side an overview of our proposed solutions.
1.Data Model and Poincaré Embeddings. We denote the data set of client iwhich is of size Miby
tuples Z(i)={zi,j,yi,j}Mi
j=1fori∈{1,...,L}. Here, for j∈{1...,Mi},zi,j∈Rdstands for the data
points of client i,whileyi,jdenotes the corresponding labels. For simplicity, we assume the data sets to
be nonintersecting (i.e., Z(i)∩Z(j)=∅,∀i̸=j, meaning that no two clients share the same data with
the same label) and generated by sampling without replacement from an underlying global data set Z.
Clientifirst embeds Z(i)intoB2
k(see Figure 1-(a)) to obtain X(i)={xi,j∈B2
k}Mi
j=1. To perform
the embedding, one can use any of the available methods described in (Sarkar, 2012; Sala et al., 2018;
Skopek et al., 2020; Klimovskaia et al., 2020; Khrulkov et al., 2020; Sonthalia & Gilbert, 2020; Lin et al.,
2023). However, an independent procedure for embedding data points at each client can lead to geometric
misalignment across clients, as the embeddings are rotationally invariant in the Poincaré disc. To resolve
this problem, one can perform a Procrustes-based matching of the point sets X(i),i∈{1,...,L}in the
hyperbolic space (Tabaghi & Dokmanić, 2021). Since Poincaré embedding procedures are not the focus of
this work, we make the simplifying modelling assumption that the point sets X(i)={xi,j∈B2
k}Mi
j=1,i∈
{1,...,L}are sampled from a global embedding over ∪L
i=1Z(i).Although such an embedding uses the
6Published in Transactions on Machine Learning Research (01/2024)
whole data set and may hence not be private, it mitigates the need to include the Procrustes processing
into the learning pipeline.
2.ClassificationattheClientLevel. WhenperformingfederatedSVMclassificationinEuclideanspaces,
each client trains its own SVM classifier over its local data set; the server then “averages” the local models
to obtain the global classifier (McMahan et al., 2017). It is not straightforward to extend such a solution
to hyperbolic spaces. Particularly, the averaging procedure of local models is not as simple as FedAvg in
Euclidean spaces due to the nonaffine operations involved in the definition of a Poincaré hyperplane (5).
We remedy the problem of aggregating classifier models at the server side by sending minimal convex
hulls of classes instead of classifier models. A minimal convex hull is defined as follows.
Definition 1. (Minimal Poincaré convex hull) Given a point set DofNpoints in the Poincaré disc,
the minimal convex hull CH(D)⊆Dis the smallest nonredundant collection of data points whose convex
hull contains all points in D.
In this setting, client i, fori∈{1,...,L}, computes the minimal convex hulls CH(i)
+andCH(i)
−ofX(i)
+
andX(i)
−respectively, as illustrated in Figure 1-(b). Here, X(i)
+andX(i)
−denote the point sets of the
client’s local classes with labels +1and−1respectively. To find such convex hulls, we devise a new form of
the Poincaré Graham scan algorithm and prove its correctness, and in the process establish computational
complexity guarantees as well. For detailed result statements, see Section 5.1. This approach is motivated
by the fact that for the server to compute a suitable global reference point, it only needs to construct the
convex hulls of the global clusters.
In the ideal scenario where all local labels are in agreement with the global labels and where no
additional processing is performed at the client’s side (such as quantization), the minimal convex
hulls of the global clusters equals CH(∪L
i=1X(i)
+)andCH(∪L
i=1X(i)
−). Furthermore, the minimal con-
vex hulls of the global and local clusters are related as CH(∪L
i=1X(i)
+) =CH(∪L
i=1CH(X(i)
+))and
CH(∪L
i=1X(i)
−) =CH(∪L
i=1CH(X(i)
−)). As a result, each client can simply communicate the extreme
points of the local convex hulls to the server; the server can then find the union of the point sets belong-
ing to the same global class, construct the global minimal convex hulls and find a global reference point
p.
This ideal setting does not capture the constraints encountered in practice since a) the extreme points
of the convex hulls have to be quantized to ensure efficient transmission; b) the issue of label switching
across clients has to be resolved; and c) secure aggregation and privacy protocols have to be put in place
before the transmission of local client information.
Poincaré Quantization. To reduce the communication complexity, extreme points of local convex
hulls are quantized before transmission to the server. In Euclidean spaces, one can perform grid based
quantization, which ensures that the distance between pairs of points from arbitrary quantization bins
are uniformly bounded. To meet a similar constraint, we describe next the quantization criterion for B2
k.
Definition 2. ( ϵ-Poincaré quantization) Forϵ >0, quantization scheme for B2
kis said to be a ϵ-
Poincaré quantization scheme if for any pair of points xandythat lie in the same quantization bin, their
hyperbolic distance dk(x,y)as defined in (4) satisfies dk(x,y)≤ϵ.
To facilitate the transmission of extreme points, we develop a novel ϵ-Poincaré quantization scheme,
which we subsequently refer to as PQϵ(·)(for a mathematical analysis of the quantization scheme, see
Section 5.2). The PQϵ(·)method uses radial and angular information (akin to polar coordinates in
Euclidean spaces). To bound the bins in the angular domain, we partition the circumference such that
the hyperbolic length of any quantization bin along the circumference is bounded from above by ϵ/2.
Similarly, we choose to partition the radial length so that all bins have hyperbolic radial length equal to
ϵ/2. This ensures that the quantization criteria in Definition 2 is satisfied. The parameter ϵdetermines
the total number of the quantization bins, and thus can be chosen to trade the quantization error with
the number of bits needed for transmitting the convex hulls. Furthermore, we choose the “bin centroids
(centers)” accordingly. Using the PQϵ(·)protocol, each client iquantizes its minimal Poincaré convex
hullsCH(i)
+andCH(i)
−to create two quantized convex hull point sets Q(i)
+={PQϵ(x) :x∈CH(i)
+}
7Published in Transactions on Machine Learning Research (01/2024)
andQ(i)
−={PQϵ(x) :x∈CH(i)
i}. The quantized convex hulls (i.e., extreme points) are also used for
measuring the information leakage of the hyperbolic classification approach, as outlined in what follows.
Privacy Leakage. A major requirement in FL is to protect the privacy of individual client
data (Bonawitz et al., 2017). One of the most frequently used privacy guarantees are that the server
cannot learn the local data statistics and/or identity of a client based on the received information (for
other related privacy criteria in federated clustering, please refer to (Pan et al., 2023b)). However, such
a privacy criteria is quite restrictive for classification tasks using Q(i)
+andQ(i)
−, fori∈[L], as the server
needs to reconstruct the hulls for subsequent processing. Hence, we consider a different notion of pri-
vacy, where some restricted information pertaining to local data statistics (specifically, Q(i)
+andQ(i)
−, for
i∈[L]), is allowed to be revealed to the server, while retaining full client anonymity.
More precisely, to quantify the information leakage introduced by our scheme, we make use of the so-called
ϵ-minimal Poincaré convex hull, defined next.
Definition 3. ( ϵ-Minimal Poincaré convex hull) Givenϵ>0, a quantization scheme PQϵ(·)forB2
k,
and a setDofNpoints in the Poincaré disc, let Q={PQϵ(x) :x∈CH(D)}be the point set obtained by
quantizing the extreme points of the convex hull of D. Theϵ-minimal Poincaré convex hull ˆCH(D)⊆Q
is the smallest nonredundant collection of points whose convex hull contains all points in Q.
To avoid notational clutter, we henceforth use “quantized convex hull” of a given point set Dto refer
to theϵ-minimal Poincaré convex hull of D. Clienticomputes two quantized convex hulls ˆCH(X(i)
+)
and ˆCH(X(i)
−)by finding the minimal convex hulls of Q(i)
+andQ(i)
−,respectively. We use the number of
extreme points of the quantized convex hull (i.e., the complexity of the convex hull), defined next, as a
surrogate function that measures privacy leakage.
Definition 4. ( ϵ-Convex hull complexity) Givenϵ >0, aϵ-Poincaré quantization scheme PQϵ(·),
and a setDofNpoints in the Poincaré disc, let Q={PQϵ(x) :x∈CH(D)}be the point set obtained
by quantizing the extreme points in the convex hull of D. Theϵ-convex hull complexity is the number of
extreme points in the convex hull of Q, i.e., the size of ˆCH(D).
We would like to point out that our notion of privacy does not rely on (local) differential privacy (DP)
or information theoretic privacy. Instead, it may be viewed as a new form of “combinatorial privacy,”
enhanced by controlled data quantization-type obfuscation. This approach to privacy is suitable for
computational biology applications, where DP noise may be undesirable as it significantly reduces the
utility of learning methods.
In addition to compromising accuracy, DP has practical drawbacks for other practical applications (Kifer
&Machanavajjhala,2011;Bagdasaryanetal.,2019;Zhangetal.,2022;Kan,2023). TheworkBagdasaryan
etal.(2019)establishedthatDPmodelsaremoredetrimentalforunderrepresentedclasses. Moreprecisely,
fairness disparity is significantly pronounced in DP models, as the accuracy drop for minority (outlier)
classes is more significant than for other classes. Additionally, in many practical scenarios, meeting
requirements for DP, which rely on particular assumptions about the data probability distribution, might
prove challenging (Kifer & Machanavajjhala, 2011; Zhang et al., 2022; Kan, 2023). For instance, when
database records exhibit strong correlations, ensuring robust privacy protection becomes difficult. All
of the above properties that cause issues are inherent to biological (genomic) data. First, genomic data
is imbalanced across classes. Second, as it derives from human patients, it is highly correlated due to
controlled and synchronized activities of genes within cells of members of the same species.
To overcome the aforementioned concerns, we require that 1) the clients reveal a small as possible number
of data points that are quantized (both for the purpose of reducing communication complexity and further
obfuscating information about the data); 2) the server collects data without knowing its provenance or
the identity of the individual clients that contribute the data through the use of secure aggregation of the
quantized minimal convex hulls of the client information. To address the first requirement, we introduce
a new notion of privacy leakage in Definition 4. If the number of quantized extreme points is small and
the quantization regions are large, the ϵ-convex hull complexity is small and consequentially the privacy
leakage is “small” (i.e, the number of points whose information is leaked is small). For bounds on the
8Published in Transactions on Machine Learning Research (01/2024)
ϵ-convex hull complexity in hyperbolic spaces (and, consequently, the privacy leakage), the reader is
referred to Section 5.3.
To address the second requirement, we adapt standard secure aggregation protocols to prevent the server
from inferring which convex hull points belong to which client, thus hiding the origin of the data.
Labeling of
Nonempty Bins
via Bh Codes
Labeling of
Nonempty Bins
via Bh Codes
Sparse Secure
Reed-Solomon-
Like Coding
&
Transmission
Server
Data
Data
Label Resolu�on
via Bh Decoding
Aggrega�on
Clients111 1
333
33
7777
121212
121211
333377
12
12121288
15
Figure 2: Proposed solution for the label switching problem. Each client has its own data set with its own
set of binary labels. Upon obtaining the convex hull, the extreme points are quantized to their nearest
bin-centers, and the corresponding bins are labeled with one of two integers from the B2sequence assigned
to each client. At the server side, the values in the nonempty bins are first aggregated. Next, by the property
of theBhsequence that the sum of any subset of size ≤hof elements in the sequence is unique, all individual
labels for the bins that are shared across multiple convex hulls (such as the bins colored in purple and brown)
are decoded. Therefore, the server is able to recover individual (quantized) local convex hulls, which are
further processed for global labeling and reference point computation (see Figure 1).
Label Switching. Another major challenge faced by the clients is “label switching.” When performing
classification in a distributed manner, unlike for the centralized case, one has to resolve the problem of
local client labels being potentially switched compared to each other and/or the ground truth. Clearly,
the mapping from the local labels at the client level to the global ground truth labels is unknown both
to the client and the server.
The label switching problem is illustrated in Figure 2, where clients use mismatched choices for assigning
thebinarylabels{−1,+1}totheirlocaldatapoints. Toenablefederatedclassification, thelabelswitching
problem has to be addressed in a private and communication efficient manner, which is challenging. Our
proposed solution is shown in Figure 2, and it is based on so-called Bhsequences. In a nutshell, Bh
sequences are sequences of positive integers with the defining property that any sum of ≤hpossibly
repeatedtermsofthesequenceuniquelyidentifiesthesummands(seeSection5.4forarigorousexposition).
Each client is assigned two different integers from the Bhsequence to be used as labels of their classes
so that neither the server nor the other clients have access to this information. Since secure aggregation
is typically performed on sums of labels, and the sum uniquely identifies the constituent labels that are
associated with the client classes, all labels can be matched up with the two ground truth classes. As an
illustrative example, the integers 1,3,7,12,20,30,44,...form aB2sequence, since no two (not necessarily
distinct) integers in the sequence produce the same sum. Now, if one client is assigned labels 1(red) and
3(green), while another is assigned 7(blue) and 12(pink), then a securely aggregated label 1 + 7 = 8
(purple) for points in the intersection of their convex hulls uniquely indicates that these points belonged
to both the cluster labeled 1at one client, and the cluster labeled 7at another client (see Figure 2, right
panel). Note that there are many different choices for Bhsequences to be used, which prevents potential
adversaries to infer information about label values and assignments.
Note that a different notion of label misalignment termed “concept shift” has been considered in prior
works, including (Sattler et al., 2020; Bao et al., 2023). In the referenced settings, label misalignment is a
result of client-specific global conditional data distributions (i.e., distributions of global labels given local
feature). Label misalignments are therefore handled by grouping clients based on the similarity of their
conditional data distributions; furthermore, a different global model is trained for each group of clients.
In contrast, we actually align labels of clients in the presence of switched labels, since for our problem,
9Published in Transactions on Machine Learning Research (01/2024)
the conditional global ground truth distributions are the same. Another related line of works is that of
“label noise” Petety et al. (2020); Song et al. (2022), where effectively, the global label for each individual
data point is assumed to be noisy, thus differing from our setting with misalignments across clients.
3.Secure Transmission. As shown in Figure 1-(d), the number of nonempty quantization bins labeled
byBhintegers is typically significantly smaller than the total number of quantization bins. As the client
only needs to communicate to the server the number of extreme points in nonempty quantization bins,
sparse coding protocols and secure aggregation protocols are desirable. In particular, the server should be
able to collect data without knowing its provenance or the individual clients/users that contributed the
data through the use of secure aggregation of the quantized minimal convex hulls of the client data, while
ensuring that the secure transmission method is also communication efficient. We address this problem
by leveraging a method proposed in (Pan et al., 2023b) termed Secure Compressed Multiset Aggregation
(SCMA). The key idea is to use Reed-Solomon-like codes to encode information about the identity of
nonempty quantization bins and the number of extreme points in them through evaluations of low-degree
polynomials over prime finite field. The bin indices represent distinct elements of the finite field (since
all bins are different), while the number of extreme points in a bin represents the coefficient of the term
corresponding to the element of the finite field encoding the bin (the number of points in different bins
may be the same).
In our FL method, we follow a similar idea, with the difference that we use the local labels (encoded using
Bhsequences) associated with the clusters to represent the coefficients of the terms, instead of using the
number of extreme points. For this, we first index the quantization bins as 1,2,3,..., starting from the
innermost angular bins and moving counter-clockwise (the first bin being at angle 0). For the quantization
bins in Figure 2, the bins are indexed 1,..., 64. For example, the four nonempty bins corresponding to
label 1are indexed by 18,21,51,53, while the five non-empty green bins are indexed by 27,31,44,46,59.
The SCMA secure transmission procedure is depicted in Figure 3, where the clients send the weighted
sum of bin indices to the server so that the weights are labels of the clusters and the server aggregates
the weighted sums. From the weighted sums, the server decodes the labels of all nonempty bins. The
underlyingfieldsizeischosentobegreaterthanthenumberofbins. Amathematicallyrigorousexposition
of the results is provided in Section 5.5.
Figure 3: The encoding and decoding procedure used in SCMA for transmitting the data sets shown in
Figure 2.
4.Classification at the Server. Upon receiving the information about quantized extreme points of the
data classes communicated by the clients, the server performs the following steps to construct a global
SVM classifier.
LabelDecoding. Theserverusesthe Bhlabelsofnonemptyquantizationbins,whichequaltheaggregate
(sum) of the corresponding local labels used by clients that contain data points in that quantization bin
(see Figure 2 where shared nonempty bins are indexed by aggregated labels 8(= 1 + 7) and15(= 3 + 12)
(purple and brown respectively)). The sums of the labels for each bin are acquired during the secure
transmission step. By the defining property of Bhsequences, the server correctly identifies the individual
10Published in Transactions on Machine Learning Research (01/2024)
local labels that contribute to a nonempty bin from their sum, as long as the parameter his larger than
or equal to the number of different local labels that contribute to the bin. Therefore, hshould be selected
as the maximum number of classes across all clients that have at least one extreme point in the same bin.
Therefore, h≤2Lin the worst case since there are at most 2Llocal classes in total. In practice, his
significantly smaller (see an example for h= 2in Figure 2). Thus, through Bhdecoding, each extreme
point in each quantized convex hull can be recovered; this implies that each individual convex hull can
also be unambiguously recovered at the server side, as shown in Figure 1-(e).
Remark 4.1. The above steps allow the server to recover the quantized convex hulls of the local classes
at each client, without learning anything about the data provenance. More precisely, the server cannot
associate the identities of the clients to the recovered convex hulls. Furthermore, the server receives only
a very small number of quantized extreme points of the convex hull of the data sets at the client side, and
the quantization noise may be roughly viewed as a noise-privacy mechanism.
Balanced Grouping of Convex Hulls. After label decoding, the next server task involves clustering
the2Lconvex hulls into 2groups, where each group corresponds to a global ground truth label. To solve
this problem, we propose a graph-theoretic approach using information about distances between pairs of
point sets in the Poincaré disc. More precisely, let the convex hulls be labeled by {1,..., 2L}, and let
CH(i)denote the ith convex hull for i∈{1,..., 2L}. We construct a weighted complete graph G(V)
withV={1,..., 2L}representing the 2Lconvex hulls. For a pair of nodes u,v∈V, the edge-weight
w({u,v}) = 1/dG(u,v), wheredG(u,v)is the average pairwise hyperbolic distance between points in the
convex hulls CH(u)andCH(v), i.e.,
dG(u,v) =/summationtext
x1∈CH(u)/summationtext
x2∈CH(v)dk(x1,x2)
|CH(u)||CH(v)|. (8)
Using the Kernighan-Lin balanced graph partitioning algorithm (Kernighan & Lin, 1970), we arrive at
two groups of aggregated local convex hulls. Particularly, Vis partitioned into two parts, each of size
L, so as to minimize the sum of the edge-weights between the two parts (i.e., the weight of the balanced
cut). This in turn results in two global groups of convex hulls. These steps are shown in Figure 1-(f).
The formal description of our approach is provided in Section 5.6.
Learning the Global SVM Classifier. Upon completion of the graph partitioning step, the server
assigns global binary labels to the two global point sets. Then, following the procedure described in
Section 3, the server constructs the global reference point p. Due to quantization, the server treats the
reconstructed global point sets as approximations/surrogates for the true global ground truth clusters,
and solves the optimization problem (6) or (7) to obtain the normal vector wof the classifier. This step
is shown in Figure 1-(g). Due to the distortions in the local hulls caused by Poincaré quantization, the
estimated global hulls are also distorted, as shown in Figure 1-(g). Hence, the performance of the trained
classifier depends on the quantization size ϵ: A smaller value of ϵleads to smaller convex hull distortions
but larger communication overhead. In Section 6, we demonstrate through extensive simulations that the
the global classifier learned using our framework performs well in practice for a relatively large range of
ϵvalues, both for synthetic and real data sets.
5 Detailed Explanations and Main Results
In what follows, we provide rigorous algorithmic results and performance guarantees for every learning and
computational step of the proposed one-shot federated SVM classification algorithm for hyperbolic spaces.
5.1 Poincaré Convex Hulls
The first step of the end-to-end solution is to construct CH(i)
+andCH(i)
−, the sets of extreme points in the
convex hulls of local clusters at the clients i∈{1,...,L}, with local labels +1and−1, respectively. For this
purpose, we adapt the Graham scan algorithm (Graham, 1972) widely used for computing convex hulls in
Euclidean space. Our Graham scan is described in Alg. 1, while the performance guarantees and complexity
of the method are summarized in the next theorem.
11Published in Transactions on Machine Learning Research (01/2024)
Algorithm 1 Poincaré Graham Scan
1:Input:PointsD={x1,···,xN}in the Poincaré disc B2
k.
2:b←the point inDat largest distance from the origin of B2
k, i.e., o= [0,0].
3:Normal vector n←−logb(o)
∥logb(o)∥,logb(o)is logarithmic map in (1).
4:Tangent vector t←R n, where R=/bracketleftbigg
0−1
1 0/bracketrightbigg
denotes theπ
2counter-clockwise rotation matrix.
5:Sorted dataV←forj∈[N], arrange points in Dbased on the principle angle of logb(xj)with respect
tot,in ascending order. For points with the same principle angle, keep only the one with the largest
norm∥logb(xj)∥.
6:V←V− b.
7:m←|V|.
8:STACK = [b].
9:forj∈[m]do
10:whilelen(STACK)>1 and CCW(STACK[-2], STACK[-1], V[j])≤0do
11:STACK.pop()
12:end while
13:STACK.append(V[j])
14:end for
15:Return STACK
Algorithm 2 CCW
1:Input:Points x,t,zinB2
k.
2:u1←logx(t)
∥logx(t)∥.
3:u2←logx(z)
∥logx(z)∥.
4:Return u1[0]u2[1]−u1[1]u2[0].
Theorem 5.1. Given a setDofNpoints in the Poincaré disc, Alg. 1 returns CH(D)inO(NlogN)time,
whereCH(D)denotes the set of extreme points in the minimal convex hull of D.
Based on this result, it is easy to see that Alg. 1 has the same complexity as the Graham Scan algorithm
for Euclidean spaces. The proof of Theorem 5.1 can be found in Appendix A.
5.2 Poincaré Quantization
Quantization is standardly used to trade-off communication overhead and accuracy of point representations.
In our FL setting, quantization also enables the new label encoding and decoding procedures that resolve
label switching, as well as secure aggregation during transmission.
The quantization approach for Poincaré discs exploits radial symmetry of the space. Furthermore, for a
given quantization parameter ϵ >0, it ensures that the hyperbolic distance between any two points in a
quantization bin is bounded by ϵ.
We assume that the data points are confined to a region with a Euclidean radius of Ron the Poincaré disc,
and correspondingly within a hyperbolic radius of RH. Given the curvature constant k, we lets= 1/√
k.
We can then relate RandRHas follows (Hitchman, 2009):
RH=sln/parenleftbiggs+R
s−R/parenrightbigg
andR=s/parenleftigg
eRH
s−1
eRH
s+ 1/parenrightigg
. (9)
Furthermore, since the Poincaré disc is conformal, it preserves angles. Hence, we directly perform quanti-
zation in the hyperbolic plane, and then map the points back to the Poincaré disc. Towards this end, we
start by observing that the circumference of a circle centered at origin and with hyperbolic radius rHequals
12Published in Transactions on Machine Learning Research (01/2024)
(Hitchman, 2009):
Cir(rH) = 2πssinh/parenleftigrH
s/parenrightig
(10)
We first partition the Poincaré disc in terms of concentric circles around the origin, and then for each such
partition, we perform further radial partition.
LetNΘdenote the number of angular quantization bins. Consider two points at the outer boundary of
the outermost quantization bin. Then, by (10), the length along the outer boundary equals Cir(RH)/NΘ.
Bounding this length by ϵ/2, we haveNΘ= 2Cir(RH)/ϵ. Clearly, the inner boundaries of the radial bins
will have a length ≤ϵ/2.
Next, letNRHdenote the number of radial bins for any given angle. We create radial bins in such a way
that their hyperbolic lengths are upper-bounded by ϵ/2. Hence, the total number of quantization bins equals
NRH=2RH/ϵ. We write ϕn1=n1
NΘ2π, wheren1∈[NΘ], and forn2∈[NRH]set
hn2=n2
NRHRH. (11)
Given this notation, a quantization bin B(n1,n2)is completelycharacterized by ϕn1−1andϕn1in theangular
domain, and hn1−1andhn1in the radial domain, where ϕ0=0andh0=0. Furthermore, any point in the bin
B(n1,n2)is mapped to the bin-center bc(n1,n2), defined as the point that partitions the bin into four parts
of the same hyperbolic radial length and angularly symmetric. As a result, the angular ϕbc(n1,n2)and radial
hbc(n1,n2)coordinates of the bin-center equal
ϕbc(n1,n2)=ϕn1−1+ϕn1
2andhbc(n1,n2)=hn2−1+hn2
2. (12)
To map the results from the hyperbolic plane to the Poincaré disc, the Euclidean radius of the corresponding
bin-center is obtained using (9) and its 2D projection is obtained using ϕbc(n1,n2).
As an example, consider a point x∈B2
kwithin a bin B(n1,n2). It is quantized to
ˆx= (αcos(ζ),αsin(ζ)), (13)
whereζ=ϕbc(n1,n2),α=s(eτ
s−1)/(eτ
s+ 1)andτ=hbc(n1,n2).
Using Alg. 1, a client i∈{1,...,L}quantizes the extreme points in the minimal convex hulls CH(i)
+and
CH(i)
−to obtain the quantized point sets Q(i)
+andQ(i)
−. Finally, to obtain the quantized convex hulls ˆCH(i)
+
and ˆCH(i)
−(see Definition 3), the client applies Alg. 1 on Q(i)
+andQ(i)
−. This ensures that the quantized
convex hulls remain convex.
5.3 Convex Hull Complexity
Since the convex hull complexity depends on the distribution of the data and characterizing convex hulls in
hyperbolic space is complicated, we do not have a general complexity analysis for arbitrary (quantized) data
distribution. Instead, we analyze the convex hull complexity assuming that the data is uniformly distributed
on the Poincaré disc. It can be shown that the expected convex hull complexity, i.e., the expected number
of extreme points in the convex hull of any collection of Npoints sampled independently and uniformly
at random from the Poincaré disc equals O(N1
3)(see Appendix D). In comparison, for a two-dimensional
Euclidean space, the expected convex hull complexity also equals O(N1
3),provided that the points are
sampled independently and uniformly at random Har-Peled (2011). Furthermore, note that our scheme
uses quantized extreme points of the convex hull, so that we need to find an upper bound on the ϵ-convex
hull complexity (see Definition 4), assuming that the data is uniformly distributed over the Poincaré disc.
Yet, rather than directly analyzing the ϵ-convex hull complexity, which is a result of constructing convex
hulls, quantizing, constructing convex hulls, we have the following result that characterizes the average
number of points in the minimal convex hull of Nquantized points sampled independently and uniformly at
random from the Poincaré disc. This result serves as an upper bound on the ϵ-convex hull complexity. The
proof of the theorem is available in Appendix E. Recall that this complexity value also captures the privacy
leakage of each client.
13Published in Transactions on Machine Learning Research (01/2024)
Theorem 5.2. Letx1,...,xNbeNpoints uniformly distributed over a Poincaré disc of radius R<s, where
Nis sufficiently large. Let ˆx1,..., ˆxNbe quantized values of x1,...,xN, obtained using the quantization
rule described in (13), with parameter ϵ=O(N−c),for some constant c>0. Then, the expected number of
extreme points of the minimal convex hull of ˆx1,..., ˆxNis at mostO(Nc)whenc <1
2, at mostO(N1−c)
when1
2≤c≤2
3, and at most O(N1
3)when2
3<c.
5.4 Label Encoding
Due to the problem of label switching, different classes at different clients may have the same label and be
confused at the server. To avoid this issue, we propose encoding the class labels using Bhsequences, defined
as follows.
Definition 5. Bhsequences (or Sidon sets of order h) are sets (sequences) of nonnegative integers such that
the sums of any h(with repetitions allowed) integers from the set are all different. Formally, a sequence
A={a1,...,am},0≤a1<a2<...<a m,is aBhsequence if for any i1,...,ih∈{1,...,m}, the multiset
{i1,...,ih}can be uniquely determined based on the sum ai1+...+aih.
Bhsequences for h= 2(Sidon sequences) have found numerous applications in error-correction coding
(Milenkovic et al., 2006; Kovačević & Tan, 2018). For Bhsequences, the question of interests is how small the
mth term,am, can be, or conversely, how large mcan be given a bound on am. It can be shown that amis at
leastmh
hsincethereare mhpossiblesumsof hintegersinAandthemaximumsum am−h+1+am−h+2+...+am
is at leastmh. Therefore, we have ham≥mh. This simple lower bound on amshows that amscales as
O(mh)for fixedh. On the other hand, the work in (Bose & Chowla, 1960) provides constructions for Bh
setsAwhereamis at mostmh, and this is asymptotically optimal based on the lower bound on am.
Note that for any BhsequenceA={a1,...,am},the setA′={a2−a1,...,am−a1}is also aBhset
(Kovacevic & Tan, 2017). Moreover, the sums a′
i1+a′
i2+...+a′
ih′are different for any h′≤handi1,...,ih′,
wherea′
i,i∈{1,...,m−1}is an element in A′. Therefore, in the following, we refer to Bhsequences as a
nonnegative integer set A′in which the sums of any collection of at most helements are different.
LetFqbe a finite field, where q≥max{(L+ 1)h,B}is a prime and Bis the number of quantized bins. We
construct a BhsequenceA′⊂F2L
q={a1,...,a 2L}over the field Fqusing the approach in (Bose & Chowla,
1960) (see Appendix G). To encode the data, we label the quantized bins applied on the quantized convex
hull of the data with unique elements from A′. As before, let ˆCH(i)
+and ˆCH(i)
−be the quantized convex
hulls of the local data at client i, and Section 5.2.
Define a label vector v(i)∈ZBbased on the sets ˆCH(i)
+and ˆCH(i)
−according to
v(i)
j=

a2i, if there is a ˆxi,j∈ˆCH(i)
+but no ˆxi,j∈ˆCH(i)
−within thejth bin,
a2i−1, if there is a ˆxi,j∈ˆCH(i)
−but no ˆxi,j∈ˆCH(i)
+within thejth bin,
a2i+a2i−1,if there is a ˆxi,j∈ˆCH(i)
+and a ˆxi,j∈ˆCH(i)
−within thejth bin,
0, otherwise.
Note that there is a one-to-one mapping between the vector v(i)and the sets ˆCH(i)
+and ˆCH(i)
−. To calculate
v(i)in a federated setting, the clients need an agreement on the client labels (so that the ith client uses a2i−1
ora2ito label the convex hull). Note that this agreement on the client labels is not revealed to the server.
Hence, the server cannot infer the identity of the client that uses labels a2i−1ora2i. The agreement can be
performed using our decentralized agreement protocol described in Appendix B.
5.5 Secure Transmission
To securely communicate the quantized extreme points ˆCH(i)
+and ˆCH(i)
−at each client i∈[L]to the server,
we leverage the SCMA scheme described in (Pan et al., 2023b), where the clients transmit the encoded
version of their data such that the server only gets the aggregated data distribution of the union of local
14Published in Transactions on Machine Learning Research (01/2024)
data sets, without leaking information about the identity of individual data. The SCMA scheme encodes
data into Reed-Solomon type codes (Reed-Solomon codes are a class of rate-optimal error-correcting codes)
and can be shown to be efficient both communication- and computation-wise (Pan et al., 2023b).
Recall that the labels of data class at each server iare described by a vector v(i)based onBhsequences.
The SCMA scheme is used to communicate the vectors v(i),i= 1,...,L. More specifically, each client
i∈{1,...,L}communicates (S(i)
1,...,S(i)
2LKmax)to the server, where Kmax= maxi∈[L](|ˆCH(i)
+|+|ˆCH(i)
−|)
andS(i)
l= (/summationtext
j∈[B]v(i)
j·jl−1+z(i)
l)modq,i∈[2KmaxL], andz(i)
lis a random key uniformly distributed
over the prime field Fqand hidden from the server. The keys {z(i)
l}i∈[L],l∈[2LKmax]are generated offline using
standard secure model aggregation so that (/summationtext
i∈[L]z(i)
l)modq= 0.
To decode, the server uses a Reed-Solomon-type decoder to recover the sum of the label vectors v(i),i∈[L].
The server first computes the sum Sl= (/summationtext
i∈[L]S(i)
l)modq. GivenSl, forl∈[2LKmax], the server computes
the sum of labels Hb=/summationtext
i∈[L]v(i)
bfor each bin index b∈[B]as follows. Since there are at most KmaxL
non-zero entries Hb, the server computes the polynomial g(z) =/producttext
b:Hb̸=0(1−b·z)using the Berlekamp-
Massey algorithm (Berlekamp, 1968; Massey, 1969). Then, the server factorizes g(z)using the algorithm
in (Kedlaya & Umans, 2011), and finds the set {b:Hb̸= 0}. Finally, the server solves the system of linear
equationsSl=/summationtext
l:Hb̸=0Hbbl−1forl∈[2KmaxL], by viewing the Hbs as unknown variables and bl−1, where
Hb̸= 0, as known coefficients.
Once the values Hb=/summationtext
i∈[L]v(i)
b,b∈[B], are obtained, the server retrieves the unique set of indices
i1,...,ih′,h′≤hsuch thatHb=ai1+...+aih′, using brute force, which can be done since a1,...,a 2L
is aBhsequence. Finally, the server includes the bin centroid of the bth bin into ˆCH(i)
−orˆCH(i)
+if
2i−1∈{i1,...,ih′}or2i∈{i1,...,ih′}, respectively. Therefore, the server recovers the convex hulls ˆCH(i)
+
and ˆCH(i)
−fori∈[L]without knowing the exact identity of the clients that contributed the points.
Remark 5.3. In the proposed framework, the server can determine the points in ˆCH(i)
+and ˆCH(i)
−for
i∈[L]. While this constitutes data privacy leakage, we emphasize again that the associated client identities
for the recovered quantized boundary sets are hidden from the FL server, thus protecting the client identity.
Furthermore, these shared convex hull sets contain quantized versions of the original extreme points, further
reducing data privacy leakage. Additionally, as observed in Theorem 5.2 and observed from our real-world
experiments, the convex hull complexity is small.
Communication Complexity. Using the SCMA transmission protocol from (Pan et al., 2023b), each
clienti∈[L]communicates O(K(i)Lmax{hlogL,logB})bits to the server, where K(i)denotes the total
number of extreme points in the quantized convex hulls ˆCH(i)
+and ˆCH(i)
−(convex hull complexity) of the
two local clusters at client i,hdenotes the number of point collisions (the number of clients that share the
same quantized point in the convex hulls of their local clusters), and B, as before, equals the total number
of quantization bins. Note that the convex hull complexity K(i)tends to be small in practice (which is also
corroborated by our experiments on different data sets). In addition, from our experiments we observe that
small values of hsuffice to avoid collisions and guarantee successful decoding of the local clusters CH(i)
+and
CH(i)
−,i∈[L], at the server side.
Computational Complexity. The encoding algorithm for the convex hulls using the SCMA protocol has
complexity O((K(i))2Llog(MiL)), where, as before, Midenotes the number of data points at client i.
The decoding of the aggregated labels/summationtext
i∈[L]v(i)in the SCMA scheme has complexity
O(maxi∈[L][(K(i)L)3
2logq+K(i)Llog2q]), whereq= max{B,hlogL}.The brute-force decoding al-
gorithm of the Bhlabels of the convex hulls of each cluster has complexity O(Lh).
15Published in Transactions on Machine Learning Research (01/2024)
5.6 Balanced Grouping of Convex Hulls at the Server
After the label decoding phase, the FL server can correctly recover the quantized convex hulls {ˆCH(i)
+}i∈[L]
and{ˆCH(i)
−}i∈[L], albeit without knowing the global ground truth labels of these point sets. The convex
hulls have to be grouped correctly to match the ground truth labels. For notational convenience, we denote
the set of all quantized convex hulls by {CH(i)}i∈[2L], where each CH(i)fori∈[2L]is one of{CH(i)
+}i∈[L]
or{CH(i)
−}i∈[L]. We construct a weighted complete undirected graph over {CH(l)}l∈[2L]as follows. Let
G(V)be a complete undirected graph with node set V={1,..., 2L}, where for simplicity, we use v∈V
to represents the convex hull CH(v). For a pair of nodes u,v∈V, the edge-weight w({u,v}) = 1/dG(u,v),
wheredG(u,v)is the average pairwise hyperbolic distance between points in the convex hulls CH(u)and
CH(v), as described in (8). The grouping of the clusters {CH(i)}i∈[2L]is determined by finding the minimum
balanced cut on the weighted graph G, i.e.,
S∗= arg min
S⊂V,|S|=L/summationdisplay
u∈S/summationdisplay
v∈V\Sw({u,v}), (14)
for which we use the algorithm in (Kernighan & Lin, 1970). The point sets PS1=/uniontext
i∈S∗CH(i)and
PS2=/uniontext
i∈[2L]\S∗CH(i)are treated as the two global data point sets, and are assigned, without loss of
generality, the global ground truth labels +1and−1.
The grouping method for the convex hulls of different clusters by their global labels has complexity
O(L2maxi1,i2∈[L],i1̸=i2K(i1)K(i2))for the graph Gconstruction and complexity O(IL2logL)for comput-
ing the balanced minimum cut on G, whereIdenotes the number of iterations of the partitioning procedure
(Kernighan & Lin, 1970). We note that due to distortions introduced in the local convex hulls due to
quantization, and the heuristic graph partitioning procedure used in obtaining the global clusters, provable
guarantees for the correctness of the grouping procedure are not easy to derive. Nevertheless, our proposed
solution works well in practice, as demonstrated by our simulation studies of SVM classifier, Section 6.
5.7 Learning the SVM Classifier at the Server
As the final step, the server learns a hyperbolic SVM classifier over the labelled global point sets PS1and
PS2, using the procedure described in Section 3. The global reference point pis computed as the geodesic
mid-point of the closest pair of extreme points in the convex hulls. The normal vector wis obtained by
solving the optimization problem in (6) or (7), as applicable.
We remark that we primarily focus on the linear SVM model. While numerous non-linear kernel SVM models
have been thoroughly investigated for Euclidean classification tasks, and adapting the centralized hyperbolic
SVM algorithm (Chien et al., 2021; Pan et al., 2023a) to accommodate non-linear kernels is straightforward,
since we have access to all data points; in the federated setting, the problem becomes significantly more
complex. Here, the server is restricted from directly accessing individual data points. Instead, the server
only receives aggregated local convex hulls from clients. While kernel SVM remains feasible at the server,
relying solely on convex hulls might compromise the performance of the kernel SVM algorithms. Therefore,
effectively integrating non-linear kernels within our current framework that relies on convex hull approaches
may be challenging. On the other hand, if one could dispose of the use of convex hulls, then potentially
non-linear kernels could be potentially be used, but then we would not know how to resolve the underlying
data privacy issues. We believe that the combination of convex hulls and SVM represents the best current
option in terms of the trade-off between privacy and the model utility.
6 Experiments
We present in what follows results from our numerical studies involving multiple data sets. First, we describe
the baseline algorithms used for comparative simulations. Then, we consider binary classification with
synthetic data sets that are generated using a newly proposed procedure for sampling uniformly at random
from the Poincaré hyperbolic disc. The most important part of the study is to demonstrate how our
proposed framework can be applied for multi-label classification tasks on three real-world biological data
16Published in Transactions on Machine Learning Research (01/2024)
sets. The procedure for extending our proposed framework to multi-label scenario is described in Appendix
H. Additionally, we provide insights into the impact of quantization bin size on classification accuracy and
convex hull complexity. Due to space constraints, some of the details are delegated to Appendix I.
6.1 Baselines
We consider the following centralized schemes for comparative simulations. (a) Centralized Poincaré (CP)
SVM: We train the linear Poincaré SVM classifier 3 from (Pan et al., 2023a) in a centralized way; (b)
Centralized Euclidean (CE): We train the SVM classifier in a centralized way. We also consider the following
federated classification baselines: (c) Federated Poincaré (FLP): We use our method from Section 4; (d)
Federated Euclidean (FLE): We perform at the server side Euclidean SVM classification over the aggregated
global convex hulls.
6.2 Synthetic Data Sets
Data generation. Our proposed pipeline comprises three main steps. First, Ndata points are obtained
using our proposed Poincaré (radius R) uniform sampling procedure described in Alg. 3 of Appendix F.
Second, a separating hyperplane is constructed by first sampling a reference point puniformly at random
at a Euclidean distance of ∥p∥=µRfrom the origin, and then sampling the normal vector w. Here,
0< µ < 1denotes a scaling parameter. Any point that is within a given hyperbolic distance of γ > 0
from the hyperplane is removed from the data set. Third, points are labeled as positive (+) or negative (-),
depending on which side of the sampled hyperplane they are on. One such example synthetic data set is
shown in Figure 4.
Figure 4: Synthetic data classes constructed as described in the Data generation subsection. The red point
denotes the sampled reference point p. The geodesic through the red point is the ground truth hyperplane
that corresponds to the sampled normal vector. In (a), we set N= 20,000, while in (b), we set N= 60,000.
Results. The results are shown in Figure 5. In the first row, we consider the impact of the choice of
reference point used to generate the synthetic data on classification accuracy. As ∥p∥increases, i.e., as the
reference point is further away from the origin, the ground truth separating hyperplane is more curved in the
Euclidean sense (see the ground truth hyperplanes in Figure 4 for comparison). Hence, by increasing ∥p∥,
the performance of both CE and FLE degrades, while the Poincaré baselines achieve near 100%accuracy
consistently. In the second row, we report the impact of varying the margin parameter γon classification
accuracy. As the margin increases, it becomes easier to learn a Euclidean classifier as the data points
corresponding to different labels gradually become linearly separable in the Euclidean sense as well. Hence,
the performance of CE as well as FLE improves with increasing γ. Furthermore, we observe that the
federated baselines are almost as good as their centralized counterparts, both for the settings in the first
and the second row, demonstrating the effectiveness of our global hulls aggregation method, and supporting
the intuition that for SVM classification, extreme points are highly informative. Finally, in the third row,
we report the impact of the quantization parameter ϵon classification accuracy. As expected, when ϵis
17Published in Transactions on Machine Learning Research (01/2024)
Figure 5: Classification accuracy results for the synthetic data sets. The shaded areas represent the 95%
confidence interval for 10independent trials. (a)-(d) Influence of the parameter ∥p∥on the classification
accuracy. (e)-(h) Influence of the margin parameter γon the classification accuracy. (i)-(l) Influence of the
Poincaré quantization parameter ϵon classification accuracy.
small, the quantization regions are small, and hence the local quantized convex hulls that are sent from each
client closely approximate the true local convex hulls, resulting in low quantization distortion and no impact
on the utility. As ϵincreases, the increased distortion of the convex hulls affects the classification accuracy
significantly, for both FLE and FLP schemes. In particular, the overlap between the quantized convex hulls
corresponding to different labels increases as ϵincreases, making it difficult for the server to learn an SVM
classifier, even in the FLP mode.
Table 1: Results for biological data sets, including mean accuracy ( %) and 95%confidence interval of the
baselines for different data sets. The results are based on 10independent trials for each setting. The FL
method with better mean accuracy is plotted in bold-faced letters.
Data set Labels CP (%) CE (%) FLP ( %) FLE ( %)
Olsson 0-7 79.17±0.0068.75±0.0086.04±1.41 75.00±3.51
Lung-Human0-4 72.11±0.0065.99±0.0069.52±2.82 63.54±2.24
[0, 1, 2, 3] 61.94±0.0061.94±0.0061.42±3.19 60.22±3.34
[0, 1, 2, 4] 78.52±0.0070.37±0.0074.15±5.23 67.19±5.98
[0, 1, 3, 4] 70.99±0.0061.07±0.0066.87±8.42 60.99±6.96
[0, 2, 3, 4] 73.33±0.0067.41±0.0075.19±1.44 71.70±3.86
[1, 2, 3, 4] 69.64±0.0066.07±0.0065.00±2.10 64.11±3.73
UC-Stromal0-3 73.20±0.0073.54±0.0070.23±4.88 68.21±4.83
[0, 1, 2] 74.65±0.0073.94±0.00 63.96±4.66 64.24±4.92
[0, 1, 3] 88.47±0.0087.39±0.0087.77±1.51 87.57±1.24
[0, 2, 3] 80.64±0.0082.29±0.0079.67±2.64 77.86±3.41
[1, 2, 3] 80.33±0.0080.33±0.0079.71±4.80 79.46±5.18
18Published in Transactions on Machine Learning Research (01/2024)
6.3 Biological Data Sets
Data sets. We consider multi-label SVM classification for three biological data sets: Olsson’s scRNA-
seq data set (Olsson et al., 2016), UC-Stromal data set (Smillie et al., 2019), and Lung-Human data
set (Vieira Braga et al., 2019). Our simulations are performed on the Poincaré embeddings of these data
sets, which can be obtained using methods described in (Klimovskaia et al., 2020; Skopek et al., 2020). We
illustrate the embeddings in Figure 8, which is also included in Appendix I. For simulating the FL multi-
label classification tasks, we use subsets of data corresponding to different combinations of labels, for both
the UC-Stromal and the Lung-Human data sets. Since the Olsson data set is quite small (it contains only
319points from 8classes), we consider the entire data set for multi-label classification. Detailed information
about the data sets and experimental settings is available in Appendix I.
Results. We present our results in Table 1, which reports mean accuracy ( %) and the 95%confidence
interval over 10independent trials for all baseline settings. We observe that FLP almost always outperforms
FLE up to∼11%, demonstrating that learning a hyperbolic SVM classifier is preferred to simply learning
a Euclidean SVM classifier. However, for UC-Stromal (Labels: [0,1,2]) data, FLE performs slightly better
than FLP. This may be attributed to the distortion of the convex hull or particular selection of the reference
point. We also observe that for the Olsson (Labels: 0−7) and Lung-Human (Labels: [0,2,3,4]) data sets,
the federated baselines have better mean accuracy than their centralized counterparts1. Furthermore, the
performance of the federated baselines demonstrates that our convex hulls aggregation and label resolution
methods indeed perform well in practice, although some of the steps do not have analytical performance
guarantees. Finally, we note that for UC-Stromal (Labels: [0,1,2]) data, federated baselines have a large gap
in accuracy compared to their centralized counterparts. This is because the points lying near the decision
boundaries of SVM classifiers are not well represented through the local quantized convex hulls.
Figure6: Analysisoftheimpactofthequantizationparameter ϵontheconvexhulldistortionandcomplexity.
The blue and green points correspond to labels 0and3in the UC-Stromal data set. The solid lines denote
the quantized convex hulls, while the dotted lines denote the actual convex hulls without quantization.
Choosing the quantization parameter ϵ.We use UC-Stromal data set as an example to illustrate the
impact of the choice of ϵon the performance of the global classifier. In Figure 6, we show how quantization
affects the shape of the quantized convex hulls by considering all the data points corresponding to labels 0
and3. As expected, when ϵincreases, the shapes of the quantized convex hulls are increasingly distorted with
respect to the original convex hulls. To more precisely examine the impact of quantization, we consider our
prior experimental setup for labels 0−3and analyze how the accuracy and convex hull complexity changes
withϵ. The results are shown in Figure 7. In (a), we observe that federated schemes can suffer a drop in
performance when ϵincreases above 0.1. In (b), we consider the complexity of the quantized convex hulls at
the clients corresponding to all the four labels, and plot the average and maximum convex hull complexities
across all clients and labels. As expected, the convex hull complexity decreases when increasing ϵ. In (c),
we present the ratio of the convex hull complexity and the class size across clients and labels for different
choices of the quantization parameter ϵ.
1We find that training SVM classifiers on extreme points of the convex hulls of the classes instead of the entire classes
improves the performance of centralized training, both for Euclidean and Poincaré methods. We believe this to be due to
quantization, which may act like a denoising step and improve the performance compared to centralized benchmarks. The
results are included in Appendix I.
19Published in Transactions on Machine Learning Research (01/2024)
Figure 7: Impact of quantization parameter ϵon the accuracy and convex hull complexity of the UC-Stromal
data set for the multi-label classification setting with labels 0−3). Here, as before, CHdenotes the quantized
convex hull of a client. Average as well as maximum complexities are calculated over all clients and across all
local quantized convex hulls. The shaded areas represent the 95%confidence interval from 10independent
trials.
7 Conclusion
We introduced a novel approach to federated learning in hyperbolic spaces, thereby addressing challenges in
processing hierarchical and tree-like data in distributed and privacy-preserving settings. Specifically, we de-
veloped an end-to-end framework for federated learning of SVM classifiers in the Poincaré disc. The key idea
behind the approach it to leverage securely aggregated convex hulls for information transfer from the clients
to the server. The complexity of convex hulls is analyzed to assess data leakage, and a simple quantization
method is proposed for efficient data communication. We also considered detrimental label switching issues
and resolved them with a new method based on number-theoretic and coding-theoretic ideas. Additionally,
we introduced a novel approach for aggregating client convex hulls using balanced graph partitioning. Ex-
perimental results on multiple single-cell RNA-seq data show improved classification accuracy compared to
Euclidean counterparts, establishing the utility of privacy-preserving learning in hyperbolic spaces.
Our work also introduced many potentially important new research questions. While information sharing
via quantized convex hulls is efficient and secure, extreme points of the quantized convex hulls can contain
important identifiable information about outliers, and addressing this limitation is an important problem
for future work. Characterizing the impact of quantization on privacy leakage and generalizing the privacy-
preserving and communication efficient federated learning protocols to other ML tasks are other questions
of interest.
Acknowledgments
This work was supported by NSF CIF grant 1956384 and the CZI grant DAF2022-249217.
References
Sasikanth Avancha, Amit Baxi, and David Kotz. Privacy in mobile technology for personal healthcare. ACM
Computing Surveys (CSUR) , 45(1):1–54, 2012.
Sara Babakniya, Souvik Kundu, Saurav Prakash, Yue Niu, and Salman Avestimehr. Revisiting sparsity
hunting in federated learning: Why does sparsity consensus matter? Transactions on Machine Learning
Research , 2023.
Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate impact on
model accuracy. Advances in neural information processing systems , 32, 2019.
Wenxuan Bao, Haohan Wang, Jun Wu, and Jingrui He. Optimizing the collaboration structure in cross-silo
federated learning. In International Conference on Machine Learning . PMLR, 2023.
Elwyn R. Berlekamp. Algebraic coding theory. In McGraw-Hill series in systems science , 1968.
20Published in Transactions on Machine Learning Research (01/2024)
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel,
Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-preserving machine
learning. In proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security ,
pp. 1175–1191, 2017.
Raj Chandra Bose and Sarvadaman Chowla. Theorems in the additive theory of numbers. Technical report,
North Carolina State University. Dept. of Statistics, 1960.
JeanBourgain. Onlipschitzembeddingoffinitemetricspacesinhilbertspace. Israel Journal of Mathematics ,
52:46–52, 1985.
Ines Chami, Albert Gu, Vaggos Chatziafratis, and Christopher Ré. From trees to continuous embeddings
and back: Hyperbolic hierarchical clustering. Advances in Neural Information Processing Systems , 33:
15065–15076, 2020a.
Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, and Christopher Ré. Low-dimensional
hyperbolic knowledge graph embeddings. arXiv preprint arXiv:2005.00545 , 2020b.
Boli Chen, Xin Huang, Lin Xiao, Zixin Cai, and Liping Jing. Hyperbolic interaction model for hierarchical
multi-label classification. In Proceedings of the AAAI conference on artificial intelligence , volume 34, pp.
7496–7503, 2020.
Yankai Chen, Menglin Yang, Yingxue Zhang, Mengchen Zhao, Ziqiao Meng, Jianye Hao, and Irwin King.
Modeling scale-free graphs with hyperbolic geometry for knowledge-aware recommendation. In Proceedings
of the Fifteenth ACM International Conference on Web Search and Data Mining , pp. 94–102, 2022.
Vivying SY Cheng and Patrick CK Hung. Health insurance portability and accountability act (hippa)
compliant access control model for web services. International Journal of Healthcare Information Systems
and Informatics (IJHISI) , 1(1):22–39, 2006.
Eli Chien, Chao Pan, Puoya Tabaghi, and Olgica Milenkovic. Highly scalable and provably accurate clas-
sification in poincaré balls. In 2021 IEEE International Conference on Data Mining (ICDM) , pp. 61–70.
IEEE, 2021.
Eli Chien, Puoya Tabaghi, and Olgica Milenkovic. Hyperaid: Denoising in hyperbolic spaces for tree-fitting
and hierarchical clustering. arXiv preprint arXiv:2205.09721 , 2022.
Hyunghoon Cho, Benjamin DeMeo, Jian Peng, and Bonnie Berger. Large-margin classification in hyperbolic
space. In International Conference on Artificial Intelligence and Statistics , pp. 1832–1840. PMLR, 2019.
AlexanderChowdhury, HasanKassem, NicolasPadoy, RenatoUmeton, andAlexandrosKarargyris. Areview
of medical federated learning: Applications in oncology and cancer research. In Brainlesion: Glioma,
Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 7th International Workshop, BrainLes 2021,
Held in Conjunction with MICCAI 2021, Virtual Event, September 27, 2021, Revised Selected Papers,
Part I, pp. 3–24. Springer, 2022.
Ittai Dayan, Holger R Roth, Aoxiao Zhong, Ahmed Harouni, Amilcare Gentili, Anas Z Abidin, Andrew Liu,
Anthony Beardsworth Costa, Bradford J Wood, Chien-Sung Tsai, et al. Federated learning for predicting
clinical outcomes in patients with covid-19. Nature medicine , 27(10):1735–1743, 2021.
Bhuwan Dhingra, Christopher Shallue, Mohammad Norouzi, Andrew Dai, and George Dahl. Embedding text
in hyperbolic spaces. In Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Lan-
guage Processing (TextGraphs-12) , pp. 59–69, New Orleans, Louisiana, USA, June 2018. Association for
Computational Linguistics. doi: 10.18653/v1/W18-1708. URL https://aclanthology.org/W18-1708 .
W Diffie and ME Hellman. " new directions in cryptography" ieee transactions on information theory, v.
it-22, n. 6. 1976.
Jiarui Ding and Aviv Regev. Deep generative model embedding of single-cell rna-seq profiles on hyperspheres
and hyperbolic spaces. Nature communications , 12(1):2554, 2021.
21Published in Transactions on Machine Learning Research (01/2024)
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations and
Trends ®in Theoretical Computer Science , 9(3–4):211–407, 2014.
Bradley Efron. The convex hull of a random set of points. Biometrika , 52(3-4):331–343, 1965.
Ahmed Roushdy Elkordy, Saurav Prakash, and Salman Avestimehr. Basil: A fast and byzantine-resilient
approachfordecentralizedtraining. IEEE Journal on Selected Areas in Communications , 40(9):2694–2716,
2022.
Pengfei Fang, Mehrtash Harandi, and Lars Petersson. Kernel methods in hyperbolic spaces. In Proceedings
of the IEEE/CVF International Conference on Computer Vision , pp. 10665–10674, 2021.
Octavian Ganea, Gary Bécigneul, and Thomas Hofmann. Hyperbolic neural networks. Advances in neural
information processing systems , 31, 2018.
Harshvardhan Gazula, Kelly Rootes-Murdy, Bharath Holla, Sunitha Basodi, Zuo Zhang, Eric Verner, Ross
Kelly, Pratima Murthy, Amit Chakrabarti, Debasish Basu, et al. Federated analysis in coinstac reveals
functional network connectivity and spectral links to smoking and alcohol consumption in nearly 2,000
adolescent brains. Neuroinformatics , pp. 1–15, 2022.
Ronald L. Graham. An efficient algorithm for determining the convex hull of a finite planar set. Info. Proc.
Lett., 1:132–133, 1972.
Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and function using
networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008.
Heini Halberstam and Klaus Friedrich Roth. Sequences . Springer Science & Business Media, 2012.
Sariel Har-Peled. On the expected complexity of random convex hulls. arXiv preprint arXiv:1111.5340 ,
2011.
Michael P Hitchman. Geometry with an introduction to cosmic topology . Jones & Bartlett Learning, 2009.
Timothy Hughes, Young Hyun, and David A Liberles. Visualising very large phylogenetic trees in three
dimensional hyperbolic space. BMC bioinformatics , 5:1–6, 2004.
Yueyu Jiang, Puoya Tabaghi, and Siavash Mirarab. Learning hyperbolic embedding for phylogenetic tree
placement and updates. Biology, 11(9):1256, 2022.
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open
problems in federated learning. Foundations and Trends ®in Machine Learning , 14(1–2):1–210, 2021.
Kazutoshi Kan. Seeking the ideal privacy protection: Strengths and limitations of differential privacy.
Monetary and Economic Studies , 41:49–80, 2023.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International
conference on machine learning , pp. 5132–5143. PMLR, 2020.
Kiran S Kedlaya and Christopher Umans. Fast polynomial factorization and modular composition. SIAM
Journal on Computing , 40(6):1767–1802, 2011.
Brian W Kernighan and Shen Lin. An efficient heuristic procedure for partitioning graphs. The Bell system
technical journal , 49(2):291–307, 1970.
Valentin Khrulkov, Leyla Mirvakhabova, Evgeniya Ustinova, Ivan Oseledets, and Victor Lempitsky. Hyper-
bolic image embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 6418–6428, 2020.
Daniel Kifer and Ashwin Machanavajjhala. No free lunch in data privacy. In Proceedings of the 2011 ACM
SIGMOD International Conference on Management of data , pp. 193–204, 2011.
22Published in Transactions on Machine Learning Research (01/2024)
Anna Klimovskaia, David Lopez-Paz, Léon Bottou, and Maximilian Nickel. Poincaré maps for analyzing
complex hierarchies in single-cell data. Nature communications , 11(1):2966, 2020.
Mladen Kovacevic and Vincent YF Tan. Improved bounds on sidon sets via lattice packings of simplices.
SIAM Journal on Discrete Mathematics , 31(3):2269–2278, 2017.
Mladen Kovačević and Vincent YF Tan. Codes in the space of multisets—coding for permutation channels
with impairments. IEEE Transactions on Information Theory , 64(7):5156–5169, 2018.
Jiaxiang Li and Shiqian Ma. Federated learning on riemannian manifolds. arXiv preprint arXiv:2206.05668 ,
2022.
Qinbin Li, Bingsheng He, and Dawn Song. Practical one-shot federated learning for cross-silo setting. arXiv
preprint arXiv:2010.01017 , 2020a.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods,
and future directions. IEEE signal processing magazine , 37(3):50–60, 2020b.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. Proceedings of Machine learning and systems , 2:429–450, 2020c.
Ya-Wei Eileen Lin, Ronald R Coifman, Gal Mishne, and Ronen Talmon. Hyperbolic diffusion embedding
and distance for hierarchical representation learning. arXiv preprint arXiv:2305.18962 , 2023.
Nathan Linial, Eran London, and Yuri Rabinovich. The geometry of graphs and some of its algorithmic
applications. Combinatorica , 15:215–245, 1995.
Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks. In Advances in Neural
Information Processing Systems , pp. 8230–8241, 2019.
Malte D Luecken and Fabian J Theis. Current best practices in single-cell rna-seq analysis: a tutorial.
Molecular systems biology , 15(6):e8746, 2019.
James Massey. Shift-register synthesis and bch decoding. IEEE transactions on Information Theory , 15(1):
122–127, 1969.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and
statistics , pp. 1273–1282. PMLR, 2017.
Olgica Milenkovic, Navin Kashyap, and David Leyba. Shortened array codes of large girth. IEEE Transac-
tions on Information Theory , 52(8):3707–3722, 2006.
Erum Mushtaq, Jie Ding, and Salman Avestimehr. What if kidney tumor segmentation challenge (kits19)
never happened. In 2022 21st IEEE International Conference on Machine Learning and Applications
(ICMLA) , pp. 1740–1747. IEEE, 2022.
Andrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. Advances
in neural information processing systems , 14, 2001.
Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical representations. In
Advances in Neural Information Processing Systems , pp. 6338–6347, 2017.
Maximillian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model of hyperbolic
geometry. In International conference on machine learning , pp. 3779–3788. PMLR, 2018.
Yue Niu, Saurav Prakash, Souvik Kundu, Sunwoo Lee, and Salman Avestimehr. Overcoming resource
constraints in federated learning: Large models can be trained with only weak clients. Transactions on
Machine Learning Research , 2023.
23Published in Transactions on Machine Learning Research (01/2024)
Andre Olsson, Meenakshi Venkatasubramanian, Viren K Chaudhri, Bruce J Aronow, Nathan Salomonis,
Harinder Singh, and H Leighton Grimes. Single-cell analysis of mixed-lineage states leading to a binary
cell fate choice. Nature, 537(7622):698–702, 2016.
Chao Pan, Eli Chien, Puoya Tabaghi, Jianhao Peng, and Olgica Milenkovic. Provably accurate and scalable
linear classifiers in hyperbolic spaces. Knowledge and Information Systems , pp. 1–34, 2023a.
Chao Pan, Jin Sima, Saurav Prakash, Vishal Rana, and Olgica Milenkovic. Machine unlearning of federated
clusters. In International Conference on Learning Representations , 2023b. URL https://openreview.
net/forum?id=VzwfoFyYDga .
Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning
in python. the Journal of machine Learning research , 12:2825–2830, 2011.
Wei Peng, Tuomas Varanka, Abdelrahman Mostafa, Henglin Shi, and Guoying Zhao. Hyperbolic deep
neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence , 44(12):
10023–10044, 2021.
Aditya Petety, Sandhya Tripathi, and N Hemachandra. Attribute noise robust binary classification (student
abstract). In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pp. 13897–13898,
2020.
Sundar Pichai. Google’s Sundar Pichai: Privacy Should Not Be a Luxury Good. In New York Times , 2019.
John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood
methods. Advances in Large Margin Classifiers , 10(3):61–74, 1999.
Saurav Prakash, Sagar Dhakal, Mustafa Riza Akdeniz, Yair Yona, Shilpa Talwar, Salman Avestimehr, and
Nageen Himayat. Coded computing for low-latency federated learning over wireless edge networks. IEEE
Journal on Selected Areas in Communications , 39(1):233–250, 2020a.
Saurav Prakash, Hanieh Hashemi, Yongqin Wang, Murali Annavaram, and Salman Avestimehr. Secure and
fault tolerant decentralized learning. arXiv preprint arXiv:2010.07541 , 2020b.
Saurav Prakash, Amirhossein Reisizadeh, Ramtin Pedarsani, and Amir Salman Avestimehr. Hierarchical
coded gradient aggregation for learning at the edge. In 2020 IEEE International Symposium on Informa-
tion Theory (ISIT) , pp. 2616–2621. IEEE, 2020c.
Félix Raimundo, Laetitia Meng-Papaxanthos, Céline Vallot, and Jean-Philippe Vert. Machine learning for
single-cell genomics data analysis. Current Opinion in Systems Biology , 26:64–71, 2021.
John G Ratcliffe, S Axler, and KA Ribet. Foundations of hyperbolic manifolds , volume 149. Springer, 1994.
John G Ratcliffe, S Axler, and KA Ribet. Foundations of hyperbolic manifolds , volume 149. Springer, 2006.
Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi Albarqouni, Spyridon Bakas,
Mathieu N Galtier, Bennett A Landman, Klaus Maier-Hein, et al. The future of digital health with
federated learning. NPJ digital medicine , 3(1):119, 2020.
Debbrata K Saha, Vince D Calhoun, Yuhui Du, Zening Fu, Soo Min Kwon, Anand D Sarwate, Sandeep R
Panta, and Sergey M Plis. Privacy-preserving quality control of neuroimaging datasets in federated envi-
ronments. Human Brain Mapping , 43(7):2289–2310, 2022.
Frederic Sala, Chris De Sa, Albert Gu, and Christopher Ré. Representation tradeoffs for hyperbolic embed-
dings. In International conference on machine learning , pp. 4460–4469. PMLR, 2018.
Saber Salehkaleybar, Arsalan Sharifnassab, and S Jamaloddin Golestani. One-shot federated learning: theo-
reticallimitsandalgorithmstoachievethem. The Journal of Machine Learning Research , 22(1):8485–8531,
2021.
24Published in Transactions on Machine Learning Research (01/2024)
Rik Sarkar. Low distortion delaunay embedding of trees in hyperbolic plane. In Graph Drawing: 19th
International Symposium, GD 2011, Eindhoven, The Netherlands, September 21-23, 2011, Revised Selected
Papers 19 , pp. 355–366. Springer, 2012.
Felix Sattler, Klaus-Robert Müller, and Wojciech Samek. Clustered federated learning: Model-agnostic
distributed multitask optimization under privacy constraints. IEEE transactions on neural networks and
learning systems , 32(8):3710–3722, 2020.
Micah J Sheller, Brandon Edwards, G Anthony Reina, Jason Martin, Sarthak Pati, Aikaterini Kotrotsou,
Mikhail Milchenko, Weilin Xu, Daniel Marcus, Rivka R Colen, et al. Federated learning in medicine:
facilitating multi-institutional collaborations without sharing patient data. Scientific reports , 10(1):1–12,
2020.
Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on pattern
analysis and machine intelligence , 22(8):888–905, 2000.
Ondrej Skopek, Octavian-Eugen Ganea, and Gary Bécigneul. Mixed-curvature variational autoencoders. In
8th International Conference on Learning Representations (ICLR 2020)(virtual) . International Conference
on Learning Representations, 2020.
Christopher S Smillie, Moshe Biton, Jose Ordovas-Montanes, Keri M Sullivan, Grace Burgin, Daniel B
Graham, Rebecca H Herbst, Noga Rogel, Michal Slyper, Julia Waldman, et al. Intra-and inter-cellular
rewiring of the human colon during ulcerative colitis. Cell, 178(3):714–730, 2019.
Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with
deep neural networks: A survey. IEEE Transactions on Neural Networks and Learning Systems , 2022.
Rishi Sonthalia and Anna Gilbert. Tree! i am no tree! i am a low dimensional hyperbolic embedding.
Advances in Neural Information Processing Systems , 33:845–856, 2020.
Puoya Tabaghi and Ivan Dokmanić. On procrustes analysis in hyperbolic space. IEEE Signal Processing
Letters, 28:1120–1124, 2021.
Puoya Tabaghi, Chao Pan, Eli Chien, Jianhao Peng, and Olgica Milenkovic. Linear classifiers in product
space forms. arXiv preprint arXiv:2102.10204 , 2021.
Tian Tian, Cheng Zhong, Xiang Lin, Zhi Wei, and Hakon Hakonarson. Complex hierarchical structures in
single-cell genomics data unveiled by deep hyperbolic manifold learning. Genome Research , 33(2):232–246,
2023.
AlexandruTifrea, GaryBecigneul, andOctavian-EugenGanea. Poincaréglove: hyperbolicwordembeddings.
InInternational Conference on Learning Representations , 2019. URL https://openreview.net/forum?
id=Ske5r3AqK7 .
Abraham A Ungar. Hyperbolic trigonometry and its application in the poincaré ball model of hyperbolic
geometry. Computers & Mathematics with Applications , 41(1-2):135–147, 2001.
J Vermeer. A geometric interpretation of ungar’s addition and of gyration in the hyperbolic plane. Topology
and its Applications , 152(3):226–242, 2005.
Felipe A Vieira Braga, Gozde Kar, Marijn Berg, Orestes A Carpaij, Krzysztof Polanski, Lukas M Simon,
Sharon Brouwer, Tomás Gomes, Laura Hesse, Jian Jiang, et al. A cellular census of human lungs identifies
novel cell states in health and in asthma. Nature medicine , 25(7):1153–1163, 2019.
Xidong Wu, Zhengmian Hu, and Heng Huang. Decentralized riemannian algorithm for nonconvex minimax
problems. arXiv preprint arXiv:2302.03825 , 2023.
Adilson Elias Xavier. The hyperbolic smoothing clustering method. Pattern Recognition , 43(3):731–737,
2010.
25Published in Transactions on Machine Learning Research (01/2024)
Tao Yu and Christopher M De Sa. Numerically accurate hyperbolic embeddings using tiling-based models.
Advances in Neural Information Processing Systems , 32, 2019.
Tao Zhang, Tianqing Zhu, Renping Liu, and Wanlei Zhou. Correlated data in differential privacy: definition
and analysis. Concurrency and Computation: Practice and Experience , 34(16):e6015, 2022.
YidingZhang,XiaoWang,ChuanShi,XunqiangJiang,andYanfangYe. Hyperbolicgraphattentionnetwork.
IEEE Transactions on Big Data , 8(6):1690–1701, 2021.
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning
with non-iid data. arXiv preprint arXiv:1806.00582 , 2018.
Yanlin Zhou, George Pu, Xiyao Ma, Xiaolin Li, and Dapeng Wu. Distilled one-shot federated learning. arXiv
preprint arXiv:2009.07999 , 2020.
A Poincaré Graham Scan
A.1 Proof of Correctness
Theorem A.1. Given a set of NpointsDin the Poincaré disc, Alg. 1 returns CH(D)inO(NlogN)time.
The proof follows the steps of the proof of the Graham Scan algorithm for Euclidean spaces. However,
one has to adapt several steps, such as the initial sorting procedure to accommodate the Poincaré disc as
presented in Alg. 1. Before we proceed with the proof, we need the following lemmas. The results use the
same definitions and notation as stated in Alg. 1.
Lemma A.2. Letlbbe the geodesic starting at bwith tangent vector t. All points inDand the origin olie
on the same side of lb.
Proof.Since bis the point of largest norm (at the largest distance from the origin), all points in Dand the
origin olie on or inside the circle centered at oand of radius∥o−b∥2. Denote this circle by O. By the
definition of a geodesic in the Poincaré disc, it is either a circle arc orthogonal to the boundary of the disc
or a Euclidean line going through o. Clearly, for b̸=o,lbis a geodesic of the first type. Also, since lbstarts
atbby definition, bis the midpoint of the circle arc. As a result, lbis tangent to Oand only intersects it
atb. Therefore, all points in Dand the origin olie on the same side of lb.
Lemma A.3. Leta1,a2,a3be points in the Poincaré disc forming a triangle ∆a1a2a3(i.e., not
all three points lie on the same geodesic). Then, sign(CCW( a1,a2,x)) = sign(CCW( a2,a3,x)) =
sign(CCW( a3,a1,x)) if and only if there exists another point x∈∆a1a2a3.
Proof.Note that a point x∈∆a1a2a3if and only if it is “on the same side” (i.e. clockwise or counter-
clockwise) with respect to all three geodesics la1→a2,la2→a3andla3→a1.This is equivalent to the condition
sign(CCW( a1,a2,x)) = sign(CCW( a2,a3,x)) = sign(CCW( a3,a1,x)), as claimed.
We are now ready to prove the correctness of Alg. 1.
Proof.By Lemma A.2 all points in D,as well as the origin oare on the same side of the geodesic lb. As
a result, they are in the same half-plane in the tangent space Tbwith a boundary (which is a straight line)
corresponding to lb. By definition, nis the normal vector of that boundary pointing outward (i.e., towards
the side that does not contain any data points). Therefore, the principal angles between each of the logb(xj)
andtlie in [0,π].
Next, we show that retaining only V∪{ b}is correct during the initial sorting step (i.e., showing that
CH(D) =CH(V∪{b})). This is true due to the following reasons. First, note that bmust be in the convex
hullCH(D). Second, let us assume that cis another point in CH(D)but it is not inV. Based on our
initial sorting step, this can only happen when there is some v∈Vsuch that cis onlb→v. That is, the
principal angles for logb(v),logb(c)are the same with respect to tbut∥logb(v)∥>∥logb(c)∥. Now there
26Published in Transactions on Machine Learning Research (01/2024)
are two possible scenarios: 1) v∈CH(D); 2)v/∈CH(D). For case 1), it is clear that none of the points
onlb→vbelong toCH(D), aslb→veither lies inside CH(D)or is the boundary of CH(D). In either case,
c/∈CH(D). For case 2), v/∈CH(D), which means that vis strictly contained within the convex hull
CH(D). Thus, all points on lb→v(excluding the starting point b) are also strictly contained in the convex
hullCH(D). As a result, we have established by contradiction that c/∈CH(D)ifc/∈V. This immediately
impliesCH(D) =CH(V∪{b}).
Next, we focus on showing that the output of Alg. 1 is indeed CH(V∪{b})and thus is equal to CH(D).
We prove this result similarly to what is done for the Graham scan in Euclidean space – via induction.
Without loss of generality, let V=v1,v2,···,vm. LetCj=CH(V[:j]∪{b})(i.e., the convex hull of the first
jpoints ofVandb). The induction hypothesis is as follows. After the jth iteration of the for loop in Alg. 1,
points in STACK are points of Cjin a counter-clockwise order. The base case can be established easily, as
C2contains points of CH({b,v1,v2}) ={b,v1,v2}(a triangle). Since we start at band sortVaccording
to the principal angle (counter-clockwise), the claim is obviously true. For the inductive step, assume the
hypothesis holds for j−1. Since the principal angle of logb(vj)is greater than logb(vj−1)due to the initial
sorting procedure, ∆bvj−1vjis not contained in Cj−1. Thus, vj∈Cj. Now, let a,cbe the two topmost
points of STACK. If CCW( c,a,vj)>0or equivalently CCW( c,vj,a)<0, the algorithm will not pop a.
This is correct since due to the initial sorting, we know that CCW( b,c,a)>0and CCW( b,a,vj)>0and
thus CCW( vj,b,a)>0. By Lemma A.3, we know that a/∈∆bcvjand thus a∈Cj. In this case, all points
inCj−1also belong toCjby the same argument (since their principal angles are sorted). For the case that
CCW( c,a,vj)≤0, again by Lemma A.3 we know that a∈∆bcvjand thus a/∈Cj. As a result, it is correct
to pop out a. Note that the while loop will correctly keep outputting points. Assume that the while loop
stops popping at vj. Then by the same argument, we know that it is correct to include the entire set Cj−1
inCj. Thus our hypothesis is true. Finally, by setting j=mwe arrive the conclusion that the output of
Alg. 1 isCm=CH(V∪{b}) =CH(D). This completes the proof.
Finally, we prove that Alg. 1 produces a minimal convex hull.
Proof.Suppose we obtained the (possibly nonminimal) convex hull C=b,c1,c2,···,cn. Note that due
to the initial sorting, we know that the “principal angles” of cjwith respect to bare listed in increasing
order and that they lie in [0,π]. As a result, in each Graham Scan check-step (line 10), cj+1is outside the
triangle of ∆bcjcj−1. Thus, even if cj+1,cj,cj−1lie on the same geodesic (i.e., CCW (cj−1,cj,cj+1) = 0),
the algorithm will remove cj(and is allowed to do so). The only two additional cases to consider are for
b,c1,c2andcn−1,cn,bto be on the same geodesic. Due to initial sorting, from a set of points with the same
principal angle, we will only keep the furthest one (line 5). Therefore, the Poincaré Graham Scan indeed
returns the minimal convex hull.
B Decentralized Order Agreement for BhLabel Assignments
We describe next how the participating clients can agree upon an ordering of themselves while keeping it
hidden from the FL server – the ordering has to be protected only from the server, and not the clients
themselves as they do not have information about the convex hulls of other clients.
Clientsparticipateinakeyexchangeprotocol, suchastheDiffie-Hellmankeyagreementmethod, thatenables
any pair of clients to securely exchange secret information over an insecure communication channel (Diffie
& Hellman, 1976; Bonawitz et al., 2017). In essence, the key agreement protocol allows any pair of clients to
establish a common secret key that can be used for secure communication, even in the presence of potential
eavesdroppers, thus ensuring the confidentiality and integrity of their communication. Particularly, in the
first step of the key agreement protocol, each client i∈[L]independently generates a pair of keys: A public
keyK(i,PK )and a secret keyK(i,SK ). As the names suggest, the public key K(i,PK )of clientiis accessible
to other clients and the server, while the private key is not. In the next step of the key agreement protocol,
each pair of clients i1,i2∈[L]agrees on a random secret key RK(i1,i2)known only to clients i1andi2, where
RK(i1,i2)=RK(i2,i1)is a function of the public key K(i1,PK)and secret keyK(i2,SK). Thus, at the end of
the key agreement process, each pair of clients shares a common secret key, not known to other clients or
the server.
27Published in Transactions on Machine Learning Research (01/2024)
For decentralized order agreement, we further assume that clients have access to a public pseudo-
random number generator PRG (·). Each client iusesPRG (·)and obtains a private random number
n(i)=PRG (K(i,SK )). Thereafter, each pair of clients i1andi2secretly exchange their private random num-
bersn(i1)andn(i2)using their common secret key RK(i1,i2)for encryption/decryption. Notably, this ensures
that the private numbers {n(i)}i∈[L]remain hidden from the server. Next, the clients sort the list of the
random numbers {n(i)}i∈[L]. Each client ithen determines its position in the shared random ordering by
finding the position of its number n(i)in the sorted list. Let Π(i)denote the corresponding position of client
iin the shared random ordering. Then, the client picks the numbers a2i−1= 2Π(i)−1anda2i= 2Π(i)from
theBhsequence for labeling of points in their quantized convex hulls ˆCH(i)
+and ˆCH(i)
−,respectively.
C Uniform Poincaré Quantization
We describe another approach for Poincaré disc quantization, using ideas that are similar to those described
in the Poincaré Quantization, Section 5.2. We once exploit radial symmetry and form the quantization bins
by intersecting straight lines passing through the origin and concentric circles around the origin. However,
in this new setting, our goal is to quantize the bounded region within the circle of hyperbolic radius RHso
thateach quantization bin has the same area. With this constraint, we can use finely-grained quantization
bins to perform uniformly at random sampling of points on the Poincaré disc.
First, observe that the area of a circle with the hyperbolic radius of rHis given by the following formula
(Hitchman, 2009):
A(rH) = 4πs2sinh2/parenleftigrH
2s/parenrightig
, (15)
wheres= 1/√
kandkis curvature constant. Let NΘdenote the number of angular quantization bins.
Additionally, let NRHdenote the number of radial quantization bins for a hyperbolic radius RHand any
given angle. The total number of quantization bins therefore equals B=NΘNRH, and the area of each
bin isA(RH)
B. Thus, the boundaries for the bins in the angular direction can be denoted by ϕn1=n1
NΘ2π,
wheren1∈[NΘ]. In what follows, we characterize the radial boundaries hn2for the quantization bins, where
n2∈[NRH]andhNRH=RH.
Consider the circle with hyperbolic radius hn2. Since each quantization bin has same area in the hyperbolic
plane, we have the following relationship between the circles with hyperbolic radii hn2andRH:
A(hn2) =n2
NRHA(RH), (16)
which results in the following relationship between hn2andRH:
hn2= 2ssinh−1/parenleftbigg/radicalbiggn2
NRHsinh/parenleftbiggRH
2s/parenrightbigg/parenrightbigg
. (17)
Therefore, a quantization bin B(n1,n2)is well-defined via its angular boundaries ϕn1−1andϕn1, and radial
boundaries hn1−1andhn1, whereϕ0=0andh0=0. Furthermore, any point in the quantization bin B(n1,n2)
is mapped to the bin-center bc(n1,n2), which we define as the point in the bin which partitions it into
four parts of equal area. Based on similar arguments as above, the angular distance ϕbc(n1,n2)and radial
hyperbolic distance hbc(n1,n2)for the bin-center are as follows:
ϕbc(n1,n2)=ϕn1−1+ϕn1
2andhbc(n1,n2)= 2ssinh−1/parenleftigg/radicaligg
n2−0.5
NRHsinh/parenleftbiggRH
2s/parenrightbigg/parenrightigg
. (18)
To map the results from the hyperbolic plane to the Poincaré disc, the Euclidean radius of the corresponding
bin-center is obtained using (9) and its 2D projection is obtained using ϕbc(n1,n2).
As an example, consider a point x∈B2
kwithin a bin B(n1,n2). It is quantized to
ˆx= (αcos(ζ),αsin(ζ)), (19)
28Published in Transactions on Machine Learning Research (01/2024)
whereζ=ϕbc(n1,n2),α=s(eτ
s−1)/(eτ
s+ 1)andτ=hbc(n1,n2). This approach of uniform area quantization
is key to implementing the Poincaré uniform sampling process used in Alg. 32, as well as for proving the
convex hull complexity result of Theorem 5.2.
D Proof for the Convex Hull Complexity
We show that the expected number of extreme points of the convex hull of Npoints uniformly sampled
from a Poincaré disc is at most O(N1
3). The proof follows similar ideas as those used for the convex hull
complexity results in Euclidean spaces (Har-Peled, 2011), which are adapted to the hyperbolic setting and
involve some technical arguments unique to hyperbolic spaces. We start with a lemma showing that the
expected fraction of points that are extreme points of the convex hull is upper bounded by the expected
fraction of the area in the Poincaré disc not covered by the convex hull. The lemma appears in (Har-Peled,
2011) for the Euclidean space, and in this setting, it is known as Efron’s theorem (Efron, 1965).
Lemma D.1. LetCbe a Poincaré disc of radius R<s. Letf(N)∈[0,1]be a function of an integer N≥0
such that the expected area of the convex hull of Npoints distributed independently and uniformly at random
overCis at least (1−f(N))Area (C). Then, the expected number of extreme points of the convex hull of the
Npoints is at most Nf(N
2).
Proof.The proof is the same as that for the Euclidean space (Har-Peled, 2011), and is presented for
completeness. Uniformly at random partition the Npoints into two subsets, S1andS2,both of cardinality
N
2. LetV1andV2bethesetsofextremepointsoftheconvexhullof S1∪S2thatareinS1andS2, respectively.
Denote the convex hull of S2asCH(S2).
It follows from definition that the expected number of extreme points of the convex hull of S1∪S2is
E[|V1|] +E[|V2|]. Moreover, we have that
E/bracketleftig
|V1|/vextendsingle/vextendsingle/vextendsingleS2/bracketrightig
≤N
2/parenleftigArea (C)−Area (CH(S2))
Area (C)/parenrightig
,
since the probability of a point in S1lying inCH(S2), and thus not in V1(because a point strictly inside
the convex hull is not an extreme point) is at leastArea (CH(S2))
Area (C). Then, it follows that
E[|V1|] =ES2/bracketleftigg
E/bracketleftig
|V1|/vextendsingle/vextendsingle/vextendsingleS2/bracketrightig/bracketrightigg
≤N
2/parenleftigf(N
2)Area (C)
Area (C)/parenrightig
=N
2f(N
2).
Similarly, we have that E[|V2|]≤N
2f(N
2). Therefore, the expected number of extreme points of the convex
hull ofS1∪S2is at mostNf(N
2).
Lemma D.1 implies that if the area of the convex hull of the independently and uniformly at random sampled
Npoints approaches the area of the Poincaré disc, then the expected fraction of the points that are extreme
points of the convex hull goes to zero. In what follows, we show that the area of the convex hull is at least
(1−O(N2
3))times the area of the Poincaré disc, i.e., f(N)≤O(N2
3). Then, it follows from Lemma D.1
that the expected convex hull complexity is at most O(N1
3).
Partition the Poincaré disc into m=N1
3sectorsT1,...,Tm, each having an angle2π
m. LetD1,...,Dm2be
m2discs such that D1is the disc Cand the area of Dlminus the area of Dl+1is the same for l∈[m2],
where the area of Dm2+1is0. Let subsector Tl,j=Tl∩Dj,l∈[m],j∈[m2]be the intersection of sector Tl
and discDj. The subsectors Tl,jhave equal area. This partition is the same as the one for the quantization
2All that is needed for uniform sampling is to construct a quantizer with a sufficiently large number of bins, and then select
points uniformly at random from a discrete collection of bin labels. The sampled point is the centroid of the bin. For more
details, see Section F.
29Published in Transactions on Machine Learning Research (01/2024)
bins described in Section C, where NΘ=mandNRH=m2. We use specific notation for the sectors only
in this section since this is required for formal analysis.
For any sector Tl, letXl∈[1,m2]be the smallest integer such that at least one of the Npoints lies in the
subsectorTl,Xl. ThenPr(Xl=t)≤(1−t−1
N)N≤e−(t−1), since the probability on the left-hand-side is at
most the probability that the subsectors Tl,1,...,Tl,t−1do not contain any of the Npoints. Hence, we have
E[Xl] =m2/summationdisplay
l=1tPr(Xl=t) =O(1). (20)
LetKobe the convex hull of the union of the Npoints and the origin o. We show that Kocompletely
covers at least m2−(Xl+1+Xl−1+O(1))subsectorsTl,Xl+1+Xl−1+O(1),...,Tl,m2in sectorTl. LetPand
Qbe the points that lie in the subsectors Tl−1,Xl−1andTl+1,Xl+1, respectively, where Tm+1,j=T1,jand
T0,j=Tm,jforj∈[1,m2]. We show that the convex hull of the origin oand the points PandQcovers at
leastm2−(Xl+1+Xl−1+O(1))subsectorsTl,Xl+1+Xl−1+O(1),...,Tl,m2in sectorTl.
Without loss of generality, assume that the length of the geodesic oP(which is a straight line in the Poincaré
disc) is larger than the geodesic oQ. Pick a point P′onoPsuch that the length of oP′equals the length
ofoQ. Then,P′lies in the subsector Tl−1,Xl+1. Consider the geodesic between P′andQ, denoted by P′Q.
We have the following lemma.
Lemma D.2. The minimum distance from the origin oto a point on the geodesic P′Qis at least
RXl+1+Xl−1+O(1), whereRj,j∈[1,m2]is the smallest radius of a disc centered at the origin that com-
pletely covers Tl,jforl∈[1,m].
By virtue of Lemma D.2, the subsectors Tl,j,j∈[Xl+1+Xl−1+O(1),m2]are covered by the convex hull of
o,P′andQ, and thus covered by Ko. Hence, the total number of subsectors Tl,jcovered by Kois at least/summationtextm
l=1[m2−(Xl+1+Xl−1+O(1))]. This implies that the expected area of Kois at least
/summationtextm
l=1[m2−(E[Xl+1] +E[Xl−1] +O(1))]
m3= 1−O(N−2
3)
times the area of the Poincaré disc. Note that the probability that the origin olies outside the convex
hull of the Npoints is the probability that there exists a point xamong the Npoints such that the
remainingN−1points lie on the same side of the geodesic passing through oandx. This probability
equalsN
2N−1. Therefore, we conclude that the expected area of the convex hull of the Npoints is at least
(1−Pr(o∈K))(1−O(N−2
3))Area (C) +Pr(o/∈K)0 = 1−O(N−2
3)Area (C), whereCis the Poincaré disc
andKis the convex hull of the Npoints.
To complete the proof, we now prove Lemma D.2. By the geometric property of a geodesic on a Poincaré
disc,P′Qis part of a circle o′centered at a point o′that is orthogonal to the Poincaré disc centered at o
with radius s, in the Euclidean space. The distance from oto the geodesic P′Qis the difference between
the length of oo′and the radius of the circle o′. LetR′be the radius of the circle o′.
Let the length of oP′berand let the line oP′intersect the circle o′at another point P′′. Let the length of
oP′′ber′. Then, we have that rr′=s2since the circle o′is orthogonal to the unit Poincaré disc centered
ato. LetMbe the midpoint of the chord P′P′′. Then o′Mis perpendicular to oMand the length of oM
isr+r′
2=r+s2
r. Let the angle between oP′andoQbeθP′Q, which is at most6π
m. Since the lengths of oP′
andoQare equal, the angle between oMandoo′isθP′Q
2. Hence, the length of oo′is
r+s2
r
2 cosθP′Q
2.
On the other hand, the length of oo′equals/radicalbig
(R′)2+s2. Therefore, we have
r+s2
r
2 cosθP′Q
2=/radicalbig
(R′)2+s2.
30Published in Transactions on Machine Learning Research (01/2024)
Then, the distance from oto the geodesic P′Qcan be written as
/radicalbig
(R′)2+s2−R′=r−(r2+s2)(θP′Q)2
2(s2
r−r)+o((θP′Q)2) =r−O(1
m2).
It is left to be shown that this distance r−O(1
m2)is at leastRXl+1+Xl−1+O(1), whereRlis the radius of
the discDl,l∈[1,m2]. Note that by assumption, the length of oP′equals the length of oQ. Hence,
r∈[RXl+1,RXl+1−1]. Moreover, Rlcan be obtained from (17) by reversing the indexing order and then
from (9), i.e.,
hl= 2ssinh−1/parenleftigg/radicalbigg
m2+ 1−l
m2sinh/parenleftbiggRH
2s/parenrightbigg/parenrightigg
,
Rl=s/parenleftigg
ehl
s−1
ehl
s+ 1/parenrightigg
, (21)
whereRH=sln/parenleftbigs+R
s−R/parenrightbig
andRis the radius of the Poincaré disc C. Since
r−O(1
m2)≥RXl+1−O(1
m2)
=s
ehXl+1−O(1
m2)
s−1
ehXl+1−O(1
m2)
s + 1
,
and
sinh(hXl+1−O(1
m2)
2s) = sinh(hXl+1
2s)−cosh(hXl+1
2s)O(1
m2)
≥sinh(hXl+1+O(1)
2s),
we havehXl+1−O(1
m2)≥hXl+1+O(1)≥hXl+Xl+1+O(1), and thus
r−O(1
m2)≥s
ehXl+Xl+1+O(1)
s−1
ehXl+Xl+1+O(1)
s + 1
=RXl+Xl+1+O(1).
E Proof of Theorem 5.2
We now present the expected convex hull complexity when the data points in the Poincaré disc are quantized,
followingthealgorithminSection 5.2. Specifically, let ϵbethedistancemarginofthequantizationalgorithm
in Section 5.2. We first restate Theorem 5.2 in the following.
Theorem. Letx1,...,xNbeNpoints uniformly distributed over a Poincaré disc of radius R<s, whereN
is sufficiently large. Let ˆx1,..., ˆxNbe quantized versions of x1,...,xN, obtained using the quantization rule
in (11) and (12), with distance margin ϵ=O(N−c),and some constant c>0. Then, the expected number
of extreme points on the minimal convex hull of ˆx1,..., ˆxNis at mostO(Nc)whenc<1
2, at mostO(N1−c)
when1
2≤c≤2
3, and at most O(N1
3)when2
3<c.
Before proving Theorem 5.2, we first state the following lemma, which is a modification of Lemma D.1 for
quantized data points.
Lemma E.1. LetCbe a Poincaré disc of radius R < sand letf1(N)∈[0,1]be a function of an integer
N≥0such that the expected area of the convex hull of ˆx1,..., ˆxNis at least (1−f1(N))Area (C). Then the
expected number of extreme points of the convex hull of ˆx1,..., ˆxNis at mostNf1(N
2).
31Published in Transactions on Machine Learning Research (01/2024)
The proof of Lemma E.1 follows along the same lines of that for Lemma D.1.
In what follows, we prove Theorem 5.2.
Proof.(of Theorem 5.2) We first prove the theorem for c<1
2. Similar to what was done in Appendix D,
we partition the Poincaré disc into NΘ= 2Cir(RH)/ϵ=O(Nc)(See Section 5.2 for definition of NΘ) sectors
T1,...,TNΘand defineNcdiscsD1,...,DNcsuch thatD1is the Poincaré disc Cand the difference between
the area of Dland the area of Dl+1is the same for l∈[Nc]. Then we define subsectors Tl,j=Tl∩Dj,
l∈[NΘ], j∈[Nc], of the same area.
Then, the probability that a given subsector Tl,jis empty is at most (1−1
NΘNc)N≤e−O(N1−2c). By the
union bound, the probability that there exists at least one empty subsector is at most NΘNce−O(N1−2c).
We now show that when every subsector contains at least one point, the number of extreme points of the
convex hull of ˆx1,..., ˆxNisNΘ=O(Nc). This follows from the fact that in each sector, each subsector Tl,1,
l∈[NΘ],that is on the boundary of the Poincaré disc Cis contained in the quantization bin given by (11)
and (12), that is on the boundary of the Poincaré disc C. Moreover, each centroid in the quantization bin
that is on the boundary of the Poincaré disc is an extreme point of the convex hull when every quantization
bin has at least one data point.
LetQdenote the event that there exists an empty quantization bin and Vthe number of extreme points of
the convex hull of ˆx1,..., ˆxN. Then, we have
E[V] =Pr(Q)E[V|Q] +Pr(Qc)E[V|Qc] =NΘ+o(1) =O(Nc).
We now consider an upper bound for the case when1
2≤c≤2
3. We partition the Poincaré disc into
N1−csectorsT1,...,TN1−c, and subpartition each sector TlintoNcsubsectorsTl,1,...,Tl,Ncof equal area,
separated by Nc−1circles. The distance between the origin and the subsector Tl,jdecreases with j∈[Nc]
for everyl∈[N1−c]. Similar to what was done in Section D, let Xlbe the smallest integer such that one
ofx1,...,xNlies in the subsector Tl,Xl. LetYlbe the smallest integer such that ˆx1,..., ˆxNlies in the
subsectorTl,Yl. The difference between the distance between the origin oandxland the distance between
oandˆxlis at mostϵ. Hence,
RYl−ϵ≤RXl≤RYl+ϵ, (22)
whereRl,l∈[Nc]is the radius of Dlin the Poincaré disc. Similarly to (21), Rlcan be obtained by
rl= 2ssinh−1/parenleftigg/radicalbigg
Nc+ 1−l
Ncsinh/parenleftbiggR
2s/parenrightbigg/parenrightigg
,
Rl=s/parenleftigg
erl
s−1
erl
s+ 1/parenrightigg
. (23)
Fromϵ=O(1
Nc), (23), and (22), we have
Xl≥Yl−O(1). (24)
and
Rl−O(1
Nc) =Rl+O(1). (25)
SinceE[Xl] =O(1)according to (20), we have E[Yl] =O(1). Let ˆPand ˆQbe two points among ˆx1,..., ˆxN
that are in the subsector Tl−1,Yl−1andTl+1,Yl+1,respectively. Following the same argument as in the proof
of Lemma D.2, we can establish that the minimum Euclidean distance between the origin oand the geodesic
ˆPˆQis at leastRYl−1+Yl+1−O(1
N2−2c)≥RYl−1+Yl+1−O(1
Nc) =RYl−1+Yl+1+O(1). Then following the same
32Published in Transactions on Machine Learning Research (01/2024)
argument as in Section D, we can prove that f1in Lemma E.1 can be upper bounded by O(1
Nc), which
by Lemma E.1 implies that the expected number of extreme points on the convex hull of ˆx1,..., ˆxNis at
mostO(N1−c).
The proof of the upper bound for the case c>2
3is similar to that for the case1
2≤c≤2
3, where instead of
partitioning the Poincaré disc into N1−cequal-sized sectors and partitioning each sector into Ncsubsectors,
we equally partition the Poincaré disc into N1
3sectors and partition each sector into N2
3subsectors. The
remaining steps are similar.
F Poincaré Uniform Sampling
Our sampling method is based on the intuition that the probability of sampling a point from a region in
the Poincaré disc should be proportional to the hyperbolic area of the region. Based on this idea, we adapt
the sampling procedure from our proposed Poincaré uniform quantization method in Appendix C by taking
the limit of both the number of angular quantization bins ( NΘ) and the number of radial quantization bins
(NRH) to infinity. The sampling procedure is described in Alg. 3.
Algorithm 3 Poincaré Uniform Sampling
1:Input:number of points N, bound on the Poincaré radius R, constant curvature k.
2:Computes= 1/√
k,RH=sln/parenleftig
s+R
s−R/parenrightig
.
3:SampleNiid pointsη1,...,ηNfrom the uniform U(0,1]distribution.
4:SampleNiid pointsζ1,...,ζNfrom the uniform U(0,2π]distribution.
5:Computeτj= 2ssinh−1/parenleftbig√ηjsinh/parenleftbigRH
2s/parenrightbig/parenrightbig
∀j∈[N].
6:Computeαj=s(eτj
s−1)/(eτj
s+ 1)∀j∈[N].
7:Obtain sample xj∈B2
kby computing xj= (αjcosζj,αjsinζj)∀j∈[N].
8:Return point set{xj}j∈[N].
G Construction of BhSequences
We now describe how to construct a Bhsequence used for label switching resolution. We start with the
construction(withproofofcorrectness)in (Bose&Chowla,1960)thatpresentsasetofnumbers a′
1,...,a′
m+1
such that: (1)m+ 1 =pℓ, wherepis a prime; (2)a′
1,...,a′
m+1∈{0,1,..., (m+ 1)h−1};(3)for all
1≤i1≤...≤ih≤m+ 1, the sumsa′
i1+a′
i2+...+a′
ihare distinct.
Letα1= 0,α2,...,αmbe the elements of a Galois field Fm+1. Letβbe a primitive root of the extension
fieldF(m+1)h, which implies that β0,β1,...,β(m+1)h−1constitute all the nonzero elements in F(m+1)hand
thatβis not the root of any irreducible polynomial over the ground field of degree smaller than h.
Let
βa′
l=β+αl, l∈[m+ 1],a′
l≤(m+ 1)h−1.
We show next that the sums a′
l1+a′
l2+...+a′
lhare all distinct, for 1≤l1≤...≤lh≤m+ 1. Suppose on
the contrary that
a′
l1+a′
l2+...+a′
lh=a′
j1+a′
j2+...+a′
jh.
Then, by definition, we have
βa′
l1βa′
l2...βa′
lh=βa′
j1βa′
j2...βa′
jh,
which implies that
(β+αl1)(β+αl2)...(β+αlh) = (β+αj1)(β+αj2)...(β+αjh).
Thenβistherootofapolynomialwithdegreelessthan handcoefficientsfromthegroundfield, contradicting
the fact that βis a primitive element of F(m+1)h. Therefore, the sums of helements in a′
1,...,a′
hare all
distinct.
33Published in Transactions on Machine Learning Research (01/2024)
Finally, as described in Section 5.4, we can construct a Bhsequence set with melements by simply taking
the differences al=a′
l+1−a′
l.
H Extension to Multi-Label Classification
We now describe how our proposed framework extends to settings with J >2global ground truth labels. The
processofcomputingthequantizedconvexhullforeachlocalclusterremainssame. Forsecurecommunication
of quantized convex hulls, each client simply needs Junique numbers from the Bhsequence instead of just
2, while all other system components remain unaffected. For graph based grouping, the FL server obtains
the representative graph with JLnodes and partitions it into Jgroups of nodes using spectral clustering
(Shi & Malik, 2000; Ng et al., 2001). The Jglobal clusters are then assigned Jdifferent labels, and a J-class
hyperbolic SVM classifier is trained as follows. For J(>2)labels, one can use the common approach of
training aJ-class classifier by using Jindependently trained binary classifiers. In particular, each of the
Jbinary classifiers corresponds to one of the Jtraining labels, and is trained independently on the same
training set with the task of separating one chosen class from the remaining classes in the training data set.
Thereafter, for each of the binary classifiers, the resulting prediction scores are computed for every data point
and are transformed into probabilities using the Platt scaling technique (Platt et al., 1999). The predicted
label for a given data point is obtained by using a maximum a posteriori criteria involving probabilities of
theJclasses.
I Experimental Setup and Additional Results
I.1 Data Sets
Synthetic Data Sets. We consider R= 0.95and curvature constant k= 1for all synthetic data sets, and
chooseN=µ·100,000, forµ∈{0.2,0.4,0.6,0.8}.Each setting is tested via 10independent trials. We use
a90%/10%random split for each data set to obtain training and test points.
Biological data sets. We consider the following data sets:
•Olsson’s scRNA-seq data set (Olsson et al., 2016), containing single-cell (sc) RNA-seq expression
data. It comprises 319points from 8classes (cell types). We use the Poincaré embeddings provided
by (Chien et al., 2021) in their public code repository3. The curvature of the embedding is k= 1.
•UC-Stromal data set4from (Smillie et al., 2019), comprising cells from 68colon mucosa biopsies
from 18ulcerative colitis patients and 12healthy individuals. The cells were sequenced using 10X
Chromium (either v1 or v2) and filtered to remove low-quality cells. We used a total of 26,678
stromal and glia cells of dimension 1,307, corresponding to the number of (highly) variable genes.
Weobtaintheembeddingsusingmixed-curvaturevariationalauto-encoders(m-VAE) (Skopeketal.,
2020) with 1MLP layer and 300hidden dimensions. The curvature for the embeddings is set to
k= 0.0071. The labels 0−3used in our results in Section 6 correspond to labels Myofibroblasts,
Pericytes, Inflammatory Fibroblasts, and Glia, respectively.
•Lung-Human-ASK440 data set5from (Vieira Braga et al., 2019), comprising human lung cells
from asthma patients and healthy controls. A total of 3,314cells from the parenchymal lung tissue
were sequenced using Drop-seq protocol. The total dimension of the data points, corresponding to
the number of genes, is 3,377. Similar to UC-Stromal, we use m-VAE with 1MLP layer and 300
hidden dimensions to obtain the Poincaré embeddings. The curvature constant for the embeddings
is set tok= 0.0015. Furthermore, the labels 0−4correspond to labels Type-2, Type-1, T cell, NK
cell, and Mast cell, respectively.
3https://github.com/thupchnsky/PoincareLinearClassification/tree/main/embedding.
4https://singlecell.broadinstitute.org/single_cell/study/SCP551/scphere. Registration is required to access the content.
5https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE130148. Accessed with GEO: GSE130148.
34Published in Transactions on Machine Learning Research (01/2024)
For each biological data set, we consider a 85%/15%random split for the training and test points, and keep
it fixed for all trials. Our implementation for the m-VAE is provided at https://github.com/thupchnsky/
sc_mvae/tree/main .
Figure 8: Visualization of the Poincaré embeddings for the three considered biological data sets. (a) Olsson’s
single-cell RNA expression. (b) Lung-Human-ASK440. (c) UC-Stromal. In (b) and (c), the boundary circle
for the Poincaré disc has been omitted for better scaling for visualization purposes.
I.2 Simulation Details
Label Switching. As the FL server applies our proposed graph partitioning based approach to aggregate
the local convex hulls after recovering them, it does not need to know the ground truth labels or the exact
local labels at each client (i.e., the Bhlabels suffice to recover the individual quantized convex hulls without
knowingtheexactidentityoftheclientsfromwhichtheycame). Hence, wedidnotsimulatelabelswitchingin
our code at any client because it does not affect the grouping procedure in our algorithm. After grouping the
local convex hulls, the FL server assigns to each group a different label of its own choice. In our simulations,
the ground truth training label associated with the majority of the local convex hulls in a given group is
assigned as the label of the data points in that group before training the SVM classifiers on the recovered
convex hulls. This is done so as to enable the computation of the accuracy of the trained classifiers at the
FL server on the test data set in each setting.
Graph-Based Hull Aggregation. For binary classification, we use the kernighan_lin_bisection() module
from the NetworkX library (Hagberg et al., 2008) in Python to group the convex hulls. For multi-label
classification, we use the SpectralClustering() module from Scikit-learn (Pedregosa et al., 2011). Unlike
kernighan_lin_bisection() which produces balanced partitions of nodes for the case of binary classification,
SpectralClustering() based graph partitioning is not guaranteed to provide balanced partitioning for multi-
label scenarios. To improve the partioning performance, we observe that the local hulls that belong to the
same client should ideally be in separate groups. After Bhdecoding, the server can identify which of the
local hulls come from the same client, albeit without knowing the identity of the specific client. Hence, in
our simulations, we use a heuristic by which we assign very small weights to edges between each pair of
local convex hulls from the same client, forcing SpectralClustering() to assign them to separate groups. This
simple heuristic works very well in practice, as witnessed by our simulation results.
Synthetic Data Sets. For both Euclidean and Poincaré SVMs, we set the regularization hyperparameter
in (7) toλ= 20,000,which essentially forces the solver to solve the hard margin SVM problem (6). For
federated baselines, we consider L= 10, and partition the training data uniformly across clients.
Biological Data Sets. For both Euclidean and Poincaré SVMs, we consider a regularization term λ= 0.1.
For federated baselines, we set L= 3, and partition the training data uniformly across clients. The default
value for the quantization parameter (i.e., distance margin) is ϵ= 0.01. For obtaining the reference points,
we find that using the approach proposed in (Chien et al., 2021) (described in Section 3) may be unstable
in some settings. Therefore, instead of selecting just the minimum distance pair, we select three pairs of
points with lowest pairwise distances, and choose the one pair which produces the best training accuracy.
35Published in Transactions on Machine Learning Research (01/2024)
Table 2: Mean accuracy ( %) and 95%confidence interval of the baselines for different data sets. The results
are based on 10independent trials for each setting.
Data set Labels CP (%) CH-CP ( %) CE (%) CH-CE ( %)
Olsson 0-7 79.17±0.0083.33±0.0068.75±0.0077.08±0.00
Lung-Human [0, 2, 3, 4] 73.33±0.0074.07±0.0067.41±0.0068.89±0.00
I.3 Additional Results
Federated Baselines Outperforming Centralized Ones. For the rows in Table 1 where federated
baselines offer better accuracy than their centralized counterparts, we carry out further analysis to examine
the reasons behind the findings. For this, we consider additional centralized baselines where before training
the SVM classifier (both for the Euclidean and Poincaré methods), we find the minimal convex hull for
each class using Alg. 1. The SVM classifiers are then trained on the minimal convex hulls instead of the
entire classes. The results are presented in Table 2, where we denote the baselines with minimal convex
hulls by CH (Convex Hull). We can observe that training on minimal convex hulls indeed provides better
performance than training on original points sets in this case.
Table 3: Mean accuracy ( %) and 95%confidence interval of the baselines for different data sets. The results
are based on 10independent trials for each setting. The FL method with best mean accuracy is written in
boldcase letters.
Data set Labels CP (%) CE (%) FLP ( %) FLE ( %)
UC-Stromal0-3 73.20±0.0073.54±0.0068.61±3.36 65.98±3.63
[0, 1, 2] 74.65±0.0073.94±0.00 65.92±6.11 67.14±5.67
[0, 1, 3] 88.47±0.0087.39±0.0089.71±1.26 88.59±1.04
[0, 2, 3] 80.64±0.0082.29±0.0079.85±2.24 78.14±3.41
[1, 2, 3] 80.33±0.0080.33±0.0083.45±0.66 82.42±1.61
Figure 9: Impact of the quantization parameter ϵon accuracy and convex hull complexity for UC-Stromal
for the multi-label classification setting with labels 0−3) andL= 5. Here,CHdenotes the quantized
convex hull for an arbitrary class at an arbitrary client. The average as well as the max are calculated over
all clients across all local quantized convex hulls. The shaded areas represent the 95%confidence interval
from 10independent trials.
Results for L= 5.Due to requiring the consent of patients for their data to be used for biological analyses,
it is usually hard to compile large biological data sets. This is also the reason why the real-world data sets
considered in this paper are (relatively) small. Furthermore, the number of participating clients with private
biological data sets is expected to be small. Hence, in all our experiments from Section 6, we considered
L= 3. Here, we provide additional results for L= 5and the UC-Stromal data set, with ϵ= 0.01. The
accuracy results for various labels are presented in Table 3, while the results illustrating the impact of ϵon
the accuracy and the convex hull complexities for the setting with [0−3]are shown in Figure 9. We observe
36Published in Transactions on Machine Learning Research (01/2024)
similar trends as for L= 3, except that the relative convex hull size increases notably, while the max still
remains under 10%.
37