Published in Transactions on Machine Learning Research (08/2022)
Recurrent networks, hidden states and beliefs in partially
observable environments
Gaspard Lambrechts gaspard.lambrechts@uliege.be
Montefiore Institute, University of Liège
Adrien Bolland adrien.bolland@uliege.be
Montefiore Institute, University of Liège
Damien Ernst dernst@uliege.be
Montefiore Institute, University of Liège
LTCI, Telecom Paris, Institut Polytechnique de Paris
Reviewed on OpenReview: https: // openreview. net/ forum? id= dkHfV3wB2l
Abstract
Reinforcement learning aims to learn optimal policies from interaction with environments
whose dynamics are unknown. Many methods rely on the approximation of a value function
to derive near-optimal policies. In partially observable environments, these functions de-
pend on the complete sequence of observations and past actions, called the history. In this
work, we show empirically that recurrent neural networks trained to approximate such value
functions internally filter the posterior probability distribution of the current state given the
history, called the belief. More precisely, we show that, as a recurrent neural network learns
theQ-function, its hidden states become more and more correlated with the beliefs of state
variables that are relevant to optimal control. This correlation is measured through their
mutual information. In addition, we show that the expected return of an agent increases
with the ability of its recurrent architecture to reach a high mutual information between its
hidden states and the beliefs. Finally, we show that the mutual information between the
hidden states and the beliefs of variables that are irrelevant for optimal control decreases
through the learning process. In summary, this work shows that in its hidden states, a re-
current neural network approximating the Q-function of a partially observable environment
reproduces a sufficient statistic from the history that is correlated to the relevant part of
the belief for taking optimal actions.
1 Introduction
Latest advances in reinforcement learning (RL) rely heavily on the ability to approximate a value function
(i.e., state or state-action value function). Modern RL algorithms have been shown to be able to produce
approximations of the value functions of Markov decision processes (MDPs) from which high-quality policies
canbederived, eveninthecaseofcontinuousandhigh-dimensionalstateandactionspaces(Mnihetal.,2015;
Lillicrap et al., 2015; Mnih et al., 2016; Haarnoja et al., 2018; Hessel et al., 2018). The adaptation of these
techniques to partially observable MDPs (POMDPs) is not straightforward. Indeed, in such environments,
the agent only receives partial observations of the underlying states of the environment. Unlike MDPs where
the value functions are written as functions of the current state, in POMDPs the value functions are written
as functions of the complete sequence of observations and past actions, called the history. Moreover, the
value functions of a history can equivalently be written as functions of the posterior probability distribution
over the current state given this history (Bertsekas, 2012). This posterior probability distribution is called
the belief and is said to be a sufficient statistic from the history for the value functions of the POMDP.
However, the computation of the belief requires one to know the POMDP model and is generally intractable
1Published in Transactions on Machine Learning Research (08/2022)
with large or continuous state spaces. For these two reasons, practical RL algorithms rely on the definition of
the value functions as functions of the complete history (i.e., history or history-action value function), while
the definition of the value functions as functions of the belief (i.e., belief or belief-action value function) is
more of theoretical interest.
Approximating the value functions as functions of the histories requires one to use function approximators
that are able to process sequences of arbitrary length. In practice, RNNs are good candidates for such
approximators (Bakker, 2001; Hausknecht & Stone, 2015; Heess et al., 2015). RNNs are parametric approx-
imators that process sequences, time step by time step, exhibiting memory through a hidden state that is
passed recurrently over time. The RNN is thus tasked with outputting the value directly from the history.
We focus on the approximation of the history-action value function, or Q-function, in POMDPs using a para-
metric recurrent Q-learning (PRQL) algorithm. More precisely, RNNs are trained with the deep recurrent
Q-network (DRQN) algorithm (Hausknecht & Stone, 2015; Zhu et al., 2017).
Since we know that the belief is a sufficient statistic from the history for the Q-function of this history
(Bertsekas, 2012), we investigate if RNNs, once trained, reproduce the belief filter when processing a history.
This investigation is conducted in this work by studying the performance of the different agents with regard
to the mutual information (MI) between their hidden states and the belief. We focus on POMDPs for which
the models are known. The benchmark problems chosen are the T-Maze environments (Bakker, 2001) and
the Mountain Hike environments (Igl et al., 2018). The first ones present a discrete state space, allowing
one to compute the belief using Bayes’ rule, and representing this distribution over the states in a vector
whose dimension is equal to the number of distinct states. The second ones present a continuous state space,
making the belief update intractable. We thus rely on particle filtering in order to approximate the belief
by a set of states, called particles, distributed according to the belief distribution. The MI between the
hidden states and the beliefs is periodically estimated during training, using the mutual information neural
estimator (MINE) algorithm (Belghazi et al., 2018). The MINE estimator is extended with the Deep Set
architecture (Zaheer et al., 2017) in order to process sets of particles in the case of POMDPs with continuous
state-spaces. This methodology allows one to measure the ability and tendency of recurrent architecture to
reproduce the belief filter when trained to approximate the Q-function.
In (Mikulik et al., 2020), a similar study is performed in the meta-learning setting. In this setting, an MDP
is drawn from a distribution of MDPs at each episode. This problem can be equivalently modeled as a
particular subclass of POMDP. The authors show empirically, among others, that the hidden state of an
RNN-based policy and the statistic of the optimal policy can be mapped one into the other with a low
dissimilarity measure. In contrast, we consider arbitrary POMDPs and show empirically that information
about the belief, a statistic known to be sufficient for the optimal control, is encoded in the hidden states.
In Section 2, we formalise the problem of optimal control in POMDPs, we present the PRQL algorithms
for deriving near-optimal policies and we explain the MINE algorithm for estimating the MI. In Section 3,
the beliefs and hidden states are defined as random variables whose MI is measured. Afterwards, Section 4
displays the main results obtained for the previously mentioned POMDPs. Finally, Section 5 concludes and
proposes several future works and algorithms motivated by our results.
2 Background
In Subsection 2.1, POMDPs are introduced, along with the belief, policy, and Q-functions associated with
such decision processes. Afterwards, in Subsection 2.2, we introduce the DRQN algorithm that is used in
our experiments. This algorithm is a particular instance of the PRQL class of algorithms that allows to
approximate the Q-function for deriving a near-optimal policy in a POMDP. Finally, in Subsection 2.3, we
present the MINE algorithm that is used for estimating the MI between the hidden states and beliefs in our
experiments.
2.1 Partially observable Markov decision processes
In this work, the environments are modelled as POMDPs. Formally, a POMDP Pis an 8-tupleP=
(S,A,O,p0,T,R,O,γ )whereSis the state space, Ais the action space, and Ois the observation space.
2Published in Transactions on Machine Learning Research (08/2022)
The initial state distribution p0gives the probability p0(s0)ofs0∈Sbeing the initial state of the decision
process. Thedynamicsaredescribedbythetransitiondistribution Tthatgivestheprobability T(st+1|st,at)
ofst+1∈Sbeing the state resulting from action at∈Ain state st∈S. The reward function Rgives the
immediate reward rt=R(st,at,st+1)obtained after each transition. The observation distribution Ogives
the probability O(ot|st)to get observation ot∈Oin state st∈S. Finally, the discount factor γ∈[0,1[
gives the relative importance of future rewards.
Taking a sequence of tactions ( a0:t−1) in the POMDP conditions its execution and provides a sequence of
t+ 1observations ( o0:t). Together, they compose the history η0:t= (o0:t,a0:t−1)∈H 0:tuntil time step
t, whereH0:tis the set of such histories. Let η∈Hdenote a history of arbitrary length sampled in the
POMDP, and let H=/uniontext∞
t=0H0:tdenote the set of histories of arbitrary length.
A policyπ∈Πin a POMDP is a mapping from histories to actions, where Π =H→Ais the set of such
mappings. A policy π∗∈Πis said to be optimal when it maximises the expected discounted sum of future
rewards starting from any history η0:t∈H 0:tat timet∈N0
π∗∈arg max
π∈ΠE
π,P/bracketleftigg∞/summationdisplay
t′=tγt′−trt′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleη0:t/bracketrightigg
,∀η0:t∈H 0:t,∀t∈N0. (1)
The history-action value function, or Q-function, is defined as the maximal expected discounted reward that
can be gathered, starting from a history η0:t∈H 0:tat timet∈N0and an action at∈A
Q(η0:t,at) = max
π∈ΠE
π,P/bracketleftigg∞/summationdisplay
t′=tγt′−trt′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleη0:t,at/bracketrightigg
,∀η0:t∈H 0:t,∀at∈A,∀t∈N0. (2)
TheQ-function is also the unique solution of the Bellman equation (Smallwood & Sondik, 1973; Kaelbling
et al., 1998; Porta et al., 2004)
Q(η,a) =E
P/bracketleftbigg
r+γmax
a′∈AQ(η′,a′)/vextendsingle/vextendsingle/vextendsingle/vextendsingleη,a/bracketrightbigg
,∀η∈H,∀a∈A (3)
whereη′=η∪(a,o′)andris the immediate reward obtained when taking action ain historyη. From
equation (1) and equation (2), it can be observed that any optimal policy satisfies
π∗(η)∈arg max
a∈AQ(η,a),∀η∈H. (4)
LetP(S)be the set of probability measures over the state space S. The belief b∈P(S)of a history η∈H
is defined as the posterior probability distribution over the states given the history, such that b(s) =p(s|
η),∀s∈S(Thrun, 2002). The belief filter f∗is defined as the function that maps a history ηto its
corresponding belief b
f∗(η) =b,∀η∈H. (5)
Formally, for an initial observation η= (o), the belief b=f∗(η)is defined by
b(s) =p0(s)O(o|s)/integraltext
Sp0(s′)O(o|s′) ds′,∀s∈S (6)
and for a history η′=η∪(a,o′), the belief b′=f∗(η′)is recursively defined by
b′(s′) =O(o′|s′)/integraltext
ST(s′|s,a)b(s) ds/integraltext
SO(o′|s′)/integraltext
ST(s′|s,a)b(s) dsds′,∀s′∈S. (7)
whereb=f∗(η). Equation (7) provides a way to update the belief btob′through a filter step fonce
observing new information (a,o′)
b′=f(b;a,o′). (8)
3Published in Transactions on Machine Learning Research (08/2022)
A statistic from the history is defined as any function of the history. The belief is known to be a sufficient
statistic from the history in order to act optimally (Bertsekas, 2012). It means that the Q-function only
depends on the history through the belief computed from this same history. It implies in particular that the
Q-function takes the following form
Q(η,a) =Q(f∗(η),a),∀η∈H,∀a∈A (9)
whereQ:P(S)×A→ Ris called the belief-action value function, or Q-function. This function gives the
maximal expected discounted reward starting from a belief b∈P(S)and an action a∈A, where the belief
b=f∗(η)results from an arbitrary history η∈H. Although the exact belief filter is often unknown or
intractable, this factorisation of the Q-function still motivates the compression of the history in a statistic
related to the belief, when processing the history for predicting the Q-function.
2.2 Parametric recurrent Q-learning
We call PRQL the family of algorithms that aim at learning an approximation of the Q-function with a
recurrent architecture Qθ, whereθ∈Rdθis the parameter vector. These algorithms are motivated by
equation (4) that shows that an optimal policy can be derived from the Q-function. The strategy consists
of minimising, with respect to θ, for all (η,a), the distance between the estimation Qθ(η,a)of the LHS of
equation (3), and the estimation of the expectation EP[r+γmax a′∈AQθ(η′,a′)]of the RHS of equation
(3). This is done by using transitions (η,a,r,o′,η′)sampled in the POMDP, with η′=η∪(a,o′). In its
simplest form, given such a transition, the PRQL algorithm updates the parameters θ∈Rdθof the function
approximator according to
θ←θ+α/parenleftbigg
r+γmax
a′∈A{Qθ(η′,a′)}−Qθ(η,a)/parenrightbigg
∇θQθ(η,a). (10)
This update corresponds to a gradient step in the direction that minimises, with respect to θthe squared
distance between Qθ(η,a)and the target r+γmax a′∈A{Qθ(η′,a′)}considered independent of θ. It can be
noted that, in practice, such algorithms introduce a truncation horizon Hsuch that the histories generated
in the POMDP have a maximum length of H. From the approximation Qθ, the policy πθis given by
πθ(η) = arg maxa∈AQθ(η,a). Equation (4) guarantees the optimality of this policy if Qθ=Q. Even though
it will alter the performance of the algorithm, any policy can be used to sample the transitions (η,a,r,o′,η′).
The function approximator Qθof PRQL algorithms should be able to process inputs η∈Hof arbitrary
length, making RNN approximators a suitable choice. Indeed, RNNs process the inputs sequentially, ex-
hibiting memory through hidden states that are outputted after each time step, and processed at the next
time step along with the following input. More formally, let x0:t= [x0,...,xt]witht∈N0be an input
sequence. At any step k∈{0,...,t}, RNNs maintain an internal memory state hkthrough the update
function (11) and output a value ykthrough the output function (12). The initial state h−1is given by the
initialization function (13).
hk=uθ(hk−1,xk),∀k∈N0, (11)
yk=oθ(hk),∀k∈N0, (12)
h−1=iθ. (13)
These networks are trained based on backpropagation through time where gradients are computed in a
backward pass through the complete sequence via the hidden states (Werbos, 1990). The following recurrent
architectures are used in the experiments: the long short-term memory (LSTM) by Hochreiter & Schmid-
huber (1997), the gated recurrent unit (GRU) by Chung et al. (2014), the bistable recurrent cell (BRC) and
recurrently neuromodulated bistable recurrent cell (nBRC) by Vecoven et al. (2021), and the minimal gated
unit (MGU) by Zhou et al. (2016).
In the experiments, we use the DRQN algorithm (Hausknecht & Stone, 2015; Zhu et al., 2017) to learn
policies. This algorithm is a PRQL algorithm that shows good convergence even for high-dimensional
problems. The DRQN algorithm is detailed in Algorithm 1 of Appendix B. In this algorithm, for a given
4Published in Transactions on Machine Learning Research (08/2022)
historyη0:tof arbitrary length t, the inputs of the RNN are xk= (ak−1,ok), k= 1,...,tandx0= (0,o0),
and the output of the RNN at the last time step yt=oθ(ht)∈R|A|gives yat
t=Qθ(η0:t,at), for any at∈A.
We also define the composition u∗
θ:H→Rdθof equation (13) and equation (11) applied on the complete
history, such that
ht=u∗
θ(η0:t) =/braceleftigg
uθ(u∗
θ(η0:t−1),xt), t≥1
uθ(iθ,xt), t = 0(14)
2.3 Mutual information neural estimator
In this work, we are interested in establishing if a recurrent function approximator reproduces the belief filter
during PRQL. Formally, this is performed by estimating the MI between the beliefs and the hidden states
of the RNN approximator Qθ. In this subsection, we recall the concept of MI and how it can be estimated
in practice.
The MI is theoretically able to measure any kind of dependency between random variables (Kraskov et al.,
2004). The MI between two jointly continuous random variables XandYis defined as
I(X;Y) =/integraldisplay
X/integraldisplay
Yp(x,y) logp(x,y)
pX(x)pY(y)dxdy (15)
whereXandYare the support of the random variables XandYrespectively, pis the joint probability
density function of XandY, andpXandpYare the marginal probability density functions of XandY,
respectively. It is worth noting that the MI can be defined in terms of the Kulback-Leibler (KL) divergence
between the joint pand the product of the marginals q=pX⊗pY, over the joint space Z=X×Y
I(X;Y) =DKL(p||q) =/integraldisplay
Zp(z) log/parenleftbiggp(z)
q(z)/parenrightbigg
dz (16)
In order to estimate the MI between random variables XandYfrom a dataset{(xi,yi)}N
i=1, we rely on the
MINE algorithm (Belghazi et al., 2018). This technique is a parametric approach where a neural network
outputs a lower bound on the MI, that is maximised by gradient ascent. The lower bound is derived from
the Donsker-Varhadan representation of the KL-divergence (Donsker & Varadhan, 1975)
DKL(p||q) = sup
T:Z→REz∼p[T(z)]−log/parenleftig
Ez∼q/bracketleftig
eT(z)/bracketrightig/parenrightig
(17)
where the supremum is taken over all functions Tsuch that the two expectations are finite. The lower bound
IΦ(X;Y)on the true MI I(X;Y)is obtained by replacing Tby a parameterised function Tϕ:Z→Rwith
ϕ∈Φ, and taking the supremum over the parameter space Φof this function. If Φcorresponds to the
parameter space of a neural network, then this lower bound can be approached by gradient ascent using
empirical means as estimators of the expectations. The resulting procedure for estimating the MI is given
in Algorithm 3 in Appendix D.
3 Measuring the correlation between the hidden states and beliefs
In this work, we study if PRQL implicitly approximates the belief filter by reaching a high MI between the
RNN’s hidden states and the beliefs, that are both generated from random histories. In this section, we
first explain the intuition behind this hypothesis, then we define the joint probability distribution over the
hidden states and beliefs that defines the MI.
As explained in Section 2, the belief filter is generally intractable. As a consequence, PRQL algorithms
use approximators Qθthat directly take the histories as input. In the DRQN algorithm, these histories
are processed recurrently according to equation (11), producing a new hidden state htafter each input
xt= (at−1,ot)
ht=uθ(ht−1; (at−1,ot)). (18)
5Published in Transactions on Machine Learning Research (08/2022)
These hidden states should thus summarise all relevant information from past inputs in order to predict the
Q-function at all later time steps. The belief is known to be a sufficient statistic from the history for these
predictions (9). Moreover, the belief btis also updated recurrently, according to equation (8) after each
transition (at−1,ot)
bt=f(bt−1;at−1,ot). (19)
The parallel between equation (18) and equation (19), knowing the sufficiency of the belief (9), justifies the
appropriateness of the belief filter fas the update function uθof the RNN approximator Qθ. It motivates
the study of the reconstruction of the belief filter by the RNN.
In practice, this is done through the measurement of the MI between the hidden state htand the belief btat
any time step t∈N0. Formally, for a given history length t∈N0, the policy πθof the learning algorithm, as
defined in Subsection 2.2, induces a distribution pπθ(η|t)over histories η∈H. This conditional probability
distribution is zero for all history of length t′̸=t. Given a distribution p(t)over the length of trajectories,
the joint distribution of handbis given by
p(h,b) =∞/summationdisplay
t=0p(t)/integraldisplay
Hp(h,b|η)pπθ(η|t) dη (20)
wherep(h,b|η)is a Dirac distribution for h=u∗
θ(η)andb=f∗(η)given by equation (14) and equation
(5), respectively. In the following, we estimate the MI between handbunder their joint distribution (20).
4 Experiments
In this section, the experimental protocol and environments are described and the results are given. More
specifically, in Subsection 4.1, we describe the estimates that are reported in the figures. The results are
reportedforfourdifferentPOMDPs: theT-MazeandStochasticT-MazeinSubsection4.2, andtheMountain
Hike and Varying Mountain Hike in Subsection 4.3. Afterwards, in Subsection 4.4, irrelevant state variables
and observations are added to the decision processes, and the MI is measured separately between the hidden
states and the belief of the relevant and irrelevant variables. Finally, in Subsection 4.5, we discuss the results
obtained in this section, and propose an additional protocol to study their generalisation.
4.1 Experimental protocol
As explained in Subsection 2.2, the parameters θof the approximation Qθare optimised with the DRQN
algorithm. After eepisodes of interaction with the POMDP, the DRQN algorithm gives the policy
πθe(η) = arg maxa∈AQθe(η,a). In the experiments, the empirical cumulative reward ˆJ(θe)of the policy
πθeis reported, along with the estimated MI ˆI(θe)between the random variables handbunder the dis-
tribution (20) implied by πθe. Each estimate is reported averaged over four training sessions. In addition,
confidence intervals show the minimum and maximum of these estimates.
The empirical return is defined as ˆJ(θe) =1
I/summationtextI−1
i=0/summationtextH−1
t=0γtri
t, whereIis the number of Monte Carlo
rollouts,Hthe truncation horizon of the DRQN algorithm, and ri
tis the reward obtained at time step tof
Monte Carlo rollout i. As far as the estimation of the MI is concerned, we sample time steps with equal
probability p(t) = 1/H, t∈{0,...,H−1}, whereHis the truncation horizon of the DRQN algorithm.
The uniform distribution over time steps and the current policy πθedefine the probability distribution (20)
over the hidden states and beliefs. The MI is estimated from samples of this distribution using the MINE
estimator ˆI(θe)(see Subsection D.1 for details). The hyperparameters of the DRQN and MINE algorithms
are given in Appendix E.
For POMDPs with continuous state spaces, the computation of the belief bis intractable. However, a set
of state particles Sthat follows the belief distribution f∗(η)can be sampled, using particle filtering (see
Appendix C). This set of particles could be used to construct an approximation of the belief in order to
estimate the MI. This density estimation procedure is nonetheless unnecessary as the MINE network can
directly process the set of particles by producing a permutation-invariant embedding of the belief using the
Deep Set architecture (Zaheer et al., 2017), see Subsection D.2 for details.
6Published in Transactions on Machine Learning Research (08/2022)
4.2 Deterministic and Stochastic T-Mazes
Up
Down
(0,0) (1,0) (2,0) (3,0) (4,0)... (L,0)(L,1)
(L,−1)(0,0) (1,0) (2,0) (3,0) (4,0)... (L,0)(L,1)
(L,−1)
Figure 1: T-Maze state space.The T-Maze is a POMDP where the agent is tasked with finding
the treasure in a T-shaped maze (see Figure 1). The state is given
by the position of the agent in the maze and the maze layout that
indicates whether the treasure lies up or down after the crossroads.
The initial state determines the maze layout, and it never changes
afterwards. The initial observation made by the agent indicates the
layout. Navigating in the maze provides zero reward, except when
bouncing onto a wall, in which case a reward of −0.1is received.
Finding the treasure provides a reward of 4. Beyond the crossroads,
the states are always terminal. The optimal policy thus consists of
goingthroughthemaze, whilerememberingtheinitialobservationin
order to take the correct direction at the crossroads. This POMDP
is parameterised by the corridor length L∈Nand stochasticity rate
λ∈[0,1]that gives the probability of moving in a random direction
at any time step. The Deterministic T-Maze ( λ= 0) was originally
proposed in (Bakker, 2001). The discount factor is γ= 0.98. This
POMDP is formally defined in Subsection A.2.
As explained in Subsection 2.2, the histories can be sampled with an arbitrary policy in PRQL algorithms.
In practice, the DRQN algorithm uses an ε-greedy stochastic policy that selects its action according to
the current policy with probability 1−ε, and according to the exploration policy E(A)with probability ε.
Usually, the exploration policy is chosen to be the uniform distribution U(A)over the action. However, for
the T-Maze, the exploration policy E(A)is tailored to this POMDP to alleviate the exploration problem,
that is independent of the study of this work. The exploration policy forces one to walk through the right
of the corridor with E(Right ) = 1/2andE(Other ) = 1/6where Other∈{Up,Left,Down}.
-1011.46Return ˆJ(θe)
0 1000 2000 3000 4000 5000
Episodee2.55.0MIˆI(θe) [bit]
1 2 3 4 5 6 7
MIˆI(θe) [bit]-101Return ˆJ(θe)
LSTM
GRU
BRC
NBRC
MGU
Figure 2: Deterministic T-Maze ( L= 50). Evolution of the return ˆJ(θe)and the MI ˆI(θe)aftereepisodes
(left), and the return ˆJ(θe)with respect to the MI ˆI(θe)(right). The maximal expected return is given by
the dotted line.
On the left in Figure 2, the expected return is shown along with the MI between the hidden states and the
belief as a function of the number of episodes, for a T-Maze of length L= 50. In order to better disambiguate
between high-quality policies, the empirical return is displayed with an exponential scale in the following
graphs. Both the performance of the policy and the MI increase during training. We also observe that,
at any given episode, RNNs that have a higher return, such as the nBRC or the BRC, correspond to cells
that have a higher MI between their hidden states and the belief. Furthermore, the LSTM that struggles to
achieve a high return has a significantly lower MI than the other cells. Finally, we can see that the evolution
of the MI and the return are correlated, which is highlighted on the right in Figure 2. Indeed, the return
increases with the MI, with a linear correlation coefficient of 0.8233and a rank correlation coefficient of
7Published in Transactions on Machine Learning Research (08/2022)
0.6419. These correlations coefficients are also detailed for each cell separately in Appendix G. It can also
be noted that no RNN with less than 5bits of MI reaches the maximal return.
-2-100.53Return ˆJ(θe)
0 2000 4000 6000 8000 10000
Episodee0.02.55.0MIˆI(θe) [bit]
1 2 3 4 5 6 7
MIˆI(θe) [bit]-2-10Return ˆJ(θe)
LSTM
GRU
BRC
NBRC
MGU
Figure 3: Deterministic T-Maze ( L= 100). Evolution of the return ˆJ(θe)and the MI ˆI(θe)aftereepisodes
(left), and the return ˆJ(θe)with respect to the MI ˆI(θe)(right). The maximal expected return is given by
the dotted line.
In Figure 3, we can see that all previous observations also hold for a T-Maze of length L= 100. On the
left, we can see that the lower the MI, the lower the return of the policy. For this length, in addition to the
LSTM, the GRU struggles to achieve the maximal return, which is reflected in the evolution of its MI that
increases more slowly than for the other RNNs. It is also interesting to notice that, on average, the MGU
overtake the BRC in term of return after 2000episodes, which is also the case for the MI. Here, the linear
correlation coefficient between the MI and the return is 0.5347and the rank correlation coefficient is 0.6666.
Once again, we observe that a minimum amount of MI between the hidden states and the belief is required
for the policy to be optimal. Here, at least 5.0bits of MI is necessary.
-2-100.28Return ˆJ(θe)
0 1000 2000 3000 4000 5000
Episodee246MIˆI(θe) [bit]
2 3 4 5 6
MIˆI(θe) [bit]-2-10Return ˆJ(θe)
LSTM
GRU
BRC
NBRC
MGU
Figure 4: Stochastic T-Maze ( L= 50,λ= 0.3). Evolution of the return ˆJ(θe)and the MI ˆI(θe)aftere
episodes (left), and the return ˆJ(θe)with respect to the MI ˆI(θe)(right). The maximal expected return is
given by the dotted line.
In Figure 4, the results are shown for the Stochastic T-Maze with L= 50andλ= 0.3. On the contrary
to the Deterministic T-Maze, where the belief is a Dirac distribution over the states, there is uncertainty
on the true state in this environment. We can nevertheless observe that previous observations hold for
this environment too. The MI and the expected return are indeed both increasing throughout the training
process, and the best performing RNNs, such as the BRC and nBRC, have a MI that increases faster and
stayshigher, whiletheLSTMstrugglestoreachbothahighreturnandahighMI.Here, thelinearcorrelation
8Published in Transactions on Machine Learning Research (08/2022)
coefficient between the MI and the return is 0.5460and the rank correlation coefficient is 0.6403. It can also
be noticed on the right that the best performing policies have a MI of at least 4.5bits in practice.
In the Deterministic T-Maze, it can be observed that the estimated lower bounds Iϕ(h,b)on the MI that
are obtained by the MINE estimator are tight. Indeed, in this environment, the hidden state and belief
are discrete random variables and their mutual information is thus upper bounded by the entropy of the
belief. Moreover, the belief is a Dirac distribution that gives the actual state with probability one. Under
the optimal policy, each state is visited with equal probability, such that the entropy of the belief is given by
log2(102) = 6.6724for the Deterministic T-Maze of length L= 50, where 102is the number of non terminal
states. As can be seen in Figure 2, the optimal policies reach an estimated MI around 6.5at maximum,
which nearly equals the upper bound. The same results is obtained for the Deterministic T-Maze of length
L= 100, where the entropy of the belief is given by log2(202) = 7.658and the optimal policies reach an
estimated MI around 7.0at maximum, as can be seen in Figure 3. We expect this result to generalise to
other environments even if this would be difficult to verify in practice for random variables with large or
continuous spaces.
4.3 Mountain Hike and Varying Mountain Hike
−1.0−0.5 0.0 0.5 1.0
x1−1.00−0.75−0.50−0.250.000.250.500.751.00x2
−1.2−1.2
−1.0−1.0−1.0
−0.8−0.8−0.8−0.8−0.8
−0.6−0.6
−0.6−0.6
−0.4−0.4
−0.4−0.2
−0.2
Figure 5: Mountain hike altitude function.The Mountain Hike environment is a POMDP modelling an
agent walking through a mountainous terrain. The agent has
a position on a two-dimensional map and can take actions to
move in four directions relative to its initial orientation: For-
ward, Backward, Right and Left. First, we consider that its
initial orientation is always North. Taking an action results in
a noisy translation in the corresponding direction. The transla-
tion noise is Gaussian with a standard deviation of σT= 0.05.
The only observation available is a noisy measure of its rel-
ative altitude to the mountain top, that is always negative.
The observation noise is Gaussian with a standard deviation
ofσO= 0.1. The reward is also given by this relative altitude,
such that the goal of this POMDP is to to obtain the highest
possible cumulative altitude. Around the mountain top, the
states are terminal. The optimal policy thus consists of going as fast as possible towards those terminal
states while staying on the crests in order to get less negative rewards than in the valleys. This environ-
ment is represented in Figure 5. This POMDP is inspired by the Mountain Hike environment described
in (Igl et al., 2018). The discount factor is γ= 0.99. We also consider the Varying Mountain Hike in
the experiments, a more difficult version of the Mountain Hike where the agent randomly faces one of the
four cardinal directions (i.e., North, West, South, East) depending on the initial state. The agent does not
observe its orientation. As a consequence, the agent needs to maintain a belief about its orientation given
the observations in order to act optimally. This POMDP is formally defined in Subsection A.3.
Figure6showsonthelefttheexpectedreturnandtheMIduringtrainingfortheMountainHikeenvironment.
It is clear that the DRQN algorithm promotes a high MI between the belief and the hidden states of the
RNN, even in continuous-state environments. It can also be seen that the evolution of the MI and the
evolution of the return are strongly linked throughout the training process, for all RNNs. We can also see
on the right in Figure 6 that the correlation between MI and performances appears clearly for each RNN.
For all RNNs, the linear correlation coefficient is 0.5948and the rank correlation coefficient is 0.2965. In
particular, we see that the best policies, with a return around −20, are clearly separated from the others
and have a significantly higher MI on average.
In Figure 7, we can see the evolution and the correlation between the return and the MI for the Varying
Mountain Hike environment. The correlation is even clearer than for the other environments. This may be
due to the fact that differences in term of performances are more pronounced than for the other experiments.
Again, the worse RNNs such as the LSTM and the BRC have a significantly lower MI compared to the other
cells. In addition, the performances of any RNN is strongly correlated to their ability to reproduce the belief
filter, as can be seen on the right, with a sharp increase in empirical return as the MI increases from 2.5to
9Published in Transactions on Machine Learning Research (08/2022)
−60−40−20Return ˆJ(θe)
0 2000 4000 6000 8000 10000
Episodee024MIˆI(θe) [bit]
0 1 2 3 4 5
MIˆI(θe) [bit]−60−50−40−30−20−10Return ˆJ(θe)
LSTM
GRU
BRC
NBRC
MGU
Figure 6: Mountain Hike. Evolution of the return ˆJ(θe)and the MI ˆI(θe)aftereepisodes (left), and the
return ˆJ(θe)with respect to the MI ˆI(θe)(right).
−80−60−40Return ˆJ(θe)
0 10000 20000 30000 40000
Episodee024MIˆI(θe) [bit]
0 1 2 3 4 5
MIˆI(θe) [bit]−80−70−60−50−40−30Return ˆJ(θe)
LSTM
GRU
BRC
NBRC
MGU
Figure 7: Varying Mountain Hike. Evolution of the return ˆJ(θe)and the MI ˆI(θe)aftereepisodes (left),
and the return ˆJ(θe)with respect to the MI ˆI(θe)(right).
4.5bits. More precisely, the linear correlation coefficient between the MI and the return is 0.5982and the
rank correlation coefficient is 0.6176. This increase occurs throughout the training process, as can be seen
on the left.
4.4 Belief of variables irrelevant for the optimal control
Despite the belief being a sufficient statistic from the history in order to act optimally, it may be that only the
belief of some state variables is necessary for optimal control. In this subsection, we show that approximating
theQ-function with an RNN will only tend to reconstruct the necessary part, naturally filtering away the
belief of irrelevant state variables.
In order to study this phenomenon, we construct a new POMDP P′from a POMDP Pby adding new
state variables, independent of the original ones, and irrelevant for optimal control. More precisely, we
adddirrelevant state variables sI
tthat follows a Gaussian random walk. In addition, the agent acting in
the POMDP P′obtains partial observations oI
tof the new state variables through an unbiased Gaussian
observation model. Formally, the new states and observations are distributed according to
p(sI
0) =ϕ(sI
0;0,1) (21)
p(sI
t+1|sI
t) =ϕ(sI
t+1;sI
t,1),∀t∈N0, (22)
10Published in Transactions on Machine Learning Research (08/2022)
p(oI
t|sI
t) =ϕ(oI
t;sI
t,1),∀t∈N0, (23)
whereϕ(x;µ,Σ)is the probability density function of a multivariate random variable of mean µ∈Rdand
covariance matrix Σ∈Rd×d, evaluated at x∈Rd, and 1is the identity matrix.
-1011.46Return ˆJ(θe)
0 1000 2000 3000 4000 5000
Episodee2.55.0MIˆI(θe) [bit]
Belief for sI
Belief for s
(a)d= 1
-1011.46Return ˆJ(θe)
0 1000 2000 3000 4000 5000
Episodee510MIˆI(θe) [bit] Belief for sI
Belief for s (b)d= 4
Figure 8: Deterministic T-Maze ( L= 50) withdirrelevant state variables. Evolution of the return ˆJ(θe)
and the MI ˆI(θe)for the belief of the irrelevant and relevant state variables after eepisodes, for the GRU
cell. The maximal expected return is given by the dotted line.
Figure 8 shows the return and the MI measured for the GRU on the T-Maze environment with L= 50.
It can be observed, as for the classic T-Maze environment, that the MI between the hidden states and the
belief of state variables that are relevant to optimal control increases with the return. In addition, the MI
with the belief of irrelevant variables decreases during training. It can also be seen that, for d= 4, the MI
with the belief of irrelevant variables remains higher than the MI with the belief of relevant variables, due to
the high entropy of this irrelevant process. Finally, it is interesting to note that the MI continues to increase
(resp. decrease) with the belief of relevant (resp. irrelevant) variables long after the optimal policy is reached,
suggesting that the hidden states of the RNN still change substantially. Similar results are obtained for the
other cells (see Appendix H).
−40−20Return ˆJ(θe)
0 2000 4000 6000 8000 10000
Episodee246MIˆI(θe) [bit]
Belief for sI
Belief for s
(a)d= 1
−40−20Return ˆJ(θe)
0 2000 4000 6000 8000 10000
Episodee510MIˆI(θe) [bit]Belief for sI
Belief for s (b)d= 4
Figure 9: Mountain Hike with with dirrelevant state variables. Evolution of the return ˆJ(θe)and the MI
ˆI(θe)for the belief of the irrelevant and relevant state variables after eepisodes, for the GRU cell.
Figure 9 shows the return and the MI measured for the GRU on the Mountain Hike environment. The
same conclusions as for the T-Maze can be drawn, with a clear increase of the MI for the relevant variables
11Published in Transactions on Machine Learning Research (08/2022)
throughout the training process, and a clear decrease of the MI for the irrelevant variables. In addition, it
can be seen that the optimal policy is reached later when there are more irrelevant variables. It is also clear
that adding more irrelevant variables increases the entropy of the irrelevant process, which leads to a higher
MI between the hidden states and the irrelevant state variables. Similar results are obtained for the other
cells (see Appendix H).
4.5 Discussion
As shown in the experiments, under the distribution induced by a recurrent policy trained using recurrent
Q-learning, its hidden state provide a high amount of information about the belief of relevant state variables,
at any time step. The hidden state of the RNN is thus a statistic from the history that encodes information
about the belief. In addition, at any time step, the network performs an update of this statistic, based on
the actions and observations that are observed. The RNN thus implements a filter that provides a statistic
encoding the belief.
However, it was only shown that the RNN produces such a statistic under the distribution of histories
induced by the learned policy. For the sake of robustness of the policy to perturbations of histories, we
might want this statistic to also provide information about the belief under other distribution of histories. In
Appendix F, we propose an experimental protocol to study the generalisation of the learned statistics. The
results show that the MI between the hidden states and the beliefs also increases throughout the training
process, under distributions induced by various ε-greedy policies, even the fully random policy. We impute
those results to the following reasons. First, the DRQN algorithm approximates the Q-function, which
generally requires a richer statistic from the history than the optimal policy. Second, the DRQN algorithm
makes use of exploration, which allows the RNN to learn from histories that are diverse. However, we still
observe that the higher the noise, the lower the MI. From these results, we conclude that the statistic that
is learned by the network generalises reasonably well to other distributions of histories.
5 Conclusions
In this work, we have shown empirically for several POMDPs that RNNs approximating the Q-function with
a recurrent Q-learning algorithm (Hausknecht & Stone, 2015; Zhu et al., 2017) produces a statistic in their
hidden states that provide a high amount of information about the belief of state variables that are relevant
for optimal control. More precisely, we have shown that the MI between the hidden states of the RNN and
the belief of states variables that are relevant for optimal control was increasing throughout the training
process. In addition, we have shown that the ability of a recurrent architecture to reproduce, through a high
MI, the belief filter conditions the performance of its policy. Finally, we showed that the MI between the
hidden states and the beliefs of state variables that are irrelevant for optimal control decreases through the
training process, suggesting that RNNs only focus on the relevant part of the belief.
This work also opens up several paths for future work. First, this work suggests that enforcing a high MI
between the hidden states and the beliefs leads to an increase in the performances of the algorithm and
in the return of the resulting policy. While other works have focused on an explicit representation of the
belief in the hidden states (Karkus et al., 2017; Igl et al., 2018), which required to design specific recurrent
architectures, we propose to implicitly embed the belief in the hidden state of any recurrent architecture by
maximising their MI. When the belief or state particles are available, this can be done by adding an auxiliary
loss such that the RNN also maximises the MI. In practice, this can be implemented by backpropagating
the MINE loss beyond the MINE architecture through the unrolled RNN architecture, such that the hidden
states are optimized to get a higher MI with the beliefs.
Moreover, this work could be extended to algorithms that approximate other functions of the histories than
theQ-function. Notably, this study could be extended to the hidden states of a recurrent policy learned by
policy-gradient algorithms or to the hidden states of the actor and the critic in actor-critic methods. We
may nevertheless expect to find similar results since the value function of a policy tends towards the optimal
value function when the policy tends towards the optimal policy.
12Published in Transactions on Machine Learning Research (08/2022)
Acknowledgments
GaspardLambrechtsgratefullyacknowledgesthefinancialsupportofthe Wallonia-Brussels Federation forhis
FRIA grant and the financial support of the Walloon Region for Grant No. 2010235 – ARIAC by DW4AI.
Adrien Bolland gratefully acknowledges the financial support of the Wallonia-Brussels Federation for his
FNRS grant. Computational resources have been provided by the Consortium des Équipements de Calcul
Intensif (CÉCI), funded by the Fonds de la Recherche Scientifique de Belgique (F.R.S.-FNRS) under Grant
No. 2502011 and by the Walloon Region.
References
Bram Bakker. Reinforcement learning with long short-term memory. Advances in neural information pro-
cessing systems , 14, 2001.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville,
and Devon Hjelm. Mutual information neural estimation. In International conference on machine learning ,
pp. 531–540, 2018.
Dimitri Bertsekas. Dynamic programming and optimal control: Volume I , volume 1. Athena scientific, 2012.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated
recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 , 2014.
Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain Markov process expecta-
tions for large time, I. Communications on Pure and Applied Mathematics , 28(1):1–47, 1975.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In International conference on machine
learning, pp. 1861–1870, 2018.
Matthew Hausknecht and Peter Stone. Deep recurrent Q-learning for partially observable MDPs. In Asso-
ciation for the advancement of artificial intelligence fall symposium series , 2015.
NicolasHeess, JonathanJHunt, TimothyPLillicrap, andDavidSilver. Memory-basedcontrolwithrecurrent
neural networks. arXiv preprint arXiv:1512.04455 , 2015.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan,
Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement
learning. In Thirty-second association for the advancement of artificial intelligence conference on artificial
intelligence , 2018.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–1780,
1997.
Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep variational re-
inforcement learning for POMDPs. In International Conference on Machine Learning , pp. 2117–2126,
2018.
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially
observable stochastic domains. Artificial intelligence , 101(1-2):99–134, 1998.
Peter Karkus, David Hsu, and Wee Sun Lee. QMDP-net: Deep learning for planning under partial observ-
ability.Advances in neural information processing systems , 30, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Physical
review E , 69(6):066138, 2004.
13Published in Transactions on Machine Learning Research (08/2022)
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971 , 2015.
Vladimir Mikulik, Grégoire Delétang, Tom McGrath, Tim Genewein, Miljan Martic, Shane Legg, and Pedro
Ortega. Meta-trained agents implement Bayes-optimal agents. Advances in neural information processing
systems, 33:18691–18703, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529–533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Inter-
national conference on machine learning , pp. 1928–1937, 2016.
Josep M. Porta, Matthijs T. J. Spaan, and Nikos Vlassis. Value iteration for continuous-state POMDPs.
Technical Report IAS-UVA-04-04 , December 2004.
Richard D Smallwood and Edward J Sondik. The optimal control of partially observable markov processes
over a finite horizon. Operations research , 21(5):1071–1088, 1973.
Sebastian Thrun. Probabilistic robotics. Communications of the ACM , 45(3):52–57, 2002.
Nicolas Vecoven, Damien Ernst, and Guillaume Drion. A bio-inspired bistable recurrent cell allows for
long-lasting memory. Plos one, 16(6):e0252676, 2021.
Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the institute
of electrical and electronics engineers , 78(10):1550–1560, 1990.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexan-
der J Smola. Deep sets. Advances in neural information processing systems , 30, 2017.
Guo-Bing Zhou, Jianxin Wu, Chen-Lin Zhang, and Zhi-Hua Zhou. Minimal gated unit for recurrent neural
networks. International Journal of Automation and Computing , 13(3):226–234, 2016.
Pengfei Zhu, Xin Li, Pascal Poupart, and Guanghui Miao. On improving deep reinforcement learning for
POMDPs. arXiv preprint arXiv:1704.07978 , 2017.
14Published in Transactions on Machine Learning Research (08/2022)
A Environments
In this section, the class of environments that are considered in this work are introduced. Then, the envi-
ronments are formally defined.
A.1 Class of environments
In the experiments, the class of POMDPs that are considered is restricted to those where we can observe
fromotif a state stis terminal. A state s∈Sis said to be terminal if, and only if
/braceleftigg
T(s′|s,a) =δs(s′),∀s′∈S,∀a∈A
R(s,a,s) = 0,∀a∈A(24)
(25)
whereδsdenotes the Dirac distribution centred in s∈S. As can be noted, the expected cumulative reward
of any policy when starting in a terminal state is zero. As a consequence, the Q-function of a history for
which we observe a terminal state is also zero for any initial action. The PRQL algorithm thus only has to
learn theQ-function of histories that have not yet reached a terminal state. It implies that the histories that
are generated in the POMDP can be interrupted as soon as a terminal state is observed.
A.2 T-Maze environments
The T-Maze environment is a POMDP (S,A,O,p0,T,R,O,γ )parameterised by the maze length L∈Nand
the stochasticity rate λ∈[0,1]. The formal definition of this environment is given below.
m=Up
m=Down
c= (0,0) c= (1,0) c= (2,0) c= (3,0) c= (4,0) c= (5,0) c= (6,0)... c= (L,0)c= (L,1)
c= (L,−1)c= (0,0) c= (1,0) c= (2,0) c= (3,0) c= (4,0) c= (5,0) c= (6,0)... c= (L,0)c= (L,1)
c= (L,−1)
Figure 10: T-Maze state space. Initial states in blue, terminal states in grey, and treasure states hatched.
State space. The discrete state space Sis composed of the set of positions Cfor the agent in each of the
two maze layouts M. The maze layout determines the position of the treasure. Formally, we have


S=M×C
M={Up,Down}
C={(0,0),..., (L,0)}∪{ (L,1),(L,−1)}(26)
(27)
(28)
A state st∈ Sis thus defined by st= (mt,ct)with mt∈ Mandct∈ C. Let us also define F=
{st= (mt,ct)∈S| ct∈{(L,1),(L,−1)}}the set of terminal states, four in number.
15Published in Transactions on Machine Learning Research (08/2022)
Action space. The discrete action space Ais composed of the four possible moves that the agent can take
A={(1,0),(0,1),(−1,0),(0,−1)} (29)
that correspond to Right, Up, Left and Down, respectively.
Observation space. The discrete observation space Ois composed of the four partial observations of the
state that the agent can perceive
O={Up,Down,Corridor,Junction}. (30)
Initial state distribution. The two possible initial states are sUp
0= (Up,(0,0))andsDown
0 =
(Down,(0,0)), depending on the maze in which the agent lies. The initial state distribution p0:S→ [0,1]
is thus given by
p0(s0) =

0.5ifs0=sUp
0
0.5ifs0=sDown
0
0otherwise(31)
Transition distribution. The transition distribution function T:S×A×S→ [0,1]is given by
T(st+1|st,at) =/braceleftigg
δst(st+1) ifst∈F
(1−λ)δf(st,at)(st+1) +λ
4/parenleftbig/summationtext
a∈Aδf(st,a)(st+1)/parenrightbig
otherwise(32)
where st∈S,at∈Aandst+1∈S, andfis given by
f(st,at) =/braceleftigg
st+1= (mt,ct+at)ifst̸∈F,ct+at∈C
st+1= (mt,ct) otherwise(33)
where st= (mt,ct)∈Sandat∈A.
Reward function. The reward function R:S×A×S→ Ris given by
R(st,at,st+1) =

0ifst∈F
0ifst̸∈F,st+1̸∈F,st̸=st+1
−0.1ifst̸∈F,st+1̸∈F,st=st+1
4ifst̸∈F,st+1∈F,ct+1=/braceleftigg
(L,1)ifmt+1=Up
(L,−1)ifmt+1=Down
−0.1ifst̸∈F,st+1∈F,ct+1=/braceleftigg
(L,−1)ifmt+1=Up
(L,+1)ifmt+1=Down(34)
where st= (mt,ct)∈S,at∈Aandst+1= (mt+1,ct+1)∈S.
Observation distribution. In the T-Maze, the observations are deterministic. The observation distribu-
tionO:S×O→ [0,1]is given by
O(ot|st) =

1ifot=Up,ct= (0,0),mt=Up
1ifot=Down,ct= (0,0),mt=Down
1ifot=Corridor,ct∈{(1,0),..., (L−1,0)}
1ifot=Junction,ct∈{(L,0),(L,1),(L,−1)}
0otherwise(35)
where st= (mt,ct)∈Sandot∈O.
16Published in Transactions on Machine Learning Research (08/2022)
Exploration policy. Theexplorationpolicy E:A→ [0,1]isastochasticpolicythatisgivenby E(Right ) =
1/2andE(Other ) = 1/6where Other∈{Up,Left,Down}. It enforces the exploration of the right hand side
of the maze layouts. This exploration policy, tailored to the T-Maze environment, allows one to speed up
the training procedure, without interfering with the study of this work.
Truncation horizon. The truncation horizon Hof the DRQN algorithm is chosen such that the expected
displacement of an agent moving according to the exploration policy in a T-Maze with an infinite corridor
on both sides is greater than L. Letr=E(Right )andl=E(Left). In this infinite T-Maze, the probability of
increasing its position is p= (1−λ)r+λ1
4and the probability of decreasing its position is q= (1−λ)l+λ1
4.
As a consequence, starting at 0, the expected displacement after one time step is ¯x1= (1−λ)(r−l). By
independence, ¯xH=H¯x1such that, for ¯xH≥L, the time horizon is given by
H=/ceilingleftbiggL
(1−λ)(r−l)/ceilingrightbigg
. (36)
A.3 Mountain Hike environments
The Varying Mountain Hike environment is a POMDP (S,A,O,p0,T,R,O,γ )parameterised by the sensor
varianceσO∈Rand the transition variance σT∈R. The formal definition of this environment is given
below.
−1.0−0.5 0.0 0.5 1.0
x1−1.00−0.75−0.50−0.250.000.250.500.751.00x2F
−1.2−1.2
−1.0−1.0−1.0
−0.8−0.8−0.8−0.8−0.8
−0.6−0.6
−0.6−0.6
−0.4−0.4
−0.4−0.2
−0.2
x1−1.0
−0.5
0.0
0.5
1.0x2
−1.0−0.50.00.51.0h(x)
−1.25−1.00−0.75−0.50−0.250.00
Figure 11: Mountain hike altitude function hinX.
State space. The state space Sis the set of positions Xand orientations Cthat the agent can take.
Formally, we have

S=X×C
X= [−1,1]2
C={0◦,90◦,180◦,270◦}(37)
(38)
(39)
The orientation c= 0◦,90◦,180◦and270◦corresponds to facing East, North, West and South, respectively.
The set of terminal states is F={s= (x,c)∈S|∥ x−(0.8,0.8)∥<0.1}.
Action space. The discrete action space Ais composed of the four possible directions in which the agent
can move
A={(0,0.1),(−0.1,0),(0,−0.1),(0.1,0)} (40)
that correspond to Forward, Left, Backward and Right, respectively.
17Published in Transactions on Machine Learning Research (08/2022)
Observation space. The continuous observation space is O=R.
Initial state distribution. The initial position is always is always x= (−0.8,−0.8)and the initial
orientation is sampled uniformly in C, such that the initial state distribution p0:S→ [0,1]is given by
p0(s0) =/summationdisplay
c∈C1
|C|δ((−0.8,−0.8),c)(s0) (41)
Transition distribution. The transition distribution T:S×A×S→ [0,1]is given by the conditional
probability distribution of the random variable (st+1|st,at)that is defined as
st+1=/braceleftigg
st ifst∈F
clampS(st+R(c)at+N(0,σT))otherwise(42)
where clampS(s)is the function that maps sto the point inSthat minimizes its distance with s, and
R(c) =/parenleftbiggcosc−sinc
sinccosc/parenrightbigg
(43)
is the two-dimensional rotation matrix for an angle c.
Reward function. The reward function R:S×A×S→ Ris given by
R(st,at,st+1) =/braceleftigg
0 ifst∈F
h(st+1)otherwise(44)
where st∈S,at∈A,st+1∈S, andh:S →R−is the function that gives the relative altitude to the
mountain top in any state. Note that the altitude is independent of the agent orientation.
Observation distribution. The observation distribution O:S×O→ [0,1]is given by
O(ot|st) =ϕ(ot;h(st),σ2
O) (45)
where st∈Sandot∈O, and where ϕ(·;µ,σ2)denotes the probability density function of a univariate
Gaussian random variable with mean µand standard deviation σ.
Mountain Hike. The Mountain Hike environment is a POMDP (S,A,O,p0,T,R,O,γ ), parameterised
by the sensor variance σO∈Rand the transition variance σT∈R. The formal definition of this environment
is identical to that of the Varying Mountain Hike, except that the initial orientation of the agent is always
North, which makes it an easier problem. The initial state distribution is thus given by
p0(s0) =δ((−0.8,−0.8),90◦)(s0). (46)
Exploration policy. The uniform distribution U(A)over the action space Ais chosen as the exploration
policyE(A).
Truncation horizon. The truncation horizon of the DRQN algorithm is chosen equal to H= 80for the
Mountain Hike environment and H= 160for the Varying Mountain Hike environment.
B Deep recurrent Q-network
The DRQN algorithm is an instance of the PRQL algorithm that introduces several improvements over
vanilla PRQL. First, it is adapted to the online setting by interleaving the generation of episodes and the
18Published in Transactions on Machine Learning Research (08/2022)
update of the estimation Qθ. In addition, in the DRQN algorithm, the episodes are generated with the ε-
greedy policy σε
θ:H→P (A), derived from the current estimation Qθ. This stochastic policy selects actions
according to arg maxa∈AQθ(·,a)with probability 1−ε, and according to an exploration policy E(A)∈P(A)
with probability ε. In addition, a replay buffer of histories is used and the gradient is evaluated on a batch
of histories sampled from this buffer. Furthermore, the parameters θare updated with the Adam algorithm
(Kingma & Ba, 2014). Finally, the target rt+γmax a∈AQθ′(η0:t+1,a)is computed using a past version Qθ′
of the estimation Qθwith parameters θ′that are updated to θless frequently, which eases the convergence
towards the target, and ultimately towards the Q-function. The DRQN training procedure is detailed in
Algorithm 1.
Algorithm 1: DRQN -Q-function approximation
Parameters: N∈Nthe buffer capacity.
C∈Nthe target update period (in episodes).
E∈Nthe number of episodes.
H∈Nthe truncation horizon.
I∈Nthe number of gradient steps after each episode.
ε∈Rthe exploration rate.
α∈Rthe learning rate.
B∈Nthe batch size.
Inputs : (S,A,O,T,R,O,p 0,γ)a POMDP.
E(A)∈P(A)the exploration policy.
1Initialise empty replay buffer B
2Initialise parameters θrandomly
3fore= 0,...,E−1do
4 ifemodC= 0then
5 Update target network with θ′←θ
// Generate new episode, store history and rewards
6Draw an initial state s0according to p0and observe o0
7Letη0:0= (o0)
8 fort= 0,...,H−1do
9 Select at∼E(A)with probability ε, otherwise select at= arg maxa∈A{Qθ(η0:t,a)}
10 Take action atand observe rtandot+1
11 Letη0:t+1= (o0,a0,o1,..., ot+1)
12 if|B|<Nthenadd(η0:t,at,rt,ot+1,η0:t+1)in replay bufferB
13 elsereplace oldest transition in replay buffer Bby(η0:t,at,rt,ot+1,η0:t+1)
14 ifot+1is terminal then
15 break
// Optimise recurrent Q-network
16 fori= 0,...,I−1do
17 SampleBtransitions (ηb
0:t,ab
t,rb
t,ob
t+1,ηb
0:t+1)uniformly from the replay buffer B
18 Compute targets yb=/braceleftbigg
rb
t+γmax a∈A/braceleftbig
Qθ′(ηb
0:t+1,a)/bracerightbig
ifob
t+1is not terminal
rb
totherwise
19 Compute loss L=/summationtextB−1
b=0/parenleftbig
yb−Qθ(ηb
0:t,ab
t)/parenrightbig2
20 Compute direction gusing Adam optimiser, perform gradient step θ←θ+αg
C Particle filtering
As explained in Section 2, the belief filter becomes intractable for certain POMDPs. In particular, POMDPs
with continuous state space require one to perform an integration over the state space. Furthermore, in
these environments, the belief should be represented by a function over a continuous domain instead of a
finite-dimensional vector. Such arbitrary beliefs cannot be represented in a digital computer.
To overcome these two difficulties, the particle filtering algorithm proposes to represent an approximation
of the belief by a finite set of samples that follows the belief distribution. In other words, we represent
bt∈P(S)by the set of Msamples
St={sm
t}M−1
m=0(47)
where sm
t∈S, m= 0,...,M−1being independent realisations of the distribution bt.
19Published in Transactions on Machine Learning Research (08/2022)
Particle filtering is a procedure that allows one to sample a set of states Stthat follow the belief distribution
bt. The set is thus updated each time that a new action at−1is taken and a new observation otis observed.
Although this procedure does not require to evaluate expression (8), it is necessary to be able to sample
from the initial state distribution p0and from the transition distribution T, and to be able to evaluate the
observation distribution O. This process, illustrated in Algorithm 2, guarantees that the successive sets
S0,...,SHhave (weighted) samples following the probability distribution b0,...,bHdefined by equation (8).
Algorithm 2: Particle filtering
Parameters: M∈Nthe number of particles
Inputs : (S,A,O,T,R,O,p 0,γ)a POMDP.
H∈Nthe number of transitions
η0:H= (o0,a0,..., oH−1,aH−1,oH)∈H 0:Ha history
// Generate weighted samples following the initial belief b0
1Sample s0
0,..., sM−1
0∼p0
2η←0
3form= 0,...,M−1do
4wm
0←O(o0|sm
0)
5η←η+wm
0
6form= 0,...,M−1do
7wm
0←wm
0/η
8S0=/braceleftbig
(sm
0,wm
0)/bracerightbigM−1
m=0
// Generate successive weighted samples following the beliefs b1,...,bH
9fort= 1,...,H do
10η←0
11 form= 0,...,M−1do
12 Samplel∈{0,...,M−1}according to p(l) =wl
t−1
13 Sample sm
t∼T(·|sl
t−1,at−1)
14 wm
t←O(ot|sm
t)
15 η←η+wm
t
16 form= 0,...,M−1do
17 wm
t←wm
t/η
18St=/braceleftbig
(sm
t,wm
t)/bracerightbigM−1
m=0
Algorithm 2 starts from Nsamples from the initial distribution p0. These samples are initially weighted
by their likelihood O(o0|sn
0). Then, we have three steps that are repeated at each time step. First, the
samples are resampled according to their weights. Then, given the action, the samples are updated by
sampling from T(·|sn
t,at). Finally, these new samples are weighted by their likelihood O(ot+1|sn
t+1)
given the new observation ot+1, as for the initial samples. As stated above, this method ensures that the
(weighted) samples follow the distribution of the successive beliefs.
D Mutual information neural estimator
In Subsection D.1, the MI estimator that is used in the experiments is formally defined, and the algorithm
that is used to derive this estimator is detailed. In Subsection D.2, we formalise the extension of the MINE
algorithm with the Deep Set architecture.
D.1 Estimator
As explained in Subsection 2.3, the ideal MI neural estimator, for a parameter space Φ, is given by
IΦ(X;Y) = sup
ϕ∈Φiϕ(X;Y) (48)
iϕ(X;Y) =Ez∼p[Tϕ(z)]−log/parenleftig
Ez∼q/bracketleftig
eTϕ(z)/bracketrightig/parenrightig
(49)
However, both the estimation of the expectations and the computation of the supremum are intractable. In
practice, the expectations are thus estimated with the empirical means over the set of samples {(xn,yn)}N−1
n=0
20Published in Transactions on Machine Learning Research (08/2022)
drawn from the joint distribution pand the set of samples {(xn,˜yn)}N−1
n=0obtained by permuting the samples
fromY, such that the pairs follow the product of marginal distributions q=pX⊗pY. In order to estimate
the supremum over the parameter space Φ, the MINE algorithm proposes to maximise iϕ(X;Y)by stochastic
gradient ascent over batches from the two sets of samples, as detailed in Algorithm 3. The final parameters
ϕ∗obtained by this maximisation procedure define the estimator
ˆI=1
NN−1/summationdisplay
n=0Tϕ∗(xn,yn)−log/parenleftigg
1
NN−1/summationdisplay
n=0eTϕ∗(xn,˜yn)/parenrightigg
(50)
that is used in the experiments. This algorithm was initially proposed in (Belghazi et al., 2018).
Algorithm 3: MINE - lower bound optimization
Parameters: E∈Nthe number of episodes.
B∈Nthe batch size.
α∈Rthe learning rate.
Inputs : N∈Nthe number of samples.
D={(xn,yn)}N−1
n=0the set of samples from the joint distribution.
1Initialise parameters ϕrandomly.
2fore= 0,...,E−1do
3Letpa random permutation of {0,...,N−1}.
4Let˜p1a random permutation of {0,...,N−1}.
5Let˜p2a random permutation of {0,...,N−1}.
6 whilei= 0,...,/floorleftbigN
B/floorrightbig
do
7 LetS←/braceleftbig
(xp(k),yp(k))/bracerightbig(i+1)B−1
k=iBa batch of samples from the joint distribution.
8 Let˜S←/braceleftbig
(x˜p1(k),y˜p2(k))/bracerightbig(i+1)B−1
k=iBa batch of samples from the product of
marginal distributions
9 Evaluate the lower bound
L(ϕ)←1
B/summationdisplay
(x,y)∈STϕ(x,y)−log
1
B/summationdisplay
(˜x,˜y)∈˜SeTϕ(˜x,˜y)

10 Evaluate bias corrected gradients G(ϕ)←˜∇ϕL(ϕ)
11 Update network parameters with ϕ←ϕ+αG(ϕ)
D.2 Deep sets
As explained in Subsection 4.1, the belief computation is intractable for environments with continuous state
spaces. Intheexperiments, thebeliefofsuchenvironmentsisapproximatedbyasetofparticles S={sm}M
m=1
that are guaranteed to follow the belief distribution, such that sm∼b,∀sm∈S(see Appendix C). Those
particles could be used for constructing an approximation of the belief distribution, a problem known as
density estimation. We nonetheless do not need an explicit estimate of this distribution. Instead, the
particles can be directly consumed by the MINE network. In this case, the two sets of input samples of the
MINE algorithm take the form
{(xn,yn)}N−1
n=0={(hn,Sn)}N−1
n=0 (51)
=/braceleftig
(hn,{sn,m}M
m=1)/bracerightigN−1
n=0. (52)
In order to process particles from sets Snas input of the neural network Tϕ, we choose an architecture that
guarantees its invariance to permutations of the particles. The deep set architecture (Zaheer et al., 2017),
that is written as ρϕ/parenleftbig/summationtext
s∈Sψϕ(s)/parenrightbig
, provides such guarantees. Moreover, this architecture is theoretically
able to represent any function on sets, under the assumption of having representative enough mappings ρϕ
andψϕand the additional assumption of using finite sets Swhen particles come from an uncountable set as
21Published in Transactions on Machine Learning Research (08/2022)
in this work. The function Tϕis thus given by
Tϕ(h,S) =µϕ/parenleftigg
h,ρϕ/parenleftigg/summationdisplay
s∈Sψϕ(s)/parenrightigg/parenrightigg
(53)
when the belief is approximated by a set of particles.
E Hyperparameters
The hyperparameters of the DRQN algorithm are given in Table 1 and the hyperparameters of the MINE
algorithm are given in Table 2. The value of those hyperparameters have been chosen a priori, except for
the number of episodes of the DRQN algorithm and the number of epochs of the MINE algorithm. These
were chosen so as to ensure convergence of the policy return and the MINE lower bound, respectively. The
parameters of the Mountain Hike and Varying Mountain Hike environments are given in Table 3.
Name Value Description
S 2Number of RNN layers
D 1 Number of linear layers (no activation function)
H 32Hidden state size
N 8192 Replay buffer capacity
C 10Target update period in term of episodes
I 10Number of gradient steps after each episode
ε 0.2Exploration rate
B 32Batch size
α 1×10−3Adam learning rate
Table 1: DRQN architecture and training hyperparameters.
Name Value Description
L 2Number of hidden layers
H 256 Hidden layer size
N 10 000 Training set size
E 200 Number of epochs
B 1024 Batch size
α 1×10−3Adam learning rate
R 16Representation size for the Deep Set architecture
α 0.01EMA rate for the bias corrected gradient
Table 2: MINE architecture and training hyperparameters.
Name Value Description
σO0.1 Standard deviation of the observation noise
σT0.05 Standard deviation of the transition noise
Table 3: Mountain Hike and Varying Mountain Hike parameters.
22Published in Transactions on Machine Learning Research (08/2022)
F Generalisation to other distribution of histories
In this section, we study if the hidden state still provides information about the belief under other distribu-
tions of histories than the one induced by the learned policy (20). This generalisation to other distributions
is desirable for building policies that are more robust to perturbations of the histories.
We propose to study the evolution of the MI between the hidden state and the belief when adding noise to
the policy used to sample the histories. Formally, instead of sampling the hidden states and beliefs according
to (20), we propose to sample those according to
pε(h,b) =∞/summationdisplay
t=0p(t)/integraldisplay
Hp(h,b|η)pσε
θ(η|t) dη (54)
wherep(t)isonceagainchosentotheuniformdistributionoverthetimesteps p(t) = 1/H, t∈{0,...,H−1},
σε
θistheε-greedypolicyasdefinedinAppendixB, and pσε
θ(η|t)givestheconditionalprobabilitydistribution
induced by the policy σε
θover histories η∈Hgiven that their length is t∈N0. Note that the training
procedure remains unchanged.
The results of this additional study can be found in Figure 12, for ε∈{0.0,0.2,0.4,0.6,0.8,1.0}. It can be
noted that p0.0is the distribution of hidden states and beliefs induced by the learned policy (20), and p1.0is
the distribution of hidden states and beliefs induced by a fully random policy. For reasons of computational
capacity, this analysis was carried out for the GRU cell only. This cell was chosen for being a standard cell
that performs well in all environments in terms of return, unlike the LSTM. As can be seen in Figure 12, the
MI between the hidden states and the beliefs increases throughout the training process, under all considered
policies, even the fully random policy. We conclude that the correlation between the hidden states and beliefs
generalises reasonably well to other distributions. In other words, the hidden states still capture information
about the beliefs even under other distributions of histories.
G Correlations between the empirical return and the estimated mutual information
The correlation between the empirical return and the estimated MI are computed with the Pearson’s linear
correlation coefficient and the Spearman’s rank correlation coefficient. These coefficients are reported for
all environments and all cells in Table 4 and Table 5. The columns named aggregated give the correlation
coefficients measured over all samples of ˆIand ˆJfrom all cells.
Environment Aggregated LSTM GRU BRC nBRC MGU
T-Maze (L= 50,λ= 0.0) 0.8233 0.7329 0.8500 0.8747 0.9314 0.9178
T-Maze (L= 100,λ= 0.0) 0.5347 0.3624 0.6162 0.6855 0.6504 0.6299
T-Maze (L= 50,λ= 0.3) 0.5460 0.2882 0.8008 0.7229 0.7424 0.6159
Mountain Hike 0.5948 0.7352 0.6177 0.4338 0.5857 0.5485
Varying Mountain Hike 0.5982 0.6712 0.4530 0.4446 0.3669 0.3006
Table 4: Pearson’s linear correlation coefficient for each environment and cell.
Environment Aggregated LSTM GRU BRC nBRC MGU
T-Maze (L= 50,λ= 0.0) 0.6419 0.7815 0.5963 0.5403 0.4009 0.5002
T-Maze (L= 100,λ= 0.0) 0.6666 0.5969 0.7108 0.5058 0.4605 0.5534
T-Maze (L= 50,λ= 0.3) 0.6403 0.3730 0.6600 0.5090 0.4706 0.6497
Mountain Hike 0.2965 0.5933 0.1443 0.2762 0.4337 0.2630
Varying Mountain Hike 0.6176 0.6869 0.3677 0.4355 0.2955 0.2266
Table 5: Spearman’s rank correlation coefficient for each environment and cell.
23Published in Transactions on Machine Learning Research (08/2022)
-1011.46Return ˆJ(θe)
0 1000 2000 3000 4000 5000
Episodee246MIˆI(θe) [bit] ε= 0.0
ε= 0.2
ε= 0.4
ε= 0.6
ε= 0.8
ε= 1.0
(a) Deterministic T-Maze ( L= 50)
-2-100.53Return ˆJ(θe)
0 2000 4000 6000 8000 10000
Episodee2.55.0MIˆI(θe) [bit]ε= 0.0
ε= 0.2
ε= 0.4
ε= 0.6
ε= 0.8
ε= 1.0 (b) Deterministic T-Maze ( L= 100 )
-2-100.28Return ˆJ(θe)
0 1000 2000 3000 4000 5000
Episodee246MIˆI(θe) [bit]ε= 0.0
ε= 0.2
ε= 0.4
ε= 0.6
ε= 0.8
ε= 1.0
(c) Stochastic T-Maze ( L= 50,λ= 0.3)
−60−40−20Return ˆJ(θe)
0 2000 4000 6000 8000 10000
Episodee024MIˆI(θe) [bit]ε= 0.0
ε= 0.2
ε= 0.4
ε= 0.6
ε= 0.8
ε= 1.0 (d) Mountain Hike
−80−60−40Return ˆJ(θe)
0 10000 20000 30000 40000
Episodee024MIˆI(θe) [bit]ε= 0.0
ε= 0.2
ε= 0.4
ε= 0.6
ε= 0.8
ε= 1.0
(e) Varying Mountain Hike
Figure 12: Evolution of the return ˆJ(θe)and the MI ˆI(θe)aftereepisodes, under distribution of histories
induced by several ε-greedy policies, for the GRU cell. The maximal expected return is given by the dotted
line.
24Published in Transactions on Machine Learning Research (08/2022)
H Belief of variables irrelevant for the optimal control
In this section, we report the evolution of the return and the MI between the hidden states and the belief of
both the relevant and irrelevant variables for the LSTM, BRC, nBRC and MGU architectures. It completes
the results obtained for the GRU cell in Subsection 4.4.
-1011.46Return ˆJ(θe)
0 1000 2000 3000 4000 5000
Episodee2.55.0MIˆI(θe) [bit] Belief for sI
Belief for s
(a)d= 1
-1011.46Return ˆJ(θe)
0 1000 2000 3000 4000 5000
Episodee510MIˆI(θe) [bit] Belief for sI
Belief for s (b)d= 4
Figure 13: Deterministic T-Maze ( L= 50) withdirrelevant state variables. Evolution of the return ˆJ(θe)
and the MI ˆI(θe)for the belief of the irrelevant and relevant state variables after eepisodes, for the LSTM
cell.
-1011.46Return ˆJ(θe)
0 1000 2000 3000 4000 5000
Episodee24MIˆI(θe) [bit]
Belief for sI
Belief for s
(a)d= 1
-1011.46Return ˆJ(θe)
0 1000 2000 3000 4000 5000
Episodee2.55.07.5MIˆI(θe) [bit] Belief for sI
Belief for s (b)d= 4
Figure 14: Deterministic T-Maze ( L= 50) withdirrelevant state variables. Evolution of the return ˆJ(θe)
and the MI ˆI(θe)for the belief of the irrelevant and relevant state variables after eepisodes, for the BRC
cell.
Figure 13, Figure 14, Figure 15, and Figure 16 show the evolution of the return and the MI for a T-Maze of
lengthL= 50withd∈{1,4}irrelevant state variables added to the process for these cells. These results
are reported for the GRU cell in Figure 8 (see Subsection 4.2). As can be seen from these figures, the return
generally increases with the MI between the hidden states and the belief of state variables that are relevant
for optimal control. Moreover, as for the GRU cell, the MI between the hidden states and the belief of
irrelevant state variables generally decreases throughout the learning process.
Additionally, it can be observed that the LSTM and BRC cells fail in achieving a near-optimal return when
d= 4. As far as the LSTM is concerned, it is reflected in its MI that reaches a lower value than the other
25Published in Transactions on Machine Learning Research (08/2022)
RNNs. Likewise, the BRC cell does not reach a high return, and the MI does not increase at all. For this
cell, it can be seen that the MI with the belief of irrelevant state variables is not decreasing, even with d= 1.
The inability of the BRC cell to increase its MI with the belief of relevant variables and to decrease its MI
with the belief of irrelevant variables might explain its bad performance in this environment.
-1011.46Return ˆJ(θe)
0 1000 2000 3000 4000 5000
Episodee246MIˆI(θe) [bit]
Belief for sI
Belief for s
(a)d= 1
-1011.46Return ˆJ(θe)
0 1000 2000 3000 4000 5000
Episodee2.55.07.5MIˆI(θe) [bit] Belief for sI
Belief for s (b)d= 4
Figure 15: Deterministic T-Maze ( L= 50) withdirrelevant state variables. Evolution of the return ˆJ(θe)
and the MI ˆI(θe)for the belief of the irrelevant and relevant state variables after eepisodes, for the nBRC
cell.
-1011.46Return ˆJ(θe)
0 1000 2000 3000 4000 5000
Episodee246MIˆI(θe) [bit]
Belief for sI
Belief for s
(a)d= 1
-1011.46Return ˆJ(θe)
0 1000 2000 3000 4000 5000
Episodee510MIˆI(θe) [bit] Belief for sI
Belief for s (b)d= 4
Figure 16: Deterministic T-Maze ( L= 50) withdirrelevant state variables. Evolution of the return ˆJ(θe)
and the MI ˆI(θe)for the belief of the irrelevant and relevant state variables after eepisodes, for the MGU
cell.
As far as the Mountain Hike is concerned, Figure 17, Figure 18, Figure 19 and Figure 20 show that all
previous observations also hold for this environment with the LSTM, BRC, nBRC and MGU cells. These
results are reported for the GRU cell in Figure 9 (see Subsection 4.2). As can be seen from these figures, the
return clearly increases with the MI between the hidden states and the belief of relevant state variables, for
all cells. In contrast, the MI with the belief of irrelevant state variables decreases throughout the learning
process.
26Published in Transactions on Machine Learning Research (08/2022)
−40−20Return ˆJ(θe)
0 2000 4000 6000 8000 10000
Episodee0.02.55.0MIˆI(θe) [bit]Belief for sI
Belief for s
(a)d= 1
−40−20Return ˆJ(θe)
0 2000 4000 6000 8000 10000
Episodee0510MIˆI(θe) [bit]Belief for sI
Belief for s (b)d= 4
Figure 17: Mountain Hike with with dirrelevant state variables. Evolution of the return ˆJ(θe)and the MI
ˆI(θe)for the belief of the irrelevant and relevant state variables after eepisodes, for the LSTM cell.
−40−20Return ˆJ(θe)
0 2000 4000 6000 8000 10000
Episodee24MIˆI(θe) [bit]
Belief for sI
Belief for s
(a)d= 1
−40−20Return ˆJ(θe)
0 2000 4000 6000 8000 10000
Episodee2.55.07.5MIˆI(θe) [bit] Belief for sI
Belief for s (b)d= 4
Figure 18: Mountain Hike with with dirrelevant state variables. Evolution of the return ˆJ(θe)and the MI
ˆI(θe)for the belief of the irrelevant and relevant state variables after eepisodes, for the BRC cell.
27Published in Transactions on Machine Learning Research (08/2022)
−40−20Return ˆJ(θe)
0 2000 4000 6000 8000 10000
Episodee345MIˆI(θe) [bit]
Belief for sI
Belief for s
(a)d= 1
−40−20Return ˆJ(θe)
0 2000 4000 6000 8000 10000
Episodee2.55.07.5MIˆI(θe) [bit] Belief for sI
Belief for s (b)d= 4
Figure 19: Mountain Hike with with dirrelevant state variables. Evolution of the return ˆJ(θe)and the MI
ˆI(θe)for the belief of the irrelevant and relevant state variables after eepisodes, for the nBRC cell.
−40−20Return ˆJ(θe)
0 2000 4000 6000 8000 10000
Episodee246MIˆI(θe) [bit]
Belief for sI
Belief for s
(a)d= 1
−40−20Return ˆJ(θe)
0 2000 4000 6000 8000 10000
Episodee2.55.07.5MIˆI(θe) [bit]Belief for sI
Belief for s (b)d= 4
Figure 20: Mountain Hike with with dirrelevant state variables. Evolution of the return ˆJ(θe)and the MI
ˆI(θe)for the belief of the irrelevant and relevant state variables after eepisodes, for the MGU cell.
28