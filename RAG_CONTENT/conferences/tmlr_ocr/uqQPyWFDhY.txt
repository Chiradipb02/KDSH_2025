Published in Transactions on Machine Learning Research (02/2024)
Error Bounds for Flow Matching Methods
Joe Benton benton@stats.ox.ac.uk
Department of Statistics
University of Oxford
George Deligiannidis deligian@stats.ox.ac.uk
Department of Statistics
University of Oxford
Arnaud Doucet doucet@stats.ox.ac.uk
Department of Statistics
University of Oxford
Reviewed on OpenReview: https: // openreview. net/ forum? id= uqQPyWFDhY
Abstract
Score-based generative models are a popular class of generative modelling techniques rely-
ing on stochastic differential equations (SDEs). From their inception, it was realized that it
was also possible to perform generation using ordinary differential equations (ODEs) rather
than SDEs. This led to the introduction of the probability flow ODE approach and denois-
ing diffusion implicit models. Flow matching methods have recently further extended these
ODE-based approaches and approximate a flow between two arbitrary probability distribu-
tions. Previous work derived bounds on the approximation error of diffusion models under
the stochastic sampling regime, given assumptions on the L2loss. We present error bounds
for the flow matching procedure using fully deterministic sampling, assuming an L2bound
on the approximation error and a certain regularity condition on the data distributions.
1 Introduction
Much recent progress in generative modelling has focused on learning a map from an easy-to-sample reference
distribution π0to a target distribution π1. Recent works have demonstrated how to learn such a map by
defining a stochastic process transforming π0intoπ1and learning to approximate its marginal vector flow, a
procedure known as flow matching (Lipman et al., 2023; Liu et al., 2023; Albergo & Vanden-Eijnden, 2023;
Albergo et al., 2023; Heitz et al., 2023). Score-based generative models (Song et al., 2021b; Song & Ermon,
2019; Ho et al., 2020) can be viewed as a particular instance of flow matching where the interpolating paths
are defined via Gaussian transition densities. However, the more general flow matching setting allows one
to consider a broader class of interpolating paths, and leads to deterministic sampling schemes which are
typically faster and require fewer steps (Song et al., 2021a; Zhang & Chen, 2023).
Given the striking empirical successes of these models at generating high-quality audio and visual data
(Dhariwal & Nichol, 2021; Popov et al., 2021; Ramesh et al., 2022; Saharia et al., 2022), there has been
significant interest in understanding their theoretical properties. Several works have sought convergence
guarantees for score-based generative models with stochastic sampling (Block et al., 2022; De Bortoli, 2022;
Lee et al., 2022), with recent work demonstrating that the approximation error of these methods decays
polynomially in the relevant problem parameters under mild assumptions (Chen et al., 2023c; Lee et al.,
2023; Chen et al., 2023a). Other works have explored bounds in the more general flow matching setting
(Albergo & Vanden-Eijnden, 2023; Albergo et al., 2023). However these works either still require some
stochasticity in the sampling procedure or do not hold for data distributions without full support.
1Published in Transactions on Machine Learning Research (02/2024)
In this work we present the first bounds on the error of the flow matching procedure that apply with fully
deterministic sampling for data distributions without full support. Our results come in two parts: first, we
control the error of the flow matching approximation under the 2-Wasserstein metric in terms of the L2
training error and the Lipschitz constant of the approximate velocity field; second, we show that, under
a smoothness assumption explained in Section 3.2, the true velocity field is Lipschitz and we bound the
associated Lipschitz constant. Combining the two results, we obtain a bound on the approximation error of
flow matching which depends polynomially on the L2training error.
1.1 Related work
Flow methods: Probability flow ODEs were originally introduced by Song et al. (2021b) in the context
of score-based generative modelling. They can be viewed as an instance of the normalizing flow framework
(Rezende & Mohamed, 2015; Chen et al., 2018), but where the additional Gaussian diffusion structure allows
for much faster training and sampling schemes, for example using denoising score matching (Vincent, 2011),
compared to previous methods (Grathwohl et al., 2019; Rozen et al., 2021; Ben-Hamu et al., 2022).
Diffusion models are typically expensive to sample and several works have introduced simplified sampling
or training procedures. Song et al. (2021a) propose Denoising Diffusion Implicit Models (DDIM), a method
usingnon-Markoviannoisingprocesseswhichallowsforfasterdeterministicsampling. Alternatively, (Lipman
et al., 2023; Liu et al., 2023; Albergo & Vanden-Eijnden, 2023) propose flow matching as a technique for
simplifying the training procedure and allowing for deterministic sampling, while also incorporating a much
wider class of possible noising processes. Albergo et al. (2023) provide an in depth study of flow matching
methods, including a discussion of the relative benefits of different interpolating paths and stochastic versus
deterministic sampling methods.
Errorbounds: Boundsontheapproximationerrorofdiffusionmodelswithstochasticsamplingprocedures
have been extensively studied. Initial results typically relied either on restrictive assumptions on the data
distribution (Lee et al., 2022; Yang & Wibisono, 2022) or produced non-quantitative or exponential bounds
(Pidstrigach, 2022; Liu et al., 2022; De Bortoli et al., 2021; De Bortoli, 2022; Block et al., 2022). Recently,
several works have derived polynomial convergence rates for diffusion models with stochastic sampling (Chen
et al., 2023c; Lee et al., 2023; Chen et al., 2023a; Li et al., 2023; Benton et al., 2023; Conforti et al., 2023).
By comparison, the deterministic sampling regime is less well explored. On the one hand, Albergo & Vanden-
Eijnden (2023) give a bound on the 2-Wasserstein distance between the endpoints of two flow ODEs which
dependsontheLipschitzconstantsoftheflows. However, theirLipschitzconstantisuniformintime, whereas
for most practical data distributions we expect the Lipschitz constant to explode as one approaches the data
distribution; for example, this will happen for data supported on a submanifold. Additionally, Chen et al.
(2023d) derive a polynomial bound on the error of the discretised exact reverse probability flow ODE, though
their work does not treat learned approximations to the flow. Li et al. (2023) also provide bounds for fully
deterministic probability flow sampling, but also require control of the difference between the derivatives of
the true and approximate scores.
On the other hand, most other works studying error bounds for flow methods require at least some stochas-
ticity in the sampling scheme. Albergo et al. (2023) provide bounds on the Kullback–Leibler (KL) error
of flow matching, but introduce a small amount of random noise into their sampling procedure in order to
smooth the reverse process. Chen et al. (2023b) derive polynomial error bounds for probability flow ODE
with a learned approximation to the score function. However, they must interleave predictor steps of the
reverse ODE with corrector steps to smooth the reverse process, meaning that their sampling procedure is
also non-deterministic.
In contrast, we provide bounds on the approximation error of a fully deterministic flow matching sampling
scheme, and show how those bounds behave for typical data distributions (for example, those supported on
a manifold).
2Published in Transactions on Machine Learning Research (02/2024)
2 Background on flow matching methods
We give a brief overview of flow matching, as introduced by Lipman et al. (2023); Liu et al. (2023); Albergo &
Vanden-Eijnden (2023); Heitz et al. (2023). Given two probability distributions π0,π1onRd, flow matching
is a method for learning a deterministic coupling between π0andπ1. It works by finding a time-dependent
vector field v:Rd×[0,1]→Rdsuch that if Zx= (Zx
t)t∈[0,1]is a solution to the ODE
dZx
t
dt=v(Zx
t,t), Zx
0=x (1)
for each x∈Rdand if we define Z= (Zt)t∈[0,1]by taking x∼π0and setting Zt=Zx
tfor allt∈[0,1], then
Z1∼π1. WhenZsolves (1) for a given function v, we say that Zis a flow with velocity field v. If we have
such a velocity field, then (Z0,Z1)is a coupling of (π0,π1). If we can sample efficiently from π0then we can
generate approximate samples from the coupling by sampling Z0∼π0and numerically integrating (1). This
setup can be seen as an instance of the continuous normalizing flow framework (Chen et al., 2018).
In order to find such a vector field v, flow matching starts by specifying a path I(x0,x1,t)between every
two points x0andx1inRd. We do this via an interpolant function I:Rd×Rd×[0,1]→Rd, which satisfies
I(x0,x1,0) =x0andI(x0,x1,1) =x1. In this work, we will restrict ourselves to the case of spatially linear
interpolants, where I(x0,x1,t) =αtx0+βtx1for functions α,β : [0,1]→Rsuch thatα0=β1= 1and
α1=β0= 0, but more general choices of interpolating paths are possible.
We then define the stochastic interpolant between π0andπ1to be the stochastic process X= (Xt)t∈[0,1]
formed by sampling X0∼π0,X1∼π1andZ∼N(0,Id)independently and setting Xt=I(X0,X1,t) +γtZ
for eacht∈(0,1)whereγ: [0,1]→[0,∞)is a function such that γ0=γ1= 0and which determines
the amount of Gaussian smoothing applied at time t. The motivation for including γtZis to smooth the
interpolant marginals, as was originally explained by Albergo et al. (2023). Liu et al. (2023) and Albergo &
Vanden-Eijnden (2023) omit γt, leading to deterministic paths between a given X0andX1, while Albergo
et al. (2023) and Lipman et al. (2023) work in the more general case.
The process Xis constructed so that its marginals deform smoothly from π0toπ1. However, Xis not a
suitable choice of flow between π0andπ1since it is not causal – it requires knowledge of X1to construct
Xt. So, we seek a causal flow with the same marginals. The key insight is to define the expected velocity of
Xby
vX(x,t) =E/bracketleftbig˙Xt|Xt=x/bracketrightbig
,for all x∈supp(Xt), (2)
where ˙Xtis the time derivative of Xt, and setting vX(x,t) = 0forx̸∈supp(Xt)for eacht∈[0,1]. Then,
the following result shows that vX(x,t)generates a deterministic flow Zbetweenπ0andπ1with the same
marginals as X(Liu et al., 2023).
Proposition 1. Suppose that Xis path-wise continuously differentiable, the expected velocity field vX(x,t)
exists and is locally bounded, and there exists a unique solution Zxto (1) with velocity field vXfor each
x∈Rd. IfZis the flow with velocity field vX(x,t)starting inπ0, then Law(Xt) = Law(Zt)for allt∈[0,1].
Sufficient conditions for Xto be path-wise continuously differentiable and for vX(x,t)to exist and be locally
bounded are that α,β, andγare continuously differentiable and that π0,π1have bounded support. We
will assume that these conditions hold from now on. We also assume that all our data distributions are
absolutely continuous with respect to the Lebesgue measure.
We can learn an approximation to vXby minimising the objective function
L(v) =/integraldisplay1
0E/bracketleftig
wt/vextenddouble/vextenddoublev(Xt,t)−vX(Xt,t)/vextenddouble/vextenddouble2/bracketrightig
dt,
over allv∈VwhereVis some class of functions, for some weighting function wt: [0,1]→(0,∞). (Typically,
we will take wt= 1for allt.) As written, this objective is intractable, but we can show that
L(v) =/integraldisplay1
0E/bracketleftig
wt/vextenddouble/vextenddoublev(Xt,t)−˙Xt/vextenddouble/vextenddouble2/bracketrightig
dt+constant, (3)
3Published in Transactions on Machine Learning Research (02/2024)
where the constant is independent of v(Lipman et al., 2023). This last integral can be empirically estimated
in an unbiased fashion given access to samples X0∼π0,X1∼π1and the functions αt,βt,γtandwt. In
practice, we often take Vto be a class of functions parameterised by a neural network vθ(x,t)and minimise
L(vθ)over the parameters θusing (3) and stochastic gradient descent. Our hope is that if our approximation
vθis sufficiently close to vX, andYis the flow with velocity field vθ, then if we take Y0∼π0the distribution
ofY1is approximately π1.
Most frequently, flow matching is used as a generative modelling procedure. In order to model a distribution
πfrom which we have access to samples, we set π1=πand takeπ0to be some reference distribution from
whichitiseasytosample, suchasastandardGaussian. Then, weusetheflowmatchingproceduretolearnan
approximateflow vθ(x,t)betweenπ0andπ1. Wegenerateapproximatesamplesfrom π1bysampling Z0∼π0
and integrating the flow equation (1) to find Z1, which should be approximately distributed according to π1.
3 Main results
We now present the main results of the paper. First, we show under three assumptions listed below that
we can control the approximation error of the flow matching procedure under the 2-Wasserstein distance
in terms of the quality of our approximation vθ. We obtain bounds that depend crucially on the spatial
Lipschitz constant of the approximate flow. Second, we show how to control this Lipschitz constant for the
true flowvXunder a smoothness assumption on the data distributions, which is explained in Section 3.2.
Combined with the first result, this will imply that for sufficiently regular π0,π1there is a choice of Vwhich
contains the true flow such that, if we optimise L(v)over allv∈V, then we can bound the error of the flow
matching procedure. Additionally, our bound will be polynomial in the L2approximation error. The results
of this section will be proved under the following three assumptions.
Assumption 1 (Bound on L2approximation error) .The true and approximate drifts vX(x,t)andvθ(x,t)
satisfy/integraltext1
0E/bracketleftbig
∥vθ(Xt,t)−vX(Xt,t)∥2/bracketrightbig
dt≤ε2.
Assumption 2 (Existence and uniqueness of smooth flows) .For each x∈Rdands∈[0,1]there exist
unique flows (Yx
s,t)t∈[s,1]and(Zx
s,t)t∈[s,1]starting in Yx
s,s=xandZx
s,s=xwith velocity fields vθ(x,t)and
vX(x,t)respectively. Moreover, Yx
s,tandZx
s,tare continuously differentiable in x,sandt.
Assumption 3 (Regularity of approximate velocity field) .The approximate flow vθ(x,t)is differentiable
in both inputs. Also, for each t∈(0,1)there is a constant Ltsuch thatvθ(x,t)isLt-Lipschitz in x.
Assumption 1 is the natural assumption on the training error given we are learning with the L2training
loss in (3). Assumption 2 is required since to perform flow matching we need to be able to solve the ODE
(1). Without a smoothness assumption on vθ(x,t), it would be possible for the marginals Ytof the solution
to (1) initialised in Y0∼π0to quickly concentrate on subsets of Rdof arbitrarily small measure under the
distribution of Xt. Then, there would be choices of vθ(x,t)which were very different from vX(x,t)on these
sets of small measure while being equal to vX(x,t)everywhere else. For these vθ(x,t)theL2approximation
error can be kept arbitrarily small while the error of the flow matching procedure is made large. We therefore
require some smoothness assumption on vθ(x,t)– we choose to make Assumption 3 since we can show that
it holds for the true velocity field, as we do in Section 3.2.
3.1 Controlling the Wasserstein difference between flows
Our first main result is the following, which bounds the error of the flow matching procedure in 2-Wasserstein
distance in terms of the L2approximation error and the Lipschitz constant of the approximate flow.
Theorem 1. Suppose that π0,π1are probability distributions on Rd, thatYis the flow starting in π0with
velocity field vθ, and ˆπ1is the law of Y1. Then, under Assumptions 1-3, we have
W2(ˆπ1,π1)≤εexp/braceleftig/integraldisplay1
0Ltdt/bracerightig
.
We see that the error depends linearly on the L2approximation error εand exponentially on the integral
of the Lipschitz constant of the approximate flow Lt. Ostensibly, the exponential dependence on/integraltext1
0Ltdt
4Published in Transactions on Machine Learning Research (02/2024)
is undesirable, since vθmay have a large spatial Lipschitz constant. However, we will show in Section 3.2
that the true velocity field is L∗
t-Lipschitz for a choice of L∗
tsuch that/integraltext1
0L∗
tdtdepends logarithmically on
the amount of Gaussian smoothing. Thus, we may optimise (3) over a class of functions Vwhich are all
L∗
t-Lipschitz, knowing that this class contains the true velocity field, and that if vθlies in this class we get
a bound on the approximation error which is polynomial in the L2approximation error, providing we make
a suitable choice of αt,βtandγt.
We remark that our Theorem 1 is similar in content to Proposition 3 in Albergo & Vanden-Eijnden (2023).
The crucial difference is that we work with a time-varying Lipschitz constant, which is required in practice
since in many cases Ltexplodes as t→0or1, as happens for example if π0orπ1do not have full support.
A key ingredient in the proof of Theorem 1 will be the Alekseev–Gröbner formula, which provides a way to
control the difference between the solutions to two different ODEs in terms of the difference between their
drifts (Gröbner, 1960; Alekseev, 1961).
Proposition 2 (Alekseev–Gröbner) .Letµ(x,t) :Rd×[0,T]→Rdbe a function which is continuous in
tand continuously differentiable in x. LetX:Rd×[0,T]2→Rdbe a continuous solution of the integral
equation
Xx
s,t=x+/integraldisplayt
sµ(r,Xx
s,r) dr,
and letY: [0,T]→Rdbe another continuously differentiable process. Then
XY0
0,T−YT=/integraldisplayT
0/parenleftig
∇xXYr
r,T/parenrightig/parenleftbigg
µ(r,Yr)−dYr
dt/parenrightbigg
dr.
We now give the proof of Theorem 1.
Proof of Theorem 1. DefineYx
s,tandZx
s,tas in Assumption 2. As Yx
s,t∈C(Rd×[0,1]2),Zx
0,t∈C1(Rd×[0,1])
andvθ(x,t)∈C0,1(Rd×[0,1])for any x∈Rd, the Alekseev–Gröbner formula implies that
Yx
0,t−Zx
0,t=/integraldisplayt
0/parenleftig
∇xYZs
s,t/parenrightig/parenleftbig
vθ(Zs,s)−vX(Zs,s)/parenrightbig
ds. (4)
We can write Yx
s,t=x+/integraltextt
svθ(Yx
s,r,r) dr, and so
∇xYx
s,t=I+/integraldisplayt
s∇xvθ(Yx
s,r,r)∇xYx
s,rdr.
It follows that
∂
∂t/vextenddouble/vextenddouble∇xYx
s,t/vextenddouble/vextenddouble
op≤/vextenddouble/vextenddouble/vextenddouble∂
∂t∇xYx
s,t/vextenddouble/vextenddouble/vextenddouble
op=/vextenddouble/vextenddouble∇xvθ(Yx
s,t,t)∇xYx
s,t/vextenddouble/vextenddouble
op≤Lt/vextenddouble/vextenddouble∇xYx
s,t/vextenddouble/vextenddouble
op,
where∥·∥opdenotes the operator norm with respect to the ℓ2metric on Rd. Therefore, by Grönwall’s lemma
we have/vextenddouble/vextenddouble∇xYx
s,t/vextenddouble/vextenddouble
op≤exp/braceleftbig/integraltextt
sLrdr/bracerightbig
≤K, whereK:= exp/braceleftbig/integraltext1
0Ltdt/bracerightbig
. Applied to (4) at t= 1, we get
∥Yx
1−Zx
1∥2≤/integraldisplay1
0/vextenddouble/vextenddouble/vextenddouble∇xYZs
s,1/vextenddouble/vextenddouble/vextenddouble
op/vextenddouble/vextenddoublevX(Zs,s)−vθ(Zs,s)/vextenddouble/vextenddouble
2ds≤K/integraldisplay1
0/vextenddouble/vextenddoublevX(Zs,s)−vθ(Zs,s)/vextenddouble/vextenddouble
2ds.
Letting x∼π0and taking expectations, we deduce
E/bracketleftbig
∥Y1−Z1∥2
2/bracketrightbig
≤K2E/bracketleftigg/parenleftbigg/integraldisplay1
0/vextenddouble/vextenddoublevX(Zt,t)−vθ(Zt,t)/vextenddouble/vextenddouble
2dt/parenrightbigg2/bracketrightigg
≤K2/integraldisplay1
0E/bracketleftig/vextenddouble/vextenddoublevθ(Xt,t)−vX(Xt,t)/vextenddouble/vextenddouble2/bracketrightig
dt.
SinceW2(ˆπ1,π1) =W2(Law(Y1),Law(Z1))≤E/bracketleftbig
∥Y1−Z1∥2
2/bracketrightbig1/2, the result follows from our assumption on
theL2error and the definition of K.
5Published in Transactions on Machine Learning Research (02/2024)
The application of the Alekseev–Gröbner formula in (4) is not symmetric in YandZ. Applying the formula
with the roles of YandZreversed would mean we require a bound on/vextenddouble/vextenddouble∇xYx
s,t/vextenddouble/vextenddouble
op, which we could bound
in terms of the Lipschitz constant of the true velocity field, leading to a more natural condition. However,
we would then also be required to control the velocity approximation error ∥vθ(Yt,t)−vX(Yt,t)∥2along the
paths ofY, which we have much less control over due to the nature of the training objective.
3.2 Smoothness of the velocity fields
As remarked in Section 3.1, the bound in Theorem 1 is exponentially dependent on the Lipschitz constant
ofvθ. In this section, we control the corresponding Lipschitz constant of vX. We prefer this to controlling
the Lipschitz constant of vθdirectly since this is determined by our choice of V, the class of functions over
which we optimise (3).
In this section, we will relax the constraints from Section 2 that γ0=γ1= 0andα0=β1= 1. We do this
because allowing γt>0for allt∈[0,1]means that we have some Gaussian smoothing at every t, which will
help ensure the resulting velocity fields are sufficiently regular. This will mean that instead of learning a flow
betweenπ0andπ1we will actually be learning a flow between ˜π0and˜π1, the distributions of α0X0+γ0Z
andβ1X1+γ1Zrespectively. However, if we centre X0andX1so that E[X0] =E[X1] = 0and letRbe
such that∥X0∥2,∥X1∥2≤R, thenW2(˜π0,π0)2≤(1−α0)2R2+dγ2
0and similarly for ˜π1,π1, so it follows
that by taking γ0,γ1sufficiently close to 0 and α0,β1sufficiently close to 1 we can make ˜π0,˜π1very close in
2-Wasserstein distance to π0,π1respectively. Note that if π0is easy to sample from then ˜π0is also easy to
sample from (we may simulate samples from ˜π0by drawing X0∼π0,Z∼N (0,Id)independently, setting
˜X0=α0X0+γ0Zand noting that ˜X0∼˜π0), so we can run the flow matching procedure starting from ˜π0.
For arbitrary choices of π0andπ1the expected velocity field vX(x,t)can be very badly behaved, and so we
will require an additional regularity assumption on the process X. This notion of regularity is somewhat non-
standard, but is the natural one emerging from our proofs and bears some similarity to quantities controlled
in stochastic localization (see discussion below).
Definition 1. For a real number λ≥1, we say an Rd-valued random variable Wisλ-regular if, whenever
we takeτ∈(0,∞)andξ∼N(0,τ2Id)independently of Wand setW′=W+ξ, for all x∈Rdwe have
/vextenddouble/vextenddoubleCovξ|W′=x(ξ)/vextenddouble/vextenddouble
op≤λτ2.
We say that a distribution on Rdisλ-regular if the associated random variable is λ-regular.
Assumption4 (Regularityofdatadistributions) .For someλ≥1,αtX0+βtX1isλ-regular for all t∈[0,1].
To understand what it means for a random variable Wto beλ-regular, note that before conditioning on W′,
we have∥Cov(ξ)∥op=τ2. We can think of conditioning on W′=xas re-weighting the distribution of ξ
proportionally to pW(x−ξ), wherepW(·)is the density function of W. So,Wisλ-regular if this re-weighting
causes the covariance of ξto increase by at most a factor of λfor any choice of τ.
Informally, if τis much smaller than the scale over which logpW(·)varies then this re-weighting should have
negligible effect, while for large τwe can write∥Covξ|W′=x(ξ)∥op=∥CovW|W′=x(W)∥opand we expect this
to be less than τ2onceτis much greater than the typical magnitude of W. Thusλ≈1should suffice for
sufficiently small or large τand the condition that Wisλ-regular controls how much conditioning on W′
can change the behavior of ξas we transition between these two extremes.
IfWis log-concave or alternatively Gaussian on a linear subspace of Rd, then we show in Appendix A.1 that
Wisλ-regular with λ= 1. Additionally, we also show that a mixture of Gaussians π=/summationtextK
i=1µiN(xi,σ2Id),
where the weights µisatisfy/summationtextK
i=1µi= 1,σ > 0, and∥xi∥ ≤Rfori= 1,...,K, isλ-regular with
λ= 1 + (R2/σ2). This shows that λ-regularity can hold even for highly multimodal distributions. More
generally, we show in Appendix A.2 that we always have ∥Covξ|W′=x(ξ)op∥≤O(dτ2)with high probability.
Then, we can interpret Definition 1 as insisting that this inequality always holds, where the dependence on
dis incorporated into the parameter λ. (We see that in the worst case λmay scale linearly with d, but we
expect in practice that λwill be approximately constant in many cases of interest.)
6Published in Transactions on Machine Learning Research (02/2024)
Controllingcovariancesofrandomvariablesgivennoisyobservationsofthosevariablesisalsoaproblemwhich
arises in stochastic localization (Eldan, 2013; 2020), and similar bounds to the ones we use here have been
established in this context. For example, Alaoui & Montanari (2022) show that E/bracketleftbig
Covξ|W′=x(W′)/bracketrightbig
≤τ2Id
in our notation. However, the bounds from stochastic localization only hold in expectation over xdistributed
according to the law of W′, whereas we require bounds that hold pointwise, or at least for xdistributed
according to the law of Yx
s,t, which is much harder to control.
We now aim to bound the Lipschitz constant of the true velocity field under Assumption 4. The first step is
to show that vXis differentiable and to get an explicit formula for its derivative. In the following sections,
we abbreviate the expectation and covariance conditioned on Xt=xtoEx[·]andCovx(·)respectively.
Lemma 1. IfXis the stochastic interpolant between π0andπ1, thenvX(x,t)is differentiable with respect
toxand
∇xvX(x,t) =˙γt
γtId−1
γtCovx(˙Xt,Z).
The proof of Lemma 1 is given in Appendix B.1. Using Lemma 1, we can derive the following theorem,
which provides a bound on the time integral of the Lipschitz constant of vθ.
Theorem 2. IfXis the stochastic interpolant between two distributions π0andπ1onRdwith bounded
support, then under Assumption 4 for each t∈(0,1)there is a constant L∗
tsuch thatvX(x,t)isL∗
t-Lipschitz
inxand/integraldisplay1
0L∗
tdt≤λ/parenleftbigg/integraldisplay1
0|˙γt|
γtdt/parenrightbigg
+λ1/2R/parenleftbigg/integraldisplay1
0|˙αt|
γtdt+/integraldisplay1
0|˙βt|
γtdt/parenrightbigg
,
where suppπ0,suppπ1⊆¯B(0,R), the closed ball of radius Raround the origin.
Proof.Using Lemma 1 plus the fact that ˙Xt= ˙αtX0+˙βtX1+ ˙γtZ, we can write
∇xvX(x,t) =˙γt
γt/parenleftig
Id−Covx(Z,Z)/parenrightig
−˙αt
γtCovx(X0,Z)−˙βt
γtCovx(X1,Z).
Since we can express Xt= (αtX0+βtX1) +γtZwhereαtX0+βtX1isλ-regular by Assumption 4 and
γtZ∼N(0,γ2
tId), we have∥Covx(γtZ)∥op≤λγ2
t. It follows that
∥Id−Covx(Z,Z)∥op≤max/parenleftig
1,∥Covx(Z,Z)∥op/parenrightig
≤λ.
Also, using Cauchy–Schwarz we have
∥Covx(X0,Z)∥op≤∥Covx(X0)∥1/2
op∥Covx(Z)∥1/2
op≤λ1/2R,
and a similar result holds for X1in place ofX0.
Putting this together, we get
/vextenddouble/vextenddouble∇xvX(x,t)/vextenddouble/vextenddouble
op≤|˙γt|
γt∥Id−Covx(Z,Z)∥op+|˙αt|
γt∥Covx(X0,Z)∥op+|˙βt|
γt∥Covx(X1,Z)∥op
≤λ|˙γt|
γt+λ1/2R/parenleftbigg|˙αt|
γt+|˙βt|
γt/parenrightbigg
. (5)
Finally, since vX(x,t)is differentiable with uniformly bounded derivative, it follows that vX(x,t)isL∗
t-
Lipschitz with L∗
t= supx∈Rd/vextenddouble/vextenddouble∇xvX(x,t)/vextenddouble/vextenddouble
op. Integrating (5) from t= 0tot= 1, the result follows.
We now provide some intuition for the bound in Theorem 2. The key term on the RHS is the first one,
which we typically expect to be on the order of log(γmax/γmin)whereγmaxandγminare the maximum and
minimum values taken by γton the interval [0,1]respectively. This is suggested by the following lemma.
Lemma 2. Suppose that γ0=γ1=γminandγtincreases smoothly from γminatt= 0toγmaxbefore
decreasing back to γminatt= 1. Then/integraltext1
0(|˙γt|/γt) dt= 2 log(γmax/γmin).
7Published in Transactions on Machine Learning Research (02/2024)
Proof.Note that we can write |˙γt|/γt=|d(logγt)/dt|, so/integraltext1
0(|˙γt|/γt) dtis simply the total variation of logγt
over the interval [0,1]. By the assumptions on γt, we see this total variation is equal to 2 log(γmax/γmin).
The first term on the RHS in Theorem 2 also explains the need to relax the boundary conditions, since
γ0= 0orγ1= 0would cause/integraltext1
0(|˙γt|/γt) dtto diverge.
We can ensure the second terms in Theorem 2 are relatively small through a suitable choice of αt,βtand
γt. Sinceαtandβtare continuously differentiable, |˙αt|and|˙βt|are bounded, so to control these terms
we should pick γtsuch thatR/integraltext1
0(1/γt) dtis small, while respecting the fact that we need γt≪1at
the boundaries. One sensible choice among many would be γt= 2R/radicalbig
(δ+t)(1 +δ−t)for someδ≪1,
where/integraltext1
0(|˙γt|/γt) dt≈2 log(1/√
δ)and/integraltext1
0(1/γt) dt≈2π. In this case, the bound of Theorem 2 implies/integraltext1
0L∗
tdt≤λlog(1/δ)+2πλ1/2supt∈[0,1](|˙αt|+|˙βt|), so ifδ≪1the dominant term in our bound is λlog(1/δ).
3.3 Bound on the Wasserstein error of flow matching
Now, we demonstrate how to combine the results of the previous two sections to get a bound on the error
of the flow matching procedure that applies in settings of practical interest. In Section 3.2, we saw that
we typically have/integraltext1
0L∗
tdt∼λlog(γmax/γmin). The combination of this logarithm and the exponential in
Theorem 1 should give us bounds on the 2-Wasserstein error which are polynomial in εandγmax,γmin.
The main idea is that in order to ensure that we may apply Theorem 1, we should optimise L(v)over a
class of functions Vwhich all satisfy Assumption 3. (Technically, we only need Assumption 3 to hold at the
minimum ofL(v), but since it is not possible to know what this minimum will be ex ante, it is easier in
practice to enforce that Assumption 3 holds for all v∈V.) Given distributions π0,π1satisfying Assumption
4 as well as specific choices of αt,βtandγtwe defineKt=λ(|˙γt|/γt) +λ1/2R{(|˙αt|/αt) + (|˙βt|/βt)}and let
Vbe the set of functions v:Rd×[0,1]→Rdwhich areKt-Lipschitz in xfor allt∈[0,1].
Theorem 3. Suppose that Assumptions 1-4 hold and that γtis concave on [0,1]. ThenvX∈Vand for any
vθ∈V, ifYis a flow starting in ˜π0with velocity field vθandˆπ1is the law of Y1,
W2(ˆπ1,˜π1)≤Cλ1/2ε/parenleftbiggγmax
γmin/parenrightbigg2λ
,
whereγmin= inft∈[0,1]γt,γmax= supt∈[0,1]γtandC= exp/braceleftbig
R{/integraltext1
0(|˙αt|/γt) dt+/integraltext1
0(|˙βt|/γt) dt}/bracerightbig
.
Proof.First, note that vXisKt-Lipschitz in xfor allt∈[0,1]by the proof of Theorem 2, so vX∈V. Then,
sinceγtis concave,/integraltext1
0(|˙γt|/γt) dt≤2 log(γmax/γmin)by Lemma 2. Thus,/integraltext1
0Ktdt≤2λlog(γmax/γmin) +
λ1/2logC. Hence for any vθ∈Vwe may apply Theorem 1 to get
W2(ˆπ1,˜π1)≤εexp/braceleftig/integraldisplay1
0Ktdt/bracerightig
≤Cλ1/2ε/parenleftbiggγmax
γmin/parenrightbigg2λ
,
where ˜π0,˜π1are replacing π0,π1since we have relaxed the boundary conditions.
Theorem 3 shows that if we optimise L(v)over the classV, we can bound the resulting distance between
the flow matching paths. We typically choose γtto scale with R, soCshould be thought of as a constant
that is Θ(1)and depends only on the smoothness of the interpolating paths. For a given distribution, λis
fixed and our bound becomes polynomial in εandγmax/γmin.
Finally, our choice of γmincontrols how close the distributions ˜π0,˜π1are toπ0,π1respectively. We can
combine this with our bound on W2(ˆπ1,˜π1)to get the following result.
Theorem 4. Suppose that Assumptions 1-4 hold, that γtis concave on [0,1], and thatα0=β1= 1and
γ0=γ1=γmin. Then, for any vθ∈V, ifYis a flow starting in ˜π0with velocity field vθandˆπ1is the law
ofY1,
W2(ˆπ1,π1)≤Cλ1/2ε/parenleftbiggγmax
γmin/parenrightbigg2λ
+√
dγmin
8Published in Transactions on Machine Learning Research (02/2024)
whereγmin,γmaxandCare as before.
Proof.We haveW2(ˆπ1,π1)≤W2(ˆπ1,˜π1) +W2(˜π1,π1). The first term is controlled by Theorem 3, while for
the second term we have W2(˜π1,π1)2≤(1−β1)R2+dγ2
0, as noted previously. Using β1= 1and combining
these two results completes the proof.
Optimizing the expression in Theorem 4 over γmin, we see that for a given L2error tolerance ε, we should
takeγmin∼d−1/(4λ+2)ε1/(2λ+1). We deduce the following corollary.
Corollary 1. In the setting of Theorem 4, if we take γmin∼d−1/(4λ+2)ε1/(2λ+1)then the total Wasserstein
error of the flow matching procedure is of order W2(ˆπ1,π1)≲d2λ/(4λ+2)ε1/(2λ+1).
4 Application to probability flow ODEs
For generative modelling applications, π1is the target distribution and π0is typically chosen to be a simple
reference distribution. A common practical choice for π0is a standard Gaussian distribution, and in this
setting the flow matching framework reduces to the probability flow ODE (PF-ODE) framework for diffusion
models (Song et al., 2021b). (Technically, this corresponds to running the PF-ODE framework for infinite
time. Finite time versions of the PF-ODE framework can be recovered by taking β0to be positive but small.)
Previously, this correspondence has been presented by taking γt= 0for allt∈[0,1]andπ0=N(0,Id)in
our notation from Section 2 (Liu et al., 2023). Instead, we choose to set αt= 0for allt∈[0,1]and have
γt>0, which recovers exactly the same framework, just with Zplaying the role of the reference random
variable rather than X0. We do this because it allows us to apply the results of Section 3 directly.
Because we have this alternative representation, we can strengthen our results in the PF-ODE setting. First,
note that Assumption 4 simplifies, so that we only need to assume that π1isλ-regular. We thus replace
Assumption 4 with Assumption 4’ for the rest of this section.
Assumption 4’ (Regularity of data distribution, Gaussian case) .For someλ≥1, the distribution π1is
λ-regular.
We also get the following alternative form of Lemma 1 in this setting, which is proved in Appendix B.2.
Lemma 3. IfXis the stochastic interpolant in the PF-ODE setting above, then vX(x,t)is differentiable
with respect to xand
∇xvX(x,t) =˙γt
γtId−/parenleftbigg˙γt
γt−˙βt
βt/parenrightbigg
Covx(Z).
Using Lemma 3, we can follow a similar argument to the proof of Theorem 2 to get additional control on
the termsL∗
t.
Theorem 5. IfXis the stochastic interpolant in the PF-ODE setting above, then under Assumption 4’ for
eacht∈(0,1)there is a constant L∗
tsuch thatvX(x,t)isL∗
t-Lipschitz in xand
/integraldisplay1
0L∗
tdt≤λ/parenleftbigg/integraldisplay1
0|˙γt|
γtdt/parenrightbigg
+/integraldisplay1
0min/braceleftbigg
λ|˙βt|
βt, λ1/2R|˙βt|
γt/bracerightbigg
dt.
Proof.From Theorem 2 we know that vX(x,t)is differentiable and Lipschitz with some constant L∗
t. From
(5) in the proof of Theorem 2, for any x∈Rdwe have
/vextenddouble/vextenddouble∇xvX(x,t)/vextenddouble/vextenddouble
op≤λ|˙γt|
γt+λ1/2R|˙βt|
γt, (6)
since ˙αt= 0in this setting. Also, using Lemma 3,
/vextenddouble/vextenddouble∇xvX(x,t)/vextenddouble/vextenddouble
op≤|˙γt|
γt∥Id−Covx(Z)∥op+|˙βt|
βt∥Covx(Z)∥op≤λ|˙γt|
γt+λ|˙βt|
βt. (7)
AsL∗
t= supx∈Rd/vextenddouble/vextenddouble∇xvX(x,t)/vextenddouble/vextenddouble
op, combining (6), (7) and integrating from t= 0tot= 1gives the result.
9Published in Transactions on Machine Learning Research (02/2024)
To interpret Theorem 5, recall that the boundary conditions we are operating under are γ0=β1= 1,
β0= 0andγ1≪1, so that ˜π0=N(0,Id)and˜π1isπ1plus a small amount of Gaussian noise at scale γ1.
The following corollary gives the implications of Theorem 5 in the standard variance-preserving (VP) and
variance-exploding (VE) PF-ODE settings of Song et al. (2021b).
Corollary 2. Suppose that we are in the setting of Theorem 5, so that vX(x,t)isL∗
t-Lipschitz in x. Then,
(i) ifγt=Rcos((π
2−δ)t)andβt= sin((π
2−δ)t)forδ≪1, corresponding to the VP ODE framework
of Song et al. (2021b), then/integraltext1
0L∗
tdt≤λ(1 + log(1/γ1));
(ii) ifβt= 1for allt∈[0,1]andγtis decreasing, corresponding to the VE ODE framework of Song
et al. (2021b), then/integraltext1
0L∗
tdt≤λlog(1/γ1).
Proof.In both cases, we apply Theorem 5 to bound/integraltext1
0L∗
t. Asγtis decreasing, we have/integraltext1
0|˙γt|/γtdt=
log(γ0/γ1), similarly to in the proof of Lemma 2. For (i), the second term on the RHS of Theorem 5 can be
bounded above by λ, while for (ii) it vanishes entirely.
In each case, we can plug the resulting bound into Theorem 1 to get a version of Theorem 3 for the PF-ODE
setting with the given noising schedule. We define Kt=λ(|˙γt|/γt) + min{λ(|˙βt|/βt),λ1/2R(|˙βt|/γt)}and let
Vbe the set of functions v:Rd×[0,1]→Rdwhich areKt-Lipschitz in xfor allt∈[0,1]as before.
Theorem 6. Suppose that Assumptions 1-3 and 4’ hold and we are in either (i) the VP ODE or (ii) the
VE ODE setting above. Then vX∈Vand for any vθ∈V, ifYis a flow starting in ˜π0with velocity field vθ
andˆπ1is the law of Y1, then
(i) in the VP ODE setting, we have W2(ˆπ1,˜π1)≤ε(e/γ1)λ;
(ii) in the VE ODE setting, we have W2(ˆπ1,˜π1)≤ε(1/γ1)λ.
Proof.ThatvX∈Vfollows analogously to the proof for Theorem 3. The results then follow from combining
Theorem 1 with the bounds in Corollary 2, as in the proof of Theorem 3.
5 Conclusion
We have provided the first bounds on the error of the general flow matching procedure that apply in the
case of completely deterministic sampling. Under the smoothness criterion of Assumption 4 on the data
distributions, we have derived bounds which for a given sufficiently smooth choice of αt,βt,γtand fixed
data distribution are polynomial in the level of Gaussian smoothing γmax/γminand theL2errorεof our
velocity approximation.
However, our bounds still depend exponentially on the parameter λfrom Assumption 4. It is therefore a
key question how λbehaves for typical distributions or as we scale up the dimension. In particular, the
informal argument we provide in Section A.2 suggests that λmay scale linearly with d. However, we expect
that in many practical cases λshould be Θ(1)even asdscales. Furthermore, even if this is not the case,
in the proof of Theorem 1 we only require control of the norm of ∇xvθ(x,t)when applied to ∇xYx
s,t. In
cases such as these, the operator norm bound is typically loose unless ∇xYx
s,tis highly correlated with the
largest eigenvectors of ∇xvθ(x,t). We see no a priori reason why these two should be highly correlated in
practice, and so we expect in most practical applications to get behaviour which is much better than linear
ind. Quantifying this behaviour remains a question of interest.
Finally, the fact that we get weaker results in the fully deterministic setting than Albergo et al. (2023) do
when adding a small amount of Gaussian noise in the reverse sampling procedure suggests that some level
of Gaussian smoothing is helpful for sampling and leads to the suppression of the exploding divergences of
paths.
10Published in Transactions on Machine Learning Research (02/2024)
Acknowledgements
We thank Michael Albergo, Nicholas Boffi and Eric Vanden-Eijnden for their comments on an early version
of this paper. Joe Benton was supported by the EPSRC through the StatML CDT (EP/S023151/1). Arnaud
Doucet acknowledges support from EPSRC grants EP/R034710/1 and EP/R018561/1.
References
Ahmed El Alaoui and Andrea Montanari. An Information-Theoretic View of Stochastic Localization. IEEE
Transactions on Information Theory , 68(11):7423–7426, 2022.
Michael S Albergo and Eric Vanden-Eijnden. Building Normalizing Flows with Stochastic Interpolants. In
International Conference on Learning Representations , 2023.
Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic Interpolants: A Unifying Frame-
work for Flows and Diffusions. arXiv preprint arXiv:2303.08797 , 2023.
Vladimir M Alekseev. An estimate for the perturbations of the solutions of ordinary differential equations.
Vestn. Mosk. Univ. Ser. I. Math. Mekh , 2:28–36, 1961.
Heli Ben-Hamu, Samuel Cohen, Joey Bose, Brandon Amos, Aditya Grover, Maximilian Nickel, Ricky T Q
Chen, and Yaron Lipman. Matching Normalizing Flows and Probability Paths on Manifolds. In Interna-
tional Conference on Machine Learning , 2022.
Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Nearly d-Linear Convergence
Bounds for Diffusion Models via Stochastic Localization. arXiv preprint arXiv:2308.03686 , 2023.
Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative Modeling with Denoising Auto-Encoders
and Langevin Sampling. arXiv preprint arXiv:2002.00107 , 2022.
Herm Jan Brascamp and Elliott H Lieb. On extensions of the Brunn-Minkowski and Prékopa-Leindler
theorems, includinginequalitiesforlogconcavefunctions, andwithanapplicationtothediffusionequation.
Journal of Functional Analysis , 22(4):366–389, 1976.
Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved Analysis of Score-based Generative Modeling:
User-Friendly Bounds under Minimal Smoothness Assumptions. In International Conference on Machine
Learning , 2023a.
Ricky T Q Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural Ordinary Differential
Equations. In Advances in Neural Information Processing Systems , 2018.
Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ODE
is provably fast. arXiv preprint arXiv:2305.11798 , 2023b.
SitanChen,SinhoChewi,JerryLi,YuanzhiLi,AdilSalim,andAnruRZhang. Samplingisaseasyaslearning
the score: theory for diffusion models with minimal data assumptions. In International Conference on
Learning Representations , 2023c.
Sitan Chen, Giannis Daras, and Alexandros G Dimakis. Restoration-Degradation Beyond Linear Diffusions:
A Non-Asymptotic Analysis For DDIM-Type Samplers. In International Conference on Machine Learning ,
2023d.
GiovanniConforti, AlainDurmus, andMartaGentiloniSilveri. Scorediffusionmodelswithoutearlystopping:
finite Fisher information is all you need. arXiv preprint arXiv:2308.12240 , 2023.
Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. Transactions
on Machine Learning Research , 2022.
11Published in Transactions on Machine Learning Research (02/2024)
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schrödinger Bridge with
Applications to Score-Based Generative Modeling. In Advances in Neural Information Processing Systems ,
2021.
Prafulla Dhariwal and Alex Nichol. Diffusion Models Beat GANs on Image Synthesis. In Advances in Neural
Information Processing Systems , 2021.
RonenEldan. ThinShellImpliesSpectralGapUptoPolylogviaaStochasticLocalizationScheme. Geometric
and Functional Analysis , 23(2):532–569, 2013.
Ronen Eldan. Taming correlations through entropy-efficient measure decompositions with applications to
mean-field approximation. Probability Theory and Related Fields , 176(3-4):737–755, 2020.
Will Grathwohl, Ricky T Q Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. FFJORD:
Free-form Continuous Dynamics for Scalable Reverse Generative Models. In International Conference on
Learning Representations , 2019.
Wolfgang Gröbner. Die Lie-Reihen und ihre Anwendungen . Berlin: VEB Deutscher Verlag der Wis-
senschaften, 1960.
Eric Heitz, Laurent Belcour, and Thomas Chambon. Iterative α-(de)Blending: a Minimalist Deterministic
Diffusion Model. In ACM SIGGRAPH Conference Prooceedings , 2023.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In Advances in Neural
Information Processing Systems , 2020.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial
complexity. In Advances in Neural Information Processing Systems , 2022.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data
distributions. In International Conference on Algorithmic Learning Theory , 2023.
GenLi, YutingWei, YuxinChen, andYuejieChi. TowardsFasterNon-AsymptoticConvergenceforDiffusion-
Based Generative Models. arXiv preprint arXiv:2306.09251 , 2023.
Yaron Lipman, Ricky T Q Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for
generative modeling. In International Conference on Learning Representations , 2023.
Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us Build Bridges: Understanding and Extending
Diffusion Generative Models. arXiv preprint arXiv:2208.14699 , 2022.
Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow Straight and Fast: Learning to Generate and Transfer
Data with Rectified Flow. In International Conference on Learning Representations , 2023.
Jakiw Pidstrigach. Score-Based Generative Models Detect Manifolds. In Advances in Neural Information
Processing Systems , 2022.
Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-TTS: A
Diffusion Probabilistic Model for Text-to-Speech. In International Conference on Machine Learning , 2021.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional
Image Generation with CLIP Latents. arXiv preprint arXiv:2204.06125 , 2022.
Danilo Jimenez Rezende and Shakir Mohamed. Variational Inference with Normalizing Flows. In Interna-
tional Conference on Machine Learning , 2015.
Noam Rozen, Aditya Grover, Maximilian Nickel, and Yaron Lipman. Moser Flow: Divergence-based Gener-
ative Modeling on Manifolds. In Advances in Neural Information Processing Systems , 2021.
12Published in Transactions on Machine Learning Research (02/2024)
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, et al. Pho-
torealistic Text-to-Image Diffusion Models with Deep Language Understanding. In Advances in Neural
Information Processing Systems , 2022.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. In International
Conference on Learning Representations , 2021a.
Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution. In
Advances in Neural Information Processing Systems , 2019.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-Based Generative Modeling through Stochastic Differential Equations. In International Conference
on Learning Representations , 2021b.
Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. Neural Computation ,
23(7):1661–1674, 2011.
Kaylee Yingxi Yang and Andre Wibisono. Convergence in KL and Rényi Divergence of the Unadjusted
Langevin Algorithm Using Estimated Score. In NeurIPS 2022 Workshop on Score-Based Methods , 2022.
Qinsheng Zhang and Yongxin Chen. Fast Sampling of Diffusion Models with Exponential Integrator. In
International Conference on Learning Representations , 2023.
13Published in Transactions on Machine Learning Research (02/2024)
A Exploration of Definition 1
A.1 Special cases of λ-regularity
First, we show that all log-concave random variables are λ-regular for λ= 1. The key ingredient in the proof
is the following result of Brascamp & Lieb (1976).
Proposition 3 (Brascamp-Lieb 1976) .Suppose that Wis anRd-valued random variable with density func-
tionpW(x) =e−φ(x), whereφis strictly convex on Rdand twice differentiable. Assume that D2φ≥µId
withµ>0andg∈C1(Rd). Then,
VarW(g(W))≤E/bracketleftbig
⟨(D2φ)−1∇g(W),∇g(W)⟩/bracketrightbig
≤1
µE/bracketleftbig
∥∇g(W)∥2/bracketrightbig
.
Lemma 4. Suppose that Wis anRd-valued log-concave random variable. Then Wisλ-regular for λ= 1.
Proof.Fixτ > 0and denote the density of WbypW(x) =e−φ(x)for some convex function φ. Take
ξ∼N(0,τ2Id)and setW′=W+ξ. Then, the density of ξconditional on W′is given by
pξ|W′(ξ|x′)∝e−∥ξ∥2/(2τ2)pW(x′−ξ) = exp/braceleftig
−∥ξ∥2
2τ2−φ(x′−ξ)/bracerightig
.
Letu∈Rdbe an arbitrary unit vector, and consider g(W) =uTW. Sinceφ′(ξ) =∥ξ∥2
2τ2+φ(x′−ξ)is strictly
convex inξ, and
D2φ′=τ−2Id+D2φ(x′−ξ)≥τ−2Id,
we can apply Theorem 3 to the random variable ξconditional on W′withµ=τ−2to get
Varξ|W′(uTξ)≤τ2E/bracketleftbig
∥u∥2
2/bracketrightbig
=τ2.
Then,/vextenddouble/vextenddoubleCovξ|W′(ξ)/vextenddouble/vextenddouble
op≤sup
∥u∥2=1Varξ|W′(uTξ)≤τ2.
Second, we show all random variables which are Gaussian on a linear subspace of Rdareλ-regular for λ= 1.
Lemma 5. Suppose that Wis anRd-valued random variable supported on some linear subspace S⊆Rdand
thatWrestricted toSis Gaussian with positive definite covariance matrix. Then Wisλ-regular for λ= 1.
Proof.We decompose ξinto two orthogonal components ξ⊥andξ∥, so thatξ=ξ⊥+ξ∥, andξ⊥is perpen-
dicular toSwhileξ∥is parallel toS. Then, we can write W′= (W+ξ∥) +ξ⊥. We denote W′
∥=W+ξ∥
and note that W′
∥∈S. From observing W′=xwe can deduce the values of both W′
∥andξ⊥. Therefore,
Covξ|W′=x(ξ) = Covξ|W′=x(ξ∥) = Covξ∥|W′
∥=x∥(ξ∥), where x∥denotes the projection of xontoS.
By restricting our attention to the subspace S, we may therefore reduce to the case where Wis Gaussian with
full support. The result then follows from Lemma 4, since Gaussians with full support are log-concave.
Third, we show that all random variables which are locally at least as smooth as a Gaussian of covariance
σ2Idand are bounded up to Gaussian tails of covariance σ2Idareλ-regular.
Lemma 6. Suppose that Wis anRd-valued random variable which can be decomposed as W=U+η, where
Uandηare independent random variables such that ∥U∥≤Rfor someR> 0andη∼N(0,σ2Id)for some
σ>0. ThenWisλ-regular for λ= 1 + (R2/σ2).
Proof.Fixτ >0, so thatW′=W+ξ=U+η+ξwhereη∼N(0,σ2Id)andξ∼N(0,τ2Id)andU,η,ξ
are all independent. By the law of total variance, we have
Covξ|W′(ξ) =E/bracketleftbig
Covξ|W′,U(ξ)|W′/bracketrightbig
+ Cov( E[ξ|W′,U]|W′).
14Published in Transactions on Machine Learning Research (02/2024)
The distribution of ξ|W′,Uis the same as the distribution of ξ|(η+ξ), so it suffices to understand the
latter. To this end, we define ρ=η+ξandω=σ2ξ−τ2η. It is straightforward to check that ρandωare
independent centered Gaussians with covariances (σ2+τ2)Idandσ2τ2(σ2+τ2)Idrespectively. Then, we
can writeξ=1
σ2+τ2(τ2ρ+ω), from which it follows that
Covξ|W′,U(ξ) =/parenleftig
1
σ2+τ2/parenrightig2
Cov(τ2ρ+ω|ρ) =/parenleftig
σ2τ2
σ2+τ2/parenrightig
Id.
Therefore, E/bracketleftbig
Covξ|W′,U(ξ)|W′/bracketrightbig
=/parenleftig
σ2τ2
σ2+τ2/parenrightig
Id. In addition, we have
E[ξ|W′,U] =/parenleftig
1
σ2+τ2/parenrightig
E/bracketleftbig
τ2ρ+ω|ρ/bracketrightbig
=/parenleftig
τ2
σ2+τ2/parenrightig
ρ,
soCov(E[ξ|W′,U]|W′) =/parenleftig
τ2
σ2+τ2/parenrightig2
Covη,ξ|W′(η+ξ). Finally, we have
/vextenddouble/vextenddoubleCovη,ξ|W′(η+ξ)/vextenddouble/vextenddouble
op=/vextenddouble/vextenddoubleCovU|W′(W′−U)/vextenddouble/vextenddouble
op=/vextenddouble/vextenddoubleCovU|W′(U)/vextenddouble/vextenddouble
op≤R2.
Putting this all together, we see that
/vextenddouble/vextenddoubleCovξ|W′(ξ)/vextenddouble/vextenddouble
op≤/vextenddouble/vextenddoubleE/bracketleftbig
Covξ|W′,U(ξ)|W′/bracketrightbig/vextenddouble/vextenddouble
op+∥Cov(E[ξ|W′,U]|W′)∥op
≤σ2τ2
σ2+τ2+R2/parenleftbiggτ2
σ2+τ2/parenrightbigg2
≤λτ2
forλ= 1 + (R2/σ2), as required.
We note in particular that Lemma 6 can be applied in the case where our target distribution is a bounded
mixture of Gaussian components with covariance σ2Idfor someσ>0. We obtain the following corollary.
Corollary 3. Suppose that π=/summationtextK
i=1µiN(xi,σ2Id)is a mixture of Gaussian components of covariance
σ2Idfor someσ >0, where the weights µisatisfy/summationtextK
i=1µi= 1and we have∥xi∥≤Rfor alli= 1,...,K.
Thenπisλ-regular for λ= 1 + (R2/σ2).
Proof.Thisfollowsimmediatelyfromthefactthat πcanberepresentedintheformrequiredtoapplyLemma
6, by taking Uto have distribution/summationtextK
i=1µiδxi.
A.2 High-probability bounds
Lemma 7. Suppose that Wis anRd-valued random variable. For any τ >0, if we take ξ∼N(0,τ2Id)and
setW′=W+ξ, then we have/vextenddouble/vextenddoubleCovξ|W′(ξ)/vextenddouble/vextenddouble
op≤2dc2τ2
with probability at least 1−6de−c2/2for anyc≥1.
Proof.First, we have
/vextenddouble/vextenddoubleCovξ|W′(ξ)/vextenddouble/vextenddouble
op≤sup
∥u∥2=1Varξ|W′(uTξ)≤sup
∥u∥2=1Eξ|W′/bracketleftbig
(uTξ)2/bracketrightbig
≤E/bracketleftbig
∥ξ∥2
2|W′/bracketrightbig
.
Next, we bound the quantity E/bracketleftbig
∥ξ∥2
2|W′/bracketrightbig
using Markov’s inequality. If ∥ξ∥2
2>dc2τ2, then we must have
ξ2
i>c2τ2for someibetween 1andd. Therefore,
E/bracketleftig
∥ξ∥2
2 1∥ξ∥2
2>dc2τ2/bracketrightig
≤dE/bracketleftig
∥ξ∥2
2 1ξ2
1>c2τ2/bracketrightig
=dE/bracketleftig
ξ2
1 1ξ2
1>c2τ2/bracketrightig
+dd/summationdisplay
i=2E/bracketleftig
ξ2
i 1ξ2
1>c2τ2/bracketrightig
=dE/bracketleftig
ξ2
1 1ξ2
1>c2τ2/bracketrightig
+d(d−1)τ2P/parenleftbig
ξ2
1>c2τ2/parenrightbig
.
15Published in Transactions on Machine Learning Research (02/2024)
A standard Chernoff bound gives P/parenleftbig
ξ2
1>c2τ2/parenrightbig
≤2e−c2/2, and
E/bracketleftig
ξ2
1 1ξ2
1>c2τ2/bracketrightig
= 2/integraldisplay∞
cτz2
√
2πτ2e−z2/(2τ2)dz
= 2τ2/integraldisplay∞
cz2
√
2πe−z2/2dz
= 2τ2/bracketleftbigg
−z√
2πe−z2/2/bracketrightbigg∞
c+ 2τ2/integraldisplay∞
c1√
2πe−z2/2dz
=2τ2c√
2πe−c2/2+ 2τ2P(ξ1>cτ)
≤2τ2e−c2/2/parenleftbiggc√
2π+ 1/parenrightbigg
≤4cτ2e−c2/2.
Hence
E/bracketleftig
∥ξ∥2
2 1∥ξ∥2
2>dc2τ2/bracketrightig
≤4dcτ2e−c2/2+ 2d2τ2e−c2/2
≤6d2cτ2e−c2/2.
It follows by Markov’s inequality that
P/parenleftig
E/bracketleftig
∥ξ∥2
2 1∥ξ∥2
2>dc2τ2|W′/bracketrightig
≥dc2τ2/parenrightig
≤E/bracketleftig
∥ξ∥2
2 1∥ξ∥2
2>dc2τ2/bracketrightig
dc2τ2
≤6de−c2/2.
Finally, we can write
E/bracketleftbig
∥ξ∥2
2|W′/bracketrightbig
≤E/bracketleftig
∥ξ∥2
2 1∥ξ∥2
2>dc2τ2|W′/bracketrightig
+dc2τ2,
and so/vextenddouble/vextenddoubleCovξ|W′(ξ)/vextenddouble/vextenddouble
op≤E/bracketleftbig
∥ξ∥2
2|W′/bracketrightbig
≤2dc2τ2with probability at least 1−6de−c2/2, as required.
B Derivation of formulae for gradients of velocity fields
B.1 Proof of Lemma 1
In order to prove Lemma 1, we use the following intermediate result.
Lemma 8. IfXis the stochastic interpolant between π0andπ1, then
∇xE/bracketleftbig
X0|Xt=x/bracketrightbig
=−1
γtCovx(X0,Z).
Moreover, a similar expression holds for X1in place of X0.
Proof.First, note that Xt|X0,X1∼N(αtX0+βtX1,γ2
tId), so
∇xlog(pXt|X0,X1(x|x0,x1)) =−1
γ2
t(x−αtx0−βtx1)T.
Therefore,
∇xlog(pXt(x)) =1
pXt(x)·∇x/parenleftbigg/integraldisplay
Rd/integraldisplay
RdpXt|X0,X1(x|x0,x1)pX0,X1(x0,x1) dx0dx1/parenrightbigg
=1
pXt(x)/integraldisplay
Rd/integraldisplay
RdpXt|X0,X1(x|x0,x1)pX0,X1(x0,x1)∇xlog(pXt|X0,X1(x|x0,x1)) dx0dx1
=−1
γ2
tE/bracketleftbig
(Xt−αtX0−βtX1)T|Xt=x/bracketrightbig
16Published in Transactions on Machine Learning Research (02/2024)
We can then calculate
∇xE/bracketleftbig
X0|Xt=x/bracketrightbig
=∇x/parenleftbigg1
pXt(x)/integraldisplay
Rd/integraldisplay
Rdx0pXt|X0,X1(x|x0,x1)pX0,X1(x0,x1) dx0dx1/parenrightbigg
=1
pXt(x)/integraldisplay
Rd/integraldisplay
Rdx0pXt|X0,X1(x|x0,x1)pX0,X1(x0,x1)∇x/parenleftbig
logpXt|X0,X1(x|x0,x1)/parenrightbig
dx0dx1
−/parenleftbig
∇xlogpXt(x)/parenrightbig/parenleftbigg1
pXt(x)/integraldisplay
Rd/integraldisplay
Rdx0pXt|X0,X1(x|x0,x1)pX0,X1(x0,x1) dx0dx1/parenrightbigg
=−1
γ2
tEx/bracketleftbig
X0(Xt−αtX0−βtX1)T/bracketrightbig
+1
γ2
tEx/bracketleftbig
(Xt−αtX0−βtX1)T/bracketrightbig
Ex/bracketleftbig
X0/bracketrightbig
=−1
γtCovx(X0,Z).
Lemma 1. IfXis the stochastic interpolant between π0andπ1, thenvX(x,t)is differentiable with respect
toxand
∇xvX(x,t) =˙γt
γtId−1
γtCovx(˙Xt,Z).
Proof.SinceXt=αtX0+βtX1+γtZand ˙Xt= ˙αtX0+˙βtX1+ ˙γtZ, we can write
E/bracketleftbig˙Xt|Xt=x,X0=x0,X1=x1/bracketrightbig
= ˙αtx0+˙βtx1+˙γt
γt(x−αtx0−βtx1)
and therefore
E/bracketleftbig˙Xt|Xt=x/bracketrightbig
=˙γt
γtx+( ˙αtγt−˙γtαt)
γt·E/bracketleftbig
X0|Xt=x/bracketrightbig
+(˙βtγt−˙γtβt)
γt·E/bracketleftbig
X1|Xt=x/bracketrightbig
.
Taking gradients with respect to xand applying Lemma 8,
∇xvX(x,t) =˙γt
γtId−( ˙αtγt−˙γtαt)
γ2
t·Covx(X0,Z)−(˙βtγt−˙γtβt)
γ2
t·Covx(X1,Z)
=˙γt
γtId+˙γt
γ2
tCovx(Xt−γtZ,Z)−1
γtCovx( ˙αtX0+˙βtX1,Z)
=˙γt
γtId−1
γtCovx(˙Xt,Z).
B.2 Proof of Lemma 3
Lemma 3. IfXis the stochastic interpolant in the PF-ODE setting above, then vX(x,t)is differentiable
with respect to xand
∇xvX(x,t) =˙γt
γtId−/parenleftbigg˙γt
γt−˙βt
βt/parenrightbigg
Covx(Z).
Proof.From Lemma 1, we have
∇xvX(x,t) =˙γt
γtId−1
γtCovx(˙Xt,Z).
Then, since αt= 0we haveXt=βtX1+γtZand ˙Xt=˙βtX1+ ˙γtZ, so we can write
∇xvX(x,t) =˙γt
γtId−1
γtCovx/parenleftbigg˙βt
βt(Xt−γtZ) + ˙γtZ,Z/parenrightbigg
=˙γt
γtId−/parenleftbigg˙γt
γt−˙βt
βt/parenrightbigg
Covx(Z),
noting that we may discard the Xtterm since we are conditioning on Xt=x.
17