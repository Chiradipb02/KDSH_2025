Under review as submission to TMLR
MANDERA: Malicious Node Detection in Federated Learn-
ing via Ranking
Anonymous authors
Paper under double-blind review
Abstract
Byzantine attacks aim to hinder the deployment of federated learning algorithms by sending
malicious gradients to degrade the model. Although the benign gradients and Byzantine
gradients are distributed diﬀerently, identifying the malicious gradients is challenging due to
(1) the gradient is high-dimensional and each dimension has its unique distribution, and (2)
the benign gradients and the malicious gradients are mixed (two-sample test methods cannot
apply directly). To address these issues, we propose MANDERA which is theoretically
guaranteed to eﬃciently detect all malicious gradients under Byzantine attacks with no
prior knowledge or history about the number of attacked nodes. More speciﬁcally, we
proposed to transfer the original updating gradient space into a ranking matrix. By such an
operation, the scales of diﬀerent dimensions of the gradients in the ranking space become
identical. Then the high-dimensional benign gradients and the malicious gradients can be
easily separated in the ranking space. The eﬀectiveness of MANDERA is further conﬁrmed
by experimentation on fourByzantine attack implementations (Gaussian, Zero Gradient,
Sign Flipping, Shifted Mean), compared with state-of-the-art defences. The experiments
cover both IID and Non-IID datasets.
1 Introduction
Federated Learning (FL) is a decentralized learning framework that allows multiple participating nodes to
learn on a local collection of training data. The updating gradient values of each respective node are sent
to a global coordinator for aggregation. The global model collectively learns from each of these individual
nodes by aggregating the gradient updates before relaying the updated global model back to the participating
nodes. The aggregation of multiple nodes allows the model to learn from a larger dataset which will result in
a model having greater performance than the ones only learning on their local subset of data. FL presents
two key advantages: (1) the increase of privacy for the contributing node as local data is not communicating
with the global coordinator, and (2) a reduction in computation by the global node as the computation is
oﬄoaded to contributing nodes.
However, FL is vulnerable to various attacks, including data poisoning attacks (Tolpegin et al., 2020) and
Byzantine attacks (Lamport et al., 2019). The presence of malicious actors in the collaborative process may
seek to poison the performance of the global model, to reduce the output performance of the model (Chen
et al., 2017; Baruch et al., 2019; Fang et al., 2020; Tolpegin et al., 2020), or to embed hidden back-doors
within the model (Bagdasaryan et al., 2020). Byzantine attack aims to devastate the performance of the
global model by manipulating the gradient values. These gradient values that have been manipulated are
sent from malicious nodes which are unknown to the global node. The Byzantine attacks can result in a
global model which produces an undesirable outcome (Lamport et al., 2019).
Researchers seek to defend FL from the negative impacts of these attacks. This can be done by either
identifying the malicious nodes or making the global model more robust to these types of attacks. In our
paper, we focus on identifying the malicious nodes to exclude the nodes which are deemed to be malicious in
the aggregation step to mitigate the impact of malicious nodes. Most of the existing methods rely on the
gradient values to determine whether a node is malicious or not, for example, Blanchard et al. (2017); Yin
1Under review as submission to TMLR
et al. (2018); Guerraoui et al. (2018); Li et al. (2020); Fang et al. (2020); Cao et al. (2020); Wu et al. (2020b);
Xie et al. (2019; 2020); Cao et al. (2021) and So et al. (2021). All the above methods are eﬀective in certain
scenarios.
10 20 30Gradient Ranking
0 20 40 600 20 40 600 20 40 600.498400.498450.49850
102030
MeanSDNode.type
Benign
Malicious
Figure 1: Patterns of nodes in gradient space and ranking space respectively under mean shift attacks. The
columns of the ﬁgure represent the number of malicious nodes among 100 nodes: 10, 20 and 30.
There is a lack of theoretical guarantee to detect all the malicious nodes in the literature. Although the extreme
malicious gradients can be excluded by the above approaches, some malicious nodes could be mis-classiﬁed
as benign nodes and vice versa. The challenging issues in the community are caused by the following two
phenomena: [F1] the gradient values of benign nodes and malicious nodes are often non-distinguishable; [F2]
the gradient matrix is always high-dimensional (large column numbers) and each dimension follows its unique
distribution. The phenomenon [F1] indicates that it is not reliable to detect malicious nodes only using a
single column from the gradient matrix. And the phenomenon [F2] hinders us from using all the columns of
the gradient matrix, because it requires a scientiﬁc way to accommodate a large number of columns which
are distributed considerably diﬀerently.
In this paper, we propose to resolve these critical challenges from a novel perspective. Instead of working on the
node updates directly, we propose to extract information about malicious nodes indirectly by transforming the
node updates from numeric gradient values to the ranking space. Compared to the original numeric gradient
values, whose distribution is diﬃcult to model, the rankings are much easier to handle both theoretically
and practically. Moreover, as rankings are scale-free, we no longer need to worry about the scale diﬀerence
across diﬀerent dimensions. We proved under mild conditions that the ﬁrst two moments of the transformed
ranking vectors carry key information to detect the malicious nodes under Byzantine attacks. Based on these
theoretical results, a highly eﬃcient method called MANDERA is proposed to separate the malicious nodes
from the benign ones by clustering all local nodes into two groups based on the ranking vectors. Figure 1
shows an illustrative motivation for our method. It demonstrates the behaviors of malicious and benign nodes
under mean shift attacks. Obviously, the malicious and benign nodes are not distinguishable in the gradient
space due to the challenges we mentioned above, while they are well separated in the ranking space.
The contributions of this work are as follows: (1)we propose the ﬁrst algorithm leveraging the ranking
space of model updates to detect malicious nodes (Figure 2); (2)we provide a theoretical guarantee for the
detection of malicious nodes based on the ranking space under Byzantine attacks; (3)our method does not
assume knowledge of the number of malicious nodes, which is required in the learning process of most of
the prior methods; (4)we experimentally demonstrate the eﬀectiveness and robustness of our defense on
Byzantine attacks, including Gaussian attack (GA), Sign Flipping attack (SF), Zero Gradient attack (ZG)
and Mean Shift attack (MF); (5)an experimental comparison between MANDERA and a collection of robust
aggregation techniques is provided.
Related works. In the literature, there have been a collection of eﬀorts along the research on defensing
Byzantine attacks. Blanchard et al. (2017) propose a defense referred to as Krum that treats local nodes
whose update vector is too far away from the aggregated barycenter as malicious nodes and precludes
them from the downstream aggregation. Guerraoui et al. (2018) propose Bulyan, a process that performs
aggregation on subsets of node updates (by iteratively leaving each node out) to ﬁnd a set of nodes with the
most aligned updates given an aggregation rule. Cao et al. (2020) maintains a trusted model and dataset on
2Under review as submission to TMLR
Global modelMANDERARejectMalicious Update0.03, 0.12, 0.06, 0.20, 0.90,  Rank 𝑴column-wise𝑴:,𝟏			𝑴:,𝟐𝑴:,𝟑			𝑴:,𝟒		…			𝑴:,𝒑0.72, 0.90, 0.69, 0.70, 0.12,  0.48, 0.42, 0.43, 0.50, 0.81,  0.18, 0.20, 0.16, 0.30, 0.00,  MeanSDFind and reject malicious cluster
Aggregate and updateglobal modelwith benign clusterCompute MeanandSDof𝑹row-wise0.90 0.21 0.22 0.31 0.10  …5, 3, 4, 2, 1,  2, 1, 4, 3, 5,  3, 5, 4, 2, 1,  2, 4, 3, 1, 5,  1 4 3 2 5  …𝑹:,𝟏					𝑹:,𝟐				𝑹:,𝟑				𝑹:,𝟒		…				𝑹:,𝒑
Figure 2: An overview of MANDERA.
which submitted node updates may be bootstrapped by weighting each node’s update in the aggregation step
based on it’s cosine similarity to the trusted update. Xie et al. (2019) compute a Stochastic Descendant Score
(SDS) based on the estimated descendant of the loss function and the magnitude of the update submitted to
the global node, and only include a predeﬁned number of nodes with the highest SDS in the aggregation.
On the other hand, Chen et al. (2021) propose a zero-knowledge approach to detect and remove malicious
nodes by solving a weighted clustering problem. The resulting clusters update the model individually and
accuracy against a validation set is checked. All nodes in a cluster with signiﬁcant negative accuracy impact
are rejected and removed from the aggregation step.
2 Defense against Byzantine attacks via Ranking
In this section, notations are ﬁrst introduced and an algorithm to detect malicious nodes is proposed.
2.1 Notations
Suppose there are nlocal nodes in the federated learning framework, where n1nodes are benign nodes whose
indices are denoted by Iband the other n0=n−n1nodes are malicious nodes whose indices are denoted
byIm. The training model is denoted by f(θ,D), whereθ∈Rp×1is ap-dimensional parameter vector
andDis a data matrix. Denote the message matrix received by the central server from all local nodes as
M∈Rn×p, whereMi,:denotes the message received from node i. For a benign node i, letDibe the data
matrix on it with Nias the sample size, we have Mi,:=∂f(θ,Di)
∂θ|θ=θ∗, whereθ∗is the parameter value from
the global model. In the rest of the paper, we suppress∂f(θ,Di)
∂θ|θ=θ∗to∂f(θ,Di)
∂θto denote the gradient
value for simplicity purpose. A malicious node j∈Im, however, tends to attack the learning system by
manipulating Mj,:in some way. Hereinafter, we denote N∗=min({Ni}i∈Ib)to be the minimal sample size
of the benign nodes.
Given a vector of real numbers a∈Rn×1, deﬁne its ranking vector as b=Rank (a)∈perm{1,···,n}, where
the ranking operator Rankmaps the vector ato an element in permutation space perm{1,···,n}which is
the set of all the permutations of {1,···,n}. For example, Rank (1.1,−2,3.2) = (2,3,1), it ranks the values
from largest to smallest. We adopt average ranking, when there are ties. With the Rankoperator, we can
transfer the message matrix Mto a ranking matrix Rby replacing its column M:,jby the corresponding
ranking vector R:,j=Rank (M:,j). Further, deﬁne
ei,1
pp/summationdisplay
j=1Ri,jandvi,1
pp/summationdisplay
j=1(Ri,j−ei)2
to be the mean and variance of Ri,:, respectively. As it is shown in later subsections, we can judge whether
nodeiis a malicious node based on (ei,vi)under various attack types. In the following, we will highlight the
behavior of the benign nodes ﬁrst, and then discuss the behavior of malicious nodes and their diﬀerence with
the benign nodes under Byzantine attacks.
2.2 Behaviors of nodes under Byzantine attacks
Byzantine attacks aim to devastate the global model by manipulating the gradient values of some local
nodes. For a general Byzantine attack, we assume that the gradient vectors of benign nodes and malicious
3Under review as submission to TMLR
5 10 15 20 25 30GA ZG SF MS
40455055 40455055 40455055 40455055 40455055 40455055203040
2530354045
25303540
102030
eisiNode.type
Benign
Malicious
Figure 3: The scatter plots of (ei,si)for the 100 nodes under four types of attack as illustrative examples
demonstrating ranking mean and standard deviation from the 1st epoch of training for the FASHION-MNIST
dataset. Four attacks are Gaussian Attack (GA), Zero Gradient attack (ZG), Sign Flipping attack (SF) and
Mean shift attack (MS).
nodes follow two diﬀerent distributions GandF. We would expect systematical diﬀerences in their behavior
patterns in the ranking matrix R, based on which malicious node detection can be achieved. Theorem 2.1
demonstrates the concrete behaviors of benign nodes and malicious nodes under general Byzantine attacks.
Theorem 2.1 (Behavior under Byzantine attacks) .For a general Byzantine attack, assume that the gradient
values from benign nodes and malicious nodes follow two distributions G(·)andF(·)respectively (both Gand
Farep-dimensional). We have
lim
N∗→∞lim
p→∞ei= ¯µb·I(i∈Ib) + ¯µm·I(i∈Im)a.s.,
lim
N∗→∞lim
p→∞vi= ¯s2
b·I(i∈Ib) + ¯s2
m·I(i∈Im)a.s.,
where (¯µb,¯s2
b)and(¯µm,¯s2
m)are highly non-linearly functions of G(·)andF(·)whose concrete form is detailed
in the Appendix A, and “a.s.” is the abbreviation of “almost surely”.
The proof can be found in the Appendix A. If the attackers can access the exact distribution G, which is very
rare, an obvious strategy to evade defense is to let F=G. In this case, the attack will have no impact on the
global model. More often, the attackers have little information about distribution G. In this case, it is a rare
event for the attackers to design a distribution Fsatisfying (¯µb,¯s2
b) = ( ¯µm,¯s2
m)for the malicious nodes to
follow. In fact, most popular Byzantine attacks never try to make such an eﬀort at all. Thus, the malicious
nodes and the benign nodes are distinguishable with respect to their feature vectors {(ei,vi)}1≤i≤n, because
(ei,vi)reaches to diﬀerent limits for begin and malicious nodes. Considering that the standard deviation
si=√viis typically of the similar scale of ei, hereinafter we employ (ei,si), instead of (ei,vi), as the feature
vector of node ifor malicious node detection.
Figure 3 illustrates the typical scatter plots of (ei,si)for benign and malicious nodes under four typical
Byzantine attacks, i.e., GA, SN, ZG and MS. It can be observed that malicious nodes and benign nodes are
all well separated in these scatter plots, indicating a proper clustering algorithm will distinguish these two
groups. We note that both siandeiare informative for malicious node detection, since in some cases (e.g.,
under Gaussian attacks) it is diﬃcult to distinguish malicious nodes from benign ones based on eionly.
2.3 Algorithm for Malicious node detection under Byzantine attacks
Theorem 2.1 implies that, under general Byzantine attacks, the feature vector (ei,si)of nodeiconverges to
two diﬀerent limits for benign and malicious nodes, respectively. Thus, for a real dataset where Ni’s andp
are all ﬁnite but reasonably large numbers, the scatter plot of {(ei,si)}1≤i≤nwould demonstrate a clustering
structure: one cluster for the benign nodes and the other cluster for the malicious nodes.
4Under review as submission to TMLR
Algorithm 1 MANDERA
Input:The message matrix M.
1:Convert the message matrix Mto the ranking matrix Rby applying Rankoperator.
2:Compute mean and standard deviation of rows in R, i.e.,{(ei,si)}1≤i≤n.
3:Run the clustering algorithm K-means to{(ei,si)}1≤i≤nwithK= 2, and predict the set of benign nodes
with the larger cluster denoted by ˆIb.
Output: The predicted benign node set ˆIb.
Based on this intuition, we propose MAlicious Node DEtection via RAnking (MANDERA) to detect the
malicious nodes, whose workﬂow is detailed in Algorithm 1. MANDERA can be applied to either a single
epoch or multiple epochs. For a single-epoch mode, the input data Mis the message matrix received from a
single epoch. For multiple-epoch mode, the data Mis the column-concatenation of the message matrices
from multiple epochs. By default, the experiments below all use a single epoch to detect the malicious nodes.
The predicted benign nodes ˆIbobtained by MANDERA naturally leads to an aggregated message ˆmb,:=
1
#(ˆIb)/summationtext
i∈ˆIbMi,:. Theorem 2.2 shows that ˆIband ˆmblead to consistent estimations of Ibandmb=
1
n1/summationtext
i∈IbMi,:respectively, indicating that MANDERA enjoys robustness guarantee Steinhardt (2018) for
Byzantine attacks.
Theorem 2.2 (Robustness guarantee) .Under Byzantine attacks, we have:
lim
N∗,p→∞P(ˆIb=Ib) = 1,lim
N∗,p→∞E||ˆmb,:−mb,:||2= 0.
The proof of Theorem 2.2 can be found in Appendix B. As E(ˆmb,:) =mb,:, MANDERA obviously satisﬁes
the(α,f)-Byzantine Resilience condition, which is used in Blanchard et al. (2017) and Guerraoui et al. (2018)
to measure the robustness of their estimators.
3 Theoretical analysis for speciﬁc Byzantine attacks
Theorem 2.1 provides us general guidance about the behavior of nodes under Byzantine attacks. In this
section, we examine the behavior for speciﬁc attacks, including Gaussian attacks, zero gradient attacks, sign
ﬂipping attacks and mean shift attacks.
As the behavior of benign nodes does not depend on the type of Byzantine attack, we can study the statistical
properties of (ei,vi)for a benign node i∈Ibbefore the speciﬁcation of a concrete attack type. For any
benign node i, the message generated for jthparameter isMi,j=1
Ni/summationtextNi
l=1∂f(θ,Di,l)
∂θj, whereDi,ldenotes the
lthsample on it. Throughout this paper, we assume that Di,l’s are independent and identically distributed
(IID) samples drawn from a data distribution D.
Lemma 3.1. Under the IID data assumption, further denote µj=E/parenleftBig
∂f(θ,Di,l)
∂θj/parenrightBig
andσ2
j=Var/parenleftBig
∂f(θ,Di,l)
∂θj/parenrightBig
<
∞, withNigoing to inﬁnity, for ∀j∈{1,···,p}, we haveMi,j→µjalmost surely (a.s.) and Mi,j→d
N/parenleftbig
µj,σ2
j/Ni/parenrightbig
.
Lemma 3.1 can be proved by using the Kolmogorov’s Strong Law of Large Numbers (KSLLN) and Central
Limit Theorem. For the rest of this section, we will derive the detailed forms of ¯µb,¯µm,¯s2
band¯s2
m, as deﬁned
in Theorem 2.1, under four speciﬁc Byzantine attacks.
3.1 Gaussian attack
Deﬁnition 3.2 (Gaussian attack) .In a Gaussian attack, the attacker generates malicious gradient values
as follows:{Mi,:}i∈Im∼MVN (mb,:,Σ), wheremb,:=1
n1/summationtext
i∈IbMi,:is the mean vector of Gaussian
distribution and Σis the covariance matrix determined by the attacker.
5Under review as submission to TMLR
0200040006000
0.00 0.25 0.50 0.75 1.00
p.valuecount
Figure 4: Independence test for 100,000 column pairs randomly chosen from message matrix Mgenerated
from FASHION-MNIST data.
Considering that Mi,j→µja.s. withNigoing to inﬁnity for all i∈Ibbased on Deﬁnition 3.2, it is
straightforward to see that limN∗→∞mb,j=µja.s.,and the distribution of Mi,jfor eachi∈Imconverges
to the Gaussian distribution centered at µj. Based on this fact, the limiting behavior of the feature vector
(ei,vi)can be established for both benign and malicious nodes. Theorem 3.3 summarizes the results, with
the proof detailed in Appendix C.
Theorem 3.3 (Behavior under Gaussian attacks) .Assuming{R:,j}1≤j≤pare independent of each other,
under the Gaussian attack, the behaviors of benign and malicious nodes are as follows:
¯µb= ¯µm=n+ 1
2,¯s2
b=1
pp/summationdisplay
j=1s2
b,j,¯s2
m=1
pp/summationdisplay
j=1s2
m,j,
wheres2
b,jands2
m,jare both complex functions of n0,n1,σ2
j,Σj,jandN∗whose concrete form is detailed in
the Appendix C.
Considering that ¯s2
b=¯s2
mif and only if Σj,j’s fall into a lower dimensional manifold whose measurement is
zero under the Lebesgue measure, we have P(¯s2
b=¯s2
m) = 0if the attacker speciﬁes the Gaussian variance
Σj,j’s arbitrarily in the Gaussian attack. Thus, Theorem 3.3 in fact suggests that the benign nodes and the
malicious nodes are diﬀerent on the value of vi, and therefore provides a guideline to detect the malicious
nodes. Although the we do need N∗andpto go to inﬁnity for getting the theoretical results in Theorem 3.3,
in practice the malicious node detection algorithm based on the theorem typically works very well when N∗
andpare reasonably large and Ni’s are not dramatically far away from each other.
The independent ranking assumption in Theorem 3.3, which assumes that {R:,j}1≤j≤pare independent of
each other, may look restrictive. However, in fact it is a mild condition that can be easily satisﬁed in practice
due to the following reasons. First, for a benign node i∈Ib,Mi,jandMi,kare often nearly independent, as
the correlation between two model parameters θjandθkis often very weak in a large deep neural network
with a huge number of parameters. To verify the statement, we implemented independence tests for 100,000
column pairs randomly chosen from the message matrix Mgenerated from the FASHION-MNIST data.
Distribution of the p-values of these tests are demonstrated in Figure 4 via a histogram, which is very close
to a uniform distribution, indicating that Mi,jandMi,kare indeed nearly independent in practice. Second,
even someM:,jandM:,kshow a strong correlation, the magnitude of the correlation would be reduced
greatly during the transformation from MtoR, as the ﬁnal ranking Ri,jalso depends on many other factors.
Actually, the independent ranking assumption could be relaxed to be an uncorrelated ranking assumption
which assumes the rankings are uncorrelated with each other. Adopting the weaker assumption will result in
a change in the convergence type of our theorems from the “almost surely convergence” to “convergence in
probability”.
3.2 Sign ﬂipping attack
Deﬁnition 3.4 (Sign ﬂipping attack) .Sign ﬂipping attack aims to generate the gradient values of malicious
nodes by ﬂipping the sign of the average of all the benign nodes’ gradient at each epoch, i.e., specifying
Mi,:=−rmb,:for anyi∈Im, wherer>0,mb=1
n1/summationtext
k∈IbMk,:.
6Under review as submission to TMLR
Based on the above deﬁnition, the update message of a malicious node iunder the sign ﬂipping attack is
Mi,:=−rmb,:=−r
n1/summationtext
k∈IbMk,:. The theorem 3.5 summarizes the behavior of malicious nodes and benign
nodes respectively, with the detailed proof provided in Appendix D.
Theorem 3.5 (Behavior under sign ﬂipping attacks) .With the same assumption as posed in Theorem 3.3,
under the sign ﬂipping attack, the behaviors of benign and malicious nodes are as follows:
¯µb=n+n0+1
2−n0ρ,¯µm=n1ρ+n0+1
2,
¯s2
b=ρS2
[1,n1]+ (1−ρ)S2
[n0+1,n]−(¯µb)2,
¯s2
m=ρS2
[n1+1,n]+ (1−ρ)S2
[1,n0]−(¯µm)2,
whereρ=limp→∞/summationtextp
j=1I(µj>0)
pwhich depends on n0andn1,S2
[a,b]=1
b−a+1/summationtextb
k=ak2. And ¯s2
mand¯s2
bare
both quadratic functions of ρ.
Considering that ¯µb=¯µmif and only if ρ=1
2, and ¯s2
b=¯s2
mif and only if ρis the solution of a quadratic
function, the probability of (¯µb,¯s2
b) = ( ¯µm,¯s2
m)is zero asp→∞. Such a phenomenon suggests that we
can detect the malicious nodes based on the moments (ei,vi)to defense the sign ﬂipping attack as well.
Noticeably, we note that the limit behavior of eiandvidoes not dependent on the speciﬁcation of r, which
deﬁnes the sign ﬂipping attack. Although such a fact looks a bit abnormal at the ﬁrst glance, it is totally
understandable once we realize that with the variance of Mi,jshrinks to zero with Nigoes to inﬁnity for
each benign node i, any diﬀerent between µjandµj(r)would result in the same ranking vector R:,jin the
ranking space.
3.3 Zero gradient attack
Deﬁnition 3.6 (Zero gradient attack) .Zero gradient attack aims to make the aggregated message to be
zero, i.e.,/summationtextn
i=1Mi,:= 0, at each epoch, by specifying Mi,:=−n1
n0mb,:for alli∈Im.
Apparently, the zero gradient attack deﬁned above is a special case of sign ﬂipping attack by specifying
r=n1
n0. The conclusions of Theorem 3.5 keep unchanged for diﬀerent speciﬁcations of r. Therefore, the
behavior follows the same limiting behaviors as described in Theorem 3.5.
3.4 Mean shift attack
Deﬁnition 3.7 (Mean shift attack) .Mean shift attack (Baruch et al., 2019) manipulates the updates
of the malicious nodes in the following fashion, mi,j=µj−z·σjfori∈Imand1≤j≤p, where
µj=1
n1/summationtext
i∈IbMi,j,σj=/radicalBig
1
n1/summationtext
i∈Ib(Mi,j−µj)2andz= arg maxtφ(t)<n−2
2(n−n0).
Mean shift attacks aim to generate malicious gradients which are not well separated, but of diﬀerent
distributions, from the benign nodes. Theorem 3.8 details the behavior of malicious nodes and benign nodes
under mean shift attacks. The proof can be found in Appendix E
Theorem 3.8. With the same assumption as posed in Theorem 3.3 and additionally nis relatively large,
under the mean shift attack, the behaviors of benign and malicious nodes are as follows:
¯µb=n+1
2+n0
n1(n1−α),¯µm=α+n0+1
2,
¯s2
b=1
n1(τ(n) +τ(α)−τ(α+ 1 +n0))−¯µ2
b,¯s2
m= 0,
where⌊·⌋denotes the ﬂoor function, α=⌊n1Φ(z)⌋,Φ(z)is the cumulative density function of the standard
normal distribution and τ(·)is the function of ‘sum of squares’, i.e., τ(n) =/summationtextn
k=1k2.
4 Experiments
In these experiments we extend the data poisoning experimental framework of Tolpegin et al. (2020); Wu
et al. (2020a), integrating Byzantine attack implementations released by Wu et al. (2020b) and the mean
7Under review as submission to TMLR
shift attack Baruch et al. (2019). The mean shift attack was designed to poison gradients by adding ‘a
little’ amount of noise, and shown to be eﬀective in defeating Krum (Blanchard et al., 2017) and Bulyan
(Guerraoui et al., 2018) defenses. The mean shift attack is deﬁned in Deﬁnition 3.7. In our experiments, we
setΣ = 30Ifor the Gaussian attack and r= 3for the sign ﬂipping attack, where Iis the identity matrix.
For all experiments we ﬁx n= 100participating nodes, of which a variable number of nodes are poisoned
|n0|∈{5,10,15,20,25,30}. The training process is run until 25 epochs have elapsed. We have described the
structure of these networks in Appendix F.
4.1 Defense by MANDERA for IID Settings
We evaluate the eﬃcacy in detecting malicious nodes within the federated learning framework with the use
of three IID datasets. The ﬁrst is the FASHION-MNIST dataset Xiao et al. (2017), a dataset of 60,000
and 10,000 training and testing samples respectively divided into 10 classes of apparel. The second is
CIFAR-10 Krizhevsky et al. (2009), a dataset of 60,000 small object images also containing 10 object classes.
The third is the MNIST Deng (2012) dataset. The MNIST dataset is a dataset of 60,000 and 10,000 training
and testing samples respectively divided into 10 classes of handwritten digits from multiple authors.
We test the performance of MANDERA on the update gradients of a model under attacks. In this section,
MANDERA acts as an observer without intervening in the learning process to identify malicious nodes with
a set of gradients from a single epoch. Each conﬁguration of 25 training epochs, with a given number of
malicious nodes was repeated 20 times. Figure 5 demonstrates the classiﬁcation performance (Metrics deﬁned
in Appendix G) of MANDERA with diﬀerent settings of participating malicious nodes and the four poisoning
attacks, i.e., GA, ZG, SF and MS.
While we have formally demonstrated the eﬃcacy of MANDERA in accurately detecting potentially malicious
nodes participating in the federated learning process. In practice, to leverage an unsupervised K-means
clustering algorithm, we must also identify the correct group of nodes as the malicious group. Our strategy
is to identify the group with the most exact gradients, or otherwise the smaller group (we regard a system
with over 50% of their nodes compromised as having larger issues than just poisoning attacks).1We also test
other clustering algorithms, such as hierarchical clustering and Gaussian mixture models Fraley & Raftery
(2002). It turns out that the performance of MANDERA is quite robust with diﬀerent choices of clustering
methods. Detailed results can be found in Appendix I. From Figure 5, it is immediately evident that the
recall of the malicious nodes for the Byzantine attacks is exceptional. However, occasionally benign nodes
have also been misclassiﬁed as malicious under SF attacks. On all attacks, in the presence of more malicious
nodes, the recall of malicious nodes trends down.
We encapsulate MANDERA into a module prior to the aggregation step, MANDERA has the sole objective of
identifying malicious nodes, and excluding their updates from the global aggregation step. Each conﬁguration
of 25 training epochs, a given poisoning attack, defense method, and a given number of malicious nodes
was repeated 10 times. We compare MANDERA against 5 other robust aggregation defense methods,
Krum Blanchard et al. (2017), Bulyan Guerraoui et al. (2018), Trimmed Mean Yin et al. (2018), Median Yin
et al. (2018) and FLTrust Cao et al. (2020). Of which the ﬁrst 2 require an assumed number of malicious
nodes, and the latter 3 only aggregate robustly.
Table 1 demonstrates the accuracy of the global model at the 25th epoch under four Byzantine attacks and
six defense strategies, using the MNIST-Digits data set. It shows MANDERA universally outperforms all the
other competing defence strategies for the MNIST-Digits data set. Note that MANDERA is approaching
(sometimes even better than) the performance of a model which is not attacked. Interestingly, FLTrust as a
standalone defense is weak in protecting against the most extreme Byzantine attacks. However, we highlight
that FLtrust is a robust aggregation method against speciﬁc attacks that may thwart defences like Krum,
Trimmed mean. We see FLTrust as a complementary defence that relies on a base method of defence against
Byzantine attacks, but expands the protection coverage of the FL system against adaptive attacks.
1More informed approaches to selecting the malicious cluster can be tested in future work. E.g. Figure 3 displays less
variation of ranking variance in malicious cluster compared to benign nodes. This could robust selection of the malicious group,
and enabling selection of malicious groups larger than 50%.
8Under review as submission to TMLR
GA ZG SF MSAccuracy Recall Precision F1
510152025305101520253051015202530510152025300.250.500.751.00
0.250.500.751.00
0.250.500.751.00
0.250.500.751.00
Number of malicious nodesScoreMetric Accuracy Recall Precision F1
(a) CIFAR-10
GA ZG SF MSAccuracy Recall Precision F1
510152025305101520253051015202530510152025300.250.500.751.00
0.250.500.751.00
0.250.500.751.00
0.250.500.751.00
Number of malicious nodesScoreMetric Accuracy Recall Precision F1 (b) FASHION-MNIST
GA ZG SF MSAccuracy Recall Precision F1
510152025305101520253051015202530510152025300.250.500.751.00
0.250.500.751.00
0.250.500.751.00
0.250.500.751.00
Number of malicious nodesScoreMetric Accuracy Recall Precision F1
(c) MNIST-Digits
Figure 5: Classiﬁcation performance of our proposed approach MANDERA under four types of attack for
three IID settings.
The performance of all the epochs for MNIST-Digits can be found in Figure 6. It consistently shows
MANDERA outperforms the other competing strategies at each epoch. For the performance of the other
two data sets, see Appendix H, where MANDERA also performs better than other defence strategies. The
corresponding model losses can be found in Appendix J.
4.2 Defense by MANDERA for non-IID Settings
In this section, we evaluate the applicability of MANDERA when applied in a non-IID setting in Federated
learning to validate its eﬀectiveness. The batch size present through the existing evaluations of Section 4.1 is
10. This low setting practically yields gradient values at each local worker node as if they were derived from
non-IID samples. This is a strong indicator that MANDERA could be eﬀective for non-IID settings. We
reinforce MANDERA’s applicability in the non-IID setting by repeating the experiment on QMNIST Yadav
& Bottou (2019), a dataset that is per-sample equivalent to MNIST Deng (2012). QMNIST, however,
additionally provides us with writer identiﬁcation information. This identity is leveraged to ensure that each
local node only trains on digits written by a set of unique users not seen by other workers. Such a setting is
widely recognized as non-IID setting in the community (Kairouz et al., 2021). For 100 nodes, this works
9Under review as submission to TMLR
Table 1: MNIST-Digits model accuracy at 25th epoch. The boldhighlights the best defense strategy under
attack. “NO-attack” is the baseline, where no attack is conducted. And n0denotes the number of malicious
nodes among 100 nodes.
Attack Defence n0= 5n0= 10n0= 15n0= 20n0= 25n0= 30
GAKrum 96.77 96.63 96.78 96.89 96.90 96.90
NO-attack 98.45 98.45 98.45 98.45 98.45 98.45
Bulyan 98.46 98.43 98.40 98.36 98.35 98.29
Median 98.33 98.31 98.32 98.31 98.31 98.34
Trim-mean 98.45 98.43 98.41 98.38 98.38 98.35
MANDERA 98.48 98.46 98.44 98.43 98.44 98.42
FLTrust 95.33 65.22 61.02 37.45 11.37 12.17
ZGKrum 96.95 96.35 96.93 96.96 97.07 96.50
NO-attack 98.45 98.45 98.45 98.45 98.45 98.45
Bulyan 97.97 98.19 98.25 98.24 98.17 98.13
Median 98.17 98.00 97.74 97.36 96.77 96.10
Trim-mean 98.12 97.89 97.54 97.06 96.55 95.69
MANDERA 98.47 98.35 98.44 98.46 98.44 98.41
FLTrust 97.78 95.42 94.09 89.74 87.33 93.08
SFKrum 96.82 96.73 96.79 96.77 96.78 96.69
NO-attack 98.45 98.45 98.45 98.45 98.45 98.45
Bulyan 98.38 98.35 98.30 98.25 98.19 98.13
Median 98.16 98.00 97.75 97.33 96.78 96.14
Trim-mean 98.24 98.03 97.69 97.17 96.58 95.56
MANDERA 98.51 98.47 98.44 98.43 98.41 98.40
FLTrust 98.28 98.02 97.55 97.02 90.58 84.53
MSKrum 98.45 98.40 98.34 98.33 98.29 98.24
NO-attack 98.45 98.45 98.45 98.45 98.45 98.45
Bulyan 98.42 98.38 98.38 98.33 98.27 98.23
Median 98.41 98.39 98.33 98.28 98.25 98.23
Trim-mean 98.46 98.41 98.38 98.34 98.29 98.26
MANDERA 98.48 98.45 98.46 98.43 98.44 98.44
FLTrust 98.46 98.44 98.45 98.42 98.42 98.38
out to be approximately 5 writers in each node. All other experimental conﬁgurations remain the same as
Section 4.1.
Figure 7 demonstrates the eﬀectiveness of MANDERA in malicious node detection for the non-IID setting.
These results are very similar to the results where data is IID settings. Except for sign-ﬂipping attacks,
MANDERA can perfectly distinguish malicious nodes from benign nodes. when the number of malicious
nodes is less than 25, MANDERA mis-classiﬁes some benign nodes as malicious under sign-ﬂipping attacks.
It is noticeable that even though MANDERA does not perform perfectly for SF attacks, the recall is always
equal to 1. This indicates that all the malicious nodes are correctly identiﬁed, but a few of benign nodes
are misclassiﬁed as malicious nodes. This is important to understand why MANDERA outperforms the
completing defence strategies, as shown in Table 2.
Table 2 shows the global model training accuracy with diﬀerent defense strategies for a non-IID setting. It
indicates that MANDERA almost universally outperforms the other defensing strategies and achieves the
best performance. Considering the performance of malicious detection under GA, ZG and MS, shown in
Figure 7, it is natural to expect a good performance of MANDERA in terms of the accuracy of the global
model. At the ﬁrst glance, it is puzzling to observe MANDERA outperforms the others under SF attacks,
considering the ‘bad’ performance of malicious node detection under SF attacks. To explain this phenomenon,
we should pay special attention to the recall in Figure 7. A recall of 1 indicates all the malicious nodes are
identiﬁed. Low values of accuracy and precision mean that some ‘extreme’ benign nodes are identiﬁed as
malicious nodes. Therefore, the aggregated gradient values using MANDERA are close to the true gradient
values, resulting in high accuracy. The results for all the epochs can be found in Figure 8. The corresponding
model losses can be found in Appendix K.
10Under review as submission to TMLR
5 10 15 20 25 30GA ZG SF MS
051015202505101520250510152025051015202505101520250510152025255075100
60708090100
60708090100
909396
Number of EpochAccuracyDefenceKrum
NO−attackBulyan
MedianTrim−mean
MANDERAFLTrust
Figure 6: Model Accuracy at each epoch of training, each line of the curve represents a diﬀerent defense against
the Byzantine attacks. Shown above is the result for MNIST-Digits, ﬁgures for CIFAR and FASHION-MNIST
can be found in the appendix.
4.3 Computational speed
MANDERA enjoys super-fast computation. We have previously been able to observe that MANDERA can
perform at par with the current highest-performing poisoning attack defenses. Another beneﬁt arises with
the simpliﬁcation of the mitigation strategy with the introduction of ranking at the core of the algorithm.
Sorting and Ranking algorithms are fast. Additionally, we only apply clustering on the two dimensions (mean
and standard deviation of the ranking), in contrast to other works that seek to cluster on the entire node
update Chen et al. (2021). The times in Table 3 for MANDERA, Krum and Bulyan do not include the
parameter/gradient aggregation step. These times were computed on 1 core of a Dual Xeon 14-core E5-2690,
with 8 Gb of system RAM and a single Nvidia Tesla P100. Table 3 demonstrates that MANDERA is able to
achieve a faster speed than that of single Krum2(by more than half) and Bulyan (by an order of magnitude).
We have listed the computational times of state-of-art methods in Table 3.
2The use of multi-krum would have yielded better protection (c.f. Section 4) at the behest of speed.
11Under review as submission to TMLR
GA ZG SF MSAccuracy Recall Precision F1
510152025305101520253051015202530510152025300.250.500.751.00
0.250.500.751.00
0.250.500.751.00
0.250.500.751.00
Number of malicious nodesScoreMetric Accuracy Recall Precision F1
Figure 7: Malicious node detection by MANDERA for a Non-IID data set: QMNIST under four diﬀerent
Byzantine attacks.
5 Discussion and Conclusion
Theorem 2.1 indicates that Byzantine attacks can only evade MANDERA when the attackers know the
distribution of benign nodes and at the same time huge computational resources are required. This makes
MANDERA a strategy which is challenging for attackers to evade.
We acknowledge FL framework may learn the global model only using subset of nodes at each round. In
these settings MANDERA would still function, as we would rank and cluster on the parameters of the
participating nodes, without assuming any number of poisoned nodes. In Algorithm 1, performance could
be improved by incorporating higher order moments. MANDERA is unable to function when gradients are
securely aggregated in its current form. However, malicious nodes can be identiﬁed and excluded from the
secure aggregation step, while still protecting the privacy of participating nodes by performing MANDERA
through secure ranking Zhang et al. (2013); Lin & Tzeng (2005) (recall that MANDERA only requires the
ranking matrix to detect poisoned nodes).
In conclusion, we proposed a novel way to tackle the challenges for malicious node detection when using the
gradient values. Our method transfers the gradient values to a ranking space. We have provided theoretical
guarantees and experimentally shown eﬃcacy in MANDERA for the detection of malicious nodes performing
poisoning attacks against federated learning. Our proposed method MANDERA, is able to achieve excellent
detection accuracy and maintain a higher model accuracy than other seminal.
12Under review as submission to TMLR
Table 2: QMNIST model accuracy at 25th epoch. The boldhighlights the best defense strategy under attack.
“NO-attack” is the baseline, where no attack is conducted. And n0denotes the number of malicious nodes
among 100 nodes.
Attack Defence n0= 5n0= 10n0= 15n0= 20n0= 25n0= 30
GAKrum 94.16 93.87 93.95 94.10 94.27 93.89
NO-attack 98.12 98.12 98.12 98.12 98.12 98.12
Bulyan 98.09 98.07 98.06 98.02 97.99 97.88
Median 97.76 97.76 97.77 97.78 97.75 97.77
Trim-mean 98.08 98.04 98.00 97.96 97.91 97.85
MANDERA 98.11 98.11 98.12 98.10 98.10 98.08
FLTrust 83.48 57.32 25.75 18.80 15.43 9.75
ZGKrum 94.21 93.90 93.92 94.11 93.84 93.95
NO-attack 98.12 98.12 98.12 98.12 98.12 98.12
Bulyan 97.58 97.83 97.90 97.87 97.79 97.71
Median 97.59 97.27 96.84 96.33 95.54 94.45
Trim-mean 97.66 97.20 96.67 96.02 95.04 93.97
MANDERA 97.85 97.78 97.64 98.21 98.13 98.09
FLTrust 91.60 95.65 92.15 85.53 88.85 89.58
SFKrum 94.22 93.92 94.01 94.20 93.89 93.84
NO-attack 98.12 98.12 98.12 98.12 98.12 98.12
Bulyan 98.01 97.96 97.98 97.93 97.81 97.66
Median 97.61 97.29 96.84 96.33 95.58 94.55
Trim-mean 97.82 97.52 96.97 96.21 94.98 93.75
MANDERA 98.20 98.23 98.22 98.19 98.15 98.14
FLTrust 97.75 97.21 96.65 88.25 89.99 88.29
MSKrum 95.97 94.09 94.17 94.28 95.23 95.80
NO-attack 98.12 98.12 98.12 98.12 98.12 98.12
Bulyan 98.07 98.01 97.97 97.92 97.84 97.82
Median 97.88 97.96 97.96 97.90 97.79 97.70
Trim-mean 98.05 97.98 97.94 97.92 97.88 97.81
MANDERA 98.11 98.12 98.10 98.08 98.08 98.06
FLTrust 98.13 98.11 98.12 98.10 98.09 98.06
Table 3: Mean and standard deviation of computational times for defense function given the same set of
gradients from 100 nodes, of which 30 were malicious. Each function was repeated 100 times.
Defense (Detection) Mean ±SD (ms) Defense (Aggregation) Mean ±SD (ms)
MANDERA 643 ±8.646 Trimmed Mean 3.96 ±0.41
Krum (Single) 1352 ±10.09 Median 9.81 ±3.88
Bulyan 27209 ±233.4 FLTrust 361 ±4.07
13Under review as submission to TMLR
5 10 15 20 25 30GA ZG SF MS
051015202505101520250510152025051015202505101520250510152025255075100
406080100
406080100
5060708090100
Number of EpochAccuracyDefenceKrum
NO−attackBulyan
MedianTrim−mean
MANDERAFLTrust
Figure 8: Model accuracy under diﬀerent defences strategies for Non-IID data set: QMNIST
14Under review as submission to TMLR
A Proof of Theorem 2.1
Proof.LetFj(x)andGj(x)be the cumulative distribution functions of Fj(·)andGj(·),fj(x)andgj(x)be
the corresponding density functions, and rj(x) =n1−n1Gj(x) +n0−n0Fj(x) + 1be the expected ranking
of valuexamong all entries in the jthcolumn of the gradient value matrix.
Further deﬁne
Ebj=/integraldisplay∞
−∞rj(x)gj(x)dx, Vbj=/integraldisplay∞
−∞(rj(x)−Ebj)2gj(x)dx,
Emj=/integraldisplay∞
−∞rj(x)fj(x)dx, Vmj=/integraldisplay∞
−∞(rj(x)−Emj)2fj(x)dx.
It can be shown for any 1≤j≤pthat
Eij=E(Ri,j) =Ebj·I(i∈Ib) +Emj·I(i∈Im),
Vij=V(Ri,j) =Vbj·I(i∈Ib) +Vmj·I(i∈Im).
Thus, we would have according to Kolmogorov’s strong law of large numbers (KSLLN) that
lim
N∗→∞lim
p→∞ei= ¯µb·I(i∈Ib) + ¯µm·I(i∈Im)a.s.,
lim
N∗→∞lim
p→∞vi= ¯s2
b·I(i∈Ib) + ¯s2
m·I(i∈Im)a.s.,
where the moments (¯µb,¯s2
b)and(¯µm,¯s2
m)are deterministic functions of (Ebj,Vbj)and(Emj,Vmj)of the
following form:
¯µb= lim
p→∞1
pp/summationdisplay
j=1Ebj, ¯µm= lim
p→∞1
pp/summationdisplay
j=1Emj,
¯s2
b= lim
p→∞1
pp/summationdisplay
j=1Vbj, ¯s2
m= lim
p→∞1
pp/summationdisplay
j=1Vmj.
It completes the proof.
B Proof of Theorem 2.2
Proof.According to Theorem 2.1, when both N∗andpare large enough, with probability 1 there exist
(eb,vb),(em,vm)andδ>0such that||(eb,vb)−(em,vm)||2>δ, and
||(ei,vi)−(eb,vb)||2≤δ
2for∀i∈Iband||(ei,vi)−(em,vm)||2≤δ
2for∀i∈Im.
Therefore, with a reasonable clustering algorithm such as K-mean with K= 2, we would expect ˆIb=Ibwith
probability 1.
Because we can always ﬁnd a ∆>0such that||Mi,:−Mj,:||2≤∆for any node pair (i,j)in a ﬁxed dataset
with a ﬁnite number of nodes, and ˆmb,:=mb,:when ˆIb=Ib, we have
E||ˆmb,:−mb,:||2≤∆·P(ˆIb/negationslash=Ib),
and thus
lim
N∗→∞lim
p→∞E||ˆmb,:−mb,:||2= 0.
It completes the proof.
15Under review as submission to TMLR
C Proof of Theorem 3.3
Proof.According to Theorem 2.1, we only need to compute ¯µb,¯µm,¯s2
band¯s2
munder the Gaussian attacks.
BecauseMi,j→dN/parenleftbig
µj,Σj,j/parenrightbig
for∀i∈ImandMi,j→dN/parenleftbig
µj,σ2
j/Ni/parenrightbig
for∀i∈IbwhenN∗→∞, it is
straightforward to see due to the symmetry of Gaussian distribution that
lim
N∗→∞Ebj= lim
N∗→∞Emj= lim
N∗→∞E(Ri,j) =n+ 1
2,1≤i≤n,1≤j≤p. (1)
Therefore, we have
¯µb= lim
N∗→∞lim
p→∞1
pp/summationdisplay
j=1Ebj=n+ 1
2,
¯µm= lim
N∗→∞lim
p→∞1
pp/summationdisplay
j=1Emj=n+ 1
2.
Moreover, assuming that the sample sizes of diﬀerent benign nodes approach to each other with N∗going to
inﬁnity, i.e.,
lim
N∗→∞1
N∗max
i,k∈Ib|Ni−Nk|= 0, (2)
for each parameter dimension j,{Mi,j}i∈Ibwould converge to the same Gaussian distribution N(µj,σ2
j/N∗)
with the increase of N∗. Thus, due to the exchangeability of {Mi,j}i∈Iband{Mi,j}i∈Im, it is easy to see
that that
lim
N∗→∞Vbj=s2
b,j, lim
N∗→∞Vmj=s2
m,j, (3)
wheres2
b,jands2
m,jare both complex functions of n0,n1,σ2
j,Σj,jandN∗, ands2
b,j=s2
m,jif and only
ifσ2
j/N∗= Σj,j. According to Theorem 2.1, ¯s2
b=limp→∞1
p/summationtextp
j=1Vbj=limN∗→∞1
p/summationtextp
j=1s2
b,jand¯s2
m=
limp→∞1
p/summationtextp
j=1Vmj= limp→∞1
p/summationtextp
j=1s2
m,j. The proof is complete.
D Proof of Theorem 3.5
Proof.According to Theorem 2.1, we only need to compute ¯µb,¯µm,¯s2
band¯s2
munder the sign ﬂipping attacks.
Lemma D.1. Under the sign ﬂipping attack, for each malicious node i∈Imand any parameter dimension
j, we haveMi,j=−r
n1/summationtext
k∈IbMk,jis a deterministic function of {Mk,j}k∈Ib, whose limiting distribution
whenN∗goes to inﬁnity is
Mi,j→dN/parenleftbig
µj(r),σ2
j(r)/parenrightbig
,1≤j≤p, (4)
whereµj(r) =−rµj,σ2
j(r) =r2·σ2
j
n1·¯Nb, and ¯Nb=n1/summationtext
k∈Ib1
Nkis the harmonic mean of {Nk}k∈Ib.
Lemma 3.1 and Lemma D.1 tell us that for each parameter dimension j, the distribution of {Mi,j}n
i=1
is a mixture of Gaussian components {N/parenleftbig
µj,σ2
j/Ni/parenrightbig
}i∈Ibcentered at µjplus a point mass located at
µj(r) =−rµj. IfNi’s are reasonably large, variances σ2
j/Ni’s would be very close to zero, and the probability
mass of the mixture distribution would concentrate to two local centers µjandµj(r) =−rµj, one for the
benign nodes and the other one for the malicious nodes.
Under the sign ﬂipping attack, because Mi,j→dN/parenleftbig
µj(r),σ2
j(r)/parenrightbig
for∀i∈ImandMi,j→dN/parenleftbig
µj,σ2
j/Ni/parenrightbig
for
∀i∈IbwhenN∗→∞, and
lim
N∗→∞(σ2
j/Ni) = lim
N∗→∞σ2
j(r) = 0.
It is straightforward to see that
lim
N∗→∞P(Mi,j>Mk,j) =I(µj>0),∀i∈Ib,∀k∈Im,
16Under review as submission to TMLR
which further indicates that
lim
N∗→∞Ebj= lim
N∗→∞E(Ri,j) =n1+ 1
2, if µj>0,
lim
N∗→∞Emj= lim
N∗→∞E(Ri,j) =n+n1+ 1
2, if µj>0,
lim
N∗→∞Ebj= lim
N∗→∞E(Ri,j) =n+n0+ 1
2if µj<0
lim
N∗→∞Emj= lim
N∗→∞E(Ri,j) =n0+ 1
2if µj<0,(5)
lim
N∗→∞E(R2
i,j) =S2
[1,n1]·I(i∈Ib) +S2
[n1+1,n]·I(i∈Im)if µj>0,
lim
N∗→∞E(R2
i,j) =S2
[1,n0]·I(i∈Im) +S2
[n0+1,n]·I(i∈Ib)if µj<0,(6)
whereS2
[a,b]=1
b−a+1/summationtextb
k=ak2.
Therefore, we have
/braceleftBigg
¯µm= limN∗→∞limp→∞1
p/summationtextp
j=1Ebj=ρ·n+n1+1
2+ (1−ρ)·n0+1
2,
¯µb= limN∗→∞limp→∞1
p/summationtextp
j=1Emjρ·n1+1
2+ (1−ρ)·n+n0+1
2,
whereρ= limp→∞/summationtextp
j=1I(µj>0)
p.
Deﬁne ¯µi= ¯µm·I(i∈Im) + ¯µb·I(i∈Ib). Considering that
lim
N∗→∞lim
p→∞1
pp/summationdisplay
j=1Vij
= lim
N∗→∞lim
p→∞1
pp/summationdisplay
j=1E(Ri,j−¯µi)2
= lim
p→∞lim
N∗→∞1
pp/summationdisplay
j=1/parenleftBig
E(R2
i,j)−2¯µiE(Ri,j) + (¯µi)2/parenrightBig
=/bracketleftbig
¯τm−(¯µm)2/bracketrightbig
·I(i∈Im) +/bracketleftbig
¯τb−(¯µb)2/bracketrightbig
·I(i∈Ib),
where
¯τb=ρ·S2
[1,n1]+ (1−ρ)·S2
[n0+1,n],
¯τm=ρ·S2
[n1+1,n]+ (1−ρ)·S2
[1,n0].
According to Theorem 2.1,
¯s2
b= lim
p→∞lim
N∗→∞1
pp/summationdisplay
j=1Vbj= ¯τb−(¯µb)2,
¯s2
m= lim
p→∞lim
N∗→∞1
pp/summationdisplay
j=1Vmj= ¯τm−(¯µm)2.
It completes the proof.
E Proof of Theorem 3.8
Proof.According to Theorem 2.1, we only need to compute ¯µb,¯µm,¯s2
band¯s2
munder the mean shift attacks.
Under the mean shift attack, all the malicious gradient will be inserted at a position which is dependent on z.
More speciﬁcally, for a relatively large n, the samples from benign nodes are normally distributed. Therefore,
17Under review as submission to TMLR
on average, with proportion Φ(z)of the benign nodes having higher values of gradient than the malicious
nodes.
First of all, we derive the property in term of the ﬁrst moment. Denote α=⌊n1Φ(z)⌋. For a benign node, we
have
lim
N∗→∞lim
n→∞Ebj= lim
N∗→∞lim
n→∞E(Ri,j) =1
n1/parenleftBiggα/summationdisplay
k=1k+n/summationdisplay
s=n0+1+αs/parenrightBigg
=n+ 1
2+n0
n1(n1−α).
For a malicious node, we have
lim
N∗→∞lim
n→∞Emj= lim
N∗→∞lim
n→∞E(Ri,j) =α+ 1 +α+n0
2=α+1 +n0
2.
Therefore, according to Theorem 2.1,
¯µb= lim
N∗→∞lim
n→∞lim
p→∞1
pp/summationdisplay
j=1Ebj=n+ 1
2+n0
n1(n1−α),
¯µm= lim
N∗→∞lim
n→∞lim
p→∞1
pp/summationdisplay
j=1Emj=α+1 +n0
2.
Now, we derive the property in term of the second moment. For a benign node, we have
lim
N∗→∞lim
n→∞E(R2
i,j) =1
n1/parenleftBiggα/summationdisplay
k=1k2+n/summationdisplay
s=n0+1+αs2/parenrightBigg
=1
n1(τ(n) +τ(α)−τ(α+ 1 +n0)),
whereτ(·)is the function of ‘sum of squares’, i.e., τ(n) =/summationtextn
k=1k2.
For a malicious node, we have
lim
N∗→∞lim
n→∞E(R2
i,j) =/parenleftbigg
α+1 +n0
2/parenrightbigg2
,
Therefore, according to Theorem 2.1,
¯s2
b= lim
N∗→∞lim
n→∞lim
p→∞1
pp/summationdisplay
j=1Vbj=1
n1(τ(n) +τ(α)−τ(α+ 1 +n0))−¯µ2
b,
¯s2
m= lim
N∗→∞lim
n→∞lim
p→∞1
pp/summationdisplay
j=1Vmj= 0.
It completes the proof.
F Neural Network conﬁgurations
We train these models with a batch size of 10, an SGD optimizer operates with a learning rate of 0.01, and
0.5 momentum for 25 epochs. The accuracy of the model is evaluated on a holdout set of 1000 samples.
F.1 FASHION-MNIST, MNIST and QMNIST
•Layer 1: 1∗16∗5, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.
•Layer 2: 16∗32∗5, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.
•Output: 10Classes, Linear.
18Under review as submission to TMLR
F.2 CIFAR-10
•Layer 1: 1∗32∗3, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.
•Layer 2: 32∗32∗3, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.
•Output: 10Classes, Linear.
G Metrics
The metrics observed in Section 4 to evaluate the performance of the defense mechanisms are deﬁned as
follows:
Precision =TP
TP+FP,
Accuracy =TP+TN
TP+FP+FN+TN,
Recall =TP
TP+FN,
F1= 2×Precision×Recall
Precision+Recall.
H Accuracy of the global model under diﬀerent attacks
In Table 4 and 5 the numeric accuracies of each experimental conﬁguration at the 25th epoch are presented.
I MANDERA performance with diﬀerent clustering algorithms
In this section, Figure 10 demonstrate that the discriminating performance of MANDERA when hierarchical
clustering and Gaussian mixture models are used in-place of K-means for FASHION-MNIST data set remain
robust.
J Model Losses on CIFAR-10, FASHION-MNIST and MNIST data
Figure 11 - 13 present the model loss to accompany the model prediction performance for CIFAR-10,
FASHION-MNIST and MNIST-Digits respectively, which are previously seen in Section 4.
K Model Losses on QMNIST data
Figure 14 presents the model loss to accompany the model prediction performance of QMNIST previously
seen in Section 4.
19Under review as submission to TMLR
Table 4: FASHION-MNIST model accuracy at 25th epoch. The boldhighlights the best defense strategy
under attack. Note “NO-attack” is the baseline, where no attack is conducted. And n0denotes the number
of malicious nodes among 100 nodes.
Attack Defence n0= 5n0= 10n0= 15n0= 20n0= 25n0= 30
GAKrum 83.66 84.13 84.09 83.30 84.22 82.32
NO-attack 87.83 87.83 87.83 87.83 87.83 87.83
Bulyan 87.80 87.80 87.79 87.73 87.67 87.69
Median 87.73 87.76 87.73 87.70 87.72 87.70
Trim-mean 87.85 87.78 87.75 87.74 87.72 87.73
MANDERA 87.81 87.83 87.82 87.77 87.80 87.76
FLTrust 66.13 36.35 50.20 17.85 16.00 9.66
ZGKrum 83.56 83.57 84.11 84.33 84.10 84.30
NO-attack 87.83 87.83 87.83 87.83 87.83 87.83
Bulyan 86.88 87.38 87.49 87.45 87.48 87.38
Median 87.36 86.91 86.20 85.33 84.07 82.45
Trim-mean 87.13 86.57 85.67 84.61 83.06 81.48
MANDERA 87.79 87.81 87.84 87.72 87.76 87.78
FLTrust 81.59 83.58 79.41 80.62 79.00 74.01
SFKrum 84.49 84.71 84.43 83.58 83.61 83.72
NO-attack 87.83 87.83 87.83 87.83 87.83 87.83
Bulyan 87.60 87.64 87.62 87.50 87.47 87.35
Median 87.40 86.91 86.21 85.36 84.11 82.31
Trim-mean 87.48 86.97 86.20 84.92 83.08 81.20
MANDERA 87.85 87.79 87.82 87.79 87.77 87.74
FLTrust 86.96 85.97 84.55 76.92 75.72 76.90
MSKrum 87.82 87.77 87.66 87.50 87.36 86.89
NO-attack 87.83 87.83 87.83 87.83 87.83 87.83
Bulyan 87.81 87.78 87.75 87.75 87.60 87.21
Median 87.75 87.78 87.69 87.52 87.26 86.99
Trim-mean 87.81 87.79 87.76 87.73 87.61 87.33
MANDERA 87.81 87.78 87.78 87.79 87.71 87.79
FLTrust 87.77 87.75 87.78 87.77 87.73 87.73
20Under review as submission to TMLR
Table 5: CIFAR-10 model accuracy at 25 th epoch. The boldhighlights the best defense strategy under
attack. Note “NO-attack” is the baseline, where no attack is conducted. And n0denotes the number of
malicious nodes among 100 nodes.
Attack Defence n0= 5n0= 10n0= 15n0= 20n0= 25n0= 30
GAKrum 47.66 47.16 47.18 47.26 47.25 46.77
NO-attack 55.78 55.78 55.78 55.78 55.78 55.78
Bulyan 55.69 55.85 55.67 55.63 55.46 55.22
Median 55.47 55.53 55.47 55.40 55.29 55.22
Trim-mean 55.77 55.72 55.56 55.50 55.43 55.31
MANDERA 55.74 55.69 55.63 55.65 55.76 55.69
FLTrust 19.66 27.54 11.99 9.21 9.73 9.96
ZGKrum 46.85 46.84 47.96 47.13 47.12 47.53
NO-attack 55.78 55.78 55.78 55.78 55.78 55.78
Bulyan 52.30 53.87 54.28 54.36 54.35 54.10
Median 54.06 52.18 50.18 48.01 44.89 38.08
Trim-mean 53.34 51.22 49.14 46.45 42.02 34.36
MANDERA 55.77 55.69 55.78 55.65 55.72 55.56
FLTrust 48.05 39.21 39.44 44.25 40.27 39.49
SFKrum 48.11 47.79 46.93 47.89 47.59 47.13
NO-attack 55.78 55.78 55.78 55.78 55.78 55.78
Bulyan 55.30 54.99 54.86 54.68 54.43 54.05
Median 53.96 52.29 50.49 47.89 44.93 37.22
Trim-mean 54.37 52.40 49.97 47.30 42.32 33.76
MANDERA 55.78 55.69 55.62 55.55 55.67 55.56
FLTrust 54.18 50.21 46.39 44.45 36.19 34.39
MSKrum 55.60 55.23 54.51 53.79 52.31 50.54
NO-attack 55.78 55.78 55.78 55.78 55.78 55.78
Bulyan 55.68 55.62 55.37 54.98 54.26 52.10
Median 55.47 55.20 54.55 53.72 52.17 50.55
Trim-mean 55.64 55.59 55.38 55.09 54.29 52.32
MANDERA 55.65 55.77 55.72 55.62 55.66 55.63
FLTrust 55.81 55.64 55.62 55.42 55.09 54.65
21Under review as submission to TMLR
5 10 15 20 25 30GA ZG SF MS
0510152025051015202505101520250510152025051015202505101520251020304050
20304050
20304050
304050
Number of EpochAccuracyDefenceKrum
NO−attackBulyan
MedianTrim−mean
MANDERAFLTrust
(a) CIFAR-10 accuracy
5 10 15 20 25 30GA ZG SF MS
051015202505101520250510152025051015202505101520250510152025255075
405060708090
5060708090
76808488
Number of EpochAccuracyDefenceKrum
NO−attackBulyan
MedianTrim−mean
MANDERAFLTrust
(b) FASHION-MNIST accuracy
Figure 9: Model Accuracy at each epoch of training, each line of the curve represents a diﬀerent defense
against the Byzantine attacks.
22Under review as submission to TMLR
GA ZG SF MSAccuracy Recall Precision F1
510152025305101520253051015202530510152025300.250.500.751.00
0.250.500.751.00
0.250.500.751.00
0.250.500.751.00
Number of malicious nodesScoreMetric Accuracy Recall Precision F1
(a) Gaussian mixture model.
GA ZG SF MSAccuracy Recall Precision F1
510152025305101520253051015202530510152025300.250.500.751.00
0.250.500.751.00
0.250.500.751.00
0.250.500.751.00
Number of malicious nodesScoreMetric Accuracy Recall Precision F1
(b) Hierarchical clustering.
Figure 10: Classiﬁcation performance of our proposed approach MANDERA (Algorithm 1) with other
clustering algorithms under four types of attack for FASHION-MNIST data. GA: Gaussian attack; ZG:
Zero-gradient attack; SF: Sign-ﬂipping; and MS: mean shift attack. The boxplot bounds the 25th (Q1) and
75th (Q3) percentile, with the central line representing the 50th quantile (median). The end points of the
whisker represent the Q1-1.5(Q3-Q1) and Q3+1.5(Q3-Q1) respectively.
23Under review as submission to TMLR
5 10 15 20 25 30GA ZG SF MS
051015202505101520250510152025051015202505101520250510152025468
2.53.03.54.04.5
2.62.83.03.2
2.62.72.82.9
Number of Epochlog(Loss)DefenceKrum
NO−attackBulyan
MedianTrim−mean
MANDERAFLTrust
Figure 11: Model Loss for CIFAR-10 data at each epoch of training, each line of the curve represents a
diﬀerent defense against the Byzantine attacks.
5 10 15 20 25 30GA ZG SF MS
0510152025051015202505101520250510152025051015202505101520252.55.07.5
123456
1.52.02.5
1.21.41.61.82.0
Number of Epochlog(Loss)DefenceKrum
NO−attackBulyan
MedianTrim−mean
MANDERAFLTrust
Figure 12: Model Loss for FASHION-MNIST data at each epoch of training, each line of the curve represents
a diﬀerent defense against Byzantine attacks.
24Under review as submission to TMLR
5 10 15 20 25 30GA ZG SF MS
0510152025051015202505101520250510152025051015202505101520250.02.55.07.5
01234
0123
01
Number of Epochlog(Loss)DefenceKrum
NO−attackBulyan
MedianTrim−mean
MANDERAFLTrust
Figure 13: Model Loss for MNIST-Digits data at each epoch of training, each line of the curve represents a
diﬀerent defense against the Byzantine attacks.
5 10 15 20 25 30GA ZG SF MS
0510152025051015202505101520250510152025051015202505101520250.02.55.07.5
0123
024
012
Number of Epochlog(Loss)DefenceKrum
NO−attackBulyan
MedianTrim−mean
MANDERAFLTrust
Figure 14: QMNIST model loss.
Figure 15: Model Loss at each epoch of training, each line of the curve represents a diﬀerent defense against
the Byzantine attacks.
25Under review as submission to TMLR
References
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor
federated learning. In International Conference on Artiﬁcial Intelligence and Statistics , pp. 2938–2948.
PMLR, 2020.
Gilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for distributed
learning. Advances in Neural Information Processing Systems , 32, 2019.
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with
adversaries: Byzantine tolerant gradient descent. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems ,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf .
Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. Fltrust: Byzantine-robust federated learning
via trust bootstrapping. arXiv preprint arXiv:2012.13995 , 2020.
Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. Provably secure federated learning against malicious
clients. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 2021.
Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial settings:
Byzantine gradient descent. Proc. ACM Meas. Anal. Comput. Syst. , 1(2), December 2017. doi: 10.1145/
3154503. URL https://doi.org/10.1145/3154503 .
Zheyi Chen, Pu Tian, Weixian Liao, and Wei Yu. Zero knowledge clustering based adversarial mitigation in
heterogeneous federated learning. IEEE Transactions on Network Science and Engineering , 8(2):1070–1083,
2021. doi: 10.1109/TNSE.2020.3002796.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine , 29(6):141–142, 2012.
Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to byzantine-robust
federated learning. In 29th{USENIX}Security Symposium ( {USENIX}Security 20) , pp. 1605–1622, 2020.
Chris Fraley and Adrian E Raftery. Model-based clustering, discriminant analysis, and density estimation.
Journal of the American statistical Association , 97(458):611–631, 2002.
Rachid Guerraoui, Sébastien Rouault, et al. The hidden vulnerability of distributed learning in byzantium.
InInternational Conference on Machine Learning , pp. 3521–3530. PMLR, 2018.
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open
problems in federated learning. Foundations and Trends ®in Machine Learning , 14(1–2):1–210, 2021.
Alex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Leslie Lamport, Robert Shostak, and Marshall Pease. The byzantine generals problem. In Concurrency: the
works of leslie lamport , pp. 203–226. ACM, 2019.
Suyi Li, Yong Cheng, Wei Wang, Yang Liu, and Tianjian Chen. Learning to detect malicious clients for
robust federated learning. arXiv preprint arXiv:2002.00211 , 2020.
Hsiao-Ying Lin and Wen-Guey Tzeng. An eﬃcient solution to the millionaires’ problem based on homomorphic
encryption. In International Conference on Applied Cryptography and Network Security , pp. 456–466.
Springer, 2005.
Jinhyun So, Başak Güler, and A. Salman Avestimehr. Byzantine-resilient secure federated learning. IEEE
Journal on Selected Areas in Communications , 39(7):2168–2181, 2021. doi: 10.1109/JSAC.2020.3041404.
Jacob Steinhardt. Robust learning: Information theory and algorithms . PhD thesis, Stanford University, 2018.
26Under review as submission to TMLR
Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. Data poisoning attacks against federated
learning systems. In European Symposium on Research in Computer Security , pp. 480–501. Springer, 2020.
Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B Giannakis. Byrd-saga - github.
https://github.com/MrFive5555/Byrd-SAGA, 2020a.
Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B Giannakis. Federated variance-reduced stochastic
gradient descent with robustness to byzantine attacks. IEEE Transactions on Signal Processing , 68:
4583–4596, 2020b.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno: Distributed stochastic gradient descent with suspicion-
based fault-tolerance. In International Conference on Machine Learning , pp. 6893–6901. PMLR, 2019.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno++: Robust fully asynchronous sgd. In International
Conference on Machine Learning , pp. 10495–10503. PMLR, 2020.
Chhavi Yadav and Léon Bottou. Cold case: The lost mnist digits. In Advances in Neural Information
Processing Systems 32 . Curran Associates, Inc., 2019.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed learning:
Towards optimal statistical rates. In International Conference on Machine Learning , pp. 5650–5659. PMLR,
2018.
Lan Zhang, Xiang-Yang Li, Yunhao Liu, and Taeho Jung. Veriﬁable private multi-party computation: ranging
and ranking. In 2013 Proceedings IEEE INFOCOM , pp. 605–609. IEEE, 2013.
27