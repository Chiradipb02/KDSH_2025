Under review as submission to TMLR
Optimizing Model-Agnostic Random Subspace Ensembles
Anonymous authors
Paper under double-blind review
Abstract
This paper presents a model-agnostic ensemble approach for supervised learning. The pro-
posed approach is based on a parametric version of Random Subspace, in which each base
model is learned from a feature subset sampled according to a Bernoulli distribution. Pa-
rameter optimization is performed using gradient descent and is rendered tractable by using
an importance sampling approach that circumvents frequent re-training of the base models
after each gradient descent step. The degree of randomization in our parametric Random
Subspace is thus automatically tuned through the optimization of the feature selection prob-
abilities. This is an advantage over the standard Random Subspace approach, where the
degree of randomization is controlled by a hyper-parameter. Furthermore, the optimized
feature selection probabilities can be interpreted as feature importance scores. Our algo-
rithm can also easily incorporate any diﬀerentiable regularization term to impose constraints
on these importance scores.
Feature samplingFeatures<latexit sha1_base64="ikzi6YRz8zRNydAzPBPL8udRcSs=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK9gPaUDbbSbt0swm7G6GG/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVDBssFrFqB1Sj4BIbhhuB7UQhjQKBrWB0M/Vbj6g0j+WDGSfoR3QgecgZNVa6f+p5vXLFrbozkGXi5aQCOeq98le3H7M0QmmYoFp3PDcxfkaV4UzgpNRNNSaUjegAO5ZKGqH2s9mpE3JilT4JY2VLGjJTf09kNNJ6HAW2M6JmqBe9qfif10lNeOVnXCapQcnmi8JUEBOT6d+kzxUyI8aWUKa4vZWwIVWUGZtOyYbgLb68TJpnVe+ien53Xqld53EU4QiO4RQ8uIQa3EIdGsBgAM/wCm+OcF6cd+dj3lpw8plD+APn8wcRqo2r</latexit>z1<latexit sha1_base64="39CbZgeH7idn5xafE3bLj1DQTyg=">AAAB6nicbVDLTgJBEOzFF+IL9ehlIjHxRHYJUY9ELx4xyiOBDZkdemHC7OxmZtYECZ/gxYPGePWLvPk3DrAHBSvppFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsGG4EthOFNAoEtoLRzcxvPaLSPJYPZpygH9GB5CFn1Fjp/qlX6RVLbtmdg6wSLyMlyFDvFb+6/ZilEUrDBNW647mJ8SdUGc4ETgvdVGNC2YgOsGOppBFqfzI/dUrOrNInYaxsSUPm6u+JCY20HkeB7YyoGeplbyb+53VSE175Ey6T1KBki0VhKoiJyexv0ucKmRFjSyhT3N5K2JAqyoxNp2BD8JZfXiXNStm7KFfvqqXadRZHHk7gFM7Bg0uowS3UoQEMBvAMr/DmCOfFeXc+Fq05J5s5hj9wPn8AEy6NrA==</latexit>z2<latexit sha1_base64="12Jo8gJdiW7DRNkK6glxVvClPHE=">AAAB6nicbVDLTgJBEOzFF+IL9ehlIjHxRHaVqEeiF48Y5ZHAhswOA0yYnd3M9Jrghk/w4kFjvPpF3vwbB9iDgpV0UqnqTndXEEth0HW/ndzK6tr6Rn6zsLW9s7tX3D9omCjRjNdZJCPdCqjhUiheR4GSt2LNaRhI3gxGN1O/+ci1EZF6wHHM/ZAOlOgLRtFK90/d826x5JbdGcgy8TJSggy1bvGr04tYEnKFTFJj2p4bo59SjYJJPil0EsNjykZ0wNuWKhpy46ezUyfkxCo90o+0LYVkpv6eSGlozDgMbGdIcWgWvan4n9dOsH/lp0LFCXLF5ov6iSQYkenfpCc0ZyjHllCmhb2VsCHVlKFNp2BD8BZfXiaNs7J3Ua7cVUrV6yyOPBzBMZyCB5dQhVuoQR0YDOAZXuHNkc6L8+58zFtzTjZzCH/gfP4AFLKNrQ==</latexit>z3<latexit sha1_base64="l+hn7j51mPYLaeYnArUkna2dhPk=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KokU9Vj04rGi/YA2lM120i7dbMLuRqihP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEsG1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6mfqtR1Sax/LBjBP0IzqQPOSMGivdP/WqvVLZrbgzkGXi5aQMOeq90le3H7M0QmmYoFp3PDcxfkaV4UzgpNhNNSaUjegAO5ZKGqH2s9mpE3JqlT4JY2VLGjJTf09kNNJ6HAW2M6JmqBe9qfif10lNeOVnXCapQcnmi8JUEBOT6d+kzxUyI8aWUKa4vZWwIVWUGZtO0YbgLb68TJrnFe+iUr2rlmvXeRwFOIYTOAMPLqEGt1CHBjAYwDO8wpsjnBfn3fmYt644+cwR/IHz+QMWNo2u</latexit>z4<latexit sha1_base64="BX0An3IYcXwaybL6ERwZyxxUyaw=">AAAB6nicbVDLTgJBEOzFF+IL9ehlIjHxRHYNPo5ELx4xyiOBDZkdGpgwO7uZmTXBDZ/gxYPGePWLvPk3DrAHBSvppFLVne6uIBZcG9f9dnIrq2vrG/nNwtb2zu5ecf+goaNEMayzSESqFVCNgkusG24EtmKFNAwENoPRzdRvPqLSPJIPZhyjH9KB5H3OqLHS/VP3vFssuWV3BrJMvIyUIEOtW/zq9CKWhCgNE1TrtufGxk+pMpwJnBQ6icaYshEdYNtSSUPUfjo7dUJOrNIj/UjZkobM1N8TKQ21HoeB7QypGepFbyr+57UT07/yUy7jxKBk80X9RBATkenfpMcVMiPGllCmuL2VsCFVlBmbTsGG4C2+vEwaZ2Xvoly5q5Sq11kceTiCYzgFDy6hCrdQgzowGMAzvMKbI5wX5935mLfmnGzmEP7A+fwBF7qNrw==</latexit>z5<latexit sha1_base64="ikzi6YRz8zRNydAzPBPL8udRcSs=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK9gPaUDbbSbt0swm7G6GG/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVDBssFrFqB1Sj4BIbhhuB7UQhjQKBrWB0M/Vbj6g0j+WDGSfoR3QgecgZNVa6f+p5vXLFrbozkGXi5aQCOeq98le3H7M0QmmYoFp3PDcxfkaV4UzgpNRNNSaUjegAO5ZKGqH2s9mpE3JilT4JY2VLGjJTf09kNNJ6HAW2M6JmqBe9qfif10lNeOVnXCapQcnmi8JUEBOT6d+kzxUyI8aWUKa4vZWwIVWUGZtOyYbgLb68TJpnVe+ien53Xqld53EU4QiO4RQ8uIQa3EIdGsBgAM/wCm+OcF6cd+dj3lpw8plD+APn8wcRqo2r</latexit>z1<latexit sha1_base64="39CbZgeH7idn5xafE3bLj1DQTyg=">AAAB6nicbVDLTgJBEOzFF+IL9ehlIjHxRHYJUY9ELx4xyiOBDZkdemHC7OxmZtYECZ/gxYPGePWLvPk3DrAHBSvppFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsGG4EthOFNAoEtoLRzcxvPaLSPJYPZpygH9GB5CFn1Fjp/qlX6RVLbtmdg6wSLyMlyFDvFb+6/ZilEUrDBNW647mJ8SdUGc4ETgvdVGNC2YgOsGOppBFqfzI/dUrOrNInYaxsSUPm6u+JCY20HkeB7YyoGeplbyb+53VSE175Ey6T1KBki0VhKoiJyexv0ucKmRFjSyhT3N5K2JAqyoxNp2BD8JZfXiXNStm7KFfvqqXadRZHHk7gFM7Bg0uowS3UoQEMBvAMr/DmCOfFeXc+Fq05J5s5hj9wPn8AEy6NrA==</latexit>z2Random subspace 1<latexit sha1_base64="ikzi6YRz8zRNydAzPBPL8udRcSs=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK9gPaUDbbSbt0swm7G6GG/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVDBssFrFqB1Sj4BIbhhuB7UQhjQKBrWB0M/Vbj6g0j+WDGSfoR3QgecgZNVa6f+p5vXLFrbozkGXi5aQCOeq98le3H7M0QmmYoFp3PDcxfkaV4UzgpNRNNSaUjegAO5ZKGqH2s9mpE3JilT4JY2VLGjJTf09kNNJ6HAW2M6JmqBe9qfif10lNeOVnXCapQcnmi8JUEBOT6d+kzxUyI8aWUKa4vZWwIVWUGZtOyYbgLb68TJpnVe+ien53Xqld53EU4QiO4RQ8uIQa3EIdGsBgAM/wCm+OcF6cd+dj3lpw8plD+APn8wcRqo2r</latexit>z1<latexit sha1_base64="12Jo8gJdiW7DRNkK6glxVvClPHE=">AAAB6nicbVDLTgJBEOzFF+IL9ehlIjHxRHaVqEeiF48Y5ZHAhswOA0yYnd3M9Jrghk/w4kFjvPpF3vwbB9iDgpV0UqnqTndXEEth0HW/ndzK6tr6Rn6zsLW9s7tX3D9omCjRjNdZJCPdCqjhUiheR4GSt2LNaRhI3gxGN1O/+ci1EZF6wHHM/ZAOlOgLRtFK90/d826x5JbdGcgy8TJSggy1bvGr04tYEnKFTFJj2p4bo59SjYJJPil0EsNjykZ0wNuWKhpy46ezUyfkxCo90o+0LYVkpv6eSGlozDgMbGdIcWgWvan4n9dOsH/lp0LFCXLF5ov6iSQYkenfpCc0ZyjHllCmhb2VsCHVlKFNp2BD8BZfXiaNs7J3Ua7cVUrV6yyOPBzBMZyCB5dQhVuoQR0YDOAZXuHNkc6L8+58zFtzTjZzCH/gfP4AFLKNrQ==</latexit>z3Random subspace 2<latexit sha1_base64="ikzi6YRz8zRNydAzPBPL8udRcSs=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK9gPaUDbbSbt0swm7G6GG/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVDBssFrFqB1Sj4BIbhhuB7UQhjQKBrWB0M/Vbj6g0j+WDGSfoR3QgecgZNVa6f+p5vXLFrbozkGXi5aQCOeq98le3H7M0QmmYoFp3PDcxfkaV4UzgpNRNNSaUjegAO5ZKGqH2s9mpE3JilT4JY2VLGjJTf09kNNJ6HAW2M6JmqBe9qfif10lNeOVnXCapQcnmi8JUEBOT6d+kzxUyI8aWUKa4vZWwIVWUGZtOyYbgLb68TJpnVe+ien53Xqld53EU4QiO4RQ8uIQa3EIdGsBgAM/wCm+OcF6cd+dj3lpw8plD+APn8wcRqo2r</latexit>z1<latexit sha1_base64="39CbZgeH7idn5xafE3bLj1DQTyg=">AAAB6nicbVDLTgJBEOzFF+IL9ehlIjHxRHYJUY9ELx4xyiOBDZkdemHC7OxmZtYECZ/gxYPGePWLvPk3DrAHBSvppFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsGG4EthOFNAoEtoLRzcxvPaLSPJYPZpygH9GB5CFn1Fjp/qlX6RVLbtmdg6wSLyMlyFDvFb+6/ZilEUrDBNW647mJ8SdUGc4ETgvdVGNC2YgOsGOppBFqfzI/dUrOrNInYaxsSUPm6u+JCY20HkeB7YyoGeplbyb+53VSE175Ey6T1KBki0VhKoiJyexv0ucKmRFjSyhT3N5K2JAqyoxNp2BD8JZfXiXNStm7KFfvqqXadRZHHk7gFM7Bg0uowS3UoQEMBvAMr/DmCOfFeXc+Fq05J5s5hj9wPn8AEy6NrA==</latexit>z2<latexit sha1_base64="BX0An3IYcXwaybL6ERwZyxxUyaw=">AAAB6nicbVDLTgJBEOzFF+IL9ehlIjHxRHYNPo5ELx4xyiOBDZkdGpgwO7uZmTXBDZ/gxYPGePWLvPk3DrAHBSvppFLVne6uIBZcG9f9dnIrq2vrG/nNwtb2zu5ecf+goaNEMayzSESqFVCNgkusG24EtmKFNAwENoPRzdRvPqLSPJIPZhyjH9KB5H3OqLHS/VP3vFssuWV3BrJMvIyUIEOtW/zq9CKWhCgNE1TrtufGxk+pMpwJnBQ6icaYshEdYNtSSUPUfjo7dUJOrNIj/UjZkobM1N8TKQ21HoeB7QypGepFbyr+57UT07/yUy7jxKBk80X9RBATkenfpMcVMiPGllCmuL2VsCFVlBmbTsGG4C2+vEwaZ2Xvoly5q5Sq11kceTiCYzgFDy6hCrdQgzowGMAzvMKbI5wX5935mLfmnGzmEP7A+fwBF7qNrw==</latexit>z5Random subspace 3Model 1Model 2Model 3Model-agnosticensemblePrediction
LossGradient descent
<latexit sha1_base64="6bep49ohelvtTnmqsX9yEbsJyb0=">AAACPHicbVDBSiNBEO1xddXorlGPXhqD4MUwswT1KAriUdGokAmhplNjGnu6h+6aXcKQD/PiR3jbkxcPinj1bCfmoMaCph/vvaKqXpIr6SgM/wdTP6Znfs7OzVcWFn/9Xqour5w7U1iBTWGUsZcJOFRSY5MkKbzMLUKWKLxIrg+G+sVftE4afUb9HNsZXGmZSgHkqU71NE6M6rp+5r8YVN4DHitMCaw1/zifVLd4jORNGhIFnXLCMOCHnWotrIej4pMgGoMaG9dxp3oXd40oMtQkFDjXisKc2iVYkkLhoBIXDnMQ13CFLQ81ZOja5ej4Ad/wTJenxvqniY/Yjx0lZG64oHdmQD33VRuS32mtgtLddil1XhBq8T4oLRQnw4dJ8q60KEj1PQBhpd+Vix5YEOTzrvgQoq8nT4LzP/Vou944adT29sdxzLE1ts42WcR22B47YsesyQS7YffskT0Ft8FD8By8vFungnHPKvtUwesbv6WvnQ==</latexit>↵ ↵ ⌘r↵FUpdateFeature selectionprobabilities(= feature importances)01<latexit sha1_base64="d97NymydqQ62PASvRSbcHcjKmhQ=">AAACE3icbVDLSsNAFJ3UV62vqEs3g0WoIiWRooKboiAuK9gHNKXcTCft0MmDmYkQQv7Bjb/ixoUibt2482+ctF1o9cBwD+fcy9x73IgzqSzryygsLC4trxRXS2vrG5tb5vZOS4axILRJQh6KjguSchbQpmKK004kKPgup213fJX77XsqJAuDO5VEtOfDMGAeI6C01DePriuOG/KBTHxdsAM8GsFF6rgeTrLjvDojUGmSZYd9s2xVrQnwX2LPSBnN0Oibn84gJLFPA0U4SNm1rUj1UhCKEU6zkhNLGgEZw5B2NQ3Ap7KXTm7K8IFWBtgLhX6BwhP150QKvsyX1p0+qJGc93LxP68bK++8l7IgihUNyPQjL+ZYhTgPCA+YoETxRBMgguldMRmBAKJ0jCUdgj1/8l/SOqnap9Xaba1cv5zFUUR7aB9VkI3OUB3doAZqIoIe0BN6Qa/Go/FsvBnv09aCMZvZRb9gfHwDA2GeRg==</latexit>F(↵;y,ˆy)
<latexit sha1_base64="HaqdKKn65ktAPUjwmw9oPZlEon4=">AAACEnicbVDLSgNBEJz1GeNr1aOXwSAkIGFXgnoM8eIxgnlANiy9k0kyyeyDmVkhLvkGL/6KFw+KePXkzb9xNtmDJhY0FFXddHd5EWdSWda3sbK6tr6xmdvKb+/s7u2bB4dNGcaC0AYJeSjaHkjKWUAbiilO25Gg4Huctrzxdeq37qmQLAzu1CSiXR8GAeszAkpLrll6cEfYkczHjg9qSIAntWnRAR4NwR2VzrDTDwVwjkeuWbDK1gx4mdgZKaAMddf8cnohiX0aKMJByo5tRaqbgFCMcDrNO7GkEZAxDGhH0wB8KrvJ7KUpPtVKD+vdugKFZ+rviQR8KSe+pzvTu+Wil4r/eZ1Y9a+6CQuiWNGAzBf1Y45ViNN8cI8JShSfaAJEMH0rJkMQQJROMa9DsBdfXibN87J9Ua7cVgrVWhZHDh2jE1RENrpEVXSD6qiBCHpEz+gVvRlPxovxbnzMW1eMbOYI/YHx+QMKbZ0U</latexit>zj⇠B(↵j),8j
<latexit sha1_base64="HaqdKKn65ktAPUjwmw9oPZlEon4=">AAACEnicbVDLSgNBEJz1GeNr1aOXwSAkIGFXgnoM8eIxgnlANiy9k0kyyeyDmVkhLvkGL/6KFw+KePXkzb9xNtmDJhY0FFXddHd5EWdSWda3sbK6tr6xmdvKb+/s7u2bB4dNGcaC0AYJeSjaHkjKWUAbiilO25Gg4Huctrzxdeq37qmQLAzu1CSiXR8GAeszAkpLrll6cEfYkczHjg9qSIAntWnRAR4NwR2VzrDTDwVwjkeuWbDK1gx4mdgZKaAMddf8cnohiX0aKMJByo5tRaqbgFCMcDrNO7GkEZAxDGhH0wB8KrvJ7KUpPtVKD+vdugKFZ+rviQR8KSe+pzvTu+Wil4r/eZ1Y9a+6CQuiWNGAzBf1Y45ViNN8cI8JShSfaAJEMH0rJkMQQJROMa9DsBdfXibN87J9Ua7cVgrVWhZHDh2jE1RENrpEVXSD6qiBCHpEz+gVvRlPxovxbnzMW1eMbOYI/YHx+QMKbZ0U</latexit>zj⇠B(↵j),8j
<latexit sha1_base64="HaqdKKn65ktAPUjwmw9oPZlEon4=">AAACEnicbVDLSgNBEJz1GeNr1aOXwSAkIGFXgnoM8eIxgnlANiy9k0kyyeyDmVkhLvkGL/6KFw+KePXkzb9xNtmDJhY0FFXddHd5EWdSWda3sbK6tr6xmdvKb+/s7u2bB4dNGcaC0AYJeSjaHkjKWUAbiilO25Gg4Huctrzxdeq37qmQLAzu1CSiXR8GAeszAkpLrll6cEfYkczHjg9qSIAntWnRAR4NwR2VzrDTDwVwjkeuWbDK1gx4mdgZKaAMddf8cnohiX0aKMJByo5tRaqbgFCMcDrNO7GkEZAxDGhH0wB8KrvJ7KUpPtVKD+vdugKFZ+rviQR8KSe+pzvTu+Wil4r/eZ1Y9a+6CQuiWNGAzBf1Y45ViNN8cI8JShSfaAJEMH0rJkMQQJROMa9DsBdfXibN87J9Ua7cVgrVWhZHDh2jE1RENrpEVXSD6qiBCHpEz+gVvRlPxovxbnzMW1eMbOYI/YHx+QMKbZ0U</latexit>zj⇠B(↵j),8j<latexit sha1_base64="g7WhyBsuq3/wYWykyEO9McFYZhQ=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkaI8FLx4r2A9oQ9lsN+3SzSbuToQQ+ie8eFDEq3/Hm//GpM1BWx8MPN6bYWaeF0lh0La/rdLG5tb2Tnm3srd/cHhUPT7pmjDWjHdYKEPd96jhUijeQYGS9yPNaeBJ3vNmt7nfe+LaiFA9YBJxN6ATJXzBKGZSfzilmCbzyqhas+v2AmSdOAWpQYH2qPo1HIcsDrhCJqkxA8eO0E2pRsEkn1eGseERZTM64YOMKhpw46aLe+fkIlPGxA91VgrJQv09kdLAmCTwss6A4tSsern4nzeI0W+6qVBRjFyx5SI/lgRDkj9PxkJzhjLJCGVaZLcSNqWaMswiykNwVl9eJ92runNdb9w3aq1mEUcZzuAcLsGBG2jBHbShAwwkPMMrvFmP1ov1bn0sW0tWMXMKf2B9/gDoz4/d</latexit>ˆy
<latexit sha1_base64="JGjRuUVipLIERhC8TkVaroYa2W8=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkaI8FLx4r2A9oQ5lsN+3SzSbuboQS+ie8eFDEq3/Hm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUUdaisYhVN0DNBJesZbgRrJsohlEgWCeY3M79zhNTmsfywUwT5kc4kjzkFI2Vun0UyRgH3qBccavuAmSdeDmpQI7moPzVH8Y0jZg0VKDWPc9NjJ+hMpwKNiv1U80SpBMcsZ6lEiOm/Wxx74xcWGVIwljZkoYs1N8TGUZaT6PAdkZoxnrVm4v/eb3UhHU/4zJJDZN0uShMBTExmT9PhlwxasTUEqSK21sJHaNCamxEJRuCt/ryOmlfVb3rau2+VmnU8ziKcAbncAke3EAD7qAJLaAg4Ble4c15dF6cd+dj2Vpw8plT+APn8we0vY+7</latexit>↵1
<latexit sha1_base64="CeMVUCE+T30emaesVpTUv5Yatug=">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4Kkkp2mPBi8cK9gPaUCbbTbt0s4m7G6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqKGvRWMSqG6BmgkvWMtwI1k0UwygQrBNMbud+54kpzWP5YKYJ8yMcSR5yisZK3T6KZIyD6qBUdivuAmSdeDkpQ47moPTVH8Y0jZg0VKDWPc9NjJ+hMpwKNiv2U80SpBMcsZ6lEiOm/Wxx74xcWmVIwljZkoYs1N8TGUZaT6PAdkZoxnrVm4v/eb3UhHU/4zJJDZN0uShMBTExmT9PhlwxasTUEqSK21sJHaNCamxERRuCt/ryOmlXK951pXZfKzfqeRwFOIcLuAIPbqABd9CEFlAQ8Ayv8OY8Oi/Ou/OxbN1w8pkz+APn8we2QY+8</latexit>↵2
<latexit sha1_base64="zolvWTduJs0o5rmCEtRzJakuNA8=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0m0aI8FLx4r2FpoQ5lsN+3SzSbuboQS+ie8eFDEq3/Hm//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCoreNUUdaisYhVJ0DNBJesZbgRrJMohlEg2EMwvpn5D09MaR7LezNJmB/hUPKQUzRW6vRQJCPsX/bLFbfqzkFWiZeTCuRo9stfvUFM04hJQwVq3fXcxPgZKsOpYNNSL9UsQTrGIetaKjFi2s/m907JmVUGJIyVLWnIXP09kWGk9SQKbGeEZqSXvZn4n9dNTVj3My6T1DBJF4vCVBATk9nzZMAVo0ZMLEGquL2V0BEqpMZGVLIheMsvr5L2RdW7qtbuapVGPY+jCCdwCufgwTU04Baa0AIKAp7hFd6cR+fFeXc+Fq0FJ585hj9wPn8At8WPvQ==</latexit>↵3
<latexit sha1_base64="T2Tn/q/4DRxu/UK0ZpPK6m3j8FU=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkaI8FLx4r2A9oQ5lsN+3SzSbuboQS+ie8eFDEq3/Hm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUUdaisYhVN0DNBJesZbgRrJsohlEgWCeY3M79zhNTmsfywUwT5kc4kjzkFI2Vun0UyRgHtUG54lbdBcg68XJSgRzNQfmrP4xpGjFpqECte56bGD9DZTgVbFbqp5olSCc4Yj1LJUZM+9ni3hm5sMqQhLGyJQ1ZqL8nMoy0nkaB7YzQjPWqNxf/83qpCet+xmWSGibpclGYCmJiMn+eDLli1IipJUgVt7cSOkaF1NiISjYEb/XlddK+qnrX1dp9rdKo53EU4QzO4RI8uIEG3EETWkBBwDO8wpvz6Lw4787HsrXg5DOn8AfO5w+5SY++</latexit>↵4
<latexit sha1_base64="lrz7uxFXSpR9ddIhc9DOFtqNKQg=">AAAB73icbVDLSgNBEOyNrxhfUY9eBoPgKeyKjxwDXjxGMA9IltA7mU2GzM6uM7NCWPITXjwo4tXf8ebfOEn2oIkFDUVVN91dQSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRS8epoqxJYxGrToCaCS5Z03AjWCdRDKNAsHYwvp357SemNI/lg5kkzI9wKHnIKRordXookhH2r/rlilt15yCrxMtJBXI0+uWv3iCmacSkoQK17npuYvwMleFUsGmpl2qWIB3jkHUtlRgx7Wfze6fkzCoDEsbKljRkrv6eyDDSehIFtjNCM9LL3kz8z+umJqz5GZdJapiki0VhKoiJyex5MuCKUSMmliBV3N5K6AgVUmMjKtkQvOWXV0nroupdVy/vLyv1Wh5HEU7gFM7Bgxuowx00oAkUBDzDK7w5j86L8+58LFoLTj5zDH/gfP4Aus2Pvw==</latexit>↵5
Figure 1: Parametric Random Subspace (PRS). A model-agnostic ensemble is built, in which each
base model is learned from a feature subset sampled according to a Bernoulli distribution with parameters
α. The training procedure consists in identifying the parameters αthat minimize any loss F(α;y,ˆ y)(which
may include a regularization term over α). This optimization problem is solved using gradient descent and
importance sampling.
1 Introduction
In supervised learning, ensemble approaches are popular techniques to improve the performance of any learn-
ing algorithm. The most prominent ensemble methods include averaging ensembles like Bagging (Breiman,
1996b), Random Subspace (Ho, 1998), or Random Forest (Breiman, 2001), as well as boosting ensembles,
such as Adaboost (Freund & Schapire, 1997) or gradient boosting (Friedman, 2001). Both Random Forest
1Under review as submission to TMLR
and Random Subspace aggregate the predictions of base models that are randomized through a random
feature selection mechanism (with additional sample bootstrapping, similarly as in Bagging, in the case of
Random Forest). However, while Random Subspace is a model-agnostic approach, i.e. an approach that
can be combined with any type of base models, Random Forest is designed speciﬁcally for the aggregation
of decision trees. Indeed, in Random Subspace, feature randomization occurs at the level of the base model,
before training the latter, and can thus be combined with any base model. On the other hand, the feature
randomization used in Random Forest is designed speciﬁcally for decision trees: it occurs at the level of
the tree node, where a feature subset is randomly sampled before selecting the best split. Note that while
boosting is a model-agnostic approach, it is designed to aggregate weak models, and is hence typically used
with shallow decision trees.
One advantage of decision trees is their interpretability. Their node splitting strategy is akin to an embedded
feature selection mechanism that makes them robust to irrelevant features and feature importance scores
can be furthermore easily derived from a trained tree model to quantitatively assess the selected features
(Breiman et al., 1984b). These characteristics carry over when decision trees are used as based learners
with the aforementioned ensemble methods, as importance scores can be averaged over all the trees in the
ensemble, which furthermore increases their stability. This arguably has participated to the popularity of
tree-based ensemble methods for the prediction of tabular data Grinsztajn et al. (2022). On the other hand,
this interpretability through feature selection and ranking is obviously lost when model-agnostic ensemble
methods are applied with other base learners that are not inherently interpretable.
In this paper, we propose a novel ensemble approach for supervised learning (Figure 1) that is fully model-
agnostic, i.e., makes no assumption about the nature of the base models, and naturally embeds a feature
selection mechanism and provides feature importance scores, irrespectively of the choice of the base model.
The proposed approach is based on a parametric version of Random Subspace (denoted PRS), in which each
base model is learned from a feature subset sampled according to a Bernoulli distribution. We formulate
the training procedure as an optimization problem where the goal is to identify the parameters of the
Bernoulli distribution that minimize the generalization error of the ensemble model, and we show that this
optimization problem can be solved using gradient descent even when the base models are not diﬀerentiable.
The optimization of the Bernoulli distribution is however intractable, as the computation of the exact
output of the full ensemble model would require the training of one model for each possible feature subset.
To render the parameter optimization tractable, we use Monte Carlo sampling to approximate the ensemble
model output. We further use an importance sampling approach that circumvents frequent re-training of
the base models after each update of the gradient descent.
The degree of randomization in our parametric Random Subspace is automatically tuned through the op-
timization of the feature selection probabilities. This is an advantage over the standard Random Subspace
approach, where the degree of randomization is controlled by a hyper-parameter. Furthermore, the opti-
mized feature selection probabilities can be interpreted as feature importance scores. Our algorithm can also
easily incorporate any diﬀerentiable regularization term to impose constraints on these importance scores.
We show the good performance of the proposed approach, both in terms of prediction and feature ranking,
on simulated and real-world datasets. We also show that PRS can be successfully used for the reconstruction
of gene regulatory networks.
2 Methods
We assume a standard supervised learning setting, where we have at our disposal a learning set containing
Ninput-output pairs {(xi,yi)}N
i=1drawn from an unknown probability distribution. Let us denote by M
the number of input variables. The output ycan be either continuous (regression problem) or discrete
(classiﬁcation problem). Our goal is to train a model-agnostic predictive model, while deriving for each
input variable a score that measures its importance for the output prediction.
Toachievethisgoal,webuildupontheRandomSubspaceapproach(RS,Ho,1998). RSconsistsinlearningan
ensemble of predictive models, where each model is built from a randomly chosen subset of Kinput variables
(withK <M), sampled according to a uniform distribution. Here, instead of using a uniform distribution,
we adopt a parametric distribution for the selection of the input features, and feature importance scores
2Under review as submission to TMLR
are derived through the identiﬁcation of the distribution parameters that yield the lowest generalization
error. In the following, after introducing the parametric RS model (Section 2.1), we show how this model
can be trained in a tractable way (Section 2.2) and we discuss our approach with respect to related works
(Section 2.3).
2.1 The Parametric Random Subspace approach (PRS)
Let us denote by z= (z1,...,zM)/latticetop∈{0,1}Ma binary vector of length Mencoding a subset of selected
input variables: zj= 1if thej-th variable is selected and zj= 0otherwise,∀j∈{1,...,M}. In the proposed
PRS approach, each indicator variable zjis assumed to follow a Bernoulli distribution with parameter αj.
The probability mass function for zis then given by:
p(z|α) =M/productdisplay
j=1αzj
j(1−αj)(1−zj), (1)
whereαj∈[0,1]is the probability of selecting the j-th variable and α= (α1,...,αM)/latticetop. LetZ=
{z1,z2,...,z|Z|}be the set of all the possible feature subsets, where |Z|= 2Mis the cardinality of Z.
We assume an ensemble method that consists in averaging base models trained independently of each other
using subsets of features drawn from p(z|α). Let us denote by Fsome functional space corresponding to a
given learning algorithm and by Fz⊆Fthe subset of functions from Fthat only depend on the variables
indicated by z. Letfzt∈Fztbe the base model learned by this learning algorithm from the feature subset
zt(∀t∈{1,...,|Z|}). Asymptotically, the prediction of the ensemble model for a given input xis given by:
E[fz(x)]p(z|α)=|Z|/summationdisplay
t=1p(zt|α)fzt(x). (2)
For a ﬁxed α, a practical approximation of E[fz(x)]p(z|α)can be obtained by Monte-Carlo sampling, i.e.
by drawing Tfeature subsets from p(z|α)and then training a model from each of these subsets, using the
chosen learning algorithm (Figure 1). If all the αj’s are equal, the resulting ensemble method is very close
to the standard RS approach, the only diﬀerence being that the number of selected features will be slightly
randomized from one model to the next. In this work, we would like however to identify the parameters α
that yield the most accurate expected predictions E[fz(x)]p(z|α)over our training set. Given a loss function
L, the corresponding optimization problem can be formulated as follows:
min
α∈[0,1]MF(α),
whereF(α) =1
NN/summationdisplay
i=1L/parenleftbig
yi,E[fz(xi)]p(z|α)/parenrightbig
.(3)
A nice advantage is that the selection probabilities αafter optimization can be interpreted as measures
of variable importances: useless variables are expected to get low selection probabilities, while the most
important ones are expected to get selection probabilities close to 1.
2.2 Training the PRS model
We propose to solve the optimization problem in Eq (3) using gradient descent. More speciﬁcally, since αj
must be between 0 and 1, ∀j, we use the projected gradient descent technique, where αis projected into
the space [0,1]Mafter each step of the gradient descent. In the following, we ﬁrst derive the analytical
formulation of the gradient of the objective function. We then explain how to estimate this gradient by
using Monte Carlo sampling and show how to incrementally update this gradient estimate using importance
sampling. Precise pseudo-code of the algorithm is given in Appendix A and our Python implementation is
available in the supplementary material1.
1Our Python code and the datasets used in this paper will be available on GitHub, should the paper be accepted.
3Under review as submission to TMLR
2.2.1 Computing the gradient
Assuming that the loss function Lis diﬀerentiable, the gradient of the objective function F(α)w.r.t. αis:
∇αF(α) =1
NN/summationdisplay
i=1dL
dE[fz(xi)]p(z|α)∇αE[fz(xi)]p(z|α). (4)
To compute the gradient ∇αE[fz(xi)]p(z|α), we resort to the score function approach (Rubinstein & Shapiro,
1993), also known as the REINFORCE method (Williams, 1992) or the likelihood-ratio method (Glynn,
1990), which allows us to express the gradient of an expectation as an expectation itself (see Appendix B):
∇αE[fz(xi)]p(z|α)=E[fz(xi)∇αlogp(z|α)]p(z|α). (5)
A major advantage of the score function approach is that, in order to compute the gradient in Eq. (5), only
the distribution p(z|α)needs to be diﬀerentiable, and not the base model fz. By using the score function
method with the Bernoulli distribution in Eq. (1), the j-th component of the gradient is given by (see
Appendix B):
∂E[fz(xi)]p(z|α)
∂αj=fα
j,1(xi)−fα
j,0(xi) (6)
where, for the simplicity of notations, we have deﬁned:
fα
j,0(xi) =E[fz(xi)|zj= 0]p(z−j|α−j), (7)
fα
j,1(xi) =E[fz(xi)|zj= 1]p(z−j|α−j), (8)
withz−j=z\zj,α−j=α\αj.fα
j,0(resp.fα
j,1) is thus the expected output of a model that does not take
(resp. takes) as input the j-th variable. We thus ﬁnally have:
∂F
∂αj=1
NN/summationdisplay
i=1dL
dE[fz(xi)]p(z|α)/parenleftbig
fα
j,1(xi)−fα
j,0(xi)/parenrightbig
. (9)
The above derivative can be easily interpreted in the context of a gradient descent approach. For example,
whendL
dE[fz(xi)]p(z|α)is positive, the loss Ldecreases with a lower model prediction E[fz(xi)]p(z|α). This
means that if fα
j,0(xi)< fα
j,1(xi), the model without variable jwill give a lower loss than the model with
variablej. In that case, the derivative∂F
∂αjis positive and a gradient descent step (i.e. αj←αj−η∂F
∂αj,
whereηis the learning rate) will decrease the value of αj.
2.2.2 Estimating the gradient
Given the current selection probabilities α, the exact computation of the expectation in Eq. (5) is obviously
intractable as it requires training |Z|models. An unbiased estimation can be obtained by Monte Carlo
sampling, i.e. by averaging over Tsubsets of features z(t)sampled from p(z|α):
E[fz(xi)∇αlogp(z|α)]p(z|α)/similarequal1
TT/summationdisplay
t=1fz(t)(xi)∇αlogp(z(t)|α), (10)
wherefz(t)is the model trained using only as inputs the features in the subset z(t).
It remains to be explained on which data the models fz(t)are trained. Using the same Nsamples as the ones
used to compute the gradient in Eq. (4) would lead to biased predictions fz(xi)and hence to overﬁtting.
We thus use a batch gradient descent approach, in which a subset of the training dataset (e.g. 10% of the
samples) are used for computing the gradient, while the remaining samples are used for training the base
models. Note that in the case where z(t)is the empty set, which can happen when all the αjparameters are
very low, we set fz(t)to a constant model that always returns the mean value of the output in the training
set (for regression problems) or the majority class (for classiﬁcation problems).
4Under review as submission to TMLR
Although the gradient estimator in Eq. (10) is unbiased, it is known to suﬀer from high variance, which can
make the gradient descent optimization very unstable. One common solution to reduce this variance is to
use the fact that, for any constant b, we have (see Appendix C.1):
E[fz(xi)∇αlogp(z|α)]p(z|α)=E[(fz(xi)−b)∇αlogp(z|α)]p(z|α). (11)
The constant bis called a baselineand its value will aﬀect the variance of the estimator. In our approach,
we use the optimal value of b, i.e. the value that minimizes the variance, which is (see Appendix C.2):
b=E[(∇αlogp(z|α))2fz(xi)]p(z|α)
E[(∇αlogp(z|α))2]p(z|α)/similarequal/summationtextT
t=1(∇αlogp(z(t)|α))2fz(t)(xi)
/summationtextT
t=1(∇αlogp(z(t)|α))2. (12)
TableS7shows,onsimulatedproblems,themeritsofapplyingthisvariancereductionapproach,asittypically
results in a better performance (in particular for the regression problems), both in terms of prediction and
feature ranking quality, and smaller feature subsets.
2.2.3 Updating the gradient
The above procedure allows us to estimate the gradient and to perform one gradient descent step. After this
step, the distribution parameters αare updated to β=α−η∇Fand we must hence compute the gradient
∇βE[fz(xi)]p(z|β)in order to do the next step. To be able to compute the approximation in Eq (10), new
models{fz(t)}T
t=1must thus in principle be learned by sampling each z(t)from the new distribution p(z|β).
This would result in a very computationally expensive algorithm where new models are learned after each
parameter update.
In order to estimate the eﬀect of a change in the feature selection probabilities αwithout learning new
models, we use the importance sampling approximation of the expectation. Given a new vector of feature
selection probabilities β/negationslash=α, any expectation under p(z|β)can be approximated through p(z|α). We have,
for any input xi:
E[(fz(xi)−b)∇βlogp(z|β)]p(z|β)=|Z|/summationdisplay
t=1p(z|β)
p(z|α)p(z|α)(fzt(xi)−b)∇βlogp(z|β) (13)
=E/bracketleftbiggp(z|β)
p(z|α)(fz(xi)−b)∇βlogp(z|β)/bracketrightbigg
p(z|α)(14)
/similarequal1
TT/summationdisplay
t=1p(z(t)|β)
p(z(t)|α)(fz(t)(xi)−b)∇βlogp(z(t)|β), (15)
where the feature subsets {z(t)}T
t=1in Eq (15) have been sampled from p(z|α). This approximation can thus
be computed for any βby using the models {fz(t)}T
t=1obtained when the z(t)were sampled from p(z|α).
As shown by Eq.(15), the importance sampling approximation consists of a weighted average over Tfeature
subsets z(t), using weights wt=p(z(t)|β)
p(z(t)|α). When βbecomes very diﬀerent from α, some of the feature subsets
will be hardly used in the average because they will have a very low weight wt. The eﬀective number of used
feature subsets can be computed as (Doucet et al., 2001):
Teff=/parenleftBig/summationtextT
t=1wt/parenrightBig2
/summationtextT
t=1w2
t. (16)
With imbalanced weights, the importance sampling approximation is equivalent to averaging over Teff
feature subsets. When Teffis too low, the gradient estimation thus becomes unreliable. When this happens,
we trainTnew models fz(t)by sampling the feature subsets z(t)from the current distribution p(z|β). In
practice, new models are trained as soon as Teffdrops below 0.9T.
5Under review as submission to TMLR
2.3 Discussion
The PRS algorithm has the advantage of being model-agnostic in that any supervised learning method can be
used to ﬁt the fz(t)models. Despite the use of gradient descent, no hypothesis of diﬀerentiability is required
for the model family. The framework can also be easily adapted to any diﬀerentiable loss and regularization
term.
Computational complexity Once the models are trained, the computation of the gradient is linear with
respect to the number Nof samples, the number Mof features and the number Tof base models in the
ensemble. The costliest step of the algorithm is the construction of the base models. The complexity of
the construction of the models depends on the type of model, but note that each model is grown only from
a potentially small subset of features. Figure S4 in the appendix shows the computing times of PRS on
simulated problems, for diﬀerent values of NandM.
Regularization While we have not used any regularization term in (3), incorporating one is straight-
forward. A natural regularization term to enforce sparsity could be simply the sum/summationtextM
j=1αj, which can
be nicely interpreted as E[||z||0]p(z|α), i.e., the average size of the subsets drawn from p(z|α). Adding this
term to (3) with a regularization coeﬃcient λwould simply consists in adding λto the gradient in (4).
We did not systematically include such regularization in our experiments below to reduce the number of
hyper-parameters. Despite the lack of regularization, PRS has a natural propensity for selecting few fea-
tures. Incorporating a useless feature jwill indeed often deteriorate the quality of the predictions and lead
to a decrease of the corresponding αj. Note however that the sparsity of the resulting selection weights
will depend on the robustness of the learning algorithm to the presence of irrelevant features. This will be
illustrated in our experiments. Besides sparsity, we will also exploit more sophisticated regularization terms,
for MNIST (where we will use a regularization term that enforces spatial smoothness) and the inference of
gene regulatory networks (where we will enforce modular networks).
Related works Our method has direct connections with the Random Subspace (RS) ensemble method
(Ho, 1998). In addition to providing a feature ranking, it has the obvious added ﬂexibility w.r.t. RS that
the feature sampling distribution (and thus also the subspace size) is automatically adapted to improve
performance. Another close work is the RaSE method (Tian & Feng, 2021), which iteratively samples a
large population of feature subsets, trains models from them and selects the Tbest ones according to a
chosen criterion (e.g., the cross-validation prediction performance). The selection probability αjof each
feature is then updated as the proportion of times it appears in the Tbest feature subsets. RaSE and PRS
are thus similar in the sense that they both iteratively sample feature subsets from an explicit probability
distribution with parameters α(although the sampling distribution is diﬀerent between the two approaches)
and update the latter. One major diﬀerence is that RaSE implicitly minimizes the expected value of the loss
function:
min
αE/bracketleftBigg
1
NN/summationdisplay
i=1L(yi,fz(xi))/bracketrightBigg
p(z|α), (17)
while we are trying to minimize the loss of the ensemble model E[fz(x)]p(z|α)(see Eq.(3)). Both approaches
also greatly diﬀer in the optimization technique: RaSE iteratively updates the parameters αfrom the best
feature subsets in the current population, while PRS is based on gradient descent and importance sampling.
Furthermore, as explained above, PRS allows the direct regularization of the parameters α, while such
regularization is not possible in RaSE. Finally, RaSE samples the size of each feature subset from a uniform
distribution whose upper bound is a hyper-parameter set by the user, while the subspace size is automatically
adapted in our approach. Both approaches will be empirically compared in Section 3.
Our optimization procedure has also some links with variational optimization (VO, Staines & Barber, 2013).
VO is a general technique for minimizing a function G(z)that is non-diﬀerentiable or combinatorial. It is
based on the bound:
min
z∈ZG(z)≤E[G(z)]p(z|α)=F(α), (18)
6Under review as submission to TMLR
Instead of minimizing Gwith respect to z, one can thus minimize the upper bound Fwith respect to α.
ReplacingGin Eq. (18) with the loss of an individual model fzyields:
min
z∈Z1
NN/summationdisplay
i=1L(yi,fz(xi))≤E/bracketleftBigg
1
NN/summationdisplay
i=1L(yi,fz(xi))/bracketrightBigg
p(z|α), (19)
where the left-hand term is the deﬁnition of the global feature selection problem, which is combinatorial over
the discrete values of z. Instead of directly solving the feature selection problem, one can thus minimize an
upper bound of it, by minimizing the expected value of the loss over the continuous α, e.g. using gradient
descent. Like in VO, the formulation in (3) allows us to use gradient descent optimization despite the fact
that the models fzare not necessarily diﬀerentiable. Note however that our goal is not to solve the feature
selection problem, but to train an ensemble and thus the function F(α)in Eq. (3), which is the loss of the
ensemble, is exactlywhat we want to minimize (and not an upper bound).
Several works have used gradient descent to solve the feature selection problem in the left-hand term of
Eq. (19), by using a continuous relaxation of the discrete variables z(Sheth & Fusi, 2020; Yamada et al.,
2020; Donà & Gallinari, 2021; Balin et al., 2019; Yang et al., 2022). However, these methods are designed to
be used with diﬀerentiable models (neural networks, polynomial models), so that both the feature selection
and the model parameters can be updated in a single gradient descent step, while PRS is model-agnostic.
Note that while PRS is a model-agnostic ensemble method, it is not an explanation (or post-hoc) method,
such as LIME (Ribeiro et al., 2016) or SHAP (Lundberg & Lee, 2017) for example. Methods such as LIME
or SHAP are designed to highlight the features that a pre-trained black-box model uses to produce its
predictions (locally or globally). They do not aﬀect the predictive performance of the models they try to
explain. PRS, on the other hand, produces an ensemble with hopefully improved predictive performance
and interpretability with respect to (and whatever) the base learning algorithm it is combined with.
3 Results
We compare below PRS against several baselines and state-of-the-art methods on simulated (Section 3.1) and
real (Section 3.2) problems. We then conduct two additional experiments, on MNIST (Section 3.3) and gene
network inference (Section 3.4), to highlight the beneﬁt of incorporating a problem-speciﬁc regularization
term.
As base model fz, we used either a CART decision tree (Breiman et al., 1984a), a k-nearest neighbors (kNN)
model (Altman, 1992) with k= 5or a support vector machine (SVM, Boser et al., 1992) with a radial basis
function kernel. All the hyper-parameters of these base models were set to the default values used in the
scikit-learn library (Pedregosa et al., 2011)
We report the predictive performance with the R2score for regression problems and the accuracy for clas-
siﬁcation problems. For PRS a ranking of features can be obtained by sorting them by decreasing value of
importances α. If the relevant variables are known, the feature ranking can be evaluated using the area
under the precision-recall curve (AUPR). A perfect ranking (i.e. all the relevant features have a higher
importance than the irrelevant ones) yields an AUPR of 1, while a random ranking has an AUPR close to
the proportion of relevant features.
We compare PRS to the following methods: the standard Random Subspace (RS), Random Forest (RF),
Gradient Boosting with Decision Trees (GBDT), and RaSE. Implementation details for all the methods are
provided in Appendix D.
3.1 Simulated Problems
We simulated four problems, for which the relevant features are known (see Appendix E.1 for the detailed
simulation protocol). Compared to single base models and RS, PRS yields higher prediction scores for all
the base models (Figure 2). The improvement of performance over RS is larger in the case of kNN and
SVM, compared to decision trees. This can be explained by the fact a decision tree, contrary to kNN and
7Under review as submission to TMLR
tree kNN SVM1.5
1.0
0.5
0.00.5R2 test scoreCheckerboard
Type
Single
RS
PRS
tree kNN SVM0.30.40.50.60.70.80.9R2 test scoreFriedman
tree kNN SVM0.50.60.70.80.91.0T est accuracy
Hypercube
tree kNN SVM0.50.60.70.80.9T est accuracyLinear
Figure 2: Predictive performance for single models versus RS and PRS ensembles. Performance
values areR2scores for the Checkerboard and Friedman problems, and accuracies for the Hypercube and
Linear problems. The boxplots summarize the values over 10 datasets.
SVM, has an inner feature selection mechanism and is hence able to maintain a good performance even in
the presence of irrelevant features. Therefore, for a given irrelevant feature j, the diﬀerence between fβ
j,0and
fβ
j,1(Eqs (7) and (8), respectively) will be lower in the case of trees, which can prevent the corresponding
αjto decrease towards zero during the gradient descent.
Note also that RS greatly improves over the single model only in the case of decision trees. The decision
tree being a model that is prone to high variance, its prediction performance is indeed usually improved by
using ensemble methods (Breiman, 1996a; 2001; Geurts et al., 2006). On the other hand, since kNN and
SVM have a suﬃciently low variance, their performance is not improved with a standard ensemble.
While the degree of randomization is controlled by the parameter K(i.e. the number of randomly sampled
features for each base model) in RS, it has the advantage to be automatically tuned in PRS. Table 1 indicates
the sum/summationtextM
j=1αj, which is equivalent to E[||z||0]p(z|α), i.e. the average number of selected variables per base
model. By comparing this average number to the optimal value of Kfor RS, we can see that PRS eﬀectively
selectsamuchlowernumberoffeatures, whilenoexplicitconstraintonsparsityisusedduringmodeltraining.
The average number of selected variables remains however slightly higher than the actual number of relevant
features, indicating that a certain degree of randomization is introduced during model construction.
Overall, PRSoutperformsRFandGBDTbothintermsofpredictiveperformanceandfeatureranking(Table
2), the best performance being obtained with kNN and SVM. For some problems (e.g. Checkerboard), PRS
is particularly better than RF and GBDT in the presence of a high number of irrelevant features (Figures S5
and S6).
Compared to RaSE, PRS yields an equivalent performance, with equivalent feature subset sizes (Table S8),
while being much less computationally expensive. Indeed, T×B×10 = 500,000base models must be trained
at each iteration of RaSE (see Appendix D.4), while in PRS the highest number of trained base models is
320,000 for the whole run of the algorithm (Table S9).
8Under review as submission to TMLR
Table 1:Number of features used per base model , i.e. for RS: the number Kof randomly sampled
features (optimized on the validation set), and for PRS: the sum/summationtextM
j=1αj. Values are mean and standard
deviation over 10 datasets.
Model Checkerboard Friedman Hypercube Linear
tree RS 53.50 ±52.13 132.00±24.49 133.20±63.76 145.60±63.17
PRS 7.29±1.32 7.82±0.75 12.10±3.04 12.68±2.95
kNN RS 97.10 ±86.99 90.60±28.15 61.70±47.24 99.10±80.20
PRS 6.49±1.91 5.49±0.47 6.83±1.20 10.12±2.30
SVM RS 111.00 ±126.96 305.00±0.00 131.80±95.29 227.30±84.00
PRS 4.85±0.55 7.28±1.31 13.54±3.01 19.97±1.90
Table 2:Comparison to RF and GBDT. We report here the prediction score on the test set ( R2score
or accuracy) and the feature ranking quality (AUPR). Values are mean and standard deviation over 10
datasets. Highest scores are indicated in bold type.
RF GBDT PRS - tree PRS - kNN PRS - SVM
Checkerboard R2-0.03±0.05 -0.09±0.10 0.29±0.14 0.60±0.060.62±0.07
AUPR 0.44 ±0.22 0.40±0.24 0.60±0.23 0.92±0.140.98±0.06
Friedman R20.73±0.04 0.86±0.03 0.83±0.03 0.88±0.030.90±0.05
AUPR 0.68 ±0.03 0.87±0.04 0.95±0.041.00±0.000.98±0.05
Hypercube Accuracy 0.85 ±0.07 0.86±0.06 0.88±0.040.90±0.060.88±0.05
AUPR 0.92 ±0.12 0.90±0.100.97±0.060.94±0.09 0.90±0.14
Linear Accuracy 0.77 ±0.06 0.85±0.04 0.78±0.03 0.88±0.030.92±0.03
AUPR 0.68 ±0.13 0.70±0.15 0.67±0.13 0.73±0.120.80±0.10
Finally, the eﬃciency of the importance sampling approach can be observed in Table S10. This table shows
the performance and training times of PRS-SVM, for diﬀerent thresholds on the eﬀective number of models
Teffas deﬁned in Eq. (16). We recall that in PRS, new base models are trained only when Teffdrops below
the chosen threshold. Setting the threshold to Tcorresponds to the case where we do not use the importance
sampling approach and new models are trained at each epoch. This signiﬁcantly increases the computing
time, with no improvement in terms of prediction score and AUPR, compared to our default threshold
0.9T. Lowering the threshold allows to decrease the training time, and there is a strong degradation of the
performance only for small threshold values ( 0.3Tand0.5T).
3.2 Real-world problems
We compared the diﬀerent approaches on benchmarks containing real-world datasets:
•Thetabularbenchmark of Grinsztajn et al. (2022). This benchmark contains 55 tabular datasets
from various domains, split into four groups (regression or classiﬁcation, with or without categorical
features). Dataset sizes are indicated in Tables S11 and S12. For each dataset, we randomly choose
3,000 samples, that we split to compose the training, validation and test sets (1,000 samples each).
The results of the diﬀerent approaches are then averaged over 10 such random samplings.
•Biological, classiﬁcation datasets from the scikit-feature repository (Li et al., 2018). These datasets
(also tabular) have the particularity to have very few ( ∼100) samples for several thousands features.
Among the biological datasets available in the repository, we ﬁltered out datasets and classes in order
to have only datasets with at least 30 samples per class. The ﬁnal dataset sizes are indicated in
Table S12. Given the small dataset sizes, we estimate the prediction accuracies on these datasets
with 5-fold cross-validation, and for each fold we use 80% of the training set to train the models and
the remaining 20% as validation set. Given the very high number of features in these datasets, we
add in the objective function of PRS a regularization term that enforces sparsity (see Section 2.3),
9Under review as submission to TMLR
Table 3: Normalized prediction scores on the real benchmarks. To aggregate the performance
across the datasets of each benchmark, we ﬁrst normalize the performance score ( R2or accuracy) between
0 and 1 via an aﬃne renormalization between the worse- and top-performing methods for each dataset. The
normalized scores are then averaged over the diﬀerent datasets and 10 data subsamplings (for the tabular
datasets) or 5 cross-validation folds (for the scikit-feature datasets). For each benchmark, the highest
performance is indicated in bold type.
Tabular Tabular Scikit-feature
Model Regression Classiﬁcation Classiﬁcation
tree Single 0.36 ±0.41 0.17±0.22 0.19±0.25
RS 0.82±0.18 0.69±0.22 0.67±0.26
RaSE 0.83±0.18 0.70±0.25 0.60±0.23
PRS 0.92±0.11 0.80±0.16 0.64±0.28
kNN Single 0.50 ±0.33 0.15±0.20 0.38±0.29
RS 0.65±0.26 0.52±0.21 0.35±0.26
RaSE 0.92±0.09 0.67±0.24 0.71±0.26
PRS 0.95±0.08 0.79±0.20 0.67±0.27
SVM Single 0.48 ±0.37 0.52±0.24 0.53±0.26
RS 0.49±0.38 0.54±0.23 0.49±0.26
RaSE 0.72±0.30 0.63±0.25 0.67±0.29
PRS 0.77±0.26 0.56±0.280.79±0.26
RF 0.93±0.090.84±0.20 0.61±0.28
GBDT0.96±0.090.83±0.21 0.60±0.22
and we select the value of the regularization coeﬃcient λ(among {0.0001, 0.001, 0.01, 0.1}) that
maximizes the accuracy on the validation set.
To aggregate the prediction performance across multiple datasets, we ﬁrst normalize the performance score
(R2or accuracy) between 0 and 1 via an aﬃne renormalization between the worse- and top-performing
methods for each dataset. The normalized scores are then averaged over the diﬀerent datasets and the 10
data subsamplings (for the tabular datasets) or 5 cross-validation folds (for the scikit-feature datasets).
Table 3 shows the aggregated prediction scores, while the raw scores for each dataset can be found in Ta-
bles S13-S16. PRS always improves over RS, except on the scikit-feature benchmark with decision trees, and
is also usually better than RaSE. PRS-SVM yields the highest performance on the scikit-feature benchmark,
but RF and GBDT remain the best performers on the tabular benchmarks, with an equivalent performance
of PRS-kNN on the regression datasets.
Overall, PRS and RaSE ensembles are sparser than RS, when comparing the (expected) number of selected
features per base model (Tables S17-S19). In particular, RaSE returns very sparse models on the scikit-
feature benchmark, as for these datasets the maximum feature subspace size is explicitly set to√
N(see
Appendix D.4), where the number Nof samples is very small (between 100 and 200).
3.3 MNIST
We applied our method to classify images of handwritten digits 5’s and 6’s. The images were taken from the
MNIST dataset (LeCun et al., 1998) and random noise was added to them to make the task more challenging
(Figure 3). We treated the image pixels as individual features and we used the following objective function
within PRS:
F(α) =1
NN/summationdisplay
i=1L/parenleftbig
yi,E[fz(xi)]p(z|α)/parenrightbig
+λ1W/summationdisplay
j=1H/summationdisplay
k=1αj,k+λ2
H/summationdisplay
j=2|αj,k−αj−1,k|+W/summationdisplay
k=2|αj,k−αj,k−1|
,(20)
10Under review as submission to TMLR
1 = 0
2 = 0
1 = 0.01
2 = 0.001
2 = 0.01
0.000.050.100.150.200.250.300.35
Figure 3: PRS-kNN feature selection probabilities on MNIST. The ﬁrst row shows three exemples
of (noisy) images from the dataset. The second and third rows show the values of the parameters αfor
diﬀerent values of the regularization coeﬃcients λ1andλ2. Increasing λ1enforces sparsity, while increasing
λ2enforces spatial smoothness.
Table 4:Test accuracies on MNIST. The right-hand part of the table indicates the accuracies of PRS
when the hyper-parameters λ1andλ2are optimized on the validation set.
Without regularization With regularization
RF GBDT PRS-tree PRS-kNN PRS-SVM PRS-tree PRS-kNN PRS-SVM
0.969 0.982 0.976 0.938 0.954 0.982 0.955 0.959
whereWandHare respectively the width and height of the image, and αj,kis the selection probability for
the pixel in the j-row andk-th column. The second term is a regularization term that enforces sparsity, while
the last term penalizes large diﬀerences between the αj,kparameters corresponding to neighbouring pixels
(Figure 3). Such regularization is known as the fused lasso (Tibshirani et al., 2005) and allows to account
for the spatial structure of the features. Without any regularization ( λ1= 0,λ2= 0), the pixels with strictly
positive feature selection probabilities tend to be spread over the whole digit (Figure 3). As regularization is
increased, they cluster around the bottom-left of the digit (as expected since this is where 5’s and 6’s diﬀer
in the images) and the prediction performance is improved (Table 4). The highest accuracies are obtained
with PRS-tree and GBDT.
3.4 Gene network inference
An open problem in computational biology is the reconstruction of gene regulatory networks, which attempt
to explain the joint variability in the expression levels of a group of genes through a sparse pattern of
interactions. One approach to gene network reconstruction is the application of a feature selection approach
that identiﬁes the regulators of each target gene. Such approach is used by GENIE3, one of the current
state-of-the-art network inference algorithms (Huynh-Thu et al., 2010). This method learns for each target
gene a RF model predicting its expression from the expressions of all the candidate regulators, and identiﬁes
the regulators of that target gene through the RF-based feature importance scores. The PRS and RaSE
approaches can be used in the same way for gene network inference, with however the advantage that
the base models are not restricted to decision trees. Furthermore, while in GENIE3 the diﬀerent models,
11Under review as submission to TMLR
Table 5:AUPRs obtained on the DREAM4 networks. The highest AUPR is indicated in bold type
for each network. Random indicates the AUPR of an approach that randomly ranks all the possible edges.
The table also indicates the average subspace size for RaSE and PRS, i.e. for RaSE: the average subspace
size over the T×Gmodels, and for PRS: the sum/summationtextM
j=1αj,g, averaged over the Ggenes.
AUPR Subspace size
Net1 Net2 Net3 Net4 Net5 Net1 Net2 Net3 Net4 Net5
Random 0.02 0.02 0.02 0.02 0.02 — — — — —
GENIE3 RF 0.17 0.15 0.25 0.23 0.22 — — — — —
RaSE tree 0.08 0.07 0.16 0.15 0.12 5.93 5.90 6.19 6.12 5.98
kNN 0.09 0.07 0.15 0.13 0.13 6.07 6.18 6.06 6.22 6.21
SVM 0.08 0.07 0.13 0.11 0.10 5.34 5.82 5.00 5.38 5.57
PRS tree 0.14 0.09 0.19 0.15 0.15 4.63 4.71 5.19 5.13 4.96
λ= 0 kNN 0.15 0.11 0.21 0.19 0.21 5.14 4.68 5.84 5.39 5.11
SVM 0.13 0.10 0.18 0.16 0.19 5.64 4.74 5.86 5.77 5.45
PRS tree 0.09 0.11 0.17 0.18 0.11 0.28 0.27 0.45 0.41 0.33
λ> 0kNN 0.16 0.190.25 0.240.21 0.30 0.30 5.84 5.39 0.54
SVM 0.180.16 0.270.19 0.240.50 0.51 1.60 1.13 0.93
corresponding to the diﬀerent target genes, are learned independently of each other, PRS can be extended
to introduce a global constraint on the topology of the network.
More speciﬁcally, we use a joint regularizer that enforces modular networks, a property often encountered in
real gene regulatory networks. Let Gbe the number of genes, among which there are Mcandidate regulators,
and let xi∈RMandyi∈RGbe respectively the expression levels of the candidate regulators and of the G
target genes in the i-th sample ( i= 1,...,N). Our goal is to identify a M×Gmatrix α, whereαj,gis the
weight of the regulatory link directed from the j-th candidate regulator to the g-th gene. In the context of
PRS, we seek to identify the matrix αthat minimizes the following objective function:
1
G1
NG/summationdisplay
g=1N/summationdisplay
i=1/parenleftbig
yi,g−E[fz(xi)]p(z|α.,g)/parenrightbig2+λM/summationdisplay
j=1/radicaltp/radicalvertex/radicalvertex/radicalbtG/summationdisplay
g=1α2
j,g, (21)
whereyi,gis the expression of the g-th gene in the i-th sample and α.,gdenotes the g-th column of the matrix
α. The second term in the above objective function is a joint regularizer (with a coeﬃcient λ) that enforces
structured sparsity, by enforcing the selection of as few rows as possible in α(Jenatton et al., 2011). Using
this joint regularizer will result in modular networks where only a few regulators control the expressions of
the diﬀerent genes.
We evaluate the ability of PRS to reconstruct the ﬁve 100-gene networks of the DREAM4 Multifactorial
Network challenge (Marbach et al., 2010; 2012), for which GENIE3 was deemed the best performer. The
DREAM4 networks are artiﬁcial networks for which the true regulatory links are known and an AUPR
can thus be computed given a predicted ranking of links. To reconstruct each network, a simulated gene
expression dataset with 100 samples was made available to the challenge participants.
The regularization coeﬃcient λdetermines the number of used candidate regulators (Figure S7), and we
selected the value of λthat yields the lowest prediction error on the validation set. Adding the regularization
term sometimes deteriorates the AUPR of PRS-tree, but it can greatly help PRS-kNN and PRS-SVM
(Table 5). The two latter methods yield the highest AUPRs, while RaSE is the worse performer. The bad
performance of RaSE compared to PRS could be explained by the lack of regularization, which leads to a
higher number of used candidate regulators per base model (Table 5).
4 Conclusions
We proposed a model-agnostic ensemble method that aggregates base models independently trained on
feature subsets sampled from a Bernoulli distribution. We show that the parameters of the latter distribution
12Under review as submission to TMLR
can be trained using gradient descent even if the base models are not diﬀerentiable. The required iterative
gradient computations can furthermore be performed eﬃciently by exploiting importance sampling. The
resulting approach uniquely combines several interesting features: it is fully model-agnostic, it can use any
combinationofdiﬀerentiablelossfunctionandregularizationterm,anditprovidesvariableimportancescores.
Experiments show that PRS almost always improves over standard RS and is competitive with respect to
RF, GBDT and RaSE, both in terms of predictive performance and feature ranking quality. We also showed
that an appropriate regularization strategy allows PRS to outperform the state-of-the-art GENIE3 in the
inference of gene regulatory networks.
While we adopted an ensemble strategy, the same optimization technique, combining gradient descent and
importance sampling, can be used to solve the feature selection problem as deﬁned in (17) and addressed
also by RaSE. It would be interesting to investigate this approach and compare it with the ensemble version
exploredinthispaper. Notehoweverthatitwouldrequiretoexploitastrongerlearningalgorithm, becauseit
would not beneﬁt from the ensemble averaging eﬀect. Applying this technique, and its associated derivation
of feature importance scores, on top of modern deep learning models would be also highly desirable given the
challenge to explain these models. This would require however to develop speciﬁc strategies to reduce the non
negligible computational burden that would arise when training multiple ensembles of deep, complex models.
Finally, exploiting more complex feature subset distributions, beyond independent Bernoulli distributions,
would be also very interesting but adapting the optimization strategy might not be trivial.
References
N. S. Altman. An introduction to kernel and nearest-neighbor nonparametric regression. The American
Statistician , 46(3):175–185, 1992.
Muhammed Fatih Balin, Abubakar Abid, and James Zou. Concrete autoencoders: Diﬀerentiable feature
selection and reconstruction. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of
the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning
Research , pp. 444–453. PMLR, 2019.
Bernhard E. Boser, Isabelle Guyon, and Vladimir N. Vapnik. A training algorithm for optimal margin
classiﬁers. In Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory , pp.
144–152. ACM Press, 1992.
L. Breiman. Bagging predictors. Machine Learning , 24(2):123–140, 1996a.
L. Breiman. Random forests. Machine Learning , 45(1):5–32, 2001.
L. Breiman, J. H. Friedman, R. A. Olsen, and C. J. Stone. Classiﬁcation and Regression Trees . Wadsworth
International (California), 1984a.
Leo Breiman. Bagging predictors. Machine Learning , 24(2):123–140, 1996b.
Leo Breiman, Jerome Friedman, Charles J. Stone, and R.A. Olshen. Classiﬁcation and Regression Trees .
Chapman and Hall/CRC, 1984b.
Jérémie Donàand PatrickGallinari. Diﬀerentiable feature selection, a reparameterizationapproach. InNuria
Oliver, Fernando Pérez-Cruz, Stefan Kramer, Jesse Read, and Jose A. Lozano (eds.), Machine Learning
and Knowledge Discovery in Databases. Research Track , pp. 414–429. Springer International Publishing,
2021.
A. Doucet, N. de Freitas, and N. Gordon. Sequential Monte Carlo Methods in Practice . Springer, New York,
2001.
YoavFreundandRobertESchapire. Adecision-theoreticgeneralizationofon-linelearningandanapplication
to boosting. Journal of Computer and System Sciences , 55(1):119–139, 1997.
J. Friedman. Multivariate adaptive regression splines. The Annals of Statistics , 19(1):1–67, 1991.
13Under review as submission to TMLR
J. H. Friedman. Greedy function approximation: A gradient boosting machine. The Annals of Statistics , 29
(5):1189–1232, 2001.
P. Geurts, D. Ernst, and L. Wehenkel. Extremely randomized trees. Machine Learning , 36(1):3–42, 2006.
Peter W. Glynn. Likelihood ratio gradient estimation for stochastic systems. Commun. ACM , 33(10):75?84,
1990.
Léo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux. Why do tree-based models still outperform deep
learning on typical tabular data? In Proceedings of the Neural Information Processing Systems Track on
Datasets and Benchmarks , 2022.
Tin Kam Ho. The random subspace method for constructing decision forests. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 20(8):832–844, 1998.
V. A. Huynh-Thu, A. Irrthum, L. Wehenkel, and P. Geurts. Inferring regulatory networks from expression
data using tree-based methods. PLoS ONE , 5(9):e12776, 2010.
R. Jenatton, J.-Y. Audibert, and F. Bach. Structured variable selection with sparsity-inducing norms.
Journal of Machine Learning Research , 12:2777–2824, 2011.
Yann LeCun, Corinna Cortes, and Christopher J. C. Burges. The MNIST database of handwritten digits.
http://yann.lecun.com/exdb/mnist/ , 1998.
Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P Trevino, Jiliang Tang, and Huan Liu.
Feature selection: A data perspective. ACM Computing Surveys (CSUR) , 50(6):94, 2018.
Scott M Lundberg and Su-In Lee. A uniﬁed approach to interpreting model predictions. In I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural
Information Processing Systems , volume 30. Curran Associates, Inc., 2017.
D. Marbach, R. J. Prill, T. Schaﬀter, C. Mattiussi, D. Floreano, and G. Stolovitzky. Revealing strengths
and weaknesses of methods for gene network inference. Proceedings of the National Academy of Sciences ,
107(14):6286–6291, 2010.
D. Marbach, J. C. Costello, R. Küﬀner, N. Vega, R. J. Prill, D. M. Camacho, K. R. Allison, the DREAM5
Consortium, M. Kellis, J. J. Collins, and G. Stolovitzky. Wisdom of crowds for robust gene network
inference. Nature Methods , 9(8):796–804, 2012.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R.Weiss,V.Dubourg,J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,M.Perrot,andE.Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research , 12:2825–2830, 2011.
Marco Túlio Ribeiro, Sameer Singh, and Carlos Guestrin. "Why should I trust you?": Explaining the
predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining , KDD ’16, pp. 1135–1144, New York, NY, USA, 2016. Association
for Computing Machinery.
Reuven Y. Rubinstein and Alexander Shapiro. Discrete event systems: sensitivity analysis and stochastic
optimization by the score function method . Wiley, 1993.
Rishit Sheth and Nicoló Fusi. Diﬀerentiable feature selection by discrete relaxation. In Silvia Chiappa and
Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference on Artiﬁcial Intel-
ligence and Statistics , volume 108 of Proceedings of Machine Learning Research , pp. 1564–1572. PMLR,
2020.
J. Staines and D. Barber. Optimization by variational bounding. In Proceedings of the 2013 European
Symposium on Artiﬁcial Neural Networks, Computational Intelligence and Machine Learning (ESANN
2013), pp. 473–478, 2013.
14Under review as submission to TMLR
Ye Tian and Yang Feng. RaSE: Random Subspace Ensemble Classiﬁcation. Journal of Machine Learning
Research , 22(45):1–93, 2021.
Robert Tibshirani, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. Sparsity and smoothness
via the fused lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology) , 67(1):
91–108, 2005.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Machine Learning , 8(3):229–256, 1992.
Yutaro Yamada, Oﬁr Lindenbaum, Sahand Negahban, and Yuval Kluger. Feature selection using stochastic
gates. In Hal Daumé and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine
Learning , volume 119 of Proceedings of Machine Learning Research , pp. 10648–10659. PMLR, 2020.
Junchen Yang, Oﬁr Lindenbaum, and Yuval Kluger. Locally sparse neural networks for tabular biomedical
data. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato
(eds.),International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,
USA, volume 162 of Proceedings of Machine Learning Research , pp. 25123–25153. PMLR, 2022.
Ruoqing Zhu, Donglin Zeng, and Michael R. Kosorok. Reinforcement learning trees. Journal of the American
Statistical Association , 110(512):1770–1784, 2015.
15Under review as submission to TMLR
A Pseudo-code
Algorithm S1 PRS training
1:Input:datasetD={(xi,yi)}N
i=1, number of models T, batch size Nb, number of epochs nepochs.
2:Output: Featureselectionprobabilities α= [α1,α2,...,αM]/latticetop, whereMisthenumberofinputfeatures,
and a trained ensemble model.
3:forj= 1toMdo
4:αj←5/T
5:end for
6:foreach batchDb={(x(i),y(i))}Nb
i=1⊂Ddo
7:fort= 1toTdo
8: Draw a feature subset z(t)fromp(z|α)
9: Learn a model fz(t)fromz(t)andD\Db
10:end for
11:end for
12:β←α,k←0
13:repeat
14:foreach batchDb={(x(i),y(i))}Nb
i=1⊂Ddo
15: fori= 1toNbdo
16: baseline =/summationtextT
t=1(∇βlogp(z(t)|β))2fz(t)(x(i))/summationtextT
t=1(∇βlogp(z(t)|β))2
17:∇βE[fz(x(i))]p(z|β)←1
T/summationtextT
t=1p(z(t)|β)
p(z(t)|α)(fz(t)(x(i))−baseline )∇βlogp(z(t)|β)
18: end for
19:∇βF←1
Nb/summationtextNb
i=1dL
dE[fz(x(i))]p(z|β)∇βE[fz(x(i))]p(z|β)
20: β←proj(β−η∇βF,[0,1])
21:end for
22:fort= 1toTdo
23:wt←p(z(t)|β)
p(z(t)|α)
24:end for
25:Teff←/parenleftbig/summationtextT
t=1wt/parenrightbig2
/summationtextT
t=1w2
t
26:ifTeff<0.9Tthen
27: α←β
28: foreach batchDb={(x(i),y(i))}Nb
i=1⊂Ddo
29: fort= 1toTdo
30: Draw a feature subset z(t)fromp(z|α)
31: Learn a model fz(t)fromz(t)andD\Db
32: end for
33: end for
34:end if
35:k←k+ 1
36:untilk=nepochs
37:α←β
38:fort= 1toTdo
39:Draw z(t)fromp(z|α)
40:Learn a model fz(t)fromDandz(t)
41:end for
42:return αand{fz(t)}T
t=1
16Under review as submission to TMLR
Algorithm S1 shows the pseudo-code for training a PRS model. Feature selection probabilities αare ﬁrst
initialized to5
T(lines 3-5). Given a batch Db, an ensemble of base models fz(t)are trained from D\Db, by
drawing feature subsets from p(z|α)(lines 7-10). The batch Dbis then used to estimate the gradient, using
importance sampling approximation (lines 15-19), and the values of the feature selection probabilities are
updated using projected gradient descent (line 20). When the eﬀective number of feature subsets ( Teff)
becomes too low, new models are trained (lines 26-34). Once the parameters αare optimized, a ﬁnal
ensemble model is learned (lines 38-41).
B Computing the gradient
The score function method allows us to express the gradient of an expectation as an expectation itself. We
have:
∇αE[fz(x)]p(z|α)=∇α/summationdisplay
zp(z|α)fz(x) (22)
=/summationdisplay
zfz(x)∇αp(z|α) (23)
=/summationdisplay
zfz(x)p(z|α)∇αlogp(z|α) (24)
=E[fz(x)∇αlogp(z|α)]p(z|α) (25)
where to obtain Eq. (24), we used the equality ∇αp(z|α) =p(z|α)∇αlogp(z|α).
In the case of a Bernoulli distribution, we have:
p(z|α) =M/productdisplay
j=1αzj
j(1−αj)(1−zj),
logp(z|α) =M/summationdisplay
j=1zjlogαj+ (1−zj) log(1−αj).
By using Eq.(25), the j-th component of the gradient is given by:
∂E[fz(x)]p(z|α)
∂αj=E/bracketleftbigg
fz(x)∂
∂αjlogp(z|α)/bracketrightbigg
p(z|α)(26)
=E/bracketleftbigg
fz(x)/parenleftbiggzj
αj−1−zj
1−αj/parenrightbigg/bracketrightbigg
p(z|α)(27)
=/summationdisplay
z:zj=1fz(x)p(z|α)
αj−/summationdisplay
z:zj=0fz(x)p(z|α)
1−αj(28)
=/summationdisplay
z:zj=1fz(x)p(z−j|α−j)−/summationdisplay
z:zj=0fz(x)p(z−j|α−j) (29)
=E[fz(x)|zj= 1]p(z−j|α−j)−E[fz(x)|zj= 0]p(z−j|α−j), (30)
where z−j=z\zjandα−j=α\αj.
C Estimating the gradient
C.1 Estimation with baseline b
We have:
E[∇αlogp(z|α)]p(z|α)=/summationdisplay
zp(z|α)∇αlogp(z|α) =/summationdisplay
z∇αp(z|α) =∇α/summationdisplay
zp(z|α) =∇α1 = 0.(31)
17Under review as submission to TMLR
Therefore, for any constant b, we have:
E[(fz(x)−b)∇αlogp(z|α)]p(z|α)=E[fz(x)∇αlogp(z|α)]p(z|α)−bE[∇αlogp(z|α)]p(z|α)(32)
=E[fz(x)∇αlogp(z|α)]p(z|α). (33)
C.2 Optimal value of the baseline b
For readability, let us drop the subscript p(z|α)in the expectations, i.e. E[·] =E[·]p(z|α), and let us deﬁne
hz(α) =∇αlogp(z|α), with E[hz(α)] = 0. The gradient estimator is hence:
E[(fz(x)−b)hz(α))], (34)
and its variance is given by:
V= var [(fz(x)−b)hz(α))] = var[hz(α)fz(x)] +b2var[hz(α)]−2bcov[hz(α)fz(x),hz(α)].(35)
The optimal value of the baseline bis the one that minimizes the variance V, which is given by:
dV
db= 0 (36)
⇔2bvar[hz(α)]−2cov[hz(α)fz(x),hz(α)] = 0 (37)
⇔b=cov[hz(α)fz(x),hz(α)]
var[hz(α)](38)
⇔b=E[h2
z(α)fz(x)]−E[hz(α)fz(x)]E[hz(α)]
E[h2z(α)]−E[hz(α)]2(39)
⇔b=E[h2
z(α)fz(x)]
E[h2z(α)], (40)
where we used the equality E[hz(α)] = 0to obtain Eq. (40).
D Implementation details
D.1 Data pre-processing
Prior to training, we apply a one-hot encoding to the categorical features and all the features are then
normalized to have zero mean and unit variance.
D.2 PRS
In all our experiments, we use ensembles of T= 100models and we initialize each αjto 0.05, so that each
feature is expected to be selected ﬁve times over the ensemble. We noticed that using lower initial αjvalues
prevents several features to be selected in the ﬁrst iterations of the algorithm, hence resulting in convergence
issues, while higher values result in larger computing times, as each base model must be trained using a
larger number of features. The algorithm is run over 3,000 epochs with the Adam optimizer, and we select
as optimal vector αthe one that yields the lowest value of the objective function on the validation set. For
regression problems we use the mean square error as loss function, while for classiﬁcation problems we use
the cross-entropy. For the simulated, scikit-feature and DREAM4 datasets, the batch size is set to 10% of
the samples of the training set, while the remaining 90% are used for training the base models. For the
MNIST dataset, which is much larger, we use 50% of the samples as batch size and the remaining 50% for
training. For the scikit-feature, MNIST and DREAM4 datasets, a grid-search strategy is used for tuning the
value(s) of the regularization coeﬃcient(s), by selecting the coeﬃcient λ(or the pair ( λ1,λ2)) that minimizes
the prediction error on the validation set. The tested values are the following:
•For scikit-feature: λ={0.0001,0.001,0.01,0.1}.
18Under review as submission to TMLR
•For MNIST: λ1,λ2={0,0.0001,0.001,0.01}.
•For DREAM4: λ={0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01}.
Regarding the predictions on a test set, the output of the PRS ensemble is computed as the average of the
predictions of the diﬀerent base models for regression problems, and as the majority class for classiﬁcation
problems.
D.3 RS, RF and GBDT
Like for PRS, standard Random Subspace (RS), Random Forest (RF) and Gradient Boosting Decision Trees
(GBDT) are all run with T= 100models per ensemble. Given Mthe total number of features, the following
hyper-parameters are optimized on the validation set:
•For RS: the number Kof randomly sampled features for each base model. Tested values are {1,M
100,
M
50,M
20,M
10,M
5,M
3,M
2,√
M,M}.
•For RF: the number Kof randomly sampled features at each tree node. Tested values are {1,M
100,
M
50,M
20,M
10,M
5,M
3,M
2,√
M,M}.
•For GBDT: the maximum tree depth d. Tested values are {1, ..., 10}.
All the remaining hyper-parameters are set to the default values used in the scikit-learn library (Pedregosa
et al., 2011). The feature rankings of RF and GBDT are computed using the standard Mean Decrease
Impurity importance measure (Breiman et al., 1984a).
D.4 RaSE
The RaSE approach (Tian & Feng, 2021) consists in iteratively sampling and evaluating a population of
feature subsets. At each iteration, a probability distribution is identiﬁed from the best feature subsets
and this distribution is used to sample new feature subsets. More speciﬁcally, given the current feature
importances α, each iteration of RaSE consists of the following steps:
1. SampleT×Bfeature subsets: for tfrom 1 toTand forbfrom 1 toB:
•Sample the feature subset size dfrom a uniform distribution U(1,D).
•Samplethefeaturesubset zt,bofsizedfromamultinomialdistributionwithparameters dand˜α,
where the selection probability ˜αjof thej-th feature is set as ˜αj=αj1(αj>C0
logM)+C0
M1(αj≤
C0
logM).
2. Evaluate each feature subset zt,bby estimating, using 10-fold cross-validation, the prediction error
of a model learned from this feature subset.
3. Fortfrom 1 toT, select the best subset zt,∗among{zt,b}B
b=1, as the one with the lowest prediction
error.
4. Setαjas the fraction of these Tsubsets where zt,∗
j= 1.
In all our experiments, we use T= 100,B= 500andC0= 0.1. As done in (Tian & Feng, 2021), the
maximum subset size is set as D= min(M,[√
N]), whereMis the number of features, Nis the number of
samples in the training dataset and [x]denotes the largest integer not larger than x. Like in PRS, each αjis
initialized to 0.05. We then run the algorithm over 10 iterations and we select as optimal vector αthe one
that yields the lowest prediction error on the validation set. Note that the chosen number of iterations is
very small because of the high computational complexity of RaSE (Tian & Feng, 2021 actually show results
for at most 3 iterations).
19Under review as submission to TMLR
E Simulated problems
E.1 Simulation protocol
Table S6: Simulated problems. Mis the total number of features and Mrelis the number of relevant
ones.
Problem Type M Mrel
Checkerboard Regression 304 4
Friedman Regression 305 5
Hypercube Classiﬁcation 305 5
Linear Classiﬁcation 310 10
We simulate four problems, where 300 irrelevant features are added to the relevant features. Let Mbe the
total number of features.
•Checkerboard : Checkerboard-like regression problem with strong correlation between features (Zhu
et al., 2015). x∼N(0M,ΣM×M), where Σi,j= 0.9|i−j|.y= 2x1x2+ 2x3x4+N(0,1).
•Friedman : Non-linear regression problem (Friedman, 1991). y= 10sin(πx1x2) + 20(x3−0.5)2+
10x4+ 5x5+ 0.1N(0,1). Like for the Checkerboard problem, we introduce a strong correlation
between the features: x∼N(0M,ΣM×M), where Σi,j=s20.9|i−j|. We uses=0.5
3, so that∼99%
of the samples have values between 0 and 1.
•Hypercube : Non-linear, binary classiﬁcation problem with 5 relevant features, generated with the
make_classiﬁcation function of the scikit-learn library (Pedregosa et al., 2011). In this problem,
each class is associated with two vertices of a hypercube of dimension 5 and samples are generated
in the neighbourhood of each vertex by using a normal distribution centred on the vertex (with
Σ =I). Irrelevant features are each sampled from N(0,1).
•Linear: Linear, binary classiﬁcation problem with 10 relevant features, generated by ﬁrst simulating
a linear regression problem with the make_regression function of the scikit-learn library and thresh-
olding the output variable so that the two classes are balanced. The output before thresholding is:
y=/summationtext10
k=1wkxk, wherewk∼U(0,100),k= 1,..., 10andxk∼N(0,1),k= 1,...,M.
For each problem, we generate 10 datasets, each with 300 training samples, 100 validation samples and 100
test samples.
20Under review as submission to TMLR
E.2 Additional results
Table S7: Performance of PRS with and without using the variance reduction technique. The
optimal value b∗of the baseline is given by Eq. (12). Setting b= 0amounts to removing the variance
reduction method. We report here the prediction score on the test set ( R2or accuracy), the feature ranking
quality (AUPR) and the number of features used per base model (subspace size), i.e. the sum/summationtextM
j=1αj.
Values are mean and standard deviation over 10 datasets.
tree kNN SVM
b=b∗b= 0 b=b∗b= 0 b=b∗b= 0
Checkerboard R20.29±0.14 0.19 ±0.15 0.60 ±0.06 0.41 ±0.05 0.62 ±0.07 0.41 ±0.05
AUPR 0.60 ±0.23 0.54 ±0.29 0.92 ±0.14 0.71 ±0.21 0.98 ±0.06 0.49 ±0.18
Subspace size 7.29 ±1.32 13.71 ±1.61 6.49 ±1.91 31.49 ±2.87 4.85 ±0.55 24.73 ±2.16
Friedman R20.83±0.03 0.77 ±0.04 0.88 ±0.03 0.72 ±0.03 0.90 ±0.05 0.78 ±0.05
AUPR 0.95 ±0.04 0.81 ±0.07 1.00 ±0.00 0.71 ±0.08 0.98 ±0.05 0.86 ±0.09
Subspace size 7.82 ±0.75 31.39 ±3.50 5.49 ±0.47 34.23 ±4.02 7.28 ±1.31 33.26 ±3.61
Hypercube Accuracy 0.88 ±0.04 0.88 ±0.05 0.90 ±0.06 0.89 ±0.04 0.88 ±0.05 0.85 ±0.06
AUPR 0.97 ±0.06 0.96 ±0.07 0.94 ±0.09 0.96 ±0.07 0.90 ±0.14 0.86 ±0.15
Subspace size 12.10 ±3.04 22.90 ±2.75 6.83 ±1.20 20.14 ±2.36 13.54 ±3.01 23.49 ±5.07
Linear Accuracy 0.78 ±0.03 0.78 ±0.04 0.88 ±0.03 0.88 ±0.03 0.92 ±0.03 0.93 ±0.03
AUPR 0.67 ±0.13 0.69 ±0.12 0.73 ±0.12 0.73 ±0.11 0.80 ±0.10 0.82 ±0.11
Subspace size 12.68 ±2.95 23.05 ±2.32 10.12 ±2.30 22.81 ±1.55 19.97 ±1.90 33.37 ±4.01
21Under review as submission to TMLR
200 400 600 800 1000
N020406080Time (min.)
Checkerboard
PRS-tree
PRS-kNN
PRS-SVM
0 200 400 600 800 1000
M050100Time (min.)
Checkerboard
200 400 600 800 1000
N020406080Time (min.)
Friedman
0 200 400 600 800 1000
M0204060Time (min.)
Friedman
200 400 600 800 1000
N10203040Time (min.)
Hypercube
0 200 400 600 800 1000
M0255075100Time (min.)
Hypercube
200 400 600 800 1000
N050100Time (min.)
Linear
0 200 400 600 800 1000
M0204060Time (min.)
Linear
FigureS4: ComputingtimesofPRS(fortraining+testing). Plainlinesandshadedareasrespectively
indicate the mean and standard deviations over 10 datasets. The left-hand plots show the computing times
for diﬀerent training set sizes ( N), with the number of irrelevant features set to 300. The right-hand plots
show the computing times for diﬀerent values of the number Mof features (we kept ﬁxed the number of
relevant features and increased the number of irrelevant features), with N= 300. The computing times were
measured on AMD Epyc Rome CPUs at 2.9 GHz and 256GB of RAM.
22Under review as submission to TMLR
200 400 600 800 1000
No. irrelevant features0.4
0.2
0.00.20.40.60.8Test R2
Checkerboard
200 400 600 800 1000
No. irrelevant features0.60.70.80.9Test R2
Friedman
200 400 600 800 1000
No. irrelevant features0.50.60.70.80.9T est accuracy
Hypercube
RF
GBDT
PRS-SVM
200 400 600 800 1000
No. irrelevant features0.700.750.800.850.900.95T est accuracy
Linear
Figure S5: Prediction score for an increasing number of irrelevant features. Plain lines and shaded
areas respectively indicate the mean and standard deviations over 10 datasets.
200 400 600 800 1000
No. irrelevant features0.00.20.40.60.81.0AUPR
Checkerboard
200 400 600 800 1000
No. irrelevant features0.60.70.80.91.0AUPR
Friedman
200 400 600 800 1000
No. irrelevant features0.20.40.60.81.0AUPR
Hypercube
RF
GBDT
PRS-SVM
200 400 600 800 1000
No. irrelevant features0.50.60.70.80.9AUPR
Linear
Figure S6: Feature ranking AUPR for an increasing number of irrelevant features. Plain lines
and shaded areas respectively indicate the mean and standard deviations over 10 datasets.
23Under review as submission to TMLR
Table S8: Comparison of PRS and RaSE. We report here the prediction score on the test set ( R2or
accuracy), the feature ranking quality (AUPR) and the number of features used per base model (subspace
size), i.e. for PRS: the sum/summationtextM
j=1αj, and for RaSE: the average feature subset size among the Ttrained
base models. Values are mean and standard deviation over 10 datasets.
tree kNN SVM
PRS RaSE PRS RaSE PRS RaSE
Checkerboard R20.29±0.14 0.25 ±0.15 0.60 ±0.06 0.61 ±0.05 0.62 ±0.07 0.62 ±0.07
AUPR 0.60 ±0.23 0.77 ±0.24 0.92 ±0.14 1.00 ±0.00 0.98 ±0.06 1.00 ±0.00
Subspace size 7.29 ±1.32 10.23 ±1.18 6.49 ±1.91 4.96 ±0.76 4.85 ±0.55 4.51 ±0.59
Friedman R20.83±0.03 0.77 ±0.03 0.88 ±0.03 0.87 ±0.03 0.90 ±0.05 0.90 ±0.05
AUPR 0.95 ±0.04 0.79 ±0.14 1.00 ±0.00 0.97 ±0.06 0.98 ±0.05 0.98 ±0.05
Subspace size 7.82 ±0.75 10.40 ±2.87 5.49 ±0.47 5.67 ±0.51 7.28 ±1.31 6.51 ±0.97
Hypercube Accuracy 0.88 ±0.04 0.86 ±0.07 0.90 ±0.06 0.92 ±0.04 0.88 ±0.05 0.90 ±0.05
AUPR 0.97 ±0.06 0.89 ±0.13 0.94 ±0.09 0.94 ±0.09 0.90 ±0.14 0.92 ±0.12
Subspace size 12.10 ±3.04 12.43 ±0.83 6.83 ±1.20 5.28 ±0.66 13.54 ±3.01 10.23 ±2.32
Linear Accuracy 0.78 ±0.03 0.77 ±0.05 0.88 ±0.03 0.88 ±0.04 0.92 ±0.03 0.91 ±0.02
AUPR 0.67 ±0.13 0.54 ±0.12 0.73 ±0.12 0.67 ±0.12 0.80 ±0.10 0.75 ±0.10
Subspace size 12.68 ±2.95 12.66 ±0.42 10.12 ±2.30 10.85 ±1.92 19.97 ±1.90 15.06 ±0.47
Table S9: Number of trained base models during the PRS training (with 3000 epochs). Values
are mean and standard deviation over 10 datasets.
tree kNN SVM
Checkerboard 308700 ±20095 119700±24661 76000±7238
Friedman 323200 ±27282 107000±18066 130800±23949
Hypercube 162600 ±24381 112700±13535 170900±41234
Linear 214400 ±70949 172300±37534 248700±42159
24Under review as submission to TMLR
Table S10: Performance of PRS-SVM for varying thresholds on Teff.The threshold Tcorresponds
to the case where new base models are trained at each epoch (no importance sampling). Values are mean
and standard deviation over 10 datasets.
Problem Teffthreshold No. trained models Training time (min.) R2/Accuracy AUPR
Checkerboard T 3001000±0 223.17 ±2.98 0.62±0.07 0.98±0.06
0.9T 76000±7238 6.89 ±0.63 0.62±0.07 0.98±0.06
0.7T 42300±2193 4.24 ±0.18 0.62±0.07 0.98±0.06
0.5T 31800±1989 3.49 ±0.26 0.62±0.08 0.96±0.08
0.3T 23400±1200 2.88 ±0.14 0.61±0.07 0.96±0.08
Friedman T 3001000±0 260.85 ±9.57 0.90±0.05 0.98±0.05
0.9T 130800±23949 12.32 ±2.25 0.90±0.05 0.98±0.05
0.7T 72400±8236 7.57 ±1.01 0.90±0.05 0.98±0.05
0.5T 53200±6720 5.87 ±0.76 0.90±0.05 0.95±0.09
0.3T 43000±6066 5.15 ±0.73 0.89±0.05 0.89±0.14
Hypercube T 3001000±0 210.56 ±41.64 0.88±0.05 0.92±0.13
0.9T 170900±41234 13.17 ±3.15 0.88±0.05 0.90±0.14
0.7T 66400±20967 9.16 ±1.60 0.87±0.05 0.83±0.14
0.5T 42200±11989 9.12 ±1.19 0.86±0.05 0.76±0.26
0.3T 18500±4944 9.36 ±1.82 0.83±0.09 0.59±0.24
Linear T 3001000±0 230.52 ±23.52 0.92±0.02 0.80±0.11
0.9T 248700±42159 20.14 ±4.01 0.92±0.03 0.80±0.10
0.7T 108600±34325 12.07 ±1.42 0.92±0.04 0.76±0.08
0.5T 65900±27351 10.64 ±1.31 0.90±0.03 0.74±0.11
0.3T 21600±4247 11.85 ±2.26 0.84±0.06 0.54±0.20
25Under review as submission to TMLR
F Real-world datasets
Table S11: Sizes of regression datasets. For datasets with categorical features, the last column indicates
the total number of features after one-hot encoding.
tabular benchmark, regression, with only numerical features
Dataset Samples Features
cpu_act 8192 21
pol 15000 26
elevators 16599 16
isolet 7797 613
wine_quality 6497 11
Ailerons 13750 33
houses 20640 8
house_16H 22784 16
diamonds 53940 6
Brazilian_houses 10692 8
Bike_Sharing_Demand 17379 6
nyc-taxi-green-dec-2016 581835 9
house_sales 21613 15
sulfur 10081 6
medical_charges 163065 3
MiamiHousing2016 13932 13
superconduct 21263 79
california 20640 8
ﬁfa 18063 5
year 515345 90
tabular benchmark, regression, with both numerical and categorical features
Dataset Samples Features
yprop_4_1 8885 82
analcatdata_supreme 4052 12
visualizing_soil 8641 5
black_friday 166821 23
diamonds 53940 26
Mercedes_Benz_Greener_Manufacturing 4209 735
Brazilian_houses 10692 17
Bike_Sharing_Demand 17379 20
OnlineNewsPopularity 39644 73
nyc-taxi-green-dec-2016 581835 31
house_sales 21613 19
particulate-matter-ukair-2017 394299 26
SGEMM_GPU_kernel_performance 241600 15
26Under review as submission to TMLR
Table S12: Sizes of classiﬁcation datasets. For datasets with categorical features, the last column
indicates the total number of features after one-hot encoding.
tabular benchmark, classiﬁcation, with only numerical features
Dataset Classes Samples Features
credit 2 16714 (8357 / 8357) 10
california 2 20634 (10317 / 10317) 8
wine 2 2554 (1277 / 1277) 11
electricity 2 38474 (19237 / 19237) 7
covertype 2 566602 (283301 / 283301) 10
pol 2 10082 (5041 / 5041) 26
house_16H 2 13488 (6744 / 6744) 16
kdd_ipums_la_97-small 2 5188 (2594 / 2594) 20
MagicTelescope 2 13376 (6688 / 6688) 10
bank-marketing 2 10578 (5289 / 5289) 7
phoneme 2 3172 (1586 / 1586) 5
MiniBooNE 2 72998 (36499 / 36499) 50
Higgs 2 940160 (470080 / 470080) 24
eye_movements 2 7608 (3804 / 3804) 20
jannis 2 57580 (28790 / 28790) 54
tabular benchmark, classiﬁcation, with both numerical and categorical features
Dataset Classes Samples Features
electricity 2 38474 (19237 / 19237) 14
eye_movements 2 7608 (3804 / 3804) 26
covertype 2 423680 (211840 / 211840) 93
rl 2 4970 (2485 / 2485) 38
road-safety 2 111762 (55881 / 55881) 35
compass 2 16644 (8322 / 8322) 59
KDDCup09_upselling 2 5128 (2564 / 2564) 104
scikit-feature benchmark, classiﬁcation, with only numerical features
Dataset Classes Samples Features
arcene 2 200 (112 / 88) 10000
CLL_SUB_111 2 100 (49 / 51) 11340
Prostate_GE 2 102 (50 / 52) 5966
SMK_CAN_187 2 187 (90 / 97) 19993
TOX_171 4 171 (45 / 45 / 39 / 42) 5748
27Under review as submission to TMLR
Table S13: Prediction performance of single models, RS, PRS, and RaSE, when the base model
isthedecisiontree. ValuesareR2scoresforregressionproblemsandaccuraciesforclassiﬁcationproblems,
averaged over 10 random data subsamplings for the tabular datasets and over 5 cross-validation folds for the
scikit-feature datasets. For each dataset, the highest performance is indicated in bold type.
Single tree RS-tree PRS-tree RaSE-tree
tabular benchmark, regression, with only numerical features
cpu_act 0.95 ±0.00 0.96 ±0.000.98±0.000.97±0.00
pol 0.89 ±0.02 0.91 ±0.020.94±0.010.94±0.01
elevators 0.38 ±0.05 0.60 ±0.040.71±0.020.70±0.02
isolet 0.32 ±0.04 0.74 ±0.020.76±0.020.75±0.02
wine_quality -0.27 ±0.110.35±0.020.34±0.02 0.16 ±0.02
Ailerons 0.60 ±0.04 0.71 ±0.020.78±0.020.73±0.03
houses 0.50 ±0.07 0.68 ±0.020.74±0.030.67±0.02
house_16H 0.05 ±0.300.48±0.100.46±0.07 0.41 ±0.18
diamonds 0.89 ±0.01 0.93 ±0.000.93±0.010.93±0.00
Brazilian_houses 0.97 ±0.02 0.98 ±0.020.98±0.020.98±0.02
Bike_Sharing_Demand 0.33 ±0.05 0.51 ±0.020.59±0.020.50±0.04
nyc-taxi-green-dec-2016 -0.10 ±0.14 0.29 ±0.040.44±0.050.39±0.08
house_sales 0.64 ±0.02 0.79 ±0.010.83±0.010.77±0.02
sulfur 0.56 ±0.13 0.64 ±0.080.68±0.080.67±0.09
medical_charges 0.96 ±0.01 0.96 ±0.000.97±0.000.97±0.01
MiamiHousing2016 0.70 ±0.03 0.85 ±0.010.87±0.010.83±0.01
superconduct 0.69 ±0.03 0.84 ±0.010.84±0.010.84±0.01
california 0.48 ±0.07 0.71 ±0.030.78±0.020.71±0.04
ﬁfa 0.29 ±0.06 0.44 ±0.030.56±0.030.43±0.05
year -0.73 ±0.09 0.13 ±0.03 0.14 ±0.040.15±0.02
tabular benchmark, regression, with both numerical and categorical features
yprop_4_1 -0.90 ±0.180.03±0.020.02±0.01 0.02 ±0.01
analcatdata_supreme 0.96 ±0.01 0.97 ±0.010.98±0.010.98±0.01
visualizing_soil 1.00 ±0.00 1.00 ±0.00 1.00 ±0.001.00±0.00
black_friday 0.12 ±0.09 0.44 ±0.040.57±0.030.56±0.03
diamonds 0.95 ±0.01 0.96 ±0.00 0.97 ±0.000.98±0.00
Mercedes_Benz_Greener_Manufacturing 0.10 ±0.13 0.44 ±0.07 0.55 ±0.050.56±0.05
Brazilian_houses 0.97 ±0.02 0.98 ±0.020.98±0.020.98±0.02
Bike_Sharing_Demand 0.68 ±0.05 0.73 ±0.030.88±0.020.86±0.02
OnlineNewsPopularity -0.87 ±0.080.08±0.020.08±0.03 0.04 ±0.03
nyc-taxi-green-dec-2016 -0.10 ±0.15 0.31 ±0.040.45±0.050.41±0.08
house_sales 0.65 ±0.02 0.79 ±0.020.84±0.010.78±0.03
particulate-matter-ukair-2017 0.28 ±0.06 0.49 ±0.020.56±0.020.49±0.03
SGEMM_GPU_kernel_performance 1.00 ±0.00 1.00 ±0.001.00±0.001.00±0.00
tabular benchmark, classiﬁcation, with only numerical features
credit 0.69 ±0.01 0.74 ±0.020.75±0.010.74±0.01
california 0.78 ±0.02 0.85 ±0.010.86±0.010.85±0.02
wine 0.72 ±0.02 0.78 ±0.020.78±0.020.75±0.02
electricity 0.73 ±0.02 0.77 ±0.010.78±0.010.77±0.01
covertype 0.69 ±0.02 0.72 ±0.020.74±0.020.73±0.02
pol 0.95 ±0.01 0.96 ±0.010.96±0.010.96±0.01
house_16H 0.78 ±0.010.86±0.010.85±0.02 0.82 ±0.02
kdd_ipums_la_97-small 0.84 ±0.01 0.87 ±0.01 0.87 ±0.010.87±0.01
MagicTelescope 0.76 ±0.020.81±0.010.80±0.01 0.79 ±0.02
bank-marketing 0.70 ±0.02 0.74 ±0.010.75±0.020.72±0.02
phoneme 0.80 ±0.02 0.80 ±0.010.84±0.010.82±0.02
MiniBooNE 0.82 ±0.01 0.90 ±0.010.90±0.010.89±0.01
Higgs 0.58 ±0.02 0.65 ±0.010.65±0.020.62±0.02
eye_movements 0.53 ±0.02 0.57 ±0.01 0.63 ±0.020.66±0.02
jannis 0.65 ±0.02 0.73 ±0.020.73±0.020.72±0.01
tabular benchmark, classiﬁcation, with both numerical and categorical features
electricity 0.73 ±0.02 0.78 ±0.01 0.78 ±0.010.78±0.02
eye_movements 0.52 ±0.02 0.57 ±0.01 0.63 ±0.010.66±0.02
covertype 0.70 ±0.020.76±0.010.75±0.02 0.75 ±0.02
rl 0.64 ±0.02 0.71 ±0.010.77±0.010.76±0.02
road-safety 0.65 ±0.01 0.71 ±0.020.71±0.020.71±0.01
compass 0.60 ±0.020.67±0.020.67±0.02 0.66 ±0.02
KDDCup09_upselling 0.75 ±0.01 0.78 ±0.01 0.79 ±0.010.79±0.01
scikit-feature benchmark, classiﬁcation, with only numerical features
arcene 0.67 ±0.05 0.80 ±0.09 0.81 ±0.050.82±0.06
CLL_SUB_111 0.62 ±0.100.73±0.080.70±0.09 0.59 ±0.11
Prostate_GE 0.75 ±0.09 0.88 ±0.080.88±0.110.87±0.08
SMK_CAN_187 0.57 ±0.08 0.66 ±0.06 0.64 ±0.070.66±0.06
TOX_171 0.56 ±0.130.80±0.060.75±0.05 0.76 ±0.0828Under review as submission to TMLR
Table S14: Prediction performance of single models, RS, PRS, and RaSE, when the base model
isthekNN. ValuesareR2scoresforregressionproblemsandaccuraciesforclassiﬁcationproblems, averaged
over 10 random data subsamplings for the tabular datasets and over 5 cross-validation folds for the scikit-
feature datasets. For each dataset, the highest performance is indicated in bold type.
Single kNN RS-kNN PRS-kNN RaSE-kNN
tabular benchmark, regression, with only numerical features
cpu_act 0.87 ±0.03 0.87 ±0.030.97±0.000.96±0.01
pol 0.87 ±0.02 0.87 ±0.020.94±0.010.94±0.01
elevators 0.54 ±0.03 0.55 ±0.030.73±0.020.73±0.02
isolet 0.68 ±0.03 0.70 ±0.030.85±0.020.82±0.02
wine_quality 0.25 ±0.04 0.34 ±0.020.34±0.020.32±0.04
Ailerons 0.64 ±0.03 0.65 ±0.030.80±0.010.80±0.01
houses 0.63 ±0.02 0.63 ±0.020.76±0.020.73±0.03
house_16H 0.41 ±0.09 0.43 ±0.10 0.42 ±0.100.44±0.10
diamonds 0.93 ±0.010.94±0.000.94±0.01 0.93 ±0.00
Brazilian_houses 0.95 ±0.01 0.95 ±0.010.98±0.010.98±0.01
Bike_Sharing_Demand 0.44 ±0.05 0.51 ±0.020.63±0.020.59±0.04
nyc-taxi-green-dec-2016 0.21 ±0.03 0.24 ±0.020.46±0.030.44±0.03
house_sales 0.73 ±0.01 0.75 ±0.010.83±0.010.83±0.01
sulfur 0.59 ±0.10 0.59 ±0.100.69±0.100.68±0.12
medical_charges 0.96 ±0.01 0.96 ±0.010.98±0.000.97±0.00
MiamiHousing2016 0.80 ±0.01 0.81 ±0.010.86±0.010.85±0.01
superconduct 0.77 ±0.01 0.80 ±0.010.82±0.010.81±0.01
california 0.67 ±0.02 0.67 ±0.020.77±0.020.73±0.03
ﬁfa 0.36 ±0.02 0.45 ±0.020.61±0.020.58±0.02
year 0.03 ±0.04 0.13 ±0.02 0.18 ±0.020.19±0.02
tabular benchmark, regression, with both numerical and categorical features
yprop_4_1 -0.10 ±0.04 0.04 ±0.010.05±0.010.05±0.01
analcatdata_supreme 0.90 ±0.02 0.90 ±0.020.98±0.010.98±0.01
visualizing_soil 0.99 ±0.00 0.99 ±0.001.00±0.001.00±0.00
black_friday -0.04 ±0.04 0.21 ±0.020.55±0.030.55±0.03
diamonds 0.77 ±0.01 0.91 ±0.01 0.96 ±0.010.97±0.00
Mercedes_Benz_Greener_Manufacturing 0.32 ±0.05 0.41 ±0.05 0.54 ±0.050.55±0.05
Brazilian_houses 0.89 ±0.02 0.93 ±0.010.98±0.010.98±0.01
Bike_Sharing_Demand 0.39 ±0.04 0.54 ±0.03 0.86 ±0.020.86±0.01
OnlineNewsPopularity -0.10 ±0.02 0.07 ±0.020.10±0.030.07±0.03
nyc-taxi-green-dec-2016 0.16 ±0.03 0.24 ±0.030.48±0.030.47±0.03
house_sales 0.72 ±0.01 0.73 ±0.020.84±0.000.83±0.00
particulate-matter-ukair-2017 0.17 ±0.04 0.39 ±0.03 0.62 ±0.020.62±0.01
SGEMM_GPU_kernel_performance 0.87 ±0.01 0.97 ±0.011.00±0.001.00±0.00
tabular benchmark, classiﬁcation, with only numerical features
credit 0.62 ±0.04 0.75 ±0.020.76±0.010.75±0.01
california 0.81 ±0.01 0.83 ±0.010.86±0.010.84±0.02
wine 0.73 ±0.02 0.77 ±0.020.77±0.020.75±0.02
electricity 0.73 ±0.01 0.75 ±0.020.77±0.010.76±0.01
covertype 0.70 ±0.02 0.72 ±0.020.76±0.020.76±0.02
pol 0.91 ±0.01 0.94 ±0.010.95±0.010.95±0.01
house_16H 0.81 ±0.01 0.84 ±0.010.85±0.010.83±0.01
kdd_ipums_la_97-small 0.81 ±0.01 0.85 ±0.01 0.87 ±0.010.88±0.01
MagicTelescope 0.78 ±0.01 0.79 ±0.010.81±0.020.80±0.02
bank-marketing 0.75 ±0.01 0.77 ±0.010.78±0.010.76±0.02
phoneme 0.83 ±0.01 0.83 ±0.010.84±0.010.83±0.01
MiniBooNE 0.84 ±0.01 0.88 ±0.010.89±0.010.88±0.02
Higgs 0.55 ±0.01 0.62 ±0.020.67±0.010.66±0.01
eye_movements 0.53 ±0.02 0.55 ±0.020.57±0.020.55±0.02
jannis 0.66 ±0.02 0.70 ±0.020.74±0.010.73±0.01
tabular benchmark, classiﬁcation, with both numerical and categorical features
electricity 0.71 ±0.02 0.76 ±0.020.77±0.020.77±0.02
eye_movements 0.53 ±0.02 0.56 ±0.020.57±0.020.55±0.03
covertype 0.72 ±0.02 0.74 ±0.020.78±0.020.78±0.02
rl 0.60 ±0.01 0.65 ±0.020.73±0.020.72±0.02
road-safety 0.66 ±0.01 0.70 ±0.010.72±0.010.71±0.01
compass 0.60 ±0.02 0.66 ±0.010.68±0.020.68±0.02
KDDCup09_upselling 0.63 ±0.02 0.68 ±0.02 0.77 ±0.010.79±0.01
scikit-feature benchmark, classiﬁcation, with only numerical features
arcene 0.80 ±0.04 0.77 ±0.02 0.81 ±0.050.82±0.06
CLL_SUB_111 0.50 ±0.09 0.48 ±0.12 0.55 ±0.160.72±0.10
Prostate_GE 0.78 ±0.04 0.79 ±0.04 0.89 ±0.070.91±0.06
SMK_CAN_187 0.63 ±0.05 0.63 ±0.050.66±0.040.64±0.09
TOX_171 0.68 ±0.12 0.71 ±0.090.88±0.060.79±0.0529Under review as submission to TMLR
Table S15: Prediction performance of single models, RS, PRS, and RaSE, when the base model
istheSVM. ValuesareR2scoresforregressionproblemsandaccuraciesforclassiﬁcationproblems,averaged
over 10 random data subsamplings for the tabular datasets and over 5 cross-validation folds for the scikit-
feature datasets. For each dataset, the highest performance is indicated in bold type.
Single SVM RS-SVM PRS-SVM RaSE-SVM
tabular benchmark, regression, with only numerical features
cpu_act 0.38 ±0.03 0.38 ±0.030.74±0.04 0.73±0.04
pol 0.41 ±0.09 0.41 ±0.09 0.45 ±0.210.76±0.03
elevators -6.97 ±1.86 -6.97 ±1.86-0.00±0.00-6.97±1.86
isolet 0.47 ±0.03 0.47 ±0.030.71±0.01 0.70±0.02
wine_quality 0.34 ±0.02 0.34 ±0.020.35±0.02 0.34±0.02
Ailerons -4.33 ±1.97 -4.33 ±1.97-0.00±0.00-4.33±1.97
houses 0.73 ±0.02 0.73 ±0.02 0.74 ±0.020.74±0.03
house_16H 0.46 ±0.12 0.46 ±0.120.46±0.11 0.46±0.11
diamonds 0.94 ±0.00 0.94 ±0.000.94±0.00 0.94±0.00
Brazilian_houses 0.96 ±0.01 0.96 ±0.010.97±0.01 0.97±0.01
Bike_Sharing_Demand 0.16 ±0.03 0.16 ±0.03 0.24 ±0.040.24±0.03
nyc-taxi-green-dec-2016 0.34 ±0.04 0.34 ±0.04 0.38 ±0.040.39±0.04
house_sales 0.77 ±0.02 0.77 ±0.010.83±0.01 0.82±0.01
sulfur -0.22 ±0.40 -0.17 ±0.340.15±0.05 -0.02±0.31
medical_charges 0.96 ±0.01 0.96 ±0.01 0.97 ±0.010.97±0.00
MiamiHousing2016 0.86 ±0.01 0.86 ±0.010.87±0.01 0.87±0.01
superconduct 0.59 ±0.03 0.59 ±0.03 0.68 ±0.020.69±0.02
california 0.75 ±0.02 0.75 ±0.020.76±0.02 0.76±0.02
ﬁfa 0.53 ±0.02 0.53 ±0.020.61±0.02 0.60±0.02
year 0.06 ±0.03 0.06 ±0.030.16±0.04 0.16±0.04
tabular benchmark, regression, with both numerical and categorical features
yprop_4_1 -0.19 ±0.33 -0.07 ±0.06-0.04±0.07-0.07±0.06
analcatdata_supreme 0.75 ±0.01 0.75 ±0.010.96±0.01 0.96±0.01
visualizing_soil 0.99 ±0.00 0.99 ±0.00 1.00 ±0.001.00±0.00
black_friday 0.12 ±0.04 0.14 ±0.03 0.48 ±0.020.49±0.02
diamonds 0.97 ±0.00 0.97 ±0.00 0.97 ±0.000.98±0.00
Mercedes_Benz_Greener_Manufacturing 0.34 ±0.03 0.35 ±0.04 0.53 ±0.050.53±0.05
Brazilian_houses 0.95 ±0.02 0.95 ±0.020.97±0.01 0.97±0.01
Bike_Sharing_Demand 0.08 ±0.02 0.08 ±0.02 0.24 ±0.030.24±0.03
OnlineNewsPopularity 0.04 ±0.03 0.06 ±0.030.08±0.03 0.08±0.03
nyc-taxi-green-dec-2016 0.34 ±0.05 0.34 ±0.05 0.39 ±0.030.40±0.04
house_sales 0.77 ±0.01 0.77 ±0.010.82±0.01 0.82±0.01
particulate-matter-ukair-2017 0.55 ±0.03 0.55 ±0.03 0.61 ±0.010.61±0.01
SGEMM_GPU_kernel_performance 0.96 ±0.00 0.96 ±0.00 0.99 ±0.000.99±0.00
tabular benchmark, classiﬁcation, with only numerical features
credit 0.70 ±0.03 0.71 ±0.03 0.72 ±0.030.73±0.02
california 0.84 ±0.01 0.84 ±0.010.85±0.01 0.85±0.01
wine 0.77 ±0.020.77±0.02 0.75±0.02 0.76 ±0.02
electricity 0.75 ±0.01 0.75 ±0.01 0.75 ±0.020.75±0.01
covertype 0.74 ±0.01 0.74 ±0.01 0.75 ±0.010.75±0.02
pol 0.93 ±0.01 0.94 ±0.01 0.93 ±0.010.94±0.01
house_16H 0.84 ±0.01 0.84 ±0.010.85±0.01 0.84±0.01
kdd_ipums_la_97-small 0.84 ±0.01 0.84 ±0.010.84±0.01 0.84±0.01
MagicTelescope 0.82 ±0.01 0.82 ±0.01 0.81 ±0.020.82±0.02
bank-marketing 0.77 ±0.01 0.77 ±0.01 0.77 ±0.010.78±0.01
phoneme 0.83 ±0.010.83±0.01 0.79±0.01 0.83 ±0.01
MiniBooNE 0.83 ±0.01 0.85 ±0.010.87±0.01 0.85±0.02
Higgs 0.60 ±0.01 0.62 ±0.02 0.65 ±0.020.65±0.02
eye_movements 0.56 ±0.01 0.56 ±0.02 0.56 ±0.010.57±0.02
jannis 0.72 ±0.01 0.72 ±0.01 0.73 ±0.010.73±0.01
tabular benchmark, classiﬁcation, with both numerical and categorical features
electricity 0.75 ±0.02 0.75 ±0.02 0.75 ±0.010.76±0.02
eye_movements 0.56 ±0.01 0.56 ±0.020.56±0.02 0.56±0.01
covertype 0.76 ±0.01 0.76 ±0.01 0.76 ±0.010.77±0.01
rl 0.62 ±0.01 0.61 ±0.010.63±0.01 0.62±0.01
road-safety 0.69 ±0.01 0.69 ±0.01 0.70 ±0.010.71±0.02
compass 0.66 ±0.01 0.66 ±0.010.68±0.02 0.68±0.02
KDDCup09_upselling 0.73 ±0.01 0.73 ±0.01 0.76 ±0.020.77±0.02
scikit-feature benchmark, classiﬁcation, with only numerical features
arcene 0.73 ±0.04 0.77 ±0.050.80±0.04 0.76±0.04
CLL_SUB_111 0.59 ±0.07 0.56 ±0.07 0.63 ±0.130.65±0.11
Prostate_GE 0.84 ±0.10 0.82 ±0.09 0.93 ±0.070.93±0.07
SMK_CAN_187 0.68 ±0.08 0.65 ±0.080.70±0.09 0.67±0.08
TOX_171 0.80 ±0.10 0.80 ±0.110.91±0.04 0.82±0.0730Under review as submission to TMLR
Table S16: Comparison to RF and GDBT. Values areR2scores for regression problems and accuracies
for classiﬁcation problems, averaged over 10 random data subsamplings for the tabular datasets and over 5
cross-validation folds for the scikit-feature datasets.
RF GBDT PRS-tree PRS-kNN PRS-SVM
tabular benchmark, regression, with only numerical features
cpu_act 0.98 ±0.000.98±0.000.98±0.00 0.97 ±0.00 0.74 ±0.04
pol 0.94 ±0.00 0.94 ±0.010.94±0.010.94±0.01 0.45 ±0.21
elevators 0.68 ±0.020.74±0.020.71±0.02 0.73 ±0.02 -0.00 ±0.00
isolet 0.71 ±0.02 0.70 ±0.02 0.76 ±0.020.85±0.020.71±0.01
wine_quality 0.37±0.020.32±0.03 0.34 ±0.02 0.34 ±0.02 0.35 ±0.02
Ailerons 0.80 ±0.020.81±0.010.78±0.02 0.80 ±0.01 -0.00 ±0.00
houses 0.74 ±0.020.77±0.020.74±0.03 0.76 ±0.02 0.74 ±0.02
house_16H 0.51±0.070.46±0.14 0.46 ±0.07 0.42 ±0.10 0.46 ±0.11
diamonds 0.94 ±0.000.94±0.000.93±0.01 0.94 ±0.01 0.94 ±0.00
Brazilian_houses 0.98 ±0.02 0.98 ±0.02 0.98 ±0.020.98±0.010.97±0.01
Bike_Sharing_Demand 0.63 ±0.020.65±0.020.59±0.02 0.63 ±0.02 0.24 ±0.04
nyc-taxi-green-dec-2016 0.39 ±0.04 0.41 ±0.05 0.44 ±0.050.46±0.030.38±0.04
house_sales 0.83 ±0.010.84±0.010.83±0.01 0.83 ±0.01 0.83 ±0.01
sulfur 0.72±0.100.71±0.10 0.68 ±0.08 0.69 ±0.10 0.15 ±0.05
medical_charges 0.97 ±0.000.98±0.000.97±0.00 0.98 ±0.00 0.97 ±0.01
MiamiHousing2016 0.86 ±0.010.88±0.010.87±0.01 0.86 ±0.01 0.87 ±0.01
superconduct 0.84 ±0.01 0.84 ±0.010.84±0.010.82±0.01 0.68 ±0.02
california 0.75 ±0.02 0.77 ±0.020.78±0.020.77±0.02 0.76 ±0.02
ﬁfa 0.62 ±0.020.64±0.020.56±0.03 0.61 ±0.02 0.61 ±0.02
year 0.15 ±0.02 0.16 ±0.04 0.14 ±0.040.18±0.020.16±0.04
tabular benchmark, regression, with both numerical and categorical features
yprop_4_1 0.04 ±0.01 0.04 ±0.01 0.02 ±0.010.05±0.01-0.04±0.07
analcatdata_supreme 0.98 ±0.010.98±0.010.98±0.01 0.98 ±0.01 0.96 ±0.01
visualizing_soil 1.00 ±0.001.00±0.001.00±0.00 1.00 ±0.00 1.00 ±0.00
black_friday 0.51 ±0.03 0.55 ±0.030.57±0.030.55±0.03 0.48 ±0.02
diamonds 0.97 ±0.000.98±0.000.97±0.00 0.96 ±0.01 0.97 ±0.00
Mercedes_Benz_Greener_Manufacturing 0.48 ±0.05 0.55 ±0.050.55±0.050.54±0.05 0.53 ±0.05
Brazilian_houses 0.98 ±0.02 0.98 ±0.02 0.98 ±0.020.98±0.010.97±0.01
Bike_Sharing_Demand 0.84 ±0.010.89±0.010.88±0.02 0.86 ±0.02 0.24 ±0.03
OnlineNewsPopularity 0.10 ±0.03 0.08 ±0.03 0.08 ±0.030.10±0.030.08±0.03
nyc-taxi-green-dec-2016 0.41 ±0.03 0.42 ±0.04 0.45 ±0.050.48±0.030.39±0.03
house_sales 0.83 ±0.010.85±0.010.84±0.01 0.84 ±0.00 0.82 ±0.01
particulate-matter-ukair-2017 0.61 ±0.020.63±0.010.56±0.02 0.62 ±0.02 0.61 ±0.01
SGEMM_GPU_kernel_performance 1.00±0.001.00±0.00 1.00 ±0.00 1.00 ±0.00 0.99 ±0.00
tabular benchmark, classiﬁcation, with only numerical features
credit 0.77 ±0.010.77±0.010.75±0.01 0.76 ±0.01 0.72 ±0.03
california 0.85 ±0.01 0.86 ±0.010.86±0.010.86±0.01 0.85 ±0.01
wine 0.79±0.020.78±0.02 0.78 ±0.02 0.77 ±0.02 0.75 ±0.02
electricity 0.79±0.010.79±0.02 0.78 ±0.01 0.77 ±0.01 0.75 ±0.02
covertype 0.75 ±0.02 0.74 ±0.01 0.74 ±0.020.76±0.020.75±0.01
pol 0.96 ±0.010.97±0.010.96±0.01 0.95 ±0.01 0.93 ±0.01
house_16H 0.86 ±0.010.86±0.010.85±0.02 0.85 ±0.01 0.85 ±0.01
kdd_ipums_la_97-small 0.88 ±0.010.88±0.010.87±0.01 0.87 ±0.01 0.84 ±0.01
MagicTelescope 0.83±0.010.82±0.01 0.80 ±0.01 0.81 ±0.02 0.81 ±0.02
bank-marketing 0.78 ±0.020.78±0.010.75±0.02 0.78 ±0.01 0.77 ±0.01
phoneme 0.86±0.010.85±0.01 0.84 ±0.01 0.84 ±0.01 0.79 ±0.01
MiniBooNE 0.90 ±0.010.90±0.010.90±0.01 0.89 ±0.01 0.87 ±0.01
Higgs 0.66 ±0.01 0.66 ±0.01 0.65 ±0.020.67±0.010.65±0.02
eye_movements 0.56 ±0.01 0.56 ±0.010.63±0.020.57±0.02 0.56 ±0.01
jannis 0.73 ±0.02 0.73 ±0.01 0.73 ±0.020.74±0.010.73±0.01
tabular benchmark, classiﬁcation, with both numerical and categorical features
electricity 0.79 ±0.010.79±0.020.78±0.01 0.77 ±0.02 0.75 ±0.01
eye_movements 0.58 ±0.02 0.57 ±0.020.63±0.010.57±0.02 0.56 ±0.02
covertype 0.78 ±0.02 0.77 ±0.02 0.75 ±0.020.78±0.020.76±0.01
rl 0.69 ±0.01 0.71 ±0.010.77±0.010.73±0.02 0.63 ±0.01
road-safety 0.72 ±0.01 0.72 ±0.01 0.71 ±0.020.72±0.010.70±0.01
compass 0.68 ±0.020.69±0.020.67±0.02 0.68 ±0.02 0.68 ±0.02
KDDCup09_upselling 0.79 ±0.010.79±0.010.79±0.01 0.77 ±0.01 0.76 ±0.02
scikit-feature benchmark, classiﬁcation, with only numerical features
arcene 0.78 ±0.05 0.74 ±0.05 0.81 ±0.050.81±0.050.80±0.04
CLL_SUB_111 0.67 ±0.13 0.70 ±0.080.70±0.090.55±0.16 0.63 ±0.13
Prostate_GE 0.90 ±0.08 0.85 ±0.10 0.88 ±0.11 0.89 ±0.070.93±0.07
SMK_CAN_187 0.65 ±0.10 0.67 ±0.04 0.64 ±0.07 0.66 ±0.040.70±0.09
TOX_171 0.71 ±0.06 0.77 ±0.07 0.75 ±0.05 0.88 ±0.060.91±0.04
31Under review as submission to TMLR
Table S17: Number of features used per base model (tree) , i.e. for RS: the number Kof randomly
sampled features (optimized on the validation test), for PRS: the sum/summationtextM
j=1αj, and for RaSE: the average
feature subset size over the Tmodels. Values are means and standard deviations over 10 random data
subsamplings for the tabular datasets and over 5 cross-validation folds for the scikit-feature datasets.
RS-tree PRS-tree RaSE-tree
tabular benchmark, regression, with only numerical features
cpu_act 11.10 ±3.30 8.47±0.57 14.92±0.83
pol 26.00 ±0.00 10.95 ±0.64 18.01±1.51
elevators 7.70 ±0.90 6.03 ±0.31 5.81±0.47
isolet 189.90 ±53.01 48.75 ±3.3527.84 ±0.46
wine_quality 5.80 ±0.60 5.77 ±0.69 1.00±0.00
Ailerons 16.00 ±0.00 6.59±0.52 17.95±1.13
houses 4.00 ±0.00 3.55±0.17 4.50±0.55
house_16H 7.30 ±1.42 5.43±1.58 10.72±1.10
diamonds 2.50 ±0.50 2.41 ±0.18 1.00±0.00
Brazilian_houses 8.00 ±0.00 4.16 ±0.31 3.04±0.46
Bike_Sharing_Demand 3.00 ±0.00 2.66 ±0.36 1.09±0.14
nyc-taxi-green-dec-2016 3.70 ±0.46 1.23±0.20 1.55±0.28
house_sales 8.00 ±0.00 6.13±0.33 9.95±0.68
sulfur 4.50 ±1.50 2.20±0.60 3.86±0.57
medical_charges 3.00 ±0.00 1.78±0.16 2.29±0.19
MiamiHousing2016 6.00±0.00 6.58±0.31 8.85 ±0.32
superconduct 22.10 ±10.14 8.91±0.71 22.69±0.68
california 4.00±0.00 4.03±0.20 4.20 ±0.50
ﬁfa 2.00 ±0.00 2.09 ±0.33 1.43±0.50
year 39.30 ±9.24 8.94±1.88 23.27±0.62
tabular benchmark, regression, with both numerical and categorical features
yprop_4_1 10.70 ±6.10 4.80 ±0.99 3.48±1.26
analcatdata_supreme 12.00 ±0.00 1.62±0.27 2.65±1.33
visualizing_soil 5.00 ±0.00 4.19 ±0.26 3.90±0.16
black_friday 12.00 ±0.00 2.58 ±0.32 2.10±0.66
diamonds 15.60 ±5.20 11.94 ±1.06 18.09±0.98
Mercedes_Benz_Greener_Manufacturing 125.10 ±33.45 34.65 ±2.1417.00 ±3.31
Brazilian_houses 17.00 ±0.00 5.01 ±0.86 3.95±0.65
Bike_Sharing_Demand 15.00 ±5.00 7.91±0.56 9.60±1.12
OnlineNewsPopularity 16.00 ±8.05 6.50±1.13 7.47±2.38
nyc-taxi-green-dec-2016 15.40 ±1.80 2.78±0.38 6.39±2.90
house_sales 10.00 ±0.00 7.55±0.70 12.65±0.98
particulate-matter-ukair-2017 13.00 ±0.00 5.42 ±1.14 1.81±0.25
SGEMM_GPU_kernel_performance 15.00 ±0.00 3.37±0.37 6.14±1.17
tabular benchmark, classiﬁcation, with only numerical features
credit 3.60 ±0.92 4.42 ±0.49 2.85±0.26
california 3.70±0.46 4.03±0.21 3.85 ±0.73
wine 5.40 ±0.92 5.29±0.44 7.71±0.68
electricity 4.00 ±0.00 3.11±0.23 3.43±0.71
covertype 4.80 ±0.60 4.05±0.19 6.13±0.62
pol 14.30 ±3.90 9.52±0.71 19.54±1.09
house_16H 7.70 ±0.90 7.62±0.58 11.43±0.79
kdd_ipums_la_97-small 8.20 ±1.99 3.42±0.57 10.30±1.60
MagicTelescope 5.00±0.00 5.43±0.29 7.35 ±0.35
bank-marketing 3.60 ±0.49 3.41±0.15 5.27±0.79
phoneme 3.50 ±1.50 3.20±0.13 4.46±0.25
MiniBooNE 18.70 ±4.61 11.94 ±1.36 24.09±0.62
Higgs 10.80 ±1.83 6.08±0.77 14.59±1.96
eye_movements 3.10 ±1.76 2.73 ±0.30 2.17±0.27
jannis 18.50 ±6.95 9.05±0.88 23.98±1.17
tabular benchmark, classiﬁcation, with both numerical and categorical features
electricity 7.00 ±0.00 5.02±0.75 7.37±0.99
eye_movements 5.40 ±2.65 3.05 ±0.61 2.40±0.39
covertype 46.00 ±0.00 14.09 ±1.61 25.53±0.76
rl 16.60 ±2.94 5.70±0.73 17.67±3.00
road-safety 15.00 ±3.00 5.62±1.26 9.40±3.73
compass 23.80 ±7.07 6.67±2.13 17.46±3.32
KDDCup09_upselling 48.60 ±6.80 6.51±1.09 9.08±2.33
scikit-feature benchmark, classiﬁcation, with only numerical features
arcene 2100.00 ±3950.19 128.03 ±192.70 8.23±0.13
CLL_SUB_111 654.80 ±824.08 407.63 ±205.17 5.68±0.60
Prostate_GE 1948.40 ±2275.37 226.31 ±113.33 5.19±0.54
SMK_CAN_187 5811.20 ±7474.69 861.53 ±326.82 7.27±0.33
TOX_171 256.40 ±187.45 172.96 ±133.50 7.99±0.0932Under review as submission to TMLR
Table S18: Number of features used per base model (kNN) , i.e. for RS: the number Kof randomly
sampled features (optimized on the validation test), for PRS: the sum/summationtextM
j=1αj, and for RaSE: the average
feature subset size over the Tmodels. Values are means and standard deviations over 10 random data
subsamplings for the tabular datasets and over 5 cross-validation folds for the scikit-feature datasets.
RS-kNN PRS-kNN RaSE-kNN
tabular benchmark, regression, with only numerical features
cpu_act 15.50 ±5.50 5.59±0.54 6.00±1.27
pol 26.00 ±0.00 7.51±0.60 10.45±1.66
elevators 10.40 ±3.67 5.68 ±0.46 4.82±0.48
isolet 295.80 ±30.60 81.65 ±6.8629.86 ±0.20
wine_quality 6.00 ±0.00 5.76±0.32 7.64±0.66
Ailerons 27.90 ±7.79 6.21 ±0.37 5.96±0.70
houses 8.00 ±0.00 2.56±0.07 3.45±0.31
house_16H 8.80 ±2.40 5.89±1.06 10.89±1.04
diamonds 2.90 ±0.30 2.62±0.19 2.85±0.34
Brazilian_houses 5.50 ±2.06 2.82 ±0.33 2.73±0.68
Bike_Sharing_Demand 3.00 ±0.00 2.58 ±0.08 2.40±0.19
nyc-taxi-green-dec-2016 3.90 ±0.30 1.31±0.21 1.43±0.23
house_sales 9.40 ±2.80 4.89±0.31 6.48±0.85
sulfur 5.70 ±0.90 2.04±0.34 2.36±0.52
medical_charges 3.00 ±0.00 1.56 ±0.05 1.01±0.01
MiamiHousing2016 6.00±0.00 6.66±0.75 8.65 ±1.28
superconduct 11.50 ±3.69 8.55±1.39 21.05±2.52
california 7.60 ±1.20 3.47±0.13 5.08±0.53
ﬁfa 2.00±0.00 2.01±0.03 2.00 ±0.00
year 30.60 ±8.57 12.45 ±1.74 23.37±2.10
tabular benchmark, regression, with both numerical and categorical features
yprop_4_1 17.70 ±6.80 7.73±1.48 19.13±3.31
analcatdata_supreme 12.00 ±0.00 1.80±0.33 2.15±0.51
visualizing_soil 5.00 ±0.00 3.85 ±0.05 3.52±0.00
black_friday 5.60 ±1.80 2.25 ±0.29 2.17±0.42
diamonds 11.00 ±2.00 7.66±0.73 8.74±0.98
Mercedes_Benz_Greener_Manufacturing 77.60 ±25.63 37.59 ±3.1020.06 ±1.44
Brazilian_houses 7.60 ±0.80 2.88±0.49 3.16±0.80
Bike_Sharing_Demand 9.40 ±1.20 5.87±0.48 6.56±0.15
OnlineNewsPopularity 13.50 ±4.50 7.18±1.15 20.27±1.59
nyc-taxi-green-dec-2016 14.20 ±2.75 3.21±0.71 4.79±0.95
house_sales 11.80 ±3.60 5.52±0.70 7.55±1.11
particulate-matter-ukair-2017 11.40 ±1.96 3.88±0.61 4.46±0.60
SGEMM_GPU_kernel_performance 8.00 ±0.00 2.93 ±0.12 2.17±0.68
tabular benchmark, classiﬁcation, with only numerical features
credit 3.00±0.00 3.48±0.23 4.27 ±0.37
california 3.30±0.46 3.39±0.15 4.41 ±0.90
wine 4.40 ±1.11 3.89±0.15 7.00±0.45
electricity 4.20 ±0.98 2.72±0.19 3.45±0.77
covertype 4.40 ±0.92 3.10±0.16 4.07±0.41
pol 13.00 ±0.00 8.41±0.30 13.84±1.11
house_16H 5.30±1.49 5.43±0.39 10.61 ±0.88
kdd_ipums_la_97-small 4.90 ±1.92 2.46±0.28 3.46±0.50
MagicTelescope 5.50 ±1.50 4.22±0.22 5.93±0.54
bank-marketing 3.80 ±0.40 3.07±0.19 3.83±0.58
phoneme 3.80 ±1.47 2.41±0.28 4.03±0.44
MiniBooNE 10.90 ±4.18 7.71±1.81 23.51±2.22
Higgs 5.30 ±0.90 4.03±0.51 6.04±0.94
eye_movements 4.10 ±2.91 3.14±0.18 8.26±2.31
jannis 14.50 ±8.80 5.89±0.55 20.43±1.81
tabular benchmark, classiﬁcation, with both numerical and categorical features
electricity 6.10 ±1.14 4.36±0.30 5.94±1.64
eye_movements 5.00 ±1.55 3.45±0.40 11.90±4.06
covertype 35.50 ±6.87 10.46 ±0.88 24.06±1.50
rl 12.00 ±2.00 4.65±0.43 14.65±2.18
road-safety 12.10 ±3.65 5.88±0.45 14.07±1.52
compass 11.80 ±4.60 6.37±0.65 16.70±3.34
KDDCup09_upselling 17.80 ±10.15 5.82 ±0.42 5.19±0.59
scikit-feature benchmark, classiﬁcation, with only numerical features
arcene 4500.00 ±4494.44 328.85 ±103.85 9.26±1.05
CLL_SUB_111 109.20 ±71.57 222.60 ±103.72 5.52±1.35
Prostate_GE 1259.80 ±2353.18 4.77±1.77 6.08±0.68
SMK_CAN_187 29.00 ±56.00 743.66 ±165.22 8.00±0.79
TOX_171 1206.80 ±2270.71 111.80 ±67.258.53±0.6433Under review as submission to TMLR
Table S19: Number of features used per base model (SVM) , i.e. for RS: the number Kof randomly
sampled features (optimized on the validation test), for PRS: the sum/summationtextM
j=1αj, and for RaSE: the average
feature subset size over the Tmodels. Values are means and standard deviations over 10 random data
subsamplings for the tabular datasets and over 5 cross-validation folds for the scikit-feature datasets.
RS-SVM PRS-SVM RaSE-SVM
tabular benchmark, regression, with only numerical features
cpu_act 21.00 ±0.00 1.87±0.17 1.91±0.13
pol 26.00 ±0.00 1.61±0.66 5.42±0.27
elevators 1.00 ±0.00 0.02±0.02 8.68±0.00
isolet 613.00 ±0.00 42.45 ±4.3329.60 ±0.29
wine_quality 8.50 ±2.50 7.45±0.59 9.15±0.44
Ailerons 1.00 ±0.00 0.00±0.00 15.14±0.00
houses 8.00 ±0.00 5.64±0.65 6.43±0.68
house_16H 16.00 ±0.00 9.01±1.18 12.43±1.61
diamonds 3.00 ±0.00 3.08 ±0.57 2.90±0.69
Brazilian_houses 8.00 ±0.00 3.12 ±0.37 2.64±0.50
Bike_Sharing_Demand 5.70 ±0.90 1.85±0.12 2.00±0.00
nyc-taxi-green-dec-2016 9.00 ±0.00 1.42±0.54 3.04±0.45
house_sales 10.10 ±3.21 5.63±0.56 7.14±0.89
sulfur 4.10 ±1.58 0.30±0.06 3.97±1.25
medical_charges 3.00 ±0.00 1.46 ±0.18 1.02±0.03
MiamiHousing2016 13.00 ±0.00 9.34±0.39 10.59±0.32
superconduct 79.00 ±0.00 9.56±2.15 12.14±3.44
california 8.00 ±0.00 4.89±0.32 5.86±0.76
ﬁfa 5.00 ±0.00 1.75±0.13 2.00±0.00
year 90.00 ±0.00 12.06 ±2.97 15.03±3.77
tabular benchmark, regression, with both numerical and categorical features
yprop_4_1 1.80±2.40 2.11±2.07 14.49 ±1.94
analcatdata_supreme 12.00 ±0.00 1.31 ±0.16 1.01±0.01
visualizing_soil 5.00 ±0.00 3.70±0.17 4.00±0.00
black_friday 13.40 ±6.64 0.95±0.08 1.00±0.00
diamonds 26.00 ±0.00 13.70 ±1.57 18.41±1.27
Mercedes_Benz_Greener_Manufacturing 149.70 ±89.27 32.27 ±6.6921.54 ±7.33
Brazilian_houses 17.00 ±0.00 3.21 ±0.38 3.03±0.68
Bike_Sharing_Demand 12.20 ±6.78 2.09±0.32 2.51±0.41
OnlineNewsPopularity 24.30 ±4.73 11.50 ±1.87 22.58±2.08
nyc-taxi-green-dec-2016 31.00 ±0.00 3.03±1.18 5.68±1.46
house_sales 17.20 ±3.60 6.25±0.76 8.08±0.85
particulate-matter-ukair-2017 26.00 ±0.00 3.66±1.08 4.06±1.09
SGEMM_GPU_kernel_performance 15.00 ±0.00 2.49 ±0.45 1.43±0.25
tabular benchmark, classiﬁcation, with only numerical features
credit 4.40 ±3.17 2.32±0.21 3.93±1.03
california 8.00 ±0.00 3.43±0.34 5.39±0.72
wine 8.10 ±2.98 2.48±0.19 8.61±0.80
electricity 6.70 ±0.90 1.98±0.20 5.08±0.42
covertype 7.50 ±2.50 2.13±0.17 4.75±0.42
pol 22.10 ±5.96 5.07±1.07 18.02±2.24
house_16H 11.60 ±4.54 4.86±0.29 11.92±1.16
kdd_ipums_la_97-small 9.20 ±3.84 1.40±0.13 8.18±2.37
MagicTelescope 10.00 ±0.00 3.33±0.23 6.99±0.52
bank-marketing 6.70 ±0.90 2.65±0.16 4.81±0.49
phoneme 5.00 ±0.00 1.67±0.12 4.63±0.35
MiniBooNE 14.00 ±5.20 4.10±0.60 14.13±5.08
Higgs 5.00 ±3.03 2.98±0.31 8.71±1.54
eye_movements 7.80 ±4.92 2.09±0.23 10.22±3.36
jannis 30.80 ±15.94 3.12±0.41 23.65±2.36
tabular benchmark, classiﬁcation, with both numerical and categorical features
electricity 10.80 ±3.97 2.83±0.43 8.30±1.39
eye_movements 10.40 ±8.52 2.01±0.22 13.07±2.96
covertype 93.00 ±0.00 8.29±1.27 26.15±1.19
rl 24.10 ±9.27 2.73±0.65 18.99±2.88
road-safety 16.80 ±7.10 1.70±0.21 14.78±3.26
compass 39.80 ±24.20 3.53±0.27 19.33±3.48
KDDCup09_upselling 98.80 ±15.60 2.65±0.35 7.48±4.11
scikit-feature benchmark, classiﬁcation, with only numerical features
arcene 100.00 ±0.00 188.33 ±138.66 9.38±0.66
CLL_SUB_111 2333.20 ±4503.59 113.33 ±82.106.63±0.54
Prostate_GE 1254.80 ±2355.60 17.19 ±12.82 5.94±0.47
SMK_CAN_187 4055.40 ±7969.05 574.63 ±377.67 8.07±0.69
TOX_171 76.00 ±0.00 94.18 ±53.14 9.35±0.4434Under review as submission to TMLR
G Additional results on the DREAM4 networks
Net1 Net2 Net3 Net4 Net502468No. regulators
PRS - tree
lambda
0.0
0.001
0.002
0.003
0.004
0.005
0.006
0.007
0.008
0.009
0.01
Net1 Net2 Net3 Net4 Net50246810No. regulators
PRS - kNN
Net1 Net2 Net3 Net4 Net502468No. regulators
PRS - SVM
FigureS7: ExpectednumberofselectedcandidateregulatorsperbasemodelinaPRSensemble ,
i.e./summationtextM
j=1αj,g, whereMis the number of candidate regulators. The boxplots summarize the values over the
100 target genes.
35