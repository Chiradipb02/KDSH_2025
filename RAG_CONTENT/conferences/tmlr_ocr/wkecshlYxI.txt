Published in Transactions on Machine Learning Research (1/2023)
Revisiting adversarial training for the worst-performing class
Thomas Pethick thomas.pethick@epfl.ch
École Polytechnique Fédérale de Lausanne (EPFL)
Grigorios G Chrysos grigorios.chrysos@epfl.ch
École Polytechnique Fédérale de Lausanne (EPFL)
Volkan Cevher volkan.cevher@epfl.ch
École Polytechnique Fédérale de Lausanne (EPFL)
Reviewed on OpenReview: https: // openreview. net/ forum? id= wkecshlYxI
Abstract
Despite progress in adversarial training (AT), there is a substantial gap between the top-
performing and worst-performing classes in many datasets. For example, on CIFAR10, the
accuracies for the best and worst classes are 74% and 23%, respectively. We argue that this
gap can be reduced by explicitly optimizing for the worst-performing class, resulting in a
min-max-max optimization formulation. Our method, called class focused online learning
(CFOL), includes high probability convergence guarantees for the worst class loss and can
be easily integrated into existing training setups with minimal computational overhead.
We demonstrate an improvement to 32% in the worst class accuracy on CIFAR10, and
we observe consistent behavior across CIFAR100 and STL10. Our study highlights the
importance of moving beyond average accuracy, which is particularly important in safety-
critical applications.
1 Introduction
The susceptibility of neural networks to adversarial attacks (Goodfellow et al., 2014; Szegedy et al., 2013)
has been a grave concern over the launch of such systems in real-world applications. Defense mechanisms
that optimize the average performance have been proposed (Papernot et al., 2016; Raghunathan et al., 2018;
Guo et al., 2017; Madry et al., 2017; Zhang et al., 2019). In response, even stronger attacks have been
devised (Carlini & Wagner, 2016; Engstrom et al., 2018; Carlini, 2019).
In this work, we argue that the average performance is not the only criterion that is of interest for real-
world applications. For classification, in particular, optimizing the average performance provides very poor
guarantees for the “weakest” class. This is critical in scenarios where we require anyclass to perform well. It
turns out that the worst performing class can indeed be much worse than the average in adversarial training.
This difference is already present in clean training but we critically observe, that the gap between the average
and the worst is greatly exacerbated in adversarial training. This gap can already be observed on CIFAR10
where the accuracy across classes is far from uniform with 51%average robust accuracy while the worst
class is 23%(see Figure 1). The effect is even more prevalent when more classes are present as in CIFAR100
where we observe that the worst class has zeroaccuracy while the average accuracy is 28%(see Appendix C
where we include other datasets). Despite the focus on adverarial training, we note that the same effect can
be observed for robust evaluation after cleantraining (see Figure 4 §C).
This dramatic drop in accuracy for the weakest classes begs for different approaches than the classical
empirical risk minimization (ERM), which focuses squarely on the average loss. We suggest a simple al-
ternative, which we call class focused online learning (CFOL), that can be plugged into existing adversarial
training procedures. Instead of minimizing the average performance over the dataset we sample from an
adversarial distribution over classes that is learned jointly with the model parameters. In this way we aim
1Published in Transactions on Machine Learning Research (1/2023)
airplane
automobilebird catdeer dog froghorseshiptruck0.00.20.40.60.81.0Clean test accuracyClean training
airplane
automobilebird catdeer dog froghorseshiptruck0.00.20.40.60.81.0Robust test accuracyAdversarial training
Figure 1: The error across classes is already not perfectly uniform in clean training on CIFAR10. However,
this phenomenon is significantly worsened in adversarial training when considering the robust accuracy. That
is, some classes perform much worse than the average. The worst class accuracy and average accuracy is
depicted with a red and black line respectively.
at ensuring some level of robustness even for the worst performing class. The focus of this paper is thus on
the robust accuracy of the weakest classes instead of the average robust accuracy.
Concretely, we make the following contributions:
•We propose a simple solution which relies on the classical bandit algorithm from the online learning
literature, namely the Exponential-weight algorithm for Exploration and Exploitation (Exp3) (Auer
et al., 2002). The method is directly compatible with standard adversarial training procedures
(Madry et al., 2017), by replacing the empirical distribution with an adaptively learned adversarial
distribution over classes.
•We carry out extensive experiments comparing CFOL against three strong baselines across three
datasets, where we consistently observe that CFOL improves the weakest classes.
•We support the empirical results with high probability convergence guarantees for the worst class
accuracy and establish direct connection with the conditional value at risk (CVaR) (Rockafellar
et al., 2000) uncertainty set from distributional robust optimization.
2 Related work
Adversarial examples Goodfellow et al. (2014); Szegedy et al. (2013) are the first to make the important
observation that deep neural networks are vulnerable to small adversarially perturbation of the input. Since
then, there has been a growing body of literature addressing this safety critical issue, spanning from certified
robust model (Raghunathan et al., 2018), distillation (Papernot et al., 2016), input augmentation (Guo et al.,
2017), to adversarial training (Madry et al., 2017; Zhang et al., 2019). We focus on adversarial training in
this paper. While certified robustness is desirable, adversarial training remains one of the most successful
defenses in practice.
Parallel work (Tian et al., 2021) also observe the non-uniform accuracy over classes in adversarial training,
further strengthening the case that lack of class-wise robustness is indeed an issue. They primarily focus
on constructing an attack that can enlarge this disparity. However, they also demonstrate the possibility
of a defense by showing that the accuracy of class 3 can be improved by manually reweighting class 5
when retraining. Our method CFOL can be seen as automating this process of finding a defense by instead
adaptively assigning more weight to difficult classes.
Minimizing the maximum Focused online learning (FOL) (Shalev-Shwartz & Wexler, 2016), takes a
bandit approach similar to our work, but instead re-weights the distribution over the Ntraining examples
2Published in Transactions on Machine Learning Research (1/2023)
independent of the class label. This leads to a convergence rate in terms of the number of examples N
instead of the number of classes kfor which usually k≪N. We compare in more detail theoretically and
empirically in Section 4.2 and Section 5 respectively. Sagawa et al. (2019) instead reweight the gradient
over known groups which coincides with the variant considered in Section 4.1. A reweighting scheme has
also been considered for data augmentation (Yi et al., 2021). They obtain a closed form solution under a
heuristically driven entropy regularization and full information. In our setting full information would imply
full batch updates and would be infeasible.
Interpolations between average and maximum loss have been considered in various other settings: for class
imbalanced datasets (Lin et al., 2017), in federated learning (Li et al., 2019), and more generally the tilted
empirical risk minimization (Li et al., 2020; Lee et al., 2020).
Distributional robust optimization The accuracy over the worst class can be seen as a particular re-
weighing of the data distribution which adversarially assigns all weights to a single class. Worst case pertur-
bationofthedatadistributionhavemoregenerallybeenstudiedundertheframeworkofdistributionalrobust
stochastic optimization (DRO) (Ben-Tal et al., 2013; Shapiro, 2017). Instead of attempting to minimizing
the empirical risk on a training distribution P0, this framework considers some uncertainty set around the
training distribution U(P0)and seeks to minimize the worst case risk within this set, supQ∈U(P0)Ex∼Q[ℓ(x)].
A choice of uncertainty set, which has been given significant attention in the community, is conditional value
at risk (CVaR), which aims at minimizing the weighted average of the tail risk (Rockafellar et al., 2000; Levy
et al., 2020; Kawaguchi & Lu, 2020; Fan et al., 2017; Curi et al., 2019). CVaR has been specialized to a
re-weighting over class labels, namely labeled conditional value at risk (LCVaR) (Xu et al., 2020). This was
originally derived in the context of imbalanced dataset to re-balance the classes. It is still applicable in our
setting and we thus provide a comparison. The original empirical work of (Xu et al., 2020) only considers
the full-batch setting. We complement this by demonstrating LCVaR in a stochastic setting.
In Duchi et al. (2019); Duchi & Namkoong (2018) they are interested in uniform performance over various
groups, which is similarly to our setting. However, these groups are assumed to be latentsubpopulations,
which introduce significant complications. The paper is thus concerned with a different setting, an example
being training on a dataset implicitly consisting of multiple text corpora.
CFOL can also be formulated in the framework of DRO by choosing an uncertainty set that can re-weight
thekclass-conditional risks. The precise definition is given in Appendix B.1. We further establish a direct
connection between the uncertainty sets of CFOL and CVaR that we make precise in Appendix B.1, which
also contains a summary of the most relevant related methods in Table 5 §B.
3 Problem formulation and preliminaries
Notation The data distribution is denoted by Dwith examples x∈Rdand classes y∈[k]. A given
iteration is characterized by t∈[T], whilepy
tindicates the ythindex of the tthiterate. The indicator
function is denoted with 1{boolean}andunif(n)indicates the uniform distribution over nelements. An
overview of the notation is provided in Appendix A.
In classification, we are normally interested in minimizing the population risk E(x,y)∼D[ℓ(θ,x,y )]over our
model parameters θ∈Rp, whereℓis some loss function of θand example x∈Rdwith an associated class
y∈[k]. Madry et al. (2017) formalized the objective of adversarial training by replacing each example with
an adversarially perturbed variant. That is, we want to find a parameterization θof our predictive model
which solves the following optimization problem:
min
θL(θ) :=E(x,y)∼D/bracketleftbigg
max
δ∈Sℓ(θ,x+δ,y)/bracketrightbigg
, (1)
where each xis now perturbed by adversarial noise δ∈S⊆ Rd. Common choices of Sinclude norm-ball
constraints (Madry et al., 2017) or bounding some notion of perceptual distance (Laidlaw et al., 2020). When
the distribution over classes is uniform this is implicitly minimizing the averageloss over all class. This does
3Published in Transactions on Machine Learning Research (1/2023)
airplane
automobilebird catdeer dog froghorseshiptruck0.000.050.100.15pClass sampling distribution
Figure 2: Contrary to ERM, which samples the examples uniformly, CFOL samples from an adaptive
distribution. The learned adversarial distribution is non-uniform over the classes in CIFAR10 when using
adversarial training. As expected, the hardest classes are also most frequently sampled.
not guarantee high accuracy for the worstclass as illustrated in Figure 1, since we only know with certainty
that max≥avg.
Instead, we will focus on a different objective, namely minimizing the worst class-conditioned risk :
min
θmax
y∈[k]/braceleftbigg
Ly(θ) :=Ex∼pD(·|y)/bracketleftbigg
max
δ∈Sℓ(θ,x+δ,y)/bracketrightbigg/bracerightbigg
. (2)
Thisfollowsthephilosophythat“ a chain is only as strong as its weakest link ". Inasafetycriticalapplication,
such as autonomous driving, modeling even just a single class wrong can still have catastrophic consequences.
Imagineforinstanceastreetsignrecognitionsystem. Evenifthesystemhas99% averageaccuracythesystem
might never label a "stop"-sign correctly, thus preventing the car from stopping at a crucial point.
As the maximum in Equation (2) is a discrete maximization problem its treatment requires more care. We
will take a common approach and construct a convex relaxation by lifting the problem in Section 4.
4 Method
Since we do not have access to the true distribution D, we will instead minimize over the provided empirical
distribution. Let Nybe the set of data point indices for class ysuch that the total number of examples is
N=/summationtextk
y=1|Ny|. Then, we are interested in minimizing the maximum empirical class-conditioned risk ,
max
y∈[k]/hatwideLy(θ) :=1
|Ny|/summationdisplay
i∈Nymax
δ∈Sℓ(θ,xi+δ,y). (3)
We relax this discrete problem to a continuous problem over the simplex ∆k,
max
y∈[k]/hatwideLy(θ)≤max
p∈∆kk/summationdisplay
y=1py/hatwideLy(θ). (4)
Note that equality is attained when pis a dirac on the argmax over classes.
Equation (4) leaves us with a two-player zero-sum game between the model parameters θand the class
distribution p. A principled way of solving a min-max formulation is through the use of no-regret algorithms.
From the perspective of p, the objective is simply linear under simplex constraints, albeit adversarially picked
4Published in Transactions on Machine Learning Research (1/2023)
by the model. This immediately makes the no-regret algorithm Hedge applicable (Freund & Schapire, 1997):
wt
y=wt−1
y−η/hatwideLy(θt),
pt
y= exp/parenleftbig
wt
y/parenrightbig
/k/summationdisplay
y=1exp/parenleftbig
wt
y/parenrightbig
.(Hedge)
To show convergence for (Hedge) the loss needs to satisfy certain assumptions. In our case of classification,
the loss is the zero-one loss ℓ(θ,x,y ) = 1[hθ(x)̸=y], wherehθ(·)is the predictive model. Hence, the loss
is bounded, which is a sufficient requirement.
Note that (Hedge) relies on zero-order information of the loss, which we indeed have available. However, in
the current form, (Hedge) requires so called full information over thekdimensional loss vector. In other
words, we need to compute /hatwideLy(θ)for ally∈[k], which would require a full pass over the dataset for every
iteration.
Following the seminal work of Auer et al. (2002), we instead construct an unbiased estimator of the k
dimensional loss vector /hatwideL(θt) := (/hatwideL1(θt),...,/hatwideLk(θt))⊤based on a sampled class ytfrom some distribution
yt∼pt. This stochastic formulation further lets us estimate the class conditioned risk /hatwideLyt(θt)with an
unbiased sample i∼unif(|Nyt|). This leaves us with the following estimator,
/tildewideLt
y=/braceleftbiggLy,i(θt)/pt
yy=yt
0 otherwise(5)
whereLy,i(θ) := maxδ∈Sℓ(θ,xi+δ,y). It is easy to verify that this estimator is unbiased,
Ey∼p/bracketleftig
/tildewideLt
y/bracketrightig
=/parenleftbig
1−pt
y/parenrightbig
·0 +pt
y·/hatwideLy(θt)
pty=/hatwideLy(θt). (6)
For ease of presentation the estimator only uses a single sample but this can trivially be extended to a
mini-batch where classes are drawn i.i.d. from pt.
We could pick ptto be the learned adversarial distribution, but it is well known that this can lead to
unbounded regret if some pt
yis small (see Appendix A for definition of regret). Auer et al. (2002) resolve
this problem with Exp3, which instead learns a distribution qt, then mixes qtwith a uniform distribution,
to eventually sample from pt
y=γ1
k+ (1−γ)qt
ywhereγ∈(0,1). Intuitively, this enforces exploration. In the
general case γneeds to be picked carefully and small enough, but we show in Theorem 1 that a larger γis
possible in our setting. We explore the effect of different choices of γempirically in Table 3.
Our algorithm thus updates qt+1with (Hedge) using the estimator /tildewideLtwith a sample drawn from ptand
subsequently computes pt+1. CFOL in conjunction with the simultaneous update of the minimization player
can be found in Algorithm 1 with an example of a learned distribution ptin Figure 2.
Practically, the scheme bears negligible computational overhead over ERM since the softmax required to
sample is of the same dimensionality as the softmax used in the forward pass through the model. This
computation is negligible in comparison with backpropagating through the entire model. For further details
on the implementation we refer to Appendix C.3.
4.1 Reweighted variant of CFOL
Algorithm 1 samples from the adversarial distribution p. Alternatively one can sample data points uniformly
and instead reweight the gradients for the model using p. In expectation, an update of these two schemes
are equivalent. To see why, observe that in CFOL the model has access to the gradient ∇θLy,i(θ). We
can obtain an unbiased estimator by instead reweighting a uniformly sampled class, i.e. Ey∼p,i[∇θLy,i(θ)] =
Ey∼unif(k),i[kpy∇θLy,i(θ)]. Withclassessampleduniformlytheunbiasedestimatorfortheadversarybecomes
/tildewideLy′= 1{y′=y}Lyk∀y′. Thus, one update of CFOL and the reweighted variant are equivalent in expectation.
However, note that we additionally depended on the internals of the model’s update rule and that the
5Published in Transactions on Machine Learning Research (1/2023)
Algorithm 1: Class focused online learning (CFOL)
Algorithm parameters: a step rule ModelUpdate for the model satisfying Assumption 1, adversarial
step-sizeη>0, uniform mixing parameter γ=1/2, and the loss Ly,i(θ).
Initialization: Set w0= 0such thatq0andp0are uniform.
foreachtin0..Tdo
yt∼pt; // sample class
it∼unif(|Nyt|); // sample uniformly from class
θt+1= ModelUpdate( θt,Lyt,it(θt)); // update model parameters
/tildewideLt
y= 1{y=yt}Ly,it(θt)/pt
y∀y; // construct estimator
wt+1
y=wt
y−η/tildewideLt
y∀y; // update the adv. class distribution
qt+1
y= exp/parenleftbig
wt+1
y/parenrightbig
//summationtextk
y=1exp/parenleftbig
wt+1
y/parenrightbig
∀y;
pt+1
y=γ1
k+ (1−γ)qt+1
y∀y;
end foreach
immediate equivalence we get is only in expectation. These modifications recovers the update used in
Sagawa et al. (2019). See Table 9 in the supplementary material for experimental results regarding the
reweighted variant.
4.2 Convergence rate
To understand what kind of result we can expect, it is worth entertaining a hypothetical worst case scenario.
Imagine a classification problem where one class is much harder to model than the remaining classes. We
would expect the learning algorithm to require exposure to examples from the hard class in order to model
that class appropriately—otherwise the classes would not be distinct. From this one can see why ERM
might be slow. The algorithm would naively pass over the entire dataset in order to improve the hard class
using only the fraction of examples belonging to that class. In contrast, if we can adaptively focus on the
difficult class, we can avoid spending time on classes that are already improved sufficiently. As long as we
can adapt fast enough, as expressed through the regret of the adversary, we should be able to improve on
the convergence rate for the worst class.
We will now make this intuition precise by establishing a high probability convergence guarantee for the
worst class loss analogue to that of FOL. For this we will assume that the model parameterized by θenjoys
a so called mistake bound of C(Shalev-Shwartz et al., 2011, p. 288). The proof is deferred to Appendix A.
Assumption 1. Foranysequenceofclasses (y1,...,yT)∈[k]Tandclassconditionedsampleindices (i1,...,iT)
withit∈Nytthe model enjoys the following bound for some C′<∞andC= max{klogk,C′},
T/summationdisplay
t=1Lyt,it(θt)≤C. (7)
Remark 1.The requirement C≥klogkwill be needed to satisfy the mild step-size requirement η≤2kin
Lemma 1. In most settings the smallest C′is some fraction of the number of iterations T, which in turn is
much larger than the number of classes k, soC=C′.
With this at hand we are ready to state the convergence of the worst class-conditioned empirical risk.
Theorem 1. If Algorithm 1 is run on bounded rewards Lyt,it(θt)∈[0,1]∀twith step-size η=/radicalbig
logk/(4kC),
mixing parameter γ=1/2and the model satisfies Assumption 1, then after Titerations with probability at
least 1−δ,
max
y∈[k]1
nn/summationdisplay
j=1/hatwideLy/parenleftbig
θtj/parenrightbig
≤6C
T+/radicalbig
4klog(2k/δ)√
T+(1 + 2k) log(2k/δ)
3T
+/radicalbig
2 log(2k/δ)√n+2 log(2k/δ)
3n,(8)
6Published in Transactions on Machine Learning Research (1/2023)
Table 1: Accuracy on CIFAR10. For both clean test accuracy ( accclean) and robust test accuracy ( accrob) we
report the average, 20% worst classes and the worst class. We compare our method (CFOL-AT) with stan-
dard adversarial training (ERM-AT) and two baselines (LCVaR-AT and FOL-AT). CFOL-AT significantly
improves the robust accuracy for both the worst class and the 20% tail.
ERM-AT CFOL-AT LCVaR-AT FOL-AT
acccleanAverage 0.8244 0.8308 0.8259 0.8280
20% tail 0.6590 0.7120 0.6540 0.6635
Worst class 0.6330 0.6830 0.6340 0.6210
accrobAverage 0.5138 0.5014 0.5169 0.5106
20% tail 0.2400 0.3300 0.2675 0.2480
Worst class 0.2350 0.3200 0.2440 0.2370
airplane
automobilebird catdeer dog froghorseshiptruck0.00.20.40.6Robust test accuracyERM-AT
airplane
automobilebird catdeer dog froghorseshiptruck0.00.20.40.6Robust test accuracyCFOL-AT
Figure 3: The robust test accuracy for CFOL-AT and ERM-AT over classes. The horizontal black and red
line depicts the average and worst class accuracy over the classes respectively. The improvement in the
minimum accuracy is notable when using CFOL-AT, while there is only marginal difference in the average
accuracy. The evolution of accuracies for all methods can be found in Figure 7 §C.
for an ensemble of size nwheretjiid∼unif(T)forj∈[n].
TocontextualizeTheorem1, letusconsiderthesimplecaseoflinearbinaryclassificationmentionedinShalev-
Shwartz & Wexler (2016). In this setting SGD needs O(CN)iterations to obtain a consistent hypothesis.
In contrast, the iteration requirement of FOL decomposes into a sum /tildewideO(C+N)which is much smaller since
bothCandNare large. When we are only concerned with the convergence of the worst class we show that
CFOL can converge as /tildewideO(C+k)where usually k≪C. Connecting this back to our motivational example
in the beginning of this section, this sum decomposition exactly captures our intuition. That is, adaptively
focusing the class distribution can avoid the learning algorithm from needlessly going over all kclasses in
order to improve just one of them.
From Theorem 1 we also see that we need an ensemble of size n= Ω(/radicalbig
log(k/δ)/ε2), which has only mild
dependency on the number of classes k. If we wanted to drive the worst class error εto zero the dependency
onεwould be problematic. However, for adversarial training, even in the best case of the CIFAR10 dataset,
theaverageerror is almost 1/2. We can expect the worst class error to be even worse, so that only a small n
is required. In practice, a single model turns out to suffice.
Note that the maximum upper bounds the average, so by minimizing this upper bound as in Theorem 1, we
are implicitly still minimizing the usual average loss. In addition, Theorem 1 shows that a mixing parameter
ofγ=1/2is sufficient for minimizing the worst class loss. Effectively, the average loss is still directly being
minimized, but only through half of the sampled examples.
7Published in Transactions on Machine Learning Research (1/2023)
Table 2: Clean test accuracy ( accclean) and robust test accuracy ( accrob) on CIFAR100 and STL10. We
compare our method (CFOL-AT) with standard adversarial training (ERM-AT) and two baselines (LCVaR-
AT and FOL-AT). CFOL-AT consistently improves the worst class accuracy as well as the 20%worst tail.
ERM-AT CFOL-AT LCVaR-AT FOL-AT
CIFAR100acccleanAverage 0.5625 0.5593 0.5614 0.5605
20% tail 0.2710 0.3550 0.2960 0.2815
Worst class 0.0600 0.2500 0.0600 0.1100
accrobAverage 0.2816 0.2519 0.2866 0.2718
20% tail 0.0645 0.0770 0.0690 0.0630
Worst class 0.0000 0.0400 0.0000 0.0000
STL10acccleanAverage 0.7023 0.6826 0.6696 0.6890
20% tail 0.4119 0.4594 0.3600 0.3837
Worst class 0.3725 0.4475 0.3462 0.3562
accrobAverage 0.3689 0.3755 0.3864 0.3736
20% tail 0.0900 0.1388 0.0944 0.0981
Worst class 0.0587 0.1225 0.0650 0.0737
Table 3: Interpolation property of γon CIFAR10. By increasing the mixing parameter γ, CFOL-AT can
closely match the average robust accuracy of ERM-AT while still improving the worst class accuracy. Stan-
dard deviations are computed over 3 independent runs.
γ 0.9 0.7 0.5
Average 0.5108±0.0015 0.5091±0.0037 0.5012±0.0008
20% tail 0.2883 ±0.0278 0.3079 ±0.0215 0.3393±0.0398
Worst class 0.2717 ±0.0331 0.2965 ±0.0141 0.3083±0.0515
5 Experiments
We consider the adversarial setting where the constraint set of the attacker Sis anℓ∞-bounded attack, which
is the strongest norm-based attack and has a natural interpretation in the pixel domain. We test on three
datasets with different dimensionality, number of examples per class and number of classes. Specifically, we
consider CIFAR10, CIFAR100 and STL10 (Krizhevsky et al., 2009; Coates et al., 2011) (see Appendix C.2
for further details).
Hyper-parameters Unless otherwise noted, we use the standard adversarial training setup of a ResNet-18
network (He et al., 2016) with a learning rate τ= 0.1, momentum of 0.9, weight decay of 5·10−4, batch
size of 128with a piece-wise constant weight decay of 0.1at epoch 100and150for a total of 200epochs
according to Madry et al. (2017). For the attack we similarly adopt the common attack radius of 8/255using
7steps of projected gradient descent (PGD) with a step-size of 2/255(Madry et al., 2017). For evaluation
we use the stronger attack of 20 step throughout, except for Table 7 §C where we show robustness against
AutoAttack (Croce & Hein, 2020).
Baselines With this setup we compare our proposed method, CFOL, against empirical risk minimization
(ERM), labeled conditional value at risk (LCVaR) (Xu et al., 2020) and focused online learning (FOL)
(Shalev-Shwartz & Wexler, 2016). We add the suffix "AT" to all methods to indicate that the training
examples are adversarially perturbed according to adversarial training of Madry et al. (2017). We consider
ERM-AT as the core baseline, while we also implement FOL-AT and LCVaR-AT as alternative methods that
can improve the worst performing class. For fair comparison, and to match existing literature, we do early
stopping based on the average robust accuracy on a hold-out set. The additional hyperparameters for CFOL-
AT, FOL-AT and LCVaR-AT are chosen to optimize the robust accuracy for the worst class on CIFAR10.
This parameter choice is then used as the basis for the subsequent datasets. More details on hyperparameters
8Published in Transactions on Machine Learning Research (1/2023)
Table 4: Comparison with larger pretrained models from the literature. ERM-AT refers to the training
scheme of Madry et al. (2017) and uses the shared weights of the larger model from Engstrom et al. (2019).
We refer to the pretrained Wide ResNet-34-10 from (Zhang et al., 2019) as TRADES . For fair comparison
we apply CFOL-AT to a Wide ResNet-34-10 (with γ= 0.8), while using the test set as the hold-out set
similarly to the pretrained models. We observe that CFOL-AT improves the worst class and 20%tail for
both clean and robust accuracy.
TRADES ERM-AT CFOL-AT
acccleanAverage 0.8492 0.8703 0.8743
20% tail 0.6845 0.7220 0.7595
Worst class 0.6700 0.6920 0.7500
accrobAverage 0.5686 0.5490 0.5519
20% tail 0.3445 0.3065 0.3575
Worst class 0.2810 0.2590 0.3280
and implementation can be found in Appendix C.1 and Appendix C.3 respectively. In Table 9 §C we
additionally provide experiments for a variant of CFOL-AT which instead reweighs the gradients.
Metrics We report the average accuracy, the worst class accuracy and the accuracy across the 20% worst
classes (referred to as the 20% tail) for both clean ( accclean) and robust accuracy ( accrob). We note that the
aim is not to provide state-of-the-art accuracy but rather provide a fair comparison between the methods.
The first core experiment is conducted on CIFAR10. In Table 1 the quantitative results are reported with
the accuracy per class illustrated in Figure 3. The results reveal that all methods other than ERM-AT
improve the worst performing class with CFOL-AT obtaining higher accuracy in the weakest class than all
methods in both the clean and the robust case. For the robust accuracy the 20%worst tail is improved
from 24.0%(ERM-AT) to 26.7%using existing method in the literature (LCVaR-AT). CFOL-AT increases
the former accuracy to 33.0%which is approximately a 40%increase over ERM-AT. The reason behind the
improvement is apparent from Figure 3 which shows how the robust accuracy of the classes catand deer
are now comparable with birdanddog. A simple baseline which runs training twice is not able to improve
the worst class robust accuracy (Table 11 §C).
The results for the remaining datasets can be found in Table 2. The results exhibit similar patterns to the
experiment on CIFAR10, where CFOL-AT improves both the worst performing class and the 20% tail. For
ERM-AT on STL10, the relative gap between the robust accuracy for the worst class ( 5.9%) and the average
robust accuracy ( 36.9%) is even more pronounced. Interestingly, in this case CFOL-AT improves boththe
accuracy of the worst class to 12.3%and the average accuracy to 37.6%. In CIFAR100 we observe that both
ERM-AT, LCVaR-AT and FOL-AT have 0%robust accuracy for the worst class, which CFOL-AT increases
the accuracy to 4%.
In the case of CIFAR10, CFOL-AT leads to a minor drop in the average accuracy when compared against
ERM-AT. For applications where average accuracy is also important, we note that the mixing parameter γ
in CFOL-AT interpolates between focusing on the average accuracy and the accuracy of the worst class. We
illustrate this interpolation property on CIFAR10 in Table 3, where we observe that CFOL-AT can match
the average accuracy of ERM-AT closely with 51%while still improving the worst class accuracy to 30%.
In the supplementary, we additionally assess the performance in the challenging case of a test time attack
that differs from the attack used for training (see Table 7 §C for evaluation on AutoAttack). The difference
between CFOL-AT and ERM-AT becomes even more pronounced. The worst class accuracy for ERM-AT
drops to as low as 12.6%on CIFAR10, while CFOL-AT improves on this accuracy by more than 60%to an
accuracy of 20.7%. We also carry out experiments under a larger attack radius (see Table 8 §C). Similarly,
we observe an accuracy drop of the worst class on CIFAR10 to 9.3%for ERM-AT, which CFOL-AT improves
to17%. We also conduct experiments with the VGG-16 architecture, on Tiny ImageNet, Imagenette and
under class imbalanced (Tables 12 to 15 §C). In all experiments CFOL-AT consistently improves the
accuracy with respect to both the worst class and the 20% tail. We additionally provide results for early
9Published in Transactions on Machine Learning Research (1/2023)
stopped models using the best worst class accuracy from the validation set (see Table 6 §C). Using the
worst class for early stopping improves the worst class accuracy as expected, but is not sufficient to make
ERM-AT consistently competitive with CFOL-AT.
Even when compared with largerpretrained models from the literature we observe that CFOL-AT on the
smaller ResNet-18 model has a better worst class robust accuracy (see Table 4 for the larger models). For
fair comparison we also apply CFOL-AT to the larger Wide ResNet-34-10 model, while using the test set
as the hold-out set similarly to the pretrained models. We observe that CFOL-AT improves both the worst
class and 20%tail when compared with both pretrained ERM-AT and TRADES (Zhang et al., 2019).
Whereas pretrained ERM-AT achieves 30.65%accuracy for the 20% tail, CFOL-AT achieves 35.75%. While
the average robust accuracy of TRADES is higher, this is achieved by trading off clean accuracy.
Discussion We find that CFOL-AT consistently improves the accuracy for both the worst class and the
20% tail across the three datasets. When the mixing parameter γ= 0.5, this improvement usual comes at a
cost of the average accuracy. However, we show that CFOL-AT can closely match the average accuracy of
ERM-AT by picking γlarger, while maintaining a significant improvement for the worst class.
Overall, our experimental results validate our intuitions that (a) the worst class performance is often
overlooked and this might be a substantial flaw for safety-critical applications and (b) that algorithmically
focusing on the worst class can alleviate this weakness. Concretely, CFOL provides a simple method for
improving the worst class, which can easily be added on top of existing training setups.
6 Conclusion
In this work, we have introduced a method for class focused online learning (CFOL), which samples from
an adversarially learned distribution over classes. We establish high probability convergence guarantees for
the worst class using a specialized regret analysis. In the context of adversarial examples, we consider an
adversarialthreatmodelinwhichtheattackercanchoosetheclasstoevaluateinadditiontotheperturbation
(as shown in Equation (2)). This formulation allows us to develop a training algorithm which focuses on
being robust across allthe classes as oppose to only providing robustness on average. We conduct a thorough
empirical validation on five datasets, and the results consistently show improvements in adversarial training
over the worst performing classes.
7 Acknowledgements
This project has received funding from the European Research Council (ERC) under the European Union’s
Horizon 2020 research and innovation programme (grant agreement n °725594 - time-data). This work was
supportedbyZeiss. ThisworkwassupportedbyHaslerFoundationProgram: HaslerResponsibleAI(project
number 21043).
References
Jean-YvesAudibert,SébastienBubeck,andRémiMunos. Banditviewonnoisyoptimization. In Optimization
for Machine Learning , chapter 1. MIT Press, optimization for machine learning edition, January 2010.
Peter Auer, Nicolò Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed
bandit problem. SIAM Journal on Computing , 32(1):48–77, January 2002. ISSN 0097-5397, 1095-7111.
doi: 10.1137/S0097539701398375.
Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust
solutions of optimization problems affected by uncertain probabilities. Management Science , 59(2):341–
357, 2013.
Nicholas Carlini. Is ami (attacks meet interpretability) robust to adversarial examples? arXiv preprint
arXiv:1902.02322 , 2019.
10Published in Transactions on Machine Learning Research (1/2023)
Nicholas Carlini and David Wagner. Defensive distillation is not robust to adversarial examples. arXiv
preprint arXiv:1607.04311 , 2016.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature
learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics ,
pp. 215–223. JMLR Workshop and Conference Proceedings, 2011.
FrancescoCroceandMatthiasHein. Reliableevaluationofadversarialrobustnesswithanensembleofdiverse
parameter-free attacks. In International conference on machine learning , pp. 2206–2216. PMLR, 2020.
Sebastian Curi, Kfir Levy, Stefanie Jegelka, Andreas Krause, et al. Adaptive sampling for stochastic risk-
averse learning. arXiv preprint arXiv:1910.12511 , 2019.
John Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust
optimization. arXiv preprint arXiv:1810.08750 , 2018.
John C Duchi, Tatsunori Hashimoto, and Hongseok Namkoong. Distributionally robust losses against mix-
ture covariate shifts. Under review , 2019.
Logan Engstrom, Andrew Ilyas, and Anish Athalye. Evaluating and understanding the robustness of adver-
sarial logit pairing. arXiv preprint arXiv:1807.10272 , 2018.
Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robustness (python
library), 2019. URL https://github.com/MadryLab/robustness .
Yanbo Fan, Siwei Lyu, Yiming Ying, and Bao-Gang Hu. Learning with average top-k loss. arXiv preprint
arXiv:1705.08826 , 2017.
YoavFreundandRobertESchapire. Adecision-theoreticgeneralizationofon-linelearningandanapplication
to boosting. Journal of computer and system sciences , 55(1):119–139, 1997.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572 , 2014.
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. Countering adversarial images
using input transformations. arXiv preprint arXiv:1711.00117 , 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Kenji Kawaguchi and Haihao Lu. Ordered sgd: A new stochastic optimization framework for empirical risk
minimization. In International Conference on Artificial Intelligence and Statistics , pp. 669–679. PMLR,
2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense against unseen
threat models. arXiv preprint arXiv:2006.12655 , 2020.
Jaeho Lee, Sejun Park, and Jinwoo Shin. Learning bounds for risk-sensitive learning. arXiv preprint
arXiv:2006.08138 , 2020.
Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distributionally
robust optimization. arXiv preprint arXiv:2010.05893 , 2020.
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning.
arXiv preprint arXiv:1905.10497 , 2019.
Tian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith. Tilted empirical risk minimization. arXiv
preprint arXiv:2007.01162 , 2020.
11Published in Transactions on Machine Learning Research (1/2023)
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object
detection. In Proceedings of the IEEE international conference on computer vision , pp. 2980–2988, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense
to adversarial perturbations against deep neural networks. In 2016 IEEE symposium on security and
privacy (SP) , pp. 582–597. IEEE, 2016.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples.
arXiv preprint arXiv:1801.09344 , 2018.
R Tyrrell Rockafellar, Stanislav Uryasev, et al. Optimization of conditional value-at-risk. Journal of risk , 2:
21–42, 2000.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale
Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015.
doi: 10.1007/s11263-015-0816-y.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural
networksforgroupshifts: Ontheimportanceofregularizationforworst-casegeneralization. arXiv preprint
arXiv:1911.08731 , 2019.
Shai Shalev-Shwartz and Yonatan Wexler. Minimizing the maximal loss: How and why. In International
Conference on Machine Learning , pp. 793–801. PMLR, 2016.
Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and trends in
Machine Learning , 4(2):107–194, 2011.
Alexander Shapiro. Distributionally robust stochastic programming. SIAM Journal on Optimization , 27(4):
2258–2275, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556 , 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 , 2013.
QiTian, KunKuang, KeluJiang, FeiWu, andYisenWang. Analysisandapplicationsofclass-wiserobustness
in adversarial training. arXiv preprint arXiv:2105.14240 , 2021.
Ziyu Xu, Chen Dan, Justin Khim, and Pradeep Ravikumar. Class-weighted classification: Trade-offs and
robust approaches. In International Conference on Machine Learning , pp. 10544–10554. PMLR, 2020.
MingyangYi, LuHou, LifengShang, XinJiang, QunLiu, andZhi-MingMa. Reweightingaugmentedsamples
by minimizing the maximal expected loss. arXiv preprint arXiv:2103.08933 , 2021.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theo-
retically principled trade-off between robustness and accuracy. In International Conference on Machine
Learning , pp. 7472–7482. PMLR, 2019.
12Published in Transactions on Machine Learning Research (1/2023)
A Convergence analysis
A.1 Preliminary and notation
Consider the abstract online learning problem where for every t∈[T]the player chooses an action xtand
subsequently the environment reveals the loss function l(·,yt). As traditional in the online learning literature
we will measure performance in terms of regret, which compares our sequence of choices {xt}T
t=1with a fixed
strategy in hindsight u,
RT(u) =T/summationdisplay
t=1l(xt,yt)−min
xT/summationdisplay
t=1l(u,yt). (9)
If we, instead of minimizing over losses l, maximize over rewards rwe define regret as,
RT(u) = max
xT/summationdisplay
t=1r(u,yt)−T/summationdisplay
t=1r(xt,yt). (10)
When we allow randomized strategy, as in the bandit setting, RTbecomes a random variable that we wish
to upper bound with high probability. For convenience we include an overview over the notation defined and
used in Section 3 and Section 4 below.
k Number of classes
T Number of iterations
pt
y Theythindex of the tthiterate
1{boolean} The indicator function
unif(n) Discrete uniform distribution over nelements
Ny Set of data point indices for class y∈[k]
N=/summationtextk
y=1|Ny| The size of the data set
Ly,i(θ) := maxδ∈Sℓ(θ,xi+δ,y)Loss on a particular example
/hatwideLy(θ) :=1
|Ny|/summationtext
i∈NyLy,i(θ)The empirical class-conditioned risk
/hatwideL(θ) := (/hatwideL1(θ),...,/hatwideLk(θ))⊤The vector of all empirical class-conditioned risks
Lt
y:=Ly,i(θt) Class-conditioned estimator at iteration twithi∼unif(|Ny|)
A.2 Convergence results
We restate Algorithm 1 while leaving out the details of the classifier for convenience. Initialize w0such that
q0andp0are uniform. Then Exp3 proceeds for every t∈[T]as follows:
1. Draw class yt∼pt
2. Observe a scalar reward Lt
yt
3. Construct estimator /tildewideLt
y=Lt
yt 1{y=yt}/pt
y∀y
4. Update distribution
wt+1=wt−η/tildewideLt
qt+1
y= exp/parenleftbig
wt+1
y/parenrightbig
//summationtextk
y=1exp/parenleftbig
wt+1
y/parenrightbig
∀y
pt+1
y=γ1
k+ (1−γ)qt+1
y∀y
We can bound the regret of Exp3 (Auer et al., 2002) in our setting, even when the mixing parameter γis not
small as otherwise usually required, by following a similar argument as Shalev-Shwartz & Wexler (2016).
13Published in Transactions on Machine Learning Research (1/2023)
For this we will use the relationship between pandqthroughout. From py=γ
k+ (1−γ)qyit can easily be
verified that,
1
py≤k
γandqy
py≤1
1−γfor ally. (11)
Lemma 1. If Exp3 is run on bounded rewards Lt
yt∈[0,1]∀twithη≤γ/kthen
Radv
T(u) =T/summationdisplay
t=1⟨u,/tildewideLt⟩−T/summationdisplay
t=1⟨qt,/tildewideLt⟩≤logk
η+ηk
(1−γ)γT/summationdisplay
t=1Lt
y (12)
Proof.Wecanwritetheestimatorvectoras /tildewideLt=Lt
yt
pt
yteytwhereeytisacanonicalbasisvector. Byassumption
we haveLt
yt∈[0,1]. Combining this with the bound on 1/pyin Equation (11) we have /tildewideLt
y≤k/γfor ally. This
lets us invoke (Shalev-Shwartz et al., 2011, Thm 2.22) as long as the step-size ηis small enough. Specifically,
we have that if η≤1//tildewideLt
y≤γ/k
T/summationdisplay
t=1⟨u,/tildewideLt⟩−T/summationdisplay
t=1⟨qt,/tildewideLt⟩≤logk
η+ηT/summationdisplay
t=1k/summationdisplay
y=1qt
y(/tildewideLt
y)2. (13)
By expanding /tildewideLt
yand applying the inequalities from Equation (11) we can get rid of the dependency on q
andp,
logk
η+ηT/summationdisplay
t=1k/summationdisplay
y=1qt
y(/tildewideLt
y)2≤logk
η+ηT/summationdisplay
t=1qt
yt
(pt
yt)2(Lt
yt)2
≤logk
η+ηT/summationdisplay
t=1k
(1−γ)γ(Lt
yt)2
≤logk
η+ηk
(1−γ)γT/summationdisplay
t=1Lt
yt,(14)
where the last line uses the boundedness assumption Lt
y∈[0,1]∀y. □
It is apparent that we need to control the sum of losses. We will do so by assuming that the model admits
a mistake bound as in Shalev-Shwartz & Wexler (2016); Shalev-Shwartz et al. (2011).
Assumption 1. For any sequence of classes (y1,...,yT)∈[k]Tand class conditioned sample indices
(i1,...,iT)withit∈Nytthe model enjoys the following bound for some C′<∞andC= max{klogk,C′},
T/summationdisplay
t=1Lyt,it(θt)≤C. (15)
Recall that we need to bound Lt
y:=Ly,it(θt), whereyare picked adversarially and itare sampled uniformly,
so the above is sufficient.
Lemma 2. If the model satisfies Assumption 1 and Algorithm 1 is run on bounded rewards Lyt,it(θt)∈[0,1]
∀twith step-size η=/radicalbig
logk/(4kC)and mixing coefficient γ=1/2then
T/summationdisplay
t=1⟨u,/tildewideLt⟩≤6C. (16)
14Published in Transactions on Machine Learning Research (1/2023)
Proof.By expanding and rearranging Lemma 1,
T/summationdisplay
t=1⟨u,/tildewideLt⟩≤logk
η+/parenleftigg
ηk
(1−γ)γ+qt
yt
pt
yt/parenrightiggT/summationdisplay
t=1Lt
y, (17)
as long asη≤γ/k. Then by using our favorite inequality in Equation (11) and under Assumption 1,
logk
η+/parenleftigg
ηk
(1−γ)γ+qt
yt
pt
yt/parenrightiggT/summationdisplay
t=1Lt
y≤logk
η+/parenleftbiggηk
(1−γ)γ+1
1−γ/parenrightbigg
C, (18)
To maximize (1−γ)γ)we now pick γ=1/2so that,
logk
η+/parenleftbiggηk
(1−γ)γ+1
1−γ/parenrightbigg
C≤logk
η+ (4ηk+ 2)C. (19)
Forη, notice that it appears in two of the terms. To minimize the bound respect to ηwe pickη=/radicalbig
logk/(4kC)such thatlogk
η= 4ηkC, which leaves us with,
logk
η+ (4ηk+ 2)C≤2/radicalbig
4kClogk+ 2C. (20)
From the first term it is clear that since C≥klogkby assumption then the original step-size requirement
ofη≤1/2kis satisfies. In this case we can additionally simplify further,
2/radicalbig
4kClogk+ 2C≤6C. (21)
□
This still only gives us a bound on a stochastic object. We will now relate it to the empirical class conditional
risk/hatwideLy(θ)by using standard concentration bounds. To be more precise, we want to show that by picking
u=eyin⟨u,/tildewideLt⟩we concentrate to /hatwideLy(θ). Following Shalev-Shwartz & Wexler (2016) we adopt their use of
a Bernstein’s type inequality.
Lemma 3 (e.g. Audibert et al. (2010, Thm. 1.2)) .LetA1,...,ATbe a martingale difference sequence with
respect to a Markovian sequence B1,...,BTand assume|At|≤VandE/bracketleftbig
A2
t|B1,...,Bt/bracketrightbig
≤s. Then for any
δ∈(0,1),
P/parenleftigg
1
TT/summationdisplay
t=1At≤/radicalbig
2slog ( 1/δ)√
T+Vlog ( 1/δ)
3T/parenrightigg
≥1−δ. (22)
Lemma 4. If Algorithm 1 is run on bounded rewards Lyt,it(θt)∈[0,1]∀twith step-size η=/radicalbig
logk/(4kC),
mixing coefficient γ=1/2and the model satisfies Assumption 1, then for any y∈[k]we obtain have the
following bound with probability at least 1−δ/k,
1
TT/summationdisplay
t=1/hatwideLy/parenleftbig
θt/parenrightbig
≤6C
T+/radicalbig
4klog (k/δ)√
T+(1 + 2k) log (k/δ)
3T. (23)
Proof.Pickanyy∈[k]andletu=eyin⟨u,/tildewideLt⟩. Byconstructionthefollowingdefinesamartingaledifference
sequence,
At=/hatwideLy(θt)−⟨ey,/tildewideLt⟩=/hatwideLy(θt)−⟨ey,1
pt
ytLyt,it(θt)eyt⟩. (24)
In particular note that itis uniformly sampled. To apply the Bernstein’s type inequality we just need to
bound|At|andE/bracketleftbig
A2
t|qt,θt/bracketrightbig
. For|At|we can crudely bound it as,
|At|≤|/hatwideLy(θt)|+|⟨ey,1
pt
ytLyt,it(θt)eyt⟩|≤1 +k
γ. (25)
15Published in Transactions on Machine Learning Research (1/2023)
To bound the variance observe that we have the following:
E/bracketleftig
⟨ey,/tildewideLt⟩2|qt,θt/bracketrightig
≤k/summationdisplay
y′=1pt
y′
(pt
y′)2E/bracketleftbig
Ly′,i(θt)2|θt/bracketrightbig
(ey)y′ (withi∼unif(|Ny′|)∀y′)
=1
ptyE/bracketleftig
Ly,i/parenleftbig
θt/parenrightbig2|θt/bracketrightig
≤1
pty≤k
γ. (by Equation (11))
It follows that E/bracketleftbig
A2
t|qt,θt/bracketrightbig
≤k/γ
Invoking Lemma 3 we get the following bound with probability at least 1−δ/k,
1
TT/summationdisplay
t=1/hatwideLy/parenleftbig
θt/parenrightbig
≤1
TT/summationdisplay
t=1⟨ey,/tildewideLt⟩+/radicalig
2k
γlog (k/δ)
√
T+(1 +k
γ) log (k/δ)
3T(26)
By bounding the first term on the right hand side with Lemma 2 and taking γ=1/2we obtain,
1
TT/summationdisplay
t=1/hatwideLy/parenleftbig
θt/parenrightbig
≤6C
T+/radicalbig
4klog (k/δ)√
T+(1 + 2k) log (k/δ)
3T(27)
This completes the proof. □
We are now ready to state the main theorem.
Theorem 1. If Algorithm 1 is run on bounded rewards Lyt,it(θt)∈[0,1]∀twith step-size η=/radicalbig
logk/(4kC),
mixing parameter γ=1/2and the model satisfies Assumption 1, then after Titerations with probability at
least 1−δ,
max
y∈[k]1
nn/summationdisplay
j=1/hatwideLy/parenleftbig
θtj/parenrightbig
≤6C
T+/radicalbig
4klog(2k/δ)√
T+(1 + 2k) log(2k/δ)
3T+/radicalbig
2 log(2k/δ)√n+2 log(2k/δ)
3n,(28)
for an ensemble of size nwheretjiid∼unif(T)forj∈[n].
Proof.If we fixyand lettjiid∼unif(T), then the following is a martingale difference sequence,
Aj=/hatwideLy/parenleftbig
θtj/parenrightbig
−1
TT/summationdisplay
t=1/hatwideLy/parenleftbig
θt/parenrightbig
, (29)
for which it is easy to see that |Aj|≤2andA2
j≤1given boundedness of the loss. This readily let us apply
Lemma 3 with high probability 1−δ/2k. Combining this with the bound of Lemma 4 with probability 1−δ/2k
by using a union bound completes the proof. □
16Published in Transactions on Machine Learning Research (1/2023)
B CVaR
Conditional value at risk (CVaR) is the expected loss conditioned on being larger than the (1−α)-quantile.
This has a distributional robust interpretation which for a discrete distribution can be written as,
CVaRα(θ,P0) := sup
p∈∆m/braceleftiggm/summationdisplay
i=1piℓ(θ,xi)s.t.∥p∥∞≤1
αm/bracerightigg
. (30)
The optimal pof the above problem places uniform mass on the tail. Practically we can compute this
best response by sorting the losses {ℓ(θ,xi)}iin descending order and assigning1
αmmass per index until
saturation.
Primal CVaR Whenmis large a stochastic variant is necessary. To obtain a stochastic subgradient for
the model parameter, the naive approach is to compute a stochastic best response over a mini-batch. This
has been studied in detail in (Levy et al., 2020).
Dual CVaR An alternative formulation relies on strong duality of CVaR originally showed in (Rockafellar
et al., 2000),
CVaRα(θ,P0) = inf
λ∈R/braceleftigg
λ+1
αmm/summationdisplay
i=1(ℓ(θ,xi)−λ)+/bracerightigg
. (31)
In practice, one approach is to jointly minimize λand the model parameters by computing stochastic
gradients as done in (Curi et al., 2019). Alternatively, we can find a close form solution for λunder a
mini-batch (see e.g. (Xu et al., 2020, Appendix I.1)).
IncomparisonCFOLactsdirectlyontheprobabilitydistributionsimilarlytoprimalCVaR.However, instead
of finding a best response on a uniformly sampled mini-batch it updates the weights iteratively and samples
accordingly (see Algorithm 1). Despite this difference, it is interesting that a direct connection can be
established between the uncertainty sets of the methods. The following section is dedicated to this.
TobepedanticitisworthpointingoutaminordiscrepancywhenapplyingCVaRinthecontextofdeeplearn-
ing. Common architectures such as ResNets incorporates batch normalization which updates the running
mean and variance on the forward pass. The implication is that the entire mini-batch is used to update these
statistics despite the gradient computation only relying on the worst subset. This makes implementation in
this setting slightly more convoluted.
B.1 Relationship with CVaR
Exp3isrunwithauniformmixingtoenforceexploration. Thisturnsouttoimplythenecessaryandsufficient
condition for CVaR. We make this precise in the following lemma:
Lemma 5. Letpbe a uniform mixing p=γ1
m+(1−γ)qwith an arbitrary distribution q∈∆mandϵ∈[0,1]
such that the distribution has a lower bound on each element pi≥γ1
m. Then an upper bound is implicitly
implied on each element, ∥p∥∞:= maxi=[m]pi≤1−(m−1)γ
m.
Proof.Considerpifor anyi. At least (m−1)γ
mof the mass must be on other components so pi≤1−(m−1)γ
m.
The result follows. □
Corollary 1. Consider the uncertainty set of Exp3,
UExp3(P0) =/braceleftbigg
p∈∆m|pi≥γ1
m∀i/bracerightbigg
. (32)
Given that the above lower bound is only introduced for practical reasons, we might as well consider an
instantiation of CVaR which turns out to be a proper relaxation,
UCVaR(P0) =/braceleftbigg
p∈∆m|∥p∥∞≤1
αm/bracerightbigg
(33)
17Published in Transactions on Machine Learning Research (1/2023)
Table 5: Summary of methods where Nis the number of samples and kis the number of classes. The
discrete distribution peither governs the distribution over all Nsamples or over the kclasses depending on
the algorithm (see columns).
Uncertainty set Over data point ( m=N) Over class labels ( m=k)
UExp3(P0) =/braceleftbig
p∈∆m|pi≥ε1
m∀i/bracerightbig
FOL (Shalev-Shwartz & Wexler, 2016) CFOL (ours)
UCVaR(P0) =/braceleftbig
p∈∆m|∥p∥∞≤1
αm/bracerightbig
CVaR (Levy et al., 2020) LCVaR (Xu et al., 2020)
withα=1
(1−γ)m+γ. This leaves us with the following primal formulation,
CVaRα(θ,P0) := sup
p∈∆m/braceleftiggm/summationdisplay
i=1piℓ(θ,xi)s.t.∥p∥∞≤1
αm/bracerightigg
. (34)
Proof.From Lemma 5 and since CVaR requires ∥p∥∞≤1
αmwe have,
∥p∥∞≤1−(m−1)γ
m=:1
αm(35)
By simple algebra we have,
α=1
(1−γ)m+γ, (36)
which completes the proof. □
So if the starting point of the uncertainty set is the simplex and the uniform mixing in Exp3 is therefore
only for tractability reasons, then we might as well minimize the CVaR objective instead. This could even
potentially lead to a more robust solution as the uncertainty set is larger (since the upper bound in UCVaR
does not imply the lower bound in UExp3).
There are two things to keep in mind though. First, αshould not be too small since the optimization
problem gets harder. It is informative to consider the case where γ= 1/2such thatα=2
m+1. From this
it becomes clear that the recasting as CVaR only works for small m. Secondly, despite the uncertainty sets
being related, the training dynamics, and thus the obtained solution, can be drastically different, as we also
observe experimentally in Section 5. For instance CVaR is known for having high variance (Curi et al., 2019)
while the uniform mixing in CFOL prevents this. It is worth noting that despite this uniform mixing we are
still able to show convergence for CFOL in terms of the worst class in Section 4.2.
In Table 5 we provide an overview of the different methods induced by the choice of uncertainty set.
18Published in Transactions on Machine Learning Research (1/2023)
C Experiments
(a)∥δ∥∞= 1/255
airplane
automobilebird catdeer dog froghorseshiptruck0.00.20.40.60.81.0Robust test accuracy (b)∥δ∥∞= 2/255
airplane
automobilebird catdeer dog froghorseshiptruck0.00.20.40.60.81.0Robust test accuracy
Figure 4: Robust test accuracy under different norm-ball sizes after cleantraining on CIFAR10. The non-
uniform distribution over class accuracies, even after clean training, indicates that the inferior performance
on some classes is not a consequence of adversarial training. Rather, the inhomogeneity after perturbation
seems to be an inherent feature of the dataset.
Classes0.00.20.40.60.81.0Clean test accuracyClean training
Classes0.00.20.40.60.81.0Robust test accuracyAdversarial training
airplanebird car catdeer doghorse
monkeyshiptruck0.00.20.40.60.81.0Clean test accuracyClean training
airplanebird car catdeer doghorse
monkeyshiptruck0.00.20.40.60.81.0Robust test accuracyAdversarial training
Figure 5: Clean training and adversarial training on CIFAR100 (top) and STL10 (bottom) using ERM-
AT with PGD-7 attacks at train time and PGD-20 attacks at test time for the robust test accuracy. The
CIFAR100 classes are sorted for convenience. Notice that CIFAR100 has a class with zero robust accuracy
with adversarial training.
19Published in Transactions on Machine Learning Research (1/2023)
Classes0.00.10.20.30.40.50.60.7Robust test accuracyERM-AT
Classes0.00.10.20.30.40.50.60.7Robust test accuracyCFOL-AT
airplanebird car catdeer doghorse
monkeyshiptruck0.00.20.40.6Robust test accuracyERM-AT
airplanebird car catdeer doghorse
monkeyshiptruck0.00.20.40.6Robust test accuracyCFOL-AT
Figure 6: Robust class accuracy for CIFAR100 and STL10 respectively. Vertical black error bars indicate
one standard deviation. The red and black horizontal line indicates the minimum and average respectively.
The classes on both ERM-AT and CFOL-AT are ordered according to the accuracy on ERM-AT to make
comparison easier.
Table 6: For fair comparison we also consider early stopping based on the worst class accuracy on the
hold-out set. As can be observed the results for CFOL-AT do not differ significantly from early stopping
using the average robust accuracy, so standard training setups do not have to be modified further.
ERM-AT CFOL-AT LCVaR-AT FOL-AT
CIFAR10acccleanAverage 0.8348 0.8336 0.8298 0.8370
20% tail 0.6920 0.7460 0.6640 0.7045
Worst class 0.6710 0.7390 0.6310 0.7000
accrobAverage 0.4907 0.4939 0.5125 0.4982
20% tail 0.2895 0.3335 0.2480 0.2840
Worst class 0.2860 0.3260 0.2450 0.2780
CIFAR100acccleanAverage 0.5824 0.5537 0.5800 0.5785
20% tail 0.3115 0.3550 0.3065 0.3105
Worst class 0.1500 0.2200 0.1500 0.1700
accrobAverage 0.2622 0.2483 0.2629 0.2552
20% tail 0.0540 0.0790 0.0605 0.0595
Worst class 0.0200 0.0400 0.0100 0.0200
20Published in Transactions on Machine Learning Research (1/2023)
Table 7: Model performance on CIFAR10 (left) and Imagenette (right) under AutoAttack (Croce & Hein,
2020). The models still uses 7 steps of PGD at training time with a ℓ∞-constraint of 8/255. Only at test time
is the attack exchanged with AutoAttack under the same constraint. CFOL-AT is robust to AutoAttack in
the sense that the worst class performance is still improved. However, as expected, the performance is worse
for both methods in comparison with 20-step PGD based attacks.
ERM-AT CFOL-AT
acccleanAverage 0.8244 0.8342
20% tail 0.6590 0.7510
Worst class 0.6330 0.7390
accrobAverage 0.4635 0.4440
20% tail 0.1465 0.2215
Worst class 0.1260 0.2070ERM-AT CFOL-AT
acccleanAverage 0.8638 0.8650
20% tail 0.7687 0.7890
Worst class 0.7150 0.7709
accrobAverage 0.5911 0.5838
20% tail 0.3576 0.4087
Worst class 0.2254 0.3187
Table 8: Comparison between different sizes of ℓ∞-ball attacks on CIFAR10. The same constraint is used
at both training and test time. When the attack size is increased beyond the usual 8/255constraint we still
observe that CFOL-AT increases the robust accuracy for the weakest classes while taking a minor drop in
the average robust accuracy. Interestingly, the gap between ERM-AT and CFOL-AT seems to enlarge. See
Appendix C.2 for more detail on the attack hyperparameters.
ERM-AT CFOL-AT
∥δ∥∞≤8/255acccleanAverage 0.8244 0.8308
20% tail 0.6590 0.7120
Worst class 0.6330 0.6830
accrobAverage 0.5138 0.5014
20% tail 0.2400 0.3300
Worst class 0.2350 0.3200
∥δ∥∞≤12/255acccleanAverage 0.7507 0.7594
20% tail 0.4565 0.6060
Worst class 0.3850 0.5730
accrobAverage 0.4054 0.3873
20% tail 0.1180 0.2095
Worst class 0.0930 0.1700
Table 9: We test the reweighted variant of CFOL-AT (described in Section 4.1) on CIFAR10 and observe
similar results as for CFOL-AT. The experimental setup is described in Section 5.
accclean accrob
Average 20% tail Worst class Average 20% tail Worst class
CFOL-AT (reweighted) 0.8286 0.7465 0.7370 0.4960 0.3290 0.3100
21Published in Transactions on Machine Learning Research (1/2023)
Table 10: Mean and standard deviation computed over 3 independent executions using different random
seeds for both ERM-AT and CFOL-AT on CIFAR10.
ERM-AT CFOL-AT
acccleanAverage 0.8324±0.0069 0.8308±0.0054
20% tail 0.6467 ±0.0273 0.7192±0.0436
Worst class 0.5747 ±0.0506 0.6843±0.0121
accrobAverage 0.5158±0.0033 0.5012±0.0008
20% tail 0.2540 ±0.0428 0.3393±0.0398
Worst class 0.2067 ±0.0254 0.3083±0.0515
airplane
automobilebird catdeer dog froghorseshiptruck0.00.20.40.6Robust test accuracyERM-AT
airplane
automobilebird catdeer dog froghorseshiptruck0.00.20.40.6Robust test accuracyCFOL-AT
Table 11: A naive baseline improves the 20%-tail (marginally) over ERM-AT but is not able to improve
the worst class. The baseline first runs ERM-AT to obtain the robust class accuracies on the test set. The
negative accurcies are pushed through a softmax to obtain a distribution qand ERM-AT is rerun with classes
sampled from p=γ1
k+ (1−γ)q.
γ= 0.5γ= 0.9
acccleanAverage 0.8384 0.8277
20% tail 0.6810 0.6470
Worst class 0.6340 0.6430
accrobAverage 0.5070 0.5094
20% tail 0.2760 0.2530
Worst class 0.2180 0.2330
Table 12: We investigate the effect of using a diffent architecture (VGG-16 (Simonyan & Zisserman, 2014))
on CIFAR10. The worst classes for CFOL-AT remains improved over ERM-AT. Unsurprisingly the older
network performs worse for both methods when compared with their ResNet-18 counterpart.
ERM-AT CFOL-AT
acccleanAverage 0.7805 0.7904
20% tail 0.5385 0.6550
Worst class 0.5020 0.6220
accrobAverage 0.4799 0.4606
20% tail 0.2165 0.3075
Worst class 0.1940 0.3070
22Published in Transactions on Machine Learning Research (1/2023)
Table 13: Experiments on Tiny ImageNet (Russakovsky et al., 2015) which consists of 100,000 images across
200 classes. CFOL-AT improves the average accuracy for the 123 worst classes without optimizing the
hyperparameters (the hyperparameters remains the same as for CIFAR10, CIFAR100 and STL10).
ERM-AT CFOL-AT
acccleanAverage 0.4784 0.4606
20% tail 0.1725 0.2685
Worst class 0.0200 0.1600
accrobAverage 0.2349 0.2023
20% tail 0.0445 0.0735
Worst class 0.0000 0.0200
Table 14: Imbalanced CIFAR10 with imbalance factor 10 such that the majority class and the minority class
has 5000 and 500 training samples respectively. We early stop based on the uniformly distributed test set.
Three variants of CFOL are considered. Both the mixing distribution and initialization of the adversary q0
can be either uniform over classes (U) or according to the empirical training distribution (E). For instance if
q0follows the empirical distribution (E) and the mixing distribution is uniform (U), then we suffix CFOL-
AT with "EU". We observe that CFOL-AT EE in particular improves the worst class accuracies, while the
average accuracy incurs an even smaller drop than under uniform CIFAR10 (Table 1). It is interesting to
understand the seeming tradeoff between clean accuracy, average robust accuracy and robust worst class
accuracy cause by different instantiations of CFOL.
ERM-AT CFOL-AT UU CFOL-AT EU CFOL-AT EE
acccleanAverage 0.7300 0.7772 0.7708 0.7586
20% tail 0.5770 0.6780 0.6450 0.6420
Worst class 0.5410 0.6730 0.6360 0.6000
accrobAverage 0.4065 0.3988 0.4098 0.3991
20% tail 0.2200 0.2550 0.2625 0.2625
Worst class 0.2140 0.2220 0.2240 0.2590
23Published in Transactions on Machine Learning Research (1/2023)
0 2 4 6 8
Classes5
25
50
95
100
102EpochCFOL
0.00.10.20.30.40.50.60.70.8
0 2 4 6 8
Classes5
25
50
95
100
103EpochERM
0.00.10.20.30.40.50.60.70.8
0 2 4 6 8
Classes5
25
50
95
100
103EpochFOL
0.00.10.20.30.40.50.60.70.8
0 2 4 6 8
Classes5
25
50
95
100
103EpochLCVaR
0.00.10.20.30.40.50.60.70.8
Figure 7: The evolution of the robust validation accuracies across epochs. The final epoch is when the model
was early stopped based on the average robust accuracy. There is a clear pattern in what classes are harder
(class 2-5). Both CFOL and FOL have a curious non-monotonic evolution for the class conditional accuracy.
Notice for instance that both methods drop in accuracy for the 0th class at the stepsize change at epoch
100.
Table 15: Imagenette classification using ResNet-18.
ERM-AT CFOL-AT
acccleanAverage 0.8638 0.8650
20% tail 0.7687 0.7890
Worst class 0.7150 0.7709
accrobAverage 0.6285 0.6181
20% tail 0.4026 0.4534
Worst class 0.2850 0.3912
C.1 Hyperparameters
In this section we provide additional details to the hyperparameters specified in Section 5.
For CFOL-AT we use the same parameters as for ERM-AT described in Section 5. We set γ=1/2and the
adversarial step-size η= 2·10−6across all experiments, if not otherwise noted. One exception is Imagenette
were number of iterations are roughly 5 times fewer due to the smaller dataset. Thus, ηis picked 2.5times
larger as suggested by theory through η=˜O(1/√
T)in Theorem 1. CFOL-AT seems to be reasonable robust
to step-size choice as seen in Table 16. Similarly for FOL-AT we use the same parameters as for ERM-AT
and setη= 1·10−7after optimizing based on the worst class robust accuracy on CIFAR10.
The adversarial step-size picked for both FOL-AT and CFOL-AT is smaller in practice than theory suggests.
This suggests that the mistake bound in Assumption 1 is not satisfied for any sequence. Instead we rely on
24Published in Transactions on Machine Learning Research (1/2023)
Table 16: Hyperparameter exploration for CFOL-AT, LCVaR-AT and FOL-AT on CIFAR10.
Parameters accrob accclean
η α Average Worst class Average Worst class
CFOL-AT1×10−6- 0.5076 0.2700 0.8372 0.7110
2×10−6- 0.5014 0.3200 0.8308 0.6830
5×10−6- 0.4963 0.3000 0.8342 0.7390
1×10−5- 0.4927 0.2850 0.8314 0.7160
LCVaR-AT- 0.1 0.4878 0.1590 0.7909 0.5120
- 0.2 0.5116 0.1250 0.8274 0.4710
- 0.5 0.5104 0.1890 0.8177 0.5550
- 0.8 0.5169 0.2440 0.8259 0.6340
- 0.9 0.5249 0.1760 0.8233 0.5170
FOL-AT1×10−7- 0.5106 0.2370 0.8280 0.6210
5×10−7- 0.5038 0.2180 0.8372 0.6100
1×10−6- 0.4885 0.2190 0.8237 0.5880
5×10−6- 0.4283 0.1560 0.8043 0.5380
the sampling process to be only mildly adversarial initially as implicitly enforced by the small adversarial
step-size. It is an interesting future direction to incorporate this implicit tempering directly into the mixing
parameterγinstead.
For LCVaR-AT we optimized over the size of the uncertainty set by adjusting the parameter αon CIFAR10.
This leads to the choice of α= 0.8which we use across all subsequent datasets. The hyperparameter
exploration can be found in Table 16.
C.2 Experimental setup
In this section we provide additional details for the experimental setup specified in Section 5. We use one
GPU on an internal cluster. The experiments are conducted on the following five datasets:
CIFAR10 includes 50,000training examples of 32×32dimensional images and 10classes.
CIFAR100 includes 50,000training examples of 32×32dimensional images and 100classes.
STL10 includes 5000training examples of 96×96dimensional images and 10classes.
TinyImageNet 100,000training examples of 64×64dimensional images and 200classes.
Imagenette19469training examples of 160×160dimensional images and 10classes.
For all experiment we use data augmentation in the form of random cropping, random horizontal flip, color
jitter and 2 degrees random rotations. Prior to cropping a black padding is added with the exception of
Imagenette. For Imagenette we follow the standard for ImageNet evaluation and center crop the validation
and test images.
Early stopping As noted in Section 5 we use the averagerobust accuracy to early stop the model. In
contrast with common practice though, we use a validation set instead of the test set to avoid overfitting to
the test set. The class accuracies across the remaining two datasets, CIFAR100 and STL10, can be found in
Figure 6. We also include results when the model is early stopped based on the worst class accuracy on the
validation set in Table 6.
1https://github.com/fastai/imagenette
25Published in Transactions on Machine Learning Research (1/2023)
Attack parameters A radius of 8/255is used for the ℓ∞-constraint attack unless otherwise noted. For
training we use 7 steps of PGD and a step-size of 2/255. At test time we use 20 steps of PGD with a step-
size of 2.5×8/255
20. For 12/255-bounded attacks in Table 8 we scale the training step-size and test step-size
proportionally.
Larger models For the larger model experiments in Table 4 we train a Wide ResNet-34-10 using CFOL-
AT (withγ= 0.8). Similarly to the pretrained weights from the literature, we early stop based on the
test-set. We compare against TRADES (Zhang et al., 2019) (Wide ResNet-34-10) and the training setup of
Madry et al. (2017), which we throughout have denoted as ERM-AT, using shared weights from Engstrom
et al. (2018) (ResNet-50).
Mean & standard deviation In Table 10 we apply CFOL-AT and ERM-AT to CIFAR10 for multiple
different random seeds and verify that the observations made in Section 5 remains unchanged.
C.3 Implementation
We provide pytorch pseudo code for how CFOL can be integrated into existing training setups in Listing 1.
FOL is similar in structure, but additionally requires associating a unique index with each training example.
This allows the sampling method to re-weight each example individually.
For LCVaR we use the implementation of (Xu et al., 2020), which uses the dual CVaR formulation described
in Appendix B. More specifically, LCVaR uses the variant which finds a closed form solution for λ, since this
was observed to be both faster and more stable in their work.
Listing 1: Pseudo code for CFOL.
fromtorch . utils . data import DataLoader
fromclass_sampler import ClassSampler
sampler = ClassSampler(dataset , gamma=0.5)
dataloader = DataLoader(dataset , ... , sampler=sampler)
...
# Training loop :
forimg,y in iter (dataloader ):
# attack img
# compute gradients and update model
# compute logits
adv_loss = logits .argmax(dim= −1) != y
sampler .batch_update(y, eta ∗adv_loss)
26