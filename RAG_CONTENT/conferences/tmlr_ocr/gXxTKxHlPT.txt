Gaussian Pre-Activations in Neural Networks:
Myth or Reality?
Anonymous authors
Paper under double-blind review
Abstract
The study of feature propagation at initialization in neural networks lies at
the root of numerous initialization designs. An assumption very commonly
made in the ﬁeld states that the pre-activations are Gaussian. Although this
convenient Gaussian hypothesis can be justiﬁed when the number of neurons
perlayertendstoinﬁnity, itischallengedbyboththeoreticalandexperimental
works for ﬁnite-width neural networks. Our major contribution is to construct
a family of pairs of activation functions and initialization distributions that en-
sure that the pre-activations remain Gaussian throughout the network’s depth,
even in narrow neural networks. In the process, we discover a set of constraints
that a neural network should fulﬁll to ensure Gaussian pre-activations. Addi-
tionally, we provide a critical review of the claims of the Edge of Chaos line of
works and build an non-asymptotic Edge of Chaos analysis. We also propose
a uniﬁed view on pre-activations propagation, encompassing the framework
of several well-known initialization procedures. Finally, our work provides a
principled framework for answering the much-debated question: is it desirable
to initialize the training of a neural network whose pre-activations are ensured
to be Gaussian?
Notations and vocabulary
Bold letters Z,W,B,x... represent tensors of order larger or equal to 1. For a tensor
W∈Rn×p, we denote by Wij∈Rits component at the intersection of the i-th row and j-th
column, Wi·∈R1×pitsi-th row and W·j∈Rnitsj-th column. Upper-case letters W,X,
Y,Z,G... represent random variables. For a random variable Z, the function fZrepresents
its density, FZits Cumulative Distribution Function (CDF), SZits survival function, and
ψZits characteristic function. The depthof a neural network is its number of layers. The
widthof one layer is its number of neurons or convolutional units. The inﬁnite-width limit
of a neural network is the limiting case where the width of each layer tends to inﬁnity.
11 Introduction
Let us take a neural network with Llayers, in which every layer l∈[1,L]performs the
following operation:
Xl+1:=φ(Zl+1), (1)
with: Zl+1:=1√nlWlXl+Bl, (2)
where Xl+1∈Rnl+1is itsactivation ,Zl+1∈Rnl+1itspre-activation ,φis the coordinate-wise
activation function ,Wl∈Rnl+1×nlis theweight matrix of the layer, Bl∈Rnl+1itsvector
of biases , and Xl∈Rnlitsinput(also the preceding layer activation). This paper focuses
on the distribution of the pre-activations Zlaslgrows, for a ﬁxed input x, and weights Wl
and biases Blrandomly sampled from known distributions.
Recurring questions arise in both Bayesian deep learning and in parameter initialization
procedures: How to choose the distribution of the parameters Wl,Bland according to
whichcriteria, andhowshouldthedistributionofthepre-activations Zllooklike? Answering
these questions is fundamental to ﬁnding eﬃcient ways of initializing neural networks, that
is, appropriate distributions for WlandBlat initialization. In Bayesian deep learning, this
question is related to the search for a suitable prior, which is still a topic of intense research
(Wenzel et al., 2020; Fortuin et al., 2022).
Initialization strategies. A whole line of works in the ﬁeld of initialization strategies
for neural networks is based on the preservation of statistical characteristics of the pre-
activations when they propagate into a network. In short, the input of the neural network is
assumed to be ﬁxed, while all the parameters are considered as randomly drawn, according
to a candidate initialization distribution. Then, by using heuristics, some statistical charac-
teristics of the pre-activations are deemed desirable. Finally, these statistical characteristics
are propagated to the initialization distribution, which indicates how to choose it.
For instance, one of the ﬁrst results of this kind, proposed by Glorot & Bengio (2010), is
based on the preservation of the variance of both the pre-activations and the backpropagated
gradients across the layers of the neural network. The resulting constraint on the initializa-
tion distribution of the weights Wlis about its variance: Var(Wl
ij/√nl) = 2/(nl+nl+1).1
Then, He et al. (2015) have reﬁned this idea by taking into account the nonlinear deforma-
tion of the pre-activations by the activation function. They also showed that the inverted
arithmetical average 2/(nl+nl+1), resulting from a compromise between the preservation of
variance during both propagation and backpropagation, can be changed into 1/nlor1/nl+1
with negligible damage to the neural network.2Notably, with φ(x) = ReLU(x) := max(0,x),
they obtain Var(Wl
ij) = 2/nl, where the factor 2is meant to compensate for the loss of in-
formation due to the fact that ReLUis zero on R−.
1In the original paper, the considered weight matrix is ˜Wl:=Wl/√nl, soVar( ˜Wl
ij) = 2/(nl+nl+1).
2More generally, any choice of the form 1/(nα
ln1−α
l+1)withα∈[0,1]is valid, as long as the same αis used
for all layers.
2Afterthesestudies, Pooleetal.(2016)andSchoenholzetal.(2017)focusedonthecorrelation
betweenthepre-activationsoftwodatapoints xaandxb. Bypreservingthiscorrelationwhen
propagating the inputs into the neural network, information about the geometry of the input
space is meant to be preserved. So, training the weights is meaningful at initialization,
regardless of their positions in the network. This heuristic is ﬁner than the preceding ones
since attention is paid to the correlation between pre-activations and their variance (a joint
criterion is used instead of a marginal one). The range of valid initialization distributions is
changed accordingly, with a relation between σ2
w= Var(Wl
ij)andσ2
b= Var(Bl
i)that should
be ensured. This speciﬁc relationship is referred to as the Edge of Chaos (EOC).
Finally, it is worth mentioning the work of Hayou et al. (2019), in which the usual claims
about the EOC initialization are tested with several choices of activation functions. Notably,
the authors have run a large series of experiments in order to check whether the intuition
behind the EOC initialization leads to better performance after training. Also, at the op-
posite of ﬁnding an initialization distribution for the parameters, Klambauer et al. (2017)
focused on tuning the parameters of the activation function (leading to the SELUactivation
function). As in the preceding techniques, they aim for variance-preserving layers.
In the following, the term “Edge of Chaos” is used in two diﬀerent manners: “EOC frame-
work”, “EOC formalism”, or “EOC theory” refer to a setup where input data points are
deterministic and weights and biases are random, whereas in the context of initialization of
weights and biases, “EOC” alone refers to a speciﬁc set of pairs (σ2
w,σ2
b)matching a given
theoretical condition (Point 2, see Section 2.1).
Bayesian prior and initialization distribution. There exists a close relationship be-
tween the initialization distribution in deterministic neural networks and the prior distribu-
tioninBayesianneuralnetworks. Forinstance,letususevariationalinferencetoapproximate
the Bayesian posterior of the parameters of a neural network (Graves, 2011). In this case,
the Bayesian posterior is approximated sequentially by performing a gradient descent over
the so-called “variational parameters” (Hoﬀman et al., 2013). This technique requires to
backpropagate the gradient of the loss through the network, as when training determinis-
tic networks. Therefore, as with the initialization distribution, the prior distribution must
be constructed in such a way that the input and the gradient of the loss propagate and
backpropagate correctly (see Sec. 2.2, Ollivier, 2018).
Gaussian hypothesis for the pre-activations. Within a context of random weights and
biases, we call the Gaussian hypothesis the assumption that all the pre-activations Zl
iare
Gaussian random variables, at any layer land for any neuron i. This hypothesis is common
in the theoretical analysis of the properties of neural networks at initialization. Speciﬁcally,
thisisafundamentalassumptionwhenstudyingthe“NeuralTangentKernels”(NTK)(Jacot
et al., 2018) or Edge of Chaos (Poole et al., 2016). In a nutshell, the NTK is an operator
describing the optimization trajectory of an inﬁnitely wide neural network (NN), which is
believed to help understand the optimization of ordinary NNs. On one side, this Gaussian
hypothesis can be justiﬁed in the case of “inﬁnitely wide” NNs (i.e., when the widths nlof
3the layers tend to inﬁnity), by application of the Central Limit Theorem (Matthews et al.,
2018). On the other side, this Gaussian hypothesis is apparently necessary to get the results
of the EOC and NTK lines of work. However, it remains debated for both theoretical and
practical reasons.
First, from a strictly theoretical point of view, it has been shown that, for ﬁnite-width NNs
(ﬁnitenl), the distribution of Zl
ihas heavier tails as lincreases, that is, as information
ﬂows from the input to the output (Vladimirova et al., 2019; 2021). Second, a series of
experiments tend to show that pushing the distribution of the pre-activations towards a
Gaussian (e.g., through a speciﬁc Bayesian prior) leads to worse performances than pushing
it towards distributions with heavier tails, e.g., Laplace distribution (Fortuin et al., 2022).
Besides, the condition under which the Gaussian hypothesis remains valid is an important
source of confusion. As an example, Sitzmann et al. (2020) state that: “for a uniform input
in[−1,1], the [pre-]activations throughout a SIREN3are standard normal distributed [...],
irrespective of the depth of the network, if the weights are distributed uniformly in the
interval [−c,c]withc=√
6/nlin each layer [ l].” (Theorem 1.8, Appendix 1.3). Though
this formal statement seems to hold for all layers and whatever their widths, it is only an
asymptotic result, since it uses the Central Limit Theorem in its proof. Consequently, this
theorem is not usable in practical SIRENs, since it does not provide any speed of convergence
of the distribution of the pre-activations to a Gaussian, as each nltends to inﬁnity.4
Contributions. Our goals are twofold: ﬁrst, we aim to reproduce and test the results of
thepapersintheEOClineofworks; second, weaimtomovebeyondtheGaussianhypothesis
in ﬁnite neural networks. Accordingly, we have obtained the following results:
•we experimentally demonstrate that the Gaussian hypothesis is mostly invalid in
multilayer perceptrons with ﬁnite width;
•we show that, contrary to a claim of Poole et al. (2016) and usual practical results
in the EOC framework, the variance of the pre-activations does not always have at
most one nonzero attraction point; we provide an example of an activation function
for which the number of such attraction points is inﬁnite;
•we deduce a set of constraints that the activation function and the initialization
distribution of weights and biases must fulﬁll to guarantee Gaussian pre-activations
at initialization (including with ﬁnite-width layers);
•we propose a new family of activation functions and initialization distributions de-
signed to achieve this goal (Gaussian pre-activations at initialization);
3Sinusoidal representation network.
4Hopefully, according to Matthews et al. (Th. 4, 2018), the pre-activations tend to become Gaussian
irrespective of the growth rates of each nl, so Theorem 1.8 of Sitzmann et al. (2020) is asymptotically true
for all layers and all growth rates of each nl. But this result still does not provide any convergence speed.
4•we demonstrate empirically that the distribution of the pre-activations always tends
to drift away from the standard Gaussian distribution during propagation; however,
this drift is much greater when using tanhandReLUactivation functions than ours.
Additionally, we train, evaluate and compare neural networks built according to our family of
activation functions and initialization distributions, and usual ones ( tanhorReLU, Gaussian
EOC initialization).
Summary of the paper. As a preliminary, we make in Section 2 a critical review of
several results about pre-activations propagation in a neural network: the discussion, ad-
ditional experiments, and the criticism we are proposing, particularly in the EOC line of
works, are the foundations of our contributions. In Section 3, we propose a new family of
activation functions, along with a family of initialization distributions. They are deﬁned so
as to ensure that the pre-activations distribution propagates without deformation across the
layers, including with networks that are far from the “inﬁnite-width limit”. More speciﬁcally,
we ensure that the pre-activations remain Gaussian at any layer, and we provide a set of
constraints that the activation function and the initialization distribution of the parameters
should match to attain this goal. Finally, we propose in Section 4 a series of simulations
in order to check whether our propositions meet the requirement of maintaining Gaussian
pre-activations across neural networks. We also show the performance of trained neural
networks in diﬀerent setups, including standard ones and the one we are proposing.
2 Propagating pre-activations
Inthissection, weproposeacriticalreviewofseveralaspectsoftheEdgeofChaosframework.
We recall the fundamental ideas of the EOC in Section 2.1. In Section 2.2, we perform some
experiments at the initialization of a multilayer perceptron, in which we propagate data
points sampled from CIFAR-10. These results illustrate a limitation of the EOC framework
when using neural networks with a small number of neurons per layer. Then, we build in
Section 2.3 an activation function such that the variance of the propagated pre-activations
admits an inﬁnite number of stable ﬁxed points, which is a counterexample to a claim of
Poole et al. (2016). Finally, we propose in Section 2.4 a uniﬁed representation of several
initialization procedures.
2.1 Propagation of the correlation between data points
“Edge of Chaos” (EOC) framework. In the EOC line of work, the inputs of the neural
network are supposed to be ﬁxed, while the weights and biases are random. In order to
study the propagation of the distribution of the pre-activations Zl, Poole et al. (2016) and
5Schoenholz et al. (2017) propose to study two quantities:
vl
a:=E[(Zl
j;a)2], (3)
cl
ab:=1/radicalBig
vl
avl
bE[Zl
j;aZl
j;b], (4)
whereZl
j;ais thej-th coordinate of the vector Zl
aof pre-activations before layer l, when the
input of the neural network is a data point xa. The expectation is computed over the full
set of the parameters, i.e., weights and biases. So, we can interpret vl
aas the variance of
the pre-activations of a data point xa, andcl
abas the correlation between the pre-activations
of two data points xaandxb, over random initializations of the parameters, distributed
independently in the following way:
Wl
ij∼Pw(σw)with E[Wl
ij] = 0and Var(Wl
ij) =σ2
w, (5)
Bl
i∼Pb(σb)with E[Bl
i] = 0and Var(Bl
i) =σ2
b. (6)
Remark 1. Since the parameters are sampled independently with zero-mean, then:
E[Zl
j1;aZl
j2;a] =vl
aδj1j2,
1/radicalBig
vl
avl
bE[Zl
j1;aZl
j2;b] =cl
abδj1j2.
This is why Deﬁnitions (3)and(4)do not depend on j, and the crossed terms in j1andj2
are not worth studying (they are zero).
Remark 2. The distributions of Wl
ijandBl
iconsidered by Poole et al. (2016) and Schoenholz
et al. (2017) are normal with zero-mean, that is:
Pw(σw) =N(0,σ2
w)and Pb(σb) =N(0,σ2
b).
We loosen this assumption in (5)and(6), where we assume that these random variables are
zero-mean with a variance we can control. Their theoretical claim remain valid under this
broader assumption.
Theoretical analysis. Given two ﬁxed inputs xaandxb, the goal of the EOC theory is
to study the propagation of the correlation cl
abofZl
j;aandZl
j;b. Poole et al. (2016) and
Schoenholz et al. (2017) were the ﬁrst to:
1. build recurrence equations for (cl
ab)lof the form: cl+1
ab=f(cl
ab);
2. describe the dynamics of (cl
ab)l;
3. provide a procedure to compute the variance of the weights’ and biases’ distributions
such that (cl
ab)ltends to 1with a sub-exponential rate (instead of an exponential
rate).
6Point 1: recurrence equations. Point 1 is achieved by using the Gaussian hypothesis
for the pre-activations. That is, the distribution of the pre-activations Zlis assumed to be
Gaussian, whatever the layer land its width nl. The recurrence equations deﬁne a variance
mapVand a correlation map Cas follows:
vl+1
a=V(vl
a|σw,σb) :=σ2
w/integraldisplay
φ/parenleftbigg/radicalBig
vl
az/parenrightbigg2
Dz+σ2
b (7)
cl+1
ab=C(cl
ab,vl
a,vl
b|σw,σb) :=1/radicalBig
vl+1
avl+1
b/bracketleftbigg
σ2
w/integraldisplay
φ/parenleftbigg/radicalBig
vl
az1/parenrightbigg
φ/parenleftbigg/radicalBig
vl
bz/prime
2/parenrightbigg
Dz1Dz2+σ2
b/bracketrightbigg
,(8)
wherez/prime
2:=cl
abz1+/radicalBig
1−(cl
ab)2z2,andDz:=1√
2πexp/parenleftBigg
−z2
2/parenrightBigg
dz. (9)
These equations are approximations of the true information propagation dynamics, which
involves necessarily the number of neurons nlper layer. Actually, passing a Gaussian vector
Zlthrough a layer with random Gaussian weights and biases produces a pre-activation Zl+1
with a distribution which is diﬃcult to describe. On one side, as the dimension nlof the
input Zltends to inﬁnity, the Central Limit Theorem (CLT) applies, and the output Zl+1
tends to become Gaussian.5The assumption that the components of Zl+1are Gaussian
is referred to as the Gaussian hypothesis . On the other side, with ﬁnite nl, the tail of
the distribution of Zl+1has been proven to become heavier than the Gaussian one, both
theoretically (Vladimirova et al., 2019; 2021) and experimentally (see Section 2.2).
Finally, by assuming that (vl
a)land(vl
b)lhave already converged to the same limit v∗/negationslash= 0as
l→∞, it is possible to rewrite Equation (8) in a nicer way:
cl+1
ab=C∗(cl
ab) :=C(cl
ab,v∗,v∗|σw,σb). (10)
Now that the dynamics of (cl
ab)lis written in the form cl+1
ab=C∗(cl
ab), it becomes suﬃcient
to plotC∗to study its convergence. The two hypotheses made, i.e., the Gaussian one and
the instant convergence of (vl
a)land(vl
b)lto a unique nonzero limit v∗, are fundamental to
obtaining the simple equation of evolution (10).
Point 2: dynamics of the correlation through the layers. Then, Point 2 can be
achieved. Now that the trajectory of (cl
ab)lis determined only by the function C∗, it be-
comes easy to ﬁnd numerically its limit c∗and its rate of convergence. Speciﬁcally, we can
distinguish three possible cases:
•chaotic phase :liml→∞cl
ab=c∗<1. The correlation between Zl
j;aandZl
j;btends to
a constant that is strictly less than 1. So, even if Z1
j;aandZ1
j;bare highly correlated
(which means that xaandxbare close to each other), they tend to decorrelate when
going deeper in the network;
5Even if the coordinates Zl
jare dependent, the CLT is still valid, as proven by Matthews et al. (2018) by
using properties of exchangeable random variables (De Finetti, 1937).
7•ordered phase :liml→∞cl
ab=c∗= 1withC/prime
∗(1)<1(the prime here denotes the
derivative of a function). The correlation between Zl
j;aandZl
j;btends to 1with an
exponential rate, including when Z1
j;aandZ1
j;bare almost fully decorrelated;
•edge of chaos :liml→∞cl
ab=c∗= 1withC/prime
∗(1) = 1. The correlation between Zl
j;aand
Zl
j;btends to 1with a sub-exponential rate, including when Z1
j;aandZ1
j;bare almost
fully decorrelated.
Point 3: best choices for the initialization distribution. Poole et al. (2016) and
Schoenholz et al. (2017) claim that pairs (σ2
w,σ2
b)which lead either to the chaotic phase or
the ordered phase should be avoided. In both cases, we expect that information contained
in the propagated data (or the backpropagated gradients) would vanish at an exponential
rate. So, we want to ﬁnd pairs (σ2
w,σ2
b)lying “at the edge of chaos”, that is, making the
sequence (cl
ab)lconverge to 1at a sub-exponential rate. The Edge of Chaos initializations are
the initialization distributions of the weights and the biases such that the pair of variances
(σ2
w,σ2
b)lies at the Edge of Chaos.
Remark 3. Even in the favorable edge of chaos conﬁguration, the sequence of correlations
(cl
ab)ltends to 1, whatever the data points xaandxb. So a loss of information at initialization
seems unavoidable in very deep networks.
If one wants to create an initialization procedure with a smaller information loss, it becomes
reasonable to consider data-dependent initialization schemes (i.e., a warm-up phase before
training). Such an initialization strategy has been sketched by Mao et al. (2021), who make
use of the “Information Bottleneck” formalism (Tishby, 1999; Shwartz-Ziv & Tishby, 2017;
Saxe et al., 2019).
Strengths and weaknesses of the Edge of Chaos framework. One key feature of the
Edge of Chaos framework is the simplicity of the recurrence equation (10): it involves only
the correlation cl
abas a variable, and all other parameters (such as vl
aandvl
b) are assumed
to be ﬁxed once and for all. Notably, in Equation (8), the computation of cl+1
abinvolves the
distribution of the pre-activations outputted by layer l, which is assumed to be D=N(0,1).
In other words, the distribution Dlof the pre-activations Zlis assumed to be constant and
equal toD. However, in neural networks with ﬁnite widths, Dlis not constant and evolves
according to a propagation equation:
Dl+1is the distribution of Zl+1=1√nlWlφ(Zl) +Bl,where Zl∼Dl.
Such an equation involves a sum of products of random variables, which is usually diﬃcult to
keep track of.6Though Noci et al. (2021) have proposed an analytical procedure to compute
theDlexplicitly, it works only for networks with ReLUor linear activation functions, and
involves Meijer G-functions (Erdélyi et al., 1953), which are diﬃcult to handle numerically.
6For instance, a product of two Gaussian random variables is not Gaussian.
82.2 Results on realistic datasets
As far as we know, there does not exist any experimental result about the propagation
of the correlations with a non-synthetic dataset and a ﬁnite-width neural network. We
propose to visualize in Figure 1 the propagation of correlation cl
abwith dataset CIFAR-10
(results on MNIST are reported in Appendix H.1), in the case of the multilayer perceptron
with various numbers of neurons per layer nl(i.e., widths). Then, we show in Figure 2
the distance between the standardized distribution of the pre-activations and the standard
GaussianN(0,1).
Propagation of the correlations. First, we have sampled randomly 10data points in
each of the 10classes of the CIFAR-10 dataset, that is 100in total for each dataset. Then, for
each tested neural network (NN) architecture, we repeated ninit= 1000times the following
operation: (i) sample the parameters according to the EOC;7(ii) propagate the 100data
points in the NN. Thereafter, for each pair (xa,xb)of the selected 100data points, we have
computed the empirical correlation cl
abbetween the obtained pre-activations, averaged over
theninitsamples. Finally, we have averaged the results over the classes: the matrix Cl
pq
plotted in Figure 1 shows the mean of the correlation cl
abfor data points xaandxbbelonging
respectively to classes pandqin{0,···,9}.8Only the experiments with CIFAR-10 are
reported in Figure 1; the results on MNIST, which are similar, are reported in Figure 14 in
Appendix H.1.
InaccordancewiththetheoryoftheEOC,weobserveinFigure1thattheaveragecorrelation
between pre-activations tends to 1, except in the case φ= ReLU andnl= 10. In this case,
it is not even clear that the sequences of correlations (Cl
pq)lconverge at all, since some inter-
class correlations are lower at l= 30thanl= 10, while we expected them to grow until 1.
There is also a diﬀerence between activation functions tanhandReLU: the convergence to
1seems to be much quicker with φ= ReLU than withφ= tanh, whennl= 100.
We observe that the rate of convergence of (Cl
pq)ltowards 1not only varies with the NN
widthnlbut also varies in diﬀerent directions depending on the activation function. When
nlgrows from 10to100, the convergence of (Cl
pq)lto1slows down with φ= tanh, while
it accelerates with φ= ReLU . Since this striking inconsistency with the EOC theory is
related tonl, it is due to the “inﬁnite-width” approximation, precisely made to eliminate the
dependency on nlin the recurrence Equations (7) and (8), and consequently simplify them.
Remark 4. According to the framework of the EOC, the inputs xaandxbare assumed to
be ﬁxed. So, it is improper to deﬁne a correlation c0
abbetween xaandxb. However, when
considering the correlation c1
abof the pre-activations right after the ﬁrst layer, the empirical
7For the tanhactivation function, a study of the EOC can be found in Poole et al. (2016). For ReLU,
the EOC study is more subtle and can be found in Hayou et al. (2019).
8Correlations cl
abwitha=bhave been excluded from the computation to show the intra-class correlation
between diﬀerent samples.
9φ input l= 10 l= 30 l= 50nl= 10neurons per layerReLU
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
tanh
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8nl= 100neurons per layerReLU
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
tanh
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
−1.0−0.5 0.0 0.5 1.0
Correlation
Figure 1 – Propagation of correlations cl
abin a multilayer perceptron with activation function
φ∈{tanh,ReLU}and inputs sampled from the CIFAR-10 dataset. The NN is initialized at
the EOC. Each plot displays a 10×10matrixCl
pqwhose entries are the average correlation
between the pre-activations propagated by samples from classes p,q∈{0,···,9}, at the
input and right after layers l∈{10,30,50}. See Fig. 14, App. H.1, for results on MNIST.
10correlation between inputs xaandxbappears naturally:
c1
ab=σ2
w·xT
axb
n+σ2
b,
where ˆc0
ab:=xT
axb/nplays the role of an empirical correlation between xaandxb, assuming
that the empirical mean and variance of both xaandxbare respectively 0and1.9
Propagation of the distances to the Gaussian distribution. We test in Figure 2 the
Gaussian hypothesis in a multilayer perceptron of L= 100layers, with a constant width
nl∈{10,100,1000}, and an activation function φ∈{ReLU,tanh}. We propagate a single
point sampled from the CIFAR-10 dataset, and we compute the empirical distribution of the
pre-activations by drawing 10000samples of the parameters of the neural network.
In Figure 2, we plot the Kolmogorov–Smirnov statistic of the standardized pre-activations
for each layer, and we compare it to a threshold corresponding to a p-value of 0.05. Thus,
according to Figure 2, the Gaussian hypothesis is rejected with a p-value of 0.05for the
ReLUactivation function in all considered setups ( nl∈{10,100,1000}), and it is rejected
too for the tanhactivation function in the narrow network setup ( nl= 10).
WeprovideallthedetailsabouttheKolmogorov–SmirnovtestinAppendixG,andadditional
details and experiments in the multilayer perceptron setup in Section 4.2.
0 20 40 60 80 100
layerl0.000.020.040.060.080.10layer width nl= 10
0 20 40 60 80 100
layerl0.000.020.040.060.080.10layer width nl= 100
0 20 40 60 80 100
layerl0.000.020.040.060.080.10layer width nl= 1000
tanh
ReLUKS, p-val = 0 .05
Figure 2 – Evolution of the L∞-distance of the standardized empirical CDF of the pre-
activations and the standardized Gaussian N(0,1), for various layer widths and activation
functions. The empirical CDF of Dlhas been computed with 10000samples. The green
dotted line corresponds to the threshold given by the Kolmogorov–Smirnov test with a p-
value of 0.05: any point above it corresponds to a distribution for which the Gaussian
hypothesis should be rejected with a p-value of 0.05.
9Usually, this assumption does not hold exactly: it is common to normalize the entire dataset in such a
way that the whole set of the features of all training data points has empirical mean 0and variance 1, but
not each data point individually. See also Deﬁnitions 5 and 6.
112.3 Convergence of the sequence of variances (vl
a)l
Multiple stable limits. As reminded in Point 1, Section 2.1, it is a key assumption of the
EOC formalism to assume that, whatever the starting point v0
a, the sequence (vl
a)lconverges
to the same limit v∗asl→∞. As far as we know, no conﬁguration where the map V
has two nonzero stable points or more has been encountered in past works. Moreover, it is
believed that such conﬁgurations do not exist, as stated for example by Poole et al. (2016):
“for monotonic nonlinearities [ φ], this length map [ V] is a monotonically increasing, concave
function whose intersections with the unity line determine its ﬁxed points.”
In this subsection, we build a monotonic activation function for which the Vmap is not
concave and admits an inﬁnitenumber of stable ﬁxed points.
Deﬁnition 1 (Activation function ϕδ,ω).Forδ∈[0,1]andω>0two real numbers, deﬁne:
ϕδ,ω(x) :=xexp/parenleftBiggδ
ωsin(ωln|x|)/parenrightBigg
.
It is easy to prove that for all δ∈[0,1]andω>0:
1.ϕδ,ω(0) = 0, by continuity;
2.ϕδ,ωis odd;
3.ϕδ,ωis strictly increasing;
4. the map10v/mapsto→/integraltextϕδ,ω(√vz)2DzisC1and strictly increasing.
Deﬁnition 2 (Stable ﬁxed points) .Let(un)nbe a sequence deﬁned by recurrence:
un+1=f(un)withu0∈R.
For any starting point a, we denote by (un(a))nthe sequence deﬁned as above, with u0=a.
We say that u∗is astable ﬁxed point offiff(u∗) =u∗and if there exists an open ball
B(u∗,/epsilon1)centered in u∗of radius/epsilon1>0such that:
∀a∈B(u∗,/epsilon1), un(a)−→n→∞u∗.
Proposition 1. Iff(u∗) =u∗andfisC1in a neighborhood of u∗withf/prime(u∗)∈(−1,1),
thenu∗is a stable ﬁxed point of f.
Proposition 2. For anyδ∈(0,1]andω>0, let us use the activation function φ=ϕδ,ωof
Deﬁnition 1. We consider the sequence (vl)ldeﬁned by:
∀l≥0, vl+1=σ2
w/integraldisplay
ϕδ,ω/parenleftBig√
vlz/parenrightBig2Dz+σ2
b,withv0∈R+
∗. (11)
Then there exist σw>0,σb≥0, and a strictly increasing sequence of stable ﬁxed points
(v∗
k)k∈Zof the recurrence equation (11).
10NotationDzis deﬁned in Eqn. (9).
12The proof can be found in Appendix A.1.
Remark 5. In short, Proposition 2 ensures that there exists an inﬁnite number of possi-
ble (nonzero) limits for the sequence (vl
a)l, depending on v0
a. Consequently, the proposed
activation functions ϕδ,ωare counterexamples to the claim of Poole et al. (2016).
Plots. Figure 3 shows the shape of several activation functions ϕδ,ωfor various ω, along
with theirVmaps. We have chosen δ= 0.99<1to ensure that ϕδ,ωis a strictly increasing
function.11We have chosen σ2
b= 0andσ2
w=σ2
ω, computed as indicated in Appendix A.2.
In Figure 3a, the proposed activation functions exhibit reasonable properties: they are non-
linear, diﬀerentiable at each point (excluding 0), and remain dominated by a linear function.
However, we expect that as ωgrows,ϕδ,ωshould become closer and closer to the identity
function,12which is not desirable for the activation function of a NN.
In Figure 3b, it is clear that the function ϕδ,ωwithω= 6is a counterexample to the claim
of Poole et al. (2016): two nonzero stable points appear. So, in that case, depending on the
square norm v0
aof the input, the variance vl
aof the pre-activations may converge to diﬀerent
values. For instance, for ω= 6and an input with square norm v0
aaround the unstable point
atv≈2.3, it may converge either to v∗≈0.8orv∗≈6.5.
Also, we observe that for ω= 6, the variance map Vtends to be closer to the identity
function than for smaller ω. Thus, we expect the sequence (vl
a)lto converge at a slower rate
withω= 6than withω= 2.
In Figure 3d, all the conﬁgurations lie in the chaotic phase. Since all the correlation maps
Care below the identity function, the sequence of correlations (cl
ab)lalways tends to 0.
However, the plots are close to the identity, so (cl
ab)lvaries very slowly, and we expect that
the correlation between data points propagates into the NN with little deformation. Despite
being not perfect and lying in the chaotic phase, this conﬁguration roughly preserves the
input correlations between data points (without performing a pre-training phase), which is
a desirable property at initialization: information propagates with little deformation to the
output, and the error can be backpropagated to the ﬁrst layers.
2.4 Maintaining a property of the pre-activations during propagation
To conclude this section and introduce the next one, we propose a common representation
of the various methods used to build initialization distributions for the weights (Wl)land
biases (Bl)l.
Several initialization methods (Glorot & Bengio, 2010; He et al., 2015; Poole et al., 2016;
Schoenholz et al., 2017) are based on the same principle: initialization should be done in such
a way that some characteristic κlof the distribution of Zlis preserved during propagation
(e.g.,κl= Var( Zl)). Intuitively, any change between κlandκl+1reﬂects a loss of information
11We have chosen δclose to 1to obtain a function ϕδ,ωthat is strongly nonlinear, but a bit lower than 1
to ensure that ϕ/prime
δ,ωremains strictly positive, in order to prevent the training process from being stuck.
12Asω→∞,ϕδ,ωconverges pointwise to the identity function.
13−4−2 0 2 4−4−2024ω= 2
ω= 3
ω= 6(a) Activation function ϕδ,ω.
0 2 4 6 8 100246810
ω= 2
ω= 3
ω= 6 (b) Variance map V(·|σw,σb= 0), linear scale.
10−210−1100101102103 10−210−1100101102103
ω= 2
ω= 3
ω= 6
(c) Variance map V(·|σw,σb= 0), log-log scale.
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
ω= 2
ω= 3
ω= 6 (d) Correlation map C(·,v∗,v∗|σw,σb).
Figure 3 – Properties of the proposed counterexamples ϕδ,ωrepresented in Fig. 3a for ω∈
{2,3,6}andδ= 0.99.
In Fig. 3b, stable points are marked by crosses ( +), and unstable points by bullets ( •),
when they are away from 0: two stable points appear for ω= 6(in red). As established in
Proposition 2, σwis tuned for every ωin such a way that Vcrosses the identity function an
inﬁnite number of times (not visible on the ﬁgure).
In Fig. 3c (log-log scale), it is clearer that Vhas an inﬁnite number of ﬁxed points, due to
regular oscillations (in log-log scale) below and above the identity.
In Fig. 3d, we show that as ωgrows, the correlation map Cbecomes closer to the identity
function, which means that the correlation between data points tends to propagate perfectly.
Note: since an inﬁnite number of stable ﬁxed points are available, we have arbitrarily picked
one for each ω, denoted by v∗. This choice does not aﬀect the plot of the correlation map C,
due to the very speciﬁc structure of ϕδ,ω.
14between ZlandZl+1, which damages propagation or backpropagation. For instance, when
Var(Zl)→0, the network output tends to become deterministic, and when Var(Zl)→∞,
the output tends to forget the operations made by the ﬁrst layers (i.e., the gradients vanish
during backpropagation).
More generally, we denote by Dlthe distribution associated to the pre-activations Zl, by
T(·;nl,Pl,φl) =:Tl[Pl](·)the transformation of Dlperformed by layer l, that isDl+1=
Tl[Pl](Dl)(where Plis the initialization distribution of (Wl,Bl)andφlis the activation
function at layer l), and byκl:=χ(Dl)the characteristic of the distribution Dlwe are
interested in. Then, according to a heuristic of “information preservation”, it is assumed
that the sequence (κl)lmust remain constant, and the initialization distributions (Pl)lare
built accordingly. In some cases, it is possible to build a map ˜Tl[Pl], so that each κl+1can be
built out of its predecessor κl, without using all the information we may have on the (Dl)l.
We summarize this way of building initialization procedures in Figure 4, and we show how
it applies to well-known initialization procedures in Table 1.
Table 1 – Examples of Dl,φlandχin various setups. Notations: for any vector x∈Rn, its
empirical mean is ¯Ex=1
n/summationtextn
i=1xiand is empirical variance is Varx=1
n−1/summationtextn
i=1(xi−¯Ex)2.
Method Dlφlχ ˜Tl[Pl](κ)Assumption
Glorot & Bengio distr. of ZlId Var σ2
wκ2+σ2
b –
He et al. distr. of ZlReLU Var1
2σ2
wκ2+σ2
b –
Poole, Schoenholz distr. of (Zl
a,Zl
b)φ corrC∗(κ)Zl
j;a,Zl
j;bGaussian,
vl
a=vl
b=v∗
Ours distr. of Zlφθ IdTl[Pl](κ)¯Exa=¯Exb= 0,
Varxa=Varxb= 1
···DlDl+1···
··· κlκl+1···Tl−1[Pl−1]
χTl[Pl]
χTl+1[Pl+1]
˜Tl−1[Pl−1] ˜Tl[Pl] ˜Tl+1[Pl+1]
Figure 4 – Building process of the initialization distributions Plof the parameters (Wl,Bl):
(i) the pre-activations-related distribution Dlpasses through a map Tl[Pl]and becomesDl+1;
(ii) some statistical characteristic κlofDlcan be computed with a function χ:κl=χ(Dl);
(iii) we tune the (Pl)lin order to make the sequence (κl)lconstant.
Remark 6. We can use Figure 4 to build new initialization distributions: ﬁrst, we choose a
statistical property of Zl, which determines Dlandχ; then, we build a framework in which
15χ(Dl)can be easily computed for every l(e.g., we choose a speciﬁc activation function, or
we make simplifying assumptions).
In the following section, we aim to impose Gaussian pre-activations through a speciﬁc ac-
tivation function φθand initialization distribution Pθ. It implies that we would preserve
perfectly the distribution Dlitself: our characteristic is χ(Dl) =Dl. That way, all the
statistical properties of Dlare preserved during propagation.
3 Imposing Gaussian pre-activations
In this section, we propose a family of pairs (Pθ,φθ), where Pθis the distribution of the
weights at initialization, φθis the activation function, and θ∈(2,∞)is a parameter, such
that the pre-activations Zl
jareN(0,1)at any layer l. Imposing such pre-activations is a way
to meet two goals.
First, inSection2.2, wehave shownthattheGaussian hypothesisisnot fulﬁlled inthecase of
realistic datasets propagated into a simple multilayer perceptron, and we have recalled in the
Introduction that the tails of the pre-activations tend to become heavier when information
propagates in a neural network. By imposing Gaussian pre-activations, we ensure that the
Gaussian hypothesis is true, which reconciles the results provided in the EOC setup (see
Eqn. (7) and (8)) and the experiments.13
Second, as we recalled in the Introduction, many initialization procedures are based on the
preservation of some characteristic of the distribution of the pre-activations (see Table 1
and Figure 4). Usual characteristics are the variance and the correlation between data
points. By imposing Gaussian pre-activations, we would ensure that the whole distribution
is propagated, and not only one of its characteristics.
Besides, we provide a set of constraints, Constraints 1, 2, 3, and 4, that the activation
function and the initialization procedure should fulﬁll in order to maintain Gaussian pre-
activations at each layer.
Summary. Formally, we aim to ﬁnd a family of pairs (Pθ,φθ)such that:
Zl
j∼N(0,1)i.i.d.
Wl
ij∼Pθi.i.d.
Bl
i= 0

⇒Zl+1
i:=1√nlWl
i·φθ(Zl) +Bl∼N(0,1),
where Wl
i·is thei-th row of the matrix Wl. In other words, the pre-activations Zl+1
iremain
Gaussian for all l.
As a result of the present section, we make the following proposition for (Pθ,φθ):
13There exists another way to solve this problem: use propagation equations which would take into account
the sequence (nl)lof layer widths, that is, adopt a non-asymptotic setup, contrary to the process leading to
Eqn. (7) and (8). However, taking into account the whole sequence (nl)lwould lead to recurrence equations
that are far less easy to use than Eqn. (10). Moreover, a precise characterization of the distributions (Dl)l
of the pre-activations (Zl
1)lmay be very diﬃcult since they would not be Gaussian anymore.
16•Pθis the symmetric Weibull distribution W(θ,1), with CDF:
FW(t) =1
2+1
2sgn(t) exp/parenleftBig
−|t|θ/parenrightBig
; (12)
•φθis computed to ensure that Zl+1
i:=1√nlWl
i·φθ(Zl) +Blis GaussianN(0,1). In
short, thefamily (φθ)θspansarangeoffunctionsfroma tanh-likefunction(as θ→2+)
to the identity function (as θ→∞).
In order to obtain this result, we:
1. reduce and decompose the initial problem (Section 3.1);
2. ﬁnd constraints on the initialization distribution of the parameters to justify our
choice Pθ=W(θ,1)(Section 3.2);
3. compute the distribution Qθofφθ(Zl
j)we must choose to ensure Gaussian pre-
activations Zl+1
i, given an initialization distribution Pθ(Section 3.3);
4. buildφθfrom Qθ(Section 3.4).
3.1 Decomposing the problem
By combining Equations (1) and (2), the operation performed by each layer is:
Zl+1=1√nlWlφ(Zl) +Bl. (13)
In this subsection, we show that ﬁnding the distribution of the weights Wland the activation
functionφin order to have:
∀l∈[1,L],∀j∈[1,nl], Zl
j∼N(0,1),
can be done if we manage to get:
Z:=Wφ(X)∼N(0,1),withX∼N(0,1),
by tuning the distribution of Wand the activation function φ.
Zl+1
ias a sum of Gaussian random variables. First, we focus on the operation made
by one layer: if each layer transforms Gaussian inputs Zl
jinto pre-activations Zl+1
ithat are
Gaussian too, then we can ensure that the pre-activations remain Gaussian after each layer.
Thus, it is suﬃcient to solve the problem for one layer. After renaming the variables as
Z←Zl+1
i,Wj←Wl
ij,Xj←Zl
j,B←Bl
i,n←nl, we have:
Z=1√nn/summationdisplay
j=1Wjφ(Xj) +B. (14)
17In the rest of this subsection, we assume that the N(0,1)Xjare independent. We discuss
the independence hypothesis in Remark 7 and Appendix B. We want to build an activation
functionφ, and distributions for (Wj)jandBsuch thatZ∼N(0,1).
Second, we narrow our search space. According to Equation (14), Zis the sum of a random
variableBand a number nof i.i.d. random variables Wjφ(Xj)/√n. SinceZmust beN(0,1)
whatever the value of n, it is both convenient and suﬃcient to check that each summand in
the right-hand side of Equation (14) is Gaussian, that is:
B∼N(0,σ2
b), Wjφ(Xj)∼N(0,1−σ2
b),
withσb∈(0,1). Inthatcase,wehave Z∼N(0,1). Forthesakeofsimplicity,weassumethat
B= 0with probability 1, so that we just have to ensure that, for all j,Wjφ(Xj)∼N(0,1).
If one wants to deal with nonzero bias B∼N (0,σ2
b), it is suﬃcient to scale the random
variablesWjφ(Xj)accordingly.
To summarize, we have chosen to build Z∼N (0,1)by ensuring that Wjφ(Xj)∼N (0,1).
WithB= 0, this choice is formally imposed by this straightforward proposition.
Proposition 3. Let(Zj)jbe a sequence of ni.i.d. random variables. Let Z=1√n/summationtextn
j=1Zj.
IfZisN(0,1), then the distribution of each Zjis alsoN(0,1).
Proof.LetψZ(x) :=E[eiZx]be the characteristic function of the distribution of Z. Besides,
the(Zj)jare i.i.d. and Z=1√n/summationtextn
j=1Zj, so:
ψZ(x) = exp/parenleftBigg
−x2
2/parenrightBigg
andψZ(x) =/bracketleftbigg
ψZ1√n(x)/bracketrightbiggn
.
This proves that ψZ1(x) =e−x2/2. So, for all jin[1,n],Zj∼N(0,1).
As a result, we obtain the ﬁrst constraint.
Constraint 1. If, for alll, the weights (Wl
ij)ijare i.i.d. and independent from the pre-
activations (Xl
j)j, which are also supposed to be i.i.d., then we must ensure that:
∀l,i,j, Wl
ijφ(Xl
j)∼N(0,1).
Remark 7. The hypothesis of independent inputs (Xl
j)jtruly holds only for the second
layer.14But, overall, the hypothesis of independent (Xl
j)jis unrealistic. So, we propose
in Appendix B an empirical study of this hypothesis, in order to identify in which cases
the dependence between the inputs of one layer damages the Gaussianity of its outputted
pre-activations.
14The inputs of the ﬁrst layer are deterministic.
18New formulation of the problem. We have proven that, to ensure that Z∼N (0,1),
it is suﬃcient to solve the following problem:
ﬁndPandφsuch that: X∼N(0,1)andW∼P⇒Wφ(X)∼N(0,1).(15)
In the following subsections, we build a family Pof initialization distributions (Section 3.2)
such that, for any P∈P, there exists a function φsuch that (P,φ)is a solution to (15). We
decompose the remaining problem into two parts, by introducing an intermediary random
variableY=φ(X):
•for a distribution P, deduce Qs.t.:W∼P,Y∼Q⇒WY =:G∼N (0,1)
(Section 3.3);
•for a distribution Q, ﬁnd a function φs.t.:X∼ N (0,1)⇒Y=φ(X)∼Q
(Section 3.4).
3.2 Why initializing the weights Waccording to a symmetric Weibull distribution?
We are looking for a family Pof distributions such that, for any P∈P, there exists Qsuch
that:
W∼P,Y∼Q⇒WY =:G∼N(0,1).
Therefore, the family Pis subject to several constraints. In this subsection, we present two
results indicating that a subset of the family of Weibull distributions is a good choice for P:
1. the density of Wat0should be 0;
2.Wshould be a generalized Weibull-tail random variable (see Section 3.2.2 or
Vladimirova et al. (2021)) with parameter θ∈(2,∞).
In the process, we are able to gather information about the distribution of |Y|, namely its
density at 0and the leading power of the logof its survival function at inﬁnity, respectively:
f|Y|(0) =/radicalBigg
2
π/bracketleftBigg/integraldisplay∞
0f|W|(t)
tdt/bracketrightBigg−1
,
logS|Y|(y)∝−y1/(1
2−1
θ).
Asaconclusionofthissubsection, weconsiderthatthedistribution P = PθofWshouldliein
the following subset of the family of symmetric Weibull distributions (deﬁned at Eqn. (12)):
P:={W(θ,1) :θ∈Θ},
Θ := (2,∞).
193.2.1 Behavior near 0
Since the product G=WYis meant to be distributed according to N(0,1), then we must
havef|G|(0) =/radicalBig
2
π∈(0,∞), which is impossible for several choices of distributions for W.
Proposition4 (Densityofaproductofrandomvariablesat 0).LetW,Ybe two independent
non-negative random variables and Z=WY. LetfW,fY,fZbe their respective densities.
Assuming that fYis continuous at 0withfY(0)>0, we have:
if lim
w→0/integraldisplay∞
wfW(t)
tdt=∞,then lim
z→0fZ(z) =∞.
Moreover, if fYis bounded:
if/integraldisplay∞
0fW(t)
tdt<∞,thenfZ(0) =fY(0)/integraldisplay∞
0fW(t)
tdt. (16)
The proof can be found in Appendix C.
Corollary 1. IffYandfWare continuous at 0withfY(0)>0andfW(0)>0, then:
lim
z→0fZ(z) =∞.
According to Corollary 1, it is impossible to obtain a Gaussian Gby multiplying two random
variablesWandYwhose densities are both continuous and nonzero at 0. So, if we want to
manipulate continuous densities, we must have either fW(0) = 0orfY(0) = 0.
Let us assume that fY(0) = 0. We want Yto be the image of X∼N (0,1)through the
functionφ, wherefX(0)>0. So, in order to obtain Ywith a zero density at 0, it is necessary
to build a function φwithφ/prime(0) =∞(see Lemma 2 in Appendix D), which is usually not
desirable for an activation function of a neural network for training stability reasons.15So,
it is preferable to design Wsuch thatfW(0) = 0.
Constraint 2. To avoid activation functions with a vertical tangent at 0, the density of
the initialization distribution of a weight Wl
ijmust be 0at0:
∀l,i,j, fWl
ij(0) = 0.
Remark 8. In the common case of neural networks with activation function φ= tanhand
weightsWinitialized according to a Gaussian distribution, if we assume that the Gaussian
hypothesis is true, then fY(0)>0andfZ(0)>0. Thus, Corollary 1 applies and the density
ofZis inﬁnite at 0.
15Ifφ/prime(0) =∞andφisC1onR∗, then numerical instabilities may occur during training: if a pre-activation
Zl
japproaches 0too closely, φ/prime(Zl
j)can explode and damage the training. These instabilities can be handled
by gradient clipping (Pascanu et al., 2013).
20Ifφ= Id,Zis the product of two independent N(0,1), whose density is well-known:
fZ(z) =K0(|z|)
π,
whereK0is the modiﬁed Bessel function of the second kind, which tends to inﬁnity at 0,
which illustrates Corollary 1.16
Finally, if Constraint 2 holds and we want f|G|(0) =/radicalBig
2
π, then, according to Equation (16),
the following constraint must hold.
Constraint 3. The density of Yat0must have a speciﬁc value depending on the dis-
tribution of W:
f|Y|(0) =/radicalBigg
2
π/bracketleftBigg/integraldisplay∞
0f|W|(t)
tdt/bracketrightBigg−1
.
3.2.2 Behavior of the tail
We use the results of Vladimirova et al. (2021) on the “generalized Weibull-tail distributions”
and start by recalling useful deﬁnitions and properties.
Deﬁnition 3 (Slowly varying function) .A measurable function f: (0,∞)→(0,∞)is said
to beslowly varying if:
∀a>0,limx→∞f(ax)
f(x)= 1.
Deﬁnition4 (GeneralizedWeibull-Tail( GWT)distribution) .A random variable Xis called
generalized Weibull-tail with parameter θ > 0, or GWT(θ), if its survival function SXis
bounded in the following way:
∀x>0,exp/parenleftBig
−xθf1(x)/parenrightBig
≤SX(x)≤exp/parenleftBig
−xθf2(x)/parenrightBig
,
wheref1andf2are slowly-varying functions and θ>0.
Proposition 5 (Vladimirova et al., 2021, Thm. 2.2) .The product of two independent non-
negative random variables |W|and|Y|which are respectively GWT(θW)andGWT(θY)is
GWT(θ), withθsuch that:
1
θ=1
θW+1
θY.
16Though, even if each Wjφ(Xj)has an inﬁnite density at 0, the density at 0of the weighted sum
Z=1√n/summationtextn
j=1Wjφ(Xj) +Bmay be ﬁnite. For instance, it occurs when all the Wjandφ(Xj)are i.i.d. and
Gaussian. But in this case, even if fZ(0)<∞, it is impossible to recover a Gaussian pre-activation (see
Prop. 3).
21Werecallthat, inourcase, |G|=|W|·|Y|istheabsolutevalueofaGaussianrandomvariable.
So|G|isGWT(2) . Thus, if we assume that |W|and|Y|are respectively GWT(θW)and
GWT(θY), then we have:
1
2=1
θW+1
θY.
Therefore we have the following constraint.
Constraint 4. The weights WareGWT(θ)withθ∈Θ = (2,∞).
3.2.3 Conclusion
Constraints 2 and 4 indicate that the distribution Pof the weights W:
(i) should have a density fWsuch thatfW(0) = 0;
(ii) should be GWT(θ)withθ∈(2,∞).
A simple choice for Pmatching these two conditions is: P = Pθ=W(θ,1)withθ∈(2,∞),
whereW(θ,1)is the symmetric Weibull distribution, deﬁned in Equation (12). Thus, we
ensure that fW(0) = 0andWis generalized Weibull-tail with a parameter θeasy to control
(see remark below).
Remark 9. IfW∼W (θ,1), thenWisGWT(θ).
3.3 Obtaining the distribution of the activations Y
Now that the distribution PofWis supposed to be symmetric Weibull, that is, P = Pθ=
W(θ,1), we are able to look for an odd17activation function φθsuch that:
W∼W (θ,1),X∼N(0,1)⇒Wφθ(X)∼N(0,1). (17)
As a ﬁrst step, we look for a distribution Qθsuch that:
W∼Pθ,Y∼Qθ⇒WY =:G∼N(0,1).
In order to “invert” this equation, it is natural to make use of the Mellin transform. A
comprehensive and historical work about Fourier and Mellin transforms can be found in
Titchmarsh(1937), andasimpleapplicationtothecomputationofthedensityoftheproduct
of two random variables can be found in Epstein (1948).
However, the technique involving the Mellin transform is very diﬃcult to use in this case,
both analytically and numerically. Details about the Mellin transform and these diﬃculties
can be found in Appendix E.
17See Remark 10 for a discussion about the parity of φθ.
22Computation of f|Y|: hand-designed parameterized function. Thus, inspired by the
shape off|Y|computed via the numerical inverse Mellin transform (see Fig. 13, App. E.2),
we build an approximation of f|Y|from the family of functions {gα,γ,λ 1,λ2:α,γ,λ 1,λ2>0}
with:
gΛ(x) :=gα,γ,λ 1,λ2(x) :=γαxα−1
λα
1exp/parenleftBigg
−xα
λα
1/parenrightBigg
+/radicalBigg
2
π1
Γ/parenleftBig
1−1
θ/parenrightBigexp/parenleftBigg
−xθ/prime
λθ/prime
2/parenrightBigg
,
whereθ/primeis the conjugate of θ:1
θ+1
θ/prime=1
2, and Λ := (α,γ,λ 1,λ2). It is clear that, whatever
the parameters, gΛ(0) =/radicalBig
2
π/bracketleftBig
Γ/parenleftBig
1−1
θ/parenrightBig/bracketrightBig−1, which is exactly Constraint 3. Moreover, when
α= 0,gmatches also Constraint 4.
Then, we optimize the vector of parameters Λwith respect to the following loss:
/lscript(Λ) :=/bardblˆFΛ−F|G|/bardbl∞
ˆFΛ(z) :=/integraldisplay∞
0F|W|/parenleftbiggz
t/parenrightbigg
gΛ(t) dt(see Eqn. (24)) ,
where ˆFΛis meant to approximate the CDF of the absolute value of a Gaussian N(0,1). The
integral is computed numerically. For the loss, we have chosen to compute the L∞-distance
between two CDFs, in order to be consistent with the Kolmogorov–Smirnov test we perform
in Section 4.1. Optimization details can be found in Appendix F.
3.4 Obtaining the activation function φθ
In the preceding section, we have computed the distribution of |Y|. We are restricting
ourselves to symmetrical Y, whose distribution is denoted by Qθ. Now, we want to build the
activation function φθ, in order to transform a pre-activation G∼N(0,1)into an activation
Y=φθ(G)distributed according to Qθ:
ifG∼N(0,1),thenφθ(G)∼Qθ.
To compute φθ, we will use the following proposition:
Proposition 6. LetXbe a random variable such that FXis strictly increasing. Let Qbe a
distribution with a strictly increasing CDF FQ. Then there exists a function φsuch that:
φ(x) :=F−1
Q(FX(x))andφ(X)∼Q.
Proof.We want to ﬁnd φsuch thatFQ(y) =P(φ(X)≤y).
Letφ(x) :=F−1
Q(FX(x)), which is a strictly increasing bijection from RtoR. We have:
P(φ(X)≤y) =P(X≤φ−1(y)) =FX(φ−1(y)) =FX(F−1
X(FQ(y))) =FQ(y).
23SincefGandfYare strictly positive, FGandFYare strictly increasing and we can use
Proposition 6:
φθ(t) :=F−1
Y(FG(t)), FY(t) :=1
2+1
2sgn(t)/integraldisplay|t|
0f|Y|(y) dy. (18)
Remark 10. The resulting activation functions φθare odd, because we have chosen a sym-
metricalY. Actually, there are several non-symmetrical Yfor whichf|Y|matches the con-
ditions imposed in the preceding sections, and that lead to derivable activation functions.
However, in the family of all possible distributions for Y, we identify only two “natural”
usable solutions: Yis symmetric or Yis non-negative. We have chosen the ﬁrst solution.
3.5 Results and limiting cases
We have plotted in Figure 5 the diﬀerent distributions related to the computation of φθ
and the functions φθthemselves. The family of the φθis a continuum spanning unbounded
functions from the tanh-like function φ2+and the identity function.
Our construction degenerates into the following two extreme cases at the boundaries of the
parameter space Θ = (2,∞):
•whenθ→∞, we have Pθd−→Randφθ→Idpointwise;
•whenθ→2, we have Pθd−→W (2,1)andφθ→φ2+pointwise;
whereRis the Rademacher distribution ( ξ∼R⇔ P(ξ=±1) =1
2),Idis the identity
function, and φ2+is a speciﬁc increasing function with: lim±∞φ2+=±1andφ/prime
2+(0) =√π.
In the limiting case θ→∞, we initialize the weights at ±1, which corresponds to binary
“weight quantization”, used in the ﬁeld of neural networks compression (Pouransari et al.,
2020), and we use a linear activation function, commonly used in theoretical analyses of
neural networks (Arora et al., 2019). In the limiting case θ→2, we recover weights with
Gaussian tails, with a tanh-like activation function.
4 Experiments
Inthissection, wetesttheGaussianhypothesiswith ReLU,tanhandouractivationfunctions
φθ, after one layer (Section 4.1) and after several layers (Section 4.2). Then, we plot the Edge
of Chaos graphs (σb,σw), which are exact with φ=φθ(Section 4.3) in the case of ﬁnite nl
with independent pre-activations. Finally, in Section 4.4, we show the training trajectories
of LeNet-type networks and multilayer perceptrons, when using tanh,ReLU, and various
activation functions we have proposed.
In the following subsections, when we use φ= tanh orReLU, we initialize the weights
according to a Gaussian at the EOC. This is, for tanh:σ2
b= 0.013,σ2
w= 1.46; and for
ReLU:σ2
b= 0,σ2
w= 2. When we use φ=φθ, we initialize the weights according to W(θ,1).
24−3−2−1 0 1 2 301234
θ= 2.05
θ= 2.50
θ= 3.00
θ= 4.00
θ= 5.00
θ= 7.00
θ= 10.00(a) Initialization distribution Pθof the
weights: symmetric Weibull W(θ,1).
−3−2−1 0 1 2 30.00.20.40.60.81.0(b) Estimated density fYof the distribution
QθofY.
−10.0−7.5−5.0−2.5 0.0 2.5 5.0 7.5 10.0−10.0−7.5−5.0−2.50.02.55.07.510.0
(c) Activation function φθ.
−3−2−1 0 1 2 3−3−2−10123 (d) Activation function φθaround zero.
Figure 5 – How to build a random variable WY =Wφθ(X) =:G∼N (0,1), whereX∼
N(0,1)? (a) Choose the distribution PθofW, then (b) deduce the distribution QθofY,
and ﬁnally (c-d) ﬁnd φθ.
Notation for the activation functions. We recall that: φdenotes an arbitrary activa-
tion function; ϕδ,ωdenotes the function deﬁned in Deﬁnition 1; φθdenotes the odd function
verifying Eqn. (17).
4.1 Testing the Gaussian hypothesis: synthetic data, one layer
Above all, we have to check experimentally that we are able to produce Gaussian pre-
activations with our family of initialization distributions and activation functions {(Pθ,φθ) :
θ∈(2,∞)}.
Framework. First, we test our setup in the one-layer neural network case with synthetic
inputs. More formally, we consider a N(0,1)pre-input18Z∈Rn, which is meant to be ﬁrst
transformed by the activation function φθ(hence the name “ pre-input”), then multiplied by
18Such a pre-input plays the role of the pre-activation outputted by a hypothetical preceding layer.
25a matrix of weights W∈R1×n. This one-neuron layer outputs a scalar Z/prime:
Z/prime:=1√nWφ(Z),withZj∼N(0,1)andW1j∼Pθfor allj.
We want to check that the distribution P/primeofZ/primeis equal toN(0,1).
Experimental results. For that, we use of the Kolmogorov–Smirnov (KS) test (Kolmogo-
roﬀ, 1941; Smirnov, 1948) (see Appendix G). We perform the KS test within two setups:
with and without preliminary standardization of the sets of samples. With a preliminary
standardization, we perform the test on (¯Z/prime
1,···,¯Z/prime
s):
¯Z/prime
k=Z/prime
k−¯µ
¯σ,¯µ=1
ss/summationdisplay
k=1Z/prime
k,¯σ2=1
s−1s/summationdisplay
k=1(Z/prime
k−¯µ)2.
For the sake of simplicity, let us denote by ˆFZ/prime:=Fsthe empirical CDF of Z/prime, computed
with thesdata samples (Z/prime
1,···,Z/prime
s), and let ˆF¯Z/primebe the empirical CDF of standardized Z/prime,
computed with (¯Z/prime
1,···,¯Z/prime
s).
We have plotted in Figure 6 the KS statistic of the distribution of the output Z/prime, when using
our activation functions φθ,tanhandReLU. Our sample size is s= 107. A small KS statistic
corresponds to a conﬁguration where Z/primeis close to beingN(0,1). If a point is above the KS
threshold (green line, dotted), then the Gaussian hypothesis is rejected with p-value 0.05.
When we perform standardization (Fig. 6b), the neurons using φθoutput always a pre-
activationZ/primethat is closer toN(0,1)than with ReLUortanh. But, despite this advantage,
the Gaussianhypothesis shouldbe rejectedwith φθwhen the neuron hasa very small number
of inputs (n<30).
In Figure 6a, we compare directly the distribution of Z/primetoN(0,1). This test is harder than
testing the Gaussian hypothesis because the variance of Z/primemust be equal to 1. We observe
that when using φθ, the KS statistic remains above the threshold (while it is still below 10−2,
and even below 6·10−3forn≥3). This result is due to the fact that our computation of φθ
is only approximate (see Section 3.4).
Remark 11. Our sample size ( s= 107) is very large, which lowers the threshold of rejection
of the Gaussian hypothesis. We have chosen this large sto reduce the noise of the KS
statistics. If we had chosen s= 18000, a threshold close to 10−2would have resulted, which
is higher than any of the KS statistics computed with φ=φθ. One will also note that, in
Section 4.2, we use only s= 10000 samples to keep a reasonable computational cost.
Remark 12. Since tanhandReLUhave not been designed such that Z/prime∼N (0,1), we did
not plot the related non-standardized KS statistics in Figure 6a. Anyway, given the standard
deviation of Z/primewhen using φ= tanhorReLU(see Table 2, Appendix H.3), very large KS
statistics are expected in this setup.
We discuss the limits of the KS test in Appendix G.
26100101102
Number of input weights10−410−310−210−1100KS-statisticφθ,θ= 2.05
φθ,θ= 2.50
φθ,θ= 3.00
φθ,θ= 4.00
φθ,θ= 5.00φθ,θ= 7.00
φθ,θ= 10.00
tanh
ReLU
KS, p-val = 0 .05(a) KS-statistic of ˆFZ/prime.
100101102
Number of input weights10−410−310−210−1100KS-statistic (b) KS-statistic of ˆF¯Z/prime(standardized samples).
Figure6–EvolutionoftheKSstatisticofthedistributionof Z/prime(Fig.6a)andthestandardized
distribution of Z/prime(Fig. 6b), with a number of inputs n∈{1,3,10,30,100}.
4.2 Testing the Gaussian hypothesis: CIFAR-10, multilayer perceptron
Now, we test our setup on a multilayer perceptron with CIFAR-10, which is more realistic.
We show in Figure 7 how the distribution of the pre-activations propagates in a multilayer
perceptron, for diﬀerent layer widths nl∈{10,100,1000}.
Setup. LetDlbe the distribution of the pre-activation Zl
1after layer l. For alll, let us
deﬁne (Zl
1;k)k∈[1,s], a sequence of i.i.d. samples drawn from Dl. The plots in Figure 7 show
the evolution of the L∞distance/bardblˆFZl
1−FG/bardbl∞between the CDF of N(0,1),FG, and the
empirical CDF of Dl,ˆFZl
1, built with s= 10000 samples (Zl
1;k)k∈[1,s].
We have built the plots of Figure 7 with the same input data point.19See Appendix H.4 for
a comparison of the propagation between diﬀerent data points.
In Figures 7a and 7c, the propagated data point has been normalized according to the whole
training dataset, that is:
Deﬁnition 5 (Input normalization over the whole dataset) .We build the normalized data
point ˆx:
ˆxa;ij:=xa;ij−µi
σi, µi:=1
pid/summationdisplay
x∈Dpi/summationdisplay
j=1xij, σ2
i:=1
pid−1/summationdisplay
x∈Dpi/summationdisplay
j=1(xij−µi)2,
wherexa;ijis thej-th component of the i-th channel of the input image xa,piis the size of
thei-th channel, and dis the size of the dataset D.
19In the PyTorch implementation of the training set of CIFAR-10: data point #47981 (class = plane).
This data point has been chosen randomly.
27In Figure 7b, the propagated data point has been normalized individually, that is:
Deﬁnition 6 (Individual input normalization) .We build the normalized data point ˆx:
ˆxa;i:=xa;i−µ
σ, µ :=1
pp/summationdisplay
i=1xi, σ2:=1
p−1p/summationdisplay
i=1(xi−µ)2,
where ˆxis the normalized data point, xa;iis thei-th component of xa∈Rp.
Results. We distinguish two measures of the distance between the distribution of Zl
1and
a Gaussian: in Figures 7a and 7b, we measure the distance between the distribution of Zl
1
andN(0,1); in Figure 7c, we measure the distance between the standardized distribution of
¯Zl
1andN(0,1). In short, we test in Figure 7c the Gaussian hypothesis, whatever the mean
and the variance of the pre-activations.
First, in all cases, ReLUleads to pre-activations that diverge from the Gaussian family. Also,
with 10neurons per layer, tanhdoes not lead to Gaussian pre-activations.
Second, our proposition of activation functions φ=φθleads to various results, depending
on the layer width nl. Above all, with individual input normalization (Fig. 7b), the ﬁrst
pre-activations are very close to N(0,1), which is what we intended. For θ= 2.05, the curve
remains below the KS threshold with p-value 0.05, whatever the layer width nl. However,
asθgrows, the related curves tend to drift away from 0, especially when nlis small.
However, when standardizing the pre-activations (Figure 7c), three of our activation func-
tions (θ∈{2.05,2.5,3}) remain below the KS threshold in the least favorable case ( nl= 10),
which is not the case for ReLUandtanh.
Conclusion. For all tested widths, combining an activation function φθwithθclose to 2
and weights sampled from W(θ,1)leads to pre-activations that are closer to N(0,1)than
withφ= tanhorReLU. However, our proposition is not perfect for all θ >2: we always
observe that the sequence (Dl)ldrifts away fromN(0,1). But, overall, this drift is moderate
with layer widths nl≥100, and even disappears when we standardize the pre-activations.
So, in order to keep Gaussian pre-activations along the entire neural network, one should
take into account this “drift”. We can interpret it as a divergence due to the fact that
Dl=N(0,1)is not a stable ﬁxed point of the recurrence relation Dl+1=Tl[Pl](Dl)(see
Fig. 4 in Section 2.4). So, it is natural that (Dl)ldrifts away fromN(0,1), and converges to
astableﬁxed point, which seems to be N(µ,σ2)with parameters µandσ2to determine, at
least fornl= 1000.
This search for stable ﬁxed points in the recurrence relation Dl+1=Tl[Pl](Dl)is closely
related to the discovery of stable ﬁxed points for the sequence of variances (vl
a)land the
sequence of correlations (cl
ab)l(Poole et al., 2016; Schoenholz et al., 2017), and may be
explored further in future works.20
20The ﬁxed points of Dl+1=Tl[Pl](Dl)can also be seen as stationary distributions of the Markov chain
(Zl
1)l, if the layer width nland the initialization distribution Plare constant.
280 20 40 60 80 100
layerl0.00.10.20.30.40.5layer width nl= 10
0 20 40 60 80 100
layerl0.00.10.20.30.40.5layer width nl= 100
0 20 40 60 80 100
layerl0.00.10.20.30.40.5layer width nl= 1000
φθ,θ= 2.05
φθ,θ= 2.50
φθ,θ= 3.00
φθ,θ= 4.00
φθ,θ= 5.00φθ,θ= 7.00
φθ,θ= 10.00
tanh
ReLU
KS, p-val = 0 .05(a) Distance/bardblˆFZl
1−FG/bardbl∞; inputs normalized over the dataset (Deﬁnition 5).
0 20 40 60 80 100
layerl0.00.10.20.30.40.5layer width nl= 10
0 20 40 60 80 100
layerl0.00.10.20.30.40.5layer width nl= 100
0 20 40 60 80 100
layerl0.00.10.20.30.40.5layer width nl= 1000
(b) Distance/bardblˆFZl
1−FG/bardbl∞; inputs normalized individually (Deﬁnition 6).
0 20 40 60 80 100
layerl0.000.020.040.060.080.10layer width nl= 10
0 20 40 60 80 100
layerl0.000.020.040.060.080.10layer width nl= 100
0 20 40 60 80 100
layerl0.000.020.040.060.080.10layer width nl= 1000
(c) Distance/bardblˆF¯Zl
1−FG/bardbl∞, where ¯Zl
1=Zl
1−¯µl
¯σlis the standardized version of Zl
1; inputs normalized
over the dataset (Deﬁnition 5). This ﬁgure has been zoomed around 0.
Figure7–Evolutionofthedistanceof DltoaGaussianaccordingtodiﬀerentmetrics, during
propagation where lvaries from 1to100.¯Zl
1is the standardized version of Zl
1;¯µland¯σlare
respectively the empirical mean and the empirical (corrected) standard deviation of Zl
1. The
dotted green line is the KS threshold: any point above it corresponds to a distribution Dlfor
which the Gaussian hypothesis should be rejected with p-value 0.05. Weight initialization is
W(θ,1)when using φ=φθand is Gaussian according to the EOC when using φ= tanhor
ReLU.
294.3 Non-asymptotic Edge of Chaos
In Figure 8, we show the Edge of Chaos graphs for several activation functions: tanhand
ReLUon one side, and our family (φθ)θon the other side. We remind that each graph
corresponds to a family of initialization standard deviations (σw,σb)such that the sequence
of correlations (cl
ab)lconverges to 1at asub-exponential rate (see Section 2.1, Point 2). Such
choices ensure that the initial correlation between two inputs changes slowly so that the
information contained in these inputs is lost at the slowest possible rate.
Instead of assuming that the pre-activations are Gaussian as an eﬀect of the “inﬁnite-width
limit” and the Central Limit Theorem, we claim that, with our activation functions φθ, for
any layer widths (including narrow layers and networks with various layer widths), the Edge
of Chaos is non-asymptotic . Therefore, the corresponding curves in Figure 8 hold for realistic
networks.
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75
σw0.00.51.01.52.02.53.03.5σbφθ,θ= 2.05
φθ,θ= 2.50
φθ,θ= 3.00
φθ,θ= 4.00
φθ,θ= 5.00
φθ,θ= 7.00
φθ,θ= 10.00
tanh
ReLU
Figure 8 – Edge of Chaos for several activation functions. The ordered phase (resp. chaotic)
lies above (resp. below) each EOC curve.
Remark 13. For the ReLUactivation function, the EOC graph reduces to one point. One
can refer to Hayou et al. (2019, Section 3.1) for a complete study of the EOC of “ ReLU-like
functions”, that is, functions that are linear on R+and on R−with possibly diﬀerent factors.
Also, as in Figure 1 (see Section 2.2), we have plotted the propagation of the correlations
(Cl
pq)lwithφ=φθand weights sampled from W(θ,1)in Appendix H.2.
304.4 Training experiments
Finally, wecompare theperformanceof atrainedneural network when using tanhandReLU,
and our activation functions. Despite the EOC framework (and ours) do not provide any
quantitative prediction about the training trajectories, it is necessary to analyze them in
order to enrich the theory.
This section starts with a basic check of the training and test performances on a common
task: training LeNet on CIFAR-10. Then, we challenge our activation function, along with
tanhandReLU, by training on MNIST a diverse set of multilayer perceptrons, some of them
being extreme (narrow and deep).
In the following, we train all the neural networks with the same optimizer, Adam, and
the same learning rate η= 0.001. We use a scheduler and an early stopping mechanism,
respectively based on the training loss and the validation loss, the test loss not being used
during training. We did not use data augmentation. All the technical details are provided
in Appendix H.5.
LeNet-type networks. We consider LeNet-type networks (LeCun et al., 1998). They are
made of two (5×5)-convolutional layers, each of them followed by a 2-stride average pooling,
and then three fully-connected layers. We denote by “ 6−16−120−84−10” a LeNet neural
network with two convolutional layers outputting respectively 6and16channels, and three
fully connected layers having respectively 120,84, and 10outputs (the ﬁnal output of size
10is the output of the network).
We have tested LeNet with several sizes (see Figure 9). In Figure 9a, we have plotted the
training loss. Overall, ReLUandtanhperform well, along with some φθwith small θand
ϕδ,ω, with (δ,ω) = (0.99,2). In Figure 9b, the results in terms of test accuracy are quite
diﬀerent: ReLUandtanhstill achieve good accuracy, but the other functions achieving
similar results on the training loss seem to be a bit behind.
So, in this standard setup, the functions we are proposing seem to make the neural network
trainable and as expressive as with other activation functions, but with some overﬁtting.
This is not surprising, since we are testing long-standing activation functions, ReLUand
tanh, which have been selected both for their ability to make the neural network converge
quickly with good generalization, against functions we have designed only according to their
ability to propagate information. Therefore, according to these plots, taking into account
generalization may be the missing piece of our study.
Multilayer perceptron. We have trained a family of multilayer perceptrons on MNIST.
They have a constant width nl∈{3,10}and a depth L∈{3,10,30}. So, extreme cases such
as a narrow and deep neural network ( nl= 3,L= 30) have been tested.
Two series of results are presented in Figure 10. The setups of Figure 10a and Figure 10b
are identical, except for the initial random seed. In terms of training, the strength of our
activation functions φθis more visible in the case of narrow neural networks ( nl= 3): in
310 50 100 150 2000.00.51.01.52.02.5LeNet: 3-8-60-42-10
0 50 100 150 200 250 3000.00.51.01.52.02.5LeNet: 6-16-120-84-10
0 50 100 1500.00.51.01.52.02.5LeNet: 12-32-240-168-10(a) Training loss (negative log-likelihood).
0 50 100 150 2000.20.30.40.50.60.7LeNet: 3-8-60-42-10
0 50 100 150 200 250 3000.20.30.40.50.60.7LeNet: 6-16-120-84-10
tanh
ReLU
ϕ0.99,ω,ω= 2.00
ϕ0.99,ω,ω= 3.00
ϕ0.99,ω,ω= 6.00
0 50 100 1500.20.30.40.50.60.7LeNet: 12-32-240-168-10
φθ,θ= 2.05
φθ,θ= 2.50
φθ,θ= 3.00
φθ,θ= 4.00
φθ,θ= 5.00
φθ,θ= 7.00
φθ,θ= 10.00
(b) Test accuracy.
Figure 9 – Training curves for LeNet with 3 diﬀerent numbers of neurons per layer.
general, the loss decreases faster and attains better optima with φ=φθthan withφ= tanh
orReLU.
We also notice that training a narrow and deep neural network with a ReLUactivation
function is challenging. This result is consistent with several observations we have made in
Section 2.2: in narrow ReLUnetworks, the sequence of correlations (Cl
pq)lfails to converge
to1(Fig. 1), and the pre-activations are far from being Gaussian (Fig. 2).
Finally,φ2.05put aside, the results regarding the activation functions φθare consistent be-
tween the two runs. This is not the case with tanhandReLU.
5 Discussion
Generality of Constraints 1 to 4. Provided that we want pre-activations that are Gaus-
sianN(0,1), and we want the weights of each layer to be i.i.d. at initialization, the set of four
constraints that we provide in Section 2 must hold. If one wants to relax these constraints,
it becomes unavoidable to break symmetries or to solve harder problems. Let us consider
two examples.
320 100 200 3000.00.51.01.52.02.5L= 3 ;nl= 3
0 50 100 150 2000.00.51.01.52.02.5L= 10 ;nl= 3
0 100 200 3000.00.51.01.52.02.5L= 30 ;nl= 3
0 50 100 150 200 2500.00.51.01.52.02.5L= 3 ;nl= 10
φθ,θ= 2.05
φθ,θ= 2.50
φθ,θ= 3.00
φθ,θ= 4.00
φθ,θ= 5.00φθ,θ= 7.00
φθ,θ= 10.00
tanh
ReLU
0 50 100 150 200 2500.00.51.01.52.02.5L= 10 ;nl= 10
0 100 200 3000.00.51.01.52.02.5L= 30 ;nl= 10(a) Series of experiments #1.
0 50 100 150 200 2500.00.51.01.52.02.5L= 3 ;nl= 3
0 50 100 150 2000.00.51.01.52.02.5L= 10 ;nl= 3
0 100 200 3000.00.51.01.52.02.5L= 30 ;nl= 3
0 100 200 3000.00.51.01.52.02.5L= 3 ;nl= 10
φθ,θ= 2.05
φθ,θ= 2.50
φθ,θ= 3.00
φθ,θ= 4.00
φθ,θ= 5.00φθ,θ= 7.00
φθ,θ= 10.00
tanh
ReLU
0 100 2000.00.51.01.52.02.5L= 10 ;nl= 10
0 50 100 150 200 2500.00.51.01.52.02.5L= 30 ;nl= 10
(b) Series of experiments #2.
Figure 10 – Training loss for a multilayer perceptron, narrow ( nl∈{3,10}) and of various
depths (L∈{3,10,30}).
33Example1: insteadofmakingallthepre-activationsGaussian, onemaywanttoimposesome
other distribution. In this case, the problem to solve would be more diﬃcult: we would have
to decompose a non-Gaussian random variable Zinto a weighted sum1√n/summationtextn
j=1Wjφ(Xj). In
this case, we cannot use Proposition 3, and we would have to make Wjφ(Xj)belong to a
family of random variables stable by multiplication by a constant and by sum, for arbitrary
n. If we aim for a non-Gaussian Z, this task is much harder. For a study of inﬁnitely wide
neural networks going beyond Gaussian pre-activations, see Peluchetti et al. (2020).
Example2: insteadofassumingthatalltheweightsofagivenlayerarei.i.d., onemaywantto
initialize them with diﬀerent distributions or to introduce a dependence structure between
them. If the goal remains to obtain Gaussian pre-activations, this kind of generalization
should be feasible without changing drastically the constraints we are proposing.
Hypothesisofindependentpre-activations. Intheconstraints, wehaveassumedthat,
for any layer, its inputs are independent. This assumption is discussed in Remark 7 and in
Appendix B. According to the experimental results presented in Appendix B, we can build
speciﬁc cases where the dependence between inputs breaks the Gaussianity of the outputted
pre-activations, including with our pairs (φθ,Pθ).
Also, when comparing the distribution of the pre-activations obtained with (φθ,Pθ)in the
case of independent inputs (Fig. 6b) and in the case of dependent inputs (Fig. 7c and 11),
it is probable that the dependence between pre-activations tends to damage the Gaussianity
after a certain number of layers, for pairs (φθ,Pθ)with largeθ.
Other families of initialization distributions and activation functions. Provided
theconstraintswehavederived, wehavemadeonechoicetoobtainourfamilyofinitialization
distributions and activation functions: we have decided that the weights should be sampled
from a symmetric Weibull distribution W(θ,1). We have made this choice because Weibull
distributionsmeetimmediatelyConstraint2, andwecanmodulatetheirGeneralizedWeibull
Tail parameter easily (see Remark 9).
One may propose another family of initialization distributions, as long as it meets the con-
straints. However, such a family should be selected wisely: if one chooses a distribution with
compact support, or GWT(θ)withθ/greatermuch2, then the related activation function φis likely to
be almost linear. Intuitively, if we want Wφ(X)to beN(0,1)withX∼N (0,1)and very
light-tailed W, thenφmust “reproduce” the tail of its input X, soφmust be approximately
linear around inﬁnity.
Preserve a characteristic during propagation or impose stable ﬁxed points? In
previous works, both ideas have been used: Glorot & Bengio (2010) wanted to preserve the
variance of the pre-activations, while Poole et al. (2016); Schoenholz et al. (2017) wanted
to impose a speciﬁc stable ﬁxed point for the correlation map C. According to the results
we have obtained in Section 4.1, it is possible to preserve approximately the distribution of
the pre-activations when passing through onelayer. However, according to the results of
34Section 4.2, a drift can appear after several layers. So, when testing an initialization setup in
the real world, with numerical errors and approximations, it is necessary to check the stable
ﬁxed points of the monitored characteristic. However, this is not easy to do in practice:
without the Gaussian hypothesis, ﬁnding the possible limits of (cl
ab)lcan be diﬃcult.
Gaussian pre-activations and Neural Tangent Kernels (Jacot et al., 2018). With
our pair of activation functions and initialization procedure, we have been able to provide
non-asymptotic Edge of Chaos, removing the inﬁnite-width assumption. Since this inﬁnite-
widthassumptionisalsofundamentalinworksontheNeuralTangentKernels(NTKs),would
it be possible to obtain the same kind of results within the NTK setup? We believe it is not.
On one side, the inﬁnite-width limit is used to end up with an NTK, which is constant during
training, in order to provide exact equations of evolution of the trained neural network. On
the other side, within our setup, we only ensure Gaussian pre-activations, and it does not
imply that the NTK would be constant during training. Nevertheless, with Gaussian pre-
activations, we might expect an improvement in the convergence rate of the neural network
towards a stacked Gaussian process, as the widths of the layers tend to inﬁnity. So, the NTK
regime would be easier to attain.
Taking into account the generalization performance. In the EOC framework and
ours, a common principle could be discussed and improved. The generalization performance
is not taken into account at any step of the reasoning. As we have seen in the training
experiments, the main diﬃculty encountered within our setup is the overﬁtting: as the
training loss decreases without obstacle, the test loss is, at the end, worse than with ReLU
andtanhactivation functions. To improve these generalization results, one may integrate
into the framework a separation between the training set and a validation set.
Precise characterization of the pre-activations distributions Dl.Finally, ﬁnding a
precise and usable characterization of the Dlremains an unsolved problem. In the EOC line
of work, the problem has been simpliﬁed by using the Gaussian hypothesis. But, as shown
in the present work, such a simpliﬁcation is too rough and leads to inconsistent results.
Nevertheless, our approach has its own drawbacks. Namely, we can ensure Gaussian pre-
activations only when using speciﬁc initialization distributions and activation functions. So,
we still miss a characterization of the distributions Dlwhich would apply to widely-used
networks without harsh approximations, and be easy to use to achieve practical goals, such
as ﬁnding an optimal initialization scheme. From this perspective, we hope that the problem
representation of Section 2.4 will result in fruitful future research.
Is it desirable to have Gaussian pre-activations? We have provided several results
that should help to answer this tough question, but it remains diﬃcult to answer it deﬁni-
tively. One shall note a paradox regarding the ReLUactivation. On one side, when ReLUis
used in narrow and deep perceptrons, the pre-activations are far from being Gaussian, the
sequence of correlations does not converge to 1, and training is diﬃcult and unstable. On the
other side, in LeNet-type networks with ReLU, training is easy, and the resulting networks
35generalize well. Besides, our activation functions φθperform quite diﬀerently depending on
the setup: with φ2.05, LeNet can achieve good training losses, but the training of narrow and
deep perceptrons may fail. We observe opposite results with φ10.00. Therefore, the strongest
answer we can give is: with Gaussian pre-activations at initialization, a neural network is
likely (but not sure) to be trainable, but it is impossible to predict its ability to generalize.
Is the “Gaussian pre-activations hypothesis” a myth or a reality? We have shown
that, when using the ReLUactivation function in several practical cases, it is largely a myth.
But, when using tanh, the results depend on the neural network width: with a suﬃcient, but
still reasonable, number of neurons per layer, it becomes a reality. In order to ensure that
this reality remains tangible for any number of neurons per layer, we have established a set
of constraints that the design of the neural networks must fulﬁll, and we have proposed a set
of solutions fulﬁlling them. As a result, several of these solutions have all the prerequisite to
become strong foundations of this Gaussian hypothesis, making it real in all tested cases.
Acknowledgements
We would like to thank an anonymous reviewer for pointing out an error in a previous version
of the paper, and proposing an example which inspired Example 1 (see App. B).
References
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep
matrix factorization. Advances in Neural Information Processing Systems , 32, 2019.
Patrick Billingsley. Probability and Measure . John Wiley & Sons, third edition, 1995.
Bruno De Finetti. La prévision: ses lois logiques, ses sources subjectives. In Annales de
l’Institut Henri Poincaré , volume 7, pp. 1–68, 1937.
Benjamin Epstein. Some applications of the Mellin transform in statistics. The Annals of
Mathematical Statistics , pp. 370–379, 1948.
Arthur Erdélyi, (Hans Heinrich) Wilhelm Magnus, Fritz Oberhettinger, Francesco Giacomo
Tricomi, David Bertin, Watson B. Fulks, A. R. Harvey, Donald L. Thomsen, Jr., Maria A.
Weber,E.L.Whitney,andRosemarieStampfel. Higher transcendental functions ,volumeI.
Bateman Manuscript Project, 1953.
Vincent Fortuin, Adrià Garriga-Alonso, Sebastian W. Ober, Florian Wenzel, Gunnar Ratsch,
Richard E Turner, Mark van der Wilk, and Laurence Aitchison. Bayesian neural network
priors revisited. In International Conference on Learning Representations , 2022.
Bruno Gabutti and Laura Sacripante. Numerical inversion of the Mellin transform by accel-
eratedseriesofLaguerrepolynomials. Journal of Computational and Applied Mathematics ,
34(2):191–200, 1991.
36Xavier Glorot and Yoshua Bengio. Understanding the diﬃculty of training deep feedfor-
ward neural networks. In Proceedings of the 13th International Conference on Artiﬁcial
Intelligence and Statistics , pp. 249–256. JMLR Workshop and Conference Proceedings,
2010.
Izrail Solomonovich Gradshteyn and Iosif Moiseevich Ryzhik. Table of Integrals, Series, and
Products . Academic Press, eighth edition, 2014.
Alex Graves. Practical variational inference for neural networks. Advances in Neural Infor-
mation Processing Systems , 24, 2011.
Souﬁane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation func-
tion on deep neural networks training. In International Conference on Machine Learning ,
pp. 2672–2680. PMLR, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the
IEEE international conference on computer vision , pp. 1026–1034, 2015.
Matthew D Hoﬀman, David M Blei, Chong Wang, and John Paisley. Stochastic variational
inference. Journal of Machine Learning Research , 2013.
ArthurJacot,FranckGabriel,andClémentHongler. Neuraltangentkernel: Convergenceand
generalization in neural networks. Advances in Neural Information Processing Systems ,
31, 2018.
Diederik P Kingma and Jimmy L Ba. Adam: A method for stochastic optimization. In
International Conference on Learning Representations , 2015.
Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-
normalizing neural networks. Advances in neural information processing systems , 30, 2017.
Andrey Kolmogoroﬀ. Conﬁdence limits for an unknown distribution function. The Annals
of Mathematical Statistics , 12(4):461–463, 1941.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
Haitao Mao, Xu Chen, Qiang Fu, Lun Du, Shi Han, and Dongmei Zhang. Neuron campaign
for initialization guided by information bottleneck theory. In Proceedings of the 30th ACM
International Conference on Information & Knowledge Management , pp. 3328–3332, 2021.
Alexander G de G Matthews, Jiri Hron, Mark Rowland, Richard E Turner, and Zoubin
Ghahramani. Gaussian process behaviour in wide deep neural networks. In International
Conference on Learning Representations , 2018.
37Lorenzo Noci, Gregor Bachmann, Kevin Roth, Sebastian Nowozin, and Thomas Hofmann.
Precise characterization of the prior predictive distribution of deep ReLU networks. Ad-
vances in Neural Information Processing Systems , 34:20851–20862, 2021.
Yann Ollivier. Online natural gradient as a Kalman ﬁlter. Electronic Journal of Statistics ,
12(2):2930–2961, 2018.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the diﬃculty of training recur-
rent neural networks. In International Conference on Machine Learning , pp. 1310–1318.
PMLR, 2013.
Stefano Peluchetti, Stefano Favaro, and Sandra Fortini. Stable behaviour of inﬁnitely wide
deep neural networks. In International Conference on Artiﬁcial Intelligence and Statistics ,
pp. 1137–1146. PMLR, 2020.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli.
Exponential expressivity in deep neural networks through transient chaos. Advances in
Neural Information Processing Systems , 29, 2016.
Hadi Pouransari, Zhucheng Tu, and Oncel Tuzel. Least squares binary quantization of neural
networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops , pp. 698–699, 2020.
Andrew M Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Bren-
dan D Tracey, and David D Cox. On the information bottleneck theory of deep learning.
Journal of Statistical Mechanics: Theory and Experiment , 2019(12):124020, 2019.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep infor-
mation propagation. In International Conference on Learning Representations , 2017.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks
via information. arXiv preprint arXiv:1703.00810 , 2017. URL https://arxiv.org/pdf/
1703.00810.pdf .
VincentSitzmann, JulienMartel, AlexanderBergman, DavidLindell, andGordonWetzstein.
Implicit neural representations with periodic activation functions. Advances in Neural
Information Processing Systems , 33:7462–7473, 2020.
Nickolay Smirnov. Table for estimating the goodness of ﬁt of empirical distributions. The
Annals of Mathematical Statistics , 19(2):279–281, 1948.
P S Theocaris and A C Chrysakis. Numerical inversion of the Mellin transform. IMA Journal
of Applied Mathematics , 20(1):73–83, 1977.
Naftali Tishby. The information bottleneck method. In Proc. 37th Annual Allerton Confer-
ence on Communications, Control and Computing, 1999 , pp. 368–377, 1999.
38Edward Charles Titchmarsh. Introduction to the theory of Fourier integrals . The Clarendon
Press, Oxford, 2nd edition, 1937.
Mariia Vladimirova, Jakob Verbeek, Pablo Mesejo, and Julyan Arbel. Understanding priors
in Bayesian neural networks at the unit level. In International Conference on Machine
Learning , pp. 6458–6467. PMLR, 2019.
Mariia Vladimirova, Julyan Arbel, and Stéphane Girard. Bayesian neural network unit
priors and generalized Weibull-tail property. In Asian Conference on Machine Learning ,
pp. 1397–1412. PMLR, 2021.
Florian Wenzel, Kevin Roth, Bastiaan Veeling, Jakub Swiatkowski, Linh Tran, Stephan
Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How
good is the Bayes posterior in deep neural networks really? In International Conference
on Machine Learning , pp. 10248–10259. PMLR, 2020.
A Activation function with inﬁnite number of stable ﬁxed points for V
A.1 Proof that Vadmits an inﬁnite number of ﬁxed points when using φ=ϕδ,ω
Proposition 2. For anyδ∈(0,1]andω >0, let us pose the activation function φ=ϕδ,ω.
We consider the sequence (vl)ldeﬁned by:
∀l≥0, vl+1=σ2
w/integraldisplay
ϕδ,ω/parenleftBig√
vlz/parenrightBig2Dz+σ2
b, (19)
v0∈R+
∗.
Then there exists σw>0,σb≥0, and a strictly increasing sequence of stable ﬁxed points
(v∗
k)k∈Zof the recurrence Equation (19).
Proof.Let us deﬁne:
˜V(v) :=1
vV(v|σw= 1,σb= 0),
so we have:V(v|σw,σb) =σ2
wv˜V(v) +σ2
b. (20)
In the following, we use a simpliﬁed notation: V(v) =V(v|σw,σb).
Our goal is to ﬁnd a sequence (v∗
k)k∈Z,σwandσbsuch that:
∀k∈Z,V(v∗
k) =v∗
kandV/prime(v∗
k)∈(−1,1),
which would ensure that all v∗
kare stable ﬁxed points of V. In order to understand how to
build the sequence (v∗
k)k∈Z, let us consider v>0. Letσ2
b= 0andσ2
w= 1/˜V(v). So we have:
V(v) =v.
39So, anyv >0can possibly be a ﬁxed point if we tune σwaccordingly. We just have to ﬁnd
av>0such that:
V/prime(v)∈(−1,1).
with:
V/prime(v) =1
˜V(v)/parenleftBig˜V(v) +v˜V/prime(v)/parenrightBig
= 1 + ˜V/prime
e(ln(v)), (21)
where ˜Ve:r/mapsto→ln(˜V(exp(r))). So, knowing that ˜VeisC1and periodic, it is suﬃcient to
prove that it is not constant to ensure that we can extract one v∗
0such thatV/prime(v∗
0)∈(−1,1).
Then, by periodicity of ˜Ve, we can build a sequence of stable ﬁxed points (v∗
k)k∈Z.
We have:
˜V(v) =1
v/integraldisplay∞
−∞ϕδ,ω(√vz)2Dz
=/integraldisplay∞
−∞z2exp/parenleftBigg
2δ
ωsin(ωln|√vz|)/parenrightBigg
Dz
= 2/integraldisplay∞
0z2exp/parenleftBigg
2δ
ωsin(ωln(√vz))/parenrightBigg
Dz
= 2/integraldisplay∞
0z2exp/parenleftBigg
2δ
ωsin/parenleftbiggω
2lnv+ωlnz/parenrightbigg/parenrightBigg
Dz.
Lemma 1. ˜Vis not constant.
Proof.
˜V(v) =1√
2π2
v3/2/integraldisplay∞
0z2exp/parenleftBigg
2δ
ωsin(ωlnz)/parenrightBigg
exp/parenleftBigg
−z2
2v/parenrightBigg
dz
=1√
2π2
v3/2/integraldisplay∞
0z
2√zexp/parenleftBigg
2δ
ωsin(ωln√z)/parenrightBigg
exp/parenleftbigg
−z
2v/parenrightbigg
dz
=1√
2πv−3/2L/bracketleftBig
ϕδ,ω(√z)/bracketrightBig/parenleftbigg1
2v/parenrightbigg
,
whereLis the Laplace transform.
Let us suppose that ˜Vis constant:∀v>0,˜V(v) =c. So, if we pose v←1
2v, we have:
∀v>0, c =2√πv3/2L/bracketleftBig
ϕδ,ω(√z)/bracketrightBig
(v),
that is:
L/bracketleftBig
ϕδ,ω(√z)/bracketrightBig
(v) =c√π
2v−3/2.
40Since the function z/mapsto→ϕδ,ω(√z)is continuous on R+, then, almost everywhere (see Thm.
22.2, Billingsley, 1995):
ϕδ,ω(√z) =c√π
2L−1[v−3/2](z) =c√z,
which is impossible for δ∈(0,1]. Hence the result.
The function ˜Ve:r/mapsto→ln(˜V(exp(r)))is continuous and4π
ω-periodic:
˜Ve(r) = ln/bracketleftBigg
2/integraldisplay∞
0z2exp/parenleftBigg
2δ
ωsin/parenleftbiggω
2r+ωlnz/parenrightbigg/parenrightBigg
Dz/bracketrightBigg
,
so˜Veis lower and upper bounded and reach its bounds (and, by Lemma 1, these bounds are
diﬀerent). We deﬁne:
˜V+
e= max ˜Ve r+
0= inf/braceleftBig
r>0 :˜Ve(r) =˜V+
e/bracerightBig
,
˜V−
e= min ˜Ve r−
0= inf/braceleftBig
r>r+
0:˜Ve(r) =˜V−
e/bracerightBig
.
By continuity, ˜Ve(r+
0) =˜V+
eand˜Ve(r−
0) =˜V−
e. Since ˜V+>˜V−and˜VeisC1, then there exists
r∗
0∈(r+
0,r−
0)such that ˜V/prime
e(r∗
0)∈(−2,0).
Since ˜Veis4π
ω-periodic, we can deﬁne a sequence (r∗
k)k∈Zsuch that:
r∗
k:=r∗
0+4kπ
ω
˜Ve(r∗
k) =˜Ve(r∗
0) =: ˜V0
e
˜V/prime
e(r∗
k) =˜V/prime
e(r∗
0)∈(−2,0).
So, by using Eqn. (20) and Eqn. (21) with σ2
b= 0andσ2
w= 1/exp(˜V0
e):
v∗
k:= exp(rk),
V(v∗
k) =1
exp(˜V0
e)v∗
k˜V(v∗
k) =v∗
k
V/prime(v∗
k) = 1 + ˜V/prime
e(r∗
0)∈(−1,1).
Thus, (v∗
k)k∈Zis a sequence of stable ﬁxed points of V(·|σw,σb)for well-chosen σwandσb.
A.2 Practical computation of σ2
w
We propose a practical method to ensure that V(·|σw,σb= 0)admits an inﬁnite number of
stable ﬁxed points when using activation function φ=ϕδ,ω.
41In order to achieve this goal, we build σ2
w=σ2
ωin the following way:
Vlow:= 2/integraldisplay∞
0z2exp/parenleftBigg
−2δ
ωsin(ωln(z))/parenrightBigg
Dz
Vupp:= 2/integraldisplay∞
0z2exp/parenleftBigg
2δ
ωsin(ωln(z))/parenrightBigg
Dz,
σ2
ω:=/bracketleftbiggVlow+Vupp
2/bracketrightbigg−1
.
In practice, for δ= 0.99, we obtain σωforω∈{2,3,6}:
σ2≈0.879, σ 3≈0.945, σ 6≈0.987.
B Discussion about the independence of the pre-activations
In Proposition 3 and Constraint 1, we assume that, for any layer l, its inputs (Zl
j)jare
independent. In general, this is not true:
Example 1. LetX∼N (0,1)be some random input of a two-layer neural network. We
perform the following operation:
Z=1√
2/bracketleftBig
W2
1φ(W1
1X) +W2
2φ(W1
2X)/bracketrightBig
,
where (W1
1,W1
2,W2
1,W2
2)be i.i.d. random variables samples from some distribution P, and
φis some activation function.
LetP =R, the Rademacher distribution, i.e., if W∼R, thenW=±1with probability 1/2.
Letφ= Id. Then we have: Y1:=W1
1X∼N(0,1)andY2:=W1
2X∼N(0,1). But they are
not independent:
W2
1W1
1X+W2
2W1
2X= (W/prime
1+W/prime
2)X,
whereW2
1W1
1andW2
2W1
2are two independent Rademacher random variables. So, Z= 0
with probability 1/2. So,Zis not Gaussian.
In this example, we build a non-Gaussian random variables with a minimal neural network,
in which we construct two dependent random variables. So, we should pay attention to this
phenomenon when propagating the pre-activations in a neural network.
One should note that the structure of dependence of W2
1W1
1XandW2
2W1
2Xdoes not involve
their correlation (which is zero), and yet breaks the Gaussianity of their sum. So, in order
to obtain a theoretical result about the distribution of Z, we should study ﬁner aspects of
the dependence structure.
42But, on the practical side, we want to answer the question: to which extent does the relation
of dependence between the pre-activations (Zl
j)jaﬀect the Gaussianity of (Zl+1
i)i?
In order to answer this question, we propose a series of experiments on a two-layer neural
network. We consider a vector of inputs X∈Rn0, where the (Xj)1≤j≤n0areN(0,1)and
i.i.d. The scalar outputted by the network is:
Z=1√n1W2φ/parenleftBigg1√n0W1X/parenrightBigg
,
where the weights W1∈Rn1×n0andW2∈R1×n1are i.i.d.
We test this setup with diﬀerent initialization distributions Pand activation functions φ:
•usual ones: φ= tanh orReLU,P =N(0,σ2
w), whereσ2
wis such that the pair
(σ2
w,σ2
b= 0.01)lies at the EOC;
•ours:φ=φθ,P = Pθ=W(θ,1).
We study three cases:
•unfavorable case: n0= 1, variousn1∈[1,10];
the intermediary features W1Xare weakly “mixed”, so it is credible that they lead
to an output that is far from being Gaussian;
•favorable case: n1= 2, variousn0∈[1,10];21
the intermediary features W1Xare “mixed” with an increasing rate as n0increases;
•same-width case: n0=n1.
According to Figure 11:
•unfavorable case (1st graph, n0= 1):Zis far from being Gaussian, regardless of
the activation function. One should note that tanhand(Pθ,φθ)with small θlead a
distribution of Zthat is closer to the Gaussian than with ReLUand(Pθ,φθ)with
largeθ;
•favorable case (2nd graph, n1= 2):Zis closer to be Gaussian with out setup (Pθ,φθ)
than withφ= tanhorReLU, especially with larger n1;
•same-width case (3rd graph, n=n0=n1): the larger the width n, the closer Zis
to being Gaussian. For a ﬁxed n, the distribution of Zis closer to a Gaussian with
smallerθ. When we use (Pθ,φθ), we are close to the performance of tanhor better.
21The casen1= 1is trivial: there is no sum of dependent random variables in the second layer.
431 2 3 4 5 6 7 8 9 10
n110−310−210−1n0= 1, varying n1
φθ,θ= 2.05
φθ,θ= 2.50
φθ,θ= 3.00
φθ,θ= 4.00
φθ,θ= 5.00φθ,θ= 7.00
φθ,θ= 10.00
tanh
tanh, P =R
ReLU
1 2 3 4 5 6 7 8 9 10
n010−310−210−1n1= 2, varying n0
1 2 3 4 5 6 7 8 9 10
n0=n110−310−210−1varyingn0,n1KS-statistic for various layer sizes in a 2-layer NNFigure 11 – Evolution of the distance of the standardized distribution of Zto theN(0,1)
according to the Kolmogorov-Smirnov statistic. Weight initialization is W(θ,1)when using
φ=φθand is Gaussian according to the EOC when using φ= tanhorReLU. For each
point, we have computed the KS-statistic over 200 000samples of (X,W1,W2).
The casen0=n1is the most realistic one: usually, the sizes of the layers of a neural network
are of the same order of magnitude. In this case, our setup is better than or equivalent to
the one with tanh.
Remark 14 (Mixing of inputs and interference phenomenon) .We see in Figure 11 that,
in every graph, setups with small θlead to better results than the ones with large θ. This
observation could be explained by Example 1. In this example, the inputs are weakly “mixed”:
sinceX0is multiplied by a Rademacher random variable, it is possible to partially reconstruct
X0after the ﬁrst layer, and then build destructive interference (leading to an output Z= 0
half of the time).
So, if we want to avoid this “interference” phenomenon, we should use initialization distri-
butions and activation functions such that every layer “mixes” strongly the inputs. Notably,
initialization distributions should be far from being a combination of Dirac distributions.
Typically, Pθ=10is close to the Rademacher distribution (see Fig. 5a).
Remark 15 (Caseφ= tanhandP =R).According to Remark 14, the relatively good per-
formance of tanhshould be explained by the choice of a Gaussian initialization P. However,
the caseφ= tanhandP =R, shown in Fig. 11 (dotted magenta line), contradicts partially
this explanation. In this case, choosing a Rademacher initialization of the weights should
lead to worse results than with a Gaussian initialization.
It is certainly true in the ﬁrst graph ( n0= 1), but it is obviously false in the third graph
(n=n0=n1) for larger n. Actually, the setup φ= tanhandP =Ris better than all the
others in this graph.
So, the contradiction between Remarks 14 and 15 indicates that the dependence between
the inputs remains to be investigated in depth. Speciﬁcally, we did not take into account
the shape of the activation function in Remark 14. For instance, tanh-like functions make
the network “forget” information about the input, which may improve the “mixing” process,
and thus lead to better results
44C Constraints on the product of two random variables
Proposition4 (Densityofaproductofrandomvariablesat 0).LetW,Ybe two independent
non-negative random variables and Z=WY. LetfW,fY,fZbe their respective density.
Assuming that fYis continuous at 0withfY(0)>0, we have:
if lim
w→0/integraldisplay∞
wfW(t)
tdt=∞,then lim
z→0fZ(z) =∞. (22)
Moreover, if fYis bounded:
if/integraldisplay∞
0fW(t)
tdt<∞,thenfZ(0) =fY(0)/integraldisplay∞
0fW(t)
tdt. (23)
Proof.Letz,z0>0:
fZ(z) =/integraldisplay∞
0fY(t)1
tfW/parenleftbiggz
t/parenrightbigg
dt
≥/integraldisplayz0
0fY(t)1
tfW/parenleftbiggz
t/parenrightbigg
dt
≥inf
[0,z0]fY·/integraldisplayz0
01
tfW/parenleftbiggz
t/parenrightbigg
dt
≥inf
[0,z0]fY·/integraldisplay∞
z/z0fW(t)
tdt.
Let us take z0=√z. We have:
fZ(z)≥inf
[0,√z]fY·/integraldisplay∞
√zfW(t)
tdt.
Then we take the limit z→0, hence:
•if/integraltext∞
0fW(t)
tdt=∞, then: limz→0fZ(z) =∞, which achieves (22);
•if/integraltext∞
0fW(t)
tdt<∞, then:fZ(0)≥fY(0)/integraltext∞
0fW(t)
tdt, which achieves one half of (23);
Let us prove the second half of (23). Let z,z0>0:
fZ(z) =/integraldisplay∞
0fY(t)1
tfW/parenleftbiggz
t/parenrightbigg
dt
=/integraldisplayz0
0fY(t)1
tfW/parenleftbiggz
t/parenrightbigg
dt+/integraldisplay∞
z0fY(t)1
tfW/parenleftbiggz
t/parenrightbigg
dt
≤sup
[0,z0]fY·/integraldisplay∞
z/z0fW(t)
tdt+/integraldisplay∞
1fY(z0t)1
tfW/parenleftbiggz
z0t/parenrightbigg
dt
45Letz0=√z. We have:
fZ(z)≤sup
[0,√z]fY·/integraldisplay∞
√zfW(t)
tdt+/integraldisplay∞
1fY/parenleftBig√zt/parenrightBig1
tfW/parenleftBigg√z
t/parenrightBigg
dt,
where:
/integraldisplay∞
1fY/parenleftBig√zt/parenrightBig1
tfW/parenleftBigg√z
t/parenrightBigg
dt≤/bardblfY/bardbl∞/integraldisplay∞
11
tfW/parenleftBigg√z
t/parenrightBigg
dt
≤/bardblfY/bardbl∞/integraldisplay√z
0fW(t)
tdt.
According to the hypotheses, we have, as z→0:
sup
[0,√z]fY→fY(0)
/integraldisplay∞
√zfW(t)
tdt→/integraldisplay∞
0fW(t)
tdt
/integraldisplay√z
0fW(t)
tdt→0,
hence the result.
D Activation functions with vertical tangent at 0
In the following lemma, we show that if we want the activation Yto have a density that is
0at0, then the activation function φshould have a vertical tangent at 0.Gplays the role
of pre-activation.
Lemma 2. Letφbe a function transforming a Gaussian random variable G∼N(0,1)into
a symmetrical random variable Ywith a density fYsuch thatfY(0) = 0. That is,Y=φ(G).
Thenφhas a vertical tangent at 0.
Proof.We have:
φ(x) =F−1
Y(FG(x)),
whereFGandFYare the respective CDFs of GandY.
Thus:
φ/prime(x) =F/prime
G(x)1
F/prime
Y(F−1
Y(FG(x)))
46Therefore:
φ/prime(0) =F/prime
G(0)1
F/prime
Y(F−1
Y(FG(0)))
=1√
2π1
F/prime
Y(F−1
Y(1/2))
=1√
2π1
F/prime
Y(0)
=∞.
E The Mellin transform
E.1 Generalities
We assume that G=WY∼N (0,1). Let us consider the random variables |W|,|Y|and
|G|=|W|·|Y|. Letf|W|,f|Y|andf|G|be their densities. Under integrability conditions,
we can express the density f|G|of the product|G|=|W||Y|with the product-convolution
operator ˙∗:
f|G|(z) = (f|W|˙∗f|Y|)(z),where (f|W|˙∗f|Y|)(z) =/integraldisplay∞
0f|W|/parenleftbiggz
t/parenrightbigg
f|Y|(t)1
tdt.
We can also express the CDF of |G|this way:
F|G|(z) =/integraldisplay∞
0F|W|/parenleftbiggz
t/parenrightbigg
f|Y|(t) dt. (24)
Then, we can use the following property of the Mellin transform M:
Mf|G|= (Mf|W|)·(Mf|Y|),where (Mf)(t) =/integraldisplay∞
0xt−1f(x) dx.
In short,Mtransforms a product-convolution into a product in the same manner as the
Fourier transform Ftransforms a convolution into a product. We have then:
f|Y|(y) :=M−1/bracketleftBiggMf|G|
Mf|W|/bracketrightBigg
(y).
Then, by symmetry, we can obtain fYfromf|Y|. However, whileMf|G|andMf|W|are easy
to compute, the inverse Mellin transform M−1seems to be analytically untractable in this
case:
(Mf|G|)(s) =2s
2−1
2Γ(s
2)√π, (Mf|W|)(s) = Γ/parenleftbiggs−1
θ+ 1/parenrightbigg
,
so:(Mf|G|)(s)
(Mf|W|)(s)=1√π2s
2−1
2Γ(s
2)
Γ/parenleftBig
s−1
θ+ 1/parenrightBig.
47E.2 Numerical inversion of the Mellin transform
Computation of f|Y|by numerical inverse Mellin transform. The Mellin transform
of a function can be inverted by using Laguerre polynomials. Speciﬁcally, we use the method
proposed by Theocaris & Chrysakis (1977) and slightly accelerated by the numerical proce-
dure of Gabutti & Sacripante (1991):
(M−1f)(z) =e−z
2∞/summationdisplay
k=0ck+1Lk/parenleftbiggz
2/parenrightbigg
,withck:=k/summationdisplay
n=1/parenleftBiggk−1
n−1/parenrightBigg
(−1)n−1f(n)
2nΓ(n),(25)
wherethe (Lk)karetheLaguerrepolynomials(seeSection7.41,Gradshteyn&Ryzhik,2014).
Experiments. A common way of computing the inverse Mellin transform consists of using
Equation (25). Speciﬁcally, the sequence (ck)kmust be computed.
In order to compute the density fYofQθwithθ= 2.05, we have computed numerically
(ck)kfork∈[1,500]. The results are plotted in Figure 12. We have tested three methods to
compute the ck:
•ﬂoating-point operations using 64 bits ﬂoats;
•ﬂoating-point operations using 128 bits ﬂoats;
•SymPy: make the whole computation using SymPy, a Python library of symbolic
computation (very slow).
In all three cases, instabilities appear before the sequence (ck)khas fully converged to 0.
Moreover, the oscillations of (ck)karound 0have an increasing wavelength, which indicates
that we may have to go far beyond k= 500to get enough coeﬃcients (ck)kto reconstruct
the wanted inverse Mellin transform.
In Figure 13, we have plotted two estimations of f|Y|: the density obtained directly by using
(ck)k∈[1,300], and the density obtained by using a sequence (ck)k∈[1,20000]where the values
(ck)k∈[301,20000]have been extrapolated from (ck)k∈[1,300].22
The resulting estimations of the density f|Y|take negative values and seem to be noised. So,
more work is needed to obtain smooth and proper densities, especially if we want them to
meet Constraints 3 and 4 (density at 0and decay rate at ∞).
Conclusion. We observe that this computation of the inverse Mellin transform has several
intrinsic problems:
•the computation of the ckcoeﬃcients involves a sum of terms with alternating signs,
which become larger (in absolute value) as kgrows and which are supposed to com-
pensate such that ck→0ask→∞. Such a numerical computation, involving both
large and small terms, makes the resulting ckvery unstable as kgrows;
22The extrapolation has been performed by modeling the graph of (ck)kas the product of a decreasing
function and a cosine with decreasing frequency.
480 100 200 300 400 500−1.0−0.50.00.51.0ﬂoating-point, 64 bits
ﬂoating-point, 128 bits
SymPyFigure 12 – Evolution of various numerical computations of ckaskgrows.
0 2 4 6 8 100.00.51.01.5
(ck)k∈[1,N],N= 300
(ck)k∈[1,N], extrapolated with N= 20000
Figure 13 – Numerical inverse Mellin transform with two diﬀerent computations of (ck)k:
direct computation of (ck)k∈[1,N]withN= 300; extrapolation of (ck)k∈[301,20000]from
(ck)k∈[1,300].
•whenθ≈2, the sequence (ck)ktends extremely slowly to 0;
•if we approximate M−1fwith the ﬁnite sum of the ﬁrst Kterms of the series in
Equation (25), we cannot guarantee the non-negativeness of the resulting function,
which is meant to be a density.
So, this method is unpractical to compute the density of a distribution in our case.
49F Obtaining f|Y|: experimental details
We optimize the vector of parameters Λwith respect to the following loss:
/lscript(Λ) :=/bardblˆFΛ−F|G|/bardbl∞
ˆFΛ(z) :=/integraldisplay∞
0F|W|/parenleftbiggz
t/parenrightbigg
gΛ(t) dt.
Dataset. We build the dataset Zof sized:
Z=/braceleftbigg
0,zmax1
d−1,zmax2
d−1,···,zmax/bracerightbigg
.
In our setup, d= 200andzmax= 5.
Computing the loss. For eachzinZ, we compute numerically ˆFΛ(z). Then, we are
able to compute /lscript(Λ). We keep track of the computational graph with PyTorch, in order to
backpropagate the gradient and train the parameters Λby gradient descent.
Initialization of the parameters. We initialize Λ = (α,γ,λ 1,λ2)in the following way:
α= 3, γ = 1, λ 1= 1, λ 2= 1.
Optimizer. We use the Adam optimizer (Kingma & Ba, 2015) with the parameters:
•learning rate: 0.001;
•β1= 0.9;
•β2= 0.999;
•weight decay: 0.
We train Λfor100epochs.
Learning rate scheduler. We use a learning rate scheduler based on the reduction of the
training loss. If the training loss does not decrease at least by a factor 0.01for20epochs,
then the learning rate is multiplied by a factor 1/3√
10. After a modiﬁcation of the learning
rate, we wait at least 20epochs before any modiﬁcation.
Scheduler for θ/prime.We recall that the deﬁnition of gΛinvolvesθ/prime, deﬁned by:1
θ+1
θ/prime=1
2.
It is not a parameter to train. Empirically, we found that the following schedule improves
the optimization process:
•from epoch 0to epoch 49,θ/primeincreases linearly from 2to its theoretical value (1
2−1
θ)−1;
50•at the beginning of epoch 50, we reinitialize the optimizer and the learning rate
scheduler;
•we ﬁnish the training normally, with θ/prime= (1
2−1
θ)−1.
G The Kolmogorov–Smirnov test
Description. We describe here the Kolmogorov–Smirnov (KS) test (Kolmogoroﬀ, 1941;
Smirnov, 1948). Given a sequence (Z/prime
1,···,Z/prime
s)ofsi.i.d. random variables sampled from P/prime:
1. we build the empirical CDF Fsof this sample:
Fs(z) =1
ss/summationdisplay
k=11Z/prime
k≤z;
2. we compare Fsto the CDF FGofG∼N(0,1)by using theL∞norm:
Ds=/bardblFs−FG/bardbl∞,
whereDsis the “KS statistic”;
3. under the null hypothesis, i.e. P/prime=N(0,1), we have:
√sDsd→K,
whereKis the Kolmogorov distribution (Smirnov, 1948). We denote by (Kα)αthe
quantiles of K:
P(K≤Kα) = 1−α,for allα∈[0,1];
4. ﬁnally, we reject the null hypothesis at level αif:
√sDs≤Kα.
Limitations. IntheKStestpresentedabove, thenullhypothesis H0isP/prime=N(0,1), which
isexactlywhatweintendtodemonstratewhenusingourfamily {(Pθ,φθ) :θ∈(2,∞)}, while
the alternative hypothesis H1isP/prime/negationslash=N(0,1). With this test design, we face a problem: it
is impossible to claim that H0holds. More precisely, we have two possible outcomes: either
H0is rejected, or it is not. Since H1is the complementary of H0, a reject of H0means an
accept of H1. But the converse does not hold: if H0is not rejected, then it is impossible to
conclude that H0is true: the sample size smay simply be too small, or the KS test may be
inadequate for our case.
51So, to be able to conclude that the pre-activations are Gaussian, we should have built an
alternative test, where the null hypothesis H/prime
0is somemisﬁtbetween P/primeandN(0,1). Or, at
least, we should have computed the power of the current KS test.
However, our experimental results are suﬃcient to compare the quality of ﬁt between P/primeand
N(0,1)in the tested setups (see Figures 6 and 7). It remains clear that, with {(Pθ,φθ) :θ∈
(2,∞)}, the Gaussian pre-activations hypothesis is far more likely to hold than in setups
involving tanh,ReLUand Gaussian weights.
H Experiments
H.1 Propagation of the correlations with MNIST
Within the setup of Section 2.2, we have plotted in Figure 14 the correlations propagated
in a multilayer perceptron with inputs sampled for MNIST ( 10samples per class, that is,
100samples in total). The layers of the perceptron have nl= 10neurons each, and the
activation function is φ= ReLU.
We observe that, in this narrow NN case ( nl= 10), some irregularities appear as information
propagates into the network: some classes seem to (de)correlate in an inconsistent way with
the others. Speciﬁcally, while most of the classes tend to correlate exactly ( C∗≈1), the
third class (corresponding to the digit “ 2”) tends to a correlation C∗≈0.7/negationslash= 1. This result
is contradictory to the EOC theory.
φ input l= 10 l= 30 l= 50
ReLU
(MNIST)
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
−1.0−0.5 0.0 0.5 1.0
Correlation
Figure14–Propagationofcorrelations cl
abinamultilayerperceptronwithactivationfunction
φ= ReLU and inputs sampled from the MNIST dataset. The neural network is initialized at
the EOC. Each plot displays a 10×10matrixCl
pqwhose entries are the average correlation
between the pre-activations propagated by samples from classes p,q∈{0,···,9}, at the
input and right after layers l∈{10,30,50}. See also Figure 1 in Section 2.2 for results on
CIFAR-10.
52H.2 Propagation of the correlations with φ=φθ
Within the setup of Section 2.2, we have plotted in Figure 15 the correlations propagated in
a multilayer perceptron with φ=φθ. The weights have been sampled from W(θ,1).
In this setup, the results are consistent, and are consistent with the results for φ= tanh(see
Figure 1): the sequence (Cl
pq)lconverges to 1, which was not the case for φ= ReLU and
nl= 10.
H.3 Variance of the pre-activation when using φ=φθ
As observed in Figure 6a, the product Wφθ(X)is above the KS threshold, corresponding to
s= 107samples and a p-value of 0.05. This is not necessarily the case in Figure 6b, where
the samples are standardized. So, we suspect that the variance of Wφθ(X)is not exactly 1.
Therefore, we have reported in Table 2 the empirical standard deviation of the product
Wφ(X)(i.e.,Z/primewithn= 1), computed with s= 107samples, where X∼N (0,1),W∼
W(θ,1)ifφ=φθandW∼N (0,1)ifφ= tanhorReLU. We observe that the standard
deviation of Z/prime, which is expected to be 1withφ=φθ, is actually a bit diﬀerent. This would
largely explain the diﬀerences between Figure 6a and Figure 6b.
Table 2 – Empirical standard deviation of Z/primewithn= 1.
φθ tanh ReLU
θ2.05 2.5 3 4 5 7 10
¯σ1.003 0.994 0.989 0.984 0.981 0.979 0.978 0.628 0.707
H.4 Propagation of the pre-activations
In this subsection, we show how the choice of the input data points aﬀect the propagation
of the pre-activations.
LetDlbe the distribution of the pre-activation Zl
1after layerl. In Figure 16, we have plotted
theL∞distance between the CDFs of DlandN(0,1)for various input points, sampled
from diﬀerent classes to improve diversity between them. We have chosen the following
setup: CIFAR-10 inputs, multilayer perceptron with 100layers and 100neurons per layer,
Gaussian initialization at the EOC when using the activation function φ= tanhorReLU,
and symmetric Weibull initialization W(θ,1)when using φ=φθ.
This setup has been selected to illustrate clearly the variability of Dlwhen using various
data points. This variability can be observed with 10or1000neurons per layer, though it
is less striking.
53φ input l= 10 l= 30 l= 50nl= 10neurons per layerφ2.05
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
φ7.00
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8nl= 100neurons per layerφ2.05
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
φ7.00
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
0 2 4 6 8
0
2
4
6
8
−1.0−0.5 0.0 0.5 1.0
Correlation
Figure15–Propagationofcorrelations cl
abinamultilayerperceptronwithactivationfunction
φθwithθ∈{2.05,7.00}and inputs sampled from the CIFAR-10 dataset. The weights are
sampled fromW(θ,1)and the biases are zero. Each plot displays a 10×10matrixCl
pqwhose
entries are the average correlation between the pre-activations propagated by samples from
classesp,q∈{0,···,9}, at the input and right after layers l∈{10,30,50}.
54With input normalization over the whole dataset. In this setup, we perform the
usual normalization over the whole dataset:
ˆxa;ij:=xa;ij−µi
σi,
µi:=1
Npi/summationdisplay
x∈Dpi/summationdisplay
j=1xij
σ2
i:=1
Npi−1/summationdisplay
x∈Dpi/summationdisplay
j=1(xij−µi)2,
where ˆxis the normalized data point, xa;ijis thej-th component of the i-th channel of the
input image xa,piis the size of the i-th channel, and Nis the size of the dataset D.
We observe that, when using the activation function φ= tanh, the shape of the curves is
the same for all data points. This is not the case for ReLU: for the input point “bird”, the
sequence (Dl)ldrifts away fromN(0,1)since the beginning, while for the input “car”, (Dl)l
ﬁrst becomes closer to N(0,1), then drifts away. It is also the case for φθwithθ= 10: for
the input “deer”, the distance remains high, while it starts low and increases very slowly
(input “dog”), or even decreases in the ﬁrst place (input “truck”).
But, in general, when the curves have converged after 100layers, it seems that the limit is
the same whatever the starting data point, and depends only on the choice of the activation
function.
With individual input normalization. In this setup, we perform the individual nor-
malization of the inputs. Without loss of generality, we deﬁne it for inputs that are order- 1
tensors as follows:
ˆxa;i:=xa;i−µ
σ,
µ:=1
pp/summationdisplay
i=1xi
σ2:=1
p−1p/summationdisplay
i=1(xi−µ)2,
where ˆxis the normalized data point, xa;iis thei-th component of xa∈Rp.
According to Figure 17, the curves seem to be approximately identical for each data point,
contrarytothesetupwithnormalizationoverthewholedataset. And, naturally, thedistance
betweenD1andN(0,1)is close to 0when using the activation functions φθ, which conﬁrms
that our method to build Gaussian pre-activations is eﬃcient at least in the beginning.
Conclusion. In the speciﬁc case of CIFAR-10 images, it seems that the trajectory of the
sequence (Dl)ldepends on the inputs, but only to the extent that they have diﬀerent norms.
As it can be seen in Figure 17, images from diﬀerent classes which have been normalized
individually lead to the same trajectories of distribution of pre-activations.
55φθ,θ= 2.05
φθ,θ= 2.50
φθ,θ= 3.00
φθ,θ= 4.00
φθ,θ= 5.00φθ,θ= 7.00
φθ,θ= 10.00
tanh
ReLU
KS, p-val = 0 .05
0 20 40 60 80 1000.00.10.20.30.40.5Data point #47981 (class = plane)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #24228 (class = car)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #10725 (class = bird)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #21574 (class = cat)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #12672 (class = deer)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #21928 (class = dog)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #19676 (class = frog)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #21007 (class = horse)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #26720 (class = ship)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #15729 (class = truck)Figure 16 – Setup with data points normalized over the whole dataset .
Propagation of the distribution Dlof the pre-activations across the layers l∈[1,100]when
inputting various data points of CIFAR-10. Each curve represents the L∞distance between
the CDF ofDland the CDF of the Gaussian N(0,1). The dashed green line is the threshold
of rejection of the Kolmogorov–Smirnov test with p-value 0.05: if a distribution Dlis repre-
sented by a point above this threshold, then the hypothesis “ Dl=N(0,1)” is rejected with
p-value 0.05.
H.5 Experimental details of the training procedure
Training, validation, and test sets. For MNIST and CIFAR-10, we split randomly the
initial training set into two sets: the training set, which will be actually used to train the
neural network, and the validation set, which will be used to stop training when the network
begins to overﬁt.
The sizes of the diﬀerent sets are as follows:
56φθ,θ= 2.05
φθ,θ= 2.50
φθ,θ= 3.00
φθ,θ= 4.00
φθ,θ= 5.00φθ,θ= 7.00
φθ,θ= 10.00
tanh
ReLU
KS, p-val = 0 .05
0 20 40 60 80 1000.00.10.20.30.40.5Data point #47981 (class = plane)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #24228 (class = car)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #10725 (class = bird)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #21574 (class = cat)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #12672 (class = deer)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #21928 (class = dog)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #19676 (class = frog)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #21007 (class = horse)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #26720 (class = ship)
0 20 40 60 80 1000.00.10.20.30.40.5Data point #15729 (class = truck)Figure 17 – Setup with data points normalized individually .
The rest of the setup is identical to the one used in Figure 16.
•MNIST: 50000 training samples; 10000 validation samples; 10000 test samples;
•CIFAR-10: 42000 training samples, 8000 validation samples; 10000 test samples.
The training sets are split into mini-batches with 200 samples each. No data augmentation
is performed.
Loss.Givenaclassiﬁcationtaskwith Pclasses,let zL∈RPbethepre-activationoutputted
by the last layer of the neural network. First, we perform a softmax operation:
yp:= softmax( zL
p) =exp(zL
p)
/summationtextP
p/prime=1exp(zL
p/prime).
57where the (yp)pare the components of y∈RPand(zL
p)pare the components of zL. Then,
we compute the negative log-likelihood loss. For a target class p∈{1,···,P}, we pose:
/lscript(y,p) :=−log(yp).
Optimizer. We use the Adam optimizer (Kingma & Ba, 2015) with the parameters:
•learning rate: 0.001;
•β1= 0.9;
•β2= 0.999;
•weight decay: 0.
Learning rate scheduler. We use a learning rate scheduler based on the reduction of the
training loss. If the training loss does not decrease at least by a factor 0.01for10epochs,
then the learning rate is multiplied by a factor 1/√
10.
Early stopping. We add an early stopping rule based on the reduction of the validation
loss. If the validation loss does not decrease at least by a factor of 0.001for30epochs, then
we stop training.
58