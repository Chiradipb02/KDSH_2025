Published in Transactions on Machine Learning Research (04/2024)
Understanding Fairness Surrogate Functions
in Algorithmic Fairness
Wei Yao⋆wei.yao@ruc.edu.cn
Gaoling School of Artificial Intelligence
Renmin University of China, Beijing
Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing
Zhanke Zhou⋆cszkzhou@comp.hkbu.edu.hk
TMLR Group, Department of Computer Science
Hong Kong Baptist University
Zhicong Li zhicongli@ruc.edu.cn
Gaoling School of Artificial Intelligence
Renmin University of China, Beijing
Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing
Bo Han bhanml@comp.hkbu.edu.hk
TMLR Group, Department of Computer Science
Hong Kong Baptist University
Yong Liu†liuyonggsai@ruc.edu.cn
Gaoling School of Artificial Intelligence
Renmin University of China, Beijing
Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing
Reviewed on OpenReview: https://openreview.net/forum?id=iBgmoMTlaz
Abstract
It has been observed that machine learning algorithms exhibit biased predictions against
certain population groups. To mitigate such bias while achieving comparable accuracy, a
promising approach is to introduce surrogate functions of the concerned fairness definition
andsolveaconstrainedoptimizationproblem.However,itisintriguinginpreviousworkthat
such fairness surrogate functions may yield unfair results and high instability. In this work,
in order to deeply understand them, taking a widely used fairness definition—demographic
parity as an example, we show that there is a surrogate-fairness gap between the fairness
definitionandthefairnesssurrogatefunction.Also,thetheoreticalanalysisandexperimental
results about the “gap” motivate us that the fairness and stability will be affected by the
points far from the decision boundary, which is the large margin points issue investigated in
this paper. To address it, we propose the general sigmoid surrogate to simultaneously reduce
both the surrogate-fairness gap and the variance, and offer a rigorous fairness and stability
upper bound. Interestingly, the theory also provides insights into two important issues that
deal with the large margin points as well as obtaining a more balanced dataset are beneficial
to fairness and stability. Furthermore, we elaborate a novel and general algorithm called
Balanced Surrogate, which iteratively reduces the “gap” to mitigate unfairness. Finally,
we provide empirical evidence showing that our methods consistently improve fairness and
stabilitywhilemaintainingaccuracycomparabletothebaselinesinthreereal-worlddatasets.
⋆indicates equal contribution.
†indicates the corresponding author.
1Published in Transactions on Machine Learning Research (04/2024)
/uni00000016
 /uni00000015
 /uni00000014
 /uni00000013 /uni00000014 /uni00000015 /uni00000016/uni00000013/uni00000011/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002b/uni0000004c/uni00000051/uni0000004a/uni00000048
/uni0000002f/uni00000052/uni0000004a/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni00000026/uni00000052/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000033/uni00000055/uni00000052/uni0000005b/uni0000005c
/uni00000032/uni00000058/uni00000055/uni00000056/uni0000001d/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002c/uni00000051/uni00000047/uni0000004c/uni00000046/uni00000044/uni00000057/uni00000052/uni00000055
(a) Surrogate Functions
/uni00000016
 /uni00000015
 /uni00000014
 /uni00000013 /uni00000014 /uni00000015 /uni00000016/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000015/uni00000018/uni00000014/uni00000011/uni00000018/uni00000013/uni00000014/uni00000011/uni0000001a/uni00000018/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002b/uni0000004c/uni00000051/uni0000004a/uni00000048
/uni0000002f/uni00000052/uni0000004a/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni00000026/uni00000052/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000033/uni00000055/uni00000052/uni0000005b/uni0000005c
/uni00000032/uni00000058/uni00000055/uni00000056/uni0000001d/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002c/uni00000051/uni00000047/uni0000004c/uni00000046/uni00000044/uni00000057/uni00000052/uni00000055 (b) Surrogate-Fairness Gap
Figure 1: (a) Some examples of fairness surrogate functions. The closer the surrogate functions are to the in-
dicator function, the better they represent DP. More details are introduced in Section 3.2. (b) The surrogate-
fairness gap of different surrogate functions. It measures the difference between surrogate functions and the
indicator function. There is a much smaller gap for our general sigmoid surrogate.
1 Introduction
Recently, increasing attention has been paid to the fairness issue in supervised machine learning. That is,
although the classifiers seek a higher accuracy, some groups with certain sensitive features (e.g., sex, race,
age) may be unfairly treated, which raises ethical problems (Julia Angwin & Kirchner, 2016; Mehrabi et al.,
2021; Caton & Haas, 2020). One can be litigated for committing adverse impacts if his/her decision-making
process disproportionately treats groups with sensitive attributes (Barocas & Selbst, 2016).
To quantitatively measure the extent of fairness violation, a usual way is adopting the fairness definition,
demographic parity (DP), which requires the decision makers to accept a roughly equal proportion of each
group (Barocas et al., 2019). Existing methods follow the fairness-aware manner of solving a constrained
optimization problem, where the learning objective is integrated with the standard loss and a fairness con-
straint. To incorporate DP into the constraint, fairness surrogate functions are used to replace the indicator
function, which is intractable for gradient-based algorithms (Lohaus et al., 2020; Bendekgey & Sudderth,
2021) (refer to Figure 1(a) for some examples). To date, various surrogate functions have been proposed to
incorporate fairness definitions into constraints (Wu et al., 2019; Goh et al., 2016; Padh et al., 2021; Zafar
et al., 2017a;b;c; Bendekgey & Sudderth, 2021). They are widely applied in various machine learning do-
mains, such as differential privacy (Ding et al., 2020), meta-learning (Zhao et al., 2020), and semi-supervised
learning (Zhang et al., 2020). Unfortunately, these surrogate functions encounter two risks. One risk is that
if these fairness constraints are used, even when the constraints are perfectly satisfied, there is no guarantee
whether DP is satisfied (Lohaus et al., 2020). And the fairness surrogate functions may lead to even unfair
solutions (Radovanović et al., 2022). Moreover, another risk arises from the high variance issue observed in
existing fairness-aware algorithms with surrogate functions (Friedler et al., 2019), rendering them unstable
for deployment under fairness requirements (Ganesh et al., 2023).
In this paper, we evaluate these multifarious surrogate functions in algorithmic fairness with both rigorous
theorems and extensive experiments. Firstly, we stress the importance of the “ surrogate-fairness gap ”,
which is the disparity between the fairness surrogate function and the fairness definition . It is the decisive
factor of whether the fairness surrogate function can lead to fair outcomes and should be minimized. Addi-
tionally, we delve into the variance of the substitute for DP, highlighting the adverse impact of unbounded
surrogates on stability. Drawing upon the inherent property of the surrogate-fairness gap and instability,
we conduct an in-depth examination of the large margin points issue within the context of unbounded
surrogate functions. To reduce the “gap”, we propose two solutions to improve the existing surrogate func-
2Published in Transactions on Machine Learning Research (04/2024)
tions: a theoretically motivated fairness surrogate function named general sigmoid with upper bounds of the
violation of DP, and a novel algorithm called the balanced surrogate to iteratively reduce the gap during
training.
Our analysis is general for fairness surrogate functions in the case of a common fairness definition, DP.
Additionally, we employ a widely recognized covariance proxy (Zafar et al., 2017c) as an illustrative instance
of fairness surrogate function. In particular, we first derive the violation of DP for the fairness surrogate
functions in Section 4. The violation of DP depends on two factors: the surrogate function itself and the
surrogate-fairness gap. The “gap” (shown in Figure 1(b)) directly determines whether a surrogate function
is an appropriate substitute for DP. Secondly, we explore the variance of the surrogate function in Section
4.1, emphasizing the detrimental impact of unbounded surrogates on stability. Furthermore, driven by the
“gap” and variance, we recognize that large margin points—those data points lying significantly distant from
the decision boundary—pose challenges in constraining the fairness and stability for unbounded surrogate
functions. This observation is validated through a case study on three real-world datasets in Section 4.2.
With theoretical motivations, we introduce the general sigmoid surrogate in Section 5.1 to address large
margin points and simultaneously bound the “gap” and variance. We theoretically demonstrate that there is
a reliable fairness and stability guarantee for it. Interestingly, the theorems also shed light on the importance
of a balanced dataset for both fairness and stability. Furthermore, in Section 5.2, we propose balanced
surrogates, a novel and general algorithm that iteratively reduces the “gap” to improve fairness. It is a
plug-and-play learning paradigm for the naive fairness-aware training framework using fairness surrogate
functions. In the experiments in Section 6 using three real-world datasets, our methods generally enhance
fair predictions and stability, while maintaining accuracy comparable to the baselines. Overall, our main
contributions are three-fold:
•We demonstrate the importance of Surrogate-fairness Gap for fairness surrogate functions and pro-
vide an analysis of the variance. We emphasize the importance for researchers to consider the impact
of thelarge margin points issue on the fairness and stability of unbounded surrogate functions.
•We propose General Sigmoid Surrogate and demonstrate that it achieves fairness and stability guar-
antees. The theoretical results further provide insights to the community that large margin points
issueneeds to be solved and a balanced dataset is beneficial to obtain a fairer and more stable
classifier.
•We present Balanced Surrogate , a novel and general method that iteratively reduces the “gap” to
improve the fairness of any fairness surrogate functions.
2 Related Work
Fairness-aware Algorithms. To mitigate bias, there are various kinds of classical fair algorithms, most
of which fall into three categories: pre-processing, in-processing, and post-processing. The pre-processing
method is to learn a fair representation that tries to remove information correlated to the sensitive feature
while preserving other information for training, e.g., (Calders et al., 2009; Kamiran & Calders, 2011; Zemel
et al., 2013; Feldman et al., 2015; Calmon et al., 2017). The downstream tasks then use the fair representation
instead of the original biased dataset. The post-processing method is to modify the prediction results to
satisfy the fairness definition, e.g., (Kamiran et al., 2012; Fish et al., 2016; Hardt et al., 2016). The in-
processing method is to remove unfairness during training. Some intuitive and easy-to-use ideas involve
applying fairness constraints (Goh et al., 2016; Zafar et al., 2017a;b;c; Bechavod & Ligett, 2017; Wu et al.,
2019;Bendekgey&Sudderth,2021;Padhetal.,2021)andaddingaregularizationtermtopenalizeunfairness
(Kamishima et al., 2012; Berk et al., 2017; Agarwal et al., 2018; Lohaus et al., 2020; Shui et al., 2022b).
Refer to Appendix C.1 for other fairness-aware in-processing approaches. Our paper focuses on in-processing
methods, with a particular emphasis on fairness surrogate functions, which are widely used in fairness
constraints and fairness regularization methods mentioned above.
Fairness Surrogate Functions. Although many existing popular surrogates work well in practice, for
example, linear (Donini et al., 2018; Agarwal et al., 2018; Bechavod & Ligett, 2017), ramp (Goh et al.,
3Published in Transactions on Machine Learning Research (04/2024)
2016; Zafar et al., 2017b), convex-concave (Zafar et al., 2017a), hinge (Wu et al., 2019), sigmoid and log-
sigmoid (Bendekgey & Sudderth, 2021). They suffer from the same issue: there is not a fairness guarantee
for them (Lohaus et al., 2020). And using fairness constraints or regularization can unexpectedly yield unfair
solutions (Radovanović et al., 2022). Refer to Appendix C.2 for a meticulous overview of existing works,
most of which present counterexamples for analysis. The high variance issue has been observed in existing
fairness-aware algorithms (Ganesh et al., 2023), including the instability of the covariance proxy (Friedler
et al., 2019). In this paper, in addition to empirically showing counter-examples, we both theoretically and
empirically underscore the significance of the surrogate-fairness gap and variance, which are fundamental
factors contributing to the two aforementioned problems, respectively. Our general sigmoid surrogate is
shown to deal with the large margin points to simultaneously reduce both the surrogate-fairness gap and the
variance. There is also fairness and stability upper bound for it, which is crucial in this field (Gallegos et al.,
2023; Mehrabi et al., 2021; Caton & Haas, 2020). Additionally, in order to reduce the gap, we also devise a
balanced surrogate approach to further improve the fairness and stability of surrogate functions, which may
deserve a deeper exploration in this field for future work.
3 Preliminaries
3.1 Fairness-aware Classification
Note the general purpose of fairness-aware classification is to find a classifier with minimal accuracy loss
while satisfying certain fairness constraints. For simplicity, we set up the problem as the binary classification
task with only a binary-sensitive feature: with the training set S={(xi,yi)}N
i=1consisting of feature vectors
xi∈Rdand the corresponding class labels yi∈{0,1}, one needs to predict the labels of a test set. Let
dθ(x)denotes the signed distance between the feature vector xand the decision boundary parameterized by
θ. Given a point xiin the test set, a classifier will predict it as positive if dθ(xi)>0and zero if dθ(xi)≤0.
Among the features of x, there is one binary sensitive attribute z∈{− 1,+1}(e.g., sex, race, age).
As introduced in Section 1, a widely used fairness definition is called the demographic parity (DP) (Mehrabi
et al., 2021; Caton & Haas, 2020). It states that each protected class should receive the positive outcome at
equal rates, i.e.,
P(dθ(x)>0|z= +1) =P(dθ(x)>0|z=−1).
And further, the difference of demographic parity (DDP) metric (Lohaus et al., 2020) can be used to measure
the degree to which demographic parity is violated:
DDP =P(dθ(x)>0|z= +1)−P(dθ(x)>0|z=−1).
Then, with this metric, whether a classifier satisfies demographic parity can be determined by the condition
|DDP|≤ϵ, whereϵ≥0is a given threshold.
3.2 Surrogate Functions
We divide the training set into four classes according to the predicted labels and sensitive features:
N1a={(xi,yi)∈S|dθ(xi)>0,zi= +1},N1b={(xi,yi)∈S|dθ(xi)>0,zi=−1},
N0a={(xi,yi)∈S|dθ(xi)≤0,zi= +1},N0b={(xi,yi)∈S|dθ(xi)≤0,zi=−1},
whereN1a,N1b,N0a,N0bare sizes ofN1a,N1b,N0a,N0b, respectively. To consider DDP as fairness constraints
for optimization, the probability in it cannot be computed directly, so frequency is used to estimate them:
\DDPS=N1a
N1a+N0a−N1b
N1b+N0b(1)
=/summationtext
(x,y)∈N 1a∪N 0a1dθ(x)>0
N1a+N0a−/summationtext
(x,y)∈N 1b∪N 0b1dθ(x)>0
N1b+N0b,
where 1[·]:R→{0,1}is the indicator function that returns 1 if the condition is true and 0 otherwise.
4Published in Transactions on Machine Learning Research (04/2024)
In application, \DDPSusually serves as a substitute for DDPto judge the fairness of a classifier. However,
dueto 1dθ(x)>0,itisintractabletodirectlyincorporate \DDPSintoconstraintsforgradient-basedalgorithms.
So smooth surrogate function ϕ:R→Ris used to replace 1dθ(x)>0withϕ(dθ(x)):
^DDPS(ϕ) =/summationtext
(x,y)∈N 1a∪N 0aϕ(dθ(x))
N1a+N0a−/summationtext
(x,y)∈N 1b∪N 0bϕ(dθ(x))
N1b+N0b. (2)
Then^DDPS(ϕ)can be incorporated into constraints as/vextendsingle/vextendsingle/vextendsingle^DDPS(ϕ)/vextendsingle/vextendsingle/vextendsingle≤ϵ, whereϵis the threshold. In this
way, the fairness-aware classification problem becomes a feasible constrained optimization problem.
Onepopularfairnesssurrogatefunctionis covariance proxy (CP),whichisintroducedby(Zafaretal.,2017c).
Empirical study shows that CP can reflect the difference of demographic parity and can be incorporated as
constraints for fairness-aware classification problem (Ding et al., 2020; Zhao et al., 2020; Zhang et al., 2020).
We defined a general version of it as
/hatwidestCovS(ϕ) =1
NN/summationdisplay
i=1(zi−z)ϕ(dθ(xi)), (3)
wherezis the mean of zover the training set. If ϕ(x) =x, the equation (3) recovers the original definition
of CP in (Zafar et al., 2017c). Also, ^DDPS(ϕ)∝/hatwidestCovS(ϕ)and the proof can be found in Appendix A.1. It
means that the original CP is equivalent to the linear surrogate function ϕ(x) =x, which is also explained
in previous work (Lohaus et al., 2020; Bendekgey & Sudderth, 2021). We provide theoretical results for
^DDPS(ϕ)in the main paper, and extend them to /hatwidestCovS(ϕ)in Appendix A.2 with the same conclusions.
4 The Surrogate-fairness Gap
We first emphasize the importance of surrogate-fairness gap, and then point out two issues: instability
(Section 4.1) and large margin points (Section 4.2), which may influence the surrogate-fairness gap.
Firstly, we connect \DDPSand^DDPS(ϕ)together, and build the surrogate-fairness gap between them.
Proposition 1. Define the magnitude of the signed distance by Dθ(x), i.e.,Dθ(x) =|dθ(x)|. It satisfies:
\DDPS−^DDPS(ϕ)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
surrogate-fairness gap=/summationtext
(x,y)∈N 1a∪N 0a/bracketleftbig
1dθ(x)>0−ϕ(dθ(x))/bracketrightbig
N1a+N0a−/summationtext
(x,y)∈N 1b∪N 0b/bracketleftbig
1dθ(x)>0−ϕ(dθ(x))/bracketrightbig
N1b+N0b.(4)
There is a surrogate-fairness gap between \DDPSand^DDPS(ϕ). For^DDPS(ϕ), it can serve as a fairness
constraint or regularization in the algorithm. The algorithm will automatically find a solution that penalizes
large/vextendsingle/vextendsingle/vextendsingle^DDPS(ϕ)/vextendsingle/vextendsingle/vextendsingle. Unfortunately, it is different for the “gap”, which comes from the inherent difference be-
tween the indicator function and the fairness surrogate function. For the ideal case ϕ(x) = 1x>0, which means
thatϕ(dθ(x)) = 1dθ(x)>0, the gap is zero and reducing the constraint or regularization term/vextendsingle/vextendsingle/vextendsingle^DDPS(ϕ)/vextendsingle/vextendsingle/vextendsingle
is equivalent to reducing/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle. However, for any surrogate function ϕ, the gap will be inevitably intro-
duced unless ϕ(x) = 1x>0. When the surrogate-fairness gap is small enough, there is fairness guarantee for
the classifier. In practice, Figure 1(b) shows the surrogate-fairness gap of different fairness surrogate func-
tions, including CP (which is equivalent to linear surrogate function) (Zafar et al., 2017c), hinge (Wu et al.,
2019), log-sigmoid as well as sigmoid (Bendekgey & Sudderth, 2021), and our general sigmoid surrogate. It
suggests that unbounded surrogate functions tend to exhibit a larger surrogate-fairness gap. The bounded
surrogate functions, such as sigmoid and general sigmoid, both exhibit a bounded “gap”.
5Published in Transactions on Machine Learning Research (04/2024)
4.1 Instability
Thevarianceof ^DDPS(ϕ)isVar/parenleftig
^DDPS(ϕ)/parenrightig
=E/parenleftig
^DDPS(ϕ)/parenrightig2
−/bracketleftig
E/parenleftig
^DDPS(ϕ)/parenrightig/bracketrightig2
.Ifwechoosebounded
surrogateϕ(x)∈[0,1], then^DDPS(ϕ)∈[−1,1], which means that Var/parenleftig
^DDPS(ϕ)/parenrightig
∈[0,1]. Therefore,
there is an stability guarantee for ^DDPS(ϕ)ifϕ(x)∈[0,1]. However, if we choose unbounded surrogate
function (such as ϕ(x) =x∈[−∞,+∞]for the original CP), the resulting values of ϕ(x)are not constrained
within the range [0,1]. Therefore, we cannot conclude that ^DDPS(ϕ)∈[−1,1]. Consequently, we also cannot
conclude that Var/parenleftig
^DDPS(ϕ)/parenrightig
∈[0,1]. As a result, there is no longer a stability guarantee for ^DDPS(ϕ).
To summarize the aforementioned problems, incorporating ^DDPS(ϕ)into fairness regularization and con-
straints to indirectly minimize \DDPSmay encounter difficulties for two reasons. Firstly, due to the existence
of the surrogate-fairness gap, minimizing ^DDPS(ϕ)is not equivalent to minimizing \DDPS. Secondly, if un-
bounded surrogate functions are employed, the uncontrollable variance of ^DDPS(ϕ)makes it even more
challenging to be an appropriate estimator of \DDPS.
4.2 The Large Margin Points
In this section, we emphasize the trouble of large margin points, which may influence the surrogate-fairness
gap. In this paper, the points with too large Dθ(x)are called large margin points and others are normal
points. Unfortunately, we regret to assert that these large margin points may simultaneously worsen the
first two issues mentioned above. To illustrate, we take CP (linear surrogate ϕ(x) =x) as an example. For
three famous real-world data sets, Adult (Kohavi, 1996), COMPAS (Julia Angwin & Kirchner, 2016) and
Bank Marketing (S. Moro & Rita, 2014), we provide the boxplot of dθ(x)in the test set in Figure 2. The
experimental details are in Appendix B.2. There are three main observations in Figure 2: ( i) Most of the
points are near the decision boundary. ( ii) Over 5% points are large margin points for Adult and COMPAS.
(iii) Almost all the large margin points are predicted as positive class.
Firstly, for the surrogate-fairness gap problem, the gap in Equation (1) may be amplified in the presence of
such large margin points. For example, Figure 2 shows that most of the large margin points are predicted
positive, so there is not a tight bound for/summationtext
(x,y)∈N 1adθ(x)and/summationtext
(x,y)∈N 1bdθ(x). Also, Figure 2 suggests
that the points with negative prediction exhibit a relatively smaller |dθ(x)|comparing to those points with
positive prediction. Thus,/summationtext
(x,y)∈N 0adθ(x)and/summationtext
(x,y)∈N 0bdθ(x)are bounded (for instance, In Figure 2(b),
we have/vextendsingle/vextendsingle/vextendsingle/summationtext
(x,y)∈N 0adθ(x)/vextendsingle/vextendsingle/vextendsingle≤2N0a). Therefore, there is still not a tight bound for/summationtext
(x,y)∈N 1a∪N 0adθ(x)
and/summationtext
(x,y)∈N 1b∪N 0bdθ(x), which may lead to a large surrogate-fairness gap in Equation (1). Finally, if the
surrogate-fairness gap becomes large, constraining the fairness surrogate function is inconsistent with the
specific fairness definition, which may lead to unfair result.
Secondly, regarding the instability issue, while the majority of points are close to the decision boundary, a
small number of large margin points contribute to the increased variance of dθ(x), thereby influencing both
^DDPS(ϕ)andVar/parenleftig
^DDPS(ϕ)/parenrightig
. The presence of large margin points, along with the use of an unbounded
surrogatefunction,surpassestheconstrainton Var/parenleftig
^DDPS(ϕ)/parenrightig
andmayresultinunstablefairnessguidance
for the classifier. These analytical insights above will be further validated through our experiments.
5 Our Approach
We devise the general sigmoid surrogate function with fairness and stability guarantees in Section 5.1.
The theory suggests that addressing the large margin points issue and obtaining a more balanced dataset
contribute to a fairer classifier. Then we present our balanced surrogates in Section 5.2, which is a novel
iterative approach to reduce the “gap” and thus improving fairness.
6Published in Transactions on Machine Learning Research (04/2024)
(a) Adult.
/uni0000000e/uni00000014/uni0000000b/uni00000058/uni0000000c /uni00000010/uni00000014/uni0000000b/uni00000058/uni0000000c /uni0000000e/uni00000014/uni0000000b/uni00000046/uni0000000c /uni00000010/uni00000014/uni0000000b/uni00000046/uni0000000c
/uni00000033/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000015
/uni00000014
/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000036/uni0000004c/uni0000004a/uni00000051/uni00000048/uni00000047/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048
 (b) COMPAS.
/uni0000000e/uni00000014/uni0000000b/uni00000058/uni0000000c /uni00000010/uni00000014/uni0000000b/uni00000058/uni0000000c /uni0000000e/uni00000014/uni0000000b/uni00000046/uni0000000c /uni00000010/uni00000014/uni0000000b/uni00000046/uni0000000c
/uni00000033/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000018
/uni00000013/uni00000018/uni00000014/uni00000013/uni00000036/uni0000004c/uni0000004a/uni00000051/uni00000048/uni00000047/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048
 (c) Bank.
Figure 2: The boxplot for the unconstrained logistic classifier (u) and logistic classifier with fairness con-
straints using linear surrogate (c) in three datasets. +1and−1represent the predicted label. The red dashed
line means dθ(x) = 0. The orange line in the box is the median. The circles outside the box are large margin
points. The rates of large margin points are 7.61%, 5.12% and 0.82% respectively.
5.1 General Sigmoid Surrogates
We generalize sigmoid function as
G(x) =σ(wx), (5)
whereσ(x)is the sigmoid function and w > 0is the parameter. The general sigmoid surrogate is flexible
because of the adjustable w. Moreover, of paramount importance, it achieves a much lower surrogate-fairness
gap, making it more consistent with DP, which is shown in Figure 1(b). Additionally, it enjoys stability
guarantees as its values fall within the range [0,1], ensuring that Var/parenleftig
^DDPS(G)/parenrightig
∈[0,1].
5.1.1 Fairness Guarantees
The following Theorem 1 provides the upper bound of/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsinglewhenG(Dθ(x))is close to 1for all the points
under the ^DDPS(ϕ)fairness constraint.
Theorem 1. We assume that G(Dθ(x))∈[1−γ,1], whereγ >0.∀ϵ>0, if/vextendsingle/vextendsingle/vextendsingle^DDPS(G)/vextendsingle/vextendsingle/vextendsingle≤ϵ, then it holds:
/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle≤1
2ϵ+γ. (6)
The proof can be found in Appendix A.3. The first term is similar to that in (4). Now with the assumption
G(Dθ(x))∈[1−γ,1], the gap here can be limited to a small range of variation. If the general sigmoid
surrogates limit G(Dθ(x))to around 1so thatγis small enough, then there are fairness guarantees for the
classifier. In contrast, the gap for CP is influenced by the magnitude of Dθ(x)for every large margin point
and thus hard to be bounded.
Remark. In Appendix D.2, considering CP, we provide extensions of Theorem 1 to other five fairness
definitions: three kinds of disparate mistreatment (Theorem 5-10 in Appendix D.2) and balance for positive
(negative) class (Theorem 11-12 in Appendix D.4). In particular, CP is generalized to disparate mistreatment
(Zafar et al., 2017a), and we theoretically devise the form of CP to better meet with disparate mistreatment,
which is empirically validated in (Zafar et al., 2019).
However, in some cases, we do not need to guarantee that the assumption G(Dθ(x))∈[1−γ,1]holds for all
the points. So the theorem below relaxes the assumption by giving the upper bound of/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsinglewhen most
of the points satisfy the assumption in Theorem 1.
Theorem 2. We assume that kpoints satisfy G(Dθ(x))∈[0,1−γ]and others satisfy G(Dθ(x))∈[1−γ,1],
whereγ >0.∀ϵ>0, if/vextendsingle/vextendsingle/vextendsingle^DDPS(G)/vextendsingle/vextendsingle/vextendsingle≤ϵ, then it holds:
7Published in Transactions on Machine Learning Research (04/2024)
/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle≤1
2ϵ+γ+1
2/parenleftbigg1
N1a+N0a+1
N1b+N0b/parenrightbigg
k
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
relaxation factor. (7)
The proof can be found in Appendix A.4. Comparing Theorem 1 with Theorem 2, the relaxation of the
assumption produces an extra relaxation factor. According to Theorem 1, if wis large, then γcan be small
enough, thus leading to a classifier with fairness guarantee. But an arbitrarily too large wmakes training
more challenging because of the diminished gradient magnitude for general sigmoid surrogate. Theorem 2
tells us that we need not to assume that all points satisfy G(Dθ(x))∈[1−γ,1]. Few points violating the
assumption (a small k) can also be tolerated. So it supports the idea that we do not have to choose a large
wbecause a relatively small wcan also guarantee fairness.
5.1.2 Insights from the Theorems
Large Margin Points Issue. An obvious takeaway of Theorem 2 is that addressing the large margin
points issue makes klower and thus obtaining a tighter bound of/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle. While Bendekgey & Sudderth
(2021)also explores theinfluence ofoutliersonthemodelunderfairnessconstraints, their theoreticalanalysis
in primarily centers on loss degeneracy under fairness constraints, whereas our paper aims to establish upper
bounds concerning fairness.
Balanced Dataset. Another interesting take-away of Theorem 1-2 is that we need a balanced dataset
to obtain a fairer classifier. The approximately same number between two sensitive groups contributes to a
tighter/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingleupperbound.Firstofall,forTheorem1,thecoefficientof ϵsatisfiesN2
4(N1a+N0a)(N1b+N0b)≥1.
The equality holds if and only if N1a+N0a=N1b+N0b, which means that the two sensitive groups share
the equal size. Therefore, a more balanced dataset will obtain a tighter/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingleupper bound. Similarly,
comparing to Theorem 1, we discover that those kpoints in Theorem 2 relaxed the original bound by a
relaxation factor. The coefficient of the relaxation factor1
2/parenleftig
N
N1a+N0a+N
N1b+N0b/parenrightig
≥2, and the equality holds
if and only if N1a+N0a=N1b+N0b. So, if we use a balanced dataset, then N1a+N0aandN1b+N0b
are close to each other, thus making the fairness bounded tighter. The imbalance issue also applies to other
surrogates and one can balance the dataset in advance to achieve better fairness performance.
Notably, certain theoretical investigations illuminate the positive impact of a balanced dataset on fostering
fairness in machine learning. For example, the impossibility theorem (Bell et al., 2023) in fairness literature
states that, in the context of binary classification, equalizing some specific set of multiple common perfor-
mance metrics between protected classes is impossible, except in two special cases: a perfect predictor and
equalbase rate (Chouldechova, 2017; Kleinberg et al., 2017; Pleiss et al., 2017). Furthermore, the reduction
of variation in group base rates has been demonstrated to yield a diminished lower bound for separation
gap and independence gap (Liu et al., 2019). Moreover, minimizing the difference in base rates results in a
decreased lower bound for joint error across both sensitive groups (Zhao & Gordon, 2022). In contrast, our
Theorem 2 provides an elucidation from the perspective of upper bound . It implies that a more balanced
dataset results in a tighter upper bound on the violation of DP.
Remark. In Appendix A.6, Theorem 4 shows that Var/parenleftig
\DDPS/parenrightig
≤1
4/parenleftig
1
Na+1
Nb/parenrightig
. Therefore, it also high-
lights the advantage of a balanced dataset in reducing variance.
5.2 Balanced Surrogates
The naive fairness-aware training framework can be formulated as
minL(θ,x,y) +ρ·^DDPS(ϕ), (8)
whereL(θ,x,y) =1
N/summationtext
(x,y)∈Sℓ(θ,x,y)is the empirical loss over the training set, ℓis a convex loss function,
^DDPS(ϕ)is the fairness regularization, and ρ >0is the coefficient. Recall in Proposition 1 that whether
8Published in Transactions on Machine Learning Research (04/2024)
^DDPS(ϕ)is an appropriate estimation of \DDPSdepends on ϕ. And as stated in Proposition 1, the fairness
surrogate function ϕdirectly affects the surrogate-fairness gap. Thus, the existence of such “gap” motivates
us that it is still necessary to reduce “gap” to improve fairness.
Our balanced surrogates approach mitigates unfairness by treating different sensitive groups differently using
a parameter being updated during training. The key idea of the updating procedure is making the magnitude
of “gap” as small as possible . It is a general plug-and-play learning paradigm for training framework using
fairness surrogate functions like (8), which is validated in the experiments.
Specifically, we consider different surrogates for two sensitive groups, i.e.,
ϕ(dθ(xi)) =/braceleftbiggϕ1(dθ(xi)), zi= +1.
ϕ2(dθ(xi)), zi=−1.(9)
With (9), we rewrite ^DDPS(ϕ)as:
^DDPS(ϕ) =/summationtext
(x,y)∈N 1a∪N 0aϕ1(dθ(x))
N1a+N0a−/summationtext
(x,y)∈N 1b∪N 0bϕ2(dθ(x))
N1b+N0b, (10)
For simplicity, we assume
ϕ2(x) =λϕ1(x), (11)
whereλ≥0is thebalance factor to be updated to reduce the gap. Our objective is
\DDPS−^DDPS(ϕ) = 0, (12)
which is equivalent to a surrogate-fairness gap of zero. Plug equations (1), (10) and (11) into the equation
(12), we then solve for λ:
λ=(N1b+N0b)/summationtext
(x,y)∈N 1a∪N 0aϕ1(dθ(x))−(N1aN0b−N0aN1b)
(N1a+N0a)/summationtext
(x,y)∈N 1b∪N 0bϕ1(dθ(x))(13)
In this way, we can first specify ϕ1in (9), initiate λasλ0, and train an unconstrained classifier with
the produced θ0as the start point of the iteration. Then we iteratively solve (8) and compute (13) until
convergence. In order to avoid oscillation and accelerate the convergence, we use exponential smoothing for
λ:
λt=/braceleftbiggλ0, t = 0.
αλ′
t+ (1−α)λt−1, t= 1,2,···,N.(14)
Whereλ′
tcomes from (13) after titerations,λtis the result of λ′
tafter exponential smoothing and 0≤α≤1
is the smoothing factor. Notice that λ≤0is meaningless, so when this happens, we abandon this algorithm
and setλtto1, which recovers (8). When the difference of λtbetween two successive iterations is less than
a termination threshold η, the algorithm is over. If we choose the smoothing factor αand the termination
thresholdηproperly, then the loop will terminate after a few runs. The algorithmic representation of the
balanced surrogates can be found in Appendix B.1.
6 Experiments
6.1 Experimental Setup
Dataset. We use three real-world datasets: Adult (Kohavi, 1996), Bank Marketing (S. Moro & Rita, 2014)
and COMPAS (Julia Angwin & Kirchner, 2016), which are commonly used in fair machine learning (Mehrabi
et al., 2021).
•Adult. The Adult dataset contains 48842 instances and 14 attributes. The goal is predicting whether
the income for a person is more than $50,000 a year. We consider sex as the sensitive feature with
values male and female.
9Published in Transactions on Machine Learning Research (04/2024)
/uni00000013/uni00000011/uni0000001a/uni0000001b /uni00000013/uni00000011/uni0000001a/uni0000001c /uni00000013/uni00000011/uni0000001b/uni00000013 /uni00000013/uni00000011/uni0000001b/uni00000014 /uni00000013/uni00000011/uni0000001b/uni00000015 /uni00000013/uni00000011/uni0000001b/uni00000016 /uni00000013/uni00000011/uni0000001b/uni00000017
/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000013/uni00000019/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000013/uni00000011/uni00000013/uni0000001b/uni00000029/uni00000044/uni0000004c/uni00000055/uni00000051/uni00000048/uni00000056/uni00000056/uni0000002f/uni00000052/uni0000004a/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055
/uni0000002b/uni0000004c/uni00000051/uni0000004a/uni00000048
/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
(a) Adult.
/uni00000013/uni00000011/uni0000001b/uni00000017 /uni00000013/uni00000011/uni0000001b/uni00000019 /uni00000013/uni00000011/uni0000001b/uni0000001b /uni00000013/uni00000011/uni0000001c/uni00000013 /uni00000013/uni00000011/uni0000001c/uni00000015 /uni00000013/uni00000011/uni0000001c/uni00000017
/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000013/uni00000019/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000029/uni00000044/uni0000004c/uni00000055/uni00000051/uni00000048/uni00000056/uni00000056/uni0000002f/uni00000052/uni0000004a/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055
/uni0000002b/uni0000004c/uni00000051/uni0000004a/uni00000048
/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047 (b) Bank.
/uni00000013/uni00000011/uni00000018/uni0000001c /uni00000013/uni00000011/uni00000019/uni00000013 /uni00000013/uni00000011/uni00000019/uni00000014 /uni00000013/uni00000011/uni00000019/uni00000015 /uni00000013/uni00000011/uni00000019/uni00000016 /uni00000013/uni00000011/uni00000019/uni00000017 /uni00000013/uni00000011/uni00000019/uni00000018 /uni00000013/uni00000011/uni00000019/uni00000019
/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000013/uni00000011/uni00000013/uni00000015/uni00000018/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000013/uni00000011/uni00000013/uni00000016/uni00000018/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000013/uni00000011/uni00000013/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000019/uni00000013/uni00000029/uni00000044/uni0000004c/uni00000055/uni00000051/uni00000048/uni00000056/uni00000056/uni0000002f/uni00000052/uni0000004a/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055
/uni0000002b/uni0000004c/uni00000051/uni0000004a/uni00000048
/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047 (c) COMPAS.
Figure 3: Results of different surrogate functions.
/uni00000013/uni00000011/uni0000001a/uni0000001b/uni00000018 /uni00000013/uni00000011/uni0000001a/uni0000001c/uni00000013 /uni00000013/uni00000011/uni0000001a/uni0000001c/uni00000018 /uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013 /uni00000013/uni00000011/uni0000001b/uni00000013/uni00000018 /uni00000013/uni00000011/uni0000001b/uni00000014/uni00000013 /uni00000013/uni00000011/uni0000001b/uni00000014/uni00000018
/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000013/uni00000011/uni00000013/uni00000016/uni00000018/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000013/uni00000011/uni00000013/uni00000018/uni00000018/uni00000029/uni00000044/uni0000004c/uni00000055/uni00000051/uni00000048/uni00000056/uni00000056/uni0000002f/uni00000052/uni0000004a/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055
/uni0000002b/uni0000004c/uni00000051/uni0000004a/uni00000048
/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
(a) Adult.
/uni00000013/uni00000011/uni0000001b/uni0000001a /uni00000013/uni00000011/uni0000001b/uni0000001b /uni00000013/uni00000011/uni0000001b/uni0000001c /uni00000013/uni00000011/uni0000001c/uni00000013
/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000014/uni00000018/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000013/uni00000011/uni00000013/uni00000015/uni00000018/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000013/uni00000011/uni00000013/uni00000016/uni00000018/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000029/uni00000044/uni0000004c/uni00000055/uni00000051/uni00000048/uni00000056/uni00000056/uni0000002f/uni00000052/uni0000004a/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055
/uni0000002b/uni0000004c/uni00000051/uni0000004a/uni00000048
/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047 (b) Bank.
/uni00000013/uni00000011/uni00000019/uni00000014/uni00000013 /uni00000013/uni00000011/uni00000019/uni00000014/uni00000018 /uni00000013/uni00000011/uni00000019/uni00000015/uni00000013 /uni00000013/uni00000011/uni00000019/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000019/uni00000016/uni00000013 /uni00000013/uni00000011/uni00000019/uni00000016/uni00000018 /uni00000013/uni00000011/uni00000019/uni00000017/uni00000013
/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000013/uni00000011/uni00000013/uni00000016/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000017/uni00000013/uni00000011/uni00000013/uni00000016/uni00000019/uni00000013/uni00000011/uni00000013/uni00000016/uni0000001b/uni00000029/uni00000044/uni0000004c/uni00000055/uni00000051/uni00000048/uni00000056/uni00000056/uni0000002f/uni00000052/uni0000004a/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055
/uni0000002b/uni0000004c/uni00000051/uni0000004a/uni00000048
/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047 (c) COMPAS.
Figure 4: Results of applying balanced surrogate to different surrogate functions. The arrow starts with
the result of a surrogate and ends with the result of the same surrogate function using balanced surrogate
method.
•Bank. The Bank Marketing dataset contains 41188 instances and 20 input features. The goal is
predicting whether the client will subscribe a term deposit. We follow (Zafar et al., 2017c) and
consider age as the binary sensitive attribute, which is discretized into the case whether the client’s
age is between 25 and 60 years.
•COMPAS . The COMPAS dataset was compiled to investigate racial bias in recidivism prediction.
The goal is predicting whether a criminal defendant will be a recidivist in two years. We use only
the subset of the data with sensitive attribute Caucasian or African-American.
Baseline. In addition to an unconstrained logistic regression classifier (denoted as ‘Unconstrained’), we
compare our general sigmoid surrogate (denoted as ‘General Sigmoid’) with other four surrogate functions
below, which have also appeared in Figure 1.
•Linear surrogate (equivalent to CP) ϕ(x) =x(Zafar et al., 2017c) (denoted as ‘Linear’).
•Hinge-like surrogate ϕ(x) = max (x+ 1,0)(Wu et al., 2019) (denoted as ‘Hinge’).
•Sigmoid surrogate ϕ(x) =σ(x)(Bendekgey & Sudderth, 2021) (denoted as ‘Sigmoid’).
•Log-sigmoid surrogate ϕ(x) =−logσ(−x)(Bendekgey & Sudderth, 2021) (denoted as ‘Log-
Sigmoid’).
6.2 Learning Fair Classifiers
We conduct two main experiments. One is the comparison among general sigmoid surrogate and other
surrogate functions on classification tasks. The other is validating the effect of balanced surrogates method
10Published in Transactions on Machine Learning Research (04/2024)
by applying it to different surrogate functions. Following Bendekgey & Sudderth (2021), a linear classifier is
used as the base classifier. The dataset is randomly divided into training set (70%), validation set (5%) and
test set (25%). The parameter setting is discribed in Appendix B.3. We report two metrics on the test set:/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle(lower is better) and accuracy (higher is better), and standard deviation is shown for the metrics.
6.3 Main Results
The results are in Figures 3-4. Refer to Appendix B.5 for specific numerical results of all three datasets.
GeneralSigmoidSurrogate. InFigure3,ontheonehand,weobservethatthegeneralsigmoidsurrogate
achievesbetterfairnessthanunboundedsurrogatefunctions(Logsigmoid,LinearandHinge),whichindicates
that the general sigmoid surrogate does effectively shorten the surrogate-fairness gap and therefore improve
fairness. On the other hand, comparing to the sigmoid surrogate function, our proposed surrogate not only
further reduces the gap, but it is also more flexible due to the parameter w. Moreover, it is intriguing to
note that the variance of fairness and accuracy of general sigmoid surrogate is also comparatively smaller
than other surrogate functions, demonstrating its superiority in terms of stability. It offers a simple solution
to the long-standing high variance issue observed in existing fairness-aware algorithms (Friedler et al., 2019;
Ganeshetal.,2023).WeprovidesometheoreticalanalysisonvariancetoAppendixA.6.Exploringautomated
methods to search for a suitable parameter wfor improved fairness performance while reducing variance
presents an intriguing avenue for future research.
Balanced Surrogates Method. In Figure 4, we observe that balanced surrogates method succeeds
in improving fairness of unbounded surrogate functions but sometimes slightly compromising fairness of
bounded surrogate functions. For the reason of this phenomenon, fairness-aware algorithms aim to reduce/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle. Firstly, fairness regularization aims at lowering/vextendsingle/vextendsingle/vextendsingle^DDPS(ϕ)/vextendsingle/vextendsingle/vextendsingle. Secondly, the key idea of balanced
surrogates is to reduce the magnitude of “gap”/vextendsingle/vextendsingle/vextendsingle\DDPS−^DDPS(ϕ)/vextendsingle/vextendsingle/vextendsinglethus indirectly lower/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle(because/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle≤/vextendsingle/vextendsingle/vextendsingle^DDPS(ϕ)/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle\DDPS−^DDPS(ϕ)/vextendsingle/vextendsingle/vextendsingle). However, an infinitesimal/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingleis not always better. In
Appendix A.5, we show in Theorem 3 that there is still a discrepancy between \DDPSandDDP, indicating
that a small enough/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingleis not equivalent to a small |DDP|. When the “gap” is large (such as the un-
bounded surrogate functions), balanced surrogates method can effectively reduce “gap” and achieves a fairer
result. But when the “gap” is limited (such as sigmoid and general sigmoid), an infinitesimal magnitude of
“gap”maysometimesunderminefairnessinstead.Interestingly,similartothestabilityenhancementobserved
in general sigmoid surrogate, the numerical results in Appendix B.5 also show that our balanced surrogates
method attains smaller variance compared to other surrogate functions. Incorporating mechanisms such as
the balanced surrogate method into existing fairness-aware algorithms to enhance both fairness and stability
is also a promising direction.
In Appendix B.6, we also compare our methods with other two in-processing methods: reduction (Agar-
wal et al., 2018) and adaptive sensitive reweighting (Krasanakis et al., 2018). The promising results also
demonstrate the superiority of our methods.
6.4 Experimental Verification for the Theoretical Insights
Large Margin Points Issue. The boxplot with our general sigmoid surrogate is shown in Figure 8 in
Appendix B.7. Recall that with an unbounded surrogate function ϕ, the large margin points will influence the
“gap”. Now they have a minor impact on the “gap” mentioned before because they are bounded ( G(Dθ(x))≤
1). Furthermore, the two edges almost overlap, indicating that the variation of Dθ(x)is small, contributing
to more stable results. Overall, the general sigmoid surrogate successfully deals with the large margin points,
mitigating both the surrogate-fairness gap and instability simultaneously.
BalancedDataset. Fromtheperspectiveofsensitiveattribute,theAdultdatasetisan imbalanced dataset:
there are 32650 male instances and 16192 female instances. The ratio of the two groups is approximately
11Published in Transactions on Machine Learning Research (04/2024)
/uni00000013/uni00000011/uni0000001a/uni00000019 /uni00000013/uni00000011/uni0000001a/uni0000001a /uni00000013/uni00000011/uni0000001a/uni0000001b /uni00000013/uni00000011/uni0000001a/uni0000001c /uni00000013/uni00000011/uni0000001b/uni00000013 /uni00000013/uni00000011/uni0000001b/uni00000014
/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000013/uni00000019/uni00000029/uni00000044/uni0000004c/uni00000055/uni00000051/uni00000048/uni00000056/uni00000056/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055
/uni00000027/uni00000052/uni0000005a/uni00000051/uni0000001d/uni00000003/uni0000002a
/uni00000027/uni00000052/uni0000005a/uni00000051/uni0000001d/uni00000003/uni0000002f
/uni00000038/uni00000053/uni0000001d/uni00000003/uni0000002a
/uni00000038/uni00000053/uni0000001d/uni00000003/uni0000002f
(a) Adult.
/uni00000013/uni00000011/uni0000001b/uni00000017 /uni00000013/uni00000011/uni0000001b/uni00000019 /uni00000013/uni00000011/uni0000001b/uni0000001b /uni00000013/uni00000011/uni0000001c/uni00000013 /uni00000013/uni00000011/uni0000001c/uni00000015
/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000014/uni00000018/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000013/uni00000011/uni00000013/uni00000015/uni00000018/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000013/uni00000011/uni00000013/uni00000016/uni00000018/uni00000029/uni00000044/uni0000004c/uni00000055/uni00000051/uni00000048/uni00000056/uni00000056/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055
/uni00000027/uni00000052/uni0000005a/uni00000051/uni0000001d/uni00000003/uni0000002a
/uni00000027/uni00000052/uni0000005a/uni00000051/uni0000001d/uni00000003/uni0000002f
/uni00000038/uni00000053/uni0000001d/uni00000003/uni0000002a
/uni00000038/uni00000053/uni0000001d/uni00000003/uni0000002f (b) Bank.
/uni00000013/uni00000011/uni00000019/uni00000013 /uni00000013/uni00000011/uni00000019/uni00000014 /uni00000013/uni00000011/uni00000019/uni00000015 /uni00000013/uni00000011/uni00000019/uni00000016 /uni00000013/uni00000011/uni00000019/uni00000017 /uni00000013/uni00000011/uni00000019/uni00000018
/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000013/uni00000019/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000029/uni00000044/uni0000004c/uni00000055/uni00000051/uni00000048/uni00000056/uni00000056/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000004a/uni00000050/uni00000052/uni0000004c/uni00000047
/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055
/uni00000027/uni00000052/uni0000005a/uni00000051/uni0000001d/uni00000003/uni0000002a
/uni00000027/uni00000052/uni0000005a/uni00000051/uni0000001d/uni00000003/uni0000002f
/uni00000038/uni00000053/uni0000001d/uni00000003/uni0000002a
/uni00000038/uni00000053/uni0000001d/uni00000003/uni0000002f (c) COMPAS.
Figure 5: The result of balanced dataset. “G” and “L” indicates general sigmoid surrogate and linear surro-
gate, respectively. “Up” and “Down” correspond to upsampling and downsampling, respectively.
2:1. The Bank Marketing and COMPAS datasets also suffer from the imbalance issue. The ratio of the two
groups are 39210:1978 (about 20:1) and 3175:2103 (about 3:2), respectively. Such imbalanced datasets lead
to a loose bound in Theorem 2. We randomly split the dataset into training set (70%) and test set (30%).
According to the number of minority group, we conduct two experiments: Downsampling and Upsampling,
which means randomly downsampling (upsampling) the majority (minority) group in the original training
set and form the new training set to make two demographic groups more balanced. Refer to Appendix B.4
for the details of our sampling schemes. The test set is partitioned in advance so that it is still imbalanced.
We choose an unbounded surrogate function: Linear, and our bounded surrogate: general sigmoid.
The results in Figure 5 show that downsampling the majority group and upsampling the minority group
contribute to a balanced dataset and a fairer result. However, in our experiments here, downsampling will
lead to reduction of the training set, and upsampling will lead to replication of the training set, which may
cause underfitting and overfitting problems, respectively. So the accuracy sometimes decreases. In conclusion,
before fairness-aware training, we suggest using fair data augmentation strategies to obtain a balanced
dataset, such as Fair Mixup (Chuang & Mroueh, 2021), and algorithms designed to address data imbalance,
such as SMOTE (Chawla et al., 2002).
7 Conclusion
In this paper, we research on surrogate functions in algorithmic fairness. We derive the surrogate-fairness gap
to indicate the difference between fairness surrogate function and . With boxplots, we find that unbounded
surrogates are especially faced with the large margin points issue, which further amplify the “gap” and
instability. To address these challenges, we propose general sigmoid surrogate with theoretically validated
fairness and stability guarantees to deal with large margin points. The theoretical analysis further provides
insights to the community that dealing with the large margin points issue as well as obtaining a more
balanced dataset contribute to a fairer and more stable classifier. We further elaborate balanced surrogates
method, which is an iterative algorithm to reduce the gap during training. It is also applicable to other
fairness surrogate functions. Finally, our experiments using three real-world datasets not only validate the
insights of our theorems, but also show that our methods get better fairness and stability performance.
8 Broader Impact and Ethics Statement
This study concentrates on better understanding the fairness surrogate functions in machine learning. Im-
portantly, if someone claims the fairness guarantee of using unbounded fairness surrogate functions, it is
worthy of suspicion and further investigation because of the surrogate-fairness gap issue discussed in this
paper. Furthermore, the motivation of our general sigmoid surrogate and balanced surrogate methods are
both centered on improving the fairness performance.
12Published in Transactions on Machine Learning Research (04/2024)
We acknowledge the sensitive nature of our study and guarantee adherence to all applicable legal and ethical
standards. Our research is conducted within a safe and controlled setting to protect real-world systems’
security. Only researchers who have received the appropriate clearance can access the most confidential
parts of our experiments. Such measures are implemented to preserve the integrity of our research and to
reduce any potential risks associated with the experiments.
Acknowledgement
We sincerely appreciate the anonymous reviewers for their helpful suggestions and constructive comments.
WY, ZCL and YL were supported by the National Natural Science Foundation of China (NO.62076234); the
Beijing Natural Science Foundation (NO.4222029); the Intelligent Social Governance Interdisciplinary Plat-
form,MajorInnovation&PlanningInterdisciplinaryPlatformforthe“DoubleFirstClass”Initiative,Renmin
University of China; the Beijing Outstanding Young Scientist Program (NO.BJJWZYJH012019100020098);
the Public Computing Cloud, Renmin University of China; the Fundamental Research Funds for the Cen-
tral Universities, and the Research Funds of Renmin University of China (NO.2021030199); the Huawei-
Renmin University joint program on Information Retrieval; the Unicom Innovation Ecological Cooperation
Plan; the CCF Huawei Populus Grove Fund; and the National Key Research and Development Project
(NO.2022YFB2703102). ZKZ and BH were supported by the NSFC General Program No. 62376235, Guang-
dong Basic and Applied Basic Research Foundation Nos. 2022A1515011652 and 2024A1515012399.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo
Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv
preprint arXiv:2303.08774 , 2023.
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions
approach to fair classification. In Proceedings of the 35th International Conference on Machine Learning ,
2018.
Solon Barocas and Andrew D Selbst. Big data’s disparate impact. California law review , pp. 671–732, 2016.
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning . fairmlbook.org, 2019.
http://www.fairmlbook.org.
Yahav Bechavod and Katrina Ligett. Penalizing unfairness in binary classification. arXiv preprint
arXiv:1707.00044 , 2017.
Andrew Bell, Lucius Bynum, Nazarii Drushchak, Tetiana Zakharchenko, Lucas Rosenblatt, and Julia Stoy-
anovich. The possibility of fairness: Revisiting the impossibility theorem in practice. In Proceedings of the
2023 ACM Conference on Fairness, Accountability, and Transparency , pp. 400–422, 2023.
Harry Bendekgey and Erik B. Sudderth. Scalable and stable surrogates for flexible classifiers with fairness
constraints. In Advances in Neural Information Processing Systems , 2021.
Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth
Neel, and Aaron Roth. A convex framework for fair regression. In Fairness, Accountability, and Trans-
parency in Machine Learning (FATML) , 2017.
Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. Understanding the
origins of bias in word embeddings. In International conference on machine learning , pp. 803–811, 2019.
Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. Building classifiers with independency constraints.
In2009 IEEE International Conference on Data Mining Workshops , 2009.
Flavio P. Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R.
Varshney. Optimized pre-processing for discrimination prevention. In Advances in Neural Information
Processing Systems , 2017.
13Published in Transactions on Machine Learning Research (04/2024)
Simon Caton and Christian Haas. Fairness in machine learning: A survey. arXiv preprint arXiv:2010.04053 ,
2020.
Junyi Chai and Xiaoqian Wang. Fairness with adaptive weights. In International Conference on Machine
Learning , pp. 2853–2866, 2022.
Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority
over-sampling technique. Journal of artificial intelligence research , 16:321–357, 2002.
Irene Chen, Fredrik D Johansson, and David Sontag. Why is my classifier discriminatory? Advances in
neural information processing systems , 31, 2018.
Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction
instruments. Big data, 5(2):153–163, 2017.
Ching-Yao Chuang and Youssef Mroueh. Fair mixup: Fairness via interpolation. In Proceedings of the
International Conference on Machine Learning , 2021.
André F Cruz, Pedro Saleiro, Catarina Belém, Carlos Soares, and Pedro Bizarro. Promoting fairness through
hyperparameter optimization. In 2021 IEEE International Conference on Data Mining (ICDM) , pp. 1036–
1041, 2021.
Jiahao Ding, Xinyue Zhang, Xiaohuan Li, Junyi Wang, Rong Yu, and Miao Pan. Differentially private and
fair classification via calibrated functional mechanism. Proceedings of the AAAI Conference on Artificial
Intelligence , 2020.
Michele Donini, Luca Oneto, Shai Ben-David, John S Shawe-Taylor, and Massimiliano Pontil. Empirical risk
minimization under fairness constraints. In Advances in Neural Information Processing Systems , 2018.
SanghamitraDutta,DennisWei,HazarYueksel,Pin-YuChen,SijiaLiu,andKushVarshney. Isthereatrade-
off between fairness and accuracy? a perspective using mismatched hypothesis testing. In International
conference on machine learning , pp. 2803–2813. PMLR, 2020.
Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian.
Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining , 2015.
Benjamin Fish, Jeremy Kun, and Ádám D. Lelkes. A confidence-based approach for balancing fairness and
accuracy. In Proceedings of the 2016 SIAM International Conference on Data Mining , 2016.
Sorelle A Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam Choudhary, Evan P Hamilton,
and Derek Roth. A comparative study of fairness-enhancing interventions in machine learning. In Pro-
ceedings of the conference on fairness, accountability, and transparency , pp. 329–338, 2019.
Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt,
Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large language models: A survey.
arXiv:2309.00770 , 2023.
Prakhar Ganesh, Hongyan Chang, Martin Strobel, and Reza Shokri. On the impact of machine learning
randomness on group fairness. In Proceedings of the 2023 ACM Conference on Fairness, Accountability,
and Transparency , pp. 1789–1800, 2023.
Gabriel Goh, Andrew Cotter, Maya Gupta, and Michael P Friedlander. Satisfying real-world goals with
dataset constraints. In Advances in Neural Information Processing Systems , 2016.
Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. In Proceedings
of the 30th International Conference on Neural Information Processing Systems , 2016.
Max Hort, Zhenpeng Chen, Jie M Zhang, Federica Sarro, and Mark Harman. Bia mitigation for machine
learning classifiers: A comprehensive survey. arXiv preprint arXiv:2207.07068 , 2022.
14Published in Transactions on Machine Learning Research (04/2024)
Surya Mattu Julia Angwin, Jeff Larson and Lauren Kirchner. Machine bias: There’s software used across
the country to predict future criminals. and it’s biased against blacks. propublica, 2016. URL https:
//www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.
Faisal Kamiran and Toon Calders. Data pre-processing techniques for classification without discrimination.
Knowledge and Information Systems , 2011.
Faisal Kamiran, Asim Karim, and Xiangliang Zhang. Decision theory for discrimination-aware classification.
In2012 IEEE 12th International Conference on Data Mining , 2012.
Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-aware classifier with prej-
udice remover regularizer. In Proceedings of the 2012th European Conference on Machine Learning and
Knowledge Discovery in Databases - Volume Part II , 2012.
Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer,
UrsGasser,GeorgGroh,StephanGünnemann,EykeHüllermeier,etal. Chatgptforgood?onopportunities
and challenges of large language models for education. Learning and individual differences , 103:102274,
2023.
Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination
of risk scores. In Innovations in Theoretical Computer Science (ITCS) , 2017.
Ron Kohavi. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. In Proceedings of the
Second International Conference on Knowledge Discovery and Data Mining , 1996.
Emmanouil Krasanakis, Eleftherios Spyromitros-Xioufis, Symeon Papadopoulos, and Yiannis Kompatsiaris.
Adaptive sensitive reweighting to mitigate bias in fairness-aware classification. In Proceedings of the 2018
world wide web conference , pp. 853–862, 2018.
Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed Chi.
Fairness without demographics through adversarially reweighted learning. Advances in neural information
processing systems , 33:728–740, 2020.
Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. A survey on fairness in large language models.
arXiv preprint arXiv:2308.10149 , 2023.
ZacharyLipton,JulianMcAuley,andAlexandraChouldechova. Doesmitigatingml’simpactdisparityrequire
treatment disparity? Advances in neural information processing systems , 31, 2018.
Lydia T Liu, Max Simchowitz, and Moritz Hardt. The implicit fairness criterion of unconstrained learning.
InInternational Conference on Machine Learning , pp. 4051–4060, 2019.
Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov,
Muhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large
language models’ alignment. arXiv preprint arXiv:2308.05374 , 2023a.
Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li,
Mengshen He, Zhengliang Liu, et al. Summary of chatgpt-related research and perspective towards the
future of large language models. Meta-Radiology , 2023b.
Michael Lohaus, Michael Perrot, and Ulrike Von Luxburg. Too relaxed to be fair. In Proceedings of the 37th
International Conference on Machine Learning , 2020.
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and trans-
ferable representations. In International Conference on Machine Learning , pp. 3384–3393, 2018.
GauravMaheshwariandMichaëlPerrot. Fairgrad:Fairnessawaregradientdescent. Transactions on Machine
Learning Research , 2023.
15Published in Transactions on Machine Learning Research (04/2024)
VienVMaiandMikaelJohansson. Stabilityandconvergenceofstochasticgradientclipping:Beyondlipschitz
continuity and smoothness. In International Conference on Machine Learning , pp. 7325–7335, 2021.
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on
bias and fairness in machine learning. ACM Computing Surveys , 54:1–35, 2021.
RaghavMehta,ChangjianShui,andTalArbel. Evaluatingthefairnessofdeeplearninguncertaintyestimates
in medical image analysis. In Medical Imaging with Deep Learning , pp. 1453–1492. PMLR, 2024.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning . MIT press,
2018.
Kirtan Padh, Diego Antognini, Emma Lejal-Glaude, Boi Faltings, and Claudiu Musat. Addressing fairness
in classification with a model-agnostic multi-objective algorithm. In Proceedings of the Thirty-Seventh
Conference on Uncertainty in Artificial Intelligence , 2021.
Dana Pessach and Erez Shmueli. Algorithmic fairness. Machine Learning for Data Science Handbook: Data
Mining and Knowledge Discovery Handbook , pp. 867–886, 2023.
Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness and cali-
bration. In Advances in Neural Information Processing Systems , pp. 5684–5693, 2017.
Sandro Radovanović, Boris Delibasić, and Milija Suknović. Do we reach desired disparate impact with
in-processing fairness techniques? Procedia Computer Science , 214:257–264, 2022.
Yuji Roh, Kangwook Lee, Steven Whang, and Changho Suh. Sample selection for fair and robust training.
Advances in Neural Information Processing Systems , 34:815–827, 2021a.
Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh. Fairbatch: Batch selection for model
fairness. In International Conference on Learning Representations , 2021b.
P. Cortez S. Moro and P. Rita. A data-driven approach to predict the success of bank telemarketing, 2014.
URL https://archive.ics.uci.edu/ml/datasets/bank+marketing.
Changjian Shui, Qi Chen, Jiaqi Li, Boyu Wang, and Christian Gagné. Fair representation learning through
implicit path alignment. In Proceedings of the 39th International Conference on Machine Learning , volume
162, pp. 20156–20175, 2022a.
ChangjianShui,GezhengXu,QiChen,JiaqiLi,CharlesXLing,TalArbel,BoyuWang,andChristianGagné.
On learning fairness and accuracy on multiple subgroups. Advances in Neural Information Processing
Systems, 35:34121–34135, 2022b.
Changjian Shui, Justin Szeto, Raghav Mehta, Douglas L Arnold, and Tal Arbel. Mitigating calibration
bias without fixed attribute grouping for improved fairness in medical imaging analysis. In International
Conference on Medical Image Computing and Computer-Assisted Intervention , pp. 189–198. Springer,
2023.
Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan,
and Daniel Shu Wei Ting. Large language models in medicine. Nature medicine , 29(8):1930–1940, 2023.
SaeidTizpaz-Niari,AshishKumar,GangTan,andAshutoshTrivedi. Fairness-awareconfigurationofmachine
learning libraries. In Proceedings of the 44th International Conference on Software Engineering , pp. 909–
920, 2022.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-
lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned
chat models. arXiv preprint arXiv:2307.09288 , 2023.
Mingyang Wan, Daochen Zha, Ninghao Liu, and Na Zou. In-processing modeling techniques for machine
learning fairness: A survey. ACM Transactions on Knowledge Discovery from Data , 17(3):1–27, 2023.
16Published in Transactions on Machine Learning Research (04/2024)
Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi
Xiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: A comprehensive assessment of trustworthiness
in gpt models. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and
Benchmarks Track , 2023.
Yongkai Wu, Lu Zhang, and Xintao Wu. On convexity and bounds of fairness-aware classification. In The
World Wide Web Conference , 2019.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi. Fairness beyond
disparate treatment &disparate impact. Proceedings of the 26th International Conference on World Wide
Web, 2017a.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna P. Gummadi, and Adrian Weller.
Fromparitytopreference-basednotionsoffairnessinclassification. In Proceedings of the 31st International
Conference on Neural Information Processing Systems , 2017b.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P. Gummadi. Fairness con-
straints:Mechanismsforfairclassification. In Proceedings of the 20th International Conference on Artificial
Intelligence and Statistics , 2017c.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness con-
straints: A flexible approach for fair classification. Journal of Machine Learning Research , 2019.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In
Proceedings of the 30th International Conference on Machine Learning , 2013.
Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu.
Data-centric artificial intelligence: A survey. arXiv preprint arXiv:2303.10158 , 2023.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial
learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society , pp. 335–340,
2018.
Tao Zhang, Tianqing Zhu, Mengde Han, Jing Li, Wanlei Zhou, and Philip S. Yu. Fairness constraints in
semi-supervised learning. arXiv preprint arXiv:2009.06190 , 2020.
Chen Zhao, Changbin Li, Jincheng Li, and Feng Chen. Fair meta-learning for few-shot classification. In
2020 IEEE International Conference on Knowledge Graph (ICKG) , 2020.
Han Zhao and Geoffrey J Gordon. Inherent tradeoffs in learning fair representations. The Journal of Machine
Learning Research , 23(1):2527–2552, 2022.
17Published in Transactions on Machine Learning Research (04/2024)
Contents
1 Introduction 2
2 Related Work 3
3 Preliminaries 4
3.1 Fairness-aware Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.2 Surrogate Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
4 The Surrogate-fairness Gap 5
4.1 Instability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
4.2 The Large Margin Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
5 Our Approach 6
5.1 General Sigmoid Surrogates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
5.1.1 Fairness Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
5.1.2 Insights from the Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5.2 Balanced Surrogates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
6 Experiments 9
6.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
6.2 Learning Fair Classifiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
6.3 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
6.4 Experimental Verification for the Theoretical Insights . . . . . . . . . . . . . . . . . . . . . . 11
7 Conclusion 12
8 Broader Impact and Ethics Statement 12
A Proof for the Main Paper 20
A.1 Analysis about CP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.2 Extension from Fairness Surrogate Functions to CP . . . . . . . . . . . . . . . . . . . . . . . 21
A.3 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
A.4 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.5 Risk Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.6 Variance Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B Further Experimental Details and Results 28
B.1 The Balanced Surrogates Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
B.2 Details in Figure 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
18Published in Transactions on Machine Learning Research (04/2024)
B.3 The Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
B.4 Upsampling and Downsampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B.5 Numerical Results of Figure 3-4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B.6 Comparing with Other Fairness-aware Algorithms . . . . . . . . . . . . . . . . . . . . . . . . 30
B.7 Boxplots with General Sigmoid Surrogate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
C Other Related Work 31
C.1 Fairness-aware Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
C.2 Fairness Surrogate Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
D Extension to Other Fairness Definitions 33
D.1 Fairness Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
D.1.1 Disparate Mistreatment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
D.1.2 Balance for Positive (Negative) Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
D.2 Deeper Understanding of the Covariance Proxy for Disparate Mistreatment . . . . . . . . . . 33
D.3 Proof for Appendix D.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
D.3.1 Overall Accuracy Equality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
D.3.2 False Positive Error Rate Balance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
D.3.3 False Negative Error Rate Balance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
D.4 Deeper Understanding of the Covariance Proxy for Balance for Positive (Negative) Class . . . 42
D.5 Proof for Appendix D.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
D.5.1 Balance for Positive Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
D.5.2 Balance for Negative Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
E Further Analysis 45
E.1 Fairness Surrogate Functions in the Future . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
E.2 The Insights for Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
E.3 Biased Estimation of CP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
E.4 Proof of Section E.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
E.5 When are CP and General Sigmoid Surrogate Close to Each Other? . . . . . . . . . . . . . . 47
E.6 Proof of Theorem 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
19Published in Transactions on Machine Learning Research (04/2024)
Appendix
In the appendix, due to the length limit of every line,/summationtext
(x,y)∈N 1a∪N 0a1dθ(x)>0is denoted as/summationtext
N1a,N0a1dθ(x)>0for better presentation because it prevents endless line breaks.
We summarize the frequently used notations in Table 1:
Table 1: The most frequently used notations in this paper.
Notations Meanings
S the training set
N the size of the training set
x feature vector of a data point
y the corresponding label
z the corresponding sensitive attribute
θ the parameters of the classifier
dθ(x) the signed distance between the feature vector and the decision boundary
Dθ(x) the absolute value of dθ(x)
ϕ(·) the surrogate function
G(·) general sigmoid surrogate function
N1a the set of data points with positive prediction and positive sensitive attribute
N1a the size ofN1a
TP+the number of data points with positive prediction and positive sensitive attribute
DDP the difference of demographic parity
\DDPS an estimation of DDP
^DDPS(ϕ) the constraint/regularization term with fairness surrogate function ϕ
/hatwidestCovS(ϕ) the covariance proxy
U1a The number of points satisfying ϕ(Dθ(x))∈[0,1−γ]inN1a
ˆP(·) the predicted probability
A Proof for the Main Paper
A.1 Analysis about CP
In this subsection, we finish the proof of ^DDPS(ϕ)∝/hatwidestCovS(ϕ).
Now this is the proof for the claim that, when degenerating ϕ(x) =x(it is a linear function), we have
^DDPS(ϕ)∝/hatwidestCovS(ϕ). It is a step-by-step derivation of (3).
1
NN/summationdisplay
i=1(zi−z)dθ(xi)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
/hatwiderCovS(ϕ),whereϕ(x)=x.
20Published in Transactions on Machine Learning Research (04/2024)
=1
N
(1−z)/summationdisplay
N1a,N0adθ(xi) + (−1−z)/summationdisplay
N1b,N0bdθ(xi)

=1
N
/parenleftbigg
1−N1a+N0a−N1b−N0b
N/parenrightbigg/summationdisplay
N1a,N0adθ(xi) +/parenleftbigg
−1−N1a+N0a−N1b−N0b
N/parenrightbigg/summationdisplay
N1b,N0bdθ(xi)

=2
N2
(N1b+N0b)/summationdisplay
N1a,N0adθ(xi)−(N1a+N0a)/summationdisplay
N1b,N0bdθ(xi)

=2(N1a+N0a)(N1b+N0b)
N2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
constant≤1
2<1/parenleftbigg/summationtext
N1a,N0adθ(xi)
N1a+N0a−/summationtext
N1b,N0bdθ(xi)
N1b+N0b/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
^DDPS(ϕ),whereϕ(x)=x.. (15)
The proof is complete.
Furthermore, in fact, we have ^DDPS(ϕ)∝/hatwidestCovS(ϕ)holds for all ϕ. But for the original CP (Zafar et al.,
2017c),ϕ(x) =xis the default. So the original CP is equivalent to linear surrogate because ϕ(x) =x. Similar
proof is also explained in previous work (Lohaus et al., 2020; Bendekgey & Sudderth, 2021), where they call
it “linear relaxations”.
A.2 Extension from Fairness Surrogate Functions to CP
This subsection aims to restate the theoretical analysis about ^DDPS(ϕ)in Proposition 1 and Theorem 1-2
from the perspective of /hatwidestCovS(ϕ).
From the above derivations, we have
/hatwidestCovS(ϕ) =2(N1a+N0a)(N1b+N0b)
N2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
constant^DDPS(ϕ).
Therefore, the theoretical analysis about ^DDPS(ϕ)in Proposition 1 and Theorem 1-2 in the main paper
can be naturally extended to /hatwidestCovS(ϕ). Just by substituting the equation above into Proposition 1 and
Theorem 1-2, we restate them below:
Proposition 1 (Restatement) .Define the magnitude of the signed distance by Dθ(x), i.e.,Dθ(x) =|dθ(x)|.
Letϕ(x) =xandS1a=/summationtext
N1aϕ(Dθ(xi)),T1a=/summationtext
N1a(ϕ(Dθ(xi))−1)and it is similar for N1b,N0a,N0b. It
satisfies:
\DDPS=N2
2(N1a+N0a)(N1b+N0b)/hatwidestCovS(ϕ)−/parenleftbiggT1a−S0a
N1a+N0a−T1b−S0b
N1b+N0b/parenrightbigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
surrogate-fairness gap. (16)
Theorem 1 (Restatement) .We assume that G(Dθ(x))∈[1−γ,1], whereγ >0.∀ϵ>0, if/vextendsingle/vextendsingle/vextendsingle/hatwidestCov(G)/vextendsingle/vextendsingle/vextendsingle≤ϵ,
then it holds:/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle≤N2
4(N1a+N0a)(N1b+N0b)ϵ+γ.
Theorem 2 (Restatement) .We assume that kpoints satisfy G(Dθ(x))∈[0,1−γ]and others satisfy
G(Dθ(x))∈[1−γ,1], whereγ >0.∀ϵ>0, if/vextendsingle/vextendsingle/vextendsingle/hatwidestCov(G)/vextendsingle/vextendsingle/vextendsingle≤ϵ, then it holds:
/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle≤N2
4(N1a+N0a)(N1b+N0b)ϵ+γ+1
2/parenleftbigg1
N1a+N0a+1
N1b+N0b/parenrightbigg
k
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
relaxation factor.
21Published in Transactions on Machine Learning Research (04/2024)
The restatement of them does not affect the analysis in the main paper since ^DDPS(ϕ)∝/hatwidestCovS(ϕ).
A.3 Proof of Theorem 1
Proof sketch. Firstly, absolute value inequality is used to derive the upper bound of/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsinglegiven the
upper bound of/vextendsingle/vextendsingle/vextendsingle^DDPS(ϕ)/vextendsingle/vextendsingle/vextendsingle. Secondly, the assumption G(Dθ(x))∈[1−γ,1]is used to derive a tighter bound
of/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingleto complete the proof.
Proof.Ifϕpasses through the origin and increases monotonically, then according to the sign of dθ(x), we
have
^DDPS(ϕ) =/summationtext
N1a,N0aϕ(dθ(xi))
N1a+N0a−/summationtext
N1b,N0bϕ(dθ(xi))
N1b+N0b
=/summationtext
N1aϕ(Dθ(xi))−/summationtext
N0aϕ(Dθ(xi))
N1a+N0a−/summationtext
N1bϕ(Dθ(xi))−/summationtext
N0bϕ(Dθ(xi))
N1b+N0b.(17)
We defineT1a=/summationtext
N1aϕ(Dθ(xi))−N1a=/summationtext
N1a(ϕ(Dθ(xi))−1). It is similar for T1b,T0a,T0b. We now
analyze the term in (17) by limiting it with a parameter δ>0:
/vextendsingle/vextendsingle/vextendsingle^DDPS(ϕ)/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext
N1aϕ(Dθ(xi))−/summationtext
N0aϕ(Dθ(xi))
N1a+N0a−/summationtext
N1bϕ(Dθ(xi))−/summationtext
N0bϕ(Dθ(xi))
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤δ(18)
⇒/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN1a−N0a
N1a+N0a−N1b−N0b
N1b+N0b+N0a−N1a+/summationtext
N1aϕ(Dθ(xi))−/summationtext
N0aϕ(Dθ(xi))
N1a+N0a+
N1b−N0b−/summationtext
N1bϕ(Dθ(xi)) +/summationtext
N0bϕ(Dθ(xi))
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤δ
⇒/vextendsingle/vextendsingle/vextendsingle/vextendsingleN1a−N0a
N1a+N0a−N1b−N0b
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤δ+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN0a−N1a+/summationtext
N1aϕ(Dθ(xi))−/summationtext
N0aϕ(Dθ(xi))
N1a+N0a+
N1b−N0b−/summationtext
N1bϕ(Dθ(xi)) +/summationtext
N0bϕ(Dθ(xi))
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
⇒/vextendsingle/vextendsingle/vextendsingle/vextendsingleN1a−N0a
N1a+N0a−N1b−N0b
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤δ+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(/summationtext
N1aϕ(Dθ(xi))−N1a)−(/summationtext
N0aϕ(Dθ(xi))−N0a)
N1a+N0a+
(/summationtext
N0bϕ(Dθ(xi))−N0b)−(/summationtext
N1bϕ(Dθ(xi))−N1b)
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
⇒/vextendsingle/vextendsingle/vextendsingle/vextendsingleN1a−N 0a
N1a+N0a−N1b−N0b
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤δ+/vextendsingle/vextendsingle/vextendsingle/vextendsingleT1a−T0a
N1a+N0a+T0b−T1b
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle
⇒/vextendsingle/vextendsingle/vextendsingle/vextendsingleN1a
N1a+N0a−/parenleftbigg
1−N1a
N1a+N0a/parenrightbigg
−N1b
N1b+N0b+/parenleftbigg
1−N1b
N1b+N0b/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤δ+/vextendsingle/vextendsingle/vextendsingle/vextendsingleT1a−T0a
N1a+N0a+T0b−T1b
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle
⇒/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingleN1a
N1a+N0a−N1b
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤1
2(δ+/vextendsingle/vextendsingle/vextendsingle/vextendsingleT1a−T0a
N1a+N0a+T0b−T1b
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle). (19)
22Published in Transactions on Machine Learning Research (04/2024)
The process of mathematical derivation for (19) mainly includes basic algebraic operations, equivalent sub-
stitution and absolute value inequalities. The inequality (19) will be used later to prove (20).
Now we begin to use the assumption G(Dθ(x))∈[1−γ,1]. A relaxed version of such assumption is
G(Dθ(x))∈[1−γ,1 +γ]. If the theorem holds with the relaxed version, then it also holds for the orig-
inal assumption. We replace the above ϕwith general sigmoid G.
WithG(Dθ(x))∈[1−γ,1 +γ],T1a∈[−N1aγ,N 1aγ]andT0a∈[−N0aγ,N 0aγ]hold. In other words,
T1a−T0a
N1a+N0a∈[−γ,γ]andT0b−T1b
N1b+N0b∈[−γ,γ], which means that:
/vextendsingle/vextendsingle/vextendsingle/vextendsingleT1a−T0a
N1a+N0a+T0b−T1b
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle∈[0,2γ].
So with (19), we have
/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingleN1a
N1a+N0a−N1b
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤1
2/parenleftbigg
δ+/vextendsingle/vextendsingle/vextendsingle/vextendsingleT1a−T0a
N1a+N0a+T0b−T1b
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
≤1
2δ+γ.(20)
Now we set δ=ϵ, combine (20) with (18), and obtain the conclusion: ∀ϵ >0, if/vextendsingle/vextendsingle/vextendsingle^DDPS(G)/vextendsingle/vextendsingle/vextendsingle≤ϵ, then we
have/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle≤1
2ϵ+γ.
The proof is complete.
A.4 Proof of Theorem 2
Proof sketch. The difference between Theorem 1 and Theorem 2 mainly lies in the assumption of Dθ(x).
Therefore, Equation (19) can be also used here because it is independent with the assumption. Consequently,
the task of the proof is providing a tighter bound than Equation (19). The process of derivation mainly
includes basic algebraic operations.
Proof.The number of points satisfying ϕ(Dθ(x))∈[0,1−γ]in the four groups are denoted as
U1a,U1b,U0a,U0b, respectively. So there holds ϕ(Dθ(x))−1∈[−1,−γ]for them and ϕ(Dθ(x))−1∈[−γ,0]
for others. Note that
U1a+U1b+U0a+U0b=k.
We have
T1a=/summationdisplay
N1a(ϕ(Dθ(x)−1))∈[−U1a−(N1a−U1a)γ,−U1aγ],
and it is similar for T1b,T0a,T0b. So
T1a−T0a
N1a+N0a∈/bracketleftbigg−N1aγ−U1a+ (U1a+U0a)γ
N1a+N0a,N0aγ+U0a−(U1a+U0a)γ
N1a+N0a/bracketrightbigg
.
For the lower bound:
−N1aγ−U1a+ (U1a+U0a)γ
N1a+N0a=−N1aγ
N1a+N0a+(U1a+U0a)γ−U1a
N1a+N0a
≥−γ+(U1a+U0a)γ−U1a
N1a+N0a
≥−γ−U1a
N1a+N0a
≥−γ−k
N1a+N0a. (21)
23Published in Transactions on Machine Learning Research (04/2024)
For the upper bound:
N0aγ+U0a−(U1a+U0a)γ
N1a+N0a=N0aγ
N1a+N0a+U0a−(U1a+U0a)γ
N1a+N0a
≤γ+U0a−(U1a+U0a)γ
N1a+N0a
≤γ+U0a
N1a+N0a
≤γ+k
N1a+N0a. (22)
Therefore, we have
T1a−T0a
N1a+N0a∈/bracketleftbigg
−γ−k
N1a+N0a,γ+k
N1a+N0a/bracketrightbigg
,
and
T0b−T1b
N1b+N0b∈/bracketleftbigg
−γ−k
N1b+N0b,γ+k
N1b+N0b/bracketrightbigg
.
So
/vextendsingle/vextendsingle/vextendsingle/vextendsingleT1a−T0a
N1a+N0a+T0b−T1b
N1b+N0b/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤2γ+/parenleftbigg1
N1a+N0a+1
N1b+N0b/parenrightbigg
k. (23)
The last step is similar to Appendix A.3, but a relaxation term is added. Combining (19) and (23) together,
we have/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle≤1
2ϵ+γ+1
2/parenleftbigg1
N1a+N0a+1
N1b+N0b/parenrightbigg
k.
The proof is complete.
A.5 Risk Bound
We show the risk bound of DDP, and then provide some discussion about the surrogate function in opti-
mization.
Theorem 3. For a given constant δ>0, there holds:
P/parenleftig/vextendsingle/vextendsingle/vextendsingle\DDPS−DDP/vextendsingle/vextendsingle/vextendsingle≤t/parenrightig
≥1−δ,
wheret=/radicalig
1
2ln2
δ·/radicalig
N
(N−1)2.
Proof.We know that
E/parenleftig
\DDPS/parenrightig
=E/parenleftbigg/summationtext
N1a,N0a1dθ(x)>0
N1a+N0a−/summationtext
N1b,N0b1dθ(x)>0
N1b+N0b/parenrightbigg
=/summationtext
N1a,N0aE( 1dθ(x)>0)
N1a+N0a−/summationtext
N1b,N0bE( 1dθ(x)>0)
N1b+N0b
=/summationtext
N1a,N0aE( 1dθ(x)>0|z=+1)
N1a+N0a−/summationtext
N1b,N0bE( 1dθ(x)>0|z=−1)
N1b+N0b
=E( 1dθ(x)>0|z=+1)−E( 1dθ(x)>0|z=−1)
=P(dθ(x)>0|z= +1)−P(dθ(x)>0|z=−1)
=DDP.
24Published in Transactions on Machine Learning Research (04/2024)
Since the indicator function satisfies 1[·]∈[0,1], so according to the Hoeffding inequality,
P(/vextendsingle/vextendsingle/vextendsingle\DDPS−DDP/vextendsingle/vextendsingle/vextendsingle≤t)≥1−2 exp (−2t2N).
We setδ= 2 exp (−2t2N)so thatt=/radicalig
1
2ln2
δ·/radicalig
1
N=O(1√
N). Substitute them into Theorem 3 above, we
can prove the result.
Fairness guarantee. From the above theorem, one can see that, with a probability more than 1−δ, the
discrepancy between the \DDPSandDDPcan be small enough with the order O(1√
N). Also, Proposition 1
and Theorem 3 together tell us that with a big dataset (N is large), if ^DDPS(ϕ)and the gap are both small
enough, then we can state with a high probability that the classifier satisfies the fairness constraint.
Recall that ^DDPS(ϕ)is used for optimization, DDPis the real fairness criterion, and \DDPSis an estimator
ofDDP. Therefore, it is crucial to theoretically build a bound between the practice ( ^DDPS(ϕ)) and the
objective (DDP). To address this issue, note that:
/vextendsingle/vextendsingle/vextendsingle^DDPS(ϕ)−DDP/vextendsingle/vextendsingle/vextendsingle≤/vextendsingle/vextendsingle/vextendsingle^DDPS(ϕ)−\DDPS/vextendsingle/vextendsingle/vextendsingle
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
surrogate-fairness gap+/vextendsingle/vextendsingle/vextendsingle\DDPS−DDP/vextendsingle/vextendsingle/vextendsingle
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
risk bound of DDP.
Therefore, if we have an very large dataset (a small risk bound of DDP) and an appropriate surrogate
function with fairness guarantee (a bounded surrogate-fairness gap), then there is theoretical guarantee for/vextendsingle/vextendsingle/vextendsingle^DDPS(ϕ)−DDP/vextendsingle/vextendsingle/vextendsingle.
Is infinitesimal \DDPSbetter? However, although the discrepancy between the \DDPSandDDPcan
be small enough, the discrepancy still exists, which means that a classifier with \DDPS≈0may not satisfy
DDP≈0. The principle is similar for ERM (Empirical Risk Minimization): a small enough empirical loss
may lead to overfitting so that an infinitesimal loss is not always better. Therefore, an infinitesimal \DDPS
is also not always better.
The surrogate loss function in machine learning. In machine learning, a surrogate loss function is an
auxiliary function used during optimization, which is easier to optimize than the original objective function.
Some examples of surrogate loss functions are in Figure 6. Surrogate loss functions are particularly useful
when dealing with complex or non-differentiable objective functions. The idea is to approximate the difficult
parts of the objective with a surrogate that has more tractable properties (like smoothness or convexity) for
optimization algorithms. Here are some examples:
•Surrogatelossfunctionsareoftensmoothandconvex,eveniftheoriginalobjectiveisnot.Thisallows
for the use of gradient-based optimization techniques, which can speed up the learning process.
•Use in various algorithms: Surrogate loss functions are used in various machine learning algorithms,
including support vector machines (where the hinge loss function acts as a surrogate for the 0-1 loss
function), boosting methods (which build a surrogate to focus on examples that are hard to predict).
•Loss approximation: In classification, surrogate loss functions are used to approximate the original
loss function. For example, logistic loss and hinge loss are surrogates for the 0-1 loss function in
logistic regression and support vector machines, respectively.
Surrogate loss functions and fairness surrogate functions. The foundation of fairness surrogate
function is similar to the surrogate loss function: both of them aim to approximate the 0-1 function. When
minimizing the loss function, there are two kinds of insights to choose surrogate functions:
25Published in Transactions on Machine Learning Research (04/2024)
Figure 6: The surrogate loss functions Ψin machine learning. The figure is adapted from this link.
•Upper bound of 0-1 function : Taking surrogate loss functions as an example. As suggested in
Figure 6, all of these surrogate loss functions are upper bounds of 0-1 function. Therefore, when the
loss function is minimized to a small value, there is generalization guarantee for the classifier (Mohri
et al., 2018). Unfortunately, as suggested in Figure 1, the fairness surrogate functions mentioned in
existing work are not upper bounds of 0-1 function.
•Estimator of 0-1 function : As shown in Figure 1, we aim to better estimate the 0-1 function for
fairness. Our empirical results show that our general sigmoid estimation performs fairer results.
Overall, we believe that the fairness community can be inspired by machine learning and optimization. How
to design better fairness surrogate functions is intriguing. Insights from surrogate loss functions may be
promising in the fairness community for future work.
A.6 Variance Analysis
Theorem 4. LetNa=N1a+N0a,Nb=N1b+N0b,Pa=P(dθ(x)>0|z= +1),Pb=P(dθ(x)>0|z=−1).
We assume that each dataset consisting of Npoints is independently drawn from a dataset distribution. The
variance is computed over the dataset distribution. Then there holds:
Var/parenleftig
\DDPS/parenrightig
≤1
4/parenleftbigg1
Na+1
Nb/parenrightbigg
. (24)
Proof.Using the equation Var(x) =E(x2)−(Ex)2, we have
Var/parenleftig
\DDPS/parenrightig
=Var/parenleftbigg/summationtext
N1a,N0a1dθ(x)>0
N1a+N0a−/summationtext
N1b,N0b1dθ(x)>0
N1b+N0b/parenrightbigg
26Published in Transactions on Machine Learning Research (04/2024)
=E/parenleftbigg/summationtext
N1a,N0a1dθ(x)>0
N1a+N0a−/summationtext
N1b,N0b1dθ(x)>0
N1b+N0b/parenrightbigg2
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T−
E(\DDPS)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=DDP
2
(25)
where
T=E

/bracketleftbigg/summationtext
N1a,N0a1dθ(x)>0
N1a+N0a/bracketrightbigg2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T1+/bracketleftbigg/summationtext
N1b,N0b1dθ(x)>0
N1b+N0b/bracketrightbigg2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T2−2(/summationtext
N1a,N0a1dθ(x)>0)(/summationtext
N1b,N0b1dθ(x)>0)
(N1a+N0a)(N1b+N0b)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T3


With the linearity of expectation, we deal with the three terms in Trespectively. First of all,
E(T1) =1
(N1a+N0a)2E
/summationdisplay
N1a,N0a1dθ(x)>0
2
(26)
and
E
/summationdisplay
N1a,N0a1dθ(x)>0
2
=Var
/summationdisplay
N1a,N0a1dθ(x)>0
+
E/summationdisplay
N1a,N0a1dθ(x)>0
2
=/summationdisplay
N1a,N0aVar/parenleftbig
1dθ(x)>0/parenrightbig
+
/summationdisplay
N1a,N0aE/parenleftbig
1dθ(x)>0/parenrightbig
2
=/summationdisplay
N1a,N0aVar/parenleftbig
1dθ(x)>0|z=+1/parenrightbig
+
/summationdisplay
N1a,N0aE/parenleftbig
1dθ(x)>0|z=+1/parenrightbig
2
= (N1a+N0a)Var/parenleftbig
1dθ(x)>0|z=+1/parenrightbig
+ (N1a+N0a)2[P(dθ(x)>0|z= +1)]2(27)
Also,
Var/parenleftbig
1dθ(x)>0|z=+1/parenrightbig
=E( 1dθ(x)>0|z=+1)2−(E( 1dθ(x)>0|z=+1))2
=E( 1dθ(x)>0|z=+1)−(P(dθ(x)>0|z= +1))2
=P(dθ(x)>0|z= +1)−(P(dθ(x)>0|z= +1))2(28)
Combining (26), (27) and (28), we have
E(T1) =/parenleftbigg
1−1
N1a+N0a/parenrightbigg
(P(dθ(x)>0|z= +1))2+1
N1a+N0aP(dθ(x)>0|z= +1).(29)
Similarly, we have
E(T2) =/parenleftbigg
1−1
N1b+N0b/parenrightbigg
(P(dθ(x)>0|z=−1))2+1
N1b+N0bP(dθ(x)>0|z=−1).(30)
Finally, because of the fact that
E
/summationdisplay
N1a,N0a1dθ(x)>0·/summationdisplay
N1b,N0b1dθ(x)>0

27Published in Transactions on Machine Learning Research (04/2024)
=E
/summationdisplay
N1a,N0a1dθ(x)>0
·E
/summationdisplay
N1b,N0b1dθ(x)>0
+Cov
/summationdisplay
N1a,N0a1dθ(x)>0,/summationdisplay
N1b,N0b1dθ(x)>0

=(N1a+N0a)P(dθ(x)>0|z= +1)·(N1b+N0b)P(dθ(x)>0|z=−1)+
/summationdisplay
N1a,N0a/summationdisplay
N1b,N0bCov/parenleftbig
1dθ(x)>0|z=+1, 1dθ(x)>0|z=−1/parenrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
0
=(N1a+N0a)P(dθ(x)>0|z= +1)·(N1b+N0b)P(dθ(x)>0|z=−1),
we have
E(T3) =E/bracketleftbigg2(/summationtext
N1a,N0a1dθ(x)>0)(/summationtext
N1b,N0b1dθ(x)>0)
(N1a+N0a)(N1b+N0b)/bracketrightbigg
=2(N1a+N0a)P(dθ(x)>0|z= +1)·(N1b+N0b)P(dθ(x)>0|z=−1)
(N1a+N0a)(N1b+N0b)
=2PaPb. (31)
Overall, combining (29),(30),(31) with (25), we have:
Var/parenleftig
\DDPS/parenrightig
=E(T1) +E(T2)−E(T3)−(DDP )2
=/parenleftbigg
1−1
Na/parenrightbigg
P2
a+1
NaPa
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
E(T1)+/parenleftbigg
1−1
Nb/parenrightbigg
P2
b+1
NbPb
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
E(T2)−2PaPb/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
E(T3)−(Pa−Pb)2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
DDP2
=−1
NaP2
a−1
NbP2
b+1
NaPa+1
NbPb
=1
4/parenleftbigg1
Na+1
Nb/parenrightbigg
−1
Na(Pa−1
2)2−1
Nb(Pb−1
2)2(Pa+Pb= 1)
=1
4/parenleftbigg1
Na+1
Nb/parenrightbigg
−/parenleftbigg1
Na+1
Nb/parenrightbigg
(Pa−1
2)2(32)
<1
For the maximum variance, Patakes the value1
2, and the maximum variance is
max/bracketleftig
Var/parenleftig
\DDPS/parenrightig/bracketrightig
=1
4/parenleftbigg1
Na+1
Nb/parenrightbigg
. (33)
WhenPa= 0orPa= 1, there holds Var/parenleftig
\DDPS/parenrightig
= 0. Furthermore, the result also provides insights into
the advantages of a balanced dataset for stability.
We place the computation of Var/parenleftig
\DDPS/parenrightig
for the convenience of researchers interested in exploration.
For example, designing alternative surrogate functions and algorithms based on these expressions to reduce
variance could be an intriguing avenue for research.
B Further Experimental Details and Results
B.1 The Balanced Surrogates Algorithm
The process of balanced surrogates is shown in Algorithm 1. Computations associated with λtcostO(N).
Consider solving (8) gives rise to O(M)computations. So Algorithm 1 costs O(t(M+N)).
28Published in Transactions on Machine Learning Research (04/2024)
Algorithm 1: Balanced Surrogates
1:Input:Training set{(xi,yi)}N
i=1.
2:Initialization: Balance factor λ, surrogate function ϕ1, termination threshold η, maximum iterations τ
and other parameters.
3:Train an unconstrained classifier and obtain the model parameter θ0.
4:Time stept= 0.
5:repeat
6:t=t+ 1.
7:Solve the optimization problem (8) (start at θ0).
8:Computedθ(xi),i= 1,···,NandN1a,N1b,N0a,N0b.
9:Obtainλ′
tvia (13).
10:Obtainλtby exponential smoothing via (14).
11:ifλt≤0then
12:Setλt= 1.
13:end if
14:until|λt−λt−1|≤ηort=τorλt≤0
15:Output:λt,dθ(xi),i= 1,···,NandN1a,N1b,N0a,N0b.
Discussion of motivation of this algorithm. One may question the motivation of balanced surrogate
thatthelearningalgorithmwilltrytofindabalancebetweentheempiricallossandthefairnessregularization,
and it may suffice to use the upper bound of \DDPSas the fairness regularization. In order to clarify it, we
provide more discussion of motivation of the balanced surrogate algorithm.
We believe that previous algorithms with fairness regularization aim to find a balance between the empirical
loss and the fairness regularization. However, in this paper, we want to uncover the issue that when we use
fairness surrogate functions as the constraint/regularization, there will be a surrogate-fairness gap between
the constraint/regularization and the true fairness definition. Therefore, we emphasize that some widely used
fairness surrogate function, such as the unbounded surrogates analyzed in this paper, may not serve as an
appropriate regularization because of the surrogate-fairness gap.
Thus,focusingonfairnesssurrogatefunctions,weprovidethebalancedsurrogatealgorithmtoreducethegap,
thus trying to help fairness surrogate functions to achieve a better fairness performance. In the experiments,
the balanced surrogate method consistently helps the unbounded surrogates (log-sigmoid, linear, and hinge
surrogates) to be fairer, while it benefits the bounded surrogates (sigmoid and the general sigmoid surrogate)
in most cases. It suggests that reducing the gap is useful for unbounded surrogates. In this case, it may not
suffice to use the unbounded upper bound of \DDPS. However, when we use bounded surrogates, making
the gap infinitesimal may not always be beneficial to fairness. In this case, such phenomenon may be in line
with your concern. Overall, how to strike a balance between the gap and the surrogate function for better
fairness is intriguing, but it may be outside the scope of this work.
B.2 Details in Figure 2
The datasets are mentioned in Section 6. We randomly split each dataset into train (70%) and test (30%)
set. Then for each dataset, we train two logistic regression classifiers, one is the unconstrained classifier and
the other is the constrained optimization problem (Zafar et al., 2017c) with 0 covariance threshold.
B.3 The Parameters
General sigmoid surrogate.
•The regularization coefficient λin (8) lies in the grid [0.1,0.2,···,5].
•The parameter win general sigmoid is chosen from {1,2,4,8,16}according to the fairness perfor-
mance on the validation set.
29Published in Transactions on Machine Learning Research (04/2024)
Balanced surrogate.
•The smoothing factor α= 0.9.
•The termination threshold η= 0.01.
Discussion of hyper-parameters in our algorithms. Indeed, our approaches do require some addi-
tional hyper-parameter tuning:
•Forwin general sigmoid, as shown in our experimental settings in the appendix, it is selected from
{1, 2, 4, 8, 16} according to the fairness performance on the validation set. Thus, it requires selecting
an approximate range for it.
•For the regularization coefficient γ, it is conventional in machine learning to adjust it. Our experi-
ments also require selecting an approximate range for it.
•The smoothing factor α, and termination threshold ηare hyper-parameters, but we fix them and
never change them throughout our experiments.
•Forγin the balanced surrogate approach, it does not require any hyper-parameter tuning because
it is automatically specified in the algorithm.
Therefore,inourexperiments,weonlyconsidertuningtwohyper-parameters: wandγ.Andhyper-parameter
optimization tools (such as hyperopt1) may be useful to aid the cost of hyper-parameter tuning.
B.4 Upsampling and Downsampling
For Adult and COMPAS, the imbalance issue is not too severe (about 2:1 and 3:2, respectively), so we
balance between different demographic groups by randomly selecting samples and form the new training
dataset such that samples in each group are of the same number . However, for Bank Marketing, the ratio
of two groups approaches 20:1, so the sampling scheme for Adult and COMPAS is not suitable because it
will cause 19 copies of the minority groups (overfitting). As a result, we only incorporate 1 extra copy of the
minority group into the training set when upsampling.
B.5 Numerical Results of Figure 3-4
Full results are in Table 2. Standard deviation is shown for the metrics.
B.6 Comparing with Other Fairness-aware Algorithms
InadditiontoCP(Zafaretal.,2017c)(denotedas‘Linear’),wecomparewithtwootherpopularin-processing
fairness-aware methods: reduction approach (Agarwal et al., 2018) (denoted as ‘reduction’) and adaptive
sensitive reweighting (Krasanakis et al., 2018) (denoted as ‘ASR’), which are not based on fairness surrogate
functions. The results are shown in Figure 7.
•In the Bank dataset, our general sigmoid surrogate and balanced surrogate approaches consistently
achieve much better fairness performance than ‘reduction’ and ‘ASR’ while maintaining comparable
accuracy.
•In the COMPAS dataset, our approaches always achieve better fairness performance than others
while maintaining comparable accuracy.
•In the Adult dataset, our methods achieve better fairness than ‘ASR’ while maintaining comparable
accuracy.
1https://github.com/hyperopt/hyperopt
30Published in Transactions on Machine Learning Research (04/2024)
Table 2: Experimental results.
Datasets Methods Accuracy/vextendsingle/vextendsingle/vextendsingle\DDPS/vextendsingle/vextendsingle/vextendsingle
AdultUnconstrained 0.838 ±0.009 0.182±0.018
Linear 0.810 ±0.031 0.060±0.020
B-Linear 0.815±0.017 0.053±0.010
Log-Sigmoid 0.808 ±0.018 0.052±0.025
B-Log-Sigmoid 0.801 ±0.018 0.041±0.020
Hinge 0.808 ±0.018 0.051±0.025
B-Hinge 0.803 ±0.016 0.042±0.019
Sigmoid 0.802 ±0.015 0.033±0.011
B-Sigmoid 0.802 ±0.012 0.037±0.012
General Sigmoid 0.805 ±0.0130.029±0.010
B-General Sigmoid 0.803 ±0.007 0.037±0.016
Bank MarketingUnconstrained 0.910 ±0.004 0.192±0.003
Linear 0.892 ±0.005 0.037±0.041
B-Linear 0.891 ±0.005 0.032±0.039
Log-Sigmoid 0.894±0.007 0.043±0.038
B-Log-Sigmoid 0.892 ±0.005 0.040±0.037
Hinge 0.891 ±0.005 0.035±0.037
B-Hinge 0.889 ±0.005 0.029±0.046
Sigmoid 0.889 ±0.004 0.021±0.044
B-Sigmoid 0.888 ±0.004 0.024±0.042
General Sigmoid 0.889 ±0.003 0.014±0.031
B-General Sigmoid 0.889 ±0.0020.013±0.027
COMPASUnconstrained 0.666 ±0.020 0.281±0.035
Linear 0.636±0.009 0.039±0.022
B-Linear 0.626 ±0.009 0.035±0.013
Log-Sigmoid 0.629 ±0.012 0.039±0.021
B-Log-Sigmoid 0.626 ±0.010 0.036±0.021
Hinge 0.625 ±0.009 0.033±0.021
B-Hinge 0.624 ±0.008 0.033±0.021
Sigmoid 0.629 ±0.006 0.034±0.013
B-Sigmoid 0.626 ±0.009 0.031±0.017
General Sigmoid 0.627 ±0.010 0.030±0.018
B-General Sigmoid 0.626 ±0.0090.029±0.016
Overall, it suggests that our methods are very competitive among these in-processing methods. We leave the
comparative study between our methods and other in-processing methods for future work. We hope that our
supplementary experiment helps people better understand the advantages of our approach.
B.7 Boxplots with General Sigmoid Surrogate
Boxplots for three datasets with general sigmoid surrogate are shown in Figure 8.
C Other Related Work
C.1 Fairness-aware Algorithms
Our paper focuses on fairness surrogate functions from the in-processing methods. In addition to the afore-
mentioned fairness constraints (Zafar et al., 2017a;c) and fairness regularization (Bendekgey & Sudderth,
2021), there are also various kinds of fairness-aware in-processing methods in the community. For example,
31Published in Transactions on Machine Learning Research (04/2024)
0.79 0.80 0.81 0.82 0.83 0.84
Accuracy0.010.020.030.040.050.060.07FairnessGeneral Sigmoid
Balanced Surrogate
Reduction
ASR
Linear
(a) Adult.
0.85 0.86 0.87 0.88 0.89 0.90 0.91 0.92 0.93
Accuracy0.010.020.030.040.050.06FairnessGeneral Sigmoid
Balanced Surrogate
Reduction
ASR
Linear (b) Bank.
0.60 0.61 0.62 0.63 0.64 0.65 0.66
Accuracy0.010.020.030.040.05FairnessGeneral Sigmoid
Balanced Surrogate
Reduction
ASR
Linear (c) COMPAS.
Figure 7: Results of other in-processing methods.
/uni0000000e/uni00000014 /uni00000010/uni00000014
/uni00000033/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000014/uni00000011/uni00000013/uni00000013
/uni00000013/uni00000011/uni0000001a/uni00000018
/uni00000013/uni00000011/uni00000018/uni00000013
/uni00000013/uni00000011/uni00000015/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000036/uni0000004c/uni0000004a/uni00000051/uni00000048/uni00000047/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048
(a) Adult.
/uni0000000e/uni00000014 /uni00000010/uni00000014
/uni00000033/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000014/uni00000011/uni00000013/uni00000013
/uni00000013/uni00000011/uni0000001a/uni00000018
/uni00000013/uni00000011/uni00000018/uni00000013
/uni00000013/uni00000011/uni00000015/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000036/uni0000004c/uni0000004a/uni00000051/uni00000048/uni00000047/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048
 (b) Bank.
/uni0000000e/uni00000014 /uni00000010/uni00000014
/uni00000033/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni0000002f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000014/uni00000011/uni00000013/uni00000013
/uni00000013/uni00000011/uni0000001a/uni00000018
/uni00000013/uni00000011/uni00000018/uni00000013
/uni00000013/uni00000011/uni00000015/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000036/uni0000004c/uni0000004a/uni00000051/uni00000048/uni00000047/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048
 (c) COMPAS.
Figure 8: Boxplots for three datasets with general sigmoid surrogate. +1and−1represent the predicted
label. The red dashed line means dθ(x) = 0. The orange line in the box is the median. The median line and
the edges of the boxplot almost overlap with the box itself.
fair adversarial learning (Zhang et al., 2018; Madras et al., 2018), fair reweighing (Krasanakis et al., 2018;
Lahoti et al., 2020; Chai & Wang, 2022; Maheshwari & Perrot, 2023), fair sample selection (Roh et al.,
2021a;b), and hyper-parameter optimization (Cruz et al., 2021; Tizpaz-Niari et al., 2022). In addition to DP,
there are also fairness-aware algorithms for other fairness definitions, such as sufficiency (Shui et al., 2022a).
And there are many other application scenarios, such as medical image analysis (Mehta et al., 2024; Shui
et al., 2023).
To gain a comprehensive understanding of fairness-aware algorithms in machine learning, thorough surveys
are conducted in Mehrabi et al. (2021); Caton & Haas (2020); Hort et al. (2022); Wan et al. (2023); Pessach
& Shmueli (2023), providing a broader view of these algorithms.
C.2 Fairness Surrogate Functions
For CP, even when the empirical covariance equals 0, there is no guarantee whether demographic parity is
already satisfied (Zafar et al., 2019; Bendekgey & Sudderth, 2021; Wu et al., 2019; Lohaus et al., 2020). These
papers give different counterexamples to explain it. Zafar et al. (2019) provides a simple example explaining
that covariance constraints may perform unfavorably in the presence of large margin points. Bendekgey
& Sudderth (2021) generates a synthetic dataset and observes that the decision boundary shifts dramati-
cally when an extreme large margin point is added. Wu et al. (2019) provides two examples, respectively,
to show that satisfying \DDPSand/hatwidestCovS(ϕ)constraints are not necessarily related. Lohaus et al. (2020)
draws heatmaps to illustrate that some popular surrogates do not manage to capture the violation of fair-
ness reasonably. Radovanović et al. (2022) also conducts extensive results to show that optimizing the loss
function given the fairness constraint or regularization for unfairness can surprisingly yield unfair solutions
for Adult (Kohavi, 1996) and COMPAS (Julia Angwin & Kirchner, 2016) datasets. Instead of presenting the
32Published in Transactions on Machine Learning Research (04/2024)
counterexamples in previous work, we systematically point out the large margin points issue for covariance
proxy as well as other unbounded surrogate functions with theoretical consideration.
D Extension to Other Fairness Definitions
To begin with, in Section D.1, we review the considered fairness definitions: disparate mistreatment, and
balance for positive (negative) class. After that, in Section D.2, we provide a deeper understanding of
CP for disparate mistreatment. The proof of the statements is in Section D.3. Finally, in Section D.4, we
provide a deeper understanding of CP for balance for positive (negative) class. The corresponding proof is
in Section D.5.
Since the only difference between the linear surrogate and the CP is a dateset-specific constant, the results in
this paper about CP is also applicable for linear surrogate. As a result, by just replacing the linear surrogate
function with other surrogate functions, we can obtain the same results for other surrogate functions.
D.1 Fairness Definitions
D.1.1 Disparate Mistreatment
It is a set of definitions based on both predicted and actual outcomes. Disparate mistreatment can be avoided
if misclassification rate for different sensitive groups of people are the same. It can be specified w.r.t. a variety
of misclassification metrics and each one corresponds to a kind of fairness definition. Note that it comes out
in (Zafar et al., 2017a), and it includes some famous fairness notions, such as equality of opportunity. We
list some examples used in this work below:
Overall accuracy equality :
To satisfy the definition, Overall Misclassification Rate (OMR) should be the same for protected and unpro-
tected group, i.e.,
P(ˆy̸=y|z=−1) =P(ˆy̸=y|z= 1).
False positive error rate balance :
To satisfy the definition, False Positive Rate (FPR) should be the same for protected and unprotected group,
i.e.,
P(ˆy̸=y|z=−1,y=−1) =P(ˆy̸=y|z= +1,y=−1).
False negative error rate balance :
To satisfy the definition, False Negative Rate (FNR) should be the same for protected and unprotected
group, i.e.,
P(ˆy̸=y|z=−1,y= +1) =P(ˆy̸=y|z= +1,y= +1).
D.1.2 Balance for Positive (Negative) Class
These two definitions consider the actual outcome as well as the predicted probability ˆP(dθ(x)>0|z= +1).
Balance for positive class states that average predicted probability should be the same for those from positive
class in protected and unprotected groups, i.e.,
ˆP(dθ(x)>0|z= +1) = ˆP(dθ(x)>0|z=−1).
Replacedθ(x)>0withdθ(x)<0above and we can get the definition of balance for negative class.
D.2 Deeper Understanding of the Covariance Proxy for Disparate Mistreatment
In this part, we deal with disparate mistreatment. We denote Sas a certain set depending on each definition
of disparate mistreatment. Notice that we have to consider misclassified points. In other words, the true label
33Published in Transactions on Machine Learning Research (04/2024)
Table 3: The corresponding gθ(y,x)for each fairness definition and the set Sused for summation. For the
first three lines, (Zafar et al., 2017a) only gives the form of gθ(y,x)andSdefaults to the whole training set.
We supplement it by providing Sfor each definition. For the remaining two lines, we design the specific forms
ofgθ(y,x)andSfor these two definitions. It is an extension work for the covariance proxy in algorithmic
fairness.
Fairness Definitions gθ(y,x) S
Overall accuracy equality min(0,ydθ(x)){(xi,yi)|i= 1,2...N}
False positive error rate balance min(0,1−y
2ydθ(x)){(xi,yi)|yi=−1,i= 1,2...,N}
False negative error rate balance min(0,1+y
2ydθ(x)){(xi,yi)|yi= +1,i= 1,2...,N}
Balance for positive class1+y
2dθ(x){(xi,yi)|yi= +1,i= 1,2...,N}
Balance for negative class1−y
2dθ(x){(xi,yi)|yi=−1,i= 1,2...,N}
yshould be considered. So gθ(y,x)is used to replace dθ(x). First of all, we prove that, with the assumption
Dθ(x) = 1, if1
N/summationtext
S(zi−z)gθ(y,xi) = 0, then it perfectly satisfies the corresponding fairness definition. In
the first three rows in Table 3, the specific forms of gθ(y,x)andSare given for each kind of definition of
disparate mistreatment. It is somewhat similar to (Lohaus et al., 2020) because they also discover that the
covariance proxy proposed by (Zafar et al., 2017a) is equivalent to convex-concave surrogates. Secondly, we
extend Theorem 1 to these three fairness definitions. Our general sigmoid surrogate can also be used for
disparate mistreatment and we leave it for future work.
Note that no matter what the definition of disparate mistreatment is, Sdefaults to the whole training set in
(Zafar et al., 2017a). But we contend that it does not match their expected fairness definitions. The authors
correct it in (Zafar et al., 2019), so our proof can also serve as an explanation of the correction. Our proof
is in Appendix D.3.
D.3 Proof for Appendix D.2
The proof process below is similar to that between the proxy and disparate impact. We provide the proof
for the three fairness definitions of disparate mistreatment one by one. Before the proof, for the convenience
of the notation, we divide the training set into 8 groups as follows:
34Published in Transactions on Machine Learning Research (04/2024)
Table 4:z= +1
ActualPredicted+1-1
+1 TP+TN+
-1 FP+FN+Table 5:z=−1
ActualPredicted+1-1
+1 TP−TN−
-1 FP−FN−
For looking up different misclassification metrics conveniently, we provide the confusion matrix below:
Table 6: Confusion matrix
ActualPredicted+1 -1
+1True Positive(TP)
PPV =TP
TP+FP
TPR =TP
TP+FNFalse Negative(FN)
FOR =FN
TN+FN
FNR =FN
TP+FN
-1False Positive(FP)
FDR =FP
TP+FP
FPR =FP
FP+TNTrue Negative(TN)
NPV =TN
TN+FN
TNR =TN
TN+FP
D.3.1 Overall Accuracy Equality
Theorem 5. If a classifier satisfies1
N/summationtextN
i=1(zi−z)gθ(y,xi) = 0, wheregθ(y,x) =min(0,ydθ(x)), then
there holds
Dθ(x)y̸=ˆy|z=+1=Dθ(x)y̸=ˆy|z=−1
Equal OMR in protected and unprotected group is equivalent to:
sign(Dθ(x))y̸=ˆy|z=+1=sign(Dθ(x))y̸=ˆy|z=−1
Proof.
gθ(y,x) =min(0,ydθ(x))
=/braceleftigg
0y= ˆy(correctly classified )
ydθ(x)y̸= ˆy(misclassified )
=/braceleftigg
0y= ˆy(correctly classified )
−Dθ(x)y̸= ˆy(misclassified )
1
NN/summationdisplay
i=1(zi−z)gθ(y,xi)
=−1
N/summationdisplay
TN +,FP +,TN−,FP−(zi−z)Dθ(xi)
=−1
N
/summationdisplay
TN +,FP +,TN−,FP−ziDθ(xi)−z/summationdisplay
TN +,FP +,TN−,FP−Dθ(xi)

=−1
N/bracketleftigg/summationdisplay
TN +,FP +Dθ(xi)−/summationdisplay
TN−,FP−Dθ(xi)
35Published in Transactions on Machine Learning Research (04/2024)
−TP++TN++FP++FN+−TP−−TN−−FP−−FN−
N/summationdisplay
TN +,FP +,TN−,FP−Dθ(xi)/bracketrightigg
=2
N2
(TP++TN++FP++FN+)/summationdisplay
TN−,FP−Dθ(xi)−(TP−+TN−+FP−+FN−)/summationdisplay
TN +,FP +Dθ(xi)

=2(TP++TN++FP++FN+)(TP−+TN−+FP−+FN−)
N2
·/parenleftigg/summationtext
TN−,FP−Dθ(xi)
TP−+TN−+FP−+FN−−/summationtext
TN +,FP +Dθ(xi)
TP++TN++FP++FN+/parenrightigg
=2(TP++TN++FP++FN+)(TP−+TN−+FP−+FN−)
N2(Dθ(x)y̸=ˆy|z=−1−Dθ(x)y̸=ˆy|z=+1)(34)
We set the formula above equal to zero so there holds
Dθ(x)y̸=ˆy|z=+1=Dθ(x)y̸=ˆy|z=−1
Equal OMR in protected and unprotected group means P(ˆy̸=y|z= +1) =P(ˆy̸=y|z=−1), i.e.,
TN−+FP−
TP−+TN−+FP−+FN−=TN ++FP+
TP++TN ++FP++FN +. It is equivalent to
sign(Dθ(x))y̸=ˆy|z=+1=sign(Dθ(x))y̸=ˆy|z=−1
So we complete the proof.
Corollary 1. We assume that Dθ(x) = 1. If a classifier satisfies1
N/summationtextN
i=1(zi−z)gθ(y,xi) = 0, where
gθ(y,x) =min(0,ydθ(x)), then it perfectly meets with overall accuracy equality.
Proof.The method is similar to that for disparate impact. We just substitute Dθ(x)withsign(Dθ(x))and
understand that this corollary is true.
Theorem 6. We assume that Dθ(x)∈[1−γ,1 +γ].∀ϵ>0, if the proxy satisfies:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
i=1(zi−z)gθ(y,xi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle<ϵ
wheregθ(y,x) =min(0,ydθ(x)), then the violation of overall accuracy equality satisfies:
|P(ˆy̸=y|z= +1)−P(ˆy̸=y|z=−1)|<N2
2(TP++TN++FP++FN+)(TP−+TN−+FP−+FN−)ϵ+
/parenleftbiggTN−+FP−
TP−+TN−+FP−+FN−+TN++FP+
TP++TN++FP++FN+/parenrightbigg
γ
Proof.It is connected to the theorem above.
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
i=1(zi−z)gθ(y,xi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle<ϵ
⇒/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2(TP++TN++FP++FN+)(TP−+TN−+FP−+FN−)
N2
·/parenleftigg/summationtext
TN−,FP−Dθ(xi)
TP−+TN−+FP−+FN−−/summationtext
TN +,FP +Dθ(xi)
TP++TN++FP++FN+/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle<ϵ
36Published in Transactions on Machine Learning Research (04/2024)
⇒/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext
TN−,FP−Dθ(xi)
TP−+TN−+FP−+FN−−/summationtext
TN +,FP +Dθ(xi)
TP++TN++FP++FN+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
<N2
2(TP++TN++FP++FN+)(TP−+TN−+FP−+FN−)ϵ
⇒/vextendsingle/vextendsingle/vextendsingle/vextendsingleTN−+FP−
TP−+TN−+FP−+FN−−TN++FP+
TP++TN++FP++FN+/vextendsingle/vextendsingle/vextendsingle/vextendsingle
<N2
2(TP++TN++FP++FN+)(TP−+TN−+FP−+FN−)ϵ
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext
TN−,FP−(Dθ(xi)−1)
TP−+TN−+FP−+FN−−/summationtext
TN +,FP +(Dθ(xi)−1)
TP++TN++FP++FN+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(35)
IfDθ(x)∈[1−γ,1 +γ], thenDθ(x)−1∈[−γ,γ], which means that:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext
TN−,FP−(Dθ(xi)−1)
TP−+TN−+FP−+FN−−/summationtext
TN +,FP +(Dθ(xi)−1)
TP++TN++FP++FN+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
</parenleftbiggTN−+FP−
TP−+TN−+FP−+FN−+TN++FP+
TP++TN++FP++FN+/parenrightbigg
γ
Notice that:
|P(ˆy̸=y|z= +1)−P(ˆy̸=y|z=−1)|=/vextendsingle/vextendsingle/vextendsingle/vextendsingleTN−+FP−
TP−+TN−+FP−+FN−−TN++FP+
TP++TN++FP++FN+/vextendsingle/vextendsingle/vextendsingle/vextendsingle
So by combining these formulas together and we can complete the proof.
D.3.2 False Positive Error Rate Balance
Theorem 7. Sdenotes the set of points which satisfy y=−1in the whole training set and gθ(y,x) =
min(0,1−y
2ydθ(x)). If a classifier satisfies1
N/summationtext
S(zi−z)gθ(y,xi) = 0, then there holds:
Dθ(x)y̸=ˆy|z=+1,y=−1=Dθ(x)y̸=ˆy|z=−1,y=−1
Equal FPR in protected and unprotected group is equivalent to:
sign(Dθ(x))y̸=ˆy|z=+1,y=−1=sign(Dθ(x))y̸=ˆy|z=−1,y=−1
Proof.
gθ(y,x) =min(0,1−y
2ydθ(x))
=/braceleftigg
ydθ(x)y=−1,ˆy= +1 (false positive )
0others
=/braceleftigg
−Dθ(x)y=−1,ˆy= +1 (false positive )
0others
1
N/summationdisplay
S(zi−z)gθ(y,xi)
=−1
N/summationdisplay
FP+,FP−(zi−z)Dθ(xi)
37Published in Transactions on Machine Learning Research (04/2024)
=−1
N
/summationdisplay
FP+,FP−ziDθ(xi)−z/summationdisplay
FP+,FP−Dθ(xi)

=−1
N
/summationdisplay
FP+Dθ(xi)−/summationdisplay
FP−Dθ(xi)−FP++FN+−G−H
FP++FN++FP−+FN−/summationdisplay
FP+,FP−Dθ(xi)

=−2(FP++FN+)(FP−+FN−)
N(FP++FN++FP−+FN−)/parenleftigg/summationtext
FP+Dθ(xi)
FP++FN+−/summationtext
FP−Dθ(xi)
FP−+FN−/parenrightigg
=−2(FP++FN+)(FP−+FN−)
N(FP++FN++FP−+FN−)/parenleftig
Dθ(x)y̸=ˆy|z=+1,y=−1−Dθ(x)y̸=ˆy|z=−1,y=−1/parenrightig
(36)
Set this equal to zero and the formula above becomes:
Dθ(x)y̸=ˆy|z=+1,y=−1=Dθ(x)y̸=ˆy|z=−1,y=−1
Equal FPR in protected and unprotected group satisfiesFP+
FP++FN +=FP−
FP−+FN−, which means
sign(Dθ(x))y̸=ˆy|z=+1,y=−1=sign(Dθ(x))y̸=ˆy|z=−1,y=−1
So we complete the proof.
Corollary 2. Sdenotes the set of points which satisfy y=−1in the whole training set and gθ(y,x) =
min(0,1−y
2ydθ(x)). We assume that Dθ(x) = 1. If a classifier satisfies1
N/summationtext
S(zi−z)gθ(y,xi) = 0, then it
meets with false positive error rate balance.
Proof.We just replace Dθ(x)withsign(Dθ(x))to prove the result.
Corollary 3. We assume that Dθ(x) = 1. If a classifier satisfies1
N/summationtextN
i=1(zi−z)gθ(y,xi) = 0, where
gθ(y,x) =min(0,1−y
2ydθ(x)), then it does not meet with false positive error rate balance.
Proof.
1
NN/summationdisplay
i=1(zi−z)gθ(y,xi)
=−1
N/summationdisplay
FP+,FP−(zi−z)Dθ(xi)
=−1
N/summationdisplay
FP+,FP−(zi−z)
=−1
N
/summationdisplay
FP+,FP−zi−/summationdisplay
FP+,FP−z

=−1
N/bracketleftbigg
(FP+−FP−)−(FP+ +FP−)TP++TN++FP++FN+−E−F−G−H
N/bracketrightbigg
=−2
N2[C(TP−+TN−+FP−+FN−)−G(TP++TN++FP++FN+)] (37)
Set this equal to zero and the formula above becomes:
FP+
TP++TN++FP++FN+=FP−
TP−+TN−+FP−+FN−
38Published in Transactions on Machine Learning Research (04/2024)
However, equal FPR in protected and unprotected group satisfies the formula:
FP+
FP++FN+=FP−
FP−+FN−
So the two equations do not match. The proof is complete.
Theorem 8. We assume that Dθ(x)∈[1−γ,1 +γ].∀ϵ>0, if the proxy satisfies:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
N/summationdisplay
S(zi−z)gθ(y,xi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle<ϵ
wheregθ(y,x) =min(0,1−y
2ydθ(x)), then the violation of false positive error rate balance satisfies:
|P(ˆy̸=y|z= +1,y=−1)−P(ˆy̸=y|z=−1,y=−1)|<N(FP++FN++FP−+FN−)
2(FP++FN+)(FP−+FN−)ϵ
+/parenleftbiggFP+
FP++FN++FP−
FP−+FN−/parenrightbigg
γ(38)
Proof.
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
N/summationdisplay
S(zi−z)gθ(y,xi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle<ϵ
⇒/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext
FP+Dθ(xi)
FP++FN+−/summationtext
FP−Dθ(xi)
FP−+FN−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle<N(FP++FN++FP−+FN−)
2(FP++FN+)(FP−+FN−)ϵ
⇒/vextendsingle/vextendsingle/vextendsingle/vextendsingleFP+
FP++FN+−FP−
FP−+FN−/vextendsingle/vextendsingle/vextendsingle/vextendsingle<N(FP++FN++FP−+FN−)
2(FP++FN+)(FP−+FN−)ϵ
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext
FP+(Dθ(xi)−1)
FP++FN+−/summationtext
FP−(Dθ(xi)−1)
FP−+FN−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(39)
IfDθ(x)∈[1−γ,1 +γ], thenDθ(x)−1∈[−γ,γ], which means that:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext
FP+(Dθ(xi)−1)
FP++FN+−/summationtext
FP−(Dθ(xi)−1)
FP−+FN−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle</parenleftbiggFP+
FP++FN++FP−
FP−+FN−/parenrightbigg
γ
Notice that
|P(ˆy̸=y|z= +1,y=−1)−P(ˆy̸=y|z=−1,y=−1)|=/vextendsingle/vextendsingle/vextendsingle/vextendsingleFP+
FP++FN+−FP−
FP−+FN−/vextendsingle/vextendsingle/vextendsingle/vextendsingle
So by combining these formulas together and we can complete the proof.
D.3.3 False Negative Error Rate Balance
Theorem 9. Sdenotes the set of points which satisfy y= +1in the whole training set and gθ(y,x) =
min(0,1+y
2ydθ(x)). If a classifier satisfies1
N/summationtext
S(zi−z)gθ(y,xi) = 0, then there holds:
Dθ(x)y̸=ˆy|z=+1,y=+1=Dθ(x)y̸=ˆy|z=−1,y=+1
Equal FNR in protected and unprotected group is equivalent to:
sign(Dθ(x))y̸=ˆy|z=+1,y=+1=sign(Dθ(x))y̸=ˆy|z=−1,y=+1
39Published in Transactions on Machine Learning Research (04/2024)
Proof.
gθ(y,x) =min(0,1 +y
2ydθ(x))
=/braceleftigg
ydθ(x)y= +1,ˆy=−1 (false negative )
0others
=/braceleftigg
−Dθ(x)y= +1,ˆy=−1 (false negative )
0others
1
N/summationdisplay
S(zi−z)gθ(y,xi)
=−1
N/summationdisplay
TN +,TN−(zi−z)Dθ(xi)
=−1
N
/summationdisplay
TN +,TN−ziDθ(xi)−z/summationdisplay
TN +,TN−Dθ(xi)

=−1
N
/summationdisplay
TN +Dθ(xi)−/summationdisplay
TN−Dθ(xi)−TP++TN+−E−F
TP++TN++TP−+TN−/summationdisplay
TN +,TN−Dθ(xi)

=−2(TP++TN+)(TP−+TN−)
N(TP++TN++TP−+TN−)/parenleftigg/summationtext
TN +Dθ(xi)
TP++TN+−/summationtext
TN−Dθ(xi)
TP−+TN−/parenrightigg
=−2(TP++TN+)(TP−+TN−)
N(TP++TN++TP−+TN−)/parenleftig
Dθ(x)y̸=ˆy|z=+1,y=+1−Dθ(x)y̸=ˆy|z=−1,y=+1/parenrightig
(40)
Set this equal to zero and the formula above becomes:
Dθ(x)y̸=ˆy|z=+1,y=+1=Dθ(x)y̸=ˆy|z=−1,y=+1
Equal FNR in protected and unprotected group satisfiesTN +
TP++TN +=TN−
TP−+TN−, which means
sign(Dθ(x))y̸=ˆy|z=+1,y=+1=sign(Dθ(x))y̸=ˆy|z=−1,y=+1
So we complete the proof.
Corollary 4. Sdenotes the set of points which satisfy y= +1in the whole training set and gθ(y,x) =
min(0,1+y
2ydθ(x)). We assume that Dθ(x) = 1. If a classifier satisfies1
N/summationtext
S(zi−z)gθ(y,xi) = 0, then it
meets with false negative error rate balance.
Proof.We just replace Dθ(x)withsign(Dθ(x))to prove the result.
Corollary 5. We assume that Dθ(x) = 1. If a classifier satisfies1
N/summationtextN
i=1(zi−z)gθ(y,xi) = 0, where
gθ(y,x) =min(0,1+y
2ydθ(x)), then it does not meet with false negative error rate balance.
Proof.
1
NN/summationdisplay
i=1(zi−z)gθ(y,xi)
=−1
N/summationdisplay
TN +,TN−(zi−z)Dθ(xi)
40Published in Transactions on Machine Learning Research (04/2024)
=−1
N/summationdisplay
TN +,TN−(zi−z)
=−1
N
/summationdisplay
TN +,TN−zi−/summationdisplay
TN +,TN−z

=−1
N/bracketleftbigg
(TN+−TN−)−(TN++TN−)TP++TN++FP++FN+−TP−−TN−−FP−−FN−
N/bracketrightbigg
=−2
N2[TN+(TP−+TN−+FP−+FN−)−TN−(TP++TN++FP++FN+)] (41)
Set this equal to zero and the formula above becomes:
TN+
TP++TN++FP++FN+=TN−
TP−+TN−+FP−+FN−
However, equal FNR in protected and unprotected group satisfies the formula:
TN+
TP++TN+=TN−
TP−+TN−
So the two equations do not match. The proof is complete.
Theorem 10. We assume that Dθ(x)∈[1−γ,1 +γ].∀ϵ>0, if the proxy satisfies:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
N/summationdisplay
S(zi−z)gθ(y,xi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle<ϵ
wheregθ(y,x) =min(0,1+y
2ydθ(x)), then the violation of false negative error rate balance satisfies:
|P(ˆy̸=y|z= +1,y= +1)−P(ˆy̸=y|z=−1,y= +1)|<N(TP++TN++TP−+TN−)
2(TP++TN+)(TP−+TN−)ϵ
+/parenleftbiggTN+
TP++TN++TN−
TP−+TN−/parenrightbigg
γ(42)
Proof.
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
N/summationdisplay
S(zi−z)gθ(y,xi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle<ϵ
⇒/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext
TN +Dθ(xi)
TP++TN+−/summationtext
TN−Dθ(xi)
TP−+TN−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle<N(TP++TN++TP−+TN−)
2(TP++TN+)(TP−+TN−)ϵ
⇒/vextendsingle/vextendsingle/vextendsingle/vextendsingleTN+
TP++TN+−TN−
TP−+TN−/vextendsingle/vextendsingle/vextendsingle/vextendsingle<N(TP++TN++TP−+TN−)
2(TP++TN+)(TP−+TN−)ϵ
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext
TN +(Dθ(xi)−1)
TP++TN+−/summationtext
TN−(Dθ(xi)−1)
TP−+TN−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(43)
IfDθ(x)∈[1−γ,1 +γ], thenDθ(x)−1∈[−γ,γ], which means that:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext
TN +(Dθ(xi)−1)
TP++TN+−/summationtext
TN−(Dθ(xi)−1)
TP−+TN−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle</parenleftbiggTN+
TP++TN++TN−
TP−+TN−/parenrightbigg
γ
Notice that
|P(ˆy̸=y|z= +1,y= +1)−P(ˆy̸=y|z=−1,y= +1)|=/vextendsingle/vextendsingle/vextendsingle/vextendsingleTN+
TP++TN+−TN−
TP−+TN−/vextendsingle/vextendsingle/vextendsingle/vextendsingle
So by combining these formulas together and we can complete the proof.
41Published in Transactions on Machine Learning Research (04/2024)
D.4 Deeper Understanding of the Covariance Proxy for Balance for Positive (Negative) Class
In this section, we deal with balance for positive/negative class. These two fairness definitions are prepared
for probabilistic classifiers, which are different from previous issues. Many probabilistic classifiers do not
model the decision boundary directly, so it is hard to relate the proxy to these two definitions. To solve this
problem, we creatively link dθ(x)and the prediction probability ˆP(dθ(x)>0|z= +1)together using an
assumption.
To begin with, we first consider the relationship between dθ(x)and the prediction probability ˆP(dθ(x)>
0|z= +1). Imagine that there exists a decision boundary for probabilistic classifiers. For a probabilistic
classifier, the positive prediction result is the same as ˆP(dθ(x)>0|z= +1)>1
2, i.e., ˆP(dθ(x)>0|z=
+1)−1
2>0. Similarly, for margin-based classifiers, it is equivalent to dθ(x)>0. Thus,dθ(x)and ˆP(dθ(x)>
0|z= +1)−1
2share a similar relative trend, but the values of them are not in the same range. dθ(x)∈R,
while ˆP(dθ(x)>0|z= +1)−1
2∈[−1
2,1
2].
This suggests that we can view probabilistic classifiers as special margin-based classifiers. They find the
location of the decision boundary, and at the same time, learn a mapping f:R∝⇕⊣√∫⊔≀→[−1
2,1
2]fromdθ(x)to
ˆP(dθ(x)>0|z= +1)−1
2. In other words,
f(dθ(x)) = ˆP(dθ(x)>0|z= +1)−1
2.
It is challenging to find the mapping fdirectly. However, intuitively, we know some properties of it. It is
a bounded and monotonically increasing function which passes through the origin. Thus, we can design a
functionϕto approximately reflect the trend of f. For example, ϕ(x) =σ(x)−1
2. We assume
ϕ(dθ(x)) = ˆP(dθ(x)>0|z= +1)−1
2,
and prove that if1
N/summationtext
S(zi−z)gθ(y,xi) = 0, then it perfectly satisfies the corresponding fairness definition.
The specific forms of Sandgθ(y,x)for these two fairness notions are shown in the last two lines in Table 3.
To the best of our knowledge, we are the first to connect the proxy to these two definitions. Please refer to
Appendix D.5 for proof details.
Althoughtheassumptionisstrong,weaimtoprovidethebasisforfutureworkabouthowtochooseasuitable
ϕto approximate f, thereby inspiring the research community. We believe that it can be an interesting
direction of future work for these fairness definitions based on predicted probability.
D.5 Proof for Appendix D.4
D.5.1 Balance for Positive Class
Theorem 11. Sdenotes the set of points which satisfy y= +1in the whole training set and gθ(y,x) =
1+y
2dθ(x). We assume that ϕ(dθ(x)) = ˆP(dθ(x)>0|z= +1)−1
2, whereϕ:R∝⇕⊣√∫⊔≀→[−1
2,1
2]and it is an odd
function. If a classifier satisfies1
N/summationtext
S(zi−z)ϕ(gθ(y,xi)) = 0, then it perfectly meets with balance for positive
class.
Proof.
gθ(y,x) =1 +y
2dθ(x)
=/braceleftigg
dθ(x)y= +1
0others
=

Dθ(x)y= +1,ˆy= +1 (true positive )
−Dθ(x)y= +1,ˆy=−1 (false negative )
0others
42Published in Transactions on Machine Learning Research (04/2024)
1
N/summationdisplay
S(zi−z)ϕ(gθ(y,xi))
=1
N/summationdisplay
TP+,TN +,TP−,TN−(zi−z)ϕ(dθ(xi))
=1
N
/summationdisplay
TP+,TP−(zi−z)ϕ(Dθ(xi))−/summationdisplay
TN +,TN−(zi−z)ϕ(Dθ(xi))

=1
N
/summationdisplay
TP+(1−z)ϕ(Dθ(xi))−/summationdisplay
TN +(1−z)ϕ(Dθ(xi)) +/summationdisplay
TP−(−1−z)ϕ(Dθ(xi))−/summationdisplay
TN−(−1−z)ϕ(Dθ(xi))

=1
N
(1−z)
/summationdisplay
TP+ϕ(Dθ(xi))−/summationdisplay
TN +ϕ(Dθ(xi))
+ (−1−z)
/summationdisplay
TP−ϕ(Dθ(xi))−/summationdisplay
TN−ϕ(Dθ(xi))


=2(TP++TN++TP−+TN−)
N
·
(TP−+TN−)
/summationdisplay
TP+ϕ(Dθ(xi))−/summationdisplay
TN +ϕ(Dθ(xi))
−(TP++TN+)
/summationdisplay
TP−ϕ(Dθ(xi))−/summationdisplay
TN−ϕ(Dθ(xi))


(44)
Note thatTP++TN++TP−+TN−represents the number of points in the training set that belong to the
positive class, so it is a constant. Set this formula equal to zero and it becomes:
/summationtext
TP+ϕ(Dθ(xi))−/summationtext
TN +ϕ(Dθ(xi))
TP++TN+=/summationtext
TP−ϕ(Dθ(xi))−/summationtext
TN−ϕ(Dθ(xi))
TP−+TN−(45)
We notice that:
ϕ(dθ(xi))z=+1,y=+1=/summationtext
TP+ϕ(Dθ(xi))−/summationtext
TN +ϕ(Dθ(xi))
TP++TN+
ϕ(dθ(xi))z=−1,y=+1=/summationtext
TP−ϕ(Dθ(xi))−/summationtext
TN−ϕ(Dθ(xi))
TP−+TN−
so ifϕ(dθ(x)) = ˆP(dθ(x)>0|z= +1)−1
2, then:
ˆP(dθ(x)>0|z= +1) =/summationtext
TP+ϕ(Dθ(xi))−/summationtext
TN +ϕ(Dθ(xi))
TP++TN++1
2
ˆP(dθ(x)>0|z=−1) =/summationtext
TP−ϕ(Dθ(xi))−/summationtext
TN−ϕ(Dθ(xi))
TP−+TN−+1
2(46)
With the equations (45) and (46), we get:
ˆP(dθ(x)>0|z= +1) = ˆP(dθ(x)>0|z=−1)
which perfectly meets with balance for positive class. The proof is complete.
43Published in Transactions on Machine Learning Research (04/2024)
D.5.2 Balance for Negative Class
Theorem 12. Sdenotes the set of points which satisfy y=−1in the whole training set and gθ(y,x) =
1−y
2ϕ(dθ(x)). We assume that ϕ(dθ(x)) = ˆP(dθ(x)<0|z= +1)−1
2, whereϕ:R∝⇕⊣√∫⊔≀→[−1
2,1
2]and it is an odd
function. If a classifier satisfies1
N/summationtext
S(zi−z)ϕ(gθ(y,xi)) = 0, then it perfectly meets with balance for negative
class.
Proof.
gθ(y,x) =1−y
2dθ(x)
=/braceleftigg
dθ(x)y=−1
0others
=

dθ(x)y=−1,ˆy= +1 (false positive )
−dθ(x)y=−1,ˆy=−1 (true negative )
0others
1
N/summationdisplay
S(zi−z)ϕ(gθ(y,xi))
=1
N/summationdisplay
FP+,FN +,FP−,FN−(zi−z)ϕ(Dθ(xi))
=1
N
/summationdisplay
FP+,FP−(zi−z)ϕ(Dθ(xi))−/summationdisplay
FN +,FN−(zi−z)ϕ(Dθ(xi))

=1
N
/summationdisplay
FP+(1−z)ϕ(Dθ(xi))−/summationdisplay
FN +(1−z)ϕ(Dθ(xi)) +/summationdisplay
FP−(−1−z)ϕ(Dθ(xi))−/summationdisplay
FN−(−1−z)ϕ(Dθ(xi))

=1
N
(1−z)
/summationdisplay
FP+ϕ(Dθ(xi))−/summationdisplay
FN +ϕ(Dθ(xi))
+ (−1−z)
/summationdisplay
FP−ϕ(Dθ(xi))−/summationdisplay
FN−ϕ(Dθ(xi))


=2 (FP++FN++FP−+FN−)
N
·
(FP−+FN−)
/summationdisplay
FP+ϕ(Dθ(xi))−/summationdisplay
FN +ϕ(Dθ(xi))
−(FP++FN+)
/summationdisplay
FP−ϕ(Dθ(xi))−/summationdisplay
FN−ϕ(Dθ(xi))


(47)
Note thatFP++FN++FP−+FN−represents the number of points in the training set that belong to the
negative class, so it is a constant. Set this formula equal to zero and it becomes:
/summationtext
FP+ϕ(Dθ(xi))−/summationtext
FN +ϕ(Dθ(xi))
FP++FN+=/summationtext
FP−ϕ(Dθ(xi))−/summationtext
FN−ϕ(Dθ(xi))
FP−+FN−(48)
We notice that:
ϕ(dθ(x))z=+1,y=−1=/summationtext
FP+ϕ(Dθ(xi))−/summationtext
FN +ϕ(Dθ(xi))
FP++FN+
44Published in Transactions on Machine Learning Research (04/2024)
ϕ(dθ(x))z=−1,y=−1=/summationtext
FP−ϕ(Dθ(xi))−/summationtext
FN−ϕ(Dθ(xi))
FP−+FN−
so ifϕ(dθ(x)) = ˆP(dθ(x)<0|z= +1)−1
2, then:
ˆP(dθ(x)<0|z= +1) =/summationtext
FP+ϕ(Dθ(xi))−/summationtext
FN +ϕ(Dθ(xi))
FP++FN++1
2
ˆP(dθ(x)<0|z=−1) =/summationtext
FP−ϕ(Dθ(xi))−/summationtext
FN−ϕ(Dθ(xi))
FP−+FN−+1
2
With the equations (48) and (49), we get:
ˆP(dθ(x)<0|z= +1) = ˆP(dθ(x)<0|z=−1)
which perfectly meets with balance for negative class. The proof is complete.
E Further Analysis
We provide some interesting results and analysis about this research. While the analysis may not have a
direct impact on the main body of the paper, they could serve as potential directions for further exploration.
E.1 Fairness Surrogate Functions in the Future
The goal of fairness surrogate functions in this paper is to approximate the indicator function. As can be
seen in Figure 1, we propose the general sigmoid surrogate to better approximate it comparing to existing
surrogate functions. However, in practice, the effect of surrogate function is also related to the dataset. We
believe that more complex and even dynamic no-linear functions can be used. Such no-linear mapping can
be learned via a neural network to automatically fit the flexible and complex mapping for each dataset. We
hope that our work will help researchers better understand the fairness surrogate functions. We also hope
that we have inspired interested researchers to invent fairness surrogate functions that can automatically fit
the data distribution. We leave such interesting exploration for future work.
E.2 The Insights for Large Language Models
Recently,asthecapabilitiesofLargeLanguageModels(LLMs)haveincreased,theyareremarkablypowerful,
transcending traditional boundaries in technology and innovation (Achiam et al., 2023; Touvron et al., 2023).
These advanced models harness vast amounts of data to understand and generate human-like text, solve
complex problems with nuanced insights, and even create content that feels genuinely human-crafted (Liu
et al., 2023b). Their applications span from enhancing natural language processing to driving advancements
in fields like healthcare (Thirunavukarasu et al., 2023) and education (Kasneci et al., 2023), showcasing a
transformative potential for both industry and society at large. While LLMs’ fairness has become a focal
point of widespread attention (Wang et al., 2023), one may ask how the insights in this paper can be extended
to LLMs.
Generally, adapting the conclusions from this paper to large language models (LLMs) involves recognizing
the complexity of biases these models can inherit from datasets (Brunet et al., 2019). For LLMs, identifying
biases requires sophisticated analysis tools that can understand language use and cultural contexts. To
our knowledge, this issue remains an unsolved challenge and has not yet been thoroughly explored (Li
et al., 2023; Liu et al., 2023a). Addressing these biases involves not just algorithmic adjustments, but also
curating training data, refining model architectures, and implementing continuous feedback loops to identify
and mitigate bias. This approach may even emphasize the need for multidisciplinary efforts, combining
technical, ethical, and social perspectives to enhance fairness in LLMs. Also, the task considered in this
paper is discriminative, while LLM is widely used in generative task. Machine learning focuses on algorithms
45Published in Transactions on Machine Learning Research (04/2024)
learning from data to make predictions or decisions, whereas natural language processing (NLP) involves
understanding, interpreting, and generating human language.
Although addressing unfairness in LLMs is crucial but goes a long way, we believe that some insights in
this machine learning paper is also beneficial and enlightening for the NLP community. For example, our
theoretical analysis highlights the risks of unbounded functions causing instability. It reminds us that when
we pre-train LLMs or conduct AI alignment, we may consider some theoretically bounded loss function
to mitigate drastic fluctuation during training. Techniques like gradient clipping are also recommended for
stability (Mai & Johansson, 2021). Furthermore, our theoretical analysis as well as the experiments also shed
light on the advantage of a balanced dataset. Additionally, our findings emphasize the benefits of balanced
datasets, suggesting that pre-training language models with data balanced across sensitive attributes may
enhance fairness. Also, we may need careful consideration of data proportions in reinforcement learning from
human feedback when we align LLM with human values, particularly fairness.
E.3 Biased Estimation of CP
It serves as a further analysis of CP. But because of the limited space, it does not appear in the main paper.
The original CP is Cov =E[(z−z)dθ(x)],wherezis the mean of zover the training set. The expectation
can not be computed directly, so its empirical form /hatwidestCov =1
N/summationtextN
i=1(zi−z)dθ(xi)is proposed to estimate
the expectation. We found that /hatwidestCovmentioned earlier is a biased estimator of Cov. The unbiased one is
1
N−1/summationtextN
i=1(zi−z)dθ(xi). The proof of it can be found in Appendix E.4. But it does not significantly impacts
the results since Nis large in the reality.
First, we will give some prerequisites for the proof. The expectation of zidθ(xj)is different for iandj:
E(zidθ(xj)) =/braceleftigg
E(zdθ(x))i=j
Ez·E(dθ(x))i̸=j(49)
Because if i=j, eachzidθ(xi)is independent of each other, so in this case, E(zidθ(xj)) =E(zdθ(x)). If
i̸=j, thenzianddθ(xj)are independent, so we derive that E(zidθ(xj)) =Ez·E(dθ(x)).
It is frequently used in the proof later. With this, now we can start the proof.
E.4 Proof of Section E.3
Proof.
E/bracketleftigg
1
NN/summationdisplay
i=1(zi−z)dθ(xi)/bracketrightigg
=1
NN/summationdisplay
i=1E(zidθ(xi))−1
NN/summationdisplay
i=1E[zdθ(xi)]
=E(zdθ(x))−1
NN/summationdisplay
i=1E[(1
NN/summationdisplay
j=1zj)dθ(xi)]
=E(zdθ(x))−1
N2N/summationdisplay
i=1N/summationdisplay
j=1E[zjdθ(xi)]
=E(zdθ(x))−1
N2N/summationdisplay
i=1N/summationdisplay
j=1
j̸=iEzj·Edθ(xi)−1
N2·N·E[zdθ(x)]
=E(zdθ(x))−N−1
NEz·Edθ(x)−1
NE[zdθ(x)]
46Published in Transactions on Machine Learning Research (04/2024)
=N−1
N[E(zdθ(x))−Ez·Edθ(x)]
=N−1
NCov. (50)
Also, we show the risk bound of CovisO(1√
N)below.
Theorem 13. We assume that Dθ(x)is bounded by β, i.e.,Dθ(x)≤β. Then for a given constant δ >0,
the inequality below holds:
P/parenleftig/vextendsingle/vextendsingle/vextendsingleCov−/hatwidestCov/vextendsingle/vextendsingle/vextendsingle≤t/parenrightig
≥1−δ,
wheret=/radicalig
2β2ln2
δ·/radicalig
N
(N−1)2.
Proof.
N
N−1(zi−z)dθ(xi)∈[LB,UB ]
N
N−1(zi−z)dθ(xi)≥N
N−1(0−z)β≥−N
N−1β=LB(Lower Bound )
N
N−1(zi−z)dθ(xi)≤N
N−1(1−z)β≤N
N−1β=UB(Upper Bound )
According to the Hoeffding inequality,
P(/vextendsingle/vextendsingle/vextendsingle/hatwidestCov−Cov/vextendsingle/vextendsingle/vextendsingle≤t)≥1−2 exp−2Nt2
(UB−LB)2
= 1−2 exp−(N−1)2t2
2Nβ2
Letδ= 2 exp (−(N−1)2t2
2Nβ2), we get that t=/radicalig
2β2ln2
δ·/radicalig
N
(N−1)2=O(1√
N).
E.5 When are CP and General Sigmoid Surrogate Close to Each Other?
We show that the number of large margin points determines how close the general sigmoid surrogate is to
CP. Under mild conditions, if there are a limited number of large margin points, then the general sigmoid
surrogate is close to CP. Our general sigmoid surrogate is designed to address large margin points. However,
when these points are scarce or even nonexistent, general sigmoid surrogate becomes close to conventional
method like CP.
In the domain of fair machine learning, numerous studies have consistently underscored the pivotal role of
data (Mehrabi et al., 2021; Caton & Haas, 2020), with a particular focus on theoretical insights (Chen et al.,
2018; Lipton et al., 2018; Dutta et al., 2020). From this perspective, the theorem underscores the importance
of data, suggesting that if we can mitigate the impact of these large-margin points either before or during
the model training process, it may also contribute to the better accuracy and fairness even we don not use
general sigmoid surrogate. This theorem expresses the following vision for data-centric AI (Zha et al., 2023):
If we can deal with the data very well, maybe we do not have to use complex or even sophisticated algorithms
that requires careful fine-tuning.
Theorem 14. We assume that kpoints satisfy G(Dθ(x))∈[ζ,µ]and others satisfy G(Dθ(x))∈[0,ζ], then
there holds:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
2w·Cov(z,dθ(x))−Cov(z,G(dθ(x)))/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤k
NQ(µ) + (1−k
N)Q(ζ), (51)
47Published in Transactions on Machine Learning Research (04/2024)
whereQ(x) =1
2wx−G(x). The proof is provided in Appendix E.6. The key motivation of Theorem 14 is that
the general sigmoid function is close to a linear function when xapproaches 0. If|Cov(z,dθ(x))|≤ϵ, which
means that/vextendsingle/vextendsingle1
2wCov (z,dθ(x))/vextendsingle/vextendsingle≤1
2wϵ. Then we have|Cov(z,G(dθ(x)))|≤1
2wϵ+k
NQ(µ) + (1−k
N)Q(ζ).
Remark. Taking Figure 2(b) as an example, the result show thatk
N= 5.12%,ζ= 2,µ= 5. If we choose
w=1
2, then for the right hand side of (51), we havek
NQ(µ) + (1−k
N)Q(ζ) = 0.038, which means that the
results of bounding the general sigmoid surrogate and bounding CP are close to each other.
E.6 Proof of Theorem 14
Proof.To start with, notice that
sign(x) = 2 1x>0−1,
where sign (x) :R→{− 1,1}returns the sign of x. So we can replace 1dθ(x)>0with sign (dθ(x))in (2).
we have
\DDPS=N1a
N1a+N0a−N1b
N1b+N0b
=1
2/parenleftbiggN1a−N 0a
N1a+N0a−N1b−N0b
N1b+N0b/parenrightbigg
=1
2/parenleftbigg/summationtext
N1a,N0asign(dθ(x))
N1a+N0a−/summationtext
N1b,N0bsign(dθ(x))
N1b+N0b/parenrightbigg
∝/summationtext
N1a,N0asign(dθ(x))
N1a+N0a−/summationtext
N1b,N0bsign(dθ(x))
N1b+N0b, (52)
Furthermore, we can design a new family of surrogates ϕto approximate the sign function instead of the
indicator function. They are equivalent to each other, but an odd function is more convenient for the proof.
So we useG(x) = 2σ(wx)−1instead ofG(x) =σ(wx)in our proof.
Before the proof, we first review some properties of general sigmoid surrogate. We notice that we have
lim
x→0dG(x)
dx= 2wg(wx) =1
2w.
whereg(x) =ex
(1+ex)2. So the theorem is intuitively reasonable because when xapproaches 0, the gradient
approaches a fixed number, and thus the function itself approaches a linear function. Now we start the proof.
We know that g(x)monotonically increases when x≤0and decreases when x>0. Sog(x)≤g(0) =1
4. Now
we consider a linear function L(x) =1
2wxand letQ(x) =L(x)−G(x)to measure the gap between the two
functions. So we have
dQ(x)
dx=1
2w−2wg(wx)≤0,
which means that Q(x)monotonically decreases in R. Notice that Q(0) = 0, so we have Q(x)≤0when
x≥0andQ(x)≥0whenx<0. AndQ(x)is an odd function because Q(−x) =−Q(x). According to these
properties of Q(x), we know that|Q(x)|monotonically increases in [0,+∞].
Then there holds:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
2w1
N−1N/summationdisplay
i=1(zi−z)dθ(xi)−1
N−1N/summationdisplay
i=1(zi−z)G(dθ(xi))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=1
N−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1(zi−z)(1
2wdθ(xi)−G(dθ(xi)))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
N−1N/summationdisplay
i=1|zi−z|/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
2wdθ(xi)−G(dθ(xi))/vextendsingle/vextendsingle/vextendsingle/vextendsingle
48Published in Transactions on Machine Learning Research (04/2024)
≤1
N−1N/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
2wdθ(xi)−G(dθ(xi))/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
N−1N/summationdisplay
i=1|Q(dθ(xi))|
=1
N−1N/summationdisplay
i=1Q(Dθ(xi))
≤1
N−1[kQ(µ) + (N−k)Q(ζ)]
≤k
NQ(µ) + (1−k
N)Q(ζ). (53)
The proof is complete.
49