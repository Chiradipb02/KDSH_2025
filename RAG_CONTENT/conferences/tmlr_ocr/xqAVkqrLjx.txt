Published in Transactions on Machine Learning Research (03/2024)
HQ-VAE: Hierarchical Discrete Representation Learning
with Variational Bayes
Yuhta Takida†, Yukara Ikemiya†, Takashi Shibuya†, Kazuki Shimada†,
Woosung Choi†, Chieh-Hsin Lai†, Naoki Murata†, Toshimitsu Uesaka†, Kengo Uchida†,
Liao WeiHsiang†, Yuki Mitsufuji‡,‡
†SonyAI, Tokyo, Japan,‡Sony Group Corporation, Tokyo, Japan
Reviewed on OpenReview: https: // openreview. net/ forum? id= xqAVkqrLjx
Abstract
Vector quantization (VQ) is a technique to deterministically learn features with discrete
codebookrepresentations. Itiscommonlyperformedwithavariationalautoencodingmodel,
VQ-VAE, which can be further extended to hierarchical structures for making high-fidelity
reconstructions. However, such hierarchical extensions of VQ-VAE often suffer from the
codebook/layer collapse issue, where the codebook is not efficiently used to express the data,
and hence degrades reconstruction accuracy. To mitigate this problem, we propose a novel
unified framework to stochastically learn hierarchical discrete representation on the basis
of the variational Bayes framework, called hierarchically quantized variational autoencoder
(HQ-VAE).HQ-VAEnaturallygeneralizesthehierarchicalvariantsofVQ-VAE,suchasVQ-
VAE-2 and residual-quantized VAE (RQ-VAE), and provides them with a Bayesian training
scheme. Our comprehensive experiments on image datasets show that HQ-VAE enhances
codebook usage and improves reconstruction performance. We also validated HQ-VAE in
terms of its applicability to a different modality with an audio dataset.
1 Introduction
Learning representations with discrete features is one of the core goals in the field of deep learning. Vector
quantization (VQ) for approximating continuous features with a set of finite trainable code vectors is a
common way to make such representations (Toderici et al., 2016; Theis et al., 2017; Agustsson et al., 2017).
It has several applications, including image compression (Williams et al., 2020; Wang et al., 2022) and audio
codecs (Zeghidour et al., 2021; Défossez et al., 2022). VQ-based representation methods have been improved
with deep generative modeling, especially denoising diffusion probabilistic models (Sohl-Dickstein et al.,
2015; Ho et al., 2020; Song et al., 2020; Dhariwal & Nichol, 2021; Hoogeboom et al., 2021; Austin et al.,
2021) and autoregressive models (van den Oord et al., 2016; Chen et al., 2018; Child et al., 2019). Learning
the discrete features of the target data from finitely many representations enables redundant information
to be ignored, and such a lossy compression can be of assistance in training deep generative models on
large-scale data. After compression, another deep generative model, which is called a prior model, can be
trained on the compressed representation instead of the raw data. This approach has achieved promising
results in various tasks, e.g., unconditional generation tasks (Razavi et al., 2019; Dhariwal et al., 2020; Esser
et al., 2021b;a; Rombach et al., 2022), text-to-image generation (Ramesh et al., 2021; Gu et al., 2022; Lee
et al., 2022a) and textually guided audio generation (Yang et al., 2022; Kreuk et al., 2022). Note that the
compression performance of VQ limits the overall generation performance regardless of the performance of
the prior model.
Vector quantization is usually achieved with a vector quantized variational autoencoder (VQ-VAE) (van den
Oord et al., 2017). Based on the construction of van den Oord et al. (2017), inputs are first encoded and
quantized with code vectors, which produces a discrete representation of the encoded feature. The discrete
representation is then decoded to the data space to recover the original input. Subsequent developments
1Published in Transactions on Machine Learning Research (03/2024)
incorporated a hierarchical structure into the discrete latent space to achieve high-fidelity reconstructions.
In particular, Razavi et al. (2019) developed VQ-VAE into a hierarchical model, called VQ-VAE-2. In this
model, multi-resolution discrete latent representations are used to extract local (e.g., texture in images)
and global (e.g., shape and geometry of objects in images) information from the target data. Another type
of hierarchical discrete representation, called residual quantization (RQ), was proposed to reduce the gap
between the feature maps before and after the quantization process (Zeghidour et al., 2021; Lee et al., 2022a).
Despite the successes of VQ-VAE in many tasks, training of its variants is still challenging. It is known
that VQ-VAE suffers from codebook collapse, a problem in which most of the codebook elements are not
being used at all for the representation (Kaiser et al., 2018; Roy et al., 2018; Takida et al., 2022b). This
inefficiency may degrade reconstruction accuracy, and limit applications to downstream tasks. The variants
with hierarchical latent representations suffers from the same issue. For example, Dhariwal et al. (2020)
reportedthatitisgenerallydifficulttopushinformationtohigherlevelsinVQ-VAE-2; i.e., codebookcollapse
often occurs there. Therefore, certain heuristic techniques, such as the exponential moving average (EMA)
update (Polyak & Juditsky, 1992) and codebook reset (Dhariwal et al., 2020), are usually implemented
to mitigate these problems. Takida et al. (2022b) claimed that the issue is triggered because the training
scheme of VQ-VAE does not follow the variational Bayes framework and instead relies on carefully designed
heuristics. They proposed stochastically quantized VAE (SQ-VAE), with which the components of VQ-
VAE, i.e., the encoder, decoder, and code vectors, are trained within the variational Bayes framework with
an SQ operator. The model was shown to improve reconstruction performance by preventing the collapse
issue thanks to the self-annealing effect (Takida et al., 2022b), where the SQ process gradually tends to a
deterministic one during training. We expect that this has the potential to stabilize the training even in a
hierarchical model, which may lead to improved reconstruction performance with more efficient codebook
usage.
Here, wepropose Hierarchically Quantized VAE (HQ-VAE ),ageneralvariationalBayesianmodelforlearning
hierarchical discrete latent representations. Figure 1 illustrates the overall architecture of HQ-VAE. The
hierarchical structure consists of a bottom-up andtop-down path pair, which helps to capture the local and
global information in the data. We instantiate the generic HQ-VAE by introducing two types of top-down
layer. These two layers formulate hierarchical structures of VQ-VAE-2 and residual-quantized VAE (RQ-
VAE) within the variational scheme, which we call SQ-VAE-2 andRSQ-VAE , respectively. HQ-VAE can be
viewed as a hierarchical version (extension) of SQ-VAE, and it has the favorable properties of SQ-VAE (e.g.,
theself-annealing effect). In this sense, it unifies the current well-known VQ models in the variational Bayes
framework and thus provides a novel training mechanism. We empirically show that HQ-VAE improves upon
conventional methods in the vision and audio domains. Moreover, we applied HQ-VAEs to generative tasks
on image datasets to show the feasibility of the learnt discrete latent representations. This study is the first
attempt at developing variational Bayes on hierarchical discrete representations.
Throughout this paper, uppercase letters ( P,Q) and lowercase letters ( p,q) respectively denote the prob-
ability mass functions and probability density functions, calligraphy letters ( P,Q) the joint probabilistic
distributions of continuous and discrete random variables, and bold lowercase and uppercase letters (e.g., x
andY) vectors and matrices. Moreover, the ith column vector in Yis written as yi,[N]denotes the set
of positive integers no greater than N∈N, andJandLdenote the objective functions of HQ-VAE and
conventional autoencoders, respectively.
2 Background
We first review VQ-VAE and its extensions to hierarchical latent models. Then, we revisit SQ-VAE, which
serves as the foundation framework of HQ-VAE.
VQ-VAE. Todiscretelyrepresentobservations x∈RD, acodebookBisusedthatconsistsoffinitetrainable
code vectors{bk}K
k=1(bk∈Rdb). A discrete latent variable Zis constructed to be in a dz-tuple ofB, i.e.,
Z∈Bdz, which is later decoded to generate data samples. A deterministic encoder and decoder pair is
used to connect the observation and latent representation, where the encoder maps xtoZand the decoder
recoversxfromZby using a decoding function fθ:Rdb×dz→RD. An encoding function, denoted as
2Published in Transactions on Machine Learning Research (03/2024)
Gϕ:RD→Rdb×dz, and a deterministic quantization operator are used together as the encoder. The
encoding function first maps xtoˆZ∈Rdb×dz; then, the quantization operator finds the nearest neighbor of
ˆzifori∈[dz], i.e.,zi= arg minbk∥ˆzi−bk∥2
2. The trainable components (encoder, decoder, and codebook)
are learned by minimizing the objective,
LVQ-VAE =∥x−fθ(Z)∥2
2+β∥ˆZ−sg[Z]∥2
F, (1)
where sg[·]is the stop-gradient operator and βis a hyperparameter balancing the two terms. The codebook
is updated by applying the EMA update to ∥sg[ˆZ]−Z∥2
F.
VQ-VAE-2. Razavi et al. (2019) incorporated hierarchical structure into the discrete latent space in VQ-
VAE to model local and global information separately. The model consists of multiple levels of latents so that
the top levels have global information, while the bottom levels focus on local information, conditioned on the
top levels. The training of the model follows the same scheme as the original VQ-VAE (e.g., stop-gradient,
EMA update, and deterministic quantization).
RQ-VAE. As an extension of VQ, RQ was proposed to provide a finer approximation of Zby taking
into account information on quantization gaps (residuals) (Zeghidour et al., 2021; Lee et al., 2022a). With
RQ,Lcode vectors are assigned to each vector zi(i∈[dz]), instead of increasing the codebook size K.
To make multiple assignments, RQ repeatedly quantizes the target feature and computes quantization
residuals, denoted as Rl. Namely, the following procedure is repeated Ltimes, starting with R0=ˆZ:
zl,i= arg minbk∥rl−1,i−bk∥2
2andRl=Rl−1−Zl. By repeating RQ, the discrete representation can be
refinedinacoarse-to-finemanner. Finally, RQdiscretelyapproximatestheencodedvariableas ˆZ≈/summationtextL
l=1Zl,
where the conventional VQ is regarded as a special case of RQ with L= 1.
SQ-VAE. Takida et al. (2022b) proposed a variational Bayes framework for learning the VQ-VAE com-
ponents to mitigate the issue of codebook collapse. The resulting model, SQ-VAE, also has deterministic
encoding/decoding functions and a trainable codebook like the above autoencoders. However, unlike the
deterministic quantization schemes of VQ and RQ, SQ-VAE adopts stochastic quantization (SQ) for the
encoded features to approximate the categorical posterior distribution P(Z|x). More precisely, it defines
a stochastic dequantization process ps2(˜zi|Z) =N(˜zi;zi,s2I), which converts a discrete variable ziinto a
continuous one ˜ziby adding Gaussian noise with a learnable variance s2. By Bayes’ rule, this process is
associated with the reverse operation, i.e., SQ, which is given by ˆPs2(zi=bk|˜Z)∝exp/parenleftig
−∥˜zi−bk∥2
2
2s2/parenrightig
. Thanks
to this variational framework, the degree of stochasticity in the quantization scheme becomes adaptive. This
allows SQ-VAE to benefit from the effect of self-annealing , where the SQ process gradually approaches a
deterministic one as s2decreases. This generally improves the efficiency of codebook usage. Other related
works in the literature of discrete posterior modeling are found in Appendix A.
3 Hierarchically quantized VAE
In this section, we formulate the generic HQ-VAE model, which learns a hierarchical discrete latent rep-
resentation within the variational Bayes framework. It serves as a backbone of the instances of HQ-VAE
presented in Section 4.
To achieve a hierarchical discrete representation of depth L, we first introduce Lgroups of discrete latent
variables, which are denoted as Z1:L:={Zl}L
l=1. For eachl∈[L], we further introduce a trainable codebook,
Bl:={bl
k}Kl
k=1, consisting of Kldb-dimensional code vectors, i.e., bl
k∈Rdbfork∈[Kl]. The variable Zlis
represented as a dl-tuple of the code vectors in Bl; namely,Zl∈Bdl
l. Similarly to conventional VAEs, the
latent variable of each group is assumed to follow a pre-defined prior mass function. We set the prior as an
i.i.d. uniform distribution, defined as P(zl,i=bk) = 1/Klfori∈[dl]. The probabilistic decoder is set to
a normal distribution with a trainable isotropic covariance matrix, pθ(x|Z1:L) =N(x;fθ(Z1:L),σ2I)with
a decoding function fθ:Rdb×d1⊕···⊕ Rdb×dL→RD. To generate instances, it decodes latent variables
sampled from the prior. Here, the exact evaluation of Pθ(Z1:L|x)is required to train the generative model
with the maximum likelihood. However, this is intractable in practice. Thus, we introduce an approximated
posterior onZ1:Lgivenxand derive the evidence lower bound (ELBO) for maximization instead.
3Published in Transactions on Machine Learning Research (03/2024)
... ...
Resblock
... ... ...For posterior
Prior distribution...
Pre-processing Decoding (   )Bottom-up path
Top-down layerTop-down path
Original Reconstruction...
(a) Overview of HQ-VAE.
........Codebook
Quantization
OutputorPrior distribution(b) First top-down layer.
Concat
Enc. block
........Codebook
QuantizationInput from top layers (         )
Input from 
bottom-up (   ) Pass
+Upsampling
Output (input to next layer)Prior distribution
(c)Injected top-down layer.
Input from top layers (          )
Pass
........Codebook
Quantization
Output (input to next layer)+Prior distribution
(d)Residual top-down layer.
Figure 1: (a) HQ-VAE consists of bottom-up andtop-down paths. Red arrows represent the approximated
posterior (Equation (3)). This process consists of the following three steps: the bottom-up path generates
the features{Hr
ϕ(x)}R
r=1at different resolutions; the top-down path fuses each of them with the information
processed in the top-down path into ˜Z1:L; the auxiliary variable at the lth layer, ˜Zl, is quantized into Zl
based on the codebook Bl. Lastly, to complete the autoencoding process, all the quantized latent variables
in the top-down path,Z1:L, are fed to the decoder fθ, recovering the original data sample. The objective
function takes the Kullback–Leibler divergence of posterior and prior (in the blue box). (b) First layer for
top-down path. Any HQ-VAE should include this layer as its top layer. An HQ-VAE that includes only
this layer in its top-down path reduces to an SQ-VAE. (c)-(d) Two types of top-down layers are proposed:
injected top-down andresidual top-down layers. Injected top-down layer newly injects the higher-resolution
features generated by the bottom-up path into the latent variables in the previous top-down layers, resulting
in a higher resolution (i.e., larger dimension) of ˜ZlandZlcompared toZ1:l−1.Residual top-down layer does
not incorporate any new features from the bottom-up path but aims to refine the processed latent variables
intop-down path with additional code vectors for representing the quantization residual from the preceding
top-down layer. As a result, it maintains the same dimensionality for Zlas in the previous layer (i.e., Zl−1).
An HQ-VAE that consists entirely of the injected (residual) top-down layers is analogous to VQ-VAE-2
(RQ-VAE), which is presented in Section 4.2.1 (4.3.1).
Inspired by the work on hierarchical Gaussian VAEs (Sønderby et al., 2016; Vahdat & Kautz, 2020; Child,
2021), we introduce HQ-VAE bottom-up andtop-down paths, as shown in Figure 1a. The approximated
posterior has a top-down structure (Z1→Z2→···→ZL). The bottom-up path first generates features
fromxasHr
ϕ(x)at different resolutions ( r∈[R]). The latent variable in each group on the top-down
path is processed from Z1toZLin that order by taking Hr
ϕ(x)into account. To do so, two features, one
extracted by the bottom-up path (Hr
ϕ(x)) and one processed on the higher layers of the top-down path
(Z1:l−1), can be fed to each layer and used to estimate the Zlcorresponding to x, which we define as
4Published in Transactions on Machine Learning Research (03/2024)
ˆZl=Gl
ϕ(Hr
ϕ(x),Z1:l−1). Thelth groupZlhas a unique resolution index r; we denote it as r(l). For
simplicity, we will ignore Hr
ϕinˆZland write ˆZl=Gl
ϕ(x,Z1:l−1). The design of the encoding function Gl
ϕ
lead us to a different modeling of the approximated posterior. A detailed discussion of it is presented in the
next section.
It should be noted that the outputs of Gl
ϕlie inRdz×db, whereas the support of Zlis restricted to Bdz
l. To
connect these continuous and discrete spaces, we use a pair of stochastic dequantization and quantization
processes, as in Takida et al. (2022b). We define the stochastic dequantization process for each group as
ps2
l(˜zl,i|Zl) =N(˜zl,i;zl,i,s2
lI), which is equivalent to adding Gaussian noise to the discrete variable the
covariance of which, s2
lI, depends on the index of the group l. We hereafter denote the set of ˜Zlas˜Z1:L,
i.e., ˜Z1:L:={˜Zl}L
l=1. Next, we derive a stochastic quantization process in the form of the inverse operator
of the above stochastic dequantization:
ˆPs2
l(zl,i=bk|˜Zl)∝exp/parenleftbigg
−∥˜zl,i−bk∥2
2
2s2
l/parenrightbigg
. (2)
By using these stochastic dequantization and quantization operators, we can connect ˆZ1:LandZ1:Lvia˜Z1:L
in a stochastic manner, which leads to the entire encoding process:
Q(Z1:L,˜Z1:L|x) =L/productdisplay
l=1dl/productdisplay
i=1ps2
l(˜zl,i|Gl
ϕ(x,Z1:l−1))ˆPs2
l(zl,i|˜Zl). (3)
The prior distribution on Z1:Land ˜Z1:Lis defined using the stochastic dequantization process as
P(Z1:L,˜Z1:L) =L/productdisplay
l=1dl/productdisplay
i=1P(zl,i)ps2
l(˜zi|Zl), (4)
where the latent representations are generated from l= 1toLin that order. The generative process does
not use ˜Z; rather it uses Zasx=fθ(Z).
4 Instances of HQ-VAE
Now that we have established the overall framework of HQ-VAE, we will consider two special cases with
different top-down layers: injected top-down orresidual top-down . We will derive two instances of HQ-VAE
that consist only of the injected top-down layer or the residual top-down layer, which we will call SQ-VAE-
2 and RSQ-VAE by analogy to VQ-VAE-2 and RQ-VAE. Specifically, SQ-VAE-2 and RSQ-VAE share the
same architectural structure as VQ-VAE-2 and RQ-VAE, respectively, but the differences lie in their training
strategies, as discussed in Sections 4.2.2 and 4.3.2. Furthermore, these two layers can be combinatorially
used to define a hybrid model of SQ-VAE-2 and RSQ-VAE, as explained in Appendix D. Note that the prior
distribution (Equation (4)) is identical across all instances.
4.1 First top-down layer
The first top-down layer is the top of layer in HQ-VAE. As illustrated in Figure 1b, it takes H1
ϕ(x)as an
input and processes it with SQ. An HQ-VAE composed only of this layer reduces to SQ-VAE.
4.2 Injected top-down layer
The injected top-down layer for the approximated posterior is shown in Figure 1c. This layer infuses the
variable processed on the top-down path with higher resolution information from the bottom-up path. The
lth layer takes the feature from the bottom-up path (Hr(l)
ϕ(x)) and the variables from the higher layers in
thetop-down path as inputs. The variable from the higher layers is first upsampled to align it with Hr(l)
ϕ(x).
These two variables are then concatenated and processed in an encoding block. The above overall process
5Published in Transactions on Machine Learning Research (03/2024)
corresponds to ˆZl=Gl
ϕ(x,Z1:l−1)in Section 3. The encoded variable ˆZlis then quantized into Zlwith the
codebookBlthrough the process described in Equation (2)1. Finally, the sum of the variable from the top
layers and quantized variable Zlis passed through to the next layer.
4.2.1 SQ-VAE-2
An instance of HQ-VAE that has only the injected top-down layers in addition to the first layer reduces to
SQ-VAE-2. Note that since the index of resolutions and layers have a one-to-one correspondence in this
structure,r(l) =landL=R. As in the usual VAEs, we evaluate the evidence lower bound (ELBO)
inequality: logpθ(x)≥−JSQ-VAE-2 (x;θ,ϕ,s2,B), where
JSQ-VAE-2 (x;θ,ϕ,s2,B) =EQ(Z1:L,˜Z1:L|x)/bracketleftbigg
−logpθ(x|Z1:L) + logQ(Z1:L,˜Z1:L|x)
P(Z1:L,˜Z1:L)/bracketrightbigg
(5)
is the ELBO, s2:={s2
l}L
l=1, andB:= (B1,···,BL). Hereafter, we will omit the arguments of the objective
functions for simplicity. By decomposing QandPand substituting parameterizations for the probabilistic
parts, we have
JSQ-VAE-2 =D
2logσ2+EQ(Z1:L,˜Z1:L|x)/bracketleftigg
∥x−fθ(Z1:L)∥2
2
2σ2+L/summationdisplay
l=1/parenleftbigg∥˜Zl−Zl∥2
F
2s2
l−H(ˆPs2
l(Zl|˜Zl))/parenrightbigg/bracketrightigg
,(6)
whereH(·)indicates the entropy of the probability mass function and constant terms have been omitted.
The derivation of Equation (6) is in Appendix C. The objective function (6) consists of a reconstruction
term and regularization terms for Z1:Land ˜Z1:L. The expectation w.r.t. the probability mass function
ˆPs2
l(zl,i=bk|˜Zl)can be approximated with the corresponding Gumbel-softmax distribution (Maddison
et al., 2017; Jang et al., 2017) in a reparameterizable manner.
4.2.2 SQ-VAE-2 vs. VQ-VAE-2
VQ-VAE-2iscomposedinasimilarfashiontoSQ-VAE-2, butistrainedwiththefollowingobjectivefunction:
LVQ-VAE-2 =∥x−fθ(Z1:L)∥2
2+βL/summationdisplay
l=1∥Gl
ϕ(x,Z1:l−1)−sg[Zl]∥2
F, (7)
where the codebooks are updated with the EMA update in the same manner as the original VQ-VAE. The
objective function (7), except for the stop gradient operator and EMA update, can be obtained by setting
boths2
landσ2to infinity while keeping the ratio of the variances as s2
l=β−1σ2forl∈[L]in Equation (6).
In contrast, since all the parameters except DandLin Equation (6) are optimized, the weight of each
term is automatically adjusted during training. Furthermore, SQ-VAE-2 is expected to benefit from the
self-annealing effect, as does the original SQ-VAE (see Section 5.3).
4.3 Residual top-down layer
In this subsection, we set R= 1for demonstration purposes (the general case of Ris in Appendix D). This
means the bottom-up andtop-down paths are connected only at the top layer. In this setup, the bottom-up
path generates only a single-resolution feature; thus, we denote H1
ϕ(x)asHϕ(x)for notational simplicity in
this subsection. We design a residual top-down layer for the approximated posterior, as in Figure 1d. This
layer is to better approximate the target feature with additional assignments of code vectors per encoded
vector. By stacking this procedure Ltimes, the feature is approximated as
Hϕ(x)≈L/summationdisplay
l=1Zl. (8)
1We empirically found setting ˜ZltoˆZlinstead of sampling ˜Zlfrom ps2(˜zl,i|ˆZl)leads to better performance (as reported in
Takida et al. (2022b); therefore, we follow the procedure in practice.
6Published in Transactions on Machine Learning Research (03/2024)
Therefore, in this layer, only the information from the higher layers (not the one from the bottom-up path)
is fed to the layer. It is desired that/summationtextl+1
l′=1Zl′approximate the feature better than/summationtextl
l′=1Zl′. On this basis,
we let the following residual pass through to the next layer:
Gl
ϕ(x,Z1:l−1) =Hϕ(x)−l−1/summationdisplay
l′=1Zl′. (9)
4.3.1 RSQ-VAE
RSQ-VAE is an instance that has only residual top-down layers and the first layer. At this point, by
following Equation (5) and omitting constant terms, we could derive the ELBO objective in the same form
as Equation (6):
Jnaïve
RSQ-VAE =D
2logσ2+EQ(Z1:L,˜Z1:L|x)/bracketleftigg
∥x−fθ(Z1:L)∥2
2
2σ2+L/summationdisplay
l=1/parenleftbigg∥˜Zl−Zl∥2
F
2s2
l−H(ˆPs2
l(Zl|˜Zl))/parenrightbigg/bracketrightigg
,(10)
where the numerator of the third term corresponds to the evaluation of the residuals Hϕ(x)−/summationtextl
l′=1Zl′
for alll∈[L]with the dequantization process. However, we empirically found that training the model with
this ELBO objective above was often unstable. We suspect this is because the objective regularizes Z1:Lto
make/summationtextl
l′=1Zl′close to the feature for all l∈[L]. We hypothesize that the regularization excessively pushes
information to the top layers, which may result in layer collapse in the bottom layers. To address the issue,
we modify the prior distribution Pto result in weaker latent regularizations. Specifically, we define the prior
distribution as a joint distribution of (Z1:L,˜Z)instead of that of (Z1:L,˜Z1:L), where ˜Z=/summationtextL
l=1˜Zl. From
the reproductive property of the Gaussian distribution, a continuous latent variable converted from Zvia
the stochastic dequantization processes, ˜Z=/summationtextL
l=1˜Zl, follows a Gaussian distribution:
ps2(˜zi|Z) =N/parenleftigg
˜zi;L/summationdisplay
l=1zl,i,/parenleftiggL/summationdisplay
l=1s2
l/parenrightigg
I/parenrightigg
, (11)
and we will instead use the following prior distribution:
P(Z1:L,˜Z) =dz/productdisplay
i=1/parenleftiggL/productdisplay
l=1P(zl,i)/parenrightigg
ps2(˜zi|Z). (12)
We now derive the ELBO using the newly established prior and posterior starting from
logpθ(x)≥−JRSQ-VAE =−EQ(Z1:L,˜Z|x)/bracketleftbigg
−logpθ(x|Z1:L) + logQ(Z1:L,˜Z|x)
P(Z1:L,˜Z)/bracketrightbigg
. (13)
The above objective can be further simplified as
JRSQ-VAE =D
2logσ2+EQ(Z1:L,˜Z1:L|x)/bracketleftigg
∥x−fθ(Z1:L)∥2
2
2σ2+∥˜Z−Z∥2
F
2/summationtextL
l=1s2
l−L/summationdisplay
l=1H(ˆPs2
l(Zl|˜Zl))/bracketrightigg
,(14)
where the third term is different from the one in Equation (10) and its numerator evaluates only the overall
quantization error Hϕ(x)−/summationtextL
l=1Zlwith the dequantization process.
4.3.2 RSQ-VAE vs. RQ-VAE
RQ-VAE and RSQ-VAE both learn discrete representations in a coarse-to-fine manner, but RQ-VAE uses
a deterministic RQ scheme to achieve the approximation in Equation (8), in which it is trained with the
following objective function:
LRQ-VAE =∥x−fθ(Z1:L)∥2
2+βL/summationdisplay
l=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleHϕ(x)−sg/bracketleftiggl/summationdisplay
l′=1Zl′/bracketrightigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
F, (15)
7Published in Transactions on Machine Learning Research (03/2024)
VQ-V AE-2 SQ-V AE-2
0.9
0.8
0.70.05
0.04
0.030.010.10RMSE (  ) LPIPS (  ) SSIM (  )
Codebook size8 16 32 64 128
Codebook size8 16 32 64 128
Codebook size8 16 32 64 128
(a) CIFAR10
Codebook size32 64 128 256 5120.85
0.800.200.24
0.320.40
Codebook size32 64 128 256 512
Codebook size32 64 128 256 512
(b) CelebA-HQ
Figure 2: Impact of codebook capacity on reconstruction of images in (a) CIFAR10 and (b) CelebA-HQ.
Two layers are tested on CIFAR10, three on CelebA-HQ.
Table 1: Evaluation on ImageNet (256 ×256) and FFHQ (1024 ×1024). RMSE (×102), LPIPS, and SSIM are
evaluated using the test set. Following Razavi et al. (2019), the codebook capacity for the discrete latent
space is set to (dl,Kl) = (322,512),(642,512)and(dl,Kl) = (322,512),(642,512),(1282,512)for ImageNet
and FFHQ, respectively. Codebook perplexity is also listed for each layer.
Dataset ModelReconstruction Codebook perplexity
RMSE↓ LPIPS↓ SSIM↑ exp(H(Q(Z1))) exp(H(Q(Z2))) exp(H(Q(Z3)))
ImageNetVQ-VAE-2 6.071±0.006 0.265±0.012 0.751±0.000 106.8±0.8 288.8±1.4
SQ-VAE-2 4.603±0.006 0.096±0.000 0.855±0.006 406.2±0.9 355.5±1.7
FFHQVQ-VAE-2 4.866±0.291 0.323±0.012 0.814±0.003 24.6±10.7 41.3±14.0 310.1±29.6
SQ-VAE-2 2.118±0.013 0.166±0.002 0.909±0.001 125.8±9.0 398.7±14.1 441.3±7.9
where the codebooks are updated with the EMA update in the same manner as VQ-VAE. The second term
of Equation (15) resembles the third term of Equation (10), which strongly enforces a certain degree of
reconstruction even with only some of the information from the higher layers. RQ-VAE benefits from such a
regularization term, which leads to stable training. However, in RSQ-VAE, this regularization degrades the
reconstruction performance. To deal with this problem, we instead use Equation (14) as the objective, which
regularizes the latent representation by taking into account of accumulated information from all layers.
Remark. HQ-VAE has favorable properties similar to those of SQ-VAE. The training scheme significantly
reduces the number of hyperparameters required for training VQ-VAE variants. First, the objective func-
tions (6) and (14) do not include any hyperparameters such as βin Equations (7) and (15), except for
the introduction of a temperature parameter for the Gumbel-softmax to enable the reparameterization of
the discrete latent variables (Jang et al., 2017; Maddison et al., 2017). Please refer to Appendix E for the
approximation of the categorical distributions Q(Z1:L,˜Z1:L|x). Moreover, our formulations do not require
heuristic techniques such as stop-gradient, codebook reset, or EMA update. Additionally, HQ-VAE has
several other empirical benefits over the VQ-VAE variants, such as the effect of self-annealing for better
codebook utilization and a better rate-distortion (RD) trade-off, which will be verified in Section 5.
5 Experiments
We comprehensively examine SQ-VAE-2 and RSQ-VAE and their applicability to generative modeling. In
particular, we compare SQ-VAE-2 with VQ-VAE-2 and RSQ-VAE with RQ-VAE to see if our framework
improves reconstruction performance relative to the baselines. We basically compare various latent capacities
in order to evaluate our methods in a RD sense (Alemi et al., 2017; Williams et al., 2020). The error plots
the RD curves in the figures below indicate standard deviations based on four runs with different training
seeds. In addition, we test HQ-VAE on an audio dataset to see if it is applicable to a different modality.
Moreover, we investigate the characteristics of the injected top-down andresidual top-down layers through
8Published in Transactions on Machine Learning Research (03/2024)
Codebook size Codebook size Codebook sizeRMSE
LPIPS
SSIM0.06
0.04
0.030.050.1
0.010.9
0.8
16 32 64 128 256 16 32 64 128 256 16 32 64 128 256RSQ-V AE
RSQ-V AERQ-V AE
RQ-V AERQ-V AE w/ code reset
RQ-V AE w/ code reset 
(codebook share) (codebook share) (codebook share)
(a) CIFAR10 ( L= 4)
Number of Layers2 4 8 16 32 64
Number of Layers2 4 8 16 32 64
Number of Layers2 4 8 16 32 64RMSE
LPIPS
SSIM0.06
0.04
0.030.040.06
0.03
0.020.9
0.8
0.7
0.6
(b) CelebA-HQ ( Kl= 32)
Perplexity
232425RSQ-V AE RQ-V AEw/ code resetRQ-V AE(c) Codebook perplexity.
Figure 3: Impact of codebook capacity on reconstructions of images from (a) CIFAR10 and (b) CelebA-HQ.
(c) Codebook perplexity at each layer is plotted, where models with 32 layers are trained on CelebA-HQ
and all layers share the same codebook.
visualizations. Lastly, we apply HQ-VAEs to generative tasks to demonstrate their feasibility as feature
extractors. Unless otherwise noted in what follows, we use the same network architecture in all models and
set the codebook dimension to db= 64. Following the common dataset splits, we utilize the validation sets to
adjust the learning rate during training and compute all the numerical metrics on the test sets. The details
of these experiments are in Appendix F.
5.1 SQ-VAE-2 vs. VQ-VAE-2
We compare our SQ-VAE-2 with VQ-VAE-2 from the aspects of reconstruction accuracy and codebook
utilization. First, we investigate their performance on CIFAR10 (Krizhevsky et al., 2009) and CelebA-HQ
(256×256) under various codebook settings, i.e., different configurations for the hierarchical structure and
numbers of code vectors ( Kl). We evaluate the reconstruction accuracy in terms of a Euclidean metric and
two perceptual metrics: the root mean squared error (RMSE), structure similarity index (SSIM) (Wang
et al., 2004), and learned perceptual image patch similarity (LPIPS) (Zhang et al., 2018). As shown in
Figure 2, SQ-VAE-2 achieves higher reconstruction accuracy in all cases. The difference in performance
between the two models is especially noticeable when the codebook size is small.
Comparison on large-scale datasets. Next, we train SQ-VAE-2 and VQ-VAE-2 on ImageNet
(256×256) (Deng et al., 2009) and FFHQ (1024 ×1024) (Karras et al., 2019) with the same latent set-
tings as in Razavi et al. (2019). As shown in Table 1, SQ-VAE-2 achieves better reconstruction performance
in terms of RMSE, LPIPS, and SSIM than VQ-VAE-2, which shows similar tendencies in the comparisons on
CIFAR10 and CelebA-HQ. Furthermore, we measure codebook utilization per layer by using the perplexity
of the latent variables. The codebook perplexity is defined as exp(H(Q(Zl))), whereQ(Zl)is a marginalized
distributionofEquation(3)with x∼pd(x). Theperplexityrangesfrom1tothenumberofcodevectors( Kl)
by definition. SQ-VAE-2 has higher codebook perplexities than VQ-VAE-2 at all layers. Here, VQ-VAE-2
did not effectively use the higher layers. In particular, the perplexity values at its top layer are extremely
low, which is a sign of layer collapse.
5.2 RSQ-VAE vs. RQ-VAE
9Published in Transactions on Machine Learning Research (03/2024)
Table 2: Evaluation on UrbanSound8K. RMSE is evaluated on the test set. The network architecture follows
the one described in Liu et al. (2021). Codebook size is set to Kl= 8.
Model Number of Layers RMSE↓
RQ-VAE 4 0.506±0.018
8 0.497±0.057
RSQ-VAE 4 0.427±0.014
8 0.314±0.013
(a) Reconstructed images and magnified differences of SQ-VAE-2
2500 5000 7500
Iteration246
0
5000 10000
Iteration
0.51.0
(b)H(ˆPs2
l(zl,i|˜Zl))in SQ-VAE-2
(c) Reconstructed images and magnified differences of RSQ-VAE
2500 5000 7500
Iteration246
5000 10000
Iteration1.0
0.0(d)H(ˆPs2
l(zl,i|˜Zl))in RSQ-VAE
Figure 4: Reconstructed samples with partial layers in (a) SQ-VAE-2 and (c) RSQ-VAE. The top row
shows the reconstructed images, while the bottom row shows the components added at each layer.
(dl,Kl) = (162,256),(322,16),(642,4)and (dl,Kl) = (322,4),(322,16),(322,256)for latent capacities, l
of 1, 2, and 3, respectively. Notice that the numbers of bits in these models are equal at each layer. For a
reasonable visualization, we apply progressive coding , which induces progressive compression, to SQ-VAE-2
(see Appendix F.6). (b) and (d) Variance parameter s2
lnormalized by the initial value s2
l,0and average
entropy of the quantization process ( H(ˆPs2
l(zl,i|˜Zl))) at each layer.
We compare RSQ-VAE with RQ-VAE on the same metrics as in Section 5.1. As codebook reset is used in
the original study of RQ-VAE (Zeghidour et al., 2021; Lee et al., 2022a) to prevent codebook collapse, we
add a RQ-VAE incorporating this technique to the baselines. We do not apply it to RSQ-VAE because it
is not explainable in the variational Bayes framework. In addition, Lee et al. (2022a) proposed that all the
layers share the codebook, i.e., Bl=Bforl∈[L], to enhance the utility of the codes. We thus test both
RSQ-VAE and RQ-VAE with and without codebook sharing. First, we investigate their performances on
CIFAR10 and CelebA-HQ (256 ×256) by varying the number of quantization steps ( l) and number of code
vectors (Kl). As shown in Figures 3a and 3b, RSQ-VAE achieves higher reconstruction accuracy in terms
of RMSE, SSIM, and LPIPS compared with the baselines, although the codebook reset overall improves the
performance of RQ-VAE overall. The performance difference was remarkable when the codebook is shared
by all layers. Moreover, there is a noticeable difference in how codes are used; more codes are assigned to the
bottom layers in RSQ-VAE than to those in the RQ-VAEs (see Figure 3c). RSQ-VAE captures the coarse
information with a relatively small number of codes and refines the reconstruction by allocating more bits
at the bottom layers.
Validation on an audio dataset. We validate RSQ-VAE in the audio domain by comparing it with RQ-
VAE in an experiment on reconstructing the normalized log-Mel spectrogram in an environmental sound
dataset, UrbanSound8K (Salamon et al., 2014). We use the same network architecture as in an audio
generation paper (Liu et al., 2021), in which multi-scale convolutional layers of varying kernel sizes are
deployed to capture the local and global features of audio signals in the time-frequency domain (Xian et al.,
10Published in Transactions on Machine Learning Research (03/2024)
RMSE
LPIPS
SSIM0.04
0.030.2
0.120.9
0.8SQ-V AE-2 RSQ-V AE
(16, 4, 2) (81, 9, 3) (256, 16, 4)
(2, 4, 16) (3, 9, 81) (4, 16, 256)
Codebook sizes Codebook sizes(16, 4, 2) (81, 9, 3) (256, 16, 4)
(2, 4, 16) (3, 9, 81) (4, 16, 256)
Codebook sizes(16, 4, 2) (81, 9, 3) (256, 16, 4)
(2, 4, 16) (3, 9, 81) (4, 16, 256)
Figure 5: Comparison of SQ-VAE-2 and RSQ-VAE in three latent-capacity cases. The x-axis in blue (red) is
(K1,K2,K3)for SQ-VAE-2 (RSQ-VAE). Note that (d1,d2,d3) = (16,32,64)for SQ-VAE-2 and (d1,d2,d3) =
(32,32,32)for RSQ-VAE, and the values at the same point on the x-axis indicate the same latent capacity.
SQ-VAE-2 outperforms RSQ-VAE at relatively large latent capacities. In contrast, RSQ-VAE achieves better
reconstruction performance at higher compression rates.
2021). The codebook size is set to Kl= 8. The number of layers is set to 4 and 8, and all the layers share
the same codebook. We run each trial with five different random seeds and obtain the average and standard
deviation of the RMSEs. As shown in Table 2, RSQ-VAE has better average RMSEs than RQ-VAE has
across different numbers of layers on the audio dataset. We also evaluate the results in terms of perceptual
quality by performing subjective listening tests, which are described in Appendix F.2.3.
5.3 Empirical study of top-down layers
In this section, we focus on visualizing the obtained discrete representations. This will provide insights into
the characteristics of the top-down layers. We train SQ-VAE-2 and RSQ-VAE, each with three layers, on
CelebA-HQ (Karras et al., 2018). Figure 4 shows the progressively reconstructed images. For demonstration
purposes, we incorporate progressive coding (Shu & Ermon, 2022) in SQ-VAE-2 to make the reconstructed
images only with the top layers interpretable. Note that progressive coding is not applied to cases other than
those illustrated in Figure 4. SQ-VAE-2 and RSQ-VAE share a similarity in that the higher layers generate
the coarse part of the image while the lower layers complement them with details. However, upon examining
Figure 4, we can see that, in the case of SQ-VAE-2, the additionally generated components (bottom row
in Figure 4a) in each layer have different resolutions. We conjecture that the different layer-dependent
resolutionsHr(l)
ϕ(x), which are injected into the top-down layers, contain different information. This implies
that we may obtain more interpretable discrete representations if we can explicitly manipulate the extracted
features in the bottom-up path to provide Hr(l)
ϕ(x), giving them more semantic meaning (e.g., texture or
color). In contrast, RSQ-VAE seems to obtain a different discrete representation, which resembles more a
decomposition. This might be due to its approximated expansion in Equation (8). Moreover, we can see
from Figures 4b and 4d that the top-down layers also benefit from the self-annealing effect.
In Appendix F.3, we explore the idea of combining the two layers to form a hybrid model. There it is shown
that the individual layers in a hybrid model produce effects similar to using them alone. That is, the outputs
from the injected top-down layers have better resolution and residual top-down layers make refinements upon
certain decompositions. Since these two layers enjoy distinct refining mechanisms, a hybrid model may bring
a more flexible approximation to the posterior distribution.
Additionally, we compare the reconstruction performances of SQ-VAE-2 and RSQ-VAE with the same ar-
chitecture for the bottom-up and top-down paths. The experimental conditions are the same as in the
visualization of Figure 4. We examine three different compression rates by changing the number of code vec-
torsKl. Interestingly, Figure 5 shows that SQ-VAE-2 achieves better reconstruction performance in the case
of the lower compression rate, whereas RSQ-VAE reconstructs the original images better than SQ-VAE-2 in
the case of the higher compression rate.
11Published in Transactions on Machine Learning Research (03/2024)
Table 3: Image generation on FFHQ. †and
‡denote the use of RQ-Transformer and con-
textual RQ-Transformer as a prior model.
Model FID ↓
VQ-GAN (Esser et al., 2021b) 11.4
RQ-VAE (Lee et al., 2022a) 10.38
RSQ-VAE†(ours) 9.74
RSQ-VAE‡(ours) 8.46Table 4: Image generation on ImageNet.
Model FID ↓
VQ-VAE-2 (Razavi et al., 2019) ∼31
DALL-E (Ramesh et al., 2021) 32.01
VQ-GAN (Esser et al., 2021b) 15.78
RQ-VAE (Lee et al., 2022a) 7.55
HQ-TVAE (You et al., 2022) 7.15
Contextual RQ-Transformer (Lee et al., 2022b) 3.41
SQ-VAE-2 (ours) 4.51
5.4 Applications of HQ-VAEs to generative tasks
Lastly, we conduct experiments in the vision domain to demonstrate the applicability of HQ-VAE to realistic
generative tasks.
First, we train RSQ-VAE on FFHQ (256 ×256) by using the same encoder–decoder architecture as in Lee
et al. (2022a). We set the latent capacities to L= 4,(dl,Kl) = (82,256)forl∈[4]by following their RQ-
VAE. We train prior models, an RQ-Transformer (Lee et al., 2022a) and a contextual RQ-Transformer (Lee
et al., 2022b), on the latent feature extracted by RSQ-VAE. Table 3 lists the Fréchet inception distance (FID)
scores for the generated images (refer to Table 7 for more detailed comparisons). According to the table,
our RSQ-VAE leads to comparable generation performance to RQ-VAE, even without adversarial training.
Figure 16 shows the generated samples from our model on FFHQ.
Next, we train SQ-VAE-2 on ImageNet (256 ×256) by using a simple resblock-based encoder–decoder ar-
chitecture. We define two levels of discrete latent spaces whose capacities are (d1,K1) = (162,2048)and
(d2,K2) = (322,1024). For training SQ-VAE-2 training, we use the adversarial training framework to achieve
aperceptualreconstructionwithafeasiblecompressionratio. Subsequently, weuseMuse(Changetal.,2023)
to train the prior model on the hierarchical discrete latent features. As reported in Table 4, our SQ-VAE-
2-based models achieve FID score that is competitive with current state-of-the-art models (refer to Table 8
for more detailed comparisons). Figure 17 shows the generated images from our model on ImageNet.
6 Discussion
6.1 Conclusion
We proposed HQ-VAE, a general VAE approach that learns hierarchical discrete representations. HQ-VAE
is formulated within the variational Bayes framework as a stochastic quantization technique, which (1)
greatly reduces the number of hyperparameters to be tuned, and (2) enhances codebook usage without
any heuristics thanks to the self-annealing effect. We instantiated the general HQ-VAE with two types
of posterior approximators for the discrete latent representations (SQ-VAE-2 and RSQ-VAE). These two
variants have infomartion passing designs similar to those of as VQ-VAE-2 and RQ-VAE, respectively, but
their latent representations are quantized stochastically. Our experiments show that SQ-VAE-2 and RSQ-
VAE outperformed their individual baselines with better reconstruction and more efficient codebook usages
in the image domain as well as the audio domain.
6.2 Concluding remarks
Replacing VQ-VAE-2 and RQ-VAE with SQ-VAE-2 and RSQ-VAE will yeild comparative improvements to
many of the previous methods in terms of reconstruction accuracy and efficient codebook usage. SQ-VAE-2
and RSQ-VAE have better RD curves than the baselines have, which means that they achieve (i) better
reconstruction performance for the same latent capacities, and (ii) comparable reconstruction performance
at higher compression rate. Furthermore, our approach eliminates the need for repetited tunings of many
hyper-parameters or the use of ad-hoc techniques. SQ-VAE-2 and RSQ-VAE are applicable to generative
modeling with additional training of prior models as was done in numerous studies. Generally, compression
12Published in Transactions on Machine Learning Research (03/2024)
models with better RD curves are more feasible for the prior models (Rombach et al., 2022); hence, the
replacement of VQ-VAE-2 (RQ-VAE) with SQ-VAE-2 (RSQ-VAE) would be beneficial even for generative
modeling. Recently, RQ-VAE has been used more often in generation tasks than VQ-VAE-, wihch has a
severe instability issue, i.e., layer collapse (Dhariwal et al., 2020). Here, we believe that SQ-VAE-2 has
potential as a hierarchical model for generation tasks since it mitigates the issue greatly.
SQ-VAE-2 and RSQ-VAE have their own unique advantages as follows. SQ-VAE-2 can learn multi-resolution
discrete representations, thanks to the design of the bottom-up path with the pooling operators (see Fig-
ure 4a). The results of this study imply that further semantic disentanglement of the discrete representation
might be possible by including specific inductive architectural components in the bottom-up path. Moreover,
SQ-VAE-2 outperforms RSQ-VAE at lower compression rates (see Section 5.3). This hints at that SQ-VAE-2
might be appropriate especially for high-fidelity generation tasks when the prior model is large enough. In
contrast, one of the strengths of RSQ-VAE is that it can easily accommodate different compression rates
simply by changing the number of layers during the inference (without changing or retraining the model).
Furthermore, RSQ-VAE outperforms SQ-VAE-2 at higher compression rates (see Section 5.3). These prop-
erties would make RSQ-VAE suitable for application to neural codecs (Zeghidour et al., 2021; Défossez et al.,
2022).
Acknowledgement
We would like to thank Marc Ferras for many helpful comments during the discussion phase. Besides, we
thank anonymous reviewers for their valuable suggestions and comments.
13Published in Transactions on Machine Learning Research (03/2024)
References
Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, and
Luc V Gool. Soft-to-hard vector quantization for end-to-end learning compressible representations. In
Proc. Advances in Neural Information Processing Systems (NeurIPS) , 2017.
Alexander A Alemi, Ben Poole, Ian Fischer, Joshua V Dillon, Rif A Saurous, and Kevin Murphy. Fixing a
broken ELBO. arXiv preprint arXiv:1711.00464 , 2017.
Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured
denoising diffusion models in discrete state-spaces. In Proc. Advances in Neural Information Processing
Systems (NeurIPS) , volume 34, pp. 17981–17993, 2021.
Shiyue Cao, Yueqin Yin, Lianghua Huang, Yu Liu, Xin Zhao, Deli Zhao, and Kaigi Huang. Efficient-
vqgan: Towards high-resolution image generation with efficient vision transformers. In Proc. IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 7368–7377, 2023.
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image
transformer. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp.
11315–11325, 2022.
Huiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,
Kevin Patrick Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse:
Text-to-image generation via masked generative transformers. In Proc. International Conference on Ma-
chine Learning (ICML) , pp. 4055–4075, 2023.
Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved autoregressive
generative model. In Proc. International Conference on Machine Learning (ICML) , pp. 864–872. PMLR,
2018.
Rewon Child. Very deep VAEs generalize autoregressive models and can outperform them on images. In
Proc. International Conference on Learning Representation (ICLR) , 2021.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse trans-
formers. arXiv preprint arXiv:1904.10509 , 2019.
Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression.
arXiv preprint arXiv:2210.13438 , 2022.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp.
248–255. Ieee, 2009.
Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In Proc. Advances
in Neural Information Processing Systems (NeurIPS) , volume 34, pp. 8780–8794, 2021.
Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Juke-
box: A generative model for music. arXiv preprint arXiv:2005.00341 , 2020.
Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn Ommer. Imagebart: Bidirectional context
with multinomial diffusion for autoregressive image synthesis. In Proc. Advances in Neural Information
Processing Systems (NeurIPS) , volume 34, pp. 3518–3532, 2021a.
Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.
InProc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 12873–12883,
2021b.
Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining
Guo. Vector quantized diffusion model for text-to-image synthesis. In Proc. IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pp. 10696–10706, 2022.
14Published in Transactions on Machine Learning Research (03/2024)
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Proc. Advances in
Neural Information Processing Systems (NeurIPS) , pp. 6840–6851, 2020.
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and multi-
nomial diffusion: Learning categorical distributions. In Proc. Advances in Neural Information Processing
Systems (NeurIPS) , volume 34, pp. 12454–12465, 2021.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional
adversarial networks. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,
pp. 1125–1134, 2017.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In Proc.
International Conference on Learning Representation (ICLR) , 2017.
Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-
resolution. In Proc. European Conference on Computer Vision (ECCV) , pp. 694–711, 2016.
Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, and Noam Shazeer.
Fast decoding in sequence models using discrete latent variables. In Proc. International Conference on
Machine Learning (ICML) , pp. 2390–2399, 2018.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved
quality, stability, and variation. In Proc. International Conference on Learning Representation (ICLR) ,
2018.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial
networks. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp.
4401–4410, 2019.
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for efficient and
high fidelity speech synthesis. Advances in Neural Information Processing Systems , 33:17022–17033, 2020.
Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi
Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv preprint
arXiv:2209.15352 , 2022.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation
using residual quantization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 11523–11532, 2022a.
Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Draft-and-revise: Effective image
generation with contextual rq-transformer. In Proc. Advances in Neural Information Processing Systems
(NeurIPS) , 2022b.
Xubo Liu, Turab Iqbal, Jinzheng Zhao, Qiushi Huang, Mark D Plumbley, and Wenwu Wang. Conditional
sound generation using neural discrete time-frequency representation learning. In IEEE Int. Workshop on
Machine Learning for Signal Processing (MLSP) , pp. 1–6, 2021.
Chris J Maddison, Andriy Mnih, and Yee Why Teh. The concrete distribution: A continuous relaxation of
discrete random variables. In Proc. International Conference on Learning Representation (ICLR) , 2017.
Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM
Journal on Control and Optimization , 30(4):838–855, 1992.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya
Sutskever. Zero-shot text-to-image generation. In Proc. International Conference on Machine Learning
(ICML), pp. 8821–8831, 2021.
15Published in Transactions on Machine Learning Research (03/2024)
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2.
InProc. Advances in Neural Information Processing Systems (NeurIPS) , pp. 14866–14876, 2019.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models. In Proc. IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pp. 10684–10695, 2022.
Aurko Roy, Ashish Vaswani, Arvind Neelakantan, and Niki Parmar. Theory and experiments on vector
quantized autoencoders. arXiv preprint arXiv:1805.11063 , 2018.
Justin Salamon, Christopher Jacoby, and Juan Pablo Bello. A dataset and taxonomy for urban sound
research. In ACM Int. Conf. on Multimedia (ACM MM) , pp. 1041–1044, 2014.
MichaelSchoeffler, SarahBartoschek, Fabian-RobertStöter, MarleneRoess, SusanneWestphal, BerndEdler,
and Jürgen Herre. webmushra—a comprehensive framework for web-based listening tests. Journal of Open
Research Software , 6(1), 2018.
B Series. Method for the subjective assessment of intermediate quality level of audio systems. International
Telecommunication Union Radiocommunication Assembly , 2014.
Rui Shu and Stefano Ermon. Bit prioritization in variational autoencoders via progressive coding. In
International Conference on Machine Learning (ICML) , pp. 20141–20155, 2022.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning
using nonequilibrium thermodynamics. In Proc. International Conference on Machine Learning (ICML) ,
pp. 2256–2265. PMLR, 2015.
Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. Ladder varia-
tional autoencoders. In Proc. Advances in Neural Information Processing Systems (NeurIPS) , pp. 3738–
3746, 2016.
Casper Kaae Sønderby, Ben Poole, and Andriy Mnih. Continuous relaxation training of discrete latent
variable image models. In Beysian DeepLearning workshop, NIPS , 2017.
Jiaming Song, ChenlinMeng, andStefano Ermon. Denoising diffusion implicit models. In Proc. International
Conference on Learning Representation (ICLR) , 2020.
YuhtaTakida, Wei-HsiangLiao, Chieh-HsinLai, ToshimitsuUesaka, ShusukeTakahashi, andYukiMitsufuji.
Preventing oversmoothing in VAE via generalized variance parameterization. Neurocomputing , 509:137–
156, 2022a.
Yuhta Takida, Takashi Shibuya, WeiHsiang Liao, Chieh-Hsin Lai, Junki Ohmura, Toshimitsu Uesaka, Naoki
Murata, Takahashi Shusuke, Toshiyuki Kumakura, and Yuki Mitsufuji. SQ-VAE: Variational bayes on
discrete representation with self-annealed stochastic quantization. In Proc. International Conference on
Machine Learning (ICML) , 2022b.
Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszár. Lossy image compression with com-
pressive autoencoders. In Proc. International Conference on Learning Representation (ICLR) , 2017.
George Toderici, Sean M O’Malley, Sung Jin Hwang, Damien Vincent, David Minnen, Shumeet Baluja,
Michele Covell, and Rahul Sukthankar. Variable rate image compression with recurrent neural networks.
InProc. International Conference on Learning Representation (ICLR) , 2016.
Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. In Proc. Advances in
Neural Information Processing Systems (NeurIPS) , volume 33, pp. 19667–19679, 2020.
Aäron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In Proc.
International Conference on Machine Learning (ICML) , pp. 1747–1756, 2016.
16Published in Transactions on Machine Learning Research (03/2024)
Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In
Proc. Advances in Neural Information Processing Systems (NeurIPS) , pp. 6306–6315, 2017.
Dezhao Wang, Wenhan Yang, Yueyu Hu, and Jiaying Liu. Neural data-dependent transform for learned
image compression. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp.
17379–17388, 2022.
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error
visibility to structural similarity. IEEE transactions on image processing , 13(4):600–612, 2004.
Will Williams, Sam Ringer, Tom Ash, John Hughes, David MacLeod, and Jamie Dougherty. Hierarchical
quantized autoencoders. arXiv preprint arXiv:2002.08111 , 2020.
Yang Xian, Yang Sun, Wenwu Wang, and Syed Mohsen Naqvi. Multi-scale residual convolutional encoder
decoder with bidirectional long short-term memory for single channel speech enhancement. In Proc.
European Signal Process. Conf. (EUSIPCO) , pp. 431–435, 2021.
Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound:
Discrete diffusion model for text-to-sound generation. arXiv preprint arXiv:2207.09983 , 2022.
Tackgeun You, Saehoon Kim, Chiheon Kim, Doyup Lee, and Bohyung Han. Locally hierarchical auto-
regressive modeling for image generation. In Proc. Advances in Neural Information Processing Systems
(NeurIPS) , volume 35, pp. 16360–16372, 2022.
Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu,
Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In Proc.
International Conference on Learning Representation (ICLR) , 2022.
Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. SoundStream: An
end-to-end neural audio codec. IEEE Trans. Audio, Speech, Lang. Process. , 30:495–507, 2021.
RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreasonableeffectiveness
of deep features as a perceptual metric. In Proc. IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , pp. 586–595, 2018.
17Published in Transactions on Machine Learning Research (03/2024)
Contents
A Literature of stochastic posterior modeling for discrete latent 18
B From SQ-VAE to HQ-VAE 18
C Derivations 19
C.1 SQ-VAE-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C.2 RSQ-VAE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
D Hybrid model 20
E Gumbel-softmax approximation 22
F Experimental details 22
F.1 SQ-VAE-2 vs VQ-VAE-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
F.2 RSQ-VAE vs RQ-VAE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
F.3 Empirical study of top-down layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
F.4 Applications of HQ-VAEs to generative tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
F.5 Perceptual loss for images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
F.6 Progressive coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
A Literature of stochastic posterior modeling for discrete latent
We briefly compare SQ-VAE, the foundation of HQ-VAE, with two other stochastic discrete models.
Sønderbyetal.(2017)proposedstochasticposteriormodelinginthediscretelatentsettingas Q(zi=bk|x)∝
−∥˜zi−bk∥2
2, followed by succeeding works (Roy et al., 2018; Williams et al., 2020). Although the approach
resembles SQ-VAE in terms of stochastic quantization, they have non-trivial differences as follows. In SQ-
VAE,Takidaetal.(2022b)introducedatrainableparameter, denotedas Σφ, byconsideringadequantization
process as the inverse operation of quantization. This leads to Q(zi=bk|x)∝−1
2(˜zi−bk)⊤Σ−1
φ(˜zi−bk),
which imposes the self-annealing effect as well as allows the posterior modeling with more general distances.
Please refer to Takida et al. (2022b) for more details about the difference and some numerical examples.
The logits modeling (LM) in dVAE (Ramesh et al., 2021) is another approach to model the discrete posterior
distribution ( Q(Z|x)) instead of vector quantization. LM assumes an encoder that directly maps from xto
the logits of Q(zi|x). Therefore, it does not have raw explicit features ˜Z. On the other hand, in SQ-VAE, the
encoded features ˜Zare directly quantized. When the learnable variance parameter s2is set (fixed) close to
0, conventional VQ is obtained. We used SQ instead of LM for the posterior modeling in HQ-VAE because
SQ can serve as a drop-in replacement of VQ and can be extended to hierarchical and residual quantizations
similar to VQ.
B From SQ-VAE to HQ-VAE
LetB∈{bk}K
k=1(bk∈Rdb) be a trainable codebook and Z∈Bdbe a discrete latent variable to represent
the target data x. An auxiliary variable, denoted as ˜Z∈Rdb×d, is used to connect xandZin the SQ
framework. InageneralVAEwithdiscreteandauxiliarylatentvariables, thegenerativeprocessismodeledas
x∼pθ(x|Z)with a prior distribution Z,˜Z)∼P(Z,˜Z). For variational inference, a variational distribution,
18Published in Transactions on Machine Learning Research (03/2024)
denoted asQ(Z,˜Z|x), is used to approximate the posterior distribution P(Z,˜Z|x). The ELBO becomes
logpθ(x)≥logpθ(x)−DKL(Q(Z,˜Z|x)∥P(Z,˜Z|x))
=EQ(Z,˜Z|x)/bracketleftbigg
logpθ(x)P(Z,˜Z|x)
Q(Z,˜Z|x)/bracketrightbigg
=EQ(Z,˜Z|x)/bracketleftbigg
logpθ(x|Z)−logQ(Z,˜Z|x)
P(Z,˜Z)/bracketrightbigg
.
In the vanilla SQ-VAE (Takida et al., 2022b), the vectors composing Zare assumed to be independent of
each other and the quantization process is applied to each vector such that P(Z,˜Z) =/producttextd
i=1P(zi)p(˜zi|zi)
andQ(Z,˜Z|x) =/producttextd
i=1q(˜zi|x)ˆP(zi|˜zi). In contrast, HQ-VAE allows dependencies in Zto be modelled by
decomposing it into Lgroups, i.e.,Z={Zl}L
l=1andZl∈Bdl(d=/summationtextL
l=1dl), and modeling dependencies
between{Zl}L
l=1by designing the approximated posterior Q(Z,˜Z|x)in decomposed ways. This paper
provides instances in the form of top-down blocks to model the dependencies, which enables graphical models
to constructed for (x,{Zl}L
l=1)in a handy way (see Figure 1a). This framework naturally covers the latent
structures in VQ-VAE-2 and RQ-VAE (see Sections 4.2.2 and 4.3.2).
C Derivations
C.1 SQ-VAE-2
The ELBO of SQ-VAE-2 is formulated by using Bayes’ theorem:
logpθ(x)≥logpθ(x)−DKL(Q(Z1:L,˜Z1:L|x)∥P(Z1:L,˜Z1:L|x))
=EQ(Z1:L,˜Z1:L|x)/bracketleftbigg
logpθ(x)P(Z1:L,˜Z1:L|x)
Q(Z1:L,˜Z1:L|x)/bracketrightbigg
=EQ(Z1:L,˜Z1:L|x)/bracketleftbigg
logpθ(x|Z1:L)−logQ(Z1:L,˜Z1:L|x)
P(Z1:L,˜Z1:L)/bracketrightbigg
=EQ(Z1:L,˜Z1:L|x)/bracketleftigg
logpθ(x|Z1:L)−L/summationdisplay
l=1dl/summationdisplay
i=1/parenleftigg
logps2
l(˜zl,i|ˆZl)
ps2
l(˜zl,i|Zl)+ logˆPs2
l(zl,i|˜Zl)
P(zl,i)/parenrightigg/bracketrightigg
=EQ(Z1:L,˜Z1:L|x)/bracketleftigg
logpθ(x|Z1:L) +L/summationdisplay
l=1dl/summationdisplay
i=1/parenleftigg
logps2
l(˜zl,i|Zl)
ps2
l(˜zl,i|ˆZl)+H(ˆPs2
l(zl,i|˜Zl))−logKl/parenrightigg/bracketrightigg
.(16)
Since the probabilistic parts are modeled as Gaussian distributions, the first and second terms can be
calculated as
logpθ(x|Z1:L) = logN(x;fθ(Z1:L),σ2I)
=−D
2log(2πσ2)−1
2σ2∥x−fθ(x)∥2
2and (17)
EQ(Z1:L,˜Z1:L|x)/bracketleftigg
ps2
l(˜zl,i|Zl)
ps2
l(˜zl,i|ˆZl)/bracketrightigg
=EQ(Z1:L,˜Z1:L|x)/bracketleftbigg
−1
2s2
l∥˜zl,i−zl,i∥2
2+1
2s2
l∥˜zl,i−ˆzl,i∥2
2/bracketrightbigg
=−EQ(Z1:L,˜Z1:L|x)/bracketleftbigg1
2s2
l∥˜zl,i−zl,i∥2
2/bracketrightbigg
+db
2. (18)
By substituting Equations (17) and (18) into Equation (16), we arrive at Equation (6), where we use ˜Zl=ˆZl
instead of sampling it in a practical implementation.
19Published in Transactions on Machine Learning Research (03/2024)
...Injected top-down layer
Residual top-down layers...
Figure 6: Top-down layers corresponding to the rth resolution of the hybrid model in Appendix D.
C.2 RSQ-VAE
The ELBO of RSQ-VAE is formulated by using Bayes’ theorem:
logpθ(x)≥logpθ(x)−DKL(Q(Z1:L,˜Z|x)∥P(Z1:L,˜Z|x))
=EQ(Z1:L,˜Z1:L|x)/bracketleftbigg
logpθ(x)P(Z1:L,˜Z|x)
Q(Z1:L,˜Z|x)/bracketrightbigg
=EQ(Z1:L,˜Z|x)/bracketleftbigg
logpθ(x|Z1:L)−logQ(Z1:L,˜Z|x)
P(Z1:L,˜Z)/bracketrightbigg
=EQ(Z1:L,˜Z|x)/bracketleftigg
logpθ(x|Z1:L)−dl/summationdisplay
i=1logps2(˜zi|ˆZ)
ps2(˜zi|Z)−L/summationdisplay
l=1dl/summationdisplay
i=1logˆPs2
l(zl,i|˜Zl)
P(zl,i)/bracketrightigg
=EQ(Z1:L,˜Z|x)/bracketleftigg
logpθ(x|Z1:L) +dl/summationdisplay
i=1logps2(˜zi|Z)
ps2(˜zi|ˆZ)+L/summationdisplay
l=1dl/summationdisplay
i=1H(ˆPs2
l(zl,i|˜Zl))−logKl/bracketrightigg
.(19)
Since the probabilistic parts are modeled as Gaussian distributions, the second term can be calculated as
EQ(Z1:L,˜Z|x)/bracketleftigg
ps2(˜zi|Z)
ps2(˜zi|ˆZ)/bracketrightigg
=EQ(Z1:L,˜Z|x)/bracketleftigg
−1
2/summationtextL
l=1s2
l∥˜zi−zi∥2
2+1
2/summationtextL
l=1s2
l∥˜zi−ˆzi∥2
2/bracketrightigg
=−EQ(Z1:L,˜Z|x)/bracketleftigg
1
2/summationtextL
l=1s2
l∥˜zi−zi∥2
2/bracketrightigg
+db
2. (20)
By substituting Equations (17) and (20) into (19), we find that Equation (14), where we use ˜Z=Hϕ(x)
instead of sampling it in a practical implementation. The above is the derivation of the ELBO objective in
the case ofR= 1. Appendix D extends this model to the general case with a variable R, which is equivalent
to the hybrid model.
D Hybrid model
Here, we describe the ELBO of a hybrid model where the two types of top-down layers are combinatorially
used to build a top-down path as in Figure 6. Some extra notation will be needed as follows: Lrindicates
the number of layers corresponding to the resolutions from the first to rth order;ℓr:={Lr−1+ 1,···,Lr}is
the set of layers corresponding to the resolution r; and the output of the encoding block in the (Lr−1+ 1)th
layer is denoted as ˜Gr
ϕ(Hr
ϕ(x),Z1:Lr−1). In Figure 6, the quantized variables Zℓraim at approximating the
variable encoded at l=Lr−1+ 1as
˜Gr
ϕ(Hr
ϕ(x),Z1:Lr−1)≈/summationdisplay
l∈ℓrZl=:Yr. (21)
20Published in Transactions on Machine Learning Research (03/2024)
On this basis, the lthtop-down layer quantizes the following information:
ˆZl=Gl
ϕ(x,Z1:l−1) =/braceleftigg
H1
ϕ(x) ( l= 1)
˜Gr(l)
ϕ(Hr(l)
ϕ(x),Z1:Lr(l)−1)−/summationtextl
l′=Lr(l)−1+1Zl′(l>1).(22)
To derive the ELBO objective, we consider conditional distributions on (Z1:L,˜Y1:R), where ˜Yr:=/summationtext
l∈ℓr˜Zl.
From the reproductive property of the Gaussian distribution, the continuous latent variable converted from
Yrvia the stochastic dequantization processes, ˜Zℓr, follows a Gaussian distribution:
ps2r(˜yr,i|Zℓr) =N/parenleftigg
˜yr,i;/summationdisplay
l∈ℓrzl,i,/parenleftigg/summationdisplay
l∈ℓrs2
l/parenrightigg
I/parenrightigg
, (23)
wheres2
r:={s2
l}l∈ℓr. We will use the following prior distribution to derive the ELBO objective:
P(Z1:L,˜Y1:R) =R/productdisplay
r=1dr/productdisplay
i=1/parenleftigg/productdisplay
l∈ℓrP(zl,i)/parenrightigg
ps2r(˜yr,i|Zℓr), (24)
wheredr:=dlforl∈ℓr. With the prior and posterior distributions, the ELBO of the hybrid model can be
formulated by invoking Bayes’ theorem:
logpθ(x)≥logpθ(x)−DKL(Q(Z1:L,˜Y1:R|x)∥P(Z1:L,˜Y1:R|x))
=EQ(Z1:L,˜Y1:R|x)/bracketleftbigg
logpθ(x)P(Z1:L,˜Y1:R|x)
Q(Z1:L,˜Y1:R|x)/bracketrightbigg
=EQ(Z1:L,˜Y1:R|x)/bracketleftbigg
logpθ(x|Z1:L)−logQ(Z1:L,˜Y1:R|x)
P(Z1:L,˜Y1:R)/bracketrightbigg
=EQ(Z1:L,˜Y1:R|x)/bracketleftigg
logpθ(x|Z1:L)−R/summationdisplay
r=1dr/summationdisplay
i=1logps2r(˜yr,i|ˆZℓr)
ps2r(˜yr,i|Zℓr)−L/summationdisplay
l=1dl/summationdisplay
i=1logˆPs2
l(zl,i|˜Zl)
P(zl,i)/bracketrightigg
=EQ(Z1:L,˜Y1:R|x)/bracketleftigg
logpθ(x|Z1:L) +R/summationdisplay
r=1dr/summationdisplay
i=1logps2r(˜yr,i|Zℓr)
ps2r(˜yr,i|ˆZℓr)+L/summationdisplay
l=1dl/summationdisplay
i=1H(ˆPs2
l(zl,i|˜Zl))−logKl/bracketrightigg
,
(25)
where ˆYr=˜Gr
ϕ(Hr
ϕ(x),Z1:Lr−1). Since we have modelled the dequantization process and the probabilistic
decoder as Gaussians, by substituting their closed forms into the above equation, we find that
JHQ-VAE =D
2logσ2
+EQ(Z1:L,˜Z1:L|x)
∥x−fθ(Z1:L)∥2
2
2σ2+R/summationdisplay
r=1/vextenddouble/vextenddouble/vextenddouble˜Gr
ϕ(Hr
ϕ(x),Z1:Lr−1)−/summationtext
l∈ℓrZl/vextenddouble/vextenddouble/vextenddouble2
F
2/summationtext
l∈ℓrs2
l−L/summationdisplay
l=1H(ˆPs2
l(Zl|˜Zl))
,
(26)
where we have used
EQ(Z1:L,˜Y1:R|x)/bracketleftigg
ps2r(˜yr,i|Zℓr)
ps2r(˜yr,i|ˆZℓr)/bracketrightigg
=EQ(Z1:L,˜Y1:R|x)/bracketleftigg
−1
2/summationtext
l∈ℓrs2
l∥˜yr,i−yr,i∥2
2+1
2/summationtext
l∈ℓrs2
l∥˜yr,i−ˆyr,i∥2
2/bracketrightigg
=−EQ(Z1:L,˜Y1:R|x)/bracketleftigg
1
2/summationtext
l∈ℓrs2
l∥˜yr,i−yr,i∥2
2/bracketrightigg
+dr
2. (27)
Here, we used ˜Yr=ˆYrinstead of sampling it in a practical implementation.
21Published in Transactions on Machine Learning Research (03/2024)
Table 5: Impact of decay factor rin the exponential schedule defined in Appendix E on reconstruction of
images in CIFAR10. RMSE ( ×102) is evaluated using the test set. Too large decay factor deteriorates the
reconstruction performance.
Model r= 1×10−6r= 3×10−6r= 1×10−5r= 3×10−5r= 1×10−4r= 3×10−4
SQ-VAE-2 5.223±0.0165.219±0.0175.264±0.0165.356±0.0167.234±0.0219.083±0.019
RSQ-VAE 6.544±0.0186.149±0.0235.697±0.0225.579±0.0155.802±0.0206.439±0.019
E Gumbel-softmax approximation
Since HQ-VAE has discrete latent space, our objective functions (6), (14), and (26) involve expectations
with respective to the categorical variables Z1:L(inQ(Z1:L,˜Z1:L|x)). As a common method to make the
sampling process of the categorical variables reparameterizable, we replace the categorical distribution (2)
with Gumbel-softmax alternatives. In a Gumbel-softmax distribution, a temperature parameter τ∈(0,∞)
is introduced. The sampling process from a Gumbel-softmax distribution is equivalent to that from a
categorical distribution as τ→0.
There is a trade-off with the temperature τ. Lower temperatures lead to sampling that closely resembles the
original categorical distribution but with larger gradient variances. Conversely, higher temperatures result
in greater discrepancies between the categorical and Gumbel-softmax distributions but with smaller gradient
variances. It has been shown that annealing the temperature from high to low values during model training
performs well in practice (Jang et al., 2017; Sønderby et al., 2017; Williams et al., 2020). Specifically, Jang
et al. (2017) proposed an exponential scheduling, τ= max{cτ,exp(−rt)}, wheretdenotes the global training
step. Following the approach of Takida et al. (2022b), we adopt the same annealing schedule with cτ= 0
andr= 10−5for all the experiments (except for Table 5).
Here, we investigate the sensitivity of the choice of the decay factor r. We train two-layer SQ-VAE-2
and RSQ-VAE with codebook capacities of (dl,Kl) = (42,512),(82,512)and(dl,Kl) = (82,32),(82,32),
respectively. We sweep rwhile keeping the other settings in Table 5. According to the table, a decay factor
that is too large, leading to fast convergence to the categorical distribution, deteriorates the reconstruction
performance.
F Experimental details
This appendix describes the details of the experiments2discussed in Section 5. For all the experiments
except for the ones of RSQ-VAE and RQ-VAE on FFHQ and UrbanSound8K in Section 5.2, we construct
architectures for the bottom-up andtop-down paths as is described in Figures 1 and 7. To build these paths,
we use two common blocks, the Resblock and Convblock by following Child (2021) in Figure 7a; these blocks
are shown in Figures 7b and 7c. Here, we denote the width and height of Hr
ϕ(x)aswrandhr, respectively,
i.e.,Hr
ϕ(x)∈Rdb×wr×hr. We setcmid= 0.5in Figure 7. For all the experiments, we use the Adam optimizer
withβ1= 0.9andβ2= 0.9. Unless otherwise noted, we reduce the learning rate in half if the validation loss
does not improve in the last three epochs.
In HQ-VAE, we deal with the decoder variance σ2by using the update scheme with the maximum likelihood
estimation (Takida et al., 2022a). We gradually reduce the temperature parameter of the Gumbel-softmax
trick with a standard schedule τ= exp(10−5·t)(Jang et al., 2017), where tis the iteration step.
We set the hyperparameters of VQ-VAE to the standard values: the balancing parameter βin Equations (7)
and (15) is set to 0.25, and the weight decay in EMA for the codebook update is set to 0.99.
Below, we summarize the datasets used in the experiments described in Section 5 below.
2The source code is attached in the supplementary material.
22Published in Transactions on Machine Learning Research (03/2024)
Table 6: Notation for the convolutional layers in Figure 7.
Notation Description
Conv(1×1)
d 2D Convolutional layer (channel =n, kernel = 1×1, stride = 1, padding = 0)
Conv(3×3)
d 2D Convolutional layer (channel =n, kernel = 3×3, stride = 1, padding = 1)
Conv(4×4)
d 2D Convolutional layer (channel =n, kernel = 4×4, stride = 2, padding = 1)
ConvT(3×3)
d2D Transpose convolutional layer (channel =n, kernel = 3×3, stride = 1, padding = 1)
ConvT(4×4)
d2D Transpose convolutional layer (channel =n, kernel = 4×4, stride = 2, padding = 1)
CIFAR10. CIFAR10 (Krizhevsky et al., 2009) contains ten classes of 32 ×32 color images, which are
separated into 50,000 and 10,000 samples for the training and test sets, respectively. We use the default split
and further randomly select 10,000 samples from the training set to prepare the validation set.
CelebA-HQ. CelebA-HQ (Karras et al., 2018) contains 30,000 high-resolution face images that are selected
from the CelebA dataset by following Karras et al. (2018). We use the default training/validation/test split
(24,183/2,993/2,824 samples). We preprocess the images by cropping and resizing them to 256 ×256.
FFHQ. FFHQ (Karras et al., 2019) contains 70,000 high-resolution face images. In Section 5.1, we split
the images into three sets: training (60,000 samples), validation (5,000 samples), and test (5,000 samples)
sets. We crop and resize them to 1024 ×1024. In Section 5.2, we follow the same preprocessing as in Lee
et al. (2022a), wherein the images are split training (60,000 samples) and validation (10,000 samples) sets
and they are cropped and resized them to 256 ×256.
ImageNet. ImageNet (Deng et al., 2009) contains 1000 classes of natural images in RGB scales. We use the
default training/validation/test split (1,281,167/50,000/100,000 samples). We crop and resize the images to
256×256.
UrbanSound8K. UrbanSound8K (Salamon et al., 2014) contains 8,732 labeled audio clips of urban sound
in ten classes, such as dogs barking and drilling sounds. UrbanSound8K is divided into ten folds, and we
use the folds 1-8/9/10 as the training/validation/test split. The duration of each audio clip is less than 4
seconds. In our experiments, to align the lengths of input audio, we pad all the audio clips to 4 seconds. We
also convert the clips to 16 bit and down-sampled them to 22,050 kHz. The 4-second waveform audio clip
is converted to a Mel spectrogram with shape 80×344. We preprocess each audio clip by using the method
described in the paper (Liu et al., 2021):
1. We extract an 80-dimensional Mel spectrogram by using a short-time Fourier transform (STFT)
with a frame size of 1024, a hop size of 256, and a Hann window.
2. We apply dynamic range compression to the Mel spectrogram by first clipping it to a minimum
value of 1×10−5and then applying a logarithmic transformation.
F.1 SQ-VAE-2 vs VQ-VAE-2
F.1.1 Comparison on CIFAR10 and CelebA-HQ
We construct the architecture as depicted in Figures 1 and 7. To build the top-down paths, we use two
injected top-down layers (i.e., R= 2) withw1=h1= 8andw2=h2= 16for CIFAR10, and three layers
(i.e.,R= 3) withw1=h1= 8,w2=h2= 16andw3=h3= 32for CelebA-HQ. For the bottom-up paths, we
repeatedly stack two Resblocks and an average pooling layer once and four times, respectively, for CIFAR10
and CelebA-HQ. We set the learning rate to 0.001 and train all the models for a maximum of 100 epochs
with a mini-batch size of 32. The sensitivity of SQ-VAE-2 in terms of batch size is investigated in Figure 8a.
23Published in Transactions on Machine Learning Research (03/2024)
(a) Basic blocks.
(b) Pre-processing and decoding
 (c) Blocks in top-down layer
Figure 7: Architecture details in Figure 1. Table 6 summarizes the notation for the convolutional layers,
Conv(k×k)
dandConvT(k×k)
d.
F.1.2 Comparison on large-scale datasets
We construct the architecture as depicted in Figures 1 and 7. To build the top-down paths, we use two
injected top-down layers (i.e., R= 2), withw1=h1= 32andw2=h2= 64for ImageNet, and three layers
(i.e.,R= 3) withw1=h1= 32,w2=h2= 64andw3=h3= 128for FFHQ, respectively. For the
bottom-up paths, we repeatedly stack two Resblocks and an average pooling layer three times and five times
respectively for ImageNet and FFHQ. We set the learning rate to 0.0005. We train ImageNet and FFHQ for
a maximum of 50 and 200 epochs with a mini-batch size of 512 and 128, respectively. Figure 9 and Figure 10
show reconstructed samples of SQ-VAE-2 on ImageNet and FFHQ.
F.2 RSQ-VAE vs RQ-VAE
F.2.1 Comparison on CIFAR10 and CelebA-HQ
We construct the architecture as depicted in Figures 1 and 7 without injected top-down layers, i.e., R= 1.
We set the resolution of Hϕ(x)tow=h= 8. For the bottom-up paths, we stack two Resblocks on the
average pooling layer once for CIFAR10 and four times for CelebA-HQ. We set the learning rate to 0.001 and
train all the models for a maximum of 100 epochs with a mini-batch size of 32. The sensitivity of RSQ-VAE
in terms of batch size is investigated in Figure 8b.
F.2.2 Improvement in perceptual quality
This experiment use the same network architecture as in Lee et al. (2022a). We set the learning rate to
0.001 and train an RSQ-VAE model for a maximum of 300 epochs with a mini-batch size of 128 (4 GPUs,
32 samples for each GPU) on FFHQ. We use our modified LPIPS loss (see Appendix F.5) in the training.
For the evaluation, we compute rFID scores with the code provided in their repository3on the validation
set (10,000 samples). Moreover, we use the pre-trained RQ-VAE model offered in the same repository for
evaluating RQ-VAE.
We will show examples of reconstructed images in Appendix F.5 after we explain our modified LPIPS loss.
3https://github.com/kakaobrain/rq-vae-transformer
24Published in Transactions on Machine Learning Research (03/2024)
0.0570.06
0.85
0.84
0.04
0.053RMSE
LPIPS
SSIM
16
Batch size32 64 128 256SQ-V AE-2 VQ-V AE-2
16
Batch size32 64 128 256 16
Batch size32 64 128 256
(a) SQ-VAE-2 and VQ-VAE-2 on CIFAR10
0.060.04 0.86
0.85
0.84
0.830.03
0.055RMSE
LPIPS
SSIM
16
Batch size32 64 128 256RSQ-V AE RQ-V AE
16
Batch size32 64 128 256 16
Batch size32 64 128 256
(b) RSQ-VAE and RQ-VAE on CIFAR10
Figure 8: Impact of batch size on reconstruction of images in CIFAR10. (a) We set the codebook capacity
for the discrete space to (dl,Kl) = (42,512),(82,512)withL= 2. (b) We set the codebook capacity for the
discrete space to (dl,Kl) = (82,32),(82,32)withL= 2. For (a) and (b), we use the same architectures as
in Sections 5.1 and 5.2, respectively. The plots suggest that batch size should be tuned to achieve better
reconstruction performance for all the models.
(a) Source images
(b) Reconstructed images
Figure 9: Reconstructed samples of SQ-VAE-2 trained on ImageNet
25Published in Transactions on Machine Learning Research (03/2024)
(a) Source images
(b) Reconstructed images
Figure 10: Reconstructed samples of SQ-VAE-2 trained on FFHQ
26Published in Transactions on Machine Learning Research (03/2024)
Rating score100
80
60
40
20
0
Rerference RSQ-V AE RQ-V AE Anchor
(original) (white noise)
(a)L= 4
Rating score100
80
60
40
20
0
Rerference RSQ-V AE RQ-V AE Anchor
(original) (white noise) (b)L= 8
Figure 11: Violin plots of MUSHRA listening test results on UrbanSound8K test set in cases of (a) four
layers and (b) eight layers. The white dots indicate the median scores, and the tops and bottoms of the
thick vertical lines indicate the first and third quartiles, respectively.
F.2.3 Validation on an audio dataset
We construct the architecture in accordance with the previous paper on audio generation (Liu et al., 2021).
For the top-down paths, the architecture consists of several strided convolutional layers in parallel (Xian
et al., 2021). We use four strided convolutional layers consisting of two sub-layers with stride 2, followed by
two ResBlocks with ReLU activations. The kernel sizes of these four strided convolutional layers are 2×2,
4×4,6×6and8×8respectively. We add the outputs of the four strided convolutional layers together and
pass the result to a convolutional layer with a kernel size of 3×3. Then, we get the resolution of Hϕ(x)
tow= 20,h= 86. For the bottom-up paths, we stack a convolutional layer with a kernel size 3×3, two
Resblocks with ReLU activations, and two transposed convolutional layers with stride 2 and kernel size 4×4.
We set the learning rate to 0.001 and train all the models for a maximum of 100 epochs with a mini-batch
size of 32.
To evaluate the perceptual quality of our results, we perform a subjective listening test using the multiple
stimulushiddenreferenceanchor(MUSHRA)protocol(Series,2014)onanaudiowebevaluationtool(Schoef-
fler et al., 2018). We randomly select an audio signal from the UrbanSound8K test set for the practicing part.
In the test part, we extract ten samples by randomly selecting an audio signal per class from the test set.
We prepare four samples for each signal: reconstructed samples from RQ-VAE and RSQ-VAE, a white noise
signal as a hidden anchor, and an original sample as a hidden reference. Because RQ-VAE and RSQ-VAE
are applied to the normalized log-Mel spectrograms, we use the same HiFi-GAN vocoder (Kong et al., 2020)
as used in the audio generation paper (Liu et al., 2021) to convert the reconstructed spectrograms to the
waveform samples. The HiFi-GAN vocoder is trained on the training set of UrbanSound8K from scratch (Liu
et al., 2021). As the upper bound of quality of the reconstructed waveform is limited to the vocoder result
of the original spectrogram, we use the vocoder result for the reference. After listening to the reference,
assessors are asked to rate the four different samples according to their similarity to the reference on a scale
of 0 to 100. The post-processing of the assessors followed the paper (Series, 2014): assessors were excluded
from the aggregated scores if they rated the hidden reference for more than 15% of the test signals with a
score lower than 90. After the post-screening of assessors (Series, 2014), a total of ten assessors participated
in the test. Figure 11 shows the violin plots of the listening test. RSQ-VAE achieves better median listening
scores comapared with RQ-VAE. The scores of RSQ-VAE are better by a large margin especially in the case
of eight layers.
As a demonstration, we randomly select audio clips from our test split of UrbanSound8K and show their
reconstructed Mel spectrogram samples from RQ-VAE and RSQ-VAE in Figure 13. While the samples from
RQ-VAE have difficulty in reconstructing the sources with shared codebooks, the samples from RSQ-VAE
reconstruct the detailed features of the sources.
27Published in Transactions on Machine Learning Research (03/2024)
Bottom-up path Top-down path
(a) Case1: ℓ1={1,2},ℓ2={3,4}andℓ3={5,6}
Bottom-up path Top-down path (b) Case2: ℓ1={1,2,3,4},ℓ2={5,6}andℓ3={7}
Figure 12: Architecture of the hybrid model in Appendix F.3.
Table 7: Image generation on FFHQ (a detailed version of Table 3).
Model Latent size ( dl) Codebook size ( Kl) GAN rFID ↓FID↓
VQ-GAN (Esser et al., 2021b) 1621024 ✓ - 11.4
RQ-VAE (Lee et al., 2022a) 82×4 2048 ×4 ✓7.29 10.38
RSQ-VAE +RQ-Transformer (ours)82×4 2048×4 8.479.74
RSQ-VAE +Contextual RQ-Transformer (ours) 8.46
F.3 Empirical study of top-down layers
As a demonstration, we build two HQ-VAEs by combinatorially using both the injected top-down and the
residual top-down layers with three resolutions, w1=h1= 16,w2=h2= 32andw3=h3= 64. We
construct the architectures as described in Figure 12 and train them on CelebA-HQ. Figure 15 shows the
progressively reconstructed images for each case. We can see the same tendencies as in Figures 4a and 4c.
F.4 Applications of HQ-VAEs to generative tasks
In training RSQ-VAE on FFHQ, we borrow the encoder–decoder architecture from Lee et al. (2022a),
which is constructed by adding an encoder and decoder block to VQ-GAN (Esser et al., 2021b) to decrease
the resolution of the feature map by half. Table 7 compares our model with other VQ-based generative
models, where it can be seem that our model does not rely on the adversarial training framework but is
still competitive with RQ-VAE, which is trained with a PatchGAN (Isola et al., 2017). If we evaluate whole
pipelines including the VAE and prior model parts as generative models, our models outperform the baseline
model. Figure 16 shows the samples generated from RSQ-VAE with the contextual Transformer.
The encoder–decoder architecture for our SQ-VAE-2 on ImageNet is built in the same manner as is shown in
Figures 1 and 7. Our architecture structure most resembles that of VQ-VAE-2 because the latent structure
is the same. Because ImageNet with a resolution of 256 is a challenging dataset to compress to discrete
representations with feasible latent sizes, we use adversarial training for reconstructed images. Subsequently,
we train two Muse (Chang et al., 2023) prior models to approximate Q(Z1|c)andQ(Z2|Z1,c), where
c∈[1000]indicates the class label. Table 8 compares our model with other VQ-based generative models
without the technique of rejection sampling in terms of various metrics. As summarized in the table, our
model is competitive with the current state-of-the-art models. Figure 17 shows the generated samples from
SQ-VAE-2 with the Muse.
28Published in Transactions on Machine Learning Research (03/2024)
Table 8: Image generation on ImageNet (a detailed version of Table 4).
Model ParamsLatent size ♯of codesGAN rFID↓FID↓IS↑(dl) ( Kl)
VQ-VAE-2 (Razavi et al., 2019) 13.5B 322,642512,512 -∼31∼45
DALL-E (Ramesh et al., 2021) 12B 3228192 - 32.01 -
VQ-GAN (Esser et al., 2021b) 1.4B 16216384 ✓4.98 15.78 78.3
VQ-Diffusion (Gu et al., 2022) 370M 16216384 ✓4.98 11.89 -
MaskGIT (Chang et al., 2022) 228M 1621024 ✓2.28 6.18 182.1
ViT-VQGAN (Yu et al., 2022) 1.6B 3228192 ✓1.99 4.17 175.1
RQ-VAE (Lee et al., 2022a) 1.4B82×4 16384×4 ✓3.208.71 119.0
RQ-VAE (Lee et al., 2022a) 3.8B 7.55 134.0
Contextual RQ-Transformer (Lee et al., 2022b) 1.4B 82×4 16384×4 ✓3.20 3.41 224.6
HQ-TVAE (You et al., 2022) 1.4B 82,1628192,8192 ✓2.61 7.15 -
Efficient-VQGAN (Cao et al., 2023) - 1621024 ✓2.34 9.92 82.2
Efficient-VQGAN (Cao et al., 2023) N/A 3221024 ✓0.95 - -
SQ-VAE-2 (ours) 1.2B +0.56B 162,3222048,1024 ✓1.73 4.51 276.81
F.5 Perceptual loss for images
We found that the LPIPS loss (Zhang et al., 2018), which is a perceptual loss for images (Johnson et al.,
2016), works well with our HQ-VAE. However, we also noticed that simply replacing ∥x−fθ(Z1:L)∥2
2in
the objective function of HQ-VAE (Equations (6) and (14)) with an LPIPS loss LLPIPS (x,fθ(Z1:L))leads
to artifacts appearing in the generated images. We hypothesize that these artifacts are caused by the max-
pooling layers in the VGGNet used in LPIPS. Signals from VGGNet might not reach all of the pixels in
backpropagation due to the max-pooling layers. To mitigate this issue, we applied a padding-and-trimming
operation to both the generated image fθ(Z1:L)and the corresponding reference image xbefore the LPIPS
loss function. That is LLPIPS (pt [x],pt [fθ(Z1:L)]), where pt [ ]denotes our padding-and-trimming operator.
The PyTorch implementation of this operation is described below.
import random
import torch
import torch.nn.functional as F
def padding_and_trimming(
x_rec, # decoder output
x # reference image
):
_, _, H, W = x.size()
x_rec = F.pad(x_rec, (15, 15, 15, 15), mode=’replicate’)
x = F.pad(x, (15, 15, 15, 15), mode=’replicate’)
_, _, H_pad, W_pad = x.size()
top = random.randrange(0, 16)
bottom = H_pad - random.randrange(0, 16)
left = random.randrange(0, 16)
right = W_pad - random.randrange(0, 16)
x_rec = F.interpolate(x_rec[:, :, top:bottom, left:right],
size=(H, W), mode=’bicubic’, align_corners=False)
x = F.interpolate(x[:, :, top:bottom, left:right],
size=(H, W), mode=’bicubic’, align_corners=False)
return x_rec, x
Note that our padding-and-trimming operation includes downsampling with a random ratio. We assume that
this random downsampling provides a generative model with diversified signals in backpropagation across
training iterations, which makes the model more generalizable.
29Published in Transactions on Machine Learning Research (03/2024)
Figure 14 shows images reconstructed by an RSQ-VAE model trained with a normal LPIPS loss,
LLPIPS (x,fθ(Z1:L)), and ones reconstructed by an RSQ-VAE model trained with our modified LPIPS loss,
LLPIPS (pt [x],pt [fθ(Z1:L)]). As shown, our padding-and-trimming technique alleviates the artifacts issue.
For example, vertical line noise can be seen in the hairs in the images generated by the former model, but
those lines are removed from or softened in the images generated by the latter model. Indeed, our technique
improves rFID from 10.07 to 8.47.
F.6 Progressive coding
For demonstration purposes in Figure 4a, we incorporate the concept of progressive coding (Ho et al., 2020;
Shu & Ermon, 2022) into our framework, which helps hierarchical models to be more sophisticated in
progressive lossy compression and may lead to high-fidelity samples being generated. Here, one can train
SQ-VAE-2 to achieve progressive lossy compression (as in Figure 4a) by introducing additional generative
processes ˜xl∼N(˜xl;fθ(Z1:l),σ2
lI)forl∈[L]. We here derive the corresponding ELBO objective with this
concept. A more reasonable reconstruction can be obtained from only the higher layers (i.e., using only
low-resolution information Hr
ϕ(x)).
We consider corrupted data ˜xlforl∈[L], which can be obtained by adding noise, for example, ˜xl=x+ϵl.
We here adopt the Gaussian distribution ϵl.d∼N (0,vl)for the noises. Note that {σ2
l}L
l=1is set to be a
non-increasing sequence. We model the generative process by using only the top lgroups, as pl
θ(˜xl) =
N(˜xl;fθ(Z1:l),σ2
lI). The ELBO is obtained as
Jprog
SQ-VAE-2 =L/summationdisplay
l=1D
2logσ2
l+EQ(Z1:L,˜Z1:L|x)/bracketleftbigg∥x−fθ(Z1:l) +Dvl∥2
2
2σ2
l+∥˜Zl−Zl∥2
F
2s2
l−H(ˆPs2
l(Zl|˜Zl))/bracketrightbigg
.
(28)
In Section 5.3, we simply set vl= 0in the above objective when this technique is activated.
This concept can be also applied to the hybrid model derived in Appendix D by considering additional
generative processes pr
θ(˜xr) =N(˜xr;fθ(Z1:Lr),σ2
rI). The ELBO objective is as follows:
Jprog
HQ-VAE =R/summationdisplay
r=1D
2logσ2
r
+EQ(Z1:L,˜Y1:R|x)
R/summationdisplay
r=1∥x−fθ(Z1:Lr) +Dvr∥2
2
2σ2r+R/summationdisplay
r=1/vextenddouble/vextenddouble/vextenddoubleˆYr−/summationtext
l∈ℓrZl/vextenddouble/vextenddouble/vextenddouble2
F
2/summationtext
l∈ℓrs2
l−L/summationdisplay
l=1H(ˆPs2
l(Zl|˜Zl))
.(29)
In Section F.3, we simply set vl= 0in the above objective when this technique is activated.
30Published in Transactions on Machine Learning Research (03/2024)
(a) Source.
(b) RQ-VAE with 4 layers.
(c) RQ-VAE with 8 layers.
(d) RSQ-VAE with 4 layers.
(e) RSQ-VAE with 8 layers.
Figure 13: Mel spectrogram of (a) sources and (b)-(e) reconstructed samples of the UrbanSound8K dataset.
The left and right panels are audio clips of dog barking and drilling, respectively. We can see that the
RQ-VAEs struggle to reconstruct the sources with shared codebooks. In contrast, the reconstruction of
RSQ-VAE reflects the details of the source samples.
31Published in Transactions on Machine Learning Research (03/2024)
(a) Source
(b) RSQ-VAE trained with a normal LPIPS loss (rFID = 10.07)
(c) RSQ-VAE trained with our improved LPIPS loss (rFID = 8.47)
Figure 14: Reconstructed samples of FFHQ.
32Published in Transactions on Machine Learning Research (03/2024)
(a) Case1: ℓ1={1,2},ℓ2={3,4}andℓ3={5,6}
(b) Case2: ℓ1={1,2,3,4},ℓ2={5,6}andℓ3={7}
Figure 15: Reconstructed images and magnified differences of HQ-VAE on CelebA-HQ
Figure 16: Samples of FFHQ from RSQ-VAE with contextual RQ-Transformer (Lee et al., 2022b).
33Published in Transactions on Machine Learning Research (03/2024)
(a) The used class labels are “0: tench, Tinca tinca”, “1: goldfish”, and “7: cock”.
(b) The used class labels are “90: lorikeet”, “130: flamingo”, and “282: tiger cat”.
(c) The used class labels are “604: hourglass”, “724: pirate, pirate ship”, and “751: racer, race car, racing car”.
(d) The used class labels are “850: teddy, teddy bear”, “933: cheeseburger”, and “980: volcano”.
Figure 17: Samples of ImageNet from SQ-VAE-2 with Muse (Chang et al., 2023). (Top) Samples generated
only with the top layer, fθ(Z1). (Bottom) Samples generated with the top and bottom layers, fθ(Z1:2).
34