Published in Transactions on Machine Learning Research (09/2023)
k-Mixup Regularization for Deep Learning via Optimal
Transport
Kristjan Greenewald kristjan.h.greenewald@ibm.com
MIT-IBM Watson AI Lab, IBM Research
Anming Gu agu2002@bu.edu
Boston University
Mikhail Yurochkin mikhail.yurochkin@ibm.com
MIT-IBM Watson AI Lab, IBM Research
Justin Solomon jsolomon@mit.edu
Massachusetts Institute of Technology
Edward Chien edchien@bu.edu
Boston University
Reviewed on OpenReview: https: // openreview. net/ forum? id= lOegPKSu04
Abstract
Mixup is a popular regularization technique for training deep neural networks that improves
generalization and increases robustness to certain distribution shifts. It perturbs input
training data in the direction of other randomly-chosen instances in the training set. To
better leverage the structure of the data, we extend mixup in a simple, broadly applicable
way tok-mixup, which perturbs k-batches of training points in the direction of other k-
batches. The perturbation is done with displacement interpolation, i.e. interpolation under
the Wasserstein metric. We demonstrate theoretically and in simulations that k-mixup
preserves cluster and manifold structures, and we extend theory studying the efficacy of
standard mixup to the k-mixup case. Our empirical results show that training with k-mixup
further improves generalization and robustness across several network architectures and
benchmark datasets of differing modalities. For the wide variety of real datasets considered,
the performance gains of k-mixup over standard mixup are similar to or larger than the
gains of mixup itself over standard ERM after hyperparameter optimization. In several
instances, in fact, k-mixup achieves gains in settings where standard mixup has negligible to
zero improvement over ERM.
1 Introduction
Standard mixup (Zhang et al., 2018) is a data augmentation approach that trains models on weighted averages
of random pairs of training points. Averaging weights are typically drawn from a beta distribution β(α,α),
with parameter αsuch that the generated training set is vicinal, i.e., it does not stray too far from the original
dataset. Perturbations generated by mixup may be in the direction of anyother data point instead of being
informed by local distributional structure. As shown in Figure 1, this property is a key weakness of mixup
that can lead to poor regularization when distributions are clustered or supported on a manifold. With larger
α, the procedure can result in averaged training points with incorrect labels in other clusters or in locations
that stray far from the data manifold.
To address these issues, we present k-mixup, which averages random pairs of setsofksamples from the
training dataset. This averaging is done using optimal transport, with displacement interpolation . The sets
1Published in Transactions on Machine Learning Research (09/2023)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
No mixup
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
1-mixup
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
32-mixup
Figure 1: Outputs of a fully-connected network trained on three synthetic datasets for binary classification,
with no mixup (ERM), 1-mixup, and 32-mixup regularization ( α= 1). Note that ERM under-regularizes
(visible through the jagged boundaries), 1-mixup over-regularizes (visible through over-smoothing), and
that 32-mixup better captures local structure (visible through less blur, increased contrast) while retaining
reasonable smoothing between the classes.
ofksamples are viewed as discrete distributions and are averaged as distributions in a geometric sense. If
k= 1, we recover standard mixup regularization. Figures 1 and 2 illustrate how k-mixup produces perturbed
training datasets that better match the global cluster or manifold support structure of the original training
dataset. The constraints of optimal transport are crucial, as for instance a nearest-neighbors approach would
avoid the cross-cluster matches necessary for smoothing.1Figure 3 illustrates the distribution of possible
matchings for a sample point and shows non-zero likelihood for these cross-cluster matches. In Section 5,
we provide empirical results that justify the above intuition. The resulting method is easy to implement,
computationally cheap, and versatile. Our contributions are as follows.
•Empirical results:
–We show improved generalization results on standard benchmark datasets showing k-mixup with
k>1consistently improves on standard mixup, where αis optimized for both methods. The
improvements are consistently similar in magnitude or larger than those of 1-mixup over basic
ERM.
–On image datasets, a heuristic of k= 16outperforms 1-mixup in nearly all cases.
–We show that k-mixup significantly improves robustness to certain distribution shifts (additive
noise and adversarial samples) over 1-mixup and ERM.
•Theoretical results:
–We argue that as kincreases, the interpolated samples are more and more likely to remain
within the data manifold (Section 3.1).
–In the clustered setting, we provide an argument that shows inter-cluster regularization interpo-
lates nearest points and better smooths interpolation of labels (Section 3.2).
–We extend the theoretical analysis of Zhang et al. (2020) and Carratino et al. (2020) to our
k-mixup setting, showing that it leverages local data distribution structure ( Diof Eq. 1) to
make more informed regularizations (Section 4).
Related works. We tackle issues noted in the papers on adaptive mixup (Guo et al., 2019) and manifold
mixup (Verma et al., 2018). The first refers to the problem as “manifold intrusion” and seeks to address it by
training data point-specific weights αand considering convex combinations of more than 2 points. Manifold
mixup deals with the problem by relying on the network to parameterize the data manifold, interpolating
in the hidden layers of the network. We show in Section 5 that k-mixup can be performed in hidden layers
to boost performance of manifold mixup. A related approach is that of GAN-mixup (Sohn et al., 2020),
which trains a conditional GAN and uses it to generate data points between different data manifolds. The
approaches above require training additional networks and are far more complex than our k-mixup method.
1To see this, note that because nearest-neighbors can be a many-to-one matching, nearly all matches would be intra-cluster
between points of the same class and thus provide few/no interpolated labels, particularly missing any interpolations in the voids
between classes where interpolation is most important. As additional support, we ran 20 Monte Carlo trials of the CIFAR-10
experiment below with a k= 16-nearest-neighbors strategy. It failed to outperform even 1-mixup (0.09% worse). Further
discussion is presented in supplement Section J, with an analogue of Figure 3 for a k-nearest-neighbors strategy.
2Published in Transactions on Machine Learning Research (09/2023)
k= 1: 1-mixup k= 32:k-mixup
Figure 2: Optimal transport couplings and vicinal datasets for k= 1(left) andk= 32(right) in 3 simple
datasets. In the bottom row, α= 1was used to generate vicinal datasets of size 512.
The recent local mixup method (Baena et al., 2022) uses a distance-based approach for modifying mixup.
This method retains the random matching strategy of mixup, but scales the contribution of a vicinal point
to the objective function according to its distance from the original training point. As noted previously,
such methods that employ random matching will fail to provide substantive data smoothing for the output
function between clusters or high-density regions in the training data.
PuzzleMix (Kim et al., 2020) also combines optimal transport ideas with mixup, extending CutMix (Yun et al.,
2019) to combine pairs of images. PuzzleMix uses transport to shift saliency regions of images, producing
meaningful combinations of input training data. Their use of optimal transport is fundamentally different
from ours and does not generalize to non-image data. There are several other image-domain specific works
that are in this vein, including CoMix (Kim et al., 2021), Automix (Liu et al., 2021), and Stackmix (Chen
et al., 2021).
Performing optimal transport between empirical samples of distributions has been considered in studies of
thesample complexity of Wasserstein distances (e.g. Weed & Bach (2019)). Unlike most settings, in our
application the underlying source and target distributions are the same; the theoretical investigation of a
generalization of variance called k-variance by Solomon et al. (2020) considers a similar setting. In other
works, transport between empirical samples has been dubbed minibatch optimal transport and has been used
in generative models (Genevay et al., 2018; Fatras et al., 2020) and domain adaptation (Damodaran et al.,
2018; Fatras et al., 2021).
2 Generalizing Mixup
Standard mixup. Mixup uses a training dataset of feature-target pairs {(xi,yi)}N
i=1, where the target yiis
a one-hot vector for classification. Weighted averages of training points construct a vicinal dataset:
(˜xλ
ij,˜yλ
ij) := (λxi+ (1−λ)xj,λyi+ (1−λ)yj).
λis sampled from a beta distribution, β(α,α), with parameter α>0usually small so that the averages are
near an endpoint. Using this vicinal dataset, empirical risk minimization (ERM) becomes:
Emix
1(f) :=Ei,j,λ/bracketleftbig
ℓ/parenleftbig
f/parenleftbig
˜xλ
ij/parenrightbig
,˜yλ
ij/parenrightbig/bracketrightbig
,
wherei,j∼U{ 1,...,N},λ∼β(α,α),fis a proposed feature-target map, and ℓis a loss function. Effectively,
one trains on datasets formed by averaging random pairs of training points. As the training points are
randomly selected, this construction makes it likely that the vicinal data points may not reflect the local
structure of the dataset, as in the clustered or manifold-support setting.
k-mixup. To generalize mixup, we sample two random subsets of ktraining points{(xγ
i,yγ
i)}k
i=1and
{(xζ
i,yζ
i)}k
i=1. For compactness, let xγ:={xγ
i}k
i=1andyγ:={yγ
i}k
i=1denote the feature and target sets
3Published in Transactions on Machine Learning Research (09/2023)
Figure 3: Locally-informed matching distributions Difork= 32, for a randomly selected point (see Equation
1 for explicit definition). These distributions reflect local manifold and cluster structure. Note that a
nearest-neighbors approach would not provide cross-cluster matchings (see Figure 7 in Appendix J).
(and likewise for ζ). A weighted average of these subsets is formed with displacement interpolation and
used as a vicinal training set. This concept is from optimal transport (see, e.g., Santambrogio (2015)) and
considers (xγ,yγ)and(xζ,yζ)as uniform discrete distributions ˆµγ,ˆµζover their supports. In this setting,
the optimal transport problem becomes a linear assignment problem (Peyré & Cuturi, 2019). The optimal
map is described by a permutation σ∈Skminimizing the cost:
W2
2(ˆµγ,ˆµζ) =1
kk/summationdisplay
i=1∥xγ
i−xζ
σ(i)∥2
2.
Here,σcan be found efficiently using the Hungarian algorithm (Bertsimas & Tsitsiklis, 1997). Figure 2 gives
intuition for this identification. When compared to the random matching used by standard mixup, our pairing
is more likely to match nearby points and to make matchings that better respect local structure—especially
by having nontrivial probability for cross-cluster matches between nearby points on the two clusters (see
Figure 3).
Given a permutation σand weight λ, the displacement interpolation between (xγ,yγ)and(xζ,yζ)is:
DIλ((xγ,yγ),(xζ,yζ)) :=/braceleftig
λ(xγ
i,yγ
i) + (1−λ)(xζ
σ(i),yζ
σ(i))/bracerightigk
i=1.
As in standard mixup, we draw λ∼β(α,α). For the loss function, we consider sampling k-subsets of the
Nsamples at random, which we can mathematically describe as choosing γ,ζ∼U{ 1,...,/parenleftbigN
k/parenrightbig
}for which
{{(xγ
i,yγ
i)}k
i=1}(N
k)
γ=1are the possible subsets.2This yields an expected loss
Emix
k(f) =Eγ,ζ,λ/bracketleftbig
ℓ(f(DIλ(xγ,xζ)),DIλ(yγ,yζ))/bracketrightbig
.
The localized nature of the matchings makes it more likely that the averaged labels will smoothly interpolate
over the decision boundaries. A consequence is that k-mixup is robust to higher values of α, since it is no
longer necessary to keep λclose to 0 or 1 to avoid erroneous labels. This can be seen in our empirical results
in Section 5 and theoretical analysis in Section 4.
k,α, and Root Mean Squared Perturbation Distance. Whenαis kept fixed and kis increased,
the perturbations become more local, and the distance of matchings (and thus perturbations) decreases.
We found that more sensible comparisons across values of kcan be obtained by increasing αin concert
withk,3so that the average perturbation’s squared distance remains constant—bearing in mind that large
squared distance values may not be achievable for high values of k. In effect, this is viewing the root mean
squared perturbation distance as the parameter, instead of α. Computation details are found in Section 5.1.
Throughout training, this tuned αis kept fixed. We typically pick an αfork= 1, get the associated root
mean squared distance ( ξ), and increase αaskincreases to maintain the fixed value of ξ.
2These possible subsets need not be enumerated since we optimize the loss with stochastic gradient descent.
3Note that this adjustment is of course not necessary for parameter tuning in practice.
4Published in Transactions on Machine Learning Research (09/2023)
Figure 4: Injectivity radius example. Various ϵ-neighborhoods, Bϵ(S), have been illustrated for a curve
S⊂R2. Intuitively, the topology of Bϵ(S)no longer reflects that of Swhenϵ>RS, the injectivity radius.
To be precise, Bϵ(S)is no longer homotopy equivalent to S(see Hatcher (2000) for definition).
Pseudocode and Computational Complexity. We emphasize the speed and simplicity of our method.
We have included a brief PyTorch pseudocode in Section 5.1 below and note that with CIFAR-10 and k= 32,
the use ofk-mixup added about one second per epoch on GPU. Section 5.1 also contains a more extended
analysis of computational cost, showing that there is little computational overhead to incorporating k-mixup
in training compared to ERM alone.
3 Manifold & Cluster Structure Preservation
Using an optimal coupling for producing vicinal data points makes it likely that vicinal data points reflect
local dataset structure. Below we argue that as kincreases, the vicinal couplings will preserve manifold
support, preserve cluster structure, and interpolate labels between clusters.
3.1 Manifold support
Suppose our training data is drawn from a distribution µon ad-dimensional embedded submanifold Sin
X×Y⊂ RM, whereXandYdenote feature and target spaces. We define an injectivity radius:
Definition 3.1 (Injectivity radius) .LetBϵ(S) ={p∈X×Y| d(p,S)<ϵ}denote the ϵ-neighborhood of S
where d(p,S)is the Euclidean distance from ptoS. Define the injectivity radius RSofSto be the infimum
of theϵ’s for which Bϵ(S)is not homotopy equivalent to S.
AsSis embedded, Bϵ(S)is homotopy equivalent to Sfor small enough ϵ, soRS>0. Essentially, Definition
3.1 grows an ϵ-neighborhood until the boundary intersects itself. See Figure 4 for a schematic example that
illustrates this. We have the following:
Proposition 3.2. For a data distribution µsupported on an embedded submanifold Swith injectivity radius
RS, with high probability any constant fraction 1−δ(for any fixed δ∈(0,1]) of the interpolated samples
induced by k-mixup will remain within Bϵ(S), forklarge enough.
The proof of this proposition is in supplement Section D. Hence, for large enough k, the interpolation induced
by optimal transport will approximately preserve the manifold support structure. While the theory requires
highkto achieve a tight bound, our empirical evaluations show good performance in the small kregime.
Some schematic examples are shown in Figures 1, 2, and 3.
3.2 Clustered distributions
With a clustered data distribution, we preserve global structure by including many within-cluster and
inter-cluster matches, where inter-cluster matches correspond (approximately) to the nearest points across
clusters. This contrasts with 1-mixup, which, when the number of clusters is large, is biased towards primarily
inter-cluster matches and does not seek to provide any structure to these random matches. These approximate
characterizations of k-mixup are achieved exactly as k→∞, as argued below.
To make precise the notion of a clustered distribution, we adopt the (m,∆)-clusterable definition used by
Weed & Bach (2019); Solomon et al. (2020). In particular, a distribution µis(m,∆)-clusterable if supp(µ)
5Published in Transactions on Machine Learning Research (09/2023)
lies in the union of mballs of radius at most ∆. Now, if our training samples (xi,yi)are sampled from such a
distribution, where the clusters are sufficiently separated, then optimal transport will prioritize intra-cluster
matchings over cross-cluster matchings.
Lemma 3.3. Draw two batches of samples {pi}N
i=1and{qi}N
i=1from a (m,∆)-clusterable distribution, where
the distance between any pair of covering balls is at least 2∆. Ifriandsidenote the number of samples in
clusteriin batch 1 and 2, respectively, then the optimal transport matching will have1
2/summationtext
i|ri−si|cross-cluster
matchings.
The proof of the above (supplement Section E) involves the pigeonhole principle and basic geometry. We also
argue that the fraction of cross-cluster identifications approaches zero as k→∞and characterize the rate of
decrease. The proof (supplement Section F) follows via Jensen’s inequality:
Theorem 3.4. Given the setting of Lemma 3.3, with probability masses p1,...,pm, and two batches
of sizekmatched with optimal transport, the expected fraction of cross-cluster identifications is
O/parenleftig
(2k)−1/2/summationtextm
i=1/radicalbig
pi(1−pi)/parenrightig
.
Notethattheabsolutenumberofcross-clustermatchesstillincreasesas kincreases, providingmoreinformation
in the voids between clusters. We emphasize that the (m,∆)assumption corresponds to a setting where
clusters are well-separated such that cross-cluster identifications are as unlikely as possible. Hence, in real
datasets, the fraction of cross-cluster identifications will be larger than indicated in Theorem 3.4. Finally,
we show that these cross-cluster matches are of length close to the distance between the clusters with high
probability (proved in supplement Section G), i.e., the endpoints of the match lie in the parts of each cluster
closest to the other cluster. This rests upon the fact that we are considering the W2cost, for which small
improvements to long distance matchings yield large cost reductions in squared Euclidean distance.
Theorem 3.5. Suppose density phas support on disjoint compact sets (clusters) A,Bwhose boundaries are
smooth, where p>0throughoutAandB. LetDbe the Euclidean distance between AandB, and letRA,RB
be the radii of A,Brespectively. Define Aϵto be the subset of set Athat is less than (1 +ϵ)Ddistance from
B, and define Bϵsimilarly. Consider two batches of size kdrawn from pand matched with optimal transport.
Then, forklarge enough, with high probability all cross-cluster matches will have an endpoint each in Aϵand
Bϵ, whereϵ=max(RA,RB)2
D2.
Theorem 3.5 implies that for large k, the vicinal distribution created by sampling along the cross-cluster
matches will almost entirely lie in the voids between the clusters. If the clusters correspond to different
classes, this will directly encourage the learned model to smoothly interpolate between the class labels as one
transitions across the void between clusters. This is in contrast to the random matches of 1-mixup, which
create vicinal distributions that can span a line crossing any part of the space without regard for intervening
clusters. The behaviors noted by Theorems 3.4 and 3.5 are visualized in Figures 1, 2, and 3, showing that
k-mixup provides smooth interpolation between clusters, and strengthens label preservation within clusters.
4 Regularization Expansions
Two recent works analyze the efficacy of 1-mixup perturbatively (Zhang et al., 2020; Carratino et al., 2020).
Both consider quadratic Taylor series expansions about the training set or a simple transformation of it, and
they characterize the regularization terms that arise in terms of label and Lipschitz smoothing. We adapt
these expansions to k-mixup and show that the resulting regularization is more locally informed via the
optimal transport coupling.
In both works, perturbations are sampled from a globally informed distribution, based upon all other samples
in the data distribution. In k-mixup, these distributions are defined by the optimal transport couplings.
Given a training point xi, we consider all k-samplingsγthat might contain it, and all possible k-samplingsζ
that it may couple to. A locally-informed distribution is the following:
Di:=1/parenleftbigN−1
k−1/parenrightbig/parenleftbigN
k/parenrightbig(N−1
k−1)/summationdisplay
γ=1(N
k)/summationdisplay
ζ=1δσγζ(xi), (1)
6Published in Transactions on Machine Learning Research (09/2023)
whereσγζdenotes the optimal coupling between k-samplingsγandζ. This distribution will be more heavily
weighted on points that xiis often matched with. An illustration of this distribution for a randomly selected
point in our synthetic examples is visible in Figure 3. We use “locally-informed” in the sense of upweighting
of points closer to the point of interest that are likely to be matched to it by k-mixup.
Zhang et al. (2020) expand about the features in the training dataset DX:={x1,...,xn}, and the perturba-
tions in the regularization terms are sampled from D. We generalize their characterization to k-mixup, with
Dreplaced byDi. Focusing on the binary classification problem for simplicity, we assume a loss of the form
ℓ(f(x),y) =h(f(x))−y·f(x)for some twice differentiable handf. This broad class of losses includes the
cross-entropy for neural networks and all losses arising from generalized linear models.
Theorem 4.1. Assuming a loss ℓas above, the k-mixup loss can be written as:
Emix
k(f) =Estd+3/summationdisplay
j=1Rj+Eλ∼β(α+1,α)[(1−λ)2ϕ(1−λ)],
where lima→0ϕ(a) = 0,Estddenotes the standard ERM loss, and the three Riregularization terms are:
R1=Eλ∼β(α+1,α)[1−λ]
n×/summationdisplayN
i=1(h′(f(xi))−yi)∇f(xi)TEr∼Di[r−xi]
R2=Eλ∼β(α+1,α)[(1−λ)2]
2n×N/summationdisplay
i=1h′′(f(xi))∇f(xi)TEr∼Di[(r−xi)(r−xi)T]∇f(xi)
R3=Eλ∼β(α+1,α)[(1−λ)2]
2n×N/summationdisplay
i=1(h′(f(xi))−yi)Er∼Di[(r−xi)∇2f(xi)(r−xi)T].
A proof is given in Section H of the supplement and follows from some algebraic rearrangement and a Taylor
expansion in terms of 1−λ. The higher-order terms are captured by Eλ∼β(α+1,α)[(1−λ)2ϕ(1−λ)].Estd
represents the constant term in this expansion, while the regularization terms Rirepresent the linear and
quadratic terms. These effectively regularize ∇f(xi)and∇2f(xi)with respect to local perturbations r−xi
sampled fromDi, ensuring that our regularizations are locally-informed. In other words, the regularization
terms vary over the support of the dataset, at each point penalizing the characteristics of the locally-informed
distribution rather than those of the global distribution. This allows the regularization to adapt better to
local data (e.g. manifold) structure. For example, R2andR3penalize having large gradients and Hessians
respectively along the directions of significant variance of the distribution Diof pointsxiis likely to be
matched to. When k= 1, thisDiwill not be locally informed, and will instead effectively be a global variance
measure. As kincreases, theDiwill instead be dominated by matches to nearby clusters, better capturing
the smoothing needs in the immediate vicinity of xi. Notably, the expansion is in the feature space alone,
yielding theoretical results in the case of 1-mixup on generalization and adversarial robustness.
An alternative approach by Carratino et al. (2020) characterizes mixup as a combination of a reversion to
mean followed by random perturbation. In supplement Section I, we generalize their result to k-mixup via a
locally-informed mean and covariance.
5 Implementation and Experiments
5.1k-mixup Implementation & Computational Cost
Figure 5 shows pseudocode for our implementation of one epoch of k-mixup training, in the style of Figure 1
of Zhang et al. (2018).4Also provided in Algorithm 1 is the procedure used in the experiments to adjust αas
kincreases (denoted αk). Whileξcannot be evaluated in closed form as a function of α, sinceξmonotonically
increases with αit is sufficient to iteratively increase αkuntil the desired ξis reached. The while loop involves
only computing the empirical expectation of a simple function of a scalar beta-distributed random variable
and is therefore fast to compute.
4Python code for applying k-mixup to CIFAR10 can be found at https://github.com/AnmingGu/kmixup-cifar10 .
7Published in Transactions on Machine Learning Research (09/2023)
# y1, y2 should be one-hot vectors
for (x1, y1), (x2, y2) in zip (loader1, loader2):
idx = numpy.zeros_like(y1)
for iin range (x1.shape[0] // k):
cost = scipy.spatial.
distance_matrix(x1[i * k:(i+1) * k], x2[i * k:(i+1) * k])
_, ix = scipy.optimize.linear_sum_assignment(cost)
idx[i * k:(i+1) * k] = ix + i * k
x2 = x2[idx]
y2 = y2[idx]
lam = numpy.random.beta(alpha, alpha)
x = Variable(lam * x1 + (1 - lam) * x2)
y = Variable(lam * y1 + (1 - lam) * y2)
optimizer.zero_grad()
loss(net(x), y).backward()
optimizer.step()
Figure 5:k-mixup implementation
Algorithm 1 Choosingαkto maintain constant ξ
Require: Chosenα1, desiredk, trialsN, constantc>1, threshold γ, dataset of interest.
1:UsingNtrials, compute ¯ξ1as the empirical average of the squared distance between two random points
in the dataset.
2:UsingN/ktrials, compute ¯ξkas the empirical average of the squared Wasserstein-2 distance between two
randomk-samples drawn from the dataset.
3:UsingNtrials, compute empirical average ¯λ1ofmin(λ2
1,(1−λ1)2)whereλ1∼β(α1,α1).
4:ξ←¯ξ1¯λ1
5:Initializeαk=α,¯λk=¯λ1.
6:while ¯λk¯ξk<ξandαk<γdo
7:αk←αkc
8:UsingNtrials, compute empirical average ¯λkofmin(λ2
k,(1−λk)2)whereλk∼β(αk,αk).
9:end while
10:Outputαk.
Computational cost. While the cost of the Hungarian algorithm is O(k3), it provides kdata points for
regularization, yielding an amortized O(k2)complexity per data point. For the smaller values of kthat we
empirically consider, approximate Sinkhorn-based methods (Cuturi, 2013) are slower in practice, and the
Hungarian cost remains small relative to that of gradient computation (e.g. for CIFAR-10 and k= 32, the
Hungarian algorithm costs 0.69 seconds per epoch in total). Computing the distance matrix input to the
OT matching costs O(k2d)wheredis dimension, yielding an amortized O(kd)complexity per data point.
With the high dimensionality of CIFAR, a naive GPU implementation of this step of the computation adds
about 0.5 seconds per epoch. Note that, on our hardware, the overall cost of an epoch is greater than 30
seconds. Moreover, training convergence speed is unaffected in terms of epoch count, unlike in manifold
mixup (Verma et al., 2018), which in our experience converges slower and has larger computational overhead.
Overall, therefore, there is little computational downside to generalizing 1-mixup to k-mixup, and, as we will
see, the potential for performance gains.
5.2 Empirical Results
The simplicity of our method allows us to test the efficacy of k-mixup over a wide range of datasets and
domains: 5 standard benchmark image datasets, 5 UCI tabular datasets, and a speech dataset; employing a
variety of fully connected and convolutional neural network architectures. Across these experiments we find
thatk-mixup for k>1consistently improves upon 1-mixup after hyperparameter optimization. The gains
8Published in Transactions on Machine Learning Research (09/2023)
are generally on par with the improvements of 1-mixup over no-mixup (ERM). Interestingly, when the mean
perturbation distances (squared) are constrained to be at a fixed level ξ2,k-mixup for k >1still usually
improves upon 1-mixup, with the improvement especially stark for larger perturbation sizes. In the tables
below,ξis used to denote the root mean square of the perturbation distances, and αdenotes the αused in
thek= 1case to achieve this ξ. We show here sweeps over kandξ, choosingkandξin practice is discussed
in Supplement Section A.
Unless otherwise stated, our training is done over 200 epochs via a standard SGD optimizer, with learning
rate 0.1 decreased at epochs 100 and 150, momentum 0.9, and weight decay 10−4. Note that throughout,
we focus on comparing ERM, 1-mixup, and k-mixup on different architectures, rather than attempting to
achieve a new state of the art on these benchmarks.
Image datasets. Our most extensive testing was done on image datasets, given their availability and the
efficacy of neural networks in this application domain. In Table 1, we show our summarized error rates across
various benchmark datasets and network architectures (all results are averaged over 20 random trials).5For
each combination of dataset and architecture, we report the improvement of k-mixup over 1-mixup, allowing
for optimization over relevant parameters. In this table, hyperparameter optimization refers to optimizing
over a discrete set of the designated hyperparameter values listed in the detailed hyperparameter result tables
shown below for each dataset and setting. As can be seen, the improvement is on par with that of 1-mixup
over ERM. We also show the performance of k= 16(a heuristic choice for k), which performed well across
all experiments, outperforming 1-mixup in nearly all instances. This k= 16-mixup performance is notable as
it only requires optimizing a single hyperparameter α/ξ, the same hyperparameter used for 1-mixup.
Table 1: Summary of image dataset error rate results (in percent, all results averaged over 20 trials). Note
that for each setting, k-mixup with hyperparameter optimization outperforms both ERM and optimized
1-mixup. We also show 16-mixup as a heuristic that only involves optimizing α/ξ, i.e. the hyperparameter
space is the same as 1-mixup. This heuristic outperforms ERM and 1-mixup in all but one instance, where
it matches the performance of 1-mixup. Note that, on average, the improvement of k-mixup over 1-mixup
tends to be similar in magnitude to the improvement of 1-mixup over ERM.
Dataset (Confidence) Architecture ERM1-mixup ( ξopt.)k-Mixup (k,ξopt.) 16-Mixup (heuristic, ξopt.)
MNIST (±.02) LeNet 0.95 0.76 0.66 0.74
CIFAR-10 (±.03) ResNet18 5.6 4.18 4.02 4.02
(±.03) DenseNet-BC-190 3.70 3.29 2.85 2.85
(±.09) WideResNet-101 11.6 11.53 11.25 11.25
CIFAR-100 (±.05) DenseNet-BC-190 18.91 18.91618.31 18.31
SVHN (±.02) ResNet18 3.37 2.93 2.78 2.93
Tiny ImageNet (±.06) ResNet18 38.50 37.58 35.67 35.67
Table 2: Results for MNIST with a LeNet architecture
(no mixup (ERM) error: 0.95%), averaged over 20 trials
(±.02confidence on test error). Note the best k-mixup
beats the best 1-mixup by 0.1%; on the same order as
the 0.19% improvement of 1-mixup over ERM.
α=.05α=.1α=.2α=.5α=1α=10α=100
kξ= 0.5ξ= 0.6ξ= 0.8ξ= 1.2ξ= 1.4ξ= 2.1ξ= 2.2
10.790.790.760.860.831.051.26
20.850.790.900.760.830.941.14
40.910.800.830.810.850.890.95
80.800.810.780.790.780.810.83
160.770.820.800.750.800.740.75
320.790.750.770.790.800.760.71
640.800.820.770.780.780.660.71Full parameter sweeps for all settings shown in Ta-
ble 1 are in Appendix C, with two presented below
that are indicative of the general pattern that we
see as we vary α/ξandk. In Table 2, we show
results for MNIST (LeCun & Cortes, 2010) with a
slightly modified LeNet architecture to accommodate
grayscale images, and for Tiny ImageNet with a Pre-
Act ResNet-18 architecture as in Zhang et al. (2018).
Each table entry is averaged over 20 Monte Carlo
trials. Here and throughout the remainder of the
paper, error bars are reported for the performance
results.7In the MNIST results, for each ξthe best
generalization performance is for some k > 1, i.e.
5As a result, Table 1 summarizes results from training 3680 neural network models in total.
6Achieved with α= 0, i.e. equivalent to ERM.
7These error bars are the one standard deviation of the Monte Carlo average, where for brevity we report only the worst such
variation over the elements of the corresponding results table.
9Published in Transactions on Machine Learning Research (09/2023)
0 50 100 150 200
Epoch5060708090% correct classification
k=1
k=32
(a) Test acc., α= 1.
0 50 100 150 200
Epoch304050607080% correct classification
k=1
k=32 (b) Train acc., α= 1.
0 50 100 150 200
Epoch405060708090% correct classification
k=1
k=32 (c) Test acc., α= 10.
0 50 100 150 200
Epoch3040506070% correct classification
k=1
k=32 (d) Train acc., α= 10.
Figure 6: Training convergence of k= 1andk= 32-mixup on CIFAR-10, averaged over 20 random trials.
Both train at roughly the same rate ( k= 32slightly faster), the train accuracy discrepancy is due to the
more class-accurate vicinal training distribution created by higher k-mixup.
k-mixup outperforms 1-mixup for all perturbation sizes. The lowest error rate overall is achieved with k= 64
andξ= 2.1:0.66%, an improvement of 0.1% over the best 1-mixup and an improvement of 0.29% over ERM.
For Tiny ImageNet, we again see that for each α/ξ, the best generalization performance is for some k>1
(note that the ξare larger for Tiny ImageNet than MNIST due to differences in normalization and size of
the images). The lowest error rate overall is achieved with k= 16andξ= 1000:35.67%, and improvement
of 1.91% over the best 1-mixup and an improvement of 2.83% over ERM. In both datasets, we see that for
each perturbation size value ξ, the best generalization performance is for some k>1, but that the positive
effect of increasing kis seen most clearly for larger ξ. This strongly supports the notion that k-mixup is
significantly more effective than 1-mixup at designing an effective vicinal distribution for larger perturbation
budgets. The lowest overall error rates are achieved for relatively high ξand highk.
Table 3: Results for Tiny ImageNet with a ResNet18 ar-
chitecture (no mixup (ERM) error: 38.50%), averaged
over 20 trials (±.06confidence on test error). Note the
bestk-mixup beats the best 1-mixup by 1.91%; better
than the 0.92% improvement of 1-mixup over ERM.
α=.1α=.2α=.5α=1α=10
kξ= 9.7ξ= 13ξ= 18ξ= 22ξ= 32
138.4738.2637.7937.6137.58
238.4838.0637.8937.4337.00
438.5138.1337.6737.4436.21
838.4238.0537.6737.1935.86
1638.4738.1037.6037.2835.67
3238.4538.0437.6037.2935.88Additionally, we show training curves (performance
as a function of epoch) demonstrating that the use of
k-mixup does not affect training convergence. These
are shown for ResNet-18 on CIFAR-10 in Figure
6, which is indicative of what is seen across all our
experiments. The training speeds (test accuracy in
terms of epoch) for 1-mixup and 32-mixup show no
loss of convergence speed with k= 32-mixup, with
(if anything) k= 32showing a slight edge. The
discontinuity at epoch 100 is due to our reduction of
the learning rate at epochs 100 and 150 to aid con-
vergence (used throughout our image experiments).
The train accuracy shows a similar convergence pro-
file between 1- and 32-mixup; the difference in absolute accuracy here (and the reason it is less than the
test accuracy) is because the training distribution is the mixup-modified vicinal distribution. The curve for
k= 32is higher, especially for α= 10, because the induced vicinal distribution and labels are more consistent
with the true distribution, due to the better matches from optimal transport. The large improvement in
train accuracy is remarkable given the high dimension of the CIFAR-10 data space, since it indicates that
k= 32-mixup is able to find significantly more consistent matches than k= 1-mixup.
UCI datasets. To demonstrate efficacy on non-image data, we tried k-mixup on UCI datasets (Dua & Graff,
2017) of varying size and dimension: Iris (150 instances, dim. 4), Breast Cancer Wisconsin-Diagnostic (569
instances, dim. 30), Abalone (4177 instances, dim. 8), Arrhythmia (452 instances, dim. 279), and Phishing
(11055 instances, dim. 30). For Iris, we used a 3-layer network with 120 and 84 hidden units; for Breast
Cancer, Abalone, and Phishing, we used a 4-layer network with 120, 120, and 84 hidden units; and lastly,
for Arrhythmia we used a 5-layer network with 120, 120, 36, and 84 hidden units. For these datasets we
used a learning rate of 0.005 instead of 0.1. Each entry is averaged over 20 Monte Carlo trials. Test error
rate is shown in Figure 4. k-mixup improves over 1-mixup in each case, although for Arrhythmia no mixup
10Published in Transactions on Machine Learning Research (09/2023)
outperforms both. In these small datasets, the best performance seems to be achieved with relatively small α
(0.05,0.1) and larger k(4 or greater).
Table 4: Test error on UCI datasets using fully connected networks, averaged over 20 random trials.
Dataset αξk= 1k= 2k= 4k= 8k= 16
Abalone 0.05.0771.3771.3271.2371.5471.47
(ERM: 72.32) 0.1.0971.3271.4171.8271.0571.07
1.0.2071.5671.8571.2671.2970.90
Arrhythmia 0.0513.9735.1534.8433.8835.1634.97
(ERM:32.06)0.117.8334.1436.1334.2935.6433.39
1.042.2834.2535.5133.7833.5533.64
Cancer 0.051910.329.089.528.639.35
(ERM: 11.25) 0.1.2610.429.759.929.1410.25
1.0.5912.3110.8310.239.859.15Dataset αξk= 1k= 2k= 4k= 8k= 16
Iris 0.05.076.704.203.302.503.70
(ERM: 4.10) 0.1.106.002.902.703.503.60
1.0.227.003.902.903.802.90
Phishing 0.05.283.303.143.053.173.06
(ERM: 3.43) 0.1.383.303.363.353.343.26
1.0.854.694.554.274.184.20
Speech dataset . Performance is also tested on a speech dataset: Google Speech Commands (Warden, 2018).
Results with a simple LeNet architecture are in Table 5. Each table entry is averaged over 20 Monte Carlo
trials (±.014confidence on test performance). We augmented the data in the same way as Zhang et al. (2018),
i.e. we sample the spectrograms from the data using a sampling rate of 16 kHz and equalize their sizes at
160×101. We also use similar training parameters: we train for 30 epochs using a learning rate of 3×10−3
that is divided by 10 every 10 epochs. The improvement of the best k-mixup over the best 1-mixup is 1.11%,
with the best k-mixup performance being for k= 16and largeα/ξ. Note this improvement is greater than
the 0.83% improvement of 1-mixup over ERM.
Table 5: Google Speech Commands
test error using LeNet architecture (no
mixup (ERM) error: 12.26%), aver-
aged over 20 Monte Carlo trials. Note
the bestk-mixup beats the best 1-
mixup by 1.11%, greater than the
0.83% improvement of 1-mixup over
ERM.
α=.1α=.2α=.5α= 1α= 10
kξ= 14ξ= 19ξ= 26ξ= 31ξ= 45
111.8911.5511.4311.5512.16
211.7611.4811.3211.3811.62
411.6811.3611.1811.1911.20
811.8311.3611.1411.1410.84
1611.7211.3611.1111.0410.32Toy datasets. Note that for completeness, in Appendix B we
provide quantitative results for the toy datasets of Figures 1, 2, and
3, confirming the qualitative analysis therein.
Distribution shift. As mentioned above, a key benefit of mixup is
that it (approximately) smoothly linearly interpolates between classes
and thereby should provide a degree of robustness to distribution
shifts that involve small perturbation of the features Zhang et al.
(2018). Here we test two such forms of distribution shift: additive
Gaussian noise and FGSM white box adversarial attacks (a full
exploration of the infinite set of possible distribution shifts is beyond
the scope of this paper). Additive Gaussian noise can arise from
unexpectednoiseintheimagingsensor, andtestingadversarialFGSM
attacks on mixup was introduced in the original mixup paper Zhang
et al. (2018) as a much more difficult perturbation test than i.i.d.
additive noise. As in Zhang et al. (2018), we limit the experiment to
basic FGSM attacks, since iterative PGD attacks are too strong, making any performance improvements seem
less relevant in practice and calling into question the realism of using PGD as a proxy for distribution shifts.
Additive Gaussian noise results for CIFAR-10 and various levels ϵof noise and mixup parameter αare shown
in Figure 6.8Note that the k= 16outperforms k= 1for all noise levels by as much as 4.29%, and ERM by
as much as 16%.
Following the experimental setup of Zhang et al. (2018), Figure 7 shows results on white-box adversarial
attacks generated by the FGSM method (implementation of Kim (2020)9) for various values of maximum
adversarial perturbation. As in Zhang et al. (2018), the goal of this test is not to achieve results comparable
to the much more computationally intensive methods of adversarial training. This would be impossible for a
non-adversarial regularization approach. We show CIFAR-10 accuracy on white-box FGSM adversarial data
(10000 points), where the maximum adversarial perturbation is set to ϵ/255; performance is averaged over
30 Monte Carlo trials ( ±0.6confidence). Note that the highest koutperforms k= 1uniformly by as much
8Smaller values of αwere tried, but this decreased performance for all kvalues.
9Software has MIT License
11Published in Transactions on Machine Learning Research (09/2023)
Table 6: Additive noise: Error for CIFAR-10 ResNet-18 with additive Gaussian noise of standard deviation
ϵ/255(results averaged over 30 trials, ±0.6confidence). k= 16-mixup outperforms 1-mixup and ERM in
each instance (best performance for each ϵin bold).
α= 2 α= 5 α= 10
NoiseERMk=1k=4k=16k=1k=4k=16k=1k=4k=16
ϵ=815.7211.1011.6511.0011.3911.1911.2811.8811.2811.53
ϵ=1022.2115.4716.0614.3415.7814.6914.2015.4314.5614.37
ϵ=1229.5521.6921.7518.7521.1919.0617.7120.5618.4517.69
ϵ=1437.9329.6928.3423.8627.4323.7221.6225.9022.6221.61
as 6.12%, and all k>1-mixups outperform or statistically match k= 1-mixup for all attack sizes. Similar
results for MNIST are in Table 7(b), with the FGSM attacks being somewhat less effective. Here again, the
highestkperforms the best for all attack sizes.
The improved robustness shown by k-mixup speaks to a key goal of mixup, that of smoothing the predictions
in the parts of the data space where no/few labels are available. This smoothness should make adversarial
attacks require greater magnitude to successfully “break” the model.
Table 7: Adversarial shifts: error on white-box FGSM attacks. Parameter αchosen to maximize k= 1
performance. Large k-mixup outperforms 1-mixup in each instance.
kϵ=.5ϵ=1ϵ=2ϵ=4ϵ=8ϵ= 16
120.6326.2531.0736.7248.5574.78
220.4025.7029.8934.3443.7872.22
420.7326.2930.5934.9443.9471.20
820.5826.0630.3134.5043.5170.18
1620.2125.5029.5733.8043.3268.66kϵ=.5ϵ=1ϵ=2ϵ=4ϵ=8ϵ= 16
11.231.662.373.887.7018.10
21.121.502.113.587.4419.81
41.071.371.943.257.3022.02
81.081.401.883.126.9221.00
160.881.111.502.425.2816.44
320.851.071.392.274.9114.95
(a) CIFAR-10 ( ±0.6confidence). α= 0.5. (b) MNIST ( ±0.1confidence). α= 10.
Manifold mixup. We have also compared to manifold mixup (Verma et al., 2018), which aims to interpolate
data points in deeper layers of the network to get more meaningful interpolations. This leads to the natural
idea of doing k-mixup in these deeper layers. We use the settings in Verma et al. (2018), i.e., for ResNet18,
the mixup layer is randomized (coin flip) between the input space and the output of the first residual block.10
Table 8: Comparison to manifold mixup
approach. “ k-manifold mixup” test error
on CIFAR-10 using ResNet18 architecture,
averaged over 20 Monte Carlo trials ( ±.03
confidence). Compare Table 1 and observe
that this “manifold k-mixup” matches the
standard manifold mixup but both under-
perform our proposed k-mixup, which
achievesa4.02%errorratewith k= 16.
kα=.1α=.2α=.5α= 1α= 10
15.00 4.73 4.41 4.26 4.81
25.09 4.78 4.44 4.31 4.71
45.01 4.78 4.45 4.25 5.02
85.08 4.78 4.42 4.28 4.71
165.14 4.82 4.42 4.31 4.53
325.14 4.91 4.58 4.47 4.54Results for CIFAR-10 with a ResNet18 architecture are in Table
8. Note that in this experiment, the α’s shown are used for
allkwithout adjustment since the matches across k-samples
happen at random layers and cannot be standardized as simply.
Numbers in this experiment are averaged over 20 Monte Carlo
trials (±.03confidence on test performance). Manifold 1-mixup
is matched by manifold k-mixup with k= 4, but both are
outperformed by standard k-mixup (Table 1). We therefore
do not find a benefit to using “ k-manifold-mixup” over
our proposed k-mixup. We also tried randomizing over
(a) the outputs of all residual blocks and (b) the outputs of
(lower-dimensional) deep residual blocks only, but found that
performance of both manifold 1-mixup and manifold k-mixup
degrades in these cases. This latter observation underscores
that mixup in hidden layer manifolds is not guaranteed to be
effective and can require tuning.
10Seehttps://github.com/vikasverma1077/manifold_mixup
12Published in Transactions on Machine Learning Research (09/2023)
6 Conclusions and Future Work
The experiments above demonstrate that k-mixup improves the generalization and robustness gains achieved
by 1-mixup. This is seen across a diverse range of datasets and network architectures. It is simple to
implement, adds little computational overhead to conventional 1-mixup training, and may also be combined
with related mixup variants. As seen in the theory presented in Sections 3 and 4, as kincreases, the induced
regularization more accurately reflects the local structure of the training data, especially in the manifold
support and clustered settings. Empirical results show that performance is relatively robust to variations in
k, especially when normalizing for similar perturbation distances (squared), ensuring that extensive tuning is
not necessary.
With the notable exception of the larger improvement on Tiny ImageNet, our experiments show the
improvement on high-dimensional datasets is sometimes smaller than on lower dimensional datasets (recall
that classic mixup also has somewhat small gains over ERM in these settings). This difference may be
influenced by the diminishing value of Euclidean distance for characterizing dataset geometry in high
dimensions (Aggarwal et al., 2001), but intriguingly this effect was not remedied by doing OT in the lower-
dimensional manifolds created by the higher layers in our manifold mixup experiments. In future work we
will consider alternative metric learning strategies, with the goal of identifying alternative high-dimensional
metrics for displacement interpolation of data points.
Acknowledgements
The MIT Geometric Data Processing group acknowledges the generous support of Army Research Office grants
W911NF2010168 and W911NF2110293, of Air Force Office of Scientific Research award FA9550-19-1-031,
of National Science Foundation grants IIS-1838071 and CHS-1955697, from the CSAIL Systems that Learn
program, from the MIT–IBM Watson AI Laboratory, from the Toyota–CSAIL Joint Research Center, from a
gift from Adobe Systems, and from a Google Research Scholar award.
References
Charu C Aggarwal, Alexander Hinneburg, and Daniel A Keim. On the surprising behavior of distance metrics
in high dimensional space. In International conference on database theory , pp. 420–434. Springer, 2001.
Raphael Baena, Lucas Drumetz, and Vincent Gripon. Preventing manifold intrusion with locality: Local
mixup, 2022.
Dimitris Bertsimas and John Tsitsiklis. Introduction to Linear Optimization . Athena Scientific, 1st edition,
1997. ISBN 1886529191.
Luigi Carratino, Moustapha Cissé, Rodolphe Jenatton, and Jean-Philippe Vert. On Mixup Regularization.
arXiv:2006.06049 [cs, stat] , June 2020. URL http://arxiv.org/abs/2006.06049 . arXiv: 2006.06049.
John Chen, Samarth Sinha, and Anastasios Kyrillidis. Stackmix: A complementary mix algorithm, 2021.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural
information processing systems , 26:2292–2300, 2013.
Bharath Bhushan Damodaran, Benjamin Kellenberger, Rémi Flamary, Devis Tuia, and Nicolas Courty.
DeepJDOT: Deep Joint Distribution Optimal Transport for Unsupervised Domain Adaptation. In Vittorio
Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision – ECCV 2018 , pp.
467–483, Cham, 2018. Springer International Publishing. ISBN 978-3-030-01225-0.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
Kilian Fatras, Younes Zine, Rémi Flamary, Remi Gribonval, and Nicolas Courty. Learning with minibatch
Wasserstein : asymptotic and gradient properties. In Proceedings of the Twenty Third International
Conference on Artificial Intelligence and Statistics , pp. 2131–2141. PMLR, June 2020. URL https:
//proceedings.mlr.press/v108/fatras20a.html . ISSN: 2640-3498.
13Published in Transactions on Machine Learning Research (09/2023)
Kilian Fatras, Thibault Sejourne, Rémi Flamary, and Nicolas Courty. Unbalanced minibatch Optimal
Transport; applications to Domain Adaptation. In Proceedings of the 38th International Conference on
Machine Learning , pp. 3186–3197. PMLR, July 2021. URL https://proceedings.mlr.press/v139/
fatras21a.html . ISSN: 2640-3498.
Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning Generative Models with Sinkhorn Divergences.
InProceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics , pp.
1608–1617. PMLR, March 2018. URL https://proceedings.mlr.press/v84/genevay18a.html . ISSN:
2640-3498.
Hongyu Guo, Yongyi Mao, and Richong Zhang. MixUp as Locally Linear Out-of-Manifold Regularization.
Proceedings of the AAAI Conference on Artificial Intelligence , 33(01):3714–3722, July 2019. ISSN 2374-3468.
doi: 10.1609/aaai.v33i01.33013714. URL https://ojs.aaai.org/index.php/AAAI/article/view/4256 .
Number: 01.
Allen Hatcher. Algebraic topology . Cambridge Univ. Press, Cambridge, 2000. URL https://cds.cern.ch/
record/478079 .
S. K. Katti. Moments of the Absolute Difference and Absolute Deviation of Discrete Distributions. The
Annals of Mathematical Statistics , 31(1):78–85, 1960. ISSN 0003-4851. URL https://www.jstor.org/
stable/2237495 . Publisher: Institute of Mathematical Statistics.
Hoki Kim. Torchattacks: A pytorch repository for adversarial attacks. arXiv preprint arXiv:2010.01950 ,
2020.
Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle Mix: Exploiting Saliency and Local Statistics
for Optimal Mixup. In International Conference on Machine Learning , pp. 5275–5285. PMLR, November
2020. URL http://proceedings.mlr.press/v119/kim20b.html . ISSN: 2640-3498.
JangHyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint mixup
with supermodular diversity. In International Conference on Learning Representations , 2021. URL
https://openreview.net/forum?id=gvxJzw8kW4b .
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.
com/exdb/mnist/ .
Zicheng Liu, Siyuan Li, Di Wu, Zhiyuan Chen, Lirong Wu, Jianzhu Guo, and Stan Z. Li. Unveiling the power
of mixup for stronger classifiers, 2021.
Gabriel Peyré and Marco Cuturi. Computational Optimal Transport: With Applications to Data Science.
Foundations and Trends ®in Machine Learning , 11(5-6):355–607, February 2019. ISSN 1935-8237, 1935-
8245. doi: 10.1561/2200000073. URL https://www.nowpublishers.com/article/Details/MAL-073 .
Publisher: Now Publishers, Inc.
Filippo Santambrogio. Optimal Transport for Applied Mathematicians: Calculus of Variations, PDEs, and
Modeling . Progress in Nonlinear Differential Equations and Their Applications. Birkhäuser Basel, 2015.
ISBN 978-3-319-20827-5. doi: 10.1007/978-3-319-20828-2. URL https://www.springer.com/gp/book/
9783319208275 .
Jy Yong Sohn, Jaekyun Moon, Kangwook Lee, and Dimitris Papailiopoulos. Gan-mixup: Augmenting across
data manifolds for improved robustness. ICML Workshop on Uncertainity and Robustness in Deep Learning ,
2020.
Justin Solomon, Kristjan Greenewald, and Haikady N Nagaraja. k-variance: A clustered notion of variance.
arXiv preprint arXiv:2012.06958 , 2020.
VikasVerma, AlexLamb, ChristopherBeckham, AmirNajafi, AaronCourville, IoannisMitliagkas, andYoshua
Bengio. Manifold Mixup: Learning Better Representations by Interpolating Hidden States. September
2018. URL https://openreview.net/forum?id=rJlRKjActQ .
14Published in Transactions on Machine Learning Research (09/2023)
Table 9: Test error on toy datasets, averaged over 5 Monte Carlo trials.
α=.25α=1α=4α=16α=64
kξ=.10ξ=.15ξ=.20ξ=.23ξ=.25
15.017.889.8510.289.75
24.285.195.876.195.79
42.933.344.103.863.92
82.502.773.243.152.78
162.422.262.542.432.30α=.25α=1α=4α=16α=64
kξ=.50ξ=.78ξ= 1.0ξ= 1.1ξ= 1.2
10.000.006.7949.6049.80
20.010.001.5347.8249.74
40.000.001.418.259.58
80.000.000.380.620.63
160.000.000.080.070.22α=.25α=1α=4α=16α=64
kξ= 1.2ξ= 1.9ξ= 2.4ξ= 2.8ξ= 3.0
10.839.5231.8437.3836.18
20.493.3028.4132.2132.75
40.321.3415.4927.5329.23
80.350.452.678.142.76
160.400.330.450.270.27
(a) One Ring (b) Four Bars (c) Swiss Roll
Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint
arXiv:1804.03209 , 2018.
Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of empirical
measures in wasserstein distance. Bernoulli , 25(4A):2620–2648, 2019.
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. CutMix:
Regularization Strategy to Train Strong Classifiers With Localizable Features. pp. 6023–6032, 2019.
URL https://openaccess.thecvf.com/content_ICCV_2019/html/Yun_CutMix_Regularization_
Strategy_to_Train_Strong_Classifiers_With_Localizable_Features_ICCV_2019_paper.html .
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond Empirical Risk
Minimization. February 2018. URL https://openreview.net/forum?id=r1Ddp1-Rb .
Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How Does Mixup Help
With Robustness and Generalization? September 2020. URL https://openreview.net/forum?id=
8yKEo06dKNo .
A Hyperparameter tuning
While as noted in the main text, the large value of k= 16withαoptimized consistently performs well,
increasingkdoes not always improve performance monotonically. This, however, is to be expected in any
real data scenario. Hence in practice, it is often appropriate to search over k. This is not too difficult as in
our experiments we found it sufficient to only try powers of 2, and performance generally is smoothly varying
overα. Several search approaches work well:
1.Very simple : setα= 1and search over k(powers of 2). It can be seen from our experiments that
except for one UCI dataset this approach outperforms or matches the 1-mixup performance at any α.
2.More complex but still relatively simple : search over ξfor 1-mixup, fix the best such ξ, and
then search over k(in powers of 2 up to k= 16or 32). This approach always outperforms 1-mixup
and does not add much hyperparameter search overhead.
3.Full hyperparameter grid search : Not too expensive for many neural networks, for instance a
full grid search (with kpowers of 2) for ResNet-18 on CIFAR-10 requires only a few hours of time on
our medium-sized cluster. If the model will be deployed in high-stakes or high-volume applications,
full search would be feasible even for larger networks.
B Test error on toy datasets.
For completeness, we provide quantitative results for the toy datasets of Figures 1, 2, and 3 (denoted “One
Ring,” “Four Bars,” and “Swiss Roll”) in Table 9, continuing the intuition-building discussion in each of
those figures. We used a fully-connected 3-layer neural network (130 and 120 hidden units). As the datasets
are well-clustered and have no noise, smoothing is not needed for generalization and performance without
mixup is typically 100%. Rather than beating ERM, applying mixup to these datasets instead aims to build
15Published in Transactions on Machine Learning Research (09/2023)
intuition, providing a view to the propensity of each variant to oversmooth, damaging performance. For each
dataset and each perturbation size ξ, higherk-mixup outperforms 1-mixup, with k= 16providing the best
performance in all but one instance. These results quantitatively confirm the intuition built in Figures 1, 2,
and 3 that k-mixup regularization more effectively preserves these structures in data, limiting losses from
oversmoothing at a fixed mean perturbation size.
C Additional Image Dataset Parameter Sweeps
In this section, we show the parameter sweep tables for the remaining rows of Table 1.
Table 10 below shows results for CIFAR-10, using the PreAct ResNet-18 architecture as in Zhang et al. (2018).
Again, increasing kfor fixedξtends to improve generalization performance, especially in the high- ξregime.
The best performance overall is achieved at k= 16withξ= 16.7. While the best k-mixup performance
exceeds that of the best 1-mixup by 0.16%, recall that in this setting, 1-mixup outperforms ERM by only 1.4%
(Zhang et al., 2018), so when combined with the low overall error rate, small gains are not surprising. Results
for DenseNet and WideResNet architectures can be found in Table 11, with the best k-mixup outperforming
the best 1-mixup by 0.44% and 0.28% respectively. Note that in the case of DenseNet, k= 16outperforms or
(statistically) matches k= 1for all values of ξ.
Table 10: Results for CIFAR-10 with ResNet18 architecture (no mixup (ERM) error: 5.6%), averaged over
20 Monte Carlo trials ( ±.03confidence on test performance). Difference between best k-mixup and best
1-mixup is 0.16%; for fixed high α(α= 100), the improvement increases to 1.19%.
α=.05α=.1α=.2α=.5α=1α=10α=100
kξ= 5.6ξ= 7.4ξ= 10.0ξ= 13.8ξ= 16.7ξ= 24.0ξ= 27.2
15.014.68 4.41 4.24 4.18 4.95 5.67
24.924.69 4.46 4.13 4.03 4.58 5.46
44.884.68 4.52 4.13 4.03 4.48 5.19
84.914.77 4.51 4.21 4.08 4.42 4.92
164.924.77 4.48 4.234.02 4.40 4.75
324.984.78 4.58 4.24 4.16 4.36 4.48
Table 11: CIFAR-10 test error for DenseNet-BC-190 and WideResNet-101 architectures. For DenseNet, the
difference between best k-mixup and best 1-mixup is 0.44%, for fixed high ξ(α= 20.2), the improvement
increases to 0.65%. For WideResNet, the difference between best k-mixup and best 1-mixup is 0.28%, for
fixed highξ(ξ= 20.2), the improvement increases to 1.25%.
α=.5α= 1α= 2α= 4
kξ= 13.8ξ= 16.7ξ= 19.0ξ= 20.2
13.35 3.29 3.42 3.57
163.38 2.982.85 2.92
(a) DenseNet-BC-190 architecture( ±.03confidence, no mixup (ERM) error: 3.7%)
α=.05α=.2α=.5α= 1α= 2α= 4
kξ= 5.6ξ= 10.0ξ= 13.8ξ= 16.7ξ= 19.0ξ= 20.2
111.54 11.53 11.62 11.78 12.25 12.99
411.36 11.47 11.27 11.41 11.59 12.16
1611.53 11.5911.25 11.34 11.38 11.74
(b) WideResNet-101 architecture ( ±.09confidence, no mixup (ERM) error: 11.6%)
Table 12 shows results for CIFAR-100 and SVHN, with DenseNet-BC-190 and ResNet-18 architectures
respectively. As before, for fixed ξ, the best performance is achieved for some k>1. The improvement of the
bestk-mixup over the best 1-mixup is 2.14% for CIFAR-100 and 0.15% for SVHN. For fixed high α, the
k-mixup improvement over 1-mixup rises to 2.85% for CIFAR-100 and 0.52% for SVHN, possibly indicating
that the OT matches yield better interpolation between classes, aiding generalization.
16Published in Transactions on Machine Learning Research (09/2023)
Table 12: CIFAR-100 (DenseNet-BC-190) and SVHN (ResNet18), averaged over 20 trials.
α=.5α=1α=2α=4
kξ= 3.4ξ= 4.2ξ= 4.8ξ= 5.4
128.52 27.76 20.45 21.53
818.35 18.78 19.16 19.71
1618.3318.31 18.53 18.85α=.1α=.2α=.5α=1α=10
kξ= 3.8ξ= 5.1ξ= 6.9ξ= 9.5ξ= 11.6
13.29 3.22 2.99 2.93 3.89
23.32 3.19 2.96 2.79 3.65
43.30 3.25 2.942.78 3.55
83.28 3.20 3.01 2.86 3.44
163.26 3.24 3.04 2.93 3.37
(a) CIFAR-100 error ( ±.05confidence,
no mixup (ERM) error: 18.91%) (b) SVHN error ( ±.02confidence,
no mixup (ERM) error: 3.37%)
D Proof of Proposition 3.2
Finite-sample convergence results for empirical measures (Theorem 9.1 of Solomon et al. (2020), Weed &
Bach (2019)) imply that for an arbitrary sampling of kpoints ˆµk, we have
W2
2(µ,ˆµk)≤O(k−2/d)
with 1−1/k2probability. The triangle inequality then implies that the Wasserstein-2 distance between our
batches ofksamples will tend to 0 at the same asymptotic rate, specifically
W2(ˆµγ
k,ˆµζ
k)≤W2(µ,ˆµζ
k) +W2(ˆµγ
k,µ)≤O(k−1/d)
again with 1−1/k2probability.
Now, recalling the definition of the optimal coupling permutation σ(i), we have
W2
2(ˆµγ
k,ˆµζ
k) =1
kk/summationdisplay
i=1∥xγ
i−xζ
σ(i)∥2
2.
Hence,
1
kk/summationdisplay
i=1∥xγ
i−xζ
σ(i)∥2
2≤O(k−2/d)
with 1−1/k2probability. Thus, for any I⊆[1,k]with∥xγ
i−xζ
σ(i)∥2
2>k−1/dfor alli∈I,
O(k−2/d)≥1
kk/summationdisplay
i=1∥xγ
i−xζ
σ(i)∥2
2>|I|
kk−1/d,
implying
|I|<O(k1−1/d) =k·O(k−1/d)≤kδ,
where the last inequality holds for any δ∈(0,1]givenklarge enough.
In essence, the fraction of matches that are long-distance (i.e. those in I) is bounded by δfor large enough k.
Under these conditions, the set of short-distance matches (i.e. those in ¯Ithe complement of I) satisfy
|¯I|≥ (1−δ)k,
where by definition, ∥xγ
i−xζ
σ(i)∥2
2≤k−1/dfor alli∈¯I. Crucially, for any chosen ϵ, we havek−1/d<ϵfork
large enough, so for klarge enough
∥xγ
i−xζ
σ(i)∥2<ϵ,∀i∈¯I.
By definition of k-mixup, for all i∈¯I, the corresponding mixup interpolated point will be an interpolation
betweenxγ
iandxζ
σ(i), i.e.
λxγ
i+ (1−λ)xζ
σ(i)
forλ∈[0,1]. Since allxγ
ilie inSand for alli∈¯I,∥xγ
i−xζ
σ(i)∥2<ϵ, the mixup interpolated point will lie in
Bϵ(S). The proposition results.
17Published in Transactions on Machine Learning Research (09/2023)
E Proof of Lemma 3.3
Firstly, observe that the maximum number of within-cluster matches is
/summationdisplay
imin(ri,si)
by definition, and the total number of matches overall must equal/summationtext
isi=/summationtext
iri. Hence the number of
cross-cluster matchings must be larger than or equal to
/summationdisplay
iri−/summationdisplay
imin(ri,si) =/summationdisplay
imax(0,ri−si),
equivalently,/summationdisplay
isi−/summationdisplay
imin(ri,si) =/summationdisplay
imax(si−ri,0).
Averaging these implies the number of cross-cluster matchings cannot be smaller than
1
2/summationdisplay
i(max(ri−si,0) + max(0,si−ri)) =1
2/summationdisplay
i|ri−si|.
It remains to show that the number of cross-cluster matchings cannot exceed1
2/summationtext
i|ri−si|. We argue by
contradiction and prove the result for m= 2first. Suppose that the number of cross-cluster matchings
exceeds|r1−s1|. Then by the pigeonhole principle (i.e. via the fact that all points must have exactly one
other matched point), there must be at least two such matchings. WLOG, let us say these cross-cluster
matches are between piandqi, andpi+1andqi+1, wherepiandqi+1must be in the same cluster, as are
qiandpi+1since there are only two clusters. By our assumption on the spacing of the clusters, the cost of
matchingpitoqiandpi+1toqi+1at least
>2(2∆)2.
However, consider the alternative matching of pitoqi+1andpi+1toqi. These are both intra-cluster matchings,
which by our assumption on the radius of the clusters must have total cost smaller than or equal to
≤2(2∆)2.
This matching has smaller cost than the inter-cluster matching strategy, so this contradicts optimality of the
inter-cluster pairing.
In the scenario with mclusters, an analogous argument works. As above, |ri−si|is the number of cluster i
elements that must be matched in a cross-cluster fashion. If there are more than the minimum1
2/summationtext
i|ri−si|,
by the pigeonhole principle, there must be additional cross-cluster matches that form a cycle in the graph
over clusters. As above, the cost would be reduced by matching points within the same clusters, so this
contradicts optimality.
Since the number of cross-cluster matchings cannot be less than or exceed1
2/summationtext
i|ri−si|, it must equal this
number and the lemma results.
F Proof of Theorem 3.4
By Lemma 3.3, if riandsidenote the number of points in cluster ifrom batch 1 and 2, the resulting number
of cross-cluster matches is1
2/summationtext
i|ri−si|. As the samples for our batches are i.i.d., these random variables ( ri,
si, whereriis independent of si) each (marginally) follow a simple binomial distribution B(k,pi). We can
bound the expectation of this quantity with Jensen’s inequality:
(E[|ri−si|])2≤E[|ri−si|2] = 2Var(ri) = 2kpi(pi−1).
18Published in Transactions on Machine Learning Research (09/2023)
This implies that
1
2/summationtext
iE|ri−si|
k≤/summationtext
i/radicalbig
2kpi(1−pi)
2k= (2k)−1
2m/summationdisplay
i=1/radicalbig
pi(1−pi),
yielding the bound in the theorem. It is also possible to get an exact rate with some hypergeometric identities
(Katti, 1960), but these simply differ by a constant factor, so we omit the exact expressions here.
G Proof of Theorem 3.5
By the smooth boundary and positive density assumptions, we know that P(Aδ)>0andP(Bδ)>0for any
δ>0. Hence, for fixed δandklarge enough, we know that with high probability the sets AδandBδeach
contain more points than the number of cross-cluster identifications.
Now consider AϵandBϵforϵ= 2δ+ (max(RA,RB)2)/(2D2). All cross-cluster matches need to be assigned.
The cost of assigning a cross-cluster match to a point in Aδand a point in Bδis at most (1 + 2δ)2D2(since
we are using W2). Furthermore, the cost of assigning a cross-cluster match that contains a point in Aoutside
Aϵand an arbitrary point in Bis at least (1 +ϵ)2D2. Consider the difference between these two costs:
(1 +ϵ)2D2−(1 + 2δ)2D2= (2(ϵ−2δ) +ϵ2−4δ2)D2>2D2max(RA,RB)2
2D2≥R2
A.
Since this difference >0and we have shown Aδcontains sufficient points for handling all assignments, this
assignment outside of Aϵwill only occur if there is a within-cluster pair which benefits from using the available
point inAϵmore than is lost by not giving it to the cross-cluster pair ( >R2
A). The maximum possible benefit
gained by the within-cluster pair is the squared radius of A, i.e.R2
A. Since we have shown that the lost cost
for the cross-cluster pair is bigger than R2
A, we have arrived at a contradiction. The proof is similar for the B
side.
We have thus shown that for klarge enough (depending on δ), with high probability all cross-cluster
matches have an endpoint each in AϵandBϵwhereϵ= 2δ+ (max(RA,RB)2)/(2D2). Setting δ=
(max(RA,RB)2)/(4D2)completes the proof.
H Proof of Theorem 4.1
We mostly follow the notation and argument of Zhang et al. (2020) (c.f. Lemma 3.1), modifying it for our
setting. There they consider sampling λ∼Beta (α,β)from an asymmetric Dirichlet distribution. Here,
we assume a symmetric Dirichlet distribution, such that α=β, simplifying most of the expressions. The
analogous results hold in the asymmetric case with simple modifications.
Consider the probability distribution ˜Dλwith probability distribution: β(α+1,α). Note that this distribution
is more heavily weighted towards 1 across all α, and forα<1, there is an asymptote as you approach 1.
Let us adopt the shorthand notation ˜xi,σγζ(i)(λ) :=λxγ
i+ (1−λ)xζ
σγζfor an interpolated feature point. The
manipulations below are abbreviated, as they do not differ much for our generalization.
Emix
k(f) =1
k/parenleftbigN
k/parenrightbig2Eλ∼β(α,α)(N
k)/summationdisplay
γ,ζ=1k/summationdisplay
i=1h(f(˜xi,σγζ(i)(λ)))−(λyγ
i+ (1−λ)yζ
σγζ(i))f(˜xi,σγζ(i)(λ))
=1
k/parenleftbigN
k/parenrightbig2Eλ∼β(α,α)EB∼Bern (λ)(N
k)/summationdisplay
γ,ζ=1k/summationdisplay
i=1B[h(f(˜xi,σγζ(i)(λ)))−yγ
if(˜xi,σγζ(i)(λ))]
+ (1−B)[h(f(˜xi,σγζ(i)(λ)))−yζ
σγζ(i)f(˜xi,σγζ(i)(λ))]
=1
k/parenleftbigN
k/parenrightbig2(N
k)/summationdisplay
γ,ζ=1k/summationdisplay
i=1Eλ∼β(α+1,α)h(f(˜xi,σγζ(i)(λ)))−yγ
if(˜xi,σγζ(i)(λ))
19Published in Transactions on Machine Learning Research (09/2023)
For the third equality above, the ordering of sampling for λandBhas been swapped via conjugacy:
λ∼β(α,α),B|λ∼Bern (λ)is equivalent to B∼U{ 0,1},λ|B∼β(α+B,α + 1−B). This is combined with
the fact that ˜xi,σγζ(i)(1−λ) = ˜xσγζ(i),i(λ)to get the last line above.
Now we can swap the sums, grouping over the initial point to express this as the following:
Emix
k(f) =1
NN/summationdisplay
i=1Eλ∼β(α+1,α)Er∼Dih(f(λxi+ (1−λ)r))−yγ
if(λxi+ (1−λ)r),
where the probability distribution Diis as described in the text.
The remainder of the argument performs a Taylor expansion of the loss term h(f(λxi+ (1−λ)r))−yγ
if(λxi+
(1−λ)r)in terms of 1−λ, and is not specific to our setting, so we refer the reader to Appendix A.1 of (Zhang
et al., 2020). for the argument.
Ik-mixup as Mean Reversion followed by Regularization
Theorem I.1. Define (˜xγ
i,˜yγ
i)as
˜xγ
i= ¯xγ
i+¯θ(xγ
i−¯xγ
i)
˜yγ
i= ¯yγ
i+¯θ(yγ
i−¯yγ
i),
where ¯xγ
i=1
(N
k)/summationtext(N
k)
ζ=1xζ
σγζ(i)and ¯yγ
i=1
(N
k)/summationtext(N
k)
ζ=1yζ
σγζ(i)are expectations under the matchings and θ∼
β[1/2,1](α,α). Further, denote the zero mean perturbations
˜δγ
i= (θ−¯θ)xγ
i+ (1−θ)xζ
σγζ(i)−(1−¯θ)¯xγ
i
˜ϵγ
i= (θ−¯θ)yγ
i+ (1−θ)yζ
σγζ(i)−(1−¯θ)¯yγ
i.
Then thek-mixup loss can be written as
EOTmixup
k(f) =1/parenleftbigN
k/parenrightbig(N
k)/summationdisplay
γ=1Eθ,ζ/bracketleftigg
1
kk/summationdisplay
i=1ℓ(˜yγ
i+ ˜ϵγ
i,f(˜xγ
i+˜δγ
i))/bracketrightigg
.
The mean ¯xγ
ibeing shifted toward is exactly the mean of the locally-informed distribution Di. Moreover, the
covariance structure of the perturbations is detailed in the proof (simplified in Section I.1) and is now also
derived from the local structure of the distribution, inferred from the optimal transport matchings.
Proof.This argument is modelled on a proof of Carratino et al. (2020), so we adopt analogous notation and
highlight the differences in our setting and refer the reader to Appendix B.1 of that paper for any omitted
details. First, let us use shorthand notation for the interpolated loss function:
mγζ
i(λ) =ℓ(f(λxγ
i+ (1−λ)xζ
σγζ(i)),λyγ
i+ (1−λ)yζ
σγζ(i)).
Then the mixup objective may be written as:
Emix
k(f) =1
k/parenleftbigN
k/parenrightbig2(N
k)/summationdisplay
γ,ζ=1k/summationdisplay
i=1Eλmγζ
i(λ).
Asλ∼β(α,α), we may leverage the symmetry of the sampling function and use a parameter θ∼β[1/2,1](α,α)
to write the objective as:
Emix
k(f) =1/parenleftbigN
k/parenrightbig(N
k)/summationdisplay
γ=1ℓi,whereℓi=1
k/parenleftbigN
k/parenrightbig(N
k)/summationdisplay
ζ=1k/summationdisplay
i=1Eθmγζ
i(θ).
20Published in Transactions on Machine Learning Research (09/2023)
To obtain the form of the theorem in the text, we introduce auxiliary variables ˜xγ
i,˜yγ
ito represent the
mean-reverted training points:
˜xγ
i=Eθ,ζ/bracketleftig
θxγ
i+ (1−θ)xζ
σγζ(i)/bracketrightig
˜yγ
i=Eθ,ζ/bracketleftig
θyγ
i+ (1−θ)yζ
σγζ(i)/bracketrightig
,
and˜δγ
i,˜ϵγ
ito denote the zero mean perturbations about these points:
˜δγ
i=θxγ
i+ (1−θ)xζ
σγζ(i)−Eθ,ζ/bracketleftig
θxγ
i+ (1−θ)xζ
σγζ(i)/bracketrightig
˜ϵγ
i=θyγ
i+ (1−θ)yζ
σγζ(i)−Eθ,ζ/bracketleftig
θyγ
i+ (1−θ)yζ
σγζ(i)/bracketrightig
.
These reduce to the simplified expressions given in the theorem if we recall that θandζare independent
random variables. Note that both the mean-reverted points and the perturbations are informed by the local
distributionDi.
I.1 Covariance structure
As in (Carratino et al., 2020), it is possible to come up with some simple expressions for the covariance
structure of the local perturbations, hence we write out the analogous result below. As the argument is very
similar to that in (Carratino et al., 2020), we omit it.
Lemma I.2. Letσ2denote the variance of β[1/2,1](α,α), andν2:=σ2+ (1−¯θ)2. Then the following
expressions hold for the covariance of the zero mean perturbations:
Eθ,ζ˜δγ
i(˜δγ
i)⊤=σ2(˜xγ
i−¯xγ
i)(˜xγ
i−¯xγ
i)⊤+ν2Σ˜xγ
i˜xγ
i
¯θ2
Eθ,ζ˜ϵγ
i(˜ϵγ
i)⊤=σ2(˜yγ
i−¯yγ
i)(˜yγ
i−¯yγ
i)⊤+ν2Σ˜yγ
i˜yγ
i
¯θ2
Eθ,ζ˜δγ
i(˜ϵγ
i)⊤=σ2(˜xγ
i−¯xγ
i)(˜yγ
i−¯yγ
i)⊤+ν2Σ˜xγ
i˜yγ
i
¯θ2,
where Σ˜xγ
i˜xγ
i,Σ˜yγ
i˜yγ
i,Σ˜xγ
i˜yγ
idenote empirical covariance matrices.
Note again, that the covariances above are locally-informed, rather than globally determined. Lastly, there is
also a quadratic expansion performed about the mean-reverted points ˜xγ
i,˜yγ
iwith terms that regularize f,
but we omit this result as the regularization of Theorem 4.1 is more intuitive (c.f. Theorem 2 of (Carratino
et al., 2020)).
J Vicinal Distribution of kNearest-Neighbors
We provide this section for explicit illustration of why knearest-neighbors is not a sensible alternative
fork-mixup. In this setting, we consider drawing two sets of ksamples{(xγ
i,yγ
i)}k
i=1and{(xζ
i,yζ
i)}k
i=1,
match each xγ
iwith its nearest-neighbor in {xζ
i}k
i=1, and then interpolate these matches to obtain the vicinal
distribution. In Figure 7, we provide example images of the generated matching distributions. Note that in
general, these matching distributions have very limited cross-cluster matchings. The “Swiss Roll” dataset is
an exception due to the interwoven arms of the spirals. Additionally, note that the intra-cluster matchings
are much more concentrated around the points in question, and do not perform as much smoothing within
clusters.
21Published in Transactions on Machine Learning Research (09/2023)
Figure 7: Example of matching distributions generated by k= 32-nearest-neighbors, using the same point as
in Figure 3.
22