Published in Transactions on Machine Learning Research (09/2024)
Multi-intention Inverse Q-learning for Interpretable
Behavior Representation
Hao Zhu zhuh@cs.uni-freiburg.de
Department of Computer Science & IMBIT//BrainLinks-BrainTools, University of Freiburg
Brice De La Crompe brice.de.la.crompe@biologie.uni-freiburg.de
Institute of Biology III & IMBIT//BrainLinks-BrainTools, University of Freiburg
Gabriel Kalweit kalweitg@cs.uni-freiburg.de
Department of Computer Science, University of Freiburg
Collaborative Research Institute Intelligent Oncology (CRIION)
Artur Schneider artur.schneider@biologie.uni-freiburg.de
Institute of Biology III & IMBIT//BrainLinks-BrainTools, University of Freiburg
Maria Kalweit kalweitm@cs.uni-freiburg.de
Department of Computer Science, University of Freiburg
Collaborative Research Institute Intelligent Oncology (CRIION)
Ilka Diester ilka.diester@biologie.uni-freiburg.de
Institute of Biology III & IMBIT//BrainLinks-BrainTools, University of Freiburg
Bernstein Center Freiburg
Joschka Boedecker jboedeck@cs.uni-freiburg.de
Department of Computer Science & IMBIT//BrainLinks-BrainTools, University of Freiburg
Collaborative Research Institute Intelligent Oncology (CRIION)
Reviewed on OpenReview: https: // openreview. net/ forum? id= hrKHkmLUFk
Abstract
In advancing the understanding of natural decision-making processes, inverse reinforcement
learning (IRL) methods have proven instrumental in reconstructing animal’s intentions un-
derlying complex behaviors. Given the recent development of a continuous-time multi-
intention IRL framework, there has been persistent inquiry into inferring discrete time-
varying rewards with IRL. To address this challenge, we introduce the class of hierarchical
inverse Q-learning (HIQL) algorithms. Through an unsupervised learning process, HIQL
divides expert trajectories into multiple intention segments, and solves the IRL problem
independently for each. Applying HIQL to simulated experiments and several real animal
behavior datasets, our approach outperforms current benchmarks in behavior prediction
and produces interpretable reward functions. Our results suggest that the intention tran-
sition dynamics underlying complex decision-making behavior is better modeled by a step
function instead of a smoothly varying function. This advancement holds promise for neu-
roscience and cognitive science, contributing to a deeper understanding of decision-making
and uncovering underlying brain mechanisms.
1 Introduction
Characterizing decision-making behavior stands as a fundamental objective within the field of behavioral
neuroscience (Niv, 2009; Wilson & Collins, 2019). Prior research has formulated a variety of mathematical
1Published in Transactions on Machine Learning Research (09/2024)
behavioralmodelsacrossdiversetasks(Ashwoodetal.,2022b;Beronetal.,2022), includinggeneralizedlinear
models and models based on reinforcement learning. These forward models facilitate the understanding and
comparison of decision-making strategies employed by both human and animal subjects. Additionally, they
offer a low-dimensional behavioral representation suitable for regression analysis with neural activities (Hat-
tori et al., 2019; Hamaguchi et al., 2022). Forward models require an empirically defined reward function that
guides subjects optimizing their behavior during decision-making. However, defining a comprehensive and
suitable reward function can pose challenges in complex behavioral tasks (Alyahyay et al., 2023; Rosenberg
et al., 2021). Inverse reinforcement learning (IRL) (Ng et al., 2000; Arora & Doshi, 2021) is a popular ap-
proach to recover a reward function that induces the observed behavior, assuming that the demonstrator was
maximizing its long-term return. Along with the significant successes of IRL in autonomous driving (Kalweit
et al., 2020; Nasernejad et al., 2023), robotics (Kumar et al., 2023; Chen et al., 2023), and healthcare do-
mains (Coronato et al., 2020; Chan & van der Schaar, 2021), it appears to be emerging as a valuable tool
for constructing mathematical behavior models in neuroscience research, as exemplified by Yamaguchi et al.
(2018); Kwon et al. (2020); Alyahyay et al. (2023).
Classic IRL methods seek to identify a single, fixed reward function for a specific scenario. In contrast,
Ashwood et al. (2022a) suggested that animal’s goals can evolve over time due to factors like fatigue, sa-
tiation, and curiosity. Under this assumption, they proposed the dynamic inverse reinforcement learning
(DIRL) framework, which parametrizes the animal’s reward function as a smoothly time-varying linear com-
bination of a small number of spatial reward maps, which are referred to as ‘goal maps’. By assuming the
existence of multiple goal maps with time-varying weights, DIRL allows the instantaneous reward function
to varycontinuously in time. This innovative framework achieved state-of-the-art performance in animal
behavior prediction. Nevertheless, demands have emerged regarding an IRL framework incorporating dis-
cretetime-varying reward functions, particularly following the proposal by Ashwood et al. (2022b) that
natural behaviors can be represented through a Markov chain characterized by alternating between discrete
intentions.
To address this requirement, we propose the novel class of hierarchical inverse Q-learning (HIQL) algorithms,
which extend the fixed-reward inverse Q-learning (IQL) framework from Kalweit et al. (2020) to solve multi-
intention IRL problems. Based on the assumption that the intention transition dynamics follows a Markov
process, HIQL integrates an expectation-maximization (EM) approach to first divide expert trajectories into
multiple intention segments in an unsupervised manner, and then solve the IRL problem independently for
each segment. We then compare the performance of HIQL and the state-of-the-art model DIRL (Ashwood
et al., 2022a) in a simulated gridworld environment and a dataset of trajectories from real mice navigating a
127-node-labyrinth (Rosenberg et al., 2021). The results show that our HIQL algorithm outperforms DIRL
in behavior prediction in both benchmarks, and produces interpretable reward functions. Finally, we applied
HIQL in a real mice decision-making dataset from a dynamic two-armed bandit task (De La Crompe et al.,
2023), andmathematicallycharacterizedexploitationandexplorationbehaviorofanimalsduringvalue-based
decision-making.
2 Related work
Various approaches have been introduced to address multi-intention IRL problems. Notably, several frame-
works based on parametric (Babes et al., 2011; Likmeta et al., 2021), or non-parametric (Choi & Kim, 2012;
Bighashdel et al., 2021) approaches allow for learning from multiple agents with distinct reward functions.
Based on the assumption that the expert sticks to the same intention through one episode, these meth-
ods iterate between clustering expert demonstrations according to different intentions and solving the IRL
problem for each cluster. However, unlike HIQL, these frameworks do not accommodate the scenario where
intentions might alternate within the same episode.
In contrast, several approaches formulate MI-IRL problem as finding the maximum-likelihood partition of
each trajectory, where each segment was generated from different locally consistent reward functions. To
solve the problem, Dimitrakakis & Rothkopf (2012); Michini & How (2012) extended the Bayesian IRL
frameworks (Ramachandran & Amir, 2007) to multi-intention scenario by introducing a Dirichlet process
prior on different reward functions. The resulting Dirichlet process mixture model (DPM) can be solved via
2Published in Transactions on Machine Learning Research (09/2024)
probabilistic inference, e.g., with Markov chain Monte Carlo (Neal, 2000). These algorithms do not require
the number of intentions as a hyperparameter due to the nonparametric nature of DPMs. However, because
of the nonparametric setting, DPMs inadequately model the temporal persistence of states and often create
redundant states and rapidly switch among them, which eventually leads to poor interpretability of the
results (Fox, 2009). This problem was addressed by Surana & Srivastava (2014), where they extended the
DPM to a sticky hierarchical Dirichlet process hidden Markov model. Nevertheless, solving the IRL problem
as Bayesian inference is still computationally intensive, which could be intractable even for moderately
sized finite-state IRL problems. Under the same problem formulation, Nguyen et al. (2015) proposed a
probabilistic graphical model, generalizing the algorithm from Babes et al. (2011). This approach avoids
the computationally intensive Bayesian inference problem, but is restricted to the case of linearly-solvable
environments. Our parametric HIQL algorithm also adapts the ‘trajectory segmentation + IRL’ problem
formulation, but on the other hand, avoids the aforementioned limitations present in the other approaches.
Particularly, we propose that in the parametric setup, manually specifying the number of intentions allows
the researchers to decide the level of abstraction on intentions according to their expertise and scientific
interest, i.e., whether to focus on the fine-grained difference between intentions ( cf.§5.3).
The state-of-the-art framework, DIRL (Ashwood et al., 2022a), can be considered as an extension of the
maximum entropy IRL algorithm (Ziebart et al., 2008; 2010) to non-stationary rewards, where the reward
function is assumed to be parameterized as a time-varying linear combination of a small number of non-
linear spatial reward maps with Gaussian random walk prior over weights. Under this assumption, the
borderline between intentions in DIRL is blur, i.e., the intention transition is a continuous process, instead
of a step-like procedure. However, such continuous time-varying reward assumed by DIRL is contradicted by
the findings from Ashwood et al. (2022b), where it was proposed that humans and animals switch between
multiple discretestrategies during decision-making. Although in theory, one can try to approximate such
step-like intention transition dynamics with DIRL by assigning a larger variance σto the Gaussian to make
the random walk less smooth, we show empirically (§5.1) that there are only minor influence from this
hyperparameter σon the learnt reward, indicating that it would be challenging for DIRL to capture such
step-like intention transition function during natural decision-making behavior. Instead, HIQL assumes that
the intention transition dynamics follows a Markov process, which is intrinsically discrete and aligns better
to the discrete switching intentions than DIRL ( cf.§5.1 and §5.2).
Last but not the least, most of the aforementioned algorithms are model-based , relying on a known tran-
sition dynamics of the environment, whereas in many scenarios, the environment model is unknown. As
an improvement, HIQL can also perform model-free learning, enabling the application in a wider range of
environments.
3 Background
Markov decision processes. A Markov decision process (MDP) can be denoted by a tuple (S,A,P,r,γ ),
whereSandAdenotethestate-andaction-space, respectively; Thefunction P:S×A×S→ [0,1]isthestate
transition function with P(s,a,s′) =P(s′|s,a)and1TP(s,a,·) = 1; The function r:S×A→ Rdefines
the reward function, and γ∈[0,1]denotes the discount factor. Additionally, the function π:S×A→ [0,1]
withπ(s,a) =P(a|s)and1Tπ(s,·) = 1is used to represent the policy according to which actions are
selected in the MDP.
Inverse Q-learning. Given the set of expert demonstrations Din an MDP, where each trajectory ξ∈D
is denoted with a sequence of state-action pairs: ξ={(s0,a0),..., (sn,an)}, the IRL problem consists in
determining a reward function rthat best explains the observed expert behavior in the form of demonstrated
trajectories. Note that both the word ‘demonstration’ and ‘trajectory’ will be used subsequently. The former
refers to one action execution and state transition of the expert, while the latter corresponds to a sequence
of actions and state transitions for an entire episode. Unfortunately, IRL is generally ill-posed because
infinitely many reward functions are consistent with the expert’s observed behavior. To resolve this issue,
various approaches have been proposed (see Arora & Doshi (2021) for a detailed review). Particularly,
3Published in Transactions on Machine Learning Research (09/2024)
Kalweit et al. (2020) formulated the IRL problem as a maximum likelihood estimation (MLE) problem:
maximize Eξ∼D[logP(ξ|πr)]
subject to πr(s,a) = exp (Q(s,a)−log/summationtextexpQ(s,·)), for alls∈S,a∈A
Q(s,a) =r(s,a) +γ/summationtext
s′∈SP(s,a,s′) maxa′∈AQ(s′,a′), for alls∈S,a∈A,(1)
whereris the optimization variable and Dis the problem data. The objective of problem (1) is maximized
when the expert policy πE(s,a)is equivalent to πr(s,a). Hence,
πE(s,a) =exp(Q(s,a))/summationtext
a′∈Aexp(Q(s,a′))=⇒Q(s,a) =Q(s,b) + log(πE(s,a))−log(πE(s,b)), (2)
for all actions a∈Aandb∈A−awhereA−a=A\{a}. Using the Bellman optimality equation in (2), the
immediate reward of action ain statescan be expressed by the immediate reward of some other action
b∈A−a, the respective log-probabilities and future action-values (Kalweit et al., 2020):
r(s,a) =µ(s,a) +1
card (A−a)/summationdisplay
b∈A−a(r(s,b)−µ(s,b)), (3)
where card (A−a)denotes the cardinality of A−a, and
µ(s,a) = log(πE(s,a))−γ/summationdisplay
s′∈SP(s,a,s′) max
a′∈AQ(s′,a′). (4)
Letcard (A) =m, for each state s∈S, formulating (3) for all actions a∈Aresults in a system of linear
equations about r(s,·):

1−1
m−1··· −1
m−1
−1
m−11··· −1
m−1............
−1
m−1−1
m−1··· 1

r(s,a1)
r(s,a2)
...
r(s,am)
=
µ(s,a1)−1
m−1/summationtext
b∈A−a1µ(s,b)
µ(s,a2)−1
m−1/summationtext
b∈A−a2µ(s,b)
...
µ(s,am)−1
m−1/summationtext
b∈A−amµ(s,b)
. (5)
Thus the unknown reward function rcan be obtained by solving (5) for all s∈Svia least squares (Kalweit
et al., 2020). This approach is called the inverse action-value iteration (IAVI) (Algorithm 1), a model-based
algorithm that solves the IRL problem analytically in closed-form. When the transition dynamics Pis
unknown, stochastic approximation can be applied to (3) and (4) such that rcan be obtained in a sampling-
based manner (Kalweit et al., 2020). The resulting inverse Q-learning (IQL) algorithm (Algorithm 2) is
a model-free extension of IAVI. The class of IQL algorithms solve the MDP underlying the demonstrated
behavior only once, leading to a speedup of up to several orders of magnitude compared to the popular max-
imum entropy IRL algorithm (Ziebart et al., 2008) and some of its variants. In addition, it can accommodate
arbitrary non-linear reward functions.
4 Hierarchical inverse Q-learning
We formulate the multi-intention IRL problem under the following assumptions:
Assumption 4.1. EachexpertdemonstrationisgeneratedaccordingtotheBoltzmannoptimalpolicyunder
one of the reward functions in a K-dimensional finite set R={r1,...,rK}, with each corresponding to one
specific intention.
Assumption 4.2. The probability that one demonstration is generated under reward function r∈Ris
controlled by a Markov chain with initial state distribution Πand transition matrix Λ, where the vector Π
and each row of the transition matrix Λi:,i= 1,...,Krepresent some probability distribution over the set
R,i.e.,Π∈∆K−1andΛi:∈∆K−1, where ∆K−1=/braceleftbig
x∈RK|x⪰0,1Tx= 1/bracerightbig
is a(K−1)-dimensional
probability simplex.
4Published in Transactions on Machine Learning Research (09/2024)
st−1 st st+1 P P . . . . . .
at−1 at
rt−1 rt Λ Λ . . . rt+1 . . .
Figure 1: Graphical representation of expert’s
decision process.The resulting hierarchical decision process of an expert
following these assumptions from the perspective of a re-
searcher is depicted graphically in Figure 1, where the
first two rows represent the MDP, and the last row repre-
sents the Markov chain for intention transition dynamics.
Solving IRL problems on such decision network with pa-
rameter space Θ ={Π,Λ,R}consists in determining 1)
a set of reward functions, and 2) the reward function in-
dex for each demonstration that best jointly explain the
observed expert behavior. An expectation-maximization
(EM) algorithm can be devised to iteratively learn Θ. For
convenience, we introduce η={z0,...,zn}to be the pre-
dicted sequence of reward function indexes for trajectory ξ∈D. Then each iteration of the EM process
can be written as an MLE problem where the likelihood is obtained by marginalizing out the sequence of
predicted reward function indexes η:
maximize J(Θ+|Θ) = Eξ∼D,η[logP(ξ,η|Θ+)], (6)
where Θ+is the optimization variable and D,Θare the problem data.
Theorem 4.3. Solving problem (6) is equivalent to solving a sequence of optimization problems:
maximize (over Π+)Eξ∼D/bracketleftig/summationtextK
i=1P(z0=i|ξ,Θ) log Π+
i/bracketrightig
subject to Π+⪰0,1TΠ+= 1,(7)
maximize (over Λ+)Eξ∼D/bracketleftig/summationtextK
i=1/summationtextK
j=1/summationtextn
t=1P(zt−1=i,zt=j|ξ,Θ) log Λ+
ij/bracketrightig
subject to Λ+
i:⪰0,1TΛ+
i:= 1, i = 1,...,K,(8)
and
maximize (over r+
i)Eξ∼D/bracketleftig/summationtextn
t=0P(zt=i|ξ,Θ) logπr+
i(st,at)/bracketrightig
subject to πr+
i(s,a) = exp (Q(s,a)−log/summationtextexpQ(s,·)), for alls∈S,a∈A
Q(s,a) =r+
i(s,a) +γ/summationtext
s′∈SP(s,a,s′) maxa′∈AQ(s′,a′), for alls∈S,a∈A.(9)
Proof.See Appendix A.1.
In practice, to evaluate the objective functions of (7), (8) and (9), the Baum-Welch algorithm (Baum &
Petrie, 1966) can be applied to obtain the required posterior probabilities P(zt=i|ξ,Θ)andP(zt−1=
i,zt=j|ξ,Θ)(cf.Appendix A.2). Then according to the Gibbs’ inequality, the optimum of (7) and (8) are
achieved by
Π+
i=E
ξ∼D[P(z0=i|ξ,Θ)], i= 1,...,K, (10)
and
Λ+
ij=Eξ∼D,t[P(zt−1=i,zt=j|ξ,Θ)]
Eξ∼D,t[P(zt−1=i|ξ,Θ)], i= 1,...,K, j = 1,...,K. (11)
To solve problem (9), note that it has the same structure as (1), thus it can be addressed by the class of
IQL algorithms (Algorithm 1 and 2) with slight adaptations. Specifically, for each i= 1,...,K, a subset of
expert demonstrations from Dwill be sampled w.r.t. the posterior probability P(zt=i|ξ,Θ), then either
Algorithm 1 or Algorithm 2 (depending on whether the transition dynamics Pis available) will be applied
to this subset of demonstrations so that r+
ican be obtained. Assembled, we call the above process of solving
multi-intention IRL problems the hierarchical inverse Q-learning (HIQL) algorithm. The corresponding
pseudo-code is listed in Algorithm 3.
5Published in Transactions on Machine Learning Research (09/2024)
5 Experiments and discussion
WeevaluatetheperformanceofHIQLonasimulatedgridworldenvironmentandarealmicebehaviordataset
obtainedfroma 127-nodelabyrinthnavigationtask(Rosenbergetal.,2021), andcomparetoDIRL(Ashwood
et al., 2022a). We then show the potential of performing model-free learning and detecting exploration
behavior with HIQL on a mice reversal-learning task (De La Crompe et al., 2023).
5.1 Gridworld benchmark
01234
0
1
2
3
4
Figure2: Thegridworldenviron-
ment.The simulated gridworld environment is a 5×5map (Figure 2), where an
agent can choose between going up,down,left,right, or tostayin place
per time step. The expert starts at origin (0,0)and any of its actions can
achieve the intended state with 90%probability, while the other 10%lead
to random directions. It has two possible policies, where πgoalprefers
going to the destination (4,4)andπabandonprefers returning to origin
(0,0). The expert starts an episode always with the πgoalpolicy. While
moving to the destination, the expert will encounter barriers ‘#’ at some
states. Every time the expert runs into a barrier, it has 30%probability of
switching to a different policy. Each episode ends when the expert reaches
either the origin or the destination, and at the 8th time step, if the episode
has not finished, the expert will have a 50%probability of switching to
πabandon(cf.Appendix C.1). Such intention transition dynamics of the
simulated expert as described above is better aligned to a step-function,
instead of a smoothly varying function.
We compared between the performance of HIAVI (the model-based variant of HIQL) and DIRL with 1or
2intention(s). Note that for the single intention case, HIAVI falls back to normal IAVI and DIRL falls
back to maximum causal entropy IRL (Ziebart et al., 2010). Trying to enable DIRL to better capture
such step-like intention transition dynamics, we varied the hyperparameter that controls the smoothness of
the time-varying reward in DIRL — the variance σfor the Gaussian of the random walk prior, from 0.01
to10. A larger σvalue corresponds to a less smooth intention transition dynamics in DIRL. The whole
expert demonstration dataset consisted of 1024trajectories. All evaluated algorithms were fit to multiple
sub-datasets with different number of expert trajectories, and each dataset was analyzed using a 5-fold
cross-validation. We list the detailed information about model training in Appendix C.1.
In general, HIAVI outperformed DIRL in predicting the expert behavior as indicated by the log-likelihood on
the test dataset (Figure 3a). Notice that HIAVI already had a better performance compared to DIRL even
in the collapsed single intention model, indicating that IAVI serves as a better inner-loop (IRL problem)
solver than maximum entropy IRL. This is probably because IAVI learns the unknown expert reward in
closed-form by solving a system of linear equations instead of performing parameter estimation as maximum
entropy IRL does, which introduces approximate error (Kalweit et al., 2020). Comparing the single- and
multi-intention variants, both HIAVI and DIRL had an increase in prediction performance. Particularly,
the HIAVI almost achieved the expert level as the number of trajectories increased, while the performance
improvement for DIRL was only minor. Besides, different σvalues of DIRL seem not to have a significant
influence on the DIRL performance when there is enough expert trajectories in the dataset.
Next we compared HIAVI and DIRL abilities in capturing the expert’s intention transition dynamics (Fig-
ure 3b) and recovering the expert policy (Figure 3c). The analysis was performed based on the learnt
policy using the whole dataset with all 1024trajectories (with cross-validation applied). As a measure of
performance in policy recovery, we used the expected value difference (EVD) metric (Levine et al., 2011).
EVD is defined as the mean square error between the state-value under the true reward function for the
expert policy and the state-value under the true reward for the optimal Boltzmann policy w.r.t. the learnt
reward. It provides an estimation of the sub-optimality of the learnt policy under the true reward function.
For HIAVI and DIRL with 2intentions, the inferred intentions were assigned to the best-fit ground truth
intentions, and the two learnt policy was evaluated under the corresponding true reward individually to
obtain the respective state-values. Under the single-intention setup, since only one learnt policy could be
6Published in Transactions on Machine Learning Research (09/2024)
(a)
0 500 1000
#trajectories−1.5−1.0−0.50.0log-likelihood
ground truth
HIVAI (1 intention)
HIAVI (2 intentions)
DIRL (1 map)
DIRL (2 maps, σ= 0.01)
DIRL (2 maps, σ= 0.1)
DIRL (2 maps, σ= 1)
DIRL (2 maps, σ= 10 ) (b)
02468101214
time0.00.51.0P(‘abandon’ )
 ground truth
HIAVI (2 intentions)
DIRL (2 maps, σ= 0.01)
DIRL (2 maps, σ= 0.1)
DIRL (2 maps, σ= 1)
DIRL (2 maps, σ= 10 )
(c)
‘goal’
(1 intention)
 (2 intentions)
 (1 map)
 (2 maps,σ= 0.01)
(2 maps,σ= 0.1)
 (2 maps,σ= 1)
 (2 maps,σ= 10 )
‘abandon’
4.2 9.5
4.2 9.5
1.5 6.1
0.0 0.5
3.0 6.7
1.8 7.0
0.0 0.2
0.0 0.1
0.0 1.7
0.0 1.8
2.7 9.4
0.0 2.1
0.0 8.9
0.0 0.1
0.0 9.3
0.0 0.1
ground truth HIAVI DIRL
HIAVI DIRL
1 intention 2 intentions 1 map2 maps
σ= 0.01σ= 0.1σ= 1 σ= 10
‘goal’ 13.96±0.235.58±0.47 47.10±0.00 27.63±6.55 2.97±0.67 11.08±3.56 37.97±1.29
‘abandon’ 45.55±0.076.39±1.80 48.17±0.00 45.90±0.15 45.48±0.49 48.15±0.05 48.15±0.06
Figure 3: Results for the gridworld benchmark. (a) Comparison of HIAVI and DIRL on datasets with
different number of expert trajectories, represented as log-likelihood on the test dataset (mean ±standard
error, 5-fold cross-validation). (b) Predicted intention dynamics from HIAVI and DIRL using the outputs
from the best cross-validation fold, represented as the posterior probability of the ‘abandon’ intention and
averaged across all trajectories (mean ±standard error across 1024trajectories). (c) Visualization of the
ground truth and learnt state-value functions from the best cross-validation fold (top), and the corresponding
EVDs (bottom, mean ±standard error, 5-fold cross-validation) from HIAVI and DIRL.
provided by the algorithms, the same output learnt policy was evaluated twice repeatedly under the different
true reward functions, respectively. Aligning with the results indicated by Figure 3a, HIAVI had smaller
EVD values than DIRL even under single-intention setup (Figure 3c). Particularly, the learnt policy from
single-intention HIAVI exhibits a mixed characteristic of both ‘goal’ and ‘thirsty’ intentions but is more
close to the former, whereas the learnt policy from single-intention DIRL deviates from both of the two
intentions. This explains why HIAVI already had a better prediction log-likelihood than DIRL under the
single-intention setup. Then by introducing the second intention, the best performance in recovering expert
policy under both intentions was achieved by HIAVI ( 5.58±0.47forπgoaland6.39±1.80forπabandon).
While by selecting an appropriate σvalue, DIRL could reconstruct the ‘goal’ policy at very high level (with
a2.97±0.67EVD when σ= 0.1), it struggled to learn the policy under ‘abandon’ intention. As a result,
the HIAVI predicted posterior probability of the expert remaining in the ‘abandon’ intention at different
time steps in a single episode matched exactly with the ground truth (Figure 3b), whereas DIRL seemed to
overestimate this probability at the beginning (where the expert reached the barrier for the first time), and
underestimate it after the 8th time step.
To sum up, the comparison between HIAVI and DIRL on the gridworld benchmark suggests that HIAVI
can better capture a step-like intention transition dynamics and reconstruct the underlying expert policies
7Published in Transactions on Machine Learning Research (09/2024)
(a)
−2.0−1.5−1.0−0.50.0
log-likelihoodrandom
DIRL
HIAVI}
}1 map
4 maps
1 intention
4 intentions (b)
1 2 3 4
#intentions/maps103104BIC
DIRL HIAVI
(c)
..
.. .
. ..
..
.
.
..
.. .
. .. .
. .
. .
. .. .
. ..
..
.
.
..
..
..
.
.
..
.
.
..
.
.
..
..
..
.
.
..
...
....
..
..
....
....
....
..
..
....
..
..
....
..
..
....
....
....
..
..
....
..intention 1: ’tired’
..
.. .
. ..
..
.
.
..
.. .
. .. .
. .
. .
. .. .
. ..
..
.
.
..
..
..
.
.
..
.
.
..
.
.
..
..
..
.
.
..
...
....
..
..
....
....
....
..
..
....
..
..
....
..
..
....
....
....
..
..
....
..intention 2: ‘thirsty’
00.010.11
 (d)
0 10 20
time0.00.51.0P(‘thirsty’ )
P(‘tired’ ) (e)
1 2
intentiont+ 11
2intentiont0.98
0.1240.02
0.876
Figure 5: Results for the navigation benchmark of the water-restricted cohort. (a) Comparison of HIAVI,
DIRL, and a random policy, represented as log-likelihood on the test dataset. (b) BIC as a function of
the number of intentions in HIAVI. (c) Learnt policy (red arrows and crosses) in the environment and
corresponding state occupancy (grey colormap) under different intentions. (d) Predicted intention dynamics
from HIAVI, averaged across all 200trajectories. Solid and shaded curves denote the mean and standard
error across 200trajectories. (e) Inferred intention transition matrix from HIAVI.
than DIRL. Although in theory one can try to approximate such step-function with DIRL by assigning a
larger variance σto the Gaussian to make the random walk less smooth, it has been shown empirically in
this experiment that it’s not trivial in practice since the influence of this hyperparameter σon DIRL would
only be limited.
5.2 Real-world mice navigation benchmark
..
.. .
. ..
..
.
.
..
.. .
. .. .
. .
. .
. .. .
. ..
..
.
.
..
..
..
.
.
..
.
.
..
.
.
..
..
..
.
.
..
...
....
..
..
....
....
....
..
..
....
..
..
....
..
..
....
....
....
..
..
....
..
Figure 4: The labyrinth environ-
ment.The expert demonstrations in this real-world dataset were originally col-
lected by Rosenberg et al. (2021) from a 127-node labyrinth navigation
task. As shown in Figure 4, each black dot represents one state in the en-
vironment and the subjects can select from 4actions:left,right,reverse,
andstayat each state. One of the 127states is occupied with water
resource (the blue square), and the optimal path from the labyrinth en-
trance to the water port is depicted as blue line. Two cohorts of 10 mice
moved freely in dark through the labyrinth over the course of 7hours,
where one cohort of animals was under water restriction while the other
was not. Difference in water restriction condition resulted in different an-
imal behavior between these two cohorts in the environment. For model
evaluation, HIAVI and DIRL with varied intentions from 1to4were
trained independently on the water-restricted and water-unrestricted ex-
pert demonstration datasets (with 200and207animal trajectories, respectively). 20%of the trajectories
from each dataset were held out as a test set. We list the detailed information about trajectory preprocessing
and model training in Appendix C.2.
We first compared between HIAVI and DIRL on the trajectories collected from the group of water restricted
animals. First of all, as the number of intentions increased, the log-likelihood value for HIAVI predictions
increased significantly, whereas the improvement of DIRL performance was rather minor (Figure 5a). This
8Published in Transactions on Machine Learning Research (09/2024)
(a)
−2.0−1.5−1.0−0.50.0
log-likelihoodrandom
DIRL
HIAVI}
}1 map
4 maps
1 intention
4 intentions (b)
1 2 3 4
#intentions/maps103104BIC
DIRL HIAVI
(c)
..
.. .
. ..
..
.
.
..
.. .
. .. .
. .
. .
. .. .
. ..
..
.
.
..
..
..
.
.
..
.
.
..
.
.
..
..
..
.
.
..
...
....
..
..
....
....
....
..
..
....
..
..
....
..
..
....
....
....
..
..
....
..intention 1: ’exploring’
..
.. .
. ..
..
.
.
..
.. .
. .. .
. .
. .
. .. .
. ..
..
.
.
..
..
..
.
.
..
.
.
..
.
.
..
..
..
.
.
..
...
....
..
..
....
....
....
..
..
....
..
..
....
..
..
....
....
....
..
..
....
..intention 2: ‘tired’
00.010.11
 (d)
0 5 10
time0.00.51.0
P(‘tired’ )
P(‘exploring’ ) (e)
1 2
intentiont+ 11
2intentiont0.441
0.0540.559
0.946
Figure 6: Results for the navigation benchmark of the water-unrestricted cohort. (a) Comparison of HIAVI,
DIRL, and a random policy, represented as log-likelihood on the test dataset. (b) BIC as a function of
the number of intentions in HIAVI. (c) Learnt policy (red arrows and crosses) in the environment and
corresponding state occupancy (grey colormap) under different intentions. (d) Predicted intention dynamics
from HIAVI, averaged across all 207trajectories. Solid and shaded curves denote the mean and standard
error across 207trajectories. (e) Inferred intention transition matrix from HIAVI.
led to a distinguishable outperformance of HIAVI compared to DIRL in this benchmark. To check whether
the better performance of HIAVI compared to DIRL was due to a larger number of papameters, we then
calculated the Bayesian information criterion (BIC) for these two approaches (Figure 5b). It shows that the
BIC values for HIAVI are consistently lower than those for DIRL when the number of intentions exceeds
one. To analyze the recovered expert policies by HIAVI, we selected the number of intentions K= 2where
the lowest BIC value was achieved. The learnt mice policy under intention 1(‘tired’) displays a preference
of moving out from the water port towards the maze entrance and stay, while the policy under intention
2(‘thirsty’) guides the mice directly to the water port along the optimal track. Correspondingly, under
the ‘tired’ intention, the highest state occupancy was observed at the entrance of the environment, while
under ‘thirsty’, it was observed at the water port (Figure 5c). To visualize the predicted intention transition
dynamics, we computed the posterior probability over mice’s intentions across all trajectories. The recovered
temporal intention dynamics shows a high probability of the ‘thirsty’ intention at the beginning but later
on drops gradually, as the ‘tired’ intention gradually becomes dominant (Figure 5d). Besides, the intention
transition matrix learnt by HIAVI (Figure 5e) suggests that while both the ‘thirsty’ and ‘tired’ intentions are
stable, the probability that the animals switch from ‘thirsty’ to ‘tired’ is relatively larger than the other way
around. These observations indicate the following interpretations on the mice behavior within this cohort,
which also align well with our intuition: The water-restricted mice are eager to find the water port as the
trial starts, and then gradually return back to the labyrinth entrance as they obtain enough water. Once
they have decided to return, the probability to go back to search for the water port would be quite low.
When comparing the dataset collected from the water-unrestricted animals, HIAVI also had a larger test
log-likelihood and a lower BIC value than DIRL (Figure 6a and 6b). Interestingly, for this dataset, increasing
the number of intentions seemed not to improve the performance of DIRL at all, which might be a result of
a more random behavior of the animals due to the lack of motivation to find water. We then selected HIAVI
withK= 2according to BIC (Figure 6b) to analyze the recovered policy and intention dynamics. For this
cohort of animals, the inferred policy under two intentions exhibits ‘exploring’ and ‘tired’ behavior. The
policy under ‘exploring’ tends to encourage the animal lingering in the labyrinth, whereas the policy under
9Published in Transactions on Machine Learning Research (09/2024)
(a)
12345
history length−0.46−0.44−0.42−0.40−0.38log-likelihood
train
test (b)
F.12 3 4 5
#intentions0.00.20.4∆log-likelihood
 (c)
1 2 3 4 5
#intentions200030004000BIC
 (d)
1 2 3
intention0.40.50.60.70.8
‘exploitation’‘disengaged’
‘exploration’
global ﬁt
P(switch )P(win-stay )
P(lose-switch )
(e)
0 5 10 15
trial#0.00.51.0
P(correct )
P(‘exploitation’ )
P(‘disengaged’ )
P(‘exploration’ ) (f)
all 1 2 3
intention0.500.751.00P(correct )0.740.86
0.64
0.56 (g)
0.0 0.5 1.0
P(‘exploitation’ )0.000.050.100.15P(‘exploration’ )
Figure 7: Results for the mice reversal-learning dataset. (a) Log-likelihood (mean ±standard error, 5-fold
cross-validation) as a function of history length of single intention HIQL. (b) Change in test set log-likelihood
as a function of the number of intentions in HIQL with ℓh= 3, relative to the fQ-learning model (labeled
‘F.’). Each trace represents a single mouse, averaged over cross-validation. Solid black indicates the mean
across animals, and the dashed curve indicates the example mouse. (c) BIC as a function of the number
of intention in HIQL with ℓh= 3. (d) Learnt mice policy represented with the probability of switch, win-
stay, and lose-switch. Each grey curve denotes one mouse. (e) Average task performance and the predicted
intention dynamics. Solid and shaded curves denote the mean and standard error across 9animals. (f)
Overall task performance (gray) and the performance under different intentions, mean ±standard error
across 9animals. (g) Relationship between the probability of the ‘exploitation’ intention, 5trials before
block switch and the probability of the ‘exploration’ intention, 5trials after block switch, mean ±standard
error across 9animals.
‘tired’ intention steers the animal back to the maze entrance (Figure 6c). Correspondingly, the posterior
probability of ‘exploring’ initially dominates at the beginning of the trial but is generally surpassed by the
‘tired’ intention over time (Figure 6d). The inferred intention transition matrix (Figure 6e) suggests that
the ‘exploring’ intention is unstable, meaning that it has a larger probability of switching to ‘tired’ intention
compared to staying at ‘exploring’. In contrast, after switching to the ‘tired’ intention, animals tend to keep
this intention until the end of trial. These observations are again consistent with our intuition. Since the
water-seeking motivation for this group of water-unrestricted mice is very low, they would prefer to explore
the labyrinth, rather than search for the water port.
In conclusion, the above results show that HIAVI outperforms DIRL in predicting animal behavior on this
labyrinth navigation benchmark for both two cohorts of mice, and is able to generate interpretable reward
functions underlying different intentions. On the other hand, the significant difference between the prediction
performance of HIAVI and DIRL indicates that the intention transition dynamics during natural decision-
making behavior may be better described with a step-function, rather that a smoothly varying function as
assumed by DIRL.
5.3 Application to mice reversal-learning behavior
Finally, we apply the HIQL algorithm to behavioral data recorded from a group of mice engaged in a dynamic
two-armed bandit reversal-learning task. The dataset was initially reported by De La Crompe et al. (2023).
10Published in Transactions on Machine Learning Research (09/2024)
At the beginning of the task, water-restricted mice may choose from two available spouts, left(L) andright
(R), with random one of them assigned water as extrinsic reward. After reaching an online performance
of75%correct in a 15-trials sliding average window and a minimum 20-trials block, the rewarded spout
is changed. To formulate the MDP, we define the action space as: A={left,right}. Every state s∈S
is defined with a tuple of truncated history information: st= (φt−1,...,φt−ℓh;at−1,...,at−ℓh), where
the positive integer ℓhdenotes the history length, a∈Adenotes history action, and φ∈{correct,error}
represents trial feedback, i.e., the extrinsic reward. Such MDP formulation allows us to avoid explicitly
establishing a partially observable MDP. Different from the first two experiments, we applied the model-free
variant of HIQL in this dynamic reversal-learning task since the environment model is unknown.
We begin from selecting the hyperparameter ℓhusing single intention HIQL (equivalent to vanilla IQL). The
dataset for all subsequent analysis consists of 64sessions (trajectories) from 9different animals and a 5-fold
cross-validation was applied over the trajectories. We compared the log-likelihood on training and test sets
of multiple IQL fitting with different ℓh(Figure 7a). The log-likelihood on test sets shows a bell-shaped
curve asℓhincreases, indicating an overfit on the training set when ℓh>3. Note that there is an abnormal
drop on training set log-likelihood at large ℓhs. This can be explained with the insufficient sampling given
the fixed set of expert demonstrations, since the size of the state space grows exponentially as the history
lengthℓhincreases. The best test log-likelihood was achieved by ℓh= 3, which was selected for subsequent
steps. Next, to determine the number of intentions Kunder which mice demonstrated the trajectories, we
fit multiple HIQL with varying number of intentions. In this step, we additionally applied a forgetting Q-
learning (fQ-learning) model (Beron et al., 2022), which has been widely recognized as a prominent forward
behavioral model for the reversal-learning task, serving as a baseline for comparative analysis. We found
that the multiple intention HIQL fitting substantially outperformed the single intention models (Figure 7b).
The BIC w.r.t. different Kindicates that both K= 2andK= 3are reasonable values (Figure 7c). Taking
biological interpretability into account, we selected K= 3for the following discussion. We list additional
details about the above procedure of fitting HIQL to this mice reversal-learning dataset in Appendix C.3.
(a)
0 20 40 60 80
trial#0.00.51.0P(z)
‘exploitation’
‘disengaged’
‘exploration’correct
error
RLchoice (b)
1 2 3
intentiont+ 11
2
3intentiont0.859
0.169
0.6460.132
0.784
0.1960.008
0.047
0.157
Figure 8: (a) Predicted intention dynamics for an ex-
ample session. Dots and triangles indicate mice ac-
tion. (b) Inferred intention transition matrix for this
example mouse.The inferred mice policies from HIQL represent dif-
ferent strategies in the task under three intentions
(Figure 7d). The policy under intention 1(‘exploita-
tion’) displays a strong preference of a ‘win-stay’ and
‘lose-switch’ strategy, which is the optimal policy in
this deterministic reward bandit task, i.e., stay on
thesamesideforthenexttrialifitisrewardedinthe
current trial, and switch to the other one otherwise.
On the other hand, under intention 2(‘disengaged’),
thepolicyexhibitsapreferenceforexploitationwhen
the previous trial was successful, but following error
trials, it employs a random action selection, indi-
cated by the ca. 0.5probability of executing a ‘lose-
switch’. Lastly, under intention 3(‘exploration’), the
subjectconsistentlyfavorsselectingtheoptionoppo-
site to the one chosen in the preceding trial, irrespec-
tive of whether they had won or lost in that particular instance. As shown in an example session (Figure 8a),
the‘exploration’intentionpredominantlyoccursattheonsetofablockandonlylastsforafewtrials, whereas
the other two intentions may persists over multiple consecutive trials. These results suggests that compared
to ‘exploitation’ and ‘disengaged’, the ‘exploration’ intention is not stable, which aligns well with the learned
intention transition matrix (Figure 8b). Additionally, it becomes evident that error trials tend to coincide
with the trials where the posterior probability of ‘disengaged’ and ‘exploration’ intentions are dominant.
The intention dynamics averaged across all blocks closely resembles those observed in the example session
(Figures 7e). As each block begins with the animal’s performance at a relatively low level, there is a decline
in the posterior probability associated with the ‘exploitation’ intention, accompanied by an increase in the
probabilities of the other two intentions associated with suboptimal exploratory strategies. Then as the
subjects’ performance steadily improves, the ‘exploitation’ intention progressively reasserts its dominance.
In contrast to the cohort’s general correct rate of 0.74±0.02, the subjects performed significantly better
11Published in Transactions on Machine Learning Research (09/2024)
within the ‘exploitation’ intention, achieving a correctness rate of 0.86±0.01, whereas they only attained
lower correctness rates of 0.64±0.03and0.56±0.06in the two other intentions (Figure 7f). Interestingly,
it was observed that the mean posterior probability of the ‘exploration’ intention at the beginning of a new
block showed a positive correlation with the average probability of the ‘exploitation’ state at the end of the
preceding block (Figure 7g). These results confirm the difference between the two suboptimal intentions
‘disengaged’ and ‘exploration’: the first strategy explores passively upon error, while the second strategy
involves a deliberate, exploration-oriented action selection when the subject is highly engaged and possess a
good understanding of the environment. Although the ‘exploration’ intention is suggested to be unstable, it
provides the opportunity to characterize a type of learning strategy based on error, which has been shown
experimentally by Kononowicz et al. (2022).
In summary, by applying HIQL in a real-world mice reversal learning dataset, we demonstrate the possibility
of performing model-free learning and producing interpretable behavior characteristics of animal experts via
HIQL. We also detected a typical exploration strategy of mice during this value-based decision-making
task via mathematical modeling, which has only be achieved in perceptual decision-making tasks (Ashwood
et al., 2022b). Last but not the least, the model training procedure that we proposed in this experiment
enables fits to individual subjects without running into the challenge of post hoc alignment of intentions
from different individuals. Although the mice subjects exhibited similar behavior during our experiment,
limiting the magnitude of observed differences in individual solutions, we anticipate that this method will
prove beneficial in tasks where subjects exhibit greater variance in policy preferences.
6 Conclusion
This paper proposes the class of EM-based hierarchical inverse Q-learning (HIQL) algorithms, extending the
IQL algorithm (Kalweit et al., 2020) for multi-intention IRL problems. Empirical results demonstrate that
HIQL outperforms the state-of-the-art on both synthesized and real-world datasets, and is able to produce
interpretable behavior characteristics. We also mathematically characterized typical exploration behavior of
rodents during value-based decision-making using HIQL.
Comparedtothestate-of-the-artalgorithmforcharacterizinganimalbehavior,DIRL(Ashwoodetal.,2022a),
the advantages of our HIQL algorithm are two fold. First, the assumptions about the underlying intention
transition dynamics in HIQL align better with those observed in real-world behavioral experiments that
animal and human alternate between discrete strategies during decision making (Ashwood et al., 2022b),
rather than optimize their policy over a continuous time-varying reward as assumed by DIRL. Second, the
inner-loop IRL problem solver adapted by HIQL, the class of IQL algorithms, were shown to have better
performance in learning the expert’s unknown reward and higher computing efficiency than the maximum
entropy IRL algorithm used by DIRL (Kalweit et al., 2020).
As future work, we plan to relax the Markov assumption about the intention transition dynamics in HIQL
to incorporate non-Markovian intention dynamics, and also extend HIQL for an unknown number of reward
functions (Dimitrakakis & Rothkopf, 2012; Michini & How, 2012; Surana & Srivastava, 2014).
Acknowledgments
This work has been funded as part of BrainLinks-BrainTools, which is funded by the Federal Ministry of
Economics, Science and Arts of Baden-Württemberg within the sustainability programme for projects of the
Excellence Initiative II; as well as the Bernstein Award 2012, the Research Unit 5159 “Resolving Prefrontal
Flexibility” (Grant DI 1908/11-1), and the Deutsche Forschungsgemeinschaft (Grants DI 1908/3-1 and DI
1908/6-1) to I.D., and CRC/TRR 384 “IN-Code” to I.D. and J.B.
References
Mansour Alyahyay, Gabriel Kalweit, Maria Kalweit, Golan Karvat, Julian Ammer, Artur Schneider, Ahmed
Adzemovic, Andreas Vlachos, Joschka Boedecker, and Ilka Diester. Mechanisms of premotor-motor cortex
interactions during goal directed behavior. bioRxiv, pp. 2023–01, 2023.
12Published in Transactions on Machine Learning Research (09/2024)
Saurabh Arora and Prashant Doshi. A survey of inverse reinforcement learning: Challenges, methods and
progress. Artificial Intelligence , 297:103500, 2021.
Zoe Ashwood, Aditi Jha, and Jonathan W Pillow. Dynamic inverse reinforcement learning for characterizing
animal behavior. Advances in Neural Information Processing Systems , 35:29663–29676, 2022a.
Zoe C Ashwood, Nicholas A Roy, Iris R Stone, International Brain Laboratory, Anne E Urai, Anne K
Churchland, Alexandre Pouget, and Jonathan W Pillow. Mice alternate between discrete strategies during
perceptual decision-making. Nature Neuroscience , 25(2):201–212, 2022b.
Monica Babes, Vukosi Marivate, Kaushik Subramanian, and Michael L Littman. Apprenticeship learning
about multiple intentions. In Proceedings of the 28th International Conference on Machine Learning
(ICML-11) , pp. 897–904, 2011.
Leonard E Baum and Ted Petrie. Statistical inference for probabilistic functions of finite state Markov
chains.The Annals of Mathematical Statistics , 37(6):1554–1563, 1966.
Celia C Beron, Shay Q Neufeld, Scott W Linderman, and Bernardo L Sabatini. Mice exhibit stochastic and
efficient action switching during probabilistic decision making. Proceedings of the National Academy of
Sciences, 119(15):e2113961119, 2022.
Ariyan Bighashdel, Panagiotis Meletis, Pavol Jancura, and Gijs Dubbelman. Deep adaptive multi-intention
inverse reinforcement learning. In Machine Learning and Knowledge Discovery in Databases. Research
Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings,
Part I 21 , pp. 206–221. Springer, 2021.
Alex J Chan and Mihaela van der Schaar. Scalable Bayesian inverse reinforcement learning. arXiv preprint
arXiv:2102.06483 , 2021.
Jiayu Chen, Tian Lan, and Vaneet Aggarwal. Option-aware adversarial inverse reinforcement learning for
robotic control. In 2023 IEEE International Conference on Robotics and Automation (ICRA) , pp. 5902–
5908. IEEE, 2023.
Jaedeug Choi and Kee-Eung Kim. Nonparametric Bayesian inverse reinforcement learning for multiple
reward functions. Advances in Neural Information Processing Systems , 25, 2012.
Antonio Coronato, Muddasar Naeem, Giuseppe De Pietro, and Giovanni Paragliola. Reinforcement learning
for intelligent healthcare applications: A survey. Artificial Intelligence in Medicine , 109:101964, 2020.
Brice De La Crompe, Megan Schneck, Florian Steenbergen, Artur Schneider, and Ilka Diester. FreiBox: A
versatile open-source behavioral setup for investigating the neuronal correlates of behavioral flexibility via
1-photon imaging in freely moving mice. Eneuro, 10(4), 2023.
ChristosDimitrakakisandConstantinA Rothkopf. Bayesianmultitask inversereinforcementlearning. In Re-
cent Advances in Reinforcement Learning: 9th European Workshop, EWRL 2011, Athens, Greece, Septem-
ber 9-11, 2011, Revised Selected Papers 9 , pp. 273–284. Springer, 2012.
Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. A density-based algorithm for discovering
clusters in large spatial databases with noise. In KDD, volume 96, pp. 226–231, 1996.
Emily Beth Fox. Bayesian Nonparametric Learning of Complex Dynamical Phenomena . PhD thesis, Mas-
sachusetts Institute of Technology, 2009.
Kosuke Hamaguchi, Hiromi Takahashi-Aoki, and Dai Watanabe. Prospective and retrospective values inte-
grated in frontal cortex drive predictive choice. Proceedings of the National Academy of Sciences , 119(48):
e2206067119, 2022.
Ryoma Hattori, Bethanny Danskin, Zeljana Babic, Nicole Mlynaryk, and Takaki Komiyama. Area-specificity
and plasticity of history-dependent value coding during learning. Cell, 177(7):1858–1872, 2019.
13Published in Transactions on Machine Learning Research (09/2024)
Gabriel Kalweit, Maria Huegle, Moritz Werling, and Joschka Boedecker. Deep inverse Q-learning with
constraints. Advances in Neural Information Processing Systems , 33:14291–14302, 2020.
Tadeusz Władysław Kononowicz, Virginie van Wassenhove, and Valérie Doyère. Rodents monitor their error
in self-generated duration on a single trial basis. Proceedings of the National Academy of Sciences , 119(9):
e2108850119, 2022.
Sateesh Kumar, Jonathan Zamora, Nicklas Hansen, Rishabh Jangir, and Xiaolong Wang. Graph inverse
reinforcement learning from diverse videos. In Conference on Robot Learning , pp. 55–66. PMLR, 2023.
Minhae Kwon, Saurabh Daptardar, Paul R Schrater, and Xaq Pitkow. Inverse rational control with partially
observable continuous nonlinear dynamics. Advances in Neural Information Processing Systems , 33:7898–
7909, 2020.
Sergey Levine, Zoran Popovic, and Vladlen Koltun. Nonlinear inverse reinforcement learning with Gaussian
processes. Advances in Neural Information Processing Systems , 24, 2011.
Amarildo Likmeta, Alberto Maria Metelli, Giorgia Ramponi, Andrea Tirinzoni, Matteo Giuliani, and Mar-
cello Restelli. Dealing with multiple experts and non-stationarity in inverse reinforcement learning: An
application to real-life problems. Machine Learning , 110:2541–2576, 2021.
Bernard Michini and Jonathan P How. Bayesian nonparametric inverse reinforcement learning. In Machine
Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2012, Bristol, UK,
September 24-28, 2012. Proceedings, Part II 23 , pp. 148–163. Springer, 2012.
Payam Nasernejad, Tarek Sayed, and Rushdi Alsaleh. Multiagent modeling of pedestrian-vehicle conflicts
using adversarial inverse reinforcement learning. Transportmetrica A: Transport Science , 19(3):2061081,
2023.
Radford M Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of Compu-
tational and Graphical Statistics , 9(2):249–265, 2000.
Andrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In International Confer-
ence on Machine Learning (ICML) , volume 1, pp. 2, 2000.
Quoc Phong Nguyen, Bryan Kian Hsiang Low, and Patrick Jaillet. Inverse reinforcement learning with
locally consistent reward functions. Advances in Neural Information Processing Systems , 28, 2015.
Yael Niv. Reinforcement learning in the brain. Journal of Mathematical Psychology , 53(3):139–154, 2009.
Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In IJCAI, volume 7, pp.
2586–2591, 2007.
Matthew Rosenberg, Tony Zhang, Pietro Perona, and Markus Meister. Mice in a labyrinth show rapid
learning, sudden insight, and efficient exploration. Elife, 10:e66175, 2021.
Amit Surana and Kunal Srivastava. Bayesian nonparametric inverse reinforcement learning for switched
Markov decision processes. In 2014 13th International Conference on Machine Learning and Applications ,
pp. 47–54. IEEE, 2014.
Robert C Wilson and Anne GE Collins. Ten simple rules for the computational modeling of behavioral data.
Elife, 8:e49547, 2019.
Shoichiro Yamaguchi, Honda Naoki, Muneki Ikeda, Yuki Tsukada, Shunji Nakano, Ikue Mori, and Shin
Ishii. Identification of animal behavioral strategies by inverse reinforcement learning. PLoS Computational
Biology, 14(5):e1006122, 2018.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse
reinforcement learning. In AAAI, volume 8, pp. 1433–1438. Chicago, IL, USA, 2008.
Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. Modeling interaction via the principle of maximum
causal entropy. 2010.
14Published in Transactions on Machine Learning Research (09/2024)
A Theoretical and technical details
A.1 Proof of Theorem 4.3
Proof.The objective function J(Θ+|Θ)from problem (6) can be written as:
J/parenleftbig
Θ+|Θ/parenrightbig
=E
ξ∼D,η/bracketleftbig
logP/parenleftbig
ξ,η|Θ+/parenrightbig/bracketrightbig
=E
ξ∼D/bracketleftigg/summationdisplay
ηP(η|ξ,Θ) log P/parenleftbig
ξ,η|,Θ+/parenrightbig/bracketrightigg
=E
ξ∼D/bracketleftigg/summationdisplay
ηP(η|ξ,Θ) log/parenleftigg
Π+
z0πr+
z0(s0,a0)n/productdisplay
t=1Λ+
zt−1ztπr+
zt(st,at)/parenrightigg/bracketrightigg
=E
ξ∼D/bracketleftigg/summationdisplay
ηP(η|ξ,Θ) log Π+
z0/bracketrightigg
+E
ξ∼D/bracketleftigg/summationdisplay
ηP(η|ξ,Θ)n/summationdisplay
t=1log Λ+
zt−1zt/bracketrightigg
+E
ξ∼D/bracketleftigg/summationdisplay
ηP(η|ξ,Θ)n/summationdisplay
t=0logπr+
zt(st,at)/bracketrightigg
=E
ξ∼D
K/summationdisplay
i=1/summationdisplay
ηP(η|ξ,Θ)Ii(z0)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
P(z0=i|ξ,Θ)log Π+
i
+E
ξ∼D
K/summationdisplay
i=1K/summationdisplay
j=1n/summationdisplay
t=1/summationdisplay
ηP(η|ξ,Θ)Ii(zt−1)Ij(zt)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
P(zt−1=i,zt=j|ξ,Θ)log Λ+
ij

+E
ξ∼D
K/summationdisplay
i=1n/summationdisplay
t=0/summationdisplay
ηP(η|ξ,Θ)Ii(zt)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
P(zt=i|ξ,Θ)logπr+
i(st,at)

=E
ξ∼D/bracketleftiggK/summationdisplay
i=1P(z0=i|ξ,Θ) log Π+
i/bracketrightigg
(A.1a)
+E
ξ∼D
K/summationdisplay
i=1K/summationdisplay
j=1n/summationdisplay
t=1P(zt−1=i,zt=j|ξ,Θ) log Λ+
ij
 (A.1b)
+E
ξ∼D/bracketleftiggK/summationdisplay
i=1n/summationdisplay
t=0P(zt=i|ξ,Θ) logπr+
i(st,at)/bracketrightigg
, (A.1c)
whereIiis the indicator function with Ii(x) = 1for allx=iand0otherwise. Thus maximizing J(Θ+|Θ)
over Θ+(problem (6)) is equivalent to separately maximizing (A.1a) over Π+, (A.1b) over Λ+, and (A.1c)
overR+=/braceleftbig
r+
1,...,r+
K/bracerightbig
. The first two optimization problems can be formally written as:
maximize (over Π+)Eξ∼D/bracketleftig/summationtextK
i=1P(z0=i|ξ,Θ) log Π+
i/bracketrightig
subject to Π+⪰0,1TΠ+= 1,(7)
and
maximize (over Λ+)Eξ∼D/bracketleftig/summationtextK
i=1/summationtextK
j=1/summationtextn
t=1P(zt−1=i,zt=j|ξ,Θ) log Λ+
ij/bracketrightig
subject to Λ+
i:⪰0,1TΛ+
i:= 1, i= 1,...,K.(8)
Note that (A.1c) has similar structure as the optimization objective of IRL (problem (1)), and can be further
separated for each r∈R, thus maximizing (A.1c) over R+is equivalent to independently solving an IRL
15Published in Transactions on Machine Learning Research (09/2024)
problem for each ri, with thetth demonstration weighted by P(zt=i|ξ,Θ):
maximize (over r+
i)Eξ∼D/bracketleftig/summationtextn
t=0P(zt=i|ξ,Θ) logπr+
i(st,at)/bracketrightig
subject to πr+
i(s,a) = exp (Q(s,a)−log/summationtextexpQ(s,·)), for alls∈S,a∈A
Q(s,a) =r+
i(s,a) +γ/summationtext
s′∈SP(s,a,s′) maxa′∈AQ(s′,a′), for alls∈S,a∈A.(9)
Similar to (1), the constraints on policy πr+
iare introduced here to make the IRL problem tractable.
A.2 Computing required posterior probabilities
To obtain the posterior probability terms P(z0=i|ξ,Θ),P(zt−1=i,zt=j|ξ,Θ), and P(zt=i|ξ,Θ)
requiredforevaluatingtheobjectivefunctionsof(7), (8)and(9), weapplytheBaum-Welchalgorithm(Baum
& Petrie, 1966) as follows.
Letαt∈RKbe the forward probability which denotes the posterior probability of observing the expert
demonstrations up until time tand thetth demonstration was generated under reward function ri,i.e.,
αt(i) =P(ξ0:t,zt=i|Θ) (A.2)
=/braceleftbiggΠiπri(s0,a0)t= 0
αT
t−1Λ:iπri(st,at)t>0,(A.3)
foralli= 1,...,K. Similarly, wedefinethebackwardprobability βt∈RKdenotingtheposteriorprobability
of observing the expert demonstrations after time t, given that the tth demonstration was generated under
reward function ri:
βt(i) =P(ξt+1:n|zt=i,Θ) (A.4)
=/braceleftbigg/summationtextK
j=1βt+1(j)Λijπrj(st+1,at+1)t<n
1 t=n,(A.5)
for alli= 1,...,K. With (A.3) and (A.5), we can efficiently obtain the forward and backward probabilities
for each demonstration in a recursive form. Finally, we can compute the posterior probabilities
P(zt=i|ξ,Θ) =αt(i)βt(i)
αT
tβt, (A.6)
and
P(zt−1=i,zt=j|ξ,Θ) =αt(i)Λijπrj(st+1,at+1)βt+1(j)
/summationtextK
i′=1/summationtextK
j′=1αt(i′)Λi′j′πrj′(st+1,at+1)βt+1(j′). (A.7)
16Published in Transactions on Machine Learning Research (09/2024)
B Algorithms
Algorithm 1 Inverse action-value iteration.
givenexpert demonstrations D.
1:initializer,Q.
2: Obtain expert policy πEby counting the state-action visitation in D.
3:repeat
4: for alls∈Sdo
5: Calculate µ(s,a):= log(πE(s,a))−γ/summationtext
s′∈SP(s,a,s′) max a′∈AQ(s′,a′)for alla∈A.
6: Update r(s,·)by solving (5) via least squares.
7: Update Q(s,a):=r(s,a) +γ/summationtext
s′∈SP(s,a,s′) max a′∈AQ(s′,a′)for alla∈A.
8: end for
9:untilstopping criterion is satisfied.
Algorithm 2 Inverse Q-learning.
givenexpert demonstrations D; learning rate αr,αQ, andαSh.
1:initializer,Q,QSh, and state-action visitation counter ρ.
2:repeat
3: for allξ∈Ddo
4: fort= 0,...,n−1do
5: Observe state-action pairs (st,at,st+1)fromξ, and update ρ(st,at):=ρ(st,at) + 1.
6: Obtain πE(st,a):=ρ(st,a)//summationtext
a′∈Aρ(st,a′)for alla∈A.
7: Update QSh(st,at):= (1−αSh)QSh(st,at) +αSh·γmaxa∈AQ(st+1,a).
8: Calculate µ(st,a):= log(πE(st,a))−QSh(st,a)for alla∈A.
9: Update r(st,at):= (1−αr)r(st,at) +αr/parenleftig
µ(st,at) +1
card (A−at)/summationtext
b∈A−at(r(st,b)−µ(st,b))/parenrightig
.
10: Update Q(st,at):= (1−αQ)Q(st,at) +αQ(r(st,at) +γmaxa∈AQ(st+1,a)).
11: end for
12: end for
13:untilstopping criterion is satisfied.
Algorithm 3 Hierarchical inverse Q-learning.
givenexpert demonstrations Dand reward set cardinality K.
1:initialize Π,Λ,r1,...,r K.
2:repeat
3: Calculating the Boltzmann policy πrifor alli= 1,...,K.
4: for allξ∈Ddo
5: fori= 1,...,K,j= 1,...,K,t= 0,...,n do
6: Calculate αt(i)andβt(i)according to (A.3) and (A.5).
7: Calculate P(zt=i|ξ,Θ)andP(zt−1=i,zt=j|ξ,Θ)according to (A.6) and (A.7).
8: end for
9: end for
10: Update ΠandΛaccording to (10) and (11).
11: fori= 1,...,K do
12: initialize expert demonstration subset D′.
13: for allξ∈Ddo
14: fort= 0,...,n−1do
15: Observe state-action pairs (st,at,st+1)fromξ; sampleu∼U(0,1).
16: ifu<P(zt=i|ξ,Θ)then
17: D′:=D′∪{(st,at,st+1)}.
18: end if
19: end for
20: end for
21: Update riby applying Algorithm 1 to D′ifPis known, elseusing Algorithm 2.
22: end for
23:untilstopping criterion is satisfied.
17Published in Transactions on Machine Learning Research (09/2024)
C Datasets and model training
C.1 Gridworld benchmark
The following pseudo-code shows the expert policy for each episode that we used to obtain expert demon-
strations in the gridworld environment in §5.1.
1:initializes:= (0,0),π:=πgoal,t:= 0.
2:repeat
3:a∼π.
4:s∼P(s,a,·).
5: ifshas barrier ‘ #’then
6: Switch to another policy (30%).
7: else ift= 8then
8:π:=πabandon(50%).
9: end if
10:t:=t+ 1.
11:until (0,0)or(4,4)is reached.
The whole expert demonstration dataset consisted of 1024trajectories. All evaluated algorithms were fit to
multiple sub-datasets with different number of expert trajectories, and each dataset was analyzed using a
5-fold cross-validation. Note that since the EM process does not guarantee to find the global optimum, for
each training, HIAVI was randomly initialized repeatedly for 10times and the best performed initialization
(evaluated by test set log-likelihood) was selected for analysis. The initial intention distribution Πwas
initialized uniformly, and the intention transition matrix Λwas initialized as: Λ = 0.95×I+N(0,0.05×I),
whereNdenotes the normal distribution and I∈R2×2is the identity matrix. This initial Λwas then
normalized so that each row added up to 1. The discount factor of the MDP was set to be γ= 0.9.
C.2 Real-world mice navigation benchmark
For comparability with the results in Ashwood et al. (2022a), we obtained their pre-processed mouse trajecto-
ries for water-restricted and water-unrestricted animals from https://github.com/97aditi/dynamic_irl .
The original recorded animal trajectories from Rosenberg et al. (2021) are provided with MIT open source
license at the following repository: https://github.com/markusmeister/Rosenberg-2021-Repository .
For the pre-processing, Ashwood et al. (2022a) used a clustering algorithm (based on DBSCAN (Ester et al.,
1996)) for aligning trajectories across animals and bouts to reduce variability. After the pre-processing, they
obtained 200trajectories from the water-restricted animals and 207trajectories from the water-unrestricted
animals. 20%of trajectories from each cohort were held out as a test set.
WeusedthesourcecodeandthebestperformedsetofhyperparametersprovidedbyAshwoodetal.(2022a)to
train DIRL on the animal trajectories. All HIAVI algorithms were trained for 10repeated runs with different
initializations, and the results from the initializations with the highest test set log-likelihood was selected for
analysis. The initial intention distribution Πwas initialized uniformly, and the intention transition matrix
Λwas initialized as: Λ = 0.95×I+N(0,0.05×I), whereNdenotes the normal distribution and I∈RK×K
is the identity matrix. This initial Λwas then normalized so that each row added up to 1. The MDP was
defined to have a deterministic state transition function Pand the discount factor was set to be γ= 0.99.
C.3 Application to mice reversal-learning behavior
The expert demonstrations for this dynamic reversal-learning task was collected from a cohort of mice
consisted of 9mice in total. Behavior recordings for each subject were repeated for at least 7independent
sessions with an average of ca. 87trials per session.
We employed a multi-stage fitting procedure to select hyperparameters and to allow us to fit HIQL indi-
vidually to each animal. In the first stage, we concatenated the data from all animals in a single dataset
18Published in Transactions on Machine Learning Research (09/2024)
together. We then performed multiple IQL (single intention HIQL) with different history length ℓh= 1,..., 5
on the concatenated dataset. Out of the 5different values, We chose the ℓhthat resulted in the best test
set log-likelihood for subsequent stages. In the second stage, we run multiple HIQL with different num-
ber of intentions K= 2,..., 5again to the concatenated dataset to obtain a global fit. For each fitting,
we performed 10different initializations and the best performed initialization was selected. The initial in-
tention distribution Πwas initialized uniformly, and the intention transition matrix Λwas initialized as:
Λ = 0.95×I+N(0,0.05×I), whereNdenotes the normal distribution and I∈RK×Kis the identity
matrix. This initial Λwas then normalized so that each row added up to 1. In the last stage of the fitting
procedure, we wanted to obtain an independent but aligned HIQL fit for each animal, so we initialized the
parameters for each animal with the best global fit parameters from all animals together, omitting the ne-
cessity to permute the retrieved intentions from each animal so as to map semantically similar intentions to
one another. A 5-fold cross-validation was used to split the training and test dataset. The discount factor
fot this experiment were set to be γ= 0.99.
D Code availability
Implementation for the class of HIQL algorithms can be found at https://github.com/haozhu10015/hiql .
19