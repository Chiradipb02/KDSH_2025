Published in Transactions on Machine Learning Research (06/2024)
Directional Convergence Near Small Initializations and
Saddles in Two-Homogeneous Neural Networks
Akshay Kumar kumar511@umn.edu
Department of Electrical and Computer Engineering
University of Minnesota, Minneapolis, MN
Jarvis Haupt jdhaupt@umn.edu
Department of Electrical and Computer Engineering
University of Minnesota, Minneapolis, MN
Reviewed on OpenReview: https: // openreview. net/ forum? id= hfrPag75Y0
Abstract
This paper examines gradient ﬂow dynamics of two-homogeneous neural networks for small
initializations, where all weights are initialized near the origin. For both square and logistic
losses, it is shown that for suﬃciently small initializations, the gradient ﬂow dynamics spend
suﬃcient time in the neighborhood of the origin to allow the weights of the neural network to
approximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of a neural
correlation function that quantiﬁes the correlation between the output of the neural network
and corresponding labels in the training data set. For square loss, it has been observed that
neural networks undergo saddle-to-saddle dynamics when initialized close to the origin.
Motivated by this, this paper also shows a similar directional convergence among weights of
small magnitude in the neighborhood of certain saddle points.
1 Introduction
Massively overparameterized deep neural networks trained with (stochastic) gradient descent are widely
known to be immensely successful architectures for inference. Recent works have attributed this success to
theimplicit regularization of gradient descent – the mysterious ability of gradient descent to ﬁnd a solution
that generalizes well despite the non-convexity of the loss landscape, the presence of spurious optima, and
no explicit regularization (Neyshabur et al., 2015; Soudry et al., 2018).
To resolve this mystery, several works have studied the dynamics of gradient descent during training of neural
networks (Jacot et al., 2018; Chizat et al., 2019; Mei et al., 2019; Chizat & Bach, 2018). An important
observation emerging from these studies has been the eﬀect of initialization on the trajectory of gradient
descent. Forlargeinitialization, thegradientdescentdynamicsareapproximatelylinearandcanbedescribed
by the so-called Neural Tangent Kernel (NTK) (Jacot et al., 2018; Arora et al., 2019b). This regime is also
referred to as lazy training (Chizat et al., 2019), since the weights of the neural networks do not change much
and remain near their initializations throughout training, preventing the neural networks from learning the
underlying features from the data.
Incontrast, forsmallinitializations, gradientdescentdynamicsishighlynon-linearandexhibits feature learn-
ing(Geiger et al., 2020; Yang & Hu, 2021; Mei et al., 2019). Additionally, the beneﬁt of small initializations
over large in terms of generalization performance has also been observed under various settings (Geiger et al.,
2020). For example, Chizat et al. (2019) train deep convolutional neural networks with varying scales of
initialization while keeping other aspects of the neural network ﬁxed, and a signiﬁcant drop in performance
is observed upon increasing the scale of initialization. In other recent works such as Jacot et al. (2022);
Boursier et al. (2022); Pesme & Flammarion (2023), a phenomenon termed saddle-to-saddle dynamics has
been observed during training. These works reveal that the trajectory of gradient descent passes through
1Published in Transactions on Machine Learning Research (06/2024)
a sequence of saddle points during training, in stark contrast to the linear dynamics observed in the NTK
regime.
The investigation into the gradient descent dynamics of neural networks with small initializations has spurred
numerous inquiries, yet a comprehensive theoretical framework remains elusive. The study of linear neural
networks has led to valuable insights into the sparsity-inducing tendencies of gradient descent (Woodworth
et al., 2020; Arora et al., 2019a). These tendencies also appear to be present in non-linear neural networks
(Chizat et al., 2019; Chizat & Bach, 2018), however, rigorous results are limited to two-layer Rectiﬁed Linear
Unit (ReLU) and Leaky-ReLU networks under various simple data-speciﬁc assumptions such as orthogonal
inputs (Boursier et al., 2022), linearly separable data (Min et al., 2024; Wang & Ma, 2022; Lyu et al.,
2021; Wang & Ma, 2023), the XOR mapping (Brutzkus & Globerson, 2019) or univariate data (Williams
et al., 2019; Safran et al., 2022). Another important line of work has uncovered an intriguing phenomenon
of directional convergence among neural network weights during the initial training stages (Maennel et al.,
2018; Luo et al., 2021; Brutzkus & Globerson, 2019; Atanasov et al., 2022; Chen et al., 2023). In Maennel
et al. (2018), it is shown that the weights of two-layer ReLU neural networks, trained using gradient ﬂow
with small initialization, converge in direction early in the training process while maintaining small norm.
Although this result primarily describes dynamics near initialization, it constitutes a crucial step towards a
comprehensive understanding of neural network training dynamics and has contributed signiﬁcantly towards
understanding the training dynamics in some of the aforementioned works (Boursier et al., 2022; Min et al.,
2024; Wang & Ma, 2023; Lyu et al., 2021). However, the work of Maennel et al. (2018) is limited to two-layer
ReLU networks and raises the question of whether this phenomenon holds for other neural networks.
2 Our Contributions
This work establishes the phenomenon of directional convergence in a more general setting. Speciﬁcally, we
study the gradient ﬂow dynamics resulting from training of two-homogeneous neural networks near small
initializations and also at certain saddle points .
A neural network H, whereH(x;w)is the real-valued output of the neural network, xis the input, and w
is a vector containing all the weights, is deﬁned here to be two-(positively) homogeneous if
H(x;cw) =c2H(x;w),for allc≥0.
While this class does not encompass deep neural networks, it is broad enough to include several interesting
types of neural networks. Let σ(x)denote the ReLU (or Leaky-ReLU) function, then, some examples of
two-homogeneous neural networks include
•Two-layer ReLU networks: H(x;{vk,uk}H
k=1) =/summationtextH
k=1vkσ(x/latticetopuk).
•Single-layer squared ReLU networks: H(x;{uk}H
k=1) =/summationtextH
k=1pkσ(x/latticetopuk)2,wherepk∈{− 1,1}.
•Deep ReLU networks with only two trainable layers, for example H(x;W1,W2) =
v/latticetopσ(W2σ(W1x)),where vis a ﬁxed vector. We emphasize that this class includes any L−layer
deep ReLU network with exactly two trainable layers (not necessarily two consecutive layers).
We consider a supervised learning setup for training and assume that {xi,yi}n
i=1is the training dataset,
X= [x1,...,xn]∈Rd×n,y= [y1,...,yn]/latticetop∈Rn, andH(X;w) = [H(x1;w),...,H(xn;w)]/latticetop∈Rnis the
vector containing the output of neural network for all inputs, where w∈Rk.We do not make any structural
assumptions on the training dataset.
In describing our results, a vital role is played by a quantity we refer to as the neural correlation function
(NCF), which for a ﬁxed vector z∈Rnand neural network Has above is deﬁned as
Nz,H(w) =z/latticetopH(X;w).
The NCF is a measure of the correlation between the vector zand the output of the neural network. For a
given NCF, we refer to the following constrained optimization problem as a constrained NCF :
max
/bardblw/bardbl2
2=1Nz,H(w).
2Published in Transactions on Machine Learning Research (06/2024)
(a)
 (b)
Figure 1: A two-dimensional scenario where a single-layer squared ReLU neural network with 20hid-
den neurons is trained by gradient descent. The network architecture is deﬁned as H(x1,x2;{ui}20
i=1) =/summationtext20
i=1max(0,u1ix1+u2ix2)2, where uirepresents the weights for the ith neuron. For training, we use 50
unit norm inputs and corresponding labels are generated using the function H∗(x1,x2) = 5 max(0 ,x1)2+
4 max(0,−x1)2. We use square loss and optimize using gradient descent for 50000 iterations with step-size
5·10−5. At initialization, the weights of each hidden neuron are drawn from Gaussian distribution with
standard deviation 10−5. Panel (a): the evolution of training loss and the /lscript2-norm of all the weights with
iterations. Panel (b): the evolution of arctan( u2i(t)/u1i(t))(the angle ui(t)makes with the positive x−axis)
for all hidden neurons. We see that the norm of the weights remain small and loss barely changes, though
the weight vectors converge in direction to their ﬁnal location (denoted with red dots).
Ourﬁrstmainresult(Theorem5.1)showsthat, forsquareandlogisticloss1, iftheinitializationissuﬃciently
small, then, the gradient dynamics spends suﬃcient time near the origin such that the weights ware either
approximately 0, or approximately converge in direction to non-negative Karush-Kuhn-Tucker (KKT) points
of the constrained NCF
max
/bardblw/bardbl2
2=1Ny,H(w).
Our next main result (Theorem 5.5) shows a similar directional convergence near certain saddle points for
square loss. Speciﬁcally, we show that if initialized in a suﬃciently small neighborhood of that saddle point,
then the gradient dynamics spends suﬃcient time near the saddle point such that the weights with small
magnitude either approximately converge in direction to non-negative KKT points of the constrained NCF
deﬁned with respect to the residual error at that saddle point, or are approximately 0.
For illustration, we provide a brief “toy” example showing the phenomenon of directional convergence near
small initialization. We train a single-layer squared ReLU neural network using gradient descent and small
initialization, and provide in Figure 1 a visual depiction of (a)the overall loss and the /lscript2norm of the network
weights, and (b)the angle the weight vectors make with the positive horizontal axis, all as a function of the
number of training iterations. (See the ﬁgure caption for more speciﬁc experimental details.) It is evident
that the training loss barely changes, and the norm of all the weights remains small, indicating that gradient
dynamics is still near the origin. This is not surprising since the origin is a saddle point. However, the direc-
tions of the individual weight vectors for the neurons undergo signiﬁcant changes. This experiment suggests
that while the gradient dynamics may not signiﬁcantly change the weight vector magnitudes, it does change
their directions. Further, and perhaps more interestingly, these directions not only change but also appear to
converge. The objective of this paper is to explain how such a phenomenon could occur by just minimizing
the loss using gradient descent, and also characterize the directions along which the weights converge.
Probably the most related existing work to our eﬀort here is Maennel et al. (2018), which exclusively focuses
on two-layer ReLU neural networks. Compared to that work, ours establishes directional convergence near
smallinitializationsfortwo-homogeneousneuralnetworks,amuchwiderclassofneuralnetworks,highlighting
1The results in Theorem 5.1 apply, in fact, to a more general class of loss functions.
3Published in Transactions on Machine Learning Research (06/2024)
the inherent importance of homogeneity for these types of phenomena. As alluded above, this class also
includes deep ReLU networks with only two trainable layers (not necessarily two consecutive layers), for
which the results of Maennel et al. (2018) are inapplicable. Further, while Maennel et al. (2018) only focuses
on initialization, we also establish directional convergence near certain saddle points. This extension is
particularly pertinent because it has been observed in previous works that neural networks exhibit saddle-
to-saddle dynamics under small initialization. Consequently, our result describing dynamics near small
initialization and saddle points could be important for a better understanding of the training dynamics in
the future.
Finally, while the result of Maennel et al. (2018) has certainly advanced our understanding, their analysis
near small initializations relies on heuristic arguments and are not completely rigorous; see Min et al. (2024,
Section 2.2) for speciﬁc details. Our proof technique is rigorous and fundamentally diﬀerent from Maennel
et al. (2018) to handle a wider class of neural networks.
3 Preliminaries
In this section, we brieﬂy describe some preliminary concepts that will be useful in rigorously describing the
problem.
Throughout the paper, /bardbl·/bardbl 2denotes the /lscript2norm for a vector and the spectral norm for a matrix. For any
N∈N, we let [N] ={1,2,...,N}denote the set of positive integers less than or equal to N. We denote
derivatives by ˙x(t) =dx(t)
dt, and for the sake of brevity we may remove the independent variable tif it is clear
from the context. For a vector x,xidenotes its i-th entry. The k-dimensional sphere is denoted by Sk−1. A
KKT point of an optimization problem is deﬁned to be a non-negative KKT point if the objective value at
that KKT point is non-negative.
A function f:X→Ris called locally Lipschitz continuous if for every x∈Xthere exists a neighborhood
Uofxsuch thatfrestricted to Uis Lipschitz continuous. A locally Lipschitz continuous function is
diﬀerentiable almost everywhere (Borwein & Lewis, 2000, Theorem 9.1.2).
For any locally Lipschitz continuous function, f:X→R, itsClarke subdiﬀerential at a point x∈Xis the
set
∂f(x) =conv/braceleftBig
lim
i→∞∇f(xi) : lim
i→∞xi=x,xi∈Ω/bracerightBig
,
where Ωis any full-measure subset of Xsuch thatfis diﬀerentiable for all x∈Ω. The set∂f(x)is nonempty,
convex, and compact for all x∈X, and the mapping x→∂f(x)is upper-semicontinuous (Clarke et al.,
1998, Proposition 1.5). We denote by ∂f(x)the unique minimum norm subgradient.
Since the neural networks considered in this paper may be non-smooth (as a function of w), to rigorously
deﬁne the gradient ﬂow for such functions we use the notion of o-minimal structures (Coste, 2000). In
particular, we consider neural networks that are deﬁnable under some o-minimal structure , a mild technical
assumption that is satisﬁed by almost all modern neural networks (Ji & Telgarsky, 2020), including the
examples presented in Section 2. Formally, an o-minimal structure is a collection S={Sn}∞
n=1where each
Snis a set of subsets of Rncontaining all algebraic subsets of Rnand is closed under ﬁnite union and
intersection, complement, projection, and Cartesian product. The elements of S1are the ﬁnite unions of
points and intervals. For a given o-minimal structure S, a set A⊂Rnis deﬁnable if A∈Sn. A function
f:D→Rmwith D⊂Rnis deﬁnable if the graph of fis inSn+m. Since a set remains deﬁnable
under projection, the domain Dis also deﬁnable; see Coste (2000) for a detailed introduction of o-minimal
structures.
Using the notion of deﬁnability under o-minimal structures, we deﬁne gradient ﬂow for non-smooth functions
following Davis et al. (2018); Ji & Telgarsky (2020); Lyu & Li (2020). A function z:I→Rdon the interval
Iis anarcif it is absolutely continuous for any compact sub-interval of I. An arc is diﬀerentiable almost
everywhere, and the composition of an arc with a locally Lipschitz function is also an arc. For any locally
Lipschitz and deﬁnable function f(x),x(t)evolves under gradient ﬂow of f(x)if it is an arc, and
˙x(t)∈−∂f(x(t)),for a.e.t≥0. (1)
4Published in Transactions on Machine Learning Research (06/2024)
Ifx(t)evolves under positive gradient ﬂow of f(x), i.e., ˙x(t)∈∂f(x(t)),for a.e.t≥0,we still call x(t)a
gradient ﬂow of f(x). In what follows, it will be clear from the context whether it is positive or negative
gradient ﬂow.
4 Problem Setup
Within the framework introduced above, we consider the minimization of
L(w) =n/summationdisplay
i=1/lscript(H(xi;w),yi), (2)
where/lscript(ˆy,y)is a loss function; in this work, we assume the loss function is deﬁnable and have locally
Lipschitz gradient.
Assumption 1. The loss function /lscript(ˆy,y)is deﬁnable under some o-minimal structure that includes poly-
nomials and exponentials, and ∇ˆy/lscript(ˆy,y)is locally Lipschitz in ˆy.
The above assumption is satisﬁed by commonly used loss functions such as square loss and logistic loss.
Next, as alluded above, we also assume that the neural networks under consideration are two-homogeneous ,
a property we formalize via the following assumption.
Assumption 2. For any ﬁxed x,H(x;w)is locally Lipschitz and deﬁnable under some o-minimal structure
that includes polynomials and exponentials, and for all c≥0,H(x;cw) =c2H(x;w).
In Wilkie (1996), it was shown that there exists an o-minimal structure in which polynomials and exponential
functions are deﬁnable. Also, the deﬁnability of a function is stable under algebraic operations, composition,
inverse, maximum, and minimum. Since ReLU/Leaky-ReLU is a maximum of two polynomials, typical
neural networks involving ReLU activation function are deﬁnable (Ji & Telgarsky, 2020). Also, under the
above assumptions, L(w)is deﬁnable. Finally, we also require Hto be two-homogeneous for our results to
hold, which rules out deep neural networks such as deep ReLU networks with more than 2 trainable layers.
Next, since L(w)is deﬁnable, the gradient ﬂow w(t)is an arc that satisﬁes for a.e. t≥0
˙w(t)∈−∂L(w(t)),w(0) =δw0, (3)
where w0is a vector and δis a positive scalar that controls the scale of initialization.
For diﬀerential inclusions, it is possible to have multiple solutions for the same initialization. This leads to
technical diﬃculties in proving our results. We will address this diﬃculty by making use of the following
deﬁnition which is inspired by Lyu et al. (2021) and will be discussed in more detail in the later sections.
Deﬁnition 4.1. Supposeg(w) :Rk→Ris locally Lipschitz and deﬁnable under some o-minimal structure,
and consider the following diﬀerential inclusion with initialization ˜w
dw
dt∈∂g(w),w(0) = ˜w,for a.e.t≥0.
We say ˜wis a non-branching initialization if the diﬀerential inclusion has a unique solution for all t≥0.
Before proceeding to the main results, we brieﬂy state additional deﬁnitions which are used in the paper.
Let
β:= sup{/bardblH(X;w)/bardbl2:w∈Sk−1},
where Xandydenote the training examples and labels. For z∈Rn, we deﬁne /lscript/prime(z,y) =
[∇ˆy/lscript(z1,y1),...,∇ˆy/lscript(zn,yn)]/latticetop∈Rn. Now, since∇ˆy/lscript(ˆy,y)is locally Lipschitz in ˆy, we deﬁne ˆβsuch that if
/bardblz/bardbl2≤β, then
/bardbl/lscript/prime(z,y)−/lscript/prime(0,y)/bardbl2≤ˆβ/bardblz/bardbl2, (4)
and ˜β=ˆββ+/bardbl/lscript/prime(0,y)/bardbl2.
5Published in Transactions on Machine Learning Research (06/2024)
5 Main Results
5.1 Directional Convergence Near Initialization
We are now in position to state our ﬁrst main result establishing approximate directional convergence of the
weights near small initialization.
Theorem 5.1. Letw0be a unit norm vector and a non-branching initialization of the diﬀerential inclusion
˙u∈∂N−/lscript/prime(0,y),H(u),u(0) = w0. (5)
For any/epsilon1∈(0,η), whereηis a positive constant2, there exist C > 1andδ>0such that the following holds:
for anyδ∈(0,δ)and solution w(t)of eq.(3)with initialization w(0) =δw0, we have
/bardblw(t)/bardbl2≤√
Cδ,for allt∈/bracketleftbig
0,T/bracketrightbig
,
whereT=ln(C)
4β˜β.Further, either
/bardblw(T)/bardbl2≥δη,andw(T)/latticetopˆu
/bardblw(T)/bardbl2≥1−/parenleftbigg
1 +3
2η/parenrightbigg
/epsilon1,
where ˆuis a non-negative KKT point of
max
/bardblu/bardbl2
2=1N−/lscript/prime(0,y),H(u) =−/lscript/prime(0,y)/latticetopH(X;u),
or
/bardblw(T)/bardbl2≤2δ/epsilon1.
Here,/epsilon1represents the level of directional convergence of the weight, and Crepresents how long gradient ﬂow
needs to stay near the origin to ensure the desired level of directional convergence.
In words, the ﬁrst part of the result establishes that for a given choice of /epsilon1>0, we can choose δsuﬃciently
small such that the norm of the weights remains small for all t∈[0,T], indicating that gradient ﬂow remains
near the origin. The second part quantiﬁes what happens at the time T; there are two possible outcomes.
In one scenario, the weights approximately converge in direction towards a non-negative KKT point of
the constrained NCF deﬁned with respect to −/lscript/prime(0,y)and neural network H(additionally,/bardblw(T)/bardbl2≥δη,
whereηis a constant that does not depend on δ.) In contrast, in the second scenario /bardblw(T)/bardbl2≤2δ/epsilon1, where
we can choose /epsilon1andδboth to be arbitrarily small. Thus, compared to the ﬁrst scenario, in the second
scenario, the weights get much closer to the origin. In fact, as it will become more clear from the proof
sketch later, this happens because the gradient dynamics of the NCF can converge to 0.
It is easy to verify that for square loss, /lscript(ˆy,y) =1
2(ˆy−y)2and−/lscript/prime(0,y) =y. Similarly, for logistic loss,
/lscript(ˆy,y) = ln(1+e−ˆyy)and−/lscript/prime(0,y) =y/2. Hence, for both of these loss functions, the weights approximately
converge in direction towards a non-negative KKT point of
max
/bardblu/bardbl2
2=1Ny,H(u) =y/latticetopH(X;u).
Now, for general training data, it may not be possible to get a closed form expression for the KKT points of
the NCF. In Appendix G we provide some simple examples where closed form expressions can be computed,
which may be helpful to the readers.
Finally, note that we require w0to be a non-branching initialization of eq. (5). The necessity for such a
requirement essentially arises because there could exist multiple solutions for diﬀerential inclusions. We
discuss it in more detail after providing the proof sketch of the above theorem. However, we note that if
the neural network H(x;w)has locally Lipschitz gradients then this requirement is always satisﬁed, since in
that case eq. (5) always has a unique solution. This would include, for example, the squared ReLU neural
network.
2Here,ηdepends on the solution of eq. (5), which solely relies on X,y,H,w0, and is independent of δ. See Lemma C.7 and
the proof for more details.
6Published in Transactions on Machine Learning Research (06/2024)
5.1.1 Proof Sketch of Theorem 5.1
We provide a brief proof sketch for Theorem 5.1 here; the complete proof can be found in Appendix C. The
proof ultimately relies upon two lemmas. The ﬁrst one describes the approximate dynamics of w(t)in the
initial stages of training for small initialization.
Lemma 5.2. LetC > 1be an arbitrarily large constant and w(t)be any solution of eq. (3)with initialization
w(0) =δw0, whereδ≤/radicalBig
1
Cand/bardblw0/bardbl2= 1. Then, for all t∈/bracketleftBig
0,ln(C)
4β˜β/bracketrightBig
,
/bardblw(t)/bardbl2≤√
Cδ. (6)
Further, for the diﬀerential inclusion
˙u∈∂N−/lscript/prime(0,y),H(u),u(0) = w0, (7)
and any/epsilon1>0there exists a small enough δ>0such that for any δ∈(0,δ),
/vextenddouble/vextenddouble/vextenddouble/vextenddoublew(t)
δ−u(t)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/epsilon1,for allt∈/bracketleftbigg
0,ln(C)
4β˜β/bracketrightbigg
, (8)
where u(t)is a certain solution of eq. (7).
The ﬁrst part of the lemma shows that for suﬃciently small δthe gradient dynamics can spend an arbitrarily
large time near the origin. To understand the implications of the second part, let us ﬁrst focus on the
diﬀerential inclusion in eq. (7), which is the positive gradient ﬂow of the NCF deﬁned with respect to
−/lscript/prime(0,y)and neural network H. From eq. (8), we observe that for small initialization, in the initial stages of
the dynamics, w(t)/δis approximately equal to u(t). To get an intuitive idea about the proof of the above
lemma, consider the evolution of w(t)near small initializations, which approximately can be expressed as
˙w∈−n/summationdisplay
i=1∇ˆy/lscript(H(xi;w),yi)∂H(xi;w)≈−n/summationdisplay
i=1∇ˆy/lscript(0,yi)∂H(xi;w).
SinceH(x;w)is two-homogeneous, we have that ∂H(x;w)is one-homogeneous. Hence, dividing the above
equation by δwe get
˙w/δ≈−1
δn/summationdisplay
i=1∇ˆy/lscript(0,yi)∂H(xi;w) =−n/summationdisplay
i=1∇ˆy/lscript(0,yi)∂H(xi;w/δ) =∂N−/lscript/prime(0,y),H(w/δ).
Since w(0)/δ=w0, from the above equation we observe that the evolution of w(t)/δis approximately
governed by the diﬀerential inclusion in eq. (7). Thus, one expects w(t)/δto be close to a certain solution
of eq. (7), provided /bardblw(t)/bardbl2remains small.
Now, since δis a positive scalar, dividing w(t)by it does not change the direction of w(t). Further, note
that the dynamics of u(t)do not depend on δand the approximation in eq. (8) can be made to hold for
an arbitrarily long time by choosing suﬃciently small δ. Thus, if we choose Clarge enough such that
the approximation in eq. (8) is valid for a suﬃciently long time in which u(t)approximately converges in
direction, then by virtue of eq. (8), w(t)would also approximately converge in direction.
Thus, our aim is to establish approximate directional convergence of u(t)within some ﬁnite time. For this,
we turn towards analyzing the gradient ﬂow dynamics of the NCF. Recall that, for a given vector z, the
NCF is deﬁned as
Nz,H(u) =z/latticetopH(X;u), (9)
and gradient ﬂow will satisfy for a.e. t≥0
du
dt∈∂Nz,H(u),u(0) = u0, (10)
7Published in Transactions on Machine Learning Research (06/2024)
Figure 2: The gradient ﬁeld of f(u1,u2) =u1|u2|.
where u0is the initialization.
Since the function value increases along the gradient ﬂow trajectory and Nz,H(u)may not be bounded from
above, the gradient ﬂow trajectory can potentially diverge to inﬁnity and take Ny,H(u)to inﬁnity along with
it. However, in the following lemma we show that the gradient ﬂow will always converge in direction, and
also characterize those directions.
Lemma 5.3. For any solution u(t)of eq.(10), either limt→∞Nz,H(u(t)) =∞orlimt→∞Nz,H(u(t)) = 0.
Also, either limt→∞u(t)
/bardblu(t)/bardbl2exists or limt→∞u(t) = 0.Iflimt→∞u(t)
/bardblu(t)/bardbl2exists then its value, say u∗, must
be a non-negative KKT point of the optimization problem
max
/bardblu/bardbl2
2=1Nz,H(u) =z/latticetopH(X;u). (11)
The above lemma states that any solution of eq. (10) will either converge to 0or converge in direction to a
KKT point of the constrained NCF. To establish directional convergence in the above lemma, we follow a
similar technique as in Ji & Telgarsky (2020, Theorem 3.1).
To prove Theorem 5.1 from here, we combine Lemma 5.2 and Lemma 5.3. From a given initialization δw0,
we get w0. Then, for the solution u(t)of the diﬀerential inclusion in eq. (7), using Lemma 5.3, we choose T
large enough such that u(T)either approximately converges in direction to the KKT point of the NCF or
gets close to 0. Then, based on that T, using Lemma 5.2, we choose δsuﬃciently small such that w(t)/δis
close to u(t), for allt∈[0,T]. The result follows.
There is, however, one issue with the above argument. The diﬀerential inclusions could have multiple
solutions, and in eq. (8), the approximation holds for somesolution of eq. (7); it is not known beforehand
which solution it would be. Therefore, we would need to choose Tlarge enough such that allsolutions of
eq. (7) have approximately converged in direction. However, this may not be possible for all initializations
w0. To illustrate this we consider a simple example.
Consider the function f(u1,u2) =u1|u2|that satisﬁes Assumption 2, and is diﬀerentiable everywhere except
along the line u2= 0. In Figure 2 we plot its gradient ﬁeld. Note that ˜u= [1,0]/latticetopis a critical point for
f(u1,u2), i.e., 0∈∂f(˜u). Thus, if initialized at ˜u, one possible gradient ﬂow solution is to stay at ˜ufor all
t≥0. However, ∂f(˜u)contains other vectors which could lead to gradient ﬂow escaping from ˜u. Moreover,
one could construct a gradient ﬂow solution that can spend arbitrary amount of time at ˜ubefore escaping
it. Speciﬁcally, for any ﬁnite T,
uT(t) =

/bracketleftbigg1
0/bracketrightbigg
,for allt∈[0,T]
/bracketleftbiggcosh (t−T)
sinh (t−T)/bracketrightbigg
,for allt≥T,
is a possible gradient ﬂow solution. We note that limt→∞uT(t)//bardbluT(t)/bardbl2= [1/√
2,1/√
2]/latticetop, however, clearly
for any ﬁnite time Twe can choose Tlarge such that uT(T)//bardbluT(T)/bardbl2stays away from [1/√
2,1/√
2]/latticetop.
8Published in Transactions on Machine Learning Research (06/2024)
Thus, we can not establish ﬁnite time approximate directional convergence for all possible gradient ﬂow
solutions. Complete details for this example can be found in Appendix E.
To address this issue, we only consider initialization which leads to a unique solution. In particular, we
assume w0to be a non-branching initialization of eq. (5). As noted earlier, if the neural network has locally
Lipschitz gradients, then this requirement is always satisﬁed. However, for more general networks such as
two-layer ReLU neural networks the above assumption may appear somewhat restrictive. That said, it is
worth noting that a similar assumption was also made in Lyu et al. (2021), where the full training dynamics
of two-layer Leaky-ReLU neural networks were investigated in a simple setting involving linearly separable
data. Furthermore, Maennel et al. (2018) addresses this challenge of non-uniqueness by asserting that the
diﬀerential inclusion resulting from the gradient ﬂow of the loss function will have a unique solution in
“almost all” cases, but do not provide a formal proof. We leave it as an important future research direction
to handle more general initializations.
Another potential research direction is to characterize the set of non-branching initializations. For example,
in the function depicted in Figure 2, it can be shown that except for the set {u2= 0,u1>0}, all the other
points are non-branching, which implies that almost all points are non-branching; for details see Lemma E.2.
Whether a similar behavior happen for a broader class of datasets and neural networks is an interesting
future direction, and a positive answer would justify the non-branching assumption in those cases. However,
establishing such a result, even for a two-layer (Leaky) ReLU neural networks, seems to be diﬃcult. In the
following lemma, we show that for two-layer Leaky-ReLU neural networks there exists sets, near certain KKT
points, withinwhichallpointsarenon-branching. Wehopeitcanhelpfutureworksinbettercharacterization
of the set of non-branching initializations.
Lemma 5.4. LetH(x;v,u) =vσ(x/latticetopu), whereσ(x) = max(x,αx ), for someα∈R. Suppose{v∗,u∗}is a
KKT point of
max
v2+/bardblu/bardbl2
2=1Nz,H(v,u) =vz/latticetopσ(X/latticetopu),
wherev∗z/latticetopσ(X/latticetopu∗)>0andmini∈[n]|x/latticetop
iu∗|>0. Then there exists a γ > 0such that every element of
the setS={v,u:sign(v∗)v >/bardblu/bardbl2,/bardblu−u∗/bardbl2≤γ}is a non-branching initialization of the diﬀerential
inclusion /bracketleftbigg˙v
˙u/bracketrightbigg
∈/bracketleftbigg∂vNz,H(v,u)
∂uNz,H(v,u)/bracketrightbigg
.
The proof can be found in Appendix F. Note that in the above lemma mini∈[n]|x/latticetop
iu∗|>0implies that the
hyperplane deﬁned by u∗does not pass through any vector in the training set. Also, although we have only
considered a single neuron, the above lemma can be extended to multi-neuron setting by ensuring that the
initialization of each neuron is in a set similar to S.
5.1.2 A Corollary for Separable Neural Networks
We next consider the case when H(x;w)is separable and can be divided into smaller neural networks. In the
following lemma, we describe the directional convergence for such neural networks near small initialization.
Corollary 5.4.1. Suppose we can write w= [w1,...,wH]/latticetopsuch thatH(x;w) =/summationtextH
i=1Hi(x;wi), for all x.
Consider the same setting as in Theorem 5.1, then, for any /epsilon1∈(0,η), whereηis a positive constant, there
existC > 1andδ>0such that for any δ∈(0,δ), and for all i∈[H], either
√
Cδ≥/bardblwi(T)/bardbl2≥δη,andwi(T)/latticetopˆui
/bardblwi(T)/bardbl2≥1−/parenleftbigg
1 +3
2η/parenrightbigg
/epsilon1,
where ˆuiis a non-negative KKT point of
max
/bardblu/bardbl2
2=1N−/lscript/prime(0,y),Hi(u) =−/lscript/prime(0,y)/latticetopHi(X;u), (12)
or
/bardblwi(T)/bardbl2≤2δ/epsilon1,whereT=ln(C)
4β˜β.
9Published in Transactions on Machine Learning Research (06/2024)
Figure 3: The lower part shows the content of Figure 1b with the horizontal and vertical axes interchanged.
The top plot shows the constrained NCF Ny,H(θ) =/summationtextn
i=1yimax(0,[cos(θ),sin(θ)]/latticetopxi)2. As predicted by
Corollary 5.4.1, the neuron weights converge in direction to the KKT points of the NCF.
The above result establishes that for separable neural networks, the weights of smaller neural networks ap-
proximately converge in direction to the KKT points of the optimization problem in eq. (12), the constrained
NCF deﬁned with respect to the output of smaller neural networks.
Indeed, thisispreciselywhatweobservedforthe“toy”experimentsdepictedinFigure1. Recall, inthatcase,
the neural network was the sum of squared ReLU functions and hence satisﬁes the setting of Corollary 5.4.1.
In the bottom of Figure 3, we again plot the evolution of the direction of the weights for each hidden neuron.
On the top, we plot the constrained NCF with respect to the output of each neuron, which will be identical
for each neuron. We clearly observe that the weights of each neuron converge in direction towards KKT
points of the constrained NCF.
Finally, we would like to emphasize that while in Theorem 5.1 the directional convergence is established
for all the weights, Corollary 5.4.1 further establishes directional convergence for weights of smaller neural
networks as well. In fact, the results in previous works on 2-layer ReLU neural network (Maennel et al.,
2018), which is separable, are along the lines of Corollary 5.4.1 and it is possible to get those results using
Corollary 5.4.1. We provide these details in Appendix H.
5.2 Directional Convergence Near Saddle Points
Several theoretical and empirical works have observed a saddle-to-saddle dynamics during training of neural
networks with small initialization and square loss (Jacot et al., 2022; Pesme & Flammarion, 2023; Boursier
et al., 2022; Jin et al., 2023). The evolution of loss alternates between being stagnant and decreasing
sharply, almost like a piecewise constant function. This indicates that weights move from one saddle of the
loss function to another during training. Some theoretical works further show that at each saddle point only
certain number of weights are non-zero. For example, Pesme & Flammarion (2023) proves saddle-to-saddle
dynamics in two-homogeneous diagonal linear networks, where at each saddle points only few weights are
non-zero. The authors of Boursier et al. (2022) study two-layer ReLU network with orthogonal inputs, and
show that gradient ﬂow enters neighborhood of a saddle point where one set of neurons have high norm
while others have zero norm.
In this section, we show that the directional convergence near initialization described in the previous section
also occurs near certain saddle points for square loss. However, there could be diﬀerent kinds of saddle
points throughout the loss landscape. The choice of saddle points considered here is motivated by the above
observations, such as only a certain number of weights being non-zero at the saddle points.
We assume that the weights of neural network wcan be divided into two sets, w= [wn,wz], such that
H(x;w) =Hn(x;wn) +Hz(x;wz), whereHn(x;wn)andHz(x;wz)each satisfy Assumption 2. For square
10Published in Transactions on Machine Learning Research (06/2024)
loss, we minimize
L(wn,wz) =1
2n/summationdisplay
i=1/bardblHn(xi;wn) +Hz(xi;wz)−yi/bardbl2=1
2/bardblHn(X;wn) +Hz(X;wz)−y/bardbl2.(13)
The saddle point of eq. (13) that we consider here satisﬁes the following.
Assumption 3. We assume{wn,wz}is a saddle point of eq. (13)such that
/bardblwn/bardbl2∈[m,M ],and/bardblwz/bardbl2= 0, (14)
wherem,Mare positive constants. Further, if Hn(x;wn)does not have a locally Lipschitz gradient, then we
assume that there exists γ,κ> 0such that for all wnsatisfying/bardblwn−wn/bardbl2≤γit holds that
/angbracketleftwn−wn,s/angbracketright≥−κ/bardblwn−wn/bardbl2
2,where s∈−∂wnL(wn,0). (15)
In the above assumption, wnis the set of weights with high norm while wzcontains sets of weights with
zero norm. Due to homogeneity, Hz(x;wz) =0, and thusHn(x;wn)is eﬀectively the output of the neural
network at{wn,wz}.
WhenHn(x;wn)does not have locally Lipschitz gradient, we require eq. (15) to ensure that if wninitialized
nearwn, then it stays near it for a suﬃciently long time. We discuss the motivation for this inequality after
discussing our main theorem of this section, stated below.
Theorem 5.5. Let{wn,wz}satisfy Assumption 3, and deﬁne y=y−Hn(X;wn). Supposeζzis a unit
norm vector and a non-branching initialization of the diﬀerential inclusion
˙u∈∂Ny,Hz(u),u(0) =ζz. (16)
For any/epsilon1∈(0,η), whereηis a positive constant3, there exist C > 1andδ>0such that the following holds:
for anyδ∈(0,δ)and gradient ﬂow solution {wn(t),wz(t)}of eq.(13)that satisﬁes for a.e. t≥0
/bracketleftbigg˙wn
˙wz/bracketrightbigg
∈−/bracketleftbigg∂wnL(wn,wz)
∂wzL(wn,wz)/bracketrightbigg
,/bracketleftbiggwn(0)
wz(0)/bracketrightbigg
=/bracketleftbiggwn+δζn
wz+δζz/bracketrightbigg
, (17)
whereζnis a unit norm vector, we have
/bardblwn(t)−wn/bardbl2
2+/bardblwz(t)−wz/bardbl2
2≤Cδ2,for allt∈/bracketleftbig
0,T/bracketrightbig
, (18)
whereT=1
M2ln (C), andM2is a constan4. Further, either
/bardblwz(T)/bardbl2≥δη,andwz(T)/latticetopˆu
/bardblwz(T)/bardbl2≥1−/parenleftbigg
1 +3
2η/parenrightbigg
/epsilon1,
where ˆuis a non-negative KKT point of
max
/bardblu/bardbl2
2=1Ny,Hz(u) =y/latticetopHz(X;u),
or
/bardblwz(T)/bardbl2≤2δ/epsilon1.
In the above theorem, the initialization is near a saddle point which satisﬁes Assumption 3, and δcontrols
how far the initialization is from the saddle point. The vector yrepresents the residual error at the saddle
point and plays the same role as −/lscript/prime(0,y)did in Theorem 5.1. Provided ζzis a non-branching initialization
3Here,ηdepends on the solution of eq. (16), which solely relies on X,¯y,Hz,ζz, and is independent of δ. See the proof for
more details.
4M2depends on β,/bardbly/bardbl2and various parameters associated with HzandHnnear{wn,wz}such as their Lipschitz constant,
maximum value etc. Importantly, it does not depend on /epsilon1andδ.
11Published in Transactions on Machine Learning Research (06/2024)
of eq. (16), we show that for suﬃciently small δ, the gradient ﬂow spends enough time near the saddle point
such that weights of small magnitude wzeither approximately converge in direction to the KKT point of the
NCF deﬁned with respect to yandHz, or gets close to 0. The above theorem, similar to Theorem 5.1, shows
directional convergence among weights of small magnitude. The proof technique is similar to the proof of
Theorem 5.1; for details see Appendix D. We also demonstrate the phenomenon of directional convergence
near saddle points using a numerical experiment in Appendix D.
We next explain the motivation for eq. (15) in Assumption 3 when Hn(x;wn)does not have locally Lipschitz
gradients. Our proof of the above theorem crucially relies on showing that wn(t)remains close to wnand
wz(t)remains small for a suﬃciently long time; i.e., eq. (18) holds. Suppose wz(t)remains small, then the
evolution of wn(t)is approximately governed by the gradient ﬂow of L(wn,0). To understand the gradient
ﬂow dynamics of L(wn,0)nearwn, we use the following lemma.
Lemma 5.6. Suppose{wn,wz}is a saddle point of eq. (13)such that/bardblwn/bardbl2∈[m,M ],and/bardblwz/bardbl2= 0.
Then,
0∈∂wnL(wn,0).
IfHn(x;wn)has locally Lipschitz gradients, then ∇wnL(wn,0)would be small and vary smoothly in the
neighborhood of wn. This suﬃces to ensure that if wn(0)is close to wn, then wn(t)remains close to wn
for a suﬃciently long time.
However, ifHn(x;wn)does not have locally Lipschitz gradients, then 0∈∂wnL(wn,0)but∂wnL(wn,0)
may be large near wn. This prevents us from ensuring wn(t)remains near wn. For example consider
g(u) = (u1|u2|−1)2and let ˜u= [1,0]T. Then, 0∈∂g(˜u). Let uδ= [1 +δ,δ]T. Then, for any δ∈(0,0.1)
ands∈−∂g(uδ),/bardbls/bardbl2≥1, and no matter how close uδis to ˜u, if initialized at uδ, the gradient ﬂow will
quickly get away from ˜u(see the Appendix I for details).
Hence, we require eq. (15) which implies that ∂wnL(wn,0)varies smoothly along wn−wn. This is suﬃcient
for us to ensure wn(t)remains near wn. In fact, ifHn(x;wn)have a locally Lipschitz gradient, then eq. (15)
is automatically satisﬁed, since
/angbracketleftwn−wn,s/angbracketright≥−/bardbl wn−wn/bardbl2/bardbl∇wnL(wn,0)/bardbl2
=−/bardblwn−wn/bardbl2/bardbl∇wnL(wn,0)−∇wnL(wn,0)/bardbl2≥−κ/bardblwn−wn/bardbl2
2,
where the equality holds since ∇wnL(wn,0) = 0from Lemma 5.6. The last inequality follows since Hn
and the loss function have locally Lipschitz gradient. We also note that if the gradient of Hn(x;wn)is not
locally Lipschitz globally, but is locally Lipschitz in the neighborhood of wn, then eq. (15) is also satisﬁed.
For example again consider g(u) = (u1|u2|−1)2and let ˜u= [1,1]T. Then, it is easy to show that 0∈∂g(˜u),
andg(u)has locally Lipschitz gradient around ˜u.
5.3 Higher Orders of Homogeneity
Given the results of Theorem 5.1 and Theorem 5.5, it is natural to ask whether neural networks with higher
orders of homogeneity also exhibit similar behavior. Presently, we are unsure if such a behavior is possible.
The main diﬃculty is due to the relative scaling between the weights and the gradient at small initialization.
From our discussion in Section 5.1.1, we know that near small initialization, the evolution of w(t)can
approximately be expressed as
˙w≈−n/summationdisplay
i=1∇ˆy/lscript(0,yi)∂H(xi;w).
Now, from Lemma B.1, we know that if H(x;w)isL-homogeneous, then ∂H(x;w)is(L−1)-homogeneous.
Thus, if/bardblw(0)/bardbl2scales asδ, then/bardbl∂H(x;w(0))/bardbl2is expected to scale as δL−1. ForL= 2,/bardblw(0)/bardbl2and
/bardbl∂H(x;w(0))/bardbl2both scale as δ.Hence, for small δ, in the initial stages of training, the gradient ﬂow will
not change the norm of the weights signiﬁcantly, but can have signiﬁcant impact on its direction. However,
forL>2and smallδ, the gradient is smaller than the weights, where the relative scaling between the two
12Published in Transactions on Machine Learning Research (06/2024)
further worsens upon increasing L. Thus, it seems that, in the initials stages of training, the gradient will
not have much impact on the direction and magnitude of the weights. Nonetheless, it is possible that in the
slightly later stages of training the gradient can be large enough to change the direction of weights. However,
it is unclear to us how to rigorously analyze such scenarios and is therefore a subject of future research.
6 Conclusions and Future Directions
In this work, we studied the gradient ﬂow dynamics of two-homogeneous neural networks near small ini-
tializations and saddle points, and showed the approximate directional convergence of their weights in the
initial stages of training.
An important future direction is, of course, to study the entire gradient ﬂow dynamics of neural networks,
and our work could be an important step towards a comprehensive understanding of training dynamics of
neural networks. Particularly, for successful training under small initialization, the gradient dynamics will
have to eventually escape the origin. The escape direction may be determined by the directions to which
the weights converge while the dynamics is near the origin. This also holds true while escaping other saddle
points encountered by gradient dynamics during the training process, which notably is known to undergo
saddle-to-saddle dynamics.
Another possible future direction is to investigate similar directional convergence in deeper neural networks.
Fornon-smoothneuralnetworks, werequiredanadditionalassumptionontheinitializationtoensureunique-
ness. It would be interesting to analyze scenarios when such assumptions do not hold. We defer that to a
future investigation.
Acknowledgments
The authors graciously acknowledge gift funding from InterDigital, which partially supported this work.
References
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization.
InAdvances in Neural Information Processing Systems , 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact
computation with an inﬁnitely wide neural net. In Advances in Neural Information Processing Systems ,
2019b.
Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners: The silent
alignment eﬀect. In International Conference on Learning Representations , 2022.
J. M. Borwein and A. S. Lewis. Convex Analysis and Nonlinear Optimization . Springer Verlag, Berlin,
Heidelberg, New York, 2000.
Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion. Gradient ﬂow dynamics of shallow reLU
networks for square loss and orthogonal inputs. In Advances in Neural Information Processing Systems ,
2022.
Alon Brutzkus and Amir Globerson. Why do larger models generalize better? A theoretical perspective via
the XOR problem. In Proceedings of the 36th International Conference on Machine Learning , 2019.
Zhengan Chen, Yuqing Li, Tao Luo, Zhangchen Zhou, and Zhi-Qin John Xu. Phase diagram of initial
condensation for two-layer neural networks, 2023.
Lénaïc Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. In Advances in Neural Information Processing Systems , volume 31.
Curran Associates, Inc., 2018.
13Published in Transactions on Machine Learning Research (06/2024)
Lénaïc Chizat, Edouard Oyallon, and Francis Bach. On lazy training in diﬀerentiable programming. In
Advances in Neural Information Processing Systems , 2019.
F. H. Clarke, Yu. S. Ledyaev, R. J. Stern, and P. R. Wolenski. Nonsmooth Analysis and Control Theory .
Springer-Verlag, Berlin, Heidelberg, 1998. ISBN 0387983368.
F.H. Clarke. Optimization and Nonsmooth Analysis . Wiley New York, 1983.
M. Coste. An Introduction to O-minimal Geometry . Dottorato di ricerca in matematica / Università di Pisa,
Dipartimento di Matematica. Istituti editoriali e poligraﬁci internazionali, 2000. ISBN 9788881472260.
Damek Davis, Dmitriy Drusvyatskiy, Sham M. Kakade, and J. Lee. Stochastic subgradient method converges
on tame functions. Foundations of Computational Mathematics , 20:119–154, 2018.
Aleksej F. Filippov. Diﬀerential equations with discontinuous righthand sides. In Mathematics and Its
Applications , 1988.
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy training
in deep neural networks. Journal of Statistical Mechanics: Theory and Experiment , 2020(11):113301, nov
2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. In Advances in Neural Information Processing Systems , 2018.
ArthurJacot, FrançoisGed, BerﬁnŞimşek, ClémentHongler, andFranckGabriel. Saddle-to-saddledynamics
in deep linear networks: Small initialization training, symmetry, and sparsity, 2022.
Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In Advances in
Neural Information Processing Systems , volume 33, 2020.
JikaiJin, ZhiyuanLi, KaifengLyu, SimonShaoleiDu, andJasonD.Lee. Understandingincrementallearning
of gradient descent: A ﬁne-grained analysis of matrix sensing. In Proceedings of the 40th International
Conference on Machine Learning , 2023.
Tao Luo, Zhi-Qin John Xu, Zheng Ma, and Yaoyu Zhang. Phase diagram for two-layer relu neural networks
at inﬁnite-width limit. Journal of Machine Learning Research , 22(71):1–47, 2021.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In
International Conference on Learning Representations , 2020.
Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on two-layer nets: Margin
maximization and simplicity bias. In Advances in Neural Information Processing Systems , volume 34,
2021.
Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient descent quantizes relu network features,
2018.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-ﬁeld theory of two-layers neural networks:
dimension-free bounds and kernel limit. In Proceedings of the Thirty-Second Conference on Learning
Theory, pp. 2388–2464, 2019.
Hancheng Min, Enrique Mallada, and Rene Vidal. Early neuron alignment in two-layer reLU networks with
small initialization. In The Twelfth International Conference on Learning Representations , 2024.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role
of implicit regularization in deep learning. In ICLR (Workshop) , 2015.
Scott Pesme and Nicolas Flammarion. Saddle-to-saddle dynamics in diagonal linear networks. In Thirty-
seventh Conference on Neural Information Processing Systems , 2023.
14Published in Transactions on Machine Learning Research (06/2024)
ItaySafran, GalVardi, andJasonD.Lee. OntheeﬀectivenumberoflinearregionsinshallowunivariatereLU
networks: Convergence guarantees and implicit bias. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022.
Daniel Soudry, Elad Hoﬀer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias
of gradient descent on separable data. J. Mach. Learn. Res. , 19(1):2822–2878, January 2018.
MingzeWangandChaoMa. Earlystageconvergenceandglobalconvergenceoftrainingmildlyparameterized
neural networks. In Advances in Neural Information Processing Systems , volume 35, pp. 743–756, 2022.
Mingze Wang and Chao Ma. Understanding multi-phase optimization dynamics and rich nonlinear behaviors
of reLU networks. In Thirty-seventh Conference on Neural Information Processing Systems , 2023.
A. J. Wilkie. Model completeness results for expansions of the ordered ﬁeld of real numbers by restricted
pfaﬃan functions and the exponential function. Journal of the American Mathematical Society , 9(4):
1051–1094, 1996. ISSN 08940347, 10886834. URL http://www.jstor.org/stable/2152916 .
Francis Williams, Matthew Trager, Daniele Panozzo, Claudio Silva, Denis Zorin, and Joan Bruna. Gradient
dynamics of shallow univariate relu networks. In Advances in Neural Information Processing Systems ,
volume 32, 2019.
Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel
Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Proceedings of Thirty
Third Conference on Learning Theory , pp. 3635–3673, 2020.
Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in inﬁnite-width neural networks. In
Proceedings of the 38th International Conference on Machine Learning , pp. 11727–11737, 2021.
Appendices
A Key Properties of o-minimal Structures and Clarke Subdiﬀerentials
In this section we give a brief overview of o-minimal structures, and the relevant properties of Clarke
subdiﬀerentials that are used in the proofs of our results. We borrow much of the discussion below from Ji
& Telgarsky (2020); Davis et al. (2018)
An o-minimal structure is a collection S={Sn}∞
n=1, where eachSnis set of subsets of Rn, that satisﬁes
following axioms:
•The elements ofS1are ﬁnite unions of points and intervals.
•All algebraic subsets of Rnare inSn.
•For alln,Snis a Boolean subalgebra of the power set of Rn.
•IfA∈Sn,B∈Sm, then A×B∈Sn+m
•IfP:Rn+1→Rnis the projection on ﬁrst ncoordinates and A∈Sn+1, thenP(A)∈Sn.
For a given o-minimal structure S, a set A⊂Rnis deﬁnable if A∈Sn. A function f:D→Rmwith
D⊂Rnis deﬁnable if the graph of fis inSn+m. Since a set remains deﬁnable under projection, the domain
Dis also deﬁnable.
In Wilkie (1996), it was shown that there exists an o-minimal structure in which polynomials and exponential
functions are deﬁnable. Also, the deﬁnability of a function is stable under algebraic operations, composition,
15Published in Transactions on Machine Learning Research (06/2024)
inverse, maximum, and minimum. Since ReLU and Leaky-ReLU can each be expressed as maximums of two
polynomials, it can be shown that the functions we will consider in this paper are deﬁnable.
It is also true that deep neural networks with ReLU or Leaky-ReLU activation, and diﬀerent kinds of layers,
are deﬁnable. For completeness, we state that result here as a lemma.
Lemma A.1. (Ji & Telgarsky, 2020, Lemma B.2) Suppose there exist k,d0,d1,...,dL>0andLdeﬁnable
functions (g1,...,gL)wheregj:Rd0×...×Rdj−1×Rk→Rdj. Leth1(x,w) :=g1(x,w), and for 2≤j≤L,
hj(x,w) :=gj(x,h1(x,w),...,hj−1(x,w),w)
then allhjare deﬁnable. (It suﬃces if each output coordinate of gjis the minimum or maximum over
some ﬁnite set of polynomials, which allows for linear, convolutional, ReLU, max-pooling layers and skip
connections.)
We also note that, since deﬁnability is stable under composition, the objective functions arising for deﬁnable
losses are also deﬁnable.
A.1 Chain Rules for Non-diﬀerentiable Functions
Recall that for any locally Lipschitz continuous function f:X→R, its Clarke subdiﬀerential at a point
x∈Xis the set
∂f(x) =conv/braceleftBig
lim
i→∞∇f(xi) : lim
i→∞xi=x,xi∈Ω/bracerightBig
,
where Ωis any full-measure subset of Xsuch thatfis diﬀerentiable at each of its points, and ∂f(x)denotes
the unique minimum norm subgradient.
The functions considered in this paper can be compositions of non-diﬀerentiable functions. We use Clarke’s
chain rule of diﬀerentiation, described in the following lemma, to compute the Clarke subdiﬀerentials in such
cases.
Lemma A.2. (Clarke, 1983, Theorem 2.3.9 ) Let h1,...,hn:Rd→Randg:Rn→Rbe locally Lipschitz
functions, and f(x) =g(h1(x),...,hn(x)), then,
∂f(x)⊆conv/braceleftBiggn/summationdisplay
i=1αiζi:ζi∈∂hi(x),α∈∂g(h1(x),...,hn(x))/bracerightBigg
.
The chain rule for gradient ﬂow described in the next lemma, which is crucial for our analysis, essentially
implies that for diﬀerential inclusions ∂f(x)plays the same role as ∇f(x)does for diﬀerential equations.
Lemma A.3. (Davis et al., 2018, Lemma 5.2)(Ji & Telgarsky, 2020, Lemma B.9) Given a locally Lipschitz
deﬁnable function f:D→Rwith an open domain D, for any interval Iand any arc x:I→D, it holds
for a.e.t∈Ithat
d(f(x(t)))
dt=/angbracketleftx∗(t),˙x(t)/angbracketright,for all x∗(t)∈∂f(x(t)).
Further, if x:I→Dsatisﬁes
˙x∈∂f(x),for a.e.t≥0,
then, it holds for a.e. t≥0that
˙x(t) =∂f(x(t)),andd(f(x(t)))
dt=/bardbl∂f(x(t))/bardbl2
2,
and therefore,
f(x(t))−f(x(0)) =/integraldisplayt
0/bardbl∂f(x(s))/bardbl2
2ds,∀t≥0.
16Published in Transactions on Machine Learning Research (06/2024)
A.2 The Kurdyka-Lojasiewicz Inequality
For gradient ﬂow trajectories that are bounded, the Kurdyka-Lojasiewicz Inequality is useful for showing
convergence, essentially by establishing the existence of a desingularizing function, which is formally deﬁned
as follows.
Deﬁnition A.1. A function Ψ : [0,ν)→Ris called a desingularizing function when Ψis continuous on
[0,ν)with Ψ(0) = 0 , and it is continuously diﬀerentiable on (0,ν)with Ψ/prime>0.
The following lemma, which can be seen as an unbounded version of the Kurdyka-Lojasiewicz Inequality,
plays an important role in establishing the directional convergence of gradient ﬂow trajectories of the neural
correlation function where the trajectories can be unbounded.
Lemma A.4. (Ji & Telgarsky, 2020, Lemma 3.6) Given a locally Lipschitz deﬁnable function fwith an
open domain D⊂{x|/bardblx/bardbl2>1}, for anyc,η > 0, there exists a ν > 0and a deﬁnable desingularizing
function Ψon[0,ν)such that
Ψ/prime(f(x))/bardblx/bardbl2/bardbl∂f(x)/bardbl2≥1,iff(x)∈(0,ν)and/bardbl∂⊥f(x)/bardbl2≥c/bardblx/bardblη
2/bardbl∂rf(x)/bardbl2,
where∂rf(x) =/angbracketleftBig
∂f(x),x
/bardblx/bardbl2/angbracketrightBig
x
/bardblx/bardbl2and∂⊥f(x) =∂f(x)−∂rf(x).
B Additional Notation and Some Preliminary Lemmata
For notational convenience when dealing with Clarke subdiﬀerentials, we introduce the following notation
for sets containing vectors.
•∀A,B⊆Rd,A±B:={x±y:x∈A,y∈B}
•∀B⊆Rd,andc∈R,cB:={cy:y∈B}
•∀B⊆Rd,andp∈Rd,/angbracketleftp,B/angbracketright:={p/latticetopy:y∈B}⊆R
•For any norm/bardbl·/bardblonRd,/bardblB/bardbl:={/bardbly/bardbl,y∈B}⊆R
The following lemma states two important properties of homogeneous functions.
Lemma B.1. ((Lyu & Li, 2020, Theorem B.2), (Ji & Telgarsky, 2020, Lemma C.1)) Let F:Rk→Rbe a
locally Lipschitz and L−positively homogeneous for some L>0, then:
1. For any w∈Rkandc≥0,
∂F(cw) =cL−1∂F(w).
2. For any w∈Rk,
w/latticetops=LF(w),for all s∈∂F(w).
This result gives rise to the following corollary, which we use frequently in our analysis.
Corollary B.1.1. For any w∈Rkandc≥0,
w/latticetops= 2H(x;w),for all s∈∂H(x;w),
and
∂H(x;cw) =c∂H(x;w).
C Proofs Omitted from Section 5.1
In this section we ﬁrst prove Lemma 5.2 and Lemma 5.3, and then use them to ultimately prove Theorem 5.1.
17Published in Transactions on Machine Learning Research (06/2024)
C.1 Proof of Lemma 5.2
To prove Lemma 5.2, we make use of the following lemma. It shows that if two diﬀerential inclusions
are initialized at the same point, and the diﬀerence between them is small in a bounded interval, then the
diﬀerence between their solutions is also small. This is a well-known stability result (Filippov, 1988, Theorem
1, Section 8); we provide a proof in Appendix J for completeness.
Lemma C.1. Consider the following diﬀerential inclusions for t∈[0,T]:
d˜u
dt∈n/summationdisplay
i=1zi∂H(xi;˜u),˜u(0) = u0, (19)
and
du
dt∈n/summationdisplay
i=1(zi+fi(t))∂H(xi;u),u(0) = u0, (20)
whereTis ﬁnite, and|fi(t)|2≤δfor alli∈[n]andt∈[0,T]. Then, for any /epsilon1 >0there exists a δ >0,
such that for each solution u(t)of eq.(20)there exists a solution ˜u(t)of eq.(19)satisfying
max
t∈[0,T]/bardblu(t)−˜u(t)/bardbl2≤/epsilon1. (21)
Proof of Lemma 5.2. Using the chain rule from Lemma A.2, the gradient ﬂow dynamics are
˙w∈−n/summationdisplay
i=1∇ˆy/lscript(H(xi;w),yi)∂H(xi;w),w(0) =δw0. (22)
Now, we deﬁne z(t) =/bardblw(t)/bardbl2
2and note that z(0) =δ2≤1/C < 1. Since w(t)is a continuous function, so
isz(t). Hence, there exists some γ >0, such that for all t∈(0,γ),z(t)<1. We deﬁne ˆTto be the smallest
t>0such thatz(ˆT) = 1. It follows that for all t∈[0,ˆT],z(t)≤1.
Now, ifz(t)≤1and sinceβ:= sup{/bardblH(X;w)/bardbl2:w∈Sk−1}, then/bardblH(X;w(t))/bardbl2≤β/bardblw(t)/bardbl2
2≤β, for all
t∈[0,ˆT]. From the deﬁnition of ˆβin eq. (4), we get
/bardbl/lscript/prime(H(X;w(t)),y)/bardbl2≤ˆβ/bardblH(X;w(t))/bardbl2+/bardbl/lscript/prime(0,y)/bardbl2≤ˆββ+/bardbl/lscript/prime(0,y)/bardbl2=˜β.
From the above equation and using Corollary B.1.1, we have
˙z= 2w/latticetop˙w=−4n/summationdisplay
i=1∇ˆy/lscript(H(xi;w),yi)H(xi;w) =−4H(X;w)/latticetop/lscript/prime(H(X;w(t))≤4β/bardblw/bardbl2
2˜β= 4β˜βz,(23)
and soz(t)≤δ2e4β˜βt, which implies
ˆT≥1
4β˜βln/parenleftbigg1
δ2/parenrightbigg
.
Further, since δ≤1√
C, we have that1
4β˜βln (C)≤ˆTimplies
z(t)≤Cδ2,∀t∈/bracketleftbigg
0,ln (C)
4β˜β/bracketrightbigg
. (24)
Now, we consider t∈/bracketleftBig
0,ln(C)
4β˜β/bracketrightBig
. Note that
/bardblH(X;w(t))/bardbl2≤β/bardblw(t)/bardbl2
2≤βCδ2. (25)
18Published in Transactions on Machine Learning Research (06/2024)
Deﬁneξ(t) :=/lscript/prime(H(X;w(t)),y)−/lscript/prime(0,y); then,
/bardblξ(t)/bardbl2=/bardbl/lscript/prime(H(X;w(t)),y)−/lscript/prime(0,y)/bardbl2≤ˆβ/bardblH(X;w(t)/bardbl2≤ˆββCδ2.
Next, the dynamics of w(t)can be written as
˙w∈−n/summationdisplay
i=1∇ˆy/lscript(H(xi;w),yi)∂H(xi;w) =n/summationdisplay
i=1(−∇ˆy/lscript(0,yi)−ξi(t))∂H(xi;w). (26)
Dividing eq. (26) by δ, from 1-homogeneity of ∂H(x;w)(corollary B.1.1) we have
˙w
δ∈1
δn/summationdisplay
i=1(−∇ˆy/lscript(0,yi)−ξi(t))∂H(xi;w) =n/summationdisplay
i=1(−∇ˆy/lscript(0,yi)−ξi(t))∂H(xi;w/δ). (27)
Now, consider the diﬀerential inclusion
d˜w
dt∈−n/summationdisplay
i=1∇ˆy/lscript(0,yi)∂H(xi;˜w),˜w(0) = w0. (28)
Using the fact that for all t∈/bracketleftBig
0,ln(C)
4β˜β/bracketrightBig
,/bardblξ(t)/bardbl2≤δ2ˆββC, and using Lemma C.1, we have that there exists
a small enough δsuch that for all δ≤δ,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜w(t)−w(t)
δ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/epsilon1,
where ˜w(t)is a solution of eq. (28).
C.2 Proof of Lemma 5.3
Recall thatNz,H(u) =z/latticetopH(X;u). Throughout this section, for the sake of brevity, we will use N(u)instead
ofNz,H(u). The gradient ﬂow u(t)satisﬁes, for a.e. t≥0,
du
dt∈∂N(u)⊆n/summationdisplay
i=1zi∂H(xi;u),u(0) = u0. (29)
We will ﬁrst prove some auxiliary lemmata. The ﬁrst follows simply from two-homogeneity of N(u)and
Lemma B.1.
Lemma C.2. For anys∈∂N(u),s/latticetopu= 2N(u). For anyc≥0,∂N(cu) =c∂N(u)
The next lemma states that if the initialization is non-zero, then gradient ﬂow stays away from the origin
for all ﬁnite time.
Lemma C.3. Suppose u(t)is a solution of eq. (29), where u0is a non-zero vector. Then, for all ﬁnite
t>0,/bardblu(t)/bardbl2>0.
Proof.Since/bardblu(0)/bardbl2>0, from continuity of u(t), there exists some γ > 0such that/bardblu(t)/bardbl2>0, for all
t∈(0,γ). For the sake of contradiction, suppose there exists some ﬁnite T > 0such that/bardblu(T)/bardbl2= 0for
the ﬁrst time. Then, for all t∈[0,T),/bardblu(t)/bardbl2>0. Since for a.e. t∈[0,T)
dlog(/bardblu/bardbl2
2)
dt=1
/bardblu/bardbl2
2d/bardblu/bardbl2
2
dt=4z/latticetopH(X;u)
/bardblu/bardbl2
2≥−4β/bardblz/bardbl2,
it follows that for all t∈(0,T),
/bardblu(t)/bardbl2
2≥/bardblu0/bardbl2
2e−4tβ/bardblz/bardbl2.
Takingt→T, we have/bardblu(T)/bardbl2
2≥/bardblu0/bardbl2
2e−4Tβ/bardblz/bardbl2>0which leads to a contradiction.
19Published in Transactions on Machine Learning Research (06/2024)
Lemma C.4. IfN(u(t0))≥0,for anyt0≥0, then,N(u(t))≥0and/bardblu(t)/bardbl2≥/bardblu(t0)/bardbl2, for allt≥t0.
Proof.Since, using Lemma A.3,
N(u(t))−N(u(t0)) =/integraldisplayt
t0/bardbl˙u(s)/bardbl2
2ds,
wehavethatfor t≥t0,N(u(t))≥N(u(t0))≥0. Thesecondclaimistruesincefora.e. t≥0,d/bardblu/bardbl2
2
dt= 4N(u)
implies
/bardblu(t)/bardbl2
2−/bardblu(t0)/bardbl2
2= 4/integraldisplayt
t0N(u(s))ds≥0.
The following lemma states the conditions for ﬁrst-order KKT point of the constrained NCF.
Lemma C.5. If a vector u∗∈Rk×1is a ﬁrst-order KKT point of
max
/bardblu/bardbl2
2=1N(u) =z/latticetopH(X;u), (30)
thenn/summationdisplay
i=1zi∂H(xi;u∗) =λ∗u∗,/bardblu∗/bardbl2
2= 1, (31)
whereλ∗∈Ris the Lagrange multiplier. Also, 2N(u∗) =λ∗and hence, for a non-negative KKT point
λ∗≥0.
Proof.The Lagrangian is equal to
L(u,λ) =N(u) +λ(/bardblu/bardbl2
2−1).
Ifu∗is a ﬁrst-order KKT point then it must satisfy the constraint set and, for some λ,
0∈∂N(u∗) + 2λu∗,
implying
0∈n/summationdisplay
i=1zi∂H(xi;u∗) + 2λu∗.
Choosingλ∗=−2λwe get eq. (31). By Lemma C.2,
λ∗=λ∗/bardblu∗/bardbl2
2=u∗/latticetop∂N(u∗) = 2N(u∗)
In the following lemma we deﬁne ˜N(u), which is central to our proof, and its minimum norm Clarke
subdiﬀerential.
Lemma C.6. For any nonzero u∈Rkwe deﬁne ˜N(u) =N(u)//bardblu/bardbl2
2, then,
∂˜N(u) =/braceleftbiggs
/bardblu/bardbl2
2−2N(u)u
/bardblu/bardbl4
2/vextendsingle/vextendsingle/vextendsingle/vextendsingles∈∂N(u)/bracerightbigg
=/braceleftbigg/parenleftbigg
I−uu/latticetop
/bardblu/bardbl2
2/parenrightbiggs
/bardblu/bardbl2
2/vextendsingle/vextendsingle/vextendsingle/vextendsingles∈∂N(u)/bracerightbigg
,
and
∂˜N(u) =/parenleftbigg
I−uu/latticetop
/bardblu/bardbl2
2/parenrightbigg∂N(u)
/bardblu/bardbl2
2.
20Published in Transactions on Machine Learning Research (06/2024)
Proof.First, note that ˜N(u)is diﬀerentiable if and only if N(u)is diﬀerentiable. Therefore, for any non-zero
usuch thatN(u)is diﬀerentiable,
∇˜N(u) =∇N(u)
/bardblu/bardbl2
2−2N(u)u
/bardblu/bardbl4
2.
The ﬁrst claim follows from the deﬁnition of Clarke subdiﬀerential and Lemma C.2. For the second claim,
note that
/vextenddouble/vextenddouble/vextenddouble/vextenddoubles
/bardblu/bardbl2
2−2N(u)u
/bardblu/bardbl4
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2=/bardbls/bardbl2
2
/bardblu/bardbl4
2−4N(u)2
/bardblu/bardbl6
2+4N(u)2
/bardblu/bardbl8
2,
where we use s/latticetopu= 2N(u), for all s∈∂N(u). Hence, for the minimum norm subdiﬀerential of ˜N(u), we
must choose the minimum norm subdiﬀerential of N(u).
We now proceed to proving Lemma 5.3. We begin by showing that either u(t)converges to 0or
limt→∞u(t)//bardblu(t)/bardbl2exists. We consider two cases.
Case 1:N(u(0))>0.
In this case, we show that limt→∞u(t)//bardblu(t)/bardbl2exists using a similar technique as in Ji & Telgarsky (2020).
Speciﬁcally, we show that the length of the curve swept by u(t)//bardblu(t)/bardbl2, which is deﬁned as
/integraldisplay∞
0/vextenddouble/vextenddouble/vextenddouble/vextenddoubled
dt/parenleftbiggu
/bardblu/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2dt,
has ﬁnite length, and thus limt→∞u(t)//bardblu(t)/bardbl2exists.
We assumeN(u(0)) =γ >0,and thus/bardblu(0)/bardbl2>0. From Lemma C.4, for all t≥0,
N(u(t))≥N(u(0)) =γ,
implying
1
2d/bardblu/bardbl2
2
dt=u/latticetop˙u= 2N(u(t))≥2γ,for a.e.t≥0,
which in turn implies
/bardblu(t)/bardbl2
2≥/bardblu(0)/bardbl2
2+ 4γt,for allt≥0. (32)
Recall that ˜N(u) =N(u)//bardblu/bardbl2
2. Since/bardblu(t)/bardbl2>0,˜N(u(t))is deﬁned for all t≥0. Also, by Lemma A.3,
for a.e.t≥0,
dN(u)
dt=/bardbl˙u/bardbl2
2,and ˙u=∂N(u).
Therefore, using the chain rule from Lemma A.3, for a.e. t≥0we have
d˜N(u)
dt=˙u/latticetop/parenleftbigg
I−uuT
/bardblu/bardbl2
2/parenrightbigg∂N(u)
/bardblu/bardbl2
2=∂N(u)/latticetop
/bardblu/bardbl2
2/parenleftbigg
I−uuT
/bardblu/bardbl2
2/parenrightbigg
∂N(u)≥0, (33)
where in the second equality we used that ˙u=∂N(u), for a.e.t≥0. Hence, for all t2≥t1≥0,
˜N(u(t2))−˜N(u(t1)) =/integraldisplayt2
t1∂N(u)/latticetop
/bardblu/bardbl2
2/parenleftbigg
I−uuT
/bardblu/bardbl2
2/parenrightbigg/latticetop
∂N(u)dt≥0.
Therefore, ˜N(u(t)))is an increasing function, and hence, for any t≥0, that ˜N(u(t))≥˜N(u(0))implies
N(u(t))≥˜N(u(0))/bardblu(t)/bardbl2
2.
21Published in Transactions on Machine Learning Research (06/2024)
From the above inequality and eq. (32), we have limt→∞N(u(t)) =∞.
Now, sinceN(u)≤/bardblz/bardbl2/bardblH(X;u)/bardbl2≤β/bardblz/bardbl2/bardblu/bardbl2
2, we have that ˜N(u)is bounded. Hence, by monotone
convergence theorem, limt→∞˜N(u(t))exists; here, we suppose it is equal to f.
Note that, by the chain rule, for a.e. t≥0,
d
dt/parenleftbiggu
/bardblu/bardbl2/parenrightbigg
=/parenleftbigg
I−uuT
/bardblu/bardbl2
2/parenrightbigg˙u
/bardblu/bardbl2=/parenleftbigg
I−uuT
/bardblu/bardbl2
2/parenrightbigg∂N(u)
/bardblu/bardbl2
implies
/vextenddouble/vextenddouble/vextenddouble/vextenddoubled
dt/parenleftbiggu
/bardblu/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg
I−uuT
/bardblu/bardbl2
2/parenrightbigg
∂N(u)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
21
/bardblu/bardbl2. (34)
Suppose ˜N(u(t))converges to fin ﬁnite time, i.e., ˜N(u(T)) =ffor some ﬁnite T. Then, for a.e. t≥T,
d˜N(u)
dt= 0implies/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg
I−u(t)u(t)T
/bardblu(t)/bardbl2
2/parenrightbigg
∂N(u(t))/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2= 0.
Therefore, from eq. (34), we have
d
dt/parenleftbiggu
/bardblu/bardbl2/parenrightbigg
=0,for a.e.t≥T,
and hence, limt→∞u(t)
/bardblu(t)/bardbl2exists and is equal tou(T)
/bardblu(T)/bardbl2.
Thus, we may assume f−˜N(u(t))>0, for all ﬁnite t. Deﬁneg(u) =f−˜N(u). Then, since
/bardbl∂r˜N(u)/bardbl2= 0,
we have that
/bardbl∂⊥g(u)/bardbl2≥/bardblu/bardbl2/bardbl∂rg(u)/bardbl2= 0.
Hence, from Theorem A.4, there exists a ν > 0and a desingularizing function Ψ(·)deﬁned on [0,ν)such
that if/bardblu/bardbl2>1andg(u)<ν, then
1≤Ψ/prime(g(u))/bardblu/bardbl2/bardbl∂g(u)/bardbl2= Ψ/prime(f−˜N(u))/bardblu/bardbl2/bardbl∂˜N(u)/bardbl2. (35)
Since limt→∞˜N(u(t)) =f, and eq. (32) holds, we may choose Tlarge enough such that /bardblu(t)/bardbl2>1, and
g(u(t))<ν, for allt≥T. Hence, for a.e. t≥T,
d˜N(u)
dt=∂N(u)/latticetop
/bardblu/bardbl2
2/parenleftbigg
I−uuT
/bardblu/bardbl2
2/parenrightbigg/latticetop
∂N(u) =/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg
I−uuT
/bardblu/bardbl2
2/parenrightbigg∂N(u)
/bardblu/bardbl2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg
I−uuT
/bardblu/bardbl2
2/parenrightbigg∂N(u)
/bardblu/bardbl2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddoubled
dt/parenleftbiggu
/bardblu/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
=/bardblu/bardbl2/vextenddouble/vextenddouble∂˜N(u)/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddoubled
dt/parenleftbiggu
/bardblu/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≥1
Ψ/prime(f−˜N(u))/vextenddouble/vextenddouble/vextenddouble/vextenddoubled
dt/parenleftbiggu
/bardblu/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
implying
/vextenddouble/vextenddouble/vextenddouble/vextenddoubled
dt/parenleftbiggu
/bardblu/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤−dΨ(f−˜N(u))
dt.
22Published in Transactions on Machine Learning Research (06/2024)
In the chain of equalities and inequalities above, we used Lemma C.6 for the fourth equality, and for the
ﬁrst inequality we used eq. (35). Now, integrating both sides of the above from Tto anyt1≥T, we have
/integraldisplayt1
T/vextenddouble/vextenddouble/vextenddouble/vextenddoubled
dt/parenleftbiggu
/bardblu/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2dt≤Ψ(f−˜N(u(T))−Ψ(f−˜N(u(t1))≤Ψ(f−˜N(u(T)))<∞,
which implies
/integraldisplay∞
0/vextenddouble/vextenddouble/vextenddouble/vextenddoubled
dt/parenleftbiggu
/bardblu/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2dt=/integraldisplayT
0/vextenddouble/vextenddouble/vextenddouble/vextenddoubled
dt/parenleftbiggu
/bardblu/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2dt+/integraldisplay∞
T/vextenddouble/vextenddouble/vextenddouble/vextenddoubled
dt/parenleftbiggu
/bardblu/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2dt
≤/integraldisplayT
0/vextenddouble/vextenddouble/vextenddouble/vextenddoubled
dt/parenleftbiggu
/bardblu/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2dt+ Ψ(f−˜N(u(T)))<∞,
completing the proof.
Case 2:N(u(0))≤0.
In this case, we may further assume that N(u(t))≤0,for allt≥0, since if for some ﬁnite t,N(u(t))>0,
we can use the proof for Case 1 by choosing tas the starting time to prove the claim. Thus, we assume
N(u(t))≤0,for allt≥0. Now, since
1
2d/bardblu/bardbl2
2
dt=u/latticetop˙u= 2N(u(t))≤0,for a.e.t≥0,
it follows that/bardblu(t)/bardbl2decreases with time. Hence, limt→∞/bardblu(t)/bardbl2exists. If limt→∞/bardblu(t)/bardbl2= 0, then
limt→∞u(t) =0andlimt→∞N(u(t)) = 0and we are done.
Otherwise, assume that limt→∞/bardblu(t)/bardbl2=η > 0. In this case, since /bardblu(t)/bardbl2is a decreasing function,
/bardblu(t)/bardbl2≥η, for allt≥0. Since by Lemma A.3, for a.e. t≥0,
dN(u)
dt=/bardbl˙u/bardbl2
2,
we have thatN(u(t))increases with time. But, we also assume N(u(t))≤0,and so by the monotone
convergence theorem, N(u(t))converges. We further claim that limt→∞N(u(t)) = 0. Suppose for the sake
of contradiction limt→∞N(u(t)) =−γ <0. SinceN(u(t))increases with time, we have N(u(t))≤−γ,for
allt≥0. Hence,
1
2d/bardblu/bardbl2
2
dt=u/latticetop˙u= 2N(u(t))≤−2γ,for a.e.t≥0.
The above equation implies that /bardblu(t)/bardbl2will become less than ηwithin a ﬁnite time, which leads to a
contradiction, and therefore, limt→∞N(u(t)) = 0.
We next show that limt→∞u(t)//bardblu(t)/bardbl2exists. We can do this in same way in the proof of Case 1. Deﬁne
ˆu(t) = 2u(t)/η, and note that if ˆu(t)converges in direction, then u(t)also converges in direction. We make
this transformation because to use Lemma A.4 we require /bardblu(t)/bardbl2>1after some time T. While u(t)may
never exceed 1, we do have/bardblˆu(t)/bardbl2≥2>1, for allt≥0.
Next, ˜N(u(t))is deﬁned for all t≥0, since/bardblu(t)/bardbl2>0for allt≥0. Also, sinceN(u(t))converges to
0,˜N(u(t))also converges to 0. Also, ˜N(u(t)) = ˜N(ˆu(t)),thus ˜N(ˆu(t))converges to 0as well. From here
to prove directional convergence of ˆu(t)we can use the the same approach as in Case 1, speciﬁcally from
eq. (34) onward.
We next turn towards showing that if limt→∞u(t)
/bardblu(t)/bardbl2exists, then the limit must be a non-negative KKT
point of the constrained NCF. Suppose u∗is the limit. We have already shown that N(u∗) = ˜N(u∗)≥0.
Thus, we only need to prove that u∗is a KKT point, i.e., from eq. (31), it must satisfy
2N(u∗)u∗∈∂N(u∗), (36)
23Published in Transactions on Machine Learning Research (06/2024)
Assume for the sake of contradiction that there exists some γ >0such that for all s∈∂N(u∗), we have
/bardbls−2N(u∗)u∗/bardbl2≥γ. (37)
Deﬁne u/epsilon1={u:/bardblu−u∗/bardbl≤/epsilon1}. Givenγ, by upper semi-continuity of the Clarke subdiﬀerential, we may
choose/epsilon1∈(0,1)suﬃciently small such that for all u∈u/epsilon1, we have
∂N(u)⊆{p:p=q+r,q∈∂N(u∗),/bardblr/bardbl2≤γ/4}. (38)
Sinceu(t)
/bardblu(t)/bardbl2converges to u∗, andN(u)is continuous, we can choose Tlarge enough such that for all t≥T
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleu(t)
/bardblu(t)/bardbl2−u∗/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/epsilon1,and/vextenddouble/vextenddouble/vextenddouble/vextenddouble2u(t)
/bardblu(t)/bardbl2N/parenleftbiggu(t)
/bardblu(t)/bardbl2/parenrightbigg
−2u∗N(u∗)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤γ/4. (39)
Suppose s∈∂N(u∗), then, for all u∈Rk\{0}we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg
I−uu/latticetop
/bardblu/bardbl2
2/parenrightbigg∂N(u)
/bardblu/bardbl2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg∂N(u)
/bardblu/bardbl2−2uN(u)
/bardblu/bardbl3
2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂N/parenleftbiggu
/bardblu/bardbl2/parenrightbigg
−2u
/bardblu/bardbl2N/parenleftbiggu
/bardblu/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≥/vextenddouble/vextenddouble/vextenddouble/vextenddoubles−2u
/bardblu/bardbl2N/parenleftbiggu
/bardblu/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2−/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂N/parenleftbiggu
/bardblu/bardbl2/parenrightbigg
−s/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≥/bardbls−2u∗N(u∗)/bardbl2−/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂N/parenleftbiggu
/bardblu/bardbl2/parenrightbigg
−s/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2−/vextenddouble/vextenddouble/vextenddouble/vextenddouble2u∗N(u∗)−2u
/bardblu/bardbl2N/parenleftbiggu
/bardblu/bardbl2/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,
where in the second equality we used 1−homogeneity of ∂N(u)and 2−homogeneity ofN(u), and the
inequalities follow from triangle inequality of norms. Hence, for a.e. t≥T, using eq. (37), eq. (38) and
eq. (39) we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg
I−u(t)u(t)/latticetop
/bardblu(t)/bardbl2
2/parenrightbigg∂N(u(t))
/bardblu(t)/bardbl2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≥γ−γ/4−γ/4 =γ/2,
implying
d
dt/parenleftbigg
N/parenleftbiggu(t)
/bardblu(t)/bardbl2/parenrightbigg/parenrightbigg
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg
I−u(t)u(t)/latticetop
/bardblu(t)/bardbl2
2/parenrightbigg∂N(u(t))
/bardblu(t)/bardbl2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2≥γ2/4,
which contradicts the fact that limt→∞N/parenleftBig
u(t)
/bardblu(t)/bardbl2/parenrightBig
converges, thus proving our claim.
Now, before we turn to prove Theorem 5.1, we state another useful lemma.
Lemma C.7. Iflimt→∞u(t)/negationslash=0,then there exists η>0andT≥0such that/bardblu(t)/bardbl2≥η,for allt≥T.
Proof.The proof is built using the argument already presented in the proof of Lemma 5.3, and we consider
two cases.
Case 1:N(u(0))>0.
We assumeN(u(0)) =γ > 0,and therefore,/bardblu(0)/bardbl2>0. From Lemma C.4, for all t≥0, we have
N(u(t))≥N(u(0)) =γ, which implies
1
2d/bardblu/bardbl2
2
dt=u/latticetop˙u= 2N(u(t))≥2γ,for a.e.t≥0, (40)
which in turn implies
/bardblu(t)/bardbl2≥/bardblu(0)/bardbl2for allt≥0. (41)
24Published in Transactions on Machine Learning Research (06/2024)
Thus, we can choose η=/bardblu(0)/bardbl2, andT= 0.
Case 2:N(u(0))≤0.
For this case we may further assume that N(u(t))≤0,for allt≥0, since if for some t,N(u(t))>0,then
using Lemma C.4, we have /bardblu(t)/bardbl2≥/bardblu(t)/bardbl2, for allt≥t. Then, sinceN(u(t))>0implies/bardblu(t)/bardbl2>0,
we may choose η=/bardblu(t)/bardbl2andT=t.
So, let us assume that N(u(t))≤0,for allt≥0. Then,
1
2d/bardblu/bardbl2
2
dt=u/latticetop˙u= 2N(u(t))≤0,for a.e.t≥0.
Therefore,/bardblu(t)/bardbl2decreases with time, and hence, limt→∞/bardblu(t)/bardbl2exists and/bardblu(t)/bardbl2≥
limt→∞/bardblu(t)/bardbl2,for allt≥0.Since we have assumed limt→∞u(t)/negationslash=0, we have limt→∞/bardblu(t)/bardbl2>0.
Thus, we may choose η= limt→∞/bardblu(t)/bardbl2, andT= 0.
C.3 Proof of Theorem 5.1
Consider the diﬀerential inclusion
˙u∈∂N−/lscript/prime(0,y),H(u)⊆−n/summationdisplay
i=1∇ˆy/lscript(0,yi)∂H(xi;u),u(0) = w0, (42)
and let u(t)be its unique solution. By Lemma 5.3, either limt→∞u(t) =0orlimt→∞u(t)
/bardblu(t)/bardbl2exists.
We ﬁrst consider the case when limt→∞u(t) =0. Here, we deﬁne η= 1and ﬁx an /epsilon1∈(0,η). Then, we
chooseTlarge enough such that
/bardblu(t)/bardbl2≤/epsilon1,for allt≥T. (43)
Next, if limt→∞u(t)/negationslash= 0, then from Lemma C.7, there exists η >0andT≥0such that/bardblu(t)/bardbl2≥2η, for
allt≥T. Also, from Lemma 5.3,
lim
t→∞u(t)//bardblu(t)/bardbl2=ˆu,
where ˆuis a non-negative KKT point of
max
/bardblu/bardbl2
2=1N−/lscript/prime(0,y),H(u) =−/lscript/prime(0,y)/latticetopH(X;u). (44)
For a ﬁxed /epsilon1∈(0,η), we choose T >Tsuch that
u(t)/latticetopˆu
/bardblu(t)/bardbl2≥1−/epsilon1,for allt≥T. (45)
Having chosen Tfor a ﬁxed/epsilon1∈(0,η)in both cases, we next choose Csuch thatln(C)
4β˜β=T.From Lemma 5.2,
there exists δsuch that for any δ≤δ
/bardblw(t)/bardbl2≤√
Cδ,for allt∈/bracketleftbigg
0,ln(C)
4β˜β/bracketrightbigg
,
and
/vextenddouble/vextenddouble/vextenddouble/vextenddoublew(T)
δ−u(T)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/epsilon1. (46)
Thus, we may writew(T)
δ=u(T) +ζ, where/bardblζ/bardbl2≤/epsilon1.Iflimt→∞u(t) =0, then, using eq. (43),
/bardblw(T)/bardbl2≤2δ/epsilon1.
25Published in Transactions on Machine Learning Research (06/2024)
Else, since/epsilon1∈(0,η), and/bardblu(T)/bardbl2≥2η, we have/bardblu(T) +ζ/bardbl2≥η. Hence,
w(T)
/bardblw(T)/bardbl2=u(T) +ζ
/bardblu(T) +ζ/bardbl2,
which implies
w(T)/latticetopˆu
/bardblw(T)/bardbl2=u(T)/latticetopˆu+ζ/latticetopˆu
/bardblu(T) +ζ/bardbl2=/parenleftbiggu(T)/latticetopˆu
/bardblu(T)/bardbl2/parenrightbigg/bardblu(T)/bardbl2
/bardblu(T) +ζ/bardbl2+ζ/latticetopˆu
/bardblu(T) +ζ/bardbl2.
Now, since
/bardblu(T)/bardbl2
/bardblu(T) +ζ/bardbl2≥/bardblu(T)/bardbl2
/bardblu(T)/bardbl2+/bardblζ/bardbl2=1
1 +/bardblζ/bardbl2
/bardblu(T)/bardbl2≥1
1 +/epsilon1
2η≥1−/epsilon1
2η,
and
ζ/latticetopˆu
/bardblu(T) +ζ/bardbl2≥−/epsilon1
η,
we have
w(T)/latticetopˆu
/bardblw(T)/bardbl2≥(1−/epsilon1)/parenleftbigg
1−/epsilon1
2η/parenrightbigg
−/epsilon1
η≥1−/parenleftbigg
1 +3
2η/parenrightbigg
/epsilon1.
C.4 Proof of Corollary 5.4.1
Consider the diﬀerential inclusion
˙u∈∂N−/lscript/prime(0,y),H(u)⊆−n/summationdisplay
i=1∇ˆy/lscript(0,yi)∂H(xi;u),u(0) = w0, (47)
and let u(t)be its unique solution. From separability, we can write u(t) = [u1(t),...,uH(t)]such that for
allj∈[H]we have
˙uj∈∂N−/lscript/prime(0,y),Hj(uj)⊆−n/summationdisplay
i=1∇ˆy/lscript(0,yi)∂Hj(xi;uj),uj(0) = w0j, (48)
where w0= [w01,...,w0H]/latticetop. By Lemma 5.3, for all j∈[H], either limt→∞uj(t) =0orlimt→∞uj(t)
/bardbluj(t)/bardbl2exists.
LetZbe the collection of all indices such that limt→∞uj(t) =0, for allj∈Z, andZcbe the complement
ofZin[H]. For allj∈Zc, from Lemma C.7, there exists ηj>0andTj≥0such that/bardbluj(t)/bardbl2≥2ηj, for
allt≥Tj. Deﬁneη= min(1,minj∈Zcηj)andT= maxj∈ZcTj, and ﬁx an /epsilon1∈(0,η).
From Lemma 5.3, for all j∈Zcwe have
lim
t→∞uj(t)//bardbluj(t)/bardbl2=ˆuj,
where ˆujis a non-negative KKT point of
max
/bardbluj/bardbl2
2=1N−/lscript/prime(0,y),Hj(uj) =−/lscript/prime(0,y)/latticetopHj(X;uj). (49)
Then, for a given /epsilon1, we choose T1>Tsuch that
uj(t)/latticetopˆuj
/bardbluj(t)/bardbl2≥1−/epsilon1,for allt≥T1,and allj∈Zc. (50)
For allj∈Z, we choose T2large enough such that
/bardbluj(t)/bardbl2≤/epsilon1,for allt≥T2and allj∈Z. (51)
26Published in Transactions on Machine Learning Research (06/2024)
DeﬁneT= max(T1,T2)and choose Csuch thatln(C)
4β˜β=T.From Lemma 5.2, there exists δsuch that for
anyδ≤δ
/bardblw(t)/bardbl2≤√
Cδ,for allt∈/bracketleftbigg
0,ln(C)
4β˜β/bracketrightbigg
,
and
/vextenddouble/vextenddouble/vextenddouble/vextenddoublew(T)
δ−u(T)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/epsilon1. (52)
By separability, we may writewj(T)
δ=uj(T) +ζj, where/bardblζj/bardbl2≤/epsilon1.Ifj∈Zc, then, using eq. (51) we have
/bardblwj(T)/bardbl2≤2δ/epsilon1.
Else, since /epsilon1∈(0,η), and/bardbluj(T)/bardbl2≥2η, we have/bardbluj(T) +ζj/bardbl2≥η. Hence, using similar reasoning as in
the later part of the proof of Theorem 5.1 we get for all j∈Zc,
wj(T)/latticetopˆuj
/bardblwj(T)/bardbl2≥1−/parenleftbigg
1 +3
2η/parenrightbigg
/epsilon1.
D Proofs Omitted from Section 5.2
We ﬁrst prove Lemma 5.6
D.1 Proof of Lemma 5.6
We note that since {wn,wz}is a saddle point of
L(wn,wz) =1
2/bardblHn(X;wn) +Hz(X;wz)−y/bardbl2, (53)
andwz= 0, we have
/bracketleftbigg0
0/bracketrightbigg
∈/bracketleftbigg∂wnL(wn,0)
∂wzL(wn,0)/bracketrightbigg
, (54)
which establishes 0∈∂wnL(wn,0).
To prove Theorem 5.5, we ﬁrst describe the approximate dynamics of {wn(t),wz(t)}near the saddle point
in the following lemma.
Lemma D.1. Let{wn,wz}satisfy Assumption 3, and deﬁne y=y−Hn(X;wn). LetC > 1be an
arbitrarily large constant and {wn(t),wz(t)}satisfy for a.e. t≥0
/bracketleftbigg˙wn
˙wz/bracketrightbigg
∈−/bracketleftbigg∂wnL(wn,wz)
∂wzL(wn,wz)/bracketrightbigg
,/bracketleftbiggwn(0)
wz(0)/bracketrightbigg
=/bracketleftbiggwn+δζn
wz+δζz/bracketrightbigg
, (55)
whereδ2≤min(1
2C,γ2
4)and/bardblζn/bardbl2=/bardblζz/bardbl2= 1. Then
/bardblwn(t)−wn/bardbl2
2+/bardblwz(t)−wz/bardbl2
2≤2Cδ2,for allt∈/bracketleftbigg
0,1
M2ln (C)/bracketrightbigg
, (56)
whereM2is a positive constant5. Further, for the diﬀerential inclusion
˙u∈∂Ny,Hz(u),u(0) =ζz, (57)
5M2here is same as in Theorem 5.5. See the statement of Theorem 5.5 and the the proof of Lemma D.1 for more details.
27Published in Transactions on Machine Learning Research (06/2024)
and for any /epsilon1>0there exists a small enough δ>0such that for any δ≤δ,
/vextenddouble/vextenddouble/vextenddouble/vextenddoublewz(t)
δ−u(t)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/epsilon1,for allt∈/bracketleftbigg
0,ln(C)
M2/bracketrightbigg
, (58)
where u(t)is a certain solution of eq. (57).
Proof.We note that since {wn,wz}is a saddle point of
L(wn,wz) =1
2/bardblHn(X;wn) +Hz(X;wz)−y/bardbl2, (59)
we have
/bracketleftbigg0
0/bracketrightbigg
∈/bracketleftbigg/summationtextn
i=1yi∂Hn(xi;wn)/summationtextn
i=1yi∂Hz(xi;wz)/bracketrightbigg
. (60)
We now deﬁne
∆n(t) =Hn(X;wn(t))−Hn(X;wn),∆z(t) =Hz(X;wz(t)),andZ(t) =/bardblwn(t)−wn/bardbl2
2+/bardblwz(t)/bardbl2
2.
Since (wn(t),wz(t))is a continuous curve, Z(t)is also a continuous curve. Note that Z(0) = 2δ2≤
min(1/C,γ2/2)<min(1,γ2). Therefore, there exists some t > 0, such that Z(t)≤min(1,γ2), for
allt∈[0,t]. LetT∗be the smallest t > 0such that Z(T∗) = min(1,γ2). Hence, for all t∈[0,T∗],
Z(t)≤min(1,γ2). Our next goal is to ﬁnd a lower bound for T∗. We operate in [0,T∗].
Recall that
/bardblHz(X;wz)/bardbl2≤β/bardblwz/bardbl2
2. (61)
Moreover, by the locally Lipschitz property of Hn(X;wn), there exists µ1>0such that
/bardbl∆N(t)/bardbl2≤µ1/bardblwn(t)−wn/bardbl2,for allt∈[0,T∗]. (62)
Deﬁne e(t) =Hn(X;wn(t)) +Hz(X;wz(t))−y, then, using Z(t)≤1,we have
/bardble(t)/bardbl2=/bardblHn(X;wn(t)) +Hz(X;wz(t))−y/bardbl2
≤/bardbly/bardbl2+/bardbl∆N(t)/bardbl2+/bardblHz(X;wz(t))/bardbl2≤/bardbly/bardbl2+µ1+β:=M1,
where in the last inequality we used eq. (62) and eq. (61). Using Corollary B.1.1, we have
1
2d/bardblwz(t)/bardbl2
2
dt=−2Hz(X;wz)/latticetope≤2βM1/bardblwz(t)/bardbl2
2. (63)
We ﬁrst consider the case when Hn(x;wn)has a locally Lipschitz gradient. Let
J(wn) =: [∇Hn(x1;wn),...,∇Hn(xn;wn)]∈Rd×n.
From eq. (60), we have
0=n/summationdisplay
i=1yi∇Hn(xi;wn) =J(wn)y. (64)
By the locally Lipschitz property of ∇Hn(x;wn), we may assume that there exists µ2>0such that
/bardblJ(wn(t))y−J(wn)y/bardbl2≤µ2/bardblwn(t)−wn/bardbl2. (65)
Further, since wn(t)is bounded for all t∈[0,T∗], we may assume there exists µ3>0such that
/bardblJ(wn(t))/bardbl2≤µ3. (66)
28Published in Transactions on Machine Learning Research (06/2024)
Thus,
1
2d/bardblwn−wn/bardbl2
2
dt=−/angbracketleftwn−wn,J(wn)e/angbracketright
=−/angbracketleftwn−wn,J(wn) (Hn(X;wn) +Hz(X;wz)−y)/angbracketright
=−/angbracketleftwn−wn,J(wn) (∆n(t) + ∆z(t)−y)/angbracketright
=/angbracketleftwn−wn,J(wn)y/angbracketright−/angbracketleftwn−wn,J(wn) (∆n(t) + ∆z(t))/angbracketright
=/angbracketleftwn−wn,J(wn)y−J(wn)y/angbracketright−/angbracketleftwn−wn,J(wn) (∆n(t) + ∆z(t))/angbracketright
≤µ2/bardblwn−wn/bardbl2
2+/bardblwn−wn/bardbl2/bardblJ(wn)/bardbl2(/bardbl∆n(t)/bardbl2+/bardbl∆z(t)/bardbl2)
≤µ2/bardblwn−wn/bardbl2
2+µ1µ3/bardblwn−wn/bardbl2
2+βµ3/bardblwn−wn/bardbl2/bardblwz/bardbl2
2
≤(µ2+µ1µ3)/bardblwn−wn/bardbl2
2+βµ3/bardblwz/bardbl2
2.
The third equality follows from deﬁnition of ∆n(t)and∆z(t). In last equality, we use eq. (64). The ﬁrst
inequality follows from Cauchy-Schwartz and eq. (65). We get second inequality from eq. (66),eq. (62) and
eq. (61). In the ﬁnal inequality, we use /bardblwn(t)−wn/bardbl2≤1, for allt∈[0,T∗].
Combining the above inequality with eq. (63), we obtain
1
2dZ(t)
dt≤(µ2+µ1µ3)/bardblwn(t)−wn/bardbl2
2+β(2M1+µ3)/bardblwz(t)/bardbl2
2≤M2Z(t),
whereM2:= max(µ2+µ1µ3,β(2M1+µ3)). Therefore, for all t∈[0,T∗],Z(t)≤Z(0)etM2implies
T∗≥1
M2ln/parenleftbigg1
Z(0)/parenrightbigg
=1
M2ln/parenleftbigg1
2δ2/parenrightbigg
.
Since we assume 2δ2≤1
C, we have that T∗≥1
M2ln (C). Thus, for all t∈/bracketleftBig
0,1
M2ln (C)/bracketrightBig
,
Z(t)≤CZ(0)≤2Cδ2,
proving eq. (56).
We next consider the case when Hn(X;wn)does not have a locally Lipschitz gradient. Recall that if
/bardblwn−wn/bardbl2≤γ, then
/angbracketleftwn−wn,s/angbracketright≥−κ/bardblwn−wn/bardbl2
2,where s∈−∂wnL(wn,0). (67)
Further, we may assume that there exists a constant µ3>0such that if/bardblwn−wn/bardbl2≤γ, then
max
i∈[n]/bardblpi/bardbl2≤µ3,where pi∈∂H(xi;wn). (68)
Using the chain rule, we also have
∂wnL(wn,wz)⊆∂wn(L(wn,wz)−L(wn,0)) +∂wnL(wn,0). (69)
Note that∂wn(L(wn,wz)−L(wn,0))⊆/summationtextn
i=1Hz(xi;wz)∂Hn(xi;wn). Therefore, if/bardblwn−wn/bardbl2≤γ, then,
using eq. (68), for any wz, and p∈∂wn(L(wn,wz)−L(wn,0)), we have
/bardblp/bardbl2≤µ3/bardblHn(X;wz)/bardbl1≤µ3√n/bardblHn(X;wz)/bardbl2≤µ3√nβ/bardblwz/bardbl2
2. (70)
Next, using eq. (69) we have
1
2d/bardblwn−wn/bardbl2
2
dt=/angbracketleftwn−wn,˙wn/angbracketright∈−/angbracketleft wn−wn,∂wnL(wn,wz)/angbracketright
∈−/angbracketleftwn−wn,∂wn(L(wn,wz)−L(wn,0)) +∂wnL(wn,0)/angbracketright.
29Published in Transactions on Machine Learning Research (06/2024)
Since/bardblwn(t)−wn/bardbl2≤γfor allt∈[0,T∗], using eq. (67) and eq. (70), we have
1
2d/bardblwn−wn/bardbl2
2
dt≤µ3√nβ/bardblwn−wn/bardbl2/bardblwz/bardbl2
2+κ/bardblwn−wn/bardbl2
2≤µ3√nβ/bardblwz/bardbl2
2+κ/bardblwn−wn/bardbl2
2,
where in the last inequality we use /bardblwn(t)−wn/bardbl2≤1, for allt∈[0,T∗]. Combining above inequality with
eq. (63), we have
1
2dZ(t)
dt≤β(2M1+µ3√n)/bardblwz(t)/bardbl2
2+κ/bardblwn−wn/bardbl2
2≤M2Z(t),
whereM2:= max(κ,β(2M1+µ3√n)). Therefore, for all t∈[0,T∗], we have that Z(t)≤Z(0)etM2implies
T∗≥1
M2ln/parenleftbigg1
Z(0)/parenrightbigg
=1
M2ln/parenleftbigg1
2δ2/parenrightbigg
.
Since we assume 2δ2≤1
C, we have that T∗≥1
M2ln (C). Thus, for all t∈/bracketleftBig
0,1
M2ln (C)/bracketrightBig
,
Z(t)≤CZ(0)≤2Cδ2,
proving eq. (56).
We now move towards proving the second part. We use a similar technique as in the proof of Lemma 5.2.
We deﬁneξ(t) =e(t) +y. Then,
/bardblξ(t)/bardbl2=/bardbly+Hn(X;wn(t)) +Hz(X;wz(t))−y/bardbl2
=/bardblHn(X;wn(t))−Hn(X;wn) +Hz(X;wz(t))/bardbl2
≤µ1/bardblwn(t)−wn/bardbl2+β/bardblwz(t)/bardbl2
2
≤µ1√
Cδ+βCδ2.
Thus, the dynamics of wz(t)can be written as
˙wz∈−n/summationdisplay
i=1ei∂Hz(xi;wz) =n/summationdisplay
i=1(yi−ξ(t))∂Hz(xi;wz). (71)
Dividing eq. (71) by δ, and using 1-homogeneity of ∂H(x;w)(Corollary B.1.1), we have
˙wz
δ∈1
δn/summationdisplay
i=1(yi−ξ(t))∂Hz(xi;wz) =n/summationdisplay
i=1(yi−ξ(t))∂Hz(xi;wz/δ). (72)
Now, consider the diﬀerential inclusion
d˜wz
dt∈n/summationdisplay
i=1yi∂Hz(xi;˜wz),˜wz(0) =ζz. (73)
Since for all t∈/bracketleftBig
0,1
M2ln (C)/bracketrightBig
,/bardblξ(t)/bardbl2≤µ1√
Cδ+βCδ2, using Lemma C.1, there exists a small enough δ
such that for all δ≤δ,/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜wz(t)−wz(t)
δ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/epsilon1,
where ˜w(t)is a solution of eq. (73).
30Published in Transactions on Machine Learning Research (06/2024)
D.2 Proof of Theorem 5.5
Consider the diﬀerential inclusion
˙u∈n/summationdisplay
i=1yi∂Hz(xi;u),u(0) =ζz, (74)
and let u(t)be its unique solution. By Lemma 5.3, either limt→∞u(t) =0orlimt→∞u(t)
/bardblu(t)/bardbl2exists.
We ﬁrst consider the case when limt→∞u(t) =0. Here, we deﬁne η= 1and ﬁx an /epsilon1∈(0,η). Then, we
chooseTlarge enough such that
/bardblu(t)/bardbl2≤/epsilon1,∀t≥T. (75)
We next consider the case when limt→∞u(t)/negationslash= 0. Then, from Lemma C.7, there exists η >0andT≥0
such that/bardblu(t)/bardbl2≥2η, for allt≥T. Also, from Lemma 5.3,
lim
t→∞u(t)//bardblu(t)/bardbl2=ˆu,
where ˆuis a non-negative KKT point of
maxHz(X;u)/latticetopy,such that/bardblu/bardbl2
2= 1. (76)
For a ﬁxed /epsilon1∈(0,η), we choose T >Tsuch that
u(t)/latticetopˆu
/bardblu(t)/bardbl2≥1−/epsilon1,for allt≥T. (77)
Having chosen Tfor a ﬁxed/epsilon1∈(0,η)in both cases, we next choose Csuch thatln(C)
M2=T.From Lemma D.1,
there exists δsuch that for any δ≤δ
/bardblwn(t)−wn/bardbl2
2+/bardblwz(t)−wz/bardbl2
2≤2Cδ2,for allt∈/bracketleftbigg
0,ln(C)
M2/bracketrightbigg
,
and
/vextenddouble/vextenddouble/vextenddouble/vextenddoublewz(T)
δ−u(T)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/epsilon1. (78)
Thus, we may writewz(T)
δ=u(T) +ζ, where/bardblζ/bardbl2≤/epsilon1.Iflimt→∞u(t) =0, then, using eq. (75),
/bardblwz(T)/bardbl2≤2δ/epsilon1.
Else, since/epsilon1∈(0,η), and/bardblu(T)/bardbl2≥2η, we have/bardblu(T) +ζ/bardbl2≥η. Hence,
wz(T)
/bardblwz(T)/bardbl2=u(T) +ζ
/bardblu(T) +ζ/bardbl2
which implies
wz(T)/latticetopˆu
/bardblwz(T)/bardbl2=u(T)/latticetopˆu+ζ/latticetopˆu
/bardblu(T) +ζ/bardbl2=/parenleftbiggu(T)/latticetopˆu
/bardblu(T)/bardbl2/parenrightbigg/bardblu(T)/bardbl2
/bardblu(T) +ζ/bardbl2+ζ/latticetopˆu
/bardblu(T) +ζ/bardbl2.
Since
/bardblu(T)/bardbl2
/bardblu(T) +ζ/bardbl2≥/bardblu(T)/bardbl2
/bardblu(T)/bardbl2+/bardblζ/bardbl2=1
1 +/bardblζ/bardbl2
/bardblu(T)/bardbl2≥1
1 +/epsilon1
2η≥1−/epsilon1
2η,
and
ζ/latticetopˆu
/bardblu(T) +ζ/bardbl2≥−/epsilon1
η,
we have that
wz(T)/latticetopˆu
/bardblwz(T)/bardbl2≥(1−/epsilon1)/parenleftbigg
1−/epsilon1
2η/parenrightbigg
−/epsilon1
η≥1−/parenleftbigg
1 +3
2η/parenrightbigg
/epsilon1.
31Published in Transactions on Machine Learning Research (06/2024)
(a)
 (b)
Figure 4: Panel (a): the evolution of training loss and the /lscript2-distance of the weights from the saddle point
with iterations. Panel (b): The lower part shows the evolution of arctan( u2i(t)/u1i(t))for the last 10 hidden
neurons. The top plot shows the constrained NCF N¯y,H(θ) =/summationtextn
i=1¯yimax(0,[cos(θ),sin(θ)]/latticetopxi)2, where
¯yis the residual error at the saddle point. We see that the weights remain near the saddle point and loss
barely changes, though the weights of the last 10 neurons converge in direction to the KKT points of the
constrained NCF.
D.3 Numerical Experiments
We experimentally show the phenomenon of directional convergence among the weights of small magnitude
near saddle points. For this, we again consider the example depicted in Figure 1. Recall, the network
architecture is deﬁned as H(x1,x2;{ui}20
i=1) =/summationtext20
i=1max(0,u1ix1+u2ix2)2, and there are 50 unit norm
inputs with corresponding labels generated using the function H∗(x1,x2) = 5 max(0 ,x1)2+ 4 max(0,−x1)2.
We consider the following saddle point {¯u1,···,¯u20}, where ui= [/radicalbig
1/2,0], ifi≤10, and ui= [0,0], if
i >10. Now,{u1,···,u20}is a saddle point since if x1≥0, thenH(x1,x2;{ui}20
i=1) = 5 max(0 ,x1)2=
H∗(x1,x2), and ifx1<0, then∇uiH(x1,x2;{ui}20
i=1) =0, for alli≤10. Moreover,∇uiH(x1,x2;{ui}20
i=1) =
0, for alli>10.
We initialize the weights by adding an i.i.d. Gaussian vector of standard deviation 10−5to{¯u1,···,¯u20}.
We use square loss and optimize using gradient descent for 50000 iterations with step-size 5·10−5. We
plot the evolution of the overall loss and the /lscript2distance of the network weights from the saddle point with
iterations in Figure 4a, and Figure 4b contains the evolution of the angle the weight vectors of the last 10
hidden neurons (they have small magnitude at initialization) make with the positive horizontal axis with
iterations and the constrained NCF deﬁned with respect to the residual error at the saddle point. It is
evident that the training loss barely changes, and the weights remains close to the saddle point. Moreover,
the individual weight vectors for the neurons with small magnitude converge to the the KKT point of the
constrained NCF. We note that the constrained NCF seems to have a set of ﬂat KKT points, and therefore,
if the weights are initialized in that set, then they remain there as seen in Figure 4b.
E Gradient Flow Dynamics of f(u1, u2) =u1|u2|
In the following lemma, we describe the gradient ﬂow solutions of f(u1,u2) =u1|u2|when initialized at
[1,0]/latticetop.
Lemma E.1. For anyT >0, consider the following time-varying function
uT(t) =/bracketleftbiggu1T(t)
u2T(t)/bracketrightbigg
=

/bracketleftbigg1
0/bracketrightbigg
,for allt∈[0,T]
/bracketleftbiggcosh (t−T)
sinh (t−T)/bracketrightbigg
,for allt≥T,(79)
32Published in Transactions on Machine Learning Research (06/2024)
then, for a.e. t≥0,uT(t)satisﬁes
/bracketleftbigg˙u1
˙u2/bracketrightbigg
∈/bracketleftbigg
∂u1f(u1,u2)
∂u2f(u1,u2)/bracketrightbigg
=/bracketleftbigg
|u2|
u1∂|u2|/bracketrightbigg
,/bracketleftbigg
u1(0)
u2(0)/bracketrightbigg
=/bracketleftbigg
1
0/bracketrightbigg
, (80)
where
∂|u2|∈

[−1,1],ifu2= 0
1,ifu2>0,
−1,ifu2<0.
Proof.Fort∈[0,T),uT(t)is a constant, hence,
/bracketleftbigg˙u1T
˙u2T/bracketrightbigg
=/bracketleftbigg0
0/bracketrightbigg
.
Sinceu1T(t) = 1andu2T(t) = 0, for allt∈[0,T), we have
/bracketleftbigg|u2T(t)|
u1T(t)∂|u2T(t)|/bracketrightbigg
=/bracketleftbigg0
[−1,1]/bracketrightbigg
/owner/bracketleftbigg0
0/bracketrightbigg
.
Fort>T,u1T(t)andu2T(t)are continuous and diﬀerentiable functions, thus,
/bracketleftbigg
˙u1T
˙u2T/bracketrightbigg
=/bracketleftbigg
sinh(t−T)
cosh(t−T)/bracketrightbigg
.
Sinceu2T(t)>0, hence, for all t>T, we have
/bracketleftbigg|u2T(t)|
u1T(t)∂|u2T(t)|/bracketrightbigg
=/bracketleftbiggsinh(t−T)
cosh(t−T)/bracketrightbigg
,
completing the proof.
The above lemma shows that for any T >0,uT(t)deﬁned in eq. (79) is a gradient ﬂow solution of f(u1,u2)
when initialized at [1,0]/latticetop. For any ﬁnite T, it is easy to see that limt→∞uT(t)//bardbluT(t)/bardbl2= [1/√
2,1/√
2]/latticetop.
Hence, for a ﬁxed Tand any/epsilon1>0, we can choose Tsuch that
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleuT(T)
/bardbluT(T)/bardbl2−/bracketleftbigg1/√
2
1/√
2/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble≥/epsilon1.
In the next lemma, we show that except for the set {u2= 0,u1>0}, all points are non-branching for
gradient ﬂow dynamics of f(u1,u2) =u1|u2|.
Lemma E.2. Let[z1,z2]/latticetop∈R2\{u2= 0,u1>0}, then the diﬀerential inclusion
/bracketleftbigg˙u1
˙u2/bracketrightbigg
∈/bracketleftbigg∂u1f(u1,u2)
∂u2f(u1,u2)/bracketrightbigg
=/bracketleftbigg|u2|
u1∂|u2|/bracketrightbigg
,/bracketleftbiggu1(0)
u2(0)/bracketrightbigg
=/bracketleftbiggz1
z2/bracketrightbigg
(81)
has a unique solution, where
∂|u2|∈

[−1,1],ifu2= 0
1,ifu2>0,
−1,ifu2<0.
Proof.We ﬁrst note that
˙u1u1= ˙u2u2,
which implies
d
dt(u2
1(t)−u2
2(t)) = 0.
33Published in Transactions on Machine Learning Research (06/2024)
Hence,
u2
1(t)−u2
2(t) =z2
1−z2
2. (82)
Now, we prove the lemma by considering diﬀerent cases.
Case 1:|z2|>|z1|. From eq. (82), we have u2
1(t)−u2
2(t) =z2
1−z2
2. Thus,u2
2(t) =u2
1(t)+z2
2−z2
1. Now, since
|z2|>|z1|, we get that u2(t)>0,∀t≥0. Sinceu2(t)stays away from the set {u2= 0},∂u1f(u1(t),u2(t))
and∂u2f(u1(t),u2(t))are always unique and hence, [u1(t),u2(t)]is unique.
Case 2:|z2| ≤ |z1|,z2/negationslash= 0,z1>0.In this case, since |z2|>0, we havez1=|z1|>0. Now, since
∂u1f(u1(t),u2(t)) =|u2(t)|≥0andz1>0, therefore, u1(t)≥z1>0,∀t≥0. Now,
˙(u2
2) = 2u2˙u2= 2|u2|u1.
Sinceu1(t)>0,∀t≥0, we getu2
2(t)≥u2
2(0) =z2
2>0,∀t≥0.Hence,u2(t)stays away from the set {u2= 0},
which implies ∂u1f(u1(t),u2(t))and∂u2f(u1(t),u2(t))are always unique and thus, [u1(t),u2(t)]is unique.
Before proceeding to the next cases, we ﬁrst show that the set S={u2= 0,u1≤0}is a stable critical point,
that is, if{u1(¯t),u2(¯t)}∈S, for some time ¯t≥0, then, [u1(t),u2(t)] = [u1(¯t),u2(¯t)],∀t≥¯t.
Proving [0,0]is a stable critical point is trivial since ∂u1f(0,0) = 0 =∂u2f(u1,u2).
Now, suppose [u1(¯t),u2(¯t)] = [ˆu1,ˆu2], where ˆu1<0andˆu2= 0. Then,f(ˆu1,ˆu2) = 0, and since gradient
ﬂow can not decrease the objective value, we have f(u1(t),u2(t))≥0, for allt≥¯t. Also, since for any
˜u1<0and˜u2/negationslash= 0, we havef(˜u1,˜u2)<0, therefore, gradient ﬂow can only move along the u2= 0axis,
i.e., onlyu1(t)can change. However, since ∂u1f(ˆu1,ˆu2) =|ˆu2|= 0,u1(t)can not change for t≥¯t. Hence,
[u1(t),u2(t)] = [u1(¯t),u2(¯t)], for allt≥¯t.
Case 3:|z2|<|z1|,z1<0.From eq. (82), we have u2
1(t)−u2
2(t) =z2
1−z2
2. Thus,u2
1(t) =u2
2(t) +z2
1−z2
2.
Now, since|z1|>|z2|, we get that u2
1(t)>0,∀t≥0. Now, since u1(0) =z1<0, therefore, u1(t)<
0,∀t≥0.Hence, ifu2(¯t)is0for the ﬁrst time at some ¯t≥0, then from the stability of the set S, we have
[u1(t),u2(t)] = [u1(¯t),u2(¯t)],∀t≥¯t. For 0≤t<¯t, sinceu2(t)/negationslash= 0,∂u1f(u1(t),u2(t))and∂u2f(u1(t),u2(t))
are always unique and hence, [u1(t),u2(t)]is unique.
Case 3:|z2|=|z1|,z1≤0.From eq. (82), we have |u1(t)|=|u2(t)|.Now, since u1(0) =z1≤0, therefore,
ifu1(¯t)is0for the ﬁrst time at some ¯t≥0, thenu2(¯t)is also 0for the ﬁrst time at ¯t. Then, from the
stability of [0,0], we have [u1(t),u2(t)] = [0,0],∀t≥¯t. For 0≤t<¯t, sinceu2(t)/negationslash= 0,∂u1f(u1(t),u2(t))and
∂u2f(u1(t),u2(t))are always unique and hence, [u1(t),u2(t)]is unique.
F Proof of Lemma 5.4
Since{v∗,u∗}is a KKT point of
max
v2+/bardblu/bardbl2
2=1Nz,H(v,u) =vz/latticetopσ(X/latticetopu),
therefore, for some λ∈R, we have
0 =λv∗+z/latticetopσ(X/latticetopu∗), (83)
0∈λu∗+v∗n/summationdisplay
i=1σ/prime(x/latticetop
iu∗)zixi, (84)
whereσ/prime(·)is the subdiﬀerential of σ(·). Also, since x/latticetop
iu∗/negationslash= 0,σ/prime(x/latticetop
iu∗)is unique for all i∈[n].
Now, we may further assume that mini∈[n]|x/latticetop
iu∗|=η1andmaxi∈[n]/bardblxi/bardbl2=η2, for someη1,η2>0.Choose
γ=η1/(2η2), and recallS={v,u:sign(v∗)v >/bardblu/bardbl2,/bardblu−u∗/bardbl2≤γ}. Note that since v∗z/latticetopσ(X/latticetopu∗)>0,
therefore,|v∗|/negationslash= 0andsign(v∗)∈{− 1,1}. Also, multiplying eq. (83) by v∗and usingv∗z/latticetopσ(X/latticetopu∗)>0,
we get that λ<0.
34Published in Transactions on Machine Learning Research (06/2024)
Now, the gradient ﬂow equation is
/bracketleftbigg˙v
˙u/bracketrightbigg
∈/bracketleftbiggz/latticetopσ(X/latticetopu)
v/summationtextn
i=1σ/prime(x/latticetop
iu)zixi,/bracketrightbigg
,/bracketleftbiggv(0)
u(0)/bracketrightbigg
=/bracketleftbiggp
q/bracketrightbigg
,
where{p,q}∈S.We ﬁrst note that
d
dt(v2−/bardblu/bardbl2
2) = 2v˙v−2u/latticetop˙u= 0,
which implies
v2(t)−/bardblu(t)/bardbl2
2=p2−/bardblq/bardbl2
2. (85)
Sincesign(v∗)p>/bardblq/bardbl2, we getsign(p) =sign(v∗)and thus|p|>/bardblq/bardbl2. Therefore, from the above equation,
we getv2(t)−/bardblu(t)/bardbl2
2>0, which implies v2(t)>0. Sincev2(t)is continuous and never becomes 0, we have
sign(v(t)) =sign(v(0)) =sign(p) =sign(v∗).
To prove uniqueness, we will show that x/latticetop
iu(t)/negationslash= 0, for alli∈[n]andt≥0. Note that, since /bardblu∗−q/bardbl2≤γ,
we may assume q=u∗+/epsilon1b, where/bardblb/bardbl2= 1and|/epsilon1|≤γ. Thus,
|x/latticetop
iu(0)|=|x/latticetop
iq|≥|x/latticetop
iu∗|−|/epsilon1||x/latticetop
ib|≥η1−γη2=η1/2,
which implies x/latticetop
iu(0)/negationslash= 0, for alli∈[n]. For the sake of contradiction, suppose there exists some ¯t >0
such that x/latticetop
iu(¯t) = 0, for somei∈[n], for the ﬁrst time. Then, for all t∈[0,¯t), we have x/latticetop
iu(t)/negationslash= 0, for all
i∈[n]. Furthermore, due to continuity of u(t), we have, for all i∈[n],
sign(x/latticetop
iu(t)) =sign(x/latticetop
iu(0)),∀t∈[0,¯t).
Now, note that
x/latticetop
iu∗x/latticetop
iu(0) = x/latticetop
iu∗x/latticetop
iu∗+/epsilon1x/latticetop
iu∗x/latticetop
ib
≥|x/latticetop
iu∗|2−|/epsilon1||x/latticetop
iu∗||x/latticetop
ib|
=|x/latticetop
iu∗|(|x/latticetop
iu∗|−|/epsilon1||x/latticetop
ib|)≥η2
1/2.
Thus, for all i∈[n],
sign(x/latticetop
iu(t)) =sign(x/latticetop
iu(0)) =sign(x/latticetop
iu∗),∀t∈[0,¯t),
which implies
u(¯t) =u(0) +/integraldisplay¯t
0v(t)n/summationdisplay
i=1σ/prime(x/latticetop
iu(t))zixidt
=u(0) +/parenleftBigg/integraldisplay¯t
0v(t)dt/parenrightBiggn/summationdisplay
i=1σ/prime(x/latticetop
iu∗)zixi
=u(0) +/parenleftBigg/integraldisplay¯t
0v(t)/v∗dt/parenrightBigg
v∗n/summationdisplay
i=1σ/prime(x/latticetop
iu∗)zixi=u(0)−λ/parenleftBigg/integraldisplay¯t
0v(t)/v∗dt/parenrightBigg
u∗,
where in the last equality we used eq. (84). Now, recall that λ<0andsign(v(t)) =sign(v∗), for allt≥0.
Thus, we can write
u(¯t) =u(0) + ¯αu∗,
for some ¯α>0. This implies that, for all i∈[n],
x/latticetop
iu∗x/latticetop
iu(¯t) =x/latticetop
iu∗x/latticetop
iu(0) + ¯α|x/latticetop
iu∗|2≥η2
1/2 + ¯αη2
1>0,
which contradicts x/latticetop
iu(¯t) = 0, for somei∈[n].
35Published in Transactions on Machine Learning Research (06/2024)
G KKT Points of NCF: Some Examples
In this section we provide some examples where KKT points of the NCF can be computed analytically.
G.1 Symmetric Data and Squared ReLU
Letnbe even, and suppose the training set is the union of {xi,yi}n/2
i=1and{−xi,−yi}n/2
i=1such that
/bardbl/summationtextn/2
i=1yixix/latticetop
i/bardbl2/negationslash= 0. Let the neural network H(x;u) =σ(x/latticetopu),whereσ(x) = max(x,αx )2, for some
α∈R. Then, for square or logistic loss, the constrained NCF is
max
/bardblu/bardbl2
2=1n/2/summationdisplay
i=1yi/parenleftbig
σ(x/latticetop
iu)−σ(−x/latticetop
iu)/parenrightbig
.
Now, since σ(x)−σ(−x) = (1 +α2)x2, the constrained NCF can be written as
max
/bardblu/bardbl2
2=1(1 +α2)u/latticetop
n/2/summationdisplay
i=1yixix/latticetop
i
u.
It is well known that KKT points of the above problem will be the eigenvectors of the matrix/summationtextn/2
i=1yixix/latticetop
i.
G.2 Symmetric Data and 2-layer ReLU
This example is inspired from Lyu et al. (2021). Let nbe even, and suppose the training set is the union of
{xi,yi}n/2
i=1and{−xi,−yi}n/2
i=1such that/bardbl/summationtextn/2
i=1yixi/bardbl2/negationslash= 0. Let the neural network H(x;v,u) =vσ(x/latticetopu),
whereσ(x) = max(x,αx ), for someα∈R\{−1}. Then, for square or logistic loss, the constrained NCF is
max
v2+/bardblu/bardbl2
2=1n/2/summationdisplay
i=1vyi/parenleftbig
σ(x/latticetop
iu)−σ(−x/latticetop
iu)/parenrightbig
.
Now, since σ(x)−σ(−x) = (1 +α)x, the constrained NCF can be written as
max
v2+/bardblu/bardbl2
2=1(1 +α)v
n/2/summationdisplay
i=1yix/latticetop
i
u.
Letq=/summationtextn/2
i=1yixi.Now,{v∗,u∗}is a KKT point of the above problem if there exists λ∗such that
λ∗v∗+q/latticetopu∗=0, (86)
λ∗u∗+v∗q=0, (87)
v2
∗+/bardblu∗/bardbl2
2= 1. (88)
Now, if we choose λ∗= 0, thenv∗= 0andu∗=ˆusatisﬁes the KKT equation, where ˆuis any vector such
that ˆu/latticetopq= 0and/bardblˆu/bardbl2= 1.
Next, if we choose λ∗/negationslash= 0, thenv∗/negationslash= 0. Because, if v∗= 0, then, from eq. (87), we get /bardblu∗/bardbl2= 0, which
leads to violation of eq. (88). Since v∗/negationslash= 0, we have u∗=−(v∗/λ∗)q. Putting this in eq. (86), we get
λ2
∗=/bardblq/bardbl2
2.From eq. (88), we gave
1 =v2
∗+/bardblu∗/bardbl2
2=v2
∗+v2
∗/λ2
∗/bardblq/bardbl2
2,
which implies v2
∗= 1/√
2. Hence,{±1/√
2,±q//parenleftbig√
2/bardblq/bardbl2/parenrightbig
}is another set of KKT points.
36Published in Transactions on Machine Learning Research (06/2024)
H Directional Convergence for 2-layer (Leaky) ReLU Neural Network
Supposeσ(x) = max(x,αx )is the Leaky ReLU activation for some α∈R, andH(x;{vk,uk}H
i=1) =/summationtextH
k=1vkσ(x/latticetopuk)is the 2-layer Leaky ReLU neural network with Hhidden neurons. Now, since His
separable, from Corollary 5.4.1, for all k∈[H], in the initial stages of training {vk,uk}will either be
approximately 0or converge in direction to a non-negative KKT point of
max
v2+/bardblu/bardbl2
2=1vy/latticetopσ(X/latticetopu), (89)
where for the sake of simplicity we have assumed the loss function to be either square or logistic. The
following lemma sheds more light into the KKT points of the above optimization problem.
Lemma H.1. Suppose{v∗,u∗}is a non-zero KKT point of eq. (89), that is,v∗y/latticetopσ(X/latticetopu∗)/negationslash= 0. Then
|v∗|=/bardblu∗/bardbl2= 1/√
2, and√
2u∗is a KKT point of
max
/bardblu/bardbl2
2=1y/latticetopσ(X/latticetopu). (90)
Using the above lemma we get that if {vk,uk}converges in direction to a non-zero KKT point of eq. (89),
thenukwill have converged in direction to a KKT point of eq. (90). This is precisely the result stated in
Maennel et al. (2018). We also note that the result in Maennel et al. (2018) were derived under the balanced
initialization assumption, where /bardbluk/bardbl2=|vk|holds at initialization and remains such throughout training.
However, our results hold for more general initializations as well.
H.1 Proof of Lemma H.1
Proof.Since{v∗,u∗}is a KKT point of eq. (89), we have
0 =λv∗+y/latticetopσ(X/latticetopu∗)
0∈λu∗+v∗Xdiag(σ/prime(X/latticetopu∗))y, (91)
whereσ/prime(·)is the subdiﬀerential of σ(·)and is applied elementwise. Also, for a vector z,diag(z)denotes a
diagonal matrix constructed using entries from z.Upon multiplying the top and bottom equation by v∗and
u/latticetop
∗respectively we get
0 =λ|v∗|2+v∗y/latticetopσ(X/latticetopu∗)
0=λ/bardblu∗/bardbl2
2+v∗u/latticetop
∗Xdiag(σ/prime(X/latticetopu∗))y=λ/bardblu∗/bardbl2
2+v∗y/latticetopσ(X/latticetopu∗),
where we used σ/prime(x)x=σ(x). Now, since v∗y/latticetopσ(X/latticetopu∗)/negationslash= 0, by adding the two equations and using
|v∗|2+/bardblu∗/bardbl2
2= 1, we getλ/negationslash= 0. Sinceλ/negationslash= 0, from the above equation we also get |v∗|2=/bardblu∗/bardbl2
2. Combining
this with the fact that |v∗|2+/bardblu∗/bardbl2
2= 1, we have|v∗|=/bardblu∗/bardbl2= 1/√
2.
Furthermore, since λ/negationslash= 0, from eq. (91) we get
0∈u∗+ (v∗/λ)Xdiag(σ/prime(X/latticetopu∗))y,
which implies√
2u∗is a KKT point of eq. (90).
I Gradient Flow Dynamics of g(u) = (u1|u2|−1)2
We ﬁrst describe the gradient ﬁeld of g(u) = (u1|u2|−1)2near [1,0]T.
Lemma I.1. Let˜u= [1,0]T, then, 0∈∂g(˜u). Further, for any δ∈(0,0.1), letuδ= [1 +δ,δ]/latticetop. Then, for
anys∈−∂g(uδ),/bardbls/bardbl2≥1.
37Published in Transactions on Machine Learning Research (06/2024)
Proof.Since/bracketleftbigg
∂u1g(u1,u2)
∂u2g(u1,u2)/bracketrightbigg
=/bracketleftbigg
2|u2|(u1|u2|−1)
2u1∂|u2|(u1|u2|−1)/bracketrightbigg
,
it is easy to show 0∈∂g(˜u). Next,
/bracketleftbigg
∂u1g(uδ)
∂u2g(uδ)/bracketrightbigg
=/bracketleftbigg
−2δ(1−δ(1 +δ))
−2(1 +δ)(1−δ(1 +δ)),/bracketrightbigg
,
and so for any s∈−∂g(uδ)we have that
/bardbls/bardbl2≥2(1 +δ)(1−δ(1 +δ))≥2(1−0.1·1.1)≥1.
The lemma above establishes that there exist points in any arbitrarily small neighborhood of the saddle
point ˜uwhere the gradient of g(u)has large norm.
In the next lemma we describe the gradient ﬂow dynamics of g(u) = (u1|u2|−1)2near [1,0]T, and show that
gradient ﬂow will escape from any arbitrarily small neighborhood of the saddle point ˜uin a constant time.
Lemma I.2. Let˜u= [1,0]T, and uδ(t)be a solution of the diﬀerential inclusion
/bracketleftbigg˙u1
˙u2/bracketrightbigg
∈−/bracketleftbigg∂u1g(u1,u2)
∂u2g(u1,u2)/bracketrightbigg
,/bracketleftbiggu1(0)
u2(0)/bracketrightbigg
=/bracketleftbigg1 +δ
δ/bracketrightbigg
. (92)
Then, for any δ∈(0,0.1),/bardbluδ(0.1)−˜u/bardbl2≥0.09.
We show that no matter how close the initialization is to the saddle point ˜u, the gradient ﬂow will escape
from it within constant time. Here, /bardbluδ(0)−˜u/bardbl2≤√
2δwhereδcan be arbitrarily small and positive.
However,/bardbluδ(0.1)−˜u/bardbl2≥0.09, thus, uδ(t)escapes from the neighborhood of ˜uwithin constant time for
any arbitrarily small δ.
Proof.Chooseδ∈(0.0.1)and letS:={(u1,u2) :u1∈[0.8,1.2],u2∈[δ/2,0.35]}. Note that for any u∈S,
we have
δ/2≤δ(1−1.2·0.35)≤−2|u2|(u1|u2|−1)≤2·0.35·(1−0.8·δ/2)≤1,and (93)
0.9≤2·0.8(1−1.2·0.35)≤−2u1(u1|u2|−1)≤2·1.2·(1−0.8·δ/2)≤2.4. (94)
Letuδ(t)be a solution of eq. (92). For the sake of brevity, we use u(t)instead of uδ(t). Note that u(0)∈S.
LetTbe the smallest t≥0such that u(T)/∈S. For allt∈[0,T],u2(t)>0, thus, u(t)satisﬁes
/bracketleftbigg˙u1
˙u2/bracketrightbigg
=/bracketleftbigg−2|u2|(u1|u2|−1)
−2u1(u1|u2|−1)/bracketrightbigg
,for allt∈[0,T]. (95)
From eq. (93) and eq. (94), for any t∈[0,T], we have
˙u1∈[δ/2,1],and ˙u2∈[0.9,2.4]. (96)
Using these bounds, we next show that T >0.1. Assume for the sake of contradiction, T≤0.1. Then, from
eq. (96), for any t∈[0,T], we have
0.8<u1(0)≤u1(0) +δt/2≤u1(t)≤u1(0) +t≤1 +δ+ 0.1<1.2,and
δ/2<u2(0)≤u2(0) + 0.9t≤u2(t)≤u2(0) + 2.4t≤δ+ 0.24≤0.34
From the above equation, we observe that u(T)∈S, which leads to a contradiction. Thus, T >0.1. Hence,
using the lower bound on ˙u2in eq. (96), we have
u2(0.1)≥u2(0) + 0.9·0.1≥0.09.
Thus,
/bardblu(0.1)−˜u/bardbl2≥|u2(0.1)|≥0.09.
38Published in Transactions on Machine Learning Research (06/2024)
J Proof of Lemma C.1
We prove Lemma C.1 in a similar way as in (Filippov, 1988), though that proof considers a more general
case. For our problem, the proof can be slightly shortened.
To prove Lemma C.1 we make use fo the following lemma
Lemma J.1. (Filippov, 1988, Lemma 13, Section 5) Let for all t∈[a,b]the vector-valued function xk(t)
be absolutely continuous, xk(t)→x(t)ask→∞, and for each k= 1,2,...the functions ˙xk(t)∈M
almost everywhere on t∈(a,b), withMbeing a bounded closed set. Then the vector-valued function x(t)is
absolutely continuous and ˙x(t)∈conv(M)almost everywhere on t∈(a,b).
Proof of Lemma C.1. For the sake of contradiction, we assume that the statement in Lemma C.1 is not true.
Thus, for some /epsilon1>0there exists a sequence of solutions uj(t)of
du
dt∈n/summationdisplay
i=1(zi+fj
i(t))∂H(xi;u),u(0) = u0,j= 1,2,..., (97)
where|fj
i(t)|≤δj, for alli∈[n]andj≥1, andδj→0, such that for any solution ˜u(t)of
d˜u
dt∈n/summationdisplay
i=1zi∂H(xi;˜u),˜u(0) = u0, (98)
we have
max
t∈[0,T]/bardbluj(t)−˜u(t)/bardbl2>/epsilon1. (99)
We also assume that δj≤B/√n, for some positive constant Band for all j≥1. We ﬁrst show that
{uj(t)}∞
j=1has a convergent subsequence. Note that for any t∈[0,T],uj(t)is bounded since
d/bardbluj/bardbl2
2
dt= 4n/summationdisplay
i=1(zi+fj
i(t))H(xi;uj)≤4β/bardbluj/bardbl2
2(/bardblz/bardbl2+B),
which implies
/bardbluj(t)/bardbl2
2≤/bardblu0/bardbl2
2e4tβ(/bardblz/bardbl2+B)≤/bardblu0/bardbl2
2e4Tβ(/bardblz/bardbl2+B):=B2
1.
We next deﬁne
χ:= sup{/bardbl∂H(x;w)/bardbl2:w∈Sk−1}.
We note that{uj(t)}∞
i=1is equicontinuous, since for any t1,t2∈[0,T]andj≥1,
/bardbluj(t1)−uj(t2)/bardbl2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplayt2
t1n/summationdisplay
i=1(zi+fj
i(s))∂H(xi;uj)ds/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/integraldisplayt2
t1n/summationdisplay
i=1|(zi+fj
i(s))|χ/bardbluj/bardbl2ds
≤|t2−t1|B1χ√n(/bardblz/bardbl2+B).
Therefore,usingtheArzelà–AscoliTheorem,thereexistsasubsequence {ujk(t)}∞
k=1thatconvergesuniformly.
We denote the limiting function by ˆu(t). We complete our proof by showing ˆu(t)is a solution of eq. (98)
since that will lead to a contradiction.
For any vector u, we deﬁne F(u) =/summationtextn
i=1zi∂H(xi;u). For anyγ >0, theγ−neighborhood of F(u), denoted
byFγ(u), is deﬁned as
Fγ(u) ={v:v=h+q,h∈F(u),/bardblq/bardbl2≤γ}.
It is easy to show that for any ﬁnite γ,Fγ(u)is a nonempty, convex, and compact set. Also, we deﬁne
ˆuη=:{u:/bardblu−ˆu/bardbl2≤η},ˆtα=:{t:|t−ˆt|≤α}
39Published in Transactions on Machine Learning Research (06/2024)
Choose any ˆt∈[0,T]. Since,F(u)is upper semicontinuous, for any γ >0, there exist a small enough η>0
such that for all u∈ˆuη(ˆt),F(u)⊆Fγ(ˆu(ˆt)). Further, since ˆu(t)is continuous, there exists a small enough
αsuch that, for all t∈ˆtα,ˆu(t)∈ˆuη(ˆt). Hence, for any γ >0, there exist a small enough α>0, such that
for allt∈ˆtα,F(ˆu(t))⊆Fγ(ˆu(ˆt)).
Next, since{ujk(t)}∞
k=1converges uniformly to ˆu(t), we can choose k1large enough such that /bardblujk(t)−
ˆu(t)/bardbl2≤η
2, forallk>k 1andt∈ˆtα. Since ˆu(t)iscontinuous, thereexist β >0, suchthat/bardblˆu(t)−ˆu(ˆt)/bardbl2≤η
2,
for allt∈ˆtβ. Thus, for all t∈ˆtα∩ˆtβandk>k 1,
/bardblujk(t)−ˆu(ˆt)/bardbl2≤/bardblujk(t)−ˆu(t)/bardbl2+/bardblˆu(t)−ˆu(ˆt)/bardbl2≤η.
Hence,F(ujk(t))⊆Fγ(ˆu(ˆt)), for allt∈ˆtα∩ˆtβandk>k 1.
Also, since δjk→0, we can choose k2large enough, such that δjk≤γ
nB1χ, for allk > k 2.Thus,
/bardbl/summationtextn
i=1fjk
i(t)∂H(xi;ujk(t))/bardbl2≤nB1χδjk≤γ, for allk > k 2.Therefore, for all k≥max{k1,k2}and
t∈ˆtα∩ˆtβ,
˙ujk(t)∈F(ujk(t)) +n/summationdisplay
i=1fjk
i(t)∂H(xi;ujk(t))∈F2γ(ˆu(ˆt)),for a.e.t∈ˆtα∩ˆtβ.
From Lemma J.1, ˆu(t)is absolutely continuous in ˆtα∩ˆtβand
˙ˆu(t)∈F2γ(ˆu(ˆt)),for a.e.t∈ˆtα∩ˆtβ.
We can cover the interval [0,T]by varying ˆt. Therefore, ˆu(t)is absolutely continuous in the interval [0,T]
and ˙ˆu(t)exist almost everywhere. Also, if for any t∈[0,T],˙ˆu(t)exists then ˙ˆu(t)∈F2γ(ˆu(t)), whereγcan
be made arbitrarily small. Hence,
˙ˆu(t)∈F(ˆu(t)),for a.e.t∈[0,T].
40