Under review as submission to TMLR
Multiplayer Information Asymmetric Contextual Bandits
Anonymous authors
Paper under double-blind review
Abstract
Single-player contextual bandits are a well-studied problem in reinforcement learning that
has seen applications in various fields such as advertising, healthcare, and finance. In light
of the recent work on information asymmetric bandits Chang et al. (2022); Chang and Lu
(2023), weproposeanovelmultiplayerinformationasymmetriccontextualbanditframework
where there are multiple players each with their own set of actions. At every round, they
observe the same context vectors and simultaneously take an action from their own set
of actions, giving rise to a joint action. However, upon taking this action the players are
subjected to information asymmetry in (1) actions and/or (2) rewards. We designed an
algorithm mLinUCB by modifying the classical single-player algorithm LinUCBin Chu et al.
(2011) to achieve the optimal regret O(√
T)when only one kind of asymmetry is present.
We then propose a novel algorithm ETCthat is built on explore-then-commit principles to
achieve the same optimal regret when both types of asymmetry are present.
1 Introduction
The problem of Multi-armed Bandits (MAB) is one of the most well-studied classic reinforcement learning
problems. The algorithms in the field are designed to find an optimal balance between the exploration-
exploitation tradeoff dilemma. In the traditional setting of this problem, a single agent chooses one action
(arm) from mavailable actions over numerous iterations, where each action gives off a reward sampled from
some unknown sub-Gaussian distribution. The primary objective is to minimize the agent’s regret, defined
as the difference between the expected reward of the agent’s chosen actions and that of the optimal actions.
Thus, the success of a policy can be measured by the regretas a function of time (number of actions taken).
Under this classical setting, Lai and Robbins (1985) showed that no policy can achieve better than O(logT)
regret. The UCB algorithm first attains this lower bound.
Althoughsingle-playerMABsarewell-studied, theyfailtomodelmorecomplexreal-worldproblemsinvolving
multipleparticipants. Recently,therehasbeenescalatinginterestincooperativemultiplayerMABchallenges,
wherein several agents aim to maximize their aggregate expected returns collaboratively Chang and Lu
(2023); Chang et al. (2022); Wang et al. (2020); Brânzei and Peres (2021); Pacchiano et al. (2023). Although
these problem settings extend the MAB problems into multiple players, they still remain restrictive in real-
world applications in these three aspects:
(1) These settings do not model the agents’ access to information that might help agents predict the
reward quality of an action (i.e. no context vectors).
(2) These settings assume the rewards obtained by each player are independent of the actions taken by
other players (i.e. joint actions are not considered).
(3) These settings assume the agents can freely communicate their actions taken and rewards received
to one another (i.e information is perfectly symmetric).
To deal with restriction (1), prior works such as Chu et al. (2011) analyze the linear contextual bandit
framework. Linear contextual bandits generalize the classical finite-armed MAB by allowing players to
utilize side information to predict the quality of rewards. In each round of the contextual bandit problem,
1Under review as submission to TMLR
the agent observes one random context vector xper action, where the expectation of the reward distribution
ofthatactionisazero-meannoiseplustheinnerproductofthecontextvector xandanunderlyingparameter
θthat is unknown to the players.1This framework Chu et al. (2011) relaxes the aforementioned restriction
(1) by allowing agents to make use of the observed context θto predict the rewards.
In this paper, we address restriction (2) by extending the contextual bandit framework into a multiplayer
setting where the joint action of all players determines the reward distribution. Furthermore, we add infor-
mation asymmetric multiplayer linear contextual bandit framework to make our setting even more general,
which we are the first to do so. At each round, each player takes an action individually andsimultaneously
resulting in a joint action. This joint action generates the rewards for all players. In every round, all agents
observe the same context vectors (one context vector per joint action). This multiplayer extension relaxes
restriction 2.
To restrict communication between players and relax restrictions 3, we separately consider the following two
types of information asymmetries: (1) Action asymmetry – At each round, each player receives the same
reward but cannot observe other player’s actions (the joint actions remains hidden). (2) Reward asymmetry
– at each round, each player receives an IID reward that can be only observed by themselves, while they are
allowed to observe the actions of other players. Although players cannot communicate during the learning
process, they are aware of the possible actions other players can take and can agree on a strategy beforehand.
Related Works The single-player contextual bandit with linear payoff functions is a well-studied problem
withefficientalgorithmicsolutionsAgarwaletal.(2014);AgrawalandGoyal(2013). Therearemanyvariants
to the single-player linear contextual bandit setting such as Agrawal and Devanur (2016) and Badanidiyuru
et al. (2014) which consider bandits with constraints on resource allocations. Furthermore, Bouneffouf
et al. (2017) studies the problem with restricted context vectors, Allesiardo et al. (2014) analyzes contextual
bandits that do not need a hypothesis on stationary properties of contexts and rewards.
Linear contextual bandits have numerous real-world applications, encompassing healthcare, recommender
systems, information retrieval, and risk management. For example, Durand et al. (2018) employs the contex-
tual bandit framework to adaptively treat mice in the early stages of cancer. Li et al. (2010) and Bouneffouf
et al. (2012) leverage contextual information to enhance mobile and news article recommendation systems.
Bouneffouf et al. (2013) applies contextual bandits to optimize context-based information retrieval. Further-
more, Soemers et al. (2018) utilizes contextual bandits to adaptively distinguish between fraud and concept
drifts in credit card transactions. Within machine learning, Laroche and Féraud (2017) employs contextual
bandits for algorithmic selection in off-policy reinforcement learning, while Bouneffouf et al. (2014) integrates
them to improve active learning.
We will now overview the literature on multiplayer bandits. Within the domain of multiplayer stochastic
bandits, numerous studies permit restricted communication, as observed in prior research such as Martínez-
Rubio et al. (2018; 2019); Szorenyi et al. (2013); Karpov et al. (2020); Tao et al. (2019). Recent studies,
building upon the foundations laid by Chang et al. (2021), have delved into investigations of information
asymmetry in the context of multiplayer bandits, as explored in works such as Mao et al. (2022); Kao et al.
(2022); Mao et al. (2021); Kao (2022).
Our Contribution This is the first paper on multiplayer contextual bandits. We propose a multiplayer
information asymmetric environment that was originally from the multi-armed bandit setting Chang et al.
(2021); Chang and Lu (2023) and apply it to contextual bandits. We then propose two algorithms that are
based on the single agent linear contextual bandit setting in Chu et al. (2011) called LinUCB. Remarkably,
we show that by modifying LinUCB slightly, we obtain an algorithm that is able to take on both forms
of information asymmetry. More specifically, through a coordination scheme, we are able to recover the
same regret bound O(√
T)as in the single-agent setting when the players receive the same reward but can’t
observe the other player’s actions (Problem A). On the other hand, when the players receive their own IID
reward but can observe the other player’s actions (Problem B), we obtain the first sublinear regret bound
ofO(√
T). Finally, when there are both types of information asymmetry (Problem C), we propose a new
1θis global and independent of the actions. Moreover, θis inherent to the contextual environment and does not change in
between rounds.
2Under review as submission to TMLR
algorithm that involves principles in the classical Explore and then commit algorithm that achieves the same
order regret bound.
2 Preliminary
We consider information asymmetric contextual bandits, which is a generalization of the single player setting
giveninChuetal.(2011). Inparticular, theyproposeaUCB-indexbasedalgorithmLinUCB,andwepropose
a multiplayer version of this algorithm.
Inparticular, wesupposethereare mplayers, andeachplayer icanpickfromasetofarms Ai. Forsimplicity,
we can assume that |Ai|=Kalthough the case where each player has a different number of arms is easy.
At every round t, each player will pick an arm at the same time and without communicating from their
arm set. This gives rise to a jointarm (which can be represented as a vector of actions from each player)
A:=A1×···×Amwhich can be denoted at Atwhich produces a stochastic reward rt,At. We will use bold
to denote any quantity that is a vector. Given a joint action awe define the term corresponding action for
playerito be theith component in the vector a. We shall use Tto denote the total number of rounds in
the learning process. The collective goal of all the players is to maximize the total expected rewards up to
horizonT.
Furthermore, at the start of each round, every player is given the same Kmcontextvectorsxt,a∈Rd
corresponding to each joint arm a∈A. LetB(0,1)be thed-dimensional unit ball centered at the origin.
Suppose that each contextual vector xt,a∈B(0,1)so that∥xt,a∥ℓ2≤Lunder theℓ2norm. The reward
that is produced from pulling joint arm asatisfies the linear realizability assumption, that is,
E[rt,a|xt,a] =⟨xt,a,θ∗⟩ (1)
for someθ∗∈Rd. This means that in order to determine which arms have the best context. One must have
an accurate estimate of θ∗.
Letatbe the joint arm that is selected at round t. Furthermore, let a∗
tbe the best arm at round t. That is
a∗
t= arg maxa∈A⟨xa,θ∗⟩. To understand the success of a policy, we shall use the notion of regret RTup to
horizonT, defined as
RT=T/summationdisplay
t=1⟨x∗
t,θ∗⟩−⟨xt,at,θ∗⟩=⟨x∗
t−xt,at,θ∗⟩ (2)
In Chu et al. (2011), they were able to prove that LinUCB attains O(√
T)regret, which matches the lower
bound for this problem. We now state the information asymmetric problems we will be studying taken from
Chang et al. (2022); Chang and Lu (2023). They are as follows (recall that all players receive the same
contexts for all the joint actions each round).
Problem A: Information asymmetry in actions. At every round, after a joint action ais taken, all the
players receive the rewards, however, they cannot observe the actions of the other players.
Problem B: Information asymmetry in rewards. At every round, after a joint action ais taken, all the
players can observe the other player’s actions. However, they each receive their own i.i.d. copy of the reward.
To be precise, since each player obtains a different reward, we should use Ri
Tto be the regret for player i.
However as the distributions of the rewards have the same mean, and regret is defined under expectation, it
follows that even in this setting each player experiences the same regret.
Problem C: Information asymmetry in both actions and rewards. This combines the challenges in problem
A and problem B where every round the players get their own i.i.d. reward (without seeing other player’s
reward)andthey cannot observe the actions taken by other players.
2.1 Challenges in the contextual bandit setting
In this section we compare our work to that given in Chang et al. (2022). In their paper, they study
the information asymmetry bandit problem for the classical multi-armed bandit setting. For problem A,
3Under review as submission to TMLR
information asymmetry in only actions, all the players receive the same reward feedback but are unable to
communicate as well as observe the other player’s actions at each round. However, because they receive the
same reward feedback, if they are to correctly infer the other player’s actions then they are able to maintain
the same UCB estimates of all the arms. Similarly, in the contextual bandit setting, they observe the same
rewards as well as the same contexts. Thus, they are able to maintain the same estimate for θ∗as well as the
same confidence set. The novelty is constructing a way to break ties when two arms have the same LinUCB
index so that each player can accurately infer the correct action that is taken at the time step despite not
being able to observe the actions of the other players. This is where definition 1 plays a role in algorithm 1.
On the other hand, problem B, which is information asymmetry in rewards is a bit more challenging. Since
each player is observes only their own IID copy of their reward, they will maintain different estimates of
θ∗(and therefore have different confidence sets for this parameter as well). In the bandit’s case studied in
Chang and Lu (2023), this issue was addressed using an UCB-interval algorithm, where initially all the arms
were pulled in a predefined order. Each player maintains for each arm their own UCB-interval and when
two UCB-intervals are disjoint. However, such an elimination method no longer applies to the contextual
bandit case because at every round the distribution of each arm changes in accordance to the context it
receives. However, this problem is easier than the bandits’ case in the sense that we only have to estimate
one parameter θ∗, whereas in the bandits case, we had to estimate the mean of each arm.
3 Main Results
3.1 LinUCB
We describe how LinUCB works from Chu et al. (2011) using the multiagent environment. In particular,
this algorithm maintains an estimate of θ∗by solving the following least squares estimator (for player i).
θi
t= arg min
θ∈Rd/parenleftiggT/summationdisplay
t=1⟨ri
t,a−θ,xt,at)2+λ∥θ∥2
ℓ2/parenrightigg
(3)
which has solution
θi
t=V−1
tT/summationdisplay
t=1xt,art,a (4)
whereVtared×dmatrices
V0=λIandVt=V0+T/summationdisplay
t=1xt,atx⊤
t,at(5)
Thisθi
tgives an estimate of θ∗in the contextual bandit setting.
For the estimate of θ∗, we construct a confidence interval Ct(θ)which is the set of vectors in Rdthat are at
most a certain distance away from θunder the norm∥v∥2
Vt−1=v⊤Vt−1v. More explicitly our confidence set
is,
Ct(θ) ={v∈Rd:∥v−θ∥2
Vt−1≤βT} (6)
For each arm, each player ican construct an Upper confidence bound by solving the following optimization
problem
max
θ∈Ct(θi
t)⟨θ,xt,at⟩ (7)
This optimization problem has the solution
⟨at,a,θi
t⟩+/radicalbig
βt∥xt,a∥V−1
t−1(8)
and each player will pick the arm with the highest index.
4Under review as submission to TMLR
In the classical case, βtcan be chosen as
/radicalbig
βn=√
λm2+/radicaligg
2 log/parenleftbigg1
δ/parenrightbigg
+dlog/parenleftbiggdλ+nL2
dλ/parenrightbigg
(9)
whereλis used to initialize V0and is in this setting can be any positive number.
In the following subsections, we will generalize the LinUCB algorithm from Chu et al. (2011) to account for
the information asymmetries namely action asymmetry (Problem A) and reward asymmetry (Problem B),
and state their regret bounds.
3.2 Asymmetry in arms
Algorithm 1 mLinUCB-A for asymmetry in actions
1:Input:α>0,K,m,d∈N
2:Vt←Id,
3:bt←0d
4:fort= 1,2,3,....,Tdo
5:θt←V−1b
6:ObserveKmarm contexts xt,afor each joint arm a∈A.
7:foreach joint arm a∈Ado
8:pt,a←θ⊤
txt,a+α/radicalig
x⊤
t,aV−1xt,a
9:end for
10:All players take their corresponding action for at∈arg maxapt,a, where joint action atis chosen so
that it’s smallest by Definition 1.2
11:Observe reward rt∈{0,1}
12:UpdateV←V+xt,atx⊤
t,at.
13:Updateb←b+xt,atrt.
14:end for
In this section, we generalize the LinUCB algorithm action asymmetry (Problem A) and call it LinUCB-A .
This is the setting where each player receives the same reward but is unable to observe the other player’s
actions at every round. Since the feedback from all the players is the same, the only challenge comes in
inferring the other player’s actions. In particular, when two joint actions have the same UCB index, there
needs to be a way to break ties. Therefore, we define the following ordering on the joint arm space.
Definition 1. Number the players 1,...,mand theKindividual actions, and consider each set of joint
actionaas anmdigit number with each digit corresponding to the joint action. Call this base Knumber
Na. For joint action a,b∈A, we say that a<bifNa<Nb.
This is similar to what is done in Chang et al. (2022). The idea is that even though the players cannot
observe, the other player’s actions, because they obtain the same feedback, they can infer what the other
players are doing as long as they have a way to break ties should two joint actions have the same index.
Because of this coordination, the players are behaving as if they were single agent in a larger joint action
space. From this we can deduce the following regret bound.
Theorem 2. In the action asymmetric (Problem A) contextual bandit setting where the context vectors, the
frequentist regret bound of Algorithm 1 is
RT=Cd√
Tlog(TL) (10)
Proof.See Corollary 19.3 of Lattimore and Szepesvári (2020).
5Under review as submission to TMLR
We note that this bound truly reduces to the single agent setting case as it doesn’t even grow with the
number of arms. This is because the success of the algorithm only depends on the accuracy in the estimate
ofθ∗. In comparison, in the multiarmed bandit problem, the regret grows with action space because every
arm needs to be estimated.
3.3 Asymmetry in rewards
In this section, we generalize the LinUCB algorithm reward asymmetry (Problem B) and call it LinUCB-B .
This is the setting where each player receives an i.i.d copy of the reward but is able to observe the other
player’s actions at every round. This algorithm is similar as mLinUCB-A but taking into account that the
reward feedback is different for different players.
Algorithm 2 mLinUCB-B for asymmetry in rewards
1:Input:α>0,K,m,d∈Nat←λId, whereλ=T1
2bi
t←0d
2:fort= 1,2,3,....,Tdo
3:Each player iupdatesθi
t←V−1bi
4:Each player Observe Kmarm contexts xt,afor each joint arm a∈A.
5:foreach joint arm a∈Ado
6:Each player iupdatespi
t,a←(θi
t)⊤xt,a+α/radicalig
x⊤
t,aV−1xt,a
7:end for
8:Each player ichooses their corresponding action for their observed at= arg maxapi
t,a.
9:Each player observe the other player’s actions.
10:Each players observes an I.I.D. reward ri
t∈{0,1}
11:Each player updates V=V+xt,atx⊤
t,at
12:Each player iupdatesbi←bi+xt,atri
t
13:end for
The central idea is to modify λand√βtso that each player’s confidence set is small enough so that for some
distribution of context vectors there is a very high probability that all the players agree on the optimal arm
for each particular round. In doing so, we allow the players to implicitly coordinate their actions without
any need for the players to communicate during the learning process. More specifically we set,
/radicalbig
βT=O/parenleftig
Tc/2∥θ∗∥2/parenrightig
(11)
withc=1
2and our initialization for V0=λIis
λ=Tc. (12)
Compare this to equation equation 9. In particular, the ratio of βT/λis much smaller for this setting than
it is for the setting in problem A. This is becauseβT
λ(as we show in Lemma 7) is the lower bound for the
radius of the confidence interval for our estimate of θ∗, and we need these to be sufficiently small in order
for the aforementioned coordination to occur.
We shall show that remarkably, even using the same algorithm as Algorithm 1 (with just modifying λ=√
T)
we can obtain a regret bound that is still sublinear. Note in this algorithm that the rewards ri
tat timet
are indexed by i, since each player observes their own copy of an IID reward, without seeing the other
players’ copy. Therefore each player has their own estimate of bias well. This also causes their estimate of
the parameter θ∗to be different from each other, resulting in different confidence sets for θ∗as well. The
explicit algorithm is stated in Algorithm 2, where the quantities that are now different for each player have
a superscript iattached to them. For this algorithm to work we have to assume that the context vectors are
generated by some fixed (but unknown) distribution in the unit ball of radius L.
To see that it is impossible to obtain sublinear regret using adversarial contexts consider the 2player
environment, and suppose each player has two arms {1,2}. Then we consider the following 2×2matrix
6Under review as submission to TMLR
where the row labels are the actions of one player and the column labels are the actions for the other player.
Furthermore each entry corresponds to a context vector for that joint action.
/bracketleftbigg1 2
1vt0
20v′
t/bracketrightbigg
where either vis the best contex vector, and v∼v′in that⟨v,θ∗⟩and⟨v′,θ∗⟩are really close to each
other. When they are sufficiently close, since the players have IID rewards, their estimates θi
twill also be
slightly different. If two players disagree on which context vector is the best, they will obtain 0reward.
For the appropriate context vectors, this happens with constant probability, and thus we obtain constant
regret. Note that we refrained from setting v=v′because when two context vectors are the same the
players can still coordinate by ordering the arms as in Definition 1 which was done while studying Problem
A. Furthermore, this is not an issue that shows up in the single-agent setting because even if the player is
unable to decide which of vorv′is better, it doesn’t matter because pulling either incurs little regret. In
the multiplayer settings, the issues show up when two players disagree on which arm to select for many of
the erounds.
Therefore, let ψ(x)be a Lebesgue integrable probability distribution density of this ball that contains the
context vectors and suppose that ∥ψ∥L∞<∞. Note that it does not need to be continuous. Letting µbe
the Lebesgue measure over (Rd,M)(withMis theσ-algebra of Borel sets), it follows that for any subset
U, we have
Pµ(x∈U) =/integraldisplay
Uψ(x)dµ(x)<∥ψ∥L∞µ(U)
It follows that as µ(U)→0, we have Pµ(x∈U)→0as well. This also means that as the players refine their
estimate of θ∗, the chances that the players will disagree on which arm to pull will decrease in probability.
This intuition is formalized in Lemma 7 and Lemma 8.
We can now state the regret bound of Algorithm 2 under reward asymmetry (Problem B).
Theorem 3. In the reward asymmetric (Problem B) contextual bandit setting where the context vectors are
distributed with fixed distribution the frequentist regret bound of Algorithm 2 is,
RT=O(mK2mLd√
Tlog(T)) (13)
The proof of this is given in the supplementary materials.
Note that this result depends on the number of actions. That’s because in order for the players to be
coordinated the context vectors of the joint action to have be sufficiently far. This is formalized in Lemma 7
3.4 Asymmetry in Both Rewards and Actions
In the previous section, we showed that LinUCB does well even when the rewards are IID (problem B). This
is because in this setting the players are still able to observe the other player’s actions and therefore they
can make the correct updates. However, in this setting, as they cannot observe the other player’s actions,
we cannot guarantee each player will attempt to pull the same joint arm. particular, in the beginning of the
learning process, when the estimate of θisn’t very accurate for any player, this increases the probability of
mis-coordination.
We circumvent this by giving an exploration sequence of time Tαwhere it will be shown in the proof that
α=1
2is optimal. During the exploration sequence all the players will pull arm 1(or any other fixed arm(s))
as long as they agree on which ones to pull at each round. In this time they will update their Vtandbi
parameters. After the exploration phase they will run regular Lin UCB, but they will not update their Vtand
bivalues.The idea is that after sufficient exploration they will each have (different) but accurate estimates
ofθ. Since the context vectors are generated at random (rather than adversarial), there is a high probability
that they will be able to successfully coordinate pulling the best action at every round.
7Under review as submission to TMLR
Algorithm 3 ETCfor asymmetry in rewards and actions
1:Input:βT>0,K,m,d∈N, exploration parameter Tα.
2:at←λId, withλ=Tαwhereα=1
2
3:bi
t←0d
4:fort= 1,2,3,....,Tαdo
5:All players will pull the corresponding arm to the joint action 1.
6:UpdateVt+1←Vt+xt,atx⊤
t,at
7:Updatebi←bi+xt,atri
t
8:end for
9:θi
t←V−1bi
10:fort=Tα+ 1,...,Tdo
11:ObserveKmarm contexts xt,afor each joint arm a∈A.
12:foreach joint arm a∈Ado
13:pi
t,a←(θi
t)⊤xt,a+√βT/radicalig
x⊤
t,aV−1xt,a
14:end for
15:Each player chooses their corresponding action for their observed at= arg maxapt,a.
16:end for
Similar to Algorithm 2, our choice of λ=√
Tis important. By selecting a large enough λwe ensure that the
confidence ball for θis sufficiently small. However, we cannot choose λtoo big, or else our our confidence
ball forθwill not contain θwith sufficiently high probability.
We have the following regret bound for this algorithm
Theorem 4. In the reward and action asymmetric (problem C) contextual bandit setting where the context
vectors are distributed with fixed distribution the frequentist regret bound of the algorithm is
RT=O(mK2mLd√
Tlog(T)) (14)
The proof of this theorem is given in the supplementary materials.
Similar to the regret bound of Algorithm 2 provided by Theorem 3, this depends on the number of actions
due to the fact that every round the players are miscoordinated (i.e. when the context vectors are too close
to each other), we incur linear regret.
4 Experiments
In this section, we execute simulations to corroborate the empirical efficacy of the proposed algorithms in
this paper. In Figure 1, we plot the regret versus time for both algorithms LinUCB-A andLinUCB-B . It should
be emphasized these algorithms assume different types of asymmetry: LinUCB-A assumes action asymmetry
while LinUCB-B assumes reward asymmetry.
4.1 Experiment Details
We conduct the simulations using θand context vectors xuniformly sampled from the unit cube [0,1√
d].
This parametrization ensures that ∥θ∥ℓ2and∥x∥ℓ2, measured using the ℓ2norm, does not exceed L=√
d, in
line with the constraints of our problem setting. Furthermore, it’s clear this uniform distribution is bounded
over our space for x. Each reward is set to be Gaussian, and the standard deviation of them is randomly
uniformly pre-selected to be from the range [0,1]. For each environment, the simulations were executed over
T= 10,000rounds. We repeat these simulations 5times to compute the median regret and report the 95%
confidence interval. The hyperparameter βTis set to√
Tfor all algorithms analyzed.
8Under review as submission to TMLR
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053 /uni00000014/uni00000048/uni00000016/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000015/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000013/uni00000011/uni00000014/uni00000015/uni00000018/uni00000026/uni00000058/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057/uni00000014/uni00000048/uni00000016
/uni00000047/uni00000003/uni00000020/uni00000003/uni00000018/uni00000030/uni00000020/uni00000015/uni00000003/uni0000002e/uni00000020/uni00000015
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000024
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000025/uni00000042/uni00000028/uni00000037/uni00000026
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000025
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000026/uni00000042/uni00000028/uni00000037/uni00000026
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053 /uni00000014/uni00000048/uni00000016/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000011/uni00000014/uni00000018/uni00000013/uni00000011/uni00000015/uni00000013/uni00000014/uni00000048/uni00000016
 /uni00000030/uni00000020/uni00000016/uni00000003/uni0000002e/uni00000020/uni00000016
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000024
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000025/uni00000042/uni00000028/uni00000037/uni00000026
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000025
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000026/uni00000042/uni00000028/uni00000037/uni00000026
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053 /uni00000014/uni00000048/uni00000016/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013/uni00000019/uni00000013/uni00000011/uni00000013/uni0000001b/uni00000013/uni00000011/uni00000014/uni00000013/uni00000014/uni00000048/uni00000016
 /uni00000030/uni00000020/uni00000017/uni00000003/uni0000002e/uni00000020/uni00000017
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000024
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000025/uni00000042/uni00000028/uni00000037/uni00000026
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000025
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000026/uni00000042/uni00000028/uni00000037/uni00000026
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053 /uni00000014/uni00000048/uni00000016/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000026/uni00000058/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057/uni00000014/uni00000048/uni00000016
/uni00000047/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000030/uni00000020/uni00000015/uni00000003/uni0000002e/uni00000020/uni00000015
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000024
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000025/uni00000042/uni00000028/uni00000037/uni00000026
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000025
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000026/uni00000042/uni00000028/uni00000037/uni00000026
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053 /uni00000014/uni00000048/uni00000016/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000011/uni00000014/uni00000018/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000014/uni00000048/uni00000016
 /uni00000030/uni00000020/uni00000016/uni00000003/uni0000002e/uni00000020/uni00000016
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000024
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000025/uni00000042/uni00000028/uni00000037/uni00000026
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000025
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000026/uni00000042/uni00000028/uni00000037/uni00000026
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013
/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053 /uni00000014/uni00000048/uni00000016/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000015/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000013/uni00000011/uni00000014/uni00000015/uni00000018/uni00000014/uni00000048/uni00000016
 /uni00000030/uni00000020/uni00000017/uni00000003/uni0000002e/uni00000020/uni00000017
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000024
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000025/uni00000042/uni00000028/uni00000037/uni00000026
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000025
/uni0000002f/uni0000004c/uni00000051/uni00000038/uni00000026/uni00000025/uni00000010/uni00000026/uni00000042/uni00000028/uni00000037/uni00000026
Figure 1: Regret plots comparing different algorithms to different information asymmetry. Red is the regret
ofLinUCB-A on Problem A (asymmetry in actions). Brown is the regret for ETCon Problem B (asymmetry
in rewards). Green is the regret plot for LinUCB-B on Problem B. Finally black is the regret for ETCon
Problem C (asymmeetry in both rewards and actions).
In the proceeding section, we perform the experiment on environments with mandKequal to 2,3,4
respectively, with d= 5,10. Moreover, we use LinUCB-B_ETC to denote the ETCalgorithm run on problem
B. Similarly, LinUCB-C_ETC is used to denote the ETCalgorithm run on problem C.3
4.2 Analysis
We note that for most of these environments LinUCB-A and LinUCB-B perform comparably well to each
other. This is because while LinUCB-A has the more favorable feedback, LinUCB-B has a larger λparameter
which encourages less exploration. In the analysis, this affects the probability of the "good event" that the θ
will stay within the confidence ball. However, in our simulations, due to the small environment, it’s unlikely
that the "bad" events. Therefore, in this case, it’s more favorable to do less exploration.
Wenotethat ETCappearstobepiecewiselinear. Inparticular, thefirstpiecewhichonlyoccursfor√
Trounds
is steeper as this is the exploration phase. In the second piece, the algorithm takes the parameters taken
from the periods of exploration and then runs LinUCBwithout updating these parameters. Philosophically,
the slope of the regret curve reflects an algorithms learning. ecause the parameters don’t update, ETCdoes
not perform better as the rounds continue (which is different than the standard LinUCB, the slope of the
regret curve remains constant. Despite being piecewise linear, however, asymptotically the regret will still
grow in the same order as LinUCB-B .
In comparing ETCandLinUCB-B on the asymmetry in the rewards environment (Problem B), we note that,
LinUCB-B performssuperior. However, ETCismorerobustasitachievesaroundthesamelevelofperformance
in both Problem B And Problem C settings. This makes sense because ETCis a fully coordinated algorithm
so it does not need to relay on observing actions to achieve its performance.
5 Conclusions and Future Work
In this paper, we adapted LinUCB from Chu et al. (2011) to the multiagent setting with different types of
information asymmetry. multiplayer information asymmetry contextual bandits setting. Namely, we studied
3All the source code that has been used to generate the results presented in this paper can be found via http://tinyurl.
com/yty68wcp .
9Under review as submission to TMLR
action asymmetry (Problem A) where each player receives the same reward but cannot observe other player’s
actions. Using a coordination scheme we were able to reduce this to the single agent setting and obtain an
O(√
T)regret bound. On the other hand, we also studied reward asymmetry (Problem B) where each player
receives an iid copy of the reward but can observe the other player’s actions. In this setting, we can prove
that if the context vectors are distributed with a fixed distribution (rather than adversarial), then we obtain
aO(T√
T)regret bound. We were able to achieve this using the same algorithm as that in Problem A but
modifyingλto be√
T. Both of these regret bounds are the first for this setting. For asymmetry in both
(Problem C), we proposed a fully coordinated ETCalgorithm which did exploration for the first√
Trounds,
and then ran LinUCBfor the remaining time, which achieved the same regret as the multiplayer LinUCB.
Finally, we corroborated our results with some simulations.
References
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. 2014. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine
Learning . PMLR, 1638–1646.
Shipra Agrawal and Nikhil Devanur. 2016. Linear contextual bandits with knapsacks. Advances in Neural
Information Processing Systems 29 (2016).
Shipra Agrawal and Navin Goyal. 2013. Thompson sampling for contextual bandits with linear payoffs. In
International conference on machine learning . PMLR, 127–135.
Robin Allesiardo, Raphaël Féraud, and Djallel Bouneffouf. 2014. A neural networks committee for the
contextual bandit problem. In Neural Information Processing: 21st International Conference, ICONIP
2014, Kuching, Malaysia, November 3-6, 2014. Proceedings, Part I 21 . Springer, 374–381.
Ashwinkumar Badanidiyuru, John Langford, and Aleksandrs Slivkins. 2014. Resourceful contextual bandits.
InConference on Learning Theory . PMLR, 1109–1134.
Djallel Bouneffouf, Amel Bouzeghoub, and Alda Lopes Gançarski. 2012. A contextual-bandit algorithm
for mobile context-aware recommender system. In Neural Information Processing: 19th International
Conference, ICONIP 2012, Doha, Qatar, November 12-15, 2012, Proceedings, Part III 19 . Springer, 324–
331.
Djallel Bouneffouf, Amel Bouzeghoub, and Alda Lopes Gançarski. 2013. Contextual bandits for context-
based information retrieval. In Neural Information Processing: 20th International Conference, ICONIP
2013, Daegu, Korea, November 3-7, 2013. Proceedings, Part II 20 . Springer, 35–42.
Djallel Bouneffouf, Romain Laroche, Tanguy Urvoy, Raphael Féraud, and Robin Allesiardo. 2014. Con-
textual bandit for active learning: Active thompson sampling. In Neural Information Processing: 21st
International Conference, ICONIP 2014, Kuching, Malaysia, November 3-6, 2014. Proceedings, Part I 21 .
Springer, 405–412.
Djallel Bouneffouf, Irina Rish, Guillermo A Cecchi, and Raphaël Féraud. 2017. Context attentive bandits:
Contextual bandit with restricted context. arXiv preprint arXiv:1705.03821 (2017).
Simina Brânzei and Yuval Peres. 2021. Multiplayer bandit learning, from competition to cooperation. In
Conference on Learning Theory . PMLR, 679–723.
William Chang, Mehdi Jafarnia-Jahromi, and Rahul Jain. 2021. Online learning for cooperative multi-player
multi-armed bandits. arXiv preprint arXiv:2109.03818 (2021).
William Chang, Mehdi Jafarnia-Jahromi, and Rahul Jain. 2022. Online learning for cooperative multi-player
multi-armed bandits. In 2022 IEEE 61st Conference on Decision and Control (CDC) . IEEE, 7248–7253.
William Chang and Terry Lu. 2023. Optimal Cooperative Multiplayer Learning Bandits with No Commu-
nication. In arxiv preprint .
10Under review as submission to TMLR
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. 2011. Contextual bandits with linear payoff functions.
InProceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics . JMLR
Workshop and Conference Proceedings, 208–214.
Audrey Durand, Charis Achilleos, Demetris Iacovides, Katerina Strati, Georgios D Mitsis, and Joelle Pineau.
2018. Contextual bandits for adapting treatment in a mouse model of de novo carcinogenesis. In Machine
learning for healthcare conference . PMLR, 67–82.
Hsu Kao. 2022. Efficient Methods for Optimizing Decentralized Multi-Agent Systems . Ph.D. Dissertation.
HsuKao, Chen-YuWei, andVijaySubramanian.2022. Decentralizedcooperativereinforcementlearningwith
hierarchical information structure. In International Conference on Algorithmic Learning Theory . PMLR,
573–605.
Nikolai Karpov, Qin Zhang, and Yuan Zhou. 2020. Collaborative top distribution identifications with limited
interaction. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS) . IEEE,
160–171.
Tze Leung Lai and Herbert Robbins. 1985. Asymptotically efficient adaptive allocation rules. Advances in
applied mathematics 6, 1 (1985), 4–22.
Romain Laroche and Raphaël Féraud. 2017. Algorithm selection of off-policy reinforcement learning algo-
rithm.arXiv preprint arXiv:1701.08810 (2017).
Tor Lattimore and Csaba Szepesvári. 2020. Bandit algorithms . Cambridge University Press.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-bandit approach to per-
sonalized news article recommendation. In Proceedings of the 19th international conference on World wide
web. 661–670.
Weichao Mao, Tamer Basar, Lin F Yang, and Kaiqing Zhang. 2021. Decentralized Cooperative Multi-Agent
Reinforcement Learning with Exploration. arXiv preprint arXiv:2110.05707 (2021).
Weichao Mao, Lin Yang, Kaiqing Zhang, and Tamer Basar. 2022. On improving model-free algorithms
for decentralized multi-agent reinforcement learning. In International Conference on Machine Learning .
PMLR, 15007–15049.
David Martínez-Rubio, Varun Kanade, and Patrick Rebeschini. 2018. Decentralized Cooperative Stochastic
Bandits. arXiv preprint arXiv:1810.04468 (2018).
David Martínez-Rubio, Varun Kanade, and Patrick Rebeschini. 2019. Decentralized cooperative stochastic
bandits. (2019).
Aldo Pacchiano, Peter Bartlett, and Michael Jordan. 2023. An instance-dependent analysis for the coop-
erative multi-player multi-armed bandit. In International Conference on Algorithmic Learning Theory .
PMLR, 1166–1215.
Dennis Soemers, Tim Brys, Kurt Driessens, Mark Winands, and Ann Nowé. 2018. Adapting to concept drift
in credit card transaction data streams using contextual bandits and decision trees. In Proceedings of the
AAAI Conference on Artificial Intelligence , Vol. 32.
Balazs Szorenyi, Róbert Busa-Fekete, István Hegedus, Róbert Ormándi, Márk Jelasity, and Balázs Kégl.
2013. Gossip-based distributed stochastic bandit algorithms. In International Conference on Machine
Learning . PMLR, 19–27.
Chao Tao, Qin Zhang, and Yuan Zhou. 2019. Collaborative learning with limited interaction: Tight bounds
for distributed exploration in multi-armed bandits. In 2019 IEEE 60th Annual Symposium on Foundations
of Computer Science (FOCS) . IEEE, 126–146.
Po-An Wang, Alexandre Proutiere, Kaito Ariu, Yassir Jedra, and Alessio Russo. 2020. Optimal algorithms
for multiplayer multi-armed bandits. In International Conference on Artificial Intelligence and Statistics .
PMLR, 4120–4129.
11Under review as submission to TMLR
6 Supplementary Material
6.1 Concentration Lemmas
The following is taken from Theorem 20.5 of Lattimore and Szepesvári (2020). It gives us the size of the
ball that contains θwith high probability.
Lemma 5. Letδ∈(0,1). Then, with probability at least 1−δ, it holds that for all t∈N,
/vextenddouble/vextenddouble/vextenddoubleˆθt−θ∗/vextenddouble/vextenddouble/vextenddouble
Vt(λ)<√
λ∥θ∗∥2+/radicaligg
2 log/parenleftbigg1
δ/parenrightbigg
+ log/parenleftbiggdetVt(λ)
λd/parenrightbigg
.
Furthermore, if∥θ∗∥2≤L, then P(existst∈N+:θ∗/∈Ct)≤δwith
Ct=/braceleftigg
θ∈Rd:/vextenddouble/vextenddouble/vextenddoubleˆθt−1−θ/vextenddouble/vextenddouble/vextenddouble
Vt−1(λ)<L√
λ+/radicaligg
2 log/parenleftbigg1
δ/parenrightbigg
+ log/parenleftbiggdetVt−1(λ)
λd/parenrightbigg/bracerightigg
.
6.2 Proofs of Main Theorems
In this section, we prove that the algorithm in 2 satisfies the regret bound given in Theorem 3 Consider the
’good’ event Edefined as follows
E=T/intersectiondisplay
t=1m/intersectiondisplay
i=1/braceleftbig
θi
t∈Ct(θ∗)/bracerightbig
(15)
This event states that at every round t∈[T], every player i∈[m]has an empirical estimate of θ∗that is
within the confidence interval centered at θ∗. This ensures that all of the player’s estimates of θ∗are not
too far from each other. This also means that despite each player having a different empirical estimate of
θ∗, if the context vectors of each arm are not too close for most rounds, then the players will be able to
coordinate properly. This is formalized in lemma 7. to do that we first show that the eigenvalues of Vtare
nondecreasing
Lemma 6. For anyλ>0andβT, we have the following inequality for each players estimate for θi
tandθ∗
/vextenddouble/vextenddoubleθi
t−θ∗/vextenddouble/vextenddouble≤βT
λ
Proof.To prove this note that Ctis an ellipsoid where the inverse of the eigenvalues of Vt−1give the lengths
of the principle axes. We first note that based on the fact that V0=λI, and therefore C0is a circle with
radiusβT
λ. We will be done if we can show that Vthasnondecreasing eigenvalues. Let σk
1≥σk
2≥···≥σd
t
be the eigenvalues of Vt.
From the definition of Vt, it’s clear that Vtis symmetric. Thus, we can apply the Courant-Fischer min-max
Theorem to obtain
σk
t(A) = min{max{RVt(v)|v∈Uandv̸= 0}|dim(U) =k}
where the Rayleigh Quotient RVt(v)is defined as,
RVt(v) =⟨Vtv,v⟩
∥v∥2
12Under review as submission to TMLR
Therefore, we have
σk
t+1= min{max{RVt+1(x)|v∈U,v̸= 0}|dim(U) =k}
= min{max{RVt+xt,atx⊤
t,at(x)|v∈U,v̸= 0}|dim(U) =k}
= min/braceleftigg
max/braceleftigg
⟨(Vt+xt,atx⊤
t,at)v,v⟩
∥v∥2/vextendsingle/vextendsingle/vextendsingle/vextendsinglev∈U,v̸= 0/bracerightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingledim(U) =k/bracerightigg
= min/braceleftigg
max/braceleftigg
⟨Vtv,v⟩
∥v∥2+⟨xt,atx⊤
t,atv,v⟩
∥v∥2/vextendsingle/vextendsingle/vextendsingle/vextendsinglev∈U,v̸= 0/bracerightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingledim(U) =k/bracerightigg
≥min/braceleftigg
max/braceleftigg
⟨Vtv,v⟩
∥v∥2/vextendsingle/vextendsingle/vextendsingle/vextendsinglev∈U,v̸= 0/bracerightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingledim(U) =k/bracerightigg
=σk
t
whereintheinequalityweusedthefactthat ⟨xt,atx⊤
t,atv,v⟩= (xt,atx⊤
t,atv)⊤v=v⊤xt,atx⊤
t,atv=/vextenddouble/vextenddoublex⊤
t,atv/vextenddouble/vextenddouble≥
0.
Now we show that when all the players have their estimates inside the confidence ball around θ∗, then they
can fully coordinate.
Lemma 7. Supposeθi
t∈Ct(θ)is an empirical estimate of θ∗for players i. Then under the good event E,
ifxt,aandxt′,a′are context vectors such that
⟨θ∗,xt,a⟩−⟨θ∗,xt,a′⟩>2βTL
λ(16)
then⟨θi
t,xt,a⟩>⟨θi
t,xt,a′⟩
Proof.From the defintion of Ct(θ), we know that
⟨θi
t,xt,a′⟩=⟨θ∗,xt,a′⟩+⟨θi
t−θ∗,xt,a′⟩ (17)
≤⟨θ∗,xt,a′⟩+/vextenddouble/vextenddoubleθi
t−θ∗/vextenddouble/vextenddouble∥xt,a′∥ (18)
≤⟨θ∗,xt,a′⟩+/vextenddouble/vextenddoubleθi
t−θ∗/vextenddouble/vextenddoubleL (19)
Similarly,
⟨θi
t,xt,a⟩=⟨θ∗,xt,a⟩+⟨θi
t−θ∗,xt,a⟩ (20)
≥⟨θ∗,xt,a⟩−/vextenddouble/vextenddoubleθi
t−θ∗/vextenddouble/vextenddouble∥xt,a∥ (21)
≥⟨θ∗,xt,a⟩−/vextenddouble/vextenddoubleθi
t−θ∗/vextenddouble/vextenddoubleL (22)
Therefore combining the two inequalities above yields
⟨θi
t,xt,a⟩−⟨θi
t,xt,a′⟩≥⟨θ∗,xt,a⟩−/vextenddouble/vextenddoubleθi
t−θ∗/vextenddouble/vextenddoubleL
−/parenleftbig
⟨θ∗,xt,a′⟩+/vextenddouble/vextenddoubleθi
t−θ∗/vextenddouble/vextenddoubleL/parenrightbig
≥⟨θ∗,xt,a⟩−⟨θ∗,xt,a′⟩
−2/vextenddouble/vextenddoubleθi
t−θ∗/vextenddouble/vextenddoubleL
FormLemma6,/vextenddouble/vextenddoubleθi
t−θ∗/vextenddouble/vextenddouble≤βT
λ, thenequationequation16willshow ⟨θ∗,xt,a⟩−⟨θ∗,xt,a′⟩−2/vextenddouble/vextenddoubleθi
t−θ∗/vextenddouble/vextenddoubleL>
0and the desired result will follow.
This proves the desired result.
13Under review as submission to TMLR
Figure 2: The set of points xa2such that equation 23 is satisfied lies outside of the region bounded by
the blue and green hyperplanes determined by xt,at. The dotted vector is θ∗, and these hyperplanes are a
distance of 4βTL
λ∥θ∗∥apart.
The next result tells us that the probability that the context vectors satisfy the hypothesis in Lemma 7 is
lower bounded by some constant that will grow to 1as asT→∞. This will be used to define the good
eventGtthat will allow the players to agree on which arm they want to pull.
Lemma 8. At any given round t, if all the context vectors xt,aare generated at random with probability
density function ψ(x)<M, with∥xt,a∥≤L, Then letPtbe the probability for the following event at a round
t: Any two joint actions aanda′satisfies the following inequality
|⟨θ∗,xt,a⟩−⟨θ∗,xt,a′⟩|>2βTL
λ(23)
Then
Pt≥1−K2mc2M(c1L)dβT
λ(24)
for universal constants c1,c2∈R.
Proof.Aribtrailiry order the joint actions as a1,a2,..., and suppose xt,a1has been placed so that the given
conditions are satisfied. Now let’s bound the volume where the next context vector can be placed. In
particular, the set of points xt,a2such that it satisfies equation equation 23 satisfies
⟨θ∗,xt,a2⟩>⟨θ∗,xt,a1⟩+ 2βTL
λor (25)
⟨θ∗,xt,a2⟩<⟨θ∗,xt,a1⟩−2βTL
λ(26)
From the definition of the inner product, the set of xt,a2that satisfy the equation above lies outside of two
hyperplanes normal to θ∗and at a distance of 4βTL
λ∥θ∗∥apart. See Figure 2 for an example in d= 3. Call the
region between these two parallel hyperplanes contained within the sphere U. Then the volume of Ucan
be bounded by the volume of a cylinder whose base is an d−1dimensional sphere with radius L, and with
height 4βTL
λ∥θ∗∥. Thus the volume of each such region is upper bounded by
µ(U) =πd−1
2
Γ(d+1
2)Ld−1/parenleftbigg
4βTL
λ∥θ∗∥/parenrightbigg
=πd−1
2
Γ(d+1
2)Ld/parenleftbigg
4βT
λ∥θ∗∥/parenrightbigg
Thus the probability that xt,a2satisfies equation equation 23 is at least
1−/integraldisplay
Uψ(x)dx≥1−µ(U)M (27)
≥1−Mπd−1
2Ld/parenleftbigg
4βT
λ∥θ∗∥/parenrightbigg
(28)
≥1−c2M(c1L)dβT
λ(29)
14Under review as submission to TMLR
for some universal constants c1,c2. Repeating inductively, the probability that all Kmcontext vectors satisfy
equation 23 is at least
Km/productdisplay
k=1/parenleftbigg
1−kc2M(c1L)dβT
λ/parenrightbigg
≥/parenleftbigg
1−Kmc2M(c1L)dβT
λ/parenrightbiggKm
(30)
≥1−K2mc2M(c1L)dβT
λ(31)
where in the last inequality we used (1−x)n≥1−nxforx≥0.
Theorem 4 In the reward and action asymmetric (problem C) contextual bandit setting where the context
vectors are distributed with fixed distribution the frequentist regret bound of algorithm is
RT=O(mK2mLd√
Tlog(T)) (32)
Proof.Consider the ’good’ event at time tdefined as,
Gt=/intersectiondisplay
a,a′∈A/braceleftbigg
|⟨θ∗,xt,a⟩−⟨θ∗,xt,a′⟩|>2βT
L/bracerightbigg
and let
G=T/intersectiondisplay
t=1Gt
This is the event that at round T, the context vectors for any two joint actions aanda′are not too close
in the sense that their inner product with θ∗is sufficiently far.
We suppose there are Tαrounds of exploration for some α∈(0,1)and then optimize over α. We can
decompose the regret as follows:
RT=E/bracketleftiggT/summationdisplay
t=1⟨θ,xat−x∗⟩/bracketrightigg
(33)
≤E/bracketleftiggTα/summationdisplay
t=1⟨θ,xat−x∗⟩+T/summationdisplay
t=Tα⟨θ,xat−x∗⟩/bracketrightigg
(34)
≤O(Tα) +E/bracketleftiggT/summationdisplay
t=Tα⟨θ,xat−x∗⟩/bracketrightigg
(35)
=O(Tα) +E/bracketleftiggT/summationdisplay
t=Tα⟨θ,xat−x∗⟩(I[Gt∩E] +I[(Gt∩E)c]/bracketrightigg
(36)
=O(Tα) +E/bracketleftiggT/summationdisplay
t=Tα⟨θ,xat−x∗⟩(I[Gt∩E] +I[(Gt∩E)c]/bracketrightigg
(37)
=O(Tα) +E/bracketleftiggT/summationdisplay
t=Tα⟨θ,xat−x∗⟩I[Gt∩E]/bracketrightigg
+T/summationdisplay
t=TαP(Gt∩E)c(38)
≤O(Tα) +E/bracketleftiggT/summationdisplay
t=Tα⟨θ,xat−x∗⟩I[Gt∩E]/bracketrightigg
+T/summationdisplay
t=Tα[P(Gc
t) +P(Ec)] (39)
AfterTαrounds of exploration, we have λ=Tα. Furthermore, as in Lemma 5, the probability that for all
playersi,∈[M]their estimator is within the confidence interval (determined by βT)is at least 1−δ. Thus
15Under review as submission to TMLR
the probability that everyone’s estimator is within this confidence interval is 1−mδ. Pickingδ=1
Tthis
gives
P(Ec)≤mδ=m
T
Using our choices of δandλ=Tα, we have
βT= 2 log (T) + log/parenleftbiggdet (VT(λ))
λd/parenrightbigg
= 2 log (T) + log/parenleftbiggdλ+TL2
Tαd/parenrightbigg
So that we can use lemma 8 to upper bound the probability of the complement of the good event happening
in Theorem 3, we have
P(Gc
t)≤K2mc2M(c1L)dβT
λ=K2mc2M(c1L)d/bracketleftig
2 log (T) + log/parenleftig
det(VT(λ))
Tαd/parenrightig/bracketrightig
Tα=O/parenleftbigg
K2mLdlog(T)
Tα/parenrightbigg
(40)
According to Lemma 7, under the event I[Gt∩E]the players are completely coordinated. This means that
we are reduced to a single agent setting with a Kmsize action space. However, the bound for the single agent
LinUCB regret bound doesn’t depend on the size of the action space so we do not expect the exponentially
larger action space to affect the regret. The regret for this can be bounded as follows.
Letrtbe the instantaneous regret in round t(under the good event Gt∩Edefined by,
rt=/angbracketleftbig
θ∗,xa∗
t−xat/angbracketrightbig
.
wherea∗
tis the optimal arm for round tbased on the context vectors received. Let ˜θt∈Ctbe the parameter
in the confidence set for which/angbracketleftbig˜θt,at/angbracketrightbig
= UCBt(at). Then, using the fact that θ∗∈Ctand the definition of
the algorithm leads to/angbracketleftbig
θ∗,xa∗
t/angbracketrightbig
≤UCBt/parenleftbig
xa∗
t/parenrightbig
≤UCBt(xat) =/angbracketleftbig˜θt,xat/angbracketrightbig
.
Using Cauchy-Schwarz inequality and the assumption that θ∗∈Ctand facts that ˜θt∈CtandCt⊆Etleads
to
rt=/angbracketleftbig
θ∗,xa∗
t−xat/angbracketrightbig
≤/angbracketleftbig˜θt−θ∗,xat/angbracketrightbig
≤∥xat∥V−1
T/vextenddouble/vextenddouble˜θt−θ∗/vextenddouble/vextenddouble
VT(41)
≤2∥xat∥V−1
T/radicalbig
βT=x⊤
atV−1
Txat/radicalbig
βT≤O/parenleftbigg
2L2√βT
Tα/parenrightbigg
(42)
Whereuseusedthefactthat/vextenddouble/vextenddoubleV−1
T/vextenddouble/vextenddouble= maxx∈Rd∥V−1
Tx∥2
∥x∥2isupperboundedbythelargesteigenvalue =O(1
Tα)
(given by1
λ) sinceV−1
tis positive semidefinite.
Therefore, picking α=1
2which gives us the tightest bound by AM-GM, we have
RT=T/summationdisplay
t=1[P(Gc
t) +P(Ec)] +T/summationdisplay
t=1rt≤O/parenleftig
L2/radicalbig
βT√
T/parenrightig
=O(mK2mLd√
Tlog(T)) (43)
We can now prove the regret bound of Algorithm 2 under reward asymmetry (Problem B).
Theorem 3 In the reward asymmetric (Problem B) contextual bandit setting where the context vectors are
distributed with fixed distribution the frequentist regret bound of Algorithm 2 is
RT=O(mK2mLd√
Tlog(T)) (44)
16Under review as submission to TMLR
Proof.Remarkably we can follow the same proof structure as in Theorem 4. In ETCwe have two main phases
1. They pull a fixed arbitrary arm for Tαexploration rounds while updating their ˆθestimate.
2. The remaining T−Tαrounds they will do regular Lin-UCB while not updating their parameters.
Even though in Algorithm LinUCB-B , they follow LinUCB for all Trounds, however, we can decompose these
rounds into the set of first Tαrounds and the remaining T−Tαrounds to capitalize on the decomposition
given by equation equation 39. This is because in the first Tαrounds we are still updating our estimate for
ˆθwhich is exactly what happens in phase 1 of ETC. Given that our initialization λ=Tαis unchanged in
LinUCB-B from ETCthis means that equation equation 40 still holds. While for the remaining T−ˆθrounds
they will do regular Lin-UCB while still sharpening the parameters which is essentially a better version of
phase 2 of ETC. This means that equation equation 42 still holds. In fact, this equation can be made slightly
sharper by
rt≤O/parenleftbigg
2L2√βT
t/parenrightbigg
(45)
Therefore we will obtain sharper bound but of the same order as O(·)hides the constants.
17