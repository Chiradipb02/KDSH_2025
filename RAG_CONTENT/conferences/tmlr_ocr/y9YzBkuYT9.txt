Under review as submission to TMLR
Efficient Weighting and Optimization in Federated Learning:
A Primal-Dual Approach
Anonymous authors
Paper under double-blind review
Abstract
Federated learning has emerged as a promising approach for constructing a large-scale cooper-
ative learning system involving multiple clients without sharing their raw data. However, the
task of finding the optimal sampling weights for each client, given a specific global objective,
remains largely unexplored. This challenge becomes particularly pronounced when clients’
data distributions are non-i.i.d. (independent and identically distributed) and when clients
only partially participate in the learning process.
In this paper, we tackle this issue by formulating the aforementioned task as a bi-level
optimization problem that incorporates the correlations among different clients. To address
this problem, we propose a double-loop primal-dual-based algorithm, designed specifically
to solve the bi-level optimization problem efficiently. To establish the effectiveness of our
algorithm, we provide rigorous convergence analysis under mild assumptions. Furthermore,
we conduct extensive empirical studies using both toy examples and learning models based
on real datasets. Through these experiments, we verify and demonstrate the effectiveness of
our proposed method.
1 Introduction
Federated learning has achieved high success in the large-scale cooperative learning system without sharing
raw data. Its ability to operate on a massive scale, involving numerous devices, has garnered significant
attention. However, with such a vast network of devices, ensuring the quality of data becomes an arduous
task. After all, the presence of noisy or low-quality data can severely hinder a model’s training effectiveness.
Recognizing this challenge, the question arises: how can we mitigate the influence of "bad" devices within
the federated learning framework? A logical solution emerges - by reducing the weight assigned to these
troublesome devices. Interestingly, popular federated training algorithms such as FedAvg (Li et al., 2019;
He et al., 2020) often assign equal weights to all devices or base the weights on the number of data points
they contribute. While these approaches have their merits, they fail to account for the varying quality
of data provided by each device. Drawing inspiration from existing federated algorithms, we introduce a
novel variable, denoted as x, which serves as a coefficient controlling the weight assigned to each device. By
incorporating this weight control mechanism, we gain the flexibility to adjust the influence of individual
devices on the model’s learning process. To assess the impact of these coefficients on the model’s performance,
we introduce a validation set on the server. This validation set acts as a gauge, allowing us to determine if the
coefficients improve the model’s overall efficacy. We formulate the whole problem as a bi-level optimization
as follows:
min
xf0(w∗(x))
s.t.w∗(x)∈arg min
wN/summationdisplay
i=1x(i)fi(w)
x∈X={x|x≥0,∥x∥1= 1},(1)
1Under review as submission to TMLR
wheref0is the validation loss evaluated in the global server and fiis the training loss evaluated on the
ithclient based on its own data. To solve problem 1, Kolstad & Lasdon (1990) propose an algorithm that
calculates the gradient of xdirectly, i.e.,
∂f0(w∗(x))
∂x(i)=−∇wf0(w∗(x))⊤/parenleftiggN/summationdisplay
i=1∇2
wfi(w∗(x))/parenrightigg−1
∇wfi(w∗(x)).
However, due to the large parameter dimension of w, taking the inverse of the Hessian or solving the related
linear system becomes computationally infeasible. Furthermore, estimating the gradient or Hessian of the
local function fidirectly is challenging due to the large amount of data on the local devices. Only stochastic
gradient and stochastic Hessian can be accessed. In light of these constraints, Ghadimi & Wang (2018)
proposed the FEDNEST algorithm, which approximates the inverse of the Hessian using a series of powers of
the Hessian, represented by/summationtextK
k=0(I−ηH)kto approximate1
ηH−1with a certain η. Similarly, Tarzanagh
et al. (2022) introduced the FEDNEST algorithm for solving stochastic bi-level optimization problems in
the federated learning setting. Both methods require an additional loop to approximate the product of the
Hessian inverse with a vector.
However, it is known that for constraint optimization, with the descent direction, the algorithm will not
converge to the optimal point or even to the first-order stationary point (Bertsekas, 2009). Therefore, obtaining
an accurate approximation of the Hessian inverse becomes crucial. Because the series of powers must start
withk= 0and require several iterations to achieve accuracy, it increases computation and communication
in federated learning. Fortunately, by leveraging the KKT condition, we can embed information about the
Hessian inverse into dual variables. Based on the smoothness of the objective, we can initialize the dual
variables effectively, instead of starting with the same initialization in each iteration (e.g., using Iin the
series approximation). Consequently, we propose a primal-dual-based algorithm to solve the problem outlined
in equation 1.
Furthermore, when solving constrained optimization problems with nonlinear equality constraints, adding the
squared norm of the equality constraint as an augmented term may not introduce convexity to the augmented
Lagrange function. Consequently, it becomes challenging for min-max optimization algorithms to find the
stationary point. In contrast, following the assumption in Ghadimi & Wang (2018), where the functions fi
are assumed to be strongly convex, adding the functions fias the augmented term helps introduce convexity
without altering the stationary point of the min-max problem. Based on this new augmented Lagrange
function, we prove that by employing stochastic gradient descent and ascent, wandλcan converge to the
KKT point. Additionally, using the implicit function theorem, when wandλapproach the stationary point
of the min-max problem, the bias in estimating the gradient of xcan be reduced to 0. Thus, by combining
the primal-dual algorithm on wandλwith stochastic projected gradient descent on x, we establish the
convergence of our algorithm.
Finally, we conduct comparisons between our algorithm and other algorithms using both a toy example and
real datasets, such as MNIST and F-MNIST with Network LeNet-5. The experimental results demonstrate
that our proposed algorithm performs well in strongly convex cases and even exhibits effectiveness in some
non-convex cases, such as neural networks. These findings provide compelling evidence of the capability and
versatility of our algorithm in various scenarios.
We summarize our contributions as follows:
•In Federated Learning, we formulate the local coefficient learning problem as a bi-level optimization
problem, which gives a way to identify the dataset quality in each local client for some specific task
(where a small validation set is given).
•In bi-level optimization, we introduce a primal-dual framework and show the convergence of the
whole algorithm in the constrained and stochastic setting.
•For some specific optimization problems with non-linear constraints, we give a new augmented term.
With the new augmented term, the primal variable and dual variable can converge to the KKT point
of the original problems.
2Under review as submission to TMLR
2 Related work
2.1 Personalized Federated Learning
The most related work in federated learning tasks will be personalized federated learning. A well-trained
local personalized model is needed for each local device in personalized federated learning. Jiang et al. (2019);
Deng et al. (2020) propose a method that they train a global model and then fine-tune the trained global
model to get the local model. T Dinh et al. (2020); Fallah et al. (2020) change the local objective function to
make each local can be different and handle individual local tasks. Li et al. (2021) introduces a two-level
optimization problem for seeking the best local model from great global models. All of these works do not
involve a validation set as a reference, but they use a few gradient steps or simple modifications and hope
the local model can both fit the local training data and use information from the global model (other local
devices). Different from these works, we explicitly formulate a bi-level optimization problem. By adding a
validation set, it can be more clearly identified the correlation between the other devices and its own.
2.2 Stochastic Bi-level Optimization
Bi-level optimization problem has been studied for a long time. One of the simplest cases in bi-level
optimization is the singleton case, where the lower-level optimization has a unique global optimal point.
Without calculating the inversion of the Hessian matrix of the lower-level optimization problem, there are
two major algorithms. Franceschi et al. (2017) approximates∂w∗(x)
∂xby∂wT
∂xwherewTis the iterate after
T steps gradient descent for the lower optimization problem. Using this method, in each iteration, we
need to communicate N(number of local devices) vectors among the server and local devices which is not
communication efficient. The other method Ghadimi & Wang (2018) is to approximate (∇2
wg(w))−1by/summationtextK
i=0(I−η∇2g(w))i, whereg(w)is the objective function of lower-level optimization problem. Although
Khanduri et al. (2021) points out that to approximate gradient for upper optimization function, we can get
rid of taking the optimal point for lower optimization in each upper-level update optimization, which seems
to get rid of double-loop approximation, it still needs a loop for approximating Hessian inverse with series.
Guo & Yang (2021) uses SVRG to reduce the noise level of estimating stochastic gradient and Hessian to get
better performance. Besides, all of the above works assume the smoothness of the local Hessian, but none
of them will apply the property directly to the algorithm. Different from the above works, we introduce
a primal-dual framework into bi-level optimization, where the dual variable can record the information of
Hessian. Also, Shi et al. (2005); Hansen et al. (1992) introduce the primal-dual framework, but they stay in
quadratic regime or mix integer programming, which is non-trivial to extend the results to federated learning
settings. Further, Tarzanagh et al. (2022) introduce bi-level optimization into the federated learning setting.
Huang et al. (2022) propose to adaptively weight nodes, which is the most related work to ours. Different
from Huang et al. (2022), we use a stochastic update for weight variable x, which makes us use a minibatch
update for xand partial participation for local clients.
3 Algorithm Design
Assume that each function of fiis a strongly convex function. Then, the optimal solution to the lower
optimization problem becomes only a single point. Thus, with the implicit function theorem, we can calculate
the gradient of f0(w∗(x))with respect to xas follows.
Proposition 1. Supposefi’s are strongly convex functions. Then for each x∈X, it holds that∂f0(w∗(x))
∂x(i)=
−∇wf0(w∗(x))⊤/parenleftig/summationtextN
j=1x(j)∇2
wfj(w∗(x))/parenrightig−1
∇wfi(w∗(x)).
With the proposition 1, one can calculate the gradient of x, whenw∗(x)and the inverse of Hessian are
given. However, for large-scale problems, none of these can be easily obtained. Fortunately, by noticing
the convexity of each function fi, we can replace the first constraint w∗(x)∈arg minw/summationtextN
i=1x(i)fi(w)with
3Under review as submission to TMLR
∇/summationtextN
i=1x(i)fi(w) = 0. For given x, we can formulate the following constrained optimization problem:
min
wf0(w)
s.t.N/summationdisplay
i=1x(i)∇wfi(w) = 0,(2)
By introducing the dual variable λ, we can easily get the Lagrange function. To solve the Lagrange function
efficiently, we propose the following augmented Lagrange function.
Lx(w,λ) =f0(w) +λ⊤N/summationdisplay
i=1x(i)∇wfi(w) + ΓN/summationdisplay
i=1x(i)fi(w). (3)
Different from the standard augmented terms, where the norm square of equality constraints is added to
achieve strong convexity of the primal problem, we add the summation of fi’s with coefficient x(i)’s. If we
use the norm square of the gradient constraint for general strongly convex functions, it will not be strongly
convex. Thus, we can not directly adopt the gradient descent ascent algorithm. With the definition, we can
obtain the following two propositions directly.
Proposition 2. Supposefi’s are strongly convex functions for i= 1,2,···,N,x(i)≥0for all i and∥x∥1= 1.
Then, Problem 2 satisfies Linear Independence Constraint Qualification, and its KKT conditions can be
written as follows:
∇wf0(w) +N/summationdisplay
i=1x(i)∇2
wfi(w)λ= 0
N/summationdisplay
i=1x(i)∇wfi(w) = 0.
Proposition 3. Supposefi’s are strongly convex functions for i= 1,2,···,N,x(i)≥0for all i and∥x∥1= 1.
Then, the stationary point of minwmaxλLx(w,λ)is unique and satisfies the KKT conditions of problem 2.
Let(ˆw∗(x),λ∗(x))be the stationary point of minwmaxλLx(w,λ). From proposition 2, it holds that ˆw∗(x) =
w∗(x)and
∂f0(w∗(x))
∂x(i)=λ∗(x)⊤∇wfi(w∗(x)). (4)
Thus, with the KKT point w∗(x)andλ∗(x), we can estimate the gradient of xwithout estimating the inverse
of Hessian. However, as λ⊤/summationtextN
i=1x(i)∇wfi(w)can be a highly non-convex function, which can be harmful to
the optimization process. We add an additional constraint on the norm of λand define the constraint set Λ.
Thus, the problem 2 becomes
min
wmax
λ∈ΛLx(w,λ) =f0(w) +λ⊤N/summationdisplay
i=1x(i)∇wfi(w) + ΓN/summationdisplay
i=1x(i)fi(w). (5)
We propose a double loop algorithm for solving problem 1. We show the algorithm in the Algorithm 1
and 2. In the inner loop, we solve the augmented Lagrange for Ksteps. In each step, the local client
will receive the iterates wt,kandλt,k. After that, each local client will calculate ˜∇fi(wt,k)and ˆ∇fi(wt,k)
based on the backpropagation through two independent batches. The term ˜∇2fi(wt,k)λt,kis calculated
with the auto-differentiable framework (i.e. Pytorch, TensorFlow) or with the closed-form multiplication.
Then the local device sends gradient estimation ˜∇wfi(wt,k)and the estimated product of Hessian and λ
(˜∇2fi(wt,k)λt,k) to the server.
For the server, in each step, the server will first send the primal variable ( wt,k) and dual variable ( λt,k) to
all local clients. Then, the server will receive the estimated gradients and estimated products from some local
clients. Because not all devices will stay online in each step, we define a set Activet,kwhich records the clients
that participate in the optimization in the (t,k)step. With the vectors collected from local clients, the server
will calculate the gradient estimator of wt,kandλt,kwith respect to function Lxt(wt,k,λt,k). And then, wt,k
4Under review as submission to TMLR
Algorithm 1 The bi-level primal-dual algorithm on local device i
1:fort= 1,2,···,Tdo
2:fork= 1,2,···,Kdo
3:Receivewt,k,λt,kfrom the server;
4:Sample a mini-batch and calculate ˜∇fi(wt,k);
5:Sample a mini-batch and calculate ˆ∇fi(wt,k);
6:Calculate ˜∇2
wfi(wt,k)λt,kwith back propagation on scalar ˆ∇wf(wt,k)λt,k;
7:Send ˜∇2fi(wt,k)λtand ˜∇fi(wt,k)to the server;
8:end for
9:end for
will be updated by a gradient descent step and λt,kwill be updated by a gradient ascent step. Different from
local devices, after K inner loop update steps, based on the λt,Kand gradient estimated in each local client,
the server will calculate the gradient of xbased on equation 4 and perform a projected gradient descent step
onx. In addition, if the ithagent is not in Activet,K, we set the gradient of x(i)to be zero.
Algorithm 2 The Bi-level primal dual algorithm on the Server
1:Initialx1,w1,1,λ1,1, total iterations: K, T and step size: ηw,ηλ,ηx.
2:fort= 1,2,···,Tdo
3:fork= 1,2,···,Kdo
4:Sendwt,k,λt,kto each local device;
5:Receive ˜∇fi(wt,k)and ˜∇2
wfi(wt,k)λtfromActivet,k;
6:gw=˜∇f0(wt,k) +N
|Activet,k|/summationtext
i∈Activet,kx(i)
t˜∇2
wfi(wt,k)λt,k+ Γ˜∇fi(wt,k);
7:wt,k+1=wt,k−ηwgw;
8:gλ=N
|Activet,k|/parenleftig/summationtext
i∈Activet,kx(i)
t˜∇wfi(wt,k)/parenrightig
;
9:λt,k+1= Π Λ(λt,k+ηλgλ);
10: end for
11:gx(i)=N
|Activet,K|λ⊤
t,K˜∇wfi(wt,K)fori∈Activet,K;
12:gx(i)= 0fori /∈Activet,K;
13:xt+1=PX(xt−ηxgx);
14:λt+1,1=λt,K+1;
15:wt+1,1=wt,K+1
16:end for
17:Output:xT,WT,K+1.
Remark 1. gx(i)can be calculated in the i-th device and sent to the server, which can reduce the computation
in the server and will increase one-round communication with one real number between the server and devices.
The rest of the analysis will remain to be the same.
4 Theoretical Analysis
In this section, we analyze the convergence property of the proposed algorithm. First, we state some
assumptions used in the analysis.
(A1)f0,f1,···,fNare lower bounded by f , andf0,f1,···,fNhaveL1Lipschitz gradient.
(A2)f1,···,fNareµ-strongly convex functions.
(A3)f1,···,fNhasL2Lipschitz Hessian.
(A4) maxi∈{0,1,···,N}maxx∈X∥∇fi(w∗(x))∥≤Dw.
(A5)Each local estimation is unbiased with bounded variance σ2.
5Under review as submission to TMLR
(A6)Activet,kis independent and sampled from the set of the nonempty subset of {1,2,···,N}, where
P(i∈Activet,k) =pfor alli∈{1,2,···,N}.
Remark 2. (A1),(A2),(A3) are commonly used in the convergence analysis for bi-level optimization
problems (Ji et al., 2021; Chen et al., 2021; Khanduri et al., 2021). Unlike Ji et al. (2021); Chen et al.
(2021), where they need to assume f0,f1,···,fNto beL0Lipschitz, we assume the gradient norm is bounded
at the optimal solution. Because for machine learning models, regularization will be added to the objective
function, making the norm of the optimal solution not large. When w∗(x)can be bounded by some constant.
(A4)is reasonable in practice. Moreover, the Lipschitz assumption on function can directly infer (A4)with
Dw=L0.(A5)is a common assumption used for stochastic gradient methods (Ghadimi et al., 2016) and
(A6)extend the assumption in Karimireddy et al. (2020) by giving the probability that local devices will be
chosen instead of uniformly sampling.
Remark 3. With (A4),Dλ= maxx∈X∥λ∗(x)∥is upper bounded by Dw/µ.
Proposition 4. When Λ ={λ|∥λ∥≤Dλ}, then the stationary point of problem 5 is the KKT point of
problem 2.
With proposition 3 and 4, the stationary point of problem 5 is unique and we denote the stationary point as
(w∗(x),λ∗(x)). To give the convergence of the whole algorithm, firstly, we give the convergence guarantee for
the inner loop.
Theorem 1. For givenx∈X, when (A1) to (A6) holds, Γ =DλL2+2L1
µ,ηw≤1
LL(1+p(1+Γ2+D2
λ))and
ηλ=64(Γµ−DλL2−L1)2
L2
1(64+(Γµ−DλL2−L1)2)ηw≤µ2
4LL(1+p)L2
d, when randomly choosing ˆk∈{1,2,···,K}with equal probability
it holds that
E/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubleλ⊤
ˆk∇wfi(wˆk)−∂f0(w∗(x))
∂x(i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
≤C4
Kηw+C5ηw+C6η2
w
where
C4=3(D2
w+DλL2
1)(64 +L2
1)µ4+ 12D2
λL2
L
L2
L
C5=/parenleftbiggLL(µ2+ (dλL2+ 2L1)2+D2
λ)
2µ2+32L2
L+µ2
16L2
L/parenrightbigg
σ2
C6=128L2
1σ2
64 +L2
1
Corollary 1. When selecting ηw= Θ(1/√
K)andηλ= Θ(1/√
K), it holds that
E/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubleλ⊤
ˆk∇wfi(wˆk)−∂f0(w∗(x))
∂x(i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
=O(1/√
K).
Thus, with theorem 1, the gradient of f0with respect to xcan be “well” estimated through the inner gradient
descent ascent method when the number of inner loop steps is large enough. Then, we can obtain the following
convergence result of the outer loop.
Theorem 2. Suppose (A1) to (A6) holds, Γ,ηwandηλare selected according to Theorem 1. We
randomly choose ˆk∈ {1,2,···,K}with equal probability to approximate the gradient of x. Define
ˆx= arg min y∈X(f0(w∗(y)) +ρ
2∥y−x∥2)and ¯∇ρf0(w∗(x)) =ρ(x−ˆx)forρ= 2Lf0,it holds that
1
TT/summationdisplay
i=1T/summationdisplay
t=1E∥¯∇ρf0(w∗(xt))∥2
≤2Lf0(f0(w∗(x1))−f) + 2L2
f0∥x1−ˆx1∥2
Tηx+ 2L2
f0δ∗+ 4L2
f0((1 +p)D2
λD2
w+ 2δ∗+σ2) + 2Lf0δ∗
whereLf0=/parenleftig
D2
w
µ2+2DwL1
µ/parenrightig√
NDwµ+√
NDwL1
µ3 andδ∗=C4
Kηw+C5ηw+C6η2
w.
6Under review as submission to TMLR
Corollary 2. When we select ηw= Θ(1/√
K),ηλ= Θ(1/√
K)andηx= Θ(1/√
T), it holds that
1
TT/summationdisplay
t=1E∥¯∇ρf0(w∗(xt))∥2=O(1/√
T+ 1/√
K).
Remark 4. To achieve ϵ-stationary point ( E∥¯∇ρf0(w∗(xt))∥2≤ϵ),O(1/ϵ4)samples are needed in each local
client and in the server. Different from the previous works on bilevel optimization(e.g. Ghadimi & Wang
(2018), Khanduri et al. (2021) and Franceschi et al. (2017)), we prove the convergence when optimization
variablexhas a convex constraint.
4.1 Proof Sketch of Theorem 1
To show the convergence of inner loop, we first construct a potential function for inner loop objective. Define
Φx(w,λ) =Lx(w,λ)−2d(λ), whered(λ) =minwLx(w,λ)for given x. The intuition of defining this potential
function is that Lx(w,λ)is not necessarily decreasing in each iteration, as λis performing a gradient ascent
step. Meanwhile, the gradient λtaken is an approximation of the gradient of d(λ). Thus, by subtracting
d(λ), we can obtain that Φwill decrease during iterations. Therefore, the first thing is to show the lower
bound of the function Φ.
Lemma 1 (Lower bound of Φ).Suppose (A1)-(A4) hold. It holds that Φx(w,λ)is bounded below by f.
The proof of this lemma is basically due to the definition of Φx(w,λ)andd(λ). Then, similar to the proof
of gradient descent, we give a lemma that shows the descent of potential function under certain choices of
hyperparameters.
Lemma 2 (Potential function descent, proof can be found in Lemma ??in Appendix) .Suppose (A1)-(A6)
hold. In addition, we assume Γ>DλL2+L1
µ, it holds that
E[Φx(wt,k,λt,k)−Φx(wt,k+1,λt,k+1)]≤−C1E∥∇wLx(wt,k,λt,k)∥2−C2E[∥λt−λ∗
t∥2] +C3σ2,
whereλ+
t= Π Λ(λt+ηλ∇d(λt)),C1= Θ(ηw−η2
w−η2
λ−ηλ),C2= Θ(ηλ)andC3=O(η2
w+η2
λ)
Thus, when choosing sufficient small ηwandηλ, we can achieve positive C1andC2. Together with the lower
bound of the function Φ, the convergence of the inner algorithm can be shown. Because of the uniqueness of
the KKT point, by choosing ηwandηλin order of 1/√
K, it can be shown that
1
KK/summationdisplay
k=1E∥wt,k−w∗(xt)∥2=O(1/√
K),1
KK/summationdisplay
k=1E∥λt,k−w∗(xt)∥2=O(1/√
K).
Therefore, with the convergence rate of wt,kandλt,kand equation 4, we can easily prove theorem 1.
4.2 Proof Sketch of Theorem 2
To apply stochastic gradient descent analysis on x, although we have smoothness for the function f0,f1,···,fN
onw, we need to verify the smoothness of f0(w∗(x))with respect to x.
Lemma 3 (Convergence of stochastic gradient descent with biased gradient estimation, proof can be found
in Lemma ??in Appendix) .Suppose function f(x)is lower bounded by fwithL-Lipshitz gradient. g(x)is
an unbiased gradient estimator of ∇f(x)satisfying that expected norm of g(x) are bounded by Gin domainX
for function f. Then with update rule xt+1= ΠX(xt−ηx(g(xt) +ξt)), whereηx= Θ(1/√
T),Xis a convex
set and E∥ξt∥2≤ϵ2. By defining ˆx=arg miny∈X(f(y) +ρ
2∥y−x∥2)and ¯∇ρf(x) =ρ(x−ˆx), whereρ= 2L,
then it holds that
1
TET/summationdisplay
t=1∥¯∇ρf(xt)∥2=O(1/√
T+ϵ2).
As Lemma 3 suggests, when f0(w∗(x))satisfyingL-Lipschitz gradient, bounded estimation error, and bounded
gradient norm, the convergence rate can achieve O(1/√
T)with an error term related to estimation error.
Theorem 1 shows the estimation error can be bounded by O(1/√
K). Combining these two results we can
prove Theorem 2.
7Under review as submission to TMLR
5 Experimental Results
In this section, we compare our algorithm with other bi-level optimization algorithms (FEDNEST (Tarzanagh
et al., 2022), SUSTAIN (Khanduri et al., 2021) and RFHO (Franceschi et al., 2017)) in two cases: the toy
example and two vision tasks. Further, in vision tasks, agnostic federated learning (AFL) is tested (Mohri
et al., 2019). When k local steps are used in each algorithm, FEDNEST, RFHO, and our algorithm will
perform 2kdreal number transmission, where dis the dimension of optimization. SUSTAIN will perform
(k+ 1)dreal number transmission. In the vision tasks, they perform the same real number of transmissions
ask= 1. For a fair comparison, we set the local device update to 1.
Figure 1: The figure shows the result of the toy example where all clients participate in the optimization
process in each iteration, and all gradient and hessian are estimated without noise. The above line shows
the stationary of xin each iteration, and the second row shows the function value of x(f(w∗(x))). The left
column shows the results when the number of local steps is 1; the middle column shows the results of 5
local steps, and the right column gives the results of 10 local steps. The shadow part of the function value
corresponds to the 0.1 standard error area, and the shadow part in stationary corresponds to the 0.5 standard
error area.
5.1 Toy Example
In this section, we apply algorithms to solve problem 1 with fias follows:
fi(w) =1
2∥Aiw−Bi∥2+cos(a⊤
iw−bi),
whereAi∈R30×20,Bi∈R30,ai∈R20andbi∈Rare all generated from Gaussian distribution. The variance
of each component in Aiandaiis1/√
20and the variance of each component in Biis1/√
30and variance
ofbiis 1. When generated function fiis not 0.1-strongly convex, we randomly generate a new one until
we get strongly convex fiwhose modular is not less than 0.1. Three local steps ( K=1, 5, 10) are tested.
Here, the local steps are used for wupdate for algorithm FEDNEST, RFHO, and our algorithm, and the
local steps are used for Hessian estimation for algorithm FEDNEST and SUSTAIN. Because we can easily
compute the Hessian matrix and its inverse for this toy example, we test the algorithm using the inverse of
the estimated Hessian to compute the gradient of xnamed GD. We test two settings of the toy example. One
is the deterministic setting, where no estimation noise or client disconnection will occur. In the other setting,
we add white Gaussian noise with a noise level of 0.5 in each estimation (including gradient estimation and
Hessian estimation). Also, each client has a 0.5 probability of connecting with the server.
8Under review as submission to TMLR
Figure 2: The figure shows the result of the toy example where the active rate is 0.5 in each iteration, and
all gradient and hessian are estimated with white-Gaussian noise with a noise level of 0.5. The above line
shows the stationary of xin each iteration, and the second row shows the function value of x(f(w∗(x))).
The left column shows the results when the number of local steps is 1; the middle column shows the results
of 5 local steps, and the right column gives the results of 10 local steps. The shadow part of the function
value corresponds to the 0.1 standard error area, and the shadow part in stationary corresponds to the 0.5
standard error area.
To evaluate the performance of different algorithms, we calculate the function value of f0(w∗(x))and the
stationary of x, i.e. x−ΠX(x−0.001∇xf0(w∗(x))), wherew∗(x)is approximate by 200 gradient steps.
We take 15 local clients and run 20 times and get the results of different algorithms. The results of the
deterministic setting are shown in Figure 1, and the results of the noise setting are shown in Figure 2.
As it is shown in Figure 1, with local steps getting larger and larger, the performance of FEDNEST, RFHO,
and SUSTAIN is getting close to GD, while the performance of the primal-dual method is similar to GD
whatever local step used in the algorithm even with only one single step. When noise is added in the Hessian,
the direct inverse may cause the biased estimation. Thus, the performance of GD gets much worse than it in
the deterministic setting shown in Figure 2. Also, in Figure 2, our algorithm can perform better than other
algorithms when the local step is small. When local steps increase to 10, FEDNEST and our algorithm have
competitive results.
5.2 Vision Tasks
In this section, we apply algorithms to train LeNet5(LeCun et al., 1998) on dataset MNIST(LeCun et al.,
1998) and Fashion-MNIST(Xiao et al., 2017). To construct non-iid datasets on different local clients and
the global server’s validation set, we randomly pick 20 samples per label out of the whole training dataset
and form the validation set. Then, the rest of the training data are divided into 3 sets, and each set will be
assigned to a local client. The first client contains samples labeled as 0,1,2,3,4, the second client contains
samples labeled as 5,6,7, and the third client contains samples labeled as 8,9 for all two datasets. To test
the algorithm’s ability to choose the proper coefficient of local clients, we add 7 noise nodes containing
5000 samples with random labels. We set the learning rate of wto be a constant learning rate without any
decay selected from {0.1,0.01,0.001}for all training methods, and the learning rate of xis selected from
{0.1,0.01,0.001,0.0001}. The batch size for all three training cases is set to 64. Γused in the proposed
algorithm is selected from {0.5,1,2}. For simplicity, we set the local step as 1. We run 2000 iterations for
MNIST and 6000 iterations for Fashion-MNIST. Active probability is set in {0.5,0.9,1}. We compare the
9Under review as submission to TMLR
Table 1: Test Accuracy and xoutput of Training LeNet 5 on MNIST. "AP" represents Active Probability,
and Accuracy stands for Test Accuracy.
AP RFHO FEDNEST SUSTAIN Ours
1Accuracy 98.34%±0.18% 98.15%±0.23% 99.02%±0.15% 98.43%±0.17%
x(1)0.488±0.104 0.425±0.081 0.411±0.069 0.455±0.016
x(2)0.311±0.104 0.245±0.133 0.305±0.045 0.334±0.020
x(3)0.197±0.031 0.294±0.176 0.282±0.029 0.212±0.026
x(4),···,(10)∼6e−4∼6e−3∼3e−4∼2e−4
0.9Accuracy 98.07%±0.4% 98.09%±0.21% 98.85%±0.29% 98.43%±0.19%
x(1)0.407±0.040 0.395±0.136 0.386±0.058 0.449±0.046
x(2)0.281±0.065 0.314±0.045 0.345±0.028 0.333±0.050
x(3)0.291±0.018 0.239±0.085 0.265±0.038 0.217±0.024
x(4),···,(10)∼4e−3∼8e−3∼7e−4∼2e−4
0.5Accuracy 97.86%±0.36% 95.37%±4.10% 97.60%±0.49% 98.24%±0.23%
x(1)0.449±0.090 0.539±0.076 0.365±0.015 0.468±0.052
x(2)0.276±0.075 0.217±0.059 0.329±0.013 0.372±0.053
x(3)0.271±0.129 0.210±0.039 0.292±0.015 0.16±0.035
x(4),···,(10)∼6e−4∼6e−3∼2e−3∼2e−4
test accuracy among different methods. As a baseline, we report the test accuracy for training with the
validation set only named val, training with the average loss of each client named avg, and training with
x= (0.5,0.3,0.2,0,···,0)named opt. All experiments run on V100 with Pytorch (Paszke et al., 2019).
Results are shown in Figure 3, Figure 4, and Table 1.
Figure 3: Test accuracy of training LeNet 5 on MNIST dataset. The left curve shows the result when the
active probability is 1; the middle curve shows the result when the active probability is 0.9, and the right
curve shows the result with the active probability of 0.5.
Figure 3 shows the test accuracy of the MNIST dataset with different active probabilities. Although SUSTAIN
works better than the primal-dual algorithm when all local devices participate in the optimization process,
when clients’ participant rate decreases to 0.5, SUSTAIN works worse than our method. Primal-dual becomes
slower than SUSTAIN may be because of the initialization of the dual variable. When the dual variable is
far from its real value it needs more time to get a good enough point. Other than SUSTAIN, our algorithm
can converge faster and more stable to a high accuracy point. Further, we list the output of xand standard
error of test accuracy for 5 different experiments for different algorithms in Table 1. According to Table 1,
our algorithm can achieve a more stable output with respect to x, and the output xis closer to 0.5,0.3,0.2,
which is related to the number of labels the first three clients holds.
Figure 4 gives the test accuracy of training LeNet 5 on the Fashion-MNIST Dataset. Similar to the results of
the MNIST dataset, when the clients’ participant is high (0.9,1), SUSTAIN works slightly better than the
primal-dual algorithm. But when more local devices disconnect to the server, the performance of SUSTAIN
drops, while the primal-dual algorithm remains fast convergence speed and high test accuracy.
10Under review as submission to TMLR
Figure 4: Test accuracy of training LeNet 5 on the Fashion-MNIST dataset. The left curve shows the result
when the active probability is 1; the middle curve shows the result when the active probability is 0.9, and the
right curve shows the result with 0.5 active probability.
6 Conclusion
In this paper, we proposed a primal-dual-based method for solving a bi-level optimization problem based on a
federated learning task (local coefficient learning). We give a theoretical analysis that shows the convergence
of the proposed algorithm. Though the analysis shows it needs more iterations for the algorithm to converge
to anϵ-stationary point, it works well with a pretty small number of local steps in both toy case and neural
network training. Other than that convergence rate can be improved (perhaps it should be in the order of
O(1/√
T)instead ofO(1/√
T+ 1/√
K)), the initialization of dual variable affects the speed for convergence,
which we leave as the future work.
References
Dimitri Bertsekas. Convex optimization theory , volume 1. Athena Scientific, 2009.
Tianyi Chen, Yuejiao Sun, and Wotao Yin. A single-timescale stochastic bilevel optimization method. arXiv
preprint arXiv:2102.04671 , 2021.
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated learning.
arXiv preprint arXiv:2003.13461 , 2020.
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning with theoretical
guarantees: A model-agnostic meta-learning approach. Advances in Neural Information Processing Systems ,
33:3557–3568, 2020.
Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based
hyperparameter optimization. In International Conference on Machine Learning , pp. 1165–1173. PMLR,
2017.
Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint
arXiv:1802.02246 , 2018.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods for
nonconvex stochastic composite optimization. Mathematical Programming , 155(1):267–305, 2016.
Zhishuai Guo and Tianbao Yang. Randomized stochastic variance-reduced methods for stochastic bilevel
optimization. arXiv e-prints , pp. arXiv–2105, 2021.
Pierre Hansen, Brigitte Jaumard, and Gilles Savard. New branch-and-bound rules for linear bilevel program-
ming.SIAM Journal on scientific and Statistical Computing , 13(5):1194–1217, 1992.
Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma,
Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang Yang,
Murali Annavaram, and Salman Avestimehr. Fedml: A research library and benchmark for federated
machine learning. CoRR, abs/2007.13518, 2020. URL https://arxiv.org/abs/2007.13518 .
11Under review as submission to TMLR
Yankun Huang, Qihang Lin, Nick Street, and Stephen Baek. Federated learning on adaptively weighted
nodes by bilevel optimization. arXiv preprint arXiv:2207.10751 , 2022.
Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design.
InInternational Conference on Machine Learning , pp. 4882–4892. PMLR, 2021.
Yihan Jiang, Jakub Konečn` y, Keith Rush, and Sreeram Kannan. Improving federated learning personalization
via model agnostic meta learning. arXiv preprint arXiv:1909.12488 , 2019.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In Inter-
national Conference on Machine Learning , pp. 5132–5143. PMLR, 2020.
Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A near-
optimalalgorithmforstochasticbileveloptimizationviadouble-momentum. Advances in Neural Information
Processing Systems , 34, 2021.
Charles D Kolstad and Leon S Lasdon. Derivative evaluation and computational experience with large bilevel
mathematical programs. Journal of optimization theory and applications , 65(3):485–499, 1990.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning
through personalization. In International Conference on Machine Learning , pp. 6357–6368. PMLR, 2021.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on
non-iid data. arXiv preprint arXiv:1907.02189 , 2019.
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In International
Conference on Machine Learning , pp. 4615–4625. PMLR, 2019.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. Advances in neural information processing systems , 32, 2019.
Chenggen Shi, Jie Lu, and Guangquan Zhang. An extended kuhn–tucker approach for linear bilevel
programming. Applied Mathematics and Computation , 162(1):51–63, 2005.
Canh T Dinh, Nguyen Tran, and Josh Nguyen. Personalized federated learning with moreau envelopes.
Advances in Neural Information Processing Systems , 33:21394–21405, 2020.
Davoud Ataee Tarzanagh, Mingchen Li, Christos Thrampoulidis, and Samet Oymak. Fednest: Federated
bilevel, minimax, and compositional optimization. In International Conference on Machine Learning , pp.
21146–21179. PMLR, 2022.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
12