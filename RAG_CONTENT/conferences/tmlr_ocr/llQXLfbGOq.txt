Published in Transactions on Machine Learning Research (09/2024)
Attention Normalization Impacts Cardinality Generalization
in Slot Attention
Markus Krimmel markus.krimmel@tuebingen.mpg.de
Embodied Vision Group, Max Planck Institute for Intelligent Systems
Jan Achterhold jan.achterhold@tuebingen.mpg.de
Embodied Vision Group, Max Planck Institute for Intelligent Systems∗
Joerg Stueckler joerg.stueckler@uni-a.de
Embodied Vision Group, Max Planck Institute for Intelligent Systems
Intelligent Perception in Technical Systems Group, University of Augsburg
Reviewed on OpenReview: https: // openreview. net/ forum? id= llQXLfbGOq
Abstract
Object-centric scene decompositions are important representations for downstream tasks in
fields such as computer vision and robotics. The recently proposed Slot Attention module,
already leveraged by several derivative works for image segmentation and object tracking
in videos, is a deep learning component which performs unsupervised object-centric scene
decomposition on input images. It is based on an attention architecture, in which latent
slot vectors, which hold compressed information on objects, attend to localized perceptual
features from the input image. In this paper, we demonstrate that design decisions on
normalizing the aggregated values in the attention architecture have considerable impact on
the capabilities of Slot Attention to generalize to a higher number of slots and objects as seen
during training. We propose and investigate alternatives to the original normalization scheme
which increase the generalization capabilities of Slot Attention to varying slot and object
counts, resulting in performance gains on the task of unsupervised image segmentation. The
newly proposed normalizations represent minimal and easy to implement modifications of
the usual Slot Attention module, changing the value aggregation mechanism from a weighted
mean operation to a scaled weighted sum operation.
1 Introduction
Object-wise scene decompositions are ubiquitous in computer vision, robotics, and related disciplines such as
reinforcement learning, since the state and actions in the environment are naturally represented in relation to
objects. Over recent years, unsupervised learning of object-centric representations from unlabelled images
and video has attracted significant interest in the machine learning community (Greff et al., 2017; Engelcke
et al., 2020; Locatello et al., 2020). The Slot Attention architecture (Locatello et al., 2020) decomposes a
two-dimensional RGB input image object-wise using an attention mechanism which updates slots, holding
information about objects, in a recurrent manner. In (Locatello et al., 2020), it was used for set prediction
and image segmentation tasks on relatively simple renderings of 2D or 3D scenes, such as CLEVR (Johnson
et al., 2017). Later in (Seitzer et al., 2023), Slot Attention has also been successfully applied to the task of
unsupervised segmentation of more realistic images (MOVi, (Greff et al., 2022)). Detecting and tracking
objects in videos using Slot Attention is described in (Kipf et al., 2022; Elsayed et al., 2022). Despite its
empirical success, a theoretical explanation of its inner workings and the inductive biases which lead to
∗Work done during PhD studies at MPI-IS. Jan Achterhold is now with Robert Bosch GmbH - Corporate Research, and the
Bosch Center for Artificial Intelligence (BCAI), Renningen, Germany.
1Published in Transactions on Machine Learning Research (09/2024)
the emergence of object-wise decompositions in Slot Attention is still under active research (Chang et al.,
2022b;a).
In this paper we investigate design choices on the normalization of aggregated attention values in Slot
Attention. We find that the normalization proposed in (Locatello et al., 2020) leads to suboptimal foreground
segmentation performance during inference with higher number of objects or slots than used for training
the model. We also investigate two alternative normalization approaches, give theoretical insights on their
behavior, and assess their performance in relation to the original Slot Attention baseline. We demonstrate
that these different approaches for normalizing the aggregated values can have a significant impact on the
generalization of Slot Attention to a varying number of slots and objects during inference.
2 Background
2.1 Slot Attention
The Slot Attention module (Locatello et al., 2020) is given a set of Ninput tokens ˜xn∈RDinput,n∈{1,...,N}
and iteratively refines a set of Kslots ˜θk∈RDslot,k∈{1,...,K}. In an object-wise scene decomposition
scenario, slots correspond to latent variables holding information on objects, while the input tokens are
localized image features, e.g., computed by a convolutional neural network. Slots bind to input tokens via a
dot-product attention mechanism (Luong et al., 2015). Learned linear maps kandqextractD-dimensional
keys and queries from the layer-normalized (Ba et al., 2016) input tokens xn:=LayerNorm (˜xn)and layer-
normalized slots θk:=LayerNorm (˜θk), respectively. In our case, we always have Dinput =Dand the layer
normalization modules that produce xnandθkdo not share parameters.
For the attention mechanism, an unnormalized N×KmatrixMof dot prod-
ucts is formed from the keys kn:=k(xn)and queries qk:=q(θk). On each
row ofM, a Softmax operator is then applied, yielding Γ = (γn,k)∈[0,1]N×K:
Mn,k:=1
τk(xn)⊤q(θk) =1
τk⊤
nqk(1) γn,k:=expMn,k/summationtextK
k′=1expMn,k′. (2)
With this, each row γn,:may be interpreted as the probability of an input token nto be assigned to a
particular slot k. The constant τcorresponds to a temperature parameter which is chosen to be√
D.
A linear map v:RDinput→RDextracts values from the input tokens and the matrix Γis used to accumulate
values into unnormalized slot-wise update codes:
˜uk:=N/summationdisplay
n=1γn,kv(xn) (3)
With the motivation to improve the stability of the attention mechanism, Slot Attention performs a normal-
ization on the update codes. Namely, the sum in (3)is scaled in such a way that it becomes a weighted mean
of the values v(xn), i.e.:
uk:=˜uk/summationtextN
n=1γn,k(4)
This normalization scheme is termed weighted mean . Locatello et al. (2020) discuss two ablations of this
normalization. The weighted sum scheme normalizes the update code by multiplication with a constant, i.e.
uk:=1
C˜uk. In the ablation study in (Locatello et al., 2020), the value chosen for Cis not discussed, and it
must be assumed that C= 1was chosen. The second ablation of (Locatello et al., 2020) is termed layer
normalization and uses a layer normalization module that is shared across slots for normalization. Concretely,
the normalized update code is computed as uk:=LayerNorm (˜uk). We refer to Appendix B for an exact
definition of layer normalization.
For each slot k, the aggregated value ukis used to update the latent representation ˜θkvia a gated recurrent
unit (GRU) (Cho et al., 2014) and a residual multilayer perceptron with ˜θnew
k:=update (˜θk,uk).
2Published in Transactions on Machine Learning Research (09/2024)
2.2 Von Mises-Fisher Distributions
Von Mises-Fisher (vMF) distributions (Fisher, 1953) are probability distributions on the unit (d−1)-sphere
inRd. Typically, they are parametrized by a mean direction θ∈Rdwith∥θ∥2= 1and a concentration
parameterτ >0. They are defined by the following density w.r.t. the usual surface measure on the (d−1)-
spheref(x|θ,τ) =1
Z(d,τ)exp/parenleftig
θ⊤x
τ/parenrightig
, whereZ(d,τ)is a normalization constant that is independent of θ. If
(θ1,...,θK)and(τ1,...,τK)are parameters of vMF distributions and (π1,...,πK)is contained in the probability
simplex, a vMF mixture model can be defined as usual via the density g(x) :=/summationtextK
k=1πkf(x|θk,τk).
3 Slot Attention and von Mises-Fisher Mixture Model Parameter Estimation
Many works (Locatello et al., 2020; Chang et al., 2022b;a; Kirilenko et al., 2023) compare Slot Attention to
expectation maximization (Dempster et al., 1977; Bishop, 2006) (EM) in Gaussian mixture models, i.e. to
soft k-means clustering. We, however, connect it with expectation maximization in a mixture model of von
Mises-Fisher (vMF) distributions (Banerjee et al., 2003), since Slot Attention uses a bilinear form on slots
and inputs as a scoring function instead of the negative Euclidean distance. In this section, we make the
parallel between Slot Attention and EM explicit by performing EM parameter estimation in a vMF mixture
model and relating each step to the corresponding step in Slot Attention. We will then view the weighted
mean, layer norm, and weighted sum normalization variants in the context of this analogy and compare them.
3.1 Relating Slot Attention to EM
We consider a case in which Npointsxnare given on the unit (d−1)-sphere in Rd. We estimate the mean
directionsθ1,...,θKofKvMF components, along with the mixture coefficients π1,...,πK. We assume that
the vMF distributions have fixed concentration, i.e., τ= 1. We interpret the parameters θkto relate to
slots in Slot Attention and the points xnto relate to the perceptual input features of the module. The
concentration parameter τcan be understood as an analogue to the temperature√
Din Slot Attention.
E-Step In the expectation step, soft assignments of datapoints to clusters (slots) are computed via the
likelihood functions of the vMF components:
γn,k:=πkexp(x⊤
nθk)/summationtextK
k′=1πk′exp(x⊤nθk′)(5)
The resulting matrix Γ∈[0,1]N×Kcorresponds to the attention matrix in Slot Attention. Equation (5)
closely resembles the computation of the attention matrix in Slot Attention with some differences: While the
inputsxnin Slot Attention do not necessarily lie on the unit sphere, we do remind the reader that they are
layer-normalized and therefore lie on ellipsoids. Similarly, the slots are layer-normalized before the attention
step. In contrast to equation 5, Slot Attention uses key and query maps instead of directly forming a dot
product between xnandθk. I.e., the dot products are formed between knandqk.
While the Slot Attention architecture does not explicitly model the mixture parameters πk, it may encode
some weighting in the layer-normalized slots. Indeed, we show in Appendix B that the keys knare contained
in some (D−1)-dimensional affine subspace A⊊ RDwhich may be written uniquely as A=a+VwhereV
is a(D−1)-dimensional linear space and a∈V⊥is perpendicular to V. IfpV:RD→Vis the orthogonal
projection onto Vandpa:RD→⟨a⟩is the orthogonal projection onto the span of a, we may decompose
anyx∈RDorthogonally as x=pV(x) +pa(x). For any key vector kn=k(xn)∈Awe therefore have
kn=a+pV(kn). The attention value γn,kin Slot Attention may now be writen as:
γn,k=exp(k⊤
nqk)/summationtext
k′exp(k⊤nqk′)=exp(a⊤pa(qk)) exp(pV(kn)⊤pV(qk))/summationtext
k′exp(a⊤pa(qk′)) exp(pV(kn)⊤pV(qk′))(6)
Hence, the term exp(a⊤pa(qk))may be interpreted as an analogue of πk, which assigns a weight to the kth
slot but is independent of the input at index n.
3Published in Transactions on Machine Learning Research (09/2024)
M-Step In the maximization step, cluster (slot) parameters are updated using the soft as-
signments Γ. In EM, the new mean directions and mixing coefficients are computed via:
θnew
k:=/summationtextN
n=1γn,kxn/vextenddouble/vextenddouble/vextenddouble/summationtextN
n=1γn,kxn/vextenddouble/vextenddouble/vextenddouble
2(7) πnew
k:=/summationtextN
n=1γn,k
N(8)
In our analogy to Slot Attention, this M-step would relate to the slot-update involving the aggregated val-
uesuk. Hence, it may be of interest in this comparison to investigate whether the values ukhold information
about the right-hand sides of equations (7) and (8).
3.2 Comparing Normalizations in EM Analogy
In the following paragraphs, we discuss how the update normalizations discussed previously compare in
the context of our EM analogy and, in particular, whether the normalized update codes ukhold sufficient
information to recover the quantities from equations (7) and (8).
Weighted Mean In the weighted mean case, the aggregated values ukcan hold sufficient information to
extract the quantities in (7)and(8)ifD=Dinputholds. Assuming that the value map is the identity, the
right-hand side of (7)may be computed as uk/∥uk∥2. However, it is not clear how πkcould be computed
fromuk. Indeed, we show in Proposition 1 that there can be no general formula as for the weighted sum case
that generalizes without exception across slot-counts. We provide a proof in Appendix C by constructing
some explicit slot settings which demonstrate that a hypothetical function fcan not map every update code
to a corresponding unique scalar.
Proposition 1. Consider Slot Attention with weighted mean normalization and any fixed model parameters
and fixed input data ˜x1,...,˜xNwithN≥1. Then, there exists no function f:RD→Rsuch that it holds
f(uk) =/summationtextN
n=1γn,k
N∀1≤k≤K (9)
for arbitrary K≥1, arbitrary slots ˜θ1,...,˜θKand resulting normalized update codes u1,...,uk.
Layer Normalization While, at least in some cases, it is possible to recover the quantity in (7)from
update codes in the layer-normalization variant, these update codes still do not contain sufficient information
to infer the quantity in (8). Indeed, the reader may verify that the same argument we presented in the proof
of Proposition 1 also holds for the layer norm variant.
Weighted Sum In the weighted sum case, we may, as for the weighted mean normalization, obtain the
right-hand side of equation (7)viauk/∥uk∥2if the value map is the identity. In contrast to the previously
discussed normalizations, we may also recover information on the column sums/summationtextN
n=1γn,k, which appear in
equation (8). We make this rigorous in Proposition 2 and provide a proof in Appendix D, where we exploit
the fact that the values vnlie in a lower-dimensional subspace.
Proposition 2. Consider Slot Attention with weighted sum normalization and fixed model parameters. Let
the number of input tokens Nbe fixed. Assume that Dinput =Dholds. For almost all (w.r.t. Lebesgue
measure) parameters of the input’s layernorm module and the value map v, there exists a map f:RD→R
(which may depend on these parameters) such that
f(uk) =/summationtextN
n=1γn,k
N∀1≤k≤K (10)
holds for any K≥1, any slots ˜θ1,...,˜θK, any input data ˜x1,...,˜xN, and the resulting attention matrix Γand
update codes u1,...,uK.
Since the assumptions of Proposition 2 only exclude a parameter subset of zero volume, its conclusion likely
holds in practice during training.
4Published in Transactions on Machine Learning Research (09/2024)
Hence, the weighted sum variant may also be seen as a generalization of the weighted mean normalization:
the update codes from weighted sum normalization hold sufficient information such that weighted-mean-
normalized update codes can be recovered from them (e.g. by the updatenetwork if it has sufficient capacity).
The reverse is not true, as demonstrated in Proposition 1.
4 Methods of Normalization
4.1 Weighted Sum Normalization with Fixed Scaling
As detailed in the above discussion, in contrast to the weighted mean normalization, the weighted sum
normalization may preserve information on the fraction πkof input tokens assigned to the slot in the slot
update codeuk. While (Locatello et al., 2020) report worse performance of the weighted sum normalization
compared to the weighted mean normalization, the value chosen for Cis not further discussed. As detailed
in our experiments, we observe that the weighted sum normalization can outperform the weighted mean
normalization for C=N, whereNis the number of input tokens. For image inputs, the number of tokens
is relatively large, e.g. N= 1282= 16,384for feature maps of CLEVR renderings. We aim to avoid
unreasonably large values in the update code uk, which may lead to numerical instabilities, such as vanishing
gradients. With C=N, it holds that ukis bounded with |uk,d|≤maxn|vn,d|∀d∈{1,...,D}, which is
independent of N. Indeed, we have the following chain of inequalities:
|uk,d|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
NN/summationdisplay
n=1γn,kvn,d/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤1
NN/summationdisplay
n=1γn,k|vn,d|≤1
NN/summationdisplay
n=1γn,kmax
n|vn,d|≤max
n|vn,d| (11)
Here, we first use the triangle inequality, followed by the crude estimate |vn,d|≤maxn|vn,d|for alln. Finally,
we used the fact that/summationtextN
n=1γn,k≤Nholds, since Γis row-stochastic with Nrows. While this provides an
argument for our choice of Cwhich is a suitable heuristics across tasks, we hypothesize that task-specific
tuning of this hyperparameter may be beneficial.
4.2 Weighted Sum Normalization with Batch Scaling
Instead of heuristically choosing a scaling parameter Cin the weighted sum normalization as above, we also
investigate an approach in which the scaling factor is learned via a form of batch normalization (Ioffe &
Szegedy, 2015) during training. Concretely, we measure the magnitude of unnormalized update vectors in the
first Slot Attention iteration of each forward pass by computing their batch statistics. These batch statistics
are used during the subsequent iterations of the forward pass to scale the update codes. While other works
using batch normalization in recurrent networks do not share statistics across time (Cooijmans et al., 2017;
Laurent et al., 2016), we find that our approach greatly simplifies varying the number of iterations during
inference. In contrast to typical implementations of batch normalization, we propose to reduce all axes during
the computation of the statistics (i.e., the batch axis, the slot axis, and the layer axis). Reducing the slot
axis is necessary to preserve slot-permutation equivariance, which is a desirable property in object-centric
learning (Locatello et al., 2020). Reducing the layer axis leads to scalar batch statistics, yielding a method
that more closely aligns with the normalization approaches we have discussed so far.
Assuming that the tensor ˜U(0)∈RL×K×Dholds the unnormalized update codes of the first SA iteration
computed for a mini-batch of size L, we define the batch statistics as:
m:=1
LKDL/summationdisplay
l=1K/summationdisplay
k=1Dslot/summationdisplay
i=1˜U(0)
l,k,iv:=1
LKD−1L/summationdisplay
l=1K/summationdisplay
k=1Dslot/summationdisplay
i=1(˜U(0)
l,k,i−m)2(12)
Note that both statistics are scalar-valued. As proposed by Ioffe & Szegedy (2015), we also learn two
parameters α,β∈Rand normalize the tensor of update codes in iteration jvia:
U(j):=α˜U(j)−m√v+ϵ+β (13)
5Published in Transactions on Machine Learning Research (09/2024)
whereϵ>0is a small constant. We stress that the values mandvare computed for each mini-batch in
the first Slot Attention iteration and are therefore independent of j. Moreover, gradients flow through m
andv. We cache an exponential moving average of the batch statistics during training and use it during
inference. Hence, during inference, the normalization in equation (13)is simply an affine transformation
with fixed weights, thereby closely resembling the weighted sum normalization presented in the previous
subsection. While the weighted sum normalization is linear and not affine, we note that this distinction
does not impact the capacity of the model. Indeed, the vectors ukare also affinely transformed within the
updatenetwork, therefore making any previous affine or linear transformation redundant from a capacity
perspective. Notwithstanding, the normalizations presented here will significantly alter the trainig trajectory
of the models. Batch normalization, in particular, has previously been shown to remedy the problem of
saturating activations and vanishing gradients (Ioffe & Szegedy, 2015; Pascanu et al., 2013), which may
arise from improper normalization (Glorot & Bengio, 2010). We provide pseudocode for the two proposed
normalization variants in Appendix G.
5 Experiments
We investigate the proposed normalizations on unsupervised object discovery tasks. To this end, we train
autoencoders on the CLEVR (Johnson et al., 2017) and MOVi-C (Greff et al., 2022) datasets, utilizing
autoencoder architectures that have been described in (Locatello et al., 2020) and (Seitzer et al., 2023),
respectively. Additional results for a property prediction task can be found in Appendix I. We provide
visualizations of scene segmentations in Appendix H. We will give a brief overview on our experimental setup
in the following and refer to the supplementary material for more details1.
Model Variants We refer to the standard normalization (weighted mean) as the baseline and to the
LayerNorm-based ablation from (Locatello et al., 2020) as the layernormalization. We term the method
detailed in Sec. 4.1 the weighted sum normalization, and the method from Sec. 4.2 the batchnormalization.
In some experiments, we will train models on filtered training sets (CLEVR6 and MOVi-C6) containing
only a limited number of objects. For clarity, we annotate each model variant with a tuple (O,K ), whereO
denotes the maximum number of objects seen in the training set and Kdenotes the number of slot latents
used during training.
CLEVR Dataset We use an extended version of the CLEVR dataset that is provided in the Multi-object
Datasets repository (Kabra et al., 2019). It consists of 100,000 2D renderings of 3D scenes depicting up to 10
objects whose shapes are geometric primitives. Each scene is annotated with a ground truth segmentation,
which we use for evaluation. Following Locatello et al. (2020), we use 70,000 images for training and further
adopt the approach of (Locatello et al., 2020; Greff et al., 2019; Burgess et al., 2019) by cropping the images
to highlight objects in the center. In contrast to (Locatello et al., 2020), we also augment the data during
training via random horizontal flips. As in (Locatello et al., 2020), we also consider a subset of the CLEVR
dataset, only consisting of images containing at most 6 objects. We refer to this dataset as CLEVR6 and will
denote the original dataset by CLEVR10.
MOVi-C Dataset Compared to CLEVR, MOVi-C represents a significant step-up in perceptual complexity.
It contains 10,986 video sequences, each consisting of 24 frames. We use 250 of these video sequences for
validation and hold out 999 sequences for testing. Each clip shows 3 to 10 highly textured 3D-scanned objects
from the Google Scanned Objects repository (Downs et al., 2022) flying into view and colliding. In our
experiments, we only consider single RGB frames from the dataset and discard any temporal relation between
them. Once again, we introduce a filtered dataset, which we denote by MOVi-C6 and which consists of frames
of clips that contain at most 6 objects. For sake of clarity, we refer to the original dataset as MOVi-C10.
MOVi-D Dataset The MOVi-D dataset consists of scenes that are visually similar to those from the MOVi-
C dataset. However, the scenes contain up to 23 (10 to 20 static, 1 to 3 moving) objects, thereby presenting a
greater challenge to object-centric method. Structurally, the dataset resembles MOVi-C, consisting of 11,000
1Code is available at https://github.com/EmbodiedVision/slot_attention_normalization .
6Published in Transactions on Machine Learning Research (09/2024)
video sequences, each made up of 24 frames. As before, we discard any temporal relationship between frames.
In our experiments we will not train on MOVi-D, but instead investigate zero-shot transfer performance of
models that were trained on MOVi-C.
CNN Autoencoder We adopt the convolutional neural network (CNN) based architecture proposed
in (Locatello et al., 2020) to train object-centric autoencoders on the CLEVR dataset. A convolutional
network transforms input images into feature maps, which are enriched by positional embeddings and spatially
flattened. The resulting sets of tokens are processed by the Slot Attention module to obtain object-centric
latent representations. A spatial broadcast decoder (Watters et al., 2019) decodes each slot latent separately
into an image and an unnormalized alpha mask. The alpha masks are normalized across the slot axis via a
softmax operation and subsequently used to linearly combine the reconstructed images, thereby producing
a reconstruction of the input. As in (Locatello et al., 2020), we perform 3 Slot Attention iterations during
training, and 5 iterations during evaluation. We further follow (Locatello et al., 2020) in that we obtain
segmentations from trained autoencoders by assigning each pixel to the slot for which the corresponding
entry in the alpha mask attains a maximum value.
Dinosaur Autoencoder To obtain object-centric behavior on the substantially more complex MOVi-C
dataset, we adopt the approach of Dinosaur (Seitzer et al., 2023). Instead of directly operating on RGB
frames, the autoencoders are trained on image features that are extracted via a pre-trained and fixed
vision transformer (ViT) (Caron et al., 2021). In spirit, the autoencoder resembles the previously discussed
architecture: A small encoder, in the form of a two-layer perceptron, processes the ViT features, which are
then transferred into a latent representation by the Slot Attention module. Each latent is decoded individually
into a reconstruction of the image features and an unnormalized alpha mask. An overall reconstruction of
the ViT features is formed by linearly combining the individual reconstructions via the normalized alpha
masks. We use the MLP-based decoder that is described in (Seitzer et al., 2023). Crucially, the autoencoder
exclusively operates on ViT features, neither receiving RGB frames as input, nor producing them as output.
Hence, the autoencoder’s reconstruction loss is also measured on ViT features, providing a training signal that
is more akin to perceptual similarity than similarity in RGB space. As in the experiments on the CLEVR
dataset, we extract segmentations from the alpha masks. Since the alpha masks (and, correspondingly, the
ViT feature maps) are of a lower resolution than the RGB frames of the MOVi-C dataset, we adopt the
approach of (Seitzer et al., 2023) and bi-linearly upscale the alpha masks before computing segmentations.
Evaluation Following related work (Locatello et al., 2020; Seitzer et al., 2023; Kipf et al., 2022; Greff et al.,
2019), we primarily judge model performance by the quality of foreground segmentations, as measured by the
foreground adjusted Rand index (Rand, 1971; Hubert & Arabie, 1985) (F-ARI). Additionally, we provide
figures regarding the overall segmentation performance when including the background (ARI). All models
are evaluated on 1,280 scenes from the respective test sets. As reconstruction losses are rarely discussed in
related work, we defer the investigation of this metric to Appendix A.
5.1 Object Discovery on CLEVR
In this subsection, we investigate our proposed normalization approaches on an object discovery task on the
CLEVR dataset. In a first set of experiments, we follow the exact training procedure detailed in (Locatello
et al., 2020) and illustrate how the different normalization methods behave as the number of slot latents Kis
changed during inference. The effect of choosing large numbers of slot latents during training is studied in a
second set of experiments. Throughout these experiments, we additionally scrutinize the impact of the object
count on model performance.
Training With 7 Slots In this first set of experiments, we follow (Locatello et al., 2020) as closely as
possible and train the previously described CNN-based architecture on the CLEVR6 dataset with 7 slots. We
compare the baseline and layer normalizations proposed in (Locatello et al., 2020) to the methods discussed
in Section 4. For each variant, we perform 5 training runs with different seeds. The trained models are
evaluated on the CLEVR6 and CLEVR10 test sets.
7Published in Transactions on Machine Learning Research (09/2024)
5 7 911 13 15 17 19 21
# Slots0.850.900.951.00F-ARI
Baseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7)
(a) F-ARI ( ↑) on CLEVR6, number
of slots varies during inference.
5 7 911 13 15 17 19 21
# Slots0.850.900.951.00F-ARI
Baseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7)(b) F-ARI ( ↑) on CLEVR10, num-
ber of slots varies during inference.
3 4 5 6 7 8 9 10
# Objects0.900.920.940.960.981.00F-ARI
Baseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7)(c) F-ARI ( ↑) on CLEVR, number
of objects varies during inference.
Evaluated with 11 slots.
5 7 911 13 15 17 19 21
# Slots0.00.10.20.30.40.5ARIBaseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7)
(d) ARI ( ↑) on CLEVR6
5 7 911 13 15 17 19 21
# Slots0.00.10.20.30.4ARIBaseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7) (e) ARI ( ↑) on CLEVR10
3 4 5 6 7 8 9 10
# Objects0.00.10.20.30.40.5ARIBaseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7) (f) ARI ( ↑) on CLEVR
Figure 1: Dependence of performance on slot and object count. Models are trained on CLEVR6 with 7 slots.
Note the non-zero y-intercept.
Input Baseline Layer Norm Weighted Sum Batch Norm
Figure 2: Qualitative results on a CLEVR10 images, showing reconstructions and (soft) segmentations. The
models are trained on CLEVR6 with 7 slots and evaluated with 21 slots.
The baseline and layer norm variants lead to object-centric behavior for all 5 seeds. For the weighted sum
normalization and the batch norm variant, however, we encounter two runs each in which the autoencoders
decompose the input spatially instead of object-wise. Following (Locatello et al., 2020), we omit these runs in
our analysis.
In Subfigures 1a and 1b, we illustrate how the foreground segmentation performance changes as we vary
the number of slots during evaluation. We note that in the baseline and layer norm variants, segmentation
performance deteriorates whenthey are presented withmore thannineslots, whileour proposednormalizations
appear to generalize well to these changes. In particular, we observe that both of our proposed normalizations
outperform the baseline when the autoencoders are evaluated with 11 slots, as is done in (Locatello et al.,
2020). We show qualitative results of the different model variants at a high slot count in Figure 2 and refer
to Appendix H for more visualizations.
8Published in Transactions on Machine Learning Research (09/2024)
5 7 911 13 15 17 19 21
# Slots0.70.80.91.0F-ARI
Baseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11)
(a) F-ARI ( ↑) on CLEVR6, number
of slots varies during inference.
5 7 911 13 15 17 19 21
# Slots0.70.80.91.0F-ARI
Baseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11)(b) F-ARI ( ↑) on CLEVR10, num-
ber of slots varies during inference.
3 4 5 6 7 8 9 10
# Objects0.70.80.91.0F-ARI
Baseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11)(c) F-ARI ( ↑) on CLEVR, object count
varies during inference. Evaluated
with 11 slots.
5 7 911 13 15 17 19 21
# Slots0.000.250.500.751.00ARIBaseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11)
(d) ARI ( ↑) on CLEVR6
5 7 911 13 15 17 19 21
# Slots0.000.250.500.751.00ARIBaseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11) (e) ARI ( ↑) on CLEVR10
3 4 5 6 7 8 9 10
# Objects0.00.20.40.60.81.0ARIBaseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11) (f) ARI ( ↑) on CLEVR
Figure 3: Dependence of segmentation performance on slot and object count. Models are trained on CLEVR6
with 11 slots.
We study how performance depends on the number of objects in Subfigure 1c. Here, we evaluate each model
variant with 11 slot latents on the CLEVR10 test set and plot average foreground segmentation performance
dependent on the object count. Overall, we observe that our proposed normalizations outperform the baseline,
independently of the object count. Also note that, across all model variants, segmentation performance trends
downwards as the number of objects in the scene increases.
Subfigures 1d-1f illustrate the behavior of the overall segmentation performance when background pixels are
taken into consideration. It appears that our proposed methods outperform the other two variants w.r.t. this
metric, although the variablity across runs is large.
Excess Slots During Training While we have so far only discussed experiments in which we increase the
number of slot latents during inference, we will now outline an experiment in which the Slot Attention module
is also provided with excess slots during training: Concretely, we train the autoencoders on the CLEVR6
dataset with 11 slot latents. We annotate the resulting variants with the tuple (6, 11) to underline that they
were trained with 11 slots on scenes consisting of at most 6 objects. To limit computational expenses, we
perform only three runs per model variant. We observe object-centric behavior in all runs for the baseline,
the layer norm variant, and the weighted sum variant. For the batch normalization, we encounter one run
in which the scenes are deconstructed spatially in vertical stripes. As before, we exclude this run from our
analysis and are therefore left with only two runs for this variant. Considering the breadth of our other
experiments and the small variability observed in this experiment, we deem this loss of information acceptable.
In this setting, the studied variants seem to perform more comparably than before w.r.t. foreground
segmentation performance (Subfigures 3a-3c). Notwithstanding, we again note that the performance of the
baseline and layer norm variants starts to suffer as we add additional slot latents during inference. In contrast,
the performance of the two proposed variants remains more stable. In general, the foreground segmentation
performance is lower than during training with few slots (Subfigures 1a-1c). All models trained with our
proposed methods learn to segment the background into a single slot, leading to high overall segmentation
performance (Subfigures 3d-3f). One model using the baseline normalization exhibits this behavior, while
none of the models using layer normalization do so.
9Published in Transactions on Machine Learning Research (09/2024)
5 7 911 13 15 17 19 21
# Slots0.550.600.650.700.750.80F-ARI
Baseline (10, 11)
Layer Norm (10, 11)
Weighted Sum (10, 11)
Batch Norm (10, 11)
(a) F-ARI ( ↑) on MOVi-C6, number
of slots varies during inference.
5 7 911 13 15 17 19 21
# Slots0.550.600.650.700.750.80F-ARI
Baseline (10, 11)
Layer Norm (10, 11)
Weighted Sum (10, 11)
Batch Norm (10, 11)(b) F-ARI ( ↑) on MOVi-C10, num-
ber of slots varies during inference.
3 4 5 6 7 8 9 10
# Objects0.550.600.650.700.750.80F-ARI
Baseline (10, 11)
Layer Norm (10, 11)
Weighted Sum (10, 11)
Batch Norm (10, 11)(c) F-ARI ( ↑) on MOVi-C, object
count varies during inference. Eval-
uated with 11 slots.
5 7 911 13 15 17 19 21
# Slots0.00.10.20.3ARI
Baseline (10, 11)
Layer Norm (10, 11)
Weighted Sum (10, 11)
Batch Norm (10, 11)
(d) ARI ( ↑) on MOVi-C6
5 7 911 13 15 17 19 21
# Slots0.00.10.20.3ARI
Baseline (10, 11)
Layer Norm (10, 11)
Weighted Sum (10, 11)
Batch Norm (10, 11) (e) ARI ( ↑) on MOVi-C10
3 4 5 6 7 8 9 10
# Objects0.00.10.20.3ARI
Baseline (10, 11)
Layer Norm (10, 11)
Weighted Sum (10, 11)
Batch Norm (10, 11) (f) ARI ( ↑) on MOVi-C
Figure 4: Dependence of segmentation performance on slot and object count. Models are trained on MOVi-C10
with 11 slots.
5.2 Object Discovery on MOVi-C
To further support the validity of our proposed normalizations, we run additional experiments, using the
Dinosaur (Seitzer et al., 2023) framework. As previously discussed, we train the MLP-based architecture
described in (Seitzer et al., 2023) on the MOVi-C dataset. While it still is a synthetic dataset, this represents
a significant step-up in complexity compared to CLEVR, approaching the complexity of real-world scenes.
Training on MOVi-C10 In our first set of experiments, we closely follow the setup detailed in (Seitzer
et al., 2023) and train the autoencoders on the full MOVi-C10 dataset, using 11 slots. As in the previous
subsection, we investigate how the foreground segmentation performance develops as we vary the number of
slot latents during inference. For each model variant, we perform 5 training runs with different seeds. For
sake of consistency with the other experiments, we evaluate the trained models both on the MOVi-C10 test
set and on the filtered MOVi-C6 dataset.
In Subfigures 4a and 4b, we observe that our proposed normalizations generally lead to improved foreground
segmentation performance compared to the baseline and layer normalization across all slot counts. In
particular, both proposed normalizations outperform the baseline and layer normalization variant with 11
slots on the MOVi-C10 dataset, as can be observed in Subfigures 4a and 4b. As in our previous experiments,
we note that foreground segmentation performance starts to suffer as the baseline variant is provided with
excess slots during inference. While our proposed methods exhibit a similar behavior in these experiments,
we note that the deterioration progresses at a slower rate. Additionally, we observe in Subfigure 4c that
performance is improved across smaller object counts.
The behavior of overall segmentation performance is shown in Subfigures 4d-4f. In line with our previous
observations, we note that performance suffers for all variants when excess slots are present during inference.
While baseline, layer normalization and batch normalization yield comparable results w.r.t. this metric, the
weighted sum variant performs noticably better.
Training on MOVi-C6 While the authors of (Seitzer et al., 2023) trained autoencoders exclusively on
the MOVi-C10 dataset, we will also investigate an approach that resembles the one described in (Locatello
et al., 2020), and which we adopted in Subsection 5.1. Namely, we train models on the filtered MOVi-C6
dataset with 7 slots and subsequently evaluate them on both MOVi-C6 and MOVi-C10. This approach may
10Published in Transactions on Machine Learning Research (09/2024)
5 7 911 13 15 17 19 21
# Slots0.550.600.650.700.750.800.85F-ARI
Baseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7)
(a) F-ARI ( ↑) on MOVi-C6, number
of slots varies during inference.
5 7 911 13 15 17 19 21
# Slots0.550.600.650.700.750.800.85F-ARI
Baseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7)(b) F-ARI ( ↑) on MOVi-C10, num-
ber of slots varies during inference.
3 4 5 6 7 8 9 10
# Objects0.550.600.650.700.750.800.85F-ARI
Baseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7)(c) F-ARI ( ↑) on MOVi-C, ob-
ject count varies during evaluation.
Scenes are evaluated with 11 slots.
5 7 911 13 15 17 19 21
# Slots0.00.10.20.30.40.5ARI
Baseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7)
(d) ARI ( ↑) on MOVi-C6
5 7 911 13 15 17 19 21
# Slots0.00.10.20.30.40.5ARI
Baseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7) (e) ARI ( ↑) on MOVi-C10
3 4 5 6 7 8 9 10
# Objects0.00.10.20.30.40.5ARI
Baseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7) (f) ARI ( ↑) on MOVi-C
Figure 5: Dependence of segmentation performance on slot and object count. Models are trained on MOVi-C6
with 7 slots.
be particularly interesting to practitioners, as reducing the number of slot latents during training can serve
to greatly reduce computational effort. To limit computational expenses, we again only perform three runs
per model variant.
We illustrate in Subfigures 5a-5c the behavior of the foreground segmentation performance. As in our previous
experiments, we observe that both of our proposed normalizations outperform the baseline when evaluated
on the MOVi-C10 test set with 11 slots. Interestingly, it can be noted that performance also improves over
the models trained on the MOVi-C10 dataset with 11 slots (Figure 4). Again, we point out that excess slot
latents during inference lead to a substantial deterioration of foreground segmentation performance in the
baseline and layer norm models.
In Subfigures 5d-5f, we plot the behavior of overall segmentation quality. Compared to the previous experiment,
varying slot count has a lesser impact on overall segmentation performance for all methods. While the
differences seem unsubstantial, the baseline appears to perform best w.r.t. this metric.
Excess Slots During Training In line with the experiments on the CLEVR dataset, we turn to a set of
experiments that investigates the impact of excess slots during training. Similar to the corresponding setup in
Subsection 5.1, we train the models on the filtered MOVi-C6 dataset, but provide them with 11 slots during
training. We generally observe that both the weighted sum normalization and the batch normalization lead
to improved foreground segmentations compared to the baseline and layer normalization, especially at higher
slot count, as can be concluded from Subfigures 6a and 6b. Subfigure 6c additionally demonstrates once
again that our proposed normalizations appear to perform at least as well as the baseline across all object
counts. In Subfigures 6d-6f we find that increased slot count during inference harms overall segmentation
quality. The weighted sum variant performs best, although the variablity in ARI appears large.
Evaluation on MOVi-D We now investigate how the previously described models (which were trained on
MOVi-C) transfer to the MOVi-D dataset. We recall that scenes of the MOVi-D dataset may contain up to
23 objects. Hence, the MOVi-D dataset allows us to study how the trained models behave when slot- and
object-count are increased significantly during inference. We evaluate the models with 24 slots. In Table 1,
we show the MOVi-D zero-shot performance of all 12 model variants we trained on MOVi-C. The batch norm
variant performs best w.r.t. our main metric, the F-ARI. This holds true both when comparing all 12 variants
to each other, and when considering the subsets of models obtained by only considering variants with any fixed
11Published in Transactions on Machine Learning Research (09/2024)
5 7 911 13 15 17 19 21
# Slots0.550.600.650.700.750.80F-ARI
Baseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11)
(a) F-ARI ( ↑) on MOVi-C6, number
of slots varies during inference.
5 7 911 13 15 17 19 21
# Slots0.550.600.650.700.750.80F-ARI
Baseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11)(b) F-ARI ( ↑) on MOVi-C10, num-
ber of slots varies during inference.
3 4 5 6 7 8 9 10
# Objects0.550.600.650.700.750.80F-ARI
Baseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11)(c) F-ARI ( ↑) on MOVi-C set, ob-
ject count varies during inferences.
Evaluated with 11 slots.
5 7 911 13 15 17 19 21
# Slots0.00.10.20.3ARI
Baseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11)
(d) ARI ( ↑) on MOVi-C6
5 7 911 13 15 17 19 21
# Slots0.00.10.20.3ARI
Baseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11) (e) ARI ( ↑) on MOVi-C10
3 4 5 6 7 8 9 10
# Objects0.00.10.20.3ARI
Baseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11) (f) ARI ( ↑) on MOVi-C
Figure 6: Dependence of segmentation performance on slot and object count. Models are trained on MOVi-C6
with 11 slots.
annotation (O,K ). Compared to their batch normalization counterparts, the weighted sum models perform
similarly w.r.t. foreground ARI, performing only slightly worse. Both the weighted sum normalization and
the batch normalization produce markedly better foreground segmentations than the baseline and layer norm
variants. With respect to overall segmentation quality (ARI), none of the normalization variants consistently
outperform the others, which is in line with our observations on MOVi-C.
For any fixed normalization method, the (6,7)variant performs better w.r.t. F-ARI than the (10,11)and
(6,11)variants. This illustrates that, firstly, training on filtered training sets with few objects can improve
performance during inference on scenes with many objects; secondly, avoiding excess slots during training
appears to be important, which is an observation that has been made in (Locatello et al., 2020) and which
we also note when comparing Figures 1 and 3. This effect underlines the importance of strong slot-count
generalization capabilities. We posit that training at low object- and slot-counts reduces the number of
"unoccupied" slots during training, thereby tightening the representational slot-bottleneck, which has been
postulated to encourage object-centricness (Locatello et al., 2020; Stange et al., 2023).
6 Related Work
In this work, we follow a longer tradition of using autoencoders to obtain semantic scene decompositions (Greff
et al., 2016; 2017; 2019; Crawford & Pineau, 2019; Burgess et al., 2019; Lin et al., 2020; Engelcke et al., 2020;
Locatello et al., 2020). An early work in this area is Neural-EM (Greff et al., 2017), which performs expectation
maximization at the image level. It is proceeded by IODINE (Greff et al., 2019) and MONet (Burgess et al.,
2019), which learn variational autoencoders. Slot Attention (Locatello et al., 2020) has already been used in
many derivative works to scale object-centric learning to increasingly complex datasets. In particular, motion
cues (Kipf et al., 2022; Elsayed et al., 2022; Wu et al., 2023), high-level image features (Seitzer et al., 2023)
and powerful decoder models (Singh et al., 2022a;b) were found to be useful for obtaining desired behavior
on real-world datasets.
Several previous works have discussed generalization capabilities of object-centric representations (Dittadi
et al., 2022; Seitzer et al., 2023). That the number of slot latents has influence on model performance has been
noted by Seitzer et al. (2023) where the authors illustrate that varying the number of slots has a significant
impact on segmentation quality, determining whether objects are split into constituent parts or discovered in
12Published in Transactions on Machine Learning Research (09/2024)
Variant F-ARI (↑)ℓ2loss (↓) ARI (↑)
Baseline (10, 11) 0.647 ±0.015 2.143 ±0.0040.172 ±0.011
Layer Norm (10, 11) 0.660 ±0.0122.146 ±0.0020.171 ±0.004
Weighted Sum (10, 11) 0.722 ±0.0052.223 ±0.0090.204 ±0.004
Batch Norm (10, 11) 0.725 ±0.0062.149 ±0.0040.182 ±0.011
Baseline (6, 11) 0.640 ±0.0092.220 ±0.0010.176 ±0.003
Layer Norm (6, 11) 0.639 ±0.0112.220 ±0.0040.176 ±0.005
Weighted Sum (6, 11) 0.709 ±0.0442.309 ±0.0140.205 ±0.147
Batch Norm (6, 11) 0.711 ±0.0052.219 ±0.0060.175 ±0.007
Baseline (6, 7) 0.741 ±0.0052.370 ±0.0090.253 ±0.018
Layer Norm (6, 7) 0.742 ±0.0192.365 ±0.0070.253 ±0.007
Weighted Sum (6, 7) 0.797 ±0.0232.475 ±0.0380.242 ±0.156
Batch Norm (6, 7) 0.809 ±0.0052.374 ±0.002 0.297 ±0.030
Table 1: Zero-shot performance on MOVi-D of models trained on MOVi-C. Evaluated with 24 slots. We show
the median ±maximum deviation across multiple runs. For each metric, we underline the most advantagous
variant in each section and mark the most advantagous variant across all sections in bold.
their entirety. To date, only few works investigate the role of attention normalization in the performance of
Slot Attention. The first ablation evaluated for Slot Attention in the original paper (Locatello et al., 2020) is
almost identical to the approach we discuss in Subsection 4.1, differing from our method crucially in that the
weighted sums are not scaled by a constant, which appears to result in poor training behavior. In a second
ablation, the authors modify the first ablation by normalizing the update codes via layer normalization (Ba
et al., 2016). Comparable performance is reported to the original method.
Normalization in Slot Attention has been investigated in (Zhang et al., 2023), where the authors propose
to use the Sinkhorn-Knopp iteration to normalize attention matrices. In contrast to our work, they do not
investigate the impact on generalization capabilities and substantially increase the complexity of the SA
module. More broadly, the use of the Sinkhorn-Knopp algorithm for normalization in multi-head attention
modules has been scrutinized before by Sander et al. (2022).
7 Conclusion
Allowing models to dynamically find suitable levels of segmentation coarseness is an important problem
in unsupervised object-centric representation learning. In Slot Attention it has been found that excess
slots oftentimes split objects into parts or distribute responsibility for individual pixels among several slots.
Hence, finding a reasonable number of slots has been crucial during training and inference. In this work,
we discussed approaches for making the Slot Attention module more robust with respect to this choice. We
studied normalizations of the slot-update vectors and analysed how they impact Slot Attention’s ability to
scale to different numbers of slots and objects during inference. On the theoretical side, we motivated this
phenomenon via an analogy between Slot Attention and parameter estimation in vMF mixture models. In
experiments, we demonstrated that our proposed normalization schemes increase the generalization capability
of Slot Attention to varying number of slots and objects during inference. With these insights, we hope to
contribute to increase performance of numerous existing and future applications of Slot Attention.
Acknowledgement
This work was supported by Max Planck Society and Cyber Valley. The authors thank the International
Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Jan Achterhold.
13Published in Transactions on Machine Learning Research (09/2024)
References
Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450,
2016. URL http://arxiv.org/abs/1607.06450 .
Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, and Suvrit Sra. Generative model-based clustering
of directional data. In Lise Getoor, Ted E. Senator, Pedro M. Domingos, and Christos Faloutsos (eds.),
Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, Washington, DC, USA, August 24 - 27, 2003 , pp. 19–28. ACM, 2003. doi: 10.1145/956750.956757.
URL https://doi.org/10.1145/956750.956757 .
Christopher M. Bishop. Pattern Recognition and Machine Learning . Springer, 2006.
Christopher P. Burgess, Loïc Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matthew M.
Botvinick, and Alexander Lerchner. MONet: Unsupervised scene decomposition and representation. CoRR,
abs/1901.11390, 2019. URL http://arxiv.org/abs/1901.11390 .
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand
Joulin. Emerging properties in self-supervised vision transformers. In International Conference on
Computer Vision (ICCV) , pp. 9630–9640. IEEE, 2021. doi: 10.1109/ICCV48922.2021.00951. URL
https://doi.org/10.1109/ICCV48922.2021.00951 .
Michael Chang, Thomas L. Griffiths, and Sergey Levine. Object representations as fixed points: Training
iterative refinement algorithms with implicit differentiation. In Alice H. Oh, Alekh Agarwal, Danielle
Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems (NeurIPS) ,
2022a. URL https://openreview.net/forum?id=-5rFUTO2NWe .
Michael Chang, Sergey Levine, and Thomas L. Griffiths. Object-centric learning as nested optimization.
InICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality , 2022b. URL
https://openreview.net/forum?id=BCzevBOL5g9 .
Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of
neural machine translation: Encoder-decoder approaches. In Dekai Wu, Marine Carpuat, Xavier Carreras,
and Eva Maria Vecchi (eds.), Workshop on Syntax, Semantics and Structure in Statistical Translation
(SSST@EMNLP) , pp. 103–111. Association for Computational Linguistics, 2014. doi: 10.3115/v1/W14-4012.
URL https://aclanthology.org/W14-4012/ .
Tim Cooijmans, Nicolas Ballas, César Laurent, Çaglar Gülçehre, and Aaron C. Courville. Recurrent batch
normalization. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 2017. URL https://openreview.net/
forum?id=r1VdcHcxx .
Eric Crawford and Joelle Pineau. Spatially invariant unsupervised object detection with convolutional neural
networks. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First
Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium
on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27
- February 1, 2019 , pp. 3412–3420. AAAI Press, 2019. doi: 10.1609/AAAI.V33I01.33013412. URL
https://doi.org/10.1609/aaai.v33i01.33013412 .
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Series B (Methodological) , 39(1):1–38, 1977. ISSN
00359246. URL http://www.jstor.org/stable/2984875 .
Andrea Dittadi, Samuele S. Papa, Michele De Vita, Bernhard Schölkopf, Ole Winther, and Francesco Locatello.
Generalization and robustness implications in object-centric learning. In Kamalika Chaudhuri, Stefanie
Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (eds.), International Conference on
Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA , volume 162 of Proceedings of
Machine Learning Research , pp. 5221–5285. PMLR, 2022. URL https://proceedings.mlr.press/v162/
dittadi22a.html .
14Published in Transactions on Machine Learning Research (09/2024)
Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B.
McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of 3D scanned household
items. In International Conference on Robotics and Automation (ICRA) , pp. 2553–2560. IEEE, 2022. doi:
10.1109/ICRA46639.2022.9811809. URL https://doi.org/10.1109/ICRA46639.2022.9811809 .
Gamaleldin Elsayed, Aravindh Mahendran, Sjoerd van Steenkiste, Klaus Greff, Michael C Mozer, and Thomas
Kipf. Savi++: Towards end-to-end object-centric learning from real-world videos. In S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems ,
volume 35, pp. 28940–28954. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/
paper_files/paper/2022/file/ba1a6ba05319e410f0673f8477a871e3-Paper-Conference.pdf .
Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. GENESIS: generative scene
inference and sampling with object-centric latent representations. In International Conference on Learning
Representations (ICLR) . OpenReview.net, 2020. URL https://openreview.net/forum?id=BkxfaTVFwH .
Ronald Aylmer Fisher. Dispersion on a sphere. Proceedings of the Royal Society of London. Series A.
Mathematical and Physical Sciences , 217(1130):295–305, 1953. doi: 10.1098/rspa.1953.0064. URL https:
//royalsocietypublishing.org/doi/abs/10.1098/rspa.1953.0064 .
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Yee Whye Teh and D. Mike Titterington (eds.), Proceedings of the Thirteenth International Conference
on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15,
2010, volume 9 of JMLR Proceedings , pp. 249–256. JMLR.org, 2010. URL http://proceedings.mlr.
press/v9/glorot10a.html .
Klaus Greff, Antti Rasmus, Mathias Berglund, Tele Hotloo Hao, Harri Valpola, and Jürgen Schmidhu-
ber. Tagger: Deep unsupervised perceptual grouping. In Daniel D. Lee, Masashi Sugiyama, Ulrike
von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing
Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain , pp. 4484–4492, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/
01eee509ee2f68dc6014898c309e86bf-Abstract.html .
Klaus Greff, Sjoerd van Steenkiste, and Jürgen Schmidhuber. Neural expectation maximiza-
tion. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus,
S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Sys-
tems (NeurIPS) , pp. 6691–6701, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
d2cd33e9c0236a8c2d8bd3fa91ad3acf-Abstract.html .
Klaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Chris Burgess, Daniel Zoran, Loic
Matthey, Matthew M. Botvinick, and Alexander Lerchner. Multi-object representation learning with
iterative variational inference. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), International
Conference on Machine Learning (ICML) , volume 97 of Proceedings of Machine Learning Research , pp.
2424–2433. PMLR, 2019. URL http://proceedings.mlr.press/v97/greff19a.html .
Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J. Fleet,
Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun,
Issam H. Laradji, Hsueh-Ti Derek Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, A. Cengiz
Öztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent
Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng
Zhong, and Andrea Tagliasacchi. Kubric: A scalable dataset generator. In Conference on Computer Vision
and Pattern Recognition (CVPR) , pp. 3739–3751. IEEE, 2022. doi: 10.1109/CVPR52688.2022.00373. URL
https://doi.org/10.1109/CVPR52688.2022.00373 .
Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of Classification , 2(1):193–218, Dec
1985. ISSN 1432-1343. doi: 10.1007/BF01908075. URL https://doi.org/10.1007/BF01908075 .
15Published in Transactions on Machine Learning Research (09/2024)
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , volume 37 of JMLR Workshop
and Conference Proceedings , pp. 448–456. JMLR.org, 2015. URL http://proceedings.mlr.press/v37/
ioffe15.html .
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B.
Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In
Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 1988–1997. IEEE Computer Society,
2017. doi: 10.1109/CVPR.2017.215. URL https://doi.org/10.1109/CVPR.2017.215 .
Rishabh Kabra, Chris Burgess, Loic Matthey, Raphael Lopez Kaufman, Klaus Greff, Malcolm Reynolds, and
Alexander Lerchner. Multi-object datasets. https://github.com/deepmind/multi_object_datasets/ ,
2019. Accessed 2023-05-12.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio
and Yann LeCun (eds.), International Conference on Learning Representations (ICLR) , 2015. URL
http://arxiv.org/abs/1412.6980 .
Thomas Kipf, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold,
Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional object-centric learning from video.
InInternational Conference on Learning Representations , 2022. URL https://openreview.net/forum?
id=aD7uesX1GF_ .
Daniil Kirilenko, Alexey Kovalev, and Aleksandr Panov. Object-centric learning with slot mixture models.
https://openreview.net/forum?id=AqX3oSbzyQ1 , 2023. URL https://openreview.net/forum?id=
AqX3oSbzyQ1 .
César Laurent, Gabriel Pereyra, Philemon Brakel, Ying Zhang, and Yoshua Bengio. Batch normalized
recurrent neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal
Processing, ICASSP 2016, Shanghai, China, March 20-25, 2016 , pp. 2657–2661. IEEE, 2016. doi: 10.1109/
ICASSP.2016.7472159. URL https://doi.org/10.1109/ICASSP.2016.7472159 .
Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, and
Sungjin Ahn. SPACE: unsupervised object-oriented scene representation via spatial attention and decom-
position. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview.net/forum?id=rkl03ySYDH .
Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob
Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances
in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/
2020/hash/8511df98c02ab60aea1b2356c013bc0f-Abstract.html .
Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based neural
machine translation. In Lluís Màrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton
(eds.),Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP
2015, Lisbon, Portugal, September 17-21, 2015 , pp. 1412–1421. The Association for Computational
Linguistics, 2015. doi: 10.18653/v1/d15-1166. URL https://doi.org/10.18653/v1/d15-1166 .
Razvan Pascanu, Tomás Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks.
InProceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA,
16-21 June 2013 , volume 28 of JMLR Workshop and Conference Proceedings , pp. 1310–1318. JMLR.org,
2013. URL http://proceedings.mlr.press/v28/pascanu13.html .
William M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American
Statistical Association , 66(336):846–850, 1971. ISSN 01621459. URL http://www.jstor.org/stable/
2284239.
16Published in Transactions on Machine Learning Research (09/2024)
Michael E. Sander, Pierre Ablin, Mathieu Blondel, and Gabriel Peyré. Sinkformers: Transformers with
doubly stochastic attention. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera (eds.),
International Conference on Artificial Intelligence and Statistics, AISTATS 2022, 28-30 March 2022,
Virtual Event , volume 151 of Proceedings of Machine Learning Research , pp. 3515–3530. PMLR, 2022. URL
https://proceedings.mlr.press/v151/sander22a.html .
Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann Simon-
Gabriel, Tong He, Zheng Zhang, Bernhard Schölkopf, Thomas Brox, and Francesco Locatello. Bridging
the gap to real-world object-centric learning. In The Eleventh International Conference on Learning
Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023. URL https:
//openreview.net/pdf?id=b9tUk-f_aG .
Gautam Singh, Fei Deng, and Sungjin Ahn. Illiterate DALL-E learns to compose. In The Tenth International
Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net,
2022a. URL https://openreview.net/forum?id=h0OYV0We3oh .
Gautam Singh, Yi-Fu Wu, and Sungjin Ahn. Simple unsupervised object-centric learning for complex
and naturalistic videos. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho,
and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, Novem-
ber 28 - December 9, 2022 , 2022b. URL http://papers.nips.cc/paper_files/paper/2022/hash/
735c847a07bf6dd4486ca1ace242a88c-Abstract-Conference.html .
Andrew Stange, Robert Lo, Abishek Sridhar, and Kousik Rajesh. Exploring the role of the bottleneck in
slot-based models through covariance regularization. CoRR, abs/2306.02577, 2023.
Nicholas Watters, Loïc Matthey, Christopher P. Burgess, and Alexander Lerchner. Spatial broadcast decoder:
A simple architecture for learning disentangled representations in VAEs. CoRR, abs/1901.07017, 2019.
URL http://arxiv.org/abs/1901.07017 .
Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models , 2019.
Accessed: 2023-05-24.
Ziyi Wu, Nikita Dvornik, Klaus Greff, Thomas Kipf, and Animesh Garg. Slotformer: Unsupervised visual
dynamics simulation with object-centric models. In The Eleventh International Conference on Learning
Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023. URL https:
//openreview.net/pdf?id=TFbwV6I0VLg .
Yan Zhang, David W. Zhang, Simon Lacoste-Julien, Gertjan J. Burghouts, and Cees G. M. Snoek. Unlocking
slot attention by changing optimal transport costs. In Andreas Krause, Emma Brunskill, Kyunghyun
Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on
Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of Proceedings of
Machine Learning Research , pp. 41931–41951. PMLR, 2023. URL https://proceedings.mlr.press/
v202/zhang23ba.html .
17Published in Transactions on Machine Learning Research (09/2024)
A Reconstruction Loss
In this subsection, we investigate how the ℓ2reconstruction loss is impacted by the choice of normalization.
As in previous figures, we explore how this objective varies as object and slot-count is varied during inference.
For each experiment provided in the main paper we provide a corresponding figure on the ℓ2loss.
CLEVR (6, 7) We first consider the experiment in which we trained a model with 7 slots on CLEVR6.
We note in Subfigures 7a and 7b that all normalization approaches suffer under excess slots during inference.
The layer norm variant generally performs best in this context, while the baseline and weighted sum variants
perform comparably to each other. In Figure 7c, we note that reconstruction quality decreases with increasing
object count for all variants. It can be observed that this deterioration progresses fastest for the baseline,
slowest with batch normalization, and at a seemingly similar rate for the layer norm and weighted sum
variants.
5 7 911 13 15 17 19 21
# Slots0.0000.0020.0040.006Reconstruction LossBaseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7)
(a) Reconstruction loss ( ↓) on
CLEVR6
5 7 911 13 15 17 19 21
# Slots0.00000.00250.00500.00750.0100Reconstruction LossBaseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7)(b) Reconstruction loss ( ↓) on
CLEVR10
3 4 5 6 7 8 9 10
# Objects0.0000.0020.0040.0060.008Reconstruction LossBaseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7)(c) Reconstruction loss ( ↓) on
CLEVR
Figure 7: Dependence of reconstruction quality on slot and object count. Models are trained on CLEVR6
with 7 slots. Note the non-zero y-intercept.
CLEVR (6, 11) Figure 8 illustrates the behavior of reconstruction losses for models trained on CLEVR6
with excess slots. We find that, analogous to our observations in Subsection 5.1, varying slot count during
inference has a lesser effect than in the previous experiment (compare Subfigures 8a and 8b to Subfigures 7a
and 7b). Generally, the weighted sum variant has the highest reconstruction loss, while the layer norm variant
has the lowest. The baseline and the batch norm variant perform comparably. As before, we observe in
Subfigure 8c that reconstruction loss increases with increasing object count.
5 7 911 13 15 17 19 21
# Slots0.0000.0020.0040.0060.008Reconstruction LossBaseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11)
(a) ℓ2loss ( ↓) on CLEVR6
5 7 911 13 15 17 19 21
# Slots0.0000.0050.0100.0150.020Reconstruction LossBaseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11) (b) ℓ2loss ( ↓) on CLEVR10
3 4 5 6 7 8 9 10
# Objects0.0000.0010.0020.0030.0040.005Reconstruction LossBaseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11) (c) ℓ2loss ( ↓) on CLEVR
Figure 8: Dependence of reconstruction quality on slot and object count. Models are trained on CLEVR6
with 11 slots.
Training on MOVi-C We now shift our focus to the Dinosaur models. Figure 9 shows the reconstruction
loss for the models trained on MOVi-C10 with 11 slots. Contrary to previous observations, we note in
Subfigures 9a and 9c that reconstruction losses improve as the slot count increases. Generally, it appears that
the weighted sum variant performs worst w.r.t. the ℓ2loss, while the other variants perform comparably to
each other. We observe analogous behavior in Figures 10 and 11.
18Published in Transactions on Machine Learning Research (09/2024)
5 7 911 13 15 17 19 21
# Slots1.61.82.02.22.4Reconstruction LossBaseline (10, 11)
Layer Norm (10, 11)
Weighted Sum (10, 11)
Batch Norm (10, 11)
(a) ℓ2loss ( ↓) on MOVi-C6
5 7 911 13 15 17 19 21
# Slots1.61.82.02.22.4Reconstruction LossBaseline (10, 11)
Layer Norm (10, 11)
Weighted Sum (10, 11)
Batch Norm (10, 11) (b) ℓ2loss ( ↓) on MOVi-C10
3 4 5 6 7 8 9 10
# Objects1.61.82.02.22.4Reconstruction LossBaseline (10, 11)
Layer Norm (10, 11)
Weighted Sum (10, 11)
Batch Norm (10, 11) (c) ℓ2loss ( ↓) on MOVi-C
Figure 9: Dependence of reconstruction quality on slot and object count. Models are trained on MOVi-C10
with 11 slots.
5 7 911 13 15 17 19 21
# Slots1.61.82.02.22.4Reconstruction LossBaseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7)
(a) ℓ2loss ( ↓) on MOVi-C6
5 7 911 13 15 17 19 21
# Slots1.61.82.02.22.4Reconstruction LossBaseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7) (b) ℓ2loss ( ↓) on MOVi-C10
3 4 5 6 7 8 9 10
# Objects1.61.82.02.22.4Reconstruction LossBaseline (6, 7)
Layer Norm (6, 7)
Weighted Sum (6, 7)
Batch Norm (6, 7) (c) ℓ2loss ( ↓) on MOVi-C
Figure 10: Dependence of reconstruction quality on slot and object count. Models are trained on MOVi-C6
with 7 slots.
5 7 911 13 15 17 19 21
# Slots1.61.82.02.22.4Reconstruction LossBaseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11)
(a) ℓ2loss ( ↓) on MOVi-C6
5 7 911 13 15 17 19 21
# Slots1.61.82.02.22.4Reconstruction LossBaseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11) (b) ℓ2loss ( ↓) on MOVi-C10
3 4 5 6 7 8 9 10
# Objects1.61.82.02.22.4Reconstruction LossBaseline (6, 11)
Layer Norm (6, 11)
Weighted Sum (6, 11)
Batch Norm (6, 11) (c) ℓ2loss ( ↓) on MOVi-C
Figure 11: Dependence of reconstruction quality on slot and object count. Models are trained on MOVi-C6
with 11 slots.
19Published in Transactions on Machine Learning Research (09/2024)
B Lemmata on Layer Normalization
In this subsection, we will present some basic lemmata on the LayerNorm module which we use in Subsection D.
For˜xn∈RDinput,LayerNorm( ˜xn)refers to the following operation:
LayerNorm( ˜xn) := diag(α)˜xn−E[˜xn] 1/radicalbig
Var[˜xn] +ϵ+β (14)
whereα,β∈RDare learnable parameters, ϵ > 0is a constant, 1is the all-ones vec-
tor, and E[˜xk]and Var[˜xk]are the expectation and variance of the entries of ˜xk, respectively:
E[˜xn] :=1
D˜x⊤
n 1 (15) Var[˜xn] :=1
D∥˜xn−E[˜xn] 1∥2
2 (16)
Given these definitions, we present the following lemma, giving an expression for the affine hull of the image
of the layer normalization module.
Lemma 1. The vector ˜xk−E[˜xk] 1is the orthogonal projection of ˜xkonto the orthogonal complement of the
span of the all-ones vector ⟨ 1⟩⊥⊊ RDinput. Hence, the image of LayerNorm is contained in a (Dinput−1)-
dimensional affine subspace of RDinput. More concretely, the affine hull of the image of a LayerNorm module
with parameters α,β∈RDinputis given via:
aff/parenleftbig
LayerNorm/bracketleftbig
RDinput/bracketrightbig/parenrightbig
= diag(α) 1⊥+β (17)
Here and in the following, the left-multiplication of the diagonal matrix diag(α)with any set (e.g. the vector
space 1⊥) refers to the image of that set under the left-multiplication.
Proof.Definee1:= 1/√
D 1and extend to an orthonormal basis e1,...,eDinputvia Gram-Schmidt. Then we
have⟨ 1⟩⊥=⟨e2,...,eDinput⟩and for an arbitrary ˜xnwe have the orthogonal decomposition:
˜xn=Dinput/summationdisplay
d=1⟨˜xn,ed⟩ed (18)
The orthogonal projection onto ⟨ 1⟩⊥is then given by
Dinput/summationdisplay
d=2⟨˜xn,ed⟩ed=˜xn−⟨˜xn,e1⟩e1 (19)
We note that we may rewrite
⟨˜xn,e1⟩e1=/parenleftbigg1√
D˜x⊤
n 1/parenrightbigg1√
D1=/parenleftbigg1
D˜x⊤
n 1/parenrightbigg
1=E[˜xk] 1 (20)
Hence, ˜xn−E[˜xn] 1indeed realizes the orthogonal projection of ˜xnonto⟨ 1⟩⊥. Clearly,⟨ 1⟩⊥is a(Dinput−1)-
dimensional linear subspace of RDinput(being the span of Dinput−1many vectors). We note that the affine
hull commutes with affine functions. Hence, we have:
aff/parenleftbig
LayerNorm/bracketleftbig
RDinput/bracketrightbig/parenrightbig
= diag(α) aff/parenleftbig
g/bracketleftbig
RDinput/bracketrightbig/parenrightbig
+β (21)
where the function gis given as:
g(˜xn) :=˜xn−E[˜xn] 1/radicalbig
Var[˜xn] +ϵ(22)
We have already shown that ˜xk−E[˜xk] 1is exactly the orthogonal projection onto 1⊥. Sincegdiffers
from this projection via a multiplicative factor, the affine span of its image is exactly the affine span of the
projection, which is 1⊥. Thus, we have shown:
aff/parenleftbig
LayerNorm/bracketleftbig
RDinput/bracketrightbig/parenrightbig
= diag(α) 1⊥+β (23)
20Published in Transactions on Machine Learning Research (09/2024)
We can write these affine subspaces in a particular fashion, as we show in the following lemma. This result
will be useful for explicitly writing down a function satisfying Proposition 2.
Lemma 2. For any affine subspace A⊂RDinput, there exists a unique vector a∈Aand a unique vectorspace
V⊂RDinputsuch thata∈V⊥is orthogonal to Vand we have A=a+V.
Proof.Given any affine subspace A⊂RDinput, leta∈Abe the orthogonal projection of the origin onto A.
By definition of the orthogonal projection, for any other a′∈A, we have (a−0)⊤(a′−a) = 0. Moreover,
V:=A−ais a linear subspace of RDinputwitha+V=A. For any arbitrary v∈V, we may write v=a′−a
for somea′∈Aand we have
a⊤v= (a−0)⊤(a′−a) = 0 (24)
Hence,a∈V⊥holds. We have so far shown the existence of some vector space Vanda∈V⊥with
A=a+V.
We now show that this decomposition A=a+Vis unique. Assume that there exists another vector space
V′⊂RDinputanda′∈V′⊥witha+V=a′+V′. We will show that V=V′anda=a′must hold.
Rearranging, we find V′= (a−a′) +VandV=V′−(a−a′). From the first equation, we may conclude
a−a′∈V′. Sincea−a′is a vector in V′, which is closed under subtraction, we find V′−(a−a′) =V′.
Hence, substituting into the previous equation, we conclude V=V′. We now show that we must also have
a=a′. We compute:
(a−a′)⊤(a−a′) =a⊤(a−a′)−a′⊤(a−a′) (25)
and recall that a−a′∈Vholds. By assumption, we have a,a′∈V⊥and therefore a⊤(a−a′) = 0and
a′⊤(a−a′) = 0. Hence, (a−a′)⊤(a−a′) = 0and we conclude from the positive-definiteles of the scalar
product that a=a′does indeed hold.
Finally, we show that for almost all parameters of the value map vand the layer normalization module, the
translation vector afrom the previous lemma does not vanish. Again, this will be crucial to show that a
function satisfying Proposition 2 exists almost always.
Lemma 3. DenoteD:=Dinputand letv:RD→RDbe a linear map that is parametrized via a matrix
B∈RD×Dwhich acts, say, via left-multiplication. Consider parameters α,β∈RDof the layer normalization.
Consider the affine hull of the image of the composition of vand the layer normalization:
A:= aff/parenleftbig
(v◦LayerNorm)/bracketleftbig
RD/bracketrightbig/parenrightbig
(26)
As detailed in Lemma 2, we may write Auniquely asa+V, whereaandVdepend on the parameters of v
and the layer normalization. In the following, we abuse notation by not making this dependence explicit. For
almost all parameters (B,α,β )(w.r.t. the Lebesgue measure on RD×D×RD×RD), we havea̸= 0.
Proof.Let the set
U:={(B,α,β )∈RD×D×RD×RD|a= 0} (27)
represent the parameters for which avanishes. We will show that this is a nullset w.r.t. Lebesgue measure.
It is a standard fact that the set of non-invertible matrices GLD(R)C⊂RD×Dis a Lebesgue-nullset. Hence,
it is already sufficient to prove that the set
U′:=U∩(GLD(R)×RD×RD) (28)
is a nullset. As we saw from the proof of Lemma 2, the vector ais exactly the orthogonal projection of the
origin onto A. Hence,a= 0holds iff 0∈Aholds. Recalling Lemma 1, we know
A:= aff/parenleftbig
(v◦LayerNorm)/bracketleftbig
RD/bracketrightbig/parenrightbig
=v(aff LayerNorm[ RD])
=B(diag(α) 1⊥+β)(29)
21Published in Transactions on Machine Learning Research (09/2024)
As we may assume that Bis invertible, it has a trivial kernel. Hence, we have 0∈Aiff0∈diag(α) 1⊥+β.
From this requirement, we obtain an explicit expression for the set U′:
U′= GLD(R)×{(α,β)|α∈RD, β∈−diag(α) 1⊥}
= GLD(R)×{(α,−diag(α)u)|α∈RD, u∈ 1⊥}(30)
We conclude by noting that {(α,−diag(α)u)|α∈RD, u∈ 1⊥}is a nullset in RD×RD, as it is the image
of a nullset under a continuously differentiable function.
C Proof of Proposition 1
Proof.Assume by contradiction that such a function f:RD→Rexists. Consider now the setting in which
we haveK:= 1and ˜θ1:= 0. Denote the resulting attention matrix by Γ(1)and the resulting update code
u1byw(1)
1. Consider also the setting in which we have K:= 2and ˜θ1=˜θ2:= 0. Denote the resulting
attention matrix by Γ(2)and the resulting update codes u1,u2byw(2)
1,w(2)
2. One may now easily verify
that all entries of Γ(1)are1and that all entries of Γ(2)are1/2. Hence, we have
w(1)
1=/summationtextN
n=1γ(1)
n,1vn
/summationtextN
n=1γ(1)
n,1=/summationtextN
n=1vn
N(31)
and
w(2)
1=w(2)
2=/summationtextN
n=11
2vn/summationtextN
n=11
2=/summationtextN
n=1vn
N(32)
By equation (32) and our assumption (namely, that equation (9) holds), we have:
f/parenleftiggN/summationdisplay
n=1vn/N/parenrightigg
=f/parenleftig
w(2)
1/parenrightig
=/summationtextN
n=1γ(2)
n,k
N=1
2(33)
At the same time, we also deduce from equation (31):
f/parenleftiggN/summationdisplay
n=1vn/N/parenrightigg
=f/parenleftig
w(1)
1/parenrightig
=/summationtextN
n=1γ(1)
n,k
N= 1 (34)
This contradicts equation (33).
D Proof of Proposition 2
Proof.We recall from Lemma 3 that for almost all parameters of the value map vand the layer normalization
module, we may choose a vector a∈RD\{0}and a vector space W⊂RDsuch thata∈W⊥holds and
for any input ˜xn, the corresponding values vnlie ina+W. Given fixed parameters (that do not lie in the
nullset outlined in Lemma 3), fix such a vector a.
Now, we define the function f:RD→Rvia:
f(u) :=Ca⊤u
N∥a∥2
2(35)
The valuesvnmay be written as
vn=a+wn (36)
22Published in Transactions on Machine Learning Research (09/2024)
wherewn∈Wanda⊤wn= 0holds. Hence, recalling that we define uk:=1
C/summationtextN
n=1γn,kvn, we may now
compute:
f(uk) =Ca⊤
N∥a∥2
21
CN/summationdisplay
n=1γn,k(a+wn) =1
N∥a∥2
2N/summationdisplay
n=1γn,ka⊤(a+wn)
=1
N∥a∥2
2N/summationdisplay
n=1γn,ka⊤a
=/summationtextN
n=1γn,k
N(37)
which is what we wanted to show.
E Technical Details Regarding Object Discovery on CLEVR
While we closely follow the descriptions of (Locatello et al., 2020), we use a re-implementation of the method
in our experiments.
DataAs in (Locatello et al., 2020), we use the extended CLEVR dataset that is provided in (Kabra et al.,
2019). This version of the dataset consists of 100,000 images in total, each being of dimension 320×240. We
follow (Locatello et al., 2020; Greff et al., 2019; Burgess et al., 2019) in the pre-processing of this data: We
use 70,000 images for training and hold out 15,000 for validation and testing. We perform a square center
crop of size 192 to increase the space occupied by objects. The cropped images are then bilinearly scaled to
shape 128×128. The corresponding ground-truth segmentation masks are pre-processed analogously, using
nearest-neighbor interpolation in place of bi-linear interpolation. In contrast to (Locatello et al., 2020), we
augment the data by performing random horizontal flips. Before feeding it to the autoencoder, the RGB data
is scaled to the interval [−1,1].
Architecture We follow the autoencoder architecture described in (Locatello et al., 2020) as closely as
possible. Conceptually, we divide the autoencoder into three distinct entities: The encoder processes the
input image and produces a set of tokens. The Slot Attention module processes these tokens and yields a
latent slot representation. The decoderdecodes the latent representation into a reconstruction of the input.
The encoder consists of a convolutional network, which we describe (as in (Locatello et al., 2020)) in Table 2.
Type In Shape Out Shape Activation Comment
Conv 5 ×5 128×128×3 128×128×64ReLU stride:1
Conv 5 ×5 128×128×64 128×128×64ReLU stride:1
Conv 5 ×5 128×128×64 128×128×64ReLU stride:1
Conv 5 ×5 128×128×64 128×128×64ReLU stride:1
Pos. Embed 128×128×64 128×128×64 - -
Spatially Flatten 128×128×64 (128·128)×64 - -
Layer Norm (128·128)×64 (128·128)×64 - per 64-dim token
Affine (128·128)×64 (128·128)×64ReLU per 64-dim token
Affine (128·128)×64 (128·128)×64 - per 64-dim token
Table 2: Encoder network for experiments on CLEVR
We implement the positional embedding as in (Locatello et al., 2020). Namely, to positionally embed a tensor
Xof shapeW×H×C, we construct a W×H×4tensorPin which each of the four channels is a linear
ramp spanning between 0 and 1, either progressing horizontally or vertically and in either of the two possible
23Published in Transactions on Machine Learning Research (09/2024)
directions (e.g. left or right). In order to embed the feature Xi,j,:, we learn an affine layer R4→RCand
compute the embedded feature:
Xi,j,:+affine(Pi,j,:) (38)
The resulting set of input tokens is then processed by the Slot Attention module, which we implement as
described in (Locatello et al., 2020). The key, query, and value maps q,k,vuse the common dimension
D= 64. Slots are also 64-dimensional. The residual MLP that is used to update the slot latents has a single
hidden layer of size 128.
We decode each slot separately, using a spatial broadcast decoder. Once again, we closely follow the approach
of (Locatello et al., 2020) and detail the decoder architecture in Table 3.
Type In Shape Out Shape Activation Comment
Spatial Broadcast 64 8×8×64 - for single slot
Pos. Embed 8×8×64 8×8×64 - -
Transposed Conv 5 ×5 8×8×64 16×16×64ReLU stride:2
padding:2
out padding: 1
Transposed Conv 5 ×5 16×16×64 32×32×64ReLU as above
Transposed Conv 5 ×5 32×32×64 64×64×64ReLU as above
Transposed Conv 5 ×5 64×64×64 128×128×64ReLU as above
Transposed Conv 5 ×5128×128×64 128×128×64ReLU stride:1
padding:2
Transposed Conv 3 ×3128×128×64 128×128×4 - stride:1
padding:1
Table 3: Decoder network for experiments on CLEVR
The4channels of the output of the decoder are split into RGB channels and an unnormalized alpha channel.
The alpha channels are normalized via a softmax operation across all slots, and the RGB reconstructions are
blended to produce an entire reconstruction.
Training We closely follow the training procedure of (Locatello et al., 2020). Namely, we train the
autoencoder with an ℓ2reconstruction loss, utilize 3 Slot Attention iterations during training, and use an
Adam (Kingma & Ba, 2015) optimizer. The models are trained for 500,000 steps. As the authors of (Locatello
et al., 2020), we linearly warm up the learning rate over the course of the first 10,000 steps, after which it
attains a peak value of 4·(0.5)0.1·10−4. Subsequently, we decay it over the course of the remaining steps,
with a half life of 100,000 steps. We use a batch size of 64.
Evaluation During evaluation, we use 5 Slot Attention iterations.
F Technical Details Regarding Object Discovery on MOVi-C
While we closely follow the descriptions of (Seitzer et al., 2023), we use a re-implementation of the method in
our experiments.
DataWe use the MOVi-C dataset from (Greff et al., 2022). In total, it contains 10,986 video sequences, each
consisting of 24 frames. We hold out 250 of the sequence for validation and 999 for testing. Following (Seitzer
et al., 2023), we pre-process the frames from the dataset by bicubicly resizing them to shape 224×224. We
use a pretrained vision transformer to pre-compute image features from these resized frames. Specifically, we
use the model vit_base_patch8_224_dino from the timm (Wightman, 2019) repository. After dropping the
class token, this model provides a 28×28×768feature map for each frame. We save these feature maps at
24Published in Transactions on Machine Learning Research (09/2024)
half precision (16 bit floating point) and use them during training. Alongside, we save the resized frames and
analogously resized (via nearest-neighbor interpolation) ground truth segmentations.
Architecture We closely follow the MLP-based autoencoder architecture detailed in (Seitzer et al., 2023).
The goal of this autoencoder is to encode and decode the pre-computed ViT features. Importantly, it does
not operate on RGB images. As before, we conceptually divide the autoencoder into the encoder, theSlot
Attention module , and the decoder.
The encoder processes each ViT feature separately via a shared MLP. In contrast to the encoder we used
in the experiments on the CLEVR dataset, no positional embedding is employed, as the ViT features still
contain a sufficient amount of positional information (see the discussions in (Seitzer et al., 2023)). We provide
a detailed description of this architecture in Table 4.
Type In Shape Out Shape Activation Comment
Layer Norm 28×28×768 28×28×768 - per feature
Affine 28×28×768 28×28×768ReLU per feature
Affine 28×28×768 28×28×128 - per feature
Table 4: Encoder network for experiments on MOVi-C
The set of tokens that is produced by the encoder is processed by the Slot Attention module to produce a
latent slot representation. Slots, keys, values, and queries are 128-dimensional. The residual MLP that is
used to update the slot latents has a single hidden layer of dimension 512.
In order to produce a reconstruction of the ViT features, an MLP-based decoder architecture is used.
Conceptually, the decoder resembles the one we used in the experiments on the CLEVR dataset: Each
slot is decoded separately into a partial reconstruction and an unnormalized alpha channel. A complete
reconstructionisformedbynormalizingthealphachannelsacrossslotsandblendingthepartialreconstructions.
Each slot is broadcasted spatially before decoding and a positional embedding is added. Once again, the
decoder consists of an MLP that operates on each spatial feature separately. We provide a detailed description
of the architecture in Table 5
Type In Shape Out Shape Activation Comment
Spatial Broadcast 128 28 ×28×128 - -
Pos. Embedding 28×28×128 28×28×128 - -
Affine 28×28×128 28×28×1024 ReLU per feature
Affine 28×28×1024 28×28×1024 ReLU per feature
Affine 28×28×1024 28×28×1024 ReLU per feature
Affine 28×28×1024 28×28×(768 + 1) - per feature
Table 5: Decoder network for experiments on MOVi-C
The positional embedding used in Table 5 differs significantly from the one we used in the experiments on
CLEVR. Namely, given an input tensor Xof shapeW×H×C, we positionally embed the feature Xi,j,:by
learning a tensor Pof shapeW×H×Cand computing:
Xi,j,:+Pi,j,: (39)
Training We follow the training procedure detailed in (Seitzer et al., 2023). Namely, we train the
autoencoder via an ℓ2reconstruction loss and use 3 Slot Attention iterations during training. The batch size
is 64 and we employ an Adam optimizer. As before, a learning rate schedule consisting of a linear increase
and exponential decay is used. Here, the peak learning rate is 4·10−4, which is reached after 10,000 steps.
Afterwards, the learning rate decays with a half life of 100,000 steps. The models are trained for 500,000
steps in total.
25Published in Transactions on Machine Learning Research (09/2024)
Evaluation During evaluation, we also utilize 3 Slot Attention iterations. Since the alpha masks that are
produced by our model are of shape 28×28, we cannot directly compare them to ground truth segmentations,
which are of shape 224×224. We follow the approach of (Seitzer et al., 2023) and bi-linearly upscale the
alpha masks to shape 224×224before deriving segmentations, which we then compare to the ground-truth.
G Pseudocode
In Algorithms 1 and 2, we illustrate how the weighted sum and batch norm variants differ from the weighted
mean variant in pseudo PyTorch code. We illustrate this in a diff format.
Algorithm 1 Diff of Weighted Sum Variant
1 ...
2 bs, N, d_in = inputs.shape
3 k, v = self.key_map(inputs), self.value_map(inputs)
4 for idx in range(num_iters):
5 slots_prev = slots
6 slots = self.norm_slots(slots)
7 q = self.query_map(slots)
8 dots = torch.einsum("bid,bjd->bij", q, k) / np.sqrt(q.size(-1))
9 attn = dots.softmax(dim=1)
10- attn = (attn + eps) / (attn + eps).sum(dim=-1, keepdim=True)
11 updates = torch.einsum("bjd,bij->bid", v, attn)
12+ updates = updates / N
13 ...
Algorithm 2 Diff of Batch Norm Variant
1 ...
2 bs, N, d_in = inputs.shape
3 k, v = self.key_map(inputs), self.value_map(inputs)
4+ var, mean = None, None
5 for idx in range(num_iters):
6 slots_prev = slots
7 slots = self.norm_slots(slots)
8 q = self.query_map(slots)
9 dots = torch.einsum("bid,bjd->bij", q, k) / np.sqrt(q.size(-1))
10 attn = dots.softmax(dim=1)
11- attn = (attn + eps) / (attn + eps).sum(dim=-1, keepdim=True)
12 updates = torch.einsum("bjd,bij->bid", v, attn)
13+ if idx == 0 and self.training:
14+ var, mean = torch.var_mean(updates, correction=0)
15+ self.update_buffers(var, mean)
16+ elif idx == 0 and not self.training:
17+ var, mean = self.var_buffer, self.mean_buffer
18+ updates = self.alpha * (updates - mean) / torch.sqrt(var + eps) + self.beta
19 ...
26Published in Transactions on Machine Learning Research (09/2024)
H Visual Results
In Tables 6 and 7, we present visual results from object discovery on the CLEVR10 and MOVi-C10
datasets, respectively. On CLEVR, we show both reconstructions and segmentations, while we only provide
segmentations on MOVi-C, as the reconstructions are high-dimensional ViT features. In both cases, we
perform inference with a high slot count (21).
Input Baseline Layer Norm Weighted Sum Batch Norm
Table 6: Visual results for object discovery on CLEVR10, trained on CLEVR6 with 7 slots. Showing
reconstructions and segmentations. Evaluated with 21 slots.
27Published in Transactions on Machine Learning Research (09/2024)
Baseline Layer Norm Weighted Sum Batch Norm
Table 7: Visual results for object discovery on MOViC10, trained on MOViC10 with 11 slots. Evaluated with
21 slots.
28Published in Transactions on Machine Learning Research (09/2024)
I Property Prediction
We further illustrate the proposed normalizations on a property prediction task on the CLEVR10 dataset.
We closely follow the experimental setup detailed by Locatello et al. (2020), using three seeds per variant.
We train the models on the CLEVR10 dataset using 10 slots. In Figure 12, we plot how the foreground ARI
varies as the number of slots is modified during inference. Consistent with our observations on the object
discovery task, we find that the weighted mean and layer norm variants suffer from excess slots, while the
proposed normalizations are robust to them. Overall, we find that the batch norm variant performs best
w.r.t. foreground segmentation quality.
7 911 13 15 17 19 21
# Slots0.00.20.40.60.8F-ARI
Baseline
Layer Norm
Weighted Sum
Batch Norm
Figure 12: F-ARI ( ↑) for property prediction on CLEVR10
In Figure 13, we additionally show the mean average prediction at various distance thresholds, as defined
in (Locatello et al., 2020). We observe a similar behavior as before, namely that the proposed variants
are robust to high slot counts during inference, while the weighted mean and layer norm variants are not.
However, we generally find that the weighted mean variant appears to perform best at low slot counts.
7 911 13 15 17 19 21
# Slots0.00.20.40.60.8mAP-1.0 Baseline
Layer Norm
Weighted Sum
Batch Norm
(a) mAP@1.0 ( ↑) on CLEVR10
7 911 13 15 17 19 21
# Slots0.00.20.40.6mAP-0.5 Baseline
Layer Norm
Weighted Sum
Batch Norm (b) mAP@0.5 ( ↑) on CLEVR10
7 911 13 15 17 19 21
# Slots0.000.050.100.150.20mAP-0.25Baseline
Layer Norm
Weighted Sum
Batch Norm
(c) mAP@0.25 ( ↑) on CLEVR10
79111315171921
# Slots0.0000.0050.0100.0150.020mAP-0.125Baseline
Layer Norm
Weighted Sum
Batch Norm (d) mAP@0.125 ( ↑) on CLEVR10
Figure 13: Mean average precision at different distance thresholds for property prediction on CLEVR.
29Published in Transactions on Machine Learning Research (09/2024)
J Ablation of Normalization Schemes
In this section, we study two alternative normalizations methods. Firstly, we consider the weighted sum
variant with the scaling parameter chosen as C= 1. We refer to this method as "unnormalized". Secondly,
we consider a variant which ablates the normalization across the batch axis from the batch normalized
variant. I.e., we compute mean and variance for each instance separately across only the slot and layer axes.
Hence, we also do not have to keep a moving average of the normalizing statistics for inference. Instead
the normalization behaves identically during training and inference. As in the batch normalized variant,
two scalar values αandβare learned. We refer to this variant as "K-D-Layer Norm", to reflect that we are
performing a layer normalization across the slot (K) and layer (D) axes. We perform experiments on the
property prediction task on CLEVR with three seeds per variant.
The unnormalized variant fails to obtain object-centric behavior, as becomes apparent from the low foreground
ARI across all slot counts, as shown in Figure 14. In Figure 15, we observe that the mean average precision
suffers correspondingly. This underscores the importance of scaling the update codes appropriately to achieve
competitive performance. While the K-D-Layer Norm performs better, we find that as the other baseline
variants, it is not robust to high slot counts. Moreover, on this specific task, it seems to generally underperform
in terms of mAP when compared to the other object-centric variants.
7 911 13 15 17 19 21
# Slots0.00.20.40.60.8F-ARIBaseline
Layer Norm
Weighted Sum
Batch Norm
K-D-Layer Norm
Unnormalized
Figure 14: F-ARI ( ↑) for property prediction on CLEVR10
7 911 13 15 17 19 21
# Slots0.00.20.40.60.8mAP-1.0Baseline
Layer Norm
Weighted Sum
Batch Norm
K-D-Layer Norm
Unnormalized
(a) mAP@1.0 ( ↑) on CLEVR10
7 911 13 15 17 19 21
# Slots0.00.20.40.6mAP-0.5Baseline
Layer Norm
Weighted Sum
Batch Norm
K-D-Layer Norm
Unnormalized (b) mAP@0.5 ( ↑) on CLEVR10
7 911 13 15 17 19 21
# Slots0.000.050.100.150.20mAP-0.25Baseline
Layer Norm
Weighted Sum
Batch Norm
K-D-Layer Norm
Unnormalized
(c) mAP@0.25 ( ↑) on CLEVR10
79111315171921
# Slots0.0000.0050.0100.0150.020mAP-0.125Baseline
Layer Norm
Weighted Sum
Batch Norm
K-D-Layer Norm
Unnormalized (d) mAP@0.125 ( ↑) on CLEVR10
Figure 15: Mean average precision at different distance thresholds for property prediction on CLEVR.
30