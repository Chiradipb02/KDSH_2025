Under review as submission to TMLR
Pathwise gradient variance reduction with control variates in
variational inference
Anonymous authors
Paper under double-blind review
Abstract
Stochastic gradient descent is a workhorse in modern deep learning. The gradient of
interest is almost always the gradient of an expectation, which is unavailable in closed
form. The pathwise and score-function gradient estimators represent the most common
approaches to estimating the gradient of an expectation. When it is applicable, the pathwise
gradient estimator is often preferred over the score-function gradient estimator because it has
substantially lower variance. Indeed, the latter is almost always applied with some variance
reduction techniques. However, a series of works suggest, in the context of variational
inference, that pathwise gradient estimators may also benefit from variance reduction. In
this work, we review existing control-variates-based variance reduction methods for pathwise
gradient estimators to determine their effectiveness. Work in this vein generally rely on
approximations of the integrand which necessitates the functional form of the variational
family be simple. In light of this limitation, we also propose applying zero-variance control
variates on pathwise gradient estimators, as the control variates have the advantage that
requires little assumption on the variational distribution, other than being able to sample
from it.
1 Introduction
Stochastic optimisation, in particular stochastic gradient descent, is ubiquitous in modern machine learning.
In many applications, deployment of stochastic gradient descent necessitates estimating the gradient of an
expectation,
g(λ) =∇λEq(z;λ)[r(z;λ)], (1)
whereris some real-valued function and λ∈Rdimλis the parameter over which we optimise. Note that since
the expectation is taken with respect to a distribution qthat is parameterised by λ, we cannot simply push
the gradient operator through the expectation. The two most common approaches to estimating g(λ)are the
pathwise gradient estimator and thescore-function gradient estimator . We will introduce them in
the context of variational inference (VI).
Given an observed dataset xthat is governed by a data generating process which depends on latent variables
z∈Rdimzand a prior p(z)of the latent variables, the posterior distribution is given by
p(z|x)∝p(x|z)p(z),
which is only known up to a normalising constant. VI seeks to approximate the posterior with a simple,
tractable distribution in the variational family Q={q(z;λ) :λ∈Rdimλ}. This is often done by finding the λ
that minimises the Kullback-Leibler divergence between the variational distribution qand the posterior (in
that order):
λ∗= arg min
λEq(z;λ)[logq(z;λ)−logp(z|x)]. (2)
In practice, this is done by maximising the so-called evidence lower bound (ELBO),
λ∗= arg max
λELBO(λ), (3)
1Under review as submission to TMLR
where
ELBO(λ) =Eq(z;λ)[logp(z,x)−logq(z;λ)].
It is easy to see that minimising the KL divergence in (2)is equivalent to maximising the ELBO in (3).
Notably, computation of the latter avoids the intractable normalising constant in the posterior p(z|x).
The closed-form solution for λ∗is generally unavailable. Stochastic VI (Hoffman et al., 2013), which optimises
with minibatch stochastic gradient descent, became a game changer and opened up new applications for VI.
Stochastic VI requires gradient computation of the minibatch ELBO ,
mELBO(λ) =Eq(z;λ)[r(z;λ)],
where
r(z;λ) =N
B/summationdisplay
i∈batch[logp(xi|z)] + logp(z)
q(z;λ). (4)
HereNandBare data and batch size respectively, and xidenotes the ithelement of the dataset.
2 Gradient estimators for VI
There are two main types of gradient estimators in the VI literature for computing the gradient of mELBO:
1) the pathwise gradient, also known as the reparametrization trick; and 2) the score-function estimator,
also known as REINFORCE. The latter is more broadly applicable than the former but comes at the cost
of larger variance. Indeed, the score-function estimator is almost always used in conjunction with control
variates to reduce its variance (see, for example Ranganath et al., 2014; Ji et al., 2021). It is far less common
to see variance reduction employed for the pathwise gradient estimator but a series of recent works suggest
potential benefits from doing so (Miller et al., 2017; Geffner & Domke, 2020). In this work, we propose a new
variance reduction technique for the pathwise gradient estimator in the context of VI. But first in this section
we review the pathwise graident estmator.
The pathwise gradient estimator is only readily applicable for reparametrizable distributions q(z;λ), i.e. dis-
tributions where zcan be equivalently generated from z=T(ϵ;λ)whereϵ∈Rdimz∼q0(ϵ)andq0is referred
as thebase distribution which is independent from λ. Takez∼N(µ,σ2I)as an example, the corresponding
transformation is T(ϵ;λ) =µ+σϵwhereϵis standard Gaussian and λ= (µ,σ).
Whenqis reparametrizable, the gradient operator can be pushed inside the expectation and we get that the
gradient of mELBO is given by
g(λ):=∇λmELBO(λ)
=Eq0(ϵ)φ(ϵ;λ), (5)
where we define
φ(ϵ;λ) =∇λ[r(T(ϵ;λ);λ)]. (6)
The pathwise gradient estimator is then simply a Monte Carlo estimator of (5)using a set of samples {ϵ[l]}L
l=1
fromq0:
ˆg(ϵ[1],...,ϵ [L];λ):=1
LL/summationdisplay
l=1φ(ϵ[l];λ). (7)
We will refer to Las the number of gradient samples.
The variance of the gradient estimator is thought to play an important factor in the convergence properties of
stochastic gradient descent. Henceforth, any expectations or variances without a subscript refer to q0. Define
the variance of the pathwise gradient estimator to be
V[ˆg] =E∥ˆg∥2−∥Eˆg∥2=1
L(E∥φ∥2−∥Eφ∥2) =1
LV[φ].
2Under review as submission to TMLR
To reduce the variance of (7), we may add a control variate to the pathwise gradient estimator
1
LL/summationdisplay
l=1/bracketleftbig
φ(ϵ[l];λ) +c(ϵ[l])/bracketrightbig
, (8)
where the control variance c(·)∈Rdimλis a random variable with zero expectation, i.e. E[φ+c] =Eφ. Let
Tr(C[φ,c])be the trace of the covariance matrix C[φ,c] =E[(φ−Eφ)(c−Ec)⊤]. A good control variate c
has a strong, negative correlation with φ, since
V[1
L/summationtextL
l=1φ(ϵ[l];λ) +c(ϵ[l])] =1
LV[φ+c] (9)
=1
L(V[φ] +V[c] + 2 Tr( C[φ,c]))
=V[ˆg] +1
L(V[c] + 2 Tr( C[φ,c]))
Therefore, as long as Tr(C[φ,c])<0and|Tr(C[φ,c])|<1
2V[c], the control-variate-adjusted gradient estimator
(8) will have a smaller variance than (7).
Finally, a control variate may also be formed as a linear combinations of control variates. Let C:Rdimz→
Rdimλ×Jbe a matrix with Jcontrol variates in the columns. This leads to the control-variate-adjusted
gradient estimator
ˆh(ϵ[1],...,ϵ [L];λ):=1
LL/summationdisplay
l=1/bracketleftbig
φ(ϵ[l];λ) +C(ϵ[l])β/bracketrightbig
, (10)
which remains a valid control variate due to the linearity of expectation operators, i.e. E[Cβ] = 0. Here
β∈RJis a vector of coefficients corresponding to each control variate. This construction allows us to combine
multiple weak control variates into a strong one by adjusting β.
For obvious reasons, applying control variates is only worthwhile if the computation of control variates is
cheaper than increasing the number of samples in (7). From(9), we can see that, for example, the estimator
variance can be reduced by half either by doubling Lor halving V[φ+c]. This presents a unique challenge
when applying control variates in the low Lregime (such as the gradient estimator of VI where Lis often
very low), since the cost of applying control variates will more likely outweigh the cost of increasing Lto
achieve the same variance reduction. The control variates that were developed in the Markov Chain Monte
Carlo (MCMC) community cannot be easily applied in our work because 1) they require large L, butLcan
be as small as one in stochastic VI, and 2) MCMC variance reduction is usually done at the very end, while
variance reduction is required for each gradient update in stochastic VI.
In this work, we review existing control-variates-adjusted pathwise gradient estimators in the context of VI.
We are primarily interested in whether employing control variates can achieve faster convergence with respect
to wall-clock time. We are also motivated by the gap in the VI literature on gradient variance reduction when
qis reparametrizable but the mean and covariance of qare not available in closed form. A good example of
suchqis normalizing flow, where zis the result of pushing forward a base distribution q0through an invertible
transformation T(·;λ)that is parameterised by λ, i.e.z=T(ϵ;λ)whereϵ∼q0. Such transformation Tcan be
arbitrarily complex and usually involves neural networks. To this end, we introduce a control-variate-adjusted
gradient estimator based on zero-variance control variates (Assaraf & Caffarel, 1999; Mira et al., 2013; Oates
et al., 2017) which does not have this limitation.
This paper is structured as follows: in Section 3, we provide a review of the latest advancements in variance
reduction techniques for pathwise gradient estimators in the context of VI. This is followed by a discussion
on methods for selecting βandCof(10)in Sections 4 and 5, respectively. The novel method based on
zero-variance control variates is introduced in Section 5.2. Finally, we present the experimental results in
Section 6.
3 Related work
Variance reduction for the pathwise gradient estimator in VI has been explored in Miller et al. (2017) and
Geffner & Domke (2020). These works focused on designing a single control variate (i.e. Conly has a column)
3Under review as submission to TMLR
with the form of C=E˜φ−˜φ, where ˜φ(ϵ;λ)is an approximation of φ(ϵ;λ). The expectation E˜φis designed to
be theoretically tractable, but this usually implies that there is some restriction imposed on T(and therefore
q) to make this expectation easy to compute.
Miller et al. (2017) proposed ˜φthat is based on the first-order Taylor expansion of ∇zlogp(z,x). However,
this Taylor expansion requires the expensive computation of the Hessian ∇2
zlogp(z,x). Geffner & Domke
(2020) improved upon their work and proposed using a quadratic function to approximate logp(z,x). Their
method only requires the first-order gradient ∇zlogp(z,x), and their E˜φis available in closed-form as a
function of the mean and covariance of q. A direct modification of their method is to estimate E˜φempirically
when the mean and covariance of qare unavailable; see Section 5.1 for more details. Both Miller et al. (2017)
and Geffner & Domke (2020) only considered Gaussian qin their work, although the latter can be applied to
a larger class of variational families where the mean and covariance of qis known.
The proposed estimator based on zero-variance control variates is similar in spirit to another work from the
same group in Geffner & Domke (2020). Like Geffner & Domke (2018), we propose combining weak control
variates into a stronger one, but our work differs in the construction of the individual control variates and
the optimisation criterion for β.
4 Selecting βfor the control-variate-adjusted pathwise gradient estimator
The utility of control variates depends on the choice of βandCin(10). In this section, we will discuss
various strategies to pick an appropriate βgiven a family of control covariates C.
4.1 A unique set of βfor each dimension of λ
The formulation in (10)suggests that the same set of βis used across the dimensions of φ. This can be too
restrictive for Cthat are weakly-correlated to φ. In such instance, having a unique set of βcoefficients for
each dimension of φcan be beneficial, as it allows the coefficients to be selected on a per-dimension basis. In
fact, this can be easily done by turning Cinto a dimλ×(Jdimλ)-dimensional, block diagonal matrix

C1,:... 0
.........
0... C dimλ,:
,
whereCi,:is theithrow of the original C. In other words, we expand the number of control variates to
Jdimλ, and each control variate will only reduce the variance of a single dimension of φ.
4.2 Optimisation criteria for β
Theβis usually chosen to minimise the variance of ˆh. In practice, this variance is usually replaced by an
empirical approximation due to the lack of its closed-form expression. There are three approximations in the
literature, the first of which is a direct approximation of the variance with samples {ϵ[l]}L
l=1,
V[φ+Cβ]≈1
L(L−1)/summationtext
l>l′∥φ(ϵ[l]) +C(ϵ[l])β−φ(ϵ[l′])−C(ϵ[l′])β∥2, (11)
as seen in Belomestny et al. (2018). The second approximation is based on the definition of variance
V[φ+Cβ] =E∥φ−E[φ+Cβ] +Cβ∥2≈min
α∈Rdimλ1
L/summationtextL
l=1∥φ(ϵ[l]) +α+C(ϵ[l])β∥2, (12)
whereα∈Rdimλis an intercept term in place of the unknown E[φ+Cβ]. The second approximation in (12)
is generally cheaper to compute than the first approximation in (11)as it only requires O(L)operations
rather than O(L2); see Si et al. (2022) for details.
4Under review as submission to TMLR
Minimising (12)with respect to βis essentially a least squares problem with a well-known closed-formed
solution
/bracketleftbiggα∗
β∗/bracketrightbigg
= arg min
α,βL/summationdisplay
l=1/vextenddouble/vextenddouble/vextenddouble/vextenddoubleφ(ϵ[l]) +/bracketleftbigIdimλC(ϵ[l])/bracketrightbig/bracketleftbiggα
β/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
(13)
=−(X⊤X)−1X⊤φ, (14)
where Idimλis an identity matrix of size dimλ, andφand the design matrix Xare given by
φ=
φ(ϵ[1])
...
φ(ϵ[L])
,X=
IdimλC(ϵ[1])
......
IdimλC(ϵ[L])
.
However, the matrix inversion in (14)can be problematic to compute, especially when XTXis rank-deficient.
In such instance, we can include a penalty in (13)and solve for the penalised least squares solution (see, for
example, South et al., 2022), or use an iterative optimisation algorithm to minimise (13), as suggested in Si
et al. (2022).
Finally the third approach relies on the assumption that E[Cβ] = 0and is based on the observation that
V[φ+Cβ] =E∥φ+Cβ∥2−∥Eφ∥2.
This suggests that the variance can be equivalently minimised by minimising simply the expected squared
norm component, E∥φ+Cβ∥2. Geffner & Domke (2018) shows that the minimiser β∗=arg minβE∥φ+Cβ∥2
is given by
β∗=−E[C⊤C]−1E[C⊤φ], (15)
and suggests replacing E[C⊤C]andE[C⊤φ]with their empirical estimates. This approach, however, requires
inverting a costly inversion of size Jmatrix.
4.3 Potential bias in the gradient estimator
The unbiasedness of the control-variate-adjusted Monte Carlo estimator (10)relies on the assumption that
theβare independent of C, since E[C(ϵ)β(ϵ)]̸= 0in general. This necessitates that βandCshould be
estimated with independent sets of ϵsamples. However, in practice, the βis estimated with the same set of ϵ
inCto save computational time at the cost of introducing bias in the gradient estimates.
5 Control variates
Having reviewed methods to select βgiven a family of control variates C, we now turn our attention to
constructing C. We will first propose a simple modification of Geffner & Domke (2020) that will enable it to
work for variational distributions qwith unknown mean and covariance. Subsequently, we will introduce
zero-variance control variates, which can be constructed without the knowledge of qorT.
5.1 Quadratic approximation control variates
In this section we review the quadratic-approximation control variates proposed in Geffner & Domke (2020).
An important distinction at the outset is their assumption that the entropy term in mELBO,
−Eq(z)logq(z;λ),
isknown. As such the focus of Geffner & Domke (2020) is to reduce the variance of
E∇λf(T(ϵ;λ)),
5Under review as submission to TMLR
where
f(z) =N
B/summationdisplay
i∈batch[logp(xi|z)] + logp(z). (16)
Geffner & Domke (2020) proposed control variates of the form
C(ϵ) =E[∇λ˜f(T(ϵ;λ))]−∇λ˜f(T(ϵ;λ)), (17)
where
˜f(z;v) =b⊤
v(z−z0) +1
2(z−z0)⊤Bv(z−z0)
is a quadratic approximation of (16). Here,v={Bv,bv}are the parameters of the quadratic equation that
are chosen to minimise the L2difference between ∇f(z)and∇˜f(z). We will drop vin˜ffor the sake of
brevity. The location parameter z0is set to ET(ϵ;λ). This quadratic approximation of fcan also be viewed
as a linear approximation on ∇f.
The first term in (17)has a closed-form expression that depends on the mean and covariance of q(z;λ),
making the expectation cheap to evaluate when they are readily available. However, this is not the case
whenT(ϵ;λ)is arbitrarily complex, e.g. normalizing flow. A direct workaround of this limitation is to replace
E∇λ˜f(T(ϵ;λ))with its empirical estimate based on a sample of ϵ’s. Note that ˜frequiresz0=ET(ϵ;λ), which
we estimate using another independent set of ϵ’s. See Algorithm 1 for a summary of the procedure.
As(16)is a part of the Monte Carlo estimator (10), it could be tempting to estimate E∇λ˜f(T(ϵ;λ))with an
average of the∇λ˜f(T(ϵ;λ))evaluations that have been computed in (10). This is to be avoided as it will
result in the two terms in (17) cancelling each other out.
As(17)is designed to be strongly correlated with φwhen ˜fis reasonably close to f, the choice of βbecomes
less significant. Geffner & Domke (2020) opted to minimise the expected squared norm with a scalar β(note
thatCis a column vector in this case), the solution of which is given in (15). In their work, the expectations
E[C⊤φ]andE[CTC]are replaced with their empirical estimates computed from Candφin(10), instead of
fresh evaluations. However, as discussed in Section 4.2, the resulting gradient estimate is biased due to the
dependency of between βandC.
While this bias is not mentioned explicitly in Geffner & Domke (2020), we conjecture that they overcame
the issue by estimating the expectations with Candφcomputed from previous iterations, as specified in
Algorithm 1. Therefore, their βis independent from Cin the current iteration. This will avoid introducing
bias to the gradient estimate at the cost of having sub-optimal β. They also claimed that their estimates
ofE[C⊤φ]andE[CTC](and by extension, β) do not differ much across iterations. Moreover, their βis
largely acting as an auxiliary ‘switch’ of the control variate when ˜fis a poor approximation of f, rather
than the primary mechanism to reduce the estimator variance, since the βwill be almost 0 when ˜fis not
approximating well (i.e. C[φ,C]≈0). TheirConly kicks in when it is sufficiently correlated to φ.
Lastly, let us return to the discussion of the entropy term at the beginning of this section. Our setup is
more general than Geffner & Domke (2020) as we does not assume the entropy term −Eq(z)logq(z;λ)to
necessarily have a closed-form expression, i.e. our r(z,λ)includes−logq(z;λ). Although it was claimed
in Geffner & Domke (2020) that their quadratic approximation control variate can also be similarly designed
forr(z,λ)in(4)rather than f(z,λ)in(16), we found the implementation difficult because the updating step
ofvrequires the gradient ∇zlogq(z;λ), and in turn∂λ
∂z, which is challenging to compute.
5.2 Zero-variance control variates
The control variates in Geffner & Domke (2020) require one to know the mean and covariance of q(z;λ). To
avoid this requirement, we propose the use of gradient-based control variates (Assaraf & Caffarel, 1999; Mira
et al., 2013; Oates et al., 2017). These control variates are generated by applying a so-called Stein operator L
to a class of user-specified functions P(z). Typically the Stein operator uses ∇zlogq(z), the gradients of
the log probability density function for the distribution over which the expectation is taken, but it does not
require any other information about φorT.
6Under review as submission to TMLR
Algorithm 1 Quadratic approximation control variates with empirical estimates of E˜f
Require: Learning rates γ(λ),γ(v).
Initialiseλ,vand control variate weight β= 0.
fork= 0,1,2,···do
Sampleϵ[1],...ϵ [L]∼q0to compute φ(ϵ[l];λk)
Generate an independent set of 100 ϵsamples to estimate z0=ET(ϵ;λ)
Generate an independent set of 100 ϵsamples to estimate E∇λ˜f(T(ϵ;λ);vk)
Computeh=1
L/summationtextL
l=1/bracketleftbig
φ(ϵ[l];λk) +C(ϵ[l])β/bracketrightbig
See (17)
Take an ascent step λk+1←λk+γ(λ)h
Estimate E[C⊤C]andE[C⊤φ]withϵ[1],...ϵ [L], and update βwith (15).
Take a descent step vk+1←vk−γ(v)1
2L/summationtextL
l=1∇v∥∇zf(T(ϵ[l];λk))−∇z˜f(T(ϵ[l];λk);vk)∥2
end for
We will focus on the form of gradient-based control variates known as zero-variance control variates (ZVCV,
Assaraf & Caffarel, 1999; Mira et al., 2013). ZVCV uses the second order Langevin Stein operator and a
polynomial P(z) =/summationtextJ
j=1βjPj(z), wherePj(z)is thejth monomial in the polynomial and Jis the number
of monomials. The control variates are
{LPj(z)}J
j=1={∆zPj(z) +∇zPj(z)·∇zlogq(z)}J
j=1,
where ∆zis the Laplacian operator and q(z)is the probability density function for the distribution over
which the expectation is taken. A sufficient condition for these control variates to have zero expectation is
that the tails of qdecay faster than polynomially (Appendix B of Oates et al., 2016), which is satisfied by
Gaussianqfor example.
In this paper, we only consider first-order polynomials so there are J= dimzcontrol variates of the form
/braceleftbigg∂
∂zjlogq(z)/bracerightbiggdimz
j=1.
Here,zjrefers to the jthdimension of z. We do not find second-order polynomials to have any advantage over
first-order polynomials; see Appendix F for a discussion. For pathwise gradient estimators using a standard
Gaussian as the base distribution, these control variates simplify further to
/braceleftbigg∂
∂ϵjlogq0(ϵ)/bracerightbiggdimz
j=1={−ϵj}dimz
j=1.
We are also using the same set of control variates across different dimensions of φ, but assigning each
dimension with a unique set of β. That is, the matrix of control variates Cis a block-diagonal matrix of size
dimλ×dimλdimz
C(ϵ) =
−ϵ⊤... 0
.........
0...−ϵ⊤
. (18)
This is in contrast to Geffner & Domke (2020) where the values in Cis different across dimensions, but
theirβis shared. The simplicity of ZVCV comes with the drawback that it is often not as correlated as the
integrand. This makes the choice of βcrucial.
5.2.1 Exact least squares
As discussed in Section 4.2, the optimal βcan be selected by solving (13), the solution of which is given
in(14). We can further exploit the block-diagonal structure of (18)and decompose (13)into a series of linear
7Under review as submission to TMLR
regression that corresponds to each dimension of λ. In other words, we can solve βfor each dimension of λ
individually/bracketleftbigg
α∗
i
β∗
i/bracketrightbigg
=−(X⊤X)−1X⊤φi, i= 1,..., dimλ, (19)
whereidenotes the dimension of λwhichα∗
i,β∗
iandφicorrespond to, e.g. φi= [φi(ϵ[1]),...,φi(ϵ[L])]⊤is a
subset of φin (14) that corresponds to the ithdimension of λ, and
X=
1−ϵ⊤
[1]
......
1−ϵ⊤
[L]
.
Note that the inversion of X⊤Xonly needs to be performed once and can be used across different φi,
thereby scaling well to models with high-dimensional λ. The control variate for φican then be computed as
φi(ϵ) =−ϵ⊤β∗
i. We also propose using ϵ[1],...,ϵ [L]that have already been generated in (10)to compute β∗,
as the resulting control variate tends to have a lower mean squared error.
5.2.2 Least squares with gradient descent
There are several challenges we face in applying ZVCV. For example, XTXin(19)must be invertible. This
is often not the case for models with large dimz, as we usually keep Llow and thus the column space of
Xis rank-deficient. This can be solved by adding a penalty term in (13)(i.e. shrinking βtowards 0), and
empirical evidence suggests that is more effective than using a subset of these control variates to achieve
better performance (Geffner & Domke, 2018; South et al., 2022). However, penalised least squares remains
prohibitively expensive to solve as it still involves inverting a matrix of size dimz.
Instead, we propose mimicking penalised least squares by minimising (13)with respect to αandβwith
gradient descent. This is done by
1. Initialise αat−1
L/summationtextL
l=1φ(ϵ[l])andβat the zero vector. Set γ(α,β)to a low value;
2. Take a descent step (αm+1,βm+1)←(αm,βm)−γ(α,β)1
L/summationtextL
l=1∇α,β∥φ(ϵ[l]) +αm+C(ϵ[l])βm∥2;
3. Repeat Step 2 for a few times.
See Algorithm 2 for a complete description. The combination of learning rate and number of iterations is
analogous to the penalty in penalised least squares: a lower number of iterations and learning rate γ(α,β)will
result in a near-zero βthat results from a stronger penalty (more shrinkage of βtowards 0). This procedure
is also similar in spirit to Si et al. (2022).
6 Experiments
In these experiments, we assess the efficacy of various control variate strategies.
Model and datasets We perform VI on the following model-dataset pairs: logistic regression on the
a1adataset, a hierarchical Poisson model on the friskdataset, and Bayesian neural network (BNN) on the
redwinedataset. For the BNN model, we consider a full-batch gradient estimator trained on a subset of 100
data points of the redwinedataset following the experimental setup in Geffner & Domke (2020) and Miller
et al. (2017). We also consider a mini-batch estimator of size 32 but trained on the full redwinedataset,
see Appendix A for more details. With the exception of mini-batch BNN, these models appeared in either
Geffner & Domke (2020) or Miller et al. (2017).
Variational families Three classes of variational families are considered:
•Mean-field Gaussian The covariance of the Gaussian distribution N(µ,Σ)is parameterised by
log-scale parameters, i.e. Σ = diag (exp(2 log σ1,..., 2 logσdimz)).
8Under review as submission to TMLR
Algorithm 2 ZVCV-GD
Require: Learning rates γ(λ),γ(α,β).
Initialiseλ
fork= 0,1,2,···do
Sampleϵ[1],...ϵ [L]∼q0
Computeφ(ϵ[l];λk),∀l= 1,...,L See (5)
Initialiseα0=−1
L/summationtextL
l=1φ(ϵ[l];λk)andβ0at the zero vector
form= 0,1,2,···do
Take a descent step (αm+1,βm+1)←(αm,βm)−γ(α,β)1
L/summationtextL
l=1∇α,β∥φ(ϵ[l];λk) +αm+C(ϵ[l])βm∥2
end for
Setβ∗as the final value of βfrom the previous inner loop
Computeh=1
L/summationtextL
l=1φ(ϵ[l];λk) +C(ϵ[l])β∗See (18)
Take an ascent step λk+1←λk+γ(λ)h.
end for
•Rank-5 Gaussian The covariance of the Gaussian distribution N(µ,Σ)is parameterised by a factor
F∈Rdimz×5and diagonal components, i.e. Σ =FF⊤+ diag (exp(2 log σ1,..., 2 logσdimz)).
•Real NVP We use a real NVP normalizing flow (Dinh et al., 2017) with two coupling layers and
compose the layers in alternate pattern. The flow has a standard multivariate Gaussian as its base
distribution. The scale and translation networks have the same architecture of 8×16×16hidden units
with ReLU activations, followed by a fully connected layer. There is an additional tanh activation at
the tail of the scale network to prevent the exponential term from blowing up.
We only present the results for mean-field Gaussian and real NVP in the main section. The results for rank-5
Gaussian are included in Appendix C, as they are largely similar to those obtained for mean-field Gaussian.
Optimiser and learning rate We use an Adam optimiser and set its learning rate γ(λ)= 0.01, except for
the BNN models with real NVP where we set γ(λ)= 0.001. These learning rates have been selected as the
most best options, in terms of convergence time to a respectable ELBO, from the set of {0.1, 0.01, 0.001,
0.0001}.
Control variates The gradient estimator is equipped with the following control variate strategies:
•NoCVThe vanilla gradient estimator without any control variates.
•ZVCV-GD A ZVCV with βminimising least squares with an inner gradient descent, as described
in Algorithm 2 and Section 5.2.2. We set the learning rate γ(α,β)= 0.001and iterated the inner
gradient descent 4 times for each outer Adam step. These hyperparameter choices may not always
yield the maximum variance reduction in every situation, but they represent a good compromise
with computation time. Additionally, we have discovered that prolonging the inner gradient descent
iterations does not necessarily lead to better variance reduction. For a more comprehensive discussion,
please refer to Appendix F.
•QuadCV This is the original algorithm presented in Geffner & Domke (2020) when qis Gaussian
(i.e. the mean and covariance of qare readily available). When qis real NVP, we use Algorithm 1 and
100 samples to estimate ET(ϵ;λ)andE∇λ˜f(T(ϵ;λ)). The learning rate γ(v)is set toγ(λ), following
the original work.
Note that above we only compare our method in detail with Geffner & Domke (2020) as it is a direct
improvement of Miller et al. (2017).
9Under review as submission to TMLR
Initialisations We repeated the experiment five times, each time using different initialisations of λto assess
the convergence performance of each method under varying initial conditions. For the mean-field Gaussian,
theλvalues were randomly sampled from a zero-mean Gaussian distribution with a scale parameter of 0.5.
In contrast, for real NVP, we initialised the λvalues using a Glorot normal initialiser (Glorot & Bengio, 2010).
These choices of initialisers were made deliberately to ensure a wide range of initial values, covering both
favourable and unfavourable starting points. Consequently, we expect to observe a diverse range of ELBO
trajectories.
Evaluation settings We report the ELBO and variance of the gradient estimators. The ELBO for
evaluation purpose is always computed with the full dataset (even when using mini-batched ELBO for
optimisation) and 500 samples from q. We also present the variance ratio, V[ˆh]/V[ˆg]where ˆgandˆhas defined
in(7)and(10)respectively, in every 50 iterations; a ratio less than 1 indicates a reduction in variance
relative to the corresponding NoCV with the same number of L. The variance of the gradient estimators is
computed by repeatedly sampling 100 gradients (say, ˆg[1],..., ˆg[100]) from the estimator and computed with
V[ˆg]≈1
100/summationtext
j∥ˆg[j]−(1
100/summationtext
iˆg[i])∥2. See Appendix B for more details on the calculation of the variance ratio.
6.1 ELBO against iteration counts
The results in Figure 1 demonstrate that QuadCV generally outperform NoCV, while ZVCV-GD provides
only marginal improvement and can even converge to a suboptimal maximum in some cases (e.g. logistic
regression, real NVP 2, and L= 10). The performance gap between the estimators also decreases as the
number of gradient samples Lincreases, as seen in the bottom rows of Figure 1a and 1b. It should be
noted that QuadCVs may perform poorly in the early stages of gradient descent (e.g. logistic regression on
mean-field Gaussian and hierarchical Poisson on real NVP) as it takes time to learn the quadratic function ˜f.
In general, there is also a high degree of variability in ELBO across different runs. This is especially noticeable
in Figure 1a due to the substantial impact of λinitialisation on optimisation convergence. For a more detailed
examination of the individual trajectories with various initialisations, please refer to Appendix E.
The variance ratio of the gradient estimators can help explain the performance gap observed in Figure 1. As
shown in Figure 2, QuadCV generally achieves a lower variance than ZVCV-GD, particularly for Gaussian q
when E˜fcan be computed exactly. The estimator with ZVCV-GD and larger Ltends to perform better in
models with fewer control variates (i.e. low dimz), as theβis less susceptible to overfitting when solving the
least squares with the gradient descent algorithm discussed in Section 5.2.2. On the contrary, in models with
large dimz, such as BNNs, ZVCV-GD fails to reduce variance.
A noteworthy characteristic of QuadCV is that variance reduction only becomes prominent after ˜fin(17)
has been adequately trained. This typically occurs as the optimisation process nears convergence. With a
QuadCV-adjusted gradient estimator, it is possible to push the ELBO at convergence a few nats further,
although significant time has to be spent to reach convergence at all. However, this raises an interesting
question about the worthiness of such an effort, as a relatively minor improvement in ELBO may not
necessarily translate into substantially improved downstream metrics; see Appendix E for a more in-depth
discussion.
The comparison between L= 10andL= 50in Figure 1 suggests that variance reduction in the early stages
can facilitate quicker convergence in terms of iteration counts (notice the leftward shift in the trajectories for
L= 50. This observation implies that employing a larger number of gradient samples is an effective strategy
to improve the convergence performance of stochastic VI, as long as the computation of additional gradient
samples remains cost-effective in the overall optimisation process. It is important to note that increasing L
from 10to50immediately reduces the gradient estimator’s variance by five-fold (equivalent to a variance
ratio of 0.2) from the very first iteration of the optimisation, in contrast to QuadCV. These results suggest
that variance reduction is more beneficial during the initial stages of optimisation when the goal is to expedite
convergence towards a satisfactory ELBO, rather than aiming to attain the maximum achievable ELBO.
6.2 ELBO against wall-clock time
10Under review as submission to TMLR
−847−846−845−844
10000 20000 30000
IterationHierarchical Poisson (L=10)
−734−732−730−728
2500 5000 7500 10000
IterationLogistic Regression (L=10)
−1850−1840−1830−1820−1810
10000 15000 20000 25000 30000
IterationMini−batch BNN (L=10)
−170−165−160−155−150
5000 10000 15000 20000 25000
IterationFull−batch BNN (L=10)
−847−846−845−844
10000 20000 30000
IterationHierarchical Poisson (L=50)
−734−732−730−728
2500 5000 7500 10000
IterationLogistic Regression (L=50)
−1850−1840−1830−1820−1810
10000 15000 20000 25000 30000
IterationMini−batch BNN (L=50)
−170−165−160−155−150
5000 10000 15000 20000 25000
IterationFull−batch BNN (L=50)
Control variate NoCV QuadCV ZVCV−GDELBO (EMA)
(a) Mean-field Gaussian with 10 (top) and 50 (bottom) gradient samples.
−3000−2800−2600−2400
0 5000 10000 15000
IterationHierarchical Poisson (L=10)
−800−780−760−740
010002000300040005000
IterationLogistic Regression (L=10)
−2760−2730−2700−2670−2640
0 10000 20000 30000
IterationMini−batch BNN (L=10)
−260−250−240
0 5000 10000 15000
IterationFull−batch BNN (L=10)
−3000−2800−2600−2400
0 5000 10000 15000
IterationHierarchical Poisson (L=50)
−800−780−760−740
010002000300040005000
IterationLogistic Regression (L=50)
−2760−2730−2700−2670−2640
0 10000 20000 30000
IterationMini−batch BNN (L=50)
−260−250−240
0 5000 10000 15000
IterationFull−batch BNN (L=50)
Control variate NoCV QuadCV ZVCV−GDELBO (EMA)
(b) Real NVP with 10 (top) and 50 (bottom) gradient samples.
Figure 1: ELBO is plotted against the number of gradient descent steps for different numbers of gradient
samplesLand two families of q. The bold lines represent the median of ELBO values recorded at the same
iteration across five repetitions. The shaded area illustrates the range of ELBO values across five repetitions.
The ELBO values are smoothed using an exponential moving average. The trajectories of ZVCV-GD and
NoCV are nearly identical in both full-batch and mini-batch BNN when L= 10. A higher ELBO indicates
better performance. See Figure 6 for plots where the bold lines represent the mean ELBO.
11Under review as submission to TMLR
0.00.30.60.9
0 10000 20000 30000
IterationHierarchical Poisson
0.00.30.60.9
0 2500 5000 7500 10000
IterationLogistic Regression
0.00.51.0
0 10000 20000 30000
IterationMini−batch BNN
0.00.51.0
0500010000 15000 20000 25000
IterationFull−batch BNN
0.00.30.60.9
0 5000 10000 15000
IterationHierarchical Poisson
0.00.30.60.9
01000 2000 3000 4000 5000
IterationLogistic Regression
0.00.51.0
0 10000 20000 30000
IterationMini−batch BNN
0.00.51.0
0 5000 10000 15000
IterationFull−batch BNNVariance ratio
Control variate NoCV QuadCV ZVCV−GD Gradient samples (L) 10 50
Figure 2: We present the variance ratio V[ˆh]/V[ˆg], where ˆgis NoCV and ˆhis either ZVCV-GD or QuadCV, at
each iteration. We show only the median variance ratios recorded at the same iteration across five repetitions,
omitting the individual variance ratios from each repetition to prevent clutter in the plots. The ratios from
mean-field Gaussian and real NVP are shown in top and bottom rows respectively. Note that NoCV (in red)
is always 1 by definition. We see that ZVCV-GD (in blue) struggles to reduce variance in the BNN models.
There is also a significant overlap in QuadCV between L= 10(solid green) and L= 50(dotted green). A
lower ratio indicates better performance. See Figure 7 for plots where the bold lines represent the mean
variance ratios.
To assess whether the computational expense of calculating control variates or additional gradient samples
justifies the potential improvement in ELBO, we measure ELBO against wall-clock time, as illustrated in
Figure 3. We timed our VI implementation in JAX and ran on an Nvidia A100 80GB GPU. It is worth
noting that recorded times may vary among computing platforms and implementations, given that our code
was compiled with XLA (resulting in platform-dependent binaries) and ran without memory constraints.
Our experiments reveal that NoCV generally converges to a respectable ELBO more swiftly. Furthermore, the
performance gap between the estimators is even narrower when L= 50. An unexpected observation is that
increasingLfrom 10 to 50 incurs negligible computational cost but produce meaningfully faster convergence,
as evident when comparing the top and bottom rows of Figure 3a and 3b. It is important to note that the
computational cost of extra gradient samples may vary depending on the construction of φ, and increasing
Lmight not always be a worthwhile strategy for achieving faster convergence (see, for example, the BNNs
experiments in Figure 4b of Appendix C).
QuadCV does succeed in increasing the maximum achievable ELBO in certain scenarios, albeit at the expense
of longer convergence times. For instance, QuadCV can improve ELBO by approximately 0.7 nats and 6
nats in hierarchical Poisson and full-batch BNN when using a mean-field Gaussian qatL= 10. However,
this comes at a cost of roughly 50% to 100% more runtime compared to NoCV. Given finite computational
resources and the absence of a universal guarantee that a slight ELBO increase will substantially enhance
downstream metrics (as discussed in, for example, Yao et al., 2018; 2019; Foong et al., 2020; Masegosa, 2020;
Deshpande et al., 2022), it is left to practitioners to determine whether implementing control variates is a
worthwhile endeavour.
7 Conclusion
In our study of the pathwise gradient estimator in VI, we reviewed the existing state-of-the-art control
variates for reducing gradient variance, namely the QuadCV in Geffner & Domke (2020). We identified a gap
in the literature of variance reduction of pathwise gradient estimators in stochastic VI resulting from the
12Under review as submission to TMLR
−847−846−845−844
5 10 15 20
Time (seconds)Hierarchical Poisson (L=10)
−734−732−730−728
0 2 4 6
Time (seconds)Logistic Regression (L=10)
−1850−1840−1830−1820−1810
6 8 10 12
Time (seconds)Mini−batch BNN (L=10)
−170−165−160−155−150
5.0 7.5 10.0
Time (seconds)Full−batch BNN (L=10)
−847−846−845−844
5 10 15 20
Time (seconds)Hierarchical Poisson (L=50)
−734−732−730−728
0 2 4 6
Time (seconds)Logistic Regression (L=50)
−1850−1840−1830−1820−1810
6 8 10 12
Time (seconds)Mini−batch BNN (L=50)
−170−165−160−155−150
5.0 7.5 10.0
Time (seconds)Full−batch BNN (L=50)
Control variate NoCV QuadCV ZVCV−GDELBO (EMA)
(a) Mean-field Gaussian with 10 (top) and 50 (bottom) gradient samples.
−3000−2800−2600−2400
0 5 10 15 20
Time (seconds)Hierarchical Poisson (L=10)
−800−780−760−740
012345
Time (seconds)Logistic Regression (L=10)
−2760−2730−2700−2670−2640
0 20 40 60
Time (seconds)Mini−batch BNN (L=10)
−260−250−240
0 10 20 30
Time (seconds)Full−batch BNN (L=10)
−3000−2800−2600−2400
0 5 10 15 20
Time (seconds)Hierarchical Poisson (L=50)
−800−780−760−740
012345
Time (seconds)Logistic Regression (L=50)
−2760−2730−2700−2670−2640
0 20 40 60
Time (seconds)Mini−batch BNN (L=50)
−260−250−240
0 10 20 30
Time (seconds)Full−batch BNN (L=50)
Control variate NoCV QuadCV ZVCV−GDELBO (EMA)
(b) Real NVP with 10 (top) and 50 (bottom) gradient samples.
Figure 3: ELBO is plotted against wall-clock time for different numbers of gradient samples Land two
families of q. The bold lines represent the median of ELBO values recorded at the same iteration across
five repetitions. The shaded area illustrates the range of ELBO values across five repetitions. The ELBO
values are smoothed using an exponential moving average. A higher ELBO indicates better performance. See
Figure 8 for plots where the bold lines represent the mean ELBO.
13Under review as submission to TMLR
setting where the variational distribution has intractable mean and covariance, rendering the state-of-the-art
represented by Geffner & Domke (2020) not applicable directly. To address this gap, we proposed using
ZVCV, which does not assume specific conditions on the variational distribution. However, our empirical
results showed that neither the ZVCV-adjusted nor the QuadCV-adjusted estimator provided substantial
improvement against our evaluation criteria that justifies their implementation. Instead, we found that
increasing the number of gradient samples is a highly cost-effective method for improving convergence time.
Taking a step back, it is worth discussing the fundamental value in performing variance reduction for pathwise
gradient estimators in stochastic VI. For one, it is quite interesting that a dramatic reduction in gradient
variance can fail to deliver any discernible effect on the ELBO. This is what was observed in the experiments
section — even when the variance ratio was substantially lower than 1, the control variate-adjusted gradient
estimator, compared to the vanilla gradient estimator, did not move the needle in a meaningful manner
on the ELBO optimisation objective. As such, it can be expected that downstream metrics including log
pointwise predictive density or predictive mean squared error will also reveal the general futility of equipping
the gradient estimator with a control variate. These findings seem to point to a negative phenomenon for
pathwise gradients in stochastic VI — reducing the gradient variance is insufficient to improving downstream
performance.
In future work, we hope to explore ZVCV-adjusted gradient estimators in generative models where it can
truly shine. Namely, ZVCV is particularly powerful when the distribution of interest is difficult to sample
from. One class of distribution models that fit this description are energy-based models (Song & Kingma,
2021).
Relatedly, there is a class of stochastic VI methods known as implicit VI. The variational distribution
employed is still required to be reparametrizable but we drop the requirement that the so-called pathwise
score,∇zlogq(z;λ), be known, e.g. as in normalizing flows. It was shown in Titsias & Ruiz (2019) that the
pathwise score may itself be written as an expectation, ∇zlogq(z;λ) =Eq(ϵ|z;λ)∇zlogq(z|ϵ;λ)whereq(ϵ|z;λ)
is referred to as the reverse conditional. In Titsias & Ruiz (2019), the expectation with respect to the reverse
conditional is based on MCMC samples. We could conceivably improve the efficiency by employing ZVCV
here.
References
Roland Assaraf and Michel Caffarel. Zero-Variance Principle for Monte Carlo Algorithms. Physical Review
Letters, 83(23):4682–4685, December 1999. doi: 10.1103/PhysRevLett.83.4682.
D. V. Belomestny, L. S. Iosipoi, and N. K. Zhivotovskiy. Variance Reduction in Monte Carlo Estimators via
Empirical Variance Minimization. Doklady Mathematics , 98(2):494–497, September 2018. ISSN 1531-8362.
doi: 10.1134/S1064562418060261.
Sameer Deshpande, Soumya Ghosh, Tin D. Nguyen, and Tamara Broderick. Are you using test log-likelihood
correctly? In I Can’t Believe It’s Not Better Workshop: Understanding Deep Learning Through Empirical
Falsification , December 2022.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. In International
Conference on Learning Representations , 2017.
Andrew Foong, David Burt, Yingzhen Li, and Richard Turner. On the Expressiveness of Approximate
Inference in Bayesian Neural Networks. In Advances in Neural Information Processing Systems , volume 33,
pp. 15897–15908. Curran Associates, Inc., 2020.
Tomas Geffner and Justin Domke. Using Large Ensembles of Control Variates for Variational Inference. In
Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018.
TomasGeffnerandJustinDomke. ApproximationBasedVarianceReductionforReparameterizationGradients.
InAdvances in Neural Information Processing Systems , volume 33, pp. 2397–2407. Curran Associates, Inc.,
2020.
14Under review as submission to TMLR
Andrew Gelman, Jeffrey Fagan, and Alex Kiss. An Analysis of the New York City Police Department’s
“Stop-and-Frisk” Policy in the Context of Claims of Racial Bias. Journal of the American Statistical
Association , 102(479):813–823, September 2007. ISSN 0162-1459. doi: 10.1198/016214506000001040.
Andrew Gelman, Hal S. Stern, John B. Carlin, David B. Dunson, Aki Vehtari, and Donald B. Rubin. Bayesian
Data Analysis . CRC Press, third edition, 2013.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks.
InProceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics , pp.
249–256. JMLR Workshop and Conference Proceedings, March 2010.
Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic Variational Inference.
Journal of Machine Learning Research , 14(40):1303–1347, 2013. ISSN 1533-7928.
Geng Ji, Debora Sujono, and Erik B. Sudderth. Marginalized Stochastic Natural Gradients for Black-Box
Variational Inference. In Proceedings of the 38th International Conference on Machine Learning , pp.
4870–4881. PMLR, July 2021.
Andres Masegosa. Learning under Model Misspecification: Applications to Variational and Ensemble methods.
InAdvances in Neural Information Processing Systems , volume 33, pp. 5479–5491. Curran Associates, Inc.,
2020.
Andrew Miller, Nick Foti, Alexander D’ Amour, and Ryan P Adams. Reducing Reparameterization Gradient
Variance. In Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017.
Antonietta Mira, Reza Solgi, and Daniele Imparato. Zero variance Markov chain Monte Carlo for Bayesian
estimators. Statistics and Computing , 23(5):653–662, September 2013. ISSN 1573-1375. doi: 10.1007/
s11222-012-9344-6.
Chris J. Oates, Theodore Papamarkou, and Mark Girolami. The Controlled Thermodynamic Integral for
Bayesian Model Evidence Evaluation. Journal of the American Statistical Association , 111(514):634–645,
April 2016. ISSN 0162-1459. doi: 10.1080/01621459.2015.1021006.
Chris J. Oates, Mark Girolami, and Nicolas Chopin. Control functionals for Monte Carlo integration. Journal
of the Royal Statistical Society. Series B (Statistical Methodology) , 79(3):695–718, 2017. ISSN 1369-7412.
Rajesh Ranganath, Sean Gerrish, and David M. Blei. Black Box Variational Inference. In Proceedings of the
Seventeenth International Conference on Artificial Intelligence and Statistics , pp. 814–822. PMLR, April
2014.
Shijing Si, Chris. J. Oates, Andrew B. Duncan, Lawrence Carin, and François-Xavier Briol. Scalable Control
Variates for Monte Carlo Methods Via Stochastic Optimization. In Alexander Keller (ed.), Monte Carlo
and Quasi-Monte Carlo Methods , Springer Proceedings in Mathematics & Statistics, pp. 205–221, Cham,
2022. Springer International Publishing. ISBN 978-3-030-98319-2. doi: 10.1007/978-3-030-98319-2_10.
Yang Song and Diederik P. Kingma. How to Train Your Energy-Based Models, February 2021.
L. F. South, C. J. Oates, A. Mira, and C. Drovandi. Regularized Zero-Variance Control Variates. Bayesian
Analysis, -1(-1):1–24, January 2022. ISSN 1936-0975, 1931-6690. doi: 10.1214/22-BA1328.
Michalis K. Titsias and Francisco Ruiz. Unbiased Implicit Variational Inference. In Proceedings of the
Twenty-Second International Conference on Artificial Intelligence and Statistics , pp. 167–176. PMLR, April
2019.
Jiayu Yao, Weiwei Pan, Soumya Ghosh, and Finale Doshi-Velez. Quality of Uncertainty Quantification for
Bayesian Neural Network Inference, June 2019.
Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Yes, but Did It Work?: Evaluating Variational
Inference. In Proceedings of the 35th International Conference on Machine Learning , pp. 5581–5590. PMLR,
July 2018.
15Under review as submission to TMLR
A Models and datasets
Logistic regression with the a1adataset We extracted the a1adataset from the repository hosting
Geffner & Domke (2020). We used the full dataset {xi,yi}1605
i=1and 90% of the dataset for training. The
responseyiis binary and is modelled as
w0,w∼N(0,102)
p(yi|xi,z) =Bernoulli/parenleftbigg1
1 + exp(−w0−wTxi)/parenrightbigg
,
wherez={w0,w}anddimz= 120. The size of training and test sets are 1440 and 165 respectively.
Hierarchical Poisson regression with the friskdataset This example is coming from Gelman et al.
(2007). We only used a subset of data (weapon-related crime, precincts with 10%-40% of black proportion),
as in Miller et al. (2017) and Geffner & Domke (2020). The response yepdenotes the number of frisk events
due to weapons crimes within an ethnicity group ein precinct pover a 15-months period in New York City:
µ∼N(0,102)
logσα,logσβ∼N(0,102)
αe∼N(0,σ2
α)
βp∼N(0,σ2
β)
logλep=µ+αe+βp+ logNep
p(yep|z) =Poisson (λep),
wherez={α1,α2,β1,...,β 32,µ,logσα,logσβ}anddimz= 37.Nepis the (scaled) total number of arrests of
ethnicity group ein precinct pover the same period of time. We do not split out a test set due to its small
size (total data size is 96).
Bayesian neural network with the redwinedataset We push a vector input xithrough a 50-unit
hidden layer and ReLU activation’s to predict wine quality. The response yiis an integer from 1 to 10
(inclusive) measuring the score of red wine. We place an uniform improper prior on the log-variance of the
weights and error (see Section 5.7 of Gelman et al., 2013, for a discussion on the prior choice):
p(logα2)∝1,equivalent to p(α)∝α−1
p(logτ2)∝1,equivalent to p(τ)∝τ−1
wi∼N(0,α2), i= 1,..., 651
yi|xi,w,τ∼N(ϕ(x,w),τ2)
whereϕis a multi-layer perception. Here, z={logα2,logτ2,w}anddimz= 653. For full-batch gradient
descent, we use two mutually exclusive subsets of 100 data point as train and test sets, as in Miller et al.
(2017) and Geffner & Domke (2020). For mini-batch gradient descent, we use 90% of the full dataset for
training and the rest for testing (size of train and test sets are 1431 and 168 respectively).
B Computation of variance ratio
The variance ratio V[ˆh]/V[ˆg]was computed with the following step:
1. Collect 100 samples of ˆgresulting in{ˆg[j]}100
j=1;
2.For each ˆg[j], compute its corresponding control-variate-adjusted gradient estimate ˆh(10)to collect
{ˆh[j]}100
j=1;
16Under review as submission to TMLR
3. Estimate V[ˆg]≈1
100/summationtext
j∥ˆg[j]−(1
100/summationtext
iˆg[i])∥2. Repeat the same step for V[ˆh];
4. Calculate the ratio V[ˆh]/V[ˆg].
This ratio is designed to evaluate the effectiveness of control variates in reducing variance relative to a
corresponding gradient estimator without control variates. Therefore, in our work, the ratio is always
computed with a pair of ˆgandˆhwith the same number of samples.
C Results from rank-5 Gaussian
The insight derived from Figure 4 and 5 below are similar to those obtained from Figure 1, 3 and 2. In most
cases, the cost for evaluating control variates outweighs the improvement in ELBO achieved through variance
reduction in the gradient estimator. We observe marginal gain in ELBO despite the estimators with control
variates taking longer time to converge.
17Under review as submission to TMLR
−880−870−860−850−840
10000 20000 30000
IterationHierarchical Poisson (L=10)
−740−730−720−710
2500 5000 7500 10000
IterationLogistic Regression (L=10)
−5000−4000−3000−2000
10000 15000 20000 25000 30000
IterationMini−batch BNN (L=10)
−500−400−300−200
5000 10000 15000 20000 25000
IterationFull−batch BNN (L=10)
−880−870−860−850−840
10000 20000 30000
IterationHierarchical Poisson (L=50)
−740−730−720−710
2500 5000 7500 10000
IterationLogistic Regression (L=50)
−5000−4000−3000−2000
10000 15000 20000 25000 30000
IterationMini−batch BNN (L=50)
−500−400−300−200
5000 10000 15000 20000 25000
IterationFull−batch BNN (L=50)
Control variate NoCV QuadCV ZVCV−GDELBO (EMA)
(a) ELBO versus iteration counts.
−880−870−860−850−840
0 10 20 30
Time (seconds)Hierarchical Poisson (L=10)
−740−730−720−710
0 10 20 30 40
Time (seconds)Logistic Regression (L=10)
−5000−4000−3000−2000
0100200300400500
Time (seconds)Mini−batch BNN (L=10)
−500−400−300−200
0100200300400500
Time (seconds)Full−batch BNN (L=10)
−880−870−860−850−840
0 10 20 30
Time (seconds)Hierarchical Poisson (L=50)
−740−730−720−710
0 10 20 30 40
Time (seconds)Logistic Regression (L=50)
−5000−4000−3000−2000
0100200300400500
Time (seconds)Mini−batch BNN (L=50)
−500−400−300−200
0100200300400500
Time (seconds)Full−batch BNN (L=50)
Control variate NoCV QuadCV ZVCV−GDELBO (EMA)
(b) ELBO versus wall-clock time.
Figure 4: ELBO is plotted against gradient descent steps and wall-clock time for varying numbers of gradient
samplesLusing rank-5 Gaussian. The bold lines represent the median of ELBO values recorded at the same
iteration across five repetitions. The ELBO values have been smoothed using an exponential moving average.
A higher ELBO indicates better performance. See Figure 9 for plots where the bold lines represent the mean
ELBO.
18Under review as submission to TMLR
0.00.30.60.9
0 10000 20000 30000
IterationHierarchical Poisson
0.00.30.60.9
0 2500 5000 7500 10000
IterationLogistic Regression
0.00.51.0
0 10000 20000 30000
IterationMini−batch BNN
0.00.51.0
0500010000 15000 20000 25000
IterationFull−batch BNN         Variance ratio
Control variate NoCV QuadCV ZVCV−GD Gradient samples (L) 10 50
(a) Variance ratio
Figure 5: We present the variance ratio V[ˆh]/V[ˆg]of rank-5 Gaussian, where ˆgis NoCV and ˆhis either
ZVCV-GD or QuadCV, at each iteration. We show only the median variance ratios recorded at the same
iteration across five repetitions, omitting the individual variance ratios from each repetition to prevent clutter
in the plots. Note that NoCV (in red) is always 1 by definition. We see that ZVCV-GD (in blue) struggles to
reduce variance in the BNN models. There is also some overlap between L= 10(solid green) and L= 50
(dotted green). A lower ratio indicates better performance. A lower ratio indicates better performance. See
Figure 10 for plots where the bold lines represent the mean variance ratios.
19Under review as submission to TMLR
D Mean ELBO trajectories and variance ratio
We have recreated the figures in Section 6 and Appendix C, with the exception that the bold lines now
represent the means of ELBO or variance ratios, as opposed to their medians. Using means provides a more
transparent depiction of the robustness of each method, although it can be substantially influenced by the
repetition that starts farthest from the optimal λ. Ideally, individual trajectories should be plotted separately
(as in Appendix E), but this is not feasible due to space limitations. Nonetheless, the findings of this study
are substantiated by interpreting either the mean or median of the evaluation statistics.
−847−846−845−844
10000 20000 30000
IterationHierarchical Poisson (L=10)
−734−732−730−728
2500 5000 7500 10000
IterationLogistic Regression (L=10)
−1850−1840−1830−1820−1810
10000 15000 20000 25000 30000
IterationMini−batch BNN (L=10)
−170−165−160−155−150
5000 10000 15000 20000 25000
IterationFull−batch BNN (L=10)
−847−846−845−844
10000 20000 30000
IterationHierarchical Poisson (L=50)
−734−732−730−728
2500 5000 7500 10000
IterationLogistic Regression (L=50)
−1850−1840−1830−1820−1810
10000 15000 20000 25000 30000
IterationMini−batch BNN (L=50)
−170−165−160−155−150
5000 10000 15000 20000 25000
IterationFull−batch BNN (L=50)
Control variate NoCV QuadCV ZVCV−GDELBO (EMA)
(a) Mean-field Gaussian with 10 (top) and 50 (bottom) gradient samples.
−3000−2800−2600−2400
0 5000 10000 15000
IterationHierarchical Poisson (L=10)
−800−780−760−740
010002000300040005000
IterationLogistic Regression (L=10)
−2760−2730−2700−2670−2640
0 10000 20000 30000
IterationMini−batch BNN (L=10)
−260−250−240
0 5000 10000 15000
IterationFull−batch BNN (L=10)
−3000−2800−2600−2400
0 5000 10000 15000
IterationHierarchical Poisson (L=50)
−800−780−760−740
010002000300040005000
IterationLogistic Regression (L=50)
−2760−2730−2700−2670−2640
0 10000 20000 30000
IterationMini−batch BNN (L=50)
−260−250−240
0 5000 10000 15000
IterationFull−batch BNN (L=50)
Control variate NoCV QuadCV ZVCV−GDELBO (EMA)
(b) Real NVP with 10 (top) and 50 (bottom) gradient samples.
Figure 6: ELBO is plotted against the number of gradient descent steps for different numbers of gradient
samplesLand two families of q. The bold lines represent the mean of ELBO values recorded at the same
iteration across five repetitions. The shaded area illustrates the range of ELBO values across five repetitions.
The ELBO values are smoothed using an exponential moving average. The trajectories of ZVCV-GD and
NoCV are nearly identical in both full-batch and mini-batch BNN when L= 10. A higher ELBO indicates
better performance. See Figure 1 for plots where the bold lines represent the median ELBO.
20Under review as submission to TMLR
0.00.30.60.9
0 10000 20000 30000
IterationHierarchical Poisson
0.00.30.60.9
0 2500 5000 7500 10000
IterationLogistic Regression
0.00.51.0
0 10000 20000 30000
IterationMini−batch BNN
0.00.51.0
0500010000 15000 20000 25000
IterationFull−batch BNN
0.00.30.60.9
0 5000 10000 15000
IterationHierarchical Poisson
0.00.30.60.9
01000 2000 3000 4000 5000
IterationLogistic Regression
0.00.51.0
0 10000 20000 30000
IterationMini−batch BNN
0.00.51.0
0 5000 10000 15000
IterationFull−batch BNNVariance ratio
Control variate NoCV QuadCV ZVCV−GD Gradient samples (L) 10 50
Figure 7: We present the variance ratio V[ˆh]/V[ˆg], where ˆgis NoCV and ˆhis either ZVCV-GD or QuadCV, at
each iteration. We show only the mean variance ratios recorded at the same iteration across five repetitions,
omitting the individual variance ratios from each repetition to prevent clutter in the plots. The ratios from
mean-field Gaussian and real NVP are shown in top and bottom rows respectively. Note that NoCV (in red)
is always 1 by definition. We see that ZVCV-GD (in blue) struggles to reduce variance in the BNN models.
There is also a significant overlap in QuadCV between L= 10(solid green) and L= 50(dotted green). A
lower ratio indicates better performance. See Figure 2 for plots where the bold lines represent the median
variance ratios.
21Under review as submission to TMLR
−847−846−845−844
5 10 15 20
Time (seconds)Hierarchical Poisson (L=10)
−734−732−730−728
0 2 4 6
Time (seconds)Logistic Regression (L=10)
−1850−1840−1830−1820−1810
5 10 15 20
Time (seconds)Mini−batch BNN (L=10)
−170−165−160−155−150
4 8 12
Time (seconds)Full−batch BNN (L=10)
−847−846−845−844
5 10 15 20
Time (seconds)Hierarchical Poisson (L=50)
−734−732−730−728
0 2 4 6
Time (seconds)Logistic Regression (L=50)
−1850−1840−1830−1820−1810
5 10 15 20
Time (seconds)Mini−batch BNN (L=50)
−170−165−160−155−150
4 8 12
Time (seconds)Full−batch BNN (L=50)
Control variate NoCV QuadCV ZVCV−GDELBO (EMA)
(a) Mean-field Gaussian with 10 (top) and 50 (bottom) gradient samples.
−3000−2800−2600−2400
0 5 10 15 20
Time (seconds)Hierarchical Poisson (L=10)
−800−780−760−740
012345
Time (seconds)Logistic Regression (L=10)
−2760−2730−2700−2670−2640
0 20 40 60
Time (seconds)Mini−batch BNN (L=10)
−260−250−240
0 10 20 30
Time (seconds)Full−batch BNN (L=10)
−3000−2800−2600−2400
0 5 10 15 20
Time (seconds)Hierarchical Poisson (L=50)
−800−780−760−740
012345
Time (seconds)Logistic Regression (L=50)
−2760−2730−2700−2670−2640
0 20 40 60
Time (seconds)Mini−batch BNN (L=50)
−260−250−240
0 10 20 30
Time (seconds)Full−batch BNN (L=50)
Control variate NoCV QuadCV ZVCV−GDELBO (EMA)
(b) Real NVP with 10 (top) and 50 (bottom) gradient samples.
Figure 8: ELBO is plotted against wall-clock time for different numbers of gradient samples Land two
families ofq. The bold lines represent the mean of ELBO values recorded at the same iteration across five
repetitions. The shaded area illustrates the range of ELBO values across five repetitions. The ELBO values
are smoothed using an exponential moving average. A higher ELBO indicates better performance. See
Figure 3 for plots where the bold lines represent the median ELBO.
22Under review as submission to TMLR
−2000−1750−1500−1250−1000
10000 20000 30000
IterationHierarchical Poisson (L=10)
−740−730−720−710
2500 5000 7500 10000
IterationLogistic Regression (L=10)
−5000−4000−3000−2000
10000 15000 20000 25000 30000
IterationMini−batch BNN (L=10)
−500−400−300−200
5000 10000 15000 20000 25000
IterationFull−batch BNN (L=10)
−2000−1750−1500−1250−1000
10000 20000 30000
IterationHierarchical Poisson (L=50)
−740−730−720−710
2500 5000 7500 10000
IterationLogistic Regression (L=50)
−5000−4000−3000−2000
10000 15000 20000 25000 30000
IterationMini−batch BNN (L=50)
−500−400−300−200
5000 10000 15000 20000 25000
IterationFull−batch BNN (L=50)
Control variate NoCV QuadCV ZVCV−GDELBO (EMA)
(a) ELBO versus iteration counts.
−2000−1750−1500−1250−1000
0 10 20 30
Time (seconds)Hierarchical Poisson (L=10)
−740−730−720−710
0 10 20 30 40
Time (seconds)Logistic Regression (L=10)
−5000−4000−3000−2000
0100200300400500
Time (seconds)Mini−batch BNN (L=10)
−500−400−300−200
0100200300400500
Time (seconds)Full−batch BNN (L=10)
−2000−1750−1500−1250−1000
0 10 20 30
Time (seconds)Hierarchical Poisson (L=50)
−740−730−720−710
0 10 20 30 40
Time (seconds)Logistic Regression (L=50)
−5000−4000−3000−2000
0100200300400500
Time (seconds)Mini−batch BNN (L=50)
−500−400−300−200
0100200300400500
Time (seconds)Full−batch BNN (L=50)
Control variate NoCV QuadCV ZVCV−GDELBO (EMA)
(b) ELBO versus wall-clock time.
Figure 9: ELBO is plotted against gradient descent steps and wall-clock time for varying numbers of gradient
samplesLusing rank-5 Gaussian. The bold lines represent the mean of ELBO values recorded at the same
iteration across five repetitions. The ELBO values have been smoothed using an exponential moving average.
A higher ELBO indicates better performance. See Figure 4 for plots where the bold lines represent the
median ELBO.
23Under review as submission to TMLR
0.00.30.60.9
0 10000 20000 30000
IterationHierarchical Poisson
0.00.30.60.9
0 2500 5000 7500 10000
IterationLogistic Regression
0.00.51.0
0 10000 20000 30000
IterationMini−batch BNN
0.00.51.0
0500010000 15000 20000 25000
IterationFull−batch BNN         Variance ratio
Control variate NoCV QuadCV ZVCV−GD Gradient samples (L) 10 50
(a) Variance ratio
Figure 10: We present the variance ratio V[ˆh]/V[ˆg]of rank-5 Gaussian, where ˆgis NoCV and ˆhis either
ZVCV-GD or QuadCV, at each iteration. We show only the mean variance ratios recorded at the same
iteration across five repetitions, omitting the individual variance ratios from each repetition to prevent clutter
in the plots. Note that NoCV (in red) is always 1 by definition. We see that ZVCV-GD (in blue) struggles to
reduce variance in the BNN models. There is also some overlap between L= 10(solid green) and L= 50
(dotted green). A lower ratio indicates better performance. See Figure 5 for plots where the bold lines
represent the median variance ratios.
24Under review as submission to TMLR
E Individual runs of full-batch BNN with mean-field Gaussian
We zoom in on a particular model and variational family from the experiments in the main text. Our aim in
this section is to look the trajectory according to each initialisation separately to help visualise the impact of
initialisation on convergence. Due to space limitations, we have only included trajectories from full-batch
BNN with mean-field Gaussian. In addition to the ELBO reported in the main text, we also report the
downstream metric, log pointwise predictive density evaluated on a test set (test lppd), which is popular in
the VI literature. Mathematically, the test lppd is defined as
/summationdisplay
x∈Dtestlog/parenleftigg
|Z|−1/summationdisplay
z∈Zp(x|z)/parenrightigg
.
Here,Dtestrepresents a test set, Zis a set of samples drawn from q(z;λ), and|Z|indicates the cardinality of
Z. We have set|Z|= 1000in our experiments. The test lppd is also referred to as the test log-likelihood,
test log-predictive, or predictive log-likelihood in the literature (see, for example, Yao et al., 2019; Deshpande
et al., 2022).
Figures 11a clearly show that trajectories vary substantially with different initialisations. This is consistent
with the high variability of ELBO trajectories in Figure 1.
In all cases, increasing L, the number of gradient samples, effectively reduces the variance of the gradient
estimator from the outset of the optimisation process. This stands in contrast to QuadCV, which only
becomes effective after the quadratic approximation ˜fin(17)has been adequately trained (Figure 11b).
Consequently, QuadCV performs poorly in the early and middle stages of optimisation (as seen in Repetitions
2 and 3 in Figure 11a).
Prior research on variance reduction in pathwise gradient estimators (e.g. Miller et al., 2017; Geffner &
Domke, 2018; 2020) often aims to push the boundaries of attainable ELBO. Achieving this typically requires
longer training periods. However, we are of the opinion that the additional ELBO gained through this effort
does not warrant the extra computational cost incurred by implementing control variates. This is particularly
relevant given that improvements in downstream metrics, such as test lppd, are marginal when compared to
improvements achieved in the earlier stages of optimisation (note the y-axis scale in Figure 12a and 12b).
For instance, in Repetition 1, there is only a 3 nats improvement in test lppd (over a test set of size 100),
while substantial improvements are observed in the earlier stages, often in the scale of hundreds. These 3
nats come at a cost of over 50% additional computation time compared to NoCV (as indicated in Figure 3a).
Furthermore, it is worth noting that an improvement in ELBO does not invariably guarantee a substantial
improvement in downstream statistics, as evidenced in previous works, such as Yao et al. (2018; 2019); Foong
et al. (2020); Masegosa (2020); Deshpande et al. (2022).
25Under review as submission to TMLR
Rep 1 Rep 2 Rep 3 Rep 4 Rep 5
05000100001500020000250000500010000150002000025000050001000015000200002500005000100001500020000250000500010000150002000025000−1000−750−500−250
IterationELBO (EMA)
Control variate NoCV QuadCV ZVCV−GD Gradient samples (L) 10 50
(a) ELBO trajectories. This is a zoomed-out version of the last column of Figure 1a. Higher values are preferred.
Rep 1 Rep 2 Rep 3 Rep 4 Rep 5
050001000015000200002500005000100001500020000250000500010000150002000025000050001000015000200002500005000100001500020000250000.00.51.01.52.0
IterationVariance ratio
Control variate NoCV QuadCV ZVCV−GD Gradient samples (L) 10 50
(b) Variance ratios. A reading of 1 indicates no variance reduction. Lower values are preferable.
Figure 11: The trajectories of ELBO and variance ratio for full-batch BNN with mean-field Gaussian are
depicted over the course of iterations, with each of the five repetitions presented individually. By definition,
the variance ratio of NoCV (red) is 1. Notably, there is a substantial overlap between NoCV (in red) and
ZVCV-GD (in blue). In some cases, the distinctions between all three methods are hardly discernible.
However, there is a relatively noticeable difference between L= 10andL= 50.
26Under review as submission to TMLR
Rep 1 Rep 2 Rep 3 Rep 4 Rep 5
05000100001500020000250000500010000150002000025000050001000015000200002500005000100001500020000250000500010000150002000025000−400−300−200
IterationLPPD (EMA)
Control variate NoCV QuadCV ZVCV−GD Gradient samples (L) 10 50
(a) Test lppd trajectories.
Rep 1 Rep 2 Rep 3 Rep 4 Rep 5
05000100001500020000250000500010000150002000025000050001000015000200002500005000100001500020000250000500010000150002000025000−145.0−142.5−140.0−137.5−135.0
IterationLPPD (EMA)
Control variate NoCV QuadCV ZVCV−GD Gradient samples (L) 10 50
(b) Test lppd trajectories, zooming in between lppd = ( −145 ,−135).
Figure 12: The trajectories of test lppd for full-batch BNN with mean-field Gaussian are depicted over the
course of iterations, with each of the five repetitions presented individually. Notably, there is a substantial
overlap between NoCV (in red) and ZVCV-GD (in blue). In some cases, the distinctions between all three
methods are hardly discernible. However, there is a relatively noticeable difference between L= 10and
L= 50. Higher values are preferred.
27Under review as submission to TMLR
F Comparison of ZVCV-GD with different hyperparameters
We conducted experiments with ZVCV-GD that explore various hyperparameter settings, running with both
first- and second-order polynomials (Figure 13), and testing different number of steps in the inner gradient
descent loop (Figure 14). We focus on the hierarchical Poisson model using a mean-field Gaussian and setting
L= 10. We repeated the experiment five times, each time with different initialisations. The red trajectories
in Figure 13 and 14 correspond to the default settings of ZVCV-GD as specified in Section 6.
Figure 13b reveals that second-order ZVCV-GD did not effectively reduce variance in the gradient estimator;
instead, it introduced additional noise into the estimator. This detrimental impact is also evident in the
ELBO trajectories, as shown in Figure 13a. In light of these findings, we concluded that the simpler first-order
ZVCV-GD is preferable over the second-order variant.
Rep 1 Rep 2 Rep 3 Rep 4 Rep 5
010000 20000 30000010000 20000 30000010000 20000 30000010000 20000 30000010000 20000 30000−1000−950−900−850
IterationELBO (EMA)
ZVCV order 12
(a) ELBO trajectories. Higher values are preferable.
Rep 1 Rep 2 Rep 3 Rep 4 Rep 5
010000 20000 30000010000 20000 30000010000 20000 30000010000 20000 30000010000 20000 300000.00.51.01.52.0
IterationVariance ratio
ZVCV order 12
(b) Variance ratios. A reading of 1 indicates no variance reduction. Lower values are preferable.
Figure 13: ELBO trajectories and variance ratios for hierarchical Poisson models using mean-field Gaussian,
ZVCV-GD with L= 10, and first- and second-order ZVCV-GD both with 4 inner GD steps. The experiment
was repeated five times, each time with different initialisations.
In Figure 14, we present the ELBO trajectories and variance ratios obtained by running the inner gradient
descent (GD) of ZVCV-GD with three different settings: 4 steps, 20 steps, and ‘until convergence’. Here,
‘convergence’ is defined as the point at which the residual of the inner least squares problem in (13)no longer
decreases substantially.
We observe that running the inner GD until convergence does not necessarily yield the greatest variance
reduction, as illustrated in Figure 14b. This phenomenon can be attributed to overfitting the linear regression
in(13), where the number of rows in Cis considerably smaller than the number of columns. On the other
hand, iterating the inner GD 20 times achieves a more substantial variance reduction compared to the default
4 steps.
28Under review as submission to TMLR
However, it is worth highlighting that there is no discernible impact on the ELBO trajectories when varying
the number of GD steps, as demonstrated in Figure 14a.
The optimal number of steps is not always evident without experimentation. Hence, we typically opt for 4
steps to balance computational efficiency and the risk of over-optimizing the inner GD process.
Rep 1 Rep 2 Rep 3 Rep 4 Rep 5
010000 20000 30000010000 20000 30000010000 20000 30000010000 20000 30000010000 20000 30000−1000−950−900−850
IterationELBO (EMA)
Inner GD steps 420 converge
(a) ELBO trajectories. Higher values are preferable.
Rep 1 Rep 2 Rep 3 Rep 4 Rep 5
010000 20000 30000010000 20000 30000010000 20000 30000010000 20000 30000010000 20000 300000.250.500.751.001.25
IterationVariance ratio
Inner GD steps 420 converge
(b) Variance ratios. A reading of 1 indicates no variance reduction. Lower values are preferable.
Figure 14: ELBO trajectories and variance ratios for hierarchical Poisson models using mean-field Gaussian,
(first-order) ZVCV-GD with L= 10, running with different number of steps in the inner gradient descent.
The experiment was repeated five times, each time with different initialisations. The ELBO trajectories for
different GD steps are practically indistinguishable. The erratic variance ratio readings occur during the
early optimisation stages in the low ELBO region, where gradient magnitudes are substantial.
29