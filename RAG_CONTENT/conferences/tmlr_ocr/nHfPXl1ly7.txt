Published in Transactions on Machine Learning Research (06/2023)
A Kernel Perspective on Behavioural Metrics for Markov
Decision Processes
Pablo Samuel Castro∗psc@google.com
Google DeepMind
Tyler Kastner∗,†tyler.kastner@mail.mcgill.ca
University of Toronto
Prakash Panangaden∗prakash@cs.mcgill.ca
McGill University
Mark Rowland∗markrowland@google.com
Google DeepMind
Reviewed on OpenReview: https: // openreview. net/ forum? id= nHfPXl1ly7
Abstract
Behavioural metrics have been shown to be an eﬀective mechanism for construct-
ing representations in reinforcement learning. We present a novel perspective on
behavioural metrics for Markov decision processes via the use of positive deﬁnite
kernels. We leverage this new perspective to deﬁne a new metric that is provably
equivalent to the recently introduced MICo distance (Castro et al., 2021). The ker-
nel perspective further enables us to provide new theoretical results, which has so far
eluded prior work. These include bounding value function diﬀerences by means of
our metric, and the demonstration that our metric can be provably embedded into a
ﬁnite-dimensional Euclidean space with low distortion error. These are two crucial
properties when using behavioural metrics for reinforcement learning representa-
tions. We complement our theory with strong empirical results that demonstrate
the eﬀectiveness of these methods in practice.
1 Introduction
As tabular methods are insuﬃcient for most state spaces, function approximation in reinforcement
learning is a well-established paradigm for estimating functions of interest, such as the expected
sum of discounted returns V(Sutton & Barto, 2018). A general value function approximator can
be seen as a composition of a m-dimensional embeddingφ:X→Rm, which maps a state space X
to am-dimensional space Rm, and afunction approximator ψ:Rm→R(e.g.V≈ψ◦φ). Whileφ
can be ﬁxed, as is the case in linear function approximation (Baird, 1995; Konidaris et al., 2011) an
increasingly popular approach is to learn both ψandφconcurrently, typically with the use of deep
neural networks. This raises the question of what constitutes a good embedding φ, which forms
the basis of the ﬁeld of representation learning.
*Authors listed in alphabetical order. See below for details of contributions.
†Work done while a student at McGill University.
1Published in Transactions on Machine Learning Research (06/2023)
Previous approaches to learning φinclude spectral decompositions of transition and reward op-
erators (Dayan, 1993; Mahadevan & Maggioni, 2007) and implicit learning through the use of
auxiliary tasks (Lange & Riedmiller, 2010; Finn et al., 2015; Jaderberg et al., 2017; Shelhamer
et al., 2017; Hafner et al., 2019; Lin et al., 2019; Bellemare et al., 2019; Yarats et al., 2021); such
work has shown that learning embeddings plays a crucial role in determining the success of deep
reinforcement learning algorithms. This motivates the development of further methods for learning
embeddings, and raises the central question as to what properties of an embedding are beneﬁcial
in reinforcement learning.
Acoreroleplayedbytheembeddingmap φistomediategeneralisationofvaluefunctionpredictions
across states; if the embeddings φ(x)andφ(y)of two states x,yare close in Rm, then by virtue of
the regularity (or more precisely, Lipschitz continuity) of the function approximator ψ, the states x
andywillbepredictedtohavesimilarvalues(Geladaetal.,2019;LeLanetal.,2021). Thus, agood
embedding should cluster together states which are known to have similar behavioural properties,
whilst keeping dissimilar states apart.
When learning value functions for reinforcement learning, how can one determine whether a rep-
resentation allows learning generalization between similar states, but avoids over-generalization
between dissimilar states? Metrics over the environment state space ( state metrics for short) aﬀord
us a natural way to do so via value function upper-bounds, by taking φto be a metric embedding
ofdinRm. To illustrate the eﬀect of metrics on generalisation, consider two metrics d1andd2
with the property that |V(x)−V(y)|≤d1(x,y)/lessmuchd2(x,y), whereV:X→Ris a value function.
The metric d2oﬀers less information about the similarity between xandythand1and may suﬀer
from under-generalization1. Thus,d1is a more useful metric for representation learning by virtue
of being a tighter upper-bound . State metrics satisfying such bounds have accordingly played an
important role in representation learning in deep RL (Castro, 2020; Zhang et al., 2021; Castro
et al., 2021). Contrastingly, a metric dthat violates said upper bound (e.g. for some states xand
y,d(x,y)<|V(x)−V(y)|) may encourage dissimlar states to be mapped near each other, and can
thus suﬀer from over-generalization.
Bisimulation metrics (Desharnais et al., 1999; van Breugel & Worrell, 2001; Ferns et al., 2004) are a
wellestablishedclassofstatemetricsthatprovidesuch value function upper bounds :|V(x)−V(y)|≤
d(x,y). Thistranslatesintoaguaranteeinthefunctionapproximationsettingundertheassumption
thatdcanbe embedded into Rm, and that, for example, ψis 1-Lipschitz continuous, yielding
|V(x)−V(y)|≈|ψ(φ(x))−ψ(φ(y))|≤/bardblφ(x)−φ(y)/bardbl≈d(x,y).
Bisimulation metrics, however, are expensive to compute, even for tabular systems, due to the fact
that one needs to ﬁnd an optimal coupling between the next-state distributions (Villani, 2008).
Castro et al. (2021) overcame this diﬃculty by replacing the optimal coupling with an independent
one, resulting in the MICo distance Uπ, which still satisﬁes the above-mentioned value function
upper bound. Interestingly, this distance is not a proper metric (nor pseudo-metric), but rather a
diﬀuse metric (a notion deﬁned by Castro et al. (2021)) that admits non-zero self-distances. The
consequences of this are that, when co-learned with ψ, the resulting feature map φ:X →Rmis
approximating a reducedversion of the MICo distance Uπ, denoted as ΠUπ. Unfortunately, this
reduced MICo does not satisfy the value function upper bound, nor was it demonstrated to be
1An extreme example of this is to take d2(x,y) =∞ 1[x/negationslash=y]. It is vacuously an upper-bound, but completely
uninformative, and will encourage state representations to be mapped far apart from each other.
2Published in Transactions on Machine Learning Research (06/2023)
embeddable in Rm(the space spanned by neural network representations). Despite demonstrating
strong empirical performance, it remained unclear whether the resulting method was theoretically
well-founded.
Thus, while there have been empirical successes in using the object ΠUπas a means of shaping
learnt representations in RL, there are two open questions that naturally arise from the preceding
discussion:
1. Is it reasonable to assume that ΠUπis embeddable into a ﬁnite-dimensional Euclidean
space?
2. Does ΠUπsatisfy some form of continuity, thus potentially explaining its utility in learning
representations for RL?
In this work we address these questions directly by taking an alternate view of state similarity: via
the use of kernels. More precisely, we introduce the concept of a positive-deﬁnite kernel on Markov
decision processes as a measure of behavioural similarity between states. This new perspective
is valuable in itself, as it enables us to leverage reproducing kernel Hilbert space (RKHS) theory
for a better understanding of state metrics. We extract a distance from this kernel, and prove
its equivalence to the reduced MICo distance. Through this perspective our work provides the
following contributions:
•We deﬁne a new state distance, dπ
ksthat is constructed from the Hilbert space distance of
the RKHS (Section 3.1).
•We demonstrate the equality of this new distance to the reduced MICo ΠUπof Castro et al.
(2021) (Section 3.2).
•We derive a novel result demonstrating that dπ
ks(and ΠUπby extension) do serve as an
upper bound to value function diﬀerences, with an additive component (Section 3.3).
•We prove that dπ
kscan be embedded into a ﬁnite-dimensional Euclidean space with low
distortion error (Section 3.4).
•We demonstrate empirically that dπ
ksperforms comparably to MICo when used in a deep
reinforcement learning setting (Section 4).
Before doing so, we provide a review of some background material in the next section.
2 Background
We provide a brief overview of some of the concepts used throughout this work, and provide a more
detailed background in the appendix.
2.1 Markov decision processes
We consider a Markov decision process (MDP) given by (X,A,P,R,γ), whereXis a ﬁnite state
space,Aa set of actions, P:X×A→ P(X)a transition kernel, and R:X×A→ P(R)a
reward kernel ( P(Z)is the set of probability distributions on a measurable set Z). We will write
Pa
x:=P(x,a)for the transition distribution from taking action ain statex,Ra
x:=R(x,a)for
3Published in Transactions on Machine Learning Research (06/2023)
the reward distribution from taking action ain statex, and write ra
xfor the expectation of this
distribution. A policy πis a mappingX→P(A). We use the notation Pπ
x=/summationtext
a∈Aπ(a|x)Pa
xto
indicate the state distribution obtained by following one step of a policy πwhile in state x. We use
Rπ
x=/summationtext
a∈Aπ(a|x)Ra
xto represent the reward distribution from xunderπ, andrπ
xto indicate the
expected value of this distribution. Finally, γ∈[0,1)is the discount factor used to compute the
discounted long-term return.
Thevalueof a policy πis the expected total return an agent attains from following π, and is
described by a function Vπ:X→R, such that for each x∈X,
Vπ(x) =Eπ
/summationdisplay
t≥0γtRt/vextendsingle/vextendsingle/vextendsingle/vextendsingleX0=x
.
Here,Rtis the random variable representing the reward received at time step twhen following
policyπstarting at state x. An optimal policy π∗is a policy which achieves the maximum value
function at each state, which we will denote V∗. It satisﬁes the Bellman optimality recurrence:
V∗(x) = max
a∈AE/bracketleftbigg
R0+γV∗(X1)/vextendsingle/vextendsingle/vextendsingle/vextendsingleX0=x,A 0=a/bracketrightbigg
. (1)
2.2 The MICo distance
A common approach to dealing with very large, or even inﬁnite, state spaces is via the use of an
embedding φ:X→Rm,whichmapstheoriginalstatespaceintoalower-dimensionalrepresentation
space. State values can then be learned on top of these representations: e.g. ˆV(x)≈ψ(φ(x)), where
ψis a function approximator.
A desirable property of representations is that they can bound diﬀerences with respect to the
function of interest (e.g. value functions). Le Lan et al. (2021) argued state behavioural metrics are
ausefulmechanismforthis, astheycanoftenbounddiﬀerencesinvalues: d(x,y)≥|V∗(x)−V∗(y)|.
Thestructureofthemetricimpactsitseﬀectiveness: picking d(x,y) = (Rmax−Rmin)(1−γ)−1allows
one to satisfy the same upper bound, but in a rather uninformative way.
Bisimulation metrics d∼(Ferns et al., 2004) are appealing metrics to use, as they satisfy the above
upper bound, while still capturing behavioural similarity that goes beyond simple value equivalence
(seeAppendixCforamorein-depthdiscussionofbisimulationmetrics). Indeed, variouspriorworks
have built reinforcement learning methods based on the notion of bisimulation metrics (Gelada
et al., 2019; Agarwal et al., 2021a; Hansen-Estruch et al., 2022). Castro (2020) demonstrated that,
with some simplifying assumptions, these metrics are learnable with neural networks; Zhang et al.
(2021), Castro et al. (2021), Kemertas & Aumentado-Armstrong (2021) and Kemertas & Jepson
(2022) demonstrated they are learnable without the assumptions.
Castro et al. (2021) introduced the MICo distance , along with a corresponding diﬀerentiable loss,
which can be added to any deep reinforcement learning agent without extra parameters. The added
loss showed statistically signiﬁcant performance improvements on the challenging Arcade Learning
Environment (Bellemare et al., 2013), as well as on the DeepMind control suite (Tassa et al., 2018).
Our paper builds on the MICo distance, and as such, we present a brief overview of the main deﬁ-
nitions and results in this section. We begin with a result that introduces the MICo operator ( Tπ
M),
deﬁnes the MICo distance as its unique ﬁxed point ( Uπ), and shows this distance provides an upper
bound on value diﬀerences between two states. Before doing so, we deﬁne the Łukaszyk–Karmowski
4Published in Transactions on Machine Learning Research (06/2023)
distance (LK-distance), which is used in the deﬁnition of the MICo distance. A more thorough
discussion of the LK-distance is provided in Appendix C.1.2.
Deﬁnition 1. Given two probability measures µandνon a setXand a metric donX, the
Łukaszyk–Karmowski distance dŁK(Łukaszyk, 2004) is deﬁned as
dŁK(d)(µ,ν) =/integraldisplay
d(x,y) d(µ×ν)(x,y),
or equivalently
dŁK(d)(µ,ν) = E
X∼µ,Y∼ν[d(X,Y )].
Theorem 2 (Castro et al. (2021)) .Given a policy π, the MICo operator Tπ
M:RX×X→RX×X,
given byTπ
M(U)(x,y) =|rπ
x−rπ
y|+γdŁK(U)(Pπ
x,Pπ
y), has a unique ﬁxed point Uπ, referred to
as the MICo distance.2Further, the MICo distance upper bounds the absolute diﬀerence between
policy-value functions. That is, for x,y∈X, we have|Vπ(x)−Vπ(y)|≤Uπ(x,y).
The MICo distance was based on the π-bisimulation metric ( dπ
∼) introduced by Castro (2020); the
key diﬀerence is that the update operator for dπ
∼uses the Kantorovich distance between probability
measures instead of dŁK(see Appendix C.1.1 for more details on the Kantorovich metric).
Castro et al. (2021) adapted the MICo distance to be used for learning state feature maps φ.
However, the authors demonstrated the features used for control were actually a “reduction” of the
diﬀuse metric approximant; this distance was dubbed the reduced MICo distance ΠUπ.
Deﬁnition 3 (Reduced MICo) .The reduced MICo distance ΠUπis deﬁned by
ΠUπ(x,y) =Uπ(x,y)−1
2(Uπ(x,x) +Uπ(y,y)).
Two immediate properties of ΠUπare that it is symmetric, and satisﬁes ΠUπ(x,x) = 0. However,
it was unknown whether ΠUπsatisﬁed the triangle inequality, as well as whether it was positive
in general. One negative result shown by Castro et al. (2021) was that the value function upper
bound does not hold for ΠUπ.
Proposition 4 (Castro et al. (2021)) .There exists an MDP with x,y∈X, and policy πwhere
|Vπ(x)−Vπ(y)|>ΠUπ(x,y).
While Castro et al. (2021) demonstrated the strong empirical performance yielded by the reduced
MICo distance in combination with deep reinforcement learning, Proposition 4 highlights that
important properties for general state similarity metrics remain unknown for the reduced MICo.
We pause here to take stock of what is unknown regarding ΠUπ:
•From a purely metric perspective, it is unknown whether ΠUπis always positive (in general,
applying the reduction operator Πto a positive function fdoes not result in Πfbeing pos-
itive), or whether ΠUπsatisﬁes the triangle inequality. These are important to understand
so as to guarantee that the learned representations are well-behaved.
•From the behavioural similarity perspective, it is not known whether ΠUπhas any quan-
titative relationship to the value function Vπ, as Proposition 4 demonstrates the standard
value function upper bound does not hold.
2The distance dŁKis deﬁned in Appendix C.1.2.
5Published in Transactions on Machine Learning Research (06/2023)
•From a practical perspective, it is unknown ΠUπis even embeddable in Rm.
In this work we seek to resolve these mysteries through the lens of reproducing kernel Hilbert
spaces, which we introduce next (a more extensive discussion is provided in Appendix C.5).
2.3 Reproducing kernel Hilbert spaces
LetXbe a ﬁnite3set, and deﬁne a function k:X×X→ Rto be a positive deﬁnite kernel if it is
symmetric and positive deﬁnite:4for any{x1,...,xn}∈X,{c1,...,cn}∈R, we have that
n/summationdisplay
i=1n/summationdisplay
j=1cicjk(xi,xj)≥0.
We will often use kernelas a shorthand for positive deﬁnite kernel. Given a kernel konX, one
can construct a space of functions Hkreferred to as a reproducing kernel Hilbert space (RKHS),
through the following steps:5
(i) Construct a vector space of real-valued functions on Xof the form{k(x,·) :x∈X}.
(ii) Equip this space with an inner product given by /angbracketleftk(x,·),k(y,·)/angbracketrightHk=k(x,y).
(iii) Take the completion of the vector space with respect to the inner product /angbracketleft·,·/angbracketrightHk.
The Hilbert space obtained at the end of step (iii) is the reproducing kernel Hilbert space for k.
It is common to introduce the notation ϕ(x):=k(x,·), whereϕ:X→His often called the feature
map, andϕ(x)is understood as the embedding ofxinH. Note that we are using ϕto represent
the mapping of states onto a Hilbert space (e.g. ϕ:X→Hk), which is distinct from the symbol φ
which we use to represent the mapping of states onto a Euclidean space (e.g. φ:X→Rm). One
can also embed probability distributions on XinHk. Given a probability distribution µonX, one
can deﬁne the embedding of µ,Φ(µ)∈Hkas
Φ(µ) =E
X∼µ[ϕ(X)] =/integraldisplay
Xϕ(x)dµ(x),
where the integral taken is a Bochner integral,6as we are integrating over Hk-valued functions.
The embeddings of measures into Hkallow one to easily compute integrals, as one can show using
the Riesz representation theorem that for f∈Hk, one has
/integraldisplay
Xfdµ=/angbracketleftf,Φ(µ)/angbracketrightHk.
These embeddings also allow us to deﬁne metrics on XandP(X)by looking at the Hilbert space
distance of their embeddings.
3The general theory of reproducing kernel Hilbert spaces holds for much more general classes of sets (Aronszajn,
1950), but as mentioned earlier in the paper, our work focuses on MDPs with ﬁnite state spaces, so we present RKHS
theory in the context of ﬁnite sets, allowing us to avoid some mathematical technicalities.
4We remark that the deﬁnition of positive deﬁnite is not consistent across the literature. We follow the convention
of the kernel methods community, and deﬁne a function to be strictly positive deﬁnite if the inequality is strict unless
c1=···=cn= 0. In the linear algebra and optimization communities however, this is referred to as positive deﬁnite,
and the deﬁnition provided is referred to as positive semideﬁnite.
5An RKHS can be alternately deﬁned by choosing a suitable set of functions and constructing the kernel kfrom
this set, we review this approach in Appendix C.5.2.
6A generalization of the Lebesgue integral to functions taking values in a Banach space, further details can be
found in Arendt et al. (2001).
6Published in Transactions on Machine Learning Research (06/2023)
Deﬁnition 5. Given a positive deﬁnite kernel k, deﬁneρkas its induced distance:
ρk(x,y) :=/bardblϕ(x)−ϕ(y)/bardblHk.
By expanding the inner product, the squared distance can be written solely in terms of the kernel k:
ρ2
k(x,y) =k(x,x) +k(y,y)−2k(x,y).
We can perform the same process to construct a metric on P(X)using Φ:
Deﬁnition 6 (Gretton et al. (2012)) .Letkbe a kernel onX, and Φ :P(X)→Hkbe as deﬁned
above. Then the Maximum Mean Discrepancy (MMD) is a pseudometric on P(X)deﬁned by
MMD (k)(µ,ν) =/bardblΦ(µ)−Φ(ν)/bardblHk.
Asemimetric is a distance function which respects all metric axioms save for the triangle inequality.
A semimetric space (X,ρ)is ofnegative type if for allx1,...,xn∈X,c1,...,cn∈Rsuch that/summationtextn
i=1ci= 0, we have
n/summationdisplay
i=1n/summationdisplay
j=1cicjρ(xi,xj)≤0.
Given a semimetric of negative type ρonX, we can deﬁne a distance on P(X)known as the energy
distance, deﬁned as
E(ρ)(µ,ν) = E
X∼µ,Y∼ν[ρ(X,Y )]−1
2/parenleftbigg
E
X1,X2∼µ[ρ(X1,X2)] + E
Y1,Y2∼ν[ρ(Y1,Y2)]/parenrightbigg
,
where the pairs of random variables in each expectation are independent.
Ifρisofnegativetype, thisguaranteesthatwehave E(ρ)(µ,ν)≥0forallµ,ν∈P(X). Semimetrics
of negative type have a connection to positive deﬁnite kernels, as shown in Sejdinovic et al. (2013):
the induced distance squared ρ2
kis a semimetric of negative type, which we say is induced by
k. Conversely, a semimetric of negative type ρinduces a family of positive deﬁnite kernels Kρ
parametrised by a chosen base point x0∈X:
Kx0ρ(x,x/prime) =1
2(ρ(x,x0) +ρ(x/prime,x0)−ρ(x,x/prime)).
The relationship is symmetric, so that each kernel k∈Kρhasρas its induced semimetric. With
this symmetry in mind, we call a kernel kand a semimetric of negative type an equivalent pair if
they induce one another through the above construction. This equivalence does not only live in X
however, as the following proposition shows that it lifts into P(X)as well.
Proposition 7 (Sejdinovic et al. (2013)) .Let(k,ρ)be an equivalent pair, and let µ,ν∈P(X).
Then we have the equivalence
MMD2(k)(µ,ν) =E(ρ)(µ,ν).
The central takeaway of the equivalences proved in Sejdinovic et al. (2013) is that using metrics of
negative type and positive deﬁnite kernels are two perspectives of the same underlying structure.
7Published in Transactions on Machine Learning Research (06/2023)
2.4 Prior work using RKHS for MDPs
We are not the ﬁrst to consider using the theory of reproducing kernel Hilbert spaces in the context
of Markov decision processes. Indeed, Ormoneit & Sen (2002) proposed to use a ﬁxed kernel to
perform approximate dynamic programming in RL problems with Euclidean state spaces; a variety
of theoretical and empirical extensions of this approach have since been considered (Lever et al.,
2016; Barreto et al., 2016; Domingues et al., 2021). The use of ﬁxed RKHSs for approximate
dynamic programming in more general state spaces has been studied by Farahmand et al. (2016),
Yang et al. (2020), and Koppel et al. (2021). In contrast, as we will discuss below, our work learns
a kernel on the state space, which classiﬁes states as similar based on their behavioural similarity
in the MDP.
3 Kernel similarity metrics
We take a new perspective on behavioural metrics in MDPs, through the use of positive deﬁnite
kernels. We deﬁne a contractive operator on the space of kernels, and show that its unique ﬁxed
point induces a behavioural distance in a reproducing kernel Hilbert space, and then prove that
this distance coincides with the reduced MICo distance ΠUπ. We then present new properties of
ΠUπobtained through this perspective.
3.1 Deﬁnition
Given an MDP M= (X,A,P,R,γ), state similarity metrics which are variants of bisimulation
generally follow the form
d(x,y) =d1(x,y) +γd2(d)(P(x),P(y)),
for statesx,yinX. Here,d1is a distance on Xrepresenting one-step diﬀerences betweenx
andy(e.g. reward diﬀerence), and d2“lifts” a distance on Xonto a distance on P(X); thus,
d2(d)(P(x),P(y))represents the long-term behavioural distance betweenxandy. It is worth
noting the similarity to the Bellman optimality recurrence introduced in Equation 1.
In this section we take a similar approach, except rather than quantifying the diﬀerence of states
(metrics), we consider quantifying the similarity of states (positive deﬁnite kernels). Following this
idea, we can deﬁne a state similarity kernel k:X×X→ Ras a positive deﬁnite kernel which takes
the following form:
k(x,y) =k1(x,y) +γk2(k)(P(x),P(y)).
Similarly to the above expression for d,k1is a kernel onXwhich measures the immediate similarity
of two states xandy, andk2lifts a kernel onXinto a kernel on P(X).
We can now present a candidate state similarity kernel. We follow Castro (2020) and Castro et al.
(2021) in measuring behavioural similarity under a ﬁxed policy, in contrast to measuring behaviour
across all possible actions, as done in bisimulation (Ferns et al., 2004; 2011). Following this, we ﬁx
a policyπwhich will be the policy under which we measure similarity. For the immediate similarity
kernel, we will assume that supp (R)⊆[−1,1],7and we set k1(x,y) = 1−1
2|rπ
x−rπ
y|, which lies in
[0,1]. This is a reasonable measure of immediate similarity, as it is maximised when two states have
identical immediate rewards, and minimised when two states have maximally distant immediate
7This assumption is purely for the clarity of presentation, and can be relaxed to assuming boundedness of reward
and setting k1(x,y) = 1−1
Rmax−Rmin|rπ
x−rπ
y|.
8Published in Transactions on Machine Learning Research (06/2023)
rewards. To lift a kernel kinto a kernel on P(X), we can use the kernel lifting construction given
in Guilbart (1979), and deﬁne k2(k)(µ,ν) =EX∼µ,Y∼ν[k(X,Y )]. Combining these, we can deﬁne
an operator on the space of kernels whose ﬁxed point would be our kernel of interest.
Deﬁnition 8. LetK(X)be the space of positive deﬁnite kernels on X. Givenπ∈P(A)X, the
kernel similarity operator Tπ
k:K(X)→K(X)is
Tπ
k(k)(x,y) =/parenleftbigg
1−1
2|rπ
x−rπ
y|/parenrightbigg
+γ E
X/prime∼Pπx,Y/prime∼Pπy[k(X/prime,Y/prime)].
The fact that Tπ
kindeed maps K(X)toK(X)follows from the previous paragraph describing
that each operator is a kernel, and that the sum of two kernels is a kernel (Aronszajn, 1950). We
now present two lemmas which are necessary to conclude whether a unique ﬁxed point of Tπ
kexists.
Lemma 9. Tπ
kis a contraction with modulus γin/bardbl·/bardbl∞.
Proof.Letk1,k2∈K(X), we can then write out
/bardblTπ
k(k1)−Tπ
k(k2)/bardbl∞= max
(x,y)∈X×X|Tπ
k(k1)(x,y)−Tπ
k(k2)(x,y)|
=γmax
(x,y)∈X×X/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleE
X/prime∼Pπx,Y/prime∼Pπy[k1(X/prime,Y/prime)]− E
X/prime∼Pπx,Y/prime∼Pπy[k2(X/prime,Y/prime)]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=γmax
(x,y)∈X×X/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleE
X/prime∼Pπx,Y/prime∼Pπy[k1(X/prime,Y/prime)−k2(X/prime,Y/prime)]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤γ/bardblk1−k2/bardbl∞.
Lemma 10. The metric space (K(X),/bardbl·/bardbl∞)is complete.
Proof.As we assumeXis ﬁnite, the space of functions RX×Xis a ﬁnite-dimensional Euclidean
vector space, and hence is complete with respect to the L∞norm. It therefore suﬃces to show
thatK(X)is closed in RX×X. We can consider a sequence {kn}n≥0inK(X)which converges
tok∈RX×Xin/bardbl·/bardbl∞and show that k∈K(X). This is equivalent to showing that kis both
symmetric and positive deﬁnite, which follows immediately from the fact that each knis and the
convergence is uniform. Hence K(X)is closed with respect to /bardbl·/bardbl∞, and thus is complete.
With these two lemmas, we can now show that the required ﬁxed point indeed exists.
Proposition 11. There is a unique kernel kπsatisfying
kπ(x,y) =/parenleftbigg
1−1
2|rπ
x−rπ
y|/parenrightbigg
+γ E
X/prime∼Pπx,Y/prime∼Pπy[kπ(X/prime,Y/prime)].
Proof.Combining Lemma 9 and Lemma 10, we know that the operator Tπ
kis a contraction in a
complete metric space. We can now use Banach’s ﬁxed point theorem to obtain the existence of a
unique ﬁxed point, which is kπ.
Having a kernel on our MDP now gives us an RKHS of functions on the MDP, which we refer to
asHkπ. Moreover, we have an embedding of each state into Hkπgiven byϕπ(x) =kπ(x,·). Using
this construction, we can deﬁne a distance between states in Xby considering their Hilbert space
distance inHkπ.
Deﬁnition 12. We deﬁne the kernel similarity metric (KSMe) as the distance function
dπ
ks(x,y) :=/bardblϕπ(x)−ϕπ(y)/bardbl2
Hkπ.
9Published in Transactions on Machine Learning Research (06/2023)
3.2 Equivalence with reduced MICo distance
We will prove a number of useful theoretical properties of dπ
ksin the rest of this section. But ﬁrst,
we demonstrate that dπ
ksis equal to the reduced MICo ( ΠUπ) from Castro et al. (2021). Given that
Castro et al. (2021) left a number of unresolved properties of ΠUπ, this equality will be important
for the remainder of the paper as it means the new theoretical insights we prove for dπ
ksalso hold
forΠUπ.
Referring to Section 2.3, we have that dπ
ksis the semimetric of negative type induced by kπ. We
now demonstrate that dπ
kscan be written as a sum of reward distance and transition distribution
distance, similar to the form of the behavioural metrics discussed in Section 3.1.
Proposition 13. The kernel similarity metric dπ
kssatisﬁes
dπ
ks(x,y) =|rπ
x−rπ
y|+γMMD2(kπ)(Pπ
x,Pπ
y).
Proof.To see this, we can write out the squared Hilbert space distance
dπ
ks(x,y) =/bardblϕπ(x)−ϕπ(y)/bardbl2
Hkπ
=kπ(x,x) +kπ(y,y)−2kπ(x,y)
=|rπ
x−rπ
y|+γ/angbracketleftΦ(Pπ
x),Φ(Pπ
x)/angbracketrightHkπ+γ/angbracketleftΦ(Pπ
y),Φ(Pπ
y)/angbracketrightHkπ−2γ/angbracketleftΦ(Pπ
x),Φ(Pπ
y)/angbracketrightHkπ
=|rπ
x−rπ
y|+γMMD2(kπ)(Pπ
x,Pπ
y),
where in the ﬁrst equality we expanded the norm as in Deﬁnition 5, and in the second line we used
kπ(x,x) =γE
X/prime
1,X/prime
2∼Pπx[k(X/prime
1,X/prime
2)]
=γ/angbracketleftΦ(Pπ
x),Φ(Pπ
x)/angbracketrightHkπ.
Before presenting the main result of this section (Theorem 15), we state a necessary technical
lemma. The proof is provided in Appendix A.
Lemma 14. For any measures µ,ν, andn≥0, we have that
E
X1,X2∼µ
Y1,Y2∼ν[kn(X1,X2) +kn(Y1,Y2)−2kn(X1,Y1)] = E
X1,X2∼µ
Y1,Y2∼ν/bracketleftbigg
Un(X1,Y1)−1
2(Un(X1,X2) +Un(Y1,Y2))/bracketrightbigg
.
The next result proves the equivalence of dπ
ksandΠUπ; the implications of this result are that
all of the properties proved for dπ
ksalso apply to ΠUπ. This is signiﬁcant, as it implies ΠUπcan
be incorporated into a non-vacuous upper bound of value function diﬀerences (Theorem 16), and
is guaranteed to be embeddable in a ﬁnite-dimensional Euclidean space of appropriate dimen-
sion (Theorem 19), addressing the two core theoretical questions raised in the introduction of this
paper.
Theorem 15. For anyx,y∈X, we have that dπ
ks(x,y) = ΠUπ(x,y).
Proof.To begin, we will make use of the sequences (kn)n≥0,(Un)n≥0deﬁned by kn≡0,kn+1=
Tπ
k(kn),Un≡0,Un+1=Tπ
M(Un). Since both Tπ
kandTπ
Mare contractions, we know that kn→kπ
andUn→Uπuniformly. To prove the statement, we will show that for all n≥0andx,y∈X, we
have that
kn(x,x) +kn(y,y)−2kn(x,y) =Un(x,y)−1
2(Un(x,x) +Un(y,y)).
10Published in Transactions on Machine Learning Research (06/2023)
We can write out
kn(x,x) +kn(y,y)−2kn(x,y)
=|rπ
x−rπ
y|+γE
X1,X2∼Pπ
x
Y1,Y2∼Pπ
y[kn(X1,X2) +kn(Y1,Y2)−2kn(X1,Y1)]
=|rπ
x−rπ
y|+γE
X1,X2∼Pπ
x
Y1,Y2∼Pπ
y/bracketleftbigg
Un(X1,Y1)−1
2(Un(X1,X2) +Un(Y1,Y2))/bracketrightbigg
(⋆)
=Un(x,y)−1
2(Un(x,x) +Un(y,y)),
where (⋆)follows from Lemma 14. Since (kn)n≥0and(Un)n≥0both converge uniformly, we can
take limits and conclude that
dπ
ks(x,y) =kπ(x,x) +kπ(y,y)−2kπ(x,y) =Uπ(x,y)−1
2(Uπ(x,x) +Uπ(y,y)) = ΠUπ(x,y).
3.3 An additive value function upper bound
Proposition 4 asserts that ΠUπ, and hence dπ
ks, does not upper bound the absolute diﬀerence in
value functions. However, the kernel perspective allows us to show that it satisﬁes an upper bound
with an additive constant. For x∈Xwe introduce the notation
∆π
n(x) = E
X/prime∼(Pπx)n/bracketleftBigg
E
X/prime/prime
1,X/prime/prime
2∼Pπ
X/prime/bracketleftBig
|rπ
X/prime/prime
1−rπ
X/prime/prime
2|/bracketrightBig/bracketrightBigg
.
Intuitively, ∆π
n(x)is the expected absolute reward diﬀerence in two trajectories from x, where the
trajectories are coupled for the ﬁrst nsteps, and proceed independently for the ﬁnal (n+ 1)th step.
With this quantity, we present the following theorem.
Theorem 16. For anyx,y∈X, we have
/vextendsingle/vextendsingleVπ(x)−Vπ(y)/vextendsingle/vextendsingle≤ΠUπ(x,y) +1
2/summationdisplay
n≥0γn(∆π
n(x) + ∆π
n(y)).
Proof.To begin, we will make use of the sequences (km)m≥0,(Vm)m≥0deﬁned bykm≡0,km+1=
Tπ
k(km),Vm≡0,Vm+1=Tπ(Vm). SinceTπ
kandTπare both contractions, we know that km→kπ
andVm→Vπuniformly. We will refer to the semimetric equivalent to the mth kernel iterate as
dm:dm(x,y) =km(x,x) +km(y,y)−2km(x,y). We will now use induction to prove that for all m,
we have that
|Vm(x)−Vm(y)|≤dm(x,y) +1
2m/summationdisplay
n=0γn(∆π
n(x) + ∆π
n(y)).
The base case m= 0is immediate, as the left hand side is identically 0. We can now assume the
induction hypothesis, and write out
/vextendsingle/vextendsingleVm+1(x)−Vm+1(y)/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglerπ
x+γE
X/prime∼Pπx[Vm(x)]−/parenleftBigg
rπ
y+γE
Y/prime∼Pπy/bracketleftbigVm(Y/prime)/bracketrightbig/parenrightBigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
11Published in Transactions on Machine Learning Research (06/2023)
≤|rπ
x−rπ
y|+γ E
X/prime∼Pπx,Y/prime∼Pπy/bracketleftbigg/vextendsingle/vextendsingleVm(X/prime)−Vm(Y/prime)/vextendsingle/vextendsingle/bracketrightbigg
≤|rπ
x−rπ
y|+γ E
X/prime∼Pπx,Y/prime∼Pπy/bracketleftBigg
dm(X/prime,Y/prime) +1
2m/summationdisplay
n=0γn(∆π
n(X/prime) + ∆π
n(Y/prime))/bracketrightBigg
=|rπ
x−rπ
y|+γ E
X/prime∼Pπx,Y/prime∼Pπy/bracketleftbigdm(X/prime,Y/prime)/bracketrightbig+1
2m+1/summationdisplay
n=1γn(∆π
n(x) + ∆π
n(y))
=|rπ
x−rπ
y|+γ E
X/prime∼Pπx,Y/prime∼Pπy/bracketleftbigdm(X/prime,Y/prime)/bracketrightbig+1
2E
X/prime,X/prime/prime∼Pπ
x
Y/prime,Y/prime/prime∼Pπ
y/bracketleftbigg
|rπ
X/prime−rπ
X/prime/prime|+|rπ
Y/prime−rπ
Y/prime/prime|/bracketrightbigg
+1
2m+1/summationdisplay
n=1γn(∆π
n(x) + ∆π
n(y))
=|rπ
x−rπ
y|+γ E
X/prime∼Pπx,Y/prime∼Pπy/bracketleftbigdm(X/prime,Y/prime)/bracketrightbig+1
2m+1/summationdisplay
n=0γn(∆π
n(x) + ∆π
n(y))
=dm+1(x,y) +1
2m+1/summationdisplay
n=0γn(∆π
n(x) + ∆π
n(y)),
where we used EX/prime∼Pπx[∆π
n(X/prime)] = ∆π
n+1(x). We note that the sum1
2/summationtext∞
n=0γn(∆π
n(x) + ∆π
n(y))is
almost surely ﬁnite, as ∆π
n(x)≤1almost surely (since we assume supp (R)⊆[−1,1]), so that
∞/summationdisplay
n=0γn
2(∆π
n(x) + ∆π
n(y))≤∞/summationdisplay
n=0γn=1
1−γ.
With this theorem, it is apparent that the amount by which the bound is broken is controlled by
the amount of dispersion in reward coming from the transition probability function Pπ. A standard
tool to measure the dispersion of a measure on Ris the variance, which is not directly applicable in
this setting asPπmaps to measures on X. However, we can map each state x∈Xto a real value
rπ
x, and we use this to apply the variance. With this motivation, we deﬁne the reward variance of
Pπforx∈Xas
VarR(Pπ
x) = E
X/prime∼Pπx/bracketleftBig
(rπ
X/prime)2/bracketrightBig
−/parenleftbigg
E
X/prime∼Pπx[rπ
X/prime]/parenrightbigg2
.
With this deﬁnition in mind, we can now demonstrate that the maximal reward variance of an
MDP controls the amount the value function diﬀerence upper bound is violated.
Proposition 17. Suppose there exists σ2∈Rsuch that for each x∈X,VarR(Pπ
x)≤σ2. Then
for everyx∈Xandk≥0we have that ∆π
k(x)≤√
2σ, and in particular
/vextendsingle/vextendsingleVπ(x)−Vπ(y)/vextendsingle/vextendsingle≤ΠUπ(x,y) +√
2σ
1−γ.
Proof.We ﬁrst note that it suﬃces to show that for any xwe have that ∆π
1(x)≤√
2σ, since for
anyn>1we have ∆π
n(x) =EX/prime∼(Pπx)n−1[∆π
1(X/prime)]:
E
X/prime∼(Pπx)n−1[∆π
1(X1)] = E
Xn−1∼(Pπx)n−1/bracketleftBigg
E
Xn∼Pπ
Xn−1/bracketleftBigg
E
X(n+1)a,X(n+1)b∼Pπ
Xn/bracketleftBig
|rπ
X(n+1)a−rπ
X(n+1)b|/bracketrightBig/bracketrightBigg/bracketrightBigg
12Published in Transactions on Machine Learning Research (06/2023)
= E
Xn∼(Pπx)n/bracketleftBigg
E
X(n+1)a,X(n+1)b∼Pπ
Xn/bracketleftBig
|rπ
X(n+1)a−rπ
X(n+1)b|/bracketrightBig/bracketrightBigg
= ∆π
n(x),
where for clarity we used the notation Xmto denote a random state after taking msteps in the
trajectory.
We can ﬁrst recall an equivalent formula for variance as Var(µ) =1
2EX,Y∼µ[|X−Y|2]. Using this
and Jensen’s inequality, we have that for any x∈X,
2σ2≥ E
X/prime∼Pπx,Y/prime∼Pπy[|rπ
X/prime−rπ
Y/prime|2]
≥/parenleftBigg
E
X/prime∼Pπx,Y/prime∼Pπy[|rπ
X/prime−rπ
Y/prime|]/parenrightBigg2
.
Taking the square root of both sides we obtain that ∆π
1(x)≤√
2σ, which combined with the above
completes the proof.
3.4 Distortion error bounds on Euclidean embeddings
While we consider various distance spaces (X,d)on theground statesXof the MDP, in practice
we typically work with a representation (φ(x) :x∈X)∈(Rm)Xof the state space, from which
values can subsequently be predicted with neural network function approximation. This highlights
an important issue in moving from the study of behavioural metrics as abstract mathematical
objects to tools for shaping neural representations, which has received relatively little attention
so far. Namely, does there exist an embedding φ:X → Rmsuch that/bardblφ(x)−φ(y)/bardbl=d(x,y)
for allx,y∈X? Stated more concisely, we may ask whether the metric space (X,d)embeds into
the Euclidean space Rm; this is an instance of the core problem of study in the ﬁeld of metric
embedding theory (Deza & Laurent, 1997; Matousek, 2013), and there are several central results
from this ﬁeld that can be employed to cast light on the embeddability of behavioural metrics.
As an initial note of caution, Schoenberg (1935) gives a precise characterisation of which ﬁnite
metric spaces can be embedded into Euclidean space, a consequence of which is that many ﬁnite
metric spaces cannot be embedded into Euclidean spaces of anydimension. A potential upshot
is that attempting to learn exact embeddings of behavioural metrics may not be possible, even
in small-scale settings. However, the kernel perspective taken earlier in the paper allows us to
make immediate progress on the question of embeddability in the speciﬁc case of the reduced MICo
metric. In particular, since Theorem 15 establishes that the reduced MICo metric can be embedded
into the Hilbert space Hkπ, we can deduce the following result.
Corollary 18. The reduced MICo metric ΠUπcan be embedded into the space R|X|with squared
Euclidean metric.
Proof.From Theorem 15 and Deﬁnition 12, we have that ΠUπ(x,y) =/bardblϕπ(x)−ϕπ(y)/bardbl2
Hkπfor all
x,y∈X. SinceHkπis a Hilbert space of dimension at most X, there is an isometry ψ:Hkπ→Rk
for somek≤|X|. The composition ψ◦ϕπtherefore embeds ΠUπexactly in Rkunder the squared
Euclidean metric.
In many practical settings, the dimensionality of the space Rminto which the representation func-
tionϕmaps is generally taken to be much smaller than |X|itself for a variety of reasons, in-
cluding computational tractability and generalisation properties of the function approximator. It
13Published in Transactions on Machine Learning Research (06/2023)
is therefore pertinent to ask whether the guarantee established in Corollary 18 can be improved
to guarantee embeddabilty in a lower-dimensional Euclidean space. While exact embeddability in
lower-dimensional spaces is not always possible, the Johnson–Lindenstrauss lemma (Johnson & Lin-
denstrauss, 1984) can be used to establish the following result, which shows that lower-dimensional
embeddings of ΠUπare possible, as long as we are prepared to accept a certain level of distortion
of the original metric.
Theorem 19. Letπbe a policy, and∼πbe the equivalence relation on Xdeﬁned byx∼πy⇐⇒
dπ
ks(x,y) = 0. For any given ε∈(0,1), ifm≥8 log(|X/∼π|)/ε2, whereX/∼πdenotes the quotient
set with respect to the equivalence relation ∼π, then there exists an embedding φ:X →Rmsuch
that for all x,y∈X,
(1−ε)dπ
ks(x,y)≤/bardblφ(x)−φ(y)/bardbl2
2≤(1 +ε)dπ
ks(x,y),
or equivalently,
(1−ε) ΠUπ(x,y)≤/bardblφ(x)−φ(y)/bardbl2
2≤(1 +ε) ΠUπ(x,y).
Proof.We recall that the RKHS Hkπis deﬁned by the formula
Hkπ=span/braceleftbigk(x,·) :x∈X/bracerightbig,
where we do not need to take the completion, as there are only ﬁnitely many x∈X, henceHkπ
is ﬁnite dimensional inner product space, and therefore complete. Hkπis in general a semi inner
productspace, andtoresolvethiswetakethequotient Hkπ/∼π. Thesetofequivalenceclassesof ∼π
is equivalent to the kernel of the seminorm /bardbl·/bardblHkπ, and in particular is a ﬁnite-dimensional vector
space, soHkπ/∼πis a proper inner product space. Let l= dim(Hkπ/∼π), and letf1,f2,...,fl
be an orthonormal basis for Hkπ/∼π. We will begin by showing that (Hkπ/∼π,/bardbl·/bardblHkπ)can be
isometrically embedded into (Rl,/bardbl·/bardbl 2). To see this, letI:Hkπ/∼π→Rlbe deﬁned by
I(fj) =ej∀j∈[l],I
/summationdisplay
j∈[l]ajfj
=/summationdisplay
j∈[l]ajI(fj)
We can now see that Iis a Hilbert space isomorphism: for all i,j∈[l],
/angbracketleftfi,fj/angbracketrightHkπ=/angbracketleftei,ej/angbracketrightRl=/angbracketleftI(fi),I(fj)/angbracketrightRl.
Lettingn=|X/∼π|, we can enumerate X/∼πasx1,...,xn, and from the above isometry we know
that{x1,...,xn}⊂Xcan be embedded as {I(ϕπ(x1)),...,I(ϕπ(xn))}⊂Rlwith no distortion.
Let us use the notation yk=I(ϕπ(xk))as the embedding of xkintoRlfor brevity.
We can now apply the Johnson-Lindenstrauss lemma: for any ε∈(0,1),m≥8
ε2log(n), there exists
a linear map f:Rl→Rmsuch that for any i,j∈[n],
(1−ε)/bardblyi−yj/bardbl2
Rl≤/bardblf(yi)−f(yj)/bardbl2
Rm≤(1 +ε)/bardblyi−yj/bardbl2
Rl.
The desired statement now follows from rearranging the formula for nand using the isometry
/bardblyi−yj/bardbl2
Rl=/bardblI(ϕπ(xi))−I(ϕπ(xj))/bardbl2
Rl=/bardblϕπ(xi)−ϕπ(xj)/bardbl2
Hkπ=dπ
ks(xi,xj) = ΠUπ(xi,xj),
and taking the map φ:X/∼π→Rmto be the composition f◦I◦ϕπ.
14Published in Transactions on Machine Learning Research (06/2023)
Remark 20. The spaces and maps used in the previous proof can be summarized in the following
commutative diagram:
X/∼π Rm
Hkπ Rlφ
ϕπ
If
Remark 21. Observe that we can improve over the result in Corollary 18 if we accept an error /epsilon1,
so long as log(|X|)<ε2|X|/8, in the sense that the result concerns a lower-dimensional embedding.
3.5 Learnable parameterizations
Despite the theoretical appeal of the discussed distances, one of the motivating forces for our
work is their applicability to online reinforcement learning with neural networks. Speciﬁcally, we
are interested in using the derived metrics as a means to learn embeddings φthat can speed up
learning value functions for control. Towards this end, Castro et al. (2021) proposed approximating
the diﬀuse metric UπviaUω, parameterized as follows:
Uω(x,y) :=/bardblφω(x)/bardbl2
2+/bardblφω(y)/bardbl2
2
2+βθ(φω(x),φω(y))
whereφis the learned representation parameterized by ω,θ(φ(x),φ(y))is the angular distance
between vectors φ(x)andφ(y), andβ∈(0,∞)is a hyperparameter that weighs the importance of
the angular distance.
A peculiar aspect of their method is that this parameterization is approximating the diﬀuse metric ,
yet when using the representations φωfor control, they are implicitly making use only of the
weighted angular distance, which can be viewed as the reducedMICo distance:
βθ(φω(x),φω(y))≈ΠUπ(x,y).
Although producing strong empirical performance, this results in a somewhat awkward dynamic:
the metric space on which the MICo loss is being optimized is diﬀerent than the metric space where
the representations used for control exist. Given the equivalence demonstrated in Section 3.2, we
can alleviate this by learning a kernel between representations in the same inner product space used
for control.
We parametrise the kernel kπ(x,y)using the natural inner product on Rm, written as
kω(x,y) =/angbracketleftφω(x),φω(y)/angbracketright (2)
From this parametrisation, we can convert the kernel update operator Tπ
kinto a learning target in a
similar manner as was done by Castro et al. (2021). Speciﬁcally, given two transitions (x,rx,x/prime)and
(y,ry,y/prime)sampled from the replay buﬀer, we can deﬁne Tk
¯ω(rx,x/prime,ry,y/prime) = 1−1
2|rx−ry|+γk¯ω(x/prime,y/prime).
Here, ¯ωis a separate copy of the parameters ωthat are updated less frequently (as suggested by
Mnih et al. (2015) and used by Castro et al. (2021)). Our kernel-based loss is then:
LKSMe (ω) =E/angbracketleftx,rx,x/prime/angbracketright,/angbracketlefty,ry,y/prime/angbracketright/bracketleftBig/parenleftbig
Tk
¯ω(rx,x/prime,ry,y/prime)−kω(x,y)/parenrightbig2/bracketrightBig
. (3)
15Published in Transactions on Machine Learning Research (06/2023)
Note that this squared semi-gradient loss is taken directly from what was used by MICo (Castro
et al., 2021), which was based on the loss originally used by DQN (Mnih et al., 2015). Further
details on theoretical properties of this loss in the general context of reinforcement learning can be
found in Bertsekas & Tsitsiklis (1996), and discussion in the speciﬁc context of metric learning is
given by Castro et al. (2021).
With this parametrisation, the KSMe distance between two points φω(x)andφω(y)is
kω(x,x) +kω(y,y)−2kω(x,y) =/bardblφω(x)/bardbl2
2+/bardblφω(y)/bardbl2
2−2/angbracketleftφω(x),φω(y)/angbracketright
=/bardblφω(x)−φω(y)/bardbl2
2.
Thatis, theparametrisedKSMedistanceisexactlytheEuclideandistancebetweentheembeddings.
This parametrisation is supported by the theory of Section 3.4, as we are approximating the Hilbert
spaceHkπwith the Hilbert space (Rm,/bardbl·/bardbl 2), which can be summarized as:
/bardblφ(x)/bardbl2=kω(x,x)≈kπ(x,x) =/bardblϕπ(x)/bardblHkπ,
/bardblφ(x)−φ(y)/bardbl2
2=dπ
ks,ω(x,y)≈dπ
ks(x,y) =/bardblϕπ(x)−ϕπ(y)/bardbl2
Hk.
4 Empirical evaluations
Having demonstrated the theoretical equivalence between dπ
ksandΠUπ, in this section we perform
an empirical investigation to validate their equivalence in practice, and return to our original
motivation of using state metrics for representation learning. We do so in small domains where
we can compute these distance exactly (Section 4.1), and in more complex domains where we use
neural networks to approximate the distances (Section 4.2). All the code for these evaluations are
available at https://github.com/google-research/google-research/tree/master/ksme.
4.1 Empirical insights into KSMe properties
To investigate the bounds discussed in Section 3.3, we empirically investigate the bounds through
the use of Garnet MDPs, as done in Castro et al. (2021), where an empirical analysis was conducted
to investigate to what degree dπ
ks(then referred to as ΠUπ) violated the value function upper bound
(e.g. howoften dπ
ks(x,y)−|Vπ(x)−Vπ(y)|wasnegative). GivenProposition17, wecannowconduct
a more precise study: knowing that the reward variance VarR(Pπ)controls the amount by which
dπ
ks(x,y)can be greater than |Vπ(x)−Vπ(y)|, we plot the bound as VarR(Pπ)changes. We plot
both the minimum and average signed diﬀerence d(x,y)−|Vπ(x)−Vπ(y)|ford={dπ
ks,dπ
∼,Uπ}.
As can be seen in Figure 1, although dπ
kscan violate the upper bound (left plot), on average across
the Garnet MDPs it is more informative of the true value diﬀerence than Uπanddπ
∼are; this
result is consistent with the ﬁndings of Castro et al. (2021).
Before proceeding to evaluating dπ
ksin a more complex domain, it is worth taking stock of the
implications of Figure 1. When the upper bound property is violated (see left plot), it could
result in dissimilar states being mapped closely together; this could lead to over-generalization , as
discussed in the introduction above. However, the right plot suggests that in practice this does not
happen very often, and is in fact more informative than dπ
∼(for which the upper bound property
is not violated). Further, the amount of over-generalization one could suﬀer is proportional to the
reward variance of the environment. When taken together, these results suggest dπ
kscanbe an
eﬀective way of shaping RL representations during learning.
16Published in Transactions on Machine Learning Research (06/2023)
0.0 0.2 0.4 0.6 0.8 1.0
Reward std. dev.0.00.51.01.52.0min(d(s,t)|V*(s)V*(t)|)
0.0 0.2 0.4 0.6 0.8 1.0
Reward std. dev.102
101
100avg(d(s,t)|V*(s)V*(t)|)
U
 d
 dksme
Figure 1: The minimum (left) and average (right) diﬀerence between the absolute value function
diﬀerence|Vπ(x)−Vπ(y)|andUπ(Castro et al., 2021), dπ
∼(Castro, 2020), and dπ
ks, across 17K
random MDPs of varying state and action sizes, as the reward standard deviation VarR(Pπ)is
increased from 0 to 1.
4.2 Large-scale evaluation of KSMe
We adapted the code provided by Castro et al. (2021) to approximate KSMe instead of MICo,
incorporating our new loss in the same way. Speciﬁcally, we use RL agents that estimate Qvalues
by composing a learned m-dimensional representation φω:X→Rm(parameterized by ω) with a
learned value approximator ψθ:Rm→R(parameterized by θ):Q(x,·)≈ψθ(φω(x)).
Given two states xandy, the similarity between their representations is expressed via the inner
product of their embeddings (as detailed in Equation (2)):
kω(x,y) =/angbracketleftφω(x),φω(y)/angbracketright,
We then replace the MICo loss of Castro et al. (2021) ( LMICo) with theKSMeloss:
LKSMe (ω) =E/angbracketleftx,rx,x/prime/angbracketright,/angbracketlefty,ry,y/prime/angbracketright/bracketleftBig/parenleftbig
Tk
¯ω(rx,x/prime,ry,y/prime)−kω(x,y)/parenrightbig2/bracketrightBig
.
As done by Castro et al. (2021), the value approximator is trained with the standard temporal
diﬀerence loss of each agent. In our evaluation, we compared the use of LKSMewithLMICoon the
JAX agents provided by the Dopamine library (Castro et al., 2018): DQN (Mnih et al., 2015),
Rainbow (Hessel et al., 2018), QR-DQN (Dabney et al., 2018b), IQN (Dabney et al., 2018a), and
M-IQN (Vieillard et al., 2020)8. We keep all hyperparameters unchanged from those used by Castro
et al. (2021).
Due to the computational expense of running these experiments, we selected four representative
Atari 2600 games from the ALE suite (Bellemare et al., 2013) which, we felt, covered the varying
8Note that these are the same agents on which MICo was originally evaluated.
17Published in Transactions on Machine Learning Research (06/2023)
0 25 50 75 100 125 150 175 200
Number of Frames (in millions)0100200300400500600700IQM Human Normalized Score
Agent
DQN
M-IQN
IQN
Rainbow
QR-DQN
Loss
MICo loss
KSMe loss
Figure 2: Interquantile mean (Agarwal et al., 2021b) comparison of adding KSMe versus MICo on
all the Dopamine (Castro et al., 2018) value-based agents, aggregated over 5 independent runs on
four representative games.
dynamics between the original agents and those with the MICo loss. We ran 5 independent seeds
for each conﬁguration. In Figure 2 we plot the Interquantile mean (IQM) values which aggregates
human-normalizedperformanceacrossallruns; thismetricwasintroducedbyAgarwaletal.(2021b)
as a more robust statistic to compare algorithmic performance.
As can be seen, the performance of KSMe is similar to that of MICo, consistent with Theorem 15,
which proved their equivalence. Since no hyperparameter optimization was performed for KSMe,
it is quite possible the KSMe parameterization can provide empirical gains over MICo. We leave
this exploration for future work. In Appendix D we provide the learning curves for each separate
agent/game combination.
5 Discussion
The empirical results in Section 4 provide experimental validation into the theoretical results of
Section 3. Figure 1 follows the trend suggested by Proposition 17, as the worst case diﬀerence
between the value function diﬀerence and dπ
ksincreased approximately linearly with respect to the
reward standard deviation. The average gap provides an interesting perspective however, as it
demonstrates that on average, as the variance increases, the size of the gap is much larger for
Uπanddπ
∼than fordπ
ks. Having features which reﬂect the underlying value function are critical
for value-based reinforcement learning, and we hypothesize that this contributes to the empirical
success achieved by using KSMe (equivalently, the reduced MICo). As demonstrated in Figure 2,
both KSMe and MICo achieve similar empirical performance, which is expected as the underlying
distance being learnt is the same. The diﬀerences in performance is then a result of learning
distances as opposed to kernels in the neural network.
These results also provide further explanation why KSMe (equivalently the reduced MICo) appears
to achieve stronger success than bisimulation in deep reinforcement learning settings (the two
distances were compared in Castro et al. (2021), where bisimulation was learnt using DBC (Zhang
18Published in Transactions on Machine Learning Research (06/2023)
et al., 2021)). Two possible reasons coming from this work are (i) tighter relationship to the
underlying value function and (ii) superior embeddability in neural networks. The hypothesis
(i) is supported empirically by Figure 1, and theoretically by the fact that in general dπ
ks(x,y)≤
dπ
∼(x,y). Webelievethatthistighterboundleadstoimprovedperformancebecausedistanceswhich
correctly approximate the value function diﬀerence between states allow for straightforward value-
based learning. Secondly, the fact that KSMe comes from a Hilbert state structure allows eﬃcient
embeddability into low-dimension Euclidean space, a very important concept in deep settings, as
this is exactly what neural networks aim to accomplish. Theorem 19 provides a result guaranteeing
this is possible. We note that it is not clear whether a similar result for bisimulation metrics is
possible, as the Kantorovich metric cannot be approximated in Euclidean spaces with low distortion
in general (Peyré & Cuturi, 2019).
6 Conclusion
In this work we have taken a kernel perspective on learning representations in reinforcement learn-
ing, and introduced a state similarity kernel on Markov decision process state spaces. This kernel
naturally induces a distance, which we proved was equal to the reduced MICo distance (Castro
et al., 2021). This allowed us to perform a theoretical analysis of the reduced MICo distance which
waspreviouslylacking, andanswerimportantquestionssuchasitsmetricpropertiesandconnection
to value functions. We then analyzed a previously-unconsidered question: how well the distance
itself can be approximated in Euclidean spaces, and prove a bound demonstrating embeddability.
We then adapted the loss introduced in Castro et al. (2021) to learn the kernel. While the distance
learnt is theoretically equivalent to theirs, our parametrization is theoretically grounded as the
neural network embeddings are an approximation to kernel Hilbert space embeddings. To the best
of our knowledge, this is the ﬁrst work which studied how well a given state similarity metric can be
approximated through a neural network, and provided bounds on the incurred error. These results
provide theoretical grounding for the reduced MICo distance, and our kernel perspective analysis
may be used in future work to analyze related distances.
It is worth noting that by focusing on the upper-bound properties of behavioural metrics we are
providingassymetricalguarantees. Namely, whiletheycanguaranteesimilarstateswillhavesimilar
values, they do not guarantee that dissimilar states will have dissimilar values. It would thus be
valuable to investigate further why these metrics can provide such strong performance despite the
asymmetrical guarantees.
19Published in Transactions on Machine Learning Research (06/2023)
Acknowledgements
We thank Gheorghe Comanici for detailed feedback on an earlier version of the paper, and the
anonymous TMLR reviewers and action editor Gergely Neu for helping us strengthen our submis-
sion.
We would also like to thank the Python community (Van Rossum & Drake Jr, 1995; Oliphant,
2007)for developingtools thatenabledthis work, including NumPy(Harris et al.,2020), Matplotlib
(Hunter, 2007) and JAX (Bradbury et al., 2018).
Author Contributions
Authors listed in alphabetical order, and individual contributions listed below.
Pablo Samuel Castro helped give direction to the work, revised the theoretical results, wrote
the code, ran the deep RL experiments, and contributed to writing the paper.
Tyler Kastner proved most of the theoretical results, ran the toy experiments, and contributed
to writing the paper.
PrakashPanangaden helpedgivedirectiontothework, revisedthetheoreticalresults, supervised
Tyler, and contributed to writing the paper.
Mark Rowland helped give direction to the work, revised the theoretical results, reviewed the
code, and contributed to writing the paper.
20Published in Transactions on Machine Learning Research (06/2023)
References
Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, and Marc G. Bellemare. Contrastive
behavioral similarity embeddings for generalization in reinforcement learning. In In Proceedings
of the Ninth International Conference on Learning Representations , 2021a.
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G. Bellemare.
Deep reinforcement learning at the edge of the statistical precipice. In Advances in Neural
Information Processing Systems , 2021b.
Wolfgang Arendt, Charles Batty, Matthias Hieber, and Frank Neubrander. Vector-valued Laplace
transforms and Cauchy problems . Springer-Verlag, Jan 2001.
Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical
society, 68(3):337–404, 1950.
Leemon C. Baird. Residual algorithms: Reinforcement learning with function approximation. In
Proceedings of the International Conference on Machine Learning , 1995.
AndréMSBarreto, DoinaPrecup, andJoellePineau. Practicalkernel-basedreinforcementlearning.
The Journal of Machine Learning Research , 17(1):2372–2441, 2016.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning Envi-
ronment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research ,
47:253–279, June 2013.
Marc G. Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel Castro, Nico-
las Le Roux, Dale Schuurmans, Tor Lattimore, and Clare Lyle. A geometric perspective on
optimal representations for reinforcement learning. In Advances in Neural Information Process-
ing Systems , 2019.
Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-dynamic programming. Athena Scientiﬁc, 1996.
RichardBlute, JoséeDesharnais, AbbasEdalat, andPrakashPanangaden. Bisimulationforlabelled
Markov processes. In Proceedings of IEEE Symposium On Logic In Computer Science , 1997.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, et al. Jax:
composable transformations of python+ numpy programs. 2018.
Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic Markov
decision processes. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 2020.
Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare.
Dopamine: A Research Framework for Deep Reinforcement Learning. arXiv, 2018.
Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, and Mark Rowland. MICo: Learning
improved representations via sampling-based state similarity for Markov decision processes. In
Advances in Neural Information Processing Systems , 2021.
Gheorghe Comanici, Prakash Panangaden, and Doina Precup. On-the-ﬂy algorithms for bisim-
ulation metrics. In Proceedings of the International Conference on Quantitative Evaluation of
Systems, 2012.
21Published in Transactions on Machine Learning Research (06/2023)
Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for
distributional reinforcement learning. In Proceedings of the 35th International Conference on
Machine Learning , volume 80 of Proceedings of Machine Learning Research , pp. 1096–1105.
PMLR, 2018a.
Will Dabney, Mark Rowland, Marc G. Bellemare, and Rémi Munos. Distributional reinforcement
learning with quantile regression. In Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence, 2018b.
Peter Dayan. Improving generalization for temporal diﬀerence learning: The successor representa-
tion.Neural Computation , 5(4):613–624, 1993.
J. Desharnais, A. Edalat, and P. Panangaden. Bisimulation for labeled Markov processes. Infor-
mation and Computation , 179(2):163–193, Dec 2002.
Josée Desharnais, Vineet Gupta, Radha Jagadeesan, and Prakash Panangaden. Metrics for labeled
Markov systems. In Proceedings of the International Conference on Concurrency Theory , 1999.
Michel Marie Deza and Monique Laurent. Geometry of cuts and metrics , volume 2. Springer, 1997.
Omar Darwiche Domingues, Pierre Ménard, Matteo Pirotta, Emilie Kaufmann, and Michal Valko.
Kernel-based reinforcement learning: A ﬁnite-time analysis. In Proceedings of the International
Conference on Machine Learning , 2021.
A.m. Farahmand, M. Ghavamzadeh, Cs. Szepesvári, and S. Mannor. Regularized policy itera-
tion with nonparametric function spaces. The Journal of Machine Learning Research , 17:1–66,
January 2016.
Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for ﬁnite Markov decision processes.
InProceedings of the Conference on Uncertainty in Artiﬁcial Intelligence , 2004.
Norm Ferns, Pablo Samuel Castro, Doina Precup, and Prakash Panangaden. Methods for com-
puting state similarity in Markov decision processes. In Conference on Uncertainty in Artiﬁcial
Intelligence (UAI) , 2006.
Norm Ferns, Prakash Panangaden, and Doina Precup. Bisimulation metrics for continuous Markov
decision processes. SIAM Journal on Computing , 40(6):1662–1714, 2011.
Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Learning
visual feature spaces for robotic manipulation with deep spatial autoencoders. arXiv, 2015.
Carles Gelada, Saurabh Kumar, Jacob Buckman, Oﬁr Nachum, and Marc G Bellemare. Deep-
MDP: Learning continuous latent space models for representation learning. In Proceedings of the
International Conference on Machine Learning , 2019.
Corrado Gini. Variabilità e mutabilità: contributo allo studio delle distribuzioni e delle relazioni
statistiche. Studi economico-giuridici pubblicati per cura della facoltà di Giurisprudenza della R.
Università di Cagliari. Tipogr. di P. Cuppini, 1912.
Robert Givan, Thomas Dean, and Matthew Greig. Equivalence notions and model minimization
in Markov decision processes. Artiﬁcial Intelligence , 147(1-2):163–223, 2003.
ArthurGretton, KarstenM.Borgwardt, MalteJ.Rasch, BernhardSchölkopf, andAlexanderSmola.
A kernel two-sample test. Journal of Machine Learning Research , 13(25):723–773, 2012.
22Published in Transactions on Machine Learning Research (06/2023)
C. Guilbart. Produits scalaires sur l’espace des mesures. Annales de l’I.H.P. Probabilités et statis-
tiques, 15(4):333–354, 1979.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In Proceedings of the International
Conference on Machine Learning , 2019.
Philippe Hansen-Estruch, Amy Zhang, Ashvin Nair, Patrick Yin, and Sergey Levine. Bisimulation
makes analogies in goal-conditioned reinforcement learning. In Proceedings of the International
Conference on Machine Learning , 2022.
Charles R Harris, K Jarrod Millman, Stéfan J Van Der Walt, Ralf Gommers, Pauli Virtanen,
David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, et al. Array
programming with numpy. Nature, 585(7825):357–362, 2020.
MatteoHessel, JosephModayil, HadovanHasselt, TomSchaul, GeorgOstrovski, WillDabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining Improvements in
Deep Reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence ,
2018.
John D Hunter. Matplotlib: A 2d graphics environment. Computing in science & engineering , 9
(03):90–95, 2007.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In
Proceedings of the International Conference on Learning Representations , 2017.
William Johnson and Joram Lindenstrauss. Extensions of Lipschitz maps into a Hilbert space.
Contemporary Mathematics , 26:189–206, 01 1984.
Leonid V. Kantorovich and G. Sh. Rubinshtein. On a space of totally additive functions. Vestnik
Leningrad. Univ , 13(7):52–59, 1958.
Mete Kemertas and Tristan Aumentado-Armstrong. Towards robust bisimulation metric learning.
Advances in Neural Information Processing Systems , 2021.
Mete Kemertas and Allan Douglas Jepson. Approximate policy iteration with bisimulation metrics.
Transactions on Machine Learning Research , 2022.
George Konidaris, Sarah Osentoski, and Philip Thomas. Value function approximation in rein-
forcement learning using the Fourier basis. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence , 2011.
Alec Koppel, Garrett Warnell, Ethan Stump, Peter Stone, and Alejandro Ribeiro. Policy evaluation
in continuous mdps with eﬃcient kernelized gradient temporal diﬀerence. IEEE Trans. Autom.
Control., 66(4):1856–1863, 2021.
Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement learning.
InProceedings of the International Joint Conference on Neural Networks , 2010.
Kim G. Larsen and Arne Skou. Bisimulation through probablistic testing. Information and Com-
putation, 94:1–28, 1991.
23Published in Transactions on Machine Learning Research (06/2023)
Charline Le Lan, Marc G. Bellemare, and Pablo Samuel Castro. Metrics and continuity in rein-
forcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 2021.
G. Lever, J. Shawe-Taylor, R. Staﬀord, and Cs. Szepesvári. Compressed conditional mean em-
beddings for model-based reinforcement learning. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , 2016.
Xingyu Lin, Harjatin Baweja, George Kantor, and David Held. Adaptive auxiliary task weighting
for reinforcement learning. In Advances in Neural Information Processing Systems , 2019.
Sridhar Mahadevan and Mauro Maggioni. Proto-value functions: A Laplacian framework for learn-
ing representation and control in Markov decision processes. Journal of Machine Learning Re-
search, 8:2169–2231, Dec 2007.
Jiri Matousek. Lectures on discrete geometry , volume 212. Springer Science & Business Media,
2013.
Stephen G. Matthews. Partial metric topology. Annals of the New York Academy of Sciences , 728
(1):183–197, 1994.
Robert Milner. A Calculus for Communicating Systems , volume 92 of Lecture Notes in Computer
Science. Springer-Verlag, 1980.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529–533, 2015.
Alfred Müller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability , 29(2):429–443, 1997. ISSN 00018678.
Travis E. Oliphant. Python for scientiﬁc computing. Computing in Science & Engineering , 9(3):
10–20, 2007. doi: 10.1109/MCSE.2007.58.
Dirk Ormoneit and Śaunak Sen. Kernel-based reinforcement learning. Machine Learning , 49(2–3):
161–178, 2002.
Prakash Panangaden. Labelled Markov processes . Imperial College Press, 2009.
David Park. Title unknown. Slides for Bad Honnef Workshop on Semantics of Concurrency, 1981.
Gabriel Peyré and Marco Cuturi. Computational optimal transport. Foundations and Trends ®in
Machine Learning , 11(5-6):355–607, 2019.
Svetlozar Rachev, Lev B. Klebanov, Stoyan V. Stoyanov, and Frank J. Fabozzi. The method of
distances in the theory of probability and statistics . Springer-Verlag, 2013.
Frigyes Riesz. Sur une espèce de Géométrie analytique des systèmes de fonctions sommables .
Gauthier-Villars, 1907.
Herbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Mathe-
matical Statistics , 22(3):400 – 407, 1951.
24Published in Transactions on Machine Learning Research (06/2023)
Walter Rudin. Functional Analysis . Tata McGraw-Hill, 1974.
Isaac J. Schoenberg. Remarks to Maurice Fréchet’s article“Sur la deﬁnition axiomatique d’une
classe d’espace distances vectoriellement applicable sur l’espace de Hilbert”. Annals of Mathe-
matics, pp. 724–732, 1935.
Bernhard Schölkopf, Alexander J. Smola, and Francis Bach. Learning with kernels: Support vector
machines, regularization, optimization, and beyond . The MIT Press, 2018.
Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, and Kenji Fukumizu. Equivalence of
distance-based and RKHS-based statistics in hypothesis testing. The Annals of Statistics , 41(5):
2263–2291, 2013.
Evan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell. Loss is its own reward:
Self-supervision for reinforcement learning. In Proceedings of the International Conference on
Learning Representations (Workshop Track) , 2017.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction . The MIT
Press, 2018.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David
Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin
Riedmiller. DeepMind control suite. arXiv, 2018.
Franck van Breugel and James Worrell. Towards quantitative veriﬁcation of probabilistic systems.
InProceedings of the International Colloquium on Automata, Languages and Programming , July
2001.
Guido Van Rossum and Fred L Drake Jr. Python reference manual . Centrum voor Wiskunde en
Informatica Amsterdam, 1995.
Nino Vieillard, Olivier Pietquin, and Matthieu Geist. Munchausen reinforcement learning. In
Advances in Neural Information Processing Systems (NeurIPS 2020) , 2020.
Cédric Villani. Optimal transport: Old and new , volume 338. Springer Science & Business Media,
2008.
Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael Jordan. Provably eﬃcient
reinforcement learning with kernel and neural function approximations. In Advances in Neural
Information Processing Systems , 2020.
Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Im-
proving sample eﬃciency in model-free reinforcement learning from images. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence , 2021.
Shlomo Yitzhaki. Gini’s mean diﬀerence: A superior measure of variability for non-normal distri-
butions. Metron - International Journal of Statistics , LXI(2):285–316, 2003.
Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Invariant repre-
sentations for reinforcement learning without reconstruction. In Proceedings of the International
Conference on Learning Representations , 2021.
Szymon Łukaszyk. A new concept of probability metric and its applications in approximation of
scattered data sets. Computational Mechanics , 33:299–304, 03 2004.
25Published in Transactions on Machine Learning Research (06/2023)
A Extra technical results
For the following two lemmas, we will make use of the sequences (kn)n≥0,(Un)n≥0deﬁned by
kn≡0,kn+1=Tπ
k(kn),Un≡0,Un+1=Tπ
M(Un).
Lemma 22. For any point (x1,x2,y1,y2)∈X4, andn≥0, we have that
E
X/prime
1∼Pπ
x1,X/prime
2∼Pπ
x2
Y/prime
1∼Pπ
y1,Y/prime
2∼Pπ
y2/bracketleftbigkn(X/prime
1,X/prime
2) +kn(Y/prime
1,Y/prime
2)−2kn(X/prime
1,Y/prime
1)/bracketrightbig
= E
X/prime
1∼Pπ
x1,X/prime
2∼Pπ
x2
Y/prime
1∼Pπ
y1,Y/prime
2∼Pπ
y2/bracketleftbigg
Un(X/prime
1,Y/prime
1)−1
2/parenleftbigUn(X/prime
1,X/prime
2) +Un(Y/prime
1,Y/prime
2)/parenrightbig/bracketrightbigg
.
Proof.We show this by induction. Both sides are identically zero at n= 0, so we set n≥0and
assume the induction hypothesis. We can then write out
E
X/prime
1∼Pπ
x1,X/prime
2∼Pπ
x2
Y/prime
1∼Pπ
y1,Y/prime
2∼Pπ
y2[kn+1(X/prime
1,X/prime
2) +kn+1(Y/prime
1,Y/prime
2)−2kn+1(X/prime
1,Y/prime
1)]
= E
X/prime
1∼Pπ
x1,X/prime
2∼Pπ
x2
Y/prime
1∼Pπ
y1,Y/prime
2∼Pπ
y2/bracketleftBigg/parenleftbigg
|rπ
X1−rπ
Y1|−1
2(|rπ
X1−rπ
X2|+|rπ
Y1−rπ
Y2|)/parenrightbigg
+ E
X/prime/prime
1∼Pπ
X/prime
1,X/prime/prime
2∼Pπ
X/prime
2
Y/prime/prime
1∼Pπ
Y/prime
1,Y/prime/prime
2∼Pπ
Y/prime
2/bracketleftBigg
kn(X/prime/prime
1,X/prime/prime
2) +kn(Y/prime/prime
1,Y/prime/prime
2)−2kn(X/prime/prime
1,Y/prime/prime
1)/bracketrightBigg/bracketrightBigg
= E
X/prime
1∼Pπ
x1,X/prime
2∼Pπ
x2
Y/prime
1∼Pπ
y1,Y/prime
2∼Pπ
y2/bracketleftBigg/parenleftbigg
|rπ
X1−rπ
Y1|−1
2(|rπ
X1−rπ
X2|+|rπ
Y1−rπ
Y2|)/parenrightbigg/bracketrightBigg
+ E
X/prime
1∼Pπ
x1,X/prime
2∼Pπ
x2
Y/prime
1∼Pπ
y1,Y/prime
2∼Pπ
y2
E
X/prime/prime
1∼Pπ
X/prime
1,X/prime/prime
2∼Pπ
X/prime
2
Y/prime/prime
1∼Pπ
Y/prime
1,Y/prime/prime
2∼Pπ
Y/prime
2/bracketleftBigg
kn(X/prime/prime
1,X/prime/prime
2) +kn(Y/prime/prime
1,Y/prime/prime
2)−2kn(X/prime/prime
1,Y/prime/prime
1)/bracketrightBigg

= E
X/prime
1∼Pπ
x1,X/prime
2∼Pπ
x2
Y/prime
1∼Pπ
y1,Y/prime
2∼Pπ
y2/bracketleftBigg/parenleftbigg
|rπ
X1−rπ
Y1|−1
2(|rπ
X1−rπ
X2|+|rπ
Y1−rπ
Y2|)/parenrightbigg/bracketrightBigg
+ E
X/prime
1∼Pπ
x1,X/prime
2∼Pπ
x2
Y/prime
1∼Pπ
y1,Y/prime
2∼Pπ
y2
E
X/prime/prime
1∼Pπ
X/prime
1,X/prime/prime
2∼Pπ
X/prime
2
Y/prime/prime
1∼Pπ
Y/prime
1,Y/prime/prime
2∼Pπ
Y/prime
2/bracketleftBigg
Un(X/prime/prime
1,Y/prime/prime
1)−1
2(Un(Y/prime/prime
1,Y/prime/prime
2) +Un(Y/prime/prime
1,Y/prime/prime
2))/bracketrightBigg

= E
X/prime
1∼Pπ
x1,X/prime
2∼Pπ
x2
Y/prime
1∼Pπ
y1,Y/prime
2∼Pπ
y2/bracketleftBigg/parenleftbigg
|rπ
X1−rπ
Y1|−1
2(|rπ
X1−rπ
X2|+|rπ
Y1−rπ
Y2|)/parenrightbigg
+ E
X/prime/prime
1∼Pπ
X/prime
1,X/prime/prime
2∼Pπ
X/prime
2
Y/prime/prime
1∼Pπ
Y/prime
1,Y/prime/prime
2∼Pπ
Y/prime
2/bracketleftBigg
Un(X/prime/prime
1,Y/prime/prime
1)−1
2(Un(Y/prime/prime
1,Y/prime/prime
2) +Un(Y/prime/prime
1,Y/prime/prime
2))/bracketrightBigg

26Published in Transactions on Machine Learning Research (06/2023)
= E
X/prime
1∼Pπ
x1,X/prime
2∼Pπ
x2
Y/prime
1∼Pπ
y1,Y/prime
2∼Pπ
y2/bracketleftbigg
Un+1(X/prime
1,Y/prime
1)−1
2(Un+1(X/prime
1,X/prime
2) +Un+1(Y/prime
1,Y/prime
2))/bracketrightbigg
,
as desired.
Lemma 14. For any measures µ,ν, andn≥0, we have that
E
X1,X2∼µ
Y1,Y2∼ν[kn(X1,X2) +kn(Y1,Y2)−2kn(X1,Y1)] = E
X1,X2∼µ
Y1,Y2∼ν/bracketleftbigg
Un(X1,Y1)−1
2(Un(X1,X2) +Un(Y1,Y2))/bracketrightbigg
.
Proof.We proceed to show this by induction. The base case is straightforward, as both sides are
identically zero. We can now assume the induction hypothesis, and can write out
E
X1,X2∼µ
Y1,Y2∼ν[kn+1(X1,X2) +kn+1(Y1,Y2)−2kn+1(X1,Y1)]
=E
X1,X2∼µ
Y1,Y2∼ν
/parenleftBig
|rπ
X1−rπ
Y1|−1
2(|rπ
X1−rπ
X2|+|rπ
Y1−rπ
Y2|)/parenrightBig
+γE
X/prime
1∼Pπ
X1
X/prime
2∼Pπ
X2
Y/prime
1∼Pπ
Y1
Y/prime
2∼Pπ
Y2/bracketleftbig
kn(X/prime
1,X/prime
2) +kn(Y/prime
1,Y/prime
2)−2kn(X/prime
1,Y/prime
1)/bracketrightbig

=E
X1,X2∼µ
Y1,Y2∼ν
/parenleftBig
|rπ
X1−rπ
Y1|−1
2(|rπ
X1−rπ
X2|+|rπ
Y1−rπ
Y2|)/parenrightBig
+γE
X/prime
1∼Pπ
X1
X/prime
2∼Pπ
X2
Y/prime
1∼Pπ
Y1
Y/prime
2∼Pπ
Y2/bracketleftBig
Un(X/prime
1,Y/prime
1)−1
2(Un(X/prime
1,X/prime
2) +Un(Y/prime
1,Y/prime
2))/bracketrightBig
(⋆)
=E
X1,X2∼µ
Y1,Y2∼ν/bracketleftBig
Un+1(X1,Y1)−1
2(Un+1(X1,X2) +Un+1(Y1,Y2)/bracketrightBig
,
where (⋆)follows from Lemma 22.
B Background
B.1 Markov decision processes
We consider Markov decision processes (MDPs) given by (X,A,P,R,γ), whereXis a ﬁnite state
space,Aa set of actions, P:X×A→ P(X)a transition kernel, and R:X×A→ P(R)a
reward kernel (where P(Z)is the set of probability distributions on a measurable set Z). We will
writePa
x:=P(x,a)for the transition distribution from taking action ain statex,Ra
x:=R(x,a)
for the reward distribution from taking action ain statex, and write ra
xfor the expectation of this
distribution. A policy πis a mappingX→P(A). We use the notation Pπ
x=/summationtext
a∈Aπ(a|x)Pa
xto
indicate the state distribution obtained by following one step of a policy πwhile in state x. We use
Rπ
x=/summationtext
a∈Aπ(a|x)Ra
xto represent the reward distribution from xunderπ, andrπ
xto indicate the
expected value of this distribution. γ∈[0,1)is the discount factor used to compute the discounted
long-term return.
27Published in Transactions on Machine Learning Research (06/2023)
We will often make use of the random trajectory (Xt,At,Rt)t≥0, whereXt,At, andRtare random
variablesrepresentingthestate, action, andrewardattime t, respectively. Wewilloccasionallytake
expectationswithrespecttopolicies, writtenas Eπ[·], whichshouldbereadastheexpectation, given
that for all t≥0we chooseAt∼π(·|Xt), and receive Rt∼R(·|Xt,At)andXt+1∼P(·|Xt,At).
Thevalueof a policy πis the expected total return an agent attains from following π, and is
described by a function Vπ:X→R, such that for each x∈X,
Vπ(x) =Eπ
/summationdisplay
t≥0γtRt/vextendsingle/vextendsingle/vextendsingle/vextendsingleX0=x
,
A related quantity is the action-value function Qπ:X×A→ R, which indicates the value of taking
an action in a state, and then following the policy:
Qπ(x,a) =Eπ
/summationdisplay
t≥0γtRt/vextendsingle/vextendsingle/vextendsingle/vextendsingleX0=x,A 0=a
.
A foundational relationship in reinforcement learning is the Bellman equation , which allows the
value function of a state to be written recursively in terms of next states. It exists in two forms,
forVπandQπrespectively:
Vπ(x) =Eπ/bracketleftbigg
R0+γVπ(X1)/vextendsingle/vextendsingle/vextendsingle/vextendsingleX0=x/bracketrightbigg
,
Qπ(x,a) =Eπ/bracketleftbigg
R0+γVπ(X1)/vextendsingle/vextendsingle/vextendsingle/vextendsingleX0=x,A 0=a/bracketrightbigg
.
Rewriting these equations without the use of the random trajectory, we have
Vπ(x) =rπ
x+γE
X/prime∼Pπx/bracketleftbigVπ(X/prime)/bracketrightbig,
Qπ(x,a) =ra
x+γ E
X/prime∼Pax,A/prime∼π(·|X/prime)/bracketleftbigQπ(X/prime,A/prime)/bracketrightbig.
The Bellman operator Tπtransforms the above equations into an operator over RX(orRX×A–
we will overload the use of Tπand let the type signature indicate which is being used), given by
TπV(x) =rπ
x+γE
X/prime∼Pπx/bracketleftbigV(X/prime)/bracketrightbig,
TπQ(x,a) =ra
x+γ E
X/prime∼Pax,A/prime∼π(·|X/prime)/bracketleftbigQ(X/prime,A/prime)/bracketrightbig.
Written in this way, we see that QπandVπare ﬁxed points of Tπ, and with some work one can
also see that Tπis a contraction with modulus γ. As a corollary of Banach’s ﬁxed point theorem,
one can choose V0arbitrarily and update Vk+1=TπVk, and converge to Vπ, this is the algorithm
known as value iteration .
An optimal policy π∗is a policy which achieves the maximum value function at each state, which
we will denote V∗. It satisﬁes the Bellman optimality recurrence:
V∗(x) = max
a∈A[R0+γV∗(X1)|X0=x].
28Published in Transactions on Machine Learning Research (06/2023)
C Behavioural metrics
In this section, we review various distances which have appeared in literature, and discuss how
they relate to each other. A behavioural metric is usually deﬁned using the following pattern. One
is comparing the diﬀerence between two states, the ﬁrst and most obvious diﬀerence between the
states is a reward diﬀerence, accordingly this is the ﬁrst term in the behavioural metric deﬁnition.
Butonewantstotakeintoaccountthediﬀerencesinthesubsequentevolutionofthesystemstarting
from the two states. Thus, one needs a notion of diﬀerence in the “next states”. However, since
these are probabilistic systems, there is no unique next state; one has a probability distribution
over the next states. Thus, one needs a metric that can measure the diﬀerences between probability
distributions.
Metrics between probability distrubutions have a rich theory (Rachev et al., 2013) and history. We
review parts of this theory in the ﬁrst subsection before using these metrics to construct behavioural
metrics between the states of an MDP.
C.1 Metrics on probability distributions
C.1.1 The Kantorovich metric
Given two probability measures µandνon a setX, a coupling λ∈P(X×X )of the measures is
a joint distribution with marginals µandν. Formally, we have that for every measurable subset
A⊂X,
λ(A×X) =µ(A)andλ(X×A) =ν(A).
We deﬁne Λ(µ,ν)to represent the set of all couplings of µandν. This set is non-empty in general,
in particular the independent coupling λ=µ×νalways exists. Couplings are essential for the
deﬁnition of the Kantorovich metric W(Kantorovich & Rubinshtein, 1958) (also known as the
Wasserstein metric), a metric on the space of probability distributions on X. Given a metric don
X9, the Kantorovich metric is deﬁned as
W(d)(µ,ν) = inf
λ∈Λ(µ,ν)/integraldisplay
d(x,y)dλ(x,y).
The coupling which attains the inﬁmum always exists, and is referred to as the optimal coupling of
µandν(Villani, 2008).
C.1.2 The Łukaszyk–Karmowski distance
We recall that the Kantorovich metric optimizes over the space of all couplings; indeed, the com-
putational diﬃculty of calculating the Kantorovich distance comes from calculating this inﬁmum,
since for each pair of measures one must solve an optimization problem. The Łukaszyk–Karmowski
distancedŁK(Łukaszyk, 2004)avoids this optimization, and instead considers the independent cou-
pling between the measures. That is,
dŁK(d)(µ,ν) =/integraldisplay
d(x,y) d(µ×ν)(x,y),
or equivalently
dŁK(d)(µ,ν) = E
X∼µ,Y∼ν[d(X,Y )].
9To be precise, we require (X,d)to be a Polish space, but we drop these technical assumptions for clarity of
presentation.
29Published in Transactions on Machine Learning Research (06/2023)
Without the need for the optimization over couplings, the computation of dŁKreduces to the
above computation, which is much more computationally eﬃcient. When the base distance dis the
Euclidean distance /bardbl·/bardbl, the Łukaszyk–Karmowski distance has been used in econometrics, usually
referred to as Gini’s coeﬃcient (Gini, 1912; Yitzhaki, 2003).
The computational advantage comes at a price, however. While the Kantorovich metric satisﬁes all
the axioms of a proper metric, the Łukaszyk–Karmowski distance does not. This is due to the fact
that measures can have non-negative self distances, meaning one may ﬁnd a measure µsuch that
dŁK(d)(µ,µ)>0. One can show that a measure µsatisﬁesdŁK(d)(µ,µ) = 0if and only if µis a
Dirac measure, that is µ=δxfor somex∈X(Łukaszyk, 2004). Intuitively, dŁK(d)(µ,µ)should be
seen as a measure of dispersion ofµ. One interpretation of this in the literature (Łukaszyk, 2004)
is thatdŁKcaptures a concept of uncertainty : given two random variables X∼µ,Y∼ν, unlessµ
andνare point masses, observed values of XandYare less likely to be equal depending on the
dispersions of µandν. HencedŁKcaptures a measure of uncertainty in the observed distance of
XandY, compared to a proper probability metric which would assign distance 0ifXL=Y.
The concept of distance functions with non-zero self distances has been considered before, in par-
ticular through partial metrics (Matthews, 1994). A partial metric is a function d:X×X→ [0,∞)
such that for any x,y,z∈X:
•0≤d(x,y) Non-negativity
•d(x,x)≤d(x,y) Small self-distances
•d(x,y) =d(y,x) Symmetry
•ifd(x,x) =d(x,y) =d(y,y), thenx=y Indistancy implies equality
•d(x,y)≤d(x,z) +d(y,z)−d(z,z) Modiﬁed triangle inequality
We note that there were additional axioms added to this deﬁnition, rather than simply removing
the requirement that d(x,x) = 0. This is due to the fact that this deﬁnition was constructed
so that one can easily construct a proper metric ˜dfrom a partial metric d, given by ˜d(x,y) =
d(x,y)−1
2(d(x,x) +d(y,y)). We can now show that this deﬁnition is indeed too strong for the
Łukaszyk–Karmowski distance, which we demonstrate in the following examples.
Example 1. The Łukaszyk–Karmowski distance does not have small self-distances.
Proof.TakeX= [0,1],d=|·|,µ=δ1/2,ν=U([0,1]). Then one can calculate dŁK(d)(ν,ν) =/integraltext1
0/integraltext1
0|x−y|dxdy =1
3, anddŁK(d)(µ,ν) =/integraltext1
0|x−1
2|dx=1
4. But then we have dŁK(d)(ν,ν) =1
3>
1
4=dŁK(d)(µ,ν).
Example 2. The Łukaszyk–Karmowski distance does not satisfy the modiﬁed triangle inequality.
Proof.TakeX= [0,1],d=|·|,µ=δ0,ν=δ1,η=1
2(δ0+δ1). We can then calculate dŁK(d)(µ,ν) =
|1−0|= 1,dŁK(d)(µ,η) =dŁK(d)(ν,η) =1
2(0) +1
2(1) =1
2,dŁK(d)(η,η) =1
4(0) +1
2(1) +1
4(0) =1
2.
Combining, we have dŁK(d)(µ,η) +d(ν,η)−d(η,η) =1
2+1
2−1
2, which then gives us
dŁK(d)(µ,ν) = 1>1
2=dŁK(d)(µ,η) +d(ν,η)−d(η,η),
breaking the modiﬁed triangle inequality.
30Published in Transactions on Machine Learning Research (06/2023)
To account for this, Castro et al. (2021) introduced a new notion of distance known as diﬀuse
metrics. A diﬀuse metric is a function d:X×X→ [0,∞)such that for any x,y,z∈X:
•0≤d(x,y) Non-negativity
•d(x,y) =d(y,x) Symmetry
•d(x,y)≤d(x,z) +d(y,z) Triangle inequality
It is straightforward to see that the Łukaszyk–Karmowski distance is a diﬀuse metric. In addition,
an attractive property of the Łukaszyk–Karmowski distance for the reinforcement learning setting
isthatitlendsitselfreadilytostochasticapproximation. Giventwoindependentstreamsofsamples
(xn)n≥1,(yn)n≥1from random variables X∼µandY∼ν, a base metric dsuch thatd(X,Y )has
ﬁnite variance, and a sequence of step sizes (αn)n≥1satisfying the Robbins-Monro conditions, one
can construct a sequence of iterates (dn)deﬁned by
dn= (1−αn)dn−1+αnd(xn,yn),
withd0= 0. Then we have dn→dŁK(d)(µ,ν)asn→∞(Robbins & Monro, 1951).
C.2 Bisimulation metrics
C.2.1 Bisimulation relations
Bisimulation was invented in the context of concurrency theory by Milner (1980) and Park (1981).
Probabilistic bisimulation (Larsen & Skou, 1991; Blute et al., 1997; Desharnais et al., 2002; Panan-
gaden, 2009) (henceforth just called bisimulation) is an equivalence on the state space of a labelled
Markov process, where two states are considered equivalent if the behaviour from the states are
indistinguishable . To deﬁne indistinguishability, we demand that transition probabilities to equiv-
alence classes should be the same for equivalent states; that is, the equivalence classes preserve the
dynamics of the process. In addition if there are more observables, for example, rewards, those
should match as well. Bisimulation for MDPs was deﬁned in Givan et al. (2003). This intuition
can now be transformed into a deﬁnition.
Deﬁnition 23. An equivalence relation RonXis a bisimulation relation if
xRy =⇒ ∀a∈A,ra
x=ra
yand∀C∈X/R,Pa
x(C) =Pa
y(C).
We say that states xandyare bisimilar if there exists a bisimulation relation Rsuch thatxRy. We
remark that there exists at least one bisimulation relation, as the diagonal relation ∆ ={(x,x) :
x∈X}is always a bisimulation relation, albeit the least interesting one. One refers to the largest
bisimulation relation as ∼, which is often the one of interest. This version is due to Larsen and
Skou (Larsen & Skou, 1991) and the extension to continuous state spaces is due to (Blute et al.,
1997; Desharnais et al., 2002).
As Markov decision processes may be seen as Markov processes with rewards, bisimulation relations
and metrics have a natural analogue in this setting.
C.2.2 Bisimulation metrics
The stringency of bisimulation relations is apparent in the MDP case: if two states have bisimilar
transition dynamics, that is we have that ∀aand∀C∈X/R,Pa
x(C) =Pa
y(C), but|ra
x−ra
y|=ε>0,
31Published in Transactions on Machine Learning Research (06/2023)
thenxandyare in diﬀerent equivalence classes. This motivates us to introduce a metric analogue
of bisimulation. We will write M(X)to represent the space of bounded pseudometrics on X.
Theorem 24 (Ferns et al. (2004)) .DeﬁneF:M(X)→M (X)as
F(d)(x,y) = max
a∈A/parenleftBig
|ra
x−ra
y|+γW(d)(Pa
x,Pa
y)/parenrightBig
.
ThenFis a contraction in /bardbl·/bardbl∞with modulus γ, and hence exhibits a unique ﬁxed point d∼, which
we denote the bisimulation metric.
Justiﬁcation for the term bisimulation metric follows from the fact that the kernel (the kernel of a
pseudometric dis the set of pairs of points deemed ‘equivalent’, formally the set {x,y:d(x,y) = 0})
ofd∼is a bisimulation relation.
Proposition 25 (Ferns et al. (2004)) .V∗is1-Lipschitz with respect to d∼, that is for any x,y∈X,
|V∗(x)−V∗(y)|≤d∼(x,y).
C.3π-bisimulation metrics
Bisimulation considers equivalence across all possible actions, which is a strong notion of equiv-
alence. In many settings, in a given state an agent may not be concerned with the behaviour
under every possible action, but instead only with the actions which it may take under a given
policy. On-policy bisimulation (Castro, 2020) was introduced to address this. The deﬁnition is a
straightforward modiﬁcation of bisimulation, adapted to a given policy.
Deﬁnition 26 (On-policy bisimulation relations) .Letπbe a ﬁxed policy. An equivalence relation
RonXis aπ-bisimulation relation if
xRy =⇒rπ
x=rπ
yand∀C∈X/R,Pπ
x(C) =Pπ
y(C).
Remark 27. It is important to note that while the deﬁnitions of bisimulation relations and π-
bisimulation relations appear very similar, they have intrinsic diﬀerences. In particular, two states
which are bisimilar need not be π-bisimilar, and two states which are π-bisimilar need not be bisim-
ilar. To see this intuitively, consider two states which are bisimilar, then any action from either
state obtains the same reward and transitions to bisimulation equivalence classes with equal proba-
bilities. But a given policy may select actions diﬀerently between the two states so that the expected
rewards under the policy are diﬀerent between the states, and in particular the two states are not π-
bisimilar. On the other hand, two states may have diﬀerent dynamics across diﬀerent actions, and
hence not be bisimilar, but the policy can balance the actions such that the states are π-bisimilar.
Theπ-bisimilarity equivalence relation, like ordinary bisimulation, is sensitive to small changes in
the system parameters; so deﬁning a metric in place of a relation is the natural next step.
Theorem 28 (Castro (2020)) .For a policy π, deﬁneFπ:M(X)→M (X)as
Fπ(d)(x,y) =|rπ
x−rπ
y|+γW(d)(Pπ
x,Pπ
y).
ThenFπis a contraction in /bardbl·/bardbl∞with modulus γ, and hence admits a unique ﬁxed point dπ
∼, which
we deﬁne to be the π-bisimulation metric.
It is straightforward to see that the kernel of dπ
∼is an on-policy bisimulation relation, justifying its
name. Akin to bisimulation metrics, π-bisimulation metrics possess desirable continuity properties
when it comes to policyvalue functions.
32Published in Transactions on Machine Learning Research (06/2023)
Proposition 29 (Castro (2020)) .Letπbe any policy, then Vπis1-Lipschitz with respect to dπ
∼,
that is for any x,y∈X,
|Vπ(x)−Vπ(y)|≤dπ
∼(x,y).
C.3.1 Learning bisimulation metrics
While bisimulation metrics come from a rich theoretical background, they lack application in prac-
tice due to the diﬃculty of learning them in online settings, which is desirable for many represen-
tation learning purposes. If PandRwere known exactly, then Fπcan be repeatedly applied in a
dynamic programming fashion, and will converge as it is a contractive map. However, when Pand
Rare unknown and only samples are available, learning d∼
πbecomes troublesome, as estimates for
Ware generally biased and result in learning diﬀerent ﬁxed points (Ferns et al., 2006; Comanici
et al., 2012).
C.4 Deep bisimulation for control
Zhang et al. (2021) propose a method of learning π-bisimulation metrics in representation space.
They train a Gaussian dynamics model ˆPto approximateP, and learn an encoder φ:X →
Rd. They train φso that representation distances approximate π-bisimulation distances, and use
gradient descent to train
/bardblφ(x)−φ(y)/bardbl1≈|rπ
x−rπ
y|+γW2(/bardbl·/bardbl 2)(/hatwiderPπx,/hatwiderPπy).
The choice ofW2(/bardbl·/bardbl 2)and Gaussian transitions ˆPis for computational eﬃciency, as
W2(/bardbl·/bardbl 2)(N(µ1,Σ1),N(µ2,Σ2)) =/bardblµ1−µ2/bardbl2
2+/vextenddouble/vextenddouble/vextenddoubleΣ1/2
1−Σ1/2
2/vextenddouble/vextenddouble/vextenddouble2
F,
where/bardbl·/bardblFis the Frobenius norm. As noted by Kemertas & Aumentado-Armstrong (2021), this
computational advantage widened the theory-practice gap, as (i) it was not proven whether a π-
bisimulation existed when W2was used (this was since proven in Kemertas & Jepson (2022)), (ii)
theL1norm was used for representation distances but the L2norm was used for the base metric
ofW2, and (iii) a dynamics model was used instead of the ground truth dynamics.
C.5 Background on reproducing kernel Hilbert spaces
Webeginbyreviewingmathematicalbackgroundcoveringvectorspaces, reproducingkernelHilbert
spaces, the MMD, and its equivalence to the energy distance.
C.5.1 Hilbert spaces
A (real) normed space is a vector space Vwith a function/bardbl·/bardbl:V→R, which satisﬁes the following
for allx,y∈V,α∈R:
•/bardblx/bardbl≥0 Positivity
•/bardblx/bardbl= 0⇐⇒x= 0 Identity of indiscernibles
•/bardblαx/bardbl=|α|/bardblx/bardbl Absolute homogeneity with respect to scalar multiplication
•/bardblx+y/bardbl≤/bardblx/bardbl+/bardbly/bardbl Triangle inequality
33Published in Transactions on Machine Learning Research (06/2023)
A normed space is a stronger notion than a metric space, since a norm induces a metric dthrough
d(x,y) =/bardblx−y/bardbl. An inner product space is a stronger notion of a normed space, which is described
as a vector space Vwith a function/angbracketleft·,·/angbracketright:V×V→Rsuch that for all x,y,z∈V,α,β∈R:
•/angbracketleftx,y/angbracketright=/angbracketlefty,x/angbracketright Symmetry
•/angbracketleftx,αy +βz/angbracketright=α/angbracketleftx,y/angbracketright+β/angbracketleftx,z/angbracketright Linearity in the ﬁrst argument
•/angbracketleftx,x/angbracketright≥0and/angbracketleftx,x/angbracketright= 0⇐⇒x= 0 Positive deﬁniteness
An inner product induces a normed space through /bardblx/bardbl=/angbracketleftx,x/angbracketright1/2. If an inner product space
induces a normed space whose topology is complete, then the space (V,/angbracketleft·,·/angbracketright)is referred to as a
Hilbert space . A normed space whose topology is complete is referred to as a Banach space , and we
remark that Hilbert spaces are a proper subset of Banach spaces.
Hilbert spaces have many desirable properties, and one which will become important for the fol-
lowing theory is the Riesz representation theorem (Riesz, 1907). Given a Hilbert space V, a map
T:V→Ris linear if:
T(αx+βy) =αT(x) +βT(y)for allx,y∈V, α,β∈R.
Continuity is easy to verify for linear maps: a linear map Tis continuous if and only if Tis bounded,
meaning that there exists C∈Rsuch that
/bardblT(x)/bardbl≤C/bardblx/bardbl,for allx∈V.
The set of all continuous linear operators on Vis known as the dual space of V, and often referred
to asV⋆. The Riesz representation theorem states that if Vis a Hilbert space, then VandV⋆are
isometrically isomorphic. Equivalently, this means that for any linear operator T:V→R, there
exists a unique xT∈Vsuch that
/angbracketleftx,xT/angbracketright=T(x)for allx∈V.
The interested reader can refer to any text on functional analysis for further details, such as Rudin
(1974).
C.5.2 Reproducing kernel Hilbert spaces and the MMD
LetXbe a ﬁnite set and Hbe a Hilbert space of real functions on X. For a point x∈X, the
evaluation functional Lx:H→Ris deﬁned by
Lx(f) =f(x).
IfLxis a continuous functional for all x∈X, we say thatHis areproducing kernel Hilbert space
(RKHS) (Schölkopf et al., 2018; Aronszajn, 1950). Suppose His a reproducing kernel Hilbert space,
then for each x∈X,Lxis linear and continuous, and the Riesz representation theorem implies
that there exists a unique kx∈Hsuch that
Lx(f) =/angbracketleftf,kx/angbracketrightH.
Sincekx∈H, we can write
kx(y) =Ly(kx) =/angbracketleftkx,ky/angbracketrightH.
34Published in Transactions on Machine Learning Research (06/2023)
This is used to deﬁne the reproducing kernel kofHask(x,y) =/angbracketleftkx,ky/angbracketrightH. We will sometimes write
kHto emphasize the dependence of the kernel on the Hilbert space. One can note that the functions
kxandkyabove can be recovered as the kernel ﬁxed at a single point, that is kx=k(x,·)∈H, and
ky=k(y,·)∈H. This is where the reproducing property comes from, as we see that k‘reproduces’
itself:
k(x,y) =/angbracketleftk(x,·),k(y,·)/angbracketrightH.
In the previous paragraphs, we began with a Hilbert space of functions whose evaluation functional
was continuous and obtained a reproducing kernel for this space. On the other hand, in Section 2.3
we took the opposite direction: we began with a positive deﬁnite kernel on a set and constructed a
Hilbertspaceoffunctions, theequivalenceofthesetwoapproachesisknownasthe Moore-Aronszajn
theorem (Aronszajn, 1950).
Let us recall the deﬁnition of the MMD introduced earlier in the main text:
Deﬁnition 6 (Gretton et al. (2012)) .Letkbe a kernel onX, and Φ :P(X)→Hkbe as deﬁned
above. Then the Maximum Mean Discrepancy (MMD) is a pseudometric on P(X)deﬁned by
MMD (k)(µ,ν) =/bardblΦ(µ)−Φ(ν)/bardblHk.
The MMD can also be seen as arising from a lifting of kernels on Xonto kernels on P(X)(Guilbart,
1979). Given a kernel konX, deﬁneK(µ,ν)forµ,ν∈P(X)as
K(µ,ν) =/angbracketleftΦ(µ),Φ(ν)/angbracketrightHk=/integraldisplay
X×Xk(x,y)d(µ⊗ν)(x,y).
It is immediate that Kretains all properties of being a positive deﬁnite kernel as it arises from the
inner product/angbracketleft·,·/angbracketrightHk. The MMD can then be seen as the metric ρkonP(X). We remark that the
MMD with Kallows one to metrize P(P(X)), but we do not need this in this work.
One can also show that the MMD is an integral probability metric (Müller, 1997), since we can
show that
MMD (k)(µ,ν) = sup
f∈Hk:/bardblf/bardblHk≤1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Xfdµ−/integraldisplay
Xfdν/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
To see that this corresponds to the MMD as deﬁned above, one can write out
sup
f∈Hk:/bardblf/bardblHk≤1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Xfdµ−/integraldisplay
Xfdν/vextendsingle/vextendsingle/vextendsingle/vextendsingle= sup
f∈Hk:/bardblf/bardblHk≤1|/angbracketleftf,Φ(µ)/angbracketrightHk−/angbracketleftf,Φ(ν)/angbracketrightHk|
= sup
f∈Hk:/bardblf/bardblHk≤1|/angbracketleftf,Φ(µ)−Φ(ν)/angbracketrightHk|
=/bardblΦ(µ)−Φ(ν)/bardblHk,
where we used the following fact for general Hilbert spaces H:supx:/bardblx/bardblH≤1/angbracketleftx,y/angbracketrightH=/bardbly/bardblH, which
follows from the Cauchy-Schwarz inequality.
35Published in Transactions on Machine Learning Research (06/2023)
D Extra empirical results
020004000600080001000012000DQN ReturnsAirRaid
025005000750010000125001500017500Asterix
050100150200BreakoutOriginal + MICo + KSMe
025005000750010000125001500017500Phoenix
02000400060008000100001200014000Rainbow Returns
010000200003000040000
0255075100125150175200
01000020000300004000050000
025005000750010000125001500017500QR-DQN Returns
02000400060008000
01020304050607080
10002000300040005000
025005000750010000125001500017500IQN Returns
05000100001500020000
050100150200
0500010000150002000025000
0 25 50 75 100 125 150 175 200
Frames (x1M)02500500075001000012500150001750020000M-IQN Returns
0 25 50 75 100 125 150 175 200
Frames (x1M)01000020000300004000050000
0 25 50 75 100 125 150 175 200
Frames (x1M)050100150200250300
0 25 50 75 100 125 150 175 200
Frames (x1M)0100002000030000400005000060000
Figure3: ComparisonofaddingKSMeversusMICoonalltheDopamine(Castroetal.,2018)value-
based agents, on four representative games. Solid lines represent the average over 5 independent
runs, while the shaded areas represent 75% conﬁdence intervals.
36