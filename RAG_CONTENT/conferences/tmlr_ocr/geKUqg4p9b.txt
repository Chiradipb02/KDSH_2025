Under review as submission to TMLR
Bayesian Inference for Sequence Mixture Density Networks
using Bézier Curve Gaussian Processes
Anonymous authors
Paper under double-blind review
Abstract
Probabilistic models for sequential data are the basis for a variety of applications concerned
with processing timely ordered information. The predominant approach in this domain is
given by recurrent neural networks, implementing either a transformative approach (e.g.
Variational Autoencoders or Generative Adversarial Networks) or a regression-based ap-
proach, i.e. variations of Mixture Density networks (MDN). In this paper, we focus on the
N-MDNvariant, which parameterizes (mixtures of) probabilistic Bézier curves ( N-Curves)
for modeling stochastic processes. While MDNs are favorable in terms of computational cost
and stability, they generally fall behind contending approaches in terms of expressiveness
and flexibility. Towards this end, we present an approach for improving MDNs in this regard
by enabling full Bayesian inference on top of N-MDNs. For this, we show that N-Curves
are a special case of non-stationary Gaussian processes (denoted as N-GP) and then derive
corresponding mean and kernel functions for different modalities. Following this, we pro-
pose the use of the N-MDN as a data-dependent generator for N-GP prior distributions.
We show the advantages granted by this combined model in an application context, using
human trajectory prediction as an example.
1 Introduction
Models of sequential data play an integral role in a range of applications related to representation learning,
sequence synthesis and prediction. Thereby, with real-world data often being subject to noise and detection
or annotation errors, probabilistic models are favorable. These take uncertainty in the data into account
and provide an implicit or explicit representation of the underlying probability distribution.
The determination of such a probabilistic sequence model is commonly layed out as a learning problem,
learning a model of an unknown underlying stochastic process from given sample sequences, which are
assumed to be realizations of this process. Extending on this by focusing on (multi-step) sequence prediction
as an exemplary task, a machine learning model is tasked to predict a distribution over a given number
of subsequent sequence elements, given some input sequence. Common approaches are based on either
Gaussian Processes (Rasmussen & Williams, 2006) (e.g. Damianou & Lawrence (2013); Mattos et al. (2016))
or more prevalently on neural networks either using stochastic weights (Bayesian Neural Networks ( BNN)
(Bishop, 1995; Blundell et al., 2015; Gal & Ghahramani, 2016)), stochastic input (transformative1neural
models, e.g. (conditional) Variational Autoencoders ( cVAE, Kingma & Welling (2014); Sohn et al. (2015);
Bowman et al. (2016)) and (conditional) Generative Adversarial Networks ( cGAN, Goodfellow et al. (2014);
Mirza & Osindero (2014); Yu et al. (2017))) or parameterizing mixture distributions (Mixture Density
Networks ( MDN) (Bishop, 1994; Graves, 2013)), combined with recurrent neural networks for handling
sequential data in an autoregressive manner. In general, the neural models containing stochastic components
allow to directly sample from the modeled stochastic process. For this, these models typically require
computationally expensive Monte Carlo methods during training and inference. In contrast, MDN-based
models are deterministic and map a given input onto the parameters of a mixture distribution. While these
1Transformative approaches take samples of a simple probability distribution (e.g. unit Gaussian) and transform them into
a sample-based representation of a more complex probability distribution.
1Under review as submission to TMLR
models are generally more stable and less computationally expensive during training, Monte Carlo methods
are still required for multi-modal inference. Further, due to MDN-based models being simpler models, they
are potentially less expressive and flexible.
In order to tackle difficulties with multi-modal inference in MDN-based probabilistic sequence models, Hug
et al. (2020) proposed a variation of MDNs, which operate in the domain of parametric curves instead of
the data domain, allowing to infer multiple time steps in a single inference step. The model is built on a
probabilistic extension of Bézier curves ( N-Curves), which assume the control points to follow independent
Gaussian distributions, thus passing stochasticity to the curve points. Following this, the N-MDNgenerates
a sequence of Gaussian mixture probability distributions in terms of a mixture of N-Curves.
Extending on this approach, in this paper we enhance MDNs for probabilistic sequence modeling in terms
of flexibility and expressiveness by establishing a connection between N-MDNs and the Gaussian process
(GP) framework. Our basic idea revolves around employing the N-MDN for determining a data-dependent
GP prior based on N-Curves, which enables the manipulation of predictions generated by an N-MDN in a
post-hoc manner. To achieve this, we first show that the underlying N-Curves are a special case of Gaussian
processes. We denote this N-Curve – induced GP as N-GP in order to reduce ambiguity2. Following this,
we derive mean and kernel functions for the N-GP considering different cases, i.e. univariate, multivariate
and multi-modal. Ultimately, this allows for a more expressive and flexible probabilistic sequence model by
employingtheGPframeworkforenablingBayesianposteriorinference, withthebenefitsofaregression-based
neural network model for parameter estimation.
In our evaluation, we explore the advantages granted by our combined model from a practical perspective.
Following this, using human trajectory prediction as an exemplary sequence predicton task, we use our
model for manipulating predictions generated by the underlying N-MDN by calculating different posterior
distributions according to the induced N-GP. For the posterior distributions, we consider two use cases.
First, improving the overall prediction performance by calculating the posterior predictive distribution given
one or more observed trajectory points. Second, we explore the possibilities of updating the predictions
under the presence of new measurements within the predictive time horizon. In our approach, this does not
require any additional passes through the N-MDN.
To summarize, our main contributions are given by:
1. A proof for probabilistic Bézier curves ( N-Curves) being Gaussian processes.
2. The derivation of GP mean and covariance functions induced by N-Curves, covering the univariate,
multivariate and multi-modal cases.
3. A probabilistic sequence model, which combines the stability and low computational complexity of
N-Curve – based MDNs with the expressiveness and flexibility of Gaussian processes.
The paper is structured as follows. Section 2 gives a brief introduction to Gaussian processes, probabilistic
Bézier curves and Mixture Density Networks, providing the basis for this paper. In section 3, we provide
an equivalence proof for N-Curves and Gaussian processes and derive N-GP mean and covariance functions
for the univariate, multivariate and multi-modal case. In order to make the paper more self-contained and
easy to follow, the corresponding subsections for each case also include a problem description and a brief
overview of prior work relevant for the derivation of the respective N-GP mean and kernel functions in
addition to ourN-GP derivation itself. In section 4 we then establish a link between N-MDNs andN-GPs
and present use cases for the combined model we consider relevant in practice. Next, section 5 provides a
brief overview of other related areas of research, complementing the references given in section 3. Finally
section 6 investigates on the viability of our combined model and section 7 concludes the paper.
2We will use the term N-Curve exclusively to refer to the polynomial curve with variance in its control and curve points
andN-GP for referring to the actual Gaussian process, which can be calculated from an N-Curve.
2Under review as submission to TMLR
2 Preliminaries
2.1 Gaussian Processes
A Gaussian process (GP, Rasmussen & Williams (2006)) is a stochastic process {Xt}t∈Twith index set
T, where the joint distribution of stochastic variables Xtifor an arbitrary, finite subset {t1,...,tN}ofT
is a multivariate Gaussian distribution. Throughout this paper, the index will be interpreted as time.
The joint distribution is obtained using an explicit mean function m(t)and positive definite covariance
functionk(ti,tj) =cov(f(ti),f(tj)), commonly referred to as the kernel of the Gaussian process, and yields
a multivariate Gaussian prior probability distribution over function space. Commonly, m(t) = 0is assumed.
Given a collection of sample training points X∗of a function f(t), the posterior (predictive) distribution
p(X|X∗)over non-observed function values Xcan be obtained. As such, Gaussian processes provide a
well-established model for probabilistic sequence modeling.
2.2 Probabilistic Bézier Curves
ProbabilisticBézierCurves( N-Curves, Hugetal.(2020))areBéziercurves(Prautzschetal.,2002)definedby
(L+ 1)independent d-dimensional Gaussian control points P={P0,...,PL}withPl∼N(µl,Σl). Through
the curve construction function
Xt=BN(t,P) =N(µP(t),ΣP(t)) (1)
with
µP(t) =L/summationdisplay
l=0bl,L(t)µl (2)
ΣP(t) =L/summationdisplay
l=0(bl,L(t))2Σl, (3)
where
bl,L(t) =/parenleftbiggL
l/parenrightbigg
(1−t)L−ltl(4)
are the Bernstein polynomials (Lorentz, 2013), the stochasticity is passed from the control points to the curve
pointsXt∼N(µP(t),ΣP(t)), yielding a sequence of Gaussian distributions {Xt}t∈[0,1]along the underlying
Bézier curve. Thus, a stochastic process with index set T= [0,1]can be defined. For representing discrete
data, i.e. sequences of length N, a discrete subset of Tcan be employed for assigning sequence indices to
evenly distributed values in [0,1], yielding
TN=/braceleftbiggv
N−1|v∈{0,...,N−1}/bracerightbigg
={t1,...,tN}. (5)
2.3 Mixture Density Networks
A Mixture Density Network (MDN, Bishop (1994)) is a feed-forward neural network
Φ(v) = ({πk,µk,Σk}k∈{1,...,K}|v), (6)
that takes an input vector vand maps it onto the parameters of a d-dimensional, K-component Gaussian
mixture distribution. In order to ensure that the MDN generates a valid set of mixture parameters, the
partitioned network output
ˆY= (˜π1,...,˜πK,˜µ1,...,˜µK,˜σ1,...,˜σK,˜ρ1,...,˜ρK),
3Under review as submission to TMLR
with
˜πk∈R,˜µk∈Rd,˜σk∈Rdand ˜ρk∈Rd2−d
2
is further transformed to meet parameter value requirements, i.e.
πk=softmax (˜π1,...,˜πK)k,
such that/summationtext
kπk= 1and
µk=˜µk
σk,i=fσ(˜σk,i)>0∀i∈{1,...,d}
ρk,j=fρ(˜ρk,j)∈[−1,1]∀j∈/braceleftbigg
1,...,d2−d
2/bracerightbigg
.
Note that the covariance matrices Σkare calculated from the standard deviations and correlations in order
to ensure positive definiteness. Common choices for fσare given by the exponential function (Bishop, 1994),
a shifted version of the Exponential Linear Unit (Clevert et al., 2015; Guillaumes, 2017) and the softplus
function (Dugas et al., 2001; Glorot et al., 2011; Iso et al., 2017). For fρ, an alternative to the originally
proposed tanhfunction is given by the softsignfunction (Glorot & Bengio, 2010; Iso et al., 2017).
For building a probabilistic sequence model using MDNs, a common choice is the Sequence MDN model
as proposed by Graves (2013). Here, an MDN is stacked on top of an LSTM (Hochreiter & Schmidhuber,
1997) network. The recurrent structure is then used for encoding an input sequence as well as for generating
predictions one step at a time.
TheN-MDN variant: Hug et al. (2020) introduced an MDN variant for probabilistic sequence modeling,
which avoids the need for Monte Carlo methods when generating multi-modal distributions over sequences
by directly parameterizing mixtures of N-Curves, referred to as N-MDN. The model consists of two main
components, a recurrent neural network (an LSTM in this case) used as a sequence encoder and an MDN.
During training, the N-MDN learns to map a given input sequence onto an N-Curve (mixture) modeling a
distribution over target sequences, e.g. subsequent sequence elements in the case of sequence prediction. To
achieve this, theN-MDN’s LSTM network encodes the input into a vector representation v, which is then
passed into the MDN in order to map it onto the parameters of said N-Curve (mixture), i.e. the mixing
weights, as well as the mean vectors and covariance matrices of the Gaussian control points.
3N-Curve Gaussian Processes
WithN-Curves providing a representation for stochastic processes {Xt}t∈Tcomprised of Gaussian random
variablesXt∼N(µ,Σ), we first show that N-Curves are a special case of GPs with an implicit mean and
covariance function3. Following the definition of GPs (MacKay, 2003; Rasmussen & Williams, 2006), an
N-Curve can be classified as a GP, if for any finite subset {t1,...,tN}ofT, the joint probability density
p(Xt1,...,XtN)of corresponding random variables is Gaussian. This property is referred to as the GP
property. We show that this property holds by reformulating the curve construction formula (see Eq. 1) into
a linear transformation4X=C·Pof the Gaussian control points stacked into a ((L+ 1)·d×1)vector
P⊤=/parenleftig
P⊤
0P⊤
1···P⊤
L/parenrightig
(7)
using a (N·d×(L+ 1)·d)transformation matrix
C=
B0,L(t1)... BL,L(t1)
.........
B0,L(tN)...BL,L(tN)
(8)
3TheN-Curve representation does not require an explicit kernel or covariance function, as is the common representation
for Gaussian processes, because the correlation between points on the curve emerges from the geometric constraints of the
underlying Bézier curve. The curve function itself provides the mean function.
4For clarity, multivariate random variables may be written in bold font occasionally.
4Under review as submission to TMLR
determined by the Bernstein polynomials, with Bl,L(tj) =bl,L(tj)Idandtj∈TN. AsPis itself a Gaussian
random vector, Xand its corresponding probability density p(X) =p(X1,...,XN)are also Gaussian.
As the Gaussians along an N-Curve are correlated through the shared set of curve control points, the mean
and kernel functions of the induced GP, denoted as N-GP in the following, can be given explicitly. In the
following sections, we thus derive the N-GP for the univariate, multivariate and multi-modal case, with
respective mean and kernel functions.
3.1 Univariate N-Curve Gaussian Processes
We first consider univariate GPs, which target scalar-valued functions f:R→R, as these on provide the
most common use case for GPs and also grant us a simple case for deriving the mean and kernel functions
induced by a given N-Curve while also allowing a visual examination of some properties of the N-GP. Here,
the stochastic control points Plare defined by the mean value µland variance σ2
l. The mean function is
equivalent to Eq. 2. Thus, we focus on the kernel kP(ti,tj)for two curve points X=f(ti) =/summationtextL
l=0bl,L(ti)Pl
andY=f(tj) =/summationtextL
l=0bl,L(tj)Plat indicestiandtjwithti,tj∈[0,1]. The respective mean values are given
byµX=/summationtextL
l=0bl,L(ti)µlandµY=/summationtextL
l=0bl,L(tj)µl. Fromk(ti,tj) =cov(f(ti),f(tj))then follows:
kP(ti,tj) =E[(X−µX)(Y−µY)]
=E/bracketleftigg/parenleftiggL/summationdisplay
l=0bl,L(ti)Pl−µX/parenrightigg/parenleftiggL/summationdisplay
l=0bl,L(tj)Pl−µY/parenrightigg/bracketrightigg
=E/bracketleftiggL/summationdisplay
l=0bl,L(ti)bl,L(tj)P2
l/bracketrightigg
+E
L/summationdisplay
l=0
L/summationdisplay
l′=0,l′̸=lbl,L(ti)bl′,L(tj)PlPl′


−µYL/summationdisplay
l=0bl,L(ti)µl
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=µYµX−µXL/summationdisplay
l=0bl,L(tj)µl
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=µXµY+µXµY.
With E[Pi·Pj] =E[Pi]·E[Pj], which follows from the independence of the control points, and E[P2
i] =
Var[Pi] + (E[Pi])2, we obtain the closed-form solution
kP(ti,tj) =L/summationdisplay
l=0bl,L(ti)bl,L(tj)(σ2
l+µ2
l) +L/summationdisplay
l=0
L/summationdisplay
l′=0,l′̸=lbl,L(ti)bl′,L(tj)µlµl′
−µXµY,(9)
wherekP(ti,tj)is a positive semi-definite function5for anyt∗∈[0,1]and arbitrary sets of Gaussian control
points with mean values µland variances σl.
As theN-GP is strongly dependent on the given set of control points, it allows for a range of different
kernels. For a brief comparison, Fig. 1 illustrates two standard kernels (Görtler et al., 2019), given by a
radial basis function (RBF) kernel
krbf
σ,l(ti,tj) =σ2exp/parenleftbigg
−∥ti−tj∥2
2l2/parenrightbigg
, (10)
withσ= 1andl= 0.25and a linear kernel
klin
σ,σb,c(ti,tj) =σ2
b+σ2(ti−c)(tj−c), (11)
withσ=σb=c= 0.5, and twoN-GP kernels kP1(ti,tj)andkP2(ti,tj).P1consists of two unit Gaussians
andP2consistsof 9zeromeanGaussiancontrolpointswithstandarddeviations σ0=σ8= 1,σ1=σ7= 1.25,
σ2=σ6= 1.5,σ3=σ5= 1.75andσ4= 2. The standard deviations vary in order to cope with non-linear
blending (see Eq. 3). The Gram matrices calculated from 20equally spaced values in [0,1]are depicted for
each kernel.
5A proof for the positive semi-definiteness of the N-GP kernel is provided in appendix A.
5Under review as submission to TMLR
Figure 1: Gram matrices for 20equally spaced values in [0,1]obtained by using different GP kernels, showing
parallels between different N-GP kernels and the RBF and linear kernels. Left to right: RBF kernel, linear
kernel andN-GP kernels kP1(ti,tj)andkP2(ti,tj).
Figure 2: Samples drawn from prior distributions using different GP kernels. The 2σregion is depicted as
a red shaded area. Left to right: RBF kernel, linear kernel and N-GP kernels kP1(ti,tj)andkP2(ti,tj).
When comparing the Gram matrices, it can be seen, that the matrix calculated with kP1is equal to that
calculated with klin
σ=σb=c=0.5(ti,tj)when normalizing its values to [0,1]. On the other hand, the matrix
obtained with kP2, which is derived from a more complex N-Curve, tends to be more comparable to the
matrix calculated with krbf
σ=1,l=0.25(ti,tj). These parallels are also visible when comparing sample functions
drawn from each GP prior, assuming a zero mean GP using the different kernels as depicted in Fig. 2.
As a final note, the N-GP is non-stationary, as its kernel depends on the actual values of tiandtj. Further,
it is non-periodic and its smoothness is controlled by the underlying Bézier curve, i.e. the position and
number of control points.
3.2 Multivariate N-Curve Gaussian Processes
Multivariate GPs target vector-valued functions f(t), which map scalar inputs onto d-dimensional vectors,
e.g.f:R→Rd. Following this, for elevating our univariate N-GP derived in the previous section to
the multivariate case, there exist two closely related approaches we can adopt. The first sticks with the
multivariateGaussiandistributionandmodelsmatrix-valuedrandomvariablesbyusingstackedmeanvectors
in combination with block partitioned covariance matrices (Álvarez et al., 2012). The other revolves around
the matrix normal distribution (Chen et al., 2020a;b), which can be transformed into a multivariate Gaussian
distribution by vectorizing the mean matrix and calculating the covariance matrix as the Kronecker product
of both scale matrices, thus establishing a connection to the former.
We adopt the first approach, as it simplifies the extension of the univariate N-GP. Following this, the Gram
matrixofa d-variateGPforafiniteindexsubsetwith |TN|=Nisdefinedbythe (Nd×Nd)blockpartitioned
matrix
Σ =
KP(t1,t1)··· KP(t1,tN)
.........
KP(tN,t1)···KP(tN,tN)
(12)
6Under review as submission to TMLR
calculated using the matrix-valued kernel KP(ti,tj) =cov(X,Y). Here,X=/summationtextL
l=0bl,L(ti)PlandY=/summationtextL
l=0bl,L(tj)Plare nowd-variate Gaussian random variables resulting from the linear combination of d-
variateN-Curve control points Pl. Thus, the multivariate generalization of Eq. 9 yields a (d×d)matrix
and is given by
KP(ti,tj) =E[(X−µX)(Y−µY)⊤]
=E/bracketleftbig
XY⊤/bracketrightbig
−E/bracketleftbig
Xµ⊤
Y/bracketrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=µXµ⊤
Y−E/bracketleftbig
µXY⊤/bracketrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=µXµ⊤
Y+µXµ⊤
Y
=L/summationdisplay
l=0bl,L(ti)bl,L(tj)/parenleftbig
Σl+µlµ⊤
l/parenrightbig
+L/summationdisplay
l=0
L/summationdisplay
l′=0,l′̸=lbl,L(ti)bl′,L(tj)µlµ⊤
l′
−µXµ⊤
Y.(13)
The (n×d)mean vector is defined as the concatenation of all mean vectors µP(ti)(see also Eq. 2) along
the underlying Bézier curve, i.e.
(mP(TN))⊤=/parenleftig
(µP(t1))⊤··· (µP(tN))⊤/parenrightig
. (14)
3.3 Multi-modal N-Curve Gaussian Processes
Withsequencemodelingtasksoftenbeingmulti-modalproblemsandGPsaspresentedbeforebeingincapable
of modeling such data, we consider multi-modal GPs as a final case. A common approach for increasing the
expressiveness of a statistical model, e.g. for heteroscedasticity or multi-modality, is to employ a mixture
model. Thereby, rather than a single model or distribution, a mixture of which are used with each component
in the mixture covering a subset of the data. A widely used mixture model is given by the Gaussian mixture
model (Bishop, 2006), which is defined as a convex combination of KGaussian distributions with (mixing)
weightsπ={π1,...,πK}and probability density function
p(x) =K/summationdisplay
k=1p(z=k)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
πkp(x|z=k)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
N(µk,Σk), z∼Categorical (π).(15)
Transferred to GPs, a popular approach is given by the mixture of Gaussian process experts (Tresp, 2000;
Rasmussen & Ghahramani, 2001; Yuan & Neubauer, 2008), which extends on the mixture of experts model
(Jacobs et al., 1991). In this approach, the mixture model is defined in terms of KGPexpertsGkwith mean
function mkand kernel Kk
K/summationdisplay
k=1p(z=k|x)Gk(mk(·),Kk(·,·)), (16)
using a conditional weight distribution Categorical (π|x)for a given sample x. The weight distribution is
generated by a gating network , which decides on the influence of each local expertfor modeling a given
sample. This is the key difference to the Gaussian mixture model, where the weight distribution is static
and determined a priori (e.g. via EM (Dempster et al., 1977) or an MDN (Bishop, 1994)). It can be noted
that mixtures of experts are also often used to lower the computational load of a GP model, as less data
points have to be considered during inference due to the use of local experts (e.g. Deisenroth & Ng (2015);
Lederer et al. (2021)).
In line with the approach described by Hug et al. (2020), which builds on Gaussian mixture models, we
define the multi-modal extension of our N-GP as a mixture of KN-GPs
MG(π,{Gk}k∈{1,...,K}) =K/summationdisplay
k=1πkGk(mPk(·),KPk(·,·)), (17)
7Under review as submission to TMLR
withN-GP components Gkand the static prior weight distribution π={π1,...πK}with/summationtextK
k=1πk= 1. The
mean and kernel functions are determined separately for each GP component according to equations 2 and
9 (unimodal case), or 13 and 14 (multi-modal case). Given these functions and the weights π, the mixture
distribution can be evaluated at a given index.
4 Bayesian Inference for N-Curve Mixture Density Networks
In order to enable GP-based Bayesian inference on top of the regression-based N-MDN model, we propose
to use theN-MDN as a data-dependent generator for prior distributions within the GP framework, where
theN-MDN is adapted to a given dataset through training6. Given an input sequence, we achieve this
by calculating an N-GP’s mean and kernel functions from each corresponding N-Curve output by the
N-MDN, yielding a prior for a GP modeling a respective target sequence. Despite the N-MDN being
a competetive7probabilistic sequence model by itself, the combination of both approaches yields mutual
advantages by providing a convenient and stable way of estimating the N-GP’s parameters and enhancing
theN-MDN’s flexibility and expressiveness in an post-hoc manner by creating the possibility of calculating
different posterior predictive distributions from the N-MDN’s output in light of (new) observations via the
inducedN-GP.
For conciseness, in the following sections we focus on the task of (probabilistic) sequence prediction, where
given a length Nininput sequence ( observation ), the subsequent Npredsequence elements need to be pre-
dicted. The length of each sequence is referred to as the observable and predictive time horizon, respectively.
First, we discuss possible real-world use cases of the combined approach (section 4.1). Afterwards, we briefly
summarize how the N-MDN network is trained (section 4.2) and how posterior predictive distributions cor-
responding to the discussed use cases are calculated using our proposed combination of N-MDNs andN-GPs
(section 4.3).
4.1 Practical Implications and Use cases
To put the value of our combined model into perspective from a practical standpoint, we discuss different
use cases that we expect to benefit from the N-GP extension, namely prediction refinement andupdate.
Refinement: We first examine the N-GP as a tool for improving the overall prediction performance
considering different posterior predictive distributions given different subsets of the input sequence. More
formally, given an input sequence {x1,...,xNin}, which is processed by the N-MDN yielding the N-GP’s prior
distribution p(X1,...,XNin+Npred), a refinement of this initial prediction is given by a posterior predictive
distribution
p({X1,...,XNin+Npred}\{Xi}i∈I|{xi}i∈I) (18)
withI ⊆{ 1,...,N in}giving a subset of the input sequence. We expect this refinement to improve the
prediction performance, as the original maximum likelihood prediction generated by the N-MDN tends to
average out small variations in the data and the refinement procedure adapts the prediction more towards
the actual observation in a controlled way. We consider this the most practically relevant use case, as it
directly affects the model’s accuracy.
Update: Another interesting option opened up by embedding the N-MDN into the GP framework is given
by the use case of updating a multi-step prediction under the presence of new data within the predictive
time horizon. For this, we calculate a posterior predictive distribution similar to Eq. 18 (refinement), but
condition on a new observation within the predictive time horizon instead, i.e. I⊆{Nin+1,...,N in+Npred}.
As sequence models usually predict several time steps into the future, an easy to calculate and fast to
compute update to the prediction under the presence of new data can be valuable. The N-GP enables
such updates without the need for additional passes through the underlying neural network. Further, it is
6Training the N-MDN does not involve the N-GP component.
7A comparison with other probabilistic sequence models commonly used in the context of human trajectory prediction is
given in appendix B.
8Under review as submission to TMLR
Figure 3: Schematic of N-MDN training for sequences of length N= 6, withNin= 3andNpred= 3.
unaffected by potentially missing intermediate observations. This is especially valuable, as common sequence
prediction models require complete sequences as input, where such gaps need to be filled with data extracted
from the model’s own prediction. This can be problematic under the presence of multiple modes in the
predicted distribution, making Monte Carlo methods a necessity. In contrast, within the GP framework
missing information between observed data points is naturally interpolated. Following this, when certain
requirements are met, our model allows to fill gaps in light of fragmented observations easily. First, a full
Nin-step input sequence is required for the underlying N-MDN for generating our prior distribution. Second,
the gaps to fill must be within the modeled predictive time horizon.
4.2 Parameter Estimation
In our approach, the N-GP relies on prior distributions generated by an N-MDN, whose parameters are
learned from data. Following Hug et al. (2020), the N-MDN is trained using a set of Mfixed-length
trajectoriesD={X1,...,XM}withXi={xi
1,...,xi
N}, applying the negative log-likelihood loss
L=1
MM/summationdisplay
j=1−logK/summationdisplay
k=1exp/braceleftig
logπk+N/summationdisplay
i=1log/parenleftig
N(xj
i|µP(ti),ΣP(ti))/parenrightig/bracerightig
(19)
in conjunction with a gradient descent policy.
In order to give an illustrative example considering a sequence prediction task, training sequences of length
Nare split into an observed portion of length Nin, which is used as input for the model, and a to be predicted
portion of length Npred, whereN=Nin+Npred. TheN-MDN then outputs an N-Curve (mixture), which
models full sequences of length N. This ensures that within the GP framework, we are able to condition on
elements within the input sequence. The negative log-likelihood loss function is calculated given full training
sequences. A schematic of this is depicted in Fig. 3.
4.3 Conditional Inference
Finally, we want to give a brief overview of how (conditional) inference works within our combined model,
especially with regard to the refinement and update use cases discussed in section 4.1. Given an input
sequence, theN-MDN generates an N-Curve mixture, which models the input as well as possible future
sequences. Using Equations 13, 14 and 17, we calculate the N-GP priorp(X1,...,XN)from this mixture,
9Under review as submission to TMLR
which is a joint K-component Gaussian mixture distribution over all Nmodeled time steps. Now, for
determining a conditional posterior predictive distribution, we first partition the joint prior distribution into
a partition containing the Nintime steps to condition on and the remaining Npredtime steps
p(X1,...,XN) =p({X1,...,XNin}∪{XNin+1,...,XN})
=p(XA∪XB),(20)
with component mean vectors and covariance matrices partitioned as
µk=/bracketleftigg
µkA
µkB/bracketrightigg
,Σk=/bracketleftigg
ΣkAA ΣkAB
ΣkBA ΣkBB/bracketrightigg
.
Following e.g. (Bishop, 2006; Petersen & Pedersen, 2008; Murphy, 2022), the posterior weights, mean vectors
and covariance matrices, conditioned on xB, can then be directly calculated as
πkA|B=πkN(xB|µkB,ΣkBB)/summationtext
jπjN(xB|µjB,ΣjBB)
µkA|B=µkA+ΣkABΣ−1
kBB(xB−µkB)
ΣkA|B=ΣkAA−ΣkABΣ−1
kBBΣkBA.(21)
The probability distribution for individual trajectory points at time tcan be extracted through marginaliza-
tion usingXA={Xt}, e.g.
p(XA) =/summationdisplay
kπkpk(XA) =/summationdisplay
kπkN(xA|µkAA,ΣkAA).
A schematic of the inference scheme for the refinement and update use cases is provided in Fig. 4. In this ex-
ample, we assume the N-GP to model 6-element sequences with prior distribution p(X1,X2,X3,X4,X5,X6),
wherep(·)was calculated from the N-Curve (mixture) output by an N-MDN given an input sequence
{x1,x2,x3}. As we are now within the GP framework, we can calculate different posterior predictive distri-
butionsusingobservedvaluesfromtheinputsequence(refinementusecase)orvaluesobservedaftertheinitial
input (update use case). Exemplary posterior distributions can then be given by p(X1,X2,X4,X5,X6|x3)
orp(X1,X2,X4,X6|x3,x5)for the refinement and update use cases, respectively.
5 Related Work
Finally, we want to summarize our contributions over Hug et al. (2020) and then give a brief overview
of additional related areas of research our work tangents, complementing the references given throughout
section 3.
Comparison with (Hug et al., 2020): The main contributions provided by Hug et al. (2020) are given
by the concept of N-Curves as a representation of a probability distribution over sequences and the N-MDN
neural network as a probabilistic sequence model using those curves as their output. In this paper, we
take the concept of N-Curves and extend the theory surrounding them by establishing a link to Gaussian
processes, providing a proof of equivalence, and deriving the mean and kernel functions for a GP induced
by a givenN-Curve (theN-GP) for the univariate, multivariate and multi-modal cases. We further extend
their sequence prediction model by re-purposing the N-MDN as a data-dependent generator for N-GP prior
distributions, with the goal of enhancing the N-MDN in terms of flexibility and expressiveness.
Multivariate GPs: A range of multivariate GPs (also referred to as multi-output GPs) are summarized
underthe Mixing Model Hierarchy (Bruinsmaetal.,2020), whichclassifiesGPmodelsbylow-rankcovariance
structure. These GPs are specializations of a Linear Mixing Model , which is defined in terms of a diagonal
multi-output kernel and a mixing matrix. Apart from these GPs, models such as (Salimbeni & Deisenroth,
10Under review as submission to TMLR
Figure 4: Schematic of the interplay between the N-MDN andN-GP components with regard to the refine-
ment and update use cases. Green circles indicate trajectory points the predictive posterior is conditioned
on.
2017) build on deep GPs (Damianou & Lawrence, 2013), which are hierarchies of multiple GPs with non-
linear mappings between layers. These approaches commonly decompose the multi-output GP problem into
a set of multiple independent single output GP problems via diagonal multivariate kernels. In contrast, in
our approach the output dimensionality is tied to the N-Curve control point’s dimensionality. Eq. 13 then
yields a (non-diagonal) multivariate kernel, where correlations between dimensions are constrained by the
shape of the underlying Bézier curve.
Learning data-dependent priors: Different approaches that combine kernel methods with deep learning
have been proposed, such as Neural Kernel Networks (Sun et al., 2018) and Deep Kernel Learning (Wilson
et al., 2016; Ober et al., 2021). Neural Kernel Networks learn a kernel from data as a composition of primitive
kernels, whereas Deep Kernel Learning uses a neural network to map inputs into an intermediate feature
space, which then serves as the input space for a GP. Opposed to that, we are using a neural network for
mapping inputs onto the parameters of an N-Curve (mixture), from which conditional prior distributions
can be derived directly.
Bernstein basis: The Bernstein polynomials have been used previously for approximating prior distri-
butions. Petrone (1999b) proposes the use of Bernstein polynomials for approximating a dirichlet process
to define prior distributions and MCMC based posterior approximation. These priors are then for example
applied for Bayesian density estimation (Petrone, 1999a). Besides that, Jørgensen & Osborne (2022) pro-
posed an approach building on the same core idea8of deriving a GP from probabilistic Bézier curves, or
Bézier surfaces in their case. However, their work puts strong emphasis on modeling spatial input, while
our approach focuses on probabilistic (multi-modal) sequence modeling. Apart from that, one of their main
concerns is given by scalability, which is a frequent issue with Gaussian processes. We, on the other hand,
rather focus on the derivation of mean and kernel functions for different use-cases. Thereby, we also provide
an equivalence proof for N-Curves being Gaussian processes, explore some properties of the N-GP and show
surface-level comparisons to other Gaussian process kernels. Finally, on a methodological level, they employ
8Approach developed concurrently to ours.
11Under review as submission to TMLR
an approach based on variational inference for parameter estimation, while we are relying on regression-based
MDNs, thus providing an alternative learning strategy.
6 Evaluation
In this section we aim to support our proposed combination of N-MDNs withN-GPs in its capabilities of
enhancing theN-MDN’s flexibility, expressiveness and overall performance, considering the refinement and
updateuse cases in the scope of an established sequence processing task, i.e. (human) trajectory prediction,
using standard benchmarking datasets. This task provides easy to visualize results while also providing
sufficientofcomplexitybeingahighlymulti-modalproblem, despiteitslowdatadimensionality. Intrajectory
prediction, given Ninpoints of a trajectory as input, a sequence model is tasked to predict the subsequent
Npredtrajectory points. Here, the N-MDN models both, the observed and to be predicted trajectory, in
order to enable GP-based inference. It should be noted, that we omit a state-of-the-art comparison of the
baseN-MDN. The reasons for this are two-fold. On the one hand, the model has been proven viable and
competetive on the human trajectory prediction task (Hug et al., 2020). On the other hand, this allows
us to keep the evaluation focused on our combined model and the use cases discussed in section 4.1. For
these reasons, we refer the reader to appendix B for additional comparisons of the base N-MDN with other
probabilistic neural sequence models relevant in the context of human trajectory prediction.
6.1 Experimental Setup
For the evaluation, we consider scenes from commonly used datasets: BIWI Walking Pedestrians (Pellegrini
et al. (2009), scenes: ETH and Hotel), Crowds by Example (Lerner et al. (2007), scenes: Zara1 and Zara2)
and theStanford Drone Dataset (Robicquet et al. (2016), scenes: Bookstore and Hyang). Following common
practice,theannotationrateofeachdatasetisadjustedto 2.5annotationspersecond. Further,theevaluation
is conducted on trajectories of fixed length N=Nin+Npred= 8+12. We trained oneN-MDN independently
for each dataset to generate N-Curve mixtures, which model complete trajectories of length N= 20, given
the firstNin= 8trajectory points as input to the model. For training, 80%of each dataset are used.
For a quantitative and qualitative evaluation, we are first considering the refinement use case. For each test
dataset, the respective N-MDN is used to generate an N-Curve mixture prediction for every test trajectory,
yielding initial predictions p(X1,...,XN), i.e. theN-GP prior. These initial predictions are then refined by
calculating the posterior distributions
p({X1,...,XN}\{XNin}|xNin)
and
p({X1,...,XN}\{X4,XNin}|x4,xNin),
conditioning on the input’s last point xNin(refinement A ) and on{x4,xNin}(refinement B ), respectively.
For more details on how the posterior distributions are obtained, we want to refer the reader to section 4.3.
By increasing the number of observed points, we expect the prediction to adapt towards a given sample
trajectory. For measuring the performance, we apply the Average Displacement Error (ADE, Kothari et al.
(2021)) according to the standard evaluation approach, using a maximum likelihood estimate9. As the ADE
does not provide an adequate measure for assessing the quality of (multi-modal) probabilistic predictions, we
use theNegative Log-Likelihood (NLL) in addition to the ADE, which is a common choice for this purpose
(Bhattacharyya et al., 2018; Ivanovic & Pavone, 2019). More detailed information on the performance
measures is provided in appendix B.2. Both the ADE and the NLL are calculated for the estimate of the
observed trajectory, as well as for the time steps to be predicted. For example for refinement B, the marginal
distributions
pobs({X1,...,XNin}\{X4,XNin}) =/integraldisplay
p({X1,...,XN}\{X4,XNin}|x4,xNin)d{XNin+1,...,XN}
9We are using the mean vectors of the marginalized posterior Gaussian mixture distributions for each time step to be
considered.
12Under review as submission to TMLR
Initial Prediction Refinement A Refinement B
ETHADE 3.85 / 11.25 3.95 / 10.12 2.39 / 10.18
NLL 6.51 / 7.58 5.43 / 7.08 -115.09 / 1.70
HotelADE 5.69 / 17.96 4.19 / 17.07 2.70 / 16.73
NLL 6.99 / 8.20 5.56 / 7.71 10.63 / 8.59
Zara1ADE 4.09 / 19.10 2.89 / 17.52 1.63 / 17.64
NLL 6.83 / 8.18 5.27 / 7.63 -51.93 / 8.15
Zara2ADE 2.98 / 21.38 2.64 / 20.07 1.69 / 20.05
NLL 6.59 / 8.09 5.08 / 7.59 -60.95 / -1.76
BookstoreADE 4.04 / 17.21 3.65 / 15.97 2.16 / 16.29
NLL 7.46 / 8.37 5.88 / 7.76 -11.89 / 7.63
HyangADE 5.51 / 36.46 5.01 / 34.05 3.16 / 32.18
NLL 8.21 / 9.42 6.65 / 8.86 -49.30 / 9.05
Table 1: Quantitative results of the initial (prior) prediction as generated by an N-MDN and posterior
refinements. Table entries report the estimation error with respect to the input trajectory ( pobs, first value)
and the trajectory to be predicted ( ppred, second value), respectively. ADE errors are reported in pixels.
Lower is better for both performance measures.
and
ppred(XNin+1,...,XN) =/integraldisplay
p({X1,...,XN}\{X4,XNin}|x4,xNin)d{X1,...,XNin}\{X4,XNin}
are evaluated. As these posteriors are Gaussian mixture distributions, these marginals can analytically be
calculated (see section 4.3). Beyond the refinement use case, we are providing a qualitative example for the
update use case. Here, the posterior distribution p({X1,...,XN}\{XNin,X14}|xNin,x14)is considered.
6.2 Results
The quantitative results for the N-GP –based predictionrefinement withrespect tothe selectedperformance
measures are depicted in Table 1. Overall, an increase in performance can be observed when refining the
estimate generated by the N-MDN using 1and2points, respectively. This supports our expectation of an
increase in prediction performance through adding GP-based Bayesian inference to the N-MDN. Despite
this, however, the results indicate an inconsistent gain in performance comparing both posteriors. Following
this, we provide qualitative examples to further investigate these findings.
Two examples highlighting commoncases fora positiveeffect of therefinement onthe predictionperformance
is given in Fig. 5. On the one hand, the refinement can lead to the estimate being pulled closer to the ground
truth in the input portion, which expands far into the future prediction. On the other hand, the refinement
can lead to the suppression of inadequate mixture components, that were assigned high weights in the prior
distribution.
In some cases it can be observed, that conditioning on 2points (refinement B) degrades the performance in
comparison to using a single point (refinement A). With respect to the NLL, this can be attributed to an
increased number of trajectory point variances decreasing or even collapsing. Then, even minor inaccuracies
in the mean prediction result in higher NLL values, even if the estimate is closer to the ground truth.
Looking at the ADE, the loss in performance can most likely be attributed to the enforced interpolation of
the condition points, which sometimes leads to unwanted deformations of the mean prediction. One of the
main causes for this is given by the input trajectories commonly being subject to noise. Examples for both
of these cases are depicted in Fig. 6. It could be noted, that a common approach for dealing with such
problems is given by adding an error term to each observed point (Görtler et al., 2019).
Lastly, we briefly showcase our approach considering the prediction update use case. An example for the
posterior distribution given an additional observation within the predictive time horizon is depicted in Fig.
7. While there are initially multiple relevant mixture components (according to their weights), the additional
13Under review as submission to TMLR
Figure 5: Examples for improved prediction through conditioning on 1or2points, respectively. Left to right:
Initial (prior) prediction, refinement A and refinement B. Yellow squares indicate condition points. Ground
truth trajectories are depicted in black and start at the bottom (top row) and in the top left (bottom row).
The first 8trajectory points are input into the N-MDN.
Figure 6: Cases for refinement leading to degraded prediction performance according to the NLL (1 & 2)
and ADE (3 & 4). 1 & 3: refinement A, 2 & 4: refinement B. Yellow squares indicate condition points.
Ground truth trajectories are depicted in black and start at the top (left example) and on the right site
(right example). The first 8trajectory points are input into the N-MDN.
observation leads to the suppression of wrong modes. Here, the new observation occurs several time steps
after the last original input. Using the N-GP, the updated prediction can be directly calculated without
requiring an additional pass through the N-MDN.
7 Summary
In this paper, we presented an approach for enabling full Bayesian inference without the need for Monte
Carlo methods on top of the N-Curve Mixture Density Network ( N-MDN), which is a regression-based
probabilistic sequence model and outputs (mixtures of) probabilistic Bézier curves ( N-Curves). In our
approach, theN-MDN is embedded in the GP framework as a generator for prior distributions. For this, we
14Under review as submission to TMLR
Figure 7: Example for updating the prediction by an N-MDN using the last observed point and an additional
point within the predictive time horizon. Yellow squares indicate condition points. Ground truth is depicted
in black.
first showed that N-Curves are a special case of Gaussian processes (denoted as N-GP) and then derived
mean and kernel functions for the univariate, multivariate and multi-modal cases. In our evaluation on the
task of human trajectory prediction, we showed that using the N-GP, predictions generated by an N-MDN
canbeimprovedbyconditioningondifferentsubsetsoftheoriginalinput. Additionally, welookedbrieflyinto
practical applications of the approach, focusing on updating predictions generated by the N-MDN in light of
new observations within the predictive time horizon. Using our approach, such updates do not require any
additional passes through the N-MDN. Further, missing intermediate observations are inherently handled
by theN-GP.
References
Mauricio A. Álvarez, Lorenzo Rosasco, and Neil D. Lawrence. Kernels for vector-valued functions: A review.
Found. Trends Mach. Learn. , 4(3):195–266, mar 2012. ISSN 1935-8237. doi: 10.1561/2200000036. URL
https://doi.org/10.1561/2200000036 .
Apratim Bhattacharyya, Bernt Schiele, and Mario Fritz. Accurate and diverse sampling of sequences based
on a “best of many” sample objective. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pp. 8485–8493, 2018.
Christopher M Bishop. Mixture density networks. 1994.
Christopher M Bishop. Neural networks for pattern recognition . Oxford university press, 1995.
Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics) .
Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2006. ISBN 0387310738.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural
network. In International Conference on Machine Learning , pp. 1613–1622. PMLR, 2015.
SamuelR.Bowman, LukeVilnis, OriolVinyals, AndrewDai, RafalJozefowicz, andSamyBengio. Generating
sentences from a continuous space. In Proceedings of The 20th SIGNLL Conference on Computational
Natural Language Learning , pp. 10–21, Berlin, Germany, August 2016. Association for Computational
Linguistics. doi: 10.18653/v1/K16-1002. URL https://aclanthology.org/K16-1002 .
Wessel Bruinsma, Eric Perim, William Tebbutt, Scott Hosking, Arno Solin, and Richard Turner. Scalable
exact inference in multi-output Gaussian processes. In Hal Daumé III and Aarti Singh (eds.), Proceedings
of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learn-
ing Research , pp. 1190–1201. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/
bruinsma20a.html .
Zexun Chen, Jun Fan, and Kuo Wang. Remarks on multivariate gaussian process. arXiv preprint
arXiv:2010.09830 , 2020a.
15Under review as submission to TMLR
Zexun Chen, Bo Wang, and Alexander N Gorban. Multivariate gaussian and student-t process regression
for multi-output prediction. Neural Computing and Applications , 32(8):3005–3028, 2020b.
Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning
by exponential linear units (elus). arXiv preprint arXiv:1511.07289 , 2015.
Andreas Damianou and Neil Lawrence. Deep gaussian processes. In Artificial Intelligence and Statistics , pp.
207–215, 2013.
Marc Deisenroth and Jun Wei Ng. Distributed gaussian processes. In International Conference on Machine
Learning , pp. 1481–1490. PMLR, 2015.
Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Society: Series B (Methodological) , 39(1):1–22, 1977.
Charles Dugas, Yoshua Bengio, François Bélisle, Claude Nadeau, and René Garcia. Incorporating second-
order functional knowledge for better option pricing. Advances in neural information processing systems ,
pp. 472–478, 2001.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty
in deep learning. In international conference on machine learning , pp. 1050–1059, 2016.
Francesco Giuliari, Irtiza Hasan, Marco Cristani, and Fabio Galasso. Transformer networks for trajectory
forecasting. In 2020 25th International Conference on Pattern Recognition (ICPR) , pp. 10335–10342.
IEEE, 2021.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks.
InProceedings of the thirteenth international conference on artificial intelligence and statistics , pp. 249–
256. JMLR Workshop and Conference Proceedings, 2010.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Proceedings
of the fourteenth international conference on artificial intelligence and statistics , pp. 315–323. JMLR
Workshop and Conference Proceedings, 2011.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Proceedings of the 27th International
Conference on Neural Information Processing Systems - Volume 2 , NIPS’14, pp. 2672–2680, Cambridge,
MA, USA, 2014. MIT Press.
Jochen Görtler, Rebecca Kehlbeck, and Oliver Deussen. A visual exploration of gaussian processes. Distill,
4(4):e17, 2019.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 , 2013.
Axel Brando Guillaumes. Mixture density networks for distribution and uncertainty estimation, 2017.
Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially ac-
ceptable trajectories with generative adversarial networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , pp. 2255–2264, 2018.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining,
inference, and prediction . Springer Science & Business Media, 2009.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–1780,
1997.
R. Hug, S. Becker, W. Hübner, and M. Arens. Particle-based pedestrian path prediction using LSTM-MDL
models. In 21st International Conference on Intelligent Transportation Systems (ITSC) , pp. 2684–2691,
2018. doi: 10.1109/ITSC.2018.8569478.
16Under review as submission to TMLR
RonnyHug, WolfgangHübner, andMichaelArens. Introducingprobabilisticbéziercurvesforn-stepsequence
prediction. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pp. 10162–10169,
2020.
HayateIso, ShokoWakamiya, andEijiAramaki. Densityestimationforgeolocationviaconvolutionalmixture
density network. arXiv preprint arXiv:1705.02750 , 2017.
Boris Ivanovic and Marco Pavone. The trajectron: Probabilistic multi-agent trajectory modeling with dy-
namic spatiotemporal graphs. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 2375–2384, 2019.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local
experts. Neural computation , 3(1):79–87, 1991.
Martin Jørgensen and Michael A Osborne. Bézier gaussian processes for tall and wide data. In Advances in
Neural Information Processing Systems , 2022.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International Conference
on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Pro-
ceedings, 2014. URL http://arxiv.org/abs/1312.6114 .
Parth Kothari, Sven Kreiss, and Alexandre Alahi. Human trajectory forecasting in crowds: A deep learning
perspective. IEEE Transactions on Intelligent Transportation Systems , 2021.
Armin Lederer, Alejandro J Ordóñez Conejo, Korbinian A Maier, Wenxin Xiao, Jonas Umlauft, and San-
dra Hirche. Gaussian process-based real-time learning for safety critical applications. In International
Conference on Machine Learning , pp. 6055–6064. PMLR, 2021.
Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B Choy, Philip HS Torr, and Manmohan Chan-
draker. Desire: Distant future prediction in dynamic scenes with interacting agents. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition , pp. 336–345, 2017.
Alon Lerner, Yiorgos Chrysanthou, and Dani Lischinski. Crowds by example. In Computer graphics forum ,
volume 26, pp. 655–664. Wiley Online Library, 2007.
George G Lorentz. Bernstein polynomials . American Mathematical Soc., 2013.
David JC MacKay. Information theory, inference and learning algorithms . Cambridge university press, 2003.
César Lincoln C. Mattos, Zhenwen Dai, Andreas C. Damianou, Jeremy Forth, Guilherme A. Barreto, and
Neil D. Lawrence. Recurrent gaussian processes. In Yoshua Bengio and Yann LeCun (eds.), 4th Inter-
national Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
Conference Track Proceedings , 2016.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784 ,
2014.
Kevin P. Murphy. Probabilistic Machine Learning: An introduction . MIT Press, 2022. URL probml.ai .
Sebastian W Ober, Carl E Rasmussen, and Mark van der Wilk. The promises and pitfalls of deep kernel
learning. In Uncertainty in Artificial Intelligence , pp. 1206–1216. PMLR, 2021.
Ehsan Pajouheshgar and Christoph H Lampert. Back to square one: probabilistic trajectory forecasting
without bells and whistles. Modeling and decision-making in the spatiotemporal domain (NeurIPS 2018
Workshop) , 2018.
Stefano Pellegrini, Andreas Ess, Konrad Schindler, and Luc Van Gool. You’ll never walk alone: Modeling
socialbehaviorformulti-targettracking. In 2009 IEEE 12th International Conference on Computer Vision ,
pp. 261–268. IEEE, 2009.
17Under review as submission to TMLR
Kaare Brandt Petersen and Michael Syskind Pedersen. The matrix cookbook. Technical University of
Denmark , 7(15):510, 2008.
Sonia Petrone. Bayesian density estimation using bernstein polynomials. Canadian Journal of Statistics , 27
(1):105–126, 1999a.
Sonia Petrone. Random bernstein polynomials. Scandinavian Journal of Statistics , 26(3):373–393, 1999b.
Hartmut Prautzsch, Wolfgang Boehm, and Marco Paluszny. Bézier and B-spline techniques . Springer Science
& Business Media, 2002.
Carl Rasmussen and Zoubin Ghahramani. Infinite mixtures of gaussian process experts. Advances in neural
information processing systems , 14, 2001.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning . Adaptive
computation and machine learning. MIT Press, 2006. ISBN 026218253X.
Alexandre Robicquet, Amir Sadeghian, Alexandre Alahi, and Silvio Savarese. Learning social etiquette:
Human trajectory understanding in crowded scenes. In European conference on computer vision , pp.
549–565. Springer, 2016.
Andrey Rudenko, Luigi Palmieri, Michael Herman, Kris M Kitani, Dariu M Gavrila, and Kai O Arras.
Human motion trajectory prediction: A survey. The International Journal of Robotics Research , 39(8):
895–935, 2020.
Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep gaussian processes.
Advances in neural information processing systems , 30, 2017.
Christoph Schöller, Vincent Aravantinos, Florian Lay, and Alois Knoll. What the constant velocity model
can teach us about pedestrian motion prediction. IEEE Robotics and Automation Letters , 5(2):1696–1703,
2020.
David W. Scott. Kernel Density Estimation , pp. 1–7. American Cancer Society, 2018. ISBN 9781118445112.
doi: https://doi.org/10.1002/9781118445112.stat07186.pub2. URL https://onlinelibrary.wiley.com/
doi/abs/10.1002/9781118445112.stat07186.pub2 .
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep condi-
tional generative models. Advances in neural information processing systems , 28:3483–3491, 2015.
Shengyang Sun, Guodong Zhang, Chaoqi Wang, Wenyuan Zeng, Jiaman Li, and Roger Grosse. Differentiable
compositional kernel learning for gaussian processes. In International Conference on Machine Learning ,
pp. 4828–4837. PMLR, 2018.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In
Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2 ,
NIPS’14, pp. 3104–3112, Cambridge, MA, USA, 2014. MIT Press.
Volker Tresp. Mixtures of gaussian processes. Advances in neural information processing systems , 13, 2000.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems ,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In
Artificial intelligence and statistics , pp. 370–378. PMLR, 2016.
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with
policy gradient. In Proceedings of the AAAI conference on artificial intelligence , volume 31, 2017.
Chao Yuan and Claus Neubauer. Variational mixture of gaussian process experts. Advances in neural
information processing systems , 21, 2008.
18Under review as submission to TMLR
A Positive Semi-definiteness of the N−GPKernel
In this section we provide proof, that the N-GP’s kernel function is positive semi-definite, i.e. kP(ti,tj)≥
0∀(ti,tj), witht∗∈[0,1]and defining Gaussian control points P={P0,...,PL}withPl∼N (µl,σ2
l). In
doing so, we limit the proof to the 1-dimensional case, as it directly translates to the higher-dimensional
case as well.
Recalling Eq. 9, the kernel function is defined as
kP(ti,tj) =L/summationdisplay
l=0bl,L(ti)bl,L(tj)(σ2
l+µ2
l) +L/summationdisplay
l=0
L/summationdisplay
l′=0,l′̸=lbl,L(ti)bl′,L(tj)µlµl′
−µXµY,
withµX=/summationtextL
l=0bl,L(ti)µlandµY=/summationtextL
l=0bl,L(tj)µl. Writing out µXandµYand combining superfluous
summations yields
L/summationdisplay
l=0bl,L(ti)bl,L(tj)(σ2
l+µ2
l) +L/summationdisplay
l=0
L/summationdisplay
l′=0,l′̸=lbl,L(ti)bl′,L(tj)µlµl′
−µXµY≥0
L/summationdisplay
l=0bl,L(ti)bl,L(tj)(σ2
l+µ2
l) +L/summationdisplay
l=0
L/summationdisplay
l′=0,l′̸=lbl,L(ti)bl′,L(tj)µlµl′
−L/summationdisplay
l=0/parenleftiggL/summationdisplay
l′=0bl,L(ti)bl′,L(tj)µlµl′/parenrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
=−/summationtextL
l=0bl,L(ti)bl,L(tj)µ2
l≥0
L/summationdisplay
l=0/parenleftbig
bl,L(ti)bl,L(tj)σ2
l+bl,L(ti)bl,L(tj)µ2
l−bl,L(ti)bl,L(tj)µ2
l/parenrightbig
≥0
L/summationdisplay
l=0bl,L(ti)bl,L(tj)σ2
l≥0,
which is independent of the control point mean values µland greater or equal to zero for any t∗∈[0,1], due
tobl,L(t∗)≥0andσ2
l>0. Thus,kP(ti,tj)is positive semi-definite for any t∗∈[0,1]and arbitrary sets of
Gaussian control points.
B State-of-the-Art Comparison of the N-MDN model
In this section, we provide state-of-the-art comparisons of the underlying N-MDN used in our paper con-
sidering several standard benchmarking datasets. Thereby, we aim to achieve a fair and concise comparison
between different approaches in the literature, focusing on base sequence models by restricting the model
input to raw positional information and disregarding variations of the same base model. Further, we are
employing common performance measures and baselines.
B.1 Human Trajectory Prediction Models
The core component of human trajectory prediction models is given by a base sequence model, which en-
codes input trajectories, the observation , and generates either single trajectories or probabilistic predictions.
Taking a range of state-of-the-art deep learning-based prediction models into consideration, these models
can be boiled down to few base sequence models. Hence, the most frequently used sequence models are
given by Recurrent Mixture Density Networks (abbrev.: R-MDN, Graves (2013)), variants of Generative Ad-
versarial Networks (abbrev.: GAN, Goodfellow et al. (2014)) and Variational Autoencoders (abbrev.: VAE,
Kingma & Welling (2014)) combined with a sequence to sequence model Sutskever et al. (2014), as well as
Transformers Vaswani et al. (2017). Note that due to the existence of many similar models, the references
in this list are reduced to the most commonly used foundational models in human trajectory prediction. For
a comprehensive overview of existing human trajectory prediction approaches, the reader may be referred to
recent surveys, e.g. Rudenko et al. (2020).
19Under review as submission to TMLR
B.2 Performance Measures
In the standard evaluation approach, the designated performance measures are given by the Average Dis-
placement Error (abbrev.: ADE) and the Final Displacement Error (abbrev.: FDE), defined as
ADE =1
M·NpredM/summationdisplay
i=1Npred/summationdisplay
t=1∥ˆyi
t−yi
t∥2 (22)
and
FDE =1
MM/summationdisplay
i=1∥ˆyi
Npred−yi
Npred∥2 (23)
for a given prediction horizon Npred, a set Y={Y1,...,YM}ofMground truth trajectories Yi=
{yi
1,...,yi
Npred}and corresponding predictions ˆYi={ˆyi
1,...,ˆyi
Npred}generated by a given prediction model.
The ADE is then defined by the average L2 distance between the ground truth and a corresponding pre-
dicted trajectory, while the FDE is defined by the L2 distance between the final ground truth and predicted
trajectory points after the prediction horizon. In the case of probabilistic sequence models, which generate a
predictive distribution p(y{1,...,N pred}|x{1,...,N in}),ˆYicorresponds to a maximum likelihood prediction given
the probabilistic output of the model.
As the ADE and FDE do not provide an adequate measure for assessing the quality of (multi-modal)
probabilistic predictions, another performance measure is required for this case. Due to the actual ground
truth probability distribution for each time step being unknown, a common choice is given by the Negative
(data) Log-Likelihood (abbrev.: NLL, e.g. Bhattacharyya et al. (2018); Ivanovic & Pavone (2019)), defined
as
NLL =1
M·NpredM/summationdisplay
i=1Npred/summationdisplay
t=1−logp(yi
t|·). (24)
Here,p(yi
t|·)denotes the predictive distribution for the t’th trajectory position as generated by the prob-
abilistic sequence model. Note that the conditional part of this distribution is not given explicitly, as it
varies between different models. It is worth mentioning, that sometimes an oracle measure (e.g. Lee et al.
(2017)) is used as a sample-based substitute for the NLL. This measure does, however, introduce another
hyperparameter, which is why the NLL is preferred in the context of this evaluation.
B.3 Baselines
In order to provide reference values for comparison, a simple baseline is given for each performance measure.
In the case of the ADE and FDE, a simple prediction model is given by a linear extrapolation calculated
from a respective observed trajectory. Here, the relative offset δi=xi
Nin−xi
Nin−1of the two most recent
observations is projected Npredsteps into the future, as these positions are assumed to have the most impact
on the future trajectory Schöller et al. (2020). In the case of the NLL measure, a sample-based prediction
can be generated for each future position by using a shotgunapproach Pajouheshgar & Lampert (2018). In
this approach, multiple future trajectories are generated by randomly altering the direction and scale of the
relative offset δibefore projection. The altered offset for each future trajectory is then given by Rα·δi·swith
α∼N(0,σα),s∼N(1,σs)and the matrix Rαdescribing a rotation by αdegrees. This yields a unimodal
probabilistic prediction with a fixed variance for each predicted time step. In the following, σα= 15◦and
σs= 0.1are used.
In addition to these two baselines, a simple LSTM baseline is provided. This mainly has two reasons. On the
one hand, the LSTM model is an integral component of multiple sequence models included in the evaluation.
On the other hand, it is a widely used baseline next to the linear extrapolation approach.
20Under review as submission to TMLR
B.4 Implementation Details
This section gives a brief overview on implementation details for the sequence models in comparison. The
implementations are based on existing approaches, which provide a publicly available implementation. These
implementations are adapted to use a common data pipeline. If necessary, components for processing addi-
tional cues, such as social context, are removed. The list of approaches the implementations are based on
alongside adaptations made is given in Table 2.
Model Based on Adaptations
R-MDNParticleLSTM Hug et al. (2018),-Own implementation
GANSocial GAN Gupta et al. (2018), Removed social context
Original implementation10Data pipeline
VAELSTM-BMS Bhattacharyya et al. (2018), PyTorch re-implementation
Original implementation11Data pipeline
Transformer TF Giuliari et al. (2021),Data pipelineOriginal implementation12
Table 2: Sequence model implementations adapted for use in this evaluation.
R-MDN: The SMC-based R-MDN variant used in this evaluation is a 1-to-1 sequence model, processing
one trajectory point at a time. As such, the model takes a discrete trajectory point as input and outputs
the parameters of a Gaussian mixture distribution modeling the next trajectory point. In order to enable
the model to generate a multi-modal prediction, multiple points are sampled from the output distribution
and fed back into the model. To prevent exponential growth of samples, subsequent output distributions are
combined and re-sampled Hug et al. (2018).
GAN: ForapplyingaconditionalGANinthecontextofhumantrajectoryprediction,asequenceprocessing
unit must be incorporated into the model. According to Gupta et al. (2018), a sequence-to-sequence LSTM
is built into the generator network and another LSTM encoder is built into the discriminator network. The
GAN encodes the observed trajectory and then adds a random noise vector to the encoded representation
in order to sequentially generate a prediction. By performing multiple passes through the decoder network
using different noise vectors, a sample-based distribution of future trajectories is generated.
VAE:Similar to the GAN extension, a sequence-to-sequence LSTM is built into the conditional VAE in
order to enable sequence processing. Further, prediction generation works similar to the GAN model by
adding a random vector to an encoded representation of an observed trajectory in order to generate multiple
future trajectories.
Transformer: Although the implementation chosen for this evaluation does not provide a probabilistic
prediction model, it is considered in this comparison, as it provides a strong contender to the established
LSTM networks built into many human trajectory prediction models. It is an attention-based sequence
model, consisting of an encoder, which encodes the entire observed trajectory into a single vector, and a
decoder, which sequentially generates one trajectory point at a time, given the encoding.
B.5 Evaluation Methodology
For achieving a reliable evaluation, a k-fold cross-validation is performed on each dataset, in order to cope
with unfavorable random training and test splits. In the following, k= 5folds are performed, as it gives a
10https://github.com/agrimgupta92/sgan
11https://github.com/apratimbhattacharyya18/CGM_BestOfMany
12https://github.com/FGiuliari/Trajectory-Transformer
21Under review as submission to TMLR
good trade-off between error bias and variance Hastie et al. (2009). In compliance with the goal of measuring
the raw single dataset performance, all prediction models are re-trained for each fold. As is common practice,
prediction models are tasked to predict Npred= 12steps ( 4.8seconds) into the future, given an observation
ofNin= 8steps ( 3.2seconds).
For generating a maximum likelihood prediction, the output of the probabilistic prediction models in com-
parison need to be processed in different ways. For the R-MDN, instead of propagating a set of particles,
the mean vector of the highest weighted mixture component is fed back into the model in each time step.
As the GAN and VAE models generate a set of sample trajectories, the mean position for each time step is
used. Finally, for the N-MDN, the mean curve of the N-Curve with the highest mixture weight is used.
Looking at the NLL measure, which requires a probability density function generated by each prediction
model, sample-based output is processed by applying a kernel density estimation Scott (2018) using a Gaus-
sian kernel in order to obtain probability density functions for each time step.
B.6 Results
For the evaluation, multiple output-related configurations are considered for the R-MDN and N-MDN mod-
els,controllingthenumberofmixturecomponentsandthe N-MDNmodel’soutputmode. Theconfigurations
are depicted in Table 3.
Configuration Description
R-MDNa Outputs a single-component mixture of Gaussians.
R-MDNb Outputs a 3-component mixture of Gaussians.
N-MDNa Models observed & future trajectory. Outputs a single N-Curve.
N-MDNb Models observed & future trajectory. Outputs mixture of 3N-Curves.
N-MDNc Models future trajectory and outputs a single N-Curve.
N-MDNd Models future trajectory and outputs a mixture of 3N-Curves.
Table 3: Output-related configurations for the R-MDN and N-MDN models in the evaluation.
Tables 4 – 9 summarize the results of the quantitative evaluation, using a per dataset 5-fold cross validation
and the ADE, FDE and NLL performance measures. Accordingly, respective averaged performance values
with corresponding standard deviation considering all 5folds are depicted. It should be noted, that the
performance values are not comparable across datasets, due to different image and ground resolutions. In
order to make values comparable, datasets would need to be projected into 3-dimensional world space.
Additionally, a re-sampling of trajectory points can be necessary in order to match motion profiles.
22Under review as submission to TMLR
Model ADE FDE NLL
Linear 21.80±1.60 47 .81±5.41 -
Shotgun - - 10.07±2.37
LSTM 13.44±0.97 26 .83±2.52 -
Transformer 19.36±2.16 35 .81±3.67 -
R-MDNa 26.12±17.42 46 .28±32.50 16 .62±3.67
R-MDNb 16.11±3.70 28 .70±7.72 2893 .14±5760.97
VAE 22.33±1.82 37 .45±4.91 8 .38±0.16
GAN 11.42±2.18 22 .26±4.48 1084 .14±988.79
N-MDNa 9.51±0.67 17 .41±1.13 7.25±0.13
N-MDNb 9.17±1.28 17.15±2.89 7 .30±0.30
N-MDNc 10.23±1.07 18 .61±2.88 7 .48±0.41
N-MDNd 9.87±1.03 18 .28±3.40 7 .27±0.17
Table 4: Quantitative results of all approaches on the biwi:eth dataset for a predictive time horizon of
Npred= 12time steps ( 4.8seconds). ADE and FDE errors are reported in pixels. Lower is better for all
performance measures.
Model ADE FDE NLL
Linear 26.65±1.24 51 .13±3.29 -
Shotgun - - 9.87±1.14
LSTM 17.91±2.16 32 .93±4.50 -
Transformer 20.42±1.48 34 .35±2.95 -
R-MDNa 24.46±4.92 43 .52±8.28 15 .52±2.93
R-MDNb 19.01±3.77 33 .57±6.71 12 .20±2.42
VAE 20.03±4.14 35 .61±7.90 8 .25±0.31
GAN 15.48±1.80 26 .38±3.15 20115 .70±38614.18
N-MDNa 16.64±3.10 30 .36±7.56 8 .26±0.48
N-MDNb 15.46±2.03 27 .28±3.75 7 .96±0.16
N-MDNc 13.76±0.94 23.82±1.90 7 .86±0.18
N-MDNd 15.30±1.98 26 .60±4.22 7.85±0.20
Table 5: Quantitative results of all approaches on the biwi:hotel dataset for a predictive time horizon of
Npred= 12time steps ( 4.8seconds). ADE and FDE errors are reported in pixels. Lower is better for all
performance measures.
23Under review as submission to TMLR
Model ADE FDE NLL
Linear 21.69±0.40 46 .98±1.39 -
Shotgun - - 10.93±0.89
LSTM 23.71±9.30 49 .93±24.18 -
Transformer 27.30±1.64 50 .04±2.67 -
R-MDNa 20.15±3.25 38 .38±6.77 14 .76±4.55
R-MDNb 16.86±0.34 31 .18±0.72 13 .31±2.26
VAE 19.21±0.74 35 .49±1.47 8 .24±0.19
GAN 15.59±0.41 30.11±0.83 363 .05±456.18
N-MDNa 18.48±0.51 35 .97±0.74 8 .30±0.03
N-MDNb 19.07±0.68 36 .49±1.40 8 .21±0.06
N-MDNc 16.60±0.44 32 .12±0.84 8 .04±0.04
N-MDNd 17.71±0.43 34 .17±0.84 7.95±0.05
Table 6: Quantitative results of all approaches on the crowds:zara01 dataset for a predictive time horizon
ofNpred= 12time steps ( 4.8seconds). ADE and FDE errors are reported in pixels. Lower is better for all
performance measures.
Model ADE FDE NLL
Linear 28.08±0.33 60 .83±0.73 -
Shotgun - - 14.97±1.72
LSTM 34.14±22.66 72 .47±52.74 -
Transformer 32.17±2.94 58 .25±4.76 -
R-MDNa 24.92±1.98 48 .18±3.59 11 .99±1.37
R-MDNb 21.72±2.59 41 .19±4.65 11 .67±1.17
VAE 23.44±0.76 43 .45±1.42 9 .29±0.27
GAN 20.01±0.72 39.57±1.35 26 .33±13.40
N-MDNa 22.34±0.92 43 .20±2.03 8 .54±0.07
N-MDNb 24.74±1.30 47 .29±2.90 8 .49±0.06
N-MDNc 20.41±1.07 40 .39±2.35 8 .33±0.07
N-MDNd 21.41±1.03 41 .55±2.45 7.98±0.03
Table 7: Quantitative results of all approaches on the crowds:zara02 dataset for a predictive time horizon
ofNpred= 12time steps ( 4.8seconds). ADE and FDE errors are reported in pixels. Lower is better for all
performance measures.
24Under review as submission to TMLR
Model ADE FDE NLL
Linear 28.39±0.23 60 .21±0.67 -
Shotgun - - 18.03±4.00
LSTM 27.91±0.95 56 .04±1.75 -
Transformer 52.48±14.88 97 .00±26.24 -
R-MDNa 50.02±13.19 93 .60±23.37 14 .28±2.55
R-MDNb 27.58±4.64 50 .11±8.56 23 .47±23.88
VAE 30.75±1.04 55 .93±1.83 8 .88±0.09
GAN 19.10±0.58 35 .45±0.96 33 .18±4.71
N-MDNa 25.24±1.24 48 .28±2.80 9 .13±0.05
N-MDNb 25.33±2.34 48 .69±5.28 9 .06±0.09
N-MDNc 22.31±1.15 41 .60±1.58 8 .89±0.07
N-MDNd 19.43±0.55 35.37±1.10 8.46±0.04
Table 8: Quantitative results of all approaches on the sdd:bookstore03 dataset for a predictive time horizon
ofNpred= 12time steps ( 4.8seconds). ADE and FDE errors are reported in pixels. Lower is better for all
performance measures.
Model ADE FDE NLL
Linear 36.25±0.79 75 .93±2.09 -
Shotgun - - 16.33±1.30
LSTM 40.52±9.44 85 .83±23.63 -
Transformer 119.70±0.68 222 .18±1.39 -
R-MDNa 90.86±23.89 168 .85±43.22 20 .43±4.03
R-MDNb 44.18±2.32 84 .29±4.17 13 .46±0.74
VAE 41.39±2.53 82 .21±5.21 9 .38±0.09
GAN 28.84±1.11 57.56±2.92 20 .48±5.26
N-MDNa 35.83±1.03 72 .74±2.49 9 .82±0.05
N-MDNb 37.62±2.57 76 .31±6.07 9 .80±0.07
N-MDNc 34.30±1.11 69 .87±2.37 9 .63±0.02
N-MDNd 29.68±1.18 58 .69±3.13 9.11±0.01
Table 9: Quantitative results of all approaches on the sdd:hyang00 dataset for a predictive time horizon of
Npred= 12time steps ( 4.8seconds). ADE and FDE errors are reported in pixels. Lower is better for all
performance measures.
25