Published in Transactions on Machine Learning Research (06/2024)
Federated TD Learning with Linear Function Approximation
under Environmental Heterogeneity
Han Wang hw2786@columbia.edu
Department of Electrical Engineering
Columbia University
Aritra Mitra amitra2@ncsu.edu
Department of Electrical and Computer Engineering
North Carolina State University
Hamed Hassani hassani@seas.upenn.edu
Department of Electrical and Systems Engineering
University of Pennsylvania
George J. Pappas pappasg@seas.upenn.edu
Department of Electrical and Systems Engineering
University of Pennsylvania
James Anderson james.anderson@columbia.edu
Department of Electrical Engineering
Columbia University
Reviewed on OpenReview: https: // openreview. net/ forum? id= hdQspgyFrk
Abstract
We initiate the study of federated reinforcement learning under environmental heterogeneity
by considering a policy evaluation problem. Our setup involves Nagents interacting with
environments that share the same state and action space but differ in their reward functions
and state transition kernels. Assuming agents can communicate via a central server, we ask:
Does exchanging information expedite the process of evaluating a common policy? To answer
this question, we provide the first comprehensive finite-time analysis of a federated temporal
difference (TD) learning algorithm with linear function approximation, while accounting for
Markovian sampling, heterogeneity in the agents’ environments, and multiple local updates
to save communication. Our analysis crucially relies on several novel ingredients: (i) deriving
perturbation bounds on TD fixed points as a function of the heterogeneity in the agents’
underlying Markov decision processes (MDPs); (ii) introducing a virtual MDP to closely
approximate the dynamics of the federated TD algorithm; and (iii) using the virtual MDP to
make explicit connections to federated optimization. Putting these pieces together, we prove
that in a low-heterogeneity regime, exchanging model estimates leads to linear convergence
speedups in the number of agents. Our theoretical contribution is significant in that it is the
first result of its kind in multi-agent/federated reinforcement learning that complements the
numerous analogous results in heterogeneous federated optimization.
1 Introduction
In the popular federated learning (FL) paradigm (Konečn` y et al., 2016; McMahan et al., 2017), a set of
agents aim to find a common statistical model that explains their collective observations. The motivation
to collaborate stems from the fact that if the underlying distributions generating the agents’ observations
are “similar", then each agent can end up learning a “better" model than if it otherwise used just its own
1Published in Transactions on Machine Learning Research (06/2024)
Figure 1: ( Left) Illustration of how FedTD(0) works. Each agent performs Klocal TD update steps on its own
MDP, and transmits its updated model to a server. The virtual MDP serves to approximate the dynamics of
FedTD(0). The global model ¯θtat the server is used to construct a linearly parameterized approximation of
the value function associated with a policy µ. (Right)FedTD(0) helps each agent converge quicklyto a ball
B(θ∗,ϵ)centered around the optimal parameter θ∗of the virtual MDP. Here, ϵcaptures the heterogeneity in
the agents’ MDPs. Using the output ¯θTofFedTD(0), each agent ican then fine-tune based on its own data
to converge exactlyto its own optimal parameter θ∗
i.
data. This idea has been formalized by the canonical FL algorithm FedAvg(and its many variants) where
agents communicate local models via a central server while keeping their raw data private. To achieve
communication-efficiency - a key consideration in FL - the agents perform multiple local model-updates
between successive communication rounds. There is a rich literature that analyzes the performance of FedAvg,
focusing primarily on the aspect of statistical heterogeneity that originates from differences in the agents’
underlying data distributions (Sahu et al., 2018; Khaled et al., 2019; 2020; Li et al., 2019; Koloskova et al.,
2020; Woodworth et al., 2020b; Malinovskiy et al., 2020; Pathak & Wainwright, 2020; Wang et al., 2020;
Karimireddy et al., 2020b; Acar et al., 2021; Gorbunov et al., 2021; Mitra et al., 2021; Mishchenko et al., 2022).
Notably, the above works focus on supervised learning problems that are modeled within the framework
of distributed optimization. However, for sequential decision-making with multiple agents interacting with
potentially different environments , little to nothing is known about the effect of heterogeneity. This is the
gap we seek to fill with our work.
The recent survey paper (Qi et al., 2021) describes a federated reinforcement learning (FRL) framework
which incorporates some of the key ideas from FL into reinforcement learning (RL); applications of FRL in
robotics (Liu et al., 2019), autonomous driving (Chen et al., 2015), and edge computing (Wang et al., 2019)
are discussed in detail in this paper. As RL algorithms often require many samples to achieve acceptable
accuracy, FRL aims to achieve sample-efficiency by leveraging information from multiple agents interacting
with similar environments. Importantly, as in standard FL, the FRL framework requires agents to keep their
raw data (e.g., rewards, states, and actions) private, and adhere to stringent communication constraints.
Motivation and Scope of this Work. While FRL is a promising idea, in reality, it will rarely be the case
that different agents end up interacting with exactly the same environment. Unfortunately, this is the running
assumption in almost all multi-agent RL (MARL) and FRL works (Doan et al., 2019; Liu & Olshevsky,
2021a; Khodadadian et al., 2022; Shen et al., 2023). Departing from this somewhat unrealistic yet prevalent
assumption, the main motivation of this paper is to build a systematic theoretical framework
for reasoning about what to expect when one mixes information from non-identical Markov
processes . The nature of this question is fundamental, and while we motivate its study from the perspective
of FRL1, it can just as easily be connected to stochastic control and estimation problems where one seeks to
“fuse” data generated from non-identical dynamical systems with noisy inputs (Wang et al., 2022b; Guo et al.,
2023; Xin et al., 2023).
1Just as statistical heterogeneity is a major challenge in FL, environmental heterogeneity is identified as a key open challenge
in FRL (Qi et al., 2021).
2Published in Transactions on Machine Learning Research (06/2024)
To initiate a principled study of heterogeneity in FRL, we focus on the simplest RL problem, namely policy
evaluation . Our setup involves Nagents where each agent interacts with an environment modeled as a
MDP. The agents’ MDPs share the same state and action space but have different reward functions and
state transition kernels, thereby capturing environmental heterogeneity. Each agent seeks to compute the
discounted cumulative reward (value function) associated with a common policy µ. Notably, the value
functions induced by µmay differ across environments. This leads to the central question we investigate: Can
an agent expedite the process of learning its own value function by leveraging information from potentially
different MDPs? As we explain shortly, this is a non-trivial question to answer even for policy evaluation;
hence, our focus on policy evaluation as a starting point. That said, recent works have shown that with
minor modifications to the analysis of TD learning for policy evaluation (Srikant & Ying, 2019), one can
analyze Q-learning for control (Chen et al., 2019). As such, we envision that the developments in this paper
can be suitably extended to control algorithms like Q-learning as well.
A typical application of the above FRL setup is that of an autonomous driving system where vehicles in
different geographical locations share local models capturing their learned experiences to train a shared model
that benefits from the collective exploration data of all vehicles. The vehicles (agents) essentially have the
same operations (e.g., steering, braking, accelerating, etc.), but can be exposed to different environments
(e.g., road and weather conditions, routes, driving regulations etc.).
1.1 Our Contributions
We study a federated version of the temporal difference (TD) learning algorithm TD(0) (Sutton, 1988). The
structure of this algorithm, which we call FedTD(0), is as follows. At each iteration, each agent plays an
action according to the policy µ, observes a reward, and transitions to a new state based on its ownMDP. It
then uses TD(0) with linear function approximation to update a local model that approximates its own value
function. To benefit from other agents’ data in a communication-efficient manner, each agent periodically
synchronizes with a central server, and performs multiple local model-updates in between - as depicted in
Figure 1. Notably, as in FL, agents only exchange models but never their personal observations. We perform a
comprehensive analysis of FedTD(0) under environmental heterogeneity, and make the following contributions:
1.Effect of heterogeneity on TD(0) fixed points. Towards understanding the behavior of FedTD(0),
we start by asking: How does heterogeneity in the transition kernels and reward functions of MDPs
manifest into differences in the long-term behavior of TD(0) (with linear function approximation) on
such MDPs? Theorem 1 provides an answer by characterizing how perturbing a MDP perturbs the
TD(0) fixed point for that MDP. To arrive at this result, we combine results from the perturbation
theories of Markov chains and linear equations. Theorem 1 establishes the first perturbation result
forTD(0) fixed points, and complements results of a similar flavor in the RL literature, such as
theSimulation Lemma due to Kearns & Singh (2002). As such, Theorem 1 can serve as a tool of
independent interest in RL.
2.The Virtual MDP framework. In FL algorithms such as FedAvg, the average of the negative
gradients of the agents’ loss functions drives the iterates of FedAvgtowards the minimizer of a global
loss function. In our setting, there is no such global loss function. So by averaging TD(0) update
directions of different MDPs, where do we end up? To answer this question, we construct a virtual
MDP in Section 3.1, and characterize several important properties of this fictitious MDP that aid
our subsequent analysis. Along the way, we derive a simple yet key result (Proposition 1) pertaining
to convex combinations of Markov matrices associated with aperiodic and irreducible Markov chains.
This result appears to be new, and may be of independent interest.
3.Linear Speedup under Markovian Sampling and Heteroegenity. Our most significant
contribution is to provide the first analysis of a federated RL algorithm, FedTD(0), that simultaneously
accounts for linear function approximation, Markovian sampling, multiple local updates, and hetero-
geneity. In Theorem 2, we prove that after Tcommunication rounds with Klocal model-updating
steps per round, FedTD(0) guarantees convergence at a rate of ˜O(1/NKT )to a neighborhood of each
agent’s optimal parameter. The size of the neighborhood depends on the level of heterogeneity in the
3Published in Transactions on Machine Learning Research (06/2024)
agents’ MDPs. The key implication of this result is that in a low-heterogeneity regime, each agent can
enjoy anN-fold linear speed-up in convergence via collaboration, and converge quickly to a vicinity
of its own optimal parameter. One can view this as a “coarse tuning phase”. As is typically done in
FL (Collins et al., 2022), each agent can use the solution of FedTD(0) to then fine-tune (personalize)
based on its own data. This is visually illustrated in Figure 1. Theorem 2 is significant in that it is
the first result in FRL that complements the myriad of federated optimization results that account
for the effects of heterogeneity (Sahu et al., 2018; Khaled et al., 2019; 2020; Li et al., 2019; Koloskova
et al., 2020; Woodworth et al., 2020b; Malinovskiy et al., 2020; Pathak & Wainwright, 2020; Wang
et al., 2020; Karimireddy et al., 2020b; Acar et al., 2021; Gorbunov et al., 2021; Mitra et al., 2021;
Mishchenko et al., 2022).
4.Novel Proof Framework. One might be tempted to think that the proof of Theorem 2 is a simple
combination of the standard FedAvganalysis with that of TD learning. We briefly explain here
why this isn’t quite the case, and defer a more elaborate explanation to Section 5.1. First, in the
centralized TD analysis (Bhandari et al., 2018; Srikant & Ying, 2019), and in the existing analysis for
MARL/FRL (Doan et al., 2019; Liu & Olshevsky, 2021a; Khodadadian et al., 2022) with identical
MDPs, the dynamics of the update rules correspond to one single MDP. In our setup, the dynamics
ofFedTD(0)may not correspond to any MDP at all! Thus, we need new tools relative to existing
RL analyses. Second, while existing FL analyses are essentially distributed optimization proofs,
federated TD learning does not correspond to minimizing any fixed loss function . Moreover, unlike
the i.i.d. data model in FL, the data tuples observed by each agent in FedTD(0) are part of a single
Markovian trajectory. This creates complex time-correlations that are challenging to deal with even
in a single-agent setting. Thus, we cannot directly employ FL proofs either. As such, we introduce a
new analysis framework where we argue that the dynamics of FedTD(0) can be approximated by that
of TD(0) on a virtual MDP, up to an error term that captures heterogeneity in the agents’ MDPs.
Carefully tracking how this error term propagates over time accounts for the effect of heterogeneity;
establishing linear speedup under Markovian sampling and local steps requires much more work.
5.Bias introduced by Heterogeneity. Our convergence result in Theorem 2 features a bias term due
to heterogeneity that cannot be eliminated even by making the step-size arbitrarily small. Is such a
term unavoidable? We explore this question in Theorem 3 by studying a “steady-state” deterministic
version of FedTD(0). Even for this simple case, we prove that a bias term depending on a natural
measure of heterogeneity shows up inevitably in the long-term dynamics of FedTD(0). This result
sheds further light on the effect of heterogeneity in FRL.
1.2 Related Work
In what follows, we discuss the most relevant threads of literature.
1.Finite-Time Analysis of TD Learning Algorithms. In their seminal paper, Tsitsiklis &
Van Roy (1997) provided an asymptotic convergence analysis of the temporal difference (TD) learning
algorithm (Sutton, 1988; Sutton et al., 1998) with value function approximation, using tools from
stochastic approximation theory. Several years later, the work by Korda & La (2015) provided
finite-time rates for TD learning. However, the authors in Narayanan & Szepesvári (2017) noted
some issues with the proofs in Korda & La (2015). Under the i.i.d. observation model described in
Section 5, Dalal et al. (2018) and Lakshminarayanan & Szepesvári (2017) were able to resolve the
issues in Korda & La (2015). Even so, a non-asymptotic convergence analysis for the challenging
Markovian setting (that we consider in this paper) remained elusive till the work by Bhandari et al.
(2018). While the authors in Bhandari et al. (2018) made some elegant connections between the
dynamics of TD learning and gradient descent, an alternative proof technique using Stein’s method
was developed by Srikant & Ying (2019). Yet another interesting interpretation was provided by Liu
& Olshevsky (2021b): they argued that the steady-state temporal difference direction acts as a
“gradient-splitting" of an appropriately chosen function. Recently, a short proof of TD learning
with linear function approximation and more general nonlinear contractive stochastic approximation
schemes was provided by Mitra (2024) based on a novel inductive proof technique. While all the
4Published in Transactions on Machine Learning Research (06/2024)
above works provide upper-bounds for the task of policy evaluation, for minimax lower bounds, we
refer the reader to the work of Khamaru et al. (2021).
2.Multi-Agent and Federated RL. In Doan et al. (2019) and Liu & Olshevsky (2021a), the authors
analyze multi-agent TD learning with linear function approximation over peer-to-peer networks.
Neither approach accounts for local steps or Markovian sampling. In Shen et al. (2023), the authors
study a parallel version of asynchronous actor-critic algorithms, and establish a linear speedup
result - albeit under an i.i.d. sampling assumption. Very recently, the authors in Khodadadian
et al. (2022) and Dal Fabbro et al. (2023) studied the effect of Markovian sampling for federated TD
learning. However, all of the above papers consider a homogeneous setting with identical MDPs for
all agents. In contrast, our work has to tackle the challenge of understanding the long-term effects
of mixing TD update directions from non-identical MDPs. The only two papers we are aware of
that perform any theoretical analysis of heterogeneity in FRL are Jin et al. (2022) and Xie & Song
(2023). However, their analyses are limited to the much simpler tabular setting with no function
approximation. In particular, the work of Xie & Song (2023) only comes with asymptotic results,
i.e., they do not provide finite-time rates. Moreover, unlike us, neither Jin et al. (2022) nor Xie &
Song (2023) provide any explicit linear speedup result. In conclusion, we are the first to establish
afinite-time theory for FRL under function approximation, environmental heterogeneity, and
Markovian sampling. Considering different settings, Zhang et al. (2024) proposed the FEDSARSA
algorithm to solve the on-policy FRL problem and Wang et al. (2023) proposed FedLQRto solve the
federated control design problem. A more detailed description of related work on federated learning
is relegated to the Appendix.
2 Model and Problem Formulation
We consider a Markov Decision Process (MDP) (Sutton et al., 1998) defined by the tuple M=⟨S,A,R,P,γ⟩,
whereSis a finite state space of size n,Ais a finite action space, Pis a set of action-dependent Markov
transition kernels, Ris a reward function, and γ∈(0,1)is the discount factor. We consider the problem
of evaluating the value function Vµof a given policy µ, whereµ:S→A. The policy µinduces a Markov
reward process (MRP) characterized by a transition matrix Pµ, and a reward function Rµ. Under the action
of the policy µat an initial state s,Pµ(s,s′)is the probability of transitioning from state sto states′, and
Rµ(s)is the expected instantaneous reward. The discounted expected cumulative reward obtained by playing
policyµstarting from initial state sis:
Vµ(s) =E/bracketleftigg∞/summationdisplay
t=0γtRµ(st)|s0=s/bracketrightigg
,
wherestis the state of the Markov chain at time t. From Tsitsiklis & Van Roy (1997), we know that Vµis
the fixed point of the policy-specific Bellman operator Tµ:Rn→Rn, i.e.,TµVµ=Vµ, where for any V∈Rn,
(TµV)(s) =Rµ(s) +γ/summationdisplay
s′∈SPµ(s,s′)V(s′),∀s∈S.
TD learning with linear function approximation. We consider the setting where the number of states
is very large, making it practically infeasible to compute the value function Vµdirectly. To mitigate the
curse of dimensionality, a common approach (Sutton et al., 1998) is to consider a low-dimensional linear
function approximation of the value function Vµ. Let{Φk}d
k=1be a set ofdlinearly independent basis vectors
inRn, and Φ∈Rn×dbe a matrix with these basis vectors as its columns, i.e., the k-th column of ΦisΦk.
A parametric approximation ˆVθofVµin the span of{Φk}d
k=1is then given by ˆVθ= Φθ, whereθ∈Rdis a
parameter vector to be learned. Notably, this is tractable since d≪n. We denote the s-th row of Φby
ϕ(s)∈Rd, and refer to it as the fixed feature vector corresponding to state s. We write ˆVθ(s) =ϕ(s)⊤θand
make the standard assumption (Bhandari et al., 2018) that ∥ϕ(s)∥2≤1,∀s∈S.
The objective is to find the best linear approximation of Vµin the span of{Φk}d
k=1. More precisely, we seek a
parameter vector θ∗that minimizes the distance between ˆVθandVµ(in a suitable sense). When the underlying
5Published in Transactions on Machine Learning Research (06/2024)
MDP isunknown , one of the most popular techniques to achieve this goal is the classical TD(0) algorithm. TD(0)
starts from an initial guess θ0∈Rd. Subsequently, at the t-th iteration, upon playing the given policy µ, a new
data tupleOt= (st,rt=Rµ(st),st+1)comprising of the current state, the instantaneous reward, and the next
state is observed. Let us define the TD(0) update direction as gt(θt)≜/parenleftig
rt+γϕ(st+1)⊤θt−ϕ(st)⊤θt/parenrightig
ϕ(st).
Using a step-size αt∈(0,1), the parameter θtis then updated as
θt+1=θt+αtgt(θt).
Under some mild technical assumptions, it was shown in Tsitsiklis & Van Roy (1997) that the TD(0) iterates
converge asymptotically almost surely to a vector θ∗, whereθ∗is the unique solution of the projected Bellman
equation ΠDTµ(Φθ∗) = Φθ∗. Here,Dis a diagonal matrix with entries given by the elements of the stationary
distribution πof the Markov matrix Pµ. Furthermore, ΠD(·)is the projection operator onto the subspace
spanned by{ϕk}d
k=1with respect to the inner product ⟨·,·⟩D.2
Objective. We study a multi-agent RL problem where agents interact with similar, but non-identical
MDPs that share the same state and action space. All agents seek to evaluate the same policy. Our goal is to
understand: Can an agent evaluate the value function of its own MDP in a more sample-efficient way by
leveraging data from other agents? Existing FL analyses that study statistical heterogeneity in supervised
learning/empirical risk minimization fall short of answering this question, since our problem does not involve
minimizing a static loss function . As such, the question we have posed above is non-trivial, and requires
several new ideas and tools. In the next section, we will start building these tools in a systematic manner by
accomplishing the following goals.
Goal 1. Formally defining what we mean by model heterogeneity in the agents’ MDPs.
Goal 2. Characterizing how such model heterogeneity translates to differences in the fixed points of the TD(0)
algorithm when run on the agents’ MDPs.
Goal 3. Introducing the notion of a virtual MDP that will play a crucial role in reasoning about the long-term
behavior of algorithms that combine information from non-identical MDPs.
3 Heterogeneous Federated RL
WeconsiderafederatedRLsettingcomprisingof Nagentsthatinteractwithpotentiallydifferentenvironments.
Agenti’s environment is characterized by the following MDP: M(i)=⟨S,A,R(i),P(i),γ⟩. While all agents
share the same state and action space, the reward functions and state transition kernels of their environments
can differ. We focus on a policy evaluation problem where all agents seek to evaluate a common policy µthat
inducesNMarkov reward processes characterized by the tuples {P(i)
µ,R(i)
µ}i∈[N].3Agentiaims to find a
linearly parameterized approximation of its ownvalue function V(i)
µ. Trivially, agent ican do so without
interacting with any other agent by simply running TD(0). However, the key question we ask pertains to the
value of side-information :By using data from other agents, can it achieve a desired level of approximation
with fewer samples relative to when it acts alone? Naturally, the answer to the above question depends on
the level of heterogeneity in the agents’ MDPs. Accordingly, we introduce the following definitions.
Assumption 1. (Markov Kernel Heterogeneity ) There exists an ϵ>0such that for all agents i,j∈[N],
it holds that|P(i)(s,s′)−P(j)(s,s′)|≤ϵ|P(i)(s,s′)|,∀s,s′∈S. Here, for each i∈[N],P(i)(s,s′)represents
the(s,s′)-th element of the matrix P(i).
Assumption 2. (Reward Heterogeneity ) There exists an ϵ1>0such that for all i,j∈[N], it holds that
∥R(i)−R(j)∥≤ϵ1.
Clearly, smaller values of ϵandϵ1capture more similarity in the agents’ MDPs. Suppose all agents can
communicate via a central server. Via such communication, the standard FL task is to find one common
2We will use∥·∥Dto denote the norm induced by the matrix D, and∥·∥to represent the standard Euclidean norm for
vectors and ℓ2induced norm for matrices.
3We will henceforth drop the dependence of P(i)andR(i)on the policy µ.
6Published in Transactions on Machine Learning Research (06/2024)
model that “fits" the data of all agents. In a similar spirit, our goal is to find a common parameter θsuch that
ˆVθ= Φθapproximates each V(i)
µ,i∈[N]. The role of this common θwill be to quickly (i.e., by leveraging
samples of allagents) provide a coarse model that the agents can then use as a warm-start to fine-tune based
on personal data. There is a natural tension here. While federation can help converge fasterto a coarse
model, such a model may not accurately capture the value function of anyagent if the agents’ MDPs are
very dissimilar. So does more data help or hurt?
Impact of Heterogeneity on TD fixed points. To answer the above question, we need to carefully
understand how the structural heterogeneity assumptions on the MDPs (namely, Assumptions 1 and 2)
manifest into differences in the long-term dynamics of TD(0) on these MDPs. Since long-term dynamics are
intimately tied to fixed points, we first set out to characterize the “closeness" in TD(0) fixed points across
different MDPs. To proceed, we make the following standard assumption.
Assumption 3. For eachi∈[N], the Markov chain induced by the policy µ, corresponding to the state
transition matrix P(i), is aperiodic and irreducible.
The above assumption implies the existence of a unique stationary distribution π(i)for eachi∈[N]; letD(i)
be a diagonal matrix with the entries of π(i)on its diagonal. For each agent i, we then use θ∗
ito denote
the solution of the projected Bellman equation ΠD(i)T(i)
µ(Φθ∗
i) = Φθ∗
ifor agenti. In words, θ∗
iis the best
linear approximation of V(i)
µin the span of{ϕk}d
k=1. From Section 2, we know that the iterates of TD(0) on
agenti’s MRP will converge to θ∗
iasymptotically almost surely. Our goal is to bound the gap ∥θ∗
i−θ∗
j∥as a
function of the heterogeneity parameters ϵandϵ1appearing in Assumptions 1 and 2. The key observation
we will exploit is that for each i∈[N],θ∗
iis the unique solution of the linear equation ¯Aiθ∗
i=¯bi, where
¯Ai= Φ⊤D(i)(Φ−γP(i)Φ)and¯bi= Φ⊤D(i)R(i).For an agent j̸=i, viewing ¯Ajand¯bjas perturbed versions
of¯Aiand¯bi, we can now appeal to results from the perturbation theory of linear equations (Horn & Johnson,
2012a, Chapter 5.8) to bound ∥θ∗
i−θ∗
j∥. To that end, we first recall a result from the perturbation theory of
Markov chains (O’cinneide, 1993) which shows that under Assumption 1, the stationary distributions π(i)
andπ(j)are close for any pair i,j∈[N].
Lemma 1. (Perturbation bound on Stationary Distributions ) Suppose Assumption 1 holds. Then,
for any pair of agents i,j∈[N], the stationary distributions π(i)andπ(j)satisfy:
∥π(i)−π(j)∥1≤2(n−1)ϵ+O(ϵ2). (1)
We will now use the above result to bound ∥¯Ai−¯Aj∥and∥¯bi−¯bj∥.To state our results, we make the
standard assumption that for each i∈[N], it holds that|R(i)(s)|≤Rmax,∀s∈S, i.e., the rewards are
uniformly bounded. In (Tsitsiklis & Van Roy, 1997), it was shown that −¯Aiis a negative definite matrix;
thus,∃δ1>0such that∥¯Ai∥≥δ1,∀i∈[N]. We also assume that ∃δ2>0such that∥¯bi∥≥δ2,∀i∈[N]. In
our first technical result, stated below, we provide a bound on the perturbation of TDfixed points.
Theorem 1. (Perturbation bounds on TD(0) fixed points ) For alli,j∈[N],we have:
1.∥¯Ai−¯Aj∥≤A(ϵ)≜γ√nϵ+ (1 +γ)/parenleftbig
2(n−1)ϵ+O(ϵ2)/parenrightbig
.
2.∥¯bi−¯bj∥≤b(ϵ,ϵ1)≜Rmax/parenleftbig
2(n−1)ϵ+O(ϵ2)/parenrightbig
+O(ϵ1).
3. Suppose∃H > 0s.t.∥θ∗
i∥≤H,∀i∈[N]. Letκ(¯Ai)be the condition number of ¯Ai. Then:
∥θ∗
i−θ∗
j∥≤Γ(ϵ,ϵ1)≜max
i∈[N]/braceleftigg
κ(¯Ai)H
1−κ(¯Ai)A(ϵ)
δ1/parenleftbiggA(ϵ)
δ1+b(ϵ,ϵ1)
δ2/parenrightbigg/bracerightigg
.
Discussion. Theorem 1 reveals how heterogeneity in the rewards and transition kernels of MDPs can be
mapped to differences in the limiting behavior of TD(0) on such MDPs from a fixed-point perspective. It
formalizes the intuition that if the level of heterogeneity - as captured by ϵandϵ1- is small, then so is the
7Published in Transactions on Machine Learning Research (06/2024)
gap in the TD(0) limit points of the agents’ MDPs. This result is novel, and complements similar perturbation
results in the RL literature such as the Simulation Lemma (Kearns & Singh, 2002).4
In what follows, we will introduce the key concept of a virtual MDP, and build on Theorem 1 to relate
properties of this virtual MDP to those of the agents’ individual MDPs.
3.1 Virtual Markov Decision Process
In a standard FL setting, the goal is to typically minimize a global loss function f(x) = (1/N)/summationtext
i∈[N]fi(x)
composed of the local loss functions of Nagents; here, fi(x)is the local loss function of agent i. In FL, due
to heterogeneity in the agents’ loss functions, there is a “drift" effect (Charles & Konečn` y, 2020; Karimireddy
et al., 2020b): the local iterates of each agent idrift towards the minimizer of fi(x). However, when the
heterogeneity is moderate, the average of the agents’ iterates converges towards the minimizer of f(x). To
develop an analogous theory for FRL, we need to first answer: When we average TD(0) update directions
from different MDPs, where does the average TD(0) update direction lead us? It is precisely to answer this
question that we introduce the concept of a virtual MDP . To model a virtual environment that captures
the “average" of the agents’ individual environments, we construct an MDP ¯M=⟨S,A,¯R,¯P,γ⟩, where
¯P= (1/N)/summationtextN
i=1P(i),and ¯R= (1/N)/summationtextN
i=1R(i). Note that the virtual MDP is a fictitious MDP that we
construct solely for the purpose of analysis, and it may not coincide with any of the agents’ MDPs, in general.
Properties of the Virtual MDP. When applied to ¯M, let the policy µthat we seek to evaluate induce
a virtual MRP characterized by the tuple {¯P,¯R}. It is easy to see that ¯P= (1/N)/summationtextN
i=1P(i),and ¯R=
(1/N)/summationtextN
i=1R(i). The following result shows how the virtual MRP inherits certain basic properties from the
individual MRPs; the result is quite general and may be of independent interest.
Proposition 1. (Convex combinations of Markov matrices ) Let{P(i)}N
i=1be a set of Markov matrices
associated with Markov chains that share the same states, and are each aperiodic and irreducible. Then, for
any set of weights {wi}N
i=1satisfyingwi≥0,∀i∈[N]and/summationtext
i∈[N]wi= 1, the Markov chain corresponding to
the matrix/summationtext
i∈[N]wiP(i)is also aperiodic and irreducible.
The above result immediately tells us that the Markov chain corresponding to ¯Pis aperiodic and irreducible.
Thus, there exists an unique stationary distribution ¯πof this Markov chain; let ¯Dbe the corresponding
diagonal matrix. As before, let us define ¯A≜Φ⊤¯D(Φ−γ¯PΦ),¯b≜Φ⊤¯D¯R, and useθ∗to denote the solution
to the equation ¯Aθ∗=¯b. Our next result is a consequence of Theorem 1, and characterizes the gap between
θ∗
iandθ∗, for eachi∈[N].
Proposition 2. (Virtual MRP is “close" to Individual MRPs ) Fix anyi∈[N]. Using the same
definitions as in Theorem 1, we have ∥¯Ai−¯A∥≤A(ϵ),∥¯bi−¯b∥≤b(ϵ,ϵ1)and∥θ∗
i−θ∗∥≤Γ(ϵ,ϵ1).
We will later argue that the federated TD algorithm (to be introduced in Section 4) converges to a ball
centered around the TD(0) fixed point θ∗of the virtual MRP. Proposition 2 is thus particularly important
since it tells us that in a low-heterogeneity regime, by converging close to θ∗, we also converge close to
the optimal parameter θ∗
iof each agent i. This justifies studying the convergence behavior of FedTD(0) on
the virtual MRP. Define Σv≜Φ⊤¯DΦ. The smallest eigenvalue of this matrix will end up dictating the
convergence rate of our proposed algorithm. We end this section with a result showing that this eigenvalue is
bounded away from zero.
Proposition 3. For the virtual MRP, it holds that λmax(Σv)≤1, and∃¯ω>0s.t.λmin(Σv)≥¯ω.
4 Federated TD Algorithm
In this section, we describe the FedTD(0) algorithm (outlined in Algorithm 1). The goal of FedTD(0) is to
generate a model θsuch that ˆVθis a good approximation of each agent i’s value function V(i)
µ, corresponding
4The simulation lemma tells us that if two MDPs with the same state and action spaces are similar, then so are the value
functions induced by a common policy on these MDPs.
8Published in Transactions on Machine Learning Research (06/2024)
Algorithm 1 Description of FedTD(0)
1:Input:Policyµ, local step-size αl, global step-size α(t)
gthat depends on communication round t
2:Initialize: ¯θ0=θ0ands(i)
0,0=s0,∀i∈[N]
3:foreach round t= 0,...,T−1do
4:foreach agent i∈[N]do
5:fork= 0,...,K−1dowith initial model θ(i)
t,0=¯θt
6: Agentiplaysµ(s(i)
t,k), observesO(i)
t,k= (s(i)
t,k,r(i)
t,k,s(i)
t,k+1), and updates local model:
θ(i)
t,k+1=θ(i)
t,k+αlgi(θ(i)
t,k),wheregi(θ(i)
t,k)≜/parenleftig
r(i)
t,k+γϕ(s(i)
t,k+1)⊤θ(i)
t,k−ϕ(s(i)
t,k)⊤θ(i)
t,k/parenrightig
ϕ(s(i)
t,k)
7:end for
8: Agentisends ∆(i)
t=θ(i)
t,K−¯θtback to the server
9:end for
10:Server broadcasts the following global model: ¯θt+1= Π 2,H(¯θt+ (α(t)
g/N)/summationtext
i∈[N]∆(i)
t)
11:end for
to the policy µ. In line with both standard FL algorithms, and also works in MARL/FRL (in homogeneous
settings) (Doan et al., 2019; Khodadadian et al., 2022), the agents keep their raw observations (i.e., their
rewards, states, and actions) private, and only exchange local models. In each round t, each agent i∈[N]
starts from a common global model ¯θtand uses its local data to perform Klocal updates of the following
form: at each local iteration k, agentitakes action µ(s(i)
t,k)and observes a data tuple O(i)
t,kbased on its own
MRP, i.e.,{P(i),R(i)}; we note here that observations are independent across agents . Using its data tuple,
agentithen updates its own local model θ(i)
t,kalong the direction gi(θ(i)
t,k)in line 6. Since each agent seeks to
benefit from the samples acquired by the other agents, there is intermittent communication via the server.
However, such communication needs to be limited as communication-efficiency is a key concern in FL. As
such, the agents upload their local models’ difference ∆(i)
tto the server only once every Ktime-steps. The
server averages these model differences and performs a projection to construct a global model ¯θt+1that is
then broadcast to all agents (line 10). Here, we use Π2,H(·)to denote the standard Euclidean projection on to
a convex compact subset H⊂Rdthat is assumed to contain each θ∗
i,i∈[N], and alsoθ∗. Such a projection
step ensures that the global models do not blow up, and is common in stochastic approximation (Borkar,
2009) and RL (Bhandari et al., 2018; Doan et al., 2019). Each agent then resumes its local updating process
from this global model.
We note that the structure of FedTD(0) mirrors that of FedAvg(and its many variants) where agents perform
multiple local model-updates in isolation using their own data (to save communication), and synchronize
periodically via a server. However, there are significant differences in the dynamics of standard FL algorithms
and FedTD(0), making it quite challenging to derive finite-time convergence results for the latter. In the
next section where we analyze FedTD(0), we will explain the nature of these challenges, and discuss how we
overcome them.
5 Main Result and Analysis
To state our main convergence result for FedTD(0), we need to introduce a few objects. First, let Hdenote
the radius of the set Hin line 10 of Algorithm 1. Also, define G≜Rmax+ 2Handν≜(1−γ)¯ω, where ¯ω
is as in Proposition 3. In our analysis, we will make use of the geometric mixing property of finite-state,
aperiodic, and irreducible Markov chains (Levin & Peres, 2017). Specifically, under Assumption 3, for each
i∈[N], there exists some mi≥1andρi∈(0,1), such that for all t≥0and0≤k≤K−1:
dTV/parenleftig
P/parenleftig
s(i)
t,k=·|s(i)
0,0=s/parenrightig
,π(i)/parenrightig
≤miρtK+k
i,∀s∈S.
9Published in Transactions on Machine Learning Research (06/2024)
Here, we use dTV(P,Q)to denote the total-variation distance between two probability measures Pand
Q.For any ¯ϵ >0, let us define the mixing time for P(i)asτmix
i(¯ϵ)≜min{t∈N0|miρt
i≤¯ϵ}. Finally, let
τ(¯ϵ) =maxi∈[N]τmix
i(¯ϵ)represent the mixing time corresponding to the Markov chain that mixes the slowest.
As one might expect, and as formalized by our main result below, it is this slowest-mixing Markov chain that
dictates certain terms in the convergence rate of FedTD(0).
Theorem 2. (Main Result ) There exists a decreasing global step-size sequence {α(t)
g}, a fixed local step-size
αl, and a set of convex weights, such that a convex combination ˜θTof the global models {¯θt}satisfies the
following for each agent i∈[N]afterTrounds:
E/bracketleftbigg/vextenddouble/vextenddouble/vextenddoubleV˜θT−Vθ∗
i/vextenddouble/vextenddouble/vextenddouble2
¯D/bracketrightbigg
≤˜O/parenleftbiggτ2G2+K2
K2T2+cquad(τ)
ν2NKT+clin(τ)
ν4KT2+Q(ϵ,ϵ1)/parenrightbigg
, (2)
whereτ=⌈τmix(α2
T)
K⌉,αT=Kαlα(T)
g, andcquad(τ)andclin(τ)are quadratic and linear functions in τ,
respectively. Moreover, B(ϵ,ϵ1) =H/parenleftbig√nϵ+ 2(n−1)ϵ+O(ϵ2) +O(ϵ1)/parenrightbig
,Γ(ϵ,ϵ1)is as defined in Theorem 1,
andQ(ϵ,ϵ1) =˜O(B(ϵ,ϵ1)G
ν+ Γ2(ϵ,ϵ1)).
The proof of the above result is deferred to Appendix I. We now discuss its impplications.
Discussion. To parse Theorem 2, let us start by noting that the term Q(ϵ,ϵ1)in Eq.(2)captures the
effect of heterogeneity; we will comment on this term later. When T≫N, the dominant term among the
first three terms in Eq. (2)iscquad(τ)/(ν2NKT ). To appreciate the tightness of this term, we note that
in a centralized setting (i.e., when N= 1), given access to KTsamples, the convergence rate of TD(0) is
O(1/(ν2KT))(Bhandari et al., 2018). Our analysis thus reveals that by communicating just Ttimes inKT
iterations, each agent ican achieve a linear speedup w.r.t. the number of agents . In a low-heterogeneity
regime, i.e., when Q(ϵ,ϵ1)is small, we note that by combining data from different MDPs, FedTD(0) guarantees
fastconvergence to a model that is a good approximation of each agent’s value function; by fast, we imply a
N-fold speedup over the rate each agent would have achieved had it not communicated at all. Thus with
little communication, FedTD(0) quickly provides each agent with a good model that it can then fine-tune
for personalization. Theorem 2 is significant in that it is the first result of its kind in MARL/FRL with
heterogeneous environments, and complements the numerous analogous results in heterogeneous federated
optimization (Sahu et al., 2018; Khaled et al., 2019; 2020; Li et al., 2019; Koloskova et al., 2020; Woodworth
et al., 2020b; Malinovskiy et al., 2020; Pathak & Wainwright, 2020; Wang et al., 2020; Karimireddy et al.,
2020b; Acar et al., 2021; Gorbunov et al., 2021; Mitra et al., 2021; Mishchenko et al., 2022).
When all the MDPs are identical, Q(ϵ,ϵ1) = 0. But when the MDPs are different, should we expect such a
term? To further understand the effect of heterogeneity, it suffices to get rid of all the randomness in our
setting. As such, suppose we replace the random TD(0) direction gi(θ(i)
t,k)of each agent iin Algorithm 1 by
itssteady-state deterministic version ¯gi(θ(i)
t,k) =¯bi−¯Aiθ(i)
t,k, where ¯Aiand¯biare as in Section 3. We call
the resulting deterministic algorithm mean-path FedTD(0). For simplicity, we skip the projection step. In
our next result, we exploit the affine nature of the steady-state TD(0) directions to characterize the effect of
heterogeneity in the limiting behavior of FedTD(0).
Theorem 3. (Heterogeneity Bias ) SupposeN= 2andK= 1. Let the step-size α=αlα(t)
gbe chosen
such thatI−αˆAis Schur stable, where ˆA=/parenleftbig¯A1+¯A2/parenrightbig
/2. Defineei,t≜¯θt−θ∗
i,i∈{1,2}. The output of
mean-path FedTD(0) then satisfies:
lim
t→∞e1,t=1
2ˆA−1¯A2(θ∗
1−θ∗
2); lim
t→∞e2,t=1
2ˆA−1¯A1(θ∗
2−θ∗
1). (3)
Discussion: For the setting described in Theorem 3, the mean-path FedTD(0) updates follow the determin-
istic recursion ¯θt+1= (I−αˆA)¯θt+αˆb, where ˆb= (1/2)(¯b1+¯b2). This is a discrete-time linear time-invariant
system (LTI). The dynamics of this system are stable if and only if the state transition matrix (I−αˆA)is
Schur stable, justifying the choice of αin Theorem 3. The main message conveyed by this result is that
the gap between the limit point of mean-path FedTD(0) and the optimal parameter θ∗
iof either of the two
10Published in Transactions on Machine Learning Research (06/2024)
MRPs bears a dependence on the difference in the optimal parameters of the MRPs - a natural indicator of
heterogeneity between the two MRPs . Furthermore, this term has no dependence on the step-size α, i.e., the
effect of the heterogeneity-induced bias cannot be eliminated by making αarbitrarily small. Aligning with
this observation, notice that Q(ϵ,ϵ1)in Eq.(2)is also step-size independent. The above discussion sheds
some light on the fact that a term of the form Q(ϵ,ϵ1)is to be expected in Theorem 2. Notably, the bias term
in Eq.(3)persists even when the number of local steps is just one, i.e., even when the agents communicate
with the server at all time steps. This is a key difference with the standard FL setting where the effect of
heterogeneity manifests itself onlywhen the number of local steps Kstrictly exceeds 1(Charles & Konečn` y,
2021; Karimireddy et al., 2020b; Mitra et al., 2021).
5.1 Main Technical Challenges and Overview of the Novel Ingredients in Our Analysis
Challenges. We summarize the major technical challenges that show up in the analysis of Theorem 2.
First, the FedTD(0) update direction may not correspond to the TD(0) update direction of anyMDP. This
challenge is unique to our setting, and neither shows up in the centralized TD(0) analysis (Bhandari et al.,
2018; Srikant & Ying, 2019), nor in the existing MARL/FRL analyses with homogeneous MDPs (Doan et al.,
2019; Khodadadian et al., 2022). Second, unlike standard FL analyses that deal with i.i.d. observations for
each agent, our setting is complicated by the fact that each agent’s data is generated from a Markov chain.
Moreover, for each agent i, the parameter sequence {θ(i)
t,k}and the data tuples {O(i)
t,k}are intricately coupled.
Third, the synchronization step in FedTD(0) creates complex statistical dependencies between the local
parameter of any given agent and the past observations of allother agents. Fourth, controlling the gradient
bias(1/NK )/summationtextN
i=1/summationtextK−1
k=0/parenleftbig
gi(θ(i)
t,k,O(i)
t,k)−¯gi(θ(i)
t,k)/parenrightbig
and the gradient norm E∥(1/NK )/summationtextN
i=1/summationtextK−1
k=0gi(θ(i)
t,k)∥2
requires a very delicate analysis when one seeks to establish the linear speedup property w.r.t. the number of
agentsN, i.e., theO(1/NKT )-type rate. In particular, naively bounding terms using the projection radius
(as in the centralized analysis (Bhandari et al., 2018)) will not yield the linear speedup property. Finally, we
need to control the “client-drift” effect due to environmental heterogeneity under the strong coupling between
the different random variables discussed above.
Proof Sketch for Theorem 2. Our first key innovation is to build on the results in Section 3 to show that
the mean-path (steady-state) FedTD(0) update direction (1/N)/summationtextN
i=1¯gi(θ)is “close" to the mean-path TD(0)
update direction ¯g(θ) =¯b−¯Aθof the virtual MRP we constructed in Section 3.1; here, ¯b,¯Aare as defined in
Section 3.1. Formally, we have the following result.
Lemma 2. (Steady-state Pseudo-Gradient Heterogeneity ) For eachθ∈H, we have:
/vextenddouble/vextenddouble/vextenddouble¯g(θ)−1
NN/summationdisplay
i=1¯gi(θ)/vextenddouble/vextenddouble/vextenddouble≤B(ϵ,ϵ1), (4)
whereB(ϵ,ϵ1)is as in Theorem 2, and ¯g(θ)is the steady-state TD(0) direction of the virtual MRP.
From Bhandari et al. (2018), we know that ¯g(θ)acts like a pseudo-gradient pointing towards the optimal model
θ∗of the virtual MRP. Since based on Proposition 2, we know that θ∗is close toθ∗
i,∀i∈[N], Lemma 2 tells
us that at least in the steady-state, the iterates of FedTD(0) will converge to a neighborhood of each agent’s
optimal model, where the size of the neighborhood depends on the level of heterogeneity. While this helps
build intuition, all the valuable insights conveyed by Lemma 2 only pertain to the steady state dynamics of
FedTD(0), i.e., all the statistical challenges we alluded to still need to be resolved. In particular, as mentioned
earlier, we cannot naively use a projection bound of the form E/bracketleftig
∥(1/NK )/summationtextN
i=1/summationtextK−1
k=0gi(θ(i)
t,k)∥2/bracketrightig
=O(G2)
from the centralized analysis in Bhandari et al. (2018), since the local models may not belong to the set H.
Also, this will obscure the linear speedup effect. We overcome this difficulty by decomposing the random TD
direction of each agent iasgi(θ(i)
t,k) =bi(O(i)
t,k)−Ai(O(i)
t,k)θ(i)
t,k. SinceAi(O(i)
t,k)andbi(O(i)
t,k)only depend on the
randomness from the Markov chain, and O(i)
t,kandO(j)
t,kare independent, we can show that the variances of
(1/NK )/summationtextN
i=1/summationtextK−1
k=0Ai(O(i)
t,k)and(1/NK )/summationtextN
i=1/summationtextK−1
k=0bi(O(i)
t,k)get scaled down by NK(up to higher order
terms). Furthermore, to account for the fact that Ai(O(i)
t,k)andbi(O(i)
t,k)differ across agents, we appeal to
11Published in Transactions on Machine Learning Research (06/2024)
Lemma 2. Putting these pieces together in a careful manner yields the final rate in Theorem 2. The detailed
analysis, along with some simulations, are deferred to the Appendix.
(a) Simulations on the effect of the linear speedup
 (b) Simulations on the effect of the heterogeneity level
Figure 2: Performance of FedTD(0) under Markovian sampling. (a)Performance of FedTD(0) for varying
number of agents N. The MDPM(1)of the first agent is randomly generated with a state space of size
n= 100. The remaining MDPs are perturbations of M(1)with the heterogeneity levels ϵ= 0.05andϵ1= 0.1.
We evaluate the convergence in terms of the running error et=∥¯θt−θ∗
1∥2.(b)Performance of FedTD(0) for
varying heterogeneity level, with a fixed number of agents N= 20. Complying with theory, increasing N
reduces the error, and increasing the level of heterogeneity increases the size of the ball to which FedTD(0)
converges. We choose the number of local steps as K= 10in both plots.
6 Conclusion
In this work, we have studied the problem of federated reinforcement learning under environmental hetero-
geneity and explored the following question: Can an agent expedite the process of learning its own value
function by using information from agents interacting with potentially different MDPs? To answer this
question, we studied the convergence of a federated TD(0) algorithm with linear function approximation,
whereNagents under different environments collaboratively evaluate a common policy. The main differences
from the existing works are: (i) proposing a new definition of environmental heterogeneity; (ii) characterizing
the effect of heterogeneity on TD(0) fixed points; (iii) introducing a virtual MDP to analyze the long-term
behavior of the FedTD(0) algorithm; and (iv) making an explicit connection between federated reinforcement
learning and federated supervised learning/optimization by leveraging the virtual MDP. With these elements,
we proved that if the environmental heterogeneity between agents’ environments is small, then FedTD(0) can
achieve a linear speedup under both i.i.d and Markovian settings, and with multiple local updates.
A few interesting extensions to this work are as follows. First, it is natural to study federated variants of
other RL algorithms beyond the TD(0) algorithm. Second, it would be interesting to investigate whether
the personalization techniques used in the traditional FL optimization literature can be applied to solve
federated RL problems. Instead of learning a common value function/policy, can we design personalized value
functions/policies that might perform better in high-heterogeneity regimes? We leave the exploration of this
interesting question as future work.
Acknowledgments
Anderson and Wang are partially supported by the NSF under awards 2144634 & 2231350 from the EPCN
program.
12Published in Transactions on Machine Learning Research (06/2024)
Hamed Hassani is supported by The Institute for Learning-enabled Optimization at Scale (TILOS), under
award number NSF-CCF-2112665.
References
Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N Whatmough, and
Venkatesh Saligrama. Federated learning based on dynamic regularization. arXiv preprint arXiv:2111.04263 ,
2021.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with
linear function approximation. In Conference on learning theory , pp. 1691–1692. PMLR, 2018.
Vivek S Borkar. Stochastic approximation: A dynamical systems viewpoint , volume 48. Springer, 2009.
Zachary Charles and Jakub Konečn` y. On the outsized importance of learning rates in local update methods.
arXiv preprint arXiv:2007.00878 , 2020.
Zachary Charles and Jakub Konečn` y. Convergence and Accuracy Trade-Offs in Federated Learning and
Meta-Learning. In International Conference on Artificial Intelligence and Statistics , pp. 2575–2583. PMLR,
2021.
Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. Deepdriving: Learning affordance for direct
perception in autonomous driving. In Proceedings of the IEEE international conference on computer vision ,
pp. 2722–2730, 2015.
Zaiwei Chen, Sheng Zhang, Thinh T Doan, Siva Theja Maguluri, and John-Paul Clarke. Performance
of Q-learning with linear function approximation: Stability and finite-time analysis. arXiv preprint
arXiv:1905.11425 , pp. 4, 2019.
Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for
personalized federated learning. In International Conference on Machine Learning , pp. 2089–2099. PMLR,
2021.
Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. FedAvg with fine tuning: Local
updates lead to representation learning. arXiv preprint arXiv:2205.13692 , 2022.
Nicolò Dal Fabbro, Aritra Mitra, and George J Pappas. Federated TD learning over finite-rate erasure
channels: Linear speedup under markovian sampling. IEEE Control Systems Letters , 2023.
Gal Dalal, Balázs Szörényi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for TD(0) with function
approximation. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018.
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated learning.
arXiv preprint arXiv:2003.13461 , 2020.
Thinh Doan, Siva Maguluri, and Justin Romberg. Finite-time analysis of distributed TD(0) with linear
function approximation on multi-agent reinforcement learning. In International Conference on Machine
Learning , pp. 1626–1635. PMLR, 2019.
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-learning
approach. arXiv preprint arXiv:2002.07948 , 2020.
Georg Frobenius, Ferdinand Georg Frobenius, Ferdinand Georg Frobenius, Ferdinand Georg Frobenius, and
Germany Mathematician. Über matrizen aus nicht negativen elementen. 1912.
Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for clustered
federated learning. Advances in Neural Information Processing Systems , 33:19586–19597, 2020.
Eduard Gorbunov, Filip Hanzely, and Peter Richtárik. Local SGD: Unified theory and new efficient methods.
InInternational Conference on Artificial Intelligence and Statistics , pp. 3556–3564. PMLR, 2021.
13Published in Transactions on Machine Learning Research (06/2024)
Taosha Guo, Abed AlRahman Al Makdah, Vishaal Krishnan, and Fabio Pasqualetti. Imitation and transfer
learning for LQG control. arXiv preprint arXiv:2303.09002 , 2023.
Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated learning.
arXiv preprint arXiv:1910.14425 , 2019.
Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Local SGD with
periodic averaging: Tighter analysis and adaptive synchronization. In Advances in Neural Information
Processing Systems , pp. 11082–11094, 2019.
Filip Hanzely, Slavomír Hanzely, Samuel Horváth, and Peter Richtárik. Lower bounds and optimal algorithms
for personalized federated learning. Advances in Neural Information Processing Systems , 33:2304–2315,
2020.
Roger A Horn and Charles R Johnson. Matrix analysis . Cambridge university press, 2012a.
Roger A Horn and Charles R Johnson. Matrix analysis . Cambridge university press, 2012b.
Hao Jin, Yang Peng, Wenhao Yang, Shusen Wang, and Zhihua Zhang. Federated Reinforcement Learning
with Environment Heterogeneity. In International Conference on Artificial Intelligence and Statistics , pp.
18–37. PMLR, 2022.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In Inter-
national Conference on Machine Learning , pp. 5132–5143. PMLR, 2020a.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In Inter-
national Conference on Machine Learning , pp. 5132–5143. PMLR, 2020b.
Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine
learning, 49(2):209–232, 2002.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. First analysis of local GD on heterogeneous
data.arXiv preprint arXiv:1909.04715 , 2019.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. Tighter theory for local SGD on identical and
heterogeneous data. In International Conference on Artificial Intelligence and Statistics , pp. 4519–4529.
PMLR, 2020.
Koulik Khamaru, Ashwin Pananjady, Feng Ruan, Martin J Wainwright, and Michael I Jordan. Is temporal
difference learning optimal? An instance-dependent analysis. SIAM Journal on Mathematics of Data
Science, 3(4):1013–1040, 2021.
Sajad Khodadadian, Pranay Sharma, Gauri Joshi, and Siva Theja Maguluri. Federated Reinforcement
Learning: Linear Speedup Under Markovian Sampling. In International Conference on Machine Learning ,
pp. 10997–11057. PMLR, 2022.
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian U Stich. A unified theory of
decentralized SGD with changing topology and local updates. arXiv preprint arXiv:2003.10422 , 2020.
Jakub Konečn` y, H Brendan McMahan, Daniel Ramage, and Peter Richtárik. Federated optimization:
Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527 , 2016.
Nathaniel Korda and Prashanth La. On TD(0) with function approximation: Concentration bounds and
a centered variant with exponential convergence. In International conference on machine learning , pp.
626–634. PMLR, 2015.
Yassine Laguel, Krishna Pillutla, Jerôme Malick, and Zaid Harchaoui. A superquantile approach to federated
learning with heterogeneous devices. In 2021 55th Annual Conference on Information Sciences and Systems
(CISS), pp. 1–6. IEEE, 2021.
14Published in Transactions on Machine Learning Research (06/2024)
Chandrashekar Lakshminarayanan and Csaba Szepesvári. Linear stochastic approximation: Constant step-size
and iterate averaging. arXiv preprint arXiv:1709.04073 , 2017.
David A Levin and Yuval Peres. Markov chains and mixing times , volume 107. American Mathematical Soc.,
2017.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on
non-iid data. arXiv preprint arXiv:1907.02189 , 2019.
Boyi Liu, Lujia Wang, and Ming Liu. Lifelong federated reinforcement learning: a learning architecture for
navigation in cloud robotic systems. IEEE Robotics and Automation Letters , 4(4):4555–4562, 2019.
Rui Liu and Alex Olshevsky. Distributed TD(0) with almost no communication. arXiv preprint
arXiv:2104.07855 , 2021a.
Rui Liu and Alex Olshevsky. Temporal difference learning as gradient splitting. In International Conference
on Machine Learning , pp. 6905–6913. PMLR, 2021b.
Grigory Malinovskiy, Dmitry Kovalev, Elnur Gasanov, Laurent Condat, and Peter Richtarik. From Local
SGD to Local Fixed-Point Methods for Federated Learning. In International Conference on Machine
Learning , pp. 6692–6701. PMLR, 2020.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-
efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics , pp.
1273–1282. PMLR, 2017.
Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richtárik. ProxSkip: Yes! Local
Gradient Steps Provably Lead to Communication Acceleration! Finally! arXiv preprint arXiv:2202.09357 ,
2022.
Aritra Mitra. A simple finite-time analysis of TD learning with linear function approximation. arXiv preprint
arXiv:2403.02476 , 2024.
Aritra Mitra, Rayana Jaafar, George J Pappas, and Hamed Hassani. Linear convergence in federated learning:
Tackling client heterogeneity and sparse gradients. Advances in Neural Information Processing Systems , 34:
14606–14619, 2021.
C Narayanan and Csaba Szepesvári. Finite time bounds for temporal difference learning with function
approximation: Problems with some “state-of-the-art” results. Technical report, Technical report, 2017.
Colm Art O’cinneide. Entrywise perturbation theory and error analysis for markov chains. Numerische
Mathematik , 65(1):109–120, 1993.
Reese Pathak and Martin J Wainwright. FedSplit: An algorithmic framework for fast federated optimization.
arXiv preprint arXiv:2005.05238 , 2020.
Hossein Pishro-Nik. Introduction to probability, statistics, and random processes. 2016.
Jiaju Qi, Qihao Zhou, Lei Lei, and Kan Zheng. Federated reinforcement learning: techniques, applications,
and open challenges. arXiv preprint arXiv:2108.11887 , 2021.
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. Fed-
paq: A communication-efficient federated learning method with periodic averaging and quantization. In
International Conference on Artificial Intelligence and Statistics , pp. 2021–2031. PMLR, 2020.
Anit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and Virginia Smith. On the
convergence of federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127 , 3,
2018.
15Published in Transactions on Machine Learning Research (06/2024)
Felix Sattler, Klaus-Robert Müller, and Wojciech Samek. Clustered federated learning: Model-agnostic
distributed multitask optimization under privacy constraints. IEEE transactions on neural networks and
learning systems , 32(8):3710–3722, 2020.
Han Shen, Kaiqing Zhang, Mingyi Hong, and Tianyi Chen. Towards understanding asynchronous advantage
actor-critic: Convergence and linear speedup. IEEE Transactions on Signal Processing , 2023.
Artin Spiridonoff, Alex Olshevsky, and Ioannis Ch Paschalidis. Local SGD With a Communication Overhead
Depending Only on the Number of Workers. arXiv preprint arXiv:2006.02582 , 2020.
Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation and TD
learning. In Conference on Learning Theory , pp. 2803–2830. PMLR, 2019.
Sebastian U Stich. Local SGD converges fast and communicates little. arXiv preprint arXiv:1805.09767 ,
2018.
Lili Su, Jiaming Xu, and Pengkun Yang. Global convergence of federated learning for mixed regression. arXiv
preprint arXiv:2206.07279 , 2022.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning , 3(1):9–44,
1988.
Richard S Sutton, Andrew G Barto, et al. Introduction to Reinforcement learning. 1998.
Canh T Dinh, Nguyen Tran, and Josh Nguyen. Personalized federated learning with moreau envelopes.
Advances in Neural Information Processing Systems , 33:21394–21405, 2020.
Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards personalized federated learning. IEEE
Transactions on Neural Networks and Learning Systems , 2022.
John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function approxi-
mation. In IEEE Transactions on Automatic Control , 1997.
Han Wang, Siddartha Marella, and James Anderson. FedADMM: A federated primal-dual algorithm allowing
partial participation. In 2022 IEEE 61st Conference on Decision and Control (CDC) , pp. 287–294. IEEE,
2022a.
Han Wang, Leonardo F Toso, and James Anderson. FedSysID: A federated approach to sample-efficient
system identification. arXiv preprint arXiv:2211.14393 , 2022b.
Han Wang, Leonardo F Toso, Aritra Mitra, and James Anderson. Model-free learning with heterogeneous
dynamical systems: A federated LQR approach. arXiv preprint arXiv:2308.11743 , 2023.
Jianyu Wang and Gauri Joshi. Cooperative SGD: A unified framework for the design and analysis of
communication-efficient SGD algorithms. arXiv preprint arXiv:1808.07576 , 2018.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency
problem in heterogeneous federated optimization. Advances in Neural Information Processing Systems , 33,
2020.
XiaofeiWang, YiwenHan, ChenyangWang, QiyangZhao, XuChen, andMinChen. In-EdgeAI:Intelligentizing
mobile edge computing, caching and communication by federated learning. IEEE Network , 33(5):156–165,
2019.
Blake Woodworth, Kumar Kshitij Patel, Sebastian U Stich, Zhen Dai, Brian Bullins, H Brendan McMa-
han, Ohad Shamir, and Nathan Srebro. Is Local SGD Better than Minibatch SGD? arXiv preprint
arXiv:2002.07839 , 2020a.
Blake E Woodworth, Kumar Kshitij Patel, and Nati Srebro. Minibatch vs local SGD for heterogeneous
distributed learning. Advances in Neural Information Processing Systems , 33:6281–6292, 2020b.
16Published in Transactions on Machine Learning Research (06/2024)
Zhijie Xie and Shenghui Song. Fedkl: Tackling data heterogeneity in federated reinforcement learning by
penalizing kl divergence. IEEE Journal on Selected Areas in Communications , 41(4):1227–1242, 2023.
Lei Xin, Lintao Ye, George Chiu, and Shreyas Sundaram. Learning dynamical systems by leveraging data
from similar systems. arXiv preprint arXiv:2302.04344 , 2023.
Chenyu Zhang, Han Wang, Aritra Mitra, and James Anderson. Finite-time analysis of on-policy heterogeneous
federated reinforcement learning. International Conference on Learning Representations , 2024.
17Published in Transactions on Machine Learning Research (06/2024)
Contents
1 Introduction 1
1.1 Our Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Model and Problem Formulation 5
3 Heterogeneous Federated RL 6
3.1 Virtual Markov Decision Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4 Federated TD Algorithm 8
5 Main Result and Analysis 9
5.1 Main Technical Challenges and Overview of the Novel Ingredients in Our Analysis . . . . . . 11
6 Conclusion 12
A Additional Literature Survey 20
B Perturbation bounds for TD(0) fixed points 21
B.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C Properties of the Virtual Markov Decision Process 22
C.1 Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.2 Proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.3 Proof of Proposition 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
D Pseudo-gradient heterogeneity: Proof of Lemma 2 23
E Auxiliary results used in the I.I.D. and Markovian settings 23
F Notation 26
G Warm-up: Analysis of FedTDunder i.i.d. sampling 27
G.1 Auxiliary lemmas for Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
G.1.1 Variance reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
G.1.2 Per Round Progress . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
G.1.3 Drift Term Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
G.1.4 Parameter Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
G.2 Proof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
18Published in Transactions on Machine Learning Research (06/2024)
H Heterogeneity bias: Proof of Theorem 3 35
I Proof of the Markovian setting 36
I.1 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
I.2 Auxiliary lemmas for Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
I.2.1 Decomposition Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
I.2.2 Variance Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
I.2.3 Bounding E/bracketleftig/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble2/bracketrightig
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
I.2.4 Drift Term Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
I.2.5 Per Round Progress . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
I.2.6 Parameter Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
I.3 Proof of Theorem 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
J Simulation Results 60
J.1 Simulation results for the I.I.D. setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
J.2 Simulation results for the Markovian setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
J.3 Simulation on the effect of the heterogeneity level for the Markovian setting. . . . . . . . . . 62
19Published in Transactions on Machine Learning Research (06/2024)
A Additional Literature Survey
Federated Learning Algorithms . The literature on algorithmic developments in federated learning is vast;
as such, we only cover some of the most relevant/representative works here. The most popularly used FL
algorithm, FedAvg, was first introduced in McMahan et al. (2017). Several works went on to provide a detailed
theoretical analysis of FedAvgboth in the homogeneous case when all clients minimize the same objective
function (Stich, 2018; Wang & Joshi, 2018; Spiridonoff et al., 2020; Reisizadeh et al., 2020; Haddadpour et al.,
2019; Woodworth et al., 2020a), and also in the more challenging heterogeneous setting (Khaled et al., 2019;
2020; Haddadpour & Mahdavi, 2019; Li et al., 2019; Koloskova et al., 2020). In the latter scenario, it was
soon realized that FedAvgsuffers from a “client-drift" effect that hurts its convergence performance (Charles
& Konečn` y, 2020; 2021; Karimireddy et al., 2020a).
Since then, a lot of effort has gone into improving the convergence guarantees of FedAvgvia a variety of
technical approaches: proximal methods in FedProx (Sahu et al., 2018); operator-splitting in FedSplit
(Pathak & Wainwright, 2020); variance-reduction in Scaffold (Karimireddy et al., 2020a) and S-Local-SVRG
(Gorbunov et al., 2021); gradient-tracking in FedLin(Mitra et al., 2021); dynamic regularization in Acar et al.
(2021); and ADMM in FedADMM (Wang et al., 2022a). While these methods improved upon FedAvgin various
ways, they all fell short of providing any theoretical justification for performing multiple local updates under
arbitrary statistical heterogeneity. Very recently, the authors in Mishchenko et al. (2022) introduced the
ProxSkip algorithm, and showed that it can indeed lead to communication savings via local steps, despite
arbitrary heterogeneity.
Some other approaches to tackling heterogeneous statistical distributions in FL include personalization (Deng
et al., 2020; Fallah et al., 2020; T Dinh et al., 2020; Hanzely et al., 2020; Tan et al., 2022), clustering (Ghosh
et al., 2020; Sattler et al., 2020; Su et al., 2022), representation learning (Collins et al., 2021), and the use of
quantiles (Laguel et al., 2021).
20Published in Transactions on Machine Learning Research (06/2024)
B Perturbation bounds for TD(0) fixed points
B.1 Proof of Theorem 1
In this section, we prove the perturbation bounds on TD(0) fixed points shown in Theorem 1. We start by
observing that:
∥¯Ai−¯Aj∥=∥Φ⊤D(i)(Φ−γP(i)Φ)−Φ⊤D(j)(Φ−γP(j)Φ)∥
≤∥Φ⊤D(i)(Φ−γP(i)Φ)−Φ⊤D(i)(Φ−γP(j)Φ)+
Φ⊤D(i)(Φ−γP(j)Φ)−Φ⊤D(j)(Φ−γP(j)Φ)∥
≤∥Φ⊤D(i)(Φ−γP(i)Φ)−Φ⊤D(i)(Φ−γP(j)Φ)∥+
∥Φ⊤D(i)(Φ−γP(j)Φ)−Φ⊤D(j)(Φ−γP(j)Φ)∥
(a)
≤γ∥Φ∥2∥D(i)∥∥P(i)−P(j)∥+∥Φ∥2∥D(i)−D(j)∥∥(I−γP(j))∥
(b)
≤γ√nϵ+ (1 +γ)[2(n−1)ϵ+O(ϵ2)], (5)
where (a) follows from the triangle inequality. The first term in (b) uses the fact that ∥Φ∥≤1,∥D(i)∥≤1,
and
∥P(i)−P(j)∥≤√n∥P(i)−P(j)∥∞≤ϵ√n∥P(i)∥∞=ϵ√n,
where we use Assumption 1 in the second inequality. The second term in (b) uses the the facts that
∥I−γP(j)∥≤1 +γ,∥D(i)−D(j)∥≤∥D(i)−D(j)∥1≤∥π(i)−π(j)∥1, along with Lemma 1.
Next, we bound
∥¯bi−¯bj∥=∥ΦD(i)R(i)−ΦD(j)R(j)∥
≤∥ΦD(i)R(i)−ΦD(i)R(j)∥+∥ΦD(i)R(j)−ΦD(j)R(j)∥
≤∥Φ∥∥D(i)∥∥R(i)−R(j)∥+∥Φ∥∥D(i)−D(j)∥∥R(j)∥
≤ϵ1+Rmax/parenleftbig
2(n−1)ϵ+O(ϵ2)/parenrightbig
, (6)
where we use Assumption 2 in the last inequality and follow the same reasoning as we used to bound ∥¯Ai−¯Aj∥
above.
We are now ready to bound the gap between fixed points as:
∥θ∗
i−θ∗
j∥
∥θ∗
i∥≤κ(¯Ai)
1−κ(¯Ai)∥¯Ai−¯Aj∥
∥¯Ai∥/parenleftbigg∥¯Ai−¯Aj∥
∥¯Ai∥+∥¯bi−¯bj∥
∥¯bi∥/parenrightbigg
. (7)
Here, we leveraged the perturbation theory of linear equations in (Horn & Johnson, 2012b) Section 5.8.
Finally, for any∥θ∗
i∥≤H, we have
∥θ∗
i−θ∗
j∥≤Γ(ϵ,ϵ1)≜κ(¯Ai)H
1−κ(¯Ai)A(ϵ)
δ1/parenleftbiggA(ϵ)
δ1+b(ϵ,ϵ1)
δ2/parenrightbigg
,
where we used the fact that δ1andδ2are positive constants that lower bound ∥¯Ai∥and∥¯bi∥, respectively.
21Published in Transactions on Machine Learning Research (06/2024)
C Properties of the Virtual Markov Decision Process
C.1 Proof of Proposition 1
Before we prove this proposition, we present the following fact from (Pishro-Nik, 2016): a Markov matrix P
is irreducible and aperiodic if and only if there exists a positive integer ksuch that every entry of the matrix
Pkis strictly positive, i.e., Pk
s,s′>0,for alls,s′∈S.
For every Markov matrix P(i), we know that there exists such an integer kiaccording to the above fact and
Assumption 3 in the paper. Then we define a set J={i∈[N]|wi>0}. Since/summationtextN
i=1wi= 1, andwi≥0holds
for alli∈[N], we know that Jis a non-empty set. If we define ¯k=mini∈[J]{ki}andj=arg mini∈[J]{ki},
then we have:

/summationdisplay
i∈[N]wiP(i)
¯k
=w¯k
j/parenleftig
P(j)/parenrightig¯k
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
positive+······/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
nonnegative, (8)
where each entry of w¯k
j/parenleftbig
P(j)/parenrightbig¯kis strictly positive while the other matrices in the summation are non-negative.
Thus, we can conclude that the Markov chain associated with the Markov matrix/summationtext
i∈[N]wiP(i)is also
irreducible and aperiodic.
C.2 Proof of Proposition 2
Following similar arguments as in Theorem 1, we bound ∥¯Ai−¯A∥:
∥¯Ai−¯A∥=∥Φ⊤D(i)(Φ−γP(i)Φ)−Φ⊤¯D(Φ−γ¯PΦ)∥
(a)
≤γ∥Φ∥2∥D(i)∥∥P(i)−¯P∥+∥Φ∥2∥D(i)−¯D∥∥(I−γ¯P)∥
(b)
≤γ√nϵ+ (1 +γ)[2(n−1)ϵ+O(ϵ2)] =A(ϵ), (9)
where inequality (a) follows the same reasoning as (a) in Eq.(5), (b) uses the same fact as (b) in Eq.(5), and
∥P(i)−¯P∥≤1
N/summationtextN
j=1∥P(i)−P(j)∥≤ϵ√nand∥D(i)−¯D∥≤2(n−1)ϵ+O(ϵ2).
Based on the above facts: (i) ∥¯R∥≤1
N/summationtextN
i=1∥R(i)∥≤Rmax, (ii)∥R(i)−¯R∥≤1
N/summationtextN
j=1∥R(i)−R(j)∥≤ϵ1
and (iii)∥D(i)−¯D∥≤2(n−1)ϵ+O(ϵ2), we finish the proof by showing that ∥¯bi−¯b∥≤b(ϵ,ϵ1). To do so, we
follow the same steps as Eq. (6), and prove the bound on ∥θ∗
i−θ∗∥by following the same analysis as Eq. (7).
C.3 Proof of Proposition 3
Since the virtual MDP is an average of the agents’ MDPs, i.e., ¯P=1
N/summationtextN
i=1P(i),the virtual Markov chain is
irreducible and aperiodic from Proposition 1. The maximum eigenvalue of a symmetric positive-semidefinite
matrix is a convex function. Then we have λmax(Φ⊤¯DΦ)≤/summationtext
s∈S¯π(s)λmax/parenleftbig
ϕ(s)ϕ(s)⊤/parenrightbig
≤/summationtext
s∈S¯π(s) = 1.
To show that there exists ω>0such thatλmin(Φ⊤¯DΦ)≥ω>0, we will establish that Φ⊤¯DΦis a positive-
definite matrix. Since Φis full-column rank, this amounts to showing that ¯Dis a positive definite matrix.
From the definition of ¯D, establishing positive-definiteness of ¯Dis equivalent to arguing that every element
of the stationary distribution vector ¯πis strictly positive; here, ¯π⊤¯P=¯π.To that end, from Proposition 1,
we know that the Markov chain associated with ¯Pis aperiodic and irreducible. From the Perron-Frobenius
theorem (Frobenius et al., 1912), we conclude that indeed every entry of ¯πis strictly positive. If we choose
ω= mins∈S{¯π(s)}>0,we haveλmin(Φ⊤¯DΦ)≥ω>0.
22Published in Transactions on Machine Learning Research (06/2024)
D Pseudo-gradient heterogeneity: Proof of Lemma 2
For eachθ∈H, we have:
/vextenddouble/vextenddouble/vextenddouble¯g(θ)−1
NN/summationdisplay
i=1¯gi(θ)/vextenddouble/vextenddouble/vextenddouble=/vextenddouble/vextenddouble/vextenddoubleΦT¯D(¯TµΦθ−Φθ)−1
N/parenleftigN/summationdisplay
i=1ΦTD(i)(T(i)
µΦθ−Φθ)/parenrightig/vextenddouble/vextenddouble/vextenddouble
(a)
≤1
NN/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddoubleΦT¯D(¯TµΦθ−Φθ)−ΦTD(i)(T(i)
µΦθ−Φθ)/vextenddouble/vextenddouble/vextenddouble
(b)
≤1
NN/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble¯D/bracketleftig1
NN/summationdisplay
j=1R(j)+γ1
NN/summationdisplay
j=1P(j)Φθ−Φθ/bracketrightig
−D(i)(T(i)
µΦθ−Φθ)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤1
NN/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble¯D/bracketleftig1
NN/summationdisplay
j=1R(j)+γ1
NN/summationdisplay
j=1P(j)Φθ−Φθ/bracketrightig
−¯D(T(i)
µΦθ−Φθ)
+¯D(T(i)
µΦθ−Φθ)−D(i)(T(i)
µΦθ−Φθ)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
(c)
≤1
NN/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble¯D/bracketleftig1
NN/summationdisplay
j=1R(j)+γ1
NN/summationdisplay
j=1P(j)Φθ−Φθ/bracketrightig
−¯D(T(i)
µΦθ−Φθ)/vextenddouble/vextenddouble/vextenddouble
+1
NN/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble¯D(T(i)
µΦθ−Φθ)−D(i)(T(i)
µΦθ−Φθ)/vextenddouble/vextenddouble/vextenddouble
≤1
NN/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble¯D/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
NN/summationdisplay
j=1R(j)−R(i)/vextenddouble/vextenddouble/vextenddouble+γ/vextenddouble/vextenddouble/vextenddouble1
NN/summationdisplay
j=1P(j)−P(i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleΦθ/vextenddouble/vextenddouble/vextenddouble
+1
NN/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble¯D−D(i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleT(i)
µΦθ−Φθ/vextenddouble/vextenddouble/vextenddouble
(d)
≤1
NN/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble1
NN/summationdisplay
j=1R(j)−R(i)/vextenddouble/vextenddouble/vextenddouble
2+γ/vextenddouble/vextenddouble/vextenddouble1
NN/summationdisplay
j=1P(j)−P(i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleΦθ/vextenddouble/vextenddouble/vextenddouble
+1
NN/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble¯D−D(i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleT(i)
µΦθ−Φθ/vextenddouble/vextenddouble/vextenddouble
(e)
≤/bracketleftig
ϵ1+γ√nϵ∥Φθ∥+/bracketleftig
2(n−1)ϵ+O(ϵ2)/bracketrightig
∥Φθ∥
≤H/bracketleftig
O(ϵ1) +γ√nϵ+ 2(n−1)ϵ+O(ϵ2)/bracketrightig
=B(ϵ,ϵ1). (10)
Inequalities (a) and (c) follow from the triangle inequality, (b) is due to ∥Φ∥≤1; (d) is due to the fact that
∥¯D∥≤1; and (e) uses the following facts: (i) ∥R(i)−¯R∥≤ϵ1; (ii)∥P(i)−P(j)∥≤√n∥P(i)−P(j)∥∞≤
ϵ√n∥P(i)∥∞=ϵ√n, which, in turn, follows from the proof of Theorem 1; (iii) ∥D(i)−¯D∥≤2(n−1)ϵ+O(ϵ2),
which, in turn, follows from the proof of Theorem 1 or Eq. (5); and (iv) ∥θ∥≤Hfor anyθ∈H.
E Auxiliary results used in the I.I.D. and Markovian settings
We make repeated use throughout the appendix (often without explicitly stating so) of the following
inequalities:
•Given any two vectors x,y∈Rd, for anyβ >0, we have
∥x+y∥2≤(1 +β)∥x∥2+/parenleftbigg
1 +1
β/parenrightbigg
∥y∥2. (11)
23Published in Transactions on Machine Learning Research (06/2024)
•Given any two vectors x,y∈Rd, for anyβ >0, we have
⟨x,y⟩≤β
2∥x∥2+1
2β∥y∥2. (12)
This inequality goes by the name of Young’s inequality.
•Givenmvectorsx1,...,xm∈Rd, the following is a simple application of Jensen’s inequality:
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1xi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤mm/summationdisplay
i=1∥xi∥2. (13)
We prove the following result for the virtual MDP.
Lemma 3. For anyθ1,θ2∈Rd,
(θ2−θ1)⊤[¯g(θ1)−¯g(θ2)]≥(1−γ)/vextenddouble/vextenddouble/vextenddoubleˆVθ1−ˆVθ2/vextenddouble/vextenddouble/vextenddouble2
¯D. (14)
Proof.Consider a stationary sequence of states with random initial state s∼¯πand subsequent state s′, which,
conditioned on s, is drawn from ¯P(·|s). Defineϕ≜ϕ(s)andϕ′≜ϕ(s′). Defineχ1≜ˆVθ2(s)−ˆVθ1(s) =
(θ2−θ1)⊤ϕandχ2≜ˆVθ2(s′)−ˆVθ1(s′)=(θ2−θ1)⊤ϕ′. By stationarity, χ1andχ2are two correlated
random variables with the same same marginal distribution. By definition, E/bracketleftbig
χ2
1/bracketrightbig
=E/bracketleftbig
χ2
2/bracketrightbig
=/vextenddouble/vextenddouble/vextenddoubleˆVθ2−ˆVθ2/vextenddouble/vextenddouble/vextenddouble2
¯D
sinces,s′are drawn from ¯π. And we have,
¯g(θ1)−¯g(θ2) =E/bracketleftig
ϕ(γϕ′−ϕ)⊤(θ1−θ2)/bracketrightig
=E[ϕ(χ1−γχ2)].
Therefore,
(θ2−θ1)⊤[¯g(θ1)−¯g(θ2)] =E[χ1(χ1−γχ2)]
=E/bracketleftbig
χ2
1/bracketrightbig
−γE[χ1χ2]
≥(1−γ)E/bracketleftbig
χ2
1/bracketrightbig
= (1−γ)/vextenddouble/vextenddouble/vextenddoubleˆVθ2−ˆVθ2/vextenddouble/vextenddouble/vextenddouble2
¯D,
where we use the Cauchy-Schwartz inequality to conclude E[χ1χ2]≤/radicalbig
E[χ2
1]/radicalbig
E[χ2
2] =E/bracketleftbig
χ2
1/bracketrightbig
.
Lemma 4. For anyθ1,θ2∈Rd,we have
∥¯g(θ1)−¯g(θ2)∥≤2/vextenddouble/vextenddouble/vextenddoubleˆVθ1−ˆVθ2/vextenddouble/vextenddouble/vextenddouble¯D. (15)
Proof.Following the analysis of Lemma 3, we have
∥¯g(θ1)−¯g(θ2)∥=∥E[ϕ(χ1−γχ2)]∥
≤/radicalbig
E[∥ϕ∥2]/radicalbigg
E/bracketleftig
(χ1−γχ2)2/bracketrightig
≤/radicalig
E[χ2
1] +γ/radicalig
E[χ2
2]
= (1 +γ)/radicalig
E[χ2
1],
24Published in Transactions on Machine Learning Research (06/2024)
where the second inequality is due to ∥ϕ∥≤1and the final equality is due to E/bracketleftbig
χ2
1/bracketrightbig
=E/bracketleftbig
χ2
2/bracketrightbig
. We finish the
proof by using the fact that E/bracketleftbig
χ2
1/bracketrightbig
=/vextenddouble/vextenddouble/vextenddoubleˆVθ2−ˆVθ2/vextenddouble/vextenddouble/vextenddouble2
¯Dand1 +γ≤2.
With this Lemma, we next show that the steady-state TD(0) update direction ¯gand¯giare2-Lipschitz.
Lemma 5. (2-Lipschitzness of steady-state TD(0) update direction) For any θ1,θ2∈Rd,we have
∥¯g(θ1)−¯g(θ2)∥≤2∥θ1−θ2∥. (16)
And for each agent i∈[N],we have
∥¯gi(θ1)−¯gi(θ2)∥≤2∥θ1−θ2∥. (17)
Proof.From Lemma 4, we can easily conclude that the steady-state TD(0) update direction ¯gfor the vitual
MDP is 2-Lipschitz, i.e.,
∥¯g(θ1)−¯g(θ2)∥≤2∥θ1−θ2∥, (18)
based on the fact that λmax(Φ⊤¯DΦ)≤1. We can follow the same reasoning to prove Eq. (17)since
∥¯gi(θ1)−¯gi(θ2)∥≤2/vextenddouble/vextenddouble/vextenddoubleˆVθ1−ˆVθ2/vextenddouble/vextenddouble/vextenddouble
Diholds for each i∈[N]from Bhandari et al. (2018).
Next, we prove an analog of the Lipschitz property in Lemma 5 for the random TD(0) update direction of
each agent i.
Lemma 6. (2-Lipschitzness of random TD(0) update direction) For any θ1,θ2∈Rdandi∈[N],we have
∥gi(θ1)−gi(θ2)∥≤2∥θ1−θ2∥.
Proof.In this proof, we will use the fact that the random TD(0)update direction of agent iat thet-th
communication round and k-th local update is an affine function of the parameter θ. In particular, we have
gi(θ) =bi(O(i)
t,k)−Ai(O(i)
t,k)θ, whereAi(O(i)
t,k) =ϕ(s(i)
t,k)(ϕ⊤(s(i)
t,k)−γϕ⊤(s(i)
t,k+1))andbi(O(i)
t,k) =r(s(i)
t,k)ϕ(s(i)
t,k).
Thus, we have
∥gi(θ1)−gi(θ2)∥=/vextenddouble/vextenddouble/vextenddoubleAi(O(i)
t,k) (θ1−θ2)/vextenddouble/vextenddouble/vextenddouble
≤/vextenddouble/vextenddouble/vextenddoubleAi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble∥θ1−θ2∥
≤/parenleftig/vextenddouble/vextenddoubleϕ/parenleftbig
si
t,k/parenrightbig/vextenddouble/vextenddouble2+γ/vextenddouble/vextenddoubleϕ/parenleftbig
si
t,k/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddoubleϕ/parenleftbig
si
t,k+1/parenrightbig/vextenddouble/vextenddouble/parenrightig
∥θ1−θ2∥
≤2∥θ1−θ2∥,
where we used that ∥ϕ(s)∥≤1,∀s∈Sin the last step.
25Published in Transactions on Machine Learning Research (06/2024)
F Notation
For our subsequent analysis, we will use Ft
kto denote the filtration that captures all the randomness up to
thek-th local step in round t. We will also use Ftto represent the filtration capturing all the randomness
up to the end of round t−1. With a slight abuse of notation, Ft
−1is to be interpreted as Ft. Based on
the description of FedTD(0), it should be apparent that for each i∈[N],θ(i)
t,kisFt
k−1-measurable and ¯θtis
Ft-measurable. Furthermore, we use Etto represent the expectation conditioned on all the randomness up
to the end of round t−1.
For simplicity, we define δt=1
NK/summationtextN
i=1/summationtextK−1
k=0/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddoubleand∆t=1
NK/summationtextN
i=1/summationtextK−1
k=0/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
. The latter
term is referred to as the drift term . Note that (δt)2≤∆tholds for all tvia Jensen’s inequality. Unless
specified otherwise, ∥·∥denotes the Euclidean norm.
Step-size: Throughout the paper, we encounter three kinds of step-sizes: local step-size αl, global step-size
αg, and the effective step-size α. Some of our results will rely on effective step-sizes that decay as a function
of the communication round t; we will use{αt}to represent such a decaying effective step-size sequence.
While the local step-size αℓwill always be held constant, the decay in the effective step-size will be achieved
by making the global step-size at the server decay with the communication round. Accordingly, we will use
{α(t)
g}to represent the decaying global step-size sequence at the server. In what follows, unless specified in
the subscript, all the step-sizes appearing in the proofs refer to the effective step-size.
26Published in Transactions on Machine Learning Research (06/2024)
G Warm-up: Analysis of FedTDunder i.i.d. sampling
To isolate the effect of heterogeneity and provide key insights regarding our main proof ideas, we will analyze
a simpler i.i.d. setting in this section. Specifically, we assume that for each agent i∈[N], the data tuples
{O(i)
t,k}are sampled i.i.d. from the stationary distribution π(i)of the Markov matrix P(i). Such an i.i.d
assumption is common in the finite-time analysis of RL algorithms (Dalal et al., 2018; Bhandari et al., 2018;
Doan et al., 2019). To proceed, for a fixed θand for each i∈[N], let us define ¯gi(θ)≜EO(i)
t,k∼π(i)[gi(θ)]as the
expected TD(0) update direction at iterate θwhen the Markov tuple O(i)
t,khits its stationary distribution π(i).
We make the following standard bounded variance assumption (Bhandari et al., 2018); similar assumptions
are also made in FL analyses.
Assumption 4. E∥gi(θ)−¯gi(θ)∥2≤σ2holds for all agents i∈[N], in each round tand local update k, and
∀θ.
LetHdenote the radius of the set H. Also, define G≜Rmax+ 2Handν= (1−γ)¯ω, where ¯ωis as in
Proposition 3. Our convergence result for FedTD(0) in the i.i.d. setting is as follows.
Theorem 4. (I.I.D. Setting ) There exists a decreasing global step-size sequence {α(t)
g}, a fixed local step-size
αl, and a set of convex weights, such that a convex combination ˜θTof the global models {¯θt}satisfies the
following for each i∈[N]afterTrounds:
E/vextenddouble/vextenddouble/vextenddoubleV˜θT−Vθ∗
i/vextenddouble/vextenddouble/vextenddouble2
¯D≤˜O/parenleftigG2
K2T2+σ2
ν2NKT+σ2
ν4KT2+Q(ϵ,ϵ1)/parenrightig
, (19)
whereQ(ϵ,ϵ1) =˜O(B(ϵ,ϵ1)G
ν+ Γ2(ϵ,ϵ1)),B(ϵ,ϵ1) =H/parenleftbig√nϵ+ 2(n−1)ϵ+O(ϵ2) +O(ϵ1)/parenrightbig
, and Γ(ϵ,ϵ1)is as
defined in Theorem 1.
In what follows, we provide a detailed convergence analysis of the above result.
G.1 Auxiliary lemmas for Theorem 4
G.1.1 Variance reduction
Lemma 7. (Variance reduction in the i.i.d. setting). In the i.i.d. setting, under Assumption 4, at each
roundt, we have E/vextenddouble/vextenddouble/vextenddouble1
NK/summationtextN
i=1/summationtextK−1
k=0/bracketleftig
gi(θ(i)
t,k)−¯gi(θ(i)
t,k)/bracketrightig/vextenddouble/vextenddouble/vextenddouble2
≤σ2
NK.
Proof.DefineY(i)
t,k≜gi(O(i)
t,k,θ(i)
t,k)−¯gi(θ(i)
t,k).Since{O(i)
t,k}is drawn i.i.d. over time from its stationary
distribution π(i), we have E[Y(i)
t,k] =E/bracketleftig
E[Y(i)
t,k|θ(i)
t,k]/bracketrightig
= 0.As we mentioned before, for each i∈[N],θ(i)
t,k
isFt
k−1-measurable. If we condition on Ft
k−1, we know that θ(i)
t,kandθ(j)
t,kare deterministic and the only
randomness in Y(i)
t,kandY(j)
t,kcome fromO(i)
t,kandO(j)
t,k,which are independent. Therefore, Y(i)
t,kandY(j)
t,kare
independent conditioned on Ft
k−1.
For everyi̸=j∈[N], we have
E/bracketleftig/angbracketleftig
Y(i)
t,k,Y(j)
t,k/angbracketrightig/bracketrightig
=E/bracketleftig
E/bracketleftig/angbracketleftig
Y(i)
t,k,Y(j)
t,k/angbracketrightig
|Ft
k−1/bracketrightig/bracketrightig(a)=E/bracketleftig/angbracketleftig
E[Y(i)
t,k|Ft
k−1],E[Y(j)
t,k|Ft
k−1]/angbracketrightig/bracketrightig
= 0,(20)
where (a) follows from the fact that Y(i)
t,kandY(j)
t,kare independent conditioned on Ft
k−1.For everyk<land
i,j∈[N],
E/bracketleftig/angbracketleftig
Y(i)
t,k,Y(j)
t,l/angbracketrightig/bracketrightig
=E/bracketleftbigg
E/bracketleftbigg/angbracketleftig
Y(i)
t,k,Y(j)
t,l/angbracketrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingleFt
l−1/bracketrightbigg/bracketrightbigg
=E/bracketleftig/angbracketleftig
Y(i)
t,k,E[Y(j)
t,l|Ft
l−1]/angbracketrightig/bracketrightig
= 0. (21)
Then,
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig
gi(θ(i)
t,k)−¯gi(θ(i)
t,k)/bracketrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
27Published in Transactions on Machine Learning Research (06/2024)
=E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Y(i)
t,k/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=1
N2K2N/summationdisplay
i=1K−1/summationdisplay
k=0E∥Y(i)
t,k∥2+2
N2K2/summationdisplay
i<jK−1/summationdisplay
k=0E[⟨Y(i)
t,k,Y(j)
t,k⟩]
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
0
+2
N2K2N/summationdisplay
i,j=1/summationdisplay
k<lE[⟨Y(i)
t,k,Y(j)
t,l⟩]
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
0
≤σ2
NK,
where the second equality is due to Eq. (20)) and Eq. (21)and the last inequality is due to Assumption 4.
G.1.2 Per Round Progress
First, we characterize the error decrease at each iteration in the following lemma.
Lemma 8. (Per Round Progress). If the local step-size αlsatisfiesαl≤(1−γ)¯ω
48K, then the updates of FedTD(0)
with any global step-size αgsatisfy
E∥¯θt+1−θ∗∥2≤(1 +ζ1)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αE⟨¯g(¯θt),¯θt−θ∗⟩+ 6α2E/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+ 4α2/parenleftbigg1
ζ1+ 6/parenrightbigg
E[∆t] +2α2σ2
NK+ 2αB(ϵ,ϵ1)G+ 6α2B2(ϵ,ϵ1), (22)
whereζ1is any positive constant, and αis the effective step-size, i.e., α=Kαlαg.
Proof.
E∥¯θt+1−θ∗∥2
=E/vextenddouble/vextenddouble/vextenddoubleΠ2,H/parenleftigg
¯θt+α
NKN/summationdisplay
i=1K−1/summationdisplay
k=0gi(θ(i)
t,k)−θ∗/parenrightigg/vextenddouble/vextenddouble/vextenddouble2
(updating rule )
≤E/vextenddouble/vextenddouble/vextenddouble¯θt+α
NKN/summationdisplay
i=1K−1/summationdisplay
k=0gi(θ(i)
t,k)−θ∗/vextenddouble/vextenddouble/vextenddouble2
(projection is non-expansive )
=E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2E⟨α
NKN/summationdisplay
i=1K−1/summationdisplay
k=0gi(θ(i)
t,k),¯θt−θ∗⟩+E/vextenddouble/vextenddouble/vextenddoubleα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0gi(θ(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
=E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+2α
NKN/summationdisplay
i=1K−1/summationdisplay
k=0E⟨gi(θ(i)
t,k)−¯gi(θ(i)
t,k),¯θt−θ∗⟩
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
C1=0
+2α
NKN/summationdisplay
i=1K−1/summationdisplay
k=0E⟨¯gi(θ(i)
t,k),¯θt−θ∗⟩+E/vextenddouble/vextenddouble/vextenddoubleα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0gi(θ(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
=E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+2α
NKN/summationdisplay
i=1K−1/summationdisplay
k=0E⟨¯gi(θ(i)
t,k),¯θt−θ∗⟩+E/vextenddouble/vextenddouble/vextenddoubleα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0gi(θ(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
≤E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+2α
NKN/summationdisplay
i=1K−1/summationdisplay
k=0E⟨¯gi(θ(i)
t,k),¯θt−θ∗⟩
+ 2E/vextenddouble/vextenddouble/vextenddoubleα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig
gi(θ(i)
t,k)−¯gi(θ(i)
t,k)/bracketrightig/vextenddouble/vextenddouble/vextenddouble2
+ 2E/vextenddouble/vextenddouble/vextenddoubleα
NKN/summationdisplay
i=1¯gi(θ(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
(Young’s inequality (12) )
28Published in Transactions on Machine Learning Research (06/2024)
(a)
≤E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+2α
NKN/summationdisplay
i=1K−1/summationdisplay
k=0E⟨¯gi(θ(i)
t,k),¯θt−θ∗⟩+2σ2
NK+ 2E/vextenddouble/vextenddouble/vextenddoubleα
NKN/summationdisplay
i=1¯gi(θ(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
=E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+2α
NKN/summationdisplay
i=1K−1/summationdisplay
k=0E⟨¯gi(θ(i)
t,k)−¯gi(¯θt) + ¯gi(¯θt)−¯g(¯θt) + ¯g(¯θt),¯θt−θ∗⟩
+ 2E/vextenddouble/vextenddouble/vextenddoubleα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0¯gi(θ(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
+2α2σ2
NK
≤E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+2α
NKN/summationdisplay
i=1K−1/summationdisplay
k=0E⟨¯gi(θ(i)
t,k)−¯gi(¯θt),¯θt−θ∗⟩+2α
NN/summationdisplay
i=1E⟨¯gi(¯θt)−¯g(¯θt),¯θt−θ∗⟩
+ 2αE⟨¯g(¯θt),¯θt−θ∗⟩+ 2E/vextenddouble/vextenddouble/vextenddoubleα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0¯gi(θ(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
+2α2σ2
NK
≤(1 +ζ1)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+1
ζ1E/vextenddouble/vextenddouble/vextenddoubleα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig
¯gi(θ(i)
t,k)−¯gi(¯θt)/bracketrightig/vextenddouble/vextenddouble/vextenddouble2
+ 2αB(ϵ,ϵ1)G
+ 2αE⟨¯g(¯θt),¯θt−θ∗⟩+ 2E/vextenddouble/vextenddouble/vextenddoubleα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0¯gi(θ(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
+2α2σ2
NK(Eq (12) and Lemma 2 )
≤(1 +ζ1)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+4α2
ζ1NKN/summationdisplay
i=1K−1/summationdisplay
k=0E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
+ 2αB(ϵ,ϵ1)G
+ 2αE⟨¯g(¯θt),¯θt−θ∗⟩+ 2E/vextenddouble/vextenddouble/vextenddoubleα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0¯gi(θ(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
+2α2σ2
NK(2-Lipschitz of ¯giin Lemma 5 )
≤(1 +ζ1)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+4α2
ζ1E[∆t] +2α2σ2
NK+ 2αB(ϵ,ϵ1)G
+ 2αE⟨¯g(¯θt),¯θt−θ∗⟩+ 2E/vextenddouble/vextenddouble/vextenddoubleα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig
¯gi(θ(i)
t,k)−¯gi(¯θt) + ¯gi(¯θt)−¯g(¯θt) + ¯g(¯θt)/bracketrightig/vextenddouble/vextenddouble/vextenddouble2
≤(1 +ζ1)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+4α2
ζ1E[∆t] +2α2σ2
NK+ 2αB(ϵ,ϵ1)G
+ 2αE⟨¯g(¯θt),¯θt−θ∗⟩+ 6E/vextenddouble/vextenddouble/vextenddoubleα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0¯gi(θ(i)
t,k)−¯gi(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+ 6E/vextenddouble/vextenddouble/vextenddoubleα
NN/summationdisplay
i=1/bracketleftbig
¯gi(¯θt)−¯g(¯θt)/bracketrightbig/vextenddouble/vextenddouble/vextenddouble2
+ 6E/vextenddouble/vextenddouble/vextenddoubleα¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
(Eq (12) and Lemma 2 )
≤(1 +ζ1)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+4α2
ζ1E[∆t] +2α2σ2
NK+ 2αB(ϵ,ϵ1)G
+ 2αE⟨¯g(¯θt),¯θt−θ∗⟩+ 24α2E[∆t] (2-Lipschitz of ¯gi)
+ 6α2B2(ϵ,ϵ1) + 6α2E/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
(Eq (12) )
= (1 +ζ1)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αE⟨¯g(¯θt),¯θt−θ∗⟩+ 6α2E/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+ 4α2/parenleftbigg1
ζ1+ 6/parenrightbigg
E[∆t] +2α2σ2
NK+ 2αB(ϵ,ϵ1)G+ 6α2B2(ϵ,ϵ1), (23)
where (a) is due to Lemma 7. Furthermore, the reason why C1= 0is as follows:
C1=N/summationdisplay
i=1K−1/summationdisplay
k=0E⟨gi(θ(i)
t,k)−¯gi(θ(i)
t,k),¯θt−θ∗⟩
29Published in Transactions on Machine Learning Research (06/2024)
=N/summationdisplay
i=1K−2/summationdisplay
k=0E⟨gi(θ(i)
t,k)−¯gi(θ(i)
t,k),¯θt−θ∗⟩+
N/summationdisplay
i=1E⟨gi(θ(i)
t,K−1)−¯gi(θ(i)
t,K−1),¯θt−θ∗⟩
=N/summationdisplay
i=1K−2/summationdisplay
k=0E⟨gi(θ(i)
t,k)−¯gi(θ(i)
t,k),¯θt−θ∗⟩+
N/summationdisplay
i=1E/bracketleftig
E/bracketleftig
⟨gi(θ(i)
t,K−1)−¯gi(θ(i)
t,K−1),¯θt−θ∗⟩|Ft
K−1/bracketrightig/bracketrightig
=N/summationdisplay
i=1K−2/summationdisplay
k=0E⟨gi(θ(i)
t,k)−¯gi(θ(i)
t,k),¯θt−θ∗⟩+
N/summationdisplay
i=1E
/angbracketleftigg
¯θt−θ∗,E/bracketleftig
gi(θ(i)
t,k)−¯gi(θ(i)
t,k)|Ft
K−1/bracketrightig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
0/angbracketrightigg

=N/summationdisplay
i=1K−2/summationdisplay
k=0E⟨gi(θ(i)
t,k)−¯gi(θ(i)
t,k),¯θt−θ∗⟩.
We can keep repeating this procedure by iteratively conditioning on Ft
K−2,···,Ft
1,Ft
0.
G.1.3 Drift Term Analysis
We now turn to bounding the drift term ∆t.
Lemma 9. (Bounded Client Drift) The drift term ∆tat thet-th round can be bounded as
E[∆t] =1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
≤27(σ2+ 3KB2(ϵ,ϵ1) + 2KG2)α2
Kα2g, (24)
provided the fixed local step-size αlsatisfiesαl≤min(1−γ)¯ω
48K.
Proof.
E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
=E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−1+αlgi(θ(i)
t,k−1)−¯θt/vextenddouble/vextenddouble/vextenddouble2
(updating rule )
=E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−1+αl¯gi(θ(i)
t,k−1)−¯θt+αl/parenleftig
gi(θ(i)
t,k−1)−¯gi(θ(i)
t,k−1)/parenrightig/vextenddouble/vextenddouble/vextenddouble2
=E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−1+αl¯gi(θ(i)
t,k−1)−¯θt/vextenddouble/vextenddouble/vextenddouble2
+α2
lE/vextenddouble/vextenddouble/vextenddoublegi(θ(i)
t,k−1)−¯gi(θ(i)
t,k−1)/vextenddouble/vextenddouble/vextenddouble2
+ 2αlE/bracketleftig
E/angbracketleftig
gi(θ(i)
t,k−1)−¯gi(θ(i)
t,k−1),θ(i)
t,k−1+αl¯gi(θ(i)
t,k−1)−¯θt/vextendsingle/vextendsingle/vextendsingleFt
k−1/angbracketrightig/bracketrightig/bracketrightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
C2=0
(a)
≤(1 +ζ2)E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−1+αl¯g(θ(i)
t,k−1)−¯θt/vextenddouble/vextenddouble/vextenddouble2
+ (1 +1
ζ2)α2
lE/vextenddouble/vextenddouble/vextenddouble¯g(θ(i)
t,k−1)−¯gi(θ(i)
t,k−1)/vextenddouble/vextenddouble/vextenddouble2
+α2
lE/vextenddouble/vextenddouble/vextenddoublegi(θ(i)
t,k−1)−¯gi(θ(i)
t,k−1)/vextenddouble/vextenddouble/vextenddouble2
(b)
≤(1 +ζ2)(1 +ζ3)E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−1+αl¯g(θ(i)
t,k−1)−¯θt−αl¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
30Published in Transactions on Machine Learning Research (06/2024)
+ (1 +ζ2)(1 +1
ζ3)α2
lE/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+ (1 +1
ζ2)α2
lE/vextenddouble/vextenddouble/vextenddouble¯g(θ(i)
t,k−1)−¯g(¯θt) + ¯g(¯θt)−¯gi(¯θt) + ¯gi(¯θt)−¯gi(θ(i)
t,k−1)/vextenddouble/vextenddouble/vextenddouble2
+α2
lσ2
(c)
≤(1 +ζ2)(1 +ζ3)E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−1+αl¯g(θ(i)
t,k−1)−¯θt−αl¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+ (1 +ζ2)(1 +1
ζ3)α2
lE/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+ 3(1 +1
ζ2)α2
lE/vextenddouble/vextenddouble/vextenddouble¯g(θ(i)
t,k−1)−¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+ 3(1 +1
ζ2)α2
lE/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)−¯gi(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+ 3(1 +1
ζ2)α2
lE/vextenddouble/vextenddouble/vextenddouble¯gi(¯θt)−¯gi(θ(i)
t,k−1)/vextenddouble/vextenddouble/vextenddouble2
+α2
lσ2
(d)
≤(1 +ζ2)(1 +ζ3)/bracketleftbig
1−(2αl(1−γ)−4α2
l)¯ω/bracketrightbig
E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−1−¯θt/vextenddouble/vextenddouble/vextenddouble2
+ (1 +ζ2)(1 +1
ζ3)α2
lE/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+ 12(1 +1
ζ2)α2
lE/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−1−¯θt/vextenddouble/vextenddouble/vextenddouble2
+ 3(1 +1
ζ3)α2
lB2(ϵ,ϵ1)
+ 12(1 +1
ζ3)α2
lE/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−1−¯θt/vextenddouble/vextenddouble/vextenddouble2
+α2
lσ2
= (1 +ζ2)(1 +ζ3)/bracketleftigg
1−(2αl(1−γ)−4α2
l)¯ω+24(1 +1
ζ3)α2
l
(1 +ζ2)(1 +ζ3)/bracketrightigg
E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−1−¯θt/vextenddouble/vextenddouble/vextenddouble2
+ (1 +ζ2)(1 +1
ζ3)α2
lE/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+ 3(1 +1
ζ3)α2
lB2(ϵ,ϵ1) +α2
lσ2
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
D1,
where we used the inequality in Eq (11)with any positive constant ζ2for (a); for (b), we used Assumption 4
and the same reasoning as Eq (11)with any positive constant ζ3; for (c), we used the inequality in Eq (13)to
bound the third term; and for (d), we used Lemma 3 and Lemma 4 to bound the first term, the 2-Lipschitz
property of ¯g,¯gi(i.e., Lemma 5) in the third term and the fifth term, and the gradient heterogeneity bound
from Lemma 2 in the fourth term. If we define ζ4≜(1 +ζ2)(1 +ζ3)/bracketleftbigg
1−(2αl(1−γ)−4α2
l)¯ω+24(1+1
ζ3)α2
l
(1+ζ2)(1+ζ3)/bracketrightbigg
and defineD1as above, we have that
E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
≤ζ4E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−1−¯θt/vextenddouble/vextenddouble/vextenddouble2
+D1. (25)
Next, we set ζ2=ζ3=1
K−1,K≥2, and choose the local step-size αlto satisfy
αl(1−γ)¯ω
2≥4α2
l¯ω&αl(1−γ)¯ω
2≥24(1 +1
ζ3)α2
l
(1 +ζ2)(1 +ζ3),
so that/bracketleftbigg
1−(2αl(1−γ)−4α2
l)¯ω+24(1+1
ζ2)α2
l
(1+ζ2)(1+ζ3)/bracketrightbigg
≤1−αl(1−γ)¯ω. These inequalities hold when αl≤
min(1−γ)¯ω
48K.Then, Eq (25) becomes
E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
≤(1 +3
K−1) [1−αl(1−γ)¯ω]E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−1−¯θt/vextenddouble/vextenddouble/vextenddouble2
+D1.
If we unroll this recurrence above, using θ(i)
r,0=¯θt, we have that
E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
≤k−1/summationdisplay
s=0D1/braceleftbigg
Πk−1
j=s+1(1 +3
K−1) [1−α(1−γ)¯ω]/bracerightbigg
(e)
≤k−1/summationdisplay
s=0/bracketleftig
α2
lσ2+ 3Kα2
lB2(ϵ,ϵ1),+2α2
lKE/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2/bracketrightig
31Published in Transactions on Machine Learning Research (06/2024)
×Πk−1
j=s+1(1 +3
K−1)[1−αl(1−γ)¯ω]
≤k−1/summationdisplay
s=0/bracketleftig
α2
lσ2+ 3α2
lKB2(ϵ,ϵ1) + 2α2
lKE/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2/bracketrightig
×(1 +3
K−1)K−1Πk−1
j=s+1[1−αl(1−γ)¯ω]
(f)
≤27(σ2+ 3KB2(ϵ,ϵ1) + 2KE/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
)k−1/summationdisplay
s=0α2
l×Πk−1
j=s+1[1−α(1−γ)¯ω]
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤1
≤27(σ2+ 3KB2(ϵ,ϵ1) + 2KG2)Kα2
l(constant local step-size )
where we used the fact that (1 +ζ2)(1 +1
ζ3)≤2Kfor(e)and(1 +3
K−1)K−1≤27for(f). we finish the proof
by substituting αl=α
Kαg.
If we incorporate Eq (24) into Eq (22), we have that
E/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
(26)
≤(1 +ζ1)E/vextenddouble/vextenddouble/vextenddouble¯θr−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αE⟨¯g(¯θr),¯θr−θ∗⟩+ 6α2E/vextenddouble/vextenddouble/vextenddouble¯g(¯θr)/vextenddouble/vextenddouble/vextenddouble2
+ 108α4
Kα2g(6 +1
ζ1)(σ2+ 3KB2(ϵ,ϵ1) + 2KG2) +2α2σ2
NK+ 2αB(ϵ,ϵ1)G+ 6α2B2(ϵ,ϵ1).(27)
G.1.4 Parameter Selection
Lemma 10. Defineν≜(1−γ)¯ω. If we choose any effective step-size α=Kαgαl<(1−γ)¯ω
96,any local
step-sizeαl≤min(1−γ)¯ω
48K, and choose the constant ζ1=αν, the updates of FedTD(0) satisfy
ν1E/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D≤(1
α−ν1)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−1
αE/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
+2ασ2
NK/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
O(α1)
+1080α2
Kα2gν(σ2+ 3KB2(ϵ,ϵ1) + 2KG2)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
O(α2)+ 2B(ϵ,ϵ1)G+ 6αB2(ϵ,ϵ1)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
heterogeneity term, (28)
whereν1=ν
4=(1−γ)¯ω
4.
Proof.From Eq (26) and ζ1=αν, we know
E/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
≤(1 +ζ1)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αE⟨¯g(¯θt),¯θt−θ∗⟩+ 6α2E/vextenddouble/vextenddouble/vextenddouble¯g(¯θr)/vextenddouble/vextenddouble/vextenddouble2
+ 108α4
Kα2g(6 +1
ζ1)(σ2+ 3KB2(ϵ,ϵ1) + 2KG2) +2α2σ2
NK+ 2αB(ϵ,ϵ1)G+ 6α2B2(ϵ,ϵ1)
≤(1 +αν−2αν)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 24α2E/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D+2α2σ2
NK(Lemma 3 and 4 )
+ 108α4
Kα2g(6 +1
αν)(σ2+ 3KB2(ϵ,ϵ1) + 2KG2) + 2αB(ϵ,ϵ1)G+ 6α2B2(ϵ,ϵ1)
≤(1−αν
2)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−αν
2E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 24α2E/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D+2α2σ2
NK
32Published in Transactions on Machine Learning Research (06/2024)
+ 108α4
Kα2g(6 +1
αν)(σ2+ 3KB2(ϵ,ϵ1) + 2KG2) + 2αB(ϵ,ϵ1)G+ 6α2B2(ϵ,ϵ1)
(a)
≤(1−αν
2)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−αν
2E/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D+αν
4E/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D+2α2σ2
NK
+ 108α4
Kα2g(6 +1
αν)(σ2+ 3KB2(ϵ,ϵ1) + 2KG2) + 2αB(ϵ,ϵ1)G+ 6α2B2(ϵ,ϵ1),
where (a)comes from λmax(ΦT¯DΦ)≤1and24α2≤24α(1−γ) ¯w
96=αν
4. Moving E/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D(on the
right-hand side of (a)) to the left hand side of the above inequality yields:
αν
4E/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D≤(1−αν
2)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−E/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
+2α2σ2
NK
+ 108(6α4
Kα2g+α3
Kα2gν)(σ2+ 3KB2(ϵ,ϵ1) + 2KG2) + 2αB(ϵ,ϵ1)G+ 6α2B2(ϵ,ϵ1).
Dividing by αon both sides of the inequality above and changing νintoν1, we have:
ν1E/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D
≤(1
α−ν1)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−1
αE/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
+2ασ2
NK
+ 108(6α3
Kα2g+4α2
Kα2gν1)(σ2+ 3KB2(ϵ,ϵ1) + 2KG2) + 2B(ϵ,ϵ1)G+ 6αB2(ϵ,ϵ1)
≤(1
α−ν1)E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−1
αE/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
+2ασ2
NK/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
O(α1)
+1080α2
Kα2gν1(σ2+ 3KB2(ϵ,ϵ1) + 2KG2)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
O(α2)+ 2B(ϵ,ϵ1)G+ 6αB2(ϵ,ϵ1)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
heterogeneity term,
where we used the fact that α≤1in the last inequality.
With these lemmas, we are now ready to prove Theorem 4, which we restate for clarity.
G.2 Proof of Theorem 4
Given a fixed local step-size αl=1
2(1−γ)¯ω
48K,decreasing effective step-sizes αt=8
ν(a+t+1)=8
(1−γ)¯ω(a+t+1),
decreasing global step-sizes α(t)
g=αt
Kαl, and weights wt= (a+t), we have that
E/vextenddouble/vextenddouble/vextenddoubleV˜θT−Vθ∗
i/vextenddouble/vextenddouble/vextenddouble2
¯D≤˜O/parenleftbiggG2
K2T2+σ2
ν4KT2+σ2
ν2NKT+B(ϵ,ϵ1)G
ν+ Γ2(ϵ,ϵ1)/parenrightbigg
(29)
holds for any agent i∈[N].
Proof.We take the effective step-size αt=8
ν(a+t+1)=2
ν1(a+t+1)fora>0. In addition, we define weights
wt= (a+t)and define
˜θT=1
WT/summationdisplay
t=1wt¯θt,
whereW=/summationtextT
t=1wt≥1
2T(a+T). By convexity of positive definite quadratic forms ( λmin(ΦT¯DΦ)≥¯ω>0),
we have that
ν1E/vextenddouble/vextenddouble/vextenddoubleV˜θT−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D
33Published in Transactions on Machine Learning Research (06/2024)
≤ν1
WT/summationdisplay
t=1(a+t)E/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D
(28)
≤ν1(a+ 1)(a+ 2)G2
2W+1
WT/summationdisplay
t=1/bracketleftbigg2(a+t)αt
NKσ2/bracketrightbigg
+1
WT/summationdisplay
t=1/bracketleftbigg1080(a+t)α2
t
Kα2gν1(σ2+ 3KB2(ϵ,ϵ1) + 2KG2)/bracketrightbigg
+1
WT/summationdisplay
t=1(a+t)/bracketleftbig
2B(ϵ,ϵ1)G+ 6αtB2(ϵ,ϵ1)/bracketrightbig
≤ν1(a+ 1)(a+ 2)G2
2W+2σ2
NKWT/summationdisplay
t=1(a+t)αt
+1080(σ2+ 3KB2(ϵ,ϵ1) + 2KG2)
Kα2gν1WT/summationdisplay
t=1(a+t)α2
t+ 2B(ϵ,ϵ1)G+6B2(ϵ,ϵ1)
WT/summationdisplay
t=1(a+t)αt
≤ν1(a+ 1)(a+ 2)G2
2W+4σ2
ν1NKW·T
+4320(σ2+ 3KB2(ϵ,ϵ1) + 2KG2)
Kα2gν3
1W·(1 + log(a+T)) + 2B(ϵ,ϵ1)G+12B2(ϵ,ϵ1)
ν1W·T,
where we used/vextenddouble/vextenddouble/vextenddoubleV¯θ0−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D≤G2.Dividing by ν1on both sides, changing ν1intoν, and using W≥T(a+T)
2,
we have:
E/vextenddouble/vextenddouble/vextenddoubleV˜θT−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D≤˜O/parenleftbiggG2
K2T2+σ2
ν4KT2+σ2
ν2NKT+B(ϵ,ϵ1)G
ν/parenrightbigg
.
We finish the proof by using the following inequality: E/vextenddouble/vextenddouble/vextenddoubleV˜θT−Vθ∗
i/vextenddouble/vextenddouble/vextenddouble2
¯D≤2E/vextenddouble/vextenddouble/vextenddoubleV˜θT−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D+ 2E/vextenddouble/vextenddouble/vextenddoubleVθ∗
i−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D,
in tandem with the third point in Theorem 1.
34Published in Transactions on Machine Learning Research (06/2024)
H Heterogeneity bias: Proof of Theorem 3
In this section, we prove Theorem 3.
Proof of Theorem 3. Asθ∗
1andθ∗
2are the TD(0) fixed points of agents 1and2, respectively, we have
θ∗
1=¯A−1
1¯b1andθ∗
2=¯A−1
2¯b2. The output of mean-path FedTD(0) withk= 1andα=αgαlsatisfies:
¯θt+1=¯θt+α(−ˆA¯θt+ˆb)
=⇒¯θt+1−θ∗
1=¯θt−θ∗
1+α(−ˆA(¯θt−θ∗
1+θ∗
1) +ˆb)
=⇒e1,t+1= (I−αˆA)e1,t−αˆAθ∗
1+αˆb
=⇒e1,t+1= (I−αˆA)e1,t−α/parenleftbigg¯A1+¯A2
2/parenrightbigg
¯A−1
1¯b1+α¯b1+¯b2
2
=⇒e1,t+1= (I−αˆA)e1,t−α¯A2¯A−1
1¯b1
2+α¯b2
2
=⇒e1,t+1= (I−αˆA)e1,t−α¯A2
2/parenleftbig¯A−1
1¯b1−¯A−1
2¯b2/parenrightbig
=⇒e1,t+1= (I−αˆA)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
˜Ae1,t+α¯A2
2(θ∗
2−θ∗
1)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
˜Y. (30)
Let us now note that e1,t+1=˜Ae1,t+˜Ycan be viewed as a discrete-time linear time-invariant (LTI) system
whereαis chosen s.t. ˜Ais Schur stable, i.e., |λmax(˜A)|<1. At thet-th iteration, we have:
e1,t=˜Ate1,0+t−1/summationdisplay
k=0˜Ak˜Y.
Ast→∞,the small gain theorem tells us that because ρ(˜A)<1(whereρ(·)denotes the spectral radius),/summationtextt−1
k=0˜Akexists and is given by (I−˜A)−1. We can then conclude that
lim
t→∞e1,t= (I−˜A)−1˜Y
=/parenleftig
αˆA/parenrightig−1α¯A2
2(θ∗
1−θ∗
2)
=1
2ˆA−1¯A2(θ∗
1−θ∗
2). (31)
The limiting expression for e2,tfollows the same analysis.
35Published in Transactions on Machine Learning Research (06/2024)
I Proof of the Markovian setting
We now turn our attention to proving the main result of the paper, namely, Theorem 2.
I.1 Outline
As mentioned in the main body, one of the main obstacles to overcome in the analysis is that in general,
E[(1/N)/summationtextN
i=1/parenleftbig
gi(θ(i)
t,k,O(i)
t,k)−¯gi(θ(i)
t,k)/parenrightbig
]̸= 0. In order to show that a linear speedup is achievable, we first
decompose the random TD direction of each agent iasgi(θ(i)
t,k) =bi(O(i)
t,k)−Ai(O(i)
t,k)θ(i)
t,kin subsection I.2.1 and
show that the variances of (1/NK )/summationtextN
i=1/summationtextK−1
k=0Ai(O(i)
t,k)and(1/NK )/summationtextN
i=1/summationtextK−1
k=0bi(O(i)
t,k)get scaled down
byNKin subsection I.2.2. To decouple the randomness between the parameter θ(i)
t,kand the observations
O(i)
t,kusing the method called information theoretic control of coupling in Bhandari et al. (2018), we need
to bound E/bracketleftig/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble2/bracketrightig
in subsection I.2.3. As the analysis in the i.i.d. setting and traditional FL, we
characterize the drift term, per-iteration error decrease, and parameter selection in subsections I.2.4, I.2.5
and I.2.6, respectively. Finally, we prove Theorem 2 in subsection I.3.
Additional Notation: Under Assumption 3, for each MDP i, there exists some mi≥1and someρi∈(0,1),
such that for all t≥0and0≤k≤K−1, it holds that
dTV/parenleftig
P/parenleftig
s(i)
t,k=·|s(i)
0,0=s/parenrightig
,π(i)/parenrightig
≤miρtK+k
i,∀s∈S.
Furthermore, we define ρ= maxi∈[N]{ρi},m= maxi∈[N]{mi}.
I.2 Auxiliary lemmas for Theorem 2
I.2.1 Decomposition Form
The first step in our proof of Theorem 2 is to rewrite agent i’s update direction of FedTD(0) as:
gi(θ(i)
t,k) =−Ai(O(i)
t,k)θ(i)
t,k+bi(O(i)
t,k)
whereAi(O(i)
t,k) =ϕ(s(i)
t,k)(ϕ⊤(s(i)
t,k)−γϕ⊤(s(i)
t,k+1))andbi(O(i)
t,k) =r(s(i)
t,k)ϕ(s(i)
t,k). Note that the steady-state
value of E[bi(O(i)
t,k)]is not equal to 0. For convenience, we apply appropriate centering to rewrite gias:
gi(θ(i)
t,k) =−Ai(O(i)
t,k)(θ(i)
t,k−θ∗
i) +bi(O(i)
t,k)−Ai(O(i)
t,k)θ∗
i/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Zi(O(i)
t,k). (32)
DefineZi(O(i)
t,k)≜bi(O(i)
t,k)−Ai(O(i)
t,k)θ∗
i. As ¯gi(θ)≜EO(i)
t,k∼π(i)[gi(θ)],we have:
¯gi(θ(i)
t,k) =−¯Ai(θ(i)
t,k−θ∗
i). (33)
where ¯Ai= Φ⊤D(i)(Φ−γP(i)Φ). Note that EO(i)
t,k∼π(i)/bracketleftig
Zi(O(i)
t,k)/bracketrightig
equals to 0. Taking into account the
definitions above, we establish the following lemmas:
Lemma 11. (Uniform norm bound) There exist some constants c1,c2,c3≥0such that/vextenddouble/vextenddouble/vextenddoubleAi/parenleftig
O(i)
t,k/parenrightig/vextenddouble/vextenddouble/vextenddouble≤c1:=
1 +γ,/vextenddouble/vextenddouble/vextenddouble¯Ai/vextenddouble/vextenddouble/vextenddouble≤c2:= 1 +γand/vextenddouble/vextenddouble/vextenddoubleZi/parenleftig
O(i)
t,k/parenrightig/vextenddouble/vextenddouble/vextenddouble≤c3:=Rmax+c1Hholds for all i∈[N].
36Published in Transactions on Machine Learning Research (06/2024)
Proof.Based on the definition and the fact that ∥ϕ(s)∥≤1, we have
/vextenddouble/vextenddouble/vextenddoubleAi/parenleftig
O(i)
t,k/parenrightig/vextenddouble/vextenddouble/vextenddouble=/vextenddouble/vextenddouble/vextenddoubleϕ(s(i)
t,k)(ϕ⊤(s(i)
t,k)−γϕ⊤(s(i)
t,k+1))/vextenddouble/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/vextenddoubleϕ(s(i)
t,k)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleϕ⊤(s(i)
t,k)−γϕ⊤(s(i)
t,k+1)/vextenddouble/vextenddouble/vextenddouble≤1 +γ.
Similarly, making use of the fact that r(s)≤Rmaxfor anys∈S,we apply the same reasoning to conclude
that /vextenddouble/vextenddouble/vextenddouble¯Ai/vextenddouble/vextenddouble/vextenddouble≤1 +γ&/vextenddouble/vextenddouble/vextenddoubleZi/parenleftig
O(i)
t,k/parenrightig/vextenddouble/vextenddouble/vextenddouble≤Rmax+c1H
.
Lemma 12. There exist some constants L1,L2≥0such that
/vextenddouble/vextenddouble/vextenddouble¯Ai−E/bracketleftig
Ai/parenleftig
O(i)
t2,k2/parenrightig
|Ft1
k1/bracketrightig/vextenddouble/vextenddouble/vextenddouble≤L1ρ(t2−t1)K+k2−k1,
/vextenddouble/vextenddouble/vextenddouble¯Ai−Et1/bracketleftig
Ai/parenleftig
O(i)
t2,k2/parenrightig/bracketrightig/vextenddouble/vextenddouble/vextenddouble≤L1ρ(t2−t1)K+k2,
/vextenddouble/vextenddouble/vextenddoubleE/bracketleftig
Zi/parenleftig
O(i)
t2,k2/parenrightig
|Ft1
k1/bracketrightig/vextenddouble/vextenddouble/vextenddouble≤L2ρ(t2−t1)K+k2−k1,
/vextenddouble/vextenddouble/vextenddoubleEt1/bracketleftig
Zi/parenleftig
O(i)
t2,k2/parenrightig/bracketrightig/vextenddouble/vextenddouble/vextenddouble≤L2ρ(t2−t1)K+k2
hold for any i∈[N],0≤k1,k2≤K−1andt2≥t1≥0.
Proof.We have:
/vextenddouble/vextenddouble/vextenddoubleE/bracketleftig
Zi/parenleftig
O(i)
t2,k2/parenrightig
|Ft1
k1/bracketrightig/vextenddouble/vextenddouble/vextenddouble=/vextenddouble/vextenddouble/vextenddouble/vextenddoubleE/bracketleftig
Zi/parenleftig
O(i)
t2,k2/parenrightig
|Ft1
k1/bracketrightig
−EO(i)
t2,k2∼π(i)/bracketleftig
Zi/parenleftig
O(i)
t2,k2/parenrightig
|Ft1
k1/bracketrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
s(i)
t2,k2,s(i)
t2+1,k2+1/parenleftig
π(i)(s(i)
t2,k2)P(s(i)
t2+1,k2+1|s(i)
t2,k2)
−P(s(i)
t2,k2=·|s(i)
t1,k1)P(s(i)
t2+1,k2+1|s(i)
t2,k2)/parenrightig
Zi(O(i)
t2,k2)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤/summationdisplay
s(i)
t2,k2/vextendsingle/vextendsingle/vextendsingleπ(i)(s(i)
t2,k2)−P(s(i)
t2,k2=·|s(i)
t1,k1)/vextendsingle/vextendsingle/vextendsingle/vextenddouble/vextenddouble/vextenddoubleZi(O(i)
t2,k2)/vextenddouble/vextenddouble/vextenddouble
(a)
≤/summationdisplay
s(i)
t2,k2/vextendsingle/vextendsingle/vextendsingleπ(i)(s(i)
t2,k2)−P(s(i)
t2,k2=·|s(i)
t1,k1)/vextendsingle/vextendsingle/vextendsingle(Rmax+c1H)
= 2(Rmax+c1H)dTV/parenleftig
P/parenleftig
s(i)
t2,k2=·|s(i)
t1,k1=s/parenrightig
,π(i)/parenrightig
≤2(Rmax+c1H)miρ(t2−t1)K+k2−k1
i
where (a)is due to Lemma 11 and the last step follows from Assumption 3. We finish the proof by choosing
L2≜maxi∈[N]{2(Rmax+c1H)mi}= 2c3m. And we follow the same analysis to bound:
/vextenddouble/vextenddouble/vextenddouble¯Ai−E/bracketleftig
Ai/parenleftig
O(i)
t2,k2/parenrightig
|Ft1
k1/bracketrightig/vextenddouble/vextenddouble/vextenddouble=/vextenddouble/vextenddouble/vextenddouble/vextenddouble∥E/bracketleftig
Ai/parenleftig
O(i)
t2,k2/parenrightig
|Ft1
k1/bracketrightig
−EO(i)
t2,k2∼π(i)/bracketleftig
Ai/parenleftig
O(i)
t2,k2/parenrightig
|Ft1
k1/bracketrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
s(i)
t2,k2,s(i)
t2+1,k2+1/parenleftig
π(i)(s(i)
t2,k2)P(s(i)
t2+1,k2+1|s(i)
t2,k2)
−P(s(i)
t2,k2=·|s(i)
t1,k1)P(s(i)
t2+1,k2+1|s(i)
t2,k2)/parenrightig
Ai(O(i)
t2,k2)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤/summationdisplay
s(i)
t2,k2/vextendsingle/vextendsingle/vextendsingleπ(i)(s(i)
t2,k2)−P(s(i)
t2,k2=·|s(i)
t1,k1)/vextendsingle/vextendsingle/vextendsingle/vextenddouble/vextenddouble/vextenddoubleAi(O(i)
t2,k2)/vextenddouble/vextenddouble/vextenddouble
37Published in Transactions on Machine Learning Research (06/2024)
(b)
≤2c1dTV/parenleftig
P/parenleftig
s(i)
t2,k2=·|s(i)
t1,k1=s/parenrightig
,π(i)/parenrightig
≤2c1miρ(t2−t1)K+k2−k1
i
We finish the proof by choosing L1≜maxi∈[N]{2c1mi}= 2c1m.We employ the same reasoning to prove the
remaining three inequalities.
I.2.2 Variance Reduction
We are now ready to present the variance reduction Lemma in the Markov setting. The following Lemma estab-
lishes an analog of the variance reduction Lemma 7 in the i.i.d. setting. Based on the assumption that trajecto-
ries are independent across agents, it is easy to understand that the variance of (1/NK )/summationtextN
i=1/summationtextK−1
k=0Ai(O(i)
t,k)
and(1/NK )/summationtextN
i=1/summationtextK−1
k=0bi(O(i)
t,k)can be scaled by the number of agents N. However, it is not obvious that
the variances can be scaled by K(the number of local iterations), since the observations of each agent O(i)
t,k1
andO(i)
t,k2are correlated at different local steps k1,k2. Due to the geometric mixing property of the Markov
chain, the correlation between O(i)
t,k1andO(i)
t,k2will geometrically decay after the mixing time. Based on
this fact, we show that the variances of (1/NK )/summationtextN
i=1/summationtextK−1
k=0Ai(O(i)
t,k)and(1/NK )/summationtextN
i=1/summationtextK−1
k=0bi(O(i)
t,k)get
scaled down by NKwith an additional additive, higher order term dependent on the mixing time τ, which is
formally stated as follows:
Lemma 13. (Variance reduction in the Markovian setting) For any 0<τ <t, there exists d1,d2>0such
that:
Et−τ/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig
Ai(O(i)
t,k)−¯Ai/bracketrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketrightigg
≤d1√
NK+ 2L1ρτK, (34)
Et−τ
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig
Ai(O(i)
t,k)−¯Ai/bracketrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤d2
1
NK+ 4L2
1ρ2τK,and (35)
Et−τ/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketrightigg
≤d2√
NK+ 2L2ρτK, (36)
Et−τ
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤d2
2
NK+ 4L2
2ρ2τK, (37)
whered1≜/radicalig
(c1+c2)2+2(c1+c2)L1ρ
1−ρandd2≜/radicalig
c2
3+2c3L2ρ
1−ρ.
Proof.
Et−τ/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketrightigg
=Et−τ
/radicaltp/radicalvertex/radicalvertex/radicalbt/parenleftigg
1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/parenrightigg⊤/parenleftigg
1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/parenrightigg

≤/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtEt−τ
/parenleftigg
1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/parenrightigg⊤/parenleftigg
1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/parenrightigg

(concavity of square root and Jensen’s inequality)
38Published in Transactions on Machine Learning Research (06/2024)
=

Et−τ
1
N2K2N/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)⊤Zi(O(i)
t,k) +2
N2K2N/summationdisplay
i=1/summationdisplay
k<lZi(O(i)
t,k)⊤Zi(O(i)
t,l)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T1
+2
N2K2/summationdisplay
i<jK−1/summationdisplay
k=0Zi(O(i)
t,k)⊤Zj(O(j)
t,k)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T2+2
N2K2/summationdisplay
i<j/summationdisplay
k<lZi(O(i)
t,k)⊤Zj(O(j)
t,l)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T3


1
2
(38)
whereT1can be further bounded by:
Et−τ[T1] =Et−τ/bracketleftigg
2
N2K2N/summationdisplay
i=1/summationdisplay
k<lZi(O(i)
t,k)⊤Zi(O(i)
t,l)/bracketrightigg
=Et−τ/bracketleftigg
2
N2K2N/summationdisplay
i=1/summationdisplay
k<lZi(O(i)
t,k)⊤E/bracketleftig
Zi(O(i)
t,l)|Ft
k/bracketrightig/bracketrightigg
≤Et−τ/bracketleftigg
2
N2K2N/summationdisplay
i=1/summationdisplay
k<l/vextenddouble/vextenddouble/vextenddoubleZi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleE/bracketleftig
Zi(O(i)
t,l)|Ft
k/bracketrightig/vextenddouble/vextenddouble/vextenddouble/bracketrightigg
(Cauchy–Schwarz inequality)
≤Et−τ/bracketleftigg
2
N2K2N/summationdisplay
i=1/summationdisplay
k<lc3L2ρ(l−k)/bracketrightigg
( Lemma 11 and 12)
≤Et−τ/bracketleftigg
2
N2K2N/summationdisplay
i=1K−1/summationdisplay
k=0∞/summationdisplay
m=1c3L2ρm/bracketrightigg
=2c3L2NK
N2K2ρ
1−ρ=2c3L2ρ
NK(1−ρ).
AndT2can be bounded by:
Et−τ[T2] =2
N2K2/summationdisplay
i<jK−1/summationdisplay
k=0Et−τ/bracketleftig
Zi(O(i)
t,k)/bracketrightig⊤
Et−τ/bracketleftig
Zj(O(j)
t,k)/bracketrightig
(O(i)
t,kandO(j)
t,kare independent )
≤2
N2K2/summationdisplay
i<jK−1/summationdisplay
k=0L2
2ρ2τK+2k(Lemma 12)
≤2
KL2
2ρ2τK.
Meanwhile, T3can be bounded by:
Et−τ[T3] =2
N2K2/summationdisplay
i<j/summationdisplay
k<lEt−τ/bracketleftig
Zi(O(i)
t,k)/bracketrightig⊤
Et−τ/bracketleftig
Zj(O(j)
t,l)/bracketrightig
(O(i)
t,kandO(j)
t,lare independent )
≤2
N2K2/summationdisplay
i<j/summationdisplay
k<lL2
2ρ2τK+k+l(Lemma 12)
≤2L2
2ρ2τK
Substituting the upper bound of T1,T2andT3into Eq (38), we have:
Et−τ/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketrightigg
39Published in Transactions on Machine Learning Research (06/2024)
≤/radicaltp/radicalvertex/radicalvertex/radicalbt1
N2K2N/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/bracketleftig
Zi(O(i)
t,k)⊤Zi(O(i)
t,k)/bracketrightig
+2c3L2ρ
NK(1−ρ)+2
KL2
2ρ2τK+ 2L2
2ρ2τK
(a)
≤/radicaligg
NK
N2K2c2
3+2c3Lρ
NK(1−ρ)+2
KL2
2ρ2τK+ 2L2
2ρ2τK
≤/radicaligg
1
NK/parenleftbigg
c2
3+2c3L2ρ
1−ρ/parenrightbigg
+ 4L2
2ρ2τK (K≥1)
≤/radicaligg
1
NK/parenleftbigg
c2
3+2c3L2ρ
1−ρ/parenrightbigg
+/radicalig
4L2
2ρ2τK=/radicaligg
1
NK/parenleftbigg
c2
3+2c3L2ρ
1−ρ/parenrightbigg
+ 2L2ρτK.
where (a)used the fact that/vextenddouble/vextenddouble/vextenddoubleZi/parenleftig
O(i)
t,k/parenrightig/vextenddouble/vextenddouble/vextenddouble≤c3mentioned in Lemma 11. The proof of other inequalities
follows the same reasoning.
I.2.3 Bounding E/bracketleftig/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble2/bracketrightig
Lemma 14. (Bounding∥θt−θt−τ∥2) Consider τ=⌈τmix(α2
T)
K⌉and choose the effective step-size
α≤min/braceleftig1
30c4(τ+ 1),1
96c2
4τ,1/bracerightig
wherec4= 3c1. For anyt≥2τ, we have the following bound:
Et−2τ/bracketleftig/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble2/bracketrightig
≤8α2τ2c2
4Et−2τ/bracketleftig/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble2/bracketrightig
+ 14α2τ2d2
2
NK+52L2
2α4τ
1−ρ2
+ 4α2c2
4ττ/summationdisplay
s=0Et−2τ[∆t−s] + 3200α2c2
4c2
1τ3Γ2(ϵ,ϵ1) + 4α2c2
1τ2Γ2(ϵ,ϵ1).(39)
Proof.For anyl≥2τ, we have
/vextenddouble/vextenddouble/vextenddouble¯θl+1−¯θl/vextenddouble/vextenddouble/vextenddouble2
=/vextenddouble/vextenddouble/vextenddoubleΠ2,H/parenleftigg
¯θl+α
NKN/summationdisplay
i=1K−1/summationdisplay
k=0gi(θ(i)
l,k)/parenrightigg
−¯θl/vextenddouble/vextenddouble/vextenddouble2
≤/vextenddouble/vextenddouble/vextenddouble¯θl+α
NKN/summationdisplay
i=1K−1/summationdisplay
k=0gi(θ(i)
l,k)−¯θl/vextenddouble/vextenddouble/vextenddouble2
=α2/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig
−Ai(O(i)
l,k)/parenleftig
θ(i)
l,k−θ∗
i/parenrightig
+Zi(O(i)
l,k)/bracketrightig/vextenddouble/vextenddouble/vextenddouble2
≤2α2/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig
−Ai(O(i)
l,k)/parenleftig
θ(i)
l,k−θ∗/parenrightig
+Zi(O(i)
l,k)/bracketrightig/vextenddouble/vextenddouble/vextenddouble2
+ 2α2/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig
−Ai(O(i)
l,k)/parenleftig
θ∗−θ∗
i/parenrightig/bracketrightig/vextenddouble/vextenddouble/vextenddouble2
(a)
≤2α2/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig
−Ai(O(i)
l,k)/parenleftig
θ(i)
l,k−θ∗/parenrightig
+Zi(O(i)
l,k)/bracketrightig/vextenddouble/vextenddouble/vextenddouble2
+ 2α2c2
1Γ2(ϵ,ϵ1)
40Published in Transactions on Machine Learning Research (06/2024)
= 6α2/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Ai(O(i)
l,k)/parenleftig
θ(i)
l,k−¯θl/parenrightig/vextenddouble/vextenddouble/vextenddouble2
+ 6α2/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Ai(O(i)
l,k)/parenleftig
¯θl−θ∗/parenrightig/vextenddouble/vextenddouble/vextenddouble2
+ 6α2/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble2
+ 2α2c2
1Γ2(ϵ,ϵ1)
≤6α2/parenleftigg
c1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/vextenddouble/vextenddouble/vextenddoubleθ(i)
l,k−¯θl/vextenddouble/vextenddouble/vextenddouble/parenrightigg2
+ 6α2c2
1/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 6α2/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble2
+ 2α2c2
1Γ2(ϵ,ϵ1), (40)
where (a)comesfromtheupperboundoffixedpointsdistanceinTheorem1andthefactthat/vextenddouble/vextenddouble/vextenddoubleAi/parenleftig
O(i)
t,k/parenrightig/vextenddouble/vextenddouble/vextenddouble≤c1
in Lemma 11. Taking square root on both sides of the inequality above, we get:
/vextenddouble/vextenddouble/vextenddouble¯θl+1−¯θl/vextenddouble/vextenddouble/vextenddouble
≤3/radicaltp/radicalvertex/radicalvertex/radicalbtα2/parenleftigg
c1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/vextenddouble/vextenddouble/vextenddoubleθ(i)
l,k−¯θl/vextenddouble/vextenddouble/vextenddouble/parenrightigg2
+ 3/radicalbigg
α2c2
1/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 3/radicaltp/radicalvertex/radicalvertex/radicalbtα2/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble2
+/radicalig
2α2c2
1Γ2(ϵ,ϵ1)
≤3αc1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/vextenddouble/vextenddouble/vextenddoubleθ(i)
l,k−¯θl/vextenddouble/vextenddouble/vextenddouble+ 3αc1/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble+ 3α/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble+ 2αc1Γ(ϵ,ϵ1).(41)
By using the fact that/vextenddouble/vextenddouble/vextenddouble¯θl+1−θ∗/vextenddouble/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/vextenddouble¯θl+1−¯θl/vextenddouble/vextenddouble/vextenddouble, we have:
/vextenddouble/vextenddouble/vextenddouble¯θl+1−θ∗/vextenddouble/vextenddouble/vextenddouble≤(1 + 3αc1)/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble+3αc1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/vextenddouble/vextenddouble/vextenddoubleθ(i)
l,k−¯θl/vextenddouble/vextenddouble/vextenddouble
+ 3α/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble+ 2αc1Γ(ϵ,ϵ1).(42)
For simplicity, we define c4≜3c1andδl≜1
NK/summationtextN
i=1/summationtextK−1
k=0/vextenddouble/vextenddouble/vextenddoubleθ(i)
l,k−¯θl/vextenddouble/vextenddouble/vextenddouble. Taking the square on both sides of
Eq (42), we have:
/vextenddouble/vextenddouble/vextenddouble¯θl+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
≤(1 +αc4)2/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
+α2c2
4δ2
l+ 9α2/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble2
+ 4α2c2
1Γ2(ϵ,ϵ1)
+ 6α(1 +αc4)/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
H1+ 2αc4(1 +αc4)/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddoubleδl
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
H2
+ 6α2c4δl/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
H3+ 4α2c1c4δlΓ(ϵ,ϵ1)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
H4
41Published in Transactions on Machine Learning Research (06/2024)
+ 4αc1(1 +αc4)/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddoubleΓ(ϵ,ϵ1)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
H5+ 12α2c1/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddoubleΓ(ϵ,ϵ1)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
H6. (43)
We can further bound H1as:
H1= 6α(1 +αc4)/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble
= 2/radicalbig
3α(1 +αc4)/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble·/radicalbig
3α(1 +αc4)/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble
≤3α(1 +αc4)/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 3α(1 +αc4)/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble2
≤6α/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 6α/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble2
. (44)
where we use the fact 1 +αc4≤2in the last inequality. Similary, we can bound H2as:
H2= 2αc4(1 +αc4)/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddoubleδl≤2α/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αc2
4δ2
l. (45)
And we bound H3as:
H3= 6α2c4δl/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble≤3α2/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble2
+ 3α2c2
4δ2
l. (46)
ForH4,H5,H6, we have:
H4= 4α2c1c4δlΓ(ϵ,ϵ1)≤2α2c2
4δ2
l+ 2α2c2
1Γ2(ϵ,ϵ1),
H5= 4αc1(1 +αc4)/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddoubleΓ(ϵ,ϵ1)≤4α/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 4αc2
1Γ2(ϵ,ϵ1),
H6= 12α2c1/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddoubleΓ(ϵ,ϵ1)≤6α2/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble2
+6α2c2
1Γ2(ϵ,ϵ1),
Substituting the upper bound of H1,H2,...,H 6into Eq(43)and noting that (1 +αc4)2≤1 + 3αc4because
αc4≤1, we have:
/vextenddouble/vextenddouble/vextenddouble¯θl+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
≤(1 +α(3c4+ 12))/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ (6α2+ 2α)c2
4δ2
l
+ (18α2+ 6α)/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble2
+ (12α2+ 4α)c2
1Γ2(ϵ,ϵ1)
≤(1 +αh1)/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 8αc2
4δ2
l+ 24α/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble2
+ 16αc2
1Γ2(ϵ,ϵ1), (47)
where we denote h1≜3c4+ 12for simplicity. For any t−τ≤l≤t, conditioning on Ft−2τon both sides of
the above inequality, we have:
Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θl+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
42Published in Transactions on Machine Learning Research (06/2024)
≤(1 +αh1)Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 24αEt−2τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
l,k)/vextenddouble/vextenddouble/vextenddouble2
+ 8αc2
4Et−2τ/bracketleftbig
δ2
l/bracketrightbig
+αM3(ϵ,ϵ1)
≤(1 +αh1)Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 24α/bracketleftbiggd2
2
NK+ 4L2
2ρ2(l−t+2τ)K/bracketrightbigg
(Lemma 13 )
+ 8αc2
4Et−2τ/bracketleftbig
δ2
l/bracketrightbig
+αM3(ϵ,ϵ1)
(a)
≤(1 +αh1)Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 24α/bracketleftbiggd2
2
NK+ 4L2
2α2ρ2(l−t+τ)K/bracketrightbigg
+ 8αc2
4Et−2τ/bracketleftbig
δ2
l/bracketrightbig
+αM3(ϵ,ϵ1)
≤(1 +αh1)Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
+αct(l) + 8αc2
4Et−2τ/bracketleftbig
δ2
l/bracketrightbig
+αM3(ϵ,ϵ1), (48)
where we denote M3(ϵ,ϵ1)≜16c2
1Γ2(ϵ,ϵ1)andct(l) = 24/bracketleftig
d2
2
NK+ 4L2
2α2ρ2(l−t+τ)K/bracketrightig
for simplicity. Inequality
(a)is due toρ2τK≤α4
T≤α2
t. In the following steps, we try to map Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θl+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
toEt−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble2
for anyt−τ≤l≤t. By applying Eq (48) recursively, we have:
Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θl+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
≤(1 +αh1)l+1−t+τEt−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble2
+αl/summationdisplay
k=t−τ(1 +αh1)l−k(ct(k) +M3(ϵ,ϵ1))
+ 8αc2
4Et−2τ/bracketleftiggl/summationdisplay
k=t−τ(1 +αh1)l−kδ2
k/bracketrightigg
(b)
≤(1 +αh1)τ+1Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble2
+αt/summationdisplay
k=t−τ(1 +αh1)l−k(ct(k) +M3(ϵ,ϵ1))
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
H7
+ 8αc2
4Et−2τ/bracketleftiggt/summationdisplay
k=t−τ(1 +αh1)l−kδ2
k/bracketrightigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
H8(49)
where (b)is due tol≤t. ForH7, we have:
H7≤t/summationdisplay
k=t−τ(1 +αh1)t−k(ct(k) +M3(ϵ,ϵ1)) (l≤t)
=τ/summationdisplay
k′=0(1 +αh1)τ−k′
(ct(k′+t−τ) +M3(ϵ,ϵ1))
(changing index kintok′withk′=k+τ−t)
≤24τ/summationdisplay
k′=0(1 +αh1)τ−k′/bracketleftbiggd2
2
NK+ 4L2
2α2ρ2k′K+M3(ϵ,ϵ1)/bracketrightbigg
(Substituting the definition of ct(k′)inside )
= 24/bracketleftigg/parenleftbiggd2
2
NK+M3(ϵ,ϵ1)/parenrightbigg(1 +αh1)τ+1−1
αh1+ 4L2
2α2(1 +αh1)ττ/summationdisplay
k′=0/parenleftbiggρ2K
1 +αh1/parenrightbiggk′/bracketrightigg
≤24/bracketleftigg/parenleftbiggd2
2
NK+M3(ϵ,ϵ1)/parenrightbigg(1 +αh1)τ+1−1
αh1+ 4L2
2α2(1 +αh1)ττ/summationdisplay
k′=0ρ2k′K/bracketrightigg
(1 +αh1≥1)
43Published in Transactions on Machine Learning Research (06/2024)
≤24/bracketleftigg/parenleftbiggd2
2
NK+M3(ϵ,ϵ1)/parenrightbigg(1 +αh1)τ+1−1
αh1+ 4L2
2α2(1 +αh1)τ1
1−ρ2/bracketrightigg
.
Here we follow the analysis in (Khodadadian et al., 2022). Notice that for x≤log 2
τ, we have (1 +x)τ+1≤
1 + 2x(τ+ 1).Ifα≤1
4h1τ≤log 2
h1τandα≤1
2h1(τ+1), we have (1 +αh1)τ+1≤1 + 2αh1(τ+ 1)≤2and
(1 +αh1)τ≤1 + 2αh1τ≤1 + 1/2≤2. Hence, we have
H7≤24/bracketleftbigg/parenleftbiggd2
2
NK+M3(ϵ,ϵ1)/parenrightbigg
2(τ+ 1) +8L2
2α2
1−ρ2/bracketrightbigg
.
We apply the similar analysis to bound H8as:
H8=τ/summationdisplay
k=0(1 +αh1)τ−kδ2
t−τ+k≤τ/summationdisplay
k=0(1 +αh1)τδ2
t−τ+k≤τ/summationdisplay
k=0(1 + 2αh1τ)δ2
t−τ+k≤2τ/summationdisplay
k=0δ2
t−k.
Substituting the upper bound of H7andH8into Eq (49), we have:
Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θl+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
≤2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 24α/bracketleftbigg/parenleftbiggd2
2
NK+M3(ϵ,ϵ1)/parenrightbigg
2(τ+ 1) +8L2
2α2
1−ρ2/bracketrightbigg
+ 16αc2
4τ/summationdisplay
k=0Et−2τ[δ2
t−k].
Then it is straightforward to bound Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
as:
Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θl−θ∗/vextenddouble/vextenddouble/vextenddouble2
≤2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 24α/bracketleftbigg/parenleftbiggd2
2
NK+M3(ϵ,ϵ1)/parenrightbigg
4τ+8L2
2α2
1−ρ2/bracketrightbigg
+ 16αc2
4τ/summationdisplay
k=0Et−2τ[δ2
t−k]. (50)
Furthermore, based on the triangle inequality, we have:
/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
≤/parenleftiggt−1/summationdisplay
s=t−τ/vextenddouble/vextenddouble/vextenddouble¯θs+1−¯θs/vextenddouble/vextenddouble/vextenddouble/parenrightigg2
≤τt−1/summationdisplay
s=t−τ/vextenddouble/vextenddouble/vextenddouble¯θs+1−¯θs/vextenddouble/vextenddouble/vextenddouble2
≤τt−1/summationdisplay
s=t−τ/bracketleftigg
α2c2
4/vextenddouble/vextenddouble/vextenddouble¯θs−θ∗/vextenddouble/vextenddouble/vextenddouble2
+α2c2
4δ2
s+ 6α2/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
s,k)/vextenddouble/vextenddouble/vextenddouble2
+ 2α2c2
1Γ2(ϵ,ϵ1)/bracketrightigg
where the last inequality is due to Eq (40)withc4= 3c1. If we take the expectation on both sides, we have:
Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
≤τt−1/summationdisplay
s=t−τ/bracketleftig
α2c2
4Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θs−θ∗/vextenddouble/vextenddouble/vextenddouble2
+α2c2
4δ2
s
+ 6α2Et−2τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
s,k)/vextenddouble/vextenddouble/vextenddouble2
+ 2α2c2
1Γ2(ϵ,ϵ1)/bracketrightig
≤τα2c2
4t−1/summationdisplay
s=t−τ/bracketleftig
2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble2
44Published in Transactions on Machine Learning Research (06/2024)
+ 24α/bracketleftbigg/parenleftbiggd2
2
NK+M3(ϵ,ϵ1)/parenrightbigg
4τ+8L2
2α2
1−ρ2/bracketrightbigg
+ 16αc2
4τ/summationdisplay
k=0Et−2τ[δ2
t−k]/bracketrightig
(Eq (50) )
+ 6α2τt−1/summationdisplay
s=t−τ/parenleftbiggd2
2
NK+ 4L2
2ρ2(s−t+2τ)K/parenrightbigg
+α2c2
4τt−1/summationdisplay
s=t−τEt−2τ[δ2
s] + 2α2c2
1τ2Γ2(ϵ,ϵ1) (Lemma 13 )
(a)
≤τ2α2c2
4/bracketleftbigg
2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 96/parenleftbiggd2
2
NKατ+2L2
2α3
1−ρ2/parenrightbigg/bracketrightbigg
+ 6α2τ/bracketleftbiggd2
2
NKτ+4L2
2α2
1−ρ2K/bracketrightbigg
+α2c2
4τ(1 + 16ατc2
4)τ/summationdisplay
s=0Et−2τ[δ2
t−s] + 96α2c2
4τ3M3(ϵ,ϵ1) + 2α2c2
1τ2Γ2(ϵ,ϵ1)
(b)
≤2τ2α2c2
4Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble2
+d2
2
NKα2τ2/parenleftbig
96ατc2
4+ 6/parenrightbig
+12L2
2α4τ
1−ρ2/parenleftbig
16αc2
4τ+ 2/parenrightbig
+α2c2
4τ(1 + 16ατc2
4)τ/summationdisplay
s=0Et−2τ[∆t−s] + 96α2c2
4τ3M3(ϵ,ϵ1) + 2α2c2
1τ2Γ2(ϵ,ϵ1) (51)
where we used the fact that ρ2τK≤α2for(a)and(b), and thatδ2
t≤∆t(via Jensens’ inequality) for all
t≥0in the last inequality. Let us choose αsuch that 96ατc2
4+ 6≤7,16αc2
4τ+ 2≤13
6and1 + 16ατc2
4≤2,
this holds when
α≤min/braceleftig1
96τc2
4,1
96c2
4τ,1
16τc2
4,1/bracerightig
.
Based on the fact that ∥¯θt−τ−θ∗∥2≤2∥¯θt−¯θt−τ∥2+ 2∥¯θt−θ∗∥2and the requirement on α, we have
2α2τ2c2
4Et−2τ∥¯θt−τ−θ∗∥2≤4α2τ2c2
4Et−2τ∥¯θt−¯θt−τ∥2+ 4α2τ2c2
4Et−2τ∥¯θt−θ∗∥2
≤0.5Et−2τ∥¯θt−¯θt−τ∥2+ 4α2τ2c2
4Et−2τ∥¯θt−θ∗∥2(4α2τ2c2
4≤0.5)
(a)
≤τ2α2c2
4Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble2
+7d2
2
2NKα2τ2+13L2
2α4τ
(1−ρ2)
+α2c2
4ττ/summationdisplay
s=0Et−2τ[∆t−s] + 48α2c2
4τ3M3(ϵ,ϵ1) +α2c2
1τ2Γ2(ϵ,ϵ1)
+ 4α2τ2c2
4Et−2τ∥¯θt−θ∗∥2(52)
where (a) is due to Eq (51)and the choice of α. Putting the term τ2α2c2
4Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble2
together by
rearranging the terms, we have:
α2τ2c2
4Et−2τ∥¯θt−τ−θ∗∥2≤7d2
2
2NKα2τ2+13L2
2α4τ
(1−ρ2)
+α2c2
4ττ/summationdisplay
s=0Et−2τ[∆t−s] + 48α2c2
4τ3M3(ϵ,ϵ1) +α2c2
1τ2Γ2(ϵ,ϵ1)
+ 4α2τ2c2
4Et−2τ∥¯θt−θ∗∥2(53)
The proof is completed by substituting this inequality into Eq (51)and the definition of M3(ϵ,ϵ1). Note that
we require the effective step-size
α≤min/braceleftig1
4h1τ,1
2h1(τ+ 1),1
96c2
4τ,1/bracerightig
in this proof, which holds when α≤min/braceleftig
1
30c4(τ+1),1
96c2
4τ,1/bracerightig
sincec4= 3c1≥1.
45Published in Transactions on Machine Learning Research (06/2024)
I.2.4 Drift Term Analysis.
Now we bound the drift term as follows:
Lemma 15. (Bounded Client Drift) If αl≤1
2√
2c1(K−1), the drift term satisfies
E[∆t] =1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
≤4α2
Kα2g/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
. (54)
Proof.
1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
=1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0E/vextenddouble/vextenddouble/vextenddouble¯θt+αlk−1/summationdisplay
s=0gi(θ(i)
t,s)−¯θt/vextenddouble/vextenddouble/vextenddouble2
=α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0E/vextenddouble/vextenddouble/vextenddoublek−1/summationdisplay
s=0−Ai(O(i)
t,s)/parenleftig
θ(i)
t,s−θ∗
i/parenrightig
+Zi(O(i)
t,s)/vextenddouble/vextenddouble/vextenddouble2
≤2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0E/vextenddouble/vextenddouble/vextenddoublek−1/summationdisplay
s=0−Ai(O(i)
t,s)/parenleftig
θ(i)
t,s−θ∗
i/parenrightig/vextenddouble/vextenddouble/vextenddouble2
+ 2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0E/vextenddouble/vextenddouble/vextenddoublek−1/summationdisplay
s=0Zi(O(i)
t,s)/vextenddouble/vextenddouble/vextenddouble2
≤2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0kk−1/summationdisplay
s=0E/vextenddouble/vextenddouble/vextenddoubleAi(O(i)
t,s)/parenleftig
θ(i)
t,s−θ∗
i/parenrightig/vextenddouble/vextenddouble/vextenddouble2
+ 2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0k−1/summationdisplay
s=0E/vextenddouble/vextenddouble/vextenddoubleZi(O(i)
t,s)/vextenddouble/vextenddouble/vextenddouble2
+ 2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0k−1/summationdisplay
s,s′=0
s̸=s′E/angbracketleftig
Zi(O(i)
t,s),Zi(O(i)
t,s′)/angbracketrightig
≤2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0kc2
1k−1/summationdisplay
s=0E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,s−θ∗
i/vextenddouble/vextenddouble/vextenddouble2
+ 2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0kc2
3(Lemma 11 )
+ 2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0k−1/summationdisplay
s,s′=0
s̸=s′E/bracketleftig
E/angbracketleftig
Zi(O(i)
t,s),Zi(O(i)
t,s′)/angbracketrightig/vextendsingle/vextendsingleFt
s/bracketrightig
≤2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0kc2
1k−1/summationdisplay
s=0E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,s−θ∗
i/vextenddouble/vextenddouble/vextenddouble2
+ 2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0kc2
3
+ 2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0k−1/summationdisplay
s,s′=0
s̸=s′E/bracketleftig/angbracketleftig
Zi(O(i)
t,s),E/bracketleftig
Zi(O(i)
t,s′)/vextendsingle/vextendsingleFt
s/bracketrightig/angbracketrightig/bracketrightig
≤2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0kc2
1k−1/summationdisplay
s=0E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,s−θ∗
i/vextenddouble/vextenddouble/vextenddouble2
+ 2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0kc2
3
+ 4α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0k−1/summationdisplay
s,s′=0
s<s′E/bracketleftig/vextenddouble/vextenddouble/vextenddoubleZi(O(i)
t,s)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleE/bracketleftig
Zi(O(i)
t,s′)/vextendsingle/vextendsingleFt
s/bracketrightig/vextenddouble/vextenddouble/vextenddouble/bracketrightig
≤4α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0kc2
1k−1/summationdisplay
s=0E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,s−¯θt/vextenddouble/vextenddouble/vextenddouble2
+ 4α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0kc2
1k−1/summationdisplay
s=0E/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗
i/vextenddouble/vextenddouble/vextenddouble2
(Eq (11) )
46Published in Transactions on Machine Learning Research (06/2024)
+ 2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0kc2
3+ 4α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0k−1/summationdisplay
s,s′=0
s<s′c3L2ρs′−s(Lemma 12 )
≤4α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0kc2
1k−1/summationdisplay
s=0E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,s−¯θt/vextenddouble/vextenddouble/vextenddouble2
+ 4α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=04kc2
1(K−1)H2
+ 2α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0kc2
3+ 4α2
l1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0k−1/summationdisplay
s,s′=0
s<s′c3L2ρs′−s
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
M1(55)
where we used the property that ¯θt,θ∗
i∈Hin the last inequality, i.e., ∥¯θt∥≤H2and∥θ∗
i∥≤H2. We now
boundM1as:
k−1/summationdisplay
s,s′=0
s<s′c3L2ρs′−s=c3L2k−1/summationdisplay
s=0k−1/summationdisplay
s′=s+1ρs′−s=c3L2k−1/summationdisplay
s=0ρ−ρs−s′
1−ρ≤c3L2ρk
1−ρ(56)
DefineRK≜/summationtextN
i=1/summationtextK−1
k=0E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
and note thatRKis monotonically increasing in K. With this
definition, if we plug in the upper bound of M1into Eq (55), we have:
RK≤4α2
lN/summationdisplay
i=1K−1/summationdisplay
k=0kc2
1k−1/summationdisplay
s=0E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,s−¯θt/vextenddouble/vextenddouble/vextenddouble2
+ 4α2
lN/summationdisplay
i=1K−1/summationdisplay
k=04kc2
1(K−1)H2
+ 2α2
lN/summationdisplay
i=1K−1/summationdisplay
k=0kc2
3+ 4α2
lN/summationdisplay
i=1K−1/summationdisplay
k=0c3L2ρk
1−ρ
≤2α2
l(K−1)NK/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+ 4α2
lc2
1(K−1)K−1/summationdisplay
k=1N/summationdisplay
i=1k−1/summationdisplay
s=0E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,s−¯θt/vextenddouble/vextenddouble/vextenddouble2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Rk
= 2α2
l(K−1)NK/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+ 4α2
lc2
1(K−1)K−1/summationdisplay
k=1Rk (57)
By the monotonicity of Rk, we have
RK≤2α2
l(K−1)NK/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+ 4α2
lc2
1(K−1)2RK−1
Let us choose αlsuch that 4α2
lc2
1(K−1)2≤1
2, i.e.,αl≤1
2√
2c1(K−1), the following recursion holds:
RK≤1
2RK−1+ 2α2
l(K−1)NK/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
(58)
for allk∈[K].Next, we unroll the recurrence, go back K−1steps and use the fact that R1= 0, we have:
RK≤/braceleftigg∞/summationdisplay
l=1/parenleftbigg1
2/parenrightbiggl/bracerightigg/parenleftbigg
2α2
l(K−1)NK/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg/parenrightbigg
= 4α2
l(K−1)NK/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
(59)
We finish the proof by dividing NKon both sides and substituting αl=α
Kαg.
47Published in Transactions on Machine Learning Research (06/2024)
I.2.5 Per Round Progress
Lemma 16. (Per Round Progress). If the local step-size αl≤1
2√
2c1(K−1), and the effective step-size
α=Kαlαgsatisfies:
α≤min{ξ1
24(c1+c2)2+ 24ξ2
1+ 16,1,ξ1(c1+c2)
2L1+ 8τ2c2
4,1
30c4(τ+ 1),1
96c2
4τ,X},5
where
X=2B(ϵ,ϵ1)G+ 3ξ1(c1+c2)Γ2(ϵ,ϵ1)
4B2(ϵ,ϵ1) + 24(c1+c2)2Γ2(ϵ,ϵ1) + 2L1Γ(ϵ,ϵ1)G+ 6400c2
1c2
4τ3Γ2(ϵ,ϵ1) + 8c2
1τ2Γ2(ϵ,ϵ1),
and choose τ=⌈τmix(α2
T)
K⌉, then we have,
Et−2τ∥¯θt+1−θ∗∥2
≤(1 + 32αξ1(c1+c2))Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αEt−2τ/angbracketleftig
¯g(¯θt),¯θt−θ∗/angbracketrightig
+ 4α2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Expected progress for the virtual MDP
+9 + 28τ2
NKα2d2
2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Linear speedup+α3/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
High order terms: O(α3)
+4α3
Kα2g(14
ξ1+ 14ξ1)(c1+c2)/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 4c2
1(K−1)H2/bracketrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
drift term
+ 4αB(ϵ,ϵ1)G+ 6αξ1(c1+c2)Γ2(ϵ,ϵ1)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
heterogeneity term. (60)
whereξ1is any universal positive constant.
Proof.According to the updating rule and the fact that the projection operator is non-expansive, we have:
Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
=Et−τ/vextenddouble/vextenddouble/vextenddoubleΠ2,H/parenleftigg
¯θt+α
NKN/summationdisplay
i=1K−1/summationdisplay
k=0gi(θ(i)
t,k)/parenrightigg
−θ∗/vextenddouble/vextenddouble/vextenddouble2
≤Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt+α
NKN/summationdisplay
i=1K−1/summationdisplay
k=0gi(θ(i)
t,k)−θ∗/vextenddouble/vextenddouble/vextenddouble2
=Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2Et−τ/angbracketleftigα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0¯gi(θ(i)
t,k),¯θt−θ∗/angbracketrightig
+ 2Et−τ/angbracketleftigα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftbig
gi(θ(i)
t,k)−¯gi(θ(i)
t,k)/bracketrightbig
,¯θt−θ∗/angbracketrightig
5This requirement is very easy to satisfy since the denominator in Xis composed by the heterogeneity terms, which is quite
small and thereby makes Xlarge. Overall, the feasible set of the step-sizes is not empty.
48Published in Transactions on Machine Learning Research (06/2024)
+α2Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0gi(θ(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
≤Et−τ/braceleftigg/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2/angbracketleftigα
NN/summationdisplay
i=1¯gi(¯θt),¯θt−θ∗/angbracketrightig
+ 2/angbracketleftigα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0¯gi(θ(i)
t,k)−¯gi(¯θt),¯θt−θ∗/angbracketrightig/bracerightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B1
+ 2αEt−τ/angbracketleftig1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftbig
gi(θ(i)
t,k)−¯gi(θ(i)
t,k)/bracketrightbig
,¯θt−θ∗/angbracketrightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B2+α2Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0gi(θ(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
B3(61)
We now begin to bound the gradient bias term B2by decomposing this term into three terms:
/angbracketleftig1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftbig
gi(θ(i)
t,k)−¯gi(θ(i)
t,k)/bracketrightbig
,¯θt−θ∗/angbracketrightig
=/angbracketleftig1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftbig
gi(θ(i)
t,k)−¯gi(θ(i)
t,k)/bracketrightbig
,¯θt−¯θt−τ/angbracketrightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B21
+/angbracketleftig1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftbig
gi(θ(i)
t,k)−gi(θ(i)
t−τ,k)−¯gi(θ(i)
t,k) + ¯gi(θ(i)
t−τ,k)/bracketrightbig
,¯θt−τ−θ∗/angbracketrightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B22
+/angbracketleftig1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftbig
gi(θ(i)
t−τ,k)−¯gi(θ(i)
t−τ,k)/bracketrightbig
,¯θt−τ−θ∗/angbracketrightig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B23. (62)
Next, we bound Et−τ[B21]as:
Et−τ/angbracketleftig1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftbig
gi(θ(i)
t,k)−¯gi(θ(i)
t,k)/bracketrightbig
,¯θt−¯θt−τ/angbracketrightig
≤Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0gi(θ(i)
t,k)−¯gi(θ(i)
t,k)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble
(a)=Et−τ/bracketleftigg/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0(−Ai(O(i)
t,k) +¯Ai)(θ(i)
t,k−θ∗
i) +Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble/bracketrightigg
≤Et−τ/bracketleftigg/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0(Ai(O(i)
t,k)−¯Ai)(θ(i)
t,k−θ∗
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble/bracketrightigg
+Et−τ/bracketleftigg/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble/bracketrightigg
≤1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/bracketleftig/vextenddouble/vextenddouble/vextenddouble(Ai(O(i)
t,k)−¯Ai)(θ(i)
t,k−θ∗
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble/bracketrightig
+α
2Et−τ/bracketleftigg/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2/bracketrightigg
+1
2αEt−τ/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
(b)
≤(c1+c2)
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−θ∗
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble
49Published in Transactions on Machine Learning Research (06/2024)
+α
2Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
+1
2αEt−τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
≤ξ1(c1+c2)
2NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−θ∗
i/vextenddouble/vextenddouble/vextenddouble2
+(c1+c2)
2ξ1NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
(Young’s inequality (12) with constant ξ1)
+α
2Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
+1
2αEt−τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
(c)
≤3ξ1(c1+c2)
2NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
+3ξ1(c1+c2)
2NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+3ξ1(c1+c2)
2NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddoubleθ∗−θ∗
i/vextenddouble/vextenddouble/vextenddouble2
+(c1+c2)
2ξ1NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
+α
2Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
+1
2αEt−τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
=3ξ1(c1+c2)
2NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
+3ξ1(c1+c2)
2Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+3ξ1(c1+c2)
2Γ2(ϵ,ϵ1) +(c1+c2)
2ξ1Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
+α
2Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
+1
2αEt−τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
(d)
≤3ξ1(c1+c2)
2NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
+3ξ1(c1+c2)
2Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+3ξ1(c1+c2)
2Γ2(ϵ,ϵ1) +c1+c2
2ξ1Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
+α
2/bracketleftbiggd2
2
NK+ 4L2
2ρ2τK/bracketrightbigg
+1
2αEt−τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
=3ξ1(c1+c2)
2NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
+3ξ1(c1+c2)
2Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+3ξ1(c1+c2)
2Γ2(ϵ,ϵ1) +/parenleftbiggc1+c2
2ξ1+1
2α/parenrightbigg
Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
+α
2
d2
2
NK+ 4L2
2ρ2τK
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤4L2
2α2
,(63)
where (a)is due togi(θ(i)
t,k) =−Ai(O(i)
t,k)(θ(i)
t,k−θ∗
i) +Zi(O(i)
t,k),(b)is due to Lemma 12 (the upper bound of
Ai(O(i)
t,k)and ¯Ai),(c)is due to Eq (13) and (d)is due to Lemma 13.
And we boundB22as:
B22=/angbracketleftig1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftbig
gi(θ(i)
t,k)−gi(θ(i)
t−τ,k)−¯gi(θ(i)
t,k) + ¯gi(θ(i)
t−τ,k)/bracketrightbig
,¯θt−τ−θ∗/angbracketrightig
≤1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/vextenddouble/vextenddouble/vextenddoublegi(θ(i)
t,k)−gi(θ(i)
t−τ,k)−¯gi(θ(i)
t,k) + ¯gi(θ(i)
t−τ,k)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble
(Cauchy-Schwarz inequality )
50Published in Transactions on Machine Learning Research (06/2024)
≤1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig/vextenddouble/vextenddouble/vextenddoublegi(θ(i)
t,k)−gi(θ(i)
t−τ,k)/vextenddouble/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/vextenddouble¯gi(θ(i)
t,k)−¯gi(θ(i)
t−τ,k)/vextenddouble/vextenddouble/vextenddouble/bracketrightig/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble
(a)
≤1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig
2/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−θ(i)
t−τ,k/vextenddouble/vextenddouble/vextenddouble+ 2/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−θ(i)
t−τ,k/vextenddouble/vextenddouble/vextenddouble/bracketrightig/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble
≤1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig
4/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble+ 4/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble+ 4/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ(i)
t−τ,k/vextenddouble/vextenddouble/vextenddouble/bracketrightig/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble
(Triangle inequality )
≤4δt/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble+ 4/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble+ 4δt−τ/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble
≤2
ξ2∆t+2
ξ2∆t−τ+ (2ξ2+ 4ξ2)/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble2
+2
ξ2/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
(Young’s inequality (12) with constants ξ2andδ2
t≤∆t)
≤2
ξ2∆t+2
ξ2∆t−τ+ 12ξ2/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ (12ξ2+2
ξ2)/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
(Eq13) (64)
where (a) is due to the 2-Lipschitz property of steady-state ¯g(i.e., Lemma 5) and random direction gi(i.e.,
Lemma 6), δt=1
NK/summationtextN
i=1/summationtextK−1
k=0/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddoubleand∆t≜1
NK/summationtextN
i=1/summationtextK−1
k=0E/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
.
Now, we boundB23as:
Et−τ[B23]
=/angbracketleftig1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/bracketleftbig
gi(θ(i)
t−τ,k)−¯gi(θ(i)
t−τ,k)/bracketrightbig
,¯θt−τ−θ∗/angbracketrightig
≤1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleEt−τ/bracketleftig
gi(θ(i)
t−τ,k)−¯gi(θ(i)
t−τ,k)/bracketrightig/vextenddouble/vextenddouble/vextenddouble(Cauchy-Schwarz inequality )
=1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleEt−τ/bracketleftig
−Ai(O(i)
t,k)(θ(i)
t−τ,k−θ∗
i) +Zi(O(i)
t,k) +¯Ai(θ(i)
t−τ,k−θ∗
i)/bracketrightig/vextenddouble/vextenddouble/vextenddouble
≤1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble/braceleftig/vextenddouble/vextenddouble/vextenddoubleEt−τ(Ai(O(i)
t,k)−¯Ai)(θ(i)
t−τ,k−θ∗
i)/vextenddouble/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/vextenddoubleEt−τ/bracketleftig
Zi(O(i)
t,k)/bracketrightig/vextenddouble/vextenddouble/vextenddouble/bracerightig
(a)
≤1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble/braceleftig
L1ρτK+k/vextenddouble/vextenddouble/vextenddoubleθ(i)
t−τ,k−θ∗
i/vextenddouble/vextenddouble/vextenddouble+L2ρτK+k/bracerightig
≤1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble
×/braceleftig
L1ρτK+k/bracketleftig/vextenddouble/vextenddouble/vextenddoubleθ(i)
t−τ,k−¯θt−τ/vextenddouble/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble+/vextenddouble/vextenddouble/vextenddoubleθ∗−θ∗
i/vextenddouble/vextenddouble/vextenddouble/bracketrightig
+L2ρτK+k/bracerightig
(b)
≤α2L1/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddoubleδt−τ+α2L1/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble2
+α2L1Γ(ϵ,ϵ1)G+α2L2G
≤α2L1/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble2
+α2L1∆t−τ+α2L1/vextenddouble/vextenddouble/vextenddouble¯θt−τ−θ∗/vextenddouble/vextenddouble/vextenddouble2
+α2L1Γ(ϵ,ϵ1)G+α2L2G
(c)
≤2α2L1G2+α2L2G+α2L1∆t−τ+α2L1Γ(ϵ,ϵ1)G, (65)
where (a) is due to Lemma 12, (b) is due to the fact that ¯θt−τ,θ∗∈H, which radius is H≤G
2, and
τ=⌈logρ(α2
T)
K⌉(i.e.,ρτK≤α2) and (c) is due to the fact that ¯θt−τ,θ∗∈H. Then, the term B2can be
51Published in Transactions on Machine Learning Research (06/2024)
bounded as:
Et−τ[B2]
=Et−τ[B21+B22+B23]
≤3ξ1(c1+c2)
2Et−τ[∆t] +3ξ1(c1+c2)
2Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+3ξ1(c1+c2)
2Γ2(ϵ,ϵ1)
+/parenleftbiggc1+c2
2ξ1+1
2α/parenrightbigg
Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
+α
2/bracketleftbiggd2
2
NK+ 4L2
2ρ2τK/bracketrightbigg
+2
ξ2Et−τ[∆t] +2
ξ2Et−τ[∆t−τ] + 12ξ2Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ (12ξ2+2
ξ2)Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
+ 2α2L1G2+α2L2G+α2L1Et−τ[∆t−τ] +α2L1Γ(ϵ,ϵ1)G
≤/parenleftbigg3ξ1(c1+c2)
2+ 12ξ2/parenrightbigg
Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+/parenleftbiggc1+c2
2ξ1+1
2α+ 12ξ2+2
ξ2/parenrightbigg
Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
+/parenleftbigg3ξ1(c1+c2)
2+2
ξ2/parenrightbigg
Et−τ[∆t] +/parenleftbigg2
ξ2+α2L1/parenrightbigg
∆t−τ+α
2/bracketleftbiggd2
2
NK+ 4L2
2α2/bracketrightbigg
+ 2α2L1G2+α2L2G+3ξ1(c1+c2)
2Γ2(ϵ,ϵ1) +α2L1Γ(ϵ,ϵ1)G (66)
Next, we boundB3as:
Et−τ[B3]
=Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig
gi(θ(i)
t,k)−¯gi(θ(i)
t,k) + ¯gi(θ(i)
t,k)−¯gi(¯θt) + ¯gi(¯θt)−¯g(¯θt) + ¯g(¯θt)/bracketrightig/vextenddouble/vextenddouble/vextenddouble2
≤4Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/parenleftig
gi(θ(i)
t,k)−¯gi(θ(i)
t,k)/parenrightig/vextenddouble/vextenddouble/vextenddouble2
+ 4Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/parenleftig
¯gi(θ(i)
t,k)−¯gi(¯θt)/parenrightig/vextenddouble/vextenddouble/vextenddouble2
+ 4Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/parenleftbig
¯gi(¯θt)−¯g(¯θt)/parenrightbig/vextenddouble/vextenddouble/vextenddouble2
+ 4Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
(Eq 13 )
= 4Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/bracketleftig/parenleftig
¯Ai−Ai(O(i)
t,k)/parenrightig
(θ(i)
t,k−θ∗
i) +Zi(O(i)
t,k)/bracketrightig/vextenddouble/vextenddouble/vextenddouble2
+ 4Et−τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+ 4Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/parenleftig
¯gi(θ(i)
t,k)−¯gi(¯θt)/parenrightig/vextenddouble/vextenddouble/vextenddouble2
+ 4Et−τ/vextenddouble/vextenddouble/vextenddouble1
NN/summationdisplay
i=1/parenleftbig
¯gi(¯θt)−¯g(¯θt)/parenrightbig/vextenddouble/vextenddouble/vextenddouble2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Lemma 2
(a)
≤8Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/parenleftig
¯Ai−Ai(O(i)
t,k)/parenrightig
(θ(i)
t,k−θ∗
i)/vextenddouble/vextenddouble/vextenddouble2
+ 8Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
+ 161
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
+ 4B2(ϵ,ϵ1) + 4Et−τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
≤8
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddouble¯Ai−Ai(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−θ∗
i/vextenddouble/vextenddouble/vextenddouble2
+ 8Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
+ 16Et−τ[∆t] + 4B2(ϵ,ϵ1) + 4Et−τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
≤8(c1+c2)2
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−θ∗
i/vextenddouble/vextenddouble/vextenddouble2
+ 8Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
+ 16Et−τ[∆t] + 4B2(ϵ,ϵ1) + 4Et−τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
52Published in Transactions on Machine Learning Research (06/2024)
≤24(c1+c2)2
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddoubleθ(i)
t,k−¯θt/vextenddouble/vextenddouble/vextenddouble2
+24(c1+c2)2
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+24(c1+c2)2
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Et−τ/vextenddouble/vextenddouble/vextenddoubleθ∗
i−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 8Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
+ 16Et−τ[∆t] + 4B2(ϵ,ϵ1) + 4Et−τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
= 24(c1+c2)2Et−τ[∆t] + 24(c1+c2)2Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 24(c1+c2)2Γ2(ϵ,ϵ1)
+ 8Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0Zi(O(i)
t,k)/vextenddouble/vextenddouble/vextenddouble2
+ 16Et−τ[∆t] + 4B2(ϵ,ϵ1) + 4Et−τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
(b)
≤(24(c1+c2)2+ 16)Et−τ[∆t] + 8(d2
2
NK+ 4L2
2ρτK
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤4L2
2α2) + 24(c1+c2)2Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 4Et−τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+ 4B2(ϵ,ϵ1) + 24(c1+c2)2Γ2(ϵ,ϵ1), (67)
where (a) is due to 2-Lipschitz of ¯gi(i.e., Lemma 5) and the gradient heterogeneity (i.e., Lemma 2) and (b) is
due to Lemma 13.
Next, we boundB1as:
Et−τ[B1]
=Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2Et−τ/angbracketleftigα
NN/summationdisplay
i=1¯gi(¯θt),¯θt−θ∗/angbracketrightig
+ 2Et−τ/angbracketleftigα
NKN/summationdisplay
i=1K−1/summationdisplay
k=0¯gi(θ(i)
t,k)−¯gi(¯θt),¯θt−θ∗/angbracketrightig
≤Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αEt−τ/angbracketleftig1
NN/summationdisplay
i=1¯gi(¯θt)−¯g(¯θt),¯θt−θ∗/angbracketrightig
+ 2αEt−τ/angbracketleftig
¯g(¯θt),¯θt−θ∗/angbracketrightig
+ 2αEt−τ/angbracketleftig1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0¯gi(θ(i)
t,k)−¯gi(¯θt),¯θt−θ∗/angbracketrightig
≤Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αEt−τ/vextenddouble/vextenddouble/vextenddouble1
NN/summationdisplay
i=1¯gi(¯θt)−¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble+ 2αEt−τ/angbracketleftig
¯g(¯θt),¯θt−θ∗/angbracketrightig
+α
ξ3Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/parenleftig
¯gi(θ(i)
t,k)−¯gi(¯θt)/parenrightig/vextenddouble/vextenddouble/vextenddouble2
+αξ3Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
(Young’s inequality Eq (12) with constant ξ3)
(a)
≤Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αB(ϵ,ϵ1)G+ 2αEt−τ/angbracketleftig
¯g(¯θt),¯θt−θ∗/angbracketrightig
+α
ξ3Et−τ/vextenddouble/vextenddouble/vextenddouble1
NKN/summationdisplay
i=1K−1/summationdisplay
k=0/parenleftig
¯gi(θ(i)
t,k)−¯gi(¯θt)/parenrightig/vextenddouble/vextenddouble/vextenddouble2
+αξ3Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
(b)
≤Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αB(ϵ,ϵ1)G+ 2αEt−τ/angbracketleftig
¯g(¯θt),¯θt−θ∗/angbracketrightig
+4α
ξ3Et−τ[∆t] +αξ3Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
, (68)
where (a) is due to the fact that ¯θt,θ∗∈Hand the gradient heterogeneity; (b) is due to 2-Lipschitz property
of function ¯gin Lemma 5.
53Published in Transactions on Machine Learning Research (06/2024)
Incorporating the upper of B1from Eq (68),B2from Eq (66) and B3from Eq (67) into Eq (61), we have:
Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
≤Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αEt−τ/angbracketleftig
¯g(¯θt),¯θt−θ∗/angbracketrightig
+ 4α2Et−τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+/parenleftbig
αξ3+α(3ξ1(c1+c2) + 24ξ2) + 24α2(c1+c2)2/parenrightbig
Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+α/parenleftbiggc1+c2
ξ1+1
α+ 24ξ2+4
ξ2/parenrightbigg
Et−τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
+9d2
2
NKα2+ 36L2
2α4+ 4α3L1G2+ 2α3L2G+/parenleftbigg4α
ξ2+ 2α3L1/parenrightbigg
∆t−τ
+α/parenleftbigg4
ξ3+ 3ξ1(c1+c2) +4
ξ2+α2/parenleftbig
24(c1+c2)2+ 16/parenrightbig/parenrightbigg
Et−τ[∆t]
+ 2αB(ϵ,ϵ1)G+ 4α2B2(ϵ,ϵ1) + 24α2(c1+c2)2Γ2(ϵ,ϵ1)
+ 3αξ1(c1+c2)Γ2(ϵ,ϵ1) + 2α3L1Γ(ϵ,ϵ1)G (69)
Conditioned onFt−2τand using Lemma 14 to give an upper bound of Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−¯θt−τ/vextenddouble/vextenddouble/vextenddouble2
, we have:
Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
≤Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αEt−2τ/angbracketleftig
¯g(¯θt),¯θt−θ∗/angbracketrightig
+ 4α2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+/parenleftbig
αξ3+α(3ξ1(c1+c2) + 24ξ2) + 24α2(c1+c2)2/parenrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
E1Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+α/parenleftbiggc1+c2
ξ1+1
α+ 24ξ2+4
ξ2/parenrightbigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
E2/braceleftbigg
8α2τ2c2
4Et−2τ/bracketleftig/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble2/bracketrightig
+ 14α2τ2d2
2
NK+52L2
2α4τ
1−ρ2
+4α2c2
4ττ/summationdisplay
s=0Et−2τ[∆t−s] + 3200α2c2
1c2
4τ3Γ2(ϵ,ϵ1) + 4α2c2
1τ2Γ2(ϵ,ϵ1)/bracerightigg
+9d2
2
NKα2+ 36L2
2α4+ 4α3L1G2+ 2α3L2G+/parenleftbigg4α
ξ2+ 2α3L1/parenrightbigg
Et−2τ[∆t−τ]
+α/parenleftbigg4
ξ3+ 3ξ1(c1+c2) +4
ξ2+α2/parenleftbig
24(c1+c2)2+ 16/parenrightbig/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
E3Et−2τ[∆t]
+ 2αB(ϵ,ϵ1)G+ 4α2B2(ϵ,ϵ1) + 24α2(c1+c2)2Γ2(ϵ,ϵ1)
+ 3αξ1(c1+c2)Γ2(ϵ,ϵ1) + 2α3L1Γ(ϵ,ϵ1)G (70)
If we choose step-size αsuch thatαE2=α/parenleftig
c1+c2
ξ1+1
α+ 24ξ2+4
ξ2/parenrightig
≤2,ξ1=ξ2=ξ3,E1=αξ3+
α(3ξ1(c1+c2) + 24ξ2) + 24α2(c1+c2)2≤28αξ1(c1+c2) + 24α2(c1+c2)2≤30αξ1(c1+c2)(c1,c2>1) and
E3=4
ξ3+ 3ξ1(c1+c2) +4
ξ2+α2/parenleftbig
24(c1+c2)2+ 16/parenrightbig
≤(9
ξ1+ 9ξ1)(c1+c2), i.e.,
α≤1/parenleftig
c1+c2
ξ1+ 24ξ2+4
ξ2/parenrightig=ξ1
(c1+c2+ 24ξ2
1+ 4)
α≤min{ξ1
12(c1+c2),1,(5
ξ1+ 5ξ1)(c1+c2)
24(c1+c2)2+ 16},
which is sufficient to hold when α≤min{ξ1
24(c1+c2)2+24ξ2
1+16,1}, then we have:
Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
54Published in Transactions on Machine Learning Research (06/2024)
≤Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αEt−2τ/angbracketleftig
¯g(¯θt),¯θt−θ∗/angbracketrightig
+ 4α2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+ 30αξ1(c1+c2)Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2/braceleftbigg
8α2τ2c2
4Et−2τ/bracketleftig/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble2/bracketrightig
+ 14α2τ2d2
2
NK+52L2
2α4τ
1−ρ2
+4α2c2
4ττ/summationdisplay
s=0Et−2τ[∆t−s] + 3200α2c2
1c2
4τ3Γ2(ϵ,ϵ1) + 4α2c2
1τ2Γ2(ϵ,ϵ1)/bracerightigg
+9d2
2
NKα2+ 36L2
2α4+ 4α3L1G2+ 2α3L2G+/parenleftbigg4α
ξ2+ 2α3L1/parenrightbigg
Et−2τ[∆t−τ]
+α/parenleftbigg4
ξ3+ 3ξ1(c1+c2) +4
ξ2+α2/parenleftbig
24(c1+c2)2+ 16/parenrightbig/parenrightbigg
Et−2τ[∆t]
+ 2αB(ϵ,ϵ1)G+ 4α2B2(ϵ,ϵ1) + 24α2(c1+c2)2Γ2(ϵ,ϵ1)
+ 3αξ1(c1+c2)Γ2(ϵ,ϵ1) + 2α3L1Γ(ϵ,ϵ1)G
≤Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αEt−2τ/angbracketleftig
¯g(¯θt),¯θt−θ∗/angbracketrightig
+ 4α2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+/parenleftbig
30αξ1(c1+c2) + 16α2τ2c2
4/parenrightbig
Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+9 + 28τ2
NKα2d2
2+ 36/parenleftbigg
1 +3τ
1−ρ2/parenrightbigg
L2
2α4+ 4α3L1G2+ 2α3L2G
+/parenleftbigg4α
ξ1+ 2α3L1/parenrightbigg
Et−2τ[∆t−τ] +α(9
ξ1+ 9ξ1)(c1+c2)Et−2τ[∆t] + 8α2c2
4ττ/summationdisplay
s=0Et−2τ[∆t−s]
+ 2αB(ϵ,ϵ1)G+ 4α2B2(ϵ,ϵ1) + 24α2(c1+c2)2Γ2(ϵ,ϵ1)
+ 3αξ1(c1+c2)Γ2(ϵ,ϵ1) + 2α3L1Γ(ϵ,ϵ1)G
+ 6400α2c2
1c2
4τ3Γ2(ϵ,ϵ1) + 8α2c2
1τ2Γ2(ϵ,ϵ1) (71)
if we choose the step-size αsuch that the high order O(α2)terms are dominanted by the first order terms
O(α),i.e.,4α2B2(ϵ,ϵ1)+24α2(c1+c2)2Γ2(ϵ,ϵ1)+2α3L1Γ(ϵ,ϵ1)G+6400α2c2
1c2
4τ3Γ2(ϵ,ϵ1)+8α2c2
1τ2Γ2(ϵ,ϵ1)≤
2αB(ϵ,ϵ1)G+ 3αξ1(c1+c2)Γ2(ϵ,ϵ1),i.e.,
α≤min{2B(ϵ,ϵ1)G+ 3ξ1(c1+c2)Γ2(ϵ,ϵ1)
4B2(ϵ,ϵ1) + 24(c1+c2)2Γ2(ϵ,ϵ1) + 2L1Γ(ϵ,ϵ1)G+ 6400c2
1c2
4τ3Γ2(ϵ,ϵ1) + 8c2
1τ2Γ2(ϵ,ϵ1),1},
we have:
Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
≤Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αEt−2τ/angbracketleftig
¯g(¯θt),¯θt−θ∗/angbracketrightig
+ 4α2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+/parenleftbig
30αξ1(c1+c2) + 16α2τ2c2
4/parenrightbig
Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+9 + 28τ2
NKα2d2
2+ 36/parenleftbigg
1 +3τ
1−ρ2/parenrightbigg
L2
2α4+ 4α3L1G2+ 2α3L2G
+/parenleftbigg4α
ξ1+ 2α3L1/parenrightbigg
Et−2τ[∆t−τ] +α(9
ξ1+ 9ξ1)(c1+c2)Et−2τ[∆t] + 8α2c2
4ττ/summationdisplay
s=0Et−2τ[∆t−s]
+ 4αB(ϵ,ϵ1)G+ 6αξ1(c1+c2)Γ2(ϵ,ϵ1) (72)
With Lemma (15), we have the upper bound of Et−2τ[∆t],Et−2τ[∆t−τ]andτ/summationtextτ
s=0Et−2τ[∆t−s].Then we
have:
Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
55Published in Transactions on Machine Learning Research (06/2024)
≤Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αEt−2τ/angbracketleftig
¯g(¯θt),¯θt−θ∗/angbracketrightig
+ 4α2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+/parenleftbig
30αξ1(c1+c2) + 16α2τ2c2
4/parenrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
E4Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+9 + 28τ2
NKα2d2
2+α3/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg
+4α2
Kα2g/parenleftbigg4α
ξ1+ 2α3L1+α(9
ξ1+ 9ξ1)(c1+c2) + 8α2c2
4τ2/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
E5/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 4c2
1(K−1)H2/bracketrightbigg
+ 4αB(ϵ,ϵ1)G+ 6αξ1(c1+c2)Γ2(ϵ,ϵ1) (73)
If we choose step-size such that E4= 30αξ1(c1+c2) + 16α2τ2c2
4≤32αξ1(c1+c2)andE5=4α
ξ1+ 2α3L1+
α(9
ξ1+ 9ξ1)(c1+c2) + 8α2c2
4τ2≤α(14
ξ1+ 14ξ1)(c1+c2),i.e.,
α≤min{ξ1(c1+c2)
8τ2c2
4,1,(1
ξ1+ξ1)(c1+c2)
2L1+ 8c2
4τ2},
which is sufficient to hold when α≤ξ1(c1+c2)
2L1+8τ2c2
4, then we have:
Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
≤Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αEt−2τ/angbracketleftig
¯g(¯θt),¯θt−θ∗/angbracketrightig
+ 4α2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+ 32αξ1(c1+c2)Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+9 + 28τ2
NKα2d2
2+α3/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg
+4α3
Kα2g(14
ξ1+ 14ξ1)(c1+c2)/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+ 4αB(ϵ,ϵ1)G+ 6αξ1(c1+c2)Γ2(ϵ,ϵ1). (74)
I.2.6 Parameter Selection
With Lemma 16, we have:
Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
≤(1 + 32αξ1(c1+c2))Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αEt−2τ/angbracketleftig
¯g(¯θt),¯θt−θ∗/angbracketrightig
+ 4α2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+9 + 28τ2
NKα2d2
2+α3/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg
+4α3
Kα2g(14
ξ1+ 14ξ1)(c1+c2)/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+ 4αB(ϵ,ϵ1)G+ 6αξ1(c1+c2)Γ2(ϵ,ϵ1). (75)
Proposition 4. Ifαsatisfies the requirement as Lemma 16, choose ξ1=(1−γ)¯ω
32(c1+c2)andτ=⌈τmix(α2
T)
K⌉, we
have:
ν1Et−2τ/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D≤(1
α−ν1)Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−1
αEt−2τ/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
+9 + 28τ2
NKαd2
2
56Published in Transactions on Machine Learning Research (06/2024)
+α2/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg
+α2c6
K/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+ 4B(ϵ,ϵ1)G+ν1Γ2(ϵ,ϵ1)(76)
whereν1=ν
4=(1−γ)¯ω
4andc6≜4
α2g(14
ξ1+ 14ξ1)(c1+c2).
Proof.Incorporating ξ1=(1−γ)¯ω
32(c1+c2),c6≜4
α2g(14
ξ1+ 14ξ1)(c1+c2)and6ξ1(c1+c2)≤ν1into Eq(75), we have
Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
≤Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 2αEt−2τ/angbracketleftig
¯g(¯θt),¯θt−θ∗/angbracketrightig
+ 4α2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯g(¯θt)/vextenddouble/vextenddouble/vextenddouble2
+α(1−γ)¯ωEt−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+9 + 28τ2
NKα2d2
2+α3/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg
+α3c6
K/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+ 4αB(ϵ,ϵ1)G+αν1Γ2(ϵ,ϵ1)
(a)
≤Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−2α(1−γ)¯ωEt−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 16α2Et−2τ/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D
+α(1−γ)¯ωEt−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+9 + 28τ2
NKα2d2
2+α3/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg
+α3c6
K/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+ 4αB(ϵ,ϵ1)G+αν1Γ2(ϵ,ϵ1)
=Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−α(1−γ)¯ω
2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−α(1−γ)¯ω
2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
+ 16α2Et−2τ/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D+9 + 28τ2
NKα2d2
2+α3/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg
+α3c6
K/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+ 4αB(ϵ,ϵ1)G+αν1Γ2(ϵ,ϵ1)
≤Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−α(1−γ)¯ω
2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−α(1−γ)¯ω
2Et−2τ/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D
+ 16α2Et−2τ/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D+9 + 28τ2
NKα2d2
2+α3/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg
+α3c6
K/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+ 4αB(ϵ,ϵ1)G+αν1Γ2(ϵ,ϵ1)
(b)
≤Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−α(1−γ)¯ω
2Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−α(1−γ)¯ω
4Et−2τ/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D
+9 + 28τ2
NKα2d2
2+α3/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg
+α3c6
K/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+ 4αB(ϵ,ϵ1)G+αν1Γ2(ϵ,ϵ1)
57Published in Transactions on Machine Learning Research (06/2024)
≤(1−2αν1)Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−αν1Et−2τ/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D+9 + 28τ2
NKα2d2
2
+α3/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg
+α3c6
K/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+ 4αB(ϵ,ϵ1)G+αν1Γ2(ϵ,ϵ1) (77)
where (a)is due to Lemma 3 and the selection of parameter; (b) is due to 16α2≤α(1−γ)¯ω
4. Rearranging the
terms and using the fact 1−2αν1≤1−αν1, we have:
αν1Et−2τ/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D≤(1−αν1)Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble/vextenddouble2
−Et−2τ/vextenddouble/vextenddouble/vextenddouble¯θt+1−θ∗/vextenddouble/vextenddouble/vextenddouble2
+9 + 28τ2
NKα2d2
2
+α3/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg
+α3c6
K/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+ 4αB(ϵ,ϵ1)G+αν1Γ2(ϵ,ϵ1)(78)
Then we finish the proof by dividing αon both sides.
With these Lemmas, we are now ready to prove Theorem 2.
I.3 Proof of Theorem 2.
Given a fixed local step-size αl≤1
4√
2c1(K−1), decreasing effective step-sizes αt=8
ν(a+t+1)=8
(1−γ)¯ω(a+t+1),
decreasing global step-sizes α(t)
g=αt
Kαland weights wt= (a+t), we have:
E/vextenddouble/vextenddouble/vextenddoubleV˜θT−Vθ∗
i/vextenddouble/vextenddouble/vextenddouble2
¯D≤˜O/parenleftbiggτ2G2
K2T2+cquad(τ)
ν2NKT+clin(τ)
ν4KT2+B(ϵ,ϵ1)G
ν+ Γ2(ϵ,ϵ1)/parenrightbigg
(79)
Proof.We take the step-size αt=8
ν(a+t+1)=2
ν1(a+t+1)fora>0. In addition, we define weights wt= (a+t)
and define
˜θT=1
WT/summationdisplay
t=1wt¯θt
whereW=/summationtextT
t=1wt≥1
2T(a+T). By convexity of positive definite quadratic forms ( λmin(ΦT¯DΦ)≥¯ω>0),
we have
ν1E/vextenddouble/vextenddouble/vextenddoubleV˜θT−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D
≤ν1
WT/summationdisplay
t=1(a+t)E/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D
≤ν1
W2τ−1/summationdisplay
t=1(a+t)E/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D+ν1
WT/summationdisplay
t=2τ(a+t)E/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D
≤ν1(2τ−1)(a+ 2τ−1)G2
W+ν1
WT/summationdisplay
t=2τ(a+t)E/vextenddouble/vextenddouble/vextenddoubleV¯θt−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D
(76)
≤ν1(2τ−1)(a+ 2τ−1)G2
W+ν1(a+ 2τ)(a+ 2τ+ 1)G2
2W
58Published in Transactions on Machine Learning Research (06/2024)
+1
WT/summationdisplay
t=2τ/bracketleftbigg(9 + 28τ2)d2
2
NK(a+t)αt+ (a+t)α2
t/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg/bracketrightbigg
+1
WT/summationdisplay
t=2τ(a+t)α2c6
K/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+1
WT/summationdisplay
t=2τ/bracketleftbig
4(a+t)B(ϵ,ϵ1)G+ (a+t)ν1Γ2(ϵ,ϵ1)/bracketrightbig
(80)
where/vextenddouble/vextenddouble/vextenddoubleV¯θ2τ−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D≤G2.We know that1
W/summationtextT
t=2τ(a+t)α2
t≤1
W/summationtextT
t=1(a+t)4
ν2
1(a+t)2≤8 log(a+T)
ν2
1T2and that
1
W/summationtextT
t=2τ(a+t)αt≤4
ν1T.Plugging in these inequalities into Eq (80), we have:
ν1E/vextenddouble/vextenddouble/vextenddoubleV¯θ−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D
≤3ν1(a+ 2τ)(a+ 2τ+ 1)G2
2W+4(9 + 28τ2)d2
2
ν1NKT
+8 log(a+T)
ν2
1T2/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg
+8c6log(a+T)
ν2
1T2K/bracketleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/bracketrightbigg
+ 4B(ϵ,ϵ1)G+ν1Γ2(ϵ,ϵ1)
=3ν1(a+ 2τ)(a+ 2τ+ 1)G2
2W+4(9 + 28τ2)d2
2
ν1NKT
+8 log(a+T)
ν2
1T2K/bracketleftbigg
K/parenleftbigg
36L2
2+108τ
1−ρ2L2
2+ 4L1G2+ 2L2G/parenrightbigg
+c6/parenleftbigg
c2
3+2c3L2ρ
1−ρ+ 8c2
1(K−1)H2/parenrightbigg/bracketrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
clin(τ)
+ 4B(ϵ,ϵ1)G+ν1Γ2(ϵ,ϵ1) (81)
wherecquad(τ) = 4d2
2(9 + 28τ2). Dividingν1on the both sides, changing ν1intoν(ν= (1−γ)¯ω) and noting
thatc6=4
α2g(14
ξ1+ 14ξ1)(c1+c2) =O(1
ν), we have:
E/vextenddouble/vextenddouble/vextenddoubleV˜θT−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D≤˜O/parenleftbiggτ2G2
K2T2+cquad(τ)
ν2NKT+clin(τ)
ν4KT2+B(ϵ,ϵ1)G
ν+ Γ2(ϵ,ϵ1)/parenrightbigg
. (82)
We finish the proof by using the inequality, E/vextenddouble/vextenddouble/vextenddoubleV˜θT−Vθ∗
i/vextenddouble/vextenddouble/vextenddouble2
¯D≤2E/vextenddouble/vextenddouble/vextenddoubleV˜θT−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯D+ 2E/vextenddouble/vextenddouble/vextenddoubleVθ∗
i−Vθ∗/vextenddouble/vextenddouble/vextenddouble2
¯Dand
combining with the third point in Theorem 1.
59Published in Transactions on Machine Learning Research (06/2024)
J Simulation Results
J.1 Simulation results for the I.I.D. setting
In this subsection, we provide numerical results for FedTD(0) under the i.i.d. sampling setting to verify the
theoretical results of Theorem 4. In particular, the MDP M(1)of the first agent is randomly generated with
a state space of size n= 100. The remaining MDPs are perturbations of M(1)with the heterogeneity levels
ϵ= 0.1andϵ1= 0.1. The number of local steps is chosen as K= 20.We evaluate the convergence in terms
of the running error et=∥¯θt−θ∗
1∥2. Each experiment is run 10times. We plot the mean and standard
deviation across the 10 runs in Figure 3.
Figure 3: Performance of FedTD(0) with i.i.d. sampling with varying number of agents N. Solid lines denote
the mean and shaded regions indicate the standard deviation over ten runs.
As shown in Fig 3, FedTD(0) converges for all choices of N. Larger values of Ndecreases the error, which is
consistent with our theoretical analysis in Theorem 4.
60Published in Transactions on Machine Learning Research (06/2024)
J.2 Simulation results for the Markovian setting
In this subsection, we provide numerical results for FedTD(0) under the Markovian sampling setting to verify
the theoretical results of Theorem 2. Here we generate all MDPs in the same way as the i.i.d setting and
choose the number of local steps as K= 20.All the remaining parameters are kept the same as those in the
subsection J.1.
Figure 4: Performance of FedTD(0) with the Markovian sampling with varying number of agents N. Solid
lines denote the mean and shaded regions indicate the standard deviation over ten runs.
As shown in Fig 4, FedTD(0) converges for all choices of N. Larger values of Ndecreases the error, which is
consistent with our theoretical analysis in Theorem 2.
61Published in Transactions on Machine Learning Research (06/2024)
J.3 Simulation on the effect of the heterogeneity level for the Markovian setting.
(a) Effect of the heterogeneity with different reward and Markov kernel heterogeneity.
(b) Effect of the heterogeneity with a fixed reward heterogeneity ϵrand different reward heterogeneity ϵ.
(c) Effect of the heterogeneity with a fixed Markov kernel heterogeneity ϵand different reward heterogeneity ϵr.
As shown in Fig (a)∼(c), we can conclude that increasing the level of heterogeneity level will increase the
size of the ball to which FedTD(0) converge, which is completely consistent with our theoretical analysis in
Theorem 2.
62