Under review as submission to TMLR
Time series recovery from partial observations via Nonneg-
ative Matrix Factorization
Anonymous authors
Paper under double-blind review
Abstract
In the modern analysis of time series, one may want to forecast several hundred or thousands
of times series with possible missing entries. We introduce a novel algorithm to address
these issues, referred to as Sliding Mask Method (SMM). SMM is a method based on the
framework of predicting a time window and using completion of nonnegative matrices. This
newprocedurecombinesnonnegativefactorizationandmatrixcompletionwithhiddenvalues
(i.e.,a partially observed matrix). From a theoretical point of view, we prove the statistical
guarantees on uniqueness and robustness of the solutions of the completion of partially
observed nonnegative matrices. From a numerical point of view, we present experiments on
real-world and synthetic data-set conﬁrm forecasting accuracy for the novel methodology.
1 Introduction
This article investigates the forecasting of several times series from partial observations. We study times
series for which one can provide a lower bound on the observations. In this case, one can assume that
the times series are entry-wise nonnegative , and one can exploit Nonnegative Matrix Factorization (NMF)
approaches, see for instance Paatero & Tapper (1994) and Lee & Seung (1999). For further details, we
refer the interested reader to the surveys Wang & Zhang (2013); Gillis (2015; 2017) and references therein.
NMF has been widely used in the contexts of document analysis Xu et al. (2003); Essid & Fevotte (2013),
hidden Markov chain Fu et al. (1999), representation learning in image recognition Lee & Seung (1999),
community discovery Wang et al. (2011), and clustering methods Turkmen (2015). This paper introduces
a novel NMF-like procedure for forecasting of several time series. Forecasting for temporal time series has
been previously done before through a mixed linear regression and matrix factorization as in Yu et al. (2016),
matrix completion for one temporal time series as in Gillard & Usevich (2018), and tensor factorization as
in de Araujo et al. (2017); Yokota et al. (2018); Tan et al. (2016).
Our proposed method, the Sliding Mask Method (SMM), inputs the forecast values and it can be viewed as
a nonnegative matrix completion algorithm under low nonnegative rank assumption. This framework raises
two issues. A ﬁrst question is the uniqueness of the decomposition, also referred to as identiﬁability of the
model. In Theorem 7, we introduce a new condition that ensures uniqueness from partial observation of
the target matrix M. An other challenge, as pointed by Vavasis (2009) for instance, is that solving exactly
NMF decomposition problem is NP-hard. Nevertheless NMF-type problems can be solved eﬃciently using
(accelerated) proximal gradient descent method Parikh & Boyd (2013) for block-matrix coordinate descent
in analternating projection scheme ,e.g., Javadi & Montanari (2020a) and references therein. We rely on
these techniques to introduce algorithms inputting the forecast values based on NMF decomposition, see
Section 2.3. Theorem 10 complements the theoretical analysis by proving the robustness of the solutions of
NMF-type algorithms when entries are missing or corrupted by noise.
Notation: Denote byA/latticetopthe transpose of matrix A. We use Rn×p
+to denoten×pnonnegative matrices.
It would be useful to consider the columns description Ak∈Rn1of matrix A= [A1···An2]and the row
decomposition A(k)∈Rn2of a matrix Ausing A/latticetop= [(A(1))/latticetop···(A(n1))/latticetop]forA∈Rn1×n2. Notation Ai,j
indicates the elements of matrix A;[n]represents the set {1,2,...,n};1dis the all-ones vector of size d;
and1Ais the indicator function of A, such that 1A= 0if conditionAis veriﬁed,∞otherwise.
1Under review as submission to TMLR
1.1 The problem of forecasting several time series
This article considers N≥1times series on the same temporal period of length T≥1in a setting where N
andTcould be such that N≥Tand possibly N/greatermuchT. We would like to forecast the next F≥1
times. Additionally, one may also aim at reduce the ambient dimension N×Twhile maintaining a good
approximationofthesetimesseries. Theobservedtimesseriescanberepresentedasamatrix MofsizeN×T.
A row M(i)ofMrepresents a time series and a column MjofMrepresents timestamp records. We assume
that there exists a targetmatrix M⋆with
M=M⋆
T+E,
where Eis some noise term and M⋆
T∈RN×T
+is a block given by
M⋆:=/bracketleftBig
M⋆
T/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
pastM⋆
F/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
future/bracketrightBig
of thetarget matrix of sizeN×(T+F). We decompose the target matrix using the timestamps up to
timeT, namely M⋆
T∈RN×T
+, and the timestamps of the future time period M⋆
F∈RN×F
+to be forecasted.
The statistical task is the following: given the observation Mpredict the future target values M⋆
F, and
incidentally a low dimensional representation M⋆
T.
1.2 The nonnegative analysis and the archetypal analysis
We aim to decompose a nonnegative matrix M∈RN×T
+as the product of nonnegative matrix W∈RN×K
and matrix H∈RK×Tby minimizing the Frobenius norm of the diﬀerence between the matrix Mand the
reconstructed matrix /hatwiderM:=WH, as in Cichocki et al. (2009),
min
W≥0,H≥0/bardblM−WH/bardbl2
F, (NMF)
which is convex in W(with ﬁxed H) and H(with ﬁxed W), but not convex in both varaibles. Another
approach consists in the archetypal analysis :
min
W≥0,W1=1
V≥0,V1=1/bardblM−WH/bardbl2
F+λ/bardblH−VM/bardbl2
F, (AMF)
whereλ>0is a tuning parameter, see for instance Javadi & Montanari (2020a). Diﬀerent normalisation and
constraints can be considered, we exhibit seven variants (see Table 7 in Appendix). We will be particularly
interested in
min
W≥0,W1K=1N
H≥0/bardblM−WH/bardbl2
F. (NNMF)
In(NMF),thesamplegivenbytherowsof Marerepresentedbyaconiccombination( i.e.,sumofnonnegative
weights) of the rows of H. Archetypal analysis (AMF) penalises the reciprocal: the rows of Hshould
be represented as a convex combination of the sample points given by the rows of M. Note that, both
parameters Kandλcan be tuned by cross-validation Arlot & Celisse (2010), as done in our experiments,
see Section 3.
1.3 Contribution: Nonnegative matrix completion for the analysis of several time series
Given the observation of several time series Msuch that M=M⋆
T+E, one would like to estimate the target
forecast M⋆
Fwhere M⋆:= [M⋆
TM⋆
F]. We will use a nonnegative matrix factorization to address it. This
factorization can be obtained by solving a block convex program, updating WthenHand repeating the
step. This class of algorithms can handle linear constraints. We will therefore allow ourselves to perform a
linear transformation on Mto predict M⋆
F. In particular, the operation which consists in taking a matrix V
and looking at the successive values given by a sliding window running through Vin the lexicographic order,
is a linear operation.
2Under review as submission to TMLR
General framework: We consider any injective linear map Φ:RN×(T+F)→Rn×pwithn≥Nand
p≥Fsuch that
∀A∈RN×T,B∈RN×F,Φ([A B]) =/bracketleftbiggΦ1(A)Φ2(A)
Φ3(A) B/bracketrightbigg
,
where Φk(k= 1,2,3) are linear maps.
Remark 1 An example of a linear map relevant for the forecast of several time series is presented in Sec-
tion 2, with n:= (B−W+ 1)N >Nandp:=WP >F .
We denote by
X0=Φ(M⋆),X=Φ([M 0N×F]),
X⋆=Φ([M⋆
T0N×F])and F=Φ([E 0N×F]).
Note that the identity M=M⋆
T+Eimplies that X=X⋆+F. Consider a so-called maskoperator T(N)
that sets to zero N×Fvalues of an n×pmatrix N. Namely, given N∈Rn×p, we deﬁne
T(N) =/bracketleftbiggN1N2
N30N×F/bracketrightbigg
and T⊥(N) =/bracketleftbigg00
0N4/bracketrightbigg
,
where (Ni)4
i=1are blocks of N=/bracketleftbiggN1N2
N3N4/bracketrightbigg
. Note that T(X0) =X⋆.
A nonnegative matrix completion problem: Our goal is the following matrix completion problem:
Given a noisy and incomplete observation
X=T(X0) +F, (1)
where Fis some noise term, ﬁnd a good estimate of the target X0.
We introduce the Mask NNMF:
min
W1=1,W≥0
H≥0
T(N)=X/bardblN−WH/bardbl2
F, (mNMF)
where solutions N∈Rn×pare such that T(N) =X(observed values) and T⊥(N) =T⊥(WH )(forecast
values). This latter formulation is an instance of Matrix Completion Nguyen et al. (2019). Forecasting
problem reduces to Matrix Completion problem, whose aim is ﬁnding the nonnegative matrix factorization
N/similarequalWHof observed matrix Xsuch that T(N) =X.
Remark 2 Problem (mNMF) is NNMF when T=I, where Iis the identity operator.
One can drop the constraint H≥0which leads to an other approach referred to as Mask AMF:
min
W≥0,W1=1
V≥0,V1=1
T(N)=X/bardblN−WH/bardbl2
F+λ/bardblH−VN/bardbl2
F (mAMF)
Remark 3 When T=I, Problem (mAMF) reduces to standard AMF formulation (AMF).
1.3.1 Uniqueness from partial observations
Recall that X⋆is themaskofX0since
X⋆=T(X0) =/bracketleftbiggX1X2
X30N×F/bracketrightbigg
,
where X1∈R(n−N)×(p−F),X2∈R(n−N)×F, and X3∈RN×(p−F)are blocks of X0. Let us consider
Ttrain(X0) := [X1X2],Ttest(X0) := [X30N×F],
TT(X0) :=/bracketleftBigX1
X3/bracketrightBig
, TF(X0) :=/bracketleftBigX2
0N×F/bracketrightBig
.
3Under review as submission to TMLR
Remark 4 LetX0:=W0H0,H0:= [H0TH0F], and W/latticetop
0:= [W0/latticetop
trainW0/latticetop
test], then
Ttrain(X0) =W0trainH0,X3=W0testH0T,
TT(X0) =W0H0T, X2=W0trainH0F.
A ﬁrst issue is the uniqueness of the decomposition W0H0given partial observations , namely proving that
Partial Observation Uniqueness (POU) property holds:
IfT(WH ) =T(W0H0)Then (W,H)≡(W0,H0), (POU)
where≡means up to positive scaling and permutation: if an entry-wise nonnegative pair (W,H)is given
then (WPD,D−1P/latticetopH)is also a nonnegative decomposition WH =WPD×D−1P/latticetopH, where Dscales
andPpermutes the columns (resp. rows) of W(resp. H). When we observe the full matrix X0=W0H0, the
issue on uniqueness has been addressed under some suﬃcient conditions on W,H,e.g.,Strongly boundary
closeness ofLaurbergetal.(2008), Complete factorial sampling ofDonoho&Stodden(2004), and Separability
of Recht et al. (2012). A necessary and suﬃcient condition exists as given by the following theorem.
Theorem 5 (Thomas (1974)) The decomposition X0:=W0H0is unique up to permutation and positive
scaling of columns (resp. rows) of W0(resp. H0)if and only if theK-dimensional positive orthant is the
onlyK-simplicial cone verifying Cone( W/latticetop
0)⊆C⊆ Cone( H0)where Cone( A)is the cone generated by the
rows of A.
Our ﬁrst main assumption is:
•Assumption (A1) In the set given by the union of sets:
{C: Cone( W0train/latticetop)⊆C⊆ Cone( H0)}/uniondisplay
{C: Cone( W/latticetop
0)⊆C⊆ Cone( H0T)},
the nonnegative orthant is the only K-simplicial cone.
It is clear that this property is implied by the following one, namely ( A’1)⇒(A1).
•Assumption (A’1) In the set
{C: Cone( W0train/latticetop)⊆C⊆ Cone( H0T)}
the nonnegative orthant is the only K-simplicial cone.
We consider the following standard deﬁnition.
Deﬁnition 6 (Javadi & Montanari (2020a)) The convex hull conv( X0)has an internal radius µ>0if
it contains an K−1dimensional ball of radius µ.
Our second main assumption is that:
•Assumption (A2) Assume that
conv( Ttrain(X0)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=W0trainH0)has internal radius µ>0. (A2)
Theorem 7 The Assumption (A1)implies the Property (POU). Moreover, if (A1)and(A2)holds,
T(WH ) =T(W0H0)andW01=W1 =1then (W,H) = ( W0,H0)up to permutation of columns
(resp. rows) of W(resp. H), and there is no scaling.
Proof.Proofs are given in Supplement Material.
Remark 8 By Theorem 5, observe that ( A’1) is a necessary and suﬃcient condition for the uniqueness of
the decomposition X1=W0trainH0T. Then, using (A’1)⇒(A1), we understand that if decomposition of
X1=W0trainH0Tis unique then (POU)holds.
4Under review as submission to TMLR
1.3.2 Robustness under partial observations
The second issue is robustness to noise . To the best of our knowledge, all the results addressing this issue
assume that the noise error term is small enough, e.g., Laurberg et al. (2008), Recht et al. (2012), or Javadi
& Montanari (2020a). In this paper, we extend these stability result to the nonnegative matrix completion
framework (partial observations) and we also assume that noise term /bardblF/bardblFis small enough.
In the normalized case ( i.e.,W1 =1), both issues (uniqueness and robustness) can be handle with the
notion ofα-uniqueness, introduced by Javadi & Montanari (2020a). This notion does not handle the matrix
completion problem we are addressing. To this end, let us introduce the following notation. Given two
matrices A∈Rna×pandB∈Rnb×pwith same row dimension, and C∈Rna×nb, deﬁne the divergence
D(A,B)as
D(A,B) := min
C≥0,C1nb=1nana/summationdisplay
a=1/vextenddouble/vextenddouble/vextenddoubleA(a)−nb/summationdisplay
b=1CabB(b)/vextenddouble/vextenddouble/vextenddouble2
F,
= min
C≥0,C1nb=1na/bardblA−CB/bardbl2
F.
which is the squared distance between rows of Aandconv( B), the convex hull of rows of B. For B∈Rn×p
deﬁne
/tildewideD(A,B) := min
C≥0,C1n=1na
T(N−B)=0/bardblA−CN/bardbl2
F.
Deﬁnition 9 ( Tα-unique) Given X0∈Rn×p,W0∈Rn×K,H0∈RK×p, the factorization X0=W0H0
isTα-unique with parameter α>0if for all H∈RK×pwith conv( X0)⊆conv( H):
/tildewideD(H,X0)1/2≥/tildewideD(H0,X0)1/2+α/braceleftBig
D(H,H0)1/2+D(H0,H)1/2/bracerightBig
.
Our third main assumption is given by:
•Assumption (A3) Assume that
X0=W0H0isTα-unique ( A3)
Theorem 10 If(A2)and(A3)hold then there exists positive reals ∆andΛ(depending on X0) such that,
for all Fsuch that/bardblF/bardblF≤∆and0≤λ≤Λ, any solution (/hatwiderW,/hatwideH)to(mAMF) (ifλ/negationslash= 0) or(mNMF) (if
λ= 0) with observation (1)is such that:
/summationdisplay
/lscript≤[K]min
/lscript/prime≤[K]/bardblH0/lscript−/hatwideH/lscript/prime/bardbl2
2≤c/bardblF/bardbl2
F,
wherecis a constant depending only on X0.
Proof.Proofs are given in Supplement Material.
1.4 Outline
The rest of the paper is organized as follows. In Section 2 we discuss the Sliding Mask Method (SMM). We
present numerical experiments in Section 3, while conclusions are drawn in Section 4. A repository on the
numerical experiments can be found at [link redacted to comply with double blind reviewing]
2 The Sliding Mask Method
2.1 Sliding window as forecasting
One is given Ntime series M(1),...,M(N)∈RTover a period of Tdates. Recall M∈RN×Tis the matrix
of observation such that M/latticetop= [(M(1))/latticetop···(M(N))/latticetop]and assumed entry-wise nonnegative. We assume
5Under review as submission to TMLR
someperiodicity in our time series, namely that M⋆can be split into Bmatrix blocks of size N×Pwhere
P= (T+F)/B, see Figure 1.
Figure 1: Target matrix M⋆can be split into Bblocks of same time length P.
GivenW≥1and aT×Nmatrix M, we deﬁne Π(M)the linear operator that piles up Wconsecutive
sub-blocks in a row, as depicted in Figure 2. This process looks at Wconsecutive blocks in a slidingmanner.
Note that Π(M)is anincomplete matrix where the missing values are depicted in orange in Figure 2, they
correspond to the time-period to be forecasted. Unless otherwise speciﬁed, these unobserved values are set
to zero. Remark that Π(M)hasWcolumns blocks, namely WPcolumns and (B−W+ 1)Nrows. By an
abuse of notation, we also denote
Π:RN×(T+F)→R(B−W+1)N×WP
the same one-to-one linear matrix operation on matrices of size N×(T+F). In this case, X0:=Π(M⋆)is
acomplete matrix where the orange values have been implemented with the future values of the target M⋆
F.
Figure 2: The operator Π(M)outputs an incomplete (B−W+1)N×WPmatrix given by a mask where the
NForange entries are not observed. These entries corresponds to future times that should be forecasted.
The rationale behind is recasting the forecasting problem as a supervised learning problem where one ob-
serves, at each line of Π(M), theWP−Fﬁrst entries and learn the next Fentries. The training set is
given by rows 1,3,5,7inΠ(M)of Figure 2 and the validation set is given by rows 2,4,6,8where one aims
at predicting the Fmissing values from the WP−Fﬁrst values of these rows.
2.2 Mask NNMF and Mask AMF
Consider a matrix completion version of NMF with observations
X=Π(M) =Π(M⋆
T)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
X⋆+Π(E)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
F,
and
min
W1=1,W≥0,H≥0/bardblX−T(WH )/bardbl2
F, (2)
6Under review as submission to TMLR
where the “mask” operator Tis deﬁned by zeroing the “future” values (in orange in Figure 2). Note that
T(Π(M⋆)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
X0) =Π(M⋆
T) =X⋆.
Moreover, note that Problem (2) is equivalent to mask NNMF (mNMF). If we drop the nonnegative con-
straints on Hand consider the archetypal approach, we obtain mask AMF (mAMF). In particular, Theo-
rem 10 applies proving that (mNMF) and (mAMF) are robust to small noise.
2.3 Algorithms
2.3.1 Alternating Least Squares for (mNMF)
The basic algorithmic framework for matrix factorization problems is Block Coordinate Descent (BCD)
method, which can be straightforwardly adapted to (mNMF) (see Supplement Material). BCD for (mNMF)
reduces to Alternating Least Squares (ALS) algorithm (see Algorithm 4 in Appendix), when an alternative
minimization procedure is performed and matrix WHis projected onto the linear subspace T(N) =Xby
means of operator PX, as follows:
N:=PX(WH ) :T(N) =XandT⊥N=WH.
Hierarchical Alternating Least Squares (HALS) is an ALS-like algorithm obtained by applying an exact
coordinate descent method Gillis (2014). Moreover, an accelerated version of HALS is proposed in Gillis &
Glineur (2012) (see Supplement Material).
2.3.2 Projected Gradient for (mAMF)
Proximal Alternative Linear Minimization (PALM) method, introduced in Bolte et al. (2014) and applied
to AMF by Javadi & Montanari (2020a), can be also generalized to (mAMF) (see Algorithm 1).
Algorithm 1 PALM for mAMF
1:Initialization : chose H0,W0≥0such that W01=1, setN0:=PX(W0H0)andi:= 0.
2:whilestopping criterion is not satisﬁed do
3:/tildewideHi:=Hi−1
γi
1Wi/latticetop/parenleftbig
WiHi−Ni/parenrightbig
⊿Gradient step on H, objective ﬁrst term
4: Hi+1:=/tildewideHi−λ
λ+γi
1/parenleftBig
/tildewideHi−Pconv(Ni)(˜Hi)/parenrightBig
⊿Gradient step on H, objective second term
5: Wi+1:=P∆/parenleftBig
Wi−1
γi
2/parenleftbig
WiHi+1−Ni/parenrightbig
Hi+1/latticetop/parenrightBig
⊿Projected gradient step on W
6: Ni+1:=PX/parenleftBig
Ni+1
γi
3/parenleftbig
Wi+1Hi+1−Ni/parenrightbig/parenrightBig
⊿Projected gradient step on N
7:i:=i+ 1
8:end while
Pconv(A)is the projection operator onto conv( A)andP∆is the projection operator onto the (N−1)-
dimensional standard simplex ∆N. The two projections can be eﬃciently computed by means, e.g., Wolfe
algorithm Wolfe (1976) and active set method Condat (2016), respectively.
Theorem 11 Letε > 0. Ifγi
1>/bardblWi/latticetopWi/bardblF,γi
2>max/braceleftBig
/bardblHi+1Hi+1/latticetop/bardblF, ε/bracerightBig
, andγi
3>1, for each
iterationi, then the sequence/parenleftbig
Hi,Wi,Ni/parenrightbig
generated by Algorithm 1 converges to a stationary point of
Ψ(H,W,N) :=f(H) +g(W) +p(N) +h(H,W,N), where:
f(H) =λD(H,N), g (W) =/summationtextK
k=11{Wk∈∆},
p(N) =1{N=PX(WH)}, h (H,W,N) =/bardblN−WH/bardbl2
F.
Proof.Proof is given in Supplement Material.
7Under review as submission to TMLR
Algorithm 2 iPALM for mAMF
1:Initialization :H0,W0≥0such that W01=1, setN0:=PX(W0H0),H−1:=H0,W−1:=W0,
N−1:=N0, andi:= 0.
2:whilestopping criterion is not satisﬁed do
3: Hi
1:=Hi+αi
1/parenleftbig
Hi−Hi−1/parenrightbig
,Hi
2:=Hi+βi
1/parenleftbig
Hi−Hi−1/parenrightbig
⊿Inertial H
4:/tildewideHi:=Hi
1−1
γi
1Wi/latticetop/parenleftbig
Hi
2Wi−Ni/parenrightbig
⊿Gradient step on H, objective ﬁrst term
5: Hi+1:=/tildewideHi−λ
λ+γi
1/parenleftBig
/tildewideHi−Pconv(Ni)(˜Hi)/parenrightBig
⊿Gradient step on H, objective second term
6: Wi
1:=Wi+αi
2/parenleftbig
Wi−Wi−1/parenrightbig
,Wi
2:=Wi
1+βi
2/parenleftbig
Wi−Wi−1/parenrightbig
⊿Inertial W
7: Wi+1:=P∆/parenleftBig
Wi
1−1
γi
2/parenleftbig
Wi
2Hi+1−Ni/parenrightbig
Hi+1/latticetop/parenrightBig
⊿Projected gradient step on W
8: Ni
1:=Ni
1+αi
3/parenleftbig
Ni−Ni−1/parenrightbig
,Ni
2:=Ni
1+βi
3/parenleftbig
Ni−Ni−1/parenrightbig
⊿Inertial N
9: Ni+1:=PX/parenleftBig
Ni
1+1
γi
3/parenleftbig
Wi+1Hi+1−Ni
2/parenrightbig/parenrightBig
⊿Projected gradient step on N
10:i:=i+ 1
11:end while
Finally, the inertial PALM (iPALM) method, introduced for NMF in Pock & Sabach (2016), is generalized
to (mAMF) in Algorithm 2.
Remark 12 If, for all iterations i,αi
1=αi
2= 0andβi
1=βi
2= 0, iPALM reduces to PALM.
2.3.3 Stopping criterion for normalized NMF
For NNMF, KKT conditions regarding matrix Ware the following (see Supplement Material):
W◦/parenleftbig
(WH−N)H/latticetop+t1/latticetop
K/parenrightbig
= 0.
By complementary condition, it follows that, ∀j,ti= ((WH−N)H/latticetop)i,j. Hence, we compute tiby selecting,
for each row W(i), any positive entry Wi,j>0.
Remark 13 Numerically to obtain a robust estimation of ti, we can average the corresponding values cal-
culated per entry Wi,j.
LetεW,εH, andεRbe three positive thresholds. The stopping criterion for the previous algorithms consists
in a combination of:
1. the maximum number of iterations;
2. the Frobenius norm of the diﬀerence of WandHat two consecutive iterations, i.e., the algorithm
stops if/bardblWi+1−Wi/bardblF≤εW∧ /bardblHi+1−Hi/bardblF≤εH;
3. a novel criterion based on KKT condition, i.e., the algorithm stops if it holds that /bardblR(Wi+1)/bardblF+
/bardblR(Hi+1)/bardblF≤εRwhere matrices R(W)andR(H)are deﬁned as R(W)i,j:=|(WH−N)H/latticetop)i,j+
ti|1{Wi,j/negationslash=0}andR(H)i,j:=|W/latticetop(WH−N))i,j|1{Hi,j/negationslash=0}, respectively.
2.4 Large-scale data-set
Assume the observed matrix X=Π(M)is large scaled, namely one has to forecast a large number Nof
times series ( e.g.more than 100,000) and possibly a large number of time stamps T. The strategy, described
in Section 1.3.1 in Cichocki et al. (2009) for NMF, is to learn the H∈RK×Tmatrix from a sub-matrix
Nr∈Rr×TofK≤r/lessmuchNrows of N∈Rn×T, and learn the W∈RN×Kmatrix from a sub-matrix
Nc∈RN×cofK≤c/lessmuchTcolumns of N∈RN×T. We denote by Hcthe sub-matrix of Hgiven by the
columns appearing in NcandWrthe sub-matrix of Hgiven by the columns appearing in Nc.
8Under review as submission to TMLR
This strategy can be generalized to (mNMF) and (mAMF). For (mNMF) this generalization is straightfor-
ward, and for (mAMF) one need to change Steps 3-5 in Algorithm 1 as follows:
/tildewideHi:=Hi−1
γi
1(Wi
r)/latticetop/parenleftbig
Wi
rHi−Ni
r/parenrightbig
Hi+1:=/tildewideHi−λ
λ+γi
1/parenleftBig
/tildewideHi−Pconv(Ni)(˜Hi)/parenrightBig
Wi+1:=P∆/parenleftbigg
Wi−1
γi
2/parenleftbig
WiHi+1
c−Ni
c/parenrightbig
(Hi+1
c)/latticetop/parenrightbigg
.
Same approach is exploited for Algorithm 2.
3 Numerical Experiments
We tested SMM with random initialization of matrices H0,W0. Each entry in H0is randomly selected
in[0,h]whereh > 0is chosen by practitioner. Each row of matrix W0is randomly generated in the
corresponding standard simplex.
For SMM we implemented both HALS for (mNMF) and PALM for (mAMF). Moreover, we consider two
diﬀerent strategies to deﬁne matrix Π(M): withnon-overlapping (mAMF and mNMF) and overlapping
sliding intervals (mAMFo and mNMFo). In the overlapping strategy, we replicate the half of each sub-block
in which the matrix Mis sub-divided.
Moreover, we have compared our method with other classically-designed mainstream time series forecasting
methods such as Random Forest Regression (RFR) and EXponential Smoothing (EXS),Long Short-Term
Memory (LSTM) and Gated Recurrent Units (GRU) deep neural networks with preliminary data standard-
ization Shewalkar et al. (2019), and Seasonal Auto-Regressive Integrated Moving Average with eXogenous
factors(SARIMAX) models Douc et al. (2014). In our computational experiments, we consider hundreds
of time series, so we do not benchmark against time series transformer model, which are suitably designed
instead for huge-scale time series forecasting problems.
The quality of the forecasted matrix MFis measured by the relative root-mean-squared error (RRMSE) and
the relative mean-percentage error (RMPE):
RRMSE =/bardblMF−M⋆
F/bardblF
/bardblM⋆
F/bardblF,RMPE =/bardblMF−M⋆
F/bardbl1
/bardblM⋆
F/bardbl1.
The interested reader may ﬁnd a github repository on numerical experiments at [link redacted to comply
with double blind reviewing]
We run all the numerical tests on a MacBook Pro mounting macOS Ventura 13.6.1 with Apple M2 chip and
8 GB LPDDR5 memory RAM.
3.1 Real-world data-sets
The numerical experiments refer to the following real-world data-sets:
•weekly and daily electricity consumption data-sets of 370Portuguese customers during the period
2011-2014, Trindade (2015)
•twin gas measurements data-set of ﬁve replicates of an 8-MOX gas sensor, Fonollosa (2016)
•Istanbul Stock Exchange returns with seven other international index for the period 2009-2011,
Akbilgic (2013)
•demand forecasting orders in a Brazilian logistics company collected for 60 days, Ferreira et al.
(2017)
9Under review as submission to TMLR
•daily electricity transformer temperature (ETT) measurements, Zhou et al. (2020)
Tables 1-2 (see Appendix) report the cross-validated RRMSE and RMPE on observed values obtained
during the computational tests for each method. Table 5 reports the CPU time for the forecasting phase.
We highlight best results in bold, and second best results are underlined .
Our method is always the best or the second best one among all the approaches for all the data-set we tested
in terms of RRMSE and RMPE indices, and there is no other method performing better. Concerning the
CPU times, our novel methods are competitive against EXP, being the fastest or the second fastest ones.
mAMFoseemstobethemostpromisingalgorithmintermsofeﬃcacyfortheﬁrstﬁvedata-sets, whilemAMF
and mNMF are the best methods for the last four ETT data-sets. The ETT data-sets are characterized by a
pronounced periodicity: in this case the plain methods performs quite well; while the ﬁrst ﬁve data-sets are
less markedly periodic and in this case the overlap versions aims to introduce a sort of artiﬁcial periodicity
in the dataset by replicating portions of the sub-block in which each row of Mis split.
3.2 Synthetic data-sets
Further computational experiments have been realized by considering additional synthetic data-sets. In
particular, we generated three data sets by replicating 1,000short time series (with 10 time periods) 10
times and adding white noise multiplied by a constant factor σto each time series entry separately. We
chooseσ∈{0.005,0.1,1}. We refer to the these data-sets as “low noise”, “medium noise”, and “high noise”,
respectively.
An additional synthetic data-set has been generated by considering few probability vectors, and computing
the entire matrix Wby randomly choosing a probability vector and adding white noise. A completely
randomly generated matrix His multiplied to Wto obtain the whole matrix M∗:=WH. We refer to this
dataset as “few distribution”.
Finally, the last synthetic data-set is obtained by generating matrix Hby replicating a small time series
(with 50 time periods) 100 times and adding white noise multiplied by a constant factor σ= 1and matrix
Wof suitable dimensions, whose rows are uniformly distributed over the corresponding dimensional simplex.
Then, we set matrix M∗:=WH. We refer to this last dataset as “periodic archetypes”.
Tables 3-4 and 6 (see Appendix) report the cross-validated RRMSE and RMPE indices, and the CPU time
for the forecasting phase, respectively, referring to synthetic generated datasets. The more pronounced
the periodicity of the time series or of the archetypes, the better the performances of our proposed NMF-
like methods: in this case, the more realistic the hypothesis that the whole data-set can be expressed
as convex combinations of few archetypes, having a low-rank representation. We are able to replicate
SARIMAX performances in a much shorter time; while, for the last two synthetic data-sets, mAMF and
mNMF outperform the benchmarks.
4 Conclusions and Perspectives
In this paper, we have introduced and described a novel approach for the time series forecasting problem
relying on nonnegative matrix factorization. We apply this algorithm to realistic data-sets and synthetics
data-sets, showing the forecasting capabilities of the proposed methodology.
Moreover, we have shown several uniqueness and robustness theoretical results for the solution of the matrix
factorization problems faced by the proposed algorithm, namely the Sliding Mask Method .
The strength of the proposed methodology consists in its relatively loose assumptions, mainly by supposing
that time series matrix can be eﬃciently described by a low rank nonnegative decomposition, and that the
time series are periodic for the Sliding Mask Method . Moreover, the Sliding Mask Method can be applied in
presence of missing entries in the dataset: in this latter case, in the mask operation one should consider only
the known past values of each time series.
10Under review as submission to TMLR
Future works consists in embedding side information in the forecasting procedure by extending algorithms
in Mei et al. (2019) to the Sliding Mask Method .
References
O. Akbilgic. Istanbul Stock Exchange. UCI Machine Learning Repository, 2013. DOI:
https://doi.org/10.24432/C54P4J.
S. Arlot and A. Celisse. A survey of cross-validation procedures for model selection. Statistics Surveys , 4:
40–79, 2010. doi: 10.1214/09-SS054.
J. Bolte, S. Sabach, and M. Teboulle. Proximal alternating linearized minimization for nonconvex and
nonsmooth problems. Mathematical Programming , 146(1–2):459–494, 2014.
A. Cichocki and R. Zdunek. NMFLAB for signal processing , 2006.
A. Cichocki, R. Zdunek, A.H. Phan, and S.-I. Amari. Nonnegative matrix and tensor factorization: Appli-
cations to exploratory multi-way data analysis and blind source separation . John Wiley and Sons, 2009.
L. Condat. Fast projection onto the simplex and the l1ball.Mathematical Programming , 158(1–2):575–585,
2016.
M.R. de Araujo, P.M.P. Ribeiro, and C. Faloutsos. Tensorcast: Forecasting with context using coupled
tensors (best paper award). In 2017 IEEE International Conference on Data Mining (ICDM) , pp. 71–80.
IEEE, 2017.
D. Donoho and V. Stodden. When does non-negative matrix factorization give a correct decomposition into
parts? In S. Thrun, L. K. Saul, and B. Schölkopf (eds.), Advances in Neural Information Processing
Systems 16 , pp. 1141–1148. MIT Press, 2004.
R. Douc, E. Moulines, and D. Stoﬀer. Nonlinear time series: Theory, methods, and applications with R
examples . Chapman & Hall/CRC, 2014.
S. Essid and C. Fevotte. Smooth nonnegative matrix factorization for unsupervised audiovisual document
structuring. IEEE Transactions on Multimedia , 15(2), 2013.
R. Ferreira, A. Martiniano, A. Ferreira, A. Ferreira, and R. Sassi. Daily Demand Forecasting Orders. UCI
Machine Learning Repository, 2017. DOI: https://doi.org/10.24432/C5BC8T.
J. Fonollosa. Twin gas sensor arrays. UCI Machine Learning Repository, 2016. DOI:
https://doi.org/10.24432/C5MW3K.
X. Fu, K. Huang, N.D. Sidiropoulos, and W.-K. Ma. Nonnegative matrix factorization for signal and data
analytics: Identiﬁability, algorithms, and applications. IEEE Signal Processing Magazine , 36(2):59–80,
1999.
J. Gillard and K. Usevich. Structured low-rank matrix completion for forecasting in time series analysis.
International Journal of Forecasting , 34(4):582–597, 2018.
N. Gillis. The why and how of nonnegative matrix factorization. In J.A.K. Suykens, M. Signoretto, and
A. Argyriou (eds.), Regularization, optimization, kernels, and support vector machines , Machine Learning
and Pattern Recognition Series, pp. 257–291. Chapman & Hall/CRC, 2014.
N.Gillis. Thewhyandhowofnonengativematrixfactorization. InJ.A.K.Suykens, M.Signoretto, andA.Ar-
gyriou (eds.), Regularization, optimization, kernels, and support vector machines . Chapman & Hall/CRC,
2015.
N. Gillis. Introduction to nonnegative matrix factorization. SIAG/OPT Views and News , 25(1):7–16, 2017.
11Under review as submission to TMLR
N. Gillis and F. Glineur. Accelerated multiplicative updates and hierarchical ALS algorithms for nonnegative
matrix factorization. Neural Computation , 24(4):1085–1105, 2012. doi: 10.1162/NECO\_a\_00256. URL
https://doi.org/10.1162/NECO_a_00256 .
N. Gillis and A. Kumarg. Exact and heuristic algorithms for semi-nonnegative matrix factorization. SIAM
Journal on Matrix Analysis and Applications , 36(4):1404–1424, 2015.
H. Javadi and A. Montanari. Nonnegative matrix factorization via archetypal analysis. Journal of the
American Statistical Association , 115(530):896–907, 2020a.
H. Javadi and A. Montanari. Supplement To “Non-negative Matrix Factorization via Archetypal Analysis”.
Journal of the American Statistical Association , 115(530):896–907, 2020b. URL https://doi.org/10.
1080/01621459.2019.1594832 .
H. Laurberg, M.G. Christensen, M.D. Plumbley, L.K. Hansen, and S.H. Jensen. Theorems on positive data:
On the uniqueness of NMF. Computational Intelligence and Neuroscience , 2008:1–9, 2008.
D.D. Lee and H.S. Seung. Learning the parts of objects by nonnegative matrix factorization. Nature, 401
(6755):788–791, 1999.
J. Mei, Y. De Castro, Y. Goude, J.-M. Azaïs, and G. Hébrail. Nonnegative matrix factorization with
side information for time series recovery and prediction. IEEE Transactions on Knowledge and Data
Engineering , 31(3):493–506, 2019.
L.T. Nguyen, K. Junhan, and S. Byonghyo. Low-rank matrix completion: A contemporary survey. IEEE
Access, 7:94215–94237, 2019. doi: 10.1109/ACCESS.2019.2928130.
P. Paatero and U. Tapper. Positive matrix factorization: A nonnegative factor model with optimal utilization
of error estimates of data values. Environmetrics , 5(1):111—126, 1994.
N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in Optimization , 1(3):123–231, 2013.
T. Pock and S. Sabach. Inertial proximal alternating linearized minimization (iPALM) for nonconvex
and nonsmooth problems. SIAM Journal on Imaging Sciences , 9(4):1756–1787, 2016. doi: 10.1137/
16M1064064. URL https://doi.org/10.1137/16M1064064 .
B. Recht, C. Re,, J. Tropp, and V. Bittorf. Factoring nonnegative matrices with linear programs. In
F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information
Processing Systems 25 , pp. 1214–1222. Curran Associates, Inc., 2012.
A. Shewalkar, D. Nyavanandi, and S.A. Ludwig. Performance evaluation of deep neural networks applied
to speech recognition: RNN, LSTM and GRU. Journal of Artiﬁcial Intelligence and Soft Computing
Research , 9:235–245, 2019.
H. Tan, Y. Wu, B. Shen, P.J. Jin, and B. Ran. Short-term traﬃc prediction based on dynamic tensor
completion. IEEE Transactions on Intelligent Transportation Systems , 17(8):2123–2133, 2016.
L. Thomas. Solution to problem 73–14, rank factorizations of nonnegative matrices. SIAM Review , 16(1):
393–394, 1974.
A. Trindade. ElectricityLoadDiagrams20112014. UCI Machine Learning Repository, 2015. DOI:
https://doi.org/10.24432/C58C86.
A.C. Turkmen. A Review of nonnegative matrix factorization methods for clustering. https://arxiv.org/
abs/1507.03194 , 2015.
S.A. Vavasis. On the complexity of nonnegative matrix factorization. SIAM Journal on Optimization , 20
(3):1364–1377, 2009. ISSN 1052–6234.
F. Wang, T. Li, X. Wang, S. Zhu, and C. Ding. Community discovery using nonnegative matrix factorization.
Data Mining and Knowledge Discovery , 22(3):493–521, 2011.
12Under review as submission to TMLR
Y.-X.WangandY.-J.Zhang. Nonnegativematrixfactorization: Acomprehensivereview. IEEE Transactions
on Knowledge and Data Engineering , 25(6):1336–1353, 2013.
T. Wolfe. Finding the nearest point in a polytope. Mathematical Programming , 11:128–149, 1976.
W.Xu, X.Liu, andY.Gong. Documentclusteringbasedonnon-negativematrixfactorization. In Proceedings
of the 26th annual international ACM SIGIR conference on Research and development in informaion
retrieval, pp. 267–273, 2003.
T. Yokota, B. Erem, S. Guler, S.K. Warﬁeld, and H. Hontani. Missing slice recovery for tensors using a
low-rank model in embedded space. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pp. 8251–8259, 2018.
H.-F. Yu, N. Rao, and I.S. Dhillon. Temporal regularized matrix factorization for high-dimensional time
series prediction. In NIPS, pp. 847–855, 2016.
H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang. Informer: Beyond eﬃcient transformer
for long sequence time-series forecasting. In AAAI Conference on Artiﬁcial Intelligence , 2020.
13Under review as submission to TMLR
Algorithms RFR LSTM GRU EXP SARIMAX
Metrics RRMSE RMPE RRMSE RMPE RRMSE RMPE RRMSE RMPE RRMSE RMPE
weekly electricity 7.25% 8.61% 27.85% 15.64% 26.04% 15.92% 10.07% 7.98% 9.05%7.42%
daily electricity 12.16% 47.78% 12.42% 46.49% 12.03% 45.90% 11.25% 43.83% 9.85% 43.16%
gas 66.80% 71.61% 62.97% 68.38% 62.87% 67.90% 63.35% 68.16% 45.58% 52.83%
Istanbul 15.37% 18.32% 16.22% 20.96% 20.01% 26.87% 15.46% 18.64% 14.75% 17.01%
demand orders 16.64% 25.12% 18.47% 29.45% 18.89% 30.60% 18.34% 25.94% 22.99% 31.42%
ETTh1 12.96% 17.98% 14.86% 18.78% 14.71% 18.85% 12.37%13.65% 13.36% 15.94%
ETTh2 6.47% 7.60% 14.17% 13.75% 14.44% 14.36% 14.06% 13.67% 12.76% 13.03%
ETTm1 12.81% 17.42% 13.39% 17.96% 14.13% 18.63% 11.45% 14.20% 12.29% 16.45%
ETTm2 5.81% 7.16% 14.29% 13.89% 14.46% 14.03% 13.18% 12.88% 13.16% 12.95%
Table 1: Real-world data-sets: RRMSE and RPME indices (benchmarks)
Algorithms mAMF mNMF mAMFo mNMFo
Metrics RRMSE RMPE RRMSE RMPE RRMSE RMPE RRMSE RMPE
weekly electricity 12.74% 23.45% 9.34% %16.76% 8.13% 12.16% 12.56% 13.93%
daily electricity 11.68% 37.20% 14.44% 40.51% 12.08% 35.92% 18.36%32.34%
gas 50.93% 46.41% 65.41% 65.40% 48.33% 49.86% 56.54% 58.62%
Istanbul 15.90% 18.49% 17.98% 17.74% 14.74% 15.46% 18.02% 18.83%
demand orders 23.02% 34.72% 24.83% 34.95% 28.25% 39.88% 17.68% 31.85%
ETTh1 11.72% 14.71% 11.93% 15.22% 14.81% 24.07% 23.86% 25.57%
ETTh2 5.90% 7.39% 4.39% 6.68% 21.86% 23.03% 18.29% 21.29%
ETTm1 10.63% 14.82% 11.26% 13.49% 15.27% 26.23% 20.02% 26.30%
ETTm2 5.83% 7.28% 5.04% 7.53% 20.82% 22.35% 16.90% 21.41%
Table 2: Real-world data-sets: RRMSE and RPME indices (novel algorithms)
14Under review as submission to TMLR
Algorithms RFR LSTM GRU EXP SARIMAX
Metrics RRMSE RMPE RRMSE RMPE RRMSE RMPE RRMSE RMPE RRMSE RMPE
low noise 8.65% 22.76% 16.52% 46.03% 16.76% 46.46% 16.97% 48.02% 0.19% 0.33%
medium noise 8.42% 21.95% 15.66% 42.68% 15.67% 42.98% 15.94% 44.25% 1.97% 4.94%
high noise 11.73% 30.26% 13.03% 33.67% 12.97% 33.47% 13.02% 33.27% 15.24%27.37%
few distributions 5.79% 9.44% 6.76% 11.87% 6.77% 11.89% 5.73% 9.32% 5.76% 8.81%
periodic arch. 17.12% 20.24% 21.55% 29.04% 21.48% 28.94% 21.09% 25.87% 28.41% 35.53%
Table 3: Synthetic data-sets: RRMSE and RMPE indices (benchmarks)
Algorithms mAMF mNMF mAMFo mNMFo
Metrics RRMSE RMPE RRMSE RMPE RRMSE RMPE RRMSE RMPE
low noise 0.61% 1.02% 1.34% 1.18% 0.60% 1.03% 1.50% 1.61%
medium noise 2.36% 5.35% 2.72% 5.84% 2.42% 5.81% 2.45% 5.67%
high noise 11.90% 28.40% 13.03% 30.83% 11.57% 28.50% 11.18% 27.53%
few distributions 4.72% 6.69% 8.67% 15.00% 5.58% 7.61% 6.29% 9.09%
periodic arch. 16.42% 18.74% 3.32% 3.82% 25.91% 32.80% 24.97% 31.01%
Table 4: Synthetic data-sets: RRMSE and RMPE indices (novel algorithms)
15Under review as submission to TMLR
Algorithms mAMF mNMF mAMFo mNMFo RFRLSTM GRUEXPSARIMAX
weekly electricity 1.48 1.23 1.86 0.1816.63286.45 340.53 0.71 98.22
daily electricity 3.670.64 5.32 1.05125.70 739.06 691.44 1.07 840.13
gas 0.32 0.18 3.77 0.1318.05330.49 466.98 0.89 90.18
Istanbul 0.03<0.01 0.15<0.01 0.39 6.62 6.350.01 4.25
demand orders 0.02 0.06 0.02 0.07 0.48 8.09 7.960.01 1.02
ETTh1 0.06 0.02 0.06 0.01 0.02 6.02 6.310.01 6.81
ETTh2 0.04 0.06 0.08 0.06 0.53 6.30 6.790.01 9.51
ETTm1 0.08 0.03 0.06 0.11 0.03 6.02 6.040.01 6.51
ETTm2 0.04 0.05 0.07 0.06 0.05 6.21 6.650.01 9.63
Table 5: Real-world data-sets: CPU times (seconds)
Algorithms mAMF mNMF mAMFo mNMFo RFR LSTM GRUEXPSARIMAX
low noise 2.161.15 2.30 1.2045.531013.18 2739.64 5.01 213.23
medium noise 1.91 0.96 1.93 0.9545.111031.13 2752.62 5.11 317.79
high noise 1.690.44 1.89 0.5345.661026.00 2763.16 5.09 265.02
periodic arch. 1.560.45 4.52 1.4959.071056.76 2610.67 6.02 1013.43
few distributions 0.56 0.60 6.67 1.6360.891097.85 2674.51 6.02 1026.83
Table 6: Synthetic data-sets: CPU times (seconds)
16Under review as submission to TMLR
A Variants of Nonnegative Matrix Factorization problems
Acronym Name Objective Constraints: W≥0+
NMFNonnegative Matrix FactorizationF1 H≥0Cichocki & Zdunek (2006)
SNMF Semi NMF Gillis & Kumarg (2015) F1
NNMF Normalized NMF F1 H≥0,W1=1
SNNMF Semi Normalized NMF F1 W1=1
AMFArchetypal Matrix FactorizationF2 W1=1,V≥0,V1=1Javadi & Montanari (2020a)
ANMF Archetypal NMF F2 H≥0,V≥0,V1=1
ANNMF Archetypal Normalized NMF F2 W1=1,H≥0,V≥0,V1=1
mNMF Mask NNMF F3 T(N) =X,W1=1,H≥0
mAMF Mask AMF F4 T(N) =X,W1=1,V≥0,V1=1
Table 7: The seven block convex programs achieving matrix factorization of nonnegative matrices. The
objectives are F1:=/bardblM−WH/bardbl2
FandF2:=/bardblM−WH/bardbl2
F+λ/bardblH−VM/bardbl2
F. The two last lines are SMM
procedureswithslidingoperator Πandobjectives F3:=/bardblN−WH/bardbl2
FandF4:=/bardblN−WH/bardbl2
F+λ/bardblH−VN/bardbl2
F.
B Proofs
B.1 Proof of Theorem 7
•We start by proving that Condition (A1)is suﬃcient for (POU ).
LetH0:= [H0TH0F],W/latticetop
0:= [W0/latticetop
trainW0/latticetop
test],H:= [HTHF], and W/latticetop:= [W/latticetop
trainW/latticetop
test]. Assumption
(A1)implies that decomposition W0trainH0andW0H0Tare unique. By Theorem 1, it holds
W0trainH0=WtrainH=⇒(W0train,H0)≡(Wtrain,H)
W0H0T=WHT=⇒(W0,H0T)≡(W,HT),
where≡stands for equality up to permutation and positive scaling of columns (resp. rows) of W0(resp. H0).
Hence, if ( A1) holds, then
(W0trainH0=WtrainH)∧(W0H0T=WHT) =⇒(W0,H0)≡(W,H). (3)
Moreover, note T(W0trainH0) =Ttrain(X0) =W0trainH0andT(W0H0T) =TT(X0) =W0H0T(same
equations holds for (W,H)). We deduce that T(W0H0) =T(WH )implies (W0trainH0=WtrainH)∧
(W0H0T=WHT). We deduce the result by (3).
•We prove that If(A1)and(A2)holds, T(WH ) =T(W0H0)andW01=W1 =1then (W,H) =
(W0,H0)up to permutation of columns (resp. rows) of W(resp. H), and there is no scaling.
By the previous point, we now that (A1)implies (W0,H0)≡(W,H). So that there exist λ1,...,λK
positive and a permutation σ(1),...,σ (K)such that
∀i∈[n−N],∀k∈[K],(W)(i)
k=λσ(k)(W0)(i)
σ(k).
Recall that W1=1(resp. W01=1) so that the rows of W(resp. W0) belongs to the aﬃne space
A1:=/braceleftBig
w∈RK:/angbracketleftw,1/angbracketright= 1/bracerightBig
.
17Under review as submission to TMLR
Namely, for a given row i∈[n−N], we have
(W0)(i)1=1⇒K/summationdisplay
k=1(W0)(i)
k= 1
W(i)1=1⇒K/summationdisplay
k=1λσ(k)(W0)(i)
σ(k)= 1
Which proves that (W0)(i)∈A1∩Aλσ−1, for alli∈[n−N], where
Aλσ−1:=/braceleftBig
w∈RK:K/summationdisplay
k=1λσ−1(k)wk= 1/bracerightBig
,
is the aﬃne space orthogonal to d:= (λσ−1(1),...,λσ−1(K)). We deduce that the rows (W0)(i)belong to the
aﬃne space
A:=/braceleftBig
w∈RK:/angbracketleftw,1/angbracketright= 1and/angbracketleftw,d/angbracketright= 1/bracerightBig
which is of:
•co-dimension 2ifdis not proportional to 1;
•co-dimension 1if there exists λ>0such that d=λ1. In this latter case, λ= 1and for allk∈[K],λk= 1,
namely there is no scaling of the columns.
IfAis of co-dimension 2thenAis of dimension K−2andConv( W0,train)⊆Acannot contain a ball of
dimensionK−1, which implies that Conv( Ttrain(X0))⊆A×His of dimension at most K−2and it cannot
contain a ball of dimension K−1(i.e., co-dimension 1), whereA×H={x:∃a∈As.t.x=a/latticetopH}. This
latter is a contradiction under ( A2). We deduce that Ais of co-dimension 2, and so there is no scaling.
B.2 Proof of Theorem 10
This proof follows the pioneering work Javadi & Montanari (2020a). In this latter paper, the authors consider
neither masks Tnor nonnegative constraints on Has in (mNMF ). Nevertheless,
1/ considering the hard constrained programs (4) and (6) below;
2/ remarking that it holds /tildewideD(H,X)≤D(H,X)andD(X,H)≤D(X,H);
then a careful reader can note that their proof extends to masks Tand nonnegative constraints on H. For
sake of completeness we reproduce here the steps that need to be changed in their proof. A reading guide of
the 60 pages proof of Javadi & Montanari (2020b) is given in Section C.
Step 1: reduction to hard constrained Programs (4)and(6)
Consider the constrained problem:
/hatwideH∈arg min
H/tildewideD(H,X)
s.t.D(X,H)≤∆2
1.(4)
where
D(X,H) := min
W≥0,W1=1/bardblT(X−WH )/bardbl2
F
Then (mAMF) can be seen as Lagrangian formulation of this problem setting ∆2
1=D(X,/hatwideH(mAMF) ),
where/hatwideH(mAMF)is a solution to (mAMF). We choose ∆1so as to bound the noise level /bardblF/bardblF
∆2
1≥/bardblF/bardbl2
F. (5)
Consider the constrained problem:
/hatwideH∈arg min
H≥0/tildewideD(H,X)
s.t.D(X,H)≤∆2
2.(6)
18Under review as submission to TMLR
Then (mNMF) can be seen as Lagrangian formulation of this problem setting ∆2
2=D(X,/hatwideH(mNMF) ),
where/hatwideH(mNMF)is a solution to (mNMF). We choose ∆1so as to bound the noise level /bardblF/bardblF
∆2
2≥/bardblF/bardbl2
F. (7)
Step 2: First bound on the loss
DenoteD:=/braceleftbig
D(H,H0)1/2+D(H0,H)1/2/bracerightbig
. By Assumption ( A2) we have
z0+UBK−1(µ)⊆conv(X0)⊆conv(H0),
where z0+UBK−1(µ)is a parametrization of the ball of center z0and radius µdescribed in Assumption
(A2) with Ua matrix whose columns are K−1orthonormal vectors. Using Lemma 15, we get that
µ√
2≤σmin(H0)≤σmax(H0),
whereσmin(H0),σmax(H0)denote its largest and smallest nonzero singular values. Then, since z0∈
conv(H0)we have z0=H/latticetop
0α0for someα0s.t.α01=1. It holds,
/bardblz0/bardbl2≤σmax(H0)/bardblα0/bardbl2≤σmax(H0). (8)
Note that
σmax(/hatwiderH−1z/latticetop
0)≤σmax(/hatwiderH) +σmax(1z/latticetop
0) =σmax(/hatwiderH) +√
K/bardblz0/bardbl2. (9)
Therefore, using Lemma 17 we have
D≤c/bracketleftBigg
K3/2∆1/2κ(P0(/hatwiderH)) +σmax(/hatwiderH)∆1/2K1/2
µ+K∆1/2/bardblz0/bardbl2
µ/bracketrightBigg
+c√
K/bardblF/bardblF, (10)
where ∆1/2equals ∆1for problem (4) and ∆2for problem (6), and κ(A)stands for the conditioning number
of matrix A. In addition, Lemma 18 implies that
L(H0,/hatwiderH)1/2≤1
αmax/braceleftBig
(1 +√
2)√
K,√
2κ(H0)/bracerightBig
D. (11)
Step 3: Combining and ﬁnal bound
By Lemma 19 it holds
D≤c/bracketleftBigK3/2D∆1/2
α(µ−2∆1/2)√
2+K2σmax(H0)∆1/2
(µ−2∆1/2)√
2+DK1/2∆1/2
αµ
+σmax(H0)∆1/2K
µ+K∆1/2/bardblz0/bardbl2
µ/bracketrightBig
+c√
K/bardblF/bardblF. (12)
We understand that D=O∆1/2→0(∆1/2)and for small enough ∆1/2there exists a constant c>0such that
D≤c∆1/2+c√
K/bardblF/bardblF
By (5) and (7), it yields that for small enough noise error /bardblF/bardblFone has
D≤c/bardblF/bardblF,
for some (other) constant c>0. Plugging this result in (11) we prove the result.
B.3 Proof of Theorem 11
N/mapsto→∇ Nh(H,W,N)is Lipschitz continuous with moduli L= 2. The statement follows from Proposition
4.1 in Javadi & Montanari (2020a) and from Theorem 1 in Bolte et al. (2014).
19Under review as submission to TMLR
C Propositions and Lemmas
•Results that we can use directly from Javadi & Montanari (2020b): Lemma B.1, Lemma B.2, Lemma B.3.
•Results of Javadi & Montanari (2020b) that has to be adapted: Lemme B.4 (done in Lemma 15), Lemma
B.5 (done in Lemma 16), and Lemma B.6 (done in Lemma 17).
Proposition 14 For/hatwiderHsolution to (4)(or(6)) one has/tildewideD(/hatwiderH,X)≤/tildewideD(H0,X).
Proof.Observe thatD(X,H0) =/bardblF/bardbl2
F. By (5) and (7), H0is feasible for (4) (or (6)) then /tildewideD(/hatwiderH,X)≤
/tildewideD(H0,X)
Lemma 15 (Adapted version of Lemma B.4 of Javadi & Montanari (2020b)) IfHis feasible for
problem (4)(or(6)) and has linearly independent rows, then we have
σmin(H)≥√
2(µ−2∆1/2), (13)
where ∆1/2equals ∆1for problem (4)and∆2for problem (6).
Proof. Consider the notation and the outline of proof Lemma B.4 in Javadi & Montanari (2020b). The
adaptation is simple here. The trick is to only consider rows in the training set, Ttrain(X0): the indice i
of proof of Lemma B.4 in Javadi & Montanari (2020b) correspond to the n−Nﬁrst rows in our case (the
training set); and one should replace X0byTtrain(X0). This proof requires only feasibility of Hand works
no matter if a nonnegative constraint on His active (as in Program (6)).
Lemma 16 (Adapted version of Lemma B.5 of Javadi & Montanari (2020b)) For/hatwiderHsolution to
(4)(or(6)), it holds
/tildewideD(/hatwiderH,X0)1/2≤/tildewideD(H0,X0)1/2+c√
K/bardblF/bardblF.
Proof.Consider the notation and the outline of proof Lemma B.5 in Javadi & Montanari (2020b). Note that
Eq. (B.103) holds by Proposition 14. Form Eq. (B.104), the proof remains unchanged once one substitutes
Dby/tildewideD.
Lemma 17 (Adapted version of Lemma B.6 of Javadi & Montanari (2020b)) For/hatwiderHthe optimal
solution of problem (4)(or(6)), we have
α(D(/hatwideH,H0)1/2+D(H0,/hatwideH)1/2)≤c/bracketleftbigg
K3/2∆1/2κ(P0(/hatwideH)) +∆1/2√
K
µσmax(/hatwideH−1zT
0)/bracketrightbigg
+c√
K/bardblF/bardblF (14)
where P0:Rd→Rdis the orthogonal projector onto aﬀ(H0)(in particular, P0is an aﬃne map), and ∆1/2
equals ∆1for problem (4)and∆2for problem (6).
Proof. Invoke the proof of Lemma B.6 in Javadi & Montanari (2020b) using the fact that /tildewideD(H,X)≤
D(H,X)andD(X,H)≤D(X,H).
Lemma 18 LetH,H0be matrices with linearly independent rows. We have
L(H0,H)1/2≤√
2κ(H0)D(H0,H)1/2+ (1 +√
2)√
KD(H,H0)1/2, (15)
whereκ(A)stands for the conditioning number of matrix A.
Proof.See Lemma B.2 in Javadi & Montanari (2020b)
Lemma 19 It holds
κ(P0(/hatwiderH))≤/bracketleftBigD
α(µ−2∆1/2)√
2+K1/2σmax(H0)
(µ−2∆1/2)√
2/bracketrightBig
.
Proof.The proof is given by Equations B.189-194 in Javadi & Montanari (2020b).
20Under review as submission to TMLR
D Algorithms for mNMF
In this section we report Block Coordinate Descend (BCD) Algorithm (see Algorithm 3) and accelerated
Hierarchical Alternate Least Square (HALS) for mNMF (see Algorithm 5), which is a generalization of
Algorithm described in Gillis & Glineur (2012) to the matrix factorization with mask.
Algorithm 3 BCD for mNMF
1:Initialization : choose H0≥0,W0≥0, and N0≥0, seti:= 0.
2:whilestopping criterion is not satisﬁed do
3: Hi+1:=update (Hi,Wi,Ni)
4: Wi+1:=update (Hi+1,Wi,Ni)
5: Ni+1:=update (Hi+1,Wi+1,Ni)
6:i:=i+ 1
7:end while
Algorithm 4 ALS for mNMF
1:Initialization : choose H0≥0,W0≥0, setN0:=PX(H0W0)andi:= 0.
2:whilestopping criterion is not satisﬁed do
3: Hi+1:= min H≥0/bardblNi−WiH/bardbl2
F
4: Wi+1:= min W≥0,W1=1/bardblNi−WHi+1/bardbl2
F
5:setNi+1:=PX(Wi+1Hi+1)
6:i:=i+ 1
7:end while
Algorithm 5 accelerated HALS for mNMF
1:Initialization : choose H0≥0,W0≥0,nonnegative rank K, andα > 0. Set N0=PX(W0H0),
ρW:= 1 +n(m+K)/(m(K+ 1)),ρH:= 1 +m(n+K)/(n(K+ 1)), andi:= 0.
2:whilestopping criterion is not satisﬁed do
3: A:=NHi/latticetop,B:=HiHi/latticetop
4:fork≤kW:=⌊1 +αρW⌋do
5:for/lscript∈[K]do
6: C/lscript:=/summationtext/lscript−1
j=1Wk+1
jBj/lscript+/summationtextK
j=/lscript+1Wk
jBj/lscript
7: Wk
/lscript:= max(0,(A/lscript−C/lscript)/B/lscript/lscript)
8:end for
9: WkW:=P∆(WkW)
10:end for
11: N:=PX(WkWHi)
12: A:=WkWN,B:=WkW/latticetopWkW
13:fork≤kH:=⌊1 +αρH⌋do
14: for/lscript∈[K]do
15: C/lscript:=/summationtext/lscript−1
j=1Hk+1
jBj/lscript+/summationtextn
j=/lscript+1Hk
jBj/lscript
16: Hk
/lscript:= max(0,(A/lscript−C/lscript)/B/lscript/lscript)
17: end for
18:end for
19: Wi+1:=WkW,Hi+1:=HkH
20: N:=PX(Wi+1Hi+1)
21:i:=i+ 1
22:end while
21Under review as submission to TMLR
E KKT conditions for mNMF
In this section we determine the KKT condition for mNMF problem, namely
min
W1=1,W≥0
H≥0
T(N)=X/bardblN−WH/bardbl2
F=:F(N,W,H). (mNMF)
Let us introduce the dual variables V≥0,G≥0,t∈Rn, and Z∈range( Π)such that T(Z) =Z. The
Lagrangian of mNMF problem is
L(N,W,H,V,G,t,Z) =F(N,W,H)−/angbracketleftW,V/angbracketright+/angbracketleftW1K−1N,t/angbracketright−/angbracketleftH,G/angbracketright−/angbracketleft N−X,Z/angbracketright.
The KKT condition are the following:
∇NL=N−WH−Z=0 ⇐⇒T(N−WH ) =Z∧T⊥(N−WH ) =0(16)
∇WL= (WH−N)H/latticetop−V−t1/latticetop
K=0⇐⇒V= (WH−N)H/latticetop−t1/latticetop
K (17)
∇HL=W/latticetop(WH−N)−G=0⇐⇒G =W/latticetop(WH−N) (18)
/angbracketleftW,V/angbracketright=0 ⇐⇒/angbracketleft W,∇WF−t1/latticetop
K/angbracketright=0 (19)
/angbracketleftH,G/angbracketright=0 ⇐⇒/angbracketleft H,∇HF/angbracketright=0 (20)
From the complementarity conditions (19), it follows:
Wi,j>0 =⇒Vi,j= 0 =⇒ti=−(∇WF)ij∀j
In order to compute ti, we can select a row W(i), ﬁnd any entry Wi,j>0and apply the previous formula.
In the practical implementation phase, in order to make numerically more stable the estimation of ti’s, we
can adopt a slightly diﬀerent strategy by averaging the values of ticomputed per row entry Wi,j>0.
22