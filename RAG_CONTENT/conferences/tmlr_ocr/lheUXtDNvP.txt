Published in Transactions on Machine Learning Research (12/2022)
GSR: A Generalized Symbolic Regression Approach
Tony Tohme tohme@mit.edu
Massachusetts Institute of Technology
Dehong Liu liudh@merl.com
Mitsubishi Electric Research Laboratories
Kamal Youcef-Toumi youcef@mit.edu
Massachusetts Institute of Technology
Reviewed on OpenReview: https: // openreview. net/ forum? id= lheUXtDNvP
Abstract
Identifying the mathematical relationships that best describe a dataset remains a very chal-
lenging problem in machine learning, and is known as Symbolic Regression (SR). In contrast
to neural networks which are often treated as black boxes, SR attempts to gain insight into
the underlying relationships between the independent variables and the target variable of a
given dataset by assembling analytical functions. In this paper, we present GSR, a General-
ized Symbolic Regression approach, by modifying the conventional SR optimization problem
formulation, while keeping the main SR objective intact. In GSR, we infer mathematical
relationships between the independent variables and some transformation of the target vari-
able. We constrain our search space to a weighted sum of basis functions, and propose
a genetic programming approach with a matrix-based encoding scheme. We show that
our GSR method is competitive with strong SR benchmark methods, achieving promising
experimental performance on the well-known SR benchmark problem sets. Finally, we high-
light the strengths of GSR by introducing SymSet, a new SR benchmark set which is more
challenging relative to the existing benchmarks.
1 Introduction
Symbolic regression (SR) aims to ﬁnd a mathematical expression that best describes the relationship between
the independent variables and the target (or dependent) variable based on a given dataset. By inspecting the
resulting expression, we may be able to identify nontrivial relations and/or physical laws which can provide
more insight into the system represented by the given dataset. SR has gained tremendous interest and
attention from researchers over the years for many reasons. First, many rules and laws in natural sciences
(e.g. in physical and dynamical systems (Schmidt & Lipson, 2009; Quade et al., 2016)) are accurately
represented by simple analytical equations (which can be explicit (Brunton et al., 2016) or implicit (Mangan
et al., 2016; Kaheman et al., 2020)). Second, in contrast to neural networks that involve complex input-
output mapping, and hence are often treated as black boxes which are diﬃcult to interpret, SR is very
concise and interpretable. Finally, symbolic equations may outperform neural networks in out-of-distribution
generalization (especially for physical problems) (Cranmer et al., 2020).
SR does not require a priorispeciﬁcation of a model. Conventional regression methods such as least squares
(Wild & Seber, 1989), likelihood-based (Edwards, 1984; Pawitan, 2001; Tohme et al., 2021), and Bayesian
regression techniques (Lee, 1997; Leonard & Hsu, 2001; Tohme, 2020; Tohme et al., 2020; Vanslette et al.,
2020) use ﬁxed-form parametric models and optimize for the model parameters only. SR seeks to ﬁnd both
a model structure and its associated parameters simultaneously.
1Published in Transactions on Machine Learning Research (12/2022)
Related Work. TheSRproblemhasbeenwidelystudiedintheliterature(Orzechowskietal.,2018;LaCava
et al., 2021). SR can be a very challenging problem and is thought to be NP-hard (Lu et al., 2016; Udrescu
& Tegmark, 2020; Petersen et al., 2021; Virgolin & Pissis, 2022). It can also be computationally expensive
as the search space is very wide (or complex) containing expressions of any size and length de França
(2018), this issue being exacerbated with the dimension of the input feature vector (i.e. the number of
independent variables). Several approaches have been suggested over the years. Most of the methods
use genetic (or evolutionary) algorithms (Koza & Koza, 1992; Schmidt & Lipson, 2009; Bäck et al., 2018;
Virgolin et al., 2019). Some more recent methods are Bayesian in nature (Jin et al., 2019), some are physics-
inspired (Udrescu & Tegmark, 2020), and others use divide-and-conquer (Luo et al., 2017) and block building
algorithms (Chen et al., 2017b; 2018a;b). Lately, researchers proposed using machine learning algorithms
and neural networks to solve the SR problem (Martius & Lampert, 2016; Sahoo et al., 2018; Udrescu et al.,
2020; Ahn et al., 2020; Al-Roomi & El-Hawary, 2020; Kim et al., 2020; Kommenda et al., 2020; Burlacu
et al., 2020; Biggio et al., 2021; Mundhenk et al., 2021; Petersen et al., 2021; Valipour et al., 2021; Razavi
& Gamazon, 2022; Zhang et al., 2022a; d’Ascoli et al., 2022; Kamienny et al., 2022; Zhang et al., 2022b).
Furthermore, some works suggested constraining the search space of functions to generalized linear space
(Nelder & Wedderburn, 1972) (e.g. Fast Function eXtraction (McConaghy, 2011), Elite Bases Regression
(Chen et al., 2017a), etc.) which proved to accelerate the convergence of genetic algorithms signiﬁcantly (at
the expense of sometimes losing the generality of the solution (Luo et al., 2017)).
Most of the SR methods use a tree-based implementation, where analytical functions are represented (or
encoded) by expression trees. Some approaches suggested encoding functions as an integer string (O’Neill
& Ryan, 2001), others proposed representing them using matrices (Luo & Zhang, 2012; Chen et al., 2017a;
de França, 2018; de Franca & Aldeia, 2020). As we will discuss in later sections, our implementation relies
on matrices to encode expressions.
Our Contribution. We present Generalized Symbolic Regression (GSR), by modifying the conventional
SR optimization problem formulation, while keeping the main SR objective intact. In GSR, we identify
mathematical relationships between the independent variables (or features) and some transformation of the
target variable. In other words, we learn the mapping from the feature space to a transformed target
space (where the transformation applied to the target variable is also learned during this process). To
ﬁnd the appropriate functions (or transformations) to be applied to the features as well as to the targets,
we constrain our search space to a weighted sum of basis functions. In contrast to conventional tree-
based genetic programming approaches, we propose a matrix-based encoding scheme to represent the basis
functions (and hence the full mathematical expressions). We run a series of numerical experiments on the
well-known SR benchmark datasets and show that our proposed method is competitive with many strong
SR methods. Finally, we introduce SymSet, a new SR benchmark problem set that is more challenging
than existing benchmarks.
2 Notation and Problem Formulation
Consider the following regression task. We are given a dataset D={xi,yi}N
i=1consisting of Ni.i.d. paired
examples, where xi∈Rddenotes the ithd-dimensional input feature vector and yi∈Rrepresents the
corresponding continuous target variable. The goal of SR is to search the space of all possible mathematical
expressionsSdeﬁned by a set of given mathematical functions (e.g., exp,ln,sin,cos) and arithmetic
operations (e.g., +,−,×,÷), along with the following optimization problem:
f∗=argmin
f∈SN/summationdisplay
i=1/bracketleftbig
f(xi)−yi/bracketrightbig2(1)
wherefis the model function and f∗is the optimal model.
2Published in Transactions on Machine Learning Research (12/2022)
3 Generalized Symbolic Regression (GSR)
In this section, we introduce our Generalized Symbolic Regression (GSR) approach. We present its problem
formulation, and discuss its solution and implementation.
3.1 Modifying the goal of symbolic regression
As highlighted in Section 2, the goal of SR is to search the function space to ﬁnd the model that best ﬁts the
mapping between the independent variables and the target variable (i.e. the mapping between xiandyi, for
alli). Since the main objective of SR is to recognize correlations and ﬁnd non-trivial interpretable models
(rather than making direct predictions), we modify the goal of SR; we instead search the function space to
ﬁnd the model that best describes the mapping between the independent variables and a transformation
of the target variable (i.e. the mapping between xiand some transformation or function of yi, for alli).
Formally, we propose modifying the goal of SR to search for appropriate (model) functions from a space of all
possible mathematical expressions Sdeﬁned by a set of given mathematical functions (e.g., exp,ln,sin,cos)
and arithmetic operations (e.g., +,−,×,÷), which can be described by the following optimization problem:
f∗,g∗=argmin
f,g∈SN/summationdisplay
i=1/bracketleftbig
f(xi)−g(yi)/bracketrightbig2(2)
wheref∗andg∗are the optimal analytical functions. In other words, instead of searching for mathematical
expressionsoftheform y=f(x)asisusuallydoneintheSRliterature, theproposedGSRapproachattempts
to ﬁnd expressions of the form g(y) =f(x). We illustrate this concept in Table 1.
Table 1: GSR ﬁnds analytical expressions of the form g(y) =f(x)instead ofy=f(x).
Ground Truth Expression Learned Expression
y=√x+ 5 y2=x+ 5
y= 1/(3x1+x3
2) y−1= 3x1+x3
2
y= (2x1+x2)−2
3 ln(y) =−2
3ln(2x1+x2)
y= ln(x3
1+ 4x1x2) ey=x3
1+ 4x1x2
y=ex3
1+2x2+cos(x3)ln(y) =x3
1+ 2x2+ cos(x3)
Although the main goal of GSR is to ﬁnd expressions of the form g(y) =f(x), we may encounter situations
where it is best to simply learn expressions of the form y=f(x)(i.e.g(y) =y). For instance, consider the
ground truth expression y= sin(x1) + 2x2. In this case, we expect to learn the expression exactly as is (i.e.
g(y) =yandf(x) = sin(x1) + 2x2) as long as the right basis functions (i.e. sin(x)andxin this case) are
within the search space, as we will see in the next sections.
Making predictions. Given a new input feature vector x∗, predicting y∗with GSR is simply a matter
of solving the equation g(y) =f(x∗)fory, or equivalently, g(y)−f(x∗) = 0. Note that f(x∗)is a known
quantity and yis the only unknown. If g(·)is an invertible function, then y∗can be easily found using
y∗=g−1/parenleftbig
f(x∗)/parenrightbig
. Ifg(·)is not invertible, then y∗will be the root of the function h(y) =g(y)−f(x∗).
Root-ﬁnding algorithms include Newton’s method. Whether the function g(·)is invertible or not, we might
end up with many solutions for y∗(an invertible function, which is not one-to-one, can lead to more than
one solution). In this case, we choose y∗to be the solution that belongs to the range of ywhich can be
determined from the training dataset.
3Published in Transactions on Machine Learning Research (12/2022)
3.2 A new problem formulation for symbolic regression
Now that we have presented the goal of our proposed GSR approach (summarized by Equation 2), we
need to constrain the search space of functions Sto reduce the computational challenges and accelerate the
convergence of our algorithm. Inspired by McConaghy (2011); Chen et al. (2017a) as well as classical system
identiﬁcation methods (Brunton et al., 2016), we conﬁne Sto generalized linear models, i.e. to functions
that can be expressed as a linear combination (or as a weighted sum) of basis functions (which can be linear
or nonlinear). In mathematical terms, for a given input feature vector xiand a corresponding target variable
yi, the search space Sis constrained to model functions of the form:
f(xi) =Mφ/summationdisplay
j=1αjφj(xi), g (yi) =Mψ/summationdisplay
j=1βjψj(yi) (3)
whereφj(·)andψj(·)are the basis functions applied to the feature vector xiand the target variable
yi, respectively, MφandMψdenote the corresponding number of basis functions involved, respectively.
In matrix form, the minimization problem described in Equation 2 is equivalent to ﬁnding the vectors of
coeﬃcientsα= [α1···αMφ]Tandβ= [β1···βMψ]Tsuch that:
α∗,β∗=argmin
α,β||Xα−Yβ||2(4)
where
X=
φ1(x1)φ2(x1)···φMφ(x1)
......···...
φ1(xN)φ2(xN)···φMφ(xN)
,Y=
ψ1(y1)ψ2(y1)···ψMψ(y1)
......···...
ψ1(yN)ψ2(yN)···ψMψ(yN)
.(5)
Note that if we examine the minimization problem as expressed in Equation 4, we can indeed minimize
||Xα−Yβ||2by simply setting α∗= 0andβ∗= 0which will not lead to a meaningful solution to our
GSR problem. In addition, to avoid reaching overly complex mathematical expressions for f(·)andg(·), we
are interested in ﬁnding sparse solutions for the weight vectors α∗andβ∗consisting mainly of zeros which
results in simple analytical functions containing only the surviving basis functions (i.e. whose corresponding
weights are nonzero). This is closely related to sparse identiﬁcation of nonlinear dynamics (SINDy) methods
(Brunton et al., 2016). To this end, we apply L1regularization, also known as Lasso regression (Tibshirani,
1996), by adding a penalty on the L1norm of the weights vector (i.e. the sum of its absolute values)
which leads to sparse solutions with few nonzero coeﬃcients. In terms of our GSR method, Lasso regression
automatically performs basis functions selection from the set of basis functions that are under consideration.
Putting the pieces together, we reformulate the minimization problem in Equation 4 as a constrained Lasso
regression optimization problem deﬁned as
w∗=argmin
w||Aw||2
2+λ||w||1
s.t.||w||2= 1(6)
whereλ>0is the regularization parameter, and
A=/bracketleftbig
X−Y/bracketrightbig
,w=/bracketleftBigg
α
β/bracketrightBigg
= [α1···αMφβ1···βMψ]T. (7)
3.3 Solving the GSR problem
To solve the GSR problem, we ﬁrst present our approach for solving the constrained Lasso problem in
Equation 6, assuming some particular sets of basis functions are given. We then outline our genetic program-
ming (GP) procedure for ﬁnding the appropriate (or optimal) sets of these basis functions, before discussing
our matrix-based encoding scheme (to represent the basis functions) that we will use in our GP algorithm.
4Published in Transactions on Machine Learning Research (12/2022)
Algorithm 1: Solving the constrained Lasso optimization problem using ADMM
Input:A,λ,ρ,w0,z0,u0
Output:w
function SolveADMM (A,λ,ρ,w0,z0,u0)
Initialization: w←w0,z←z0,u←u0;
whileNot Converge do
w←/parenleftbig
2ATA+ρI/parenrightbig−1·ρ(z−u);
w←w/||w||2;
z←Sλ/ρ/parenleftbig
w+u/parenrightbig
;
u←u+w−z;
end
end function
3.3.1 Solving the Lasso optimization problem given particular sets of basis functions
We assume for now that, in addition to the dataset D={xi,yi}N
i=1, we are also given the sets of basis
functions{φj(xi)}Mφ
j=1and{ψj(yi)}Mψ
j=1used with the input feature vector xiand its corresponding target
variableyi, respectively, for all i. In other words, we assume for now that the matrix Ain Equation 7 is
formed based on particular sets of basis functions/parenleftbig
i.e.{φj(xi)}Mφ
j=1and{ψj(yi)}Mψ
j=1/parenrightbig
, and we are mainly
interestedinsolvingtheconstrainedoptimizationprobleminEquation6. Applyingthealternatingdirection
methodofmultipliers(ADMM)(Boydetal.,2011), theoptimizationprobleminEquation6canbe written as
w∗=argmin
w||Aw||2
2+λ||z||1
s.t.||w||2= 1
w−z= 0(8)
whereλ>0is the regularization parameter. The scaled form of ADMM (see Boyd et al. (2011) for details)
for this problem is
wk←argmin
||w||2=1Lρ(w,zk−1,uk−1)
zk←Sλ/ρ/parenleftbig
wk+uk−1/parenrightbig
uk←uk−1+wk−zk(9)
whereuis the scaled dual vector, and
Lρ(w,z,u) =||Aw||2
2+ρ
2/vextendsingle/vextendsingle/vextendsingle/vextendsinglew−z+u/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
2(10)
whereρ>0is the penalty parameter and the soft thresholding operator Sis deﬁned as
Sκ(a) =

a−κ a>κ
0|a|≤κ
a+κ a<−κ(11)
To ﬁnd the minimizer wkin the ﬁrst step of the ADMM algorithm above (in Equation 9), we ﬁrst com-
pute the gradient of the function Lρ(w,zk−1,uk−1)with respect to w, set it to zero, and then normalize
the resulting vector solution:
0 =∇wLρ(w,zk−1,uk−1)/vextendsingle/vextendsingle
w=wk
= 2ATAwk+ρ(wk−zk−1+uk−1)(12)
5Published in Transactions on Machine Learning Research (12/2022)
It follows that
wk=/parenleftbig
2ATA+ρI/parenrightbig−1·ρ(zk−1−uk−1)
wk=wk/||wk||2(13)
Algorithm1outlinestheoverallprocessforsolvingtheconstrainedLassooptimizationprobleminEquation 6,
for a given matrix A, regularization parameter λ, penalty parameter ρ, and initial guesses w0,z0,u0.
3.3.2 Finding the appropriate sets of basis functions using genetic programming
Now that we have presented Algorithm 1 that solves the constrained Lasso optimization problem in
Equation 6 for particular sets of basis functions, we go through our procedure for ﬁnding the optimal sets
of basis functions/parenleftbig
and hence, the optimal analytical functions f(·)andg(·)/parenrightbig
.
Encoding Scheme
Most of the SR methods rely on expression trees in their implementation. That is, each mathematical
expression is represented by a tree where nodes (including the root) encode arithmetic operations (e.g. +,
−,×,÷) or mathematical functions (e.g. cos,sin,exp,ln), and leaves contain the independent variables
(i.e.x1,...,xd) or constants. Inspired by Luo & Zhang (2012); Chen et al. (2017a), we use matrices instead
of trees to represent the basis functions. However, we propose our own encoding scheme that we believe is
general enough to handle/recover a wide range of expressions.
We introduce the basis matrices BφandBψto represent the basis functions φ(·)andψ(·)used with the
featurevector xandthetargetvariable y, respectively. Thebasismatrices BφandBψareofsizesnBφ×mBφ
andnBψ×1respectively (i.e. Bψis a column vector), and take the form
Bφ=
bφ
1,1... bφ
1,mBφ
...···...
bφ
nBφ,1... bφ
nBφ,mBφ
,Bψ=
bψ
1,1
...
bψ
nBψ,1
, (14)
where the entries bφ
i,jandbψ
i,1are all integers. The ﬁrst column bφ
•,1ofBφand the ﬁrst (and only) column
bψ
•,1ofBψindicate the mathematical function (or transformation) to be applied (on the input feature vector
xand the target variable y, respectively). In Bφ, the second column bφ
•,2speciﬁes the type of argument (see
Table 2), and the remaining nv=mBφ−2columnsbφ
•,3,···,bφ
•,mBφindicate which independent variables (or
features) are involved (i.e. the active operands). The quantity nvrepresents the maximum total multiplicity
of all the independent variables included in the argument. Note that nBφandnBψspecify the number of
transformations to be multiplied together (i.e. each basis function φ(·)andψ(·)will be a product of nBφ
andnBψtransformations, respectively).
The encoding/decoding process happens according to a table of mapping rules that is very straightforward to
understand and employ. For instance, consider the mapping rules outlined in Table 2, where dis the dimen-
sion of the input feature vector. As we will see in our numerical experiments in Section 4, we will adopt this
table for many SR benchmark problems. Other mapping tables are deﬁned according to diﬀerent benchmark
problems1(more details about the SR benchmark problem speciﬁcations can be found in Appendix C). The
encoding from the analytical form of a basis function to the basis matrix is straightforward. For example,
ford= 3,nBφ= 4andmBφ= 5(i.e.nv= 3), the basis function φ(x) =x2cos(x2
1x2) ln(x1+x3)can be
generated according to the encoding steps shown in Table 3.
1Each SR benchmark problem uses a speciﬁc set (or library) of allowable mathematical functions (e.g. cos,sin,exp,log),
and hence, we mainly modify the ﬁrst two rows of the mapping tables.
6Published in Transactions on Machine Learning Research (12/2022)
Table 2: Example table of mapping rules for a basis
function. The identity operator is denoted by •1.
b•,1 0 1 2 3 4 5
Transformation ( T)1•1cos sin exp ln
b•,2 0 1 2
Argument Type ( arg)x/summationtext /producttext
b•,3,···,b•,mB 0 1 2 3 ···d
Variable (v) skipx1x2x3···xdTable 3: Encoding steps corresponding to the
basis function φ(x) =x2cos(x2
1x2) ln(x1+x3).
StepT arg v 1v2v3Update
1•1x x 2— —T1(x) =x2
2 cos/producttext
x1x1x2T2(x) = cos(x2
1x2)
3 ln/summationtext
x1x3skipT3(x) = ln(x1+x3)
4 1— — — — T4(x) = 1
Final Update: φ(x) =T1(x)·T2(x)·T3(x)·T4(x)
Based on the mapping rules in Table 2 and the encoding steps in Table 3, the basis function
φ(x) =x2cos(x2
1x2) ln(x1+x3)can be described by a 4×5matrix as follows:
Bφ=
1 0 2• •
2 2 1 1 2
5 1 1 3 0
0• • • •
(15)
Remark 3.1.In Table 3, — denotes entries that are ignored during the construction of the basis function.
The argument type in Step 1 is xwhich implies that we only select the ﬁrst variable (encoded by b•,3) out of
thenvvariables as an argument, and hence the entries corresponding to v2andv3are ignored. Similarly, the
transformation in Step 4 is T= 1which implies that the argument type and the nvvariables are all ignored.
These are the only two cases where some entries are ignored during the construction process. The same
ignored entries are reﬂected in the matrix Bφusing•. More encoding examples can be found in Appendix B.
Remark 3.2.The term ‘skip’ can be thought of as 0or1when the argument type is summation/summationtextor
multiplication/producttextrespectively. To account for the case where the argument type is x, we letb•,3∈{1,...,d}
(i.e. we exclude 0) as b•,3is the only entry considered in this case (see Remark 3.1).
Remark 3.3.The same basis function can be represented by several matrices for three reasons:
i) Each basis function is a product of transformations where each transformation is represented by a row in
the basis matrix. Hence, a new basis matrix for the same basis function is formed by simply swapping rows.
ii) When the argument type is/summationtextor/producttext, the order of the nvvariables (including ‘skip’) starting from the
third column of the matrix Bφdoes not aﬀect the expression. Hence a new basis matrix for the same basis
function is formed by simply swapping these columns.
iii) As mentioned in Remark 3.1, some entries are ignored in some cases. Hence a new basis matrix for the
same basis function is formed by simply modifying these entries.
Remark3.4.In the example above, we showed how we can produce the matrix Bφto represent a basis func-
tionφ(x). A similar (and even simpler) procedure can be applied to produce the matrix Bψthat represents
a basis function ψ(y); we only need a mapping table corresponding to the set of allowable transformations
(e.g. the ﬁrst two rows of Table 2).
Note that the decoding from the basis matrix to the expression of a basis function is trivial; we go
through the rows of the basis matrix and convert them into transformations according to a mapping ta-
ble (e.g. Table 2), before ﬁnally multiplying them together. Also note that the search space of basis
functions/parenleftbig
mainlyφ(·)/parenrightbig
is huge in general which makes enumeration impractical, and hence, we will rely
on GP for eﬀective search process.
7Published in Transactions on Machine Learning Research (12/2022)
Genetic Programming (Evolutionary Algorithm)
The SR problem has been extensively studied in the literature, and a wide variety of methods has been
suggested over the years to tackle it. Most of these methods are based on genetic programming (GP) (Koza
& Koza, 1992; Schmidt & Lipson, 2009; Bäck et al., 2018; Virgolin et al., 2019). This is a heuristic search
technique that tries to ﬁnd the optimal mathematical expression (in the SR context) among all possible
expressions within the search space. The optimal (or best) expression is found by minimizing some objective
function, known as the ﬁtness function.
GP is an evolutionary algorithm that solves the SR problem. It starts with an initial population (or ﬁrst
generation) of Nprandomly generated individuals (i.e. mathematical expressions), then recursively applies
theselection,reproduction (crossover) , andmutation operations until termination . During the selection
operation, the ﬁtness of each of the Npindividuals of the current generation is evaluated (according to
the ﬁtness function), and the npﬁttest (or best) individuals are selected for reproduction and mutation
(the selected individuals are part of the new generation and can be thought of as parents). The reproduc-
tion (crossover) operation generates new individuals (oﬀsprings) by combining random parts of two parent
individuals. The mutation operation produces a new individual by changing a random part of some par-
ent individual. Finally, the recursion terminates, when some individual reaches a predeﬁned ﬁtness level
(i.e. until some stopping criterion is satisﬁed).
InourGSRapproach,weuseaslightlymodiﬁedversionoftheGPalgorithmdescribedabove. Eachindividual
in the population initially consists of two sets of MφandMψrandomly generated basis functions encoded
by basis matrices. Such matrices will form the functions f(·)andg(·)to be used with the input feature
vector xand the target variable y, respectively. This is diﬀerent from the GP algorithm described above
where individuals typically represent the full mathematical expression or function as a whole. In addition,
theNpindividuals in the population of a new generation consist of the npﬁttest individuals of the current
generation in addition to Np−npindividuals generated as follows. With probability1
4, a new individual
is generated (reproduced) by randomly combining basis functions (i.e. basis matrices) from two parent
individuals (i.e. crossover) selected from the npsurviving individuals. With probability1
4, a new individual
is generated by randomly choosing one of the npsurviving individuals, and replacing (mutating) some of its
basis functions (i.e. basis matrices) with completely new ones (i.e. randomly generated). With probability
1
2, a completely new individual is randomly generated (in the same way we generate the individuals of the
initial population). Randomly generating individuals enhances diversity in the basis functions and avoids
reaching a plateau. Indeed, this is just one of many ways that can be followed to apply some sort of
crossover/mutation on individuals deﬁned by their sets of basis functions instead of their full mathematical
expression. A pseudocode of our proposed GSR algorithm is provided in Appendix A.
4 Experimental Results
We evaluate our proposed GSR method through a series of numerical experiments on a number of common
SR benchmark datasets. In particular, we compare our approach to existing state-of-the-art methods using
three popular SR benchmark problem sets: Nguyen (Uy et al., 2011), Jin (Jin et al., 2019), and Neat (Trujillo
et al., 2016). In addition, we demonstrate the beneﬁts of our proposed method on the recently introduced
SR benchmark dataset called Livermore (Mundhenk et al., 2021), which covers problems with a wider range
of diﬃculty compared to the other benchmarks. Finally, we introduce a new and more challenging set of
SR benchmark problems, which we call SymSet, mainly for two reasons: i) Our GSR algorithm achieves
perfect scores on Nguyen, and almost perfect scores on Jin, Neat, and Livermore, and hence we introduced a
benchmark problem set that is more challenging, ii) The existing SR benchmark problem sets do not really
reﬂectthestrengthsofourproposedmethod, andthuswedesignedSymSettoexplicitlyhighlightthebeneﬁts
we gain from using our proposed approach. SymSet contains benchmark problems with similar properties
as Nguyen, Jin, Neat, and Livermore benchmarks, but with an additional function composition (or symbolic
layer). Each SR benchmark problem consists of a ground truth expression, a training and test dataset, and
a set (or libary) of allowable arithmetic operations and mathematical functions. Speciﬁcations of all the SR
benchmark problems are described in Appendix C. Hyperparameters and additional experiment details are
provided in Appendix A.
8Published in Transactions on Machine Learning Research (12/2022)
Table 4: Recovery rate comparison of GSR against several algorithms on the Nguyen benchmark set over
100independent runs. The formulas for these benchmarks are shown in Appendix Table 19.
Recovery Rate ( %)
Benchmark Expression GSR NGGPPS DSR Eureqa
Nguyen-1 y=x3+x2+x 100 100 100 100
Nguyen-2 y=x4+x3+x2+x 100 100 100 100
Nguyen-3 y=x5+x4+x3+x2+x 100 100 100 95
Nguyen-4 y=x6+x5+x4+x3+x2+x 100 100 100 70
Nguyen-5 y= sin(x2) cos(x)−1 100 100 72 73
Nguyen-6 y= sin(x) + sin(x+x2) 100 100 100 100
Nguyen-7 y= ln(x+ 1) + ln(x2+ 1) 100 97 35 85
Nguyen-8 y=√x 100 100 96 0
Nguyen-9 y= sin(x1) + sin(x2
2) 100 100 100 100
Nguyen-10 y= 2 sin(x1) cos(x2) 100 100 100 64
Nguyen-11 y=xx2
1 100 100 100 100
Average 100 99.73 91.18 80.64
Across our experiments, we compare our GSR approach against several strong SR benchmark methods:
Neural-guided genetic programming population seeding (NGGPPS) : A hybrid approach of neural-
guided search and GP, which uses a recurrent neural network (RNN) to seed the starting population for GP
(Mundhenk et al., 2021). NGGPPS achieves strong results on the well-known SR benchmarks.
Deep Symbolic Regression (DSR) : A reinforcement learning method that proposes a risk-seeking policy
gradient to train an RNN to produce better-ﬁtting expressions (Petersen et al., 2021). DSR is the “RNN
only” version of NGGPPS, and is also considered a strong performer on the common SR benchmarks.
Bayesian Symbolic Regression (BSR) : A Bayesian framework which carefully designs prior dis-
tributions to incorporate domain knowledge (e.g. preference of basis functions or tree structure),
and which employs eﬃcient Markov Chain Monte Carlo (MCMC) methods to sample symbolic
trees from the posterior distributions (Jin et al., 2019).
Neat-GP : a GP approach which uses the NeuroEvolution of Augmenting Topologies (NEAT) algorithm
that greatly reduces the eﬀects of bloat (i.e. controls the growth in program size) (Trujillo et al., 2016).
PSTree: A piece-wise non-linear SR method based on decision tree and GP techniques (Zhang et al., 2022a).
PSTree can generate explainable models with high accuracy in a short period of time. PSTree is the cur-
rent top performer on SRBench datasets (La Cava et al., 2021), achieving state-of-the-art performance and
beating other competitive SR methods such as Operon (Kommenda et al., 2020; Burlacu et al., 2020) and
AI Feynman (Udrescu et al., 2020).
PySR: A fast and parallelized SR method in Python/Julia (Cranmer, 2020), which uses evolutionary algo-
rithms to search for symbolic expressions by optimizing a particular objective; the metric used for scoring
equations is based on the work by Cranmer et al. (2020).
gplearn: A Koza-style SR method in Python, which starts with a random population of models, and then
iteratively performs tournament selection, crossover, and mutation (Koza & Koza, 1992).
We ﬁrst compare GSR against NGGPPS, DSR, as well as Eureqa (a popular GP-based commercial software
proposed in Schmidt & Lipson (2009)) on the Nguyen benchmarks. We follow their experimental procedure
and report the results in Table 4. We use recovery rate as our performance metric, deﬁned as the fraction of
independent training runs in which an algorithm’s resulting expression achieves exact symbolic equivalence
comparedtothegroundtruthexpression(asveriﬁedusingacomputeralgebrasystemsuchasSymPy(Meurer
et al., 2017)). Table 4 shows that GSR signiﬁcantly outperforms DSR and Eureqa in exactly recovering the
Nguyen benchmark expressions. As NGGPPS achieves nearly perfect scores on the Nguyen benchmarks,
GSR shows only a slight improvement (on Nguyen-7) compared to NGGPPS. However, GSR exhibits faster
runtime than NGGPPS; by running each benchmark problem, GSR takes an average of 2.5minutes per
run on the Nguyen benchmarks compared to 3.2minutes for NGGPPS. Runtimes on individual Nguyen
benchmark problems are shown in Appendix Table 11.
9Published in Transactions on Machine Learning Research (12/2022)
Table 5: Comparison of mean root-mean-
square error (RMSE) for GSR against
several methods on the Jin benchmark
problem set over 50independent runs. The
formulas for these benchmarks are shown in
Appendix Table 19.
Mean RMSE
Benchmark GSR NGGPPS DSR BSR
Jin-1 0 0 0 .46 2 .04
Jin-2 0 0 0 6 .84
Jin-3 0 0 0 .00052 0 .21
Jin-4 0 0 0 .00014 0 .16
Jin-5 0 0 0 0 .66
Jin-6 0.018 0 2 .23 4 .63
Average 0.0030 0 0.45 2 .42Table 6: Comparison of median RMSE for GSR against
several methods on the Neat benchmark problem set over
30independent runs. The formulas for these benchmarks
are shown in Appendix Table 19.
Median RMSE
Benchmark GSR NGGPPS DSR Neat-GP
Neat-1 0 0 0 0 .0779
Neat-2 0 0 0 0 .0576
Neat-3 0 0 0 .0041 0 .0065
Neat-4 0 0 0 .0189 0 .0253
Neat-5 0 0 0 0 .0023
Neat-6 2.0×10−46.1×10−60.2378 0 .2855
Neat-7 0.0521 1 .0028 1 .0606 1 .0541
Neat-8 4.0×10−40.0228 0 .1076 0 .1498
Neat-9 8.1×10−90 0 .1511 0 .1202
Average 0.0059 0.1139 0 .1756 0 .1977
We next evaluate GSR on the Jin and Neat benchmark sets. The results are reported in Tables 5 and 6
respectively. A RMSE value of 0indicates exact symbolic equivalence. From Table 5, we can clearly
observe that GSR outperforms DSR and BSR and performs nearly as good as NGGPPS recovering all
the Jin problems (accross all independent runs) except Jin- 6. Table 6 shows that GSR outperforms all
other methods (NGGPPS, DSR, and Neat-GP) on the Neat benchmarks. Note that expressions containing
divisions (i.e. Neat-6, Neat-8, and Neat-9) are not exactly recovered by GSR (i.e. only approximations are
recovered) since the division operator is not included in our scheme (see Appendix E for details).
We then run experiments on the Livermore benchmark set which contains problems with a large range of
diﬃculty. In addition to NGGPPS and DSR, we compare against NGGPPS using the soft length prior (SLP)
and hierarchical entropy regularizer (HER) recently introduced in Larma et al. (2021). We also compare
against a recently proposed method, known by genetic expert-guided learning (GEGL) (Ahn et al., 2020),
which trains a molecule-generating deep neural network (DNN) guided with genetic exploration. Table 7
shows that our GSR method outperforms all other methods on both the Nguyen and Livermore benchmark
sets, beating NGGPPS+SLP/HER which was the top performer on these two benchmark sets.
We highlight the strengths of GSR on the new SymSet benchmark problem set, and show the beneﬁts of
searching for expressions of the form g(y) =f(x)instead ofy=f(x). Typical expressions, with exact
symbolic equivalence, recovered by GSR are shown in Appendix Table 26. The key feature of GSR lies in its
ability to recover expressions of the form g(y) =f(x). To better highlight the beneﬁts oﬀered by this feature,
we disable it by constraining the search space in GSR to expressions of the form y=f(x)(which is the most
criticalablation). WerefertothisspecialversionofGSRass-GSR.NotethatmostoftheSymSetexpressions
cannot be exactly recovered by s-GSR (i.e. they can only be approximated). We compare the performance of
GSR against s-GSR on the SymSet benchmarks in terms of accuracy and runtime (see Table 8). The results
clearly show that GSR is faster than s-GSR, averaging around 2minutes per run on the SymSet benchmarks
compared to 2.27minutes for s-GSR (i.e. ∼11%runtime improvement). In addition, GSR is more accurate
than s-GSR by two orders of magnitude. This is due to the fact that GSR exactly recovers the SymSet
expressions across most of the runs, while s-GSR only recovers approximations for most of these expressions.
This reﬂects the superiority of GSR over s-GSR, which demonstrates the beneﬁts of learning expressions of
the formg(y) =f(x)in SR tasks. We further compare GSR against several strong SR methods with similar
(or better) expression ability. In particular, we experiment on SymSet with NGGPPS, PSTree, PySR, and
gplearn (see Table 8). GSR is more accurate than all these methods by three orders of magnitude, which
further demonstrates the advantage of our proposed approach. As for the runtime, PSTree is the fastest
method, averaging around 16 seconds per run on the SymSet expressions, while maintaining solid accuracies.
This comes as no surprise given its state-of-the-art performance on SRBench datasets (La Cava et al., 2021).
10Published in Transactions on Machine Learning Research (12/2022)
Table 7: Recovery rate comparison of GSR
against several algorithms on the Nguyen and
Livermore benchmark sets over 25independent
runs. Recovery rates on individual benchmark
problems are shown in Appendix Table 10.
Recovery Rate/parenleftbig
%/parenrightbig
All Nguyen Livermore
GSR 90.59 100.00 85.45
NGGPPS+SLP/HER 82.59 92.00 77.45
NGGPPS 78.59 92.33 71.09
GEGL 66.82 86.00 56.36
DSR 49.18 83.58 30.41Table 8: Average performance in mean RMSE and
runtime, along with their standard errors, for GSR
against s-GSR and several strong SR methods on the
SymSet benchmark problem sets over 25independent
runs. Mean RMSE and runtime values on individual
benchmarkproblemsareshownin Appendix Table 12.
SymSet Average
Mean RMSE Runtime (sec)
GSR 2.66×10−4±1.59×10−4120.84±4.22
s-GSR 2.56×10−2±5.27×10−3136.19±4.56
PSTree 4.57×10−1±7.98×10−216.23±0.67
NGGPPS 4.65×10−1±1.24×10−1158.57±2.59
PySR 4.99×10−1±1.76×10−187.07±21.6
gplearn 7.22×10−1±1.64×10−1163.86±2.94
5 Discussion
Limitations. GSR, including state-of-the-art methods, have diﬃculty with expressions containing divisions.
For GSR, this is due to the way we deﬁne our encoding scheme. Other methods fail even though the division
is included in their framework. GSR can overcome this issue by modifying its encoding scheme to include
divisions within the basis functions (at the expense of signiﬁcantly increasing the complexity of the search
space). Another limiting factor to GSR is that it cannot recover expressions containing composition of
functions, such as y=ecos(x)+ ln(x). This could be overcome by modifying the search space (e.g. one
could expand the deﬁnition of a basis function to account for composition of functions up to some number of
layers, or completely modify the search space to a symbolic neural network as in Martius & Lampert (2016);
Sahoo et al. (2018); Kim et al. (2020)). Another challenging task for GSR is to reach, although expressible,
expressions containing multiple complex basis functions simultaneously. This can be due to the choice of the
hyperparameters or the GP search process. A more elaborate discussion about the limitations of GSR can
be found in Appendix E. These limitations will be addressed in a future paper. Indeed, there are plenty of
expressions that still cannot be fully recovered by GSR. This is the case for all other SR methods as well.
Closely related work. There has been growing attention on the SR task with non explicit (or implicit)
mathematical equations and several works have been attempted to address this interesting task. In partic-
ular, implicit sparse identiﬁcation of nonlinear dynamics (implicit-SINDy) (Mangan et al., 2016; Kaheman
et al., 2020) introduces the concept of identifying implicit expressions of the form f(x,y) = 0in the context
of diﬀerential equations/parenleftbig
i.e.y= ˙xi=dxi
dtfori∈{1,...,d}where x∈Rd/parenrightbig
. Further, Eureqa (Schmidt &
Lipson, 2009), a well-established baseline SR algorithm, focuses on discovering invariants rather then trying
to perform prediction directly. Inspired by the two aforementioned methods, and by the fact that the main
objective of SR is to recognize correlations and deﬁne non-trivial interpretable models, GSR identiﬁes rela-
tions between the input and a transformed output through searching for expressions of the form g(y) =f(x),
which keeps the possibility open for predicting the output yin a straightforward manner.
Computational complexity. Although genetic algorithms are inherently heuristic, understanding how
our GSR algorithm operates and scales could still be valuable. Following Algorithm 2 from Appendix A, we
can approximate the time complexity of GSR as:
O/parenleftBig
N/epsilon1·Np·/parenleftbig
Mφ·nBφ·mBφ+Mψ·nBψ+Nδ+N+ logNp/parenrightbig/parenrightBig
(16)
whereN/epsilon1is the number of generations until the GP algorithm converges, and Nδis the number of iterations
until the ADMM algorithm converges. Recall that Npis the population size, MφandMψdenote the number
ofnBφ×mBφandnBψ×1basis matrices applied to xandy, respectively, and Nis the number of paired
training examples. More details about GSR’s computational complexity can be found in Appendix A.
11Published in Transactions on Machine Learning Research (12/2022)
Compared to GSR, the special version s-GSR adopts a vanilla SR (where g(y)is simplyy) with the same GP
algorithm and coeﬃcient optimization process (through ADMM) as GSR. Hence, s-GSR’s time complexity
can be approximated as:
O/parenleftBig
N/epsilon1·Np·/parenleftbig
Mφ·nBφ·mBφ+Nδ+N+ logNp/parenrightbig/parenrightBig
(17)
Although GSR’s computational complexity contains an additional term of O(N/epsilon1·Np·Mψ·nBψ), the
number of GP generations N/epsilon1produced by GSR is often much less than that of s-GSR, which explains
the runtime advantage of GSR over s-GSR shown in Table 8.
GSR’s expression ability. Theterm Generalized inGSRmainlystandsforitsabilitytodiscoveranalytical
mappings from the input space to a transformed output space through expressions of the form g(y) =f(x).
This generalizes the classical SR task of identifying expressions of the form y=f(x)(i.e. the latter is simply
a special case of GSR with g(y) =y). In addition, the term Generalized can denote the fact that we constrain
the search space to generalized linear models, keeping in mind that the search space could be conﬁned to
other generalized spaces. Note that, by ﬁnding relations of the form g(y) =f(x), the expression ability of
GSR could resemble that of classical SR tasks which search for relations y=fc(x)where the composition
functionfc(·)is deﬁned as fc(·):=h◦f(·) =h(f(·))withh(·) =g−1(·)ifg(·)is invertable, or a function
class of similar expression ability as g−1(·)ifg(·)is not invertable. However, GSR takes advantage of the fact
that the target yis a scalar, and hence, we can apply many basis functions to y(throughg(·)) without much
increasing the complexity of the expression. In other words, we can avoid searching for functions equivalent
tog−1(·)in a space that could grow exponentially with the dimension of the input feature vector by simply
searching for their corresponding inverse transformations applied to the scalar target variable. This concept,
which happens implicitly in our algorithm provides an edge for GSR over traditional SR methods in terms of
runtime, complexity, and smoothness of the search space. In short, GSR discovers simpliﬁed expressions by
reducing redundancies in the search space, which greatly saves the computational complexity of the search
process. It is worth mentioning that, in principle, GSR’s concept of ﬁtting g(y) =f(x)(instead of y=f(x))
could be applied in conjunction with other classical SR methods; this may require some modiﬁcations to
their parameter/coeﬃcient optimization process.
GSR: a simple yet promising algorithm. GSR combines features and beneﬁts from the usually dis-
parate ﬁelds of system identiﬁcation and genetic programming. On the one hand, SINDy methods use
some LASSO-like approaches (or sequential thresholded least squares) to conduct their sparse non-linear
regression for ﬁnding solutions that take the form of a linear combination of basis functions. On the other
hand, evolutionary algorithms are eﬀective in ﬁnding basis functions that achieve optimal solution. In other
words, GSR combines well established evolutionary methods with more classical system identiﬁcation meth-
ods. Although each of the algorithm components are relatively simple, the overall GSR algorithm achieves
promising experimental performance, highlighting new insights, which can open up new research directions
for future improvement.
6 Conclusion
We introduce GSR, a Generalized Symbolic Regression approach by modifying the formulation of the conven-
tional SR optimization problem. In GSR, we identify mathematical relationships between the input features
and some transformation of the target variable. That is, we infer the mapping from the feature space to a
transformed target space, by searching for expressions of the form g(y) =f(x)instead ofy=f(x). We con-
ﬁne our search space to a weighted sum of basis functions and use genetic programming with a matrix-based
encoding scheme to extract their expressions. We perform several numerical experiments on well-known SR
benchmark datasets and show that our GSR approach is competitive with strong SR benchmark methods.
We further highlight the strengths of GSR by introducing SymSet, a new SR benchmark set which is more
challenging relative to the existing benchmarks. In principle, GSR’s concept of ﬁtting g(y) =f(x)could be
extended to existing SR methods and could boost their performance.
12Published in Transactions on Machine Learning Research (12/2022)
References
Sungsoo Ahn, Junsu Kim, Hankook Lee, and Jinwoo Shin. Guiding deep molecular optimization with genetic
exploration. Advances in neural information processing systems , 33:12008–12021, 2020.
Ali R Al-Roomi and Mohamed E El-Hawary. Universal functions originator. Applied Soft Computing , 94:
106417, 2020.
Thomas Bäck, David B Fogel, and Zbigniew Michalewicz. Evolutionary computation 1: Basic algorithms
and operators . CRC press, 2018.
LucaBiggio, TommasoBendinelli, AlexanderNeitz, AurelienLucchi, andGiambattistaParascandolo. Neural
symbolic regression that scales. In International Conference on Machine Learning , 2021.
StephenBoyd, NealParikh, andEricChu. Distributed optimization and statistical learning via the alternating
direction method of multipliers . Now Publishers Inc, 2011.
Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by
sparse identiﬁcation of nonlinear dynamical systems. Proceedings of the national academy of sciences , 113
(15):3932–3937, 2016.
Bogdan Burlacu, Gabriel Kronberger, and Michael Kommenda. Operon c++: An eﬃcient genetic program-
mingframeworkforsymbolicregression. In Proceedings of the 2020 Genetic and Evolutionary Computation
Conference Companion , GECCO ’20, pp. 1562–1570, New York, NY, USA, 2020. Association for Com-
puting Machinery. ISBN 9781450371278. doi: 10.1145/3377929.3398099.
Chen Chen, Changtong Luo, and Zonglin Jiang. Elite bases regression: A real-time algorithm for symbolic
regression. In 2017 13th International conference on natural computation, fuzzy systems and knowledge
discovery (ICNC-FSKD) , pp. 529–535. IEEE, 2017a.
Chen Chen, Changtong Luo, and Zonglin Jiang. Fast modeling methods for complex system with separable
features. In 2017 10th International Symposium on Computational Intelligence and Design (ISCID) ,
volume 1, pp. 201–204. IEEE, 2017b.
Chen Chen, Changtong Luo, and Zonglin Jiang. Block building programming for symbolic regression.
Neurocomputing , 275:1973–1980, 2018a.
Chen Chen, Changtong Luo, and Zonglin Jiang. A multilevel block building algorithm for fast modeling
generalized separable systems. Expert Systems with Applications , 109:25–34, 2018b.
Miles Cranmer. Pysr: Fast & parallelized symbolic regression in python/julia, September 2020.
Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, and
Shirley Ho. Discovering symbolic models from deep learning with inductive biases. arXiv preprint
arXiv:2006.11287 , 2020.
Stéphane d’Ascoli, Pierre-Alexandre Kamienny, Guillaume Lample, and Francois Charton. Deep symbolic
regression for recurrence prediction. In International Conference on Machine Learning , pp. 4520–4536.
PMLR, 2022.
Fabrício Olivetti de França. A greedy search tree heuristic for symbolic regression. Information Sciences ,
442:18–32, 2018.
Fabricio Olivetti de Franca and Guilherme Seidyo Imai Aldeia. Interaction-transformation evolutionary
algorithm for symbolic regression. Evolutionary Computation , pp. 1–25, 2020.
Anthony William Fairbank Edwards. Likelihood . CUP Archive, 1984.
Ying Jin, Weilin Fu, Jian Kang, Jiadong Guo, and Jian Guo. Bayesian symbolic regression. arXiv preprint
arXiv:1910.08892 , 2019.
13Published in Transactions on Machine Learning Research (12/2022)
Kadierdan Kaheman, J Nathan Kutz, and Steven L Brunton. Sindy-pi: a robust algorithm for parallel im-
plicit sparse identiﬁcation of nonlinear dynamics. Proceedings of the Royal Society A , 476(2242):20200279,
2020.
Pierre-Alexandre Kamienny, Stéphane d’Ascoli, Guillaume Lample, and François Charton. End-to-end sym-
bolic regression with transformers. arXiv preprint arXiv:2204.10532 , 2022.
Samuel Kim, Peter Y Lu, Srijon Mukherjee, Michael Gilbert, Li Jing, Vladimir Čeperić, and Marin Soljačić.
Integration of neural network-based symbolic regression in deep learning for scientiﬁc discovery. IEEE
Transactions on Neural Networks and Learning Systems , 2020.
Michael Kommenda, Bogdan Burlacu, Gabriel Kronberger, and Michael Aﬀenzeller. Parameter identiﬁcation
for symbolic regression using nonlinear least squares. Genetic Programming and Evolvable Machines , 21
(3):471–501, 2020.
John R Koza and John R Koza. Genetic programming: on the programming of computers by means of natural
selection, volume 1. MIT press, 1992.
William La Cava, Patryk Orzechowski, Bogdan Burlacu, Fabrício Olivetti de França, Marco Virgolin, Ying
Jin, Michael Kommenda, and Jason H Moore. Contemporary symbolic regression methods and their
relative performance. arXiv preprint arXiv:2107.14351 , 2021.
Mikel Landajuela Larma, Brenden K Petersen, Soo K Kim, Claudio P Santiago, Ruben Glatt, T Nathan
Mundhenk, Jacob F Pettit, and Daniel M Faissol. Improving exploration in policy gradient search: Ap-
plication to symbolic optimization. arXiv preprint arXiv:2107.09158 , 2021.
Peter M Lee. Bayesian statistics . Arnold Publication, 1997.
Thomas Leonard and John SJ Hsu. Bayesian methods: an analysis for statisticians and interdisciplinary
researchers , volume 5. Cambridge University Press, 2001.
Qiang Lu, Jun Ren, and Zhiguang Wang. Using genetic programming with prior formula knowledge to solve
symbolic regression problem. Computational intelligence and neuroscience , 2016, 2016.
Changtong Luo and Shao-Liang Zhang. Parse-matrix evolution for symbolic regression. Engineering Appli-
cations of Artiﬁcial Intelligence , 25(6):1182–1193, 2012.
Changtong Luo, Chen Chen, and Zonglin Jiang. A divide and conquer method for symbolic regression. arXiv
preprint arXiv:1705.08061 , 2017.
Niall M Mangan, Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Inferring biological networks by
sparse identiﬁcation of nonlinear dynamics. IEEE Transactions on Molecular, Biological and Multi-Scale
Communications , 2(1):52–63, 2016.
Georg Martius and Christoph H Lampert. Extrapolation and learning equations. arXiv preprint
arXiv:1610.02995 , 2016.
TrentMcConaghy. Ffx: Fast, scalable, deterministicsymbolicregressiontechnology. In Genetic Programming
Theory and Practice IX , pp. 235–260. Springer, 2011.
Aaron Meurer, Christopher P Smith, Mateusz Paprocki, Ondřej Čertík, Sergey B Kirpichev, Matthew Rock-
lin, AMiT Kumar, Sergiu Ivanov, Jason K Moore, Sartaj Singh, et al. Sympy: symbolic computing in
python.PeerJ Computer Science , 3:e103, 2017.
T Nathan Mundhenk, Mikel Landajuela, Ruben Glatt, Claudio P Santiago, Daniel M Faissol, and Brenden K
Petersen. Symbolic regression via neural-guided genetic programming population seeding. arXiv preprint
arXiv:2111.00053 , 2021.
John Ashworth Nelder and Robert WM Wedderburn. Generalized linear models. Journal of the Royal
Statistical Society: Series A (General) , 135(3):370–384, 1972.
14Published in Transactions on Machine Learning Research (12/2022)
Michael O’Neill and Conor Ryan. Grammatical evolution. IEEE Transactions on Evolutionary Computation ,
5(4):349–358, 2001.
Patryk Orzechowski, William La Cava, and Jason H Moore. Where are we now? a large benchmark study
of recent symbolic regression methods. In Proceedings of the Genetic and Evolutionary Computation
Conference , pp. 1183–1190, 2018.
Yudi Pawitan. In all likelihood: statistical modelling and inference using likelihood . Oxford University Press,
2001.
Brenden K Petersen, Mikel Landajuela Larma, Terrell N. Mundhenk, Claudio Prata Santiago, Soo Kyung
Kim, and Joanne Taery Kim. Deep symbolic regression: Recovering mathematical expressions from data
via risk-seeking policy gradients. In International Conference on Learning Representations , 2021.
Markus Quade, Markus Abel, Kamran Shaﬁ, Robert K Niven, and Bernd R Noack. Prediction of dynamical
systems by symbolic regression. Physical Review E , 94(1):012214, 2016.
Shahab Razavi and Eric R Gamazon. Neural-network-directed genetic programmer for discovery of governing
equations. arXiv preprint arXiv:2203.08808 , 2022.
Subham Sahoo, Christoph Lampert, and Georg Martius. Learning equations for extrapolation and control.
InInternational Conference on Machine Learning , pp. 4442–4450. PMLR, 2018.
Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. science, 324
(5923):81–85, 2009.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society:
Series B (Methodological) , 58(1):267–288, 1996.
Tony Tohme. The Bayesian validation metric: a framework for probabilistic model calibration and validation .
PhD thesis, Massachusetts Institute of Technology, 2020.
Tony Tohme, Kevin Vanslette, and Kamal Youcef-Toumi. A generalized bayesian approach to model cali-
bration. Reliability Engineering & System Safety , 204:107141, 2020.
Tony Tohme, Kevin Vanslette, and Kamal Youcef-Toumi. Improving regression uncertainty estimation under
statistical change. arXiv preprint arXiv:2109.08213 , 2021.
Leonardo Trujillo, Luis Muñoz, Edgar Galván-López, and Sara Silva. neat genetic programming: Controlling
bloat naturally. Information Sciences , 333:21–43, 2016.
Silviu-Marian Udrescu and Max Tegmark. Ai feynman: A physics-inspired method for symbolic regression.
Science Advances , 6(16):eaay2631, 2020.
Silviu-MarianUdrescu, AndrewTan, JiahaiFeng, OrisvaldoNeto, TailinWu, andMaxTegmark. Aifeynman
2.0: Pareto-optimal symbolic regression exploiting graph modularity. arXiv preprint arXiv:2006.10782 ,
2020.
Nguyen Quang Uy, Nguyen Xuan Hoai, Michael O’Neill, Robert I McKay, and Edgar Galván-López.
Semantically-based crossover in genetic programming: application to real-valued symbolic regression. Ge-
netic Programming and Evolvable Machines , 12(2):91–119, 2011.
Mojtaba Valipour, Bowen You, Maysum Panju, and Ali Ghodsi. Symbolicgpt: A generative transformer
model for symbolic regression. arXiv preprint arXiv:2106.14131 , 2021.
Kevin Vanslette, Tony Tohme, and Kamal Youcef-Toumi. A general model validation and testing tool.
Reliability Engineering & System Safety , 195:106684, 2020.
Marco Virgolin and Solon P Pissis. Symbolic regression is np-hard. arXiv preprint arXiv:2207.01018 , 2022.
15Published in Transactions on Machine Learning Research (12/2022)
Marco Virgolin, Tanja Alderliesten, and Peter AN Bosman. Linear scaling with and within semantic
backpropagation-based genetic programming for symbolic regression. In Proceedings of the genetic and
evolutionary computation conference , pp. 1084–1092, 2019.
CJ Wild and GAF Seber. Nonlinear regression . New York: Wiley, 1989.
Hengzhe Zhang, Aimin Zhou, Hong Qian, and Hu Zhang. Ps-tree: A piecewise symbolic regression tree.
Swarm and Evolutionary Computation , 71:101061, 2022a.
Michael Zhang, Samuel Kim, Peter Y Lu, and Marin Soljačić. Deep learning and symbolic regression for
discovering parametric equations. arXiv preprint arXiv:2207.00529 , 2022b.
16Published in Transactions on Machine Learning Research (12/2022)
A Implementation, Hyperparameters, and Additional Experiment Details
Implementation. Our GSR method discovers expressions of the form g(y) =f(x)wherey∈R,x∈Rd,
and where the search space for f(·)andg(·)is constrained to a weighted sum of MφandMψbasis functions/parenleftbig
namelyφ(·)andψ(·)/parenrightbig
, respectively. We use a matrix-based encoding scheme to represent φ(·)andψ(·)
using basis matrices BφandBψof sizesnBφ×mBφandnBψ×1, respectively (where mBφ=nv+ 2andnv
is deﬁned in Section 3.3.2). Hence, in addition to the number of basis functions MφandMψ, the parameters
nBφ,nv, andmBφaﬀectthecomplexityoftheevolvedexpressions, andhencecanbecontrolledtoconﬁnethe
search space (although dalso aﬀects the complexity of the expression, it is given by the problem and cannot
be controlled). Although more than one basis matrix can lead to the same basis function (see Remark 3.3),
thesearchspaceofbasisfunctions/parenleftbig
mainlyφ(·)/parenrightbig
isstillhugeingeneral, andthus, enumeratingallthepossible
basis functions is not practical. Hence, we will rely on genetic programming (GP) for eﬀective search process.
A pseudocode of our GP-based GSR algorithm is outlined in Algorithm 2.
Algorithm 2: GP Procedure for GSR
Input: Np,np,Mφ,Mψ,Lx,Ly
Output:I∗
function SolveGSR (Np,np,Mφ,Mψ,Lx,Ly)
Initialize population: //I(k)←{I 1(k),I2(k), . . . ,INp(k)}
k←0; // Initialize the generation (or iteration) counter
fori= 1toNpdo
Ii(k)←GenerateRandomIndividual (Mφ, Mψ,Lx,Ly);
/* Each individual Ii(k)contains two randomly generated sets of MφandMψbasis matrices
respectively */
end
Evaluate each individual Ii(k)with respect to the ﬁtness function;
/* For each individual Ii(k), form the matrix Ai(k), solve for the optimal coefficients vector
wi(k)←SolveADMM (Ai(k),···), then compute its fitness */
I(k)←sorted (I(k)); // in ascending order of fitness
whileStopping Criterion not Satisﬁed do
k←k+ 1; // Increment the generation (or iteration) counter
UpdateCriterion ();
/* Start with a strict stopping criterion (e.g. a very low error threshold) and slowly relax it
(e.g. gradually increase the error threshold) */
Ls
x,Ls
y←ChooseSublibrary (Lx,Ly);
/* Choose sublibraries Ls
x⊆LxandLs
y⊆Lyof allowable operations to be used with xandy
respectively */
I[1:np](k)←I[1:np](k−1);
/* The npfittest individuals of the previous generation are copied to the current new one */
fori=np+ 1toNpdo
u←GenerateRandomInteger (1,4);
ifu = 1 then
Ii(k)←Reproduce (I[1:np](k),Ls
x,Ls
y);
/* Crossover based on the surviving individuals */
else ifu = 2 then
Ii(k)←Mutate (I[1:np](k),Ls
x,Ls
y);
/* Mutation based on the surviving individuals */
else
Ii(k)←GenerateRandomIndividual (Mφ, Mψ,Ls
x,Ls
y);
/* Randomly generate a completely new individual */
end
end
Evaluate each individual Ii(k)with respect to the ﬁtness function;
I(k)←sorted (I(k)); // in ascending order of fitness
end
I∗←I1(k); // return the fittest individual
end function
The main inputs to our GP-based algorithm are Np,np,Mφ,Mψ,Lx, andLy. Recall that Npis the
population size and npis the number of surviving individuals per generation. LxandLyare the libraries
of allowable transformations that can be used with xandy, respectively. These libraries form the ﬁrst
17Published in Transactions on Machine Learning Research (12/2022)
two rows of mapping tables, e.g. Table 13, and are deﬁned by the benchmark problem. Note that the
division operator is not part of our GSR architecture. That is, the main arithmetic operations used by
GSR are{+,−,×}. For example, for the Nguyen benchmark dataset, the library of allowable operations is
L0={+,−,×,÷,cos,sin,exp,ln}as shown in Table 19. In this case, we deﬁne Lx={1,•1,cos,sin,exp,ln}
(resulting in the mapping Table 2) and Ly={1,•1,exp,ln}. Regarding the stopping criterion, common
terminating conditions for GP include: i) a solution reaches minimum criterion (e.g. error threshold), ii) the
algorithm reaches a ﬁxed number of generations (or iterations), iii) the algorithm generates a ﬁxed number
of individuals (or candidates expressions), iv) the algorithm reaches a plateau such that new generations no
longer improve results, v) combinations of the above conditions. In our case, the algorithm terminates when
the solution hits a minimum root-mean-square error (RMSE) threshold. To accelerate termination, we slowly
relax the error threshold by gradually increasing it. To avoid reaching a plateau and since we are dealing
with a small population size as shown in Table 9, we enhance diversity (in the basis functions) by producing
completely new individuals with probability 1/2per generation (while performing crossover and mutation
with probability 1/4each per generation). To speed up the search process, we employ sublibraries Ls
x⊆Lx
andLs
y⊆Lyof allowable transformations, used when generating completely new individuals (or completely
new basis functions in the case of mutation). For x, we mainly rely on three sublibraries which are the most
common: a polynomial sublibrary Lpoly, a trigonometric sublibrary Ltrig, and the original library Lxitself.
For the Nguyen benchmark example above, Lpoly={1,•1}andLtrig={1,•1,cos,sin}. Note that power
operators such as •2,•3would be included in these sublibraries if they were part of the original library
deﬁned by the benchmark problem. The function ChooseSublibrary ( )works according to some cycle.
For example, assuming kis the generation (or iteration) counter, if k≤1,500, each cycle consists of 70
iterations broken into three stages, the ﬁrst stage consists of 15iterations and assigns Ls
x←L x, the second
stage consists of 25iterations and assigns Ls
x←L poly, and the third and ﬁnal stage consists of the remaining
35iterations and assigns Ls
x←L trig. This cycle repeats until k= 1,500, after which the cycle’s size becomes
1,500iterations broken into three equal stages (i.e. 500iterations per sublibrary). For y, the cycle consists of
20iterations, in which we equally alternate between the polynomial sublibrary Lpolyand the original library
Lyitself (i.e. 10iterations for each sublibrary). Indeed, the use of sublibraries is only possible when the
corresponding operations are included in the original library deﬁned by the benchmark problem (e.g. Neat-6
and Neat-8 cannot use trigonometric sublibraries since {cos,sin}are not included in their corresponding
original libraries, as shown in Table 19). In addition, it is up to the user to specify the cycle’s size and how
to alternate between sublibraries, or even decide whether to use sublibraries in the ﬁrst place.
Hyperparameters. Throughout our experiments, we adopt the following hyperparameter values. For
GP, we use a population size Np= 30, and we allow for np= 10surviving individuals per generation.
We perform crossover with probability Pc=1
4and allow for only 2parents to be involved in the process
(i.e. new individuals are formed by combing basis functions from two randomly chosen parent individuals).
We apply mutation with probability Pm=1
4and allow for 3basis functions (randomly selected from an
individual) to be mutated (i.e. to be discarded and replaced by completely new basis functions). We generate
a (completely new) random individual with probability Pr=1
2. For ADMM, we use a regularizer λ= 0.4,
a penaltyρ= 0.1. The algorithm terminates when the /lscript2-norm of the diﬀerence between the weight vectors
from two consecutive iterations falls below a threshold of δ= 10−5. Regarding initial conditions, we use
w0=/hatwide1
2=[1
2···1
2]T√
1
4+···+1
4(where “/hatwide” denotes a normalized vector), z0=1= [1···1]T,u0=0= [0···0]T.
For GSR, we allow for a maximum of Mφ= 15basis functions φ(·)for each expression of f(·)(this is the
maximum number since some of the Mφbasis functions will be multiplied by 0, i.e. at most we get Mφ
nonzero coeﬃcients multiplying the basis fcuntions). To avoid overﬁtting and overly complex expressions,
we allow for a maximum of Mψ= 1basis function ψ(·)for each expression of g(·)(in this case the maximum
and minimum are both 1andg(·)will consist of a single basis function). It is worth noting that we use
Mψ= 2for SymSet-11. Each basis ψ(·)will consist of a single transformation nBψ= 1. Each basis φ(·)
will be a product of Nttransformations, where Ntis a random integer between 1and3, i.e.nBφ∈{1,2,3}.
For each of these Nttransformations, the maximum total multiplicity of all the independent variables (or
features) in an argument is a random integer between 2and5, i.e.nv∈{2,3,4,5}. GSR terminates when a
candidate expression achieves a RMSE lower than a threshold with a starting value of /epsilon1= 10−6(recall that
this threshold is slowly relaxed during the process, e.g. by progressively multiplying it by a factor of√
10
for every 1,500iterations). All hyperparameter values are summarized in Table 9.
18Published in Transactions on Machine Learning Research (12/2022)
Table 9: Hyperparameter values for GSR for all experiments, unless otherwise speciﬁed.
Hyperparameter Symbol Value
GP Parameters
Population size Np 30
Number of survivors per generation np 10
Crossover probability Pc 1/4
Number of parents involved in crossover — 2
Mutation probability Pm 1/4
Number of bases to mutate — 3
Randomly generated individual probability Pr 1/2
ADMM Parameters
Regularization parameter λ 0.4
Penalty parameter ρ 0.1
Tolerance on the solution error δ 10−5
Initial guesses w0,z0,u0/hatwide1
2,1,0
GSR Parameters
Maximum number of basis functions φ(·)for each expression of f(·)Mφ 15
Maximum number of basis functions ψ(·)for each expression of g(·)Mψ 1
Maximum total multiplicity of all features in an argument nv{2,3,4,5}
Number of tranformations multiplied together per basis φ(·) nBφ{1,2,3}
Number of tranformations multiplied together per basis ψ(·) nBψ 1
Tolerance on the solution error (RMSE) /epsilon1 10−6
Computational complexity. Although genetic algorithms are inherently heuristic, understanding how our
GSR algorithm operates and scales could still be valuable. Following Algorithm 2 above, we can approximate
the time complexity of GSR as:
O/parenleftbigg
Np·/parenleftBig
Mφ·nBφ·mBφ+Mψ·nBψ·1 +Nδ+O(ﬁtness )/parenrightBig
+O(NplogNp)
+N/epsilon1·/parenleftBig
(Np−np)·/parenleftbig
Pc·O(crossover ) +Pm·O(mutation ) +Pr·(Mφ·nBφ·mBφ+Mψ·nBψ·1)
+Nδ+O(ﬁtness )/parenrightbig
+O(NplogNp)/parenrightBig/parenrightbigg
(18)
whereN/epsilon1is the number of generations until the GP algorithm hits the tolerance /epsilon1, andNδis the number
of iterations until the ADMM algorithm hits the tolerance δ. Note that performing crossover or mutation
operations takes O(1)time (i.e. a constant amount of time), and computing the ﬁtness (which calculates
RMSE onNpaired training examples) takes O(N)time. Also note that Pc,Pm, andPrare probabilities
which can be treated as constants. Hence, GSR’s time complexity reduces to:
O/parenleftBig
N/epsilon1·Np·/parenleftbig
Mφ·nBφ·mBφ+Mψ·nBψ+Nδ+N+ logNp/parenrightbig/parenrightBig
(19)
Compared to GSR, the special version s-GSR adopts a vanilla SR (where g(y)is simplyy) with the same GP
algorithm and coeﬃcient optimization process (through ADMM) as GSR. Thus, s-GSR’s time complexity
can be approximated as:
O/parenleftBig
N/epsilon1·Np·/parenleftbig
Mφ·nBφ·mBφ+Nδ+N+ logNp/parenrightbig/parenrightBig
(20)
Although GSR’s computational complexity contains an additional term of O(N/epsilon1·Np·Mψ·nBψ), the
number of GP generations N/epsilon1produced by GSR is often much less than that of s-GSR, which explains
the runtime advantage of GSR over s-GSR shown in Table 12.
19Published in Transactions on Machine Learning Research (12/2022)
Additional experiment details. For all benchmark problems, we run GSR for multiple independent trials
using diﬀerent random seeds (following the experimental procedure in Petersen et al. (2021); Mundhenk et al.
(2021)). Table 10 shows the recovery rates of GSR against literature-reported values from several algorithms
on the Nguyen and Livermore individual benchmark problems. We ﬁrst note that, due to the wide domain of
sampled input points imposed by Livermore-1 (i.e. [−10,10]), we observed some instabilities in the solution
due to the presence of the exponential function, which we decided to exclude from the library of allowable
operations during the search process for this benchmark. In what follows, we provide explanations for
the results shown in Table 10. Note that Livermore-5 is diﬃcult to recover by NGGPPS+SLP/HER and
the remaining methods as it contains subtractions. Subtraction is more diﬃcult than addition since it is
not cumulative. This is not an issue for GSR since both additions and subtractions are equally recovered
through the sign of the optimal coeﬃcients multiplying the basis functions. Livermore-10 and Livermore-17
are more challenging than Nguyen-10 since they require adding the same basis function many more times
(which is apparent through the poor recovery rates of the diﬀerent methods). Fortunately, this is also
not a problem for GSR since it can be easily solved by ﬁnding the right coeﬃcient multiplying the basis
function. Livermore-18 is more challenging than Livermore-2 and Nguyen-5 since it requires recovering
the constant 5 without a constant optimizer/parenleftbig
which can be recovered asx+x+x+x+x
x/parenrightbig
. For GSR, this can
be recovered by naturally solving for the real-valued coeﬃcient. The problem of the diﬀerent methods on
Livermore-22 lies in the constant 0.5, which requires ﬁndingx
x+xcompared to GSR which simply solves for
the optimal parameter multiplying x2. We observe that GSR performs poorly on Livermore-9 and Livermore-
21 compared to NGGPPS+SLP/HER. This can be due to the choice of hyperparameters (e.g. Mφ,nBφ,
andnv) as well as the GP-based search process. These two benchmarks require ﬁnding the ﬁrst 9and8
powers ofxsimultaneously, respectively, which can be diﬃcult to achieve by GSR, especially that we only
considerMφ= 15basis functions φ(·)per expression of f(·), as mentioned earlier. Note that polynomials
were not an issue for GSR up to the 6thorder (i.e. Nguyen-4). We also tried experimenting with a 7thorder
polynomial (i.e. y=x7+x6+x5+x4+x3+x2+x) and GSR achieved 100% recovery rate. We started
observing a decline in the recovery rate when we added the 8thpower ofx. In other words, Livermore-21
(the 8thorder polynomial) seems to be the limit that GSR can reach with polynomials while Livermore-9
(the 9thorder polynomial) becomes very diﬃcult to recover. It is worth noting that if the libraries for the
Livermore-9 and Livermore-21 problems contained the square and cube operators {•2,•3}(as is the case
for the Jin benchmarks described in Table 19), then GSR would easily recover these two problems. Finally,
GSR is not able to recover Livermore-7/parenleftbig
y= sinh(x)/parenrightbig
and Livermore-8/parenleftbig
y= cosh(x)/parenrightbig
since both benchmarks
require ﬁnding the basis function e−•,2which cannot be expressed using our current encoding scheme unless
it is available as a transformation by itself. That is, the exponential operator e•is not enough to recover
1
e•=e−•using our current encoding scheme. Had the negative exponential operator e−•been part of the
library of allowable operations deﬁned by Livermore-7 and Livermore-8, GSR would easily recover these two
benchmarks. As we can see for SymSet-1/parenleftbig
y=xsinh(x)−4
5/parenrightbig
, we added the operator e−•to the library of
allowable operations (see Table 20), which made GSR’s mission much simpler and it was able to recover the
corresponding ground truth expression as shown in Table 26. It is worth mentioning that, although ground
truth expressions are not expressible, GSR was naturally able to recover the best approximations possible
for Livermore-7 and Livermore-8, which turned out to be their Taylor expansions around 0. GSR’s typical
output expressions were as follows:
Livermore-7:
0.51655y= +0.51655x+ 0.48165x(x×x) + 0.48165x(x×x)
−0.048738(x+x+x)(x+x+x)(x+x) + 0.0022335(x+x)(x)(x×x×x)
⇐⇒y≈x+ 0.166x3+ 0.00865x5
≈x+x3
3!+x5
5!/parenleftbig
the ﬁrst three terms of the Taylor series of sinh(x)around 0/parenrightbig
≈sinh(x)
2Livermore-7 and Livermore-8 can be expressed as sinh(x) =ex−e−x
2andcosh =ex+e−x
2respectively.
20Published in Transactions on Machine Learning Research (12/2022)
Livermore-8:
−0.8616y=−0.2154−0.042956(x+x)x−0.035877(x×x)(x×x)−0.2154−0.2154
−0.0012368(x×x)(x×x×x×x)−0.042956x(x+x)−0.25898x(x)−0.2154
⇐⇒y≈1 + 0.5x2+ 0.0416x4+ 0.00144x6
≈1 +x2
2!+x4
4!+x6
6!/parenleftbig
the ﬁrst four terms of the Taylor series of cosh(x)around 0/parenrightbig
≈cosh(x)
We next perform a runtime comparison between GSR and NGGPPS on the Nguyen benchmark problem set.
We run each benchmark problem and report the runtimes in Table 11. We ﬁnd that GSR exhibits faster
runtime than NGGPPS, averaging 2.5minutes per run on the Nguyen benchmarks compared to 3.2minutes
for NGGPPS. It is worth noting that although GSR is, on average, faster than NGGPPS, it still exhibits
slower runtime on some problems (e.g. Nguyen-6 and Nguyen-9 in Table 11). This is due to the randomness
of the search process as well as the use of sublibraries as mentioned earlier in the Appendix. Indeed, the
runtime depends on the stopping criterion or condition. For example, one can shorten the runtime further
if the interest is just in an approximation rather than an exact recovery. Our GSR method recovers exact
expressions in the order of few minutes.
We further highlight the strengths of GSR on the new SymSet benchmark problem set, and show the
beneﬁts of searching for expressions of the form g(y) =f(x)instead ofy=f(x). Typical expressions,
with exact symbolic equivalence, recovered by GSR are shown in Table 26. The key feature of GSR lies
in its ability to recover expressions of the form g(y) =f(x). To better highlight the beneﬁts oﬀered by
this feature, we disable it by constraining the search space in GSR to expressions of the form y=f(x)
(which is the most critical ablation). We refer to this special version of GSR as s-GSR. Note that most
of the SymSet expressions cannot be exactly recovered by s-GSR (i.e. they can only be approximated).
We compare the performance of GSR against s-GSR on the SymSet benchmarks in terms of accuracy and
runtime (see Table 12). The results clearly show that GSR is faster than s-GSR, averaging around 2minutes
per run on the SymSet benchmarks compared to 2.27minutes for s-GSR (i.e. ∼11%runtime improvement).
In addition, GSR is more accurate than s-GSR by two orders of magnitude. This is due to the fact that GSR
exactly recovers the SymSet expressions across most of the runs, while s-GSR only recovers approximations
for most of these expressions. It is worth mentioning that on SymSet-1, SymSet-4, SymSet-5, SymSet-10,
and SymSet-12, we observe mean RMSE values of the same order of magnitude between GSR and s-GSR,
since these expressions can be exactly recovered by simply learning expressions of the form y=f(x). As
GSR has to perform a search to discover that g(y)is simplyyfor these expressions, it exhibits slower runtime
than s-GSR in recovering these expressions (see Table 12).
In addition, we compare GSR against several strong SR methods with similar (or better) expression ability.
In particular, we experiment on SymSet with NGGPPS, PSTree, PySR, and gplearn (see Table 12). GSR
is more accurate than all these methods by three orders of magnitude, which further demonstrates the
advantage of our proposed approach. As for the runtime, PSTree is the fastest method, averaging around 16
seconds per run on the SymSet expressions, while maintaining solid accuracies. This comes as no surprise
given its state-of-the-art performance on SRBench datasets (La Cava et al., 2021). It is worth mentioning
that on SymSet-16, all the methods (i.e. NGGPPS, PSTree, PySR, and gplearn) exhibited some instabilities
in the solution over all independent runs. Hence, we excluded SymSet-16 for these methods in Table 12.
21Published in Transactions on Machine Learning Research (12/2022)
Table 10: Recovery rate comparison of GSR against literature-reported values from several algorithms on
the Nguyen and Livermore benchmark problem sets over 25independent runs. The ground truth expressions
for these benchmarks are shown in Tables 19 and 20.
Recovery Rate ( %)
Benchmark GSR NGGPPS + SLP/HER NGGPPS GEGL DSR
Nguyen-1 100 100 100 100 100
Nguyen-2 100 100 100 100 100
Nguyen-3 100 100 100 100 100
Nguyen-4 100 100 100 100 100
Nguyen-5 100 100 100 92 72
Nguyen-6 100 100 100 100 100
Nguyen-7 100 100 96 48 35
Nguyen-8 100 100 100 100 96
Nguyen-9 100 100 100 100 100
Nguyen-10 100 100 100 92 100
Nguyen-11 100 100 100 100 100
Nguyen-12⋆100 4 12 0 0
Nguyen Average 100 92.00 92 .33 86 .00 83.58
Livermore-1 100 100 100 100 3
Livermore-2 100 100 100 44 87
Livermore-3 100 100 100 100 66
Livermore-4 100 100 100 100 76
Livermore-5 100 40 4 0 0
Livermore-6 100 100 88 64 97
Livermore-7 0 4 0 0 0
Livermore-8 0 0 0 0 0
Livermore-9 4 88 24 12 0
Livermore-10 100 8 24 0 0
Livermore-11 100 100 100 92 17
Livermore-12 100 100 100 100 61
Livermore-13 100 100 100 84 55
Livermore-14 100 100 100 100 0
Livermore-15 100 100 100 96 0
Livermore-16 100 100 92 12 4
Livermore-17 100 36 68 4 0
Livermore-18 100 48 56 0 0
Livermore-19 100 100 100 100 100
Livermore-20 100 100 100 100 98
Livermore-21 76 88 24 64 2
Livermore-22 100 92 84 68 3
Livermore Average 85.45 77.45 71.09 56 .36 30.41
All Average 90.59 82.59 78 .59 66 .82 49.18
22Published in Transactions on Machine Learning Research (12/2022)
Table 11: Runtimes of GSR vs. NGGPPS on the Nguyen benchmarks. The ground truth expressions for
these benchmarks are shown in Table 19.
Runtime (sec)
Benchmark GSR NGGPPS
Nguyen-1 18.66 27 .05
Nguyen-2 25.96 59 .79
Nguyen-3 38.77 151 .06
Nguyen-4 63.82 268 .88
Nguyen-5 447.01 501 .65
Nguyen-6 465.79 43 .96
Nguyen-7 33.35 752 .32
Nguyen-8 93.89 123 .21
Nguyen-9 391.79 31 .17
Nguyen-10 68.05 103 .72
Nguyen-11 38.86 66 .50
Average 153.27 193.57
23Published in Transactions on Machine Learning Research (12/2022)
Table 12: Average performance in mean RMSE and runtime, along with their standard errors, for GSR
against s-GSR and several strong SR methods on the SymSet benchmark problem sets over 25independent
runs. The ground truth expressions for these benchmarks are shown in Table 20.
Mean RMSE Runtime (sec)
Benchmark GSR s-GSR GSR s-GSR
SymSet-1 2.52×10−5±9.29×10−63.69×10−5±1.62×10−549.94±2.61 43.81±1.95
SymSet-2 4.28×10−7±3.35×10−82.34×10−3±1.21×10−375.24±2.83 91.95±4.26
SymSet-3 6.58×10−6±1.79×10−61.22×10−3±8.17×10−466.34±2.51 88.14±3.49
SymSet-4 7.94×10−4±5.19×10−41.37×10−4±1.46×10−3428.23±8.46 405.54±6.04
SymSet-5 3.63×10−5±9.92×10−54.98×10−5±7.37×10−571.86±4.12 64.19±3.82
SymSet-6 4.38×10−5±8.09×10−68.12×10−2±1.93×10−276.59±3.72 94.18±4.16
SymSet-7 2.39×10−4±1.71×10−56.83×10−2±6.56×10−393.61±2.54 109.94±4.98
SymSet-8 6.83×10−4±1.95×10−57.88×10−3±3.88×10−368.07±2.47 90.61±3.06
SymSet-9 3.57×10−6±3.41×10−56.32×10−3±1.39×10−357.36±2.68 83.18±2.09
SymSet-10 2.12×10−4±4.43×10−43.24×10−4±7.58×10−5393.62±5.47 379.56±6.86
SymSet-11 1.43×10−3±9.97×10−49.39×10−2±6.78×10−3124.38±9.65 187.49±8.35
SymSet-12 1.22×10−5±5.37×10−57.09×10−5±2.88×10−478.53±4.02 69.72±2.97
SymSet-13 2.31×10−5±1.83×10−59.13×10−2±3.28×10−296.14±5.48 114.85±4.61
SymSet-14 2.18×10−5±9.47×10−66.14×10−2±7.78×10−3112.32±4.57 137.61±4.53
SymSet-15 1.81×10−6±4.75×10−56.71×10−3±3.57×10−446.97±2.33 85.07±4.14
SymSet-16 7.24×10−4±3.58×10−49.86×10−3±2.19×10−398.37±3.71 126.16±5.94
SymSet-17 2.57×10−4±7.03×10−53.41×10−3±4.54×10−3116.78±4.49 143.31±6.28
Average 2.66×10−4±1.59×10−42.56×10−2±5.27×10−3120.84±4.22 136.19±4.56
Benchmark NGGPPS PSTree NGGPPS PSTree
SymSet-1 3.66×10−1±7.81×10−37.92×10−3±2.23×10−3175.32±1.54 42.98±3.23
SymSet-2 4.75×10−1±5.92×10−21.96×10−1±2.72×10−2171.46±2.03 24.68±1.05
SymSet-3 1.34×10−2±1.91×10−33.73×10−3±4.01×10−4167.11±1.69 21.68±1.89
SymSet-4 1.76×100±9.89×10−11.32×100±1.61×10−1171.85±2.01 24.14±1.09
SymSet-5 4.21×10−1±3.58×10−23.19×10−1±9.94×10−2177.98±1.57 13.09±0.37
SymSet-6 2.08×100±4.80×10−11.42×100±3.72×10−1179.34±1.75 13.28±0.36
SymSet-7 2.43×10−2±9.62×10−35.25×10−1±7.62×10−2133.89±6.95 11.78±0.24
SymSet-8 3.93×10−1±5.08×10−24.02×10−1±6.42×10−2169.46±1.81 11.54±0.41
SymSet-9 1.34×10−1±3.55×10−21.23×10−1±1.90×10−2175.04±1.04 12.23±0.29
SymSet-10 3.32×10−1±3.09×10−29.91×10−1±9.59×10−2178.27±1.75 11.06±0.17
SymSet-11 2.49×10−1±2.71×10−21.11×10−1±2.24×10−2166.91±1.66 21.26±0.53
SymSet-12 8.76×10−1±6.89×10−28.32×10−1±1.39×10−1178.56±1.53 11.65±0.32
SymSet-13 2.19×10−1±1.77×10−18.69×10−1±1.71×10−1126.39±7.88 9.92±0.23
SymSet-14 4.93×10−17±2.85×10−188.24×10−2±8.50×10−399.85±3.81 9.95±0.21
SymSet-15 2.86×10−17±4.49×10−186.22×10−2±1.28×10−294.42±2.84 9.93±0.18
SymSet-17 9.16×10−2±6.52×10−35.44×10−2±5.51×10−3171.25±1.55 10.46±0.19
Average 4.65×10−1±1.24×10−14.57×10−1±7.98×10−2158.57±2.59 16.23±0.67
Benchmark PySR gplearn PySR gplearn
SymSet-1 1.10×10−3±3.01×10−44.45×10−2±2.91×10−361.21±19.72 140.44±7.27
SymSet-2 5.75×10−2±2.28×10−23.42×10−1±5.12×10−2119.55±39.39 145.84±1.67
SymSet-3 1.71×10−3±2.02×10−41.72×10−2±9.41×10−312.62±0.69 169.38±1.19
SymSet-4 6.14×10−1±6.12×10−12.89×100±5.77×10−179.39±1.98 248.12±9.71
SymSet-5 5.91×10−2±1.28×10−22.64×10−1±1.76×10−269.47±0.49 190.83±1.41
SymSet-6 6.57×100±1.96×1003.94×100±1.31×10067.45±0.67 215.11±6.69
SymSet-7 5.22×10−2±4.64×10−28.93×10−1±1.50×10−1117.47±28.85 169.94±1.91
SymSet-8 3.57×10−1±3.85×10−24.35×10−1±3.16×10−2135.13±30.82 167.64±0.98
SymSet-9 2.58×10−2±8.51×10−32.19×10−1±4.17×10−2162.11±45.01 159.22±1.86
SymSet-10 4.14×10−2±1.72×10−21.63×100±3.38×10−148.17±0.43 192.92±1.72
SymSet-11 2.81×10−2±3.40×10−31.88×10−1±2.91×10−249.88±0.63 184.38±1.34
SymSet-12 2.53×10−2±1.11×10−22.62×10−1±3.11×10−2136.14±41.77 187.32±2.90
SymSet-13 8.44×10−2±6.39×10−21.07×10−16±1.29×10−1716.01±2.98 12.52±0.77
SymSet-14 1.59×10−2±4.51×10−39.82×10−2±1.59×10−257.54±9.81 142.12±5.79
SymSet-15 6.90×10−3±4.02×10−32.05×10−1±1.36×10−293.54±56.54 147.58±1.12
SymSet-17 3.94×10−2±5.21×10−31.18×10−1±8.20×10−3167.45±65.81 148.42±0.67
Average 4.99×10−1±1.76×10−17.22×10−1±1.64×10−187.07±21.60 163.86±2.94
24Published in Transactions on Machine Learning Research (12/2022)
B More Examples on Our Matrix-Based Encoding Scheme
The encoding process happens according to a table of mapping rules that is very straightforward to under-
stand and use. For example, consider the mapping rules shown in Table 13 below, where dis the dimension
of the input feature vector. Note that Table 13 involves more transformations than Table 2.
Table 13: An example table of mapping rules for a basis function. Placeholder operands are denoted by •,
e.g.•2corresponds to the square operator. The identity operator is denoted by •1.
b•,1 0 1 2 3 4 5 6 7 8 9
Transformation ( T)1•1•−1•2•3cos sin exp ln√•
b•,2 0 1 2
Argument Type ( arg)x/summationtext /producttext
b•,3,···,b•,mB 0 1 2 3 ···d
Variable (v) skipx1x2x3···xd
Example 1. Ford= 2,nBφ= 2andmBφ= 4(i.e.nv= 2), the basis function φ(x) =x2
1ex1x2can be
generated according to the encoding steps shown in Table 14.
Table 14: Encoding steps corresponding to the basis function φ(x) =x2
1ex1x2.
StepT arg v 1v2Update
1•2x x 1—T1(x) =x2
1
2 exp/producttextx1x2T2(x) =ex1x2
Final Update: φ(x) =T1(x)·T2(x)
Based on the mapping rules in Table 13 and the encoding steps in Table 14, the basis function φ(x) =x2
1ex1x2
can be encoded by a 2×4matrix as follows:
Bφ=/bracketleftbigg3 0 1•
7 2 1 2/bracketrightbigg
(21)
Example 2. Ford= 3,nBφ= 5andmBφ= 5(i.e.nv= 3), the basis function
φ(x) =x3
2sin(x2x3)√x2+2x3
2x1+x2can be generated according to the encoding steps shown in Table 15.
Table 15: Encoding steps corresponding to the basis function φ(x) =x3
2sin(x2x3)√x2+2x3
2x1+x2.
StepT arg v 1v2v3Update
1•−1/summationtextx1x1x2T1(x) = (2x1+x2)−1
2•3x x 2— —T2(x) =x3
2
3 sin/producttextx2x3—T3(x) = sin(x2x3)
4√•/summationtextx2x3x3T4(x) =√x2+ 2x3
51 — — — — T5(x) = 1
Final Update: φ(x) =T1(x)·T2(x)·T3(x)·T4(x)·T5(x)
Based on the mapping rules in Table 13 and the encoding steps in Table 15, the basis function
φ(x) =x3
2sin(x2x3)√x2+2x3
2x1+x2can be encoded by a 5×5matrix as follows:
Bφ=
2 1 1 1 2
4 0 2• •
6 2 2 3•
9 1 2 3 3
0• • • •
(22)
25Published in Transactions on Machine Learning Research (12/2022)
Example 3. FornBψ= 2, the basis function ψ(y) =y3√ycan be generated according to the encoding
steps shown in Table 16.
Table 16: Encoding steps corresponding to the basis function ψ(y) =y3√y.
StepTUpdate
1•3T1(y) =y3
2√•T2(y) =√y
Final Update: ψ(y) =T1(y)·T2(y)
Based on the mapping rules in Table 13 and the encoding steps in Table 17, the basis function ψ(y) =y3√y
can be encoded by a 2×1matrix as follows:
Bψ=/bracketleftbigg4
9/bracketrightbigg
(23)
Example 4. FornBψ= 3, the basis function ψ(y) = ln(y)can be generated according to the encoding
steps shown in Table 17.
Table 17: Encoding steps corresponding to the basis function ψ(y) = ln(y).
StepTUpdate
11T1(y) = 1
2 lnT2(y) = ln(y)
31T3(y) = 1
Final Update: ψ(y) =T1(y)·T2(y)·T3(y)
Based on the mapping rules in Table 13 and the encoding steps in Table 17, the basis function ψ(y) = ln(y)
can be encoded by a 3×1matrix as follows:
Bψ=
0
8
0
 (24)
Example 5. FornBψ= 1, the basis function ψ(y) =eycan be generated according to the encoding steps
shown in Table 18.
Table 18: Encoding steps corresponding to the basis function ψ(y) =ey.
StepTUpdate
1 expT1(y) =ey
Final Update: ψ(y) =T1(y)
Based on the mapping rules in Table 13 and the encoding steps in Table 18, the basis function ψ(y) =ey
can be encoded by a 1×1matrix as follows:
Bψ=/bracketleftbig7/bracketrightbig
(25)
26Published in Transactions on Machine Learning Research (12/2022)
C Symbolic Regression Benchmark Problem Sets
Table 19: Speciﬁcations of the Symbolic Regression (SR) benchmark problems. Input variables are de-
noted byxfor 1-dimensional problems, and by (x1,x2)for 2-dimensional problems. U(a,b,c )indicates
crandom points uniformly sampled between aandbfor every input variable; diﬀerent random seeds are
used for the training and test sets. E(a,b,c )indicatescevenly spaced points between aandbfor ev-
ery input variable; the same points are used for the training and test sets/parenleftbig
except Neat- 6, which uses
E(1,120,120)as test set, and the Jin tests, which use U(−3,3,30)as test set/parenrightbig
. To simplify the notation,
libraries (of allowable arithmetic operators and mathematical functions) are deﬁned relative to a ‘base’
libraryL0={+,−,×,÷,cos,sin,exp,ln}. Placeholder operands are denoted by •, e.g.•2corresponds
to the square operator.
Benchmark Expression Dataset Library
Nguyen-1 y=x3+x2+x U(−1,1,20)L0
Nguyen-2 y=x4+x3+x2+x U(−1,1,20)L0
Nguyen-3 y=x5+x4+x3+x2+x U(−1,1,20)L0
Nguyen-4 y=x6+x5+x4+x3+x2+x U(−1,1,20)L0
Nguyen-5 y= sin(x2) cos(x)−1 U(−1,1,20)L0
Nguyen-6 y= sin(x) + sin(x+x2) U(−1,1,20)L0
Nguyen-7 y= ln(x+ 1) + ln(x2+ 1) U(0,2,20)L0
Nguyen-8 y=√x U(0,4,20)L0
Nguyen-9 y= sin(x1) + sin(x2
2) U(0,1,20)L0
Nguyen-10 y= 2 sin(x1) cos(x2) U(0,1,20)L0
Nguyen-11 y=xx2
1 U(0,1,20)L0
Nguyen-12 y=x4
1−x3
1+1
2x2
2−x2 U(0,1,20)L0
Nguyen-12⋆y=x4
1−x3
1+1
2x2
2−x2 U(0,10,20)L0
Jin-1 y= 2.5x4
1−1.3x3
1+ 0.5x2
2−1.7x2U(−3,3,100)L0−{ln}∪{•2,•3,const}
Jin-2 y= 8x2
1+ 8x3
2−15 U(−3,3,100)L0−{ln}∪{•2,•3,const}
Jin-3 y= 0.2x3
1+ 0.5x3
2−1.2x2−0.5x1U(−3,3,100)L0−{ln}∪{•2,•3,const}
Jin-4 y= 1.5ex1+ 5 cos(x2) U(−3,3,100)L0−{ln}∪{•2,•3,const}
Jin-5 y= 6 sin(x1) cos(x2) U(−3,3,100)L0−{ln}∪{•2,•3,const}
Jin-6 y= 1.35x1x2+ 5.5 sin ((x1−1)(x2−1))U(−3,3,100)L0−{ln}∪{•2,•3,const}
Neat-1 y=x4+x3+x2+x U(−1,1,20)L0∪{1}
Neat-2 y=x5+x4+x3+x2+x U(−1,1,20)L0∪{1}
Neat-3 y= sin(x2) cos(x)−1 U(−1,1,20)L0∪{1}
Neat-4 y= ln(x+ 1) + ln(x2+ 1) U(0,2,20)L0∪{1}
Neat-5 y= 2 sin(x1) cos(x2) U(−1,1,100)L0
Neat-6 y=/summationtextx
k=11
kE(1,50,50){+,×,÷,•−1,−•,√•}
Neat-7 y= 2−2.1 cos(9.8x1) sin(1.3x2)E(−50,50,105)L0∪{tan,tanh,•2,•3,√•}
Neat-8 y=e−(x1−1)2
1.2+(x2−2.5)2 U(0.3,4,100){+,−,×,÷,exp,e−•,•2}
Neat-9 y=1
1+x−4
1+1
1+x−4
2E(−5,5,21)L0
27Published in Transactions on Machine Learning Research (12/2022)
Table 20: Speciﬁcations of the Symbolic Regression (SR) benchmark problems. Input variables are de-
noted byxfor 1-dimensional problems, by (x1,x2)for 2-dimensional problems, and by (x1,x2,x3)for 3-
dimensional problems. U(a,b,c )indicatescrandom points uniformly sampled between aandbfor every
input variable; diﬀerent random seeds are used for the training and test sets. To simplify the notation, li-
braries (of allowable arithmetic operators and mathematical functions) are deﬁned relative to ‘base’ libraries
L0={+,−,×,÷,cos,sin,exp,ln}orLc
0=L0∪{const}. Placeholder operands are denoted by •, e.g.•2
corresponds to the square operator.
Benchmark Expression Dataset Library
Livermore-1 y=1
3+x+ sin(x2) U(−10,10,1000)L0
Livermore-2 y= sin(x2) cos(x)−2 U(−1,1,20)L0
Livermore-3 y= sin(x3) cos(x2)−1 U(−1,1,20)L0
Livermore-4 y= ln(x+ 1) + ln(x2+ 1) + ln(x) U(0,2,20)L0
Livermore-5 y=x4
1−x3
1+x2
1−x2 U(0,1,20)L0
Livermore-6 y= 4x4+ 3x3+ 2x2+x U(−1,1,20)L0
Livermore-7 y= sinh(x) U(−1,1,20)L0
Livermore-8 y= cosh(x) U(−1,1,20)L0
Livermore-9 y=x9+x8+x7+x6+x5+x4+x3+x2+xU(−1,1,20)L0
Livermore-10 y= 6 sin(x1) cos(x2) U(0,1,20)L0
Livermore-11 y=x2
1x2
1
x1+x2U(−1,1,50)L0
Livermore-12 y=x5
1
x3
2U(−1,1,50)L0
Livermore-13 y=x1
3 U(0,4,20)L0
Livermore-14 y=x3+x2+x+ sin(x) + sin(x2) U(−1,1,20)L0
Livermore-15 y=x1
5 U(0,4,20)L0
Livermore-16 y=x2
5 U(0,4,20)L0
Livermore-17 y= 4 sin(x1) cos(x2) U(0,1,20)L0
Livermore-18 y= sin(x2) cos(x)−5 U(−1,1,20)L0
Livermore-19 y=x5+x4+x2+x U(−1,1,20)L0
Livermore-20 y=e−x2U(−1,1,20)L0
Livermore-21 y=x8+x7+x6+x5+x4+x3+x2+xU(−1,1,20)L0
Livermore-22 y=e−0.5x2U(−1,1,20)L0
SymSet-1 y=xsinh(x)−4
5U(−1,1,20)Lc
0−{ln}∪{e−•}
SymSet-2 y= (x5−3x4−2.8x+ 5)−1U(−1,1,20)Lc
0∪{•−1}
SymSet-3 y= (x4−1.2x2+ 11.5)1
3 U(−1,1,20)Lc
0∪{•2,•3}
SymSet-4 y= 0.8−cos(x) + 4.2exsin(x2) U(−3,3,20)Lc
0
SymSet-5 y= 4.5x2
1+x1x3
2−1.7x2−3.1 U(−1,1,20)Lc
0
SymSet-6 y=5
3x1−x3
2U(−1,1,20)Lc
0∪{•−1}
SymSet-7 y= ln(x3
1+ 4x1x2) U(0,2,20)Lc
0
SymSet-8 y=/radicalbig
5x5
1+ 14x3
1x4
2−2x2+ 7 U(−1,1,20)Lc
0∪{•2,•3}
SymSet-9 y= (2x1+x2)−2
3 U(0,2,20)Lc
0
SymSet-10 y= 1.5 cos(x1) ln(x1x2)−2.5 U(0,1,20)Lc
0
SymSet-11 y=/radicalbig
2 cos(x1) + 30ex2+ 4 U(−1,1,20)Lc
0∪{•2}
SymSet-12 y= 0.4x4
1+ 6.2x2−3.5x1x3−4.5 U(−1,1,20)Lc
0
SymSet-13 y=2x2
x1+x3U(0,1,20)Lc
0
SymSet-14 y=x1x2x3
x1+x2+x3U(0,2,20)Lc
0
SymSet-15 y= (x1+x2)x3 U(0,1,20)Lc
0
SymSet-16 y=e2.6x1−ln(x2)+9.8 cos(x3)U(0,1,20)Lc
0
SymSet-17 y= ln/parenleftbig
0.2ex1+x2+ 0.5 cos(x2
3)/parenrightbig
U(0,1,20)Lc
0
28Published in Transactions on Machine Learning Research (12/2022)
D Typical Recovered Expressions
Table21: Typicalexpressions(withexactsymbolicequivalence)recoveredbyGSRfortheNguyenbenchmark
set. Note that the coeﬃcients in the GSR expressions form a unit vector due to the normalization constraint
imposed by the Lasso problem in Equation 6. It is easy to verify (by simpliﬁcation) that the GSR expressions
are symbolically equivalent to the ground truth expressions.
Benchmark Expression
Truthy=x3+x2+x
Nguyen-1 −0.558y=−0.1395(x×x×x)−0.1395(x×x×x)−0.2697x−0.1395(x×x×x)
GSR −0.2697x−0.2697x+ 0.0651(x+x+x+x) + 0.0651(x+x+x+x)
−0.1395(x×x×x)−0.2697x−0.558(x×x)
Truthy=x4+x3+x2+x
Nguyen-2 −0.58554y=−0.09759x(x+x)−0.58554x−0.19518(x×x)x−0.09759(x+x)x
GSR −0.19518(x×x×x)−0.29277x(x×x×x)−0.29277x(x×x×x)
−0.09759(x+x)x−0.19518(x×x)x
Truthy=x5+x4+x3+x2+x
Nguyen-3 0.5y= +0.125x+ 0.25(x×x×x×x) + 0.125(x)x+ 0.5(x×x)x
GSR +0.125x+ 0.5x(x×x×x×x) + 0.125x+ 0.125(x×x) + 0.125x
+0.125(x)x+ 0.125(x×x) + 0.25x(x×x×x)
Truthy=x6+x5+x4+x3+x2+x
−0.48318y=−0.096636x−0.48318x(x×x)(x×x)−0.16106(x×x)−0.16106(x×x)
Nguyen-4 GSR −0.24159(x×x)x−0.096636x−0.48318(x×x×x)(x×x)x
−0.24159(x×x×x)−0.24159(x×x×x)(x+x)−0.096636x
−0.096636x−0.096636x−0.16106(x×x)
Nguyen-5 Truthy= sin(x2) cos(x)−1
GSR 0.63246y= 0.63246 cos(x) sin(x×x)−0.31623−0.31623
Nguyen-6 Truthy= sin(x) + sin(x+x2)
GSR 0.5y= 0.5 cos(x) sin(x×x) + 0.5 sin(x) + 0.5 cos(x×x) sin(x)
Truthy= ln(x+ 1) + ln(x2+ 1)
0.70956ey= +0.1095x(x×x×x) + 0.1095(x×x×x×x) + 0.17082x+ 0.23652
Nguyen-7 GSR +0.17082x+ 0.12702(x×x)(x+x) + 0.17082x−0.1095x(x+x)(x×x)
+0.22776x(x×x) + 0.23652 + 0.22776(x×x)x+ 0.23652
+0.23652(x+x+x)x+ 0.17082x+ 0.01314(x+x)
Nguyen-8 Truthy=√x
GSR 0.83654 ln(y) =−0.032175 ln(x×x×x×x) + 0.54697 ln(x)
Nguyen-9 Truthy= sin(x1) + sin(x2
2)
GSR−0.57735y=−0.57735 sin(x1)−0.57735 sin(x2×x2)
Nguyen-10 Truthy= 2 sin(x1) cos(x2)
GSR 0.44721y= 0.89442 sin(x1) cos(x2)
Nguyen-11 Truthy=xx2
1
GSR 0.70711 ln(y) = 0.70711x2ln(x1)
Truthy=x4
1−x3
1+1
2x2
2−x2
Nguyen-12 GSR−0.4y=−0.2(x2×x2)−0.4x1−0.4x1+ 0.4(x1+x2+x1) + 0.4(x1×x1)x1
−0.4x1(x1×x1×x1)
Truthy=x4
1−x3
1+1
2x2
2−x2
Nguyen-12⋆GSR−0.6y=−0.1(x2+x2+x2)x2−0.3(x1+x1)(x1×x1×x1) + 0.3(x1×x1×x1)
+0.3x1(x1×x1) + 0.6x2
29Published in Transactions on Machine Learning Research (12/2022)
Table 22: Typical expressions (with exact symbolic equivalence) recovered by GSR for the Jin benchmark
set. Note that the coeﬃcients in the GSR expressions form a unit vector due to the normalization constraint
imposed by the Lasso problem in Equation 6. It is easy to verify (by simpliﬁcation) that the GSR expressions
are symbolically equivalent to the ground truth expressions. Although GSR does not exactly recover Jin-6,
it recovers approximations with very low RMSE, as shown in Table 5.
Benchmark Expression
Jin-1 Truthy= 2.5x4
1−1.3x3
1+ 0.5x2
2−1.7x2
GSR 0.30664y= +0.15332x2
2−0.398632x3
1−0.260644x2−0.260644x2+ 0.7666x1x3
1
Jin-2 Truthy= 8x2
1+ 8x3
2−15
GSR 0.06909y=−0.518175 + 0.55272x3
2−0.518175 + 0.27636x2
1+ 0.27636x2
1
Jin-3 Truthy= 0.2x3
1+ 0.5x3
2−1.2x2−0.5x1
GSR−0.7943y=−0.11914x2−0.15886x3
1+ 0.39715(x2+x2+x2+x1)−0.11915x2−0.39715x3
2
Jin-4 Truthy= 1.5ex1+ 5 cos(x2)
GSR−0.25198y=−0.37797ex1−0.62995 cos(x2)−0.62995 cos(x2)
Jin-5 Truthy= 6 sin(x1) cos(x2)
GSR 0.13484y=−0.80904 sin(x2) cos(x1) + 0.40452 sin(x2+x1) + 0.40452 sin(x2+x1)
Jin-6 Truthy= 1.35x1x2+ 5.5 sin ((x1−1)(x2−1))
GSRNot exactly recovered
Table 23: Typical expressions (with exact symbolic equivalence) recovered by GSR for the Neat benchmark
set. Note that the coeﬃcients in the GSR expressions form a unit vector due to the normalization constraint
imposed by the Lasso problem in Equation 6. It is easy to verify (by simpliﬁcation) that the GSR expressions
are symbolically equivalent to the ground truth expressions. Although GSR does not exactly recover Neat-6,
Neat-7, Neat-8, and Neat-9, it recovers approximations with very low RMSE, as shown in Table 6.
Benchmark Expression
Truthy=x4+x3+x2+x
Neat-1 −0.64952y=−0.32476x(x+x)(x×x)−0.064952(x+x) + 0.082996(x+x+x)
GSR −0.32476(x×x)−0.2129x−0.32476(x×x)−0.18764x(x+x)x
−0.064952(x+x)−0.2129x−0.2129x−0.27424(x×x×x)
Truthy=x5+x4+x3+x2+x
0.3914y= +0.3914x(x×x×x×x) + 0.18274x+ 0.27676x(x) + 0.18274x
Neat-2 GSR +0.02794(x+x) + 0.3914x(x×x)−0.02702(x+x+x)(x+x)
+0.18274x+ 0.27676(x×x)−0.12682(x+x+x) + 0.3914(x×x×x)x
+0.18274x+ 0.18274x−0.12682(x+x+x) + 0.18274x
Neat-3 Truthy= sin(x2) cos(x)−1
GSR−0.57735y=−0.57735 sin(x×x) cos(x) + 0.57735
Neat-4 Truthy= ln(x+ 1) + ln(x2+ 1)
GSR−0.5ey=−0.25(x×x)−0.5−0.25x−0.25x−0.5x(x×x)−0.25(x×x)
Neat-5 Truthy= 2 sin(x1) cos(x2)
GSR 0.57735y= 0.57735 cos(x2) sin(x1) + 0.57735 cos(x2) sin(x1)
Neat-6 Truthy=/summationtextx
k=11
k
GSRNot exactly recovered
Neat-7 Truthy= 2−2.1 cos(9.8x1) sin(1.3x2)
GSRNot exactly recovered
Neat-8 Truthy=e−(x1−1)2
1.2+(x2−2.5)2
GSRNot exactly recovered
Neat-9 Truthy=1
1+x−4
1+1
1+x−4
2
GSRNot exactly recovered
30Published in Transactions on Machine Learning Research (12/2022)
Table 24: Typical expressions (with exact symbolic equivalence) recovered by GSR for the Livermore bench-
mark set. Note that the coeﬃcients in the GSR expressions form a unit vector due to the normalization con-
straint imposed by the Lasso problem in Equation 6. It is easy to verify (by simpliﬁcation) that the GSR ex-
pressionsaresymbolicallyequivalenttothegroundtruthexpressions. AlthoughGSRdoesnotexactlyrecover
Livermore-7andLivermore-8, itnaturallyrecoverstheirTaylorapproximations, as discussed in Appendix A.
Benchmark Expression
Livermore-1 Truthy=1
3+x+ sin(x2)
GSR 0.65079y= 0.21693 + 0.65079 sin(x×x) + 0.325395(x+x)
Livermore-2 Truthy= sin(x2) cos(x)−2
GSR−0.40825y=−0.40825 cos(x) sin(x×x) + 0.8165
Livermore-3 Truthy= sin(x3) cos(x2)−1
GSR−0.57735y= 0.57735−0.57735 sin(x×x×x) cos(x×x)
Truthy= ln(x+ 1) + ln(x2+ 1) + ln(x)
Livermore-4 GSR−0.50998ey=−0.25499(x×x)−0.21064x−0.022175(x+x)−0.21064x−0.25499(x×x)
−0.50998(x×x)(x×x)−0.50998x(x×x)−0.022175(x+x)
Livermore-5 Truthy=x4
1−x3
1+x2
1−x2
GSR 0.44721y=−0.44721x2+ 0.44721(x1×x1)−0.44721(x1×x1)x1+ 0.44721(x1×x1)(x1×x1)
Truthy= 4x4+ 3x3+ 2x2+x
Livermore-6 −0.15763y=−0.26677x−0.26677x−0.093216(x+x)−0.23918(x×x)
GSR −0.03804(x+x)x−0.63052x(x)(x×x)−0.47289(x×x×x)
−0.093216(x+x) + 0.253886(x+x+x+x)−0.26677x
Livermore-7 Truthy= sinh(x)
GSRNot exactly recovered
Livermore-8 Truthy= cosh(x)
GSRNot exactly recovered
Truthy=x9+x8+x7+x6+x5+x4+x3+x2+x
GSR−0.30767y=−0.30767x(x×x×x)(x×x)−0.30767(x×x×x)x−0.266671(x×x)
Livermore-9 −0.30767(x×x×x)(x)(x×x×x)−0.153835x−0.30767(x×x×x)
−0.30767(x×x×x×x×x)(x×x×x×x)−0.30767(x×x×x×x)x
+0.056037(x+x+x+x)(x+x+x+x)−0.266671(x×x)−0.153835x
−0.30767(x×x×x×x)(x)(x×x×x)−0.22364x(x+x+x)
Livermore-10 Truthy= 6 sin(x1) cos(x2)
GSR 0.22942y= 0.68826 cos(x2) sin(x1) + 0.68826 sin(x1) cos(x2)
Livermore-11 Truthy=x2
1x2
1
x1+x2
GSR−0.40825 ln(y) =−0.8165 ln(x1×x1) + 0.40825 ln(x2+x1)
31Published in Transactions on Machine Learning Research (12/2022)
Table 25: Typical expressions (with exact symbolic equivalence) recovered by GSR for the Livermore bench-
mark set (cont’d). Note that the coeﬃcients in the GSR expressions form a unit vector due to the normal-
ization constraint imposed by the Lasso problem in Equation 6. It is easy to verify (by simpliﬁcation) that
the GSR expressions are symbolically equivalent to the ground truth expressions.
Benchmark Expression
Livermore-12 Truthy=x5
1
x3
2
GSR 0.16903 ln(y) = 0.84515 ln(x1)−0.50709 ln(x2)
Livermore-13 Truthy=x1
3
GSR 0.97332 ln(y) = 0.16222 ln(x) + 0.16222 ln(x)
Livermore-14 Truthy=x3+x2+x+ sin(x) + sin(x2)
GSR 0.40825y= 0.40825x(x×x) + 0.40825 sin(x) + 0.40825(x×x) + 0.40825 sin(x×x) + 0.40825x
Livermore-15 Truthy=x1
5
GSR 0.99015 ln(y) = 0.099015 ln(x) + 0.099015 ln(x)
Livermore-16 Truthy=x2
5
GSR 0.99504 ln(y) = 0.099504 ln(x×x×x×x)
Livermore-17 Truthy= 4 sin(x1) cos(x2)
GSR 0.17408y= 0.69632 sin(x2+x1)−0.69632 cos(x1) sin(x2)
Livermore-18 Truthy= sin(x2) cos(x)−5
GSR−0.19245y= 0.96225−0.19245 cos(x) sin(x×x)
Truthy=x5+x4+x2+x
Livermore-19 GSR−0.46202y=−0.46202x(x×x)x−0.015609(x+x+x) +−0.46202∗(x1∗x1)−0.46202x(x×x)(x×x)
−0.290313x−0.15297(x+x)−0.15297(x+x) + 0.12175(x+x+x+x)
Livermore-20 Truthy=e−x2
GSR−0.89442 ln(y) = 0.44721(x+x)x
Truthy=x8+x7+x6+x5+x4+x3+x2+x
−0.38914y=−0.027357(x+x+x)x−0.38914(x×x×x)(x×x)−0.38914x(x×x×x×x)(x×x×x)
Livermore-21 GSR +0.0714425x(x+x)(x+x)−0.38914x−0.38914(x×x×x×x)−0.22497(x×x×x)
−0.19457(x×x×x×x)(x+x)(x×x)−0.22497x(x×x)−0.027357x(x+x+x)
−0.22497(x×x)x−0.224998(x×x)−0.097285x(x+x+x+x)(x×x×x×x)
Livermore-22 Truthy=e−0.5x2
GSR 0.8165 ln(y) =−0.40825(x+x)x+ 0.40825(x×x)
32Published in Transactions on Machine Learning Research (12/2022)
Table26: Typicalexpressions(withexactsymbolicequivalence)recoveredbyGSRfortheSymSetbenchmark
set. Note that the coeﬃcients in the GSR expressions form a unit vector due to the normalization constraint
imposed by the Lasso problem in Equation 6. It is easy to verify (by simpliﬁcation) that the GSR expressions
are symbolically equivalent to the ground truth expressions.
Benchmark Expression
SymSet-1 Truthy=xsinh(x)−4
5
GSR 0.70448y= 0.35224exx−0.17612xe−x−0.563584exe−x−0.17612xe−x
Truthy= (x5−3x4−2.8x+ 5)−1
SymSet-2 −0.11335y−1=−0.23188x−0.11335 + 0.50651(x+x)−0.11335−0.23188x
GSR −0.11335−0.11335−0.23188x−0.11335(x×x×x×x)x
−0.11335 + 0.34005x(x×x×x)
Truthy= (x4−1.2x2+ 11.5)1
3
SymSet-3 GSR 0.1173y3= +0.26576x2+ 0.26576x2+ 0.26576(x×x) + 0.44965 + 0.44965
+0.02346(x+x+x+x+x)x3−0.23451(x+x+x+x)x+ 0.44965
SymSet-4 Truthy= 0.8−cos(x) + 4.2exsin(x2)
GSR 0.22485y=−0.112425 cos( x) + 0.94437 sin(x×x)ex−0.112425 cos( x) + 0.17988
SymSet-5 Truthy= 4.5x2
1+x1x3
2−1.7x2−3.1
GSR−0.16964y=−0.16964(x2×x2×x2×x1)−0.76338(x1×x1) + 0.525884 + 0.288388x2
SymSet-6 Truthy=5
3x1−x3
2
GSR 0.84515y−1=−0.16903(x2×x2)x2+ 0.50709x1
SymSet-7 Truthy= ln(x3
1+ 4x1x2)
GSR−0.482715ey=−0.64362(x1+x1+x1)x2+ 0.21883x2−0.482715(x1×x1×x1)−0.21883x2
SymSet-8 Truthy=/radicalbig
5x5
1+ 14x3
1x4
2−2x2+ 7
GSR 0.060634y2= 0.30317(x2
1×x3
1) + 0.848876x2
1x3
2(x1×x2) + 0.424438−0.060634(x2+x2)
SymSet-9 Truthy= (2x1+x2)−2
3
GSR 0.83205 ln(y) =−0.5547 ln(x1+x2+x1)
SymSet-10 Truthy= 1.5 cos(x1) ln(x1x2)−2.5
GSR 0.32444y=−0.8111 + 0.48666 ln(x1×x2) cos(x1)
SymSet-11 Truthy=/radicalbig
2 cos(x1) + 30ex2+ 4
GSR−0.228568y+ 0.028571y2=−0.457136 + 0.85713ex2+ 0.057142 cos( x1)
Truthy= 0.4x4
1+ 6.2x2−3.5x1x3−4.5
SymSet-12 GSR 0.16288y=−0.18324 + 0.065152x1(x1)(x1×x1)−0.57008(x3×x1) + 0.504928x2−0.18324
−0.18324−0.18324 + 0.504928x2
SymSet-13 Truthy=2x2
x1+x3
GSR 0.57735 ln(y) = 0.57735 ln(x2+x2)−0.57735 ln(x1+x3)
SymSet-14 Truthy=x1x2x3
x1+x2+x3
GSR 0.57735 ln(y) = 0.57735 ln(x1×x2×x3)−0.57735 ln(x2+x3+x1)
SymSet-15 Truthy= (x1+x2)x3
GSR 0.70711 ln(y) = 0.70711x3ln(x1+x2)
SymSet-16 Truthy=e2.6x1−ln(x2)+9.8 cos(x3)
GSR 0.098035 ln(y) = 0.960743 cos( x3)−0.0490175 ln( x2×x2) + 0.254891x1
SymSet-17 Truthy= ln/parenleftbig
0.2ex1+x2+ 0.5 cos(x2
3)/parenrightbig
GSR 0.88045ey= 0.17609ex1+x2+ 0.440225 cos( x3×x3)
33Published in Transactions on Machine Learning Research (12/2022)
E Limitations
Although GSR achieves great results whether by recovering exact expressions or approximations with low
errors, it still has several limitations:
Absence of division operations. The primary limiting factor to our GSR method is that it still cannot
handle divisions. This is due to the way we deﬁne our encoding scheme. In this current version, we only
consider a weighted sum of basis functions where the basis functions are a product of transformations; no
divisions are involved. We can overcome this issue by modifying the encoding scheme to include divisions
within the basis functions/parenleftbig
e.g. a negative integer in the ﬁrst column of the basis functions implies a division
by the corresponding transformation, i.e. using Table 13, a ﬁrst-column entry of −8encodes the division1
ln/parenrightbig
.
However, this will signiﬁcantly increase the total number of possible combinations in which we can form basis
matrices. Due to the lack of divisions in its current version, GSR suﬀers on some benchmarks such as Neat-6,
Neat-8, Neat-9, Livermore-7, Livermore-8. It is worth noting that GSR can recover some divisions with the
help of the lnor•−1operators (see Livermore-11, Livermore-12, Livermore-20, Livermore-22, SymSet-2,
SymSet-6, SymSet-9, SymSet-13, and SymSet-14). This is only possible when the original function consists
of only one term (not a sum of terms).
Composition of tranformations. Another limiting factor to the current version of GSR is that it cannot
recover expressions containing composite functions, such as y=ecos(x)+ ln(x). In this example, the basis
functionecos(x)cannot be recovered by GSR due to our encoding scheme. Again, if ln(x)was not there,
that is, if the function contained the ﬁrst term only, i.e. y=ecos(x), then GSR can handle the situation
by recovering ln(y) = cos(x). The beneﬁts of using g(y) =f(x)can be clearly observed on the SymSet
benchmark problems (especially SymSet-16, and SymSet-17).
Choice of hyperparameters and search process. Throughout our experiments, we have observed that,
for some benchmarks (such as Jin-6 and Neat-7), althought they are expressible by GSR, they were not
fully recovered. GSR only recovered approximations for these benchmarks with very low errors. This can be
explained by two reasons: i) The choice of hyperparameters aﬀects the search process, ii) Our matrix-based
GPsearchprocessmaynotbeveryeﬀectiveonthesebenchmarks, giventhecomplexityoftheircorresponding
basis functions, and thus they may require a huge number of iterations to be recovered. That is, if we keep
our code running for a very long time, we may be able to recover these benchmarks. This can be veriﬁed by
expanding Jin-6 and Neat-7 as follows:
Jin-6:
y= 1.35x1x2+ 5.5 sin ((x1−1)(x2−1))
= 1.35x1x2+ 5.5 sin (x1x2−x1−x2+ 1)
= 1.35x1x2+ 5.5/bracketleftbig
sin (−x1−x2) cos (x1x2+ 1) + cos (−x1−x2) sin (x1x2+ 1)/bracketrightbig
= 1.35x1x2−5.5 cos(1) sin( x1+x2) cos(x1x2)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
φ1(x)+5.5 sin(1) sin( x1+x2) sin(x1x2)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
φ2(x)
+ 5.5 cos(1) cos( x1+x2) sin(x1x2)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
φ3(x)+5.5 sin(1) cos( x1+x2) cos(x1x2)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
φ4(x)
Neat-7:
y= 2−2.1 cos(9.8x1) sin(1.3x2)
= 2−2.1/parenleftbig
cos(9.8) cos(x1)−sin(9.8) sin(x1)/parenrightbig/parenleftbig
sin(1.3) cos(x2) + cos(1.3) sin(x2)/parenrightbig
= 2−2.1 cos(9.8) sin(1.3) cos(x1) cos(x2)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
φ1(x)+2.1 sin(9.8) sin(1.3) sin(x1) cos(x2)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
φ2(x)
−2.1 cos(9.8) cos(1.3) cos(x1) sin(x2)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
φ3(x)+2.1 sin(9.8) cos(1.3) sin(x1) sin(x2)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
φ4(x)
As we can see, GSR has to ﬁnd the four corresponding basis functions simultaneously in order to recover
the expressions.
Indeed, there are plenty of expressions that still cannot be fully recovered by our GSR method. This is the
case for all the other methods as well.
34