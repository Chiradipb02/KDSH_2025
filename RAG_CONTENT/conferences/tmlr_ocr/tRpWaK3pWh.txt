Published in Transactions on Machine Learning Research (12/2024)
A Generalization Bound for Nearly-Linear Networks
Eugene Golikov evgenii.golikov@epfl.ch
Chair of Statistical Field Theory
École Polytechnique Fédérale de Lausanne (EPFL)
Reviewed on OpenReview: https: // openreview. net/ forum? id= tRpWaK3pWh
Abstract
We consider nonlinear networks as perturbations of linear ones. Based on this approach, we
present a novel generalization bound that become non-vacuous for networks that are close
to being linear. The main advantage over the previous works which propose non-vacuous
generalization bounds is that our bound is a priori: performing the actual training is not
required for evaluating the bound. To the best of our knowledge, it is the first non-vacuous
generalization bound for neural nets possessing this property.
1 Introduction
Despite huge practical advancements of deep learning, the main object of this field, a neural network, is not
yet fully understood. As we do not have a complete understanding of how neural networks learn, we are not
able to answer the main question of deep learning theory: why do neural networks generalize on unseen data?
While the above question is valid for any supervised learning model, it is notoriously difficult to answer
for neural nets. The reason is that modern neural nets have billions of parameters and as a result, huge
capacity. Therefore among all parameter configurations that fit the training data, there provably exist such
configurations that do not fit the held-out data well (Zhang et al., 2021). This is the reason why classical
approaches for bounding a generalization gap, i.e. the difference between distribution and train errors, fall
short on neural networks: such approaches bound the gap uniformly over a model class. That is, if weights
for which the network performs poorly exist, we bound our trained network’s performance by performance of
that poor one.
As we observe empirically, networks commonly used in practice do generalize, which means that training
algorithms we use (i.e. gradient descent or its variants) choose "good" parameter configurations despite the
existence of poor ones. In other words, these algorithms are implicitly biased towards good solutions.
Unfortunately, implicit bias of gradient descent is not fully understood yet. This is because the training
dynamics is very hard to integrate, or even characterize, analytically. There are two main obstacles we could
identify. First, modern neural networks have many layers, resulting in a high-order weight evolution equation.
Second, activation functions we use are applied to hidden representations elementwise, destroying a nice
algebraic structure of stacked linear transformations.
If we remove all activation functions, the training dynamics of gradient descent can be integrated analytically
under certain assumptions (Saxe et al., 2013). However, the resulting model, a linear network, is as expressive
as a linear model, thus loosing one of the crucial advantages of neural nets.
Idea.The idea we explore in the present paper is to consider nonlinear nets as perturbations of linear
ones. We show that the original network can be approximated with a proxy-model whose parameters can be
computed using parameters of a linear network trained the same way as the original one. Since the proxy-
model uses the corresponding linear net’s parameters, its generalization gap can be meaningfully bounded
with classical approaches. Indeed, if the initial weights are fixed, the result of learning a linear network to
minimize square loss on a dataset (X∈Rd×m,Y∈Rdout×m)is determined uniquely by YX⊤∈Rdout×dand
1Published in Transactions on Machine Learning Research (12/2024)
XX⊤∈Rd×d, whered,doutare the input and output dimensions, and mis the dataset size. The number of
parameters in these two matrices is much less than the total number of parameters in the network, making
classical counting-based approaches meaningful.
Contributions. Our main contribution is a generalization bound given by Theorem 5.2, which is ready to
apply in the following setting: (1) fully-connected networks, (2) gradient descent with vanishing learning
rate (gradient flow), (3) binary classification with MSE loss. The main disadvantage of our bound is that it
diverges as training time tgoes to infinity. We discuss how to choose the training time in such a way that the
bound stays not too large while the training risk diminishes significantly, in Section 5.3. We validate our
bound on a simple fully-connected network trained on a downsampled MNIST dataset, and demonstrate that
it becomes non-vacuous in this scenario (Section 6). We list advantages and disadvantages of our bound over
that of existing generalization bounds in Section 2. We discuss the assumptions we use, as well as possible
improvements of our bound, in Appendix A. Finally, we discuss how far the approach we choose in this work,
i.e. generalization bounds based on deviation from specific proxy models, could lead us in the best case
scenario (Appendix B).
2 Comparison to previous work
Advantages. The bound of our Theorem 5.2 has the following advantages over some other non-vacuous
bounds available in the literature (e.g. Biggs & Guedj (2022); Galanti et al. (2023); Dziugaite & Roy (2017);
Zhou et al. (2019)):
1.It is ana prioribound, i.e. getting the actual trained the model is not required for evaluating it. To
the best of our knowledge, it is the first non-vacuous a priori bound available for neural nets. All
works mentioned above, despite providing non-vacuous bounds, could be evaluated only on a trained
network thus relying on the implicit bias phenomenon which is not well understood yet.
2.Related to the previous point, to the best of our knowledge, our bound is the first to incorporate
the implicit bias of gradient flow explicitly. This is done by demonstrating that the model does not
deviate much from a specific proxy model. The class of realizable proxy models has a small number
of effective parameters. Informally, this indicates that this class is "simple"; hence the gradient flow
is biased towards this simple class, and our bound exploits this property.
3.It does not require a held-out dataset to evaluate it, compared to Galanti et al. (2023) and coupled
bounds of Biggs & Guedj (2022). Indeed, if one had a held-out dataset, they could just use it directly
to evaluate the trained model, thus questioning the practical utility of such bounds.
4.It does not grow with network width. In contrast, PAC-Bayesian bounds, Biggs & Guedj (2022);
Dziugaite & Roy (2017); Zhou et al. (2019), might grow with width.
5.Similarly to Biggs & Guedj (2022); Galanti et al. (2023), we bound the generalization gap of the
original trained model, not its proxy. In contrast, Dziugaite & Roy (2017) introduces a Gaussian
noise to the learned weights, while Zhou et al. (2019) crucially relies on quantization and compression
after training.
Related to the second point, we bound the generalization gap for the proxy model with a simple parameter-
counting bound (similar to that of Vapnik & Chervonenkis (1971)), thus demonstrating that, contrary to a
common judgement, such bounds could be useful in the context of models with many more parameters than
data points.
Disadvantages. For a fair comparison, we also list the disadvantages our present bound has:
1.The bound of our Theorem 5.2 becomes non-vacuous only when the following two conditions hold.
First, a simple counting-based generalization bound for a linear model evaluated in the same setting
should be non-vacuous. Such a bound is vacuous even for binary classification on the standard
MNIST dataset, but becomes non-vacuous if we downsample the images.
2Published in Transactions on Machine Learning Research (12/2024)
2.Second, the activation function has to be sufficiently close to being linear. To be specific, for a
two-layered leaky ReLU neural net trained on MNIST downsampled to 7x7, one needs the negative
slope to be not less than 0.99(1corresponds to a linear net, while ReLU corresponds to 0).
3.One may hope for the bound to be non-vacuous only for a partially-trained network, while for a
fully-trained network the bound diverges.
4.Even in the most optimistic scenario, when we manage to tighten the terms of our bound as much as
possible, our bound stays non-vacuous only during the early stage of training when the network has
not started "exploiting" its nonlinearity yet (Appendix B). However, the minimal negative ReLU
slope for which the bound stays non-vacuous is much smaller, 0.6.
3 Related work
Non-vacuous generalization bounds. While first generalization bounds date back to Vapnik & Chervo-
nenkis (1971), the bounds which are non-vacuous for realistically large neural nets appeared quite recently.
Specifically, Dziugaite & Roy (2017) constructed a PAC-Bayesian bound which was non-vacuous for small
fully-connected networks trained on MNIST and FashionMNIST. The bound of Zhou et al. (2019), also of
PAC-Bayesian nature, relies on quantization and compression after training. It ends up being non-vacuous for
VGG-like nets trained on large datasets of ImageNet scale. The bound of Biggs & Guedj (2022) is the first
non-vacuous bound that applies directly to the learned model and not to its proxy. It is not obvious whether
their construction could be generalized to neural nets with more than two layers. The bound of Galanti et al.
(2023) is not PAC-Bayesian in contrast to the previous three. It is based on the notion of effective depth: it
assumes that a properly trained network has small effective depth, even if it is deep. Therefore its effective
capacity is smaller than its total capacity, which allows for a non-vacuous bound. See the previous section for
discussion of some of the features of these bounds.
A priori and a posteriori generalization bounds. Most of the generalization bounds available in
the literature are a posteriori . That is, some PAC-Bayesian bounds (Dziugaite & Roy, 2017; Neyshabur
et al., 2018; Biggs & Guedj, 2022), as well as Rademacher complexity-based bounds (Bartlett et al., 2017),
depend on norms of learned weights, or distances between the learned weights and their initializations, so
they cannot be computed before these norms or distances are known. The bound of Zhou et al. (2019)
depends on the trained model size after compression and quantization, which cannot be predicted before
training and compression. That is, this bound implicitly relies on the fact that the training procedure prefers
well-compressible models; to the best of our knowledge, this fact does not have a solid explanation yet. The
bound of Galanti et al. (2023) depends on a so-called effective depth of a learned model, which also cannot be
predicted in advance.
The only a priori bounds we are aware of are the classical uniform bounds based on VC-dimension and
Rademacher complexity, see Vapnik & Chervonenkis (1971). These bounds grow with model expressivity,
hence with both width and depth, which make them vacuous in most of the practical scenarios involving
neural nets. The reason they are vacuous is also related to the fact that among all models that interpolate
the data, one can often find a model that does not generalize well, see empirical results of Zhang et al. (2021).
Uniform bounds bound the generalization gap in the worst case, hence if a "bad" model exists in our model
class, the whole generalization bound has no chance to be good.
Overall, generalization bounds that use some complexity of the whole function class fall short due to the
trade-off they impose: low complexity =provably good generalization, high complexity = maybe poor
generalization. Modern neural architectures are deep and wide, hence they have high complexity, while they
still generalize well. The reason is that the training procedure we often use is likely to be implicitly biased
towards well-generalizing solutions. If we bound the generalization gap uniformly over the whole function
class, we ignore this effect. The a posteriori bounds we discuss above do take some form of implicit bias into
account (i.e. low weight norm, compressibility etc.). So does our bound: we exploit the fact that a nearly
linear network is close to a proxy which uses only weights of a linear network. In their turn, the trained
weights of a linear network depend only on a few parameters as long as their initialization is given. The
3Published in Transactions on Machine Learning Research (12/2024)
difference is that our bound does not need any prior info about the learned weights (only on their initialization
and the optimization procedure); hence it is a priori.
Linear networks training dynamics. The pioneering work that integrates the training dynamics of a
linear network under gradient flow to optimize square loss is Saxe et al. (2013). This work crucially assumes
that the initial weights are well aligned with the eigenvectors of the optimal linear regression weights YX+,
where (X,Y )is the train dataset. As the initialization norm approaches zero, the learning process becomes
more sequential: components of the data are learned one by one starting from the strongest. Li et al. (2021)
conjecture that the same happens for any initialization approaching zero excluding some set of directions of
measure zero. They prove this result for the first, the strongest, component, but moving further seems more
challenging. See also Yun et al. (2021); Jacot et al. (2021). We note a recent result of Tu et al. (2024) who
propose a so-called mixeddynamics for two-layered linear networks that does not suffer from this issue.
Nearly-linear neural nets. Our generalization gap bound decreases as activation functions get closer
to linearity. While nearly-linear activations do not conform with the usual practice, nearly-linear networks
have been studied before. That is, Li et al. (2022) demonstrated that when width nand depth Lgo to
infinity with constant ratio, the hidden layer covariances at initialization admit a meaningful limit as long as
the ReLU slopes behave as 1±c√n, i.e. become closer and closer to linearity. Noci et al. (2024) explored a
similar limit for Transformers (Vaswani et al., 2017). Another example is Kumar et al. (2024), who used a
toy small-ϵmodel to explain the phenomenon of grokking (Power et al., 2022) as delayed feature learning.
Linearized training of neural networks and NTK. It is important to emphasize that linear networks
we consider in the present work are fundamentally different from the NTK model introduced by Jacot et al.
(2018) resulted from linearized training of a neural network. That is, by a linear network we mean a network
fθ(x)who is linear in its input, x, but not necessarily in its weights θ. Whereas if we linearize the training
procedure, the trained model becomes linear in θ, but not in x. Jacot et al. (2018) demonstrated that
training a neural network becomes equivalent to training a kernel method under specific (non-standard)
parameterization as width goes to infinity (NTK limit). Since training a kernel method is equivalent to
training a random feature model with sufficiently large number of features, the corresponding training
procedure is indeed linear in weights. See also Yang & Littwin (2021) for proving NTK limit for general
architectures, and Chizat et al. (2019) for demonstrating linearized training for large weight initializations.
As in the NTK limit training a neural net becomes equivalent to training a kernel method, features the model
uses stay the same over the whole process of training. In other words, the model does not enjoy feature
learning when training is linearized. In contrast, even a linear network with two layers trained with gradient
descent does demonstrate feature learning: see Saxe et al. (2013); Tu et al. (2024). The feature learning it
exhibits is kernel alignment and a so-called momentum effect : the associated kernel (NTK), or equivalently,
the features the model uses, aligns over strong components of the data and extends along them Tu et al.
(2024). This way, these strong components get learned first, before weak components, often resulted from
data noise.
We also emphasize that the proxy models we use to prove our results, while exploiting weights of a trained
linear network, are not linear themselves; neither in x, nor in weights. Therefore they do not suffer from
expressivity issues as linear networks (which are all functionally equivalent to linear models x→Wx), while
they do enjoy feature learning (since linear networks do).
Therefore our approach is quite orthogonal to NTK and generalizability of kernel methods. In terms of
high-level methodology, the closest paper we could mention is Arora et al. (2019), where they consider a
two-layer NTK-parameterized MLP, and prove a generalization bound for it when width is sufficiently large.
For this, they combine a generalization bound for the infinite-width limit when the kernel is constant, and a
term that takes into account the fact that the kernel for finite width deviates from the limit one. Whereas
what we do is we combine a generalization bound for a proxy model that exploits the weights of a linear
network and a proxy deviation bound. The bound of Arora et al. (2019) becomes good when the width
is large (hence the finite-width NTK is close to the limit NTK), while our bound becomes good when the
activation functions are close to be linear (hence the proxy does not deviate from the original model much).
4Published in Transactions on Machine Learning Research (12/2024)
4 Our approach
Before formally stating our main bound, we present the high-level approach for constructing generalization
bounds we take in our work.
Generalization bound based on proxy models. Letfbe a model trained on a binary classification
task. We will look for a proxy model gthat satisfies the following properties: (1) it is close enough to f, and
(2) the generalization gap for gcould be bounded well enough. Given such g, we bound the distribution risk
offas follows:
R[f]≤ˆRγ[f] +/parenleftig
RC
γ[g]−ˆRC
γ[g]/parenrightig
+1
γEx∼D|f(x)−g(x)|+1
γ1
mm/summationdisplay
k=1|f(xk)−g(xk)|, (1)
whereγ >0and we used three different risk functions:
1. Misclassification risk: r(y,ˆy) = [yˆy<0],
2. Margin risk: rγ(y,ˆy) = [yˆy<γ ],
3. Continuous margin risk: rC
γ(y,ˆy) = [yˆy<0] +/parenleftig
1−yˆy
γ/parenrightig
[yˆy∈[0,γ]],
giving rise to the distribution risk R[f] =Ex,y∼Dr(y,f(x)), and similarly Rγ[f]andRC
γ[f], and its empirical
counterpart ˆR[f] =1
m/summationtextm
k=1r(yk,f(xk)), and similarly ˆRγ[f]and ˆRC
γ[f]. HereDis the data distribution,
and(xk,yk)m
k=1is the training dataset samled from Din an iid manner.
We derived Equation (1) simply from the fact that rC
γ(·,y)is1/γ-Lipschitz, and r≤rC
γ≤rγ, see Equation (22)
below for a more elaborate derivation. The first term on the right hand side of Equation (1) is the empirical
margin risk of the original model f. The second term is the generalization gap of the proxy-model g(with
respect to continuous margin risk) which we assume to be easy to bound. The two remaining terms are
deviations of gfromfaveraged over the whole data distribution Dand the dataset, divided by γ. The
parameterγcontrols the trade-off between the first term and the other three: the first term increases with γ
(eventually reaching 1), while the other three decrease.
Proxy models for a nearly-linear network. The problem now boils down to finding a good proxy gthat
approximates the trained model fwell, and whose generalization gap could be well-bounded. Let fϵ
θ(x)be a
neural network with parameters θ, inputx∈Rd, scalar output, and activation functions ϕϵ(z) =z+ϵψ(z),
ψis a positively homogeneous map (e.g. ReLU). That is, f0
θ(x)is linear inx(but not necessarily in θ), andϵ
characterizes the deviation from linearity.
Letfϵ
θϵbe the trained model whose generalization gap we would like to bound, and let f0
θ0be a linear
model trained the same way. We consider the following proxies: g1(x) =fϵ
θ0(x)andg2(x) =g1(x) +
(fϵ
θϵ(X)−g1(X))X+x, whereX∈Rd×mis a matrix with rows (xk)m
k=1, andX+is its Moore-Penrose
pseudo-inverse.
In words, the first proxy is a nonlinear network that uses weights of the trained linear one, while the second
proxy results from linearly correcting the first one on the train dataset. These proxies deviate from the
original model as follows: |gκ(x)−fϵ
θϵ(x)|=O(ϵκ)forκ∈{1,2}and any given x∈Rd.
Under certain assumptions, θ0, the trained linear model weights, depend only on YX+∈Rd, whereY∈R1×d
is a row-vector of targets. Since g1is fully described by θ0, the class of models realizable by it has only d
parameters. Since g2is merelyg1plus a linear correction, its respective class of models has only d+d= 2d
parameters. In both cases, the number of parameters the proxy has is much smaller than the total number of
weights of the network, which is (d+ 1)n+ (L−2)n2. Then classical uniform bound techniques (Vapnik
& Chervonenkis, 1971) result in a generalization bound for proxy models that grows as/radicalig
κd
m. This bound
depends neither on width of the network, nor on its depth.
5Published in Transactions on Machine Learning Research (12/2024)
5 Main result
Notations. For integer L≥0,[L]denotes the set{1,...,L}. For integers l≤L,[l,L]denotes the set
{l,...,L}. For a vector x,∥x∥denotes its Euclidean norm, while ∥x∥1denotes its l1-norm. For a matrix X,
∥X∥denotes its maximal singular value, while ∥X∥Fdenotes its Frobenius norm.
5.1 Setup
Model. The model we study is a fully-connected LeakyReLU network with Llayers:
fϵ
θ(x) =WLxϵ
θ,L−1(x), xϵ
θ,0(x) =x, xϵ
θ,l(x) =ϕϵ/parenleftbig
Wlxϵ
θ,l−1(x)/parenrightbig
∀l∈[L−1], (2)
whereθ∈RNdenotes the vector of all weights, i.e. θ=cat({vec(Wl)}L
l=1),Wl∈Rnl×nl−1∀l∈[L], andϕϵis
a Leaky ReLU with a negative slope 1−ϵ, that is:
ϕϵ(x) =x−ϵmin(0,x). (3)
Since we are going to consider only binary classification in the present work, we take nL= 1. We also define
d=n0to denote the input dimension.
Data.Data points (x,y)come from a distribution D. We assume all xfromDto lie on a unit ball, ∥x∥≤1,
andy∈{− 1,1}. During the training phase, we sample a set of mpoints iid fromDto form a dataset (X,Y ),
whereX∈Rd×mandY∈R1×m.
Training. Assuming rkX=d(which implies m≥d, i.e. the data is abundant), we train our model on
(X,Y )with gradient flow to optimize square loss on whitened data, i.e. on (˜X,Y )for˜X= Σ−1/2
XX, where
ΣX=1
mXX⊤∈Rd×dis an empirical feature correlation matrix. That is,
dWϵ
l(t)
dt=−∂/parenleftbigg
1
2m/vextenddouble/vextenddouble/vextenddoubleY−fϵ
θϵ(t)(˜X)/vextenddouble/vextenddouble/vextenddouble2
F/parenrightbigg
∂Wl∀l∈[L]. (4)
Note that ˜X˜X⊤=mId.
Inference. To conform with the above training procedure, we take the model output at a point xto be
fϵ
θ/parenleftbig
Σ−1/2x/parenrightbig
, where Σis a (distribution) feature correlation matrix: Σ =E(x,y)∼D(xx⊤)∈Rd×d. We assume
this matrix to be known; in practice, we could substitute it with ΣX.
Performance measure. Recall the definitions of r,rγ, andrC
γfrom Section 4. Slightly abusing the
notation, we define the empirical (train) risk on the dataset (X,Y)and the distribution risk of the model
trained for time tas
ˆRϵ(t) =1
mm/summationdisplay
k=1r/parenleftig
yk,fϵ
θϵ(t)(Σ−1/2xk)/parenrightig
, Rϵ(t) =E(x,y)∼Dr/parenleftig
y,fϵ
θϵ(t)(Σ−1/2x)/parenrightig
. (5)
We defineRϵ
γ(t)andRC,ϵ
γ(t)analogously to Rϵ(t), and ˆRϵ
γ(t)and ˆRC,ϵ
γ(t)analogously to ˆRϵ(t).
5.2 Generalization bound
We will need the following assumption on the training process:
Assumption 5.1. ∀t≥0/vextenddouble/vextenddouble/vextenddouble/parenleftig
Y−fϵ
θϵ(t)(˜X)/parenrightig
˜X⊤/vextenddouble/vextenddouble/vextenddouble2
F≤/vextenddouble/vextenddouble/vextenddouble/parenleftig
Y−fϵ
θϵ(0)(˜X)/parenrightig
˜X⊤/vextenddouble/vextenddouble/vextenddouble2
F.
Note that/vextenddouble/vextenddouble/vextenddoubleY−fϵ
θϵ(t)(˜X)/vextenddouble/vextenddouble/vextenddouble2
F≤/vextenddouble/vextenddouble/vextenddoubleY−fϵ
θϵ(0)(˜X)/vextenddouble/vextenddouble/vextenddouble2
Fsince we minimize the loss monotonically with gradient
flow. This implies that the above assumption holds automatically, whenever ˜Xis a (scaled) orthogonal matrix
6Published in Transactions on Machine Learning Research (12/2024)
(which happens when m=d). It is easy to show that it provably holds for a linear network ( ϵ= 0), see
Appendix E, and we found this assumption to hold empirically for all of our experiments with nonlinear
networks too, see Figure 7.
We are now ready to formulate our main result:
Theorem 5.2. Fixβ,γ > 0,t≥0,δ∈(0,1),ϵ∈[0,1], andκ∈{1,2}. Letpbe the floating point arithmetic
precision (32 by default). Under the setting of Section 5.1 and Assumption 5.1, for any weight initialization
satisfying∥Wϵ
l(0)∥≤β∀l∈[L], w.p.≥1−δover sampling the dataset (X,Y ),
Rϵ(t)≤ˆRϵ
γ(t) + Υκ+∆κ,β(t)ϵκ
γ, (6)
for the terms in the rhs defined as
Υκ=/radicalbigg
κpdln 2 + ln(1/δ)
2m, ∆κ,β(t) = Φκvβ(t)uL−1
β(t), (7)
where
Φκ=/braceleftigg
L√
d+ 1 + (L−1)ρ, κ = 1;
(L−1)/bracketleftig
(L+ 1 + (L−1)ρ)√
d+ 2(1 + (L−1)ρ)/bracketrightig
, κ = 2,(8)
whereρ=∥Wϵ
1(0)∥F
∥Wϵ
1(0)∥is the square root of the stable rank of the input layer at initialization, and the definitions
ofuβandvβare given below.
Define
•s=/vextenddouble/vextenddouble/vextenddoubleYX⊤Σ−1/2
X/vextenddouble/vextenddouble/vextenddouble;¯1 = 1 +ρβL;
•For a given r≥0,¯sr= (1−ϵ)(s+ρβL) +ϵ√r(L−1)(1 +ρβL);
•¯s= ¯s1; ˆs=L
1+(L−1)ρ¯s;¯β=β¯sρ
¯s1.
We have
uβ(t) =/braceleftigg¯βe¯st, L = 2;
/parenleftbig¯β2−L−(L−2)¯st/parenrightbig1
2−L, L≥3;(9)
vβ(t) =L−1
Lˆs2−L
LuL−1
β(t)[w(uβ(t))−w(β)]euL
β(t)
ˆs, (10)
where1
w(u) =−¯1
¯sΓ/parenleftbigg2−L
L,uL
ˆs/parenrightbigg
−ρˆs
¯sΓ/parenleftbigg2
L,uL
ˆs/parenrightbigg
. (11)
This theorem gives an a prioribound for the generalization gap Rϵ−ˆRϵ
γ, i.e. it could be computed without
performing the actual training.
Dimension dependency. Our bound does not depend on width nl∀l∈[L−1], in contrast to the bounds
of Dziugaite & Roy (2017); Zhou et al. (2019); Biggs & Guedj (2022). However, both penalty terms of
Theorem 5.2, Υκand∆β,κ(t), grow as√
dwith the input dimension.
1Γis an upper-incomplete gamma-function defined as Γ(s,x) =/integraltext∞
xts−1e−tdt.
7Published in Transactions on Machine Learning Research (12/2024)
5.3 Choosing the training time
As we see, ∆κ,β(t)diverges super-exponentially as t→∞forL= 2and ast→¯β2−L
(L−2)¯sforL≥3, so the
bound eventually becomes vacuous. For the bound to make sense, we should be able to find tsmall enough
for∆κ,β(t)to stay not too large, and at the same time, large enough for the training risk ˆRϵ
γ(t)to get
considerably reduced.
In the present section, we are going to demonstrate that for values of twhich correspond to partially learning
the dataset (i.e. for which ˆRϵ
γ(t)∈(0,1)), the last term of the bound, ∆κ,β(t)/γ, admits a finite limit as
β→0.2
We do not know how ˆRϵ
γ(t)decreases with tin our case. However, when the network is linear, this can be
computed explicitly when either the weight initialization is properly aligned with the data, or the initialization
normβvanishes. We are going to perform our whole analysis for L= 2in the main, and defer the case L≥3
to Appendix D.2.
That is, consider an SVD: Y˜X+=1√mYX⊤(XX⊤)−1/2=PSQ⊤, wherePandQare orthogonal and S
is diagonal; note that S11=s. Observe also that s=∥Y˜X+∥≤∥Y∥∥˜X+∥≤1sincey=±1. Saxe et al.
(2013)3have demonstrated for linear nets ( ϵ= 0) andL= 2that when the weight initialization is properly
aligned with PandQ4,∥W0
1(t)∥=∥W0
2(t)∥= ¯u(t), where ¯usatisfies5
d¯u(t)
dt= ¯u(t)(s−¯u2(t)), ¯u(0) =β, (12)
which gives the solution in implicit form:
t=/integraldisplayd¯u
¯u(s−¯u2)=1
2sln/parenleftbigg¯u2(t)(s−β2)
β2(s−¯u2(t))/parenrightbigg
. (13)
One could resolve ¯uexplicitly to get
¯u(t) =se2st
e2st−1 +s/β2. (14)
Saxe et al. (2013) observed this expression to hold with good precision for random (not aligned) initializations
whenβis small enough. This is supported by further works (Li et al., 2021; Jacot et al., 2021): when a linear
network is initialized close to the origin and the initial weights do not lie on a "bad" subspace, the gradient
flow nearly follows the same trajectory as studied by Saxe et al. (2013).
Plugging ¯u2(t) =αsinto Equation (13), we get the time required for a linear network to learn a fraction αof
the strongest mode:
t∗
α(β) =1
2sln/parenleftbiggα(s−β2)
(1−α)β2/parenrightbigg
. (15)
Clearly, the learning time t∗
α(β)diverges whenever β→0, orα→1.
Sincet∗
α(β)is the time sufficient to learn a network for ϵ= 0, we suppose it also suffices to learn a nonlinear
network. Thus, we are going to evaluate our bound at t=t∗
α. Since we need ˆRγ(t∗
α)<1for the bound to be
non-vacuous, we should take γsmall relative to α. We consider γ=αν/qforν,q≥1.
Since the linear network learning time t∗
α(β)is correct for almost all initialization only when βvanishes, we
are going to work in the limit of β→0. Since we need α∈(β2/s,1), otherwise the linear training time is
negative, we take α=r
sβλforλ∈(0,2]andr>1.
2In the linear case ( ϵ= 0), this corresponds to the saddle-to-saddle regime of Jacot et al. (2021).
3Saxe et al. (2013) assumed XX⊤=Idand∥YX⊤∥=s, while not introducing the factor1
mas we do in Equation (4). It is
easy to see that the our gradient flow has exactly the same dynamics as the one studied by Saxe et al. (2013).
4That is, when W2(0) =P¯S1/2
2R⊤,W1(0) =R¯S1/2
1Q⊤, whereRis orthogonal, and ¯S1and ¯S2are constructed from Sby
adding or removing zero rows and columns.
5Our ¯ucorresponds to u1/(Nl−1)of Saxe et al. (2013).
8Published in Transactions on Machine Learning Research (12/2024)
Let us compute the quantities which appear in our bounds, at t=t∗
α(β):
u=¯β/parenleftbiggα(s−β2)
(1−α)β2/parenrightbigg¯s
2s
=¯sρ
¯s1r¯s
2sβ1+¯s
s(λ
2−1)(1 +O(β)), (16)
where we omitted the argument t∗
α(β)for brevity.
Since Γ(0,x) =−Ei(−x) =−lnx+Ox→0(1)andΓ(1,x) =e−x=Ox→0(1), we havew(β) =¯1
¯sln(β2) +O(1).
Similarly, whenever 1 +¯s
s/parenleftbigλ
2−1/parenrightbig
>0, we haveu=o(β), andw(u) =¯1
¯sln/parenleftig
β2+¯s
s(λ−2)/parenrightig
+O(1).
Consider first λ<2and suppose ν= 1. In this case, w(u)−w(β) =¯1
s/parenleftbigλ
2−1/parenrightbig
ln(β2) +O(1), and
2uv
γ=¯s2
ρ
¯s2
1qr¯s
s−1¯1
β(¯s
s−1)(2−λ)/parenleftbiggλ
2−1/parenrightbigg
ln(β2)(1 +O(1/lnβ)). (17)
Since ¯s≥s, this expression diverges as β→0. Note that we get the same order divergence even also for
1 +¯s
s/parenleftbigλ
2−1/parenrightbig
≤0. Clearly, for ν >1, we get even faster divergence.
On the other hand, if λ= 2thenw(u)−w(β) =¯1
¯sln/parenleftig¯s2
ρ
¯s2
1r¯s
s/parenrightig
+O(β), and
2uv
γ=sν¯s2
ρ
¯s3
1qr¯s
s−ν¯1 ln/parenleftigg
¯s2
ρ
¯s2
1r¯s
s/parenrightigg
β2(1−ν)(1 +O(β)). (18)
We get a finite limit only for ν= 1, i.e. when γ∝α. In this case, the last term of the bound Equation (6)
becomes
∆κ,βϵκ
γ=¯1s¯s2
ρ
2¯s3
1qr¯s
s−1Λκln/parenleftigg
¯s2
ρ
¯s2
1r¯s
s/parenrightigg
ϵκ(1 +O(β)). (19)
Summing up, we expect the empirical risk ˆRγ(t∗
α)to be smaller than1
2for large enough q(i.e. for small
γcompared to α), and the last term to stay finite as β→0and vanish as ϵκ. Therefore as long as Υκis
not large enough (i.e. when κpdis small compared to m), the overall bound becomes non-vacuous for small
enoughϵat least at the (linear) training time t∗
α. We are going to evaluate our bound empirically in the
upcoming section.
6 Experiments
Setup. We consider an L-layer bias-free fully-connected network of width 64 and train it to classify 0-4
versus 5-9 digits of MNIST (i.e. m= 60000). In order to approximate the gradient flow dynamics, we run
gradient descent with learning rate 0.001. By default, we take L= 2, the floating point precision to be
p= 32, downsample the images to 7x7, and initialize the layers randomly in a standard Pytorch way (plus, we
rescale the weights to match the required layer norm β). For some experiments, we consider deeper networks,
half-precision p= 16, downsample not so aggresively, or enforce the input layer weight matrix to have rank 1.
Observations:
•When we take κ= 2andβ≤0.1, the bound becomes nonvacuous up to ϵ= 0.001(Figure 1) and
even up to ϵ= 0.01if we enforce ρ= 1(Figure 3);
•We can have a non-vacuous bound also for 14x14 MNIST if we take ϵ= 0.001, half-precision ( p= 16),
and enforce ρ= 1(Figure 5); the bound slightly exceeds the random guess risk for the full-sized,
28x28 MNIST;
•The bound for κ= 2is much stronger than that for κ= 1(Figure 2);
9Published in Transactions on Machine Learning Research (12/2024)
Figure 1: We consider 7x7 binary MNIST, L= 2,κ= 2,ϵ= 0.001, and varyβ. The bound of Theorem 5.2
converges as βvanishes and increases as βgrows. The bound stays non-vacuous for a small enough βand a
properly choosen γ. We consider γ=β2/qforq∈{1,10,100}.
Figure 2: We consider 7x7 binary MNIST, L= 2,β= 0.001,ϵ= 0.001, and compare different kappas of
Theorem 5.2. The bound for κ= 2is much stronger than that for κ= 1.
Figure 3: We consider 7x7 binary MNIST, L= 2,β= 0.001,ϵ= 0.01,κ= 2, and vary the stable rank at
initialization ρand floating point precision p. Initializing the input layer with a rank one matrix considerably
improves the bound. Moreover, it also improves the convergence speed.
10Published in Transactions on Machine Learning Research (12/2024)
Figure 4: We consider 7x7 binary MNIST, L= 2,β= 0.001,ϵ= 0.01,κ= 2, and compare different
components of the bound. The left figure corresponds to the full bound, while for the central one we forget
about the generalization gap bound for the proxy model Υκ, and for the rightmost one, we forget about the
deviation term∆κ,βϵκ
γ. We see that both terms are of the same order; one therefore has to work on reducing
both in order to reduce the overall bound.
Figure 5: We consider binary MNIST, L= 2,β= 0.001,ϵ= 0.001,κ= 2,p= 16, rank one input layer
initialization, and vary image dimensions. In this "gentle" scenario, the bound stays non-vacuous for 14x14
MNIST, and only slightly exceeds the random guess risk for the full-sized, 28x28 MNIST.
Figure 6: We consider binary 7x7 MNIST, β= 0.001,ϵ= 0.001,κ= 2,p= 16, rank one input layer
initialization, and vary depth. Even in this "gentle" scenario, the bound gets considerably worse with depth.
11Published in Transactions on Machine Learning Research (12/2024)
Figure 7: We consider binary MNIST, L= 2,β= 0.001,ϵ= 0.001, and measure the differences L(0)−L(t)
(solid lines) andLX(0)−LX(t)(dashed lines). Here L(t) =1
2m/vextenddouble/vextenddouble/vextenddoubleY−fϵ
θϵ(t)(˜X)/vextenddouble/vextenddouble/vextenddouble2
Fis the loss at time t, while
LX(t) =1
2m/vextenddouble/vextenddouble/vextenddouble/parenleftig
Y−fϵ
θϵ(t)(˜X)/parenrightig
˜X⊤/vextenddouble/vextenddouble/vextenddouble2
Fis the "projected" loss at time t. WhileL(t)should clearly decrease for
gradient descent with small enough steps, it is not a priori clear thatLX(t)also does. As we see from the
plots, it does for βlarge and small, and for ϵup to 1, which corresponds to conventional ReLU activations.
These results validate our Assumption 5.1. Note that we added a small quantity 10−8in order to make zero
visible.
•The bound improves and converges as βvanishes (Figure 1);
•Assumption 5.1 holds empirically (Figure 7);
•The bound improves if we enforce ρ= 1; this also results in faster convergence (Figure 3);
•The bound slightly improves for the half floating point precision (Figure 3);
•Forϵ= 0.01at the training time, all components of our bound are of the same order, while for larger
ϵ, the last one starts to dominate (Figure 4);
•The bound deteriorates consideraly when we increase depth (Figure 6).
7 Proof of the main result
7.1 Proof of Theorem 5.2 for κ= 1
We start with approximating our learned network fϵ
θϵwithfϵ
θ0, i.e. with a nonlinear network that uses the
weights learned by the linear one. This approximation deviates from the original network the following way:
∀x∈Rd,ϵ∈[0,1],
1
ϵ∥fϵ
θϵ(x)−fϵ
θ0(x)∥=1
ϵ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplayϵ
0∂fϵ
θτ(x)
∂τdτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤sup
τ∈[0,ϵ]/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂fϵ
θτ(x)
∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤sup
τ∈[0,ϵ]L/summationdisplay
l=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Wτ
l
∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/productdisplay
k̸=l∥Wτ
k∥∥x∥(20)
since we use Leaky ReLUs. We omitted the argument tfor brevity. We get a similar deviation bound on the
training dataset:
1
ϵ/vextenddouble/vextenddoublefϵ
θϵ(˜X)−fϵ
θ0(˜X)/vextenddouble/vextenddouble≤sup
τ∈[0,ϵ]/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Wτ
1˜X
∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
FL/productdisplay
k=2∥Wτ
k∥+ sup
τ∈[0,ϵ]L/summationdisplay
l=2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Wτ
l
∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble∥Wτ
1˜X∥F/productdisplay
k∈[2:L]\{l}∥Wτ
k∥.(21)
We complete the above deviation bound with bounding weight norms and norms of weight derivatives:
Lemma 7.1. Under Assumption 5.1, ∀τ∈[0,1],t≥01√m∥Wτ
1(t)˜X∥F≤ρu(t),1√m/vextenddouble/vextenddouble/vextenddouble∂Wτ
1˜X
∂τ(t)/vextenddouble/vextenddouble/vextenddouble
F≤v(t),
and∀l∈[L]∥Wτ
l(t)∥≤u(t),/vextenddouble/vextenddouble/vextenddouble∂Wτ
l
∂τ(t)/vextenddouble/vextenddouble/vextenddouble≤v(t)foru,vdefined in Theorem 5.2.
12Published in Transactions on Machine Learning Research (12/2024)
See Section 7.3 and Appendix D.1 for the proof.
Now we can relate the risk of the original model with the risk of the approximation. Since r≤rC
γ≤rγand
rC
γ(·,y)is1/γ-Lipschitz for any fixed y,
R(fϵ
θϵ)−ˆRγ(fϵ
θϵ)≤RC
γ(fϵ
θϵ)−ˆRC
γ(fϵ
θϵ)
≤RC
γ(fϵ
θ0)−ˆRC
γ(fϵ
θ0) +1
γE∥fϵ
θϵ(˜x)−fϵ
θ0(˜x)∥+1
γm/vextenddouble/vextenddoublefϵ
θϵ(˜X)−fϵ
θ0(˜X)/vextenddouble/vextenddouble
1,(22)
where the expectation is over the data distribution D, and ˜x= Σ−1/2xis the actual input of the network.
As for the last term, the deviation on the train dataset, we use that ∥z∥1≤√m∥z∥for anyz∈Rm, and
Equation (21). As for the deviation on the test dataset, due to Equation (20) and Lemma 7.1, in order to
bound the last term, it suffices to bound E∥˜x∥. Since Σ =E[xx⊤], we get
E∥Σ−1/2x∥2=E/bracketleftbig
x⊤Σ−1x/bracketrightbig
=Etr/bracketleftbig
xx⊤Σ−1/bracketrightbig
= tr[Id] =d. (23)
This gives E∥Σ−1/2x∥≤/radicalbig
E∥Σ−1/2x∥2≤√
d.
The first two terms is a generalization gap of the proxy-model. We use a simple counting-based bound:
RC
γ/parenleftig
fϵ
θ0(t)/parenrightig
−ˆRC
γ/parenleftig
fϵ
θ0(t)/parenrightig
≤/radicalbigg
ln|Fϵ
1(t;θ(0))|−lnδ
2m, (24)
w.p.≥1−δover sampling the dataset (X,Y ), whereFϵ
1(t;θ(0))denotes the set of functions representable
withfϵ
θ0(t)for a given initial weights θ(0), whereθ0(t)is a result of running the gradient flow Equation (4)
for timet. As long as we work with finite precision, this class is finite:
Lemma 7.2.∀θ(0),ϵ,t≥0,|Fϵ
1(t;θ(0))|≤2pd.
Proof.Since we run our gradient flow Equation (4) on whitened data to optimize squared loss, the initial
weights are fixed, and the network we train is linear, the resulting weights depend only on Y˜X⊤which hasd
parameters. Since each function in our class is completely defined with the resulting weights, and each weight
occupiespbits, we get the above class size.
We could have used a classical VC-dimension-based bound instead (Vapnik & Chervonenkis, 1971). However,
we found it to be numerically larger compared to the counting-based bound above.
This finalizes the proof of Theorem 5.2 for κ= 1.
7.2 Proof of Theorem 5.2 for κ= 2
What changes for κ= 2is a proxy-model. Consider the following:
˜fϵ
θ0,θϵ(x) =fϵ
θ0(x) +/parenleftbig
fϵ
θϵ(˜X)−fϵ
θ0(˜X)/parenrightbig˜X+x. (25)
That is, we take the same proxy-model as before, but we add a linear correction term. This correction term
aims to fit the proxy model to the original one, fϵ
θϵ, on the training dataset ˜X. We prove the following lemma
in Appendix C.1:
Lemma 7.3. Under the premise of Lemma 7.1, ∀t≥0∀ϵ∈[0,1]∀x∈Rdwe have
/vextenddouble/vextenddouble/vextenddoublefϵ
θϵ(t)(x)−˜fϵ
θ0(t),θϵ(t)(x)/vextenddouble/vextenddouble/vextenddouble≤(L−1)(L+ 1 +ρ(L−1))uL−1(t)v(t)∥x∥ϵ2; (26)
/vextenddouble/vextenddouble/vextenddoublefϵ
θϵ(t)(˜X)−˜fϵ
θ0(t),θϵ(t)(˜X)/vextenddouble/vextenddouble/vextenddouble≤2(L−1)(1 +ρ(L−1))uL−1(t)v(t)√mϵ2. (27)
13Published in Transactions on Machine Learning Research (12/2024)
The deviation now scales as ϵ2instead ofϵ.
What remains is to bound the size of Fϵ
2(t;θ(0)), which denotes the set of functions representable with our
new proxy-model ˜fϵ
θ0(t),θϵ(t)for given initial weights θ(0). Since ˜fϵ
θ0(t),θϵ(t)is a sum of fϵ
θ0(t)and a linear
model, its size is at most 2pdtimes larger:
|Fϵ
2(t;θ(0))|≤|Fϵ
1(t;θ(0))|2pd≤22pd. (28)
This finalizes the proof of Theorem 5.2 for κ= 2.
7.3 Proof of Lemma 7.1
Below, we are going to present the proof only for L= 2, and defer the case of L≥3to Appendix D.1.
7.3.1 Weight norms
Let us expand the weight evolution Equation (4):
dWτ
1
dt=/bracketleftig
Dτ⊙Wτ,⊤
2Ξτ/bracketrightig
˜X⊤,dWτ
2
dt= Ξτ/bracketleftbig
Dτ⊙Wτ
1˜X/bracketrightbig⊤, (29)
where we define
Ξτ=1
m/parenleftbig
Y−Wτ
2/bracketleftbig
Dτ⊙Wτ
1˜X/bracketrightbig/parenrightbig
, (30)
andDτ= (ϕϵ)′/parenleftbig
Wτ
1˜X/parenrightbig
is an×mmatrix with entries equal to 1or1−τ. We express it as Dτ=
(1−τ)1n×m+τ∆, where ∆is an×m0-1 matrix.
Let us bound the evolution of weight norms:
d∥Wτ
1∥
dt≤/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1
dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤(1−τ)∥Wτ
2∥/vextenddouble/vextenddoubleΞτ˜X⊤/vextenddouble/vextenddouble+τ/vextenddouble/vextenddouble/vextenddouble∆⊙Wτ,⊤
2Ξτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜X/vextenddouble/vextenddouble. (31)
Since multiplying by a 0-1 matrix elementwise does not increase Frobenius norm, we get
/vextenddouble/vextenddouble/vextenddouble∆⊙Wτ,⊤
2Ξτ/vextenddouble/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/vextenddouble∆⊙Wτ,⊤
2Ξτ/vextenddouble/vextenddouble/vextenddouble
F≤∥Wτ
2∥∥Ξτ∥F. (32)
Noting that ΞτandΞτ˜X⊤are row matrices, this results in
d∥Wτ
1∥
dt≤/parenleftbig
(1−τ)∥Ξτ˜X⊤∥+τ∥Ξτ∥∥˜X∥/parenrightbig
∥Wτ
2∥. (33)
By a similar reasoning,
d∥Wτ
2∥
dt≤(1−τ)∥Ξτ˜X⊤∥∥Wτ
1∥+τ∥Ξτ∥∥Wτ
1˜X∥F. (34)
Consider the following system of ODEs:
dg1(t)
dt= ¯s2g2(t),dg2(t)
dt=g1(t), g 1(0) =β¯sρ;g2(0) =β. (35)
We then make use of the following lemma which we prove in Appendix C.2:
Lemma 7.4.∥Ξτ∥=∥Ξτ∥F≤1√m(1 +ρβL). If we additionally take Assumption 5.1 then ∥Ξτ˜X⊤∥=
∥Ξτ˜X⊤∥F≤s+ρβL.
This lemma implies g1(t)≥(1−τ)(s+ρβ2)∥Wτ
1(t)∥+τ(1 +ρβ2)∥Wτ
1(t)˜X∥Fandg2(t)≥∥Wτ
2(t)∥.
The above system of ODEs could be solved analytically:
g1(t) =β/radicalbig
¯sρ−¯s1cosh/parenleftbigg
¯st+ tanh−1/parenleftbigg¯s1
¯sρ/parenrightbigg/parenrightbigg
, g 2(t) =/radicalig
¯β2−β2sinh/parenleftbigg
¯st+ tanh−1/parenleftbigg¯s1
¯sρ/parenrightbigg/parenrightbigg
,(36)
14Published in Transactions on Machine Learning Research (12/2024)
where ¯β=β¯sρ
¯s1. This gives the bound for the input layer weight norms:
∥Wτ
1(t)∥≤β+ ¯s/integraldisplayt
0g2(t)dt=β−¯β+/radicalig
¯β2−β2cosh/parenleftbigg
¯st+ tanh−1/parenleftbigg¯s1
¯sρ/parenrightbigg/parenrightbigg
. (37)
For further analysis, we will need simpler-looking bounds. Consider a looser bound: g2(t)≤u(t)and
g1(t)≤¯su(t), where
du(t)
dt= ¯su(t), u (0) =β¯sρ
¯s1. (38)
This ODE solves as u(t) =¯βe¯st.
So, we have∥Wτ
2(t)∥≤u(t). As for the input layer weights, we get
∥Wτ
1(t)∥≤β+ ¯s/integraldisplayt
0u(t)dt=β−¯β+u(t) (39)
Similarly,1√m∥Wτ
1(t)˜X∥F=βρ−¯β+u(t). As we do not want additive terms, we bound from above:
∥Wτ
1(t)∥≤u(t)and1√m∥Wτ
1(t)˜X∥≤ρu(t).
7.3.2 Norms of weight derivatives
In Appendix C.3, we follow the same logic to demonstrate that/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ/vextenddouble/vextenddouble/vextenddouble,1√m/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ˜X/vextenddouble/vextenddouble/vextenddouble
F, and/vextenddouble/vextenddouble/vextenddoubledWτ
2
dτ/vextenddouble/vextenddouble/vextenddoubleare all
bounded by the same vwhich satisfies
dv
dt=v/bracketleftbig
(1 +ρ)u2+ ¯s/bracketrightbig
+u/bracketleftbig¯1 +ρu2/bracketrightbig
, v(0) = 0. (40)
It is a linear ODE in v(t), andu(t)is given:u(t) =¯βe¯st. We solve it in Appendix C.4 to get v(t)from
Theorem 5.2.
8 Conclusion
We have derived a novel generalization bound for LekyReLU networks. Our bound could be evaluated
before the actual training and does not depend on network width. Our bound becomes non-vacuous for
partially-trained nets with activation functions close to being linear.
Broader Impact Statement
This is a theoretical work studying generalization in neural nets; its main result is a form of a generalization
guarantee. Having a good generalization guarantee is necessary to make the models we use in practice more
reliable. We do not foresee any negative societal or ethical impact our research could potentially cause.
References
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and
generalization for overparameterized two-layer neural networks. In International Conference on Machine
Learning , pp. 322–332, 2019.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural
networks. In Advances in Neural Information Processing Systems , pp. 6240–6249, 2017.
Felix Biggs and Benjamin Guedj. Non-vacuous generalisation bounds for shallow neural networks. In
International Conference on Machine Learning , pp. 1963–1981. PMLR, 2022.
15Published in Transactions on Machine Learning Research (12/2024)
Blake Bordelon, Alexander Atanasov, and Cengiz Pehlevan. A dynamical model of neural scaling laws. In
Forty-first International Conference on Machine Learning , 2024a. URL https://openreview.net/forum?
id=nbOY1OmtRc .
Blake Bordelon, Alexander Atanasov, and Cengiz Pehlevan. How feature learning can improve neural scaling
laws.arXiv preprint arXiv:2409.17858 , 2024b.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. Advances
in neural information processing systems , 32, 2019.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochas-
tic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008 ,
2017.
Tomer Galanti, Liane Galanti, and Ido Ben-Shaul. Comparative generalization bounds for deep neural
networks. Transactions on Machine Learning Research , 2023.
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. In Advances in neural information processing systems , pp. 8571–8580, 2018.
ArthurJacot, FrançoisGed, FranckGabriel, BerfinŞimşek, andClémentHongler. Deeplinearnetworksdynam-
ics: Low-rank biases induced by initialization scale and l2 regularization. arXiv preprint arXiv:2106.15933 ,
2021.
Tanishq Kumar, Blake Bordelon, Samuel J. Gershman, and Cengiz Pehlevan. Grokking as the transition
from lazy to rich training dynamics. In The Twelfth International Conference on Learning Representations ,
2024. URL https://openreview.net/forum?id=vt5mnLVIVo .
Mufan Li, Mihai Nica, and Dan Roy. The neural covariance sde: Shaped infinite depth-and-width networks
at initialization. Advances in Neural Information Processing Systems , 35:10795–10808, 2022.
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent for matrix
factorization: Greedy low-rank learning. In International Conference on Learning Representations , 2021.
URL https://openreview.net/forum?id=AHOs7Sm5H7R .
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-
normalized margin bounds for neural networks. In International Conference on Learning Representations ,
2018. URL https://openreview.net/forum?id=Skz_WfbCZ .
Lorenzo Noci, Chuning Li, Mufan Li, Bobby He, Thomas Hofmann, Chris J Maddison, and Dan Roy.
The shaped transformer: Attention models in the infinite depth-and-width limit. Advances in Neural
Information Processing Systems , 36, 2024.
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization
beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177 , 2022.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks. arXiv preprint arXiv:1312.6120 , 2013.
Zhenfeng Tu, Santiago Aranguri, and Arthur Jacot. Mixed dynamics in linear networks: Unifying the lazy
and active regimes. Advances in Neural Information Processing Systems , 2024.
V N Vapnik and A Ya Chervonenkis. O ravnomernoj skhodimosti chastot pojavlenija sobytiy k ih veroyatnos-
tiam.Teorija verojatnostiej i jejo primenenija , 16(2):264–279, 1971.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30,
2017.
16Published in Transactions on Machine Learning Research (12/2024)
Greg Yang and Etai Littwin. Tensor Programs IIb: Architectural universality of neural tangent kernel training
dynamics. In International Conference on Machine Learning , pp. 11762–11772. PMLR, 2021.
Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view on implicit bias in training linear neural
networks. In International Conference on Learning Representations , 2021. URL https://openreview.
net/forum?id=ZsZM-4iMQkH .
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning (still) requires rethinking generalization. Communications of the ACM , 64(3):107–115, 2021.
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P. Adams, and Peter Orbanz. Non-vacuous generalization
bounds at the imagenet scale: a PAC-bayesian compression approach. In International Conference on
Learning Representations , 2019. URL https://openreview.net/forum?id=BJgqqsAct7 .
A Discussion
A.1 Assumptions
Whitened data. Overall, the assumption on whitened data is not necessary for a result similar to
Theorem 5.2 to hold. We assume the data to be whitened for two reasons. First, it legitimates the choice
of training time t∗
α(β)since it is based on the analysis of Saxe et al. (2013), which assumed the data to be
whitened. If we dropped it, we could still evaluate the bound of Theorem 5.2 at t=t∗
α(β), but it would be
less clear whether the trainining risk ˆRγbecomes already small by this time.
Second, we had to bound ∥˜X∥throughout the proof of Theorem 5.2 and ∥˜X+∥in the proof of Lemma 7.3.
For whitened data, these are simply√mand1/√m, which is a clear dependence on m, making the final
bound look cleaner. Otherwise, they would be random variables whose dependence on mwould be less
obvious.
Third, if we considered training on the original dataset (Y,X)instead of the whitened one, (Y,˜X), we would
have to know YX⊤andXX⊤in order to determine θ0(t)for a given tand initialization θ0(0). These two
matrices have d+d(d+1)
2parameters, compared to just dforY˜X⊤. This way, Υκwould grow as dinstead of√
d.
Gradient flow. We expect our technique to follow through smoothly for finite-step gradient descent.
Introducing momentum also seems doable. However, generalizing it to other training procedures, e.g. the
ones which use normalized gradients, might pose problems since it is not clear how to bound the norm of the
elementwise ratio of two matrices reasonably.
Assumption 5.1. We use this assumption to prove the second part of Lemma 7.4. If we dropped it,
the bound would be ∥Ξτ˜X⊤∥F≤∥Ξτ∥F∥˜X⊤∥≤1 +ρβLinstead ofs+ρβL. This would result in larger
exponents in the definition uin Theorem 5.2.
As an argument in favor of this assumption, we demonstrate it empirically first (Figure 7), and we prove it
for the linear case after, see Appendix E.
A.2 Proof
We expect the bounds on weight norms u(t)to be quite loose since we use Lemma 7.4 to bound the loss.
This lemma bounds the loss with its value at the initialization, while the loss should necessarily decrease. If
we could account for the loss decrease, the resulting u(t)would increase with a lower exponent, or even stay
bounded, as ¯u(t), which corresponds to a linear model, does. This way, we would not have to assume ϵto
vanish asβvanishes in order to keep the bound non-diverging for small βat the training time t∗
α(β). Also,
the general bound of Theorem 5.2 would diverge with training time tmuch slower. We leave it for future
work.
17Published in Transactions on Machine Learning Research (12/2024)
As we see from our estimates, Υκbecomes the main bottleneck of our bound for small ϵ. The bound we used
forΥκis very naive; we believe that better bounds are possible.
Indeed, since the proxy corresponding to κ= 1depends only on θ0, the trained linear model weights which
one could obtain explicitly, this proxy, fϵ
θ0, is also given explicitly. One could try to estimate its generalization
gap explicitly, as done for kernel methods (Bordelon et al., 2024a) and for more complex models, which are
closer to realisic neural nets (Bordelon et al., 2024b). We leave this direction for future work.
A.3 Other architectures
As becomes apparent from the proof in Appendix C.1, the proxy-model for κ= 2, Equation (25), deviates
from the original model fϵ
θϵasO(ϵ2)for any map (ϵ,θ,x )→fϵ
θ(x)as long as the following holds:
1.f0
θ(x)is linear in xfor anyθ;
2.∂2fϵ
θ(x)
∂ϵ∂θis continuous as a function of (ϵ,θ,x );
3. the result of learning θϵ(t)is differentiable in ϵfor anyt.
This is directly applicable to convolutional networks with no other nonlinearities except for ReLU’s; in
partiular, without max-pooling layers. One may introduce max-poolings by interpolating between average-
poolings (which are linear) for ϵ= 0and max-poolings for ϵ= 1. This is not applicable to Transformers
(Vaswani et al., 2017) since attention layers are inherently nonlinear: queries and keys have to be multiplied.
Compared to the fully-connected case of the present work, our bound might become even tighter for
convolutional nets since dbecomes the number of color channels (up to 3) instead of the whole image size in
pixels. However, the corresponding proxy-models might be over-simplistic: the linear net they will deviate
from is just a global average-pooling followed by a linear Rd→Rmap. We leave exploring the convolutional
net case for future work.
B How far could we get with our approach?
Recall our general proxy-based generalization bound of Equation (1):
R[f]≤ˆRγ[f] +/parenleftig
RC
γ[g]−ˆRC
γ[g]/parenrightig
+1
γEx∼D|f(x)−g(x)|+1
γ1
mm/summationdisplay
k=1|f(xk)−g(xk)|. (41)
Informally, we say that performance of fis worse than that of gat most by some deviation term.
The bound ends up to be good whenever (a) the generalization gap of gcould be well-bounded, and (b) g
does not deviate from fmuch. That is why we considered proxy-models based on linear learned weights: their
generalization gap could be easily bounded analytically and they do not deviate much from corresponding
leaky ReLU nets as long as ReLU negative slopes are close to one.
The biggest conceptual disadvantage of this approach is that, given both fandglearn the training dataset,
we have no chance proving that fperforms better than g, we could only prove that fperforms not much
worsethang. Do the proxy-models we use in the present paper perform well, and how much do they deviate
from original models? Our main theoretical result, Theorem 5.2, bounds the proxy-model generalization gap
and the deviation from above. These bounds are arguably not optimal. It is therefore instructive to examine
how well the bound would perform if we could estimate Equation (41) exactly.
B.1 Empirical validation
Setup. We work under the same setup as in Section 66, but instead of evaluating the bound of Theorem 5.2,
we actually train a linear model with exactly the same procedure as for the original model, in order to get
6We also downsample MNIST images to 14x14 instead of 7x7. The reason why we do it is that on one hand, we wanted to
test our bounds on more realistic scenarios, while on the other, Xdoes not appear to be full-rank for the original 28x28 MNIST.
18Published in Transactions on Machine Learning Research (12/2024)
Figure 8: We examine the optimistic bound of Equation (41) for the proxy-models proposed in our paper:
theκ= 1one,fϵ
θ0, theκ= 2one of Equation (25), and also a linear proxy, f0
θ0. The bottom row demonstrates
the full bound (green lines), while the top one depicts the two components of the bound, namely, the proxy
model generalization gap RC
γ(g)−ˆRC
γ(g)and the proxy model deviation E|f(x)−g(x)|+ˆE|f(x)−g(x)|,
separately. Different lines of the same color (e.g. solid green and dashed black lines on the bottom row)
correspond to different values of γ. Proxy generalization gap stays low during the whole training (top left
figure), while the train risk and the model deviation over gamma contribute significantly (bottom row, two
groups of lines correspond to the minimal and the maximal γwe considered). The optimistic bound for the
1st-order proxy (bottom left) gets non-vacuous only at the moment when GF escapes the origin and reaches
the linear model loss. The bound for the 2nd-order proxy (bottom right) becomes non-vacuous soon after the
original model becomes non-vacuous (but still stays near the origin), and stays non-vacuous until the model
starts exploiting its nonlinearity to reduce the loss below the optimal linear model loss level (the last drop of
purple and black lines).
Figure 9: The optimistic bound of Equation (41) based on our κ= 2proxy stays non-vacuous up to ϵ= 0.4
until the gradient flow starts "exploiting" the nonlinearity (the last drop of purple and black lines).
19Published in Transactions on Machine Learning Research (12/2024)
trained linear weights θ0. We then evaluate the proxy-models considered in the present work: (1) the one for
κ= 1,fϵ
θ0, (2) the one for κ= 2, see Equation (25), and also (3) the linear network, f0
θ0. We then evaluate
the rhs of Equation (41) using a test part of the MNIST dataset. For this "optimistic" bound, we consider
much larger values of epsilon: ϵ≥0.1, i.e. our model much less "nearly-linear" now than before.
Figures. We present the results on Figures 8 and 9. Dashed lines correspond to the train set, while solid
ones correspond to the test set. Black lines are risks of the actual trained model fϵ
θϵ(t):R(t)and ˆRγ(t),
respectively. Green lines are our "optimistic" bound Equation (41) evaluated at different values of γ. Purple
lines denote MSE loss of the actual trained model.
We also put three baselines on the plots. The dotted black line is the classification risk (and the MSE
loss) of a zero model f≡0. The brown dashed line is the MSE (train) loss of the optimal linear model,
f:x→Y˜X+x. Finally, the red dashed line is the (train) classification risk of the optimal linear model.
Training phases. As we observe on risk plots (Figure 9 and the bottom row of Figure 8), the training
process could be divided into three phases. During the first phase, the risk decreases until it reaches the risk
of the optimal linear model, while the loss stays at the level of f≡0. This indicates that while the weights
stay very close to the origin, the network outputs already align with the outputs of the optimal linear model.
During the second phase, both the loss and the risk stay at the level of the optimal linear model. As for the
following phase, both the risk and the loss drop below the optimal linear model level. Therefore from this
point on, the network starts to "exploit" its nonlinearity in order to reduce the train loss.
Observations:
•The generalization gap stays negligible for all models and γ’s considered (Figure 8, top left);
•The proxy-model for κ= 2approximates the original model best among all three proxies considered
(Figure 8, top right);
•While the linear approximation deviates from the original model more than the one for κ= 2during
the first phase, their deviations are similar during the subsequent phases;
•At the same time, the κ= 1approximation deviates more than that of κ= 2during the second
phase;
•The transition between the first and the second phases results in a nonmonotonic behavior of the
deviation from the original model for κ= 1and linear proxy-models;
•The resulting optimistic bound for κ= 2(green lines of Figure 8, bottom right) stays non-vacuous
during the first two phases for ϵ= 0.1, while this is not the case for κ= 1(green lines of Figure 8,
bottom left);
•The optimistic bound for κ= 2stays non-vacuous up to ϵ= 0.4(green lines of Figure 9).
It is tempting to assume that the weights θϵfollow the same trajectory as the weights of the linear model, θ0,
during the first two phases. However, if it was the case, the κ= 1proxy-model, fϵ
θ0, would coincide with the
original one, fϵ
θϵ, during this period. Then their quality would be the same; however, Figure 8, top right,
demonstrates the opposite.
C Missing calculations in Section 7
C.1 Proof of Lemma 7.3
We have:
1
ϵ2/vextenddouble/vextenddoublefϵ
θϵ(x)−˜fϵ
θ0,θϵ(x)/vextenddouble/vextenddouble=1
ϵ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplayϵ
0/parenleftbigg∂fϵ
θτ(x)
∂τ−∂fϵ
θτ(˜X)˜X+x
∂τ/parenrightbigg
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤1
ϵsup
τ∈[0,ϵ]/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂fϵ
θτ(x)
∂τ−∂fϵ
θτ(˜X)
∂τ˜X+x/vextenddouble/vextenddouble/vextenddouble/vextenddouble.
(42)
20Published in Transactions on Machine Learning Research (12/2024)
Sincef0
θτis a linear network and rk˜X=d, we havef0
θτ(x) =f0
θτ(˜X)˜X+xand∂f0
θτ(x)
∂τ=∂f0
θτ(˜X)˜X+x
∂τ. This
implies
1
ϵ/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂fϵ
θτ(x)
∂τ−∂fϵ
θτ(˜X)
∂τ˜X+x/vextenddouble/vextenddouble/vextenddouble/vextenddouble=1
ϵ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplayϵ
0/parenleftbigg∂2fρ
θτ(x)
∂ρ∂τ−∂2fρ
θτ(˜X)˜X+x
∂ρ∂τ/parenrightbigg
dρ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤sup
ρ∈[0,ϵ]/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2fρ
θτ(x)
∂ρ∂τ−∂2fρ
θτ(˜X)
∂ρ∂τ˜X+x/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤sup
ρ∈[0,ϵ]/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2fρ
θτ(x)
∂ρ∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble+ sup
ρ∈[0,ϵ]/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2fρ
θτ(˜X)
∂ρ∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble∥˜X+x∥.(43)
Since we use LeakyReLU,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2fρ
θτ(x)
∂ρ∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤(L−1)L/summationdisplay
l=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Wτ
l
∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/productdisplay
k̸=l∥Wτ
k∥∥x∥; (44)
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2fρ
θτ(˜X)
∂ρ∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤(L−1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Wτ
1˜X
∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/productdisplay
k∈[2,L]∥Wτ
k∥+ (L−1)∥Wτ
1˜X∥FL/summationdisplay
l=2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Wτ
l
∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/productdisplay
k∈[2,L]\{l}∥Wτ
k∥.(45)
Finally, since ˜X˜X⊤=mId, we have∥˜X+∥=1√m. Combining everything together, we arrive into
/vextenddouble/vextenddouble/vextenddoublefϵ
θϵ(x)−˜fϵ
θ0,θϵ(x)/vextenddouble/vextenddouble/vextenddouble
(L−1)ϵ2∥x∥≤sup
τ∈[0,ϵ]
L/summationdisplay
l=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Wτ
l
∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/productdisplay
k̸=l∥Wτ
k∥

+ sup
τ∈[0,ϵ]
1√m∥Wτ
1˜X∥FL/summationdisplay
l=2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Wτ
l
∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/productdisplay
k∈[2,L]\{l}∥Wτ
k∥

+ sup
τ∈[0,ϵ]
1√m/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Wτ
1˜X
∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/productdisplay
k∈[2,L]∥Wτ
k∥
.(46)
Plugging the bounds from Lemma 7.1 then gives
/vextenddouble/vextenddoublefϵ
θϵ(x)−˜fϵ
θ0,θϵ(x)/vextenddouble/vextenddouble≤(L−1)(L+ 1 + (L−1)ρ)uL−1v∥x∥ϵ2. (47)
By a similar reasoning,
/vextenddouble/vextenddouble/vextenddoublefϵ
θϵ(˜X)−˜fϵ
θ0,θϵ(˜X)/vextenddouble/vextenddouble/vextenddouble
(L−1)ϵ2≤2 sup
τ∈[0,ϵ]
∥Wτ
1˜X∥FL/summationdisplay
l=2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Wτ
l
∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/productdisplay
k∈[2,L]\{l}∥Wτ
k∥

+ 2 sup
τ∈[0,ϵ]
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂Wτ
1˜X
∂τ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/productdisplay
k∈[2,L]∥Wτ
k∥
.(48)
Plugging the same bounds,
/vextenddouble/vextenddoublefϵ
θϵ(˜X)−˜fϵ
θ0,θϵ(˜X)/vextenddouble/vextenddouble≤2(L−1)(1 + (L−1)ρ)uL−1vϵ2√m. (49)
C.2 Proof of Lemma 7.4
Recall the definition of Ξτ:
Ξτ(t) =1
m/parenleftbig
Y−Wτ
L(t)/bracketleftbig
Dτ
L−1(t)⊙Wτ
L−1(t)/bracketleftbig
...Wτ
2(t)/bracketleftbig
Dτ
1(t)⊙Wτ
1(t)˜X/bracketrightbig/bracketrightbig/bracketrightbig/parenrightbig
. (50)
21Published in Transactions on Machine Learning Research (12/2024)
Sincem∥Ξτ∥2
Fis the loss function we optimize, it does not increase under gradient flow. Hence, since
multiplying by Dτ
lelementwise does not increase Frobenius norm,
m∥Ξτ(t)∥F≤m∥Ξτ(0)∥F≤∥Y∥F+∥Wτ
1(0)˜X∥FL/productdisplay
l=2∥Wτ
l(0)∥. (51)
We have∥Wτ
1(0)˜X∥F=ρ∥Wτ
1(0)˜X∥≤∥Wτ
1(0)∥∥˜X∥. Since allyfrom suppDare±1and ˜X˜X⊤=mId, we
get∥Ξτ(t)∥F=1√m(1 +ρβL).
Due to Assumption 5.1, m∥Ξτ˜X⊤∥2
Fdoes not increase under gradient flow:
m∥Ξτ(t)˜X⊤∥F≤m∥Ξτ(0)˜X⊤∥F≤∥Y˜X⊤∥F+∥˜X∥∥Wτ
1(0)˜X∥FL/productdisplay
l=2∥Wτ
l(0)∥. (52)
SinceY˜X+=s, we haveY˜X⊤=ms. Therefore∥Ξτ(t)˜X⊤∥F=s+ρβL.
Finally, since Ξτ∈R1×m,∥Ξτ∥=∥Ξτ∥Fand∥Ξτ˜X⊤∥=∥Ξτ˜X⊤∥F.
C.3 Bounding derivatives in the proof of Lemma 7.1
Let us start with Wτ
1:
d2Wτ
1
dtdτ=/bracketleftigg
Dτ⊙/parenleftigg
dWτ,⊤
2
dτΞτ+Wτ,⊤
2dΞτ
dτ/parenrightigg/bracketrightigg
˜X⊤−/bracketleftig
¯∆⊙Wτ,⊤
2Ξτ/bracketrightig
˜X⊤; (53)
/vextenddouble/vextenddouble/vextenddouble/vextenddoubled2Wτ
1
dtdτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
2
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbig
(1−τ)∥Ξτ˜X⊤∥+τ∥Ξτ∥F∥˜X∥/parenrightbig
+∥Wτ
2∥/parenleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubledΞτ
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F+∥Ξτ∥F/parenrightbigg
∥˜X∥.(54)
We need a bound for/vextenddouble/vextenddoubledΞτ
dτ/vextenddouble/vextenddouble:
mdΞτ
dτ=Wτ
2/bracketleftbig¯∆⊙Wτ
1˜X/bracketrightbig
−Wτ
2/bracketleftbigg
Dτ⊙dWτ
1
dτ˜X/bracketrightbigg
−dWτ
2
dτ/bracketleftbig
Dτ⊙Wτ
1˜X/bracketrightbig
; (55)
m/vextenddouble/vextenddouble/vextenddouble/vextenddoubledΞτ
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤∥Wτ
2∥∥Wτ
1˜X∥F+∥Wτ
2∥/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ˜X/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F+/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
2
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble∥Wτ
1˜X∥F
≤u2ρ√m+u/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ˜X/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F+uρ√m/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
2
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble.(56)
This results in
d
dt/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/vextenddouble/vextenddoubled2Wτ
1
dtdτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤u/parenleftbigg
1 +ρβ2+ρu2+u1√m/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ˜X/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/parenrightbigg
+/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
2
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble(s+ (1−s)τ+ρβ2+ρu2).
(57)
Similarly, we have an evolution of Wτ
1˜X:
d
dt/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ˜X/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
2
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbig
(1−τ)∥Ξτ˜X⊤∥F∥˜X∥+τ∥Ξτ∥F∥˜X⊤˜X∥/parenrightbig
+∥Wτ
2∥/parenleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubledΞτ
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F+∥Ξτ∥F/parenrightbigg
∥˜X⊤˜X∥
≤u/parenleftbigg
1 +ρβ2+ρu2+u1√m/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ˜X/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/parenrightbigg√m+/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
2
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble(s+ (1−s)τ+ρβ2+ρu2)√m.
(58)
Finally, consider Wτ
2:
d2Wτ
2
dtdτ=dΞτ
dτ/bracketleftbig
Dτ⊙Wτ
1˜X/bracketrightbig⊤+ Ξτ/bracketleftbigg
Dτ⊙dWτ
1
dτ˜X−¯∆⊙Wτ
1˜X/bracketrightbigg⊤
; (59)
22Published in Transactions on Machine Learning Research (12/2024)
/vextenddouble/vextenddouble/vextenddouble/vextenddoubled2Wτ
2
dtdτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbig
(1−τ)∥Ξτ˜X⊤∥+τ∥Ξτ∥F∥˜X∥/parenrightbig
+/parenleftbigg
∥Ξτ∥+/vextenddouble/vextenddouble/vextenddouble/vextenddoubledΞτ
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenrightbigg
∥Wτ
1˜X∥F;(60)
d
dt/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
2
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/vextenddouble/vextenddoubled2Wτ
2
dtdτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤u/parenleftbigg
1 +ρβ2+ρu2+u1√m/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ˜X/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/parenrightbigg
+/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble(s+ (1−s)τ+ρβ2) +ρu2/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
2
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble.(61)
We see that/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ/vextenddouble/vextenddouble/vextenddouble,1√m/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ˜X/vextenddouble/vextenddouble/vextenddouble
F, and/vextenddouble/vextenddouble/vextenddoubledWτ
2
dτ/vextenddouble/vextenddouble/vextenddoubleare all bounded by the same vwhich satisfies
dv(t)
dt=v(t)(¯s+ (1 +ρ)u2(t)) +u(t)(¯1 +ρu2(t)), v (0) = 0, (62)
where ¯s=s+ (1−s)τ+ρβ2and¯1 = 1 +ρβ2.
C.4 Solving the ODE for v(t)
Recallu(t) =¯βe¯st. We solve the homogeneous equation to get
v(t) =C(t)e¯st+1+ρ
2¯s¯β2e2¯st=C(t)e(L−1) lnu(t)+1+(L−1)ρ
¯sLuL(t)=C(t)¯β−1u(t)e1+ρ
2¯su2(t), (63)
whereC(t)satisfies
dC(t)
dtu(t)e1+ρ
2¯su2(t)=¯βu(t)/bracketleftbig¯1 +ρu2(t)/bracketrightbig
. (64)
Recall forL= 2,ˆs=2
1+ρ¯s. Then
C(t) =¯β/integraldisplay
e−u2(t)
ˆs/bracketleftbig¯1 +ρu2(t)/bracketrightbig
dt
=¯β
2/bracketleftbigg¯1
¯s/parenleftbigg
Ei/parenleftbigg
−u2(t)
ˆs/parenrightbigg
−Ei/parenleftbigg
−¯β2
ˆs/parenrightbigg/parenrightbigg
−ρˆs
¯s/parenleftbigg
e−u2(t)
ˆs−e−¯β2
ˆs/parenrightbigg/bracketrightbigg
.(65)
This gives the final solution:
v(t) =1
2u(t)[w(u(t))−w(β)]eu2(t)
ˆs, (66)
where we took
w(u) =¯1
¯sEi/parenleftbigg
−u2(t)
ˆs/parenrightbigg
−2ρ
1 +ρe−u2(t)
ˆs. (67)
D Deep networks
D.1 Proof of Lemma 7.1 for L≥3
Bounding weight norms. Forl∈[2,L],
dWτ
l
dt=/bracketleftig
Dτ
l⊙Wτ,⊤
l+1.../bracketleftig
Dτ
L−1⊙Wτ,⊤
LΞτ/bracketrightig/bracketrightig/bracketleftbig
Dτ
l−1⊙Wτ
l−1.../bracketleftbig
Dτ
1⊙Wτ
1˜X/bracketrightbig/bracketrightbig⊤; (68)
/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
l
dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤/parenleftbig
(1−τ)∥Ξτ˜X⊤∥∥Wτ
1∥+τ(L−1)∥Ξτ∥F∥Wτ
1˜X∥F/parenrightbig/productdisplay
k∈[2,L]\{l}∥Wτ
k∥. (69)
Forl= 1,
dWτ
1
dt=/bracketleftig
Dτ
1⊙Wτ,⊤
2.../bracketleftig
Dτ
L−1⊙Wτ,⊤
LΞτ/bracketrightig/bracketrightig
˜X⊤; (70)
23Published in Transactions on Machine Learning Research (12/2024)
/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1
dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤/parenleftbig
(1−τ)∥Ξτ˜X⊤∥+τ(L−1)∥Ξτ∥F∥˜X∥/parenrightbig/productdisplay
k∈[2,L]∥Wτ
k∥; (71)
/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1˜X
dt/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F≤/parenleftbig
(1−τ)∥Ξτ˜X⊤∥F∥˜X∥+τ(L−1)∥Ξτ∥F∥˜X⊤˜X∥/parenrightbig/productdisplay
k∈[2,L]∥Wτ
k∥. (72)
Using Lemma 7.4, we get (1−τ)∥Ξτ˜X⊤∥∥Wτ
1∥+τ(L−1)∥Ξτ∥∥Wτ
1˜X∥F≤g1(t)and∥Wτ
l∥≤g2(t)∀l∈[2 :L],
where
dg1(t)
dt= ¯s2gL−1
2(t),dg2(t)
dt=g1(t)gL−2
2(t), g 1(0) =β((1−τ)(s+βL)+τ(L−1)(1+βL)ρ), g 2(0) =β.
(73)
We have the following first integral:
d
dt/parenleftbig
g2
1(t)−¯s2g2
2(t)/parenrightbig
= 0, g2
1(0)−¯s2g2
2(0) =β2/parenleftbig
((1−τ)(s+βL) +τ(L−1)(1 +βL)ρ)2−¯s2/parenrightbig
.(74)
Therefore
dg2(t)
dt=gL−2
2(t)/radicalig
¯s2g2
2(t)−¯s2g2
2(0) +g2
1(0), g 2(0) =β. (75)
Supposeρ>1. Theng2
1(0)−¯s2g2
2(0)>0and the solution is given in the following implicit form:
t=g3−L
2(t)2F1/parenleftig
1
2,3−L
2,5−L
2,−¯s2g2
2(t)
g2
1(0)−¯s2β2/parenrightig
−β3−L
2F1/parenleftig
1
2,3−L
2,5−L
2,−¯s2β2
g2
1(0)−¯s2β2/parenrightig
(3−L)/radicalbig
g2
1(0)−¯s2β2. (76)
The above expression cannot be made explicit for general L(but we could get explicit expression for
L∈{2,3,4}). As an alternative, we consider a looser bound u(t):
du(t)
dt= ¯s1uL−1(t), u (0) =β¯sρ
¯s1, (77)
where ¯sρ:= (1−τ)(s+βL) +τ(L−1)(1 +βL)ρ; note that ¯s1=¯s. We haveu(t)≥g2(t)and¯su(t)≥g1(t)
∀t≥0. This ODE solves as
u(t) =/parenleftbig
¯s(2−L)t+¯β2−L/parenrightbig1
2−L, (78)
where ¯β=β¯sρ
¯s1. Note that the solution exists only for t<¯β2−L
(L−2)¯s.
For the input layer weights, we get
d∥Wτ
1(t)∥
dt≤¯suL−1(t). (79)
The solution is given by
∥Wτ
1(t)∥≤β+ ¯s/integraldisplayt
0/parenleftbig
¯s(2−L)t+¯β2−L/parenrightbigL−1
2−Ldt=β−¯β+/parenleftbig
¯s(2−L)t+¯β2−L/parenrightbig1
2−L=β−¯β+u(t).(80)
Similarly, we have
1√m∥Wτ
1(t)˜X∥F≤βρ+ ¯s/integraldisplayt
0/parenleftbig
¯s(2−L)t+¯β2−L/parenrightbigL−1
2−Ldt=βρ−¯β+u(t). (81)
For brevity, we define
b=β−¯β=β/parenleftbigg
1−¯sρ
¯s1/parenrightbigg
=−βτ(L−1)(ρ−1)1 +βL
¯s, (82)
and
bρ=βρ−¯β=β/parenleftbigg
ρ−¯sρ
¯s1/parenrightbigg
=β(1−τ)(ρ−1)s+βL
¯s. (83)
As a simpler option, we could just say ∥Wτ
1(t)∥≤u(t)and1√m∥Wτ
1(t)˜X∥≤ρu(t).
24Published in Transactions on Machine Learning Research (12/2024)
Bounding norms of weight derivatives. Recall the definition of Ξτ:
Ξτ=1
m/parenleftbig
Y−Wτ
L/bracketleftbig
Dτ
L−1⊙Wτ
L−1.../bracketleftbig
Dτ
1⊙Wτ
1˜X/bracketrightbig/bracketrightbig/parenrightbig
; (84)
m/vextenddouble/vextenddouble/vextenddouble/vextenddoubledΞτ
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤(L−1)∥Wτ
1˜X∥FL/productdisplay
l=2∥Wτ
l∥+/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1˜X
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddoubleL/productdisplay
l=2∥Wτ
l∥+∥Wτ
1˜X∥FL/summationdisplay
k=2/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
k
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/productdisplay
l∈[2:L]\{k}∥Wτ
l∥
≤uL−1/bracketleftigg
(L−1)√ρmu +/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1˜X
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F+√ρmL/summationdisplay
k=2/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
k
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketrightigg
.
(85)
Forl∈[2,L],
/vextenddouble/vextenddouble/vextenddouble/vextenddoubled2Wτ
l
dtdτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤/parenleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubledΞτ
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F+ (L−1)∥Ξτ∥F/parenrightbigg
∥Wτ
1˜X∥F/productdisplay
k∈[2,L]\{l}∥Wτ
k∥
+/summationdisplay
j∈[2:L]\{l}/parenleftbig
(1−τ)∥Ξτ˜X⊤∥∥Wτ
1∥+τ(L−1)∥Ξτ∥F∥Wτ
1˜X∥F/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
j
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/productdisplay
k∈[2,L]\{l,j}∥Wτ
k∥
+/parenleftbigg
(1−τ)∥Ξτ˜X⊤∥/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble+τ(L−1)∥Ξτ∥F/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1˜X
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/parenrightbigg/productdisplay
k∈[2,L]\{l}∥Wτ
k∥
≤/parenleftigg
uL−1/bracketleftigg
(L−1)ρu+1√m/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1˜X
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F+ρL/summationdisplay
k=2/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
k
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketrightigg
+¯1(L−1)/parenrightigg
ρuL−1
+ ¯suL−2/summationdisplay
j∈[2:L]\{l}/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
j
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble+/parenleftbigg
(1−τ)(s+βL)/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble+τ¯1(L−1)1√m/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1˜X
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/parenrightbigg
uL−2.(86)
Forl= 1,
/vextenddouble/vextenddouble/vextenddouble/vextenddoubled2Wτ
1
dtdτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤/parenleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddoubledΞτ
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F+ (L−1)∥Ξτ∥F/parenrightbigg
∥˜X∥/productdisplay
k∈[2,L]∥Wτ
k∥
+L/summationdisplay
j=2/parenleftbig
(1−τ)∥Ξτ˜X⊤∥+τ(L−1)∥Ξτ∥F∥˜X∥/parenrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
j
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/productdisplay
k∈[2,L]\{j}∥Wτ
k∥
≤/parenleftigg
uL−1/bracketleftigg
(L−1)ρu+1√m/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
1˜X
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F+ρL/summationdisplay
k=2/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
k
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketrightigg
+¯1(L−1)/parenrightigg
uL−1+ ¯suL−2L/summationdisplay
j=2/vextenddouble/vextenddouble/vextenddouble/vextenddoubledWτ
j
dτ/vextenddouble/vextenddouble/vextenddouble/vextenddouble.
(87)
/vextenddouble/vextenddouble/vextenddoubledWτ
l
dτ/vextenddouble/vextenddouble/vextenddouble∀l∈[L], as well as1√m/vextenddouble/vextenddouble/vextenddoubledWτ
1˜X
dτ/vextenddouble/vextenddouble/vextenddouble
F, are all bounded by v(t)which satisfies
dv
dt=/bracketleftbig
(L−1)ρuL+ (1 + (L−1)ρ)uL−1v+ (L−1)¯1/bracketrightbig
uL−1+ ¯s(L−1)uL−2v
=v/bracketleftbig
(1 + (L−1)ρ)u2L−2+ ¯s(L−1)uL−2/bracketrightbig
+ (L−1)uL−1/bracketleftbig¯1 +ρuL/bracketrightbig
, v (0) = 0,(88)
where ¯1 = 1 +βL.
Solving the ODE for v(t). Recall
u(t) =/parenleftbig
¯s(2−L)t+¯β2−L/parenrightbig1
2−L. (89)
We solve the homogeneous equation to get
v(t) =C(t)eL−1
2−Lln(¯s(2−L)t+¯β2−L)+1+(L−1)ρ
¯sL (¯s(2−L)t+¯β2−L)L
2−L
=C(t)e(L−1) lnu(t)+1+(L−1)ρ
¯sLuL(t)=C(t)uL−1(t)e1+(L−1)ρ
¯sLuL(t),(90)
25Published in Transactions on Machine Learning Research (12/2024)
whereC(t)satisfies
dC(t)
dtuL−1(t)e1+(L−1)ρ
¯sLuL(t)= (L−1)uL−1(t)/bracketleftbig¯1 +ρuL(t)/bracketrightbig
. (91)
Let us introduce ˆs=L
1+(L−1)ρ¯s. Then
C(t) = (L−1)/integraldisplay
e−uL(t)
ˆs/bracketleftbig¯1 +ρuL(t)/bracketrightbig
dt
=L−1
Lˆs2−L
L/bracketleftbigg¯1
¯s/parenleftbigg
Γ/parenleftbigg2−L
L,βL
ˆs/parenrightbigg
−Γ/parenleftbigg2−L
L,uL(t)
ˆs/parenrightbigg/parenrightbigg
+ρˆs
¯s/parenleftbigg
Γ/parenleftbigg2
L,βL
ˆs/parenrightbigg
−Γ/parenleftbigg2
L,uL(t)
ˆs/parenrightbigg/parenrightbigg/bracketrightbigg
.
(92)
This gives the final solution:
v(t) =L−1
Lˆs2−L
LuL−1(t)[w(u(t))−w(β)]euL(t)
ˆs, (93)
where we took
w(u) =−¯1
¯sΓ/parenleftbigg2−L
L,uL
ˆs/parenrightbigg
−Lρ
1 + (L−1)ρΓ/parenleftbigg2
L,uL
ˆs/parenrightbigg
. (94)
D.2 Evaluating the solution at the learning time
For a properly initialized linear network, ∀l∈[L]∥W0
l(t)∥= ¯u(t), whereu(t)satisfies (Saxe et al., 2013)
d¯u(t)
dt= ¯uL−1(t)(s−¯uL(t)), ¯u(0) =β, (95)
which gives the solution in implicit form7:
t∗
α(β) =/integraldisplayd¯u
¯uL−1(s−¯uL)=¯u2−L(t)
s(2−L)2F1/parenleftbigg
1,2
L−1,2
L;¯uL(t)
s/parenrightbigg
−β2−L
s(2−L)2F1/parenleftbigg
1,2
L−1,2
L;βL
s/parenrightbigg
.(96)
Suppose we are going to learn a fixed fraction of the data, i.e. take ¯u(t) = (αs)1/Lforα∈(0,1). Then
t∗
α(β) =β2−L2F1/parenleftig
1,2
L−1,2
L;βL
s/parenrightig
−(αs)2
L−12F1/parenleftbig
1,2
L−1,2
L;α/parenrightbig
s(L−2). (97)
Sincet∗
α(β)is the time sufficient to learn a network for ϵ= 0, we suppose it also suffices to learn a nonlinear
network. So, we are going to evaluate our bound at t=t∗
α. Since we need ˆRγ(t∗
α)<1for the bound to be
non-vacuous, we should take γsmall relative to α. We consider γ=αν/qforν,q≥1.
Since the linear network learning time t∗
α(β)is correct for almost all initialization only when βvanishes, we
are going to work in the limit of β→0. Since we need α∈(βL/s,1), otherwise the linear training time is
negative, we take α=r
sβλforλ∈(0,L]andr>1.
Consider first λ<L:
t∗
α(β) =β2−L
s(L−2)+O(β2λ/L). (98)
Let us evaluate uat this time:
u(t∗
α(β)) =/parenleftbigg
¯β2−L−¯s
sβ2−L+O(β2λ/L)/parenrightbigg1
2−L
=β/parenleftigg
¯s2−L
ρ
¯s2−L
1−¯s
s/parenrightigg1
2−L/parenleftig
1 +O/parenleftig
βL−2+2λ/L/parenrightig/parenrightig
.(99)
72F1is a hypergeometric function defined as a series 2F1(a,b,c,z ) = 1+/summationtext∞
k=1(a)k(b)k
(c)kzk
k!, where (q)k=q(q+1)...(q+k−1).
26Published in Transactions on Machine Learning Research (12/2024)
Apparently, this expression does not make sense for ρ= 1and even close to it, so we switch to λ=L, which
we expect to be the right exponent:
t∗
α(β) =β2−L1−r2
L−1
s(L−2)+O(β2). (100)
Let us evaluate uat this time:
u(t∗
α(β)) =/parenleftbigg
¯β2−L−¯s
s/parenleftig
1−r2
L−1/parenrightig
β2−L+O(β2)/parenrightbigg1
2−L
=β/parenleftigg
¯s2−L
ρ
¯s2−L
1−¯s
s/parenleftig
1−r2
L−1/parenrightig/parenrightigg1
2−L/parenleftbig
1 +O/parenleftbig
βL/parenrightbig/parenrightbig
.
(101)
This expression makes sense whenever¯s2−L
ρ
¯s2−L
1−¯s
s/parenleftig
1−r2
L−1/parenrightig
>0, i.e. when ris close enough to 1.
Let us evaluate wat the training time now. Since Γ(a,x) = Γ(a)−xa
a+O(xa+1), we get
w(u(t∗
α(β)))−w(β) =¯1
¯sL
2−LˆsL−2
L/parenleftbig
u2−L(t∗
α(β))−β2−L/parenrightbig
+O(β2).
=¯1
¯sL
2−LˆsL−2
Lβ2−L/parenleftigg
¯s2−L
ρ
¯s2−L
1−¯s
s/parenleftig
1−r2
L−1/parenrightig
−1/parenrightigg
+O(β2).(102)
Then the quantity of interest becomes
LuL−1(t∗
α(β))v(t∗
α(β))
γ=L−1
γu2L−2(t∗
α(β))¯1
¯sL
2−Lβ2−L/parenleftigg
¯s2−L
ρ
¯s2−L
1−¯s
s/parenleftig
1−r2
L−1/parenrightig
−1/parenrightigg
(1 +O(βL))
=qsν
rν¯1
¯sL(L−1)
2−LβL(1−ν)/parenleftigg
¯s2−L
ρ
¯s2−L
1−¯s
s/parenleftig
1−r2
L−1/parenrightig/parenrightigg2L−2
2−L/parenleftigg
¯s2−L
ρ
¯s2−L
1−¯s
s/parenleftig
1−r2
L−1/parenrightig
−1/parenrightigg
(1 +O(βL)).
(103)
This expression does not diverge as β→0whenν= 1. We will also have a finite limL→∞limβ→0whenever
ϵ∝(L−1)−1.
E Proving Assumption 5.1 for linear nets
We have for l= 1,
∇1:=1
2m∂/vextenddouble/vextenddoubleY−fϵ
θϵ(˜X)/vextenddouble/vextenddouble2
F
∂Wϵ
1=−/bracketleftig
Dϵ
1⊙Wϵ,⊤
2.../bracketleftig
Dϵ
L−1⊙Wϵ,⊤
LΞϵ/bracketrightig/bracketrightig
˜X⊤. (104)
Forl∈[2,L],
∇l:=1
2m∂/vextenddouble/vextenddoubleY−fϵ
θϵ(˜X)/vextenddouble/vextenddouble2
F
∂Wϵ
l=−/bracketleftig
Dϵ
l⊙Wϵ,⊤
l+1.../bracketleftig
Dϵ
L−1⊙Wϵ,⊤
LΞϵ/bracketrightig/bracketrightig/bracketleftbig
Dϵ
l−1⊙Wϵ
l−1.../bracketleftbig
Dϵ
1⊙Wϵ
1˜X/bracketrightbig/bracketrightbig⊤.
(105)
We also have
∇X
1:=1
2m∂/vextenddouble/vextenddouble/parenleftbig
Y−fϵ
θϵ(˜X)/parenrightbig˜X⊤/vextenddouble/vextenddouble2
F
∂Wϵ
1=−/bracketleftig
Dϵ
1⊙Wϵ,⊤
2.../bracketleftig
Dϵ
L−1⊙Wϵ,⊤
LΞϵ˜X⊤˜X/bracketrightig/bracketrightig
˜X⊤.(106)
∇X
l:=1
2m∂/vextenddouble/vextenddouble/parenleftbig
Y−fϵ
θϵ(˜X)/parenrightbig˜X⊤/vextenddouble/vextenddouble2
F
∂Wϵ
l
=−/bracketleftig
Dϵ
l⊙Wϵ,⊤
l+1.../bracketleftig
Dϵ
L−1⊙Wϵ,⊤
LΞϵ˜X⊤˜X/bracketrightig/bracketrightig/bracketleftbig
Dϵ
l−1⊙Wϵ
l−1.../bracketleftbig
Dϵ
1⊙Wϵ
1˜X/bracketrightbig/bracketrightbig⊤.(107)
The statement of Assumption 5.1 follows from
27Published in Transactions on Machine Learning Research (12/2024)
Conjecture E.1. ∀ϵ∈[0,1]∀t≥0/summationtextL
l=1tr/bracketleftbig
∇X
l∇⊤
l/bracketrightbig
≥0.
Indeed, the above conjecture states that loss gradients wrt weights and "projected" loss gradients wrt weights
are positively aligned, so, whenever loss does not increase, neither does projected loss. Since we use gradient
flow, loss is guaranteed to not increase. Below, we prove the conjecture for linear nets.
Proof of Conjecture E.1 for ϵ= 0.Sinceϵis zero, we omit the corresponding sup-index:
tr/bracketleftbig
∇X
1∇⊤
1/bracketrightbig
= tr/bracketleftig/bracketleftbig
W⊤
2...W⊤
LΞ˜X⊤˜X/bracketrightbig˜X⊤˜X/bracketleftbig
W⊤
2...W⊤
LΞ/bracketrightbig⊤/bracketrightig
= tr/bracketleftig/bracketleftbig
W⊤
2...W⊤
LΞ˜X⊤/bracketrightbig/bracketleftbig
W⊤
2...W⊤
LΞ˜X⊤/bracketrightbig⊤/bracketrightig
= tr/bracketleftbig
WL...W 2W⊤
2...W⊤
L/bracketrightbig
tr/bracketleftbig
ΞX⊤XΞ⊤/bracketrightbig
(108)
by the circular property of trace, and the fact that Ξis a matrix with a single row. Since dout= 1, both
traces are just squared Euclidean norms of vectors, hence they are non-negative: tr/bracketleftbig
∇X
1∇⊤
1/bracketrightbig
≥0.
Let us do the same for the other layers:
tr/bracketleftbig
∇X
l∇⊤
l/bracketrightbig
= tr/bracketleftig/bracketleftbig
W⊤
l+1...W⊤
LΞ˜X⊤˜X/bracketrightbig/bracketleftbig
Wl−1...W 1˜X/bracketrightbig⊤/bracketleftbig
Wl−1...W 1˜X/bracketrightbig/bracketleftbig
W⊤
l+1...W⊤
LΞ/bracketrightbig⊤/bracketrightig
= tr/bracketleftig/bracketleftbig
W⊤
l+1...W⊤
LΞ˜X⊤W⊤
1...W⊤
l−1/bracketrightbig/bracketleftbig
W⊤
l+1...W⊤
LΞ˜X⊤W⊤
1...W⊤
l−1/bracketrightbig⊤/bracketrightig
= tr/bracketleftbig
WL...Wl+1W⊤
l+1...W⊤
L/bracketrightbig
tr/bracketleftbig
ΞX⊤W⊤
1...W⊤
l−1Wl−1...W 1XΞ⊤/bracketrightbig
≥0(109)
for the same reasons as before.
Clearly, Conjecture E.1 should also hold for small enough ϵwhenever it holds for ϵ= 0. However, the bound
for the maximal ϵfor which we were able to guarantee the conjecture statement, vanishes with time t, as our
weight bounds are too loose. For this reason, we do not include it here. See Section 6 for empirical validation
of Assumption 5.1.
28