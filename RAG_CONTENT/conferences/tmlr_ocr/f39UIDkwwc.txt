Published in Transactions on Machine Learning Research (07/2023)
Contrastive Attraction and Contrastive Repulsion for
Representation Learning
Huangjie Zheng⋆huangjie.zheng@utexas.edu
Department of Statistics and Data Science
The University of Texas at Austin
Xu Chen⋆xuchen2016@sjtu.edu.cn
Shanghai Jiao Tong University
Alibaba Group
Jiangchao Yao sunaker@sjtu.edu.cn
Cooperative Medianet Innovation Center, Shanghai Jiao Tong University
Shanghai AI Laboratory
Hongxia Yang hongxia.yang1@gmail.com
Shanghai Institute for Advanced Study of Zhejiang University (SIAS)
Chunyuan Li chunyuan.li@microsoft.com
Microsoft Research, Redmond
Ya Zhang ya_zhang@sjtu.edu.cn
Cooperative Medianet Innovation Center, Shanghai Jiao Tong University
Shanghai AI Laboratory
Hao Zhang zhanghao01@xidian.edu.cn
Xidian University
Ivor Tsang ivor_tsang@cfar.a-star.edu.sg
A⋆STAR Centre for Frontier AI Research (CFAR)
Jingren Zhou jingren.zhou@alibaba-inc.com
Alibaba Group
Mingyuan Zhou mingyuan.zhou@mccombs.utexas.edu
McCombs School of Business
The University of Texas at Austin
Reviewed on OpenReview: https: // openreview. net/ forum? id= f39UIDkwwc
Abstract
Contrastive learning (CL) methods eﬀectively learn data representations in a self-supervision
manner, where the encoder contrasts each positive sample over multiple negative samples via
a one-vs-many softmax cross-entropy loss. By leveraging large amounts of unlabeled image
data, recent CL methods have achieved promising results when pretrained on large-scale
datasets, such as ImageNet. However, most of them consider the augmented views from
the same instance are positive pairs, while views from other instances are negative ones.
Such binary partition insuﬃciently considers the relation between samples and tends to
yield worse performance when generalized on images in the wild. In this paper, to further
improve the performance of CL and enhance its robustness on various datasets, we propose a
⋆The ﬁrst two authors share equal contribution
1Published in Transactions on Machine Learning Research (07/2023)
doubly CL strategy that separately compares positive and negative samples within their own
groups, and then proceeds with a contrast between positive and negative groups. We realize
this strategy with contrastive attraction and contrastive repulsion (CACR), which makes
the query not only exert a greater force to attract more distant positive samples but also
do so to repel closer negative samples. Theoretical analysis reveals that CACR generalizes
CL’s behavior by positive attraction and negative repulsion, and it further considers the
intra-contrastive relation within the positive and negative pairs to narrow the gap between the
sampled and true distribution, which is important when datasets are less curated. With our
extensive experiments, CACR not only demonstrates good performance on CL benchmarks,
but also shows better robustness when generalized on imbalanced image datasets. Code and
pre-trained checkpoints are available at https://github.com/JegZheng/CACR-SSL .
1 Introduction
The conventional Contrastive Learning (CL) loss (Oord et al., 2018; Poole et al., 2018) has achieved remarkable
success in representation learning, beneﬁting downstream tasks in a variety of areas (Misra & Maaten, 2020;
He et al., 2020; Chen et al., 2020a; Fang & Xie, 2020; Giorgi et al., 2020). This loss typically appears in
a one-vs-many softmax form to make the encoder distinguish the positive sample within multiple negative
samples. In image representation learning, this scheme is widely used to encourage the encoder to learn
representations that are invariant to unnecessary details in the representation space, for which the unit
hypersphere is the most common assumption (Wang et al., 2017; Davidson et al., 2018; Hjelm et al., 2018;
Tian et al., 2019; Bachman et al., 2019). Meanwhile, the contrast with negative samples is demystiﬁed as
avoiding the collapse issue, where the encoder outputs a trivial constant, and uniformly distributing samples
on the hypersphere (Wang & Isola, 2020). Beyond the usage of negative samples, several non-contrastive
methods in parallel considers using momentum encoders, stop gradient operation (Caron et al., 2020; Chen &
He, 2021; Chen et al., 2020a; Caron et al., 2021), etc.
To improve the quality of the contrast, various methods, such as large negative memory bank (Chen et al.,
2020c), hard negative mining (Chuang et al., 2020; Kalantidis et al., 2020), and using strong or multi-view
augmentations (Chen et al., 2020a; Tian et al., 2019; Caron et al., 2020), are proposed and succeed in learning
powerful representations. Since the conventional CL loss achieves the one-vs-many contrast with a softmax
cross-entropy loss, a notable concern is that the contrast could be sensitive to the sampled positive and
negative pairs (Saunshi et al., 2019; Chuang et al., 2020). Given a sampled query, conventional CL methods
usually randomly take one positive sample and multiple negative samples, and equally treat them in a softmax
cross-entropy form, regardless of how informative they are to the query. The sampled positive pair could make
the contrast either easy or diﬃcult, while trivially selecting hard negative pairs could make the pretraining
ineﬃcient, making the pretraining become less eﬀective when generalized to real-world data, where the labels
are rarely distributed in a balanced manner (Li et al., 2020b; 2021). In recent studies, a large of negative
sample manipulation is proposed to make the contrast more eﬀective, such as ring annealing (Wu et al.,
2021), maximizing margin within negatives (Shah et al., 2022), hard/soft nearest neighbor selection (Dwibedi
et al., 2021; GE et al., 2023).
Considering the CL loss aims to train the encoder to distinguish the positive sample from multiple negative
samples, an alternative intuition is that the positive samples need to be pulled close, while negative samples
need to be pushed far away from the given query in the representation space. In addition to such a push-pull
diagram, the intra-relation within positive and negative samples should also be considered. This motivates us
to investigate the CL in a view of transport and propose Contrastive Attraction and Contrastive Repulsion
(CACR),adoublyCLframeworkwherethepositiveandnegativesamplesareﬁrstcontrastedwithinthemselves
before getting pulled and pushed from the query, respectively. As shown in Figure 1, unlike conventional
CL, which equally treats samples and pulls/pushes them in the softmax cross-entropy contrast, CACR not
only considers moving positive/negative samples close/away, but also models two conditional distributions to
guide the movement of diﬀerent samples. The conditional distributions apply a doubly-contrastive strategy
to compare the positive samples and the negative ones within themselves separately. As an interpretation,
if a selected positive sample is far from the query, it indicates the encoder does not suﬃciently capture
2Published in Transactions on Machine Learning Research (07/2023)
/gid00004/gid00042/gid00041/gid00047/gid00045/gid00028/gid00046/gid00047/gid00036/gid00049/gid00032/gid00001/gid00002/gid00047/gid00047/gid00045/gid00028/gid00030/gid00047/gid00036/gid00042/gid00041/gid00001/gid00129/gid00001/gid00004/gid00042/gid00041/gid00047/gid00045/gid00028/gid00046/gid00047/gid00036/gid00049/gid00032/gid00001/gid00019/gid00032/gid00043/gid00048/gid00039/gid00046/gid00036/gid00042/gid00041/gid00015/gid00132
/gid00015/gid00133
/gid00015/gid00134/gid00015/gid00135/gid00017/gid00132
/gid00017/gid00133/gid00017/gid00134
/gid00015/gid00132
/gid00015/gid00133
/gid00015/gid00134/gid00015/gid00135/gid00017/gid00132
/gid00004/gid00042/gid00041/gid00049/gid00032/gid00041/gid00047/gid00036/gid00042/gid00041/gid00028/gid00039/gid00001/gid00004/gid00042/gid00041/gid00047/gid00045/gid00028/gid00046/gid00047/gid00036/gid00049/gid00032/gid00001/gid00013/gid00032/gid00028/gid00045/gid00041/gid00036/gid00041/gid00034/gid00018/gid00048/gid00032/gid00045/gid00052 /gid00017/gid00042/gid00046/gid00036/gid00047/gid00036/gid00049/gid00032/gid00001 /gid00015/gid00032/gid00034/gid00028/gid00047/gid00036/gid00049/gid00032 /gid00017/gid00042/gid00046/gid00036/gid00047/gid00036/gid00049/gid00032/gid00001/gid00187/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00031/gid00188 /gid00015/gid00032/gid00034/gid00028/gid00047/gid00036/gid00049/gid00032/gid00001/gid00187/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00031/gid00188
/gid00018/gid00048/gid00032/gid00045/gid00052
/gid00015/gid00032/gid00034/gid00028/gid00047/gid00036/gid00049/gid00032/gid00001/gid00132 /gid00015/gid00032/gid00034/gid00028/gid00047/gid00036/gid00049/gid00032/gid00001/gid00133 /gid00015/gid00032/gid00034/gid00028/gid00047/gid00036/gid00049/gid00032/gid00001/gid00134 /gid00015/gid00032/gid00034/gid00028/gid00047/gid00036/gid00049/gid00032/gid00001/gid00135/gid00017/gid00042/gid00046/gid00036/gid00047/gid00036/gid00049/gid00032/gid00001/gid00132/gid00001 /gid00017/gid00042/gid00046/gid00036/gid00047/gid00036/gid00049/gid00032/gid00001/gid00001/gid00133 /gid00017/gid00042/gid00046/gid00036/gid00047/gid00036/gid00049/gid00032/gid00001/gid00134
Figure 1: Comparison of conventional contrastive learning (CL) and the proposed Contrastive Attraction and
Contrastive Repulsion (CACR) framework. For conventional CL, given a query, the model randomly takes one positive
sample to form a positive pair and compares it against multiple negative pairs, with all samples equally treated. For
CACR, using multiple positive and negative pairs, the weight of a sample (indicated by point scale) is contrastively
computed to allow the query to not only more strongly pull more distant positive samples, but also more strongly
push away closer negative samples.
some information. CACR will then assign a higher probability for the query to pull this positive sample.
Conversely, if a selected negative sample is too close to the query, it indicates the encoder has diﬃculty
distinguishing them, and CACR will assign a higher probability for the query to push away this negative
sample. This double-contrast method contrast positive samples from negative samples, in a context of the
relation within positives and negatives. We theoretically analyze CACR is universal under general situations
or conditions, without the need for modiﬁcation, and empirically demonstrate the learned representations are
more eﬀective and robust in various tasks. Our main contributions include:
i)We propose CACR, which achieves contrastive learning and produces useful representations by attracting
the positive samples towards the query and repelling the negative samples away from the query, guided by
two conditional distributions.
ii)Our theoretical analysis shows that CACR generalizes the conventional CL loss. The conditional
distributions help treat the samples diﬀerently by modeling the intra-relation of positive/negative samples,
which is proved to be important when the datasets are less curated.
iii)Our experiments demonstrate the eﬀectiveness of CACR in a variety of standard CL settings, with both
convolutional and transformer-based architectures on various benchmark datasets. Moreover, in the case
where the dataset has an imbalanced label distribution, CACR has better robustness and provides consistent
better pretraining results than conventional CL.
2 Related work
Plenty of unsupervised representation learning (Bengio et al., 2013) methods have been developed to learn
good data representations, e.g.,PCA (Tipping & Bishop, 1999), RBM (Hinton & Salakhutdinov, 2006),
VAE (Kingma & Welling, 2014). Among them, CL (Oord et al., 2018) is investigated as a lower bound
of mutual information in early-stage (Gutmann & Hyvärinen, 2010; Hjelm et al., 2018). Recently, many
studies reveal that the eﬀectiveness of CL is not just attributed to the maximization of mutual information
(Tschannen et al., 2019; Tian et al., 2020a). In vision tasks, SimCLR (Chen et al., 2020a;b) studies extensive
augmentations for positive and negative samples and intra-batch-based negative sampling. A memory bank
that caches representations (Wu et al., 2018) and a momentum update strategy are introduced to enable the
use of an enormous number of negative samples (He et al., 2020; Chen et al., 2020c). Tian et al. (2019; 2020b)
consider the image views in diﬀerent modalities and minimize the irrelevant mutual information between
them. Empirical researches observe the merits of using “hard” negative samples in CL, motivating additional
techniques, such as Mixup and adversarial noise (Bose et al., 2018; Cherian & Aeron, 2020; Li et al., 2020a).
CL has also been developed in learning representations for text (Logeswaran & Lee, 2018), sequential data
(Oord et al., 2018; Hénaﬀ et al., 2019), structural data like graphs (Sun et al., 2020a; Li et al., 2019; Hassani
& Khasahmadi, 2020; Velickovic et al., 2019), reinforcement learning (Srinivas et al., 2020), downstream
ﬁne-tuning scenarios (Khosla et al., 2020; Sylvain et al., 2020; Cui et al., 2021). Besides vision tasks, CL
3Published in Transactions on Machine Learning Research (07/2023)
methods are widely applied to beneﬁt in a variety of areas such as NLP, graph learning, and cross-modality
learning (Misra & Maaten, 2020; He et al., 2020; Chen et al., 2020d; Fang & Xie, 2020; Giorgi et al., 2020;
Gao et al., 2021; Korbar et al., 2018; Jiao et al., 2020; Li & Zhao, 2021; Monfort et al., 2021).
In a view that not all negative pairs are “true” negatives (Saunshi et al., 2019), Chuang et al. (2020) propose
a decomposition of the data distribution to approximate the true negative distribution. RingCL (Wu et al.,
2021) proposes to use “neither too hard nor too easy” negative samples by predeﬁned percentiles, and
HN-CL (Robinson et al., 2021) applies Monte-Carlo sampling for selecting hard negative samples. Zhang et al.
selects the top-k important samples based on feature similarity. Shah et al. (2022) selects negatives as the
sparse support vectors and optimize in a max-margin manner. Besides, hard/soft nearest neighbor selection
are also consider as an eﬀective way to select useful negative samples (Dwibedi et al., 2021; GE et al., 2023).
Following works like Wang & Isola (2020), which reveal the contrastive scheme is optimizing the alignment
of positive samples and keeping the uniformity of negative pairs, instead of abusively using negative pairs,
recent self-supervised methods do not necessarily require negative pairs, avoiding the collapse issue with
stop gradient or a momentum updating strategy (Chen & He, 2021; Grill et al., 2020; Caron et al., 2021).
In addition, Zbontar et al. (2021) propose to train the encoder to make positive feature pairs have higher
correlation and decrease the cross-correlation in diﬀerent feature dimensions to avoid the collapse. Another
category is based on clustering, Caron et al. (2020), Feng & Patras (2022) and Li et al. (2020c) introduce the
prototypes as a proxy and train the encoder by learning to predict the cluster assignment. CACR is closely
related to the previous methods, and additionally consider the relation within positive samples and negative
samples. In our work, we leverage two conditional distribution to describe the relation between both positives
and negatives with respect to the query samples.
3 The proposed approach
In CL, for observations x0:M∼pdata(x), we commonly assume that each xican be transformed in certain
ways, with the samples transformed from the same and diﬀerent data regarded as positive and negative samples,
respectively. Speciﬁcally, we denote T(xi,/epsilon1i)as a random transformation of xi, where/epsilon1i∼p(/epsilon1)represents
the randomness injected into the transformation. In computer vision, /epsilon1ioften represents a composition of
random cropping, color jitter, Gaussian blurring, etc.For eachx0, with query x=T(x0,/epsilon10), we sample a
positive pair (x,x+), wherex+=T(x0,/epsilon1+), andMnegative pairs{(x,x−
i)}1:M, wherex−
i=T(xi,/epsilon1−
i).
Denoteτ∈R+, where R+:={x:x>0}, as a temperature parameter. With encoder fθ:Rn→Sd−1, where
we follow the convention to restrict the learned d-dimensional features with a unit norm, we desire to have
similar and distinct representations for positive and negative pairs, respectively, via the contrastive loss as
E
(x,x+,x−
1:M)/bracketleftBigg
−lnefθ(x)/latticetopfθ(x+)/τ
efθ(x)/latticetopfθ(x+)/τ+/summationtext
iefθ(x−
i)/latticetopfθ(x)/τ/bracketrightBigg
. (1)
Note by construction, the positive sample x+is independent of xgivenx0and the negative samples x−
i
are independent of x. Intuitively, this 1-vs- Msoftmax cross-entropy encourages the encoder to not only
pull the representation of a randomly selected positive sample closer to that of the query, but also push the
representations of Mrandomly selected negative samples away from that of the query.
3.1 Contrastive attraction and contrastive repulsion
In the same spirit of letting the query attract positive samples and repel negative sam-
ples, Contrastive Attraction and Contrastive Repulsion (CACR) directly models the cost
of moving from the query to positive/negative samples with a doubly contrastive strategy:
LCACR :=Ex∼p(x)Ex+∼π+
θ(·|x,x0)/bracketleftbig
c(fθ(x),fθ(x+))/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Contrastive Attraction
+Ex∼p(x)Ex−∼π−
θ(·|x)/bracketleftbig
−c(fθ(x),fθ(x−))/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Contrastive Repulsion,
:=LCA+LCR, (2)
4Published in Transactions on Machine Learning Research (07/2023)
𝑥!𝑓"encoderconditionaldistribution
sampleconstructionembeddinghypersphere𝑥#𝑧$#parametersharingparametersharingdistancemetricXX𝑥:positiveconditionalweights:negativeconditionalweights:positivepaircost:negativepaircost:query/positive/negativeembedding
distancemetriccostmetricconditionaldistribution𝑓"
𝑓"Π!"
Π!#
𝑧$𝑧$!contrastiveattraction
contrastiverepulsion
Figure 2: Illustration of the CACR framework. The encoder extracts features from samples and the conditional
distributions help weigh the samples diﬀerently given the query, according to the distance of a query xand its
contrastive samples x+,x−.⊗means element-wise multiplication between costs and conditional weights.
where we denote π+andπ−as the conditional distributions of intra-positive contrasts and intra-negative
contrasts, respectively, and c(z1,z2)as the point-to-point cost of moving between two vectors z1andz2,e.g.,
the squared Euclidean distance /bardblz1−z2/bardbl2or the negative inner product −zT
1z2. In the following we explain
the doubly contrastive components with more details.
Contrastive attraction : The intra-positive contrasts is deﬁned in a form of the conditional probability,
where the positive samples compete to gain a larger probability to be moved from the query. Here we adapt
to CACR a Bayesian strategy in Zheng & Zhou (2021), which exploits the combination of an energy-based
likelihoodtermandapriordistribution, toquantifythediﬀerencebetweentwoimplicitprobabilitydistributions
given their empirical samples. Speciﬁcally, denoting dt+(·,·)as a distance metric with temperature t+∈R+,
e.g.,dt+(z1,z2) =t+/bardblz1−z2/bardbl2, given a query x=T(x0,/epsilon10), we deﬁne the conditional probability for moving
it to positive sample x+=T(x0,/epsilon1+)as
π+
θ(x+|x,x0) :=edt+(fθ(x),fθ(x+))p(x+|x0)
Q+(x|x0)
Q+(x|x0) =:/integraldisplay
edt+(fθ(x),fθ(x+))p(x+|x0)dx+, (3)
wherefθ(·)is an encoder parameterized by θandQ+(x)is a normalization term. This construction makes it
more likely to pull xtowards a positive sample that is more distant in their latent representation space. With
equation 3, the contrastive attraction loss LCAmeasures the cost of moving a query to its positive samples,
as deﬁned in equation 2, which more heavily weighs c(fθ(x),fθ(x+))iffθ(x)andfθ(x+)are further away
from each other, providing ﬂexible distributions in hard-positive selection (Wang et al., 2020).
Contrastive repulsion : On the contrary of the contrastive attraction shown in equation 3, we deﬁne the
conditional probability for moving query xto a negative sample as
π−
θ(x−|x) :=e−dt−(fθ(x),fθ(x−))p(x−)
Q−(x),
Q−(x) :=/integraldisplay
e−dt−(fθ(x),fθ(x−))p(x−)dx−, (4)
wheret−∈R+is the temperature. This construction makes it more likely to move query xto a negative
sample that is closer from it in their representation space. With equation 4, the contrastive repulsion loss
LCRmeasures the expected cost to repel negative samples from the query shown in equation 2, which more
heavily weighs c(fθ(x),fθ(x−))iffθ(x)andfθ(x−)are closer to each other. To this sense, the distribution
π−
θ(x−|x)also assigns larger weights hard-negatives (Robinson et al., 2021).
Choice of c(·,·),dt+(·,·)anddt−(·,·).There could be various choices for the point-to-point cost function
c(·,·), distance metric dt+(·,·)in equation 3, and dt−(·,·)in equation 4. Considering the encoder fθoutputs
normalized vectors on the surface of a hypersphere, maximizing the inner product is equivalent to minimizing
5Published in Transactions on Machine Learning Research (07/2023)
Table 1: Comparison with representative CL methods. KandMdenotes the number of positive and negative
samples, respectively.
Method Contrast LossIntra-positive Intra-negative
contrast contrast
CL (Chen et al., 2020a) 1-vs-Mcross-entropy % %
AU-CL (Wang & Isola, 2020) 1-vs-Mcross-entropy % %
HN-CL (Robinson et al., 2021) 1-vs-Mcross-entropy % !
CMC (Tian et al., 2019)/parenleftbigK
2/parenrightbig
×(1-vs-Mcross-entropy) % %
CACR (ours) Intra-K-positive vs Intra- M-negative ! !
squared Euclidean distance. Without loss of generality, we deﬁne them as
c(x,y) =/bardblx−y/bardbl2
2
dt+(x,y) =t+/bardblx−y/bardbl2
2;t+∈R+,
dt−(x,y) =t−/bardblx−y/bardbl2
2;t−∈R+.
wheret+,t−∈R+. There are other choices for c(·,·)and we show the ablation study in Appendix B.5.
3.2 Mini-batch based stochastic optimization
Under the CACR loss as in equation 2, to make the learning of fθ(·)amenable to mini-batch stochastic
gradient descent (SGD) based optimization, we draw (xdata
i,/epsilon1i)∼pdata(x)p(/epsilon1)fori= 1,...,Mand then
approximate the distribution of the query using an empirical distribution of Msamples as
ˆp(x) =1
M/summationtextM
i=1δ(x−xi);xi=T(xdata
i,/epsilon1i).
where theδ(·)denotes the Dirac delta function, and we note δxias the Dirac function centered at xii.e.,
δxi=δ(x−xi). With query xiand/epsilon11:Kiid∼p(/epsilon1), we approximate p(x−
i)for equation 4 and p(x+
i|xdata
i)for
equation 3 with x+
ik=T(xdata
i,/epsilon1k):
ˆp(x−
i) =1
M−1/summationtext
j/negationslash=iδxj,ˆp(x+
i|xdata
i) =1
K/summationtextK
k=1δx+
ik. (5)
Note we may improve the accuracy of ˆp(x−
i)in equation 5 by adding previous queries into the support of this
empirical distribution. Other more sophisticated ways to construct negative samples (Oord et al., 2018; He
et al., 2020; Khosla et al., 2020) could also be adopted to deﬁne ˆp(x−
i). We will elaborate these points when
describing experiments.
Plugging equation 5 into equation 3 and equation 4, we approximate the conditional distributions
with discrete distributions and obtain a mini-batch based CACR loss as ˆLCACR =ˆLCA+ˆLCR, where
ˆLCA:=1
M/summationtextM
i=1/summationtextK
k=1edt+(fθ(xi),fθ(x+
ik))
/summationtextK
k/prime=1edt+(fθ(xi),fθ(x+
ik/prime))×c(fθ(xi),fθ(x+
ik)),
ˆLCR:=−1
M/summationtextM
i=1/summationtext
j/negationslash=ie−dt−(fθ(xi),fθ(xj))/summationtext
j/prime/negationslash=ie−dt−(fθ(xi),fθ(xj/prime))×c(fθ(xi),fθ(xj)).
We optimize θvia SGD using∇θˆLCACR, with the framework instantiated as in Figure 2.
3.3 Relation with typical CL loss
As shown in equation 2, with both the contrastive attraction component and contrastive repulsion component,
CACR loss shares the same intuition of conventional CL (Oord et al., 2018; Chen et al., 2020a) in pulling
positive samples closer to and pushing negative samples away from the query in their representation space.
However, CACR realizes this intuition by introducing the double-contrast strategy on the point-to-point
6Published in Transactions on Machine Learning Research (07/2023)
moving cost, where the contrasts appear in the intra-comparison within positive and negative samples,
respectively. The use of the double-contrast strategy clearly diﬀers the CACR loss in equation 2 from the
conventional CL loss in equation 1, which typically relies on a softmax-based contrast formed with a single
positive sample and multiple equally-weighted independent negative samples. The conditional distributions
in CA and CR loss also provide a more ﬂexible way to deal with hard-positive/negative samples (Robinson
et al., 2021; Wang et al., 2020; 2019; Tabassum et al., 2022; Xu et al., 2022) and does not require heavy labor
in tuning the hyper-parameters for the model. A summary of the comparison between some representative
CL losses and CACR is shown in Table 1.
4 Property analysis of CACR
4.1 On contrastive attraction
We ﬁrst analyze the eﬀects w.r.t.the positive samples. With contrastive attraction, the property below
suggests that the optimal encoder produces representations invariant to the noisy details.
Property 1.The contrastive attraction loss LCAis optimized if and only if all positive samples of a query share
the same representation as that query. More speciﬁcally, for query xthat is transformed from x0∼pdata(x),
its positive samples share the same representation with it, which means
fθ(x+) =fθ(x)for anyx+∼π(x+|x,x0). (6)
This property coincides with the characteristic (learning invariant representation) of the CL loss in Wang &
Isola (2020) when achieving the optima. However, the optimization dynamic in contrastive attraction evolves
in the context of x+∼πθ(x+|x,x0), which is diﬀerent from that in the CL.
Lemma 4.1. Let us instantiate c(fθ(x),fθ(x+)) =−fθ(x)/latticetopfθ(x+). Then, the contrastive attraction loss
LCAin equation 2 can be re-written as
Ex0Ex,x+∼p(·|x0)/bracketleftBig
−fθ(x)/latticetopfθ(x+)π+
θ(x+|x,x0)
p(x+|x0)/bracketrightBig
,
which could further reduce to the alignment loss Ex0∼pdata(x)Ex,x+∼p(·|x0)/bracketleftbig
−fθ(x)/latticetopfθ(x+))/bracketrightbig
in Wang &
Isola (2020), iﬀπ+
θ(x+|x,x0) =p(x+|x0).
Property 1 and Lemma 4.1 jointly show contrastive attraction in CACR and the alignment loss in CL
reach the same optima, while working in diﬀerent sampling mechanism. In practice x+andxare usually
independently sampled augmentations in a mini-batch, as shown in Section 3.2, which raises a gap between
the empirical distribution and the true distribution. Our method makes the alignment more eﬃcient by
considering the intra-relation of these positive samples to the query.
4.2 On contrastive repulsion
Next we analyze the eﬀects w.r.t.the contribution of negative samples. Wang & Isola (2020) reveal that a
perfect encoder will uniformly distribute samples on a hypersphere under an uniform isometric assumption,
i.e., for any uniformly sampled x,x−iid∼p(x), their latent representations z=fθ(x)andz−=fθ(x−)also
satisfyp(z) =p(z−). We follow their assumption to analyze contrastive repulsion via the following lemma.
Lemma 4.2. Without loss of generality, we deﬁne the moving cost and metric in the conditional distribution
asc(z1,z2) =d(z1,z2) =/bardblz1−z2/bardbl2
2. When we are with an uniform prior, namely p(x) =p(x−)for
anyx,x−iid∼p(x)andp(z) =p(z−)given their latent representations z=fθ(x)andz−=fθ(x−), then
optimizingθwithLCRin equation 2 is the same as optimizing θto minimize the mutual information between
xandx−:
I(X;X−) =Ex∼p(x)Ex−∼π−
θ(·|x)/bracketleftBig
lnπ−
θ(x−|x)
p(x−)/bracketrightBig
, (7)
7Published in Transactions on Machine Learning Research (07/2023)
and is also the same as optimizing θto maximize the conditional diﬀerential entropy of x−givenx:
H(X−|X) =Ex∼p(x)Ex−∼π−
θ(·|x)[−lnπ−
θ(x−|x)]. (8)
Here the minimizer θ⋆ofLCRis also that of I(X;X−), whose global minimum zero is attained iﬀXandX−
are independent, and the equivalent maximum of H(X−|X)indicates the optimization of LCRis essentially
aimed towards the uniformity of representation about negative samples.
We notice that one way to reach the optimum suggested in the above lemma is optimizing θby contrastive
repulsion until that for any x∼p(x),d(fθ(x),fθ(x−))is equal for all x−∼π−
θ(·|x). This means for any
sampled negative samples, their representations are also uniformly distributed after contrastive repulsion.
Interestingly, this is consistent with the uniformity property achieved by CL (Wang & Isola, 2020), which
connects contrastive repulsion with CL in the perspective of negative sample eﬀects.
Note that, although the above analysis builds upon the uniform isometric assumption, our method actually
does not rely on it. Here, we formalize a more general relation between the contrastive repulsion and the
contribution of negative samples in CL without this assumption as follows.
Lemma 4.3. As the number of negative samples Mgoes to inﬁnity, the contribution of the negative samples
to the CL loss becomes the Uniformity Loss in AU-CL (Wang & Isola, 2020), termed as Luniformfor simplicity.
It can be expressed as an upper bound of LCRby adding the mutual information I(X;X−)in equation 7:
Ex∼p(x)/bracketleftBig
lnEx−∼p(x−)efθ(x−)/latticetopfθ(x)/τ/bracketrightBig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Luniform+I(X;X−)>LCR,
As shown in Lemma 4.3, the mutual information I(X;X−)helps quantify the diﬀerence between Luniformand
LCR. The diﬀerence between drawing x−∼π−
θ(x−|x)(in CR) and drawing x−independently in a mini-
batch (in CL) is non-trivial as long as I(X;X−)is non-zero. In practice, this is true almost everywhere since
we have to handle the skewed data distribution in real-world applications, e.g., the label-shift scenarios (Garg
et al., 2020). In this view, CR does not require the representation space to be uniform like CL does, and is
more robust to the complex cases through considering the intra-contrastive relation within negative samples.
5 Experiments and empirical analysis
In this section, we ﬁrst study the CACR behaviors with small-scale experiments, where we use CIFAR-10,
CIFAR-100 (Hinton, 2007) and create two class-imbalanced CIFAR datasets as empirical veriﬁcation of
our theoretical analysis. We mainly compare with representative CL methods, divided into two diﬀerent
categories according to their positive sampling size: K= 1andK= 4. For methods with a single positive
sample (K= 1), the baseline methods include the conventional CL loss (Oord et al., 2018), AlignUniform CL
loss (AU-CL) (Wang & Isola, 2020), and the CL loss with hard negative sampling (HN-CL) (Robinson et al.,
2021). In the case of K= 4, we take contrastive multi-view coding (CMC) loss (Tian et al., 2019) (align with
our augmentation settings and use augmentation views instead of channels) as the comparison baseline. For
a fair comparison, we keep for all methods with the same experiment setting including learning-rate, training
epochs,etc., but use their best temperature parameters; the mini-batch size for K= 4is divided by 4 from
those when K= 1to make sure the encoder leverages same samples in each iteration.
For large-scale datasets, we use ImageNet-1K (Deng et al., 2009) and compare with the state-of-the-art
frameworks (He et al., 2020; Zbontar et al., 2021; Chen et al., 2020a; Caron et al., 2020; Grill et al., 2020;
Huynh et al., 2020) on linear probing, where we report the Top-1 validation accuracy on ImageNet-1K
data. We also report the results of object detection/segmentaion following the transfer learning protocol.
To further justify our analysis, we also leverage two large-scale but label-imbalanced datasets (Webvision
v1 and ImageNet-22K) for linear probing pretraining. The reported numbers for baselines are from the
original papers if available, otherwise we report the best ones reproduced with the settings according to their
corresponding papers. Please refer to Appendix C for detailed experiment setups.
8Published in Transactions on Machine Learning Research (07/2023)
2.53.95.3Nats
(unif)
(CACR (K=4))
(CMC (K=4))
0 25 50 75 100 125 150 175 200
Epoch5.55.96.3Nats (unif)
(CL)
(AU-CL)
(HN-CL)
(CACR (K=1))
2.53.95.3Nats
(unif)
(CACR (K=4))
(CMC (K=4))
0 25 50 75 100 125 150 175 200
Epoch5.55.96.3Nats (unif)
(CL)
(AU-CL)
(HN-CL)
(CACR (K=1))
Figure 3: Conditional entropy H(X−|X)w.r.t.epoch on CIFAR-10 ( left) and linearly label-imbalanced CIFAR-10
(right). The maximal possible conditional entropy is marked by a dotted line.
5.1 Studies and analysis on small-scale datasets
Classiﬁcation accuracy: To facilitate the analysis, we apply all methods with an AlexNet-based encoder
following the setting in Wang & Isola (2020), trained in 200 epochs. We pretrained the encoder on regular
CIFAR-10/100 data and create class-imbalanced cases by randomly sampling a certain number of samples from
each class with a “linear” or “exponentional” rule by following the setting in Kim et al. (2020). Speciﬁcally,
given a dataset with Cclasses, for class l∈{1,2,...,C}, we randomly take samples with proportion ⌊l
C⌋
for “linear” rule and proportion exp(⌊l
C⌋)for “exponential” rule. For evaluation we keep the standard
validation/testing datasets. Thus there is a label-shift between the training and testing data distributions.
Summarized in Table 2 are the results on both regular and class-imbalanced datasets. The ﬁrst two columns
show the results pretrained with curated data, where we can observe that in the case of K= 1, where the
intra-positive contrast of CACR degenerates, CACR slightly outperforms all CL methods. When K= 4, it
is interesting to observe an obvious boost in performance, where CMC improves CL by around 2-3% while
CACR improves CL by around 3-4%, which supports our analysis that CA is helpful when the intra-positive
contrast is not degenerated. The right four columns present the linear probing results pretrained with
class-imbalanced data, which show all the methods have a performance drop. It is clear that CACR has the
least performance decline in most cases. Especially, when K= 4, CACR shows better performance robustness
due to the characteristic of doubly contrastive within positive and negative samples. For example, in the
“exponentional” setting of CIFAR-100, CL and HN-CL drop 12.57% and 10.73%, respectively, while CACR
(K= 4) drops 9.24%. It is also interesting to observe HN-CL is relatively better among the baseline methods.
According to Robinson et al. (2021), in HN-CL the negative samples are sampled according to the “hardness”
w.r.t.the query samples with an intra-negative contrast. Its loss could converge to CACR ( K= 1) with
inﬁnite negative samples. This performance gap indicates that directly optimizing the CACR loss could be
superior when we have a limited number of samples. With this class-imbalanced datasets, we provide the
empirical support to our analysis: When the condition in Lemma 4.2 is violated, CACR shows a clearer
diﬀerence than CL and a better robustness with its unique doubly contrastive strategy within positive and
negative samples.
On the eﬀect of CA and CR: To further study the contrasts within positive and negative samples, in
each epoch, we calculate the conditional entropy with equation 8 on every mini-batch of the validation
dataand take the average across mini-batches. Then, we illustrate in Figure 3 the evolution of conditional
entropyH(X−|X)w.r.t.the training epoch on regular CIFAR-10 and class-imbalanced CIFAR-10. As
shown,H(X−|X)is getting maximized as the encoder is getting optimized, indicating the encoder learns
to distinguish the negative samples from given query. It is also interesting to observe that in the case with
multiple positive samples, this process is much more eﬃcient, where the conditional entropy reaches the
possible biggest value rapidly. This implies the CA module can further boost the repulsion of negative samples.
From the gap between CACR and CMC, we can learn although CMC uses multiple positive in CL loss,
the lack of intra-positive contrast shows the gap of attraction eﬃciency. In the right panel of Figure 3, the
diﬀerence between CACR and baseline methods are more obvious, where we can ﬁnd the conditional entropy
9Published in Transactions on Machine Learning Research (07/2023)
Table 2: The linear classiﬁcation accuracy ( %) of diﬀerent contrastive objectives on small-scale datasets, pretrained
on regular and label-imbalanced CIFAR10/100 with AlexNet backbone. “Linear” and “Exponentional” indicate
the number of samples in each class are chosen by following a linear rule or an exponential rule, respectively. The
performance drops compared with the performance in regular CIFAR data are shown next to each result.
Label imbalance Regular Linear Exponential
Dataset CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100
SimCLR (CL) 83.47 55 .41 79.883.59↓52.293.57↓71.7411.73↓43.2912.57↓
AU-CL 83.49 55 .31 80.253.14↓52.742.57↓71.6211.76↓44.3810.93↓
HN-CL 83.67 55 .8780.51 3.15↓52.723.14↓72.7410.93↓45.1310.73↓
CACR (K= 1)83.73 56.52 80.463.27↓54.12 2.40↓73.02 10.71↓46.59 9.93↓
CMC (K= 4) 85.54 58 .64 82.203.34↓55.383.26↓74.7710.77↓48.879.77↓
CACR (K= 4)86.54 59.41 83.62 2.92↓56.91 2.50↓75.89 10.65↓50.17 9.24↓
Table 3: The top-1 classiﬁcation accuracy ( %) of diﬀerent contrastive objectives with diﬀerent training epochs
on small-scale datasets, following SimCLR setting and applying the AlexNet-based encoder.
DatasetTrained with 400 epochs Trained with 200 epochs
CL AU-CL HN-CL CACR(K=1) CMC(K=4) CACR(K=4)
CIFAR-10 83.61 83.57 83.72 83.86 85.54 86.54
CIFAR-100 55.41 56.07 55.80 56.41 58.64 59.41
STL-10 83.49 83.43 82.41 84.56 84.50 85.59
of baselines is slightly lower than pretrained with regular CIFAR-10 data. Especially for vanilla CL loss, we
can observe the conditional entropy has a slight decreasing tendency, indicating the encoder hardly learns to
distinguish negative samples in this case. Conversely, CACR still shows to remain the conditional entropy at
a higher level, which explains the robustness shown in Table 2, and indicating a superior learning eﬃciency of
CACR. See Appendix B.2 for similar observations on CIFAR-100 and exponential label-imbalanced cases. In
that part, we provide more quantitative and qualitative studies on the eﬀects of conditional distributions.
Does CACR( K>2) outperform by seeing more samples? To address this concern, in our main
paper, we intentionally decrease the mini-batch size as M= 128. Thus the total number of samples used per
iteration is not greater than those used when K= 1. To further justify if the performance boost comes from
seeing more samples when using multiple positive pairs, we also let the methods allowing single positive pair
train with double epochs. As shown in Table 3, we can observe even trained with 400 epochs, the performance
of methods using single positive pair still have a gap from those using multiple positive pairs.
5.2 Experiments on large-scale datasets
For large-scale experiments, we follow the self-supervised evaluation pipeline to examine the performance of
CACR: we ﬁrst leverage MoCov2 Chen et al. (2020c) design to pre-train a ResNet-50 with CACR loss, and
then evaluate the capacity of the pre-trained model in a variety of tasks, including linear probing, downstream
few/full-shot image classiﬁcation, and detection/segmentation. Besides these tasks, additional ablation studies
are provided in Appendix B.
Linear probing: Table 4 summarizes the results of linear classiﬁcation, where a linear classiﬁer is trained
on ImageNet-1K on top of ﬁxed representations of the pretrained ResNet50 encoder. Similar to the case
on small-scale datasets, CACR consistently shows better performance than the baselines using contrastive
loss, improving SimCLR and MoCov2 by 2.7% and 2.2% respectively. Compared with other non-contrastive
self-supervised SOTAs, CACR also shows on par performance.
Label-imbalanced case: To strengthen our analysis on small-scale label-imbalanced data, we specially
deploy two real-world, but less curated datasets Webvision v1 and ImageNet-22K that have long-tail label
distributions for encoder pretraining and evaluate the linear classiﬁcation accuracy on ImageNet-1K. We
pretrain encoder with 100/20 epochs on Webvision v1/ImageNet-22K and compare with the encoder pretrained
with 200 epochs on ImageNet-1K to make sure similar samples have been seen in the pretraining. The results
are shown in Table 5, where we can see CACR still outperforms the MoCov2 baseline and shows better
robustness when generalized to wild image data.
10Published in Transactions on Machine Learning Research (07/2023)
Table 4: Top-1 classiﬁcation accuracy ( %) comparison with SOTAs
including non-contrastive and contrastive methods, pretrained with
ResNet50encoderonImageNet-1Kdataset. WemarkTop-3bestresults
in bold and highlight CL methods.
Methods Batch-size Accuracy
Non-ContrastiveBarlowTwins 1024 73.2
Simsiam 256 71.3
(wo. Negatives)SWAV (wo/w multi-crop) 4096 71.8 /75.3
BYOL 4096 74.3
ContrastiveSimCLR 4096 71.7
MoCov2 256 72.2
ASCL 256 71.5
(w. Negatives)FNC (w multi-crop) 4096 74.4
ADACLR 4096 72.3
CACR (K=1) 256 73.7
CACR (K=4) 256 74.7Table 5: Top-1 classiﬁcation accuracy (%) on
ImageNet-1K, with the pre-trained ResNet50
on large-scale regular (200 epochs) and label-
imbalanced (100/20 epochs) datasets. The
performance drops are shown next to each
result.
Pretrained data Methods Accuracy
ImageNet-1KMoCov2 67.5
CACR (K=1) 69.5
CACR (K=4) 70.4
Webvision v1MoCov2 62.35.2↓
CACR (K=1) 64.55.0↓
CACR (K=4) 66.1 4.3↓
ImageNet-22KMoCov2 59.97.6↓
CACR (K=1) 61.97.6↓
CACR (K=4) 64.5 5.9↓Dataset
Caltech101
CIFAR10
CIFAR100
Country211
DescriTextures
EuroSAT
FER2013
FGVC Aircraft
Food101
GTSRB
HatefulMemes
KITTI
MNIST
Oxford Flowers
Oxford Pets
PatchCamelyon
Rendered SST2
RESISC45
Stanford Cars
VOC2007
Mean Acc.
# Wins5-FTMoCov3 73.7 70.3 17.4 2.3 45.6 60.0 13.5 7.2 27.6 16.5 50.8 43.5 18.1 65.7 77.1 50.9 50.7 58.2 11.2 25.7 39.3 4
CACR 84.8 67.6 24.3 2.5 51.2 73.6 23.0 21.4 17.0 23.7 51.8 45.4 44.0 81.1 79.4 58.4 51.2 49.1 10.8 69.0 46.5 16
Gains +11.1 -2.7 +6.9 +0.2 +5.6 +13.6 +9.5 +14.2 -10.6 +7.2 +1.0 +1.9 +25.9 +15.4 +2.3 +7.5 +0.4 -9.1 -0.3 +43.3 +7.25-LPMoCov3 80.8 78.5 60.5 4.8 57.1 77.1 20.5 11.8 36.6 31.4 50.7 46.7 64.1 79.5 76.2 54.7 50.0 61.1 13.4 47.9 50.2 4
CACR 79.3 85.4 62.9 4.7 57.1 76.1 18.3 21.6 40.9 32.9 50.9 50.3 69.2 84.6 81.2 56.9 51.8 61.7 21.1 74.4 54.1 15
Gains -1.5 +6.9 +2.4 -0.1 +0.0 -1.0 -2.2 +9.8 +4.3 +1.5 +0.2 +3.6 +5.1 +5.1 +5.0 +2.2 +1.8 +0.6 +7.7 +26.5 +3.9Full-FTMoCov3 93.3 98.1 88.7 11.7 71.3 97.3 68.3 51.9 84.1 98.8 54.5 80.5 99.6 87.1 90.9 91.4 52.5 88.6 67.9 77.6 77.7 3
CACR 93.3 98.1 89.9 12.9 72.0 97.7 68.3 56.3 85.2 99.1 54.8 80.6 99.1 89.3 91.6 88.1 56.6 88.8 79.1 75.4 78.8 15
Gains +0.0 +0.0 +1.2 +1.2 +0.7 +0.4 +0.0 +4.4 +1.1 +0.3 +0.3 +0.1 -0.5 +2.2 +0.7 -3.3+4.1 +0.2 +11.2 -2.2 +1.1Full-LPMoCov3 92.1 96.9 85.3 13.7 73.1 95.9 60.1 48.0 78.0 78.7 53.7 68.8 98.4 89.5 91.4 86.7 57.1 86.3 63.0 81.7 74.9 8
CACR 92.9 96.9 85.1 13.3 74.1 96.4 59.8 47.8 78.6 77.9 54.5 68.1 98.6 92.9 92.6 85.2 56.5 86.7 64.1 83.4 75.3 11
Gains +0.8 +0.0 -0.2 -0.4 +1.0 +0.5 -0.3 -0.2 +0.6 -0.8+0.8 -0.7 +0.2 +3.4 +1.2 -1.5 -0.6 +0.4 +1.1 +1.7 +0.4
Table 6: Comparison of CACR and MoCov3 pre-trained ViT-B/16 encoder on ELEVATER benchmark (Li et al.,
2022) . We conduct ﬁne-tuning (FT) and linear-probing (LP) in both 5-shot (top 2 rows) and full-show (bottom 2
rows) on 20 datasets. We calculate the gains marked in green for positive results. The mean score and number of wins
are reported in the last two columns.
Downstream image classiﬁcation: To measure the eﬃciency in adapting the pre-trained model to a
wide range of downstream data-sets (Kornblith et al., 2021), we employ the recently developed ELEVATER
benchmark (Li et al., 2022) to consider both 5-shot and full-shot transfer learning setting: the pre-trained
ViT-B/16 is evaluated with ﬁne-tuning and linear probing on 20 public image classiﬁcation data sets, where
for each data set 5 training samples are randomly selected in 5-shot setting, otherwise all data are used
to train the model for 50 epochs before the test score is reported, and 3 random seeds are considered for
each data set. We deploy the automatic hyper-parameter tuning pipeline implemented in ELEVATER that
searches for the best parameter for each model to make a fair ﬁne-tuning and linear probing comparison of
pre-trained models. The original metrics of each dataset are used with more details provided in Li et al.
(2022) and Appendix C. To measure the overall performance, we consider the average scores over 20 datasets,
and “# Wins" indicates the number of data sets on which the current model outperforms its counterpart.
As shown in Table 6, we observe in 5-shot scenarios linear probing tends to outperform ﬁne-tuning, likely
due to the model being heavily adapted to the pre-training data, thus making it less ﬂexible for new tasks.
Despite such as challenge, we can still observe CACR preserves a better transferability with a signiﬁcant
gain. As the amount of task-speciﬁc data increases, we observe that ﬁne-tuning starts to outperform linear
probing, and CACR still outperforms MoCov3, even though the gap between these two methods become
smaller. Overall, in both settings, CACR outperforms MoCov3 in 75% of the downstream datasets, indicating
the representation eﬃciency of transferring in downstream applications.
Object detection and segmentation: Besides the linear classiﬁcation evaluation, following the protocols
in previous works (Tian et al., 2019; He et al., 2020; Chen et al., 2020c; Wang & Isola, 2020), we use the
11Published in Transactions on Machine Learning Research (07/2023)
Table 7: Results of transferring features to object detection and segmentation task on Pascal VOC, with the pre-trained
ResNet50 on ImageNet-1k. Contrastive learning methods are highlighted.
MethodVOC 07+12 detection COCO detection COCO instance seg.
AP50AP AP 75AP50AP AP 75AP50AP AP 75
scratch 60.2 33.8 33.1 44.0 26.4 27.8 46.9 29.3 30.8
supervised 81.3 53.5 58.8 58.2 38.2 41.2 54.7 33.3 35.2
Non-ContrastiveBYOL 81.4 55.3 61.1 57.8 37.9 40.9 54.3 33.2 35.0
SwAV 81.5 55.4 61.4 57.6 37.6 40.3 54.2 33.1 35.1
(wo. Negatives)SimSiam 82.4 57.0 63.7 59.3 39.2 42.1 56.034.4 36.7
Barlow Twins 82.6 56.8 63.4 59.0 39.2 42.5 56.034.3 36.5
ContrastiveSimCLR 81.8 55.5 61.4 57.7 37.9 40.9 54.6 33.3 35.3
MoCov2 82.3 57.0 63.3 58.8 39.2 42.5 55.5 34.3 36.6
(w. Negatives)AU-CL 82.5 57.2 63.8 58.4 39.1 42.2 55.7 34.1 36.3
CACR(K=1) 82.857.8 64.2 58.9 39.3 42.5 55.6 34.4 36.7
CACR(K=4) 82.8 57.9 64.9 59.8 40.0 42.7 55.835.0 37.0
pretrained ResNet50 on ImageNet-1K for object detection and segmentation task on Pascal VOC (Everingham
et al., 2010) and COCO (Lin et al., 2014) by using detectron2 (Wu et al., 2019). The experimental setting
details are shown in Appendix C.2 and kept the same as He et al. (2020) and Chen et al. (2020c). The
test AP, AP 50, and AP 75of bounding boxes in object detection and test AP, AP 50, and AP 75of masks in
segmentation are reported in Table 7. We can observe that the performances of CACR are consistently better
than baselines using contrastive objectives, and better than non-contrastive self-supervised learning SOTAs.
6 Conclusion
In this paper, we rethink the limitation of conventional contrastive learning (CL) methods that use the
contrastive loss but merely consider the intra-relation between samples. In the spirit of a distributional
transport between positive and negative samples, we introduce Contrastive Attraction and Contrastive
Repulsion (CACR) loss with a doubly contrastive strategy, which constructs for two conditional distributions
to respectively model the importance of a positive sample and that of a negative sample to the query
according to their distances to the query. Our theoretical analysis and empirical results show that the CACR
loss can eﬀectively attract positive samples and repel negative ones from the query as CL intends to do,
but is more robust in more general cases. Extensive experiments on small, large-scale, and imbalanced
datasets consistently demonstrate the superiority and robustness of CACR over the state-of-the-art methods
in contrastive representation learning and related downstream tasks.
Broader Impact Statement
Contrastive learning (CL) is eﬀective in learning data representations without label supervision and has led to
notable recent progresses in a variety of research areas, such as computer vision. Recently proposed advanced
CL methods often require a huge amount of data and thus cost large computational energy. Especially in the
case where one needs to use multiple positive pairs in the contrast. Instead of contrasting each positive pair
over multiple negative pairs with the classic softmax cross-entropy, our work discovers that the contrastive
attraction within positives and contrastive repulsion within negatives bring new insight in self-supervised
representation learning. CACR, which naturally takes multiple positive samples in the contrast without
making the contrast complexity become combinatorial in the number of positive pairs, has demonstrated
clear improvements over existing CL methods. However, the same as existing CL methods, our method is not
designed to resist the potential biases existing in the dataset, e.g.the false negatives in data. At the current
stage, CACR relies on the positive contrast to implicitly alleviate this issue: if a false negative sample is
repelled too far away from the query, in the positive attraction, it will be assigned with larger probability to
be pulled back. This raises the risk of the quality of learned representations. In the future work, we aim
and also encourage other researchers to consider the resistance of these potential risks to make the learned
representations more robust and powerful.
12Published in Transactions on Machine Learning Research (07/2023)
Acknowledgments
H. Zheng and M. Zhou acknowledge the support of NSF-IIS 2212418 and TACC.
References
Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual
information across views. In Advances in Neural Information Processing Systems , 2019.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives.
IEEE transactions on pattern analysis and machine intelligence , 35(8):1798–1828, 2013.
Avishek Joey Bose, Huan Ling, and Yanshuai Cao. Adversarial contrastive estimation. arXiv preprint
arXiv:1805.03642 , 2018.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised
learning of visual features by contrasting cluster assignments. In Proceedings of Advances in Neural
Information Processing Systems (NeurIPS) , 2020.
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand
Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International
Conference on Computer Vision (ICCV) , 2021.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. A simple framework for contrastive
learning of visual representations. arXiv preprint arXiv:2002.05709 , 2020a.
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoﬀrey Hinton. Big self-supervised
models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029 , 2020b.
Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 15750–15758, 2021.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive
learning. arXiv preprint arXiv:2003.04297 , 2020c.
Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers.
InProceedings of the IEEE/CVF International Conference on Computer Vision , pp. 9640–9649, 2021.
Xu Chen, Ya Zhang, Ivor Tsang, and Yuangang Pan. Learning robust node representations on graphs. arXiv
preprint arXiv:2008.11416 , 2020d.
Anoop Cherian and Shuchin Aeron. Representation learning via adversarially-contrastive optimal transport.
arXiv preprint arXiv:2007.05840 , 2020.
Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie Jegelka. Debiased
contrastive learning. Advances in Neural Information Processing Systems , 33, 2020.
Jiequan Cui, Zhisheng Zhong, Shu Liu, Bei Yu, and Jiaya Jia. Parametric contrastive learning. In Proceedings
of the IEEE/CVF international conference on computer vision , pp. 715–724, 2021.
Tim R Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M Tomczak. Hyperspherical
variational auto-encoders. arXiv preprint arXiv:1804.00891 , 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248–255. Ieee,
2009.
AlexeyDosovitskiy, LucasBeyer, AlexanderKolesnikov, DirkWeissenborn, XiaohuaZhai, ThomasUnterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
13Published in Transactions on Machine Learning Research (07/2023)
Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a little
help from my friends: Nearest-neighbor contrastive learning of visual representations. In Proceedings of the
IEEE/CVF International Conference on Computer Vision , pp. 9588–9597, 2021.
Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal
visual object classes (VOC) challenge. International journal of computer vision , 88(2):303–338, 2010.
Hongchao Fang and Pengtao Xie. CERT: Contrastive self-supervised learning for language understanding.
arXiv preprint arXiv:2005.12766 , 2020.
Chen Feng and Ioannis Patras. Adaptive soft contrastive learning. In 2022 26th International Conference on
Pattern Recognition (ICPR) , pp. 2721–2727. IEEE, 2022.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings.
arXiv preprint arXiv:2104.08821 , 2021.
Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary C Lipton. A uniﬁed view of label shift
estimation. arXiv preprint arXiv:2003.07554 , 2020.
Chongjian GE, Jiangliu Wang, Zhan Tong, Shoufa Chen, Yibing Song, and Ping Luo. Soft neighbors are
positive supporters in contrastive visual representation learning. In The Eleventh International Conference
on Learning Representations , 2023. URL https://openreview.net/forum?id=l9vM_PaUKz .
John M Giorgi, Osvald Nitski, Gary D. Bader, and Bo Wang. DeCLUTR: Deep contrastive learning for
unsupervised textual representations. ArXiv, abs/2006.03659, 2020.
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew
Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training imagenet in 1 hour.
arXiv preprint arXiv:1706.02677 , 2017.
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,
Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your
own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems ,
33, 2020.
Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for
unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artiﬁcial
Intelligence and Statistics , pp. 297–304, 2010.
Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. In
International Conference on Machine Learning , pp. 3451–3461, 2020.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90.
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask R-CNN. In Proceedings of the IEEE
international conference on computer vision , pp. 2961–2969, 2017.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised
visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , June 2020.
Olivier J Hénaﬀ, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-eﬃcient image
recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272 , 2019.
Geoﬀrey E Hinton. Learning multiple layers of representation. Trends in cognitive sciences , 11(10):428–434,
2007.
Geoﬀrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks.
science, 313(5786):504–507, 2006.
14Published in Transactions on Machine Learning Research (07/2023)
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler,
and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization.
InInternational Conference on Learning Representations , 2018.
Tri Huynh, Simon Kornblith, Matthew R Walter, Michael Maire, and Maryam Khademi. Boosting contrastive
self-supervised learning with false negative cancellation. arXiv preprint arXiv:2011.11765 , 2020.
Jianbo Jiao, Yifan Cai, Mohammad Alsharid, Lior Drukker, Aris T Papageorghiou, and J Alison Noble.
Self-supervised contrastive video-speech representation learning for ultrasound. In International Conference
on Medical Image Computing and Computer-Assisted Intervention , pp. 534–543. Springer, 2020.
Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative
mixing for contrastive learning. arXiv preprint arXiv:2010.01028 , 2(6), 2020.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot,
Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing
Systems, 33, 2020.
Yechan Kim, Younkwan Lee, and Moongu Jeon. Imbalanced image classiﬁcation with complement cross
entropy. arXiv preprint arXiv:2009.02189 , 2020.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. International Conference on Learning
Representations , 2014.
Bruno Korbar, Du Tran, and Lorenzo Torresani. Cooperative learning of audio and video models from
self-supervised synchronization. arXiv preprint arXiv:1807.00230 , 2018.
Simon Kornblith, Ting Chen, Honglak Lee, and Mohammad Norouzi. Why do better loss functions lead
to less transferable features? In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan (eds.), Advances in Neural Information Processing Systems , volume 34, pp. 28648–28662. Curran
Associates, Inc., 2021.
Chunyuan Li, Xiujun Li, Lei Zhang, Baolin Peng, Mingyuan Zhou, and Jianfeng Gao. Self-supervised
pre-training with hard examples improves visual representations. arXiv preprint arXiv:2012.13493 , 2020a.
Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng Gao.
Eﬃcient self-supervised vision transformers for representation learning. arXiv preprint arXiv:2106.09785 ,
2021.
Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin,
Yong Jae Lee, Houdong Hu, Zicheng Liu, and Jianfeng Gao. Elevater: A benchmark and toolkit for
evaluating language-augmented visual models. arXiv preprint arXiv:2204.08790 , 2022.
Junnan Li, Caiming Xiong, and Steven CH Hoi. Mopro: Webly supervised learning with momentum
prototypes. arXiv preprint arXiv:2009.07995 , 2020b.
Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi. Prototypical contrastive learning of unsupervised
representations. arXiv preprint arXiv:2005.04966 , 2020c.
Yante Li and Guoying Zhao. Intra- and inter-contrastive learning for micro-expression action unit detection.
InProceedings of the 2021 International Conference on Multimodal Interaction , ICMI ’21, pp. 702–706,
New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450384810.
Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching networks for
learning the similarity of graph structured objects. In International Conference on Machine Learning , pp.
3835–3845, 2019.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and
C Lawrence Zitnick. Microsoft COCO: Common objects in context. In European conference on computer
vision, pp. 740–755. Springer, 2014.
15Published in Transactions on Machine Learning Research (07/2023)
Lajanugen Logeswaran and Honglak Lee. An eﬃcient framework for learning sentence representations. In
International Conference on Learning Representations , 2018.
Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 6707–6717,
2020.
Mathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio Feris, James Glass, and Aude Oliva.
Spoken moments: Learning joint audio-visual representations from video descriptions. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 14871–14881, 2021.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding.
arXiv preprint arXiv:1807.03748 , 2018.
Ben Poole, Sherjil Ozair, Aäron van den Oord, Alexander A Alemi, and George Tucker. On variational lower
bounds of mutual information. In NeurIPS Workshop on Bayesian Deep Learning , 2018.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural
language supervision. In International Conference on Machine Learning , pp. 8748–8763. PMLR, 2021.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection
with region proposal networks. IEEE transactions on pattern analysis and machine intelligence , 39(6):
1137–1149, 2016.
Joshua David Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with
hard negative samples. In International Conference on Learning Representations , 2021.
Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A
theoretical analysis of contrastive unsupervised representation learning. In International Conference on
Machine Learning , pp. 5628–5637. PMLR, 2019.
Florian Schroﬀ, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed embedding for face recognition
and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.
815–823, 2015.
Anshul Shah, Suvrit Sra, Rama Chellappa, and Anoop Cherian. Max-margin contrastive learning. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 36, pp. 8220–8230, 2022.
Aravind Srinivas, Michael Laskin, and Pieter Abbeel. CURL: Contrastive unsupervised representations for
reinforcement learning. In International Conference on Machine Learning , pp. 10360–10371, 2020.
Fan-Yun Sun, Jordan Hoﬀmann, Vikas Verma, and Jian Tang. InfoGraph: Unsupervised and semi-supervised
graph-level representation learning via mutual information maximization. In International Conference on
Learning Representations , 2020a.
Yifan Sun, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao Wang, and Yichen Wei.
Circle loss: A uniﬁed perspective of pair similarity optimization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 6398–6407, 2020b.
Tristan Sylvain, Linda Petrini, and Devon Hjelm. Locality and compositionality in zero-shot learning. In
International Conference on Learning Representations , 2020.
Afrina Tabassum, Muntasir Wahed, Hoda Eldardiry, and Ismini Lourentzou. Hard negative sampling strategies
for contrastive representation learning. arXiv preprint arXiv:2206.01197 , 2022.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint
arXiv:1906.05849 , 2019.
16Published in Transactions on Machine Learning Research (07/2023)
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In International
Conference on Learning Representations , 2020a.
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for
good views for contrastive learning. In Advances in Neural Information Processing Systems , 2020b.
Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal of the
Royal Statistical Society: Series B (Statistical Methodology) , 61(3):611–622, 1999.
Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. arXiv preprint arXiv:1907.13625 , 2019.
Laurens van der Maaten and Geoﬀrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning
Research , 9(86):2579–2605, 2008.
Petar Velickovic, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon Hjelm. Deep
Graph Infomax. In International Conference on Learning Representations , 2019.
Feng Wang, Xiang Xiang, Jian Cheng, and Alan Loddon Yuille. Normface: L2 hypersphere embedding for
face veriﬁcation. In Proceedings of the 25th ACM international conference on Multimedia , pp. 1041–1049,
2017.
Feng Wang, Huaping Liu, Di Guo, and Sun Fuchun. Unsupervised representation learning by invariance
propagation. Advances in Neural Information Processing Systems , 33:3510–3520, 2020.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and
uniformity on the hypersphere. In International Conference on Machine Learning , pp. 9929–9939. PMLR,
2020.
Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R Scott. Multi-similarity loss with
general pair weighting for deep metric learning. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 5022–5030, 2019.
Mike Wu, Milan Mosse, Chengxu Zhuang, Daniel Yamins, and Noah Goodman. Conditional negative sampling
for contrastive learning of visual representations. In International Conference on Learning Representations ,
2021.
Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https:
//github.com/facebookresearch/detectron2 , 2019.
Zhirong Wu, Yuanjun Xiong, X Yu Stella, and Dahua Lin. Unsupervised feature learning via non-parametric
instancediscrimination. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,
2018.
Lanling Xu, Jianxun Lian, Wayne Xin Zhao, Ming Gong, Linjun Shou, Daxin Jiang, Xing Xie, and Ji-Rong
Wen. Negative sampling for contrastive representation learning: A review. arXiv preprint arXiv:2206.00212 ,
2022.
Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning
via redundancy reduction. arXiv preprint arXiv:2103.03230 , 2021.
Shaofeng Zhang, Junchi Yan, and Xiaokang Yang. Adaptive contrastive learning of representation by nearest
positive expansion.
Huangjie Zheng and Mingyuan Zhou. Exploiting chain rule and bayes’ theorem to compare probability
distributions. Advances in Neural Information Processing Systems , 34, 2021.
17Published in Transactions on Machine Learning Research (07/2023)
Appendix
A Proofs and detailed derivation
Proof of Property 1 .By deﬁnition, the point-to-point cost c(z1,z2)is always non-negative. Without loss
of generality, we deﬁne it with the Euclidean distance. When equation 6 is true, the expected cost of moving
between a pair of positive samples, as deﬁned as LCAin equation 2, will reach its minimum at 0. When
equation 6 is not true, by deﬁnition we will have LCA>0,i.e.,LCA= 0is possible only if equation 6 is
true.
Proof of Lemma 4.1 .By changing the reference distribution of the expectation from π+
θ(·|x,x0)to
p(·|x0), we can directly re-write the CA loss as:
LCA=Ex∼p(x)Ex+∼π+
θ(·|x,x0)/bracketleftbig
c(fθ(x),fθ(x+))/bracketrightbig
=Ex0Ex,x+∼p(·|x0)/bracketleftBig
−fθ(x)/latticetopfθ(x+)π+
θ(x+|x,x0)
p(x+|x0)/bracketrightBig
,
which complete the proof.
Proof of Lemma 4.2 .Denoting
Z(x) =/integraldisplay
e−d(fθ(x),fθ(x−))p(x−)dx−,
we have
lnπ−
θ(x−|x) =−d(fθ(x),fθ(x−)) + lnp(x−)−lnZ(x).
Thus we have
LCR=Ex∼p(x)Ex−∼π−
θ(·|x)[lnπ−
θ(x−|x)−lnp(x−) + lnZ(x)]
=C1+C2−H(X−|X) (9)
whereC1=Ex∼p(x)Ex−∼π−
θ(·|x)[lnp(x−)]andC2=−Ex∼p(x)lnZ(x). Under the assumption of a uniform
prior onp(x),C1becomes a term that is not related to θ. Under the assumption of a uniform prior on p(z),
wherez=fθ(x), we have
Z(x) =Ex−∼p(x)[e−d(fθ(x),fθ(x−))]
=Ez−∼p(z)[e−(z−−z)T(z−−z)]
∝/integraldisplay
e−(z−−z)T(z−−z)dz−
=√π, (10)
which is also not related to θ. Therefore, under the uniform prior assumption on both p(x)andp(z),
minimizingLCRis the same as maximizing H(X−|X), as well as the same as minimizing I(X,X−).
Proof of Lemma 4.3 .The CL loss can be decomposed as an expected dissimilarity term and a log-sum-exp
term:
LCL:= E
(x,x+,x−
1:M)/bracketleftBigg
−lnefθ(x)/latticetopfθ(x+)/τ
efθ(x)/latticetopfθ(x+)/τ+/summationtext
iefθ(x−
i)/latticetopfθ(x)/τ/bracketrightBigg
=E(x,x+)/bracketleftbigg
−1
τfθ(x)/latticetopfθ(x+)/bracketrightbigg
+ E
(x,x+,x−
1:M)/bracketleftBigg
ln/parenleftBigg
efθ(x)/latticetopfθ(x+)/τ+M/summationdisplay
i=1efθ(x−
i)/latticetopfθ(x)/τ/parenrightBigg/bracketrightBigg
,
18Published in Transactions on Machine Learning Research (07/2023)
where the positive sample x+is independent of xgivenx0and the negative samples x−
iare independent of
x. As the number of negative samples goes to inﬁnity, following Wang & Isola (2020), the normalized CL loss
is decomposed into the sum of the align loss, which describes the contribution of the positive samples, and
the uniform loss, which describes the contribution of the negative samples:
lim
M→∞LCL−lnM=E(x,x+)/bracketleftbigg
−1
τfθ(x)/latticetopfθ(x+)/bracketrightbigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
contribution of positive samples+E
x∼p(x)/bracketleftbigg
ln E
x−∼p(x−)efθ(x−)/latticetopfθ(x)/τ/bracketrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
contribution of negative samples
With importance sampling, the second term in the RHS of the above equation can be further derived into:
E
x∼p(x)/bracketleftbigg
ln E
x−∼p(x−)efθ(x−)/latticetopfθ(x)/τ/bracketrightbigg
=E
x∼p(x)/bracketleftbigg
ln E
x−∼πθ(x−|x)/bracketleftbigg
efθ(x−)/latticetopfθ(x)/τp(x−)
πθ(x−|x)/bracketrightbigg/bracketrightbigg
Apply the Jensen inequality, the second term is decomposed into the negative cost plus a log density ratio:
E
x∼p(x)/bracketleftbigg
ln E
x−∼p(x−)efθ(x−)/latticetopfθ(x)/τ/bracketrightbigg
>E
x∼p(x)/bracketleftbigg
E
x−∼πθ(x−|x)/bracketleftbig
fθ(x−)/latticetopfθ(x)/τ/bracketrightbig/bracketrightbigg
+E
x∼p(x)/bracketleftbigg
E
x−∼πθ(x−|x)/bracketleftbigg
lnp(x−)
πθ(x−|x)/bracketrightbigg/bracketrightbigg
=E
x∼p(x)/bracketleftbigg
E
x−∼πθ(x−|x)/bracketleftbig
fθ(x−)/latticetopfθ(x)/τ/bracketrightbig/bracketrightbigg
−I(X;X−)
Deﬁning the point-to-point cost function between two unit-norm vectors as c(z1,z2) =−z/latticetop
1z2(same as the
Euclidean cost since /bardblz1−z2/bardbl2
2/2 = 1−z/latticetop
1z2), we have
E
x∼p(x)/bracketleftbigg
lnE
x−∼p(x)efθ(x−)/latticetopfθ(x)/τ/bracketrightbigg
+I(X;X−)
>E
x∼p(x)/bracketleftbigg
E
x−∼πθ(x−|x)/bracketleftbig
fθ(x−)/latticetopfθ(x)/τ/bracketrightbig/bracketrightbigg
=−E
x∼p(x)/bracketleftbigg
E
x−∼πθ(x−|x)/bracketleftbig
c(fθ(x−),fθ(x))/τ/bracketrightbig/bracketrightbigg
=LCR.
This concludes the relation between the contribution of the negative samples in CL and that in CACR.
19Published in Transactions on Machine Learning Research (07/2023)
B Additional experimental results
In this section, we provide additional results in our experiments, including ablation studies, and corresponding
qualitative results.
B.1 Additional results with AlexNet and ResNet50 encoder on small-scale datasets
Following benchmark works in contrative learning, we add STL-10 dataset to evaluate CACR in small-scale
experiments. As an additional results on small-scale datasets, we test the performance of CACR two diﬀerent
encoder backbones. Here we strictly follow the same setting of Wang & Isola (2020) and Robinson et al.
(2021), and the results are shown in Table 8 and 9. We can observe with ResNet50 encoder backbone, CACR
with single positive or multiple positive pairs consistently outperform the baselines. Compared with the
results in Table 8, the CACR shows a more clear improvement over the CL baselines.
Table 8: The top-1 classiﬁcation accuracy ( %) of diﬀerent contrastive objectives with SimCLR framework on
small-scale datasets. All methods follow SimCLR setting and apply AlexNet encoder and trained with 200
epochs.
Dataset CL AU-CL HN-CL CACR(K=1) CMC(K=4) CACR(K=4)
CIFAR-10 83.47 83.39 83.67 83.73 85.54 86.54
CIFAR-100 55.41 55.31 55.87 56.52 58.64 59.41
STL-10 83.89 84.43 83.27 84.51 84.50 85.59
Table 9: The top-1 classiﬁcation accuracy ( %) of diﬀerent contrastive objectives with SimCLR framework on
small-scale datasets. All methods follow SimCLR setting and apply a ResNet50 encoder and trained with 400
epochs.
Dataset CL AU-CL HN-CL CACR(K=1) CMC(K=4) CACR(K=4)
CIFAR-10 88.70 88.63 89.02 90.97 90.05 92.89
CIFAR-100 62.00 62.57 62.96 62.98 65.19 66.52
STL-10 84.60 83.81 84.29 88.42 91.40 93.04
B.2 On the eﬀects of conditional distribution
Supplementary studies of CA and CR: As a continuous ablation study shown in Figure 3, we also
conduct similar experiments on CIFAR-100, where we study the evolution of conditional entropy H(X−|X)
w.r.t.the training epoch. The results are shown in Figure 4, and the results of exponential label-imbalanced
data are shown in Figure 5. Similar to the observation on CIFAR-10, shown in Figure 3, we can observe
H(X−|X)is getting maximized as the encoder is getting optimized with these methods, as suggested in
Lemma 4.2. In the right panel, We can observe baseline methods have lower conditional entropy, which
indicates the encoder is less eﬀective in distinguish the nagative samples from query, while CACR consistently
provides better performance than the other methods indicating the better robustness of CACR.
As a qualitative veriﬁcation, we randomly take a query from a mini-batch, and illustrate its positive and
negative samples and their conditional probabilities in Figure 6. As shown, given this query of a dog image,
the positive sample with the largest weight contains partial dog information, indicating the encoder to focus on
texture information; the negatives with larger weights are more related to the dog category, which encourages
the encoder to focus on distinguishing these “hard” negative samples. In total, the weights learned by CACR
enjoy the interpretability compared to the conventional CL.
We study diﬀerent deﬁnition of the conditional distribution. From Table 10, we can observe that the results
are not sensitive to the distance space. In addition, as we change π+to assign larger probability to closer
samples, the results are similar to those using single positive pair (K=1). Moreover, the performance drops if
we changeπ−to assign larger probability to more distant negative samples.
Uniform Attraction and Uniform Repulsion: A degenerated version of CACR
20Published in Transactions on Machine Learning Research (07/2023)
2.53.95.3Nats
(unif)
(CACR (K=4))
(CMC (K=4))
0 25 50 75 100 125 150 175 200
Epoch5.55.96.3Nats (unif)
(CL)
(AU-CL)
(HN-CL)
(CACR (K=1))
2.53.95.3Nats
(unif)
(CACR (K=4))
(CMC (K=4))
0 25 50 75 100 125 150 175 200
Epoch5.55.96.3Nats (unif)
(CL)
(AU-CL)
(HN-CL)
(CACR (K=1))
Figure 4: ( Supplementary to Figure 3 ) Conditional entropy H(X−|X)w.r.t.training epoch on CIFAR-100
(left) and linear label-imbalanced CIFAR-100 ( right). The maximal possible conditional entropy is indicated
by a dotted line.
2.53.95.3Nats
(unif)
(CACR (K=4))
(CMC (K=4))
0 25 50 75 100 125 150 175 200
Epoch5.55.96.3Nats (unif)
(CL)
(AU-CL)
(HN-CL)
(CACR (K=1))
2.53.95.3Nats
(unif)
(CACR (K=4))
(CMC (K=4))
0 25 50 75 100 125 150 175 200
Epoch5.55.96.3Nats (unif)
(CL)
(AU-CL)
(HN-CL)
(CACR (K=1))
Figure 5: ( Supplementary to Figure 3 ) Conditional entropy H(X−|X)w.r.t.training epoch on exponential
label-imbalanced CIFAR-10 ( left) and CIFAR-100 ( right). The maximal possible conditional entropy is
indicated by a dotted line.
To reinforce the necessity of the contrasts within positives and negatives before the attraction and repulsion,
we introduce a degenerated version of CACR here, where the conditional distributions are forced to be
uniform. Remind c(z1,z2)as the point-to-point cost of moving between two vectors z1andz2,e.g., the
squared Euclidean distance /bardblz1−z2/bardbl2or the negative inner product −zT
1z2. In the same spirit of equation 1,
we have considered a uniform attraction and uniform repulsion (UAUR) without doubly contrasts within
positive and negative samples, whose objective is
min
θ/braceleftbig
Ex0∼pdata(x)E/epsilon10,/epsilon1+∼p(/epsilon1)/bracketleftbig
c(fθ(x),fθ(x+))/bracketrightbig
−Ex,x−∼p(x)/bracketleftbig
c(fθ(x),fθ(x−))/bracketrightbig/bracerightbig
. (11)
The intuition of UAUR is to minimize/maximize the expected cost of moving the representations of posi-
tive/negative samples to that of the query, with the costs of all sample pairs being uniformly weighted. While
equation 1 has been proven to be eﬀective for representation learning, our experimental results do not ﬁnd
equation 11 to perform well, suggesting that the success of representation learning is not guaranteed by
uniformly pulling positive samples towards and pushing negative samples away from the query.
Distinction between CACR and UAUR : Compared to UAUR in equation 11 that uniformly weighs
diﬀerent pairs, CACR is distinct in considering the dependency between samples: as the latent-space distance
between the query and its positive sample becomes larger, the conditional probability becomes higher,
encouraging the encoder to focus more on the alignment of this pair. In the opposite, as the distance between
the query and its negative sample becomes smaller, the conditional probability becomes higher, encouraging
the encoder to push them away from each other.
21Published in Transactions on Machine Learning Research (07/2023)
/gid00018/gid00048/gid00032/gid00045/gid00052/gid00005/gid00028/gid00047/gid00028
/gid00017/gid00042/gid00046/gid00036/gid00047/gid00036/gid00049/gid00032/gid00001/gid00046/gid00028/gid00040/gid00043/gid00039/gid00032/gid00046/gid00021/gid00042/gid00043/gid00001/gid00135/gid00001/gid00015/gid00032/gid00034/gid00028/gid00047/gid00036/gid00049/gid00032/gid00001/gid00046/gid00028/gid00040/gid00043/gid00039/gid00032/gid00046/gid00001 /gid00003/gid00042/gid00047/gid00047/gid00042/gid00040/gid00001/gid00135/gid00001/gid00015/gid00032/gid00034/gid00028/gid00047/gid00036/gid00049/gid00032/gid00001/gid00046/gid00028/gid00040/gid00043/gid00039/gid00032/gid00046/gid00001
Figure 6: Illustration of positive/negative samples and their corresponding weights. ( Left) For a query augmented
from the original dog image, 4 positive samples are shown, with their weights visualized as the blue distribution.
(Right) The sampling weights for negative samples are visualized as the red distribution; we visualize 4 negative
samples with the highest and 4 with the lowest weights, with their original images shown below.
Table 10: Linear classiﬁcation performance (%) of diﬀerent variants of conditional probability. This experiment
is done on CIFAR-10, with K= 4and mini-batch size M= 128.
π+
e+dt+(fθ(x),fθ(x+))p(x+|x0)/integraltext
e+dt+(fθ(x),fθ(x+))p(x+|x0)dx+e−dt+(fθ(x),fθ(x+))p(x+|x0)/integraltext
e−dt+(fθ(x),fθ(x+))p(x+|x0)dx+
π−e−dt−(fθ(x),fθ(x−))p(x−)/integraltext
e−dt−(fθ(x),fθ(x−))p(x−)dx−86.48 83.91
e+dt−(fθ(x),fθ(x−))p(x−)/integraltext
e+dt−(fθ(x),fθ(x−))p(x−)dx−79.46 74.91
In order to further explore the eﬀects of the conditional distribution, we conduct an ablation study to
compare the performance of diﬀerent variants of CACR with/without conditional distributions. Here,
we compare 4 conﬁgurations of CACR ( K= 4):(i)CACR with both positive and negative conditional
distribution; (ii)CACR without the positive conditional distribution; (iii)CACR without the negative
conditional distribution; (iv)CACR without both positive and negative conditional distributions, which
refers to UAUR model (see Equation 11). As shown in Table 11, when discarding the positive conditional
distribution, the linear classiﬁcation accuracy slightly drops. As the negative conditional distribution is
discarded, there is a large performance drop compared to the full CACR objective. With the modeling
of neither positive nor negative conditional distribution, the UAUR shows a continuous performance drop,
suggesting that the success of representation learning is not guaranteed by uniformly pulling positive samples
closer and pushing negative samples away. The comparison between these CACR variants shows the necessity
of the conditional distribution.
Table 11: Linear classiﬁcation performance (%) of diﬀerent variants of our method. “CACR” represents
the normal CACR conﬁguration, “w/o π+
θ” means without the positive conditional distribution, “w/o π−
θ”
means without the negative conditional distribution. “UAUR” indicates the uniform cost (see the model we
discussed in Equation 11), i.e.without both positive and negative conditional distribution. This experiment
is done on all small-scale datasets, with K= 4and mini-batch size M= 128.
Methods CIFAR-10 CIFAR-100 STL-10
CACR 85.94 59.51 85.59
w/oπ+
θ85.22 58.74 85.06
w/oπ−
θ78.49 47.88 72.94
UAUR 77.17 44.24 71.88
22Published in Transactions on Machine Learning Research (07/2023)
As qualitative illustrations, we randomly ﬁx one mini-batch, and randomly select one sample as the query.
Then we extract the features with the encoder trained with CL loss and CACR ( K= 1) loss at epochs 1,
20, and 200, and visualize the (four) positives and negatives in the embedding space with t-SNE (van der
Maaten & Hinton, 2008). For more clear illustration, we center the query in the middle of the plot and only
show samples appearing in the range of [−10,10]on bothxandyaxis. The results are shown in Figure 7(c),
from which we can ﬁnd that as the the encoder is getting trained, the positive samples are aligned closer
and the negative samples are pushed away for both methods. Compared to the encoder trained with CL, we
can observe CACR shows better performance in achieving this goal. Moreover, we can observe the distance
between any two data points in the plot is more uniform, which conﬁrms that CACR shows better results in
the maximization of the conditional entropy H(X−|X).
10.0
 7.5
 5.0
 2.5
 0.0 2.5 5.0 7.5 10.010.0
7.5
5.0
2.5
0.02.55.07.510.0
 Query
Positive
Negative
10.0
 7.5
 5.0
 2.5
 0.0 2.5 5.0 7.5 10.010.0
7.5
5.0
2.5
0.02.55.07.510.0
10.0
 7.5
 5.0
 2.5
 0.0 2.5 5.0 7.5 10.010.0
7.5
5.0
2.5
0.02.55.07.510.0
10.0
 7.5
 5.0
 2.5
 0.0 2.5 5.0 7.5 10.010.0
7.5
5.0
2.5
0.02.55.07.510.0
 Query
Positive
Negative
(a) Epoch 1
10.0
 7.5
 5.0
 2.5
 0.0 2.5 5.0 7.5 10.010.0
7.5
5.0
2.5
0.02.55.07.510.0
 (b) Epoch 20
10.0
 7.5
 5.0
 2.5
 0.0 2.5 5.0 7.5 10.010.0
7.5
5.0
2.5
0.02.55.07.510.0
 (c) Epoch 200
Figure 7: The t-SNE visualization of the latent space at diﬀerent training epochs, learned by CL loss ( top)
and CACR loss ( bottom). The picked query is marked in green, with its positive samples marked in blue
and its negative samples marked in red. The circle with radius t−is shown as the black dashed line. As the
encoder gets trained, we can observe the positive samples are aligned closer to the query (Property 1), and the
conditional diﬀerential entropy H(X−|X)is progressively maximized, driving the distances d(fθ(x),fθ(x−))
towards uniform (Lemma 4.2).
B.3 Ablation study
On the eﬀects of negative sampling size: We investigate the model performance and robustness with
diﬀerent sampling size by varying the mini-batch size used in the training. On all the small-scale datasets, the
mini-batches are applied with size 64, 128, 256, 512, 768 and the corresponding linear classiﬁcation results are
shown in Figure 8. From this ﬁgure, we can see that CACR ( K= 4) consistently achieves better performance
than other objectives. For example, when mini-batch size is 256, CACR ( K= 4) outperforms CMC by about
0.4%-1.2%. CACR ( K= 1) shows better performance in most of the cases, while slightly underperforms
23Published in Transactions on Machine Learning Research (07/2023)
than the baselines with mini-batch size 64. A possible explanation could be the estimation of the conditional
distribution needs more samples to provide good guidance for the encoder.
84.585.085.586.0Accuracy(%)
CMC(K=4) CACR(K=4)
64128 256 512 768
Batch Size82.583.083.584.0Accuracy(%)
 CL
AU-CLHN-CL
CACR(K=1)
(a) CIFAR-10
58.058.559.059.5Accuracy(%)
CMC(K=4) CACR(K=4)
64128 256 512 768
Batch Size55.055.556.056.5Accuracy(%)
 CL
AU-CLHN-CL
CACR(K=1) (b) CIFAR-100
84.084.585.085.586.0Accuracy(%)
CMC(K=4) CACR(K=4)
64128 256 512 768
Batch Size828384Accuracy(%)
 CL
AU-CLHN-CL
CACR(K=1) (c) STL-10
Figure 8: The linear classiﬁcation results of training with diﬀerent sampling size on small-scale datasets. The
training batch size is proportional to the negative sampling size.
On the eﬀects of positive sampling size: We conduct experiments to investigate the model performance
with diﬀerent positive sampling size by using diﬀerent Kvalues in the pretraining: K∈{1,2,4,6,8,10}on
CIFAR-10/100 and K∈{1,2,3,4}on ImageNet-1K. Similar to our experiment setting, in 200 epochs, we
apply AlexNet encoder on CIFAR-10 and CIFAR-100 and apply ResNet50 encoder on ImageNet-1K. Shown
in Figure 9, we can observe as Kincreases, the linear classiﬁcation accuracy increases accordingly.
1 2 4 6 8 10
K84.084.585.085.586.086.587.087.5Accuracy (%)
(a) CIFAR-10
1 2 4 6 8 10
K56.557.057.558.058.559.059.560.060.5Accuracy (%)
 (b) CIFAR-100
1 2 3 4
K69.669.870.070.270.4Accuracy (%)
 (c) ImageNet
Figure 9: The linear classiﬁcation results of training with diﬀerent positive sampling size on CIFAR-10,
CIFAR-100 and ImageNet-1K. An AlexNet encoder is applied on CIFAR-10 and CIFAR-100; ResNet50
encoder is applied on ImageNet.
On the eﬀects of hyper-parameter t+,t−:Remind in the deﬁnition of positive and negative conditional
distribution, two hyper-parameters t+andt−are involved as following:
π+
θ(x+|x,x0) :=et+/bardblfθ(x)−fθ(x+)/bardbl2p(x+|x0) /integraltext
et+/bardblfθ(x)−fθ(x+)/bardbl2p(x+|x0)dx+;π−
θ(x−|x) :=e−t−/bardblfθ(x)−fθ(x−)/bardbl2p(x−) /integraltext
e−t−/bardblfθ(x)−fθ(x−)/bardbl2p(x−)dx−.
In this part, we investigate the eﬀects of t+andt−on representation learning performance on small-scale
datasets, with mini-batch size 768 ( K= 1) and 128 ( K= 4) as an ablation study. We search in a range
{0.5,0.7,0.9,1.0,2.0,3.0}. The results are shown in Table 12 and Table 13.
As shown in these two tables, from Table 12, we observe the CACR shows better performance with smaller
values fort+. Especially when t+increases to 3.0, the performance drops up to about 1.9% on CIFAR-100.
For analysis, since we have K= 4positive samples for the computation of positive conditional distribution,
24Published in Transactions on Machine Learning Research (07/2023)
Table 12: The classiﬁcation accuracy(%) of CACR ( K= 4, M = 128) with diﬀerent hyper-parameters t+on
small-scale datasets.
Method Dataset 0.5 0.7 0.9 1.0 2.0 3.0
CACR (K= 4)CIFAR-10 86.07 85.78 85.90 86.54 84.85 84.76
CIFAR-100 59.47 59.61 59.41 59.41 57.82 57.55
STL-10 85.9085.91 85.81 85.59 85.65 85.14
using a large value for t+could result in an over-sparse conditional distribution, where the conditional
probability is dominant by one or two positive samples. This also explains why the performance when
t+= 3.0is close to the classiﬁcation accuracy of CACR ( K= 1).
Similarly, from Table 13, we can see that a small value for t−will lead to the degenerated performance. Here,
since we are using mini-batches of size 768 ( K= 1) and 128 ( K= 4), a small value for t−will ﬂatten the
weights of the negative pairs and make the conditional distribution closer to a uniform distribution, which
explains why the performance when t−= 0.5is close to those without modeling π−
θ. Based on these results,
the values of t+∈[0.5,1.0]andt−∈[0.9,2.0]could be good empirical choices according to our experiment
settings on these datasets.
B.4 Additional comparisons
In this part we provide more comparisons with baseline methods. For small-scale experiments, we still
compare with contrastive learning methods, conventional CL loss, align-uniform loss, and hard negative
sampling CL loss. For large-scale experiments, we continue to compare with contrastive learning loss on
ImageNet-100 and ImagNet-1K with MoCov2 framework, and provide comparisons with SOTAs pretrained
with diﬀerent epochs.
Training eﬃciency on small-scale datasets: On CIFAR-10, CIFAR-100 and STL-10, we pretrained
AlexNet encoder in 200 epochs and save linear classiﬁcation results with learned representations every 10
epochs. Shown in Figure 10, CACR consistently outperforms the other methods in linear classiﬁcation with the
learned representations at the same epoch, indicating a superior learning eﬃciency of CACR. Correspondingly,
we also evaluate the GPU time of CACR loss with diﬀerent choices of K, as shown in Table 14.
Comparison with contrastive learning methods on ImageNet: For large-scale experiments, following
the convention, we adapt all methods into the MoCo-v2 framework and pre-train a ResNet50 encoder in
200 epochs with mini-batch size 128/256 on ImageNet-100/ImageNet-1k. Table 15 summarizes the results
of linear classiﬁcation on these two large-scale datasets. Similar to the case on small-scale datasets, CACR
consistently shows better performance, improving the baselines at least by 1.74% on ImageNet-100 and 0.71%
on ImageNet-1K. In MoCo-v2, with multiple positive samples, CACR improves the baseline methods by
2.92% on ImageNet-100 and 2.75% on ImageNet-1K. It is worth highlighting that the improvement of CACR
is more signiﬁcant on these large-scale datasets, where the data distribution could be much more diverse
compared to these small-scale ones. This is not surprising, as according to our theoretical analysis, CACR’s
Table 13: The classiﬁcation accuracy(%) of CACR ( K= 1, M = 768) and CACR ( K= 4, M = 128) with
diﬀerent hyper-parameters t−on small-scale datasets.
Methods Dataset 0.5 0.7 0.9 1.0 2.0 3.0
CACR (K= 1)CIFAR-10 81.66 82.40 83.07 82.74 83.73 83.11
CIFAR-100 51.42 52.81 53.36 54.20 56.21 56.52
STL-10 80.37 81.47 84.46 82.16 84.21 84.51
CACR (K= 4)CIFAR-10 85.67 86.19 86.54 86.41 85.94 85.69
CIFAR-100 58.17 58.63 59.37 59.35 59.41 59.31
STL-10 83.81 84.42 84.71 85.25 85.59 85.41
25Published in Transactions on Machine Learning Research (07/2023)
758085Accuracy(%)
CMC(K=4) CACR(K=4)
0 50 100 150 200
Epoch65707580Accuracy(%)
 CL
AU-CLHN-CL
CACR(K=1)
(a) CIFAR-10
45505560Accuracy(%)
CMC(K=4) CACR(K=4)
0 50 100 150 200
Epoch40455055Accuracy(%)
 CL
AU-CLHN-CL
CACR(K=1) (b) CIFAR-100
77.580.082.585.0Accuracy(%)
CMC(K=4) CACR(K=4)
0 50 100 150 200
Epoch68737883Accuracy(%)
 CL
AU-CLHN-CL
CACR(K=1) (c) STL-10
Figure 10: Comparison of training eﬃcientcy: Linear classiﬁcation with learned representations w.r.t.training
epoch on CIFAR-10, CIFAR-100 and STL-10.
Table 14: GPU time (s) per iteration of CACR w.r.t.diﬀerent Kon CIFAR-10 with AlexNet framework (mini-batch
size is 128), tested on Tesla-v100 GPU.
K 1 2 4 6 8 10
GPU time (s) / iteration 0.0021 0.0026 0.0035 0.0045 0.0054 0.0064
double-contrast within samples enhances the eﬀectiveness of the encoder’s optimization. Moreover, we can
see CACR ( K= 1) shows a clear improvement over HN-CL. A possible explanation is that although both
increasing the negative sample size and selecting hard negatives are proposed to improve the CL loss, the
eﬀectiveness of hard negatives is limited when the sampling size is increased over a certain limit. As CACR
targets to repel the negative samples away, the conditional distribution still eﬃciently guides the repulsion
when the sampling size becomes large.
Table 15: Comparison with contrastive learning
methods: Top-1 classiﬁcation accuracy ( %) of diﬀer-
entcontrastivelearningobjectivesonMoCo-v2frame-
work and ResNet50 encoder, pretrained on ImageNet-
1K dataset with 200 epochs. The results from paper
or Github page are marked by ⋆.
Methods ImageNet-100 ImageNet-1K
MoCov2 (CL) 77.54⋆67.50⋆
AU-CL 77.66⋆67.69⋆
HN-CL 76.34 67 .41
CACR (K= 1)79.40 68.40
CMC (CL, K= 4)78.84 69.45
CACR (K= 4)80.46 70.35Table 16: Comparison with state-of-the-arts on linear
probe classiﬁcation accuracy, pretrained with diﬀer-
ent epochs, using ResNet50 encoder backbone on
ImageNet-1k.
Epochs 100 200 400 800 1000
BYOL 66.5 70.6 73.2 74.3-
BarlowTwins - - - - 73.2
SWAV 66.5 69.1 70.7 71.8 -
Simsiam 68.1 70.0 70.8 71.3 -
SimCLR 66.5 68.3 69.8 70.4 71.7
MoCov2 67.4 69.9 71.0 72.2 -
FNC (multi-crop) 70.4- - - 74.4
CACR 68.370.4 73.8 74.074.4
Comparison with other SOTAs: Besides the methods using contrastive loss, we continue to compare
with the self-supervised learning methods like BYOL, SWaV, SimSiam, etc.that do not involve the contrasts
with negative samples. Table 16 provides more detailed comparison with all state-of-the-arts in diﬀerent
epochs and could better support the eﬀectiveness of CACR: We can observe CACR achieves competitive
results and generally outperforms most of SOTAs at the same epoch in linear classiﬁcation tasks. We also
compare the computation complexity. Table 17 reports computation complexity to provide quantitative
results in terms of positive number K, where we can observe the computation cost of CACR slightly increases
as K increase, but does not increase as that when using multi-positives in CL loss.
26Published in Transactions on Machine Learning Research (07/2023)
Table 17: GPU time (s) per iteration of diﬀerent loss on MoCov2 framework, tested on 32G-V100 GPU
Methods CL AU-CL HN-CL CACR(K=1) CL (K=4) CACR(K=2) CACR(K=3) CACR(K=4)
Batch size M 256 256 256 256 64 128 64 64
# samples (KxM) / iteration 256 256 256 256 256 256 192 256
GPU time (s) / iteration 0.837 0.840 0.889 0.871 3.550 0.996 1.017 1.342
MethodResNet50 ViT-B/16
FT Lin-cls FT Lin-cls
SimCLRv2 77.2 71.7 83.1 73.9
MoCov3 77.0 73.8 83.2 76.5
CACR 78.174.783.4 76.8
SWAV†77.8 75.3 82.8 71.6
CACR†78.475.383.4 77.1
Table 18: Comparison with state-of-the-arts on ﬁne-tuning and linear probing classiﬁcation accuracy (%),
pre-trained using ResNet50 and ViT-Base/16 encoder backbone on ImageNet-1k.†indicates using SWAV
multi-crops.
Comparison with advanced architectures: Beyond the conventional evaluation on linear probing,
recent self-supervised learning methods use advanced encoder architecture such as Vision Transformers
(ViT) (Dosovitskiy et al., 2020), and are evaluated with end-to-end ﬁne-tuning. We incorporate these
perspectives with CACR for a complete comparison. Table 18 provides a comparison with the state-of-the-arts
using ResNet50 and ViT-Base/16 as backbone, where we follow their experiment settings and pre-train
ResNet50 with 800 epochs and ViT-B/16 with 300 epochs. We can observe CACR generally outperforms
these methods in both ﬁne-tuning and linear probing classiﬁcation tasks.
CLIP Radford et al. (2021) CLIP-reproduced CACR
19.8 19.2 22.7
Table 19: Top-1 zero-shot classiﬁcation accuracy (%) on ImageNet1K, pre-trained using ResNet50 on CC3M dataset.
Multi-modal contrastive learning: Besides self-supervised learning on vision tasks, we follow CLIP Rad-
ford et al. (2021) to evaluate CACR on multi-modal representation learning. We compare CACR’s performance
with CLIP, with our reproduced result and the results reported in Li et al. (2022) in Table 19. All methods
are pre-trained on CC3M dataset with ResNet50 backbone for 32 epochs. We can observe CACR surpasses
CLIP by 2.9% in terms of zero-shot accuracy on ImageNet.
B.5 Connection to other representation learning methods
Results of diﬀerent cost metrics
Recall that the deﬁnition of the point-to-point cost metric is usually set as the quadratic Euclidean distance:
c(fθ(x),fθ(y)) =||fθ(x)−fθ(y)||2
2. (12)
In practice, the cost metric deﬁned in our method is ﬂexible to be any valid metrics. Here, we also investigate
the performance when using the Radial Basis Function (RBF) cost metrics:
cRBF(fθ(x),fθ(y)) =−e−t||fθ(x)−fθ(y)||2
2, (13)
wheret∈R+is the precision of the Gaussian kernel. With this deﬁnition of the cost metric, our method is
closely related to the baseline method AU-CL (Wang & Isola, 2020), where the authors calculate pair-wise
RBF cost for the loss w.r.t.negative samples. Following Wang & Isola (2020), we replace the cost metric
27Published in Transactions on Machine Learning Research (07/2023)
when calculate the negative repulsion cost with the RBF cost and modify ˆLCRas:
ˆLCR−RBF:= ln/bracketleftbig1
MM/summationdisplay
i=1/summationdisplay
j/negationslash=ie−dt−(fθ(xi),fθ(xj))/summationtext
j/prime/negationslash=ie−dt−(fθ(xi),fθ(xj/prime))×cRBF(fθ(xi),fθ(xj))/bracketrightbig(14)
= ln/bracketleftbig1
MM/summationdisplay
i=1/summationdisplay
j/negationslash=ie−dt−(fθ(xi),fθ(xj))/summationtext
j/prime/negationslash=ie−dt−(fθ(xi),fθ(xj/prime))×e−t/bardblfθ(xi)−fθ(xj)/bardbl2/bracketrightbig.
Here the negative cost is in log scale for numerical stability. When using the RBF cost metric, we use the
same setting in the previous experiments and evaluate the linear classiﬁcation on all small-scale datasets.
The results of using Euclidean and RBF cost metrics are shown in Table 20. From this table, we see that
both metrics achieve comparable performance, suggesting the RBF cost is also valid in our framework. In
CACR, the cost metric measures the cost of diﬀerent sample pairs and is not limited on speciﬁc formulations.
More favorable cost metrics can be explored in the future.
Table 20: The classiﬁcation accuracy ( %) of CACR ( K= 1) and CACR ( K= 4) with diﬀerent cost metrics
on CIFAR-10, CIFAR-100 and STL-10. Euclidean indicates the cost deﬁned in 12, and RBF indicates the
cost metrics deﬁned in 13.
Methods Cost Metric CIFAR-10 CIFAR-100 STL-10
CACR (K= 1)Euclidean 83.73 56.21 83.55
RBF 83.08 55.90 84.20
CACR (K= 4)Euclidean 85.94 59.41 85.59
RBF 86.20 58.81 85.80
Discussion: Relation to triplet loss CACR is also related to the widely used triplet loss (Schroﬀ et al.,
2015; Sun et al., 2020b). A degenerated version of CACR where the conditional distributions are all uniform
can be viewed as triplet loss, while underperform the proposed CACR, as discussed in Section B.2. In the
view of triplet loss, CACR is dealing with the margin between expected positive pair similarity and negative
similarity:
LCACR = [Eπt+(x+|x)[c(x,x+)]−Eπt−(x−|x)[c(x,x−)] +m]+
whichdegeneratestothegenerictripletlossiftheconditionaldistributiondegeneratestoauniformdistribution:
LUAUR = [Ep(x+)[c(x,x+)]−Ep(x−)[c(x,x−)] +m]+= [c(x,x+)−c(x,x−) +m]+
This degeneration also highlights the importance of the Bayesian derivation of the conditional distribution.
The experimental results of the comparison between CACR and the degenerated uniform version (equivalent
to generic triplet loss) are presented in Table 11.
Moreover, CACR loss can degenerate to a triplet loss with hard example mining if πt+(x+|x)andπt+(x+|x)
are suﬃciently concentrated, where the density shows a very sharp peak:
LCACR = [max(c(x,x+))−min(c(x,x−)) +m]+
which corresponds to the loss shown in Schroﬀ et al. (2015). As shown in Table 12 and 13, when varying
t+andt−to sharpen/ﬂatten the conditional distributions. Based on our observations, when t+= 3and
t−= 3, the conditional distributions are dominated by 1-2 samples, where CACR can be regarded as the
above-mentioned triplet loss, and this triplet loss with hard mining slightly underperforms CACR. From
these views, CACR provides a more general form to connect the triplet loss. Meanwhile, it is interesting to
notice CACR explains how triplet loss is deployed in the self-supervised learning scenario.
Relation to CT . The CT framework of Zheng & Zhou (2021) is primarily focused on measuring the diﬀerence
between two diﬀerent distributions, which are referred to as the source and target distributions, respectively.
28Published in Transactions on Machine Learning Research (07/2023)
It deﬁnes the expected CT cost from the source to target distributions as the forward CT, and that from the
target to source as the backward CT. Minimizing the combined backward and forward CT cost, the primary
goal is to optimize the target distribution to approximate the source distribution with both mode-covering
and mode-seeking properties. In CACR, we did not ﬁnd any performance boost by modeling the reverse
conditional transport, since the marginal distributions of xandx+are the same and these of xandx−are
also the same, there is no need to diﬀerentiate the transporting directions. In addition, the primary goal
of CACR is not to regenerate the data but to learn fθ(·)that can provide good latent representations for
downstream tasks.
29Published in Transactions on Machine Learning Research (07/2023)
C Experiment details
On small-scale datasets, all experiments are conducted on a single GPU, including NVIDIA 1080 Ti and
RTX 3090; on large-scale datasets, all experiments are done on 8 Tesla-V100-32G GPUs.
C.1 Small-scale datasets: CIFAR-10, CIFAR-100, and STL-10
For experiments on CIFAR-10, CIFAR-100, and STL-10, we use the following conﬁgurations:
•Data Augmentation : We strictly follow the standard data augmentations to construct positive and
negative samples introduced in prior works in contrastive learning (Wu et al., 2018; Tian et al., 2019;
Hjelm et al., 2018; Bachman et al., 2019; Chuang et al., 2020; He et al., 2020; Wang & Isola, 2020).
The augmentations include image resizing, random cropping, ﬂipping, color jittering, and gray-scale
conversion. We provide a Pytorch-style augmentation code in Algorithm 1, which is exactly the same
as the one used in Wang & Isola (2020).
Algorithm 1 PyTorch-like Augmentation Code on CIFAR-10, CIFAR-100 and STL-10
import torchvision.transforms as transforms
# CIFAR-10 Transformation
def transform_cifar10():
return transforms.Compose([
transforms.RandomResizedCrop(32, scale=(0.2, 1)),
transforms.RandomHorizontalFlip(),# by default p=0.5
transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),
transforms.RandomGrayscale(p=0.2),
transforms.ToTensor(), # normalize to value in [0,1]
transforms.Normalize(
(0.4914, 0.4822, 0.4465),
(0.2023, 0.1994, 0.2010),
)
])
# CIFAR-100 Transformation
def transform_cifar100():
return transforms.Compose([
transforms.RandomResizedCrop(32, scale=(0.2, 1)),
transforms.RandomHorizontalFlip(),# by default p=0.5
transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),
transforms.RandomGrayscale(p=0.2),
transforms.ToTensor(), # normalize to value in [0,1]
transforms.Normalize(
(0.5071, 0.4867, 0.4408),
(0.2675, 0.2565, 0.2761),
)
])
# STL-10 Transformation
def transform_stl10():
return transforms.Compose([
transforms.RandomResizedCrop(64, scale=(0.08, 1)),
transforms.RandomHorizontalFlip(),# by default p=0.5
transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),
transforms.RandomGrayscale(p=0.2),
transforms.ToTensor(), # normalize to value in [0,1]
transforms.Normalize(
(0.4409, 0.4279, 0.3868),
(0.2683, 0.2610, 0.2687),
)
])
•Feature Encoder : Following the experiments in Wang & Isola (2020), we use an AlexNet-based
encoder as the feature encoder for these three datasets, where encoder architectures are the same
30Published in Transactions on Machine Learning Research (07/2023)
Table 21: The 100 randomly selected classes from ImageNet forms the ImageNet-100 dataset. These classes
are the same as (Wang & Isola, 2020; Tian et al., 2019).
ImageNet-100 Classes
n02869837 n01749939 n02488291 n02107142 n13037406 n02091831 n04517823 n04589890 n03062245 n01773797
n01735189 n07831146 n07753275 n03085013 n04485082 n02105505 n01983481 n02788148 n03530642 n04435653
n02086910 n02859443 n13040303 n03594734 n02085620 n02099849 n01558993 n04493381 n02109047 n04111531
n02877765 n04429376 n02009229 n01978455 n02106550 n01820546 n01692333 n07714571 n02974003 n02114855
n03785016 n03764736 n03775546 n02087046 n07836838 n04099969 n04592741 n03891251 n02701002 n03379051
n02259212 n07715103 n03947888 n04026417 n02326432 n03637318 n01980166 n02113799 n02086240 n03903868
n02483362 n04127249 n02089973 n03017168 n02093428 n02804414 n02396427 n04418357 n02172182 n01729322
n02113978 n03787032 n02089867 n02119022 n03777754 n04238763 n02231487 n03032252 n02138441 n02104029
n03837869 n03494278 n04136333 n03794056 n03492542 n02018207 n04067472 n03930630 n03584829 n02123045
n04229816 n02100583 n03642806 n04336792 n03259280 n02116738 n02108089 n03424325 n01855672 n02090622
as those used in the corresponding experiments in Tian et al. (2019) and Wang & Isola (2020).
Moreover, we also follow the setups in Robinson et al. (2021) and test the performance of CACR
with a ResNet50 encoder (results are shown in Table 9).
•Model Optimization : We apply the mini-batch SGD with 0.9 momentum and 1e-4 weight decay.
The learning rate is linearly scaled as 0.12 per 256 batch size (Goyal et al., 2017). The optimization is
done over 200 epochs, and the learning rate is decayed by a factor of 0.1 at epoch 155, 170, and 185.
•Parameter Setup : On CIFAR-10, CIFAR-100, and STL-10, we follow Wang & Isola (2020) to set
the training batch size as M= 768for baselines. The hyper-parameters of CL, AU-CL⋆, and HN-CL⋆
are set according to the original paper or online codes. Speciﬁcally, the temperature parameter of CL
isτ= 0.19, the hyper-parameters of AU-CL are t= 2.0,τ= 0.19, and the hyper-parameter of HN-CL
areτ= 0.5,β= 1.0⋆, which shows the best performance according to our tuning. For CMC and
CACR with multiple positives, the positive sampling size is K= 4. To make sure the performance
is not improved by using more samples, the training batch size is set as M= 128. For CACR, in
both single and multi-positive sample settings, we set t+= 1.0for all small-scale datasets. As for
t−, for CACR ( K= 1),t−is 2.0, 3.0, and 3.0 on CIFAR-10,CIFAR100, and STL-10, respectively.
For CACR ( K= 4),t−is 0.9, 2.0, and 2.0 on CIFAR-10, CIFAR100, and STL-10, respectively. For
further ablation studies, we test t+andt−with the search in the range of [0.5,0.7,0.9,1.0,2.0,3.0],
and we test all the methods with several mini-batch sizes M∈{64,128,256,512,768}.
•Evaluation : The feature encoder is trained with the default built-in training set of the datasets. In
the evaluation, the feature encoder is frozen, and a linear classiﬁer is trained and tested on the default
training set and validation set of each dataset, respectively. Following Wang & Isola (2020), we train
the linear classiﬁer with Adam optimizer over 100 epochs, with β1= 0.5,β2= 0.999,/epsilon1= 10−8, and
128 as the batch size. The initial learning rate is 0.001 and decayed by a factor of 0.2 at epoch 60 and
epoch 80. Extracted features from “fc7” are employed for the evaluation. For the ResNet50 setting
in Robinson et al. (2021), the extracted features are from the encoder backbone with dimension 2048.
C.2 Large-scale datasets
For large-scale datasets, the Imagenet-1K is the standard ImageNet dataset that has about 1.28 million images
of 1000 classes. The ImageNet-100 contains randomly selected 100 classes from the standard ImageNet-1K
dataset, and the classes used here are the same with Tian et al. (2019) and Wang & Isola (2020), listed in
Table 21. For pretraining with less curated data⋆, considering Webvision v1 and ImageNet-22k respectively
have 2.4 and 14.2 million images, we decrease the training epoch to 1/2 and 1/10 when pretraining on these
two datasets as done in Li et al. (2021). We follow the standard settings in these works and describe the
experiment conﬁgurations as follows:
⋆https://github.com/SsnL/align_uniform
⋆https://github.com/joshr17/HCL
⋆Please refer to the original paper for the speciﬁc meanings of the hyper-parameter in baselines.
⋆https://data.vision.ee.ethz.ch/cvl/webvision/dataset2017.html
31Published in Transactions on Machine Learning Research (07/2023)
Algorithm 2 PyTorch-like Augmentation Code on ImageNet-100 and ImageNet-1K
import torchvision.transforms as transforms
# ImageNet-100 and ImageNet-1K Transformation
# MoCo v2’s aug: similar to SimCLR https://arxiv.org/abs/2002.05709
def transform_imagenet():
return transforms.Compose([
transforms.RandomResizedCrop(224, scale=(0.2, 1.)),
transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)
], p=0.8),
transforms.RandomGrayscale(p=0.2),
transforms.RandomApply([moco.loader.GaussianBlur([.1, 2.])], p=0.5),
transforms.RandomHorizontalFlip(),
transforms.ToTensor(),
transforms.Normalize(mean=[0.485, 0.456, 0.406],
std=[0.229, 0.224, 0.225])
])
•Data Augmentation : Following Tian et al. (2019; 2020b); Wang & Isola (2020); Chuang et al.
(2020); He et al. (2020), and Chen et al. (2020c), the data augmentations are the same as the standard
protocol, including resizing, 1x image cropping, horizontal ﬂipping, color jittering, and gray-scale
conversion with speciﬁc probability. The full augmentation combination is shown in Algorithm 2.
•Feature Encoder : For the main experiments, we apply MoCo-v3 setting, where the framework
consists of a teacher and a student encoder. The teacher encoder follows a EMA updating strategy
and all experiment settings follows the MoCo-v3 paper (Chen et al., 2021). We apply the MoCo-v2
framework (Chen et al., 2020c) for further justiﬁcation that CACR is also applicable with framework
including a large queue, where the ResNet50 (He et al., 2016) is a commonly chosen feature encoder
architecture. The output dimension of the encoder is set as 128.
•Model Optimization : Following the standard setting in He et al. (2020); Chen et al. (2020c); Wang
& Isola (2020), the training mini-batch size is set as 128 on ImageNet-100 and 256 on ImageNet-1K.
We use a mini-batch stochastic gradient descent (SGD) optimizer with 0.9 momentum and 1e-4
weight decay. The total number of training epochs is set as 200. The learning rate is initialized as
0.03, decayed by a cosine scheduler for MoCo-V2 at epoch 120 and epoch 160. In all experiments,
the momentum of updating the oﬄine encoder is 0.999.
•Parameter Setup : On ImageNet-100 and ImageNet-1K, for all methods, the queue size for negative
sampling is 65,536. The training batch size is 128 on ImageNet-100 and 256 on ImageNet. For
CACR, we train with two positive sampling sizes K= 1andK= 4and the parameters in the
conditional weight metric are respectively set as t+= 1.0, t−= 2.0. For baselines, according to
their papers and Github pages (Tian et al., 2019; Wang & Isola, 2020; Robinson et al., 2021), the
temperature parameter of CL is τ= 0.2, the hyper-parameters of AU-CL are t= 3.0, τ= 0.2, and
the hyper-parameters of HN-CL are τ= 0.5, β= 1.0. Note that CMC ( K= 1) reported in the main
paper is trained with 240 epochs and with its own augmentation methods (Tian et al., 2019). For
CMC (K= 4), the temperature is set τ= 0.07according to the setting in Tian et al. (2019) and
the loss is calculated with Equation (8) in the paper, which requires more GPU resources than 8
Tesla-V100-32G GPUs with the setting on ImageNet-1K.
•Linear Classiﬁcation Evaluation : Following the standard linear classiﬁcation evaluation (He
et al., 2020; Chen et al., 2020c; Wang & Isola, 2020), the pre-trained feature encoders are ﬁxed, and
a linear classiﬁer added on top is trained on the train split and test on the validation split. The
linear classiﬁer is trained with SGD over 100 epochs, with the momentum as 0.9, the mini-batch size
as 256, and the learning rate as 30.0, decayed by a factor of 0.1 at epoch 60 and epoch 80.
•Feature Transferring Evaluation (Detection and Segmentation) : The pre-trained models
are transferred to various tasks including PASCAL VOC⋆and COCO⋆datsets. Strictly following the
⋆http://host.robots.ox.ac.uk/pascal/VOC/index.html
⋆https://cocodataset.org/#download
32Published in Transactions on Machine Learning Research (07/2023)
same setting in He et al. (2020), for the detection on Pascal VOC, a Faster R-CNN (Ren et al., 2016)
with an R50-C4 backbone is ﬁrst ﬁne-tuned end-to-end on the VOC 07+12 trainval set and then
evaluated on the VOC 07 test set with the COCO suite of metrics (Lin et al., 2014). The image
scale is [480, 800] pixels during training and 800 at inference. For the detection and segmentation on
COCO dataset, a Mask R-CNN (He et al., 2017) with C4 backbone (1x schedule) is applied for the
end-to-end ﬁne-tuning. The model is tuned on the train2017 set and evaluate on val2017 set, where
the image scale is in [640, 800] pixels during training and is 800 in the inference.
•Image-Text representation learning : Following the CLIP setting (Radford et al., 2021), we
adopt ResNet50 as the vision encoder backbone. All methods are pre-trained for 32 epochs, with the
mini-batch size as 2048, and the learning rate as 5e-4, with cosine annealing scheduler.
•Estimation of ˆπ−
θwith MoCo-v2: Following the strategy in Wang & Isola (2020), we estimate ˆπ−
θ
with not only the cost between queries and keys, but also with the cost between queries. Speciﬁcally,
at each iteration, let Mbe the mini-batch size, Nbe the queue size, {fqi}M
i=1be the query features,
and{fkj}N
j=1be the key features. The conditional distribution is calculated as:
ˆπ−
θ(·|fqi) =e−dt−(·,fqi)/summationtextN
j=1e−dt−(fkj,fqi)+/summationtext
j/negationslash=ie−dt−(fqj,fqi)
To be clear, the Pytorch-like pseudo-code is provided in Algorithm 3. In MoCo-v2 framework, as the
keys are produced from the momentum encoder, this estimation could help the main encoder get
involved with the gradient from the conditional distribution, which is consistent with the formulation
in Section 3.2.
33Published in Transactions on Machine Learning Research (07/2023)
Algorithm 3 PyTorch-like style pseudo-code of CACR with MoCo-v2 at each iteration.
######################## Inputs ######################
# t_pos, t_neg: hyper-parameters in CACR
# m: momentum
# im_list=[B0, B2, ..., BK] list of mini-batches of length (K+1)
# B: mini-batches (Mx3x224x224), M denotes batch_size
# encoder_q: main encoder; encoder_k: momentum encoder
# queue: dictionary as a queue of N features of keys (dxN); d denotes the feature dimension
######## compute the embeddings of all samples #################
q_list = [encoder_q(im) for im in im_list] # a list of (K+1) queries q: M x d
k_list = [encoder_k(im) for im in im_list]
stacked_k = torch.stack(k_list, dim=0) # keys k: (K+1) x M x d
################### compute the loss ####################
CACR_loss_pos, CACR_loss_neg = 0.0, 0.0
for k in range(len(im_list)): #load a mini-batch with M samples as queries
q = q_list[k]
mask = list(range(len(im_list)))
mask.pop(k) # the rest mini-batches are used as positive and negative samples
##################### compute the positive cost #####################
# calculate the cost of moving positive samples: M x K
cost_for_pos = (q - stacked_k[mask]).norm(p=2, dim=-1).pow(2).transpose(1, 0) # point-to-point cost
with torch.no_grad(): # the calculation involves momentum encoder, so with no grad here.
# calculate the conditional distribution: M x K
weights_for_pos = torch.softmax(cost_for_pos.mul(t_pos), dim=1) # calculate the positive conditional distribution
# calculate the positive cost with the empirical mean
CACR_loss_pos += (cost_for_pos*weights_for_pos).sum(1).mean()
#################### compute the loss of negative samples ######################
# calculate the cost and weights of negative samples from the queue: M x K
sq_dists_for_cost = (2 - 2 * mm(q, queue))
sq_dists_for_weights = sq_dists_for_cost
if with_intra_batch: # compute the distance of negative samples in the mini-batch: Mx(M-1)
intra_batch_sq_dists = torch.norm(q[:,None] - q, dim=-1).pow(2).masked_select(~torch.eye(q.shape[0], dtype=bool).
cuda()).view(q.shape[0], q.shape[0] - 1)
# combine the distance of negative samples from the queue and intra-batch: Mx(K+M-1)
sq_dists_for_cost = torch.cat([sq_dists_for_cost, intra_batch_sq_dists], dim=1)
sq_dists_for_weights = torch.cat([sq_dists_for_weights, intra_batch_sq_dists], dim=1)
# calculate the negative conditional distribution: if with_intra_batch==True Mx(K+M-1), else MxK
weights_for_neg = torch.softmax(sq_dists_for_weights.mul(-t_neg), dim=1)
# calculate the negative cost with the empirical mean
CACR_loss_neg += (sq_dists_for_cost.mul(-1.0)*weights_for_neg).sum(1).mean()
# combine the loss of positive cost and negative cost and update main encoder
CACR_loss = CACR_loss_pos/len(im_list)+CACR_loss_neg/len(im_list)
CACR_loss.backward()
update(encoder_q.params) # SGD update: main encoder
encoder_k.params = m*encoder_k.params+(1-m)*encoder_q.params #momentum update: key encoder
# update the dictionary, dequeue and enqueue
enqueue(queue, k_list[-1]) # enqueue the current minibatch
dequeue(queue) # dequeue the earlist minibatch
pow: power function; mm: matrix multiplication; cat: concatenation.
34