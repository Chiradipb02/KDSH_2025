How DNNs break the Curse of Dimensionality:
Compositionality and Symmetry Learning
Anonymous Author(s)
Affiliation
Address
email
Abstract
We show that deep neural networks (DNNs) can efficiently learn any composition 1
of functions with bounded F1-norm, which allows DNNs to break the curse of 2
dimensionality in ways that shallow networks cannot. More specifically, we 3
derive a generalization bound that combines a covering number argument for 4
compositionality, and the F1-norm (or the related Barron norm) for large width 5
adaptivity. We show that the global minimizer of the regularized loss of DNNs can 6
fit for example the composition of two functions f∗=h◦gfrom a small number 7
of observations, assuming gis smooth/regular and reduces the dimensionality (e.g. 8
gcould be the modulo map of the symmetries of f∗), so that hcan be learned in 9
spite of its low regularity. The measures of regularity we consider is the Sobolev 10
norm with different levels of differentiability, which is well adapted to the F1norm. 11
We compute scaling laws empirically and observe phase transitions depending on 12
whether gorhis harder to learn, as predicted by our theory. 13
1 Introduction 14
One of the fundamental features of DNNs is their ability to generalize even when the number of 15
neurons (and of parameters) is so large that the network could fit almost any function [ 46]. Actually 16
DNNs have been observed to generalize best when the number of neurons is infinite [ 8,21,20]. 17
The now quite generally accepted explanation to this phenomenon is that DNNs have an implicit 18
bias coming from the training dynamic where properties of the training algorithm lead to networks 19
that generalize well. This implicit bias is quite well understood in shallow networks [ 11,36], in 20
linear networks [ 24,30], or in the NTK regime [ 28], but it remains ill-understood in the general deep 21
nonlinear case. 22
In both shallow networks and linear networks, one observes a bias towards small parameter norm 23
(either implicit [ 12] or explicit in the presence of weight decay [ 42]). Thanks to tools such as the 24
F1-norm [ 5], or the related Barron norm [ 44], or more generally the representation cost [ 14], it is 25
possible to describe the family of functions that can be represented by shallow networks or linear 26
networks with a finite parameter norm. This was then leveraged to prove uniform generalization 27
bounds (based on Rademacher complexity) over these sets [ 5], which depend only on the parameter 28
norm, but not on the number of neurons or parameters. 29
Similar bounds have been proposed for DNNs [ 7,6,39,33,25,40], relying on different types of 30
norms on the parameters of the network. But it seems pretty clear that we have not yet identified 31
the ‘right’ complexity measure for deep networks, as there remains many issues: these bounds are 32
typically orders of magnitude too large [ 29,23], and they tend to explode as the depth Lgrows [ 40]. 33
Two families of bounds are particularly relevant to our analysis: bounds based on covering numbers 34
which rely on the fact that one can obtain a covering of the composition of two function classes from 35
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.covering of the individual classes [ 7,25], and path-norm bounds which extend the techniques behind 36
theF1-norm bound from shallow networks to the deep case [32, 6, 23]. 37
Another issue is the lack of approximation results to accompany these generalization bounds: many 38
different complexity measures R(θ)on the parameters θof DNNs have been proposed along with 39
guarantees that the generalization gap will be small as long as R(θ)is bounded, but there are often 40
little to no result describing families of functions that can be approximated with a bounded R(θ) 41
norm. The situation is much clearer in shallow networks, where we know that certain Sobolev spaces 42
can be approximated with bounded F1-norm [5]. 43
We will focus on approximating composition of Sobolev functions, and obtaining close to optimal 44
rates. This is quite similar to the family of tasks considered [ 39], though the complexity measure we 45
consider is quite different, and does not require sparsity of the parameters. 46
1.1 Contribution 47
We consider Accordion Networks (AccNets), which are the composition of multiple shallow 48
networks fL:1=fL◦ ··· ◦ f1, we prove a uniform generalization bound L(fL:1)−˜LN(fL:1)≲ 49
R(f1, . . . , f L)logN√
N, for a complexity measure 50
R(f1, . . . , f L) =LY
ℓ=1Lip(fℓ)LX
ℓ=1∥fℓ∥F1
Lip(fℓ)p
dℓ+dℓ−1
that depends on the F1-norms ∥fℓ∥F1and Lipschitz constanst Lip(fℓ)of the subnetworks, and the 51
intermediate dimensions d0, . . . , d L. This use of the F1-norms makes this bound independent of the 52
widths w1, . . . , w Lof the subnetworks, though it does depend on the depth L(it typically grows 53
linearly in Lwhich is still better than the exponential growth often observed). 54
Any traditional DNN can be mapped to an AccNet (and vice versa), by spliting the middle weight 55
matrices Wℓwith SVD USVTinto two matrices U√
Sand√
SVTto obtain an AccNet with 56
dimensions dℓ= Rank Wℓ, so that the bound can be applied to traditional DNNs with bounded rank. 57
We then show an approximation result: any composition of Sobolev functions f∗=f∗
L∗◦ ··· ◦ f∗
158
can be approximated with a network with either a bounded complexity R(θ)or a slowly growing one. 59
Thus under certain assumptions one can show that DNNs can learn general compositions of Sobolev 60
functions. This ability can be interpreted as DNNs being able to learn symmetries, allowing them to 61
avoid the curse of dimensionality in settings where kernel methods or even shallow networks suffer 62
heavily from it. 63
Empirically, we observe a good match between the scaling laws of learning and our theory, as well as 64
qualitative features such as transitions between regimes depending on whether it is harder to learn the 65
symmetries of a task, or to learn the task given its symmetries. 66
2 Accordion Neural Networks and ResNets 67
Our analysis is most natural for a slight variation on the traditional fully-connected neural networks 68
(FCNNs), which we call Accordion Networks, which we define here. Nevertheless, all of our results 69
can easily be adapted to FCNNs. 70
Accordion Networks (AccNets) are simply the composition of Lshallow networks, that is fL:1= 71
fL◦ ··· ◦ f1where fℓ(z) =Wℓσ(Vℓz+bℓ)for the nonlinearity σ:R→R, thedℓ×wℓmatrix 72
Wℓ,wℓ×dℓ−1matrix Vℓ, and wℓ-dim. vector bℓ, and for the widths w1, . . . , w Land dimensions 73
d0, . . . , d L. We will focus on the ReLU σ(x) = max {0, x}for the nonlinearity. The parameters θare 74
made up of the concatenation of all (Wℓ, Vℓ, bℓ). More generally, we denote fℓ2:ℓ1=fℓ2◦ ··· ◦ fℓ175
for any 1≤ℓ1≤ℓ2≤L. 76
We will typically be interested in settings where the widths wℓis large (or even infinitely large), while 77
the dimensions dℓremain finite or much smaller in comparison, hence the name accordion. 78
If we add residual connections, i.e. fres
1:L= (fL+id)◦ ··· ◦ (f1+id)for the same shallow nets 79
f1, . . . , f Lwe recover the typical ResNets. 80
2Remark. The only difference between AccNets and FCNNs is that each weight matrix Mℓof the 81
FCNN is replaced by a product of two matrices Mℓ=VℓWℓ−1in the middle of the network (such a 82
structure has already been proposed [ 34]). Given an AccNet one can recover an equivalent FCNN by 83
choosing Mℓ=VℓWℓ−1,M0=V0andML+1=WL. In the other direction there could be multiple 84
ways to split Mℓinto the product of two matrices, but we will focus on taking Vℓ=U√
Sand 85
Wℓ−1=√
SVTfor the SVD decomposition Mℓ=USVT, along with the choice dℓ= Rank Mℓ. 86
2.1 Learning Setup 87
We consider a traditional learning setup, where we want to find a function f: Ω⊂Rdin→Rdout88
that minimizes the population loss L(f) =Ex∼π[ℓ(x, f(x))]for an input distribution πand a 89
ρ-Lipschitz and ρ-bounded loss function ℓ(x, y)∈[0, B]. Given a training set x1, . . . , x Nof size N 90
we approximate the population loss by the empirical loss ˜LN(f) =1
NPN
i=1ℓ(xi, f(xi))that can be 91
minimized. 92
To ensure that the empirical loss remains representative of the population loss, we will prove high 93
probability bounds on the generalization gap ˜LN(f)−L(f)uniformly over certain functions families 94
f∈ F. 95
Forregression tasks , we assume the existence of a true function f∗and try to minimize the distance 96
ℓ(x, y) =∥f∗(x)−y∥pforp≥1. If we assume that f∗(x)andyare uniformly bounded then one 97
can easily show that ℓ(x, y)is bounded and Lipschitz. We are particularly interested in the cases 98
p∈ {1,2}, with p= 2representing the classical MSE, and p= 1representing a L1distance. The 99
p= 2case is amenable to ‘fast rates’ which take advantage of the fact that the loss increases very 100
slowly around the optimal solution f∗, We do not prove such fast rates (even though it might be 101
possible) so we focus on the p= 1case. 102
Forclassification tasks onkclasses, we assume the existence of a ‘true class’ function f∗: Ω→ 103
{1, . . . , k }and want to learn a function f: Ω→Rksuch that the largest entry of f(x)is the f∗(k)-th 104
entry. One can consider the hinge cost ℓ(x, y) = max {0,1−(yf∗(k)−max i̸=f∗(x)yi)}, which is 105
zero whenever the margin yf∗(k)−max i̸=f∗(x)yiis larger than 1and otherwise equals 1minus the 106
margin. The hinge loss is Lipschitz and bounded if we assume bounded outputs y=f(x). The 107
cross-entropy loss also fits our setup. 108
3 Generalization Bound for DNNs 109
The reason we focus on accordion networks is that there exists generalization bounds for shallow 110
networks [ 5,44], that are (to our knowledge) widely considered to be tight, which is in contrast to the 111
deep case, where many bounds exist but no clear optimal bound has been identified. Our strategy 112
is to extend the results for shallow nets to the composition of multiple shallow nets, i.e. AccNets. 113
Roughly speaking, we will show that the complexity of an AccNet fθis bounded by the sum of the 114
complexities of the shallow nets f1, . . . , f Lit is made of. 115
We will therefore first review (and slightly adapt) the existing generalization bounds for shallow 116
networks in terms of their so-called F1-norm [ 5], and then prove a generalization bound for deep 117
AccNets. 118
3.1 Shallow Networks 119
The complexity of a shallow net f(x) = Wσ(V x+b), with weights W∈Rw×doutand 120
V∈Rdin×w, can be bounded in terms of the quantity C=Pw
i=1∥W·i∥q
∥Vi·∥2+b2
i. 121
First note that the rescaled function1
Cfcan be written as a convex combination1
Cf(x) = 122
Pw
i=1∥W·i∥√
∥Vi·∥2+b2
i
C¯W·iσ(¯Vi·x+¯bi)for¯W·i=W·i
∥W·i∥,¯Vi·=Vi·√
∥Vi·∥2+b2
i, and ¯bi=bi√
∥Vi·∥2+b2
i, 123
since the coefficients∥W·i∥√
∥Vi·∥2+b2
i
Care positive and sum up to 1. Thus fbelongs to Ctimes the 124
convex hull 125
BF1= Convn
x7→wσ(vTx+b) :∥w∥2=∥v∥2+b2= 1o
.
3We call this the F1-ball since it can be thought of as the unit ball w.r.t. the F1-norm ∥f∥F1which we 126
define as the smallest positive scalar ssuch that11
sf∈BF1. For more details in the single output 127
case, see [5]. 128
The generalization gap over any F1-ball can be uniformly bounded with high probability: 129
Theorem 1. For any input distribution πsupported on the L2ballB(0, b)with radius b, we have 130
with probability 1−δ, over the training samples x1, . . . , x N, that for all f∈BF1(0, R) =R·BF1131
L(f)−˜LN(f)≤ρbRp
din+doutlogN√
N+c0r
2 log 2/δ
N
This theorem is a slight variation of the one found in [ 5]: we simply generalize it to multiple outputs, 132
and also prove it using a covering number argument instead of a direct computation of the Rademacher 133
complexity, which will be key to obtaining a generalization bound for the deep case. But due to this 134
change of strategy we pay a logNcost here and in our later results. We know that the logNterm 135
can be removed in Theorem 1 by switching to a Rademacher argument, but we do not know whether 136
it can be removed in deep nets. 137
Notice how this bound does not depend on the width w, because the F1-norm (and the F1-ball) 138
themselves do not depend on the width. This matches with empirical evidence that shows that 139
increasing the width does not hurt generalization [8, 21, 20]. 140
To use Theorem 1 effectively we need to be able to guarantee that the learned function will have a 141
small enough F1-norm. The F1-norm is hard to compute exactly, but it is bounded by the parameter 142
norm: if f(x) =Wσ(V x+b), then∥f∥F1≤1
2
∥W∥2
F+∥V∥2
F+∥b∥2
, and this bound is tight 143
if the width wis large enough and the parameters are chosen optimally. Adding weight decay/ L2- 144
regularization to the cost then leads to bias towards learning with small F1norm. 145
3.2 Deep Networks 146
Since an AccNet is simply the composition of multiple shallow nets, the functions represented by an 147
AccNet is included in the set of composition of F1balls. More precisely, if ∥Wℓ∥2+∥Vℓ∥2+∥bℓ∥2≤ 148
2RℓthenfL:1belongs to the set {gL◦ ··· ◦ g1:gℓ∈BF1(0, Rℓ)}for some Rℓ, which is width 149
agnostic. 150
As already noticed in [ 7], the covering number number is well-behaved under composition, this 151
allows us to bound the complexity of AccNets in terms of the individual shallow nets it is made up of: 152
Theorem 2. Consider an accordion net of depth Land widths dL, . . . , d 0, with corresponding set of 153
functions F={fL:1:∥fℓ∥F1≤Rℓ,Lip(fℓ)≤ρℓ}. With probability 1−δover the sampling of the 154
training set Xfrom the distribution πsupported in B(0, b), we have for all f∈ F 155
L(f)−˜LN(f)≤Cρbρ L:1LX
ℓ=1Rℓ
ρℓp
dℓ+dℓ−1logN√
N(1 +o(1)) + c0r
2 log 2/δ
N.
Theorem 2 can be extended to ResNets (fL+id)◦ ··· ◦ (f1+id)by simply replacing the Lipschitz 156
constant Lip(fℓ)byLip(fℓ+id). 157
The Lipschitz constants Lip(fℓ)are difficult to compute exactly, so it is easiest to simply bound it 158
by the product of the operator norms Lip(fℓ)≤ ∥Wℓ∥op∥Vℓ∥op, but this bound can often be quite 159
loose. The fact that our bound depends on the Lipschitz constants rather than the operator norms 160
∥Wℓ∥op,∥Vℓ∥opis thus a significant advantage. 161
This bound can be applied to a FCNNs with weight matrices M1, . . . , M L+1, by replacing the middle 162
Mℓwith SVD decomposition USVTin the middle by two matrices Wℓ−1=√
SVTandVℓ=U√
S, 163
so that the dimensions can be chosen as the rank dℓ= Rank Mℓ+1. The Frobenius norm of the new 164
matrices equal the nuclear norm of the original one ∥Wℓ−1∥2
F=∥Vℓ∥2
F=∥Mℓ∥∗. Some bounds 165
1This construction can be used for any convex set Bthat is symmetric around zero ( B=−B) to define a
norm whose unit ball is B. This correspondence between symmetric convex sets and norms is well known.
4Figure 1: Visualization of scaling laws. We observe that deep networks (either AccNets or DNNs)
achieve better scaling laws than kernel methods or shallow networks on certain compositional tasks,
in agreement with our theory. We also see that our new generalization bounds approximately recover
the right saling laws (even though they are orders of magnitude too large overall). We consider
a compositional true function f∗=h◦gwhere gmaps from dimension 15 to 3 while h maps
from 3 to 20, and we denote νg, νhfor the number of times g, hare differentiable. In the first plot
νg= 8, νh= 1so that gis easy to learn while his hard, whereas in the second plot νg= 9, νh= 9,
so both gandhare relatively easier. The third plot presents the decay in test error and generalization
bounds for networks evaluated using the real-world dataset, WESAD [37].
assuming rank sparsity of the weight matrices also appear in [ 41]. And several recent results have 166
shown that weight-decay leads to a low-rank bias on the weight matrices of the network [ 27,26,19] 167
and replacing the Frobenius norm regularization with a nuclear norm regularization (according to the 168
above mentioned equivalence) will only increase this low-rank bias. 169
We compute in Figure 1 the upper bound of Theorem 2 for both AccNets and DNNs, and even though 170
we observe a very large gap (roughly of order 103), we do observe that it captures rate/scaling of the 171
test error (the log-log slope) well. So this generalization bound could be well adapted to predicting 172
rates, which is what we will do in the next section. 173
Remark. Note that if one wants to compute this upper bound in practical setting, it is important to 174
train with L2regularization until the parameter norm also converges (this often happens after the 175
train and test loss have converged). The intuition is that at initialization, the weights are initialized 176
randomly, and they contribute a lot to the parameter norm, and thus lead to a larger generalization 177
bound. During training with weight decay, these random initial weights slowly vanish, thus leading 178
to a smaller parameter norm and better generalization bound. It might be possible to improve our 179
generalization bounds to take into account the randomness at initialization to obtain better bounds 180
throughout training, but we leave this to future work. 181
4 Breaking the Curse of Dimensionality with Compositionality 182
In this section we study a large family of functions spaces, obtained by taking compositions of 183
Sobolev balls. We focus on this family of tasks because they are well adapted to the complexity 184
measure we have identified, and because kernel methods and even shallow networks do suffer from 185
the curse of dimensionality on such tasks, whereas deep networks avoid it (e.g. Figure 1). 186
More precisely, we will show that these sets of functions can be approximated by a AccNets with 187
bounded (or in some cases slowly growing) complexity measure 188
R(f1, . . . , f L) =LY
ℓ=1Lip(fℓ)LX
ℓ=1∥fℓ∥F1
Lip(fℓ)p
dℓ+dℓ−1.
This will then allow us show that AccNets can (assuming global convergence) avoid the curse of 189
dimensionality, even in settings that should suffer from the curse of dimensionality, when the input 190
dimension is large and the function is not very smooth (only a few times differentiable). 191
5Figure 2: A comparison of empirical and theoretical error rates. The first plot illustrates the log
decay rate of the test error with respect to the dataset size Nbased on our empirical simulations.
The second plot depicts the theoretical decay rate of the test error as discussed in Section 4.1,
−min{1
2,νg
din,νh
dmid}. The final plot on the right displays the difference between the two. The lower
left region represents the area where gis easier to learn than h, the upper right where his easier to
learn than g, and the lower right region where both fandgare easy.
.
4.1 Composition of Sobolev Balls 192
The family of Sobolev norms capture some notion of regularity of a function, as it measures the size 193
of its derivatives. The Sobolev norm of a function f:Rdin→Ris defined in terms of its derivatives 194
∂α
xffor some din-multi-index α, namely the Wν,p(π)-Sobolev norm with integer νandp≥1is 195
defined as 196
∥f∥p
Wν,p(π)=X
|α|≤ν∥∂α
xf∥p
Lp(π).
Note that the derivative ∂α
xfonly needs to be defined in the ‘weak’ sense, which means that even 197
non-differentiable functions such as the ReLU functions can actually have finite Sobolev norm. 198
The Sobolev balls BWν,p(π)(0, R) ={f:∥f∥Wν,p(π)≤R}are a family of function spaces with a 199
range of regularity (the larger ν, the more regular). This regularity makes these spaces of functions 200
learnable purely from the fact that they enforce the function fto vary slowly as the input changes. 201
Indeed we can prove the following generalization bound: 202
Proposition 3. Given a distribution πwith support the L2ball with radius b, we have that with 203
probability 1−δfor all functions f∈ F={f:∥f∥Wν,2≤R,∥f∥∞≤R} 204
L(f)−˜LN(f)≤2ρC1REν/d(N) +c0r
2 log 2/δ
N.
where Er(N) =N−1
2ifr >1
2,Er(N) =N−1
2logNifr=1
2, and Er(N) =N−rifr <1
2. 205
But this result also illustrates the curse of dimensionality: the differentiability νneeds to scale with 206
the input dimension dinto obtain a reasonable rate. If instead νis constant and dingrows, then the 207
number of datapoints Nneeded to guarantee a generalization gap of at most ϵscales exponentially in 208
din, i.e.N∼ϵ−din
ν. One way to interpret this issue is that regularity becomes less and less useful the 209
larger the dimension: knowing that similar inputs have similar outputs is useless in high dimension 210
where the closest training point xito a test point xis typically very far away. 211
4.1.1 Breaking the Curse of Dimensionality with Compositionality 212
To break the curse of dimensionality, we need to assume some additional structure on the data or task 213
which introduces an ‘intrinsic dimension’ that can be much lower than the input dimension din: 214
Manifold hypothesis : If the input distribution lies on a dsurf-dimensional manifold, the error rates 215
typically depends on dsurf instead of din[38, 10]. 216
6Figure 3: Comparing error rates for shallow and AccNets: shallow nets vs. AccNets, and kernel
methods vs. AccNets. The left two graphs shows the empirical decay rate of test error with respect to
dataset size (N) for both shallow nets and kernel methods. In contrast to our earlier empirical findings
for AccNets, both shallow nets and kernel methods exhibit a slower decay rate in test error. The right
two graphs present the differences in log decay rates between shallow nets and AccNets, as well as
between kernel methods and AccNets. AccNets almost always obtain better rates, with a particularly
large advantage at the bottom and middle-left.
.
Known Symmetries: Iff∗(g·x) =f∗(x)for a group action ·w.r.t. a group G, then f∗can be 217
written as the composition of a modulo map g∗:Rdin→Rdin/Gwhich maps pairs of inputs which 218
are equivalent up to symmetries to the same value (pairs x, ys.t.y=g·xfor some g∈G), and then 219
a second function h∗:Rdin/G→Rdout, then the complexity of the task will depend on the dimension 220
of the modulo space Rdin/Gwhich can be much lower. If the symmetry is known, then one can for 221
example fix g∗and only learn h∗(though other techniques exist, such as designing kernels or features 222
that respect the same symmetries) [31]. 223
Symmetry Learning: However if the symmetry is not known then both g∗andh∗have to be learned, 224
and this is where we require feature learning and/or compositionality. Shallow networks are able 225
to learn translation symmetries, since they can learn so-called low-index functions which satisfy 226
f∗(x) =f∗(Px)for some projection P(with a statistical complexity that depends on the dimension 227
of the space one projects into, not the full dimension [ 5,2]). Low-index functions correspond exactly 228
to the set of functions that are invariant under translation along the kernel kerP. To learn general 229
symmetries, one needs to learn both h∗and the modulo map g∗simultaneously, hence the importance 230
of feature learning. 231
Forg∗to be learnable efficiently, it needs to be regular enough to not suffer from the curse of 232
dimensionality, but many traditional symmetries actually have smooth modulo maps, for example 233
the modulo map g∗(x) =∥x∥2for rotation invariance. This can be understood as a special case of 234
composition of Sobolev functions, whose generalization gap can be bounded: 235
Theorem 4. Consider the function set F =FL◦ ··· ◦ F 1where Fℓ= 236
fℓ:Rdℓ−1→Rdℓs.t.∥fℓ∥Wνℓ,2≤Rℓ,∥fℓ∥∞≤bℓ, Lip(fℓ)≤ρℓ	
, and let rmin= min ℓrℓfor 237
rℓ=νℓ
dℓ−1, then with probability 1−δwe have for all f∈ F 238
L(f)−˜LN(f)≤ρC0 LX
ℓ=1(CℓρL:ℓ+1Rℓ)1
rmin+1!rmin+1
Ermin(N) +c0r
2 log 2/δ
N,
where Cℓdepends only on dℓ−1, dℓ, νℓ, bℓ−1. 239
We see that only the smallest ratio rminmatters when it comes to the rate of learning. And actually 240
the above result could be slightly improved to show that the sum over all layers could be replaced by 241
a sum over only the layers where the ratio rℓleads to the worst rate Erℓ(N) =Ermin(N)(and the 242
other layers contribute an asymptotically subdominant amount). 243
Coming back to the symmetry learning example, we see that the hardness of learning a function of 244
the type f∗=h◦gwith inner dimension dmidand regularities νgandνh, the error rate will be (up 245
to log terms) N−min{1
2,νg
din,νh
dmid}. This suggests the existence of three regimes depending on which 246
term attains the minimum: a regime where both gandhare easy to learn and we have N−1
2learning, 247
a regime gis hard, and a regime where his hard. The last two regimes differentiate between tasks 248
7where learning the symmetry is hard and those where learning the function knowing its symmetries is 249
hard. 250
In contrast, without taking advantage of the compositional structure, we expect f∗to be only 251
min{νg, νh}times differentiable, so trying to learn it as a single Sobolev function would lead to an 252
error rate of N−min{1
2,min{νg,νh}
din}=N−min{1
2,νg
din,νh
din}which is no better than the compositional 253
rate, and is strictly worse whenever νh< νgandνh
din<1
2(we can always assume dmid≤dinsince 254
one could always choose d=id). 255
Furthermore, since multiple compositions are possible, one can imagine a hierarchy of symmetries 256
that slowly reduce the dimensionality with less and less regular modulo maps. For example one could 257
imagine a composition fL◦ ··· ◦ f1with dimensions dℓ=d02−ℓand regularities νℓ=d02−ℓso that 258
the ratios remain constant rℓ=d02−ℓ
d02−ℓ+1=1
2, leading to an almost parametric rate of N−1
2logN 259
even though the function may only be d02−Ltimes differentiable. Without compositionality, the rate 260
would only be N−2−L. 261
Remark. In the case of a single Sobolev function, one can show that the rate Eν/d(N)is in some 262
sense optimal, by giving an information theoretic lower bound with matching rate. A naive argument 263
suggests that the rate of Emin{r1,...,r L}(N)should similarly be optimal: assume that the minimum 264
rℓis attained at a layer ℓ, then one can consider the subset of functions such that the image 265
fℓ−1:1(B(0, r))contains a ball B(z, r′)⊂Rdℓ−1and that the function fL:ℓ+1isβ-non-contracting 266
∥fL:ℓ+1(x)−fL:ℓ+1(y)∥ ≥β∥x−y∥, then learning fL:1should be as hard as learning fℓover the 267
ballB(z, r′)(more rigorously this could be argued from the fact that any ϵ-covering of fL:1can be 268
mapped to an ϵ/β-covering of fℓ), thus forcing a rate of at least Erℓ(N) =Emin{r1,...,r L}(N). 269
An analysis of minimax rates in a similar setting has been done in [22]. 270
4.2 Breaking the Curse of Dimensionality with AccNets 271
Now that we know that composition of Sobolev functions can be easily learnable, even in settings 272
where the curse of dimensionality should make it hard to learn them, we need to find a model that can 273
achieve those rates. Though many models are possible2, we focus on DNNs, in particular AccNets. 274
Assuming convergence to a global minimum of the loss of sufficiently wide AccNets with two types 275
of regularization, one can guarantee close to optimal rates: 276
Theorem 5. Given a true function f∗
L∗:1=f∗
L∗◦ ··· ◦ f∗
1going through the dimensions d∗
0, . . . , d∗
L∗, 277
along with a continuous input distribution π0supported in B(0, b0), such that the distributions πℓ 278
off∗
ℓ(x)(forx∼π0) are continuous too and supported inside B(0, bℓ)⊂Rd∗
ℓ. Further assume 279
that there are differentiabilities νℓand radii Rℓsuch that ∥f∗
ℓ∥Wνℓ,2(B(0,bℓ))≤Rℓ, and ρℓsuch that 280
Lip(f∗
ℓ)≤ρℓ. For an infinite width AccNet with L≥L∗and dimensions dℓ≥d∗
1, . . . , d∗
L∗−1, we 281
have for the ratios ˜rℓ=νℓ
d∗
ℓ+3: 282
•At a global minimizer ˆfL:1of the regularized loss f1, . . . , f L7→ ˜LN(fL:1) + 283
λQL
ℓ=1Lip(fℓ)PL
ℓ=1∥fℓ∥F1
Lip(fℓ)p
dℓ−1+dℓ, we have L(ˆfL:1) =˜O(N−min{1
2,˜r1,...,˜rL∗}). 284
•At a global minimizer ˆfL:1of the regularized loss f1, . . . , f L7→˜LN(fL:1)+λQL
ℓ=1∥fℓ∥F1, 285
we have L(ˆfL:1) =˜O(N−1
2+PL∗
ℓ=1max{0,˜rℓ−1
2}). 286
There are a number of limitations to this result. First we assume that one is able to recover the global 287
minimizer of the regularized loss, which should be hard in general3(we already know from [ 5] that 288
this is NP-hard for shallow networks and a simple F1-regularization). Note that it is sufficient to 289
recover a network fL:1whose regularized loss is within a constant of the global minimum, which 290
2One could argue that it would be more natural to consider compositions of kernel method models, for
example a composition of random feature models. But this would lead to a very similar model: this would
be equivalent to a AccNet where only the Wℓweights are learned, while the Vℓ,bℓweights remain constant.
Another family of models that should have similar properties is Deep Gaussian Processes [15].
3Note that the unregularized loss can be optimized polynomially, e.g. in the NTK regime [ 28,3,16], but this
is an easier task than findinig the global minimum of the regularized loss where one needs to both fit the data,
and do it with an minimal regularization term.
8might be easier to guarantee, but should still be hard in general. The typical method of training with 291
GD on the regularized loss is a greedy approach, which might fail in general but could recover almost 292
optimal parameters under the right conditions (some results suggest that training relies on first order 293
correlations to guide the network in the right direction [2, 1, 35]). 294
We propose two regularizations because they offer a tradeoff: 295
First regularization: The first regularization term leads to almost optimal rates, up to the change 296
fromrℓ=νℓ
d∗
ℓtorℓ=νℓ
d∗
ℓ+3which is negligible for large dimensions dℓand differentiabilities νℓ. The 297
first problem is that it requires an infinite width at the moment, because we were not able to prove 298
that a function with bounded F1-norm and Lipschitz constant can be approximated by a sufficiently 299
wide shallow networks with the same (or close) F1-norm and Lipschitz constant (we know from [ 5] 300
that it is possible without preserving the Lipschitzness). We are quite hopeful that this condition 301
might be removed in future work. 302
The second and more significant problem is that the Lipschitz constants Lip(fℓ)are difficult to 303
optimize over. For finite width networks it is in theory possible to take the max over all linear regions, 304
but the complexity might be unreasonable. It might be more reasonable to leverage an implicit bias 305
instead, such as a large learning rate, because a large Lipschitz constant implies that the nework is 306
sensible to small changes in its parameters, so GD with a large learning rate should only converge to 307
minima with a small Lipschitz constant (such a bias is described in [ 26]). It might also be possible to 308
replace the Lipschitz constant in our generalization bounds, possibly along the lines of [43]. 309
Second regularization: The second regularization term actually does not require an infinite width, 310
only a sufficiently large one. Also its regularization term is equivalent toQ(∥Wℓ∥2+∥Vℓ∥2+∥bℓ∥2) 311
which is much closer to the traditional L2-regularization (and actually one could prove the same 312
or very similar rates for L2-regularization). The issue is that it lead to rates that could be far from 313
optimal depending on the ratios ˜rℓ: it recovers the same rate as the first regularization term if no 314
more than one ratio ˜rℓis smaller than1
2, but if many of these ratios are above1
2, it can be arbitrarily 315
smaller. 316
In Figure 2, we compare the empirical rates (by doing a linear fit on a log-log plot of test error as a 317
function of N) and the predicted optimal rates min{1
2,νg
din,νh
dmid}and observe a pretty good match. 318
Though surprisingly, it appears the the empirical rates tend to be slightly better than the theoretical 319
ones. 320
Remark. As can be seen in the proof of Theorem5, when the depth Lis strictly larger than the true 321
depth L∗, one needs to add identity layers, leading to a so-called Bottleneck structure, which was 322
proven to be optimal and observed empirically in [ 27,26,45]. These identity layers add a term 323
that scales linearly in the additional depth(L−L∗)d∗
min√
Nto the first regularization, and an exponential 324
prefactor (2d∗
min)L−L∗to the second. It might be possible to remove these factors by leveraging the 325
bottleneck structure, or simply by switching to ResNets. 326
5 Conclusion 327
We have given a generalization bound for Accordion Networks and as an extension Fully-Connected 328
networks. It depends on F1-norms and Lipschitz constants of its shallow subnetworks. This allows us 329
to prove under certain assumptions that AccNets can learn general compositions of Sobolev functions 330
efficiently, making them able to break the curse of dimensionality in certain settings, such as in the 331
presence of unknown symmetries. 332
References 333
[1]Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: 334
a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer 335
neural networks. In Conference on Learning Theory , pages 4782–4887. PMLR, 2022. 336
[2]Emmanuel Abbe, Enric Boix-Adserà, Matthew Stewart Brennan, Guy Bresler, and 337
Dheeraj Mysore Nagaraj. The staircase property: How hierarchical structure can guide deep 338
learning. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances 339
in Neural Information Processing Systems , 2021. 340
9[3]Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via 341
over-parameterization. pages 242–252, 2019. 342
[4]Kendall Atkinson and Weimin Han. Spherical harmonics and approximations on the unit 343
sphere: an introduction , volume 2044. Springer Science & Business Media, 2012. 344
[5]Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal 345
of Machine Learning Research , 18(1):629–681, 2017. 346
[6]Andrew R Barron and Jason M Klusowski. Complexity, statistical risk, and metric entropy of 347
deep nets using total path variation. stat, 1050:6, 2019. 348
[7] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds 349
for neural networks. Advances in neural information processing systems , 30, 2017. 350
[8]Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine- 351
learning practice and the classical bias–variance trade-off. Proceedings of the National Academy 352
of Sciences , 116(32):15849–15854, 2019. 353
[9]M. S. Birman and M. Z. Solomjak. Piecewise-polynomial approximations of functions of the 354
classes Wα
p.Mathematics of The USSR-Sbornik , 2:295–317, 1967. 355
[10] Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Nonparametric regression on 356
low-dimensional manifolds using deep relu networks: Function approximation and statistical 357
recovery. Information and Inference: A Journal of the IMA , 11(4):1203–1253, 2022. 358
[11] Lénaïc Chizat and Francis Bach. On the Global Convergence of Gradient Descent for Over- 359
parameterized Models using Optimal Transport. In Advances in Neural Information Processing 360
Systems 31 , pages 3040–3050. Curran Associates, Inc., 2018. 361
[12] Lénaïc Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural 362
networks trained with the logistic loss. In Jacob Abernethy and Shivani Agarwal, editors, 363
Proceedings of Thirty Third Conference on Learning Theory , volume 125 of Proceedings of 364
Machine Learning Research , pages 1305–1338. PMLR, 09–12 Jul 2020. 365
[13] Feng Dai. Approximation theory and harmonic analysis on spheres and balls . Springer, 2013. 366
[14] Zhen Dai, Mina Karzand, and Nathan Srebro. Representation costs of linear neural networks: 367
Analysis and design. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan, editors, 368
Advances in Neural Information Processing Systems , 2021. 369
[15] Andreas Damianou and Neil D. Lawrence. Deep Gaussian processes. In Carlos M. Carvalho 370
and Pradeep Ravikumar, editors, Proceedings of the Sixteenth International Conference on 371
Artificial Intelligence and Statistics , volume 31 of Proceedings of Machine Learning Research , 372
pages 207–215, Scottsdale, Arizona, USA, 29 Apr–01 May 2013. PMLR. 373
[16] Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably 374
optimizes over-parameterized neural networks. In International Conference on Learning 375
Representations , 2019. 376
[17] I. Dumer, M.S. Pinsker, and V .V . Prelov. On coverings of ellipsoids in euclidean spaces. IEEE 377
Transactions on Information Theory , 50(10):2348–2356, 2004. 378
[18] Lawrence C Evans. Partial differential equations , volume 19. American Mathematical Society, 379
2022. 380
[19] Tomer Galanti, Zachary S Siegel, Aparna Gupte, and Tomaso Poggio. Sgd and weight decay 381
provably induce a low-rank bias in neural networks. arXiv preprint arXiv:2206.05794 , 2022. 382
[20] Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stéphane d’Ascoli, 383
Giulio Biroli, Clément Hongler, and Matthieu Wyart. Scaling description of generalization 384
with number of parameters in deep learning. Journal of Statistical Mechanics: Theory and 385
Experiment , 2020(2):023401, 2020. 386
10[21] Mario Geiger, Stefano Spigler, Stéphane d’Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio 387
Biroli, and Matthieu Wyart. Jamming transition as a paradigm to understand the loss landscape 388
of deep neural networks. Physical Review E , 100(1):012115, 2019. 389
[22] Matteo Giordano, Kolyan Ray, and Johannes Schmidt-Hieber. On the inability of gaussian 390
process regression to optimally learn compositional functions. In Alice H. Oh, Alekh Agarwal, 391
Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing 392
Systems , 2022. 393
[23] Antoine Gonon, Nicolas Brisebarre, Elisa Riccietti, and Rémi Gribonval. A path-norm toolkit 394
for modern networks: consequences, promises and challenges. In The Twelfth International 395
Conference on Learning Representations , 2023. 396
[24] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias 397
in terms of optimization geometry. In Jennifer Dy and Andreas Krause, editors, Proceedings of 398
the 35th International Conference on Machine Learning , volume 80 of Proceedings of Machine 399
Learning Research , pages 1832–1841. PMLR, 10–15 Jul 2018. 400
[25] Daniel Hsu, Ziwei Ji, Matus Telgarsky, and Lan Wang. Generalization bounds via distillation. 401
InInternational Conference on Learning Representations , 2021. 402
[26] Arthur Jacot. Bottleneck structure in learned features: Low-dimension vs regularity tradeoff. In 403
A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in 404
Neural Information Processing Systems , volume 36, pages 23607–23629. Curran Associates, 405
Inc., 2023. 406
[27] Arthur Jacot. Implicit bias of large depth networks: a notion of rank for nonlinear functions. In 407
The Eleventh International Conference on Learning Representations , 2023. 408
[28] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural Tangent Kernel: Convergence and 409
Generalization in Neural Networks. In Advances in Neural Information Processing Systems 31 , 410
pages 8580–8589. Curran Associates, Inc., 2018. 411
[29] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic 412
generalization measures and where to find them. arXiv preprint arXiv:1912.02178 , 2019. 413
[30] Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient 414
descent for matrix factorization: Greedy low-rank learning. In International Conference on 415
Learning Representations , 2020. 416
[31] Stéphane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics , 417
65(10):1331–1398, 2012. 418
[32] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural 419
networks. In Conference on learning theory , pages 1376–1401. PMLR, 2015. 420
[33] Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under 421
neural tangent kernel regime. In International Conference on Learning Representations , 2020. 422
[34] Greg Ongie and Rebecca Willett. The role of linear layers in nonlinear interpolating networks. 423
arXiv preprint arXiv:2202.00856 , 2022. 424
[35] Leonardo Petrini, Francesco Cagnetta, Umberto M Tomasini, Alessandro Favero, and Matthieu 425
Wyart. How deep neural networks learn compositional data: The random hierarchy model. 426
arXiv preprint arXiv:2307.02129 , 2023. 427
[36] Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time 428
convergence and asymptotic error scaling of neural networks. In Advances in Neural Information 429
Processing Systems 31 , pages 7146–7155. Curran Associates, Inc., 2018. 430
[37] Philip Schmidt, Attila Reiss, Robert Duerichen, Claus Marberger, and Kristof Van Laerhoven. 431
Introducing wesad, a multimodal dataset for wearable stress and affect detection. In Proceedings 432
of the 20th ACM International Conference on Multimodal Interaction , ICMI ’18, page 400–408, 433
New York, NY , USA, 2018. Association for Computing Machinery. 434
11[38] Johannes Schmidt-Hieber. Deep relu network approximation of functions on a manifold. arXiv 435
preprint arXiv:1908.00695 , 2019. 436
[39] Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU 437
activation function. The Annals of Statistics , 48(4):1875 – 1897, 2020. 438
[40] Mark Sellke. On size-independent sample complexity of relu networks. Information Processing 439
Letters , page 106482, 2024. 440
[41] Taiji Suzuki, Hiroshi Abe, and Tomoaki Nishimura. Compression based bound for non- 441
compressed network: unified generalization error analysis of large compressible deep neural 442
network. In International Conference on Learning Representations , 2020. 443
[42] Zihan Wang and Arthur Jacot. Implicit bias of SGD in l2-regularized linear DNNs: One- 444
way jumps from high to low rank. In The Twelfth International Conference on Learning 445
Representations , 2024. 446
[43] Colin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via 447
lipschitz augmentation. Advances in Neural Information Processing Systems , 32, 2019. 448
[44] E Weinan, Chao Ma, and Lei Wu. Barron spaces and the compositional function spaces for 449
neural network models. arXiv preprint arXiv:1906.08039 , 2019. 450
[45] Yuxiao Wen and Arthur Jacot. Which frequencies do cnns need? emergent bottleneck structure 451
in feature learning. to appear at ICML , 2024. 452
[46] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding 453
deep learning requires rethinking generalization. ICLR 2017 proceedings , Feb 2017. 454
12The Appendix is structured as follows: 455
1.In Section A, we describe the experimental setup and provide a few additional experiments. 456
2. In Section B, we prove Theorems 1 and 2 from the main. 457
3. In Section C, we prove Proposition 3 and Theorem 4. 458
4.In Section D, we prove Theorem 5 and other approximation results concerning Sobolev 459
functions. 460
5. In Section E, we prove a few technical results on the covering number. 461
A Experimental Setup4462
In this section, we review our numerical experiments and their setup both on synthetic and real-world 463
datasets in order to address theoretical results more clearly and intuitively. 464
A.1 Dataset 465
A.1.1 Emperical Dataset 466
The Matérn kernel is considered a generalization of the radial basis function (RBF) kernel. It 467
controls the differentiability, or smoothness, of the kernel through the parameter ν. Asν→ ∞ , the 468
Matérn kernel converges to the RBF kernel, and as ν→0, it converges to the Laplacian kernel, a 469
0-differentiable kernel. In this study, we utilized the Matérn kernel to generate Gaussian Process (GP) 470
samples based on the composition of two Matérn kernels, KgandKh, with varying differentiability 471
in the range [0.5,10] ×[0.5,10]. The input dimension ( din) of the kernel, the bottleneck mid-dimension 472
(dmid), and the output dimension ( dout) are 15, 3, and 20, respectively. 473
This outlines the general procedure of our sampling method for synthetic data: 474
1. Sample the training dataset X∈RD×din475
2. From X, compute the D×Dkernel Kgwith given νg 476
3. From Kg, sample Z∈RD×dmidwith columns sampled from the Gaussian N(0, Kg). 477
4. From Z, compute Kgwith given νh 478
5.From Kh, sample the test dataset Y∈RD×doutwith columns sampled from the Gaussian 479
N(0, Kh). 480
We utilized four AMD Opteron 6136 processors (2.4 GHz, 32 cores) and 128 GB of RAM to generate 481
our synthetic dataset. The maximum possible dataset size for 128 GB of RAM is approximately 482
52,500; however, we opted for a synthetic dataset size of 22,000 due to the computational expense 483
associated with sampling the Matérn kernel. This decision was made considering the time complexity 484
ofO(n3)and the space complexity of O(n2)involved. Out of the 22,000 dataset points, 20,000 were 485
allocated for training data, and 2,000 were used for the test dataset 486
A.1.2 Real-world dataset: WESAD 487
In our study, we utilized the Wearable Stress and Affect Detection (WESAD) dataset to train our 488
AccNets for binary classification. The WESAD dataset, which is publicly accessible, provides 489
multimodal physiological and motion data collected from 15 subjects using devices worn on the wrist 490
and chest. For the purpose of our experiment, we specifically employed the Empatica E4 wrist device 491
to distinguish between non-stress (baseline) and stress conditions, simplifying the classification task 492
to these two categories. 493
After preprocessing, the dataset comprised a total of 136,482 instances. We implemented a train-test 494
split ratio of approximately 75:25, resulting in 100,000 instances for the training set and 36,482 495
instances for the test set. The overall hyperparameters and architecture of the AccNets model applied 496
to the WESAD dataset were largely consistent with those used for our synthetic data. The primary 497
differences were the use of 100 epochs for each iteration of Ni from Ns, and a learning rate set to 498
1e-5. 499
4The code used for experiments are publicly available here
13Figure 4: A comparison: singular values of the weight matrices for DNN and AccNets models.
The first two plots represent cases where N= 10000 while the right two plots correspond to N=
200.The number of outliers at the top of each plot signifies the rank of each network. The plots with
N= 10000 datasets demonstrate a clearer capture of the true rank compared to those with N= 200
indicating that a higher dataset count provides more accurate rank determination
.
A.2 Model setups 500
To investigate the scaling law of test error for our synthetic data, we trained models using Ni 501
datapoints from our training data, where N= [100 ,200,500,1000,2000,5000,10000 ,20000] . The 502
models employed for this analysis included the kernel method, shallow networks, fully connected 503
deep neural networks (FC DNN), and AccNets. For FC DNN and AccNets, we configured the 504
network depth to 12 layers, with the layer widths set as [din,500,500, ...,500, dout]for DNNs, and 505
[din,900,100,900, ...,100,900, dout]for AccNets. 506
To ensure a comparable number of neurons, the width for the shallow networks was set to 50,000, 507
resulting in dimensions of [din,50000 , dout]. 508
We utilized ReLU as the activation function and L1-norm as the cost function, with the Adam 509
optimizer. The total number of batch was set to 5, and the training process was conducted over 3600 510
epochs, divided into three phases. The detailed optimizer parameters are as follows: 511
1. For the first 1200 epochs: learning rate (lr) = 1 .5∗0.001, weight decay = 0 512
2. For the second 1200 epochs: lr= 0.4∗0.001, weight decay = 0.002 513
3. For the final 1200 epochs: lr= 0.1∗0.001, weight decay = 0.005 514
We conducted experiments utilizing 12 NVIDIA V100 GPUs (each with 32GB of memory) over a 515
period of 6.3 days to train the synthetic dataset. In contrast, training the WESAD dataset required 516
only one hour on a single V100 GPU. 517
A.3 Additional experiments 518
B AccNet Generalization Bounds 519
The proof of generalization for shallow networks (Theorem 1) is the special case L= 1of the proof 520
of Theorem 2, so we only prove the second: 521
Theorem 6. Consider an accordion net of depth Land widths dL, . . . , d 0, with corresponding set 522
of functions F={fL:1:∥fℓ∥F1≤Rℓ,Lip(fℓ)≤ρℓ}with input space Ω = B(0, r). For any 523
ρ-Lipschitz loss function ℓ(x, f(x))with|ℓ(x, y)| ≤c0, we know that with probability 1−δover the 524
sampling of the training set Xfrom the distribution π, we have for all f∈ F 525
L(f)−˜LN(f)≤CρL:1rLX
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1logN√
N(1 +o(1)) + c0r
2 log 2/δ
N.
Proof. The strategy is: (1) prove a covering number bound on F(2) use it to obtain a Rademacher 526
complexity bound, (3) use the Rademacher complexity to bound the generalization error. 527
14(1) We define fℓ=Vℓ◦σ◦Wℓso that fθ=fL:1=fL◦ ··· ◦ f1. First notice that we can write each 528
fℓas convex combination of its neurons: 529
fℓ(x) =wℓX
i=1vℓ,iσ(wT
ℓ,ix) =RℓwℓX
i=1cℓ,i¯vℓ,iσ( ¯wT
ℓ,ix)
for¯wℓ,i=wℓ,i
∥wℓ,i∥,¯vℓ,i=vℓ,i
∥vℓ,i∥,Rℓ=Pℓ
i=1∥vℓ,i∥∥wℓ,i∥andcℓ,i=1
Rℓ∥vℓ,i∥∥wℓ,i∥. 530
Let us now consider a sequence ϵk= 2−kfork= 0, . . . , K and define ˜v(k)
ℓ,i,˜w(k)
ℓ,ito be the ϵk-covers 531
of¯vℓ,i,¯wℓ,i, furthermore we may choose ˜v(0)
ℓ,i= ˜w(0)
ℓ,i= 0since every unit vector is within a ϵ0= 1 532
distance of the origin. We will now show that on can approximate fθby approximating each of the fℓ 533
by functions of the form 534
˜fℓ(x) =RℓKℓX
k=11
Mk,ℓMk,ℓX
m=1˜v(k)
ℓ,i(k)
ℓ,mσ( ˜w(k)T
ℓ,i(k)
ℓ,mx)−˜v(k−1)
ℓ,i(k)
ℓ,mσ( ˜w(k−1)T
ℓ,i(k)
ℓ,mx)
for indices i(k)
ℓ,m= 1, . . . , w ℓchoosen adequately. Notice that the number of functions of this type 535
equals the number of Mk,ℓquadruples (˜v(k)
ℓ,i(k)
ℓ,m,˜w(k)T
ℓ,i(k)
ℓ,m,˜v(k−1)
ℓ,i(k)
ℓ,m,˜w(k−1)T
ℓ,i(k)
ℓ,m)where these vectors belong 536
to the ϵk- resp. ϵk−1-coverings of the din- resp. dout-dimensional unit sphere. Thus the number of 537
such functions is bounded by 538
KℓY
k=1 
N2(Sdin−1, ϵk)N2(Sdout−1, ϵk)N2(Sdin−1, ϵk−1)N2(Sdout−1, ϵk−1)Mk,ℓ,
and we have this choice for all ℓ= 1, . . . , L . We will show that with sufficiently large Mk,ℓthis set 539
of functions ϵ-covers Fwhich then implies that 540
logN2(F, ϵ)≤2LX
ℓ=1KℓX
k=1Mk,ℓ 
logN2(Sdin−1, ϵk−1) + log N2(Sdin−1, ϵk−1)
.
We will use the probabilistic method to find the right indices i(k)
ℓ,mto approximate a function fℓ= 541
RℓPwℓ
i=1cℓ,i¯vℓ,iσ( ¯wT
ℓ,ix)with a function ˜fℓ. We take all i(k)
ℓ,mto be i.i.d. equal to the index i= 542
1,···, wℓwith probability cℓ,i, so that in expectation 543
E˜fℓ(x) =RℓKℓX
k=1wℓX
i=1cℓ,i
˜v(k)
ℓ,iσ( ˜w(k)T
ℓ,ix)−˜v(k−1)
ℓ,iσ( ˜w(k−1)T
ℓ,ix)
=RℓwℓX
i=1cℓ,i˜v(K)
ℓ,iσ( ˜w(K)T
ℓ,ix).
We will show that this expectation is O(ϵKℓ)-close to fℓand that the variance of ˜fℓgoes to zero as 544
theMℓ,ks grow, allowing us to bound the expected error EfL:1−˜fL:12
π≤ϵ2which then implies 545
that there must be at least one choice of indices i(k)
ℓ,msuch thatfL:1−˜fL:1
π≤ϵ. 546
15Let us first bound the distance 547
fℓ(x)−E˜fℓ(x)=RℓwℓX
i=1cℓ,i
¯vℓ,iσ( ¯wT
ℓ,ix)−˜v(K)
ℓ,iσ( ˜w(K)T
ℓ,ix)
≤RℓwℓX
i=1cℓ,i
¯vℓ,i−˜v(K)
ℓ,i
σ( ¯wT
ℓ,ix)+˜v(K)
ℓ,i
σ( ¯wT
ℓ,ix)−σ( ˜w(K)T
ℓ,ix)
≤RℓwℓX
i=1cℓ,i¯vℓ,i−˜v(K)
ℓ,i¯wT
ℓ,ix+˜v(K)
ℓ,i¯wT
ℓ,ix−˜w(K)T
ℓ,ix
≤2RℓwℓX
i=1cℓ,iϵKℓ∥x∥
= 2RℓϵKℓ∥x∥.
Then we bound the trace of the covariance of ˜fℓwhich equals the expected square distance between 548
˜fℓand its expectation: 549
E˜fℓ(x)−E˜fℓ(x)2
=KℓX
k=1R2
ℓ
M2
k,ℓMk,ℓX
m=1E˜v(k)
ℓ,i(k)
ℓ,mσ( ˜w(k)T
ℓ,i(k)
ℓ,mx)−˜v(k−1)
ℓ,i(k)
ℓ,mσ( ˜w(k−1)T
ℓ,i(k)
ℓ,mx)−E
˜v(k)
ℓ,i(k)
ℓ,mσ( ˜w(k)T
ℓ,i(k)
ℓ,mx)−˜v(k−1)
ℓ,i(k)
ℓ,mσ( ˜w(k−1)T
ℓ,i(k)
ℓ,mx)2
≤KℓX
k=1R2
ℓ
M2
k,ℓMk,ℓX
m=1E˜v(k)
ℓ,mσ( ˜w(k)T
ℓ,mx)−˜v(k−1)
ℓ,mσ( ˜w(k−1)T
ℓ,mx)2
=KℓX
k=1R2
ℓ
Mk,ℓwℓX
i=1ci˜v(k)
ℓ,iσ
˜w(k)T
ℓ,ix
−˜v(k−1)
ℓ,iσ
˜w(k−1)T
ℓ,ix2
≤KℓX
k=12R2
ℓ∥x∥2
Mk,ℓwℓX
i=1ci˜v(k)
ℓ,i2˜w(k)
ℓ,i−˜w(k−1)
ℓ,i2
+ci˜v(k)
ℓ,i−˜v(k−1)
ℓ,i2˜w(k−1)
ℓ,i2
≤KℓX
k=14R2
ℓ∥x∥2
Mk,ℓ(ϵk+ϵk−1)2
≤KℓX
k=136R2
ℓ∥x∥2
Mk,ℓϵ2
k.
Putting both together, we obtain 550
Efℓ(x)−˜fℓ(x)2
≤4R2
ℓϵ2
Kℓ∥x∥2+KℓX
k=136R2
ℓ∥x∥2
Mk,ℓϵ2
k
= 4R2
ℓ∥x∥2 
ϵ2
Kℓ+ 9KℓX
k=1ϵ2
k
Mk,ℓ!
.
We will now use this bound, together with the Lipschitzness of fℓto bound the error 551
EfL:1(x)−˜fL:1(x)2
. We will do this by induction on the distances Efℓ:1(x)−˜fℓ:1(x)2
. 552
We start by 553
Ef1(x)−˜f1(x)2
≤4R2
1∥x∥2 
ϵ2
Kℓ+ 9KℓX
k=1ϵ2
k
Mk,1!
.
16And for the induction step, we condition on the layers fℓ−1:1 554
Efℓ:1(x)−˜fℓ:1(x)2
=E
Efℓ:1(x)−˜fℓ:1(x)2
|˜fℓ−1:1
=Efℓ:1(x)−Eh
˜fℓ:1(x)|˜fℓ−1:1i2
+EE˜fℓ:1(x)−Eh
˜fℓ:1(x)|˜fℓ−1:1i2
|˜fℓ−1:1
=Efℓ:1(x)−fℓ(˜fℓ−1:1(x))2
+EE˜fℓ:1(x)−fℓ(˜fℓ−1:1(x))2
|˜fℓ−1:1
≤ρ2
ℓEfℓ−1:1(x)−˜fℓ−1:1(x)2
+ 4R2
ℓE˜fℓ−1:1(x)2 
ϵ2
Kℓ+ 9KℓX
k=1ϵ2
k
Mk,ℓ!
.
Now since 555
E˜fℓ−1:1(x)2
≤ ∥fℓ−1:1(x)∥2+Efℓ−1:1(x)−˜fℓ−1:1(x)2
≤ρ2
ℓ−1···ρ2
1∥x∥2+Efℓ−1:1(x)−˜fℓ−1:1(x)2
we obtain that 556
Efℓ:1(x)−˜fℓ:1(x)2
≤ 
ρ2
ℓ+ 4R2
ℓ 
ϵ2
Kℓ+ 9KℓX
k=1ϵ2
k
Mk,ℓ!!
Efℓ−1:1(x)−˜fℓ−1:1(x)2
+ 4R2
ℓρ2
ℓ−1···ρ2
1∥x∥2 
ϵ2
Kℓ+ 9KℓX
k=1ϵ2
k
Mk,ℓ!
.
We define ˜ρ2
ℓ=ρ2
ℓh
1 + 4R2
ℓ
ρ2
ℓ
ϵ2
Kℓ+ 9PKℓ
k=1ϵ2
k
Mk,ℓi
and obtain 557
EfL:1(x)−˜fL:1(x)2
≤4LX
ℓ=1˜ρ2
L:ℓ+1R2
ℓρ2
ℓ−1:1∥x∥2 
ϵ2
Kℓ+ 9KℓX
k=1ϵ2
k
Mk,ℓ!
.
Thus for any distribution πover the ball B(0, r), there is a choice of indices i(k)
ℓ,msuch that 558
fL:1−˜fL:12
π≤4LX
ℓ=1˜ρ2
L:ℓ+1R2
ℓρ2
ℓ−1:1r2 
ϵ2
Kℓ+ 9KℓX
k=1ϵ2
k
Mk,ℓ!
.
We now simply need to choose KℓandMk,ℓadequately. To reach an error of 2ϵ, we choose 559
Kℓ=&
−logϵ+1
2log"
4ρ2
L:1r2 LX
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1!
Rℓ
ρℓp
dℓ+dℓ−1#'
where ρL:1=QL
ℓ=1ρℓ. Notice that that ϵ2
Kℓ≤1
4ρ2
L:1r2PL
ℓ′=1Rℓ′
ρℓ′√
dℓ′+dℓ′−1ρℓ√
dℓ+dℓ−1
Rℓϵ2. 560
Given s0=P∞
k=1√
k2−k≈1.3473 <∞, we define 561
Mk,ℓ=&
36ρ2
L:1r2s0 LX
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1!
Rℓ
ρℓp
dℓ+dℓ−12−k
√
k1
ϵ2'
.
17So that for all ℓ 562
4R2
ℓ
ρ2
ℓ 
ϵ2
Kℓ+ 9KℓX
k=1ϵ2
k
Mk,ℓ!
≤Rℓ
ρℓp
dℓ+dℓ−1
ρ2
L:1r2PL
ℓ′=1Rℓ
ρℓp
dℓ+dℓ−1ϵ2
+Rℓ
ρℓp
dℓ+dℓ−1
ρ2
L:1r2PL
ℓ′=1Rℓ
ρℓp
dℓ+dℓ−1ϵ2PKℓ
k′=1√
k′2−k′
s0
≤2Rℓ
ρℓp
dℓ+dℓ−1
ρ2
L:1r2PL
ℓ′=1Rℓ
ρℓp
dℓ+dℓ−1ϵ2.
Now this also implies that 563
˜ρℓ≤ρℓexp
2Rℓ
ρℓp
dℓ+dℓ−1
ρ2
L:1r2PL
ℓ′=1Rℓ
ρℓp
dℓ+dℓ−1ϵ2

and thus 564
˜ρL:ℓ+1≤ρL:ℓ+1exp
2PL
ℓ′=ℓ+1Rℓ
ρℓp
dℓ+dℓ−1
ρ2
L:1r2PL
ℓ′=1Rℓ
ρℓp
dℓ+dℓ−1ϵ2
≤ρL:ℓ+1exp2
ρ2
L:1r2ϵ2
.
Putting it all together, we obtain that 565
fL:1−˜fL:12
π≤4LX
ℓ=1˜ρ2
L:ℓ+1R2
ℓρ2
ℓ−1:1r2 
ϵ2
Kℓ+ 9KℓX
k=1ϵ2
k
Mk,ℓ!
≤exp2
ρ2
L:1r2ϵ2
ρ2
L:1r2LX
ℓ=14R2
ℓ
ρ2
ℓ 
ϵ2
Kℓ+ 9KℓX
k=1ϵ2
k
Mk,ℓ!
≤2 exp2
ρ2
L:1r2ϵ2
ϵ2
= 2ϵ2+O(ϵ4).
Now since logN2(Sdℓ−1, ϵ) =dℓlog 1
ϵ+ 1
and 566
Mk,ℓ≤36ρ2
L:1r2s0 LX
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1!
Rℓ
ρℓp
dℓ+dℓ−12−k
√
k1
ϵ2+ 1,
we have 567
logN2(F,√
2 expϵ2
ρ2
L:1r2
ϵ)≤2LX
ℓ=1KℓX
k=1Mk,ℓ 
logN2(Sdℓ−1, ϵk−1) + log N2(Sdℓ−1−1, ϵk−1)
≤2LX
ℓ=1KℓX
k=1Mk,ℓ(dℓ+dℓ−1) log(1
ϵk−1+ 1)
≤72s0ρ2
L:1r2 LX
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1!LX
ℓ=1Rℓ
ρℓp
dℓ+dℓ−1KℓX
k=12−klog(1
ϵk−1+ 1)
√
k1
ϵ2
+ 2LX
ℓ=1(dℓ+dℓ−1)KℓX
k=1log(1
ϵk−1+ 1)
≤72s2
0ρ2
L:1r2 LX
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1!2
1
ϵ2+o(ϵ−2).
18The diameter of Fis smaller than ρL:1r, so for all δ≥ρL:1r,logN2(F, δ) = 0 . For all δ≤ρL:1r 568
we choose ϵ=δ√
2eso that√
2 exp
ϵ2
ρ2
L:1r2
ϵ≤δand therefore 569
logN2(F, δ)≤144s2
0eρ2
L:1r2 LX
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1!2
1
δ2+o(δ−2).
(2) Our goal now is to use chaining / Dudley’s theorem to bound the Rademacher complexity 570
R(F(X))evaluated on a set Xof size N(e.g. Lemma 27.4 in [Understanding Machine Learning]) 571
of our set: 572
Lemma 7. Letc= max f∈F1√
N∥f(X)∥, then for any integer M > 0, 573
R(F(X))≤c2−M+6c√
NMX
k=12−kq
logN(F, c2−k).
To apply it to our setting, first note that for all x∈B(0, r),∥fL:1(x)∥ ≤ ρL:1rso that c= 574
max f∈F1√
N∥f(X)∥ ≤ρL:1r, we then have 575
R(F(X))≤c2−M+6c√
NMX
k=12−k12s0√eρL:1rLX
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1c−12k(1 +o(1))
=c2−M+72√
NMs0√eρL:1rLX
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1(1 +o(1)).
Taking M=l
−log2
72√
Ns0√ePL
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1m
, we obtain 576
R(F(X))≤72√
NMs0√eρL:1rLX
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1(1 +M(1 +o(1)))
≤144√
NMs0√eρL:1rLX
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1&
−log2 
72√
Ns0√eLX
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1!'
(1 +o(1))
≤CρL:1rLX
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1logN√
N(1 +o(1)).
(3) For any ρ-Lipschitz loss function ℓ(x, f(x))with|ℓ(x, y)| ≤c0, we know that with probability 577
1−δover the sampling of the training set Xfrom the distribution π, we have for all f∈ F 578
Ex∼π[ℓ(x, f(x))]−1
NNX
i=1ℓ(xi, f(xi))≤2EX′[R(ℓ◦ F(X′))] + c0r
2 log 2/δ
N
≤2CρL:1rLX
ℓ′=1Rℓ′
ρℓ′p
dℓ′+dℓ′−1logN√
N(1 +o(1)) + c0r
2 log 2/δ
N.
579
C Composition of Sobolev Balls 580
Proposition 8 (Proposition 3 from the main.) .Given a distribution πwith support in B(0, r), we 581
have that with probability 1−δfor all functions f∈ F={f:∥f∥Wν,2≤R,∥f∥∞≤R} 582
L(f)−˜LN(f)≤2C1REν/d(N) +c0r
2 log 2/δ
N.
where Er(N) =N−1
2ifr >1
2,Er(N) =N−1
2logNifr=1
2, and Er(N) =N−rifr <1
2. 583
19Proof. (1) We know from Theorem 5.2 of [ 9] that the Sobolev ball BWν,2(0, R)over any d- 584
dimensional hypercube Ωsatisfies 585
logN2(BWν,2(0, R), π, ϵ)≤C0R
ϵd
ν
for a constant cand any measure πsupported in the hypercube. 586
(2) By Dudley’s theorem we can bound the Rademacher complexity of our function class B(X) 587
evaluated on any training set X: 588
R(B(X))≤R2−M+6R√
NMX
k=12−ks
C0R
R2−kd
ν
=R2−M+6R√
Np
C0MX
k=12k(d
2ν−1).
If2ν=d, we take M=1
2logNand obtain the bound 589
R√
N+6R√
Np
C01
2logN≤C1RlogN√
N.
If2ν > d , we take M=∞and obtain the bound 590
6R√
Np
C0 
2d
2ν−1
1−2d
2ν−1!
≤C1R1√
N.
If2ν < d , we take M=ν
dlogNand obtain the bound 591
R2−M+6R√
Np
C02d
2ν−1 
2M(d
2ν−1)−1
2d
2ν−1−1!
≤C1RN−ν
d.
Putting it all together, we obtain that R(B(X))≤C1Eν/d(N). 592
(3) For any ρ-Lipschitz loss function ℓ(x, f(x))with|ℓ(x, y)| ≤c0, we know that with probability 593
1−δover the sampling of the training set Xfrom the distribution π, we have for all f∈ F 594
Ex∼π[ℓ(x, f(x))]−1
NNX
i=1ℓ(xi, f(xi))≤2EX′[R(ℓ◦ F(X′))] + c0r
2 log 2/δ
N
≤2C1Eν/d(N) +c0r
2 log 2/δ
N.
595
Proposition 9. LetF1, . . . ,FLbe set of functions mapping through the sets Ω0, . . . , ΩL, then if all 596
functions in Fℓareρℓ-Lipschitz, we have 597
logN2(FL◦ ··· ◦ F 1,LX
ℓ=1ρL:ℓ+1ϵℓ)≤LX
ℓ=1logN2(Fℓ, ϵℓ).
Proof. For any distribution π0onΩthere is a ϵ1-covering ˜F1ofF1with˜F1≤ N 2(F1, ϵ1)then 598
for any ˜f1∈˜F1we choose a ϵ2-covering ˜F2w.r.t. the measure π1which is the measure of f1(x)if 599
x∼π0ofF2with˜F2≤ N 2(F2, ϵ), and so on until we obtain coverings for all ℓ. Then the set 600
˜F=n
˜fL◦ ··· ◦ ˜f1:˜f1∈˜F1, . . . , ˜fL∈˜FLo
is aPL
ℓ=1ρL:ℓ+1ϵℓ-covering of F=FL◦ ··· ◦ F 1, 601
20indeed for any f=fL:1we choose ˜f1∈˜F1, . . . , ˜fL∈˜FLthat cover f1, . . . , f L, then ˜fL:1covers 602
fL:1: 603
fL:1−˜fL:1
π≤LX
ℓ=1fL:ℓ◦˜fℓ−1:1−fL:ℓ+1◦˜fℓ:1
π
≤LX
ℓ=1fL:ℓ−fL:ℓ+1◦˜fℓ
πℓ−1
≤LX
ℓ=1ρL:ℓ+1ϵℓ,
and log cardinality of the set ˜Fis boundedPL
ℓ=1logN2(Fℓ, ϵℓ). 604
Theorem 10. Let F = FL◦ ··· ◦ F 1 where Fℓ = 605
fℓ:Rdℓ−1→Rdℓs.t.∥fℓ∥Wνℓ,2≤Rℓ,∥fℓ∥∞≤bℓ, Lip(fℓ)≤ρℓ	
, and let r∗= min ℓrℓ 606
forrℓ=νℓ
dℓ−1, then with probability 1−δwe have for all f∈ F 607
L(f)−˜LN(f)≤ρC0 LX
ℓ=1(CℓρL:ℓ+1Rℓ)1
r∗+1!r∗+1
Er∗(N) +c0r
2 log 2/δ
N,
where Cℓdepends only on dℓ−1, dℓ, νℓ, bℓ−1. 608
Proof. (1) We know from Theorem 5.2 of [ 9] that the Sobolev ball BWνℓ,2(0, Rℓ)over any dℓ- 609
dimensional hypercube Ωsatisfies 610
logN2(BWν,2(0, Rℓ), πℓ−1, ϵℓ)≤
CℓRℓ
ϵℓ1
rℓ
for a constant Cℓthat depends on the size of hypercube and the dimension dℓand the regularity νℓ 611
and any measure πℓ−1supported in the hypercube. 612
Thus Proposition 9 tells us that the composition of the Sobolev balls satisfies 613
logN2(FL◦ ··· ◦ F 1,LX
ℓ=1ρL:ℓ+1ϵℓ)≤LX
ℓ=1
CℓRℓ
ϵℓ1
rℓ.
Given r∗= min ℓrℓ, we can bound it byPL
ℓ=1
CℓRℓ
ϵℓ1
r∗and by then choosing ϵℓ= 614
ρ−1
L:ℓ+1(ρL:ℓ+1CℓRℓ)1
r∗+1
P
ℓ(ρL:ℓ+1CℓRℓ)1
r∗+1ϵ, we obtain that 615
logN2(FL◦ ··· ◦ F 1, ϵ)≤ LX
ℓ=1(ρL:ℓ+1CℓRℓ)1
r∗+1!r∗+1
ϵ−1
r∗.
(2,3) It the follows by a similar argument as in points (2, 3) of the proof of Proposition 8 that there is 616
a constant C0such that with probability 1−δfor all f∈ F 617
L(f)−˜LN(f)≤C0 LX
ℓ=1(ρL:ℓ+1CℓRℓ)1
r∗+1!r∗+1
Er∗(N) +c0r
2 log 2/δ
N
618
D Generalization at the Regularized Global Minimum 619
In this section, we first give the proof of Theorem 5 and then present detailed proofs of lemmas used 620
in the proof. The lemmas are largely inspired by [5] and may be of independent interest. 621
21D.1 Theorem 5 in Section 4.2 622
Theorem 11 (Theorem 5 in the main) .Given a true function f∗
L∗:1=f∗
L∗◦···◦ f∗
1going through the 623
dimensions d∗
0, . . . , d∗
L∗, along with a continuous input distribution π0supported in B(0, b0), such 624
that the distributions πℓoff∗
ℓ(x)(forx∼π0) are continuous too and supported inside B(0, bℓ)⊂ 625
Rd∗
ℓ. Further assume that there are differentiabilities νℓand radii Rℓsuch that ∥f∗
ℓ∥Wνℓ,2(B(0,bℓ))≤ 626
Rℓ, and ρℓsuch that Lip(f∗
ℓ)≤ρℓ. For a infinite width AccNet with L≥L∗and constant width 627
d≥d∗
1, . . . , d∗
L∗−1, we have for the ratios ˜rℓ=νℓ
d∗
ℓ+3: 628
•At a global minimizer ˆfL:1of the regularized loss f1, . . . , f L7→˜LN(fL:1)+λR(f1, . . . , f L), 629
we have L(ˆfL:1) =˜O(N−min{1
2,˜r1,...,˜rL∗}). 630
•At a global minimizer ˆfL:1of the regularized loss f1, . . . , f L7→˜LN(fL:1)+λQL
ℓ=1∥fℓ∥F1, 631
we have L(ˆfL:1) =˜O(N−1
2+PL∗
ℓ=1max{0,˜rℓ−1
2}). 632
Proof. Iff∗=f∗
L∗◦ ··· ◦ f∗
1withL∗≤L, intermediate dimensions d∗
0, . . . , d∗
L∗, along with a 633
continuous input distribution π0supported in B(0, b0), such that the distributions πℓoff∗
ℓ(x)(for 634
x∼π0) are continuous too and supported inside B(0, bℓ)⊂Rd∗
ℓ. Further assume that there are 635
differentiabilities ν∗
ℓand radii Rℓsuch that ∥f∗
ℓ∥Wν∗
ℓ,2(B(0,bℓ))≤Rℓ. 636
We first focus on the L=L∗case and then extend to the L > L∗case. 637
Each f∗
ℓcan be approximated by another function ˜fℓwith bounded F1-norm and Lipschitz constant. 638
Actually if 2ν∗
ℓ≥d∗
ℓ−1+ 3one can choose ˜fℓ=f∗
ℓsince∥f∗
ℓ∥F1≤CℓRℓby Lemma 14, and by 639
assumption Lip(˜fℓ)≤ρℓ. If2ν∗
ℓ< d∗
ℓ−1+ 3, then by Lemma 13 we know that there is a ˜fℓwith 640˜fℓ
F1≤CℓRℓϵ−1
2˜rℓ+1
ℓandLip(˜fℓ)≤CℓLip(f∗
ℓ)≤Cℓρℓand error 641
f∗
ℓ−˜fℓ
L2(πℓ−1)≤cℓf∗−˜fℓ
L2(B(0,bℓ))≤cℓϵℓ.
Therefore the composition ˜fL:1satisfies 642
f∗
L:1−ˆfL:1
L2(πℓ−1)≤LX
ℓ=1˜fL:ℓ+1◦f∗
ℓ:1−˜fL:ℓ◦f∗
ℓ−1:1
L2(π)
≤LX
ℓ=1Lip(˜fL:ℓ+1)cℓϵℓ
≤LX
ℓ=1ρL:ℓ+1CL:ℓ+1cℓϵℓ.
For any L≥L∗, dimensions dℓ≥d∗
ℓand widths wℓ≥N, we can build an AccNet that fits eactly 643
˜fL:1, by simply adding zero weights along the additional dimensions and widths, and by adding 644
identity layers if L > L∗, since it is possible to represent the identity on Rdwith a shallow network 645
with2dneurons and F1-norm 2d(by having two neurons eiσ(eT
i·)and−eiσ(−eT
i·)for each basis 646
ei). Since the cost in parameter norm of representing the identity scales with the dimension, it is 647
best to add those identity layers at the minimal dimension min{d∗
0, . . . , d∗
L∗}. We therefore end up 648
with a AccNet with L−L∗identity layers (with F1norm 2 min{d∗
0, . . . , d∗
L∗}) and L∗layers that 649
approximate each of the f∗
ℓwith a bounded F1-norm function ˜fℓ. 650
Since f∗
L:1has zero population loss, the population loss of the AccNet ˜fL:1is bounded by 651
ρPL
ℓ=1ρL:ℓ+1CL:ℓ+1cℓϵℓ. By McDiarmid’s inequality, we know that with probability 1−δover the 652
sampling of the training set, the training loss is bounded by ρPL
ℓ=1ρL:ℓ+1CL:ℓ+1cℓϵℓ+Bq
2 log 2/δ
N. 653
(1) The global minimizer ˆfL:1=ˆfL◦ ··· ◦ ˆf1of the regularized loss (with the first regularization 654
22term) is therefore bounded by 655
ρLX
ℓ=1ρL:ℓ+1CL:ℓ+1cℓϵℓ+Br
2 log 2/δ
N
+λ√
2d"L∗Y
ℓ=1CℓρℓL∗X
ℓ=11
Cℓρℓ(
CℓRℓ 2ν∗
ℓ≥d∗
ℓ−1+ 3
CℓRℓϵ−1
2˜rℓ+1
ℓ2ν∗
ℓ< d∗
ℓ−1+ 3+ 2(L−L∗) min{d∗
0, . . . , d∗
L∗}#
.
Taking ϵℓ=E˜rmin(N)andλ=N−1
2logN, this is upper bounded by 656
"
ρLX
ℓ=1ρL:ℓ+1CL:ℓ+1cℓ+C√
2drL∗Y
ℓ=1CℓρℓL∗X
ℓ=1Rℓ
ρℓ+ 2(L−L∗) min{d∗
0, . . . , d∗
L∗}#
E˜rmin(N)+Br
2 log 2/δ
N.
which implies that at the globla minimizer of the regularized loss, the (unregularized) train loss is of 657
order E˜rmin(N)and the complexity measure R(ˆf1, . . . , ˆfL)is of order1
NE˜rmin(N)which implies 658
that the test error will be of order 659
L(f)≤"
2ρLX
ℓ=1ρL:ℓ+1CL:ℓ+1cℓ+ 2C√
2drL∗Y
ℓ=1CℓρℓL∗X
ℓ=1Rℓ
ρℓ+ 2(L−L∗) min{d∗
0, . . . , d∗
L∗}#
E˜rmin(N)
+ (2B+c0)r
2 log 2/δ
N.
(2) Let us now consider adding the closer to traditional L2-regularization Lλ(fL:1) =L(fL:1) + 660
λQL
ℓ=1∥fℓ∥F1. ,We see that the global minimizer ˆfL:1of the L2-regularized loss is upper bounded 661
by 662
ρLX
ℓ=1ρL:ℓ+1CL:ℓ+1cℓϵℓ+Br
2 log 2/δ
N+λ"L∗Y
ℓ=1(
CℓRℓ 2ν∗
ℓ≥d∗
ℓ−1+ 3
CℓRℓϵ−1
2˜rℓ+1
ℓ2ν∗
ℓ< d∗
ℓ−1+ 3#
(2 min {d∗
0, . . . , d∗
L∗})(L−L∗).
Which for ϵℓ=E˜rmin(N)andλ=N−1
Nis upper bounded by 663
ρLX
ℓ=1ρL:ℓ+1CL:ℓ+1cℓE˜rmin(N)+Br
2 log 2/δ
N+N−1
2"L∗Y
ℓ=1CℓRℓ√
NE˜rmin(N)#
(2 min {d∗
0, . . . , d∗
L∗})(L−L∗).
Which implies that both the train error is of order N−1
2QL∗
ℓ=1√
NE˜rmin(N)and the product of the 664
F1-norms is of orderQL∗
ℓ=1√
NE˜rmin(N). 665
Now note that the product of the F1-norms bounds the complexity measure up to a constant since 666
Lip(f)≤ ∥f∥F1667
R(f1, . . . , f L) =rLY
ℓ=1Lip(fℓ)LX
ℓ=1∥fℓ∥F1
Lip(fℓ)p
dℓ−1+dℓ≤L√
2dLY
ℓ=1∥f∥F1.
And since at the global minimum the product of the F1-norms is of orderQL∗
ℓ=1√
NE˜rmin(N)the 668
test error will of orderQL∗
ℓ=1√
NE˜rℓ(N)
logN√
N. 669
Note that if there is at a most one ℓwhere ˜rℓ>1
2then the rate is up to log term the same as 670
E˜rmin(N). 671
D.2 Lemmas on approximating Sobolev functions 672
Now we present the lemmas used in this proof above that concern the approximation errors and 673
Lipschitz constants of Sobolev functions and compositions of them. We will bound the F2-norm and 674
note that the F2-norm is larger than the F1-norm, cf. [5, Section 3.1]. 675
23Lemma 12 (Approximation for Sobolev function with bounded error and Lipschitz constant) . 676
Suppose g:Sd→Ris an even function with bounded Sobolev norm ∥g∥2
Wν,2≤Rwith2ν≤d+ 2, 677
with inputs on the unit d-dimensional sphere. Then for every ϵ >0, there is ˆg∈ G2with small 678
approximation error ∥g−ˆg∥L2(Sd)=C(d, ν, R )ϵ, bounded Lipschitzness Lip(ˆg)≤C′(d)Lip( g), 679
and bounded norm 680
∥ˆg∥F2≤C′′(d, ν, R )ϵ−d+3−2ν
2ν.
Proof. Given our assumptions on the target function g, we may decompose g(x) =P∞
k=0gk(x) 681
along the basis of spherical harmonics with g0(x) =R
Sdg(y)dτd(y)being the mean of g(x)over the 682
uniform distribution τdoverSd. The k-th component can be written as 683
gk(x) =N(d, k)Z
Sdg(y)Pk(xTy)dτd(y)
withN(d, k) =2k+d−1
k k+d−2
d−1
and a Gegenbauer polynomial of degree kand dimension d+ 1: 684
Pk(t) = (−1/2)kΓ(d/2)
Γ(k+d/2)(1−t2)(2−d)/2dk
dtk(1−t2)k+(d−2)/2,
known as Rodrigues’ formula. Given the assumption that the Sobolev norm ∥g∥2
Wν,2is upper 685
bounded, we have ∥f∥2
L2(Sd)≤C0(d, ν)Rforf= ∆ν/2gwhere ∆is the Laplacian on Sd[18,5]. 686
Note that gkare eigenfunctions of the Laplacian with eigenvalues k(k+d−1)[4], thus 687
∥gk∥2
L2(Sd)=∥fk∥2
L2(Sd)(k(k+d−1))−ν≤ ∥fk∥2
L2(Sd)k−2ν≤C1(d, ν, R )k−2ν−1(1)
where the last inequality holds because ∥f∥2
L2(Sd)=P
k≥0∥fk∥2
L2(Sd)converges. Note using the 688
Hecke-Funk formula, we can also write gkas scaled pkfor the underlying density pof the F1and 689
F2-norms: 690
gk(x) =λkpk(x)
where λk=ωd−1
ωdR1
−1σ(t)Pk(t)(1−t2)(d−2)/2dt= Ω( k−(d+3)/2)[5, Appendix D.2] and ωd 691
denotes the surface area of Sd. Then by definition of ∥ · ∥F2, for some probability density p, 692
∥g∥2
F2=Z
Sd|p|2dτ(v) =∥p∥2
L2(Sd)=X
0≤k∥pk∥2
L2(Sd)=X
0≤kλ−2
k∥gk∥2
L2(Sd).
Now to approximate g, consider function ˆgdefined by truncating the “high frequencies” of g, i.e. 693
setting ˆgk= 1[k≤m]gkfor some m > 0we specify later. Then we can bound the norm with 694
∥ˆg∥2
F2=X
0≤k:λk̸=0λ−2
k∥ˆgk∥2
L2(Sd)=X
0≤k≤m
λk̸=0λ−2
k∥gk∥2
L2(Sd)
(a)
≤C2(d, ν, R )X
0≤k≤mkd+2−2ν
(b)
≤C3(d, ν, R )md+3−2ν
where (a) uses Eq 1 and λk= Ω(k−(d+3)/2); (b) approximates by integral. 695
To bound the approximation error, 696
∥g−ˆg∥2
L2(Sd)=X
k>mgk2
L2(Sd)≤X
k>m∥gk∥2
L2(Sd)
≤C4(d, ν, R )X
k>mk−2ν−1
≤C5(d, ν, R )m−2νby integral approximation.
Finally, choosing m=ϵ−1
ν, we obtain ∥g−ˆg∥L2(Sd)≤C(d, ν, R )ϵand 697
∥ˆg∥F2≤C′(d, ν, R )ϵ−d+3−2ν
2ν.
24Then it remains to bound Lip(ˆg)for our constructed approximation. By construction and by [ 13, 698
Theorem 2.1.3], we have ˆg=g∗hwith now 699
h(t) =mX
k=0hkPk(t), t∈[−1,1]
by orthogonality of the Gegenbauer polynomial Pk’s and the convolution is defined as 700
(g∗h)(x):=1
ωdZ
Sdg(y)h(⟨x, y⟩)dy.
The coefficients for 0≤k≤mgiven by [13, Theorem 2.1.3] are 701
hk(a)=ωd+1
ωdΓ(d−1)
Γ(d−1 +k)Pk(1)k!(k+ (d−1)/2)Γ(( d−1)/2)2
π22−dΓ(d−1 +k)(b)=Ok
Γ(d−1 +k)
where (a) follows from the (inverse of) weighted L2norm of Pk; (b) plugs in the unit constant 702
Pk(1) =Γ(k+d−1)
Γ(d−1)k!and suppresses the dependence on d. Note that the constant factorΓ(d−1)
Γ(d−1+k)703
comes from the difference in the definitions of the Gegenbauer polynomials here and in [ 13]. Then 704
we can bound 705
∥∇ˆg(x)∥op≤Z
Sd∥∇g(y)∥op|h(⟨x, y⟩)|dy
≤Lip(g)Z
Sd|h(⟨x, y⟩)|dy
≤√ωdLip(g)Z
Sdh(⟨x, y⟩)2dy1/2
by Cauchy-Schwartz
=√ωdLip(g)
mX
k,j=0Z
SdhkhjPk(⟨x, y⟩)Pj(⟨x, y⟩)dy
1/2
=√ωdLip(g)
mX
k,j=0Z1
−1hkhjPk(t)Pj(t)(1−t2)d−2
2dt
1/2
by [13, Eq A.5.1]
=√ωdLip(g) mX
k=0h2
kZ1
−1Pk(t)2(1−t2)d−2
2dt!1/2
by orthogonality of Pk’s w.r.t. this measure
=√ωdLip(g) mX
k=0h2
kπ22−dΓ(d−1 +k)
k!(k+ (d−1)/2)Γ(( d−1)/2)2!1/2
=√ωdLip(g) 
O(1) +mX
k=1Ok
Γ(d−1 +k)k!!1/2
=√ωdLip(g)C(d)
for some constant C(d)that only depends on d. Hence Lip(ˆg) =C′(d)Lip( g). 706
The next lemma adapts Lemma 12 to inputs on balls instead of spheres following the construction in 707
[5, Proposition 5]. 708
Lemma 13. Suppose f:B(0, b)→Rhas bounded Sobolev norm ∥f∥2
Wν,2≤Rwithν≤(d+2)/2 709
even, where B(0, b) ={x∈Rd:∥x∥2≤b}is the radius- bball. Then for every ϵ >0there exists 710
fϵ∈ F2such that ∥f−fϵ∥L2(B(0,b))=C(d, ν, b, R )ϵ,Lip(fϵ)≤C′(b, d)Lip( f), and 711
∥fϵ∥F2≤C′′(d, ν, b, R )ϵ−d+3−2ν
2ν
Proof. Define g(z, a) =f 2bz
a
aon(z, a)∈Sdwithz∈Rdand1√
2≤a∈R. One may 712
verify that unit-norm (z, a)witha≥1√
2is sufficient to cover B(0, b)by setting x=bz
aand 713
25solve for (z, a). Then we have bounded ∥g∥2
Wν,2≤bνRand may apply Lemma 12 to get ˆgwith 714
∥g−ˆg∥L2(Sd)≤C(d, ν, b, R )ϵ.Letting fϵ(x) = ˆg ax
b, a
a−1for the corresponding ax
b, a
∈Sd 715
gives the desired upper bounds. 716
Lemma 14. Suppose f:B(0, b)→Rhas bounded Sobolev norm ∥f∥2
Wν,2≤Rwithν≥(d+3)/2 717
even. Then f∈ F2and∥f∥F2≤C(d, ν)bνR. 718
In particular, Wν,2⊆ F 2forν≥(d+ 3)/2even. 719
Proof. This lemma reproduces [ 5, Proposition 5] to functions with bounded Sobolev L2norm instead 720
ofL∞norm. The proof follows that of Lemma 12 and Lemma 13 and noticing that by Eq 1, 721
∥g∥2
F2=X
0≤k:λk̸=0λ−2
k∥gk∥2
L2(Sd)
≤X
0≤kkd+3−2ν∥(∆ν/2g)k∥2
L2(Sd)
≤ ∥∆ν/2g∥2
L2(Sd)
≤C1(d, ν)∥g∥2
Wν,2
≤C1(d, ν)R.
722
Finally, we remark that the above lemmas extend straightforward to functions f:B(0, b)→Rd′
723
with multi-dimensional outputs, where the constants then depend on the output dimension d′too. 724
D.3 Lemma on approximating compositions of Sobolev functions 725
With the lemmas given above and the fact that the F2-norm upper bounds the F1-norm, we can find 726
infinite-width DNN approximations for compositions of Sobolev functions, which is also pointed out 727
in the proof of Theorem 5. 728
Lemma 15. Assume the target function f: Ω→Rdout, with Ω⊆B(0, b)⊆Rdin, satisfies: 729
•f=gk◦ ··· ◦ g1a composition of kSobolev functions gi:Rdi→Rdi+1with bounded 730
norms ∥gi∥2
Wνi,2≤Rfori= 1, . . . , k , with d1=din; 731
•fis Lipschitz, i.e. Lip(gi)<∞fori= 1, . . . , k . 732
Ifνi≤(di+ 2)/2for any i, i.e. less smooth than needed, for depth L≥kand any ϵ >0, there is an 733
infinite-width DNN ˜fsuch that 734
•Lip(˜f)≤C1Qk
i=1Lip(gi); 735
•∥˜f−f∥L2≤C2ϵ; 736
the constants C1depends on all of the input dimensions di(togi) and dout, and C2depends on 737
di, dout, νi, b, R, k , and Lip(gi)for all i. 738
If otherwise νi≥(di+ 3)/2for all i, we can have ˜f=fwhere each layer has a parameter norm 739
bounded by C3R, with C3depending on di, dout, νi, and b. 740
Proof. Note that by Lipschitzness, 741
(gi◦ ··· ◦ g1)(Ω)⊆B
0, biY
j=1Lip(gj)
,
i.e. the pre-image of each component lies in a ball. By Lemma 12, for each gi, ifνi≤(di+ 2)/2, 742
we have an approximation ˆgion a slightly larger ball b′
i=bQi−1
j=1C′′(dj, dj+1)Lip( gj)such that 743
26•∥gi−ˆgi∥L2≤C(di, di+1, νi, b′
i, R)ϵ; 744
•∥ˆgi∥F2≤C′(di, di+1, νi, b′
i, R)ϵdi+3−2νi
2νi; 745
•Lip(ˆgi)≤C′′(di, di+1)Lip( gi); 746
where diis the input dimension of gi. Write the constants as Ci,C′
i, and C′′
ifor notation simplicity. 747
Note that the Lipschitzness of the approximations ˆgi’s guarantees that, when they are composed, 748
(ˆgi−1◦ ··· ◦ ˆg1)(Ω) lies in a ball of radius b′
i=bQi−1
j=1C′′
jLip(gj), hence the approximation error 749
remains bounded while propagating. While each ˆgiis a (infinite-width) layer, for the other L−k 750
layers, we may have identity layers5. 751
Let˜fbe the composed DNN of these layers. Then we have 752
Lip(˜f)≤kY
i=1C′′
iLip(gi) =C′′(d1, . . . , d k, dout)kY
i=1Lip(gi)
and approximation error 753
∥˜f−f∥L2≤kX
i=1CiϵY
j>iC′′
jLip(gj) =O(ϵ)
where the last equality suppresses the dependence on di, dout, νi, b, R, k , and Lip(gi)fori= 754
1, . . . , k . 755
In particular, by Lemma 14, if νi≥(di+ 3)/2for any i= 1, . . . , k , we can take ˆgi=gi. If this 756
holds for all i, then we can have ˜f=fwhile each layer has a F2-norm bounded by O(R). 757
E Technical results 758
Here we show a number of technical results regarding the covering number. 759
First, here is a bound for the covering number of Ellipsoids, which is a simple reformulation of 760
Theorem 2 of [17]: 761
Theorem 16. Thed-dimensional ellipsoid E={x:xTK−1x≤1}with radii√λiforλithei-th 762
eigenvalue of Ksatisfies logN2(E, ϵ) =Mϵ(1 +o(1)) for 763
Mϵ=X
i:√λi≥ϵlog√λi
ϵ
if one has log√λ1
ϵ=o
M2
ϵ
kϵlogd
forkϵ=
i:√λi≥ϵ	764
For our purpose, we will want to cover a unit ball B={w:∥w∥ ≤1}w.r.t. to a non-isotropic norm 765
∥w∥2
K=wTKw, but this is equivalent to covering Ewith an isotropic norm: 766
Corollary 17. The covering number of the ball B={w:∥w∥ ≤1}w.r.t. the norm ∥w∥2
K=wTKw 767
satisfies logN(B,∥·∥K, ϵ) =Mϵ(1 +o(1)) for the same Mϵas in Theorem 16 and under the same 768
condition. 769
Furthermore, logN(B,∥·∥K, ϵ)≤TrK
2ϵ2(1 +o(1)) as long as logd=o√
TrK
ϵ
log√
TrK
ϵ−1
. 770
Proof. If˜Eis an ϵ-covering of Ew.r.t. to the L2-norm, then ˜B=K−1
2˜Eis an ϵ-covering of B 771
w.r.t. the norm ∥·∥K, because if w∈B, then√
Kw∈Eand so there is an ˜x∈˜Esuch that 772x−√
Kw≤ϵ, but then ˜w=√
K−1xcovers wsince∥˜w−w∥K=x−√
Kw
K≤ϵ. 773
5Since the domain is always bounded here, one can let the bias translate the domain to the first quadrant and
let the weight be the identity matrix, cf. the construction in [45, Proposition B.1.3].
27Since λi≤TrK
i, we have K≤¯Kfor¯Kthe matrix obtained by replacing the i-th eigenvalue λiof 774
KbyTrK
i, and therefore N(B,∥·∥K, ϵ)≤ N (B,∥·∥¯K, ϵ)since∥·∥K≤ ∥·∥ ¯K. We now have the 775
a[proximation logN(B,∥·∥¯K, ϵ) =¯Mϵ(1 +o(1)) for 776
¯Mϵ=¯kϵX
i=1log√
TrK√
iϵ
¯kϵ=TrK
ϵ2
.
We now have the simplification 777
¯Mϵ=kϵX
i=1log√
TrK√
iϵ=1
2¯kϵX
i=1log¯kϵ
i=¯kϵ
2(Z1
0log1
xdx+o(1)) =¯kϵ
2(1 +o(1))
where the o(1)term vanishes as ϵ↘0. Furthermore, this allows us to check that as long as 778
logd=o√
TrK
4ϵlog√
TrK
ϵ
, the condition is satisfied 779
log√
TrK
ϵ=o¯kϵ
4 logd
=o¯M2
ϵ
¯kϵlogd
.
780
Second we prove how to obtain the covering number of the convex hull of a function set F: 781
Theorem 18. LetFbe a set of B-uniformly bounded functions, then for all ϵK=B2−K782
p
logN2(Conv F,2ϵK)≤√
18KX
k=12K−kq
logN2(F, B2−k).
Proof. Define ϵk=B2−kand the corresponding ϵk-coverings ˜Fk(w.r.t. some measure π). For any 783
f, we write ˜fk[f]for the function ˜fk[f]∈˜Fkthat covers f. Then for any functions finConvF, we 784
have 785
f=mX
i=1βifi=mX
i=1βi
fi−˜fK[fi]
+KX
k=1mX
i=1βi
˜fk[fi]−˜fk−1[fi]
+˜f0[fi].
We may assume that ˜f0[fi] = 0 since the zero function ϵ0-covers the whole Fsince ϵ0=B. 786
We will now use the probabilistic method to show that the sumsPm
i=1βi
˜fk[fi]−˜fk−1[fi]
787
can be approximated by finite averages. Consider the random functions ˜g(k)
1, . . . , ˜g(k)
mk788
sampled iid with Ph
˜g(k)
ji
=
˜fk[fi]−˜fk−1[fi]
with probability βi. We have E[˜g(k)
j] = 789
Pm
i=1βi
˜fk[fi]−˜fk−1[fi]
and 790
EKX
k=11
mkmkX
j=1˜g(k)
j−KX
k=1mX
i=1βi
˜fk[fi]−˜fk−1[fi]p
Lp(π)≤KX
k=11
mp
kmkX
j=1E˜g(k)
jp
Lp(π)
=KX
k=11
mkmX
i=1βi˜fk[fi]−˜fk−1[fi]p
Lp(π)
≤KX
k=132ϵ2
k
mk.
28Thus if we take mk=1
ak(3ϵk
ϵK)2withPak= 1we know that there must exist a choice of ˜g(k)
js such 791
that 792 KX
k=11
mkmkX
j=1˜g(k)
j−KX
k=1mX
i=1βi
˜fk[fi]−˜fk−1[fi]
Lp(π)≤ϵK.
This implies that finite the set ˜C=nPK
k=11
mkPmk
j=1˜g(k)
j: ˜g(k)
j∈˜Fk−˜Fk−1o
is an2ϵKcovering 793
ofC= Conv F, since we know that for all f=Pm
i=1βifithere are ˜g(k)
jsuch that 794
KX
k=11
mkmkX
j=1˜g(k)
j−mX
i=1βifi
Lp(π)≤mX
i=1βi
fi−˜fK[fi]
Lp(π)
+KX
k=11
mkmkX
j=1˜g(k)
j−mX
i=1βi
˜fk[fi]−˜fk−1[fi]
Lp(π)
≤2ϵK.
Since˜C=QK
k=1˜Fkmk˜Fk−1mk, we have 795
logNp(C,2ϵK)≤KX
k=11
ak(3ϵk
ϵK)2(logNp(F, ϵk) + log Np(F, ϵk−1))
≤18KX
k=11
ak22(K−k)logN2(F, ϵk).
This is minimized for the choice 796
ak=2(K−k)p
logN2(F, ϵk)P2(K−k)p
logN2(F, ϵk),
which yields the bound 797
q
logNp(C,2ϵK)≤√
18KX
k=12K−kp
logN2(F, ϵk)
798
NeurIPS Paper Checklist 799
1.Claims 800
Question: Do the main claims made in the abstract and introduction accurately reflect the 801
paper’s contributions and scope? 802
Answer: [Yes] 803
Justification: The contribution section accurately describes our contributions, and all 804
theorems are proven in the appendix. 805
Guidelines: 806
•The answer NA means that the abstract and introduction do not include the claims 807
made in the paper. 808
•The abstract and/or introduction should clearly state the claims made, including the 809
contributions made in the paper and important assumptions and limitations. A No or 810
NA answer to this question will not be perceived well by the reviewers. 811
•The claims made should match theoretical and experimental results, and reflect how 812
much the results can be expected to generalize to other settings. 813
29•It is fine to include aspirational goals as motivation as long as it is clear that these goals 814
are not attained by the paper. 815
2.Limitations 816
Question: Does the paper discuss the limitations of the work performed by the authors? 817
Answer: [Yes] 818
Justification: We discuss limitations of our Theorems after we state them. 819
Guidelines: 820
•The answer NA means that the paper has no limitation while the answer No means that 821
the paper has limitations, but those are not discussed in the paper. 822
• The authors are encouraged to create a separate "Limitations" section in their paper. 823
•The paper should point out any strong assumptions and how robust the results are to 824
violations of these assumptions (e.g., independence assumptions, noiseless settings, 825
model well-specification, asymptotic approximations only holding locally). The authors 826
should reflect on how these assumptions might be violated in practice and what the 827
implications would be. 828
•The authors should reflect on the scope of the claims made, e.g., if the approach was 829
only tested on a few datasets or with a few runs. In general, empirical results often 830
depend on implicit assumptions, which should be articulated. 831
•The authors should reflect on the factors that influence the performance of the approach. 832
For example, a facial recognition algorithm may perform poorly when image resolution 833
is low or images are taken in low lighting. Or a speech-to-text system might not be 834
used reliably to provide closed captions for online lectures because it fails to handle 835
technical jargon. 836
•The authors should discuss the computational efficiency of the proposed algorithms 837
and how they scale with dataset size. 838
•If applicable, the authors should discuss possible limitations of their approach to 839
address problems of privacy and fairness. 840
•While the authors might fear that complete honesty about limitations might be used 841
by reviewers as grounds for rejection, a worse outcome might be that reviewers 842
discover limitations that aren’t acknowledged in the paper. The authors should use 843
their best judgment and recognize that individual actions in favor of transparency play 844
an important role in developing norms that preserve the integrity of the community. 845
Reviewers will be specifically instructed to not penalize honesty concerning limitations. 846
3.Theory Assumptions and Proofs 847
Question: For each theoretical result, does the paper provide the full set of assumptions and 848
a complete (and correct) proof? 849
Answer: [Yes] 850
Justification: All assumptions are either stated in the Theorem statements, except for a few 851
recurring assumptions that are stated in the setup section. 852
Guidelines: 853
• The answer NA means that the paper does not include theoretical results. 854
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 855
referenced. 856
•All assumptions should be clearly stated or referenced in the statement of any theorems. 857
•The proofs can either appear in the main paper or the supplemental material, but if 858
they appear in the supplemental material, the authors are encouraged to provide a short 859
proof sketch to provide intuition. 860
•Inversely, any informal proof provided in the core of the paper should be complemented 861
by formal proofs provided in appendix or supplemental material. 862
• Theorems and Lemmas that the proof relies upon should be properly referenced. 863
4.Experimental Result Reproducibility 864
30Question: Does the paper fully disclose all the information needed to reproduce the 865
main experimental results of the paper to the extent that it affects the main claims and/or 866
conclusions of the paper (regardless of whether the code and data are provided or not)? 867
Answer: [Yes] 868
Justification: The experimental setup is described in the Appendix. 869
Guidelines: 870
• The answer NA means that the paper does not include experiments. 871
•If the paper includes experiments, a No answer to this question will not be perceived 872
well by the reviewers: Making the paper reproducible is important, regardless of 873
whether the code and data are provided or not. 874
•If the contribution is a dataset and/or model, the authors should describe the steps taken 875
to make their results reproducible or verifiable. 876
•Depending on the contribution, reproducibility can be accomplished in various ways. 877
For example, if the contribution is a novel architecture, describing the architecture fully 878
might suffice, or if the contribution is a specific model and empirical evaluation, it may 879
be necessary to either make it possible for others to replicate the model with the same 880
dataset, or provide access to the model. In general. releasing code and data is often 881
one good way to accomplish this, but reproducibility can also be provided via detailed 882
instructions for how to replicate the results, access to a hosted model (e.g., in the case 883
of a large language model), releasing of a model checkpoint, or other means that are 884
appropriate to the research performed. 885
•While NeurIPS does not require releasing code, the conference does require all 886
submissions to provide some reasonable avenue for reproducibility, which may depend 887
on the nature of the contribution. For example 888
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 889
to reproduce that algorithm. 890
(b)If the contribution is primarily a new model architecture, the paper should describe 891
the architecture clearly and fully. 892
(c)If the contribution is a new model (e.g., a large language model), then there should 893
either be a way to access this model for reproducing the results or a way to reproduce 894
the model (e.g., with an open-source dataset or instructions for how to construct 895
the dataset). 896
(d)We recognize that reproducibility may be tricky in some cases, in which case 897
authors are welcome to describe the particular way they provide for reproducibility. 898
In the case of closed-source models, it may be that access to the model is limited in 899
some way (e.g., to registered users), but it should be possible for other researchers 900
to have some path to reproducing or verifying the results. 901
5.Open access to data and code 902
Question: Does the paper provide open access to the data and code, with sufficient 903
instructions to faithfully reproduce the main experimental results, as described in 904
supplemental material? 905
Answer: [Yes] 906
Justification: We use openly available data or synthetic data, with a description of how to 907
build this synthetic data. 908
Guidelines: 909
• The answer NA means that paper does not include experiments requiring code. 910
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 911
public/guides/CodeSubmissionPolicy ) for more details. 912
•While we encourage the release of code and data, we understand that this might not be 913
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 914
including code, unless this is central to the contribution (e.g., for a new open-source 915
benchmark). 916
•The instructions should contain the exact command and environment needed to run to 917
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 918
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 919
31•The authors should provide instructions on data access and preparation, including how 920
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 921
•The authors should provide scripts to reproduce all experimental results for the new 922
proposed method and baselines. If only a subset of experiments are reproducible, they 923
should state which ones are omitted from the script and why. 924
•At submission time, to preserve anonymity, the authors should release anonymized 925
versions (if applicable). 926
•Providing as much information as possible in supplemental material (appended to the 927
paper) is recommended, but including URLs to data and code is permitted. 928
6.Experimental Setting/Details 929
Question: Does the paper specify all the training and test details (e.g., data splits, 930
hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand 931
the results? 932
Answer: [Yes] 933
Justification: In the experimental setup section in the Appendix. 934
Guidelines: 935
• The answer NA means that the paper does not include experiments. 936
•The experimental setting should be presented in the core of the paper to a level of detail 937
that is necessary to appreciate the results and make sense of them. 938
•The full details can be provided either with the code, in appendix, or as supplemental 939
material. 940
7.Experiment Statistical Significance 941
Question: Does the paper report error bars suitably and correctly defined or other appropriate 942
information about the statistical significance of the experiments? 943
Answer: [No] 944
Justification: The numerical experiments are mostly there as a visualization of the theoretical 945
results, our main goal is therefore clarity, which would be hurt by putting error bars 946
everywhere. 947
Guidelines: 948
• The answer NA means that the paper does not include experiments. 949
•The authors should answer "Yes" if the results are accompanied by error bars, 950
confidence intervals, or statistical significance tests, at least for the experiments that 951
support the main claims of the paper. 952
•The factors of variability that the error bars are capturing should be clearly stated (for 953
example, train/test split, initialization, random drawing of some parameter, or overall 954
run with given experimental conditions). 955
•The method for calculating the error bars should be explained (closed form formula, 956
call to a library function, bootstrap, etc.) 957
• The assumptions made should be given (e.g., Normally distributed errors). 958
•It should be clear whether the error bar is the standard deviation or the standard error 959
of the mean. 960
•It is OK to report 1-sigma error bars, but one should state it. The authors should 961
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 962
of Normality of errors is not verified. 963
•For asymmetric distributions, the authors should be careful not to show in tables or 964
figures symmetric error bars that would yield results that are out of range (e.g. negative 965
error rates). 966
•If error bars are reported in tables or plots, The authors should explain in the text how 967
they were calculated and reference the corresponding figures or tables in the text. 968
8.Experiments Compute Resources 969
Question: For each experiment, does the paper provide sufficient information on the 970
computer resources (type of compute workers, memory, time of execution) needed to 971
reproduce the experiments? 972
32Answer: [Yes] 973
Justification: In the experimental setup section of the Appendix. 974
Guidelines: 975
• The answer NA means that the paper does not include experiments. 976
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 977
or cloud provider, including relevant memory and storage. 978
•The paper should provide the amount of compute required for each of the individual 979
experimental runs as well as estimate the total compute. 980
•The paper should disclose whether the full research project required more compute 981
than the experiments reported in the paper (e.g., preliminary or failed experiments that 982
didn’t make it into the paper). 983
9.Code Of Ethics 984
Question: Does the research conducted in the paper conform, in every respect, with the 985
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 986
Answer: [Yes] 987
Justification: We have read the Code of Ethics and see no issue. 988
Guidelines: 989
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 990
•If the authors answer No, they should explain the special circumstances that require a 991
deviation from the Code of Ethics. 992
•The authors should make sure to preserve anonymity (e.g., if there is a special 993
consideration due to laws or regulations in their jurisdiction). 994
10.Broader Impacts 995
Question: Does the paper discuss both potential positive societal impacts and negative 996
societal impacts of the work performed? 997
Answer: [NA] 998
Justification: The paper is theoretical in nature, so it has no direct societal impact that can 999
be meaningfully discussed. 1000
Guidelines: 1001
• The answer NA means that there is no societal impact of the work performed. 1002
•If the authors answer NA or No, they should explain why their work has no societal 1003
impact or why the paper does not address societal impact. 1004
•Examples of negative societal impacts include potential malicious or unintended uses 1005
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 1006
(e.g., deployment of technologies that could make decisions that unfairly impact specific 1007
groups), privacy considerations, and security considerations. 1008
•The conference expects that many papers will be foundational research and not tied 1009
to particular applications, let alone deployments. However, if there is a direct path to 1010
any negative applications, the authors should point it out. For example, it is legitimate 1011
to point out that an improvement in the quality of generative models could be used to 1012
generate deepfakes for disinformation. On the other hand, it is not needed to point out 1013
that a generic algorithm for optimizing neural networks could enable people to train 1014
models that generate Deepfakes faster. 1015
•The authors should consider possible harms that could arise when the technology is 1016
being used as intended and functioning correctly, harms that could arise when the 1017
technology is being used as intended but gives incorrect results, and harms following 1018
from (intentional or unintentional) misuse of the technology. 1019
•If there are negative societal impacts, the authors could also discuss possible mitigation 1020
strategies (e.g., gated release of models, providing defenses in addition to attacks, 1021
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 1022
feedback over time, improving the efficiency and accessibility of ML). 1023
11.Safeguards 1024
33Question: Does the paper describe safeguards that have been put in place for responsible 1025
release of data or models that have a high risk for misuse (e.g., pretrained language models, 1026
image generators, or scraped datasets)? 1027
Answer: [NA] 1028
Justification: Not relevant to our paper. 1029
Guidelines: 1030
• The answer NA means that the paper poses no such risks. 1031
•Released models that have a high risk for misuse or dual-use should be released with 1032
necessary safeguards to allow for controlled use of the model, for example by requiring 1033
that users adhere to usage guidelines or restrictions to access the model or implementing 1034
safety filters. 1035
•Datasets that have been scraped from the Internet could pose safety risks. The authors 1036
should describe how they avoided releasing unsafe images. 1037
•We recognize that providing effective safeguards is challenging, and many papers do 1038
not require this, but we encourage authors to take this into account and make a best 1039
faith effort. 1040
12.Licenses for existing assets 1041
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 1042
the paper, properly credited and are the license and terms of use explicitly mentioned and 1043
properly respected? 1044
Answer: [Yes] 1045
Justification: In the experimental setup section of the Appendix. 1046
Guidelines: 1047
• The answer NA means that the paper does not use existing assets. 1048
• The authors should cite the original paper that produced the code package or dataset. 1049
•The authors should state which version of the asset is used and, if possible, include a 1050
URL. 1051
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 1052
•For scraped data from a particular source (e.g., website), the copyright and terms of 1053
service of that source should be provided. 1054
•If assets are released, the license, copyright information, and terms of use in the 1055
package should be provided. For popular datasets, paperswithcode.com/datasets 1056
has curated licenses for some datasets. Their licensing guide can help determine the 1057
license of a dataset. 1058
•For existing datasets that are re-packaged, both the original license and the license of 1059
the derived asset (if it has changed) should be provided. 1060
•If this information is not available online, the authors are encouraged to reach out to 1061
the asset’s creators. 1062
13.New Assets 1063
Question: Are new assets introduced in the paper well documented and is the documentation 1064
provided alongside the assets? 1065
Answer: [NA] 1066
Justification: We do not release any new assets. 1067
Guidelines: 1068
• The answer NA means that the paper does not release new assets. 1069
•Researchers should communicate the details of the dataset/code/model as part of their 1070
submissions via structured templates. This includes details about training, license, 1071
limitations, etc. 1072
•The paper should discuss whether and how consent was obtained from people whose 1073
asset is used. 1074
•At submission time, remember to anonymize your assets (if applicable). You can either 1075
create an anonymized URL or include an anonymized zip file. 1076
3414.Crowdsourcing and Research with Human Subjects 1077
Question: For crowdsourcing experiments and research with human subjects, does the paper 1078
include the full text of instructions given to participants and screenshots, if applicable, as 1079
well as details about compensation (if any)? 1080
Answer: [NA] 1081
Justification: Not relevant to this paper. 1082
Guidelines: 1083
•The answer NA means that the paper does not involve crowdsourcing nor research with 1084
human subjects. 1085
•Including this information in the supplemental material is fine, but if the main 1086
contribution of the paper involves human subjects, then as much detail as possible 1087
should be included in the main paper. 1088
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 1089
or other labor should be paid at least the minimum wage in the country of the data 1090
collector. 1091
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 1092
Subjects 1093
Question: Does the paper describe potential risks incurred by study participants, whether 1094
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 1095
approvals (or an equivalent approval/review based on the requirements of your country or 1096
institution) were obtained? 1097
Answer: [NA] 1098
Justification: Not relevant to this paper. 1099
Guidelines: 1100
•The answer NA means that the paper does not involve crowdsourcing nor research with 1101
human subjects. 1102
•Depending on the country in which research is conducted, IRB approval (or equivalent) 1103
may be required for any human subjects research. If you obtained IRB approval, you 1104
should clearly state this in the paper. 1105
•We recognize that the procedures for this may vary significantly between institutions 1106
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 1107
guidelines for their institution. 1108
•For initial submissions, do not include any information that would break anonymity (if 1109
applicable), such as the institution conducting the review. 1110
35