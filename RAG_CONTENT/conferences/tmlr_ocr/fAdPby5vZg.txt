Under review as submission to TMLR
Relationship between Batch Size and Number of Steps
Needed for Nonconvex Optimization of Stochastic Gradient
Descent using Armijo Line Search
Anonymous authors
Paper under double-blind review
Abstract
Stochastic gradient descent (SGD) is the simplest deep learning optimizer with which to
train deep neural networks. While SGD can use various learning rates, such as constant
or diminishing rates, the previous numerical results showed that SGD performs better than
other deep learning optimizers using when it uses learning rates given by line search methods.
In this paper, we perform a convergence analysis on SGD with a learning rate given by
an Armijo line search for nonconvex optimization. The analysis indicates that the upper
bound of the expectation of the squared norm of the full gradient becomes small when
the number of steps and the batch size are large. Next, we show that, for SGD with the
Armijo-line-search learning rate, the number of steps needed for nonconvex optimization
is a monotone decreasing convex function of the batch size; that is, the number of steps
needed for nonconvex optimization decreases as the batch size increases. Furthermore, we
show that the stochastic ﬁrst-order oracle (SFO) complexity, which is the stochastic gradient
computation cost, is a convex function of the batch size; that is, there exists a critical batch
size that minimizes the SFO complexity. Finally, we provide numerical results that support
our theoretical results. The numerical results indicate that the number of steps needed for
training deep neural networks decreases as the batch size increases and that there exist the
critical batch sizes that can be estimated from the theoretical results.
1 Introduction
1.1 Background
Nonconvex optimization is useful for training deep neural networks, since the loss functions called the ex-
pected risk and empirical risk are nonconvex and they need only be minimized in order to ﬁnd the model
parameters. Deep-learning optimizers have been presented for minimizing the loss functions. The simplest
one is stochastic gradient descent (SGD) ( Robbins & Monro ,1951;Zinkevich ,2003;Nemirovski et al. ,2009;
Ghadimi & Lan ,2012;2013) and there are numerous theoretical analyses on using SGD for nonconvex opti-
mization ( Jain et al. ,2018;Vaswani et al. ,2019;Fehrman et al. ,2020;Chen et al. ,2020;Scaman & Malherbe ,
2020;Loizou et al. ,2021). Variants have also been presented, such as momentum methods ( Polyak ,1964;
Nesterov ,1983) and adaptive methods including Adaptive Gradient (AdaGrad) ( Duchi et al. ,2011), Root
Mean Square Propagation (RMSProp) ( Tieleman & Hinton ,2012), Adaptive Moment Estimation (Adam)
(Kingma & Ba ,2015), Adaptive Mean Square Gradient (AMSGrad) ( Reddi et al. ,2018), and Adam with
decoupled weight decay (AdamW) ( Loshchilov & Hutter ,2019). SGD and its variants are useful for training
not only deep neural networks but also generative adversarial networks ( Heusel et al. ,2017;Naganuma &
Iiduka ,2023;Sato & Iiduka ,2023).
The performance of deep-learning optimizers for nonconvex optimization depends on the batch size. The
previous numerical results in ( Shallue et al. ,2019) and ( Zhang et al. ,2019) have shown that the number of
steps Kneeded to train a deep neural network halves for each doubling of the batch size band that there is
a region of diminishing returns beyond the critical batch size b⋆. This fact can be expressed as follows: there
1Under review as submission to TMLR
is a positive number Csuch that N:=Kb≈Cforb≤b⋆andN:=Kb≥Cforb≥b⋆. The deep neural
network model uses bgradients of the loss functions per step. Hence, when Kis the number of steps required
to train a deep neural network, the model has a stochastic gradient computation cost of Kb. We will deﬁne
thestochastic ﬁrst-order oracle (SFO) complexity (Iiduka ,2022;Sato & Iiduka ,2023) of a deep-learning
optimizer to be N:=Kb. From the previous numerical results in ( Shallue et al. ,2019) and ( Zhang et al. ,
2019), the SFO complexity is minimized at a critical batch size b⋆and there are diminishing returns once
the batch size exceeds b⋆. Therefore, it is desirable to use the critical batch size when minimizing the SFO
complexity of the deep-learning optimizer.
Not only a batch size but also a learning rate aﬀects the performance of deep-learning optimizers for noncon-
vex optimization. A performance measure of a deep-learning optimizer generating a sequence (θk)k∈Nis the
expectation of the squared norm of the gradient of a nonconvex loss function f, denoted by E[∥∇f(θk)∥2].
If this performance measure becomes small when the number of steps kis large, the deep-learning optimizer
approximates a local minimizer of f. For example, let us consider the problem of minimizing a smooth
function f(see Section 2.1for the deﬁnition of smoothness). Here, SGD using a constant learning rate
α=O(1
L)satisﬁes min k∈[K]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
=O(1
K+α
b), where Lis the Lipschitz constant of ∇f,bis the
batch size, and [K] :={1,2, . . . , K}(see also Table 1). Moreover, SGD using a learning rate satisfying the
Armijo condition was presented in ( Vaswani et al. ,2019). The Armijo line search (Nocedal & Wright ,2006,
Chapter 3.1) is a standard method for ﬁnding an appropriate learning rate αkgiving a suﬃcient decrease in
f, i.e., f(θk+1)< f(θk)(see Section 2.3.1 for the deﬁnition of the Armijo condition).
1.2 Motivation
The numerical results in ( Vaswani et al. ,2019) indicated that the Armijo-line-search learning rate is superior
to using a constant learning rate when using SGD to train deep neural networks in the sense of minimizing the
training loss functions and improving test accuracy. Motivated by the useful numerical results in ( Vaswani
et al. ,2019), we decided to perform convergence analyses on SGD with the Armijo-line-search learning rate
for nonconvex optimization in deep neural networks.
Theorem 3 in ( Vaswani et al. ,2019) is a convergence analysis of SGD with the Armijo-line-search learning
rate for nonconvex optimization under a strong growth condition that implies the interpolation property.
Here, let f:Rd→Rbe an empirical risk deﬁned by f(θ) :=1
n/summationtext
i∈[n]fi(θ), where nis the number of
training data and fi:Rd→Ris a loss function corresponding to the i-th training data zi. We say that fhas
the interpolation property if ∇f(θ) =0implies∇fi(θ) =0(i∈[n]). The interpolation property holds for
optimization of a linear model with the squared hinge loss for binary classiﬁcation on linearly separable data
(Vaswani et al. ,2019, Section 2). However, the interpolation condition would be unrealistic for deep neural
networks, since their loss functions are nonconvex. The motivation behind this work is thus to show that
SGD with the Armijo-line-search learning rate can solve nonconvex optimization problems in deep neural
networks.
As indicated the second paragraph in Section 1.1, the batch size has a signiﬁcant eﬀect on the performance
of SGD. Hence, in accordance with the ﬁrst motivation stated above, we decided to investigate appropriate
batch sizes for SGD with the Armijo-line-search learning rate. In particular, we were interested in verifying
whether a critical batch size b⋆minimizing the SFO complexity Nexists for training deep neural networks
with SGD using the Armijo condition in theory and in practice. This is because the previous studies in
(Shallue et al. ,2019;Zhang et al. ,2019;Iiduka ,2022;Sato & Iiduka ,2023) showed the existence of critical
batch sizes for training deep neural networks or generative adversarial networks with optimizers with constant
or diminishing learning rates and without Armijo-line-search learning rates.
We are also interested in estimating critical batch sizes before implementing SGD with the Armijo-line-
search learning rate. The previous results in ( Iiduka ,2022;Sato & Iiduka ,2023) showed that, for optimizers
using constant learning rates, the critical batch sizes determined from numerical results are close to the
theoretically estimated sizes. Motivated by the results in ( Iiduka ,2022;Sato & Iiduka ,2023), we sought to
verify whether, for SGD with the Armijo-line-search learning rate, the measured critical batch sizes are close
to the batch sizes estimated from theoretical results.
2Under review as submission to TMLR
1.3 Contribution
1.3.1 Convergence analysis of SGD with Armijo-line-search learning rates
The ﬁrst contribution of this paper is to present a convergence analysis of SGD with Armijo-line-search
learning rates for general nonconvex optimization (Theorem 3.1); in particular, it is shown that SGD with
this rate αksatisﬁes that, for all K≥1,
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤C1/bracehtipdownleft /bracehtipupright/bracehtipupleft /bracehtipdownright
2(f(θ0)−f∗)
(2−Lnα)α1
K/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B(θ0,K)+C2/bracehtipdownleft/bracehtipupright/bracehtipupleft /bracehtipdownright
ασ2
(2−Lnα)α1
b/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
V(σ2,b), (1)
where the parameters are deﬁned in Table 1(see also Theorem 3.1). The inequality ( 1) indicates that the
upper bound of the performance measure min k∈[0:K−1]E[∥∇f(θk)∥2]that consists of a bias term B(θ0, K)
and variance term V(σ2, b)becomes small when the number of steps Kis large and the batch size bis large.
Therefore, it is desirable to set Klarge and blarge so that Algorithm 1will approximate a local minimizer
off.
The essential lemma to proving ( 1) is the guarantee of the existence of a lower bound on the learning rates
satisfying the Armijo condition (Lemma 2.1). Although, in general, learning rates satisfying the Armijo
condition do not have any lower bound (Lemma 2.1(i)), the corresponding learning rates computed by a
backtracking line search (Algorithm 2) have a lower bound (Lemma 2.1(ii)). In addition, the descent lemma
(i.e., f(y)≤f(x) +⟨∇f(x),y−x⟩+Ln
2∥y−x∥2(x,y∈Rd)) holds from the smoothness condition on f.
Thus, we can prove ( 1) by using the existence of a lower bound on the learning rates satisfying the Armijo
condition and the descent lemma (see Appendix A.2for details of the proof of Theorem 3.1).
Table 1: Relationship between batch size band number of steps Kto achieve an ϵ–approximation deﬁned
bymin k∈[0:K−1]E[∥∇f(θk)∥2]≤C1
K+C2
b=ϵ2for SGD with a constant learning rate α∈(0,2
Ln)and for
SGD with the Armijo-line-search learning rate αk∈[α,α]([0 :K−1] :={0,1, . . . , K−1},f:=1
n/summationtext
i∈[n]fi
is bounded below by f∗,Liis the Lipschitz constant of ∇fi,Ln:=1
n/summationtext
i∈[n]Li, and σ2is the upper bound
of the variance of the stochastic gradient)
Learning Rate Upper BoundC1
K+C2
bSteps K SFO N Critical Batch b⋆
Constant α∈/parenleftbigg
0,2
Ln/parenrightbiggC12(f(θ0)−f∗)
(2−Lnα)α K=C1b
ϵ2b−C2N=C1b2
ϵ2b−C2b⋆=2C2
ϵ2
C2Lnασ2
2−Lnα
Armijo (c, δ∈(0,1))C12(f(θ0)−f∗)
(2−Lnα)α K=C1b
ϵ2b−C2N=C1b2
ϵ2b−C2b⋆=2C2
ϵ2
C2ασ2
(2−Lnα)α
To show the merit of SGD with the Armijo-line-search learning rate, we compare an implementation using
this rate with one using a constant learning rate (see, e.g., ( Scaman & Malherbe ,2020, Section 4) for
convergence analyses of SGD with constant learning rates). In this case, we need to set a constant learning
rateα∈(0,2
Ln)depending on the Lipschitz constant Lnof∇f(see also Table 1). However, computing Ln
is NP-hard ( Virmaux & Scaman ,2018), so it would be unrealistic to set a constant learning rate depending
onLnbefore implementing SGD. Meanwhile, we need to set c, δ∈(0,1)in order to use SGD with the
Armijo-line-search learning rate. We would like to emphasize here that we can choose anyc, δ∈(0,1)to
implement SGD. That is, for any c, δ∈(0,1), there exists a learning rate satisfying the Armijo condition
(see ( 11) for the deﬁnition of the Armijo condition) and it can be found by conducting a simple backtracking
line search (Algorithm 2) instead of performing a complicated computation such as of the Lipschitz constant
of∇f(Lemma 2.1).
3Under review as submission to TMLR
1.3.2 Steps needed for ϵ–approximation of SGD with Armijo line-search-learning rates
The previous results in ( Shallue et al. ,2019;Zhang et al. ,2019;Iiduka ,2022;Sato & Iiduka ,2023) indicated
that, for optimizers, the number of steps Kneeded to train a deep neural network or generative adversarial
networks decreases as the batch size increases. The second contribution of this paper is to show that, for
SGD with the Armijo-line-search learning rate, the number of steps Kneeded for nonconvex optimization
decreases as the batch size increases. Let us consider the case in which the right-hand side of ( 1) is equal to
ϵ2, where ϵ >0is the precision. Then, Kis a rational function deﬁned for a batch size bby
K=K(b) =C1b
ϵ2b−C2, (2)
where C1andC2are the positive constants deﬁned in ( 1) (see also Table 1). We can easily show that K
deﬁned above is a monotone decreasing and convex function with respect to b(Theorem 3.2). Accordingly,
the number of steps needed for nonconvex optimization decreases as the batch size increases.
1.3.3 Critical batch size minimizing SFO complexity of SGD with Armijo-line-search learning rates
Using Kdeﬁned by ( 2) above, we can further deﬁne the SFO complexity Nof SGD with Armijo-line-search
learning rates (see also Table 1):
N=Kb=K(b)b=C1b2
ϵ2b−C2. (3)
We can easily show that Nis convex with respect to band that a global minimizer
b⋆=2C2
ϵ2=2ασ2
(2−Lnα)αϵ2(4)
exists for it (Theorem 3.3). Accordingly, there is a critical batch size b⋆at which Nis minimized.
Here, we compare the number of steps KCand the SFO complexity NCfor SGD using a constant learning
rateαwith KAandNAfor SGD using the Armijo-line-search learning rate αk(∈[α,α]). Let C1,C(resp.
C2,C) be C1(resp. C2) in Table 1for SGD using a constant learning rate and let C1,A(resp. C2,A) be C1
(resp. C2) in Table 1for SGD using the Armijo-line-search learning rate. We have that
C1,A< C 1,Ciﬀα>2−Lnα
2−Lnαα,
C2,A< C 2,Ciﬀα
α<σ2
C(2−Lnα)
σ2
A(2−Lnα)Lnα,(5)
where σ2
C(resp. σ2
A) denotes the upper bound of the variance of the stochastic gradient for SGD using
a constant learning rate α(resp. the Armijo-line-search learning rate). If ( 5) holds, then SGD using the
Armijo-line-search learning rate converges faster than SGD using a constant learning rate in the sense that
C1,Ab
ϵ2b−C2,A=KA< K C=C1,Cb
ϵ2b−C2,CandC1,Ab2
ϵ2b−C2,A=NA< N C=C1,Cb2
ϵ2b−C2,C.
It would be diﬃcult to check exactly that ( 5) holds before implementing SGD, since ( 5) involves unknown
parameters, such as Ln=1
n/summationtext
i∈[n]Li,σ2
C, and σ2
A. However, it can be expected that ( 5) holds, since it
is known empirically ( Vaswani et al. ,2019, Figure 5) that the relationship between the Armijo-line-search
learning rate αkand a constant learning rate αisα < α k∈[α,α](Section 3.3provides the derivation of
condition ( 5)).
1.3.4 Numerical results supporting our theoretical results
The numerical results in ( Vaswani et al. ,2019) showed that SGD with the Armijo-line-search learning rate
performs better than other optimizers in training deep neural networks. Hence, we sought to verify whether
4Under review as submission to TMLR
the numerical results match our theoretical results (Sections 1.3.1,1.3.2, and 1.3.3). We trained residual
networks (ResNets) on the CIFAR-10 and CIFAR-100 datasets and a two-hidden-layer multi-layer perceptron
(MLP) on the MNIST dataset. We numerically found that increasing the batch size bdecreases the number
of steps Kneeded to achieve high training accuracies and that there are critical batch sizes minimizing the
SFO complexities. We also estimated batch sizes using ( 4) for the critical batch size b⋆and compared them
with ones determined from the numerical results. We found that the estimated batch sizes are close to the
ones determined from the numerical results. To verify whether SGD using the Armijo-line-search learning
rate performs better than SGD using a constant learning rate (see the discussion in condition ( 5)), we
numerically compared SGD using the Armijo-line-search learning rate with not only SGD using a constant
learning rate but also variants of SGD, such as the momentum method, Adam, AdamW, and RMSProp. We
found that SGD using the Armijo-line-search learning rate and the critical batch size performs better than
other optimizers in the sense of minimizing the number of steps and the SFO complexities needed to achieve
high training accuracies (Section 4).
2 Mathematical Preliminaries
2.1 Deﬁnitions
LetNbe the set of nonnegative integers, [n] :={1,2, . . . , n}forn≥1, and [0 :n] :={0,1, . . . , n}forn≥0.
LetRdbe a d–dimensional Euclidean space with inner product ⟨·,·⟩inducing the norm ∥·∥.
Letf:Rd→Rbe continuously diﬀerentiable. We denote the gradient of fby∇f:Rd→Rd. Let L > 0.
f:Rd→Ris said to be L–smooth if∇f:Rd→RdisL–Lipschitz continuous, i.e., for all x,y∈Rd,
∥∇f(x)−∇f(y)∥≤L∥x−y∥. When f:Rd→RisL–smooth, the following inequality, called the descent
lemma ( Beck ,2017, Lemma 5.7), holds: for all x,y∈Rd,f(y)≤f(x) +⟨∇f(x),y−x⟩+L
2∥y−x∥2. Let
f∗∈R.f:Rd→Ris said to be bounded below by f∗if, for all x∈Rd,f(x)≥f∗.
2.2 Assumptions and problem
Given a parameter θ∈Rdand a data point zin a data domain Z, a machine learning model provides a
prediction whose quality can be measured by a diﬀerentiable nonconvex loss function f(θ;z). We aim to
minimize the empirical average loss deﬁned for all θ∈Rdby
f(θ) =1
n/summationdisplay
i∈[n]f(θ;zi) =1
n/summationdisplay
i∈[n]fi(θ),
where S= (z1, z2, . . . , z n)denotes the training set and fi(·) :=f(·;zi)denotes the loss function corresponding
to the i-th training data zi.
This paper considers the following smooth nonconvex optimization problem.
Problem 2.1 Suppose that fi:Rd→R(i∈[n])isLi–smooth and bounded below by fi,∗. Then,
minimize f(θ) :=1
n/summationdisplay
i∈[n]fi(θ)subject to θ∈Rd.
We assume that a stochastic ﬁrst-order oracle (SFO) exists such that, for a given θ∈Rd, it returns a
stochastic gradient Gξ(θ)of the function f, where a random variable ξis supported on a ﬁnite/an inﬁnite
setΞ(i.e., supp( ξ) ={x∈Ξ:ξ(x)̸= 0}) independently of θ. We make the following standard assumptions.
Assumption 2.1
(A1) Let(θk)k∈N⊂Rdbe the sequence generated by SGD. For each iteration k,
Eξk[Gξk(θk)] =∇f(θk), (6)
5Under review as submission to TMLR
where ξ0, ξ1, . . .are independent samples and the random variable ξkis independent of (θl)k
l=0. There
exists a nonnegative constant σ2such that
Eξk/bracketleftbig
∥Gξk(θk)−∇f(θk)∥2/bracketrightbig
≤σ2. (7)
(A2) For each iteration k, SGD samples a batch Bkof size bindependently of kand estimates the full
gradient∇fas
∇fBk(θk) :=1
b/summationdisplay
i∈[b]Gξk,i(θk) =1
b/summationdisplay
i∈[b]∇fξk,i(θk),
where ξk,iis a random variable generated by the i-th sampling in the k-th iteration.
2.3 Stochastic gradient descent using Armijo line search
2.3.1 Armijo condition
Suppose that f:Rd→Ris continuously diﬀerentiable. We would like to ﬁnd a stationary point θ⋆∈Rd
such that∇f(θ⋆) =0by using an iterative method deﬁned by
θk+1:=θk+αkdk, (8)
where αk>0is the step size (called a learning rate in themachine learning ﬁeld) and dk∈Rdis the search
direction. Various methods can be used depending on the search direction dk. For example, the method
(8) with dk:=−∇f(θk)is gradient descent, while the method ( 8) with dk:=−∇f(θk) +βk−1dk−1, where
βk≥0, is the conjugate gradient method. If we deﬁne dk(e.g., dk:=−∇f(θk)), it is desirable to set α⋆
k
satisfying
f(θk+α⋆
kdk) = min
α>0f(θk+αdk). (9)
The step size α⋆
kdeﬁned by ( 9) can be easily computed when fis quadratic and convex. However, for a
general nonconvex function f, it is diﬃcult to compute the step size α⋆
kin (9) exactly. Here, we can use the
Armijo condition for ﬁnding an appropriate step size αk: Let c∈(0,1). We would like to ﬁnd αk>0such
that
f(θk+αkdk)≤f(θk) +cαk⟨∇f(θk),dk⟩. (10)
When dksatisﬁes the descent property deﬁned by ⟨∇f(θk),dk⟩<0(e.g., gradient descent using dk:=
−∇f(θk)has the property such that ⟨∇f(θk),dk⟩=−∥∇f(θk)∥2<0), the Armijo condition ensures that
f(θk+1) =f(θk+αkdk)< f(θk). Accordingly, αksatisfying the Armijo condition ( 10) is appropriate in the
sense of minimizing f.
The existence of step sizes satisfying the Armijo condition ( 10) is guaranteed.
Proposition 2.1 (Nocedal & Wright ,2006, Lemma 3.1) Letf:Rd→Rbe continuously diﬀerentiable. Let
θk∈Rdand let dk(̸=0)have the descent property deﬁned by ⟨∇f(θk),dk⟩<0. Let c∈(0,1). Then, there
exists γk>0such that, for all αk∈(0, γk], the Armijo condition ( 10) holds.
2.3.2 Stochastic gradient descent under Armijo condition
The objective of this paper is to solve Problem 2.1using mini-batch SGD under Assumption 2.1deﬁned by
θk+1=θk+αkdk=θk−αk∇fBk(θk) =θk−αk
b/summationdisplay
i∈[b]Gξk,i(θk),
where b >0is the batch size and αk>0is the learning rate. For each iteration k, we can use θk,fBk, and
∇fBk. Hence, the Armijo condition ( Vaswani et al. ,2019, (1)) at the k-th iteration for SGD can be obtained
by replacing fin (10) with fBkand using dk=−∇fBk(θk):
fBk(θk−αk∇fBk(θk))≤fBk(θk)−cαk∥∇fBk(θk)∥2. (11)
6Under review as submission to TMLR
The Armijo condition ( 11) ensures that fBk(θk+1) =fBk(θk−αk∇fBk(θk))< f Bk(θk); i.e., the Armijo
condition ( 11) is appropriate in the sense of minimizing the estimated objective function fBkfrom the full
objective function f. In fact, the numerical results in ( Vaswani et al. ,2019, Section 7) indicate that SGD
using the Armijo condition ( 11) is superior to using other deep-learning optimizers to train deep neural
networks.
Algorithm 1is the SGD algorithm using the Armijo condition ( 11).
Algorithm 1 Stochastic gradient descent using Armijo line search
Require: c∈(0,1)(hyperparameter), b >0(batch size), θ0∈Rd(initial point), K≥1(steps)
Ensure: θK∈Rd
k←0
fork= 0,1, . . . , K−1do
Compute αk>0satisfying fBk(θk−αk∇fBk(θk))≤fBk(θk)−cαk∥∇fBk(θk)∥2◁Algorithm 2
Compute θk+1=θk−αk∇fBk(θk)
end for
The search direction of Algorithm 1isdk=−∇fBk(θk) (̸=0)which has the descent property deﬁned by
⟨∇fBk(θk),dk⟩=−∥∇fBk(θk)∥2<0. Hence, from Proposition 2.1, there exists a learning rate αk∈(0, γk]
satisfying the Armijo condition ( 11). Moreover, the proposition guarantees that the learning rate can be
chosen to be suﬃciently small, e.g., lim inf k→+∞αk= 0.
The convergence analyses of Algorithm 1use a lower bound of αk∈(0, γk]satisfying the Armijo condition
(11). To guarantee the existence of such a lower bound, we use the backtracking method (( Nocedal & Wright ,
2006, Algorithm 3.1) and ( Vaswani et al. ,2019, Algorithm 2)) described in Algorithm 2.
Algorithm 2 Backtracking Armijo-line-search method ( Nocedal & Wright ,2006, Algorithm 3.1)
Require: c, δ,1
γ∈(0,1)(hyperparameters), α=γb
nαk−1(initialization), θk∈Rd,fBk:Rd→R
Ensure: αksatisfying fBk(θk−αk∇fBk(θk))≤fBk(θk)−cαk∥∇fBk(θk)∥2
repeat
α←δα
until fBk(θk−α∇fBk(θk))≤fBk(θk)−cα∥∇fBk(θk)∥2
The following lemma guarantees the existence of a lower bound on the learning rates computed by Algorithm
2. The proof is given in Appendix A.1.
Lemma 2.1 Consider Algorithm 1under Assumption 2.1for solving Problem 2.1. Let αkbe a learning
rate satisfying the Armijo condition ( 11) (whose existence is guaranteed by Proposition 2.1), let LBkbe the
Lipschitz constant of ∇fBk, and let Lbe the maximum value of the Lipschitz constant Liof∇fi. Then, the
following hold.
(i)[Counter-example of ( Vaswani et al. ,2019, Lemma 1)] There exists Problem 2.1such that αkdoes
not satisfy min{2(1−c)
LBk,α}≤αk, where αis an upper bound of αk.
(ii)[Lower bound on learning rate determined by backtracking line search method] Ifαkcan be computed
by Algorithm 2, then there exists a lower bound of αksuch that 0< α :=2δ(1−c)
L≤2δ(1−c)
LBk≤αk.
3 Analysis of SGD using Armijo Line Search
3.1 Convergence analysis of Algorithm 1
Here, we present a convergence analysis of Algorithm 1. The proof of Theorem 3.1is given in Appendix A.2.
Theorem 3.1 (Upper bound of the squared norm of the full gradient) Consider the sequence
(θk)k∈Ngenerated by Algorithm 1under Assumption 2.1for solving Problem 2.1and suppose that the
7Under review as submission to TMLR
learning rate αk∈[α,α]is computed by Algorithm 2. Then, for all K≥1,
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤C1/bracehtipdownleft /bracehtipupright/bracehtipupleft /bracehtipdownright
2(f(θ0)−f∗)
(2−Lnα)α1
K/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B(θ0,K)+C2/bracehtipdownleft/bracehtipupright/bracehtipupleft /bracehtipdownright
ασ2
(2−Lnα)α1
b/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
V(σ2,b),
where δ, c∈(0,1),L:= max i∈[n]Li,Ln:=1
n/summationtext
i∈[n]Li(≤L),f∗:=1
n/summationtext
i∈[n]fi,∗,α:=2δ(1−c)
L, and α <1
Ln.
Theorem 3.1indicates that the upper bound of the minimum value of E[∥∇f(θk)∥2]consists of a bias term
B(θ0, K)and variance term V(σ2, b). When the number of steps Kis large and the batch size bis large,
B(θ0, K)andV(σ2, b)become small. Therefore, we need to set Klarge and blarge so that Algorithm 1will
approximate a local minimizer of f.
For the sake of convenience, we list below all assumptions considered in Theorem 3.1:
(1)[Smoothness of loss functions] fi:Rd→R(i∈[n])isLi–smooth and bounded below by fi,∗(see
Problem 2.1). This implies that f:=1
n/summationtext
i∈[n]fiisLn–smooth.
(2)[Conditions of stochastic gradient] Let θkbe the k-th iteration generated by SGD and let Gξk(θk)
be the stochastic gradient of f.
(a)[Unbiased estimator] Eξk[Gξk(θk)] =∇f(θk)(see ( 6) in Assumption 2.1(A1)).
(b)[Bounded variance] Eξk/bracketleftbig
∥Gξk(θk)−∇f(θk)∥2/bracketrightbig
≤σ2(see ( 7) in Assumption 2.1(A1)).
(c)[Mini-batch stochastic gradient] ∇fBk(θk) :=1
b/summationtext
i∈[b]Gξk,i(θk) =1
b/summationtext
i∈[b]∇fξk,i(θk)(see As-
sumption 2.1(A2)).
(3)[Armijo-line-search learning rate] Let αkbe a learning rate satisfying the Armijo condition ( 11)
(whose existence is guaranteed by Proposition 2.1).
(a)[Computability] We assume that αkcan be computed by using the backtracking Armijo-line-
search method (Algorithm 2). Lemma 2.1(ii) thus guarantees the existence of a lower bound
α:=2δ(1−c)
Lofαk. The existence of a upper bound αofαkis guaranteed by Proposition 2.1.
(b)[Condition of upper bound] We assume that α <1
Lnto ensure that C1andC2are positive (see
Theorem 3.1for the deﬁnitions of C1andC2).
Here, we compare Theorem 3.1with the convergence analysis of SGD using a constant learning rate. SGD
using a constant learning rate α∈(0,2
Ln)satisﬁes
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤2(f(θ0)−f∗)
(2−Lnα)α1
K+Lnασ2
2−Lnα1
b(12)
(The proof of ( 12) is given in Appendix A.5). We need to set a constant learning rate α∈(0,2
Ln)depending
on the Lipschitz constant Lnof∇f. However, since computing Lnis NP-hard ( Virmaux & Scaman ,2018),
it is diﬃcult to set α∈(0,2
Ln). Meanwhile, it is suﬃcient to set c, δ∈(0,1)in Algorithms 1and2without
computing the Lipschitz constant of ∇f.
We also compare Theorem 3.1with Theorem 3 in ( Vaswani et al. ,2019). Theorem 3 in ( Vaswani et al. ,
2019) indicates that, under a strong growth condition with a constant ρ(i.e.,Ei[∥∇fi(θ)∥2]≤ρ∥∇f(θ)∥2
(θ∈Rd)) and the Armijo condition, SGD satisﬁes that
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤f(θ0)−f(θ⋆)
∆K,
where c > 1−L
ρLn,α <2
ρLn,∆ := ( α+2(1−c)
L)−ρ(α−2(1−c)
L+Lnα2), and θ⋆is a local minimizer of
f. Theorem 3.1is a convergence analysis of Algorithm 1without assuming the strong growth condition or
limiting the hyperparameter c. Moreover, Theorem 3.1shows that using large batch size is appropriate for
SGD using the Armijo line search (Algorithm 1).
8Under review as submission to TMLR
3.2 Steps needed for ϵ–approximation
To investigate the relationship between the number of steps Kneeded for nonconvex optimization and the
batch size b, we consider an ϵ–approximation of Algorithm 1deﬁned as follows:
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤ϵ2, (13)
where ϵ >0is the precision.
Theorem 3.1leads to the following theorem indicating the relationship between band the values of Kthat
achieves an ϵ–approximation. The proof of Theorem 3.2is given in Appendix A.3.
Theorem 3.2 (Steps needed for nonconvex optimization of SGD using Armijo line search)
Suppose that the assumptions in Theorem 3.1hold. Deﬁne K:R→Rfor all b >C2
ϵ2by
K(b) =C1b
ϵ2b−C2, (14)
where the positive constants C1andC2are deﬁned as in Theorem 3.1. Then, the following hold:
(i)[Steps needed for nonconvex optimization] Kdeﬁned by ( 14) achieves an ϵ–approximation ( 13).
(ii)[Properties of the steps] Kdeﬁned by ( 14) is monotone decreasing and convex for b >C2
ϵ2.
Theorem 3.2ensures that the number of steps Kneeded for SGD using the Armijo line search to be an
ϵ–approximation is small when the batch size bis large. Therefore, it is useful to set a suﬃciently large
batch size in the sense of minimizing the steps needed for an ϵ–approximation of SGD using the Armijo line
search.
We also consider setting small batch sizes, e.g., b= 1. From the condition of the domain of K, we need to
satisfy
b= 1>C2
ϵ2iﬀα
α<(2−Lnα)ϵ2
σ2(15)
to ensure the results in Theorem 3.2. If the upper bound αsatisﬁes the more restricted condition ( 15) than
α <1
Ln, then SGD using the Armijo line search with b= 1andK(1) =C1
ϵ2−C2is an ϵ–approximation ( 13).
3.3 Critical batch size minimizing SFO complexity
The following theorem shows the existence of a critical batch size for SGD using the Armijo line search. The
proof of Theorem 3.3is given in Appendix A.4.
Theorem 3.3 (Existence of critical batch size for SGD using Armijo line search) Suppose that
the assumptions in Theorem 3.1hold. Deﬁne SFO complexity N:R→Rfor the number of steps K, deﬁned
by (14), needed for an ϵ–approximation ( 13) and for a batch size b >C2
ϵ2by
N(b) =K(b)b=C1b2
ϵ2b−C2, (16)
where the positive constants C1andC2are deﬁned as in Theorem 3.1. Then, the following hold:
(i)[SFO complexity] Ndeﬁned by ( 16) is convex for b >C2
ϵ2.
(ii)[Critical batch size] There exists a critical batch size
b⋆=2C2
ϵ2=2ασ2
(2−Lnα)αϵ2(17)
such that b⋆minimizes the SFO complexity ( 16).
9Under review as submission to TMLR
(iii) [Upper bound on critical batch size] The critical batch size b⋆deﬁned by ( 17) satisﬁes
b⋆≤ασ2
{α−δ(1−c)α}ϵ2. (18)
Here, we compare the number of steps KCand the SFO complexity NCfor SGD using a constant learning
rateαwith KAandNAfor SGD using the Armijo-line-search learning rate αk(∈[α,α]). Let C1,C(resp.
C2,C) beC1(resp. C2) in Table 1(see also ( 12)) for SGD using a constant learning rate and let C1,A(resp.
C2,A) beC1(resp. C2) in Table 1(see also Theorem 3.1) for SGD using the Armijo-line-search learning rate.
We have that
2(f(θ0)−f∗)
(2−Lnα)α=C1,A< C 1,C=2(f(θ0)−f∗)
(2−Lnα)αiﬀα>2−Lnα
2−Lnαα. (19)
Moreover,
ασ2
A
(2−Lnα)α=C2,A< C 2,C=Lnασ2
C
2−Lnαiﬀα
α<σ2
C(2−Lnα)
σ2
A(2−Lnα)Lnα, (20)
where σ2
C(resp. σ2
A) denotes the upper bound of the variance of the stochastic gradient for SGD using a
constant learning rate α(resp. the Armijo-line-search learning rate). If ( 19) and ( 20) hold, then SGD using
the Armijo-line-search learning rate converges faster than SGD using a constant learning rate in the sense
that
C1,Ab
ϵ2b−C2,A=KA< K C=C1,Cb
ϵ2b−C2,CandC1,Ab2
ϵ2b−C2,A=NA< N C=C1,Cb2
ϵ2b−C2,C.
It can be expected that ( 19) and ( 20) hold, since it is known empirically ( Vaswani et al. ,2019, Figure 5) that
the relationship between the Armijo-line-search learning rate αkand constant learning rate αisα < α k.
The next section numerically compares SGD using the Armijo-line-search learning rate with not only SGD
using a constant learning rate but also variants of SGD and examines the performance of SGD using the
Armijo-line-search learning rate.
The previous results in ( Shallue et al. ,2019;Zhang et al. ,2019;Iiduka ,2022) show that, for deep-learning
optimizers, there are critical batch sizes at which the SFO complexities are minimized. We are interested in
verifying whether a critical batch size exists for SGD using the Armijo line search. Theorem 3.3(iii) indicates
that an upper bound on the critical batch size can be obtained from some hyperparameters. Accordingly,
we would like to estimate the critical batch size using the upper bound ( 18). Therefore, the next section
numerically examines the relationship between the batch size band the number of steps Kneeded for
nonconvex optimization and the relationship between band the SFO complexity Nto check if there is a
critical batch size b⋆minimizing Nand if the critical batch size b⋆can be estimated from our theoretical
results.
4 Numerical Results
We veriﬁed whether numerical results match our theoretical results (Theorems 3.2and3.3), that is, the
relationship between Kandband the relationship between Nandbfor Algorithm 1. We also compared
the performance of Algorithm 1with the performances of other optimizers, such as SGD with a constant
learning rate (SGD), momentum method (Momentum), Adam, AdamW, and RMSProp. The learning rate
and hyperparameters of the ﬁve optimizers used in each experiment were determined on the basis of a grid
search.
The metrics were the number of steps Kand the SFO complexity N=Kbindicating that the training
accuracy is higher than a certain score. We used Algorithm 1with the Armijo-line-search learning rate
computed by Algorithm 2with γ= 2,δ= 0.5,α= 10 (see https://github.com/IssamLaradji/sls for
the setting of parameters), and various values ofc. The stopping condition was 200 epochs. The experimental
environment consisted of eight NVIDIA DGX A100 GPUs and two Dual AMD Rome7742 2.25-GHz, 128-
Core CPUs. The software environment was Python 3.8.2, PyTorch 1.6.0, and CUDA 11.6. The code is
available at https://anonymous.4open.science/r/armijo_linesearch-C1C3 .
10Under review as submission to TMLR
4.1 Training ResNet and MLP on the CIFAR-10, CIFAR-100, and MNIST datasets
We trained ResNet-34 on the CIFAR-10 dataset ( n= 50000 ). Figure 1plots the number of steps needed for
the training accuracy to be more than 0.99for Algorithm 1versus batch size. It can be seen that Algorithm
1decreases the number of steps as the batch size increases. Figure 2plots the SFO complexities of Algorithm
1versus the batch size. It indicates that there are critical batch sizes that minimize the SFO complexities.
2526272829210
Batch Size02000400060008000100001200014000Steps
c=0.05
c=0.10
c=0.15
c=0.20
c=0.25
c=0.30
Figure 1: Number of steps for Algorithm 1versus
batch size needed to train ResNet-34 on CIFAR-
10
2526272829210
Batch Size0.40.60.81.01.21.41.6SFO1e6
c=0.05
c=0.10
c=0.15
c=0.20
c=0.25
c=0.30Figure 2: SFO complexity for Algorithm 1versus
batch size needed to train ResNet-34 on CIFAR-
10 (The double-circle symbol denotes the mea-
sured critical batch size)
2526272829210
Batch Size02000400060008000100001200014000Steps
SGD+Armijo
SGD
Momentum
Adam
AdamW
RMSProp
Figure 3: Number of steps for Algorithm 1with
c= 0.20and variants of SGD versus batch size
needed to train ResNet-34 on CIFAR-10
2526272829210
Batch Size300000400000500000600000700000800000900000SFO
SGD+Armijo
SGD
Momentum
Adam
AdamW
RMSPropFigure 4: SFO complexity for Algorithm 1with
c= 0.20and variants of SGD versus batch size
needed to train ResNet-34 on CIFAR-10
Figures 3and4compare the performance of Algorithm 1with c= 0.20with those of variants of SGD. The
ﬁgures indicate that, when the batch sizes are from 25to29, SGD+Armijo (Algorithm 1) performs better
than the other optimizers. In particular, the SFO complexity of SGD+Armijo (Algorithm 1) using c= 0.20
and the critical batch size ( b⋆= 25) is the smallest of other optimizers for any batch size.
We also considered the case of training ResNet-34 on the CIFAR-100 dataset ( n= 50000 ). Figure 5plots
the number of steps needed for the training accuracy to be more than 0.99for Algorithm 1versus the batch
size, and Figure 6plots the SFO complexities of Algorithm 1versus the batch size. As in Figures 1and2,
these ﬁgures show that Algorithm 1decreases the number of steps as the batch size increases and there are
critical batch sizes that minimize the SFO complexities.
Figures 7and8compare the performance of Algorithm 1with c= 0.25with those of variants of SGD.
The ﬁgures indicate that, when the batch sizes are from 25to29, SGD+Armijo (Algorithm 1) performs
well. In particular, the SFO complexities of SGD and SGD+Armijo (Algorithm 1) using c= 0.25and the
critical batch size ( b⋆= 26) are smaller than the SFO complexities of the other optimizers for any batch
size. However, Figure 8indicates that the SFO complexity of SGD+Armijo (Algorithm 1) increases once
the batch size exceeds the critical value, as promised in Theorem 3.3.
We trained a two-hidden-layer MLP with widths of 512 and 256 on the MNIST dataset ( n= 60000 ). Figure
9plots the number of steps needed for the training accuracy to be more than 0.97for Algorithm 1versus
11Under review as submission to TMLR
2526272829210
Batch Size05000100001500020000250003000035000Steps
c=0.05
c=0.10
c=0.15
c=0.20
c=0.25
c=0.30
Figure 5: Number of steps for Algorithm 1versus
batch size needed to train ResNet-34 on CIFAR-
100
2526272829210
Batch Size0.51.01.52.02.53.0SFO1e6
c=0.05
c=0.10
c=0.15
c=0.20
c=0.25
c=0.30Figure 6: SFO complexity for Algorithm 1versus
batch size needed to train ResNet-34 on CIFAR-
100 (The double-circle symbol denotes the mea-
sured critical batch size)
2526272829210
Batch Size050001000015000200002500030000Steps
SGD+Armijo
SGD
Momentum
Adam
AdamW
RMSProp
Figure 7: Number of steps for Algorithm 1with
c= 0.25and variants of SGD versus batch size
needed to train ResNet-34 on CIFAR-100
2526272829210
Batch Size0.500.751.001.251.501.752.002.25SFO1e6
SGD+Armijo
SGD
Momentum
Adam
AdamW
RMSPropFigure 8: SFO complexity for Algorithm 1with
c= 0.25and variants of SGD versus batch size
needed to train ResNet-34 on CIFAR-100
12Under review as submission to TMLR
the batch size, and Figure 10plots the SFO complexities of Algorithm 1versus the batch size. As in Figures
1,2,5, and 6, these ﬁgures show that Algorithm 1decreases the number of steps as the batch size increases
and there are critical batch sizes that minimize the SFO complexities.
2526272829210211212213
Batch Size1000150020002500Steps
c=0.05
c=0.10
c=0.15
c=0.20
c=0.25
c=0.30
Figure 9: Number of steps for Algorithm 1versus
batch size needed to train MLP on MNIST
2526272829210211212213
Batch Size0.00.20.40.60.81.0SFO1e7
c=0.05
c=0.10
c=0.15
c=0.20
c=0.25
c=0.30Figure 10: SFO complexity for Algorithm 1ver-
sus batch size needed to train MLP on MNIST
(The double-circle symbol denotes the measured
critical batch size)
2526272829210211212213
Batch Size0100020003000400050006000Steps
SGD+Armijo
SGD
Momentum
Adam
AdamW
RMSProp
Figure 11: Number of steps for Algorithm 1with
c= 0.05and variants of SGD versus batch size
needed to train MLP on MNIST
2526272829210211212213
Batch Size02468SFO1e6
SGD+Armijo
SGD
Momentum
Adam
AdamW
RMSPropFigure 12: SFO complexity for Algorithm 1with
c= 0.05and variants of SGD versus batch size
needed to train MLP on MNIST
26272829210211212
Batch Size050001000015000200002500030000Steps
c=3e-5
c=1e-4
c=3e-4
c=1e-3
c=3e-3
Figure 13: Number of steps for Algorithm 1ver-
sus batch size needed to train Wide ResNet-50-2
on CIFAR-10
26272829210211212
Batch Size1.01.52.02.53.03.54.0SFO1e6
c=3e-5
c=1e-4
c=3e-4
c=1e-3
c=3e-3Figure 14: SFO complexity for Algorithm 1ver-
sus batch size needed to train Wide ResNet-50-2
on CIFAR-10 (The double-circle symbol denotes
the measured critical batch size)
Figures 11and12compare the performance of Algorithm 1with c= 0.05with those of variants of SGD.
The ﬁgures indicate that SGD+Armijo (Algorithm 1) using c= 0.05and the critical batch size (b⋆= 25)
performs better than the other optimizers in the sense of minimizing the SFO complexity. However, as was
seen in Figure 8, Figure 12indicates that the SFO complexity of SGD+Armijo (Algorithm 1) increases once
the batch size exceeds the critical value.
13Under review as submission to TMLR
We trained Wide ResNet-50-2 ( Zagoruyko & Komodakis ,2017) on the CIFAR-10 dataset ( n= 50000 ).
Figure 13plots the number of steps needed for the training accuracy to be more than 0.99for Algorithm
1versus the batch size. It can be seen that Algorithm 1decreases the number of steps as the batch size
increases. Figure 14plots the SFO complexities of Algorithm 1versus the batch size. It indicates that there
are critical batch sizes that minimize the SFO complexities.
Figures 15and16compare the performance of Algorithm 1with c= 3×10−4with those of variants of SGD.
The ﬁgures indicate that SGD+Armijo (Algorithm 1) using c= 3×10−4and the critical batch size (b⋆= 26)
performs better than the other optimizers in the sense of minimizing the SFO complexity. However, as seen
in Figures 8and12, Figure 16indicates that the SFO complexity of SGD+Armijo (Algorithm 1) increases
once the batch size exceeds the critical value.
26272829210211212
Batch Size05000100001500020000Steps
SGD+Armijo
SGD
Momentum
Adam
AdamW
RMSProp
Figure 15: Number of steps for Algorithm 1
with c= 3×10−4and variants of SGD versus
batch size needed to train Wide ResNet-50-2 on
CIFAR-10
26272829210211212
Batch Size1.01.52.02.53.03.54.0SFO1e6
SGD+Armijo
SGD
Momentum
AdamW
Adam
RMSPropFigure 16: SFO complexity for Algorithm 1
with c= 3×10−4and variants of SGD versus
batch size needed to train Wide ResNet-50-2 on
CIFAR-10
Therefore, we can conclude that Algorithm 1using the critical batch size b⋆(∈{25,26}) performs better
than other optimizers using any batch size in the sense of minimizing the SFO complexities needed to achieve
high training accuracies.
4.2 Estimation of critical batch size
We estimated the critical batch size by using Theorem 3.3(iii) and the ideas presented in ( Iiduka ,2022) and
(Sato & Iiduka ,2023). We used Algorithm 1with c= 0.05for training ResNet-34 on the CIFAR-100 dataset
(Figures 5and6). Theorem 3.3(iii) indicates that the upper bound of the critical batch size involves the
unknown value σ2. We checked that the Armijo-line-search learning rates for Algorithm 1with c= 0.05are
about 10(see also ( Vaswani et al. ,2019, Figure 5 (Left))). Hence, we used α≈α≈10. We estimated the
unknown value X=σ2
ϵ2in the upper bound ( 18) of the critical batch size by using δ= 0.5,b⋆= 25(see
Figure 6), and α≈α≈10as follows:
b⋆≈ασ2
{α−δ(1−c)α}ϵ2≈X,i.e.,X≈32.
Let us estimate the critical batch size using X≈32and Theorem 3.3(iii). For example, when using Algorithm
1with c= 0.25for training ResNet-34 on the CIFAR-100 dataset, the upper bound of the critical batch size
is
α
α−δ(1−c)αX≈51.2≈26=b⋆,
which implies that the estimated critical batch size 51.2is close to the measured critical batch size b⋆= 26=
64in Figure 6.
14Under review as submission to TMLR
5 Conclusion
This paper presented a convergence analysis of SGD using the Armijo line search for nonconvex optimization.
The analysis indicates that the upper bound of the expectation of the squared norm of the full gradient
becomes smaller as the number of steps and the batch size grow. Moreover, we investigated the relationship
between the number of steps and the batch size needed for nonconvex optimization of SGD using the Armijo
line search. We showed that the number of steps needed for nonconvex optimization is monotone decreasing
and convex with respect to the batch size; i.e., the steps decrease in number as the batch size increases. We
also showed that the SFO complexity needed for nonconvex optimization is convex with respect to the batch
size and that there exists a critical batch size at which the SFO complexity is minimized. In addition, we gave
an upper bound on the critical batch size and showed that it can be estimated by using some parameters.
Finally, we provided numerical results that support our theoretical ﬁndings. We trained ResNets on the
CIFAR-10 and CIFAR-100 datasets and MLP on the MNIST dataset and found that SGD using the Armijo
line search decreases the number of steps as the batch size increases and that SGD using the Armijo line
search and the critical batch size performs better than other optimizers for any batch size in the sense of
minimizing the SFO complexities needed to achieve high training accuracies. Moreover, we showed that the
batch sizes estimated from the upper bound of the critical batch size are close to those of the numerical
results.
References
Amir Beck. First-Order Methods in Optimization . Society for Industrial and Applied Mathematics, Philadel-
phia, PA, 2017.
Hao Chen, Lili Zheng, Raed AL Kontar, and Garvesh Raskutti. Stochastic gradient descent in correlated set-
tings: A study on Gaussian processes. In Advances in Neural Information Processing Systems , volume 33,
2020.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research , 12:2121–2159, 2011.
Benjamin Fehrman, Benjamin Gess, and Arnulf Jentzen. Convergence rates for the stochastic gradient
descent method for non-convex objective functions. Journal of Machine Learning Research , 21:1–48, 2020.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochas-
tic composite optimization I: A generic algorithmic framework. SIAM Journal on Optimization , 22:1469–
1492, 2012.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochas-
tic composite optimization II: Shrinking procedures and optimal algorithms. SIAM Journal on Optimiza-
tion, 23:2061–2089, 2013.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems , volume 30, 2017.
Hideaki Iiduka. Critical bach size minimizes stochastic ﬁrst-order oracle complexity of deep learning optimizer
using hyperparameters close to one. arXiv: 2208.09814, 2022.
Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Parallelizing
stochastic gradient descent for least squares regression: Mini-batching, averaging, and model misspeciﬁ-
cation. Journal of Machine Learning Research , 18(223):1–42, 2018.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of The
International Conference on Learning Representations , 2015.
Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien. Stochastic polyak step-size for
SGD: An adaptive learning rate for fast convergence. In Proceedings of the 24th International Conference
on Artiﬁcial Intelligence and Statistics , volume 130, 2021.
15Under review as submission to TMLR
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of The International
Conference on Learning Representations , 2019.
Hiroki Naganuma and Hideaki Iiduka. Conjugate gradient method for generative adversarial networks. In
Proceedings of The 26th International Conference on Artiﬁcial Intelligence and Statistics , volume 206 of
Proceedings of Machine Learning Research , pp. 4381–4408. PMLR, 2023.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approxima-
tion approach to stochastic programming. SIAM Journal on Optimization , 19:1574–1609, 2009.
Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of convergence
O(1/k2).Doklady AN USSR , 269:543–547, 1983.
J. Nocedal and S. J. Wright. Numerical Optimization . Springer Series in Operations Research and Financial
Engineering. Springer, New York, 2nd edition, 2006.
Boris T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational
Mathematics and Mathematical Physics , 4:1–17, 1964.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In Proceedings
of The International Conference on Learning Representations , 2018.
Herbert Robbins and Herbert Monro. A stochastic approximation method. The Annals of Mathematical
Statistics , 22:400–407, 1951.
Naoki Sato and Hideaki Iiduka. Existence and estimation of critical batch size for training generative
adversarial networks with two time-scale update rule. In Proceedings of the 40th International Conference
on Machine Learning , volume 202 of Proceedings of Machine Learning Research , pp. 30080–30104. PMLR,
23–29 Jul 2023.
Kevin Scaman and Cédric Malherbe. Robustness analysis of non-convex stochastic gradient descent using
biased expectations. In Advances in Neural Information Processing Systems , volume 33, 2020.
Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E.
Dahl. Measuring the eﬀects of data parallelism on neural network training. Journal of Machine Learning
Research , 20:1–49, 2019.
Tijmen Tieleman and Geoﬀrey Hinton. RMSProp: Divide the gradient by a running average of its recent
magnitude. COURSERA: Neural networks for machine learning , 4:26–31, 2012.
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-Julien.
Painless stochastic gradient: Interpolation, line-search, and convergence rates. In Advances in Neural
Information Processing Systems , volume 32, 2019.
Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: analysis and eﬃcient
estimation. In Advances in Neural Information Processing Systems , volume 31, 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv: 1605.07146, 2017.
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl, Christopher J.
Shallue, and Roger Grosse. Which algorithmic choices matter at which batch sizes? Insights from a noisy
quadratic model. In Advances in Neural Information Processing Systems , volume 32, 2019.
Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings
of the 20th International Conference on Machine Learning , pp. 928–936, 2003.
16Under review as submission to TMLR
A Appendix
A.1 Proof of lemma 2.1
(i) Let k∈Nand let LBkbe the Lipschitz constant of ∇fBk. Lemma 1 in ( Vaswani et al. ,2019) is as follows:
∀fBk:Rd→R∀c∈(0,1)∀θk∈Rd∀α > 0
∃αk∈(0,α] (fBk(θk−αk∇fBk(θk))≤fBk(θk)−cαk∥∇fBk(θk)∥2)
⇒min/braceleftbigg2(1−c)
LBk,α/bracerightbigg
≤αk.(21)
The negative proposition of ( 21) is as follows:
∃fBk:Rd→R∃c∈(0,1)∃θk∈Rd∃α > 0
∃αk∈(0,α] (fBk(θk−αk∇fBk(θk))≤fBk(θk)−cαk∥∇fBk(θk)∥2)
∧min/braceleftbigg2(1−c)
LBk,α/bracerightbigg
> α k.(22)
We will prove that ( 22) holds. Let n=b= 1,d= 1,c= 0.1,α= 1, and f(θ) =fBk(θ) =θ2. From
∇f(θ) = 2 θ, we have that LBk= 2. Since θ∗= 0is the global minimizer of f, we set θk∈Rsuch that
θk̸=θ∗. The Armijo condition in this case is such that (θk−2αkθk)2≤θ2
k−cαk(2θk)2, which is equivalent
toαk≤1−c= 0.9. Hence,
∃αk∈(0,1] (αk≤0.9)∧(min{0.9,1}> α k)
⇔∃αk∈(0,α] (αk≤1−c)∧/parenleftbigg
min/braceleftbigg2(1−c)
LBk,α/bracerightbigg
> α k/parenrightbigg
⇔∃αk∈(0,α] (fBk(θk−αk∇fBk(θk))≤fBk(θk)−cαk∥∇fBk(θk)∥2)
∧min/braceleftbigg2(1−c)
LBk,α/bracerightbigg
> α k,
which implies that ( 22) holds.
(ii) Sinceαk
δdoes not satisfy the Armijo condition ( 11), we have that
fBk/parenleftBig
θk−αk
δ∇fBk(θk)/parenrightBig
> fBk(θk)−cαk
δ∥∇fBk(θk)∥2. (23)
TheLBk–smoothness of fBkensures that the descent lemma is true, i.e.,
fBk/parenleftBig
θk−αk
δ∇fBk(θk)/parenrightBig
≤fBk(θk) +/angbracketleftBig
∇fBk(θk),/parenleftBig
θk−αk
δ∇fBk(θk)/parenrightBig
−θk/angbracketrightBig
+LBk
2/vextenddouble/vextenddouble/vextenddouble/parenleftBig
θk−αk
δ∇fBk(θk)/parenrightBig
−θk/vextenddouble/vextenddouble/vextenddouble2
,
which implies that
fBk/parenleftBig
θk−αk
δ∇fBk(θk)/parenrightBig
≤fBk(θk) +αk
δ/parenleftbiggLBkαk
2δ−1/parenrightbigg
∥∇fBk(θk)∥2. (24)
Hence, ( 23) and ( 24) imply that
−cαk
δ∥∇fBk(θk)∥2≤αk
δ/parenleftbiggLBkαk
2δ−1/parenrightbigg
∥∇fBk(θk)∥2,
which in turn implies that
αk
δ/parenleftbiggLBkαk
2δ−(1−c)/parenrightbigg
∥∇fBk(θk)∥2≥0.
17Under review as submission to TMLR
Accordingly,
LBkαk
2δ−(1−c)≥0,i.e.,αk≥2δ(1−c)
LBk.
From LBk=1
b/summationtext
i∈[b]Lξk,i≤L:= max i∈[n]Li(k∈N), we also have that αk≥2δ(1−c)
LBk≥2δ(1−c)
L.
A.2 Proof of Theorem 3.1
The deﬁnition of f(θ) :=1
n/summationtext
i∈[n]fi(θ)and the Li–smoothness of fi(i∈[n])imply that, for all θ1,θ2∈Rd,
∥∇f(θ1)−∇f(θ2)∥≤1
n/summationdisplay
i∈[n]∥∇fi(θ1)−∇fi(θ2)∥≤/summationtext
i∈[n]Li
n∥θ1−θ2∥,
which in turn implies that ∇fis Lipschitz continuous with Lipschitz constant Ln:=1
n/summationtext
i∈[n]Li. Hence,
the descent lemma ensures that, for all k∈N,
f(θk+1)≤f(θk) +⟨∇f(θk),θk+1−θk⟩+Ln
2∥θk+1−θk∥2,
which, together with θk+1:=θk−αk∇fBk(θk), implies that
f(θk+1)≤f(θk)−αk⟨∇f(θk),∇fBk(θk)⟩+Lnα2
k
2∥∇fBk(θk)∥2. (25)
From⟨x,y⟩=1
2(∥x∥2+∥y∥2−∥x−y∥2) (x,y∈Rd), we have that, for all k∈N,
⟨∇f(θk),∇fBk(θk)⟩=1
2/parenleftbig
∥∇f(θk)∥2+∥∇fBk(θk)∥2−∥∇ f(θk)−∇fBk(θk)∥2/parenrightbig
.
Accordingly, ( 25) implies that, for all k∈N,
f(θk+1)≤f(θk)−αk
2/parenleftbig
∥∇f(θk)∥2+∥∇fBk(θk)∥2−∥∇ f(θk)−∇fBk(θk)∥2/parenrightbig
+Lnα2
k
2∥∇fBk(θk)∥2
=f(θk)−αk
2∥∇f(θk)∥2+1
2(Lnαk−1)αk∥∇fBk(θk)∥2+αk
2∥∇f(θk)−∇fBk(θk)∥2.
From 0< α≤αk≤α <1
Ln, we have that, for all k∈N,
(Lnαk−1)αk≤(Lnα−1)αk≤(Lnα−1)α<0.
Hence, for all k∈N,
f(θk+1)≤f(θk)−α
2∥∇f(θk)∥2+1
2(Lnα−1)α∥∇fBk(θk)∥2+α
2∥∇f(θk)−∇fBk(θk)∥2. (26)
Assumption 2.1guarantees that
E[∇fBk(θk)|θk] =∇f(θk)andE/bracketleftbig
∥∇fBk(θk)−∇f(θk)∥2|θk/bracketrightbig
≤σ2
b. (27)
Hence, we have
E/bracketleftbig
∥∇fBk(θk)∥2|θk/bracketrightbig
=E/bracketleftbig
∥∇fBk(θk)−∇f(θk) +∇f(θk)∥2|θk/bracketrightbig
=E/bracketleftbig
∥∇fBk(θk)−∇f(θk)∥2|θk/bracketrightbig
+ 2E[⟨∇fBk(θk)−∇f(θk),∇f(θk)⟩|θk] +E/bracketleftbig
∥∇f(θk)∥2|θk/bracketrightbig
≥∥∇ f(θk)∥2.(28)
18Under review as submission to TMLR
Inequalities ( 26), (27), and ( 28) guarantee that, for all k∈N,
E[f(θk+1)|θk]≤f(θk)−α
2∥∇f(θk)∥2+1
2(Lnα−1)α∥∇f(θk)∥2+ασ2
2b. (29)
Taking the total expectation on both sides of ( 29) thus ensures that, for all k∈N,
1
2{α−(Lnα−1)α}E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤E[f(θk)−f(θk+1)] +ασ2
2b. (30)
LetK≥1. Summing ( 30) from k= 0tok=K−1ensures that
(2−Lnα)α
2K−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤E[f(θ0)−f(θK)] +ασ2K
2b,
which, together with the boundedness of f, i.e., f∗≤f(θk), implies that
(2−Lnα)α
2K−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤E[f(θ0)−f∗] +ασ2K
2b.
Therefore, we have
1
KK−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤2(f(θ0)−f∗)
(2−Lnα)αK+ασ2
(2−Lnα)αb.
Moreover, since we have
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤1
KK−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
,
the assertion in Theorem 3.1holds.
A.3 Proof of Theorem 3.2
(i) We have
C1
K+C2
b=ϵ2
is equivalent to
K=K(b) =C1b
ϵ2b−C2.
Hence, Theorem 3.1leads to an ϵ–approximation.
(ii) We have
dK(b)
db=−C1C2
(ϵ2b−C2)2≤0andd2K(b)
db2=2C1C2ϵ2
(ϵ2b−C2)3≥0,
which implies that Kis monotone decreasing and convex with respect to b.
A.4 Proof of Theorem 3.3
(i) From
N(b) =C1b2
ϵ2b−C2,
19Under review as submission to TMLR
we have
dN(b)
db=C1b(ϵ2b−2C2)
(ϵ2b−C2)2andd2N(b)
db2=2C1C2
2
(ϵ2b−C2)3≥0,
which implies that Nis convex with respect to b.
(ii) We have
dN(b)
db

<0ifb < b⋆,
= 0 ifb=b⋆=2C2
ϵ2,
>0ifb > b⋆.
Hence, the point b⋆minimizes N.
(iii) Lemma 2.1(iii) ensures that
Ln:=1
n/summationdisplay
i∈[n]Li≤L=2δ(1−c)
α.
Hence,
b⋆=2C2
ϵ2=2ασ2
(2−Lnα)αϵ2≤2ασ2
αϵ2α
2{α−δ(1−c)α}=ασ2
{α−δ(1−c)α}ϵ2.
A.5 Proof of ( 12)
LetK≥1. From ( 25) and αk:=α > 0, we have that, for all k∈N,
f(θk+1)≤f(θk)−α⟨∇f(θk),∇fBk(θk)⟩+Lnα2
2∥∇fBk(θk)∥2.
Hence, ( 27) and ( 28) ensure that, for all k∈N,
E[f(θk+1)]≤E[f(θk)]−αE/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
+Lnα2
2/parenleftbigg
E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
+σ2
b/parenrightbigg
,
which implies that, for all k∈N,
α/parenleftbigg
1−Lnα
2/parenrightbigg
E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤E[f(θk)−f(θk+1)] +Lnα2σ2
2b.
Summing the above inequalities from k= 0tok=K−1ensures that
α/parenleftbigg
1−Lnα
2/parenrightbiggK−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤E[f(θ0)−f(θK)] +Lnα2σ2K
2b.
Since fis bounded below by f∗:=1
n/summationtext
i∈[n]fi,∗, we have
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤1
KK−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤2E[f(θ0)−f∗]
α(2−Lnα)K+Lnασ2
(2−Lnα)b.
20