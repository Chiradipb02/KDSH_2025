Published in Transactions on Machine Learning Research (08/2023)
About the Cost of Central Privacy in Density Estimation
Clément Lalanne clement.lalanne@ens-lyon.fr
Univ. Lyon, ENS Lyon, UCBL, CNRS, Inria, LIP, F-69342, Lyon Cedex 07, France
Aurélien Garivier aurelien.garivier@ens-lyon.fr
Univ. Lyon, ENS Lyon, UMPA UMR 5669, 46 allée d’Italie, F-69364, Lyon cedex 07
Rémi Gribonval remi.gribonval@inria.fr
Univ. Lyon, ENS Lyon, UCBL, CNRS, Inria, LIP, F-69342, Lyon Cedex 07, France
Reviewed on OpenReview: https://openreview.net/forum?id=uq29MIWvIV
Abstract
We study non-parametric density estimation for densities in Lipschitz and Sobolev spaces,
and under centralprivacy. In particular, we investigate regimes where the privacy budget is
notsupposed to be constant. We consider the classical definition of central differential pri-
vacy, but also the more recent notion of central concentrated differential privacy. We recover
the result of Barber & Duchi (2014) stating that histogram estimators are optimal against
Lipschitz distributions for the L2risk and, under regular differential privacy, we extend it
to other norms and notions of privacy. Then, we investigate higher degrees of smoothness,
drawing two conclusions: First, and contrary to what happens with constant privacy budget
(Wasserman & Zhou, 2010), there areregimes where imposing privacy degrades the regular
minimax risk of estimation on Sobolev densities. Second, so-called projection estimators are
near-optimal against the same classes of densities in this new setup with pure differential
privacy, but contrary to the constant privacy budget case, it comes at the cost of relaxation.
With zero concentrated differential privacy, there is no need for relaxation, and we prove
that the estimation is optimal.
1 Introduction
Thecommunicationofinformationbuiltonusers’dataleadstonewchallenges, andnotablyprivacyconcerns.
It is now well documented that the release of various quantities can, without further caution, have disastrous
repercussions(Narayanan&Shmatikov,2006;Backstrometal.,2007;Fredriksonetal.,2015;Dinur&Nissim,
2003; Homer et al., 2008; Loukides et al., 2010; Narayanan & Shmatikov, 2008; Sweeney, 2000; Gonon et al.,
2023; Wagner & Eckhoff, 2018; Sweeney, 2002). In order to address this issue, differential privacy (DP)
(Dwork et al., 2006b) has become the gold standard in privacy protection. The idea is to add a proper layer
of randomness in order to hide each user’s data. It is notably used by the US Census Bureau (Abowd, 2018),
Google (Erlingsson et al., 2014), Apple (Thakurta et al., 2017) and Microsoft (Ding et al., 2017), among
many others.
As with other forms of communication or processing constraints (Barnes et al., 2019; 2020; Acharya et al.,
2021a;b;c;d), privacy recently gained a lot of attention from the statistical and theoretical machine learning
communities. At this point, the list of interesting publications is far too vast to be exhaustive, but here is a
sample : Wasserman & Zhou (2010) is the first article to consider problems analogous to the ones presented
in this article. It notably studies the problem of nonparametric density estimation, to which we provide many
complements. Duchi et al. (2014; 2013; 2016); Barber & Duchi (2014); Acharya et al. (2021e); Lalanne et al.
(2023a) present general frameworks for deriving minimax lower-bounds under privacy constraints. Many
parametric problems have already been studied, notably in Acharya et al. (2018; 2021e); Karwa & Vadhan
(2018); Kamath et al. (2019); Biswas et al. (2020); Lalanne et al. (2022; 2023b); Kamath et al. (2022);
Singhal (2023). Recently, some important contributions were made. For instance, Asi et al. (2023) sharply
characterized the equivalence between private estimation and robust estimation with the inverse sensitivity
1Published in Transactions on Machine Learning Research (08/2023)
mechanism (Asi & Duchi, 2020a;b), and Kamath et al. (2023) detailed the bias-variance-privacy trilemma,
proving in particular the necessity of adding bias, even on distributions with bounded support, for many
private estimation problems.
We address here the problem of privately estimating a probability density, which fits in this line of work.
GivenX:= (X1,...,Xn)∼P⊗n
π, where Pπrefers to a distribution of probability that has a density π
with respect to the Lebesgue measure on [0,1], how to estimate πprivately? Technically, what metrics or
hypothesis should be set on π? What is the cost of privacy? Are the methods known so far optimal? Such
are the questions that are investigated in the rest of this article.
1.1 Related work
Non-parametric density estimation has been an important topic of research in statistics for many decades
now. Among the vast literature on the topic, let us just mention the important references Györfi et al.
(2002); Tsybakov (2009).
Recently, the interest for private statistics has shone a new light on this problem. Remarkable early con-
tributions (Wasserman & Zhou, 2010; Hall et al., 2013) adapted histogram estimators, so-called projection
estimators and kernel estimators to satisfy the privacy constraint. They conclude that the minimax rate of
convergence, n−2β/(2β+1), wherenis the sample size and βis the (Sobolev) smoothness of the density, is
not affected by centralprivacy (also known as globalprivacy). However, an important implicit hypothesis in
this line of work is that ϵ, the parameter that decides how private the estimation needs to be, is supposed
not to depend on the sample size. This hypothesis may seem disputable, and more importantly, it fails to
precisely characterize the tradeoff between utility and privacy.
Indeed, differential privacy gives guarantees on how hard it is to tell if a specific user was part of the dataset.
Despite the fact that one could hope to leverage the high number of users in a dataset in order to increase the
privacy w.r.t. each user, previous studies (Wasserman & Zhou, 2010; Hall et al., 2013) cover an asymptotic
scenario with respect to the number of samples n, forfixedϵ. In contrast, our study highlights new behaviors
for this problem. For each sample size, we emphasize the presence of two regimes: when the order of ϵis
larger than some threshold (dependent on n) that we provide, privacy can be obtained with virtually no
cost; when ϵis smaller than this threshold, it is the limiting factor for the accuracy of estimation.
To the best of our knowledge, the only piece of work that studies this problem under centralprivacy when ϵis
not supposed constant is Barber & Duchi (2014). They study histogram estimators on Lipschitz distributions
for the integrated risk. They conclude that the minimax risk of estimation is max/parenleftbig
n−2/3+ (nϵ)−1/parenrightbig
, showing
how smallϵcan be before the minimax risk of estimation is degraded. Our article extends such results to
high degrees of smoothness, to other definitions of centraldifferential privacy, and to other risks.
In the literature, there exist other notions of privacy, such as the much stricter notion of localdifferential
privacy. Under this different notion of privacy, the problem of non-parametric density estimation has already
been extensively studied. We here give a few useful bibliographic pointers. A remarkable early piece of work
Duchi et al. (2016) has brought a nice toolbox for deriving minimax lower bounds under local privacy that
has proven to give sharp results for many problems. As a result, the problem of non-parametric density
estimation (or its analogous problem of non-parametric regression) has been extensively studied under local
privacy. For instance, Butucea et al. (2019) investigates the elbow effect and questions of adaptivity over
Besov ellipsoids. Kroll (2021) and Schluttenhofer & Johannes (2022) study the density estimation problem
at a given point with an emphasis on adaptivity. Universal consistency properties have recently been derived
in Györfi & Kroll (2023). Analogous regression problems have been studied in Berrett et al. (2021) and in
Györfi & Kroll (2022). Finally, the problem of optimal non-parametric testing has been studied in Lam-Weil
et al. (2022).
1.2 Contributions
In this article, we investigate the impact of centralprivacy when the privacy budget is not constant. We
treat multiple definitions of centralprivacy and different levels of smoothness for the densities of interest.
In terms of upper-bounds, we analyze histogram and projection estimators at a resolution that captures
the impact of the privacy and smoothness parameters. We also prove new lower bounds using the classical
2Published in Transactions on Machine Learning Research (08/2023)
packing method combined with new tools that characterize the testing difficulty under central privacy from
Acharya et al. (2021e); Kamath et al. (2022); Lalanne et al. (2023a).
In particular, for Lipschitz densities and under pure differential privacy, we recover the results of Barber &
Duchi (2014) with a few complements. We then extend the estimation on this class of distributions to the
context of concentrated differential privacy (Bun & Steinke, 2016), a more modern definition of privacy that
is compatible with stochastic processes relying on Gaussian noise. We finally investigate higher degrees of
smoothness by looking at periodic Sobolev distributions. The main results are summarized in Table 1.2.
ϵ-DPEquation (1) ρ-zCDPEquation (2)
Lipschitz
Equation (4)Upper-bound:
O/parenleftbig
max/braceleftbig
n−2/3,(nϵ)−1/bracerightbig/parenrightbig
(Barber & Duchi, 2014) & Theorem 1
Lower-bounds:
-Pointwise: Ω/parenleftbig
max/braceleftbig
n−2/3,(nϵ)−1/bracerightbig/parenrightbig
Theorem 2 & Corollary 1
-Integrated: Ω/parenleftbig
max/braceleftbig
n−2/3,(nϵ)−1/bracerightbig/parenrightbig
(Barber & Duchi, 2014) & Theorem 3Upper-bound:
O/parenleftbig
max/braceleftbig
n−2/3,(n√ρ)−1/bracerightbig/parenrightbig
Theorem 1
Lower-bounds:
-Pointwise: Ω/parenleftbig
max/braceleftbig
n−2/3,(n√ρ)−1/bracerightbig/parenrightbig
Theorem 2 & Corollary 1
-Integrated: Ω/parenleftbig
max/braceleftbig
n−2/3,(n√ρ)−1/bracerightbig/parenrightbig
Theorem 3
Periodic Sobolev
Smoothness β
Equation (9)Upper-bounds:
-Pure DP:O/parenleftig
max/braceleftig
n−2β
2β+1,(nϵ)−2β
β+3/2/bracerightig/parenrightig
Theorem 4
-Relaxed: max/braceleftigg
n−2β
2β+1,/parenleftbigg
nϵ√
ln(1.25/δ)/parenrightbigg−2β
β+1/bracerightigg
Section 4.4
Lower-bound:
Ω/parenleftig
max/braceleftig
n−2β
2β+1,(nϵ)−2β
β+1/bracerightig/parenrightig
Theorem 5Upper-bound:
O/parenleftig
max/braceleftig
n−2β
2β+1,(n√ρ)−2β
β+1/bracerightig/parenrightig
Theorem 4
Lower-bound:
Ω/parenleftig
max/braceleftig
n−2β
2β+1,(n√ρ)−2β
β+1/bracerightig/parenrightig
Theorem 5
Table 1: Summary of the results
The paper is organized as follows. The required notions regarding centraldifferential privacy are recalled in
Section 2. Histogram estimators and projection estimators are respectively studied in Section 3, on Lipschitz
densities, and in Section 4, on periodic Sobolev densities. A short conclusion in provided in Section 5.
2 Central Differential Privacy
We recall in this section some useful notions of centralprivacy. Here,Xandnrefer respectively to the
sample space and to the sample size.
Given two datasets X= (X1,...,Xn),Y= (Y1,...,Yn)∈Xn, theHamming distance between XandYis
defined as
dham(X,Y) =n/summationdisplay
i=11Xi̸=Yi.
Givenϵ>0andδ∈[0,1), a randomized mechanism M:Xn→codom (M)(forcodomain or image of M)
is(ϵ,δ)-differentially private (or (ϵ,δ)-DP) (Dwork et al., 2006b;a) if for all X,Y∈Xnand all measurable
S⊆codom (M):
dham(X,Y)≤1 =⇒PM(M(X)∈S)≤eϵPM(M(Y)∈S) +δ, (1)
wheredham(·,·)denotes the Hamming distance on Xn.
InordertosharplycounttheprivacyofacompositionofmanyGaussianmechanisms(seeAbadietal.(2016)),
privacy is also often characterized in terms of Renyi divergence (Mironov, 2017). Nowadays, it seems that
all these notions tend to converge towards the definition of zero concentrated differential privacy (Dwork &
3Published in Transactions on Machine Learning Research (08/2023)
Rothblum, 2016; Bun & Steinke, 2016). Given ρ∈(0,+∞), a randomized mechanism M:Xn→codom (M)
isρ-zero concentrated differentially private ( ρ-zCDP) if for all X,Y∈Xn,
dham(X,Y)≤1 =⇒ ∀ 1<α< +∞,Dα(M(X)∥M(Y))≤ρα (2)
where Dα(·∥·)denotes the Renyi divergence of level α, defined when α>1as
Dα(P∥Q):=1
α−1ln/integraldisplay/parenleftbiggdP
dQ/parenrightbiggα−1
dQ.
For more details, we recommend referring to the excellent article van Erven & Harremoës (2014).
There are links between (ϵ,δ)-DP andρ-zCDP. For instance, if a mechanism is ρ-zCDP, then (Bun & Steinke,
2016, Proposition 3) it is (ϵ,δ)-DP for a collection of (ϵ,δ)’s that depends on ρ. Conversely, if a mechanism
is(ϵ,0)-DP, then (Bun & Steinke, 2016, Proposition 4) it is also ϵ2/2-zCDP.
Given a deterministic function fmapping a dataset to a quantity in Rd, the Laplace mechanism (Dwork
et al., 2006b) and Gaussian mechanism (Bun & Steinke, 2016) are two famous ways to turn finto a private
mechanism. Defining the l1sensitivity of fas
∆1f:= sup
X,Y∈Xn:dham(X,Y)≤1/vextenddouble/vextenddoublef(X)−f(Y)/vextenddouble/vextenddouble
1,
the Laplace mechanism instantiated with fandϵ>0is defined as
X∝⇕⊣√∫⊔≀→f(X) +∆1f
ϵL(Id),
whereL(Id)refers to a random vector of dimension dwith independent components that follow a centered
Laplace distribution of parameter 1. Notice that we took the liberty to use the same notation for the random
variable and for its distribution. We made this choice for brevity, and because it does not really create any
ambiguity. It is (ϵ,0)-DP (simply noted ϵ-DP) (Dwork et al., 2006a;b). Likewise, defining the L2sensitivity
offas
∆2f:= sup
X,Y∈Xn:dham(X,Y)≤1/vextenddouble/vextenddoublef(X)−f(Y)/vextenddouble/vextenddouble
2,
the Gaussian mechanism instantiated with fandρ>0is defined as
X∝⇕⊣√∫⊔≀→f(X) +∆2f√2ρN(0,Id),
whereN(0,Id)refers to a random vector of dimension dwith independent components that follow a centered
Normal distribution of variance 1. It isρ-zCDP (Bun & Steinke, 2016).
A quick word on local privacy. Central privacy comes with the hypothesis of a trusted aggregator (also
known as a curator, hence the alternative name a "trusted curator model" for central privacy, which is also
known under the name global privacy ) that sees the entire dataset, and builds an estimator with it. Only
the produced estimator is private. In order to give an example, this is like having a datacenter that stores
all the information about the users of a service, but only outputs privatized statistics.
Local privacy on the other hand does not make that hypothesis. Each piece of data is anonymized locally
(on the user’s device) and then it is communicated to an aggregator. Any locally private mechanism is also
centrally private, but the converse is not true.
At first, local privacy can seem more appealing : it is indeed a stronger notion of privacy. However, it
degrades the utility much more than central privacy. As a result, both notions are interesting, and the use
of one or the other must be weighted for a given problem. This work focuses on the centralvariant.
4Published in Transactions on Machine Learning Research (08/2023)
3 Histogram Estimators and Lipschitz Densities
Histogram estimators approximate densities with a piecewise continuous function by counting the number of
points that fall into each bin of a partition of the support. Since those numbers follow binomial distributions,
the study of histogram estimators is rather simple. Besides, they are particularly interesting when privacy
is required, since the sensitivity of a histogram query is bounded independently of the number of bins. They
were first studied in this setup in Wasserman & Zhou (2010), while Barber & Duchi (2014) provided new
lower-bounds that did not require a constant privacy budget.
As a warm-up, this section proposes a new derivation of known results in more modern lower-bounding
frameworks (Acharya et al., 2021e; Kamath et al., 2022; Lalanne et al., 2023a), and then extends these
upper-bounds and lower-bounds to the case of zCDP. Furthermore, it also covers the pointwise risk as well
as the infinite-norm risk.
Leth >0be a given bandwidth or binsize. In order to simplify the notation, we suppose without loss of
generality that 1/h∈N\{0}(if the converse is true, simply take h′= 1/⌈1/h⌉where⌈x⌉refers to the
smallest integer bigger than x).[0,1]is partitioned in1
hsub-intervals of length h, which are called the bins
of the histogram. Let Z1,...,Z 1/hbe independent and identically distributed random variables with the
same distribution as a random variable Zthat is supposed to be centered and to have a finite variance.
Given a dataset X= (X1,...,Xn), the (randomized) histogram estimator is defined for x∈[0,1]as
ˆπhist(X)(x):=/summationdisplay
b∈bins1b(x)1
nh/parenleftiggn/summationdisplay
i=11b(Xi) +Zb/parenrightigg
. (3)
We indexed the Z’s by a bin instead of an integer without ambiguity. Note that by taking Zalmost-surely
constant to 0, one recovers the usual (non-private) histogram estimator of a density.
3.1 General utility of histogram estimators
Characterizing the utility of (3) typically requires assumptions on the distribution πto estimate. The class
ofL-Lipschitz densities is defined as
ΘLip
L:=/braceleftigg
π∈C0([0,1],R+)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/braceleftigg
∀x,y∈[0,1],|π(y)−π(x)|≤L|y−x|,/integraltext
[0,1]π= 1./bracerightigg
. (4)
The following general-purpose lemma gives an upper-bound on the error that the histogram estimator makes
on Lipschitz distributions:
Lemma 1 (General utility of (3)) .There exists CL>0, a positive constant that only depends on L, such
that
sup
x0∈[0,1]sup
π∈ΘLip
LEX∼P⊗n
π,ˆπhist/parenleftig/parenleftbig
ˆπhist(X)(x0)−π(x0)/parenrightbig2/parenrightig
≤CL/parenleftbigg
h2+1
nh+V(Z)
n2h2/parenrightbigg
.
The proof is given in Appendix C. The term h2corresponds to the bias of the estimator. The variance
term1
nh+V(Z)
n2h2exhibits two distinct contributions : the sampling noise1
nhand the privacy noiseV(Z)
n2h2. In
particular, the utility of ˆπhistchanges depending whether the variance is dominated by the sampling noise
or by the privacy noise.
3.2 Privacy and bin size tuning
ˆπhist(X)is a simple function of the bin count vector f(X):=/parenleftbig/summationtextn
i=11b1(Xi),...,/summationtextn
i=11b1/h(Xi)/parenrightbig
. In
particular, since the bins form a partition of [0,1], changing the value of one of the X’s can change the values
of at most two components of f(X)by at most 1. Hence, the l1andl2sensitivities of fare respectively 2
and√
2. By a direct application of the Laplace or Gaussian mechanisms, and by choosing the binsize that
minimizes the variance, we obtain the following privacy-utility result :
Theorem 1 (Privacy and utility of (3) - DP case) .Givenϵ>0, using ˆπhistwithh= max(n−1/3,
(nϵ)−1/2)andZ=2
ϵL(1), whereL(1)refers to a random variable following a Laplace distribution of param-
eter1, leads to an ϵ-DP procedure. Furthermore, in this case, there exists CL>0, a positive constant that
5Published in Transactions on Machine Learning Research (08/2023)
only depends on L, such that
sup
x0∈[0,1]sup
π∈ΘLip
LEX∼P⊗n
π,ˆπhist/parenleftig/parenleftbig
ˆπhist(X)(x0)−π(x0)/parenrightbig2/parenrightig
≤CLmax/braceleftig
n−2/3,(nϵ)−1/bracerightig
.
Furthermore, given ρ>0, using ˆπhistwithh= max(n−1/3,(n√ρ)−1/2)andZ=/radicalig
1
ρN(0,1), whereN(0,1)
refers to a random variable following a centered Gaussian distribution of variance 1, leads to a ρ-zCDP
procedure. Furthermore, in this case, there exists CL>0, a positive constant that only depends on L, such
that
sup
x0∈[0,1]sup
π∈ΘLip
LEX∼P⊗n
π,ˆπhist/parenleftig/parenleftbig
ˆπhist(X)(x0)−π(x0)/parenrightbig2/parenrightig
≤CLmax/braceleftig
n−2/3,(n√ρ)−1/bracerightig
.
Note that this bound is uniform in x0, which is more general than the integrated upper-bounds presented in
Barber & Duchi (2014). In particular, by integration on [0,1], the same bound also holds for the integrated
risk (inL2norm), which recovers the version of Barber & Duchi (2014). As expected, the optimal bin size
hdepends on the sample size nand on the parameter ( ϵorρ) tuning the privacy. Also note that ρ-zCDP
version may also be obtained by the relations between ϵ-DP andρ-zCDP (see Bun & Steinke (2016)).
3.3 Lower-bounds and minimax optimality
All lower-bounds will be investigated in a minimax sense. Given a class Πof admissible densities, a semi-
norm∥·∥on a space containing the class Π, and a non-decreasing positive function Φsuch that Φ(0) = 0 ,
the minimax risk is defined as
inf
ˆπs.t.Csup
π∈ΠEX∼P⊗n
π,ˆπΦ(∥ˆπ(X)−π∥),
whereCis a condition that must satisfy the estimator (privacy in our case).
General framework. A usual technique for the derivation of minimax lower bounds on the risk uses a
reduction to a testing problem (see Tsybakov (2009)). Indeed, if a family Π′:={π1,...,πm}⊂Πof cardinal
mis an Ω-packing of Π(that is ifi̸=j=⇒ ∥πi−πj∥≥2Ω), then a lower bound is given by
inf
ˆπs.t.Csup
π∈ΠEX∼P⊗n
π,ˆπΦ(∥ˆπ(X)−π∥)
≥Φ(Ω) inf
ˆπs.t.C
Ψ:codom(ˆπ)→{1,...,m}max
i∈{1,...,m}PX∼P⊗n
πi,ˆπ(Ψ (ˆπ(X))̸=i).(5)
For more details, see Duchi et al. (2016); Acharya et al. (2021e); Lalanne et al. (2023a). The right-hand
side characterizes the difficulty of discriminating the distributions of the packing by a statistical test. Inde-
pendently on the condition C, it can be lower-bounded using information-theoretic results such a Le Cam’s
lemma (Rigollet & Hütter, 2015, Lemma 5.3) or Fano’s lemma (Giraud, 2021, Theorem 3.1). When Cis
a local privacy condition, Duchi et al. (2016) provides analogous results that take privacy into account.
Recent work (Acharya et al., 2021e; Kamath et al., 2022; Lalanne et al., 2023a) provides analogous forms for
multiple notions of centralprivacy. When using this technique, finding good lower-bounds on the minimax
risk boils down to finding a packing of densities that are far enough from one another without being too easy
to discriminate with a statistical test.
It is interesting to note that for the considered problem, this technique does not yield satisfying lower-bounds
withρ-zCDP every time Fano’s lemma is involved. Systematically, a small order is lost. To circumvent that
difficulty, we had to adapt Assouad’s technique to the context of ρ-zCDP. Similar ideas have been used
in Duchi et al. (2016) for lower-bounds under localdifferential privacy and in Acharya et al. (2021e) for
regularcentraldifferential privacy. To the best of our knowledge, such a technique has never been used in
the context of central concentrated differential privacy, and is presented in Appendix D. In all the proofs of
the lower-bounds, we systematically presented both approaches whenever there is a quantitative difference.
This difference could be due to small suboptimalities in Fano’s lemma for concentrated differential privacy,
or simply to the use of a suboptimal packing.
6Published in Transactions on Machine Learning Research (08/2023)
3.3.1 Pointwise lower-bound
The first lower-bound that will be investigated is with respect to the pointwise risk. Pointwise, that is to
say givenx0∈[0,1], the performance of the estimator ˆπis measured by how well it approximates πatx0
with the quadratic risk EX∼P⊗n
π,ˆπ/parenleftig
(ˆπ(X)(x0)−π(x0))2/parenrightig
. Technically, it is the easiest since it requires a
"packing" of only two elements, which gives the following lower-bound:
Theorem 2 (Pointwise lower-bound) .There exists CL>0, a positive constant depending only on Lsuch
that, for any x0∈[0,1], there exist n0(x0,L)∈Nandc0(x0,L)>0such that for any n≥n0, and any
α≥c0/n
inf
ˆπs.t.Csup
π∈ΘLip
LEX∼P⊗n
π,ˆπ/parenleftig
(ˆπ(X)(x0)−π(x0))2/parenrightig
≥C−1
Lmax/braceleftig
n−2/3,(nα)−1/bracerightig
, (6)
whereα=ϵwhen the condition Cis theϵ-DP condition and α=√ρwhenCisρ-zCDP.
Proof idea. Letx0∈[0,1]. As explained above, finding a "good" lower-bound can be done by finding and
analyzing a "good" packing of the parameter space. Namely, in this case, we have to find distributions on
[0,1]that have a L-Lipschitz density (w.r.t. Lebesgue’s measure) such that the densities are far from one
another atx0, but such that it is not extremely easy to discriminate them with a statistical test. We propose
to use a packing{Pf,Pg}of two elements where gis the constant function on [0,1](hence Pgis the uniform
distribution) and fdeviates from gby a small triangle centered at x0. The two densities are represented in
Figure 1.
After analyzing various quantities about these densities, such as their distance at x0, their KL divergences
or their TV distance, we leverage Le Cam-type results to conclude.
The full proof can be found in Appendix E.
Additionally, we can notice that, when applied to any fixed x0∈[0,1], Theorem 2 immediately gives the
following corollary for the control in infinite norm :
Corollary 1 (Infinite norm lower-bound) .There exists CL>0, a positive constant depending only on L
such that there exist n0(L)∈Nandc0(L)>0such that for any n≥n0, and anyα≥c0/n
inf
ˆπs.t.Csup
π∈ΘLip
LEX∼P⊗n
π,ˆπ∥ˆπ(X)−π∥2
∞≥C−1
Lmax/braceleftig
n−2/3,(nα)−1/bracerightig
, (7)
whereα=ϵwhen the condition Cis theϵ-DP condition and α=√ρwhenCisρ-zCDP.
On the optimality and on the cost of privacy. Theorem 1, Theorem 2 and Corollary 1 give the
following general result : Under ϵ-DP or under ρ-zCDP, histogram estimators have minimax-optimal rates
of convergence against distributions with Lipschitz densities, for the pointwise risk or the risk in infinite
norm. In particular, in the low privacy regime (“large” α), the usual minimax rate of estimation of n−2
3is
not degraded. This includes the early observations of Wasserman & Zhou (2010) in the case of constant α
(ϵor√ρ). However, in the high privacy regimes (α≪n−1
3), these results prove a systematic degradation
of the estimation. Those regimes are the same as in Barber & Duchi (2014), the metrics on the other hand
are different.
3.3.2 Integrated lower-bound
The lower-bound of Theorem 2 is interesting, but its pointwise (or in infinite norm in the case of Corollary 1)
nature means that much global information is possibly lost. Instead, one can look at the integrated risk
EX∼P⊗n
π,ˆπ∥ˆπ(X)−π∥2
L2. Given Lemma 1 and the fact that we work on probability distributions with a
compact support, upper-bounding this quantity is straightforward.
The lower-bound for the integrated risk is given by :
7Published in Transactions on Machine Learning Research (08/2023)
Theorem 3 (Integrated lower-bound) .There exists CL>0, a positive constant depending only on Lsuch
that, there exist n0(L)∈Nandc0(L)>0such that for any n≥n0, and anyα≥c0/n
inf
ˆπs.t.Csup
π∈ΘLip
LEX∼P⊗n
π,ˆπ∥ˆπ(X)−π∥2
L2≥C−1
Lmax/braceleftig
n−2/3,(nα)−1/bracerightig
whereα=ϵwhenCis theϵ-DP condition, and α=√ρwhenCis theρ-zCDP condition.
Proof idea. If we were to use the same packing (see Figure 1) as in the proof of Theorem 2, the lower-bounds
would not be good. Indeed, moving from the pointwise difference to the L2norm significantly diminishes
the distances in the packing. Instead, we will use the same idea of deviating from a constant function
by triangles, except that we authorize more than one deviation. More specifically, we consider a packing
consisting of densities fω’s where the ω’s are a well-chosen family of {0,1}m(mis fixed in the proof) (Van der
Vaart, 1998). Then, for a given ω∈{0,1}m,fωhas a triangle centered oni
m+1iffwi̸= 0.
We then leverage Fano-type inequalities, and we use Assouad’s method in order to find the announced
lower-bounds.
The full proof is in Appendix F.
Since the lower-bounds of Theorem 3 match the upper-bounds of Theorem 1, we conclude that the corre-
sponding estimators are optimal in terms of minimax rate of convergence.
4 Projection Estimators and Periodic Sobolev Densities
The Lipschitz densities considered in Section 3 are general enough to be applicable in many problems.
However, this level of generality becomes a curse in terms of rate of estimation. Indeed, as we have seen,
the optimal rate of estimation is max/parenleftbig
n−2/3,(nϵ)−1/parenrightbig
. To put it into perspective, for many parametric
estimation procedures, the optimal rate of convergence usually scales as max/parenleftbig
n−1,(nϵ)−2/parenrightbig
(Acharya et al.,
2021e). This section studies the estimation of smoother distributions, for different smoothness levels, at
the cost of generality. In particular, it establishes that the smoother the distribution class is, the closer
the private rate of estimation is to max/parenleftbig
n−1,(nϵ)−2/parenrightbig
. In other words, it means that the more regular the
density is supposed to be, the closer we get to the difficulty of parametric estimation.
When the density of interest πis inL2([0,1]), it is possible to approximate it by projections. Indeed,
L2([0,1])being a separable Hilbert space, there exists a countable orthonormal family (ϕi)i∈N\{0}that is a
Hilbert basis. In particular, if θi:=/integraltext
[0,1]πϕithen
N/summationdisplay
i=1θiϕiL2
−→
N→+∞π.
LetNbe a positive integer, Z1,...,ZNbe independent and identically distributed random variables with
the same distribution as a centered random variable Zhaving a finite variance. Given a dataset X=
(X1,...,Xn), that is also independent of Z1,...,ZN, the (randomized) projection estimator is defined as
ˆπproj(X) =N/summationdisplay
i=1/parenleftbigg
ˆθi+1
nZi/parenrightbigg
ϕiwhere ˆθi:=1
nn/summationdisplay
j=1ϕi(Xj). (8)
The truncation order Nand the random variable Zare tuned later to obtain the desired levels of privacy
and utility. L2([0,1])has many well known Hilbert bases, hence multiple choices for the family (ϕi)i∈N\{0}.
For instance, orthogonal polynomials, wavelets, or the Fourier basis, are often great choices for projection
estimators. Because of the privacy constraint however, it is better to consider a uniformly bounded Hilbert
basis (Wasserman & Zhou, 2010), which is typically not the case with a polynomial or wavelet basis. From
now on, this work will focus on the following Fourier basis :
ϕ1(x) = 1
ϕ2k(x) =√
2 sin (2 πkx)k≥1
ϕ2k+1(x) =√
2 cos (2 πkx)k≥1.
8Published in Transactions on Machine Learning Research (08/2023)
Note that we used the uppernotation πto refer to the real number 3,14..., which is not to be mistaken
for thelowernotationπ, which is reserved for the density of the distribution of interest. This shouldn’t
introduce any ambiguity since πis only used locally when looking at Fourier coefficients, and is often simply
hidden in the constants.
4.1 General utility of projection estimators
By the Parseval formula, the truncation resulting of approximating the density πon a finite family of N
orthonormal functions induces a bias term that accounts for/summationtext
i≥N+1θ2
iin the mean square error. Charac-
terizing the utility of ˆπprojrequires controlling this term, and this is usually done by imposing that πis in
a Sobolev space. We recall the definition given in Tsybakov (2009): given β∈N\{0}andL>0, the class
ΘSob
L,βof Sobolev densities of parameters βandLis defined as
ΘSob
L,β:=

π∈Cβ([0,1],R+)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle

π(β−1)is absolutely continuous ,/integraltext
[0,1]/parenleftbig
π(β)/parenrightbig2≤L2,/integraltext
[0,1]π= 1.

.
For a function f, we used the notation f(β)to refer to its derivative of order β. In addition, the class ΘPSob
L,β
of periodic Sobolev densities of parameters βandLis defined as
ΘPSob
L,β :=/braceleftig
π∈ΘSob
L,β/vextendsingle/vextendsingle/vextendsingle∀j∈{0,...,β−1},π(j)(0) =π(j)(1)/bracerightig
. (9)
Finally,werecallthefollowinggeneral-purposelemma(Tsybakov,2009)thatallowscontrollingthetruncation
bias :
Fact 1(Ellipsoid reformulation (Tsybakov, 2009)) .A non-negative function πwith integral 1belongs to
ΘPSob
L,βif and only if∞/summationdisplay
i=1a2β
iθ2
i≤L2
π2β, whereaj:=jifjis even and aj:=j−1ifjis odd.
In this class, one can characterize the utility of projection estimators with the following lemma:
Lemma 2 (General utility of (8)) .There is a constant CL,β>0, depending only on L,β, such that
sup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπproj∥ˆπproj(X)−π∥2
L2≤CL,β/parenleftbigg1
N2β+N
n+NV(Z)
n2/parenrightbigg
.
The proof can be found in Appendix G
4.2 Privacy and bias tuning
The estimator ˆπproj(X)is a function of the sums/parenleftig/summationtextn
j=1ϕ1(Xj),...,/summationtextn
j=1ϕN(Xj)/parenrightig
. In particular, it is
possible to use Laplace and Gaussian mechanisms on this function in order to obtain privacy. Since the
functions|ϕi|are bounded by√
2for anyi, thel1sensitivity of this function is 2√
2Nand itsl2sensitivity
is2√
2√
N. Applying the Laplace and the Gaussian mechanism and tuning Nto optimize the utility of
Lemma 2 gives the following result:
Theorem 4 (Privacy and utility of (8)) .Given any ϵ >0and truncation order N, using ˆπprojwithZ=
2N√
2
ϵL(1), whereL(1)refers to a random variable following a Laplace distribution of parameter 1, leads to
anϵ-DP procedure. Moreover, there exists CL,β>0, a positive constant that only depends on Landβ, such
that ifNis of the order of min/parenleftig
n1
2β+1,(nϵ)1
β+3/2/parenrightig
,
sup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπproj∥ˆπproj(X)−π∥2
L2≤CL,βmax/braceleftig
n−2β
2β+1,(nϵ)−2β
β+3/2/bracerightig
.
Furthermore, given any ρ >0, and truncation order N, using ˆπprojwithZ=2√
N√ρN(0,1), whereN(0,1)
refers to a random variable following a centered Gaussian distribution of variance 1, leads to a ρ-zCDP
9Published in Transactions on Machine Learning Research (08/2023)
procedure. Moreover, there exists CL,β>0, a positive constant that only depends on Landβ, such that, if
Nis of the order of min/parenleftig
n1
2β+1,/parenleftbig
n√ρ/parenrightbig1
β+1/parenrightig
sup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπproj∥ˆπproj(X)−π∥2
L2≤CL,βmax/braceleftig
n−2β
2β+1,(n√ρ)−2β
β+1/bracerightig
.
We now discuss these guarantees depending on the considered privacy regime.
Low privacy regimes. According to Theorem 4, when the privacy-tuning parameters are not too small
(i.e. when the estimation is not too private), the usual rate of convergence n−2β
2β+1is not degraded. In
particular, for constant ϵorρ, this recovers the results of Wasserman & Zhou (2010).
High privacy regimes. Furthermore, Theorem 4 tells that in high privacy regimes ( ϵ≪n−β−1/2
2β+1or
ρ≪n−2β+2
2β+1), the provable guarantees of the projection estimator are degraded compared to the usual rate
of convergence. Is this degradation constitutive of the estimation problem, or is it due to a suboptimal
upper-bound? Section 4.3 shows that this excess of risk is in fact almost optimal.
4.3 Lower-bounds
AswiththeintegratedriskonLipschitzdistributions, obtaininglower-boundsfortheclassofperiodicSobolev
densities is done by considering a packing with many elements. The idea of the packing is globally the same
as for histograms, except that the uniform density is perturbed with a general C∞kernel with compact
support instead of simple triangles. In the end, we obtain the following result:
Theorem 5 (Integrated lower-bound) .GivenL,β > 0there exists constants CL,β>0,n0(L,β)∈N, and
c0(L,β)>0, such that for any n≥n0, and anyα≥c0/n
inf
ˆπs.t.Csup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπ∥ˆπ(X)−π∥2
L2≥C−1
L,βmax/braceleftig
n−2β
2β+1,(nα)−2β
β+1/bracerightig
whereα=ϵwhenCis theϵ-DP condition, and α=√ρwhenCis theρ-zCDP condition.
Proof idea. As with the proof of Theorem 3, this lower-bound is based on the construction of a packing of
densitiesfω’s where the ω’s are a well-chosen family of {0,1}m(mis fixed in the proof). Then, for a given
ω∈{0,1}m,fωdeviates from a constant function aroundi
m+1if, and only if, wi̸= 0. Contrary to the
proof of Theorem 3 however, the deviation cannot be by a triangle : Indeed, such a function wouldn’t even
be differentiable. Instead, we use a deviation by a C∞kernel with compact support. Even if the complete
details are given in the full proof, Figure 3 gives a general illustration of the packing.
Again, Fano-type inequalities (for the ϵ-DP case), and Assouad’s lemma (for the ρ-zCDP case) are used to
conclude.
The full proof can be found in Appendix H. In comparison with the upper-bounds of Theorem 4, for ϵ-DP
the lower-bound almostmatches the guarantees of the projection estimator. In particular, the excess of risk
in the high privacy regime is close to being optimal. Section 4.4 explains how to bridge the gap even more,
at the cost of relaxation.
Underρ-zCDP, the lower-bounds and upper-bounds actually match. We conclude that projection estimators
withρ-zCDP obtain minimax-optimal rates of convergence.
4.4 Near minimax optimality via relaxation
An hypothesis that we can make on the sub-optimality of the projection estimator against ϵ-DP mechanisms
is that thel1sensitivity of the estimation of NFourier coefficients scales as Nwhereas its l2sensitivity scales
as√
N. Traditionally, the Gaussian mechanism (Dwork et al., 2006a;b) has allowed to use the l2sensitivity
instead of the l1one at the cost of introducing a relaxation term δin the privacy guarantees, leading to
(ϵ,δ)-DP.
10Published in Transactions on Machine Learning Research (08/2023)
A direct application of the Gaussian mechanism Dwork & Roth (2014) thus tells that ˆπprojwithZ=
4√
ln (1.25/δ)√
N
ϵN(0,1)is(ϵ,δ)-DP and, by Lemma 2, has an error bounded as
sup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπproj∥ˆπproj(X)−π∥2
L2≤CL,β/parenleftbigg1
N2β+N
n+16N2ln (1.25/δ)
ϵ2n2/parenrightbigg
.
Thus, choosing Nof the order of min/parenleftigg
n1
2β+1,/parenleftbigg
nϵ√
ln(1.25/δ)/parenrightbigg1
β+1/parenrightigg
leads to a general error as
sup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπproj∥ˆπproj(X)−π∥2
L2≤CL,βmax

n−2β
2β+1,/parenleftigg
nϵ/radicalbig
ln (1.25/δ)/parenrightigg−2β
β+1

.
Finally, it can be interesting to look at prescribed rates for δas a function of n.
Corollary 2 (Privacy and utility of (8) with relaxation) .Considerγ > 0,nand integer, and 0< ϵ≤
8 lnnγ. Defining ˜ρ:=1
16ϵ2
ln(nγ)and using ˆπprojwithZ=2√
N√˜ρN(0,1), whereN(0,1)refers to a random
variable following a centered Gaussian distribution of variance 1, leads to an/parenleftbig
ϵ,1
nγ/parenrightbig
-DP procedure. there
existsCL,β>0, a positive constant that only depends on Landβ, such that if Nis of the order of
min/parenleftbigg
n1
2β+1,/parenleftig
n√
lnn·ϵ√γ/parenrightig1
β+1/parenrightbigg
then
sup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπproj∥ˆπproj(X)−π∥2
L2≤CL,βmax/braceleftig
n−2β
2β+1,Pβ,γ(ln(n))(nϵ)−2β
β+1/bracerightig
,
wherePβ,γis a polynomial expression depending on βandγ.
Proof.Sinceϵ≤8 ln (nγ), we have ˜ρ≤2/radicalbig
˜ρln(nγ). By Theorem 4 the mechanism is ˜ρ-zCDP, and satisfies
the claimed upper bounds for Non the considered order. By Bun & Steinke (2016) (that states that if a
mechanism Misρ-zCDP, then it is/parenleftig
ρ+ 2/radicalbig
ρln(1/δ),δ/parenrightig
-DP for any δ >0) it is thus/parenleftig
4/radicalbig
˜ρln(nγ),1
nγ/parenrightig
-
DP.
In order to understand the implications of this result, one must understand the role of δin(ϵ,δ)-differential
privacy. It is usually interpreted as the probability of the procedure not respecting the ϵ-DP condition
(Dwork & Roth, 2014). Hence, with probability δ, the result is not guaranteed to be private. A general rule
of thumb for choosing δis to take it much smaller than 1/nso that each individual of the database only
has a small chance of seeing its data leak (Dwork & Roth, 2014). Choosing δ= 1/nγforγ > 1is hence
considered a good choice for δ.
With this relaxation, the upper-bound of Corollary 2 matches the lower-bound of Theorem 5 for ϵ-DP up to
polylog factors.
5 Conclusion
As we have seen throughout this article, under central privacy, one can usually distinguish two estimation
regimes. In the lowprivacy regime, on the one hand, the estimation rate is not degraded compared to its
non-private counterpart. This notably covers the early observation of Wasserman & Zhou (2010) for constant
privacy budget. In the highprivacy regime on the other hand, a provable degradation is unavoidable, and
we extended the study of such regimes beyond the cases covered in Barber & Duchi (2014).
Besides examples in which the estimation is sharp in both regimes, we also presented some example in which
there are smallgaps between the proved upper-bounds and lower-bounds. These gaps are nevertheless very
small, especially for high degrees of smoothness, and they can be bridged up to logarithmic factors with a
reasonable and quite standard relaxation.
11Published in Transactions on Machine Learning Research (08/2023)
Acknowledgement
Aurélien Garivier acknowledges the support of the Project IDEXLYON of the University of Lyon, in the
frameworkoftheProgrammeInvestissementsd’Avenir(ANR-16-IDEX-0005), andChaireSeqALO(ANR-20-
CHIA-0020-01). This project was supported in part by the AllegroAssai ANR project ANR-19-CHIA-0009.
Additionally, we thank the anonymous reviewers for their precious inputs and suggestions.
References
Martín Abadi, Andy Chu, Ian J. Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Edgar R. Weippl, Stefan Katzenbeisser, Christopher
Kruegel, Andrew C. Myers, and Shai Halevi (eds.), Proceedings of the 2016 ACM SIGSAC Conference
on Computer and Communications Security, Vienna, Austria, October 24-28, 2016 , pp. 308–318. ACM,
2016. doi: 10.1145/2976749.2978318. URL https://doi.org/10.1145/2976749.2978318 .
John M Abowd. The us census bureau adopts differential privacy. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining , pp. 2867–2867, 2018.
Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private testing of identity and closeness of
discrete distributions. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò
Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: An-
nual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montréal, Canada , pp. 6879–6891, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/
7de32147a4f1055bed9e4faf3485a84d-Abstract.html .
Jayadev Acharya, Clement Canonne, AdityaVikram Singh, and HimanshuTyagi. Optimalrates for nonpara-
metric density estimation under communication constraints. In M. Ranzato, A. Beygelzimer, Y. Dauphin,
P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , vol-
ume 34, pp. 26754–26766. Curran Associates, Inc., 2021a. URL https://proceedings.neurips.cc/
paper_files/paper/2021/file/e1021d43911ca2c1845910d84f40aeae-Paper.pdf .
JayadevAcharya, ClémentL.Canonne, PrathameshMayekar, andHimanshuTyagi. Information-constrained
optimization: can adaptive processing of gradients help? CoRR, abs/2104.00979, 2021b. URL https:
//arxiv.org/abs/2104.00979 .
Jayadev Acharya, Clément L. Canonne, Zuteng Sun, and Himanshu Tyagi. Unified lower bounds for inter-
active high-dimensional estimation under information constraints. CoRR, abs/2010.06562, 2021c. URL
https://arxiv.org/abs/2010.06562 .
Jayadev Acharya, Clément L. Canonne, Cody Freitag, Ziteng Sun, and Himanshu Tyagi. Inference under
information constraints iii: Local privacy constraints. IEEE Journal on Selected Areas in Information The-
ory, 2(1):253–267, 2021d. doi: 10.1109/JSAIT.2021.3053569. URL https://doi.org/10.1109/JSAIT.
2021.3053569 .
Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private assouad, fano, and le cam. In
Vitaly Feldman, Katrina Ligett, and Sivan Sabato (eds.), Algorithmic Learning Theory, 16-19 March
2021, Virtual Conference, Worldwide , volume 132 of Proceedings of Machine Learning Research , pp. 48–
78. PMLR, 2021e. URL http://proceedings.mlr.press/v132/acharya21a.html .
Hilal Asi and John C. Duchi. Near instance-optimality in differential privacy. CoRR, abs/2005.10630, 2020a.
URL https://arxiv.org/abs/2005.10630 .
Hilal Asi and John C. Duchi. Instance-optimality in differential privacy via approximate inverse sensitivity
mechanisms. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020b. URL https://
proceedings.neurips.cc/paper/2020/hash/a267f936e54d7c10a2bb70dbe6ad7a89-Abstract.html .
12Published in Transactions on Machine Learning Research (08/2023)
Hilal Asi, Jonathan R. Ullman, and Lydia Zakynthinou. From robustness to privacy and back. CoRR,
abs/2302.01855, 2023. doi: 10.48550/arXiv.2302.01855. URL https://doi.org/10.48550/arXiv.2302.
01855.
Lars Backstrom, Cynthia Dwork, and Jon M. Kleinberg. Wherefore art thou r3579x?: anonymized social
networks, hidden patterns, and structural steganography. In Carey L. Williamson, Mary Ellen Zurko,
Peter F. Patel-Schneider, and Prashant J. Shenoy (eds.), Proceedings of the 16th International Conference
on World Wide Web, WWW 2007, Banff, Alberta, Canada, May 8-12, 2007 , pp. 181–190. ACM, 2007.
doi: 10.1145/1242572.1242598. URL https://doi.org/10.1145/1242572.1242598 .
Rina Foygel Barber and John C. Duchi. Privacy and statistical risk: Formalisms and minimax bounds, 2014.
Leighton Pate Barnes, Yanjun Han, and Ayfer Ozgur. Fisher information for distributed estimation under
a blackboard communication protocol. In 2019 IEEE International Symposium on Information Theory
(ISIT), pp. 2704–2708, 2019. doi: 10.1109/ISIT.2019.8849821.
Leighton Pate Barnes, Yanjun Han, and Ayfer Özgür. Lower bounds for learning distributions under com-
munication constraints via fisher information. Journal of Machine Learning Research , 21:Paper No. 236,
30, 2020. ISSN 1532-4435. URL https://jmlr.csail.mit.edu/papers/volume21/19-737/19-737.pdf .
Thomas B. Berrett, László Györfi, and Harro Walk. Strongly universally consistent nonparametric regression
and classification with privatised data. Electronic Journal of Statistics , 15(1):2430 – 2453, 2021. doi:
10.1214/21-EJS1845. URL https://doi.org/10.1214/21-EJS1845 .
Sourav Biswas, Yihe Dong, Gautam Kamath, and Jonathan R. Ullman. Coinpress: Practical pri-
vate mean and covariance estimation. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Had-
sell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
a684eceee76fc522773286a895bc8436-Abstract.html .
Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and lower
bounds. In Martin Hirt and Adam D. Smith (eds.), Theory of Cryptography - 14th International Con-
ference, TCC 2016-B, Beijing, China, October 31 - November 3, 2016, Proceedings, Part I , volume 9985
ofLecture Notes in Computer Science , pp. 635–658, 2016. doi: 10.1007/978-3-662-53641-4\_24. URL
https://doi.org/10.1007/978-3-662-53641-4_24 .
Cristina Butucea, Amandine Dubois, Martin Kroll, and Adrien Saumard. Local differential privacy: Elbow
effect in optimal density estimation and adaptation over besov ellipsoids. CoRR, abs/1903.01927, 2019.
URL http://arxiv.org/abs/1903.01927 .
Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data privately. In
Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N.
Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30:
Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA , pp. 3571–3580, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
253614bbac999b38b5b60cae531c4969-Abstract.html .
Irit Dinur and Kobbi Nissim. Revealing information while preserving privacy. In Frank Neven, Catriel Beeri,
and Tova Milo (eds.), Proceedings of the Twenty-Second ACM SIGACT-SIGMOD-SIGART Symposium
on Principles of Database Systems, June 9-12, 2003, San Diego, CA, USA , pp. 202–210. ACM, 2003. doi:
10.1145/773153.773173. URL https://doi.org/10.1145/773153.773173 .
John C. Duchi, Michael I. Jordan, and Martin J. Wainwright. Local privacy and statistical minimax rates.
In51st Annual Allerton Conference on Communication, Control, and Computing, Allerton 2013, Allerton
Park & Retreat Center, Monticello, IL, USA, October 2-4, 2013 , pp. 1592. IEEE, 2013. doi: 10.1109/
Allerton.2013.6736718. URL https://doi.org/10.1109/Allerton.2013.6736718 .
13Published in Transactions on Machine Learning Research (08/2023)
John C. Duchi, Michael I. Jordan, and Martin J. Wainwright. Local privacy, data processing inequalities,
and statistical minimax rates, 2014. URL https://arxiv.org/abs/1302.3203 .
John C. Duchi, Martin J. Wainwright, and Michael I. Jordan. Minimax optimal procedures for locally private
estimation. CoRR, abs/1604.02390, 2016. URL http://arxiv.org/abs/1604.02390 .
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Found. Trends
Theor. Comput. Sci. , 9(3-4):211–407, 2014. doi: 10.1561/0400000042. URL https://doi.org/10.1561/
0400000042 .
Cynthia Dwork and Guy N Rothblum. Concentrated differential privacy. arXiv preprint arXiv:1603.01887 ,
2016.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, our-
selves: Privacy via distributed noise generation. In Serge Vaudenay (ed.), Advances in Cryptology -
EUROCRYPT 2006, 25th Annual International Conference on the Theory and Applications of Crypto-
graphic Techniques, St. Petersburg, Russia, May 28 - June 1, 2006, Proceedings , volume 4004 of Lec-
ture Notes in Computer Science , pp. 486–503. Springer, 2006a. doi: 10.1007/11761679\_29. URL
https://doi.org/10.1007/11761679_29 .
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivity in
private data analysis. In Shai Halevi and Tal Rabin (eds.), Theory of Cryptography, Third Theory of
Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006, Proceedings , volume 3876
ofLecture Notes in Computer Science , pp. 265–284. Springer, 2006b. doi: 10.1007/11681878\_14. URL
https://doi.org/10.1007/11681878_14 .
Úlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: randomized aggregatable privacy-
preserving ordinal response. In Gail-Joon Ahn, Moti Yung, and Ninghui Li (eds.), Proceedings of the 2014
ACM SIGSAC Conference on Computer and Communications Security, Scottsdale, AZ, USA, November
3-7, 2014 , pp. 1054–1067. ACM, 2014. doi: 10.1145/2660267.2660348. URL https://doi.org/10.1145/
2660267.2660348 .
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence
information and basic countermeasures. In Indrajit Ray, Ninghui Li, and Christopher Kruegel (eds.),
Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, Denver,
CO, USA, October 12-16, 2015 , pp. 1322–1333. ACM, 2015. doi: 10.1145/2810103.2813677. URL https:
//doi.org/10.1145/2810103.2813677 .
Christophe Giraud. Introduction to high-dimensional statistics . Chapman and Hall/CRC, 2021. ISBN
9781003158745. doi: 10.1201/9781003158745.
Antoine Gonon, Léon Zheng, Clément Lalanne, Quoc-Tung Le, Guillaume Lauga, and Can Pouliquen.
Sparsity in neural networks can improve their privacy, 2023.
László Györfi and Martin Kroll. On rate optimal private regression under local differential privacy. arXiv
preprint arXiv:2206.00114 , 2022.
László Györfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A Distribution-Free Theory of Nonparamet-
ric Regression . Springer series in statistics. Springer, 2002. ISBN 978-0-387-95441-7. doi: 10.1007/b97848.
URL https://doi.org/10.1007/b97848 .
László Györfi and Martin Kroll. Multivariate density estimation from privatised data: universal consistency
and minimax rates. Journal of Nonparametric Statistics , 0(0):1–23, 2023. doi: 10.1080/10485252.2022.
2163634. URL https://doi.org/10.1080/10485252.2022.2163634 .
Rob Hall, Alessandro Rinaldo, and Larry A. Wasserman. Differential privacy for functions and functional
data.J. Mach. Learn. Res. , 14(1):703–727, 2013. doi: 10.5555/2567709.2502603. URL https://dl.acm.
org/doi/10.5555/2567709.2502603 .
14Published in Transactions on Machine Learning Research (08/2023)
Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill Muehling, John V
Pearson, Dietrich A Stephan, Stanley F Nelson, and David W Craig. Resolving individuals contributing
trace amounts of dna to highly complex mixtures using high-density snp genotyping microarrays. PLoS
Genet, 4(8):e1000167, 2008.
Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan R. Ullman. Privately learning high-dimensional
distributions. In Alina Beygelzimer and Daniel Hsu (eds.), Conference on Learning Theory, COLT 2019,
25-28 June 2019, Phoenix, AZ, USA , volume 99 of Proceedings of Machine Learning Research , pp. 1853–
1902. PMLR, 2019. URL http://proceedings.mlr.press/v99/kamath19a.html .
Gautam Kamath, Xingtu Liu, and Huanyu Zhang. Improved rates for differentially private stochastic convex
optimizationwithheavy-taileddata. InKamalikaChaudhuri, StefanieJegelka, LeSong, CsabaSzepesvári,
Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23
July 2022, Baltimore, Maryland, USA , volume 162 of Proceedings of Machine Learning Research , pp.
10633–10660. PMLR, 2022. URL https://proceedings.mlr.press/v162/kamath22a.html .
Gautam Kamath, Argyris Mouzakis, Matthew Regehr, Vikrant Singhal, Thomas Steinke, and Jonathan
Ullman. A bias-variance-privacy trilemma for statistical estimation, 2023.
Vishesh Karwa and Salil P. Vadhan. Finite sample differentially private confidence intervals. In Anna R.
Karlin (ed.), 9th Innovations in Theoretical Computer Science Conference, ITCS 2018, January 11-14,
2018, Cambridge, MA, USA , volume 94 of LIPIcs, pp. 44:1–44:9. Schloss Dagstuhl - Leibniz-Zentrum für
Informatik, 2018. doi: 10.4230/LIPIcs.ITCS.2018.44. URL https://doi.org/10.4230/LIPIcs.ITCS.
2018.44.
Martin Kroll. On density estimation at a fixed point under local differential privacy. Electronic Jour-
nal of Statistics , 15(1):1783 – 1813, 2021. doi: 10.1214/21-EJS1830. URL https://doi.org/10.1214/
21-EJS1830 .
Clément Lalanne, Clément Gastaud, Nicolas Grislain, Aurélien Garivier, and Rémi Gribonval. Private
quantiles estimation in the presence of atoms. CoRR, abs/2202.08969, 2022. URL https://arxiv.org/
abs/2202.08969 .
Clément Lalanne, Aurélien Garivier, and Rémi Gribonval. On the Statistical Complexity of Estimation and
Testing under Privacy Constraints. Transactions on Machine Learning Research Journal , April 2023a.
URL https://hal.science/hal-03794374v2 .
Clément Lalanne, Aurélien Garivier, and Rémi Gribonval. Private Statistical Estimation of Many Quantiles.
InICML 2023 - 40th International Conference on Machine Learning , Honolulu, United States, July 2023b.
URL https://hal.science/hal-03986170 .
Joseph Lam-Weil, Béatrice Laurent, and Jean-Michel Loubes. Minimax optimal goodness-of-fit testing for
densities and multinomials under a local differential privacy constraint. Bernoulli , 28(1):579–600, 2022.
Grigorios Loukides, Joshua C. Denny, and Bradley A. Malin. The disclosure of diagnosis codes can breach
research participants’ privacy. J. Am. Medical Informatics Assoc. , 17(3):322–327, 2010. doi: 10.1136/
jamia.2009.002725. URL https://doi.org/10.1136/jamia.2009.002725 .
Ilya Mironov. Rényi differential privacy. In 30th IEEE Computer Security Foundations Symposium, CSF
2017, Santa Barbara, CA, USA, August 21-25, 2017 , pp. 263–275. IEEE Computer Society, 2017. doi:
10.1109/CSF.2017.11. URL https://doi.org/10.1109/CSF.2017.11 .
Arvind Narayanan and Vitaly Shmatikov. How to break anonymity of the netflix prize dataset. CoRR,
abs/cs/0610105, 2006. URL http://arxiv.org/abs/cs/0610105 .
Arvind Narayanan and Vitaly Shmatikov. Robust de-anonymization of large sparse datasets. In 2008 IEEE
Symposium on Security and Privacy (S&P 2008), 18-21 May 2008, Oakland, California, USA ,pp.111–125.
IEEE Computer Society, 2008. doi: 10.1109/SP.2008.33. URL https://doi.org/10.1109/SP.2008.33 .
15Published in Transactions on Machine Learning Research (08/2023)
Phillippe Rigollet and Jan-Christian Hütter. High dimensional statistics. MIT lecture notes for course
18S997, 2015. URL https://math.mit.edu/~rigollet/PDFs/RigNotes17.pdf .
Sandra Schluttenhofer and Jan Johannes. Adaptive pointwise density estimation under local differential
privacy, 2022.
Vikrant Singhal. A polynomial time, pure differentially private estimator for binary product distributions,
2023.
Latanya Sweeney. Simple demographics often identify people uniquely. Health (San Francisco) , 671(2000):
1–34, 2000.
Latanya Sweeney. k-anonymity: A model for protecting privacy. Int. J. Uncertain. Fuzziness Knowl.
Based Syst. , 10(5):557–570, 2002. doi: 10.1142/S0218488502001648. URL https://doi.org/10.1142/
S0218488502001648 .
Abhradeep Guha Thakurta, Andrew H Vyrros, Umesh S Vaishampayan, Gaurav Kapoor, Julien Freudiger,
Vivek Rangarajan Sridhar, and Doug Davidson. Learning new words. Granted US Patents , 9594741, 2017.
Alexandre B. Tsybakov. Introduction to Nonparametric Estimation . Springer series in statistics. Springer,
2009. ISBN 978-0-387-79051-0. doi: 10.1007/b13794. URL https://doi.org/10.1007/b13794 .
A. W. Van der Vaart. Asymptotic Statistics . Cambridge Series in Statistical and Probabilistic Mathematics.
Cambridge University Press, 1998. doi: 10.1017/CBO9780511802256.
Tim van Erven and Peter Harremoës. Rényi divergence and kullback-leibler divergence. IEEE Trans. Inf.
Theory, 60(7):3797–3820, 2014. doi: 10.1109/TIT.2014.2320500. URL https://doi.org/10.1109/TIT.
2014.2320500 .
Isabel Wagner and David Eckhoff. Technical privacy metrics: A systematic survey. ACM Comput. Surv. , 51
(3):57:1–57:38, 2018. doi: 10.1145/3168389. URL https://doi.org/10.1145/3168389 .
Larry A. Wasserman and Shuheng Zhou. A statistical framework for differential privacy. Journal of the
American Statistical Association , 105(489):375–389, 2010. doi: 10.1198/jasa.2009.tm08651. URL https:
//doi.org/10.1198/jasa.2009.tm08651 .
16Published in Transactions on Machine Learning Research (08/2023)
A Useful results from the litterature
Fact 2(Neyman-Pearson & Le Cam’s lemma (Rigollet & Hütter, 2015, Lemma 5.3)) .LetP1,
P2be two probability distributions on a measure space E, then
inf
Ψ:E→{1,2}max
i∈{1,2}PX∼Pi(Ψ (X)̸=i)≥1
2inf
Ψ:E→{1,2}2/summationdisplay
i=1PX∼Pi(Ψ (X)̸=i)
=1
2(1−TV (P1,P2)).(10)
Fact 3(Fano’slemma(Giraud,2021, Theorem3.1)) .Let(Pi)i∈{1,...,N}be a family of probability distributions
on a measure space E. For any probability distribution QonEsuch that Pi≪Qfor alli, and for any test
function Ψ :Xn→{1,...,N},
max
i∈{1,...,N}PX∼Pi(Ψ (X)̸=i)≥1
NN/summationdisplay
i=1PX∼Pi(Ψ (X)̸=i)
≥1−1 +1
N/summationtextN
i=1KL (Pi∥Q)
ln(N).(11)
Often Qis set to1
N/summationtextN
i=1Pi.
Fact 4 (Le Cam’s lemma for differential privacy (Lalanne et al., 2023a, Theorem 1)) .If a randomized
mechanism Msatisfies (ϵ,δ)-DP, then for any test function Ψ : codom ( M)→{1,2}and any probability
distributions P1andP2onXwe have
max
i∈{1,2}PX∼P⊗n
i,M(Ψ (M(X))̸=i)
≥1
2/parenleftbig/parenleftbig
1−/parenleftbig
1−e−ϵ/parenrightbig
TV (P1,P2)/parenrightbign−2ne−ϵδTV (P1,P2)/parenrightbig
.
Fact 5. Le Cam’s lemma for concentrated differential privacy (Lalanne et al., 2023a, Theorem 2)] If a
randomized mechanism Msatisfiesρ-zCDP, then for any test function Ψ : codom ( M)→{1,...,N}and
any probability distributions P1andP2onX,
max
i∈{1,2}PX∼P⊗n
i,M(Ψ (M(X))̸=i)≥1
2/parenleftig
1−n/radicalbig
ρ/2TV ( P1,P2)/parenrightig
.
Fact 6(Fano’s lemma for differential privacy (Lalanne et al., 2023a, Theorem 3)) .If a randomized mech-
anismMsatisfiesϵ-DP, then for any test function Ψ : codom ( M)→{1,...,N}, any family of probability
distributions (Pi)i∈{1,...,N}onX,
max
i∈{1,...,N}PX∼P⊗n
i,M(Ψ (M(X))̸=i)≥1−1 +nϵ
N2/summationtext
i,j2TV(Pi,Pj)
1+TV( Pi,Pj)
ln(N).
Fact 7(Fano’s lemma for differential privacy (Lalanne et al., 2023a, Theorem 4)) .If a randomized mech-
anismMsatisfiesϵ-DP, then for any test function Ψ : codom ( M)→{1,...,N}, any family of probability
distributions (Pi)i∈{1,...,N}onX,
max
i∈{1,...,N}PX∼P⊗n
i,M(Ψ (M(X))̸=i)≥1−1 +n2ρ
N2/summationtext
i,j1
n2TV(Pi,Pj)
1+TV( Pi,Pj)+/parenleftig
2TV(Pi,Pj)
1+TV( Pi,Pj)/parenrightig2
ln(N).
17Published in Transactions on Machine Learning Research (08/2023)
x0 0Slope−L SlopeL
11
Tuneable deviation supporty=g(x)
y=f(x)
xy
Figure 1: Packing for Theorem 2
i
m+1whenωi= 1 0Slope−L SlopeL
11
Tuneable deviation supporty=fω(x)
xy
j
m+1whenωj= 0
Figure 2: Packing for Theorem 3
B Figures
C Proof of Lemma 1
Letπ∈ΘLip
L,x0∈[0,1]. The classical bias-variance decomposition gives that
E/parenleftig/parenleftbig
ˆπhist(X)(x0)−π(x0)/parenrightbig2/parenrightig
=/parenleftbig
E/parenleftbig
ˆπhist(X)(x0)/parenrightbig
−π(x0)/parenrightbig2+V/parenleftbig
ˆπhist(X)(x0)/parenrightbig
.
For anyx∈[0,1], we note bin (x)the bin of the histogram in which xfalls into. Notice that, for any
x0∈[0,1]and any integer i, the random variable 1bin(x0)(Xi)follows a Bernoulli distribution of probability
of success/integraltext
bin(x0)π. Let us first study the bias, using the definition (3) of ˆπhist
/vextendsingle/vextendsingleE/parenleftbig
ˆπhist(X)(x0)/parenrightbig
−π(x0)/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nhn/summationdisplay
i=1E/parenleftbig
1bin(x0)(Xi)/parenrightbig
−π(x0)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/integraltext
bin(x0)π(x)dx
nh−π(x0)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=1
h/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
bin(x0)(π(x)−π(x0))dx/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤1
h/integraldisplay
bin(x0)|π(x)−π(x0)|dx≤L
h/integraldisplay
bin(x0)|x−x0|dx≤Lh
2.
18Published in Transactions on Machine Learning Research (08/2023)
i
m+1whenωi= 1 0Derivative of order β
with bounded energyC∞
11
Tuneable deviation supporty=fω(x)
xy
j
m+1whenωj= 0
Figure 3: Packing for Theorem 5
Let us now look at the variance. By independence of Xi’s andZj’s,
V/parenleftbig
ˆπhist(X)(x0)/parenrightbig
=1
n2h2/parenleftiggn/summationdisplay
i=1V/parenleftbig
1bin(x0)(Xi)/parenrightbig
+V/parenleftbig
Zbin(x0)/parenrightbig/parenrightigg
=1
n2h2/parenleftigg
n/parenleftigg/integraldisplay
bin(x0)π/parenrightigg/parenleftigg
1−/integraldisplay
bin(x0)π/parenrightigg
+V(Z)/parenrightigg
≤1
nh2/parenleftigg/integraldisplay
bin(x0)π/parenrightigg
+V(Z)
n2h2.
SinceπisL-Lipschitz on [0,1]and has to integrate to 1(because it is a density), πis uniformly bounded
from above by L+ 1on[0,1]. Hence,/integraltext
bin(x0)π≤(L+ 1)hand the result follows.
D Assouad’s lemma with concentrated differential privacy.
As the reduction to a testing problem between multiple hypotheses, Assouad’s lemma relies on similar ideas,
where the packing has to be parametrized by a hypercube. Its advantage over tools like Fano’s lemma is
that it only makes tests between pairs of hypotheses (instead of all of them at the same time). The cost of
this is that the control of the packing is slightly more difficult.
Suppose that the set of distributions of interest Pcontains a family of distributions (Pω)ω∈{0,1}mfor a certain
positive integer m. If the loss function (taken quadratic for simplicity) can be decomposed as
∀ω,ω′∈{0,1}m,∥fω−fω′∥2
L2≥2τm/summationdisplay
i=11ωi̸=ω′
i= 2τdham(ω,ω′), (12)
where for any ω,fωrepresents the density of Pω, then the minimax risk can be lower-bounded as (the proof
is classical and can be found in Acharya et al. (2021e, Section 5.4))
inf
ˆπs.t.Csup
P∈PEX∼P,ˆπ(∥ˆπ(X)−π∥2
L2)
≥τ
16m/summationdisplay
i=1inf
Ms.t.C
Ψ:codom( M)→{0,1}PX∼P⊗n
ωi,0,M(Ψ (M(X))̸= 0) + PX∼P⊗n
ωi,1,M(Ψ (M(X))̸= 1).(13)
where Pωi,0andPωi,1are themixturedistributions
Pωi,0:=1
2m−1/summationdisplay
ω∈{0,1}m|ωi=0Pωand Pωi,0:=1
2m−1/summationdisplay
ω∈{0,1}m|ωi=1Pω. (14)
19Published in Transactions on Machine Learning Research (08/2023)
The term
PX∼P⊗n
ωi,0,M(Ψ (M(X))̸= 0) + PX∼P⊗n
ωi,1,M(Ψ (M(X))̸= 1)
characterizes the testingdifficulty between Pωi,0andPωi,1. It can be controlled by Le Cam’s lemma, and by
its variants when working under privacy (see Acharya et al. (2021e); Lalanne et al. (2023a) for differential
privacy and Lalanne et al. (2023a) for concentrated differential privacy). Such results are reminded in
Appendix A.
E Proof of Theorem 2
Letx0∈(0,1). As explained in the sketch of the proof, we build a packing consisting of two elements, and
after controlling quantities such as their KL divergences or their TV distances, we leverage Le Cam-type
inequalities in order to obtain lower-bounds.
Packing construction. We define the functions fL,x0,h,∀h>0as
∀x∈[0,1], fL,x0,h(x):=

1−Lh2ifx∈[0,x0−h)∪[x0+h,1],
1−Lh2+Lh+L(x−x0)ifx∈[x0−h,x0).
1−Lh2+Lh−L(x−x0)ifx∈[x0,x0+h)(15)
Note that as soon as h≤min{x0,1−x0},fL,x0,h∈ΘLip
L. The case x0∈{0,1}is treated in the exact same
fashion, but by considering functions that only contain "half of a spike" centered on x0. Furthermore, let us
notegthe function that is constant to 1on[0,1](we haveg∈ΘLip
L).
We start by recalling the total variation distance between two probability distributions, and we recall some
useful alternative expressions that are used in the proofs of this article. Given (U,T)a setUequipped with
aσ-algebraT, and two probability measures P1andP2two probability distributions on U, and compatible
withT, the total variation distance TV (·,·)between P1andP2is defined as
TV (P1,P2):= sup
S∈T|P1(S)−P2(S)|.
Furthermore, when P1,P2are dominated by a common σ-finite measure µon(U,T), by noting p1:=dP1
dµ
andp2:=dP2
dµ, the Radon-Nikodym derivatives of P1andP2with respect to µ, the following alternative
expressions to the total variation can be useful :
TV (P1,P2):= sup
S∈T|P1(S)−P2(S)|=P1({p1>p2})−P2({p1>p2})
=/integraldisplay
{p1>p2}(p1−p2)dµ=/integraldisplay
{p2≥p1}(p2−p1)dµ
=1
2/integraldisplay
U|p1−p2|dµ= 1−/integraldisplay
Umin(p1,p2)dµ.
These expressions simply come from considering the events {p1>p2}and{p2≥p1}that form a partition
ofU, and from the relation |a−b|=a+b−2 min(a,b)for any real numbers aandb.
Jumping back to our original proof, when fL,x0,h∈ΘLip
L, we can compute the total variation between PfL,x0,h
andPgthe distributions of probability with densities fL,x0,handgwith respect to Lebesgue’s measure on
[0,1],
TV/parenleftbig
PfL,x0,h,Pg/parenrightbig
= 1−/integraldisplay
[0,1]min (fL,x0,h,g)Constant part
≤ 1−/integraldisplay
[0,1]1−Lh2dx=Lh2.(16)
Anotherimportantmeasureofdiscrepancybetweenprobabilitydistributionsistheso-calledKullback-Leibler
(KL) divergence. For two probability distributions PandQsuch that P≪Q(absolute continuity), it is
defined as
KL (P∥Q) =/integraldisplay
ln/parenleftbiggdP
dQ/parenrightbigg
dP.
20Published in Transactions on Machine Learning Research (08/2023)
Back to our problem, for hin a neighborhood of 0, we also have the following Taylor expansion on their KL
divergence:
KL/parenleftbig
Pg∥PfL,x0,h/parenrightbig
=/integraldisplay
[0,1]ln/parenleftbiggg
fL,x0,h/parenrightbigg
g= ln/parenleftbigg1
1−Lh2/parenrightbigg
(1−2h) + 2/integraldisplayh
0ln/parenleftbigg1
1−Lh2+Lt/parenrightbigg
dt
≤C/parenleftbig
h3+O(h4)/parenrightbig
,(17)
whereCis a positive constant depending only on LtheOonly hides constant factors. Furthermore,
|g(x0)−fL,x0,h(x0)|=L|h2−h|and{g,fL,x0,h}is thus aL
2|h2−h|packing of ΘLip
Lw.r.t the seminorm
f,g∝⇕⊣√∫⊔≀→∥f−g∥:=|f(x0)−g(x0)|.
Recovering the usual lower-bound By the classical minimax reduction as hypothesis testing Equa-
tion (5),
inf
ˆπs.t.Csup
π∈ΘLip
LEX∼P⊗n
π,ˆπ/parenleftig
(ˆπ(X)(x0)−π(x0))2/parenrightig
≥L2
4/parenleftbig
h2−h/parenrightbig2inf
ˆπs.t.Cinf
Ψ:ΘLip
L→{0,1}max/braceleftig
PX∼P⊗n
g,ˆπ(Ψ(ˆπ(X))̸= 0),
PX∼P⊗n
fL,x0,h,ˆπ(Ψ(ˆπ(X))̸= 1)/bracerightbigg
Fact 2
≥L2
8h2(1−h)2/parenleftig
1−TV/parenleftig
P⊗n
g,P⊗n
fL,x0,h/parenrightig/parenrightig
Pinsker
≥L2
8h2(1−h)2/parenleftigg
1−/radicalbigg
KL/parenleftig
P⊗ng/vextenddouble/vextenddoubleP⊗n
fL,x0,h/parenrightig
/2/parenrightigg
Tensorization=L2
8h2(1−h)2/parenleftbigg
1−/radicalig
nKL/parenleftbig
Pg∥PfL,x0,h/parenrightbig
/2/parenrightbigg
(17)
≥L2
8h2(1−h)2/parenleftigg
1−/radicaligg
n
2/parenleftbiggh3L2
3+O(h4)/parenrightbigg/parenrightigg
.(18)
The second inequality comes from the so-called Le Cam’s lemma Rigollet & Hütter (2015) that lower-bounds
thetestingdifficulty(withoutfurtherconstraints)betweentwodistributions. Thenextinequalitycomesfrom
the so-called Pinsker’s inequality Tsybakov (2009), that states that for two probability distributions Pand
Q,TV (P,Q)≤/radicalbig
KL (P∥Q)/2. The last inequality is the result of the so-called tensorization property of
the KL divergence that states that for two probability distributions PandQ, and for an integer n≥1,
KL (P⊗n∥Q⊗n)≤nKL (P∥Q).
When possible (i.e. when nis big enough), setting h=/parenleftbig1
4nL2/parenrightbig1/3leads to, for nbig enough (so that
1−h≥1/2and|O(h4)|≤h3L2
3),
inf
ˆπs.t.Csup
π∈ΘLip
LEX∼P⊗n
π,ˆπ/parenleftig
(ˆπ(X)(x0)−π(x0))2/parenrightig
≥L2
64/parenleftbigg1
4L2/parenrightbigg2/3
n−2/3.
This implies the first lower bound.
ϵ-DP overhead. ByEquation(18)andbyLeCam’slemmafordifferentialprivacyonproductdistributions
(Fact 4),
inf
ˆπ ϵ-DPsup
π∈ΘLip
LEX∼P⊗n
π,ˆπ/parenleftig
(ˆπ(X)(x0)−π(x0))2/parenrightig
≥L2
8h2(1−h)2e−nϵTV/parenleftbig
PfL,x0,h,Pg/parenrightbig
(16)
≥L2
8h2(1−h)2e−Lnϵh2.
21Published in Transactions on Machine Learning Research (08/2023)
When possible (i.e. when nϵis big enough), setting h= 1/√nϵleads to, when nϵis large enough to ensure
1−h≥1/2,
inf
ˆπ ϵ-DPsup
π∈ΘLip
LEX∼P⊗n
π,ˆπ/parenleftig
(ˆπ(X)(x0)−π(x0))2/parenrightig
≥L2e−L
32(nϵ)−1.
ρ-zCDP overhead. ByLeCam’slemmaforzero-concentrateddifferentialprivacyonproductdistributions
(Fact 5) in (18),
inf
ˆπ ϵ-DPsup
π∈ΘLip
LEX∼P⊗n
π,ˆπ/parenleftig
(ˆπ(X)(x0)−π(x0))2/parenrightig
≥L2
8h2(1−h)2/parenleftig
1−n/radicalbig
ρ/2TV/parenleftbig
PfL,x0,h,Pg/parenrightbig/parenrightig
(16)
≥L2
8h2(1−h)2/parenleftig
1−n/radicalbig
ρ/2Lh2/parenrightig
.
When possible (i.e. when n√ρis large enough), setting h=/parenleftig
1√
2Ln√ρ/parenrightig1/2
leads to, when n√ρis large
enough (so that 1−h≥1/2),
inf
ˆπ ρ-zCDPsup
π∈ΘLip
LEX∼P⊗n
π,ˆπ/parenleftig
(ˆπ(X)(x0)−π(x0))2/parenrightig
≥L
64(n√ρ)−1.
F Proof of Theorem 3
Letm∈N\{0}that will be fixed later in the proof. As explained in the sketch of the proof, we build a
packing consisting of functions that are parametrized by a vector ω∈{0,1}m. After controlling quantities
such as their pairwise TV distances, and their KL divergences to the uniform distribution, we leverage
Fano-type inequalities in order to obtain lower-bounds.
Packing construction. For anyω∈{0,1}mdifferent from 0and anyh>0, we define the function gL,ω,h
as
gL,ω,h :=1
∥ω∥1m/summationdisplay
i=1ωif∥ω∥1L,i
m+1,h, (19)
where the functions fare defined in (15). Note that gL,ω,hisL-Lipschitz and that as soon as h≤hm:=
1
2(m+1)it is also a valid density so that gL,ω,h∈ΘLip
L. Notice that the function gL,ω,his constant to
1−∥ω∥1Lh2everywhere except on each interval/bracketleftig
i
m+1−h,i
m+1+h/bracketrightig
withisuch thatωi̸= 0, on which it
deviates by a triangle of slopes +Land−L.
By denoting by Kthe triangle kernel such that K(t) =Lt1[−h,0](t)−Lt1(0,h](t), it might be easier to
visualizegL,ω,has
∀t∈[0,1], gL,ω,h(t) = 1−∥ω∥1/integraldisplay
K+m/summationdisplay
i=1ωiK/parenleftbigg
t−1
m+ 1/parenrightbigg
, (20)
where/integraltext
K=Lh2.
22Published in Transactions on Machine Learning Research (08/2023)
Forω,ω′∈{0,1}mand forhsmall enough (i.e. h≤hm), we can bound the total variation between PgL,ω,h
andPgL,ω′,has
TV/parenleftig
PgL,ω,h,PgL,ω′,h/parenrightig
=1
2/integraldisplay
|gL,ω,h−gL,ω′,h|
(20)=1
2/integraldisplay/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∥(ω′∥1−∥ω∥1)/integraldisplay
K+m/summationdisplay
i=1(ω′
i−ωi)K/parenleftbigg
·−1
m+ 1/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
2/integraldisplay
|∥ω′∥1−∥ω∥1|/integraldisplay
K+m/summationdisplay
i=1|ω′
i−ωi|K/parenleftbigg
·−1
m+ 1/parenrightbigg
=1
2/parenleftbigg
|∥ω′∥1−∥ω∥1|+dham(ω,ω′)/parenrightbigg/integraldisplay
K (21)
≤mLh2. (22)
The KL divergence between PgL,ω,handPg, withgthe density constant equal to 1on[0,1], satisfies
KL/parenleftbig
PgL,ω,h/vextenddouble/vextenddoublePg/parenrightbig
=/integraldisplay
[0,1]ln (gL,ω,h)gL,ω,h
= ln/parenleftbig
1−∥ω∥1Lh2/parenrightbig/parenleftbig
1−∥ω∥1Lh2/parenrightbig
(1−∥ω∥12h)
+ 2∥ω∥1/integraldisplayh
0ln/parenleftbig
1−∥ω∥1Lh2+Lt/parenrightbig/parenleftbig
1−∥ω∥1Lh2+Lt/parenrightbig
dt
ln(1+·)≤·
≤/parenleftbig
−∥ω∥1Lh2/parenrightbig/parenleftbig
1−∥ω∥1Lh2/parenrightbig
(1−∥ω∥12h)
+ 2∥ω∥1/integraldisplayh
0/parenleftbig
−∥ω∥1Lh2+Lt/parenrightbig/parenleftbig
1−∥ω∥1Lh2+Lt/parenrightbig
dt
Calculus=L2
3∥ω∥1h3(2−3∥ω∥1h).(23)
Finally, we lower bound the squared L2distance between gL,ω,handgL,ω′,h:
/integraldisplay
[0,1](gL,ω,h−gL,ω′,h)2
=m/summationdisplay
i=11ωi̸=ω′
i/integraldisplayi
m+1+h
i
m+1−h/parenleftbigg
(∥ω′∥1−∥ω∥1)/integraldisplay
K+ (ωi−ω′
i)K/parenleftbigg
t−i
m+ 1/parenrightbigg/parenrightbigg2
dt
≥m/summationdisplay
i=11ωi̸=ω′
i/integraldisplayi
m+1+h
i
m+1−h/parenleftbigg
K/parenleftbigg
t−i
m+ 1/parenrightbigg
−|∥ω∥1−∥ω′∥1|/integraldisplay
K/parenrightbigg2
dt
≥m/summationdisplay
i=11ωi̸=ω′
i/integraldisplayi
m+1+h
i
m+1−h/braceleftigg/parenleftbigg
K/parenleftbigg
t−i
m+ 1/parenrightbigg/parenrightbigg2
−2K/parenleftbigg
t−i
m+ 1/parenrightbigg
|∥ω∥1−∥ω′∥1|/integraldisplay
K/bracerightbigg
dt
≥dham(ω,ω′)/parenleftigg/integraldisplay
K2−2m/parenleftbigg/integraldisplay
K/parenrightbigg2/parenrightigg
≥2dham(ω,ω′)L2/parenleftbiggh3
3−mh4/parenrightbigg
=2dham(ω,ω′)L2h3(1−3mh)
3.(24)
23Published in Transactions on Machine Learning Research (08/2023)
By the Varshamov-Gilbert theorem (Tsybakov, 2009, Lemma 2.7), as long as m≥8, there exist M∈N
andω(0),...,ω(M)∈{0,1}msuch thatM≥2m/8,ω(0)={0}mandi̸=j=⇒dham/parenleftbig
ω(i),ω(j)/parenrightbig
≥m/8.
According to (24), the family/parenleftbig
gL,ω(i),h/parenrightbig
i=1,...,Mis then an Ω:=1
2/radicalig
mL2(h3−3mh4)
12packing of ΘLip
Lfor the
L2distance.
Recovering the usual lower-bound. By Equation (5) with Φ(·):= (·)2and∥·∥theL2norm,
inf
ˆπs.t.Csup
π∈ΘLip
LEX∼P⊗n
π,ˆπ/parenleftigg/integraldisplay
[0,1](ˆπ(X)−π)2/parenrightigg
≥mL2h3(1−3mh)
48inf
ˆπs.t.Cinf
Ψ:ΘLip
L→{0,1}max
i=1,...,MPX∼P⊗n
gL,ω(i),h,ˆπ(Ψ(ˆπ(X))̸=i)
Fact 3
≥mL2h3(1−3mh)
48
1−1 +1
M/summationtext
1≤i≤MKL/parenleftig
P⊗n
gL,ω(i),h/vextenddouble/vextenddouble/vextenddoubleP⊗n
g/parenrightig
ln(M)

Tensorization=mL2h3(1−3mh)
48
1−1 +n
M/summationtext
1≤i≤MKL/parenleftig
PgL,ω(i),h/vextenddouble/vextenddouble/vextenddoublePg/parenrightig
ln(M)

(23)&∥ω∥1≤m,M≥2m/8
≥mL2h3(1−3mh)
48/parenleftigg
1−1 +L2
3nmh3(2−3mh)
ln(2)m/8/parenrightigg
.(25)
So, by choosing m=/ceilingleftbig
n1/3/ceilingrightbig
andh=c
mwhere c is a positive constant small enough we get, for nbig enough,
inf
ˆπ ϵ-DPsup
π∈ΘLip
LEX∼P⊗n
π,ˆπ/parenleftigg/integraldisplay
[0,1](ˆπ(X)−π)2/parenrightigg
≥C−1(n)−2/3,
whereCis a positive constant depending only on L.
ϵ-DP overhead. By the same reduction and Fano’s lemma for differential privacy on product distributions
(Fact 6), we get for any h≤hm,
inf
ˆπ ϵ-DPsup
π∈ΘLip
LEX∼P⊗n
π,ˆπ/parenleftigg/integraldisplay
[0,1](ˆπ(X)−π)2/parenrightigg
≥mL2h3(1−3mh)
48
1−1 +nϵ
M22/summationtext
1≤i,j≤MTV/parenleftig
PgL,ω(i),h,PgL,ω(j),h/parenrightig
ln(M)

(22)&M≥2m/8
≥mL2h3(1−3mh)
48/parenleftbigg
1−1 + 2nϵmLh2
ln(2)m/8/parenrightbigg
.
So, by choosing m=⌈√nϵ⌉andh=c
mwhere c is small enough a positive constant (depending only on L),
we get, as soon as min(n,nϵ)is big enough,
inf
ˆπ ϵ-DPsup
π∈ΘLip
LEX∼P⊗n
π,ˆπ/parenleftigg/integraldisplay
[0,1](ˆπ(X)−π)2/parenrightigg
≥C′−1(nϵ)−1,
whereC′is a positive constant depending only on L.
ρ-zCDP overhead. Forρ-zCDP, we present the proof using both Fano’s lemma and Assouad’s method.
We will see that Assouad gives better results
24Published in Transactions on Machine Learning Research (08/2023)
Fano version. By again the same reduction and Fano’s lemma for zero-concentrated differential privacy
(Fact 7), denoting ti,j:= TV/parenleftig
PgL,ω(i),h,PgL,ω(j),h/parenrightig
, we get for any h≤hm,
inf
ˆπ ρ-zCDPsup
π∈ΘLip
LEX∼P⊗n
π,ˆπ/parenleftigg/integraldisplay
[0,1](ˆπ(X)−π)2/parenrightigg
≥mL2h3(1−3mh)
48/parenleftigg
1−1 +n2ρ
M24/summationtext
1≤i,j≤M/parenleftbig1
nti,j+t2
i,j/parenrightbig
ln(M)/parenrightigg
(22)
≥mL2h3(1−3mh)
48
1−1 +n2ρ4/parenleftig
mLh2
n+m2L2h4/parenrightig
ln(2)m/8
.
So, by choosing m=/ceilingleftig/parenleftbig
n√ρ/parenrightbig2
3/ceilingrightig
andh=c
mforcsmall enough (depending only on L), ifn
ρis big enough,
we get that
inf
ˆπs.t.ρ-zCDPsup
π∈ΘLip
LEX∼P⊗n
π,ˆπ/parenleftigg/integraldisplay
[0,1](ˆπ(X)−π)2/parenrightigg
≥C′′−1(n√ρ)−4/3
whereC′′is a positive constant depending only on L.
Assouad version. From Equation (24), we can see that when h:=c
mfor a positive cthat is small enough,
the condition expressed in Equation (12) is satisfied for τ= Ω(h3). To apply (14), the only missing ingredient
is to bound the testing difficulties between the mixtures on the hypercube.
In the sequel, Pωis used as a short for PgL,ω,h. We need to bound the total variation between the mixtures
on the hypercube (see (14)) as
TV (Pωi,0,Pωi,1) = TV
1
2m−1/summationdisplay
ω∈{0,1}m|ωi=0PgL,ω,h,1
2m−1/summationdisplay
ω∈{0,1}m|ωi=1PgL,ω,h

=1
21
2m−1/integraldisplay/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
ω∈{0,1}m|ωi=0gL,ω,h−/summationdisplay
ω∈{0,1}m|ωi=1gL,ω,h/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=1
2m/integraldisplay/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
ω1,...,ωi−1,ωi+1...,ωm∈{0,1}/parenleftbig
gL,(ω1,...,ωi−1,0,ωi+1...,ωm),h−
gL,(ω1,...,ωi−1,1,ωi+1...,ωm),h/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
2m/summationdisplay
ω1,...,ωi−1,ωi+1...,ωm∈{0,1}/integraldisplay/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglegL,(ω1,...,ωi−1,0,ωi+1...,ωm),h−
gL,(ω1,...,ωi−1,1,ωi+1...,ωm),h/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
(21)
≤1
2m/summationdisplay
ω1,...,ωi−1,ωi+1...,ωm∈{0,1}2Lh2
=O/parenleftbig
h2/parenrightbig
.
Here and in the sequel, the asymptotic comparators only hide constants and terms that depend on L. All
in all, by using Le Cam’s lemma for product distribution and ρ-zCDP Fact 5, and by Equation (13), since
τ= Ω(h3)we obtain
inf
ˆπ ρ-zCDPsup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπ/parenleftigg/integraldisplay
[0,1](ˆπ(X)−π)2/parenrightigg
= Ω/parenleftbig
mh3/parenrightbig/parenleftbig
1−n√ρO/parenleftbig
h2/parenrightbig/parenrightbig
. (26)
25Published in Transactions on Machine Learning Research (08/2023)
Settingh≈/parenleftbig
n√ρ/parenrightbig−1
2concludes the proof.
G proof of Lemma 2
Letπ∈ΘPSob
L,β. We have,
E/parenleftigg/integraldisplay
[0,1]/parenleftbig
ˆπproj(X)−π/parenrightbig2/parenrightigg
Parseval=E/parenleftiggN/summationdisplay
i=1/parenleftbigg
ˆθi−θi+1
nZi/parenrightbigg2
++∞/summationdisplay
i=N+1θ2
i/parenrightigg
=N/summationdisplay
i=1E/parenleftigg/parenleftbigg
ˆθi−θi+1
nZi/parenrightbigg2/parenrightigg
++∞/summationdisplay
i=N+1θ2
i.
Furthermore, for any i, sinceZis centered
E/parenleftig
ˆθi/parenrightig
=E
1
nn/summationdisplay
j=1ϕi(Xj)
=1
nn/summationdisplay
j=1E(ϕi(Xj))Xji.i.d.=EX∼P⊗n
πϕi(X1) =/integraldisplay
πϕi=θi
Hence, for any i, sinceZiis independent from the dataset
E/parenleftigg/parenleftbigg
ˆθi−θi+1
nZi/parenrightbigg2/parenrightigg
=V/parenleftig
ˆθi/parenrightig
+1
n2V(Zi)Independence of Xj=1
n2n/summationdisplay
j=1V(ϕi(Xj)) +1
n2V(Zi)
|ϕi|≤√
2
≤2
n+1
n2V(Z).
Finally, with aj:=j−1, Fact 1 allows bounding/summationtext+∞
i=m+1θ2
ias
+∞/summationdisplay
i=N+1θ2
i≤1
N2β+∞/summationdisplay
i=N+1a2β
iθ2
i≤1
N2β+∞/summationdisplay
i=1a2β
iθ2
iFact 1
≤1
N2βL2
π2β.
This yields the conclusion with CL,β:= max(2,L2/π2β).
H Proof of Theorem 5
Let us consider the following well-known function :
∀x∈R, K 0(x):=e−1
1−x21(−1,1)(x).
We can notice that for any β > 0there exists ν > 0such that the kernel K(x):=νK0(2x)satisfies
K∈ C∞(R,[0,+∞)),/integraltext/parenleftbig
K(β)/parenrightbig2≤1andK(x)>0iffx∈(−1/2,1/2). Furthermore, for any i∈N,
K(i)(x) = 0for everyx∈(−∞,−1/2]∪[1/2,+∞).
Packing construction. Letm∈N\{0}that will be fixed later. For any h > 0, andω∈{0,1}m, we
define the function gL,β,ω,has,
∀x∈[0,1], gL,β,ω,h (x):= 1−∥ω∥1Lhβ+1/integraldisplay
K+Lhβm/summationdisplay
i=1ωiK/parenleftigg
x−i
m+1
h/parenrightigg
. (27)
Note that when h<1
m+1we have/integraltext1
0gL,β,ω,h = 1; whenmh/integraltext/parenleftbig
K(β)/parenrightbig2≤1, we havegL,β,ω,h≥0; and when
both hold we have gL,β,ω,h∈ΘPSob
L,β(see Equation (9)). Indeed, under these hypotheses, the periodicity
conditions are immediate (the function is constant on neighborhoods of 0and1, with the same value). The
26Published in Transactions on Machine Learning Research (08/2023)
energy of the βth derivative can be bounded as
/integraldisplay/parenleftig
g(β)
L,β,ω,h/parenrightig2
=/integraldisplay
Lhβm/summationdisplay
i=1ωi/parenleftigg
x∝⇕⊣√∫⊔≀→K/parenleftigg
x−i
m+1
h/parenrightigg/parenrightigg(β)
2
=/integraldisplay/parenleftigg
Lm/summationdisplay
i=1ωiK(β)/parenleftigg
·−i
m+1
h/parenrightigg/parenrightigg2
disjoint support=L2m/summationdisplay
i=1ωi/integraldisplay/parenleftigg
K(β)/parenleftigg
·−i
m+1
h/parenrightigg/parenrightigg2
=L2mh/integraldisplay/parenleftig
K(β)/parenrightig2
≤L2.
In the sequel of this proof, this hypothesis will always be satisfied asymptotically for all the values of mand
hthat will be considered. From now on, we may consider it valid.
Givenh>0andω,ω′∈{0,1}m, whengL,β,ω,h,gL,β,ω′,h∈ΘPSob
L,β, we can bound the total variation between
PgL,β,ω,handPgL,β,ω′,has,
TV/parenleftig
PgL,β,ω,h,PgL,β,ω′,h/parenrightig
=1
2/integraldisplay
|gL,β,ω,h−gL,β,ω′,h|
=1
2/integraldisplay/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∥(ω′∥1−∥ω∥1)Lhβ+1/integraldisplay
K+m/summationdisplay
i=1(ω′
i−ωi)LhβK/parenleftigg
·−1
m+1
h/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
2/integraldisplay
∥|ω′∥1−∥ω∥1|Lhβ+1/integraldisplay
K+m/summationdisplay
i=1|ω′
i−ωi|LhβK/parenleftigg
·−1
m+1
h/parenrightigg
=1
2/parenleftbigg
|∥ω′∥1−∥ω∥1|+dham(ω,ω′)/parenrightbigg
Lhβ+1/integraldisplay
K (28)
≤mLhβ+1. (29)
27Published in Transactions on Machine Learning Research (08/2023)
The KL divergence between PgL,β,ω,handPg, the uniform distribution on [0,1], is bounded as
KL/parenleftbig
PgL,β,ω,h/vextenddouble/vextenddoublePg/parenrightbig
=/integraldisplay
[0,1]ln (gL,β,ω,h )gL,β,ω,h
=/integraldisplay
[0,1]\∪i:ωi̸=0[i
m+1−h
2,i
m+1+h
2]ln/parenleftbigg
1−∥ω∥1Lhβ+1/integraldisplay
K/parenrightbigg/parenleftbigg
1−∥ω∥1Lhβ+1/integraldisplay
K/parenrightbigg
dt
+∥ω∥1/integraldisplayh
2
−h
2ln/parenleftbigg
1−∥ω∥1Lhβ+1/integraldisplay
K+LhβK/parenleftbiggt
h/parenrightbigg/parenrightbigg
/parenleftbigg
1−∥ω∥1Lhβ+1/integraldisplay
K+LhβK/parenleftbiggt
h/parenrightbigg/parenrightbigg
dt
ln(1+·)≤·
≤ (1−∥ω∥1h)/parenleftbigg
−∥ω∥1Lhβ+1/integraldisplay
K/parenrightbigg/parenleftbigg
1−∥ω∥1Lhβ+1/integraldisplay
K/parenrightbigg
+∥ω∥1/integraldisplayh
2
−h
2/parenleftbigg
−∥ω∥1Lhβ+1/integraldisplay
K+LhβK/parenleftbiggt
h/parenrightbigg/parenrightbigg
/parenleftbigg
1−∥ω∥1Lhβ+1/integraldisplay
K+LhβK/parenleftbiggt
h/parenrightbigg/parenrightbigg
dt
Calculus=∥ω∥1L2h2β+1/integraldisplay
K2−∥ω∥2
1L2h2β+2/integraldisplay
K
≤∥ω∥1L2h2β+1/integraldisplay
K2≤mL2h2β+1/integraldisplay
K2.(30)
Finally, the squared L2distance between gL,β,ω,handgL,β,ω′,hcan be lower bounded as,
/integraldisplay
[0,1](gL,β,ω,h−gL,β,ω′,h)2
=m/summationdisplay
i=11ωi̸=ω′
i/integraldisplayi
m+1+h
2
i
m+1−h
2/parenleftigg
Lhβ+1(∥ω′∥1−∥ω∥1)/integraldisplay
K+ (ωi−ω′
i)LhβK/parenleftigg
t−i
m+1
h/parenrightigg/parenrightigg2
dt
≥m/summationdisplay
i=11ωi̸=ω′
i/integraldisplayi
m+1+h
2
i
m+1−h
2/parenleftigg
LhβK/parenleftigg
t−i
m+1
h/parenrightigg
−Lhβ+1|∥ω∥1−∥ω′∥1|/integraldisplay
K/parenrightigg2
dt
≥m/summationdisplay
i=11ωi̸=ω′
i/integraldisplayi
m+1+h
2
i
m+1−h
2

/parenleftigg
LhβK/parenleftigg
t−i
m+1
h/parenrightigg/parenrightigg2
−2LhβK/parenleftigg
t−i
m+1
h/parenrightigg
Lhβ+1|∥ω∥1−∥ω′∥1|/integraldisplay
K/bracerightigg
dt
≥dham(ω,ω′)L2h2β+1/parenleftigg/integraldisplay
K2−2mh/parenleftbigg/integraldisplay
K/parenrightbigg2/parenrightigg
.(31)
By the Varshamov-Gilbert theorem (Tsybakov, 2009, Lemma 2.7), as long as m≥8, there exist M∈N
andω(0),...,ω(M)∈{0,1}msuch thatM≥2m/8,ω(0)={0}mandi̸=j=⇒dham/parenleftbig
ω(i),ω(j)/parenrightbig
≥m/8.
Accordingto(31), thefamily/parenleftbig
gL,β,ω(i),h/parenrightbig
i=1,...,Misthena Ω =1
2/radicalbigg
m
8L2h2β+1/parenleftig/integraltext
K2−2mh/parenleftbig/integraltext
K/parenrightbig2/parenrightig
packing
ofΘPSob
L,βfor theL2distance.
28Published in Transactions on Machine Learning Research (08/2023)
Recovering the usual lower-bound. By Equation (5) with Φ(·):= (·)2and∥·∥theL2norm,
inf
ˆπs.t.Csup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπ/integraldisplay
[0,1](ˆπ(X)−π)2
≥L2
32mh2β+1/parenleftigg/integraldisplay
K2−2mh/parenleftbigg/integraldisplay
K/parenrightbigg2/parenrightigg
inf
ˆπs.t.Cinf
Ψ:ΘPSob
L,β→{0,1}max
i=1,...,MPX∼P⊗n
gL,β,ω(i),h,ˆπ(Ψ(ˆπ(X))̸=i)
Fact 3
≥L2
32mh2β+1/parenleftigg/integraldisplay
K2−2mh/parenleftbigg/integraldisplay
K/parenrightbigg2/parenrightigg

1−1 +1
M/summationtext
1≤i≤MKL/parenleftig
P⊗n
gL,β,ω(i),h/vextenddouble/vextenddouble/vextenddoubleP⊗n
g/parenrightig
ln(M)

Tensorization=L2
32mh2β+1/parenleftigg/integraldisplay
K2−2mh/parenleftbigg/integraldisplay
K/parenrightbigg2/parenrightigg

1−1 +n
M/summationtext
1≤i≤MKL/parenleftig
PgL,β,ω(i),h/vextenddouble/vextenddouble/vextenddoublePg/parenrightig
ln(M)

(30)&M≥2m/8
≥L2
32mh2β+1/parenleftigg/integraldisplay
K2−2mh/parenleftbigg/integraldisplay
K/parenrightbigg2/parenrightigg/parenleftbigg
1−1 +nmL2h2β+1/integraltext
K2
ln(2)m/8/parenrightbigg
.(32)
Finally, setting m=/ceilingleftig
n1
2β+1/ceilingrightig
andh=c
mforcsmall enough gives that, for nbig enough,
inf
ˆπ ϵ-DPsup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπ/integraldisplay
[0,1](ˆπ(X)−π)2≥C−1n−2β
2β+1,
whereCis a positive constant depending only on Landβ.
ϵ-DP overhead. By the same reduction and Fano’s lemma for differential privacy on product distributions
(Fact 6), we get
inf
ˆπ ϵ-DPsup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπ/integraldisplay
[0,1](ˆπ(X)−π)2
≥L2
32mh2β+1/parenleftigg/integraldisplay
K2−2mh/parenleftbigg/integraldisplay
K/parenrightbigg2/parenrightigg

1−1 +nϵ
M22/summationtext
1≤i,j≤MTV/parenleftig
PgL,β,ω(i),h,PgL,β,ω(j),h/parenrightig
ln(M)

(29)
≥L2
32mh2β+1/parenleftigg/integraldisplay
K2−2mh/parenleftbigg/integraldisplay
K/parenrightbigg2/parenrightigg/parenleftbigg
1−1 + 2nϵmLhβ+1/integraltext
K
ln(2)m/8/parenrightbigg
.
Settingm=/ceilingleftig
(nϵ)1
β+1/ceilingrightig
andh=c
mforcsmall enough leads to, for nϵbig enough,
inf
ˆπ ϵ-DPsup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπ/integraldisplay
[0,1](ˆπ(X)−π)2≥C′−1(nϵ)−2β
β+1,
whereC′is a constant depending only on Landβ.
29Published in Transactions on Machine Learning Research (08/2023)
ρ-zCDP overhead. Forρ-zCDP, we present the proof using both Fano’s lemma and Assouad’s method.
We will see that Assouad gives better results.
Fano version. By again the same reduction and Fano’s lemma for zero-concentrated differential privacy
(Fact 7), denoting ti,j:= TV/parenleftig
PgL,β,ω(i),h,PgL,β,ω(j),h/parenrightig
, we get
inf
ˆπ ρ-zCDPsup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπ/integraldisplay
[0,1](ˆπ(X)−π)2
≥L2
32mh2β+1/parenleftigg/integraldisplay
K2−2mh/parenleftbigg/integraldisplay
K/parenrightbigg2/parenrightigg
/parenleftigg
1−1 +n2ρ
M24/summationtext
1≤i,j≤M1
nti,j+t2
i,j
ln(M)/parenrightigg
(29)
≥L2
32mh2β+1/parenleftigg/integraldisplay
K2−2mh/parenleftbigg/integraldisplay
K/parenrightbigg2/parenrightigg

1−1 + 4n2ρ/parenleftbigg
mLhβ+1/integraltext
K
n+/parenleftbig
mLhβ+1/integraltext
K/parenrightbig2/parenrightbigg
ln(2)m/8
.
So, by choosing m=/ceilingleftig/parenleftbig
n√ρ/parenrightbig2
2β+1/ceilingrightig
andh=c
mforcsmall enough, if n√ρandn
(n√ρ)2β
2β+1=/parenleftbig
n√ρ/parenrightbig1
2β+1/√ρ
are big enough,
inf
ˆπ ϵ-DPsup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπ/integraldisplay
[0,1](ˆπ(X)−π)2≥C′′−1(n√ρ)−2β
β+1/2,
whereC′′is a constant depending only on Landβ.
Assouad version. From Equation (31), we can see that when h:=c
mfor a positive cthat is small
enough, the condition expressed in Equation (12) is satisfied for τ= Ω(h2β+1). To apply (14), the only
missing ingredient is to bound the testing difficulties between the mixtures on the hypercube.
30Published in Transactions on Machine Learning Research (08/2023)
In the sequel, Pωis used as a short for PgL,β,ω,h. We need to bound the total variation between the mixtures
on the hypercube (denoted Pωi,0andPωi,1, cf (14)) as
TV (Pωi,0,Pωi,1)
=1
21
2m−1/integraldisplay/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
ω∈{0,1}m|ωi=0gL,β,ω,h−/summationdisplay
ω∈{0,1}m|ωi=1gL,β,ω,h/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=1
2m/integraldisplay/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
ω1,...,ωi−1,ωi+1...,ωm∈{0,1}/parenleftbig
gL,β,(ω1,...,ωi−1,0,ωi+1...,ωm),h−
gL,β,(ω1,...,ωi−1,1,ωi+1...,ωm),h/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
2m/summationdisplay
ω1,...,ωi−1,ωi+1...,ωm∈{0,1}/integraldisplay/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglegL,β,(ω1,...,ωi−1,0,ωi+1...,ωm),h−
gL,β,(ω1,...,ωi−1,1,ωi+1...,ωm),h/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
(28)
≤1
2m/summationdisplay
ω1,...,ωi−1,ωi+1...,ωm∈{0,1}2Lhβ+1/integraldisplay
K
=O/parenleftbig
hβ+1/parenrightbig
.
Here and in the sequel, the asymptotic comparators only hide constants (such as/integraltext
Kor/integraltext
K2) and terms
that depends on Landβ. All in all, by using Le Cam’s lemma for product distribution and ρ-zCDP (Fact 5),
and by leveraging Equation (13), with τ= Ω(h2β+1),
inf
ˆπ ρ-zCDPsup
π∈ΘPSob
L,βEX∼P⊗n
π,ˆπ/integraldisplay
[0,1](ˆπ(X)−π)2= Ω/parenleftbig
mh2β+1/parenrightbig/parenleftbig
1−n√ρO/parenleftbig
hβ+1/parenrightbig/parenrightbig
. (33)
Settingh≈/parenleftbig
n√ρ/parenrightbig−1
β+1andm=c/hforcsmall enough concludes the proof by yielding a lower bound
Ω/parenleftbigg/parenleftbig
n√ρ/parenrightbig−2β
2β+1/parenrightbigg
.
31