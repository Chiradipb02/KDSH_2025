Under review as submission to TMLR
Linear Speedup in Personalized Collaborative Learning
Anonymous authors
Paper under double-blind review
Abstract
Collaborative training can improve the accuracy of a model for a user by trading off the
model’sbias(introducedbyusingdatafromotheruserswhoarepotentiallydifferent)against
itsvariance(duetothelimitedamountofdataonanysingleuser). Inthiswork, weformalize
the personalized collaborative learning problem as a stochastic optimization of a task 0while
given access to Nrelated but different tasks 1,...,N. We give convergence guarantees for
two algorithms in this setting—a popular collaboration method known as weighted gradient
averaging , and a novel bias correction method—and explore conditions under which we can
achieve linear speedup w.r.t. the number of auxiliary tasks N. Further, we also empirically
study their performance confirming our theoretical insights.
1 Introduction
Collaborative learning is the setup where agents/users/clients collaborate in hope of better performance
(faster convergence, smaller inference time, or generalization) compared to each agent working alone. Fed-
erated learning and auxiliary learning are two examples of collaborative learning. In Federated Learning
multiple users train a machine learning model on their combined datasets (Kairouz et al., 2019). Col-
laboration vastly increases the amount of data available for training. However, the other users may be
heterogeneous, i.e., they may have datasets and objectives which do not match those of the considered user.
Combining data from such heterogeneous users can significantly hamper performance, with even worse per-
formance than when training alone (Yu et al., 2020).
Training alone and on combined data represent two extremes, with the former having no bias but high
variance and the latter having low variance but high bias. Alternatively, personalized collaborative learning
algorithms (where each user only cares about its own performance) (Wang et al., 2019; Mansour et al.,
2020) attempt to find ‘in-between’ models that trade off some bias against variance. In the best case, we
can use the data from the Nother users to reduce our variance by a factor N(calledlinear speedup ) while
simultaneously not incurring any bias.
In auxiliary Learning, the goal is to train one main task by combining it with auxiliary tasks that are related
in some sense to the main task. In this sense, auxiliary learning can be seen as more general than personal-
ization in Federated Learning.
In this work, we explore from a purely theoretical lens, under what conditions a given agent can benefit from
personalized collaborative learning. We consider an idealized scenario where the goal is optimizing a fixed
user’s stochastic function f0(x), while also giving access to stochastic gradients of Nother collaborators
{f1(x),...,fN(x)}. We also neglect communication issues: the users can be all on the same server for exam-
ple or the collaborators can be treated as auxiliary functions. In the latter case, one important question is to
know how much can we benefit from such auxiliary "information" available to us (maybe for free). We start
with the simple strategy of weighted gradient averaging that uses a weighted average of the gradient
estimates as a pseudo-gradient and then takes an SGD step. We show that while there do exist scenarios
where this simple strategy suffices, it can also incur significant bias introduced by the collaborators. This
then motivates our main method of bias correction which uses the past observed gradients to estimate
and correct for these biases. We show that our proposed solution solves the problems WGA had with bias.
Furthermore, we get a linear speedup in the number of agents that satisfy a mild dissimilarity constraint.
Contributions. Our main contributions include:
1Under review as submission to TMLR
•Formalizing the collaborative stochastic optimization problem where an agent is required to minimize
their objective by collaborating with other agents, in contrast to traditional federated learning.
•Proving convergence rates for weighted gradient averaging and proposing and analyzing a novel bias
correction algorithm.
•Showing that with the correct choice of hyper-parameters and under a mild condition on the dissimilarity
between agents, bias correction enjoys a linear speedup in the number of (relatively similar) collaborators,
with variance reducing as collaborators increase (and a bias going to zero in the number of steps).
2 Related Work
Federated and Decentralized Learning. Federated learning (FL) (Konecny et al., 2016; McMahan et al.,
2017; Mohri et al., 2019) denotes a machine learning setting where a global set of training data is distributed
over multiple users (also called agents or clients). These users form a ‘federation’ to train a global model
on the union of all users’ data. The training is coordinated by a central server, and the users’ local data
never leaves its device of origin. Owing to data locality and privacy awareness, FL has become prominent for
privacy-preserving machine learning (Kairouz et al., 2019; Li et al., 2020a; Wang et al., 2021). Our studied
setting is different because we learn the objective of one specific user, not the union of users. Decentralized
learning refers to the analogous more general setting without a central server, where users communicate
peer-to-peer during training, see e.g. (Nedic, 2020).
Personalization. Due to device heterogeneity, and data heterogeneity, a ‘one model fits all approach leads
to poor accuracy on individual users. Instead, we need to learn personalized models for each user. Prominent
approaches for this include performing additional local adaptation or fine-tuning (Wang et al., 2019; Fallah
et al., 2020), or weighted averaging between a global model and a locally trained model (Mansour et al.,
2020; Deng et al., 2020; Hanzely & Richtárik, 2020). Collins et al. (2020); Khodak et al. (2019) investigate
how such local fine-tuning can improve performance in some simple settings if the users’ optima are close
to each other. In another highly relevant line of work, Maurer et al. (2016); Tripuraneni et al. (2020);
Koshy Thekumparampil et al. (2021); Feng et al. (2021) shows how a shared representation can be leveraged
to perform efficient transfer of knowledge between different tasks (and users). Li et al. (2020c); Mohri
et al. (2019); Yu et al. (2020) investigate how FL distributes the accuracy across users and show that
personalization gives a more equitable distribution. We refer to (Kulkarni et al., 2020) for a broader survey
of personalization methods.
Unlike most of the above works, we consider the perspective of a single agent/user . Further, while our
weighted gradient averaging is closely related to weighted model averaging, the bias correction method is
novel and is directly motivated by our theory. Finally, while several of the above works (e.g. Mansour et al.,
2020; Deng et al., 2020) also provide theoretical guarantees, they use a statistical learning theory viewpoint
whereas we use a stochastic optimization lens.
Perhapstheworksclosesttooursare(Donahue&Kleinberg,2020)and(Grimbergetal.,2021), bothofwhom
study model averaging. The former uses game theory to investigate whether self-interested players have an
incentive to join an FL task. This is true as long as users achieve significantly better performance when
training together than when training alone. Their work further highlights the importance of understanding
when personalization can improve performance. More recently, Grimberg et al. (2021) consider a weighted
model averaging of two users for mean estimation in 1D. Both these works study only toy settings with
restrictive assumptions. Our results are more general and include non-convex optimization.
More recently there was an attempt to formalize a new selfish variant of Federated Learning (Ruichen et al.,
2022), a new setting where we only care about the performance of a subset of internal clients all the while
using/collaborating with external clients. This setting is a particular case of the one considered here (by
taking client 0 to be the average of internal clients). Also, (Mestoukirdi et al., 2021) proposes a user-centric
formulation of federated learning that can be seen as a particular case of our weighted gradient averaging
scheme, further they show empirically that communication load problems can be overcome by clustering
agents. The last two works lack rigorous theory to back their results.
Auxiliary learning. More generally, there is a framework that combines a main task with auxiliary tasks
in order to minimize the performance of the main task; this is usually done by minimizing a weighted linear
combination of the main loss with the auxiliary losses which is similar to WGA. In most of the works in
this area, there is a lack of convergence guarantees. The auxiliary tasks or collaborators as we call them can
2Under review as submission to TMLR
be seen as (hopefully more informative) priors as in (Baifeng et al.). Such works that use this idea are for
example (Xingyu et al.) for reinforcement learning (which optimizes the collaboration weights as well) and
(Aviv et al.) which considers a general combination (not necessarily linear) implemented by a neural network
and performs optimization via implicit differentiation. All these works are based on approximations and lack
convergence guarantees as stated before. In this work, we propose a simpler model (constant collaboration
weights) but analyze rigorously the convergence of all algorithms which has not been done before.
Lately, Chayti & Karimireddy (2022) took inspiration from stochastic variance reduction methods (Johnson
& Zhang, 2013) to propose a way to perform optimization when having access to auxiliary information and
give theoretical guarantees, however, this work does not have the linear speedup in terms of the number of
collaborators that we have in this work.
Control variates. There is some similarity between our bias correction method and other control variate
methods such as SCAFFOLD (Karimireddy et al., 2019), however the local vs global objectives as well as the
resulting updates are different. Also, we use an exponential moving average whereas other control variates
use mainly an SVRG-like correction (for a detailed discussion see Appendix A.2).
3 Setup and Assumptions
In this section, we formalize personalized collaborative optimization and discuss our assumptions.
3.1 Personalized Collaborative Stochastic Optimization
We model collaborative optimization as an environment where N+ 1users denoted 0,...,Ncan interact
with each other. Each user khas only access to its own objective fk(x) :=Eξ(k)[fk(x;ξ(k))](e.g. a loss
function evaluated on their own data), where ξ(k)is a random variable from which we can sample without
necessarily knowing its distribution (this covers the online optimization setting as well as optimizing over
finite training data sets). The users can collaborate by sharing (stochastic) gradients that they compute on
their private loss function fkon a shared input parameter x.
We formalize the personalized collaborative stochastic optimization problem as solving for user 0’s goal:
min
x∈Rdf0(x), (1)
by exchanging gradients with the other users. This exchange of information between the main user ‘0’ and
their collaborators can be done in many ways. In this work, to solve problem (1), user 0updates their state
xtby using different variants of a gradient estimate g(xt)and step size ηt:
xt+1=xt−ηtg(xt). (2)
As illustrated in Algorithm 1, each collaborator kcomputes an unbiased local gradient estimate gk(xt) :=
∇xfk(xt;ξ(k)
t)of∇xfk(xt)atxt, and shares those with the main user 0. Using these helper gradients as
well as its own gradient, user 0then forms the final g(xt)and takes an update step.
The simplest baseline to consider (henceforth called the ‘Alone’ method) is the case where user 0ignores
the collaborators and decides to work alone by setting g(xt) =g0(xt). In general, g(xt)can be formed in
several different ways, using current gradients as well as past gradients.
3.2 Assumptions
Notation. For each user k, we denote by x⋆
ka stationary point of fk, andf⋆
kits corresponding value. We
denote the gradient noise nk(x,ξ) =gk(xt)−∇xfk(xt).
We make the following common assumptions:
A1 (Smoothness) f0isLsmooth, i.e.,∀x,y∈Rd:
∥∇xf0(x)−∇xf0(y)∥≤L∥y−x∥.
3Under review as submission to TMLR
Algorithm 1 Collaborative Stochastic Optimization
Require: Collaborators k= 0,...,N
Require: x0;ηt;T
fort= 0...T−1do
for allusersk= 0,...,Nin parallel do
Sampleξ(k)
t
Compute gk(xt) :=∇xfk(xt;ξ(k)
t)
end parfor
▽Aggregation on user 0:
Formg(xt)using received{gk(xt′)}t′≤t
k=0,...,N
xt+1=xt−ηtg(xt)
end for return xT
A2 (µ-PL)f0satisfies the µ−PL condition, i.e.:
∀x∈Rd:∥∇xf0(x)∥2≥2µ(f0(x)−f⋆
0).
And for each agent k∈{0,...,N}:
A3 (δ-Bounded Hessian Dissimilarity, or δ-BHD)
∀x∈Rd:∥∇2
xfk(x)−∇2
xf0(x)∥≤δ.
A4 (Gradient Similarity) ∃m,ζ2
k≥0s.t.∀x∈Rd:
∥∇xfk(x)−∇xf0(x)∥2≤m∥∇xf0(x)∥2+ζ2
k.
A5 (Bounded variance) ∃σ2
k≥0s.t.∀x∈Rd:
E[∥nk(x,ξ(k)
t)∥2]≤σ2
k.
A1isaverygenericassumption. A2isnotassumedinthegeneralnon-convexcase, butonlyinthe µ-PLcases
in our theorems, instead of convexity. A3is implied by smoothness, and equivalent up to multiplying δby a
constant to (Karimireddy et al., 2020, Assumption A2), and appears for quadratic functions in (Shamir et al.,
2014; Reddi et al., 2016; Karimireddy et al., 2019). A4is also very generic, and coincides with (Ajalloeian &
Stich, 2020, Assumption 4). Similar assumptions to bound the bias appeared also in (Bertsekas & Tsitsiklis,
2000, though they require vanishing bias), in (Bertsekas, 2002, pg. 38–39) and more recently in (Karimireddy
et al., 2020; 2019; Deng et al., 2020). A5can be relaxed to allow an additional unbounded variance term
which grows with the norm of the estimated gradient. Convergence results under this relaxed assumption
are provided in the supplementary material. Our main conclusions are maintained in this generalized case.
Hessian dissimilarity δ: We note that Hessian dissimilarity as in A2forδ= 2Lis directly implied by
L-smoothness of the users. In practice, if users are similar (and not adversarial) we expect δ≪L.
Bias parameters mandζ2: To showcase the intuition behind the bias parameters mandζwe can limit
ourselves to the case of one collaborator ‘1’. The parameter ζquantifies translation between f0andf1,
whilemquantifies the scaling. To be more precise, if we were collaborating with a translated copy i.e.
f1(x)≡f0(x) +a⊤x+bthenζ2=∥a∥2andm= 0. If we were collaborating with a scaled copy i.e.
f1(x) =sf0(x)thenm= (1−s)2andζ= 0. Even simpler than this, mdetermines whether the bias is
bounded or not, m= 0means the bias can be bounded independently of x, this should be the simplest
case. The constant bias term ζ2also quantifies how much the two collaborators’ goals are different, this can
be seen from the approximation ζ2≈∥∇ xf1(x⋆
0)∥2∝f1(x⋆
0)−f1(x⋆
1), in other words how much distant
are their two stationary points that they would have found by ignoring each other. In particular, ζ2= 0
corresponds to the case of f0andf1sharing the same optimum.
4 Weighted Gradient Averaging
As a first basic algorithm, we here introduce weighted gradient averaging and analyze its convergence in the
non-convex case and under the µ-PL condition. We show that for the special case of collaborative mean
4Under review as submission to TMLR
estimation when every user has its own distribution, we exactly recover the existing theoretical results of
Grimberg et al. (2021). While recuperating analogous main ideas, our results are more general applying to
any smooth stochastic optimization problem in arbitrary dimensions with multiple collaborators.
WGA Algorithm. As illustrated in Algorithm 2, at each time step t, using the current state xt, each
Algorithm 2 WGA variant of Algorithm 1
Require: x0;ηt;αt;{τk}N
k=1;T
... as Algorithm 1 ...
▽Aggregation on user 0:
g(xt) := (1−αt)g0(xt) +αt/summationtextN
k=1τkgk(xt)
collaborator k= 1...Ncomputes gk(xt)an unbiased local gradient estimate of ∇xfk(xt), and sends those
to user 0. Then using these gradient estimates and the collaboration weights αt∈[0,1],{τi}∈{τi≥
0,/summationtext
iτi= 1}, the main user 0forms
g(xt) := (1−αt)g0(xt) +αtN/summationdisplay
k=1τkgk(xt),
and performs an SGD step with the obtained gradient estimate g(xt), reaching the new state xt+1=
xt−ηtg(xt).
Remark. WGA is SGD applied to the “modified “ function x∝⇕⊣√∫⊔≀→(1−αt)f0(x) +αt/summationtextN
k=1τkfk(x).
We analyze now precisely the convergence rate of Algorithm 2 under heterogeneous data across the users, in
the non-convex and µ-PL case in the following Theorem 4.1.
Theorem 4.1 (Convergence of WGA) .Under Assumptions A1, A4, A5, Algorithm 2 after Trounds for
constant collaboration weight αt:=α <1/√m, and constant step-size ηt:=ηsatisfies the following conver-
gence bound, (where Ft:=E[f0(xt)]−f⋆
0):
Non-convex case. Forη= min/parenleftig
1
L,/radicalig
2F0
L˜σ2T/parenrightig
:
1−α2m
2TT−1/summationdisplay
t=0E/bracketleftbig
∥∇f0(xt)∥2/bracketrightbig
=O/parenleftbiggLF0
T+/radicalbigg
LF0˜σ2(α)
T+α2ζ2/parenrightbigg
.
µ-PL case. If in addition A2 holds, then for the choice η= min/parenleftigg
1
L,log(max(1,2µF0T
3L˜σ(α)2))
(1−α2m)µT/parenrightigg
:FT=
˜O/parenleftbigg
F0exp/parenleftbig
−µT
L/parenrightbig
+L˜σ(α)2
µ2T(1−α2m)2+α2ζ2
µ(1−α2m)/parenrightbigg
,
where ˜Osuppresses log(T)factors and we defined ˜σ2(α) := (1−α)2σ2
0+α2/summationtextN
k=1τ2
kσ2
kandζ2=/summationtextN
k=1τkζ2
k.
Similar to (Karimi et al., 2016, Theorem 4), we can get rid of the logarithmic factors in the µ-PL case by
choosing a decreasing step size.
Bias-variance trade-off. Crucially, the collaborative variance ˜σ2(α)is smaller than the individual variance
σ2
0of user 0’s gradient estimates, however, this decrease in variance is accompanied by an additional bias
termO(α2ζ2), hence we have established a bias-variance trade-off, which motivates the proper choice of the
collaboration weight α.
Choice of{τk}N
k=1.The best choice of {τk}N
k=1is based on a constrained quadratic programming problem
(see App. C.2). However as T→∞this best choice of the weights {τk}N
k=1is completely dictated by the bias
term. We have τk∝1{k=arg minlζ2
l}i.e the best we can do is collaborate with the agents with the smallest
bias.
Application of WGA to collaborative mean estimation. Weighted gradient averaging generalizes the
model averaging problem studied in (Donahue & Kleinberg, 2020; Grimberg et al., 2021). We show how to
recover their results here.
5Under review as submission to TMLR
Supposewewanttoestimatethemean µ0ofrealrandomstochasticsamples {z(0)
0,...,z(T)
0}withE[z(t)
0] =µ0.
Consider
min
xf0(x) :=1
2(x−µ0)2,
with unbiased stochastic gradients given as ∇f(x;zt
0) = (x−zt
0). Similarly, we define our collaborator
f1(x) :=1
2(x−µ1)2with a different mean µ1and its stochastic gradients. We have that f0is1-PL, 1-
smooth,ζ2= (µ1−µ0)2, andm= 0. Let us also use a starting point x0=z0
0to getE[F0]≤σ2
0. Plugging
these values into Theorem 4.1, we get that
E(xT−µ0)2≤˜O/parenleftbigg
σ2
0exp (−T) +˜σ(α)2
T+α2(µ0−µ1)2/parenrightbigg
Note thatThere represents the number of stochastic samples of µ0we use. Compare this with (Grimberg
et al., 2021) who show a rate of O/parenleftbig˜σ(α)2
T+α2(µ0−µ1)2/parenrightbig
. Thus, we recover their results for a large enough T
and ignoring logarithmic factors. These logarithmic factors can be avoided by using a decreasing step size
(see Appendix C.2).
Speedup over training alone. Due to the bias-variance trade-off in Theorem 4.1, the best choice of αis
αopt= arg min
α∈(0,1√m)L˜σ(α)2
µ2T(1−α2m)2+α2ζ2
µ(1−α2m).
We show that a linear speedup can only be obtained if m= 0andζ2= 0, this means fk≡f0(collaboration
withNcopies), in this case the inverse of the speedup is given by 1−αopt=1
N+1. However, when the
functions are minimized at the same point ( ζ2= 0) but with unbounded bias ( m > 0), the collaboration
weightαis bounded by1√mdue to the term 1−α2min the denominator and leads to a speedup relative to
training alone that is sub-linear (see Figure 6).
In the case where ζ2>0, the speedup gained due to weighted averaging is further limited. In fact, in this
case whenT→∞we haveαopt→0making the gain 0. Intuitively, WGA controls for the bias introduced
by using gradient estimates from the collaborators by down-weighting them. While this may reduce the bias
in a single round, the bias keeps accumulating over multiple rounds. Thus, the benefit of WGA diminishes
with increasing T. In the next section, we see how to directly remove this bias.
5 Bias Correction
In Section 4, bias was identified as the major problem limiting the performance of WGA. Therefore we
propose a bias correction algorithm that directly tackles this issue. Our strategy consists of estimating
the bias between the gradients of f0and its collaborators {fk}N
k=1using past gradients. Then, this bias is
subtracted from the current gradient estimates of each collaboratorWe first demonstrate the utility of such
bias correction assuming access to some ideal bias oracle. Then, we show how to use an exponential moving
average of past gradients to approximate the oracle.
Algorithm 3 Bias correction variant of Algorithm 1
Require: x0;ηt;αt;βt;T;c0=b0
... as Algorithm 1 ...
▽Aggregation on user 0:
gavg:=/summationtextN
k=1τkgk(xt)
g(xt) := (1−αt)g0(xt) +αt(gavg−ct) ▷update
bt:=gavg(xt)−g0(xt) ▷observed bias
ct+1:= (1−βt)ct+βtbt ▷next bias estimate
BC Algorithm. As usual, at each time t, each userk= 0,...,Ncomputes their own local gradient estimate
gk(xt). Then, as illustrated in Algorithm 3, user 0usesct—an estimate of the bias ct≈(/summationtextN
k=1τk∇fk(x)−
∇f0(x))—and the collaboration weight αt:
6Under review as submission to TMLR
g(xt) := (1−αt)g0(xt) +αt/parenleftigN/summationdisplay
k=1τkgk(xt)−ct/parenrightig
.
Then user 0updates their parameters using this pseudo gradient as xt+1=xt−ηtg(xt). We next discuss
how to compute this estimate ct.
5.1 Using a Bias Oracle
As a warm-up, let us suppose we have access to an oracle that gives a noisy unbiased estimate of the true
bias
coracle,t=N/summationdisplay
k=1τk∇xfk(xt)−∇xf0(xt) +noracle,t.
The quantity noracle,tis the noise of the oracle and is independent of the gradient estimates. Using this, we
have that the update satisfies
E/bracketleftbigN/summationdisplay
k=1τkgk(xt)−coracle,t/bracketrightbig
=∇f0(x).
Hence, this becomes similar to the case where ζ2= 0andm= 0with WGA, enabling linear speedup.
Theorem 5.1 formalizes this intuition.
Theorem 5.1 (Convergence given a bias oracle) .Under Assumption A1, using an ideal oracle of the mean
biascoracle,twith variance E[∥noracle,t∥2] =v2/N(i.e.,v2is the variance of the bias oracle associated to each
collaborator), for constant collaboration weight αt:=α, and constant step-size ηt:=ηwe have the following:
Non-convex case. Forη= min/parenleftbigg
1
L,/radicalig
2F0
L˜σ2(α)/parenrightbigg
:
1
2TT−1/summationdisplay
t=0E[∥∇f0(xt)∥2] =O/parenleftigg
LF0
T+/radicalbigg
LF0˜σ2(α)
T/parenrightigg
.
µ-PL case. If in addition A2 holds, then for the choice η= min/parenleftigg
1
L,log(max(1,2µF0T
3L˜σ(α)2))
µT/parenrightigg
:
FT=˜O/parenleftbigg
F0exp (−µT
L) +L˜σ(α)2
µ2T/parenrightbigg
,
where ˜σ2(α) = (1−α)2σ2
0+α2(σ2
a+v2
N),σ2
a=/summationtextN
k=1τ2
kσ2
k.
Choice of the weights τk.We choose these weights so that we minimize ˜σ2(α), it is easy to show that
there is a choice such that σ2
a≤/summationtextN
k=1σ2
k
N2. To simplify the discussion we suppose/summationtextN
k=1σ2
k
N=σ2
0and replace
σ2
abyσ2
0/N.
Speedup over training alone. First, note that the rate of Theorem 5.1 when v2= 0matches Theorem 4.1
withm= 0andζ2= 0. We examine two cases.
•Ifσ2
0>0: In this case, we choose
αopt∈arg min
α˜σ2(α) =N
N+ 1 +v2
σ2
0,
giving ˜σ2(αopt) =σ2
01+v2
σ2
0
N+1+v2
σ2
0. ForNlarge enough ( N≥v2
σ2+ 1), this simplifies to ˜σ2(αopt) =O(σ2
0
N)and
a convergence rate of O/parenleftbig/radicalig
σ2
0
NT/parenrightbig
in the general non-convex case and O/parenleftbigσ2
0
µ2NT/parenrightbig
withµ-PL inequality. Thus,
we achieve linear speedup.
•Ifσ2
0= 0the baseline here is gradient descent. If v2̸= 0then both the non-convex and µ-PL convergence
rates are slower than GD. The best choice of collaboration weight αhere isα= 0.
7Under review as submission to TMLR
5.2 Approximating the Oracle Using EMA
Clearly, the previous discussion shows that given access to a bias oracle, using bias correction gives significant
speedup even when we have a large bias i.e. mandζ2are large. Algorithm 3 shows how we can use the
exponential moving average of past gradients to estimate this bias without an oracle:
ct+1:= (1−βt)ct+βt/parenleftigN/summationdisplay
k=1τkgk(xt)−g0(xt)/parenrightig
.
Intuitively, this averages over ≈1
βpast independent stochastic bias estimates reducing the variance of ct.
We next examine the effect of replacing our bias oracle using such a ct.
Theorem 5.2 (Convergence of bias correction) .Under Assumptions A1 and A3–A5, Algorithm 3 for con-
stant collaboration weight αt:=α, constant step-size ηt:=η≤min(1
L,1
6α2δ2)satisfies the following:
Non-convex case. forβt= min(1,/parenleftbig10δ2(˜ζ2/T+σ2
0+σ2
a)
σ2
0+σ2a/parenrightbig1/3η2/3)we have:
1
4TT−1/summationdisplay
t=0E[∥∇f0(xt)∥2]≤F0
ηT+4α2E0
βT+12α2/parenleftbig
(σ2
0+σ2
a)(˜ζ2/T+σ2
0+σ2
a)/parenrightbig1/3(δη)2/3+Lσ2(α)
2η+10α2δ2σ2(α)η2.
µ-PL case. forβt= min(1,/parenleftbig
10δ2/parenrightbig1/3η2/3)we have:
FT≤(1−µη
2)TΦ0+Lσ2(α)
µη+ 24α2/parenleftbig
σ2
0+σ2
a/parenrightbig2/3(δη)2/3/µ.
where Φ0=F0+2α2η
βE0
c+10α2η
β2˜ζ,Ft=E[f0(xt)]−f⋆
0,E0=E[∥c0−∇f1(x0) +∇f0(x0)∥2],σ2(α) :=
(1−α)2σ2
0+α2σ2
a,˜ζ2:= 2(1 +m)(E[∥∇f0(x0)∥2] + 2ζ2),σ2
a=/summationtextN
k=1τ2
kσ2
kandζ2=/summationtextN
k=1τkζ2
k.
Discussion:
•Significance of the terms. In the non-convex inequality the first term in the inequality in theorem
5.2 measures how fast the initial condition is forgotten, the second term measures how the initial
bias estimation affects the optimization whereas the third term measures the effect of having used
noisy (and dependent on the past) estimates of the bias.
•Bias correction works We see that ζ2is divided by Twhich means that our bias correction
strategy works indeed in correcting the bias ζ2. However, using EMA adds the term 12α2/parenleftbig
(σ2
0+
σ2
a)(˜ζ2/T+σ2
0+σ2
a)/parenrightbig1/3(δη)2/3which is greater than the noise termLσ2(α)
2ηunless we limit ourselves
to collaborators with small Hessian dissimilarity δ.
•Condition on the dissimilarity δ.Theorem 5.2 shows that to gain from the collaboration (be
better than training alone) we need (δη)2/3≪η. Now if we fix T, The optimal ηin the non-convex
case is of order1√
T, then we would need δ2=o(1√
T)and in theµ-PL case the optimal ηscales as
1
Tso that we need δ2=o(1
T)in this case .
Remark. The condition on the similarity parameter δis reasonable since, in particular, it eliminates
adversarial agents that would have a big δ.
Choice of the weights τk.We show (See C.4) that as T→∞the best choice of these weights is completely
dictated by the variance term σ2
a. In particular there is always a choice such that σ2
a≤/summationtextN
k=1σ2
k
N2. This means
thatσ2
ascales as 1/Nso for simplification’s sake we suppose/summationtextN
k=1σ2
k
N≤σ2
0and replace σ2
abyσ2
0/N.
8Under review as submission to TMLR
Corollary 5.3 (linear speedup of BC) .Forσ2
0>0and a fixed horizon T, supposing that we have a
mechanism to select collaborators with δ2=o(1√
T)in the non-convex case and δ2=o(1
T)in theµ-PL case.
Then there is an appropriate choice of the weights α,{τk}for which, in leading order of Twe have :
Non-convex case. Forη= min(1/L,1/(6α2δ2),/radicalig
2F0
Lσ2(α)T):
1
TT−1/summationdisplay
t=0E[∥∇f0(xt)∥2] =O/parenleftbigg/radicaligg
LF0σ2
0
(N+ 1)T/parenrightbigg
.
µ-PL case. forη= min(1/L,1/(6α2δ2),log(max(2,2µΦ0T
3Lσ2(α))
µT):
FT=˜O/parenleftig
Φ0Lσ2
0
µ2(N+ 1)T/parenrightig
Remark. It is not hard to see that the quantities ζandδare "perpendicular" in the sense that δcan be
small andζvery big. For example, we can take f0(x) =1
2x2andf1(x) =1+δ
2(x−ζ
1+δ)2. Corollary 5.3
means that we can benefit optimally from all agents that have a small δirrespective of their bias ζ2.
Conclusion: BC solves "partially" the problems of WGA. From the above discussion, we see that
BC solves the problems WGA had with bias parameters mandζ2. First of all, there is no dependence on
the heterogeneity parameter m, in particular the collaboration weight αcan range freely in the interval [0,1].
Secondly, with BC, the bias ζ2does not accumulate with time. However, we only benefit optimally from our
EMA approach when the dissimilarity δbetween the collaborators is small δ2=o(1√
T)(No Free Lunch).
6 Experiments
To validate our theory we consider the noisy quadratic model i.e. optimizing a function of the type
f0(x) :=1
2(x−m⋆
0)⊤A0(x−m⋆
0),m⋆
0∼N(x⋆
0,Σ0).
While simple, this model can serve as an illustrative test for our theory and is often used to test machine
learning and federated learning algorithms (Schaul et al., 2013; Wu et al., 2018; Martens & Grosse, 2015;
Zhang et al., 2019). One common simplification is to consider both A0andΣ0to be diagonal (or co-
diagonalizable). This assumption makes it possible to optimize the function f0over each of its dimensions
independently. So it suffices to consider a noisy quadratic model in 1D: optimizing f0(x) :=1
2a0(x−x⋆
0+
ξ0
a0)2,ξ0∼N(0,σ2)by collaborating with favg(x) :=1
2a1(x−x⋆
1+ξ1
a1)2,ξ1∼N(0,σ2
N). Here, we have Nas
the number of collaborators, δ=∥a0−a1∥, andζ2=∥a1(x⋆
1−x⋆
0)∥2. The quantity f0,test=1
2a0(x−x⋆
0)2
can be interpreted as a test loss (called simply loss in the plots). In our plots we use by default δ= 1,ζ= 4
andσ= 10.
Convergence speed. Figure 1 shows convergence curves of the three competing algorithms we have
discussedbefore: workingalone, weightedgradientaveraging(WGA),andbiascorrection(BC).Inparticular,
we see that BC reaches a lower error level compared to both other algorithms. This confirms our theory that
BC reduces the bias in the algorithm enabling it to reach a lower error level. The initial increase in the loss
is also characteristic of BC and is because during the initial stages our EMA estimate of the bias is quite
poor. Eventually, the bias estimate improves and we get fast convergence.
Dependence on data heterogeneity. Figure 2 shows how the bias parameter ζ2influences the per-
formance of BC. As predicted by the theory, we see that BC always converges to the same error level
uninfluenced by ζ2. This bias only effects the time horizon needed for convergence. In contrast, WGA is
strongly influenced by the bias as we see in Figure 3. In fact, the convergence error level of WGA is directly
proportional to α2ζ2, meaning that we would need to set α= 0(i.e train alone) to ensure low error. This
demonstrates that the bias correcting technique employed by BC indeed succeeds, validating our theory.
9Under review as submission to TMLR
0 100000 200000 300000 400000 500000
iterations103
102
101
100lossComparing BC to WGA and training alone
Alone
BC
WGA
Figure 1: Comparing Bias Correction (orange) to WGA (green) and training alone (blue). BC achieves a
lower loss than training alone or using WGA. The step sizes were tuned for training alone and WGA, but
not for BC.
0 100000 200000 300000 400000 500000
iterations102
100102104106108losseffect of  on BC
=10
=100
=1000
=10000
=100000
Figure 2: Effect of the bias ζon the convergence of BC for a fixed choice of step-size η= 10−4, BC weight
β= 10−4and collaboration weight α=N
N+1, whereN= 10. We can see that ζinfluences the time needed
for convergence but eventually all curves converge to the same error level.
Dependence on the number of collaborators. Figure 4 shows how the number of collaborators N
influences the convergence of BC for a relatively big δ= 1. We see that increasing Ndoes have a positive
effect on BC and decreases the error level to which it converges. However, the benefit saturates quickly.
While there is a substantial improvement from N= 1toN= 10, the rest only sees negligible improvement.
10Under review as submission to TMLR
0 20000 40000 60000 80000 100000
iterations102
101
100101102103losseffect of  on WGA
=10
=100
=1000
=10000
=100000
Figure 3: Effect of the bias ζon the convergence of WGA for a fixed choice of step-size η= 5×10−4,
collaboration weight α= 10−3andN= 10. We can see that the bigger ζis the bigger the final loss will be.
In fact, WGA can only converge up to O(α2ζ2).
0 100000 200000 300000 400000 500000
iterations103
102
101
100lossEffect of N on BC
N=1
N=10
N=20
N=50
Figure 4: Effect of the number of collaborators Non the convergence of BC for a fixed choice of step-size
η= 5×10−4, BC weight β= 10−4, collaboration weights α=N
N+1andδ= 1(not very small). We can see
that increasing Ndoes improve the level to which BC converges, due to the smaller variance with larger N.
We expect this to result from using a big δsince our theory only predicts linear speedup in Nforδvery
small.
11Under review as submission to TMLR
7 Limitations and Extensions
Bias and generalization. We have proposed a strategy to correct for the ”gradient” bias between the
main agent and its collaborators, but in doing so we have put a lot of faith in the ”quality” of the main
agent’s gradients. However, in the case where the main agent has a very limited dataset, some bias might
be good to make out for the lack of data.
Bias Correction in deep learning. In this work we have employed the idea of gradient bias correction
using SGD. Our methods can also be extended to other optimizers such as momentum or Adam. A larger
empirical exploration of such algorithms, as well as more real-world deep learning experiments, would be
valuable but is out of scope for our more theoretical work.
Adding local steps. Currently, the users communicate with each other after every gradient computation.
This is a problem for Federated Learning (which is not the aim of this paper). More communication-
efficient schemes can be developed by instead allowing multiple local steps before communication, such as
in FedAvg (McMahan et al., 2017). Similarly, extending our algorithms to allow personalization for all users
instead of focusing only on user 0 would improve practicality in the federated learning setting.
Fine-grained measures of similarity. Our algorithms, as well as the assumptions, use static global
measures of dissimilarity. Time-varying adaptive weighting strategies such as cosine similarity between
gradients may further improve our algorithms. Using individual user-level similarities, such as in (Grimberg
et al., 2020) would also be a fruitful extension. Similarity-based user selection rules are also closely related
to Byzantine robust learning, where they are used to exclude malicious participants (Blanchard et al., 2017;
Baruch et al., 2019; Karimireddy et al., 2021).
8 Conclusion
In this work, we have introduced the collaborative stochastic optimization framework where one "main” user
collaborates with a set of willing-to-help collaborators. We considered the simplest method to solve this
problem: using SGD with weighted gradient averaging . We discussed in detail the limitations of this idea
arising mainly due to the bias introduced by the collaboration. To solve this bias problem, we proposed
a second algorithm bias correction . We showed that our bias correction algorithm manages to remove the
effect of this bias and, under some optimal choices of its parameters, leads to a linear speedup as we increase
the number of collaborators.
12Under review as submission to TMLR
References
AhmadAjalloeianandSebastianU.Stich. Ontheconvergenceofsgdwithbiasedgradients. arXiv:2008.00051
[cs.LG], 2020.
Navon Aviv, Achituve Idan, Maron Haggai, Chechik Gal, and Fetay Ethan. Auxiliary learning by implicit
differentiation. ICLR 2021. URL https://arxiv.org/pdf/2007.02693.pdf .
Shi Baifeng, Hoffman Judy, Saenko Kate, Darrell Trevor, and Xu Huijuan. Auxiliary task reweighting for
minimum-data learning. 34th Conference on Neural Information Processing Systems (NeurIPS 2020),
Vancouver, Canada. URL https://arxiv.org/pdf/2010.08244.pdf .
Moran Baruch, Gilad Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for distributed
learning. arXiv preprint arXiv:1902.06156 , 2019.
Martin Beaussart, Felix Grimberg, Mary-Anne Hartley, and Martin Jaggi. Waffle: Weighted averaging for
personalized federated learning. In NeurIPS 2021 Workshop on New Frontiers in Federated Learning .
2021.
Dimitri Bertsekas. Nonlinear Programming . Athena scientific, 2002.
Dimitri P. Bertsekas and John N. Tsitsiklis. Gradient convergence in gradient methods with errors. SIAM
Journal on Optimization , 10(3):627–642, 2000.
Peva Blanchard, El Mahdi Mhamdi, Rachid Guerraoui, and Julien Stainer. Byzantine-tolerant machine
learning. arXiv preprint arXiv:1703.02757 , 2017.
El Mahdi Chayti and Sai Praneeth Karimireddy. Optimization with access to auxiliary information.
arXiv:2206.00395 [cs.LG] https: // arxiv. org/ abs/ 2206. 00395 ], 2022.
Liam Collins, Aryan Mokhtari, and Sanjay Shakkottai. Why does maml outperform erm? an optimization
perspective. arXiv preprint arXiv:2010.14672 , 2020.
Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd.
arXiv:1905.10018 [cs.LG] , 2019.
A. Defazio, F. Bach, and S. Lacoste-Julien. Saga: A fast incremental gradient method with support for
non-strongly convex composite objectives. In NIPS 27, pages 1646—1654. , 2014.
Y. Deng, M. M. Kamani, and M. Mahdavi. Adaptive personalized federated learning. arXiv:2003.13461 [cs,
stat], 2020.
K. Donahue and J Kleinberg. Model-sharing games: Analyzing federated learning under voluntary partici-
pation.arXiv preprint arXiv:2010.00753 , 2020.
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-learning
approach. arXiv preprint arXiv:2002.07948 , 2020.
Zhili Feng, Shaobo Han, and Simon S Du. Provable adaptation across multiway domains via representation
learning. arXiv preprint arXiv:2106.06657 , 2021.
Felix Grimberg, Mary-Anne Hartley, Martin Jaggi, and Sai Praneeth Karimireddy. Weight erosion: An
update aggregation scheme for personalized collaborative machine learning. In MICCAI Workshop on
Distributed and Collaborative Learning , pp. 160–169. 2020.
FelixGrimberg, Mary-AnneHartley, SaiPraneethKarimireddy, andMartinJaggi. Optimalmodelaveraging:
Towards personalized collaborative learning. ICML Workshop on Federated Learning for User Privacy and
Data Confidentiality https: // fl-icml. github. io/ 2021/ papers/ FL-ICML21_ paper_ 56. pdf , 2021.
Filip Hanzely and Peter Richtárik. Federated learning of a mixture of global and local models.
arXiv:2002.05516 [cs.LG] , 2020.
13Under review as submission to TMLR
Filip Hanzely, Boxin Zhao, and Mladen Kolar. Personalized federated learning: A unified framework and
universal optimization techniques. arXiv:2102.09743 [cs.LG] , 2021.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction.
NeurIPS , 2013.
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles,
G. Cormode, R. Cummings, R. G. L. D’Oliveira, S. E. Rouayheb, D. Evans, J. Gardner, Z. Garrett,
A. Gascón, B. Ghazi, P. B. Gibbons, M. Gruteser, Z. Harchaoui, C. He, L. He, Z. Huo, B. Hutchinson,
J. Hsu, M. Jaggi, T. Javidi, G. Joshi, M. Khodak, J. Konecný, A. Korolova, F. Koushanfar, S. Koyejo,
T. Lepoint, Y. Liu, P. Mittal, M. Mohri, R. Nock, A. Özgür, R. Pagh, M. Raykova, H. Qi, D. Ramage,
R. Raskar, D. Song, W. Song, S. U. Stich, Z. Sun, A. T. Suresh, F. Tramèr, P. Vepakomma, J. Wang,
L. Xiong, Z. Xu, Q. Yang, F. X. Yu, H. Yu, , and S. Zhao. Advances and open problems in federated
learning. arXiv:1912.04977 [cs, stat] , 2019.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear Convergence of Gradient and Proximal-Gradient
Methods Under the Polyak-Łojasiewicz Condition. In ECML - European Conference on Machine Learning
and Knowledge Discovery in Databases - Volume 9851 , pp. 795–811, 2016.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich,
and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning.
arXiv:1910.06378v4[cs.LG] , 2019.
Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich,
and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in federated learning.
arXiv:2008.03606 [cs.LG] , 2020.
Sai Praneeth Karimireddy, Lie He, and Martin Jaggi. Learning from history for byzantine robust optimiza-
tion. InInternational Conference on Machine Learning , pp. 5311–5319. PMLR, 2021.
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta-learning meth-
ods.arXiv preprint arXiv:1906.02717 , 2019.
Jakub Konecny, H. Brendan McMahan, Daniel Ramage, and Peter Richtarik. Federated optimization :
Distributed machine learning for on-device intelligence. arxiv.org/abs/1610.02527 , 2016.
Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Sample efficient linear
meta-learning by alternating minimization. arXiv e-prints , pp. arXiv–2105, 2021.
Viraj Kulkarni, Milind Kulkarni, and Aniruddha Pant. Survey of personalization techniques for federated
learning. arXiv:2003.08673 [cs.LG] , 2020.
T. Li, A. K. Sahu, A. Talwalkar, and V. Smith. Federated learning: Challenges, methods, and future
directions. IEEE Signal Processing Magazine, 37(3):50–60 , 2020a.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning
through personalization. arXiv:2012.04221 [cs.LG] , 2020b.
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning.
InICLR - International Conference on Learning Representations , 2020c.
Y.Mansour, M.Mohri, andA.T.Suresh. Threeapproachesforpersonalizationwithapplicationstofederated
learning. arXiv:2002.10619 [cs, stat] , 2020.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored ap- proximate cur-
vature.In International conference on machine learning, pages 2408–2417 , 2015.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask represen-
tation learning. Journal of Machine Learning Research , 17(81):1–32, 2016.
14Under review as submission to TMLR
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of
deep networks from decentralized data. In Proceedings of AISTATS, pp. 1273–1282 , 2017.
Mohamad Mestoukirdi, Matteo Zecchin, David Gesbert, Qianrui Li, and Nicolas Gresset. User-centric
federated learning. arXiv:2110.09869 [cs.LG] , 2021.
M. Mohri, G. Sivek, and A. T. Suresh. Agnostic federated learning. arXiv preprint arXiv:1902.00146 , 2019.
A. Nedic. Distributed gradient methods for convex machine learning problems in networks: Distributed
optimization. IEEE Signal Processing Magazine, 37(3):92–101 , 2020.
Sashank J Reddi, Jakub Konečn` y, Peter Richtárik, Barnabás Póczós, and Alex Smola. Aide: Fast and
communication efficient distributed optimization. arXiv preprint arXiv:1608.06879 , 2016.
Luo Ruichen, Hu Shoubo, and Yu Lequan. Rethinking client reweighting for selfish federated learning.
InSubmitted to The Tenth International Conference on Learning Representations , 2022. URL https:
//openreview.net/forum?id=qfGcsAGhFbc .
Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In International Conference on
Machine Learning, pages 343–351 , 2013.
M. Schmidt, N. Le Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient.
arXiv:1309.2388 [math.OC] , 2013.
Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization using an ap-
proximate newton-type method. In International conference on machine learning , pp. 1000–1008. PMLR,
2014.
Nilesh Tripuraneni, Michael I Jordan, and Chi Jin. On the theory of transfer learning: The importance of
task diversity. arXiv preprint arXiv:2006.11650 , 2020.
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen
Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated optimization.
arXiv preprint arXiv:2107.06917 , 2021.
K.Wang, R.Mathews, c.Kiddon, H.Eichner, F.Beaufays, andD.Ramage. Federatedevaluationofon-device
personalization. arXiv preprint arXiv:1910.10252 , 2019.
Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse. Understanding short-horizon bias in stochastic
meta-optimization. arXiv preprint arXiv:1803.02021 , 2018.
Lin Xingyu, Singh Baweja Harjatin, Kantor George, and Held David. Adaptive auxiliary task weighting
for reinforcement learning. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019),
Vancouver, Canada. URL https://openreview.net/pdf?id=rkxQFESx8S .
Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. Salvaging federated learning by local adaptation.
arXiv preprint arXiv:2002.04758 , 2020.
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E Dahl, Christopher J
Shallue, and Roger Grosse. Which algorithmic choices matter at which batch sizes? insights from a noisy
quadratic model. arXiv preprint arXiv:1907.04164 , 2019.
Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M. Alvarez. Personalized federated
learning with first order model optimization. ICLR2021 , 2021.
15Under review as submission to TMLR
A More related work and discussion
A.1 Related work
In personalized Federated Learning, a prominent approach consists in using a local-global interpolation such
as in (Hanzely & Richtárik, 2020) which proposes to use a consensus-like regularization to make such an
interpolation, however, they only give convergence of the global model. In a second work (Hanzely et al.,
2021) study the problem of optimizing an objective that has both local and global parameters, they propose
an SVRG-like algorithm to reduce the variance of local gradient estimates. We reiterate that our goal differs
from that of Federated Learning, we care about the performance of one particular agent, and the bias we
have is inherent to collaborating with different agents, it is not a result of using local steps as in Federated
Learning. Also, our bias correction method has the main goal to reduce the bias, the reduction in variance
is a result of averaging and further using an exponential moving average to reduce the variance of our bias
estimates. (Li et al., 2020b) discusses variance trade-offs for point estimation and linear regression problems,
our results are more general from this perspective.
In the personalized optimization setting, two very recent empirical works propose rules to learn collaboration
weights. Beaussart et al. (2021) modify Scaffold (Karimireddy et al., 2019) to use Euclidean distances of
the updates between different agents to derive a heuristic for weight definition. It uses both local and global
control variates, though without a decay mechanism. (Zhang et al., 2021) on the other hand uses an idea
from meta-learning to learn the collaboration weights, by using a first-order approximation of the objective
with respect to these weights. While demonstrating practical performance on deep learning tasks, neither
of the two methods comes with convergence guarantees. Our approach in contrast chooses collaboration
weights to achieve provable convergence as well as speedup with the number of workers.
A.2 Comparison with other control variate techniques
Control variates have been used extensively in variance reduction techniques such as SVRG (Johnson &
Zhang, 2013), SAGA (Defazio et al., 2014), SAG (Schmidt et al., 2013), MVR (Cutkosky & Orabona, 2019).
The main idea is the following: given an unbiased gradient estimate g(x)atx, to reduce its variance we
replaceg(x)byg(x)−y+E[y]where yis a random variable that correlates positively with g(x), this
is true for all the methods cited before except for SAG which does not bother to keep the new gradient
estimate unbiased. This idea is used in Federated Learning to correct for the bias introduced by the use of
local steps, In SCAFFOLD (Karimireddy et al., 2019) for example, g(x)is theith client gradient estimate
atxthe current local model, and yis the gradient estimate of the same client but at the last received
server model zandE[y]is client average true gradient at z. Thus the bias of this new gradient estimate is
∇fi(x)−∇f(x)−∇f(z) +∇f(z), by assuming fi−fhas a hessian bounded by δit is easy to see that the
norm of the bias will be bounded by δ∥x−z∥, all that is left is to efficiently bound this norm ∥x−z∥.
In our case, the bias does not come from local steps but is a result of collaborating with potentially different
agents. We note here that our goal is different from that of Federated Learning which aims to train the
average model, whereas we train one model using/collaborating with other models and we only care about
local performance. Again, the bias is inherent to the collaboration, our solution to reduce it is estimating
the future bias based on past observed biases and then subtracting it from the current gradient estimate.
For simplicity, let’s discuss the case β= 1which means we only use the last observed bias to estimate the
currentbias. Inthiscase,thebiasofourcorrectedgradientis: α(∇f1(xt)−∇f0(xt)−∇f1(xt−1)+∇f0(xt−1))
, using the bounded hessian dissimilarity assumption, it is easy to see that the norm of this quantity is
bounded by αδ∥xt−xt−1∥, if we can efficiently bound this quantity, the convergence proof will be easy. It
turned out that using only the last observed bias as a bias estimate incurs the same additional variance in
all steps, to solve this we propose the use of an exponential average of all past observed biases.
One important idea about these approaches that use bounded hessian dissimilarity and lead to bounding
the bias as above is that this makes it possible to control the bias of the model indirectly by controlling the
step size.
16Under review as submission to TMLR
B Relaxing Noise Assumptions
We start by relaxing our assumptions about the noise. In general, we can make the following assumptions:
First relaxation of A5 (Bounded variance) for each agent k∈{0,...,N}∃Mk,σ2
k≥0s.t.∀x∈Rd:
/braceleftig
E[∥nk(x,ξ(k)
t)∥2]≤Mk∥∇xfk(x)∥2+σ2
k.
The quantity σ2
kis the variance of collaborator’s gradient estimates when agent khas converged to a sta-
tionary point. Using this new assumption with the gradient dissimilarity assumption:
A4 (Gradient Similarity) ∃m,ζ2
k≥0s.t.∀x∈Rd:
∥∇xfk(x)−∇xf0(x)∥2≤m∥∇xf0(x)∥2+ζ2
k.
Now if we denote favg=/summationtextN
k=1τkfkandnavgthe variance associated to its gradient estimate, we have :


E[∥navg(x,ξi=(1...N)
t )∥2]≤Mavgm′∥∇xf0(x)∥2/N+ ˜σ2
avg/N,
˜σ2
avg =N/summationtextN
k=1τ2
k˜σ2
k,
˜σ2
k =σ2
k+ 2Mkζ2
k,
Mavg = 2N/summationtextN
k=1τ2
kMk,
m′= 2(1 +m).
The quantity ˜σ2
avgmeasures the average variance of collaborators’ gradient estimates this time when agent
"0" has converged to a stationary point. Mkζ2
kis the variance resulting from collaborator kbeing biased
from agent 0 and thus converging to a different minimizer. We can argue that when the hessian dissimilarity
parameterδ= 0i.e. each collaborator fkis a translated copy of f0then the noise will not be changed from
its original level by translation (adding a constant to a random variable does not change its variance) and
thusMkζ2
kshould be replaced by a quantity that is proportional to the parameter δ. This motivates the
final form of our assumption:
Final form of A5 (Bounded variance) ∃σ2
k,D2
k≥0s.t.∀x∈Rd:


E[∥navg(x,ξi=(1...N)
t )∥2]≤Mavgm′∥∇xf0(x)∥2/N+ ˜σ2
avg/N,
˜σ2
avg =N/summationtextN
k=1τ2
k˜σ2
k,
˜σ2
k =σ2
k+ 2δMkD2
k,
Mavg = 2N/summationtextN
k=1τ2
kMk,
m′= 2(1 +m).
D2
kis a constant that can be interpreted as a diameter of the parameter space for agent k.
We note that we can still safely go to the other forms of this assumption without affecting the proofs, we
can always replace δD2
kbyζ2
kin our next result if the reader is not convinced by the dependence of the noise
with respect to δ, and we can replace ˜σ2
avgbyσ2
avg=N/summationtextN
k=1τ2
kσ2
kif we don’t want to make the noise of the
collaborators when agent "0" has converged depend on their bias.
We will do the proofs for only N= 1and without taking into account the dependence of the noise of agent
"1" on its bias with respect to "0". To be explicit, for a collaboration with one agent "1" we make the following
assumption on the noise:
/braceleftigg
E[∥n0(x,ξ(0)
t)∥2]≤M0∥∇xf0(x)∥2+σ2
0,
E[∥n1(x,ξi=(1)
t)∥2]≤M1m∥∇xf0(x)∥2+σ2
1.
This will not make us lose any generality since we can replace M1byMavg/Nandσ2
1byσ2
avg/Nor˜σ2
avg/N.
Furthermore, we would also need to replace ζ2by/summationtextN
k=1τkζ2
k.
17Under review as submission to TMLR
C Missing Proofs
C.1 SGD with biased gradients
If we are optimizing an L-smooth function f0onRdusing SGD iterations xt+1=xt−ηtg(xt)with a gradient
that can be written in the form
g(xt) =∇xf0(xt) +b(xt)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
bias+nt/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
noise
Then denoting Ft=E[f0(xt)]−f⋆
0, we have for ηt≤1/L:
Ft+1−Ft≤η
2(−∥∇ xf0(xt)∥2+∥b(xt)∥2) + +Lη2
2E[∥nt∥2] (3)
Proof.Using theL-smoothness of f0we have:
E[f0(xt+1)]−f0(xt)≤⟨∇ xf0(xt),E[xt+1−xt]⟩+L
2Eξt[∥xt+1−xt∥2]
=−η⟨∇xf0(xt),E[g(xt)]⟩+L
2η2Eξt[∥g(xt)∥2]
=−η⟨∇xf0(xt),∇xf0(xt) +b(xt)⟩+L
2η2(∥(∇xf0(xt) +b(xt)∥2+E[∥nt∥2])
UsingLη≤1 :
E[f0(xt+1)]−f0(xt)≤η
2(−2⟨∇xf0(xt),∇xf0(xt) +b(xt)⟩+∥∇xf0(xt) +b(xt)∥2) +Lη2
2E[∥nt∥2])
=η
2(−[∥∇xf0(xt)∥2+∥b(xt)∥2) +Lη2
2E[∥nt∥2]
Taking an overall expectation yields the desired result.
All of the proofs will use this inequality as a starting point.
C.2 Proof of Theorem 4.1
In this section, we present the detailed proof of Theorem 1 i.e the convergence of WGA for both the non-
convex and µ-PL case.
We denote n(x,ξt) = (1−α)n(x,ξ(0)
t) +αn1(x,ξ(1)
t)the noise of the weighted average.
Bounding the average noise. Using Assumption A5(Bounded noise), we can bound the noise in the
following way:
Eξ[∥n(x,ξt)∥2]≤M(α)∥∇xf0(x)∥2+ ˜σ2(α),
Where ˜σ2(α) := (1−α)2σ2
0+α2σ2
1andM(α) := (1−α)2M0+α2M1m≤M=M0+M1m.
Proof.
Eξ[∥n(x,ξt)∥2] = (1−α)2Eξ(0)
t[∥n0(x,ξ(0)
t)∥2] +α2Eξ(1)
t[∥n1(x,ξ(1)
t)∥2]
≤(1−α)2{M0∥∇xf0(x)∥2+σ2
0}+α2{M1m∥∇xf0(x)∥2+σ2
1}
≤M(α)∥∇xf0(x)∥2+ ˜σ2(α).
18Under review as submission to TMLR
Main inequality. Now denoting Ft=E[f0(xt)]−f⋆
0, forη≤1/L, we have :
Ft+1−Ft≤η
2(−1 +α2m+LMη )E/bracketleftig
∥∇xf0(xt)∥2/bracketrightig
+ηα2
2ζ2+Lη2
2˜σ2(α)
Proof.WithL-smoothness of f0andηL≤1we can use (3) with b(xt) =α(∇xf1(xt)−∇xf0(xt))and
nt=n(x,ξt)
the Bounded Gradient Dissimilarity assumption ( A4) lets us upper-bound the term ∥b(xt)∥2≤
α2(m∥∇xf0(x)∥2+ζ2).
Eξt[f0(xt+1)]−f0(xt)≤η
2(−∥∇ xf0(xt)∥2+α2∥∇xf0(xt)−∇xf1(xt)∥2) +Lη2
2(M∥∇xf0(x)∥2+ ˜σ2(α))
≤η
2(−1 +α2m+LMη )∥∇xf0(xt)∥2+ηα2
2ζ2+Lη2
2˜σ2(α)
All that is left is to take an overall expectation.
Now ifM=M0+mM 1̸= 0, then we choose η≤1−α2m
2LMwhich gives
Ft+1−Ft≤−η
4(1−α2m)∥∇xf0(xt)∥2+ηα2
2ζ2+Lη2
2˜σ2(α)
And ifM= 0, then we get
Ft+1−Ft≤−η
2(1−α2m)∥∇xf0(xt)∥2+ηα2
2ζ2+Lη2
2˜σ2(α)
We combine these two inequalities into one:
Ft+1−Ft≤−η
c(1−α2m)∥∇xf0(xt)∥2+ηα2
2ζ2+Lη2
2˜σ2(α) (4)
The constant cis equal to 2ifM= 0and equal to 4otherwise. This constant is not very important since
we can always choose the step-size ηsmall enough to make cclose to 1.
Remark. We need 1−α2m>= 0i.e.α≤1/√mif this bound is to guarantee any convergence.
Non-convex case of Theorem 4.1. To prove the non-convex result, it suffices to rearrange the terms in
(4), sum for t= 0tot=T−1and divide by T. This manipulation gives:
(1−α2m)
cTT−1/summationdisplay
t=0E/bracketleftbig
∥∇f0(xt)∥2/bracketrightbig
≤1
ηTT−1/summationdisplay
t=0(Ft−Ft+1) +Lη
2˜σ2(α) +1
2α2ζ2
≤F0
ηT+Lη
2˜σ2(α) +1
2α2ζ2
This is true for all η≤ηmax:= min/parenleftig
1
L,1−α2m
2LM/parenrightig
.
Choosingη= min/parenleftig
ηmax,/radicalig
2F0
L˜σ2T/parenrightig
leads to the following result:
1−α2m
cTT−1/summationdisplay
t=0E/bracketleftbig
∥∇f0(xt)∥2/bracketrightbig
≤F0
ηmaxT+/radicalbigg
2LF0˜σ2(α)
T+1
2α2ζ2.
µ-PL case of Theorem 4.1. To prove the µ-PL result, we start from (4), we use Assumption A2i.e.f0
satisfies the µ-PL condition:∀x∈Rd∥∇xf0(xt)∥2≥2µ(f0(x)−f⋆
0), this yields:
Ft+1≤/parenleftig
1−2µη
c(1−α2m)/parenrightig
Ft+ηα2
2ζ2+Lη2
2˜σ2(α) (5)
19Under review as submission to TMLR
Repeating (5) recursively we get:
FT≤/parenleftig
1−2µη
c(1−α2m)/parenrightigT
F0+/parenleftigηα2
2ζ2+Lη2
2˜σ2(α)/parenrightigT−1/summationdisplay
i=0/parenleftig
1−2µη
c(1−α2m)/parenrightigi
≤/parenleftig
1−2µη
c(1−α2m)/parenrightigT
F0+/parenleftigηα2
2ζ2+Lη2
2˜σ2(α)/parenrightigc
2µη(1−α2m)
=/parenleftig
1−2µη
c(1−α2m)/parenrightigT
F0+cα2
4µ(1−α2m)ζ2+cLη
4µ(1−α2m)˜σ2(α)
Choosing 2(1−α2m)η/c= min/parenleftigg
ηmax,log(max(2,2µF0T
3L˜σ(α)2))
2µT/parenrightigg
we get:
FT=˜O/parenleftbigg
F0exp/parenleftbig
−µηmaxT/parenrightbig
+L˜σ(α)2
µ2T(1−α2m)2+α2
µ(1−α2m)ζ2/parenrightbigg
.
This concludes the proof of Theorem 1 in the µ-PL case.
In the article, we argued that we can get rid of the logarithmic factors hidden in the notation ˜O. We show
now how to do it for the µ-PL case.
µ-PLwithadecreasingstep-size. startingfrom(5), wechooseastepsize ηtsuchthat 1−2µηt
c(1−α2m) =
t2
(t+1)2, this means ηt=c(2t+1)
2µ(1−α2m)(t+1)2, this choice transforms (5) into
(t+ 1)2Ft+1≤t2Ft+c(2t+ 1)α2
4µ(1−α2m)ζ2+c2L(2t+ 1)2
8µ2(1−α2m)2(t+ 1)2˜σ2(α)
Summing the last inequality for t= 0tot=T−1, and using the fact/summationtextT−1
t=02t+1 =T2and2t+1≤2(t+1),
we get:
T2FT≤cT2α2
4µ(1−α2m)ζ2+c2LT
2µ2(1−α2m)2˜σ2(α)
Dividing by T2:
FT≤cα2
4µ(1−α2m)ζ2+c2L
2µ2(1−α2m)2T˜σ2(α)
This indeed is the same rate but without any hidden logarithmic factors in T.
To be rigorous, we need to make sure that our decreasing step-size verifies ηt≤ηmax, this will mean we can’t
sum starting from t= 0, but instead we need to start from t=t0such thatηt0≤ηmaxis verified. Doing
this will lead to
FT≤cα2
4µ(1−α2m)ζ2+c2L
2µ2(1−α2m)2T˜σ2(α) +t2
0Ft0
T2
In the general case where we are collaborating with N agents and using the weights {τk}N
k=1, as discussed
before, it suffices to replace M1byMavg/Nandσ2
1byσ2
avg/N.
Choice of the weights {τk}N
k=1.Based on the µ-PL bound, the best choice of the weights {τk}N
k=1is given
by the following constrained quadratic programming problem :
min
τ1≥0,...,τN≥0,/summationtext
jτj=1N/summationdisplay
k=1L
µT(1−α2m)τ2
kσ2
k+τkζ2
k,
AsT→∞, the program becomes that of minimizing the average bias i.e.
min
τ1≥0,...,τN≥0,/summationtext
jτj=1N/summationdisplay
k=1τkζ2
k,
20Under review as submission to TMLR
102
101
100101
L2
0/T2
102
101
100101N2.0005.0007.000optimal speedup for m=0
246810
1
(1 opt)
Figure 5: Collaborative training speedup factor 1/(1−αopt), indicated as color, as a function of the number
of collaborators N(y-axis) andLσ2
0
µTζ2(x-axis) for m= 0. The bigger Nand the smaller Tζ2(cumulative
bias) is relative to σ2
0, the bigger the resulting speedup from collaboration.
The solution to this problem is easy, only the agents who have the smallest bias will get a non-zero weight.
However, for Tfinite, the term/summationtextN
k=1τ2
kσ2
kalso plays a role and the weights should be taken to minimize it
too. What is important is that as expected, the smaller ζ2
kandσ2
kare the bigger the weight it will be given
to agentk.
To study the effect of Non the convergence rate, we will pick a middle ground where σ2
k=σ2
0andζk=ζ
for all agents k.
Choice of the collaboration weight α.The collaboration weight αis chosen as follows:
α∈arg min
α∈(0,1/√m)L˜σ(α)2
µ2T(1−α2m)2+α2
µ(1−α2m)ζ2
Form= 0, which means the bias is bounded, we have αopt= (1 +1
N+µζ2T
Lσ2
0)−1and we obtain a speed-up
FT=˜O/parenleftig
Lσ2
0
2µ2T(1−αopt)/parenrightig
. The speedup factor 1/(1−αopt)is illustrated in Figure 5.
We note in particular that for ζ2= 0, the speedup is linear, and only, in this case, do we get such a speedup.
Now ifm̸= 0and even in the favorable case ζ2= 0, Figure 6 shows how much we deviate from linear
speedup (obtained for m= 0) asmis different than zero.
21Under review as submission to TMLR
101102
N+1101102speedupspeedup for different values of m and =0
m=0.2
m=0.5
m=1
m=0 (linear)
Figure 6: Effect of m(which controls the non-constant noise and is related to scaling) on the speedup of
WGA when ζ= 0(avg converges to the same point as 0). The dashed line represents the linear speedup
N+ 1∝⇕⊣√∫⊔≀→N+ 1encountered for m= 0(N+ 1is the total number of agents including 0). We notice that as
mgrows the speedup becomes more and more sub-linear.
C.3 Proof of Theorem 5.1
We will use a bias oracle on only one agent. The bias oracle gives an independent noisy estimate of the true
gradient bias between agent 1 and agent 0. This bias oracle is given by ct,oracle =∇xf1(x)−∇xf0(x) +
nt,oraclewhere nt,oracleis an independent noise of variance v2. Using such an oracle means we are working
with an unbiased estimate of ∇xf0(x)with a variance equal to ˜σ2(α) = (1−α)2σ2
0+α2(σ2
1+v2).
Now using (3) and Lη≤1, we get:
Eξt[f0(xt+1)]−f0(xt)≤−η
2∥∇xf0(xt)∥2+ +Lη2
2(M∥∇xf0(x)∥2+ ˜σ2(α))
≤η
2(−1 +LMη )∥∇xf0(xt)∥2+Lη2
2˜σ2(α)
Forη≤1
2MLwe get:
Ft+1−Ft≤−η
cE/bracketleftig
∥∇xf0(xt)∥2/bracketrightig
+Lη2
2˜σ2(α) (6)
Where the constant c= 2ifM= 0andc= 4otherwise.
Non-convex case of Theorem 5.1. We rearrange the terms in (6), sum for t= 0tot=T−1and divide
byT, we get∀η≤ηmax:= min(1
L,1
2ML),
1
cTT−1/summationdisplay
t=0E/bracketleftig
∥∇xf0(xt)∥2/bracketrightig
≤F0
η+Lη
2˜σ2(α).
22Under review as submission to TMLR
Choosingη= min/parenleftig
ηmax,/radicalig
2F0
L˜σ2T/parenrightig
, we get:
1
cTT−1/summationdisplay
t=0E/bracketleftig
∥∇xf0(xt)∥2/bracketrightig
≤F0
ηmaxT+/radicalbigg
2LF0˜σ2(α)
T.
µ-PL case of Theorem 5.1. We use Assumption A2, to have for all η≤ηmax= min(1
L,1
2ML),
Ft+1≤(1−2ηµ
c)Ft+Lη2
2˜σ2(α). (7)
A recurrence on (7) yields:
FT≤(1−2ηµ
c)TF0+Lη
2˜σ2(α)T−1/summationdisplay
i=0(1−2ηµ
c)i≤(1−2ηµ
c)TF0+cLη
4µ˜σ2(α)
All is left is to set 2η/c= min/parenleftigg
ηmax,log(max(2,2µF0T
3L˜σ(α)2))
2µT/parenrightigg
to get:
FT=˜O/parenleftbigg
F0exp/parenleftbig
−µηmaxT/parenrightbig
+L˜σ(α)2
µ2T/parenrightbigg
.
C.4 Proof of Theorem 5.2
The gradient estimator used in our bias correction algorithm g(xt) := (1−αt)g0(xt) +αt(g1(xt)−ct)can
be decomposed into a bias term and a noise term in the following way
g(xt) :=∇xf0(xt) +αE[bt−ct]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
bias+nt,total/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
noise
Where bt=g1(xt)−g0(xt)is the observed stochastic gradient bias at time t. Using the L-smoothness of
f0andη<1/L, (3) would give us the following inequality:
Ft+1−Ft≤η
2/parenleftig
−E/bracketleftig
∥∇f0(xt)∥2/bracketrightig
+α2E/bracketleftig
∥E[bt−ct]∥2/bracketrightig/parenrightig
+Lη2
2E/bracketleftig
∥nt,total∥2/bracketrightig
However, due to the dependence of cton the past, this is not true . For this reason, we use a different
proof strategy.
We have:
g(xt) = (1−α)g0(xt) +αg1(xt)−αct
Where
ct= (1−β)ct−1+β(g1(xt−1)−g0(xt−1))
Descent Lemma. Using theL-smoothness of f0we have :
f0(xt+1)−f0(xt)≤−η⟨∇f0(xt),g(xt)⟩+Lη2
2∥g(xt)∥2
2
Due to the dependence of xtonct, we cannot take the expectation inside the inner-product. However, if we
condition on the past (it will be denoted Et) then ctis constant and we have :
Et⟨∇f0(xt),g(xt)⟩=⟨∇f0(xt),(1−α)∇f0(xt) +α∇f1(xt)−αct⟩
23Under review as submission to TMLR
And
Et∥g(xt)∥2
2=σ2(α)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=(1−α)2σ2
0+α2σ2
1+∥(1−α)∇f0(xt) +α∇f1(xt)−αct∥2
2
So
Etf0(xt+1)−f0(xt)≤−η⟨∇f0(xt),(1−α)∇f0(xt) +α∇f1(xt)−αct⟩
+Lη2
2/parenleftbig
σ2(α) +∥(1−α)∇f0(xt) +α∇f1(xt)−αct∥2
2/parenrightbig
≤η
2/parenleftbig
−∥∇f0(xt)∥2
2+α2∥∇f1(xt)−∇f0(xt)−ct∥2
2/parenrightbig
+Lη2
2σ2(α)
Where we have used above ηL≤1and the identity−2⟨a+b,a⟩+∥a+b∥2
2=∥b∥2
2−∥a∥2
2.
So
E[f0(xt+1)]−E[f0(xt)]≤η
2/parenleftbig
−E[∥∇f0(xt)∥2
2] +α2E[∥∇f1(xt)−∇f0(xt)−ct∥2
2]/parenrightbig
+Lη2
2σ2(α)
≤−η
2E[∥∇f0(xt)∥2
2] +Lη2
2σ2(α)
+α2ηE[∥∇f1(xt)−∇f0(xt)−f1(xt−1) +∇f0(xt−1)∥2
2]
+α2ηE[∥ct−f1(xt−1) +∇f0(xt−1)∥2
2]
Using theδ−BHD assumption, we have :
E[∥∇f1(xt)−∇f0(xt)−f1(xt−1) +∇f0(xt−1)∥2
2]≤δ2E[∥xt−xt−1∥2
2] :=δ2∆t
Wewillusethenotation: Et
c=E[∥ct−f1(xt−1)+∇f0(xt−1)∥2
2],Gt=E[∥∇f0(xt)∥2
2]andFt=E[f0(xt)]−f⋆
0.
All in all, we have :
Ft+1−Ft≤−η
2Gt+Lη2
2σ2(α) +α2δ2η∆t+α2ηEt
c (8)
Bounding ∆t.We also show that :
∆t≤η2/parenleftbig
σ2(α) + 3Gt−1+ 3α2δ2∆t−1+ 3α2Et−1
c/parenrightbig
(9)
Proof.
∆t=E[∥xt−xt−1∥2
2]
=η2E[∥g(xt−1)∥2
2]
=η2/parenleftbig
σ2(α) +E[∥∇f0(xt−1) +α(∇f1(xt−1)−∇f0(xt−1)−ct−1)∥2
2]/parenrightbig
≤η2/parenleftbig
σ2(α) + 3E[∥∇f0(xt−1)∥2
2]
+ 3α2E[∥∇f1(xt−1)−∇f0(xt−1)−∇f1(xt−2) +∇f0(xt−2))∥2
2] + 3α2E[∥ct−1−∇f1(xt−1) +∇f0(xt−2)∥2
2]/parenrightbig
≤η2/parenleftbig
σ2(α) + 3Gt−1+ 3α2δ2∆t−1+ 3α2Et−1
c/parenrightbig
Bounding momentum error Et
c.Using the recursive definition of ct, it is easy to prove:
Et
c≤(1−β)Et−1
c+2δ2
β∆t−1+β2(σ2
0+σ2
1) (10)
24Under review as submission to TMLR
Proof.
Et
c=E[∥ct−∇f1(xt−1) +∇f0(xt−1)∥2]
=E[∥(1−β)ct−1+β(g1(xt−1)−g0(xt−1))−∇f1(xt−1) +∇f0(xt−1)∥2]
=β2(σ2
0+σ2
1) + (1−β)2E[∥ct−1−∇f1(xt−1) +∇f0(xt−1)∥2]
≤β2(σ2
0+σ2
1) + (1−β)2(1 +β
2)Et−1
c+ (1−β)2(1 +2
β)E[∥∇f1(xt−1)−∇f0(xt−1)−f1(xt−2) +f1(xt−2)∥2]
≤β2(σ2
0+σ2
1) + (1−β)2(1 +β
2)Et−1
c+ (1−β)2(1 +2
β)δ2E[∥xt−1−xt−2∥2
2]
≤(1−β)Et−1
c+2δ2
β∆t−1+β2(σ2
0+σ2
1)
Non-convex case.
Combining Inequalities 8, 9 and 10, we prove that for η≤1/(6α2δ2):
Φt+1−Φt≤Lσ2(α)
2η2+10α2δ2σ2(α)
β2η3+ 2α2βη(σ2
0+σ2
1)−η
4Gt(11)
For the potential Φt=Ft+2α2η
βEt
c+10α2δ2η
β2∆t.
By adding the terms in Inequality 11 from t= 0toT−1and by noting that ∆0≤η2(2ζ2+ 2(1 +
m)E[∥∇f0(x0)∥2]) :=η2˜ζ2, we get :
1
4TT−1/summationdisplay
t=0Gt≤F0
ηT+2α2
βTE0
c+Lσ2(α)
2η+10α2δ2η2
β2(˜ζ2/T+σ2(α)) + 2α2β(σ2
0+σ2
1)
At this level, we choose β∈arg minβ∈[0,1]10α2δ2η2
β2(˜ζ2/T+σ2(α)) + 2α2β(σ2
0+σ2
1)this means choosing
β= min(1,/parenleftbig10δ2(˜ζ2/T+σ2(α))
σ2
0+σ2
1/parenrightbig1/3η2/3). This choice gives the inequality in theorem 5.2 :
1
4TT−1/summationdisplay
t=0E[∥∇f0(xt)∥2]≤F0
ηT+4α2E0
βT+12α2/parenleftbig
(σ2
0+σ2
a)(˜ζ2/T+σ2(α))/parenrightbig1/3(δη)2/3+Lσ2(α)
2η+10α2δ2σ2(α)η2.
The term4α2E0
βThas a smaller magnitude than the termF0
ηT(because limη→0η/β= 0). Furthermore, using
a batchStimes larger for estimating the first bias means that E0≤(σ2
0+σ2
a)/S.
µ-PL case. For theµ-PL case we use the fact that 2µ(f0(x)−f⋆
0)≤∥∇f0(x)∥2
2≤2L(f0(x)−f⋆
0)which is
equivalent (in our notation) to 2µFt≤Gt≤2LFt.
Combining this with Inequalities 8, 9 and 10 we get :
Ft+1≤(1−ηµ)Ft+Lη2
2σ2(α) +α2δ2η∆t+α2ηEt
c
∆t≤η2/parenleftbig
σ2(α) + 6LFt−1+ 3α2δ2∆t−1+ 3α2Et−1
c/parenrightbig
Et
c≤(1−β)Et−1
c+2δ2
β∆t−1+β2(σ2
0+σ2
1)
Combining these three inequalities we get :
25Under review as submission to TMLR
Φt+1≤(1−µη
2)Φt+Lσ2(α)
2η2+10α2δ2σ2(α)
β2η3+ 2α2βη(σ2
0+σ2
1) (12)
For the same potential as in the Non-convex case.
Reiterating this inequality gives :
ΦT≤(1−µη
2)TΦ0+Lσ2(α)
µη+20α2δ2σ2(α)
µβ2η2+4α2β
µ(σ2
0+σ2
1) (13)
At this point, we choose βthat optimizes the right-hand side in the previous inequality, and we obtain
β= min(1,/parenleftbig10δ2σ2(α)
σ2
0+σ2
1/parenrightbig1/3η2/3)
We get then
ΦT≤(1−µη
2)TΦ0+Lσ2(α)
µη+ 24α2/parenleftbig
(σ2
0+σ2
a)σ2(α))1/3(δη)2/3/µ (14)
To beat training alone we would need (δη)2/3≪ηwhich means δ2≪η. As it is known ηis of order1
Tin
theµ-PL case, this means we need δ2=o(1
T)to beat training alone.
Choosingη= min(ηmax,log(max(2,2µΦ0T
3Lσ2(α))
µT)we get :
ΦT∈˜O/parenleftig
Φ0exp/parenleftbig
−µηmaxT/2/parenrightbig
+Lσ2(α)
µ2T+ 24α2/parenleftbig
(σ2
0+σ2
a)σ2(α))1/3δ2/3/(µ5/3T2/3)/parenrightig
Choices of the weights. The optimal choices of the weights αandτkare obtained by minimizing the
right-hand-side of the above inequality, this will give a quadratic problem that needs to be solved under the
conditions/summationtextN
k=1τk= 1andτk≥0. AsTgoes to∞, the biasζ2disappears and this choice is fully dictated
by the variance. In fact we can simply minimize the variance σ2(α) = (1−α)2σ2
0+α2/summationtext
kτ2
kσ2
k.
Proof of Corollary 5.3 :
Now supposing δ2=o(1√
T), for example δ2=δ2
0
T3a+1/2for somea > 0, then by choosing η=
min(1/L,1/(6α2δ2),/radicalig
2F0
Lσ2(α)T)we get :
1
4TT−1/summationdisplay
t=0E[∥∇f0(xt)∥2]≤3/radicalbigg
LF0σ2(α)
T+ 12α2/parenleftbigσ2
0+σ2
a
σ2(α)(˜ζ2/T+σ2(α))2δ2
0F0
L/parenrightbig1/31
T1/2+a/parenrightig
+ 4α2E0/parenleftbigL(σ2
0+σ2
a)
10δ2F0/parenrightbig1/31
T2/3
+(L+α2δ2+α2δ2/L)F0+ 4α2E0
T
We can choose αand the weights τkin such a way to optimize σ2(α) = (1−α)2σ2
0+α2/summationtext
kτ2
kσ2
k, but we
can simply choose α=N
N+1andτk=1
Nthis will guarantee that σ2(α) =σ2
avg
Nforσ2
avg=/summationtextN
k=0σ2
k
Nis the
average variance. This choice of the weights implies that the dominant order in Thas a linear speedup in
Nwhich is the statement of Corollary 5.3.
D Code
The code for our experiments can be found at https://anonymous.4open.science/r/
LinSpeedUpCode-F695 .
26