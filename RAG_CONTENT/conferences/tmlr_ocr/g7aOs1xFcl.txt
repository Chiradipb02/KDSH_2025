Under review as submission to TMLR
Robust Contextual Linear Bandits
Anonymous authors
Paper under double-blind review
Abstract
Model misspeciﬁcation is a major consideration in applications of statistical methods and machine
learning. However, it is often neglected in contextual bandits. This paper studies a common form
of misspeciﬁcation, an inter-arm heterogeneity that is not captured by context. To address this is-
sue, we assume that the heterogeneity arises due to arm-speciﬁc random variables, which can be
learned. We call this setting a robust contextual bandit . The arm-speciﬁc variables explain the un-
known inter-arm heterogeneity, and we incorporate them in the robust contextual estimator of the
mean reward and its uncertainty. We develop two efﬁcient bandit algorithms for our setting: a UCB
algorithm called RoLinUCB and a posterior-sampling algorithm called RoLinTS . We analyze both
algorithms and bound their n-round Bayes regret. Our experiments show that RoLinTS is com-
parably statistically efﬁcient to the classic methods when the misspeciﬁcation is low, more robust
when the misspeciﬁcation is high, and signiﬁcantly more computationally efﬁcient than its naive
implementation.
1 Introduction
Astochastic contextual bandit (Auer et al., 2002; Li et al., 2010; Lattimore & Szepesvari, 2019) is an online learning
problem where a learning agent sequentially interacts with an environment over nrounds. In each round, the agent
observes context , pulls an armconditioned on the context, and receives a corresponding stochastic reward . Contextual
bandits have many applications in practice, such as in personalized recommendations (Li et al., 2010; Jeunen &
Goethals, 2021). This is because the mean rewards of the arms are tied together through known context and learned
model parameters. Thus the contextual approach can be more statistically efﬁcient than a naive multi-armed bandit
solution (Auer et al., 2002; Agrawal & Goyal, 2012). The linear model, where the mean reward of an arm is the dot
product of its context and an unknown parameter, is versatile and popular (Dani et al., 2008; Rusmevichientong &
Tsitsiklis, 2010; Abbasi-Yadkori et al., 2011; Agrawal & Goyal, 2013), and we consider it in this work.
There are two common approaches to using linear models in contextual bandits. One maintains a separate parameter
per arm (Section 3.1 in Li et al. (2010)). While this approach can learn complex models, it is not very statistically
efﬁcient because the arm parameters are not shared. This is especially important when each arm is pulled a different
number of times. The other approach maintains a single shared parameter for all arms. While this approach can be
statistically efﬁcient, it is more rigid and likely to fail due to model misspeciﬁcation; when the optimal arm under the
assumed model is not the actual optimal arm.
To address the above issues, we propose a new contextual linear model. This model assumes that the mean reward of
an arm is a dot product of its context and an unknown shared parameter, which is offset by an arm-speciﬁc variable.
This approach is statistically efﬁcient because the model parameter is shared by all arms; yet ﬂexible because the
arm-speciﬁc variables can address model misspeciﬁcation. We call this setting a robust contextual linear bandit . To
provide an efﬁcient solution to the problem, we assume that the arm-speciﬁc variables are random and drawn from
a distribution known by the agent. This allows us to develop a joint estimator of the shared parameter and the arm-
speciﬁc variables, which interpolates between the two and also uses the context.
One motivating example for our setting are recommender systems, where redeach item is considered as an arm and
the features of an item cannot explain all information about the item, such as its intrinsic popularity (Koren et al.,
2009). This is why the so-called behavioral features, the features that summarize the past engagement with the item,
exist. The intrinsic popularity can be viewed as the average engagement, click or purchase rate, in the absence of
1Under review as submission to TMLR
any other information. The item features then offset the engagement, either up or down, depending on their afﬁnity.
For instance, a feature representing the position of the item in the recommended list would have a negative weight,
meaning that lower ranked items are less likely to be clicked, no matter how engaging they are. Luckily, our method
can address it by deﬁning item-speciﬁc variables to explain the intrinsic popularity.
We make the following contributions. First, we propose robust contextual linear bandits, where the model misspeci-
ﬁcation can be learned using arm-speciﬁc variables (Section 3). Under the assumption that the variables are random,
both the Bayesian and random-effect viewpoints can be used to derive efﬁcient joint estimators of the shared model
parameter and arm-speciﬁc variables. We derive the estimators in Section 4, and show how to incorporate them in
the estimate of the mean arm reward and its uncertainty. Second, we propose upper conﬁdence bound (UCB) and
Thompson sampling (TS) algorithms for this problem, RoLinUCB andRoLinTS (Section 5). Both algorithms are com-
putationally efﬁcient and robust to model misspeciﬁcation. We analyze both algorithms and derive upper bounds on
theirn-round Bayes regret (Section 6). Our proofs rely on analyzing an equivalent linear bandit, and the resulting
regret bounds improve in constants due to the special covariance structure of learned parameters. Our algorithms are
also signiﬁcantly more computationally efﬁcient than naive implementations, which take O((d+K)3)time ford
dimensions and Karms, instead of our O(d2(d+K)). Finally, we evaluate RoLinTS on both synthetic and real-world
problems. We observe that RoLinTS is comparably statistically efﬁcient to the classic methods when the misspeciﬁ-
cation is low, more robust when the misspeciﬁcation is high, and signiﬁcantly more computationally efﬁcient than its
naive implementation.
2 Related Work
Our model is related to a hybrid linear model (Section 3.2 in Li et al. (2010)) with shared and arm-speciﬁc parameters.
Unlike the hybrid linear model, where the coefﬁcients of some features are shared by all arms while the others are
not, we introduce arm-speciﬁc random variables to capture the model misspeciﬁcation. We further study the impact
of this structure on regret and propose an especially efﬁcient implementation for this setting. Another related work
ishLinUCB of Wang et al. (2016). hLinUCB is a variant of LinUCB that learns a portion of the feature vector and we
compare to it in Section 7.
Due to our focus on robustness, our work is related to misspeciﬁed linear bandits. Ghosh et al. (2017) proposed
an algorithm that switches from a linear to multi-armed bandit algorithm when the linear model is detected to be
misspeciﬁed. Differently from this work, we adapt to misspeciﬁcation. We do not compare to this algorithm in the
main paper because it is non-contextual; and thus would have a linear regret in our setting. However, we conduct
a comparison in a non-textual setting in Appendix C. Foster et al. (2020) and Krishnamurthy et al. (2021) proposed
oracle-efﬁcient algorithms that reduce contextual bandits to online regression, and are robust to misspeciﬁcation. Since
Safe -FALCON of Krishnamurthy et al. (2021) is an improvement upon Foster et al. (2020), we discuss it in more detail.
Safe -FALCON is more general than our approach because it does not make any distributional assumptions. On the
other hand, it is very conservative in our setting because of inverse gap weighting. We compare to it in Section 7.
Takemura et al. (2021) proposed the ﬁrst algorithm for the misspeciﬁed linear contextual bandit problem without
knowledge of the approximation parameter. This algorithm is an extension of SupLinUCB (Chu et al., 2011) and, at
roundtiteratively narrows down the set of arms in each stage until an arm is chosen from the set. It operates under
the assumption that the approximation error is bounded. In contrast to their approach, our algorithm takes a Bayesian
approach to estimate the approximation error in a Bayesian framework and subsequently constructs the corresponding
UCB or TS algorithm. Dong & Yang (2023) investigated the role of sparsity in improving misspeciﬁed bandit learning,
and Zhang et al. (2023) studied how the interplay between the misspeciﬁcation level and the sub-optimality gap affects
the performance of linear contextual bandits. These theoretical studies differs from ours in terms of motivation and
content. In our work, we focus on modeling the misspeciﬁed component and developing efﬁcient algorithms tailored
to address this aspect. Finally, Bogunovic et al. (2021) and Ding et al. (2022) proposed linear bandit algorithms that
are robust to adversarial noise attack. The notion of robustness in these works is very different from ours.
Our work is also related to random-effect bandits (Zhu & Kveton, 2022). As in Zhu & Kveton (2022), we assume that
each arm is associated with a random variable that can help with explaining its unknown mean reward. Zhu & Kveton
(2022) used this structure to design a bandit algorithm that is comparably efﬁcient to TS without knowing the prior.
Their algorithm is UCB not contextual. A similar idea was explored by Wan et al. (2022) and applied to structured
bandits. This work is also non-contextual. Wan et al. (2021) assumed a hierarchical structure over tasks and modeled
2Under review as submission to TMLR
inter-task heterogeneity. We focus on a single task and model inter-arm heterogeneity. Our work is also related to
recent papers on hierarchical Bayesian bandits (Kveton et al., 2021; Basu et al., 2021; Hong et al., 2022). All of these
papers considered a similar graphical model to Wan et al. (2021) and therefore model inter-task heterogeneity.
3 Robust Contextual Linear Bandits
We adopt the following notation. For any positive integer n, we denote by [n]the set{1,...,n}. We let 1{·}be the
indicator function. For any matrix M∈Rd×d, the maximum eigenvalue is λ1(M)and the minimum is λd(M). The
big O notation up to logarithmic factors is ˜O.
We consider a contextual linear bandit (Li et al., 2010), where the relationship between the mean reward of an arm and
its context is represented by a model. In round t∈[n], an agent pulls one of Karms with feature vectors xi,t∈Rd
fori∈[K]. The vector xi,tsummarizes information speciﬁc to arm iin roundtand we call it a context . Compared to
context-free bandits (Lai & Robbins, 1985; Auer et al., 2002; Agrawal & Goyal, 2012), contextual linear bandits have
more practical applications because they model the reward as a function of context. For instance, in online advertising,
the arms would be different ads, the context would be user features, and the contextual bandit agent would pull arms
according to user features (Li et al., 2010; Agrawal & Goyal, 2013). More formally, in round t, the agent pulls arm
It∈[K]based on context and rewards in past rounds; and receives the reward of arm It,rIt,t, whose mean reward
depends on the context xIt,t. Since the number of contexts is large, the agent assumes some generalization model,
such as that the mean reward is linear in xi,tand some unknown parameter. When this model is incorrectly speciﬁed,
the contextual bandit algorithm may perform poorly.
To improve the robustness of contextual linear bandits to misspeciﬁcation, we introduce a Bayesian hierarchical mod-
eling assumption. Speciﬁcally, the reward ri,tof armiin roundtis generated as
ri,t=µi,t+/epsilon1i,t, (1)
µi,t=x/latticetop
i,tθ+vi, (2)
θ∼Pθ(0,λ−1Id), (3)
vi∼Pv(0,σ2
0), (4)
/epsilon1i,t∼P/epsilon1(0,σ2). (5)
Hereµi,tand/epsilon1i,tare the mean reward and reward noise, respectively, of arm iin roundt. The mean reward µi,thas
two terms: a linear function of context xi,tand parameter θ∈Rdshared by all arms, and the inter-arm heterogeneity
vi∈R, which is an unobserved arm-speciﬁc random variable. The distributions of θ,vi, and/epsilon1i,tare denoted by Pθ,
Pv, andP/epsilon1; and their hyper-parameters are λ,σ2
0, andσ2. We call our model a robust contextual linear bandit because
vimakes it robust to the misspeciﬁcation due to context. Our model can be viewed as an instance of unobserved-effect
models commonly used in panel and longitudinal data analyses (Wooldridge, 2001; Diggle et al., 2002). For brevity,
and since we only study linear models, we often call our model a robust contextual bandit .
Our goal is to design an algorithm that minimizes its regret with respect to the optimal arm-selection strategy. The
n-round Bayes regret R(n)of an agent is deﬁned as
R(n) =E/bracketleftBiggn/summationdisplay
t=1µI∗
t,t−µIt,t/bracketrightBigg
, (6)
whereI∗
tis the arm with highest mean reward in round tandItis the pulled arm in round t. The expectation is under
the randomness of ItandI∗
t; and those ofθ,vi, and/epsilon1i,t. Given that our model is within a Bayesian setting, this paper
focuses on the Bayesian regret, wherein the expectation is taken with respect to the distribution over the inter-arm
heterogeneity. Through an analysis of Bayesian regret, we quantify the expected regret concerning vi.
3.1 Discussion
We introduce an unobserved effect vi, which can be interpreted as capturing the characteristics of arm ithat is not
explained by context, but is assumed not to change over nrounds. We call it the inter-arm heterogeneity . For example,
3Under review as submission to TMLR
in online advertising, the arms would be different ads and the context would be user features. In this problem, vimay
contain unobserved ad characteristics, such as its intrinsic quality, that can be viewed as roughly constant.
We assume that the parameter θis shared by all arms, while the inter-arm heterogeneity is modeled by arm-speciﬁc
variables. From the statistical-efﬁciency viewpoint, this model reduces the number of parameters compared to model-
ing arms separately, and therefore increases statistical efﬁciency. Li et al. (2010) proposed hybrid linear models, where
the coefﬁcients of some features are shared by all arms while the others are arm-speciﬁc. However, choosing features
to share may be challenging in practice. From the practical viewpoint, it is more convenient to apply the robust con-
textual bandit, as it avoids the challenging choice of the shared features. In particular, the model is still ﬂexible enough
because it uses the unobserved effect vito capture inter-arm heterogeneity, information not explained by the context.
For instance, imagine a contextual recommendation problem with Karms, where arms represent items. In addition
to what the item and user features can explain, there may still be item-speciﬁc biases (Koren et al., 2009). The main
difference from the work of Li et al. (2010) is that our approach is a two-level Bayesian model. We model the linear
relationship between rewards and contexts. At the arm level, we incorporate a Bayesian assumption, positing that the
inter-arm heterogeneity vifollows the distribution in (4). This assumption translates to how we estimate µi,t, which
is a weighted average of the linear model and the inter-arm heterogeneity. These weights are adjusted to reﬂect the
relative magnitudes of the lack of ﬁt of the linear model and the variance of the inter-arm heterogeneity. We present
more details in Section 4.
4 Estimation
This section introduces our estimators for robust contextual bandits. In Section 4.1, we derive the estimators of θand
vifori∈[K]. In Section 4.2, we derive the estimator of µi,t=x/latticetop
i,tθ+viand its uncertainty.
4.1 Maximum a Posteriori Estimation of θandvi
Fix roundt. LetTi,tbe the set of rounds where arm iis pulled by the beginning of round tandni,t=|Ti,t|be the
size ofTi,t. Letri,t= (ri,/lscript)/latticetop
/lscript∈Ti,tbe the column vector of rewards obtained by pulling arm i,/epsilon1i,t= (/epsilon1i,/lscript)/latticetop
/lscript∈Ti,tbe the
column vector of the corresponding reward noise, and Xi,t= (xi,/lscript)/latticetop
/lscript∈Ti,tbe ani,t×dmatrix with the corresponding
contexts. From (1) and (2),
ri,t=Xi,tθ+vi1ni,t+/epsilon1i,t,
where 1kis a vector of length kwhose all entries are one. The covariance matrix Vi,tfor the vector ri,tis given
byVi,t=σ2
01ni,t1/latticetop
ni,t+σ2Ini,t, where Ikis the identify matrix of size k×k. The terms σ2
01ni,t1/latticetop
ni,tandσ2Ini,t
represent the randomness from viand/epsilon1i,t, respectively. By the Woodbury matrix identity,
V−1
i,t=σ−2Ini,t−σ−2n−1
i,twi,t1ni,t1T
ni,t. (7)
Assuming that Pθ,Pv,P/epsilon1are Gaussian, the maximum a posteriori (MAP) estimation is equivalent to minimizing the
following loss function
L(v1,···,vK,θ) =K/summationdisplay
i=1/bracketleftbig
σ−2/bardblri,t−Xi,tθ−vi/bardbl2+σ−2
0v2
i/bracketrightbig
+λ/bardblθ/bardbl2(8)
with respect to (vi)i∈[K]andθ, where/bardbl·/bardbl is the Euclidean norm. The term σ−2/summationtextK
i=1/bardblri,t−Xi,tθ−vi/bardbl2is from
the conditional likelihood of ri,tgiven (vi)i∈[K]andθ. The regularization term σ−2
0/summationtextK
i=1v2
iis from the prior of
(vi)i∈[K]in (4). The other term λ/bardblθ/bardbl2is from the prior of θin (3).
Differentiating L(v1,···,vK,θ)with respect to viand putting it equal to zero, viis estimated by
˜vi,t=wi,t(¯ri,t−¯x/latticetop
i,tθ), (9)
where ¯ri,t=n−1
i,t/summationtext
/lscript∈Ti,tri,/lscriptis the average reward of arm iup to round t,¯xi,t=n−1
i,t/summationtext
/lscript∈Ti,txi,/lscriptis the average context
associated with the pulls of arm iup to roundt, and
wi,t=σ2
0
σ2
0+σ2/ni,t(10)
4Under review as submission to TMLR
is a weight that interpolates between the context and the arm-speciﬁc variable. We discuss its role in Section 4.2.
Letui,t=ri,t−Xi,tθand¯ui,t=n−1
i,t/summationtext
/lscript∈Ti,tui,/lscript, whereui,/lscriptis the/lscript-th element of ui,t. Inserting (9) into (8), it
follows that
L(θ) =K/summationdisplay
i=1/bracketleftbig
σ−2/bardblui,t−wi,t¯ui,t/bardbl2+σ−2
0w2
i,t¯u2
i,t/bracketrightbig
+λ/bardblθ/bardbl2
=σ−2K/summationdisplay
i=1/bracketleftbig
/bardblui,t/bardbl2−ni,twi,t¯u2
i,t/bracketrightbig
+λ/bardblθ/bardbl2
=K/summationdisplay
i=1/bracketleftbig
(ri,t−Xi,tθ)/latticetopV−1
i,t(ri,t−Xi,tθ)/bracketrightbig
+λ/bardblθ/bardbl2,
where the last step is from (7).
To obtain the MAP estimate of θ, we minimize L(θ)with respect to θand get
ˆθt=/parenleftBigg
λId+K/summationdisplay
i=1X/latticetop
i,tV−1
i,tXi,t/parenrightBigg−1K/summationdisplay
i=1X/latticetop
i,tV−1
i,tri,t. (11)
To obtain the MAP estimate of vi, we insert ˆθtinto (9),
ˆvi,t=wi,t(¯ri,t−¯x/latticetop
i,tˆθt). (12)
4.2 Prediction of µi,tand Its Uncertainty
Based on (11) and (12), the estimated mean reward of arm iin context xi,tin roundtis
ˆµi,t=xi,tˆθt+wi,t(¯ri,t−¯x/latticetop
i,tˆθt). (13)
In (2),virepresents the inter-arm heterogeneity. It is the arm-speciﬁc effect that cannot be explained by context. This
effect is estimated in (13) using wi,t(¯ri,t−¯x/latticetop
i,tˆθt). Now consider wi,tin (10). If the arm has not been pulled, ni,t= 0
andwi,t= 0. Therefore, ˆµi,t=x/latticetop
i,tˆθt. Similarly, if the arm has not been pulled often, ˆµi,tis close to xi,tˆθt. This
means that the prediction ˆµi,tis statistically efﬁcient for small ni,t. This is helpful in the initial rounds when there are
only a few observations of arms.
We further explain (13) by rewriting it as
ˆµi,t=wi,t¯ri,t+ (xi,t−wi,t¯xi,t)/latticetopˆθt. (14)
Here ˆµi,tis a weighted estimator of two terms: the sample mean of arm i,¯ri,t, and additional calibration from contexts
(xi,t−wi,t¯xi,t)/latticetopˆθt. The weight is wi,t. Whenni,t→∞ ,wi,t→1andˆµi,t→¯ri,t+ (xi,t−¯xi,t)/latticetopˆθt. This shows
why our prediction ˆµi,tis robust. Informally, it uses ¯ri,tas a baseline and corrects it using context as (xi,t−¯xi,t)/latticetopˆθt.
Therefore, we reduce the reliance on the contextual model by automatically balancing the contextual and multi-armed
bandits. This is why we call our framework a robust contextual bandit.
The prediction ˆµi,tcan also degenerate to that of a contextual linear bandit (Rusmevichientong & Tsitsiklis, 2010;
Agrawal & Goyal, 2013). More speciﬁcally, wi,t→0asσ2
0→0, and then ˆµi,tapproaches
ˆµlin
i,t=xi,t/parenleftBigg
λId+K/summationdisplay
i=1X/latticetop
i,tXi,t/parenrightBigg−1K/summationdisplay
i=1X/latticetop
i,tri,t,
which is the prediction of a simple linear model without inter-arm heterogeneity. This observation is important because
it shows that our framework is as general as the contextual linear bandit.
5Under review as submission to TMLR
Now we characterize the uncertainty of ˆµi,tin (14) for efﬁcient exploration. We measure the uncertainty by the mean
squared error E [(ˆµi,t−µi,t)2]. Let ¯/epsilon1i,t=n−1
i,t/summationtext
/lscript∈Ti,t/epsilon1i,/lscriptandMt=λId+K/summationtext
i=1X/latticetop
i,tV−1
i,tXi,t. A direct calculation
shows that
E[(ˆµi,t−µi,t)2]
=E[((wi,t−1)vi+wi,t¯/epsilon1i,t+ (xi,t−wi,t¯xi,t)/latticetop(ˆθt−θ))2]
=σ2
0(1−wi,t) + (xi,t−wi,t¯xi,t)/latticetopM−1
t(xi,t−wi,t¯xi,t)
=:τ2
i,t, (15)
where the last step is from E [(wi,tvi−vi+wi,t¯/epsilon1i,t)(ˆθt−θ)/latticetop(xi,t−wi,t¯xi,t)] = 0 shown in Kachar & Harville
(1984).
4.3 Computational Efﬁciency
We also investigate if the robust contextual bandit can be implemented as computationally efﬁciently as a contextual
linear bandit. As discussed in Section 6.2, our model is equivalent to a linear model augmented by Kfeatures,
indicating which unobserved vicorresponds to arm i. The computational cost of posterior sampling or computing
upper conﬁdence bounds in this model is O((d+K)3)per round, due to inverting (d+K)×(d+K)precision
matrices. On the other hand, the robust contextual bandit can be implemented as computationally efﬁciently as a
contextual linear bandit with dfeatures, with O(d2(d+K))computational cost per round. Speciﬁcally, using (7),
X/latticetop
i,tV−1
i,tXi,t=σ−2(X/latticetop
i,tXi,t−wi,tni,t¯xi,t¯x/latticetop
i,t),
X/latticetop
i,tV−1
i,tri,t=σ−2(X/latticetop
i,tri,t−wi,tni,t¯xi,t¯ri,t).
These identities can be used to rederive all statistics as
Mt=λId+σ−2K/summationdisplay
i=1(X/latticetop
i,tXi,t−wi,tni,t¯xi,t¯x/latticetop
i,t),
ˆθt=M−1
t/bracketleftBigg
σ−2K/summationdisplay
i=1(X/latticetop
i,tri,t−wi,tni,t¯xi,t¯ri,t)/bracketrightBigg
, (16)
ˆµi,t=wi,t¯ri,t+ (xi,t−wi,t¯xi,t)/latticetopˆθt, (17)
τ2
i,t=σ2
0(1−wi,t) + (xi,t−wi,t¯xi,t)/latticetopM−1
t(xi,t−wi,t¯xi,t). (18)
The main cost in the above formulas is due to calculating M−1
t, which isO(d3)per round. All remaining operations
areO(d2K). Therefore, the computational cost of prediction in the robust contextual bandit is O(d2(d+K))and
comparable to the contextual linear bandit.
5 Algorithms
Upper conﬁdence bounds (UCBs) (Auer et al., 2002) and Thompson sampling (TS) (Thompson, 1933) are two popular
bandit algorithm designs. We propose UCB and TS algorithms for robust contextual bandits based on the estimate
ofµi,tand its uncertainty (Section 4). The UCB algorithm is called RoLinUCB because it can be viewed as a robust
variant of LinUCB (Abbasi-Yadkori et al., 2011). From Section 4, ˆµi,tandτ2
i,tare the posterior mean and variance,
respectively, of µi,tin roundt. This observation motivates a posterior-sampling algorithm that uses the posterior of
µi,t. We call it RoLinTS because it can be viewed as a robust variant of LinTS (Agrawal & Goyal, 2013).
Both algorithms work as follows. Let the history at the beginning of round tbe all actions and observations of the
agent up to that round, Ht= (xI/lscript,/lscript,I/lscript,rI/lscript,/lscript)t−1
/lscript=1. In roundt, the algorithms observe context xi,tof each armiand then
compute the MAP estimate ˆµi,tofµi,tand its uncertainty τ2
i,tconditioned on Ht.RoLinUCB pulls the arm with the
highest upper conﬁdence bound, It= arg maxi∈[K]Ui,t, whereUi,t= ˆµi,t+/radicalBig
2τ2
i,tlogn.RoLinTS samplesUi,t∼
6Under review as submission to TMLR
N(ˆµi,t,τ2
i,t)and then pulls the arm with the highest mean reward under the posterior sample, It= arg maxi∈[K]Ui,t.
After pulling arm Itand observing the corresponding reward, the algorithms update all statistics in Section 4.3. The
pseudo-code of RoLinTS andRoLinUCB is presented in Algorithm 1.
Algorithm 1 RoLinUCB andRoLinTS for robust contextual bandits.
1:fort= 1,...,n do
2: fori= 1,...,K do
3: Observe contexts xi,t
4: Obtain ˆµi,tfrom (17) and τ2
i,tfrom (18)
5: Deﬁne
RoLinUCB :Ui,t= ˆµi,t+/radicalBig
2τ2
i,tlogn
RoLinTS :Ui,t∼N(ˆµi,t,τ2
i,t)
6: end for
7:It←arg maxi∈[K]Ui,t
8: Pull armItand observe reward rIt,t
9:nIt,t←nIt,t+ 1
10: Update all statistics in Section 4.3
11:end for
6 Regret Analysis
We prove upper bounds on the n-round regret of RoLinUCB andRoLinTS . Similarly to random-effect bandits (Zhu
& Kveton, 2022), ˆµi,tis the MAP estimate of µi,tgiven history Ht, under the assumptions that Pθ,Pv, andP/epsilon1are
Gaussian distributions. Thus we adopt the Bayes regret (Russo & Van Roy, 2014) to analyze RoLinUCB andRoLinTS .
Let the optimal arm in round tbeI∗
t= arg maxi∈[K]µi,t. The regret is the difference between the rewards that we
would have obtained by pulling the optimal arm I∗
tand the rewards that we did obtain by pulling Itovernrounds.
The regret is formally deﬁned in (6) and we bound it below.
Theorem 1. Consider the robust contextual bandit where
Pθ=N(0,λ−1Id), Pv=N(0,σ2
0), P/epsilon1=N(0,σ2).
Let the hyper-parameters λ,σ2
0,σ2be known by the learning agent. Let /bardblxi,t/bardbl≤L. Then then-round Bayes regret
ofRoLinUCB andRoLinTS is bounded as
R(n)≤σmax/radicalbig
2c(d+K)nlog(n) +/radicalbig
2/πσmaxK,
where
c= log/parenleftbigg
1 +σ2
maxn
σ2(d+K)/parenrightbigg/slashBig
log(1 +σ−2σ2
max)
andσ2
max=σ2
0+L2λ−1.
6.1 Discussion
Theorem 1 shows that the Bayes regret of both RoLinUCB andRoLinTS isO(/radicalbig
(σ2
0+L2λ−1)(d+K)n)up to log-
arithmic factors. The dependence on horizon nis optimal. The dependence on d+Karises due to learning d+K
parameters in the equivalent linear bandit: dfor the linear model and one parameter per arm. This would be optimal
in a general linear bandit. However, the dependence on Kdoes not vanish as the inter-arm heterogeneity diminishes.
This limitation is inherent in our analysis, as we employ the linear bandit framework with d+Kparameters. Em-
pirically, we have observed that the dependence on Kwould vanish as σ2
0→0(Appendix C). The structure of our
7Under review as submission to TMLR
problem is captured by constant σ2
0+L2λ−1. The regret increases when the shared parameter θis more uncertain, λ
is low; when the inter-arm heterogeneity is high, σ0is high; and when the feature vectors of arms are long, Lis high.
Theorem 1 is proved under the assumption that the hyper-parameters λ,σ2
0,σ2are known to the learning agent. We
evaluated RoLinTS empirically to hyper-parameter misspeciﬁciation (Figure 3 in Appendix C). Notably, our ﬁndings
demonstrate that the algorithm maintains its performance.
Our analysis in Section 6.2 improves upon a trivial linear bandit analysis by using the structure of the posterior variance
in (15). A trivial analysis, which would only use the structure in the prior covariance,
/bardblzi,t/bardbl2
Σt≤/bardblzi,t/bardbl2
Σ0≤max/braceleftbig
σ2
0,λ−1/bracerightbig
(L2+ 1),
would replace the factor σ2
0+L2λ−1in our regret bound with max/braceleftbig
σ2
0,λ−1/bracerightbig
(L2+ 1) . Note that
σ2
0+L2λ−1≤max/braceleftbig
σ2
0,λ−1/bracerightbig
(L2+ 1)
for anyλ,σ0, andL; and hence our analysis is always an improvement. This improvement can be signiﬁcant when
the parameter θis nearly certain and K/greatermuchd. In this case, our bound approaches that of a K-armed Bayesian bandit
with priorN(0,σ2
0), which isO(/radicalbig
σ2
0Kn); and the other bound is O(/radicalbig
σ2
0L2Kn), whereL2could beO(d).
Beyond improvements in regret, the structure of our problem can be used to get major improvements in computational
efﬁciency (Section 4.3), from O((d+K)3)time per round for a naive implementation to O(d2(d+K)).
6.2 Proof of Theorem 1
First, we note that (2) can be rewritten as a single linear model by augmenting features. Speciﬁcally, let u⊕vbe the
concatenation of vectors uandv; andei∈RKbe an indicator vector of the i-th dimension, ei,j=1{i=j}. Using
this notation, let zi,t=xi,t⊕eibe the augmented feature vector of arm iin roundtandγ=θ⊕(vi)i∈[K]be the
augment parameter vector. Then the model in (2), (3), and (4) can be expressed as a simple Bayesian linear regression
model,
µi,t=z/latticetop
i,tγ,γ∼N(0,Σ0), (19)
where Σ0is a block-diagonal matrix. Its upper d×dblock isλ−1Idand the lower K×Kblock isσ2
0IK.
Letrt= (rI/lscript,/lscript)/lscript∈[t−1]be a column vector of all rewards up to round tandZt= (zI/lscript,/lscript)/lscript∈[t−1]be a(t−1)×(d+K)
matrix of augmented features up to round t. From (19), we have that
γ|Ht∼N(γt,Σt),
whereγt=σ−2ΣtZ/latticetop
trtandΣ−1
t=Σ−1
0+σ−2Z/latticetop
tZt. It follows that
µi,t|Ht∼N(z/latticetop
i,tγt,z/latticetop
i,tΣtzi,t).
We prove in Lemma 1 that µi,t|Ht∼N(ˆµi,t,τ2
i,t). Thus we can equivalently analyze the reformulation in (19).
Now we apply the general regret bound in Theorem 2 (Appendix B) to (19) and get
R(n)≤σmax/radicalbig
2c(d+K)nlog(1/δ) +/radicalbig
2/πσmaxKnδ,
where
c= log/parenleftbigg
1 +σ2
maxn
σ2(d+K)/parenrightbigg/slashBig
log(1 +σ−2σ2
max)
andσmax= maxi∈[K],t∈[n]/bardblzi,t/bardblΣtis a problem-speciﬁc quantity that we bound next. Speciﬁcally, since /bardblzi,t/bardbl2
Σt=
τ2
i,t, we have
/bardblzi,t/bardbl2
Σt≤σ2
0+L2λ1(M−1
t) =σ2
0+L2λ−1
d(Mt)≤σ2
0+L2λ−1.
The ﬁrst inequality is from the deﬁnition of τ2
i,tin (15) and that/bardblxi,t/bardbl≤L. The second inequality is from the
observation that both components of Mtin (15) are positive semi-deﬁnite (PSD). Speciﬁcally, all eigenvalues of λId
areλand each X/latticetop
i,tV−1
i,tXi,tis PSD because Vi,tis. To complete the proof, we set δ= 1/n.
8Under review as submission to TMLR
0 2000 4000 6000 8000 10000
Round n0100200300400500600700Regret¾0 = 0.00
0 2000 4000 6000 8000 10000
Round n0200400600800¾0 = 0.25
0 2000 4000 6000 8000 10000
Round n050010001500200025003000¾0 = 0.50
LinTSRoLinTS (s0 = 0.25)RoLinTS (s0 = 0.5)RoLinTS (s0 = 1) hLinUCB Safe-FALCON
Figure 1: Comparison of RoLinTS to three baselines on synthetic problems in Section 7.1. The results are averaged
over100runs.
7 Experiments
We conduct three main experiments. In Section 7.1, we evaluate RoLinTS in synthetic bandit problems. In Section 7.2,
we apply it to the problem of learning a linear model with misspeciﬁed features. In Section 7.3, we compare a naive
implementation of RoLinTS to that in Section 4.3.
We conduct four additional experiments in Appendix C. In Figure 3, we study the robustness of RoLinTS to parameter
misspeciﬁcations. In Figure 4, we evaluate RoLinUCB . In Figure 5, we study how the regret of RoLinTS increases
with the numbers of arms K. Finally, in Figure 6, we compare to Ghosh et al. (2017) on non-contextual problems.
7.1 Synthetic Experiment
Our ﬁrst experiment is with three robust contextual bandits where σ0∈{0,0.25,0.5}. Note thatσ0= 0corresponds
to a contextual linear bandit. We set K= 50 andd= 10 . The model parameter and features are sampled from
N(0,Id)and uniformly at random from [−1,1]d, respectively. The reward noise is σ= 1. We compare RoLinTS ,
run withσ0∈{0.25,0.5,1}, toLinTS , which can be viewed as RoLinTS withσ0= 0 (Section 4.2). To distinguish
σ0inRoLinTS from the environment parameter, we denote the former by s0. We consider two additional baselines,
Safe -FALCON of Krishnamurthy et al. (2021) and hLinUCB of Wang et al. (2016), which are introduced in Section 2.
We set the complexity term in Safe -FALCON toξ(n,ζ) =dlog(1/ζ)/n, since we have d-dimensional regression
problems. In hLinUCB , we learnKadditional features per arm. This choice is motivated by the fact that RoLinTS can
be implemented inefﬁciently as LinTS withKadditional features per arm (Section 6.2).
Our results are reported in Figure 1. In the left plot, the environment is a contextual linear bandit and LinTS has a
sublinear regret. In this case, RoLinTS is not expected to outperform it, because it learns an additional random-effect
parameter per arm. Nevertheless, all variants of RoLinTS have a sublinear regret in n. A higher regret corresponds to
higher values of s0, which is expected since RoLinTS with a higher value of s0is more uncertain about the underlying
model being linear. In the middle plot, the environment is a robust contextual bandit with σ0= 0.25. Although
this model is only slightly misspeciﬁed, LinTS fails and has a linear regret. In comparison, all variants of RoLinTS
have a sublinear regret, even with the misspeciﬁed random effect s0∈ {0.5,1}. This highlights the robustness
of our approach to misspeciﬁcation. Finally, in the right plot, the environment is a robust contextual bandit with
σ0= 0.5. We observe that the gap in the regret of LinTS andRoLinTS increases with σ0. In this case, RoLinTS has
at least twice lower regret than LinTS for all values of s0. When the model is correctly speciﬁed ( σ0= 0),hLinUCB
outperforms both LinTS andRoLinTS . When the model is misspeciﬁed ( σ0>0),hLinUCB performs similarly to
LinTS andRoLinTS outperforms it. Safe -FALCON is too conservative to be competitive. In all experiments, its regret
atn= 10 000 rounds is an order of magnitude higher than that of RoLinTS .
9Under review as submission to TMLR
01000 2000 3000 4000 5000
Round n02004006008001000120014001% training data
01000 2000 3000 4000 5000
Round n0200400600800100012002% training data
01000 2000 3000 4000 5000
Round n02004006008001000Regret5% training data
LinTSRoLinTS (s0 = 0.25)RoLinTS (s0 = 0.5)RoLinTS (s0 = 1) hLinUCB Safe-FALCON
Figure 2: Comparison of RoLinTS to three baselines on a movie recommendation problem with misspeciﬁed item
features. The results are averaged over 500runs.
7.2 MovieLens Experiment
This experiment shows the utility of RoLinTS in a linear bandit with misspeciﬁed features. Speciﬁcally, we have a
linear function u/latticetopvthat models the mean rating of user ufor moviev, whereuis an unknown preference vector of the
user andvis a feature vector of a movie. The challenge is that the agent only knows ˆv, an estimate of vfrom logged
data. All parameters in this experiment are estimated by matrix completion: uandvare learned from the test set, and
ˆvis learned from the training set. When the training set is small, ˆvis a poor estimate of v, and thus the mean rating of
a movie is not linear in ˆv. This can be addressed by learning a separate bias term per movie, which is what RoLinTS
does.
The experiment is speciﬁcally set up as follows. We take the MovieLens 1M dataset (Lam & Herlocker, 2016) with
nu= 6 000 users,ni= 4 000 items, and 1million ratings. We divide the dataset equally into the training and test
sets. In the training set, we apply alternating least-squares to complete the rating matrix. The result are latent user
ˆU∈Rnu×dand item ˆV∈Rni×dfactors, where dis the factorization rank and ˆUˆV/latticetopestimates the mean ratings for
all user-item pairs. We apply the same approach to the test set, and obtain latent user U∈Rnu×dand item V∈Rni×d
factors.
The interaction with a recommender system is simulated as follows. We choose a random user and K= 50 random
movies, and the goal is to learn to recommend the best of these movies to the chosen user. This is repeated 500times.
The feature vector of movie iisˆVi,:, which is the i-th row of matrix ˆV. The challenge is that the mean rating of
movieifor userjisUj,:V/latticetop
i,:. Therefore, it is linear in the unobserved Vi,:but not in the observed ˆVi,:.RoLinTS can
adapt to this misspeciﬁcation by essentially learning Uj,:(Vi,:−ˆVi,:)/latticetop. The rating noise is N(0,σ2)withσ= 0.759,
which is estimated from data.
Our results are reported in Figure 2. In the left plot, we only use 1%of the training set to estimate movie features
ˆV. In this case, the estimated features are highly uncertain and the regret of LinTS is clearly linear n. The regret of
RoLinTS is signiﬁcantly lower for all s0∈{0.25,0.5,1}, up to twice for s0= 1atn= 5 000 . This clearly shows that
RoLinTS can partially address the problem of misspeciﬁed features. In the next two plots, we use 2%and5%of the
training set to estimate movie features ˆV. As the features become more precise, all methods improve. Nevertheless,
the beneﬁt of adapting to model misspeciﬁcation persists. We also observe that hLinUCB performs better than LinTS
but is worse than RoLinTS .Safe -FALCON is too conservative to be competitive. Its regret at n= 5 000 rounds is an
order of magnitude higher than that of RoLinTS .
7.3 Run Time
The challenge with implementing RoLinTS naively (Section 6.2) is O((d+K)3)computational cost, due to inverting
(d+K)×(d+K)precision matrices. Our efﬁcient implementation (Section 4.3) inverts only d×dprecision matrices
and its cost is O(d2(d+K))per round. To show this empirically, we compare the run time of RoLinTS to its naive
10Under review as submission to TMLR
K RoLinTS LinTS
16 0.7 1.1
32 0.8 2.0
64 0.9 4.6
128 1.1 14.7
256 1.5 75.6
512 2.9 498.7
1 024 5.24 275.4
Table 1: Run times of RoLinTS andLinTS as functions of the number of arms K. The time is measured in seconds.
implementation, called LinTS for simplicity. We take the setup from Figure 1, vary Kfrom 16to1024 , and measure
the run time of 100random runs over a horizon of n= 500 rounds.
Our results are reported in Table 1. These results conﬁrm our expectation. For large K, the run time of RoLinTS
doubles when Kdoubles. Therefore, it is linear in K. On the hand, for large K, the run time of LinTS is nearly cubic
inK. For a moderately large number of arms, K= 128 ,RoLinTS is10times faster than LinTS . For a large number
of arms,K= 1 024 ,RoLinTS is a thousand times faster than LinTS .
8 Conclusions
Model misspeciﬁcation in bandits, when the optimal arm under the assumed model is not optimal, can lead to catas-
trophic failures of contextual bandit algorithms and linear regret. We mitigate this by proposing robust contextual
linear bandits. The key idea in our model is that the mean reward of an arm is a dot product of its context and a shared
model parameter, which is offset by an arm-speciﬁc variable. This approach is statistically efﬁcient because the model
parameter is shared by all arms; yet quite robust to model misspeciﬁcation due to learning the arm-speciﬁc variables.
We propose UCB and posterior-sampling algorithms for our setting, show how to implement them efﬁciency, prove
regret bounds that reﬂect the structure of our problem, and also validate the algorithms empirically.
Our work has several limitations that can be addressed by future works. For instance, although Theorem 1 reﬂects some
structure of our problem (Section 6.1), it is not completely satisfactory. In particular, as the inter-arm heterogeneity
diminishes, σ0→0, one would expected a regret bound of O(√
dn)while we get O(/radicalbig
(d+K)n). Proving of
the improved bound seems highly non-trivial due to complex correlations of all estimated model parameters in the
posterior. Another limitation of our work is that we focus on linear models. We plan to extend robust contextual
bandits to generalized linear models. Finally, although we have not discussed hyper-parameter tuning and learning,
we do so in Appendix D.
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. In Ad-
vances in Neural Information Processing Systems 24 , pp. 2312–2320, 2011.
Shipra Agrawal and Navin Goyal. Analysis of Thompson sampling for the multi-armed bandit problem. In Proceeding
of the 25th Annual Conference on Learning Theory , pp. 39.1–39.26, 2012.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In Proceedings of
the 30th International Conference on Machine Learning , pp. 127–135, 2013.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine
Learning , 47:235–256, 2002.
Soumya Basu, Branislav Kveton, Manzil Zaheer, and Csaba Szepesvari. No regrets for learning the prior in bandits.
InAdvances in Neural Information Processing Systems 34 , 2021.
I. Bogunovic, A. Losalka, A. Krause, and J. Scarlett. Stochastic linear bandits robust to adversarial attacks. In
Proceedings of the 24th International Conference on Artiﬁcial Intelligence and Statistics , 2021.
11Under review as submission to TMLR
B.P. Carlin and T.A Louis. Bayes and Empirical Bayes Methods for Data Analysis . Chapman & Hall/CRC, 2000.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Proceedings
of the 14th International Conference on Artiﬁcial Intelligence and Statistics , pp. 208–214, 2011.
Varsha Dani, Thomas Hayes, and Sham Kakade. Stochastic linear optimization under bandit feedback. In Proceedings
of the 21st Annual Conference on Learning Theory , pp. 355–366, 2008.
P.J. Diggle, P. Heagerty, K.-Y . Liang, and S.L. Zeger. Analysis of Longitudinal Data . Oxford University Press, second
edition edition, 2002.
Q. Ding, C.-J. Hsieh, and J. Sharpnack. Robust stochastic linear contextual bandits under adversarial attacks. In
Proceedings of the 25th International Conference on Artiﬁcial Intelligence and Statistics , 2022.
Jialin Dong and Lin F. Yang. Does sparsity help in learning misspeciﬁed linear bandits? In Proceedings of the 40th
International Conference on Machine Learning , volume 202, pp. 8317–8333, 2023.
D.J. Foster, C. Gentile, M. Mohri, and J. Zimmert. Adapting to misspeciﬁcation in contextual bandits. In 34th
Conference on Neural Information Processing Systems , 2020.
A. Ghosh, S.R. Chowdhury, and A. Gopalan. Misspeciﬁed linear bandits. In Proceedings of the Thirty-First AAAI
Conference on Artiﬁcial Intelligence , 2017.
D.A. Harville. Maximum likelihood approaches to variance component estimation and to related problems. Journal
of the American Statistical Association , 72:320–338, 1988.
Joey Hong, Branislav Kveton, Manzil Zaheer, and Mohammad Ghavamzadeh. Hierarchical Bayesian bandits. In
Proceedings of the 25th International Conference on Artiﬁcial Intelligence and Statistics , 2022.
O. Jeunen and B. Goethals. Pessimistic reward models for off-policy learning in recommendation. In The Fifteenth
ACM Conference on Recommender Systems , 2021.
R. Kachar and D.A. Harville. Approximations for standard errors of estimators of ﬁxed and random effect in mixed
linear models. Journal of the American Statistical Association , 79:853–862, 1984.
Emilie Kaufmann, Olivier Cappe, and Aurelien Garivier. On Bayesian upper conﬁdence bounds for bandit problems.
InProceedings of the 15th International Conference on Artiﬁcial Intelligence and Statistics , pp. 592–600, 2012.
Yehuda Koren, Robert Bell, and Chris V olinsky. Matrix factorization techniques for recommender systems. IEEE
Computer , 42(8):30–37, 2009.
Sanath Kumar Krishnamurthy, Vitor Hadad, and Susan Athey. Adapting to misspeciﬁcation in contextual bandits
with ofﬂine regression oracles. In Proceedings of the 38th International Conference on Machine Learning , pp.
5805–5814, 2021.
Branislav Kveton, Mikhail Konobeev, Manzil Zaheer, Chih-Wei Hsu, Martin Mladenov, Craig Boutilier, and Csaba
Szepesvari. Meta-Thompson sampling. In Proceedings of the 38th International Conference on Machine Learning ,
2021.
T. L. Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied Mathematics ,
6(1):4–22, 1985.
Shyong Lam and Jon Herlocker. MovieLens Dataset. http://grouplens.org/datasets/movielens/, 2016.
Tor Lattimore and Csaba Szepesvari. Bandit Algorithms . Cambridge University Press, 2019.
L. Li, W. Chu, J. Langford, , and R. E. Schapire. A contextual-bandit approach to personalized news article recom-
mendation. In Proceedings of the 19th international conference on World wide web , pp. 661–670, 2010.
Paat Rusmevichientong and John Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations Research , 35
(2):395–411, 2010.
12Under review as submission to TMLR
D. Russo and B. Van Roy. Learning to optimize via posterior sampling. Mathematics of Operations Research , 39(4):
1221–1243, 2014.
Kei Takemura, Shinji Ito, Daisuke Hatano, Hanna Sumita, Takuro Fukunaga, Naonori Kakimura, and Ken-ichi
Kawarabayashi. A parameter-free algorithm for misspeciﬁed linear contextual bandits. In Arindam Banerjee and
Kenji Fukumizu (eds.), Proceedings of The 24th International Conference on Artiﬁcial Intelligence and Statistics ,
volume 130 of Proceedings of Machine Learning Research , pp. 3367–3375. PMLR, 2021.
William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of
two samples. Biometrika , 25(3-4):285–294, 1933.
Runzhe Wan, Lin Ge, and Rui Song. Metadata-based multi-task bandits with Bayesian hierarchical models. In
Advances in Neural Information Processing Systems 34 , 2021.
Runzhe Wan, Lin Ge, and Rui Song. Towards scalable and robust structured bandits: A meta-learning framework.
CoRR , abs/2202.13227, 2022. URL https://arxiv.org/abs/2202.13227 .
Huazheng Wang, Qingyun Wu, and Hongning Wang. Learning hidden features for contextual bandits. In Proceedings
of the 25th ACM International on Conference on Information and Knowledge Management , pp. 1633–1642, 2016.
J. M. Wooldridge. Econometric Analysis of Cross Section and Panel Data . The MIT Press, 2001.
Weitong Zhang, Jiafan He, Zhiyuan Fan, and Quanquan Gu. On the interplay between misspeciﬁcation and sub-
optimality gap in linear contextual bandits. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara En-
gelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine
Learning , volume 202 of Proceedings of Machine Learning Research , pp. 41111–41132. PMLR, 2023.
Rong Zhu and Branislav Kveton. Random effect bandits. In Proceedings of The 25th International Conference on
Artiﬁcial Intelligence and Statistics , volume 151, pp. 3091–3107, 2022.
Appendix
The appendix is organized as follows. Appendix A presents the posterior of µi,t. Appendix B provides a general
Bayes regret analysis, which is further demonstrated in the subsequent four sections: Appendix B.1 and Appendix B.2
delve into the Bayes regret of LinTS , addressing scenarios involving both an inﬁnite and ﬁnite number of contexts.
Appendix B.3 extends the Bayes regret of LinTS toLinUCB . Appendix B.4 provides the upper bound on the sum of
posterior variance. Appendix C presents two supplementary experiments. Finally, Appendix D explores the empirical
Bayes method for hyper-parameter selection.
A Posterior of µi,t
Lemma 1. Letzi,j∼N (0,σ2)andµi∼N (0,σ2
0). Assuming that σ2andσ2
0are known, and θ∼N (0,λI), we
have thatµi,t|Ht∼N(ˆµi,t,τ2
i,t).
Proof. Recall the following well-known identity. Let Y|X=x∼N(ax+b,σ2)andX∼N(µ,σ2
x). Then
Y∼N(aµ+b,a2σ2
x+σ2).
Obviously, if we set Xtoθ|HtandYtoµi,t|Ht, we can apply this result to obtain the distribution of µi,t|Ht
from the distributions of µi,t|θ,Htandθ|Ht. Simple calculation shows that µi,t|θ,Htis a Gaussian with mean
in˜µi,t=xi,tθ+wi,t(¯ri,t−¯x/latticetop
i,tθ)and variance in σ2
0(1−wi,t).
Now we derive the distribution of θ|Ht. Assumingθ∼N (0,λI),¯ri,tcan be considered to be generated from the
following Bayesian model:
¯ri,t|θ∼N(x/latticetop
i,tθ,σ2
0+σ2/nk),θ∼N(0,λI).
13Under review as submission to TMLR
Thus, the distribution of θ|Htis easily obtained as
θ|Ht∼N(ˆθt,Qt),
where
ˆθt=/parenleftBigg
λI+K/summationdisplay
i=1X/latticetop
i,tV−1
i,tXi,t/parenrightBigg−1K/summationdisplay
i=1X/latticetop
i,tV−1
i,tri,t,and
Qt=/parenleftBigg
λI+K/summationdisplay
i=1X/latticetop
i,tV−1
i,tXi,t/parenrightBigg−1
.
Using the above results, we obtain the distribution of µi,t|Htfrom the distributions of µi,t|θ,Htandθ|Ht. That
distribution is a Gaussian with mean in (14) and variance in (15). This completes the proof.
14Under review as submission to TMLR
B General Bayes Regret Analysis
We study a linear bandit in ddimensions with action set A⊆Rd. The model parameter is θ∗∈Rdand we assume
that it is drawn from a prior as θ∗∼N(θ0,Σ0). The mean reward of action a∈A isa/latticetopθ∗. In roundt, the learning
agent takes action At∈At, whereAt⊆A is the set of feasible actions in round t. The round-dependent action set
Atcan be used to model context. After the agent takes action At, it observes its noisy reward Yt=A/latticetop
tθ∗+εt, where
εt∼N(0,σ2)is an independent Gaussian noise.
The history in round tisHtand the posterior distribution is N(ˆθt,ˆΣt), where
ˆθt=ˆΣt/parenleftBigg
Σ−1
0θ0+σ−2t−1/summationdisplay
/lscript=1A/lscriptY/lscript/parenrightBigg
,ˆΣ−1
t= Σ−1
0+Gt, Gt=σ−2t−1/summationdisplay
/lscript=1A/lscriptA/latticetop
/lscript.
The optimal action in round tisAt,∗= arg maxa∈Ata/latticetopθ∗and our performance metric is the n-round Bayes regret
R(n) =E/bracketleftBiggn/summationdisplay
t=1A/latticetop
t,∗θ∗−A/latticetop
tθ∗/bracketrightBigg
,
where the expectation in R(n)is over random observations Yt, random actions At, and random model parameter θ∗.
We analyze two algorithms: linear Thompson sampling ( LinTS ) and Bayesian LinUCB .LinTS is implemented as
follows. In round t, it samples the model parameter as θt∼N(ˆθt,ˆΣt)and then takes action At= arg maxa∈Ata/latticetopθt.
Bayesian LinUCB is implemented as follows. In round t, it computes a UCB for each action a∈AtasUt(a) =
a/latticetopˆθt+α/bardbla/bardblˆΣtand then takes action At= arg maxa∈AtUt(a), where/bardbla/bardblM=√
a/latticetopMa andα > 0is a tunable
parameter.
The ﬁnal regret bound is stated below.
Theorem 2. For anyδ >0, then-round Bayes regret of both LinTS and Bayesian LinUCB withα=/radicalbig
2dlog(1/δ)
is bounded as
R(n)≤σmaxd/radicalBigg
2n
log(1 +σ−2σ2max)log/parenleftbigg
1 +σ2maxn
σ2d/parenrightbigg
log(1/δ) +/radicalbig
2/πσmaxd3
2nδ,
whereσmax= maxa∈A,t∈[n]/bardbla/bardblˆΣt. Moreover, when |At|≤Kholds in all rounds t, then-round Bayes regret of
both LinTS and Bayesian LinUCB withα=/radicalbig
2 log(1/δ)is bounded as
R(n)≤σmax/radicalBigg
2dn
log(1 +σ−2σ2max)log/parenleftbigg
1 +σ2maxn
σ2d/parenrightbigg
log(1/δ) +/radicalbig
2/πσmaxKnδ.
B.1 Inﬁnite Number of Contexts
We start the analysis with a useful lemma for LinTS .
Lemma 2. For anyδ>0, then-round Bayes regret of LinTS is bounded as
R(n)≤/radicalbig
2dnV(n) log(1/δ) +/radicalbig
2/πσmaxd3
2nδ,
whereσmax= maxa∈A,t∈[n]/bardbla/bardblˆΣtandV(n) =E/bracketleftBig/summationtextn
t=1/bardblAt/bardbl2
ˆΣt/bracketrightBig
.
Proof. Fix roundt. Since ˆθtis a deterministic function of Ht, andAt,∗andAtare i.i.d. given Ht, we have
E/bracketleftbig
A/latticetop
t,∗θ∗−A/latticetop
tθ∗/bracketrightbig
=E/bracketleftBig
E/bracketleftBig
A/latticetop
t,∗(θ∗−ˆθt)/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig/bracketrightBig
+E/bracketleftBig
E/bracketleftBig
A/latticetop
t(ˆθt−θ∗)/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig/bracketrightBig
. (20)
15Under review as submission to TMLR
Now note that ˆθt−θ∗is a zero-mean random vector independent of At, and thus E/bracketleftBig
A/latticetop
t(ˆθt−θ∗)/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
= 0. So we
only need to bound the ﬁrst term in (20). Let
Et=/braceleftBig
/bardblθ∗−ˆθt/bardblˆΣ−1
t≤/radicalbig
2dlog(1/δ)/bracerightBig
be the event that a high-probability conﬁdence interval for the model parameter θ∗in roundtholds. Fix history Ht.
Then by the Cauchy-Schwarz inequality,
E/bracketleftBig
A/latticetop
t,∗(θ∗−ˆθt)/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
≤E/bracketleftBig
/bardblAt,∗/bardblˆΣt/bardblθ∗−ˆθt/bardblˆΣ−1
t/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
(21)
=E/bracketleftBig
/bardblAt,∗/bardblˆΣt/bardblθ∗−ˆθt/bardblˆΣ−1
t1{Et}/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
+E/bracketleftBig
/bardblAt,∗/bardblˆΣt/bardblθ∗−ˆθt/bardblˆΣ−1
t1/braceleftbig¯Et/bracerightbig/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
≤/radicalbig
2dlog(1/δ)E/bracketleftBig
/bardblAt,∗/bardblˆΣt/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
+ max
a∈A/bardbla/bardblˆΣt
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤σmaxE/bracketleftBig
/bardblθ∗−ˆθt/bardblˆΣ−1
t1/braceleftbig¯Et/bracerightbig/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
=/radicalbig
2dlog(1/δ)E/bracketleftBig
/bardblAt/bardblˆΣt/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
+σmaxE/bracketleftBig
/bardblθ∗−ˆθt/bardblˆΣ−1
t1/braceleftbig¯Et/bracerightbig/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
.
The second equality follows from the observation that ˆΣtis a deterministic function of Ht, and thatAt,∗andAtare
i.i.d. givenHt. Now we focus on the second term above. First, note that
/bardblθ∗−ˆθt/bardblˆΣ−1
t=/bardblˆΣ−1
2
t(θ∗−ˆθt)/bardbl2≤√
d/bardblˆΣ−1
2
t(θ∗−ˆθt)/bardbl∞.
By deﬁnition, θ∗−ˆθt|Ht∼N(0,ˆΣt), and thus ˆΣ−1
2
t(θ∗−ˆθt)|Htis ad-dimensional standard normal variable. In
addition, note that ¯Etimplies/bardblˆΣ−1
2
t(θ∗−ˆθt)/bardbl∞≥/radicalbig
2 log(1/δ). Finally, we combine these facts with a union bound
over all entries of ˆΣ−1
2
t(θ∗−ˆθt)|Ht, which are standard normal variables, and get
E/bracketleftBig
/bardblˆΣ−1
2
t(θ∗−ˆθt)/bardbl∞1/braceleftbig¯Et/bracerightbig/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
≤2d/summationdisplay
i=11√
2π/integraldisplay∞
u=√
2 log(1/δ)uexp/bracketleftbigg
−u2
2/bracketrightbigg
du=/radicalbigg
2
πdδ.
Now we combine all inequalities and have
E/bracketleftBig
A/latticetop
t,∗(θ∗−ˆθt)/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
≤/radicalbig
2dlog(1/δ)E/bracketleftBig
/bardblAt/bardblˆΣt/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
+/radicalbigg
2
πσmaxd3
2δ.
Since the above bound holds for any history Ht, we combine everything and get
E/bracketleftBiggn/summationdisplay
t=1A/latticetop
t,∗θ∗−A/latticetop
tθ∗/bracketrightBigg
≤/radicalbig
2dlog(1/δ)E/bracketleftBiggn/summationdisplay
t=1/bardblAt/bardblˆΣt/bracketrightBigg
+/radicalbigg
2
πσmaxd3
2nδ
≤/radicalbig
2dnlog(1/δ)/radicaltp/radicalvertex/radicalvertex/radicalbtE/bracketleftBiggn/summationdisplay
t=1/bardblAt/bardbl2
ˆΣt/bracketrightBigg
+/radicalbigg
2
πσmaxd3
2nδ.
The last step uses the Cauchy-Schwarz inequality and the concavity of the square root. This completes the proof.
B.2 Finite Number of Contexts
The proof of Lemma 2 relies on conﬁdence intervals that hold for any context. Further improvements, from O(√
d)
toO(√logK), are possible when the number of contexts is |At|≤K. This is due to improving the ﬁrst term in (21).
Speciﬁcally, ﬁx round tand let
Et,a=/braceleftBig
|a/latticetop(θ∗−ˆθt)|≤/radicalbig
2 log(1/δ)/bardbla/bardblˆΣt/bracerightBig
16Under review as submission to TMLR
be the event that a high-probability conﬁdence interval for action a∈Atin roundtholds. Then we have
E/bracketleftBig
A/latticetop
t,∗(θ∗−ˆθt)/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
≤/radicalbig
2 log(1/δ)E/bracketleftBig
/bardblAt,∗/bardblˆΣt/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
+E/bracketleftBig
A/latticetop
t,∗(θ∗−ˆθt)1/braceleftbig¯Et,At,∗/bracerightbig/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
.
Now note that for any action a,a/latticetop(θ∗−ˆθt)//bardbla/bardblˆΣtis a standard normal variable. It follows that
E/bracketleftBig
A/latticetop
t,∗(θ∗−ˆθt)1/braceleftbig¯Et,At,∗/bracerightbig/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
≤2/summationdisplay
a∈At/bardbla/bardblˆΣt1√
2π/integraldisplay∞
u=√
2 log(1/δ)uexp/bracketleftbigg
−u2
2/bracketrightbigg
du≤/radicalbigg
2
πσmaxKδ,
whereσmaxis deﬁned as in (21). The rest of the proof is as in Lemma 2 and leads to the regret bound below.
Lemma 3. For anyδ>0, then-round Bayes regret of LinTS is bounded as
R(n)≤/radicalbig
2nV(n) log(1/δ) +/radicalbig
2/πσmaxKnδ,
whereσmaxandV(n)are deﬁned as in Lemma 2.
B.3 Bayesian LinUCB
This section generalizes Lemmas 2 and 3 to Bayesian LinUCB . The key observation is that the decomposition in (20)
can be replaced with
E/bracketleftbig
A/latticetop
t,∗θ∗−A/latticetop
tθ∗/bracketrightbig
≤E/bracketleftbig
E/bracketleftbig
A/latticetop
t,∗θ∗−Ut(At,∗)/vextendsingle/vextendsingleHt/bracketrightbig/bracketrightbig
+E/bracketleftbig
E/bracketleftbig
Ut(At)−A/latticetop
tθ∗/vextendsingle/vextendsingleHt/bracketrightbig/bracketrightbig
, (22)
whereUt(At)≥Ut(At,∗)holds by the design of Bayesian LinUCB . The second term in (22) can be rewritten as
E/bracketleftbig
Ut(At)−A/latticetop
tθ∗/vextendsingle/vextendsingleHt/bracketrightbig
=E/bracketleftBig
A/latticetop
t(ˆθt−θ∗) +α/bardblAt/bardblˆΣt/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
=αE/bracketleftBig
/bardblAt/bardblˆΣt/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
,
where the last inequality follows from E/bracketleftBig
A/latticetop
t(ˆθt−θ∗)/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
= 0, by the same argument as in Appendix B.1. To
bound the ﬁrst term in (22), we rewrite it as
E/bracketleftbig
A/latticetop
t,∗θ∗−Ut(At,∗)/vextendsingle/vextendsingleHt/bracketrightbig
=E/bracketleftBig
A/latticetop
t,∗(θ∗−ˆθt)−α/bardblAt,∗/bardblˆΣt/vextendsingle/vextendsingle/vextendsingleHt/bracketrightBig
.
Now we have two options for deriving the upper bound. The ﬁrst is
A/latticetop
t,∗(θ∗−ˆθt)−α/bardblAt,∗/bardblˆΣt≤/bardblAt,∗/bardblˆΣt/bardblθ∗−ˆθt/bardblˆΣ−1
t−α/bardblAt,∗/bardblˆΣt
≤/bardblAt,∗/bardblˆΣt/bardblθ∗−ˆθt/bardblˆΣ−1
t1/braceleftBig
/bardblθ∗−ˆθt/bardblˆΣ−1
t>α/bracerightBig
≤σmax/bardblθ∗−ˆθt/bardblˆΣ−1
t1/braceleftbig¯Et/bracerightbig
,
whereEtis deﬁned in Appendix B.1 and thus α=/radicalbig
2dlog(1/δ). Following Appendix B.1, we get
E/bracketleftbig
A/latticetop
t,∗θ∗−Ut(At,∗)/vextendsingle/vextendsingleHt/bracketrightbig
≤/radicalbig
2/πσmaxd3
2δ.
The second option is
A/latticetop
t,∗(θ∗−ˆθt)−α/bardblAt,∗/bardblˆΣt≤A/latticetop
t,∗(θ∗−ˆθt)1/braceleftBig
A/latticetop
t,∗(θ∗−ˆθt)>α/bardblAt,∗/bardblˆΣt/bracerightBig
≤A/latticetop
t,∗(θ∗−ˆθt)1/braceleftbig¯Et,At,∗/bracerightbig
,
whereEt,ais deﬁned in Appendix B.2 and thus α=/radicalbig
2 log(1/δ). Following Appendix B.2, we get
E/bracketleftbig
A/latticetop
t,∗θ∗−Ut(At,∗)/vextendsingle/vextendsingleHt/bracketrightbig
≤/radicalbig
2/πσmaxKδ.
It follows that the regret of Bayesian LinUCB is identical to that of LinTS .
17Under review as submission to TMLR
B.4 Upper Bound on the Sum of Posterior Variances
Now we bound the sum of posterior variances V(n)in Lemmas 2 and 3. Fix round tand note that
/bardblAt/bardbl2
ˆΣt=σ2A/latticetop
tˆΣtAt
σ2≤c1log(1 +σ−2A/latticetop
tˆΣtAt) =c1log det(Id+σ−2ˆΣ1
2
tAtA/latticetop
tˆΣ1
2
t) (23)
for
c1=σ2
max
log(1 +σ−2σ2max).
This upper bound is derived as follows. For any x∈[0,u],
x=x
log(1 +x)log(1 +x)≤/parenleftbigg
max
x∈[0,u]x
log(1 +x)/parenrightbigg
log(1 +x) =u
log(1 +u)log(1 +x).
Then we set x=σ−2A/latticetop
tˆΣtAtand use the deﬁnition of σmax.
The next step is bounding the logarithmic term in (23), which can be rewritten as
log det(Id+σ−2ˆΣ1
2
tAtA/latticetop
tˆΣ1
2
t) = log det( ˆΣ−1
t+σ−2AtA/latticetop
t)−log det( ˆΣ−1
t).
Because of that, when we sum over all rounds, we get telescoping and the total contribution of all terms is at most
n/summationdisplay
t=1log det(Id+σ−2ˆΣ1
2
tAtA/latticetop
tˆΣ1
2
t) = log det( ˆΣ−1
n+1)−log det( ˆΣ−1
1) = log det(Σ1
2
0ˆΣ−1
n+1Σ1
2
0)
≤dlog/parenleftbigg1
dtr(Σ1
2
0ˆΣ−1
n+1Σ1
2
0)/parenrightbigg
=dlog/parenleftBigg
1 +1
σ2dn/summationdisplay
t=1tr(Σ1
2
0AtA/latticetop
tΣ1
2
0)/parenrightBigg
=dlog/parenleftBigg
1 +1
σ2dn/summationdisplay
t=1A/latticetop
tΣ0At/parenrightBigg
≤dlog/parenleftbigg
1 +σ2
maxn
σ2d/parenrightbigg
.
This completes the proof.
18Under review as submission to TMLR
02000 4000 6000 8000 10000
Round n02004006008001000Regret¾0 = 0.00
02000 4000 6000 8000 10000
Round n02004006008001000¾0 = 0.25
02000 4000 6000 8000 10000
Round n050010001500200025003000¾0 = 0.50
LinTSRoLinTS (s0 = 0.25)RoLinTS (s0 = 0.5)RoLinTS (s0 = 1) hLinUCB Safe-FALCON
Figure 3: Comparison of misspeciﬁed RoLinTS to three baselines on synthetic problems in Section 7.1. The results
are averaged over 100runs.
0 2000 4000 6000 8000 10000
Round n050100150200250Regret¾0 = 0.00
0 2000 4000 6000 8000 10000
Round n0200400600800¾0 = 0.25
0 2000 4000 6000 8000 10000
Round n050010001500200025003000¾0 = 0.50
LinTSRoLinUCB (s0 = 0.25)RoLinUCB (s0 = 0.5)RoLinUCB (s0 = 1) hLinUCB Safe-FALCON
Figure 4: Comparison of RoLinUCB to three baselines on synthetic problems in Section 7.1. The results are averaged
over100runs.
C Additional Experiments
We conduct four additional experiments on synthetic problems in Section 7.1.
In the ﬁrst experiment, we randomly perturb parameters λandσofRoLinTS to test its robustness. For each parameter,
we choose a number u∈[1,3]uniformly at random. Then we multiply it by uwith probability 0.5and divide it by u
otherwise. That is, the parameter is increased up to three fold or decreased up to three fold. Our results are reported
in Figure 3. We observe that the regret of RoLinTS increases slightly. Nevertheless, all trends are similar to Figure 1.
We conclude that RoLinTS is robust to parameter misspeciﬁcation.
In the second experiment, we evaluate our UCB algorithm RoLinUCB . The results are reported in Figure 4. The setting
is the same as in Figure 1 and we also observe similar trends. This is expected, since Bayesian UCB algorithms are
known to be competitive with Thompson sampling (Kaufmann et al., 2012).
In the third experiment, we evaluate the dependence on Kin Theorem 1. As discussed in Section 6.1, the dependence
on√
Kdoes not vanish for any σ0>0due to the suboptimality of our proof technique. To show this, we demonstrate
thatRoLinTS withσ0→0has a comparable regret to LinUCB in a linear bandit. Note that the state-of-the-art regret
bounds of LinUCB are at mostO(logK)(Lattimore & Szepesvari, 2019). The linear bandit is the synthetic problem
in Section 7.1 with σ0= 0andK∈{20,50,100}.RoLinTS is run withσ0∈{0.1,0.05,0.01}. To distinguish σ0in
RoLinTS from that in the environment, we denote the former by s0. Our results are reported in Figure 5. We observe
that the regret of RoLinTS approaches that of LinTS asσ0→0for allK. Whenσ0= 0.01, the regret of RoLinTS is
19Under review as submission to TMLR
0 2000 4000 6000 8000 10000
Round n020406080100120140160RegretK = 20
0 2000 4000 6000 8000 10000
Round n050100150200K = 50
0 2000 4000 6000 8000 10000
Round n050100150200250K = 100
LinTSRoLinTS (s0 = 0.1)RoLinTS (s0 = 0.05)RoLinTS (s0 = 0.01)
Figure 5: Regret of RoLinTS as a function of the numbers of arms K.
0 5000 10000 15000 20000
Round n02004006008001000120014001600Regret¾0 = 0.25
0 5000 10000 15000 20000
Round n02004006008001000120014001600¾0 = 0.50
0 5000 10000 15000 20000
Round n0500100015002000¾0 = 1.00
LinUCB UCB1RoLinTS (s0 = 0.25)RoLinTS (s0 = 0.5)RoLinTS (s0 = 1)
Figure 6: Comparison of RoLinUCB to two baselines on synthetic problems in Section 7.1. The features are sampled
only once in round 1and then kept ﬁxed. The results are averaged over 100runs.
almost indistinguishable from that of LinTS . This validates our hypothesis that the regret of RoLinTS does not seem
to increase signiﬁcantly with Kasσ0→0.
In the last experiment, we try to compare to Ghosh et al. (2017). Their algorithm performs an initial exploration, and
then runs either LinUCB (Abbasi-Yadkori et al., 2011) or UCB1 (Auer et al., 2002). A comparison to this algorithm is
challenging for two reasons. First, the algorithm is non-contextual and thus would have linear regret in Figure 1. To
have a more fair comparison, we experiment with non-contextual problems here. Speciﬁcally, the features of the arms
in Section 7.1 are sampled only once in round t= 1and then kept ﬁxed. Second, the algorithm of Ghosh et al. (2017)
has tunable parameters. Rather than ﬁxing them, which could put the algorithm at a disadvantage, we compare to its
two components: LinUCB andUCB1 . IfRoLinTS can outperform both, the algorithm of Ghosh et al. (2017) could not
be competitive with RoLinTS . Our results are reported in Figure 6. In all problems, UCB1 has a high regret because
the number of arms is large, K= 50 . Moreover, when the model misspeciﬁcation is high, σ0≥0.5,LinUCB has a
linear regret. Therefore, when σ0≥0.5,RoLinTS would outperform the algorithm of Ghosh et al. (2017).
20Under review as submission to TMLR
D Choosing Hyper-Parameters
In this paper, we assume that the hyper-parameters are known by the agent. When the hyper-parameters are unknown,
they have to be estimated. We end this paper by discussing this challenge.
For a fully-Bayesian treatment of hyper-parameters, it is desirable to marginalize over them. However, this is compu-
tationally intensive, often involving an integration. Let ψ= (σ2,σ2
0,λ)/latticetopandˆµi,t(ψ)be the prediction as a function
ofψ. By assuming some distribution on ψ, the fully-Bayesian method tries to take the following integration:
ˆµFB
i,t=/integraldisplay
ˆµi,t(ψ)p(ψ|Ht)dψ.
Although the integral can be done using many powerful tools in Bayesian statistics, the fully-Bayesian method is
sensible to the prior setting of ψ, more seriously, is computationally intensive in bandit algorithms that requires
sequential updating.
Compared to a fully-Bayesian treatment, a simpler method is the empirical Bayes method (Carlin & Louis, 2000),
which plugs in the estimates of the hyper-parameters from data. Various methods of obtaining consistent estimators
are available, including the method of moments, maximum likelihood, and restricted maximum likelihood (Harville,
1988). Here we adopt the method of moments, since it has explicit formulas to update each round.
Unbiased quadratic estimate of σ2is given by
ˆσ2=/parenleftBiggK/summationdisplay
i=1(ni,t−d)/parenrightBigg−1K/summationdisplay
i=1/summationdisplay
/lscript∈Ti,tˆe2
i,/lscript,
wherent=/summationtextK
i=1ni,t, and ˆei,/lscriptare the residuals for the regression of the rdeviation,ri,/lscript−¯ri,t, on the context x
deviations, xi,/lscript−¯xi,t, for those rewards with ni,t>1. Unbiased quadratic estimate of σ2
0is given by
˜σ2
0=(nt−d0)−1
K/summationdisplay
i=1/summationdisplay
/lscript∈Ti,tˆs2
i,/lscript−(nt−d)ˆσ2
,
whered0= (/summationtextK
i=1X/latticetop
i,tXi,t)−1/summationtextK
i=1n2
i,t¯xi,t¯x/latticetop
i,tandˆsi,/lscriptare the residuals for the regression of ri,/lscripton the context
xi,/lscript. It is possible that ˜σ2
0is negative. Thus, we deﬁne ˆσ2
0= max{˜σ2
0,0}. For simplifying the choice of λ, it is not
bad to letλequal a tiny constant, such as λ= 0.01.
Although the empirical Bayes method with plugged-in variance estimates is convenient and useful, it is challenging
to analyze. The reason is that the randomness in the estimated hyper-parameters needs to be considered in the regret
analysis. We leave the theoretical investigation of this challenge as an open question of interest.
21