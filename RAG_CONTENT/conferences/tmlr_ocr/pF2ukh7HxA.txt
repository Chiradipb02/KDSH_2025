Under review as submission to TMLR
FlashAttention on a Napkin: A Diagrammatic Approach to
Deep Learning IO-Awareness
Anonymous authors
Paper under double-blind review
Abstract
Optimizing deep learning algorithms currently requires slow, manual derivation, potentially
leaving much performance untapped. Methods like FlashAttention have achieved a ×6per-
formance improvement over native PyTorch by avoiding unnecessary data transfers, but
required three iterations over three years. Automated compiled methods have consistently
lagged behind. GPUs are limited by both transfers to processors and available compute,
with transfer bandwidth having improved at a far slower pace. Already, transfer bandwidth
accounts for 46% of GPU energy costs. This indicates the future of energy and capital-
efficient algorithms relies on improved consideration of transfer costs (IO-awareness) and
a systematic method for deriving optimized algorithms. In this paper, we present a dia-
grammatic approach to deep learning models which, with simple relabelings, derive optimal
implementations and performance models that consider low-level memory. Diagrams gen-
eralize down the GPU hierarchy, providing a universal performance model for comparing
hardware and quantization choices. Diagrams generate pseudocode, which reveals the appli-
cation of hardware-specific features such as coalesced memory access, tensor core operations,
and overlapped computation. We present attention algorithms for Ampere, which fits 13
warps per SM (FlashAttention fits 8), and for Hopper, which has improved overlapping and
may achieve 1.32 PFLOPs.
1 Introduction
1.1 Background
To execute an operation, graphical processing units (GPUs) must move data from high-level DRAM to
low-level compute cores. GPUs are as limited as much by GB/s of memory bandwidth as TFLOPs of avail-
able compute. However, AI models have passed the memory wall —algorithms are increasingly limited by
bandwidth/transfer costs (Ootomo & Yokota, 2023; Ivanov et al., 2021; Gholami et al., 2024), as compute
capabilityhasimprovedfarmorequickly ×3/2ythanDRAMbandwidth ×1.6/2y(Gholamietal.,2024). Fur-
thermore, DRAM already accounts for 46%of total system power (Ghose et al., 2018). As memory becomes
increasingly inefficient relative to compute, the importance of considering transfer costs— IO-awareness (Dao
et al., 2022; Aggarwal & Vitter, 1988)—will become even more critical.
FlashAttention (Dao et al., 2022; Dao, 2023; Shah et al., 2024) is an IO-aware approach to attention that
overcomes the memory wall. Attention (Vaswani et al., 2017) is central to generative models, including
large language models (LLMs) (Jiang et al., 2024; Dubey et al., 2024) and image generation algorithms (Ho
et al., 2020; Esser et al., 2024; Rombach et al., 2022; Podell et al., 2023). FlashAttention fusesthe steps
of attention. It computes all sequential steps on low-level memory, avoiding unnecessary intermediate data
transfers. It achieves a ×6increase in throughput compared to standard PyTorch, arguably making large
contemporary models possible.
However, the conditions under which fusion is possible are not generally exploited. Simple cases like element-
wise functions can be compiled into matrix multiplications (Li et al., 2020; Paszke et al., 2019; Sabne, 2020),
but the bespoke techniques of FlashAttention required manual derivation and three iterations over three
1Under review as submission to TMLR
years to take full advantage of Hopper hardware (NVIDIA, 2022) features. Triton(Tillet et al., 2019) offers
some compilation for hardware features but has lagged behind new FlashAttention algorithms (Dao, 2023;
Shah et al., 2024). The current best technique for generating IO-aware algorithms that exploit hardware
features remains slow, manual derivation.
Innovating new optimized algorithms is essential to efficient deployment of models. In addition to FlashAt-
tention, methods like grouped query attention (Ainslie et al., 2023), KV-caching (Shazeer, 2019), and quan-
tization (Frantar et al., 2023; Gholami et al., 2021) all reduce transfer costs while having minimal impact on
the function we implement or the quality of model outputs. Much like fusion, the success of these approaches
relies on understanding the compositional structure of algorithms so that similar but less costly algorithms
can be executed. A systematic approach to innovating optimized algorithms will require a mechanism for
understanding the compositional structure of algorithms along with a performance model which compares
varying means of executing the same operation.
The hardware characteristics of GPUs have a significant impact on performance which varies depending on
the target algorithm. When choosing between A100s, H100 SXM5s, or H100 PCIes (NVIDIA, 2022, p.39),
we must consider the varying compute, bandwidth, intermediate hierarchies, architecture features, and low-
level memory, for which we pay in environmental and economic resources. The application of these features
is often non-obvious, FlashAttention-2 (Dao, 2023) was released while the hardware for FlashAttention-3
already existed (Shah et al., 2024), which achieved ∼75%improvement in forward speed. Understanding
the impact of GPU features is a necessary component of innovating optimized approaches and making full
use of deployed resources.
1.2 Contributions
This paper contributes a diagrammatic scheme for representing deep learning algorithms based off Neural
Circuit Diagrams (Abbott,2023)(Section2)whichcanbeusedtoquicklyderivemethodslikeFlashAttention
along with a performance model for transfer costs which factors in lower-level cache size (Section 3). We then
show how the performance model scales to a multi-level hierarchy, providing expressions that considers GPU
hierarchyconfigurationsandthememorysensitivityofalgorithms(Section4). Finally, weshowhowdiagrams
can be converted to pseudocode, which reveals the application of hardware-specific features like coalesced
memory access, tensor-core operations, and overlapping operations (Section 5). To show the advantages of
this approach, we present Ampere and Hopper attention algorithms with reduced low-level memory usage
compared to FlashAttention.
2 Diagramming Deep Learning Algorithms
2.1 Diagramming Functions and Data Types
Diagrams have alternating columns of data types and functions. Data type columns are shown in Figure
1. Arrays such as Ra×b×care represented by a wire for each axis labeled with the size. Data types may be
tuples of arrays, such as Ra×b×c×Rd×e×c, and are represented by placing a dashed line between constituent
arrays.
Figure 1: We represent arrays, of forms such
asRa×b×c, by labeling stacked wires in a column
witha,b, andc. To represent data types that
consist of lists of arrays, such as Ra×b×c×Rd×e,
we place a dashed line between them.
Functions between data types are represented by labeled boxes or pictograms with their input/output shapes
to the left/right. Sequential execution ( composition ) of functions is shown by horizontal placement (Figure
2, creating a diagram with alternating columns representing data types and functions. Parallel execution
2Under review as submission to TMLR
(concatenation ) of functions stacks them with a dashed line in between (Figure 3). A concatenated function
takes concatenated inputs and provides concatenated outputs. The change in the input/output is reflected
by the diagram.
Figure 2: Functions are represented by la-
beled boxes or pictograms, which aid intuition.
These representations can be horizontally com-
posed, which represents sequential execution and
yields a diagram with alternating data type and
function columns. We represent composition by
F;G=G◦F.
Figure 3: Functions can also be stacked with a
separating dashed line, which concatenates their
inputs and outputs. Concatenating tuples ⊗is
considered to be associative. For concatenated
functionsF⊗G, ifF(x) =x′andG(y) =y′
then (F⊗G)(x⊗y) =F(x)⊗G(y).
We represent identity functions that leave inputs unchanged by extending the data type. This reflects that
composition with identities leaves a function unchanged. Functions are stateless and are defined by how they
map inputs to outputs. Therefore, we concatenate with identities to represent a function acting on some
data but leaving the rest unchanged and available. With these tools, we can build compound diagrams such
as Figure 4.
Figure 4: A compound diagram can be dis-
assembled into columns representing alternating
functions and data types. Stacked functions and
data types can be further decomposed to find the
core units concatenated to construct them. Iden-
tities are represented by continuing the represen-
tation of data types.
Functions can be mapped over an additional axis, represented by weaving the axis over the function’s outputs
and a choice of the inputs (Figure 5). This diagrammatic implementation naturally updates the input and
output sizes for the mapped function. When an input segment is not weaved, its data is copied to evaluate
each index along the outputs of the new axis. The axis can be weaved into any location of the target
segments.
Weaving a function allows for complex mappings to be represented and avoids the ambiguity of typical
expressions. We can weave primitives defined on items, such as multiplication, addition, and copying. We
use weaving to represent the splitting and joining of shared axes, which overcomes the typical difficulties of
expressing how arrays are partitioned and concatenated. We show this in Figure 6.
3Under review as submission to TMLR
Figure 5: A function can be weaved, which adds an axis to the outputs and some of the inputs. The
function is mapped over this axis. When we weave the “item” Rarray represented by a thick dotted wire,
we can remove it. Here, we provide a weaving for SoftMax, represented by a triangle, to have it act over
each row of an array, and of linear contraction ( dot/inner product ), which provides matrix multiplication.
Figure 6: Weaving primitive functions let us express the addition of vectors to each row of an array,
multiplication over a specific axis, and the copying of arrays. We can use weaving to split an array along
slices of an axis or show the axes over which arrays are joined.
2.2 Representing Deep Learning Algorithms
We have so far expressed functions — maps between inputs and outputs — diagrammatically. Deep learning
models employ algorithms , which implement a function but have resource usages and inputs/outputs located
at different levels. We embed algorithms in a hierarchy. A hierarchy consists of levels connected with pipes
(as in Figure 7), which allow for memory sharing with a family of cores located at the level below. The
available algorithms are restricted to those provided at each level of the hierarchy.
Figure7: Wecandiagramhierarchies
using a graph showing the available
levels and their connections. For hi-
erarchies representative of real GPUs,
we note the layer abstraction and the
physical memory it occupies in italics.
We use colors to represent levels, and color arrays to diagram where they are located. In this section, we use
a two-level model where higher level ℓ0arrays are colored black and lower level ℓ1arrays are colored orange.
This lets us diagram algorithms as in Figure 8.
We are interested in algorithms’ total transfer cost Hℓand maximum memory usage per core Mℓ(memory
usage). These resource usages are defined per level and measured in number of values, and can be determined
4Under review as submission to TMLR
Figure 8: Here, we diagram an algorithm which takes a SoftMax
over aq×xarray and contracts it over a x×darray. We per-
form transfers to move data to lower levels for computation. This
diagram shows sequential execution, concatenation, and weaving of
algorithms diagrammatically.
from diagrams as in Figure 9. Total transfer costs are equal to the total size of data loaded to and saved
from a level, equal to the sum of the size of arrays changing colors. Memory usage is lower bounded by
the maximum size of data at a level for any column. We aim to minimize the total transfers while keeping
memory usage below a hardware limit per level, Mmax
ℓ.
Figure 9: The SoftMax-Contraction algorithm from
Figure 8 can have its transfer cost derived from the total
size of data changing colors and its memory usage lower
bound determined by the maximum data present at the
lower level at any point. We assume that x>d.
As diagrams show all data types, operations, and their arrangement, we can adapt our performance model to
consider all aspects of an algorithm’s performance. Using diagrams, we can approximate compute by taking
the compute required to execute an algorithm multiplied by the size of axes it is weaved over. A k-size
contraction requires 2kFLOPs; therefore, m×kbyk×nmatrix multiplication requires 2mknFLOPs. In
Appendix A.5.3, this is used to find the clock cycles required per column to overlap computation.
2.3 Group Partitioning
The first optimization strategy we introduce is group partitioning ( tiling) a mapped algorithm (Figure 10).
If an algorithm is weaved over an axis, then the axis can be split, the mapped function applied, and the axis
rejoined to yield the same result. Each sub-algorithm acts on a batch of the data in a separate core with
reduced low-level memory usage.
Figure 10: A weaved algorithm is functionally
equivalent to sub-algorithms acting on partitions
of the weaved axis. We use ≡to indicate func-
tional equivalence, meaning algorithms map the
same inputs to outputs but may have distinct
resource consumption profiles, and therefore are
not strictly equivalent. These partitions can be
of any size, which we write as ga. We can recur-
sively expand the expression on the right until
a′≤ga. The unweaved segment of the data must
be loaded by each sub-algorithm.
We can diagram this strategy by labeling the weaved axis awith the target group size gawhile at the lower
level as in Figure 11. The low-level memory usage Mℓfor the diagram is then calculated using this group
size, not the full size of the axis. Each sub-algorithm needs to load and save its batch input and output
data. The per-group transfer cost Hℓ,gis calculated using the gagroup size for the partitioned axis which is
multiplied by Nℓ,g=a/gabatches to attain Hℓ=Nℓ,gHℓ,g.
Non-grouped inputs are sent to all active cores at the lower level, meaning their transfer costs are multiplied
byNℓ,gwithout reduced per-group transfer costs. Smaller group sizes gadecrease memory usage but increase
5Under review as submission to TMLR
Nℓ,g, increasing Hℓif there is an unweaved input. To reduce total transfer costs, we must find the maximum
gavalue that does not exceed maximum memory usage Mmax
ℓ.
Figure 11: Group partitioning can be represented by
relabeling an algorithm with the partition batch size
(group size) at the lower level. The group size is used for
memory usage and per-group transfer cost calculations
for the lower level. Per-group transfer costs are then
multiplied by the number of groups Nℓ,g=a/gato give
the overall transfer costs.
If multiple weaves are relabeled, the data is batched over each as in Figure 12. The total number of sub-
algorithmsistheproductofthenumberofbatchesforeachaxis, Nℓ,g=ab/gagb. Therelabeledsub-algorithm
represents the memory usage and per-group transfer costs of each sub-algorithm, using the group sizes gaand
gbfor resource usage calculations. We then multiply the transfer costs by Nℓ,g. We can use this to determine
the optimal group sizes for an algorithm grouped over multiple axes, such as matrix multiplication (see
Section 3.1), and to determine whether it is worth grouping a small axis or transferring its full size.
Figure 12: A function with multiple weaves can
have a relabeling applied to each of its weaves.
The relabeled sub-algorithm provides the mem-
ory usage and per-group transfer costs using the
group sizes.
2.4 Stream Partition
Stream partitions ( recomputation ) exploit recursively decomposable polymorphic functions to feed data in
batches while maintaining intermediate outputs on-chip, reducing low-level memory usage. Functions can
be streamed if they are polymorphic for a specific axis (defined for that axis being of any size) and have an
accumulator that can incorporate incoming data to recompute the maintained output, as shown in Figure
13. This allows for a recursive expansion (see Figure 14) that maintains minimum data on-chip at any point.
Figure 13: The condition for streaming re-
quires that a function Fbe polymorphic along
the axisaand can be decomposed in the man-
ner above, requiring the existence of another
polymorphic function Bcalled the accumula-
tor.
Figure 14: If the condition in Fig-
ure13ismet,thefunctioncanbere-
cursively decomposed until a′≤sa.
Thisallowsthefunctiontobeevalu-
ated from batches of the input data.
If an axis originates from a transfer and is fed to a recursively decomposable polymorphic function, then
it can be relabeled with the streaming batch size (stream size) sb. This creates a representation of the
sub-algorithm Bwhich is repeatedly applied to process the data. We need to add the output size yin
6Under review as submission to TMLR
parentheses at the input to consider its contribution to memory usage. The memory usage of the algorithm
is then determined using the stream size sbinstead of the full axis size b. As we eventually stream the entire
axis, we use the full axis size bto evaluate transfer costs. Typically, we strictly benefit from limiting the
stream size to 1as this reduces memory usage while imposing no increase in transfer costs.
Figure 15: We can relabel a streamable axis with the batch
sizesbas it is transferred. We are required to add the y
output array shape at the input to the streamed algorithm.
This lets the relabeled diagram derive the memory usage at
the lower level using the stream size sb. As all data along
the axis is transferred to the chip, the full axis size bmust
be used for transfer costs.
Per the fusion theorems of Appendix A.1, streamable axes are resistant to modifications. The streamable
axis may be a single axis of an array, and composing or weaving a streamable algorithm while maintaining
this axis yields a streamable algorithm. This allows the stream labeling to be maintained for resource usage
evaluation as in Figure 16. This allows the streamability of complex functions like attention to be derived
from a streamable kernel. In Figure 17, we apply group partitioning to a mapped streamable algorithm. We
usegqfor per-group transfer evaluations, and both gqandsbto evaluate memory usage.
Figure 16: For a modified streamed algorithm,
we can continue to use the stream batch size sb
for memory usage evaluations. As the function
generatesq×rdistinctyvalues, it needs to main-
tain each on memory, resulting in y×q×rmain-
tained memory before and after the repeated E
algorithm.
Figure 17: We can apply multiple relabelings
to an algorithm. This lets us find the per-group
memory usage and transfer cost. As the func-
tion is mapped within each group, it needs to
maintaingqcopies of the maintained ydata, in-
creasing its memory usage.
3 Examples
3.1 Matrix Multiplication
As contraction (dot product) is streamable (see Appendix A.3.1), we can use it as a kernel for deriving
the streamability of matrix multiplication, its weaved form. This provides a diagram that supplies a per-
formance model. We then optimize for the batch sizes to minimize total transfers given some maximum
lower-level memory usage M. Unlike standard approaches (Gholami et al., 2024; Ootomo & Yokota, 2023),
this performance model indicates that the transfer cost of n=a=b=cmatrix multiplication is cubic for
n≥√
M/2.
7Under review as submission to TMLR
Ng=ac
gagcH=NgHg
Hg=gab+bgc+gagc =ac
gagc(gab+bgc+gagc)
M⩾gagc+gasb+sbgc = 2abc g−1
a+ac
√
M⩾gc=ga ⩾2abc M−0.5+ac
Figure 18: The dot product is a streamable function. Therefore, matrix multiplication, which is the weaved
form of it, is also streamable and can be group partitioned.
3.2 Attention
We derive the streamability of attention from the fusion theorems. We begin with the fact that SoftMax-
Contraction is streamable (Appendix A.4). Then, we can compose with a contraction over the queries as an
Ealgorithm from Figure 16. This generates a streamable algorithm, which we weave with the qanddaxes.
This generates Figure 19. Correctness is ensured as the diagram gives the typical expression for attention,
O=SoftMax/parenleftbig
Q·KT/parenrightbig
·V, with axes clearly indicated. We can then label qto distribute the queries across
processors, yielding the FlashAttention algorithm. Figure 19, then, can be seen as deriving and providing a
performance model for FlashAttention.
Ng=q/gq H=NgHg
Hg= 2gqd+ 2xd =q
gq(2gqd+ 2xd)
M⩾2gqd+ 2sxd = 2qd+ 2xdq g−1
q
∴gq⩽M/2d ⩾2qd+ 4xqd2M−1
Figure 19: SoftMax followed by a contraction is streamable. We precompose with a contraction Eweaved
bysxand provide weavings by qat the top and dat the bottom to construct attention.
We can apply a similar technique to find the transfer cost of grouped query attention (Ainslie et al., 2023)
(Figure 20) and multi-head attention (Vaswani et al., 2017) (Figure 21). These use additional weaves, but
their evaluation remains straightforward. This shows how diagrams can be used to both derive optimizations
and experiment with modifications to the algorithm, motivating further innovation.
Figure 20: Grouped
query attention has
an additional weave
accompanying the
queries. This provides
additional fidelity for
the algorithm with
minimal impact on
total transfers.
Ng=gq/(gqgg)H=NgHg
Hg= 2gqggd+ 2xd =gq
gqgg(2gqggd+ 2xd)
M⩾2gqggd+ 2sxd = 2gqd+ 2xdq/(gqgg)
∴gqgg⩽M/2d ⩾2gqd+ 4xqd2M−1
8Under review as submission to TMLR
Figure 21: Multi-head
attention conducts multi-
ple attention algorithms in
parallel. It is represented
by an additional haxis
weaved over every opera-
tion. Notice how scaling
hleads to a linear change
in costs, while dleads to a
quadratic change.
Ng=qh/(gqgh)H=NgHg
Hg= 2gqghd+ 2xd =qh
gqgh(2gqghd+ 2xd)
M⩾2gqghd+ 2sxd = 2qhd+ 2xqhd/(gqgg)
∴gqgh⩽M/2d ⩾2qhd+ 4xqhd2M−1
4 Analysis of Performance Models
Once a two-level model optimization of an algorithm is found, we can extend it to consider a multi-level
hierarchy. Each lower level has tiles which fit into the level above (Figure 22), meaning the optimal strategy
and performance model extend in a generalizable manner. We can create universal performance model for
transfer costs which considers the impact of the GPU hierarchy and the transfer rate and memory caches
at different levels. This allows us to make informed choices between different GPU architectures given their
energy and capital costs, levels of quantization we employ, and the configuration of GPU hierarchies.
4.1 Optimal Transfers H∗(⃗ a,M )
Applying the two-layer model to diagrams provides optimal transfer costs H∗(⃗ a,M )given some configuration
of axis sizes ⃗ aand lower-level memory M. So far, these expressions have a standard form given by the sum
of power functions which solves for Equation 4 in Appendix A.2:
H∗(⃗ a,M ) =/summationdisplay
tαt(⃗ a)M−βt(1)
The indextiterates over terms, the coefficient αt(⃗ a)is dependent on the axis sizes, and βt⩾0is an exponent
greater than zero as transfers necessarily decrease with increased memory size. The exponents βtindicate
the sensitivity of performance to memory size and indicate how data is distributed. For attention, the M−1
factor indicates the data is broadcast to all groups, while for matrix multiplication the M−0.5factor indicates
square tile distribution.
4.2 Multi-Level Performance Models
An algorithm with multiple levels requires H∗(⃗ a,Mℓ)transfers for each. Even though data cannot be directly
transferredfromthehighesttolowestlevels, lowerlevelscanutilizethedataloadedandsavedbyintermediate
levels. The execution of H∗(⃗ a,Mℓ1)intermediate level transfers makes data available for the lower level ℓ2
and accounts for saving it back. Each intermediate-level tile fits a larger number of low-level tiles, among
which it distributes its data. This fitting process has a negligible error with large M. We assign a weighted
transfer cost ˙H−1
ℓto each level. For the highest level, we assume Mℓ0→∞and ˙H−1
ℓ0= 0, as data is already
present. This means that the total weighted transfer cost of an algorithm can be expressed by:
H∗=/summationdisplay
ℓ˙H−1
ℓH∗(⃗ a,Mℓ) =/parenleftigg/summationdisplay
tα(⃗ a)/summationdisplay
ℓ˙H−1
ℓM−βt
ℓ/parenrightigg
(2)
Therefore, the relative performance of GPUs is determined not just by the raw transfer rates but also by the
memory size of available levels and the specific algorithm being implemented. For attention, the key factor
per level is∼˙H−1/M. In Appendix A.5.3, we see that Hopper, by effectively doubling low-level memory,
doubles the amount that K,Vstreams are shared. This is equivalent to an Ampere architecture with double
the bandwidth, highlighting the importance of low-level memory and architecture features.
9Under review as submission to TMLR
4.3 Quantization
Equation 2 and the two-level model consider transfers and storage limits in terms of number of values.
However, GPUs are restricted by the number of bytes we can transfer and store. If we have qbytes per
value, then the maximum number of values Mℓ=MBytes
ℓ/qand the transfer weight is ˙H−1
ℓ= (˙HBytes
ℓ/q)−1.
Substituting these expressions into the total transfer cost, we get:
H∗Bytes=/summationdisplay
ℓ(˙HBytes
ℓ/q)−1H∗/parenleftig
⃗ a,MBytes
ℓ/q/parenrightig
=/parenleftigg/summationdisplay
tα(⃗ a)/summationdisplay
ℓ(˙HBytes
ℓ)−1(MBytes
ℓ)−βq1+β/parenrightigg
(3)
As1 +β⩾1, total transfers are superlinear to the degree of quantization. Halving the quantization from
FP32toFP16can accelerate attention by up to ×4, and improves large matrix multiplication by a factor
of21.5≈2.83. This indicates that a generous use of quantization and specialized kernels is critical to
high-throughput implementations of models.
4.4 Intermediate Caching
We can choose to store output data at lower levels, and save it up in chunks. This changes the level
immediately above the lower level to a caching level, which we indicate by adding asterisks to its output
data as in Figure 22. The size of this column is not memory restricted by the intermediate level which is
only used to temporarily store data as it is sent up. However, the lower levels must remain active to store
data, and this imposes the restriction that Ng,ℓ2/Ng,ℓ1⩽Nmax
ℓ2which is a hardware limit. With an output
restricted algorithm, this results in H∗(⃗ a,Nmax
ℓ2Mℓ2)transfers being required for the intermediate level ℓ1,
using the total lower level memory Nmax
ℓ2Mℓ2instead of its own hardware maximum memory Mmax
ℓ1. This is
elaborated in Appendix A.2.1.
Figure 22: With multiple levels in a hierarchy,
we can maintain output or streamed data at the
lowest levels and use the intermediate levels as
a cache. The cache size does not contribute to
theMmax
ℓ1restriction. Instead, we require that
Nℓ2,g/Nℓ1,g⩽Nmax
ℓ2. If an algorithm is output
restricted, we can implement this by the cache
size being less than Mℓ=Mℓ1Nmax
ℓ2.
4.5 Cross-Transfer Levels
Our model can encompass levels that perform inter-core communication instead of providing shared memory
byusingmodifiedweightedtransferweights. Thesecross-transferlevelsencompassH100threadblockclusters
(Luo et al., 2024), multi-GPU systems, and intra-warp communication. We set up has a higher level, xas a
cross-transfer level, and cas a child level composed of linked processors. Instead of sending H∗(⃗ a,Mc)data
directly to children, we send H∗(⃗ a,McNmax
c)data to any of the interconnected children and cross-transfer
the remaining data as in Figure 23. This results in a performance model with modified transfer weights
and levels, adding a level xbetweenhandcwith transfer weight ˙H−1
h→c−˙H−1
x→cand memory McNmax
c, and
replacing the transfer weight of level cwith ˙H−1
x→c. We outline this derivation in the Appendix A.2.2.
10Under review as submission to TMLR
Figure 23: To send data to children, we
directly transfer H∗(⃗ a,McNmax
c)distributed
across the child processors, treating the cross-
transfer level as having memory of size
McNmax
c. We then perform the remaining
H∗(⃗ a,Mc)−H∗(⃗ a,McNmax
c)transfers as fast
cross-transfers.
5 Pseudocode and Hardware Optimizations
The abstract models we have provided so far hint at optimal algorithms and provide resource usage scaling
lawsatgreaterresolutionthanthetheoremsfromFlashAttention(Daoetal.,2022). Shiftingfromanabstract
model to applications requires considering specific batch-size configurations and expanding relabeled stream
diagrams into looped diagrammatic pseudocode. We use abstract models as a guide to the ideal size of
axes, and then impose that they should be integers divisible by certain sizes to take advantage of coalesced
memory access and tensor cores. We can expand streamed diagrams into loops where all variables and the
accumulator Bare fully expressed, allowing for fine-tuned configuration of batch sizes and the exploitation
of Hopper overlapped computation.
5.1 Coalesced Memory Transfer
Between the device and SMEM memory, GPUs move data in chunks of 128Bof consecutive memory. This
is remarkably straightforward to represent with diagrams. Arrays represent how data is distributed across
each stride, so the lowermost axis of an array represents consecutively stored memory. If we enforce that the
lowermost axis is divisible by 128B/qwhen assembled in the device and transferred between GMEM and
SMEM, then we can assure coalesced memory access. This may require that each thread block stream loads
data for multiple lower-level streams. If using SMEM as a cache, there is usually plentiful memory available
for larger streams.
A floating divisor in the superscript of an axis/batch size is used to indicate a value it is divisible by (see
Figure 24). This is done at the point where the restriction is imposed and along the immediately weaved
axis. Multiple divisors impose the least common multiple.
5.2 Tensor Core Operations
Tensor cores provide very fast matrix multiplication. Modern GPU tensor cores have far more FLOPs
available than general-purpose register operations (NVIDIA, 2020; 2022). We can re-express matrix mul-
tiplications as tensor core operations. This requires saving to and loading from SMEM memory if data is
initially at the register level. Tensor cores ( wmma) can only manage data at certain sizes and quantizations
(NVIDIA, 2024), which must be considered by diagrams. We can add a floating tag to indicate quantization.
Matrix multiplications of larger sizes can be implemented by adding multiple smaller matrix multiplica-
tions, making divisibility by the available sizes the critical factor. We can enforce this restriction by placing
superscripts for tensor core axes.
Figure 24: Multi-level matrix multiplication
usesthe SMEMlevelto cache data for lower-level
tensor core operations. We enforce the divisibil-
ityrestrictionsforcoalescedSMEMtransfersand
tensor cores using superscripts.
11Under review as submission to TMLR
5.3 From Diagrams to Pseudocode
We can expand streamed algorithms into looped pseudocode forms where all variables are explicitly shown as
in Figure 25. The columns of pseudocode diagrams provide the size of variables required in memory and the
transfers/operations we need to apply. This allows us to pre-allocate memory to derive the exact memory
usages, as well as per-group transfer and compute costs. Columns act like lines of code but more clearly
express the details of axes and available optimization strategies than textual methods. As polymorphic
streamed algorithms are defined for the stream axis being of any size, we can begin the algorithm with a
headFtaking an axis of size 0, initializing the maintained output to incorporate further information.
Figure 25: A streamed algorithm can be re-expressed with an explicit loop. The diagram illustrates how
axes are partitioned, which variables are maintained, and what operations are applied iteratively.
We can use these diagrams to express the exact details of algorithms like Ampere attention in Figure 26.
Pseudocode expansions allow us to transfer to and from fragmented memory to execute both tensor-core
and general operations. This requires us to use exactly 32times as many thread groups as tensor core
groups. Furthermore, we can place dotted enclosed sub-loops. We add a circle to incoming or outgoing axes,
which are iterated over the original size within the sub-loop. This constructs matrix-multiply add operations
and lower-level substreams while accurately presenting the size of required variables, and imposes a divisor
constraint.
As maintaining a diagram’s shape ensures correctness, pseudocode expressions allow extensive tinkering.
We present an Ampere Attention algorithm in Figure 26. Unlike FlashAttention-2 (Dao, 2023), we use
registers to store and scale the maintained variable instead of tensor cores. We use smaller sub-loop stream
sizessx→u(8)
xandd→16to reduce register memory usage. Extending this algorithm to grouped query
attention and multi-head attention simply involves weaving in additional axes, as we did in Figures 20 and
21. In Appendix A.5.2, we derive the memory usage for our algorithm and show that it can fit 13 warps per
thread block instead of FlashAttention-2’s 4-8 warps.
TheAmperediagramhintsatthetechniquesofFlashAttention-3(Shahetal.,2024), whichexploitshardware
features of the H100 Hopper architecture. Hopper allows for warpgroups, enabling 128 thread groups per
tensor core and storing some of their memory on SMEM. Hopper enables explicit asynchrony, allowing
different warpgroups to simultaneously execute different operations. We allocate some warp groups to load
data (producer-consumer asynchrony). Distinct processors on the SM execute different operations, so we
can stagger different warpgroups to simultaneously execute green and blue columns. We can divide compute
cost per column by operations per clock cycle to determine ideal overlapping. In Figure 27, we show our
Hopper Attention. In Appendix A.5.3, we show the required memory and the staggering strategy indicated
by the diagram. This algorithm can theoretically achieve close to the maximum 1.34PFLOPs of H100 SXM5
compute.
12Under review as submission to TMLR
Figure 26: Ampere Attention shown using pseudocode. The diagram begins by distributing Qvalues to
tensor cores, then each loop loads and computes a streamed portion of KandV. The sub-loop splits the
streamed axes into smaller chunks. Certain operations (SMEM loads, constructed matrix multiply-add)
impose divisor restrictions, which we place. We show required memory allocations in Appendix A.5.2.
Figure 27: Hopper attention incorporates larger tensor core groups and FP8for specific operations. The
algorithm is divided into producer and consumer workflows, as indicated by the separation between active
green and blue columns, enabling staggered warp group operations.
6 Conclusions
In this work, we have used diagrams to derive, analyze, and fine-tune optimized deep learning algorithms.
The complexities of generating optimized algorithms: tiling, streaming, and applying hardware features, are
reduced to simple relabelings of diagrams. This vastly improves over existing manual derivations.
This work also compels future research. The performance model and the hypothesized algorithm remain to
be empirically validated. Mixture-of-expert models (Jiang et al., 2024) use immense resources and therefore
optimizations are particularly impactful. Additional strategies can be formalized. Convolution and sliding
window attention (Beltagy et al., 2020) reindex weavings (Abbott, 2023), which changes the amount of data
accessed per group. We can use accumulators to congregate data processed on different cores, which is
required to parallelize streamable algorithms weaved over a small axis.
Furthermore, diagrams conform to a category-theoretic description, which is not covered in this paper.
However, a categorical perspective would allow back propagation (Fong et al., 2019; Cruttwell et al., 2021),
compilation (Wilson, 2023), and error propagation (Perrone, 2022) to be understood. Formalizing the cate-
gorical aspects of this work would integrate it into existing research and provide a systematic framework for
expressing, optimizing, back-propagating, and compiling deep learning models.
13Under review as submission to TMLR
The performance model we provide considers both the characteristics of algorithms and the hardware they
run on. This performance model can incorporate increasing fidelity, all the way down to the clock cycles per
operation. This invites a systematic analysis of hardware design that relates requirements (transistor count,
energy usage, chip area, production reliability) to functionalities (compute, available memory, bandwidth),
which can be systematically conducted using categorical co-design (Zardini, 2023). This would allow us to
use a shared mathematical framework for describing and optimizing deep learning algorithms and designing
the hardware they run on.
References
Vincent Abbott. Robust diagrams for deep learning architectures: Applications and theory, 2023. URL
https://vtabbott.io/honours-thesis.
Vincent Abbott. Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and
Analysis of Deep Learning Architectures, 2023. URL https://openreview.net/forum?id=RyZB4qXEgt.
Alok Aggarwal and Jeffrey Vitter, S. The input/output complexity of sorting and related problems. 31
(9):1116–1127, 1988. ISSN 0001-0782. doi: 10.1145/48529.48535. URL https://dl.acm.org/doi/10.1145/
48529.48535.
Joshua Ainslie, James Lee-Thorp, Michiel de Jong, et al. GQA: Training generalized multi-query transformer
models from multi-head checkpoints, 2023. URL http://arxiv.org/abs/2305.13245.
Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020. URL
http://arxiv.org/abs/2004.05150. version: 2.
G. S. H. Cruttwell, Bruno Gavranović, Neil Ghani, et al. Categorical foundations of gradient-based learning,
2021. URL http://arxiv.org/abs/2103.01931.
Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning, 2023. URL
http://arxiv.org/abs/2307.08691.
Tri Dao, Daniel Y. Fu, Stefano Ermon, et al. FlashAttention: Fast and memory-efficient exact attention
with IO-awareness, 2022. URL http://arxiv.org/abs/2205.14135.
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, et al. The llama 3 herd of models, 2024. URL
http://arxiv.org/abs/2407.21783.
Patrick Esser, Sumith Kulal, Andreas Blattmann, et al. Scaling rectified flow transformers for high-resolution
image synthesis, 2024. URL http://arxiv.org/abs/2403.03206.
Brendan Fong, David I. Spivak, and Rémy Tuyéras. Backprop as functor: A compositional perspective on
supervised learning, 2019. URL http://arxiv.org/abs/1711.10455.
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training quanti-
zation for generative pre-trained transformers, 2023. URL http://arxiv.org/abs/2210.17323.
Amir Gholami, Sehoon Kim, Zhen Dong, et al. A survey of quantization methods for efficient neural network
inference, 2021. URL http://arxiv.org/abs/2103.13630.
Amir Gholami, Zhewei Yao, Sehoon Kim, et al. AI and memory wall, 2024. URL http://arxiv.org/abs/2403.
14123.
Saugata Ghose, Abdullah Giray Yağlıkçı, Raghav Gupta, et al. What your DRAM power models are not
telling you: Lessons from a detailed experimental study, 2018. URL http://arxiv.org/abs/1807.05102.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, et al. (eds.), Advances in Neural Information Pro-
cessing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.
14Under review as submission to TMLR
AndreiIvanov, NikoliDryden, TalBen-Nun, etal. Datamovementisallyouneed: Acasestudyonoptimizing
transformers, 2021. URL http://arxiv.org/abs/2007.00072.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, et al. Mixtral of experts, 2024. URL http://arxiv.
org/abs/2401.04088.
Mingzhen Li, Yi Liu, Xiaoyan Liu, et al. The deep learning compiler: A comprehensive survey, 2020. URL
http://arxiv.org/abs/2002.03794.
Weile Luo, Ruibo Fan, Zeyu Li, et al. Benchmarking and dissecting the NVIDIA Hopper GPU architecture,
2024. URL http://arxiv.org/abs/2402.13499. version: 1.
NVIDIA. NVIDIA a100 tensor core GPU architecture overview, 2020. URL https://docs.nvidia.com/cuda/
pdf/ptx_isa_8.5.pdf.
NVIDIA. NVIDIA h100 tensor core GPU architecture overview, 2022. URL https://resources.nvidia.com/
en-us-tensor-core.
NVIDIA. PTX ISA 8.5, 2024. URL https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf.
NVIDIA. CUDA C++ Programming Guide, 2024. URL https://docs.nvidia.com/cuda/pdf/CUDA_C_
Programming_Guide.pdf.
Hiroyuki Ootomo and Rio Yokota. Reducing shared memory footprint to leverage high throughput on tensor
cores and its flexible API extension library, 2023. URL http://arxiv.org/abs/2308.15152.
Adam Paszke, Sam Gross, Francisco Massa, et al. PyTorch: An imperative style, high-performance deep
learning library, 2019. URL http://arxiv.org/abs/1912.01703.
Paolo Perrone. Markov categories and entropy, 2022. URL http://arxiv.org/abs/2212.11719.
Dustin Podell, Zion English, Kyle Lacey, et al. SDXL: Improving latent diffusion models for high-resolution
image synthesis, 2023. URL http://arxiv.org/abs/2307.01952.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, et al. High-resolution image synthesis with latent
diffusion models, 2022. URL http://arxiv.org/abs/2112.10752.
Amit Sabne. Xla: Compiling machine learning for peak performance. 2020. URL http://research.google/
pubs/xla-compiling-machine-learning-for-peak-performance/.
Jay Shah, Ganesh Bikshandi, Ying Zhang, et al. FlashAttention-3: Fast and accurate attention with asyn-
chrony and low-precision, 2024. URL http://arxiv.org/abs/2407.08608.
Noam Shazeer. Fast transformer decoding: One write-head is all you need, 2019. URL http://arxiv.org/
abs/1911.02150.
Philippe Tillet, H. T. Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural
network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine
Learning and Programming Languages , MAPL 2019, pp. 10–19. Association for Computing Machinery,
2019. ISBN 978-1-4503-6719-6. doi: 10.1145/3315508.3329973. URL https://dl.acm.org/doi/10.1145/
3315508.3329973.
Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. Attention is all you need. In Isabelle Guyon,
Ulrike von Luxburg, Samy Bengio, et al. (eds.), Advances in Neural Information Processing Sys-
tems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, USA , pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
Paul William Wilson. Category-theoretic data structures and algorithms for learning polynomial circuits,
2023. URL https://eprints.soton.ac.uk/483757/.
Gioele Zardini. Co-design of complex systems: From autonomy to future mobility systems, 2023. URL
https://www.research-collection.ethz.ch/handle/20.500.11850/648075. Accepted: 2023-12-19T10:03:57Z.
15Under review as submission to TMLR
A Appendix
A.1 Fusion Theorem
Theorem 1 (Fusion Theorem) Composition and weaving of a streamable algorithm which does not re-
move the streamed axis yields a streamable algorithm.
Streamable algorithms satisfies the form in Figure 28, allowing the recursive expansion of Figure 29. Stream-
ability depends on polymorphism over the aaxis, meaning the function/algorithm is defined for the axis
being of any size, and the existence of an accumulator B, which allows the streamed input data to be split.
Figure 28: A streamable algorithm re-
quires an accumulator Bwhich allows
the polymorphic streamable axis to be
split. We can fuse the algorithm with a
head and tail, which do not require ad-
ditional loads and saves if their memory
usage is sufficiently small.
This allows for recursive decomposition, reducing the size of the remainder a′axis toa′≤saas in Figure 29.
Here, we add a head and a tail, which are not expanded but can be executed without an additional transfer
and are hence fused. This requires their memory usage to be sufficiently small to not exceed hardware
limitations. Composition on GorKsimply replaces those algorithms with the composed form, yielding a
modified head/tail for the streamable algorithm.
Figure 29: If Figure 28 is satisfied, then the algorithm can be recursively decomposed. This reduces the size
of chunks until the on-chip memory is sufficiently small.
Composing by Eon theb-axis with an algorithm weaved by the streamable saaxis, we can exploit a partition
copy (see Figure 10) to show that the composed F′has an accumulator B′.
16Under review as submission to TMLR
Finally, we are required to show that weaving preserves streamability. This exploits a characteristic of map-
ping composed functions. Mapping a composed function over an additional axis is equivalent to composing
the individual functions mapped over that axis. Therefore, we can show that a weaved Bis an accumulator
for a weaved Fas in Figure 30.
Figure 30: Weaving over Fcomposed with EandGat its inputs is equivalent to weaving over the smaller
FandBalgorithms which, when composed, give F. This weaves need to trace over the inputs which they
target.
We can combine all the above expressions into the single form of Figure 31, where we also apply group
partitioning. This separates the mapped axis into groups of size gqdistributed across processors. We can
iteratively apply these rules; an algorithm can be composed with an algorithm Eweaved over the streamable
axis. This generates a streamable algorithm, which can be weaved over one of the inputs introduced by E.
This is used to construct streamable (flash) attention from a SoftMax-Contraction kernel.
Figure 31: We can combine the streaming theorems into a single expression. Given that the algorithm Fis
streamable along the a/saaxis, we can add modifications to generate a new streamable algorithm. We can
group partition along the streamed axes, mapping groups of size gq×grto different processors.
17Under review as submission to TMLR
A.2 Multi-Level Performance Models
We define for a two-level model diagram the optimal transfers H∗(⃗ a,M ), where⃗ aare the axis sizes and M
is the maximum memory available at the lower level. We use ⃗ gto indicate some configuration of group sizes.
We are interested in memory usage in the limit of large Mand⃗ g, in which case the limiting factor is some
yΠigiweaved by all grouped axes, with ytypically being the output. For smaller M, we aim for a specific
configuration as in Section 5. Two-level diagrams derived optimal transfers solve for Equation 4, where i
iterates over the grouped axes:
H∗(⃗ a,M ) = min
⃗ gΠiai
ΠigiHg(⃗ a,⃗ g)givenM⩾yΠigi (4)
We extend this to multiple levels be assigning a memory Mℓto each level and a weighted transfer cost ˙H−1
ℓ,
which represents bandwidth. As described in Section 4, this conforms to;
H∗=/summationdisplay
ℓ˙H−1
ℓH∗(⃗ a,Mℓ) (5)
How additional levels are used, either as intermediate caches or cross-transfer levels, use the same general
performancemodelofEquation5butwithalteredweightedtransfercosts ˙H−1
ℓandper-leveleffectivememory
Mℓ. UsingMmax
ℓto indicate the maximum memory per level, Nmax
ℓto indicate the maximum number of
child nodes per higher level node, hfor the immediately higher level and cfor the immediately lower, and
˙H−1
ℓ→ℓ′as the raw weighted transfer cost between ℓandℓ′, we adapt our performance model to get;
˙H−1
ℓ=/braceleftigg˙H−1
h→c−˙H−1
ℓ→cℓis a cross-transfer level
˙H−1
h→ℓelse
Mℓ=/braceleftigg
Nmax
cMcℓis a cross-transfer or intermediate cache
Mmax
ℓ else
Therefore, using cross-transfer or caching levels conforms to our standard performance models. This lets #
act as a universal performance model for multi-level GPUs, and ensures that ˙H−1
ℓM−β
ℓis the characteristic
property for comparing various GPU architectures.
A.2.1 Intermediate Caching
Theorem 2 An intermediate caching level ℓ1for an output restricted algorithm with a number restric-
tionNmax
ℓ2⩾Ng,ℓ2/Ng,ℓ1conforms to the standard performance model with Mℓ1=Mℓ2Nmax
ℓ2, requiring
H∗(⃗ a,Mℓ2Nmax
ℓ2)total transfers.
For an intermediate caching model which is output limited, we have the standard constraint for the lower
level,Mℓ2⩾yΠigℓ2,i, and the number restriction, Nmax
ℓ2⩾Ng,ℓ2/Ng,ℓ1= Πigℓ1,i/Πigℓ2,i. We aim to find
the configuration of ⃗ gℓ1and⃗ gℓ2which minimizes the number of transferred values. Assuming the algorithm
is output limited, the effective size of the caching column is yΠigℓ1
i. This lets us express the restrictions as:
H∗
ℓ1= min
⃗ gΠiai
Πigℓ1,iHg(⃗ a,⃗ gℓ1) givenNmax
ℓ2⩾Πigℓ1,i/Πigℓ2,i (6)
H∗
ℓ2= min
⃗ gΠiai
Πigℓ2,iHg(⃗ a,⃗ gℓ2) givenMℓ2⩾yΠigℓ2,i (7)
We can multiply the restriction of (6) by the restriction of (7) to get (8). Assuming that the inequalities are
sufficiently close to equalities, we outline the new problem to solve:
18Under review as submission to TMLR
H∗
ℓ1= min
⃗ gΠiai
Πigℓ1,iHg(⃗ a,⃗ gℓ1) givenMℓ2Nmax
ℓ2⩾yΠigℓ1.i (8)
H∗
ℓ2= min
⃗ gΠiai
Πigℓ2,iHg(⃗ a,⃗ gℓ2) givenMℓ2⩾yΠigℓ2
i (9)
These restrictions correspond to Equation 4, so we can substitute in H∗(⃗ a,Mℓ)for both levels, but using
Mℓ2Nmax
ℓ2for the intermediate level instead of its own maximum memory, Mmax
ℓ1. We therefore have;
H∗
ℓ1=H∗(⃗ a,Mℓ2Nmax
ℓ2)
H∗
ℓ2=H∗(⃗ a,Mℓ2)
A caching level therefore conforms to our standard multi-level performance model derived from a two-level
diagram, but with the intermediate level using total lower level memory instead of its own.
A.2.2 Cross-Transfer Level
Theorem 3 Introducing a cross-transfer level xbetween a higher level hand a lower level callows us to
replace the weighted transfer cost of an output-limited algorithm;
H∗=...+˙H−1
h→cH∗(⃗ a,Mc) +...
With,
H∗=...+ ( ˙H−1
h→c−˙H−1
x→c)H∗(⃗ a,Nmax
cMc) +˙H−1
x→cH∗(⃗ a,Mc) +...
Where ˙H−1
h→cis the weighted transfer cost of sending data to children, and ˙H−1
x→cis the weighted transfer cost
of sending data between children.
The child level crequires a total of H∗(⃗ a,Mc)data transfers from the higher level, typically incurring a
weighted transfer cost per value of ˙H−1
h→c. With a cross-transfer level xbetweenhandc, we can send some
dataH∗
xto any of the children and cross-transfer the remaining at a weighted transfer cost of ˙H−1
x→cper
value. We need to derive the optimal configuration of ⃗ gxto minimize H∗
x. This configuration incurs a number
restriction, as the child processors need to remain active. We therefore have,
H∗
x= min
⃗ gΠiai
Πigx,iHg(⃗ a,⃗ gx) givenNmax
c⩾Πigx,i/Πigc,i
H∗
c= min
⃗ gΠiai
Πigc,iHg(⃗ a,⃗ gc) givenMc⩾yΠigc,i
We perform a similar substitution to Section A.2.1, yielding a new set of restrictions;
H∗
x= min
⃗ gΠiai
Πigx,iHg(⃗ a,⃗ gx) givenMcNmax
c⩾yΠigx,i
H∗
c= min
⃗ gΠiai
Πigc,iHg(⃗ a,⃗ gc) givenMc⩾yΠigc,i
These restrictions conform to the two-level model optimal, with required transfers being H∗
x=
H∗(⃗ a,McNmax
c)andH∗
c=H∗(⃗ a,Mc). When considering transfers for the total weighted transfer cost
calculation, we can subtract the required transfers to xfrom the transfers required to c. This is because
data is transferred from the higher level to the cross-transfer level by sending it to children, so much of the
data is already available. This results in the substituting the total weighted transfer costs (10) with (11):
H∗=...+˙H−1
h→cH∗(⃗ a,Mc) +... (10)
∝⇕⊣√∫⊔≀→H∗=...+˙H−1
h→cH∗(⃗ a,Nmax
cMc) +˙H−1
x→c(H∗(⃗ a,Mc)−H∗(⃗ a,Nmax
cMc)) +... (11)
19Under review as submission to TMLR
We can rearrange (11) so that we have the standard format of each level having H∗(⃗ a,Mℓ)transfers by
instead modifying the transfer cost:
H∗=...+/parenleftbig˙H−1
h→c−˙H−1
x→c/parenrightbig
H∗(⃗ a,Nmax
cMc) +˙H−1
x→cH∗(⃗ a,Mc) +... (12)
Therefore, a cross-transfer level conforms to the standard performance model. Instead of using the weighted
transfer cost of sending data to the children ˙H−1
h→cfor the cross-transfer level, we set ˙H−1
x=˙H−1
h→c−˙H−1
x→c
and we remap ˙Hc=˙H−1
x→c. This lets us express (12) as the expression below, which conforms to the standard
multi-level performance model of (5):
H∗=...+˙H−1
xH∗(⃗ a,Nmax
cMc) +˙H−1
cH∗(⃗ a,Mc) +...
A.2.3 Additional Notes
In the case of a multi-GPU hierarchy, the interconnected topology is a cross-transfer level xwhich distributes
data among child GPUs cat a weighted transfer cost of ˙H−1
x→c. If we assume data is already distributed
across GPUs, then the number of GPU cross-transfers is H∗(⃗ a,Mmax
c)−H∗(⃗ a,Nmax
cMmax
c). So far, we
have considered the highest level to have unlimited memory and zero weighted transfer cost. We can model
multi-GPU systems by the highest level (the multi-GPU level x) as having a memory equal to Nmax
cMmax
c
and negative weighted transfer cost −˙H−1
x→c, which assumes data is already distributed among GPUs. This
provides a rough model, which can be refined by aligning the group sizes used in diagrams.
Often, we can configure the number of cross-transfer level children to alter the cross-transfer bandwidth. In
(Luo et al., 2024), it was found that the bandwidth of H800 ( Chinese market Hopper GPUs ) SM-SM transfers
varies from 3.27TB/swith a cluster size N= 2to2.65TB/swith a cluster size of N= 4, compared to
2.04TB/sof GMEM-SMEM bandwidth. This imposes a trade-off; smaller cluster sizes improve effective
˙Hcbut reduce the cross-transfer discount H∗(⃗ a,Nmax
cMmax
c). (Luo et al., 2024) notes that balancing this
trade-off is an important direction for exploration. We can use our model to find the difference in weighted
transfer costs with (11) and without (10) a cross-transfer level, providing an equation to optimize for N.
∆H∗=/parenleftbig
...+˙H−1
h→cH∗(⃗ a,Mc) +.../parenrightbig
−/parenleftbig
...+/parenleftbig˙H−1
h→c−˙H−1
x→c/parenrightbig
H∗(⃗ a,N Mc) +˙H−1
x→cH∗(⃗ a,Mc) +.../parenrightbig
=/parenleftbig˙H−1
h→c−˙H−1
x→c/parenrightbig
H∗(⃗ a,Mc)−/parenleftbig˙H−1
h→c−˙H−1
x→c/parenrightbig
H∗(⃗ a,N Mc)
=/parenleftbig˙H−1
h→c−˙H−1
x→c/parenrightbig
(H∗(⃗ a,Mc)−H∗(⃗ a,N Mc))
∆H∗= ∆ ˙H−1(N)/summationdisplay
tαt(⃗ a)M−β
c/parenleftbig
1−N−β/parenrightbig
A.3 Streamability
A.3.1 Contraction
The streamability of contraction (a dot product) requires that an accumulator exists of the form in Figure
28. Contraction for vectors v,w∈Rais given by v·w= Σn−1
i=0vi·wi, which can be expressed as v·w=
Σs′−1
i=0vi·wi+ Σa−1
i=s′vi·wi, where the underlined portion is the accumulator. We diagrammatically show
this in Figure 32, highlighting the accumulator in blue.
Figure 32: We can re-express contraction as an initial
function followed by the accumulator. Any size can be
chosen forsands′, and the expression can be recursively
expanded until s′≤safor some target stream batch size
sa.
20Under review as submission to TMLR
A.4 Fusion of SoftMax-Contraction
SoftMax-Contraction is streamable by maintaining a running maximum and sum on chip as auxiliary vari-
ables. We express this by streaming unscaled SoftMax-Contraction with the initialization of the auxiliary
variables as the head and scaling by the sum as a tail as in Figure 33.
Figure 33: Streamable SoftMax-
Contraction is implemented by accumu-
lating the results of unscaled SoftMax-
Contraction applied to segments, adjusting
the baseline relative to the current maxi-
mum value. We apply the normalization by
zas a tail for the expression.
We can derive streamable SoftMax-Contraction by taking recursively expanded Auxiliary SoftMax (Figure
34) and contracting its output. Recursively expanded Auxiliary SoftMax is not streamable as the memory
usageincreaseswiththeinputsize. However, itterminatesinajoin, allowingittobefusedwithacontraction.
Figure 34: Auxiliary SoftMax (defined in
Table 1), where we maintain auxiliary vari-
ables, can be recursively expanded.
Base SoftMax Auxiliary SoftMax Unscaled SoftMax
SoftMax (⃗ x) :
µ←max(βxi)
si←exp(βxi−µ)
z←Σisi
yi←si/z
Return⃗ yInitialize () :
Return (−∞,0,0)
SoftMax 0(⃗ x,(µ′,δz′,z′)) :
µ←max(βxi,µ′)
si←exp(βxi−µ)
δ←exp(µ′−µ)
z←δz′+ Σisi
yi←si/z
Return⃗ y,(µ,δz′,z)
Scaling (on prior values),
⃗ y′∗=⃗ y′δz′/zInitialize () :
Return (−∞,0,0)
SoftMax 1(⃗ s,(µ′,δ′,z′)) :
µ←max(βxi,µ′)
si←exp(βxi−µ)
δ←exp(µ′−µ)
z←δ∗z′+ Σisi
Return⃗ s,(µ,δ,z )
Scaling (on prior values),
⃗ y′∗=⃗ y′δ
Scaling (at tail),
⃗ y∗=⃗ y/z
Table1: WeprovidediagramsandcodeforvariousformsofSoftMax. βistheinversetempuratureparameter,
and is set to d−0.5.
FusingSoftMaxwithacontractionlimitstheoutputsize. Thejointailallowsittobefusedwithacontraction.
This limits the size of the output, yielding a streamable algorithm. To streamline the derivation, we do not
explicitly draw the updated maintained variables as in Figure 34. We can then apply rearrangements to
recover a streamable form of the composed function.
21Under review as submission to TMLR
We can then replace SoftMax with unscaled SoftMax, which is done in FlashAttention-2. This lets us move
the shared “ /z” factor to the end of the expression, producing the numerically stable form we use. We forego
drawing the auxiliary variables lines to streamline the derivation but add them later.
This analysis derives the streamability of SoftMax and its fusion with contraction as a standard procedure
using diagrams. Diagrams, by showing the structure of constituent algorithms, act as algebraic tools for
deriving fusion.
22Under review as submission to TMLR
A.5 Configuration Tables
A.5.1 Justification for Register Row-Wise Multiplication-Addition
For our algorithms, we opt for scaling on registers instead of using tensor core diagonal matrices as FlashAt-
tention does. This makes algorithms extremely memory efficient, as no scale_o(Shah et al., 2024, p.21)
variable needs to be stored. We can accumulate in groups along d, leading to a small memory usage. How-
ever, these algorithms may suffer in performance. Even though we avoid the wasteful computations of a
diagonal matrix, FP16 operations (312/989 TFLOPs at FP16 for A100/H100 (NVIDIA, 2022)) are much
slower than tensor cores (78/134), and we require a multiply-add for each sub-loop. However, the com-
piler mightdecide to overlap tensor core operations on some warps with FP16 operations on others. This
overlapping would be revealed by implementing the algorithm and assessing whether tensor core and FP16
multiply-add operations are both fully utilized.
A.5.2 Ampere Attention
To find an exact configuration for our version of Ampere Attention from Figure 26, we first label all the
required variables and their sizes in Figure 35. Variables must be pre-allocated with a maximum size, and
can encompass one array per column. This generates a configuration table (Table 2), which allows us to
choose batch sizes. We configure the batch sizes in Table 2 to derive an implementation of the algorithm
which conforms to hardware constraints. The variable dis typically configured to be 128, and it is reasonable
to set the batch sizes to their smallest possible size. This results in wq= 32,sx= 16,tq= 1,d′= 16. Once
we have found the total register and SMEM memory usage per warp, we can find the maximum number of
warps per thread block (virtual SMEM). Each thread block can contain up to 256KB of register memory,
163KB/227KB of SMEM (A100/H100) (NVIDIA, 2024, p. 492), and 32 warps. Each thread is limited to
255 registers, containing 4B each (1KB total). We can only share SMEM data between virtual cores, so the
number of warps determines our “discount” on distributed key/value loads.
Figure 35: FlashAttention with required memory-allocated variables labeled. All arrays must be contained
in a variable at its level in which it fits, and a variable cannot be used twice in a column.
After performing the substitution of batch sizes in Table 2, we find the register and SMEM bytes per warp
in Table 3. Using our strategy, we can theoretically fit up to 13warps per thread block. We can see that
SMEM is not an issue, nor is the limit of 1KBper thread (32KB/warp). This is significantly larger than
the 4-8 warps per thread block given in the FlashAttention-2 paper, implying improved performance. The
reasons for this are difficult to tell, as diagrams give much more detail about variable sizes than traditional
pseudocode. A significant factor is likely to be avoiding scale_oand instead relying on register level row-
wise multiplication. We also benefit from a small stream size and sub-loops which reuse variables. How
this alternative strategy plays out remains to be tested, and may fail because of factors such as the small
sub-stream size introducing excessive error.
23Under review as submission to TMLR
Variable Size Location
AQueries w(32)
q×d(64)Registers
BKeys/Values Stream s(16)
x×d(64)SMEM
CTransposed Keys d(64)×s(16)
xSMEM
DAuxiliary tq×3 Registers
EOutput tq×d Registers
FTensor Core Primary 32×8 Registers
GTensor Core Secondary 8×16 Registers
HTensor Core Accumulator 32×16 Registers
ITransfer Cache g(32)
q×s(16)
xSMEM
JProcessed Values tq×u(8)
xRegisters
KSubloop Cache g(32)
q×d′(16)SMEM
Table 2: The configuration table for A100 attention lists all the required variables, their size, and location.
Variable Constant per
Thread BlockValues / Warp Location
AQueries 4096 Registers
BKeys/Values Stream 1024 SMEM
CTransposed Keys 1024 SMEM
DAuxiliary 96 Registers
EOutput 4096 Registers
FTensor Core Primary 256 Registers
GTensor Core Secondary 128 Registers
HTensor Core Accumulator 512 Registers
ITransfer Cache 512 SMEM
JProcessed Values 256 Registers
KSubloop Cache 512 SMEM
Total 2048= 4096B 1024= 2048B SMEM
9440= 18,880B Register
Maximum Bytes
per Thread BlockMaximum Num-
ber of Warps
SMEM 163KB 79.5 ( max 32)
Register 256KB 13.89 ( max 32)
Table 3: We substitute in the minimum configuration for batch sizes, deriving the bytes of SMEM and
register memory required per warp.
A.5.3 Hopper Attention
For our Hopper Attention, we again employ register register FP16 scaling operations. We will calculate the
amount of registers and SMEM required per warp group of 128 threads, and must account for the different
quantizations of data present. We identify the variables in Figure 36 and use the configuration Table 4 to
find the maximum number of warpgroups per thread block. Furthermore, we can assess the clock cycle of
each column to derive a strategy for asynchronous execution.
24Under review as submission to TMLR
Figure 36: We perform a similar process to Figure 35 of identifying all required memory-allocated variables.
Variable Size Q. Level Per T.B. Per W.G.
AQueries w(128)
q×d(128)FP8SMEM 16384
BKeys Stream s(32)
x×d(128)FP8SMEM 4096
CTransposed Keys d(128)×s(32)
xFP8SMEM 4096
FValues Stream s(16)
x×d(128)FP16SMEM 2048
DAuxiliary tq×3 FP16Registers 384
EOutput tq×d(128)FP16Registers 16384
GFP8 Tensor Core Secondary 128×32 FP8SMEM 4096
HFP16 Tensor Core Accumulator 64×32 FP16Registers* 2048
ITransfer Cache g(64)
q×s(32)
xFP16SMEM 4096
JProcessed Values tq×u(8)
xFP16Registers 1024
KFP16 Tensor Core Primary 64×8 FP16SMEM 512
LFP16 Tensor Core Secondary 8×16 FP16SMEM 128
MFP16 Tensor Core Accumulator 64×16 FP16Register* 1024
NSub-loop Transfer Cache g(64)
q×d′(16)FP16SMEM 2048
Total (KB=1024B) SMEM 12 33.25
Total (KB) Register 40.75( max
128)
Maximum Bytes
per Thread BlockMaximum Num-
ber of Warpgroups
SMEM 228KB 6.50 ( max 8)
Register 256KB 6.28 ( max 8)
Table 4: We list the required variables and substitute their size. The total bytes needs to consider the
quantization employed. * Warpgroup tensor core operations require the accumulator to be in register memory.
Referring to Table 4, we can run 6consumer warpgroups per thread block. Collectively, this represents
24producer warps per threadblock, double the amount for Ampere algorithm. From our model, we can
understand why. Storing tensor core operations on SM effectively doubles the lower-level memory available,
doubling the amount of processors which share a stream of keys and values. If we had simply assumed that
lower-level memory Mℓis doubled without any further information, we would have estimated 26warps which
is6.5warp groups for the Hopper algorithm.
Note how in Figure 36 columns alternate between blue (tensor core) and green (register) operations. This
indicates we can overlap computation. Each SM has cores dedicated to different operations (NVIDIA, 2022,
p.21). These can run simultaneously, and Hopper supports user-configured asynchronous execution between
25Under review as submission to TMLR
warpgroups. We aim to overlap tensor core, general purpose FP16 operations, and special functions (the
exponential). Furthermore, we can dedicate the remaining 2warpgroups per thread block to be producers ,
asynchronously providing data from higher levels, which uses minimal registers.
We calculate the clock cycles for each column per thread, given in Table 5. We use the sub-loop sizes, as
sub-loops can be exploited to overlap computation. Clock cycles are found by dividing TFLOPs by the
number of SMs (132) and clock speed (1830MHz) (NVIDIA, 2022, p.39), or from the CUDA programming
guide (NVIDIA, 2024).
FP8 MatMul SoftMax FP16 Mat-
MulFP16
Multiply-
Add
Operation Tensor Core
(FP8)Special Func-
tion Unit
(exponent)Tensor Core
(FP16)Non-Tensor
FP16
Ops / Thread 2dsx= 8192 ux= 8 2uxd′= 256 2d′= 32
Ops / SM Cycle 8192 16 4096 512
SM Cycles / Thread 1 0.5 1/16 1/16
Number per ux n/a 1 d/d′= 8 d/d′= 8
SM Cycles / Thread
peruxn/a 0.5 0.5 0.5
Table 5: Ignoring the weaving over thread groups, we can find the operations for each column per thread
from the diagram, Figure 36.
Eachsxstream requires an FP8 tensor-core operation followed by four SoftMax and FP16 stages. From
Table 5, we see that each uxsub-stream requires 0.5 clock cycles for SoftMax, FP16 tensor core, and thread-
level scaling. Within warpgroups, we have two independent tiles of size 64 as indicated by g(64)
x. We split
each warpgroup into two 64 thread groups and alternate FP16 tensor core operations on one with non-tensor
FP16 operations on the other. Furthermore, we overlap SoftMax with FP16 operations on other warpgroups.
In Figure 37, we represent overlapping between warpgroups.
Figure 37: We can overlap the computation of different stages. Thick lines indicate wait blocks, syncing
the processors.
In Figure 37, each column takes 0.5 clock cycles per active thread. By having two pipelines, half the
warpgroups are active, and therefore each column requires 192 clock cycles. Overall, the algorithm takes
2304 clock cycles. At 1830MHz, this takes 1259ns. Across the SM, 2×8192gq= 1.26e7floating point
operations are performed, giving 9.99TFLOPs per core, or 1.32PFLOPs on the 132 SMs of an H100
SXM5. Each SM loads 3sxd= 12KiBof data. To not be bandwidth bottlenecked, this requires 9.76 GB/s
per SM, or 1.29 TB/s per GPU, which SXM5s can easily achieve. The 1.32PFLOPs figure improves on
FlashAttention-3’s 1.2PFLOPs, and is equal to the maximum throughput of tensor cores where compute is
evenly distributed between FP8 and FP16 values.
However, the figure given in Table 5 is an underestimate of the clock cycles required for SoftMax. We require
an additional exponential and various conversions between FP32 and FP16 for SoftMax, requiring more than
26Under review as submission to TMLR
0.5 cycles per thread. We achieve full throughput if the tensor cores are continuously active, which is the
case if tensor core operations bottleneck waits. We can use 64×64by64×32FP8 tensor core operations,
splitting the computation into four steps. After each uxFP16 stage, we precompute a quarter of the next
Ivariable. This means each sxstream begins with the Iinput computed. This requires an additional I
variable in SMEM, which does not reduce the number of warpgroups.
Figure 38: We can split the FP8 tensor core operation into four stages, which can shadow the execution of
the slow SoftMax operation.
27