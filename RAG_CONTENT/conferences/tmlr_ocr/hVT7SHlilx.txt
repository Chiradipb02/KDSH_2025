Published in Transactions on Machine Learning Research (1/2023)
Named Tensor Notation
David Chiang
University of Notre Dame
Alexander M. Rush
Cornell University
Boaz Barak
Harvard University
Reviewed on OpenReview: https://openreview.net/forum?id=hVT7SHlilx
Abstract
We propose a notation for tensors with named axes, which relieves the author, reader, and
future implementers of machine learning models from the burden of keeping track of the
order of axes and the purpose of each. The notation makes it easy to lift operations on
low-order tensors to higher order ones, for example, from images to minibatches of images,
or from an attention mechanism to multiple attention heads.
After a brief overview and formal definition of the notation, we illustrate it through several
examplesfrommodernmachinelearning, frombuildingblockslikeattentionandconvolution
to full models like Transformers and LeNet. We then discuss differential calculus in our
notation and compare with some alternative notations. Our proposals build on ideas from
manypreviouspapersandsoftwarelibraries. Wehopethatournotationwillencouragemore
authors to use named tensors, resulting in clearer papers and more precise implementations.
1 Introduction
Formal descriptions of neural networks primarily adopt the notation of vectors and matrices from applied
linear algebra (Goodfellow et al., 2016). When used to describe vector spaces, this notation is both concise
and unambiguous. However, when applied to neural networks, these properties are lost. Consider the
equation for attention as notated in the Transformer paper (Vaswani et al., 2017):
Attention (Q,K,V ) =/parenleftbigg
softmaxQK⊤
√dk/parenrightbigg
V.
The equation relates Q,K, andV(for query, key, and value, respectively) as sequences of feature vectors,
packed into possibly identically-sized matrices. While concise, this equation is ambiguous. Does the product
QK⊤sum over the sequence, or over the features? We know that it sums over columns, but there is not
enough information to know what the columns represent. Is the softmax taken over the query sequence or the
key sequence? The usual notation does not offer an answer. Perniciously, the implementation of an incorrect
interpretation might still run without errors. With the addition of more axes, like multiple attention heads
or multiple sentences in a minibatch, the notation becomes even more cumbersome.
We propose an alternative mathematical notation for tensors with named axes .1The notation has a formal
underpinning, but is hopefully intuitive enough that machine learning researchers can understand it without
much effort. In named tensor notation, the above equation becomes
Attention :Rkey×Rseq×key×Rseq×val→Rval
1We follow NumPy in using the term axis. Other possible terms would be index ,dimension ,way, or mode (Tucker, 1964),
but we felt that axishad the least potential for confusion.
1Published in Transactions on Machine Learning Research (1/2023)
Attention (Q,K,V ) =
softmax
seqQ⊙
keyK
/radicalbig
|key|
⊙
seqV.
The type signature introduces three named axes: the keyaxis is for features of queries and keys, the val
axis is for features of values, and the seqaxis is for tokens in a sequence. (Please see Section 2.2 for an
explanation of our naming convention.) This notation makes the types of each input tensor explicit. Tensor
Qis a query vector that is compared with key vectors, so it has a keyaxis. Tensor Kis a sequence of key
vectors, so it has seqand keyaxes. Tensor Vis a sequence of value vectors, so it has seqand valaxes. Unlike
with matrix notation, the reader is not required to remember whether seqcorresponds to rows or columns
in either of these tensors.
The function itself uses the named axes to precisely apply operations. The expression Q⊙
keyKis a dot product
over the keyaxis shared between KandQ; there is no ambiguity about rows or columns. Similarly, the
softmax function is annotated with the axis along which it is applied, removing any ambiguity or reliance
on convention.
Furthermore, named tensor notation naturally extends to lifting(also known as vectorizing and/or broad-
casting) a function to tensors with more axes. For example, if instead of being a tensor with the single axis
key,Qhas three axes key,seqand batch(corresponding to tokens of a sequence and examples in a minibatch,
respectively) then the Attention function works as written, acting on each example in a minibatch in parallel.
Similarly, we can also add a headsaxis to the inputs to get multiple attention heads. These additional axes
are often elided in neural network papers, possibly avoiding notational complexity, but possibly also hiding
critical model details.
Our contributions. This work proposes a mathematical notation for named tensors and a fully specified
semantic interpretation for the notation. Through examples, we demonstrate that this notation enables
specifying machine learning models and operations in a succinct yet precise manner. The need for named
tensors has been recognized by several software packages, including xarray (Hoyer & Hamman, 2017), Nexus
(Chen, 2017), tsalib (Sinha, 2018), axisarrays (Bauman, 2018), NamedTensor (Rush, 2019), PyTorch (Torch
Contributors, 2019), Dex (Paszke et al., 2021), JAX (JAX authors, 2021), einops (Rogozhnikov, 2022), and
torchdim (DeVito, 2023). While our notation is inspired by these efforts, our focus is on mathematical
notation to be used in papers, whereas previous efforts have focused on code. Our hope is that our notation
will be adopted by authors, leading to clearer, more replicable papers, and that this, in turn, will encourage
more implementers to adopt named tensor libraries, leading to clearer, more correct code.
2 Named Tensors
In standard notation, a vector, matrix, or tensor is indexed by an integer or sequence of integers; if it has
dimensions n1,...,nr, it can be thought of as a map that takes as input (i1,...,ir)∈[n1]×···× [nr]and
outputs a real number (or an element of a different field). For example, if A∈R3×3, then the order of the
two axes matters: A1,3andA3,1are not the same element. It is up to the reader to remember what each
axis of each tensor stands for. This problem is exacerbated in modern machine learning, where tensors have
multiple axes with different meanings (batches, channels, etc.), and different operations act on different axes.
In contrast, we propose named tensors , in which each axis has a namethat describes it and ensures there
is no confusion between axes. We write ax[n]for an axis with name axand sizen, and we write ax(i)to
index thei-th element along axis ax. So if a tensor has axes ax1[n1],..., axr[nr](with ax1,..., axrbeing
distinct names), it can be thought of as a map that takes as input a record{ax1(i1),..., axr(ir)}, with
i1∈[n1],...,ir∈[nr], and outputs a field element.
In summary the key difference is that, while a tensor in standard notation takes as input an ordered tuple
of indices, a named tensor takes as input a record, which is an unordered set of named indices. We illustrate
with some examples below, then give formal definitions.
2Published in Transactions on Machine Learning Research (1/2023)
2.1 By example
For example, if Arepresents a 3×3grayscale image, we can make it a named tensor like so (writing it two
equivalent ways to show that the order of axes does not matter):
A∈Rheight [3]×width [3]=Rwidth [3]×height [3]
A=heightwidth
3 1 4
1 5 9
2 6 5
=widthheight
3 1 2
1 5 6
4 9 5
. (1)
We access elements of Ausing named indices, whose order again does not matter: Aheight (1),width (3)=
Awidth (3),height (1)= 4. We also allow partial indexing:
Aheight (1)=width/bracketleftbig3 1 4/bracketrightbig
Awidth (3)=height/bracketleftbig4 9 5/bracketrightbig
.
It does not matter if we write Aheight (1)orAwidth (3)as row and column vectors. In many contexts, an axis
name is used with only one size. If so, we can simply write heightfor the unique axis with name height, as
inRheight×width. We can leave the size of an axis unspecified at first, and specify its size later (e.g., deferring
it to an appendix on experimental details). For example, we can specify |height|=|width|= 28if we want
to prescribe the precise size of an image, or just write |height|=|width|to specify that it’s a square image.
2.2 What’s in a name?
Althoughusersofthisnotationarefreetochooseanynamesforaxes, weofferthefollowingrecommendations.
First, we recommend wordsinstead of single letters, to communicate better the meaning of each axis.
More subtly, we recommend words that describe a wholerather than its parts. For example, to represent
a minibatch of examples, we would name the axis batch; to represent a sequence of tokens, we would name
the axis seq. One reason for this choice is that there are cases, like heightand width, where there is a name
for the whole, but no unambiguous name for the part. By contrast, in cases where there is a name for the
part but not the whole, it’s always possible to use the plural form of the name of the part. For example, if
we wanted Ato have red, green, and blue channels, we would name the axis chans.
Section 4 contains many more examples of axis names.
2.3 Formal definition
We now define formally the notation we use.
Definition 1 (Names, indices, and axes) .Anaxisis a pair, written ax[I], where
•axis thenameof the axis, which is simply a string of letters. We write both names and variables
ranging over names using sans-serif font.
•Iis a set of indices. In this paper, Iis always of the form {1,...,n}for somen, so we abbreviate
ax[{1,...,n}]asax[n].
In many contexts, there is only one axis with name ax, and so we refer to the axis simply as ax. The context
always makes it clear whether axis a name or an axis. If axis an axis, we write ind( ax)for its index set,
and we write|ax|as shorthand for|ind( ax)|.
Definition 2 (Named indices and records) .Ifax[I]is an axis and i∈I, then anamed index is a pair, written
ax(i). Arecordis a set of named indices {ax1(i1),..., axr(ir)}, where ax1,... axrare pairwise distinct names.
3Published in Transactions on Machine Learning Research (1/2023)
Definition 3 (Shapes).Ashapeis a set of axes, written ax1[I1]×···× axr[Ir], where ax1,... axrare pairwise
distinct names. We write ∅for the empty shape. A shape defines a set of records:
rec(ax1[I1]×···× axr[Ir]) ={{ax1(i1),..., axr(ir)}|i1∈I1,...,ir∈Ir}.
We say two shapes SandTarecompatible if whenever ax[I]∈Sand ax[J]∈T, thenI=J. We say that
SandTareorthogonal if there is no axsuch that ax[I]∈Sand ax[J]∈Tfor anyI,J. Ift∈recTand
S⊆T, then we write t|Sfor the unique record in recSsuch thatt|S⊆t.
Definition 4 (Named tensors) .LetFbe a field and let Sbe a shape. Then a named tensor over Fwith
shapeSis a mapping from recStoF. IfXhas shapeSthen we write shpX=S. We write the set of all
named tensors with shape SasFS.
We don’t make any distinction between a scalar (an element of F) and a named tensor with empty shape
(an element of F∅).
IfX∈FS, then we access an element of Xby applying it to a record s∈recS; but we write this using the
usual subscript notation: Xsrather than X(s). To avoid clutter, in place of X{ax1(i1),...,axr(ir)}, we usually
writeXax1(i1),...,axr(xr). When a named tensor is an expression like (X+Y), we index it by surrounding it
with square brackets like this: [X+Y]ax1(i1),...,axr(xr).
We also allow partial indexing. If Xis a tensor with shape Tands∈recSwhereS⊆T, then we define
Xsto be the named tensor with shape T\Ssuch that, for any t∈rec(T\S ),
[Xs]t=Xs∪t.
(For the edge case T=∅, our definitions for indexing and partial indexing coincide: one gives a scalar and
the other gives a tensor with empty shape, but we don’t distinguish between the two.)
3 Operations
A significant benefit of named tensor notation is that it allows one to unambiguously specify operations
that map tensors to tensors, and defines precisely how operations can be liftedwhen an operation is applied
to tensors with more axes than are present in its signature and how broadcasting happens when different
arguments add different axes.
We start with the formal definition of named tensor operations and lifting, then show how this definition
leads to many common operations.
3.1 Formal definition
By(named) tensor function or(named) tensor operation , we mean not only functions from tensors to tensors,
but also operators like negation ( −), addition ( +), and so on. We will extend the standard function/operator
notation by allowing tensor operations to be liftedto higher-order tensors.
Definition 5 (lifting, unary) .Letf:FS→GTbe a function from tensors to tensors. For any shape S′
orthogonal to both SandT, we can define the liftfS′offwith the shapeS′to be the map
fS′:FS∪S′→GT∪S′
/bracketleftig
fS′(X)/bracketrightig
s′=f(Xs′)for allX∈FS∪S′ands′∈recS′.
Usually, we simply write finstead offS′. That is, for every tensor Xwith shapeR⊇S, we letf(X) =
fR\S(X).
Iffis a multary function, we can lift each of its arguments to larger shapes, and we don’t have to add
the same axes to all the arguments; an axis present in one argument but not another is broadcast from the
former to the latter. We consider just the case of two arguments; three or more arguments are analogous.
4Published in Transactions on Machine Learning Research (1/2023)
Definition 6 (lifting, binary) .Letf:FS×GT→HUbe a binary function from tensors to tensors. For
any shapesS′andT′that are compatible with each other and orthogonal to SandT, respectively, and such
thatS′∪T′is orthogonal toU, we can lift fto:
fS′∪T′:FS∪S′×GT∪T′→HU∪S′∪T′
/bracketleftig
fS′∪T′(X,Y )/bracketrightig
s′=f/parenleftbig
Xs′|S′,Ys′|T′/parenrightbig
for allX∈FS∪S′,Y∈FT∪T′,s′∈rec(S′∪T′).
Again, we usually write finstead offS′∪T′.
In the following sections, we present some consequences of the above lifting rules. In particular, we show how
they allow one to lift some common operations from operating on scalars, vectors, or matrices to operating
on tensors with more axes, and how they correspond to vectorizing and broadcasting (as implemented by
NumPy and related packages).
3.2 Elementwise operations and broadcasting
Any function from a scalar to a scalar corresponds to a tensor function with signature F∅→F∅. Hence
lifting it to any tensor shape, by Definition 5, corresponds to elementwise application. For example, given
the logistic sigmoid function,
σ:R→R
σ(x) =1
1 + exp(−x)
we can lift it to tensors. If A∈Rheight [3]×width [3]is the example tensor (1), then
σ(A) = heightwidth
1
1+exp(−3)1
1+exp(−1)1
1+exp(−4)
1
1+exp(−1)1
1+exp(−5)1
1+exp(−9)
1
1+exp(−2)1
1+exp(−6)1
1+exp(−5)
.
Similarly for rectified linear units (relu (x) = max(0,x)), negation, and so on.
Any function with signature R×R→R, including binary operators like addition ( +), can be applied to two
named tensors with the same shape. But if we apply a binary function or operator to tensors with different
shapes, then, by Definition 6, broadcasting applies. For example, let
x∈Rheight [3]y∈Rwidth [3]
x=height
2
7
1
 y=width/bracketleftbig1 4 1/bracketrightbig
.
(We writexas a column just to make the broadcasting easier to visualize.) Then, to evaluate A+x, we
effectively replace xwith a new tensor with a copy of xfor every index of axis width. Likewise for A+y:
A+x=heightwidth
3 + 2 1 + 2 4 + 2
1 + 7 5 + 7 9 + 7
2 + 1 6 + 1 5 + 1
 A+y=heightwidth
3 + 1 1 + 4 4 + 1
1 + 1 5 + 4 9 + 1
2 + 1 6 + 4 5 + 1
.
Similarly for other operations. We write elementwise multiplication (Hadamard product) as ⊙.
5Published in Transactions on Machine Learning Research (1/2023)
3.3 Reductions
Thesamerulesapplytofunctionsfromvectorstoscalars, called reductions . Wespecifywhichaxisareduction
applies to using a subscript (equivalent to the axisargument in NumPy and dimin PyTorch), so that even
after lifting, we know which axis to reduce. For example, we can define summation:
/summationdisplay
ax−:Rax[I]→R
/summationdisplay
axX=/summationdisplay
i∈IXax(i)
and use it on Afrom Eq. (1):
/summationdisplay
heightA=/summationdisplay
iAheight (i)=width/bracketleftbig3 + 1 + 2 1 + 5 + 6 4 + 9 + 5/bracketrightbig
/summationdisplay
widthA=/summationdisplay
jAwidth (j)=height/bracketleftbig3 + 1 + 4 1 + 5 + 9 2 + 6 + 5/bracketrightbig
.
We can also write multiple names to sum over multiple axes:
/summationdisplay
height
widthA=/summationdisplay
i/summationdisplay
jAheight (i),width (j)= 3 + 1 + 4 + 1 + 5 + 9 + 2 + 6 + 5 .
But a summation with an index variable (like iorjabove) is a standard summation over values of that
variable, and a summation with no subscript is a standard summation over a set.
Other examples of reductions include:
norm
axX=/radicaligg/summationdisplay
axX2 norm p
axX=/parenleftbigg/summationdisplay
axXp/parenrightbigg1
p
min
axX= min{Xax(i)|i∈I} max
axX= max{Xax(i)|i∈I}
mean
axX=1
|ax|/summationdisplay
axX var
axX=1
|ax|/summationdisplay
ax(X−mean
axX)2
3.4 Contraction
The vector dot-product is a function from twovectors to a scalar. We write it as follows:
−⊙
ax−:Rax[I]×Rax[I]→R
X⊙
axY=/summationdisplay
i∈IXax(i)Yax(i)
When lifted to higher-order tensors, the dot-product generalizes to the ubiquitous contraction operator,
which can also be thought of as elementwise multiplication followed by summation over one axis, that is,
X⊙
axY=/summationdisplay
axX⊙Y. (2)
For example,
A⊙
heightx=/summationdisplay
heightA⊙x=width/bracketleftbig3·2 + 1·7 + 2·1 1·2 + 5·7 + 6·1 4·2 + 9·7 + 5·1/bracketrightbig
6Published in Transactions on Machine Learning Research (1/2023)
A⊙
widthy=/summationdisplay
widthA⊙y=height
3·1 + 1·4 + 4·1
1·1 + 5·4 + 9·1
2·1 + 6·4 + 5·1
.
Again, we can write multiple names to contract multiple axes at once:
A⊙
height
widthA=/summationdisplay
height
widthA⊙A= 3·3 + 1·1 + 4·4 + 1·1 + 5·5 + 9·9 + 2·2 + 6·6 + 5·5
A⊙with no axis name under it contracts zero axes and is equivalent to elementwise multiplication, which
is the reason we use the same symbol ⊙for elementwise multiplication and contraction. The contraction
operator can be used for many multiplication-like operations:
x⊙
heightx=/summationdisplay
heightx⊙x inner product
x⊙y=heightwidth
2·1 2·4 2·1
7·1 7·4 7·1
1·1 1·4 1·1
 outer product
A⊙
widthy=/summationdisplay
widthA⊙y matrix-vector product
x⊙
heightA=/summationdisplay
heightx⊙A vector-matrix product
A⊙
widthB=/summationdisplay
widthA⊙B matrix-matrix product (B∈Rwidth×width′)
A contraction of three more tensors can be written as a sum. For example, the three-way inner product of
vectorsx,y,z∈Rwidthcan be written as/summationtext
widthx⊙y⊙z.
Like the dot-product from which it is lifted, but unlike matrix multiplication, the contraction operator is
commutative, but not associative. However, contraction does obey the following associative-like law.
X⊙
S∪T(Y⊙
UZ) = (X⊙
SY)⊙
T∪UZ ifS∩shpZ=U∩shpX=∅. (3)
The special case
X⊙
S(Y⊙
UZ) = (X⊙
SY)⊙
UZ ifS∩shpZ=U∩shpX=∅ (4)
will be useful in Section 5 for moving Zfrom inside one or more sets of parentheses to the outside.
3.5 Vectors to vectors and beyond
Functions fromvectors tovectors ( Rax[I]→Rax[I]) lift tofunctions ontensorsthat operate alongone axis, but
leave the tensor shape unchanged. Such functions are particularly problematic in standard notation, which
does not provide any way (to our knowledge) of specifying which axis the operation should be performed
over. Such functions include:
softmax
axX=expX/summationtext
axexpX(5a)
argmax
axX= lim
α→∞softmax
axαX (5b)
7Published in Transactions on Machine Learning Research (1/2023)
argmin
axX= lim
α→−∞softmax
axαX (5c)
For example, we can clearly distinguish between two ways of performing a softmax on A:
softmax
heightA=heightwidth
exp 3
exp 3+exp 1+exp 2exp 1
exp 1+exp 5+exp 6exp 4
exp 4+exp 9+exp 5
exp 1
exp 3+exp 1+exp 2exp 5
exp 1+exp 5+exp 6exp 9
exp 4+exp 9+exp 5
exp 2
exp 3+exp 1+exp 2exp 6
exp 1+exp 5+exp 6exp 5
exp 4+exp 9+exp 5

softmax
widthA=heightwidth
exp 3
exp 3+exp 1+exp 4exp 1
exp 3+exp 1+exp 4exp 4
exp 3+exp 1+exp 4
exp 1
exp 1+exp 5+exp 9exp 5
exp 1+exp 5+exp 9exp 9
exp 1+exp 5+exp 9
exp 2
exp 2+exp 6+exp 5exp 6
exp 2+exp 6+exp 5exp 5
exp 2+exp 6+exp 5

3.6 Renaming and reshaping
It’s often useful to rename an axis (analogous to a transpose operation in standard notation). We can think
of this as the lift of a function from vectors to vectors, but with different input and output axes:
[−]ax→ax′:Rax[I]→Rax′[I]
[Xax→ax′]ax′(i)=Xax(i)
For example,
Aheight→height′=height′width
3 1 4
1 5 9
2 6 5
.
We can also define notation for reshaping two or more axes into one axis:
A(height,width )→layer=layer/bracketleftbig3 1 4 1 5 9 2 6 5/bracketrightbig
The order of elements in the new axis is undefined. Authors who need a particular ordering may write a
more specific definition.
3.7 Indexing2
NumPy and its derivatives provide various ways to recombine elements of a tensor to form a new tensor:
integer array indexing, and functions like numpy.take ,numpy.take_along_axis ,torch.index_select , and
torch.gather . Using named tensors, we can write nearly all of these operations as lifts of a single function:
index
ax:Rax[n]×[n]→R
index
ax(X,i) =Xax(i).
For example, suppose we have
E∈Rvocab [n]×emb
i∈[n]
I∈[n]seq
P∈Rseq×vocab [n]
2We are grateful to Tongfei Chen and Chu-Cheng Lin for contributing the original idea behind this section, as well as the
example.
8Published in Transactions on Machine Learning Research (1/2023)
TensorEcontains word embeddings for all the words in the vocabulary. Integer iis the numeric identifier
of a word, while tensor Iis a sequence of numeric identifiers of words. Tensor Pcontains a sequence of
probability distributions over the vocabulary (e.g., the predictions of a language model). Then:
•index
vocab(E,i)broadcasts the embaxis fromEtoi, giving the word embedding of word i. This is the
same as partial indexing ( Evocab (i)).
•index
vocab(E,I)also broadcasts the seqaxis fromItoE, giving a sequence of word embeddings. This
is the same as integer array indexing ( E[I]),numpy.take( E,I, 0), ortorch.index_select( E,
0,I).
•index
vocab(P,I)alignsP’s andI’sseqaxes, giving a sequence of probabilities. This is the same as
numpy.take_along_axis( P,I, 0)ortorch.gather( P, 0,I).
•IfPandIadditionally had a batchaxis (before the other axes), then index
vocab(P,I)would be the same
astensorflow.gather( P,I, axis=1, batch_dims=1) .
In NumPy, indexing using two or more integer arrays requires a special definition with some surprising
special cases. With named tensors, we simply apply the indexing function twice. For example, if we wanted
to get probabilities of words Jat a subset Iof positions, we could let:
|seq|=m
I∈[m]subseqpositions
J∈[n]subseqnumeric identifiers
S= index
vocab(index
seq(P,I),J)∈Rsubseq
so that
Ssubseq (k)=Pseq(Isubseq (k)),vocab (Isubseq (k)).
4 Worked Examples: Neural Networks
In this section we give a series of worked examples illustrating how standard neural network model com-
ponents can be written using named tensors. Appendix A builds some of these components into complete
specifications of the Transformer and LeNet.
4.1 Feedforward neural networks
A multi-layer, feedforward neural network with different-sized layers can be written as:
X0∈Rinput
X1=σ(W1⊙
inputX0+b1) W1∈Rhidden1×inputb1∈Rhidden1
X2=σ(W2⊙
hidden1X1+b2) W2∈Rhidden2×hidden1 b2∈Rhidden2
X3=σ(W3⊙
hidden2X2+b3) W3∈Rout×hidden2 b3∈Rout
The layer sizes can be specified by writing |hidden1|=n1, etc. As noted above, σis applied elementwise and
does not require additional annotation.
9Published in Transactions on Machine Learning Research (1/2023)
Alternatively, the layer equation can be abstracted by defining:
FullConnl(x) =σ/parenleftig
Wl⊙
layerx+bl/parenrightig
layer′→layer
where
Wl∈Rlayer′[nl]×layer[nl−1]
bl∈Rlayer′[nl].
The function FullConnlencapsulates both the equation for layer las well as its parameters Wl,bl(analogous
to what TensorFlow and PyTorch call modules). Since we chose to use the same axis name layerfor all the
layers (with different sizes nl), FullConnltemporarily computes its output over axis layer′, then renames it
back to layer. The network can be defined like this:
X0∈Rlayer[n0]
X1=FullConn1(X0)
X2=FullConn2(X1)
X3=FullConn3(X2).
4.2 Recurrent neural networks
As a second example, we consider a simple (Elman) RNN. This model is similar to the feedforward network,
except that the number of timesteps is variable and parameters are shared over time. At each time step, it
produces a tensor with a new axis hidden′which is then renamed hiddenfor the next step in the recursion.
xt∈Rinputt= 1,...,n
Wh∈Rhidden×hidden′|hidden|=|hidden′|
Wi∈Rinput×hidden′
b∈Rhidden′
h0∈Rhidden
ht=σ/parenleftig
Wh⊙
hiddenht−1+Wi⊙
inputxt+b/parenrightig
hidden′→hiddent= 1,...,n
4.3 Attention
In the introduction (§1), we described difficulties in interpreting the equation for attention as used with
Transformers (Vaswani et al., 2017). In our notation, it looks like this:
Attention :Rkey×Rseq×key×Rseq×val→Rval(6)
Attention (Q,K,V ) =
softmax
seqQ⊙
keyK
/radicalbig
|key|
⊙
seqV. (7)
This definition takes a single query Qvector and returns a single result vector (and actually could be further
reduced to a scalar values as valis not strictly necessary). To apply to a sequence, we can give Qaseq′
axis, and the function will compute an output sequence. Providing Q,K, andVwith a headsaxis lifts the
function to compute multiple attention heads.
For Transformers we often need to apply a mask to ensure attention is only applied to valid keys (e.g. for
causal language models). We can modify the equation to include this mask:
Attention :Rkey×Rseq×key×Rseq×val×Rseq→Rval
10Published in Transactions on Machine Learning Research (1/2023)
Attention (Q,K,V,M ) =
softmax
seq
Q⊙
keyK
/radicalbig
|key|+M

⊙
seqV.
Appendix A.1 includes a full specification of the complete Transformer model using the named tensor nota-
tion.
4.4 Convolution
Standard neural network convolutions can be specified by “unrolling” a vector and then applying a standard
dotproduct. Wedefineanaxis-parameterizedunrollingfunctionthatconvertsaone-axistensortoasequence
ofkernelsized vectors:
unroll
seq
kernel:Rseq[n]→Rseq[n−|kernel|+1],kernel
unroll
seq
kernelX=Y,where
Yseq(i),kernel (j)=Xseq(i+j−1).
A 1d convolution with input channels chansand output channels chans′consists of unrolling along the seq
axis and then taking a dot product:
Conv1d :Rchans×seq[n]→Rchans′×seq[n′]
Conv1d (X;W,b) =W⊙
chans
kernelunroll
seq
kernelX+b
where
W∈Rchans′×chans×kernel
b∈Rchans′
Unrolling easily generalizes to higher-dimensional convolutions:
Conv2d :Rchans×height [h]×width [w]→Rchans′×height [h′]×width [w′]
Conv2d (X;W,b) =W⊙
chans
kh,kwunroll
height
khunroll
width
kwX+b
where
W∈Rchans′×chans×kh×kw
b∈Rchans′.
4.5 Pooling
Pooling is similar to convolutions. We first define a function to partition a tensor into windows.
pool
seq,kernel:Rseq[n]→Rseq[n/|kernel|],kernel
pool
seq,kernelX=Y,where
Yseq(i),kernel (j)=Xseq((i−1)·|kernel|+j).
11Published in Transactions on Machine Learning Research (1/2023)
Then we can define aggregations over kernel. We define max-pooling as:
MaxPool1d k:Rseq[n]→Rseq[n/k]
MaxPool1d k(X) = max
kernelpool
seq,kernelX
|kernel|=k
MaxPool2d kh,kw :Rheight [h]×width [w]→Rheight [h/kh ]×width [w/kw ]
MaxPool2d kh,kw (X) = max
kh,kwpool
height,khpool
width,kwX
|kh|=kh
|kw|=kw.
4.6 Normalization layers
Normalization layers are used in all large-scale deep learning models, with different architectures requiring
different types of normalization. However, despite their importance, the differences between them are often
notclearlycommunicated. Forexample, thePyTorchdocumentation(PyTorchContributors,2022)describes
all of them using the same equation (where ϵ>0is a small constant for numerical stability):
Y=X−mean(X)/radicalbig
var(X) +ϵ⊙γ+β
Wu & He (2018) give essentially the same equation and explain the differences using a combination of
equations, words, and pictures. But they do not capture differences in γandβamong different normalization
layers.
Critically, the layers do differ by which axes are standardized as well as their parameters. We define a single
named standardization function as:
standardize
ax:Rax→Rax
standardize
ax(X) =X−mean
ax(X)
/radicalbigg
var
ax(X) +ϵ
Then, wecandefinethethreekindsofnormalizationlayers, allwithtype Rbatch×chans×layer→Rbatch×chans×layer.
While superficially similar, these functions differ in their standardized axes and their parameter shape.
BatchNorm (X;γ,β) = standardize
batch,layer(X)⊙γ+β γ,β ∈Rchans
InstanceNorm (X;γ,β) = standardize
layer(X)⊙γ+β γ,β ∈Rchans
LayerNorm (X;γ,β) = standardize
layer,chans(X)⊙γ+β γ,β ∈Rchans,layer
5 Differential Calculus
In many machine learning applications, we need to compute derivatives of functions from tensors to tensors.
In standard vector/matrix notation, this can become complicated. For example, if fmaps from vectors
to vectors, then the partial derivatives of fform a matrix (the Jacobian). It has an “input” axis for the
directions in which Xcould change, and an “output” axis for the directions in which f(X)could change.
But there are conflicting conventions about whether the first axis is the input axis (“denominator layout”)
or the output axis (“numerator layout”). The derivative of a function from vectors to matrices or matrices
to vectors cannot be represented as a matrix at all, so one must resort to flattening the matrices into vectors.
12Published in Transactions on Machine Learning Research (1/2023)
With non-named tensor index notation, taking derivatives is not difficult (Laue et al., 2018), but again a
convention must be adopted that the input axes come after the output axes, separated by a comma.
With named tensors, axes are not ordered, so we don’t need to remember whether the input or output axes
come first. But we do need to ensure that the input and output axes have different names.
5.1 Definition
Definition 7. LetS=ax1×···× axrbe a shape. Then we write S∗=ax∗
1×···× ax∗
r, and ifs=
{ax1(i1),... axr(ir)}∈recS, then we write s∗={ax∗
1(i1),... ax∗
r(ir)}. Furthermore, if X∈RSthen we
writeX∗=XS→S∗.
Definition 8. Letf:RS→RT. Thederivative off(X)with respect to X∗is the tensor such that
∂f(X)
∂X∗∈RS∗∪T
/bracketleftbigg∂f(X)
∂X∗/bracketrightbigg
s∗,t=∂f(X)t
∂Xs
for alls∈recSandt∈recT.
The above definition assumes that S∗andTare orthogonal; if not, the axes in Sshould be renamed to
something else. For example, the second derivative (the Hessian) could be
∂2f(X)
∂X∗∂X†∈RS∗∪S†∪T
/bracketleftbigg∂2f(X)
∂X∗∂X†/bracketrightbigg
r∗,s†,t=∂2f(X)t
∂Xr∂Xs
for allr,s∈recSandt∈recT.
5.2 Differentials
We could derive rules like the chain rule and the sum and product rules, and use them to compute derivatives;
however, ensuring that input and output shapes are orthogonal is inconvenient. Instead, we recommend the
method of differentials (Magnus & Neudecker, 1985), which reduces renaming to a minimum.
The first-order Taylor approximation of faroundX∈RSis
f(X+H)≈f(X) +∂f(X)
∂X∗⊙
S∗H∗H∈RS.
Thedifferential off(X)with increment H, written∂f(X;H), is the second term of this approximation; we
defer a formal definition to Appendix B.
For example,
•If id is the identity function, then
id(X+H) =X+H
∂id(X;H) =H. (8a)
•Iff(X) =X⊙
axXwhereX∈Rax, then
f(X+H) = (X+H)⊙
ax(X+H)
=X⊙
axX+ 2X⊙
axH+H⊙
axH
∂f(X;H) = 2X⊙
axH. (8b)
13Published in Transactions on Machine Learning Research (1/2023)
It’s often more convenient to work directly with the expression X⊙
axXinstead off(X), and to write ∂(X⊙
axX)
for∂f(X;H). Then, since ∂X=∂id(X;H) =H, we can write Eq. (8b) simply as
∂(X⊙
axX) = 2X⊙
ax∂X
so that the Hhas beeen “hidden” inside ∂X. More generally, we can derive rules like the following:
∂(U+V) =∂U+∂V (9a)
∂(U⊙V) =U⊙∂V+V⊙∂U (9b)
∂/parenleftbiggU
V/parenrightbigg
=V⊙∂U−U⊙∂V
V2(9c)
∂/summationdisplay
axU=/summationdisplay
ax∂U (9d)
∂(U⊙
axV) =U⊙
ax∂V+V⊙
ax∂U (9e)
∂Us= [∂U]s(9f)
∂Uax→ax′= [∂U]ax→ax′. (9g)
The chain rule for differentials is
∂f(U) =∂f(X)
∂X∗/vextendsingle/vextendsingle/vextendsingle/vextendsingle
X=U⊙
S∗∂US→S∗ f:RS→RT. (9h)
Recall that fcan be lifted to shapes larger than S. In that case, the rule above still applies, but note that
the contraction will still be over S. A special case of this is when S=T=∅:
∂f(U) =df(x)
dx/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=U⊙∂U f :R→R. (9i)
For example, letting f(x) = expxgives the rule
∂(expU) = expU⊙∂U. (9j)
Using these rules we can compute the differential of a wide variety of expressions. For example, the softmax
operator:
∂(softmax
axX)(5a)=∂/parenleftbiggexpX/summationtext
axexpX/parenrightbigg
(9c)=/parenleftbig/summationtext
axexpX/parenrightbig
⊙∂(expX)−expX⊙∂/parenleftbig/summationtext
axexpX/parenrightbig
/parenleftbig/summationtext
axexpX/parenrightbig2
(9d)=/parenleftbig/summationtext
axexpX/parenrightbig
⊙∂(expX)−expX⊙/summationtext
ax∂(expX)
/parenleftbig/summationtext
axexpX/parenrightbig2
(9j)=/parenleftbig/summationtext
axexpX/parenrightbig
⊙expX⊙∂X−expX⊙/summationtext
ax(expX⊙∂X)
/parenleftbig/summationtext
axexpX/parenrightbig2
(2)=/parenleftbig/summationtext
axexpX/parenrightbig
⊙expX⊙∂X−expX⊙(expX⊙
ax∂X)
/parenleftbig/summationtext
axexpX/parenrightbig2
14Published in Transactions on Machine Learning Research (1/2023)
=expX/summationtext
axexpX⊙∂X−expX/summationtext
axexpX⊙/parenleftigg
expX/summationtext
axexpX⊙
ax∂X/parenrightigg
(5a)= softmax
axX⊙∂X−softmax
axX⊙(softmax
axX⊙
ax∂X)
= softmax
axX⊙(∂X−softmax
axX⊙
ax∂X). (10)
We stop when the only differentials left are ∂X.
5.3 Derivatives via differentials
If we can get ∂f(X)into so-called canonical form ,
∂f(X) =D⊙
S∗∂X∗+const. (11)
where “const.” stands for terms not depending on ∂X, then by Magnus & Neudecker’s first identification
theorem (Theorem 1 in Appendix B), we can conclude that
∂f(X)
∂X∗=D.
When trying to get expressions into canonical form, one helpful fact is that renaming can be thought of as
contraction with an identity matrix. First we define the identity matrix with shape ax×ax′:
[Iax,ax′]ax(i),ax′(j)=/braceleftigg
1i=j
0i̸=j.
Then for any tensor Awith an axis ax,
Aax→ax′=Iax,ax′⊙
axA. (12)
More specifically, if ∂X∈RS, then
∂X=IS,S∗⊙
S∗∂X∗(13)
and then Eq. (4) can usually be used to move the ⊙
S∗∂XS→S∗to the outermost level of the expression.
Above, we found the differential of the softmax function; now let us find its derivative.
∂(softmax
axX)(10)= softmax
axX⊙(∂X−softmax
axX⊙
ax∂X)
(13)= softmax
axX⊙(Iax,ax∗⊙
ax∗∂X∗−softmax
axX⊙
ax(Iax,ax∗⊙
ax∗∂X∗))
(4)= (softmax
axX⊙(Iax,ax∗−softmax
axX⊙
axIax,ax∗))⊙
ax∗∂X∗
(12)= (softmax
axX⊙(Iax,ax∗−(softmax
axX)∗))⊙
ax∗∂X∗.
This is in canonical form, so we have
∂
∂X(softmax
axX) = softmax
axX⊙(Iax,ax∗−(softmax
axX)∗). (14)
15Published in Transactions on Machine Learning Research (1/2023)
5.4 Lifting
Recall that fS′is the lift of f:RS→RTwithS′, and in most contexts we can simply write finstead of
fS′. However, derivatives are one place where some extra caution is in order. To lighten notation, let’s write
gfor the derivative of f:
g(X) =∂f(X)
∂X∗.
Recall that the chain rule (9h) works under lifting, so
∂fS′(X) =gS′(X)⊙
S∗∂XS→S∗.
But the contraction is only over S∗, so it would be incorrect to conclude that∂fS′(X)
∂X=gS′(X). The
derivative of a lift is notthe lift of a derivative. We must rename and contract S′as well:
∂fS′(X)(9h)=gS′(X)⊙
S∗∂XS→S∗
(13)=gS′(X)⊙
S∗(IS′,S′∗⊙
S′∗∂XS∪S′→(S∪S′)∗)
(3)= (gS′(X)⊙IS′,S′∗)⊙
(S∪S′)∗∂XS∪S′→(S∪S′)∗
∂fS′(X)
∂X∗=gS′(X)⊙IS′,S′∗.
In general, then, the derivative of a lift is the lift of the derivative, multiplied by the identity matrix for the
newaxes. Intuitively, thisisbecausethederivativeisalineartransformation—beforelifting, atransformation
fromS∗toT. When lifting to S∪S′, this transformation must also be lifted, which is what multiplication
byIS′,S′∗accomplishes.
5.5 Extended example
As a more elaborate example, we find the derivative of self-attention. For brevity, we omit the factor1√
|key|,
and we write α= softmax
seq(Q⊙
keyK).
∂Att(Q,K,V )(7)=∂(α⊙
seqV)
(9e)=α⊙
seq∂V+V⊙
seq∂α. (15)
Focus first on the first term, which is the only term depending on ∂V:
α⊙
seq∂V(13)=α⊙
seq((Iseq,seq∗⊙Ival,val∗)⊙
seq∗
val∗∂V∗)
(4)= ((α⊙
seqIseq,seq∗)⊙Ival,val∗)⊙
seq∗
val∗∂V∗
(12)= (αseq→seq∗⊙Ival,val∗)⊙
seq∗
val∗∂V∗
∂
∂V∗Att(Q,K,V ) =αseq→seq∗⊙Ival,val∗.
16Published in Transactions on Machine Learning Research (1/2023)
Next, focus on the second term of Eq. (15):
V⊙
seq∂α(10)=V⊙
seq(α⊙(∂(Q⊙
keyK)−α⊙
seq∂(Q⊙
keyK)))
(9e)=V⊙
seq(α⊙(Q⊙
key∂K+K⊙
key∂Q−α⊙
seq(Q⊙
key∂K+K⊙
key∂Q))). (16)
Keeping only terms depending on ∂Q, we get
V⊙
seq(α⊙(K⊙
key∂Q−α⊙
seq(K⊙
key∂Q)))
(13)=V⊙
seq(α⊙(K⊙
key(Ikey,key∗⊙
key∗∂Q∗)−α⊙
seq(K⊙
key(Ikey,key∗⊙
key∗∂Q∗))))
(4)=/parenleftig
V⊙
seq(α⊙(K⊙
keyIkey,key∗−α⊙
seq(K⊙
keyIkey,key∗)))/parenrightig
⊙
key∗∂Q∗
(4)=/parenleftig
V⊙
seq(α⊙(Kkey→key∗−α⊙
seqKkey→key∗))/parenrightig
⊙
key∗∂Q∗
∂
∂Q∗Att(Q,K,V ) =V⊙
seq(α⊙(Kkey→key∗−α⊙
seqKkey→key∗)).
Similarly, keeping only terms from Eq. (16) depending on ∂K, we get
V⊙
seq(α⊙(Q⊙
key∂K−α⊙
seq(Q⊙
key∂K)))
=/parenleftig
V⊙
seq(α⊙(Q∗⊙Iseq,seq∗)−α⊙
seq(Q∗⊙Iseq,seq∗)))/parenrightig
⊙
key∗
seq∗∂K∗
∂
∂K∗Att(Q,K,V ) =V⊙
seq(α⊙(Q∗⊙Iseq,seq∗)−α⊙
seq(Q∗⊙Iseq,seq∗))).
6 Alternatives and Related Work
6.1 Index notations
Among alternatives to standard vector and matrix notation, the most common one is index notation as used
in physics (Ricci & Levi-Civita, 1900). Related notations are used in other fields as well (Harshman, 2001).
In this notation, axes are ordered, and every equation is written in terms of tensor components. If an index
appears on both sides of an equation, then the equation must hold for each value of the index, and the
Einstein summation convention (Einstein, 1916) is that if an index appears twice on one side and not on the
other, there is an implicit summation over that index.
Attention :Rdk×Rn×dk×Rn×dv→Rdv
[Attention (Q,K,V )]k= softmax
i/parenleftbiggQjKij√dk/parenrightbigg
Vik.
Becausekappears on both sides, the equation must hold over all values of this index. But because iandj
occur twice on only the right-hand side, they are both summed over. We would have to define exactly what
theiunder the softmax means ( iis bound inside the softmax and free outside it), and since softmax doesn’t
distribute over addition, we would need to modify the summation convention so that the summation over j
occurs inside the softmax.
Aside from these correctable issues, this notation scales very well to more than two axes and is concise and
unambiguous. But it does not solve the main problem we set out to solve, which is that ordered axes force
the author and reader to remember the purpose of each axis. The indices do act as symbolic names for axes
17Published in Transactions on Machine Learning Research (1/2023)
(indeed, in abstract index notation (Penrose & Rindler, 1984), they really are symbols, not variables), but
they are temporary names; they could be totally different in the next equation. It would be up to the author
to choose to use consistent names, and to do so correctly.
A second issue is that because it depends on repetition of indices to work, index notation can be more verbose
than our notation, particularly for reductions and contractions:
C= max
iAi C= max
axA
C=AiBi C=A⊙
axB.
Finally, index notation requires us to write out all indices explicitly. So if we wanted to lift attention to
minibatches ( b∈[B]), multiple heads ( h∈[H]) and multiple query tokens ( i′∈[n′]), we would write:
Attention :RB×H×n′×dk×RB×H×n×dk×RB×H×n×dv→RB×H×n′×dv
[Attention (Q,K,V )]bhi′k= softmax
i/parenleftbiggQbhi′jKbhij√dk/parenrightbigg
Vbhik.
We could adopt a convention that lifts a function on tensors to tensors that have extra axes to the left, but
such conventions tend to lead to messy reordering and squeezing/unsqueezing of axes. Named axes make
such conventions unnecessary.
6.2 Graphical notations
In the graphical notation of Penrose (1971), a node in the graph stands for a tensor, and its incident edges
stand for its indices. The edges are ordered from left to right. An edge connecting two nodes denotes
contraction. The notation of Alsberg (1997) is similar, except that edges are named, not ordered.
Graphs are commonly used in machine learning for representing probability models (Koller & Friedman,
2009). A node in the graph stands for a random variable, and an edge or hyperedge stands for a dependency
between variables. If random variables have finite domains, then a (hyper)edge with rendpoints can be
thought of as an r-th order tensor. A graph can then be thought of as a product and contraction. Extensions
that allow for a choice between two subgraphs (e.g., Minka & Winn, 2008) can be thought of as addition.
Our assessment of graphical notations like these is that, on the positive side, they have obvious value for
visualization, and they at least have the potential to represent indices in a purely unordered way. On the
negative side, these notations seem best suited for representing linear functions, and even for this purpose,
some other practical considerations are that drawing pictures requires more effort from the author, and that
pictures will have a less transparent relationship with their implementation in most programming languages.
6.3 Relational algebra
In relational algebra (Codd, 1970), the basic objects are sets of r-tuples, which could be thought of as
tensors of order rwith Boolean-valued entries. In the original formulation, the members of the tuples, which
correspond to axes, were both named andordered, although later definitions (e.g. Pirotte, 1982) made them
unordered.
Probabilistic variants of relational algebra also exist (e.g. Dey & Sarkar, 1996; Fuhr & Rölleke, 1997), whose
relations would correspond to tensors of probabilities.
While relational algebra and tensor notations are designed for totally different purposes, the notation of
relational algebra generally has a similar flavor to ours (for example, our contraction operator is similar to
the▷◁operator, and our renaming operator is the same as the ρoperator).
18Published in Transactions on Machine Learning Research (1/2023)
6.4 Programming languages
One response to the notation presented here, as well as the alternative notations mentioned in this section,
is that research papers in machine learning should simply present models as code rather than equations. But
we argue that a model’s mathematical specification should abstract away from details of its implementation.
Conceptually, it is important to have a distinct specification to define what makes an implementation (both
the original implementation and any reimplementations) correct or incorrect. If the implementation is its
own specification, it cannot be correct or incorrect; it will be “not even wrong.”
Practically, abstracting away from implementation is important because we do not want the interpretation of
research papers to be subject to differences across programming languages and libraries, or versions thereof.
Forexample, PyTorch’s Dropout2d on order-3 tensors hasone behaviorin versions 1.11 and 1.13, but another
behavior in 1.10, 1.12, and future versions. It would be problematic for correct understanding of a paper to
depend on such differences.
7 Conclusions
Named tensor notation is a system of formal notation for representing operations between tensors in a
non-ambiguous way while remaining intuitive for practitioners. The system is motivated by challenges
that arise from taking notation designed for applied linear algebra and using it for representing neural
networks, as demonstrated through examples of canonical deep-learning components such as attention and
layer normalization. However, named tensors are not limited to specifying neural networks. We have also
explained how to integrate our notation with Magnus & Neudecker (1985)’s method of differentials for matrix
calculus. While there are other conventions that such as index notation that have some usage in the machine
learning community, these conventions either lack the conciseness of named tensors or are not well-suited to
non-linear operations. For these reasons, we encourage members of the machine learning community to try
out named tensor notation for teaching, research, and software documentation.
Acknowledgements
We would like to thank Ekin Akyürek, Justin Bayer, Tongfei Chen, Chu-Cheng Lin, Colin McDonald, Adam
Poliak, Matt Post, Chung-chieh Shan, Nishant Sinha, and Yee Whye Teh for their input to this document
(or the ideas in it). We also thank the anonymous TMLR reviewers for their feedback, which substantially
improved the quality of the paper, especially Section 5.
This material is based upon work supported by the National Science Foundation under Grants No. CCF-
2019291 and DMS-2134157, as well as a Sloan Fellowship, Simons Investigator Fellowship, DARPA grant
W911NF2010021, and DOE grant DE-SC0022199. Any opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the authors and do not necessarily reflect the views of the
funding agencies.
References
Bjørn K. Alsberg. A diagram notation for n-mode array equations. Journal of Chemometrics , 11:251–266,
1997. doi: 10.1002/(SICI)1099-128X(199705)11:3<251::AID-CEM472>3.0.CO;2-Q.
Matt Bauman. Axisarrays, 2018. URL https://github.com/JuliaArrays/AxisArrays.jl . Open-source
software.
Tongfei Chen. Typesafe abstractions for tensor operations. In Proc. 8th ACM SIGPLAN International
Symposium on Scala , pp. 45–50, 2017. doi: 10.1145/3136000.3136001.
E. F. Codd. A relational model of data for large shared data banks. Communications of the ACM , 13(6):
377–387, June 1970. doi: 10.1145/362384.362685.
19Published in Transactions on Machine Learning Research (1/2023)
Zachary DeVito. Named tensors using first-class dimensions in PyTorch, 2023. URL https://github.com/
facebookresearch/torchdim . Open-source software.
Debabrata Dey and Sumit Sarkar. A probabilistic relational model and algebra. ACM Transactions on
Database Systems , 21(3):339–369, September 1996. doi: 10.1145/232753.232796.
A. Einstein. Die Grundlage der allgemeinen Relativitätstheorie. Annalen der Physik , 354(7):769–880, 1916.
doi: 10.1002/andp.19163540702.
Norbert Fuhr and Thomas Rölleke. A probabilistic relational algebra for the integration of information
retrieval and database systems. ACM Transactions on Information Systems , 15(1):32–66, January 1997.
doi: 10.1145/239041.239045.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning . MIT Press, 2016.
Richard A. Harshman. An index formalism that generalizes the capabilities of matrix notation and algebra
to n-way arrays. Journal of Chemometrics , 15:689–714, 2001. doi: 10.1002/cem.665.
Stephan Hoyer and Joe Hamman. xarray: N-D labeled arrays and datasets in Python. Journal of Open
Research Software , 5(1):10, 2017. doi: 10.5334/jors.148.
JAX authors. Named axes and easy-to-revise parallelism, 2021. URL https://jax.readthedocs.io/en/
latest/notebooks/xmap_tutorial.html .
Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques . MIT Press,
2009.
Soeren Laue, Matthias Mitterreiter, and Joachim Giesen. Computing higher order derivatives of matrix and
tensor expressions. In Proc. NeurIPS , pp. 2750–2759, 2018. URL https://proceedings.neurips.cc/
paper/2018/file/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf .
Jan R. Magnus and H. Neudecker. Matrix differential calculus with applications to simple, Hadamard,
and Kronecker products. Journal of Mathematical Psychology , 29(4):474–492, 1985. doi: 10.1016/0022-
2496(85)90006-9.
Tom Minka and John Winn. Gates. In Proc. NeurIPS , pp. 1073–1080, 2008. URL https://papers.nips.
cc/paper/3379-gates .
AdamPaszke,DanielD.Johnson,DavidDuvenaud,DimitriosVytiniotis,AlexeyRadul,MatthewJ.Johnson,
JonathanRagan-Kelley, andDougalMaclaurin. Gettingtothepoint: Indexsetsandparallelism-preserving
autodiff for pointful array programming. Proc. ACM on Programming Languages , 5(ICFP), August 2021.
doi: 10.1145/3473593.
R. Penrose and W. Rindler. Spinors and space-time , volume 1. Cambridge University Press, 1984.
Roger Penrose. Applications of negative dimensional tensors. In D. J. A. Welsh (ed.), Combinatorial
Mathematics and its Applications , pp. 221–244. Academic Press, 1971.
Alain Pirotte. A precise definition of basic relational notations and of the relational algebra. ACM SIGMOD
Record, 13(1):30–45, 1982. doi: 10.1145/984514.984516.
PyTorch Contributors. PyTorch documentation, 2022. URL https://pytorch.org/docs/1.12/ . version
1.12.
M.M.G.RicciandT.Levi-Civita. Méthodesdecalculdifférentielabsoluetleursapplications. Mathematische
Annalen, 54:125–201, 1900. doi: 10.1007/BF01454201.
Alex Rogozhnikov. Einops: Clear and reliable tensor manipulations with Einstein-like notation. In Proc.
ICLR, 2022. URL https://openreview.net/pdf?id=oapKSVM2bcj .
20Published in Transactions on Machine Learning Research (1/2023)
Alexander Rush. Named tensors, 2019. URL https://github.com/harvardnlp/NamedTensor . Open-source
software.
Nishant Sinha. Tensor shape (annotation) library, 2018. URL https://github.com/ofnote/tsalib . Open-
source software.
Torch Contributors. Named tensors, 2019. URL https://pytorch.org/docs/stable/named_tensor.html .
PyTorch documentation.
L. R. Tucker. The extension of factor analysis to three-dimensional matrices. In H. Gulliksen and N. Fred-
eriksen (eds.), Contributions to Mathematical Psychology , pp. 110–127. Holt, Rinehart and Winston, New
York, 1964.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Proc. NeurIPS , pp. 5998–6008, 2017. URL https:
//proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .
Yuxin Wu and Kaiming He. Group normalization. In Proc. ECCV , 2018. URL https://openaccess.
thecvf.com/content_ECCV_2018/papers/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.pdf .
A Extended Examples
A.1 Transformer
We define a Transformer used autoregressively as a language model. The input is a sequence of one-hot
vectors, from which we compute word embeddings and positional encodings:
I∈{0,1}seq×vocab/summationdisplay
vocabI= 1
W= (E⊙
vocabI)/radicalbig
|layer| E∈Rvocab×layer
P∈Rseq×layer
Pseq(p),layer(i)=/braceleftigg
sin((p−1)/10000(i−1)/|layer|)iodd
cos((p−1)/10000(i−2)/|layer|)ieven.
Then we use Llayers of self-attention and feed-forward neural networks:
X0=W+P
T1=LayerNorm1(SelfAtt1(X0) +X0)
X1=LayerNorm1′(FFN1(T1) +T1)
...
TL=LayerNormL(SelfAttL(XL−1) +XL−1)
XL=LayerNormL′(FFNL(TL) +TL)
O= softmax
vocab(E⊙
layerXL)
where LayerNorm, SelfAtt and FFN are defined below.
Layer normalization ( l= 1,1′,...,L,L′):
LayerNorml:Rlayer→Rlayer
LayerNorml(X) = standardize
layer(X)⊙γl+βl
21Published in Transactions on Machine Learning Research (1/2023)
βl,γl∈Rlayer
We defined attention in §4.3; the Transformer uses multi-head self-attention, in which queries, keys, and
values are all computed from the same sequence.
SelfAttl:Rseq×layer→Rseq×layer
SelfAttl(X) =Y
where
|seq|=|seq′|
|key|=|val|=|layer|/|heads|
Q=Wl,Q⊙
layerXseq→seq′ Wl,Q∈Rheads×layer×key
K=Wl,K⊙
layerX Wl,K∈Rheads×layer×key
V=Wl,V⊙
layerX Wl,V∈Rheads×layer×val
M∈Rseq×seq′
Mseq(i),seq′(j)=/braceleftigg
0i≤j
−∞otherwise
Y=Wl,O⊙
heads
valAttention (Q,K,V,M )seq′→seq Wl,O∈Rheads×val×layer
Feedforward neural networks:
FFNl:Rlayer→Rlayer
FFNl(X) =X2
where
X1=relu(Wl,1⊙
layerX+bl,1) Wl,1∈Rhidden×layerbl,1∈Rhidden
X2=Wl,2⊙
hiddenX1+bl,2Wl,2∈Rlayer×hiddenbl,2∈Rhidden.
A.2 LeNet
X0∈Rbatch×chans [c0]×height×width
T1=relu(Conv1(X0))
X1=MaxPool1(T1)
T2=relu(Conv2(X1))
X2=MaxPool2(T2)(height,width,chans )→layer
X3=relu(W3⊙
layerX2+b3) W3∈Rhidden×layerb3∈Rhidden
O= softmax
classes(W4⊙
hiddenX3+b4) W4∈Rclasses×hiddenb4∈Rclasses
As an alternative to the flattening operation in the equation for X2, we could have written
X2=MaxPool2(T2)
22Published in Transactions on Machine Learning Research (1/2023)
X3=relu(W3⊙
height
width
chansX2+b3) W3∈Rhidden×height×width×chans.
The convolution and pooling operations are defined as follows:
Convl(X) =Conv2d (X;Wl,bl)chans′→chans
where
Wl∈Rchans′[cl]×chans [cl−1]×kh[khl]×kw[kwl]
bl∈Rchans′[cl]
and
MaxPooll(X) =MaxPool2d phl,phl(X).
B Differentiation: Formal Definitions
The following definition and theorem come directly from the paper by Magnus & Neudecker (1985), but
generalized to named tensors.
For anyX∈RS, we write∥X∥= norm
SX.
Definition 9. Letf:S→RTwhereS⊆RS. LetAbe an interior point of RS, that is, for some r >0,
B(A;r) ={X|∥X−A∥<r}⊆S. If there is a tensor D(A)∈RS∗∪TandR(A,H )∈RTsuch that
f(A+H) =f(A) +D(A)⊙
S∗HS→S∗+R(A,H )
for allH∈RSwith∥H∥<r, and
lim
H→0R(A,H )
∥H∥=0,
thenfis said to be differentiable atA; the tensor
∂f(A;H) =D(A)⊙
S∗HS→S∗
is then called the (first) differential of fatAwith increment H.
Magnus & Neudecker give their (first) identification theorem twice, once for vector-to-vector functions and
once for matrix-to-matrix functions (but omitting vector-to-matrix and matrix-to-vector functions). Here,
we only need one version, which works for functions from tensors to tensors of any shape.
Theorem 1. Letf:S→RT, whereS⊆RS, be differentiable at A∈S. LetD(X)∈RS∗∪T. Then
for allH,∂f(A;H) =D(X)⊙
S∗HS→S∗iff∂f(X)
∂X∗/vextendsingle/vextendsingle/vextendsingle/vextendsingle
X=A=D(X).
C LATEX Macros
Many of the L ATEX macros used in this document are available in the style file namedtensor.sty , available
on CTAN at https://ctan.org/pkg/namedtensor . To use it, put
\usepackage{namedtensor}
23Published in Transactions on Machine Learning Research (1/2023)
in the preamble of your L ATEX source file (after \documentclass{article} but before \begin{document} ).
We write axis names in sans-serif font. To make this easier, \name{ax} prints an axis name (like this: ax),
and\ndef{\ax}{ax} defines a macro \axthat does the same.
•Binary operators
–UseA \ndot{\ax} B for contraction: A⊙
axB. You can use \\to stack up several names.
–In general, you can use \nbinto make a new binary operator with a name under it:
A \nbin{\ax}{\star} B gives youA ⋆
axB.
•Functions
–Use\nsum{\ax} A for summation:/summationtext
axA.
–In general, you can use \nfunto make a function with a name under it: \nfun{\ax}{qux} A
gives you qux
axA.
24