Under review as submission to TMLR
Distillation Policy Optimization
Anonymous authors
Paper under double-blind review
Abstract
While on-policy algorithms are known for their stability, they often demand a substantial
number of samples. In contrast, off-policy algorithms, which leverage past experiences, are
considered sample-efficient but tend to exhibit instability. Can we develop an algorithm that
harnesses the benefits of off-policy data while maintaining stable learning? In this paper,
we introduce an actor-critic learning framework that harmonizes two data sources for both
evaluation and control, facilitating rapid learning and adaptable integration with on-policy
algorithms. Thisframeworkincorporatesvariancereductionmechanisms, includingaunified
advantage estimator (UAE) and a residual baseline, improving the efficacy of both on- and
off-policy learning. Our empirical results showcase substantial enhancements in sample
efficiency for on-policy algorithms, effectively bridging the gap to the off-policy approaches.
It demonstrates the promise of our approach as a novel learning paradigm.
1 Introduction
Deep model-free reinforcement learning (RL) has emerged as a promising solution for tackling a wide range
of tasks autonomously. Its effectiveness relies on innovations in neural network adaptation, notably the
use of replay buffers Lin (1992) Mnih et al. (2013), which helps decorrelate experiences and facilitate more
effective weight updates. Off-policy algorithms like DDPG Lillicrap et al. (2016), TD3 Fujimoto et al. (2018),
and SAC Haarnoja et al. (2018a) harness these techniques to achieve scalable performance in continuous
control tasks. However, off-policy learning encounters the challenge known as the "deadly triad" Sutton &
Barto (2018) van Hasselt et al. (2018), which can lead to instability when combining bootstrapping and
function approximation. Conversely, on-policy algorithms collect extensive data under the same policy,
providing more reliable statistics and greater stability. However, they remain sample-intensive. Recognizing
this complementary nature of on-policy and off-policy methods, our objective is to design an algorithm that
marries stability with sample efficiency by leveraging the strengths of both approaches.
However, directly applying off-policy techniques to on-policy algorithms can be challenging. In on-policy
algorithms, the traditional use of the state value function limits the scope of policy gradients (PG). This
contrasts with off-policy algorithms that often employ a broader class of policy gradients, such as the deter-
ministic PG Silver et al. (2014) and the reparameterized PG Haarnoja et al. (2018a). These more versatile
PG methods are common in off-policy settings but pose a challenge when adapting to on-policy algorithms.
This incompatibility further renders the on-policy techniques such as GAE Schulman et al. (2016) ineffec-
tive, which along with the value function as a baseline for variance reduction is crucial for performance.
To leverage the advantages of off-policy gradients while retaining the benefits of on-policy methods, we in-
troduce UAE, a unified technique that can accommodate any state-dependent baseline and free the choice
of the bootstrapped value. Notably, UAE encompasses GAE as a strict special case. To fully unleash the
potential of UAE, we also propose a residual baseline related to the optimal baseline Greensmith et al. (2001)
that enhances on-policy gradient estimate. Furthermore, this baseline can be seamlessly integrated into the
off-policy gradient, mitigating instability and expediting the learning process. Unlike the traditional value
baseline, it exhibits higher sample efficiency, as it is trained entirely off-policy.
With the necessary prerequisites and variance reduction mechanisms in place, our high-level algorithmic
design is geared towards fully utilizing off-policy data for both policy evaluation and improvement. In the
context of policy evaluation, algorithms often rely on the fitted Q-iteration (FQI) Ernst et al. (2005) Fan
1Under review as submission to TMLR
ei∼πθoldBatch
Vw→Vπθold
πθold→πθnew
Clear Batch
ei∼πθoldBufferBatch
Qw→Qπold
Clear BatchQw→Qπθold
πθold→πθnew
Figure 1: Training procedure comparison between PPO and DPO (Top: PPO; Bottom: DPO), in which
ei= (si,ai,ri,s′
i), generated by interacting with environment, repeating until it reaches a maximum horizon
T.Batchonly stores data sampled from the current policy, and will be emptied after a training cycle, while
Bufferrefers to the relay buffer. Cyan line indicates data from the source for multiple uses, and Yellow
lineindicates data from the source for individual use. DPO leverages two data sources for both evaluation
and control, whereas PPO exclusively relies on the on-policy data.
et al. (2020). This method employs mean squared error loss with stochastic targets. While the Monte
Carlo estimate provides unbiased estimates, it can be susceptible to increased variance due to trajectory
noise. On the other hand, the use of temporal difference targets, whether deterministic Mnih et al. (2013)
or stochastic Haarnoja et al. (2018a), leverages replayed experiences to make predictions, which is amenable
to the online learning with high sample efficiency but may introduce bias. We aim to harness the strengths
of both approaches to enhance accuracy and generalization. In pursuit of an efficient solution, as illustrated
in Figure 1, we adopt a bi-level approach. During the environment interaction, it iteratively updates the
critic using replayed experiences. Subsequently, it performs batch updates employing the on-policy data.
However, in practice, we have observed that the standard approach, such as FQI, does not perform well
at the first level, primarily due to the challenge of long-horizon prediction. To address this issue, we have
incorporated distributional learning, which offers a more favorable optimization landscape. To improve the
policy, we leverage UAE to estimate the on-policy gradient, which is then interpolated with an optimistic
off-policy objective, promoting sample efficiency, stable learning, and inherent exploration.
In this paper, we introduce a general learning framework called Distillation Policy Optimization (DPO),
which can be readily applied to various on-policy learners, consistently outperforming its on-policy counter-
part and even the state-of-the-art off-policy algorithms on continuous benchmark tasks. Our contributions
can be summarized in three main aspects:
•We extend GAE to UAE, offering greater flexibility with different choices of baseline and critic
functions.
•We propose a sample-efficient baseline, not only yielding a superior on-policy gradient estimator
accompanied with UAE but also effectively facilitating off-policy learning when incorporated into
the off-policy gradient.
•We enhance sample efficiency with full engagement of off-policy data through 1) merged policy eval-
uation with a distributional modeling; 2) interpolated policy gradient with an optimistic objective.
Throughout the paper, we provide comprehensive theoretical insights and empirical results that confirm the
effectiveness of DPO, establishing it as a strong competitor in the field.
2Under review as submission to TMLR
2 Preliminaries
2.1 Notation
We consider an infinite-horizon discounted MDP, which formulates how the agent interacts with the envi-
ronment dynamics. Reinforcement learning aims to solve a sequential problem. Being at the state st∈S,
the agent takes an action at∈ Aaccording to some policy π, which assigns a probability π(at|st)to
the choice. After the environment receives at, it emits a reward rt, and sends the agent to a new state
st+1∼P(st+1|st,at). Following this procedure, we can collect a trajectory τ= (s0,a0,s1,a1,...), where
s0is sampled from the distribution of the initial state ρ0. The ultimate goal of the agent is to maximize
the expected discounted reward η(π) =Eτ/bracketleftbigg∞/summationtext
t=0γtrt/bracketrightbigg
, with a discount factor γ∈[0,1). We also define
the unnormalized discounted state visitation distribution (improper) as ρπ=∞/summationtext
t=0γtP(st=s|ρ0,π), and
dπ(s,a) =ρπ(s)π(a|s), corresponding to the state-action one. Whenever noticed, the policy will be parame-
terized asπθ, sometimes abbreviated as πfor simplicity’s sake. Thus the objective turns out to be η(θ). We
declare that we are using the l2-norm variance of a random vector X, that is, V[X] =E[∥X−E[X]∥2
2].
2.2 Policy Gradient
The policy gradient Sutton et al. (1999) can be expressed as:
∇θη(θ) =Es∼ρπθ,a∼πθ/bracketleftbig
∇θlogπθ(a|s)Qπθ(s,a)/bracketrightbig
. (1)
Inpractice,incorporatingastate-dependentbaselinefunction b(s)cannotonlyreducethevariancedrastically
but also not intervene with the expectation Greensmith et al. (2001). Combine it, we have:
∇θη(θ) =Es∼ρπθ,a∼πθ/bracketleftbig
∇θlogπθ(a|s)(Qπθ(s,a)−b(s))/bracketrightbig
=Es∼ρπθ,a∼πθ/bracketleftbig
∇θlogπθ(a|s)Aπθ,b(s,a)/bracketrightbig
.(2)
2.3 Optimal Baseline
The optimal baseline can be derived by obtaining the fixed point of the variance of Equation 2:
b⋆(s) =Eπθ[uθ(s,a)⊤uθ(s,a)Qπθ(s,a)|s]
Eπθ[uθ(s,a)⊤uθ(s,a)|s], (3)
whereuθ(s,a) =∇θlogπθ(a|s). Its derivation can be found in Appendix A.
However, this baseline is rarely used in practice, because it is extremely demanding for computing the
uθ(st,at)for each time step of the available data.
2.4 Distributional Reinforcement Learning
Distributional reinforcement learning Bellemare et al. (2017) abstracts the appraisal Qπ(s,a)as a distri-
butionZπ(s,a), whose expectation corresponds to the actual value of Q. In this perspective, the Bellman
expectation operator is reloaded as:
TπZ(s,a)D:=r(s,a) +γZ(s′,a′)
s′∼P(·|s,a),a′∼π(·|s′),(4)
where equality is held under probability laws.
3 Unified Advantage Estimator
To combat the noise arising from the long-delayed signals, GAE reduces the temporal spread by letting
b(s) =V(s)andthenshrinksthelong-termeffectwithasteeperparameter λ. LetδV
t=rt+γV(st+1)−V(st),
3Under review as submission to TMLR
we have:
ˆAGAE (γ,λ)
t =∞/summationdisplay
l=0(γλ)lδV
t+l. (5)
One major limitation is that we don’t have more flexible choices except we can stick both of the bootstrapped
valuefunctionandthebaselinefunctiontothevaluefunction V. Thisrestrictedapplicabilitymakesitdifficult
to improve the baseline function for further variance reduction and miss out on a potentially broader class
of policy gradients that rely on the state-action value function. We would therefore aim to relax it for both
parts. Analogously, we can define a new TD residual δt=rt+γΨt+1−b(st), where the bootstrapped value
function Ψt+1can either be Q(st+1,at+1)orV(st+1), andb(s)is an arbitrary state-dependent function. The
core idea is that we can introduce a correction term zt= Ψt−b(st)to make the n-step estimator unbiased
when the true value of Ψtis attained
Proposition 3.1. For anyn∈N+,A(n)
tis an unbiased estimator of Aπ,b
t, where
A(n)
t=δt+n−1/summationdisplay
l=1γl(δt+l−zt+l)
=rt+γrt+1+···+γn−1rt+n−1+γnΨt+n−bt.(6)
Be that as it may, in practice, Ψtis the approximate value and thus the A(n)
tis referenced as ˆA(n)
t. Similarly,
we can introduce a steeper parameter λto shrink the long-term effect, by telescoping on which, we would
arrive at the unified advantage estimator (UAE):
ˆAUAE (γ,λ)
t =δt+∞/summationdisplay
l=1(γλ)l(δt+l−zt+l). (7)
Algorithm 1 Unified Advantage Estimator
Input:γ,λ, Batch size T, rewardsr, Q valuesQ, base-
linesb, donesd
Initialize uae = 0
fort=T−1,T−2,..., 0do
δ=rt+γQ t+1(1−dt+1)−bt
z=Qt−bt
discounted uae =γλ(1−dt+1)uae
At=δ+discounted uae
uae = (δ−z) +discounted uae
end for
return advantages AThe intuition behind this estimator is that we cor-
rect any TD residual term one step beyond tto the
TD errorδt+l−zt+l=rt+l+γΨt+l+1−Ψt+l,l≥1,
and leave the first term δt=rt+γΨt+1−btto
have a potential lower variance dependent on bt. It
is worth noting that if we set Ψt+1=V(st+1)and
b(st) =V(st), then we have GAE exactly. How-
ever, its usefulness extends beyond that, as we have
the flexibility to choose any state-dependent base-
line and extend it to the state-action value function.
When combined with a high-quality baseline, it can
effectively reduce both the instantaneous variance
resulting from sampling from the state and policy
distribution, as well as the variance arising from
sampling a trajectory τ. A truncated version of such
an estimator is summarized in Algorithm 1.
Connection to SARSA( λ)TD(λ) updates the value function towards the λ-returnGλ,V
t, for which a
useful identity is often used to establish the connection between the forward- and backward-view Sutton
(1988) Sutton & Barto (2018):
Gλ,V
t= (1−λ)∞/summationdisplay
n=1λn−1G(n),V
t
=V(st) +∞/summationdisplay
n=0(γλ)nδV
t+n,(8)
whereG(n),V
t =n−1/summationtext
k=0γkrt+k+γnV(st+n)is then-step return.
4Under review as submission to TMLR
ItisevidentthatGAEisavariance-reduced λ-returnwithabaseline V, representedas ˆAGAE (γ,λ)
t =Gλ,V
t−Vt.
Similarly, we can interpret the UAE in the same way – UAE is a variance-reduced λ-return with an arbitrary
baselineb, denoted as ˆAUAE (γ,λ)
t =Gλ,Q
t−bt, since this identity also holds for SARSA( λ) (see Appendix B.3
for derivation).
To fully exploit the potential of UAE, in the next section, our goal is to learn a more adaptable baseline
that can be seamlessly integrated into the off-policy gradient at a later stage.
4 Residual Baseline
The optimal baseline (Equation 10) is essential as it offers the most significant reduction in policy gradient
variance, thereby maximizing performance gains.. However, in practice, obtaining the uθ(s,a)can be com-
putationally demanding, especially when dealing with a large batch of data. Consequently, achieving the
optimal baseline becomes challenging. Moreover, in practical scenarios, the value function used as a baseline
does not accurately capture its true value due to estimation errors Ilyas et al. (2020). We aim to alleviate the
computational overhead and enhance the quality of the baseline. By observing the structure of the optimal
baseline, we define:
˜π(a|s) =π(a|s)u⊤
θuθ
Eπ[u⊤
θuθ|s], l (s,a) =u⊤
θuθ
Eπ[u⊤
θuθ|s]. (9)
Then we can rewrite the optimal baseline as:
b⋆(s) =E˜π[Qπ(s,a)|s]. (10)
Using the importance sampling, we have:
b⋆(s) =Eπ[˜π(a|s)
π(a|s)Qπ(s,a)|s]
=Eπ[l(s,a)Qπ(s,a)|s].(11)
We would directly parameterize the l(s,a), or, alternatively, introduce a “residual” term rϕ(s,a)to reformu-
late thel(s,a)as1 + rϕ(s,a), since Eπ[l(s,a)] = 1. This transformation would induce a symmetric behavior
forrϕ(s,a), which is favored by the neural network. In the hope that Qwis a good approximation to Qπ, it
translates to approximate the Equation 11 as:
bπ
ϕ(s) =Eπ[(1 + rϕ(s,a))Qw(s,a)|s]. (12)
In practice, we can sample mactionsa1,a2,...,amfrom theπ(·|s)to approximate the outer expectation.
We then construct a magnitude-free objective to represent the amount of the variance associated with the
approximate baseline, which is more tractable and easier to optimize Gu et al. (2017b) Mnih & Gregor
(2014):
J(ϕ) =Edβ[(Qw(s,a)−bπ
ϕ(s))2], (13)
wheredβis a mixture of the joint distributions of the past policy sequences. This objective has a wider
coverage of past experiences than solely relying on the on-policy data, which is critical to reducing the
variance of both on- and off-policy gradient. It is this reason that allows us to make the most of the
advantages of the two kinds.
5 Practical Algorithm
In this section, we propose a sample-efficient algorithm that combines on- and off-policy data, leveraging
the variance reduction techniques discussed earlier. Our approach begins with policy evaluation, updating
5Under review as submission to TMLR
the critic not only during the environment interaction using replayed experiences but also through batch
updates. Then we interpolate the on-policy gradient with an optimistic objective that incorporates the
residual baseline for stable and efficient learning.
5.1 Policy Evaluation: Distributional Regression
To being with, we delve into the details of policy evaluation, which employs a bi-level optimization, merging
bothon-andoff-policyupdates. Itintegrateswithdistributionallearning1—astrategicresponsetochallenges
observed, notably the long-horizon prediction issue encountered with standard approaches like FQI.
The distributional perspective enriches predictions by incorporating input-dependent variance to accommo-
date uncertainty. It can penalize high-noise input regions and enhance representation learning Shahriari
et al. (2022). Leveraging this representation, we develop an efficient update scheme based on the analytical
Gaussian modeling, conducted during the environment interaction using the off-policy data. This scheme,
akin to the FQI used in TD3 or SAC, shares the same time complexity but does not require excessive samples
from the distributional critic (see Appendix D.3). The general update rule can be expressed as:
J(w) =E(s,a,r,s′)∼D[DKL(r+γZ¯w(s′,a′)||Zw(s,a))], (14)
wherea′∼π(·|s′), andZ¯wis the target network, commonly used in the off-policy learning to stabilize the
neural network.
Once the rollouts are collected, we sample multiple instances from the distributional critic to create a vector# »Zof lengthlfor each (s,a)in the batchB. We then compute a collection of advantages by UAE as# »Aand
replenish the baseline to construct a target vector# »U=# »A+1·b, where 1is an all-one vector of length l.
As the entropy term in the KL divergence does not provide gradient information, we minimize the empirical
cross-entropy as follows:
ˆJ(w) =E(s,a)∼B[−1
l1⊤logPw(# »U|s,a)]. (15)
This approach effectively harnesses the benefits of both off-policy learning’s high sample efficiency and on-
policy learning’s informative target estimates. Additionally, the estimated advantage is a key component of
the on-policy gradient, as discussed later.
5.2 Policy Improvement: Advantageous Interpolation
We aim to integrate the off-policy policy gradient with the on-policy policy gradient to enable faster learning,
boost sample efficiency, and encourage exploration. One solution for this integration involves introducing an
interpolating parameter w∈[0,1]to directly adjust both gradients, as suggested by IPG Gu et al. (2017a):
ωEρπθ,πθ[Aπθ,V] + (1−ω)Eρβ,πθ[Qw]. (16)
While approximating a value function Vfor advantage calculation, it also maintains an off-policy critic Qw.
This separate estimation can pose a problem as it leads to varying magnitudes between the two types of
policy gradients. Even when working with off-policy data, the estimation of Qwcan still be prone to errors
and noise. Therefore, we incorporate the residual baseline to mitigate the noise and only update actions
that provide an advantage. This approach can be efficiently optimized with trust region methods:
J(θ) =ωEρπθold,πθ[Aπθold,bπθold
ϕ] + (1−ω)Eρβ,πθ[A+−αlogπθ(a|s)], (17)
whereA+= (Qw−bπθ
ϕ)+is the positive advantage, of which (x)+stands for max(x,0). To enhance
exploration and prevent premature convergence, we include an entropy bonus controlled by parameter α
Mnih et al. (2016). This bonus improves the exploration of the off-policy data. While our off-policy gradient
resembles the likelihood ratio gradient estimator of SAC, the purpose of the entropy term in our approach
differs. It is not considered as part of the task-specific reward for the agent.
1The procedure is generally applicable to other alternatives in place of the distributional loss.
6Under review as submission to TMLR
Algorithm 2 Distillation Policy Optimization
Initialize parameters w,¯w,θ,ϕ
foreach iteration do
foreach environment step do
Executeat∼π(·|st), observe reward rtand next state st+1
Store transition (st,at,rt,st+1)to the replay buffer Dand the batchB
Sample mini-batch of ntransitions (s,a,r,s′)fromD
Update critic with ∇wJ(w)(Equation 14)
Update target network ¯w←τw+ (1−τ) ¯w
end for
foreach baseline update do
Sample mini-batch of ntransitions (s,a,r,s′)fromD
Update baseline with ∇ϕJ(ϕ)(Equation 13)
end for
Calculate ˆAforBby Algorithm 1
foreach epoch do
foreach policy update do
Sample mini-batch from B, and computeJon-policy (θ)
Sample mini-batch of ntransitions (s,a,r,s′)fromD, and computeJoff-policy (θ)
Update policy with ∇θJ(θ)(Equation 17)
Update critic with ∇wˆJ(w)(Equation 15)
Update target network ¯w←τw+ (1−τ) ¯w
end for
end for
Reset the batchB
end for
Without the cancellation of the negative part, when an action is perceived as unfavorable, it steers away
from that choice and increases the likelihood of exploring unknown actions. This introduces a risk of
making completely wrong decisions. However, the process we employ, which eliminates negative aspects and
emphasizes positive signals through the residual baseline, prevents detrimental updates and ensures that the
movement direction is always advantageous.
6 Theoretical Analysis
In this section, we present a theoretical analysis of the proposed methods. We explore three key questions:
(1)How does UAE outperform GAE, and what’s their relationship? (2)Can the residual baseline effectively
minimize variance in the off-policy gradient? (3)What advantages does our interpolated policy gradient
offer? These questions form the foundation of our theoretical investigation.
Assumption 6.1. sups,a|Qw|is bounded by some constant M.
Assumption 6.2. sups,a∥∇ϕrϕ∥is bounded by some constant G.
Assumption 6.3. sups,a|rϕ|is bounded by some constant K.
We begin by examining the relationship between UAE and GAE in terms of the variance of the on-policy
gradient.
Theorem 6.4. For any choice of Ψbeing either QπorVπ, andbstate-dependent, then
Vst,at[uθAUAE (γ,λ)
t ]−Vst,at[uθAGAE (γ,λ)
t ] =irreducible/bracehtipdownleft /bracehtipupright/bracehtipupleft /bracehtipdownright
Est,at/bracketleftig
u⊤
θuθ/parenleftbigg∞/summationdisplay
l=0(γλ)2l·γ2(1−λ2)Est+l+1,at+l+1/bracketleftbig
(Ψ−Vπ)2/bracketrightbig/parenrightbigg/bracketrightig
+
Est,at[u⊤
θuθ(b2−Vπ2−2Qπ(b−Vπ))]/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
reducible.
(18)
Notably, if Ψ =Qπ, an irreducible error arises from the additional action in Qπ(s,a). In the case of episodic
tasks, it provides insights that bootstrapping from Qπintroduces additional complexity when λis low, but
it becomes eliminable when full return is achieved. With Ψ =Vπ, the problem simplifies, allowing for a
direct comparison.
7Under review as submission to TMLR
Corollary 6.5. IfΨ =Vπ, for any baseline b(s)that reduces variance no less than Vπ(s), then
Vst,at[uθAUAE (γ,λ)
t ]≤Vst,at[uθAGAE (γ,λ)
t ]. (19)
UAE’s advantage over GAE is its capacity to broaden baseline options, notably enhancing variance reduction
withimprovedbaselinesbeyond Vπ. ThisunderscoresthecrucialroleofbaselineselectioninunlockingUAE’s
full potential.
Wewillnowdelveintotheunderstandingoftheresidualbaselineconcerningdataexposureanditsapplication
for the off-policy gradient.
Theorem 6.6. Under Assumption 6.2, at nth iteration, let k= min{n,/floorleftig
|D|
T/floorrightig
}, for the current policy πn,
along with its predecessors {πn−i}k−1
i=1, if for any s∈S,supiDTV(dn−i(s,a)
dβ(s)||π)<ϵ
4andEπ[|rϕ⋆(s,a)|]<ϵ
2,
then∥∇ϕJ(ϕ⋆)∥<2GM2ϵ.
This suggests that optimizing the residual baseline may become increasingly challenging as the volume of
its training data grows. Therefore, in practice, selecting an appropriate size for training baseline is of vital
importance.
Corollary 6.7. Under Assumption 6.3, for any successor policy ˜πofπ, ifsupsDKL(˜π||π)<ϵ
2, then
Edβ[(Qw(s,a)−b˜π
ϕ⋆(s))2]≤2J(ϕ⋆) + 2/parenleftbig
(K+ 1)M/parenrightbig2ϵ. (20)
Following the optimization of the residual baseline, its integration with trust region methods ensures effective
maintenance of low magnitude-free variance in evolving policies. This, in turn, promotes stable parameter
adjustments, ultimately leading to a smoother learning process.
Regarding our interpolated policy gradient, its advantage lies in two folds: self-annealing effect and potential
for a tighter lower bound.
Theorem 6.8. (Self-annealing effect) Under Assumption 6.3, for any policy sequence {πk}such that its
limiting point π⋆lies in the deterministic optimal policy set, if for any s∈S,limk→∞Eπk[rϕk] = 0, then
lim sup
k→∞A+
k= 0. (21)
It indicates that as the positive advantage diminishes, the surrogate reduces to encourage exploration only.
This self-annealing effect is helpful since as the learning evolves, the direction of the policy update will close
to the on-policy gradient, which is generally stabler.
Theorem 6.9. (Bounded bias) Let ∆ = max s,a|Qπ−Qw|,Ω = max s|E˜π[Qw−b˜π
ϕ]|,Υ =
maxs|E˜π[Qπ−bπ
ϕ]|, and define
Lπ(˜π) =η(π) +ωEρπ,˜π[Qπ−bπ
ϕ] + (1−ω)Eρβ,˜π[(Qw−b˜π
ϕ)+−αlog ˜π], (22)
then
|η(˜π)−Lπ(˜π)|≤2γΥ
(1−γ)2/radicalig
Dmax
KL(π||˜π)+(1−ω)(∆+C1/radicalig
2Dmax
KL(π||˜π)+2γΩ
(1−γ)2/radicalig
Dmax
KL(π||β)+C2).(23)
This theorem provides a comprehensive bound on the bias introduced by the on-policy state distribution
mismatch and the off-policy learning, from Lπ(˜π)to the true objective η(˜π). The accuracy of this approx-
imation depends on the deviation from the original policy π, the approximation quality of QwtoQπ, and
the extent of off-policyness. Our combined policy evaluation greatly reduces the ∆gap. In the case where
r≡0, it further eliminates bias from off-policy learning as Ω = 0for any ˜π. IPG, on the other hand, fits
Qwusing only off-policy data and struggles to manage off-policyness as their Ωis policy-dependent. While
the residual term is typically non-zero, it remains relatively small (see Appendix I.4, Figure 10(a)), reducing
off-policyness. When combined with any on-policy gradient with an enforced trust region, it can effectively
8Under review as submission to TMLR
Figure 2: Learning curves on continuous control tasks, averaged over 5 random seeds and shaded with
standard error.
utilize recent rollouts while constraining the deviation from the origin. The shifting constant C2can also be
controlled, depending on the desired level of exploration and the portion of the negative off-policy gradient
to be canceled2.
7 Experiments
Our goal is to validate sample efficiency and stable learning while understanding the contributions of different
algorithmic components. We perform our algorithm on several continuous control tasks from the OpenAI
Gym Brockman et al. (2016) with the MuJoCo simulator Todorov et al. (2012).
Evaluation Since our algorithm is in a hybrid fashion, the policy that we update would not strictly follow
the sampling policy. We thus evaluate our algorithm by executing the mean action with 10 trails, for which
we report the averaged episodic reward every 4096 steps. We run each task with 5 random seeds, whose
total environment step is 1 million.
As our default on-policy learner is PPO, it is a direct baseline to verify whether our method realizes an
improvement. And we test the advantage of the unified learning of the critic and the optimistic policy
gradient against IPG. We also made comparisons with the state-of-the-art off-policy algorithms, such as
SAC Haarnoja et al. (2018a) and TD3 Fujimoto et al. (2018). We defer additional comparisons to related
baselines that combine on-policy methods with off-policy data to Appendix I.2.
The learning curves are presented in the Figure 2. DPO demonstrates superior or comparable performance
across all tasks, notably excelling in the high-dimensional Humanoid task. Other DPO variants, such as
DPO(A2C) and DPO(TRPO), exhibit significant improvements over their on-policy counterparts (Table
1). This highlights the potential of our method as a promising learning paradigm for a range of on-policy
algorithms.
2Although we mainly focus on the negative portion, the bound is generally applicable to any removed portion.
9Under review as submission to TMLR
Sample Efficiency DPO combines both on- and off-policy evaluation and employs off-policy gradient
interpolationtoenhancesampleefficiency. AsdepictedinFigure3(a), DPOachievesexceptionalperformance
more rapidly and with significantly less time compared to off-policy algorithms. To achieve comparable
performance, on-policy algorithms like PPO necessitate 10 times more samples than DPO. This underscores
DPO’s enhanced sample efficiency over on-policy algorithms and improved time efficiency compared to off-
policy algorithms. Additionally, DPO only performs 4%of the total number of policy gradients, compared
to off-policy algorithms like TD3 and SAC, highlighting DPO’s superior data utilization per update.
(a) Time Complexity
 (b) Relative Policy Updates
Figure 3: Comparison of time complexity and relative parameter updates.
Stable Learning DPO preserves the stable learning characteristics typically associated with on-policy
algorithms. We assess stability by measuring the variability in policy changes and critic mean squared error
(MSE) loss. These metrics are computed using the variance of parameter updates and the average total
variation respectively (see definitions in the Appendix F.3). Both metrics are calculated based on 25%of
the data within fixed intervals. To ensure a fair comparison, we normalize the losses before calculating the
latter metric, as loss functions can vary in magnitude for different algorithms. Figure 4(a) shows that DPO
maintains smoother policy changes compared to SAC, even without the use of learning rate scheduling as
seen in PPO. Furthermore, Figure 4(b) highlights that DPO exhibits reduced variability in critic MSE loss,
suggesting the effectiveness of the combined policy evaluation within a more stable optimization landscape.
(a) Variance Of Policy Updates
 (b) Average Total Variation
Figure 4: Comparison of variance of policy updates and average total variation of critic MSE loss.
General Framework One of our key innovations is the provision of the implementation for a versatile
learning framework that can adapt to various on-policy gradient estimators, including A2C, TRPO, and
10Under review as submission to TMLR
(a) Trajectory variance
 (b) Variance of both on- and off-policy
gradient
(c) Varying training samples for baseline
learning
Figure 5: Variance reduction of UAE and residual baseline, and data exposure of baseline learning.
PPO (details can be found in Appendix F.8). Within this framework, these estimators share the same set of
hyperparameters across different tasks, except for any algorithmic specifications. The remarkable improve-
ment in sample efficiency is evident in Table 1 and Figure 2, enabling on-policy algorithms to successfully
tackle previously unsolvable tasks. It also narrows the performance gap with off-policy approaches, while
increasing time efficiency.
Table 1: Head-to-head comparison between other variants of DPO to its counterpart.
Method Walker2d Hopper Swimmer Ant Humanoid Avg.
A2C 134 ±45 156 ±30 17 ±4 942 ±3 163 ±50 282 ±26
DPO(A2C) 2786 ±681 2108 ±712 40±53581 ±905 3980 ±2231 2499 ±907
TRPO 2449 ±251 2142 ±591 103±21 68 ±50 503 ±23 853 ±187
DPO(TRPO) 3581 ±516 2025 ±1120 55 ±24 4615 ±129 5011 ±1197 3057 ±597
Variance Reduction We explore two methods for variance reduction: UAE and a residual baseline. We
investigate three key questions: (1)Does UAE effectively mitigate long-term noise? (2)Does the residual
baseline reduce both on-policy and off-policy gradient variance? (3)How does the baseline’s data exposure
impact its performance?
For the first question, we evaluate the variance of the policy gradient equipped with UAE using the law
of total variance, as expressed in Equation 24. Our focus is primarily on Στ, which arises from trajectory
sampling. We investigate how varying the value of λallows us to reduce the temporal spread and mitigate
the trajectory noise, as depicted in Figure 5(a).
Vs,a[uθAUAE (γ,λ)] =Es,a[Vτ|s,a[uθAUAE (γ,λ)]] +Vs,a[Eτ|s,a[uθAUAE (γ,λ)]]
= Στ+ Σs,a.(24)
Regarding the second question, we compare our residual baseline bπ
ϕ(s)with an approximate value baseline
ˆV(s), the sample mean of Qw(s,ai)(withai∼π), and a zero baseline. The results demonstrate the
significance of incorporating a baseline. The residual baseline reduces off-policy variance more effectively
while maintaining a similar reduction in on-policy variance (see Figure 5(b)).
To address the third question, we examine how the baseline’s exposure to data impacts its effectiveness.
Gradually increasing the number of segments reveals that training the baseline with more data enhances its
ability to stabilize the learning process (see Figure 5(c)).
Distributional Critic DPO proactively performs off-policy evaluation steps before the policy improve-
ment starts. In such a long-horizon scenario (as seen in Figure 1), it requires preventing overfitting on the
finite dataset and generalizing well on the unseen data. We empirically test this ability on our method along
11Under review as submission to TMLR
with other predominant evaluation methods, such as MSE with single Q Ernst et al. (2005) Fan et al. (2020),
and MSE with double Q Fujimoto et al. (2018). The results in Figure 6 demonstrate that the distributional
critic in DPO excels in both online prediction and training generalization.
(a) Prediction Error
 (b) Generalization Error
Figure 6: Left: root mean squared error (RMSE) be-
tween E[r+γZw(s′,a′)]andE[Zw(s,a)]on the on-
policy rollouts; Right: mean absolute error (MAE)
between the true value Qπand the fitted value Qwon
the test data.Table 2: Ablation study on key components, pol-
icy gradient, and data usage.
Method Walker2d Hopper Ant
no-UAE 3710 ±562 2436 ±1282 1747 ±227
no-RB 4128 ±807 1090 ±213 3866 ±1251
no-DIST 4408 ±388 2988 ±239 3961 ±921
no-INT 3341 ±1718 2800 ±1030 3560 ±1775
no-ENT 4194 ±790 2830 ±910 4095 ±1175
only-ON 4174 ±980 2601 ±1107 4053 ±814
only-OFF 3743 ±1186 3223 ±329 3857 ±1083
DPO 4860±680 3187 ±351 5278±173
Ablation Study We analyze the contributions of UAE, the residual baseline, and distributional learning
to the performance improvement achieved by DPO. The results are presented in the first group of Table 2.
Removing these components leads to varying degrees of performance decrease, underscoring the effectiveness
of each component. In the second group, when we eliminate interpolation, both entropy regularization
and positive advantage are removed, resulting in inferior performance. Retaining the positive advantage but
removing the entropy regularization leads to improved results but still falls short of comparable performance.
The results in the third group demonstrate that combining on- and off-policy evaluation is essential for
realizing the full benefits of distributional learning. These findings highlight how DPO enhances sample
efficiency, promotes exploration, and enables stable learning.
8 Related Work
Variance reduction techniques State-dependent baseline has been widely studied in Greensmith et al.
(2001)Weaver&Tao(2001),whoseapplicationcanbefoundinPeters&Schaal(2008). Althoughtheoptimal
baselinesoundsfromatheoreticalperspective, itspracticaluseisrare. Theconventionalizedalternative–the
value function that can be estimated directly from the interaction, prevails in on-policy algorithms, such as
REINFORCE Williams (1992) and A2C Sutton & Barto (2018). However, it is often brittle to the quality of
the function approximation Ilyas et al. (2020) and has a gap between the optimal one. There are also fruitful
works that focus on the action-dependent baseline, with stein identity Liu et al. (2018), or factorization
Wu et al. (2018). But Tucker et al. (2018) points out that the gain of the action-dependent baseline is
often insignificant due to the function approximation and overweighed by other variance components such
as the trajectory variance. GAE Schulman et al. (2016) accounts for this by exponentially interpolating
different advantage estimators to reduce the temporal spread, while introducing some bias. In Monte Carlo
theory, the baseline methods are different kinds of control variate. Beyond RL, to reduce variance, gradient-
based optimization can combine both control variate and the reparameterization estimator Grathwohl et al.
(2018), and inference problems Mnih & Gregor (2014) Paisley et al. (2012) utilize the control variate for
score function estimator.
Distributional learning The distributional perspective in reinforcement learning (RL) extends scalar
value functions to distributions, a concept first systematically explored by Bellemare et al. (2017). In line
with this approach, QR-DQN Dabney et al. (2018b) and IQN Dabney et al. (2018a) have been developed for
discrete control tasks. For continuous control, D4PG Barth-Maron et al. (2018), which builds upon DDPG
12Under review as submission to TMLR
and incorporates a distributional critic, has achieved state-of-the-art performance. In D4PG, various dis-
tributional forms, such as categorical and mixtures of Gaussians, were explored, with the latter minimizing
empirical cross-entropy. Shahriari et al. (2022) conducted empirical investigations into the effects of Ma-
halanobis reweighting and representation learning in a Gaussian critic, concluding that both are beneficial.
While our work primarily focuses on the mean value of the value distribution, its analytical and compu-
tationally efficient nature makes it applicable to a wide range of settings, distinguishing it from previous
methods.
Policy gradient interpolation Q-prop Gu et al. (2017b) employs an additional off-policy critic, which
serves two purposes: 1) reducing variance in on-policy gradient through control variate; and 2) combining
DDPG-style policy updates with on-policy gradient solely on on-policy data. To fully exploit two sources
of data, IPG Gu et al. (2017a) interpolates an on-policy gradient with an off-policy gradient Degris et al.
(2012), butseparatelyapproximatesavaluefunctionandastate-actionvaluecritic, whichrisksaccumulating
the compounding error. In practice, it is hard to determine the interpolating parameter. P3O Fakoor
et al. (2019) adaptively adjusts the hyperparameter and uses KL divergence to control the off-policyness.
PGQL O’Donoghue et al. (2017) combines an entropy-regularized policy gradient with a Q-learning style
policy gradient. ACER Wang et al. (2017) enhances sample efficiency through off-policy updates, and the
techniques they use, including Retrace Munos et al. (2016), bias-corrected truncated PG with a trust region
approach, are complementary to our methods for efficient policy evaluation and stable policy optimization.
Positive advantage Though the topic is not widely studied, it serves straightforward purposes – selecting
advantageous actions or avoiding bad updates. Tessler et al. (2019) fits an autoregressive actor network to
the action that has a positive advantage. van Hasselt (2012) remains the actor unchanged if the TD error is
negative to avoid bad updates.
9 Conclusion
In this paper, we proposed a novel learning framework that incorporates several key components. Our
algorithm is the first to seamlessly integrate on-policy algorithms with off-policy data, striking a balance
betweenstablelearningandsampleefficiency. Weprovidetheoreticalinsightsandexperimentaljustifications,
offering a comprehensive understanding of each algorithmic component. This kind of mixture would be
inspiring for future algorithm design.
References
Gabriel Barth-Maron, Matthew W. Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva TB, Alistair
Muldal, Nicolas Heess, and Timothy P. Lillicrap. Distributed distributional deterministic policy gradients.
In6th International Conference on Learning Representations , 2018.
Marc G. Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement learning.
InProceedings of the 34th International Conference on Machine Learning , pp. 449–458, 2017.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. Openai gym. CoRR, 2016.
Po-Wei Chou, Daniel Maturana, and Sebastian A. Scherer. Improving stochastic policy gradients in con-
tinuous control with deep reinforcement learning using the beta distribution. In Proceedings of the 34th
International Conference on Machine Learning , pp. 834–843, 2017.
Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit quantile networks for distributional
reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning , pp.
1104–1113, 2018a.
Will Dabney, Mark Rowland, Marc G. Bellemare, and Rémi Munos. Distributional reinforcement learning
with quantile regression. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence ,
pp. 2892–2901, 2018b.
13Under review as submission to TMLR
Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. arXiv preprint
arXiv:1205.4839 , 2012.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John
Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines, 2017.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal
of Machine Learning Research , 2005.
Rasool Fakoor, Pratik Chaudhari, and Alexander J. Smola. P3O: policy-on policy-off policy optimization. In
Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence , pp. 1017–1027, 2019.
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-learning. In
Proceedings of the 2nd Annual Conference on Learning for Dynamics and Control , pp. 486–489, 2020.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic
methods. In Proceedings of the 35th International Conference on Machine Learning , pp. 1582–1591, 2018.
Scott Fujimoto, David Meger, Doina Precup, Ofir Nachum, and Shixiang Shane Gu. Why should I trust
you, bellman? the bellman error is a poor replacement for value error. In International Conference on
Machine Learning , pp. 6918–6943, 2022.
Yasuhiro Fujita and Shin-ichi Maeda. Clipped action policy gradient. In Proceedings of the 35th International
Conference on Machine Learning , pp. 1592–1601, 2018.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, and David Duvenaud. Backpropagation through
the void: Optimizing control variates for black-box gradient estimation. In 6th International Conference
on Learning Representations , 2018.
Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient
estimates in reinforcement learning. In Advances in Neural Information Processing Systems , pp. 1507–
1514, 2001.
ShixiangGu, TimLillicrap, RichardE.Turner, ZoubinGhahramani, BernhardSchölkopf, andSergeyLevine.
Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement
learning. In Advances in Neural Information Processing Systems , pp. 3846–3855, 2017a.
Shixiang Gu, Timothy P. Lillicrap, Zoubin Ghahramani, Richard E. Turner, and Sergey Levine. Q-prop:
Sample-efficient policy gradient with an off-policy critic. In 5th International Conference on Learning
Representations , 2017b.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International
Conference on Machine Learning , pp. 1856–1865, 2018a.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Ku-
mar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algorithms and
applications. CoRR, 2018b.
Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and
Aleksander Madry. A closer look at deep policy gradients. In 8th International Conference on Learning
Representations , 2020.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based
policy optimization. In Advances in Neural Information Processing Systems , pp. 12498–12509, 2019.
Gregory Kahn, Tianhao Zhang, Sergey Levine, and Pieter Abbeel. PLATO: policy learning using adaptive
trajectory optimization. In International Conference on Robotics and Automation , pp. 3342–3349, 2017.
14Under review as submission to TMLR
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International
Conference on Learning Representations , 2015.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In 4th International
Conference on Learning Representations , 2016.
Long-Ji Lin. Reinforcement learning for robots using neural networks . Carnegie Mellon University, 1992.
Hao Liu, Yihao Feng, Yi Mao, Dengyong Zhou, Jian Peng, and Qiang Liu. Action-dependent control variates
for policy optimization via stein identity. In 6th International Conference on Learning Representations ,
2018.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In Proceedings
of the 31th International Conference on Machine Learning , pp. 1791–1799, 2014.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, 2013.
Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Pro-
ceedings of the 33nd International Conference on Machine Learning , pp. 1928–1937, 2016.
Rémi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and efficient off-policy
reinforcement learning. In Advances in Neural Information Processing Systems , pp. 1046–1054, 2016.
Brendan O’Donoghue, Rémi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. Combining policy gradient
and q-learning. In 5th International Conference on Learning Representations , 2017.
John W. Paisley, David M. Blei, and Michael I. Jordan. Variational bayesian inference with stochastic search.
InProceedings of the 29th International Conference on Machine Learning , 2012.
Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural Networks ,
pp. 682–697, 2008.
Stéphane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In Proceedings of the Fourteenth International Conference on
Artificial Intelligence and Statistics , pp. 627–635, 2011.
John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region policy
optimization. In Proceedings of the 32nd International Conference on Machine Learning , pp. 1889–1897,
2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. In 4th International Conference on Learning
Representations , 2016.
Bobak Shahriari, Abbas Abdolmaleki, Arunkumar Byravan, Abe Friesen, Siqi Liu, Jost Tobias Springenberg,
Nicolas Heess, Matt Hoffman, and Martin A. Riedmiller. Revisiting gaussian mixture critics in off-policy
reinforcement learning: a sample-based approach. CoRR, 2022.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin A. Riedmiller. De-
terministic policy gradient algorithms. In Proceedings of the 31th International Conference on Machine
Learning , pp. 387–395, 2014.
Richard S. Sutton. Learning to predict by the methods of temporal differences. Mach. Learn. , pp. 9–44,
1988.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction . The MIT Press, second
edition, 2018.
15Under review as submission to TMLR
Richard S. Sutton, David A. McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Advances in Neural Information Processing
Systems, pp. 1057–1063, 1999.
Chen Tessler, Guy Tennenholtz, and Shie Mannor. Distributional policy optimization: An alternative ap-
proach for continuous control. In Advances in Neural Information Processing Systems , pp. 1350–1360,
2019.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In
International Conference on Intelligent Robots and Systems , pp. 5026–5033, 2012.
GeorgeTucker, SuryaBhupatiraju, ShixiangGu, RichardE.Turner, ZoubinGhahramani, andSergeyLevine.
The mirage of action-dependent baselines in reinforcement learning. In Proceedings of the 35th Interna-
tional Conference on Machine Learning , pp. 5022–5031, 2018.
Hado van Hasselt. Reinforcement learning in continuous state and action spaces. In Reinforcement Learning ,
pp. 207–251. 2012.
Hado van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil. Deep
reinforcement learning and the deadly triad. CoRR, 2018.
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Rémi Munos, Koray Kavukcuoglu, and Nando
deFreitas. Sampleefficientactor-criticwithexperiencereplay. In 5th International Conference on Learning
Representations , 2017.
Lex Weaver and Nigel Tao. The optimal reward baseline for gradient-based reinforcement learning. In
Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence , pp. 538–545, 2001.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Mach. Learn. , pp. 229–256, 1992.
Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M. Bayen, Sham M. Kakade, Igor
Mordatch, and Pieter Abbeel. Variance reduction for policy gradient with action-dependent factorized
baselines. In 6th International Conference on Learning Representations , 2018.
A Optimal Baseline
We define gradient components as follows:
g=uθ(s,a)(Qπ(s,a)−b(s))
g1=uθ(s,a)Qπ(s,a)
g2=uθ(s,a)b(s)(25)
Note thatg=g1−g2, therefore,
V[g] =Eρπ,π[(g−Eρπ,π[g])T(g−Eρπ,π[g])]
=Eρπ,π/bracketleftig/parenleftbig
(g1−Eρπ,π[g1])−(g2−Eρπ,π[g2])/parenrightbigT/parenleftbig
(g1−Eρπ,π[g1])−(g2−Eρπ,π[g2])/parenrightbig/bracketrightig
=V[g1] +V[g2]−2Eρπ,π[(g1−Eρπ,π[g1])T(g2− :0Eρπ,π[g2])]
=V[g1] +V[g2]−2Eρπ,π[gT
1g2]−2
 :0
(Eρπ,π[g1])TEρπ,π[g2]
=V[g1] +V[g2]−2Eρπ,π[gT
1g2](26)
16Under review as submission to TMLR
Given a state s, we can omit the expectation over ρπ, which is taken on the state space. It turns out to be:
V[g|s] =V[g1|s] +V[g2|s]−2Eπ[gT
1g2|s]
=Eπ[uθ(s,a)Tuθ(s,a)Qπ(s,a)2|s]−
(Eπ[uθ(s,a)Qπ(s,a)|s])2+
Eπ[uθ(s,a)Tuθ(s,a)b(s)2|s]−
2Eπ[uθ(s,a)Tuθ(s,a)Qπ(s,a)b(s)|s](27)
In an attempt to minimize this variance, using the fact that V[g1|s]doesn’t dependent on b(s), we can
differentiate it w.r.t. b, immediately, we get:
b⋆(s) =Eπ[uθ(s,a)Tuθ(s,a)Qπ(s,a)|s]
Eπ[uθ(s,a)Tuθ(s,a)|s](28)
Define
˜π(a|s) =π(a|s)u⊤
θuθ
Eπ[u⊤
θuθ|s]l(s,a) =u⊤
θuθ
Eπ[u⊤
θuθ|s](29)
then
b⋆(s) =E˜π[Qπ(s,a)|s] (30)
Control Variate We are interested in computing Ep(x)[f(x)]. However, it may have a high variance. If
we introduce another quantity g(x)with a known expectation Ep(x)[g(x)]or whose approximation can be
easy. Then we can let f′=f−a(g−E[g]), which will have a same expectation as Ep(x)[f(x)].
We can write out the variance of the new quantity as
V(f′) =V(f)−2acov(f,g) +a2V(g) (31)
Appropriately adjusting the scalar a, we can achieve a lower variance but not change the expectation. It
can be analytically solved by minimizing the above quantity w.r.t. a
a⋆=cov(f,g)
V(g)(32)
The reduction in ratio can be expressed as
V(f′)
V(f)= 1−corr(f,g) (33)
The greater correlation between fandgis, the greater variance reduction would attain.
In terms of policy gradient, we can consider f=uθQπandg=uθb, where b is a state-dependent baseline.
Since E[uθb] = 0, the new estimator will be f′=f−a(uθb−0) =uθ(Qπ−ab), whose optimal value of ais
a⋆=cov(uθQπ,uθb)
V(uθb)=E[u⊤
θuθQπb]
E[u⊤
θuθb2]=E[u⊤
θuθQπ]
E[u⊤
θuθb]·1
b(34)
Thenb⋆=a⋆b=E[u⊤
θuθQπ]
E[u⊤
θuθb], as what optimal baseline is.
B Unified Advantage Estimator
B.1 Proof of Proposition 3.1
For anyn∈N+, we telescope over residual terms
n−1/summationdisplay
l=0γlδt+l=/parenleftbiggn−1/summationdisplay
l=0γlrt+l+γnΨt+n−bt/parenrightbigg
+n−1/summationdisplay
l=1γl(Ψt+l−bt+l) (35)
17Under review as submission to TMLR
Since the Ψis the true quantity we care about, being either QπorVπ, thus the Bellman expectation equation
is naturally agreed. Denote zt= Ψt−bt, and move the ztterms from the lefthand side to the righthand size,
by taking the expectation of the both sides, it can be shown that
Eπ[A(n)
t] =Eπ/bracketleftbiggn−1/summationdisplay
l=0γlrt+l+γnΨt+n−bt/bracketrightbigg
= Ψt−bt
=Aπ,b
t(36)
B.2 Proof of Equation 5
ˆAUAE (γ,λ)
t = (1−λ)(ˆA(1)
t+λˆA(2)
t+λ2ˆA(3)
t+...)
= (1−λ)/parenleftbig
δt+λ(δt+γδt+1−γzt+1) +λ2(δt+γδt+1−γzt+1+γ2δt+2−γ2zt+2) +.../parenrightbig
= (1−λ)/parenleftig/parenleftbig
δt(1 +λ+...) +γδt+1(λ+λ2+...) +.../parenrightbig
−/parenleftbig
γzt+1(λ+λ2+...) +γ2zt+2(λ2+λ3+...) +.../parenrightbig/parenrightig
= (1−λ)/parenleftig/parenleftbigδt
1−λ+ (γλ)δt+1
1−λ+.../parenrightbig
−/parenleftbig
(γλ)zt+1
1−λ+ (γλ)2zt+2
1−λ+.../parenrightbig/parenrightig
=∞/summationdisplay
l=0(γλ)lδt+l−∞/summationdisplay
l=1(γλ)lzt+l
=δt+∞/summationdisplay
l=1(γλ)l(δt+l−zt+l)(37)
B.3 Connection between UAE and SARSA( λ)
We will first show the identity also holds for SARSA( λ), and then connects it with UAE.
Gλ,Q
t= (1−λ)∞/summationdisplay
n=1G(n),Q
t
= (1−λ)(rt+γQt+1)+
(1−λ)(rt+γrt+1+γ2Qt+2)+
...(38)
By iteratively merging every same reward term of each expansion, we have
Gλ,Q
t=rt+ (1−λ)γλ0Qt+1
+ (1−λ)γλ1(rt+1+γQt+2)
+ (1−λ)γλ2(rt+1+γrt+2+γ2Qt+3)
+...
=rt+γλ0(1−λ)Qt+1+
γλrt+2+γ2λ1(1−λ)Qt+2
...(39)
18Under review as submission to TMLR
Add (Qt−Qt) to Equation 39 without changing the value
Gλ,Q
t=Qt−Qt+rt+γQt+1
−γλQt+1+γλrt+1+γ2λQt+2
−γ2λ2Qt+2+...
=Qt+∞/summationdisplay
n=0(γλ)nδQ
t+n(40)
Since we can pull out the first residual term
Gλ,Q
t=Qt+δQ
t+∞/summationdisplay
n=1(γλ)nδQ
t+n
=Qt+rt+γQt+1−Qt+∞/summationdisplay
n=1(γλ)nδQ
t+n
=rt+γQt+1+∞/summationdisplay
n=1(γλ)nδQ
t+n(41)
then it follows that
Gλ,Q
t−bt=rt+γQt+1−bt+∞/summationdisplay
n=1(γλ)nδQ
t+n
=δt+∞/summationdisplay
l=1(γλ)l(δt+l−zt+l)
=ˆAUAE (γ,λ)
t(42)
B.4 Proof of Theorem 6.4
Lemma B.1. For any 0≤i<j, it holds that
cov(rt+i+γVπ
t+i+1−Vπ
t+i,rt+j+γVπ
t+j+1−Vπ
t+j)
= cov(rt+i+γVπ
t+i+1−Vπ
t+i,rt+j+γQπ
t+j+1−Qπ
t+j)(43)
Proof.First note
Eτ|st,at[rt+j+γVπ
t+j+1−Vπ
t+j] = 0
Eτ|st,at[rt+j+γQπ
t+j+1−Qπ
t+j] = 0(44)
And denote
δVπ
i=rt+i+γVπ
t+i+1−Vπ
t+i
δQπ
j=rt+j+γQπ
t+j+1−Qπ
t+j(45)
then
cov(δVπ
i,δVπ
j) =Eτ|st,at[δVπ
iδVπ
j]
cov(δVπ
i,δQπ
j) =Eτ|st,at[δVπ
iδQπ
j](46)
19Under review as submission to TMLR
By law of total expectation and Markov property
cov(δVπ
i,δQπ
j) =Eτ|st,at[δVπ
iδQπ
j]
=Est+i,at+i,st+i+1,st+j,st+j+1|st,atEτ|st+i,at+i,st+i+1,st+j,st+j+1,st,at[δVπ
iδQπ
j]
=Est+i,at+i,st+i+1,st+j,st+j+1|st,at[δVπ
iEτ|st+i,at+i,st+i+1,st+j,st+j+1,st,at[δQπ
j]]
=Est+i,at+i,st+i+1,st+j,st+j+1|st,at[δVπ
iEat+j+1,at+j|st+j,st+j+1[rt+j+γQπ
t+j+1−Qπ
t+j]]
=Est+i,at+i,st+i+1,st+j,at+j,st+j+1|st,at[δVπ
i(rt+j+γVπ
t+j+1−Vπ
t+j)]
=Est+i,at+i,st+i+1,st+j,at+j,st+j+1|st,at[δVπ
iδVπ
j]
=Eτ|st,at[δVπ
iδVπ
j](47)
By law of total variance, we have
Vst,at[uθAUAE (γ,λ)
t ] =Est,at[Vτ|st,at[uθtAUAE (γ,λ)]] +Vst,at[Eτ|st,at[uθAUAE (γ,λ)]]
=Est,at[u⊤
θtuθtVτ|st,at[AUAE (γ,λ)]] +Vst,at[uθEτ|st,at[AUAE (γ,λ)]]
=Est,at[u⊤
θtuθtVτ|st,at[AUAE (γ,λ)]] +Vst,at[uθ(Qπ−b)](48)
and similarly for GAE
Vst,at[uθAGAE (γ,λ)
t ] =Est,at[u⊤
θtuθtVτ|st,at[AGAE (γ,λ)]] +Vst,at[uθ(Qπ−Vπ)] (49)
The variance due to sampling a trajectory τis
Vτ|st,at[AUAE (γ,λ)] =V(δt) +∞/summationdisplay
l=1(γλ)2lV(δt+l−zt+l)+
2/summationdisplay
1≤i<j(γλ)i+jcov(δt+i−zt+i,δt+j−zt+j)+
2/summationdisplay
j>0(γλ)jcov(δt,δt+j−zt+j)(50)
and
Vτ|st,at[AGAE (γ,λ)] =∞/summationdisplay
l=0(γλ)2lV(δV
t+l)+
2/summationdisplay
0≤i<j(γλ)i+jcov(δV
t+i,δV
t+j)(51)
We first compare covariance terms.
Fori= 0
UAE : cov(rt+γΨt+1−bt,rt+j+γΨt+j+1−Ψt+j)
GAE : cov(rt+γVπ
t+1−Vπ
t,rt+j+γVπ
t+j+1−Vπ
t+j)(52)
the difference of which is
cov(rt+γΨt+1−bt,rt+j+γΨt+j+1−Ψt+j)−cov(rt+γVπ
t+1−Vπ
t,rt+j+γVπ
t+j+1−Vπ
t+j)
=Eτ|st,at[(rt+γΨt+1−bt)(rt+j+γΨt+j+1−Ψt+j)]−Eτ|st,at/bracketleftbig
(rt+γVπ
t+1−Vπ
t)(rt+j+γVπ
t+j+1−Vπ
t+j)/bracketrightbig
≜χ(0,j|Ψ)
(53)
IfΨ =Vπ, then it can be merged as
χ(0,j|Vπ) =Eτ|st,at/bracketleftbig
(Vπ
t−bt)(rt+j+γVπ
t+j+1−Vπ
t+j)/bracketrightbig
= (Vπ
t−bt)Eτ|st,at/bracketleftbig
rt+j+γVπ
t+j+1−Vπ
t+j/bracketrightbig
= 0(54)
20Under review as submission to TMLR
IfΨ =Qπ, by Lemma B.1, and denoting Xi= (st+i,at+i)we have
χ(0,j|Qπ) =Eτ|X0/bracketleftbig/parenleftbig
γ(Qπ
t+1−Vπ
t+1)−(bt−Vπ
t)/parenrightbig
(rt+j+γQπ
t+j+1−Qπ
t+j)/bracketrightbig
=EX1,Xj,Xj+1|X0/bracketleftbig/parenleftbig
γ(Qπ
t+1−Vπ
t+1)−(bt−Vπ
t)/parenrightbig
(rt+j+γQπ
t+j+1−Qπ
t+j)/bracketrightbig
=EX1,Xj|X0EXj+1|X1,Xj,X0/bracketleftbig/parenleftbig
γ(Qπ
t+1−Vπ
t+1)−(bt−Vπ
t)/parenrightbig
(rt+j+γQπ
t+j+1−Qπ
t+j)/bracketrightbig
=EX1,Xj|X0/bracketleftbig/parenleftbig
γ(Qπ
t+1−Vπ
t+1)−(bt−Vπ
t)/parenrightbig
EXj+1|X1,Xj,X0/bracketleftbig
(rt+j+γQπ
t+j+1−Qπ
t+j)/bracketrightbig/bracketrightbig
=EX1,Xj|X0/bracketleftbig/parenleftbig
γ(Qπ
t+1−Vπ
t+1)−(bt−Vπ
t)/parenrightbig
0/bracketrightbig
= 0(55)
Fori>0
UAE : cov(rt+i+γΨt+i+1−Ψt+i,rt+j+γΨt+j+1−Ψt+j)
GAE : cov(rt+i+γVπ
t+i+1−Vπ
t+i,rt+j+γVπ
t+j+1−Vπ
t+j)(56)
IfΨ =Vπ, it is obvious that χ(i,j|Vπ) = 0. IfΨ =Qπ, similarly we have
χ(i,j|Qπ) =Eτ|X0/bracketleftbig/parenleftbig
γ(Qπ
t+i+1−Vπ
t+i+1)−(Qπ
t+i−Vπ
t+i)/parenrightbig
(rt+j+γQπ
t+j+1−Qπ
t+j)/bracketrightbig
=EXi+1,Xj,Xj+1|X0/bracketleftbig/parenleftbig
γ(Qπ
t+i+1−Vπ
t+i+1)−(Qπ
t+i−Vπ
t+i)/parenrightbig
(rt+j+γQπ
t+j+1−Qπ
t+j)/bracketrightbig
=EXi+1,Xj|X0EXj+1|Xi+1,Xj,X0/bracketleftbig/parenleftbig
γ(Qπ
t+i+1−Vπ
t+i+1)−(Qπ
t+i−Vπ
t+i)/parenrightbig
(rt+j+γQπ
t+j+1−Qπ
t+j)/bracketrightbig
=EXi+1,Xj|X0/bracketleftbig/parenleftbig
γ(Qπ
t+i+1−Vπ
t+i+1)−(Qπ
t+i−Vπ
t+i)/parenrightbig
EXj+1|Xi+1,Xj,X0/bracketleftbig
(rt+j+γQπ
t+j+1−Qπ
t+j)/bracketrightbig/bracketrightbig
=EXi+1,Xj|X0/bracketleftbig/parenleftbig
γ(Qπ
t+i+1−Vπ
t+i+1)−(Qπ
t+i−Vπ
t+i)/parenrightbig
0/bracketrightbig
= 0
(57)
And next we will focus on the variance terms.
Forl= 0
V(δt)−V(δVπ
t)
=Eτ|st,at[(rt+γΨt+1−Qπ
t)2]−Eτ|st,at[(rt+γVπ
t+1−Qπ
t)2]
=Eτ|st,at[(rt+γΨt+1)2]−(Qπ
t)2−(Eτ|st,at[(rt+γVπ
t+1)2]−(Qπ
t)2)
=Eτ|st,at[(rt+γΨt+1)2]−Eτ|st,at[(rt+γVπ
t+1)2]
=Eτ|st,at[γ2(Ψ2
t+1−(Vπ
t+1)2) + 2γ(Ψt+1−Vπ
t+1)]
=Eτ|st,at[γ2(Ψ2
t+1−(Vπ
t+1)2)] + 2γEτ|st,at[Ψt+1−Vπ
t+1]
=γ2Eτ|st,at[(Ψ2
t+1−(Vπ
t+1)2)]
=γ2Est+1,at+1|st,at[Ψ2
t+1]−Est+1|st,at[(Eat+1|st+1,st,at[Qπ
t+1])2]
=/braceleftigg
γ2Est+1|st,at[Vat+1|st+1,st,at(Qπ
t+1)]ifΨ =Qπ
0 ifΨ =Vπ
=γ2Eτ|st,at[(Ψt+1−Vπ
t+1)2](58)
Forl>0
V(δt+l)−V(δVπ
t+l)
=Eτ|st,at[(rt+l+γΨt+l+1−Ψt+l)2]−Eτ|st,at[(rt+l+γVπ
t+l+1−Vπ
t+l)2]
=Eτ|st,at[(rt+l+γΨt+l+1)2]−Eτ|st,at[(rt+l+γVπ
t+l+1)2]−Est+l,at+l|st,at[(Ψt+l−Vπ
t+l)2]
=Eτ|st,at[γ2(Ψ2
t+l+1−(Vπ
t+l+1)2) + 2γrt+l(Ψt+l+1−Vπ
t+l+1)]−Est+l,at+l|st,at[(Ψt+l−Vπ
t+l)2]
=Eτ|st,at[γ2(Ψ2
t+l+1−(Vπ
t+l+1)2)] + 2γrt+lEτ|st,at[Ψt+l+1−Vπ
t+l+1]−Est+l,at+l|st,at[(Ψt+l−Vπ
t+l)2]
=γ2Eτ|st,at[(Ψ2
t+l+1−(Vπ
t+l+1)2)]−Est+l,at+l|st,at[(Ψt+l−Vπ
t+l)2]
=γ2Est+l+1,at+l+1|st,at[Ψ2
t+l+1]−Est+l+1|st,at[(Eat+l+1|st+l+1,st,at[Qπ
t+l+1])2]−Est+l,at+l|st,at[(Ψt+l−Vπ
t+l)2]
=γ2Eτ|st,at[(Ψt+l+1−Vπ
t+l+1)2]−Eτ|st,at[(Ψt+l−Vπ
t+l)2]
(59)
21Under review as submission to TMLR
Combining all the results above, we have
Vst,at[uθAUAE (γ,λ)
t ]−Vst,at[uθAGAE (γ,λ)
t ]
=Est,at/bracketleftig
u⊤
θuθ/parenleftbigg∞/summationdisplay
l=0(γλ)2l·γ2(1−λ2)Est+l+1,at+l+1/bracketleftbig
(Ψ−Vπ)2/bracketrightbig/parenrightbigg/bracketrightig
+
Est,at[u⊤
θuθ(b2−Vπ2−2Qπ(b−Vπ))](60)
B.5 Proof of corollary 6.5
IfΨ =Vπ, then the first term of Equation 60 vanishes, it reduces to the difference between the variance of
policy gradient w.r.t. different baselines. Since breduces variance no less than Vπ, it follows that
Vst,at[uθAUAE (γ,λ)
t ]−Vst,at[uθAGAE (γ,λ)
t ] =Est,at[u⊤
θuθ(b2−Vπ2−2Qπ(b−Vπ))]
=Est,at[u⊤
θuθ(Qπ−b)2]−Est,at[u⊤
θuθ(Qπ−Vπ)2]
≤0(61)
C Residual Baseline
C.1 Proof of Theorem 6.6
Our analysis relies on the fact that for a sufficient large segment length T, the empirical distribution pDis an
approximation of a mixture of the joint distributions of the past policy sequences dβ(s,a) =1
kk−1/summationtext
i=0dπn−i(s,a),
whose marginal state distribion is dβ(s) =/integraltext
Adβ(s,a)da, and conditional action distribution β(a|s) =dβ(s,a)
dβ(s).
For any past policy, the joint distribution and marginal state distribution are abbreviated as dn−i(s,a)and
dn−i(s)respectively for simplicity’s sake. And πnis recurrently referenced as πwhenever noticed.
Since supiDTV(dn−i(s,a)
dβ(s)||π)<ϵ
4andEπ[|rϕ⋆(s,a)|]<ϵ
2for anys∈S, it follows that
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglek−1/summationdisplay
i=0/integraldisplay
A/parenleftbigdn−i(s)
dβ(s)πn−i−(1 + rϕ⋆)πn/parenrightbig
Qwda/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglek−1/summationdisplay
i=0/integraldisplay
A/parenleftbigdn−i(s,a)
dβ(s)−(1 + rϕ⋆)πn/parenrightbig
Qwda/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤k−1/summationdisplay
i=0/vextendsingle/vextendsingle/vextendsingle/integraldisplay
A/parenleftbigdn−i(s,a)
dβ(s)−(1 + rϕ⋆)πn/parenrightbig
Qwda/vextendsingle/vextendsingle/vextendsingle
≤k−1/summationdisplay
i=0/integraldisplay
A/vextendsingle/vextendsingle/vextendsingledn−i(s,a)
dβ(s)−(1 + rϕ⋆)πn/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleQw/vextendsingle/vextendsingle/vextendsingleda
≤Mk−1/summationdisplay
i=0/integraldisplay
A/vextendsingle/vextendsingle/vextendsingledn−i(s,a)
dβ(s)−(1 + rϕ⋆)πn/vextendsingle/vextendsingle/vextendsingleda
≤Mk−1/summationdisplay
i=0/parenleftigg
2·/parenleftig1
2/integraldisplay
A/vextendsingle/vextendsingle/vextendsingledn−i(s,a)
dβ(s)−πn/vextendsingle/vextendsingle/vextendsingleda/parenrightig
+/integraldisplay
Aπn|rϕ⋆|da/parenrightigg
<Mk−1/summationdisplay
i=0/parenleftig
2ϵ
4+ϵ
2/parenrightig
=Mkϵ(62)
22Under review as submission to TMLR
Henceforth
/vextendsingle/vextendsingleEβ[Qw]−Eπ[(1 + rϕ⋆)Qw]/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
kk−1/summationdisplay
i=1/parenleftbigg
Eπn−i/bracketleftbiggdn−i(s)
dβ(s)Qw/bracketrightbigg
+Eπ/bracketleftbiggdπ(s)
dβ(s)Qw/bracketrightbigg/parenrightbigg
−Eπ[(1 + rϕ⋆)Qw]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
kk−1/summationdisplay
i=1/parenleftbigg
Eπn−i/bracketleftbiggdn−i(s)
dβ(s)Qw/bracketrightbigg
−Eπ/bracketleftbigg/parenleftbigg
k+krϕ⋆−dπ(s)
dβ(s)/parenrightbigg
Qw/bracketrightbigg/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
(a)=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
kk−1/summationdisplay
i=1/parenleftbigg
Eπn−i/bracketleftbiggdn−i(s)
dβ(s)Qw/bracketrightbigg
−Eπ/bracketleftbigg/parenleftbigg
(k−1)dπ(s)
dβ(s)+k(1−dπ(s)
dβ(s)) +krϕ⋆/parenrightbigg
Qw/bracketrightbigg/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
kk−1/summationdisplay
i=1/integraldisplay
A/parenleftbiggdn−i(s)
dβ(s)πn−i−dπ(s)
dβ(s)π/parenrightbigg
Qwda−kEπ/bracketleftbigg
(1−dπ(s)
dβ(s)+ rϕ⋆)Qw/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
(b)=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
kk−1/summationdisplay
i=0/integraldisplay
A/parenleftbiggdn−i(s)
dβ(s)πn−i−dπ(s)
dβ(s)π/parenrightbigg
Qwda−kEπ/bracketleftbigg
(1−dπ(s)
dβ(s)+ rϕ⋆)Qw/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
kk−1/summationdisplay
i=0/integraldisplay
A/parenleftbiggdn−i(s)
dβ(s)πn−i−dπ(s)
dβ(s)π−(1−dπ(s)
dβ(s)+ rϕ⋆)π/parenrightbigg
Qwda/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
kk−1/summationdisplay
i=0/integraldisplay
A/parenleftbiggdn−i(s)
dβ(s)πn−i−(1 + rϕ⋆)π/parenrightbigg
Qwda/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=1
k/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglek−1/summationdisplay
i=0/integraldisplay
A/parenleftbiggdn−i(s)
dβ(s)πn−i−(1 + rϕ⋆)πn/parenrightbigg
Qwda/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
kMkϵ
=Mϵ(63)
where (a) holds by adding kdπ(s)
dβ(s)−kdπ(s)
dβ(s)without changing the quantity, and (b) by notingdn−i(s)
dβ(s)πn−i−
dπ(s)
dβ(s)π= 0wheni= 0. Letgrϕ⋆(s) =Eπ[∇ϕrϕ⋆Qw], by assumption it follows that
∥grϕ⋆(s)∥≤GM (64)
Then
∥∇ϕJ(ϕ⋆)∥=∥Edβ{2·(Eπ[(1 + rϕ⋆)Qw]−Qw)grϕ⋆(s)}∥
≤E(s,a)∼dβ(s,a){2·|(Eπ[(1 + rϕ⋆)Qw]−Qw)|∥grϕ⋆(s)∥}
≤Es∼dβ(s){2·|Eπ[(1 + rϕ⋆)Qw]−Ea∼β[Qw]|∥grϕ⋆(s)∥}
≤GM2ϵ(65)
23Under review as submission to TMLR
C.2 Proof of Corollary 6.7
ByD2
TV(p||q)≤1
2DKL(p||q)(Pinsker’s inequality), we have supsDTV(˜π||π)<√ϵ
2
Edβ/bracketleftbig
(b˜π
ϕ⋆−bπ
ϕ⋆)2/bracketrightbig
=Edβ/bracketleftig/parenleftig/integraldisplay
A(˜π−π)(1 + rϕ⋆)Qwda/parenrightig2/bracketrightig
=Edβ/bracketleftig/vextendsingle/vextendsingle/vextendsingle/integraldisplay
A(˜π−π)(1 + rϕ⋆)Qwda/vextendsingle/vextendsingle/vextendsingle2/bracketrightig
≤Edβ/bracketleftig/parenleftig/integraldisplay
A|˜π−π|(1 +|rϕ⋆|)|Qw|da/parenrightig2/bracketrightig
<((K+ 1)M)2Edβ/bracketleftig/parenleftbig
2·1
2/integraldisplay
A|˜π−π|da/parenrightbig2/bracketrightig
<((K+ 1)M)2Edβ/bracketleftig/parenleftbig
2·√ϵ
2/parenrightbig2/bracketrightig
= ((K+ 1)M)2ϵ(66)
And with the fact that (a+b)2≤2(a2+b2), we have that
Edβ[(Qw−b˜π
ϕ⋆)2] =Edβ[(Qw−bπ
ϕ⋆+bπ
ϕ⋆−b˜π
ϕ⋆)2]
≤2·/parenleftbigg
Edβ(s,a)[(Qw−bπ
ϕ⋆)2] +Edβ(s)/bracketleftbig
(b˜π
ϕ⋆−bπ
ϕ⋆)2/bracketrightbig/parenrightbigg
<2J(ϕ⋆) + 2((K+ 1)M)2ϵ(67)
D Practical Algorithm
D.1 Contraction properties in mean and variance
∥ETπZ1−ETπZ2∥∞=γ∥Es′,a′[EZ1−EZ2]∥∞
≤γ∥EZ1−EZ2∥∞(68)
∥VTπZ1−VTπZ2∥∞= sup
s,a|VTπZ1(s,a)−VTπZ2(s,a)|
= sup
s,a|E[VTπZ1(s,a)−VTπZ2(s,a)]|
= sup
s,aγ2|E[VZ1(S′,A′)−VZ2(S′,A′)]|
≤sup
s′,a′γ2|VZ1(s′,a′)−VZ2(s′,a′)|
=γ2∥VZ1−VZ2∥∞(69)
D.2 Equivalence between KL divergence and MSE
Suppose (µi,σi),i= 1,2indexed by 1is the parameters of the target distribution, and 2of the learnable
distribution. Since σ1andσ2are constantly equal as σ, it means that both of them are not parameterized,
then it immediately follows with the definition of the KL divergence between two normal distributions
DKL(N(µ1,σ1),N(µ2(ϕ),σ2)) = log/parenleftigσ2
σ1/parenrightig
+σ2
1+ (µ1−µ2(ϕ))2
2σ2
2−1
2
= log/parenleftigσ
σ/parenrightig
+σ2
2σ2−1
2+(µ1−µ2(ϕ))2
2σ2
=1
σ2MSE (µ1,µ2(ϕ))(70)
24Under review as submission to TMLR
D.3 Distributionl critic update
Suppose the learnable distribution is parameterized as N(µ(ϕ),σ(ζ)). Taking gradient of the KL divergence
loss, it can be attained
∇(ϕ,σ)DKL(N(µ1,σ1),N(µ2(ϕ),σ2(ζ))) =−/parenleftbig
(σ2
1−σ2
2(ζ)) + (µ1−µ2(ϕ))2/parenrightbig
∇ζσ2(ζ)
σ3
2(ζ)−(µ1−µ2(ϕ))∇ϕµ2(ϕ)
σ2
2(ζ)
= ∆σ2+ ∆µ2
(71)
In practice, the sampling is involved as transition dynamics evolves, thus the µ1=r+γ·mean (Z¯w(s′,a′)),
andσ1=γ·stddev (Z¯w(s′,a′)). Andµ2=mean (Zw(s,a)), andσ2=stddev (Zw(s,a)). It is clear that the
parameters of the learnable distribution is always chasing for discounted ones of the target distribution.
D.4 Connection between DPO and SAC
Our off-policy gradient can be viewed as an optimistic likelihood ratio gradient estimator of SAC, that is
∇θJoff-policy (θ) =∇θEs∼D/bracketleftig
−DKL(πθ(·|s)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
αexpA+(s,·)
N(s))/bracketrightig
≜∇θJOptSAC (θ) (72)
whereN(s)is the partition function.
Since that
Es∼D,a∼πθ[∇θlogπθ(a|s) logN(s)] =/integraldisplay
dD(s)/integraldisplay
∇θπθ(a|s) logN(s)dads
=/integraldisplay
dD(s) logN(s)(∇θ/integraldisplay
πθ(a|s)da)ds
= 0(73)
and
Es∼D,a∼πθ[∇θlogπθ(a|s)] =/integraldisplay
dD(s)/integraldisplay
πθ(a|s)∇θπθ(a|s)
πθ(a|s)dads
=/integraldisplay
dD(s)/integraldisplay
∇θπθ(a|s)dads
=/integraldisplay
dD(s)(∇θ/integraldisplay
πθ(a|s)da)ds
= 0(74)
Then
∇θJOptSAC (θ) =∇θEs∼D,a∼πθ[logπθ(a|s)(A+(s,a)−αlogπθ(a|s)−logN(s))]
=Es∼D,a∼πθ[∇θlogπθ(a|s)(A+(s,a)
−αlogπθ(a|s)−logN(s))]−Es∼D,a∼πθ[∇θlogπθ(a|s) logN(s)]
+Es∼D,a∼πθ[∇θlogπθ(a|s)]
=Es∼D,a∼πθ[∇θlogπθ(a|s)(A+(s,a)−αlogπθ(a|s))]
=∇θJoff-policy (θ)(75)
D.5 Proof of Theorem 6.8
Since
|πkrϕk|≤K (76)
and
lim
k→∞πkrϕk=π⋆rϕ⋆ (77)
25Under review as submission to TMLR
by Lebesgue bounded convergence theorem, we have
lim
k→∞Eπk[rϕk] =Eπ⋆[rϕ⋆] (78)
Then /integraldisplay
Aπ⋆(a|s)rϕ⋆(s,a)Qπ⋆(s,a)da
=/integraldisplay
A\A0π⋆(a|s)rϕ⋆(s,a)Qπ⋆(s,a)da
=Q⋆/integraldisplay
A\A0π⋆(a|s)rϕ⋆(s,a)da
=Q⋆/integraldisplay
Aπ⋆(a|s)rϕ⋆(s,a)da
=Q⋆Eπ⋆[rϕ⋆]
= 0(79)
whereQ⋆stands for the maximum action-value. As the π⋆is deterministic, then Qπ⋆(s,a) =Q⋆for any
a∈A\A0. Thus it follows
lim sup
k→∞A+
k= lim sup
k→∞(Qπk(s,a)−bϕk(s))+
= lim sup
k→∞(Qπk(s,a)−bϕk(s))+
= (Qπ⋆(s,a)−E[(1 + rϕ⋆(s,a))Qπ⋆(s,a)])+
= (Qπ⋆(s,a)−Vπ⋆(s)−Q⋆/integraldisplay
A\A0π⋆(a|s)rϕ⋆(s,a)da)+
= (Qπ⋆(s,a)−Vπ⋆(s))+= 0(80)
Last equality holds for that Vπ⋆(s) = maxa∈AQπ⋆(s,a).
D.6 Proof of Theorem 6.9
Lemma D.1. Gu et al. (2017a)
∥ρπ−ρβ∥1≤2tDmax
TV(π||β)≤2t/radicalig
Dmax
KL(π||β) (81)
which can be adopted from Ross et al. (2011) Kahn et al. (2017) Schulman et al. (2015) Janner et al. (2019).
Denote
¯Lπ(˜π) =η(π) +Eρπ,˜π[Qπ−bπ
ϕ] (82)
then
|η(˜π)−¯Lπ(˜π)|=|Eρ˜π,˜π[Qπ−bπ
ϕ]−Eρπ,˜π[Qπ−bπ
ϕ]|
=∞/summationdisplay
t=0γt|Eρ˜π
t/bracketleftbig
E˜π[Qπ−bπ
ϕ]/bracketrightbig
−Eρπ
t/bracketleftbig
E˜π[Qπ−bπ
ϕ]/bracketrightbig
|
≤Υ∞/summationdisplay
t=0γt∥ρ˜π
t−ρπ
t∥1
≤2Υ∞/summationdisplay
t=0γtt/radicalig
Dmax
KL(π||˜π)
=2γΥ
(1−γ)2/radicalig
Dmax
KL(π||˜π)(83)
26Under review as submission to TMLR
And we relate Lπ(˜π)to¯Lπ(˜π)
|¯Lπ(˜π)−Lπ(˜π)|=|η(π) +Eρπ,˜π[Qπ−bπ
ϕ]−η(π)−ωEρπ,˜π[Qπ−bπ
ϕ]−(1−ω)Eρβ,˜π[(Qw−b˜π
ϕ)++αlog ˜π]|
= (1−ω)|Eρπ,˜π[Qπ−bπ
ϕ]−Eρβ,˜π[(Qw−b˜π
ϕ)++αlog ˜π]|
≤(1−ω)/parenleftigg
|Eρπ,˜π[Qπ−bπ
ϕ]−Eρπ,˜π[Qw−b˜π
ϕ]|
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(1)+
|Eρπ,˜π[Qw−b˜π
ϕ]−Eρβ,˜π[Qw−b˜π
ϕ]|
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(2)+
|Eρβ,˜π[(Qw−b˜π
ϕ)−+αlog ˜π]|
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
(3)/parenrightigg
(84)
where
(1)≤Eρπ,˜π|Qπ−Qw−(Eπ[(1 + rϕ)Qw]−E˜π[(1 + rϕ)Qw])|
≤∆ + 2Dmax
TV(π||β)(1 +K)M
≤∆ +/radicalig
2Dmax
KL(π||β)(1 +K)M
≜∆ +/radicalig
2Dmax
KL(π||β)C1(85)
(2)≤∞/summationdisplay
t=0γt∥ρπ
t−ρβ
t∥1|E˜π[Qw−b˜π
ϕ]|
≤2Ω∞/summationdisplay
t=0γtt/radicalig
Dmax
KL(π||β)
≤2γΩ
(1−γ)2/radicalig
Dmax
KL(π||β)(86)
(3)≤αCH+C−≜C2 (87)
Combining all parts, we have
|η(˜π−Lπ(˜π))|≤|η(˜π−¯Lπ(˜π))|+|¯Lπ(˜π)−Lπ(˜π)|
=2γΥ
(1−γ)2/radicalig
Dmax
KL(π||˜π) + (1−ω)(∆ +C1/radicalig
2Dmax
KL(π||˜π) +2γΩ
(1−γ)2/radicalig
Dmax
KL(π||β) +C2)
(88)
E Action Bounds Transformation
The support of Beta distribution is [0,1], for multivariate case with ndimensions which are mutually inde-
pendent, it would be nproducts of [0,1]. In practice, the action bounds doesn’t have to fit into this domain,
we therefore need to apply a linear transformation upon each dimension to coincide with the actual bound
[mi,Mi],i= 1,2,...,n, wheremiis the lower bound, and Miupper bound. It would be convenient to
vectorize those bounds as mandM.
Letx∈Rnbe a Beta-distributed random vector whose density is f(x|s). It is transformed as a new random
vector a=x·(M−m) +mto be the action executed in the environment, at which product is element-wise.
Denote k=M−mandb=m, its log-likelihood is given by
logπ(a|s) = logf(x|s)−1⊤logk (89)
27Under review as submission to TMLR
where 1⊤is an-dimensional vector whose entry is one. Since 1⊤logkis a constant, the difference of the
log-likelihood πreduces to the that of the original one f, namely
log ˜π(a|s)−logπ(a|s) = log ˜f(x|s)−logf(x|s) (90)
This invariance is useful especially for the case that the log-likelihood policy gradient is involved e.g.
A2C, TRPO and PPO. And it should be noted that only the f(x|s)is parameterized as fθ(x|s), thus
∇θlogπ(a|s) =∇θlogfθ(x|s). We store the transformed action aand the untransformed log-likelihood
logf(x|s), which is used in the policy improvement phase, to either calculate the difference of the log-
likelihoods (TRPO, PPO) or the log-likelihood only (A2C). By detransforming the action ato the original
onex,˜f(x|s)can be calculated for those aforementioned purposes.
F Experimental Details
F.1 Figure 2
All the on-policy algorithms are implemented from OpenAI Baselines Dhariwal et al. (2017), including A2C,
TRPO and PPO. It is implemented with the latest version of SAC with auto-temperature Haarnoja et al.
(2018b), from the author’s repository3, TD34and IPG5as well. We perform all algorithms on the MuJoCo
tasks with v3 version, except PPO, which is on the v2 version. The reason is that we observe the OpenAI
Baselines implementation doesn’t learn on the v3 version tasks at all, to make the results comparable, and
respect the originality of the specific algorithm implementation, we have to make this compromise.
F.2 Figure 3
The time complexity comparison was benchmarked with a single core on an Intel Xeon Platinum 8358 CPU.
And the relative policy updates were calculated by dividing the total number of policy gradient updates by
the total number of updates.
F.3 Figure 4
The fixed interval is comprised of 2048 steps. We log the metric for 25%even spaced data within every
interval.
Variance of Policy Updates We monitor two consecutive time steps’ policy parameters θtandθt+1, and
compute the policy change ∆θt=θt+1−θt, with which we have
VPU =V[∆θt] (91)
=E[∥∆θt−E[∆θt]∥2] (92)
Average Total Variation We define the average total variation as the sample average of the absolute
differences between adjacent sample points
ATV =1
NN−1/summationdisplay
i=0|LMSE
i+1−LMSE
i| (93)
whereLMSEis the critic MSE loss. For PPO and SAC, it is the same as the critic loss, whereas we re-evaluate
for DPO as it employs KL divergence loss.
3https://github.com/rail-berkeley/softlearning
4https://github.com/sfujim/TD3
5https://github.com/rlbayes/rllabplusplus
28Under review as submission to TMLR
F.4 Figure 5(a)
The calculation of trajectory variance is borrowed from Tucker et al. (2018), but with a tighter statistics.
We first follow the current policy πto collect a large batch of data with a size of 25000, from which we
uniformly draw a subset of data Dtestwith a size of 1000 as an estimation of the outer expectation. To
estimate the trajectory variance, for each (s,a)∈Dtest, we independently sample two trajectories τ,τ′∼
τ|s,awith a maximum horizon 1000, with which we have a single variance estimator
∥uθ(s,a)∥2/parenleftig
ˆA(s,a|τ)2−ˆA(s,a|τ)ˆA(s,a|τ′)/parenrightig
(94)
where ˆAcan be either ˆAUAE (γ,λ)orˆAMC. In the end, we average all the estimators and take log scale for
better readability.
The reason we don’t compare with GAE is that
•GAE is a strict special case of UAE.
•The critic of our algorithm is Qw, for which GAE is no longer applicable.
F.5 Figure 5(b)
We estimate both on- and off-policy gradient variance. In on-policy case, we perform the estimation on the
collected rollouts (with a total steps 2048)
1
|B|/summationdisplay
(s,a)∼B∥uθ∥2(Qw−bπ
ϕ)2(95)
And for the case of off-policy, we uniformly draw a subset of states Doff(size of 10000) from the replay buffer,
then sample a new action a∼πθfor each state. The estimator is
1
|Doff|/summationdisplay
s∼Doff,a∼πθ∥uθ∥2(Qw−bπ
ϕ)2(96)
F.6 Figure 5(c)
In our setting, the maximum horizon T= 2048, and the size of the replay buffer |D|= 1e6, we therefore
gradually increase the number of samples training the baseline starting from totally on-policy ( N= 1·T)
to the full samples ( N=|D|).
F.7 Figure 6
We borrow the metrics from Fujimoto et al. (2022) with modifications. The mean absolute error (MAE),
namely generalization error, is performed on the subset Deof the test dataDtest(will be introduced later)
1
|De|/summationdisplay
(s,a)∼De|Qw(s,a)−Qπ(s,a)| (97)
where the test data Dtestis collected by executing current policy πin the environment with 50000 steps,
andDeis formed as the uniformly sampled data (size = 1000) from the test data. For each (s,a)in theDe,
the true action-value function Qπis approximated by the Monte-Carlo estimate with 100 episodes, each of
which has a timelimit 1000 (which means each episode cannot exceed that limit). The root mean squared
error (RMSE), namely prediction error, is performed on the data of the batch B(size = 2048)
1
|B|/summationdisplay
(s,a)∼B/radicalig/parenleftbig
Qw(s,a)−(r+γQw(s′,a′))/parenrightbig2(98)
TheQwall of above is the mean value of the learned distribution Qw(s,a) =E[Zw(s,a)].
29Under review as submission to TMLR
F.8 Table 1
DPO(A2C) implementation A2C is not a trust region method, therefore we only perform the policy
update once.
DPO(TRPO) implementation TRPO finds a search direction ∆θby the conjugate gradient algorithm
with a backtracking line search. Though the interpolation is not straightforward at the first glance, we
propose to firstly calculate off-policy loss (1−w)Joff policyunder the current policy πθ, then search for an
increment of w∆θ. By parallelogram law, we can increment the policy parameter with w∆θ, and then update
the policy based on the pre-calculated off-policy loss.
And at each policy update, we update the distributional critic 10 times upon the minibatch drawn from B.
F.9 Table 2
Setting Description
First group
no-UAE replace UAE with Monte-carlo estimator
no-RB set r≡0
no-DIST replace distributional critic and KL divergence with a single Q and MSE respectively
Second group
no-INT set ω= 1
no-ENT set α= 0
Third group
only-ON without off-policy evaluation at the interaction level
only-OFF without on-policy evaluation at the batch level
G Implementation Details
G.1 Network Architecture
Majority of the DRL algorithms parameterize the policy as a Gaussian distribution, due to its simplicity.
However, the boundary effect of it is observed in several works Chou et al. (2017) Fujita & Maeda (2018).
And in practice, the action space is usually bounded, thus it is beneficial to parameter our policy as a
beta policy. Like Chou et al. (2017), the shape parameters α,βis forwarded through a fullly connected
neural network, and converted to the range [1,∞)by a softplus activation added with a constant 1. For the
critic, we parameterize it as a Gaussian distribution, output the mean value through the last layer, and the
standard deviation operated by a softplus activation similarly. For the baseline, it is simply a fully connected
neural network that outputs the residual term, when approximating the baseline, a constant 1 is added (see
Equation 16). All the networks are with 2 hidden layers, and tanhactivation function, each of which has
256 neurons. This architecture is robust to the hyperparameter changes, especially when different on-policy
learners are involved.
G.2 Advantage Interpolation
Weintroduceanotherinterpolatingparameter νforadvantageestimateover Bbeforethepolicyimprovement.
Since we evaluate at each environment step, we expect a good quality that the expected value Qw=E[Zw]
would achieve.
ˆA= (1−ν)AUAE+ν(Qw−bϕ) (99)
30Under review as submission to TMLR
G.3 Normalization
Unlike PPO, we minimize the code-level optimization, such as network initialization, learning rate decay,
gradient clipping etc. Like PPO, we adopt observation normalization, reward normalization, and advantage
normalization (batch level), as it is beneficial to stabilize the behavior of the neural network and enable
faster learning.
H Hyperparameters
Hyperparameter Value
optimizer Adam Kingma & Ba (2015)
learning rate 3×10−4
size of replay buffer D 1000000
size of mini batch from B 256
size of mini batch from D 256
discounted factor γ 0.99
UAEλ 0.95
target smoothing parameter τ 5e−3
policy interpolating parameter ω 0.7
advantage interpolating parameter ν 0.3
temperature α 0.03
num samples 30
critic samples 25
on-policy learner PPO
clipping parameter ε 0.2
size of batchB 2048
epochs per batch 10
baseline updates 12
on-policy learner A2C
size of batchB 256
epochs per batch 1
baseline updates 4
on-policy learner TRPO
max kl 0.1
damping 0.1
size of batchB 4096
epochs per batch 1
baseline updates 12
Table 3: Hyperparameters of DPO
31Under review as submission to TMLR
I Additional Experiments
I.1 Variants of DPO
Figure7: ComparisonbetweenDPO(A2C)andA2C,averagedover5randomseedsandshadedwithstandard
error.
Figure 8: Comparison between DPO(TRPO) and TRPO, averaged over 5 random seeds and shaded with
standard error.
32Under review as submission to TMLR
I.2 Additional Baselines
Table 4: Comparison to other algorithms combining on-policy methods with off-policy data.
Method Walker2d Hopper Swimmer Humanoid Ant Avg.
DPO (1M) 4860 ±680 3187 ±351 112±136285 ±852 5278 ±173 3944 ±414
Q-prop (1M) 358 ±26 1464 ±203 59 ±4 355 ±3 -46 ±8 438 ±49
P3O (3M) 3771 2334 - 2057 4727 3222
Results for Q-prop were replicated by following the official repository, while P3O’s results are based on the
paper.
I.3 Effect of Parameters
(a)α
 (b)ω
 (c)ν
(d) baseline updates
 (e) num samples
 (f) critic samples
Figure 9: Varying levels of different parameters of DPO: (a)temperature α,(b)interpolating parameter ω,
(c)advantage interpolating parameter ν,(d)baseline updates, (e)num samples and (f)critic samples.
33Under review as submission to TMLR
I.4 Validation
(a) Absolute Residual
 (b) Positive Advantage
I.5 Wall Clock Time
J A New Learning Paradigm
General DPO demonstrates robustness across various on-policy learners and the majority of hyperparam-
eters.
Lightweight DPO boasts an impressive reduction in training time, requiring only 4%of the policy gradi-
ents when compared to off-policy algorithms like TD3 and SAC.
Parallelable DPO can efficiently utilize multiple environments to collect samples like PPO, improving
training efficiency and reducing training time.
Sample Efficient DPO fully exploits two sources of data for both evaluation and control, greatly boosting
the sample efficiency of on-policy algorithms.
Stable DPO stands out for its stability, characterized by smoother policy changes and reduced variability
in critic MSE loss, ensuring more reliable learning.
34