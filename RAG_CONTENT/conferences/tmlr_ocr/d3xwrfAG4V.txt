Published in Transactions on Machine Learning Research (01/2024)
Transfer Learning for High-dimensional Quantile Regression
with Statistical Guarantee
Sheng Qiao sqiao@ucsd.edu
Department of Mathematics
University of California San Diego
San Diego, CA 92093, USA
Yong He heyong@sdu.edu.cn
Institute for Financial Studies
Shandong University
Jinan, 250100, China
Wen-Xin Zhou wenxinz@uic.edu
Department of Information and Decision Sciences
University of Illinois at Chicago
Chicago, IL 60607, USA
Reviewed on OpenReview: https: // openreview. net/ forum? id= d3xwrfAG4V
Abstract
The task of transfer learning is to improve estimation/inference of a target model by mi-
grating data from closely related source populations. In this article, we propose transfer
learning algorithms for high-dimensional Quantile Regression (QR) models with the tech-
nique of convolution-type smoothing. Given the transferable source populations, we derive
/lscript1//lscript2-estimation error bounds for the estimators of the target regression coeﬃcients under
mild conditions. Theoretical analysis shows that the upper bounds are improved over those
of the classical penalized QR estimator with only the target data, as long as the target and
the sources are suﬃciently similar to each other. When the set of informative sources is un-
known, a transferable source detection algorithm is proposed to detect informative sources
from all available sources. Thorough simulation studies justify our theoretical analysis.
1 Introduction
Transfer learning (Torrey & Shavlik, 2010) has been growing popular and drawing increasing attention in
machine learning, which achieves great success in a wide range of real applications with limited available
training data. Transfer learning aims to transfer knowledge from related source tasks/domains to enhance
the learning or performance of the target task/domain, which typically involves two main subproblems.
First, some criteria should be come up with to quantify the relatedness/similarity among target and source
tasks. Intuitively, a high similarity would enhance the performance, while a low similarity would be harmful
for the target task, which is known as “negative transfer” in the literature (Torrey & Shavlik, 2010). Second,
a transfer procedure should be carefully designed to transfer the “critical” knowledge from source domains,
just like the human intelligence of leveraging prior experiences to tackle novel problems. A well designed
transfer algorithm should not only identify the positive transfer sources thereby enlarging their impact, but
also avoid the negative transfer in any case. All in all, transfer learning has become an active and promising
research area, and substantial contributions has also been made recently to the theoretical guarantee for
transfer learning in both supervised, semi-supervised, and unsupervised settings, see for example the context
of classiﬁcation by Cai & Wei (2021); Reeve et al. (2021), high-dimensional (generalized) linear regression by
Li et al. (2022); Tian & Feng (2023); Lin & Li (2022), graphical model by Li et al. (2023); He et al. (2022).
1Published in Transactions on Machine Learning Research (01/2024)
As far as we know, there exist no work on transfer learning for quantile regression and we aim to ﬁll this
gap in this paper.
Comparison with the existing work and our contribution
A few works explore transfer learning under the high-dimensional setting. Bastani (2021) studied the transfer
learning problem under a high-dimensional generalized linear models (GLM) with one single known trans-
ferable source data and the dimensionality pis assume to be larger than the sample size of the target dataset
ntargetwhile smaller than that of the source dataset nsource. A two-step transfer learning algorithm was
developed and the /lscript1-estimation error bound was derived when the diﬀerence between the target and source
coeﬃcient is /lscript0-sparse. More speciﬁcally, their estimator requires ntarget =O(s2log2(p/ξ)/ξ2)as long as
nsource&O(s2p2log2(p/ξ)/ξ2), whereξdenotes a parameter which is less than the /lscript1-norm of the diﬀerence
vector between the coeﬃcients of the target and source, pis the number of features, and sis the sparsity
of the diﬀerence in coeﬃcients between the target and source. Li et al. (2022) studied the high-dimensional
linear regression problem under some weaker assumptions, where both target and source samples are high-
dimensional. Multiple source datasets are available and the transferable set may even be unknown in their
paper. With /lscriptq-sparse diﬀerence vector between the coeﬃcients of the target and source for q∈[0,1)and
/lscript0-sparse target parameter, the /lscript2-estimation error bound was derived and proved to be minimax optimal
under some conditions. In the setting where the transferable set is unknown, a source detection algorithm
was proposed to consistently select the informative sources. Tian & Feng (2023) further investigated multi-
source transfer learning on high-dimensional generalized linear models (GLM). They assumed both target
and source data to be high-dimensional and the disparity in coeﬃcients between the target and source to
be/lscript1-sparse. Given the informative sources to transfer, the /lscript1//lscript2-estimation error was derived and proved
to be minimax optimal under mild conditions. Tian & Feng (2023) also established a transferable source
detection algorithm to identify the informative sources. In addition, they constructed the corresponding
conﬁdence interval for individual regression parameter. Li et al. (2021) proposed a federated transfer learn-
ing approach to consolidate data from diﬀerent populations and from multiple medical associations. The
target and source data are both high-dimensional in their discussion and they characterized the vector of
disparities between the target and source parameters to be /lscript0-sparse. Compared with Tian & Feng (2023),
their approach achieves a faster convergence rate under some conditions and has weaker requirements on the
level of heterogeneity for data from diverse populations.
Inspired by the two-step algorithm in Bastani (2021), Li et al. (2022) and Tian & Feng (2023), we propose
a multi-source transfer learning method under high-dimensional quantile regression. To overcome the non-
smoothness and non-convexity of the quantile loss, motivated by He et al. (2021) and Tan et al. (2022),
we employ the convolution-type smoothed quantile regression. With the help convolution smoothing, Tan
et al. (2022) proposed a gradient-based algorithm that is more scalable to large-scale problems with either
large sample size or high dimensionality compared with other methods for ﬁtting high-dimensional quantile
regression. Assuming the diﬀerence vector between the target and each source coeﬃcients to be /lscript1-sparse,
we establish the /lscript1//lscript2-estimation error bounds that are proved to be sharper than the bounds of the classical
/lscript1-penalized quantile regression (Belloni & Chernozhukov, 2011) under some conditions.
In this paper, we propose transfer learning algorithms for quantile regression with high-dimensional data and
we assume the diﬀerence between target and source coeﬃcients to be /lscript0-sparse or/lscript1-sparse. In the setting
where the sources are suﬃciently close to the target, our theoretical analysis and simulation results show
that the estimation error bound of the target coeﬃcients is improved compared to the classical /lscript1-penalized
quantile regression model (Belloni & Chernozhukov, 2011) using only the target data under mild conditions.
To overcome the lack of smoothness and convexity of the check loss, we employed the convolution-type
smoothed quantile regression and analyzed the (local) restricted strong convexity of the empirical smoothed
quantile loss functions in the transferring and debiasing steps. We also extended the source detection
algorithminTian&Feng(2023)tothequantileregressionsetting. Simulationresultsshowthatthealgorithm
works well in discovering useful sources. In contrast to the case with /lscript1-sparse diﬀerence vector between the
target and source coeﬃcients, the algorithm with /lscript0-sparse one learns the source coeﬃcients independently,
which greatly reduces the communications cost across diﬀerent sources. Furthermore, the algorithm with
/lscript0-sparse diﬀerence vector has fewer assumptions on the level of heterogeneity for data from diﬀerent sources.
2Published in Transactions on Machine Learning Research (01/2024)
The most related work is a concurrent paper by Zhang & Zhu (2022), which also considered the smoothed
quantile regression models under transfer learning framework. They proposed a smoothed two-step transfer
learning algorithm as well as a new source detection method based on the K-means clustering algorithm,
which does not need the input of a threshold in contrast to the source detection algorithm in Tian & Feng
(2023). In addition, they further extended their work to the distributed quantile regression and model
averaging setup. However, compared with Zhang & Zhu (2022), our work doesn’t require the restrictive
conditions on the kernels that sup|h|≤1K(u/h)/h<Mkalmost everywhere in u. In addition, given that the
disparity vector is characterized in /lscript0-norm instead of /lscript1-norm, we introduce an algorithm which is motivated
from Li et al. (2022) and Li et al. (2021). The /lscript1//lscript2-estimation error bounds are also established and proved
to be sharper than the bounds of the classical /lscript1-penalized quantile regression (Belloni & Chernozhukov,
2011) under some mild conditions.
Before ending this section, we introduce the notations used throughout the paper. For every integer k≥1,
we use Rkto denote the k-dimensional Euclidean space, and write [k] ={1,...,k}. Fork≥2,Sk−1={u∈
Rk:||u||2= 1}denotes the unit sphere in Rk. For any symmetric, positive semideﬁnite matrix A∈Rk×k,
if its vector of eigenvalues is denoted by γ(A)and ordered as γ1(A)≥,...,≥γp(A)≥0, the operator norm
ofAis||A||2=γ1(A). Moreover, the vector norm induced by Ais||u||A=||A1/2u||2for anyu∈Rk. For
any real numbers sandt,s∨tdenotes max(s,t)ands∧tdenotes min(s,t). For two sequences {an}n≥1and
{bn}n≥1, which consist of non-negative numbers, an.bnmeans that there exists a constant C > 0such
thatan≤Cbn.an/equivasymptoticbnis equivalent to an.bnandbn.an. Forr,l> 0, deﬁne the /lscript2-ball and/lscript1-cone as
BA(r) ={δ∈Rp:||δ||A≤r}andCA(l) ={δ∈Rp:||δ||1≤l||δ||A}.
2 Methodology
2.1 Problem setup
Given the predictors x∈Rpand a scalar response variable y∈R, theτ-th conditional quantile functions of
ygivenxis written as
F−1
y|x(τ) = inf{y:Fy|x(y)≥τ},
whereFy|x(·)is the conditional distribution function of ygivenx. Consider the following linear quantile
regression model at a given τ∈(0,1):
F−1
y|x(τ) =xTβ∗(τ),
whereβ∗(τ) = (β∗
1(τ),...,β∗
p(τ))T∈Rpis the true quantile regression coeﬃcient.
Let{(yi,xi)}n
i=1be a random sample from (y,x). The preceding model assumption is equivalent to the
following model
yi=xT
iβ∗+/epsilon1iandP(/epsilon1i≤0|xi) =τ.
The/lscript1-penalized quantile regression estimator (Belloni & Chernozhukov, 2011) is generally deﬁned as one
of the solution to the optimization problem
minimize
β=(β1,...,β p)T∈Rp/braceleftBigg
1
nn/summationdisplay
i=1ρτ(yi−xT
iβ)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:ˆQ(β)+λ||β||1/bracerightBigg
, (1)
whereρτ(u)is deﬁned as ρτ(u) =u{τ− 1(u < 0)}, also referred to as the τ-quantile check loss function.
LetˆF(·;β)be the empirical cumulative distribution function of the residuals {ri(β) :=yi−xTβ}n
i=1, i.e.,
ˆF(u;β) = (1/n)/summationtextn
i=11{ri(β)≤u}for anyu∈R. Then the empirical quantile loss ˆQ(β)in (1) can be
written as
ˆQ(β) =/integraldisplay∞
−∞ρτ(u)dˆF(u;β). (2)
As the empirical cumulative distribution function ˆF(·;β)is discontinuous, the empirical quantile loss is non-
diﬀerentiable, which brings great challenges to both computation and statistical theory establishment. The
3Published in Transactions on Machine Learning Research (01/2024)
kernel smoothing method (Horowitz, 1998) is commonly utilized to tackle this issue. However, the smoothed
loss is still non-convex, thereby we further consider the convolution-type smoothed quantile loss function,
which is not only convex but also diﬀerentiable and brings great convenience in terms of both computation
and theoretical analysis. In the following, we brieﬂy introduce the convolution-type smoothed quantile loss
function, which was ﬁrstly introduced by Tan et al. (2022).
LetK(·)be a non-negative kernel function that is symmetric around 0and integrates to 1, andh>0be a
bandwidth. That is
Kh(u) = (1/h)K(u/h),¯K(u) =/integraldisplayu
−∞K(v)dvand ¯Kh(u) =¯K(u/h), u∈R.
The empirical smoothed loss function can be deﬁned as
ˆQh(β) =1
nn/summationdisplay
i=1lh(yi−xT
iβ)withlh(u) = (ρτ∗Kh)(u) =/integraldisplay∞
−∞ρτ(v)Kh(v−u)dv,
where∗denotes the convolution operator. Therefore, the /lscript1-penalized convolution smoothed estimator is
given by
ˆβ∈arg min
β∈Rp/braceleftBig
ˆQh(β) +λ||β||1/bracerightBig
,
where the smoothing bandwidth hadapts to the sample size nand the dimension pwhile ˆβdepends on the
quantile index τ, bandwidth h, and penalty level λ.
Remark 2.1. To better understand this smoothing mechanism, we compute the smoothed loss lh(u)ex-
plicitly for several widely used kernel functions. Recall that ρτ(u) =|u|/2 + (τ−1/2)u.
(i) (Gaussian kernel) The Gaussian kernel K(u) =φ(u), whereφ(·)is the density function of a standard
normal distribution. The resulting smoothed loss is lh(u) = (h/2)G(u/h) + (τ−1/2)u, whereG(u) =
(2/π)1/2e−u2/2+u{1−2Φ(−u)}.
(ii) (Uniform kernel) The uniform kernel is K(u) = (1/2) 1(|u|≤1), which is the density function of the
uniform distribution on [−1,1]. the resulting smoothed loss is lh(u) = (h/2)U(u/h)+(τ−1/2)u, where
U(u) = (u2/2 + 1/2) 1(|u|≤1) +|u| 1(|u|>1)is a Huber-type loss. Convolution plays a role of random
smoothing in the sense that lh(u) = (1/2)E(|Zu|) + (τ−1/2)u, where for every u∈R,Zudenotes a
random variable uniformly distributed between u−handu+h.
(iii) (Laplacian kernel) The Laplacian kernel is K(u) =e−|u|/2. We havelh(u) =ρτ(u) +he−|u|/h/2.
(iv) (Logistic kernel) The logistic kernel is K(u) =e−u/(1 +e−u)2. The resulting smoothed loss is lh(u) =
τu+hlog(1 +e−u/h).
(v) (Triangular kernel) The triangular kernel is K(u) = (1−|u|) 1(|u|≤1). The resulting smoothed loss is
lh(u) = (h/2)ltr(u/h) + (τ−1/2)u, whereltr(u) := (u2−|u|3/3 + 1/3) 1(|u|≤1) +|u| 1(|u|>1).
(vi) (Epanechnikov kernel) The Epanechnikov kernel is K(u) = (3/4)(1−u2) 1(|u|≤1). The resulting
smoothed loss is lh(u) = (h/2)E(u/h) + (τ−1/2)u, whereE(u) := (3u2/4−u4/8 + 3/8) 1(|u| ≤
1) +|u| 1(|u|>1).
One can easily check that all the empirical smoothed loss functions above are convex. See Figure 1 for a
visualization of Horowitz’s and convolution smoothing methods.
In the following, we consider the multi-source transfer learning scenario, where we have a target data set
(X(0),y(0))andKsource data sets with the k-th source denoted as (X(k),y(k)), whereX(k)∈Rnk×p,
y(k)∈Rnkfork= 0,...,K. Thei-th row ofX(k)and thei-th element of y(k)are denoted as x(k)
iandy(k)
i,
respectively. The goal is to transfer useful information from the source datasets to improve the estimation
4Published in Transactions on Machine Learning Research (01/2024)
Figure 1: Plots of a standard quantile loss, Horowitz’s smoothed quantile loss (Horowitz, 1998) and some
proposed convolution-type smoothed quantile loss with diﬀerent widely used kernel functions.
accuracy of the target parameters. Denote the true target parameter as β∗=ω(0). We assume the responses
in the target and source data all follow the linear quantile regression model, that is,
y(k)
i= (x(k)
i)Tω(k)+/epsilon1(k)
iandP(/epsilon1i≤0|x(k)
i) =τ, k = 0,...,K.
We build our quantile regression transfer learning procedure in the high-dimensional regime with a sparsity
assumption. In other words, we assume the dimension pis much larger than the sample size nkfor allk
while the target model is s-sparse, which satisﬁes ||β∗||0=s. Deﬁne the k-th contrast as δ(k)=β∗−ω(k)
and||δ(k)||qis referred to as the transferring level of source kin the literature, where q∈{0,1}. Deﬁne the
level-mtransferring setAm={k:||δ(k)||q≤m}as the set of sources which has transferring level lower than
m. DenotenAm=/summationtext
k∈Amnk,αk=nk/(nAm+n0)fork∈{0}∪AmandKAm=|Am|.
As stated in the introduction, we will consider two types of transferring level, corresponding to q∈{0,1}
respectively. In the case of q= 0, the transferring set corresponds to the source data whose contrast vectors
have at most mnonzero elements. In the case of q= 1, all the coeﬃcients of the contrast vectors can be
nonzero, but their absolute magnitude decays at a relatively rapid rate. It will be seen later that as long
asmis relatively small, the source data in Amcan be useful in improving the estimation accuracy of β∗.
In addition, the logic of the algorithm with /lscript1-normedAmand the algorithm with /lscript2-normedAmare quite
diﬀerent and we will elaborate on these two diﬀerent algorithms in the following sections.
2.2 The proposed algorithm with an /lscript1-norm constrained transferring set
In this section, we propose the transfer learning algorithm with /lscript1-norm constrained transferring set, which
is motivated by Tian & Feng (2023). This algorithm involves two steps. The ﬁrst step of our algorithm is
to transfer the information from useful sources by pooling all the data in transferable set Amand target set
A0to obtain a primal estimator. We also call it the transferring step. To be more precise, we deﬁne a total
smoothed loss function for the target and source datasets in the transferable set Am, i.e.,
ˆQh(ω) =1
nAm+n0/summationdisplay
k∈Amnk/summationdisplay
i=1lh/parenleftbig
y(k)
i−(x(k)
i)Tω/parenrightbig
,
5Published in Transactions on Machine Learning Research (01/2024)
where
lh(u) = (ρτ∗Kh)(u) =/integraldisplay∞
−∞ρτ(v)Kh(v−u)dv.
Then for the transferring step, we aim to ﬁnd the minimizer to the following optimization problem with
respect tow∈Rp:
minimize
ω/braceleftbigˆQh(ω) +λω||ω||1/bracerightbig
.
We denote the minimizer as ˆωAm, i.e., ˆωAm= arg minω/braceleftBig
ˆQh(ω) +λω||ω||1/bracerightBig
. By selecting an appropriate
bandwidth h, the iteratively reweighted /lscript1-penalized SQR estimator proposed by Tan et al. (2022) shares
the same upper bounds for both /lscript1and/lscript2errors as the /lscript1-QR estimator, as indicated by Belloni & Cher-
nozhukov (2011). Furthermore, they introduced coordinate descent and ADMM-based algorithms for solving
/lscript1-penalized quantile regression, which are computationally eﬃcient especially for large-scale problems.
Denote the true parameter in the ﬁrst step as ωAm, andωAmhas the following explicit form:
ωAm=β+δAm,
whereδAm=/summationtext
k∈Amαkδ(k)andαk=nk/(nAm+n0). For the second step (the debiasing step), we correct
the bias,δAm, based on the estimator ˆωAmacquired in the transferring step. The smoothed loss function
for the target data with respect to δis deﬁned as
ˆQ(0)
g(ˆωAm+δ) =1
n0n0/summationdisplay
i=1lg/parenleftbig
y(0)
i−(x(0)
i)T(ˆωAm+δ)/parenrightbig
.
The error of the debiasing step is under control for a relatively small m, sinceδAmis a/lscript1-sparse high-
dimensional vector.
We call this algorithm Oracle /lscript1-Trans-SQR as we ﬁrst assume that all useful sources are known as a priori.
Algorithm 1 formally presents the Oracle /lscript1-Trans-SQR algorithm.
Algorithm 1: Oracle/lscript1-Trans-SQR
Input:Target data (X(0),y(0)), source data{(X(k),y(k))}K
k=1, penalty parameters λωandλδ,
transferring setAm.
Output: The estimator ˆβ.
1Transferring step:
ˆωAm←arg min
ω/braceleftBig
ˆQh(ω) +λω||ω||1/bracerightBig
.
2Debiasing step:
ˆδAm←arg min
δ/braceleftBig
ˆQ(0)
g(ˆωAm+δ) +λδ||δ||1/bracerightBig
.
3return ˆβ=ˆωAm+ˆδAm.
IfAmis unknown, then we need a detection algorithm to ﬁnd useful transferable sets in practice. We propose
a transferrable source detection algorithm which is inspired from the Algorithm 2 in Tian & Feng (2023).
Firstly, partition the target data into qsubsets. Secondly, ﬁt the penalized smoothed quantile regression
on each combination of (q−1)target subsets and calculate the loss on the remaining target subset. In the
following, consider the average cross-validation loss ˆL(0)
0. Run the transferring step on each combination of
(q−1)target subsets and each source data, and evaluate the loss function on the remaining target subset.
Similarly compute the average cross-validation loss ˆL(k)
0for each source. Thirdly, calculate the diﬀerence
between ˆL(0)
0and ˆL(k)
0for eachkand compare it with a predeﬁned threshold. Finally select the sources
6Published in Transactions on Machine Learning Research (01/2024)
Algorithm 2: Transferable Source Detection
Input:Target data (X(0),y(0)), all source data{(X(k),y(k))}K
k=1, a threshold C0, penalty
parameters{{λ(k)[a]}K
k=0}q
a=1, whereqis the number of folds chosen.
Output: The set of transferable sources ˆA.
1Randomly divide (X(0),y(0))intoqequal-sized sets{(X(0)[i],y(0)[i])}q
i=1.
2fora= 1toqdo
3 ˆβ(0)[a]←ﬁt the penalized quantile regression on {(X(0)[i],y(0)[i])}q
i=1\(X(0)[a],y(0)[a])with
penalty parameter λ(0)[a].
4 ˆβ(k)[a]←run the transferring step in Algorithm 1 with
{(X(0)[i],y(0)[i])}q
i=1\(X(0)[a],y(0)[a])∪(X(k),y(k))and penalty parameter λ(k)[a]for allk/negationslash= 0.
5Calculate the loss function ˆL[a]
0(ˆβ(k)[a])on(X(0)[a],y(0)[a])fork= 1,...,K.
6ˆL(k)
0←/summationtextq
a=1ˆL[a]
0(ˆβ(k)[a])/q,ˆL(0)
0←/summationtextq
a=1ˆL[a]
0(ˆβ(0)[a])/q,
ˆσ=/radicalBig/summationtextq
a=1(ˆL[a]
0(ˆβ(k)[a])−ˆL(0)
0)2/(q−1).
7ˆA←{k/negationslash= 0 : ˆL(k)
0−ˆL(0)
0≤C0(ˆσ∨0.01)}.
8return ˆA.
whose diﬀerence is less than the threshold and include them in the set ˆA. The detailed transferable source
detection procedure is summarized in Algorithm 2.
With the transferrable source detection algorithm, we propose a feasible Algorithm 3 in practice, in
which we ﬁrst detect useful source datasets ˆAby Algorithm 2 and then run Algorithm 1 using datasets
{(X(k),y(k))}k∈{0}∪ˆA.
Algorithm 3: Trans-SQR
Input:Target data (X(0),y(0)), all source data{(X(k),y(k))}K
k=1, a threshold C0and penalty
parameters{{λ(k)[a]}K
k=0}q
a=1.
Output: The estimator ˆβ.
1Run Algorithm 2 (Transferable Source Detection Algorithm) and output ˆA.
2Run Algorithm 1 (Oracle Trans-SQR) using data {(X(k),y(k))}k∈{0}∪ˆA.
3return ˆβ.
2.3 The proposed algorithm with an /lscript0-norm constrained transferring set
In this section we consider a more strict transferable set A/prime
m={k:||δ(k)||0≤m}, where the /lscript1-norm
discussed in Section 2.2 is replaced by /lscript0-norm. Compared with the /lscript1-norm, the theoretical analysis of the
transfer learning procedure under /lscript0-norm is free of the restrictive Assumption 3.4 below, which requires
“suﬃcient” similarity between the target covariance matrix and transferable source covariance matrices.
However, as /lscript0-norm is not additive, it is not easy to combine target and source data to estimate a primary
estimator for the true target parameter. Instead, we correct each source data independently and incorporate
the corrected source and target data to make predictions. Certain adjustments need to be made on the
proposed transfer learning procedure in Algorithm 1.
This/lscript0-norm constrained transfer algorithm is inspired by the idea in Li et al. (2021). Unlike the transferring
step in Algorithm 1, the ﬁrst step of the algorithm in this section is to train each source separately to get
primal estimators of ω(k),k∈{1,...,K}, where the smoothed loss function for each source kis
ˆQ(k)
h(ω) =1
nknk/summationdisplay
i=1lh/parenleftbig
y(k)
i−(x(k)
i)Tω/parenrightbig
.
7Published in Transactions on Machine Learning Research (01/2024)
In the second step, as the debiasing step in Algorithm 1, we adjust for the diﬀerences ˆδ(k)for allkusing the
target data, which is obtained via
ˆδ(k)= arg min
δ/braceleftbigg
ˆQ(0)
g(ˆω(k)+δ) +λδ||δ||1/bracerightbigg
,
where the smoothed loss function with respect to δis deﬁned as
ˆQ(0)
g(ˆω(k)+δ) =1
n0n0/summationdisplay
i=1lg/parenleftbig
y(0)
i−(x(0)
i)T(ˆω(k)+δ)/parenrightbig
.
Then a threshold for each ˆδ(k)is computed by only keeping the largest/radicalbig
n0/logpelements of ˆδ(k)and
letting all the other elements be zero. In the third step, with the estimated “bias” from the second step, the
corrected source data has the following form:
/braceleftBig/parenleftBig
X(k),y(k)+X(k)˜δ(k)/parenrightBig/bracerightBigK
k=1.
Then, we combine all the corrected sources and target data to estimate the parameter βwhich is of our
interest. The above algorithm estimate the source parameters and the contrast vectors individually, while in
the Oracle/lscript1-Trans-SQR proposed in Section 2.2, a pooled analysis is conducted with data from target and
sources, which relies on the homogeneous designs of the covariance matrices among target and source data.
Algorithm 4: Oracle Trans-SQR with /lscript0-norm constrained transferring set
Input:Target data (X(0),y(0)), source data{(X(k),y(k))}K
k=1, penalty parameters λω,λδandλβ,
transferring setA/prime
m. Letn=n0+nA/primem.
Output: The estimator ˆβ.
1For eachk∈A/prime
m,
ˆω(k)←arg min
ω/braceleftbigg
ˆQh(ω) +λ(k)
ω||ω||1/bracerightbigg
.
2For eachk∈A/prime
m,
ˆδ(k)←arg min
δ/braceleftbigg
ˆQ(0)
g(ˆω(k)+δ) +λδ||δ||1/bracerightbigg
.
3Threshold ˆδ(k)via˜δ(k)=H√
n0/logp(ˆδ(k)), whereHk(b)is formed by setting all but the largest k
elements ofbto zero.
4Joint estimation using source and target data:
ˆβ←arg min
β/braceleftbigg1
nn0/summationdisplay
i=1lw/parenleftbig
y(0)
i−(x(0)
i)Tβ/parenrightbig
+1
n/summationdisplay
k∈A/primemnk/summationdisplay
i=1lw/parenleftbig
y(k)
i−(x(k)
i)T(β−˜δ(k))/parenrightbig
+λβ||β||1/bracerightbigg
.
5return ˆβ.
3 Statistical theory
In this section, we establish theoretical guarantees on the algorithms in the above section.
8Published in Transactions on Machine Learning Research (01/2024)
Assumption 3.1. There exists ¯f≥f>0such that the conditional density of /epsilon1(k)givenx(k)satisﬁes
f≤f/epsilon1(k)|x(k)(0)≤¯falmost surely (over x(k)) for allk= 0,...,K. Moreover, there exists l0>0such that
|f/epsilon1(k)|x(k)(u)−f/epsilon1(k)|x(k)(v)|≤l0|u−v|for allu,v∈Ralmost surely (over x(k)), andz(k)= (Σ(k))−1/2x(k)
fork= 0,...,K, where Σ(k)denote the covariance matrix of x(k), and
inf
t∈[0,1],v∈Sp−1E/bracketleftBig
f/epsilon1(k)|x(k)/parenleftbig
t(z(k))Tv/parenrightbig/parenleftbig
(z(k))Tv/parenrightbig2/bracketrightBig
≥f.
Assumption 3.2. The kernel function K:R→[0,∞)is symmetric,that is, K(u) =K(−u), and satisﬁes
that/integraltext∞
−∞K(u)du= 1and/integraltext∞
−∞u2K(u)du <∞. Fora= 1,2,..., letκa=/integraltext∞
−∞|u|aK(u)dube thea-th
absolute moment of K(·). Assume supu∈RK(u)≤¯κfor some ¯κ∈(0,1].
Assumption 3.3. Fork= 0,...,K,Σ(k)=E[x(k)(x(k))T]is positive deﬁnite and z(k)= (Σ(k))−1/2x(k)∈
Rpis sub-exponential: there exist constants v0,c0≥1such that P(|(z(k))Tu|≥v0||u||2·t)≤c0e−tfor all
u∈Rpandt≥0. For convenience, we assume c0= 1, and write σ2
x= max 1≤j≤pE(x2
j).
Under Assumption 3.3, the a-th (a≥3) absolute moments of all the one-dimensional marginals of zare
uniformly bounded: µa:= supu∈Sp−1E|(z(k))Tu|a≤a!va
0. In particular, µ1≤µ1/2
2= 1.
Meanwhile, for every δ∈(0,1], deﬁne
ηδ= inf/braceleftBig
η>0 :E/bracketleftBig/parenleftbig
(z(k))Tv/parenrightbig21/parenleftbig
|(z(k))Tv|>η/parenrightbig/bracketrightBig
≤δfor allv∈Sp−1/bracerightBig
. (3)
Since E[(z(k))Tv]2= 1for anyv∈Sp−1,ηδis well-deﬁned for each δ, and depends implicitly on the
distribution of z(k).
Assumption 3.4. Denote
˜Σ =K/summationdisplay
k=0αk/integraldisplay1
0∇2Q(k)((1−t)β∗+tω∗)dt
˜Σ(k)=/integraldisplay1
0∇2Q(k)((1−t)β∗+tω(k))dt,
where∇2Q(k)((1−t)β∗+tω) =E{f/epsilon1|x(tω−tβ∗)·x(k)(x(k))T}. Deﬁne
C1= sup
0≤k≤K||˜Σ−1˜Σ(k)||1.
LetC1be bounded, that is C1<∞.
Assumption 3.1 imposes the Lipschitz continuity on the conditional density f/epsilon1|x(·). Assumption 3.2 holds
for most commonly used kernel functions, for instance, uniform kernel, Gaussian kernel, etc.
Assumption 3.3 assumes a sub-exponential condition on the random covariates characterized by a well-
behaved covariance structure. In particular, µ4can be regarded as the uniform kurtosis parameter.
Assumption 3.4 restricts the diﬀerence between the target covariance matrix and transferable source co-
variance matrix in some sense, which guarantees the estimator at the transferring step is close to the true
parameterβ∗. This assumption is commonly used in other transfer learning works, Tian & Feng (2023); Li
et al. (2022); Zhang & Zhu (2022); Huang et al. (2022).
Formally, we consider the parameter space
Θ(s,m) =/braceleftBig
β∗,{ω(k)}k∈Am:||β∗||0≤s,sup
k∈Am||ω(k)−β∗||1≤m/bracerightBig
.
3.1 Estimation with an /lscript1-norm constrained transferring set
Proposition 3.1. (LocalRestrictedStrongConvexity)AssumeAssumptions3.1-3.3hold. Let ∆ =ω−ω∗,
n=nAm+n0andκ= min|u|≤1K(u)>0. If(r,h,n,d )satisﬁes
max{4η1/4r,32v0γ1/2
1dµ1/2
4}≤h≤f/l0andnh&¯ff−2η2
1/4µ4σ2
xlogp,
9Published in Transactions on Machine Learning Research (01/2024)
for anyω∈ω∗+BΣ(r)andω∗∈ω(k)+B1(d),
ˆQh(ω)−ˆQh(ω∗)−/parenleftbig
∇ˆQh(ω∗)/parenrightbigT(ω−ω∗)≥φ1||∆||2
Σ−φ2/radicalbigg
logp+ logn
nh||∆||1||∆||Σ, (4)
with probability at least 1−(pn)−1, whereφ1=κ·f/10andφ2>0is a constant depending only on (κ,f).
Proposition 3.2. Assume Assumptions 3.1 - 3.3 hold. Let v=β1−β2,φ/prime
1=κ·f/25andφ/prime
2=C2
κ,f/(2φ/prime
1).
Ifmax{4η1/4r,8v0rµ1/2
4}≤g≤¯f/l0withη1/4deﬁnedin(3)and n0g&¯ff−2η2
1/4µ4s, thenforany v∈BΣ(r),
ˆQ(0)
g(β1)−ˆQ(0)
g(β2)−/parenleftbig
∇ˆQ(0)
g(β2)/parenrightbigT(β1−β2)≥α1||v||2
Σ−Cκ,f/radicalBigg
logp+ logn0
n0g||v||1||v||Σ,
with probability at least 1−(pn0)−1, whereCκ,f>0is a constant depending only on (κ,f).
By the arithmetic mean-geometric mean inequality
Cκ,f/radicalBigg
logp+ logn0
n0g||v||1||v||Σ≤φ/prime
1
2||v||2
Σ+C2
κ,f
2φ/prime
1logp+ logn0
n0g||v||2
1,
we have
ˆQ(0)
g(β1)−ˆQ(0)
g(β2)−/parenleftbig
∇ˆQ(0)
g(β2)/parenrightbigT(β1−β2)≥φ/prime
1
2||v||2
Σ−φ/prime
2logp+ logn0
n0g||v||2
1,
with probability at least 1−(pn0)−1.
In the debiasing step, we need another restricted strong convexity condition with both ||·||2
1and||·||2
Σin
the lower bound. Proposition 3.2 provides that kind condition.
Finally, with the above establishments of restricted strong convexity, we are able to obtain the main result
for the two-step transfer learning algorithm on quantile regression.
Theorem 3.1. Assume Assumptions 3.1 - 3.4 hold. Suppose m≤s/radicalbig
log(p)/n0,n0≥Cs2logpand
nAm&n0, whereC > 0is a constant. Also let
log(p)/(nAm+n0).h≤min{f/(2l0κ1),(s1/2λω)1/2}
slog(p)/n0.g≤/parenleftbig
log(p)/n0/parenrightbig1/4.
We takeλω=Cω/radicalbig
log(p)/(nAm+n0),λδ=Cδ/radicalbig
log(p)/n0, whereCωandCδare suﬃciently large con-
stants, then
||ˆβ−β∗||Σ.√m/parenleftbigglogp
n0/parenrightbigg1/4
+√s/parenleftbigglogp
n0/parenrightbigg1/4/parenleftbigglogp
nAm+n0/parenrightbigg1/4
, (5)
||ˆβ−β∗||1.s/radicalBigg
logp
nAm+n0+/parenleftbigglogp
nAm+n0/parenrightbigg1
4√sm+m, (6)
with probability at least 1−p−1.
Remark 3.1. In the trivial case where Amis an empty set, the upper bound in (5) is OP(/radicalbig
slog(p)/n0).
WhenAmis non-empty, the upper bound in (5) is sharper than/radicalbig
slog(p)/n0and the upper bound in (6)
is sharper than s/radicalbig
log(p)/n0, ifnAm&n0andm<s (log(p)/n0)1/2.
The above theorem gives the convergence rate of the Trans-SQR estimator under /lscript1//lscript2-errors. As the above
remarksstated, ifthetotalsamplesizeofthetransferablesourcesissigniﬁcantlylargerthanthetargetsample
size, the Trans-SQR estimator could even achieve a sharper convergence rate with some proper choices of
the transferable level of the contrasts and the smoothing bandwidth in the debiasing step. As some previous
works show, our theorem shares similar estimation error bounds as the results in Tian & Feng (2023) and
Li et al. (2022).
10Published in Transactions on Machine Learning Research (01/2024)
3.2 Estimation with an /lscript0-norm constrained transferring set
Assumption 3.5. Fork= 0,...,K, the covariate vector x(k)is compactly supported with
ζ(k)
p:= sup
x(k)∈Rp/vextenddouble/vextenddouble(Σ(k))−1/2x(k)/vextenddouble/vextenddouble
2<∞,
and||x(k)||∞≤Balmost surely for some B≥1, where Σ(k)is positive deﬁnite. Without loss of generality,
assumeB= 1. In addition, µa= supu∈Sp−1E|(z(k))Tu|a<∞fora= 1,..., 4.
Remark 3.2. Assumption 3.5 is a stronger version of Assumption 3.3. Note that quantile regression has
Hessian matrix∇2ˆQh(β) = (1/n)/summationtextn
i=1Kh(xT
iβ−yi)xixT
i, where ˆQh(β)is the smoothed empirical quantile
loss andKh(u) = (1/h)K(u/h). Unlike the generalized linear regression, there is a smoothing bandwidth hin
the denominator. We import Assumption 3.5 to provide convenience for bounding the diﬀerence ∇ˆQh(β)−
∇ˆQh(β∗).
Proposition 3.3. (RSC in Step 2) Assume Assumptions 3.1, 3.2, 3.5 hold. Let v=β1−β2. If
max{4rη1/4,32v0rµ1/2
4}≤g≤¯f/l0andn0g&¯ff−2η2
1/4µ4s, then for any v∈BΣ(r),
ˆQ(0)
g(β1)−ˆQ(0)
g(β2)−/parenleftbig
∇ˆQ(0)
g(β2)/parenrightbigT(β1−β2)≥0.1κ·f||v||2
Σ,
with probability at least 1−(pn0)−1.
Theorem 3.2. Assume Assumptions 3.1, 3.2, 3.5 hold. Let
log(p)/n0.h≤min{f/(2l0κ1),(s1/2λω)1/2}
(s+m) log(p)/n0.g≤/parenleftbig
(s+m) log(p)/n0/parenrightbig1/4
mlog(p)/n.w≤/parenleftbig
mlog(p)/n/parenrightbig1/4,
wheren=n0+nA/primem. Meanwhile, suppose m≤s,nk≥n0andn0≥Cs2logp, whereC > 0is a constant.
We takeλ(k)
ω=Cω/radicalbig
log(p)/nk,λδ=Cδ/radicalbig
log(p)/n0andλβ=Cβ/radicalbig
log(p)/n, whereCω,CδandCβare
suﬃciently large constants, then
||ˆβ−β∗||Σ./radicalbigg
slogp
n+/radicalbigg
smlogp
n0, (7)
||ˆβ−β∗||1.s/radicalbigg
logp
n+s/radicalbigg
mlogp
n0, (8)
with probability at least 1−p−1.
The above theorem gives the convergence rate of the Trans-SQR estimator under /lscript1//lscript2-errors, where the
contrastvectorsarecharacterizedintermsofthe /lscript0-norm. Ifthesamplesizeofthetargetdataislargeenough
and the total sample size of the transferable sources is signiﬁcantly larger than the target sample size, the
Trans-SQR estimator could achieve a sharp convergence rate with some proper choices of the transferable
level of the contrasts.
Remark 3.3. As mentioned above, Assumption 3.4 is to make sure that the estimation error in the trans-
ferring step is small enough. However, Theorem 3.2 does not require Assumption 3.4 because Algorithm 4
learns the parameter w(k)independently in Step 1 and reduces the bias in Step 2. For Step 1, the upper
bound of the diﬀerence between the estimator ˆw(k)and true parameter w(k)can be controlled by the sample
size of the each source data and the /lscript0transferable level m. For Step 2, the estimated ˆδcould also be closed
enough to the true diﬀerence between the target and source parameter by having an appropriate target
sample size. Therefore, if both the target and source sample sizes are large enough, the error of Algorithm
4 would be well controlled without Assumption 3.4.
11Published in Transactions on Machine Learning Research (01/2024)
4 Numerical studies
In this section, we evaluate the performance of our proposed algorithms via numerical experiments. The
methods in the following section include Smoothed Quantile Regression (SQR) on target data, the Oracle-
Trans-SQR,Am-Trans-SQR and the Naive-Trans-SQR, which naïvely assumes Am= 1,...,Kin the Oracle
Trans-SQR. The purpose of including the Naive-Trans-Lasso is to understand the overall informative level
of the auxiliary samples.
4.1 Transfer learning on an /lscript1-normedAm
We consider p= 500,n0= 200, andn1,...,n 10= 150. The covariates from target x(0)
iare i.i.d. Gaussian
with mean zero and covariance matrix Σwith Σjj/prime= 0.5|j−j/prime|for alli= 1,...,n 0and/epsilon1(0)
iare i.i.d. Gaussian
with mean zero and variance one for all i. Fork∈Am,x(k)
i∼N (0p,Σ+/epsilon1/epsilon1T), where/epsilon1∼N (0p,0.32Ip).
For the target, the true parameter β∗, we sets= 5,βj= 0.5forj∈{1,...,s}, andβj= 0otherwise.
DenoteR(k)
paspindependent Rademacher variables. R(k)
pis independent with R(k/prime)
pfor anyk/negationslash=k/prime. For
any source data kinAm, we let the true parameter ω(k)=β∗+ (m/p)R(k)
p, wherem∈{5,10}. For any
source data k/primenot inAm, the true parameter ω(k/prime)=β∗+ (2m/p)R(k/prime)
p. We train the four methods with
100 reproductions and record their average /lscript2-estimation errors under diﬀerent settings of τ. Figure 2 shows
the changes of the estimation errors along with the amount of the transferable sources.
We observe from Figure 2 that the Oracle-Trans-SQR has the best performance among all the methods and
Am-Trans-SQR has almost the same performance as the Oracle-Trans-SQR, which indicates that the trans-
ferable source detection algorithm still works under the smoothed quantile regression models. Meanwhile,
comparedwithSQRontarget, theestimationerrorsoftheOracle-Trans-SQRand Am-Trans-SQRarealways
smaller, which means that the source data which share some similarities in /lscript1-norm with the target data
could improve the estimation. Another observation is that the performance of Am-Trans-SQR consistently
improves as more and more source data are transferable. This matches the theoretical /lscript2-estimation error
bounds which become sharper as nAmgrows.
4.2 Transfer learning on an /lscript0-normedAm
We consider p= 500,n0= 200, and assume that there are 2,4,6,8,10transferable sources with the sample
sizes 400. The covariates from target x(0)
iare i.i.d. Gaussian with mean zero and covariance matrix Σ
with Σjj/prime= 0.5|j−j/prime|. The covariates from source x(k)
iare also i.i.d. Gaussian with mean zero, but with
covariance matrix Σ+/epsilon1/epsilon1T, where/epsilon1∼N(0p,0.32Ip). For the target, the true parameter β∗, we sets= 5,
βj= 1forj∈{1,...,s}, andβj= 0otherwise. For the source, their true parameter w(k)is generated from
w(k)
j=β∗
j+ ∆ 1(j∈M), whereMis a random subset of [p]with|M|=m. We takem∈{2,4}, and ∆ = 2.
Figure 4 and 5 show the /lscript2-estimation errors in diﬀerent settings of m.
From the results, Trans-SQR with /lscript0-norm constrained transferring set has better performances than SQR
only on target and SQR on all sources and target. Meanwhile, when the target data sample size n0becomes
larger, the performance of Trans-SQR increases quickly, which accords with our results that the estimation
error is depend on the target sample size. There are considerable decreases in estimation errors of Trans-
SQR when the transferable level increases or ∆increases, which corresponds to the diﬀerence on components
between target and source populations.
5 Conclusion
This paper studies transfer learning for high-dimensional quantile regression models, employing convolution-
type smoothing techniques. The proposed algorithms focus on leveraging /lscript1//lscript0-normed transferable source
populations to improve estimation accuracy of the target regression coeﬃcients. We derive error bounds
for the estimators in terms of /lscript1//lscript2-norms for the algorithms. Theoretical analysis reveals that these error
bounds surpass those of the classical penalized quantile regression estimator, which only utilizes the target
12Published in Transactions on Machine Learning Research (01/2024)
Figure2:/lscript2estimationerrorsofseveralmethodsunderquantilelevels τ= 0.25,0.5,0.75, over100repetitions,
where Oracle-Trans-SQR is Algorithm 1.
data, provided that the target and source populations exhibit suﬃcient similarity. Furthermore, we propose
a transferable source detection algorithm to identify informative sources from the available sources when the
set of informative sources is unknown. Numerical experiments validate our theoretical results.
Acknowledgments
Wen-Xin Zhou is supported by the NSF grant NSF DMS-2401268. Yong He is supported by NSF China
(12171282) and Qilu Young Scholars Program of Shandong University.
13Published in Transactions on Machine Learning Research (01/2024)
Figure 3:/lscript2estimation errors of several methods for Gaussian and t2errors, over 100 repetitions.
14Published in Transactions on Machine Learning Research (01/2024)
Figure 4:/lscript2estimation errors of several methods with /lscript0constraints for t2errors, over 100 repetitions.
15Published in Transactions on Machine Learning Research (01/2024)
Figure 5:/lscript2estimation errors of several methods with /lscript0constraints for Gaussian errors, over 100 repetitions.
16Published in Transactions on Machine Learning Research (01/2024)
References
Hamsa Bastani. Predicting with proxies: Transfer learning in high dimension. Management Science , 67(5):
2964–2984, 2021.
Alexandre Belloni and Victor Chernozhukov. /lscript1-penalized quantile regression in high-dimensional sparse
models. The Annals of Statistics , 39(1):82–130, 2011.
OlivierBousquet. Concentrationinequalitiesforsub-additivefunctionsusingtheentropymethod. In Stochas-
tic inequalities and applications , pp. 213–247. Springer, 2003.
T Tony Cai and Hongji Wei. Transfer learning for nonparametric classiﬁcation: Minimax rate and adaptive
classiﬁer. The Annals of Statistics , 49(1):100–128, 2021.
Xuming He, Xiaoou Pan, Kean Ming Tan, and Wen-Xin Zhou. Smoothed quantile regression with large-scale
inference. Journal of Econometrics , 2021.
Yong He, Qiushi Li, Qinqin Hu, and Lei Liu. Transfer learning in high-dimensional semiparametric graphical
models with application to brain connectivity analysis. Statistics in medicine , 41(21):4112–4129, 2022.
Joel L Horowitz. Bootstrap methods for median regression models. Econometrica , pp. 1327–1351, 1998.
JiayuHuang, MingqiuWang, andYuanshanWu. Transferlearningwithhigh-dimensionalquantileregression.
arXiv preprint arXiv: 2211.14578 , 2022.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes , volume 23.
Springer Science & Business Media, 1991.
Sai Li, Tianxi Cai, and Rui Duan. Targeting underrepresented populations in precision medicine a federated
transfer learning approach. arXiv preprint arXiv:2108.12112 , 2021.
Sai Li, T Tony Cai, and Hongzhe Li. Transfer learning for high-dimensional linear regression: Prediction,
estimation and minimax optimality. Journal of the Royal Statistical Society. Series B, Statistical Method-
ology, 84(1):149–173, 2022.
Sai Li, T Tony Cai, and Hongzhe Li. Transfer learning in large-scale gaussian graphical models with false
discovery rate control. Journal of the American Statistical Association , 118(543):2171–2183, 2023.
Lu Lin and Weiyu Li. A correlation-ratio transfer learning and variational stein’s paradox. arXiv preprint
arXiv:2206.06086 , 2022.
Po-Ling Loh and Martin J Wainwright. Regularized m-estimators with nonconvexity: Statistical and algo-
rithmic theory for local optima. Journal of Machine Learning Research , 16:559–616, 2015.
Henry WJ Reeve, Timothy I Cannings, and Richard J Samworth. Adaptive transfer learning. The Annals
of Statistics , 49(6):3618–3649, 2021.
Kean Ming Tan, Lan Wang, and Wen-Xin Zhou. High-dimensional quantile regression: Convolution smooth-
ing and concave regularization. Journal of the Royal Statistical Society Series B , 84(1):205–233, 2022.
Ye Tian and Yang Feng. Transfer learning under high-dimensional generalized linear models. Journal of the
American Statistical Association , 118(544):2684–2697, 2023.
Lisa Torrey and Jude Shavlik. In handbook of research on machine learning applications and trends: algo-
rithms, methods, and techniques: algorithms, methods, and techniques .IGI global , pp. 242–264, 2010.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science , vol-
ume 47. Cambridge university press, 2018.
Xiao-TongYuan, PingLi, andTongZhang. Gradienthardthresholdingpursuit. Journal of Machine Learning
Research , 18(166):1–43, 2018.
Yijiao Zhang and Zhongyi Zhu. Transfer learning for high-dimensional quantile regression via convolution
smoothing. arXiv preprint arXiv:2212.00428 , 2022.
17Published in Transactions on Machine Learning Research (01/2024)
A Appendix: Proofs of the main results
A.1 Technical Lemmas
Forω∈Rp, suppose ∆ =ω−ω∗. Deﬁne
ˆRh(∆) = ˆQh(ω)−ˆQh(ω∗)−/parenleftbig
∇ˆQh(ω∗)/parenrightbigT(ω−ω∗),
ˆDh(∆) = ˆQh(ω)−ˆQh(ω∗),
and their population counterparts Rh(∆) = E{ˆRh(∆)}andDh(∆) = E{ˆDh(∆)}, whereω∗is the true
parameter of the transferring step in the algorithm.
Lemma A.1. Letβ∗be the true target parameter, then ||ω∗−β∗||1≤C1m, whereC1= supk||˜Σ−1˜Σ(k)||1
and˜Σ−1,˜Σ(k)are given in Assumption 3.4.
Note thatw∗has the explicit form, w∗=β∗+δ∗. Lemma A.1 gives an upper bound of the distance between
the trueβ∗and the true estimate in transferring step. In other words, the /lscript1-norm ofδ∗is controlled by m.
Lemma A.2. Deﬁneπ∗
h=πh(β∗)∈Rp, whereπh(β) =∇ˆQh(β)−∇Qh(β). Assumptions 3.1 - 3.3 ensure
that for any t>0,
||π∗
h||∞≤σ/radicalBigg
{τ(1−τ) +Ch2}2t
nAm+n0+ max(1−τ,τ)t
nAm+n0,
with probability at least 1−2pe−t, whereC= (τ+ 1)l0κ2andσ= max 1≤j≤pσjj.
In both transferring and debiased steps, we need to restrict the regularization parameters λω(orλδ) to be
no smaller than 2||π∗
h||∞(or2||π∗
g||∞). This Lemma helps to specify the choice of the parameters.
Lemma A.3. Deﬁneb∗
h=||Σ−1/2∇Qh(ω∗)||2, which quantiﬁes the smoothing bias, then for some κ2>0
b∗
h≤l0κ2h2
2,
wherel0is the Lipschitz constant of the density f/epsilon1|x(·).
Lemma A.4. Forr,l> 0, deﬁne
ψ(r,l) = sup
β∈β∗+BΣ(r)∩B1(l)/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1/braceleftbig¯Kh(xT
iβ−yi)−¯Kh(xT
iβ∗−yi)/bracerightbig
xi/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞.
For anyt>0, with probability at least 1−e−t,
ψ(r,l).l
h/radicalbigg
logp
n+¯f1/2r/radicalbigg
t+ logp
nh+t+ logp
n.
A.2 Proof of Proposition 3.1
Deﬁne the Taylor error
T(ω,ω∗) =ˆQh(ω)−ˆQh(ω∗)−/parenleftbig
∇ˆQh(ω∗)/parenrightbigT(ω−ω∗).
In the following proofs, we consider the subset of ω,ω∈ω∗+BΣ(r)∩CΣ(l), andω∗∈ω(k)+B1(d).
18Published in Transactions on Machine Learning Research (01/2024)
It follows from a second-order Taylor expansion that
T(ω,ω∗)
=1
2(ω−ω∗)T∇2ˆQh/parenleftbig
tω+ (1−t)ω∗/parenrightbig
(ω−ω∗)
=1
2(nAm+n0)/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1Kh/braceleftbig
y(k)
i−(x(k)
i)T(t(k)
iω+ (1−t(k)
i)ω∗)/bracerightbig/parenleftbig
(x(k)
i)T(ω−ω∗)/parenrightbig2
=1
2(nAm+n0)/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1Kh/braceleftbig
/epsilon1i−t(k)
i(x(k)
i)T(ω−ω∗)−(x(k)
i)T(ω∗−ω(k))/bracerightbig
·/parenleftbig
(x(k)
i)T(ω−ω∗)/parenrightbig2,
for somet(k)
i∈[0,1]. For eachiandk, deﬁne the event Fi,k,
Fi,k=/braceleftbig
|/epsilon1i|≤h/4/bracerightbig
∩/braceleftbig
|(x(k)
i)T(ω−ω∗)|≤||ω−ω∗||Σ·h/(2r)/bracerightbig
∩/braceleftbig
|(x(k)
i)T(ω∗−ω(k))|≤h/4/bracerightbig
,
for allω−ω∗∈BΣ(r). Thus
T(ω,ω∗)≥κ
2(nAm+n0)h/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1/parenleftbig
(x(k)
i)T(ω−ω∗)/parenrightbig21Fi,k, (9)
whereκ= min|u|≤1K(u). For a truncation level R> 0, deﬁne functions
ϕR(u) =

u2|u|≤R
2,
(R−|u|)2R
2<|u|≤R,
0|u|>R.
By this construction, ϕR(u)≤u2· 1{|u|≤R},ϕcR(cu) =c2ϕR(u)andϕRis R-Lipschitz.
In addition, we deﬁne the trapezoidal function
ψR(u) =

1|u|≤R
2,
2−2
R|u|R
2<|u|≤R,
0|u|>R,
and note that ψRis(2/R)-Lipschitz and ψR(u)≤ 1{|u|≤R}.
With the two new-deﬁned function and the notation ∆ =ω−ω∗,n=nAm+n0, we have established the
lower bound of (9)
T(ω,ω∗)
≥κ
2nh||∆||2
Σ/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=11{|/epsilon1i|≤h/4}ϕ||∆||Σ·h/(2r)/parenleftbig
(x(k)
i)T∆/parenrightbig
ψh/4/parenleftbig
(x(k)
i)T(ω∗−ω(k))/parenrightbig
≥κ
2||∆||2
Σ·1
nh/summationdisplay
k,i1{|/epsilon1i|≤h/4}ϕh/(2r)/parenleftbig
(x(k)
i)T∆/||∆||Σ/parenrightbig
ψh/4/parenleftbig
(x(k)
i)T(ω∗−ω(k))/parenrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
D0(ω,ω∗)(10)
In the following proofs, we bound ED0(ω,ω∗)andD0(ω,ω∗)−ED0(ω,ω∗), respectively. First, we show
that
ED0(ω,ω∗)≥0.21f. (11)
19Published in Transactions on Machine Learning Research (01/2024)
Note that
/vextendsingle/vextendsingle/vextendsingleh
2f/epsilon1|x(0)/vextendsingle/vextendsingle/vextendsingle−/vextendsingle/vextendsingle/vextendsingleE/bracketleftbig
1{|/epsilon1i|≤h/4}|x(k)
i/bracketrightbig/vextendsingle/vextendsingle/vextendsingle≤/vextendsingle/vextendsingle/vextendsingleE/bracketleftbig
1{|/epsilon1i|≤h/4}|x(k)
i/bracketrightbig
−h
2f/epsilon1|x(0)/vextendsingle/vextendsingle/vextendsingle
≤/integraldisplayh/4
−h/4|f/epsilon1|x(t)−f/epsilon1|x(0)|dt
≤l0h2
16.
Hence we obtain/vextendsingle/vextendsingle/vextendsingleE[ 1{|/epsilon1i|≤h/4}|x(k)
i]/vextendsingle/vextendsingle/vextendsingle≥h
2f−l0h2
16.
Providedh≤f/l0≤¯f/l0, we have
/vextendsingle/vextendsingle/vextendsingleE[ 1{|/epsilon1i|≤h/4}|x(k)
i]/vextendsingle/vextendsingle/vextendsingle≥7fh
16.
Meanwhile/vextendsingle/vextendsingle/vextendsingleE[ 1{|/epsilon1i|≤h/4}|x(k)
i]/vextendsingle/vextendsingle/vextendsingle−/vextendsingle/vextendsingle/vextendsingleh
2f/epsilon1|x(0)/vextendsingle/vextendsingle/vextendsingle≤/integraldisplayh/4
−h/4|f/epsilon1|x(t)−f/epsilon1|x(0)|dt
implies
/vextendsingle/vextendsingle/vextendsingleE[ 1{|/epsilon1i|≤h/4}|x(k)
i]/vextendsingle/vextendsingle/vextendsingle≤9¯fh
16.
Then
ED0(ω,ω∗)
=1
nh/summationdisplay
k,iE/bracketleftBig
E/bracketleftbig
1{|/epsilon1i|≤h/4}/vextendsingle/vextendsinglex(k)
i/bracketrightbig
ϕh/(2r)/parenleftbig
(x(k)
i)T∆/||∆||Σ/parenrightbig
ψh/4/parenleftbig
(x(k)
i)T(ω∗−ω(k))/parenrightbig/bracketrightBig
≥7f
16E/bracketleftBig
ϕh/(2r)/parenleftbig
xT∆/||∆||Σ/parenrightbig
ψh/4/parenleftbig
xT(ω∗−ω(k))/parenrightbig/bracketrightBig
≥7f
16E/bracketleftBig/parenleftbig
xT∆/||∆||Σ/parenrightbig21/braceleftbig
|xT∆/||∆||Σ|≤h/(4r)/bracerightbig
·ψh/4/parenleftbig
xT(ω∗−ω(k))/parenrightbig/bracketrightBig
≥7f
16/braceleftbigg
1−E/bracketleftBig/parenleftbig
xT∆/||∆||Σ/parenrightbig21/braceleftbig
|xT∆/||∆||Σ|>h/ (4r)/bracerightbig/bracketrightBig
−E/bracketleftBig/parenleftbig
xT∆/||∆||Σ/parenrightbig21/braceleftbig
|xT(ω∗−ω(k))|>h/ 8/bracerightbig/bracketrightBig/bracerightbigg
≥7f
16/braceleftbigg
1−E/bracketleftBig/parenleftbig
xT∆/||∆||Σ/parenrightbig21/braceleftbig
|xT∆/||∆||Σ|>h/ (4r)/bracerightbig/bracketrightBig
−µ1/2
4P/parenleftbig
|xT(ω∗−ω(k))|>h/ 8/parenrightbig1/2/bracerightbigg
.
By the deﬁnition of ηδ, as long as 0<r≤h/(4η1/4),
sup
∆∈BΣ(r)E/bracketleftBig/parenleftbig
xT∆/||∆||Σ/parenrightbig21/braceleftbig/vextendsingle/vextendsinglexT∆/||∆||Σ/vextendsingle/vextendsingle>h/ (4r)/bracerightbig/bracketrightBig
≤1
4.
Moreover,ω∗∈ω(k)+B1(d). Hence
/vextenddouble/vextenddoubleΣ1/2(ω∗−ω(k))/vextenddouble/vextenddouble
2≤/vextenddouble/vextenddoubleΣ1/2/vextenddouble/vextenddouble
2/vextenddouble/vextenddoubleω∗−ω(k)/vextenddouble/vextenddouble
2≤γ1/2
1d.
Under Assumption 3.3 with v0≥1, the tail bounds of sub-exponential z= Σ−1/2ximplys that
P/parenleftbig
|xT(ω∗−ω(k))|>h/ 8/parenrightbig
≤2 exp/braceleftbigg
−h
8v0γ1/2
1d/bracerightbigg
.
20Published in Transactions on Machine Learning Research (01/2024)
Leth≥32v0γ1/2
1dµ1/2
4. It then follows from a numerical calculation that
/braceleftbigg
1−E/bracketleftBig/parenleftbig
xT∆/||∆||Σ/parenrightbig21/braceleftbig
|xT∆/||∆||Σ|>h/ (4r)/bracerightbig/bracketrightBig
−µ1/2
4P/parenleftbig
|xT(ω∗−ω(k))|>h/ 8/parenrightbig1/2/bracerightbigg
≥0.49
holds uniformly over ∆∈BΣ(r)∩CΣ(l). Putting together the pieces yields
ED0(ω,ω∗)>0.21f.
Next we ﬁnd a lower bound of D0(ω,ω∗)− ED0(ω,ω∗)overω∈ω∗+BΣ(r)∩CΣ(l). Deﬁne
Ω(r,l) = sup
ω∈ω∗+BΣ(r)∩CΣ(l){−D0(ω,ω∗) +ED0(ω,ω∗)}.
WriteD0(ω,ω∗) =n−1/summationtext
k∈Am∪{0}/summationtextnk
i=1wi,k(ω,ω∗), where
wi,k(ω,ω∗) =h−11{|/epsilon1i|≤h/4}ϕh/(2r)/parenleftbig
(x(k)
i)T∆/||∆||Σ/parenrightbig
ψh/4/parenleftbig
(x(k)
i)T(ω∗−ω(k))/parenrightbig
satisﬁes 0≤wi,k(ω,ω∗)≤h/(4r)2, sinceϕR(u)≤(R/2)2andψR(u)∈[0,1]. Moreover,
Ew2
i,k(ω,ω∗) =E/bracketleftBig
h−21{|/epsilon1i|≤h/4}ϕ2
h/(2r)/parenleftbig
(x(k)
i)T∆/||∆||Σ/parenrightbig
ψ2
h/4/parenleftbig
(x(k)
i)T(ω∗−ω(k))/parenrightbig/bracketrightBig
≤9¯f
16h·Eϕ2
h/(2r)/parenleftbig
(x(k)
i)T∆/||∆||Σ/parenrightbig
≤9¯f
16h·E/parenleftbig
(x(k)
i)T∆/||∆||Σ/parenrightbig4=9¯fµ4
16h.
Using Bousquet’s version of Talagrand’s inequality yields that, for any z>0,
Ω(r,l)≤EΩ(r,l) +{EΩ(r,l)}1/21
2r/radicalbigg
hz
n+3√
2µ1/2
4
4/radicalBigg
¯fz
nh+h
(4r)2z
3n
≤EΩ(r,l) +1
4EΩ(r,l) +1
4r2hz
n+3√
2µ1/2
4
4/radicalBigg
¯fz
nh+h
(4r)2z
3n
≤5
4EΩ(r,l) +3√
2µ1/2
4
4/radicalBigg
¯fz
nh+13
3hz
(4r)2n(12)
holds with probability at least 1−e−z. To bound the expectation EΩ(r,l), using Rademacher symmetrization
and the connection between Gaussian and Rademacher complexities, Lemma 5.5 in Ledoux & Talagrand
(1991), we have
EΩ(r,l)≤2/radicalbiggπ
2E/bracketleftBigg
sup
(ω,ω∗)∈Λ(r,l)Gω,ω∗/bracketrightBigg
, (13)
where
Gω,ω∗
:=1
nh/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1ei 1{|/epsilon1i|≤h/4}ϕh/(2r)/parenleftbig
(x(k)
i)T∆/||∆||Σ/parenrightbig
ψh/4/parenleftbig
(x(k)
i)T(ω∗−ω(k))/parenrightbig
,
andeiare independent standard normal variables. Note that Gω,ω∗is a Gaussian process conditioned on
{(y(k)
i,x(k)
i)}nk
i=1fork∈Am∪{0}. For (ω,ω∗)and(ω/prime,ω/prime∗), write ∆ =ω−ω∗,∆/prime=ω/prime−ω/prime∗and
21Published in Transactions on Machine Learning Research (01/2024)
χi= 1{|/epsilon1i|≤h/4}, then
Gω,ω∗−Gω/prime,ω/prime∗
=Gω,ω∗−Gω/prime,ω/prime+∆+Gω/prime,ω/prime+∆−Gω/prime,ω/prime∗
=1
nh/summationdisplay
k,ieiχiϕh/(2r)/parenleftbig
(x(k)
i)T∆/||∆||Σ/parenrightbig/braceleftbig
ψh/4/parenleftbig
(x(k)
i)T(ω∗−ω(k))/parenrightbig
−ψh/4/parenleftbig
(x(k)
i)T(ω/prime∗−ω(k))/parenrightbig/bracerightbig
+1
nh/summationdisplay
k,ieiχiψh/4/parenleftbig
(x(k)
i)T(ω/prime∗−ω(k))/parenrightbig/braceleftbig
ϕh/(2r)/parenleftbig
(x(k)
i)T∆/||∆||Σ/parenrightbig
−ϕh/(2r)/parenleftbig
(x(k)
i)T∆/prime/||∆/prime||Σ/parenrightbig/bracerightbig
.
Note thatϕRandψRare Lipschitz continuous, and ϕR(u)≤(R/2)2. LetE∗be the conditional expectation
given{(y(k)
i,x(k)
i)}nk
i=1. Consequently,
E∗/parenleftbig
Gω,ω∗−Gω/prime,ω/prime+∆/parenrightbig2≤1
(nh)2/parenleftbigg8
h/parenrightbigg2/parenleftbiggh
4r/parenrightbigg4/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1χi/parenleftbig
(x(k)
i)T(ω−ω/prime)/parenrightbig2
=1
4r4n2/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1χi/parenleftbig
(x(k)
i)T(ω−ω/prime)/parenrightbig2(14)
and
E∗/parenleftbig
Gω/prime,ω/prime+∆−Gω/prime,ω/prime∗/parenrightbig2(15)
≤1
(nh)2/parenleftbiggh
2r/parenrightbigg2/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1χi/braceleftbig
(x(k)
i)T∆/||∆||Σ−(x(k)
i)T∆/prime/||∆/prime||Σ/bracerightbig2
=1
4r2n2/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1χi/braceleftbig
(x(k)
i)T(∆/||∆||Σ−∆/prime/||∆/prime||Σ)/bracerightbig2. (16)
Motivated by the last two inequalities, we have
E∗/parenleftbig
Gω,ω∗−Gω/prime,ω/prime∗/parenrightbig2≤1
2r4n2/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1χi/parenleftbig
(x(k)
i)T(ω−ω/prime)/parenrightbig2
+1
2r2n2/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1χi/braceleftbig
(x(k)
i)T(∆/||∆||Σ−∆/prime/||∆/prime||Σ)/bracerightbig2.
Deﬁne another Gaussian process Zω,ω∗as
Zω,ω∗=1
21/2r2n/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1e/prime
iχi(x(k)
i)T(ω∗−ω(k))
+1
21/2rn/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1e/prime/prime
iχi(x(k)
i)T∆/||∆||Σ
such that E∗(Gω,ω∗−Gω/prime,ω/prime∗)2≤E∗(Zω,ω∗−Zω/prime,ω/prime∗)2, where{e/prime
i}and{e/prime/prime
i}are two dependent copies of
{ei}. Applying Theorem 7.2.11 in Vershynin (2018), we obtain
E∗/parenleftBig
sup
ω,ω∗Gω,ω∗/parenrightBig
≤E∗/parenleftBig
sup
ω,ω∗Zω,ω∗/parenrightBig
. (17)
22Published in Transactions on Machine Learning Research (01/2024)
To bound the supremum of Zω,ω∗, using the cone constraint and ||ω∗−ω(k)||1≤d, we have
E∗/parenleftBig
sup
ω,ω∗Zω,ω∗/parenrightBig
≤√
2d
2r2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1e/prime
iχix(k)
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞
+√
2l
2rE/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1e/prime/prime
iχix(k)
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞. (18)
Thus, by (13) (17) and (18), we have
EΩ(r,l)≤√π/braceleftbiggd
r2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1e/prime
iχix(k)
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞+l
rE/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n/summationdisplay
k∈Am∪{0}nk/summationdisplay
i=1e/prime/prime
iχix(k)
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞/bracerightbigg
.(19)
It remains to ﬁnd the bound of the two /lscript∞-norm terms on the right-hand side of (19). Note that the variable
|n−1/summationtext
k∈Am∪{0}/summationtextnk
i=1e/prime
iχix(k)
ij|is zero-mean for j= 1,...,p.
E/bracketleftbigg
exp/parenleftbigg
λ/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
knk/summationdisplay
i=1e/prime
iχix(k)
ij/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg/bracketrightbigg
≤/productdisplay
knk/productdisplay
i=1E/bracketleftbigg
exp/parenleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleλe/prime
iχix(k)
ij
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg/bracketrightbigg
≤/productdisplay
knk/productdisplay
i=1exp/braceleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleλ2χ2
i(x(k)
ij)2
2n2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracerightbigg
= exp/braceleftbiggλ2
2n2/summationdisplay
knk/summationdisplay
i=1χ2
i(x(k)
ij)2/bracerightbigg
.
Thus,|n−1/summationtext
k∈Am∪{0}/summationtextnk
i=1e/prime
iχix(k)
ij|is sub-Gaussian with parameter n−1/radicalBig/summationtext
k/summationtextnk
i=1χ2
i(x(k)
ij)2.
Applying Lemma 15 in Loh & Wainwright (2015), we have
E/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n/summationdisplay
knk/summationdisplay
i=1e/prime
iχix(k)
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞/vextendsingle/vextendsingle/vextendsingle/vextendsinglex(k)
i/bracketrightbigg
≤c
n·max
j=1,...,p/radicaltp/radicalvertex/radicalvertex/radicalbt/summationdisplay
knk/summationdisplay
i=1χ2
i(x(k)
ij)2·/radicalbig
logp
≤c√logp
n/radicaltp/radicalvertex/radicalvertex/radicalbt/summationdisplay
knk/summationdisplay
i=1χ2
i(x(k)
ij)2,
implying that
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n/summationdisplay
knk/summationdisplay
i=1e/prime
iχix(k)
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤c/radicalbigg
logp
n·E/bracketleftBigg/radicalBigg/summationtext
k/summationtextnk
i=1χ2
i(x(k)
ij)2
n/bracketrightBigg
≤c/radicalbigg
logp
n·/radicalBigg
E/bracketleftbigg/summationtext
k/summationtextnk
i=1χ2
i(x(k)
ij)2
n/bracketrightbigg
≤cσx/radicalbigg
logp
n·/radicalBigg
9¯fh
16(20)
Similarly,
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n/summationdisplay
knk/summationdisplay
i=1e/prime/prime
iχix(k)
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤cσx/radicalbigg
logp
n·/radicalBigg
9¯fh
16. (21)
Finally, if we take r=h/(4η1/4),d=h(γ1µ4)−1/2/(32v0)andz=t+ logp, combining (19), (20), (21) with
Bousquet’s version inequality (12), we conclude that
Ω(r,l)≤0.01f+c/primel/radicalBig/parenleftbig
t+ logp/parenrightbig
/(nh)
23Published in Transactions on Machine Learning Research (01/2024)
with probability at least 1−p−1e−tfor anyt>0, as long as
nh&¯ff−2η2
1/4µ4σ2
xlog(p).
This, together with (9), (10) and (11), we have
T(ω,ω∗)≥κ
2||∆||2
Σ/bracketleftBigg
0.21f−/parenleftbigg
0.01f+c/primel/radicalbigg
t+ logp
nh/parenrightbigg/bracketrightBigg
≥κ
2||∆||2
Σ/parenleftbigg
0.2f−c/primel/radicalbigg
t+ logp
nh/parenrightbigg
(22)
with probability at least 1−p−1e−t.
It remains to extend this bound to one that is uniform in the ratio ||∆||1/||∆||Σ, which we do via a peeling
argument. Consider the inequality
1
||∆||2
ΣT(ω,ω∗)≥κ·f
10−2c/prime/prime||∆||1
||∆||Σ/radicalbigg
t+ logp
nh. (23)
Since
γ−1/2
1≤||∆||1
||∆||Σ≤κ·f
20c/prime/prime/radicalBigg
nh
t+ logp:=ζ,
we deﬁne the set
Θk=/braceleftbig
∆∈Rp:γ−1/2
12k−1≤||∆||1/||∆||Σ≤γ−1/2
12k/bracerightbig
,
for eachk= 1,...,N :=⌈log2(c/radicalbig
nh/logp)⌉to let
/braceleftBigg
∆∈Rp:γ−1/2
1≤||∆||1
||∆||Σ≤ζ/bracerightBigg
⊆∪N
k=1Θk.
By the union bound, we then have
P/braceleftBigg
∃∆∈/braceleftbig
∆∈Rp:γ−1/2
1≤||∆||1/||∆||Σ≤ζ/bracerightbig
s.t.κ·f
10−1
||∆||2
ΣT(ω,ω∗)>2c/prime/prime||∆||1
||∆||Σ/radicalbigg
t+ logp
nh/bracerightBigg
≤N/summationdisplay
k=1P/braceleftBigg
∃∆∈Θks.t.κ·f
10−1
||∆||2
ΣT(ω,ω∗)>c/prime/primeγ−1/2
12k/radicalbigg
t+ logp
nh/bracerightBigg
≤N/summationdisplay
k=1P/braceleftBigg
sup
||∆||1/||∆||Σ≤γ−1/2
12kκ·f
10−1
||∆||2
ΣT(ω,ω∗)>c/prime/primeγ−1/2
12k/radicalbigg
t+ logp
nh/bracerightBigg
≤N/summationdisplay
k=1p−1e−t≤N·p−1e−t.
Takingt= log{log2(c/radicalbig
nh/logp)}+uyields that with probability at least 1−p−1e−u,
κ·f
10−1
||∆||2
ΣT(ω,ω∗)≤2c/prime/prime||∆||1
||∆||Σ/radicalBigg
logp+ log{log2(c/radicalbig
nh/logp)}+u
nh.
Multiplying by||∆||2
Σon both sides yields
T(ω,ω∗)≥κ·f
10||∆||2
Σ−2c/prime/prime||∆||1||∆||Σ/radicalBigg
logp+ log{log2(c/radicalbig
nh/logp)}+u
nh,
wherec/prime/prime>0is a constant depending only on (κ,f).
24Published in Transactions on Machine Learning Research (01/2024)
A.3 Proof of Proposition 3.2
The Taylor error around β2in the direction β1−β2is given by
T(β1,β2) =ˆQ(0)
g(β1)−ˆQ(0)
g(β2)−/parenleftbig
∇ˆQ(0)
g(β2)/parenrightbigT(β1−β2).
For a given kernel function K(·)and bandwidth g >0, the smoothed quantile loss ˆQ(0)
gcan be written as
(n0g)−1/summationtextn0
i=1/integraltext∞
−∞ρτ(u)K{(u+ (x(0)
i)Tβ−y(0)
i)/g}du. Therefore
T(β1,β2) =1
2n0n0/summationdisplay
i=1Kg/braceleftbig
/epsilon1i−(x(0)
i)T/parenleftbig
(1−t)β1+tβ2−β∗/parenrightbig/bracerightbig/braceleftbig
(x(0)
i)T(β1−β2)/bracerightbig2
=1
2n0n0/summationdisplay
i=1Kg/braceleftbig
/epsilon1i−t(x(0)
i)T(β2−β1)−(x(0)
i)T(β1−β∗)/bracerightbig/braceleftbig
(x(0)
i)T(β1−β2)/bracerightbig2,
for somet∈[0,1], For eachi, deﬁne the event Ei,
Ei={|/epsilon1i|≤g/4}∩/braceleftbig/vextendsingle/vextendsingle(x(0)
i)T(β1−β2)/vextendsingle/vextendsingle≤g||β1−β2||Σ/(2r)/bracerightbig
∩/braceleftbig/vextendsingle/vextendsingle(x(0)
i)T(β1−β∗)/vextendsingle/vextendsingle≤g/4/bracerightbig
,
for allβ1−β2∈BΣ(r). Thus
T(β1,β2)≥κ
2n0gn0/summationdisplay
i=1/braceleftbig
(x(0)
i)Tδ/bracerightbig21Ei, (24)
whereδ=β1−β2. For a truncation level R> 0, deﬁne functions
ϕR(u) =

u2|u|≤R
2,
(R−|u|)2R
2<|u|≤R,
0|u|>R.
By this construction, ϕR(u)≤u2· 1{|u|≤R},ϕcR(cu) =c2ϕR(u)andϕRis R-Lipschitz.
In addition, we deﬁne the trapezoidal function
ψR(u) =

1|u|≤R
2,
2−2
R|u|R
2<|u|≤R,
0|u|>R,
and note that ψRis(2/R)-Lipschitz and ψR(u)≤ 1{|u|≤R}.
From these two new-deﬁned function, (24) implies
T(β1,β2)≥κ
2n0g||δ||2
Σn0/summationdisplay
i=11{|/epsilon1i|≤g/4}ϕg||δ||Σ/(2r)/parenleftbig
(x(0)
i)Tδ/parenrightbig
ψg/4/parenleftbig
(x(0)
i)T(β1−β∗)/parenrightbig
≥κ
2||δ||2
Σ·1
n0gn0/summationdisplay
i=11{|/epsilon1i|≤g/4}ϕg/(2r)/parenleftbig
(x(0)
i)Tδ/||δ||Σ/parenrightbig
ψg/4/parenleftbig
(x(0)
i)T(β1−β∗)/parenrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
D0(β1,β2)(25)
In the following proofs, we bound ED0(β1,β2)andD0(β1,β2)−ED0(β1,β2), respectively. Note that
/vextendsingle/vextendsingle/vextendsingleg
2f/epsilon1|x(0)/vextendsingle/vextendsingle/vextendsingle−/vextendsingle/vextendsingle/vextendsingleE[ 1{|/epsilon1i|≤g/4}|x(0)
i]/vextendsingle/vextendsingle/vextendsingle≤/vextendsingle/vextendsingle/vextendsingleE[ 1{|/epsilon1i|≤g/4}|x(0)
i]−g
2f/epsilon1|x(0)/vextendsingle/vextendsingle/vextendsingle
≤/integraldisplayg/4
−g/4|f/epsilon1|x(t)−f/epsilon1|x(0)|dt.
25Published in Transactions on Machine Learning Research (01/2024)
Then,
/vextendsingle/vextendsingle/vextendsingleg
2f/epsilon1|x(0)/vextendsingle/vextendsingle/vextendsingle−/vextendsingle/vextendsingle/vextendsingleE[ 1{|/epsilon1i|≤g/4}|x(0)
i]/vextendsingle/vextendsingle/vextendsingle≤l0g2
16/vextendsingle/vextendsingle/vextendsingleE[ 1{|/epsilon1i|≤g/4}|x(0)
i]/vextendsingle/vextendsingle/vextendsingle≥g
2f−l0g2
16.
Providedg≤f/l0≤¯f/l0, we have
/vextendsingle/vextendsingle/vextendsingleE[ 1{|/epsilon1i|≤g/4}|x(0)
i]/vextendsingle/vextendsingle/vextendsingle≥7fg
16.
Meanwhile/vextendsingle/vextendsingle/vextendsingleE[ 1{|/epsilon1i|≤g/4}|x(0)
i]/vextendsingle/vextendsingle/vextendsingle−/vextendsingle/vextendsingle/vextendsingleg
2f/epsilon1|x(0)/vextendsingle/vextendsingle/vextendsingle≤/integraldisplayg/4
−g/4|f/epsilon1|x(t)−f/epsilon1|x(0)|dt
implies/vextendsingle/vextendsingle/vextendsingleE[ 1{|/epsilon1i|≤g/4}|x(0)
i]/vextendsingle/vextendsingle/vextendsingle≤9¯fg
16.
Then
ED0(β1,β2)
=1
n0gn0/summationdisplay
i=1E/bracketleftBig
E[ 1{|/epsilon1i|≤g/4}|x(0)
i]ϕg/(2r)/parenleftbig
(x(0)
i)Tδ/||δ||Σ/parenrightbig
ψg/4/parenleftbig
(x(0)
i)T(β1−β∗)/parenrightbig/bracketrightBig
≥7f
16E/bracketleftBig
ϕg/(2r)(xTδ/||δ||Σ)ψg/4/parenleftbig
xT(β1−β∗)/parenrightbig/bracketrightBig
≥7f
16/braceleftbigg
1−E/bracketleftBig
(xTδ/||δ||Σ)21/braceleftbig
|xTδ/||δ||Σ|>g/(4r)/bracerightbig/bracketrightBig
−E/bracketleftBig
(xTδ/||δ||Σ)21/braceleftbig
|xT(β1−β∗)|>g/8/bracerightbig/bracketrightBig/bracerightbigg
.
≥7f
16/braceleftbigg
1−E/bracketleftBig/parenleftbig
xTδ/||δ||Σ/parenrightbig21/braceleftbig
|xTδ/||δ||Σ|>g/(4r)/bracerightbig/bracketrightBig
−µ1/2
4P/parenleftbig
|xT(β1−β∗)|>g/8/parenrightbig1/2/bracerightbigg
.
By the deﬁnition of ηδ, as long as 0<r≤g/(4η1/4),
sup
δ∈BΣ(r)E/bracketleftBig
(xTδ/||δ||Σ)21/braceleftbig
|xTδ/||δ||Σ|>g/(4r)/bracerightbig/bracketrightBig
≤1
4.
Moreover,β1∈β∗+BΣ(r/2). Under Assumption 3.3 with v0≥1, the tail bounds of sub-exponential
z= Σ−1/2ximplys that
P/parenleftbig
|xT(β1−β∗)|>g/8/parenrightbig
≤2 exp/braceleftbigg
−g
4v0r/bracerightbigg
.
Letg≥8v0rµ1/2
4and then it follows from a numerical calculation that
/braceleftbigg
1−E/bracketleftBig/parenleftbig
xTδ/||δ||Σ/parenrightbig21/braceleftbig
|xTδ/||δ||Σ|>g/(4r)/bracerightbig/bracketrightBig
−µ1/2
4P/parenleftbig
|xT(β1−β∗)|>g/8/parenrightbig1/2/bracerightbigg
≥0.23
holds uniformly over ∆∈BΣ(r)∩CΣ(l). Putting together the pieces yields
ED0(β1,β2)>0.1f. (26)
Next we ﬁnd a lower bound of D0(β1,β2)− ED0(β1,β2)over Λ(r,l) :={(β1,β2) :β1∈β∗+BΣ(r/2),β2∈
β1+BΣ(r)∩CΣ(l),supp(β1)⊆S}. Deﬁne
Ω(r,l) = sup
(β1,β2)∈Λ(r,l){−D0(β1,β2) +ED0(β1,β2)}.
26Published in Transactions on Machine Learning Research (01/2024)
WriteD0(β1,β2) = (1/n0)/summationtextn0
i=1wi(β1,β2), where
wi(β1,β2) =g−11{|/epsilon1i|≤g/4}ϕg/(2r)/parenleftbig
(x(0)
i)Tδ/||δ||Σ/parenrightbig
ψg/4/parenleftbig
(x(0)
i)T(β1−β∗)/parenrightbig
satisﬁes 0≤wi(β1,β2)≤g/(4r)2, sinceϕR(u)≤(R/2)2andψR(u)∈[0,1]. Moreover,
Ew2
i(β1,β2) =E/bracketleftBig
g−21{|/epsilon1i|≤g/4}ϕ2
g/(2r)/parenleftbig
(x(0)
i)Tδ/||δ||Σ/parenrightbig
ψ2
g/4/parenleftbig
(x(0)
i)T(β1−β∗)/parenrightbig/bracketrightBig
≤9¯f
16g·Eϕ2
g/(2r)/parenleftbig
(x(0)
i)Tδ/||δ||Σ/parenrightbig
≤9¯f
16g·E/parenleftbig
(x(0)
i)Tδ/||δ||Σ/parenrightbig4=9¯fµ4
16g.
Using Bousquet’s version of Talagrand’s inequality yields that, for any z>0,
Ω(r,l)≤EΩ(r,l) +{EΩ(r,l)}1/21
2r/radicalbigggz
n0+3√
2µ1/2
4
4/radicalBigg
¯fz
n0g+g
(4r)2z
3n0
≤EΩ(r,l) +1
4EΩ(r,l) +1
4r2gz
n0+3√
2µ1/2
4
4/radicalBigg
¯fz
n0g+g
(4r)2z
3n0
≤5
4EΩ(r,l) +3√
2µ1/2
4
4/radicalBigg
¯fz
n0g+13
3gz
(4r)2n0(27)
holds with probability at least 1−e−z. To bound the expectation EΩ(r,l), using Rademacher symmetrization
and the connection between Gaussian and Rademacher complexities, Lemma 5.5 in Ledoux & Talagrand
(1991), we have
EΩ(r,l)≤2/radicalbiggπ
2E/bracketleftBigg
sup
(β1,β2)∈Λ(r,l)Gβ1,β2/bracketrightBigg
, (28)
where Gβ1,β2:= (n0g)−1/summationtextn0
i=1ei 1{|/epsilon1i|≤g/4}ϕg/(2r)((x(0)
i)Tδ/||δ||Σ)ψg/4((x(0)
i)T(β1−β∗))andeiare inde-
pendent standard normal variables. Note that Gβ1,β2is a Gaussian process conditioned on {(y(0)
i,x(0)
i)}n0
i=1
andGβ∗,β∗= 0. For (β1,β2)and(β/prime
1,β/prime
2), writeδ=β1−β2,δ/prime=β/prime
1−β/prime
2andχi= 1{|/epsilon1i|≤g/4}, then
Gβ1,β2−Gβ/prime
1,β/prime
2
=Gβ1,β2−Gβ/prime
1,β/prime
1+δ+Gβ/prime
1,β/prime
1+δ−Gβ/prime
1,β/prime
2
=1
n0gn0/summationdisplay
i=1eiχiϕg/(2r)/parenleftbig
(x(0)
i)Tδ/||δ||Σ/parenrightbig/braceleftBig
ψg/4/parenleftbig
(x(0)
i)T(β1−β∗)/parenrightbig
−ψg/4/parenleftbig
(x(0)
i)T(β/prime
1−β∗)/parenrightbig/bracerightBig
+1
n0gn0/summationdisplay
i=1eiχiψg/4/parenleftbig
(x(0)
i)T(β/prime
1−β∗)/parenrightbig/braceleftBig
ϕg/(2r)/parenleftbig
(x(0)
i)Tδ/||δ||Σ/parenrightbig
−ϕg/(2r)/parenleftbig
(x(0)
i)Tδ/prime/||δ/prime||Σ/parenrightbig/bracerightBig
.
Note thatϕRandψRare Lipschitz continuous, and ϕR(u)≤(R/2)2. LetE∗be the conditional expectation
given{(y(0)
i,x(0)
i)}n0
i=1. Consequently,
E∗/parenleftbig
Gβ1,β2−Gβ/prime
1,β/prime
1+δ/parenrightbig2≤1
(n0g)2/parenleftbigg8
g/parenrightbigg2/parenleftbiggg
4r/parenrightbigg4n0/summationdisplay
i=1χi/parenleftbig
(x(0)
i)T(β1−β/prime
1)/parenrightbig2
=1
4r4n2
0n0/summationdisplay
i=1χi/parenleftbig
(x(0)
i)T(β1−β/prime
1)/parenrightbig2(29)
and
E∗/parenleftbig
Gβ/prime
1,β/prime
1+δ−Gβ/prime
1,β/prime
2/parenrightbig2≤1
(n0g)2/parenleftbiggg
2r/parenrightbigg2n0/summationdisplay
i=1χi/parenleftbig
(x(0)
i)Tδ/||δ||Σ−(x(0)
i)Tδ/prime/||δ/prime||Σ/parenrightbig2
=1
4r2n2
0n0/summationdisplay
i=1χi/parenleftbig
(x(0)
i)T(δ/||δ||Σ−δ/prime/||δ/prime||Σ)/parenrightbig2. (30)
27Published in Transactions on Machine Learning Research (01/2024)
Motivated by the last two inequalities, we have
E∗/parenleftbig
Gβ1,β2−Gβ/prime
1,β/prime
2/parenrightbig2
≤1
2r4n2
0n0/summationdisplay
i=1χi/parenleftbig
(x(0)
i)T(β1−β/prime
1)/parenrightbig2+1
2r2n2
0n0/summationdisplay
i=1χi/parenleftbig
(x(0)
i)T(δ/||δ||Σ−δ/prime/||δ/prime||Σ)/parenrightbig2.
Deﬁne another Gaussian process Zβ1,β2as
Zβ1,β2=1√
2r2n0n0/summationdisplay
i=1e/prime
iχi(x(0)
i)T(β1−β∗) +1√
2rn0n0/summationdisplay
i=1e/prime/prime
iχi(x(0)
i)T(β2−β1)/||δ||Σ
=1√
2r2n0n0/summationdisplay
i=1e/prime
iχi(x(0)
i,S)T(β1−β∗)S+1√
2rn0n0/summationdisplay
i=1e/prime/prime
iχi(x(0)
i)T(β2−β1)/||δ||Σ
such that E∗(Gβ1,β2−Gβ/prime
1,β/prime
2)2≤E∗(Zβ1,β2−Zβ/prime
1,β/prime
2)2, where{e/prime
i}and{e/prime/prime
i}are two dependent copies
of{ei}. The second equlity holds since supp(β1),supp(β∗)⊆S. Applying Theorem 7.2.11 in Vershynin
(2018), we obtain
E∗/parenleftBig
sup
β1,β2Gβ1,β2/parenrightBig
≤E∗/parenleftBig
sup
β1,β2Zβ1,β2/parenrightBig
. (31)
To bound the supremum of Zβ1,β2, using the cone constraint and β1∈β∗+BΣ(r/2), we have
E∗/parenleftBig
sup
β1,β2Zβ1,β2/parenrightBig
≤√
2
4rE/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n0n0/summationdisplay
i=1e/prime
iχiS−1/2x(0)
i,S/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2+√
2l
2rE/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n0n0/summationdisplay
i=1e/prime/prime
iχix(0)
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞
≤√
2
4r/radicalBigg
9¯fg
16s
n0+√
2l
2rE/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n0n0/summationdisplay
i=1e/prime/prime
iχix(0)
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞, (32)
whereS= ΣSS=E(xSxT
S). Thus, by (28) (31) and (32), we have
EΩ(r,l)≤√π/braceleftbigg3
8r/radicalBigg
¯fgs
n0+l
rE/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n0n0/summationdisplay
i=1e/prime/prime
iχix(0)
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞/bracerightbigg
. (33)
It remains to ﬁnd the bound of the second term on the right-hand side of (33). Note that the variable
|n−1
0/summationtextn0
i=1e/prime/prime
iχix(0)
ij|is zero-mean for j= 1,...,p.
E/bracketleftbigg
exp/parenleftbigg
λ/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n0n0/summationdisplay
i=1e/prime/prime
iχix(0)
ij/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg/bracketrightbigg
≤n0/productdisplay
i=1E/bracketleftbigg
exp/parenleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleλe/prime/prime
iχix(0)
ij
n0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg/bracketrightbigg
≤n0/productdisplay
i=1exp/braceleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleλ2χ2
i(x(0)
ij)2
2n2
0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracerightbigg
= exp/braceleftbiggλ2
2n2
0n0/summationdisplay
i=1χ2
i(x(0)
ij)2/bracerightbigg
.
Thus,|n−1
0/summationtextn0
i=1e/prime/prime
iχix(0)
ij|is sub-Gaussian with parameter n−1
0/radicalBig/summationtextn0
i=1χ2
i(x(0)
ij)2. Applying Lemma 15 in
Loh & Wainwright (2015), for some universal constant c1>0, we have
E/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n0n0/summationdisplay
i=1e/prime/prime
iχix(0)
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞/vextendsingle/vextendsingle/vextendsingle/vextendsinglex(0)
i/bracketrightbigg
≤c1
n0·max
j=1,...,p/radicaltp/radicalvertex/radicalvertex/radicalbtn0/summationdisplay
i=1χ2
i(x(0)
ij)2·/radicalbig
logp
≤c1√logp
n0/radicaltp/radicalvertex/radicalvertex/radicalbtn0/summationdisplay
i=1χ2
i(x(0)
ij)2,
28Published in Transactions on Machine Learning Research (01/2024)
implying that
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n0n0/summationdisplay
i=1e/prime/prime
iχix(0)
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤c1/radicalbigg
logp
n0·E/bracketleftBigg/radicalBigg/summationtextn0
i=1χ2
i(x(0)
ij)2
n0/bracketrightBigg
≤c1/radicalbigg
logp
n0·/radicalBigg
E/bracketleftbigg/summationtextn0
i=1χ2
i(x(0)
ij)2
n0/bracketrightbigg
≤c1σx/radicalbigg
logp
n0·/radicalBigg
9¯fg
16
Therefore,
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
n0n0/summationdisplay
i=1e/prime/prime
iχix(0)
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤3c1σx
4/radicalBigg
¯fglogp
n0. (34)
Plug this bound to (33), we obtain
EΩ(r,l)≤√π/braceleftBigg
3
8r/radicalBigg
¯fgs
n0+3c1σxl
4r/radicalBigg
¯fglogp
n0/bracerightBigg
. (35)
Finally, if we take r= min{g/(4η1/4),g/(8v0µ1/2
4)}andz=t+logp, combining (35) with Bousquet’s version
inequality (27), we conclude that
Ω(r,l)≤√π/braceleftBigg
15
32r/radicalBigg
¯fgs
n0+15c1σxl
16r/radicalBigg
¯fglogp
n0/bracerightBigg
+3√
2µ1/2
4
4/radicalBigg
¯fz
n0g+13
3gz
(4r)2n0
≤0.02f+c/primel/radicalbig
(t+ logp)/(n0g)
with probability at least 1−p−1e−tas long asn0g&¯ff−2η2
1/4µ4σ2
xs. This, together with (24), (25) and
(26), we have
T(β1,β2)≥κ
2||δ||2
Σ/bracketleftBigg
0.1f−/parenleftBigg
0.02f+c/primel/radicalBigg
t+ logp
n0g/parenrightBigg/bracketrightBigg
≥κ
2||δ||2
Σ/parenleftbigg
0.08f−c/primel/radicalBigg
t+ logp
n0g/parenrightbigg
(36)
with probability at least 1−p−1e−t.
It remains to extend this bound to one that is uniform in the ratio ||δ||1/||δ||Σ, which we do via a peeling
argument. Consider the inequality
T(β1,β2)≥κ·f
25||δ||2
Σ−2c/prime/prime||δ||1||δ||Σ/radicalBigg
t+ logp
n0g/parenrightbigg
1
||δ||2
ΣT(β1,β2)≥κ·f
25−2c/prime/prime||δ||1
||δ||Σ/radicalBigg
t+ logp
n0g
κ·f
25−1
||δ||2
ΣT(β1,β2)≤2c/prime/prime||δ||1
||δ||Σ/radicalBigg
t+ logp
n0g.
For positive integers k= 1,...,N :=⌈log2(c/radicalbig
n0g/logp)⌉, deﬁne the set Θk={δ∈Rp:γ−1/2
12k−1≤
||δ||1/||δ||Σ≤γ−1/2
12k}, so that
/braceleftBigg
δ∈Rp:γ−1/2
1≤||δ||1
||δ||Σ≤κ·f
50c/prime/prime/radicalbiggn0g
t+ logp:=ζ/prime/bracerightBigg
⊆∪N
k=1Θk.
29Published in Transactions on Machine Learning Research (01/2024)
Then
P/braceleftBigg
∃δ∈/braceleftbig
δ∈Rp:γ−1/2
1≤||δ||1/||δ||Σ≤ζ/prime/bracerightbig
s.t.
κ·f
25−1
||δ||2
ΣT(β1,β2)>2c/prime/prime||δ||1
||δ||Σ/radicalBigg
t+ logp
n0g/bracerightBigg
≤N/summationdisplay
k=1P/braceleftBigg
∃δ∈Θks.t.κ·f
25−1
||δ||2
ΣT(β1,β2)>c/prime/primeγ−1/2
12k/radicalBigg
t+ logp
n0g/bracerightBigg
≤N/summationdisplay
k=1P/braceleftBigg
sup
||δ||1/||δ||Σ≤γ−1/2
12kκ·f
25−1
||δ||2
ΣT(β1,β2)>c/prime/primeγ−1/2
12k/radicalBigg
t+ logp
n0g/bracerightBigg
≤N/summationdisplay
k=1p−1e−t≤⌈log2(c/radicalbig
n0g/logp)⌉p−1e−t.
Takingt= log{log2(c/radicalbig
n0g/logp)}+uyields that with probability at least 1−p−1e−u,
κ·f
25−1
||δ||2
ΣT(β1,β2)≤2c/prime/prime||δ||1
||δ||Σ/radicalBigg
logp+ log{log2(c/radicalbig
n0g/logp)}+u
n0g.
Multiplying by||δ||2
Σon both sides yields
T(β1,β2)≥κ·f
25||δ||2
Σ−2c/prime/prime||δ||1||δ||Σ/radicalBigg
logp+ log{log2(c/radicalbig
n0g/logp)}+u
n0g,
wherec/prime/prime>0is a constant depending only on (κ,f).
A.4 Proof of Theorem 3.1
Transferring step: Letω∗be the true parameter of the transferring step and Sbe the active set of β∗
with cardinality s. The symmetric Bregman divergence between ˆωAmandω∗is deﬁned as
/parenleftBig
∇ˆQh(ˆωAm)−∇ˆQh(ω∗)/parenrightBigT
(ˆωAm−ω∗)≥0. (37)
Letˆ∆ = ˆωAm−ω∗. By optimality, there exists a subgradient ˆν∈∂/summationtextn
i=1qλω(|ωi|), such that
∇ˆQh(ˆωAm) +λωˆν= 0.
Then (37) is equivalent to
−/parenleftbig
∇ˆQh(ω∗)/parenrightbigTˆ∆−λωˆνTˆ∆≥0. (38)
Note that
ˆνT(ω∗−ˆωAm)≤||ω∗||1−||ˆωAm||1=||ω∗
S||1+||ω∗
Sc||1−||ˆ∆ +ω∗||1
=||ω∗
S||1+||ω∗
Sc||1−||ˆ∆S+ω∗
S||1−||ˆ∆Sc+ω∗
Sc||1
≤||ˆ∆S||1−||ˆ∆Sc||1+ 2||ω∗
Sc||1.
30Published in Transactions on Machine Learning Research (01/2024)
Then,
/parenleftbig
∇ˆQh(ˆωAm)−∇ˆQh(ω∗)/parenrightbigTˆ∆
=λωˆνT(ω∗−ˆωAm) +/parenleftbig
∇ˆQh(ω∗)−∇Qh(ω∗)/parenrightbigT(ω∗−ˆωAm) +/parenleftbig
∇Qh(ω∗)/parenrightbigT(ω∗−ˆωAm)
≤λω/parenleftBig
||ˆ∆S||1−||ˆ∆Sc||1+ 2||ω∗
Sc||1/parenrightBig
+||∇ˆQh(ω∗)−∇Qh(ω∗)||∞/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
||π∗
h||∞||ˆ∆||1
+||Σ−1/2∇Qh(ω∗)||2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
b∗
h||ˆ∆||Σ. (39)
Conditioned on the event {λω≥2||π∗
h||∞}, (39) becomes
/parenleftbig
∇ˆQh(ˆωAm)−∇ˆQh(ω∗)/parenrightbigTˆ∆≤λω/parenleftBig
||ˆ∆S||1−||ˆ∆Sc||1+ 2||ω∗
Sc||1/parenrightBig
+λω
2||ˆ∆||1+b∗
h||ˆ∆||Σ.
Lemma A.1 implies ||ω∗
Sc||1≤C1m, so we have
/parenleftbig
∇ˆQh(ˆωAm)−∇ˆQh(ω∗)/parenrightbigTˆ∆≤3
2λω||ˆ∆S||1−1
2λω||ˆ∆Sc||1+ 2λωC1m+b∗
h||ˆ∆||Σ.
Since (∇ˆQh(ˆωAm)−∇ˆQh(ω∗))Tˆ∆≥0,/hatwide∆satisﬁes the constraint ||ˆ∆Sc||1≤3||ˆ∆S||1+4C1m+2λ−1
ωb∗
h||ˆ∆||Σ,
from which it follows that
||ˆ∆||1≤4s1/2||ˆ∆||2+ 4C1m+ 2λ−1
ωb∗
h||ˆ∆||Σ. (40)
Now we claim that when λω≥2||π∗
h||∞, with probability at least 1−p−1, it holds that
||ˆ∆||Σ≤8φ2C1m
φ1/radicalBigg
logp+ log(nAm+n0)
nAm+n0+3λωγ1/2
1s1/2+ 2b∗
h
φ1+ 2/radicalBigg
C1λωm
φ1. (41)
If the claim does not hold , consider C={∆ : 1.5λω||∆S||1−0.5λω||∆Sc||1+ 2λωC1m+b∗
h||∆||Σ≥0}. For
anyt∈(0,1),
1
2λω||tˆ∆Sc||1=t·1
2λω||ˆ∆Sc||1≤t·/parenleftbigg3
2λω||ˆ∆S||1+ 2λωC1m+b∗
h||ˆ∆||Σ/parenrightbigg
≤3
2λω||tˆ∆S||1+ 2λωC1m+b∗
h||tˆ∆||Σ,
which implies that tˆ∆∈C. We could ﬁnd some tsatisfying that||t/hatwide∆||Σ≤1and
||tˆ∆||Σ>8φ2C1m
φ1/radicalBigg
logp+ log(nAm+n0)
nAm+n0+3λωγ1/2
1s1/2+ 2b∗
h
φ1+ 2/radicalBigg
C1λωm
φ1.
Denote ˜∆ =tˆ∆andF(∆) = ˆQh(ω∗+∆)−ˆQh(ω∗)+λω(||ω∗+∆||1−||ω∗||1). SinceF(0) = 0andF(ˆ∆)≤0,
by convexity,
F(˜∆) =F(tˆ∆ + (1−t)0)≤tF(ˆ∆)≤0.
However,
F(˜∆) = ˆDh(˜∆)−λω||ω∗||1+λω||ω∗+˜∆||1
=ˆRh(˜∆) +/parenleftbig
∇ˆQh(ω∗)/parenrightbigT˜∆−λω||ω∗||1+λω||ω∗+˜∆||1
=ˆRh(˜∆)−/parenleftBig
λω||ω∗||1−λω||ω∗+˜∆||1−/parenleftbig
∇ˆQh(ω∗)−∇Qh(ω∗)/parenrightbigT˜∆
−/parenleftbig
∇Qh(ω∗)/parenrightbigT˜∆/parenrightBig
.
31Published in Transactions on Machine Learning Research (01/2024)
Then by Proposition 3.1 and (39),
F(˜∆)≥φ1||˜∆||2
Σ−φ2/radicalbigg
logp+ logn
nh||˜∆||1||˜∆||Σ−3
2λω||˜∆S||1+1
2λω||˜∆Sc||1
−2λωC1m−b∗
h||˜∆||Σ
≥φ1||˜∆||2
Σ−φ2/radicalbigg
logp+ logn
nh||˜∆||1||˜∆||Σ−3
2λω||˜∆S||1−2λωC1m−b∗
h||˜∆||Σ.
Note that||˜∆S||1≤s1/2||˜∆||2≤γ1/2
1s1/2||˜∆||Σand (40). Therefore, when
(nAm+n0)h>16φ−2
1φ2
2(logp+ logn) max{16sγ1,4λ−2
ω(b∗
h)2},
we haveφ2/radicalbig
(logp+ logn)/(nh)(4√sγ1/2
1+ 2λ−1
ωb∗
h)≤φ1/2. Then it follows that,
F(˜∆)≥1
2φ1||˜∆||2
Σ−/parenleftbigg
4φ2/radicalbigg
logp+ logn
nhC1m+3
2λωγ1/2
1s1/2+b∗
h/parenrightbigg
||˜∆||Σ−2λωC1m
>0,
which contradicts with F(˜∆)≤0. Therefore the claim holds.
It remains to control the probability of the event {λω≥||π∗
h||∞}and the probability of the local RSC
condition. By Lemma A.2, we pick
λω= 2/bracketleftBigg
σ/radicalBigg
{τ(1−τ) +Ch2}4 log(2p)
nAm+n0+ max(1−τ,τ)2 log(2p)
nAm+n0/bracketrightBigg
,
so that{λω≥2||π∗
h||∞}. From Lemma A.3, we have b∗
h≤Ch2. Now with probability at least 1−(pn)−1,
||ˆ∆||Σ.m/radicalBigg
logp+ log(nAm+n0)
nAm+n0+/radicalBigg
slogp
nAm+n0+h2+/parenleftbigglogp
nAm+n0/parenrightbigg1
4√m.
We then let h2≤s1/2λω, so that
||ˆ∆||Σ.m/radicalBigg
logp+ log(nAm+n0)
nAm+n0+/radicalBigg
slogp
nAm+n0+/parenleftbigglogp
nAm+n0/parenrightbigg1
4√m, (42)
with probability at least 1−(pn)−1.
Note that
||ˆ∆||1≤4||ˆ∆S||1+ 4C1m+ 2λ−1
ωb∗
h||/hatwide∆||Σ
≤4√s||ˆ∆||2+ 4C1m+ 2λ−1
ωb∗
h||/hatwide∆||Σ
≤(4√sγ1/2
1+l0κ2λ−1
ωh2)||ˆ∆||Σ+ 4C1m,
which encloses
||ˆ∆||1.m/radicalBigg
slog(p) +slog(nAm+n0)
nAm+n0+s/radicalBigg
logp
nAm+n0+/parenleftbigglogp
nAm+n0/parenrightbigg1
4√sm+m,
with probability at least 1−(pn)−1.
Debiasing step: Denoteδ∗=β∗−ω∗,ˆδAm=ˆβ−ˆωAmandˆvAm=ˆδAm−δ∗.
32Published in Transactions on Machine Learning Research (01/2024)
Similar to (39), we have
/parenleftbig
∇ˆQ(0)
g(ˆωAm+ˆδAm)−∇ˆQ(0)
g(β∗)/parenrightbigT(ˆβ−β∗)
≤λδ/parenleftbig
||β∗−ˆωAm||1−||ˆβ−ˆωAm||1/parenrightbig
+/vextenddouble/vextenddouble∇ˆQ(0)
g(β∗)−∇Q(0)
g(β∗)/vextenddouble/vextenddouble
∞/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
||π∗g||∞/vextenddouble/vextenddoubleˆβ−β∗/vextenddouble/vextenddouble
1
+/vextenddouble/vextenddoubleΣ−1/2∇Q(0)
g(β∗)/vextenddouble/vextenddouble
2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
b∗g/vextenddouble/vextenddoubleˆβ−β∗/vextenddouble/vextenddouble
Σ
≤3
2λδ||β∗−ˆωAm||1−1
2λδ||ˆβ−ˆωAm||1+b∗
g/vextenddouble/vextenddoubleˆβ−β∗/vextenddouble/vextenddouble
Σ
≤3
2λδ||β∗−ω∗||1+3
2λδ||ˆ∆||1−1
2λδ||ˆβ−ˆωAm||1+b∗
g/vextenddouble/vextenddoubleˆβ−β∗/vextenddouble/vextenddouble
Σ
≤3
2λδCm+3
2λδ||ˆ∆||1−1
2λδ||ˆβ−ˆωAm||1+b∗
g/vextenddouble/vextenddoubleˆβ−β∗/vextenddouble/vextenddouble
Σ. (43)
On the other hand,
/parenleftbig
∇ˆQ(0)
g(ˆωAm+ˆδAm)−∇ˆQ(0)
g(β∗)/parenrightbigT(ˆβ−β∗)
≤λδ/parenleftbig
||β∗−ˆωAm||1−||ˆβ−ˆωAm||1/parenrightbig
+λδ
2/vextenddouble/vextenddoubleˆβ−β∗/vextenddouble/vextenddouble
1+b∗
g/vextenddouble/vextenddoubleˆβ−β∗/vextenddouble/vextenddouble
Σ
≤λδ||β∗
S−ˆωAm
S||1+λδ||β∗
Sc−ˆωAm
Sc||1−λδ||ˆβS−ˆωAm
S||1−λδ||ˆβSc−ˆωAm
Sc||1
+λδ
2/vextenddouble/vextenddoubleˆβ−β∗/vextenddouble/vextenddouble
1+b∗
g/vextenddouble/vextenddoubleˆβ−β∗/vextenddouble/vextenddouble
Σ
≤λδ/parenleftBig
||β∗
S−ˆωAm
S||1−||ˆβS−ˆωAm
S||1/parenrightBig
−λδ/parenleftBig
||β∗
Sc−ˆωAm
Sc||1+||ˆβSc−ˆωAm
Sc||1/parenrightBig
+ 2λδ||β∗
Sc−ˆωAm
Sc||1+λδ
2||ˆβ−β∗||1+b∗
g/vextenddouble/vextenddoubleˆβ−β∗/vextenddouble/vextenddouble
Σ
≤λδ||β∗
S−ˆβS||1−λδ||β∗
Sc−ˆβSc||1+ 2λδ||β∗
Sc−ω∗
Sc||1+ 2λδ||ˆ∆Sc||1
+λδ
2||ˆβ−β∗||1+b∗
g/vextenddouble/vextenddoubleˆβ−β∗/vextenddouble/vextenddouble
Σ
≤3
2λδ||β∗
S−ˆβS||1−1
2λδ||β∗
Sc−ˆβSc||1+ 2λδC1m+ 2λδ||ˆ∆Sc||1+b∗
g/vextenddouble/vextenddoubleˆβ−β∗/vextenddouble/vextenddouble
Σ
≤3
2λδ||β∗
S−ˆβS||1−1
2λδ||β∗
Sc−ˆβSc||1+ 2λδC1m+b∗
g/vextenddouble/vextenddoubleˆβ−β∗/vextenddouble/vextenddouble
Σ
+ 2λδ/parenleftBigg
m/radicalBigg
slog(p) +slog(nAm+n0)
nAm+n0+s/radicalBigg
logp
nAm+n0+/parenleftbigglogp
nAm+n0/parenrightbigg1
4√sm+m/parenrightBigg
.(44)
Thus
||β∗−ˆβ||1≤4γ1/2
1√s||β∗−ˆβ||Σ+ 2b∗
gλ−1
δ||β∗−ˆβ||Σ
+ 4/parenleftBigg
m/radicalBigg
slog(p) +slog(nAm+n0)
nAm+n0+s/radicalBigg
logp
nAm+n0+/parenleftbigglogp
nAm+n0/parenrightbigg1
4√sm+m/parenrightBigg
(45)
Setr=g/(48c)for somec > 0andR= (4γ1/2
1√s+ 2b∗
gλ−1
δ)r+ 4C√s, ifm≤C√sfor some positive
constantCandnAm+n0≥slogp. Denote Θ(r,R) =BΣ(r)∩CΣ(R)and ˜β= (1−η)β∗+ηˆβ, where
η= sup{u∈[0,1] :β∗+u(ˆβ−β∗)∈β∗+ Θ(r,R)}. If ˆβ/∈Θ(r,R), thenη∈(0,1)and ˜βfalls onto the
boundary of Θ(r,R); otherwise ˜β=ˆβ.
Combining (43) and Proposition 3.2, we have
α1
2||˜β−β∗||2
Σ−α2·logp+ logn0
n0g||˜β−β∗||2
1≤3
2λδCm+3
2λδ||˜∆||1+b∗
g/vextenddouble/vextenddouble˜β−β∗/vextenddouble/vextenddouble
Σ.(46)
33Published in Transactions on Machine Learning Research (01/2024)
Besides, (43) implies
||˜β−ˆωAm||1≤3Cm+ 3||ˆ∆||1+2b∗
g
λδ/vextenddouble/vextenddouble˜β−β∗/vextenddouble/vextenddouble
Σ.
As a result,
||˜β−β∗||1≤||ˆβ−ˆωAm||1+||ˆωAm−β∗||1
≤4Cm+ 4||ˆ∆||1+2b∗
g
λδ/vextenddouble/vextenddouble˜β−β∗/vextenddouble/vextenddouble
Σ.
Letα=α1−4b∗2
gλ−2
δ, then (46) becomes
α1||˜β−β∗||2
Σ≤2α2·logp+ logn0
n0g(16Cm2+ 16||ˆ∆||2
1+ 4b∗2
gλ−2
δ/vextenddouble/vextenddouble˜β−β∗/vextenddouble/vextenddouble2
Σ)
+ 3λδCm+ 3λδ||ˆ∆||1+ 2b∗
g/vextenddouble/vextenddouble˜β−β∗/vextenddouble/vextenddouble
Σ
α||˜β−β∗||2
Σ−2b∗
g/vextenddouble/vextenddouble˜β−β∗/vextenddouble/vextenddouble
Σ+b∗2
g
α.logp
n0g(m2+||ˆ∆||2
1) +λδm+λδ||ˆ∆||1
α/parenleftbigg
||˜β−β∗||Σ−b∗
g
α/parenrightbigg2
.logp
n0g/parenleftBig
m2+||ˆ∆||2
1/parenrightBig
+λδm+λδ||ˆ∆||1.
Thus,
||˜β−β∗||Σ./radicalBigg
logp
n0g(m+||ˆ∆||1) +/radicalbig
λδm+/radicalBig
λδ||ˆ∆||1+b∗
g.
Letλδ=C/radicalbig
log(p)/n0andg/equivasymptotic(log(p)/n0)1/4, then
||˜β−β∗||Σ./parenleftbigglogp
n0/parenrightbigg3/8
||ˆ∆||1+√m/parenleftbigglogp
n0/parenrightbigg1/4
+/parenleftbigglogp
n0/parenrightbigg1/4/radicalBig
||ˆ∆||1+/parenleftbigglogp
n0/parenrightbigg1/2
.m/parenleftbigglogp
n0/parenrightbigg3/8
+s/parenleftbigglogp
n0/parenrightbigg3/8/radicalBigg
logp
nAm+n0
+√sm/parenleftbigglogp
n0/parenrightbigg3/8/parenleftbigglogp
nAm+n0/parenrightbigg1/4
+√m/parenleftbigglogp
n0/parenrightbigg1/4
+√s/parenleftbigglogp
n0/parenrightbigg1/4/parenleftbigglogp
nAm+n0/parenrightbigg1/4
+ (sm)1/4/parenleftbigglogp
n0/parenrightbigg1/4/parenleftbigglogp
nAm+n0/parenrightbigg1/8
.
Ifn0>s2logp,ˆβfalls in the interior of Θ(r,R), so we must have ˆβ∈Θ(r,R). Consequently, ˆβ=˜βsatisﬁes
the claimed bound,
||ˆβ−β∗||Σ.√m/parenleftbigglogp
n0/parenrightbigg1/4
+√s/parenleftbigglogp
n0/parenrightbigg1/4/parenleftbigglogp
nAm+n0/parenrightbigg1/4
. (47)
In addition, if m≤s/radicalbig
log(p)/n0, the above upper bound is sharper than/radicalbig
slog(p)/n0. Then by (45), we
have
||ˆβ−β∗||1.s/radicalBigg
logp
nAm+n0+/parenleftbigglogp
nAm+n0/parenrightbigg1
4√sm+m.
A.5 Proof of Proposition 3.3
The method is similar to the proof of Proposition 3.2. At ﬁrst, the divergence is given by
D(β1,β2) =/parenleftbig
∇ˆQ(0)
g(β1)−∇ˆQ(0)
g(β2)/parenrightbigT(β1−β2).
34Published in Transactions on Machine Learning Research (01/2024)
For a given kernel function K(·)and bandwidth g >0, the smoothed quantile loss ˆQ(0)
gcan be written as
(n0g)−1/summationtextn0
i=1/integraltext∞
−∞ρτ(u)K{(u+ (x(0)
i)Tβ−y(0)
i)/g}du. Therefore
D(β1,β2)≥κ
n0gn0/summationdisplay
i=1/parenleftbig
(x(0)
i)T(β1−β2)/parenrightbig21Ei,
where the eventEiis deﬁned by,
Ei={|/epsilon1i|≤g/4}∩/braceleftBig/vextendsingle/vextendsingle(x(0)
i)T(β1−β2)/vextendsingle/vextendsingle≤g||β1−β2||Σ/(2r)/bracerightBig
∩/braceleftBig/vextendsingle/vextendsingle(x(0)
i)T(β1−β∗)/vextendsingle/vextendsingle≤g/4/bracerightBig
.
for allβ1−β2∈BΣ(r). For a truncation level R> 0, deﬁne functions ϕR(u)andψR(u)as previous proof.
By this construction, ϕR(u)≤u2· 1{|u|≤R},ϕcR(cu) =c2ϕR(u),ϕRis R-Lipschitz, ψRis(2/R)-Lipschitz
andψR(u)≤ 1{|u|≤R}.
From these two new-deﬁned function, we have
D(β1,β2)≥κ
n0g||δ||2
Σn0/summationdisplay
i=11{|/epsilon1i|≤g/4}ϕg||δ||Σ/(2r)/parenleftbig
(x(0)
i)Tδ/parenrightbig
ψg/4/parenleftbig
(x(0)
i)T(β1−β∗)/parenrightbig
≥κ||δ||2
Σ·1
n0gn0/summationdisplay
i=11{|/epsilon1i|≤g/4}ϕg/(2r)/parenleftbig
(x(0)
i)Tδ/||δ||Σ/parenrightbig
ψg/4/parenleftbig
(x(0)
i)T(β1−β∗)/parenrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
D0(β1,β2),(48)
whereδ=β1−β2. Finally, with a similar proof as Proposition 3.2, if r= min{g/(4η1/4),g/(32v0µ1/2
4)}and
n0g&¯ff−2η2
1/4µ4σ2
xslogp, then
D0(β1,β2)≥0.1f,
with probability at least 1−(pn0)−1. Therefore,
D(β1,β2)≥0.1κ·f||β1−β2||2
Σ.
A.6 Proof of Theorem 3.2
For step 1, the parameter ω(k)is at most s+msparse. Therefore, similarly as Theorem 1 in Tan et al.
(2022), we have
/bardblˆω(k)−ω(k)/bardbl2
2.(s+m) logp
n(k),/bardblˆω(k)−ω(k)/bardbl1.(s+m)/radicalbigg
logp
n(k), k∈A/prime
m
with probability at least 1−p−1, provided that the bandwidth hsatisﬁes
max/parenleftbiggσx
f/radicalbigg
(s+m) logp
n(k),σ2
x¯f
f2(s+m) logp
n(k)/parenrightbigg
.h≤min{f/(2l0),(s1/2λ(k)
ω)},
whereσ2
x= max 1≥j≥pσjj,σjjare the diagonal elements of Σ.
For step 2, denote δ(k)=β∗−ω(k),ˆδ(k)=ˆβ−ˆω(k)andˆv(k)=ˆδ(k)−δ(k). For eachk∈A/prime
m,
/parenleftbig
∇ˆQ(0)
g(ˆω(k)+ˆδ(k))−∇ˆQ(0)
g(ˆω(k)+δ(k))/parenrightbigTˆv(k)
≤λδ/parenleftbig
||δ∗||1−||ˆδ(k)||1/parenrightbig
+/parenleftbig
∇ˆQ(0)
g(ˆω(k)+δ∗)−∇ˆQ(0)
g(β∗)/parenrightbigT(δ(k)−ˆδ(k))
+/vextenddouble/vextenddouble∇ˆQ(0)
g(β∗)−∇Q(0)
g(β∗)/vextenddouble/vextenddouble
∞/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
||π∗g||∞/vextenddouble/vextenddoubleˆv(k)/vextenddouble/vextenddouble
1+/vextenddouble/vextenddoubleΣ−1/2∇Q(0)
g(β∗)/vextenddouble/vextenddouble
2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
b∗g/vextenddouble/vextenddoubleˆv(k)/vextenddouble/vextenddouble
Σ. (49)
35Published in Transactions on Machine Learning Research (01/2024)
Then by Lemma A.4 with t= 2 logpand for each k∈A/prime
m, we letrk=/radicalbig
(s+m) log(p)/n(k)andlk=
(s+m)/radicalbig
log(p)/n(k). We have when λδ≥2||π∗
g||∞,
/parenleftbig
∇ˆQ(0)
g(ˆω(k)+ˆδ(k))−∇ˆQ(0)
g(ˆω(k)+δ(k))/parenrightbigTˆv(k)
≤λδ/parenleftbig
||δ(k)||1−||ˆδ(k)||1/parenrightbig
+C/parenleftbiggs+m
g/radicalbigg
logp
n(k)/radicalbigg
logp
n0+logp
n0+√
s+m/radicalbigg
logp
n(k)/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Cv/vextenddouble/vextenddoubleˆv(k)/vextenddouble/vextenddouble
1
+λδ
2/vextenddouble/vextenddoubleˆv(k)/vextenddouble/vextenddouble
1+b∗
g/vextenddouble/vextenddoubleˆv(k)/vextenddouble/vextenddouble
Σ.
Since
/vextenddouble/vextenddoubleδ(k)
Sk/vextenddouble/vextenddouble
1−/vextenddouble/vextenddoubleˆδ(k)
Sk/vextenddouble/vextenddouble
1≤/vextenddouble/vextenddouble/parenleftbig
δ(k)−ˆδ(k)/parenrightbig
Sk/vextenddouble/vextenddouble
1and/vextenddouble/vextenddoubleδ(k)
Sc
k/vextenddouble/vextenddouble
1−/vextenddouble/vextenddoubleˆδ(k)
Sc
k/vextenddouble/vextenddouble
1=−/vextenddouble/vextenddouble/parenleftbigˆδ(k)−δ(k)/parenrightbig
Sc
k/vextenddouble/vextenddouble
1,
whenCv<λδ/2, we obtain
/parenleftbig
∇ˆQ(0)
g(ˆω(k)+ˆδ(k))−∇ˆQ(0)
g(ˆω(k)+δ(k))/parenrightbigTˆv(k)≤/parenleftbigg3
2λδ+Cv/parenrightbigg/vextenddouble/vextenddouble/parenleftbig
δ(k)−ˆδ(k)/parenrightbig
Sk/vextenddouble/vextenddouble
1+b∗
g/vextenddouble/vextenddoubleˆv(k)/vextenddouble/vextenddouble
Σ
≤m1/2/parenleftbigg3
2λδ+Cv/parenrightbigg/vextenddouble/vextenddoubleˆv(k)/vextenddouble/vextenddouble
2+b∗
g/vextenddouble/vextenddoubleˆv(k)/vextenddouble/vextenddouble
Σ.
By Proposition 3.3, the RSC of (∇ˆQ(0)
g(ˆω(k)+ˆδ(k))−∇ˆQ(0)
g(ˆω(k)+δ(k)))Tˆv(k), we have
0.1κ·f/vextenddouble/vextenddoubleˆv(k)/vextenddouble/vextenddouble2
Σ≤m1/2/parenleftbigg3
2λδ+Cv/parenrightbigg/vextenddouble/vextenddoubleˆv(k)/vextenddouble/vextenddouble
2+b∗
g/vextenddouble/vextenddoubleˆv(k)/vextenddouble/vextenddouble
Σ,
with probability at least 1−(pn0)−1. Therefore, if we let g2≤m1/2λδ,
/vextenddouble/vextenddoubleˆv(k)/vextenddouble/vextenddouble2
Σ.mlogp
n0.
By Lemma 17 in Yuan et al. (2018) and the condition m./radicalbig
n0/logp, we have
/vextenddouble/vextenddouble˜δ(k)−δ(k)/vextenddouble/vextenddouble2
Σ.mlogp
n0and/vextenddouble/vextenddouble˜δ(k)−δ(k)/vextenddouble/vextenddouble
1.m/radicalbigg
logp
n0.
For step 3, let ˜δ(0)=δ(0)= 0, then the loss function in step 3 could be written as:
1
n0+nA/primem/summationdisplay
k∈{0}∪A/primemnk/summationdisplay
i=1lw/parenleftbig
y(k)
i−(X(k)
i)T(β−˜δ(k))/parenrightbig
=:/summationdisplay
k∈{0}∪A/primemˆQ(k)
w(β−˜δ(k)).
The symmetric Bregman divergence is deﬁned as
/summationdisplay
k∈{0}∪A/primem/parenleftbig
∇ˆQ(k)
w(ˆβ−˜δ(k))−∇ˆQ(k)
w(β∗−˜δ(k))/parenrightbigT(ˆβ−β∗).
36Published in Transactions on Machine Learning Research (01/2024)
To simplify the notations, deﬁne ∇ˆRw(β) =/summationtext
k∈{0}∪A/prime
m∇ˆQ(k)
w(β−˜δ(k)). Similarly as above, we have an
oracle inequality for ˆβ,
/parenleftbig
∇ˆRw(ˆβ)−∇ˆRw(β∗)/parenrightbigT(ˆβ−β∗)
≤λβ/parenleftbig/vextenddouble/vextenddoubleβ∗/vextenddouble/vextenddouble
1−/vextenddouble/vextenddoubleˆβ/vextenddouble/vextenddouble
1/parenrightbig
+/summationdisplay
k∈{0}∪A/primem/parenleftbig
∇ˆQ(k)
w(ω(k)+δ(k)−˜δ(k))−∇ˆQ(k)
w(ω(k))/parenrightbigT(β∗−ˆβ)
+/summationdisplay
k∈{0}∪A/primem/parenleftbig
∇ˆQ(k)
w(ω(k))−∇Q(k)
w(ω(k))/parenrightbigT(β∗−ˆβ) +/summationdisplay
k∈{0}∪A/primem/parenleftbig
∇Q(k)
w(ω(k))/parenrightbigT(β∗−ˆβ)
≤λβ/parenleftbig/vextenddouble/vextenddoubleβ∗/vextenddouble/vextenddouble
1−/vextenddouble/vextenddoubleˆβ/vextenddouble/vextenddouble
1/parenrightbig
+/summationdisplay
k∈{0}∪A/primem/parenleftbig
∇ˆQ(k)
w(ω(k)+δ(k)−˜δ(k))−∇ˆQ(k)
w(ω(k))/parenrightbigT(β∗−ˆβ)
+/summationdisplay
k∈{0}∪A/primem/vextenddouble/vextenddouble∇ˆQ(k)
w(ω(k))−∇Q(k)
w(ω(k))/vextenddouble/vextenddouble
∞/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
||π(k)
w||∞/vextenddouble/vextenddoubleβ∗−ˆβ/vextenddouble/vextenddouble
1
+/summationdisplay
k∈{0}∪A/primem/vextenddouble/vextenddoubleΣ−1/2∇Q(k)
w(ω(k))/vextenddouble/vextenddouble
2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
b∗w/vextenddouble/vextenddoubleβ∗−ˆβ/vextenddouble/vextenddouble
Σ.
For the second term above, by Lemma A.4,
/parenleftbig
∇ˆQ(k)
w(ω(k)+δ(k)−˜δ(k))−∇ˆQ(k)
w(ω(k))/parenrightbigT(β∗−ˆβ)
≤C/prime/parenleftbiggm
w/radicalbigg
logp
n0/radicalBigg
logp
n0+nA/primem+logp
n0+nA/primem+√m/radicalbigg
logp
n0/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
C/primev/vextenddouble/vextenddoubleβ∗−ˆβ/vextenddouble/vextenddouble
1.
If we setλβ≥2||π∗
w||∞andC/prime
v≤λβ/2, then
/parenleftbig
∇ˆRw(ˆβ)−∇ˆRw(β∗)/parenrightbigT(ˆβ−β∗)
≤/parenleftbigg3
2λβ+C/prime
v/parenrightbigg/vextenddouble/vextenddouble/parenleftbig
β∗−ˆβ/parenrightbig
S/vextenddouble/vextenddouble
1+b∗
w/vextenddouble/vextenddoubleβ∗−ˆβ/vextenddouble/vextenddouble
Σ
≤s1/2/parenleftbigg3
2λβ+C/prime
v/parenrightbigg/vextenddouble/vextenddoubleβ∗−ˆβ/vextenddouble/vextenddouble
2+b∗
w/vextenddouble/vextenddoubleβ∗−ˆβ/vextenddouble/vextenddouble
Σ.
Under the RSC of (∇ˆRw(ˆβ)−∇ˆRw(β∗))T(ˆβ−β∗), we have
/parenleftbig
∇ˆRw(ˆβ)−∇ˆRw(β∗)/parenrightbigT(ˆβ−β∗)≥c1/vextenddouble/vextenddoubleβ∗−ˆβ/vextenddouble/vextenddouble2
Σ,
with probability as least 1−(pn)−1, wheren=n0+nA/primemandc1is a positive constant. The proof of the
RSC in step 3 is similar to Proposition 3.3. Thus,
/vextenddouble/vextenddoubleβ∗−ˆβ/vextenddouble/vextenddouble
Σ≤s1/2/parenleftbigg3
2λβ+C/prime
v/parenrightbigg
γ−1/2
p +b∗
w.
Through a similar proof as Lemma A.2, we obtain λβ./radicalbig
log(p)/n. Ifslog(p)/n≤w2≤s1/2λβ, we have
/vextenddouble/vextenddoubleβ∗−ˆβ/vextenddouble/vextenddouble
Σ./radicalbigg
slogp
n+/radicalbigg
smlogp
n0and/vextenddouble/vextenddoubleβ∗−ˆβ/vextenddouble/vextenddouble
1.s/radicalbigg
logp
n+s/radicalbigg
mlogp
n0,
with probability at least 1−p−1.
37Published in Transactions on Machine Learning Research (01/2024)
B Proof of Lemmas
B.1 Proof of Lemma A.1
Deﬁneω(k)forall 0≤k≤Kasthetrueparametersofeachlocalsourcemodel, thennotethat ∇Q(k)(ω(k)) =
0and∇Q(ω∗) =/summationtextK
k=0αk∇Q(k)(ω∗) = 0. So we have
∇Q(ω∗)−∇Q(β∗) +∇Q(β∗)−K/summationdisplay
k=1αk∇Q(k)(ω(k)) = 0
∇Q(ω∗)−∇Q(β∗) =K/summationdisplay
k=1αk∇Q(k)(ω(k))−∇Q(β∗)
Note that∇Q(0)(ω(0)) =Q(0)(β∗) = 0, so
K/summationdisplay
k=0αk(∇Q(k)(ω∗)−∇Q(k)(β∗)) =K/summationdisplay
k=1αk(∇Q(k)(ω(k))−∇Q(k)(β∗))
By the second-order Taylor expansions and Assumption 3.4,
K/summationdisplay
k=0αk/integraldisplay1
0∇2Q(k)((1−t)β∗+tω∗)dt(ω∗−β∗) =K/summationdisplay
k=1αk/integraldisplay1
0∇2Q(k)((1−t)β∗+tω(k))dt(ω(k)−β∗)
||ω∗−β∗||1≤K/summationdisplay
k=1αk||˜Σ−1˜Σ(k)||1·||ω(k)−β∗||1.
By the deﬁnition of the parameter space
Θ(s,m) =/braceleftBig
β∗,{ω(k)}:||β∗||0≤s,sup
k∈Am||ω(k)−β∗||1≤m/bracerightBig
,
We have||ω(k)−β∗||1≤m. LetC1= supk||˜Σ−1˜Σ(k)||1. Then Lemma A.1 is proved.
B.2 Proof of Lemma A.2
For the transferring steps,
∇ˆQh(ω) =1
nAm+n0K/summationdisplay
k=0nk/summationdisplay
i=1/braceleftBigg
¯K/parenleftbigg(x(k)
i)Tω−y(k)
i
h/parenrightbigg
−τ/bracerightBigg
x(k)
i
∇2ˆQh(ω) =1
nAm+n0K/summationdisplay
k=0nk/summationdisplay
i=1K/parenleftbigg(x(k)
i)Tω−y(k)
i
h/parenrightbigg
x(k)
i(x(k)
i)T.
Letξ(k)
i=¯K{((x(k)
i)Tω−y(k)
i)/h}−τ, then∇ˆQh(ω) = (nAm+n0)−1/summationtextK
k=0/summationtextnk
i=1ξ(k)
ix(k)
iand
||π∗
h||∞=/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nAm+n0K/summationdisplay
k=0nk/summationdisplay
i=1/braceleftbig
ξ(k)
ix(k)
i−E(ξ(k)
ix(k)
i)/bracerightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞.
The upper bound of ||π∗
h||∞involves two quantities that are related to
E/bracketleftBigg
¯K2/parenleftbigg(x(k))T(ω−ω(k))−/epsilon1
h/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglex(k)/bracketrightBigg
andE/bracketleftBigg/parenleftbigg(x(k))T(ω−ω(k))−/epsilon1
h/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglex(k)/bracketrightBigg
.
38Published in Transactions on Machine Learning Research (01/2024)
For the ﬁrst term, by a change of variable and integration by parts, we obtain
E/bracketleftBigg
¯K2/parenleftbigg(x(k))T(ω−ω(k))−/epsilon1
h/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglex(k)/bracketrightBigg
=/integraldisplay∞
−∞¯K2(−u/h)f/epsilon1|x(u)du
=h/integraldisplay∞
−∞¯K2(v)f/epsilon1|x(−vh)dv
= 2/integraldisplay∞
−∞K(v)¯K(v)F/epsilon1|x(−vh)dv. (50)
By the fact that F/epsilon1|x(0) =τ, we have
F/epsilon1|x(−vh) =F/epsilon1|x(0) +/integraldisplay−vh
0f/epsilon1|x(t)dt
=τ−hvf/epsilon1|x(0) +/integraldisplay−vh
0{f/epsilon1|x(t)−f/epsilon1|x(0)}dt. (51)
Moreover, it can be shown that
aK:=/integraldisplay∞
−∞vK(v)¯K(v)dv=/integraldisplay∞
0K(v){1−K(v)}dv> 0 andaK≤κ1, (52)
whereκ1=/integraltext
|u|K(u)du.
Substituting (51) into (50), and by (52), we obtain
E/bracketleftBigg
¯K2/parenleftbigg(x(k))T(ω−ω(k))−/epsilon1
h/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglex(k)/bracketrightBigg
= 2τ/integraldisplay∞
−∞K(v)¯K(v)dv−2hf/epsilon1|x(0)/integraldisplay∞
−∞vK(v)¯K(v)dv
+ 2/integraldisplay∞
−∞/integraldisplay−vh
0{f/epsilon1|x(t)−f/epsilon1|x(0)}K(v)¯K(v)dtdv
≤τ−2aKhf/epsilon1|x(0) +l0h2/integraldisplay∞
−∞v2K(v)¯K(v)dv
≤τ+l0κ2h2,
wheretheﬁrstinequalityholdsusingtheLipschitzconditionon f/epsilon1|xinAssumption3.1andthelastinequality
holds by Assumption 3.2. Through a similar calculation, the Lipschitz condition on f/epsilon1|xensures that
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleE/bracketleftBigg/parenleftbigg(x(k))T(ω−ω(k))−/epsilon1
h/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglex(k)/bracketrightBigg
−τ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤l0
2κ2h2.
Hence
E(ξ(k)
ix(k)
ij)2=Ex/braceleftbig
(x(k)
ij)2·E((ξ(k)
i)2|x(k)
i)/bracerightbig
E(ξ2|x) =E/bracketleftBigg/parenleftBigg
¯K/parenleftbiggxTω−y
h/parenrightbigg
−τ/parenrightBigg2/vextendsingle/vextendsingle/vextendsingle/vextendsinglex/bracketrightBigg
=E/bracketleftBigg
¯K2/parenleftbiggxTω−y
h/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglex/bracketrightBigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤τ+l0κ2h2−2τE/bracketleftBigg
¯K/parenleftbiggxTω−y
h/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglex/bracketrightBigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≥τ−l0
2κ2h2+τ2
≤τ(1−τ) +Ch2,
whereC= (τ+ 1)l0κ2. Then, by Assumption 4.3, we have
E(ξ(k)
ix(k)
ij)2≤τ(1−τ)σjj+Cσjjh2.
39Published in Transactions on Machine Learning Research (01/2024)
Also by Assumption 3.3 and |ξ(k)
i|≤max(1−τ,τ), ford= 3,4,...,
E(|ξ(k)
ix(k)
ij|d)≤max(1−τ,τ)d−2Ex{|x(k)
ij|d·E[(ξ(k)
i)2|x(k)
i]}
≤max(1−τ,τ)d−2/braceleftbig
τ(1−τ) +Ch2/bracerightbig
≤d!
2/braceleftbig
τ(1−τ) +Ch2/bracerightbig
max(1−τ,τ)d−2.
Thus it follows from Bernstein’s inequality and union bound that for every t≥0,
||π∗
h||∞≤σ/radicalBigg
{τ(1−τ) +Ch2}2t
nAm+n0+ max(1−τ,τ)t
nAm+n0
with probability at least 1−2pe−t.
For the debiasing step, through the similar proof we could get same results with diﬀerent sample size and
smoothing bandwidth.
B.3 Proof of Lemma A.3
Note that
b∗
h=||Σ−1/2∇Qh(ω∗)||2
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleΣ−1/2/parenleftBiggK/summationdisplay
k=0αkE/bracketleftBigg
E/braceleftBigg
¯K/parenleftbigg(x(k))Tω∗−y(k)
h/parenrightbigg
−τ/vextendsingle/vextendsingle/vextendsingle/vextendsinglex(k)/bracerightBigg
x(k)/bracketrightBigg/parenrightBigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤sup
u∈Sp−1K/summationdisplay
k=0E/bracketleftBigg
¯K/parenleftbigg(x(k))Tω∗−y(k)
h/parenrightbigg
−τ/bracketrightBigg
/parenleftbig
Σ−1/2x(k)/parenrightbigTu
≤l0
2κ2h2.
B.4 Proof of Lemma A.4
Fork= 1,...,p, deﬁne that
ψk(r,l) = sup
v∈BΣ(r)∩B1(l)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1(1−E)/braceleftbig¯Kh(xT
iv−/epsilon1i)−¯Kh(−/epsilon1i)/bracerightbig
xik/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
=:gv,k(yi,xi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle,
wherev=β−β∗. Note that ψ(r,l)≤max 1≤k≤p{ψk(r,l) +|Egv,k(yi,xi)|}. In the following, we bound
ψk(r,l)andEgv,k(yi,xi), respectively.
Letσbe any positive constant such that σ2≥supv∈BΣ(r)∩B1(l)Eg2
v,k(yi,xi). By the bounded design, we
note that supv|gv,k(yi,xi)|≤|xik|≤1. Applying Theorem 7.3 in Bousquet (2003), Bousquet’s version of
Talagrand’s inequality, we obtain that for any z>0,
ψk(r,l)≤Eψk(r,l) +/radicalbigg
{σ2+ 2Eψk(r,l)}2z
n+z
3n(53)
40Published in Transactions on Machine Learning Research (01/2024)
holds with probability at least 1−e−z. For the second moment Eg2
v,k(yi,xi), by a change of variable and
Minkowski’s integral inequality we derive that
Eg2
v,k(yi,xi) =E/bracketleftBigg
x2
ik/integraldisplay∞
−∞/braceleftbig¯Kh(xT
iv−t)−¯Kh(−t)/bracerightbig2f/epsilon1i|xi(t)dt/bracketrightBigg
=E/bracketleftBigg
x2
ik/integraldisplay∞
−∞/braceleftbig¯Kh(u)−¯Kh(u−xT
iv)/bracerightbig2f/epsilon1i|xi(xT
iv−u)du/bracketrightBigg
=hE/bracketleftBigg
x2
ik/integraldisplay∞
−∞/braceleftbig¯K(v)−¯K(v−xT
iv/h)/bracerightbig2f/epsilon1i|xi(xT
iv−vh)dv/bracketrightBigg
≤¯fh−1E/bracketleftBigg
x2
ik(xT
iv)2/integraldisplay∞
−∞/braceleftBigg/integraldisplay1
0K(v−wxT
iv/h)dw/bracerightBigg2
dv/bracketrightBigg
≤¯fh−1E/parenleftBigg
x2
ik(xT
iv)2/bracketleftBigg/integraldisplay1
0/braceleftBigg/integraldisplay∞
−∞K2(v−wxT
iv/h)dv/bracerightBigg1/2
dw/bracketrightBigg2/parenrightBigg
≤¯κ¯fh−1E(xik·xT
iv)2≤¯κ¯fh−1r2.
It remains to bound Eψk(r,l). Note that|gv,k(yi,xi)−gv/prime,k(yi,xi)|≤(¯κ/h)|xT
iv−xT
iv/prime|, for anyv,v/prime.
Hence using Rademacher symmetrization and Talagrand’s contraction principle, we have
Eψk(r,l)≤2E/bracketleftBigg
sup
v∈BΣ(r)∩B1(l)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1eigv,k(yi,xi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightBigg
≤4¯κE/bracketleftBigg
sup
v∈BΣ(r)∩B1(l)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nhn/summationdisplay
i=1eixT
iv/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightBigg
≤4¯κl
hE/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1eixi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞, (54)
wheree1,...,enare independent Rademacher variables. Applying Hoeﬀding’s moment inequality,
Ee/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1eixi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤max
1≤k≤p/parenleftBiggn/summationdisplay
i=1x2
ik/parenrightBigg1/2/radicalbig
2 log(2p)
n, (55)
where Eedenotes the expectation over {ei}n
i=1. By (54) and (55), we obtain
Eψk(r,l)≤4¯κl
h/radicalbigg
2 log(2p)
n.
Takingz=t+ logpin (53), we have that
ψk(r,l).l
h/radicalbigg
logp
n+¯f1/2r/radicalbigg
t+ logp
nh+t+ logp
n(56)
holds with probability at least 1−e−t.
Next we ﬁnd an union upper bound of |Egv,k(yi,xi)|. Similarly as the method to bound the second moment,
we derive that
Egv,k(yi,xi) =hE/bracketleftBigg
xik/integraldisplay∞
−∞/braceleftbig¯K(v)−¯K(v−xT
iv/h)/bracerightbig
f/epsilon1i|xi(xT
iv−vh)dv/bracketrightBigg
≤¯fE/bracketleftBigg
|xik||xT
iv|/integraldisplay∞
−∞/braceleftBigg/integraldisplay1
0K(v−wxT
iv/h)dw/bracerightBigg
dv/bracketrightBigg
≤¯fE/parenleftBigg
|xik||xT
iv|/bracketleftBigg/integraldisplay1
0/braceleftBigg/integraldisplay∞
−∞K(v−wxT
iv/h)dv/bracerightBigg
dw/bracketrightBigg/parenrightBigg
≤¯κ¯fE|xikxT
iv|≤¯κ¯fr.
41Published in Transactions on Machine Learning Research (01/2024)
Finally taking the union bound, we obtain that with probability at least 1−e−t,
ψ(r,l).l
h/radicalbigg
logp
n+¯f1/2r/radicalbigg
t+ logp
nh+t+ logp
n.
C Empirical results on the similarity of target and source data
This section presents the empirical results of the relationship between target and source data with diﬀerent
degrees of similarity to the target. For the similarity deﬁned in /lscript1-norm of the contrast of each source, we
considerp= 500and the sample size of the target and each source is 400. For the transferable source k,
we letω(k)=β∗+ (m/p)R(k)
pto satisfy the transferring level ||δ(k)||1=||ω(k)−β∗||1≤m, whereβ∗is
the target parameter, R(k)
pis a vector of pindependent Rademacher variables, and m= 10. For any source
datak/primethat is not transferable, we let ω(k/prime)=β∗+ (2m/p)R(k/prime)
p. All the other settings are the same as the
numerical studies in section 4
Figure 6 and 7 illustrate that when the contrast is relatively small in /lscript1-norm, there is signiﬁcant overlap
between the target and source data. When the contrast is relatively large, the source data would have much
more frequency at the two tails, which may cause the negative transfer if those sources are used in transfer
learning.
(a)||δ(k)||1≤10
 (b)||δ(k)||1≤10
(c)||δ(k/prime)||1>10
 (d)||δ(k/prime)||1>10
Figure 6: The predictors are from t-distributions with 4 degrees of freedom.
For the similarity deﬁned in /lscript0-norm of the contrast of each source, we consider p= 500and the sample size
of the target and each source is 400. For the transferable source k,ω(k)is generated from ω(k)
j=β∗
j+2· 1(j∈
M), whereMis a random subset of [p]with|M|= 2. For the source k/primethat is not transferable, ω(k/prime)is
generated from ω(k/prime)
j=β∗
j+ 2· 1(j∈M/prime), whereM/primeis a random subset of [p]with|M/prime|= 4.
Figure 8 and 9 demonstrate that with a relatively small contrast in /lscript0-norm, most of the target and source
data also overlap. However, it is easy to observe that the source data has a distribution with a relatively long
tail. Conversely, when the contrast becomes larger, the distribution of the source data at the tail becomes
more distinct from the distribution of the target.
42Published in Transactions on Machine Learning Research (01/2024)
(a)||δ(k)||1≤10
 (b)||δ(k)||1≤10
(c)||δ(k/prime)||1>10
 (d)||δ(k/prime)||1>10
Figure 7: The predictors are from Gaussian distributions.
(a)||δ(k)||0≤2
 (b)||δ(k)||0≤2
(c)||δ(k/prime)||0>2
 (d)||δ(k/prime)||0>2
Figure 8: The predictors are from t-distributions with 4 degrees of freedom.
43Published in Transactions on Machine Learning Research (01/2024)
(a)||δ(k)||0≤2
 (b)||δ(k)||0≤2
(c)||δ(k/prime)||0>2
 (d)||δ(k/prime)||0>2
Figure 9: The predictors are from Gaussian distributions.
44