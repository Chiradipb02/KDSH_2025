Published in Transactions on Machine Learning Research (02/2024)
From Continuous Dynamics to Graph Neural Networks:
Neural Diffusion and Beyond
Andi Han andi.han@sydney.edu.au
University of Sydney
Dai Shi dai.shi@sydney.edu.au
University of Sydney
Lequan Lin lequan.lin@sydney.edu.au
University of Sydney
Junbin Gao junbin.gao@sydney.edu.au
University of Sydney
Reviewed on OpenReview: https: // openreview. net/ forum? id= fPQSxjqa2
Abstract
Graph neural networks (GNNs) have demonstrated significant promise in modelling relational
data and have been widely applied in various fields of interest. The key mechanism behind
GNNs is the so-called message passing where information is being iteratively aggregated to
central nodes from their neighbourhood. Such a scheme has been found to be intrinsically
linked to a physical process known as heat diffusion, where the propagation of GNNs naturally
corresponds to the evolution of heat density. Analogizing the process of message passing to
the heat dynamics allows to fundamentally understand the power and pitfalls of GNNs and
consequently informs better model design. Recently, there emerges a plethora of works that
proposes GNNs inspired from the continuous dynamics formulation, in an attempt to mitigate
the known limitations of GNNs, such as oversmoothing and oversquashing. In this survey, we
provide the first comprehensive review of studies that leverage the continuous perspective of
GNNs. To this end, we introduce foundational ingredients for adapting continuous dynamics
to GNNs, along with a general framework for the design of graph neural dynamics. We
then review and categorize existing works based on their driven mechanisms and underlying
dynamics. We also summarize how the limitations of classic GNNs can be addressed under
the continuous framework. We conclude by identifying multiple open research directions.
1 Introduction
Graph neural networks (GNNs) (Kipf & Welling, 2017; Veličković et al., 2018; Gilmer et al., 2017) have
emerged as one of the most popular choices for processing and analyzing relational data. The main goal of
GNNs is to acquire expressive representations at node, edge or graph level, primarily for tasks including but
not limited to node classification, link prediction and graph property prediction. Such a learning process
requires not only utilizing the node features but also the underlying graph topology. GNNs have been
successful in various fields of application where relational structures are critical for learning quality, including
recommendation systems (Fan et al., 2019), transportation networks (Jiang & Luo, 2022), particle system
(Shlomi et al., 2020), molecule and protein designs (Hoogeboom et al., 2022; Watson et al., 2023), neuroscience
(Bessadok et al., 2022) and material science (Xie & Grossman, 2018), among many others.
The effectiveness of GNNs primarily roots in the message passing paradigm, in which information is iteratively
aggregated among the neighbouring nodes to update representation of the center node. One prominent
1Published in Transactions on Machine Learning Research (02/2024)
example is the graph convolution network (GCN) (Kipf & Welling, 2017) where each layer updates node
representation by taking a degree-weighted average of its neighbours’ representation. Although being effective
in capturing dependencies among the nodes, the number of layers of message passing requires to be carefully
chosen so as to avoid performance degradation. This is unlike the classic (feedforward) neural networks where
increasing depth generally leads to improved predictive performance. In particular, due to the nature of
message passing, deeper GNNs have a tendency to over-smooth the features, leading to un-informative node
representations (Rusch et al., 2023a). On the other hand, shallow GNNs are likely to suffer from information
bottleneck where signals from distant nodes (with regards to graph topology) exert little influence on the
centered node (Topping et al., 2022; Di Giovanni et al., 2023).
In order to analyze and address the aforementioned issues, many recent works have cast GNNs as discretization
of certain continuous dynamical systems, by analogizing propagation through the layers to time evolution
of dynamics. Indeed, the idea of viewing neural networks in the continuous limits is not new and has been
explored for classic neural networks (E, 2017; Haber & Ruthotto, 2017; Liu & Theodorou, 2019; Li et al., 2022;
Ruthotto & Haber, 2020). The continuous formulation provides a unified framework for understanding and
designing the propagation of neural networks, leveraging various mathematical tools from control theory and
differential equations. One seminal work (Chen et al., 2018) proposes neural ordinary differential equation
(Neural ODE), which can be viewed as a continuous-depth residual network (He et al., 2016). Another
work explores the connections between the convolutional networks with partial differential equations (PDEs)
(Ruthotto & Haber, 2020).
In GNNs, in particular, the message passing mechanism can be viewed as realization of a class of PDEs, namely
the heat diffusion equation (Chamberlain et al., 2021a). Such a novel perspective allows to better dissect the
behaviours of GNNs. As an example, because heat diffusion is known to dissipate energy and converge to a
steady state of heat equilibrium, the phenomenon of oversmoothing corresponds to the equilibrium state of
diffusion where heat distributes uniformly across spatial locations. Apart from offering theoretical insights
into the evolution of GNNs, the continuous dynamics perspective also allows easy integration of structural
constraints and desired properties into the dynamical system, such as energy conservation, (ir)reversibility,
and boundary conditions. Meanwhile advanced numerical integrators, like high-order and implicit schemes,
can be employed to enhance the efficiency and stability of discretization (Butcher, 2016). Finally, the variety
of continuous dynamics grounded with physical substance can inform better designs of GNNs to enhance the
representation power and overcome the performance limitations (Chamberlain et al., 2021a; Thorpe et al.,
2022; Eliasof et al., 2021; Rusch et al., 2022; Maskey et al., 2023; Zhao et al., 2023b; Choi et al., 2023; Eliasof
et al., 2023; Gruber et al., 2023; Kang et al., 2023).
The theory of differential equations and dynamical systems is well-developed with a long-standing history
(Verhulst, 2006; Perko, 2013), while graph neural network is a comparatively nascent field, garnering attention
within the past decade. This offers great potential in harnessing the rich foundations of the dynamical system
theory for enhancing the understanding and functionality of GNNs. In this work, we provide a comprehensive
review of existing developments in continuous dynamics inspired GNNs, which we collectively refer to as
graph neural dynamics. To the best of our knowledge, this is the first survey on the connections between
continuous dynamical systems and graph neural networks. Nevertheless, we would like to highlight a related
line of research that utilizes deep learning to solve differential equations (Huang et al., 2022; Gupta et al.,
2023; Kumar & Yadav, 2023; Kovachki et al., 2023). In particular, graph neural operators (Anandkumar
et al., 2020; Li et al., 2020) employ GNNs to solve PDEs by discretizing the domains and constructing graph
structure based on spatial proximity. In contrast, the idealization that we focus in this work is the reverse
where PDEs inform the designs of graph neural networks.
Organization. We organize the rest of the paper as follows. In Section 2, we introduce diffusion equations
from first principles and then review discrete operators on graphs such as gradient and divergence, which
are crucial for formulations of continuous GNNs. This section concludes with the connection between
diffusion equation with the famous graph convolutional network. Section 3 then presents the framework
for designing GNNs from continuous dynamics and summarizes the existing works based on the underlying
physical processes. In Section 4, we explain how the various dynamics help to tackle the shortcomings of
existing GNNs, in terms of oversmoothing, oversquashing, poor performance on heterophilic graphs (where
2Published in Transactions on Machine Learning Research (02/2024)
neighbouring nodes do not share similar information), as well as adversarial robustness and training stability.
We then in Section 5 discuss the various numerical schemes for propagating and learning the continuous
GNN dynamics. In Section 6, we summarize and compare the computational complexities of the dynamics
discussed in this work and section 7 discusses empirical benchmarks for evaluating graph neural dynamics.
We then conclude the survey by outlining several open research questions and challenges in Section 8.
2 From diffusion equations to graph neural networks
A fundamental building block of graph neural networks is message passing where information flows between
neighbouring nodes. Message passing has natural connection to diffusion equations, which describe how
certain quantities of interest, such as mass or heat disperse spatially, as a function of time.
Diffusion equation in continuous domains. The two laws governing any diffusion equation are Fick’s law
andmass conservation law . The former states the diffusion happens from the regions of higher concentration
to regions of lower concentration and the rate of diffusion is proportional to the mass difference (the gradient).
The latter states the mass cannot be created nor destroyed through the process of diffusion. Formally, let
x: Ω×R→Rrepresent the mass distribution, i.e., x(u,t)over the spatial locations u∈Ωand timet∈R.
Denote∇x:=∂x
∂uas the gradient of mass across the space. Fick’s law states that a flux J(quantifying the
direction and magnitude of mass movement) points towards lower mass concentrated regions, i.e., J=−D∇x,
whereDis the diffusivity coefficient that potentially depends on both space and time. When Ω⊆Rd, we can
write∇x= [∂x
∂u1,...∂x
∂ud]andD∈Rd×dandJcorresponds to a flux in ddirections. The mass conservation
law then leads to the following continuity equation∂x
∂t=−divJ, where divis the divergence operator that
computes the mass changes at certain location and time. Combining the two equations yields the fundamental
(heat) diffusion equation
∂x
∂t= div(D∇x). (1)
The diffusion process is called homogeneous when the diffusivity coefficient Dis space independent, and is
calledisotropic whenDdepends only on the location, and called anisotropic whenDdepends on both the
location and the direction of the gradient. In the case of homogeneous diffusion, we can write the diffusion
equation in terms of the Laplacian operator ∆:= div·∇, i.e.,∂x
∂t=D∆x.
Graphs and discrete differential operators. In order to properly characterize diffusion dynamics over
graphs, it is necessary to generalize the concepts from differential geometry to the discrete space, such as
gradient and divergence.
A graph can be represented by a tuple G= (V,E)whereV,Edenote the set of nodes and edges respectively.
In this work, we focus on the undirected graphs, i.e., if (i,j)∈E, then (j,i)∈E. Leveraging tools from
differential geometry, let L2(V)andL2(E)be Hilbert spaces for real-valued functions on VandErespectively
with the inner product given by
⟨f,g⟩L2(V)=/summationdisplay
i∈Vfigi,⟨F,G⟩L2(E)=/summationdisplay
(i,j)∈EFijGij
forf,g:V→RandF,G :E→R. This allows to generalize the definitions of gradient and divergence to
graph domains (Bronstein et al., 2017). Formally, graph gradient is defined as ∇:L2(V)→L2(E)such that
(∇f)ij=fj−fi. Here we assume the edges are anti-symmetric, namely (∇f)ij=−(∇f)ji. Graph divergence
div :L2(E)→L2(V), is defined as the converse of graph gradient such that (divF)i=/summationtext
j:(i,j)∈EFij.
By the discrete nature, graphs and functions/signals on a graph can be compactly represented via vectors and
matrices. In a graph with nnodes (|V|=n), eachf∈L2(V)can be written as n-dimensional vector fand
F∈L2(E)can be written as a matrix Fof sizen×n(with nonzero i,j-th entry Fi,jonly when (i,j)∈E)
An adjacency matrix is A∈{0,1}n×nsuch that Ai,j= 1if(i,j)∈Eand0otherwise. The degree matrix D
is a diagonal degree matrix with i-th diagonal entry Di,i=degi=/summationtext
jAi,j. Graph Laplacian is then defined
3Published in Transactions on Machine Learning Research (02/2024)
Table 1: List of commonly used notations
Notations Explanations
G= (V,E) GraphGwith node setVand edge setE
L2(V),L2(E) Hilbert space for functions on V,E
X∈RN×cNode signal matrix
A,/hatwideA∈RN×NGraph adjacency, normalized adjacency matrix
/hatwideL∈RN×NNormalized Laplacian matrix
A(X)∈RN×NGraph attention matrix computed on X
degi Degree of node i
div,∇ Divergence and gradient operator
Edir Dirichlet energy.
∂X
∂t,∂2X
∂t2 First-order and second-order time derivative of node signal
⊙,·,⊗ elementwise product/ elementwise product (broadcast across channels)/ tensor product
asL=D−Aand one can verify
div(∇f) =/summationdisplay
j:(i,j)∈E(fj−fi) =−Lf.
Further, graph gradient can be represented with the incidence matrix G∈Re×n. Specifically Gk,i= 1if edge
kenters node i,−1if edgekleaves node iand0otherwise. Graph divergence is thus given by −G⊤, which is
the negative adjoint of the gradient. The edge direction in Gcan be arbitrarily chosen for undirected graphs
because the Laplacian is indifferent to the choice of direction as L=G⊤G.
Graph diffusion and graph neural networks. In this work, we consider x:V− →Rcas a multi-channel
signal (or feature) over the node set. We denote xi∈Rcas the signal over node iand let X∈Rn×ccollects
the signals over all the nodes. The previously defined discrete gradient and divergence operators lead to the
following graph diffusion process, which can be seen as a discrete version of heat diffusion equation in (1):
∂xi
∂t= div(D∇X)i=/summationdisplay
j:(i,j)∈ED(xi,xj,t)(xj−xi),
where the diffusivity D(xi,xj,t)is often scalar-valued and positive, which applies channel-wise. In the special
case of homogeneous diffusion, i.e., D(xi,xj,t) = 1, the diffusion process can be written as∂xi
∂t=div(∇X)i=
−(LX)i. This process is known as the Laplacian smoothing where the signals become progressively smooth
by within-neighbourhood averaging. The solution to the diffusion equation is given by the heat kernel, i.e.,
X(t) =exp(−tL)X(0). In fact, the graph heat equation can also be derived as the gradient flow of the
so-called Dirichlet energy, which measures the local variations: Edir(X) =/summationtext
(i,j)∈E∥xi−xj∥2= tr(X⊤LX).
Thegraphdiffusionprocessisrelatedtothefamousgraphconvolutionalnetwork(GCN)(Kipf&Welling,2017),
where the latter can be viewed as taking the Euler discretization of the former with a unit stepsize. In addition,
GCNdefinesdiffusionwith(symmetrically)normalizedadjacency /hatwideA=D−1/2AD−1/2andLaplacian /hatwideL=I−/hatwideA.
The use of normalized Laplacian /hatwideLin place of the combinatorial Laplacian Lswitches the dynamics from
being homogeneous to isotropic (where diffusion is weighted by node degrees). More precisely, the discretized
dynamics gives the update Xℓ+1=Xℓ−/hatwideLXℓ=/hatwideAXℓ. It can be readily verified that GCN corresponds to the
gradient flow of a normalized Dirichlet energy Edir(X) =/summationtext
(i,j)∈E∥xi//radicalbig
degi−xj//radicalbigdegj∥2=tr(X⊤/hatwideLX).
Additional channel mixing Wand nonlinear activation σ(·)are added, leading to a single GCN layer,
Xℓ+1=σ/parenleftbig/hatwideAXℓWℓ/parenrightbig
.
In the more general setup with anisotropic diffusion coefficients D(xi,xj,t), the diffusion process relates
to the general form of message passing neural network (MPNN) (Gilmer et al., 2017). Taking the Euler
4Published in Transactions on Machine Learning Research (02/2024)
discretization of the diffusion equation with stepsize τ, we can rewrite the process as
xℓ+1
i=xℓ
i+τ/summationdisplay
j:(i,j)∈ED(xℓ
i,xℓ
j)(xℓ
j−xℓ
i) =Uℓ/parenleftbig
xℓ
i,/summationdisplay
j:(i,j)∈EMℓ(xℓ
i,xℓ
j)/parenrightbig
,
whereM(xi,xj) =D(xi,xj)(xj−xi)represents the message passing between two nodes i,jandU(xi,mi) =
xi+τmirepresents the message aggregation within the neighbourhood of node i. The channel-mixing matrix
and nonlinear activation can be added for both the message passing and aggregation steps.
We finally remark that there exist GNNs that cannot be easily interpreted within the continuous diffusion
framework. This includes MPNNs where the update involves operations such as concatenation (Kearnes et al.,
2016), gated recurrent unit (Li et al., 2015). In addition, spectral GNNs, such as ChebGCN (Defferrard et al.,
2016), also do not have a natural correspondence in the physical process. Hence we exclude such GNNs from
the discussion of this work.
3 A general framework of continuous dynamics informed GNNs beyond diffusion
Diffusion dynamics has been shown to underpin the design of message passing and graph convolutional
networks. Whileshowingpromiseinmanyapplications, vanilladiffusiondynamicsmaysufferfromperformance
degradation due to the following four types of causes:
•Oversmoothing : signals become increasingly similar as the propagation of GNN.
•Oversquashing : message cannot be easily communicated
•Heterophily : neighbouring nodes do not share similar features
•Instability : Sensitivity to adversarial perturbation and vanishing or exploding gradients when
increasing depth.
This has motivated the consideration of alternative dynamics other than isotropic diffusion, including
anisotropic diffusion, diffusion with source term, geometric diffusion, oscillation, convection, advection and
reaction. Many of them are directly adapted from the existing physical processes (as we discuss in this work).
This section summarizes existing developments on graph neural dynamics under a general framework and
categories the literature by the underlying continuous system. We follow the same notations in Section 2
where a graph is represented as G= (V,E)with|V|=nand|E|=e. The graph signals or features are
encoded in a matrix X, which is in general time-dependent. Unless mentioned otherwise, we omit the time
dependence and treat XasX(t)for notation clarity. We also denote X0=X(0)as the initial conditions for
the system.
The general framework for designing continuous graph dynamics is given as follows, which relates the time
derivatives of the signals with spatial derivatives on graphs.
/bracketleftig∂X
∂t,∂2X
∂t2/bracketrightig
=FG(X,∇X), (2)
whereFGis a spatial coupling function that is usually parameterized by neural networks. The initial condition
of the system is generally the input graph signals (after an encoder). We have summarized and compared
existing works discussed in the survey in Table 2 in terms of the driving mechanisms and problems addressed.
As we shall see, most of the existing works consider the first-order time derivative while some works explore
the second-order time derivative to encode oscillatory systems.
As a summary, anisotropic diffusion provides more flexibility in adapting to different graph types by allowing
different diffusivity coefficients, which may depend on the current state of graph signals. This can be achieved
by explicitly factoring out the edge indicators or implicitly modelled via attention mechanism. The main
driving mechanism is the diffusion with learnable coefficients, i.e.,∂X
∂t=div/parenleftbig
A(X)·∇X/parenrightbig
, for some learnable
diffusion coefficients. Oscillatory processes model the acceleration through second-order differential equations,
5Published in Transactions on Machine Learning Research (02/2024)
i.e.,∂2X
∂t2=FG(X,∇X). The energy conservation of such processes help to avoid converging to trivial solutions
of constants and thus circumvent oversmoothing. This also prevents energy from exploding, which helps
stabilize the training. Non-local dynamics allows information exchange at a longer distance, and thus have a
wider receptive field at each step. This is in contrast to diffusion based dynamics where communications
happen locally. This can be beneficial for tasks where long-range dependencies are crucial, such as node
classification for heterophilic graphs. External forces can be further injected to modulate the direction,
magnitude and even the nature of information flow on a graph, which leads to∂X
∂t=Fdiff(X) +Fext(X),
for some diffusion dynamics Fdiffand some mechanisms governed by external forces Fext. When equipping
graphs with additional geometric structures, the resulting geometry-underpinned dynamics offers a brand-
new perspective in controlling the behaviours of GNNs by modifying the underlying geometries. That is,
∂X
∂t=FG,M(X)where the dynamics rely on the additional geometry Mimposed on the graph. Furthermore,
dynamics can often be interpreted as inherently minimizing some energy functional and thus can be seen as
itsgradient flow . This suggests directly modifying the energy rather than the dynamics to achieve desired
properties. At last, multi-scale diffusion separates the diffusion dynamics for low-pass and high-pass filters,
and thus achieve greater control for learning different frequency components.
3.1 Anisotropic diffusion
The first class of dynamics, anisotropic diffusion , generalizes the isotropic diffusion in GCN, offering great
flexibility in controlling the local diffusivity patterns. In image processing, anisotropic diffusion has been
extensively applied for low-level tasks, such as image denoising, restoration and segmentation (Weickert
et al., 1998). In particular, the Perona-Malik model (Perona & Malik, 1990) sets the diffusivity coefficient
D∝|∇x|−1, which is often called the edge indicator as it preserves the sharpness of signals by slowing down
diffusion in regions of high variations.
The idea of using anisotropic diffusion to define continuous GNN dynamics has been firstly considered by Poli
et al. (2019) and formalized by Chamberlain et al. (2021a), where a class of graph neural diffusion (GRAND)
dynamics is proposed. The anisotropic diffusion process is formally given by
GRAND :∂X
∂t= div/parenleftbig
A(X)·∇X/parenrightbig
,
where A(X)∈Rn×nencodes the anisotropic diffusivity along the edges. Here ∇X∈Rn×n×ccollects the
gradient along edges and we use ·to represent the elementwise multiplication of diffusivity coefficients
broadcast across the channel dimension. The coefficient is determined by the feature similarity, i.e., A(X) =
[a(xi,xj)](i,j)∈Ewherea(xi,xj) = ( WKxi)⊤WQxjcomputes the dot product attention with learnable
parameters WK,WQ. Softmax normalization is performed on A(X)to ensure it is right-stochastic (row
sums up to one). Notably, the explicit-Euler discretization of GRAND corresponds to the propagation of
graph attention network (GAT) (Veličković et al., 2018). Several versions of GRAND are proposed to render
the dynamics more adaptable compared to the discretized version in (Veličković et al., 2018). In particular,
the diffusivity matrix A(X)can be fixed as A(X0)that only depends on the initial features. This leads to a
GAT propagation with shared attention weights. In addition, A(X)can vary according to a dynamically
rewired edge set based on the attention score, i.e., E←{ (i,j) : (i,j)∈E,a(xi,xj)>ρ}for some threshold
ρ>0. Instructively, this interprets the graph structure as discrete realization of certain underlying domains
where graph rewiring dynamically changes the spatial discretization.
Building on the formulation of GRAND, BLEND (Chamberlain et al., 2021b) further augments the input
signals xiwith position coordinates uifor each node. The diffusion process then becomes
BLEND :∂[X,U]
∂t= div( A([X,U])·∇[X,U]).
The joint diffusion over both positions and signals is motivated by diffusion on Riemannian manifolds with the
Laplace-Beltrami operator, in which the gradient, diffusivity and divergence all depend on the Riemannian
metric (that varies according to the position). In a similar vein, augmenting the position information while
diffusing on graphs produces joint evolution of features as well as topology, which further allows graph
rewiring to improve the information flow. In particular, based on the evolved positional information, the
6Published in Transactions on Machine Learning Research (02/2024)
Table 2: Summary of continuous dynamics informed graph neural networks, including the driving mechanism
and the problems addressed, including oversmoothing (OSM), oversquashing (OSQ), graph heterophily
(HETERO), and stability (STAB) with respect to perturbation or with training, such as gradient vanishing
or explosion associated with increased depth. Note we show the problems addressed either theoretically or
empirically in the paper (unless proven otherwise in subsequent literature). More detailed discussions are in
Section 4.
Methods MechanismProblems Tackled
OSM OSQ HETERO STAB
Anisotropic
diffusionGRAND (Chamberlain et al., 2021a) Attention diffusivity & rewiring ✔∗✔
BLEND (Chamberlain et al., 2021b) Position augmentation ✔∗✔
Mean Curvature (Song et al., 2022)
Beltrami (Song et al., 2022)Non-smooth edge indicators ✔
p-Laplacian (Fu et al., 2022) p-Laplacian regularization ✔†
DIFFormer (Wu et al., 2023b) Full graph transformer ✔
DIGNN (Fu et al., 2023) Parameterized Laplacian ✔†
GRAND++ (Thorpe et al., 2022) Source term ✔
PDE-GCN D(Eliasof et al., 2021) Nonlinear diffusion ✔ ✔
GIND (Chen et al., 2022) Implicit nonlinear diffusion ✔ ✔
OscillationPDE-GCN H(Eliasof et al., 2021) Wave equation ✔ ✔
GraphCON (Rusch et al., 2022) Damped coupled oscillation ✔ ✔ ✔
Non-local
dynamicsFLODE (Maskey et al., 2023) Fractional Laplacian ✔ ✔ ✔
QDC (Markovich, 2023) Quantum diffusion ✔ ✔
TIDE (Behmanesh et al., 2023) Learnable heat kernel ✔
G2TN (Toth et al., 2022) Hypo-elliptic diffusion ✔
Diffusion
with external
forcesCDE (Zhao et al., 2023b) Convection diffusion ✔
GREAD (Choi et al., 2023) Reaction diffusion ✔ ✔
ACMP (Wang et al., 2023)Allen-Chan retraction
with negative diffusivity✔ ✔
ODNet (Lv et al., 2023) Diffusion with confidence ✔ ✔
ADR-GNN (Eliasof et al., 2023) Advection reaction diffusion ✔ ✔
G2(Rusch et al., 2023b) Diffusion gating ✔ ✔
MHKG (Shao et al., 2023) Reverse diffusion ✔ ✔ ✔
A-DGN (Gravina et al., 2023) Anti-symmetric weight ✔ ✔ ✔
Geometry
underpinned
dynamicsNSD (Bodnar et al., 2022) Sheaf diffusion ✔ ✔
Hamiltonian G, etc.(Gruber et al., 2023)Bracket dynamics with
higher-order cliques✔
HamGNN(Kang et al., 2023)
(Zhao et al., 2023a)Learnable Hamiltonian dynamics ✔ ✔
Gradient flow GRAFF (Giovanni et al., 2023) Parameterized gradient flow ✔ ✔
Multi-scale
diffusionGradFUFG (Han et al., 2022)Separated diffusion for low-pass
and high-pass at different scales✔ ✔
∗The ability of the methods to mitigate oversquashing is through graph rewiring.†p-Laplacian and DIGNN avoids oversmoothing
provided the input dependent regularization (i.e., a soure term) is added.
graph is dynamically rewired as E←{ (i,j) :dC(ui,uj)<r}or by k-nearest neighbour graph. The positional
information can be pre-computed by personalized PageRank (Gasteiger et al., 2019), deepwalk (Perozzi et al.,
2014) or even learned hyperbolic embeddings (Chami et al., 2019).
In BLEND, the diffusivity is given by the attention score over both the positional features and signals, which
is in fact the core idea of transformers (Vaswani et al., 2017) that augments samples with positional encoding.
Wu et al. (2023b) develop a transformer-based diffusion process (called DIFFormer) where attention diffusivity
coefficients are computed on a fully connected graph V×V. The input graph (represented via input adjacency
matrix A0) serves as a geometric prior that augments the learned attention matrix.
DIFFormer :∂X
∂t= divV×V/parenleftbig
(A0+A(X))·∇X/parenrightbig
,
7Published in Transactions on Machine Learning Research (02/2024)
where divV×V(F)i=/summationtext
j∈VFi,jdenotes the divergence operator on the complete graph and A(X)computes
the diffusivity by a transformer block (Vaswani et al., 2017), different to the ones used by GRAND and
BLEND. A recent work (Wu et al., 2023a) has shown the evolution via both local message passing through
A0and global attention diffusion through A(X)improves the generalization under topological distribution
shift, i.e., when training and test graph topology differs.
Apart from its connection to transformer-based dynamics, BLEND is in fact motivated by a geometric
evolution known as Beltrami flow where the diffusion over a non-Euclidean domains also depends on the
varying metric. In (Song et al., 2022), the Beltrami flow, along with mean-curvature flow (a geometric flow
where the movement is governed by the mean curvature at each point on the surface), is generalized to
graph domains by explicitly factorizing out the edge indicator ∥δxi∥:=/radicalig/summationtext
j:(i,j)∈E∥xj−xi∥2. Formally, let
Smc(X),Sbel(X)be the diffusivity coefficients of mean curvature and Beltrami diffusion, with their elements
defined as [Smc(X)]i,j=1
∥δxi∥+1
∥δxj∥and[Sbel(X)]i,j=1
∥δxi∥2+1
∥δxi∥∥δxj∥. The diffusion processes Song
et al. (2022) propose are
Mean Curvature :∂X
∂t= div/parenleftig/parenleftbig
A(X)⊙Smc(X)/parenrightbig
·∇X/parenrightig
,
Beltrami :∂X
∂t= div/parenleftig/parenleftbig
A(X)⊙Sbel(X)/parenrightbig
·∇X/parenrightig
,
where A(X)is computed from the dot product attention following the previous works (Chamberlain et al.,
2021a;b). For both dynamics, non-smooth signals are preserved by slowing down diffusion where signal
abruptly changes. As commented by the paper, positional information can be added in a similar way as
BLEND (Chamberlain et al., 2021b). It should be noticed that although BLEND originates from the Beltrami
flow, the dynamics turns out to be the same as GRAND, augmented with positional embeddings. In contrast,
Song et al. (2022) explicitly separates the edge indicator out from the attention matrix A(X).
A more general p-Laplacian based graph neural network is proposed by Fu et al. (2022) where the diffusion is
derived from a p-Laplacian regularization framework.
p-Laplacian :∂X
∂t= div/parenleftbig
∥∇X∥p−2·∇X/parenrightbig
−µ(X−S),
withSas a source term and µ>0controlling the regularization strength. The diffusivity ∥∇X∥p−2is an
n×nmatrix with elements [∥∇X∥p−2]i,j=∥xj−xi∥p−2if(i,j)∈E. The injection of source information
can be understood physically as the heat exchange from the system to the outside. In the paper the source
term is simply the input feature matrix, i.e., S=X0. Whenp= 2, the diffusion reduces to the heat diffusion
with classic Laplacian. When p= 1, the dynamics recovers the mean curvature flow (although the definition
slightly differs from the one in (Song et al., 2022)). As a result, with properly selected p, the process is
flexible in adapting to different types of graphs and able to perverse the boundaries without oversmoothing
the signals. It is worth mentioning that unlike previous works that use discretization to update X, the paper
directly solves for the equilibrium state by setting∂X
∂t= 0, which leads to an implicit graph diffusion layer
given byp-Laplacian message passing.
Thep-Laplacian diffusion corresponds to the gradient flow of a p-Dirichlet energy, defined by Ep
dir(X) =/summationtext
(i,j)∈E∥xi−xj∥pgiven with a regularization term ∥X−S∥2. A similar idea has been considered in (Dan
et al., 2023) where the Dirichlet energy is replaced with the total variation on graph gradients, which leverages
theL1norm (which is different to the case of p= 1in thep-Laplacian diffusion (Song et al., 2022)). A
dual-optimization scheme is introduced due to the non-differentiablity of the objective at zero.
A recent paper (Fu et al., 2023) parameterizes the graph gradient and divergence instead of the diffusivity
coefficients as in previous works, and defines a parameterized graph Laplacian for diffusion process. In
particular, Fu et al. (2023) consider weighted inner products for both the L2(V)andL2(E), i.e.,⟨f,g⟩L2(V)=/summationtext
i∈Vχifigiand⟨F,G⟩L2(E)=ϕi,jFi,jGi,j. The graph gradient is defined as (∇Θf)i,j:=ψi,j(fj−fi), which
also leads to a notion of graph divergence (divΘF)i=1
2χi/summationtext
j∈Niψi,jϕi,j(Fi,j−Fj,i), whereχi,ϕi,j,ψi,jare
strictly positive real-valued functions on nodes and edges respectively. Here we denote ∇Θ,divΘto emphasize
that the gradient and divergence operators are parameterized. Because the graph gradient is parameterized
8Published in Transactions on Machine Learning Research (02/2024)
and may not be anti-symmetric, i.e., (∇Θf)i,j̸=−(∇Θf)j,i, the divergence encodes directional information.
The paper also parameterizes the weighting functions to be node-dependent, i.e., χi=χi(xi),ϕi,j=ϕi,j(xi,xj)
andψi,j=ψi,j(xi,xj), which involves learnable parameters. The diffusion process the paper considers is thus
DIGNN :∂X
∂t= div Θ(∇ΘX)−µ(X−S),
where a regularization term ∥X−S∥2is added similarly as in (Fu et al., 2022).
The idea of adding an energy source has also been considered in GRAND++ (Thorpe et al., 2022) based on
the framework of anisotropic diffusion of GRAND:
GRAND++ :∂X
∂t= div( A(X)·∇X) +S
where S∈Rn×cis a source term. The paper proposes a random walk viewpoint to show that, without the
source term, the dynamics reduces to GRAND and is guaranteed to converge to a stationary distribution
independent of the initial conditions X0. Each row of the source term Sis defined as si=/summationtext
j∈Iδij(x0
j−¯x0)
withI⊆Va selected node subset used as the source term and ¯x0=1
|I|/summationtext
j∈Ix0
jis the average signal. δij
denotes the initial transition probability from node jtoi. By construction, the limiting signal distribution is
close to an interpolation of the source signals in the selected subset I. Similar idea of source term injection
has also been considered in earlier work (Xhonneux et al., 2020), which can be seen as the homogeneous
diffusion with both a source term and a residual term. The proposed model (called CGNN) follows the
dynamics∂X
∂t=−LX+XW +X0, for some learnable channel mixing matrix W.
Anisotropic diffusion is generally nonlinear in the sense that diffusivity depends nonlinearly on the mass along
the evolution. This is the case in edge-preserving dynamics as the diffusivity explicitly depends nonlinearly
on the gradient. GRAND-based dynamics is also nonlinear as long as the attention coefficients is computed
for each timestep. In (Eliasof et al., 2021; Chen et al., 2022), additional nonlinearity is further incorporated
by factoring the diffusivity Das the composition of a linear operator Kand its adjointK∗, i.e.,D=K∗K.
Then a pointwise nonlinearity function σ(·)is added, leading to
∂X
∂t= div/parenleftbig
K∗σ(K∇X)/parenrightbig
.
In the case when σis the identity map, the dynamics recovers the anisotropic diffusion. Such a nonlinear
system has been firstly proposed by Ruthotto & Haber (2020) for defining convolutional residual networks
for images. In (Eliasof et al., 2021), the idea is generalized to graphs by defining Kas learnable pointwise
convolution. Specifically, the dynamics, called PDE-GCN D, can be written in terms of the gradient and
divergence operator as follows.
PDE-GCN D:∂X
∂t=−G⊤K⊤σ(KGX ),
where Kis a learnable parameter and Gis the gradient operator defined in Section 2. It can be readily
observed that when σis identity and K=I, the dynamics reduces to the heat diffusion implemented by
GCN.
In the follow-up work (Chen et al., 2022), the linear operator K(parameterized by K) is applied over the
channel space instead of the edge space, i.e.,
GIND :∂X
∂t=−G⊤σ(GXK⊤)K.
Motivated by Gu et al. (2020), Chen et al. (2022) consider an implicit propagation of GIND as Z=
−G⊤σ/parenleftbig
G(Z+bΩ(X0))K⊤/parenrightbig
K, wherebΩ(·)is an affine transformation with parameter Ω. The model
corresponds to a refinement process for the flux Zand the output is given by a decoder over Z+X0. It has
been shown the equilibrium state of the implicit diffusion corresponds to the minimizer of a convex objective
function provided the nonlinearity is monotone and Lipschitz and K⊗Gis upper bounded in norm. This
result guarantees the convergence of the dynamics and allows structural constraints to be embedded to the
dynamics by explicitly modifying the objective.
9Published in Transactions on Machine Learning Research (02/2024)
3.2 Oscillations
The phenomenon of oscillation has been widely found in physics, which primarily features the repetitive
motion. Unlike diffusion that dissipates energy, oscillatory system often preserves energy and is thus reversible.
Oscillatory processes are often modelled as second-order ordinary/partial differential equations. One simple
example of oscillatory process is characterized by the wave equation∂2x
∂t2=c∆x, which is a hyperbolic PDE.
The wave equation has been considered in (Eliasof et al., 2021) for defining dynamics on graphs, which follows
the nonlinear formalism in (Ruthotto & Haber, 2020):
PDE-GCN H:∂2X
∂t2= div/parenleftbig
K∗σ(K∇X)/parenrightbig
=−G⊤K⊤σ(KGX ).
In addition, GraphCON (Rusch et al., 2022) considers more general oscillatory dynamics which combines a
damped oscillating process with a coupling function. That is,
GraphCON :∂2X
∂t2=σ(FG(X))−γX−α∂X
∂t,
whereFG(X)i=FG(xi,{xj}j∈Ni)is the coupling function, α≥0andσ(·)is some nonlinear activation. When
σ(FG(X)) = 0,α= 0, the system reduces to the classic harmonic oscillation for each node independently.
Adding the damping term −∂X
∂tmimics the frictional force that diminishes the oscillation. Finally, due to the
presence of interdependence between the nodes, the coupling function is required to model the interactions
between the nodes. The paper mainly considers two choices of coupling function, with isotropic and anisotropic
diffusion (which leads to GCN and GAT respectively). Formally, FG(X)i=/summationtext
j:(i,j)∈EA(X)i,jxjrepresents
the message passing with normalized adjacency or learned attention scores. When γ= 1,σis identity, the
GraphCON can be rewritten as∂2X
∂t=div(A(X)·∇X)−α∂X
∂t, which is effectively the wave equation with a
damping term. GraphCON is flexible in that the coupling term can accommodate arbitrary message passing
scheme. In addition, it possesses greater expressive power by showing the GNN induced by the coupling
function approaches the steady state of GraphCON, while the latter explores the entire trajectory.
3.3 Non-local dynamics
We have so far focused on local dynamics, in the sense that the diffusion or oscillation happens locally within
the neighbourhood. Thus it usually requires sufficiently large timestep for one node’s influence to reach
a distant node (with respect to graph topology). Graph rewiring employed in GRAND and BLEND can
be utilized to enable long-range diffusion by modifying the graph topology. This section explores various
dynamics-based formulations that transcend the local community when propagating information, resulting in
non-localized dynamics.
Fractional Laplacian. Fractional Laplacian (Pozrikidis, 2018; Lischke et al., 2020) has been effective to
represent complex anomalous processes, such as fluids dynamics in porous medium (Caffarelli & Vazquez,
2011). A recent work (Maskey et al., 2023) utilizes the fractional graph Laplacian/adjacency matrix −/hatwideAα
(for someα∈R) in order to define a non-local diffusion process as
FLODE :∂X
∂t=−/hatwideAαX.
where/hatwideAis the symmetric normalized adjacency matrix. A critical difference compared to p-Laplacian in
terms of the order is that here αcan be fractional, instead of being restricted to integers. The fractional
Laplacian is often dense, and thus the corresponding diffusion is non-local where long-range interactions are
captured. When coupled with a symmetric channel mixing matrix W, i.e.,∂X
∂t=−/hatwideAαXW, the flexibility
in the choice of αallows dynamics to accommodate both smoothing and sharpening effects, which avoids
oversmoothing and is suited for heterophilic graphs. The work also extends the formulation to directed
graphs and correspondingly defines the notions of oversmoothing and Dirichlet energy with the asymmetric
Laplacian. On directed graphs, the fractional Laplacian is defined through singular value decomposition, i.e.,
10Published in Transactions on Machine Learning Research (02/2024)
/hatwideAα:=UΣαVH, where U,V∈Cn×nare unitary singular vectors and Σ∈Rn×ncontains singular values on
the diagonal. Similar idea of using SVD for directed graph processing has been explored in (Zou et al., 2023).
Finally, the paper also considers a Schrödinger equation based diffusion as∂X
∂t= i/hatwideAαX, where i =√−1
represents the imaginary unit.
Quantum diffusion kernel. The Schrödinger’s equation i∂ψ(u,t)
∂t=Hψ(u,t)has also been considered
in (Markovich, 2023), where His the Hamiltonian operator (composed of a kinetic and potential energy
operator). ψ(u,t)denotes the (complex-valued) quantum wave function at position uat timet, and the
square modulus of the wave function |ψ(u,t)|2indicates the probability density of a particle. With the
quantum system defined by ψ(u,t), a quantum state |ψ(t)⟩refers to the superposition of all the states, i.e.,
|ψ(t)⟩=/integraltext
ψ(u,t)|u⟩du, where|u⟩is the position state (a basis vector for representing the quantum state).
One can recover the components of the quantum state by the inner product between quantum state vector
|ψ(t)⟩and position vector |u⟩, i.e.,ψ(u,t) =⟨u|ψ(t)⟩. In a simple quantum system where the Hamiltonian H
is time-independent, the solution to the Schrödinger’s equation can be written in terms of quantum state as
|ψ(t)⟩=e−iHt|ψ(0)⟩.
On a graph with nnodes,|ψ(t)⟩∈Cncan be interpreted as the state vector of particles across all nodes,
and the position state refers to the node set. In a system without a potential and thus the potential energy
term is zero, the Hamiltonian reduces to the negative Laplacian, and the Schrödinger’s equation reduces
toi∂|ψ(t)⟩
∂t=−L|ψ(t)⟩. The eigenvectors of L, denoted as{|ϕi⟩}n
i=1provides a complete orthogonal basis,
which can be treated as the position basis so that one can express the solution as |ψ(t)⟩=/summationtextn
k=1ckeiλkt|ϕk⟩,
whereλkis thek-th eigenvalue and ck=⟨ψ(0)|ϕk⟩. Instead of working with the solution which involves the
complex unit, Markovich (2023) define a kernel that models the average overlap between any two nodes i,j
as the inner product between |ψ(t)⟩iand|ψ(t)⟩j, which is/summationtextn
k,l=1c∗
kcl|ϕk⟩∗
i|ϕl⟩j. To engineer observation
operators as a spectral filter, the paper further leverages a Gaussian filter P=/summationtextn
k=1e−(λk−µ)2/2σ2, which
leads to the proposed quantum diffusion kernel (QDC) Q∈Rn×n, where
QDC :Qi,j=n/summationdisplay
k=1e−(λk−µ)2/2σ2|ϕk⟩∗
i|ϕk⟩j
The kernel matrix Qis interpreted as the transition matrix between the nodes and hence can be supplemented
for any message-passing-based graph neural networks. Hence the resulting quantum diffusion corresponds to
the anisotropic diffusion where the diffusivity is given by the quantum diffusion kernel. The kernel matrix
can be further sparsified either using a threshold or KNN. The kernel allows non-local message passing due
to the quantum interference across all the position states (i.e., Qi,jis computed with all the states). Further
a multi-scale variant of quantum diffusion is proposed that combines the propagation from quantum diffusion
and standard graph diffusion.
Time derivative diffusion. Another work (Behmanesh et al., 2023) introduces a non-local message passing
scheme by combining local message passing with a learnable-timestep heat kernel. Recall the heat diffusion
follows∂X
∂t=−LX, where its solution is given by the heat kernel as X(t) =exp(−tL)X(0). One can generalize
the heat kernel to capture the transition between any two states in time, i.e., X(t) =exp(−(t−s)L)X(s).
Instead of setting a pre-defined t,s, the paper parameterizes the heat kernel as exp(−tθL)wheretθis
the learnable timestep in order to adapt the diffusion range to different types of a dataset. Further, to
simultaneously account for local message passing, the paper combines the adjacency matrix Awith learned
heat kernel, which leads to the proposed dynamics as X(t) =Aexp(−tθL)X(s). This corresponds to the
following continuous dynamics (up to some normalizing constants):
TIDE :∂X
∂t= div( Aexp(−tθL)·∇X).
It is clear that when tθ= 0, the model reduces to the classic GCN. The learnability of tθensures the model
is flexible to capture both local and multi-hop communication.
Hypo-elliptic diffusion. In (Toth et al., 2022), non-local and higher-order information is captured via
the so-called hypo-elliptic diffusion , which is based on a tensor-valued Laplacian that aggregates the entire
11Published in Transactions on Machine Learning Research (02/2024)
trajectories of random walks on graphs. The sequential nature of the path can be characterized with
the free (associative) algebra, which lifts the sequence injectively to a vector space with non-commutative
multiplication (i.e., an algebra). More formally, an algebra HoverRccan be realized as a sequence of tensors
with increasing order, i.e., H:={v= (v0,v1,v2,...):vm∈(Rc)⊗m}, where (Rc)⊗mdenotes the space of
m-order tensors. For example, (Rc)⊗0≡R,(Rc)⊗1≡Rc. The scalar multiplication and vector addition of H
is defined according to λv:= (λvm)m≥0forλ∈Randv+w:= (vm+wm)m≥0. The algebra multiplication
ofHis given by v·w:=/parenleftbig/summationtextm
k=0vk⊗wm−k/parenrightbig
m≥0.Hcan be further made into a Hilbert space with the
chosen inner product ⟨v,w⟩:=/summationtext
m≥0⟨vm,wm⟩mwhere⟨·,·⟩mdenotes the classic inner product on the tensor
space (Rc)⊗m.
With the properly defined algebra, one can lift a sequence to such space, thus summarizing the full details of
its trajectory. Specifically, denote the space of sequences in RcasSeq(Rc):=/uniontext∞
k=0(Rc)k+1, where (Rc)k+1
denotes the k+ 1-product space of Rc. A sequence, denoted as x= (x0,x1,...,xk)∈(Rc)k+1is an element
of the sequence space Seq(Rc). Letφ:Rc→Hbe an algebra lifting, which allows to define a sequence
feature map ˜φ(x) =φ(x0)·φ(x1−x0)···φ(xk−xk−1)∈H. One example of such injective map is the tensor
exponential given by φ(u) =exp⊗(u):=/parenleftbigu⊗m
m!/parenrightbig
m≥0foru∈Rc, wherex⊗m:=x⊗x···⊗x/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
m. Such a feature
map is able to summarize the entire sequence path up to step k.
On a graph, let xk
i∈Rcdenote the signals at node i∈Vat diffusion step k≥0. Instead of focusing on the
signals at a particular timestep k, the paper leverages the sequence map to capture the entire past trajectory
of the diffusion process through ˜φ(xi)∈Hwhere xi:= (x0
i,x1
i,...,xk
i). The corresponding diffusion process
requires a tensor adjacency matrix, /tildewideA∈Hn×nwith the entries /tildewideAi,j=φ(x0
j−x0
i)∈Hif(i,j)∈Eand0
otherwise. The associated Laplacian /tildewideLcan be defined accordingly. For example, the random walk Laplacian
has entries/tildewideLi,i= 1and/tildewideLi,j=−1
degiφ(x0
j−x0
i)if(i,j)∈Eand0otherwise. The classic graph heat diffusion
is generalized to hypo-elliptic graph diffusion as
G2TN :∂˜φ(X)
∂t=−/tildewideL˜φ(X),
where we let ˜φ(X):= [˜φ(x1),...,˜φ(xn)]∈Hn. The multiplication of /tildewideLand ˜φ(X)is defined over the space of
algebraH(similarly as how classic matrix multiplication works). In the case of random walk Laplacian, the
paper verifies that the solution of the hypo-elliptic diffusion summarizes the entire random walk histories of
each node, in contrast to the snapshot state at each timestep given by the classic diffusion equation. Notice
here instead of working with node signals xdirectly, the diffusion concerns all the node signals along the
trajectory, i.e., φ(x). Thus ˜φ(x)is more expressive compared to x. The paper further adapts the attention
mechanism to define a weighted hypo-elliptic adjacency as /tildewideAi,j=a(x0
i,x0
j)φ(x0
j−x0
i), which correspondingly
defines an anisotropic hypo-elliptic diffusion.
3.4 Diffusion with external forces
Most of the aforementioned processes are only controlled by a single mechanism, either diffusion or oscillation.
This section discusses systems that impart external forces to the diffusion dynamics, such as convection ,
advection , andreaction. In particular, convection and advection are widely known in physical sciences that
describe how the mass transports as a result of the movement of the underlying fluid. Such a process is
characterized by∂x
∂t=−div(vx)where vrepresents the velocity field of the fluid motion. Reaction process is
more general and often found in chemistry where chemical substance interacts with each other and leads to a
change of mass and substance, i.e.,∂x
∂t=r(x), for some reaction function r(·). Other mechanisms such as
gating, reverse diffusion and anti-symmetry have also been exploited in literature to modulate and control
the diffusion dynamics.
Convection-diffusion. Convection-diffusion dynamics combines the convection with diffusion process in
which mass not only transports but also disperse in space. Zhao et al. (2023b) generalize the convection-
diffusion equation (CDE) to graphs as
CDE :∂X
∂t= div( A(X)·∇X) + div( V◦X)
12Published in Transactions on Machine Learning Research (02/2024)
where Vdenotes a velocity field and (V◦X)i,j:=Vi,j⊙xjwith⊙representing the elementwise product.
In particular, Zhao et al. (2023b) define Vi,j=σ(W(xj−xi))∈Rcfor some nonlinear activation σ(·)and
learnable matrix W. Such a choice is motivated from the heterophilic graphs where neighbouring nodes
exhibit diverse features. Hence Vi,jcaptures the dissimilarity between the nodes, which ultimately guides
the diffusion process. Accordingly, div(V◦X)i=/summationtext
j:(i,j)∈EVi,j⊙xjmeasures the flow of density at node i
in the direction of Vi,j. The nonlinear parameterization of the gradient further enhances the dynamics to
adapt to different graphs with varying homophily levels.
Reaction-diffusion. A more general reaction-diffusion process is considered in (Choi et al., 2023):
GREAD :∂X
∂t= div( A(X)·∇X) +R(X),
whereR(X)is the reaction term, and proper choice of R(·)recovers many existing works as special cases.
For example, the Fisher reaction (Fisher, 1937) is given by R(X) =κX⊙(1−X), which can be used to
model the spread of biological populations where κrepresents the intrinsic growth rate. Other reaction
processes include Allen-Cahn (Allen & Cahn, 1979) R(X) =X⊙(1−X⊙X)and Zeldovich (Gilding &
Kersner, 2004) R(X) =X⊙(X−X⊙X). Apart from the physics informed choices of reaction term, Choi
et al. (2023) also consider R(X) =X0, which follows GRAND++, CGNN to incorporate a source term, and
also proposes several high-pass reaction term based on graph structure, aiming to induce a sharpening effect.
For example, the blurring-sharpening reaction is defined as R(X) = (A(X)−A(X)2)X, which corresponds
to performing a low-pass filter A(X)−Ifollowed by a high-pass filter I−A(X). The paper also considers
two learnable coefficients α,βto control the emphasis on diffusion and reaction terms respectively, i.e.,
αdiv(A(X)·∇X) +βR(X).
A closely related work (Wang et al., 2023) adopts the Allen-Cahn reaction term for the reaction-diffusion
process. Further, the paper allows negative diffusion coefficients, which is able to induce a repulsive force
between the nodes:
ACMP :∂X
∂t= div(( A(X)−B)·∇X) +X⊙(1−X⊙X)
where B>0is a bias term controlling the strength and direction of message passing. This allows A(X)−B
to model the interactive forces and can become negative. Further, the first term of ACMP corresponds to
the gradient flow of a pseudo Dirichlet energy given by/summationtext
(i,j)∈E(Ai,j−Bi,j)∥xi−xj∥2where we ignore
the dependence of AonXfor the time being. This suggests, when Ai,j−Bi,j>0, nodeiis attracted by
nodejby minimizing the difference between xiandxjand when Ai,j−Bi,j<0, nodeiis repelled by node
j. It is noticed that the presence of negative weights can cause the energy to be unbounded and thus the
dynamics may not be convergent. To resolve the issue, the paper considers an external potential, namely the
double-well potential (δ/4)/summationtext
i∈V(1−∥xi∥2)2. ACMP is indeed derived as the gradient flow of the pseudo
Dirichlet energy combined with the double-well potential. Theoretically, the Dirichlet energy Edirof ACMP
evolution is upper bounded due to the Allen-Cahn reaction term as well as lower bounded due to the repulsive
forces. Hence, the system remains stable while avoiding oversmoothing. For practical implementation, ACMP
setsBi,j=β >0, a tunable hyperparameter for simplicity of optimization. Two channel-wise coefficient
vectors are added to balance the diffusion and reaction similarly in (Choi et al., 2023). Unlike in ACMP
(Wang et al., 2023) where the repulsive components Bi,jare treated as hyperparameters, a recent work (Shi
et al., 2024) proposes a more principled strategy for incorporating negative weights through augmented nodes
with label information.
In addition, the idea of incorporating repulsive force in the message passing has also been explored in (Lv
et al., 2023). The work proposes to view the message passing mechanism on graphs in the framework of
opinion dynamics. The work explores the notion of bounded confidence from the Hegselmann-Krause (HK)
model (Hegselmann & Krause, 2002) where only similar opinions (up to some cut-off threshold) are exchanged.
This motivates the following dynamics on graphs that separates messages according to the similarity level of
graph signals:
ODNet :∂X
∂t= divV×V(Φ(A(X))·∇X) +R(X),
13Published in Transactions on Machine Learning Research (02/2024)
where divV×Vdefines message aggregation over the complete graph. Φ(A(X))is an elementwise scalar-valued
function (called influence function) on diffusivity and is required to be non-decreasing. In (Lv et al., 2023),
Φ(·)is chosen to be piecewise linear as follows.
Φ(s) =

µs, ifs>ϵ 2,
s, ifϵ1≤s≤ϵ2,
ν(1−s),otherwise
whereµ>0andν≤0are hyperparameters. In addition, ϵ1,ϵ2defines the influence regions (which resembles
bounded confidence in HK model). Because νcan be negative, it is able to induce repulsive forces by
separating the node representations. Empirically, the work chooses ν= 0for homophilic graphs and ν <0for
heterophilic graphs. It is noticed that when ν <0, the message can propagate even for unconnected nodes in
the case of ν <0.
Advection-diffusion-reaction. ADR-GNN (Eliasof et al., 2023) further adds an explicit advection term
on top of the reaction-diffusion process considered in (Choi et al., 2023).
ADR-GNN :∂X
∂t= div( A(X)·∇X) + div( V◦X) +R(X).
The work considers homogeneous diffusion with channel scaling, i.e., div(A(X)·∇X) =−LXdiag(θ),
where θ∈Rcis the channel-wise scaling factor. In contrast to CDE (Zhao et al., 2023b), the advection
term concerns two directional velocity fields Vi,j,Vj,i∈Rc, which measures both in-flow and out-flow
of density, with (V◦X)i,j:=Vj,i⊙xj−Vi,j⊙xi. By further ensuring channel-wise row stochasticity
of both Vi,jandVj,i,Vi,j⊙xiis interpreted as the mass of node jto be transported to node i. Thus
the advection term, given by div(V◦X)i=/summationtext
j:(i,j)∈EVj,i⊙xj−xi, quantifies the net flow of density at
nodei. The reaction term R(X)is parameterized by additive and multiplicative MLPs with a source term,
R(X) =σ(XW 1+tanh(XW 2)⊙X+X0W3). Different to previous works, the paper considers operator
splitting for discretizing the continuous dynamics, i.e., by separating the propagation of the three processes.
Such a scheme allows separate treatment and analysis of each process. Particularly, Eliasof et al. (2023)
demonstrate the mass preserving property and stability of the advection operator. Empirically, ADR-GNN
has shown promising performance for modelling not only the static graphs but also spatial temporal graphs
where advection-diffusion-reaction process has been successful (Fiedler & Scheel, 2003).
Gating. It has been shown that controlling the speed of diffusion through convection/advection term is able
to counteract the smoothing process. Rusch et al. (2023b) adopt a different strategy by explicitly modelling a
gating function on the diffusion:
G2:∂X
∂t=T(X)⊙div(A(X)·∇X)
where T(X)∈[0,1]n×ccollects the rate of speed for each node and across each channel. Specifically, the rates
depend on the graph gradient as T(X)i,k=tanh(/summationtext
j∈Ni|ˆXj,k−ˆXi,k|p),p> 0where ˆxi=/summationtext
j∈Ni/hatwideA(X)i,jxj
and/hatwideA(X)is another message aggregation. Conceptually, the gating rates T(X)i,:for nodeidepend on the
channel-wise graph gradients to all its neighbours. The use of tanh(·)ensures when/summationtext
j∈Ni|ˆXj,k−ˆXi,k|p→0,
the rateT(X)i,kvanishes at a faster rate. This correspondingly shuts down the update for node iand
thus avoid oversmoothing. The paper also considers more general choices of coupling functions in place of
div(A(X)·∇X)where nonlinearity is added.
The idea of gating has been similarly explored in DeepGRAND (Nguyen et al., 2023), which utilizes a
channel-wise scaling factor ⟨X⟩p∈Rn×din place of T(X)where⟨X⟩p
:,k=∥X:,k∥p1n. The dynamics also
incorporates a perturbation to the diffusivity as A(X)−(1 +ϵ)I. The scaling factor and perturbation help
regulate the convergence of node features so that the node features neither explodes nor converges too fast to
the steady state.
14Published in Transactions on Machine Learning Research (02/2024)
Reverse diffusion. Similar to the idea in (Choi et al., 2023) that simultaneously accounts for low-pass and
high-pass filters, Shao et al. (2023) introduce a reverse diffusion process based on the heat kernel. When
coupled with heat diffusion, it leads to a process that accommodates both smoothing and sharpening effects:
MHKG :∂X
∂t=/parenleftbig
diag(θ1) exp(f(/hatwideL)) + diag( θ2) exp(g(/hatwideL))−I/parenrightbig
X,
where θ1,θ2are learnable filters and f,gare scalar-valued functions defined over the eigenvalues of /hatwideL,
e.g.,f(/hatwideL):=Uf(Λ)U⊤wheref(Λ) =diag([f(λi)]n
i=1), with Λbeing the diagonal matrix collecting the
eigenvalues and Ucollecting the eigenvectors of the normalized Laplacian /hatwideL. In particular, f,gare assumed
to be opposite in terms of monotonicity. In the simplest case, suppose f(/hatwideL) =−/hatwideLandg(/hatwideL) =/hatwideL, then the
two terms in MHKG correspond to the heat kernel and its reverse. The filtering coefficients θ1,θ2controls
the relative dominance of the two terms, where the former smooths while the latter sharpens the signals.
Anti-symmetry. In (Gravina et al., 2023), a stable and non-dissipative system is proposed by imposing
the additional anti-symmetric structure for channel mixing matrix as
A-DGN :∂X
∂t=X(W−W⊤) +FG(X),
where we omit the nonlinearity and a bias term to show only the driving factors. Here, FG(X)is a coupling
functionsimilarasin(Ruschetal.,2022), suchasasimplehomogeneousmessagepassing FG(X)i=/summationtext
j∈NiVxj
for some weight matrix Vor the one with attention mechanism (Veličković et al., 2018). The incorporation
of anti-symmetry constraint for the channel mixing renders the system to be stable and non-dissipative,
both due to the fact that the Jacobian of dynamics has pure imaginary eigenvalues, i.e., all real parts of
the eigenvalues are zero. This suggests the solutions to the system remain bounded under perturbation
of initial conditions, which concludes the stability of the evolution. In addition, the zero real part of the
eigenvalues suggests the sensitivity of the node signals to its initial values, i.e., the magnitude of∂xi(t)
∂xi(0),∀i,t
stays constant throughout the dynamics. This result infers that oversmoothing in the limit does not occur
as the final state still depends on the initial conditions as limt→∞∂xi(t)
∂xi(0)̸= 0. Further, this also suggests
the magnitude of∂Lloss
∂xi(0)remains unchanged over time and hence gradient vanishing or explosion is avoided
during backpropagation. This allows the dynamics to be propagating to the limit and capture long-range
interactions without facing the issue of oversmoothing or training instability.
3.5 Geometry-underpinned dynamics
Previous sections have viewed graphs from its trivial topology, and standard dynamics on graphs amounts to
propagating information from node to edge space and back, only utilizing the connectivity between nodes.
In fact, graphs can often be viewed as discrete approximations of more general topological spaces such as
Riemannian manifolds, which possess complex continuous geometries. In this section, we show how dynamics
on graphs can be underpinned with additional geometric structure, such as sheaves and stalks in (Bodnar
et al., 2022), and cliques and cochains in (Gruber et al., 2023).
Sheaf diffusion. Hansen & Gebhart (2020); Barbero et al. (2022a) and Bodnar et al. (2022) leverage
cellular sheave theory to endow a geometric structure for graphs. Specifically, each node i∈Vand edge
eij={i,j}∈E(undirected) is equipped with a vector space structure F(i),F(eij), with a linear map
Fi◁e ij:F(i)→F (eij)(called restriction map) that connects the node to edge spaces. Its adjoint operator
F⊤
i◁e ij:F(eij)→F (i)does the reverse. The direct sum of all vector spaces of nodes is called the space of
0-cochains, denoted by C0(G;F):=/circleplustext
i∈VF(i). Suppose, without loss of generality, that all vector spaces
F(i),F(eij)ared-dimensional, we can represent Fi◁e ijas ad×dmatrix. Further, suppose x∈C0(G;F),
then each xi∈Rdandx∈Rndby stacking the feature vectors across all nodes.
Under the construction of vector spaces on nodes and edges, the concepts of graph gradient and divergence
require to utilize the restriction maps because the vector spaces are not directly comparable. That is, the
graph gradient (also known as the co-boundary map) is defined as (∇Fx)i,j:=Fj◁e ijxj−Fi◁e ijxi, where
15Published in Transactions on Machine Learning Research (02/2024)
the restriction maps transport the signals to a common disclosure space. The graph divergence is thus
similarly defined as (divFG)i=/summationtext
j∈NiF⊤
i◁e ijGeij, forGeij∈F(eij). This leads to the definition of sheaf
Laplacian as LF(x)i=divF(∇Fx)i=/summationtext
j∈NiF⊤
i◁e ij(Fi◁e ijxi−Fj◁e ijxj). The sheaf Laplacian is an
nd×ndblock positive semi-definite matrix, with the diagonal blocks (LF)i,i=/summationtext
j∈NiF⊤
i◁e ijFi◁e ijand
off-diagonal blocks (LF)i,j=−F⊤
i◁e ijFj◁e ij. A symmetrically normalized sheaf Laplacian can be similarly
computed by D−1/2
FLFD−1/2
FwithDFis the block diagonal of LF. The corresponding sheaf diffusion process
is∂X
∂t= divF(∇FX) =−LFX
where here X∈R(nd)×c, wherecis the feature channels and divF,∇F,LFare applied channel-wise. The
Sheaf diffusion turns out to be the gradient flow of the sheaf Dirichlet energy tr(X⊤LFX), which measures
the smoothness of signals in the disclosure space. For practical settings where sheaf structure is unavailable,
one can construct such a feature through input embedding. It is worth highlighting that, when d= 1, the
sheaf Laplacian reduces to the classic graph Laplacian and the sheaf diffusion becomes the heat diffusion. In
(Bodnar et al., 2022), a variety of restriction maps are constructed, which leads to dynamics flexible enough
to handle different types of graphs and avoid oversmoothing. The paper also develops a general framework
for learning the sheaf Laplacian from the features and include channel mixing and nonlinearity to increase
the expressive power:
NSD :∂X
∂t=−σ(LF(X)(I⊗W1)XW 2)
where W1∈Rd×dtransforms the feature vectors and W2∈Rc×c′mixes the channels and ⊗denotes the
Kronecker product. LF(X)is parameterized via a matrix-valued function on the current feature values.
In (Bodnar et al., 2022), the sheaf Laplacian is parameterized by Fi◁e ij=σ(V[xi∥xj])where·∥·denotes
concatenation. A follow-up work (Barbero et al., 2022b) devises a sheaf attention mechanism to further
enhance the diffusion process. Let A(X)∈Rn×nbe a matrix of learnable attention coefficients (the same as
in GAT (Veličković et al., 2018)), and let /hatwideA(X):=A(X)⊗1d×dthat assigns uniform attention coefficients
for each feature dimension within a vector space. The attentive sheaf diffusion (SheafAN) is introduced as
∂X
∂t= (/hatwideA(X)⊙AF(X)−I)X, where (AF(X))i,j=F⊤
i◁e ijFj◁e ijis the sheaf adjacency matrix with self-loop,
i.e.,eii∈E. A second-order sheaf PDE (NSP) is proposed in (Suk et al., 2022) using the wave equation as
∂2X
∂t2= divF(∇FX).
Bracket dynamics. Gruber et al. (2023) propose to use geometric brackets that implicitly parameterize
dynamics on graphs that satisfy certain properties while equipping graphs with higher-order structures. The
formulation requires concepts from structure-preserving bracket-based dynamics and exterior calculus. In
general, for a state variable x, its dynamics can be given by some combination of reversible and irreversible
brackets. The reversible bracket (also known as Poisson bracket) is denoted by {A,E}:=⟨∂A
∂x,˜L∂E
∂x⟩for some
skew-symmetric operator ˜L∗and some inner product ⟨·,·⟩. The reversibility is a result of energy conservation.
Theirreversible bracket is defined by [A,E]:=⟨∂A
∂x,M∂E
∂x⟩for some (either positive or negative) semi-definite
operatorM. The irreversibility describes the loss of energy from the system due to friction or dissipation.
The double bracket {{A,E}}:=⟨∂A
∂x,˜L2∂E
∂x⟩is an irreversible bracket.
For simplicity, the paper considers A=xand one can simplify the brackets as [x,E] =M∂E
∂xand{x,E}=
˜L∂E
∂x. The paper considers four different types of dynamics leveraging both reversible and irreversible brackets:
Hamiltonian :∂x
∂t={x,E};
Gradient :∂x
∂t=−[x,E];
Double bracket :∂x
∂t={x,E}+{{x,E}};
Metriplectic :∂x
∂t={x,E}+ [x,S],(3)
whereE(x)is referred to as the energy of the state and S(x)is the entropy. The dynamics of each process
captures fundamentally different systems and has natural substance in physics. The Hamiltonian dynamics
∗Here, in order not to be confused with the Laplacian Lused in previous discussions, we use ˜L.
16Published in Transactions on Machine Learning Research (02/2024)
leads to a complete, isolated system in the sense that no energy is lost to the external environment. In
contrast, both the gradient and double bracket dynamics are incomplete where the energy is lost through the
process. Metriplectic dynamics is complete by further requiring the degeneracy conditions ˜L∂S
∂x=M∂E
∂x= 0.
These conditions ensure the conservation of energy, i.e.,∂E
∂t= 0and the entropy inequality, i.e.,∂S
∂t≥0in an
isolated system.
In order to properly generalize the dynamics to discrete domains like graphs, one requires to identify the state
variable, an inner product structure as well as energy and entropy. Rather than only considering node features
as the state variable, Gruber et al. (2023) extend the framework to higher-order clique cochains, including
edges and cycles. Formally, let Ωkbe the set of k-cliques on a graph G, which contains ordered, complete,
subgraphs generated by (k+ 1)-nodes. For example, nodes, edges and triangles, correspond to the 0-clique,
1-clique and 2-clique respectively. The exterior derivative operator is denoted as dk:F(Ωk)→F(Ωk+1)where
F(Ω)represents a function space over the domain Ω. The specific structure of Fdepends on the chosen
inner product⟨·,·⟩. The dual derivative d∗
k:F(Ωk+1)→F(Ωk)is given as the adjoint of the dkthat satisfies
⟨dkf,G⟩k+1=⟨f,d∗
kG⟩kfor anyf∈F(Ωk),G∈F(Ωk+1). One common choice of Fis theL2space, where
one can derive d∗
k=d⊤
k. For example, d0, is the graph gradient on nodes and d∗
0becomes the graph divergence
on edges. The classic graph Laplacian can be computed as d∗
0d0=d⊤
0d0=G⊤Gas we have shown in Section
2.
Instead, the paper pursues an inner product parameterized by positive definite matrices A0,A1,...,Akup to
k-cliques. For example on node space Ω0, where the L2space has inner product f⊤gforf,g∈F(Ω0), the
generalized inner product is given by f⊤A0g. Under such choice, one can show d∗
k=A−1
kd⊤
kAk+1.
The state variable is set to be a node-edge feature pairs, denoted by x= (q,p), which can be treated as
the position and momentum of a phase space. Further, the following operators are utilized to extend the
dynamics in (3) to graphs,
˜L=/parenleftbigg0−d∗
0
d00/parenrightbigg
, ˜G=/parenleftbiggd∗
0d0 0
0d∗
1d1+d0d∗
0/parenrightbigg
,and ˜M=/parenleftbigg0 0
0A1d∗
1d1A1/parenrightbigg
.
It can be verified that ˜Lis skew-symmetric and ˜G,˜Mare symmetric positive definite with respect to the
block-diagonal inner product parameterized by A=diag(A0,A1). Furthermore, let X= (Q,P)denote the
tuple of node and edge feature matrices, the energy considered is the total kinetic energy on both node and
edge spaces, i.e., E(X) =1
2(∥Q∥2+∥P∥2).The gradient with respect to the generalized inner product (called
A-gradient) can be computed as ∇AE(X) = [A−1
0∂E
∂Q,A−1
1∂E
∂P]⊤= [A−1
0Q,A−1
1P]⊤. For the Metriplectic
dynamics, it is in general non-trivial to identify an entropy such that the degeneracy conditions hold. Hence
the paper constructs a separate energy and entropy function pair as Em(X) =fE(Q) +gE(d0d⊤
0P)and
Sm(X) =gS(d⊤
1d1P), for some node function fEand edge functions gE,gSapplied channel-wise. The
A-gradient is derived as
∇AEm(X) =/parenleftbigg
A−1
01⊗∇AfE(Q)
d0d⊤
01⊗∇AgE(d0d⊤
0P)/parenrightbigg
,∇ASm(X) =/parenleftbigg0
A−1
1d⊤
1d11⊗∇AgS(d1d⊤
1P)/parenrightbigg
.
Importantly, the degeneracy conditions ˜L∇AS=˜M∇AE= 0are satisfied by construction.
Finally the generalized dynamics from (3) to graphs are
Hamiltonian G:∂X
∂t=˜L(X)∇AE(X);
GradientG:∂X
∂t=−˜G(X)∇AE(X);
Double bracket G:∂X
∂t=˜L(X)∇AE(X) +˜L2(X)∇AE(X);
MetrplecticG:∂X
∂t=˜L(X)∇AEm(X) +˜M(X)∇ASm(X),
where the operators ˜L(X),˜G(X),˜M(X)are state-dependent through attention mechanism to construct the
metric tensor A0,A1, which thus parameterizes the exterior derivative operators.
17Published in Transactions on Machine Learning Research (02/2024)
Remark 1 (Connection to GCN and GAT/GRAND) .GCN can be seen as the GradientGdynamics with
A0,A1parameterized by node degrees. Setting A0as a diagonal matrix (on nodes) with diagonal entries
a0,ii=/radicalbig
degiandA1as the identity matrix (over edges), we can verify (d∗
0d0A−1
0Q)i=/summationtext
j∈Ni(qi/degi−
qj//radicalbigdegidegj) = (/hatwideLQ)i. In this case, GradientGrecovers the heat equation (with normalized Laplacian)
when P= 0and thus leads to GCN under discretization.
Similarly, GAT can be seen as the same GradientGdynamics while learning a metric structure A0,A1. That
is, choosea0,ii=/radicalig/summationtext
j∈Niexp(attn( qi,qj))anda1,eij=exp(attn(qi,qj)). Let the attention coefficient be
a(qi,qj) =a1,eij/a0,ii. Then one can show GradientGdynamics with P= 0corresponds to a (symmetrically)
normalized version of GRAND.
This result demonstrates the irreversible nature of GCN or GAT/GRAND dynamics where energy dissipates,
while other dynamics are either conservative or partially dissipative.
Hamiltonian mechanics. TheHamiltonian mechanics has also been considered in (Kang et al., 2023)
where instead of using the edge features as the momentum, the work parameterizes the momentum with
a neural network from the node features. This leads to distinction compared to the previous works in
the evolution of the node features, which is decoupled from the graph structure. Let Qdenote the node
features and the momentum is computed as P=MLP Θ(Q). The Hamiltonian mechanics is determined by a
Hamiltonian function H(Q,P), which characterizes the total energy of the system. The dynamics is governed
by the Hamiltonian equation
∂Q
∂t=∂H
∂P,∂P
∂t=−∂H
∂Q.
The paper motivates a variety of parameterization for the Hamiltonian H. One specific choice of Hamiltonian
isH(Q,P) =tr(P⊤M(Q)P)where M(Q)represents the inverse metric tensor at Q(also learnable in the
local neighbourhood). Its solution Q(t)recovers the geodesic (a locally shortest curve) on the manifold with
metric parameterized by M(Q)−1atQ. Hence, nodes are effectively embedded to a (implicit) manifold space
where the metric is learnable. Unlike previous methods, where the dynamics of the features depend on a
coupling function regulated by the graph topology, here the evolution of Qis independent across the nodes.
To further incorporate the graph structure, message passing by neighbourhood aggregation is performed
after the feature evolution via Hamiltonian equation. Multiple layers of Hamiltonian dynamics and message
passing are stacked to model complex geometries and node embeddings.
A follow-up work (Zhao et al., 2023a) extends the formulation by considering general graph-coupled Hamilto-
nian functionHG(Q,P). For example, the paper considers a Hamiltonian function defined as the norm of
the output from a two-layer GCN, where∂HG
∂Q,∂HG
∂Pare computed from auto-differentiation. Further, the
paper studies various notions of stability on graphs from the theory of dynamical system, including BIBO,
Lyapunov, structural and conservative stability. The work conducts a systematic analysis and comparison
of proposed Hamiltonian-based dynamics with existing graph neural dynamics, such as GRAND, BLEND,
Mean Curvature and Beltrami. It is found that the conservative Hamiltonian dynamics has shown improved
robustness against adversarial attacks.
3.6 Dynamics as gradient flow
Most of the aforementioned designs of GNNs are inspired by evolution of some underlying dynamics. The
learnable parameters, such as channel mixing, are usually added after discretization to increase the expressive
power. In (Giovanni et al., 2023), the dynamics is instead given as the gradient flow of some learnable energy.
The framework is general and includes many of the existing works as special cases (as long as the channel
mixing matrix is symmetric)†. The parameterized energy takes the following form:
Eθ(X) =1
2tr(X⊤XΩ)−1
2tr(X⊤AXW ) +φ0(X,X0),
†Although many existing works can be written as gradient flow of some energy, their motivation comes mostly from the
dynamics, not from the energy.
18Published in Transactions on Machine Learning Research (02/2024)
where Ais the (normalized) adjacency matrix and Ω,W∈Rc×care assumed to be symmetric‡. The first
term determines the external forces exerted upon the system and the second term reflects the pairwise
interactions while the last term quantifies the energy preserved by the source term X0. Although φ0can
be general, the paper considers a form φ0(X,X0) =tr(X⊤X0˜W). The gradient flow of Eθ(X)yields the
dynamics of the following general form,
GRAFF :∂X
∂t=−∇Eθ(X) =−XΩ+AXW−X0˜W.
This formulation includes many of the existing dynamics-motivated GNNs. When Ω=W,˜W= 0, this
corresponds to the evolution of (residual) GCN or GAT/GRAND (Chamberlain et al., 2021a) if Ais
constructed by attention mechanism. If ˜W̸= 0, this corresponds to the GRAND++ (Thorpe et al., 2022) and
thus also the CGNN (Xhonneux et al., 2020). The decrease of the general energy does not necessarily lead to
a decrease in the Dirichlet energy (which is a special case of Eθ(X)withΩ=W=I,φ0= 0). This is mainly
due to the occurrence of both attractive and repulsive effects along the positive and negative eigen-directions
ofW. More formally, one decompose W=Θ⊤
+Θ+−Θ⊤
−Θ−and rewrite the energy (without φ0)as
Eθ(X) =1
2/summationdisplay
i∈V⟨xi,(Ω−W)xi⟩+1
4/summationdisplay
(i,j)∈E∥Θ+(∇X)i,j∥2−1
2/summationdisplay
(i,j)∈E∥Θ−(∇F)i,j∥2.
It has been shown that the gradient flow minimizes the gradient along the positive eigen-directions (which
leads to smoothing effect) while maximizing the gradient along the negative eigen-directions (which leads to
sharpening effect). In contrast, minimizing the Dirichlet energy always induces smoothing effect because
W=Iwith only positive eigen-directions. This allows GRAFF to avoid oversmoothing and produce
sharpening effects as long as the Whas sufficiently large negative spectrum.
3.7 Multi-scale diffusion
Previous sections have mostly focused on dynamics with local diffusion. In other words, the signal/density at
certain node changes depending on its immediate neighbourhood. The communication with distant nodes
only happens when diffusion time is sufficiently large. Section 3.3 discusses dynamics that leverages non-local
diffusion, but nonetheless mostly restricted to a single scale at each diffusion step, e.g., single αin fractional
diffusion (Maskey et al., 2023) and tθfor time derivative diffusion (Behmanesh et al., 2023). Multi-scale graph
neural networks, such as ChebyNet (Defferrard et al., 2016), LanczosNet (Liao et al., 2018) and Framelet
GNN (Zheng et al., 2021b) are capable of capturing multi-scale graph properties through spectral filtering on
the graph Laplacian. The eigen-pairs of graph Laplacian encode structural information at different levels of
granularity and separate processing of each resolution provides insights into both local and global patterns.
Several recent works have adapted the continuous dynamics formulation to multi-scale GNN. Han et al.
(2022) introduce a multi-scale diffusion process via graph framelet . Apart from the multi-scale properties
shared with other spectral GNNs, graph framelet further separates the low-pass from high-pass filters. Let
W0,J∈Rn×ndenote the low-pass framelet transform and Wr,j∈Rn×n,r= 1,...,R,j= 1,...,Jdenote the
high-pass framelet transforms, where Ris the number of high-pass filter banks and Jis the scale level. For a
multi-channel graph signal X∈Rn×c,W0,JXandWr,jX,r= 1,...,R,j= 1,...,Jrepresent low-pass and
high-pass framelet coefficients. For notation clarity let I={(0,J)}∪{ (r,j)}1≤r≤R,1≤j≤Jbe the framelet
index set. Due to the reconstruction property of framelets, it satisfies that/summationtext
(r,j)∈IW⊤
r,jWr,jX=X.
The spatial framelet diffusion proposed in (Han et al., 2022) is given as
GradFUFG :∂X
∂t=−/summationdisplay
(r,j)∈I/parenleftbig
W⊤
r,jWr,jXΩr,j−W⊤
r,j/hatwideAWr,jXWr,j/parenrightbig
,
where Ωr,j,Wr,jare symmetric channel mixing matrices. When Ωr,j=Wr,j=I, by the tightness of framelet
transform, GradFUFG reduces to the heat equation as∂X
∂t=−/hatwideLX. The spatial-based framelet diffusion
‡The symmetric assumption is not required except for the purpose of simplification for the gradient derivation and analysis.
19Published in Transactions on Machine Learning Research (02/2024)
can be seen as gradient flow of a generalized Dirichlet energy, which also includes the parameterized energy
considered by GRAFF (Giovanni et al., 2023) as a special case. It can be seen that, due to the separate
modelling of low-pass and high-pass as well as different scale levels, the dynamics can adapt to datasets of
different homophily levels as well as has the potential of avoiding oversmoothing.
4 On oversmoothing, oversquashing, heterophily and robustness of GNN dynamics
As we have briefly mentioned, the main hurdles for successful designs of GNN include oversmoothing,
oversquashing, graph heterophily and adversarial perturbations. The framework of continuous GNNs allows
principled analysis and treatment for the issues identified. In fact, many of the previously introduced dynamics
are motivated to overcome the limitations of existing GNN designs. Nonetheless, there exist possible trade-offs
such as between oversmoothing and oversquashing (Giraldo et al., 2022; Shao et al., 2023), which makes it
challenging for GNN dynamics to avoid both phenomenon at the same time. Conceptually, common strategies
for mitigating oversquashing relies on graph rewiring (Topping et al., 2022) that increases propagation
strength, which likely enhances the smoothing effects. Apart from the trade-off, new problems may emerge as
a result of resolving the existing ones. One example is the training difficulty (from gradient vanishing or
explosion) associated with complex and long-range dynamics.
This section provides a holistic overview on these undesired behaviours of GNNs through the lens of continuous
dynamics, along with a discussion on how existing approaches address the issues.
Oversmoothing. Oversmoothing refers to a phenomenon that node signals become progressively similar
as a result of message passing, and converge to a state independent of the input signals. Although there
are many different characterizations for oversmoothing, most if not all rely on the (normalized) Dirichlet
energy that measures node similarity. Recall from Section 2, the Dirichlet energy is defined as Edir(X) =
1
2/summationtext
(i,j)∈E∥xi//radicalbig
degi−xj//radicalbigdegj∥2=tr(X⊤/hatwideLX), where/hatwideLis the symmetrically normalized Laplacian.
Because oversmoothing claims a loss of distinguishability across the nodes, a natural quantification of
oversmoothing is Edir(X(t))→0ast→∞. From the definition of the Dirichlet energy, in the limiting
state, nodes with same degree collapse into a single representation. A stronger notion of oversmoothing
has been considered in (Rusch et al., 2022; 2023a) requiring an exponential decay of the Dirichlet energy
for oversmoothing to occur, i.e., Edir(X(t))≤C1e−C2t,∀t>0and for some constants C1,C2>0. This is
equivalent to claiming that the system has a node-wise constant, exponentially stable equilibrium state. In
this work, we focus on a notion of oversmoothing called low-frequency dominance (LFD) put forward by
Giovanni et al. (2023). A dynamics is said to be low-frequency dominant if Edir(X(t)/∥X(t)∥)→0. It can
be readily verifies that when Edir(X(t))→0, thenEdir(X(t)/∥X(t)∥)→0, while the reverse argument is
false (see Appendix B.2 of Giovanni et al. (2023) more discussions). The normalization by ∥X(t)∥reveals
the dependence of asymptotic behaviour to the dominant spectrum of the dynamics, where low-frequency is
related to the smoothing effect.
In the sense of LFD, it can be shown that the dynamics relying on (anisotropic) diffusion would incur
oversmoothing in the limit, such as the linear versions of GRAND (Chamberlain et al., 2021a), BLEND
(Chamberlain et al., 2021b), and DIFFormer (Wu et al., 2023b). See for example (Giovanni et al., 2023,
Theorem B.4) for a formal proof. Dynamics that involves (non-smooth) edge indicators, like Mean Curvature,
Beltrami flows (Song et al., 2022), p-Laplacian (Fu et al., 2022) and DIGNN (Fu et al., 2023) also correspond
to LFD dynamics although the convergence is slower compared to (smooth) diffusion dynamics. Such a
statement has been formalized in (Fu et al., 2023).
We summarize existing remedies for oversmoothing as follows.
(1)Source term : One remedy that steers the dynamics away from the low-frequency dominance is to
include a source term. For example, in p-Laplacian (Fu et al., 2022) and DIGNN (Fu et al., 2023),
the source term is implicitly added in terms of input dependent regularization, while GRAND++
(Thorpe et al., 2022) and CGNN (Xhonneux et al., 2020) explicitly inject the source information to
the dynamics. The presence of a source term ensures the Dirichlet energy is lower bounded above
zero where the limiting state is non-constant.
20Published in Transactions on Machine Learning Research (02/2024)
(2)Non-smoothing dynamics : Choosing a non-smoothing dynamics can help avoid oversmoothing, such
as an oscillatory process as in PDE-GCN H(Eliasof et al., 2021) and GraphCON (Rusch et al.,
2022). Oscillatory system usually conserves rather than dissipates energy. In particular, it has been
proved that the constant equilibrium state of the dynamics is not exponentially stable because any
perturbation remains due to the energy conservation. The reversible dynamics, such as Hamiltonian
and metriplectic in (Gruber et al., 2023) is also non-smoothing by construction.
(3)Diffusion modulators : Imposing external forces to counteract or modulate the diffusion mechanism is
beneficial for mitigating oversmoothing. Examples include the high-pass filters in GREAD (Choi
et al., 2023) and GradFUFG (Han et al., 2022), repulsive diffusion coefficients in ACMP (Wang
et al., 2023) and ODNet (Lv et al., 2023), negative spectrum in GRAFF (Giovanni et al., 2023),
reverse heat kernel in MHKG (Shao et al., 2023), anti-symmetry in A-DGN (Gravina et al., 2023),
diffusion gating in G2(Rusch et al., 2023b) and DeepGRAND (Nguyen et al., 2023). In essence,
oversmoothing can be avoided in the above dynamics by diminishing smoothing effects in the limit of
evolution.
(4)Higher-order geometries : Bodnar et al. (2022) demonstrate the cause of oversmoothing from the
perspective of choosing a trivial geometry. In contrast, by enriching the graph with a higher-order
sheaf topology, the asymptotic behaviour of the diffusion process can be better controlled. Indeed,
the sheaf Dirichlet energy can be increased with a suitable choice of non-symmetric sheaf structure
on graphs, thus avoiding oversmoothing under this regime (Bodnar et al., 2022, Proposition 17).
Heterophily. Unlike homophilic graphs where neighbouring nodes tend to share similar features and labels,
nodes in heterophilic graphs usually exhibit dissimilar patterns compared to the neighbours. GNN that is
dominated by smoothing effect is generally less preferred for such graphs. Following (Giovanni et al., 2023),
the ability of a dynamics to adapt for heterophilic graphs is measured in terms of high-frequency dominance
(HFD). A dynamics is said to be high-frequency dominant if Edir(X(t)/∥X(t)∥)→ρL, whereρL≤2is
the largest eigenvalue of /hatwideL. Intuitively, HFD dynamics is dominated by a sharpening effect and eventually
converge to the highest-frequency eigenvectors of the Laplacian where node information gets separated. In
this work, we equate the ability of a dynamics handling heterophily with its ability to become dominated by
the highest frequency. Although HFD dynamics is also undesired due to information loss related to the low
frequencies, here we focus on the ability, meaning that a system should have the flexibility to converge to
the eigenspace associated with the highest frequency, which allows separability of bipartite graphs. Under
such notion, dynamics that is purely smoothing cannot be HFD. The incorporation of a source term and
(positive) graph re-weighting alone are not sufficient to turn a smoothing dynamics into a sharpening one
even though empirically these strategies boost performance on heterophilic datasets. This is because the
driving mechanism of the resulting system is still diffusion.
Existing dynamics that provably accommodates separating effects can be classified as follows.
(1)Sharpening induced forces : One scheme for a system to be HFD is to explicitly introduce sharpening
induced forces. Most of the diffusion modulators identified for tackling oversmoothing amounts to
incorporate such anti-smoothing forces, including GREAD (Choi et al., 2023), GradFUFG (Han
et al., 2022), ACMP (Wang et al., 2023), ODNet (Lv et al., 2023), GRAFF (Giovanni et al., 2023),
and MHKG (Shao et al., 2023). Conceptually, the dynamics is dominated by the high frequency if
the magnitude of the sharpening effect surpasses the smoothing effect.
(2)Higher-order geometries : Once the graph is equipped with a sheaf topology, Bodnar et al. (2022)
have shown that choosing a higher-dimensional stalks and non-symmetric restriction maps allows
sheaf diffusion to gain linear separability in the limit for heterophilic graphs.
We highlight that other schemes, such as oscillatory dynamics in GraphCON (Rusch et al., 2022) and
convection/advection process in (Zhao et al., 2023b; Eliasof et al., 2023) also demonstrate empirical success
under heterophilic settings. Nevertheless, theoretical guarantee for theses schemes in adapting to heterophilic
graphs is currently unavailable.
21Published in Transactions on Machine Learning Research (02/2024)
Oversquashing. The phenomenon of oversquashing has firstly been empirically observed by Alon & Yahav
(2021) and later formally studied by Topping et al. (2022); Di Giovanni et al. (2023). Such a phenomenon has
been identified as a key factor hindering expressive power of GNNs (Di Giovanni et al., 2023). Oversquashing
concerns a scenario where long-range communication between distant nodes matters for the downstream
tasks whereas message passing squashes information due to the exponentially growing size of the receptive
field. A closely related concept is under-reaching, which refers to shallow GNN not being able to fully explore
long-range interactions (Barceló et al., 2020). A critical difference between under-reaching and oversquashing
is that the former can often be alleviated by increasing the depth of a GNN while the latter can occur even
with deep GNNs. One major cause of such a phenomenon is the local diffusion process according to the
given graph structure, where messages are exchanged only within the neighbourhood. This can be formally
characterized by the sensitivity across nodes, measured through the Jacobian. More formally, consider a
(discretized) dynamics Xℓ+1=AG(Xℓ), whereAGis a coupling function that aggregates information based
on input graphG. The sensitivity between nodes i,j∈Vafterℓiterations of updates can be computed as/vextenddouble/vextenddouble∂xℓ
i
∂x0
j/vextenddouble/vextenddouble
2, where a smaller value indicates lower sensitivity and thus higher degree of oversquashing. In the
simple case of isotropic diffusion AG(Xℓ) =σ(AXℓW), one can show/vextenddouble/vextenddouble∂xℓ
i
∂x0
j/vextenddouble/vextenddouble
2≤cℓ(Aℓ)i,j, where Aℓdenotes
theℓ-th matrix power of Aandc>0is a Lipschitz constant that depends on the activation function and
W. It can be readily noticed that the graph structure encoded in Acritically determines the severity of
oversquashing.
Below are strategies employed in GNN system that avoids oversquashing.
(1)Graph topology modification : One natural strategy in addressing the oversquashing is to enhance
communications by rewiring the graph structure. GRAND (Chamberlain et al., 2021a) and BLEND
(Chamberlain et al., 2021b) dynamically modify the connectivity according to the updated node
features, which amounts to changing the spatial discretization over time. DIFFormer (Wu et al.,
2023b), on the other hand, adopts a fully connected graph and thus avoids oversquashing by allowing
message passing between all pairs of nodes.
(2)Non-local dynamics : Alternatively, non-local dynamics constructs dense graph communication
function and thus mitigates oversquashing by design. This can be achieved for example by taking the
fractional power of message passing matrix as in FLODE (Maskey et al., 2023), leveraging the dense
quantum diffusion kernel in QDC (Markovich, 2023), learning the timestep of heat kernel in TIDE
(Behmanesh et al., 2023), and capturing the entire path of diffusion as in G2TN (Toth et al., 2022).
(3)Multi-scale and spectral filtering : Multi-scale diffusion in GradFUFG (Han et al., 2022) has the poten-
tial to mitigate oversquashing by simultaneously accounting for information at different frequencies.
Further, properly controlling the magnitude of high-pass versus low-pass filtering coefficients as in
MHKG (Shao et al., 2023) (in the HFD setting) can provably improve the upper bound of the node
sensitivity and thus oversquashing is alleviated.
Despite the success of aforementioned techniques in addressing oversquashing, a potential downside is the
increased complexity of propagating information at each update, which is often associated with the more dense
message passing matrix. We also remark that some methods, including GIND (Chen et al., 2022) and A-DGN
(Gravina et al., 2023) are able to capture long-range dependencies without suffering from oversmoothing. In
particular, the driving mechanisms for such purpose are the implicit diffusion in GIND and anti-symmetric
weight in A-DGN where the former can be viewed as an infinite-depth GNN and the latter allows to build
deep GNNs with constant sensitivity to avoid oversmoothing.
Stability, gradient vanishing and explosion. Lastly, we highlight several other potential pitfalls of GNN
dynamics, that are often less explored in the literature compared to the previous issues, such as robustness
and stability, and gradient vanishing and exploding during training.
Stability and robustness against adversarial attacks is a critical factor in assessing the performance of GNNs.
For continuous dynamics, Song et al. (2022) verify that the graph neural diffusion, like GRAND (Chamberlain
et al., 2021a) and BLEND (Chamberlain et al., 2021b), are empirically more stable to graph topology
22Published in Transactions on Machine Learning Research (02/2024)
perturbation compared to other discrete variants, which follows from the derived theoretical results based on
heat kernel. Song et al. (2022) also prove that, due to the row-stochastic normalization of the diffusivity
matrix in graph neural diffusion, the stability against node perturbation is guaranteed. In (Gravina et al.,
2023), the robustness to the change in initial conditions is explicitly enforced with the required anti-symmetry
(due to the zero real parts of the Jacobian eigenvalues). In (Zhao et al., 2023a), the conservative stability
offered by Hamiltonian dynamics has shown enhanced robustness to adversarial attacks. In addition, Wu
et al. (2023a) improve the generalization ability of GNNs with respect to topological distribution shifts, via
local diffusion and global attention.
Another plight of deep neural networks are vanishing and exploding gradients, which refers to the situation
when gradient exponentially converges to zero or infinity as a result of increasing depth. Because classic graph
neural networks are often designed to be shallow as in GCN (Kipf & Welling, 2017) and GAT (Veličković
et al., 2018), in order to avoid oversmoothing, gradient vanishing or explosion have received less attention in
the GNN community. However, in the continuous regimes, especially when depth increases, these problems
can emerge and severely escalate the training difficulty. In (Rusch et al., 2022), it has been verified that
GraphCON is able to mitigate the vanishing and exploding gradient problems because the gradient of
GraphCON is upper bounded at most quadratically in depth and is shown to be independent of the depth in
terms of decaying behaviour. Due to the anti-symmetric channel mixing in A-DGN (Gravina et al., 2023),
the magnitude of the gradient stays constant during backpropation and hence avoids the problem.
5 On training graph neural dynamics
Most of the works based on continuous GNN dynamics consider the following architecture, which is firstly
introduced in GRAND (Chamberlain et al., 2021a).
X(0) = Emb( Xin), X(T) =X(0) +/integraldisplayT
0∂X(t)
∂tdt, Y= Out( X(T)), (4)
where∂X(t)
∂t=FG(X,∇X)are the graph coupled, parameterized, differential equation introduced in Section
3.§Emb(·),Out(·)are respectively the input embedding and output decoding functions, which are usually
learnable. The formulation in (4)provides a general framework for various learning tasks on graphs, from
node-level to graph-level, from transductive to inductive (see more discussions in Section 7). Similar to classic
GNNs, the training for graph neural dynamics consists of forward and backward propagation. The forward
propagation solves the graph differential equation via numerical integrators, while backward propagation
consists of solving an adjoint differential equation to compute the gradient.
Forward propagation. There exist a variety of numerical solvers for differential equations, once the
continuous dynamics is formulated on graphs. In fact, with the discrete differential operators defined on
graphs, the PDE reduces to an ODE, which can be solved with standard numerical integrators. One natural
strategy for discretizing a continuous dynamics is through finite differences . Particularly, the forward Euler
and leapfrog methods are commonly employed for first-order and second-order ODE respectively. For a
first-order ODE given by∂X
∂t=F(X,t), the forward Euler discretization leverages forward finite time
difference, which gives the update X(t+ 1) = X(t) +τF(X(t),t)for some stepsize τ >0. The forward Euler
is an explicit method in that X(t+ 1)depends on X(t)explicitly. In contrast, backward Euler method is
implicit by discretization X(t+ 1) = X(t) +τF(X(t+ 1),t+ 1), which involves solving a (nonlinear) system
of equations to obtain X(t+ 1).
For a second-order ODE∂2X
∂t=F(X,t), the leapfrog method that leverages centered finite differences yields
the update X(t+ 1) = 2 X(t)−X(t−1) +τ2F(X(t),t), which has been considered for PDE-GCN H(Eliasof
et al., 2021). The leapfrog method can be equivalently derived by rewriting the second-order ODE into a
system of first-order ODEs (concerning both the position and velocity) and then use forward Euler method
to alternatively update the two. In GraphCON (Rusch et al., 2022), a similar idea has been applied for the
more complex second-order dynamics.
§For second-order dynamics like in PDE-GCN Hand GraphCON, the same formulation applies if the second-order ODE is
rewritten as a system of first-order ODEs.
23Published in Transactions on Machine Learning Research (02/2024)
Higher-order methods employ high-order approximations for solving both first- or second-order ODEs and can
beeitherexplicitorimplicit. CommonchoicesincludeRunge–Kutta, whichisaclassofsingle-stepmethodsthat
requires evaluating Fat multiple extrapolated state. One popular variant of Runge-Kutta method is Runge-
Kutta 4 that balances the accuracy and efficiency. It computes X(t+1) = X+τ
6(K1+2K2+2K3+K4)where
k1=F(X(t),t),K2=F(X(t) +τK1/2,t+τ/2),K3=F(X(t) +τK2/2,t+τ/2),K4=F(X(t) +τK3,t+τ).
Multi-step methods, such as Adams-Bashford, store multiple previous steps in order to approximate high-order
derivatives. More advanced solvers set the adaptive stepsize based on the error estimated at each iteration.
Dormand–Prince is a Runge-Kutta method with step-size control and has been widely adopted as the default
solver for ODEs (Dormand, 1996).
Backward propagation To compute the gradient, it is generally memory-consuming to differentiate
directly through the numerical integrators used in the forward pass. Instead, Chen et al. (2018) considered
theadjoint sensitivity method (Pontryagin, 2018), which requires to solve an adjoint differential equation,
which is more memory-efficient and accurate by explicitly controlling the numerical error. Such a scheme
allows to decouple the forward and backward pass where the numerical solvers can be chosen differently. This
is unlike the classic GNNs, where the complexity of the backward pass highly depends on the forward pass.
The forward and backward propagation schemes can be implemented through torchdiffeq package (Chen,
2018). Choosing suitable integration schemes depends on the trade-off between efficiency, accuracy and
stability. From the comparisons in (Chamberlain et al., 2021a;b), it is shown that explicit solvers, like
forward Euler is more efficient, at the cost of being less accurate and stable regarding the choice of step
size. On the other hand, implicit method requires less steps to reach a desired accuracy but requiring high
computational cost per step. We refer to https://github.com/twitter-research/graph-neural-pde for
typical implementation of continuous GNNs.
6 Complexity and scalability of graph neural dynamics
In this section, we compare the computational complexity of introduced graph neural dynamics in this work.
We assume the implementation follows (4)and hence only compares the cost for evaluating∂X
∂t. Here we
denote X∈Rn×cwherenis the number of nodes and cas the number of channels after embedding. We also
letdbe the dimension of attention transformation matrix.
Most dynamics of anisotropic diffusion , such as GRAND (Chamberlain et al., 2021a), require a time complexity
ofO(|E|c)without attention or O(|E|cd). For BLEND, cis further augmented by the dimension of positional
embedding, i.e., O(|E|(c+cpos)). Although DIFFormer (Wu et al., 2023b) requires to compute attention over
the fully connected graph, a linear attention scheme is employed to lower the complexity to O(ncd+n2c)
instead ofO(n2cd). Foroscillatory processes, including PDE-GCN H(Eliasof et al., 2021) and GraphCON
(Rusch et al., 2022), the complexity is on the same order as O(|E|cd)by rewriting the propagation into a
system of first-order dynamics.
Non-local dynamics generally demands a higher complexity by propagation information on a denser graph,
such as QDC (Markovich, 2023), with a complexity O(|E′|c)where|E′|is the edge set corresponding to a
denser graph. For the spectral-based propagation, like FLODE (Maskey et al., 2023), TIDE (Behmanesh
et al., 2023), the cost scales with O(n2c)(after pre-computing eigendecomposition at a cost of O(n3)). For
G2TN (Toth et al., 2022) that requires to capture a long path of information with higher-order tensors, the
complexity depends on the order of tensor mand the number of propagation step L, i.e.,O(|E|m2L+|E|mc).
For diffusion dynamics with external forces , the complexity depends on whether the added forces dominate
the computation of diffusion. For CDE (Zhao et al., 2023b), GREAD (Choi et al., 2023), ACMP (Wang
et al., 2023), ODNet (Hegselmann & Krause, 2002) and G2(Rusch et al., 2023b), A-DGN (Gravina et al.,
2023), the complexity is still dominated by the diffusion term O(|E|c)(orO(|E|cd)). For ADR-GNN (Eliasof
et al., 2023) that requires to compute the edge weights further increases the complexity to O((n+|E|)c2).
Lastly, MHKG (Shao et al., 2023) operates ove the spectral domain and hence costs O(n2c).
ForGeometry-underpinned dynamics , diffusion based on sheaf structure (Bodnar et al., 2022) requires at least
O(|E|fc)(using diagonal maps) where fis the stalk dimension. The bracket dynamics (Gruber et al., 2023)
24Published in Transactions on Machine Learning Research (02/2024)
Table 3: Summary of experiments conducted for graph neural dynamics. HomoandHeterorefers to
experiments for node classification with homophilic and heterophilic graphs. Inductive refers to experiments
validating generalization to unseen graphs. Scalability refers to experiments on large-scale graph benchmarks.
OSM/OSQrefers to experiments validating the oversmoothing and oversquashing issues. STABrefers to
experiments showing robustness and stability against perturbations.
Homo Hetero Inductive Scalability Graph-level OSM OSQ STAB Others experiments
GRAND1✔ ✔ ✔
BLEND2✔ ✔
Mean Curvature3
Beltrami3 ✔ ✔ ✔
p-Laplacian4✔ ✔ ✔ ✔ ✔
DIFFormer5✔ ✔ ✔Image/text classification,
Spatial-temporal prediction
DIGNN6✔ ✔ ✔ ✔ ✔
GRAND++7✔ ✔ ✔ Varying label rate
PDE-GCN8✔ ✔ ✔ ✔ Dense shape correspondence
GIND9✔ ✔ ✔ ✔ ✔
GraphCON10✔ ✔ ✔ ✔ ✔
FLODE11✔ ✔ ✔ Directed graphs
QDC12✔ ✔
TIDE13✔ ✔ ✔ ✔ ✔ ✔ ✔ Geometric graphs
G2TN14✔ ✔ ✔
CDE15✔ ✔
GREAD16✔ ✔ ✔
ACMP17✔ ✔ ✔ ✔
ODNet18✔ ✔ ✔Hypergraph diffusion,
Network simplification
ADR-GNN19✔ ✔ ✔ ✔ Spatial-temporal prediction
G2 20✔ ✔ ✔ ✔ Node regression
MHKG21✔ ✔ ✔ ✔
A-GCN22✔ ✔ ✔ ✔ ✔ Tree-NeighborsMatch
NSD23✔ ✔ ✔
Hamiltonian G, etc.24✔ ✔ Trajectory prediction
HamGNN25✔ ✔ ✔ ✔Link prediction,
Mixed-geometry graphs
HamGNN26✔ ✔ ✔ ✔
GRAFF27✔ ✔ ✔
1(Chamberlain et al., 2021a),2(Chamberlain et al., 2021b),3(Song et al., 2022),4(Fu et al., 2022),5(Wu et al., 2023b),6(Fu et al.,
2023),7(Thorpe et al., 2022),8(Eliasof et al., 2021),9(Chen et al., 2022),10(Rusch et al., 2022),11(Maskey et al., 2023),12(Markovich,
2023),13(Behmanesh et al., 2023),14(Toth et al., 2022),15(Zhao et al., 2023b),16(Choi et al., 2023),17(Wang et al., 2023),18(Lv et al.,
2023),19(Eliasof et al., 2023),20(Rusch et al., 2023b),21(Shao et al., 2023),22(Gravina et al., 2023),23(Bodnar et al., 2022),24(Gruber
et al., 2023),25(Kang et al., 2023),26(Zhao et al., 2023a),27(Giovanni et al., 2023)
scales linearly with the k-cliques and hence if only nodes and edges are considered as in the classic GNNs, the
complexity is O(|E|cd), matching the diffusion dynamics with attention. Finally, GRAFF (Giovanni et al.,
2023) requires a complexity of O(n2c+|E|c)and GradFUFG (Han et al., 2022) has a complexity of O(n2c)
due to the framelet transform.
In summary, the complexity/scalability of graph neural dynamics is mainly determined by the design of graph
differential equations, which matches the discrete counterparts. In fact, using Euler discretization for solving
the differential equations reduces the continuous dynamics into the classic discrete versions, such as GCN
and GAT.
7 Empirical benchmarks and evaluations of graph neural dynamics
This section summarizes empirical experimental procedures for evaluating the performance of different graph
neural dynamics. Because graph neural dynamics can be viewed as generalization of GNNs via continuous
formulation, classic benchmarks for evaluating discrete GNNs are still applicable for the continuous GNNs.
25Published in Transactions on Machine Learning Research (02/2024)
Node-level classification The most widely considered task is node classification, which aims to classify
test nodes in a graph under semi-supervised setting (Kipf & Welling, 2017). For this task, the output Yin(4)
is passed through a linear layer for predicting the probabilities for each class. Common benchmark datasets
include citation networks, Cora (McCallum et al., 2000), Citeseer (Sen et al., 2008), Pubmed (Namata et al.,
2012), co-authorship graphs, including CoauthorCS, CoauthorPhysics (Shchur et al., 2018), co-purchase
graphs, including Computer, and Photo (McAuley et al., 2015). These graph datasets are known to be
homophilic in the sense that neighbours tend to share similar features and labels. To verify whether a
dynamics can adapt to heterophilic graphs, benchmarks including web-page link graphs, Cornell, Texas,
Wisconsin (Pei et al., 2019), actor co-occurrence network (Tang et al., 2009), Wikipedia networks, including
Chameleon and Squirrel (Rozemberczki et al., 2021). Previous datasets are only used for transductive learning,
i.e., classifying nodes in the same graph that is used for training. For inductive learning setup, models
are trained only on training graphs and are expected to predict node labels for test graphs. One popular
dataset is PPI (Hamilton et al., 2017), where each graph corresponds to a different human tissue documenting
protein-protein interactions.
Graph-level classification Apart from the major experiments on node classification, some works consider
graph-level prediction for evaluating the dynamics in extracting graph-level information. For this purpose,
the output Yis passed through some pooling layer for aggregating the information across nodes for before
final prediction. Commonly considered datasets, include MUTAG (Debnath et al., 1991), PTC (Helma
et al., 2001), COX2 (Sutherland et al., 2003), NCI1 (Wale et al., 2008) which collect molecules or chemical
compounds where each node represents an atom and edges represent chemical bonds; PROTEINS (Borgwardt
et al., 2005), which collects proteins where each node represents amino acids and edges are constructed based
on the spatial proximity. The task is to predict the properties at molecule, compound or protein level.
Evaluation for oversmoothing and oversquashing. To evaluate whether a dynamics can potentially
mitigate or even avoid oversmoothing , it is common to show node classification performance as the number
of layers (or equivalently the integration time) increases. Alternatively, showing the evolution of Dirichlet
energy in terms of depth is equally indicative of oversmoothing. On the other hand, it is generally difficult to
explicitly measure the level of oversquashing . Nevertheless, many synthetic experiments have been designed
to test whether a graph dynamics is able to capture long range dependencies. For example in (Gu et al.,
2020; Chen et al., 2022; Di Giovanni et al., 2023), data is created to test the ability of classifying nodes
in a given distance away. It is expected that a dynamics that mitigates oversquashing will suffer less from
the performance degradation as the distance increases. For real applications, datasets that are known to
exhibit strong long-range dependencies are often used for implicitly quantifying the degree of oversquashing
for GNNs, including PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and Peptides-struct (Dwivedi
et al., 2022).
Evaluation for scalability and stability. GNNs are known to suffers from poor scalability to large
graphs because the message passing scheme requires to input the entire graph each step. Common large-scale
graph benchmarks include OGB-arxiv, OGB-proteins and OGB-products (Hu et al., 2020) where the number
of nodes ranges from 13k to2500k. It thus becomes critical to report the runtime and memory consumption of
graph neural dynamics on such large-scale datasets. In addition to scalability, stability and robustness against
adversarial perturbation is another important spectrum of benchmarking different dynamics. In general,
various adversarial attacks, such as adding/removing nodes, injecting noise to node features, modifying
graph topology, can be implemented through Graph Robustness Benchmark (GRB) (Zheng et al., 2021a). A
dynamics that is robust to such attacks is generally preferred.
Other experiments. Apart from the aforementioned benchmark experiments, some works consider tailored
experiments showing the exclusive benefits of the proposed dynamics. This includes experiments on classifying
images and texts where graphs are constructed based on feature similarities (Wu et al., 2023b), predicting
node labels for spatial temporal graphs (Wu et al., 2023b; Eliasof et al., 2023), and processing geometric
graphs (Eliasof et al., 2021; Behmanesh et al., 2023). Some works have generalized the dynamics to other
graph types, including directed graphs (Maskey et al., 2023) and hypergraphs (Lv et al., 2023) and thus
experiments for such graph types are considered.
26Published in Transactions on Machine Learning Research (02/2024)
We have summarized in Table 3 all the experiments performed for each dynamics introduced in Section 3.
8 Open research questions
The recent success of graph neural dynamics has presented numerous opportunities for future explorations.
This section summarizes several exciting research directions that remain open.
Systematic empirical comparisons of graph neural dynamics. From the summary in Section 3,
many dynamics share similar characteristics, especially the ones within the same categories. Moreover,
the components of different dynamics can be combined together to form new dynamics. For example, the
oscillatory process can be coupled with external forces like reaction and convection, and the augmentation
of positional encoding is universal for all dynamics. Conducting controlled experiments that systematically
compare the variety of graph neural dynamics through ablation and combination is an important future work
for better understanding the utility of different components. This would provide principled guidelines for
designing tailored dynamics for specific settings.
Interpretation of spectral GNNs from the perspective of continuous dynamics. We have
summarized the dynamics mostly under the paradigm of spatial GNNs due to the nice interpretation
of message passing as a form of diffusion. Spectral GNNs on the other hand, usually transforms the graph
signals into coefficients weighted by eigenvectors of graph Laplacian (a process known as graph Fourier
transform). Then filters are designed and learned to alter the frequency components of graph signals for
downstream tasks. Although some existing works like (Han et al., 2022) have made preliminary attempts in
formulating spectral GNNs through continuous dynamics, it is generally unknown whether there exists a
unified framework for designing continuous dynamics on the spectral domain. Such a framework could be
beneficial in bridging the gap between spatial and spectral methods and could further motivate designs of
dynamics that take advantage of both paradigms.
Dynamics with high-order graph structures and spatial derivatives. Most existing continuous
GNNs are limited to evolution over nodes, except for (Gruber et al., 2023) that considers higher-order cliques.
Meanwhile graphs often encode more intricate topology where higher-order substructures, such as paths,
triangles and cycles are crucial for downstream tasks (Thiede et al., 2021). For example, aromatic rings are
cycle structures commonly appearing in molecule, which determines various chemical properties, such as
solubility and acidity. Thus designing a dynamics aware of such local substructures is beneficial. In addition,
current dynamics often rely on a coupling function that involves only the first-order spatial derivative (i.e.,
the gradient). It is thus interesting to investigate how to properly define higher-order spatial derivatives (e.g.,
the Hessian) and incorporate into the dynamics formulation.
Explore the potential of graph neural dynamics for other applications. Existing studies proposing
continuous GNNs mostly focus on the node-level prediction tasks while the continuous formalism presents a
general framework for modelling not limited to the evolution of nodes, but also edges, communities and even
the entire graph. It is thus rewarding to adapt graph neural dynamics for other types of applications, such as
link and graph-level prediction, anomaly detection, time series forecasting. For example, Elhag et al. (2022)
leverage both linear and anisotropic diffusion for molecule property prediction; Bhaskar et al. (2023) utilize
graph heat and wave equation for graph topology recovery; Eliasof et al. (2023) demonstrate the promise of
using advection process for spatial temporal graph learning.
Expressivity of graph neural dynamics. The unprecedented success of GNNs have propelled researchers
to study their expressive power in distinguishing non-isomorphic graphs (Xu et al., 2019; Morris et al., 2019),
counting subgraph structures (Chen et al., 2020), etc. While the continuous formalism present a framework
for interpreting and designing GNNs, there have been few works that characterize the expressivity of graph
neural dynamics, especially on graph and subgraph levels. There also lacks theoretical understanding on how
the choice of numerical integrators affects the performance of the neural dynamics. In addition, an equally
fruitful direction is to explore whether the theory of dynamical systems, such as energy conservation and
reversibility can be leveraged to design theoretically more powerful graph neural networks.
27Published in Transactions on Machine Learning Research (02/2024)
Continuous formulation for other graph types. Most of the continuous GNNs focus primarily on
dynamics over static, undirected, homogeneous graphs. Nevertheless, other graph types have also witnessed
wide applicability, including signedordirected graphs (where edges can be negative or directed) (Derr et al.,
2018; Huang et al., 2019; Monti et al., 2018; Tong et al., 2020), heterogeneous graphs (where nodes and edges
have multiple types) (Wang et al., 2019; Ji et al., 2021), geometric graphs (where nodes or edges respect
geometric constraints) (Bronstein et al., 2021), spatial-temporal graphs (where graph topology also evolves in
time) (Jin et al., 2023b;a). The continuous formulation of GNNs for these more complex graph types could
be beneficial for both enhancing the understanding and representation power of existing GNNs. However,
such generalization requires nontrivial efforts. For example, the dynamics should preserve symmetries for
geometric graphs and account for both evolution of graph topology and signals for spatial-temporal graphs.
9 Conclusion
In this work, we conduct a comprehensive review of recent developments on continuous dynamics informed
GNNs, an increasingly growing field that marries the theory of dynamical system and differential equation
with graph representation learning. In particular, we provide mathematical formulations for a diverse range of
graph neural dynamics, and show how the common plights of GNNs can be alleviated through the continuous
formulation. We highlight several open challenges and fruitful research directions that warrant further
exploration. We hope this survey brings attention to the potential of the continuous formalism of GNNs,
and severs as a starting point for future endeavors in harnessing classic theories of continuous dynamics for
enhancing the explainability and expressivity of GNN designs.
References
Samuel M Allen and John W Cahn. A microscopic theory for antiphase boundary motion and its application
to antiphase domain coarsening. Acta metallurgica , 27(6):1085–1095, 1979.
Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In
International Conference on Learning Representations , 2021.
Anima Anandkumar, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Nikola Kovachki, Zongyi Li, Burigede
Liu, and Andrew Stuart. Neural operator: Graph kernel network for partial differential equations. In ICLR
Workshop on Integration of Deep Neural Models and Differential Equations , 2020.
Federico Barbero, Cristian Bodnar, Haitz Sáez de Ocáriz Borde, Michael Bronstein, Petar Veličković, and
Pietro Lio. Sheaf neural networks with connection Laplacians. In ICML Topological, Algebraic and
Geometric Learning Workshops , pp. 28–36. PMLR, 2022a.
Federico Barbero, Cristian Bodnar, Haitz Sáez de Ocáriz Borde, and Pietro Lio. Sheaf attention networks. In
NeurIPS Workshop on Symmetry and Geometry in Neural Representations , 2022b.
Pablo Barceló, Egor V. Kostylev, Mikael Monet, Jorge Pérez, Juan Reutter, and Juan Pablo Silva. The
logical expressiveness of graph neural networks. In International Conference on Learning Representations ,
2020.
Maysam Behmanesh, Maximilian Krahn, and Maks Ovsjanikov. TIDE: Time derivative diffusion for deep
learning on graphs. In International Conference on Machine Learning , pp. 2015–2030. PMLR, 2023.
Alaa Bessadok, Mohamed Ali Mahjoub, and Islem Rekik. Graph neural networks in network neuroscience.
IEEE Transactions on Pattern Analysis and Machine Intelligence , 45(5):5833–5848, 2022.
Dhananjay Bhaskar, Yanlei Zhang, Charles Xu, Xingzhi Sun, Oluwadamilola Fasina, Guy Wolf, Maximilian
Nickel, Michael Perlmutter, and Smita Krishnaswamy. Graph topological property recovery with heat and
wave dynamics-based features on graphsd. arXiv:2309.09924 , 2023.
Cristian Bodnar, Francesco Di Giovanni, Benjamin Paul Chamberlain, Pietro Liò, and Michael M. Bronstein.
Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in GNNs. In Advances
in Neural Information Processing Systems , 2022.
28Published in Transactions on Machine Learning Research (02/2024)
Karsten M Borgwardt, Cheng Soon Ong, Stefan Schönauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter
Kriegel. Protein function prediction via graph kernels. Bioinformatics , 21(suppl_1):i47–i56, 2005.
Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep
learning: going beyond Euclidean data. IEEE Signal Processing Magazine , 34(4):18–42, 2017.
Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veličković. Geometric deep learning: Grids,
groups, graphs, geodesics, and gauges. arXiv:2104.13478 , 2021.
John Charles Butcher. Numerical methods for ordinary differential equations . John Wiley & Sons, 2016.
Luis Caffarelli and Juan Vazquez. Nonlinear porous medium flow with fractional potential pressure. Archive
for Rational Mechanics & Analysis , 202(2), 2011.
Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, and Emanuele
Rossi. GRAND: Graph neural diffusion. In International Conference on Machine Learning , pp. 1407–1418.
PMLR, 2021a.
Benjamin Chamberlain, James Rowbottom, Davide Eynard, Francesco Di Giovanni, Xiaowen Dong, and
Michael Bronstein. Beltrami flow and neural diffusion on graphs. Advances in Neural Information Processing
Systems, 34:1594–1609, 2021b.
Ines Chami, Zhitao Ying, Christopher Ré, and Jure Leskovec. Hyperbolic graph convolutional neural networks.
Advances in Neural Information Processing Systems , 32, 2019.
Qi Chen, Yifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Optimization-induced graph implicit
nonlinear diffusion. In International Conference on Machine Learning , pp. 3648–3661. PMLR, 2022.
Ricky T. Q. Chen. torchdiffeq, 2018. URL https://github.com/rtqichen/torchdiffeq .
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential
equations. Advances in Neural Information Processing Systems , 31, 2018.
Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures?
Advances in Neural Information Processing Systems , 33:10383–10395, 2020.
Jeongwhan Choi, Seoyoung Hong, Noseong Park, and Sung-Bae Cho. GREAD: Graph neural reaction-diffusion
equations. In International Conference on Machine Learning . PMLR, 2023.
Tingting Dan, Jiaqi Ding, Ziquan Wei, Shahar Z Kovalsky, Minjeong Kim, Won Hwa Kim, and Guorong
Wu. Re-think and re-design graph neural networks in spaces of continuous graph diffusion functionals.
arXiv:2307.00222 , 2023.
Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch.
Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation
with molecular orbital energies and hydrophobicity. Journal of Medicinal Chemistry , 34(2):786–797, 1991.
Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs
with fast localized spectral filtering. Advances in Neural Information Processing Systems , 29, 2016.
Tyler Derr, Yao Ma, and Jiliang Tang. Signed graph convolutional networks. In IEEE International Conference
on Data Mining (ICDM) , pp. 929–934. IEEE, 2018.
Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael M Bronstein.
On over-squashing in message passing neural networks: The impact of width, depth, and topology. In
International Conference on Machine Learning , pp. 7865–7885. PMLR, 2023.
John R Dormand. Numerical methods for differential equations: a computational approach , volume 3. CRC
press, 1996.
29Published in Transactions on Machine Learning Research (02/2024)
Vijay Prakash Dwivedi, Ladislav Rampášek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and
Dominique Beaini. Long range graph benchmark. Advances in Neural Information Processing Systems , 35:
22326–22340, 2022.
Weinan E. A proposal on machine learning via dynamical systems. Communications in Mathematics and
Statistics , 5:1–11, 2017.
Ahmed AA Elhag, Gabriele Corso, Hannes Stärk, and Michael M Bronstein. Graph anisotropic diffusion for
molecules. In ICLR Machine Learning for Drug Discovery , 2022.
Moshe Eliasof, Eldad Haber, and Eran Treister. PDE-GCN: Novel architectures for graph neural networks
motivated by partial differential equations. Advances in Neural Information Processing Systems , 34:
3836–3849, 2021.
Moshe Eliasof, Eldad Haber, and Eran Treister. ADR-GNN: Advection-diffusion-reaction graph neural
networks. arXiv preprint arXiv:2307.16092 , 2023.
Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks for
social recommendation. In The World Wide Web Conference , pp. 417–426, 2019.
Bernold Fiedler and Arnd Scheel. Spatio-temporal dynamics of reaction-diffusion patterns. Trends in nonlinear
analysis, pp. 23–152, 2003.
Ronald Aylmer Fisher. The wave of advance of advantageous genes. Annals of Eugenics , 7(4):355–369, 1937.
Guoji Fu, Peilin Zhao, and Yatao Bian. p-laplacian based graph neural networks. In International Conference
on Machine Learning , pp. 6878–6917. PMLR, 2022.
Guoji Fu, Mohammed Haroon Dupty, Yanfei Dong, and Lee Wee Sun. Implicit graph neural diffusion based
on constrained Dirichlet energy minimization. arXiv:2308.03306 , 2023.
Johannes Gasteiger, Stefan Weißenberger, and Stephan Günnemann. Diffusion improves graph learning.
Advances in Neural Information Processing Systems , 32, 2019.
Brian H Gilding and Robert Kersner. Travelling waves in nonlinear diffusion-convection reaction , volume 60.
Springer Science & Business Media, 2004.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message
passing for quantum chemistry. In International Conference on Machine Learning , pp. 1263–1272. PMLR,
2017.
Francesco Di Giovanni, James Rowbottom, Benjamin Paul Chamberlain, Thomas Markovich, and Michael M.
Bronstein. Understanding convolution on graphs via energies. Transactions on Machine Learning Research ,
2023. ISSN 2835-8856.
Jhony H Giraldo, Fragkiskos D Malliaros, and Thierry Bouwmans. Understanding the relationship between
over-smoothing and over-squashing in graph neural networks. arXiv:2212.02374 , 2022.
Alessio Gravina, Davide Bacciu, and Claudio Gallicchio. Anti-symmetric DGN: a stable architecture for deep
graph networks. In International Conference on Learning Representations , 2023.
Anthony Gruber, Kookjin Lee, and Nathaniel Trask. Reversible and irreversible bracket-based dynamics for
deep graph neural networks. arXiv:2305.15616 , 2023.
Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, and Laurent El Ghaoui. Implicit graph neural
networks. Advances in Neural Information Processing Systems , 33:11984–11995, 2020.
Jayant Gupta, Bharat Jayaprakash, Matthew Eagon, Harish Panneer Selvam, Carl Molnar, William Northrop,
Shashi Shekhar, et al. A survey on solving and discovering differential equations using deep neural networks.
arXiv:2304.13807 , 2023.
30Published in Transactions on Machine Learning Research (02/2024)
Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse problems , 34(1):
014004, 2017.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances
in Neural Information Processing Systems , 30, 2017.
Andi Han, Dai Shi, Zhiqi Shao, and Junbin Gao. Generalized energy and gradient flow via graph framelets.
arXiv:2210.04124 , 2022.
Jakob Hansen and Thomas Gebhart. Sheaf neural networks. In NeurIPS Topological Data Analysis and
Beyond Workshop , 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Conference on Computer Vision and Pattern Recognition , pp. 770–778, 2016.
Rainer Hegselmann and Ulrich Krause. Opinion dynamics and bounded confidence: models, analysis and
simulation. Journal of Artifical Societies and Social Simulation (JASSS) vol , 5(3), 2002.
Christoph Helma, Ross D. King, Stefan Kramer, and Ashwin Srinivasan. The predictive toxicology challenge
2000–2001. Bioinformatics , 17(1):107–108, 2001.
Emiel Hoogeboom, Vıctor Garcia Satorras, Clément Vignac, and Max Welling. Equivariant diffusion for
molecule generation in 3d. In International Conference on Machine Learning , pp. 8867–8887. PMLR, 2022.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and
Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in Neural
Information Processing Systems , 33:22118–22133, 2020.
Junjie Huang, Huawei Shen, Liang Hou, and Xueqi Cheng. Signed graph attention networks. In Artificial
Neural Networks and Machine Learning–ICANN 2019: Workshop and Special Sessions: 28th International
Conference on Artificial Neural Networks , pp. 566–577. Springer, 2019.
Shudong Huang, Wentao Feng, Chenwei Tang, and Jiancheng Lv. Partial differential equations meet deep
neural networks: A survey. arXiv:2211.05567 , 2022.
Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A survey on knowledge graphs:
Representation, acquisition, and applications. IEEE Transactions on Neural Networks and Learning
Systems, 33(2):494–514, 2021.
Weiwei Jiang and Jiayun Luo. Graph neural network for traffic forecasting: A survey. Expert Systems with
Applications , 207:117921, 2022.
Guangyin Jin, Yuxuan Liang, Yuchen Fang, Jincai Huang, Junbo Zhang, and Yu Zheng. Spatio-temporal
graph neural networks for predictive learning in urban computing: A survey. arXiv:2303.14483 , 2023a.
Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Geoffrey I Webb, Irwin King, and
Shirui Pan. A survey on graph neural networks for time series: Forecasting, classification, imputation, and
anomaly detection. arXiv:2307.03759 , 2023b.
Qiyu Kang, Kai Zhao, Yang Song, Sijie Wang, and Wee Peng Tay. Node embedding from neural Hamiltonian
orbits in graph neural networks. In International Conference on Machine Learning , 2023.
Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph convolutions:
moving beyond fingerprints. Journal of Computer-Aided Molecular Design , 30:595–608, 2016.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In
International Conference on Learning Representations , 2017.
31Published in Transactions on Machine Learning Research (02/2024)
Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart,
and Anima Anandkumar. Neural operator: Learning maps between function spaces with applications to
pdes.Journal of Machine Learning Research , 24(89):1–97, 2023. URL http://jmlr.org/papers/v24/
21-1524.html .
Harender Kumar and Neha Yadav. Deep learning algorithms for solving differential equations: a survey.
Journal of Experimental & Theoretical Artificial Intelligence , pp. 1–46, 2023.
Qianxiao Li, Ting Lin, and Zuowei Shen. Deep learning via dynamical systems: An approximation perspective.
Journal of the European Mathematical Society , 25(5):1671–1709, 2022.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks.
arXiv:1511.05493 , 2015.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya,
and Anima Anandkumar. Multipole graph neural operator for parametric partial differential equations.
Advances in Neural Information Processing Systems , 33:6755–6766, 2020.
Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard Zemel. LanczosNet: Multi-scale deep graph
convolutional networks. In International Conference on Learning Representations , 2018.
Anna Lischke, Guofei Pang, Mamikon Gulian, Fangying Song, Christian Glusa, Xiaoning Zheng, Zhiping Mao,
Wei Cai, Mark M Meerschaert, Mark Ainsworth, et al. What is the fractional laplacian? a comparative
review with new results. Journal of Computational Physics , 404:109009, 2020.
Guan-Horng Liu and Evangelos A Theodorou. Deep learning theory review: An optimal control and dynamical
systems perspective. arXiv:1908.10920 , 2019.
Outongyi Lv, Bingxin Zhou, Jing Wang, Xiang Xiao, Weishu Zhao, and Lirong Zheng. A unified view on
neural message passing with opinion dynamics for social networks. arXiv:2310.01272 , 2023.
Thomas Markovich. QDC: Quantum diffusion convolution kernels on graphs. arXiv:2307.11234 , 2023.
Sohir Maskey, Raffaele Paolino, Aras Bacho, and Gitta Kutyniok. A fractional graph laplacian approach to
oversmoothing. Advances in Neural Information Processing Systems , 2023.
Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based recommendations
on styles and substitutes. In International ACM SIGIR Conference on Research and Development in
Information Retrieval , pp. 43–52, 2015.
Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construction
of internet portals with machine learning. Information Retrieval , 3:127–163, 2000.
Federico Monti, Karl Otness, and Michael M Bronstein. Motifnet: a motif-based graph convolutional network
for directed graphs. In 2018 IEEE Data Science Workshop (DSW) , pp. 225–228. IEEE, 2018.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan,
and Martin Grohe. Weisfeiler and Leman go neural: Higher-order graph neural networks. In AAAI
Conference on Artificial Intelligence , volume 33, pp. 4602–4609, 2019.
Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. Query-driven active surveying for
collective classification. In International Workshop on Mining and Learning with Graphs , volume 8, pp. 1,
2012.
Khang Nguyen, Nong Minh Hieu, Tan Minh Nguyen, Nguyen Duy Khuong, and Vinh Duc NGUYEN.
DeepGRAND: Deep graph neural diffusion, 2023. URL https://openreview.net/forum?id=wTGORH_
cHPX.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-GCN: Geometric graph
convolutional networks. In International Conference on Learning Representations , 2019.
32Published in Transactions on Machine Learning Research (02/2024)
Lawrence Perko. Differential equations and dynamical systems , volume 7. Springer Science & Business Media,
2013.
Pietro Perona and Jitendra Malik. Scale-space and edge detection using anisotropic diffusion. IEEE
Transactions on Pattern Analysis and Machine Intelligence , 12(7):629–639, 1990.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In
Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pp.
701–710, 2014.
Michael Poli, Stefano Massaroli, Junyoung Park, Atsushi Yamashita, Hajime Asama, and Jinkyoo Park.
Graph neural ordinary differential equations. arXiv:1911.07532 , 2019.
Lev Semenovich Pontryagin. Mathematical theory of optimal processes . Routledge, 2018.
Constantine Pozrikidis. The fractional laplacian . CRC Press, 2018.
Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal of
Complex Networks , 9(2):cnab014, 2021.
T Konstantin Rusch, Ben Chamberlain, James Rowbottom, Siddhartha Mishra, and Michael Bronstein.
Graph-coupled oscillator networks. In International Conference on Machine Learning , pp. 18888–18909.
PMLR, 2022.
T Konstantin Rusch, Michael M Bronstein, and Siddhartha Mishra. A survey on oversmoothing in graph
neural networks. arXiv:2303.10993 , 2023a.
T Konstantin Rusch, Benjamin Paul Chamberlain, Michael W Mahoney, Michael M Bronstein, and Siddhartha
Mishra. Gradient gating for deep multi-rate learning on graphs. In International Conference on Learning
Representations , 2023b.
Lars Ruthotto and Eldad Haber. Deep neural networks motivated by partial differential equations. Journal
of Mathematical Imaging and Vision , 62:352–364, 2020.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective
classification in network data. AI Magazine , 29(3):93–93, 2008.
Zhiqi Shao, Dai Shi, Andi Han, Yi Guo, Qibin Zhao, and Junbin Gao. Unifying over-smoothing and
over-squashing in graph neural networks: A physics informed approach and beyond. arXiv:2309.02769 ,
2023.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph
neural network evaluation. arXiv:1811.05868 , 2018.
Dai Shi, Andi Han, Lequan Lin, Yi Guo, Zhiyong Wang, and Junbin Gao. Design your own universe: A
physics-informed agnostic method for enhancing graph neural networks. arXiv:2401.14580 , 2024.
Jonathan Shlomi, Peter Battaglia, and Jean-Roch Vlimant. Graph neural networks in particle physics.
Machine Learning: Science and Technology , 2(2):021001, 2020.
Yang Song, Qiyu Kang, Sijie Wang, Kai Zhao, and Wee Peng Tay. On the robustness of graph neural diffusion
to topology perturbations. Advances in Neural Information Processing Systems , 35:6384–6396, 2022.
Julian Suk, Lorenzo Giusti, Tamir Hemo, Miguel Lopez, Konstantinos Barmpas, and Cristian Bodnar. Surfing
on the neural sheaf. In NeurIPS Workshop on Symmetry and Geometry in Neural Representations , 2022.
Jeffrey J Sutherland, Lee A O’brien, and Donald F Weaver. Spline-fitting with a genetic algorithm: A
method for developing classification structure- activity relationships. Journal of Chemical Information and
Computer Sciences , 43(6):1906–1915, 2003.
33Published in Transactions on Machine Learning Research (02/2024)
Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. Social influence analysis in large-scale networks. In ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining , pp. 807–816, 2009.
Erik Thiede, Wenda Zhou, and Risi Kondor. Autobahn: Automorphism-based graph neural nets. Advances
in Neural Information Processing Systems , 34:29922–29934, 2021.
Matthew Thorpe, Tan Minh Nguyen, Hedi Xia, Thomas Strohmer, Andrea Bertozzi, Stanley Osher, and Bao
Wang. GRAND++: Graph neural diffusion with a source term. In International Conference on Learning
Representations , 2022.
Zekun Tong, Yuxuan Liang, Changsheng Sun, David S Rosenblum, and Andrew Lim. Directed graph
convolutional network. arXiv:2004.13970 , 2020.
Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M. Bronstein.
Understanding over-squashing and bottlenecks on graphs via curvature. In International Conference on
Learning Representations , 2022.
Csaba Toth, Darrick Lee, Celia Hacker, and Harald Oberhauser. Capturing graphs with hypo-elliptic diffusions.
Advances in Neural Information Processing Systems , 35:38803–38817, 2022.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems , 30,
2017.
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio.
Graph attention networks. In International Conference on Learning Representations , 2018.
Ferdinand Verhulst. Nonlinear differential equations and dynamical systems . Springer Science & Business
Media, 2006.
Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical compound
retrieval and classification. Knowledge and Information Systems , 14:347–375, 2008.
Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Heterogeneous graph
attention network. In The World Wide Web Conference , pp. 2022–2032, 2019.
Yuelin Wang, Kai Yi, Xinliang Liu, Yu Guang Wang, and Shi Jin. ACMP: Allen-Cahn message passing
with attractive and repulsive forces for graph neural networks. In International Conference on Learning
Representations , 2023.
Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach,
Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein
structure and function with RFdiffusion. Nature, pp. 1–3, 2023.
Joachim Weickert et al. Anisotropic diffusion in image processing , volume 1. Teubner Stuttgart, 1998.
Qitian Wu, Chenxiao Yang, Kaipeng Zeng, Fan Nie, Michael Bronstein, and Junchi Yan. Advective diffusion
transformers for topological generalization in graph learning. arXiv:2310.06417 , 2023a.
Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, and Junchi Yan. DIFFormer: Scalable
(graph) transformers induced by energy constrained diffusion. In International Conference on Learning
Representations , 2023b.
Louis-Pascal Xhonneux, Meng Qu, and Jian Tang. Continuous graph neural networks. In International
Conference on Machine Learning , pp. 10432–10441. PMLR, 2020.
Tian Xie and Jeffrey C Grossman. Crystal graph convolutional neural networks for an accurate and
interpretable prediction of material properties. Physical Review Letters , 120(14):145301, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In
International Conference on Learning Representations , 2019.
34Published in Transactions on Machine Learning Research (02/2024)
Kai Zhao, Qiyu Kang, Yang Song, Rui She, Sijie Wang, and Wee Peng Tay. Adversarial robustness in graph
neural networks: A Hamiltonian approach. Advances in Neural Information Processing Systems , 2023a.
Kai Zhao, Qiyu Kang, Yang Song, Rui She, Sijie Wang, and Wee Peng Tay. Graph neural convection-diffusion
with heterophily. In International Joint Conference on Artificial Intelligence , 2023b.
Qinkai Zheng, Xu Zou, Yuxiao Dong, Yukuo Cen, Da Yin, Jiarong Xu, Yang Yang, and Jie Tang. Graph
robustness benchmark: Benchmarking the adversarial robustness of graph machine learning. In Neural
Information Processing Systems Datasets and Benchmarks Track , 2021a.
Xuebin Zheng, Bingxin Zhou, Junbin Gao, Yuguang Wang, Pietro Lió, Ming Li, and Guido Montufar.
How framelets enhance graph neural networks. In International Conference on Machine Learning , pp.
12761–12771. PMLR, 2021b.
Chunya Zou, Andi Han, Lequan Lin, Ming Li, and Junbin Gao. A simple yet effective Framelet-based graph
neural network for directed graphs. IEEE Transactions on Artificial Intelligence , 2023.
35