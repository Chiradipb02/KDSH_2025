Published in Transactions on Machine Learning Research (07/2024)
Incorporating Unlabelled Data into Bayesian Neural Networks
Mrinank Sharma mrinank@robots.ox.ac.uk
University of Oxford, UK
Tom Rainforth rainforth@stats.ox.ac.uk
University of Oxford, UK
Yee Whye Teh y.w.teh@stats.ox.ac.uk
University of Oxford, UK
Vincent Fortuin vincent.fortuin@tum.de
Helmholtz AI, Munich, Germany
Technical University of Munich, Germany
Reviewed on OpenReview: https://openreview.net/forum?id=q2AbLOwmHm
Abstract
Conventional Bayesian Neural Networks (BNNs) are unable to leverage unlabelled data
to improve their predictions. To overcome this limitation, we introduce Self-Supervised
Bayesian Neural Networks , which use unlabelled data to learn models with suitable prior
predictive distributions. This is achieved by leveraging contrastive pretraining techniques and
optimising a variational lower bound. We then show that the prior predictive distributions
of self-supervised BNNs capture problem semantics better than conventional BNN priors. In
turn, our approach offers improved predictive performance over conventional BNNs, especially
in low-budget regimes.
1 Introduction
Bayesian Neural Networks (BNNs) are powerful probabilistic models that combine the flexibility of deep
neural networks with the theoretical underpinning of Bayesian methods ( Mackay,1992;Neal,1995). Indeed,
as they place priors over their parameters and perform posterior inference, BNN advocates consider them a
principled approach for uncertainty estimation ( Wilson & Izmailov ,2020;Abdar et al. ,2021), which can be
helpful for label-efficient learning ( Gal et al. ,2017). It has even recently been argued that improving them
will be crucial for large language models ( Papamarkou et al. ,2024) and generative AI as a whole ( Manduchi
et al.,2024).
Conventionally, BNN researchers have focused on improving predictive performance using human-crafted
priors over network parameters or predictive functions (e.g., Louizos et al. ,2017;Tran et al. ,2020;Matsubara
et al.,2021;Fortuin et al. ,2021a). However, several concerns have been raised with BNN priors ( Wenzel
et al.,2020;Noci et al. ,2021). It also stands to reason that the vast store of semantic information contained
in unlabelled data should be incorporated into BNN priors, and that the potential benefit of doing so likely
exceeds the benefit of designing better, but ultimately human-specified, priors over parameters or functions.
Unfortunately, as standard BNNs are explicitly only models for supervised prediction, they cannot leverage
such semantic information from unlabelled data by conditioning on it.
To overcome this shortcoming, we introduce Self-Supervised Bayesian Neural Networks (§3), which
use unlabelled data to learn improved priors over functions. In other words, our approach improves the
BNN prior predictive distribution (which we will just call prior predictive in the remainder of the paper) by
incorporating unlabelled data into it. This contrasts with designing different but ultimately human-specified
priors, which is the prevalent approach.
1Published in Transactions on Machine Learning Research (07/2024)
less likelySemantically consistent images are 
more  likely to have the same label ✅
Density
more likely(a) Unsupervised Prior Learning
(b) Self-Supervised BNN Prior Pr edictive
Learn prior s.t. augmented images  have the same label
 Learn prior s.t. distinct images  have dif ferent labels
Semantically inconsistent images are 
less likely to have the same label ✅
(c) Conventional BNN Prior Pr edictive
less likely
Semantically consistent images  and
semantically inconsistent images are
similarly likely  to have the same label ❌Density
more likely
Figure 1: Self-Supervised Bayesian Neural Networks . (a) Pre-training in self-supervised BNNs
corresponds to unsupervised prior learning. We learn a model with a prior distribution such that augmented
images likely have the same label and distinct images likely have different labels under the prior predictive.
(b) Self-supervised BNN priors assign higher probabilities to semantically consistent image pairs having the
same label compared to semantically inconsistent image pairs. Here, semantically consistent image pairs have
thesame ground-truth label , andsemantically inconsistent image pairs have different ground-truth labels .
The plot shows a kernel density estimate of the log-probability that same-class anddifferent-class image pairs
are assigned the same label under the prior. (c) Unlike self-supervised prior predictives, conventional BNN
prior predictives assign similar probabilities to semantically consistent andsemantically inconsistent image
pairs having the same label.
In practice, self-supervised BNNs generate pseudo-labelled data using unlabelled data and data augmentation,
similar to contrastive learning ( Oord et al. ,2019;Chen et al. ,2020a;b;Grill et al. ,2020;Hénaff et al. ,2020).
We use this generated data to learn models with powerful prior predictive distributions. To do this, we
perform unsupervised model learning by optimising a lower bound of a log-marginal likelihood dependent
on the pseudo-labelled data. This biases the prior towards functions that assign augmented image pairs a
larger likelihood of having the same label than distinct images (Fig. 1a). Following pretraining, we perform
inference in the learnt model to make predictions.
We then further demonstrate that self-supervised BNN prior predictives reflect input-pair
semantic similarity better than normal BNN priors (§4). To do so, we develop a methodology to
better understand the prior predictive distributions of BNNs. Our approach is to measure the probability
ofpairsof data points having the same label under the prior. Intuitively, pairs of points that are more
semantically similar should be more likely to have the same label under the prior predictive. Applying this
methodology, we see that the functional priors learned by self-supervised BNNs distinguish same-class input
pairs and different-class input pairs much better than conventional BNNs (Fig. 1b).
Finally, we empirically demonstrate that the improved prior predictives of self-supervised BNNs translate to
improved predictive performance, especially in problem settings with few labelled examples (§ 5).
2Published in Transactions on Machine Learning Research (07/2024)
2 Background: Bayesian Neural Networks
Letfθ(x)be a neural network with parameters θandD={(xi, yi)}N
i=1be an dataset. We want to predict y
from x. A BNN specifies a prior over parameters, p(θ), and a likelihood, p(y|fθ(x)), which in turn define
the posterior p(θ|D)∝p(θ)/producttext
ip(yi|fθ(xi)). To make predictions, we approximate the posterior predictive
p(y⋆|x⋆,D) =Ep(θ|D)[p(y⋆|fθ(x⋆))].
Improving BNN priors has been a long-standing goal for the community, primarily through improved human-
designed priors. One approach is to improve the prior over the network’s parameters ( Louizos et al. ,2017;
Nalisnick ,2018). Others place priors directly over predictive functions ( Flam-Shepherd et al. ,2017;Sun
et al.,2019;Matsubara et al. ,2021;Nalisnick et al. ,2021;Raj et al. ,2023). Both approaches, however,
present challenges—the mapping between the network’s parameters and predictive functions is complex, while
directly specifying our beliefs over predictive functions is itself a highly challenging task. For these reasons,
as well as convenience, isotropic Gaussian priors over network parameters remain the most common choice
(Fortuin,2022), despite concerns ( Wenzel et al. ,2020). In contrast to these works, we propose to learnbetter
functional priors from unlabelled data via contrastive learning.
3 Self-Supervised BNNs
Conventional BNNs are unable to use unlabelled data to improve their predictions. To overcome this
limitation, we introduce Self-Supervised BNNs . At a high level, self-supervised BNNs allow unlabelled data
to be incorporated by using it to learn a powerful prior that captures known similarities between inputs. In
practice, we can utilise ideas from contrastive learning to learn models with prior predictive distributions
that reflect the semantics of different input pairs. The high-level idea is thus to use prior knowledge in the
form of data augmentations, for which we believe that the semantic content of the data should be invariant
to them. We can then use a variational method to learn a function-space prior that assigns more weight to
functions, whose outputs on unlabelled data are also invariant to these augmentations.
Problem Specification. Suppose Du={xu
i}N
i=1is an unlabelled dataset of examples xu
i∈Rn. Let
Dt={(xt
i, yt
i)}T
i=1be a labelled dataset corresponding to a supervised “downstream” task, where yt
iis the
target associated with xt
i. We want to use both DuandDtto train a deep learning model for predicting y
given xwith probabilistic parameters θ, where all information about the data is incorporated through the
distribution on θ. That is, we predict using p(y|x, θ)for a given θ.
3.1 Incorporating Unlabelled Data into BNNs
The simplest way one might proceed, is to place a prior on θand then condition on both DuandDt, leading
to a posterior p(θ|Du,Dt)∝p(θ|Du)p(Dt|Du, θ). However, if we are working with conventional BNNs, which
are explicitly models for supervised prediction, then p(θ|Du) =p(θ). Further, as the predictions depend only
on the parameters, p(Dt|Du, θ) =p(Dt|θ), which then means that p(θ|Du,Dt) =p(θ|Dt). Thus, we cannot
incorporate Duby naïvely conditioning on it.
To get around this problem, we propose to instead use Duto generate hypothetical labelled data and then
condition our predictive model on it. In other words, we will use Duto guide a self-supervised training of the
model, thereby incorporating the desired information from our unlabelled data. To do this, we will draw
on data augmentation ( Yaeger et al. ,1996;Krizhevsky et al. ,2012;Shorten & Khoshgoftaar ,2019) and
contrastive learning ( Oord et al. ,2019;Chen et al. ,2020b;a;Grill et al. ,2020;Hénaff et al. ,2020;Chen & He ,
2020;Foster et al. ,2020;Miao et al. ,2023).
Indeed, suchapproachesthatusedataaugmentationprovideeffectivemeansformakinguseofpriorinformation.
Although it is difficult to encode our prior beliefs with hand-crafted priors over neural network parameters,
we can construct augmentation schemes that we expect to preserve the semantic properties of different inputs.
We thus also expect these augmentation schemes to preserve the unknown downstream labels of different
inputs. The challenge is now to transfer the beliefs—implicitly defined through our data augmentation
scheme—into our model.
3Published in Transactions on Machine Learning Research (07/2024)
(a) Conventional BNN
Downstream Dataset
(b) Self-Supervised BNN
Contrastive Dataset(s)Downstream Dataset
Figure 2: BNN Probabilistic Models. (a) Probabilistic model for conventional BNNs. (b) Probabilistic
model for self-supervised BNNs. We share parameters between different tasks, which allows us to condition
on generated self-supervised data. jindexes self-supervised tasks, iindexes datapoints.
One simple way to do this would be to just augment the data in Dtwhen training a standard BNN with
standard data augmentation. However, this would ignore the rich information available in Du. Instead, we
will use a construct from contrastive learning to generate pseudo-labelled data from Du, and then condition θ
on both this pseudo data and Dt.
Concretely, suppose we have a set of data augmentations A={a:Rn→Rn}that preserve semantic content.
We use AandDuto generate a contrastive dataset Dcthat reflects our subjective beliefs by:
1.Drawing Mexamples from Duat random, {ˆxi}M
i=1, where iindexes the subset, not Du;
2.For each ˆxi, sampling aA, aB∼ Aand augmenting, giving ˜xA
i=aA(ˆxi)and ˜xB
i=aB(ˆxi);
3.Forming Dcby assigning ˜xA
iand ˜xB
ithe same class label, which is the subset index i.
We thus have Dc={(xc
i, yc
i)}2M
i=1={(˜xA
i, i)}M
i=1∪ {(˜xB
i, i)}M
i=1, where the labels are between 1andM. The
task associated with our generated data is thus to predict the subset index corresponding to each augmented
example. We can repeat this process Ltimes and create a set of contrastive task datasets, {Dc
j}L
j=1. Here,
we consider the number of generated datasets Lto be a fixed, finite hyper-parameter, but we discuss the
implications of setting L=∞in Appendix A. Note that rather than using a hand-crafted prior to capture
our semantic beliefs, we have instead used data augmentation in combination with unlabelled data.
Next, to link each Dc
jwith the downstream predictions, we use parameter sharing (see Fig. 2). Specifically,
we introduce parameters θc
jfor each Dc
j, parameters θtforDt, and shared-parameters θsthat are used for
both the downstream and the contrastive tasks. Duthus informs downstream predictions through θs, via
{Dc
j}L
j=1. For example, θtandθc
jcould be the parameters of the last layer of a neural network, while θscould
be the shared parameters of earlier layers.
Learning in this Framework. We now discuss different options for learning in this framework. Using the
Bayesian approach, one would place priors over θs,θt, and each θc
j. This then defines a posterior distribution
given the observed data {Dc
j}L
j=1andDt. To make predictions on the downstream task, which depend on θs
andθtonly, we would then use the posterior predictive:
p(yt
⋆|x⋆,{Dc
j}L
j=1,Dt) =Ep(θs|{Dc
j}L
j=1,Dt)[Ep(θt|θs,Dt)[p(yt
⋆|x⋆, θs, θt)]], (1)
4Published in Transactions on Machine Learning Research (07/2024)
where we have (i) noted that the downstream task parameters θtare independent of {Dc
j}L
j=1given the shared
parameters θsand (ii) integrated over each θc
jandθtin the definition of p(θs|{Dc
j}L
j=1,Dt).
Alternatively, one can learn a point estimate for θs, e.g., with MAP estimation, and perform full posterior
inference for θtandθc
jonly. This would be a partially stochastic network, which Sharma et al. (2023) showed
often outperforms fully stochastic networks while being more practical. In the case where all parameters
are shared up to the last (linear) layer, this is also known as the neural linear model (Lázaro-Gredilla &
Figueiras-Vidal ,2010), which has been shown to have many desirable properties ( Ober & Rasmussen ,2019;
Harrison et al. ,2023). Learning in this way is also known as model learning , as used in deep kernels and
variational autoencoders ( Kingma & Welling ,2013;Rezende et al. ,2014;Wilson et al. ,2015;2016), where in
the case of Gaussian processes (GPs), a point estimate of kernel parameters is learned to also define a learned
prior over functions. Note that our approach can also be considered kernel learning, where the representations
of input data after the shared layers θsdefine the reproducing kernel Hilbert space (RKHS) and the kernel is
given by their inner products. In learning a point estimate for θs, one is learning a suitable model to perform
inference in. This model has the following prior predictive:
p(yt
⋆|x⋆,{Dc
j}L
j=1) =Ep(θt)[p(yt
⋆|x⋆, θs
⋆, θt)], (2)
where θs
⋆is the learnt value of θs.1We would then update our beliefs over θtin light of observed data.
Through θs
⋆, we are using Duto effectively learn a prior over the functions that can be represented by the
network in combination with θt. Learning a point estimate for θsis thus our main approach.
3.2 Self-Supervised BNNs in Practice
We now use our framework to propose a practical two-step algorithm for self-supervised BNNs.
Preliminaries. We focus on image-classification problems. We use an encoder fθs(·)that maps images to
representations and is shared across the contrastive tasks and the downstream task. The shared parameters
θsthus are the base encoder’s parameters. We also normalise the representations produced by this encoder.
For the downstream dataset, we use a linear readout layer from the encoder representations, i.e., we have
θt={Wt, bt}andyt
i∼softmax (Wtfθs(xi) +bt). The rows of Wt
jare thus class template vectors, that is,
a data point will achieve the highest possible softmax probability for a certain class if its representation is
equal to (a scaled version of) the corresponding row of the weight matrix. For the contrastive tasks, we use a
linear layer without biases, i.e., θc
j=Wc
j, and jindexes contrastive tasks. We place Gaussian priors over θs,
θt, and each θc
j.
Pre-training θs(Step I). Here, we learn a point estimate for the base encoder parameters θs, which
induces a functional prior over the downstream task labels (see Eq. 2). To learn θs, we want to optimise the
(potentially penalised) log-likelihood logp({Dc
j}L
j=1,Dt|θs), but this would require integrating over θtand
each θc. Instead, we use the evidence lower bound (ELBO):
˜Lc
j(θs) =Eq(θc
j)[logp(Dc
j|θs, θc
j)]−DKL(q(θc
j)||p(θc
j))≤logp(Dc
j|θs), (3)
where q(θc
j)is a variational distribution over the contrastive task parameters.
Rather than learning a different variational distribution for each contrastive task j, we amortise the inference
and exploit the structure of the contrastive task. The contrastive task is to predict the corresponding source
image index for each augmented image. That is, for the first pair of augmented images in a given contrastive
task dataset, we want to predict class “1”, for the second pair, we want to predict class “2”, and so forth.
The label is the index within the contrastive dataset, not the full dataset. To predict these labels, we use a
linear layer applied to an encoder that produces normalised representations. We want a suitable variational
distribution for this linear layer.
To make progress, we define ˜zA
i=fθs(˜xA
i)and ˜zB
i=fθs(˜xB
i), which are the representations of given images.
To solve the contrastive task well, we want to map zA
1andzB
1to class “1”, zA
2andzB
2to class “2”, and so forth.
1Alternatively, this approach can be understood as learning the prior p(θs, θt) = p(θt)δθs=θs⋆, where δis the Dirac delta
function.
5Published in Transactions on Machine Learning Research (07/2024)
Algorithm 1 Self-Supervised BNNs
Input:augmentations A, unlabelled data Du, task data Dt, contrastive prior p(Wc)
forj= 1, . . . , Ldo ⊿Unsupervised prior learning
Draw subset {ˆxi}M
i=1, setDc
j={}
fori= 1, . . . , Mdo ⊿Create contrastive task
Sample aA, aB∼ A
˜xA
i=aA(ˆxi),˜xB
i=aB(ˆxi)
˜zA
i=fθs(˜xA
i),˜zB
i=fθs(˜xB
i)
ωi= 0.5(˜zA
i+ ˜zB
i)
Add (˜xA
i, i)and (˜xB
i, i)toDc
j.
end for
Wc
j=/bracketleftbig
ωT
1. . . ωT
M/bracketrightbig
/τ+/epsilon1, with /epsilon1∼ N (0, σ2I)
˜L(τ, σ2, θs) = log p(θs) +1
2MEq(Wc
j)[p(Dc
j|θs, Wc
j)]−¯DKL[q(Wc
j)||p(Wc))]
Update θs, τ, σ2to maximise ˜L(τ, σ2, θs)
end for
Approximate p(θt|Dt, θs)/similarequalq(θt) ⊿Evaluation
Predict using Eq(θt)[p(yt
⋆|x⋆, θs, θt)]
We define ωi= 0.5(˜zA
i+˜zB
i), i.e., ωiis the mean representation for each augmented pair of images. Because
the rows of the linear layer weight matrix Wc
jare effectively class templates, we use q(Wc
j;τ, σ2) =N(µc
j, σ2I)
with µc
j=/bracketleftbig
ωT
1. . . ωT
M/bracketrightbig
/τ. In words, the mean representation of each augmented image pair is the class
template for each source image, which should solve the contrastive task well. Note that since this makes the
last-layer weights data-dependent, it also renders them softmax outputs invariant to the arbitrary ordering of
the data points in the batch, since if one permutes the xi, and thus zi, this also automatically permutes the
ωiand thus rows of Wc
jin the same way. Also, recall that the Wc
jare auxiliary parameters that are just
needed during the contrastive learning to learn a θsthat induces a good functional prior, but are discarded
afterwards and not used for the actual supervised task of interest. The variational parameters τandσ2
determine the magnitude of the linear layer and the per-parameter variance, vary throughout training, are
shared across contrastive tasks j, and are learnt by maximising Eq. ( 3) with reparameterisation gradients
(Price,1958;Kingma & Welling ,2013).
Both the contrastive tasks and downstream task provide information about the base encoder parameters
θs. One option would be to learn the base encoder parameters θsusing only data derived from Du(Eq.3),
which would correspond to a standard self-supervised learning setup. In this case, the learnt prior would
be task-agnostic. An alternative approach is to use both DtandDuto learn θs, which corresponds to a
semi-supervised setup. To do so, we can use the ELBO for the downstream data:
˜Lt(θs) =Eq(θt)[logp(Dt|θt, θs)]−DKL[q(θt)||p(θt)]≤logp(Dt|θs), (4)
where q(θt) =N(θt;µt,Σt)is a variational distribution over the downstream task parameters; Σtis diagonal.
We can then maximise/summationtext
j˜Lc
j(θs) +α·˜Lt(θs), where αis a hyper-parameter that controls the weighting
between the downstream task and contrastive task datasets. We consider both variants of our approach,
usingSelf-Supervised BNNs to refer to the variant that pre-trains only with {Dc
j}L
j=1andSelf-Supervised
BNNs*to refer to the variant that uses both {Dc
j}L
j=1andDt.
Downstream Inference (Step II). Having learnt a point estimate for θs, we can use any approximate
inference algorithm to infer θt. Here, we use a post-hoc Laplace approximation ( Daxberger et al. ,2021).
Algorithm 1summarises Self-Supervised BNNs , which learn θswithDuonly. We found tempering with the
mean-per-parameter KL divergence, ¯DKL, improved performance, in line with other work (e.g., Krishnan et al. ,
2022). Moreover, we generate a new Dc
jper gradient step so Lcorresponds to the number of gradient steps. As
shown on Algorithm 1, our full loss is ˜L(τ, σ2, θs) =logp(θs)+1
2MEq(Wc
j)[p(Dc
j|θs, Wc
j)]−¯DKL[q(Wc
j)||p(Wc))].
6Published in Transactions on Machine Learning Research (07/2024)
The first term of this loss is a prior over the shared parameters, in our case a Gaussian prior, which is
equivalent to weight decay. The second term is where the actual contrastive learning happens, namely it is
an expected Categorical log-likelihood (i.e., cross-entropy) over the softmax logits under Wc
j. Recall that
the rows of this weight matrix are the mean embeddings vectors ωi, so this likelihood encourages the inner
products ω/latticetop
i˜z·
jto be large for i=j, that is, drawing two augmentations of the same image towards their
mean and thus each other, and to be small for i/negationslash=j, that is, pushing augmentations of different images away
from each other. Finally, the third KL term places a Gaussian prior on Wc, which in our case means that the
ωithat make up this matrix (and thus the embeddings ˜z·
j) cannot grow without bounds to maximize the
likelihood score, but have to stay reasonably close together. Moreover, following best-practice for contrastive
learning ( Chen et al. ,2020a), we use a non-linear projection head gψ(·)onlyfor the contrastive tasks. For
further details, see Appendix A.
Pre-training as Prior Learning. In this work, our central aim is to incorporate unlabelled data into
BNNs. To achieve this, in practice, we perform model learning using contrastive datasets generated from
the unlabelled data and data augmentation. This corresponds to an unsupervised prior learning step. Since
our objective function during this is a principled lower bound on the log-marginal likelihood, it is similar to
type-II maximum likelihood (ML), which is often used to learn parameters for deep kernels ( Wilson et al. ,
2015) of Gaussian processes ( Williams & Rasmussen ,2006), and recently also for BNNs ( Immer et al. ,2021a).
As such, similar to type-II ML, our approach can be understood as a form of prior learning. Although we
learn only a point-estimate for θs, this fixed value induces a prior distribution over predictive functions
through the task-specific prior p(θt). However, while normal type-II ML learns this prior using the observed
data itself, our approach maximises a marginal likelihood derived from unsupervised data.
4 How Good Are Self-Supervised BNN Prior Predictives?
We showed our approach incorporates unlabelled data into the downstream task prior predictive distribution
(Eq.2). We also argued that, as the generated contrastive data encodes our beliefs about the semantic
similarity of different image pairs, incorporating the unlabelled data should improve the functional prior. We
now examine whether this is indeed the case.
Unfortunately, prior predictive checks are hard to apply to BNNs because of the high dimensionality of the
input space. We will therefore introduce our own novel metric to assess the suitability of the prior predictive.
The basis for our approach is to note that, intuitively, a suitable prior should reflect a belief that the higher
the semantic similarity between pairs of inputs, the more likely these inputs are to have the same label .
Therefore, rather than inspecting the prior predictive at single points in input space, we examine the joint
prior predictive of pairsof inputs with known semantic relationships. Indeed, it is far easier to reason about
the relationship between examples than to reason about distributions over high-dimensional functions.
Note that this is of course only a reasonable assumption in cases where we believe to have sufficiently good
knowledge of semantic similarity in our data domain. That is, we need to have a set of data augmentations
for the contrastive tasks, for which we can be reasonably certain that the true labels in our downstream
task will be invariant to them. Recent results in contrastive learning suggest that this is indeed the case for
natural images paired with the augmentations used in SimCLR ( Chen et al. ,2020a), which is why we use
these in our experiments.
To compute our proposed metric, we consider different groups of input pairs. Each group is comprised of
input pairs with known semantic similarity. For example, for image data, we could use images of the same
class as a group with high semantic similarity, and image pairs from different classes as a group with lower
semantic similarity. To investigate the properties of the prior, we can evaluate the probability that input pairs
from different groups are assigned the same label under the prior predictive. We can qualitatively investigate
the behaviour of this probability across and within different groups. For a prior to be more adapted to the
task than an uninformative one, input pairs from groups with higher semantic similarity should be more
likely to have the same label under the prior predictive.
7Published in Transactions on Machine Learning Research (07/2024)
1.0
 0.5
log(x,z)
05DensityConventional BNN: =0.27
Augmented
Same class
Diff. class
3
 2
 1
log(x,z)
02Self-Supervised BNN: =0.78
Figure 3: BNN Prior Predictives. We investigate prior predictives by computing the probability ρthat
particular image pairs have the same label under the prior, and examining the distribution of ρacross different
sets of image pairs. We consider three sets of differing semantic similarity: (i) augmented images ; (ii)images
of the same class ; and (iii) images of different classes . Left: Conventional BNN prior. Right: Self-supervised
BNN learnt prior predictive. The self-supervised learnt prior reflects the semantic similarity of the different
image pairs better than the BNN prior, which is reflected in the spread between the different distributions.
Table 1: Prior Evaluation Scores . Mean and standard deviation across three seeds shown. Self-supervised
priors are better than standard BNN priors.
Prior Predictive Prior Evaluation Score α
BNN — Gaussian 0.261 ±0.024
BNN — Laplace 0.269 ±0.007
Self-Supervised BNN 0.680 ±0.063
Moreover, we can extend this methodology to quantitatively evaluate the prior. Suppose we have Ggroups of
input pairs, Gg={(xg
i,ˆxg
i)|Gg|
i=1}with g= 1, . . . , G, and suppose G1is the group with the highest semantic
similarity, G2is the group with the second highest semantic similarity, and so forth. We define ρ(x,ˆx)as the
probability that inputs x,ˆxhave the same label under the prior predictive, i.e., ρ(x,ˆx) =Eθ[p(y(x) =y(ˆx)|θ)]
where y(x)is the label corresponding to input x. We then define the prior evaluation score ,α, as:
α=E[I(ρ(x1,ˆx1)> . . . > ρ (xG,ˆxG))], (5)
where we compute the expectation sampling (x1,ˆx1)∼ G 1and so forth. This is the probability that the
prior ranks randomly sampled input pairs correctly, in terms of semantically similar groups being assigned
higher probabilities of their input pairs having the same label. We now use this methodology to compare
conventional BNNs and self-supervised BNNs.
Experiment Details. We investigate the different priors on CIFAR10. For the BNN, we follow Izmailov
et al.(2021b) and use a ResNet-20-FRN with a N(0,1/5)prior over the parameters. For the self-supervised
BNN, we learn a base encoder of the same architecture with Duonly and sample from the prior predictive
using Eq. ( 2).θtare the parameters of the linear readout layer. For the image pair groups, we use: (i) an
image from the validation set (the “base image”) and an augmented version of the same image; (ii) a base
image and another image of the same class; and (iii) a base image and an image of a different class. As these
image pair groups have decreasing semantic similarity, we want the first group to be the most likely to have
the same label, and the last group to be the least likely. See Appendix B.3for more details.
Graphical Evaluation. First, we visualise the BNN and self-supervised BNN prior predictive (Fig. 1and
3). The standard BNN prior predictive reflects a belief that all three image pair groups are similarly likely to
have the same label, and thus does not capture semantic information well. In contrast, the self-supervised
prior reflects a belief that image pairs with higher semantic similarity are more likely to have the same label.
8Published in Transactions on Machine Learning Research (07/2024)
In particular, the self-supervised prior is able to distinguish between image pairs of the same class and of
different classes, even without access to any ground-truth labels .
Quantitative Evaluation. We now quantify how well different prior predictives reflect data semantics.
In Table 1, we see that conventional BNN priors reflect semantic similarity much less than self-supervised
BNN priors, matching our qualitative evaluation. Note that this measure has of course been designed by us
to capture the kind of property in the prior that our contrastive training is meant to induce, and should
therefore just be seen as a confirmation that our proposed approach works as expected. There are, naturally,
many other properties that one could desire in a prior, which are not captured by this metric.
5 Self-Supervised BNNs Excel in Low-Label Regimes
In the previous section, we showed that self-supervised BNN prior predictives reflect semantic similarity
of input pairs better than conventional BNNs (§ 4). One hopes that this translates to improved predictive
performance, particularly when conditioning on small numbers of labels, which is where the prior has the
largest effect ( Gelman et al. ,1995;Murphy,2012). We now show that this is indeed the case. Self-supervised
BNNs offer improved predictive performance over standard BNNs, with especially large gains when making
predictions given small numbers of observed labels.
5.1 Semi-Supervised Learning
Training Datasets. We evaluate the performance of different BNNs on the CIFAR10 and CIFAR100
datasets, which are standard benchmarks within the BNN community. We evaluate the performance of
different baselines when conditioning on 50, 500, 5000, and 50000 labels from the training set.
Algorithms. As baselines, we consider the following BNNs: MAP, SWAG ( Maddox et al. ,2019), a deep
ensemble with 5ensemble members ( Lakshminarayanan et al. ,2017), and last-layer Laplace ( Daxberger
et al.,2021). The conventional baselines use standard data augmentation and were chosen because they
support batch normalisation ( Ioffe & Szegedy ,2015). We consider two variants of self-supervised BNNs:
Self-Supervised BNNs pretrain using Duonly, while Self-Supervised BNNs* also use Dt. Both variants use a
non-linear projection head when pretraining and the data augmentations suggested by Chen et al. (2020a).
We use a post-hoc Laplace approximation for the task-specific parameters (the last-layer parameters). We
further consider ensembling self-supervised BNNs.
Evaluation. To evaluate the predictive performance of these BNNs, we report the negative log-likelihood
(NLL). This is a proper scoring rule that simultaneously measures the calibration and accuracy of the different
networks, and is thus an appropriate measure for overallpredictive performance ( Gneiting & Raftery ,2007).
We also report the accuracy and expected calibration error (ECE) in Appendix Table D.1. We further assess
out-of-distribution (OOD) generalisation from CIFAR10 to CIFAR10-C ( Hendrycks & Dietterich ,2019).
Moreover, we evaluate whether these BNNs can detect out-of-distribution inputs from SVHN ( Netzer et al. ,
2011) when trained on CIFAR10. We report the area under the receiver operator curve (AUROC) metric
using the predictive entropy. We want the OOD inputs from SVHN to have higher predictive entropies than
the in-distribution inputs from CIFAR10.
Results. In Table 2, we report the NLL for each BNN when making predictions with different numbers of
labelled examples. We see that self-supervised BNNs offer improved predictive performance over the baselines.
In fact, on the full CIFAR-10 test set, a single self-supervised BNN* outperforms a deep ensemble, whilst
being 5x cheaper when making predictions. We also show that self-supervised BNNs can also be ensembled
to further improve their predictive performance. Incorporating the labelled training data during pretraining
(SS BNN* ) usually improves predictive performance. Self-supervised BNNs also offer strong performance
out-of-distribution and consistently are able to perform out-of-distribution detection. Indeed, they are the
only method with an AUROC exceeding 90% at all dataset sizes. In a further analysis in Appendix Table D.1,
we also see that the improved NLL of self-supervised BNNs is in large part due to improvements in predictive
accuracy, and that incorporating labelled data during pretraining also boosts accuracy. Overall, these results
accord with our earlier findings about the improved prior predictives of self-supervised BNNs compared to
standard BNNs, and highlight the substantial benefits of incorporating unlabelled data into the BNN pipeline.
9Published in Transactions on Machine Learning Research (07/2024)
Table 2: BNN Predictive Performance . We measure the performance of different BNNs for different
numbers of labels. We consider in-distribution prediction, out-of-distribution (OOD) generalisation, and OOD
detection. Shown is the mean and standard error across 3-5 seeds. The out-of-distribution generalisation
results average over all corruptions with intensity level five from CIFAR10-C ( Hendrycks & Dietterich ,2019).
Recall that the SS BNN is performing the contrastive learning separately from and the SS BNN* jointly
with the downstream task. We see that self-supervised BNNs offer improved predictive performance over
conventional BNNs, especially in the low-data regime.
Dataset# labelled
points↓Negative Log Likelihood
MAP LL Laplace SWAG SS BNN SS BNN*Deep SS BNN SS BNN*
Ensemble Ensemble Ensemble
CIFAR10 50 7.594 ±1.0922.259 ±0.0122.332 ±0.0051.047 ±0.0220.996 ±0.0133.689 ±0.1740.980 ±0.0060.953 ±0.010
500 2.504 ±0.1821.895 ±0.0202.072 ±0.0910.454 ±0.0040.441 ±0.0041.805 ±0.0160.399 ±0.0010.384 ±0.002
5000 1.570 ±0.0211.327 ±0.0421.028 ±0.0230.361 ±0.0030.369 ±0.0130.846 ±0.0120.309 ±0.0010.292 ±0.002
50000 0.613 ±0.0440.424 ±0.0130.312 ±0.0080.325 ±0.0050.256 ±0.0020.272 ±0.0020.270 ±0.0010.204 ±0.001
CIFAR100 50 11.86 ±0.344.585 ±0.0066.840 ±0.5394.505 ±0.0024.496 ±0.00210.38 ±0.1104.450 ±4e-44.492 ±4e-4
500 5.536 ±0.0604.359 ±0.0195.282 ±0.2852.640 ±0.0062.614 ±0.0104.867 ±0.0072.533 ±0.0022.510 ±0.002
5000 4.319 ±0.1633.362 ±0.0323.518 ±0.1691.689 ±0.0031.910 ±0.0063.052 ±0.0091.524 ±0.0011.644 ±0.001
50000 1.834 ±0.0641.469 ±0.301.250 ±0.0101.435 ±0.0041.139 ±0.0041.088 ±0.0121.212 ±0.0020.929 ±0.001
CIFAR10 to CIFAR10-C 50 7.140 ±0.8592.275 ±0.0062.353 ±0.0071.723 ±0.0041.697 ±0.0103.970 ±0.1881.638 ±0.0061.603 ±0.004
(OOD Generalisation) 500 2.838 ±0.1752.045 ±0.0112.355 ±0.0831.272 ±0.0141.260 ±0.0102.101 ±0.0211.164 ±0.0041.113 ±0.005
5000 2.423 ±0.2671.644 ±0.0461.705 ±0.0841.235 ±0.0071.237 ±0.0441.382 ±0.0231.103 ±0.0061.096 ±0.001
50000 1.944 ±0.2231.244 ±0.0671.215 ±0.0511.287 ±0.0131.225 ±0.0140.984 ±0.0261.126 ±0.0071.048 ±0.004
↑AUROC (%)
CIFAR10 vs SVHN 50 54.4 ±4.5348.4 ±3.0452.3 ±1.3787.1 ±1.2692.4 ±1.0153.6 ±2.4291.3 ±0.2590.9 ±0.16
(OOD Detection) 500 61.2 ±0.9461.1 ±1.1951.1 ±1.8994.9 ±0.1294.2 ±0.4562.1 ±2.3596.2 ±0.0595.9 ±0.07
5000 83.3 ±2.8784.6 ±0.6359.6 ±0.9996.1 ±0.0794.6 ±1.0092.9 ±0.1997.0 ±0.0196.9 ±0.12
50000 93.8 ±1.1392.6 ±2.0176.4 ±0.5595.6 ±0.1595.5 ±0.1696.8 ±0.3897.0 ±0.0597.7 ±0.06
Moreover, we perform an ablation of our variational distribution q(Wc
j)in Appendix Table C.2, where we see
that our data-dependent mean is indeed needed for good performance.
Note that our goal in this experiment is mainly to compare our self-supervised BNNs against other BNN
methods on equal grounds, not necessarily to reach state-of-the-art performance on the used benchmark
datasets. Indeed, reaching higher performances usually requires computationally expensive hyperparameter
tuning (which we have not systematically performed) as well as using many engineering tricks, such as data
augmentation and batch normalization. These tricks generally affect the likelihood in complicated ways and
are thus often omitted from Bayesian neural networks (see, e.g., the discussions in Nabarro et al. (2021) and
Krishnan et al. (2022)). This is why our results are empirically on par with many recent papers in Bayesian
deep learning (e.g., Immer et al. ,2021b;Ober & Aitchison ,2021;Izmailov et al. ,2021b). However, it should
be noted that some recent attempts have been made to reconcile BNNs with common practical deep learning
tricks to reach high performance (c.f., Rudner et al. ,2023). Adding these orthogonal ideas to our proposed
framework would be a promising avenue for improving its performance to reach state-of-the-art levels.
5.2 Active Learning
We now highlight the benefit of incorporating unlabelled data in an active learning problem. We consider
low-budget active learning, which simulates a scenario where labelling examples is extremely expensive. We
use the CIFAR10 training set as the unlabelled pool set from which to label points. We assume an initial
train set of 50 labelled points, randomly selected, and a validation set of the same size. We acquire 10 labels
per acquisition round up to 500 labels and evaluate using the full test set. We compare self-supervised BNNs
to a deep ensemble, the strongest BNN baseline. We use BALD ( Houlsby et al. ,2011) as the acquisition
function for the deep ensemble and self-supervised BNN, which provide epistemic uncertainty estimates.
We further compare to SimCLR using predictive entropy for acquisition because SimCLR does not model
epistemic uncertainty.
In Fig.4, we see that the methods that leverage unlabelled data perform the best. In particular, the
self-supervised BNN with BALD acquisition achieves the highest accuracy across most numbers of labels, and
substantially outperforms the deep ensemble. This confirms the benefit of incorporating unlabelled data in
10Published in Transactions on Machine Learning Research (07/2024)
100 200 300 400 500
Number of acquired labels20406080Test accuracy (%)Self-Supervised
BNN (Ours)
Ensemble
SimCLR
Figure 4: Low-Budget Active Learning on CIFAR10. We compare (i) a self-supervised BNN, (ii) SimCLR,
and (iii) a deep ensemble. For the self-supervised BNN and the ensemble, we acquire points with BALD.
We use predictive entropy for SimCLR, which does not provide epistemic uncertainty estimates. Mean and
std. shown (3 seeds). The methods that incorporate unlabelled data perform best by far, with our method
slightly outperforming SimCLR.
active learning settings, which by definition are semi-supervised and include unlabelled data. Moreover, our
approach slightly outperforms SimCLR, suggesting that our Bayesian treatment of contrastive learning yields
better uncertainties than conventional non-Bayesian contrastive learning. This is also confirmed in Appendix
Fig.C.1, where we see that our approaches yield consistently lower calibration errors than SimCLR.
6 Related Work
Improving BNN Priors. We demonstrated that BNNs have poor prior predictive distributions (§ 4),
a concern shared by others (e.g., Wenzel et al. ,2020;Noci et al. ,2021;Izmailov et al. ,2021a). The most
common approaches to remedy this are through designing better priors, typically over network parameters
(Louizos et al. ,2017;Nalisnick ,2018;Atanov et al. ,2019;Fortuin et al. ,2021b) or predictive functions
directly ( Sun et al. ,2019;Tran et al. ,2020;Matsubara et al. ,2021;D’Angelo & Fortuin ,2021, seeFortuin
(2022) for an overview). In contrast, our approach incorporates vast stores of unlabelled data into the prior
distribution through variational model learning. Similarly, other work also learnspriors, but typically using
labelled data e.g., by using meta-learning ( Garnelo et al. ,2018;Rothfuss et al. ,2021) or type-II maximum
likelihood ( Wilson et al. ,2015;Immer et al. ,2021a;Dhahri et al. ,2024), or by using transfer learning in an
ad hocway (Shwartz-Ziv et al. ,2022). Notably, function-space variational inference methods ( Sun et al. ,
2019;Rudner et al. ,2023) often also use unlabelled data, which has been shown to potentially improve the
out-of-distribution performance of these models ( Lin et al. ,2023). However, in this case, the unlabelled data
is only used for evaluating the KL divergence in function space, a practice which has theoretically been shown
to be insufficient Burt et al. (2020). Conversely, our work uses semantic information from the unlabelled
data to actually informthe function-space prior. Another related line of work is concerned with learning
invariances from data in Bayesian models using the marginal likelihood ( van der Wilk et al. ,2018;Immer
et al.,2022). This case is essentially the opposite of our setting, as there, the labels are known but the
augmentations are learned, while in our case, the augmentations constitute our prior knowledge, but we do
not know the data labels.
A Perspective on Contrastive Learning. We offer a Bayesian interpretation and understanding of
contrastive learning (§ 3). Under our framework, pretraining is understood as model learning—a technique
for finding probabilistic models with prior predictive distributions that capture our semantic beliefs. There
has been much other work on understanding contrastive learning (e.g., Wang & Isola ,2020;Wang & Liu ,
2021). Some appeal to the InfoMax principle ( Becker & Hinton ,1992).Zimmermann et al. (2022) argue that
contrastive learning inverts the data-generating process, while Aitchison (2021) cast InfoNCE as the objective
11Published in Transactions on Machine Learning Research (07/2024)
of a self-supervised variational auto-encoder. Ganev & Aitchison (2021) formulate several semi-supervised
learning objectives as lower bounds of log-likelihoods in a probabilistic model of data curation.
Semi-Supervised Deep Generative Models. Deep generative models (DGMs) are a fundamentally
different approach for label-efficient learning ( Kingma & Welling ,2013;Kingma et al. ,2014;Joy et al. ,2020).
A semi-supervised DGM models the full distribution p(x, y)with generative modelling, and so incorporates
unlabelled data by learning to generate it. Unlike BNNs, we can condition the parameters of a DGM on
unlabelled data. In contrast, our approach does not model the data distribution—the unlabelled data is used
to construct pseudo-labelled tasks that encode our prior beliefs. Self-supervised BNNs are discriminative
models, which tend to be more scalable and perform better for discriminative tasks compared to full generative
modelling ( Ng & Jordan ,2001;Bouchard & Triggs ,2004). Finally, Sansone & Manhaeve (2022) try to unify
self-supervised learning and generative modelling under one framework.
7 Conclusion
We introduced Self-Supervised Bayesian Neural Networks , which allow semantic information from unlabelled
data to be incorporated into BNN priors. Using a novel evaluation scheme, we showed that self-supervised
BNNs learn functional priors that better reflect the semantics of the data than conventional BNNs. In turn,
they offer improved predictive performance over conventional BNNs, especially in low-data regimes. Going
forward, we believe that effectively leveraging unlabelled data will be critical to the success of BNNs in many,
if not most, potential applications. We hope our work encourages further development in this crucial area.
Acknowledgments
MS was supported by the EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines and
Systems (EP/S024050/1), and thanks Rob Burbea for inspiration and support. VF was supported by a
Postdoc Mobility Fellowship from the Swiss National Science Foundation, a Research Fellowship from St
John’s College Cambridge, and a Branco Weiss Fellowship.
References
Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh,
Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. A review of uncertainty
quantification in deep learning: Techniques, applications and challenges. Information Fusion , 76:243–297,
2021.
Laurence Aitchison. InfoNCE is a variational autoencoder, July 2021. URL http://arxiv.org/abs/
2107.02495 . arXiv:2107.02495 [cs, stat].
Andrei Atanov, Arsenii Ashukha, Kirill Struminsky, Dmitry Vetrov, and Max Welling. The Deep Weight
Prior.arXiv:1810.06943 [cs, stat] , February 2019. URL http://arxiv.org/abs/1810.06943 . arXiv:
1810.06943.
Suzanna Becker and Geoffrey E. Hinton. Self-organizing neural network that discovers surfaces in random-
dot stereograms. Nature, 355(6356):161–163, January 1992. ISSN 1476-4687. doi: 10.1038/355161a0.
URLhttps://www.nature.com/articles/355161a0 . Number: 6356 Publisher: Nature Publishing
Group.
Guillaume Bouchard and Bill Triggs. The tradeoff between generative and discriminative classifiers. In 16th
IASC International Symposium on Computational Statistics (COMPSTAT’04) , pp. 721–728, 2004.
David R Burt, Sebastian W Ober, Adrià Garriga-Alonso, and Mark van der Wilk. Understanding variational
inference in function-space. In Third Symposium on Advances in Approximate Bayesian Inference , 2020.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework for Contrastive
Learning of Visual Representations. arXiv:2002.05709 [cs, stat] , June 2020a. URL http://arxiv.org/
abs/2002.05709 . arXiv: 2002.05709.
12Published in Transactions on Machine Learning Research (07/2024)
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big Self-Supervised
Models are Strong Semi-Supervised Learners. arXiv:2006.10029 [cs, stat] , October 2020b. URL http:
//arxiv.org/abs/2006.10029 . arXiv: 2006.10029.
Xinlei Chen and Kaiming He. Exploring Simple Siamese Representation Learning, November 2020. URL
http://arxiv.org/abs/2011.10566 . arXiv:2011.10566 [cs].
Francesco D’Angelo and Vincent Fortuin. Repulsive deep ensembles are bayesian. Advances in Neural
Information Processing Systems , 34:3451–3465, 2021.
Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, and Philipp
Hennig. Laplace redux-effortless bayesian deep learning. Advances in Neural Information Processing
Systems, 34:20089–20103, 2021.
Rayen Dhahri, Alexander Immer, Betrand Charpentier, Stephan Günnemann, and Vincent Fortuin. Shaving
weights with occam’s razor: Bayesian sparsification for neural networks using the marginal likelihood.
arXiv preprint arXiv:2402.15978 , 2024.
Daniel Flam-Shepherd, James Requeima, and David Duvenaud. Mapping gaussian process priors to bayesian
neural networks. In NIPS Bayesian deep learning workshop , volume 3, 2017.
Vincent Fortuin. Priors in bayesian deep learning: A review. International Statistical Review , 2022.
Vincent Fortuin, Adrià Garriga-Alonso, Mark van der Wilk, and Laurence Aitchison. BNNpriors: A library
for Bayesian neural network inference with different prior distributions. Software Impacts , 9:100079, 2021a.
Vincent Fortuin, Adrià Garriga-Alonso, Florian Wenzel, Gunnar Rätsch, Richard Turner, Mark van der Wilk,
and Laurence Aitchison. Bayesian Neural Network Priors Revisited. arXiv:2102.06571 [cs, stat] , February
2021b. URL http://arxiv.org/abs/2102.06571 . arXiv: 2102.06571.
Adam Foster, Rattana Pukdee, and Tom Rainforth. Improving transformation invariance in contrastive
representation learning. In International Conference on Learning Representations , 2020.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In
International Conference on Machine Learning , pp. 1183–1192. PMLR, 2017.
Stoil Ganev and Laurence Aitchison. Semi-supervised learning objectives as log-likelihoods in a generative
model of data curation, October 2021. URL http://arxiv.org/abs/2008.05913 . arXiv:2008.05913
[cs, stat].
Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J. Rezende, S. M. Ali Eslami,
and Yee Whye Teh. Neural Processes, July 2018. URL http://arxiv.org/abs/1807.01622 .
arXiv:1807.01622 [cs, stat].
Andrew Gelman, John B Carlin, Hal S Stern, and Donald B Rubin. Bayesian data analysis . Chapman and
Hall/CRC, 1995.
Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of
the American statistical Association , 102(477):359–378, 2007.
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya,
Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray
Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-
supervised Learning, September 2020. URL http://arxiv.org/abs/2006.07733 . arXiv:2006.07733
[cs, stat].
James Harrison, John Willes, and Jasper Snoek. Variational bayesian last layers. In The Twelfth International
Conference on Learning Representations , 2023.
13Published in Transactions on Machine Learning Research (07/2024)
Dan Hendrycks and Thomas Dietterich. Benchmarking Neural Network Robustness to Common Corruptions
and Perturbations. In International Conference on Learning Representations , 2018.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions
and perturbations. Proceedings of the International Conference on Learning Representations , 2019.
Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. Bayesian active learning for classification
and preference learning. arXiv preprint arXiv:1112.5745 , 2011.
Olivier J. Hénaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami, and Aaron
van den Oord. Data-Efficient Image Recognition with Contrastive Predictive Coding, July 2020. URL
http://arxiv.org/abs/1905.09272 . arXiv:1905.09272 [cs].
Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar Rätsch, and Khan Mohammad Emtiyaz. Scalable
marginal likelihood estimation for model selection in deep learning. In International Conference on Machine
Learning , pp. 4563–4573. PMLR, 2021a.
Alexander Immer, Maciej Korzepa, and Matthias Bauer. Improving predictions of bayesian neural nets via
local linearization. In International conference on artificial intelligence and statistics , pp. 703–711. PMLR,
2021b.
Alexander Immer, Tycho van der Ouderaa, Gunnar Rätsch, Vincent Fortuin, and Mark van der Wilk.
Invariance learning in deep neural networks with differentiable laplace approximations. Advances in Neural
Information Processing Systems , 35:12449–12463, 2022.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International conference on machine learning , pp. 448–456. PMLR, 2015.
Pavel Izmailov, Patrick Nicholson, Sanae Lotfi, and Andrew Gordon Wilson. Dangers of Bayesian Model
Averaging under Covariate Shift. arXiv:2106.11905 [cs, stat] , June 2021a. URL http://arxiv.org/
abs/2106.11905 . arXiv: 2106.11905.
Pavel Izmailov, Sharad Vikram, Matthew D. Hoffman, and Andrew Gordon Wilson. What Are Bayesian
Neural Network Posteriors Really Like? arXiv:2104.14421 [cs, stat] , April 2021b. URL http://arxiv.
org/abs/2104.14421 . arXiv: 2104.14421.
Tom Joy, Sebastian M Schmon, Philip HS Torr, N Siddharth, and Tom Rainforth. Capturing label character-
istics in vaes. arXiv preprint arXiv:2006.10102 , 2020.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 ,
2013.
Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, and Max Welling. Semi-Supervised Learning with
Deep Generative Models, October 2014. URL http://arxiv.org/abs/1406.5298 . arXiv:1406.5298
[cs, stat].
Ranganath Krishnan, Pi Esposito, and Mahesh Subedar. Bayesian-Torch: Bayesian neural network layers for
uncertainty estimation, January 2022. URL https://github.com/IntelLabs/bayesian-torch .
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional
neural networks. Advances in neural information processing systems , 25, 2012.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty
estimation using deep ensembles. Advances in neural information processing systems , 30, 2017.
Miguel Lázaro-Gredilla and Aníbal R Figueiras-Vidal. Marginalized neural network mixtures for large-scale
regression. IEEE transactions on neural networks , 21(8):1345–1351, 2010.
14Published in Transactions on Machine Learning Research (07/2024)
Jihao Andreas Lin, Joe Watson, Pascal Klink, and Jan Peters. Function-space regularization for deep bayesian
classification. In Fifth Symposium on Advances in Approximate Bayesian Inference , 2023.
Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. Advances in
neural information processing systems , 30, 2017.
David J C Mackay. Bayesian Methods for Adaptive Models. Technical report, 1992.
Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple
baseline for bayesian uncertainty in deep learning. Advances in Neural Information Processing Systems , 32,
2019.
Laura Manduchi, Kushagra Pandey, Robert Bamler, Ryan Cotterell, Sina Däubener, Sophie Fellenz, Asja
Fischer, Thomas Gärtner, Matthias Kirchler, Marius Kloft, Yingzhen Li, Christoph Lippert, Gerard
de Melo, Eric Nalisnick, Björn Ommer, Rajesh Ranganath, Maja Rudolph, Karen Ullrich, Guy Van den
Broeck, Julia E Vogt, Yixin Wang, Florian Wenzel, Frank Wood, Stephan Mandt, and Vincent Fortuin.
On the challenges and opportunities in generative ai. arXiv preprint arXiv:2403.00025 , 2024.
Takuo Matsubara, Chris J Oates, and François-Xavier Briol. The ridgelet prior: A covariance function
approach to prior specification for bayesian neural networks. The Journal of Machine Learning Research ,
22(1):7045–7101, 2021.
Ning Miao, Tom Rainforth, Emile Mathieu, Yann Dubois, Yee Whye Teh, Adam Foster, and Hyunjik Kim.
Learning instance-specific augmentations by capturing local invariances. International Conference on
Machine Learning , 2023.
Kevin P Murphy. Machine learning: a probabilistic perspective . MIT press, 2012.
Seth Nabarro, Stoil Ganev, Adrià Garriga-Alonso, Vincent Fortuin, Mark van der Wilk, and Laurence
Aitchison. Data augmentation in Bayesian neural networks and the cold posterior effect. arXiv:2106.05586
[cs, stat], June 2021. URL http://arxiv.org/abs/2106.05586 . arXiv: 2106.05586.
Eric Nalisnick, Jonathan Gordon, and José Miguel Hernández-Lobato. Predictive complexity priors. In
International Conference on Artificial Intelligence and Statistics , pp. 694–702. PMLR, 2021.
Eric Thomas Nalisnick. On priors for Bayesian neural networks . University of California, Irvine, 2018.
Radford M Neal. BAYESIAN LEARNING FOR NEURAL NETWORKS. Technical report, 1995.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. 2011.
Andrew Ng and Michael Jordan. On discriminative vs. generative classifiers: A comparison of logistic
regression and naive bayes. Advances in neural information processing systems , 14, 2001.
Lorenzo Noci, Kevin Roth, Gregor Bachmann, Sebastian Nowozin, and Thomas Hofmann. Disentangling the
Roles of Curation, Data-Augmentation and the Prior in the Cold Posterior Effect. arXiv:2106.06596 [cs] ,
June 2021. URL http://arxiv.org/abs/2106.06596 . arXiv: 2106.06596.
Sebastian W Ober and Laurence Aitchison. Global inducing point variational posteriors for bayesian neural
networks and deep gaussian processes. In International Conference on Machine Learning , pp. 8248–8259.
PMLR, 2021.
Sebastian W Ober and Carl E Rasmussen. Benchmarking the neural linear model for regression. In Second
Symposium on Advances in Approximate Bayesian Inference , 2019.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive Predictive
Coding, January 2019. URL http://arxiv.org/abs/1807.03748 . arXiv:1807.03748 [cs, stat].
15Published in Transactions on Machine Learning Research (07/2024)
Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan Arbel, David
Dunson, Maurizio Filippone, Vincent Fortuin, Philipp Hennig, Aliaksandr Hubin, Alexander Immer,
Theofanis Karaletsos, Mohammad Emtiyaz Khan, Agustinus Kristiadi, Yingzhen Li, Stephan Mandt,
Christopher Nemeth, Michael A Osborne, Tim GJ Rudner, David Rügamer, Yee Whye Teh, Max Welling,
Andrew Gordon Wilson, and Ruqi Zhang. Position paper: Bayesian deep learning in the age of large-scale
ai.arXiv preprint arXiv:2402.00809 , 2024.
Robert Price. A useful theorem for nonlinear devices having gaussian inputs. IRE Transactions on Information
Theory, 4(2):69–72, 1958.
Vishnu Raj, Tianyu Cui, Markus Heinonen, and Pekka Marttinen. Incorporating functional summary
information in bayesian neural networks using a dirichlet process likelihood approach. In International
Conference on Artificial Intelligence and Statistics , pp. 6741–6763. PMLR, 2023.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate
inference in deep generative models. In International conference on machine learning , pp. 1278–1286.
PMLR, 2014.
Jonas Rothfuss, Vincent Fortuin, Martin Josifoski, and Andreas Krause. Pacoh: Bayes-optimal meta-learning
with pac-guarantees. In International Conference on Machine Learning , pp. 9116–9126. PMLR, 2021.
Tim GJ Rudner, Sanyam Kapoor, Shikai Qiu, and Andrew Gordon Wilson. Function-space regularization
in neural networks: A probabilistic perspective. In International Conference on Machine Learning , pp.
29275–29290. PMLR, 2023.
Emanuele Sansone and Robin Manhaeve. Gedi: Generative and discriminative training for self-supervised
learning. arXiv preprint arXiv:2212.13425 , 2022.
Mrinank Sharma, Sebastian Farquhar, Eric Nalisnick, and Tom Rainforth. Do bayesian neural networks need
to be fully stochastic? In International Conference on Artificial Intelligence and Statistics , pp. 7694–7722.
PMLR, 2023.
Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. Journal
of big data , 6(1):1–48, 2019.
Ravid Shwartz-Ziv, Micah Goldblum, Hossein Souri, Sanyam Kapoor, Chen Zhu, Yann LeCun, and An-
drew Gordon Wilson. Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors, May
2022. URL http://arxiv.org/abs/2205.10279 . arXiv:2205.10279 [cs].
Shengyang Sun, Guodong Zhang, Jiaxin Shi, and Roger Grosse. Functional variational bayesian neural
networks. arXiv preprint arXiv:1903.05779 , 2019.
Ba-Hien Tran, Simone Rossi, Dimitrios Milios, and Maurizio Filippone. All you need is a good functional
prior for bayesian deep learning. arXiv preprint arXiv:2011.12829 , 2020.
Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the marginal
likelihood. Advances in Neural Information Processing Systems , 31, 2018.
Feng Wang and Huaping Liu. Understanding the Behaviour of Contrastive Loss. In 2021 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 2495–2504, Nashville, TN, USA,
June 2021. IEEE. ISBN 978-1-66544-509-2. doi: 10.1109/CVPR46437.2021.00252. URL https://
ieeexplore.ieee.org/document/9577669/ .
Tongzhou Wang and Phillip Isola. Understanding Contrastive Representation Learning through Alignment and
UniformityontheHypersphere. In Proceedingsof the 37th International Conference on Machine Learning , pp.
9929–9939.PMLR,November2020. URL https://proceedings.mlr.press/v119/wang20k.html .
ISSN: 2640-3498.
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient pseudo-independent
weight perturbations on mini-batches. arXiv preprint arXiv:1803.04386 , 2018.
16Published in Transactions on Machine Learning Research (07/2024)
Florian Wenzel, Kevin Roth, Bastiaan S. Veeling, Jakub Swiatkowski, Linh Tran, Stephan Mandt, Jasper
Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How Good is the Bayes Posterior in
Deep Neural Networks Really? arXiv, February 2020. URL http://arxiv.org/abs/2002.02405 .
Publisher: arXiv.
Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning , volume 2.
MIT press Cambridge, MA, 2006.
Andrew G Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective of generalization.
Advances in neural information processing systems , 33:4697–4708, 2020.
Andrew G Wilson, Zhiting Hu, Russ R Salakhutdinov, and Eric P Xing. Stochastic variational deep kernel
learning. Advances in neural information processing systems , 29, 2016.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep Kernel Learning,
November 2015. URL http://arxiv.org/abs/1511.02222 . arXiv:1511.02222 [cs, stat].
Larry Yaeger, Richard Lyon, and Brandyn Webb. Effective training of a neural network character classifier
for word recognition. Advances in neural information processing systems , 9, 1996.
Yang You, Igor Gitman, and Boris Ginsburg. Scaling sgd batch size to 32k for imagenet training. arXiv
preprint arXiv:1708.03888 , 6(12):6, 2017.
Roland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel. Contrastive
Learning Inverts the Data Generating Process, April 2022. URL http://arxiv.org/abs/2102.08850 .
arXiv:2102.08850 [cs].
17Published in Transactions on Machine Learning Research (07/2024)
A Self-Supervised BNNs: Further Considerations
We introduced Self-Supervised BNNs (§3), which benefit from unlabelled data for improved predictive
performance within the probabilistic modelling framework. To summarise, our conceptual framework uses
data augmentation to create a set of contrastive datasets {Dc
j}L
j=1. In our probabilistic model, conditioning
on this data is equivalent to incorporating unlabelled data into the task prior predictive. We now discuss
further considerations and provide further details.
Theoretical Considerations. In §3.1, we treated the number of contrastive task datasets, L, as a fixed
hyper-parameter. However, one could generate an potentially infinite number of datasets, in which case, the
posterior p(θs|{Dc
j}L
j=1,Dt)∝p(θs)·p(Dt|θs)·/producttextL
j=1p(Dc
j|θs)will collapse to a delta function, and will be
dominated by the contrastive tasks. This justified learning a point estimate for θs, but if one wanted to avoid
this behaviour, one could re-define the posterior:
˜p(θs|{Dc
j}L
j=1,Dt)∝p(θs)·p(Dt|θs)·L/productdisplay
j=1p(Dc
j|θs)γ/L. (A.1)
The log-posterior would equal, up to a constant:
log ˜p(θs|{Dc
j}L
j=1,Dt) = log p(θs) + log p(Dt|θs) +γ
LL/summationdisplay
j=1logp(Dc
j|θs). (A.2)
Here, the total evidence contributed by the contrastive datasets is independent of L, and instead controlled
by the hyper-parameter γ. This is equivalent to posterior tempering. The final term of the above equation
could also be re-defined as an averagelog-likelihood when sampling different Dc, i.e., we could use:
log ˜p(θs|Du,Dt) = log p(θs) + log p(Dt|θs) +γEDc[p(Dc|θs)]. (A.3)
In this case, we look for a distribution over θswhere Dchas a high likelihood on average. Our practical
algorithm samples a different Dcper gradient step, and instead weights the Dtterm with α, which is similar
to the above approach if we set α= 1/γand modify the prior term as needed. Re-defining the framework in
this way allows it to support potentially infinite numbers of generated contrastive datasets. We further note
that this framework could naturally be extended to multi-task scenarios.
Practical Considerations. In practice, we share parameters θsacross all tasks (i.e., across the generated
contrastive tasks and the actual downstream task), and learn a θtseparately for each task. We focus on
image classification problems and let θsbe the parameters of a base encoder , which produces a representation
z.θtare the parameters of a linear readout layer that makes predictions from z. We discussed learning a
point-estimate for θsby optimising an ELBO derived from the unlabelled data only:
˜Lc
j(θs) =Eq(θc
j)[logp(Dc
j|θs, θc
j)]−DKL(q(θc
j)||p(θc
j))≤logp(Dc
j|θs). (A.4)
We (optionally) further include an ELBO derived using task-specific data:
˜Lt(θs) =Eq(θt)[logp(Dt|θt, θs)]−DKL[q(θt)||p(θt)]≤logp(Dt|θs). (A.5)
Our final objective is:
L(θs) = log p(θs) +α˜Lt(θs) +EDc[˜Lc
j], (A.6)
where α= 0would learn the base-encoder parameters only using the unlabelled data. αcontrols the weighting
between the generated contrastive task data and the downstream data. We see the above objective is closely
related to a (lower bound of) Eq. ( A.3). We further modify this objective function, following best practice,
either in the contrastive learning or Bayesian deep learning communities, and improve performance by:
18Published in Transactions on Machine Learning Research (07/2024)
1.We add a non-linear projection head to the base encoder architecture only for the contrastive task
datasets. As such, we use z=gφ(fθs(x))/||gφ(fθs(x))||forDc.gφ(·)is the projection head, and the
representation is normalised. For the downstream tasks, we use z=fθs(x), i.e., we “throw-away”
the projection head. This is best practice within the contrastive learning community ( Chen et al. ,
2020a;b).
2.Wetemperthe KL divergence term, using the mean-per-parameter KL divergence (denoted as
¯DKL(·||·)). Tempering is necessary for several Bayesian deep learning algorithms to perform well
(Wenzel et al. ,2020;Krishnan et al. ,2022).
3.We generate a new contrastive task dataset per gradient step, update on that dataset, and then
discard it. This follows standard contrastive learning algorithms ( Chen et al. ,2020a).
4.We rescale the likelihood terms in the ELBOs ˜Lt(θs)and ˜Lc
j(θs)to be average per-datapoint
log-likelihoods, e.g., we use1
|Dc
j|logp(Dc
j|θt
j, θs).
5.Instead of having an explicit prior distribution over θs, we use standard weight-decay for training,
i.e., we specify a penalty on the norm of the weights of the encoder per gradient step .
Together, these changes yield the objective function used in Algorithm 1. Finally, we note that different
practical algorithms ensue depending on the choice of θt, the choice of θs, and the techniques used to perform
approximate inference. We employ variational inference and learn a point estimate for θs, but there are other
choices possible.
B Experiment Details
We now provide further experiment details and additional results. The vast majority of experiments were
run on an internal compute cluster using Nvidia Tesla V100 or A100 GPUs. The maximum runtime for an
experiment was less than 16 hours.
B.1 Semi-Supervised Learning (§ 5)
B.1.1 Datasets
We consider the CIFAR10 and CIFAR100 datasets ( Krizhevsky et al. ,2009). The entire training set is the
unsupervised set, and we suppose that we have access to different numbers of labels. For the evaluation
protocols, we reserve a validation set of 1000 data points from the test set and evaluate using the remaining
9000 labels.
To assess out-of-distribution generalisation, we further evaluation on the CIFAR-10-C dataset ( Hendrycks &
Dietterich ,2018). We compute the average performance across all corruptions with intensity level five.
B.1.2Self-Supervised BNNs
Base Architecture. We use a ResNet-18 architecture, modified for the size of CIFAR10 images, following
Chen et al. (2020a). The representations produced by this architecture have dimensionality 512. Further, for
the non-linear projection head, we use a 2 layer multi-layer perceptron (MLP) with output dimensionality
128.
Contrastive Augmentations. We follow Chen et al. (2020a) and compose a random resized crop, a random
horizontal flip, random colour jitter, and random grayscale for the augmentation. These augmentations
make up the contrastive augmentation set A. We finally normalise the images to have mean 0 and standard
deviation 1 per channel, as is standard.
Hyperparameters. We use a N(0,1
τ2p)prior over the linear parameters θt, and tune τpfor each dataset.
As such, τcan be understood as the prior temperature. We use τp= 0.65for CIFAR10 and τp= 0.6for
CIFAR100. We use weight decay 1e−6for the base encoder and projection head parameters.
19Published in Transactions on Machine Learning Research (07/2024)
Variational Distribution. We parameterize the temperature and noise scale using the logof their values.
That is, we have σ= exp ˜ σ.
Optimisation Details. We use the LARS optimiser ( You et al. ,2017), with batch size 1000and momentum
0.9. We train for 1000 epochs, using a linear warmup cosine annealing learning rate schedule. The warmup
starting learning rate for the base encoder parameters is 1e−3with a maximum learning rate of 0.6. For
the variational parameters, the maximum learning rate is 1e−3, which we found to be important for the
stability of the algorithm.
Laplace Evaluation Protocol. We find a point estimate for θtfound using the standard linear evaluation
protocol (i.e., SGD training). We then apply a post-hoc Laplace approximation using the generalised Gauss-
Newton approximation to the Hessian using the laplace library (Daxberger et al. ,2021). For CIFAR10,
we use a full covariance approximation and for CIFAR100 we use a Kroenker-factorised approximation for
the Hessian of the last layer weights and biases. We tune the prior precision by maximising the likelihood
of a validation set. For predictions, we use the (extended) probit approximation. These choices follow
recommendations from Daxberger et al. (2021).
B.1.3Self-Supervised BNNs*
Self-Supervised BNNs* additionally leverage labelled data when training the base encoder by including an
additional ELBO ˜Ltthat depends on Dt.
We use mean-field variational inference over θtwith a Gaussian approximate posterior and Flipout ( Wen et al. ,
2018). We use the implementation from Krishnan et al. (2022), and temper by setting β= 1/|θt|, meaning
we use the average per-parameter KL divergence. αis a hyperparameter that controls the relative weighting
between the generated contrastive task datasets and the observed label data, and is tuned. For CIFAR10, We
useα= 5·10−5when we have fewer than 100 labels, and α= 5·10−3otherwise. For CIFAR100, We use
α= 5·10−5when we have fewer than 1000 labels, and α= 5·10−3otherwise. For p(θt), we use a N(0,1)
prior. For downstream evaluation, we use the Laplace evaluation protocol.
All other details follow Self-Supervised BNNs .
B.1.4 BNN Baselines
All baselines use the same ResNet-18 architecture, which was modified for the image size used in the
CIFAR image datasets. The baselines we considered were chosen because they are all compatible with batch
normalisation, which is included in the base architecture. We provide further details about the baselines
below.
MAP. For the maximum-a-posterior network, we use the Adam optimiser with learning rate 10−3, default
weight decay, and batch size 1000. We train for a minimum of 25 epochs and a maximum of 300 epochs,
terminating training early if the validation loss increases for 3 epochs in a row.
Last-Layer Laplace. For the Last-Layer Laplace baseline, we perform a post-hoc Laplace approximation
to a MAP network trained using the protocol above. We use the same settings as for the self-supervised
BNN’s Laplace evaluation.
Deep Ensemble. For the deep ensemble baseline, we train 5 MAP networks starting from different
initialisations using the above protocol, and aggregate their predictions.
SWAG. For the SWAG baseline, we first a MAP network using the above protocol. We then run SGD from
this solution for 10 epochs, taking 4 snapshots per epoch, and using K= 20as the rank of the covariance
matrix. We choose the SWAG learning rate per run using the validation set, and consider 10−2,10−3, and
10−4.
20Published in Transactions on Machine Learning Research (07/2024)
B.2 Active Learning (§ 5)
We simulate a low-budget active learning setting. For each method, we use their default implementation
details as outlined in this Appendix. With regards to the active learning setup, we assume that we have
access to a small validation set of 50labelled examples and are provided 50labelled training examples. We
acquire 10examples per acquisition round up to a maximum of 500labelled examples, which corresponds to
1% of the labels in the training set. We evaluate using the full test set. The deep ensemble and self-supervised
BNNs provide epistemic uncertainty estimates, so we perform active learning by selecting the points with the
highest BALD metric ( Houlsby et al. ,2011). For SimCLR, we acquire points using the highest predictive
entropy, a commonly used baseline ( Gal et al. ,2017). For this experiment, we use only the CIFAR10 dataset.
SimCLR and the self-supervised BNNs here are pretrained on 500 epochs, not 1000 epochs as default.
B.3 Prior Predictive Checks (§ 4)
BNN Prior Predictive. We use the ResNet-20-FRN architecture, which is the architecture used by
Izmailov et al. (2021b). Note that this architecture does not include batch normalisation, which means the
prior over parameters straightforwardly corresponds to a prior predictive distribution. We use a N(0,1
5)prior
over all network weights, again following Izmailov et al. (2021b), and sample from the prior predictive using
8192 Monte Carlo samples.
Self-Supervised Prior Predictives. We primarily follow the details outlined earlier, except we use
the same ResNet-20-FRN architecture as used for the BNNs and batch size of 500 rather than 1000. To
sample from the prior predictive, we use Eq. ( 2) and have y∼softmax (W fθs(x)), where we normalise the
representations produced by the base encoder to have zero mean and unit variance, and we have W∼ N (0,20),
with the prior precision chosen by hand. We neglect the biases because they introduce additional variance.
The prior evaluation scores are not sensitive to the prior variance choice, and are evaluated by sampling
images from the validation set, which was not seen during training. We used 4096Monte Carlo samples from
the prior.
21Published in Transactions on Machine Learning Research (07/2024)
C Ablation Studies
Effect of Batch Size. We study the effect of the pre-training batch size on the performance of our
self-supervised BNNs. We run one seed for 100 epochs across three different batch sizes on CIFAR10. We see
in Table C.1, the performance on CIFAR10 is robust to reducing the batch size. We hypothesise this is due
to the noise injected during pre-training.
Table C.1: Effect of pretraining batch size on self-supervised BNN.
Batch Size CIFAR10 Accuracy (%)
100 0.81
500 0.81
1000 0.80
Effect of Variational Distribution. We run an ablation study changing the variational distribution
mean on CIFAR10. We evaluated using one seed, training for 100 epochs only. We consider setting the mean
of the variational distribution for image i,ωi, to be: 0.5(˜zA
i+˜zB
i),˜zA
i, and 0. We see that a suitable mean is
required for good performance.
Table C.2: Effect of the pretraining variational distribution on self-supervised BNN performance on CIFAR10.
We see that some variant of our data-dependent mean is needed for good performance.
Variational Dist. Mean CIFAR10 Accuracy (%)
0 0.19
˜zA
i 0.79
0.5(˜zA
i+ ˜zB
i) 0.80
Effect of Pretraining and Inference. To better understand the effect of the pretraining objective and
the approximate inference scheme used, we performed an ablation study on CIFAR10. We considered both
variants of our variational pretraining, and additionally deterministic pretraining that uses the NT-XENT
loss. We also consider either using Laplace approximate inference or MAP estimation for the task parameters.
Using the NT-XENT loss and MAP inference corresponds to SimCLR ( Chen et al. ,2020a), as widely used
in the self-supervised learning community. For NT-XENT, we use τ= 0.45for CIFAR10 and τ= 0.3for
CIFAR100. For these experiments, we only train for 500 epochs.
In Fig.C.1, we see that incorporating the labelled data during pretraining boosts accuracy, but surprisingly
decreases calibration. Relative to SimCLR, all of the self-supervised BNNs offer improved calibration at all
dataset sizes. All approaches have high accuracy at low data regimes, highlighting the benefit of leveraging
unlabelled data. Both deterministic pretraining and variational pretraining behave similarly, but performing
approximate inference over task parameters substantially improves calibration.
22Published in Transactions on Machine Learning Research (07/2024)
102103104
Num. Labels708090Test accuracy (%)
102103104
Num. Labels0510Test ECE (%)
Self-Supervised BNN (Ours)
Self-Supervised BNN* (Ours)SimCLR
Self-Supervised BNN
Deterministic Pretraining (Ours)
Figure C.1: Effect of Pretraining and Inference on CIFAR10. On the left plot, red, green, and blue
lines overlap. On the right plot, blue and red lines overlap. Recall that the SS BNN is performing the
contrastive learning separately from and the SS BNN* jointly with the downstream task. We see that our SS
BNN*slightly outperforms the other approaches in terms of accuracy and that all of our approaches yield
better-calibrated uncertainties than SimCLR.
D Additional Results
We additionally report the in-distribution accuracy and expected calibration error (ECE) of different BNNs
when observing different numbers of labels. These metrics, unlike the log-likelihood, are interpretable. But
note that for a useful classifier, we need to have bothaccurate andwell-calibrated predictions.2
In Table D.1, we see that self-supervised BNNs substantially outperform conventional BNNs in terms of
in-distribution accuracy. The gains are particularly large at smaller dataset sizes, precisely where improved
priors are expected to make the biggest difference. Moreover, in terms of calibration, they consistently offer
well-calibrated uncertainty estimates. Even though LL Laplace offers well-calibrated uncertainty estimates
at a low numbers of labels, the predictions are much less accurate than self-supervised BNNs. We also see
that incorporating labelled data during pretraining or ensembling self-supervised BNNs boosts accuracy, but
surprisingly can harm calibration. Curiously, we find that the calibration of ensemble methods also sometimes
worsens as we condition on more data.
2There are perfectly calibrated but useless classifiers, e.g., if 70% of examples are class A and 30% are class B, predicting
p(class A ) = 0 .7on every input achieves perfect ECE but does not discriminate between examples at all.
23Published in Transactions on Machine Learning Research (07/2024)
Table D.1: Bayesian Neural Network Predictive Performance . Here, relative to the main results table,
we report the accuracy and expected calibration error of different methods separately. Recall that the SS
BNNis performing the contrastive learning separately from and the SS BNN* jointly with the downstream
task.
Dataset# labelled
points↑Accuracy (%)
MAP LL Laplace SWAG SS BNN SS BNN*Deep SS BNN SS BNN*
Ensemble Ensemble Ensemble
CIFAR10 50 14.6 ±0.714.9 ±0.814.8 ±0.266.3 ±0.868.3 ±0.219.0 ±0.368.9 ±0.169.5 ±0.2
500 31.2 ±0.432.4 ±0.931.5 ±7.484.8 ±0.286.2 ±0.139.2 ±0.587.0 ±0.187.6 ±0.1
5000 56.5 ±3.153.4 ±2.466.4 ±1.287.7 ±0.188.6 ±0.272.1 ±0.389.6 ±0.290.9 ±0.03
50000 83.4 ±1.985.7 ±0.390.5 ±0.688.6 ±0.191.7 ±0.191.8 ±0.390.7 ±0.293.2 ±0.07
CIFAR100 50 3.6±0.23.6±0.33.1±0.614.5 ±0.114.7 ±0.33.8±0.0616.2 ±0.0216.5 ±0.1
500 7.0±0.26.1±0.27.7±0.338.5 ±0.239.0 ±0.39.2±0.140.8 ±0.141.3 ±0.01
5000 22.7 ±0.721.2 ±0.625.6 ±0.954.5 ±0.156.0 ±0.328.2 ±0.158.4 ±0.162.5 ±0.1
50000 60.5 ±2.359.6 ±1.167.6 ±0.559.9 ±0.169.2 ±0.170.7 ±0.566.4 ±0.174.9 ±0.1
CIFAR10 to CIFAR10-C 50 13.6 ±0.713.8 ±0.714.0 ±1.645.5 ±0.0345.5 ±0.516.9 ±0.447.7 ±0.148.0 ±0.1
(OOD Generalisation) 500 26.7 ±0.427.2 ±0.826.2 ±5.257.8 ±0.459.0 ±0.232.0 ±0.560.3 ±0.161.8 ±0.1
5000 42.1 ±0.943.2 ±1.150.5 ±1.659.6 ±0.160.3 ±1.155.1 ±0.362.9 ±0.264.0 ±0.3
50000 59.2 ±3.762.1 ±1.669.0 ±0.459.8 ±0.363.1 ±0.570.1 ±0.463.6 ±0.266.8 ±0.2
↓Expected Calibration Error (ECE; %)
CIFAR10 50 65.9 ±3.22.7±0.76.2±2.01.7±0.021.9±0.335.3 ±2.81.9±0.12.0±0.2
500 30.0 ±2.13.2±0.613.4 ±0.61.5±0.22.6±0.110.6 ±0.62.3±0.12.0±0.1
5000 21.0 ±0.54.5±0.410.0 ±0.31.1±0.082.1±0.13.8±0.62.3±0.12.6±0.2
50000 8.4±0.51.5±0.42.8±0.40.8±0.021.2±0.13.7±0.32.8±0.12.1±0.1
CIFAR100 50 61.6 ±1.1 - 17.5 ±6.011.6 ±0.111.7 ±0.338.5 ±0.713.4 ±0.0213.6 ±0.1
500 25.3 ±0.71.8±1.122.0 ±4.52.1±0.12.7±0.314.9 ±0.23.3±0.042.9±0.1
5000 32.3 ±2.11.4±0.0616.4 ±6.41.5±0.27.8±0.34.4±0.53.2±0.112.3 ±0.2
50000 17.8 ±2.21.6±0.28.9±0.91.7±0.12.5±0.14.7±0.086.5±0.16.6±0.1
CIFAR10 to CIFAR10-C 50 67.6 ±3.04.0±0.76.9±0.88.4±0.69.2±0.737.3 ±2.96.1±0.15.9±0.1
(OOD Generalisation) 500 33.7 ±1.97.4±0.818.0 ±8.08.1±0.78.9±0.615.5 ±0.37.2±0.17.6±0.4
5000 30.4 ±2.69.2±1.319.1 ±0.87.4±0.47.8±0.86.9±0.35.0±0.16.5±0.2
50000 24.1 ±3.011.1 ±0.613.6 ±1.39.1±0.212.2 ±0.56.0±0.66.1±0.16.8±0.01
24