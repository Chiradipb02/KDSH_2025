Published in Transactions on Machine Learning Research (05/2023)
Efficient Model-Based Multi-Agent Mean-Field
Reinforcement Learning
Barna Pásztor barna.pasztor@ai.ethz.ch
ETH Zürich
Ilija Bogunovic i.bogunovic@ucl.ac.uk
University College London
Andreas Krause krausea@ethz.ch
ETH Zürich
Reviewed on OpenReview: https: // openreview. net/ forum? id= gvcDSDYUZx
Abstract
Learning in multi-agent systems is highly challenging due to several factors including
the non-stationarity introduced by agents’ interactions and the combinatorial nature of
their state and action spaces. In particular, we consider the Mean-Field Control (MFC)
problem which assumes an asymptotically infinite population of identical agents that
aim to collaboratively maximize the collective reward. In many cases, solutions of an
MFC problem are good approximations for large systems, hence, efficient learning for
MFC is valuable for the analogous discrete agent setting with many agents. Specifically,
we focus on the case of unknown system dynamics where the goal is to simultaneously
optimize for the rewards and learn from experience. We propose an efficient model-based
reinforcement learning algorithm, M3–UCRL, that runs in episodes, balances between
exploration and exploitation during policy learning, and provably solves this problem.
Our main theoretical contributions are the first general regret bounds for model-based
reinforcement learning for MFC, obtained via a novel mean-field type analysis. To learn
the system’s dynamics, M3–UCRL can be instantiated with various statistical models, e.g.,
neural networks or Gaussian Processes. Moreover, we provide a practical parametrization of
the core optimization problem that facilitates gradient-based optimization techniques when
combined with differentiable dynamics approximation methods such as neural networks.
1 Introduction
Multi-Agent Reinforcement Learning (MARL) extends the scope of reinforcement learning (RL) to multiple
agents acting in a shared system. It is receiving considerable attention given a great number of real-world
applications including autonomous driving (Shalev-Shwartz et al., 2016), finance (Lee et al., 2002; Lee et al.,
2007; Lehalle & Mouzouni, 2019), social science (Castelfranchi, 2001; Leibo et al., 2017), swarm motion
(Almulla et al., 2017), e-sports (Vinyals et al., 2019; Pachocki et al., 2018), and traffic routing (El-Tantawy
et al., 2013), to name a few. Despite the recent popularity, analyzing agents’ performance in these systems
remains a particularly challenging task for several reasons including non-stationarity, scalability, competing
learning goals, and varying information availability of agents. In this work, we target the challenges of
non-stationarity andscalability . From the perspective of an individual agent, the dynamics of the system
are non-stationary, since other agents update their behaviour, which alters over time how the environment
reacts to certain actions of the agent. Multi-agent systems also suffer from a combinatorial nature due to
the exponential growth of the state and action space as the number of agents in the system increases.
To tackle the previous challenges, Mean-Field Control (MFC) exploits the insight that many relevant MARL
problems involve a large number of very similar agents that are coordinated via a centralized controller.
1Published in Transactions on Machine Learning Research (05/2023)
As a motivating application, consider a ride-hailing service in which a central dispatcher coordinates the
routes of many drivers around the city. MFC considers the limiting regime of controlling infinitely many
identical collaborative agents, and utilizes the notion of mean-field approximation from statistical physics
(Weiss, 1907) to model their interactions. Specifically, instead of focusing on the individual agents and
their interactions, the state of the system is described via the agents’ state distribution and the dynamics
and rewards are formulated accordingly. Despite the introduced approximations, the solution to the MFC
problem often remains adequate for the finite agent equivalent (Lacker, 2017).
ThefocusofthisworkisonlearningtheoptimalpoliciesinMFCwhentheunderlyingdynamicsare unknown .
Incomplexdomainssuchasfleetsofautonomousvehiclesorrobotswarms, thisisoftenthecase, andthemod-
els consequently need to be learnt from data, giving rise to a well-known exploration–exploitation dilemma.
In this paper, we propose the Model-Based Multi-Agent Mean-Field Upper-Confidence RL algorithm (M3–
UCRL) that is provably efficient and can effectively trade-off between exploration and exploitation during
policylearning. SimilarlytootherModel-BasedReinforcementLearning(MBRL)algorithms, M3–UCRLcol-
lects data about the unknown dynamics online (i.e., by proposing and executing a policy in the true system)
and estimates the possible dynamics the system might follow. To optimize the agent’s actions, it simulates
the unknown system by using the estimated dynamics. Chua et al. (2018) has shown that MBRL can solve
challenging single-agent RL problems with low sample complexity. We transfer this property to multi-agent
problems and show that M3–UCRL can efficiently solve the MFC problem with unknown dynamics.
Related work. Our M3–UCRL extends the class of Model-Based Reinforcement Learning (MBRL) algo-
rithms that rely on the optimism-in-the-face-of-uncertainty principle. Algorithms based on this approach
achieve optimal regret for tabular MDPs (Brafman & Tennenholtz, 2002; Auer et al., 2009; Efroni et al.,
2019; Domingues et al., 2020), while in the continuous setting, Abbasi-Yadkori & Szepesvaŕi (2011) and
Jin et al. (2020) show ˜O(√
T)regret bounds for LQR and linear MDPs, respectively. The recently designed
episodic model-based algorithms (Chowdhury & Gopalan, 2019; Kakade et al., 2020; Curi et al., 2020;
Sessa et al., 2022) achieve similar regret in the kernelized setting. Sessa et al. (2022) consider the general
multi-agent setting, and prove a regret bound with O(NH/2)dependence on the number Nof agents. In
comparison, M3–UCRL assumes an asymptotically large population of identical agents. Crucially, as we will
show, its performance does notdepend on the number of agents, making it more suitable for large systems.
Multi-Agent Reinforcement Learning (MARL) has seen tremendous progress in recent years as many
real-world applications involve interactions among a number of agents. Buşoniu et al. (2008) provide an
overview of classical and early MARL results. Several other MARL surveys focus on: non-stationarity
induced by the agent interactions (Hernandez-Leal et al., 2017), deep MARL (Hernandez-Leal et al., 2019;
Nguyen et al., 2020), theoretical foundations in Markov/stochastic/extensive-form games (Zhang et al.,
2019) and cooperative MARL (OroojlooyJadid & Hajinezhad, 2019). Recently, Zhang et al. (2019) and
Nguyen et al. (2020) recognize the lack of practical model-based MARL algorithms (with an early exception
of the R-Max algorithm proposed by Brafman & Tennenholtz (2000; 2002) for two-player zero-sum Markov
Games). We fill this gap by designing a practical model-based algorithm that considers a large population
of agents, and that is compatible with deep models.
Modeling interaction via the mean-field approach in MARL has recently gained popularity due to the
introduction of Mean-Field Games (MFG) (Lasry & Lions, 2006a;b; Huang et al., 2006; 2007). MFG consider
the limiting regime of a competitive game among identical agents with the objective of reaching a Nash
Equilibrium. Various works (Yin et al., 2010; 2013; Cardaliaguet & Hadikhanloo, 2017; Guéant et al., 2011;
Bensoussan et al., 2013; Carmona & Delarue, 2013; Carmona et al., 2018; Gomes et al., 2014) mainly consider
solving MFGs in continuous-time, while some very recent ones (Yang et al., 2017; 2018; Fu et al., 2020; Guo
et al., 2019; 2020; Elie et al., 2020; Agarwal et al., 2019) analyze the problem from the discrete-time RL
perspective. The progress of the field has been recently summarized by Laurière et al. (2022).
At the same time, cooperative discrete-time Mean-Field Control , the main focus of this paper, has received
significantly less attention. Gast et al. (2012); Motte & Pham (2019); Gu et al. (2020; 2021) and Bäuerle
(2021) consider planning with known dynamics. In the learning setting, Wang et al. (2020); Carmona et al.
(2019b); Gu et al. (2021); Angiuli et al. (2022; 2021) extend the Q-Learning algorithm and show convergence
for specific variants of MFC focusing on discrete state and action spaces. The mean-field extension of the
2Published in Transactions on Machine Learning Research (05/2023)
linear-quadratic problem has been studied by Carmona et al. (2019a); Wang et al. (2021); Carmona et al.
(2021) with policy optimisation methods. Subramanian & Mahajan (2019) also proposes a policy-gradient
based algorithm that solves Mean-Field Games and Control problems locally. Additionally, Chen et al.
(2021); Li et al. (2021); Wang et al. (2020) consider the approximation error between the Mean-Field and
the related Multi-Agent Reinforcement Learning problems. These previous model-free methods are either
sample inefficient, assume access to a simulator or consider finite state and action spaces. On the contrary,
our algorithm is model-based, sample-efficient, works for continuous spaces and learns the policy online by
performing exploration on a real system.
Main contributions. We design a novel Model-based Multi-agent Mean-field Upper Confidence RL (M3–
UCRL) algorithm for the centralized control problem of a large population of collaborative agents. M3–
UCRL is practical, performs exploration on a real system (requires no simulator and efficient in terms of
samples), and is compatible with neural network policy learning. Our main contributions are the general
theoretical regret bounds obtained via a novel mean-field-based analysis, that we also specialize to Gaussian
Process models. In the main step of our theoretical analysis, we bound the distance between the mean-field
distributions under the true and the approximated dynamics via the model’s total estimated uncertainty.
Finally, we pose the exploration via entropy maximization problem used in a recent MFG survey (Laurière
et al., 2022) in the continuous state and action domain and demonstrate the performance of our algorithm
in it. Our results show that M3–UCRL is capable of finding close-to-optimal policies within a few episodes,
in contrast to model-free algorithms that require at least six orders of magnitude more samples.
2 Problem Statement
We consider the episodic Mean-Field Control (MFC) problem with time-horizon H, compact state space
S ⊆Rpand compact action space A⊆Rqthat are common for all the agents in the system. We index
episodes with the variable tand useTto denote the number of episodes passed. In standard N-agent
settings, the system’s state in episode tat timeh∈{0,...,H−1}is described with individual agents’
states (s(1)
t,h,...,s(N)
t,h)∈SNwhich grows exponentially in the number of agents Nand represents a common
issue in scaling MARL algorithms. In MFCs, we assume that all agents are identical and the population
isasymptotically infinite , i.e.,N→∞, therefore, the agents’ states can be described by the mean-field
distribution :
µt,h(s) = lim
N→∞1
NN/summationdisplay
i=1I{s(i)
t,h=s},
whereµt,hbelongs to the space of probability measures P(S)overS. Due to the assumption of identical
agentsinMFC,wecanfocusona representative agent fromthepopulationthatinteractswiththedistribution
of agents instead of individual agents and interactions.
System Dynamics. Before every episode t, the representative agent selects a policy profile
πt= (πt,0,...,πt,H−1)where the Hindividual policies πt,h:S×P (S)→ Aare chosen from a set
of admissible policies Π(e.g., we parametrize our polices via neural networks; see below). At every time
h, the representative agent selects an action at,h=πt,h(st,h,µt,h). The environment then returns reward
r(st,h,at,h,µt,h), and the agent observes its new state st,h+1and the mean-field distribution µt,h+1.
In MFCs, the system’s dynamics are typically given by a McKean-Vlasov type of stochastic processes and
depend on the agent’s state, action and the mean-field distribution:
st,h+1=f(st,h,at,h,µt,h) +ωt,h, (1)
whereωt,his ani.i.d.additive noise vector. This is in contrast to the standard single-agent model-based
RL (cf. Chowdhury & Gopalan (2019); Curi et al. (2020); Kakade et al. (2020)), where the dynamics
only depend on agent’s action and state. Crucially, since the focus of this work is on learning in MFC, we
assume that the true dynamics are unknown , and the goal is to explore the space and learn about fover
a number of episodes.1To do so, the representative agent relies on the collected random observations,
1We expect our results to easily extend and account for unknown rewards by using the same modeling assumptions for the
reward function (e.g., as in Chowdhury & Gopalan (2019)).
3Published in Transactions on Machine Learning Research (05/2023)
Dt={((st,h,at,h,µt,h),st,h+1)}H−1
h=0, which come from the interaction with the true system (i.e., from policy
rollouts) during episode t.
Finally, after every episode, we assume that the whole system is reset and the agent’s initial state s0is drawn
from a known initial distribution µ0, i.e.,s0∼µ0, that remains the same in every episode.
Mean-field Flow. Since, in MFC, all agents are identical, interact in a common environment, and
follow the same policyπt= (πt,0,...,πt,H−1), the subsequent mean-field distributions satisfy the following
mean-field flow property as shown by Gu et al. (2020):
Lemma 1. For a given initial distribution µ0, dynamics fand policyπ= (π0,...,πh−1), the mean-field
distribution trajectory {µh}H−1
h=0, follows
µh+1(ds′) =/integraldisplay
s∈Sµh(ds)P[sh+1∈ds′], (2)
whereµh∈P(S)for allh≥0,ah=πh(sh,µh),sh+1is given by Eq. (1), andµh(ds) =P[sh∈ds]underπ.
We provide the proof in Appendix B.2. To shorten the notation, we use Φ(µt,h,πt,h,f)to denote the
mean-field transition function from Eq. (2), i.e., we have µt,h+1= Φ(µt,h,πt,h,f). The lemma shows that the
next mean-field distribution explicitly depends on the unknown f(sincest,h+1=f(st,h,at,h,µt,h) +ωt,h),
policy played by the agents πt,h, and previous state distribution µt,h.
Performance metric. Given a policy profile π= (π1,...,πH−1), the performance of the representative
agent is measured via the expected cumulative reward:
J(π) =E/bracketleftbiggH−1/summationdisplay
h=0r(sh,ah,µh)/bracketrightbigg
(3a)
s.t.ah=πh(sh,µh), (3b)
sh+1=f(sh,ah,µh) +ωh, (3c)
µh+1= Φ(µh,πh,f), (3d)
where the expectation is taken over the noise in the transitions and initial distribution s0∼µ0. By
considering the representative agent’s perspective, the goal is to discover a sociallyoptimal policy π∗that
maximizes the expected total reward, i.e.,
π∗∈arg max
πJ(π) (4)
In our theoretical analysis, we make the following assumptions regarding the system’s true dynamics, reward
functionandthesetofadmissiblepolicies. Weuse z,z′∈Z=S×A×P (S)todenote (s,a,µ )and(s′,a′,µ′),
respectively, and we make use of the Wasserstein-1 metric: W1(µ,µ′) = infν/integraltext
S×S∥x−y∥2dν(x,y),where
νis any probability measure on S×Swith marginals µandµ′(see Appendix A for useful properties of
W1(µ,µ′)that are used in our analysis).
Assumption 1. The transition function fisLf–Lipschitz-continuous, i.e., ∥f(z)−f(z)∥2≤Lfd(z,z′)
whered(z,z′) :=∥s−s′∥2+∥a−a′∥2+W1(µ,µ′)andωt,harei.i.d.additiveσ-sub-Gaussian noise vectors
for allt≥1andh∈{0,...,H−1}.
Assumption 2. The set of admissible policies Πconsists ofLπ–Lipschitz-continuous policies such that for
anyπ∈Π:∥π(s,µ)−π(s′,µ′)∥2≤Lπ(∥s−s′∥2+W1(µ,µ′)),and the reward function is Lr–Lipschitz-
continuous, i.e.,|r(z)−r(z′)|≤Lrd(z,z′).
Regularity assumptions like these are standard in the single-agent model-based reinforcement learning liter-
ature (Jaksch et al., 2010; Curi et al., 2020; Chowdhury & Gopalan, 2019; Sessa et al., 2022) and mild since,
in practice, the policy and reward function classes are typically chosen to satisfy the previous smoothness
assumptions. In our experiments (see Section 5), we parametrize our policies with Neural Networks with
Lipschitz continuous activations (e.g., tanh,ReLUandlinear). We note that the Lipschitzness of such poli-
cies then follows from that of the activations when the network’s weights are bounded (in practice, this can
be done by directly bounding them or via regularization).
4Published in Transactions on Machine Learning Research (05/2023)
Remark1. The episodic Mean-Field Control problem relates closely to the finite-horizon case of the evolutive
Mean-Field Game problem described in Laurière et al. (2022). Both settings consider mean-field distributions
evolving over time depending on the agents’ policies, however, the MFC problem focuses on collaborative
agents while agents in the evolutive MFG are competitive. This distinction leads to different solution concepts
of the problems; social welfare in MFC and Nash Equilibrium in MFG.
3 The M3–UCRL Algorithm
In this section, we tackle the Mean-Field Control Problem with unknowndynamics from Eq. (4) by relying on
amodel-based learning scheme. Our Model-based Multi-agent Mean-field Upper Confidence RL (M3–UCRL)
algorithm uses a statistical model to estimate the system’s dynamics and to effectively trade-off between
exploration and exploitation during policy learning. Before stating our algorithm, we consider the main
properties of the considered dynamics models.
Statistical Model. Our representative agent learns about the unknown dynamics from the data collected
during the interactions with the environment, i.e., the policy rollouts. In particular, we take a model-based
perspective (see Algorithm 1), where the agent models the unknown dynamics and sequentially, after every
episodet, updates and improves its model estimates based on the previous transition observations, i.e.,
D1∪···∪DtwhereDt={((st,h,at,h,µt,h),st,h+1)}H−1
h=0is the set of state transition observations in episode
t. To reason about plausible models at every episode t, the representative agent can take a frequentist or
Bayesian perspective. In the first case, it estimates the mean mt:Z→Sand confidence Σt:Z→Rp×p
functions. In the second case, the agent estimates the posterior distribution over dynamical models p(˜f|Dt),
that leads to mt(z) =E˜f∼p(˜f|Dt)[˜f(z)], andΣ2
t(z) =Var[˜f(z)]. We denote σt(·) = diag( Σt(·))and make
the following assumptions regarding the considered statistical model irrespective of the taken perspective:
Assumption 3 (Calibrated model) .The statistical model is calibrated w.r.t. f, i.e., there is a known
non-decreasing sequence of confidence parameters {βt}t≥0, eachβt∈R>0and depending on δ, such that
with probability at least 1−δ, it holds jointly for all tandz∈Zthat|f(z)−mt(z)|≤βtσt(z)elementwise.
Assumption 4. The function σt(·)isLσ-Lipschitz-continuous for all t≥1.
The calibrated model assumption (or its equivalents) is standard in online model-based learning (Srinivas
et al., 2010; Chowdhury & Gopalan, 2017; 2019; Curi et al., 2020; Sessa et al., 2022). It states that the agent
canbuildhighprobabilityconfidenceboundsaroundtheunknowndynamicsatthebeginningofeveryepisode.
As more observations become available, we expect the epistemic uncertainty of the model (that arises due to
the lack of data and is encoded in σ(·)) to shrink, and consequently allow the representative agent to make
better decisions as it becomes more confident about the true dynamics. Our theoretical results obtained in
Section 4 hold for general model classes as long as Assumption 3 and Assumption 4 are satisfied. Below and
in Section 4, we provide concrete conditions (about f) and models for which this assumption provably holds.
Algorithm. At the beginning of episode t, the representative agent constructs the confidence set of
dynamics functions, denoted by Ft−1, satisfying the elementwise confidence interval in Assumption 3 with
mt−1(·)andσt−1(·)estimated based on the observations up until the end of the previous episode t−1, i.e.,
Ft−1=/braceleftig
˜f:|˜f(z)−mt−1(z)|≤βt−1σt−1(z)elementwise and∀z∈Z/bracerightig
.
Then, the agent selects the optimistic policy πtwhich achieves the highest possible cumulative reward over
the set of admissible policies, Π, and plausible system dynamics Ft−1. In particular, the representative
agent solves the following problem:
πt= arg max
π∈Πmax
˜f∈Ft−1E/bracketleftiggH−1/summationdisplay
h=0r(˜st,h,˜at,h,˜µt,h)/bracketrightigg
(5a)
s.t. ˜at,h=πt,h(˜st,h,˜µt,h), (5b)
˜st,h+1=˜f(˜st,h,˜at,h,˜µt,h) + ˜ωt,h, (5c)
˜µt,h+1= Φ(˜µt,h,πt,h,˜f), (5d)
5Published in Transactions on Machine Learning Research (05/2023)
Algorithm 1 Model-based RL for Mean-field Control
Input:Calibrated dynamical model, reward function r(st,h,at,h,µt,h), horizonH, initial state s1,0∼µ0
fort= 1,2,...do
Use M3–UCRL to select policy profile πt= (πt,0,...,πt,h−1)by using the current dynamics model and
reward function, i.e., solve Eq. (5)
forh= 0,...,H−1do
at,h=πt,h(st,h,µt,h),
st,h+1=f(st,h,at,h,µt,h) +ωt,h
µt,h+1= Φ(µt,h,πt,h,f)
end for
Update agent’s statistical model with new observations {(st,h,at,h,µt,h),st,h+1}H−1
h=0
Reset the system to µt+1,0←µ0andst+1,0∼µt+1,0
end for
where the expectation in Eq. (5a) is taken w.r.t. the initial state distribution µ0and noise in transitions.
The policyπtfound by solving the previous problem is then used in the true multi-agent system (e.g.,
used by every driver in a ride-hailing service) for one episode. It induces the observed mean-field flow, and
aims to improve the total collective reward. After the episode ends, the representative agent augments its
observed data, i.e., D1:t=D1:t−1∪Dt, and improves its model estimates mt(·)andσt(·). The algorithm is
summarized in Algorithm 1. Since solving Eq. (5) is a challenging task even for known dynamics and the
focus of this work is on the statistical complexity of learning in MFC (similarly to Kakade et al. (2020);
Chowdhury & Gopalan (2019); Curi et al. (2020); Sessa et al. (2022)), we assume that we can solve Eq. (5)
for a givenFt−1andΠ. We propose a practical implementation below with details on solving Eq. (5).
The algorithm implements the upper-confidence bound principle, since the system’s true dynamics fbelong
to the confidence set Ft−1with high-probability (by Assumption 3). Consequently, the reward achieved by
πtunder the best possible dynamics ˜fupper bounds the performance of πtunder the true dynamics f.
Practical Implementation. In general, optimizing over the set Ft−1is not tractable. However, a
practical problem reformulation has been designed (Moldovan et al., 2015; Curi et al., 2020) for single-agent
problems, which defines an auxiliary policy to select the dynamics function ˜f∈Ft−1. We generalize this
approach to the MFC setting to implement M3–UCRL for our experiments (see Appendix D.1 for details).
M3–UCRL can be combined with any statistical model that satisfies Assumption 3. For example, under mild
assumptions on the unknown dynamics, Gaussian Processes (GP) models (Rasmussen, 2003) can be prov-
ably calibrated under some regularity assumptions on the dynamics (Srinivas et al., 2010; Abbasi-Yadkori &
Szepesvaŕi, 2011). Neural Network (NN) models (Anthony & Bartlett, 2009) such as Probabilistic and De-
terministic Ensembles (Lakshminarayanan et al., 2017; Chua et al., 2018) are more scalable, and even though
they are not calibrated by default, their empirical recalibration is possible (Kuleshov et al., 2018). Finally,
using such differentiable dynamics models and parameterizing πt(·)and the auxiliary policy selecting ˜fvia
separateneuralnetworks, allowsforoptimizingEq.(5)byusinggradient-basedapproaches(seeAppendixD).
Remark 2. M3–UCRL uses an optimistic upper-confidence bound strategy similarly to single-agent model-
based upper-confidence algorithms (Chowdhury & Gopalan, 2019; Kakade et al., 2020; Curi et al., 2020;
Sessa et al., 2022), however, it addresses several important differences: (i) dynamics and rewards explicitly
depend on the states of all agents through the mean-field distribution; (ii) each agent’s action depends on the
mean-field distribution and the evolution of the mean-field trajectory is induced by the collection of actions
taken by the agents; (iii) crucially, M3–UCRL employs the principle that each agent in the system executes
the same policy selected by the representative agent; Moreover, we note that single-agent MBRL algorithms
are inherently unsuitable for solving the Mean Field Control problem.
4 Theoretical Analysis
We analyze our approach using the standard notion of cumulative regret . It measures the difference in
the cumulative expected reward between the socially optimal policy π∗(from Eq. (4)) and the individual
6Published in Transactions on Machine Learning Research (05/2023)
policies selected by M3–UCRL in every episode: RT=/summationtextT
t=1(J(π∗)−J(πt)). We also use rtto denote
the regret incurred during episode t, i.e.,rt:=J(π∗)−J(πt). We aim to show that M3–UCRL achieves
sublinear cumulative regret, i.e., limT→∞1
TRT= 0. This implies that as the number of episodes increases,
the performance of the selected policies converges to that of the optimal policy: J(πt)→J(π∗). We defer
all the proofs from this section to Appendix B and only highlight the main contributions of our work.
In general, the speed of convergence of M3–UCRL will depend on the difficulty of estimating f. For more
complex functions, we expect our models to require more observations to achieve close approximation of
the underlying dynamics. Consequently, we use the following model-based complexity measure to quantify
this aspect of the learning problem:
IT= max
˜D1,...,˜DTT/summationdisplay
t=1/summationdisplay
z∈˜D1∪···∪ ˜Dt∥σt−1(z)∥2
2, (6)
where ˜Dtis a set of possible observations in an episode for each t= 1,...,T, andσt−1(·)is the confidence
estimate of the selected statistical model computed based on ˜D1∪···∪ ˜Dt−1. We note that the analogous
notion of model complexity has been recently used in single agent model-based RL (Chowdhury & Gopalan,
2019; Curi et al., 2020) and the multi-agent setting (Sessa et al., 2022). Intuitively, ITmeasures the chosen
model’s total confidence in estimating the dynamics over Tepisodes in the worst-case, i.e., when the sets
of observations ˜D1,..., ˜DTare the least informative about the unknown dynamics f. For statistical models
that fail to learn and generalize, ITmay grow linearly with T. On the other hand, for models that learn
from a "small" number of samples, we expect σt−1(·)to shrink fast.
General Model-based Regret Bound. The following theorem bounds the cumulative regret after T
episodes in terms of the model complexity measure ITdefined in Eq. (6).
Theorem 1. Under Assumptions 1 to 4, let LT−1= 1 + 2(1 + Lπ) [Lf+ 2βT−1Lσ], and letµt,h∈P(S)
for allt,h > 0. Then for all T≥1and fixed constant H > 0, with probability at least 1−δ, the regret of
M3–UCRL is at most:
RT=O/parenleftig
βTLr(1 +Lπ)LH−1
T−1/radicalbig
H3TIT/parenrightig
.
The obtained regret bound explicitly depends on constant factors that describe the environment such as the
episode horizon Hand the Lipschitz constants. The dependence on the size of the problem, i.e., pandq, is
implicit inβTandITand specific to the statistical model used in the algorithm. We provide more explicit
dependencies for Gaussian Processes below and in Appendix C.1. We also note that βTis a function of δ
(see Assumption 3). When polices are selected according to Eq. (5) at every episode, our result implies that
the obtained performance J(πt)eventually converges to the optimal one J(π∗)if the joint dependence on
the model-dependent quantities ITandβTscales asO(√
T).
In comparison to other model-based algorithms, both M3–UCRL and H-UCRL (Curi et al., 2020) achieve
O(√H3TIT)regret while H-MARL (Sessa et al., 2022) achieves O(NH/2√H3TIT). The additional factor of
O(NH/2)comes from the fact that H-MARL considers separate agents with individual action spaces while
M3–UCRL optimises for a representative agent instead. In terms of tightness of our results, we expect
that the dependency on O(√TIT)in the regret bound is unavoidable. Scarlett et al. (2017) showed that
this is a lower bound for the kernelized bandit problem for specific kernels and using a Gaussian Process.
This result relates to the special case of MFC when H= 1and using Gaussian Process statistical model to
estimatethetransitiondynamics. Thedependencyon H3, however, couldbeimprovedundermorerestrictive
assumptions.
In our theoretical analysis, we address non-trivial challenges that arise from considering an infinite number
of agents. In particular, we first translate the Mean Field Control to a non-standard MDP with the dis-
tributional state space P(S)and action space Πof all admissible policies (for details, see Appendix B.2).
This enables us to use the upper-confidence bound principle with the assumption of well-calibrated models
to show the following bound on the episodic regret:
rt≤2Lr(1 +Lπ)H−1/summationdisplay
h=1W1(µt,h,˜µt,h),
7Published in Transactions on Machine Learning Research (05/2023)
(a)
 (b)
Figure 1: Fig. 1a shows the initial mean-field distribution µh,0for each episode h. Fig. 1b shows the
mean-field distribution at the end of an episode using a policy that selects actions uniformly at random.
whereµt,hand˜µt,hare the mean-field distributions when the policy πtis deployed in (i) the true multi-agent
system and (ii) the system with dynamics given by the transition function ˜ftselected by the oracle corre-
sponding toπt. The main theoretical contribution of our work is provided in Lemma 5 (see Appendix B.3)
which shows the following bound on the Wasserstein-1 distance between the two mean-field distributions:
W1(˜µt,h,µt,h)≤2βt−1Lh−1
t−1h−1/summationdisplay
i=1/integraldisplay
S∥σt−1(s,πt(s,µt,i),µt,i)∥2µt,i(ds).
This shows that the deviation between the used ˜µt,handµt,his bounded by the uncertainty of the statistical
model integrated over µt,h. As the model’s uncertainty decreases, the set Ftshrinks towards the true
dynamicsfand the dynamics ˜fselected in Eq.(5) is restricted to be close to f.
Next, we show an example of how the previously obtained regret bounds can be instantiated for particular
models. To do so, we consider the case of Gaussian Process models.
Bounds for Gaussian Process (GP) Models. GP models are frequently used to model unknown
dynamics in the model-based literature (see, e.g., Srinivas et al. (2010); Chowdhury & Gopalan (2019); Curi
et al. (2020); Sessa et al. (2022)). They can successfully distinguish between model’s and noise uncertainty,
while their expressiveness follows from different kernel functions that can be used to model different types
of correlation in data.
Under some regularity assumptions on f, i.e., when the true fhas a bounded norm in the Reproducing
Kernel Hilbert Space (RKHS) induced by the GP kernel function, these models provably satisfy Assump-
tion 3. Moreover, we formally show in Appendix C that for such a GP statistical model, we can bound IT
by theGP maximum mutual information (MMI) . Similarly, we can express βTvia the same quantity, and
conclude that sublinearity of our regret depends on the MMI rates. Srinivas et al. (2010) obtain sublinear
(in the number of episodes T) upper bounds for this quantity in case of the most commonly-used kernels,
such as the linear, squared-exponential, or Matérn kernels, while Krause & Ong (2011) prove sublinear
bounds for certain composite kernels.
Finally, we note that the aforementioned works consider kernels defined on Euclidean spaces while our input
space forf, namelyZ=S×A×P (S), includes the space of probability densities P(S). Providing bounds
on the GP maximum mutual information capacity in case of kernels defined on probability spaces is an
interesting direction for future work.
8Published in Transactions on Machine Learning Research (05/2023)
(a)
 (b)
Figure 2: Convergence of the M3–UCRL algorithm under system dynamics given by Eq. (7). Fig. 2a
shows the rewards defined in Eq. (4) achieved by M3–UCRL over 30 episodes. Confidence bounds show
the minimum and maximum over 10 independent experiments. The BPTT line corresponds to the rewards
achieved under known dynamics and considered to be optimal in the environment. Fig. 2b shows the close
to uniform mean-field distribution at the end of the episode, h= 20, for a policy optimised by M3–UCRL.
5 Experiments
In this section, we demonstrate the performance of the M3–UCRL algorithm on the exploration via entropy
maximization problem introduced by Geist et al. (2021) and used as a benchmark problem in a recent
survey on Mean-Field Games (Laurière et al., 2022). Different from previous works, we formulate the
problem in the continuous state and action spaces as follows. The state-space of the model is described by a
2-dimensional space in [0,11]2which is split into 4equal-sized rooms separated by unit-sized walls with one
corridor connecting neighboring rooms (See Fig. 1a). Agents are free to move around within the area and
are stopped by the walls if they try to move through them. Each episode tconsists of 21steps starting from
h= 0and in each time-step hthe representative agent chooses its actions from the action space A= [0,1]2.
The dynamics of the system in Eq. (1) are of the following form
f(st,h,at,h,µt,h) =st,h+at,h, (7)
and the additive noise is Gaussian with zero mean, variance σ2and independent dimensions, i.e., ωt,h∼
N(0,σ2I2)for alltandhwhereI2is the 2×2unit matrix. The individual reward function is defined
asr(s,a,µ ) =−log(µ(s))meaning that the representative agent aims to avoid crowded areas. On the
whole population’s level, the average one-step reward is given by Es∼µ[−log(µ(s)] =−/integraltext
s∈Sµ(s) log(µ(s)),
so maximizing the average reward is equivalent of maximizing the population’s entropy. The population is
initially condensed in the bottom-left corner (as shown in Fig. 1a) in a unit square, therefore, a policy is
optimal if it disperses the population quickly to reach a uniform distribution.
We parameterize our policy with a Neural Network and use a Deep Ensemble model (Lakshminarayanan
et al., 2017) for estimating the system dynamics. To represent the mean-field distribution, we discretize
the state-space to unit squares and assign each cell the probability of the representative agent is inside of
it. We provide further details on the implementation in Appendix D. Additionally, we ran experiments
with Gaussian Process models for the swarm motion problem to showcase the flexibility of design choices in
M3–UCRL. Details and results are described in Appendix E.2.
Laurière et al. (2022) shows that exploration is not trivial in the discrete state and action space equivalent
of this environment by considering the uniform random policy which fails to reach the uniform distribution
within the episode. This result holds in the continuous environment as well as shown by Fig. 1b.
Results. Due to the lack of an analytical solution to the problem, we compare the rewards achieved by
M3–UCRL in the policy roll-outs (i.e., the interactions with the environment) to the rewards achieved by
9Published in Transactions on Machine Learning Research (05/2023)
the policy optimised via the back-propagation through time (BPTT) algorithm with knowndynamics. We
denotethispolicyby πBPTTandfindittobeadequatesincewhenitisused, thepopulation’sentropyquickly
reaches the maximum achievable corresponding to a uniform distribution. We provide further analysis of
this policy in Appendix D.4.
Fig. 2a shows the rewards achieved by M3–UCRL over 30 episodes. The learning can be characterized by
two phases; in the first 10episodes M3–UCRL learns how to navigate the whole state-space, after which it
reduces exploration and focuses more on fine-grained improvements. Learning to navigate the environment
is challenging due to the precise movements needed to enter the corridors, especially, from an angle. Small
changes in the action can decide whether the representative agent moves through the corridor to another
room or is stopped by a wall. As shown by the swift improvement in M3–UCRL’s performance, it successfully
accumulates information in the first phase of the learning process in order to have sufficiently precise policies
that can navigate the agent population through the corridors. In the second phase, it gradually refines
its policy further to distribute the agents among the rooms quicker and achieve a much closer to uniform
distribution as shown on Fig. 2b. The optimised policy after 30episodes achieves a reward of 77.86compared
to78.26that ofπBPTT, i.e., less than 1%difference.2
While M3–UCRL converges within 30 episodes, a comparable tabular Q-learning algorithm for the Mean-
Field Control problem fails to achieve similar performance in more than 10 million training episodes in the
discrete problem defined in Geist et al. (2021) and Laurière et al. (2022). This is most likely due to the
large state space, the limitation of deterministic actions, and challenging exploration in the environment.
We provide further details of the comparison and why Q-learning requires long training for convergence in
Appendix E.1.
6 Conclusion
Inthiswork, weproposethefirstmodel-basedalgorithmfortheMean-FieldControlproblem. TheM3–UCRL
algorithm runs in episodes, builds optimistic performance estimates, and uses them to efficiently explore the
space during policy learning. We proved general regret bounds and showed how they can be specialized to
Gaussian Process models. Furthermore, our experiments showcase the practicality of the algorithm and how
it can be easily combined with deep Neural Networks. We believe that our approach provides an important
step towards making model-based MARL algorithms more principled, scalable, and sample-efficient. We see
the following topics important for the further progression of the model-based mean-field learning for both the
mean-fieldcontrolandmean-fieldgameproblems: 1)PlanningwithknowndynamicstosolveEq.(5)similarly
to Gu et al. (2020); Motte & Pham (2019), 2) Eliminating the need of recalibration to have Neural Networks
satisfying Assumption 3, 3) Maximum mutual information bounds for kernels defined on probability spaces.
Acknowledgments
This project has received funding from the European Research Council (ERC) under the European Unions
Horizon 2020 research and innovation program grant agreement No 815943 and ETH Zurich Postdoctoral
Fellowship 19-2 FEL-47. This publication was made possible by an ETH AI Center doctoral fellowship to
Barna Pasztor.
References
Yasin Abbasi-Yadkori and Csaba Szepesvaŕi. Regret bounds for the adaptive control of linear quadratic
systems. Journal of Machine Learning Research , 19:1–26, 2011.
Mridul Agarwal, Vaneet Aggarwal, Arnob Ghosh, and Nilay Tiwari. Reinforcement learning for mean field
game.arXiv preprint arXiv:1905.13357 , 2019.
Noha Almulla, Rita Ferreira, and Diogo Gomes. Two numerical approaches to stationary mean-field games.
Dynamic Games and Applications , 7(4):657–682, 2017.
2Animations of episodes with the BPTT policy ( known_dynamics_animation.mp4 ) and the M3–UCRL policy
(m3_ucrl_animation_episode_t.mp4 ) are included in the supplementary material. Episode 1to10illustrate the quick learning
during the exploration phase while episode 26is the best-performing one also used for Fig. 2b.
10Published in Transactions on Machine Learning Research (05/2023)
Andrea Angiuli, Jean-Pierre Fouque, and Mathieu Lauriere. Reinforcement Learning for Mean Field Games,
with Applications to Economics. arXiv preprint arXiv:2106.13755 , 2021.
Andrea Angiuli, Jean-Pierre Fouque, and Mathieu Laurière. Unified reinforcement q-learning for mean field
game and control problems. Mathematics of Control, Signals, and Systems , pp. 1–55, 2022.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations . Cambridge Uni-
versity Press, 2009.
Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. In
Advances in Neural Information Processing Systems , 2009.
Nicole Bäuerle. Mean Field Markov Decision Processes. arXiv preprint arXiv:2106.08755 , 2021.
Alain Bensoussan, Jens Frehse, Phillip Yam, et al. Mean field games and mean field type control theory ,
volume 101. Springer, 2013.
Ronen I Brafman and Moshe Tennenholtz. A near-optimal polynomial time algorithm for learning in certain
classes of stochastic games. Artificial Intelligence , 121(1-2):31–47, 2000.
Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-optimal
reinforcement learning. Journal of Machine Learning Research , 3:213–231, 2002.
LucianBuşoniu,RobertBabuška,andBartDeSchutter. Acomprehensivesurveyofmultiagentreinforcement
learning. IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews , 38(2):
156–172, 2008.
Pierre Cardaliaguet and Saeed Hadikhanloo. Learning in mean field games: the fictitious play. ESAIM:
Control, Optimisation and Calculus of Variations , 23(2):569–591, 2017.
René Carmona and François Delarue. Probabilistic analysis of mean-field games. SIAM Journal on Control
and Optimization , 51(4):2705–2734, 2013.
René Carmona, François Delarue, et al. Probabilistic Theory of Mean Field Games with Applications I-II .
Springer, 2018.
René Carmona, Mathieu Laurière, and Zongjun Tan. Linear-quadratic mean-field reinforcement learning:
convergence of policy gradient methods. arXiv preprint arXiv:1910.04295 , 2019a.
René Carmona, Mathieu Laurière, and Zongjun Tan. Model-free mean-field reinforcement learning: mean-
field mdp and mean-field q-learning. arXiv preprint arXiv:1910.12802 , 2019b.
René Carmona, Kenza Hamidouche, Mathieu LauriÉre, and Zongjun Tan. Linear-quadratic zero-sum mean-
field type games: Optimality conditions and policy optimization. Journal of Dynamics and Games. 2021,
Volume 8, Pages 403-443 , 8(4):403, 2021. ISSN 21646074.
Cristiano Castelfranchi. The theory of social functions: challenges for computational social science and
multi-agent learning. Cognitive Systems Research , pp. 5–38, 2001.
Minshuo Chen, Yan Li, Ethan Wang, Zhuoran Yang, Zhaoran Wang, and Tuo Zhao. Pessimism Meets Invari-
ance: Provably Efficient Offline Mean-Field Multi-Agent RL. Advances in Neural Information Processing
Systems, 34:17913–17926, 12 2021.
Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International Conference
on Machine Learning , 2017.
Sayak Ray Chowdhury and Aditya Gopalan. Online learning in kernelized Markov decision processes. In
Proceedings of Machine Learning Research , volume 89, pp. 3197–3205, 2019.
11Published in Transactions on Machine Learning Research (05/2023)
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in
a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing
Systems, pp. 4754–4765, 2018.
Sebastian Curi, Felix Berkenkamp, and Andreas Krause. Efficient model-based reinforcement learning
through optimistic policy search and planning. In Advances in Neural Information Processing Systems ,
2020.
Omar Darwiche Domingues, Pierre Ménard, Matteo Pirotta, Emilie Kaufmann, and Michal Valko. Regret
bounds for kernel-based reinforcement learning. arXiv preprint arXiv:2004.05599 , 2020.
Audrey Durand, Odalric-Ambrym Maillard, and Joelle Pineau. Streaming kernel regression with provably
adaptive mean, variance, and regularization. The Journal of Machine Learning Research , 2018.
Yonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, and Shie Mannor. Tight regret bounds for
model-based reinforcement learning with greedy policies. In Advances in Neural Information Processing
Systems, pp. 12224–12234, 2019.
S. El-Tantawy, B. Abdulhai, and H. Abdelgawad. Multiagent reinforcement learning for integrated network
of adaptive traffic signal controllers (marlin-atsc): Methodology and large-scale application on downtown
toronto. IEEE Transactions on Intelligent Transportation Systems , 14(3):1140–1150, 2013.
Romuald Elie, Julien Pérolat, Mathieu Laurière, Matthieu Geist, and Olivier Pietquin. On the convergence
of model free learning in mean field games. AAAI Conference on Artificial Intelligence , 34(05):7143–7150,
2020.
Zuyue Fu, Zhuoran Yang, Yongxin Chen, and Zhaoran Wang. Actor-critic provably finds nash equilibria
of linear-quadratic mean-field games. In International Conference on Learning Representations . OpenRe-
view.net, 2020.
Nicolas Gast, Bruno Gaujal, and Jean-Yves Le Boudec. Mean field for Markov decision processes: from
discrete to continuous optimization. IEEE Transactions on Automatic Control , pp. 2266–2280, 2012.
MatthieuGeist, JulienPérolat, MathieuLaurière, RomualdElie, SarahPerrin, OlivierBachem, RémiMunos,
and Olivier Pietquin. Concave Utility Reinforcement Learning: the Mean-Field Game Viewpoint. Pro-
ceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS ,
1:489–497, 6 2021. ISSN 15582914.
Diogo A Gomes et al. Mean field games models—a brief survey. Dynamic Games and Applications , 4(2):
110–154, 2014.
HaotianGu, XinGuo, XiaoliWei, andRenyuanXu. Dynamicprogrammingprinciplesformean-fieldcontrols
with learning. arXiv preprint arXiv:1911.07314 , 2020.
HaotianGu, XinGuo, XiaoliWei, andRenyuanXu. Mean-fieldcontrolswithq-learningforcooperativemarl:
Convergence and complexity analysis. SIAM Journal on Mathematics of Data Science , 3(4):1168–1196,
2021.
Olivier Guéant, Jean-Michel Lasry, and Pierre-Louis Lions. Mean field games and applications. In Paris-
Princeton lectures on mathematical finance 2010 , pp. 205–266. Springer, 2011.
Xin Guo, Anran Hu, Renyuan Xu, and Junzi Zhang. Learning mean-field games. In Advances in Neural
Information Processing Systems , volume 32, pp. 4966–4976, 2019.
Xin Guo, Anran Hu, Renyuan Xu, and Junzi Zhang. A general framework for learning mean-field games.
arXiv preprint arXiv:2003.06069 , 2020.
Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A survey of learning
in multiagent environments: Dealing with non-stationarity. arXiv preprint arXiv:1707.09183 , 2017.
12Published in Transactions on Machine Learning Research (05/2023)
Pablo Hernandez-Leal, Bilal Kartal, and Matthew E. Taylor. A survey and critique of multiagent deep
reinforcement learning. Autonomous Agents and Multi-Agent Systems , 33(6):750–797, 2019.
Minyi Huang, Roland P Malhamé, Peter E Caines, et al. Large population stochastic dynamic games: closed-
loop mckean-vlasov systems and the nash certainty equivalence principle. Communications in Information
& Systems , 6(3):221–252, 2006.
Minyi Huang, Peter E Caines, and Roland P Malhamé. Large-population cost-coupled lqg problems with
nonuniform agents: individual-mass behavior and decentralized ϵ-equilibria. IEEE Transactions on Auto-
matic Control , 52(9):1560–1571, 2007.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning.
Journal of Machine Learning Research , 2010.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with
linear function approximation. In Conference on Learning Theory , pp. 2137–2143, 2020.
Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun. Information theo-
retic regret bounds for online nonlinear control. arXiv preprint arXiv:2006.12466 , 2020.
Leonid Kantorovich and Gennady S. Rubinstein. On a space of totally additive functions. Vestnik Leningrad.
Univ, 13:52–59, 1958.
Andreas Krause and Cheng Soon Ong. Contextual Gaussian process bandit optimization. Advances in
Neural Information Processing Systems , 2011.
Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning using
calibrated regression. In International Conference on Machine Learning , pp. 2796–2804, 2018.
Daniel Lacker. Limit theory for controlled mckean–vlasov dynamics. SIAM Journal on Control and Opti-
mization , 55(3):1641–1672, 2017.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncer-
tainty estimation using deep ensembles. In Advances in Neural Information Processing Systems , 2017.
Jean-Michel Lasry and Pierre-Louis Lions. Jeux à champ moyen. i–le cas stationnaire. Comptes Rendus
Mathématique , 343(9):619–625, 2006a.
Jean-Michel Lasry and Pierre-Louis Lions. Jeux à champ moyen. ii–horizon fini et contrôle optimal. Comptes
Rendus Mathématique , 343(10):679–684, 2006b.
Mathieu Laurière, Sarah Perrin, Matthieu Geist, and Olivier Pietquin. Learning Mean Field Games: A
Survey.arXiv preprint arXiv:2205.12944v2 , 2022.
J. W. Lee, J. Park, J. O, J. Lee, and E. Hong. A multiagent approach to q-learning for daily stock trading.
IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans , pp. 864–877, 2007.
Jae Won Lee, Byoung-Tak Zhang, et al. Stock trading system using reinforcement learning with cooperative
agents. In International Conference on Machine Learning , pp. 451–458, 2002.
Charles-Albert Lehalle and Charafeddine Mouzouni. A mean field game of portfolio trading and its conse-
quences on perceived correlations. arXiv preprint arXiv:1902.09606 , 2019.
Joel Z. Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. Multi-agent rein-
forcement learning in sequential social dilemmas. In Conference on Autonomous Agents and MultiAgent
Systems, pp. 464–473, 2017.
Yan Li, Lingxiao Wang, Jiachen Yang, Ethan Wang, Zhaoran Wang, Tuo Zhao, and Hongyuan Zha. Permu-
tation Invariant Policy Optimization for Mean-Field Multi-Agent Reinforcement Learning: A Principled
Approach. arXiv preprint arXiv:2105.08268 , 2021.
13Published in Transactions on Machine Learning Research (05/2023)
Teodor Mihai Moldovan, Sergey Levine, Michael I Jordan, and Pieter Abbeel. Optimism-driven exploration
for nonlinear systems. In IEEE International Conference on Robotics and Automation , 2015.
Médéric Motte and Huyên Pham. Mean-field Markov decision processes with common noise and open-loop
controls. arXiv preprint arXiv:1912.07883 , 2019.
Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. Deep reinforcement learning for multiagent
systems: A review of challenges, solutions, and applications. IEEE transactions on cybernetics , 50(9):
3826–3839, 2020.
Afshin OroojlooyJadid and Davood Hajinezhad. A review of cooperative multi-agent deep reinforcement
learning. arXiv preprint arXiv:1908.03963 , 2019.
Jakub Pachocki, Greg Brockman, Jonathan Raiman, Susan Zhang, Henrique Pondé, Jie Tang, Filip Wolski,
Christy Dennison, Rafal Jozefowicz, Przemyslaw Debiak, et al. Openai five. URL https://blog. openai.
com/openai-five, 2018.
Victor M. Panaretos and Yoav Zemel. Statistical Aspects of Wasserstein Distances. Annual Review of
Statistics and Its Application , 6(1):405–431, 2019. ISSN 2326-8298.
Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer School on Machine Learning ,
2003.
Jonathan Scarlett, Ilija Bogunovic, and Volkan Cevher. Lower bounds on regret for noisy Gaussian process
bandit optimization. In Satyen Kale and Ohad Shamir (eds.), Proceedings of the 2017 Conference on
Learning Theory , volume 65 of Proceedings of Machine Learning Research , pp. 1723–1742. PMLR, 07–10
Jul 2017.
Pier Giuseppe Sessa, Maryam Kamgarpour, and Andreas Krause. Efficient Model-based Multi-agent Rein-
forcement Learning via Optimistic Equilibrium Computation, 2022. ISSN 2640-3498.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for
autonomous driving. arXiv preprint arXiv:1610.03295 , 2016.
Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in
the bandit setting: No regret and experimental design. In International Conference on Machine Learning ,
pp. 1015–1022, 2010.
Ingo Steinwart and Christmann Andreas. Support vector machines. In Climate Change 2013 - The Physical
Science Basis . Springer, 2008. ISBN 978-0-387-77241-7.
Jayakumar Subramanian and Aditya Mahajan. Reinforcement learning in stationary mean-field games. In
International Conference on Autonomous Agents and MultiAgent Systems , pp. 251–259, 2019.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,
David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using
multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.
Lingxiao Wang, Zhuoran Yang, and Zhaoran Wang. Breaking the curse of many agents: Provable mean
embedding q-iteration for mean-field reinforcement learning. In International Conference on Machine
Learning , pp. 10092–10103, 2020.
Weichen Wang, Jiequn Han, Zhuoran Yang, and Zhaoran Wang. Global Convergence of Policy Gradient for
Linear-Quadratic Mean-Field Control/Game in Continuous Time, 2021. ISSN 2640-3498.
Pierre Weiss. L’hypothèse du champ moléculaire et la propriété ferromagnétique. J. Phys. Theor. Appl. , 6
(1):661–690, 1907.
Jiachen Yang, Xiaojing Ye, Rakshit Trivedi, Huan Xu, and Hongyuan Zha. Learning deep mean field games
for modeling large population behavior. International Conference on Learning Representations , pp. 1–15,
2017.
14Published in Transactions on Machine Learning Research (05/2023)
Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multi-agent
reinforcement learning. In International Conference on Machine Learning , pp. 5571–5580, 2018.
Huibing Yin, Prashant G Mehta, Sean P Meyn, and Uday V Shanbhag. Learning in mean-field oscillator
games. In 49th IEEE Conference on Decision and Control (CDC) , pp. 3125–3132, 2010.
Huibing Yin, Prashant G Mehta, Sean P Meyn, and Uday V Shanbhag. Learning in mean-field games. IEEE
Transactions on Automatic Control , 59(3):629–644, 2013.
Kaiqing Zhang, Zhuoran Yang, and Tamer Başar. Multi-agent reinforcement learning: A selective overview
of theories and algorithms. arXiv preprint arXiv:1911.10635 , 2019.
15Published in Transactions on Machine Learning Research (05/2023)
A Preliminaries on the Wasserstein Distance
In this section, we provide a brief overview on the Wasserstein distance and its properties used in the proof
of our general regret bound in Theorem 1.
Definition 1 (Wasserstein Distance) .Letµandνbe two probability distributions on Rd, then fork∈
{1,2,...}the Wasserstein distance of order kis defined by
Wk(µ,ν) =/bracketleftbigg
inf
γ/integraldisplay
Ω×Ω∥x−y∥kdγ(x,y)/bracketrightbigg1/k
where the infimum is taken over all joint distributions γwith marginals µ,ν. Fork= 1, a standard dual
formulation derived by Kantorovich & Rubinstein (1958) is given by:
W1(µ,ν) = sup
f∈F/braceleftig/integraldisplay
f(x)d(µ−ν)(x)/bracerightig
(8)
whereF={f:Rd→Rsuch that|f(x)−f(y)|≤∥x−y∥ ∀x,y∈Rd}. Both formulations above gen-
eralises to laws defined on more general space, e.g., complete and separable metric spaces and infinite-
dimensional function spaces such as L2[0,1].
The Wasserstein distance is usually interpreted as the minimum transportation distance between the distri-
butionsµandν. For random variables X∼µandY∼ν, we use the notations Wk(X,Y )andWk(µ,ν)
interchangeably. Assuming that Wk(X,Y )is finite, i.e., E[∥X∥k] +E[∥Y∥k]<∞, the distance obeys the
following properties Panaretos & Zemel (2019):
•Form≤n, by Jensen’s Inequality, we have:
Wm(X,Y )≤Wn(X,Y ) (9)
•Fora∈R, it holds
Wk(aX,aY ) =|a|Wk(X,Y ) (10)
•Forx∈Rp, it holds
Wk(X+x,Y+x) =Wk(X,Y ) (11)
•Forx∈Rp, we have that
W2
2(X+x,Y) =∥x+E[X]−E[Y]∥2+W2
2(X,Y ) (12)
Corollary 1. LetX,Ybe independent identically distributed random variables on RpwithE[∥X∥k]<∞
andx,y∈Rp. Then,
W1(X+x,Y+y)≤∥x−y∥2
Proof.
W2
2(X+x,Y+y) =W2
2(X+x−y,Y) ByEq.(11)
=∥x−y+E[X]−E[Y]∥2
2+W2
2(X,Y ) ByEq.(12)
=∥x−y∥2
2 XandYare i.i.d.
Therefore,
W1(X+x,Y+y)≤W2(X+x,Y+y) ByEq.(9)
≤∥x−y∥2
16Published in Transactions on Machine Learning Research (05/2023)
B Regret Bound Proof
In the following subsections, we present the theoretical analysis that leads to the results of Theorem 1.
B.1 Measuring performance under arbitrary transition function
Before we start proving the main theorem, we introduce the following notation of ˜J(π,˜f)to measure the
performance of a policy πunder the transition function ˜f:
˜J(π,˜f) :=E/bracketleftiggH−1/summationdisplay
h=0r(˜sh,˜ah,˜µh)/bracketrightigg
(13a)
s.t. ˜ah=πh(˜sh,˜µh) (13b)
˜sh+1=˜f(˜sh,˜ah,˜µh) + ˜ωh (13c)
˜µh+1= Φ(˜µh,πh,˜f), (13d)
where the expectation in Eq. (13a) is taken with respect to initial state distribution µ0and noise in
transitions. We note that we can rewrite the optimization problems in Eq. (4) and Eq. (5) such that
π∗∈arg maxπ∈ΠJ(π) = arg maxπ∈Π˜J(π,f)andπt= arg maxπ∈Πmax ˜f∈Ft−1˜J(π,˜f), respectively.
B.2 Measure Formulation
First, we provide the proof of Lemma 1. Then, we reformulate the optimization problems in Eq. (4) and
Eq. (5a) as a Markov Decision Process on a probability measure space with transition dynamics defined in
Lemma 1. The following proof follows Lemma 2.2 in Gu et al. (2020). We are stating it here for the sake of
completeness.
Proof of Lemma 1. Fixπand letφbe a bounded measurable function on S. Recall that shdenotes random
variable of the representative agent’s state at time tand, by the definition of µh,sh∼µh.
First note that
Esh+1[φ(sh+1)] =/integraldisplay
s′∈Sµh+1(ds′)φ(s′).
On the other hand, by the law of iterated conditional expectation,
Esh+1[φ(sh+1)] =Es0,...,sh/bracketleftig
Esh+1[φ(sh+1)|s0,...,sh]/bracketrightig
=Es0,...,sh/bracketleftigg/integraldisplay
s′∈Sφ(s′)P[sh+1∈ds′|s0,...,sh]/bracketrightigg
=Es0,...,sh/bracketleftigg/integraldisplay
s′∈Sφ(s′)P[f(sh,πh(sh,µh),µh) +ωh∈ds′]/bracketrightigg
=/integraldisplay
s′∈Sφ(s′)Es0,...,sh/bracketleftig
P[f(sh,πh(sh,µh),µh) +ωh∈ds′]/bracketrightig
=/integraldisplay
s′∈Sφ(s′)/integraldisplay
s∈Sµh(ds)P[f(s,πh(s,µh),µh) +ωh∈ds′].
In the third equality, we used the dynamics of the system Eq. (1) which states that sh+1is determined by
sh,ah,µhassh+1∼f(sh,ah,µh) +ωh. The last equality follows from µh(ds) =P[sh∈ds]. Therefore,
/integraldisplay
s′∈Sµh+1(ds′)φ(s′) =/integraldisplay
s′∈Sφ(s′)/integraldisplay
s∈Sµh(ds)P[f(s,πh(s,µh),µh) +ωh∈ds′],
17Published in Transactions on Machine Learning Research (05/2023)
which implies the desired results
µh+1(ds′) =/integraldisplay
s∈Sµh(ds)P[f(s,πh(s,µh),µh) +ωh∈ds′].
Remark 2. The relationship described in Lemma 1 is the discrete-time equivalent of the Fokker-Planck
(Kolmogorov forward) equation for the McKean-Vlasov Process. While we provided a proof for the dynamics
in Eq.(1), it holds for any other functions ˜ft∈Ftfort= 1,2,....
Now, we consider the objective under the true dynamics, Eq. (4). The following lemma is based on Lemma
2.1. in Gu et al. (2020).
Lemma 2. For any policy π= (π0,...,πH−1),
J(π) =H−1/summationdisplay
h=0ˆr(µh,πh),
where ˆris the integrated reward function defined as
ˆr(µ,π) =/integraldisplay
Sµ(ds)r(s,π(s,µ),µ), (14)
andµhis the mean-field distribution induced by the policy πfollowing the dynamics defined in Eq. (2).
Proof.
J(π) =Es0∼µ0,ω0,...,ωH−1/bracketleftiggH−1/summationdisplay
h=0r(sh,πh(sh,µh),µh)/bracketrightigg
=H−1/summationdisplay
h=0/bracketleftbig
Es0∼µ0,ω0,...,ωh−1r(sh,πh(sh,µh),µh)/bracketrightbig
=H−1/summationdisplay
h=0/integraldisplay
SP[sh∈ds]r(s,πh(s,µh),µh)
=H−1/summationdisplay
h=0/integraldisplay
Sµh(ds)r(s,πh(s,µh),µh)
=H−1/summationdisplay
h=0ˆr(µh,πh).
The expectation in the first line is taken over the random variables s0,ω0,...,ωH−1that induce the state
trajectorys0,s1,...,sH. In particular, for a fixed policy the continuous random variable shis a func-
tion ofs0,ω0,...,ωh−1. In the second line, we use the linearity of expectation and sh’s independence of
ωh,...,ωH−1. The third line follows from the definition of expectation.
Note that the fixed and known initial distribution µ0, the true transition function f, and the fixed policy
πdefine the mean-field distributions µh, for all 0≤h≤Hvia Eq. (2). The fourth equality follows from
Lemma 1, i.e., µh(ds) =P[sh∈ds].
Corollary 2. The arguments in Lemma 2 apply in the case when the transition function of the system is
unknown but approximated via ˜f. We can rewrite ˜J(π,˜f)defined in Eq. (13a)as
˜J(π,˜f) =H−1/summationdisplay
h=0ˆr(˜µt,h,πh).
where ˜µt,his the mean-field distribution induced by the policy πunder the estimated transition function ˜f.
18Published in Transactions on Machine Learning Research (05/2023)
Corollary 3. By using the objective formulation from Lemma 2, we can restate the optimization problem
Eq.(4)as
max
πJ(π) =H−1/summationdisplay
h=0ˆr(µh,πh) (15)
s.t.µh+1= Φ(µh,πh,f), (16)
where Φis defined in Eq. (2). The same holds for the optimization under any other transition function ˜fin
Eq.(5).
Remark 3. Corollary 3 turns the optimization problem Eq. (4)into a Markov Decision Process where
the state space is the space of probability measures P(S)and the action space is the space of functions
Π ={π:S×P (S)→A}. This reformulation comes with a certain trade-off: P(S)andΠare more complex
thanS⊂RqandA⊂Rpbut, in contrast to Eq. (4), the transition function in Eq. (16)is deterministic.
B.3 Proof of Theorem 1
As described in Section 3, at the beginning of episode t, the representative agent optimizes over both the
set of admissible policies, Π, and plausible transition functions, Ft−1, satisfying Assumptions 1 to 3. In the
following, we denote the transition function corresponding to the optimal policy by ˜ft−1∈Ft−1, i.e.,
πt,˜ft−1= arg max
π,˜f˜J(π,˜f).
Lemma 3. Conditioning on the event in Assumption 3 holding true in episode t, the following holds for
episodic regret:
rt=J(π∗)−J(πt)≤˜J(πt,˜ft−1)−J(πt). (17)
Proof.Recall thatπ∗denotes the socially optimal policy from Eq. (4) and let ˜f∗
t−1= arg max ˜f∈Ft−1˜J(π∗,˜f).
The well-calibrated property from Assumption 3 implies that the true transition function fis in the set of
functionsFt−1meaning that the true dynamics of the system fhas been considered in Eq. (5) at the
beginning of episode t. Then, since we provide additional flexibility to the optimization, we have
˜J(π∗,˜f∗
t−1)≥J(π∗)
Using this inequality we can bound the episodic regret as follows:
rt=J(π∗)−J(πt)≤˜J(π∗,˜f∗
t−1)−J(πt)
≤˜J(πt,˜ft−1)−J(πt),
where the last inequality comes from the fact that M3–UCRL selects the policy πtaccording to (πt,˜ft−1) =
arg maxπ,˜f˜J(π,˜f).
Lemma 4. Under Assumptions 2 to 3 and conditioning on that the event defined in Assumption 3 holds
true, it follows for all t≥1:
rt≤˜J(πt,˜ft−1)−J(πt)≤2Lr(1 +Lπ)H−1/summationdisplay
h=1W1(˜µt,h,µt,h), (18)
whereµt,his the mean-field distribution observed in the true unknown environment with dynamics fas in
Eq.(3)when agents follow πtand˜µt,his the mean-field distribution under the approximated dynamics ˜ft−1
as in Eq. (13a).
Proof.By Lemma 3, we have that
rt≤˜J(πt,˜ft−1)−J(πt).
19Published in Transactions on Machine Learning Research (05/2023)
By using Lemma 2, Corollary 2 and the triangle inequality we get
|˜J(πt,˜ft−1)−J(πt)|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleH−1/summationdisplay
h=0ˆr(˜µt,h,πt,h)−H−1/summationdisplay
h=0ˆr(µt,h,πt,h)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤H−1/summationdisplay
h=0|ˆr(˜µt,h,πt,h)−ˆr(µt,h,πt,h)|.
Consider a fixed h∈{0,...,H−1}, then
|ˆr(˜µt,h,πt,h)−ˆr(µt,h,πt,h)|
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Sr(s,πt,h(s,˜µt,h),˜µt,h)˜µt,h(ds)−/integraldisplay
Sr(s,πt,h(s,µt,h),µt,h)µt,h(ds)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleFromEq.(14)
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Sr(s,πt,h(s,˜µt,h),˜µt,h)˜µt,h(ds)−/integraldisplay
Sr(s,πt,h(s,µt,h),µt,h)˜µt,h(ds)
+/integraldisplay
Sr(s,πt,h(s,µt,h),µt,h)˜µt,h(ds)−/integraldisplay
Sr(s,πt,h(s,µt,h),µt,h)µt,h(ds)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤/integraldisplay
S|r(s,πt,h(s,˜µt,h),˜µt,h)−r(s,πt,h(s,µt,h),µt,h))|˜µt,h(ds) (19)
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Sr(s,πt,h(s,µt,h),µt,h)(˜µt,h(ds)−µt,h(ds))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle, (20)
where the last inequality follows from the triangle inequality and the inequality for the absolute value of
definite integrals.
We start by considering the integrand of the term in Eq. (19)
|r(s,πt,h(s,˜µt,h),˜µt,h)−r(s,πt,h(s,µt,h),µt,h)|
≤Lr(∥s−s∥2+∥πt,h(s,˜µt,h)−πt,h(s,µt,h)∥2+W1(˜µt,h,µt,h))
≤Lr(Lπ(∥s−s∥2+W1(˜µt,h,µt,h)) +W1(˜µt,h,µt,h))
≤Lr(1 +Lπ)W1(˜µt,h,µt,h), (21)
where the inequalities follow from Assumption 2.
Next, we consider the function g(s) =1
Lr(1+Lπ)r(s,πt,h(s,µt,h),µt,h)
|g(x)−g(y)|=1
Lr(1 +Lπ)|r(x,πt,h(x,µt,h),µt,h)−r(y,πt,h(y,µt,h),µt,h)|
≤Lr
Lr(1 +Lπ)(∥x−y∥2+∥πt,h(x,µt,h)−πt,h(y,µt,h)∥2+W1(µt,h,µt,h)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=0)
≤Lr
Lr(1 +Lπ)(∥x−y∥2+Lπ(∥x−y∥2+W1(µt,h,µt,h)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=0))
=∥x−y∥2,
where Assumption 2 has been applied in the second and third lines. Therefore, gis a Lipschitz-1 function.
20Published in Transactions on Machine Learning Research (05/2023)
We bound the second term in Eq. (20) by using the previously defined function g
/integraldisplay
Sr(s,πt,h(s,µt,h),µt,h)(˜µt,h(ds)−µt,h(ds))
=Lr(1 +Lπ)/integraldisplay
S1
Lr(1 +Lπ)r(s,πt,h(s,µt,h),µt,h)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
=g(s)(˜µt,h(ds)−µt,h(ds))
≤Lr(1 +Lπ)W1(˜µt,h,µt,h). (22)
The last inequality comes from the Kantorovich-Rubinstein dual formulation of the Wasserstein-1 distance
in Eq. (8). Using the inequalities in Eq. (21) and Eq. (22) for Eq. (19) and Eq. (20), respectively, we get
that
|ˆr(˜µt,h,πt,h)−ˆr(µt,h,πt,h)|≤2Lr(1 +Lπ)W1(˜µt,h,µt,h).
Summing it up from h= 1toh=H−1, we obtain the desired result:
|˜J(πt,˜ft−1)−J(πt)|≤2Lr(1 +Lπ)H−1/summationdisplay
h=1W1(˜µt,h,µt,h).
We note that h= 0is removed from the sum since the initial distribution is fixed, ˜µt,0=µt,0=µ0, therefore
W1(˜µt,0,µt,0) = 0.
Lemma 5. Under Assumptions 1 to 4, and assuming that event in Assumption 3 holds true, and for
h∈{1,...,H}andt≥1and fixed policy πt, it holds:
W1(˜µt,h,µt,h)≤2βt−1Lh−1
t−1h−1/summationdisplay
i=0/integraldisplay
S∥σt−1(s,πt,hh(s,µt,i),µt,i)∥2µt,i(ds), (23)
whereLt−1= 1 + 2(1 + Lπ) [Lf+ 2βt−1Lσ].
Proof.To simplify the notation when it comes to both fand ˜ft−1, we omit the action variable as it is deter-
mined viaπ,µand the current state, i.e., f(s,µ) :=f(s,πt(s,µ),µ)and similarly for ˜ft−1. Furthermore,
we fixt≥1.
First, we establish a relationship between W1(˜µt,h+1,µt,h+1)andW1(˜µt,h,µt,h), i.e., the change in the
Wasserstein distance between the state distributions under the true dynamics and the approximated dynam-
ics˜ft−1. Then, we will use this relationship and the fact that the initial distribution is fixed, i.e., µt,0= ˜µt,0,
to bound the distance at time h.
First, we rewrite W1(˜µt,h+1,µt,h+1)in terms of πt,h,˜µt,handµt,h, use the triangle inequality for the
Wasserstein-1 distance, and use the Kantorovich and Rubinstein formulation from Eq. (8).
W1(˜µt,h+1,µt,h+1)
=W1(Φ(πt,h,˜µt,h,˜ft−1),Φ(πt,h,µt,h,f))
≤W1(Φ(πt,h,˜µt,h,˜ft−1),Φ(πt,h,µt,h,˜ft−1)) +W1(Φ(πt,h,µt,h,˜ft−1),Φ(πt,h,µt,h,f))
= sup
v:Lip(v)≤1/integraldisplay
s′∈Sv(s′)/parenleftigg/integraldisplay
s∈SP[˜ft−1(s,˜µt,h) + ˜ωt,h∈ds′](˜µt,h(ds)−µt,h(ds))/parenrightigg
(24)
+ sup
v:Lip(v)≤1/integraldisplay
s′∈Sv(s′)/parenleftigg/integraldisplay
s∈S/parenleftbig
P[˜ft−1(s,˜µt,h) + ˜ωt,h∈ds′]−P[f(s,µt,h) +ωt,h∈ds′]/parenrightbig
µt,h(ds)/parenrightigg
.(25)
Next, we consider terms from Eq. (24) and Eq. (25) separately, and obtain upper-bounds for both of them
in terms of W1(˜µt,h,µt,h)andσt−1(s,πt,h(s,˜µt,h),˜µt,h).
21Published in Transactions on Machine Learning Research (05/2023)
Step 1: Bounding Eq. (25) First, we fix vsuch thatLip(v)≤1. Then, consider Eq. (25)
/integraldisplay
s′∈Sv(s′)/parenleftigg/integraldisplay
s∈S/parenleftbig
P[˜ft−1(s,˜µt,h) + ˜ωt,h∈ds′]−P[f(s,µt,h) +ωt,h∈ds′]/parenrightbig
µt,h(ds)/parenrightigg
=/integraldisplay
s∈S/integraldisplay
s′∈Sv(s′)/bracketleftig
P[˜ft−1(s,˜µt,h) + ˜ωt,h∈ds′]−P[f(s,µt,h) +ωt,h∈ds′]/bracketrightig
µt,h(ds)Fubini’s thm.
≤/integraldisplay
s∈SW1(˜ft−1(s,˜µt,h) + ˜ωt,h,f(s,µt,h) +ωt,h)µt,h(ds) Eq.(8)
≤/integraldisplay
s∈S/vextenddouble/vextenddouble˜ft−1(s,˜µt,h)−f(s,µt,h)/vextenddouble/vextenddouble
2µt,h(ds). Corollary 1(26)
Note that the state space is compact hence the first moment of ˜µt,h+1andµt,h+1exist and
W1(˜µt,h+1,µt,h+1)<∞, therefore, the integration above is finite and Fubini’s theorem is applicable. Next,
we consider the following two claims.
Claim 5.1. Under Assumption 3 and assuming that the event defined in it holds true, we have that
/vextenddouble/vextenddouble˜ft−1(s,˜µt,h)−f(s,˜µt,h)/vextenddouble/vextenddouble
2≤2βt−1∥σt−1(s,πt,h(s,˜µt,h),˜µt,h)∥2.
Proof.
/vextenddouble/vextenddouble˜ft−1(s,˜µt,h)−f(s,˜µt,h)/vextenddouble/vextenddouble
2
=/vextenddouble/vextenddouble˜ft−1(s,˜µt,h)−mt−1(s,˜µt,h) +mt−1(s,˜µt,h)−f(s,˜µt,h)/vextenddouble/vextenddouble
2
≤/vextenddouble/vextenddouble˜ft−1(s,˜µt,h)−mt−1(s,˜µt,h)/vextenddouble/vextenddouble
2+∥mt−1(s,˜µt,h)−f(s,˜µt,h)∥2
=/radicaltp/radicalvertex/radicalvertex/radicalbtp/summationdisplay
i=1/bracketleftbig˜ft−1(s,˜µt,h)−mt−1(s,˜µt,h)/bracketrightbig2
i+/radicaltp/radicalvertex/radicalvertex/radicalbtp/summationdisplay
i=1[mt−1(s,˜µt,h)−f(s,˜µt,h)]2
i
≤2/radicaltp/radicalvertex/radicalvertex/radicalbtp/summationdisplay
i=1β2
t−1[σt−1(s,πt,h(s,˜µt,h),˜µt,h)]2
i
= 2βt−1∥σt−1(s,πt,h(s,˜µt,h),˜µt,h)∥2
where [·]idenotes the i-th coordinate. The first inequality follows from the triangle inequality, after which,
we used the event defined in Assumption 3 and the fact that ˜ft−1is chosen to satisfy the same condition.
Claim 5.2. Under Assumption 1 and Assumption 2, we have that
∥f(s,˜µt,h)−f(s,µt,h)∥2≤Lf(1 +Lπ)W1(˜µt,hµt,h)).
Proof.From Assumption 1 and Assumption 2, it follows that
∥f(s,˜µt,h)−f(s,µt,h)∥2≤Lf(∥πt,h(s,˜µt,h)−πt,h(s,µt,h)∥2+W1(˜µt,hµt,h))
≤Lf(1 +Lπ)W1(˜µt,hµt,h).
Now, we can combine Claim 5.1 and Claim 5.2 to get
/vextenddouble/vextenddouble˜ft−1(s,˜µt,h)−f(s,µt,h)/vextenddouble/vextenddouble
2(27)
≤/vextenddouble/vextenddouble˜ft−1(s,˜µt,h)−f(s,˜µt,h)/vextenddouble/vextenddouble
2+∥f(s,˜µt,h)−f(s,µt,h)∥2
≤2βt−1∥σt−1(s,πt,h(s,˜µt,h),˜µt,h)∥2+Lf(1 +Lπ)W1(˜µt,h,µt,h). (28)
22Published in Transactions on Machine Learning Research (05/2023)
Substituting back Eq. (28) into Eq. (26) at the beginning of Step 1, we obtain:
/integraldisplay
s′∈Sv(s′)/integraldisplay
s∈S/bracketleftig
P[˜ft−1(s,˜µt,h) + ˜ωt,h∈ds′]−P[f(s,µt,h) +ωt,h∈ds′]/bracketrightig
µt,h(ds)
≤/integraldisplay
s′∈S/vextenddouble/vextenddouble˜ft−1(s,˜µt,h)−f(s,µt,h)/vextenddouble/vextenddouble
2µt,h(ds)
≤Lf(1 +Lπ)W1(˜µt,h,µt,h) + 2βt−1/integraldisplay
s′∈S∥σt−1(s,πt,h(s,˜µt,h),˜µt,h)∥2µt,h(ds). (29)
Therefore, we obtain an upper bound for Eq. (25).
Step 2: Bounding Eq. (24)
Next, we consider the other part, Eq. (24). Again let vbe a fixed function such that Lip(v)≤1. We have:
/integraldisplay
s′∈S/integraldisplay
s∈Sv(s′)P[˜ft−1(s,˜µt,h) + ˜ωt,h∈ds′](˜µt,h(ds)−µt,h(ds))
=/integraldisplay
s∈S/integraldisplay
s′∈Sv(s′)/parenleftig
P[˜ft−1(s,˜µt,h) + ˜ωt,h∈ds′]−P[f(s,˜µt,h) +ωt,h∈ds′]/parenrightig
(˜µt,h(ds)−µt,h(ds))(30)
+/integraldisplay
s∈S/integraldisplay
s′∈Sv(s′)P[f(s,˜µt,h) +ωt,h∈ds′](˜µt,h(ds)−µt,h(ds)). (31)
We use Fubini’s theorem again to change the order of integration, and we consider the inner integration from
Eq. (30):
/integraldisplay
s′∈Sv(s′)/parenleftig
P[˜ft−1(s,˜µt,h) + ˜ωt,h∈ds′]−P[f(s,˜µt,h) +ωt,h∈ds′]/parenrightig
≤W1(˜ft−1(s,˜µt,h) + ˜ωt,h,f(s,˜µt,h) +ωt,h) Eq.(8)
≤/vextenddouble/vextenddouble˜ft−1(s,˜µt,h)−f(s,˜µt,h)/vextenddouble/vextenddouble
2Corollary 1
≤2βt−1∥σt−1(s,πt,h(s,˜µt,h),˜µt,h)∥2. Claim 5.1
Letg(s) =1
Lσ(1+Lπ)∥σt−1(s,πt,h(s,˜µt,h),˜µt,h)∥2, then note that
|g(x)−g(y)|=1
Lσ(1 +Lπ)/vextendsingle/vextendsingle/vextendsingle∥σt−1(x,πt,h(x,˜µt,h),˜µt,h)∥2−∥σt−1(y,πt,h(y,˜µt,h),˜µt,h)∥2/vextendsingle/vextendsingle/vextendsingle
≤1
Lσ(1 +Lπ)∥σt−1(x,πt,h(x,˜µt,h),˜µt,h)−σt−1(y,πt,h(y,˜µt,h),˜µt,h)∥2
≤1
1 +Lπ(∥x−y∥2+∥π(x,˜µt,h)−π(y,˜µt,h)∥2) Assumption 4
≤∥x−y∥2, Assumption 2
where the first inequality follows from the reverse triangle inequality. Therefore, we can bound Eq. (30) as
/integraldisplay
s∈S/integraldisplay
s′∈Sv(s′)/parenleftigg
P[˜ft−1(s,˜µt,h) + ˜ωt,h∈ds′]−P[f(s,µt,h) +ωt,h∈ds′]/parenrightigg
(˜µt,h(ds)−µt,h(ds))
≤2βt−1/integraldisplay
s∈S∥σt−1(s,πt,h(s,˜µt,h),˜µt,h)∥2(˜µt,h(ds)−µt,h(ds))
= 2βt−1Lσ(1 +Lπ)/integraldisplay
s∈Sg(s)(˜µt,h(ds)−µt,h(ds))
≤2βt−1Lσ(1 +Lπ)W1(˜µt,h,µt,h). (32)
The last inequality comes from the dual formulation of the Wasserstein-1 distance Eq. (8) and the previously
shown fact that gis 1-Lipschitz.
23Published in Transactions on Machine Learning Research (05/2023)
Next, we consider the remaining term in Eq. (31), namely,
/integraldisplay
s∈S/integraldisplay
s′∈Sv(s′)P[f(s,˜µt,h) +ωt,h∈ds′](˜µt,h(ds)−µt,h(ds)).
Letq(s) =1
Lf(1+Lπ)/integraltext
s′∈Sv(s′)P[f(s,˜µt,h) +ωt,h∈ds′], then
|q(x)−q(y)|=1
Lf(1 +Lπ)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
s′∈Sv(s′)/parenleftig
P[f(x,˜µt,h) +ωt,h∈ds′]
−P[f(y,˜µt,h) +ωt,h∈ds′]/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
Lf(1 +Lπ)/vextendsingle/vextendsingle/vextendsingleW1(f(x,˜µt,h) +ωt,h,f(y,˜µt,h) +ωt,h)/vextendsingle/vextendsingle/vextendsingle Eq.(8)
≤1
Lf(1 +Lπ)∥f(x,˜µt,h)−f(y,˜µt,h)∥2Corollary 1
≤1
Lf(1 +Lπ)/parenleftig
∥x−y∥2+∥πt,h(x,˜µt,h)−πt,h(y,˜µt,h)∥2/parenrightig
Assumption 1
≤∥x−y∥2. Assumption 2
It follows that qis 1-Lipschitz, and therefore
/integraldisplay
s∈S/integraldisplay
s′∈Sv(s′)P[f(s,µt,h) +ωt,h∈ds′](˜µt,h(ds)−µt,h(ds))
=Lf(1 +Lπ)/integraldisplay
s∈Sq(s)(˜µt,h(ds)−µt,h(ds))
≤Lf(1 +Lπ)W1(˜µt,h,µt,h) (33)
The first equality follows directly from the definition of the function q, while the last inequality comes from
the Wasserstein Dual formulation of W1in Eq. (8).
Combining Step 1 and Step 2:
By combining Eq. (29), Eq. (32) and Eq. (33), we get
W1(˜µt,h+1,µt,h+1)≤[2Lf(1 +Lπ) + 2βt−1Lσ(1 +Lπ)]W1(˜µt,hµt,h))
+ 2βt−1/integraldisplay
s∈S∥σt−1(s,πt,h(s,˜µt,h),˜µt,h)∥2µt,h(ds).
Also, note that
∥σt−1(s,πt,h(s,˜µt,h),˜µt,h)∥2
=∥σt−1(s,πt,h(s,˜µt,h),˜µt,h)−σt−1(s,πt,h(s,µt,h),µt,h) +σt−1(s,πt,h(s,µt,h),µt,h)∥2
≤Lσ(∥πt,h(s,˜µt,h)−πt,h(s,µt,h)∥2+W1(˜µt,h,µt,h)) +∥σt−1(s,πt,h(s,µt,h),µt,h)∥2
≤Lσ(1 +Lπ)W1(˜µt,h,µt,h) +∥σt−1(s,πt,h(s,µt,h),µt,h)∥2,
where the first inequality comes from Assumption 4 and the second from Assumption 2. Therefore, we have
the following upper-bound on W1(˜µt,h+1,µt,h+1):
W1(˜µt,h+1,µt,h+1)≤2(1 +Lπ) [Lf+ 2βt−1Lσ]W1(˜µt,hµt,h)
+ 2βt−1/integraldisplay
s∈S∥σt−1(s,πt,h(s,µt,h),µt,h)∥2µt,h(ds).
The above inequality establishes the relationship between W1(˜µt,h+1,µt,h+1)andW1(˜µt,h,µt,h). We use the
fact thatW1(˜µt,0,µt,0) = 0and apply this relationship repeatedly h−1times to derive the following upper
24Published in Transactions on Machine Learning Research (05/2023)
bound forW1(˜µt,h,µt,h):
W1(˜µt,h,µt,h)
≤2βt−1h−1/summationdisplay
i=0[2(1 +Lπ) [Lf+ 2βt−1Lσ]]h−1−i/integraldisplay
s∈S∥σt−1(s,πt,i(s,µt,i),µt,i)∥2µt,i(ds)
≤2βt−1h−1/summationdisplay
i=0/bracketleftig
1 + 2(1 +Lπ) [Lf+ 2βt−1Lσ]/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
=:Lt−1/bracketrightigh−1−i/integraldisplay
s∈S∥σt−1(s,πt,i(s,µt,i),µt,i)∥2µt,i(ds)
≤2βt−1Lh−1
t−1h−1/summationdisplay
i=0/integraldisplay
s∈S∥σt−1(s,πt,i(s,µt,i),µt,i)∥2µt,i(ds).
Remark 4. Consider the setting of Lemma 5. Note that {µt,i}h−1
i=0is the trajectory of state distributions
i.e.P[st,i∈A] =µt,i(A)for allA⊆Sas shown in Lemma 1. Therefore,
h−1/summationdisplay
i=0/integraldisplay
s∈S∥σt−1(s,πt,i(s,µt,i),µt,i)∥2µt,i(ds) =h−1/summationdisplay
i=0E/bracketleftbig
∥σt−1(zt,i)∥2/bracketrightbig
=E/bracketleftiggh−1/summationdisplay
i=0∥σt−1(zt,i)∥2/bracketrightigg
,
wherezt,i= (st,i,πt,i(st,i,µt,i),µt,i)and the expectation is taken over the initial distribution and the tran-
sition stochasticity. The first equality comes from the aforementioned fact, P[st,i∈A] =µt,i(A), while the
second equality comes from the linearity of expectation.
Therefore,
W1(˜µt,h,µt,h)≤2βt−1Lh−1
t−1E/bracketleftiggh−1/summationdisplay
i=0∥σt−1(zt,i)∥2/bracketrightigg
. (34)
Lemma 6. Under the setting of Lemma 5 and assuming that the event in Assumption 3 holds true,
rt≤4βt−1Lr(1 +Lπ)LH−1
t−1HE/bracketleftiggH−1/summationdisplay
h=0∥σt−1(zt,h)∥2/bracketrightigg
.
Proof.First, note that under the setting of Lemma 6, Lemmas 3 to 5 also hold.
rt≤|˜J(πt,˜ft−1)−J(πt)| Eq.(17)
≤2Lr(1 +Lπ)H−1/summationdisplay
h=1W1(˜µt,h,µt,h) Eq.(18)
≤4βt−1Lr(1 +Lπ)H−1/summationdisplay
h=1Lh−1
t−1E/bracketleftiggh−1/summationdisplay
i=0∥σt−1(zt,i)∥2/bracketrightigg
Eq.(34)
≤4βt−1Lr(1 +Lπ)LH−1
t−1HE/bracketleftiggH−1/summationdisplay
i=0∥σt−1(zt,i)∥2/bracketrightigg
.
In the last inequality, we used the fact that Lt−1≥1henceLH−1
t−1≥Lh−1
t−1. Furthermore,∥σt−1(zt,i)∥2≥0
for any input combination, hence/summationtextH−1
i=0∥σt−1(zt,i)∥2≥/summationtexth−1
i=0∥σt−1(zt,i)∥2.
Now, we are ready to prove Theorem 1.
25Published in Transactions on Machine Learning Research (05/2023)
Proof of Theorem 1.
R2
T=/parenleftiggT/summationdisplay
t=1rt/parenrightigg2
≤TT/summationdisplay
t=1r2
t Cauchy-Schwartz ineq.
≤TT/summationdisplay
t=116pβ2
T−1L2
r(1 +Lπ)2L2H−2
T−1H2E/bracketleftiggH−1/summationdisplay
i=0∥σt−1(zt,i)∥2/bracketrightigg2
Lemma 6
≤16pTβ2
T−1L2
r(1 +Lπ)2L2H−2
T−1H2T/summationdisplay
t=1E/bracketleftiggH−1/summationdisplay
i=0∥σt−1(zt,i)∥2/bracketrightigg2
≤16pTβ2
T−1L2
r(1 +Lπ)2L2H−2
T−1H3E/bracketleftiggT/summationdisplay
t=1H−1/summationdisplay
i=0∥σt−1(zt,i)∥2
2/bracketrightigg
Jensen’s ineq.
≤16pTβ2
T−1L2
r(1 +Lπ)2L2H−2
T−1H3IT.
The expectations in the expressions above are taken over the initial state and transition noises. For the
second inequality, we used that βt−1is non-decreasing in tby Assumption 3 while the last inequality follows
from/summationtextT
t=1/summationtextH−1
i=0∥σt−1(zt,i)∥2
2≤ITfor allzt,i∈Zfort= 1,...,Tandi= 0,...,H−1. Taking square
root of both sides yields the desired result.
C Gaussian Processes
In this section, we specialize our main theorem, Theorem 1, to Gaussian Process models.
C.1 Gaussian Process
The main step of relating Theorem 1 to the Gaussian Process models is to use a Gaussian Process to
approximate the unknown dynamics function f. In particular, we select a GP with prior GP(0,k)where
kis a positive semi-definite kernel on Z× [p]where [p] ={1,...,p}andi∈[p]specifies the index of
the output dimension. The posterior of this Gaussian Process is calculated under the assumption that the
observed noise, ξi,h=si,h+1−f(zi,h), is drawn independently from N(0,λIp)for alliandh. Hereλis a
free-parameter that does not necessarily depend on the distribution of ωt,hfor anytandh. Then, at the
beginning of episode t, the posterior mean and variance functions conditioned on D1∪···∪D t−1obtain a
closed-form solution (Rasmussen, 2003):
mt−1(z,j) =kt−1(z,j)⊺(Kt−1+λIt−1)−1yt−1, (35a)
σ2
t−1(z,j) =k((z,j),(z,j))−kt−1(z,j)⊺(Kt−1+λIt−1)−1kt−1(z,j), (35b)
whereIt−1∈R(t−1)Hp×(t−1)Hpis the identity matrix and
yt−1=/bracketleftbig
[si,h]l/bracketrightbigt−1,H,p
i,h,l=1∈R(t−1)Hp,
kt−1(z) =/bracketleftbig
k((z,j),(zi,h,l))/bracketrightbigt−1,H,p
i,h,l=1∈R(t−1)Hp,
Kt−1=/bracketleftbig
k((zi,h,l),(zi′,h′,l′))/bracketrightbigt−1,H,p,t−1,H,p
i,h,l,i′,h′,l′=1∈R(t−1)Hp×(t−1)Hp,
where [si,h]ldenotes the l-th element of the vector si,h. We use the following notation for the mean and
variance vectors mt−1(z) := [mt−1(z,1),...,mt−1(z,p)]andσ2
t−1(z) := [σ2
t−1(z,1),...,σ2
t−1(z,p)]for allz
inZ. In particular, we choose λ=pH.
26Published in Transactions on Machine Learning Research (05/2023)
C.2 Assumptions and Relevant Results
We make the following assumptions on the semi-definite kernel kand the true dynamics fof the system.
Assumption 5. The unknown function fbelongs to the Reproducing Kernel Hilbert Space (RKHS) of a
positive semi-definite kernel k: (Z× [p])×(Z× [p])→R, and has bounded RKHS norm, i.e., ∥f∥k=/radicalbig
⟨f,f⟩k≤Bfwhere⟨·,·⟩kis the inner product of the RKHS and Bfis a fixed known positive constant.
Furthermore, we assume that ωt,hisσ-sub-Gaussian for all tandh.
This assumption is standard when Gaussian Process models are used (e.g., Srinivas et al. (2010); Chowdhury
& Gopalan (2019); Curi et al. (2020)).
Assumption 6. The positive semi-definite kernel kis symmetric, continuously differentiable with bounded
derivative, and k(z,z)≤1,∀z∈Z. We define the kernel metric as dk(z,z′) =/radicalbig
k(z,z) +k(z′,z′)−2k(z,z′)
and assume that it is Lipschitz-continuous, i.e., dk(z,z′)≤Ldd(z,z′)whered(z,z′) :=∥s−s′∥2+∥a−a′∥2+
W1(µ,µ′)andLdis some positive constant.
Wenotethatthepreviousassumptionisverymildsinceitholdsforthemostcommonlyusedkernelfunctions.
Assumption 5 and Assumption 6 also imply Assumption 1 which states that the unknown function fis
Lipschitz-continuous, i.e., ∥f(z)−f(z′)∥2≤Lfd(z,z′)whereLfis a positive constant (see Corollary 4.36
in Steinwart & Andreas (2008)).
First, we show that under Assumption 5 the Gaussian Process conditioned on D1∪···∪Dtis calibrated for
allt≥1. Our argument follows from the following lemma:
Lemma 7 (Concentration of an RKHS member, Lemma 5 in Chowdhury & Gopalan (2019) ) .Letk:
X×X→ Rbe a symmetric, positive semi-definite kernel and f:X→Rbe a member of the RKHS Hk(X)
of real-valued functions on Xwith kernel k. Let{xt}t≥1and{ϵt}t≥1is conditionally R-sub-Gaussian for a
positive constant R, i.e.,
∀t≥0,∀λ∈R,E/bracketleftbig
eλϵt|Ft−1/bracketrightbig
≤exp/parenleftigg
λ2R2
2/parenrightigg
,
whereFt−1is theσ-algebra generated by {xs,ϵs}t−1
s=1andxt. Let{yt}t≥1be a sequence of noisy observations
at the query points {xt}t≥1, whereyt=f(xt) +ϵt. Forλ>0andx∈X, let
µt−1(x) :=kt−1(x)⊺(Kt−1+λI)−1Yt−1,
σ2
t−1(x) :=k(x,x)−kt−1(x)⊺(Kt−1+λI)−1kt−1(x),
whereYt−1:= [y1,...,yt−1]⊺denotes the vector of observations at {x1,...,xt−1}. Then, for any 0<δ≤1,
with probability at least 1−δ, uniformly over t≥1,x∈X,
|f(x)−µt−1(x)|≤/parenleftigg
∥f∥k+R√
λ/radicaltp/radicalvertex/radicalvertex/radicalbt2/parenleftbigg
log(1/δ) +1
2t−1/summationdisplay
s=1log(1 +λ−1σ2
t−1(xs))/parenrightbigg/parenrightigg
σt−1(x).
We note that similar results have been shown independently in Durand et al. (2018).
Before showing that the above specified Gaussian Process is calibrated, we define the maximum mutual
information ,γt, to measure the maximum information gain the representative agent could possibly obtain
onfby observing any set A⊂Z× [p]of sizet.
Definition 2 (Maximum Mutual Information) .Letf:X→Rbe a real-valued function defined on a domain
X, andta positive integer. For each subset A⊂X, letyAdenote a noisy version of fAwithP[yA|fA]. The
Maximum Mutual Information after tnoisy observations is defined as
γt(f,X) := max
A⊂X:|A|=tI(fA;yA),
whereI(fA;yA)denotes the mutual information between fAandyA.
27Published in Transactions on Machine Learning Research (05/2023)
Using the Maximum Mutual Information, the following corollary shows that the Gaussian Process satisfies
Assumption 3 under Assumption 5 and Assumption 6.
Corollary 4. Under Assumption 5 and Assumption 6, with probability 1−δfor allt≥0andz∈Z
∥f(z)−mt(z)∥2≤βt∥σt(z)∥2, (37)
where
βt=Bf+σ√
λ/radicalig
2(log(1/δ) +γptH(k,Z×[p])).
Proof.This corollary follows from Lemma 10 in Chowdhury & Gopalan (2019).
Additionally, the GP also satisfies Assumption 4 under Assumption 5 and Assumption 6. Lemma 8 follows
from Lemma 12 from Curi et al. (2020)
Lemma 8. For allzandz′inZand allt≥0, we have
∥σt(z)−σt(z′)∥2≤dk(z,z′)
wheredkis the kernel metric defined in Assumption 6.
We note that Lemma 8 and Assumption 6 immediately imply that σtsatisfies Assumption 4 with Lipschitz
constantLdfor allt≥0.
C.3 Regret Bound
Now, we are ready to show the regret bound under the choice of a Gaussian Process model.
First, we recall a partial result in Eq. (39) from Lemma 11 in Chowdhury & Gopalan (2019) stating that
under Assumption 5 and Assumption 6
T/summationdisplay
t=1/summationdisplay
z∈˜D1∪···∪ ˜Dt−1∥σt−1(z)∥2
2≤2epHγpHT(k,Z×[p]) (38)
where ˜Dtis an arbitrary observation sets during episode tfort= 1,2,3,....
Theorem 2. LetLT−1= 1 + 2(1 + Lπ) [Lf+ 2βT−1Lσ], and letµt,h∈P(S)for allt,h> 0. Then, under
Assumption 2, Assumption 5, and Assumption 6 and using a Gaussian Process as described in Appendix C.1,
for allT≥1and fixed constant H > 0, with probability at least 1−δ, the regret of M3–UCRL is at most:
RT≤O/parenleftigg
βT−1Lr(1 +Lπ)LH−1
T−1H2/radicalig
TγpHT(k,Z×[p])/parenrightigg
. (39)
Proof.As noted before, Assumption 5 and Assumption 6 imply Assumption 1, Assumption 3, and Assump-
tion 4, hence, by Theorem 1, with probability at least 1−δ
RT=O/parenleftig
βTLr(1 +Lπ)LH−1
T−1/radicalbig
H3TIT/parenrightig
=O/parenleftigg
βTLr(1 +Lπ)LH−1
T−1/radicaltp/radicalvertex/radicalvertex/radicalbtH3Tmax
˜D1,...,˜DTT/summationdisplay
t=1/summationdisplay
z∈˜D1∪···∪ ˜Dt∥σt−1(z)∥2
2/parenrightigg
,
whereLt−1= 1 + 2(1 + Lπ) [Lf+ 2βt−1Lσ]. Then we can substitute in Eq. (38) to reach the desired result.
28Published in Transactions on Machine Learning Research (05/2023)
Theorem 2 shows that the sublinearity of our regret depends on the Maximum Mutual Information
rates, in particular, for sublinear MMI rate the regret is sublinear. Srinivas et al. (2010) consider
the most commonly used kernels, and obtain the following sublinear rates for D⊂Rd: Linear ker-
nelsγt(f,D) =O(dlogt), Squared Exponential kernels γt(f,D) =O((logt)d+1), and Matérn kernels
γt(f,D) =O(td(d+1)/(2α+d(d+1))(logt))withα > 1. Furthermore, Krause & Ong (2011) show the sub-
linear property of specific composite kernels. We note that this line of work provides large freedom for kernel
design on Euclidean spaces, however, the state space in our problem includes the mean-field distribution
spaceP(S), therefore, the results are not readily applicable. Proving similar bounds on probability measure
spaces is a promising direction for future works.
D Experiment Implementation
D.1 Hallucinated Control Implementation
In Section 3, we introduced M3–UCRL, an algorithm that optimizes over the set of plausible dynamics Ft−1
and admissible policies Π(see Eq. (5)). However, as noted in the paper, optimizing over Ft−1is intractable
in most of the cases. In this sections, we describe a practical and equivalent reformulation of Eq. (5) that
helps parametrizing the problem and enables using gradient based optimization to find πtat every episode t.
First, we introduce an auxiliary function η:Z→ [−1,1]p(as in Moldovan et al. (2015); Curi et al. (2020))
wherepis the dimensionality of the state space Sand define the following hypothetical dynamics model:
˜ft−1(z) =mt−1(z) +βt−1Σt−1(z)η(z). (40)
Conditioned on the event from Assumption 3 holding true, ˜ft−1for anyηis calibrated, i.e., ˜ft−1∈Ft−1.
Furthermore, by the confidence interval in Assumption 3, every function in Ft−1can be written in the form
of Eq. (40), i.e.,
∀˜ft−1∈Ft−1,∃η:Z→ [−1,1]psuch that ˜ft−1(z) =mt−1(z) +βt−1Σt−1(z)η(z)∀z∈Z.
Therefore, we can reformulate Eq. (5) as an optimization over the set of admissible policies Πand auxiliary
functionη:Z→ [−1,1]pas
πt= arg max
π∈Πmax
η:Z→[−1,1]pE/bracketleftiggH−1/summationdisplay
h=0r(˜st,h,˜at,h,˜µt,h)/bracketrightigg
(41a)
s.t. ˜at,h=πt,h(˜st,h,˜µt,h),˜zt,h= (˜zt,h), (41b)
˜ft−1(˜zt,h) =mt−1(˜zt,h) +βt−1Σt−1(˜zt,h)η(˜zt,h), (41c)
˜st,h+1=˜ft−1(˜zt,h) + ˜ωt,h, (41d)
˜µt,h+1= Φ(˜µt,h,πt,h,˜ft−1). (41e)
Wenotethatforafixedpolicy π, theauxiliaryfunction η(·)canberewrittenas η(s,a,µ ) =η(s,π(s,µ),µ) =
η(s,µ). In essence, this turns the function η(·)into an additional policy that exerts “hallucinated” control
over the set of plausible models F(Curi et al., 2020).3
Taking expectation in Eq. (41a) still hinders the practicality of the algorithm, however, we note that Corol-
lary 3 applies to Eq. (41) and the following optimization problem is equivalent,
πt= arg max
π∈Πmax
η:Z→[−1,1]pH−1/summationdisplay
h=0ˆr(˜µh,πt,h) (42a)
s.t.˜ft−1(˜zt,h) =mt−1(˜zt,h) +βt−1Σt−1(˜zt,h)η(˜zt,h), (42b)
˜µt,h+1= Φ(˜µt,h,πt,h,˜ft−1) (42c)
3We remark that the introduction of η(·)policy is a simple trick used before in Moldovan et al. (2015) and Curi et al. (2020)
that is suitable for practical implementation of model-based reinforcement learning algortihms.
29Published in Transactions on Machine Learning Research (05/2023)
where Φis defined in Eq. (2) and ˆris defined in Eq. (14) as
ˆr(µ,π) =/integraldisplay
Sµ(ds)r(s,π(s,µ),µ).
Reformulating the optimization problem as in Eq. (42) and choosing parametrizable functions, e.g., Neural
Networks, for π,η, and the statistical estimators mt−1andΣt−1allow using gradient ascent to find an
optima for Eq. (42a). Even though running gradient ascent does not guarantee the discovery of global
optimum, we found in our experiments (see in Section 5) that it still provides a close approximation. We
provide further details on the implementation of the transition function Φin Appendix D.2
D.2 Transition Function Implementation
The main challenges of implementing the mean-field distribution function Φfrom Eq. (2) are representing the
mean-field distributions and taking the integral over the state space S. For our experiments, we discretize
the state-spaceSwith 104unit-sized cells. We denote these cells by Cijfori,j∈{0,..., 10}whereCij
denotes the cell [i,i+ 1)×[j,j+ 1). Note that cells corresponding to walls (see Fig. 1a), e.g., C5,0,C5,1,
orC55, are not part of the state-space. For each episode tand time-step h, thei,jentry ofµt,hrepresents
the probability that the representative agent lies in that cell, i.e., [µt,h]i,j=P[st,h∈[i,i+ 1)×[j,j+ 1)].
Furthermore, we denote the middle of each cell Ci,jbyci,j, i.e.,ci,j= (i+ 0.5,j+ 0.5).
The transition function Φis then implemented as follows, for every i,j∈{0,..., 10}(for whichCijdoes not
correspond to a wall), every episode t= 1,2,..., and every time-step h= 0,..., 19
[µt,h+1]i,j=/summationdisplay
k,lP[f(ck,l,πt,h(ck,l,µt,h),µt,h) +ωt,h∈Ci,j][µt,h]k,l
We assume that the noise term ωt,his Gaussian with 0mean andσ2variance and the two dimensions are
independent, therefore,
P[f(ck,l,πt,h(ck,l,µt,h),µt,h) +ωt,h∈Ci,j]
=P[f(ck,l,πt,h(ck,l,µt,h),µt,h)1+ωt,h,1∈[i,i+ 1)]×P[f(ck,l,πt,h(ck,l,µt,h),µt,h)2+ωt,h,2∈[j,j+ 1)]
= (ϕ(i−f(ck,l,πt,h(ck,l,µt,h),µt,h)1)−ϕ(i+ 1−f(ck,l,πt,h(ck,l,µt,h),µt,h)1))
×(ϕ(j−f(ck,l,πt,h(ck,l,µt,h),µt,h)2)−ϕ(j+ 1−f(ck,l,πt,h(ck,l,µt,h),µt,h)2))
whereϕis the cumulative distribution function of N(0,σ2).
In the implementation, we also redistribute some of the probability mass during transition based
on the walls location. If there is a wall between Ci,jandf(ck,l,πt,h(ck,l,µt,h),µt,h) +ωt,h, we set
P[f(ck,l,πt,h(ck,l,µt,h),µt,h) +ωt,h∈Ci,j] = 0. Similarly, for Ci,jadjacent to a wall or border we in-
crease P[f(ck,l,πt,h(ck,l,µt,h),µt,h) +ωt,h∈Ci,j]by the amount that is set to zero due to the adjacent
wall.
D.3 Transition Model estimation
Toestimatetheunknowntransitionfunction finSection5, weusedeepensemblemodels(Lakshminarayanan
et al., 2017). In particular, we use an ensemble of 10 feed-forward Neural Networks (NNs) with one hidden
layer of size 32 and leaky ReLu action functions. We use two output layers joined to the hidden middle
layer. The first one uses linear activation and returns the mean of the function while the second returns
the estimated variance using softplus activation. We follow the optimisation procedure described in (Laksh-
minarayanan et al., 2017) that minimizes the negative log-likelihood for each NN under the assumption of
heteroscedastic Gaussian noise. We included the adversarial training procedure as well for robustness and
smoothing. In prediction time, the ensemble estimates the mean and variance of the unknown function via
the empirical mean and variance of the Neural Networks’ mean outputs.
30Published in Transactions on Machine Learning Research (05/2023)
(a) Mean-field distribution under the BPTT optimised pol-
icy at time-step h= 3.
(b)Mean-fielddistributionundertheBPTToptimisedpol-
icy at time-step h= 5.
(c) Mean-field distribution under the BPTT optimised pol-
icy at time-step h= 8.
(d)Mean-fielddistributionundertheBPTToptimisedpol-
icy at time-step h= 20.
Figure 3: Mean-field distribution following the policy optimised via BPTT at four time-steps ( h= 3,5,8,20)
in a single policy roll-out. Arrows depict the actions taken by the agents in the corresponding squares. Note
that the coloring range changes with the time-steps.
D.4 Policy optimisation with known environment
We consider the policy optimised via back-propagation through time, πBPTT, as a reasonable point-of-
reference for our convergence analysis as it successfully disperses the population in the state space quickly
without any notable improvements in its strategy.
Fig. 3 shows the mean-field distribution over one episode following πBPTT 4. As shown on Fig. 3a, in the
first few steps πBPTTspreads the distribution evenly in the bottom-left corner, however, in the next two
steps as shown on Fig. 3b it concentrates a larger mass into the corridors to the neighbouring rooms. It is
an optimal choice of action because the corridors act as bottlenecks hence the more it can push through at
once the easier it will be to achieve a uniform distribution in the latter time-steps. In the next 3steps until
h= 8,πBPTTfurther propagates a larger portion of the population mass to the empty top-right room. In
the meanwhile, it starts to distribute the population in the top-left and bottom-right rooms as well. In the
following time-steps, πBPTTfocuses on achieving an even distribution within the rooms without pushing
4The supplementary material includes animation of the whole episode named as known_dynamics_animation.mp4
31Published in Transactions on Machine Learning Research (05/2023)
Figure 4: Reward and population action size per time-step in one episode. The policy followed by the agents
were optimised via BPTT.
Figure 5: Rewards achieved by U2-MF-QL-FH over10million training episodes. Training Rewards show the
estimated reward by U2-MF-QL-FH whileEvaluation Rewards show the performance of the greedy policy in
a roll-out. The policy is evaluated after every million training episodes. The hyperparameters of the training
wereωQ= 0.7,ων= 0.05andϵ= 0.15.
significant population mass through the corridors. Finally, Fig. 3d shows the mean-field distribution at the
end of the episode, h= 20, when the population is distributed uniformly and maximized its entropy. We
provide a further animation with every time-steps as part of the supplementary material.
Fig. 4 shows the mean-field distribution entropy over the time-steps of the same episode as in Fig. 3. πBPTT
successfully increases the entropy until time-step 14where it reaches the maximum achievable by the uniform
distribution and then maintains this level. The small stagnation in entropy from h= 4toh= 5is due to
the bottleneck of the corridors as shown on Fig. 3b.
32Published in Transactions on Machine Learning Research (05/2023)
E Additional Experiments
E.1 Comparison to model-free reinforcement learning
M3–UCRL is a unique algorithm in the Mean-Field Control literature because it considers continuous state
and action spaces and finite time-horizon. As described in Section 2, the goal in this setting is to find an
optimal policy that considers the evolution of the mean-field distribution over one episode for a fixed initial
distribution. In comparison, the majority of the works consider finite spaces and infinite time-horizon in
which the goal is to find the optimal stationary pair of policy and mean-field distribution. We note that this
is a significantly different solution concept to ours, hence, proposed algorithms are not suitable to solve the
exploration via entropy maximization problem described in Section 5. To the best of our knowledge, the
only algorithm that considers a finite time-horizon problem is the U2-MF-QL-FH proposed by Angiuli et al.
(2021). This algorithm is a Q-Learning variant that optimises the Q values via a two timescale approach.
Even though, the problem formulation for U2-MF-QL-FH is similar to ours described in Section 2, there
are notable differences. First, U2-MF-QL-FH is proposed for finite state and action spaces, uses a tabular
representation for the Q values and select deterministic actions. Second, it is model-free and assumes access
to simulator of one-step transitions. On the other hand, M3–UCRL does not rely on a tabular representation,
learns in continuous spaces, and have limited interactions with the environment to reduce costs associated
with these interactions.
Due to the formulation of U2-MF-QL-FH , we ran experiments in the finite state and action space variant
of the Exploration via entropy maximization problem as described in Geist et al. (2021); Laurière et al.
(2022). As we demonstrate below, the deterministic policy used by U2-MF-QL-FH is a significant limitation,
therefore, we also alter its action space to be a set of 9distributions over the 5potential actions from which
U2-MF-QL-FH chooses5. We call the former Standard formulation and the latter Extended .
As shown on Fig. 5, U2-MF-QL-FH converges slowly with high variance in rewards for both formulations,
however, the Extended version outperforms the Standard version for both training and evaluation perfor-
mance as expected. One of the main factors for slow learning is that the Q values are hard to estimate
due to the large number of state-action-time pairs and infrequent visits of many states. In particular, the
Q value table has |S|×|A|× H= 104×5×20 = 10,400entries ( 18,720for theExtended version), and
states far from the bottom left corner are rarely visited because they can be reached only in the later steps
of an episode. While extending the action-space to non-deterministic actions clearly improves performance,
it increases significantly the tabular representation leading to even longer training required. Even after 10
million episodes, we observed large changes in the Q value table indicating that the algorithm would need
further iterations, but we had to terminate it due to computational limits. These results further highlights
the contribution of M3–UCRL to the literature.
E.2 Swarm Motion Experiments
To complement experiments in Section 5, we also implemented M3–UCRL with Gaussian Process models for
theswarm motion problem. This setting considers an asymptotically large ( N→∞) population of agents
moving around in a space, maximizing a location-dependent reward function, and avoiding congested areas
(Almulla et al., 2017).
We choose the state space Sas the [0,1]interval with periodic boundary conditions, i.e., the unit torus,
and the action space Aas the compact interval [−7,7]. Each episode partitions the unit time length into
H= 200equal steps of length ∆h= 1/200. The unknown dynamics of the system is given by
f(s,a) =s+a∆h, (43)
andωt,h∼N(0,∆h)for alltandh. We note that, for now, the dynamics do not depend on the mean-field
distribution, but later on in Eq. (45), we also consider such a case. The reward function is of the form
5The set of distribution consists of the following: 5deterministic actions ( 4directions plus staying idle), 4 50% −50%split
between adjacent moves (up-right, right-down, down-left, left-up) and 1distribution that assigns 25%to all 4directions. Note
that the action space of the Standard formulation is a subset of this action space, therefore, it is expected that the Extended
version achieves better performance.
33Published in Transactions on Machine Learning Research (05/2023)
150
100
50
Episode Rewards
1234567891011121314151617181920
Number of episodes3000
2000
RewardM3UCRL Median
M3UCRL Min-Max interval
*
C
Known Dynamics
400
200
Episode Rewards
1234567891011121314151617181920
Number of episodes3000
2000
RewardM3UCRL Median
M3UCRL Min-Max interval
*
C
Known Dynamics
(a) Rewards achieved by M3–UCRL over 20 episodes.
0.0 0.2 0.4 0.6 0.8 1.0
State: s5.0
2.5
0.02.55.0(s)
Actions
M3UCRL
*
C
Known Dynamics
0.0 0.2 0.4 0.6 0.8 1.0
State: s5.0
2.5
0.02.55.0(s)
Actions
M3UCRL
*
C
Known Dynamics
(b) Actions in the ergodic optimal state.
0.0 0.2 0.4 0.6 0.8 1.0
State: s0123(s)
Mean-Field Distributions
M3UCRL
*
C
Known Dynamics
0.0 0.2 0.4 0.6 0.8 1.0
State: s0123(s)
Mean-Field Distributions
M3UCRL
*
C
Known Dynamics
(c) Mean-field distribution in the ergodic optimal state.
Figure6: Resultsfordynamicsmodel: (Leftcolumn) f(s,a) =s+a∆h, and(Rightcolumn) f(s,a,µ ) =
s+a(4−4µ(s))∆h.(π∗
C,µ∗
C)represent an analytic solution to the continuous time swarm motion problem,
and thus, it is not discrete-time optimal. The main discrete-time benchmark is Known Dynamics that unlike
our M3–UCRL knows the true dynamics. In both cases (left and right), M3–UCRL converges in a small
number of episodes and discovers a near-optimal policy.
r(s,a,µ ) =φ(s)−1
2|a|−ln(µ(s)),where the second term penalizes large actions, the third term introduces
the aversion to crowded regions and the first term defines the reward received at the position s, namely,
φ(s) =−2π2[−sin(2πs) +|cos(2πs)|2] + 2 sin(2πs).
Theadvantageofthistaskisthatonecananalyticallyobtainthefollowingoptimalsolutioninthe continuous-
timesetting when ∆h→0and the time horizon is infinite, i.e., H→∞Almulla et al. (2017):
π∗
C(s,µ) = 2πcos(2πs),
µ∗
C(s) =exp(2 sin(2πs)/integraltext
exp(2 sin(2πs′))ds′,(44)
whereπ∗
Candµ∗
Cform an ergodic solution, i.e., µ∗
C= Φ(µ∗
C,π∗
C,f). We use the continuous time optimal
solutions from Eq. (44) (where the subscript Cstands for continuous-time solutions) as a benchmark for
comparison, however, we note that our time discretization introduces certain deviations. Consequently, π∗
C
might no longer be optimal, and so we also compare our solution to the one obtained under the same setup
of discrete time steps and optimization procedure when the true dynamics are known(Known Dynamics).
This corresponds to our main benchmark.
34Published in Transactions on Machine Learning Research (05/2023)
0.0 0.2 0.4 0.6 0.8 1.0
State: s0123(s)
h=0h=4
h=8h=12h=16Mean-Field Distributions with 0 N(0.5,0.04)
*
C
Known Dynamics
0.0 0.2 0.4 0.6 0.8 1.0
State: s0123(s)
h=0h=4h=8h=12h=16Mean-Field Distributions with 0 U(0,1)
*
C
Known Dynamics
Figure 7: Mean-Field distributions over one episode with different initial distributions and dynamics given
by Eq. (43) after training has converged. ( Left side )µ0∼N(0.5,0.04)(Right side )µ0∼U(0,1). Already
ath= 16, the distribution induced by M3–UCRL policy nearly-matches the one when dynamics are known.
Finally, we also consider the modified swarm motion problem with dynamics given by
f(s,a,µ ) =s+a(4−4µ(s))∆h. (45)
The additional term that depends on the mean-field distribution models the effect of congestion on the
transition. Specifically, in comparison to Eq. (43), the more congested an area is, the larger the action an
agent must exert to achieve the same movement.
While swarm motion resembles the exploration via entropy maximization problem in Section 5, there are
meaningful differences besides the lower dimensional state and action spaces. Namely, certain areas in the
state space rewarded differently and large actions are explicitly penalized, therefore, agents not only have to
explore the state space but also learn the trade-off between areas and actions. Eq. (45) further extends the
swarm motion problem with a dependency on the mean-field distribution that increases the complexity of
the agent’s learning problem.
Fig. 6 depicts the results when the true dynamics are given by Eq. (43) (Left column) and Eq. (45) (Right
column). In Fig. 6a (Left), we show the rewards achieved over 20episodes. In Fig. 6b (Left) and Fig. 6c
(Left), we show the ergodic solution of M3–UCRL and compare it with (π∗
C,µ∗
C)and the solution found
when dynamics are known. As expected, the introduced time-discretization results in some deviations from
Eq. (44) which allow the policy found when the dynamics are known to exploit them and achieve higher
reward. On the other hand, M3–UCRL converges in a small number of episodes and drives the environment
into an ergodic state with policy and mean-field distribution almost identical to the solution under known
dynamics (see Fig. 6b (Left) and Fig. 6c (Left)) . We note that, while minimal fluctuations in the episodic
rewards are present, the M3–UCRL is robust in finding close to optimal solutions.
In Fig. 6 (Right column), we show the results obtained for the dynamics from Eq. (45). Even though π∗and
µ∗in Eq. (44) are no longer applicable for this problem, we include them in the figures to show the effect of
the introduced congestion term. Similarly to our previous experiments, M3–UCRL successfully converges to
the solution obtained under known dynamics. Fig. 6b (Right) shows that the two policies are not completely
aligned after the same number of episodes as before, which is due to the harder estimation problem and
larger confidence estimates.
In the previous experiments, we selected µ∗as the initial distribution to focus on finding an optimal policy
functionπ. To evaluate the robustness of M3–UCRL and its ability to drive an arbitrary initial distribution
towards the optimal solution, we perform additional experiments with uniform and normal initial distri-
butions. Fig. 7 shows the subsequent mean-field distributions in one episode after the algorithm reached a
stablepolicy. WeobservethatM3–UCRLisrobusttotheinitialdistributionandswiftlydrivesthemean-field
distribution towards a steady state that maximizes the rewards over time.
35