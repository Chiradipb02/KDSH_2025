Published in Transactions on Machine Learning Research (11/2022)
Fast and Accurate Spreading Process Temporal Scale Esti-
mation
Abram Magner amagner@albany.edu
University at Albany, SUNY
Carolyn Kaminski ckaminski@albany.edu
University at Albany, SUNY
Petko Bogdanov pbogdanov@albany.edu
University at Albany, SUNY
Reviewed on OpenReview: https: // openreview. net/ forum? id= k4iWTEdUSF
Abstract
Spreading processes on graphs arise in a host of application domains, from the study of on-
line social networks to viral marketing to epidemiology. Various discrete-time probabilistic
models for spreading processes have been proposed. These are used for downstream statisti-
cal estimation and prediction problems, often involving messages or other information that
is transmitted along with infections caused by the process. These models generally model
cascade behavior at a small time scale but are insuﬃciently ﬂexible to model cascades that
exhibit intermittent behavior governed by multiple scales. We argue that the presence of
such time scales that are unaccounted for by a cascade model can result in degradation
of performance of models on downstream statistical and time-sensitive optimization tasks.
To address these issues, we formulate a model that incorporates multiple temporal scales
of cascade behavior. This model is parameterized by a clock, which encodes the times at
which sessions of cascade activity start. These sessions are themselves governed by a small-
scale cascade model, such as the discretized independent cascade (IC) model. Estimation
of the multiscale cascade model parameters leads to the problem of clock estimation in
terms of a natural distortion measure that we formulate. Our framework is inspired by
the optimization problem posed by DiTursi et al, 2017, which can be seen as providing
one possible estimator (a maximum-proxy-likelihood estimator) for the parameters of our
generative model. We give a clock estimation algorithm, which we call FastClock, that runs
in linear time in the size of its input and is provably statistically accurate for a broad range
of model parameters when cascades are generated from any spreading process model with
well-concentrated session infection set sizes and when the underlying graph is at least in the
semi-sparse regime. We exemplify our algorithm for the case where the small-scale model
is the discretized independent cascade process and extend substantially to processes whose
infection set sizes satisfy a general martingale diﬀerence property. We further evaluate the
performance of FastClock empirically in comparison to the state of the art estimator from
DiTursi et al, 2017. We ﬁnd that in a broad parameter range on synthetic networks and
on a real network, our algorithm substantially outperforms that algorithm in terms of both
running time and accuracy. In all cases, our algorithm’s running time is asymptotically
lower than that of the baseline.
1 Introduction
There are a variety of well-established and simple probabilistic generative models for graphs and and in-
fectious processes that run over these graphs. In this work we speciﬁcally focus on models for spreading
1Published in Transactions on Machine Learning Research (11/2022)
processes on networks such as the diﬀusion of innovation (Montanari & Saberi, 2010), information (Bakshy
et al., 2012) and misinformation (Shin et al., 2018) in social networks. In such models, every node at a given
time has a state, and the model stipulates the conditional probabilities of nodes being in given states at
future times, given their states at the current time.
Typically, these models only focus on a single, uniform time scale, thus ignoring global synchronizing events
that occur in reality (such as periodic daily/weekly business schedules governing the actions of network
agents). In this work, we address modeling and estimation of parameters in the common scenario where
spreading process activity occurs intermittently, according to a global synchronizing schedule, as in Exam-
ple 1.
Example 1 (A scenario leading to intermittent spreading process activity) .Consider a cascade (of, say, a
hashtag) among users of an online social network. These users may interact with the network only during
certain times of day, owing to their work schedules. For instance, the bulk of interactions may occur in
the morning, during one’s lunch break, during one’s afternoon tea, and after work. Infections are likely to
proceed only during these periods of activity, during which they spread stochastically according to a more
granular temporal process. See Example 2 for a reﬁnement of this scenario, using the terminology of our
model.
The scenario just presented involves two time scales:
1. A larger time scale, consisting of a sequence of sessions during which local spreading process activity
occurs.
2. A smaller time scale during each session, consisting of local spreading process activity as captured
by a typical model.
It is important to take these multiple scales into account in model building and subsequent parameter
estimation, for a variety of reasons:
1. Failing to take the larger time scale into account (i.e., assuming that the process is entirely governed
by the smaller time scale) leads to erroneous parameter estimates for the smaller time scale model.
2. Even when the smaller time scale model parameters are known, failure to account for the larger
time scale leads to error in downstream statistical inference and optimization tasks, such as cascade
doubling time prediction (Cheng et al., 2014) and message model parameter estimation.
3. An estimate of the larger time scale or parameters of a model generating it can themselves be used as
features in machine learning problems involving cascades, such as cascade classiﬁcation (e.g., where
class labels are “misinformation” or “true information”).
We discuss a few motivations in more detail in Section 1.2. Furthermore, in general, given cascade data
in a large network, the form of the intermittencies in the large time scale are not obvious and may vary
depending on the community from which the cascades originated since diﬀerent communities and cascades
consisting of diﬀerent types of information (e.g., long-form essays versus infographics versus videos versus
single-panel web comics) may operate according to diﬀerent schedules. It is thus important to learn these
intermittencies from observations, as we do in the present work.
Given these motivations, the present paper is concerned with
•the formulation of a generative model for network cascades that is parametrized by an arbitrary
largetime scale (which itself may contain events on multiple scales), with the smaller time scale
generated by some given standard model;
•the formulation of the estimation problem for the parameters of the large time scale from cascade
observations;
2Published in Transactions on Machine Learning Research (11/2022)
•algorithmic techniques for provably accurate and computationally eﬃcient solution of the above
estimation problem.
Our work is inspired by the works of DiTursi et al. (2017) and DiTursi et al. (2019), which took the following
approach: they formulated a problem of clock recovery , in which a “ground-truth” cascade is generated by the
independent cascade model of Kempe et al. (2003), then perturbed by a “clock transformation”, parametrized
by a clock (a sequence of observation timestep endpoints) to yield an observed cascade. The task that they
formulated is an optimization one: from the observed cascade, recover the clock with maximum likelihood.
Among other contributions, they formulated a dynamic programming algorithm that provably achieves
this maximum likelihood, but with a large asymptotic running time (see the discussion in Section 1.3).
Concretely, their dynamic programming algorithm can be seen as implementing one possible
estimator for the clock parameter of our generative model, in the case where the cascade,
viewed on the large time scale, is distributed according to the independent cascade model
of Kempe et al. (2003). We are the ﬁrst to empirically study the accuracy of their estimator, in terms
of distance to the ground truth parameters, and, thus, it is not a priori clear what types of graphs are most
favorable for its performance.
In the present work, we take a more statistical approach: we rephrase the clock estimation problem via a
generative model that produces cascades with multiple temporal scales. In this model, clocks become the
parameters of the larger temporal scale, and the task is to estimate the unknown clock (the parameter)
from the data that the model produces. We make this estimation task rigorous by introducing a notion of
distortion between clocks, which measures the accuracy of an estimated clock. The clocks in our work are
the same type of object as those in the prior work (but interpreted diﬀerently).
Our reformulation is valuable for both performance and philosophical reasons:
1. Inthepriorwork, itisunclearwhatphysicalmechanismisbeingmodeledbytheclocktransformation
which operates on an already generated cascade, while, in contrast, in our work, clocks are a natural
product of global intermittent inﬂuences on individual node behavior.
2. In the prior work, the “ground-truth” cascade uses a notion of timesteps that do not map to physical
times, while in our work, the observed cascade operates in a discretization of real, physical time.
3. Prior work does not attempt, even empirically, to quantify the distance between the ground-truth
clock (the one that actually generated the data) and the estimated one – maximizing the likelihood
of the estimated clock, rather than ensuring accuracy, is treated as the end goal. In contrast, in
our work we give a rigorous estimation formulation, which allows us to quantify the accuracy of
the maximum likelihood estimator versus alternatives (such as the estimator that we introduce).
As detailed throughout this paper, our resulting estimator substantially outperforms the previous
work’s dynamic programming-based maximum likelihood estimator in terms of running time and, on
all of our synthetic data and in a broad range of parameters on real networks, in terms of accuracy.
Across the entire parameter range on real networks, the average accuracy of our estimator is quite
low (consistently less than 0.07).
1.1 High-level problem formulation
We next state our problem at a slightly more technical level (we give all formal formulations in Section 2).
To do so, we ﬁrst introduce preliminaries on spreading process models.
Preliminaries: spreading process models. Several well-studied information diﬀusion models on a
graphGassume a discrete timeline in which at every time step nodes of Gparticipate in the diﬀusion process
(i.e., become “infected”) based on inﬂuence from network neighbors who became infected in past time steps.
Here, a timeline is simply an ordered sequence of indices: 0,1,2,.... The output of a spreading process
model after timestep Nis aninfection sequence of disjoint vertex subsets S0,S1,...,SN, whereStis the set
of vertices of Gthat became infected during timestep t. For example, according to the independent cascade
modelinfected nodes have one chance to infect their neighbors, while in the linear threshold model nodes
3Published in Transactions on Machine Learning Research (11/2022)
get infected when a critical fraction of their neighbors have been infected in any prior time steps (Kempe
et al., 2003). In addition to the discrete time models above, there also exist continuous-time counterparts.
For the purposes of computation, the latter are generally discretized by a standard recipe: given a resolution
δ > 0and a cascade sampled from a continuous-time model over a time interval [0,T], one deﬁnes the
δ-discretization of the cascade to be the infection sequence S= (S0,S1,...,S⌈T/δ⌉), whereSjconsists of the
set of vertices infected during the time interval jδ. In the discussion that follows, we will thus state our
formulation in the discrete-time case, with the understanding that this is without loss of generality.
Formulation of our two-scale model. To capture our motivating scenario, we will deﬁne a new two-
scale cascade model, where the smaller scale is given by an arbitrary ﬁxed spreading process model M0with
the property that, at any given time t, a vertex may only transmit infection if it is in the set of active nodes
Atat that time.
The generative process that we will deﬁne is parametrized by a sequence of time points t0,t1,...,tN, giving
the larger time scale. Intuitively, tjis the time at which the jth session starts. The activity within the jth
session (comprising the smaller scale of the model) is dictated by M0as follows: for any time t∈[tj,tj+1),
the distribution of new vertex infection events is given by M0conditioned on the active set remaining ﬁxed
to its value at time tj. In other words, vertices may become infectedin the time interval [tj,tj+1), but the
active set is not updated until time tj+1. This assumption is justiﬁed whenever nodes’ “attention spans”
have a ﬁnite granularity, in the sense that newly infected vertices do not become infectious immediately.
This is aligned with the intuition of daily activity periods from Example 1 and more broadly justiﬁed in
spreading processes on social networks. We give a more speciﬁc scenario in Example 2 to illustrate this. In
the appendix, Section G, we explain the behavior of our estimator when this assumption does not strictly
hold.
Example 2 (Scenarios illustrating our active set intermittent update assumption) .Here, we give more
detailed examples of scenarios in which spreading process activity satisﬁes our assumption regarding inter-
mittent updating of the active set.
Consider a social network such as Facebook, where nodes are people and edges are friendships. We consider
cascades in which a person is said to be infected when they post a particular piece of content. Note that
infection of a person is detectable immediately, and can thus be recorded in a dataset. A person is active
(i.e., infectious) when other people see what that person has posted. We assume, furthermore, that people
have, in any given session (deﬁned below), a limited reserve of attention, and will thus be focused on posts
that occurred prior to the current session.
Now, a session starts when some break in the day happens and ends when the break stops. At the start of
a cascade, some initially active vertices Ssess(0)are seen by users in the ﬁrst session. A subset Ssess(1)
of these users, during that same session, repost, becoming infected. Neighbors of newly infected vertices do
not see these reposts during this session, because of the aforementioned limitations of attention. Thus, the
elements of Ssess(1)are infected, but they are not active until the nextsession (which may occur several
hours after the current one), at the beginning of which the attention reserves of their neighbors have been
replenished.
An alternative mechanism that could produce the same eﬀect as the attention reserve is a platform playing
an active role in displaying posts non-chronologically, resulting in delays in the display of a post.
We note that in the above example, diﬀerent types of content may plausibly spread according to diﬀerent
clocks. For instance, long videos may be shared only after work hours, while technical essays may be shared
during work. Comedic webcomics may be shared throughout the day. Thus, it is of interest to estimate the
clocks of diﬀerent cascades from data, rather than assuming some hard-coded clocks.
Formulation of our estimation problem. Having formulated our two-scale model, we next formu-
late the central estimation question that we study in this work: given a sample infection sequence
(Sobs(0),Sobs(1),...,Sobs(ˆN))from the two-scale model parametrized by the larger time scale t0,t1,...,tN, the
natural question to ask is to what extent t0,...,tNcan be estimated. For many models, this is reducible to a
more fundamental problem, which is the central one that we tackle in this work: estimate the assignment of
4Published in Transactions on Machine Learning Research (11/2022)
01 3
4
2 5
6012345 Timestep
S(1)
∆1= [0,0],[1,4],[5,5]6
25
0
S(2)
∆2= [0,2],[3,4],[5,5]6
25
0
Figure 1: An example network and a cascade Sobswith corresponding observed infection sequence Sobs =
({6},{2},{},{},{5},{0}), encoding that 6was infected in timestep 0,2in timestep 1,5in timestep 4, and 0in timestep
5. Suppose that this cascade was generated according to our two-scale model with clock ∆1= ([0,0],[1,4],[5,5]), so that there
are6timesteps and 3sessions. Then the (unobserved) session-level infection sequence is S(1)
sess= ({6},{2,5},{0}). An esti-
mator of the clock, given Sobs, might output a diﬀerent clock: say, ∆2= ([0,2],[3,4],[5,5]), yielding an estimated session-level
infection sequence of S(2)
sess= ({6,2},{5},{0}). In Deﬁnition 4, we will deﬁne a distance between clocks, under which ∆1and
∆2have nonzero distance, because ∆2entails that vertices 6and2were both infected in the ﬁrst session.
vertex infection events to the indices of sessions during which they occurred. We call this the clock estimation
problem. We elucidate this estimation problem by the example in Figure 1.
1.2 Motivating applications
We next discuss several motivating applications for our generative model and for the clock recovery problem.
In the listed applications, we explain how failing to account for multiple time scales results in degraded
statistical accuracy or optimization performance on downstream tasks. We note that all of these applications
involveanontrivialstatisticaloroptimizationtaskevenwhentheparametersofthesmall-scalecascademodel
are known.
1.Parameter estimation for information models running on top of cascade processes: In
one class of applications, one wants to make inferences about some extra piece of information (a
message, sentiment, opinion, etc.) that is spread along with an infection, according to a parametric
statistical model (which we will call, for this discussion, an information model), and the information
associated with a given node is statistically dependent on the corresponding information associated
with the nodes that infected it.
As a concrete example, consider a model of the spread of a sentiment (an information model) that
runs on cascades generated by our two-scale model parametrized by some nontrivial clock, with the
smaller scale generated by the discretized independent cascade model. In particular, the seed nodes
v∈S0of the cascade are endowed with sentiments Xvuniformly distributed in [0,1]. When a node
wbecomes infected, say, by an unknown subset I(w)of active neighbors of w, the sentiment of wis
given as follows:
Xw=θ1θ2+ (1−θ2)1
|I(w)|/summationdisplay
v∈I(W)Xv. (1)
Here,θ1is a bias parameter ∈[0,1]that represents some base sentiment, and θ2∈[0,1]is a social
inﬂuence parameter that governs how much each node wtends to adopt the average sentiment of
its infectors. The goal in such a model is to estimate θ1,θ2from observation of a cascade as well as
measured sentiments Xw.
Note that even when the parameters of the smaller-scale spreading process model are known, it is of
interest to estimate the parameters of the information model from sample cascades. Furthermore, in
such applications, clock estimation – the main subject of this paper – is an essential preprocessing
step in order to reduce the complexity of parameter estimation by determining the set of vertices
thatcould have infected a given vertex w(i.e., the set of active neighbors of w). This set is not
observable, owing to lack of knowledge of the clock. Furthermore, failure to consider multiple time
scales introduces error into the log likelihood function of the information model parameters since
5Published in Transactions on Machine Learning Research (11/2022)
the correct set of active neighbors is unknown. Ultimately, this results in erroneous parameter
estimation.
2.Time-critical tasks: In certain applications, the goal is to optimize some objective function
deﬁned on the state of the process, subject to a time deadline that is given as input. For instance,
in time-critical inﬂuence maximization (Chen et al., 2012b), the problem is to choose seed nodes to
optimize the expected number of infections occurring by a given timestep.
For a given seed set, the expected number of vertices infected by the deadline may change dramat-
ically depending on whether or not a large time scale is incorporated into the model. Intuitively,
this is because delays introduced by a large time scale may make the infection of a particularly
well-connected vertex less likely. Thus, taking into account multiple time scales is essential for the
success of model-based time-critical optimization tasks. This is exacerbated even further in cases
where diﬀerent subsets of nodes operate according to diﬀerent large time scales.
3.Cascade doubling time prediction: The cascade doubling time prediction problem was posed
in Cheng et al. (2014). A cascade is observed up to a certain time t, and the goal is to produce an
accurateestimateofthetime atwhichthenumberof infectednodesdoublesfromthe numberattime
t. This problem was introduced as a prototypical prediction problem for cascades. In Example 3 in
Appendix C, we spell out the details of a concrete example in which failure to account for a large
time scale results in avoidable inaccuracy in doubling time prediction.
1.3 Prior work
The general topic of analysis of cascades has received a large amount of attention, both from theoretical and
empirical perspectives. There are many cascade models, with features depending on application domains.
For example, the independent cascade (IC) and linear threshold (LT) models were popularized in Kempe
et al. (2003) for the application of inﬂuence maximization . This problem continues to be studied, even in
settings where cascade model parameters are known (Lee et al., 2016; Abbe et al., 2017). Variations on the
inﬂuence maximization problem that have time-critical components and, thus, require accurate modeling of
multiple time scales in the sense that we study here, have also been studied (Chen et al., 2012a; Ali et al.,
2019). These models are also used outside the context of inﬂuence maximization, e.g., in modeling the spread
of memes on social networks (Adamic et al., 2016).
Statistical prediction tasks involving cascades have also been posed. For instance, the cascade doubling time
prediction task was considered in Cheng et al. (2014). Other works propose models in which a piece of
information, such as a message, an opinion, or a viral genome, is transmitted along with the infection of
a node (Eletreby et al., 2020; De et al., 2016; Park et al., 2020). For such statistical problems, statistical
inferencesaboutthetransmittedinformationcanbedisruptedbyinaccurateestimationofthesetofinfectious
vertices at a given time, further motivating generative modeling and estimation that takes into account
multiple time scales.
The estimation method that we propose and study here, FastClock , bears a resemblance to methods in the
online change-point detection literature (Veeravalli & Banerjee, 2014). Broadly speaking, the goal of that
problem is to sequentially observe independent and identically distributed random variables X1,X2,...and
to detect, with as few samples as possible, an index after which the distribution of the variables changes.
One procedure for this, called CuSum, evaluates for each index ja statisticYj=Yj(Xj)and maintains a
sumZt=/summationtextt
j=1Yt. A changepoint is declared once Ztexceeds a certain threshold. In this paper, we deﬁne
a similar rule for detecting session endpoints . In the context of change-point detection, CuSum has been
analyzed in both the iid and more general hidden Markov setting (Fuh, 2003). However, our analysis is
necessarily substantially diﬀerent from that of CuSum, which rests on stringent assumptions (e.g., that the
Xjare sampled from an ergodic Markov chain).
In DiTursi et al. (2017) (see also followup work in DiTursi et al. (2019)), the authors formulated a version
of the problem of clock recovery from cascade data generated according to an adversarially chosen clock
as a problem of maximization of a function of the clock that serves as a proxy (in particular, an upper
bound) for the log likehood of the observed cascades. They proposed a solution to this problem via a
6Published in Transactions on Machine Learning Research (11/2022)
dynamic programming algorithm. While the dynamic programming algorithm is an exact solution to their
formulation of the problem, it has a running time of Θ(n4), wherenis the total number of vertices in the
graph on which the observed cascade runs. This is prohibitively expensive for graphs of moderate to large
size. Furthermore, their formulation of the problem makes no comparison of the estimated clock with the
ground truth one, and thus there are no theoretical guarantees or empirical evaluations of the accuracy
of their estimator (which we call the maximum likelihood proxy (MLP) estimator ) as an approximation to
the ground truth clock. In contrast, the present work gives a rigorous formulation of the problem as one of
statistical estimation of the ground truth clock from observed cascades. We compare our proposed algorithm
and estimator with the MLP estimator in this framework in terms of both accuracy and running time.
1.4 Our contributions
Our contributions in this work are as follows:
•Novel problem formulation. We formulate a two-scale generative model for cascades, where the larger
time scale is parametrized by an arbitrary clock (which may itself be generated by a model with arbitrarily
many time scales). We formalize the problem of clock estimation from observed cascades with respect to a
natural distortion measure. This distortion measure allows us to quantify the proximity of estimated clocks
to the ground truth in a principled manner.
•Provably accurate and computationally eﬃcient solutions. We propose a linear-time algorithm
with provable approximation guarantees for the clock estimation problem. In particular, the distortion
between the true clock and our estimate tends to 0at a polynomial rate with respect to the average degree
of the graph.
•Generality. We ﬁrst prove our results in the context of Erdős-Rényi graphs in the semi-sparse regime
and the independent cascade model. The proofs for this setting contain almost all necessary ingredients
for extension of guarantees to a much more general setting: the results also hold when the cardinalities of
infection sets Sisatisfy a martingale diﬀerence property and the graph is sampled from a suﬃciently dense
sparse graphon model. We provide the remaining ingredients in Section 3.5 and culminate with a more
general theorem.
•Conﬁrmation of results in simulation on synthetic and real graphs. We bolster our theoretical
results via experiments on both real and synthetic graphs and synthetic cascades. We ﬁnd that the FastClock
estimator empirically outperforms the dynamic programming-based estimator from DiTursi et al. (2017) in
terms of accuracy and on all of our synthetic graphs and in a broad range of cascade model parameters on a
real graph, and substantially in terms of running time on all of our synthetic graphs and for all investigated
model parameters on a real graph. We give our simulation results in Section 3.4 and Appendix D.
2 Problem formulation and notation
Our goal in this section is to formulate our generative model and the problem of clock estimation . We give
examples of all deﬁnitions in Appendix C.
Preliminary deﬁnitions: We ﬁx a graph Gon the vertex set [n] ={1,...,n}, and we deﬁne the timeline
of lengthN, for any number N∈N, to be the set [[N]] ={0,1,...,N}. The ﬁrst ingredient of our framework
is acascade model . We will be concerned in the present work with compartmental models, wherein each node
may be susceptible (uninfected), active (i.e., infected and able to transmit the infection to its neighbors),
or infected (but unable to transmit its infection). We collect the diﬀerent possible compartments into a set
Ω ={S,A,I}. Nodes may carry additional information from an arbitrary set Ω/prime. At any given time t, a
nodev∈[n]is in some state Ψ(v,t)∈Ω×Ω/prime. We call a network state any element of (Ω×Ω/prime)n.
Deﬁnition 1 (Cascade model, observation model) .A (discrete-time) cascade model Cis a conditional
distribution PC(·|X), whereX∈(Ω×Ω/prime)nis a network state.
For the bulk of the paper, we will be concerned with the following model of observations of cascades: for
a sequence of network states (X0,X1,...,XN), we observe a corresponding sequence, called an infection
7Published in Transactions on Machine Learning Research (11/2022)
sequence:(Sobs(0),Sobs(1),...,Sobs(N)), whereSobs(j)consists of the set of vertices at time jthat move
into either compartment AorIaccording to Xj. Note that we do notobserve which vertices are active or
any of the side information. Any cascade model thus induces a probability distribution on infection sequences.
To begin to deﬁne our two-scale generative model, we next deﬁne a clock, which encodes the timesteps
belonging to each session. This will be a parameter of our model.
Deﬁnition 2 (Clock).A clockCwithN+1sessions on the timeline [[N/prime]]is a partition of [[N/prime]]intoN+1
closed subintervals (i.e., contiguous integer subsets) C0,...,CN. We call the jth such subinterval, for j= 0
toN, thejth session interval .
Given an observed infection sequence Sobs= (Sobs(0),...Sobs(N/prime)), a clockCinduces an infection sequence
Ssess = (Ssess(0),Ssess(1),...,Ssess(N)), whereSsess(j) =∪k∈CjSobs(k). We call this the infection
sequence induced by ConSobs.
For algorithmic purposes, we note that a clock may be encoded as a sequence of non-negative integers giving
its right interval endpoints.
Main deﬁnition of the generative model: We now deﬁne our main generative model. We ﬁx a cascade
modelC0, which will govern the dynamics of the process during each session. This is the small-scale model.
In general, it may depend on the length of the session.
Deﬁnition 3 (Two-scale generative model for cascades) .The two-scale generative model M(C,X 0,T) =
M(C)is parametrized by a clock C, an initial network state Z0∈(Ω×Ω/prime)n, and a number of sessions
T. Its output is a sequence (X0,X1,...,XN)of network states, inducing an observed infection sequence
(Sobs(0),...,Sobs(N)).
Forj= 1,...,T, at the beginning of the jth session, the network state is given by Zj−1. At each timestep t
in thejth session, a network state is sampled from C0, conditioned on the current network state. Any vertex
that is newly activated according to C0is set to infected in Xtand active in Zj. For convenience, we also
deﬁne the infection sequence Ssess= (Ssess(0),Ssess(1),...,Ssess(T)), whereSsess(j)consists of the set of
vertices infected during session j(soSsessis just the infection sequence induced by ConSobs). We call
Ssessthesession-level infection sequence .
Intuitively, in this model, vertices infected during a given session only become activein the next session.
This is justiﬁed in scenarios where nodes are not immediately infectious when they become infected. In this
case, sessions may be seen as periods during which a current set of active vertices causes infections.
Measuring distortion between clocks: To formulate the problem of estimating the clock of our model,
given an infection sequence produced by it, we need a means of measuring distortion between clocks. This
is our next goal.
An infection sequence Snaturally induces a partial order on the set of vertices: namely, for two vertices a,b,
a<bif and only if a∈Si,b∈Sjfor somei<j. Similarly, a clock Capplied to an infection sequence Sobs,
in the sense of Deﬁnition 2, induces a partial order. This partial order is the one induced by the infection
sequenceSthatCinduces on ˆS.
We will consider two clocks C0,C1to be equivalent with respect to a given observed infection sequence Sobs
if they induce the same partial order. The reason for this is that two equivalent clocks separate vertices in
the same way into a sequence of sessions. We will sometimes abuse terminology and use “clock” to mean
“clock equivalence class”.
We next deﬁne a distortion function on clock equivalence classes. This will allow us to measure how far
a given estimated clock is from the ground truth. Note that given an observed infection sequence Sobs, a
clock cannot reverse the order of any pair of events, so that the standard Kendall τdistance between partial
orders is not appropriate here.
Deﬁnition 4 (Distortion function on clock pairs) .Consider two clocks C0,C1with respect to an observed
infection sequence Sobs. We deﬁne DisC0,C1(i,j)to be the indicator that the clocks C0andC1order vertices
8Published in Transactions on Machine Learning Research (11/2022)
iandjdiﬀerently (i.e., that the partial order on vertices induced by Cbordersiandjand the partial order
induced by C1−bdoes not, for bequal to either 0or1). If the clocks in question are clear from context, we
may drop the subscript.
We deﬁne the following distortion measure on clock pairs:
dSobs(C0,C1) =1/parenleftbign
2/parenrightbig/summationdisplay
i<jDisC0,C1(i,j). (2)
Main problem statement: We ﬁnally come to the general problem that we would like to solve:
Deﬁnition 5 (Clockestimation) .Fix a graph G, a small-scale cascade model C0, and a clock C. An infection
sequenceSobs∼M (C)is generated on G. Our goal is to produce an estimator ˆC=ˆC(Sobs)ofCso as to
minimize E[dSobs(C,ˆC)]. This is called the clock estimation problem .
The above deﬁnition implicitly assumes knowledge of the parameters of the small-scale cascade model.
Estimation of these parameters has been studied extensively in the literature and need not necessarily come
from cascade observations. We discuss this in the appendix, in Section E. Furthermore, knowledge of the
initial conditions of the cascade is necessary in order to achieve an expected estimation error that tends to 0
in general. We thus assume that the initial network state is given to us. Under mild additional assumptions
on the model (e.g., that Ssess(0)consists of Θ(1)vertices chosen uniformly at random, and that the graph
is sparse, so that Ssess(0)is an independent set with high probability), the initial set Ssesscan be inferred
with high probability. We discuss how to do this at length in the appendix, in Section F.
Speciﬁc small-scale cascade model: Having deﬁned our multiscale generative model, we specify an
example small-scale cascade model for our problem. Our approach generalizes beyond this one, as we will
explain in Section 3.5.
We deﬁne the discretized independent cascade (IC) process.
Deﬁnition 6 (Discretized independent cascade process) .We ﬁx a graph G, an initial infection set S0of
vertices inG(given by elements of [n] ={1,...,n}), a number of timesteps K(giving the length of the session)
and probability parameters pnandpe, both in [0,1]. Here,pndenotes the probability of transmission of an
infection across an edge, and pedenotes the probability of infection from an external source. Additionally,
we ﬁx a probability distribution DonN(for simplicity, we will choose the geometric distribution with mean
1).
When a node vbecomes active, for every one of its susceptible neighbors w, it draws an independent
Bernoulli(pn)random variable. If it is 1,vstarts a timer by drawing a sample XfromD, then deﬁn-
ingτv,wto be the minimum of Xand the number of timesteps remaining in the session. The vertex vthen
becomes inactive (moves to the Istate). After τv,wtimesteps,wbecomes active.
In parallel, in a given timestep, each susceptible vertex becomes active with probability p/prime
e= 1−(1−pe)1/K,
independent of anything else. This models infection by external circumstances, with probability peover the
entire session.
We denote by Sjthe set of nodes that become active in timestep j. The process terminates either after K
steps or after all nodes are infected.
There is a natural connection between the discretized IC model and the IC model deﬁned in Kempe et al.
(2003) via our two-scale model: when the small-scale model is the discretized IC model, the sequence
Ssess= (Ssess(0),Ssess(1),...,Ssess(T))of sets of vertices infected in the sessions of our model is distributed
according to the model in Kempe et al. (2003). In that model, there is a seed set of vertices S0that are
active at time 0. In each timestep (our sessions), the vertices that were infected in the previous timestep
become active, and the vertices that were active in the previous timestep become inactive but infected. Each
active vertex chooses to infect each of its neighbors independently with probability pn. Furthermore, each
uninfected vertex becomes infected with probability pe.
9Published in Transactions on Machine Learning Research (11/2022)
3 Main results: Algorithm, approximation and running time guarantees, generality
Inthissection,wepresentourproposedalgorithm(Algorithm1)forclockestimation,whichwecall FastClock .
We give full proofs of all theorems in Appendix B.
FastClock takes as input a graph G, an observed infection sequence Sobs= (Sobs(0),...,Sobs(N)), and the
parameters θof the cascade model, including the initial infection set Ssess(0)(see our discussion of this
assumption in the previous section), but notincluding session lengths. The output of the algorithm is an
estimated clock ˆC, which takes the form of a sequence of interval right endpoints ˆt0,ˆt1,...,ˆtˆN∈[[N]], for
some ˆNand is an estimate of the ground truth clock Cspeciﬁed by t0,...,tN.
Our algorithm proceeds by iteratively computing the estimate ˆtj. In the (j+ 1)-st iteration, to compute
ˆtj+1, it chooses the size of the next interval of the clock so as to match as closely as possible the expected
number of newly infected nodes in the next session. We prove that the resulting clock estimate is very close,
in terms of dSobs(·,·), to the ground truth clock, using concentration inequalities.
The correctness of FastClock is based on the following intuition: if we manage to correctly estimate t0,...,tj,
then we can estimate the conditional expected number of vertices infected in the (j+ 1)-st session of the
process (i.e.,|Ssess(j+ 1)|). We can show a conditional concentration result for |Ssess(j+ 1)|around its
expectation. Thus, we output as our next clock interval endpoint ˆtj+1the smallest integer t≥ˆtjfor which
the number of vertices in/uniontextt
k=tj+1Sobs(t)does not exceed its conditional expectation, corrected by a small
quantity. This quantity is determined by the concentration properties of the random variable |Ssess(j+ 1)|
conditioned on the state of the process given by Sobs(0),...,Sobs(tj). We choose the threshold to be such
that, under this conditioning, the number of vertices infected in the next session is slightly less than it
with probability tending exponentially to 1. Our approximation analysis illustrates that the approximation
quality depends on the graph structure and the model parameters.
The signiﬁcance of the approximation and running time results (Theorems 1 and 2 below for the independent
cascade model) is that the clock parameter of our multiscale cascade model can be quickly estimated with
provably high accuracy using relatively simple expected value calculations. The generality of our results
(well beyond the IC model) is discussed in Section 3.5. As long as the expected number of nodes infected in
the next session can be calculated eﬃciently, the FastClock algorithm can be adapted to a wide variety of
cascade models.
3.1 The FastClock algorithm
Before we deﬁne our algorithm we introduce some necessary notation. For a session-level infection sequence
˜Sand a session index t∈|˜S|, deﬁneσt(˜S)to be theσ-ﬁeld generated by the event that the ﬁrst tsession-
level infection sets of the cascade process are given by ˜S0,˜S1,...,˜St. That is, the event in question is that
Ssess(0) = ˜S0,...,Ssess(t) =˜St. We also deﬁne µt(˜S)to beµt(˜S) =E[|Ssess(t+ 1)||σt(˜S)]. For a vertex
v, we denote byN(v)the set of neighbors of vinG. The algorithm is given in Algorithm 1.
After an initialization, the main loop in FastClock (Steps 5-11) iteratively determines the last infection
event in the next session of the process (whose endpoint timestep we are trying to estimate), by estimating
the expected number of nodes µtto be infected in that session (Step 6). The key step in this procedure is
the computation of µt, which we discuss next.
Computing µt(˜S)in the discretized IC model. Let us be more precise in specifying how to compute
µt(˜S) =E[|Ssess(t+ 1)||σt(˜S)]when the small-scale model is the discretized independent cascade model
(see Deﬁnition 6). A node can be infected in one of two ways: through external factors (governed by pe) or
via transmission from a vertex in ˜Stthrough an edge. In the latter case, the node must lie in the frontier
setFt(˜S), deﬁned as follows: Ft(˜S)is the set Ft(˜S) =N(˜St)\/uniontextt
j=0˜St;i.e., it is the set of neighbors of ˜St
that we believe to be uninfected at the beginning of cascade session t. See Example 5 in the appendix for
an illustration of the deﬁnition of frontier sets.
For a set of vertices W⊆[n]and a vertex v∈[n], let degW(v)denote the number of edges incident on v
that are also incident on vertices in W. We can use linearity of expectation to derive a closed-form formula
10Published in Transactions on Machine Learning Research (11/2022)
Algorithm 1: FastClock
Data:GraphG, small-scale cascade model parameters θ, observed infection sequence
Sobs= (Sobs(0),...,Sobs(N))
Result: An estimated clock ˆC.
// An initially empty list for the estimated clock. This will eventually contain a
sequence of estimated endpoints of clock intervals.
1SetˆC= ();
//t: index of the next estimated clock interval, i.e., tis an index in Ssess, the
session-level infection sequence.
//tobs: the index in Sobsof the beginning of the next estimated clock interval
2Sett= 1,tobs= min{j≤N:|∪j
k=0Sobs(k)|=Ssess(0)};
//˜S: the estimated infection sequence approximating the session-level sequence S.
3Set ˜S0=∪tobs
k=0Sobs(k);
4AppendtobstoˆC;
5whiletobs/negationslash=Ndo
// Compute the expected number µtof infected nodes in a single session of the
cascade process, starting from the state of the process estimated so far.
6Setµt=E[|Ssess(t+ 1)||σt(˜S0,˜S1,...,˜St)];
7Set
t/prime
obs=tobs+ max/braceleftBigg
∆|tobs+∆/summationdisplay
i=tobs+1|Sobs(i)|≤µt·(1 +µ−1/3
t)/bracerightBigg
(3)
8Appendt/prime
obstoˆC;
9Set˜St=∪t/prime
obs
i=tobs+1Sobs(i);
10Sett=t+ 1;
11Settobs=t/prime
obs;
12end
13return ˆC;
forµt(˜S):
µt(˜S) =pe·
n−|Ft(˜S)|−t/summationdisplay
j=0|˜Sj|
+/summationdisplay
v∈Ft(˜S)(pe+ (1−pe)(1−(1−pn)deg˜St(v))). (4)
A similar expression can be derived for the more general case where transmission probabilities across edges
may diﬀer from each other. The calculation of the summation/summationtextt
j=0|˜Sj|can be performed eﬃciently by
keeping track of its value in the t-th iteration of the loop of the algorithm. In the t-th iteration, the value of
the summation is updated by adding |˜St|to the running total. Note also that this estimation will be the only
diﬀerence in our algorithm when applied to alternative cascade models such as the linear threshold model.
3.2 Approximation guarantee for FastClock
Our ﬁrst theorem gives an approximation guarantee for FastClock in the case where the small-scale model
is the discretized IC model. It is subject to an assumption about the parameters of the graph model from
whichGis sampled and about the parameters of the small-scale cascade model, which we state next. It is,
however, important to note that FastClock itself does not assume anything about the graph. Furthermore,
we generalize our guarantees beyond these assumptions in Section 3.5, Theorem 3.
Assumption 1 (Assumptions on random graph model parameters) .We assume that G∼G(n,p)(i.e., that
Gis sampled from the Erdős-Rényi model), where psatisﬁes the following relation with the ground truth
11Published in Transactions on Machine Learning Research (11/2022)
number of sessions T+ 1:p=o(n−T
T+1).andp≥Clogn/n,for someC > 1. We substantially relax our
modeling assumptions in Section 3.5.
The former condition on pmay be viewed as a constraint on T. It is natural in light of the fact that, together
with our assumptions on pnandpebelow, it implies that the cascade does not ﬂood the graph, in the sense of
infecting a Θ(1)-fraction of nodes. Many cascades in practice do not ﬂood the graph in this sense. The lower
bounding condition on pimplies that the graph is connected with high probability. In terms of density, this
covers graphs in the semi-sparse regime, wherein the number of edges grows superlinearly with the number of
vertices. This is common in real networks with communities. We exhibit in Section H several real networks
with average degrees that are well within the bounds of our assumption.
Regarding the small-scale cascade process, we assume that it is the discretized IC model from Deﬁnition 6,
with the following constraints on the parameters: we assume that pnis some ﬁxed positive constant and that
pe=o(p). Our results also hold if pnis diﬀerent for every edge e(so thatpn=pn(e)), provided that there
are two positive constants 0<c0,c1<1such that for every edge e,pn(e)∈[c0,c1].
The assumption that pnis constant with respect to nis natural in the sense that, for many infectious processes,
the probability of transmission from one node to another should not depend on the number of nodes. The
assumption on pe, the probability of infection from an external source, is reasonable when the cascade is
overwhelmingly driven by network eﬀects, rather than external sources.
Theorem 1 (Main FastClock approximation theorem) .Suppose that Assumption 1 holds. We have, with
probability at least 1−e−Ω(np),
dSobs(C,ˆC) =O((np)−1/3), (5)
where we recall that Sobsis the observed infection sequence generated by the two-scale cascade model
parametrized by the ground truth clock C,Ssessis the session-level infection sequence, and ˆCis the es-
timate ofCthat is output by FastClock.
The proof of Theorem 1 uses an auxiliary result (Theorem 4 in the appendix, which we call the FastClock
utility theorem ) stating that with high probability, for every i, the intersection of the session-level infection
sequence element Ssess(i)with the estimated infection sequence element ˜Siis asymptotically equivalent in
cardinality to Ssess(i)itself. We prove this by induction on the session index i, which requires a careful design
oftheinductivehypothesis. Giventheutilitytheorem, theupperboundonthedistortion dSobs(C,ˆC)follows
by summing over all possible pairs Ssess(i),Ssess(j)of infection sequence elements in Ssess, the session-level
infection sequence, then summing over all vertex pairs u∈Ssess(i),v∈Ssess(j). This inner sum is
approximated using the utility theorem.
3.3 Running time analysis
We have a strong guarantee on the running time of FastClock in the independent cascade case. The run-
ning time of FastClock is asymptotically much smaller than that of the dynamic programming estimator
from DiTursi et al. (2017).
Theorem2 (RunningtimeofFastClock) .The FastClock algorithm for the case where the small-scale cascade
model is the discretized independent cascade model runs in time O(N+n+m), wheremis the number of
edges in the input graph.
3.4 Empirical results on synthetic graphs
In this section, we present empirical results on synthetic graphs and cascades. Our goal is to conﬁrm the
theoretical guarantees of FastClock and compare it to the dynamic programming (DP) algorithm optimizing
a proxy of the maximum likelihood for observed cascades proposed by DiTursi et al. (2017). Our comparative
analysis focuses on (i) distance of the estimated clock from the ground truth clock (see Deﬁnition 4) and (ii)
empirical running time of both techniques. More extensive empirical results on synthetic and real graphs
are included in the appendix, Section D.
12Published in Transactions on Machine Learning Research (11/2022)
1000 3000 5000 10000
number of nodes0.00.10.20.30.4distanceFastClock
DP
(a)
1000  3000  5000  10000
number of nodes10−210−1100101102runtimes (seconds)F astClock
DP (b)
0.01 0.05 0.1 0.15 0.2 0.25 0.3
pn0.00.10.20.30.4distanceFastClock
DP (c)
0.01  0.05  0.1  0.15  0.2  0.25  0.3
pn100101102103runtimes (seconds)F astClock
DP (d)
Figure 2: Comparison of the distance and runtime of the estimated clocks by FastClock and the baseline DP from DiTursi
et al. (2017) on Erdős–Rényi graphs (default parameters for all experiments: pn= 0.1,pe= 10−7,n= 3000,p=n−1/3, stretch
l= 2unless varying in the speciﬁc experiment). (a),(b): Varying graph size. (c),(d): Varying infection probability pn.
We generate synthetic graphs using the Erdős–Rényi model (experiments using stochastic block models are
included in Section D). We then generate synthetic cascades on each graph using the independent cascade
(IC) model. Since both algorithms under consideration are invariant to infection timers in the small-scale
model, we determine the set of vertices infected in each session according to IC model of Kempe et al. (2003),
then assign to each infected vertex a uniformly random infection time within its session, which we ﬁx to
have length l(some integer which we call the stretch factor of our cascade). As in our theorems, we denote
bySsessthe session-level infection sequence and by Sobsthe observed process infection sequence. These
implicitly specify a ground-truth clock C. We note that while all of our experiments involve sessions with
uniform length, our theoretical contributions are more general. We then employ both FastClock and the
maximum likelihood proxy algorithm to estimate the ground truth clock from Sobs. We draw 50samples for
each setting and report average and standard deviation for both running time and quality of estimations for
each setting.
Experiments on Erdős–Rényi graphs. We report a subset of the results of our experiment on Erdős-
Rényi graphs in Figure 2. With increasing graph size FastClock ’s distance from the ground truth clock
diminishes (as expected based on Theorem 1), while that of DP increases (Fig. 2(a)). Note that DP optimizes
a proxy to the cascade likelihood and in our experiments tend to associate too many early timesteps with
early sessions, which for large graph sizes results in incorrect recovery of the ground truth clock. Similarly,
FastClock ’s estimate quality is better than that of DP for varying on pn(Fig. 2(c)), graph density (Fig. 4(e)
in the appendix) and stretch factor for the cascades (Fig. 4(g) in the appendix), with distance from ground
truth close to 0for regimes aligned with the key assumptions we make for our main results (Assumption 1 or,
more generally, 2). In addition to superior accuracy, FastClock ’s running time scales linearly with the graph
size and is orders of magnitude smaller than that of DP for suﬃciently large instances (Figs. 2(b), 2(d)).
3.5 Generality of FastClock
We have shown that FastClock achieves small expected distortion on the clock recovery problem when the
small-scale model is as in Deﬁnition 6. However, the algorithm works substantially more generally – all that
is needed is concentration of the number of vertices infected in each session, given previous session, around
its conditional expectation, along with concentration of the frontier size in each session. In particular, the
model-dependent parts of the proof of Theorem 1 lie entirely in the proof of Theorem 4 and the associated
auxiliary lemmas.
Below, we formulate suﬃcient conditions on our model to guarantee these properties. Our discussion culmi-
nates in Assumption 2, which is used as the main hypothesis in Theorem 3.
Concentration of the number of vertices infected in each session A suﬃcient condition for the
required concentration property to hold is a martingale diﬀerence property : for any session index t, we
may write|Ssess(t)|as a sum of indicators of vertex infection events: letting X(v,t)denote the indicator
13Published in Transactions on Machine Learning Research (11/2022)
thatv∈Ssess(t), we have|Ssess(t)|=/summationtext
vX(v,t).We deﬁneZ(Ssess,t,v)as follows: we ﬁrst order the
vertices in F(Ssess,t)arbitrarily and denote them by v1,...,v|F(Ssess,t)|. Then we deﬁne Z(Ssess,t,j) =
E[|Ssess(t)||σt−1(Ssess),X(v1,t),...,X (vj,t)].
Then{Z(Ssess,t,j)}|F(Ssess,t)|
j=1 is the Doob martingale with respect to the natural ﬁltration on the vertex
infection events in the frontier up to vertex vj. By the Azuma-Hoeﬀding inequality, the desired concentration
property holds whenever
|Z(Ssess,t,j)−Z(Ssess,t,j−1)|=O(|F(S,t)|1/2−const), (6)
for allt,j, with probability 1, where the O(·)is uniform in all parameters and 0<const< 1/2. Intuitively,
this means that knowledge of whether or not a given susceptible vertex becomes infected in session tdoes not
substantially alter our best guess of the total number of vertices infected in that session. This encompasses
our IC-based model but is substantially more general, since it allows for, e.g., weak dependence among
infection events.
Concentration of the frontier size Concentration of the frontier size at each step is assured when G
is drawn from any random graph model coming from a sparse graphon (Borgs et al., 2017) Wwith density
parameterρnwithin the range of pspeciﬁed by Assumption 1 and with all entries bounded away from 0.
More precisely, all that is needed is concentration of the frontier size, conditioned on the latent positions of
all nodes. This concentration follows immediately from a Chernoﬀ bound.
To make this rigorous, we ﬁrst explain the sparse graphon framework. This is encapsulated in Deﬁnitions 7
and 8 below.
Deﬁnition 7 (Graphon) .A graphon is a symmetric, Lebesgue-measurable function W: [0,1]2→[0,1].
Deﬁnition 8 (Sparse random graph model associated with a graphon) .The sparse random graph model
G(W,ρn)with sparsity parameter ρn(a function of nwhose codomain is [0,1]) associated with the graphon
Wis the following distribution on graphs with nvertices{1,2,...,n}: ﬁrst,nnumbersX1,...,Xnare sampled
uniformly at random from [0,1]. Conditioned on these, an edge exists between vertices i,jindependently of
anything else with probability ρn·W(Xi,Xj).
We can now formulate the more general assumptions under which we can prove an accuracy guarantee for
FastClock.
Assumption 2 (General assumptions on the random graph model and the small-scale model) .We assume
that the graph Gis sampled from a sparse graphon G(W,ρn), whereρnsatisﬁes the same properties that
pdoes in Assumption 1. Furthermore, we assume that there exist two constants p0,p1∈(0,1)such that
p0≤W(x,y)≤p1for allx,y∈[0,1]. We denote p=p0·ρn.
We assume that the small-scale cascade model C0satisﬁes the following conditions:
a) The martingale diﬀerence condition described in (6).
b) At any timestep tand for any vertex vin the current frontier, the conditional probability that v
becomes infected during timestep tdepends only on the set of its active neighbors.
c) There exists a universal constant c0>0such that in any timestep, the probability of infection of
any frontier vertex is bounded below by c0.
Then our generalized accuracy guarantee for FastClock is given in the following theorem.
Theorem 3 (FastClock accuracy guarantee, generalized) .Suppose that Assumption 2 holds. We have, with
probability at least 1−e−Ω(nρn),
dSobs(C,ˆC) =O((nρn)−1/3). (7)
We note that Theorem 1 is a consequence of Theorem 3. To show that this is the case, it suﬃces to check
that Assumption 2 follows from Assumption 1. Since Gis sampled from an Erdős-Rényi model with edge
14Published in Transactions on Machine Learning Research (11/2022)
probability p=p0ρn, it is equivalently a sample from a sparse graphon model WsatisfyingW(x,y) =p0
for allx,y∈[0,1], and with ρnas a sparsity parameter. Regarding the assumptions about the small-scale
model, assumptions (b) and (c) hold for the IC model trivially: speciﬁcally, the constant c0is simply the
minimum transmission probability for any edge. Assumption (a), the martingale diﬀerence condition, is also
checked simply: in the IC model, the terms of the sum/summationtext
jX(v,t)deﬁning|Ssess(t)|are independent, and so
it may be checked that |Z(Ssess,t,j)−Z(Ssess,t,j−1)|≤|X(vj,t)−E[X(vj,t)|σt−1(Ssess)]|≤1, where
the last step is because X(vj,t)is an indicator function. This martingale diﬀerence bound is substantially
less than the required one . This completes the proof that Assumption 2 follows from Assumption 1.
4 Conclusions and future work
We have formulated a generative model and statistical estimation framework for network spreading processes
whose activity periods (which we called sessions) are intermittent. The model is parametrized by a clock,
which encodes the large-scale behavior of the process. We showed that this clock can be estimated with
high accuracy and low computational cost, subject to certain natural constraints on the structure of the
underlying graph and on the small-scale cascade model: in essence, these must be such that the graph is an
expander with appropriate parameters; that, conditioned on an estimated current state of the process at any
time, the expected number of vertices infected in the next session is immune to small errors in the estimated
state; and that the number of vertices infected in the next session is well-concentrated around its conditional
expected value. We empirically showed that the FastClock algorithm is superior in accuracy and running
time to the current state of the art dynamic programming algorithm. Furthermore, unlike this baseline,
FastClock comes with theoretical accuracy guarantees. Our results hold for a broad class of small-scale
cascade models, provided that they satisfy a certain martingale diﬀerence property. Furthermore, the class
of random graph models for which our guarantees hold is similarly broad.
We intend to pursue further work on this problem: most pressingly, our empirical results and intuition
derived from our theorems indicate that FastClock may not perform accurately when the graph contains
very sparse cuts (so that it is not an expander graph, and it cannot have been generated by a sparse graphon
model with the assumed parameter ranges, except with very small probability). Our empirical evidence is
consistent with the conjecture that the degradation of performance on real networks in comparison to the
DP algorithm is a result of sparse cuts. In such graphs, we envision that FastClock can be used within
communities as a subroutine of a more complicated algorithm. Further work is needed to determine whether
accuracy and computational eﬃciency can be achieved for such graphs. Additionally, our model can be
extended to the case where diﬀerent subsets of nodes behave according to distinct clocks. It is important to
clarify the information-theoretic limits of clock estimation and related hypothesis testing questions in this
scenario.
5 Acknowledgements
This research is funded by an academic grant from the National Geospatial-Intelligence Agency (Award
No. # HM0476-20-1-0011, Project Title: Optimizing the Temporal Resolution in Dynamic Graph Mining).
Approved for public release, NGA-U-2022-02239. The work was also supported in part by the NSF Smart
and Connected Communities (SC&C) CMMI grant 1831547 and by NSF CCF grant number #2212327.
References
Emmanuel Abbe, Sanjeev Kulkarni, and Eun Jee Lee. Nonbacktracking bounds on the inﬂuence in
independent cascade models. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems , vol-
ume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf .
Lada A. Adamic, Thomas M. Lento, Eytan Adar, and Pauline C. Ng. Information evolution in social
networks. In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining ,
15Published in Transactions on Machine Learning Research (11/2022)
WSDM ’16, pp. 473–482, New York, NY, USA, 2016. Association for Computing Machinery. ISBN
9781450337168. doi: 10.1145/2835776.2835827. URL https://doi.org/10.1145/2835776.2835827 .
Junaid Ali, Mahmoudreza Babaei, Abhijnan Chakraborty, Baharan Mirzasoleiman, Krishna P. Gummadi,
and Adish Singla. On the fairness of time-critical inﬂuence maximization in social networks, 2019.
Eytan Bakshy, Itamar Rosenn, Cameron Marlow, and Lada Adamic. The role of social networks in infor-
mation diﬀusion. In Proceedings of the 21st international conference on World Wide Web , pp. 519–528,
2012.
Petko Bogdanov, Michael Busch, Jeﬀ Moehlis, Ambuj K. Singh, and Boleslaw K. Szymanski. The social
mediagenome: Modelingindividualtopic-speciﬁcbehaviorinsocialmedia. 2013 IEEE/ACM International
Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013) , pp. 236–242, 2013.
Christian Borgs, Jennifer T. Chayes, Henry Cohn, and Nina Holden. Sparse exchangeable graphs and their
limits via graphon processes. J. Mach. Learn. Res. , 18:210:1–210:71, 2017. URL http://jmlr.org/
papers/v18/16-421.html .
Wei Chen, Wei Lu, and Ning Zhang. Time-critical inﬂuence maximization in social networks with time-
delayed diﬀusion process. In Proceedings of the Twenty-Sixth AAAI Conference on Artiﬁcial Intelligence ,
AAAI’12, pp. 592–598. AAAI Press, 2012a.
Wei Chen, Wei Lu, and Ning Zhang. Time-critical inﬂuence maximization in social networks with time-
delayed diﬀusion process. In Proceedings of the Twenty-Sixth AAAI Conference on Artiﬁcial Intelligence ,
AAAI’12, pp. 592–598. AAAI Press, 2012b.
Justin Cheng, Lada Adamic, P. Alex Dow, Jon Michael Kleinberg, and Jure Leskovec. Can cascades be
predicted? In Proceedings of the 23rd International Conference on World Wide Web , WWW ’14, pp.
925–936, New York, NY, USA, 2014. Association for Computing Machinery. ISBN 9781450327442. doi:
10.1145/2566486.2567997. URL https://doi.org/10.1145/2566486.2567997 .
Abir De, Isabel Valera, Niloy Ganguly, Sourangshu Bhattacharya, and Manuel Gomez-Rodriguez. Learning
and forecasting opinion dynamics in social networks. In Proceedings of the 30th International Conference
on Neural Information Processing Systems , NIPS’16, pp. 397–405, Red Hook, NY, USA, 2016. Curran
Associates Inc. ISBN 9781510838819.
Daniel J DiTursi, Gregorios A Katsios, and Petko Bogdanov. Network clocks: Detecting the temporal scale
of information diﬀusion. In 2017 IEEE International Conference on Data Mining (ICDM) , pp. 841–846.
IEEE, 2017.
Daniel J DiTursi, Carolyn S Kaminski, and Petko Bogdanov. Optimal timelines for network processes. In
2019 IEEE International Conference on Data Mining (ICDM) , pp. 1024–1029. IEEE, 2019.
Rashad Eletreby, Yong Zhuang, Kathleen Carley, Osman Yagan, and H. Vincent Poor. The eﬀects of
evolutionaryadaptationsonspreadingprocessesincomplexnetworks. Proceedings of the National Academy
of Sciences , 117:201918529, 03 2020. doi: 10.1073/pnas.1918529117.
Cheng-Der Fuh. SPRT and CUSUM in hidden Markov models. The Annals of Statistics , 31(3):942 – 977,
2003. doi: 10.1214/aos/1056562468. URL https://doi.org/10.1214/aos/1056562468 .
David Kempe, Jon Kleinberg, and Éva Tardos. Maximizing the spread of inﬂuence through a social network.
InProceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, KDD ’03, pp. 137–146, New York, NY, USA, 2003. Association for Computing Machinery. ISBN
1581137370. doi: 10.1145/956750.956769. URL https://doi.org/10.1145/956750.956769 .
Eun Jee Lee, Sudeep Kamath, Emmanuel Abbe, and Sanjeev R. Kulkarni. Spectral bounds for independent
cascade model with sensitive edges. In 2016 Annual Conference on Information Science and Systems
(CISS), pp. 649–653, 2016. doi: 10.1109/CISS.2016.7460579.
16Published in Transactions on Machine Learning Research (11/2022)
Andrea Montanari and Amin Saberi. The spread of innovations in social networks. Proceedings of the
National Academy of Sciences , 107(47):20196–20201, 2010.
Sang Park, Benjamin Bolker, David Champredon, David Earn, Michael Li, Joshua Weitz, Bryan Grenfell,
and Jonathan Dushoﬀ. Reconciling early-outbreak estimates of the basic reproductive number and its
uncertainty: framework and applications to the novel coronavirus (sars-cov-2) outbreak. Journal of The
Royal Society Interface , 17:20200144, 07 2020. doi: 10.1098/rsif.2020.0144.
Jieun Shin, Lian Jian, Kevin Driscoll, and François Bar. The diﬀusion of misinformation on social media:
Temporal pattern, message, and source. Computers in Human Behavior , 83:278–287, 2018.
Venugopal V. Veeravalli and Taposh Banerjee. Chapter 6 - quickest change detection. In Abdelhak M.
Zoubir, Mats Viberg, Rama Chellappa, and Sergios Theodoridis (eds.), Academic Press Library in Signal
Processing: Volume 3 , volume 3 of Academic Press Library in Signal Processing , pp. 209–255. Elsevier,
2014. doi: https://doi.org/10.1016/B978-0-12-411597-2.00006-0. URL https://www.sciencedirect.
com/science/article/pii/B9780124115972000060 .
A Glossary of notation
Here we collect the notation that is used in the main body of the paper and in the proofs in the appendix.
1.N(S): Neighborhood of the set Sof vertices in a given graph.
2.Ssess= (Ssess(0),Ssess(1),...,Ssess(T))– A session-level infection sequence with T+ 1sessions.
EachSsess(j)is a subset of vertices, and Ssess(i)∩Ssess(j) =∅fori/negationslash=j. We denote by|Ssess|the
number of sessions of Ssess:T+ 1.
3.Sobs= (Sobs(0),Sobs(1),...,Sobs(N))– The observed infection sequence, as generated by our multi-
scale cascade model.
4.C– The ground-truth clock in our estimation problem.
5.ˆC– The clock estimated by our algorithm.
6.˜S– The estimate of the session-level infection sequence Ssessinduced by our estimate ˆCof the clock
Capplied to the observed infection sequence Sobs.
7.σt(S), for an infection sequence Sand a timestep index t∈|S|– Theσ-ﬁeld generated by the event
that the ﬁrst tsession-level infection sets of the process are given by S0,...,St.
8.µt(S), for an infection sequence Sand a timestep index t∈|S|–E[|Ssess(t+ 1)||σt(S)]. This is the
expected number of vertices infected in the t+ 1st session, given the session-level infection sequence
up to and including timestep t.
9.N– The index of the last observed infection set. That is, |Sobs|=N+ 1.
10.T– The index of the last session-level infection set. That is, |Ssess|=T+ 1.
11.n– The size of the graph.
12.pn– The probability in the IC model of transmission across an edge in a single timestep.
13.pe– The probability of infection of a vertex in a single timestep by a non-network source.
14.R(S,i)– For an infection sequence Sand an index i, deﬁne the ithrunning sum to be
R(S,i) =/uniondisplay
j≤iSj. (8)
17Published in Transactions on Machine Learning Research (11/2022)
15.Fi(S) =F(S,i)– For an infection sequence Sand an index i∈{0,1,...,|S|}, deﬁne the ith frontier
set to be
F(S,i) =N(Si)\R(S,i). (9)
Theith frontier with respect to Sis the set of neighbors of vertices infected in session ithat have
not infected by the end of session i.
16.CF(S,i)– The candidate frontier set at the end of session iin infection sequence S. That is, this is
CF(S,i) = [n]\R(S,i). (10)
Note that F(S,i)⊆CF (S,i).
17.CCF(S,˜S,i,j )– The common candidate frontier:
CCF(S,˜S,i,j ) =CF(S,i)∩CF (˜S,j). (11)
B Proofs
In this section, we give full proofs of all results.
B.1 Proof of Theorem 1
To prove the main FastClock approximation theorem, we start by characterizing the growth of µi(Ssess)and
|Ssess(i)|as a function of nandi. Note that this is a result about the independent cascade process, not the
FastClock algorithm.
Lemma 1 (Growth of µi(Ssess)and|Ssess(i)|).We have that, with probability at least 1−e−np, for all
i≤T,
µi(Ssess) = Θ((np)i+1), (12)
where the Θ(·)is uniform in i. Furthermore, with probability at least 1−e−np, we have
|Ssess(i)|= Θ((np)i) (13)
for everyi.
Proof.We prove this by induction on iand use the formula (4) throughout.
Base case ( i= 0):In the base case, we are to verify that µ0(Ssess) = Θ(np). The ﬁrst term of (4) is
non-negative and at most pe·n. By our assumption, we have that pe=o(pn), implying that the ﬁrst term
iso(np). Thus, it remains for us to show that the second sum is Θ(np). The dominant contribution comes
from the second term of each term of the sum:
/summationdisplay
v∈F0(Ssess)(pe+ (1−pe)(1−(1−pn))degSsess(0)(v)) = Θ(/summationdisplay
v∈F0(Ssess)1−(1−pn)degSsess(0)(v)) (14)
= Θ(|F0(Ssess)|−/summationdisplay
v∈F0(Ssess)(1−pn)degSsess(0)(v)).(15)
In the ﬁnal expression above, the remaining sum is lower bounded by 0and upper bounded by C|F0(Ssess)|
for some constant C < 1, since each term is between 0andC. Thus, we have shown that, with probability
exactly 1,
µ0(Ssess) = Θ(|F0(Ssess)|) +o(np). (16)
18Published in Transactions on Machine Learning Research (11/2022)
Since|F0(Ssess)|is the set of uninfected neighbors of all vertices in Ssess(0), and, by assumption, |Ssess(0)|=
Θ(1), we have that with probability at least 1−e−np,
|F0(Ssess)|= Θ(np). (17)
Thus, we have
µ0(Ssess) = Θ(np) (18)
with probability≥1−e−np. Conditioning on this event (which is only an event dealing with the graph struc-
ture), we have that |Ssess(1)|∼Binomial(Θ( np),pn), and a Chernoﬀ bound gives us that with probability
1−e−Ω(np),|Ssess(1)|= Θ(np), as desired. This completes the proof of the base case.
Induction ( i > 0, and we verify the inductive hypothesis for i):We assume that µj(Ssess) =
Θ((np)j+1)and|Ssess(j+ 1)|= Θ((np)j+1)forj= 0,1,...,i−1. We must verify that it holds for j=i,
with probability at least 1−e−np. As in the base case, the ﬁrst term of (4) is O(npe)/lessmuchnp/lessmuch(np)i+1.
It is, therefore, negligible with probability 1. The second term again provides the dominant contribution
and is easily seen to be Θ(|Fi(Ssess)|), just as in the base case. Thus, it remains to show that |Fi(Ssess)|=
Θ((np)i+1)with probability at least 1−e−Ω(np), which implies the desired result for µi(Ssess). The inductive
hypothesis implies that |Ssess(i)|= Θ((np)i), and the number of uninfected vertices is n−/summationtexti
j=0|Ssess(j)|=
n−Θ((np)i+1). Sincei≤T−1, this is asymptotically equivalent to n.
Now, conditioned on the ﬁrst ielements of Ssess, theith frontier|Fi(Ssess)|∼Binomial(n·(1−o(1)),1−
(1−p)|Ssess(i)|). Thus, with probability at least 1−e−Ω((np)i), we have
|Fi(Ssess)|= Θ(n·(1−(1−p)|Ssess(i)|)). (19)
Now,
1−(1−p)|Ssess(i)|= 1−(1−p)(np)i. (20)
Sincep=o(1), we have
1−(1−p)(np)i∼1−e−pi+1ni(21)
Now, using the fact that the fact that p=o(n−T
T+1), we have
pi+1ni=o(n−T
T+1(i+1)+i), (22)
from our assumption on the growth of p. Thus, in particular,
pi+1ni=o(1). (23)
This implies that
1−e−pi+1ni= 1−(1−pi+1ni)(1 +O(pi+1ni)) =pi+1ni(1 +o(1)). (24)
Thus, with probability at least 1−e−Ω((np)i),
|Fi(Ssess)|= Θ((np)i+1), (25)
which implies that
µi(Ssess) = Θ((np)i+1). (26)
By concentration of |Ssess(i)|, we then have that with probability at least 1−e−Ω(µi(Ssess)),
|Ssess(i)|= Θ((np)i+1), (27)
as desired.
19Published in Transactions on Machine Learning Research (11/2022)
Completing the proof LetGibe the event that the inductive hypothesis holds for index i= 0,1,...,T−1.
Then we have
Pr[∩i≥0Gi] = Pr[G0]·/productdisplay
i≥1Pr[Gi| ∩i−1
j=0Gj]≥T−1/productdisplay
i=0(1−e−Ω((np)i))1−e−Ω((np)). (28)
This completes the proof.
Next, we state and prove a utility theorem (Theorem 4 below). To state it, we need some notation: our
estimated clock ˆCinduces an estimate ˜Sof the ground truth session-level infection sequence Ssess. In
particular, ˜Sis the unique infection sequence such that distorting ˜Saccording to ˆCyieldsSobsas an
observed infection sequence.
Theorem 4 (Main FastClock analysis utility theorem) .We have that with probability 1−e−Ω(np), for every
i≤T−1,
|Ssess(i)∩˜Si|=|Ssess(i)|·(1−O((np)−1/3)). (29)
Wewillprovethistheorembyinductionon i. Theinductivehypothesisneededissubtle,asanaivehypothesis
is too weak. To formulate it and to prove our result, we need some notation: for an infection sequence W,
we deﬁne the ith running sum to be
R(W,i) =i/uniondisplay
j=0Wj. (30)
We deﬁne the frontierandrunning sum discrepancy sets between two session-level infection sequences S,ˆS
as follows:
∆F(S,ˆS,i,j ) =Fi(S)/triangleFj(ˆS) (31)
∆R(S,ˆS,i,j ) =R(S,i)/triangleR(ˆS,j), (32)
where/triangledenotes the symmetric diﬀerence between two sets.
We deﬁne the candidate frontier at indexiin infection sequence Sto be
CF(S,i) = [n]\R(S,i). (33)
This is the set of vertices that are not yet infected after index i.
We deﬁne the common candidate frontier to be
CCF(S,ˆS,i,j ) =CF(S,i)∩CF (ˆS,j). (34)
With this notation in hand, we deﬁne the following inductive hypotheses:
Hypothesis 1. There is a small discrepancy between the running sums of the true and estimated clocks:
||R(Ssess,i)|−|R(˜S,i||≤f1(n,i), (35)
where we set, with foresight, f1(n,i) =µi−1(Ssess).66=o(µi−1(Ssess)2/3).
Hypothesis 2. There is a small discrepancy between Ssess(i)and ˜Si:
1−|Ssess(i)∩˜Si|
|Ssess(i)|≤f2(n,i), (36)
where we set, with foresight, f2(n,i) =D·µi−1(Ssess)−1/3, for some large enough constant D.
20Published in Transactions on Machine Learning Research (11/2022)
We will use these to prove Theorem 4. The base case and inductive steps are proven in Propositions 1 and 2
below. First, we start by proving an upper bound (Theorem 5) on the following diﬀerence:
|µi(Ssess)−µi(˜S)|. (37)
In essence, the upper bound says that at any given clock time step, the expected number of nodes infected
in the next timestep is almost the same according to both the true and estimated clock. This will later be
used to verify the two inductive hypotheses stated above.
Theorem 5 (Upper bound on (37)) .Granted the inductive hypotheses explained above, we have that
|µi(Ssess)−µi(˜S)|≤pµi−1(Ssess)2/3µi(Ssess), (38)
with probability≥1−e−Ω(µi(Ssess)).
Proof.To upper bound (37), we apply the triangle inequality to (4) to get
|µi(Ssess)−µi(˜S)|≤pe·/vextendsingle/vextendsingle|Fi(Ssess)|−|Fi(˜S)|/vextendsingle/vextendsingle (39)
+pe/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglei/summationdisplay
j=0|˜Sj|−i/summationdisplay
j=0|Ssess(j)|/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(40)
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
v∈Fi(Ssess)Q(i,Ssess,v)−/summationdisplay
v∈Fi(˜S)Q(i,˜S,v)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle, (41)
whereQ(i,Ssess,v) =pe+ (1−pe)(1−(1−pn)degSsess(i)(v)).
We will upper bound each of the three terms (39), (40), and (41) separately.
Upper bounding (39) by O(pe|Fi(Ssess)|):We ﬁrst note that
/vextendsingle/vextendsingle|Fi(Ssess)|−|Fi(˜S)|/vextendsingle/vextendsingle≤|∆F(Ssess,˜S,i,i )|. (42)
So it is enough to upper bound the frontier discrepancy set cardinality. In order to do this, we decompose
it as follows:
|∆F(Ssess,˜S,i,i )|=|∆F(Ssess,˜S,i,i )∩∆R(Ssess,˜S,i,i )|+|∆F(Ssess,˜S,i,i )∩CCF (Ssess,˜S,i,i )|.(43)
This decomposition holds for the following reason: let vbe a vertex in the frontier discrepancy set
∆F(Ssess,˜S,i,i ). Suppose, further, that vis not in the common candidate frontier for Ssess(i),˜Si(so it
does not contribute to the second term on the right-hand side of (43)). We will show that it must be a
member of ∆R(Ssess,˜S,i,i ), which will complete the proof of the claimed decomposition. Then vmust be
a member of at least one of R(Ssess,i),R(˜S,i)(i.e., it must already be infected in at least one of these). If
it were a member of both, then it would not be a member of either frontier, so it could not be a member
of the frontier discrepancy set. Thus, vis only a member of one of R(Ssess,i)orR(˜S,i). This implies that
v∈∆R(Ssess,˜S,i,i ). This directly implies the claimed decomposition (43).
Wenowcomputetheexpectedvalueofeachtermoftheright-handsideof(43), wheretheexpectationistaken
with respect to the graph G. After upper bounding the expectations, standard concentration inequalities
will complete our claimed bound on the size of the frontier discrepancy set.
In the ﬁrst term, the size of the intersection of the frontier discrepancy with the running sum discrepancy
is simply the number of vertices in the running sum discrepancy set that have at least one edge to some
vertex inSsess(i)(here we assume, without loss of generality, that |R(Ssess,i)|≤|R(˜S,i)|). Using linearity
of expectation, the expected number of such vertices is
E[|∆F(Ssess,˜S,i,i )∩∆R(Ssess,˜S,i,i )|] =|∆R(Ssess,˜S,i,i )|·(1−(1−p)|Ssess(i)|). (44)
21Published in Transactions on Machine Learning Research (11/2022)
Here (1−(1−p)|Ssess(i)|)is the probability that, for a ﬁxed vertex w∈∆R(Ssess,˜S,i,i ), there is at least
one edge between wand some vertex in Ssess(i).
We compute the expected value of the second term of (43) as follows.
We claim that
∆F(Ssess,˜S,i,i )∩CCF (Ssess,˜S,i,i )⊆CCF (Ssess,˜S,i,i )∩(N(∆R(Ssess,˜S,i,i ))\N(Ssess(i))).(45)
To show this, let v∈∆F(Ssess,˜S,i,i )∩CCF (Ssess,˜S,i,i ). The fact that vis in the frontier discrepancy
set means that it has an edge to exactly one of Ssess(i),˜Si. This implies that it has an edge to the running
sum discrepancy set. Recalling that we assumed wlog that |R(Ssess,i)|≤|R(˜S,i)|, we must have that
∆R(Ssess,˜S,i,i )∩Ssess(i) =∅, and so we must also have that there are no edges from vtoSsess(i). This
completes the proof of the claimed set inclusion. This implies that
E[|∆F(Ssess,˜S,i,i )∩CCF (Ssess,˜S,i,i )|]≤E[|CCF (Ssess,˜S,i,i )∩(N(∆R(Ssess,˜S,i,i ))\N(Ssess(i)))|].
(46)
As above, the expectation is taken with respect to the random graph G.
For a single vertex in the common candidate frontier, the probability that it lies in the frontier discrepancy
set is thus at most
(1−(1−p)|∆R(Ssess,˜S,i,i)|)·(1−p)|Ssess(i)|. (47)
Thus, using linearity of expectation, the expected size of the second term in (43) is upper bounded by
E[|∆F(Ssess,˜S,i,i )∩CCF (Ssess,˜S,i,i )||σi(Ssess)] (48)
≤|CCF (Ssess,˜S,i,i )|·(1−(1−p)|∆R(Ssess,˜S,i,i)|)·(1−p)|Ssess(i)|. (49)
Combining (44) and (49) and deﬁning q= 1−p, we have the following expression for the expected size of
the frontier discrepancy set:
E[|∆F(Ssess,˜S,i,i )|] (50)
=|∆R(Ssess,˜S,i,i )|·(1−q|Ssess(i)|) (51)
+|CCF (Ssess,˜S,i,i )|·(1−q|∆R(Ssess,˜S,i,i)|)·q|Ssess(i)|. (52)
We would like this to be O(E[|Fi(Ssess)||σi(Ssess)]). Note that E[|Fi(Ssess)||σi(Ssess)]can be expressed
as follows:
E[|Fi(Ssess)||σi(Ssess)] = (|∆R(Ssess,˜S,i,i )| (53)
+|CCF (Ssess,˜S,i,i )|)·(1−q|Ssess(i)|). (54)
The intuition behind (50) being O(E[|Fi(Ssess)||σi(Ssess)])is as follows: the ∆Rterm is exactly the same as
in (53). However, this term is negligible compared to the common candidate frontier term in both expected
values. The second term, (52), can be asymptotically simpliﬁed as follows: we have
1−q|∆R(Ssess,˜S,i,i)|= 1−(1−p)|∆R(Ssess,˜S,i,i)|(55)
∼1−(1−p)·|∆R(Ssess,˜S,i,i )|) (56)
=p·|∆R(Ssess,˜S,i,i )| (57)
=p|Ssess(i)|·|∆R(Ssess,˜S,i,i )|
|Ssess(i)|(58)
∼(1−q|Ssess(i)|)·|∆R(Ssess,˜S,i,i )|
|Ssess(i)|. (59)
Here, we have used the following facts:
22Published in Transactions on Machine Learning Research (11/2022)
•Fortheﬁrstasymptoticequivalence,weusedthefactthat p|∆R(Ssess,˜S,i,i )|=o(1). Moreprecisely,
we have from the inductive hypothesis that
|∆R(Ssess,˜S,i,i )|=o(µi−1(Ssess)0.66) =o((np)i·0.66), (60)
so we have
p|∆R(Ssess,˜S,i,i )|=o(p0.66i+1n0.66i) =o(n−T/(T+1)+0.66i/(T+1)), (61)
which is polynomially decaying in n.
•For the second asymptotic equivalence, we used the fact that p|Ssess(i)|=o(1). More precisely, this
comes from the fact that
p|Ssess(i)|=O(p(np)i) =O(pi+1ni). (62)
Now, we use the fact that p=o(n−T
T+1):
pi+1ni=o(n−T
T+1(i+1)+i), (63)
from our assumption on the growth of p. Now, we need to show that the exponent is suﬃciently
negative and bounded away from 0.
−T
T+ 1(i+ 1) +i=−T·(i+ 1) +i·(T+ 1)
T+ 1=−T+i
T+ 1≤−1
T+ 1. (64)
We have used the fact that i≤T−1. Now, the constraints that we imposed on pimply that
T=o(logn), so
n−1
T+1=e−logn
T+1=o(1), (65)
as desired, since the exponent tends to −∞asn→∞.
Let us be more precise about what we proved so far. We have
E[|∆F(Ssess,˜S,i,i )||σi(Ssess)]∼(1−q|Ssess(i)|)·|CCF (Ssess,˜S,i,i )|·/parenleftbigg|∆R|
|CCF|+|∆R|
|Ssess(i)|q|Ssess(i)|/parenrightbigg
.
(66)
Meanwhile,
E[|Fi(Ssess)||σi(Ssess)] = (1−q|Ssess(i)|)·|CCF|·/parenleftbigg
1 +|∆R|
|CCF|/parenrightbigg
. (67)
We have that
E[|∆Fi||σi(Ssess)]
E[|Fi||σi(Ssess)]∼|∆R|
|CCF|+|∆R|
|Ssess(i)|·q|Ssess(i)|
1 +|∆R|
|CCF|. (68)
This can be simpliﬁed as follows:
E[|∆Fi||σi(Ssess)]
E[|Fi||σi(Ssess)]∼|∆R|
|CCF|+|∆R|
|Ssess(i)|·q|Ssess(i)|
1 +|∆R|
|CCF|=|∆R|·/parenleftBig
1 +|CCF|
|Ssess(i)|q|Ssess(i)|/parenrightBig
|CCF| +|∆R|.(69)
This can be upper bounded as follows, by distributing in the numerator and upper bounding |∆R|by
|∆R|+|CCF|in the numerator of the resulting ﬁrst term:
|∆R|·/parenleftBig
1 +|CCF|
|Ssess(i)|q|Ssess(i)|/parenrightBig
|CCF| +|∆R|≤1 +|∆R|·|CCF|q|Ssess(i)|
|Ssess(i)|(|CCF| +|∆R|). (70)
23Published in Transactions on Machine Learning Research (11/2022)
We can further upper bound by noticing that |CCF| +|∆R|≥|CCF| , so
E[|∆Fi||σi(Ssess)]
E[|Fi||σi(Ssess)]≤1 +|∆R|
|Ssess(i)|. (71)
Now, by our inductive hypothesis, we know that |∆R|i=o(µi−1(Ssess)0.66), and by concentration, we know
that|Ssess(i)|= Θ(µi−1(Ssess)). Thus, we have
E[|∆Fi||σi]
E[|Fi||σi]≤1 +|∆R|
|Ssess(i)|= 1 +o(µi−1(Ssess)−(1−0.66)) =O(1). (72)
Thus,
E[|∆F(Ssess,˜S,i,i )||σi(Ssess)] =O(E[|Fi(Ssess)||σi(Ssess)]). (73)
Now, remember that our goal is to show that |∆F(Ssess,˜S,i,i )|=O(|Fi(Ssess)|)with high probability,
conditioned on σi(Ssess). This follows from the expectation bound above and the fact that the size of the
frontier in both clocks is binomially distributed, so that standard concentration bounds apply. This results
in the following:
pe|∆Fi|=O(pe|Fi|) (74)
with conditional probability at least 1−e−Ω((np)i).
Upper bounding (40) by o(pe|R(Ssess,i)|):To upper bound (40), we note that
i/summationdisplay
j=0|Ssess(j)|=|R(Ssess,i)|, (75)
and an analogous identity holds with ˜Sin place ofSsess. Moreover,
/vextendsingle/vextendsingleR(Ssess,i)−R(˜S,i)/vextendsingle/vextendsingle=|∆R(Ssess,˜S,i,i )|. (76)
Thus, we have
(40) =pe|∆R(Ssess,˜S,i,i )|≤pef1(n,i), (77)
where the inequality is by the inductive hypothesis. We want this to be o(pe·|R(Ssess,i)|), which means
that we want|∆R(Ssess,˜S,i,i )|=o(|R(Ssess,i)|). This follows from the inductive hypothesis. In particular,
we know that|R(Ssess,i)|≥|Ssess(i)|, sinceSsess(i)⊂R(Ssess,i). Furthermore, we have by the inductive
hypothesis that|∆R(Ssess,˜S,i,i )|=o(µi−1(Ssess)0.66) =o(|Ssess(i)|0.66). Thus, we have
pe|∆R(Ssess,˜S,i,i )|=o(pe|R(Ssess,i)|), (78)
with (conditional) probability 1, as desired.
Upper bounding (41) by/summationtext
v∈Fi(Ssess)Q(i,Ssess,v)pµ2/3
i−1(Ssess(i)):To upper bound (41), we apply the
triangle inequality and extend both sums to vinFi(Ssess)∪Fi(˜S). This results in the following upper bound:
(41)≤/summationdisplay
v∈Fi(Ssess)∪Fi(˜S)|Q(i,Ssess,v)−Q(i,˜S,v)|. (79)
To proceed, we will upper bound the number of nonzero terms in (79). Each nonzero term can be upper
bounded by 1, sinceQ(i,Ssess,v),Q(i,˜S,v)are both probabilities. We will show that the number of nonzero
terms is at most O(|Fi(Ssess)|p·µ2/3
i−1(Ssess(i)))with high probability.
24Published in Transactions on Machine Learning Research (11/2022)
We write
Q(i,Ssess,v)−Q(i,˜S,v) (80)
=pe+ (1−pe)(1−(1−pn)degSsess(i)(v))−pe−(1−pe)(1−(1−pn)deg˜Si(v)) (81)
= (1−pe)((1−pn)deg˜Si(v)−(1−pn)degSsess(i)(v)). (82)
Thus, a term in the sum (79) is nonzero if and only if degSsess(i)(v)/negationslash= deg ˜Si(v). This happens if and only
ifvhas at least one edge to some vertex in ˜Si/triangleSsess(i). Thus, our task reduces to ﬁguring out how many
verticesvthere are that connect to some element of ˜Si/triangleSsess(i). The expected number of such vertices is
|Fi(Ssess)∪Fi(˜S)|·(1−q|˜Si/triangleSsess(i)|). (83)
This is an upper bound on the contribution of (41). We thus have
(41)≤|Fi(Ssess)∪Fi(˜S)|·(1−q|˜Si/triangleSsess(i)|). (84)
Next, we showthat |Fi(Ssess)∪Fi(˜S)|=O(|Fi(Ssess)|). To dothis, we applytheresults from upper bounding
(39). In particular,
|Fi(Ssess)∪Fi(˜S)|=|Fi(Ssess)∩Fi(˜S)|+|∆F(Ssess,˜S,i,i )| (85)
≤|Fi(Ssess)|+|∆F(Ssess,˜S,i,i )|=O(|Fi(Ssess)|). (86)
Next, we show that 1−q|˜Si/triangleSsess(i)|=pµ2/3
i−1(Ssess(i)). We can write
q|˜Si/triangleSsess(i)|= (1−p)|˜Si/triangleSsess(i)|∼e−p|˜Si/triangleSsess(i)|, (87)
provided that p·|˜Si/triangleSsess(i)|=o(1). Now from the inductive hypothesis, |˜Si/triangleSsess(i)|=O(|Ssess(i)|2/3),
and from Lemma 1, we know that |Ssess(i)|=O((np)i). Then we have that
1−q|˜Si/triangleSsess(i)|≤1−e−O(p(np)2/3i). (88)
In order for this second term to be 1−o(1), it is suﬃcient to have that
pi+1ni=o(1). (89)
This happens if and only if
pi+1=o(n−i)⇐⇒p=o(n−i
i+1). (90)
This is guaranteed by our assumption that p=o(n−T
T+1). Thus,
1−q|˜Si/triangleSsess(i)|≤1−e−O(p(np)2/3i)∼pµ2/3
i−1(Ssess(i)). (91)
We have shown that
(41) =O(|Fi(Ssess)|˙pµ2/3
i−1(Ssess(i))). (92)
Next, we show that/summationtext
v∈Fi(Ssess)Q(i,Ssess,v) = Ω(|Fi(Ssess)|). We have
Q(i,Ssess,v)≥1−(1−pn)degSsess(i)(v). (93)
Since the sum is over v∈Fi(Ssess), this implies that degSsess(i)(v)≥1. So
Q(i,Ssess,v)≥1−(1−pn) =pn= Ω(1). (94)
25Published in Transactions on Machine Learning Research (11/2022)
Thus,
/summationdisplay
v∈Fi(Ssess)Q(i,Ssess,v)≥|Fi(Ssess)|·pn= Ω(|Fi(Ssess)|). (95)
Thus, we have shown that
(41)≤const·/summationdisplay
v∈Fi(Ssess)Q(i,Ssess,v)·(1−q|˜Si/triangleSsess(i)|) =const/summationdisplay
v∈Fi(Ssess)Q(i,Ssess,v)pµ2/3
i−1(Ssess),(96)
with conditional probability at least 1−e−Ω((np)i).
Completing the proof We combine (96), (74), and (78) to complete the proof.
So we have that the diﬀerence between µi(Ssess)andµi(˜S)is negligible in relation to µi(Ssess).
Now, the next two propositions give the base case and inductive step of the proof of Theorem 4.
Proposition 1 (Base case of the proof of Theorem 4) .We have that, with probability 1,|∆R0|=
|∆R(Ssess,˜S,0,0)|= 0,and|˜S0/triangleS0|= 0.
Proof.This follows directly from the assumed initial conditions.
Proposition 2 (Inductive step of the proof of Theorem 4) .Assume that the inductive hypotheses (35) and
(36) hold for i. Then we have the following:
|˜Si+1/triangleSsess(i+ 1)|≤|∆Ri|+o(1) =|∆R(Ssess,˜S,i,i )|+o(1) =o(µi−1(Ssess)2/3) =o(µi(Ssess)2/3).(97)
Equivalently,
1−|˜Si+1∩Ssess(i+ 1)|
|Ssess(i)|=o(µi(Ssess)−1/3). (98)
Furthermore,
|∆Ri+1|≤|∆Ri|≤o(µi−1(Ssess)2/3) =o(µi(Ssess)2/3). (99)
In other words, both inductive hypotheses Hypothesis 1 and Hypothesis 2 are satisﬁed for i+ 1. This holds
with probability at least 1−e−Ω(µi(Ssess)).
Proof.To prove this, we ﬁrst need a few essential inequalities.
•By deﬁnition of the algorithm,
|˜Si+1|≤µi(˜S)(1 +µi(˜S)−1/3), (100)
with probability 1.
•We will also need to prove an upper bound on |˜Si+1|−|Ssess(i+ 1)|. In particular, we will show
that with probability at least 1−e−Ω(µi(Ssess)),
|˜Si+1|≤|Ssess(i+ 1)|·(1 +O(µi(Ssess)−1/3)). (101)
We show this as follows. From Theorem 5,
µi(˜S)≤µi(Ssess)(1 +pµi−1(Ssess)2/3),
26Published in Transactions on Machine Learning Research (11/2022)
with probability≥1−e−Ω(µi(Ssess)). This implies, via (100), that
|˜Si+1|≤µi(Ssess)·(1 +pµi−1(Ssess)2/3)·(1 +O(µi(Ssess)−1/3)).
By concentration of |Ssess(i+ 1)|, with probability at least 1−e−Ω(µi(Ssess)), this is upper bounded
as follows:
|˜Si+1|≤|Ssess(i+ 1)|(1 +O(|Ssess(i+ 1)|−1/2+const))(1 +pµi−1(Ssess)2/3)(1 +O(µi(Ssess)−1/3)).
Now, we can see from (63) that this is equal to the desired upper bound. We have thus shown (101).
Now, with the preliminary inequalities proven, we proceed to prove the proposition. We split into two cases:
•Ssess(i+ 1)begins before ˜Si+1(in other words, |R(Ssess,i)|<|R(˜S,i)|).
In this case, we will show (i) that Ssess(i+ 1)must end before ˜Si+1(i.e., that|R(Ssess,i+ 1)|≤
|R(˜S,i+ 1)|) with high probability, (ii) that
∆Ri+1=o(µi(Ssess).66), (102)
and (iii) that
|˜Si+1/triangleSsess(i+ 1)|≤2|∆Ri|+o(1) =o(µi(Ssess).66). (103)
To show that (i) is true, we note that because Ssess(i+ 1)begins before ˜Si+1,Ssess(i+ 1)consists
of an initial segment Sobs(j1),Sobs(j1+ 1),...,Sobs(j2)with total cardinality |∆Ri|, ending in a
session endpoint (speciﬁcally, the one corresponding to R(˜S,i)), followed by a segment Sobs(j2+
1),...,Sobs(j3)of total cardinality |Ssess(i+ 1)|−|∆Ri|, again ending in a session endpoint. This is
true by deﬁnition of ∆Ri. The second segment begins at the same point as ˜Si+1(that is,R(˜S,i) =/uniontextj2+1
j=0Sobs(j)), and we know that it has cardinality
|Ssess(i+ 1)|−|∆Ri|≤|Ssess(i+ 1)|≤µi(Ssess)(1 +µi(Ssess)−1/2+const)≤µi(˜S)(1 +µi(˜S)−1/3),
(104)
by concentration of |Ssess(i+1)|. The last inequality follows from the fact that µi(Ssess) = Θ(µi(˜S)).
Thus, the second segment of Ssess(i+ 1)must be contained in ˜Si+1, by (100), by deﬁnition of the
FastClock algorithm, as desired.
This has the following implication: we can express |∆Ri+1|as
|∆Ri+1|=|˜Si+1|−(|Ssess(i+ 1)|−|∆Ri|)≤µi(Ssess)−1/3+|∆Ri|. (105)
We have used (101). Since, by the inductive hypothesis, we have |∆Ri|=o(µi−1(Ssess)0.66) =
o(µi(Ssess).66), and since µi(Ssess)→∞, this implies that
|∆Ri+1|=o(µi(Ssess).66). (106)
Thus, we have established (ii).
We next show (iii). We have
|˜Si+1/triangleSsess(i+ 1)|=|∆Ri|+|∆Ri+1|, (107)
and by the proof of (ii) we can upper bound |∆Ri+1|to get
|˜Si+1/triangleSsess(i+ 1)|≤2|∆Ri|+o(1) =o(µi(Ssess).66). (108)
This completes the proof of (iii).
27Published in Transactions on Machine Learning Research (11/2022)
•OrSsess(i+ 1)begins after or at the same time as ˜Si+1(in other words, |R(Ssess,i)|≥
|R(˜S,i)|).
In this case, we will show (i) that
|∆Ri+1|=o(µi(Ssess).66), (109)
and (ii) that
|˜Si+1/triangleSsess(i+ 1)|≤2|∆Ri|+o(1) =o(µi(Ssess).66). (110)
To prove (i), we start by showing the following identity:
|˜Si+1|=|∆Ri|+|Ssess(i+ 1)|+|∆Ri+1|Ii+1, (111)
where
Ii+1=/braceleftBigg
1Ssess(i+ 1)stops before ˜Si+1
−1otherwise(112)
The identity (111) is a consequence of the following derivation, which relies on the deﬁnitions of all
involved terms.
|∆Ri|+|Ssess(i+ 1)|+|∆Ri+1|Ii+1
=i/summationdisplay
k=0|Ssess(k)|−i/summationdisplay
k=0|˜Sk|+|Ssess(i+ 1)|+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglei+1/summationdisplay
k=0|Ssess(k)|−i+1/summationdisplay
k=0|˜Sk|/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleIi+1
=i+1/summationdisplay
k=0|Ssess(k)|−i/summationdisplay
k=0|˜Sk|−/parenleftBiggi+1/summationdisplay
k=0|Ssess(k)|−i+1/summationdisplay
k=0|˜Sk|/parenrightBigg
=|˜Si+1|.
Rearranging (111) to solve for |∆Ri+1|, we have that
|∆Ri+1|=||˜Si+1|−|∆Ri|−|Ssess(i+ 1)||≤|| ˜Si+1|−|Ssess(i+ 1)||+|∆Ri|
=||˜Si+1|−|Ssess(i+ 1)||+o(µi−1(Ssess)2/3)
≤O(µi(Ssess)−1/3) +o(µi−1(Ssess)2/3) =o(µi(Ssess).66).
Here, we have used the triangle inequality and the inductive hypothesis Hypothesis 1 on |∆Ri|,
followed by the inequality (101). This completes the proof of (i).
Furthermore, this implies, by the same logic as in the previous case (108), that
|˜Si+1/triangleSsess(i+ 1)|≤2|∆Ri|+o(1) =o(µi(Ssess).66), (113)
which veriﬁes the inductive hypothesis Hypothesis 2 on |˜Si+1/triangleSsess(i+ 1)|.
The inductive hypotheses Hypotheses 1 and 2 follow directly from the above.
We can now prove the utility theorem, Theorem 4.
Proof of Theorem 4. LetBidenote the badevent that either inductive hypothesis fails to hold at step i. We
will lower bound Pr[/intersectiontextT−1
i=0¬Bi].By the chain rule, we have
Pr[T−1/intersectiondisplay
i=0¬Bi] = Pr[¬B0]T−1/productdisplay
i=1Pr[¬Bi|i−1/intersectiondisplay
j=0¬Bj]. (114)
28Published in Transactions on Machine Learning Research (11/2022)
From Proposition 2, Proposition 1, and Lemma 1, this is lower bounded by
T−1/productdisplay
i=1(1−e−D·(np)i+1) = exp/parenleftBiggT−1/summationdisplay
i=1log/parenleftBig
1−e−D(np)i+1/parenrightBig/parenrightBigg
= exp/parenleftBigg
−T−1/summationdisplay
i=1e−D(np)i+1·(1 +o(1))/parenrightBigg
= 1−e−Ω(np).
Now, the event that none of the bad events hold implies the claim, which completes the proof.
With Theorem 4 in hand, we can prove the main result, Theorem 1.
Proof of Theorem 1. Let us recall the deﬁnition of dSobs(C,ˆC). We have
dSobs(C,ˆC) =1/parenleftbign
2/parenrightbig/summationdisplay
i<jDisC,ˆC(i,j). (115)
What we need is an upper bound on this quantity in terms of the error term f(n) = (np)−1/3in Theorem 4.
To this end, we partition the sum according to vertex membership in clock intervals as follows:
/parenleftbiggn
2/parenrightbigg
dSobs(C,ˆC) =|S|/summationdisplay
k1=1/summationdisplay
i<j∈Sk1DisC,ˆC(i,j) +|S|/summationdisplay
k1=1|S|/summationdisplay
k2=k1+1/summationdisplay
i∈Sk1,j∈Sk2DisC,ˆC(i,j). (116)
In the ﬁrst sum, iandjarenotordered by C, because they lie in the same set in S. We consider the
corresponding set in ˜S. From the theorem, at least/parenleftbig|Ck1|·(1−f(n))
2/parenrightbig
vertex pairs from Sk1are correctly placed
together in ˜S. Furthermore, at least
|Sk1|·(1−f(n))·|S|/summationdisplay
k2=k1+1(1−f(n))|Sk2| (117)
pairs of vertices with one vertex in Sk1are correctly placed in diﬀerent intervals. So the number of correctly
ordered/unordered vertex pairs is at least
|S|/summationdisplay
k1=1
|Sk1|2·(1−f(n))2
2+|S|/summationdisplay
k2=k1+1|Sk1||Sk2|(1−f(n))2
∼/parenleftbiggn
2/parenrightbigg
·(1−f(n))2. (118)
Sincef(n) =o(1), this is asymptotically equal to/parenleftbign
2/parenrightbig
·(1−2f(n)).
This completes the proof.
B.2 Proof of Theorem 2
Weanalyzetheworst-caserunningtimeofFastClockasfollows: initializationtakes O(1)time. Thedominant
contribution to the running time is the whileloop. Since tobsis initially 0and increases by at least 1in each
iteration, the total number of iterations is at most N. The remaining analysis involves showing that each
vertex and edge is only processed, a constant number of times, in O(1)of these loop iterations, so that the
running time is at most O(N+n+m), as claimed.
In particular, the calculation of µtin every step involves a summation over all edges from currently active
vertices to their uninfected neighbors, along with a calculation involving the current number of uninfected
vertices (which we can keep track of using O(1)calculations per iteration of the loop). A vertex is only active
in a single iteration of the loop. Thus, each of these edges is only processed once in this step. The calculation
oft/prime
obsentails calculating a sum over elements of Sobsthat are only processed once in all of the iterations of
the loop. The calculation of all of the |Sobs(i)|can be done as a preprocessing step via an iteration over all n
vertices ofG. Finally, the calculation of Ft+1entails a union over the same set of elements of Sobsas in the
29Published in Transactions on Machine Learning Research (11/2022)
calculation of the maximum, followed by a traversal of all edges incident on elements of ˜St+1whose other
ends connect to uninfected vertices. These operations involve processing the vertices in ˜St+1(which happens
only in a single iteration of the loop, and, thus, with the preprocessing step of calculating the |Sobs(i)|, only
a constant number of times in the entire algorithm). The edges leading to elements of Ft+1from elements
of˜St+1are traversed at most twice in the loop: once in the building of Ft+1and once in the next iteration
in the calculation of µt.
This implies that each vertex and edge is only processed O(1)times in the entire algorithm. This leads to
the claimed running time of O(N+n+m), which completes the proof.
B.3 Proof of Theorem 3
To generalize the proof of Theorem 1, we need to generalize the following auxiliary results:
1. Concentration of |Ssess(i)|. This follows from the martingale diﬀerence property, immediately. Con-
centration is then used in the inductive proof Proposition 2, as well as in Lemma 1, which we
generalize to Lemma 2.
2. Lemma 1 on the growth of µi(Ssess)and|Ssess(i)|. This follows simply from the graphon assumption
and from concentration of |Ssess(i)|. The resulting generalization is as follows.
Lemma 2 (Growth of µi(Ssess)and|Ssess(i)|).We have that, with probability at least 1−e−Ω(nρn),
for alli≤T,
µi(Ssess) = Ω((np0ρn)i+1),O((np1ρn)i+1), (119)
where the Ω,Oare uniform in i. Furthermore, with probability at least 1−e−Ω(nρn), we have
|Ssess(i)|= Ω((np0ρn)i),O((np1ρn)i) (120)
for everyi.
Proof.The structure of the proof is exactly as in that of Lemma 1. Namely, we prove this by
induction on i. The base case follows from the graphon density assumption, along with the assumed
lowerbound(theassumptionc))ontheprobabilityofinfectionofeachfrontiervertextoestablishthe
bounds on µ0(Ssess). The bounds on |Ssess(1)|then follow from the assumption a). The inductive
step follows in exactly the same way.
3. Theorem 5, upper bounding the diﬀerence |µi(Ssess)−µi(˜S)|.
Theorem 6 (Generalized upper bound on |µi(Ssess)−µi(˜S)|).Granted the inductive hypotheses
Hypothesis 1 and 2, we have the following upper bound:
|µi(Ssess)−µi(˜S)|≤p1ρnµi−1(Ssess)2/3µi(Ssess), (121)
with probability≥1−e−Ω(µi(Ssess)).
Proof.The contribution of pein the general case is exactly as before, since peplays the same role.
We thus ignore it in the following analysis. Recalling that µi(Ssess)is an expected value, we can
decompose it by linearity of expectation as follows:
µi(Ssess) =/summationdisplay
v∈F(Ssess,i)Pr[X(v,i) = 1|σi−1(Ssess)] (122)
We may upper bound |µi(Ssess)−µi(˜S)|as follows:
|µi(Ssess)−µi(˜S)|≤/summationdisplay
v∈F(Ssess,i)∪F(˜S,i)|Pr[X(v,i) = 1|σi−1(Ssess)]−Pr[X(v,i) = 1|σi−1(˜S)]|.
(123)
30Published in Transactions on Machine Learning Research (11/2022)
Exactly as in the proof of Theorem 5, we upper bound the sum in (123) by its number of nonzero
terms. By Assumption 2, any term in this sum is nonzero only if the vertex vhas at least one active
neighbor in ˜Si/triangleSsess(i). The remainder of the proof, in which we upper bound the number of such
vertices, is exactly the same as in the proof of Theorem 5, with the exception that qis replaced by
the interval [1−p1ρn,1−p0ρn].
With these ingredients, the statement of Proposition 2 remains the same. Its proof changes slightly: we use
Theorem 6 in place of Theorem 5, and in place of pwe usep1ρn.
Propositions 2 and Lemma 2 allow us to prove a generalization of the utility theorem Theorem 4, which is
as follows.
Theorem 7 (Generalization of the FastClock utility theorem) .We have that with probability 1−e−Ω(nρn),
for everyi≤T−1,
|Ssess(i)∩˜Si|=|Ssess(i)|·(1−O(nρn)−1/3). (124)
Proof.The proof steps are exactly the same, except that we apply the lower bounds given in Lemma 2,
rather than Lemma 1.
Finally, Theorem 1 generalizes to Theorem 3, whose proof remains intact, except that we use the generalized
version of the utility theorem in place of Theorem 4. This concludes the proof of Theorem 3.
C Examples
Example 3 (Failure to account for multiple time scales aﬀects downstream statistical inference) .Here we
describe a simple example that demonstrates that failure to account for multiple time scales in a cascade
can negatively impact the accuracy of downstream statistical inference. As this is only an example, we opt
for simplicity of analysis and, thus, leave certain details to the reader. These may be ﬁlled in by standard
concentration arguments.
Speciﬁcally, we focus on the problem of cascade doubling time prediction, studied in Cheng et al. (2014). The
problem is stated as follows: given cascade observations up to/including a time t∈Rin whichmvertices
are infected, the task is to predict an interval [a,b]such that, with probability at least 1−δ, for some ﬁxed
parameterδ>0, the time of the 2m-th infection event lies in [a,b].
Consider a small-scale cascade model Mgiven by the discretized independent cascade model as detailed in
Deﬁnition 6 on a graph Gwithnvertices, with edge transmission parameter pn= 1, probability of infection
from an external source pe= 0, and transmission timer distributed according to a geometric distribution
with success parameter λ. Consider a cascade generated by our two-scale model with small-scale model M
and clockC= ([0,k−1],[k,2k−1],[2k,3k−1],[3k,4k−1],...), for some ﬁxed k∈N. Assume that Gis a
complete binary tree and that the infection starts at the root node. Finally, assume that we have observed
the cascade up to and including the midpoint of session j:t= (j+ 1/2)k.
We ﬁrst calculate the typical number of vertices infected up to and including time t. The number of vertices
infected by the beginning of session jis given by/summationtextj−1
i=02i= 2j−1,by the geometric sum formula. This
holds with probability 1. The number of vertices infected by time tis then 2j−1 +C·2j+1,for some speciﬁc,
computable constant C > 0. Thus, easily, the number of infected vertices approximately doubles by time t+k.
Next, we consider what our estimate of the doubling time would be if we were to incorrectly assume that the
cascade is generated by model M(that is, if we were to ignore the existence of the larger time scale). To do
this, we need to introduce some notation:
•Xj(t)– the number of vertices with juninfected children at the end of timestep t.
•Y(t)– the number of uninfected children of infected parents at the end of timestep t. We call such
nodes “open slots”.
31Published in Transactions on Machine Learning Research (11/2022)
•Z(t)– the number of vertices infected in timestep t.
•W(t)– the total number of vertices infected by the end of timestep t.
For anyt/prime> t, we have that Z(t/prime)is approximately λ·Y(t/prime−1), andY(t/prime)is exactlyY(t/prime−1) +Z(t/prime),
because each new infection removes an open slot (the newly infected vertex) and creates two new open slots
(the children of the newly infected vertex). Thus, we have that Z(t/prime)≈λ(Y(t/prime−2) +Z(t/prime−1)), so that
Z(t/prime)≈λY(t/prime−1) =λ(Y(t/prime−2) +Z(t/prime−1)) (125)
=λ(Y(t) +Z(t+ 1) +Z(t+ 2) +...+Z(t/prime−1)) (126)
=λ(Y(t) +Z(t+ 1) +...+Z(t/prime−2)) +λZ(t/prime−1) (127)
= (1 +λ)Z(t/prime−1) = (1 +λ)t/prime−t−1Z(t+ 1). (128)
Next, note that Y(t)can be related to W(t): since each new infection increases the number of open slots by
1, we must have that Y(t) =W(t) + 1.So we have Z(t/prime)≈(1 +λ)t/prime−t−1λ(W(t) + 1).Then
W(t/prime)−W(t) =t/prime/summationdisplay
i=t+1Z(i)≈λ(W(t) + 1)t/prime/summationdisplay
i=t+1(1 +λ)i−t−1=λ(W(t) + 1)·t/prime−t−1/summationdisplay
i=0(1 +λ)i
=λ(W(t) + 1)·(1 +λ)t/prime−t−1
λ= (W(t) + 1)·((1 +λ)t/prime−t−1).
Then, in order for t/primeto be the doubling time for t, we must have that ((1 +λ)t/prime−t−1)≥1.It is then easily
seen that the required t/primeis constant with respect to k, so that we substantially underestimate the doubling
time when kis large. Thus, failure to account for the larger time scale in this setting leads to substantial
and, in this setting, avoidable inaccuracy.
More realistic empirical experiments in DiTursi et al. (2017; 2019) conﬁrm that accounting for multiple
timescales can, in practical settings, improve performance on doubling time prediction and several other
downstream statistical tasks.
Example 4 (Infection sequences and clocks) .Consider a graph Gwithn= 10vertices. A cascade may
produce an observed infection sequence Sobs= ({7},{4},{1,3},{5},{2},{8},{},{10},{6,9}),indicating that
vertex 7is infected at time t= 0,4at timet= 1,1and3at time 2, etc. If the cascade was generated
according to our multiscale model with session end points at t= 1,3,4,8, then this would induce the following
session infection sequence: Ssess= ({4,7},{1,3,5},{2},{6,8,9,10}).The clock that induced this session
infection sequence is given by the session end points. As a partition into subintervals, it is encoded by
([0,1],[2,3],[4,4],[5,8]).
Example 5 (Frontier sets of an infection sequence) .Consider the graph depicted in Figure 3, where
sets of vertices are arranged from left to right according to the example infection sequence Ssess =
({1},{3,5},{2,4},{6}). The frontier sets corresponding to Ssesson this graph are
F0(Ssess) ={3,4,5,6}, (129)
F1(Ssess) ={2,4,7}, (130)
F2(Ssess) ={6,7}, (131)
F3(Ssess) =∅. (132)
D Empirical results on synthetic and real graphs
In this section, we give our complete set of empirical results, continuing the material in Section 3.4.
ExperimentsonErdős–Rényigraphs. WeﬁrstexperimentwithErdős–Rényitoconﬁrmthetheoretical
behavior of our estimator and compare its running time and quality to the DP baseline. We report the results
32Published in Transactions on Machine Learning Research (11/2022)
3 1 2
4 56
7
Figure 3: The network for the frontier example, Example 5. Distinct shaded regions from left to right denote distinct
infection sets from the sequence Ssessin the example.
1000 3000 5000 10000
number of nodes0.00.10.20.30.4distanceFastClock
DP
(a)
1000  3000  5000  10000
number of nodes10−210−1100101102runtimes (seconds)F astClock
DP (b)
0.01 0.05 0.1 0.15 0.2 0.25 0.3
pn0.00.10.20.30.4distanceFastClock
DP (c)
0.01  0.05  0.1  0.15  0.2  0.25  0.3
pn100101102103runtimes (seconds)F astClock
DP (d)
1 3 5 7 9
density (alpha)0.00.10.20.30.40.50.6distanceFastClock
DP
(e)
1  3  5  7  9
densit  (alpha)10−410−310−210−1100101runtimes (seconds)F astClock
DP (f)
2 4 6 8
stretch0.000.050.100.150.200.25distanceFastClock
DP (g)
2  4  6  8
stretch100101102103runtimes (seconds)F astClock
DP (h)
Figure 4: Comparison of the distance and runtime of the estimated clocks by FastClock and the baseline DP from DiTursi
et al. (2017) on Erdős–Rényi graphs (default parameters for all experiments: pn= 0.1,pe= 10−7,n= 3000,p=n−1/3, stretch
l= 2unless varying in the speciﬁc experiment). (a),(b): Varying graph size. (c),(d): Varying infection probability pn. (e),(f):
Varying graph density p=n−1/α. (g),(h): Varying stretch.
in Figure 4. With increasing graph size FastClock ’s distance from the ground truth clock diminishes (as
expected based on Theorem 1), while that of DP increases (Fig. 4(a)). Note that DP optimizes a proxy to
the cascade likelihood and in our experiments tend to associate too many early timesteps with early sessions,
which for large graph sizes results in incorrect recovery of the ground truth clock. Similarly, FastClock ’s
estimate quality is better than that of DP for varying on pn(Fig. 4(c)), graph density (Fig. 4(e)) and stretch
factor for the cascades (Fig. 4(g)), with distance from ground truth close to 0for regimes aligned with the
key assumptions we make for our main results (Assumption 1 or, more generally, 2). In addition to superior
accuracy, FastClock ’s running time scales linearly with the graph size and is orders of magnitude smaller
than that of DP for suﬃciently large instances (Figs. 4(b), 4(d), 4(f), 4(h)).
Experiments on Stochastic Block Model (SBM) graphs. We would also like to understand the
behavior of our estimator on graphs with communities where the cascade may cross community boundaries.
To this end, we experiment with SBM graphs varying the inter-block connectivity and virality ( pn) of the
cascades and report results in Fig. 5. As the cross-block connectivity increases and approaches that within
blocks (i.e. the graph structure approaches that of an ER graph) FastClock ’s quality improves and is
signiﬁcantly better than that of DP (Fig. 5(a)). When, however, the transmission probability pnis high,
coupled with sparse inter-block connectivity, FastClock ’s estimation quality deteriorates beyond that of DP
(Fig. 5(c)). This behavior is due to the relatively large variance of µtwhen the cascade crosses a sparse cut
33Published in Transactions on Machine Learning Research (11/2022)
0.01 0.05 0.1 0.15 0.2
connectivity−0.10.00.10.20.30.40.5distanceFastClock
DP
(a)
0.01  0.05  0.1  0.15  0.2
connectivity100101runtimes (seconds)F astClock
DP (b)
0.01 0.05 0.1 0.15 0.2 0.25 0.3
pn0.00.10.20.30.40.50.6distanceFastClock
DP (c)
0.01  0.05  0.1  0.15  0.2  0.25  0.3
pn10−310−210−1100101102runtimes (seconds)F astClock
DP (d)
Figure 5: Comparison of the distance and runtime of the estimated clocks by FastClock and the baseline DP from DiTursi
et al. (2017) on Stochastic Block Model graphs (default parameters: n= 5000, two blocks/communities of sizes n/√nand
n−n/√n,pe= 10−7, stretch l= 2). (a),(b): Varying inter-block connectivity ( pn= 0.1) where a setting of 0.2makes the
graph equivalent to an Erdős–Rényi graph with p= 0.2. (c),(d): Varying infection probability pn(inter-block connectivity is
set to 0.01).
in the graph with high probability. This challenging scenario opens an important research direction we plan
to explore in future work.
D.1 Empirical results on a real graph
Here we evaluate the performance of FastClock and the DP algorithm from DiTursi et al. (2017) via synthetic
cascades on a real network. The graph in question, harvested by Bogdanov et al. (2013), is a Twitter
subnetwork consisting of 3000nodes, with edges representing follower-followee relationships. The network
was constructed by a breadth-ﬁrst search starting from a set of seed nodes consisting of the authors of that
work and their labmates. We generated cascades on this network with a single random initially infected node
and variedpnfrom 0.01to0.15. We plotted the accuracy and running time (Figure 6) of the two algorithms
versuspn, averaged over 50trials. For all values of pn, the running time of FastClock is substantially
smaller than that of the DP algorithm. Regarding accuracy, for smaller pn, FastClock’s average distance is
smaller than or equal to that of DP. The accuracy of FastClock decays past approximately pn= 0.15but
remains below an average distance of 0.07. We note that these larger values for pnmay not be practically
relevant. E.g., in Kempe et al. (2003), values of pnbetween 0.01and0.1(and, in general, varying inversely
in proportion to the degrees of nodes) were considered relevant.
E Discussion of knowledge of small-scale cascade model parameters
Our algorithm relies on knowledge of the parameters of the small-scale cascade model. Here we discuss this
aspect further. Our main messages are as follows:
1. Joint estimation of small-scale model parameters and the clock from a sample cascade is, without
any assumptions, information-theoretically impossible. This is to be expected: the small-scale model
parameters determine the local temporal dynamics of the process, and large-scale eﬀects can distort
these local dynamics.
This makes the issue subtle and motivates work that is beyond the scope of the present paper.
2. In many use cases, small-scale model parameters may be estimated from data beyond sample cas-
cades. We give an example setting where this is plausible. In other settings, with certain assump-
tions, joint estimation from a sample cascade is possible. The main message here is that estimation
of small-scale model parameters is of interest, but a full exploration of practical methods is beyond
the scope of this paper. It is appropriate and plausible to regard these cascade parameters as being
given to us.
3. We can show that our algorithm is robust to random model parameter uncertainty.
34Published in Transactions on Machine Learning Research (11/2022)
0.01 0.05 0.1 0.15
Pn−0.050.000.050.100.150.200.250.300.35distanceFastClock
DP
0.01  0.05  0.1  0.15
pn10−1100101102runtimes (seconds)F astClock
DP
Figure 6: 50 Synthetic cascades generated on a real-world Twitter graph of about 3000 nodes. a and b)
distance and runtime over pn. DP exhibits higher accuracy at higher pnand FastClock does better at lower
pn. FastClock is much faster than DP, but loses a small amount of accuracy.
E.1 Estimation of transmission probabilities from non-cascade data
In some situations, cascade model parameters (i.e., transmission probabilities for individual edges) are de-
termined by observable pairwise node features and, hence, do not require cascade observations and ground
truth clock information for estimation. In other words, in these scenarios, it is plausible and appropriate
to think of the cascade model parameters as being already determined and available to us when we observe
sample cascades and estimate clocks.
More concretely, nodes vmay be endowed with feature vectors /vectorfv∈Rd, for some dimension d, and the
single-timestep transmission probability across an edge (v,w)may be stipulated to depend deterministically
on/vectorfvand/vectorfw, in a way to be determined. I.e., the transmission probability is stipulated to be of the
formFθ(/vectorfv,/vectorfw), whereFθ(·,·)is a known function depending on a parameter θ, which is to be estimated.
This can be estimated in certain settings by experiment: for a suﬃciently large number of pairs (v,w)of
individuals (not necessarily coming from a graph), the experimenter infects vertex vand exposes vtow. If
the experimenter can reliably determine whether or not wbecomes infected, then this provides an estimate
of the probability of transmission Fθ(/vectorfv,/vectorfw), and could be used to estimate θ.
E.2 Robustness to cascade model parameter uncertainty
We can extend our theoretical guarantees for FastClock to the case where the small-scale cascade model
parameters are not known exactly, but where instead each edge transmission probability is an independent
sample from a ﬁxed unknown distribution with known expected value and support bounded away from 0
and1. That is, ﬁx an arbitrary distribution Dsupported on [A,B], where 0< A≤B < 1, with known
expected value pn. We stress that we do not know AorB. For each ordered pair of vertices (v,w)such that
{v,w}is an edge in the graph, pn((v,w))∼D.
TheFastClockalgorithm’sguaranteesremainthesame. Thisisbecausethecrucialpropertyonwhichitrelies
is concentration of each |Ssess(i)|around its expected value, conditioned on σi−1(Ssess). This conditional
expectation is µi(Ssess)in the setting where Dassigns probability 1to some ﬁxed pn(or all of the pn(e)are
deterministic). When Dis a nontrivial distribution, the analysis is slightly more complicated: we must show
that|Ssess(i)|remains concentrated around its conditional expected value (call it¯|Ssess(i)|), but this number
is no longer exactly equal to µi(Ssess). Under our assumptions on the random graph model,¯|Ssess(i)|is
asymptotically close to µi(Ssess)whenever|Ssess(i−1)|→∞. This is the case when i>1. Thus, the only
35Published in Transactions on Machine Learning Research (11/2022)
trouble comes when i= 1and the seed set Ssess(0)has small cardinality ( Θ(1)with respect to n). In this
case, with high probability, every vertex in F0(Ssess)has exactly one neighbor in Ssess(0)(unless the graph
is extremely dense). For each vertex v∈F0(Ssess), lete(v)denote this unique edge from vtoSsess(0). Then
¯|Ssess(1)|is asymptotically equivalent to µ1(Ssess)provided that
/summationdisplay
v∈F0(Ssess)(1−pn(e(v)))∼/summationdisplay
v∈F0(Ssess)(1−pn). (133)
This is trivially the case because E[pn(e(v))] =pnand because the pn(e(v)), for diﬀerent v, are independent.
Thus, our algorithm’s accuracy and running time guarantees are robust to random uncertainty in model
parameters.
F Estimation of Ssess(0)
Here we describe how |Ssess(0)|might be estimated under various assumptions.
In some application scenarios, we are free to choose cascade seeds: e.g., in inﬂuence maximization. In this
case, the problem is trivial.
Alternatively, if we assume that the seeds are uniformly randomly chosen, then with high probability they
form an independent set (if the graph is sparse enough). We can estimate Ssess(0)to beS/prime
0=/uniontextk∗
j=0Sobs(j),
wherek∗= max{k:∪k
j=0Sobs(j)is an independent set in G }. This yields the correct result with high
probability if peis small enough (which we assume in our theorems).
Without any assumptions, any algorithm must be given Ssess(0), because, e.g., it is impossible to distinguish
between a cascade CwithTsessions versus zero sessions of a cascade whose seed set is the ﬁnal state of C.
G Behavior of FastClock under deviations from the modeling assumption
Here we consider deviations from one of the modeling assumptions and their consequences for FastClock.
Speciﬁcally, we consider what happens when, within any given session, a small set of vertices that become
infected also can immediately begin the process of infecting a subset of their neighbors.
To examine this scenario, we consider a relaxation of our model, wherein, in each session j, a subset of at
mostkjvertices may be “non-compliant”, in the sense that they become active immediately. Let us call
the non-compliant set for session j NCj. In this case, the total number of vertices in the infection trees of
these vertices is at most/summationtext
v∈NCjdeg(v), which is, with high probability, at most O(kjnp). Provided that
kj/lessmuch(np)j, this does not aﬀect the performance guarantee on FastClock.
In an extreme scenario where allvertices are non-compliant, we can modify the FastClock algorithm as
follows: starting at the beginning of each session, we keep a running total of the conditional expected
number of vertices infected since the start of the session. Simultaneously, we keep a running total of the
numberofverticesactuallyinfectedsincethesessionstart. Attheﬁrsttimepointatwhichthesetwonumbers
diﬀer by more than a certain threshold, we declare the previous timestep to be the end of the current session.
The threshold is chosen in the same manner as in the original FastClock algorithm, to provide a certain
probability of error guarantee.
H Real network statistics
Table 1 lists the structural statistics for large online social networks often used in empirical network science
research. Such networks are well within the expected density ranges we employ in our theoretical analysis,
namely their average degree is in the order (and typically higher) than a natural logarithm of the number
of nodes.
36Published in Transactions on Machine Learning Research (11/2022)
Name Description Type Nodes NEdges AVG degree ln(N)
ego-Facebook Social circles from Facebook (anonymized) Undirected 4,039 88,234 22 8
ego-Gplus Social circles from Google+ Directed 107,614 13,673,453 127 12
ego-Twitter Social circles from Twitter Directed 81,306 1,768,149 22 11
soc-Epinions1 Who-trusts-whom network of Epinions.com Directed 75,879 508,837 7 11
soc-LiveJournal1 LiveJournal online social network Directed 4,847,571 68,993,773 14 15
soc-Pokec Pokec online social network Directed 1,632,803 30,622,564 19 14
soc-Slashdot0811 Slashdot social network from November 2008 Directed 77,360 905,468 12 11
soc-Slashdot0922 Slashdot social network from February 2009 Directed 82,168 948,464 12 11
wiki-Vote Wikipedia who-votes-on-whom network Directed 7,115 103,689 15 9
Table 1: Statistics of some of the real-world social and information network datasets from SNAP ( http:
//snap.stanford.edu/data ).
37