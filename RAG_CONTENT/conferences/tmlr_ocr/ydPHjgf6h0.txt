Published in Transactions on Machine Learning Research (05/2024)
Adversarial Imitation Learning from Visual Observations
using Latent Information
Vittorio Giammarino vgiammar@bu.edu
Division of Systems Engineering
Boston University
James Queeney queeney@merl.com
Mitsubishi Electric Research Laboratories
Ioannis Ch. Paschalidis yannisp@bu.edu
Department of Electrical and Computer Engineering
Division of Systems Engineering
Faculty of Computing & Data Sciences
Boston University
Reviewed on OpenReview: https: // openreview. net/ forum? id= ydPHjgf6h0
Abstract
We focus on the problem of imitation learning from visual observations, where the learning
agent has access to videos of experts as its sole learning source. The challenges of this frame-
work include the absence of expert actions and the partial observability of the environment,
as the ground-truth states can only be inferred from pixels. To tackle this problem, we first
conduct a theoretical analysis of imitation learning in partially observable environments.
We establish upper bounds on the suboptimality of the learning agent with respect to the
divergence between the expert and the agent latent state-transition distributions. Moti-
vated by this analysis, we introduce an algorithm called Latent Adversarial Imitation from
Observations, which combines off-policy adversarial imitation techniques with a learned
latent representation of the agent’s state from sequences of observations. In experiments
on high-dimensional continuous robotic tasks, we show that our model-free approach in
latent space matches state-of-the-art performance. Additionally, we show how our method
can be used to improve the efficiency of reinforcement learning from pixels by leveraging
expert videos. To ensure reproducibility, we provide free access to all the learning curves
and open-source our code.
1 Introduction
Learning from videos represents a compelling opportunity for the future, as it offers a cost-effective and
efficient way to teach autonomous agents new skills and behaviors. Compared to other methods, video
recording is a faster and more flexible alternative for gathering data. Moreover, with the abundance of
high-quality videos available on the internet, learning from videos has become increasingly accessible in
recent years. However, despite the potential benefits, this approach remains challenging as it involves several
technical problems that must be addressed simultaneously in order to succeed. These problems include
representation learning, significant computational demands due to high-dimensional observation space, the
partial observability of the decision process, and lack of expert actions. Our objective is to establish algorithms
capable of overcoming all of these challenges, enabling the learning of complex robotics tasks directly from
videos of experts.
Formally, our focus is on the problem of Visual Imitation from Observations (V-IfO) . In V-IfO, the learning
agent does not have access to a pre-specified reward function, and instead has to learn by imitating an
1Published in Transactions on Machine Learning Research (05/2024)
Table 1: A summary of imitation from experts: Imitation Learning (IL), Imitation from Observations (IfO),
Visual Imitation Learning (V-IL), and Visual Imitation from Observations (V-IfO).
IL IfO V-IL V-IfO
Fully observable environment ✓ ✓ ✗ ✗
Access to expert actions ✓ ✗ ✓ ✗
expert’s behavior. Additionally, in V-IfO, expert actions are not accessible during the learning process,
and the pixel-based observations we obtain from video frames result in partial observability. The absence
of expert actions and the partial observability of the environment distinguish V-IfO from other types of
imitation from experts. Specifically, we identify three other frameworks previously addressed in the literature:
Imitation Learning (IL) (Atkeson & Schaal, 1997; Abbeel & Ng, 2004; Ross & Bagnell, 2010; Reske et al., 2021;
Giammarino et al., 2023a) where states are fully observable and expert state-action pairs are accessible, Visual
Imitation Learning (V-IL) (Rafailov et al., 2021) which explores the idea of imitating directly from pixels but
still assumes that expert actions are provided to the learning agent, and Imitation from Observations (IfO)
(Torabi et al., 2018b;a) which retains full observability but considers only the availability of expert states.
Table 1 summarizes these frameworks.
In order to address the V-IfO problem, this paper introduces both theoretical and algorithmic contributions.
First, we provide a theoretical analysis of the problem and demonstrate that the suboptimality of the
learning agent can be upper bounded by the divergence between the expert and the agent latent state-
transition distributions. Our analysis motivates the reduction of the V-IfO problem to two subproblems:
(i)estimating a proper latent representation from sequences of observations and (ii)efficiently minimizing the
divergence between expert and agent distributions in this latent space. Next, we propose practical solutions
to these subproblems. By doing so, we formalize a novel algorithm called Latent Adversarial Imitation from
Observations (LAIfO) , which tackles the divergence minimization step using off-policy adversarial imitation
techniques (Ghasemipour et al., 2020) and recovers a latent representation of the ground-truth state by means
of observations stacking (Mnih et al., 2013; 2015) and data augmentation (Laskin et al., 2020b; Kostrikov
et al., 2020; Yarats et al., 2021). We evaluate our algorithm on the DeepMind Control Suite (Tassa et al.,
2018), demonstrating that our model-free approach in latent space achieves state-of-the-art performance. We
conclude by showing how LAIfO can be used on challenging environments, such as the humanoid from pixels
(Tassa et al., 2018), to improve Reinforcement Learning (RL) efficiency by leveraging expert videos.
The remainder of the paper is organized as follows: Section 2 provides a summary of the most related works
to this paper. Section 3 introduces notation and background on RL and IL. Section 4 provides a theoretical
analysis of the V-IfO problem. Section 5 introduces our algorithm, LAIfO, and outlines how it can leverage
expert videos to improve data efficiency of RL from pixels. Finally, Section 6 presents our experimental
results and Section 7 concludes the paper providing a general discussion on our findings.
2 Related work
In recent years, several studies have focused on the IL problem (Atkeson & Schaal, 1997; Abbeel & Ng, 2004;
Ross & Bagnell, 2010; Reddy et al., 2019; Reske et al., 2021; Giammarino et al., 2023a) and, in particular, on
the generative adversarial IL framework (Ho & Ermon, 2016) which has emerged as one of the most promising
approaches for IL. Adversarial IL builds upon a vast body of work on inverse RL (Russell, 1998; Ng et al.,
2000; Abbeel & Ng, 2004; Syed & Schapire, 2007; Ziebart et al., 2008; Syed et al., 2008). The primary goal of
inverse RL is to identify a reward function that enables expert trajectories (i.e., state-action pairs) to be
optimal. The reward function obtained from the inverse RL step is then used to train agents in order to
match the expert’s expected reward. In the fully observable setting, adversarial IL was originally formalized
in Ho & Ermon (2016) and Fu et al. (2017). It was later extended to the observation only setting in Torabi
et al. (2018b) and to the visual setting in Karnan et al. (2022). Furthermore, the adversarial IfO problem
has been theoretically analyzed in Yang et al. (2019) and Cheng et al. (2021). Note that all of these studies
are built upon on-policy RL (Schulman et al., 2017), which provides good learning stability but is known
2Published in Transactions on Machine Learning Research (05/2024)
for poor sample efficiency. In recent works, this efficiency issue has been addressed by using off-policy RL
algorithms in the adversarial optimization step (Haarnoja et al., 2018; Lillicrap et al., 2015). These include
DAC (Kostrikov et al., 2018), SAM (Blondé & Kalousis, 2019), and ValueDICE (Kostrikov et al., 2019)
for the adversarial IL problem, and OPOLO (Zhu et al., 2020) and MobILE (Kidambi et al., 2021) for the
adversarial IfO problem. Another line of research has tackled IfO by directly estimating expert actions and
subsequently deploying IL techniques on the estimated state-action pairs (Torabi et al., 2018a; Liu et al.,
2018; Behbahani et al., 2019; Zhang & Ohn-Bar, 2021; Zhang et al., 2022; Shaw et al., 2023; Yang et al.,
2023). Finally, recent studies have investigated offline alternatives to the adversarial IL framework (Dadashi
et al., 2020).
All of the aforementioned works consider fully observable environments modeled as Markov Decision Processes
(MDPs). However, when dealing with pixels, individual observations alone are insufficient for determining
optimal actions. As a result, recent works (Rafailov et al., 2021; Hu et al., 2022) have treated the V-IL
problem as a Partially Observable Markov Decision Process (POMDP) (Astrom, 1965). In particular, Rafailov
et al. (2021) addressed the V-IL problem by proposing a model-based extension (Hafner et al., 2019a;b) of
generative adversarial IL called VMAIL. The work in Gangwani et al. (2020) also considered IL in a POMDP
in order to handle missing information in the agent state, but did not directly focus on learning from pixels.
The more difficult V-IfO problem, on the other hand, has received less attention in the literature. To the best
of our knowledge, this problem has only been considered by the recent algorithm PatchAIL (Liu et al., 2023),
where off-policy adversarial IL is performed directly on the pixel space. Different from Liu et al. (2023), we
first study V-IfO from a theoretical perspective, which motivates an algorithm that performs imitation on a
latent representation of the agent state rather than directly on the pixel space as in PatchAIL. This difference
is crucial to ensure improved computational efficiency.
Our work is also related to the RL from pixels literature which tackles the challenge of maximizing an agent’s
expected return end-to-end, from pixels to actions. This approach has proven successful in playing Atari
games (Mnih et al., 2013; 2015). Recently, RL from pixels has also been extended to tackle continuous action
space tasks, such as robot locomotion, by leveraging either data augmentation techniques (Laskin et al.,
2020a;b; Kostrikov et al., 2020; Lee et al., 2020; Raileanu et al., 2021; Yarats et al., 2021) or variational
inference (Hafner et al., 2019a;b; Lee et al., 2020; Hafner et al., 2020).
Finally, another line of research has focused on the visual imitation problem in the presence of domain
mismatch, also known as third-person imitation learning (Stadie et al., 2017; Okumura et al., 2020; Cetin &
Celiktutan, 2021; Giammarino et al., 2023b). This paradigm relaxes the assumption that the agent and the
expert are defined on the same decision process and represents a generalization of the imitation from experts
frameworks introduced in Table 1.
3 Preliminaries
Unless indicated otherwise, we use uppercase letters (e.g., St) for random variables, lowercase letters (e.g.,
st) for values of random variables, script letters (e.g., S) for sets, and bold lowercase letters (e.g., θ) for
vectors. Let [t1:t2]be the set of integers tsuch thatt1≤t≤t2; we write Stsuch thatt1≤t≤t2as
St1:t2. We denote with E[·]expectation, with P(·)probability, and with Df(·,·)anf-divergence between
two distributions of which the total variation (TV) distance, DTV(·,·), and the Jensen-Shannon divergence,
DJS(·||·), are special cases.
We model the decision process as an infinite-horizon discounted POMDP described by the tuple
(S,A,X,T,U,R,ρ0,γ), whereSis the set of states, Ais the set of actions, and Xis the set of obser-
vations.T:S×A→P(S)is the transition probability function where P(S)denotes the space of probability
distributions over S,U:S→P(X)is the observation probability function, and R:S×A→ Ris the
reward function which maps state-action pairs to scalar rewards. Alternatively, the reward function can also
be expressed asR:S×S→ Rmapping state-transition pairs to scalar rewards rather than state-action
pairs. Finally, ρ0∈P(S)is the initial state distribution and γ∈[0,1)the discount factor. The true
environment state s∈Sis unobserved by the agent. Given an action a∈A, the next state is sampled such
thats′∼T(·|s,a), an observation is generated as x′∼U(·|s′), and a rewardR(s,a)orR(s,s′)is computed.
Note that an MDP is a special case of a POMDP where the underlying state sis directly observed.
3Published in Transactions on Machine Learning Research (05/2024)
Reinforcement learning Given an MDP and a stationary policy π:S→P(A), the RL objective is to
maximize the expected total discounted return J(π) =Eτπ[/summationtext∞
t=0γtR(st,at)]whereτπ= (s0,a0,s1,a1,...).
A stationary policy πinduces a normalized discounted state visitation distribution defined as dπ(s) =
(1−γ)/summationtext∞
t=0γtP(st=s|ρ0,π,T), and we define the corresponding normalized discounted state-action
visitation distribution as ρπ(s,a) =dπ(s)π(a|s). Finally, we denote the state value function of πasVπ(s) =
Eτπ[/summationtext∞
t=0γtR(st,at)|S0=s]and the state-action value function as Qπ(s,a) =Eτπ[/summationtext∞
t=0γtR(st,at)|S0=
s,A0=a]. When a function is parameterized with parameters θ∈Θ⊂Rkwe writeπθ.
Generative adversarial imitation learning Assume we have a set of expert demonstrations τE=
(s0:T,a0:T)generated by the expert policy πE, a set of trajectories τθgenerated by the policy πθ, and a
discriminator network Dχ:S×A→ [0,1]parameterized by χ. Generative adversarial IL (Ho & Ermon,
2016) optimizes the min-max objective
min
θmax
χEτE[log(Dχ(s,a))] +Eτθ[log(1−Dχ(s,a))]. (1)
Maximizing (1) with respect to χis effectively an inverse RL step where a reward function, in the form of the
discriminator Dχ, is inferred by leveraging τEandτθ. On the other hand, minimizing (1) with respect to θcan
be interpreted as an RL step, where the agent aims to minimize its expected cost. It has been demonstrated
that optimizing the min-max objective in (1) is equivalent to minimizing DJS(ρπθ(s,a)||ρπE(s,a)), so we are
recovering the expert state-action visitation distribution (Ghasemipour et al., 2020).
Latent representation in POMDP When dealing with a POMDP, a policy πθ(xt)that selects an action
atbased on a single observation xt∈Xis likely to perform poorly since xtlacks enough information about
the unobservable true state st. It is therefore beneficial to estimate a distribution of the true state from the
full history of prior experiences. To that end, we introduce a latent variable zt∈Zsuch thatzt=ϕ(x≤t,a<t),
whereϕmaps the history of observations and actions to Z. Alternatively, when actions are not observable,
we havezt=ϕ(x≤t). The latent variable ztshould be estimated such that P(st|x≤t,a<t)≈P(st|zt), meaning
thatztrepresents a sufficient statistic of the history for estimating a distribution of the unobservable true
statest. It is important to clarify that this does not imply Z≡S.
4 Theoretical analysis
Recall that we consider the V-IfO problem where expert actions are not available and the ground-truth states
s∈Sare not observable (see Table 1). As a result, a latent representation z∈Zis inferred from the history
of observations and used by the learning agent to make decisions.
Throughout the paper we make the following assumptions: (i)the expert and the agent act on the same
POMDP and (ii)the latent variable zcan be estimated from the history of observations as zt=ϕ(x≤t)
such that P(st|zt,at) =P(st|zt) =P(st|x≤t,a<t). Assumption (i)is instrumental for both our derivations
and experiments. Relaxing this assumption would lead to dynamics mismatch (Gangwani et al., 2022) and
visual domain adaptation problems (Giammarino et al., 2023b), which represent interesting extensions for
future work. On the other hand, assumption (ii)explicitly states the characteristics required by the latent
variablez; i.e.,ztcan be successfully estimated from the history of observations x≤tin order to approximate
a sufficient statistic of the history. Note that this is a common assumption in the IL literature for POMDPs
(Gangwani et al., 2020; Rafailov et al., 2021), and estimating such a variable is a non-trivial problem that
we address in the next section. We further discuss the importance of this assumption from a theoretical
perspective in Appendix B (Remark 1).
On the latent space Z, we can define the normalized discounted latent state visitation distribution as
dπθ(z) = (1−γ)/summationtext∞
t=0γtP(zt=z|ρ0,πθ,T,U)and the normalized discounted latent state-action visitation
distribution as ρπθ(z,a) =dπθ(z)πθ(a|z). Further, we define the latent state-transition visitation distribution
asρπθ(z,z′) =dπθ(z)/integraltext
AP(z′|z,¯a)πθ(¯a|z)d¯aand the normalized discounted joint distribution as ρπθ(z,a,z′) =
ρπθ(z,a)P(z′|z,a), where
P(z′|z,a) =/integraldisplay
S/integraldisplay
S/integraldisplay
XP(z′|x′,a,z)U(x′|s′)T(s′|s,a)P(s|z)dx′ds′ds. (2)
4Published in Transactions on Machine Learning Research (05/2024)
Finally, we obtain Pπθ(a|z,z′)as
Pπθ(a|z,z′) =P(z′|z,a)πθ(a|z)/integraltext
AP(z′|z,¯a)πθ(¯a|z)d¯a.
Note that we write Pπθ, withπθas subscript, in order to explicitly denote the dependency on the policy and
omit the subscript, as in (2), when such probability depends only on the environment.
We start by considering the case in which R:S×A→ RandJ(π) =Eτπ[/summationtext∞
t=0γtR(st,at)]. The following
Theorem shows how the suboptimality of πθcan be upper bounded by the TV distance between latent
state-transition visitation distributions, reducing the V-IfO problem to a divergence minimization problem in
the latent spaceZ.
Theorem 1. Consider a POMDP, and let R:S×A→ Randzt=ϕ(x≤t)such that P(st|zt,at) =P(st|zt) =
P(st|x≤t,a<t). Then, the following inequality holds:
/vextendsingle/vextendsingleJ(πE)−J(πθ)/vextendsingle/vextendsingle≤2Rmax
1−γDTV/parenleftbig
ρπθ(z,z′),ρπE(z,z′)/parenrightbig
+C,
whereRmax= max (s,a)∈S×A|R(s,a)|and
C=2Rmax
1−γEρπθ(z,z′)/bracketleftbig
DTV/parenleftbig
Pπθ(a|z,z′),PπE(a|z,z′)/parenrightbig/bracketrightbig
. (3)
Proof.Using the definition of J(πθ), we first upper bound the performance difference between expert and
agent by DTV/parenleftbig
ρπθ(s,a),ρπE(s,a)/parenrightbig
. Next, we bound the latter divergence by DTV/parenleftbig
ρπθ(z,a),ρπE(z,a)/parenrightbig
using
the assumption P(st|zt,at) =P(st|zt)and noticing that P(st|zt)is policy independent. Finally, we bound
this last divergence in terms of DTV/parenleftbig
ρπθ(z,z′),ρπE(z,z′)/parenrightbig
(Lemma 3 in Appendix B). We provide the full
derivations in Appendix C.
Theorem 1 addresses the challenge of considering rewards that depend on actions without the ability to
observe expert actions. Consequently, in our setting, we cannot compute Cin (3). Similar to the MDP case
(Yang et al., 2019), a sufficient condition for C= 0is the injectivity of P(z′|z,a)in (2) with respect to a,
indicating that there is only one action corresponding to a given latent state transition. This property ensures
thatP(a|z,z′)remains unaffected by different executed policies, ultimately reducing Cto zero. For the sake of
completeness, we formally state this result in Appendix C. However, in our setting, it is difficult to guarantee
the injectivity of P(z′|z,a)due to its dependence on both the environment through U(x′|s′)andT(s′|s,a),
and the latent variable estimation method through P(z′|x′,a,z)andP(s|z). Instead, we demonstrate in
Theorem 2 how redefining the reward function as R:S×S→ R, which is commonly observed in robotics
learning, allows us to reformulate the result in Theorem 1 without the additive term Cin (3).
Theorem 2. Consider a POMDP, and let R:S×S→ Randzt=ϕ(x≤t)such that P(st|zt,at) =P(st|zt) =
P(st|x≤t,a<t). Then, the following inequality holds:
/vextendsingle/vextendsingleJ(πE)−J(πθ)/vextendsingle/vextendsingle≤2Rmax
1−γDTV/parenleftbig
ρπθ(z,z′),ρπE(z,z′)/parenrightbig
,
whereRmax= max (s,s′)∈S×S|R(s,s′)|.
Proof.The proof proceeds similarly to the one for Theorem 1, by using that P(s,s′|z,z′)is not characterized
by the policy but only by the environment. We show the full proof in Appendix C.
In summary, Theorems 1 and 2 show that, assuming we have a latent space Zthat can effectively approximate
a sufficient statistic of the history, the imitation problem can be performed entirely on this latent space. Note
that this is in contrast with the existing literature (Liu et al., 2023), where imitation is performed on the
observation space X. As a result of this analysis, our algorithm is characterized by two main ingredients: a
practical method to estimate z∈Zfrom sequences of observations, and an efficient optimization pipeline to
minimize DTV/parenleftbig
ρπθ(z,z′),ρπE(z,z′)/parenrightbig
.
5Published in Transactions on Machine Learning Research (05/2024)
5 Latent Adversarial Imitation from Observations
In the following, we introduce the main components of our algorithm LAIfO. Motivated by our theoretical
analysis in the previous section, our algorithm combines techniques for adversarial imitation from observations
and latent variable estimation. First, we outline our adversarial imitation pipeline in the latent space Z,
which leverages off-policy adversarial imitation from observations (Kostrikov et al., 2018; Blondé & Kalousis,
2019; Zhu et al., 2020) in order to minimize the divergence between the latent state-transition visitation
distributions of the agent and expert. Then, we describe a simple and effective approach for estimating
the latent state zthat makes use of observations stacking (Mnih et al., 2013; 2015) and data augmentation
(Laskin et al., 2020b; Kostrikov et al., 2020; Yarats et al., 2021). Finally, we show how LAIfO can leverage
expert videos to enhance the efficiency of RL from pixels in a number of highly challenging tasks.
Off-policy adversarial imitation from observations Based on the results in Section 4, given a latent
variablezthat captures a sufficient statistic of the history, we can minimize the suboptimality of the policy
πθby solving the minimization problem
min
θDTV/parenleftbig
ρπθ(z,z′),ρπE(z,z′)/parenrightbig
. (4)
We propose to optimize the objective in (4) using off-policy adversarial IfO. We initialize two replay buffers
BEandBto respectively store the sequences of observations generated by the expert and the agent policies,
from which we infer the latent state-transitions (z,z′). Note that we write (z,z′)∼Bto streamline the
notation. Then, given a discriminator Dχ:Z×Z→ [0,1], we write
max
χE(z,z′)∼BE[log(Dχ(z,z′))] +E(z,z′)∼B[log(1−Dχ(z,z′))] +g/parenleftbig
∇χDχ/parenrightbig
, (5)
whereg(·)is defined in (6). As mentioned, alternating the maximization of the loss in (5) with an RL step
leads to the minimization of DJS/parenleftbig
ρπθ(z,z′)||ρπE(z,z′)/parenrightbig
(Goodfellow et al., 2020). Since DJS(·||·)can be used
to upper bound DTV(·,·)(cf. Lemma 1 in Appendix B), this approach effectively minimizes the loss in (4).
In order to stabilize the adversarial training process, it is important to ensure local Lipschitz-continuity of the
learned reward function (Blondé et al., 2022). Therefore, as proposed in Gulrajani et al. (2017), we include in
(5) the gradient penalty term
g/parenleftbig
∇χDχ/parenrightbig
=λE(ˆz,ˆz′)∼P(ˆz,ˆz′)[(||∇χDχ(ˆz,ˆz′)||2−1)2], (6)
whereλis a hyperparameter, and P(ˆz,ˆz′)is defined such that (ˆz,ˆz′)are sampled uniformly along straight lines
between pairs of transitions respectively sampled from BEandB. For additional details on the importance of
the term in (6) for improved stability, refer to our ablation experiments in Appendix E and to Gulrajani et al.
(2017). Finally, from a theoretical standpoint, note that we should perform importance sampling correction in
order to account for the effect of off-policy data when sampling from B(Queeney et al., 2021; 2022). However,
neglecting off-policy correction works well in practice and does not compromise the stability of the algorithm
(Kostrikov et al., 2018).
Latent variable estimation from observations Note that the problem in (4) is defined on the latent
spaceZ. Therefore, we now present a simple and effective method to estimate the latent variable zfrom
sequences of observations. Inspired by the model-free RL from pixels literature, we propose to combine the
successful approaches of observations stacking (Mnih et al., 2013; 2015) and data augmentation (Laskin et al.,
2020b; Kostrikov et al., 2020; Yarats et al., 2021). We stack together the most recent d∈Nobservations,
and provide this stack as an input to a feature extractor which is trained during the RL step. More
specifically, we define a feature extractor ϕδ:Xd→Zsuch thatz=ϕδ(xt−:t)wheret−t−+ 1 =d. When
learning from pixels, we also apply data augmentation to the observations stack to improve the quality of
the extracted features as in Kostrikov et al. (2020). We write aug(xt−:t)to define the augmented stack of
observations. The latent representations zandz′are then computed respectively as z=ϕδ(aug(xt−:t))and
z′=ϕδ(aug(xt−+1:t+1)). We train the feature extractor ϕδwith the critic networks Qψk(k= 1,2) in order
6Published in Transactions on Machine Learning Research (05/2024)
to minimize the loss function
Lδ,ψk(B) =E(z,a,z′)∼B[(Qψk(z,a)−y)2],
y=rχ(z,z′) +γmin
k=1,2Q¯ψk(z′,a′),(7)
rχ(z,z′) =Dχ/parenleftbig
z,z′/parenrightbig
, (8)
whereDχ/parenleftbig
z,z′/parenrightbig
is the discriminator optimized in (5). In (7), ais an action stored in Bused by the agent to
interact with the environment, while a′=πθ(z′) +ϵwhereϵ∼clip(N(0,σ2),−c,c)is a clipped exploration
noise with cthe clipping parameter and N(0,σ2)a univariate normal distribution with zero mean and σ
standard deviation. The reward function rχ(z,z′)is defined as in (8), and ¯ψ1and ¯ψ2are the slow moving
weights for the target Q networks. We provide more implementation details and the complete pseudo-code
for our algorithm in Appendix D.
Note that the feature extractor ϕδis shared by both the critics Qψk, the policy πθ, and the discriminator
Dχ. However, we stop the backpropagation of the gradient from πθandDχintoϕδ. The logic of this
choice involves obtaining a latent variable zthat is not biased towards any of the players in the adversarial
IfO game in (5), but only provides the information necessary to determine the expert and agent expected
performance. This design is motivated by our theoretical analysis which shows how, provided ϕδ:Xd→Z
whereZapproximates a sufficient statistic of the history, all the networks in the adversarial IfO game
can be directly defined on ZasDχ:Z×Z→ [0,1],Qψk:Z×A→ Randπθ:Z→P(A). This is in
contrast with the current literature on V-IfO where all the networks are defined on the observation space X
asDχ:X×X→ [0,1],Qψk:X×A→ Randπθ:X→P(A).
Finally, note that latent variable estimation is an active research area, and it is possible to apply other
techniques such as variational inference (Lee et al., 2020) and contrastive learning (Chen et al., 2020; Grill et al.,
2020). However, we will show in our experiments that the straightforward approach of observations stacking
and data augmentation leads to strong performance in practice, without the need for more complicated
estimation procedures. We include an ablation study on the importance of data augmentation in Appendix E.
Improving RL from pixels using expert videos We have so far considered the pure imitation setting
where a reward function can only be estimated from expert data. However, for many real-world tasks a simple
objective can often be provided to the learning agent. Assuming that videos of experts are also available, we
show how we can use LAIfO to accelerate the RL learning process.
We combine the standard RL objective with our V-IfO objective in (4), leading to the combined problem
max
θEτθ/bracketleftigg∞/summationdisplay
t=0γtR(st,at)/bracketrightigg
−DTV/parenleftbig
ρπθ(z,z′),ρπE(z,z′)/parenrightbig
. (9)
Using the adversarial IfO pipeline presented in (5), we can rewrite (9) as
max
θEτθ/bracketleftigg∞/summationdisplay
t=0γt/parenleftig
R(st,at) +rχ/parenleftbig
zt,zt+1/parenrightbig/parenrightig/bracketrightigg
, (10)
withrχin (8). By learning rχwith LAIfO and optimizing the problem in (10) throughout training, we will
show that we are able to significantly improve sample efficiency on challenging humanoid from pixels tasks
(Tassa et al., 2018) compared to state-of-the-art RL from pixels algorithms (Yarats et al., 2021).
6 Experiments
In this section, we conduct experiments that aim to answer the following questions:
(1)For the V-IfO problem, how does LAIfO compare to PatchAIL (Liu et al., 2023), a state-of-the-art
approach for V-IfO, in terms of asymptotic performance and computational efficiency?
(2)How does the V-IL version of LAIfO with access to expert actions, named Latent Adversarial Imitation
Learning (LAIL) , compare to VMAIL (Rafailov et al., 2021), a state-of-the-art approach for V-IL?
7Published in Transactions on Machine Learning Research (05/2024)
Table 2: Experimental results for V-IfO (i.e., imitation from experts with partial observability and without
access to expert actions). We use DDPG to train experts in a fully observable setting and collect 100
episodes of expert data. All of the expert policies can be downloaded by following the instructions in our
code repository. BC is trained offline using expert observation-action pairs for 104gradient steps. All the
other algorithms are trained for 3×106frames in walker run, hopper hop, cheetah run, quadruped run,
and quadruped walk, and 106frames for the other tasks. We evaluate the learned policies using average
performance over 10episodes. We run each experiment for 6seeds. In the third, fourth and fifth columns, we
report mean and standard deviation of final performance over seeds. In the last column, we report the ratio
of wall-clock times between LAIfO and PatchAIL to achieve 75%of expert performance. For each task, we
highlight the highest asymptotic performance between LAIfO and PatchAIL.
Expert BC LAIfO (our)PatchAIL-W
(Liu et al., 2023)Wall-clock time ratio
to75% expert performance
(LAIfO (our) / PatchAIL-W)
Cup Catch 980 971 ±9.7 967±7.6 804±357 0.69
Finger Spin 932 542 ±219 926±10.7 885±30.8 0.67
Cartpole Swingup 881 329 ±25.0 873±3.6 842±6.7 0.63
Cartpole Balance 990 648 ±36.4 878±239 966±5.5 0.61
Pendulum Swingup 845 427 ±142 786±70.4 829±23.7 0.90
Walker Walk 960 723 ±137 960±2.2 955±7.0 0.15
Walker Stand 980 871 ±77.7 961±20.0 971±10.5 0.27
Walker Run 640 133 ±27.8 618±4.6 569±53.2 0.22
Hopper Stand 920 398 ±96.4 800±46.7 867±33.9 0.16
Hopper Hop 217 45.9±22.1 206±8.5 191±13.0 0.16
Quadruped Walk 970 337 ±50.5594±92.9 263±242 NA∗
Quadruped Run 950 340 ±75.2 516±132 471±219 NA∗
Cheetah Run 900 106 ±26.3773±41.2 695±312 0.46
*75% of the expert performance was not achieved by any algorithm.
(3)What is the impact on performance due to partial observability and the absence of expert actions?
(4)Can LAIfO leverage expert videos to improve the efficiency of RL from pixels in high-dimensional
continuous robotic tasks?
For more details about the hardware used to carry out these experiments, all the learning curves, additional
ablation studies, and other implementation details, refer to Appendix E and to our code.
Visual Imitation from Observations In order to address Question (1), we evaluate LAIfO and
PatchAIL (Liu et al., 2023), in its weight regularized version denoted by PatchAIL-W, on 13different
tasks from the DeepMind Control Suite (Tassa et al., 2018). We also compare these algorithms to Behavioral
Cloning (BC) (Pomerleau, 1988) for reference. Note that BC uses observation-action pairs. The results
are summarized in Table 2, Figure 1, and Figure 2. Table 2 includes the asymptotic performance of each
algorithm, as well as the ratio of wall-clock times between LAIfO and PatchAIL to achieve 75% of expert
performance. Figure 1 depicts the average return per episode throughout training as a function of wall-clock
time. Moreover, we include in Figure 2 plots showing the average return per episode as a function of training
steps. These results demonstrate that LAIfO can successfully solve the V-IfO problem, achieving asymptotic
performance comparable to the state-of-the-art baseline PatchAIL. Importantly, LAIfO is significantly more
computationally efficient than PatchAIL . This is well highlighted both in Table 2 and in Figure 1, where
we show that LAIfO always converges faster than PatchAIL in terms of wall-clock time . This improved
computational efficiency is the result of performing imitation on the latent space Z, instead of directly on the
high-dimensional observation space X(i.e., pixel space) as in PatchAIL. Finally, in Table 4 we examine the
impact of the amount of expert data on performance. Throughout these experiments, LAIfO does not exhibit
a tangible drop in performance due to the decrease of available expert data, and it consistently outperforms
PatchAIL.
8Published in Transactions on Machine Learning Research (05/2024)
0 5 10 15 20
Hours020040060080010001200Episode ReturnCup Catch
0 5 10 15 20
Hours02004006008001000Finger Spin
0 5 10 15
Hours02004006008001000Cartpole Swingup
0 5 10
Hours20040060080010001200Cartpole Balance
0 5 10
Hours02004006008001000Pendulum Swingup
0 10 20 30
Hours02004006008001000Episode ReturnWalker Walk
0 10 20 30
Hours2004006008001000Walker Stand
0 25 50 75 100
Hours0100200300400500600Walker Run
0 5 10 15
Hours0200400600800Hopper Stand
0 20 40
Hours050100150200Hopper Hop
0 10 20 30
Hours02004006008001000Episode ReturnQuadruped Walk
0 20 40
Hours0200400600800Quadruped Run
0 20 40
Hours02004006008001000Cheetah Run
LAIfO (our)
PatchAIL-W
BC
expert
Figure 1: Learning curves for the V-IfO results in Table 2. Plots show the average return per episode as
a function of wall-clock time. Our algorithm LAIfO achieves state-of-the-art asymptotic performance, and
significantly reduces computation time compared to PatchAIL.
0.00 0.25 0.50 0.75 1.00
Steps (×106)020040060080010001200Episode ReturnCup Catch
0.00 0.25 0.50 0.75 1.00
Steps (×106)02004006008001000Finger Spin
0.00 0.25 0.50 0.75 1.00
Steps (×106)02004006008001000Cartpole Swingup
0.00 0.25 0.50 0.75 1.00
Steps (×106)20040060080010001200Cartpole Balance
0.00 0.25 0.50 0.75 1.00
Steps (×106)02004006008001000Pendulum Swingup
0.00 0.25 0.50 0.75 1.00
Steps (×106)02004006008001000Episode ReturnWalker Walk
0.00 0.25 0.50 0.75 1.00
Steps (×106)2004006008001000Walker Stand
0 1 2 3
Steps (×106)0100200300400500600Walker Run
0.00 0.25 0.50 0.75 1.00
Steps (×106)0200400600800Hopper Stand
0 1 2 3
Steps (×106)050100150200Hopper Hop
0 1 2 3
Steps (×106)02004006008001000Episode ReturnQuadruped Walk
0 1 2 3
Steps (×106)0200400600800Quadruped Run
0 1 2 3
Steps (×106)02004006008001000Cheetah Run
LAIfO (our)
PatchAIL-W
BC
expert
Figure 2: Learning curves for the V-IfO results in Table 2. Plots show the average return per episode as a
function of training steps.
9Published in Transactions on Machine Learning Research (05/2024)
Table 3: Experimental results for V-IL (i.e., imitation from experts with partial observability and access to
expert actions). We use DDPG to train experts in a fully observable setting and collect 100episodes of expert
data. The experiments are conducted as in Table 2. In the third, fourth and fifth columns, we report mean
and standard deviation of final performance over seeds. In the last column, we report the ratio of wall-clock
times between the two algorithms to achieve 75%of expert performance. For each task, we highlight the
highest asymptotic performance between LAIL and VMAIL.
Expert BC LAIL (our)VMAIL
(Rafailov et al., 2021)Wall-clock time ratio
to75% expert performance
(LAIL (our) / VMAIL)
Cup Catch 980 971 ±9.7 962±18.0 939±37.4 0.31
Finger Spin 932 542 ±219 775±345 476±352 0.21
Cartpole Swingup 881 329 ±25.0 873±3.0 512±291 0.13
Cartpole Balance 990 648 ±36.3 982±1.6 868±104 0.10
Pendulum Swingup 845 427 ±142 825±45.1 723±143 1.43
Walker Walk 960 723 ±137 946±8.5 939±9.8 0.40
Walker Stand 980 871 ±77.7893±106 805±309 0.82
Walker Run 640 133 ±27.8 625±5.1 516±224 0.58
Hopper Stand 920 398 ±96.4764±111 567±285 0.12
Hopper Hop 217 45.9±22.1208±3.1 72.3±73.0 0.23
Quadruped Walk 970 337 ±50.5500±182 223±51.9 NA∗
Quadruped Run 950 340 ±75.2697±102 127±66.0 NA∗∗
Cheetah Run 900 106 ±26.3811±67.9 539±367 0.83
*75% of the expert performance was not achieved by any algorithm.
**75% of the expert performance was not achieved by VMAIL.
Visual Imitation Learning To answer Question (2), we test LAIL, the V-IL version of LAIfO, and
VMAIL (Rafailov et al., 2021) using the same experimental setup that we considered in the V-IfO setting.
As for V-IfO, we also compare these algorithms to BC for reference. VMAIL stands for Variational Model
Adversarial Imitation Learning , and represents a model-based version of generative adversarial IL built upon
the variational models presented in Hafner et al. (2019b;a; 2020). LAIL is obtained by simply defining the
discriminator in (5) as Dχ:Z×A→ [0,1]rather than Dχ:Z×Z→ [0,1]as in LAIfO. The results for
these experiments are summarized in Table 3, Figure 3, and Figure 4. Compared to VMAIL, we see that
LAIL achieves better asymptotic performance and better computational efficiency . While both algorithms
perform imitation on a latent space Z, LAIL is a model-free algorithm that requires a lower number of
learnable parameters compared to the model-based VMAIL. VMAIL must learn an accurate world model
during training, which can be a challenging and computationally demanding task. The model learning
process contributes to higher wall-clock times, and can also lead to instability in the training process for
some environments (cf. Figure 4). On the other hand, the model-free approach of LAIL results in stable
training that yields faster convergence and better efficiency. Finally, in Table 4 we examine the impact of the
amount of expert data on the final results. Throughout these experiments, both LAIL and VMAIL exhibit
reliable results and no tangible decrease in performance is observed due to relying on less expert data.
Ablation study In order to answer Question (3), we compare performance for each type of imitation
from experts in Table 1. For the partially observable setting, we consider our algorithms LAIL and LAIfO.
For the fully observable setting, we consider DAC (Kostrikov et al., 2018) and our implementation of DAC
from Observations (DACfO) . We provide the full learning curves for DAC and DACfO in Appendix E (cf.
Table 6 and Figure 7). The results are summarized in Figure 5, which shows the average normalized return
obtained by each algorithm throughout the different tasks in Table 2. These experiments highlight how our
algorithms can successfully address the absence of expert actions and partial observability, suffering only
marginal performance degradation due to these additional challenges. As explained in our theoretical analysis
in Section 4, partial observability is addressed by estimating a latent state representation that successfully
approximates a sufficient statistic of the history. On the other hand, marginal degradation due to the absence
of expert actions occurs either because we are in the context described by Theorem 2, where the environment
reward function does not depend on actions, or because Cin Theorem 1 becomes negligible.
10Published in Transactions on Machine Learning Research (05/2024)
0 5 10 15 20
Hours020040060080010001200Episode ReturnCup Catch
0 5 10 15 20
Hours020040060080010001200Finger Spin
0 5 10
Hours02004006008001000Cartpole Swingup
0 5 10 15
Hours2004006008001000Cartpole Balance
0 5 10 15
Hours02004006008001000Pendulum Swingup
0 10 20
Hours02004006008001000Episode ReturnWalker Walk
0 10 20 30
Hours20040060080010001200Walker Stand
0 20 40 60
Hours0200400600Walker Run
0 5 10 15
Hours02004006008001000Hopper Stand
0 10 20 30 40
Hours050100150200Hopper Hop
0 10 20 30 40
Hours02004006008001000Episode ReturnQuadruped Walk
0 10 20 30 40
Hours0200400600800Quadruped Run
0 10 20 30
Hours02004006008001000Cheetah Run
LAIL (our)
VMAIL
BC
expert
Figure 3: Learning curves for the V-IL results in Table 3. Plots show the average return per episode as
a function of wall-clock time. LAIL outperforms VMAIL in terms of both asymptotic performance and
computational efficiency.
0.00 0.25 0.50 0.75 1.00
Steps (×106)020040060080010001200Episode ReturnCup Catch
0.00 0.25 0.50 0.75 1.00
Steps (×106)020040060080010001200Finger Spin
0.00 0.25 0.50 0.75 1.00
Steps (×106)02004006008001000Cartpole Swingup
0.00 0.25 0.50 0.75 1.00
Steps (×106)2004006008001000Cartpole Balance
0.00 0.25 0.50 0.75 1.00
Steps (×106)02004006008001000Pendulum Swingup
0.00 0.25 0.50 0.75 1.00
Steps (×106)02004006008001000Episode ReturnWalker Walk
0.00 0.25 0.50 0.75 1.00
Steps (×106)20040060080010001200Walker Stand
0 1 2 3
Steps (×106)0200400600Walker Run
0.00 0.25 0.50 0.75 1.00
Steps (×106)02004006008001000Hopper Stand
0 1 2 3
Steps (×106)050100150200Hopper Hop
0 1 2 3
Steps (×106)02004006008001000Episode ReturnQuadruped Walk
0 1 2 3
Steps (×106)0200400600800Quadruped Run
0 1 2 3
Steps (×106)02004006008001000Cheetah Run
LAIL (our)
VMAIL
BC
expert
Figure 4: Learning curves for the V-IL results in Table 3. Plots show the average return per episode as a
function of training steps.
11Published in Transactions on Machine Learning Research (05/2024)
Table 4: Performance for different numbers of expert episodes on Walker Run. Except for the number of
expert episodes, the experiments are conducted as in Table 2 and Table 3. We report mean and standard
deviation after training for 106frames. For both the V-IL and the V-IfO settings, we highlight the highest
performance.
Walker Runn Expert episodes
1 10 20 50 100
BC 28.6±1.9 70.7±1.8 87.0±1.8 113 ±1.8 133 ±1.8
VMAIL (Rafailov et al., 2021) 520±224 630±21.9 517±221 619±11.8 524±222
LAIL (our) 614±7.6 626±7.3 627±6.8 611±22.5613±20.3
PatchAIL-W (Liu et al., 2023) 494±199 561 ±36.0 586 ±39.7 604 ±22.5 598 ±34.7
LAIfO (our) 612±22.8 620 ±6.4 623 ±7.6 618 ±10.1 621 ±4.8
Imitation 
 without expert actions 
 and partial observability 
 (V-IfO)Imitation 
 with expert actions 
 and partial observability 
 (V-IL)Imitation 
 without expert actions 
 and full observability 
 (IfO)Imitation 
 with expert actions 
 and full observability 
 (IL)0.60.70.80.91.01.1normalized returnexpert performance
0.89 0.890.920.97
Figure 5: Normalized returns obtained by each type of imitation from experts over the tasks in Table 2. For
each task and random seed, normalized returns represent performance divided by the expert performance for
the considered task (Expert column in Table 2 and Table 3). For each type of imitation from experts, we plot
mean and standard deviation over the full set of runs. The performance of our algorithms in the partially
observable setting are comparable to the performance in the fully observable setting, and the absence of
expert actions and partial observability leads only to marginal performance degradation.
Improving RL using expert videos We answer Question (4)by applying LAIfO to the problem in
(9) for the humanoid from pixels environment. We consider the state-of-the-art model-free RL from pixels
algorithms, DrQv2 (Yarats et al., 2021) as a baseline. The results are illustrated in Figure 6. By leveraging
expert videos, we see that our algorithm significantly outperforms the baseline in terms of sample efficiency.
This result highlights the value of leveraging expert videos to improve the sample efficiency of RL algorithms,
in particular when dealing with challenging tasks.
0 10 20 30
Steps (×106)0500Episode ReturnHumanoid Stand
0 10 20 30
Steps (×106)0500Humanoid Walk
0 10 20 30
Steps (×106)0100200300Humanoid Run
RL + LAIfO DrQ v2
Figure 6: Performance using the multi-objective RL framework in (9) on the humanoid environment. The
experiments are designed as in Table 2. We report mean and standard error over seeds. For DrQv2, note
that we use as comparison the curves provided in the official Github repository of the paper.
12Published in Transactions on Machine Learning Research (05/2024)
7 Conclusion
In this work, we formally analyzed the V-IfO problem and introduced our algorithm LAIfO as an effective
solution. We experimentally showed that our approach matches the performance of state-of-the-art V-IL
and V-IfO methods, while requiring significantly less computational effort due to our model-free approach
in latent space. Furthermore, we showed how LAIfO can be used to improve the efficiency and asymptotic
performance of RL methods by leveraging expert videos.
Limitations and future work Despite the advancement in addressing the V-IfO problem, it is important
to understand the limitations of our approach. The primary limitation arises from the assumption that
the expert and the agent act within the same POMDP. In realistic scenarios, such alignment rarely occurs,
emphasizing the need for methods that can handle dynamics mismatch and visual domain adaptation. This
is a crucial next step towards enabling successful learning from expert videos. Furthermore, throughout
this work we have used adversarial learning for divergence minimization between distributions. Adversarial
learning can introduce optimization challenges and stability issues. While we propose practical solutions to
mitigate these problems, exploring alternatives to this framework offers another interesting avenue for future
research. Additionally, from an experimental standpoint, our emphasis has been on robotics control tasks.
In the future, we plan to address navigation tasks, considering not only third-view perspectives but also
egocentric camera viewpoints. In this context, a challenging and relevant consideration is the correspondence
problem, i.e., the problem of enabling egocentric policy learning directly from third-view videos of experts.
Finally, we are interested in testing our algorithms on more realistic scenarios that go beyond simulated
environments. Our long-term goal is to provide solutions for real-world problems, such as vehicle navigation
and robotic manipulation, and to enable imitation directly from videos of biological systems such as humans
and animals.
8 Acknowledgements
Vittorio Giammarino and Ioannis Ch. Paschalidis were partially supported by the NSF under grants IIS-
1914792, CCF-2200052, and ECCS-2317079, by the ONR under grant N00014-19-1-2571, by the DOE under
grant DE-AC02-05CH11231, by the NIH under grant UL54 TR004130, and by Boston University. James
Queeney was exclusively supported by Mitsubishi Electric Research Laboratories.
References
Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings
of the twenty-first International Conference on Machine Learning , pp. 1, 2004.
Karl J. Astrom. Optimal control of markov decision processes with incomplete state estimation. J. Math.
Anal. Applic. , 10:174–205, 1965.
Christopher G. Atkeson and Stefan Schaal. Robot learning from demonstration. In Proceedings of the
fourteenth International Conference on Machine Learning , pp. 12–20, 1997.
Feryal Behbahani, Kyriacos Shiarlis, Xi Chen, Vitaly Kurin, Sudhanshu Kasewa, Ciprian Stirbu, Joao Gomes,
Supratik Paul, Frans A. Oliehoek, Joao Messias, et al. Learning from demonstration in the wild. In 2019
International Conference on Robotics and Automation (ICRA) , pp. 775–781. IEEE, 2019.
Lionel Blondé and Alexandros Kalousis. Sample-efficient imitation learning via generative adversarial nets. In
The 22nd International Conference on Artificial Intelligence and Statistics , pp. 3138–3148. PMLR, 2019.
Lionel Blondé, Pablo Strasser, and Alexandros Kalousis. Lipschitzness is all you need to tame off-policy
generative adversarial imitation learning. Machine Learning , 111(4):1431–1521, 2022.
Edoardo Cetin and Oya Celiktutan. Domain-robust visual imitation learning with mutual information
constraints. arXiv preprint arXiv:2103.05079 , 2021.
13Published in Transactions on Machine Learning Research (05/2024)
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
learning of visual representations. In International conference on machine learning , pp. 1597–1607. PMLR,
2020.
Zhihao Cheng, Liu Liu, Aishan Liu, Hao Sun, Meng Fang, and Dacheng Tao. On the guaranteed almost
equivalence between imitation learning from observation and demonstration. IEEE Transactions on Neural
Networks and Learning Systems , 2021.
Robert Dadashi, Léonard Hussenot, Matthieu Geist, and Olivier Pietquin. Primal wasserstein imitation
learning. arXiv preprint arXiv:2006.04678 , 2020.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement
learning. arXiv preprint arXiv:1710.11248 , 2017.
Tanmay Gangwani, Joel Lehman, Qiang Liu, and Jian Peng. Learning belief representations for imitation
learning in POMDPs. In Uncertainty in Artificial Intelligence , pp. 1061–1071. PMLR, 2020.
Tanmay Gangwani, Yuan Zhou, and Jian Peng. Imitation learning from observations under transition model
disparity. arXiv preprint arXiv:2204.11446 , 2022.
Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimization perspective
on imitation learning methods. In Proceedings of the Conference on Robot Learning , pp. 1259–1277. PMLR,
2020.
Vittorio Giammarino, Matthew F Dunne, Kylie N Moore, Michael E Hasselmo, Chantal E Stern, and
Ioannis Ch Paschalidis. Combining imitation and deep reinforcement learning to human-level performance
on a virtual foraging task. Adaptive Behavior , 2023a.
Vittorio Giammarino, James Queeney, Lucas C. Carstensen, Michael E. Hasselmo, and Ioannis Ch. Paschalidis.
Opportunities and challenges from using animal videos in reinforcement learning for navigation. IFAC-
PapersOnLine , 56(2):9056–9061, 2023b. 22nd IFAC World Congress.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM , 63(11):
139–144, 2020.
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,
Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your
own latent-a new approach to self-supervised learning. Advances in neural information processing systems ,
33:21271–21284, 2020.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved
training of Wasserstein GANs. Advances in Neural Information Processing Systems , 30, 2017.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar,
Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv
preprint arXiv:1812.05905 , 2018.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603 , 2019a.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson.
Learninglatentdynamicsforplanningfrompixels. In Proceedings of the thirty-sixth International Conference
on Machine Learning , pp. 2555–2565. PMLR, 2019b.
Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world
models.arXiv preprint arXiv:2010.02193 , 2020.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in Neural Information
Processing Systems , 29, 2016.
14Published in Transactions on Machine Learning Research (05/2024)
Anthony Hu, Gianluca Corrado, Nicolas Griffiths, Zak Murez, Corina Gurau, Hudson Yeo, Alex Kendall,
Roberto Cipolla, and Jamie Shotton. Model-based imitation learning for urban driving. arXiv preprint
arXiv:2210.07729 , 2022.
Haresh Karnan, Faraz Torabi, Garrett Warnell, and Peter Stone. Adversarial imitation learning from
video using a state observer. In 2022 International Conference on Robotics and Automation (ICRA) , pp.
2452–2458. IEEE, 2022.
Rahul Kidambi, Jonathan Chang, and Wen Sun. Mobile: Model-based imitation learning from observation
alone.Advances in Neural Information Processing Systems , 34:28598–28611, 2021.
Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson.
Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learning.
arXiv preprint arXiv:1809.02925 , 2018.
Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution matching.
arXiv preprint arXiv:1912.05032 , 2019.
Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep
reinforcement learning from pixels. arXiv preprint arXiv:2004.13649 , 2020.
Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representations for
reinforcement learning. In Proceedings of the thirty-seventh International Conference on Machine Learning ,
pp. 5639–5650. PMLR, 2020a.
Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement
learning with augmented data. Advances in Neural Information Processing Systems , 33:19884–19895, 2020b.
Alex X. Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic: Deep
reinforcement learning with a latent variable model. Advances in Neural Information Processing Systems ,
33:741–752, 2020.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver,
and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 ,
2015.
Minghuan Liu, Tairan He, Weinan Zhang, Shuicheng Yan, and Zhongwen Xu. Visual imitation learning with
patch rewards. arXiv preprint arXiv:2302.00965 , 2023.
YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning
to imitate behaviors from raw video via context translation. In 2018 IEEE International Conference on
Robotics and Automation (ICRA) , pp. 1118–1125. IEEE, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 , 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex
Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529–533, 2015.
Andrew Y. Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In Proceedings of the
seventeenth International Conference on Machine Learning , volume 1, pp. 2, 2000.
Ryo Okumura, Masashi Okada, and Tadahiro Taniguchi. Domain-adversarial and-conditional state space
model for imitation learning. In 2020 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS) , pp. 5179–5186. IEEE, 2020.
Yury Polyanskiy and Yihong Wu. Information theory: From coding to learning, 2022.
15Published in Transactions on Machine Learning Research (05/2024)
Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in Neural Information
Processing Systems , 1, 1988.
JamesQueeney, IoannisCh.Paschalidis, andChristosG.Cassandras. Generalizedproximalpolicyoptimization
with sample reuse. Advances in Neural Information Processing Systems , 34:11909–11919, 2021.
James Queeney, Ioannis Ch. Paschalidis, and Christos G. Cassandras. Generalized policy improvement
algorithms with theoretically supported sample reuse. arXiv preprint arXiv:2206.13714 , 2022.
Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. Visual adversarial imitation learning
using variational models. Advances in Neural Information Processing Systems , 34:3016–3028, 2021.
Roberta Raileanu, Maxwell Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus. Automatic data
augmentation for generalization in reinforcement learning. Advances in Neural Information Processing
Systems, 34:5402–5415, 2021.
Siddharth Reddy, Anca D Dragan, and Sergey Levine. Sqil: Imitation learning via reinforcement learning
with sparse rewards. In International Conference on Learning Representations , 2019.
Alexander Reske, Jan Carius, Yuntao Ma, Farbod Farshidian, and Marco Hutter. Imitation learning from mpc
for quadrupedal multi-gait control. In 2021 IEEE International Conference on Robotics and Automation
(ICRA), pp. 5014–5020. IEEE, 2021.
Stéphane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings of the thirteenth
International Conference on Artificial Intelligence and Statistics , pp. 661–668. JMLR Workshop and
Conference Proceedings, 2010.
Stuart Russell. Learning agents for uncertain environments. In Proceedings of the eleventh Annual Conference
on Computational Learning Theory , pp. 101–103, 1998.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347 , 2017.
Kenneth Shaw, Shikhar Bahl, and Deepak Pathak. Videodex: Learning dexterity from internet videos. In
Proceedings of The sixth Conference on Robot Learning , pp. 654–665. PMLR, 2023.
Bradly C. Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning. arXiv preprint
arXiv:1703.01703 , 2017.
Umar Syed and Robert E. Schapire. A game-theoretic approach to apprenticeship learning. Advances in
Neural Information Processing Systems , 20, 2007.
Umar Syed, Michael Bowling, and Robert E. Schapire. Apprenticeship learning using linear programming. In
Proceedings of the twenty-fifth International Conference on Machine Learning , pp. 1032–1039, 2008.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas
Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690 ,
2018.
Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. arXiv preprint
arXiv:1805.01954 , 2018a.
Faraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observation. arXiv
preprint arXiv:1807.06158 , 2018b.
Chao Yang, Xiaojian Ma, Wenbing Huang, Fuchun Sun, Huaping Liu, Junzhou Huang, and Chuang Gan.
Imitation learning from observations by minimizing inverse dynamics disagreement. Advances in Neural
Information Processing Systems , 32, 2019.
Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foundation models
for decision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129 , 2023.
16Published in Transactions on Machine Learning Research (05/2024)
Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control:
Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645 , 2021.
Jimuyang Zhang and Eshed Ohn-Bar. Learning by watching. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 12711–12721, 2021.
Jimuyang Zhang, Ruizhao Zhu, and Eshed Ohn-Bar. Selfd: self-learning large-scale driving policies from
the web. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.
17316–17326, 2022.
Zhuangdi Zhu, Kaixiang Lin, Bo Dai, and Jiayu Zhou. Off-policy imitation learning from observations.
Advances in Neural Information Processing Systems , 33:12402–12413, 2020.
Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, Anind K. Dey, et al. Maximum entropy inverse
reinforcement learning. In Twenty-Third AAAI Conference on Artificial Intelligence , volume 8, pp.
1433–1438. Chicago, IL, USA, 2008.
17