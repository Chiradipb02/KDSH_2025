Published in Transactions on Machine Learning Research (08/2024)
AdaStop: adaptive statistical testing for sound comparisons
of Deep RL agents
Timothée Mathieu timothee.mathieu@inria.fr
Inria, Université de Lille, CNRS, Centrale Lille, UMR 9189 – CRIStAL
Riccardo Della Vecchia ric.della.vecchia@gmail.com
Inria, Université de Lille, CNRS, Centrale Lille, UMR 9189 – CRIStAL
Alena Shilova alena.shilova@inria.fr
Inria, Université de Lille, CNRS, Centrale Lille, UMR 9189 – CRIStAL
Matheus Medeiros Centa matheus.medeiros-centa@inria.fr
Université de Lille, Inria, CNRS, Centrale Lille, UMR 9189 – CRIStAL
Hector Kohler hector.kohler@inria.fr
Université de Lille, Inria, CNRS, Centrale Lille, UMR 9189 – CRIStAL
Odalric-Ambrym Maillard odalric.maillard@inria.fr
Inria, Université de Lille, CNRS, Centrale Lille, UMR 9189 – CRIStAL
Philippe Preux philippe.preux@inria.fr
Université de Lille, Inria, CNRS, Centrale Lille, UMR 9189 – CRIStAL
Reviewed on OpenReview: https: // openreview. net/ forum? id= lXyZr9TLEU
Abstract
Recently, the scientific community has questioned the statistical reproducibility of many
empirical results, especially in the field of machine learning. To contribute to the resolution
of this reproducibility crisis, we propose a theoretically sound methodology for comparing
the performance of a set of algorithms. We exemplify our methodology in Deep Reinforce-
ment Learning (Deep RL). The performance of one execution of a Deep RL algorithm is
a random variable. Therefore, several independent executions are needed to evaluate its
performance. When comparing algorithms with random performance, a major question
concerns the number of executions to perform to ensure that the result of the comparison
is theoretically sound. Researchers in Deep RL often use less than 5 independent execu-
tions to compare algorithms: we claim that this is not enough in general. Moreover, when
comparing more than 2 algorithms at once, we have to use a multiple tests procedure to
preserve low error guarantees. We introduce AdaStop , a new statistical test based on
multiple group sequential tests. When used to compare algorithms, AdaStop adapts the
number of executions to stop as early as possible while ensuring that enough information has
been collected to distinguish algorithms that have different score distributions. We prove
theoretically that AdaStop has a low probability of making a (family-wise) error. We illus-
trate the effectiveness of AdaStop in various use-cases, including toy examples and Deep
RL algorithms on challenging Mujoco environments. AdaStop is the first statistical test
fitted to this sort of comparisons: it is both a significant contribution to statistics, and an
important contribution to computational studies performed in reinforcement learning and
in other domains.
1Published in Transactions on Machine Learning Research (08/2024)
1 Introduction
In many fields of computer science, it is customary to perform an experimental investigation to compare the
practical performance of two or more algorithms. When the behavior of an algorithm is non-deterministic
(for instance because the algorithm is non-deterministic, or because the data it is fed upon is random),
the performance of the algorithm is a random variable. Then, the way to do this comparison is not clear.
Usually, one executes the algorithm (or rather an implementation of the algorithm: this point will soon be
clarified) several times in order to obtain an average performance and its variability. How much “several” is
depends on the authors, and some contingencies: thanks to the law of large numbers, one may think that
the larger the better, the more accurate the estimates of the average and the variability. This may be a
satisfactory answer, but when a single execution lasts days or even weeks or months (such as LLM training),
performing such a “large enough” number of executions is impossible.
To illustrate this point, let us consider the field of deep reinforcement learning (Deep RL), that is reinforce-
ment learning algorithms that use a neural network to represent what they learn. We surveyed all deep
RL papers published in the proceedings of the International Conference on Machine Learning in 2022 (see
Fig.1a). In the vast majority of these papers, only a few executions have been performed: among the 18
papers using the Mujoco tasks, only 3 papers performed more than 10 runs, and 11 papers used only 3 to 5
runs. This begs the question: if we can be confident that 3 executions are not enough, how many executions
would be enough to draw statistically significant conclusions? Conversely, are 80 executions not too much?
This is not only a matter of computation time and computational resources occupation. Each execution
contributes to pollute our planet and to the climate change. There is no answer to these questions today.
Moreover, if someone redoes the comparison of the same algorithms on the same task, the conclusions should
be the same: this concept is known as statistical reproducibility (Agarwal et al., 2021; Colas et al., 2019;
Goodman et al., 2016) and should be a feature of any good experimental design.
This paper provides a partial answer to the problem: we introduce AdaStop , a new statistical test tailored
to the problem of small sample sizes in which we cannot suppose that the data are Gaussian. On the
application side, we demonstrate the use of AdaStop in practice and show its efficiency in dealing with
comparison of Deep RL algorithms. On the theory side, we show the usual non-asymptotic and asymptotic
guarantees of such a nonparametric test: we show that for large sample sizes, AdaStop controls the false
positive rate appropriately, and we prove a non-asymptotic bound on false positive ( i.e.family-wise error)
when comparing the distributions. We keep as future work the question of the non-asymptotic comparison
of the means of distributions.
This paper is accompanied by a software program that implements the test which is very easy to use. In
short, in this paper, we provide a methodological approach and its actual implementation to compare the
performance of algorithms having random performance in a statistically significant way, while trying to
minimize the computational effort to reach such a conclusion. Such experimental investigations arise in
various fields of computational AI such as machine learning, and computational optimization. We will use
AdaStop in the field of reinforcement learning to illustrate this paper, but its application to other fields of
computational AI is straightforward, as well as in many other fields of science using computational studies
to compare the performance of algorithms.
The organization of the paper is as follows: in an informal way, we detail the requirements that has to fulfil
a statistical test to fit our expectations in Section 2. In this section, we also make clear the limitations
ofAdaStop . Section 3 introduces the main ingredients of our test, AdaStop . Section 4 in the formal
analysis of the properties of AdaStop . Proofs are established in appendices C to F. We illustrate the use of
AdaStop in Section 5 before concluding. Appendix A lists all the notations used in this paper. Appendix
B provides a minimal exposition of the main concepts of hypothesis testing for readers who are not trained
in statistics.
To reproduce the experiments of this paper, the python code is freely available on GitHub at https:
//github.com/TimotheeMathieu/Adaptive_stopping_MC_RL . In addition, we provide a library and
command-line tool that can be used independently: the AdaStop Python package is available at https:
//github.com/TimotheeMathieu/adastop .
2Published in Transactions on Machine Learning Research (08/2024)
2 Statistical reproducibility in RL through the lens of statistical tests
In this section we begin by identifying the key concepts necessary to discuss reproducibility in the case of
Deep RL. Then we investigate the pros and cons of statistical tests to answer the statistical reproducibility
problem in Deep RL, and we compare this methodology to current practices in experimental Deep RL.
2.1 Definitions
First, we define some key terms that we use in the rest of the paper.
As raised above, we do not compare algorithms but a certain implementation of an algorithm using a certain
set of values for its hyperparameters. In D. Knuth’s spirit (Knuth, 1968), we use the term algorithm in
its usual meaning in computer science as the description of the basic operations required to transform a
certain input into a certain output. By basic operations we mean the use of variables and simple operations
(arithmetical, logical, etc), along with assignments to variables, sequences of instructions, tests and loops.
Such an algorithm typically has some hyperparameters that control its behavior (a threshold, the dimension
of the input domain, how a certain variable decays in time, the architecture of a neural network, etc). In
this regard, we may say that as presented in (Schulman et al., 2017), PPO is an algorithm. However, one
should be cautious that many aspects of PPO may be defined in various ways, and that the notion of “the
PPO algorithm” is not as clearly defined as e.g.“the quicksort algorithm”. The same may be said for all
Deep RL algorithms for which there exist many variations.
Let us make clear the distinction between parameters and hyperparameters: parameters are learned from
data, while hyperparameters are set a priori. For instance, the weights of a neural network are parameters,
while the architecture of the network is a hyperparameter (unless this architecture is also learned during the
training of the agent, which is far from being a common practice in Deep RL).
We use the term agentto refer to a certain implementation of an algorithm along with the value of its
hyperparameters. If an agent is run several times, the value of the hyperparameters is the same at the
beginning of each run. A special case concerns the seed of the pseudo-random number generator. Deep RL
algorithms typically use a different seed to initialize each run. So the seed is not part of the definition of
an agent. In the context of reinforcement learning, we call policya trained agent. A policy is a decision
function that maps observations to actions.
Thescoreof an agent is a numerical value that quantifies its performance. Its definition is really up
to the experimenter and the objective of the experiments she designed. In the case of an RL agent, the
score is usually a numerical value computed from policy evaluations. For instance, the score used in a
given experiment may be the mean episodic return, or its variance, or many other quantities (running time,
memory consumption, number of updates, etc). In most Deep RL research, agents are compared as follows:
1) an agent is trained on one random seed, 2) after training, the policy is evaluated for a given number
of episodes to compute one score, 3) this evaluation is repeated Ktimes eventually providing Kscores,
4) then a statistic is computed over these Kscores, 5) some conclusion about the performance of the various
agents/algorithms is drawn. We suppose that enough policy evaluations have been performed to account
for the possible stochasticity of both the policy and the environment. AdaStop is concerned with having a
significant comparison of the theoretical mean of the scores while using as few random seeds as possible.
2.2 Ingredients for an appropriate statistical test
Our goal is to provide a statistical test to decide whether one agent performs better than some others.
However, strictly speaking, we really decide whether two agents perform equally or not by comparing the
statistic of their scores measured on a set of (evaluation) runs, up to some confidence α. Let us assume that
the statistic is the mean. Then, when AdaStop concludes that the means of the 2 agents differ, it is common
practice to rank the agents based on their comparison. However, strictly speaking, this is an abuse and this
conclusion is only valid up to some probability which is usually considered so large that its alternative can
be rejected. To stress this point, we will write that an agent is “most likely performing better” than an other
agent when the test of their mean performances is rejected. We will denote this with the acronym MLB.
3Published in Transactions on Machine Learning Research (08/2024)
The reader should note that this subtlety is not only true for AdaStop but it is true for all statistical tests
based on the equality of the means of two distributions. This point is rarely made clear in publications.
Now, let us list the requirements and difficulties we have to face in the design of such a statistical test.
First, many tests in statistics are defined for Gaussian random variables. However, one quickly figures out
that the observed performance of an agent is usually not Gaussian. Fig.1b illustrates this point: we represent
the distribution of the performance of 4 agents implementing 4 different RL algorithms (PPO, SAC, DDPG,
TRPO): the performance is usually multi-modal, and it is not even a mixture of a few Gaussian distributions
as it may seem. This leads us to a nonparametric test. Second, we would like to perform the minimal number
of runs. This leads us to the use of a sequential adaptive test that tells us if we need to run the agents a
few more times, or if we can take a decision in a statistically significant way with the already collected data.
Third, we want to be able to compare more than 2 agents which leads us to multiple testing. Fourth, we
want the conclusions of the test to be statistically reproducible, that is, if someone reproduces the execution
of the agents and applies the test in the same way as someone else, the conclusion is the same. Fifth, we want
to keep the number of runs within reasonable limits: for that purpose, we set a maximum number of scores
to collect: if no decision can be made using this budget, the test can not decide whether an agent is MLB
than the others. The first 3 requirements call for a nonparametric, sequential, multiple test. A candidate
statistical test that may verify all these properties can be found in group sequential permutation test (see
the textbook (Jennison & Turnbull, 1999) on general group sequential tests). We use these 5 ingredients to
construct AdaStop .
0 10 20 30 40 50 60 70 800123456
(a)
1000
 0 1000 2000 3000 4000 5000 60000.00000.00020.00040.00060.0008DensityPPO
SAC
DDPG
TRPO (b)
Figure 1: Motivations for AdaStop . (a): a census of the number of scores ( Nscore) used in RL papers using
Mujoco environments published in the proceedings of ICML 2022. (b): estimations of the score distributions
of 4 well-known Deep RL agents on Hopper, a Mujoco environment, with Nscore = 30. We see from the
census that most experiments used 5 or less scores ( Nscore≤5) to draw conclusions. Those conclusions are
most likely statistically wrong as they are equivalent to drawing 5 samples from distributions similar to the
right plot to draw conclusions about the empirical means.
AdaStop , our proposed statistical test, meets all these expectations. Before diving into the technical details,
let us briefly explain how AdaStop is used in practice. Fig. 2 illustrates an execution of AdaStop on a
small example. Let us suppose that we want to compare the performance of 2 agents, a green agent and
a blue agent. Fig. 2 illustrates the sequential nature of the test from top to bottom. Initially, each agent
is executed N= 5times. This yields 5 scores for each agent: these 10 initial scores are shown on the top-
leftmost part of Fig. 2 labelled Interim 1, N = 5 , along with their barplots. After the test statistics are
computed, AdaStop decides that this information is not enough to conclude, and more scores are needed.
This set of actions (collection of 5 scores for each agent, computation of the statistics, and decision) is known
as aninterim. As no conclusion can be drawn, a second interim is performed: both agents are run 5 more
times, yielding 10 more scores. In the middle of Fig. 2, these additional scores are combined with the first 5
ones of each agent: these 20 scores are represented, as well as the barplot for each agent. The test statistics
are computed using these 20 scores. Again, the difference is not big enough to make a decision given the
available information, so we make a third interim: each agent is executed 5 more times. At the bottom
of Figure 2, these 30 scores are represented, as well as the barplot for each agent. The test statistics are
4Published in Transactions on Machine Learning Research (08/2024)
450 455 460 465 470Interim 3, N= 15Interim 2, N= 10Interim 1, N= 5
 Decision
  reject  continuecontinue
Figure 2: Illustration of the sequential and adaptive aspects of AdaStop .
computed again on these 15 scores per agent. Now, AdaStop decides to reject the null hypothesis, which
means that the two set of scores of the two agents are indeed different, and terminates: the compared agents
do not perform similarly. As explained in Section 2.5, this shows that the green agent performs most likely
better than the blue one.
2.3 Current approaches for RL agents comparison
In the RL community, different approaches currently exist to compare agents. In (Colas et al., 2018; 2019),
the authors show how to use hypothesis testing to test the equality between agents. Compared to our
work, their approach is non-adaptive and only compares two agents. In (Patterson et al., 2023), the authors
explore a similar workflow with added steps specialized to deep RL (hyperparameter optimization, choice
of the testing environment...). Another line of works can be found in (Agarwal et al., 2021) in which the
authors compare many agents using confidence intervals.
In this section, we summarize some of the problems we identify with the current approaches used to compare
two or more RL agents in research articles.
How many scores should we use? The number of scores used in practice in RL is quite arbitrary and
often quite small (see Figure 1a). An intuition comes from the law of large numbers. As the performance of
an agent is represented by the true mean of its scores, the more scores, the more precise the estimation of
its performance. However, this does not tell us anything about what is a sufficient number of scores to draw
a statistically significant conclusion.
Theoretically sound comparison of multiple agents.
According to statistical theory, in order to compare more than 2 agents, we need more samples from each
agent than when we compare only two agents. The basic idea is that there is a higher chance to make an
error when we perform multiple comparisons than when we compare only two agents, hence we need more
data to have a lower probability of error at each comparison. This informal argument is formalized in the
theory of multiple testing. However, the theory of multiple testing has almost never been used to compare
5Published in Transactions on Machine Learning Research (08/2024)
RL agents (with the notable exception of Patterson et al. (2023, Section 4.5)). In this paper, we remedy this
withAdaStop giving a theoretically sound workflow to compare 2 or more agents.
Theoretically sound study when comparing agents on a set of tasks.
Atari environments (Bellemare et al., 2013) are famous benchmarks in Deep RL. Due to time constraints,
when using these environments, it is customary to use very few scores for one given game (typically 3 scores)
and compare the agents on many different games. The comparisons are then aggregated: agent A1perform
better than agent A2on more than 20games out of the 26games consider in the experiments. In terms
of rigorous statistics, this kind of aggregation is complex to analyse properly because reward distributions
are not the same in all games. A2may be better than A1only on some easy games: does this mean that
A1is better than A2? Up to our knowledge, there is not any proper statistical guarantee for this kind of
comparison.
Advances have been made in (Agarwal et al., 2021) to interpret and visualize the results of RL agents in
Atari environments. In particular the authors advise plotting confidence intervals and using the interquartile
mean instead of the mean as aggregation functions. Correctly aggregating the comparisons on several games
in Atari is still an open problem , and it is beyond the scope of this article . In this article, we suppose that we
compare the agents on a single task, and we leave the comparison on a set of different tasks for future work.
A discussion on these methods and the challenges of aggregating the results from several Atari environments
can be found in the Appendix G.
2.4 Some methodologies for comparison of RL agents
Figure 1 of (Patterson et al., 2023) defines a workflow for the meaningful comparison of two RL agents given
anenvironmentandaperformancemeasure. Inadditiontotheusualconsiderationsregardingwhichstatistics
to compare (Colas et al., 2018; Agarwal et al., 2021), (Patterson et al., 2023) also include hyperparameter
choice in the workflow. For that, they recommend to use 3 scores per algorithm per set of hyperparameters
to identify a good choice of hyperparameters for a given algorithm. When this is done, a fixed number of
scores (set using expert knowledge on the environment) are computed for each of these fully-specified agents.
These scores are then used for statistical comparison. AdaStop fits at the end of this workflow.
In Fig 3, we showcase the use of AdaStop to compare SAC (Haarnoja et al., 2018) to other Deep RL
algorithms on HalfCheetah and Hopper Mujoco tasks. One can imagine a scenario in which SAC inventors
follow (Patterson et al., 2023) methodology. After finding the best hyperparameters for TRPO, PPO and
DDPG (Schulman et al., 2015; 2017; Lillicrap et al., 2015) agents are compared with AdaStop using a
minimal number of scores to get significant statistics. (Patterson et al., 2023) recommends 15 scores per
agent per environment for a maze environment but this number of scores should vary in other environments,
and it is not clear how many scores should be used for HalfCheetah and Hopper. This is why we need to
useAdaStop .
Using AdaStop , we collect scores in an adaptive manner. This allows us to state that we collected enough
scores to conclude that the SAC agent is MLB than other agents on HalfCheetah, and MLB than DDPG
and TRPO agents on Hopper. As in all statistical tests, the conclusion holds up to a certain confidence level.
A more in-depth study of the agents performance on Mujoco environments is given in Section 5.3.
2.5 Limitations of AdaStop and comparison to other statistical tests
Before presenting the theory behind AdaStop , we want to make it clear that doing a statistically sound
comparison with very few samples is a hard problem, and AdaStop is one way to partially answer this
problem. In particular, (i) we do not claim that AdaStop is optimal, and (ii) more work is still neces-
sary to prove theoretical guarantees for AdaStop to support the empirical performances exhibited in the
experiments (see in particular Section 5.2 and discussions on the power of the test).
The main theoretical limitation of AdaStop comes from the fact that AdaStop is distribution-free: it does
not make any assumption on the distribution of the scores except a finite variance to get the asymptotic
guarantees. As a consequence, the only non-asymptotic guarantees that we have are based on the comparison
6Published in Transactions on Machine Learning Research (08/2024)
Nscore 5 5 5 5
≥≥≥  
PPO TRPO DDPG SAC
500010000
(a) Comparisons on HalfCheetah.
Nscore 10 10 30 30
≥≥=  
DDPG TRPO PPO SAC
020004000
 (b) Comparisons on Hopper.
Figure 3: Example of the use of AdaStop to benchmark SAC in practice. We set the maximum number
of runsBof each agent to 30. The upper row tables represent the conclusions when comparing SAC to the
agents in the column using Nscorescores. For example, on HalfCheetah, AdaStop concludes that SAC is
MLB than PPO using 5 scores for SAC and 5 scores for PPO. On Hopper, 10 scores are enough to conclude
that SAC is MLB than DDPG and TRPO, and AdaStop concludes that SAC and PPO perform equally
using the maximum budget of Bscores for both SAC and PPO.
of score distributions and not on the comparison of their means. Formally, this means that we have control
over the error (in Theorem 1, the error is shown to be equal to the parameter of the test α) when doing the
tests:
Hj:Pl1=Pl2,againstH′
j:Pl1̸=Pl2
wherePliisthedistributionofscoresforagent li, and (l1,l2)aretheindicesoftheagentswewanttocompare.
ThiscomesincontrastwithtraditionalGaussiantestslikethet-test, whichhavestrongtheoreticalguarantees
under strong Gaussian assumptions (see Table 1 for a comparison of the guarantees of some classical tests,
in particular for nonparametric tests, a finite variance is not sufficient to get strong non-asymptotic results).
Comparing distributions is not what we really want to do, but it is how to proceed when performing
distribution-free tests and if the distributions concentrate sufficiently well ( i.e., have a finite variance) and
the maximum sample size N×Kis not too low. The Gaussian approximation of the sample mean justifies
the way we use AdaStop in this article. In particular, if we suppose a finite variance (a hypothesis that
corresponds to a very weak concentration hypothesis necessary to get a central limit theorem), we give
asymptotic guarantees on the comparison of the means (see Theorem 2):
Hj:EPl1[X] =EPl2[X],againstH′
j:EPl1[X]̸=EPl2[X]. (1)
Asymptotic guarantees remain unsatisfactory because we are targeting small sample sizes. Non-asymptotic
guarantees are harder to obtain and would typically require concentration assumptions such as sub-Gaussian
or bounded distributions; we leave such theoretical concerns for future works. In practice, this means that
AdaStop approximately compares the means for a large enough sample size, and the practitioner should
be aware that if the size of an interim Nis too small, AdaStop could give wrong results . For this
reason, we advise the practitioner to set NandKsuch thatN×K≥30. For a more precise study of the
sample size effect, see Section 5.2. Please note that AdaStop being an adaptive test, this does not mean
that at least 30 scores have to be collected for each agent. This subtle point is illustrated in the experimental
section of this paper.
Finally, note that the test expressed by Equation (1) is bidirectional, which means that in theory, concluding
on this test does not tell us which of Pl1orPl2has the largest mean score. Rather, it tells us that they
are different. In practice, we use the sign of the difference in the empirical means to conclude which mean
is larger. Using a bidirectional test and concluding on the direction afterward is often done by statisticians
(see, in particular, the discussion in (Leventhal & Huynh, 1996) and the references therein), but this remains
conceptually unsatisfactory. We keep the question of directional error for later work, which is why we do not
7Published in Transactions on Machine Learning Research (08/2024)
Test Hypothesis SequentialTheoretical Guarantees
testP=QvsP̸=Q testµP=µQvsµP̸=µQ
Asymptotic Non-Asymptotic Asymptotic Non-Asymptotic
t-test Gaussian ✗ ✓ ✓ ✓ ✓
Wilcoxon None ✗ ✓ ✓ ✗ ✗
Permutation Finite variance ✗ ✓ ✓ ✓ ✗
Bootstrap Finite variance ✗ ✓ ✗ ✓ ✗
Gaussian GST Gaussian ✓ ✓ ✓ ✓ ✓
AdaStop Finite variance ✓ ✓ ✓ ✓ ✗
Table 1: Comparison of the properties of several statistical methods. The Wilcoxon, Permutation and
Bootstrap tests are nonparametric tests used in (Colas et al., 2019) for comparison in Deep RL and the
Gaussian Group-Sequential test (Jennison & Turnbull, 1999) is a (parametric) sequential test often used in
the context of clinical trials.
write that an agent performs “better” than another in this article but rather “most likely better (MLB)”.
See Section B.2 for a discussion on directional error.
3 Hypothesis testing to compare agent performance
Inthissection, weprovidethebackgroundmaterialonthestatisticalteststhatweusetoconstruct AdaStop .
First we review the statistical evaluation methods found in the literature, and then we describe our method-
ology, and we express some results on AdaStop .
3.1 Literature overview of evaluation methods
In this section, we present some relevant references connected to statistical evaluation methodology.
Nonparametric (and non-sequential) hypothesis testing. One of our main challenges is to deal
with the nonparametric nature of the data at hand. In the literature there has been a lot of works on
nonparametric testing (see (Lehmann et al., 2005) for a comprehensive overview). Traditionally, the focus
has been on asymptotic results due to challenges in deriving optimality for a nonparametric model. For
most nonparametric tests, only rather weak (exact) theoretical results can be given, mostly on type I error
(Romano, 1989; Shapiro & Hubert, 1979). Recent work on sequential nonparametric tests (Shin et al., 2021;
Howard et al., 2021) show strong non-asymptotic results using concentration inequalities. However, these
results often involve non-optimal constants, failing to explain non-asymptotic efficiency. In contrast, we
use permutation tests due to their empirical (Ludbrook & Dudley, 1998) and theoretical (Kim et al., 2022)
efficiency for small sample sizes.
Sequential tests. A closely related method for adaptive hypothesis testing consists in sequential tests. Two
commonly used sequential tests are the Sequential Probability Ratio test (Wald, 1945) and the Generalized
Likelihood Ratio test (Kaufmann & Koolen, 2021). In sequential testing, the scores are compared one after
the other in a completely online manner. This is not adapted to our situation because in RL practice, one
often trains several agents in parallel, obtaining a batch of scores at once. This motivates the use of group
sequential tests (Jennison & Turnbull, 1999).
Parametric group sequential tests . In traditional hypothesis testing, data is analysed as a whole once
it has been collected. Conversely, in a Group Sequential Test (GST) data are collected sequentially, the
tests being performed at interimtime points. At each interim, a new set of Nscores is collected (see
(Jennison & Turnbull, 1999; Gordon Lan & DeMets, 1983; Pocock, 1977; Pampallona & Tsiatis, 1994) for
references on GST). GST are often used in clinical trials to minimize the amount of data needed to conclude
and this makes them well adapted for our purpose. The decision to continue sampling or conclude (with
a controlled probability of error) depends on pre-defined stopping criteria used to define the tests. GST
often makes strong assumptions on the data. In particular, it is often assumed that the data are i.i.d.and
drawn from a Gaussian distribution (Jennison & Turnbull, 1999). This contrasts with our approach which
is nonparametric.
8Published in Transactions on Machine Learning Research (08/2024)
Bandits (Best arm identification or ranking). Our objective is close to the one of bandit algo-
rithms (Lattimore & Szepesvári, 2020): we minimize the stopping time (as in the fixed-confidence setting)
of the test, and we have a fixed maximum budget (as in a fixed-budget setting). In our test, we allow a type
I error with probability α∈(0,1), which is similar to the fixed confidence setting while still having a fixed
budget. In practice, our approach is more sample efficient than fixed budget bandit algorithms because these
algorithms will always exhaust their budget and thereby achieve lower error rates. In contrast, AdaStop
allows larger error rate in exchange to higher sample efficiency.
3.2 Background material on the building-blocks of AdaStop
This section describes the basic building blocks used to construct AdaStop : group sequential testing,
permutation tests, and step-down method for multiple hypothesis testing. We explain these items separately,
and then we combine them to create AdaStop in Section 4. We also provide a small recap on hypotheses
testing in the Appendix B for readers unfamiliar with these notions.
In order to perform the minimal number of runs, we propose to use group sequential testing (GST) with a
nonparametric approach using permutation tests. Our approach is similar to (Mehta et al., 1994) but for
multiple hypothesis testing. Compared to GST, we keep the i.i.d. assumption, but we do not assume that
the data are drawn from a specific family of parametric distribution. In (Mehta et al., 1994), the authors use
rank tests with group-sequential testing. In contrast with our work, (Mehta et al., 1994) does not provide
theoretical guarantees and considers only the case of 2 agents.
3.2.1 Permutation tests
Permutation tests are nonparametric tests that are exact for testing the equality of distributions. This
means that the type I error of the test ( i.e.the probability to make a mistake and reject the equality of two
agents when their scores are statistically the same) is controlled by the parameter of the test α, and that this
is true for any fixed sample size N. Permutation tests are also well-known to work well in practice on very
small sample sizes and are used extensively in biology. They were originally introduced by (Pitman, 1937)
and (Fisher, 1936) (see (Lehmann et al., 2005, Chapter 17) for a textbook introduction). More recently
(Chung & Romano, 2013) have studied asymptotic properties of this class of tests, while (Romano & Wolf,
2003) have focused on stepdown methods for multiple hypothesis testing.
Let us recall the basic formulation of a two-sample permutation test. Let X1,...,XNbe i.i.d. sampled from
a lawPandY1,...,YNi.i.d. sampled from a law Q. We want to test P=QagainstP̸=Q. LetZi=Xi
ifi≤NandZi=Yi−Nifi > N,Z1,...,Z 2N. The test proceeds as follows: we reject P=QifT(id) =/vextendsingle/vextendsingle/vextendsingle1
N/summationtextN
i=1(Zi−ZN+i)/vextendsingle/vextendsingle/vextendsingleis larger than a proportion (1−α)of the values T(σ) =/vextendsingle/vextendsingle/vextendsingle1
N/summationtextN
i=1(Zσ(i)−Zσ(N+i))/vextendsingle/vextendsingle/vextendsingle
whereσenumerates all possible permutations of {1,..., 2N}andidis the identity permutation ( id(i) =i
for alli). Formally, we define the (1−α)-quantile as
BN= inf/braceleftigg
b>0 :1
N!/summationdisplay
σ∈SN1{T(σ)≥b}≤α/bracerightigg
and we reject P=QwhenT(id)≥BN. The idea is that if P̸=Q, thenT(id)should be large, and due
to compensations, most T(σ)should be smaller than T(id). Conversely, if P=Q, the difference of mean
T(σ)will be closer to zero. It is then sufficient to compute T(id)andBNin order to compute the decision
of the test. Please note that this is a fairly usual simplification in the nonparametric tests literature to test
the equality in distribution instead of the equality of the mean, because equality between distributions is
easier to deal with1. It can be shown that permutation tests are nonetheless a good approximation of doing
a comparison on the means (see Appendix E).
1In a statistical test, we want to have control on the error when H0is true.H0can be seen as asserting that the distribution
that generated the data is in a certain set of distributions P0. The larger P0, the more complicated it is to make a statistical test
without using strong assumption on the data. When comparing distributions, P0={P,Qprobability distributions |P=Q},
while when comparing the means, P0is much larger because P0={P,Qprobability distributions |EP[X] =EQ[X]}.
9Published in Transactions on Machine Learning Research (08/2024)
3.2.2 GST comparison of two agents
In this section, we compare two agents A1andA2through a group sequential test that can be seen as a
particular case of AdaStop for two agents (see Section 4). In this simplified case, the testing procedure is
presented in Algorithm 1. We leave the case with more than 2 agents to compare and the full version of
AdaStop , including multiple hypothesis testing, for Section 4. Algorithm 1 uses a permutation test where,
at each interim, the boundary deciding the rejection is derived from the permutation distribution of the
difference of empirical means observed across all previously obtained data. In what follows, Nto denote the
number of scores collected at interim k.
We denote by S2Nthe set of permutations of {1,..., 2N},σ∈S2None permutation and σ(n)then-th
element ofσforn∈{1,...2N}. In the GST setting, we perform a permutation test at each interim k, and
σk∈S2Ndenotes the permutation at interim k. Forσ1,σ2,...,σk∈S2N, we denote σ1:k=σ1·σ2·...·σk
the concatenation2of the permutation σ1done in interim 1withσ2done on interim 2,..., andσkon interim
k. Then,eσi(n),idenotes the score corresponding to the n-th element of the permuted sample at interim i,
permuted by σi(in the notations of Section 3.2.1 this corresponds to Zσ(i)but now, we specify the interim
number in the notation). We denote:
TN,k(σ1:k) =/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglek/summationdisplay
i=1/parenleftiggN/summationdisplay
n=1eσi(n),i−2N/summationdisplay
n=N+1eσi(n),i/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle, (2)
and the decision boundary:
BN,k∈inf

b>0 :1
((2N)!)k/summationdisplay
σ1:k∈/hatwideSN,k1{TN,k(σ1:k)≥b}≤α
K

, (3)
whereKis the total number of interims and /hatwideSN,kis the set of permutations σ1:k∈(S2N)ksuch that the
test was not rejected before interim k,e.g.
/hatwideSN,k={σ1:k∈(S2N)k:∀m<k, T N,m(σ1:m)≤BN,m}. (4)
One can show that this is equivalent to choosing BN,ksuch that
Pσ1:k(TN,m(σ1:m)>BN,k,∀m<k, T N,m(σ1:m)≤BN,m)≤α
K.
We will see in Theorem 1 that this allows us to have control on the type I error of the test.
3.2.3 Multiple hypothesis testing
Inordertocomparemorethantwoagentsweneedtoperformmultiplecomparisons. Thiscallsfora multiple
simultaneous statistical tests (Lehmann et al., 2005, Chapter 9). The idea is that the probability to
mistakenly reject a null hypothesis (type I error) generally applies only to each test considered individually.
On the other hand, in order to conclude on all the tests at once, it is desirable to have an error controlled
over the whole family of simultaneous tests. For this purpose, we use the family-wise error rate (Tukey,
1953) which is defined as the probability of making at least one type I error.
Definition 1 (Family-Wise Error (Tukey, 1953)) .Given a set of hypothesis Hjforj∈ {1,...,J}, its
alternative H′
j, andI⊂{1,...,J}the set of the true hypotheses among them, then the family-wise error
(FWE) is defined by:
FWE = PHj,j∈I(∃j∈I:rejectHj),
where PHj,j∈Idenotes the probability distribution for which all hypotheses j∈Ihold true3. We say that an
algorithm has a weak FWE control at a joint level α∈(0,1)if the FWE is smaller than αwhen all the
hypotheses are true, that is I={1,...,J}but not necessarily otherwise. We say it has strong FWE control
if FWE is smaller than αfor any non-empty set of true hypotheses I̸=∅.
2Here, the word “concatenation” means that we apply σ1on{1,...,N }, thenσ2on{N+ 1,..., 2N}and so on, so that
σ1:k(Nj+k) =σj(k).
3See also Appendix B for further explanations on this concept.
10Published in Transactions on Machine Learning Research (08/2024)
Algorithm 1: Adaptive stopping to compare two RL agents. This algorithm is expressed in the context
of the comparison of RL agents. It is easy to adapt to other types of computational agents.
Parameters: AgentsA1,A2, environmentE, number of interims K∈N∗, size of an interim N, error
parameterα∈(0,1).
1fork= 1,...,Kdo
2forl= 1,2do
3 Train agent Alon environmentEN times, with the seeds sl,(k−1)N+1,...,sl,kN. This generates
Npoliciesπl,(k−1)N+1,...,πl,kN
4 Collect scores e1,k(Al),...,eN,k(Al)by running each policy πl,(k−1)N+1,...,πl,kN.
5end
6Compute the boundary BN,kusing Equation (3).
7ifTN,k(id)≥BN,kthen
8 return reject
9end
10return accept
If we want to test the equality of Ldistributions P1,P2,...,PL, the straightforward way is to do a pairwise
comparison. This creates J=L(L−1)
2hypotheses. We let C={c1,...,cJ}be the set of all possible
comparisons between the distributions, where cj= (l1,l2)∈Cdenotes a comparison between distributions
Pl1andPl2forl1,l2∈{1,2,...,L}. Therefore, for cj= (l1,l2),Hcjdenotes a hypothesis stating that Pl1
andPl2are equal and its alternative H′
cjis thatPl1andPl2are different. On the other hand, if one wants
to compare an agent to agents whose ranking does not interest us, it may be sufficient to only compare this
agent to all the others yielding the L−1comparisons c1= (1,l2)for2≤l2≤L.
There are several procedures that can be used to control the FWE. The most famous one is Bonferroni’s
procedure (Bonferroni, 1936) recalled in the Appendix (Section B). As Bonferroni’s procedure can be very
conservative in general, we prefer a step-down method (Romano & Wolf, 2003) that performs better in
practice because it implicitly estimates the dependence structure of the test statistics. The step-down
method is detailed in the next section.
3.2.4 Step-down procedure
(Romano & Wolf, 2003) proposed the step-down procedure to solve a multiple hypothesis testing problem.
It is defined as follows (for a non group-sequential test ): for a permutation σ∈S2Nand for (en(j))1≤n≤2N
the random variables being compared in hypothesis j, the permuted test statistic of hypothesis jis defined
by:
T(j)
N(σ) =/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
n=1eσ(n)(j)−2N/summationdisplay
n=N+1eσ(n)(j)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle. (5)
This test statistic is extended to any subset of hypothesis C⊂{1,...,J}with the following formula:
T(C)
N(σ) = max
j∈CT(j)
N(σ). (6)
To specify the test, one compares T(C)
N(id)to some threshold value B(C)
N, that is: we accept all hypotheses
inCsuch thatT(C)
N(id)≤B(C)
N. The threshold of the test B(C)
Nis defined as the quantile of order 1−αof
the permutation law of T(C)
N(σ):
B(C)
N= inf/braceleftigg
b>0 :/parenleftigg
1
(2N)!/summationdisplay
σ∈S2N1{T(C)
N(σ)≥b}/parenrightigg
≤α/bracerightigg
. (7)
In other words, B(C)
Nis the real number such that an αproportion of the values of T(C)
N(σ)exceeds it, when
σenumerates all the permutations of {1,..., 2N}. The permutation test is summarized in Algorithm 2.
11Published in Transactions on Machine Learning Research (08/2024)
Algorithm 2: Multiple testing by step-down permutation test.
Parameters: α∈(0,1)
Input:en(j)for1≤n≤2Nandj∈C0={c1,...,cJ}.
1InitializeC←C0.
2while C̸=∅do
3ComputeT(C)
N(σ)for everyjand everyσusing Equation (6).
4ComputeB(C)
nusing Equation (7).
5ifT(C)
N(id)≤B(C)
Nthen
6 Accept all the hypotheses Hj,j∈Cand exit the loop.
7else
8 RejectHcjmaxwherecjmax= arg max
cj∈CT(j)
N(id).
9 DefineC=C\{cjmax}
10end
11end
Algorithm 2 is initialized with C=C0containing all the comparisons we want to test. Then, it enters a
loop where the test decides to reject or not the most extreme hypothesis in C,i.e.Hjmaxwherejmax=
arg maxj∈CT(j)
N(id). Here,Cis the current set of neither yet rejected nor accepted hypotheses. If the test
statisticT(j)
N(id)for the most extreme hypothesis in C(i.e.TC
N(id)) does not exceed the given threshold
B(C)
n, then all hypotheses in Care accepted, and the loop is exited. Otherwise, the most extreme hypothesis
is discarded from the set Cand another iteration is performed until either all remaining hypothesis are
accepted, or the set of remaining hypotheses is empty.
The maximum of the statistic in Equation (6) for σ= idallows us to test intersections of hypotheses, while
the threshold B(C)
n, under the null hypotheses of equality of distribution, allows for strong control on the
FWE (i.e.FWE≤α). This last result follows from (Romano & Wolf, 2003, Corollary 3) and is a particular
case of Theorem 1 for AdaStop . In fact, this procedure is not specific to permutation tests, and it can be
used for other tests provided some properties hold on the thresholds B(C)
n.
4 AdaStop: adaptive stopping for nonparametric group-sequential multiple tests
In this section, we present the construction and the theoretical properties of AdaStop (see Algorithm 3).
AdaStop compares the scores of multiple agents in an adaptive rather than a fixed way. We consider L≥2
agentsA1,...,AL. As above, we let C0={c1,...,cJ}⊆{ 1,...,L}2be the set of all the comparisons to
make between the agents. Idenotes the set of indices of the true hypotheses among {1,...,J}.
Algorithm 3 specifies AdaStop .AdaStop relies on the test statistic T(C)
N,k(σ1:k)defined in equation (9),
and the boundary thresholds BC
N,kdefined in Equation (10). We discuss a few implementation details in the
rest of this section.
Definition of the test statistic. Lete1,i(j),...,e 2N,i(j)denote the 2Nscores used at interim i. They are
obtained through the execution of the policies resulting from the training of the two agents Al1andAl2for
whichcj= (l1,l2). We also consider permutations of these scores to define the test statistics T(j)
N,kbelow.
For a comparison j, we consider a permutation σi∈S2Nat interimithat reshuffles the order of the scores
mappingn∈{1,..., 2N}onσi(n)∈{1,..., 2N}. Note that if n∈{1,...,N}andσi(n)∈{N+ 1,..., 2N},
we are exchanging a score of the first agent with a score of the second agent in the comparison. It can also
happen that instead, we permute two scores of the same agent. The difference between the two cases is
12Published in Transactions on Machine Learning Research (08/2024)
important for the definition of the following permutation statistic:
T(j)
N,k(σ1:k) =/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglek/summationdisplay
i=1/parenleftiggN/summationdisplay
n=1eσi(n),i(j)−2N/summationdisplay
n=N+1eσi(n),i(j)/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle. (8)
In other words, T(j)
N,k(σ1:k)is the absolute value of the sum of differences of all scores until interim kafter
consecutive permutations of the concatenation of the two agents scores by σ1,...,σk∈S2N. LetC⊆C0
be a subset of the set of considered hypothesis and let us denote:
T(C)
N,k(σ1:k) = max
j∈CT(j)
N,k(σ1:k), (9)
T(C)
N,k(σ1:k)is the test statistic used in AdaStop . The construction of the test is inspired by the permutation
tests of Equation (5) used to test intersection of hypotheses as in Equation (6) in the step-down method
presented in Section 3.2. Still, it also incorporates group sequential tests from Section 3.2.2 and its test
statistic introduced in Equation (2).
Choice of permutations. Instead of using all the permutations as we did for now, one may use a random
subset among all permutations SN,k⊂{σ1:k,∀i≤k,σi∈S2N}to speed-up computations. The theoretical
guarantees persist as long as the choice of the permutations is made independent on the data. Using a small
number of permutations will decrease the total power of the test, but with a sufficiently large number of
random permutations (typically for the values of NandKwe consider, 104permutations are sufficient) the
loss in power is acceptable. We need to include the identity in addition to the random permutations to keep
the type I error guarantee (Phipson & Smyth, 2010).
TN,kdoes not change when the permutation does not exchange any index from {1,...,N}with an index of
{N+1,..., 2N}. In essence, choosing a permutation is equivalent to choosing the signs in/summationtextN
n=1eσi(n),i(j)−/summationtext2N
n=N+1eσi(n),i(j). And because we take the absolute value, there are1
2/parenleftbig2N
N/parenrightbig
possible unique values to TN,1
(up to ties when the score distributions are discrete). Then, by enumerating all the permutations for the
other interims, there are1
2/parenleftbig2N
N/parenrightbigkpossible unique values to TN,k.
In practice, we use a parameter B∈Nand the number of permutations used at interim kwill be|SN,k|=
mk= min/parenleftig
B,1
2/parenleftbig2N
N/parenrightbigk/parenrightig
,i.e.whenever possible, we use all the permutations and if this is too much, we use
permutations drawn at random.
Definition of the boundaries. With these permutations, we define the boundary thresholds B(C)
N,kby:
B(C)
N,k= inf

b>0 :1
mk/summationdisplay
σ∈/hatwideSN,k1{T(C)
N,k(σ1:k)≥b}≤qk

. (10)
where/summationtextk
j=1qj≤kα
Kandwhere/hatwideSN,kisthesubsetofSN,ksuchthatthestatisticassociatedtothepermutation
would not have been rejected before. Formally, /hatwideSN,kis the following set of permutations:
/hatwideSN,k=/braceleftbigg
σ1:k:∀m<k,T(C)
N,m(σ1:m)≤B(C)
N,m/bracerightbigg
. (11)
Note thatq1is not equal to α/K. Due to discreetness (we use an empirical quantile over a finite number
of values),q1is chosen equal to ⌊α
2K/parenleftbig2N
N/parenrightbig
⌋/(1
2/parenleftbig2N
N/parenrightbig
). Similarly, q2is chosen to be as large as possible while
havingq1+q2≤2α/K. And so on and so forth: in practice this means taking the qi’s as follows:
qi=⌊αi
2K(2N
N)⌋/(1
2(2N
N))−qi−1for2≤i≤K,andq0= 0. (12)
One may notice that due to the discrete nature of the permutation distribution, the confidence of the test
performed by AdaStop is notαbut the sum of the qi’s which may be smaller than the prescribed α.
13Published in Transactions on Machine Learning Research (08/2024)
Algorithm 3: AdaStop (main algorithm) in the context of the comparison of 2 RL agents. Its appli-
cation to other types of computational agents is straightforward.
Parameters: AgentsA1,A2,...,AL, environmentE, comparison pairs (cj)j≤Jwhereciis a couple of
agents that we want to compare. Integers K,N∈N∗, test parameter α.
1SetC={1,...,J}the set of indices for the comparisons to perform.
2fork= 1,...,Kdo
3forl= 1...Ldo
4 TrainNtimes agent Alon environmentE. This generates Npolicies.
5 Collect the Nscores of the Npolicies.
6end
7whileTruedo
8 Compute the boundaries B(C)
N,kusing Equation (10).
9ifT(C)
N,k(id)>B(C)
N,kthen
10 RejectHjmaxwherejmax= arg max/parenleftig
T(j)
N,k(id), j∈C/parenrightig
.
11 C=C\{jmax}
12 else
13 Exit the while loop.
14 end
15end
16ifC=∅thenExit the loop and return the decision of the test.
17ifk=KthenExit the loop and accept all hypotheses remaining in C.
18end
4.1 Theoretical guarantees of AdaStop
One of the basic properties of two-sample permutation tests is that when the null hypothesis is true, then
all permutations are as likely to give a certain value and permuting the sample should not change the test
statistic too much. Following our choice of BN,kas a quantile of the law given the data, the algorithm has a
probability to wrongly reject the hypothesis bounded by α. This informal statement is made precise in the
following theorem.
Theorem 1 (Controlled family-wise error) .Letα∈(0,1), and consider the multiple testing problem Hj:
Pl1=Pl2againstH′
j:Pl1̸=Pl2for all the couples cj= (l1,l2)∈{c1,...,cJ}. Then, the test resulting from
Algorithm 3 has a strong control on the family-wise error for the multiple test, i.e. if we suppose that all the
hypotheses Hi,i∈Iare true and the others are false, then
P(∃j∈I: rejectHj)≤α.
The proof of Theorem 1 is given in the Appendix (Section C).
Hypotheses of the test : in Theorem 1 we show that Algorithm 3 tests the equality of the distributions
Pl1=Pl2versusPl1̸=Pl2, whereas in practice we would prefer to compare the means of the distribution
µl1=µl2versusµl1̸=µl2, see Section 2.5 for a discussion on the differences between the two. We show in
the Appendix E that for large N, the test comparing the means µl1=µl2versusµl1̸=µl2has the right
guarantees (FWE smaller than α). This shows that even though we test the distributions, we also have an
approximate test on the means. More precisely, we show the following in the Appendix E for the comparison
of the means of two distributions.
Theorem 2. Suppose that α∈(0,1), suppose that PandQboth have a finite variance, and consider the
two-sample testing problem H0:EP[X] =EQ[X]againstH′
0:EP[X]̸=EQ[X]. Then, the test resulting from
Algorithm 3 has an asymptotic type I error of α
lim
N→∞PH0(rejectH0) =α.
14Published in Transactions on Machine Learning Research (08/2024)
The proof of Theorem 2 is given in the Appendix (Section E).
Power of the test : in Theorem 1, there is no information on the power of the test. For any N,K, we show
that, the FWE is upper-bounded by α. In the appendix E, we establish Theorem 3 that can be used to get
a control on the asymptotic power. On the other hand, having non-asymptotic information on the power
would allow us to give a rule for the choice of NandK. However, non-asymptotic power analysis in a
nonparametric setting is in general hard, and it is beyond the scope of this article. Instead, we compute
empirically the power of our test and show that it performs well empirically compared to non-adaptive
approaches. See Section 5.2 for the empirical power study on a Mujoco environment.
5 Experimental study
This section demonstrates AdaStop from a practitioner perspective. First, we illustrate the statistical
properties of AdaStop on toy examples in which the scores of the agents are sampled from known distribu-
tions. Then, we compare empirically AdaStop to non-adaptive approaches. Finally, we exemplify the use
ofAdaStop on a real case to compare a set of Deep RL agents.
To reproduce the experiments of this paper, the python code is freely available on GitHub at https:
//github.com/TimotheeMathieu/Adaptive_stopping_MC_RL .
5.1 Toy examples
To start with, let us consider a toy example. In what follows, let us denote: (i) N(µ,σ2)the normal
distribution with mean µand standard deviation σ, (ii)MN
f(µ1,σ2
1;µ2,σ2
2)the mixture of 2 Gaussian
distributionsN(µ1,σ2
1)andN(µ2,σ2
2)with weights fand1−frespectively.
We compare two agents A1andA2for which we know the distributions of their scores. We consider three
cases in Fig. 4. In the 3 cases, the scores of agent A1are drawn fromN(0,0.01)whereas the scores of the
agentA2are drawn from a certain mixture detailed below. We use a hyperparameter ∆which is the distance
between the two modes of the mixture ( ∆ =|µ1−µ2|). Our goal is to compare the performance of agent A1
with the performance of agent A2and we study the outcome of AdaStop when ∆is increasing from 0 to
1. In the first case (upper-left part), the scores of A2are drawn fromMN
1
2(−∆/2,0.01; ∆/2,0.01): in this
situation, the means of both distributions are both equal to 0, but the distributions are different (except
when ∆ = 0). In the second case (upper-right part), the scores of A2are drawn fromMN
1
2(0,0.01; ∆,0.01):
in this situation, as ∆increases, the means of the two distributions get more and more apart making the
rejection of the null hypothesis easier and easier. In the third case (lower part), the scores of A2are drawn
fromMN
0.9(−0.1∆,0.01; 0.9∆,0.01): in this situation, the distributions are different but their means are both
0. In all three cases, we run AdaStop withK= 5,N= 5andα= 0.05. We also limit the maximum
number of permutations to B= 104. At the bottom of each subfigure of Figure 4, a color bar indicates the
rejection rate of the null hypothesis that the compared distributions are the same for ∆∈[0,1]. By varying
∆from 0to1, we observe the evolution of the power of the test, i.e.the probability of rejecting the null
hypothesis when it is indeed false. Figure 4a shows that the error of the test remains around 0.05for all ∆
(it is at most 0.1for the most extreme case). Indeed, even though the distributions are different, their means
remain the same. If the null hypothesis states that the means are the same, then AdaStop will return the
correct answer with type I error not larger than 0.095(see Figure 4a) for α= 0.05, which is larger than α
due to the fact that the test for comparison of the means is asymptotic. This is an illustration of the fact
that in addition to performing a test on the distributions, AdaStop approximates the test on the means as
shown theoretically in the asymptotic result in Appendix E and as discussed at the end of Section 4.1. In
contrast, Figure 4b demonstrates an increasing trend, reaching a confidence level close to 1when ∆>0.6,
which corresponds to the case where the two modes are separated by 3standard deviations from both sides.
Finally, in Figure 4c, we use a distribution made of an unbalanced mixture of 2 normal distributions. This
is meant to model an agent that does not perform optimally except in some rare cases ( e.g.the agent has
a score around 0 for 90% of the evaluation runs). As in case 1, the mean of the scores for both agents is 0
andAdaStop accepts the equality most of the time, with an error of 0.2in the most extreme case which
we think is acceptable given the difficulty of this setting.
15Published in Transactions on Machine Learning Research (08/2024)
To obtain an estimation of the error, we have executed each comparison M= 5·103times, and we plot
confidence intervals corresponding to 3σ/√
M(confidence larger than 99%) where σis the standard deviation
of the test decision. In addition to cases 1, 2 and 3, we also provide a fourth experiment with a comparison
of10agents in Appendix H.1.
Laws :A1∼N (0,0.01),A2∼MN
1/2(−∆/2,0.01; ∆/2,0.01)
−0.6−0.4−0.2 0.0 0.2 0.4 0.6∆ = 0.0∆ = 0.2∆ = 0.4∆ = 0.6
01
92
93
94
95
96
97
98
91
∆0.029
(±0.007)0.034
(±0.008)0.034
(±0.008)0.038
(±0.008)0.036
(±0.008)0.040
(±0.008)0.045
(±0.009)0.046
(±0.009)0.050
(±0.009)0.048
(±0.009)Rejection Frequencies
(a) Case 1
Laws :A1∼N (0,0.01),A2∼MN
1/2(0,0.01; ∆,0.01)
−0.2 0.0 0.2 0.4 0.6 0.8 1.0∆ = 0.0∆ = 0.2∆ = 0.4∆ = 0.6
01
92
93
94
95
96
97
98
91
∆0.029
(±0.007)0.195
(±0.017)0.572
(±0.021)0.761
(±0.018)0.850
(±0.015)0.892
(±0.013)0.916
(±0.012)0.923
(±0.011)0.934
(±0.011)0.937
(±0.010)Rejection Frequencies (b) Case 2
Laws :A1∼N (0,0.01),A2∼MN
0.9(−0.1∆,0.01; 0.9∆,0.01)
−0.2 0.0 0.2 0.4 0.6 0.8∆ = 0.0∆ = 0.2∆ = 0.4∆ = 0.6
01
92
93
94
95
96
97
98
91
∆0.029
(±0.007)0.030
(±0.007)0.032
(±0.007)0.038
(±0.008)0.056
(±0.010)0.072
(±0.011)0.097
(±0.013)0.127
(±0.014)0.162
(±0.016)0.192
(±0.017)Rejection Frequencies
(c) Case 3
Figure 4: Toy examples. For each of these 3 cases, we plot the distribution of scores of each of the 2 agents
that are compared, as well as the frequency of rejection of the null hypothesis. Please, refer to the text for
details.
5.2 Comparison with non-adaptive approach
(Colas et al., 2019) share the same objective with us. However, they use non-adaptive tests unlike AdaStop .
We follow their experimental protocol and compare AdaStop and non-adaptive approaches empirically in
terms of statistical power as a function of the sample size (number of scores). In particular, we use the data4
they provide for a SAC agent and for a TD3 agent evaluated on HalfCheetah (see Fig. 12 in the Appendix).
Similarly to (Colas et al., 2019, Table 15), we compute the empirical statistical power of AdaStop as a
function of the number of scores of the RL agents (Table 2). To compute the empirical statistical power for
a given number of scores, we make the hypothesis that the distribution of SAC and TD3 agents scores are
different, and we count how many times AdaStop decides that one agent is MLB than the other (number
of true positives). As the test is adaptive, we also report the effective number of scores that are necessary
4available at https://github.com/flowersteam/rl_stats/tree/master/data .
16Published in Transactions on Machine Learning Research (08/2024)
N\K2 3 4 5 6
10.0 (2.0) 0.0 (3.0) 0.277 (4.0) 0.465 (5.0) 0.56 (6.0)
20.005 (4.0) 0.33 (6.0) 0.531 (6.96) 0.602 (8.345) 0.704 (9.198)
30.213 (5.984) 0.506 (8.085) 0.627 (10.212) 0.689 (11.02) 0.785 (11.52)
40.371 (7.616) 0.611 (9.648) 0.744 (11.7) 0.82 (12.08) 0.845 (13.89)
50.465 (9.044) 0.691 (11.031) 0.78 (13.28) 0.853 (14.27) 0.884 (14.532)
60.534 (10.4) 0.73 (12.306) 0.837 (14.124) 0.89 (14.94) 0.911 (15.978)
70.599 (11.358) 0.779 (13.404) 0.879 (14.916) 0.92 (15.495) 0.939 (16.404)
80.635 (12.322) 0.818 (13.95) 0.885 (15.824) 0.942 (16.03) 0.961 (17.268)
Table2: Averageempiricalstatisticalpowerand, inparentheses, effectivenumberofscoresusedby AdaStop
as a function of the total number of scores ( N×K) when comparing SAC and TD3 agents on Mujoco
HalfCheetah task. The number of permutations Bis set to 104andαis set to 0.05.AdaStop is run 103
times for each (N,K )pair. The shades of blue are proportional to the power, a value in [0,1](we use the
same color scheme as in (Colas et al., 2018)). It can seem strange that the power is 0 in some cases. This
situation is explained in the text.
to make a decision with 0.95confidence level. For each number of scores, we have run AdaStop 103times.
For example, when comparing the scores of SAC and TD3 on HalfCheetah using AdaStop withN= 4and
K= 5, the maximum number of scores that is used is N×K= 20. However, we observe in Table 2 that when
N= 4andK= 5,AdaStop can make a decision with a power of 0.82using only 12 scores. In (Colas et al.,
2019, Table 15), the minimum number of scores required to obtain a statistical power of 0.8when comparing
SAC and TD3 agents is 15 when using either a t-test, or a Welch test, or a bootstrapping test. With this
example, we first show that being an adaptive test, AdaStop may save computations. We also show that
as long as the scores of agents are made available, AdaStop can use them to provide a statistically sound
conclusion, and as such, AdaStop may be used to assess the initial conclusions, hopefully strengthening
them with a statistically significant argument.
Remark: as already mentioned above, when NandKare both small, it can happen that AdaStop does not
allow an error up to αbut it will restrict itself to a smaller error and this will lead to a much lower power. For
instance, if N= 2,K= 2,α= 0.05for the first interim, we have ⌊1
2/parenleftbig2N
N/parenrightbig
α/2⌋= 0hence the first boundary
is necessarily infinity and AdaStop never rejects, e.g.with the notations of Equation (12), q1= 0. Then
at the second interim ⌊1
2(/parenleftbig2N
N/parenrightbig
)2α/2⌋= 3and we use q1+q2=⌊1
2(/parenleftbig2N
N/parenrightbig
)2α/2⌋/(1
2(/parenleftbig2N
N/parenrightbig
)2)≃0.041which
is smaller than the 5%that we allow for the test, hence the test is more conservative than needed. This is
an extreme case happening in the first few interims when Nis small explaining some values in Table 2 that
may seem strange at first glance (for instance the power is equal to 0whenN= 1andK≤3).
5.3 AdaStop for Deep Reinforcement Learning
In this section, we use AdaStop to compare four commonly-used Deep RL agents on the MuJoCo5(Todorov
et al., 2012) benchmark for high-dimensional continuous control. We use the Gymnasium6implementation.
More specifically, we train agents on the Ant-v3, HalfCheetah-v3, Hopper-v3, Humanoid-v3, and Walker-v3
environments using PPO from rlberry (Domingues et al., 2021), SAC from Stable-Baselines3 (Raffin et al.,
2021), DDPG from CleanRL (Huang et al., 2022), and TRPO from MushroomRL (D’Eramo et al., 2021).
PPO, SAC, DDPG, and TRPO are Deep RL algorithms used for continuous control tasks. We choose these
algorithms because they are commonly used and they represent a diverse set of approaches from different
RL libraries. We use different RL libraries in order to demonstrate the flexibility of AdaStop , as well as to
provide examples on how to use these popular libraries with AdaStop .
On-policy algorithms, such as PPO and TRPO, update their policies based on the current data they collect
during training, while off-policy algorithms, such as SAC and DDPG, can learn from any data, regardless
of how it was collected. This difference may make off-policy algorithms more sample-efficient but less stable
5We use MuJoCo version 2.1, as required by https://github.com/openai/mujoco-py.
6https://github.com/Farama-Foundation/Gymnasium.
17Published in Transactions on Machine Learning Research (08/2024)
DDPG TRPO PPO SACDDPG
TRPO
PPO
SAC →≤↓→≤↓→≤↓
→≥↓  →≤↓→≤↓
→≥↓→≥↓  →≤↓
→≥↓→≥↓→≥↓  Ant-v3
DDPG TRPO PPO SAC →≥↓→≥↓→≤↓
→≤↓  →≥↓→≤↓
→≤↓→≤↓  →≤↓
→≥↓→≥↓→≥↓  HalfCheetah-v3
DDPG TRPO PPO SAC →≤↓→≤↓→≤↓
→≥↓  →≤↓→≤↓
→≥↓→≥↓  →=↓
→≥↓→≥↓→=↓  Hopper-v3
DDPG TRPO PPO SACDDPG
TRPO
PPO
SAC →≤↓→≤↓→≤↓
→≥↓  →≤↓→≤↓
→≥↓→≥↓  →≤↓
→≥↓→≥↓→≥↓  Humanoid-v3
DDPG TRPO PPO SAC →≤↓→≤↓→≤↓
→≥↓  →≤↓→≤↓
→≥↓→≥↓  →≤↓
→≥↓→≥↓→≥↓  Walker2d-v3
Ant
HalfCheetahHopper
HumanoidWalker2dDDPG
TRPO
PPO
SAC15 25 30 15 25
15 25 30 15 25
15 25 30 15 10
15 5 30 15 10Used Budget
Figure 5: AdaStop decision tables for each MuJoCo environment, and the budget used to make these
decisions (bottom right). The notation →≥↓means that the agent which name is on the left of the row is
MLB than the agent which name is at the bottom of the column. See Appendix H.2 for further details.
than on-policy algorithms. Furthermore, SAC typically outperforms DDPG in continuous control robotics
tasks due to its ability to handle stochastic policies, while DDPG restricts itself to deterministic policies
(Haarnoja et al., 2018). Finally, PPO is generally considered performing better than TRPO in terms of
cumulative reward (Engstrom et al., 2020).
For each algorithm, we fix the hyperparameters to those used by the library authors in their benchmarks
for one of the MuJoCo environments. Appendix H.2 lists the values that were used and we further discuss
the experimental setup. We compare the four agents in each environment using AdaStop withN= 5and
K= 6. Fig. 5 shows the AdaStop decision tables for each environment, as well as the number of scores
per agent and environment. As expected, SAC is MLB than all the other agents in each environment. In
contrast, all agents are MLB than DDPG; this may be due to the restriction to deterministic policies which
hurts exploration in high-dimensional continuous control environments such as the MuJoCo benchmarks.
Furthermore, we observe that the expected ordering between PPO and TRPO is generally respected, with
TRPO MLB than PPO in only one environment. Finally, we note that PPO performs particularly well
in some environments obtaining scores that are comparable to those of SAC, while also being the worst-
performing algorithm on HalfCheetah-v3. Overall, the AdaStop rankings in these experiments are not
unexpected.
Moreover, our experiments demonstrate that AdaStop can make decisions with fewer scores, thus reducing
the computational cost of comparing Deep RL agents. For instance, as expected, SAC is MLB than all
the other agents on the environment HalfCheetah-v3, and AdaStop required only five scores to make all
decisions involving SAC. Additionally, we observed that the decisions requiring the entire budget of NK = 30
scores were the ones in which AdaStop determined that the agents were equivalent in terms of their scores.
This decision process can be sped-up by using the early accept heuristic which is presented in the Appendix
(Section F). For instance in the Walker2d-v3 environment, early accept allows us to take all the decisions
after only 10scores have been collected for each agent.
6 Conclusion and future works
In this paper, we introduce AdaStop which is a sequential group test aiming at ranking agents based on
their practical performance. AdaStop may be applied in various fields where the performance of algorithms
are random, such as machine learning, or optimization. Our goal is to provide statistical grounding to
18Published in Transactions on Machine Learning Research (08/2024)
define the number of times a set of agents should be run to be able to confidently rank them, up to some
confidence level α. This is the first such test, and we think this is an important contribution to computational
studies in reinforcement learning and other domains. From a statistical point of view, we have been able
to demonstrate the soundness of AdaStop as a statistical test. Using AdaStop is simple. We provide
open source software to use it. Experiments demonstrate how AdaStop may be used in practice, even in a
retrospective manner using logged data: this allows one to diagnose prior studies in a statistically significant
way, hopefully confirming their conclusions, possibly showing that the same conclusions could have been
obtained with fewer computations.
Currently, AdaStop considers a set of agents facing one single task. Our next step will be to extend the test
to experimental settings where a set of agents are compared on a collection of tasks, such as the set of Atari
games or the set of Mujoco tasks in reinforcement learning. Properly dealing with such experimental settings
requires a careful statistical analysis. Moreover, additional theoretical guarantees for the early accept and
the non-asymptotic control of the power, FWE, and directional error of AdaStop for the comparison of
means would also greatly improve the interpretability of the conclusions of AdaStop . We are currently
investigating these questions.
As this is illustrated in the experimental section, AdaStop can be run on already collected scores: we do not
need to run anew the agents as long as scores are available. This remark calls for an effort of the community
to make their scores publicly available so that it is easy for anyone to compare one’s new agent with others
already proposed.
Acknowledgments
The authors would like to thank the action editor and the reviewers for their many remarks that guided us to
make this paper clearer, more legible, and to improve the rigor of the exposition. O-A.Maillard and Ph.Preux
acknowledge the support of the Métropole Européenne de Lille (MEL), ANR, Inria, Université de Lille,
through the AI chair Apprenf number R-PILOTE-19-004-APPRENF. R.Della Vecchia acknowledges the
fundingreceivedbytheCHIST-ERAProjectCausaleXplainationsinReinforcementLearning–CausalXRL7.
A.Shilova acknowledges the funding from the HPC-BigData Inria Project Lab8. T.Mathieu acknowledges
the funding received by the SR4SG Inria exploratory action9. M.Centa Medeiros and H.Kohler acknowledge
the funding of their Ph.D.by an ANR AI_PhD@Lille grant. This research is also partially supported by the
CornelIA Hauts-de-France project. All the authors acknowledge the Scool research group for its outstanding
working environment.
References
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep
reinforcement learning at the edge of the statistical precipice. Advances in neural information processing
systems, 34:29304–29320, 2021.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research , 47:253–279, 2013.
CarloBonferroni. Teoriastatisticadelleclassiecalcolodelleprobabilita. Pubblicazioni delR Istituto Superiore
di Scienze Economiche e Commericiali di Firenze , 8:3–62, 1936.
EunYi Chung and Joseph P. Romano. Exact and asymptotically robust permutation tests. The An-
nals of Statistics , 41(2):484 – 507, 2013. doi: 10.1214/13-AOS1090. URL https://doi.org/10.1214/
13-AOS1090 .
Cédric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. How many random seeds? statistical power analysis
in deep reinforcement learning experiments. arXiv preprint arXiv:1806.08295 , 2018.
7https://www.chistera.eu/projects/causalxrl
8https://project.inria.fr/hpcbigdata/
9https://project.inria.fr/sr4sg/home/
19Published in Transactions on Machine Learning Research (08/2024)
Cédric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. A hitchhiker’s guide to statistical comparisons of
reinforcement learning algorithms. arXiv preprint arXiv:1904.06979 , 2019.
Geoff Cumming and Sue Finch. Inference by eye: confidence intervals and how to read pictures of data.
American psychologist , 60(2):170, 2005.
Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Mushroomrl: Simplifying
reinforcement learning research. Journal of Machine Learning Research , 22(131):1–5, 2021. URL http:
//jmlr.org/papers/v22/18-056.html .
Omar Darwiche Domingues, Yannis Flet-Berliac, Edouard Leurent, Pierre Ménard, Xuedong Shang, and
Michal Valko. rlberry - A Reinforcement Learning Library for Research and Education, 10 2021. URL
https://github.com/rlberry-py/rlberry .
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and
Aleksander Madry. Implementation matters in deep policy gradients: A case study on PPO and TRPO.
arXiv preprint arXiv:2005.12729 , 2020.
Ronald Aylmer Fisher. Design of experiments. British Medical Journal , 1(3923):554, 1936.
Yannis Flet-Berliac, Reda Ouhamma, Odalric-Ambrym Maillard, and Philippe Preux. Learning value func-
tions in deep policy gradients using residual variance. In ICLR 2021-International Conference on Learning
Representations , 2021.
Steven N Goodman, Daniele Fanelli, and John PA Ioannidis. What does research reproducibility mean?
Science translational medicine , 8(341):341ps12–341ps12, 2016.
KK Gordon Lan and David L DeMets. Discrete sequential boundaries for clinical trials. Biometrika , 70(3):
659–663, 1983.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In International conference on machine
learning, pp. 1861–1870. PMLR, 2018.
Steven R. Howard, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon. Time-uniform, nonparametric,
nonasymptotic confidence sequences. The Annals of Statistics , 49(2):1055 – 1080, 2021. doi: 10.1214/
20-AOS1991. URL https://doi.org/10.1214/20-AOS1991 .
Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta,
and João G.M. Araújo. Cleanrl: High-quality single-file implementations of deep reinforcement learning
algorithms. Journal of Machine Learning Research , 23(274):1–18, 2022. URL http://jmlr.org/papers/
v23/21-1342.html .
Christopher Jennison and Bruce W Turnbull. Group sequential methods with applications to clinical trials .
CRC Press, 1999.
Emilie Kaufmann and Wouter M Koolen. Mixture martingales revisited with applications to sequential tests
and confidence intervals. The Journal of Machine Learning Research , 22(1):11140–11183, 2021.
Ilmun Kim, Sivaraman Balakrishnan, and Larry Wasserman. Minimax optimality of permutation tests. The
Annals of Statistics , 50(1):225 – 251, 2022. doi: 10.1214/21-AOS2103. URL https://doi.org/10.1214/
21-AOS2103 .
D. Knuth. The art of programming, volume 1, Fundamental Algorithms . Addison-Wesley, 1968.
Tor Lattimore and Csaba Szepesvári. Bandit algorithms . Cambridge University Press, 2020.
Erich Leo Lehmann, Joseph P Romano, and George Casella. Testing statistical hypotheses , volume 3.
Springer, 2005.
20Published in Transactions on Machine Learning Research (08/2024)
Les Leventhal and Cam-Loi Huynh. Directional decisions for two-tailed tests: Power, error rates, and sample
size.Psychological Methods , 1(3):278, 1996.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971 , 2015.
John Ludbrook and Hugh Dudley. Why permutation tests are superior to t and f tests in biomedical research.
The American Statistician , 52(2):127–132, 1998.
Cyrus R Mehta, Nitin Patel, Pralay Senchaudhuri, and Anastasios Tsiatis. Exact permutational tests for
group sequential clinical trials. Biometrics , pp. 1042–1053, 1994.
Sandro Pampallona and Anastasios A Tsiatis. Group sequential designs for one-sided and two-sided hypoth-
esis testing with provision for early stopping in favor of the null hypothesis. Journal of Statistical Planning
and Inference , 42(1-2):19–35, 1994.
Andrew Patterson, Samuel Neumann, Martha White, and Adam White. Empirical design in reinforcement
learning, 2023.
Belinda Phipson and Gordon K Smyth. Permutation p-values should never be zero: calculating exact p-
values when permutations are randomly drawn. Statistical applications in genetics and molecular biology ,
9(1), 2010.
Edwin JG Pitman. Significance tests which may be applied to samples from any populations. Supplement
to the Journal of the Royal Statistical Society , 4(1):119–130, 1937.
Stuart J Pocock. Group sequential methods in the design and analysis of clinical trials. Biometrika , 64(2):
191–199, 1977.
Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann.
Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Re-
search, 22(268):1–8, 2021. URL http://jmlr.org/papers/v22/20-1364.html .
Joseph P Romano. Bootstrap and randomization tests of some nonparametric hypotheses. The Annals of
Statistics , 17(1):141–159, 1989.
Joseph P. Romano and Michael Wolf. Exact and approximate stepdown methods for multiple hypothesis
testing.SSRN Journal Electronic Journal , 2003. doi: 10.2139/ssrn.563267. URL https://doi.org/10.
2139%2Fssrn.563267 .
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy
optimization. In International conference on machine learning , pp. 1889–1897. PMLR, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimiza-
tion algorithms. arXiv preprint arXiv:1707.06347 , 2017.
Connie P Shapiro and Lawrence Hubert. Asymptotic normality of permutation statistics derived from
weighted sums of bivariate functions. The Annals of Statistics , 7(4):788–794, 1979.
Jaehyeok Shin, Aaditya Ramdas, and Alessandro Rinaldo. Nonparametric iterated-logarithm extensions of
the sequential generalized likelihood ratio test. IEEE Journal on Selected Areas in Information Theory , 2
(2):691–704, 2021.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012
IEEE/RSJ international conference on intelligent robots and systems , pp. 5026–5033. IEEE, 2012.
John Wilder Tukey. The problem of multiple comparisons. Multiple comparisons , 1953.
A. Wald. Sequential Tests of Statistical Hypotheses. The Annals of Mathematical Statistics , 16(2):117 – 186,
1945. doi: 10.1214/aoms/1177731118. URL https://doi.org/10.1214/aoms/1177731118 .
21Published in Transactions on Machine Learning Research (08/2024)
A Index of notations
•Nscore: the total number of scores used for one agent.
•N: the number of scores per interim.
•K: the total number of interims.
•k: the current interim number.
•L: the number of agents to compare.
•A1,A2,...,AL: the agents to compare.
•E[X]: expectation of the random variable X.
•α: type I error or Family-wise error of a test.
•β: type II error of a test.
•Sn: set of all the permutations of {1,...,n}.
•σ: generic notation for a permutation, σ∈Snfor somen∈N∗.
•id: the identity permutation, i.e.id(k) =kfor all 1≤k≤nforid∈Sn.
•B: the number of random permutations used to approximate the test statistic.
•σ1:k: shorthand for the concatenation of permutations σ1,...,σk,i.e.σ1:k(en,i) =σi(en,i)for all
1≤i≤k.
•Hj: denotes hypothesis jin a multiple test, H′
jdenotes the alternative of hypothesis Hj.
•cj: denotes a comparison. This is a couple (l1,l2)in{1,...,L}2.
•j: shorthand for denoting comparison cj.
•C0: set of all the comparisons done in AdaStop .
•C: current set of undecided comparisons in AdaStop , a subset of C0.
•Ck: state ofCat interimkinAdaStop .
•ei(j)orei,k(j): score corresponding to run number iwhen doing the test for comparison cjfor
interimk.
•TN(σ)andT(j)
N,k(σ): test statistic. Defined in Equation (6) and Equation (8).
•B(C)
N,k: boundaryforteststatistic T(C)
N,k, suchthatif T(C)
N,k(id)>B(C)
N,k, thenrejectthesetofhypotheses
associated to C.
•I: set of true hypotheses and Icits complement.
•FWE: family-wise error, see Definition 1.
•N(µ,σ2): law of a Gaussian probability distribution with mean µand variance σ2.
•t(µ,ν): law of a translated Student probability distribution with center of symmetry µandνdegrees
of freedom.
•MN
f(µ1,σ2
1;µ2,σ2
2): mixture of the two normal probability distributions N(µ1,σ2
1)andN(µ2,σ2
2).
Each component of the mixture in weigh by fand1−frespectively.
•Mt
f(µ1,ν1;µ2,ν2): mixture of two Student probability distributions t(µ1,ν1)andt(µ2,ν2). Each
component of the mixture in weigh by fand1−frespectively.
•PHj,j∈I: probability distribution when Hj,j∈Iare true and Hj,j /∈Iare false.
22Published in Transactions on Machine Learning Research (08/2024)
B Basics on hypothesis testing
To be fully understood, this paper requires the knowledge of some notions of statistics. In the hope of
widening the audience of this paper, we recall notions of statistics related to hypothesis testing that are
essential to understand the AdaStop test.
B.1 Type I and type II error
In its simplest form, a statistical test is aimed at deciding whether a given collection of data X1,...,XN
adheres to some hypothesis H0(called the null hypothesis), or if it is a better fit for the alternative hypothesis
H1. Typically, the null hypothesis states that the mean of the distribution from which the Xi’s are sampled
is equal to some µ0:H0:µ=µ0andH1:µ̸=µ0whereµis the mean of the distribution of X1,...,XN.
Becauseµis unknown, it has to be estimated using the data. Often this is done using the empirical mean
/hatwideµ=1
N/summationtextN
i=1Xi./hatwideµis a random variable and some deviation from µis to be expected. The theory of
hypothesis tests is concerned in finding a threshold csuch that if|/hatwideµ−µ0|>cthen we say that H0is false
because the deviation is greater than what was expected by the theory. A slightly more complex problem is
to consider two samples X1,...,XNandY1,...,YNand do a two-sample test deciding whether the mean of
the distribution of the Xi’s is equal to the mean of the distribution of the Yi’s.
In both cases, the result of a test is either reject H0or to not reject H0. This answer is not a ground truth:
there is some probability that we make an error. However, this probability of error is often controlled and
can be decomposed in type I error and type II error (often denoted αandβrespectively, see Table 3 and
Figure 6). In words, a type I error is when we conclude that the null hypothesis is false ( HOis rejected,
means are different) whereas means are equal. αis the probability that this event occurs. A type II error is
when we conclude that the null hypothesis is true ( H0is accepted, means are equal) whereas means are not
equal.βis the probability that this event occurs.
H0is true H0is false
We acceptH0No error Type II error β
We rejectH0Type I error αNo error
Table 3: Type I and type II error.
The problem is not symmetric: failing to reject the null hypothesis does not mean that the null hypothesis is
true. Itcanbethatthereisnotenoughdatatoreject H0. Ithasbeenshownthatatestcannotsimultaneously
minimizeαandβ. It is customary to minimize βfor a given value of α(typicallyαis set to 0.05). The
probability to reject H0when it is false is 1−βand it is usually called the powerof a test.
B.2 Directional error
Statistical tests are often formulated as bilateral problems, which means that we test µ=µ0versus the
bilateral interval µ∈(−∞,µ0)∪(µ0,+∞). In practice (and in the case of this article), after rejecting the
equality, we want to know if µ>µ 0or ifµ<µ 0. This can be done using the sign of the test statistic /hatwideµ−µ0.
However this is not a direct consequence of the test because the test did not suppose a direction. This means
that in addition to the type I and type II, we can have a type III error which is the probability to say that
µ>µ 0when in fact µ<µ 0or thatµ<µ 0when in fact µ>µ 0.
Intuitively, if the type II error is low, then type III error (Leventhal & Huynh, 1996) tends to be even
lower provided that the data distribution is sufficiently concentrated around its mean. See Figure 6 (right
sub-figure) for a visual representation of the errors, here the probability that /hatwideµ>µ 0+cwhenµ<µ 0. Type
III error is often negligible, not considered in practice, and even unknown to many researchers.
23Published in Transactions on Machine Learning Research (08/2024)
c
0cType I error
c
0cType II error
c
0cType III error
Figure 6: Illustration of type I, II and III errors: in these 3 plots, the shaded areas represent the probability
of type I error (left), type II error (middle) and type III error (right) when testing µ= 0vs.µ̸= 0.cdenotes
the threshold on test statistic /hatwideµafter which we take a decision.
B.3 Multiple tests and FWE
When performing simultaneously Lstatistical tests for L>1, one must be careful that the error of each test
accumulates. If one is not cautious, the overall error may become non-negligible. As a consequence, multiple
strategies have been developed to deal with multiple testing problem.
To deal with the multiple testing problem, the first step is to define what is an error. There are several
definitions of error in multiple testing among which is the False Discovery Rate which measures the expected
proportion of false rejections. Another possible measure of error is the Family-Wise Error (the error we use
in this article) which is defined as the probability to make at least one false rejection:
FWE = PHj,j∈I(∃j∈I:rejectHj),
where PHj,j∈Iis used to denote a probability. I⊂{1,...,L}is the set of indices of the hypotheses that are
true (and Icthe set of hypotheses that are actually false). To construct a procedure with an FWE smaller
thanα, the simplest method is perhaps Bonferroni’s correction (Bonferroni, 1936) in which a statistical test
is applied on each of the Jcouples of hypotheses to be tested ( J=L(L−1)/2). And then, one would tune
each hypothesis test to have a type I error α/J. The union bound then implies that the FWE is bounded
byα:
FWE =PHj,j∈I/parenleftig/uniondisplay
j∈I{rejectHj}/parenrightig
≤/summationdisplay
i∈IPHj,j∈I(rejectHj)≤|I|α
J≤α.
Bonferroni’s correction has the advantage of being very simple to implement. However, it is often very
conservative and the FWE is most often a lot smaller than α. An alternative method that performs well in
practice is the step-down method that we use in this article and is presented in Section 3.2.4.
C Proof of Theorem 1
The proof of Theorem 1 is based on an extension of the proof of the control of FWE in the non-sequential
case and the proof of the step-down method (Romano & Wolf, 2003). The interested reader may refer to
Lemma 1, in the Appendix C.1, where we reproduce the proof of the bound on FWE for simple permutation
tests as it is a good introduction to permutation tests. The proof proceeds as follows: first, we prove weak
control10on the FWE by decomposing the error as the sum of the errors on each interim and using the
properties of permutation tests to show that the error done at each interim is controlled by α/K. Then,
using the step-down method construction, we show that the strong control of the FWE is a consequence of
the weak control because of monotony properties on the boundary values of a permutation test.
C.1 Simplified proof for L= 2agents, and K= 1
The proof of this theorem is a bit technical. We begin by showing it in a very simplified case where L= 2
agents, and K= 1.
10A procedure controls has a weak control on the FWE if the FWE is smaller than αwhen all null hypotheses are true. On
the other hand, strong control is when FWE is smaller than αwhatever the set of true null hypotheses.
24Published in Transactions on Machine Learning Research (08/2024)
Lemma 1. LetX1,...,XNbe i.i.d.from a distribution PandY1,...,YNbe i.i.d. from a distribution Q.
LetZ2N
1=X1,...,XN,Y1,...,YNbe the concatenation of XN
1andYN
1. Letα∈(0,1)and defineBNas:
BN= inf/braceleftigg
b>0 :1
(2N)!/summationdisplay
σ∈S2N1/braceleftigg
1
NN/summationdisplay
i=1(Zσ(i)−Zσ(N+i))>b/bracerightigg
≤α/bracerightigg
.
Then, ifP=Q, we have:
P/parenleftigg
1
NN/summationdisplay
i=1(Xi−Yi)>BN/parenrightigg
≤α.
Proof.Let us denote T(σ) =1
N/summationtextN
i=1(Zσ(i)−Zσ(n+i)). SinceP=Q, for anyσ,σ′∈S2Nwe have
T(σ)d=T(σ′). Then, because BNdoes not depend on the permutation σ(but it depends on the values
ofZ2N
1), we have, for any σ∈S2N
P(T(id)>BN) =P(T(σ)>BN).
Now, we sum all the permutations:
P(T(id)>BN) =1
(2N)!/summationdisplay
σ∈S2NE[ 1{T(σ)>BN}]
=E/bracketleftigg
1
(2N)!/summationdisplay
σ∈S2N1{T(σ)>BN}/bracketrightigg
≤α,
which proves the result.
Next, we prove weak control in the general case.
C.2 Proof of Theorem 1
In this section, we use the shorthand Pinstead of PHj,j∈Iand omitHj,j∈IbecauseIwill always be the set
of true hypotheses and the meaning should be clear from the context.
Weak control on FWE: First, we prove weak control on the FWE. This means that we suppose that all
the hypotheses are true ( I={1,...,J}), and we control the probability to make at least one rejection. We
have:
FWE = P(∃j∈I:Hjis rejected ).
We decompose the FWE on the set of interims, using the fact that a rejection happens if and only if we
reject a true hypothesis for the first time at interim kfor somek= 1,...,K. These events are mutually
exclusive therefore we have that their probabilities sum up and we can rewrite the FWE as:
FWE =K/summationdisplay
k=1P/parenleftig
T(I)
N,k(id)>B(I)
N,k,NRk(id)/parenrightig
, (13)
where NRk(id)istheeventonwhichwedid Not Reject (NR)beforeinterim k.NRk(id)canbedefineddirectly
for a concatenation of permutations σ1:kasNRk(σ1:k) ={∀m<k,T(I)
N,m(σ1:k)≤B(I)
N,m}. We introduced the
definition for σ1:kand not only for idsince this will be useful later on, for example in Equation (14).
Then, similarly as in the proof of Lemma 1, we want to use the invariance by permutation to make the
link with the definition of B(I)
N,k. For this purpose, we introduce the following lemma, that we prove in
Appendix D.
Lemma 2. Fork≤K, for anyσ1:kconcatenation of kpermutations:
(T(I)
N,l(id),B(I)
N,l)l≤kd= (T(I)
N,l(σ1:l),B(I)
N,l)l≤k.
25Published in Transactions on Machine Learning Research (08/2024)
Using Lemma 2, we have for any interim kand concatenation of permutations σ1:k
P/parenleftig
T(I)
N,k(id)>B(I)
N,k,NRk(id)/parenrightig
=P/parenleftig
T(I)
N,k(σ1:k)>B(I)
N,k,NRk(σ1:k)/parenrightig
. (14)
Combing this equation with equation (13), we have:
FWE≤K/summationdisplay
k=11
mk/summationdisplay
σ1:k∈SN,kP/parenleftig
T(I)
N,k(σ1:k)>B(I)
N,k,NRk(σ1:k)/parenrightig
=K/summationdisplay
k=1E
1
mk/summationdisplay
σ1:k∈SN,k1/braceleftig
T(I)
N,k(σ1:k)>B(I)
N,k,NRk(σ1:k)/bracerightig
.
Then, use that σ1:k∈/hatwideSN,kif and only if σ1:k∈SN,kandNRk(σ1:k)is true. Hence,
FWE≤K/summationdisplay
k=1E
1
mk/summationdisplay
σ1:k∈/hatwideSN,k1/braceleftig
T(I)
N,k(σ1:k)>B(I)
N,k/bracerightig
≤K/summationdisplay
k=1qk≤α,
where we use the definition of B(I)
N,kto make the link with α
Strong control of FWE: To prove strong control, it is sufficient to show the following Lemma (see
Appendix D for a proof), which is an adaptation of the proof of step-down multiple-test strong control of
FWE from (Romano & Wolf, 2003).
Lemma 3. Suppose that I⊂{1,...,J}is the set of true hypotheses. We have:
FWE = P(∃j∈I:Hjis rejected )≤P/parenleftig
∃k≤K:T(I)
N,k(id)>B(I)
N,k/parenrightig
.
Lemma 3 shows that to control the FWE, it is sufficient to control the probability to reject on Igiven by
P/parenleftig
∃k≤K:T(I)
N,k(σ1:k)>B(I)
N,k/parenrightig
and this quantity, in turn, is exactly the FWE of the restricted problem
of testing (Hj)j∈Iagainst (H′
j)j∈I. In other words, Lemma 3 says that to prove strong FWE control for
AdaStop , it is sufficient to prove weak FWE control, and we already did that in the first part of the proof.
D Proof of Lemmas 2 and 3
D.1 Proof of Lemma 2
In this section, for an easier understanding, we change the notation for the score e(j)
n,k(σ)toen,k(Al)thenth
score of agent Alat interimk. In other words, we change the notation of the comparison of agent Al1versus
agentAl2: in the main text a comparison was denoted by cj∈{1,...,J}2but here we make explicit the
agent from which the score has been computed resulting in the following equalities: en,k(Al1) =e(j)
n,k(id)and
en,k(Al2) =e(j)
N+n,k(id)forn≤N.
We denote the comparisons by (ci)i∈I. The set of comparisons can be represented as a graph in which
each node represents one of the agents to compare, and there exists an edge from j1toj2denoted (j1,j2)
if(j1,j2)∈(ci)i∈Iis one of the comparisons that corresponds to a true hypothesis. This graph is not
necessarily connected. We denote C(i)the connected component to which node l(e.g.agentl) belongs,
i.e.for anyl1,l2∈C(l)there exists a path going from l1tol2.C(l)cannot be equal to the singleton {l},
because this would mean that all the comparisons with lare in fact false hypotheses, and then lwould not
belong to a couple in I.
26Published in Transactions on Machine Learning Research (08/2024)
Then, it follows from the construction of permutation test that jointly on k≤Kandcj= (l1,l2)∈C(l), we
haveT(j)
N,k(id)d=T(j)
N,1(σ1:k)for anyσ1,...,σk∈S2N.
Let us illustrate that on an example. Suppose that N= 2andJ= 3so that the comparison are (A1,A2),
(A1,A3),(A2,A3). Consider the permutation
σ1=/parenleftbigg
1 2 3 4
3 1 2 4/parenrightbigg
Because all the scores are i.i.d., we have the joint equality in distribution:

|e1,1(A1) +e2,1(A1)−e1,1(A2)−e2,1(A2)|
|e1,1(A3) +e2,1(A3)−e1,1(A2)−e2,1(A2)|
|e1,1(A1) +e2,1(A1)−e1,1(A3)−e2,1(A3)|
d=
|e2,1(A1) +e1,1(A2)−e1,1(A1)−e2,1(A2)|
|e2,1(A3) +e1,1(A2)−e1,1(A3)−e2,1(A2)|
|e2,1(A1) +e1,1(A3)−e1,1(A1)−e2,1(A3)|

and hence,
(T(j)
N,1(id)) 1≤j≤3d= (T(j)
N,1(σ1))1≤j≤3.
Fork= 2, we have for σ2=σ1,

|e1,1(A1) +e2,1(A1)−e1,1(A2)−e2,1(A2)|
|e1,1(A3) +e2,1(A3)−e1,1(A2)−e2,1(A2)|
|e1,1(A1) +e2,1(A1)−e1,1(A3)−e2,1(A3)|
|e1,1(A1) +e2,1(A1)−e1,1(A2)−e2,1(A2) +e1,2(A1) +e2,2(A1)−e1,2(A2)−e2,2(A2)|
|e1,1(A3) +e2,1(A3)−e1,1(A2)−e2,1(A2) +e1,2(A3) +e2,2(A3)−e1,2(A2)−e2,2(A2)|
|e1,1(A1) +e2,1(A1)−e1,1(A3)−e2,1(A3) +e1,2(A1) +e2,2(A1)−e1,2(A3)−e2,2(A3)|

d=
|e1,1(A1) +e2,1(A2)−e1,1(A1)−e2,1(A2)|
|e1,1(A3) +e2,1(A2)−e1,1(A3)−e2,1(A2)|
|e1,1(A1) +e2,1(A3)−e1,1(A1)−e2,1(A3)|
|e1,1(A1) +e2,1(A2)−e1,1(A1)−e2,1(A2) +e1,2(A1) +e2,2(A2)−e1,2(A1)−e2,2(A2)|
|e1,1(A3) +e2,1(A2)−e1,1(A3)−e2,1(A2) +e1,2(A3) +e2,2(A2)−e1,2(A3)−e2,2(A2)|
|e1,1(A1) +e2,1(A3)−e1,1(A1)−e2,1(A3) +e1,2(A1) +e2,2(A3)−e1,2(A1)−e2,2(A3)|

and then, we get jointly
(T(j)
N,k(id)) 1≤j≤3,k≤2d= (T(j)
N,k(σ1·σ2))1≤j≤3,k≤2.
This reasoning can be generalized to any N,JandK:
(T(j)
N,k(id))k≤K,cj∈C(l)2d= (T(j)
N,k(σ1:k))k≤K,cj∈C(l)2.
Then, by construction, the different connected component C(l)are independent of one another and hence,
(T(j)
N,k(id))k≤K,j∈Id= (T(j)
N,k(σ1:k))k≤K,j∈I.
Lemma 2 follows from taking the maximum on all the comparisons and because the boundaries do not
depend on the permutation.
D.2 Proof of Lemma 3
Denote by Ckthe (random) value of Cat the beginning of interim k. We have:
FWE = P(∃j∈I:Hjis rejected )
=P/parenleftigg
∃k≤K:T(Ck)
N,k(id)>B(Ck)
N,k,arg max
j,cj∈CkT(j)
N,k(id)∈I/parenrightigg
. (15)
27Published in Transactions on Machine Learning Research (08/2024)
Then, letk0correspond to the very first rejection (if any) in the algorithm. Having that the argmax is
attained in I,
T(Ck0)
N,k0(id) = max{T(j)
N,k0(id),cj∈Ck0}= max{T(j)
N,k0(id),j∈I}=T(I)
N,k0(id)
Moreover, having that the comparisons indexed by Iare included into Ck0, we have B(Ck0)
N,k0≥B(I)
N,k0.
Injecting these two relations in Equation (15), we obtain:
FWE≤P/parenleftigg
∃k≤K:T(I)
N,k(id)>B(I)
N,k,arg max
j∈CkT(j)
N,k(id)∈I/parenrightigg
≤P/parenleftig
∃k≤K:T(I)
N,k(id)>B(I)
N,k/parenrightig
.
This proves the desired result.
E Asymptotic results for two agents
E.1 Convergence of boundaries and comparing the means
Because there are only two agents and no early stopping, we simplify the notations and denote
tN,i(σi) =N/summationdisplay
n=1eσi(n),i(2)−2N/summationdisplay
n=N+1eσi(n),i(1),
and
TN,k(σ1:k) =/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglek/summationdisplay
i=1/parenleftiggN/summationdisplay
n=1eσi(n),i(2)−2N/summationdisplay
n=N+1eσi(n),i(1)/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglek/summationdisplay
i=1tN,i(σi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle,
and
BN,k= inf

b>0 :1
((2N)!)k/summationdisplay
σ1,...,σk∈Sk
2N1{TN,k(σ1:k)≥b}≤qk

.
When there is only one interim ( K= 1), we have the following convergence of the randomization law of
TN,1(σ).
Proposition 1 (Theorem 17.3.1 in (Lehmann et al., 2005)) .Supposee1,1(1),...,eN,1(1)are i.i.d.from P
ande1,1(2),...,eN,1(2)are i.i.d.from Qand bothPandQhave finite variance. Then, we have:
sup
t/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
(2N)!/summationdisplay
σ∈S2N1/braceleftbigg1√
NTN,1(σ)≤t/bracerightbigg
−Φ (t/τ(P,Q))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleP−−−−→
N→∞0
where Φis the normal c.d.f.and τ(P,Q)2=σ2
P+σ2
Q+(µP−µQ)2
2.
Using the non-sequential result from proposition 1, we can show the following theorem that controls the
asymptotic law of the sequential test.
Theorem 3. Suppose that PandQboth have a finite second moment. Then, for any 1≤k≤K,
1√
NBN,k−−−−→
N→∞bkwhere the real numbers bkare defined as follows. Let W1,...,WKbe i.i.d.random
variables with law N(0,1). Thenb1is the solution of the following equation:
P/parenleftbigg
|W1|≥b1
τ(P,Q)/parenrightbigg
=α
K,
28Published in Transactions on Machine Learning Research (08/2024)
and for any 1<k≤K,bkis the solution of
P
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
kk/summationdisplay
j=1Wj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle>bl
τ(P,Q),∀j <k,/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
jj/summationdisplay
i=1Wi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤bj
τ(P,Q)
=α
K.
The conditions on Theorem 3 are very weak: the existence of a finite second moment to be able to use the
central limit theorem.
The test performed in AdaStop when comparing two agents corresponds to testing
1/braceleftbigg
∃k≤K:1√
NTN,k(id)>1√
NBN,k/bracerightbigg
and from Theorem 3 and the central-limit theorem,1√
NTN,k(id)converges to/summationtextk
j=1Wj/radicalig
σ2
P+σ2
Q, and
BN,k/√
Nconverges to bk. Hence the test is asymptotically equivalent to:
1

∃k≤K:k/summationdisplay
j=1Wj/radicalig
σ2
P+σ2
Q>bk

.
Then, in the case in which µP=µQ, we haveτ(P,Q) =/radicalig
σ2
P+σ2
Qand
FWE = P
∃k≤K:k/summationdisplay
j=1Wj/radicalig
σ2
P+σ2
Q>bk

=K/summationdisplay
k=1P
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
kk/summationdisplay
j=1Wj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle>bl
τ(P,Q),∀j <k,/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
jj/summationdisplay
i=1Wi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤bj
τ(P,Q)

=K/summationdisplay
k=1α
K=α.
Hence, for the test H0:µP=µQversusH1:µP̸=µQ, our test has asymptotic type I error α.
E.2 Proof of Theorem 3
Forx∈R, we denote:
RN,k(x) =1
(2N)!/summationdisplay
σk∈S2N1{tN,k(σk)≤x}.
RN,kis the c.d.f.of the randomization law of tN,k(σk), and by Proposition 1, it converges uniformly to a
Gaussian c.d.f when Ngoes to infinity.
Convergence of BN,1By definition of the boundary, we have
1√
NBN,1=1√
Nmin/braceleftigg
b>0 :1
(2N)!/summationdisplay
σ1∈S2N1{|TN,1(σ1)|>b}≤α
K/bracerightigg
.
This implies
1
(2N)!/summationdisplay
σ1∈S2N1{|TN,1(σ1)|≤BN,1}=/hatwideRN,1/parenleftbigg1√
NBN,1/parenrightbigg
−/hatwideRN,1/parenleftbigg
−1√
NBN,1/parenrightbigg
≥1−α
K
29Published in Transactions on Machine Learning Research (08/2024)
and for any b<BN,1, we have:
1
(2N)!/summationdisplay
σ1∈S2N1{|TN,1(σ1)|≤b}=/hatwideRN,1/parenleftbiggb√
N/parenrightbigg
−/hatwideRN,1/parenleftbigg
−b√
N/parenrightbigg
<1−α
K.
Then,
Φ/parenleftbiggBN,1
τ(P,Q)√
N/parenrightbigg
−Φ/parenleftbigg
−BN,1
τ(P,Q)√
N/parenrightbigg
≥/hatwideRN,1/parenleftbiggBN,1√
N/parenrightbigg
−/hatwideRN,1/parenleftbigg
−BN,1√
N/parenrightbigg
−/vextendsingle/vextendsingle/vextendsingle/vextendsingleΦ/parenleftbiggBN,1
τ(P,Q)√
N/parenrightbigg
−/hatwideRN,1/parenleftbiggBN,1√
N/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
−/vextendsingle/vextendsingle/vextendsingle/vextendsingleΦ/parenleftbigg
−BN,1
τ(P,Q)√
N/parenrightbigg
−/hatwideRN,1/parenleftbigg
−BN,1√
N/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≥1−α
K−2 sup
t/vextendsingle/vextendsingle/vextendsingle/vextendsingleΦ/parenleftbiggt
τ(P,Q)/parenrightbigg
−/hatwideRN,1(t)/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
Hence, by taking Nto infinity, we have from Proposition 1:
lim inf
N→∞Φ/parenleftbiggBN,1
τ(P,Q)√
N/parenrightbigg
−Φ/parenleftbigg
−BN,1
τ(P,Q)√
N/parenrightbigg
≥1−α
K.
And for any ε>0, because of the definition of BN,1as a supremum, we have:
lim sup
N→∞Φ/parenleftbiggBN,1+ε
τ(P,Q)√
N/parenrightbigg
−Φ/parenleftbigg
−BN,1+ε
τ(P,Q)√
N/parenrightbigg
<1−α
K.
By continuity of Φ, this implies that1√
NBN,1converges almost surely and its limit is such that:
Φ/parenleftigg
limN→∞BN,1/√
N
τ(P,Q)/parenrightigg
−Φ/parenleftigg
−limN→∞BN,1/√
N
τ(P,Q)/parenrightigg
= 1−α
K.
Or said differently, let W∼N(0,1), then we have the almost sure convergence limN→∞1√
NBN,1=b1where
b1is the real number defined by
P/parenleftbigg
|W|≥b1
τ(P,Q)/parenrightbigg
=α
K.
Convergence of BN,kfork>1.We proceed by induction. Suppose that1√
NBN,k−1converges to some
bk−1>0and that for any d1,...,dk−1, the randomization probability, it holds that:
sup
d1,...,dk−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
((2N)!)k−1/summationdisplay
σ1,...,σk−1∈S2N1/braceleftigg
∀j≤k−1,j/summationdisplay
i=1tN,i(σi)≤dj√
N/bracerightigg
−P/parenleftigg
∀j≤k−1,j/summationdisplay
i=1Wi≤dj
τ(P,Q)/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN→∞−−−−→
a.s.0,(16)
whereW1,...,Wk−1are i.i.d.N(0,1)random variables. In other words, the randomization law converges
uniformly to the joint law described above with the sum of Gaussian random variables.
Then, by uniform convergence and by convergence of the BN,j, we have that:
1
((2N)!)k−1/summationdisplay
σ1,...,σk−1∈S2N1{TN,j(σ1:j)>BN,k−1,∀j <k−1, TN,j(σ1:j)≤BN,j}
30Published in Transactions on Machine Learning Research (08/2024)
converges to
P/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglel/summationdisplay
i=1Wi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle>bl
τ(P,Q),∀j <l,/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglej/summationdisplay
i=1Wi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤bj
τ(P,Q)/parenrightigg
=α
K. (17)
the last equality follows by construction of BN,jforj <k.
We have:
BN,k= min

b>0 :1
((2N)!k)/summationdisplay
σ1,...,σk∈S2N1/braceleftbigg/vextendsingle/vextendsingle/summationtextk
j=0tN,j(σj)/vextendsingle/vextendsingle≥b,
∀j<k,/vextendsingle/vextendsingle/summationtextj
i=0tN,i(σi)/vextendsingle/vextendsingle≤BN,j/bracerightbigg
+k−1/summationdisplay
i=1qi≤kα
K

.
By the induction hypothesis, we have qi−−−−→
n→∞α/Kfor anyi<k.
LetW1,...,Wkbe i.i.d.N(0,1)random variables. We show the following lemma that proves part of the step
kof the induction hypothesis, proved in Section E.3.
Lemma 4. Suppose Equation (17)is true. Then,
sup
d1,...,dk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
((2N)!)k/summationdisplay
σ1,...,σk∈S2N1/braceleftigg
∀j≤k,j/summationdisplay
i=1tN,i(σi)≤dj√
N/bracerightigg
−P/parenleftigg
∀j≤k,j/summationdisplay
i=1Wi≤dj
τ(P,Q)/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN→∞−−−−→
a.s.0,
Then, what remains is to prove the convergence of BN,k. Let us denote:
Ψk(dk) =P/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglek/summationdisplay
i=1Wi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle>dk
τ(P,Q),∀j≤k−1,/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglej/summationdisplay
i=1Wi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤bj
τ(P,Q)/parenrightigg
.
From Lemma 4, we have that
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleΨk/parenleftbiggBN,k√
N/parenrightbigg
−1
((2N)!)k/summationdisplay
σ1,...,σk∈S2N1{TN,k(σ1:k)>BN,k,∀j <k, TN,j(σ1:j)≤BN,j}/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
converges to 0asNgoes to infinity. Hence,
lim sup
N→∞Ψk/parenleftbiggBN,k√
N/parenrightbigg
≤α/K.
Then, similarly to the case k= 1, we also have for any ε>0:
lim inf
N→∞Ψk/parenleftbiggBN,k−ε√
N/parenrightbigg
≥α/K
and by continuity of Ψk(which is a consequence of the continuity of the joint c.d.f.of Gaussian random
variables) we conclude that BN,k/√
Nconverges almost surely to bk.
E.3 Proof of Lemma 4
In this proof, let Eσ1:k(x)denote the expectation of the randomization law defined for some function f:
Sk
2N→Rby:
Eσ1:k[f(σ1:k)] =1
((2N)!)k/summationdisplay
σ1,...,σk∈S2Nf(σ1:k).
31Published in Transactions on Machine Learning Research (08/2024)
Please note that this expectation is still random and should be differentiated from the usual expectation E.
First, let us first handle the convergence of step k. We have:
1
(2N)!/summationdisplay
σk∈S2N1

k/summationdisplay
j=1tN,j(σj)≤dk√
N


=1
(2N)!/summationdisplay
σk∈S2N1

1√
NtN,k(σk)≤dk−1√
Nk−1/summationdisplay
j=1tN,j(σj)


=/hatwideRn,k
dk−1√
Nk−1/summationdisplay
j=1tN,j(σj)
.
Because the convergence in proposition 1 is uniform, we have:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/hatwideRn,k
dk−1√
Nk−1/summationdisplay
j=1tN,j(σj)
−Φ
1
τ(P,Q)
dk−1√
Nk−1/summationdisplay
j=1tN,j(σj)

/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤sup
t/vextendsingle/vextendsingle/vextendsingle/vextendsingle/hatwideRn(t)−Φ/parenleftbiggt
τ(P,Q)/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle−−−−→
n→∞0.
Then, using this convergence we have that, when Ngoes to infinity,
Eσ1:k/bracketleftigg
1/braceleftigg
∀j <k,j/summationdisplay
i=1tN,i(σi)≤dj√
N/bracerightigg/bracketrightigg
converges uniformly on d1,...,dkto:
Eσ1:k/bracketleftigg
1/braceleftigg
∀j <k−1,j/summationdisplay
i=1tN,i(σi)≤dj√
N/bracerightigg
P/parenleftigg
Wk≤1
τ(P,Q)/parenleftigg
dk−1√
Nk−1/summationdisplay
i=1tN,i(σi)/parenrightigg/parenrightigg/bracketrightigg
=E/bracketleftbigg
Eσ1:k−1/bracketleftbigg
1/braceleftbigg
∀j<k−2,/summationtextj
i=1tN,i(σi)≤dj√
N,
1√
N/summationtextk−1
i=1tN,j(σi)≤min(dk−τ(P,Q)Wk,dk−1)/bracerightbigg/bracketrightbigg/bracketrightbigg
. (18)
Then, using the induction hypothesis, Equation (18) converges to Equation (16), hence in the limit we have
E/bracketleftbigg
1/braceleftbigg
∀j<k−2,/summationtextj
i=1Wi≤dj
τ(P,Q),
1√
N/summationtextk−1
i=1Wi≤1
τ(P,Q)min(dk−Wk,dk−1)/bracerightbigg/bracketrightbigg
=E/bracketleftigg
1/braceleftigg
∀j <k,j/summationdisplay
i=1Wi≤dj
τ(P,Q)/bracerightigg/bracketrightigg
=P/parenleftigg
∀j≤k,j/summationdisplay
i=1Wi≤dj
τ(P,Q)/parenrightigg
.
F On early accept in AdaStop
LetC⊂{c1,...,cJ}be a subset of the set of comparisons that we want to do. Let us denote:
T(C)
N,k(σk
1) := max/parenleftig
T(j)
N,k(σk
1),cj∈C/parenrightig
andT(C)
N,k(σk
1) := min/parenleftig
T(j)
N,k(σk
1),cj∈C/parenrightig
,
and
B(C)
N,k:= inf

b>0 :1
mk/summationdisplay
σ∈/hatwideSN,k1{T(C)
N,k(σk
1)≥b}≤qk

, (19)
32Published in Transactions on Machine Learning Research (08/2024)
and
B(C)
N,k:= sup

b>0 :1
mk/summationdisplay
σ∈/hatwideSN,k1{T(C)
N,k(σk
1)≤b}≤qk

(20)
where theqi=⌊αi
2K/parenleftbig2N
N/parenrightbig
⌋/(1
2/parenleftbig2N
N/parenrightbig
)−qi−1andqi=⌊βi
2K/parenleftbig2N
N/parenrightbig
⌋/(1
2/parenleftbig2N
N/parenrightbig
)−qi−1and where/hatwideSN,kis defined in
Equation (11). By construction of /hatwideSN,k, for eachσk
1∈/hatwideSN,k, we have the following property
∀m<k,T(C)
N,m(σk
1)≤B(C)
N,mandT(C)
N,m(σk
1)≥B(C)
N,m.
InAdaStop , modify the decision step (lines 9 to 13 in Algorithm 3) to Algorithm 4.
Algorithm 4: Early accept.
1ifT(C)
N,k(id)>B(C)
N,kthen
2RejectHjmaxwherecjmax= arg max/parenleftig
T(j)
N,k(id),cj∈C/parenrightig
.
3UpdateC=C\{cjmax}
4else ifT(C)
n,k(id)<B(C)
N,kthen
5AcceptHjminwherecjmin= arg min/parenleftig
T(j)
N,k(id),cj∈C/parenrightig
.
6UpdateC=C\{cjmin}
The resulting algorithm has a small probability to accept a decision early, and as a consequence it may be
unnecessary to compute the score of some of the agents in the subsequent steps.
Asanillustrationoftheperformanceofearlyaccept, ifwerun AdaStop withearlyparameter β= 0.01(used
to define the boundary in Equation (20)) on the Walker2D-v3 experiment from Section 5.3, the experiment
stops at interim 2and10scores have been collected for each agent. This has to be compared with the fact
that, in Section 5.3, we showed that without early accept, AdaStop uses 30scores for DDPG and TRPO.
In this example, early stopping saves a lot of computations and results in a significant speed-up without
affecting the final decisions.
G Agent comparison on Atari environments
In this section, we discuss the approach of (Agarwal et al., 2021) on the problem of comparing RL agents
on Atari environments. The methodology from (Agarwal et al., 2021) prescribes to give confidence intervals
around some measure of location (typically mean or interquartile mean, IQM) of the agents score. We
demonstrate this approach in the left and middle sub-figures of Figure 7. However, it is not clear how to
draw a conclusion from such graphs (see (Cumming & Finch, 2005) for a discussion on doing inference with
confidence intervals). Moreover, there is no definite criterion on how to construct the confidence interval.
For example, this approach gives rise to three very different confidence interval plots in Figure 7: the plots
on the left and in the middle are the ones presented in (Agarwal et al., 2021) and the plot on the right is a
modified version of the plot in the middle, where we added the error due to performing multiple comparisons.
In the leftmost plot of Figure 7, it is easy to draw conclusions but on the other hand the theoretical
interpretation is not clear because the fact that different games have different laws for their scores is not
taken into account. The middle and rightmost plots in Figure 7 are a naive approach to the problem that
assumes that all the games are very different from one another, and as such the confidence intervals are too
large to draw conclusions. We think that there should be an intermediate approach that considers a cluster
of similar games ( e.g.all the easy games, all the maze-like games, all the difficult games, etc.) and treat
these games as having all the same law. This approach would produce smaller (and more interpretable)
confidence intervals compared to the naive approach, providing a middle-ground between the first plot and
second plot.
33Published in Transactions on Machine Learning Research (08/2024)
1.0 1.5 2.0DQN (Nature)DQN (Adam)C51REMIQNRainbowM-IQNDreamerV2IQM
1 2 3 4IQM stratified
2 4 6IQM stratified corrected
Human Normalized Score
Figure 7: 95%confidence intervals on the IQM of the "Human Normalized Score" of a set of algorithms with
various methods using the results from (Agarwal et al., 2021).
NN(0,0.01)
*NN(0,0.01)
MG1MN
1/2(−1,0.01; 1,0.01)
*MG1MN
1/2(−1,0.01; 1,0.01)
MG2MN
1/2(−0.2,0.01; 0.2,0.01)
tS1 t(0,3)
MG3MN
1/2(−1,0.01; 8,0.01)
*MG3MN
1/2(−1,0.01; 8,0.01)
MtSMt
1/2(0,3; 0,8)
tS2 t(8,3)
2
 0 2 4 6 8 10
Figure 8: Toy example 3, with an illustration of the involved distributions.
H Implementation details and additional experiments
H.1 Complementary experiment for Section 5.1
Weconsider10 agentswhichscore distributions arelistedin Fig.8, wherethe firstcolumnindicatesthe labels
of the agents as they are used in Fig. 9. We add some more notations: t(µ,ν)is the t-Student distribution
with mean µandνdegrees of freedom, and Mt
1(t(µ1,ν1),t(µ2,ν2))is a mixture of 2 t-Student distributions
t(µ1,ν1)andt(µ2,ν2), each weights fand1−f.
Similarly to the setup of cases 1 and 2 (see Section 5.1), we execute AdaStop withK= 5,N= 5,α= 0.05.
As indicated in Section 4, we do not enumerate all the permutations in the permutation test as this would
be too expensive. Instead we use 104randomly selected permutations to compute our test statistics at each
interim.
In contrast to the experiments reported in the main text, we use early accept (with β= 0.01) to avoid
situations where all agents are compared with the maximum number of scores, i.e.NKscores. This may
occur when each agent has a similar distribution to at least one other agent.
34Published in Transactions on Machine Learning Research (08/2024)
n_iter 20 15 15 15 20 20 15 15 15 15
N tS1 MG2 *N *MG1 MG1 *MG3 MtS MG3 tS2N
tS1
MG2
*N
*MG1
MG1
*MG3
MtS
MG3
tS2 =
=
=
=
  =
=
=
=
=
=
  =
=
=
=
=
=
  =
=
=
=
=
  =
=
=
=
=
  
 =
=
=
  =
=
=
  
 
N tS1 MG2 *N *MG1 MG1 *MG3 MtS MG3 tS24
2
0246810
Figure 9: AdaStop decision table (left) and measured empirical distributions (right).
We show the performance of AdaStop for multiple agents comparison in Fig. 9, which corresponds to
the output of one execution of AdaStop . The table (left part of Fig. 9) summarizes the decisions of the
algorithm for every pair of comparisons. The violin plots (right part of Fig. 9) reflect empirically measured
distributions in the comparison. From this figure, we can see that almost all agents are clustered according
to the mean of this distribution, except for *MG3, Mt5 and MG3 that are assigned to two different groups
at once. Interestingly, except for *MG3, these clusters are correctly formed. Moreover, similarly to the two
previous cases, we have executed AdaStopM= 5 000times to measure the FWE of the test. Empirically,
we measured a FWE of 0.0178for the test comparing the distributions and a FWE of 0.0472for the test
comparingthemeans: botharebelow 0.05. Thisexampleillustratesthefactthat AdaStop canbeefficiently
used to compare the score of several agents simultaneously.
35Published in Transactions on Machine Learning Research (08/2024)
H.2 MuJoCo Experiments
DDPG TRPO PPO SAC
γ 0.99 0.99 0.99 0.99
Learning Rate 1×10−31×10−33×10−43×10−4
Batch Size 128 64 64 256
Buffer Size 1061024 2048 106
Value Loss MSE MSE AVEC (Flet-Berliac et al., 2021) MSE
Use gSDE No No No Yes
Entropy Coef. - 0 0 auto
GAEλ - 0.95 0.95 -
Advantage Norm. - Yes Yes -
Target Smoothing 0.005 - - 0.005
Learning Starts 104- - 104
Policy Frequency 32 - - -
Exploration Noise 0.1 - - -
Noise Clip 0.5 - - -
Max KL - 10−2- -
Line Search Steps - 10 - -
CG Steps - 100 - -
CG Damping - 10−2- -
CG Tolerance - 10−10- -
LR Schedule - - Linear to 0 -
Clipϵ - - 0.2 -
PPO Epochs - - 10 -
Value Coef. - - 0.5 -
Train Freq. - - - 1 step
Gradient Steps - - - 1
Table 4: Hyperparameters used for the MuJoCo experiments.
In this section, we detail the experimental setup of the MuJoCo experiments, as well as adding new plots.
Hyperparameters. Table 4 lists the hyperparameters used for each Deep RL agent on the MuJoCo
benchmark. Each agent is trained during 106interactions with its environment in the cases of HalfCheetah-
v3, Hopper-v3, andWalker2d-v3. Itistrainedduring 2.106interactionsinthecasesofAnt-v3andHumanoid-
v3. In all cases, a training episode is made of no more than 103interactions.
Scores. According to algorithm 3, after each agent is trained, the resulting policy performs 50 evaluations
episodes. The score of the agent is the mean performance on these 50 episodes.
Learning curves. Fig. 11 presents sample efficiency curves for all algorithms in each environment. The
shaded areas represent 95%bootstrapped confidence intervals, computed using rliable (Agarwal et al.,
2021). Note that each curve may be an aggregation of a different number of scores. The number of scores
can be found in the bottom right table of Fig. 10.
Additional comparison plots. Fig. 10 expands upon the comparisons given in the main text (in Fig. 5)
by also plotting the score distributions of each agent using boxplots.
36Published in Transactions on Machine Learning Research (08/2024)
DDPG
TRPO
PPO
SAC →≤↓→≤↓→≤↓
→≥↓  →≤↓→≤↓
→≥↓→≥↓  →≤↓
→≥↓→≥↓→≥↓  Ant-v3
 →≥↓→≥↓→≤↓
→≤↓  →≥↓→≤↓
→≤↓→≤↓  →≤↓
→≥↓→≥↓→≥↓  HalfCheetah-v3
 →≤↓→≤↓→≤↓
→≥↓  →≤↓→≤↓
→≥↓→≥↓  →=↓
→≥↓→≥↓→=↓  Hopper-v3
DDPG TRPO PPO SAC
0200040006000DDPG TRPO PPO SAC
25005000750010000DDPG TRPO PPO SAC
01000200030004000
DDPG
TRPO
PPO
SAC →≤↓→≤↓→≤↓
→≥↓  →≤↓→≤↓
→≥↓→≥↓  →≤↓
→≥↓→≥↓→≥↓  Humanoid-v3
 →≤↓→≤↓→≤↓
→≥↓  →≤↓→≤↓
→≥↓→≥↓  →≤↓
→≥↓→≥↓→≥↓  Walker2d-v3
DDPG TRPO PPO SAC
0200040006000
DDPG TRPO PPO SAC
020004000
Ant
HalfCheetahHopper
HumanoidWalker2dDDPG TRPO PPO SAC15 25 30 15 25
15 25 30 15 25
15 25 30 15 10
15 5 30 15 10Used Budget
Figure 10: AdaStop decision tables (top) and score distributions (bottom) for each MuJoCo environment,
and the budget used to make these decisions (bottom right). The medians are represented by green triangles
and the means by horizontal orange lines.
0.5 1.0 1.5 2.0
Time Steps1e60246Mean of Evaluation Returns1e3 Ant-v3
0.2 0.4 0.6 0.8 1.0
Time Steps1e60.00.20.40.60.81.0Mean of Evaluation Returns1e4 HalfCheetah-v3
0.2 0.4 0.6 0.8 1.0
Time Steps1e60123Mean of Evaluation Returns1e3 Hopper-v3
0.5 1.0 1.5 2.0
Time Steps1e60123456Mean of Evaluation Returns1e3 Humanoid-v3
0.2 0.4 0.6 0.8 1.0
Time Steps1e6012345Mean of Evaluation Returns1e3 Walker2d-v3SAC DDPG PPO TRPO
Figure 11: Mean of scores with 95% stratified bootstrap confidence intervals. Note that the curves in one
figure may use a different number of scores, depending on when AdaStop made the decisions.
H.3 Additional plot for Section 5.2
To illustrate Section 5.2, Figure 12 represents the kernel density estimation of the distribution of scores from
(Colas et al., 2019).
37Published in Transactions on Machine Learning Research (08/2024)
2500
 0 2500 5000 7500 10000 12500 15000
evaluations means0.00000.00010.00020.00030.00040.0005DensitySAC
TD3
Figure 12: Distributions of scores for a SAC agent and a TD3 agent on HalfCheetah obtained with 192
independent scores. Each score is obtained on an agent that has been trained during 2.106interactions.
38