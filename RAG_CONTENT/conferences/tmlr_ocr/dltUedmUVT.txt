Published in Transactions on Machine Learning Research (04/2024)
Temporal Difference Learning with Compressed Updates:
Error-Feedback meets Reinforcement Learning
Aritra Mitra amitra2@ncsu.edu
Department of Electrical and Computer Engineering
North Carolina State University
George J. Pappas pappasg@seas.upenn.edu
Department of Electrical and Systems Engineering
University of Pennsylvania
Hamed Hassani hassani@seas.upenn.edu
Department of Electrical and Systems Engineering
University of Pennsylvania∗
Reviewed on OpenReview: https: // openreview. net/ forum? id= dltUedmUVT
Abstract
In large-scale distributed machine learning, recent works have studied the effects of compress-
ing gradients in stochastic optimization to alleviate the communication bottleneck. These
works have collectively revealed that stochastic gradient descent (SGD) is robust to struc-
tured perturbations such as quantization, sparsification, and delays. Perhaps surprisingly,
despite the surge of interest in multi-agent reinforcement learning, almost nothing is known
about the analogous question: Are common reinforcement learning (RL) algorithms also
robust to similar perturbations? We investigate this question by studying a variant of the
classical temporal difference (TD) learning algorithm with a perturbed update direction,
where a general compression operator is used to model the perturbation. Our work makes
three important technical contributions. First, we prove that compressed TD algorithms,
coupled with an error-feedback mechanism used widely in optimization, exhibit the same
non-asymptotic theoretical guarantees as their SGD counterparts. Second, we show that our
analysis framework extends seamlessly to nonlinear stochastic approximation schemes that
subsume Q-learning. Third, we prove that for multi-agent TD learning, one can achieve linear
convergence speedups with respect to the number of agents while communicating just ˜O(1)
bits per iteration. Notably, these are the first finite-time results in RL that account for general
compression operators and error-feedback in tandem with linear function approximation and
Markovian sampling. Our proofs hinge on the construction of novel Lyapunov functions that
capture the dynamics of a memory variable introduced by error-feedback.
1 Introduction
Stochastic gradient descent (SGD) is at the heart of large-scale distributed machine learning paradigms such
as federated learning (FL) (Konečn` y et al., 2016). In these applications, the task of training high-dimensional
weight vectors is distributed among several workers that exchange information over networks of limited
bandwidth. While parallelization at such an immense scale helps to reduce the computational burden, it
creates several other challenges: delays, asynchrony, and most importantly, a significant communication
bottleneck. The popularity and success of SGD can be attributed in no small part to the fact that it
is extremely robustto such deviations from ideal operating conditions. In fact, by now, there is a rich
∗This work was supported by NSF Award 1837253, NSF CAREER award CIF 1943064, and The Institute for Learning-enabled
Optimization at Scale (TILOS), under award number NSF-CCF-2112665.
1Published in Transactions on Machine Learning Research (04/2024)
literature that analyzes the robustness of SGD to a host of structured perturbations that include lossy
gradient-quantization (Seide et al., 2014; Alistarh et al., 2017) and sparsification (Wen et al., 2017; Stich
et al., 2018). For instance, SignSGD - a variant of SGD where each coordinate of the gradient is replaced
by its sign - is extensively used to train deep neural networks (Aji & Heafield, 2017; Bernstein et al., 2018).
Inspired by these findings, in this paper, we ask a different question: Are common reinforcement learning
(RL) algorithms also robust to similar structured perturbations?
Motivation. Perhaps surprisingly, despite the recent surge of interest in multi-agent/federated RL, almost
nothing is known about the above question. To fill this void, we initiate the study of a robustness theory for
iterative RL algorithms with compressed update directions . We primarily focus on the problem of evaluating
the value function associated with a fixed policy µin a Markov decision process (MDP). Just as SGD is the
workhorse of stochastic optimization, the classical temporal difference (TD) learning algorithm (Sutton, 1988)
for policy evaluation forms the core subroutine in a variety of decision-making algorithms in RL (e.g., Watkin’s
Q-learning algorithm). In fact, in their book (Sutton & Barto, 2018), Sutton and Barto mention: “If one had
to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference
(TD) learning." Thus, it stands to reason that we center our investigation around a variant of the TD(0)
learning algorithm with linear function approximation, where the TD(0) update direction is replaced by a
compressed version of it. Moreover, to account for general biasedcompression operators (e.g., sign and
Top-k), we couple this scheme with an error-feedback mechanism that retains some memory of TD(0) update
directions from the past.
Other than the robustness angle, a key motivation of our work is to design communication-efficient algorithms
for the emerging paradigms of multi-agent RL (MARL) and federated RL (FRL). In existing works on
these topics (Doan et al., 2019; Qi et al., 2021; Jin et al., 2022; Khodadadian et al., 2022), agents typically
exchange high-dimensional models (parameters) or model-differentials (i.e., gradient-like update directions)
overlow-bandwidth channels , keeping their personal data (i.e., rewards, states, and actions) private. In the
recent survey paper on FRL by (Qi et al., 2021), the authors explain how the above issues contribute to
a major communication bottleneck, just as in the standard FL setting. However, no work on MARL or
FRL provides any theory whatsoever when it comes to compression/quantization in RL . Our work takes a
significant step towards filling this gap via a suite of comprehensive theoretical results that we discuss below.
1.1 Our Contributions
The main contributions of this work are as follows.
1.Algorithmic Framework for Compressed Stochastic Approximation. We develop a general
framework for analyzing iterative compressed stochastic approximation (SA) algorithms with error-
feedback. The generality of this framework stems from two salient features: (i) it applies to nonlinear
SA with Markovian noise; and (ii) the compression operator covers several common quantization
and (biased) sparsification schemes studied in optimization. As an instance of our framework, we
propose a new algorithm called EF-TD(Algorithm 1) to study extreme distortions to the TD(0) update
direction. Examples of such distortion include, but are not limited to, replacing each coordinate
of the TD(0) update direction with just its sign, or just retaining the coordinate with the largest
magnitude. While such distortions have been extensively studied for SGD, we are unaware of any
analagous algorithms or analysis in the context of RL .
2.Analysis under Markovian Sampling. In Theorem 1, we show that with a constant step-
size, EF-TDguarantees linear convergence to a ball centered around the optimal parameter. Up
to a compression factor, our result mirrors existing rates in RL without compression (Srikant &
Ying, 2019). Moreover, the effect of the compression factor exactly mirrors that for compressed
SGD (Beznosikov et al., 2020). The significance of this result is twofold: (i) It is the first result in
RL that accounts for general compression operators and error-feedback in tandem with Markovian
sampling and linear function approximation; and (ii) It is the first theoretical result on the robustness
of TD learning algorithms to structured perturbations. One interesting takeaway from Theorem 1 is
that“slowly-mixing" Markov chains have more inherent robustness to distortions/compression . This
appears to be a new observation that we elaborate on in Section 4.
2Published in Transactions on Machine Learning Research (04/2024)
3.Analysis for General Nonlinear Stochastic Approximation. In Section 5, we study Algorithm 1
in its full generality by considering a compressed nonlinear SA scheme with error-feedback, and
establishing an analog of Theorem 1 in Theorem 2. The significance of this result lies in revealing
that the power and scope of the popular error-feedback mechanism (Seide et al., 2014) in large-scale
ML is not limited to the optimization problems it has been used for thus far. In particular, since
the nonlinear SA scheme we study captures certain instances of Q-learning (Chen et al., 2019),
Theorem 2 conveys that our results extend well beyond policy evaluation to decision-making (control)
problems as well.
4.Communication-Efficient MARL. Since one of our main goals is to facilitate communication-
efficient MARL, we consider a collaborative MARL setting that has shown up in several recent
works (Doan et al., 2019; Khodadadian et al., 2022; Liu & Olshevsky, 2021a). Here, Magents interact
with the sameMDP, but observe potentially different realizations of rewards and state transitions.
These agents exchange compressed TD update directions via a central server to speed up the process
of evaluating a specific policy. In this context, we ask the following fundamental question: How
much information needs to be transmitted to achieve the optimal linear-speedup (w.r.t. the number of
agents) for policy-evaluation? To answer this question, we propose a multi-agent version of EF-TD.
In Theorem 3, we prove that by collaborating, each agent can achieve an M-fold speedup in the
dominant term of the convergence rate. Importantly, the effect of compression only shows up in
higher-order terms, leaving the dominant term unaffected. Thus, we prove that by transmitting
just ˜O(1)bits per-agent per-round, one can preserve the same asymptotic rates as vanilla
TD(0), while achieving an optimal linear convergence speedup w.r.t. the number of
agents. We envision this result will have important implications for MARL. Our analysis also leads
to atighterlinear dependence on the mixing time compared to the quadratic dependence in the only
other paper that establishes linear speedups under Markovian sampling (Khodadadian et al., 2022).
5.Technical Challenges and Novel Proof Techniques. One might wonder whether our results
above follow as simple extensions of known analysis techniques in RL. In what follows, we explain
why this isn’t the case by outlining the key technical challenges uniqueto our setup, and our novel
proof ideas to overcome them. First, a non-asymptotic analysis of even vanilla TD(0) under Markovian
sampling is known to be extremely challenging due to complex temporal correlations. Our setting
is further complicated by the fact that the dynamics of the parameter andthe memory variable
are intricately coupled with the temporally-correlated Markov data tuples. This leads to a complex
stochastic dynamical system that has not been analyzed before in RL. Second, we cannot directly
appeal to existing proofs of compression in optimization since the problems we study do not involve
minimizing a static loss function. Moreover, the aspect of temporally correlated observations is
completely absent in compressed optimization since one deals with i.i.d. data. The above discussion
motivates the need for new analysis tools beyond what are known for both RL and optimization.
•New Proof Ingredients for Theorems 1 and 2. To prove Theorem 1, our first innovation is to
construct a novel Lyapunov function that accounts for the joint dynamics of the parameter and the
memory variable introduced by error-feedback. The next natural step is to then analyze the drift of
this Lyapunov function by appealing to mixing time arguments - as is typically done in finite-time
RL proofs (Bhandari et al., 2018; Srikant & Ying, 2019). This is where we again run into difficulties
since the presence of the memory variable due to error-feedback introduces non-standard delay terms
in the drift bound. Notably, this difficulty does not show up when one analyzes error-feedback in
optimization since there is no need for mixing time arguments of Markov chains. To overcome this
challenge, we make a connection to what might at first appear unrelated: the Incremental Aggregated
Gradient (IAG) method for finite-sum optimization. Our main observation here is that the elegant
analysis of the IAGmethod by Gurbuzbalaban et al. (2017) shows how one can handle the effect
of “shocks" (delayed terms), as long as the amplitude and duration of the shocks is not too large.
This ends up being precisely what we need for our purpose: we carefully relate the amplitude of the
shocks to our Lyapunov function, and their duration to the mixing time of the underlying Markov
chain. We spell out these details explicitly in Section 7.
3Published in Transactions on Machine Learning Research (04/2024)
•New Proof Ingredients for Theorem 3. Despite the long list of papers on MARL, the only
one that establishes a linear speedup (w.r.t. the number of agents) in sample-complexity under
Markovian sampling is the very recent paper by Khodadadian et al. (2022); all other papers end up
making a restrictive i.i.d. sampling assumption (Doan et al., 2019; Liu & Olshevsky, 2021a; Shen
et al., 2023) to show a speedup. The proof in (Khodadadian et al., 2022) is quite involved, and
relies on Generalized Moreau Envelopes. This makes it particularly challenging to extend their proof
framework to our setting where we need to additionally contend with the effects of compression
and error-feedback. As such, we provide an alternate proof technique for establishing the linear
speedup effect that relies crucially on a new variance reduction lemma under Markovian data, namely
Lemma 2. This lemma is not just limited to TD learning, and may be of independent interest to the
MARL literature. Lemma 2, coupled with a finer Lyapunov function (relative to that for proving
Theorem 1) enable us to establish the desired linear speedup result under Markovian sampling for
our setting. We elaborate on these points in Section 7.
Scope of our Work. We note that our work is primarily theoretical in nature, in the same spirit as several
recent finite-time RL papers (Dalal et al., 2018; Bhandari et al., 2018; Srikant & Ying, 2019; Doan et al., 2019;
Khodadadian et al., 2022), none of which come with any simulations. We do, however, report simulations on
moderately sized (100 states) MDPs that reveal: (i) without error-feedback (EF), compressed TD(0) can end
up making little to no progress ; and (ii) with EF, the performance of compressed TD(0) complies with our
theory. These simulations are in line with those known in optimization for deep learning (Stich et al., 2018;
Aji & Heafield, 2017; Lin et al., 2017; Stich & Karimireddy, 2019), where one empirically observes significant
benefits of performing EF. Succinctly, we convey: compressed TD learning algorithms with EF are just as
robust as their SGD counterparts, and exhibit similar convergence guarantees .
1.2 Related Work
In what follows, we discuss the most relevant threads of related work.
•Analysis of TD Learning Algorithms . An analysis of the classical temporal difference learning
algorithm (Sutton, 1988) with value function approximation was first provided by Tsitsiklis & Van Roy
(1997). They employed the ODE method (Borkar & Meyn, 2000) - commonly used to study stochastic
approximation algorithms - to provide an asymptotic convergence rate analysis of TD learning algorithms.
Finite-time rates for such algorithms remained elusive for several years, till the work by Korda & La (2015).
Soon after, Narayanan & Szepesvári (2017) noted some issues with the proofs in (Korda & La, 2015).
Under an i.i.d. sampling assumption, Lakshminarayanan & Szepesvári (2017) and Dalal et al. (2018) went
on to provide finite-time rates for TD learning algorithms. For the more challenging Markovian setting,
finite-time rates have been recently derived using various perspectives: (i) by making explicit connections
to optimization (Bhandari et al., 2018); (ii) by taking a control-theoretic approach and studying the drift
of a suitable Lyapunov function (Srikant & Ying, 2019); and (iii) by arguing that the mean-path temporal
difference direction acts as a “gradient-splitting" of an appropriately chosen function (Liu & Olshevsky,
2021b). Each of these interpretations provides interesting new insights into the dynamics of TD algorithms.
The above works focus on the vanilla TD learning rule. Our work adds to this literature by providing an
understanding of the robustness of TD learning algorithms subject to structured distortions.
•Communication-Efficient Algorithms for (Distributed) Optimization and Learning. In the last
decade or so, a variety of both scalar (Seide et al., 2014; Wen et al., 2017; Bernstein et al., 2018; Alistarh
et al., 2017), and vector (Mayekar & Tyagi, 2020; Gandikota et al., 2021) quantization schemes have been
explored for optimization/empirical risk minimization. In particular, an aggressive compression scheme
employed popularly in deep learning is gradient sparsification, where one only transmits a few components of
the gradient vector that have the largest magnitudes. While empirical studies (Aji & Heafield, 2017; Lin
et al., 2017) have revealed the benefits of extreme sparsification, theoretical guarantees for such methods
are much harder to establish due to their biasednature: the output of the compression operator is not an
unbiased version of its argument. In fact, for biased schemes, naively compressing gradients can lead to
diverging iterates (Karimireddy et al., 2019; Beznosikov et al., 2020). In (Stich et al., 2018; Alistarh et al.,
2018), and later in (Karimireddy et al., 2019; Lin et al., 2022), it was shown that the above issue can be
4Published in Transactions on Machine Learning Research (04/2024)
“fixed" by using a mechanism known as error-feedback that exploits memory. This idea is also related to
modulation techniques in coding theory (Gray, 2012). Recently, Beznosikov et al. (2020) and Gorbunov et al.
(2020) provided theoretical results on biased sparsification for a master-worker type distributed architecture,
and Mitra et al. (2021) studied sparsification in a federated learning context. For more recent work on the
error-feedback idea, we refer the reader to (Richtárik et al., 2021; Gruntkowska et al., 2022).
Our work can be seen as the first analog of the above results in general, and the error-feedback idea in
particular, in the context of iterative algorithms for RL.
Finally, we note that while several compression algorithms have been proposed and analyzed over the years,
fundamental performance bounds were only recently identified by Mayekar & Tyagi (2020), Gandikota et al.
(2021), and Lin et al. (2022). Computationally efficient algorithms that match such performance bounds were
developed by Saha et al. (2022). While all the above results pertain to static optimization, some recent works
have also explored quantization schemes in the context of sequential-decision making problems, focusing on
multi-armed bandits (Hanna et al., 2022; Mitra et al., 2022; Pase et al., 2022).
2 Model and Problem Setup
We consider a Markov Decision Process (MDP) denoted by M= (S,A,P,R,γ), whereSis a finite state
space of size n,Ais a finite action space, Pis a set of action-dependent Markov transition kernels, Ris a
reward function, and γ∈(0,1)is the discount factor. For the majority of the paper, we will be interested in
thepolicy evaluation problem where the goal is to evaluate the value function Vµof a given policy µ; here,
µ:S→A. The policy µinduces a Markov reward process (MRP) characterized by a transition matrix P,
and a reward function R.1In particular, P(s,s′)denotes the probability of transitioning from state sto state
s′under the action µ(s);R(s)denotes the expected instantaneous reward from an initial state sunder the
action of the policy µ. The discounted expected cumulative reward obtained by playing policy µstarting
from initial state sis given by:
Vµ(s) =E/bracketleftigg∞/summationdisplay
t=0γtR(st)|s0=s/bracketrightigg
, (1)
wherestrepresents the state of the Markov chain (induced by µ) at timet, when initiated from s0=s. It is
well-known (Tsitsiklis & Van Roy, 1997) that Vµis the fixed point of the policy-specific Bellman operator
Tµ:Rn→Rn, i.e.,TµVµ=Vµ, where for any V∈Rn,
(TµV)(s) =R(s) +γ/summationdisplay
s′∈SP(s,s′)V(s′),∀s∈S.
Linear Function Approximation. In practice, the size of the state space Scan be extremely large. This
renders the task of estimating Vµexactly(based on observations of rewards and state transitions) intractable.
One common approach to tackle this difficulty is to consider a parametric approximation ˆVθofVµin the
linear subspace spanned by a set {ϕk}k∈[K]ofK≪nbasis vectors, where ϕk= [ϕk(1),...,ϕk(n)]⊤∈Rn.
Specifically, we have ˆVθ(s) =/summationtextK
k=1θ(k)ϕk(s),whereθ= [θ(1),...,θ (K)]⊤∈RKis a weight vector. Let
Φ∈Rn×Kbe a matrix with ϕkas itsk-th column; we will denote the s-th row of Φbyϕ(s)∈RK, and refer
to it as the feature vector corresponding to state s. Compactly, ˆVθ= Φθ, and for each s∈S, we have that
ˆVθ(s) =⟨ϕ(s),θ⟩. As is standard, we assume that the columns of Φare linearly independent, and that the
feature vectors are normalized, i.e., for each s∈S,∥ϕ(s)∥2
2≤1.
The TD(0) Algorithm. The goal now is to find the best parameter vector θ∗that minimizes the distance
(in a suitable norm) between ˆVθandVµ. To that end, we will focus on the classical TD(0) algorithm within
the family of TD learning algorithms. Starting from an initial estimate θ0, at each time-step t= 0,1,..., this
algorithm receives as observation a data tuple Xt= (st,st+1,rt=R(st))comprising of the current state,
the next state reached by playing action µ(st), and the instantaneous reward rt. Given this tuple Xt, let us
1For simplicity of notation, we have dropped the dependence of PandRon the policy µ.
5Published in Transactions on Machine Learning Research (04/2024)
Figure 1: A geometric interpretation of the operator Qδ(·)in Algorithm 1. A larger δinduces more distortion.
Algorithm 1 The EF-TDAlgorithm
1:Input:Initial estimate θ0∈RK, initial error e−1= 0, and step-size α∈(0,1).
2:fort= 0,1,...do
3:Observe tuple Xt= (st,st+1,rt).
4:Compute perturbed TD(0) direction ht:
ht=Qδ(et−1+gt(θt)). (3)
5:Update parameter: θt+1=θt+αht.
6:Update error: et=et−1+gt(θt)−ht.
7:end for
definegt(θ) =g(Xt,θ)as:
gt(θ)≜(rt+γ⟨ϕ(st+1),θ⟩−⟨ϕ(st),θ⟩)ϕ(st),∀θ∈RK.
The TD(0) update to the current parameter θtwith step-size αt∈(0,1)can now be described succinctly as:
θt+1=θt+αtgt(θt). (2)
Under some mild technical conditions, it was shown in (Tsitsiklis & Van Roy, 1997) that the iterates generated
byTD(0) converge almost surely to the best linear approximator in the span of {ϕk}k∈[K]. In particular,
θt→θ∗, whereθ∗is the unique solution of the projected Bellman equation ΠDTµ(Φθ∗) = Φθ∗. Here,Dis a
diagonal matrix with entries given by the elements of the stationary distribution πof the kernel P. Moreover,
ΠD(·)is the projection operator onto the subspace spanned by {ϕk}k∈[K]with respect to the inner product
⟨·,·⟩D. The key question we explore in this paper is: What can be said of the convergence of TD(0) when
the update direction gt(θt)in equation 2 is replaced by a distorted/compressed version of it? In the next
section, we will make the notion of distortion precise, and outline the various technical challenges that make
it non-trivial to answer the above question.
3 Error-Feedback meets TD Learning
In this section, we propose a general framework for analyzing TD learning algorithms with distorted update
directions. Extreme forms of such distortion could involve replacing each coordinate of the TD(0) direction
with just its sign, or retaining just k∈[K]of the largest magnitude coordinates and zeroing out the rest. In
the sequel, we will refer to these variants of TD(0) as SignTD(0) and Topk-TD(0), respectively. Coupled with
a memory mechanism known as error-feedback, it is known that analogous variants of SGDexhibit, almost
remarkably, the same behavior as SGDitself (Stich et al., 2018). Inspired by these findings, we ask: Does
compressed TD(0) with error-feedback exhibit behavior similar to that of TD(0)?
Our motivation behind studying the above question is to build an understanding of: (i) communication-efficient
versions of MARL algorithms where agents exchange compressed model-differentials, i.e., the gradient-like
updates (see Section 6); and (ii) the robustness of TD learning algorithms to structured perturbations. It is
6Published in Transactions on Machine Learning Research (04/2024)
important to reiterate here that our motivation mirrors that of studying perturbed versions of SGDin the
context of optimization.
Description of EF-TD.In Algorithm 1, we propose the compressed TD(0) algorithm with error-feedback
(EF-TD). Compared to equation 2, we note that the parameter θtis updated based on ht- a compressed
version of the actual TD(0) direction gt(θt), where the compression is due to the operator Qδ:RK→RK.
The information lost up to time t−1due to compression is accumulated in the memory variable et−1, and
injected back into the current step as in equation 3. In line with compressors used for optimization, we
consider a fairly general operator Qδthat is required to only satisfy the following contraction property for
someδ≥1:
∥Qδ(θ)−θ∥2
2≤/parenleftbigg
1−1
δ/parenrightbigg
∥θ∥2
2,∀θ∈RK. (4)
A distortion perspective. Forθ̸= 0, it is easy to verify based on equation 4 that ⟨Qδ(θ),θ⟩≥1/(2δ)∥θ∥2
2>
0, i.e., theanglebetween Qδ(θ)andθisacute. Thisprovidesanalternativeviewtothecompressionperspective:
one can think of Qδ(θ)as a tilted version of θ, with a larger δimplying more tilt, and δ= 1implying no
distortion at all. See Figure 1 for a visual interpretation of this point.
The contraction property in equation 4 is satisfied by several popular quantization/sparsification schemes.
These include the sign operator, and the Top- koperator that selects kcoordinates with the largest magnitude,
zeroing out the rest. Importantly, note that the operator Qδdoes not necessarily yield an unbiased version of
its argument. In optimization, error-feedback serves to counter the bias introduced by Qδ. In fact, without
error-feedback, algorithms like SignSGD can converge very slowly, or not converge at all (Karimireddy et al.,
2019). In a similar spirit, we empirically observe in Fig. 2 (later in Section 8) that without error-feedback,
SignTD(0) can end up making little to no progress towards θ∗. However, understanding whether error-feedback
guarantees convergence to θ∗is quite non-trivial. We now provide some intuition as to why this is the case.
Need for Technical Novelty. Error-feedback ensures that past (pseudo)-gradient information is injected
with some delay. Thus, for optimization, error-feedback essentially leads to a delayed SGD algorithm. As
long as the objective function is smooth, the gradient does not change much, and the delay-effect can be
controlled. This intuition does not carry over to our setting since the TD(0) update direction is not a stochastic
gradient of any fixed objective function .2Thus, controlling the effect of the compressor-induced delay requires
new techniques for our setting. Moreover, unlike the SGD noise, the data tuples {Xt}are part of the same
Markov trajectory, introducing further complications. Finally, since the parameter updates of Algorithm 1
are intricately linked with the error variable et, to analyze their joint behavior, we need to perform a more
refined Lyapunov drift analysis relative to the standard TD(0) analysis. Despite these challenges, in the sequel,
we will establish that EF-TDretains almost the same convergence guarantees as vanilla TD(0).
Remark 1. It is important to emphasize that even though the error vector etinEF-TDmight be dense, et
never gets transmitted. The communication-efficient aspect of EF-TDstems from the fact that it only requires
communicating the compressed update direction ht- a vector that can be represented by using just a few bits
(depending on the level of compression). This fact is further clarified in our description of the multi-agent
version of EF-TDin Section 6 (see line 6 of Algorithm 2 in Section 6).
4 Analysis of EF-TDunder Markovian Sampling
The goal of this section is to provide a rigorous finite-time analysis of EF-TDunder Markovian sampling. To
that end, we need to introduce a few concepts and make certain standard assumptions. We start by assuming
that all rewards are uniformly bounded by some ¯r>0, i.e.,|R(s)|≤¯r,∀s∈S. This ensures that the value
functions exist. Next, we state a standard assumption that shows up in the finite-time analysis of iterative
RL algorithms (Bhandari et al., 2018; Srikant & Ying, 2019; Chen et al., 2019; Patil et al., 2023; Doan et al.,
2019; Khodadadian et al., 2022).
Assumption 1. The Markov chain induced by the policy µis aperiodic and irreducible.
2As observed by Bhandari et al. (2018) and Liu & Olshevsky (2021b), this can be seen from the fact that the derivative of
the TD update direction produces a matrix that is not necessarily symmetric, unlike the symmetric Hessian matrix of a fixed
objective function.
7Published in Transactions on Machine Learning Research (04/2024)
The above assumption implies that the Markov chain induced by µadmits a unique stationary distribution
π(Levin & Peres, 2017). Let Σ = Φ⊤DΦ. Since Φis full column rank, Σis full rank with a strictly
positive smallest eigenvalue ω<1. To appreciate the implication of the above assumption, let us define the
“steady-state" version of the TD update direction as follows: for a fixed θ∈RK, let
¯g(θ)≜Est∼π,st+1∼Pµ(·|st)[g(Xt,θ)].
We now introduce the notion of the mixing time τϵ.
Definition 1. Defineτϵ≜min{t≥1 :∥E[g(Xk,θ)|X0]−¯g(θ)∥2≤ϵ(∥θ∥2+ 1),∀k≥t,∀θ∈RK,∀X0}.
Assumption 1 implies that the total variation distance between the conditional distribution P(st=·|s0=s)
and the stationary distribution πdecays geometrically fast for all t≥0, regardless of the initial state
s∈S(Levin & Peres, 2017). As a consequence of this geometric mixing of the Markov chain, it is not hard to
show thatτϵin Definition 1 is O(log(1/ϵ)); see, for instance, Chen et al. (2019). The precision that suffices
for all results in this paper is ϵ=α2, where recall that αis the step-size. Henceforth, we will simply use τ
as a shorthand for τα. Finally, let us define σ≜max{1,¯r,∥θ∗∥2}as the variance of our noise model, and
dt≜∥θt−θ∗∥2. We can now state the first main result of this paper.
Theorem 1. Suppose Assumption 1 holds. There exist universal constants c,C≥1such that the iterates
generated by EF-TDwith step-size α≤ω(1−γ)
cmax{δ,τ}satisfy the following ∀T≥τ:
E/bracketleftbig
d2
T/bracketrightbig
≤C1/parenleftbigg
1−αω(1−γ)
Cτ/parenrightbiggT−τ
+O/parenleftbiggα(τ+δ)σ2
ω(1−γ)/parenrightbigg
,whereC1=O(d2
0+σ2). (5)
The proof of Theorem 1 is provided in Appendix D. We now discuss the key implications of this result.
Discussion. Theorem 1 tells us that EF-TDguarantees linear convergence of the iterates to a ball around
θ∗, where the size of the ball scales with the variance σ2; this exactly matches the behavior of vanilla
TD (Bhandari et al., 2018; Srikant & Ying, 2019). Since the step-size αscales inversely with the distortion
factorδ, we observe from equation 5 that the exponent of linear convergence gets slackened by δ; once again,
this is consistent with analogous results for SGD with (biased) compression (Beznosikov et al., 2020). The
variance term, namely the second term in equation 5, has the exact same dependence on τ,ω, andγas one
observes for vanilla TD (Bhandari et al., 2018, Theorem 3). Observe that in this term, the effects of τandδ
in inflating the variance are additive. Moreover, even in the absence of compression, the dependence of the
variance term on the mixing time τis known to be unavoidable (Nagaraj et al., 2020). This immediately
leads to the following interesting conclusion: when the underlying Markov chain induced by the policy mixes
“slowly", i.e., has a large mixing time τ, one can afford to be more aggressive in terms of compression, i.e.,
use a larger δ, since this would lead to a variance bound that is no worse than in the uncompressed setting.
Said differently, Theorem 1 reveals that slowly-mixing Markov chains have a higher tolerance to distortions.
This observation is novel since no prior work has studied the effect of distortions to RL algorithms under
Markovian sampling. It is also interesting to note here that the phenomenon described above shows up in
other contexts too: for instance, the authors in Gandikota et al. (2021) showed that certain quantization
mechanisms in optimization automatically come with privacy guarantees.
Theorem 1 is significant in that it is the first result to reveal that coupled with error-feedback,
EF-TDis robust to extreme distortions. We will corroborate this phenomenon empirically as well in
Section 8. The other key takeaway from Theorem 1 is that the scope of the error-feedback mechanism - now
popularly used in distributed learning - extends to stochastic approximation problems well beyond just static
optimization settings.
In Appendix B, we analyze a simpler steady-state version of EF-TDto help build intuition. There, we also
provide an analysis of compressed TD learning algorithms that do not employ any error-feedback mechanism.
Our analysis reveals that such schemes can also converge, provided the compression parameter δsatisfies a
restrictive condition. Notably, as evident from the statement of Theorem 1, we do not need to impose any
restrictions on δfor the convergence of EF-TD.
8Published in Transactions on Machine Learning Research (04/2024)
5 Compressed Nonlinear Stochastic Approximation with Error-Feedback
For the TD(0) algorithm, the update direction gt(θ)is affine in the parameter θ, i.e.,gt(θ)is of the form
A(Xt)θ−b(Xt). As such, the recursion in equation 2 can be seen as an instance of linear stochastic
approximation (SA), where the end goal is to use the data samples {Xt}to find aθthat solves the linear
equationAθ=b; here,Aandbare the steady-state versions of A(Xt)andb(Xt), respectively. The more
involved TD(λ) algorithms within the TD learning family also turn out to be instances of linear SA. Instead
of deriving versions of our results for TD(λ) algorithms in particular, we will instead consider a much more
general nonlinear SA setting. Accordingly, we now study a variant of Algorithm 1 where g(Xt,θ)is a general
nonlinear map, and as before, Xtcomes from a finite-state Markov chain that is assumed to be aperiodic and
irreducible. We assume that the nonlinear map satisfies the following regularity conditions.
Assumption 2. There exist L,σ≥1s.t. the following hold for any Xin the space of data tuples: (i)
∥g(X,θ 1)−g(X,θ 2)∥2≤L∥θ1−θ2∥2,∀θ1,θ2∈RK, and (ii)∥g(X,θ)∥2≤L(∥θ∥2+σ),∀θ∈RK.
Assumption 3. Let¯g(θ)≜EXt∼π[g(Xt,θ)],∀θ∈RK, whereπis the stationary distribution of the Markov
process{Xt}. The equation ¯g(θ) = 0has a solution θ∗, and∃β >0s.t.
⟨θ−θ∗,¯g(θ)−¯g(θ∗)⟩≤−β∥θ−θ∗∥2
2,∀θ∈RK. (6)
In words, Assumption 2 says that g(X,θ)is globally uniformly (w.r.t. X) Lipschitz in the parameter θ.
Assumption 3 is a strong monotone property of the map −¯g(θ)that guarantees that the iterates generated by
the steady-state version of equation 2 converge exponentially fast to θ∗. To provide some context, consider
theTD(0) setting. Under feature normalization and the bounded rewards assumption, the global Lipschitz
property is immediate. Moreover, Assumption 3 corresponds to negative-definiteness of the steady-state
matrixA=EXt∼π[A(Xt)]; this negative-definite property is also easy to verify (Srikant & Ying, 2019). For
optimization, Assumptions 2 and 3 simply correspond to L-smoothness and β-strong-convexity of the loss
function, respectively. For simplicity, we state the main result of this section for L= 2; the analysis for a
generalLfollows identical arguments.
Theorem 2. Suppose Assumption 2 holds with L= 2, and Assumption 3 holds. Let ¯β=min{β,1/β}. There
exist universal constants c,C≥1such that Algorithm 1 with step-size α≤¯β
cmax{δ,τ}guarantees
E/bracketleftbig
d2
T/bracketrightbig
≤C1/parenleftbigg
1−αβ
Cτ/parenrightbiggT−τ
+O/parenleftbiggα(τ+δ)σ2
β/parenrightbigg
,∀T≥τ,whereC1=O(d2
0+σ2). (7)
Discussion. We note that the guarantee in Theorem 2 mirrors that in Theorem 1, and represents our
setting in its full generality, accounting for nonlinear SA, Markovian noise, compression, and error-feedback.
Providing an explicit finite-time analysis for this setting is one of the main contributions of our paper. Now
let us comment on the applications of this result. As noted earlier, our result applies to TD(λ) algorithms
and SGD under Markovian noise (Doan, 2022); the effect of compression and error-feedback was previously
unknown for both these settings. More importantly, certain instances of Q-learning with linear function
approximation can also be captured via Assumptions 2 and 3 (Chen et al., 2019). The key implication is that
our analysis framework is not just limited to policy evaluation, but rather extends gracefully to decision-making
(control) problems. This speaks to the significance of Theorem 2.
6 Communication-Efficient Multi-Agent Reinforcement Learning (MARL)
A key motivation of our work is to develop communication-efficient algorithms for MARL. This is particularly
relevantfornetworked/federatedversionsofRLproblemswherecommunicationimposesamajorbottleneck(Qi
et al., 2021). To that end, we consider a collaborative MARL setting that has appeared in various recent
works (Qi et al., 2021; Doan et al., 2019; Liu & Olshevsky, 2021a; Jin et al., 2022; Khodadadian et al.,
2022; Shen et al., 2023). The setup comprises of Magents, all of whom interact with the sameMDP,
and can communicate via a central server. Every agent seeks to evaluate the samepolicyµ. The purpose
of collaboration is as in the standard FL setting: to achieve a M-fold reduction in sample-complexity by
leveraging information from all agents. In particular, we ask: By exchanging compressed information via the
9Published in Transactions on Machine Learning Research (04/2024)
Algorithm 2 Multi-Agent EF-TD
1:Input:Initial estimate θ0∈RK, initial errors ei,−1= 0,∀i∈[M], and step-size α∈(0,1).
2:fort= 0,1,...do
3:Server sends θtto all agents.
4:fori∈[M]do
5: Observe tuple Xi,t= (si,t,si,t+1,ri,t).
6: Compute compressed TD(0) direction hi,t=Qδ(ei,t−1+gi,t(θt)), and sendhi,tto server.
7: Update error: ei,t=ei,t−1+gi,t(θt)−hi,t.
8:end for
9:Server updates the model as follows: θt+1=θt+α¯ht, whereht= (1/M)/summationtext
i∈[M]hi,t.
10:end for
server, is it possible to expedite the process of learning by achieving a linear speedup in the number of agents?
While such questions have been extensively studied for supervised learning, we are unaware of any work that
addresses them in the context of MARL. We further note that even without compression or error-feedback,
establishing linear speedups under Markovian sampling is highly non-trivial, and the only other paper that
does so is the very recent work of Khodadadian et al. (2022). As we explain later in Section 7, our proof
technique departs significantly from (Khodadadian et al., 2022).
We propose and analyze a natural multi-agent version of EF-TD, outlined as Algorithm 2. In a nutshell,
multi-agent EF-TDoperates as follows. At each time-step, the server sends down a model θt; each agent i
observes a local data sample, computes and uploads the compressed direction hi,t, and updates its local error.
The server then updates the model. Transmitting compressed TD(0) pseudo-gradients is consistent with both
works in FL (Karimireddy et al., 2020), and in MARL (Qi et al., 2021; Doan et al., 2019; Khodadadian et al.,
2022), where the agents essentially exchange model-differentials (i.e., the update directions), keeping their
raw data private. It is worth noting here that while all agents play the same policy, the realizations of the
data tuples{Xi,t}may vary across agents. Let ˜θt=θt+α¯et−1, where ¯et= (1/M)/summationtext
i∈[M]ei,t, and define
˜dt≜∥˜θt−θ∗∥2. We can now state our main convergence result for Algorithm 2.
Theorem 3. Suppose Assumption 1 holds. There exist universal constants c,C≥1such that with step-size
α≤ω(1−γ)
cmax{δ,τ}, andC1=O(d2
0+σ2), Algorithm 2 guarantees the following ∀T≥2τ:
E/bracketleftbig˜d2
T/bracketrightbig
≤C1/parenleftbigg
1−αω(1−γ)
Cτ/parenrightbiggT−2τ
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T1+O/parenleftbiggατ
ω(1−γ)/parenrightbiggσ2
M/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T2+O/parenleftbiggα2max{δ,τ}δ
ω2(1−γ)2/parenrightbigg
σ2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T3. (8)
The proof of Theorem 3 is deferred to Appendix E. There are several key messages conveyed by Theorem 3.
We discuss them below.
Message 1: Linear Speedup. Observe that the noise variance terms in equation 8 are T2andT3, where
T3isO(α2), i.e., a higher-order term in α. Thus, for small α,T2is the dominant noise term. Compared
to the noise term for vanilla TD in (Bhandari et al., 2018, Theorem 3), T2in our bound has exactly the
same dependence on τ,ω, andγ, and importantly, exhibits an inverse scaling w.r.t. M,implying a M-fold
reduction in the variance σ2relative to the single agent setting. This is precisely what we wanted. For exact
convergence, we can set α=O(log(MT2)/T)to makeT1andT3of order ˜O(1/T2), and the dominant term
T2=˜O(σ2/(MT)). Thus, relative to the O(σ2/T)rate of TD(0),we achieve a linear speedup w.r.t. the
number of agents Min our dominant term T2.
Message 2: Communication-efficiency comes (nearly) for free. The second key thing to note is that
the dominant T2term iscompletely unaffected by the compression factor δ. Indeed, the compression factor δ
only affects higher-order terms that decay much faster than T2.This means that we can significantly ramp
upδ, thereby communicating very little, while preserving the asymptotic rate of convergence of TD(0) and
achieving optimal speedup in the number of agents, i.e., communication-efficiency comes almost for free. For
instance, suppose Qδ(·)is a top- 1operator, i.e., hi,thas only one non-zero entry which can be encoded using
10Published in Transactions on Machine Learning Research (04/2024)
˜O(1)bits. In this case, although δ=K, whereKis the potentially large number of features, the dominant
˜O/parenleftbig
σ2/(MT)/parenrightbig
term remains unaffected by K.Thus, in the context of MARL, our work is the first to
show that asymptotically optimal rates can be achieved by transmitting just ˜O(1)bits per-agent
per time-step .
Message 3: Tight Dependence on the Mixing Time. Compared to Khodadadian et al. (2022) - the only
other paper in MARL that provides a linear speedup under Markovian sampling - our dominant term T2has
a tighterO(τ)dependence on the mixing time τas compared to the O(τ2)dependence in Khodadadian et al.
(2022, Theorem4.1). Itshouldbenotedherethatthe O(τ)dependenceisknowntobeinformation-theoretically
optimal (Nagaraj et al., 2020).
A few remarks are in order before we end this section.
Remark 2. We note that the bound in Theorem 3 is stated for ˜θT, notθT. Since ˜θT=θT+α¯eT−1, to output
˜θT, the server needs to query ei,T−1from each agent ionly once at time T−1. We believe that this one
extra step of communication can also be avoided via a slightly sharper analysis. We provide such an analysis
in Appendix F, albeit under a common i.i.d. sampling model (Dalal et al., 2018; Doan et al., 2019; Liu &
Olshevsky, 2021a).
Remark 3. While our MARL result in Theorem 3 is for TD learning, in light of the developments in
Section 5, we note that one can develop analogs of Theorem 3 for multi-agent Q-learning as well.
Remark 4. Suppose we set M= 1in equation 8, and compare the resulting bound with that in equation 5.
While the effect of the distortion δshows up as a higher-order O(α2)term in the former, it manifests as
aO(α)term in the latter. The difference in these bounds can be attributed to the fact that we use a finer
Lyapunov function to prove Theorem 3 relative to that which we use to prove Theorem 1. We do so primarily
for the sake of exposition: the relatively simpler proof of Theorem 1 (compared to Theorem 3) helps build
much of the key intuition needed to understand the proof of Theorem 3.
7 Technical Challenges in Analysis and Overview of our Proof Techniques
We now discuss the novel steps in our analysis. Complete proof details are provided in the Appendix.
Proof Sketch for Theorem 1. Inspired by the perturbed iterate framework of Mania et al. (2015), our first
step is to define the perturbed iterate ˜θt=θt+αet−1. Simple calculations reveal: ˜θt+1=˜θt+αgt(θt). Notice
that this recursion is almost the same as equation 2, other than the fact that the TD(0) update direction is
evaluated at θtinstead of ˜θt. Thus, to analyze the above recursion, we need to account for the gap between
θtand˜θt, the cause of which is the memory variable et−1. Accordingly, our next key step is to construct a
novel Lyapunov function ψtthat captures the joint dynamics of˜θtandet−1:
ψt≜E[˜d2
t+α2∥et−1∥2
2],where ˜d2
t=∥˜θt−θ∗∥2
2. (9)
As far as we are aware, a potential function of the above form has not been analyzed in prior RL work. Our
goal now is to exploit the geometric mixing property in Assumption 1 to establish that ψtdecays over time
(up to noise terms). This is precisely where the intricate coupling between the parameter ˜θt, the memory
variableet, and the Markovian data tuples {Xt}makes the analysis quite challenging. Let us elaborate.
To exploit mixing-time arguments, we need to condition sufficiently into the past and bound drift terms of
the form∥˜θt−˜θt−τ∥2.This is where the coupling between ˜θtandetintroduces non-standard delay terms,
precluding the direct use of prior approaches in RL (Bhandari et al., 2018; Srikant & Ying, 2019; Chen et al.,
2019). This difficulty does not show up in compressed optimization since one deals with i.i.d. data, precluding
the need for mixing-time arguments. A workaround here is to employ a projection step (as in Bhandari et al.
(2018); Doan et al. (2019)) to simplify the analysis. This is not satisfying for two reasons: (i) as we show
in the Appendix, the simplification in the analysis comes at the cost of a sub-optimal dependence on the
distortionδ; and (ii) to project, one needs prior knowledge of the set containing θ∗; this turns out to be
unnecessary. Our key innovation here to bound the drift ∥˜θt−˜θt−τ∥is the following technical lemma.
Lemma 1. (Relating Drift to Past Shocks ) For EF-TD, the following is true ∀t≥τ:
E[∥˜θt−˜θt−τ∥2
2] =O(α2τ2) max
t−τ≤ℓ≤tψℓ+O(α2τ2σ2). (10)
11Published in Transactions on Machine Learning Research (04/2024)
100101102103104101102103104
100101102103104101102103104
Figure 2: Plots of the mean-squared error Et=∥θT−θ∗∥2
2for vanilla TD(0) without compression, and
SignTD(0) with ( EF-SignTD ) and without ( SignTD) error-feedback. ( Left) Discount factor γ= 0.5. (Right)
Discount factor γ= 0.9.
The above lemma relates the drift to damped“shocks" (delay terms) from the past, where (i) the amplitude
of the shocks is captured by our Lyapunov function; (ii) the duration of these shocks is the mixing time
τ; and (iii) the damping arises from the fact that shocks are scaled by α2. In attempting to overcome one
difficulty, we have created another for ourselves via the “max" term in equation 10. This is where we make a
connection to the analysis of the IAGalgorithm in Gurbuzbalaban et al. (2017) where a similar “max" term
shows up. Via this connection, we are able to analyze the following final recursion:
ψt+1≤(1−cαω(1−γ))ψt+O(α2τ) max
t−τ≤ℓ≤tψℓ+O(α2)(τ+δ)σ2,wherec<1. (11)
Proof Sketch for Theorem 3. To establish the linear speedup property, we need a finer Lyapunov function
(and much finer analysis) relative to Theorem 1. Accordingly, we construct:
Ξt≜E/bracketleftbig
∥˜θt−θ∗∥2
2/bracketrightbig
+Cα3Et−1,whereEt≜1
ME/bracketleftiggM/summationdisplay
i=1∥ei,t∥2
2/bracketrightigg
, (12)
andCis a suitable design parameter. Using the techniques in Bhandari et al. (2018), Srikant & Ying (2019),
and Chen et al. (2019) to bound ∥gi,t(θ)∥2unfortunately do not yield the desired linear speedup. Moreover,
it is unclear whether the Generalized Moreau Envelope framework in Khodadadian et al. (2022) can be
extended to analyze equation 12. As such, we depart from these works by establishing the following key
result by carefully exploiting the geometric mixing property.
Lemma 2. Letzt(θt)≜(1/M)/summationtext
i∈[M]gi,t(θt). Under Assumption 1, the following bound holds for Algo-
rithm 2: E[∥zt(θt)∥2
2] =O(1)E[˜d2
t] +O(α2)Et−1+O(1/M+α4)σ2,∀t≥τ.
The above norm bound on the average TD direction turns out to be the main ingredient in bounding the
drift terms for Algorithm 2, and eventually establishing a recursion of the form in equation 11 for Ξt.
8 Simulations
To corroborate our theory, we construct a MDP with 100states, and use a feature matrix ΦwithK= 10
independent basis vectors. Using this MDP, we generate the state transition matrix Pµand reward vector Rµ
associated with a fixed policy µ. We compare the performance of the vanilla uncompressed TD(0) algorithm
with linear function approximation to SignTD(0) with and without error-feedback. We perform 30independent
trials, and average the errors from each of these trials to report the mean-squared error Et=∥θT−θ∗∥2
2for
each of the algorithms mentioned above. The results of this experiment are reported in Fig. 2. We observe
that in the absence of error-feedback, SignTD(0) can make little to no progress towards θ∗. In contrast, the
behavior of SignTD(0) with error-feedback almost exactly matches that of vanilla uncompressed TD(0). Our
12Published in Transactions on Machine Learning Research (04/2024)
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000051015202530
Figure 3: Plot of the mean-squared error Et=∥θT−θ∗∥2
2forEF-TD(Algo. 1), withQδ(·)chosen to be the
top-koperator. We study the effect of varying the number of components transmitted k.
10010110210310410-310-210-1100101
10010110210310410-510-410-310-210-1100101
Figure 4: Plots of the mean-squared error Et=∥θT−θ∗∥2
2for multi-agent EF-TD(Algorithm 2). ( Left)Qδ(·)
is the sign operator. ( Right)Qδ(·)is the Top-koperator with k= 2.
results align with similar observations made in the context of optimization, where SignSGD with error-feedback
retains almost the same behavior as SGDwith no compression (Stich et al., 2018; Karimireddy et al., 2019).
We provide some additional experiments below.
•Simulation of Topk-TD(0) with Error-Feedback . We simulate the behavior of EF-TDwith the operator
Qδ(·)chosen to be a top- koperator. We consider the same MDP as above, with the rewards for this
experiment chosen from the interval/bracketleftbig0 1/bracketrightbig
. The size of the parameter vector Kis50, and the discount factor
γis set to be 0.5. We vary the level of distortion introduced by Qδ(·)by changing the number of components
ktransmitted. Note that δ=K/k. As one might expect, increasing the distortion δby transmitting fewer
components impacts the exponential decay rate; this is clearly reflected in Fig. 3.
Simulation of Multi-Agent EF-TD. To simulate the behavior of multi-agent EF-TD(Algorithm 2), we
consider the same MDP on 100 states as before with rewards in [0 1]. The dimension Kof the parameter
vector is set to 10and the discount factor γto0.3. We consider two cases: one where Qδ(·)is the sign
operator, and another where it is a top- 2operator, i.e., only two components are transmitted by each agent
at every time-step. We report our findings in Fig. 4, where we vary the number of agents M, and observe
that for both the sign- and top- koperator experiments, the residual mean-squared error goes down as we
scale upM. Since the residual error is essentially an indicator of the noise variance, our plots display a clear
effect of variance reduction by increasing the number of agents. Our observations thus align with Theorem 3.
13Published in Transactions on Machine Learning Research (04/2024)
9 Conclusion and Future Work
We contributed to the development of a robustness theory for iterative RL algorithms subject to general
compression schemes. In particular, we proposed and analyzed EF-TD- a compressed version of the classical
TD(0) algorithm coupled with error-compensation. We then significantly generalized our analysis to nonlinear
stochastic approximation and multi-agent settings. Concretely, our work conveys the following key messages:
(i) compressed TD learning algorithms with error-feedback can be just as robust as their optimization
counterparts; (ii) the popular error-feedback mechanism extends gracefully beyond the static optimization
problems it has been explored for thus far; and (iii) linear convergence speedups in multi-agent TD learning
can be achieved with very little communication. Our work opens up several research directions. Studying
alternate quantization schemes for RL and exploring other RL algorithms (beyond TD and Q-learning) are
immediate next steps. A more open-ended question is the following. SignSGD with momentum (Bernstein
et al., 2018) is known to exhibit convergence behavior very similar to that of adaptive optimization algorithms
such as ADAM(Balles & Hennig, 2018), explaining their use in fast training of deep neural networks. In a
similar spirit, can we make connections between SignTDand adaptive RL algorithms? This is an interesting
question to explore since its resolution can potentially lead to faster RL algorithms.
14Published in Transactions on Machine Learning Research (04/2024)
References
Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent. arXiv preprint
arXiv:1704.05021 , 2017.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communication-efficient
sgd via gradient quantization and encoding. Advances in Neural Information Processing Systems , 30:
1709–1720, 2017.
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and Cédric Renggli.
The convergence of sparsified gradient methods. In Advances in Neural Information Processing Systems ,
pp. 5973–5983, 2018.
Lukas Balles and Philipp Hennig. Dissecting adam: The sign, magnitude and variance of stochastic gradients.
InInternational Conference on Machine Learning , pp. 404–413. PMLR, 2018.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signsgd:
Compressed optimisation for non-convex problems. In International Conference on Machine Learning , pp.
560–569. PMLR, 2018.
Aleksandr Beznosikov, Samuel Horváth, Peter Richtárik, and Mher Safaryan. On biased compression for
distributed learning. arXiv preprint arXiv:2002.12410 , 2020.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with
linear function approximation. In Conference on learning theory , pp. 1691–1692. PMLR, 2018.
Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint , volume 48. Springer, 2009.
Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation and
reinforcement learning. SIAM Journal on Control and Optimization , 38(2):447–469, 2000.
Zaiwei Chen, Sheng Zhang, Thinh T Doan, Siva Theja Maguluri, and John-Paul Clarke. Performance
of q-learning with linear function approximation: Stability and finite-time analysis. arXiv preprint
arXiv:1905.11425 , pp. 4, 2019.
Gal Dalal, Balázs Szörényi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td (0) with function
approximation. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018.
Thinh Doan, Siva Maguluri, and Justin Romberg. Finite-time analysis of distributed td (0) with linear
function approximation on multi-agent reinforcement learning. In International Conference on Machine
Learning , pp. 1626–1635. PMLR, 2019.
Thinh T Doan. Finite-time analysis of markov gradient descent. IEEE Transactions on Automatic Control ,
2022.
Hamid Reza Feyzmahdavian, Arda Aytekin, and Mikael Johansson. A delayed proximal gradient method with
linear convergence rate. In 2014 IEEE international workshop on machine learning for signal processing
(MLSP), pp. 1–6. IEEE, 2014.
Venkata Gandikota, Daniel Kane, Raj Kumar Maity, and Arya Mazumdar. vqsgd: Vector quantized stochastic
gradient descent. In International Conference on Artificial Intelligence and Statistics , pp. 2197–2205.
PMLR, 2021.
Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richtárik. Linearly converging error
compensated sgd. Advances in Neural Information Processing Systems , 33, 2020.
Robert M Gray. Source coding theory , volume 83. Springer Science & Business Media, 2012.
Kaja Gruntkowska, Alexander Tyurin, and Peter Richtárik. Ef21-p and friends: Improved theoretical
communication complexity for distributed optimization with bidirectional compression. arXiv preprint
arXiv:2209.15218 , 2022.
15Published in Transactions on Machine Learning Research (04/2024)
Mert Gurbuzbalaban, Asuman Ozdaglar, and Pablo A Parrilo. On the convergence rate of incremental
aggregated gradient algorithms. SIAM Journal on Optimization , 27(2):1035–1048, 2017.
OsamaAHanna, LinYang, andChristinaFragouli. Solvingmulti-armbanditusingafewbitsofcommunication.
InInternational Conference on Artificial Intelligence and Statistics , pp. 11215–11236. PMLR, 2022.
Roger A Horn and Charles R Johnson. Matrix analysis . Cambridge university press, 2012.
Hao Jin, Yang Peng, Wenhao Yang, Shusen Wang, and Zhihua Zhang. Federated reinforcement learning with
environment heterogeneity. In International Conference on Artificial Intelligence and Statistics , pp. 18–37.
PMLR, 2022.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U Stich, and Martin Jaggi. Error feedback fixes
signsgd and other gradient compression schemes. arXiv preprint arXiv:1901.09847 , 2019.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In Inter-
national Conference on Machine Learning , pp. 5132–5143. PMLR, 2020.
Sajad Khodadadian, Pranay Sharma, Gauri Joshi, and Siva Theja Maguluri. Federated reinforcement
learning: Linear speedup under markovian sampling. In International Conference on Machine Learning ,
pp. 10997–11057. PMLR, 2022.
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian U Stich. A unified theory of
decentralized sgd with changing topology and local updates. arXiv preprint arXiv:2003.10422 , 2020.
Jakub Konečn` y, H Brendan McMahan, Daniel Ramage, and Peter Richtárik. Federated optimization:
Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527 , 2016.
Nathaniel Korda and Prashanth La. On td(0) with function approximation: Concentration bounds and
a centered variant with exponential convergence. In International conference on machine learning , pp.
626–634. PMLR, 2015.
Chandrashekar Lakshminarayanan and Csaba Szepesvári. Linear stochastic approximation: Constant step-size
and iterate averaging. arXiv preprint arXiv:1709.04073 , 2017.
David A Levin and Yuval Peres. Markov chains and mixing times , volume 107. American Mathematical Soc.,
2017.
Chung-Yi Lin, Victoria Kostina, and Babak Hassibi. Differentially quantized gradient methods. IEEE
Transactions on Information Theory , 2022.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient compression: Reducing the
communication bandwidth for distributed training. arXiv preprint arXiv:1712.01887 , 2017.
Rui Liu and Alex Olshevsky. Distributed td (0) with almost no communication. arXiv preprint
arXiv:2104.07855 , 2021a.
Rui Liu and Alex Olshevsky. Temporal difference learning as gradient splitting. In International Conference
on Machine Learning , pp. 6905–6913. PMLR, 2021b.
Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, and
Michael I Jordan. Perturbed iterate analysis for asynchronous stochastic optimization. arXiv preprint
arXiv:1507.06970 , 2015.
Prathamesh Mayekar and Himanshu Tyagi. Ratq: A universal fixed-length quantizer for stochastic opti-
mization. In International Conference on Artificial Intelligence and Statistics , pp. 1399–1409. PMLR,
2020.
16Published in Transactions on Machine Learning Research (04/2024)
Aritra Mitra, Rayana Jaafar, George J Pappas, and Hamed Hassani. Linear convergence in federated learning:
Tackling client heterogeneity and sparse gradients. Advances in Neural Information Processing Systems , 34:
14606–14619, 2021.
Aritra Mitra, Hamed Hassani, and George J Pappas. Linear stochastic bandits over a bit-constrained channel.
arXiv preprint arXiv:2203.01198 , 2022.
Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, and Praneeth Netrapalli. Least squares regression
with markovian data: Fundamental limits and algorithms. Advances in neural information processing
systems, 33:16666–16676, 2020.
C Narayanan and Csaba Szepesvári. Finite time bounds for temporal difference learning with function
approximation: Problems with some “state-of-the-art” results. Technical report, Technical report, 2017.
Angelia Nedic, Asuman Ozdaglar, and Pablo A Parrilo. Constrained consensus and optimization in multi-agent
networks. IEEE Transactions on Automatic Control , 55(4):922–938, 2010.
Francesco Pase, Deniz Gündüz, and Michele Zorzi. Remote contextual bandits. In 2022 IEEE International
Symposium on Information Theory (ISIT) , pp. 1665–1670. IEEE, 2022.
Gandharv Patil, LA Prashanth, Dheeraj Nagaraj, and Doina Precup. Finite time analysis of temporal
difference learning with linear function approximation: Tail averaging and regularisation. In International
Conference on Artificial Intelligence and Statistics , pp. 5438–5448. PMLR, 2023.
Jiaju Qi, Qihao Zhou, Lei Lei, and Kan Zheng. Federated reinforcement learning: techniques, applications,
and open challenges. arXiv preprint arXiv:2108.11887 , 2021.
Peter Richtárik, Igor Sokolov, and Ilyas Fatkhullin. Ef21: A new, simpler, theoretically better, and practically
faster error feedback. Advances in Neural Information Processing Systems , 34:4384–4396, 2021.
Rajarshi Saha, Mert Pilanci, and Andrea J Goldsmith. Efficient randomized subspace embeddings for
distributed optimization under a communication budget. IEEE Journal on Selected Areas in Information
Theory, 2022.
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its
application to data-parallel distributed training of speech dnns. In Fifteenth Annual Conference of the
International Speech Communication Association , 2014.
Han Shen, Kaiqing Zhang, Mingyi Hong, and Tianyi Chen. Towards understanding asynchronous advantage
actor-critic: Convergence and linear speedup. IEEE Transactions on Signal Processing , 2023.
Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation andtd
learning. In Conference on Learning Theory , pp. 2803–2830. PMLR, 2019.
Sebastian U Stich. On communication compression for distributed optimization on heterogeneous data. arXiv
preprint arXiv:2009.02388 , 2020.
Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for sgd with
delayed gradients and compressed communication. arXiv preprint arXiv:1909.05350 , 2019.
Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified sgd with memory. In Advances in
Neural Information Processing Systems , pp. 4447–4458, 2018.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning , 3(1):9–44,
1988.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function approxi-
mation. In IEEE Transactions on Automatic Control , 1997.
17Published in Transactions on Machine Learning Research (04/2024)
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: Ternary
gradients to reduce communication in distributed deep learning. In Advances in neural information
processing systems , pp. 1509–1519, 2017.
18Published in Transactions on Machine Learning Research (04/2024)
Contents
1 Introduction 1
1.1 Our Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Model and Problem Setup 5
3 Error-Feedback meets TD Learning 6
4 Analysis of EF-TDunder Markovian Sampling 7
5 Compressed Nonlinear Stochastic Approximation with Error-Feedback 9
6 Communication-Efficient Multi-Agent Reinforcement Learning (MARL) 9
7 Technical Challenges in Analysis and Overview of our Proof Techniques 11
8 Simulations 12
9 Conclusion and Future Work 14
A Preliminary Results and Facts 20
B Building Intuition: Analysis of Mean-Path EF-TD 22
B.1 Can Compressed TD Methods Without Error-Feedback Still Converge? . . . . . . . . . . . . 25
C Warm-Up: Analysis of EF-TDwith a Projection Step 27
D Analysis of EF-TDwithout Projection: Proof of Theorem 1 and Theorem 2 34
D.1 Bounding the Drift and Bias Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
E Analysis of Multi-Agent EF-TD: Proof of Theorem 3 42
F Analysis of Multi-Agent EF-TDunder an I.I.D. Sampling Assumption 50
19Published in Transactions on Machine Learning Research (04/2024)
A Preliminary Results and Facts
In this section, we will compile and derive some preliminary results that will play a key role in our subsequent
analysis. In what follows, unless otherwise stated, we will use ∥·∥to refer to the standard Euclidean norm.
The next three results are from Bhandari et al. (2018).
Lemma 3. For allθ1,θ2∈RK, we have:
√ω∥θ1−θ2∥≤∥ ˆVθ1−ˆVθ2∥D≤∥θ1−θ2∥.
We remind the reader here that in the above result, ωis the smallest eigenvalue of the matrix Σ = Φ⊤DΦ.
Before stating the next result, we recall the definition of the steady-state TD update direction: for a fixed
θ∈RK, let
¯g(θ)≜Est∼π,st+1∼Pµ(·|st)[g(Xt,θ)].
The next lemma provides intuition as to why the expected steady-state TD(0) update direction ¯g(θ)acts like
a “pseudo-gradient", driving the TD(0) iterates towards the minimizer θ∗of the projected Bellman equation.
Lemma 4. For anyθ∈RK, the following holds:
⟨θ∗−θ,¯g(θ)⟩≥(1−γ)∥ˆVθ∗−ˆVθ∥2
D.
We will have occasion to use the following upper bound on the norm of the steady-state TD(0) update direction.
Lemma 5. For anyθ∈RK, the following holds:
∥¯g(θ)∥≤2∥ˆVθ∗−ˆVθ∥D.
The following bound on the norm of the random TD(0) update direction will also be invoked several times in
our analysis (Srikant & Ying, 2019).
Lemma 6. For anyθ∈RK, the following holds ∀t≥0:
∥gt(θ)∥≤2∥θ∥+ 2¯r≤2∥θ∥+ 2σ, (13)
whereσ= max{1,¯r,∥θ∗∥}.
As mentioned earlier in Section 3, compressed SGD with error-feedback essentially acts like delayed SGD.
Thus, for smooth functions where the gradients do not change much, the effect of the delay can be effectively
controlled. Unlike this optimization setting, we do not have a fixed objective function at our disposal. So how
do we leverage any kind of smoothness property? Fortunately for us, the steady-state TD(0) update direction
does satisfy a Lipschitz property; we prove this fact below.
Lemma 7. (Lipschitz property of steady-state TD(0) update direction ) For allθ1,θ2∈RK, we have:
∥¯g(θ1)−¯g(θ2)∥≤∥θ1−θ2∥.
Proof.We will make use of the explicit affine form of ¯g(θ)shown below (Tsitsiklis & Van Roy, 1997):
¯g(θ) = Φ⊤D(TµΦθ−Φθ) =¯Aθ−¯b,where ¯A= Φ⊤D(γPµ−I) Φ,and¯b=−Φ⊤DRµ.(14)
In Bhandari et al. (2018), it was shown that ¯A⊤¯A⪯Σ, where Σ = Φ⊤DΦ. Furthermore, due to feature
normalization, it is easy to see that λmax(Σ)≤1. Using these properties, we have:
∥¯g(θ1)−¯g(θ2)∥2= (θ1−θ2)⊤¯A⊤¯A(θ1−θ2)
≤λmax(¯A⊤¯A)∥θ1−θ2∥2
≤λmax(Σ)∥θ1−θ2∥2
≤∥θ1−θ2∥2,(15)
which leads to the desired claim. For the first inequality above, we used the Rayleigh-Ritz theorem (Horn &
Johnson, 2012, Theorem 4.2.2).
20Published in Transactions on Machine Learning Research (04/2024)
An immediate consequence of the above result, in tandem with the fact that ¯g(θ∗) = 0, is the following upper
bound on the norm of the steady-state TD(0) update direction: ∀θ∈RK, we have
∥¯g(θ)∥=∥¯g(θ)−¯g(θ∗)∥≤∥θ−θ∗∥≤∥θ∥+σ. (16)
Essentially, this shows that the bound in Lemma 6 applies to the steady-state TD(0) update direction as well.
Next, we prove an analog of the Lipschitz property in Lemma 7 for the random TD(0) update direction.
Lemma 8. (Lipschitz property of the noisy TD(0) update direction ) For allθ1,θ2∈RK, we have:
∥gt(θ1)−gt(θ2)∥≤2∥θ1−θ2∥.
Proof.As in the proof of Lemma 7, we will use the fact that the TD(0) update direction is an affine function
of the parameter θ. In particular, we have
gt(θ) =A(Xt)θ−b(Xt),whereA(Xt) =γΦ(st)Φ⊤(st+1)−Φ(st)Φ⊤(st),andbt=−ϕ(st)rt.
Thus, we have
∥gt(θ1)−gt(θ2)∥=∥A(Xt) (θ1−θ2)∥
≤∥A(Xt)∥∥θ1−θ2∥
≤/parenleftbig
γ∥Φ(st)∥∥Φ(st+1)∥+∥Φ(st)∥2/parenrightbig
∥θ1−θ2∥
≤2∥θ1−θ2∥,(17)
where for the last step we used that ∥Φ(s)∥≤1,∀s∈S.
In addition to the above results, we will make use of the following facts.
•Given any two vectors x,y∈RK, the following holds for any η>0:
∥x+y∥2≤(1 +η)∥x∥2+/parenleftbigg
1 +1
η/parenrightbigg
∥y∥2. (18)
•Givenmvectorsx1,...,xm∈RK, the following is a simple application of Jensen’s inequality:
/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1xi/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤mm/summationdisplay
i=1∥xi∥2. (19)
21Published in Transactions on Machine Learning Research (04/2024)
B Building Intuition: Analysis of Mean-Path EF-TD
Since the dynamics of EF-TDare quite complex, and have not been studied before, we provide an analysis in
this section for the simplest setting in our paper: the noiseless steady-state version of EF-TDwheregt(θt)in
line 4 of Algorithm 1 is replaced by ¯g(θt). We refer to this variant as the mean-path version of EF-TD, and
compile its governing equations below:
ht=Qδ(et−1+ ¯g(θt)),
θt+1=θt+αht,
et=et−1+ ¯g(θt)−ht,(20)
where the above equations hold for t= 0,1,..., withθ0∈RK, ande−1= 0. The goal of this section is to
provide an analysis of mean-path EF-TD, and, in the process, outline some of the key ideas that will aid us in
the more involved settings to follow. We have the following result.
Theorem 4. (Noiseless Setting ) There exist universal constants c,C≥1, such that the iterates generated
by the mean-path version of EF-TDwith step-size α= (1−γ)/(cδ)satisfy the following after Titerations:
∥θT−θ∗∥2
2≤2/parenleftbigg
1−(1−γ)2ω
Cδ/parenrightbiggT
∥θ0−θ∗∥2
2. (21)
Discussion. Theorem 4 reveals linear convergence of the iterates to θ∗. Whenδ= 1, i.e., when there is
no distortion to the TD(0) direction, the linear rate of convergence exactly matches that in Bhandari et al.
(2018). Moreover, when δ>1, the slowdown in the linear rate by a factor of δis also exactly consistent with
analogous results for SGD with (biased) compression (Beznosikov et al., 2020). Thus, our result captures - in
a transparent way - precisely what one could have hoped for.
To proceed with the analysis of mean-path EF-TD, we will make use of the perturbed iterate framework from
Mania et al. (2015). In particular, let us define the perturbed iterate ˜θt≜θt+αet−1.Using equation 20, we
then obtain:˜θt+1=θt+1+αet
=θt+αht+α(et−1+ ¯g(θt)−ht)
=˜θt+α¯g(θt).(22)
The final recursion above looks almost like the the standard steady-state TD(0) update direction, other
than the fact that ¯g(θt)is evaluated at θt, and not ˜θt. To account for this “mismatch" introduced by the
memory-variable et−1, we will analyze the following composite Lyapunov function:
ψt≜∥˜θt−θ∗∥2+α2∥et−1∥2. (23)
Note that the above energy function captures the joint dynamics of the perturbed iterate and the memory
variable. Our goal is to prove that this energy function decays exponentially over time. To that end, we start
by establishing a bound on ∥˜θt+1−θ∗∥2in the following lemma.
Lemma 9. (Bound on Perturbed Iterate ) Suppose the step-size αis chosen such that α≤(1−γ)/8.
Then, the iterates generated by the mean-path version of EF-TDsatisfy:
∥˜θt+1−θ∗∥2≤∥˜θt−θ∗∥2−α(1−γ)
4∥ˆV˜θt−ˆVθ∗∥2
D+5α3
(1−γ)∥et−1∥2. (24)
Proof.Subtracting θ∗from each side of equation 22 and then squaring both sides yields:
∥˜θt+1−θ∗∥2=∥˜θt−θ∗∥2+ 2α⟨˜θt−θ∗,¯g(θt)⟩+α2∥¯g(θt)∥2
=∥˜θt−θ∗∥2+ 2α⟨θt−θ∗,¯g(θt)⟩+α2∥¯g(θt)∥2+ 2α⟨˜θt−θt,¯g(θt)⟩
(a)
≤∥˜θt−θ∗∥2+ 2α⟨θt−θ∗,¯g(θt)⟩+α/parenleftbigg
α+1
η/parenrightbigg
∥¯g(θt)∥2+αη∥˜θt−θt∥2
(b)=∥˜θt−θ∗∥2+ 2α⟨θt−θ∗,¯g(θt)⟩+α/parenleftbigg
α+1
η/parenrightbigg
∥¯g(θt)∥2+α3η∥et−1∥2.(25)
22Published in Transactions on Machine Learning Research (04/2024)
For (a), we used the fact that for any two vectors x,y∈RK, the following holds for all η>0,
⟨x,y⟩≤1
2η∥x∥2+η
2∥y∥2.
We will pick an appropriate ηshortly. For (b), we used the fact that from the definition of the perturbed
iterate, it holds that ˜θt−θt=αet−1.We will now make use of Lemma’s 4 and 5. We proceed as follows.
∥˜θt+1−θ∗∥2≤∥˜θt−θ∗∥2+ 2α⟨θt−θ∗,¯g(θt)⟩+α/parenleftbigg
α+1
η/parenrightbigg
∥¯g(θt)∥2+α3η∥et−1∥2
(a)
≤∥˜θt−θ∗∥2−2α(1−γ)∥ˆVθt−ˆVθ∗∥2
D+α/parenleftbigg
α+1
η/parenrightbigg
∥¯g(θt)∥2+α3η∥et−1∥2
(b)
≤∥˜θt−θ∗∥2−α/parenleftbigg
2(1−γ)−4/parenleftbigg
α+1
η/parenrightbigg/parenrightbigg
∥ˆVθt−ˆVθ∗∥2
D+α3η∥et−1∥2
(c)
≤∥˜θt−θ∗∥2−α(1−γ)/parenleftbigg
1−4α
(1−γ)/parenrightbigg
∥ˆVθt−ˆVθ∗∥2
D+4α3
(1−γ)∥et−1∥2
(d)
≤∥˜θt−θ∗∥2−α(1−γ)
2∥ˆVθt−ˆVθ∗∥2
D+4α3
(1−γ)∥et−1∥2.(26)
In the above steps, (a) follows from Lemma 4; (b) follows from Lemma 5; (c) follows by setting η= 4/(1−γ);
and (d) is a consequence of the fact that α≤(1−γ)/8. To complete the proof, we need to relate ∥ˆVθt−ˆVθ∗∥2
D
to∥ˆV˜θt−ˆVθ∗∥2
D. We do so by using the fact that for any x,y∈Rn, it holds that∥x+y∥2
D≤2∥x∥2
D+ 2∥y∥2
D.
This yields:
∥ˆV˜θt−ˆVθ∗∥2
D≤2∥ˆVθt−ˆVθ∗∥2
D+ 2∥ˆV˜θt−ˆVθt∥2
D
(a)
≤2∥ˆVθt−ˆVθ∗∥2
D+ 2∥˜θt−θt∥2
≤2∥ˆVθt−ˆVθ∗∥2
D+ 2α2∥et−1∥2,(27)
where for (a), we used Lemma 3. Rearranging and simplifying, we obtain:
−∥ˆVθt−ˆVθ∗∥2
D≤−1
2∥ˆV˜θt−ˆVθ∗∥2
D+α2∥et−1∥2.
Plugging the above inequality in equation 26 leads to equation 24. This completes the proof.
The last term in equation 24 is one which does not show up in the standard analysis of TD(0), and is unique to
our setting. In our next result, we control this extra term (depending on the memory variable) by appealing
to the contraction property of the compression operator Qδ(·)in equation 4.
Lemma 10. (Bound on Memory Variable ) For the mean-path version of EF-TD, the following holds:
∥et∥2≤/parenleftbigg
1−1
2δ+ 4α2δ/parenrightbigg
∥et−1∥2+ 16δ∥ˆV˜θt−ˆVθ∗∥2
D. (28)
Proof.We begin as follows:
∥et∥2=∥et−1+ ¯g(θt)−ht∥2
=∥et−1+ ¯g(θt)−Qδ(et−1+ ¯g(θt))∥2
(a)
≤/parenleftbigg
1−1
δ/parenrightbigg
∥et−1+ ¯g(θt)∥2
(b)
≤/parenleftbigg
1−1
δ/parenrightbigg/parenleftbigg
1 +1
η/parenrightbigg
∥et−1∥2+/parenleftbigg
1−1
δ/parenrightbigg
(1 +η)∥¯g(θt)∥2,(29)
23Published in Transactions on Machine Learning Research (04/2024)
for someη>0to be chosen by us shortly. Here, for (a), we used the contraction property of Qδ(·)in equation 4;
for (b), we used the relaxed triangle inequality equation 18. To ensure that ∥et∥2contracts over time, we
want /parenleftbigg
1−1
δ/parenrightbigg/parenleftbigg
1 +1
η/parenrightbigg
<1 =⇒η>(δ−1).
Accordingly, suppose η=δ−1.Simple calculations then yield
/parenleftbigg
1−1
δ/parenrightbigg/parenleftbigg
1 +1
η/parenrightbigg
=/parenleftbigg
1−1
2δ/parenrightbigg
;/parenleftbigg
1−1
δ/parenrightbigg
(1 +η)<2δ.
Plugging these bounds back in equation 29, we obtain
∥et∥2≤/parenleftbigg
1−1
2δ/parenrightbigg
∥et−1∥2+ 2δ∥¯g(θt)∥2
≤/parenleftbigg
1−1
2δ/parenrightbigg
∥et−1∥2+ 2δ∥¯g(θt)−¯g(˜θt) + ¯g(˜θt)∥2
≤/parenleftbigg
1−1
2δ/parenrightbigg
∥et−1∥2+ 4δ∥¯g(θt)−¯g(˜θt)∥2+ 4δ∥¯g(˜θt)∥2
(a)
≤/parenleftbigg
1−1
2δ/parenrightbigg
∥et−1∥2+ 4δ∥θt−˜θt∥2+ 4δ∥¯g(˜θt)∥2
=/parenleftbigg
1−1
2δ+ 4α2δ/parenrightbigg
∥et−1∥2+ 4δ∥¯g(˜θt)∥2
(b)
≤/parenleftbigg
1−1
2δ+ 4α2δ/parenrightbigg
∥et−1∥2+ 16δ∥ˆV˜θt−ˆVθ∗∥2
D.(30)
In the above steps, for (a) we used the Lipschitz property of the steady-state TD(0) update direction, namely
Lemma 7; and for (b), we used Lemma 5. This concludes the proof.
Lemmas 9 and 10 reveal that the error-dynamics of the perturbed iterate and the memory variable are
coupled with each other. As such, they cannot be studied in isolation. This precisely motivates the choice of
the Lyapunov function ψtin equation 23. We are now ready to complete the proof of Theorem 4.
Proof.(Proof of Theorem 4 ) Using the bounds from Lemmas 9 and 10, and recalling the definition of the
Lyapunov function ψtfrom equation 23, we have
ψt+1≤∥˜θt−θ∗∥2−α(1−γ)
4/parenleftbigg
1−64αδ
(1−γ)/parenrightbigg
∥ˆV˜θt−ˆVθ∗∥2
D+α2/parenleftbigg
1−1
2δ+ 4α2δ+5α
(1−γ)/parenrightbigg
∥et−1∥2
≤/parenleftbigg
1−αω(1−γ)
4/parenleftbigg
1−64αδ
(1−γ)/parenrightbigg/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
A1∥˜θt−θ∗∥2+α2/parenleftbigg
1−1
2δ+ 4α2δ+5α
(1−γ)/parenrightbigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
A2∥et−1∥2,(31)
where we used Lemma 3 in the last step. Our goal is to establish an inequality of the form ψt+1≤νψtfor some
ν <1. To that end, the next step of the proof is to pick the step-size αin a way such that max{A1,A2}<1.
Accordingly, with α= (1−γ)/(128δ), we have that
A1=/parenleftbigg
1−(1−γ)2ω
1024δ/parenrightbigg
;andA2≤/parenleftbigg
1−1
4δ/parenrightbigg
.
Furthermore, it is easy to check that A2≤A 1. Combining these observations with equation 31, we obtain:
ψt+1≤/parenleftbigg
1−(1−γ)2ω
1024δ/parenrightbigg/parenleftbig
∥˜θt−θ∗∥2+α2∥et−1∥2/parenrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
ψt.
24Published in Transactions on Machine Learning Research (04/2024)
Unrolling the above recursion yields:
ψT≤/parenleftbigg
1−(1−γ)2ω
1024δ/parenrightbiggT
ψ0
=/parenleftbigg
1−(1−γ)2ω
1024δ/parenrightbiggT
∥θ0−θ∗∥2,(32)
where for the last step, we used the fact that e−1= 0. To conclude the proof, it suffices to notice that:
∥θT−θ∗∥2=∥θT−˜θT+˜θT−θ∗∥2
≤2∥˜θT−θ∗∥2+ 2∥θT−˜θT∥2
= 2∥˜θT−θ∗∥2+ 2α2∥eT−1∥2
= 2ψT.(33)
B.1 Can Compressed TD Methods Without Error-Feedback Still Converge?
Earlier in this section, we provided intuition as to why EF-TDconverges by studying its dynamics in the
steady-state. One might ask: Can compressed TD algorithms without error-feedback still converge? If so,
under what conditions? We turn to answering these questions in this subsection. In what follows, we will
show that compressed TD without error-feedback can still converge, provided certain restrictive conditions
on the compression parameter δare met. Notably, these conditions are no longer needed when one employs
error-feedback. To convey the key ideas, we consider a mean-path version of compressed TD shown below:
θt+1=θt+αQδ(¯g(θt)), (34)
whereQδ(·)is the compression operator in equation 4. From the above display, we immediately have
∥θt+1−θ∗∥2=∥θt−θ∗∥2+ 2α⟨θt−θ∗,Qδ(¯g(θt))⟩+α2∥Qδ(¯g(θt))∥2. (35)
Among the three terms on the R.H.S. of the above equation, notice that the only term that can lead to a
decrease in the iterate error ∥θt+1−θ∗∥2is clearly 2α⟨θt−θ∗,Qδ(¯g(θt))⟩. As such, let us fix a θ∈RK, and
investigate what we can say about ⟨θ−θ∗,Qδ(¯g(θ))⟩.First, notice that if there is no compression, i.e., δ= 1,
thenQδ(¯g(θ)) = ¯g(θ), and we know from Lemmas 3 and 4 that
⟨θ−θ∗,¯g(θ)⟩≤−β∥θ−θ∗∥2, (36)
whereβ=ω(1−γ)∈(0,1).It is precisely the above key property that causes uncompressed TD to converge
toθ∗. Now let us observe:
⟨θ−θ∗,Qδ(¯g(θ))⟩=⟨θ−θ∗,¯g(θ)⟩+⟨θ−θ∗,Qδ(¯g(θ))−¯g(θ)⟩
≤⟨θ−θ∗,¯g(θ)⟩+∥θ−θ∗∥∥Qδ(¯g(θ))−¯g(θ)∥
(a)
≤⟨θ−θ∗,¯g(θ)⟩+/radicaligg/parenleftbigg
1−1
δ/parenrightbigg
∥θ−θ∗∥∥¯g(θ)∥
(b)
≤⟨θ−θ∗,¯g(θ)⟩+/radicaligg/parenleftbigg
1−1
δ/parenrightbigg
∥θ−θ∗∥2
(c)
≤−/parenleftigg
β−/radicaligg/parenleftbigg
1−1
δ/parenrightbigg/parenrightigg
∥θ−θ∗∥2.(37)
25Published in Transactions on Machine Learning Research (04/2024)
In the above steps, (a) follows from equation 4, (b) follows from equation 16, and (c) from equation 36.
Comparing equation 37 to equation 36, we conclude that for the distorted TD direction Qδ(¯g(θ))to ensure
progress towards θ∗, we need the following condition to hold:
/radicaligg/parenleftbigg
1−1
δ/parenrightbigg
<β. (38)
Simplifying, the above condition amounts to
δ<1
(1−β2). (39)
The parameter β∈(0,1)gets fixed when one fixes an MDP, the policy to be evaluated, and the feature
vectors for linear function approximation. The condition for contraction/convergence in equation 39 tells us
that this parameter βlimits the extent of compression δ. Said differently, one cannot choose the compression
levelδto be arbitrarily large; rather it is dictated by the problem-dependent parameter β.It is important to
note here that no such restriction on δis necessary when one uses error-feedback, as revealed by our analysis
for mean-path EF-TD. This highlights the benefit of using error-feedback in the context of compressed TD
learning. With these observations in place, let us return to our analysis of the update rule in equation 34.
For ease of notation, let us define
ζ≜/parenleftigg
β−/radicaligg/parenleftbigg
1−1
δ/parenrightbigg/parenrightigg
,
and note that if the compression parameter δsatisfies the condition in equation 39, then ζ >0. Plugging the
bound from equation 37 in equation 35, we obtain
∥θt+1−θ∗∥2=∥θt−θ∗∥2+ 2α⟨θt−θ∗,Qδ(¯g(θt))⟩+α2∥Qδ(¯g(θt))∥2
≤(1−2αζ)∥θt−θ∗∥2+α2∥Qδ(¯g(θt))−¯g(θt) + ¯g(θt)∥2
≤(1−2αζ)∥θt−θ∗∥2+ 2α2∥Qδ(¯g(θt))−¯g(θt)∥2+ 2α2∥¯g(θt)∥2
(a)
≤(1−2αζ)∥θt−θ∗∥2+ 2/parenleftbigg
2−1
δ/parenrightbigg
α2∥¯g(θt)∥2
(b)
≤/parenleftbig
1−2αζ+ 4α2/parenrightbig
∥θt−θ∗∥2,(40)
where (a) follows from equation 4 and (b) from equation 16. Thus, with α≤ζ/4, we have
∥θt+1−θ∗∥2≤(1−αζ)∥θt−θ∗∥2.
We conclude that when the compression parameter δsatisfies the condition in equation 39, and the step-size
is chosen to be suitably small, the compressed TD update rule in equation 34 does converge linearly to θ∗.
26Published in Transactions on Machine Learning Research (04/2024)
C Warm-Up: Analysis of EF-TDwith a Projection Step
Before attempting to prove Theorem 1, it is instructive to analyze the behavior of EF-TDwith a projection
step. The benefit of this projection step is that it makes it relatively easier to argue that the iterates generated
byEF-TDremain uniformly bounded; nonetheless, as we shall soon see, the analysis remains quite non-trivial
even in light of this simplification. Let us now jot down the governing equations of the dynamics we plan to
study.
ht=Qδ(et−1+gt(θt)),
θt+1= Π 2,B(θt+αht),
et=et−1+gt(θt)−ht,(41)
where Π2,B(·)denotes the standard Euclidean projection on to a convex compact subset B⊂RKthat is
assumed to contain the fixed point θ∗. We also note here that a projection step of the form in equation 41 is
common in the literature on stochastic approximation (Borkar, 2009) and RL (Bhandari et al., 2018; Doan
et al., 2019).
Our main result concerning the performance of the projected version of EF-TDis the following.
Theorem 5. Suppose Assumption 1 holds. There exists a universal constant c≥1such that the iterates
generated by the projected version of EF-TD(i.e., equation 41) with step-size α≤(1−γ)/csatisfy the following
afterT≥τiterations:
E/bracketleftbig
∥θT−θ∗∥2
2/bracketrightbig
≤C1(1−αω(1−γ))T−τ+O/parenleftbiggατδ2G2
ω(1−γ)/parenrightbigg
, (42)
whereC1=O(α2δ2G2+G2), andGis the radius of the convex compact set B.
Main Takeaway. We note that the nature of the above guarantee is similar to that of Theorem 1. That
said, while the noise term in Theorem 1 is O(τ+δ), it isO(τδ2)in Theorem 5. In words, with the somewhat
cruder bounds we obtain via projection, we end up with a looser dependency on the distortion parameter δ.
Moreover, the mixing time τand the distortion parameter δshow up in multiplicative form in Theorem 5. In
Section D, we will provide a finer analysis (without the need for projection) that yields the tighter O(τ+δ)
bound.
We now proceed with the proof of Theorem 5. Let us start by defining the projection error ep,tat time-step tas
follows:ep,t=θt−(θt−1+αht−1). We also define an intermediate sequence {¯θt}as follows: ¯θt≜θt−1+αht−1.
Thus,θt−¯θt=ep,t.Next, inspired by the perturbed iterate framework in Mania et al. (2015), we define a
modified perturbed iterate as follows:
˜θt≜¯θt+αet−1. (43)
Based on the above definitions, observe that
˜θt+1=¯θt+1+αet
=θt+αht+α(et−1+gt(θt)−ht)
=θt+αgt(θt) +αet−1
=¯θt+αet−1+αgt(θt) +ep,t
=˜θt+αgt(θt) +ep,t.(44)
Subtracting θ∗from each side of equation 44 and then squaring both sides, we obtain:
∥˜θt+1−θ∗∥2=∥˜θt−θ∗∥2+ 2α⟨˜θt−θ∗,gt(θt)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
C1+α2∥gt(θt)∥2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
C2+ 2⟨˜θt−θ∗+αgt(θt),ep,t⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
C3+∥ep,t∥2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
C4.(45)
In what follows, we outline the key steps of our proof that involve bounding each of the terms C1−C4.
•Step 1. The dynamics of the model parameter θt, the Markov variable Xt, the memory variable et, and
the projection error ep,tare all closely coupled, leading to a dynamical system far more complex than the
27Published in Transactions on Machine Learning Research (04/2024)
standard TD(0) system. To start unravelling this complex dynamical system, our key strategy is to disentangle
the memory variable and the projection error from the perturbed iterate and the Markov data tuple . To do so,
we derive uniform bounds on et,ht, andep,tby exploiting the contraction property in equation 4. This is
achieved in Lemma 12.
•Step 2. Using the uniform bounds from the previous step in tandem with properties of the Euclidean
projection operator, we control terms C2−C4in Lemma 13.
•Step 3. BoundingC1takes the most work. For this step, we exploit the idea of conditioning on the system
state sufficiently into the past, and using the geometric mixing property of the Markov chain. As we shall see,
conditioning into the past creates the need to control ∥θt−θt−τ∥,∀t≥τ, whereτis the mixing time. This is
done in Lemma 14. Using the result from Lemma 14, we bound C1in Lemma 15.
At the end of the three steps above, what we wish to establish is a recursion of the following form ∀t≥τ:
E/bracketleftbig
∥˜θt+1−θ∗∥2/bracketrightbig
≤(1−αω(1−γ))E/bracketleftbig
∥˜θt−θ∗∥2/bracketrightbig
+O(α2τδ2G2).
To proceed with Step 1, we recall the following result from Nedic et al. (2010).
Lemma 11. LetBbe a nonempty, closed, convex set in RK. Then, for any x∈RK, we have:
(a)⟨Π2,B(x)−x,x−y⟩≤−∥ Π2,B(x)−x∥2,∀y∈B.
(b)∥ΠB(x)−y∥2≤∥x−y∥2−∥ΠB(x)−x∥2,∀y∈B.
To lighten notation, let us assume without loss of generality that all rewards are uniformly bounded by 1.
Our results can be trivially extended to the case where the uniform bound is some finite number Rmax. To
make the calculations cleaner, we also assume that the projection radius Gis greater than 1. We have the
following key result that provides uniform bounds on the memory variable and the projection error.
Lemma 12. (Uniform bounds on memory variable and projection error ) For the dynamics in equa-
tion 41, the following hold ∀t≥0:
(a)∥et∥≤6δG.
(b)∥ht∥≤15δG.
(c)∥ep,t∥≤15αδG.
Proof.We start by noting that for all t≥0,
∥gt(θt)∥=∥A(Xt)θt−b(Xt)∥≤∥A(Xt)∥∥θt∥+∥b(Xt)∥≤2G+ 1≤3G, (46)
where we used (i) the feature normalization property; (ii) the fact that the rewards are uniformly bounded by
1; and (iii) the fact that due to projection, ∥θt∥≤1,∀t≥0.Next, observe that
∥et∥2=∥et−1+gt(θt)−ht∥2
=∥et−1+gt(θt)−Qδ(et−1+gt(θt))∥2
(a)
≤/parenleftbigg
1−1
δ/parenrightbigg
∥et−1+gt(θt)∥2
(b)
≤/parenleftbigg
1−1
δ/parenrightbigg/parenleftbigg
1 +1
η/parenrightbigg
∥et−1∥2+/parenleftbigg
1−1
δ/parenrightbigg
(1 +η)∥gt(θt)∥2,(47)
for someη>0to be chosen by us shortly. Here, for (a), we used the contraction property of Qδ(·)in equation 4;
for (b), we used the relaxed triangle inequality in equation 18. To ensure that ∥et∥2contracts over time, we
want /parenleftbigg
1−1
δ/parenrightbigg/parenleftbigg
1 +1
η/parenrightbigg
<1 =⇒η>(δ−1).
28Published in Transactions on Machine Learning Research (04/2024)
Accordingly, suppose η=δ−1.Simple calculations then yield
/parenleftbigg
1−1
δ/parenrightbigg/parenleftbigg
1 +1
η/parenrightbigg
=/parenleftbigg
1−1
2δ/parenrightbigg
;/parenleftbigg
1−1
δ/parenrightbigg
(1 +η)<2δ.
Plugging these bounds back in equation 47 and using equation 46, we obtain
∥et∥2≤/parenleftbigg
1−1
2δ/parenrightbigg
∥et−1∥2+ 2δ∥gt(θt)∥2
≤/parenleftbigg
1−1
2δ/parenrightbigg
∥et−1∥2+ 18δG2.
Unrolling the dynamics of the memory variable thus yields:
∥et∥2≤/parenleftbigg
1−1
2δ/parenrightbiggt+1
∥e−1∥2+ 18δG2t/summationdisplay
k=0/parenleftbigg
1−1
2δ/parenrightbiggk
≤18δG2∞/summationdisplay
k=0/parenleftbigg
1−1
2δ/parenrightbiggk
= 36δ2G2,(48)
where we used the fact that e−1= 0. Thus,∥et∥≤6δG, which establishes part (a). For part (b), we notice
thatht=et−1−et+gt(θt).This immediately yields:
∥ht∥≤∥et∥+∥et−1∥+∥gt(θt)∥≤12δG+ 3G≤15δG,
where we used the fact that δ≥1, the bound from part (a), and the uniform bound on gt(θt)established
earlier.
Next, for part (c), we use part (b) of Lemma 11 to observe that
∥ep,t∥2=∥Π2,B(¯θt)−¯θt∥2≤∥¯θt−θ∥2,∀θ∈B.
Since the above bound holds for all θ∈B, andθt−1∈B, we have
∥ep,t∥2≤∥¯θt−θt−1∥2=α2∥ht∥2≤225α2δ2G2,
where we used the fact that ¯θt=θt−1+αht−1by definition, and also the bound on ∥ht∥from part (b). This
concludes the proof.
From the proof of the above lemma, bounds on terms C2andC4in equation 45 follow immediately. In our
next result, we bound the term C3.
Lemma 13. For the dynamics in equation 41, the following holds for all t≥0:
2⟨˜θt−θ∗+αgt(θt),ep,t⟩≤45α2δ2G2.
Proof.We start by decomposing the term we wish to bound into three parts:
2⟨˜θt−θ∗+αgt(θt),ep,t⟩= 2⟨¯θt−θ∗+αet−1+αgt(θt),ep,t⟩
= 2⟨¯θt−θ∗,ep,t⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
C31+ 2α⟨et−1,ep,t⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
C32+ 2α⟨gt(θt),ep,t⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
C33. (49)
We now bound each of the three terms above separately. For C31, we have
2⟨¯θt−θ∗,ep,t⟩= 2⟨¯θt−θ∗,θt−¯θt⟩≤− 2∥θt−¯θt∥2=−2∥ep,t∥2, (50)
29Published in Transactions on Machine Learning Research (04/2024)
where we used part (a) of the projection lemma, namely Lemma 11, with x=¯θtandy=θ∗; here, note that
we used the fact that θ∗∈B. Next, forC32, observe that
2α⟨et−1,ep,t⟩≤α2∥et−1∥2+∥ep,t∥2
≤36α2δ2G2+∥ep,t∥2,(51)
where we used the bound on ∥et−1∥from part (a) of Lemma 12. Notice that we have kept ∥ep,t∥2as is in the
above bound since we will cancel off its effect with one of the negative terms from the upper bound on C31.
Finally, we bound the term C33as follows:
2α⟨gt(θt),ep,t⟩≤α2∥gt(θt)∥2+∥ep,t∥2
≤9α2G2+∥ep,t∥2,(52)
where we used the uniform bound on gt(θt)from equation 46. Combining the bounds in equations 50, 51,
and 52, and using the fact that δ≥1yields the desired result.
Notice that up until now, we have not made any use of the geometric mixing property of the underlying
Markov chain. We will call upon this property while bounding C1. But first, we need the following intermediate
result.
Lemma 14. For the dynamics in equation 41, the following holds for all t≥τ:
∥θt−θt−τ∥≤60ατδG. (53)
Proof.Based on equation 44, observe that
∥˜θt+1−˜θt∥≤α∥gt(θt)∥+∥ep,t∥
≤3αG+ 15αδG
≤18αδG.(54)
We also note that
˜θt−˜θt−τ=t−1/summationdisplay
k=t−τ/parenleftbig˜θk+1−˜θk/parenrightbig
.
Based on equation 54, we then immediately have
∥˜θt−˜θt−τ∥≤t−1/summationdisplay
k=t−τ∥˜θk+1−˜θk∥
≤t−1/summationdisplay
k=t−τ(18αδG)
≤18ατδG.(55)
Our goal is to now relate the above bound on ∥˜θt−˜θt−τ∥to one on∥θt−θt−τ∥. To that end, observe that
˜θt=θt+αet−1−ep,t,
˜θt−τ=θt−τ+αet−τ−1−ep,t−τ.
This gives us exactly what we need:
∥θt−θt−τ∥≤∥ ˜θt−˜θt−τ∥+α∥et−1−et−τ−1∥+∥ep,t−τ−ep,t∥
≤∥˜θt−˜θt−τ∥+α∥et−1∥+α∥et−τ−1∥+∥ep,t−τ∥+∥ep,t∥
≤18ατδG + 12αδG + 30αδG≤60ατδG,(56)
where we used equation 55, parts (a) and (c) of Lemma 12, and assumed that the mixing time τ≥1to state
the bounds more cleanly. This completes the proof.
30Published in Transactions on Machine Learning Research (04/2024)
We now turn towards establishing an upper bound on the term C1in equation 45.
Lemma 15. Suppose Assumption 1 holds. For the dynamics in equation 41, the following then holds ∀t≥τ:
E/bracketleftbig
2α⟨˜θt−θ∗,gt(θt)⟩/bracketrightbig
≤−2α(1−γ)E/bracketleftbig
∥Vθt−Vθ∗∥2
D/bracketrightbig
+ 1454α2δτG2.
Proof.Let us first focus on bounding T≜⟨θt−θ∗,gt(θt)−¯g(θt)⟩. Trivially, observe that
T=⟨θt−θt−τ,gt(θt)−¯g(θt)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T1+⟨θt−τ−θ∗,gt(θt)−¯g(θt)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T2.
To boundT1, we recall from Lemma 7 that for all θ1,θ2∈RK, it holds that
∥¯g(θ1)−¯g(θ2)∥≤∥θ1−θ2∥.
Since ¯g(θ∗) = 0, the above inequality immediately implies that ∥¯g(θ)∥≤∥θ∥+∥θ∗∥,∀θ∈RK. In particular,
for anyθ∈B, we then have that ∥¯g(θ)∥≤2G(sinceθ∗∈B). Using the bound on ∥θt−θt−τ∥from Lemma
14, and the uniform bound on the noisy TD(0) update direction from equation 46, we then obtain
T1≤(∥θt−θt−τ∥) (∥gt(θt)−¯g(θt)∥)
≤(∥θt−θt−τ∥) (∥gt(θt)∥+∥¯g(θt)∥)
≤60ατδG (3G+ 2G) = 300ατδG2.(57)
To boundT2, we further split it into two parts as follows:
T2=⟨θt−τ−θ∗,gt(θt−τ)−¯g(θt−τ)⟩/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T21+⟨θt−τ−θ∗,gt(θt)−gt(θt−τ) + ¯g(θt−τ)−¯g(θt)⟩/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T22.
To boundT22, we will exploit the Lipschitz property of the TD(0) update directions in tandem with Lemma
14. Specifically, observe that:
T22≤∥θt−τ−θ∗∥(∥gt(θt)−gt(θt−τ)∥+∥¯g(θt−τ)−¯g(θt)∥)
(a)
≤2G(∥gt(θt)−gt(θt−τ)∥+∥¯g(θt−τ)−¯g(θt)∥)
(b)
≤6G∥θt−θt−τ∥
(c)
≤360ατδG2.(58)
In the above steps, (a) follows from projection; (b) follows from Lemmas 7 and 8; and (c) follows from Lemma
14. It remains to bound T21. This is precisely the only place in the entire proof that we will use the geometric
mixing property of the Markov chain in Definition 1. We proceed as follows.
E[T21] =E[⟨θt−τ−θ∗,gt(θt−τ)−¯g(θt−τ)⟩]
=E[E[⟨θt−τ−θ∗,gt(θt−τ)−¯g(θt−τ)⟩|θt−τ,Xt−τ]]
=E[⟨θt−τ−θ∗,E[gt(θt−τ)−¯g(θt−τ)|θt−τ,Xt−τ]⟩]
≤E[∥θt−τ−θ∗∥∥E[gt(θt−τ)−¯g(θt−τ)|θt−τ,Xt−τ]∥]
(a)
≤2αG(E[∥θt−τ−θ∗∥])
≤4αG2,(59)
where (a) follows from the definition of the mixing time τ. Combining the above bound with those in
equations 57 and 58, we obtain
E[T]≤664ατδG2, (60)
31Published in Transactions on Machine Learning Research (04/2024)
where we used τ≥1andδ≥1.
We can now go back to bounding C1as follows:
C1= 2α⟨θt−θ∗+αet−1−ep,t,gt(θt)⟩
= 2α⟨θt−θ∗,gt(θt)⟩+ 2α2⟨et−1,gt(θt)⟩−2α⟨ep,t,gt(θt)⟩
≤2α⟨θt−θ∗,gt(θt)⟩+ 2α2∥et−1∥∥gt(θt)∥+ 2α∥ep,t∥∥gt(θt)∥
≤2α⟨θt−θ∗,gt(θt)⟩+ 126α2δG2,(61)
where we used equation 46, and parts (a) and (c) of Lemma 12. We continue as follows:
C1≤2α⟨θt−θ∗,¯g(θt)⟩+ 2αT+ +126α2δG2.
Using Lemma 4 and the bound we derived on Tin equation 60, we finally obtain
E[C1]≤−2α(1−γ)E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+ 1454α2δτG2,
where in the last step, we used Lemma 4.
We can now complete the proof of Theorem 5.
Proof.(Proof of Theorem 5 ) We combine the bounds derived previously on the terms C1-C4in Lemmas 12,
13, and 15 to obtain that ∀t≥τ,
E/bracketleftbig
∥˜θt+1−θ∗∥2
2/bracketrightbig
≤E/bracketleftbig
∥˜θt−θ∗∥2
2/bracketrightbig
−2α(1−γ)E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+ 1454α2δτG2
+ 9α2G2+ 45α2δ2G2+ 225α2δ2G2
≤E/bracketleftbig
∥˜θt−θ∗∥2
2/bracketrightbig
−2α(1−γ)E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+ 1733α2δ2τG2.(62)
To proceed, we need to relate ∥ˆVθt−ˆVθ∗∥2
Dto∥ˆV˜θt−ˆVθ∗∥2
D. We do so by using the fact that for any x,y∈Rn,
it holds that∥x+y∥2
D≤2∥x∥2
D+ 2∥y∥2
D.This yields:
∥ˆV˜θt−ˆVθ∗∥2
D≤2∥ˆVθt−ˆVθ∗∥2
D+ 2∥ˆV˜θt−ˆVθt∥2
D
(a)
≤2∥ˆVθt−ˆVθ∗∥2
D+ 2∥˜θt−θt∥2,(63)
where for (a), we used Lemma 3. We thus have
−2α(1−γ)∥ˆVθt−ˆVθ∗∥2
D≤−α(1−γ)∥ˆV˜θt−ˆVθ∗∥2
D+ 2α(1−γ)∥˜θt−θt∥2
≤−α(1−γ)∥ˆV˜θt−ˆVθ∗∥2
D+ 2α(1−γ)∥αet−1−ep,t∥2
≤−α(1−γ)∥ˆV˜θt−ˆVθ∗∥2
D+ 4α(1−γ)/parenleftbig
α2∥et−1∥2+∥ep,t∥2/parenrightbig
≤−α(1−γ)∥ˆV˜θt−ˆVθ∗∥2
D+ 1044α3(1−γ)δ2G2,(64)
where we used Lemma 12. Plugging the above bound back in equation 62 yields:
E/bracketleftbig
∥˜θt+1−θ∗∥2
2/bracketrightbig
≤E/bracketleftbig
∥˜θt−θ∗∥2
2/bracketrightbig
−α(1−γ)E/bracketleftig
∥ˆV˜θt−ˆVθ∗∥2
D/bracketrightig
+ 2777α2δ2τG2
≤(1−αω(1−γ))E/bracketleftbig
∥˜θt−θ∗∥2
2/bracketrightbig
+ 2777α2δ2τG2,∀t≥τ,(65)
where we used Lemma 3 in the last step. Unrolling the above recursion starting from t=τ, we obtain
E/bracketleftbig
∥˜θT−θ∗∥2
2/bracketrightbig
≤(1−αω(1−γ))T−τE/bracketleftbig
∥˜θτ−θ∗∥2
2/bracketrightbig
+ 2777α2δ2τG2∞/summationdisplay
k=0(1−αω(1−γ))k
= (1−αω(1−γ))T−τE/bracketleftbig
∥˜θτ−θ∗∥2
2/bracketrightbig
+ 2777ατδ2G2
ω(1−γ).(66)
32Published in Transactions on Machine Learning Research (04/2024)
We now make use of the following equation twice.
˜θt=θt+αet−1−ep,t.
First, setting t=τin the above equation, subtracting θ∗from both sides, and then simplifying, we observe
that
∥˜θτ−θ∗∥≤∥θτ−θ∗∥+α∥eτ−1∥+∥ep,τ∥=O(αδG +G),
where we invoked Lemma 12. Thus,
E/bracketleftbig
∥˜θτ−θ∗∥2
2/bracketrightbig
=O/parenleftbig
α2δ2G2+G2/parenrightbig
.
Using similar arguments as above, one can also show that
∥θT−θ∗∥2≤3∥˜θT−θ∗∥2+ 3α2∥et−1∥2+ 3∥ep,t∥2≤3∥˜θT−θ∗∥2+O(α2δ2G2).
Plugging the two bounds we derived above in equation 66 completes the proof.
33Published in Transactions on Machine Learning Research (04/2024)
D Analysis of EF-TDwithout Projection: Proof of Theorem 1 and Theorem 2
In this section, we will prove Theorem 1. In particular, via a finer analysis relative to that in Appendix C, we
will (i) show that the iterates generated by EF-TDremain bounded without the need for an explicit projection
step to make this happen; and (ii) obtain a tighter bound w.r.t. the distortion parameter δ. At this stage, we
remind the reader about the dynamics we are interested in analyzing:
ht=Qδ(et−1+gt(θt)),
θt+1=θt+αht,
et=et−1+gt(θt)−ht.(67)
To proceed with the analysis of the above dynamics, let us define the perturbed iterate ˜θt≜θt+αet−1.
Using equation 67, we then obtain:
˜θt+1=θt+1+αet
=θt+αht+α(et−1+gt(θt)−ht)
=˜θt+αgt(θt).(68)
The final recursion above looks almost like the TD(0) update, other than the fact that gt(θt)is evaluated at
θt, and not ˜θt. To account for this “mismatch" introduced by the memory-variable et−1, we will analyze the
following composite Lyapunov function:
ψt≜E[˜d2
t+α2∥et−1∥2],where ˜d2
t=∥˜θt−θ∗∥2. (69)
Note that the above energy function captures the joint dynamics of the perturbed iterate and the memory
variable. Our goal is to prove that this energy function decays exponentially over time (up to noise terms).
To that end, we start by establishing a bound on ˜d2
t+1in the following lemma.
Lemma 16. (Bound on Perturbed Iterate ) Suppose the step-size αsatisfiesα≤1/12. For the EF-TD
algorithm, the following bound then holds for ∀t≥0:3
˜d2
t+1≤/parenleftbig
1−αω(1−γ) + 24α2/parenrightbig˜d2
t+6α3
ω(1−γ)∥et−1∥2+ 2α⟨˜θt−θ∗,gt(˜θt)−¯g(˜θt)⟩+ 32α2σ2.(70)
Proof.Subtracting θ∗from each side of equation 68 and then squaring both sides yields:
˜d2
t+1=˜d2
t+ 2α⟨˜θt−θ∗,gt(θt)⟩+α2∥gt(θt)∥2
=˜d2
t+ 2α⟨˜θt−θ∗,gt(˜θt)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(∗)+ 2α⟨˜θt−θ∗,gt(θt)−gt(˜θt)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(∗∗)+α2∥gt(θt)∥2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(∗∗∗). (71)
We now have:
(∗) = 2α⟨˜θt−θ∗,¯g(˜θt)⟩+ 2α⟨˜θt−θ∗,gt(˜θt)−¯g(˜θt)⟩
≤−2αω(1−γ)˜d2
t+ 2α⟨˜θt−θ∗,gt(˜θt)−¯g(˜θt)⟩,(72)
where in the second step, we invoked Lemmas 3 and 4. To bound (∗∗), we proceed as follows:
(∗∗)(a)
≤4α˜dt∥θt−˜θt∥
(b)
≤2α
η˜d2
t+ 2αη∥θt−˜θt∥2
(c)=2α
η˜d2
t+ 2α3η∥et−1∥2,(73)
3The requirement that α≤1/12is not necessary to obtain the type of bound in equation 70. Instead, it only serves to
simplify some of the leading constants in the bound.
34Published in Transactions on Machine Learning Research (04/2024)
whereη >0is a constant to be decided shortly. In the above steps, (a) follows from the Cauchy-Schwarz
inequality and the Lipschitz property of the TD(0) update direction in Lemma 8. For (b), we used the fact
that for any two scalars x,y∈R, the following holds for all η>0,
xy≤1
2ηx2+η
2y2.
Finally, for (c), we simply used the fact that ˜θt−θt=αet−1.To bound (∗∗∗), observe that
∥gt(θt)∥2(a)
≤4(∥θt∥+σ)2
≤8(∥θt∥2+σ2)
(b)
≤8/parenleftbig
3∥θt−˜θt∥2+ 3∥˜θt−θ∗∥2+ 3∥θ∗∥2+σ2/parenrightbig
(c)
≤24α2∥et−1∥2+ 24 ˜d2
t+ 32σ2,(74)
where (a) follows from Lemma 6, (b) follows from equation 19, and (c) follows from noting that ∥θ∗∥≤σ.
Plugging the bounds in equations 72, 73, and 74 in equation 71, we obtain:
˜d2
t+1≤/parenleftbigg
1−2αω(1−γ) +2α
η+ 24α2/parenrightbigg
˜d2
t+ 2α3(η+ 12α)∥et−1∥2+ 2α⟨˜θt−θ∗,gt(˜θt)−¯g(˜θt)⟩
+ 32α2σ2.(75)
The result follows from setting η=2
ω(1−γ), and simplifying using α≤1/12.
Unlike the standard TD(0) analysis, we note from Lemma 16 that the distance to optimality of the iterates is
intimately coupled with the magnitude of the memory variable et. As such, to proceed, we need to bound the
growth of this memory variable. We do so in the following lemma.
Lemma 17. (Bound on Memory Variable ) For the EF-TDalgorithm, the following bound holds for
∀t≥0:
∥et∥2≤/parenleftbigg
1−1
2δ+ 16α2δ/parenrightbigg
∥et−1∥2+ 64δ˜d2
t+ 96δσ2. (76)
Proof.We begin as follows:
∥et∥2=∥et−1+gt(θt)−ht∥2
=∥et−1+gt(θt)−Qδ(et−1+gt(θt))∥2
(a)
≤/parenleftbigg
1−1
δ/parenrightbigg
∥et−1+gt(θt)∥2
(b)
≤/parenleftbigg
1−1
δ/parenrightbigg/parenleftbigg
1 +1
η/parenrightbigg
∥et−1∥2+/parenleftbigg
1−1
δ/parenrightbigg
(1 +η)∥gt(θt)∥2,(77)
where (a) follows from the contraction property of Qδ(·)in equation 4, (b) makes use of the relaxed triangle
inequality in equation 18, and η>0is a constant to be chosen by us shortly. To ensure that ∥et∥contracts
35Published in Transactions on Machine Learning Research (04/2024)
over time, we set η=δ−1to obtain:
∥et∥2≤/parenleftbigg
1−1
2δ/parenrightbigg
∥et−1∥2+ 2δ∥gt(θt)∥2
≤/parenleftbigg
1−1
2δ/parenrightbigg
∥et−1∥2+ 2δ∥gt(θt)−gt(˜θt) +gt(˜θt)∥2
≤/parenleftbigg
1−1
2δ/parenrightbigg
∥et−1∥2+ 4δ∥gt(θt)−gt(˜θt)∥2+ 4δ∥gt(˜θt)∥2
(a)
≤/parenleftbigg
1−1
2δ/parenrightbigg
∥et−1∥2+ 16δ∥θt−˜θt∥2+ 4δ∥gt(˜θt)∥2
=/parenleftbigg
1−1
2δ+ 16α2δ/parenrightbigg
∥et−1∥2+ 4δ∥gt(˜θt)∥2
(b)
≤/parenleftbigg
1−1
2δ+ 16α2δ/parenrightbigg
∥et−1∥2+ 32δ/parenleftbig
∥˜θt∥2+σ2/parenrightbig
≤/parenleftbigg
1−1
2δ+ 16α2δ/parenrightbigg
∥et−1∥2+ 32δ/parenleftbig
2˜d2
t+ 3σ2/parenrightbig
.(78)
In the above steps, for (a) we used the Lipschitz property of the noisy TD(0) update direction, and for (b), we
appealed to Lemma 6. This completes the proof.
D.1 Bounding the Drift and Bias Terms
Inspecting Lemma 16, it is apparent that we need to bound the “bias" term ⟨˜θt−θ∗,gt(˜θt)−¯g(˜θt)⟩. This
requires some work for the following reasons.
•In the (compressed) optimization setting, one does not encounter this term since gt(˜θt)is an unbiased
version of ¯g(˜θt). Thus, taking expectations causes this term to vanish.
•In the standard analysis of TD(0), while one does encounter such a bias term (under Markovian
sampling), such a term features the true iterate θt, and not its perturbed version ˜θt. This is where
we again need to carefully account for the error between θtand˜θt.
•In Appendix C, we derived a bound on the bias term by leveraging the uniform bounds on the
memory variable in Lemma 12. Such uniform bounds were made possible via the projection step.
Since we no longer have such a projection step at our disposal, we need an alternate proof technique.
In order to bound ⟨˜θt−θ∗,gt(˜θt)−¯g(˜θt)⟩, we will require a mixing time argument where we condition
sufficiently into the past. This, in turn, will create the need to bound the drift ∥˜θt−˜θt−τ∥of the perturbed
iterate ˜θt; here, recall that τis the mixing time. In the analysis of vanilla TD(0), the authors in Srikant &
Ying (2019) show how such a drift term can be related to the distance to optimality of the (true) iterate at
timet. The presence of the memory variable et(that accounts for past errors) makes it hard to establish
such a result for our setting. As such, we will now establish a different bound on the drift ∥˜θt−˜θt−τ∥as a
function of the maximum amplitude of our constructed Lyapunov function (in equation 69) over the interval
[t−τ,t]. In this context, we have the following key result (Lemma 1 in the main body of the paper).
Lemma 18. (Relating Drift to Past Shocks ) For EF-TD, the following is true ∀t≥τ:
E[∥˜θt−˜θt−τ∥2]≤12α2τ2max
t−τ≤ℓ≤t−1ψℓ+ 48α2τ2σ2. (79)
36Published in Transactions on Machine Learning Research (04/2024)
Proof.Starting from equation 68, observe that
∥˜θt+1−˜θt∥≤α∥gt(θt)∥
≤α/parenleftbig
∥gt(θt)−gt(˜θt)∥+∥gt(˜θt)∥/parenrightbig
(a)
≤2α/parenleftbig
∥θt−˜θt∥+∥˜θt∥+σ/parenrightbig
≤2α/parenleftbig
α∥et−1∥+∥˜θt∥+σ/parenrightbig
≤2α/parenleftbig
α∥et−1∥+˜dt+ 2σ/parenrightbig
,(80)
where (a) follows from Lemmas 6 and 8. We thus have
E/bracketleftbig
∥˜θt+1−˜θt∥2/bracketrightbig
≤12α2E/bracketleftbig
α2∥et−1∥2+˜d2
t+ 4σ2/bracketrightbig
= 12α2ψt+ 48α2σ2,(81)
where for the first step, we used equation 19, and for the second, the definition of ψtin equation 69. Appealing
to equation 19 again, observe that
E/bracketleftbig
∥˜θt−˜θt−τ∥2/bracketrightbig
≤τt−1/summationdisplay
ℓ=t−τE/bracketleftbig
∥˜θℓ+1−˜θℓ∥2/bracketrightbig
equation 81
≤ 12α2τt−1/summationdisplay
ℓ=t−τ/parenleftbig
ψℓ+ 4σ2/parenrightbig
≤12α2τ2max
t−τ≤ℓ≤t−1ψℓ+ 48α2τ2σ2,(82)
which is the desired claim.
Interpreting ψℓas a “shock" from time-step ℓ, Lemma 18 tells us that the drift of the perturbed iterate over
the interval [t−τ,t]can be bounded above by the maximum shock over this interval (up to noise terms).
Fortunately, the effect of this shock is dampened by the presence of the O(α2)term multiplying it. Equipped
with Lemma 18, we now proceed to bound the bias term.
Lemma 19. (Bounding the Bias ) Suppose Assumption 1 holds. Let the step-size αbe such that ατ≤1/6.
ForEF-TD, the following is then true ∀t≥τ:
E/bracketleftbig
⟨˜θt−θ∗,gt(˜θt)−¯g(˜θt)⟩/bracketrightbig
≤31ατE/bracketleftbig˜d2
t/bracketrightbig
+ 103ατVt+ 454ατσ2, (83)
where
Vt≜ max
t−τ≤ℓ≤t−1ψℓ.
Proof.We start by decomposing the bias term T=⟨˜θt−θ∗,gt(˜θt)−¯g(˜θt)⟩as follows:
T=⟨˜θt−˜θt−τ,gt(˜θt)−¯g(˜θt)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T1+⟨˜θt−τ−θ∗,gt(˜θt)−¯g(˜θt)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T2.
To boundT1, we note that
T1≤∥˜θt−˜θt−τ∥∥gt(˜θt)−¯g(˜θt)∥
≤1
2ατ∥˜θt−˜θt−τ∥2+ατ
2∥gt(˜θt)−¯g(˜θt)∥2
≤1
2ατ∥˜θt−˜θt−τ∥2+ατ/parenleftbig
∥gt(˜θt)∥2+∥¯g(˜θt)∥2/parenrightbig
(a)
≤1
2ατ∥˜θt−˜θt−τ∥2+ 10ατ(∥˜θt∥2+σ2)
≤1
2ατ∥˜θt−˜θt−τ∥2+ 10ατ(2˜d2
t+ 3σ2),(84)
37Published in Transactions on Machine Learning Research (04/2024)
where for (a), we used Lemma 6 and equation 16. Taking expectations on both sides of the above inequality,
and using Lemma 18, we obtain:
E[T1]≤20ατE/bracketleftbig˜d2
t/bracketrightbig
+ 6ατVt+ 54ατσ2. (85)
Next, to bound T2, we decompose it as follows:
T2=⟨˜θt−τ−θ∗,gt(˜θt−τ)−¯g(˜θt−τ)⟩/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(∗)+⟨˜θt−τ−θ∗,gt(˜θt)−gt(˜θt−τ)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
(∗∗)+⟨˜θt−τ−θ∗,¯g(˜θt−τ)−¯g(˜θt)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(∗∗∗).
We now proceed to bound each of the three terms above. Observe:
(∗∗)≤˜dt−τ∥gt(˜θt)−gt(˜θt−τ)∥
(a)
≤2˜dt−τ∥˜θt−˜θt−τ∥
≤2(˜dt+∥˜θt−˜θt−τ∥)∥˜θt−˜θt−τ∥
(b)
≤2/parenleftbigg√ατ˜dt+∥˜θt−˜θt−τ∥√ατ/parenrightbigg2
≤4/parenleftbigg
ατ˜d2
t+∥˜θt−˜θt−τ∥2
ατ/parenrightbigg
,(86)
where for (a), we used the Lipschitz property in Lemma 8, and for (b), we used the fact that ατ≤1.Taking
expectations on each side of the above inequality and appealing to Lemma 18, we obtain
E[(∗∗)]≤4ατE/bracketleftbig˜d2
t/bracketrightbig
+ 48ατVt+ 192ατσ2. (87)
Using Lemma 7 and the same arguments as above, one can establish the exact same bound on E[(∗∗∗)]as
in equation 87. Before proceeding to bound (∗), we make the observation that ˜θtinherits its randomness
from all the Markov data tuples up to time t−1, i.e., from{Xk}t−1
k=0. We now have:
E[(∗)] =E/bracketleftbig
⟨˜θt−τ−θ∗,gt(˜θt−τ)−¯g(˜θt−τ)⟩/bracketrightbig
=E/bracketleftbig
E/bracketleftbig
⟨˜θt−τ−θ∗,gt(˜θt−τ)−¯g(˜θt−τ)⟩|˜θt−τ,Xt−τ/bracketrightbig/bracketrightbig
=E/bracketleftbig
⟨˜θt−τ−θ∗,E/bracketleftbig
gt(˜θt−τ)−¯g(˜θt−τ)|˜θt−τ,Xt−τ/bracketrightbig
⟩/bracketrightbig
≤E/bracketleftbig˜dt−τ∥E/bracketleftbig
gt(˜θt−τ)−¯g(˜θt−τ)|˜θt−τ,Xt−τ/bracketrightbig
∥/bracketrightbig
(a)
≤αE/bracketleftbig˜dt−τ/parenleftbig
∥˜θt−τ∥+ 1/parenrightbig/bracketrightbig
≤αE/bracketleftbig˜dt−τ/parenleftbig
∥˜θt−τ−˜θt∥+˜dt+∥θ∗∥+ 1/parenrightbig/bracketrightbig
≤αE/bracketleftbig
(˜dt+∥˜θt−˜θt−τ∥)/parenleftbig
∥˜θt−τ−˜θt∥+˜dt+∥θ∗∥+ 1/parenrightbig/bracketrightbig
≤αE/bracketleftbig
(˜dt+∥˜θt−˜θt−τ∥)/parenleftbig
∥˜θt−τ−˜θt∥+˜dt+ 2σ/parenrightbig/bracketrightbig
(b)
≤ατE/bracketleftbig
(˜dt+∥˜θt−˜θt−τ∥+ 2σ)2/bracketrightbig
≤3ατE/bracketleftbig˜d2
t/bracketrightbig
+ 3ατE/bracketleftbig
∥˜θt−˜θt−τ∥2/bracketrightbig
+ 12ατσ2
(c)
≤3ατE/bracketleftbig˜d2
t/bracketrightbig
+ατVt+ 16ατσ2.(88)
In the above steps, (a) follows from the mixing property in Definition 1, (b) follows from the fact that τ≥1,
and (c) follows by invoking Lemma 18 and simplifying using ατ≤1/6.Combining the above bound with
that in equation 87, we conclude that
E[T2]≤11ατE/bracketleftbig˜d2
t/bracketrightbig
+ 97ατVt+ 400ατσ2.
Combining the above bound with that in equation 85 completes the proof.
38Published in Transactions on Machine Learning Research (04/2024)
We now have all the pieces needed to prove Theorem 1.
Proof.(Proof of Theorem 1 ) We break up the proof into two parts. In the first step, we establish a
recursion for our potential function ψt. In the second step, we analyze this recursion by making a connection
to the analysis of the Incremental Aggregated Gradient ( IAG) algorithm in Gurbuzbalaban et al. (2017).
Step 1: Establishing a Recursion for ψt. Combining the bound on the bias term in Lemma 19 with
Lemma 16, and simplifying using τ≥1, we obtain the following inequality ∀t≥τ:
E/bracketleftbig˜d2
t+1/bracketrightbig
≤/parenleftbig
1−αω(1−γ) + 86α2τ/parenrightbig
E/bracketleftbig˜d2
t/bracketrightbig
+ 206α2τVt+6α3
ω(1−γ)E/bracketleftbig
∥et−1∥2/bracketrightbig
+ 940α2τ2σ2.
Combining the above display with the bound on the memory variable in Lemma 17, and using the definition
of the potential function ψt, we then obtain∀t≥τ:
ψt+1≤/parenleftbig
1−αω(1−γ) + 86α2τ+ 64α2δ/parenrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
A1E/bracketleftbig˜d2
t/bracketrightbig
+ 206α2τVt
+α2/parenleftbigg
1−1
2δ+ 16α2δ+6α
ω(1−γ)/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
A2E/bracketleftbig
∥et−1∥2/bracketrightbig
+α2(940τ+ 96δ)σ2.(89)
Our immediate goal is to pick αsuch that max{A1,A2}<1. Accordingly, it is easy to check that if
α≤ω(1−γ)
344τandα≤ω(1−γ)
256δ, (90)
then
A1≤1−αω(1−γ)
2.
It is also easily verified that with the choice of step-size in equation 90, the following hold:
16α2δ≤1
8δand6α
ω(1−γ)≤1
8δ,
implying that
A2≤1−1
4δ.
Finally, note that based on the choice of αin equation 90, we have
1−1
4δ≤1−αω(1−γ)
2.
Combining all the above observations, we obtain that for all t≥τ:
ψt+1≤/parenleftbigg
1−αω(1−γ)
2/parenrightbigg
ψt+ 206α2τ/parenleftbigg
max
t−τ≤ℓ≤tψℓ/parenrightbigg
+O(α2(τ+δ)σ2). (91)
If the second term (i.e., the “max" term) in the above bound were absent, one could easily unroll the resulting
recursion and argue linear convergence to a noise ball. In what follows, we will show that one can still
establish such linear convergence guarantees for equation 91.
Step 2: Analyzing equation 91 via a connection to the IAGalgorithm. To see how we can
analyze equation 91, we take a quick detour and recap the basic idea behind the IAGmethod for finite-sum
optimization. Say we want to minimize
f(x) =1
M/summationdisplay
i∈[M]fi(x),
39Published in Transactions on Machine Learning Research (04/2024)
where each component function is smooth. The IAGmethod does so in a computationally-efficient manner by
processing each of the component functions one at a time in a deterministic order, and crucially, by maintaining
amemory of the most recent gradient values of each of the component functions. This memory introduces
certaindelayedgradient terms in the update rule. In Gurbuzbalaban et al. (2017), it was shown that the
presence of these delayed terms leads to a recursion of the form in equation 91. This turns out to be the key
observation needed to complete our analysis. In particular, we recall an important lemma from Feyzmahdavian
et al. (2014) (used in Gurbuzbalaban et al. (2017)) that will help us reason about equation 91.
Lemma 20. Let{Gt}be a sequence of non-negative real numbers satisfying
Gt+1≤pGt+qmax
(t−τt)+≤ℓ≤tGℓ+r, t∈N,
for some non-negative constants p,q,andr. Here, for any real scalar x, we use the notation (x)+=max{x,0}.
Ifp+q<1and0≤τt≤τmax,∀t≥0for some positive constant τmax, then
Gt≤ρtG0+ε,∀t≥0,
where
ρ= (p+q)1
1+τmax,andε=r
(1−p−q).
Comparing equation 91 to Lemma 20, we note that for us:
p= 1−αω(1−γ)
2,q= 206α2τ,r=O(α2(τ+δ)σ2),andτt=τ,
whereτis the mixing time. Now suppose αis chosen such that
α≤ω(1−γ)
824τ. (92)
Then, we immediately obtain that
p+q= 1−αω(1−γ)
2+ 206α2τ <1−αω(1−γ)
4.
SettingC1= max 0≤k≤τψk, and appealing to Lemma 20 then yields the following ∀T≥τ:
ψT≤C1/parenleftbigg
1−αω(1−γ)
8τ/parenrightbiggT−τ
+O/parenleftbiggα(τ+δ)σ2
ω(1−γ)/parenrightbigg
,
where we used the facts that (1−x)a≤1−axforx,a∈[0,1], andτ≥1, to simplify the final expression.
Next, note that
E/bracketleftbig
∥θT−θ∗∥2/bracketrightbig
=E/bracketleftig
∥θT−˜θT+˜θT−θ∗∥2/bracketrightig
≤2E/bracketleftig
∥˜θT−θ∗∥2/bracketrightig
+ 2E/bracketleftig
∥θT−˜θT∥2/bracketrightig
= 2E/bracketleftbig˜d2
t/bracketrightbig
+ 2α2E/bracketleftig
∥eT−1∥2/bracketrightig
= 2ψT.(93)
We conclude that ∀T≥τ,
E/bracketleftbig
r2
T/bracketrightbig
≤2C1/parenleftbigg
1−αω(1−γ)
8τ/parenrightbiggT−τ
+O/parenleftbiggα(τ+δ)σ2
ω(1−γ)/parenrightbigg
.
Furthermore, from the requirements on the step-size αin equations 90 and 92, we note that for the above
inequality to hold, it suffices for αto satisfy:
α≤ω(1−γ)
824 max{τ,δ}.
40Published in Transactions on Machine Learning Research (04/2024)
The only thing that remains to be shown is that C1=max 0≤k≤τψk=O(d2
0+σ2)based on our choice of
step-size above. This follows from straightforward calculations that we provide below for completeness.
From equation 68, we have
˜d2
t+1=˜d2
t+ 2α⟨˜θt−θ∗,gt(θt)⟩+α2∥gt(θt)∥2
≤(1 +α)˜d2
t+ 2α∥gt(θt)∥2
≤(1 + 49α)˜d2
t+ 48α3∥et−1∥2+ 64ασ2,(94)
where in the last step, we used equation 74. Combining the above bound with Lemma 17, we obtain the
following inequality for all t≥0:
ψt+1≤(1 + 49α+ 64α2δ)E/bracketleftbig˜d2
t/bracketrightbig
+α2/parenleftbigg
1−1
2δ+ 16α2δ+ 48α/parenrightbigg
E/bracketleftbig
∥et−1∥2/bracketrightbig
+ (64α+ 96α2δ)σ2.(95)
Using the fact that αδ≤1/824,we can simplify the above display to obtain
ψt+1≤(1 + 50α)ψt+ 65ασ2,∀t≥0. (96)
Unrolling the above inequality and using e−1= 0, we have that for 0≤k≤τ:
ψk≤(1 + 50α)kd2
0+ 65ασ2k−1/summationdisplay
j=0(1 + 50α)j. (97)
Now since (1 +x)≤ex,∀x∈R, andα≤1/(200τ), note that (1 + 50α)k≤(1 + 50α)τ≤e0.25≤2.Thus, for
0≤k≤τ, we have
ψk≤2d2
0+ 130ατσ2≤2d2
0+σ2,
where in the last step, we used 130ατ≤1.Thus,C1=max 0≤k≤τψk=O(d2
0+σ2). This concludes the
proof.
We conclude this section with a note on the proof of Theorem 2.
Proof of Theorem 2 : A careful inspection of the proof of Theorem 1 reveals that we never explicitly used
the fact that the TD update direction is an affine function of the parameter θ. This was done on purpose to
provide a unified analysis framework to not only reason about linear stochastic approximation schemes with
error-feedback, but also their nonlinear counterparts. As such, under Assumptions 2 and 3, the analysis for
the nonlinear setting in Section 5 follows exactlythe same steps as the proof of Theorem 1. All one needs to
do is replace ω(1−γ)in the proof of Theorem 1 with β, whereβis as in Assumption 3. Everything else
essentially remains the same. We thus omit routine details here.
41Published in Transactions on Machine Learning Research (04/2024)
E Analysis of Multi-Agent EF-TD: Proof of Theorem 3
In this section, we will analyze the multi-agent version of EF-TDoutlined in Algorithm 2. The main technical
challenge relative to the single-agent analysis conducted in Appendices C and D is in establishing the
linear speedup property w.r.t. the number of agents Munder the Markovian sampling assumption. As we
mentioned earlier in the paper, this turns out to be highly non-trivial even in the absence of compression and
error-feedback. The only work that establishes such a speedup (under Markovian sampling) is Khodadadian
et al. (2022), where the authors use the framework of Generalized Moreau Envelopes to perform their analysis.
Although the analysis in Khodadadian et al. (2022) is elegant, it is quite involved, and it is unclear whether
their framework can accommodate the error-feedback mechanism. Moreover, the analysis in Khodadadian
et al. (2022) leads to a sub-optimal O(τ2)dependence on the mixing time τin the main noise/variance term.
In light of the above discussion, we will provide a different analysis in this section that:
•Departs from the Moreau Envelope approach in Khodadadian et al. (2022),
•Achieves the optimal O(τ)dependence on the mixing time τin the dominant noise term,
•Establishes the desired linear speedup property, and
•Shows that the effect of the distortion parameter δcan be relegated to a higher-order term.
Crucial to achieving all of the above desiderata are a few novel ingredients in the proof that we now outline.
First, we will require a more refined Lyapunov function than we used earlier. Before we introduce this
Lyapunov function, let us define a couple of objects:
¯et≜1
M/summationdisplay
i∈[M]ei,t,and¯ht≜1
M/summationdisplay
i∈[M]hi,t.
Next, let us define a perturbed iterate for this setting as ˜θt≜θt+α¯et−1. The potential function we employ
is as follows:
Ξt≜E/bracketleftbig
∥˜θt−θ∗∥2/bracketrightbig
+Cα3Et−1,whereEt≜1
ME/bracketleftiggM/summationdisplay
i=1∥ei,t∥2/bracketrightigg
, (98)
andC > 1is a constant that will be chosen by us later. Compared to the Lyapunov function ψtin equation 69
that we used for the single-agent setting, Ξtdiffers in two ways. First, it incorporates the memory dynamics
ofallthe agents. A more subtle difference, however, stems from the fact that the second term of Ξtis scaled
byα3, and notα2(as in equation 69). This higher-order dependence on αwill serve a twofold purpose: (i)
help to shift the effect of δto a higher-order term, and (ii) partially help in achieving the linear-speedup
property. Unfortunately, however, the new Lyapunov function on its own will not suffice in terms of achieving
the linear speedup effect completely. For this, we need a careful way to bound the norm of the average TD
direction defined below:
zt(θ)≜1
M/summationdisplay
i∈[M]gi,t(θ),∀θ∈RK. (99)
An immediate way to bound zt(θ)is to appeal to the bound on the norm of the TD update direction in
Lemma 6. Indeed, this is what we did while proving Theorem 1, and this is also the typical approach for
bounding norms of TD update directions in the centralized setting (Srikant & Ying, 2019; Chen et al., 2019).
The issue with adopting this approach in the multi-agent analysis is that it completely ignores the fact that
the observations of the agents are statistically independent . As such, following this route will not lead to
any “variance-reduction" effect (key to the linear speedup property). At the same time, it is important to
realize here that while the Markov data tuples are independent across agents, for any fixed agent i,{Xi,t}
comes from a single Markov chain. This makes it trickier to analyze the variance of zt(θt). Via a careful
mixing time argument, our next result (Lemma 2 in the main body of the paper) shows how this can be
done. Before stating this result, we remind the reader that we have used τas a shorthand for τϵ, withϵ=α2.
While a precision of ϵ=αsufficed for all our prior single-agent analyses, we will need ϵ=α2for the MARL
case (to create higher-order noise terms in α).
42Published in Transactions on Machine Learning Research (04/2024)
Lemma 21. (Controlling the norm of the Average TD Direction ) Suppose Assumption 1 holds. For
Algorithm 2, the following are then true ∀t≥τ:
E/bracketleftbig
∥zt(θt)∥2/bracketrightbig
≤8E/bracketleftbig
d2
t/bracketrightbig
+/parenleftbigg32
M+ 8α4/parenrightbigg
σ2,wheredt=∥θt−θ∗∥,and (100)
E/bracketleftbig
∥zt(θt)∥2/bracketrightbig
≤16E/bracketleftbig˜d2
t/bracketrightbig
+ 16α2Et−1+/parenleftbigg32
M+ 8α4/parenrightbigg
σ2,where ˜dt=∥˜θt−θ∗∥. (101)
Proof.Let us start with the following set of observations:
∥zt(θt)∥2=1
M2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleM/summationdisplay
i=1gi,t(θt)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=1
M2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleM/summationdisplay
i=1(gi,t(θt)−gi,t(θ∗)) +M/summationdisplay
i=1gi,t(θ∗)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤2
M2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleM/summationdisplay
i=1(gi,t(θt)−gi,t(θ∗))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+2
M2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleM/summationdisplay
i=1gi,t(θ∗)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤2
MM/summationdisplay
i=1∥gi,t(θt)−gi,t(θ∗)∥2+2
M2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleM/summationdisplay
i=1gi,t(θ∗)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
≤8d2
t+2
M2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleM/summationdisplay
i=1gi,t(θ∗)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
,(102)
where in the last step, we used the Lipschitz property in Lemma 8. Next, to bound the second term in the
above display, we split it into two parts as follows.
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleM/summationdisplay
i=1gi,t(θ∗)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=M/summationdisplay
i=1∥gi,t(θ∗)∥2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(∗)+M/summationdisplay
i,j=1
i̸=j⟨gi,t(θ∗),gj,t(θ∗)⟩
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(∗∗).
To bound (∗), we simply use Lemma 6 and the fact that ∥θ∗∥≤σto conclude that
M/summationdisplay
i=1∥gi,t(θ∗)∥2≤8M(∥θ∗∥2+σ2)≤16Mσ2.
Now to bound (∗∗), let us zoom in on a particular cross-term, and write it out in a way that highlights the
sources of randomness. Accordingly, consider the term
T=⟨gi,t(θ∗),gj,t(θ∗)⟩=⟨g(Xi,t,θ∗),g(Xj,t,θ∗)⟩.
Sinceθ∗is deterministic, we note that the randomness in Toriginates from the Markov data samples Xi,t
andXj,t. Moreover, since Xi,tandXj,tare independent for i̸=j, we have
E[T] =⟨E[g(Xi,t,θ∗)],E[g(Xj,t,θ∗)]⟩.
Now ifXi,tandXj,twere sampled i.i.d. from the stationary distribution π, each of the two expectations
within the above inner-product would have amounted to ¯g(θ∗) = 0. Thus, the cross-terms would have
vanished. Since for each agent i,Xi,tcomes from a Markov chain, these expectations do not, unfortunately,
43Published in Transactions on Machine Learning Research (04/2024)
vanish any longer. Nonetheless, we now show that for t≥τ, one can still make the cross-terms suitably
“small" by exploiting the mixing property in Definition 1. Observe:
E[T] =⟨E[g(Xi,t,θ∗)],E[g(Xj,t,θ∗)]⟩
(a)=⟨E[E[g(Xi,t,θ∗)|Xi,t−τ]−¯g(θ∗)],E[E[g(Xj,t,θ∗)|Xj,t−τ]−¯g(θ∗)]⟩
(b)
≤∥E[E[g(Xi,t,θ∗)|Xi,t−τ]−¯g(θ∗)]∥×∥E[E[g(Xj,t,θ∗)|Xj,t−τ]−¯g(θ∗)]∥
(c)
≤E[∥E[g(Xi,t,θ∗)|Xi,t−τ]−¯g(θ∗)∥]×E[∥E[g(Xj,t,θ∗)|Xj,t−τ]−¯g(θ∗)∥]
(d)
≤α4(∥θ∗∥+ 1)2
≤4σ2α4.(103)
In the above steps, (a) follows from the tower property of expectation in conjunction with ¯g(θ∗) = 0. For (b),
we used the Cauchy-Schwarz inequality, and (c) follows from Jensen’s inequality. Finally, (d) is a consequence
of the mixing property in Definition 1. We immediately conclude:
E[(∗∗)]≤4M2σ2α4.
Putting the above pieces together and simplifying leads to equation 100. To go from equation 100 to equa-
tion 101, we simply note that
E/bracketleftbig
d2
t/bracketrightbig
≤2E/bracketleftbig˜d2
t/bracketrightbig
+ 2α2E/bracketleftbig
∥˜θt−θt∥2/bracketrightbig
= 2E/bracketleftbig˜d2
t/bracketrightbig
+ 2α2E/bracketleftbig
∥¯et−1∥2/bracketrightbig
≤2E/bracketleftbig˜d2
t/bracketrightbig
+ 2α2Et−1.(104)
This concludes the proof.
Essentially, Lemma 21 shows how the variance of the average TD update direction can be scaled down by
a factor of M, up to aO(α4)term. As we shall soon see, this result will play a key role in our subsequent
analysis of Algorithm 2. We now proceed to derive a multi-agent version of Lemma 16.
Lemma 22. Suppose Assumption 1 holds and the step-size αsatisfiesα≤1/16. For Algorithm 2, the
following bound then holds for ∀t≥τ:
E/bracketleftbig˜d2
t+1/bracketrightbig
≤/parenleftbig
1−αω(1−γ) + 16α2/parenrightbig
E/bracketleftbig˜d2
t/bracketrightbig
+5α3
ω(1−γ)Et−1+ 2αE[A] +α2/parenleftbigg32
M+ 8α4/parenrightbigg
σ2,(105)
whereA=⟨˜θt−θ∗,zt(˜θt)−¯g(˜θt)⟩.
Proof.Simple calculations reveal that
˜θt+1=˜θt+α
1
M/summationdisplay
i∈[M]gi,t(θt)
=˜θt+αzt(θt). (106)
Subtracting θ∗from each side of the above equation and then squaring both sides yields:
˜d2
t+1=˜d2
t+ 2α⟨˜θt−θ∗,zt(θt)⟩+α2∥zt(θt)∥2
=˜d2
t+ 2α⟨˜θt−θ∗,zt(˜θt)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(∗)+ 2α⟨˜θt−θ∗,zt(θt)−zt(˜θt)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(∗∗)+α2∥zt(θt)∥2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(∗∗∗). (107)
We now have:
(∗) = 2α⟨˜θt−θ∗,¯g(˜θt)⟩+ 2α⟨˜θt−θ∗,zt(˜θt)−¯g(˜θt)⟩
≤−2αω(1−γ)˜d2
t+ 2α⟨˜θt−θ∗,zt(˜θt)−¯g(˜θt)⟩,(108)
44Published in Transactions on Machine Learning Research (04/2024)
where in second step, we invoked Lemmas 3 and 4. To bound (∗∗), we proceed as follows:
(∗∗)≤4α˜dt∥θt−˜θt∥
≤2α
η˜d2
t+ 2αη∥θt−˜θt∥2
=2α
η˜d2
t+ 2α3η∥¯et−1∥2
≤2α
η˜d2
t+ 2α3ηEt−1,(109)
whereη>0is a constant to be decided shortly. In the first step above, we used the fact that since each gi,t(θ)
is2-Lipschitz (see Lemma 8), the definition of zt(θ)in equation 99 implies that zt(θ)is also 2-Lipschitz. To
bound (∗∗∗), we directly use Lemma 21. Combining the above bounds, and simplifying by setting η=2
ω(1−γ)
and usingα≤1/16leads to the desired claim.
In what follows, we will focus on bounding the bias term A=⟨˜θt−θ∗,zt(˜θt)−¯g(˜θt)⟩by following the same
high-level steps as in the proof of Theorem 1. The key difference, however, will come from invoking Lemma 21
instead of Lemma 6. We start with a bound on the drift ∥˜θt−˜θt−τ∥.
Lemma 23. Suppose Assumption 1 holds. Then for Algorithm 2, we have the following bound ∀t≥2τ:
E[∥˜θt−˜θt−τ∥2]≤α2τ2max
t−τ≤ℓ≤t−1Gℓ,whereGℓ≜16E/bracketleftbig˜d2
ℓ/bracketrightbig
+ 16α2Eℓ−1+/parenleftbigg32
M+ 8α4/parenrightbigg
σ2.(110)
Proof.The proof is a direct application of Lemma 21. Indeed, notice that
E/bracketleftbig
∥˜θt−˜θt−τ∥2/bracketrightbig
≤τt−1/summationdisplay
ℓ=t−τE/bracketleftbig
∥˜θℓ+1−˜θℓ∥2/bracketrightbig
equation 106
≤α2τt−1/summationdisplay
ℓ=t−τE/bracketleftbig
∥zℓ(θℓ)∥2/bracketrightbig
equation 101
≤α2τ2max
t−τ≤ℓ≤t−1Gℓ,(111)
which is the desired claim. In the last step, we invoked Lemma 21 by noting that since t≥2τ, we have that
ℓ≥τin the above steps - a requirement for using Lemma 21.
Equipped with Lemmas 21 and 23, we now proceed to bound the bias term following steps similar in spirit to
the proof of Lemma 19.
Lemma 24. Suppose Assumption 1 holds. Let the step-size αbe such that ατ≤1/3. For Algorithm 2, the
following is then true ∀t≥2τ:
E/bracketleftbig
⟨˜θt−θ∗,zt(˜θt)−¯g(˜θt)⟩/bracketrightbig
≤13ατE/bracketleftbig˜d2
t/bracketrightbig
+ 11ατmax
t−τ≤ℓ≤tGℓ+ 12α2σ2. (112)
Proof.We start by decomposing the bias term ⟨˜θt−θ∗,zt(˜θt)−¯g(˜θt)⟩as follows:
A=⟨˜θt−˜θt−τ,zt(˜θt)−¯g(˜θt)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
A1+⟨˜θt−τ−θ∗,zt(˜θt)−¯g(˜θt)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
A2.
45Published in Transactions on Machine Learning Research (04/2024)
To boundA1, we note that
A1≤∥˜θt−˜θt−τ∥∥zt(˜θt)−¯g(˜θt)∥
≤1
2ατ∥˜θt−˜θt−τ∥2+ατ
2∥zt(˜θt)−¯g(˜θt)∥2
≤1
2ατ∥˜θt−˜θt−τ∥2+ατ/parenleftbig
∥zt(˜θt)∥2+∥¯g(˜θt)∥2/parenrightbig
≤1
2ατ∥˜θt−˜θt−τ∥2+ατ/parenleftbig
∥zt(˜θt)∥2+ 4˜d2
t/parenrightbig
,(113)
where the last step follows from Lemmas 3 and 5. Taking expectations on both sides of the above inequality,
and using Lemmas 21 and 23 yields:
E[A1]≤4ατE/bracketleftbig˜d2
t/bracketrightbig
+ατ
2max
t−τ≤ℓ≤t−1Gℓ+ατGt
≤4ατE/bracketleftbig˜d2
t/bracketrightbig
+ 2ατmax
t−τ≤ℓ≤tGℓ.(114)
Next, to bound A2, we decompose it as follows:
A2=⟨˜θt−τ−θ∗,zt(˜θt−τ)−¯g(˜θt−τ)⟩/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(∗)+⟨˜θt−τ−θ∗,zt(˜θt)−zt(˜θt−τ)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
(∗∗)+⟨˜θt−τ−θ∗,¯g(˜θt−τ)−¯g(˜θt)⟩/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(∗∗∗).
We now proceed to bound each of the three terms above. Let us start by observing that
(∗∗)≤˜dt−τ∥zt(˜θt)−zt(˜θt−τ)∥
(a)
≤2˜dt−τ∥˜θt−˜θt−τ∥
≤2(˜dt+∥˜θt−˜θt−τ∥)∥˜θt−˜θt−τ∥
(b)
≤2/parenleftbigg√ατ˜dt+∥˜θt−˜θt−τ∥√ατ/parenrightbigg2
≤4/parenleftbigg
ατ˜d2
t+∥˜θt−˜θt−τ∥2
ατ/parenrightbigg
,(115)
where for (a), we used the fact that zt(θ)is 2-Lipschitz, and for (b), we used the fact that ατ≤1.Taking
expectations on each side of the above inequality and invoking Lemma 23, we obtain
E[(∗∗)]≤4ατE/bracketleftbig˜d2
t/bracketrightbig
+ 4ατmax
t−τ≤ℓ≤tGℓ. (116)
46Published in Transactions on Machine Learning Research (04/2024)
As before, the exact same bound as in the above display applies to E[(∗∗∗)]. We now turn to the main step
in this proof.
E[(∗)] =E/bracketleftbig
⟨˜θt−τ−θ∗,zt(˜θt−τ)−¯g(˜θt−τ)⟩/bracketrightbig
=E/bracketleftigg
⟨˜θt−τ−θ∗,1
MM/summationdisplay
i=1/parenleftbig
gi,t(˜θt−τ)−¯g(˜θt−τ)/parenrightbig
⟩/bracketrightigg
=E/bracketleftigg
E/bracketleftigg
⟨˜θt−τ−θ∗,1
MM/summationdisplay
i=1/parenleftbig
gi,t(˜θt−τ)−¯g(˜θt−τ)/parenrightbig
⟩|˜θt−τ,{Xj,t−τ}j∈[M]/bracketrightigg/bracketrightigg
=E/bracketleftigg
⟨˜θt−τ−θ∗,1
MM/summationdisplay
i=1/parenleftbig
E/bracketleftbig
gi,t(˜θt−τ)|˜θt−τ,{Xj,t−τ}j∈[M]/bracketrightbig
−¯g(˜θt−τ)/parenrightbig
⟩/bracketrightigg
(a)=E/bracketleftigg
⟨˜θt−τ−θ∗,1
MM/summationdisplay
i=1/parenleftbig
E/bracketleftbig
gi,t(˜θt−τ)|˜θt−τ,Xi,t−τ/bracketrightbig
−¯g(˜θt−τ)/parenrightbig
⟩/bracketrightigg
(b)
≤E/bracketleftigg
˜dt−τ1
MM/summationdisplay
i=1/vextenddouble/vextenddoubleE/bracketleftbig
gi,t(˜θt−τ)|˜θt−τ,Xi,t−τ/bracketrightbig
−¯g(˜θt−τ)/vextenddouble/vextenddouble/bracketrightigg
(c)
≤α2E/bracketleftbig˜dt−τ/parenleftbig
∥˜θt−τ∥+ 1/parenrightbig/bracketrightbig
(d)
≤3α2E/bracketleftbig˜d2
t/bracketrightbig
+ 3α2E/bracketleftbig
∥˜θt−˜θt−τ∥2/bracketrightbig
+ 12α2σ2
(e)
≤3α2E/bracketleftbig˜d2
t/bracketrightbig
+ 3α4τ2max
t−τ≤ℓ≤t−1Gℓ+ 12α2σ2
(f)
≤ατE/bracketleftbig˜d2
t/bracketrightbig
+ατmax
t−τ≤ℓ≤tGℓ+ 12α2σ2.(117)
In the above steps, (a) follows from the fact that the Markov data tuples are independent across agents; (b)
follows from the Cauchy-Schwarz and the triangle inequality; (c) is a consequence of the mixing property in
Definition 1; for (d), we used steps similar to those for arriving at equation 88; for (e), we used the bound on
the drift from Lemma 23; and finally for (f), we simplified terms using ατ≤1/3andτ≥1. Combining the
above bounds, we obtain
E[A2]≤9ατE/bracketleftbig˜d2
t/bracketrightbig
+ 9ατmax
t−τ≤ℓ≤tGℓ+ 12α2σ2.
The above display, in tandem with equation 114, leads to the claim of the lemma.
47Published in Transactions on Machine Learning Research (04/2024)
We are now ready to prove Theorem 3.
Proof.(Proof of Theorem 3 ) As in the proof of Theorem 1, our first step is to establish a recursion for the
Lyapunov function Ξtin equation 98. To that end, appealing to Lemmas 22 and 24, using the definition of
Gℓ, and some algebra leads to the following bound for all t≥2τ:
E/bracketleftbig˜d2
t+1/bracketrightbig
≤/parenleftbig
1−αω(1−γ) + 42α2τ/parenrightbig
E/bracketleftbig˜d2
t/bracketrightbig
+5α3
ω(1−γ)Et−1
+ 352α2τmax
t−τ≤ℓ≤tE/bracketleftbig˜d2
ℓ/bracketrightbig
+ 352α4τmax
t−τ≤ℓ≤tEℓ−1+ 24α2/parenleftbigg
τ/parenleftbigg32
M+ 8α4/parenrightbigg
+α/parenrightbigg
σ2.(118)
Now similar to the proof of Lemma 17, we have the following for each i∈[M]:
E/bracketleftbig
∥ei,t∥2/bracketrightbig
≤/parenleftbigg
1−1
2δ/parenrightbigg
E/bracketleftbig
∥ei,t−1∥2/bracketrightbig
+ 2δE/bracketleftbig
∥gi,t(θt)∥2/bracketrightbig
≤/parenleftbigg
1−1
2δ/parenrightbigg
E/bracketleftbig
∥ei,t−1∥2/bracketrightbig
+ 16δE/bracketleftbig
∥θt−˜θt∥2/bracketrightbig
+ 4δE/bracketleftbig
∥gi,t(˜θt)∥2/bracketrightbig
≤/parenleftbigg
1−1
2δ/parenrightbigg
E/bracketleftbig
∥ei,t−1∥2/bracketrightbig
+ 16α2δEt−1+ 64δE/bracketleftbig˜d2
t/bracketrightbig
+ 96δσ2.(119)
Averaging the above bound across all agents then yields:
Et≤/parenleftbigg
1−1
2δ+ 16α2δ/parenrightbigg
Et−1+ 64δE/bracketleftbig˜d2
t/bracketrightbig
+ 96δσ2. (120)
Combining the above display with equation 118, and using the definition of Ξt, we obtain∀t≥2τ:
Ξt+1≤/parenleftbig
1−αω(1−γ) + 42α2τ+ 64Cδα3/parenrightbig
E/bracketleftbig˜d2
t/bracketrightbig
+Cα3/parenleftbigg
1−1
2δ+ 16α2δ+5
Cω(1−γ)/parenrightbigg
Et−1
+ 352α2τmax
t−τ≤ℓ≤tE/bracketleftbig˜d2
ℓ/bracketrightbig
+ 352α4τmax
t−τ≤ℓ≤tEℓ−1+R
≤/parenleftbig
1−αω(1−γ) + 42α2τ+ 64Cδα3/parenrightbig
E/bracketleftbig˜d2
t/bracketrightbig
+Cα3/parenleftbigg
1−1
2δ+ 16α2δ+5
Cω(1−γ)/parenrightbigg
Et−1
+ 352α2τmax
t−τ≤ℓ≤tΞℓ+352ατ
Cmax
t−τ≤ℓ≤tCα3Eℓ−1+R
≤/parenleftbig
1−αω(1−γ) + 42α2τ+ 64Cδα3/parenrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B1E/bracketleftbig˜d2
t/bracketrightbig
+Cα3/parenleftbigg
1−1
2δ+ 16α2δ+5
Cω(1−γ)/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B2Et−1
+ 352ατ/parenleftbigg
α+1
C/parenrightbigg
max
t−τ≤ℓ≤tΞℓ+R,(121)
where
R= 24α2/parenleftbigg
τ/parenleftbigg32
M+ 8α4/parenrightbigg
+α/parenrightbigg
σ2+ 96Cα3δσ2.
Our goal is to now carefully pick αandCso as to ensure that max{B1,B2}<1.Let us start with B2.
Suppose
α<1
12δandC=2816 max{δ,τ}
ω(1−γ). (122)
It is easy to then verify that
B2<1−1
4δ.
As forB1, we note that if
α≤ω(1−γ)
850 max{δ,τ}, (123)
48Published in Transactions on Machine Learning Research (04/2024)
then
B1≤1−αω(1−γ)
2.
Also, under the requirement on the step-size αin equation 123, it is easy to check that
1−1
4δ<1−αω(1−γ)
2.
In light of the above discussion, we conclude that ∀t≥2τ:
Ξt+1≤/parenleftbigg
1−αω(1−γ)
2/parenrightbigg
Ξt+ 352ατ/parenleftbigg
α+1
C/parenrightbigg
max
t−τ≤ℓ≤tΞℓ+R. (124)
We are almost in a position to invoke Lemma 20. All that remains to be verified is whether
1−αω(1−γ)
2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
p+ 352α2τ+ 352ατ
C/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
q<1.
With the choice of Cin equation 122, if we set
α≤ω(1−γ)
2816τ,
then one can verify that
p+q<1−αω(1−γ)
4.
Combining all our prior requirements on α, we conclude that if
α≤ω(1−γ)
2816 max{δ,τ}, (125)
then the following holds for all T≥2τ:
ΞT≤C1/parenleftbigg
1−αω(1−γ)
8τ/parenrightbiggT−2τ
+4R
αω(1−γ), (126)
whereC1= max 0≤k≤2τΞk.Simple calculations reveal that
4R
αω(1−γ)=O/parenleftbiggατ
ω(1−γ)/parenrightbiggσ2
M+O/parenleftbiggα2max{δ,τ}δ
ω2(1−γ)2/parenrightbigg
σ2.
Using the above bound, and noting that E/bracketleftbig˜d2
t/bracketrightbig
≤ΞT,we have that∀T≥2τ:
E/bracketleftbig˜d2
t/bracketrightbig
≤C1/parenleftbigg
1−αω(1−γ)
8τ/parenrightbiggT−2τ
+O/parenleftbiggατ
ω(1−γ)/parenrightbiggσ2
M+O/parenleftbiggα2max{δ,τ}δ
ω2(1−γ)2/parenrightbigg
σ2.(127)
The fact that C1=O(d2
0+σ2)follows from straightforward algebra similar to that in the proof of Theorem 1.
This completes the proof.
49Published in Transactions on Machine Learning Research (04/2024)
F Analysis of Multi-Agent EF-TDunder an I.I.D. Sampling Assumption
In this section, we provide a simpler (relative to that in Appendix E) analysis of the MARL setting under
a common i.i.d. sampling assumption. Essentially, we consider a setting where for each agent i, at each
time-stept,si,tis sampled independently (from the past and across agents) from the stationary distribution
π, and thensi,t+1is sampled from Pµ(·|si,t).As it turns out, this particular “i.i.d. model" has been widely
studied in the RL literature (Lakshminarayanan & Szepesvári, 2017; Dalal et al., 2018; Bhandari et al., 2018;
Doan et al., 2019; Liu & Olshevsky, 2021a); thus, we believe that providing an analysis for this setting would
be useful to the reader. Our main result for this setting is as follows.
Theorem 6. There exist universal constants c,C≥1, a step-size α≤(1−γ)/(cδ), and a set of convex
weights{¯wt}, such that the iterates generated by Algorithm 2 satisfy the following after Titerations:
E/bracketleftig
∥ˆV¯θT−ˆVθ∗∥2
D/bracketrightig
=O/parenleftbigg
exp/parenleftbigg−ω(1−γ)2T
Cδ/parenrightbigg/parenrightbigg
+˜O/parenleftbiggσ2
ω(1−γ)2MT/parenrightbigg
+T3, (128)
whereT3=˜O/parenleftbig
δ2σ2/(ω2(1−γ)4T2)/parenrightbig
,¯θT=/summationtextT
t=0¯wtθt, andσ2≜E/bracketleftbig
∥gt(θ∗)∥2
2/bracketrightbig
.4
As in Theorem 3 where we analyzed the multi-agent setting under Markovian sampling, the above result
also establishes a linear speedup in sample-complexity w.r.t. the number of agents M. The above result is
somewhat cleaner in the sense that it provides a bound on the performance of the true iterate sequence {θt},
as opposed to the perturbed iterate sequence {˜θt}. We believe that it should be possible to provide a bound
on the true iterates of the form in Theorem 6 under Markovian sampling as well; we leave this as future work.
To proceed with the analysis, we will require two auxiliary results. The first is taken from Bhandari et al.
(2018).
Lemma 25. Fix anyθ∈RK. The following holds under the i.i.d. sampling model:
E/bracketleftbig
∥gt(θ)∥2/bracketrightbig
≤2σ2+ 8∥ˆVθ−ˆVθ∗∥2
D.
The next result is an “averaging lemma" that has been adapted from Koloskova et al. (2020) and Stich (2020).
Lemma 26. Let{pt}t≥0and{st}t≥0be sequences of positive numbers satisfying
pt+1≤(1−αA)pt−Bαst+¯Cα2+Dα3,
for some positive constants A,B≥0,C,D≥0, and for constant step-sizes 0<α≤1
E, whereE > 0. Then,
there exists a constant step-size α≤1
Esuch that
B
WTT/summationdisplay
t=0wtst≤p0(E+A) exp/parenleftbigg
−A
E(T+ 1)/parenrightbigg
+2Cln ¯τ
A(T+ 1)+Dln2¯τ
A2(T+ 1)2, (129)
forwt≜(1−αA)−(t+1),WT≜/summationtextT
t=0wt, and
¯τ= max{exp (1),min{A2p0(T+ 1)2/C,A3p0(T+ 1)3/D}}.
We start with the following result.
Lemma 27. Suppose the step-size αis chosen such that α≤(1−γ)/112. Then, the iterates generated by
Algorithm 2 satisfy the following under the i.i.d. observation model:
E/bracketleftbig
∥˜θt+1−θ∗∥2/bracketrightbig
≤/parenleftbigg
1−αω(1−γ)
8/parenrightbigg
E/bracketleftbig
∥˜θt−θ∗∥2/bracketrightbig
−α(1−γ)
4E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+5α3
(1−γ)Et−1+8α2σ2
M.
(130)
4We remind the reader here that ωis the smallest eigenvalue of the matrix Σ = Φ⊤DΦ.
50Published in Transactions on Machine Learning Research (04/2024)
Proof.Starting from equation 106, we have:
E/bracketleftbig
∥˜θt+1−θ∗∥2/bracketrightbig
=E/bracketleftbig
∥˜θt−θ∗∥2/bracketrightbig
+2α
ME
⟨˜θt−θ∗,/summationdisplay
i∈[M]gi,t(θt)⟩

/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
T1+α2E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
M/summationdisplay
i∈[M]gi,t(θt)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
T2.(131)
To boundT1andT2, let us first define by Ft−1the sigma-algebra generated by all the agents’ observations
up to time-step t−1, i.e., the sigma-algebra generated by {Xi,k}i∈[M],k=0,1,...,t−1. From the dynamics of
Algorithm 2, observe that θt,˜θt, and{ei,t}i∈[M]are allFt−1-measurable. Using this fact, we bound T1as
follows:
T1=2α
ME
E
⟨˜θt−θ∗,/summationdisplay
i∈[M]gi,t(θt)⟩|Ft−1


(a)= 2αE/bracketleftbig
⟨˜θt−θ∗,¯g(θt)⟩/bracketrightbig
= 2αE[⟨θt−θ∗,¯g(θt)⟩] + 2αE/bracketleftbig
⟨˜θt−θt,¯g(θt)⟩/bracketrightbig
≤2αE[⟨θt−θ∗,¯g(θt)⟩] +α(1−γ)
4E/bracketleftbig
∥¯g(θt)∥2/bracketrightbig
+4α
(1−γ)E/bracketleftbig
∥θt−˜θt∥2/bracketrightbig
≤2αE[⟨θt−θ∗,¯g(θt)⟩] +α(1−γ)
4E/bracketleftbig
∥¯g(θt)∥2/bracketrightbig
+4α3
(1−γ)E/bracketleftbig
∥¯et−1∥2/bracketrightbig
(b)
≤2αE[⟨θt−θ∗,¯g(θt)⟩] +α(1−γ)
4E/bracketleftbig
∥¯g(θt)∥2/bracketrightbig
+4α3
(1−γ)Et−1
(c)
≤−α(1−γ)E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+4α3
(1−γ)Et−1.(132)
In the above steps, (a) follows from the fact that the agents’ observations are assumed to have been drawn
i.i.d. (over time and across agents) from the stationary distribution π; for (b), we applied equation 19; and
(c) follows from Lemmas 4 and 5. To bound T2, we first split it into two parts as follows:
T2=α2
M2E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
i∈[M](gi,t(θt)−¯g(θt)) +M¯g(θt)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

≤2α2
M2E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
i∈[M](gi,t(θt)−¯g(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
+ 2α2E/bracketleftig
∥¯g(θt)∥2/bracketrightig
.(133)
To simplify the first term in the above inequality further, let us define Yi,t≜gi,t(θt)−¯g(θt),∀i∈[M].
Conditioned onFt−1, observe that (i) Yi,thas zero mean for all i∈[M]; and (ii)Yi,tandYj,tare independent
fori̸=j(this follows from the fact that Xi,tandXj,tare independent by assumption). As a consequence of
the two facts above, we immediately have
E[⟨Yi,t,Yj,t⟩|Ft−1] = 0,∀i,j∈[M]s.t.i̸=j.
51Published in Transactions on Machine Learning Research (04/2024)
We thus conclude that
E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
i∈[M](gi,t(θt)−¯g(θt))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=E
E
∥/summationdisplay
i∈[M]Yi,t∥2|Ft−1


=E
/summationdisplay
i∈[M]E/bracketleftbig
∥Yi,t∥2|Ft−1/bracketrightbig

(a)=M/parenleftbig
E/bracketleftbig
E/bracketleftbig
∥Yi,t∥2|Ft−1/bracketrightbig/bracketrightbig/parenrightbig
=M/parenleftbig
E/bracketleftbig
∥Yi,t∥2/bracketrightbig/parenrightbig
,(134)
where (a) follows from the fact that conditioned on Ft−1,Yi,tandYj,tare identically distributed for all
i,j∈[M]withi̸=j. Plugging the result in equation 134 back in equation 133, we obtain
T2≤2α2
ME/bracketleftig
∥(gi,t(θt)−¯g(θt))∥2/bracketrightig
+ 2α2E/bracketleftig
∥¯g(θt)∥2/bracketrightig
≤4α2
ME/bracketleftig
∥gi,t(θt)∥2/bracketrightig
+ 2α2/parenleftbigg
1 +2
M/parenrightbigg
E/bracketleftig
∥¯g(θt)∥2/bracketrightig
≤56α2E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+8α2σ2
M,(135)
where in the last step, we used Lemmas 5 and 25. Now that we have bounds on each of the terms T1andT2,
we plug them back in equation 131 to obtain
E/bracketleftbig
∥˜θt+1−θ∗∥2/bracketrightbig
≤E/bracketleftbig
∥˜θt−θ∗∥2/bracketrightbig
−α(1−γ)/parenleftbigg
1−56α
(1−γ)/parenrightbigg
E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+4α3
(1−γ)Et−1
+8α2σ2
M
≤E/bracketleftbig
∥˜θt−θ∗∥2/bracketrightbig
−α(1−γ)
2E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+4α3
(1−γ)Et−1+8α2σ2
M,(136)
where in the last step, we used the fact that α≤(1−γ)/112. By splitting the second term in the above
inequality into two equal parts, using Lemma 3, and the fact that
−E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
≤−1
2E/bracketleftig
∥ˆV˜θt−ˆVθ∗∥2
D/bracketrightig
+α2Et−1,
we further obtain that
E/bracketleftbig
∥˜θt+1−θ∗∥2/bracketrightbig
≤/parenleftbigg
1−αω(1−γ)
8/parenrightbigg
E/bracketleftbig
∥˜θt−θ∗∥2/bracketrightbig
−α(1−γ)
4E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+5α3
(1−γ)Et−1+8α2σ2
M,
which is the desired conclusion.
We now complete the proof of Theorem 6 as follows.
Proof.(Proof of Theorem 3 ) Our goal is to establish a recursion of the form in Lemma 26. To that end,
we need to first control the aggregate effect of the memory variables of all agents, as captured by the term Et.
This is easily done by first using the same analysis as in Lemma 17, and then appealing to Lemma 25, to
conclude
E/bracketleftbig
∥ei,t∥2/bracketrightbig
≤/parenleftbigg
1−1
2δ/parenrightbigg
E/bracketleftbig
∥ei,t−1∥2/bracketrightbig
+ 16δE/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+ 4δσ2,∀i∈[M].
Averaging the above inequality over all agents, and using the definition of Etyields:
Et≤/parenleftbigg
1−1
2δ/parenrightbigg
Et−1+ 16δE/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+ 4δσ2.
52Published in Transactions on Machine Learning Research (04/2024)
Using the above bound along with Lemma 27, we obtain:
Ξt+1≤/parenleftbigg
1−αω(1−γ)
8/parenrightbigg
E/bracketleftbig
∥˜θt−θ∗∥2/bracketrightbig
−α(1−γ)
4E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+5α3
(1−γ)Et−1+8α2σ2
M+Cα3Et
≤/parenleftbigg
1−αω(1−γ)
8/parenrightbigg
E/bracketleftbig
∥˜θt−θ∗∥2/bracketrightbig
−/parenleftbiggα(1−γ)
4−16Cα3δ/parenrightbigg
E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+Cα3/parenleftbigg
1−1
2δ+5
C(1−γ)/parenrightbigg
Et−1+8α2σ2
M+ 4Cα3δσ2.
(137)
Based on the above inequality, our goal is to now choose αandCin a way such that we can establish a
contraction (up to higher order noise terms). Accordingly, let us pick these parameters αandCas follows:
C=20δ
(1−γ);α≤(1−γ)
55δ.
With some simple algebra, it is then easy to verify that:
Ξt+1≤/parenleftbigg
1−αω(1−γ)
8/parenrightbigg
E/bracketleftbig
∥˜θt−θ∗∥2/bracketrightbig
−α(1−γ)
8E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+Cα3/parenleftbigg
1−1
4δ/parenrightbigg
Et−1
+8α2σ2
M+80α3δ2σ2
(1−γ)
≤/parenleftbigg
1−αω(1−γ)
8/parenrightbigg/parenleftbig
E/bracketleftbig
∥˜θt−θ∗∥2/bracketrightbig
+Cα3Et−1/parenrightbig
−α(1−γ)
8E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+8α2σ2
M+80α3δ2σ2
(1−γ)
=/parenleftbigg
1−αω(1−γ)
8/parenrightbigg
Ξt−α(1−γ)
8E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
+8α2σ2
M+80α3δ2σ2
(1−γ).(138)
We have thus succeeded in establishing a recursion of the form in Lemma 26. To spell things out explicitly in
the language of Lemma 26, we have
pt= Ξt;st=E/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
;A=ω(1−γ)
8;B=(1−γ)
8;¯C=8σ2
M;D=80δ2σ2
(1−γ),
andα≤(1−γ)/(112δ)suffices for the recursion in equation 138 to hold. Thus, for us, E=112δ
(1−γ). Applying
Lemma 26 along with some simplifications then yields:
T/summationdisplay
t=0¯wtE/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
≤O/parenleftbigg∥θ0−θ∗∥2δ
(1−γ)2/parenrightbigg
exp/parenleftbigg−ω(1−γ)2T
C′δ/parenrightbigg
+˜O/parenleftbiggσ2
ω(1−γ)2MT/parenrightbigg
+˜O/parenleftbiggσ2δ2
ω2(1−γ)4T2/parenrightbigg
,(139)
where ¯wt≜wt/WT, andC′is a suitably large constant. The result follows by noting that
E/bracketleftig
∥ˆV¯θT−ˆVθ∗∥2
D/bracketrightig
≤T/summationdisplay
t=0¯wtE/bracketleftig
∥ˆVθt−ˆVθ∗∥2
D/bracketrightig
,
where ¯θT=/summationtextT
t=0¯wtθt.
53