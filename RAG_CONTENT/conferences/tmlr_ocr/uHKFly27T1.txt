Under review as submission to TMLR
Adaptively Phased Algorithm for
Linear Contextual Bandits
Anonymous authors
Paper under double-blind review
Abstract
We propose a novel algorithm for the linear contextual bandit problem when the set of
arms is finite. Recently the minimax expected regret for this problem is shown to be
Ω(√dTlogTlogK)withTrounds,d-dimensional contexts, and K≤2d/2arms per time.
Previous works on phased algorithms attain this lower bound in the worst case up to loga-
rithmic factors (Auer, 2002; Chu et al., 2011) or iterated logarithmic factors (Li et al., 2019),
but require a priori knowledge of the time horizon Tto construct the phases, which limits
theiruseinpractice. Inthispaperweproposeanovelphasedalgorithmthatdoesnotrequire
a priori knowledge of T, but constructs the phases in an adaptive way. We show that the
proposed algorithm guarantees a regret upper bound of order O(dα/radicalbig
TlogT(logK+ logT))
where1
2≤α≤1. The proposed algorithm can be viewed as a generalization of Rarely
Switching OFUL (Abbasi-Yadkori et al., 2011) by capitalizing on a tight confidence bound
for the parameter in each phase obtained through independent rewards in the same phase.
1 Introduction
We consider a sequential decision problem where the learning agent is repeatedly faced with a set of available
actions, chooses an action from this set, and receives a random reward. We assume that the expected value
of the reward is an unknown linear function of the context information of the chosen action. This is a
linear contextual bandit problem, where taking actions is characterized as pulling the arms of a bandit slot
machine. At each time step, the learner has access to the context vector of each arm. The goal of the learner
is to minimize the cumulative gap between the rewards of the optimal arm and the chosen arm, namely
regret. Due to uncertainty about the compensation mechanism of the reward, the learner should balance
the trade-off between exploitation, pulling the seemingly best arm based on the function learned so far, and
exploration, pulling other arms that would help learn the unknown function more accurately.
Auer (2002), Li et al. (2010), and Abbasi-Yadkori et al. (2011) proposed the LinRel, LinUCB, and OFUL
algorithms, respectively, for the linear bandit problem. The underlying principle of these algorithms is
optimism-in-the-face-of-uncertainty, where the arms are pulled to maximize the optimistic estimate of the
expected reward. Based on the same principle, Auer (2002), Chu et al. (2011) and Li et al. (2019) presented
phased algorithms, which are different from the aforementioned methods in two aspects: the number of
updates for the estimate is smaller, and the order of regret upper bound in terms of the dimension of
contexts is smaller. In this paper, we focus on the phased algorithms.
Most algorithms update the estimates every round and pull the arm based on the most up-to-date estimate.
This does not raise an issue when the computation of the estimates is not costly, however, could become an
issue when computation is heavy. Recently, contextual bandit algorithm is proposed for deep neural networks
(Zhou et al., 2020; Zhang et al., 2021) requiring computation of a deep model every round. Similarly, for
bandit applications attempting to enhance deep learning such as hyperparameter search (Li et al., 2017)
and active learning (Ganti & Gray, 2013), the rewards are error rates of the fitted model. In some of these
applications, updating the estimates every round may not be practically feasible. There have been a number
of phased algorithms, SupLinRel (Auer, 2002), SupLinUCB (Chu et al., 2011), and Variable-Confidence-
Level SupLinUCB (Li et al., 2019). The central idea of the phased algorithms is to assign the decision steps
1Under review as submission to TMLR
into separate⌈logT⌉phases. In phased algorithms, the estimates are not updated within the same phase
until the uncertainty estimates decrease below a predefined level. Less frequent updates of phased algorithms
offer substantial advantages when the computation of the estimates is costly.
As for the order of regret upper bound, Dani et al. (2008) showed that when contexts are d-dimensional,
there exists a distribution of contexts such that any algorithm has regret at least Ω(d√
T), whereTis the
time horizon. Under additional assumption that the number of arms Kin each step is finite, Chu et al.
(2011) showed a lower bound of Ω(√
dT)and proposed an algorithm which matches this bound up to poly-
logarithmic factors in the worst case. Recently, Li et al. (2019) refined this lower bound to Ω(√dTlogTlogK),
showing that some logarithmic terms are unavoidable for any algorithm.
On the other hand the OFUL (Abbasi-Yadkori et al., 2011) and the LinUCB (Li et al., 2010) algorithms
have a regret upper bound of order O(d√
TlogT)regardless of the number of arms. When the number of
arms is large as K≥2d/2, the regrets of OFUL and LinUCB match the best lower bound up to logarithmic
factors. However when Kis small (K≪2d/2), the regrets are larger by a factor of O(√
d)compared to the
best possible bound.
The extraO(√
d)term appears in the estimation error bound of the linear regression parameter due to
dependence of chosen contexts on the rewards. Since LinUCB and OFUL update the arm choice rule in each
round based on the rewards observed up to that round, the chosen contexts are correlated with the rewards.
To some extent, such correlation is necessary because we cannot minimize the regret without any adaptation
to the past observations unless we completely know the parameter values. However, we can carefully control
the amount of correlation to make use of the tighter confidence result. Aforementioned phased algorithms
have addressed this issue by handling computation separately for each phase. In these algorithms the arms
in the same phase are chosen without making use of the rewards in the same phase. Consequently in each
phase, due to independence, a tight confidence bound could be constructed for the parameter without extra
factorO(√
d).
Despite the strong theoretical guarantee, the aforementioned phased algorithms, SupLinRel, SupLinUCB,
and VCL SupLinUCB, are not practical since the implementation requires a priori knowledge of the time
horizonTto determine the number of phases. A loose upper bound of Tcould result in a larger number of
phases than it is needed, reducing the number of samples in each phase and increasing the regret. Another
difficulty is that the phases switch around over the time round. The doubling trick (Auer et al., 1995) can be
used, but is wasteful. Even under the knowledge of T, the algorithms are mostly outperformed by LinUCB
(Valko et al., 2014).
In this paper, we propose a novel phased algorithm where changes in phases are monotone in time. The
proposed method does not require a priori knowledge of time horizon Tsince the switching time of the phase
is determined in an adaptive way and the number of phases is determined accordingly. At a given phase,
the arms are pulled based on the estimate from the previous phase. Hence, the chosen contexts are not
correlated with the rewards of the current phase. We do not switch phases until the upper bound of the
regret in that phase exceeds a predefined constant. The proposed algorithm achieves a regret upper bound
ofO(dα/radicalbig
TlogT(logK+ logT)), whereα∈[1
2,1]depends on the number of phases.
1.1 Related works
A number of authors have studied phased algorithms that save computational cost. When Abbasi-
Yadkori et al. (2011) proposed OFUL in their paper, they also proposed a phased algorithm called Rarely
Switching OFUL to recompute the estimates only O(logT)times. However, their regret upper bound is
O(d√
TlogT). SupLinRel (Auer, 2002) and SupLinUCB (Chu et al., 2011) both achieve a regret upper
bound ofO(√
dTlog3/2(KTlogT)), removing the extra O(√
d)factor when K≪2d/2. The recently pro-
posed Variable-Confidence-Level (VCL) SupLinUCB (Li et al., 2019) refines the SupLinUCB and achieves
a tighter bound, O(√dTlogTlogK(loglogT)γ), withγ >0. The difference between Rarely Switching OFUL
and others is that the estimates in Rarely Switching OFUL are based on the data from the beginning to the
previous phase, while SupLinRel, SupLinUCB and VCL SupLinUCB estimate the parameter in each phase
separately.
2Under review as submission to TMLR
Valko et al. (2014) and Lattimore & Szepesvári (2020) also proposed phase-wise algorithms with
O(√dTlogKlogT)regret guarantees. Their methods however are restricted to the cases where the set of
arms is fixed over time. Both works use the phase-wise arm elimination idea of Auer & Ortner (2010),
eliminating subotpimal arms at the end of each phase. Due to elimination, the maximum possible regret
decreases after each phase. In each phase, the algorithm either pulls the most uncertain arms (Valko et al.,
2014) or pulls each arm according to an optimal design strategy (Lattimore & Szepesvári, 2020), without any
dependence on the rewards of the current phase. In this paper, we allow the arm set to change arbitrarily
over time.
1.2 Contributions
The main contributions of the paper are as follows.
•We propose a novel phased algorithm for the linear contextual bandit problem where the estimates
are updated only O(logT)times and a tight confidence bound for the linear parameter is used.
•The proposed algorithm does not require prior knowledge of T. The changes in phases are monotone
in time, and the number of phases is determined in an adaptive mechanism.
•We prove that the high-probability regret upper bound of the proposed algorithm ranges between
O(d/radicalbig
TlogT(logK+ logT))andO(/radicalbig
dTlogT(logK+ logT)), depending on the number of phases.
2 Problem formulation
At each time t, the learner is faced with Kalternative arms. The i-th arm (i= 1,···,K) yields a random
rewardri(t)with unknown mean. Prior to the choice at time t, the learner has access to a finite-dimensional
context vector bi(t)∈Rdassociated with each arm i. Then the learner pulls one arm a(t)and observes the
corresponding reward ra(t)(t). We also make the following assumptions, from A1 to A4.
A1. Linear reward. For an unknown vector µ∈Rd,
E[ri(t)|bi(t)] =bi(t)Tµ.
A2. Bounded norms. Without loss of generality, ||bi(t)||2≤1,||µ||2≤1.
A3. Sub-Gaussian error. The errorηi(t) :=ri(t)−bi(t)TµisR-sub-Gaussian for some R> 0, i.e., for
everyϵ∈R,
E[exp(ϵηi(t))]≤exp(ϵ2R2/2).
A4. Oblivious adversary. The sequence of contexts is chosen by an oblivious adversary. An oblivious
adversary may know the algorithm’s code, but does not have access to the randomized results of the algorithm.
Assumption A4 is used in Auer (2002), Chu et al. (2011), and Li et al. (2019) which consider the same
problem as ours. Under assumption A1, the optimal arm at time tisa∗(t) := argmaxi{bi(t)Tµ}. We define
theregret (t)as the difference between the expected reward of the optimal arm and the expected reward of
the arm chosen by the learner at time t, i.e.,
regret (t) =ba∗(t)(t)Tµ−ba(t)(t)Tµ.
Then, the goal of the learner is to minimize the sum of regrets over Tsteps,R(T) :=/summationtextT
t=1regret (t).
3 Proposed method
Our strategy is to adaptively combine methods for (B1) and (B2) described below, and the derivation of
the adapting conditions is the key of this Section. The phased algorithms use (B1) which yields a tighter
3Under review as submission to TMLR
confidence bound but they require the knowledge of T, and the Rarely Switching OFUL invokes (B2)
adaptively but with a wider bound. The two bounds play a crucial role in deriving the phase-switching
conditions for our method. In Section 3.1, we first review the difference in the prediction error bounds of
the estimate for the expected reward when, within the phase,
(B1) arms are chosen independently of the rewards,
(B2) arms are chosen adaptively based on the rewards observed so far.
Then in Section 3.2, we present a new class of phase-wise Upper Confidence Bound (UCB) algorithm and
discuss on the phase-switching conditions to bound the regret. Finally in Section 3.3, we propose the
adaptively phased Upper Confidence Bound (AP-UCB) algorithm.
3.1 Parameter estimation
LetSbe the set of context-reward pairs of the chosen arms at time points in the set T ⊂N, i.e.,S=
{(ba(τ)(τ),ra(τ)(τ)),τ∈T}, where Nis the set of natural numbers. We use the following Ridge estimator
with some constant λ>0to estimate the linear parameter µ.
ˆµ=/parenleftig
λId+/summationdisplay
(bτ,rτ)∈SbτbT
τ/parenrightig−1/summationdisplay
(bτ,rτ)∈Sbτrτ.
Chu et al. (2011) and Abbasi-Yadkori et al. (2011) analyzed the upper bound of the prediction error
|bi(t)T(ˆµ−µ)|for case (B1) and (B2), respectively.
Lemma 3.1. (Lemma 1 of Chu et al., 2011) Suppose that the samples in Sare such that for fixed ba(τ)(τ)
withτ∈T, the rewards ra(τ)(τ)’s are independent random variables with means E[ra(τ)(τ)] =ba(τ)(τ)Tµ.
Then for a fixed tand for all 1≤i≤K, we have with probability at least 1−δ
t2,
|bi(t)T(ˆµ−µ)|≤/parenleftig
2R/radicalbigg
log/parenleftbig2Kt
δ/parenrightbig
+√
λ/parenrightig
st,i,
wherest,i=/radicalbig
bi(t)TB−1bi(t)andB=λId+/summationtext
(bτ,rτ)∈SbτbT
τ.
Lemma 3.2. (Theorem 2 of Abbasi-Yadkori et al., 2011) DefineHt−1as the history until time
t−1, i.e.,Ht−1={a(τ),ra(τ)(τ),{bi(τ)}K
i=1,τ= 1,···,t−1},and the filtration Ft−1as the union of
Ht−1, the contexts at time t, and the action at time t, i.e.,Ft−1={Ht−1,{bi(t)}K
i=1,a(t)}fort= 1,···,T.
Suppose that the samples in Sare such that for each τ∈T,E[ra(τ)(τ)|Fτ−1] =ba(τ)(τ)Tµandηa(τ)(τ)is
conditionally R-sub-Gaussian given Fτ−1, i.e.,
E[exp(ϵηa(τ)(τ))|Fτ−1]≤exp(ϵ2R2/2).
Then for a fixed tand for all 1≤i≤K, we have with probability at least 1−δ
t2,
|bi(t)T(ˆµ−µ)|≤/parenleftig
R/radicalbigg
3dlog/parenleftbigt
δ/parenrightbig
+√
λ/parenrightig
st,i.
The bound in Lemma 3.2 does not depend on K, but has an extra O(√
d)factor compared to Lemma 3.1.
The key point in Lemma 3.1 is that the error ˆµ−µ≈B−1/summationtextbτηa(τ)(τ)can be expressed as the sum of
independent, mean zero variables. This is because bτ’s andηa(τ)(τ)’s are independent so bτ’s andBcan
be considered as fixed variables. Hence, we can directly apply the Chernoff inequality and obtain a tight
bound. On the other hand, in Lemma 3.2, the error is not the sum of independent variables due to the
correlation between the context variables inside Bandηa(τ)(τ)’s. Hence, we should invoke the Cauchy-
Schwarz inequality which gives two terms, one corresponding to st,iand the other including the normalized
sum ofηa(τ)(τ)′swhich can be bounded by the self-normalized theorem. Since each term contributes a factor
of√
d, the combined bound has an order of d.
4Under review as submission to TMLR
3.2 Phase-switching condition
To make use of the tight confidence result in Lemma 3.1, we propose a phase-wise algorithm which updates
the regression parameter only at the end of each phase. Algorithm 1 shows an example of such phase-wise
linear bandit algorithm. The arm choice of the m-th phase depends on the estimate ˆµm−1and matrix Bm−1
constructed in the preceding phase. Hereby, the estimate of each phase has a small prediction error. There
are two differences comparing with LinUCB: first it updates the estimate of µand the matrix Binfrequently,
second, the estimates are based on the data from the previous phase as marked by lines 8–11 in the algorithm.
Comparing with Rarely Switching OFUL, the difference lies in line 11, where the set Sstores data only from
the current phase. For now, we do not specify the phase-switching condition but simply denote the end point
of them-th phase as tm.
Algorithm 1 phase-wise UCB
1:Input:α,λ
2:Set:S={},m= 1,ˆµ0= 0d, B0=λId
3:fort= 1,···,Tdo
4:Pull arma(t) = argmax1≤i≤K{bi(t)Tˆµm−1+αst,i}wherest,i=/radicalig
bi(t)TB−1
m−1bi(t)
5:Observe reward ra(t)(t)
6:S←S∪{ (ba(t)(t),ra(t)(t))}
7: ift=tmthen
8:Bm←λId+/summationtext
(bτ,rτ)∈SbτbT
τ
9: ˆµm←B−1
m/summationtext
(bτ,rτ)∈Sbτrτ
10:m←m+ 1
11:S←{}
12: end if
13:end for
We derive an upper bound of the regret of Algorithm 1 and deduce the phase-switching condition that
minimizes this upper bound. Let α= 2R/radicalig
log/parenleftbig2KT
δ/parenrightbig
+√
λ. Consider time tin them-th phase, i.e.,
tm−1<t≤tmwitht0= 0. We have with probability at least 1−δ
t2,
ba∗(t)(t)Tµ≤ba∗(t)(t)Tˆµm−1+αst,a∗(t)
≤ba(t)(t)Tˆµm−1+αst,a(t)
≤ba(t)(t)Tµ+αst,a(t)+αst,a(t), (1)
where the first and third inequalities are due to Lemma 3.1 and the second inequality is due to the arm
selection mechanism. Therefore,
regret (t) =ba∗(t)(t)Tµ−ba(t)(t)Tµ≤2αst,a(t). (2)
Applying the union bound to all time points, we have with probability at least 1−δ,
R(T)≤2αT/summationdisplay
t=1st,a(t)= 2/parenleftig
2R/radicalbigg
log(2KT
δ) +√
λ/parenrightigT/summationdisplay
t=1st,a(t).
In LinUCB, the matrix Bat timetis the Gram matrix of all the chosen contexts up to time t−1,B(t) =
λId+/summationtextt−1
τ=1ba(τ)(τ)ba(τ)(τ)T, and the sum/summationtextT
t=1st,a(t)can be shown to be less than O(√dTlogT)by the
elliptical potential lemma of Abbasi-Yadkori et al. (2011). However in the phase-wise algorithm, we always
haveBm−1≼B(t)for anytm−1<t≤tm. Therefore, the elliptical potential lemma cannot apply. We have
5Under review as submission to TMLR
instead,
T/summationdisplay
t=1st,a(t)=M/summationdisplay
m=1tm/summationdisplay
t=tm−1+1/radicalig
ba(t)(t)TB−1
m−1ba(t)(t)≤/radicaltp/radicalvertex/radicalvertex/radicalbtTM/summationdisplay
m=1tm/summationdisplay
t=tm−1+1ba(t)(t)TB−1
m−1ba(t)(t)
=/radicaltp/radicalvertex/radicalvertex/radicalbtTM/summationdisplay
m=1trace/parenleftig
B−1
m−1tm/summationdisplay
t=tm−1+1ba(t)(t)ba(t)(t)T/parenrightig
≤/radicaltp/radicalvertex/radicalvertex/radicalbtTM/summationdisplay
m=1trace/parenleftig
B−1
m−1Bm/parenrightig
,(3)
whereMdenotes the total number of phases and the first inequality is due to Jensen’s inequality.
Thebound(3) motivates aphase-switchingcondition. Supposeweswitchfrom the m-thphase tothe (m+1)-
th phase as soon as the trace of B−1
m−1BmexceedsAdfor some predefined constant A > 0. Then we can
bound/summationtextst,a(t)≤√
MAdT, which has the same order as the bound of the elliptical potential lemma. The
problem is that we do not have control over the number of phases, M. A sufficient condition for bounding
Mis that the determinant of Bmis larger than the determinant of CBm−1for some predefined constant
C > 1. Ifdet(Bm)≥det(CBm−1)for all 1≤m≤M, we have,
det(BM)≥det(CBM−1)≥det(C2BM−2)≥···≥det(CMB0).
Thus,det(CMλId)≤det(BM),withdet(CMλId) = (λCM)dand
det(BM)≤det/parenleftig
λId+T/summationdisplay
t=1ba(t)(t)ba(t)(t)T/parenrightig
≤/parenleftigλd+/summationtextT
t=1ba(t)(t)Tba(t)(t)
d/parenrightigd
≤/parenleftbig
λ+T
d/parenrightbigd,
where the second inequality is due to the determinant-trace inequality and the third inequality due to
Assumption A2. Therefore, Mcan be kept small as M≤logC/parenleftbig
1 +T/dλ/parenrightbig
.
When the two conditions (C1) trace (B−1
m−1Bm)≤Adand (C2)det(Bm)≥det(CBm−1)are satis-
fied for every m, the regret of the proposed algorithm achieves a tight regret upper bound, R(T)≤
O(/radicalbig
dTlogT(logK+ logT)). However, imposing (C1) does not always guarantee (C2). In the next sec-
tion, we suggest a remedy when (C2) does not hold.
3.3 Algorithm
To check conditions (C1) and (C2) at phase m, we do not need to observe the rewards of the chosen arms. We
canchoosethe arms based on ˆµm−1given in line 9 of Algorithm 1, and compute/summationtexttm
t=tm−1+1ba(t)(t)ba(t)(t)T
until condition (C1) is violated. Then at that round, if condition (C2) holds, we can pullthe chosen arms.
We call this an independent phase, because it uses information only from the preceding phase. If condition
(C2) does not hold, we can go back to the beginning of the current phase, and pull the arms based on
˜µm−1given in line 28 in Algorithm 2, which uses samples in all cumulative phases. We call this phase an
adaptive phase. For the adaptive phase, we use the phase switching condition of Rarely Switching OFUL
(Abbasi-Yadkori et al., 2011), which is given in line 21 of Algorithm 2 and is different from the switching
condition for the independent phase.
In every phase, we start with an independent phase using the estimate from the most recent indepen-
dent phase. Algorithm 2 presents the proposed AP-UCB algorithm. We remark that when condition
(C2) is never met, AP-UCB is identical to the Rarely Switching OFUL algorithm which guarantees a re-
gret less than O(d√
TlogT). In contrast when condition (C2) is always satisfied, the regret is less than
O(/radicalbig
dTlogTlog(KT))as we have seen in Section 3.2.
4 Regret analysis
The following theorem derives a high-probability upper bound of the regret incurred by the proposed algo-
rithm in terms of the number of independent phases, M1, and the number of adaptive phases, M2.
6Under review as submission to TMLR
Algorithm 2 AP-UCB
1:Input:α,β,λ,A> 0,C > 1,E > 1
2:Set: ˆµ0= ˜µ0= 0d, B0=B′
0=λId,t=t0= 0,¯m= 1.
3:form= 1,···do
4:B←λId
5: whiletrace (B−1
¯m−1B)≤Addo
6:t←t+ 1
7: Choose arm a(t) = argmaxi{bi(t)Tˆµ¯m−1+αst,i}wherest,i=/radicalig
bi(t)TB−1
¯m−1bi(t)
8:B←B+ba(t)(t)ba(t)(t)T
9: end while
10: ifdet(B)≥det(CB ¯m−1)then
11:tm←t
12: Pull arms{a(τ)}tm
τ=tm−1+1and observe{ra(τ)(τ)}tm
τ=tm−1+1.
13:B¯m←B
14: ˆµ¯m←B−1
¯m/summationtexttm
τ=tm−1+1ba(τ)(τ)ra(τ)(τ)
15:B′
m←B′
m−1+/summationtexttm
τ=tm−1+1ba(τ)(τ)ba(τ)(τ)T
16: ˜µm←B′−1
m/braceleftbig
B′
m−1˜µm−1+/summationtexttm
τ=tm−1+1ba(τ)(τ)ra(τ)(τ)/bracerightbig
17: ¯m←¯m+ 1
18: else
19:t←tm−1
20:B′
m←B′
m−1
21: whiledet(B′
m)≤Edet (B′
m−1)do
22: t←t+ 1
23: Choose arm ˜a(t) = argmaxi{bi(t)T˜µm−1+βs′
t,i}wheres′
t,i=/radicalig
bi(t)TB′−1
m−1bi(t)
24: B′
m←B′
m+b˜a(t)(t)b˜a(t)(t)T
25: end while
26:tm←t
27: Pull arms{˜a(τ)}tm
τ=tm−1+1and observe{r˜a(τ)(τ)}tm
τ=tm−1+1.
28: ˜µm←B′−1
m/braceleftbig
B′
m−1˜µm−1+/summationtexttm
τ=tm−1+1b˜a(τ)(τ)r˜a(τ)(τ)/bracerightbig
29: end if
30:end for
Theorem 4.1. Regret of AP-UCB . Suppose assumptions A1, A2, A3, and A4 hold. If we set α=
2R/radicalig
log/parenleftbig2KT
δ/parenrightbig
+√
λandβ=R/radicalig
3dlog/parenleftbigT
δ/parenrightbig
+√
λfor someδ∈(0,1),we have with probability at least 1−2δ,
R(T)≤/radicalbigg
16dT/braceleftig
2M1AR2log/parenleftbig2KT
δ/parenrightbig
+ 3M2ER2logElog/parenleftbigT
δ/parenrightbig/bracerightig
+ 16Tλ
=O/parenleftbig/radicalbig
dT(logK+ logT)(M1+M2)/parenrightbig
.
A sketch of proof. We first have,
R(T) =M/summationdisplay
m=1tm/summationdisplay
t=tm−1+1regret (t)≤/radicaltp/radicalvertex/radicalvertex/radicalbtTM/summationdisplay
m=1tm/summationdisplay
t=tm−1+1regret (t)2,
where the inequality follows from Jensen’s inequality. When mis the ¯m-th independent phase, arms are
chosen based on ˆµ¯m−1andB¯m−1from the ( ¯m−1)-th independent phase. Using Lemma 3.1 and similar
7Under review as submission to TMLR
arguments as in (1) and (2), we can show that with probability at least 1−δ,
t=tm/summationdisplay
t=tm−1+1regret (t)2≤t=tm/summationdisplay
t=tm−1+14α2ba(t)(t)TB−1
¯m−1ba(t)(t)
= 4α2trace/parenleftig
B−1
¯m−1t=tm/summationdisplay
t=tm−1+1ba(t)(t)ba(t)(t)T/parenrightig
≤4α2Ad
for every independent phase, where the last inequality follows from condition (C1). On the other hand, when
mis an adpative phase, arms are chosen based on ˜µm−1andB′
m−1constructed from all cumulative samples
up to the (m−1)-th phase. Using Lemma 3.2 instead of Lemma 3.1, we have with probability at least 1−δ,
t=tm/summationdisplay
t=tm−1+1regret (t)2≤t=tm/summationdisplay
t=tm−1+14β2b˜a(t)(t)TB′−1
m−1b˜a(t)(t)≤8β2ElogE
for every adaptive phase, where the last inequality is due to phase-switching condition for the adaptive phase.
Therefore, with probability at least 1−2δ,
R(T)≤/radicalbig
T{M14α2Ad+M28β2ElogE}.
Plugging in the definition of αandβgives the theorem.
Lemma 4.2 shows the upper bounds of M1andM2. WhileM1is at mostO(logT), the biggest possible value
ofM2scales with O(dlogT), introducing an extra O(√
d)factor to the regret bound. However, M2reaches
the upper bound when the AP-UCB consists of only adaptive phases without independent phases. The
lemma implies that we can avoid an extra O(√
d)factor by keeping M2small andM1as large as possible.
Lemma 4.2. In the AP-UCB algorithm, we have
M1≤logC/parenleftbig
1 +T/dλ/parenrightbig
,M2≤dlogE/parenleftbig
1 +T/dλ/parenrightbig
.
Detailed proofs of Theorem 4.1 and Lemma 4.2 are presented in the Supplementary Material.
5 Experiments
We conduct simulation studies to compare the performance of the proposed algorithm with LinUCB and
Rarely Switching (RS) UCB. We construct a similar environment to the design of Chu et al. (2011), where
the distribution of the contexts and rewards is such that the regret is at least Ω(√
dT)for any algorithm.
We setK= 2andd= 11. Detailed structures are presented in the Supplementary material.
LinUCB and RS-UCB require an input parameter controlling the degree of exploration, which has the same
theoretical order as βof AP-UCB. RS-UCB and AP-UCB also share the parameter Ein the phase-switching
condition for the adaptive phases. AP-UCB has additional hyperparameters, α,A,andC. We fixA= 1.5,
C= 1.2, andE= 5. We consider some candidate parameters for αandβand report the results of the
values that incur minimum median regret over 30 experiments.
Figure 1 shows the cumulative regret R(t)according to time t. The proposed AP-UCB has the minimum
median regret. LinUCB and RS-UCB have similar performance, which is in accordance with the theory. AP-
UCB has 2 long independent phases, followed by 12 adaptive phases. In contrast, RS-UCB has 20 adaptive
phases in total. Long independent phases may have led to a better performance of AP-UCB by gathering
diverse context variables.
6 Concluding remarks
In this paper, we propose an adaptively phased algorithm for the linear contextual bandit problem with
finitely many arms. The algorithm does not require a priori knowledge of the time horizon and saves
8Under review as submission to TMLR
Figure 1: Median (solid), first and third quartiles (dashed) of the cumulative regret over 30 replications.
computational cost by updating the estimate only O(logT)times. The high-probability upper bound of the
regret is tight and matches the lower bound up to logarithmic factors when the number of phases is small.
Numerical studies demonstrate a good performance of the proposed method.
Broader Impact
In this work, we present a novel algorithm for sequential decision. The main point is that the proposed
method has low computational cost while achieving comparable performance to existing methods. The work
mainly focuses on theoretical development of the algorithm, and uses simulated data for empirical evaluation.
We believe that this work does not involve any ethical issue, and has no direct societal consequence.
References
Y. Abbasi-Yadkori, D. Pál, and C. Szepesvári. Improved algorithms for linear stochastic bandits. In Advances
in Neural Information Processing Systems , pp. 2312–2320, 2011.
P. Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning
Research , 3:397–422, 2002.
P.AuerandR.Ortner. Ucbrevisited: Improvedregretboundsforthestochasticmulti-armedbanditproblem.
Periodica Mathematica Hungarica , 61(1-2):55–65, 2010.
P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. Gambling in a rigged casino: The adversarial
multi-armed bandit problem. In Proceedings of IEEE 36th Annual Foundations of Computer Science , pp.
322–331, 1995.
W. Chu, L. Li, L. Reyzin, and R.E. Schapire. Contextual bandits with linear payoff functions. In Proceedings
of the 14th International Conference on Artificial Intelligence and Statistics , pp. 208–214, 2011.
V. Dani, T.P. Hayes, and S.M. Kakade. Stochastic linearoptimization underbandit feedback. In Conference
on Learning Theory , pp. 355–366, 2008.
R. Ganti and A. G. Gray. Building bridges: viewing active learning from the multi-armed bandit lens. In
Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence , pp. 232–241, 2013.
T. Lattimore and C. Szepesvári. Bandit Algorithms . Cambridge University Press, 2020.
L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news article
recommendation. In Proceedings of the 19th International Conference on World wide web , pp. 661–670,
2010.
L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband: A novel bandit-based
approach to hyperparameter optimization. Journal of Machine Learning Research , 18:6765–6816, 2017.
9Under review as submission to TMLR
Y. Li, Y. Wang, and Y. Zhou. Nearly minimax-optimal regret for linearly parameterized bandits. In
Conference on Learning Theory , pp. 2173–2174, 2019.
M. Valko, R. Munos, B. Kveton, and T. Kocák. Spectral bandits for smooth graph functions. In International
Conference on Machine Learning , pp. 46–54, 2014.
W. Zhang, D. Zhou, L. Li, and Q. Gu. Neural thompson sampling. In International Conference on Learning
Representations , 2021.
D. Zhou, L. Li, and Q. Gu. Neural contextual bandits with ucb-based exploration. In International Confer-
ence on Machine Learning , pp. 11492–11502, 2020.
A Appendix
A.1 Proofs
A.1.1 Preliminaries
Lemma A.1.1. (Lemma 12 of Abbasi-Yadkori et al., 2011) LetX,Y, andZbe positive semi-definite
matrices such that Z=X+Y. Then, we have,
sup
x̸=0xTZx
xTYx≤det(Z)
det(Y).
Lemma A.1.2. Elliptical Potential Lemma (Lemma 11 of Abbasi-Yadkori et al., 2011) Let
V0∈Rd×dbe positive definite and v1,···,vn∈Rdbe a sequence of vectors with ||vt||2≤1for all 1≤t≤n.
DefineVt=V0+/summationtextt
τ=1vτvT
τ.Then,
n/summationdisplay
t=1/parenleftbig
1∧vT
tV−1
t−1vt/parenrightbig
≤2log/parenleftigdet(Vn)
det(V0)/parenrightig
.
A.1.2 Proof of Theorem 4.1
Theorem 4.1. Regret of AP-UCB . Suppose assumptions A1, A2, A3, and A4 hold. If we set α=
2R/radicalig
log/parenleftbig2KT
δ/parenrightbig
+√
λandβ=R/radicalig
3dlog/parenleftbigT
δ/parenrightbig
+√
λfor someδ∈(0,1),we have with probability at least 1−2δ,
R(T)≤/radicalbigg
16dT/braceleftig
2M1AR2log/parenleftbig2KT
δ/parenrightbig
+ 3M2ER2logElog/parenleftbigT
δ/parenrightbig/bracerightig
+ 16Tλ
=O/parenleftbig/radicalbig
dT(logK+ logT)(M1+M2)/parenrightbig
.
Proof.We first have,
R(T) =M/summationdisplay
m=1tm/summationdisplay
t=tm−1+1regret (t)≤/radicaltp/radicalvertex/radicalvertex/radicalbtTM/summationdisplay
m=1tm/summationdisplay
t=tm−1+1regret (t)2, (4)
where the inequality follows from Jensen’s inequality. Suppose mis the ¯m-th independent phase. For any
t∈[tm−1+ 1,tm],the arma(t)is chosen based on ˆµ¯m−1andB¯m−1from the ( ¯m−1)-th independent phase.
Thus for any t∈[tm−1+ 1,tm],we have with probabiltiy at least 1−δ/t2,
ba∗(t)(t)Tµ≤ba∗(t)(t)Tˆµ¯m−1+α/radicalig
ba∗(t)(t)TB−1
¯m−1ba∗(t)(t)
≤ba(t)(t)Tˆµ¯m−1+α/radicalig
ba(t)(t)TB−1
¯m−1ba(t)(t)
≤ba(t)(t)Tµ+ 2α/radicalig
ba(t)(t)TB−1
¯m−1ba(t)(t), (5)
10Under review as submission to TMLR
where the first and third inequalities are due to Lemma 3.1 and the second inequality is due to the arm
selection mechanism. Applying the union bound, we have with probability at least 1−/summationtexttm
t=tm−1+1δ/t2,
tm/summationdisplay
t=tm−1+1regret (t)2≤tm/summationdisplay
t=tm−1+14α2ba(t)(t)TB−1
¯m−1ba(t)(t)
= 4α2trace/parenleftig
B−1
¯m−1tm/summationdisplay
t=tm−1+1ba(t)(t)ba(t)(t)T/parenrightig
≤4α2trace/parenleftig
B−1
¯m−1B¯m/parenrightig
, (6)
whereB¯m=λId+/summationtexttm
t=tm−1+1ba(t)(t)ba(t)(t)T. Due to lines 5–9 in Algorithm 2, the matrix B¯mwithout the
last sample ba(tm)(tm)satisfies
trace/braceleftig
B−1
¯m−1/parenleftbig
B¯m−ba(tm)(tm)ba(tm)(tm)T/parenrightbig/bracerightig
≤Ad.
Then,
trace (B−1
¯m−1B¯m)≤Ad+ba(tm)(tm)TB−1
¯m−1ba(tm)(tm)
≤Ad+1
λ, (7)
due to Assumption A2 and the fact B¯m−1≽λId.
On the other hand, when mis an adaptive phase, arms are chosen based on ˜µm−1andB′
m−1constructed
from all cumulative samples up to the (m−1)-th phase. Under similar arguments as in (5) but with Lemma
3.2 instead of Lemma 3.1, we have with probability at least 1−/summationtexttm
t=tm−1+1δ/t2,
tm/summationdisplay
t=tm−1+1regret (t)2≤tm/summationdisplay
t=tm−1+14β2b˜a(t)(t)TB′−1
m−1b˜a(t)(t).
Since we trivially have regret (t)≤2andβ≥1,we also have,
tm/summationdisplay
t=tm−1+1regret (t)2≤tm/summationdisplay
t=tm−1+14β2/parenleftbig
1∧b˜a(t)(t)TB′−1
m−1b˜a(t)(t)/parenrightbig
. (8)
We follow the lines of Abbasi-Yadkori et al. (2011) to further bound (8). For any t∈[tm−1+ 1,tm], let
B(t−1) =B′
m−1+/summationtextt−1
τ=tm−1+1b˜a(τ)(τ)b˜a(τ)(τ)T.Then
(8)≤tm/summationdisplay
t=tm−1+14β2/parenleftbig
1∧b˜a(t)(t)TB(t−1)−1b˜a(t)(t)/parenrightbigdet(B(t−1))
det(B′
m−1)
≤tm/summationdisplay
t=tm−1+14β2/parenleftbig
1∧b˜a(t)(t)TB(t−1)−1b˜a(t)(t)/parenrightbig
E
≤8β2Elog/parenleftigdet(B′
m)
det(B′
m−1)/parenrightig
, (9)
where the first inequality is due to Lemma A.1.1 and the fact that B(t−1)≽B′
m−1, the second inequality
is due to line 21 of Algorithm 2, and the third inequality follows from Lemma A.1.2. Due to lines 21–25 in
Algorithm 2, B′
mwithout the last sample b˜a(tm)(tm)satisfies,
det/parenleftig
B′
m−b˜a(tm)(tm)b˜a(tm)(tm)T/parenrightig
≤Edet (B′
m−1).
11Under review as submission to TMLR
Then,
det(B′
m) =det/parenleftig
B′
m−b˜a(tm)(tm)b˜a(tm)(tm)T/parenrightig
×/braceleftig
1 +b˜a(tm)(tm)T/parenleftig
B′
m−b˜a(tm)(tm)b˜a(tm)(tm)T/parenrightig−1
b˜a(tm)(tm)/bracerightig
≤det/parenleftig
B′
m−b˜a(tm)(tm)b˜a(tm)(tm)T/parenrightig/parenleftbig
1 +1
λ/parenrightbig
≤/parenleftbig
1 +1
λ/parenrightbig
Edet (B′
m−1). (10)
Due to (4), (6), (7), (8), (9), and (10) and applying the union bound, we have with probability at least
1−/summationtextM
m=1/summationtexttm
t=tm−1+1δ/t2,
R(T)≤/radicalbig
T{M14α2(Ad+ 1/λ) +M28β2Elog(E+E/λ)},
where
M/summationdisplay
m=1tm/summationdisplay
t=tm−1+1δ
t2=δ+T/summationdisplay
t=2δ
t2≤δ+/integraldisplay∞
t=1δ
t2dt= 2δ.
Plugging in the definition of αandβgives the theorem.
A.1.3 Proof of Lemma 4.2
Lemma 4.2. In the AP-UCB algorithm, we have
M1≤logC/parenleftbig
1 +T/dλ/parenrightbig
,M2≤dlogE/parenleftbig
1 +T/dλ/parenrightbig
.
Proof.Derivation of the bound of M1is given in the main text. The upper bound of M2follows from the
phase-switching condition for the adaptive phase given in line 21 of Algorithm 2. If mis an adaptive phase,
we have,
det(B′
m)≥Edet (B′
m−1).
Ifmis an independent phase, we have at least,
det(B′
m)≥det(B′
m−1),
sinceB′
m≽B′
m−1.Therefore,det(B′
M)≥EM2det(λId)withdet(B′
M)≤/parenleftbig
λ+T
d/parenrightbigdanddet(λId) =λd.Thus,
M2≤dlogE/parenleftbig
1 +T/dλ/parenrightbig
.
A.2 Experiment Details
We follow the design of Chu et al. (2011) to generate the contexts and the linear parameter µ. We setK= 2
andd= 11. We divide the T= 2000rounds into h= (d−1)/2groups ofT′=T/hrounds such that each
group has a different best arm. Time tbelongs to group r∈[0,1,···,h−1]if the remainder of dividing (t−1)
byhisr. Whentbelongs to group r, we letb1(t)have 0.5 in the first and (2r+ 2)-th components, and 0in
the remaining components. On the other hand, we let b2(t)have 0.5 in the first and (2r+ 3)-th components,
and0in the remaining components. The parameter µhas value 0.5 in the first component, 10/√
T′in either
the(2r+ 2)-th or (2r+ 3)-th coordinate for each group r, and 0 in the remaining components.
We generate the errors ηi(t)’s independently from the normal distribution N(0,1). We run the experiments
withβ∈[0.1,0.6,1.1,···,4.6]andα∈[0.1,0.2,···,0.9],and report the results of the values that incur
minimum median regret over 30 experiments.
12