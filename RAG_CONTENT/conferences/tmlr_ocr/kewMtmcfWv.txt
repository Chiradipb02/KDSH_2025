Under review as submission to TMLR
Double Descent and Other Interpolation
Phenomena in GANs
Anonymous authors
Paper under double-blind review
Abstract
We study overparameterization in generative adversarial networks (GANs) that can inter-
polate the training data. We show that overparameterization can improve generalization
performance and accelerate the training process . We study the generalization error as a
function of latent space dimension and identify two main behaviors, depending on the learn-
ing setting. First, we show that overparameterized generative models that learn distribu-
tions by minimizing a metric or f-divergence do not exhibit double descent in generalization
errors; speciﬁcally, allthe interpolating solutions achieve the same generalization error. Sec-
ond, we develop a novel pseudo-supervised learning approach for GANs where the training
utilizes pairs of fabricated (noise) inputs in conjunction with real output samples. Our
pseudo-supervised setting exhibits double descent (and in some cases, triple descent) of gen-
eralization errors. We combine pseudo-supervision with overparameterization (i.e., overly
large latent space dimension) to accelerate training while matching or even surpassing gen-
eralization performance without pseudo-supervision. While our analysis focuses mostly on
linear models, we also apply important insights for improving generalization of nonlinear,
multilayer GANs.
1 Introduction
Generative adversarial networks (GANs) ( Goodfellow et al. ,2014) are a prominent concept for addressing
data generation tasks in contemporary machine learning. GANs learn a data generator model that produces
new instances from a data class represented by a set of training examples. A GAN’s generator network
is trained in conjunction with a discriminator network that evaluates the generator’s ability and directs
it towards better performance. GANs have an intricate design and training philosophy whose theory and
practice are still far from being suﬃciently understood.
A key aspect that complicates the understanding of GANs is that, like many other deep learning architectures,
they are highly complex models with typically many more parameters than the number of training data
samples. Therefore, GANs are overparameterized models that can be trained to interpolate (i.e., memorize)
their training examples. Yet, overparameterized GANs are capable of generating high quality data beyond
their training datasets. The analysis of overparameterized machine learning is a highly active research area
that is mainly focused on supervised learning problems such as regression ( Belkin et al. ,2019b ;Bartlett
et al. ,2020;Muthukumar et al. ,2020b ;dAscoli et al. ,2020) and classiﬁcation ( Muthukumar et al. ,2020a ;
Deng et al. ,2019;Kini & Thrampoulidis ,2020).The study of overparameterization in the unsupervised
learning and data generation problems relevant to GANs is uncharted territory that we are ﬁrst to explore
in this paper.
This paper develops a new framework for the study of generalization and overparameterization in GANs
and PCA. We examine the generalization of linear GANs at diﬀerent parameterization levels by varying the
latent space dimension , which in a GAN is the dimension of the input (random noise) vectors to the data
generator. This is a practical way of controlling the parameterization of our models, since we do not need to
consider modifying the width or depth of the generator network. Our framework leads us to the following
1Under review as submission to TMLR
key insights on how the generalization performance of overparameterized linear GANs is aﬀected by the
training approach.
First, GAN training via minimization of a distribution metric or f-divergence results in unsatisfactory
generalization performance when the generator model is overparameterized and interpolates its noisy training
data. Speciﬁcally, we prove that under such a training process alloverparameterized solutions have the same
generalization performance. Moreover, the best generalization is obtained by an underparameterized solution
with the same dimension as the true latent space dimension of the data, which is usually unknown. This
set of interpolating solutions which have constant test error establishes a new generalization behavior of
generative models.
Second, our theoretical studies inspire a new pseudo-supervised training regime for GANs and show
that it can improve generalization performance in overparameterized settings where interpolation of noisy
training data occurs. Our pseudo-supervised approach selects a subset (or all) of the training data examples
and individually associates them with random (noise) vectors that act as their latent representations (i.e.,
the inputs given to the generator to yield the respective training data). Pseudo-supervision accelerates
the training process and improves generalization by reducing the number of eﬀective degrees of freedom in
overparameterized GAN learning (although in many cases the learned GAN can still interpolate the training
data). We develop several implementations for the pseudo-supervised optimization objective and examine
their respective generalization behaviors, which we show to include double descent and also triple descent of
generalization errors as a function of the latent space dimension of the learned GAN.
Third, encouraged by our new insights into linear GANs, we explore their implications for nonlinear, mul-
tilayer GANs. Speciﬁcally, we implement and study our pseudo-supervised learning scheme for a gradient-
penalized Wasserstein GAN ( Gulrajani et al. ,2017) on the MNIST digit dataset of binary images. Our
results demonstrate that pseudo-supervised learning signiﬁcantly improves generalization performance and
accelerates training when compared to training the same GAN without pseudo-supervision.
2 Related work
GANs ( Goodfellow et al. ,2014) have been very successful in modeling complex data distributions, such as
distributions of images ( Brock et al. ,2018;Karras et al. ,2019a ;b). These models are usually trained by
having two competing networks: a generator network which attempts to approximate the data distribution
and a discriminator network which attempts to classify between data from the training set and generated
data. The objective function can either be an f-divergence ( Goodfellow et al. ,2014;Nowozin et al. ,2016;
Sarraf & Nie ,2021) or a metric ( Arjovsky et al. ,2017;Gulrajani et al. ,2017) and is typically minimized
by the generator while simultaneously being maximized by the discriminator. This minmax game can be
unstable ( Salimans et al. ,2016;Mescheder et al. ,2018) and is hard to analyze in full generality; therefore
we turn to linear GANs.
Feizi et al. (2020) have studied GANs with linear generators, quadratic discriminators, and Gaussian data
(this has been named the LQG setting).
In this setting, the objective loss is the 2-Wasserstein distance between two Gaussian distributions N(µ1,Σ1)
andN(µ2,Σ2):
W2
2(N(µ1,Σ1),N(µ2,Σ2)) =/bardblµ1−µ2/bardbl2
2 (1)
+Tr(Σ1) +Tr(Σ2)−2Tr/parenleftBig/parenleftbig
Σ1
2
1Σ2Σ1
2
1/parenrightbig1
2/parenrightBig
.
This distance is well known ( Givens et al. ,1984;Olkin & Pukelsheim ,1982) and is even used in the calculation
of the well known evaluation metric FID ( Heusel et al. ,2017) in the GAN literature. One result in the LQG
setting ( Feizi et al. ,2020) is that the principal component analysis (PCA) solution is an optimal solution for
the generator in the minmax optimization.
In supervised problems, it was widely believed that the generalization error behavior as a function of the
learned model complexity is completely characterized by the bias-variance tradeoﬀ, i.e., in a supervised
setting, the test error goes down and then back up as the learned model is more complex (e.g., has more
2Under review as submission to TMLR
parameters). Relatively recently, it has been shown that test errors can have a double descent shape ( Spigler
et al. ,2018;Belkin et al. ,2019a ) as a function of the learned model complexity. Speciﬁcally, in the double
descent shape the test error goes back down when the learned model is suﬃciently complex (i.e., overparam-
eterized) to interpolate the training data (i.e., achieve zero training error). Remarkably, the double descent
shape implies that the best generalization performance can be achieved despite perfect ﬁtting of noisy train-
ing data. Typically, when models have many more parameters than training data, many mappings can
be learned to perfectly ﬁt (i.e., interpolate) the supervised pairs of examples. Therefore, a mapping with
small norm is a natural (parsimonious) choice and tends to yield low test error even when the number of
parameters is large. The research on overparameterized learning and double descent phenomena has been
mostly focused on regression ( Belkin et al. ,2019b ;Bartlett et al. ,2020;Muthukumar et al. ,2020b ;dAscoli
et al. ,2020) and classiﬁcation ( Muthukumar et al. ,2020a ;Deng et al. ,2019;Kini & Thrampoulidis ,2020)
problems. Some work has been done in overparameterized GANs ( Balaji et al. ,2021) to understand how
training stability is aﬀected by increasing the width and depth of networks. By contrast, we are the ﬁrst to
study generalization performance and double descent behavior in GANs.
Since linear GANs are associated with PCA, this study relates to work on overparameterization in PCA ( Dar
et al. ,2020) showing that, as one relaxes the orthonormal constraints and adds supervision to PCA, double
descent emerges. Moreover, if the learning is fully supervised and has no orthonormal constraints, then the
problem becomes linear regression that estimates a linear subspace. Hence, one can solve learning problems
that are partly supervised and partly orthonormally constrained to obtain solutions to problems that are
in-between PCA and linear regression. We will leverage this powerful idea to study overparameterization in
linear GANs.
3 Bad generalization: Test errors are constant in the overparameterized regime
3.1 No double descent in generative models that minimize a metric or f-divergence
The goal of training GANs and generative models in general is to learn the distribution of the data. This is
typically done by minimizing a distance between a ﬁxed (i.e., given) distribution pf, such as the empirical
distribution of the training data, and the generated distribution pθwith parameters θ. The training dataset
Dincludes nexamples {xi}n
i=1∈Rd. The next observation characterizes interpolating solutions for these
kinds of problems.
Observation 1. LetPbe the set of all probability distributions deﬁned on the measurable space (Ω,F)
equipped with any metric or f-Divergence denoted q. We let our training loss be Ltrain({xi}n
i=1,θ) =q(pf, pθ)
forpf, pθ∈Pand the test error is given by Ltest(θ) =q(pt, pθ)for the true distribution pt∈P. Then, for
any interpolating solution θ, i.e., any θso that Ltrain({xi}n
i=1,θ) = 0 , we have that
Ltest(θ) =Ltest
interpolate
where Ltest
interpolate is a non-negative constant that depends on qandpf. Moreover, pθis unique.
Proof. Letθ∗andθbe two interpolating solutions. Since qis a metric or f-divergence, the zero training
errors of the interpolating solutions θ∗andθimply that pθ∗=pfandpθ=pf, implying uniqueness since
pθ=pθ∗. Additionally,
Ltest(θ) =q(pt, pθ) =q(pt, pf) =q(pt, pθ∗) =Ltest(θ∗).
By letting Ltest
interpolate ,q(pt, pf)≥0, we get the desired result.
Corollary 1. There is no double descent in generative models that minimize a metric or f-divergence, e.g.,
PCA for subspace learning, Jensen-Shannon GANs, WGANs, etc.
In other words, there is no double descent behavior because the test error is constant in the overparameterized
regime of interpolating solutions. This diﬀers from the widely studied regression setup in that here we
are trying to minimize the distance between two distributions rather than data points drawn from those
3Under review as submission to TMLR
0 20 40 60051015 n
Latent dimensionality kTrain errorInterpolation at k≥n
0 20 40 6068n m
Latent dimensionality kTest errorConstant for k≥n
Figure 1: PCA and linear GAN’s test error becomes constant when the model interpolates, i.e., when the latent
dimensionality kequals the number of training samples n. Therefore, the overparameterized regime does not exhibit
double descent but rather a constant error. The test error achieves its minimum when the latent dimensionality
kis near the true model’s dimensionality m. The train errors (left subﬁgure) and test errors (right subﬁgure) are
calculated with the 2-Wasserstein metric; the training error is between the training data and the PCA estimate and
the test error is between the true distribution and the PCA estimate.
distributions. In other words, generative modeling treats the data itself as a distribution and not as a set
of data points. As a consequence, although there may exist more than one interpolating solution θ, there
is only one unique interpolating distribution pθ. Importantly, this result is not speciﬁc to GANs but to any
generative model that is trained to minimize the distance between the generated distribution and a ﬁxed
distribution.
We now narrow our focus to a speciﬁc data model to help understand the constant regime of generalization
errors. Recall from Section 2that in the LQG setting, PCA is a solution for the optimal linear generator.
Hence, we can study PCA solutions and evaluate them using the 2-Wasserstein metric (Equation ( 1)) to see
the generalization error of the linear generator in the LQG setting. We assume that our training data {xi}n
i=1
are realizations of a random vector x∈Rdthat satisﬁes the noisy linear model x=Γz+/epsilon1. Here Γ∈Rd×m
is a deterministic rank mmatrix (for m < d ),z∈Rmis a latent random vector of a zero-mean isotropic
Gaussian distribution, and /epsilon1∼ N (0, σ2Id)is a noise vector. The true latent dimension mis unknown to the
learner; hence, we will pick k > 0and learn a generator matrix G∈Rd×k. The true, noiseless distribution
is Gaussian: xtrue =Γz∼ N (0,ΓΓ/latticetop). Thus, if the learned latent dimension kequals the true latent
dimension m, then G=Γis an optimal solution. We assume that m < d , hence the covariance matrix of
x∼ N (0,ΓΓ/latticetop+σ2Id)is the sum of a low rank covariance matrix ΓΓ/latticetopand a full rank noise covariance
matrix, our choice of kwill aﬀect how much we overﬁt to the noise distribution.
We consider m < n < d , i.e., the number of training examples nis higher than the true latent space dimension
m, and lower than the data dimension d, for several reasons. Most importantly, data is often assumed to lie
on a low dimensional manifold in a higher dimensional space. Thus if m≥d, then ΓΓ/latticetopwill have rank d
and the noiseless data xtruewill have a non-zero probability of being in any open set in Rd, which is clearly
not true for many types of data, such as natural images. We also choose to study m < d because it will
allow our model to overﬁt (when the learned latent dimension k > m ). Now we turn to our choice of nand
note that if n≥d, we get the typical U-shaped curve of the bias-variance tradeoﬀ for generalization error as
a function of the learned latent dimension k. Ifn≤m, then the generalization error is just monotonically
decreasing in kand is of little interest because it is unlikely that we overﬁt to our data. For these reasons,
we consider only m < n < d , which permits study of the double descent phenomenon.
We train a linear GAN by picking the top kprincipal components ( k≤d), namely, minimizing the training
loss
Ltrain(G,X) =/bardbl(Id−GG/latticetop)X/bardbl2
F (2)
under the constraint that the d×kmatrix Ghas orthonormal columns. Moreover, X∈Rd×ndenotes the
data matrix with ntraining examples as its columns. If k > n , we run out of nonzero eigenvalues and cannot
add any more; the learned generator interpolates by producing zero training error. However, the test error
4Under review as submission to TMLR
will increase if we learn noise, i.e., if the eigenvalues and eigenvectors of ΓΓ/latticetop+σ2Idare corrupted by the
noise covariance σ2Id. Figure 1shows the train and test errors for the learned model as a function of the
learned latent dimension k. We obtain generalization behavior in two stages. First, there is a U-shape with
a minimum around k=m; then, as the solutions start to interpolate in the overparameterized regime of
k > n , we observe a constant test error.
To relate this back to Observation 1and Corollary 1, here the training data distribution pfisN(/hatwideµx,/hatwideΣx),
where /hatwideµx∈Rdand/hatwideΣx∈Rd×dare the empirical mean vector and covariance matrix of the training data,
respectively. Roughly speaking, we can think of the generator as learning the true distribution with some
noise for the ﬁrst mcomponents and then just learning noise in the subspace orthogonal to the data;
technically, we learn wrong directions in the data for small kif the noise variance σ2is very large. In this
setting, where the number of training samples nallows us to interpolate, the best that one can do is to
try to guess mby using prior knowledge or training multiple models using cross-validation. These solutions
are not satisfactory in many scenarios, so we delve deeper into understanding why the test error in the
overparameterized regime is constant. Speciﬁcally, we study the overparameterized regime to see if it can be
modiﬁed in a beneﬁcial way.
3.2 Double descent: Getting double descent through actual supervision
Since PCA gives us a solution to GANs trained in the LQG setting, we turn to studying overparameterization
in PCA to understand overparameterization in GANs. It was shown that PCA (with soft orthonormality
constraints) does exhibit double descent if supervision was added to the training ( Dar et al. ,2020). This
is consistent with our understanding, since supervision will cause the learned map to have fewer degrees
of freedom and ﬁt the data exactly. The work in ( Dar et al. ,2020) focuses on learning a linear subspace
for the purpose of dimensionality reduction, therefore we extend that idea to work for the purpose of data
generation.
For a start, consider an ideal setting where nsupout of the ntraining examples are given with their true
latent vectors. Namely, the training dataset Dincludes nsup∈ {0, . . . , n }supervised examples {(xi,zi)}nsup
i=1
andnunsup =n−nsupunsupervised examples {xi}n
i=nsup+1. The training data vectors are organized as the
columns of the matrices Xsup∈Rd×nsup,Zsup∈Rm×nsup, andXunsup∈Rd×nunsup, respectively.
Since in the ideal setting of this subsection we have true samples of the latent vectors zthat correspond
to data points x, this means that we know the true latent space dimension m. Hence, we can control the
parameterization of the learned model by choosing a latent dimension k≤mvia subsampling of coordinates
inz(it turns out that subsampling allows us to optimize over a pseudometric; see Appendix A). Namely,
for a set of k≤munique coordinate indices S ⊂ { 1, . . . , m }, we deﬁne {zi,S}nsup
i=1as the corresponding
subvectors of the training data {zi}nsup
i=1. The matrix Zsup
S∈Rk×nsuphas the subsampled vectors {zi,S}nsup
i=1
as its columns.
To train our model, we use a PCA loss term from ( 2) on the unsupervised portion of the data Xunsupmixed
with a supervised loss term on the supervised portion of the data Zsup,Xsup:
Ltrain(G,D) =1
nsup/bardblGZsup
S−Xsup/bardbl2
F+1
nunsup/bardbl(Id−GG/latticetop)Xunsup/bardbl2
F (3)
for a generator matrix G∈Rd×k(that is not explicitly constrained to have orthonormal columns). Unlike
the PCA optimization in ( 2), the optimizations in the current and following subsections do not include any
explicit orthonormal constraints on the columns of the learned matrix G. Here, the supervised portion of
the data gives us speciﬁc information about Γ, which we can use to train a better model. This model is
trained by minimizing the loss in Equation ( 3) with gradient descent.
Figure 2shows that our model does indeed exhibit double descent . However, there are a few limitations with
the setup which we will now enumerate.
L1This setup requires supervised pairs: We may not have access to the true latent vectors in
practice.
5Under review as submission to TMLR
0 10 20 30 400204060
Latent dimensionality k2-Wasserstein distanceTrain error
0 10 20 30 40050100150200
Latent dimensionality k2-Wasserstein distanceTest error
supervised
semi-supervised
unsupervised
Figure 2: The fully supervised model achieves a peak when the latent dimensionality kis equal to the number of
training samples n. The unsupervised model stops changing as soon as it interpolates at k=n. The semi-supervised
model with nsup= 12 behaves in a way that is somewhat in-between the other two. The train errors (left subﬁgure)
and test errors (right subﬁgure) are calculated with the 2-Wasserstein metric; the training error is between the training
data and the PCA estimate and the test error is between the true distribution and the PCA estimate. For other
values of nsupand implementation details, see Appendix D.
L2This setup requires us to throw away data: We can only vary kfrom 1tombecause we
are subsampling coordinates from the true latent vectors in Rm. In practice, we would not want to
subsample as this throws away potentially useful data information.
L3This setup is not typical: Because k < m , we must have n < m to get a peak in the test error
atk=nwhich is not the interesting/typical setting where m < n < d (see Section 3.1). Thus, we
are not overﬁtting, which happens when we learn eigenvectors in the noise directions orthogonal to
the data directions, for which k > m .
L4This setup is supervised, unlike most generative models: We would like to study generative
models, which are typically unsupervised. By adding supervision, we are not actually studying an
unsupervised setting but rather a setting which is semi-supervised.
L5This setup is not beneﬁcial: The double descent here actually does not improve performance.
Recall that we want to investigate the overparameterization of generative models to ﬁnd settings that are
realistic and beneﬁcial. We resolve all these problems with pseudo-supervision, deﬁned in the next section.
4 Pseudo-supervision: A practical alternative to adding supervision
4.1 Deﬁnition of pseudo-supervision
Input-output pairs of points are not realistically available in GAN training, which is unsupervised. Therefore,
we will make up latent vectors that correspond to true data points in our training set. We call these vectors
pseudo-supervised latent vectors. Although it may seem odd to partially fabricate training data, there are
many advantages to it, starting with not needing access to supervised data (solving L1). Because we have full
control over the generation of latent vectors, we can make them of any dimension (solving L2). Consequently,
we do not need to know the true latent dimensionality mand can study when k > m (solving L3). Since
the pseudo-supervised latent vectors are artiﬁcial, this is still an unsupervised problem because we have no
supervised data (solving L4). The obvious question is of course “is this beneﬁcial?” The rest of the paper
will show that there are generalization and convergence beneﬁts to pseudo-supervision (solving
L5).
To understand why pseudo-supervision works, consider the supervised scenario discussed in Section 3.2
except with only one supervised sample: (z1,x1). Now suppose that zps∈Rmis a completely fabricated
6Under review as submission to TMLR
sample, independent of x1, drawn from the same distribution as z1. We know that if Gunsupis a solution to
the unsupervised optimization, then so is GunsupUwhere Uzps=z1andUis an orthonormal matrix (see
Theorem 7.3.11 in ( Horn & Johnson ,2012)). This is because (GunsupU)(GunsupU)/latticetop=Gunsup(Gunsup)/latticetop
since positive deﬁnite matrices are unique up to a orthonormal transformation. Such a matrix exists if
/bardblzps/bardbl2=/bardblz1/bardbl2because Uis a norm-preserving operator. In other words, it doesn’t matter if we use z1
orzpsas long as /bardblzps/bardbl2=/bardblz1/bardbl2. Miraculously, by the curse of dimensionality, /bardblzps/bardbl2=/bardblz1/bardbl2with high
probability if kis large enough! This line of reasoning can be extended past one pseudo-supervised example
nps= 1 tonps=k(see Appendix B), after which we incur a penalty for learning a bad representation
(because we cannot ﬁnd an orthonormal matrix which will satisfy the conditions above). Therefore, because
of positive deﬁnite matrix symmetries and the curse of dimensionality, we can use pseudo-supervision in a
very similar way to supervision without actually knowing any additional information.
In the following subsections we will deﬁne several pseudo-supervised settings, in all of which npsout of the n
given training examples {xi}n
i=1are associated with pseudo (i.e., artiﬁcial) latent vectors of dimension k > 0
(because the true latent dimension is unknown in general). Speciﬁcally, the training dataset Dincludes
nps∈ {0, . . . , n }pseudo-supervised examples {(xi,zi)}nps
i=1, where {zi}nps
i=1are i.i.d. samples of N(0,Ik),
andnunsup =n−npsunsupervised examples {xi}n
i=nps+1. Therefore, the pseudo-supervised vectors are
independent and identically distributed samples from an isotropic Gaussian distribution. The training data
vectors are organized as the columns of Xps∈Rd×nps,Zps∈Rk×nps, andXunsup∈Rd×nunsup.
4.2 Double descent and superior performance with pseudo-supervision
Our ﬁrst pseudo-supervised experiment is a straightforward modiﬁcation of the experiment in Section 3.2.
We modify Equation ( 3) to get the new pseudo-supervised loss
Ltrain(G,D) =1
nps/bardblGZps
S−Xps/bardbl2
F+1
nunsup/bardbl(Id−GG/latticetop)Xunsup/bardbl2
F, (4)
where Xps∈Rd×npsandZps∈Rk×npsare the pseudo-supervised matrices. For nunsup = 0ornps= 0, we
only use the ﬁrst or second term in the loss, respectively. We provide a detailed explanation of the gradient
calculations and optimization procedure in Appendix D. Note that since the pseudo-supervised latent vectors
are completely fabricated, we do not have to subsample their coordinates (i.e., as in the supervised setting
of Section 3.2) and we can choose kto be any natural number. As shown in the ﬁrst column of Figure 3,
we achieve beneﬁcial double descent behavior of test error. To the best of our knowledge, this is the ﬁrst
time that double descent has been used beneﬁcially in an unsupervised setting.
We have extremely low generalization error when nps=n, even though the loss function does not try to
optimize any PCA-type loss. When nps=n, we have nunsup = 0and the loss Ltrain(G,D) =/bardblGZps
S−Xps/bardbl2
Fis
completely pseudo-supervised. One would expect this scenario to perform poorly since the pseudo-supervised
examples do not provide any information, and indeed it does – for small k. However, when kis large, we
perform well, even though the loss does not attempt to minimize the original PCA loss. Thus, instead
of guessing the true latent dimension mthat is required for good performance in the standard setting of
Section 3.1,we can simply add pseudo-supervision and increase overparameterization to achieve
low generalization error!
As can be seen from the ﬁrst column of Figure 3, we achieve better generalization performance via the double
descent phenomenon, and we also accelerate training convergence. Interestingly, convergence time actually
exhibits double descent as well. The accelerated convergence may be partly due to the unsupervised loss
dropping oﬀ when nps=n; however, we will address this in the next section by having a more regularized
loss function.
4.3 Regularized pseudo-supervision
In the previous section, as npsincreases, the unsupervised term in the loss drops oﬀ. This term, in some
sense, regularizes the optimization by encouraging the solution to be orthonormal. This is because, if G
has orthonormal columns, then (Id−GG/latticetop)x= 0for all xin the columnspace of G. We will then use the
7Under review as submission to TMLR
0 50 10005101520
Latent dimensionality kTest errorTrained using Equation ( 4)
0 50 10005101520
Latent dimensionality kTrained using Equation ( 5)
0 50 10005101520
Latent dimensionality kTrained using Equation ( 7)
nps= 20
nps= 12
nps= 0
0 50 10000.511.52
Latent dimensionality kTrain errorTrained using Equation ( 4)
0 50 10000.511.52
Latent dimensionality kTrained using Equation ( 5)
0 50 10000.511.52
Latent dimensionality kTrained using Equation ( 7)
nps= 20
nps= 12
nps= 0
0 50 1000200400
Latent dimensionality k# Iterations to convergenceTrained using Equation ( 4)
0 50 1000200400
Latent dimensionality kTrained using Equation ( 5)
0 50 1000200400
Latent dimensionality kTrained using Equation ( 7)
nps= 20
nps= 12
nps= 0
Figure 3: Evaluation of test error and training convergence speed in learning of linear GANs using the three diﬀerent
training loss formulations in ( 4),(5),(7). In the ﬁrst column of subﬁgures, we use ( 4) and get double descent that
beats the unsupervised baseline in both generalization performance and convergence speed in the overparameterized
range of solutions (the baseline corresponds to the case of no pseudo-supervised training samples nps= 0). In the
second column of subﬁgures, we use ( 5) and squash the double descent to get lower generalization error for small
latent dimensionality k. In the third column of subﬁgures, we get triple descent (one peak at k=nand one peak
atk=d) as well as low generalization errors and extremely fast training speed for large k. In these experiments,
the true data is m= 10 dimensional, the data space is d= 64 dimensional, and we have n= 20 total training data
samples. The null estimator ( G=0d×k) achieves a test error of approximately 13, so all of these models perform
better for large enough k. For additional plots, see Appendix D.
full data matrix X(which is a horizontal concatenation of XpsandXunsup) in the second term of the loss
function:
Ltrain(G,D) =1
nps/bardblGZps
S−Xps/bardbl2
F+1
n/bardbl(Id−GG/latticetop)X/bardbl2
F. (5)
The results for this optimization are shown in the second column of Figure 3.
8Under review as submission to TMLR
This regularized setting with pseudo-supervision outperforms the completely unsupervised setting, but we do
not interpolate (i.e., we do not achieve zero train loss), and, consequently, do not see a double descent. This
is typical for more regularized problems, as regularization tends to attenuate the double descent phenomenon
(see, e.g., for orthonormality constraints in ( Dar et al. ,2020), or for ridge regularization in ( Hastie et al. ,2019;
Nakkiran et al. ,2020)). However, this suggests that the relative importance between the ﬁrst and second
term may signiﬁcantly impact double descent behavior. More speciﬁcally, the only diﬀerence between this
optimization and the one discussed in Section 4.2is that the second term uses all the data even when nps>0.
Thus, we can think of the second term as a regularizer for the loss. On the other hand, we can view the ﬁrst
term as constraining the optimization to ﬁt our pseudo-supervised pairs of points, and thus also a regularizer.
Therefore, depending on the point of view, each term can regularize the loss.
Since either of the terms in the training loss in ( 5) can be perceived as a regularizer, we augment ( 5) with
disproportionate weighting in order to see if this aﬀects the generalization behavior (e.g., the existence of
double descent phenomena):
Ltrain(G,D) =α
nps/bardblGZps
S−Xps/bardbl2
F+1−α
n/bardbl(Id−GG/latticetop)X/bardbl2
F, (6)
forα∈[0,1]. A ﬁgure of the results is shown in Appendix D. Surprisingly, weighting the loss function in this
manner actually does achieve double descent, which leads to lower test error. We discuss this model here in
order to highlight that the relative importance between the pseudo-supervised and unsupervised loss terms
can induce double descent behavior.
Since we do not exhibit double descent using Equation ( 5), it is interesting to characterize how the pseudo-
supervision term aﬀects the solution set of the problem. The next two theorems characterize the usual
unsupervised solutions and the pseudo-supervised solutions. Their proofs are included in Appendix C.
Theorem 1. Suppose that X∈Rd×nhas full rank of min{d, n}. For the unsupervised loss L/latticetop
unsup (G,X)∆=
/bardbl(Id−GG/latticetop)X/bardbl2
F, letS/latticetop
unsup (k)∆={G∈Rd×k:L/latticetop
unsup (G,X) = 0}be the set of interpolating solutions.
Then,
1.S/latticetop
unsup (k) =∅ifn > k .
2.S/latticetop
unsup (k)is a smooth manifold of dimensionn(n−1)
2when n=k.
3.S/latticetop
unsup (k)is the union of/parenleftbign
k/parenrightbig
smooth manifolds of dimensionn(n−1)
2(k−n)(d−n)when k > n .
Theorem 2. Suppose that X∈Rd×nhas full rank of min{d, n}and let λ > 0be given. For the pseudo-
supervised loss L/latticetop
ps(G,X;λ)∆=λ
nps/bardblGZps
S−Xps/bardbl2
F+1
n/bardbl(Id−GG/latticetop)X/bardbl2
F, letS/latticetop
ps(k)∆={G∈Rd×k:
L/latticetop
ps(G,X, λ) = 0}be the set of interpolating solutions. Then,
1.S/latticetop
ps(k) =∅ifn > k andZ∈Rk×nis arbitrary.
2.S/latticetop
ps(k)has only one element if n=kandZ∈Rk×nis given so that Z/latticetopZ=X/latticetopX.
3.S/latticetop
ps(k)is the union of/parenleftbign
k/parenrightbig
smooth manifolds of dimension (k−n)(d−n)ifk > n andZ=/bracketleftbiggZ1
0/bracketrightbigg
∈
Rk×nis given so that Z/latticetop
1Z1=X/latticetopX.
Note that although there exists many unsupervised solutions (Theorem 1), the pseudo-supervised solutions
(Theorem 2) depend heavily on the condition that Z/latticetopZ=X/latticetopX. Indeed this condition does not happen in
practice with a Gaussian Z, resulting in no interpolation and a regularizing eﬀect. This restrictive condition
comes from the transpose in the unsupervised term. In the next section we will relax this into a pseudo-inverse
in order to interpolate better.
9Under review as submission to TMLR
4.4 Triple descent and huge latent spaces
The similar losses of Equations ( 4) to ( 6) indirectly encourage learning semi-orthogonal generator matrices.
We can relax this constraint and let our generator learn more complex linear functions by optimizing
Ltrain(G,D) =1
nps/bardblGZps
S−Xps/bardbl2
F+1
n/bardbl(Id−GG†)X/bardbl2
F, (7)
where G†is the Moore-Penrose pseudo-inverse of the matrix G. Training this loss may seem similar to the
others, but the results are quite diﬀerent.
With this new loss, we achieve triple descent and desirable generalization and convergence behavior when
the latent dimensionality kis larger than the data space dimensionality d(third column of Figure 3). This
scenario is most closely related to neural networks because the models that we learn are very general and
typically not constrained (e.g., to have orthonormal layers). Moreover, the pseudo-supervised optimization
converges to a solution which beats the unsupervised baseline with few iterations.
Just as we did in Section 4.4, we will characterize the solution sets for unsupervised pseudoinverse loss and
the pseudo-supervised pseudoinverse loss. The next two theorems do this and show that we no longer have
the restrictive Z/latticetopZ=X/latticetopXcondition. Their proofs are located in Appendix C.
Theorem 3. Suppose that X∈Rd×nhas full rank of min{d, n}. For the unsupervised loss L†
unsup (G,X)∆=
/bardbl(Id−GG†)X/bardbl2
F, letS†
unsup (k)∆={G∈Rd×k:L†
unsup (G,X) = 0}be the set of interpolating solutions.
Then,
1.S†
unsup (k) =∅ifn > k .
2.S†
unsup (k)is a smooth manifold of dimension n2when n=k.
3.S†
unsup (k)is the union of/parenleftbign
k/parenrightbig
smooth manifolds of dimension n2(k−n)dwhen k > n .
Theorem 4. Suppose that X∈Rd×nhas full rank of min{d, n}and let λ > 0be given. L†
ps(G,X;λ)∆=
λ
nps/bardblGZps
S−Xps/bardbl2
F+1
n/bardbl(Id−GG†)X/bardbl2
F, letS†
ps(k)∆={G∈Rd×k:L†
ps(G,X, λ) = 0 }be the set of
interpolating solutions. Then,
1.S†
ps(k) =∅ifn > k .
2.S†
ps(k)has only one element when n=k.
3.S†
ps(k)an aﬃne space of dimension (k−n)dwhen k > n .
5 Nonlinear GANs: Double descent and faster training
In this section we show that double descent can occur in nonlinear, multilayer GANs trained with pseudo-
supervision. Finding the right experimental setting for double descent was diﬃcult because the level of
parameterization is much harder to quantify in a multilayer network. We still determined the overparame-
terization solely by modifying the latent dimensionality kand not by making the networks wider or deeper.
The right side of Figure 4shows double descent for our pseudo-supervised model. We trained a total of 430
GANs (with diﬀerent latent dimensionalities and initializations) to make that ﬁgure, which is why a study
like this would be computationally prohibitive on models that take a signiﬁcant amount of time to train.
We also found that these realistic GANs trained with pseudo-supervision converge to a good solution much
faster than they would have without the pseudo-supervision. Figures 4to7show the test errors as training
progressed for diﬀerent latent dimensionalities. The pseudo-supervised models converge much faster and
10Under review as submission to TMLR
10110210300.20.40.6
Latent dimensionality kTest errorEpoch = 948
10010110210300.20.40.6
Latent dimensionality kEpoch = 2052
10010110210300.20.40.6
Latent dimensionality kEpoch = 3000
Pseudo-supervised
Baseline
Figure 4: Test errors for multilayer, nonlinear GANs trained on the MNIST digit dataset. On the left we see that
the baseline error resembles a noisy version of the test error in Figure 1, characterized by an initial dip and then
high levels of error. Our pseudo-supervision training beats the baseline here. As we continue to train (epoch 2052),
we see that the baseline error reduces, which may be due to some kind of implicit regularization. On the right, our
pseudo-supervised model achieves double descent at epoch 3000. Here the test error is measured by geometry score.
performed very well for models trained on both MNIST and CelebA. On the MNIST training, the pseudo-
supervised models converged to the lowest test error after only about 750 epochs compared to about 1,500
epochs in the baseline case. On the CelebA training, the pseudo-supervised models converged to the lowest
test error after only about 10,000 epochs compared to about 40,000 epochs in the baseline case.
The test error in Figure 4for the MNIST baseline had an initial dip then continued up to high levels around
epoch 948, suggesting overﬁtting similar to what we saw in the linear models. We suspect that this overﬁtting
was reduced as we continued to train because of some internal regularization, such as the batch norm in the
model.
We performed these experiments with some non-standard procedures to aid in our understanding of general-
ization and double descent phenomena in GANs. In this work, we are not concerned with training state-of-
the-art GANs. For this reason, our experiments are on MNIST ( LeCun et al. ,1998) and CelebA ( Liu et al. ,
2015). Since MNIST is not very complex, we only use a random subset of 4,096 training data points and
perform gradient descent using a gradient penalized Wasserstein GAN1(for SGD results, see Appendix E).
Commonly used performance metrics such as FID ( Heusel et al. ,2017) and IS ( Salimans et al. ,2016) are
made for natural images since they use the Inception v3 ( Szegedy et al. ,2016) model trained on ILSVRC
2012 ( Russakovsky et al. ,2015). Therefore, we use the geometry score ( Khrulkov & Oseledets ,2018), which
is better suited for MNIST2. The experiments on CelebA only uses a random subset of 128 training data
points for the same reasons. However, we use FID to evaluate the CelebA experiments with FID evaluated
using 128 images for computational eﬃciency. In addition to the reasons stated above, for the MNIST and
CelebA experiments, we use subsets of data because if we use the whole dataset we run out of orthogonal
pseudo-supervised vectors. This is a limitation of our work when it comes to smaller networks which have
small latent spaces compared to the number of data points. See Appendix Efor more details on the training.
6 Conclusion
We have demonstrated that pseudo-supervision can be used to achieve beneﬁcial double descent phenomena
in unsupervised models, speciﬁcally in linear GANs and nonlinear, multilayer GANs. Pseudo-supervision
can help accelerate training and lower generalization error. This opens up areas of research in understanding
overparameterization and double descent behavior in unsupervised models. Moreover, our ﬁndings suggest
that an empirical study on ImageNet with more complex networks is beneﬁcial to improve state-of-the-art
generalization error and convergence speed.
1The architecture implementation can be found here
2The geometry score implementation can be found here
11Under review as submission to TMLR
1 7001603000
Latent dimensionality kEpochBaseline
0.00.20.40.60.81.0Test Error
1 7001603000
Latent dimensionality kEpochPseudo-supervised
0.00.20.40.60.81.0Test Error
Figure 5: These test error heatmaps for multilayer, nonlinear GANs trained on MNIST show that the pseudo-
supervised models converge faster than the baseline models. The baseline model has high test error until around
epoch 1500, unlike the pseudo-supervised models which have the test error drop oﬀ at around epoch 750. The baseline
model only beats the pseudo-supervised model later in the training (around epoch 2500), when the pseudo-supervised
loss increases and admits a double descent shape. The test error is measured by geometry score here. The k-axis
is plotted so that each column corresponds to the next entry for better visualization, even though the spacing is
k∈ {1,2,4,6, . . . , 70,100,200,300, . . . , 700}.
2 128 8192103104105
Latent dimensionality kEpochBaseline
100150200250Test FID
2 128 8192103104105
Latent dimensionality kEpochPseudo-supervised
100150200250Test FID
Figure 6: These test error (measured via FID) heatmaps for multilayer, nonlinear GANs trained on CelebA show
that the pseudo-supervised models converge faster than the baseline models. The baseline model has high test error
until around epoch 40,000, unlike the pseudo-supervised models which have the test error drop oﬀ at around epoch
10,000. The baseline model only beats the pseudo-supervised model later in the training, however only in the lower
parameterized regime. The k-axis is plotted for k∈ {2,4,8, . . . , 4096 ,8192}and the epoch axis is plotted from 1000
to100,000.
12Under review as submission to TMLR
2226210100150200
Latent dimensionality kTest FIDEpoch = 10k
2226210100150200
Latent dimensionality kEpoch = 25k
2226210100150200
Latent dimensionality kEpoch = 100k
Pseudo-supervised
Baseline
Figure 7: Test errors (measured via FID) for a multilayer, nonlinear GAN trained on the CelebA dataset. On the
left we see that the baseline error is quite high and our pseudo-supervision training has almost converged after only
10k epochs. As we continue to train (epoch 25k), we see that the baseline error reduces along with the pseudo-
supervised error. On the right, we see that although the baseline error can even beat the pseudo-supervised error for
certain model parameterizations, this is not the case for highly overparameterized models where pseudo-supervision
still outperforms the baseline.
13Under review as submission to TMLR
References
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In
International conference on machine learning , pp. 214–223. PMLR, 2017.
Sheldon Axler. Measure, Integration & Real Analysis . Springer Nature, 2020.
Sheldon Jay Axler. Linear algebra done right , volume 2. Springer, 1997.
Yogesh Balaji, Mohammadmahdi Sajedi, Neha Mukund Kalibhat, Mucong Ding, Dominik Stöger, Mahdi
Soltanolkotabi, and Soheil Feizi. Understanding overparameterization in generative adversarial networks.
arXiv preprint arXiv:2104.05605 , 2021.
P. L. Bartlett, P. M. Long, G. Lugosi, and A. Tsigler. Benign overﬁtting in linear regression. Proceedings of
the National Academy of Sciences , 2020.
M. Belkin, D. Hsu, S. Ma, and S. Mandal. Reconciling modern machine-learning practice and the classical
bias–variance trade-oﬀ. Proceedings of the National Academy of Sciences , 116(32):15849–15854, 2019a.
M. Belkin, D. Hsu, and J. Xu. Two models of double descent for weak features. arXiv preprint
arXiv:1903.07571 , 2019b.
Christopher M Bishop. Pattern recognition and machine learning . springer, 2006.
Andrew Brock, Jeﬀ Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity natural image
synthesis. arXiv preprint arXiv:1809.11096 , 2018.
Y. Dar and R. G. Baraniuk. Double double descent: On generalization errors in transfer learning between
linear regression tasks. SIAM Journal on Mathematics of Data Science , 4(4):1447–1472, 2022.
Yehuda Dar, Paul Mayer, Lorenzo Luzi, and Richard Baraniuk. Subspace ﬁtting meets regression: The eﬀects
of supervision and orthonormality constraints on double descent of generalization errors. In International
Conference on Machine Learning , pp. 2366–2375. PMLR, 2020.
Z. Deng, A. Kammoun, and C. Thrampoulidis. A model of double descent for high-dimensional binary linear
classiﬁcation. arXiv preprint arXiv:1911.05822 , 2019.
Stéphane dAscoli, Maria Reﬁnetti, Giulio Biroli, and Florent Krzakala. Double trouble in double descent:
Bias and variance(s) in the lazy regime. In International Conference on Machine Learning , pp. 2280–2290.
PMLR, 2020.
Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding GANs in the LQG setting: Formu-
lation, generalization and stability. IEEE Journal on Selected Areas in Information Theory , 2020.
James E Gentle. Matrix algebra. Springer texts in statistics, Springer, New York, NY, doi , 10:978–0, 2007.
Clark R Givens, Rae Michael Shortt, et al. A class of Wasserstein metrics for probability distributions. The
Michigan Mathematical Journal , 31(2):231–240, 1984.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing
Systems (NIPS) , pp. 2672–2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved
training of Wasserstein GANs. arXiv preprint arXiv:1704.00028 , 2017.
T. Hastie, A. Montanari, S. Rosset, and R. J. Tibshirani. Surprises in high-dimensional ridgeless least squares
interpolation. arXiv preprint arXiv:1903.08560 , 2019.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining,
inference, and prediction . Springer Science & Business Media, 2009.
14Under review as submission to TMLR
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems (NIPS) , pp. 6626–6637, 2017.
Roger A Horn and Charles R Johnson. Matrix analysis . Cambridge university press, 2012.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp.
4401–4410, 2019a.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and
improving the image quality of StyleGAN. arXiv preprint arXiv:1912.04958 , 2019b.
Valentin Khrulkov and Ivan Oseledets. Geometry score: A method for comparing generative adversarial
networks. In International Conference on Machine Learning , pp. 2621–2629. PMLR, 2018.
Ganesh Ramachandra Kini and Christos Thrampoulidis. Analytic study of double descent in binary classi-
ﬁcation: The impact of loss. In 2020 IEEE International Symposium on Information Theory (ISIT) , pp.
2527–2532, 2020.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
John M Lee. Smooth manifolds. In Introduction to Smooth Manifolds , pp. 1–29. Springer, 2003.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV) , December 2015.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually
converge? In International conference on machine learning , pp. 3481–3490. PMLR, 2018.
V. Muthukumar, A. Narang, V. Subramanian, M. Belkin, D. Hsu, and A. Sahai. Classiﬁcation vs regression
in overparameterized regimes: Does the loss function matter? arXiv preprint arXiv:2005.08054 , 2020a.
V. Muthukumar, K. Vodrahalli, V. Subramanian, and A. Sahai. Harmless interpolation of noisy data in
regression. IEEE Journal on Selected Areas in Information Theory , 2020b.
P. Nakkiran, P. Venkat, S. Kakade, and T. Ma. Optimal regularization can mitigate double descent. arXiv
preprint arXiv:2003.01897 , 2020.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using
variational divergence minimization. arXiv preprint arXiv:1606.00709 , 2016.
Ingram Olkin and Friedrich Pukelsheim. The distance between two random vectors with given dispersion
matrices. Linear Algebra and its Applications , 48:257–263, 1982.
KB Petersen and MS Pedersen. The matrix cookbook, version 20121115. Technical Univ. Denmark, Kongens
Lyngby, Denmark, Tech. Rep , 3274, 2012.
Alfréd Rényi et al. On measures of entropy and information. In Proceedings of the Fourth Berkeley Symposium
on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics . The
Regents of the University of California, 1961.
Halsey Lawrence Royden and Patrick Fitzpatrick. Real analysis , volume 32. Macmillan New York, 1988.
Walter Rudin. Principles of mathematical analysis . McGraw-hill New York, third edition, 1964.
Walter Rudin. Real and complex analysis. 1987. Cited on , 156, 1987.
15Under review as submission to TMLR
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale
Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015.
doi: 10.1007/s11263-015-0816-y.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training GANs. arXiv preprint arXiv:1606.03498 , 2016.
Aydin Sarraf and Yimin Nie. RGAN: Rényi generative adversarial network. SN Computer Science , 2(1):1–8,
2021.
S. Spigler, M. Geiger, S. d’Ascoli, L. Sagun, G. Biroli, and M. Wyart. A jamming transition from under-to
over-parametrization aﬀects loss landscape and generalization. arXiv preprint arXiv:1810.09665 , 2018.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioﬀe, Jon Shlens, and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pp. 2818–2826, 2016.
Cédric Villani. Topics in optimal transportation . American Mathematical Soc., 2003.
Cédric Villani. Optimal transport: old and new , volume 338. Springer Science & Business Media, 2008.
16Under review as submission to TMLR
Appendices
The appendices below support the main paper as follows. Appendix Aprovides additional details on how
subsampling (or zeroing) coordinates of the data is equivalent to training with a pseudometric as discussed in
Section 3.2of the main paper. In Appendix Bwe expand on pseudo-supervision and explain when it can be
used to mimic supervision. Appendix Dincludes additional empirical results and details for the linear GAN
problems from Sections 3.2and4.2to4.4of the main paper. Appendix Eprovides additional experimental
results and details for the multilayer, nonlinear GAN from Section 5of the main paper; we also include
results for SGD-based training of GANs using the complete MNIST dataset.
A Training with pseudometric and subsampling data
The problem in Section 3.1is that we are optimizing over a metric q, which has a deﬁniteness property,
i.e.,q(x, y) = 0 if and only if x=y. If we relax this property, we are left with a pseudometric; similarly,
we can relax this property to obtain a non-deﬁnite f-divergence. Interestingly, we found that subsampling
coordinates of the data is equivalent to using a pseudometric, and we will use subsampling in the next section
to control our level of parameterization. We provide a detailed discussion on the pseudometric formulation in
Appendix Asince several papers on double descent use feature subsampling to control the parameterization
of the model ( Belkin et al. ,2019b ;Dar et al. ,2020;Dar & Baraniuk ,2022).
Proposition 1. Letqdbe any metric on Rdandqkbe any metric on Rk. Suppose that xd,yd∈Rdare
subsampled (in the same way) to xk,yk∈Rk, with k < d , i.e., the elements of xkandykare a subset of the
elements of xdandyd, respectively. Then, the function q/prime:Rd×Rd→Rdeﬁned as q/prime(xd,yd) =qk(xk,yk)
is a pseudo-metric in Rd.
Proof. We ﬁrst show that q/primeis strictly semi-deﬁnite. For all xd∈Rdwe have that q/prime(xd,xd) =qk(xk,xk) = 0 .
Note however, that if we modify an element of xdwhich is not present in xkto create x/prime
d/negationslash=xdwe will
still get q/prime(xd,x/prime
d) = qk(xk,xk) = 0 , implying that q/primeis not strictly deﬁnite and hence that q/primecannot
be a proper metric. Moreover, we see that q/primeis symmetric because for all xd,yd∈Rdwe have that
q/prime(xd,yd) =qk(xk,yk) =qk(yk,xk) =q/prime(yd,xd).
Now we must show that the triangle inequality holds for q/prime. Suppose that xd,yd,zd∈Rdare subsampled to
xk,yk,zk∈Rk. Then,
q/prime(xd,zd) =qk(xk,zk)
≤qk(xk,yk) +qk(yk,zk) (triangle inequality on qk)
=q/prime(xd,yd) +q/prime(yd,zd),
as desired. Thus, q/primeis a pseudo-metric.
A.1 Subsampling the data features
In Section 3.1we saw that if we optimize an objective function which is a metric ( Rudin ,1964) or an f-
divergence ( Rényi et al. ,1961;Nowozin et al. ,2016), the resulting generalization error will be constant for
any interpolating solution. This is due to the deﬁniteness of the metric or f-divergence. In this section we will
relax this property for the 2-Wasserstein metric ( Villani ,2003;2008); extensions to this relaxation can be done
forf-divergences and other metrics. The resulting mathematical object is called a pseudometric ( Royden &
Fitzpatrick ,1988), which has been studied thoroughly in the context of Lpmetrics in Banach spaces ( Axler ,
2020;Royden & Fitzpatrick ,1988).
Deﬁnition 1. We denote qdto be the standard Euclidean metric on Rd(Rudin ,1964). Let P(Rd)be the
set of all probability distributions deﬁned on the measurable space (Rd,B(Rd)), where B(Rd)is the Borel
σ-algebra on Rd(Axler ,2020;Rudin ,1987). We denote Wd:P(Rd)×P(Rd)→Rto be the 2-Wasserstein
metric:
Wd(P, P/prime) =/radicalBigg
inf
γ∈Π(P,P/prime)/integraldisplay
Rd×Rdq2
d(x,y)dγ(x,y),
17Under review as submission to TMLR
where γ∈Π(P, P/prime)is any joint distribution of PandP/prime. For a set A⊂ {1, . . . , d }, we deﬁne the pseudometric
Wd,A:Rd×Rd→Rto be the 2-Wasserstein metric on Rd−|A|on the indices not in A. For example, if
P2,...,d, P/prime
2,...,dare the marginals (after integrating out the ﬁrst component) of PandP/prime, respectively, then
Wd,{1}(P, P/prime):=Wd−1(P2,...,d, P/prime
2,...,d).
Clearly, Wd,Ais a pseudometric as it derives all metric properties from Wd−|A|except the deﬁniteness
property.
This pseudometric is constructed by integrating out certain coordinates of the distributions and using a
metric on the resulting marginal distributions. Therefore it is possible to have zero distance between two
distributions that diﬀer along the coordinates which are integrated out. This is equivalent to subsampling
or zeroing out the desired coordinates, which we will shortly show. Thus, for the linear case, we can learn a
generator Gwhich maps our latent space to Rdand which learns the training data distributions pfexcept
for the ignored coordinates. Of course, now we have a whole (aﬃne) subspace of matrices G∈Rd×kthat
we can learn. In other words, using a pseudometric, an interpolating solution G∈Rd×kforms an aﬃne
subspace of Rd×kif modiﬁed along the ignored coordinates. As we will see in Appendix B, we can also
transform Gby an orthonormal transformation to get more degrees of freedom than just this aﬃne space.
In this setting, the min-norm solution will not project anything on the ignored coordinates.
Theorem 5. LetPandP/primebe two distributions deﬁned on Rd. Let A⊂ {1, . . . , d }be a subset of the axis
indices. We deﬁne a new distribution QAonRdas the product of |A|univariate point masses at 0and the
marginal distribution PAC. The point masses are located so that the univariate marginals of QAare point
masses along the coordinates in A. We deﬁne Q/prime
Asimilarly. Then,
Wd,A(P, P/prime):=Wd−|A|(PAC, P/prime
AC) =Wd(QA, Q/prime
A). (8)
Proof. An application of Tonelli’s Theorem ( Axler ,2020) shows that
Wd(QA, Q/prime
A) =/radicalBigg
inf
γ/integraldisplay
Rd×Rdq2
d(x,y)dγ
=/radicaltp/radicalvertex/radicalvertex/radicalbtinf
γd/summationdisplay
i=1/integraldisplay
Rd×Rd|xi−yi|2dγ
=/radicalBigg
inf
γ/summationdisplay
i∈A/integraldisplay
Rd×Rd|xi−yi|2dγ+/summationdisplay
i∈AC/integraldisplay
Rd×Rd|xi−yi|2dγ
=/radicalBigg
inf
γ/integraldisplay
R2|A|/summationdisplay
i∈A|xi−yi|2dγA+/integraldisplay
R2(d−|A|)/summationdisplay
i∈AC|xi−yi|2dγAC (Tonelli)
=/radicalBigg
inf
γAC/integraldisplay
R2(d−|A|)/summationdisplay
i∈AC|xi−yi|2dγAC (∗)
=Wd−|A|(PAC, P/prime
AC)
=Wd,A(P, P/prime),
where γAandγACare the joints of the marginals over Aand over AC, respectively. We also use independence
when using Tonelli’s Theorem, because QAandQ/prime
Aare product measures by construction. In (∗), we pick
γAto be the independent joint distribution so that each random variable with index in Ais independent.
Since each of these random variables is identical, the integral term on the left vanishes and is therefore the
minimizer of the inﬁmum.
Theorem 5shows that we can train with a pseudometric by simply zeroing the coordinates of the data that
we wish to ignore; alternatively, we can also subsample the features so that we keep the features with indices
18Under review as submission to TMLR
inAC. This allows us to consider a pseudometric Wd,Awhich is invariant to the data features with indices
inA. Suppose that we instead want Wd,Ato be invariant to a speciﬁc subspace. It turns out that these two
concepts are closely related.
Theorem 6. LetV⊂Rdbe a subspace spanned by the orthonormal vectors v1, . . . ,vm; the rest of Rd
is spanned by vm+1, . . . ,vdso that {vi}d
i=1is an orthonormal basis for Rd. We also have a data matrix
X∈Rd×n. Then, we can construct a pseudometric Wd,Vto be invariant to the subspace Vby replacing the
ﬁrstmrows of U/latticetopXwith zeros for U=/bracketleftbigv1. . .vd/bracketrightbig
∈Rd×d.
Proof. Letv∈Vbe given. Then, we can write v=/summationtextm
i=1civi. Clearly, we have that U/latticetopv=/summationtextm
i=1ciU/latticetopvi=/bracketleftbigc1. . . c m0. . . 0/bracketrightbig/latticetop. Similarly, if w∈Vis arbitrary, then we have that U/latticetopw=/bracketleftbiga1. . . a mam+1. . . a d/bracketrightbig/latticetopfor some numbers ai∈R. Hence, by replacing the ﬁrst mcoordinates by 0
we project onto the subspace orthogonal to V. Applying U/latticetopto each column of Xis equivalent to computing
U/latticetopX.
Thus, without loss of generality, we consider only subsampling feature indices. If we want to ignore a
subspace, we simply multiply our data matrix by the correct matrix U.
A.2 Subsampling the latent vector coordinates
In the previous section, we considered subsampling the data features. However, we know that supervision has
enabled double descent in PCA-type problems ( Dar et al. ,2020). Thus, we would like to study supervision
in the GAN context, as discussed in Section 3.2. In a supervised linear regression setting using the 2-norm
loss, we know that we must take a pseudoinverse of the input matrix ( Hastie et al. ,2009), which induces
double descent. In this setting, that is the latent space matrix Z. Therefore, we enable double descent by
subsampling the latent vector coordinates. Doing this is very similar to subsampling the features in the data
space. For example, if we zero out the ﬁrst coordinate of the latent distribution, we are essentially zeroing
out the subspace corresponding to the ﬁrst column of the matrix G. Since we learn G, this is a type of
adaptive pseudometric procedure, where we learn which subspaces to use and which subspaces to ignore.
B Pseudo-supervision and the curse of dimensionality
This appendix provides further detail regarding the scenario described in Section 4.1. Suppose that G∈
Rd×mis a solution which provides zero test error. Now, let z∈Rmcorrespond to the true vector which
generates x∈Rdso that Gz=x. Now suppose that zps∈Rmis any vector so that /bardblzps/bardbl2=/bardblz/bardbl2. Then,
we can ﬁnd an orthonormal matrix U∈Rm×mso that Uzps=z. We see that GU is also a solution which
gives zero train error, because the isotropic covariance matrix of the generated distribution is not changed
if we right multiply Gwith an orthonormal matrix ( Horn & Johnson ,2012). However, if we pick zpsfrom
N(0,Im)where mis large, we see that /bardblzps/bardbl2=/bardblz/bardbl2with high probability because high dimensional
Gaussians concentrate on a thin shell in high-dimensional space ( Bishop ,2006). This is typically considered
a bad thing, hence its name: the curse of dimensionality. However, here we use the curse of dimensionality
to allow fabricated latent vectors zpsto mimic supervised latent vectors z. Moreover, we can come up with
linearly independent pseudo-supervised latent vectors up to mtimes, after which we can no longer ﬁnd an
m×morthonormal matrix U. The more pseudo-supervised samples we have, the fewer matrices Gwe can
learn, resulting in faster gradient descent convergence since the feasible set is smaller.
We will encounter a problem if k < m , i.e., if the latent dimension we pick is lower than the true latent
dimension, because we cannot learn a perfect representation (assuming that the linear operator Γin the data
model is full rank). However, if we let kbe larger than m, then we can learn a solution which gives us zero
test error. Although the true vectors are m-dimensional, we can always learn a generator matrix Gwhich
ignores certain coordinates. For such solutions, we can also construct pseudo-supervised samples up to k
times. Therefore the overparameterized regime, where kis large, is very desirable from the pseudo-supervised
point of view.
19Under review as submission to TMLR
If we ﬁx npsto some value, note that by the above argument, we will incur a penalty if k < n psbecause we
will not be able to ﬁnd a suitable U. However, if kis larger than mand larger than nps, we can mimic the
behavior of supervised samples because we will be able to ﬁnd an orthonormal matrix which will transform
those pseudo-supervised latent vectors into vectors that equal the true vectors along mcoordinates. For this
reason, we consider pseudo-supervision when kis large.
C The dimension of the solution sets
C.1 The unsupervised solutions
Theorem 1. Suppose that X∈Rd×nhas full rank of min{d, n}. For the unsupervised loss L/latticetop
unsup (G,X)∆=
/bardbl(Id−GG/latticetop)X/bardbl2
F, letS/latticetop
unsup (k)∆={G∈Rd×k:L/latticetop
unsup (G,X) = 0}be the set of interpolating solutions.
Then,
1.S/latticetop
unsup (k) =∅ifn > k .
2.S/latticetop
unsup (k)is a smooth manifold of dimensionn(n−1)
2when n=k.
3.S/latticetop
unsup (k)is the union of/parenleftbign
k/parenrightbig
smooth manifolds of dimensionn(n−1)
2(k−n)(d−n)when k > n .
Proof for Theorem 1.Suppose that n > k and, for the sake of a contradiction, that S/latticetop
unsup (k)/negationslash=∅. This
means that there exists a G∈ S/latticetop
unsup (k)so that /bardbl(Id−GG/latticetop)X/bardbl2
F= 0. Thus, for each column xiofXwe
have that /bardbl(Id−GG/latticetop)xi/bardbl2
2= 0. In other words, we know that GG/latticetopxi=xiimplies that GG/latticetophas at least n
eigenvalues of 1corresponding to neigenvectors since the samples are linearly independent. By the spectral
theorem (Theorem 2.5.6 in Horn & Johnson (2012)),GG/latticetopis diagonalizable and thus rank(GG/latticetop)≥n.
However, we know that rank(GG/latticetop)≤kby simple rank inequalities (0.4.5 (a) in Horn & Johnson (2012)),
a contradiction. Therefore, we know that S/latticetop
unsup (k) =∅.
Now suppose that n=k. We take a singular value decomposition of XintoX=USV/latticetop, where U∈
Rd×d,V∈Rk×kare real orthogonal matrices and S∈Rd×kis zero except on the diagonal (Corollary 2.6.7
inHorn & Johnson (2012)). Then, we let G0=Ukbe the ﬁrst kcolumns of U. We see that
G0G/latticetop
0X=UkU/latticetop
kUSV/latticetop=Uk/bracketleftbig
Ik0k×d−k/bracketrightbig
SV/latticetop=/bracketleftbig
Uk0k×d−k/bracketrightbig
SV/latticetop=USV/latticetop=X
means that G0∈ S/latticetop
unsup (k)is one interpolating solution, since (Id−G0G/latticetop
0)X= 0. We will use G0to generate
more solutions. In fact, for each real orthogonal matrix U∈Rk×k, we see that G0U∈ S/latticetop
unsup (k)because
(G0U)(G0U)/latticetop=G0UU/latticetopG/latticetop
0=G0G/latticetop
0. We can identify solutions of the form G0Uwith the orthogonal
group O(k)which is a smooth manifold (in particular, a real Lie group) of dimensionk(k−1)
2(Lee,2003).
We deﬁne Sorth(G0) ={G∈Rd×k:G=G0U,U∈O(k)}and maintain that Sorth(G0)⊂ S/latticetop
unsup (k).
Now, we consider interpolating solutions for when n=kand show that Sorth(G0)⊃ S/latticetop
unsup (k)so
thatS/latticetop
unsup (k)is a smooth manifold of dimensionk(k−1)
2. Let G∈ S/latticetop
unsup (k)be given. For any
x∈span{x1, . . . ,xn}, it is clear that GG/latticetopx=x. Moreover, since GG/latticetophas rank n=k, we see that
anyythat is independent of our samples must be in the null space of GG/latticetopso that GG/latticetopy= 0. Thus,
GG/latticetopis an orthogonal projection matrix ( Axler ,1997). Thus, range(GG/latticetop) =span (x1, . . . ,xn)and the
eigenvalues of GG/latticetopare all either 0or1.
LetG=USV/latticetopbeG0=U0SV/latticetop
0two singular value decompositions; note that the singular value matrix
S=/bracketleftbiggIk
0d−k×k/bracketrightbigg
is the same in these two because they have the same singular values. Then, we deﬁne
20Under review as submission to TMLR
W=G/latticetop
0Gso that
G0W=G0G/latticetop
0G
=GG/latticetopG
=GVS/latticetopU/latticetopUSV/latticetop
=G. (Since S/latticetopS=In)
Note that WW/latticetop=G/latticetop
0GG/latticetopG0=G/latticetop
0G0G/latticetop
0G0=IkandW/latticetopW=G/latticetopG0G/latticetop
0G=G/latticetopGG/latticetopG=Ikimply
thatW∈O(k)is a real orthogonal matrix. Thus, any arbitrary interpolating solution Gcan be written as
G=G0UforU∈O(k). This means that Sorth(G0)⊃ S/latticetop
unsup (k)implying that Sorth(G0) =S/latticetop
unsup (k), as
desired. Thus, S/latticetop
unsup (k)is a smooth manifold with dimensionk(k−1)
2=n(n−1)
2when n=k.
Now we consider the k > n case. For any interpolating G∈ S/latticetop
unsup (k), we must have that ncolumns of G
span the column space of X. That leaves k−ncolumns of G, each in Rd, free. Suppose that the ﬁrst n
columns of Gare the ones which span the column space of Xand the next k−ncolumns are arbitrary;
we will write G=/bracketleftbigGXGa/bracketrightbig
. Thus, we have that GG/latticetop=GXG/latticetop
X+GaG/latticetop
a, meaning that we only
interpolate if range(Ga)∩span (x1, . . . ,xn) ={0}. Of course, by the arguments made above, we see that
GXUwill work instead of GXfor any U∈O(n), this means that for an appropriate selection of Ga, we
have that {G∈Rd×k:L/latticetop
unsup (G,X) = 0 ,G=/bracketleftbigGXGa/bracketrightbig
,Gaﬁxed}has dimensionn(n−1)
2. Since Gacan
be arbitrary (outside of the column span of X), it is of dimension (k−n)(d−n). To see this, take k=n+ 1.
Then, Gais a single vector which can span a d−ndimensional subspace of Rd. Ifk=n+ 2, both vectors
can span the d−n-dimensional subspace of Rdmaking them 2(d−n) = (k−n)(d−n)-dimensional. Thus,
we see that {G∈Rd×k:L/latticetop
unsup (G,X) = 0 ,G=/bracketleftbigGXGa/bracketrightbig
}must have dimensionn(n−1)
2(k−n)(d−n).
This doesn’t completely characterize S/latticetop
unsup (k)because we ﬁxed the structure of G.
Suppose that G∈ S/latticetop
unsup (k)is an interpolating solution. Then, any ncolumns of Gcan span the column
space of Xand the remaining k−ncolumns are free. For each such combination of data-spanning and
free columns, we get a smooth manifold of dimensionn(n−1)
2(k−n)(d−n). Thus, S/latticetop
unsup (k)is the union
of/parenleftbign
k/parenrightbig
smooth manifolds of dimensionn(n−1)
2(k−n)(d−n). One can check that each of these manifolds
is disjoint, and hence this union results in another smooth manifold of the same dimension. For if this is
not true, the same Gis in two combinations of data-spanning and free column conﬁgurations. Since the
data spanning vectors are non-zero and linearly independent of the free vectors, this is a contradiction. In
summary, S/latticetop
unsup (k)is a smooth manifold of dimensionn(n−1)
2(k−n)(d−n).
Theorem 3. Suppose that X∈Rd×nhas full rank of min{d, n}. For the unsupervised loss L†
unsup (G,X)∆=
/bardbl(Id−GG†)X/bardbl2
F, letS†
unsup (k)∆={G∈Rd×k:L†
unsup (G,X) = 0}be the set of interpolating solutions.
Then,
1.S†
unsup (k) =∅ifn > k .
2.S†
unsup (k)is a smooth manifold of dimension n2when n=k.
3.S†
unsup (k)is the union of/parenleftbign
k/parenrightbig
smooth manifolds of dimension n2(k−n)dwhen k > n .
Proof for Theorem 3.Suppose that n > k and, for the sake of a contradiction, that S†
unsup (k)/negationslash=∅. This
means that there exists a G∈ S†
unsup (k)so that /bardbl(Id−GG†)X/bardbl2
F= 0. Thus, for each column xiofXwe
have that /bardbl(Id−GG†)xi/bardbl2
2= 0. In other words, we know that GG†xi=xiimplies that GG†has at least n
eigenvalues of 1corresponding to neigenvectors since the samples are linearly independent. By the spectral
theorem (Theorem 2.5.6 in Horn & Johnson (2012)),GG†is diagonalizable and thus rank(GG†)≥n.
However, we know that rank(GG†)≤kby simple rank inequalities (0.4.5 (a) in Horn & Johnson (2012)), a
contradiction. Therefore, we know that S†
unsup (k) =∅.
21Under review as submission to TMLR
Now suppose that n=k. LetG0=X. Clearly, G0has full column rank and thus has an explicit pseudo-
inverse formulation. Then, we see that
G0G†
0X=G0(G/latticetop
0G0)−1G/latticetop
0X=X(X/latticetopX)−1X/latticetopX=X
means that G0∈ S†
unsup (k)is one interpolating solution, since (Id−G0G†
0)X= 0. We will use G0to
generate more solutions. In fact, for each invertible matrix A∈Rk×k, we see that G0A∈ S†
unsup (k)because
(G0A)(G0A)†=G0G†
0. We can identify solutions of the form G0Awith the real general linear group
GL(k)which is a smooth manifold (in particular, a real Lie group) of dimension n2Lee(2003). We deﬁne
SGL(G0) ={G∈Rd×k:G=G0A,A∈GL(k)}and maintain that SGL(G0)⊂ S†
unsup (k).
Now, we consider interpolating solutions for when n=kand show that SGL(G0)⊃ S†
unsup (k)so that
S†
unsup (k)is a smooth manifold of dimension n2. LetG∈ S†
unsup (k)be given. For any x∈span{x1, . . . ,xn},
it is clear that GG†x=x. Moreover, since GG†has rank n=k, we see that any ythat is independent of
our samples must be in the null space of GG†so that GG†y= 0. Thus, GG†is an orthogonal projection
matrix Axler (1997). Thus, range(GG†) =span (x1, . . . ,xn)and the eigenvalues of GG†are all either 0or
1. Note that since GG†andG0G†
0have rank nand are projection matrices onto the the column space of
X, we have that GG†=G0G†
0. Now we deﬁne A=G†
0Gso that
G0A=G0G†
0G
=GG†G
=G
Note that A∈GL(k)is invertible, otherwise Gwould be rank deﬁcient and not able to span the column
space of X. Thus, any arbitrary interpolating solution Gcan be written as G=G0AforA∈GL(k).
This means that SGL(G0)⊃ S†
unsup (k)implying that SGL(G0) =S†
unsup (k), as desired. Thus, S†
unsup (k)is a
smooth manifold with dimension n2when n=k.
Now we consider the k > n case. For any interpolating G∈ S†
unsup (k), we must have that ncolumns
ofGspan the column space of X. That leaves k−ncolumns of G, each in Rd, free to be arbitrary.
Suppose that the ﬁrst ncolumns of Gare the ones which span the column space of Xand the next k−n
columns are arbitrary; we will write G=/bracketleftbigGXGa/bracketrightbig
. In this case, Gacan be completely arbitrary and even
contain columns which span the data. To see this, let Gabe arbitrary and note that GG†G=Gfor any
pseudo-inverse (even if we don’t have full rank). Thus,
GG†G=GG†/bracketleftbigGXGa/bracketrightbig
=/bracketleftbig
GG†GXGG†Ga/bracketrightbig
=G=/bracketleftbigGXGa/bracketrightbig
implies that GG†GX=GX. Since GXhas the form GX=XAforA∈GL(k), we see that GG†XA =XA.
Finally, since Ais invertible, we have that GG†X=X.
SinceGXAwill work instead of GXfor any A∈GL(n), this means that for ﬁxed Ga, we have that
{G∈Rd×k:/bardbl(Id−GG†)X/bardbl= 0,G=/bracketleftbigGXGa/bracketrightbig
,Gaﬁxed}has dimension n2. Since Gacan be arbitrary
and is of dimension (k−n)d, we see that {G∈Rd×k:/bardbl(Id−GG†)X/bardbl= 0,G=/bracketleftbigGXGa/bracketrightbig
}must have
dimension n2(k−n)d. This doesn’t completely characterize S†
unsup (k)because we ﬁxed the structure of G.
Suppose that G∈ S†
unsup (k)is an interpolating solution. Then, any ncolumns of Gcan span the column
space of Xand the remaining k−ncolumns can be arbitrary. For each such combination of data-spanning
and arbitrary columns, we get a smooth manifold of dimension n2(k−n)d. Thus, S†
unsup (k)is the union
of/parenleftbign
k/parenrightbig
smooth manifolds of dimension n2(k−n)d. However, S†
unsup (k)need not be a manifold because
for one conﬁguration the arbitrary columns of Gcan equal the data-spanning columns of Gfor another
conﬁguration. This results in self-intersection. For a concrete example, let n= 1, k= 2. Then, for the two
conﬁgurations/bracketleftbig
GxGa/bracketrightbig
}and/bracketleftbig
GaGx/bracketrightbig
}, we can have the solution/bracketleftbig
GxGx/bracketrightbig
}. In summary, S†
unsup (k)
is the union of/parenleftbign
k/parenrightbig
smooth manifolds of dimension n2(k−n)d.
22Under review as submission to TMLR
C.2 The pseudo-supervised solutions
Theorem 2. Suppose that X∈Rd×nhas full rank of min{d, n}and let λ > 0be given. For the pseudo-
supervised loss L/latticetop
ps(G,X;λ)∆=λ
nps/bardblGZps
S−Xps/bardbl2
F+1
n/bardbl(Id−GG/latticetop)X/bardbl2
F, letS/latticetop
ps(k)∆={G∈Rd×k:
L/latticetop
ps(G,X, λ) = 0}be the set of interpolating solutions. Then,
1.S/latticetop
ps(k) =∅ifn > k andZ∈Rk×nis arbitrary.
2.S/latticetop
ps(k)has only one element if n=kandZ∈Rk×nis given so that Z/latticetopZ=X/latticetopX.
3.S/latticetop
ps(k)is the union of/parenleftbign
k/parenrightbig
smooth manifolds of dimension (k−n)(d−n)ifk > n andZ=/bracketleftbiggZ1
0/bracketrightbigg
∈
Rk×nis given so that Z/latticetop
1Z1=X/latticetopX.
Proof for Theorem 2.SinceS/latticetop
ps(k)⊂ S/latticetop
unsup (k), this implies that S/latticetop
ps(k) =∅when n > k .
Suppose that n=kand from Theorem 1we see that the solutions to L/latticetop
ps(G,X, λ)must have the form of
UkWforW∈O(k). From the condition Z/latticetopZ=X/latticetopX, we see that Z=ADV/latticetopwith a unitary matrix A
and the other matrices from the SVD of X=U/bracketleftbiggD
0/bracketrightbigg
V/latticetop; this joint decomposition can be derived from the
fact that our condition implies that the singular values and right singular vectors of ZandXare the same.
Thus, we see that
GZ−X=UkWZ−U/bracketleftbiggD
0/bracketrightbigg
V/latticetop
=UkWADV/latticetop−U/bracketleftbiggD
0/bracketrightbigg
V/latticetop(Z/latticetopZ=X/latticetopX)
=UkWADV/latticetop−UkDV/latticetop
=Uk/parenleftBig
WA−Ik/parenrightBig
DV/latticetop
=0
if and only if W=A, which is permissible since both are unitary. Hence, we have only one solution.
Now suppose that k > n . Then, we have that our solution from Theorem 1has the form of G=/bracketleftbigUnW G a/bracketrightbig
,
where W∈O(n)andGais arbitrary. Then,
GZ−X=/bracketleftbigUnW G a/bracketrightbig/bracketleftbigg
ADV/latticetop
0/bracketrightbigg
−UnDV/latticetop
=UnWADV/latticetop−UnDV/latticetop
= 0
if and only if W=A/latticetop. So we have one solution in this case, but since Gais arbitrary, we have (k−n)(d−n)
solutions. Moreover, since the columns of Gwere chosen in this convenient way, we actually have a union
of/parenleftbign
k/parenrightbig
solutions spaces of dimension (k−n)(d−n).
Theorem 4. Suppose that X∈Rd×nhas full rank of min{d, n}and let λ > 0be given. L†
ps(G,X;λ)∆=
λ
nps/bardblGZps
S−Xps/bardbl2
F+1
n/bardbl(Id−GG†)X/bardbl2
F, letS†
ps(k)∆={G∈Rd×k:L†
ps(G,X, λ) = 0 }be the set of
interpolating solutions. Then,
1.S†
ps(k) =∅ifn > k .
2.S†
ps(k)has only one element when n=k.
23Under review as submission to TMLR
S/latticetop
unsup S†
unsup S†
ps
n < k 0 0 0
n=kn(n−1)
2n21
k > nn(n−1)
2(k−n)(d−n)n2(k−n)d(k−n)d
Table 1: The size of the solution sets
3.S†
ps(k)an aﬃne space of dimension (k−n)dwhen k > n .
Proof for Theorem 4.Clearly, S†
ps⊂ S/latticetop
unsup , implying that S†
ps=∅when n > k .
When n=k, we see that the pseudo-supervised term reaches zero only when we have the unique solution of
G=XZ−1. Note that for this selction of Gwe have that
GG†X=XZ−1(Z−/latticetopX/latticetopXZ−1)−1Z−/latticetopX/latticetopX=X(X/latticetopX)−1X/latticetopX=X,
meaning that G=XZ−1is the only unique solution to the problem when n=k.
Now suppose that k > n and let Z=USV/latticetopbe decomposed via SVD, where S=/bracketleftbiggΣ
0k−n×n/bracketrightbigg
with invertible
Σ∈Rn×n. For an arbitrary solution Gwe can write it as G=/bracketleftbig
G/primeGa/bracketrightbig
U/latticetopforG/prime∈Rd×n,Ga∈Rd×k−n.
Note that since we are multiplying by U/latticetop, which is invertible, this does not change the solution set we are
interested in but merely rotates it. Hence, we have that
X=GZ =/bracketleftbigG/primeGa/bracketrightbig
U/latticetopUSV/latticetop=/bracketleftbigG/primeGa/bracketrightbig/bracketleftbiggΣ
0/bracketrightbigg
V/latticetop=G/primeΣV/latticetop.
Thus,G/prime=XVΣ−1uniquely since both ΣandV/latticetopare invertible. On the other hand, Gacan be anything.
Thus, {G∈Rd×k:/bardblGZ−X/bardbl2
F= 0}can be identiﬁed with the matrices Ga∈Rd×k−n. Moreover, note
that any solution of the form G=/bracketleftbigXVΣ−1Ga/bracketrightbig
U/latticetopmust have that GG†XVΣ−1=XVΣ−1as shown
in the proof of Theorem 3. Thus, GG†X=Xmeaning that G∈ S†
ps(k). Thus, S†
ps(k)is an aﬃne space of
dimension d(k−n).
Recall that in the proof of Theorem 3we have that the part of Gwhich contributes has the form XAfor any
invertible A. We see here that the pseudo-supervision forces Ato be equal to VΣ−1(which are completely
constructed from Z) and removes all the degrees of freedom in A. We summarize these results in the Table 1.
D Experiments on linear models and gradient details
D.1 Details regarding linear experiments
In the linear setting, we set Γ∈Rd×mto be the ﬁrst m= 10 columns of a Hadamard matrix multiplied
by1√
d, where d= 64 . Trails using random orthonormal columns for Γyielded extremely similar results,
therefore we only show plots for the Hadamard Γ. Then, we create our data by drawing n= 20 samples
fromΓz+/epsilon1, where z∼ N (0,Im)and/epsilon1=N(0,0.152Id). Our initial matrix G∈Rd×kis drawn from an
isotropic Gaussian with 0.03standard deviation. We have k∈ {1,3,5, . . . , 127}for the pseudo-supervised
experiments and k∈ {1,2, . . . , 40}for the supervised experiments. For all these experiments, we have nps
andnsuptake values in {0,2,4,12,18,20}.
We perform gradient descent with a maximum of 500 iterations. The initial step size
is0.0001 after which we adaptively pick the current iteration’s step size which will reduce
the training loss most. We do this by multiplying the current step size by values in
{0.0000001 ,0.000005 ,0.000001 ,0.00001 ,0.0001,0.001,0.01,0.1,1,10,100}and picking the value which will
24Under review as submission to TMLR
0 5 10 15 20 25 30 35 40020406080
Latent dimensionality k2-Wasserstein distanceTrain error
nsup= 2
nsup= 4
nsup= 12
nsup= 18
nsup= 20
nsup= 0
0 5 10 15 20 25 30 35 40050100150200250
Latent dimensionality kTest error
nsup= 20
nsup= 18
nsup= 12
nsup= 4
nsup= 2
nsup= 0
Figure 8: In this ﬁgure, we minimize the loss in Equation ( 3). The legends are displayed in the same order as the
curves appear on the plot for clarity. This ﬁgure is a more detailed version of Figure 2. These plots are averaged over
200 experiments, so we plot the standard deviation instead of the standard error (which would be about 14 times
smaller).
yield the lowest training loss. If the matrix Gdoes not change more than 0.00001 in Frobenius norm for
more than 5 iterations, then the optimization also stops. If the Frobenius norm of the gradient is less than
0.05, then the optimization stops. The gradients are calculated in Appendix D.2.
We run all these experiments 200times and average the results. For each experiment, we pick a new seed
and re-run the same script. Therefore, the pseudo-supervised examples are ﬁxed for each experiment as we
vary kandnps. Hence, the errorbars in Figures 8to12show one standard deviation of how the choice of
matrix initialization, pseudo-supervision samples, and data samples all aﬀect the test error.
D.2 Gradient calculations
The losses introduced in Equations ( 4) to ( 6) all have similar forms, so we only show what is the gradient
for Equation ( 4) and the other ones are easily obtained. For completeness, we restate the loss:
Ltrain(G,D) =1
nps/bardblGZps
S−Xps/bardbl2
F+1
nunsup/bardbl(Id−GG/latticetop)Xunsup/bardbl2
F.
The gradient of the ﬁrst term is
∇G1
nps/bardblGZps
S−Xps/bardbl2
F=1
nps∇G/bardbl(Zps
S)/latticetopG/latticetop−(Xps)/latticetop/bardbl2
F (Frobenius transpose invariance)
=1
nps/parenleftbigg
∇G/latticetop/bardbl(Zps
S)/latticetopG/latticetop−(Xps)/latticetop/bardbl2
F/parenrightbigg/latticetop
(Section 4.2.3 of ( Gentle ,2007))
=1
nps/parenleftbigg
2Zps
S/parenleftbig
(Zps
S)/latticetopG/latticetop−(Xps)/latticetop/parenrightbig/parenrightbigg/latticetop
=2
nps(GZps
S−Xps)(Zps
S)/latticetop.
25Under review as submission to TMLR
0 50 10005101520
Latent dimensionality kTest errornps= 20
nps= 18
0 50 10005101520
Latent dimensionality knps= 12
nps= 4
0 50 10005101520
Latent dimensionality knps= 2
nps= 0
0 50 1000200400600
Latent dimensionality k# Iterations to convergencenps= 20
nps= 18
0 50 1000200400600
Latent dimensionality knps= 12
nps= 4
0 50 1000200400600
Latent dimensionality knps= 2
nps= 0
Figure 9: In this ﬁgure, we minimize the loss in Equation ( 4). This ﬁgure is a more detailed version of the ﬁrst
column of Figure 3. We show results for six diﬀerent pseudo-supervision levels (i.e., npsvalues). For visual clarity,
each subﬁgure includes results for only two npsvalues. These plots are averaged over 200 experiments, so we plot the
standard deviation instead of the standard error (which would be about 14 times smaller).
26Under review as submission to TMLR
0 50 10005101520
Latent dimensionality kTest errornps= 20
nps= 18
0 50 10005101520
Latent dimensionality knps= 12
nps= 4
0 50 10005101520
Latent dimensionality knps= 2
nps= 0
0 50 1000200400600
Latent dimensionality k# Iterations to convergencenps= 20
nps= 18
0 50 1000200400600
Latent dimensionality knps= 12
nps= 4
0 50 1000200400600
Latent dimensionality knps= 2
nps= 0
Figure 10: In this ﬁgure, we minimize the loss in Equation ( 5). This ﬁgure is a more detailed version of the center
column of Figure 3. We show results for six diﬀerent pseudo-supervision levels (i.e., npsvalues). For visual clarity,
each subﬁgure includes results for only two npsvalues. These plots are averaged over 200 experiments, so we plot the
standard deviation instead of the standard error (which would be about 14 times smaller).
27Under review as submission to TMLR
0 50 10005101520
Latent dimensionality kTest errornps= 20
nps= 18
0 50 10005101520
Latent dimensionality knps= 12
nps= 4
0 50 10005101520
Latent dimensionality knps= 2
nps= 0
0 50 1000200400600
Latent dimensionality k# Iterations to convergencenps= 20
nps= 18
0 50 1000200400600
Latent dimensionality knps= 12
nps= 4
0 50 1000200400600
Latent dimensionality knps= 2
nps= 0
Figure 11: In this ﬁgure, we minimize the loss in Equation ( 6). This ﬁgure shows that you can achieve good
performance and double descent behavior if you weigh the pseudo-supervised and unsupervised terms in the loss
disproportionately. Note that in the nps= 0 case, we are eﬀectively reducing the step size by making αlarge.
In these experiments, we picked α= 0.98. We show results for six diﬀerent pseudo-supervision levels (i.e., nps
values). For visual clarity, each subﬁgure includes results for only two npsvalues. These plots are averaged over
200 experiments, so we plot the standard deviation instead of the standard error (which would be about 14 times
smaller).
28Under review as submission to TMLR
0 50 10005101520
Latent dimensionality kTest errornps= 20
nps= 18
0 50 10005101520
Latent dimensionality knps= 12
nps= 4
0 50 10005101520
Latent dimensionality knps= 2
nps= 0
0 50 1000200400600
Latent dimensionality k# Iterations to convergencenps= 20
nps= 18
0 50 1000200400600
Latent dimensionality knps= 12
nps= 4
0 50 1000200400600
Latent dimensionality knps= 2
nps= 0
Figure 12: In this ﬁgure, we minimize the loss in Equation ( 7). This ﬁgure is a more detailed version of the right
column of Figure 3. We show results for six diﬀerent pseudo-supervision levels (i.e., npsvalues). For visual clarity,
each subﬁgure includes results for only two npsvalues. These plots are averaged over 200 experiments, so we plot the
standard deviation instead of the standard error (which would be about 14 times smaller).
29Under review as submission to TMLR
The gradient of the second term in the considered loss function is a bit more tricky. We simplify it ﬁrst to
get
/bardbl(Ip−GG/latticetop)Xunsup
S/bardbl2
F=Tr((Xunsup
S )/latticetop(Ip−GG/latticetop)(Ip−GG/latticetop)Xunsup
S )
=Tr((Xunsup
S )/latticetop(Ip−2GG/latticetop+GG/latticetopGG/latticetop)Xunsup
S )
=/bardblXunsup
S/bardbl2
F−2Tr((Xunsup
S )/latticetopGG/latticetopXunsup
S ) +Tr((Xunsup
S )/latticetopGG/latticetopGG/latticetopXunsup
S )
which we separate into three terms:
f1(G) =/bardblXunsup
S/bardbl2
F
f2(G) =−2Tr((Xunsup
S )/latticetopGG/latticetopXunsup
S )
f3(G) =Tr((Xunsup
S )/latticetopGG/latticetopGG/latticetopXunsup
S ).
Clearly, we have that ∇G/bardbl(Ip−GG/latticetop)Xunsup
S/bardbl2
F=∇Gf1+∇Gf2+∇Gf3and that ∇Gf1= 0. By using
some matrix identities, we get that
∇Gf2=−2∇GTr((Xunsup
S )/latticetopGG/latticetopXunsup
S )
=−4Xunsup
S (Xunsup
S )/latticetopG ((119) from ( Petersen & Pedersen ,2012))
and
∇Gf3=∇GTr((Xunsup
S )/latticetopGG/latticetopGG/latticetopXunsup
S )
=/parenleftbig
∇G/latticetopTr((Xunsup
S )/latticetopGG/latticetopGG/latticetopXunsup
S )/parenrightbig/latticetop(Section 4.2.3 of ( Gentle ,2007))
=/parenleftbig
2G/latticetopGG/latticetopXunsup
S (Xunsup
S )/latticetop+ 2G/latticetopXunsup
S (Xunsup
S )/latticetopGG/latticetop/parenrightbig/latticetop((123) of ( Petersen & Pedersen ,2012))
= 2Xunsup
S (Xunsup
S )/latticetopGG/latticetopG+ 2GG/latticetopXunsup
S (Xunsup
S )/latticetopG
Hence, the gradient of the second term in the considered loss function becomes
∇G/bardbl(Ip−GG/latticetop)Xunsup
S/bardbl2
F=∇Gf1+∇Gf2+∇Gf3
=−4Xunsup
S (Xunsup
S )/latticetopG+ 2Xunsup
S (Xunsup
S )/latticetopGG/latticetopG
+ 2GG/latticetopXunsup
S (Xunsup
S )/latticetopG
Thus, the total gradient for Equation ( 4) becomes
∇G/parenleftbigg1
nps/bardblGZps
S−Xps/bardbl2
F+1
nunsup/bardbl(Id−GG/latticetop)Xunsup/bardbl2
F/parenrightbigg
=2
nps(GZsup
S−Xsup)(Zsup
S)/latticetop
−4
nunsupXunsup(Xunsup)/latticetopG
+2
nunsupXunsup(Xunsup)/latticetopGG/latticetopG
+2
nunsupGG/latticetopXunsup(Xunsup)/latticetopG.
The gradient for the loss in Equation ( 7) is similar. Again, we restate the loss for completeness:
Ltrain(G,D) =1
nps/bardblGZps
S−Xps/bardbl2
F+1
n/bardbl(Id−GG†)X/bardbl2
F
The gradient of the ﬁrst term of Equation ( 7) is the same as in the above result for the loss in Equation ( 4).
The gradient of the second term of Equation ( 7) requires more work. With B=XX/latticetopfor shorthand and
30Under review as submission to TMLR
assuming that Ghas full column rank, we see that
∇G/bardbl(Id−GG†)X/bardbl2
F=∇GTr((Id−GG†)(Id−GG†)B)
=∇GTr((Id−2GG†+GG†GG†)B)
=∇GTr((Id−2GG†+GG†)B)
=∇GTr((Id−GG†)B)
=∇GTr(B)− ∇GTr(GG†B)
=−∇GTr(G(G/latticetopG)−1G/latticetopB)
=−∇GTr((G/latticetopG)−1G/latticetopBG)
=−∇GTr((G/latticetopG)−1G/latticetopBG)
= 2G(G/latticetopG)−1G/latticetopBG(G/latticetopG)−1−2BG(G/latticetopG)−1.((126) in ( Petersen & Pedersen ,2012))
Thus, the total gradient for Equation ( 7) becomes
∇G/parenleftbigg1
nps/bardblGZps
S−Xps/bardbl2
F+1
n/bardbl(Id−GG†)X/bardbl2
F/parenrightbigg
=2
nps(GZps
S−Xps)(Zps
S)/latticetop
+2
nG(G/latticetopG)−1G/latticetopXX/latticetopG(G/latticetopG)−1
−2
nXX/latticetopG(G/latticetopG)−1.
IfGhas full row rank instead, one gets a similar gradient expression. During the minimization of the loss
in Equation ( 7), the matrix Gmay become close to low rank and make the gradient calculation unstable.
For numerical stability of the gradient, we calculate (G†)/latticetopinstead of G(G/latticetopG)†.
E Experiments on nonlinear, multilayer GANs on MNIST
In this section we provide details for the experiments on nonlinear, multilayer GANs. One of these exper-
iments is discussed in Section 5and the other is an additional experiment which is not in the main paper.
The details here are relevant to both experiments.
We train a gradient penalized Wasserstein GAN (WGAN-GP) ( Gulrajani et al. ,2017) on MNIST ( LeCun
et al. ,1998). The architecture output directly from PyTorch is shown below with the latent dimensionality
changed to k, as it varies in our experiments:
Generator(
(model): Sequential(
(0): Linear(in_features=k, out_features=128, bias=True)
(1): LeakyReLU(negative_slope=0.2, inplace)
(2): Linear(in_features=128, out_features=256, bias=True)
(3): BatchNorm1d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)
(4): LeakyReLU(negative_slope=0.2, inplace)
(5): Linear(in_features=256, out_features=512, bias=True)
(6): BatchNorm1d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)
(7): LeakyReLU(negative_slope=0.2, inplace)
(8): Linear(in_features=512, out_features=1024, bias=True)
(9): BatchNorm1d(1024, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)
(10): LeakyReLU(negative_slope=0.2, inplace)
(11): Linear(in_features=1024, out_features=784, bias=True)
(12): Tanh()
)
)
31Under review as submission to TMLR
Discriminator(
(model): Sequential(
(0): Linear(in_features=784, out_features=512, bias=True)
(1): LeakyReLU(negative_slope=0.2, inplace)
(2): Linear(in_features=512, out_features=256, bias=True)
(3): LeakyReLU(negative_slope=0.2, inplace)
(4): Linear(in_features=256, out_features=1, bias=True)
)
)
Note that we control the parameterization of our GAN by modifying just the latent dimension. However,
since these networks are multi-layer, one could investigate the behavior of a GAN by varying the widths of
all the layers at once or even one layer at a time. This is an interesting topic which merits further exploration
in future work.
The networks are trained with a gradient penalty weight of λGP= 10. The pseudo-supervised sample pairs
were ﬁxed as we varied kso that the plots were comparable. However, we ran both of these experiments
over 10trials, with 10sets of pseudo-supervised samples corresponding to 10subsets of the training data.
Let us denote LGPas the WGAN-GP objective function. We trained the discriminator as usual, and trained
the generator with the following modiﬁed objective function:
LG(Xbatch,Xps,Zps) =LGP(Xbatch) +/bardblG(Zps)−Xps/bardbl2
F
withXbatch∈Rd×nbatch size ,X∈Rd×nps, andZps∈Rk×npsfor generator G. Additionally, one could weigh
this pseudo-supervised term more or less, however we found that a weight of 1was adequate to get our
results.
For all of our experiments with nonlinear, multilayer GANs, we have a batch size of 4096, an ADAM
learning rate of 0.0002, ADAM hyperparameters β= (0.5,0.999), and clip value of 0.01. We train the
discriminator 5times per iteration. The optimizer values are used for both generator and discriminator. In
the main paper, we train for 3000 iterations and use 4096 total samples from the training data so that we
are performing gradient descent instead of stochastic gradient descent (SGD). We also do experiments for
SGD in Appendix E.1.
We measure test error with geometry score ( Khrulkov & Oseledets ,2018) as it is better suited for MNIST
than other performance measures, such as Fréchet Inception Distance ( Heusel et al. ,2017) and Inception
Score ( Salimans et al. ,2016) which are better suited for natural images. For our calculation of the geometry
score, we pick L0= 32, γ=1
1000, imax = 100 ,andn= 100 as done in the original paper when computing
scores for the MNIST dataset. Moreover, we generate 10,000 images and compare these generated images
to the MNIST test set, which also contains 10,000 images.
In Figure 13we provide errorbars for Figure 4to show that pseudo-supervision lowers variance.
E.1 Pseudo-supervision with stochastic gradient descent
In this section, we train a WGAN-GP just as above except for two changes: we train using all the training
data ( 60000 samples) for 200iterations using SGD. We only train for 200iterations because now each epoch
has about 15batches instead of the single batch in the previous section.
Our results are shown in Figure 14and Figure 15. We see that with SGD, we lose the double descent but
consistently beat the baseline. We also converge faster than the baseline, but not as fast as the pure gradient
descent setting. Moreover, we reduce the variance in the test error across experiments drastically compared
to the baseline.
32Under review as submission to TMLR
10010110210300.20.40.60.81
Latent dimensionality kTest errorEpoch = 948 — baseline
10010110210300.20.40.60.81
Latent dimensionality kTest errorEpoch = 948 — pseudo-supervised
10010110210300.20.40.60.81
Latent dimensionality kTest errorEpoch = 2,052 — baseline
10010110210300.20.40.60.81
Latent dimensionality kTest errorEpoch = 2,052 — pseudo-supervised
10010110210300.20.40.60.81
Latent dimensionality kTest errorEpoch = 3,000 — baseline
10010110210300.20.40.60.81
Latent dimensionality kTest errorEpoch = 3,000 — pseudo-supervised
Figure 13: In this ﬁgure, we train a WGAN-GP on MNIST using gradient descent on a subset of 4096 training
images. This ﬁgure is a more detailed version of Figure 4. . We plot the standard errors as errorbars.
33Under review as submission to TMLR
10010110210300.20.40.60.81
Latent dimensionality kTest errorEpoch = 63 — baseline
10010110210300.20.40.60.81
Latent dimensionality kTest errorEpoch = 63 — pseudo-supervised
10010110210300.20.40.60.81
Latent dimensionality kTest errorEpoch = 137 — baseline
10010110210300.20.40.60.81
Latent dimensionality kTest errorEpoch = 137 — pseudo-supervised
10010110210300.20.40.60.81
Latent dimensionality kTest errorEpoch = 200 — baseline
10010110210300.20.40.60.81
Latent dimensionality kTest errorEpoch = 200 — pseudo-supervised
Figure 14: In this ﬁgure, we train a WGAN-GP on the full MNIST dataset using SGD. The pseudo-supervised
GAN has much lower variance and outperforms the baseline later in training. Convergence speed is also faster for
the pseudo-supervised model. We plot the standard errors as errorbars.
34Under review as submission to TMLR
1 70025200
Latent dimensionality kEpochBaseline
0.00.20.40.60.81.0Test Error
1 70025200
Latent dimensionality kEpochPseudo-supervised
0.00.20.40.60.81.0Test Error
Figure 15: In this ﬁgure, we train a WGAN-GP on the full MNIST dataset using SGD. The pseudo-supervised
GAN converges to a low error much faster than the baseline. Just as in Figure 5, each point in the heatmap
is an average test error over 10networks The test error is measured by geometry score here. The k-axis is
plotted so that each column corresponds to the next entry for better visualization, even though the spacing is
k∈ {1,2,4,6, . . . , 70,100,200,300, . . . , 700}.
35