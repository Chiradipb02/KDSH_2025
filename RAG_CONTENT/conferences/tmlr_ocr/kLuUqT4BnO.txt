Under review as submission to TMLR
Bayesian Extreme Learning
Anonymous authors
Paper under double-blind review
Abstract
This paper presents a Bayesian extreme learning framework for analyzing high-dimensional
datasets impacted by extreme events. By employing information-theoretic measures, the
framework reﬁnes posterior distributions and incorporates a regularization term that opti-
mizes the balance between model complexity and data ﬁt. This approach reduces the risk
of overﬁtting. Contributions include deriving convergence properties and establishing uni-
versal approximation capabilities for continuous extreme value distributions. The empirical
study utilizes diverse economic and ﬁnancial sectors to examine extremities posed by the
COVID-19 pandemic. The ﬁndings highlight the framework’s predictive accuracy compared
to conventional statistical and machine learning techniques.
1 Introduction
There is growing interest in eﬀectively predicting and managing risks associated with extreme values. Tra-
ditional methods often struggle to capture the tail behaviors that characterize ﬁnancial crises and market
shocks. This paper introduces a Bayesian extreme learning (BEL) framework to improve risk assessment
and predictive accuracy in high-dimensional datasets inﬂuenced by extreme values. By integrating Bayesian
inference, extreme value theory, and information theory, the BEL framework addresses the challenges posed
by high dimensionality in extreme value modeling.
For a continuous random variable Xwith probability density function (PDF) f(x), the Shannon entropy H
is deﬁned as
H(X) =−/integraldisplay∞
−∞f(x) logf(x)dx. (1)
Shannon entropy quantiﬁes the uncertainty associated with continuous distributions (Shannon, 1948). This
entropy measure is a fundamental building block for information-theoretic measures, particularly in com-
paring probability distributions. One such measure is the Kullback-Leibler (KL) divergence, which extends
the idea of entropy to a comparative framework. The KL divergence between two continuous probability
distributions fandgis deﬁned as
K(f;g) =/integraldisplay∞
−∞f(x) log/parenleftbiggf(x)
g(x)/parenrightbigg
dx. (2)
This divergence measure quantiﬁes the diﬀerence between two continuous distributions (Kullback & Leibler,
1951). Theapplicationsoftheseconceptshavebeenextensivelystudied. SeeArdakanietal.(2018), Ardakani
et al. (2020), Ardakani (2022), Ardakani & Saenz (2022), Sooﬁ et al. (1995), and Sooﬁ (1992) for examples.
These measures become pertinent in Bayesian analysis, where the posterior distribution is updated based on
prior beliefs and new data. Given a continuous parameter θ, iff(θ)andf(D|θ)represent its prior density
and likelihood of observing data Dgivenθ, then the posterior density is given by Bayes’ theorem as
f(θ|D) =f(D|θ)f(θ)/integraltext
f(D|θ/prime)f(θ/prime)dθ/prime. (3)
The prior in Bayesian models plays a regularizing role for continuous distributions. The KL divergence
between a continuous prior and posterior provides insight into how our beliefs update after observing new
1Under review as submission to TMLR
data (Kullback, 1959). This Bayesian framework addresses the limitations of traditional learning algorithms,
as identiﬁed by Ditzler et al. (2015), which often underperform for non-stationary data. By incorporating
prior knowledge and iteratively adjusting to new observations, Bayesian methods oﬀer a robust solution for
navigating the challenges presented by extreme events (Berry et al., 2010).
Theextremevaluetheory(EVT)emphasizestailsindatasets. WhenintegratedwithBayesianmethodologies,
EVT methods are robust for extreme event analysis. The block maxima approach divides data into blocks,
selecting the maximum from each block. These maxima are modeled using the generalized extreme value
(GEV) distribution (Coles et al., 2001). The GEV cumulative distribution function (CDF) is given by
F(z) = exp/braceleftBigg
−/bracketleftbigg
1 +ξ/parenleftbiggz−µ
σ/parenrightbigg/bracketrightbigg−1/ξ/bracerightBigg
1 +ξ(z−µ)/σ> 0, (4)
whereµis the location parameter, σ > 0is the scale parameter, and ξis the shape parameter. Recently,
Ardakani (2023) integrates information theory and EVT to illustrate that the entropy of block maxima
converges to the entropy of the GEV distribution. The peak-over-threshold method, however, focuses on
data exceeding a high threshold, using the generalized Pareto (GP) distribution to model the tail. This
method is detailed in Smith (1985) and Davison & Smith (1990). The GP CDF is given by
F(y) = 1−/parenleftbigg
1 +ξy
σ/parenrightbigg−1/ξ
,1 +ξy/σ> 0, (5)
whereσ >0is the scale parameter, and ξis the shape parameter. Bayesian methods with EVT provide a
dynamic framework to analyze and understand distribution tails, especially with limited data on extreme
events. This approach is elaborated in Beirlant et al. (2006).
This paper integrates these fundamental concepts to introduce a framework for analyzing high-dimensional
datasets where extreme values are of interest. A key feature is its mechanism for ﬁltering extreme values and
incorporating a regularization term derived from entropy and KL divergence. The properties of the BEL
framework are established as follows. First, the KL divergence between consecutive posterior distributions
converges to zero with increasing observations, providing consistent posterior estimates. Second, the model
achievesnear-optimalinformationextractionfromhigh-dimensionaldatasets, particularlythosewithextreme
values, by minimizing the KL divergence between consecutive posterior distributions. Finally, a key result
is the model’s ability to approximate any continuous extreme value distribution within a given tolerance,
highlighting the model’s universal approximation.
Substantial progress has been made in high-dimensional data analysis, focusing on datasets with extreme
values. For instance, Einmahl et al. (2001) and Engelke & Hitz (2020) highlight the complexities in ex-
treme value analysis, but integrating these techniques into a high-dimensional Bayesian framework is still
an evolving area. This gap is especially evident in the existing literature regarding balancing regularization
and extracting meaningful information from extreme values in large datasets. This paper addresses these
challenges by combining EVT with Bayesian principles, a combination suggested by Chavez-Demoulin &
Davison (2005).
This study contributes to the existing literature by demonstrating consistent posterior estimates and ad-
dressing the need for reliability in posterior convergence in extreme value models. The proposed framework
comprises both a statistical model and a computational method. The statistical model involves specifying
prior distributions, likelihood functions, and the resultant posterior. The method, on the other hand, en-
compasses the computational techniques and regularization mechanisms used to ﬁlter extreme values and
optimize the posterior under high-dimensional settings.
The empirical study leverages a dataset of exchange-traded funds spanning diverse economic sectors, in-
cluding communication, energy, real estate, ﬁnancials, healthcare, industrials, technology, and utilities, from
July 2018 to September 2023. This period includes the COVID-19 pandemic, which introduced distinct
economic disruptions across these sectors. The empirical analysis employs the BEL framework to handle
the asymmetries observed in extreme returns and present its predictive accuracy compared across diﬀerent
methods, such as support vector machines, random forest, and neural networks.
2Under review as submission to TMLR
This paper is organized as follows. Section 2 outlines the foundations of Bayesian extreme learning, including
entropy considerations and regularization in the context of extreme values. This section demonstrates the
convergence properties and the robustness of the BEL framework in high-dimensional scenarios. Section 3
gives an implementation algorithm and provides an empirical study to evaluate its eﬃcacy by comparing it
against conventional statistical and machine learning methods. Section 4 provides concluding remarks.
2 Bayesian extreme learning
This section addresses the challenges posed by extreme value models in high-dimensional datasets. These
challenges include the sparsity of extremes , where traditional methods may fail due to the rare occurrence
of such values (Leadbetter et al., 2012). The curse of dimensionality presents another signiﬁcant hurdle,
as it complicates the extraction of meaningful information from large sets of data with many dimensions,
leading to issues with computational feasibility and data sparsity (Bellman, 1958). Additionally, high-
dimensional datasets are often susceptible to noise and outliers , which can skew results and make robust
analysis challenging (Huber, 1981; Rousseeuw & Leroy, 2005).
2.1 Model speciﬁcations
Deﬁnition 1 LetDbe a high-dimensional dataset consisting of nrandom variables X={x1,x2,...,xn},
where each xican potentially represent an extreme value. The Bayesian Extreme Learning (BEL) model aims
to learn the distributional parameters θthat best explain these extreme values under a Bayesian framework.
The model operates as follows:
f(θ|X) =f(X|θ)f(θ)/integraltext
f(X|θ/prime)f(θ/prime)dθ/prime, (6)
wheref(θ)represents the prior distribution of θ, andf(X|θ)is the likelihood of observing Xgivenθ. Extreme
values are isolated using a ﬁltering function F:X→Y, whereYcontains only the extreme values of X.
The entropy of Y, denotedH(Y), and the KL divergence between the distributions f(Y|θ)and a reference
distribution g(Y), denotedK(f(Y|θ);g(Y)), deﬁne a regularization term:
R(Y) =αH(Y) +βK(f(Y|θ);g(Y)), (7)
whereαandβare coeﬃcients controlling the balance between entropy maximization and distributional align-
ment tog(Y). The loss function optimized by the BEL model is given by
L(Y,θ) =E[f(Y|θ)]−λR(Y), (8)
whereλis a hyperparameter that balances the likelihood ﬁt and regularization.
A direct consequence of the Bayesian updating mechanism ensures that as more data is observed, the
divergence between consecutive posteriors diminishes, which can be stated as the following lemma. This
is consistent with the convergence properties discussed in Ghosal & Van der Vaart (2017) and Schwartz
(1965), which demonstrates the asymptotic stability of the posterior distribution as the sample size increases,
ensuring that consecutive posterior distributions become arbitrarily similar.
Lemma 1 Assume{Xn}is a sequence of independent and identically distributed (i.i.d.) observations. In
the Bayesian formulation of the BEL model, for every /epsilon1>0, there exists an integer N(/epsilon1), depending on the
distribution of Xn, the prior distribution f(θ), and the likelihood function f(X|θ), such that for all n>N (/epsilon1),
K(f(θ|Xn);f(θ|Xn+1))</epsilon1. (9)
Proof.Consider the Bayesian updating rule for the posterior distribution:
f(θ|Xn+1) =f(Xn+1|θ)f(θ|Xn)/integraltext
f(Xn+1|θ/prime)f(θ/prime|Xn)dθ/prime. (10)
3Under review as submission to TMLR
The KL divergence between f(θ|Xn)andf(θ|Xn+1)is given by
K(f(θ|Xn);f(θ|Xn+1)) =/integraldisplay
f(θ|Xn) log/parenleftbiggf(θ|Xn)
f(θ|Xn+1)/parenrightbigg
dθ. (11)
Substituting the expression from equation 10 into equation 11, we ﬁnd:
K(f(θ|Xn);f(θ|Xn+1)) =/integraldisplay
f(θ|Xn) log/parenleftbiggf(θ|Xn)/integraltext
f(Xn+1|θ/prime)f(θ/prime|Xn)dθ/prime
f(Xn+1|θ)f(θ|Xn)/parenrightbigg
dθ. (12)
By the law of large numbers and the central limit theorem, f(Xn+1|θ)converges in distribution to f(Xn+1|θ0)
asn→∞, whereθ0is the true parameter value. Hence, the ratio inside the logarithm approaches 1, which
implies that
log/parenleftbiggf(θ|Xn)/integraltext
f(Xn+1|θ/prime)f(θ/prime|Xn)dθ/prime
f(Xn+1|θ)f(θ|Xn)/parenrightbigg
→0.
Consequently, for any /epsilon1>0, there exists N(/epsilon1)such that for all n>N (/epsilon1),
K(f(θ|Xn);f(θ|Xn+1))</epsilon1.
From the properties of the extreme value ﬁlter, which concentrates on regions of the dataset with higher
unpredictability and, consequently, greater entropy (Cover & Thomas, 1991; Leadbetter et al., 2012), we
also have the following lemma.
Lemma 2 Consider a dataset X={x1,x2,...,xn}subjected to a ﬁltering function F:X→Y, which
isolates extreme values, resulting in a transformed dataset Y. In the BEL model, the diﬀerential entropy of
Yis greater than that of X, assuming Yrepresents a re-distribution focusing on the tails of X’s distribution:
H(Y)>H(X). (13)
Proof.The diﬀerential entropy Hfor a continuous random variable with PDF f(x)is given by Equation 1.
Upon applying the ﬁlter function F, the dataset Xis transformed into Y, focusing on the extreme values.
This operation modiﬁes the PDF from f(x)tog(y), whereg(y)encompasses a higher proportion of the
distribution’s tails. Assuming that extreme values are less frequent but more variable, the density g(y)typically
exhibits heavier tails compared to f(x). Heavier-tailed distributions are known to have higher entropy because
they encapsulate greater uncertainty and variability. Consequently, the entropy of Yis expected to be higher
thanH(X). This follows from the fact that increased variance and spread in a distribution, characteristic of
heavier tails, generally lead to an increase in entropy. Therefore, we can conclude that H(Y)>H(X).
This implies that the entropy of the ﬁltered dataset capturing extreme values is greater than the original
dataset. The BEL framework employs regularization to ensure proximity to a given reference distribution,
achieving optimality, aligning with the principles established by Kullback & Leibler (1951) and discussed in
Hastie et al. (2009).
Lemma 3 Given a regularization term R(X)and a reference distribution g(X), the optimal distribution
f∗(X|θ)that minimizes the KL divergence from the reference distribution g(X)is obtained by adjusting the
model parameters θ. Formally, the optimal parameters θ∗are those for which f∗(X|θ)satisﬁes:
θ∗= arg min
θK(f(X|θ);g(X)), (14)
wheref∗(X|θ)is the distribution associated with θ∗that achieves the minimum KL divergence to g(X).
Theorem 1 GivenH(X)and the properties of the BEL framework, the model achieves near-optimal in-
formation extraction from high-dimensional datasets characterized by extreme values. This optimality is
demonstrated through the minimization of the KL divergence between consecutive posterior distributions,
indicating eﬀective learning and adaptation to the data’s extremal aspects.
4Under review as submission to TMLR
Proof.From Lemma 1, the KL divergence between consecutive posterior distributions converges to zero
as the sample size increases. This convergence implies that with an increasing amount of data, the BEL
model’s posterior estimates become increasingly precise, reﬂecting a consistent reﬁnement of the model’s
parameters in capturing the underlying distributional characteristics of the data. Also, according to Lemma
2, focusing on extreme values through a transformation function leads to an increase in entropy. This
increase in entropy when isolating extreme values indicates that the model eﬀectively captures and retains
more information from the tails of the distribution, which are often the most informative yet least predictable
parts of the data. Lastly, as established in Lemma 3, the optimal parameters θ∗are those that minimize the
KL divergence between the model’s predicted distribution f(X|θ)and the reference distribution g(X). This
criterion ensures that the parameters are adjusted to best match the theoretical or expected behavior of the
dataset as described by the reference distribution. Combining these aspects demonstrates that the BEL model
adapts to increasing data and optimizes the extraction of information from high-dimensional datasets with
extreme values, achieving near-optimal performance as evidenced by the minimization of informational loss
(KL divergence) and maximization of entropy.
The approach of minimizing the KL divergence between consecutive posterior distributions as more data
is observed aligns with the key concepts in Bayesian learning, where the accumulation of data continually
reﬁnes the model’s predictions (Tipping, 2001; Bishop & Nasrabadi, 2006). This continual reﬁnement and
adaptation process is particularly crucial in the context of extreme value analysis within high-dimensional
datasets. In such scenarios, the ability of the model to adapt and learn from new, possibly extreme, ob-
servations is essential for maintaining accuracy and relevance. Tipping (2001) emphasizes the importance
of sparse Bayesian learning for eﬃcient computational performance, especially in high-dimensional spaces.
Furthermore, Bishop & Nasrabadi (2006) discuss how Bayesian methods, particularly those involving proba-
bilistic models, are adept at uncovering latent patterns in complex data. This ability is critical when dealing
with extreme values, as these values often carry signiﬁcant information about the underlying phenomena
being modeled.
Example 1 Assume the true data-generating process follows a Pareto distribution, with parameters kand
α. The PDF of a Pareto distribution is given by
f(x|k,α) =αkα
xα+1forx≥k, (15)
whereα>0is the shape parameter and k>0is the scale parameter. Assume we have a dataset Dsampled
from a Pareto distribution with k= 1andα= 2. We choose an appropriate prior for θ= (α,k). A common
choice for the Pareto parameters would be a gamma distribution for αand an exponential distribution for k.
Now, given a sample X=x1,x2,...,xnfromD, the BEL model will update its beliefs about the parameters θ
using Bayesian updating. For simplicity, let’s focus on updating beliefs about the shape parameter αusing a
gamma prior. If f(X|θ)is our likelihood function derived from the Pareto distribution and f(θ)is our prior,
the posterior after observing Xis given by the BEL model’s formulation in Equation 6.
Next, we use the function Fto extract extreme values. For Pareto-distributed data, values much larger than
the scale parameter kcan be considered extreme. So, Fﬁlters out values above a certain threshold, say
k/prime> k. This results in a dataset Ywith higher unpredictability and greater entropy than X, which is in
line with Lemma 2. Using the BEL model’s formulation for the regularization term R(X)and our chosen
reference distribution g(X)(for simplicity, we could use an exponential distribution as our reference), we
can compute the loss L(X,θ). Given this loss function, the BEL model can be trained to ﬁnd the optimal
parameter values θ∗that minimize this loss, keeping in line with Lemma 3.
Figure 1 presents histograms of the Pareto distribution and its Bayesian updating. We observe a histogram of
the Pareto-distributed data, characterized by a pronounced heavy tail. The center plot demonstrates Bayesian
updating for the shape parameter, α, using the prior G(2,1). The right plot shows the Bayesian updating
under an alternative prior, G(4,2). This comparison underscores the sensitivity of the posterior distribution
to changes in the prior. G(2,1)is a relatively uninformative prior. It gives the data a considerable inﬂuence
in determining the posterior. On the other hand, G(2,1)is more informative. This prior suggests a stronger
belief that the parameter of interest is around the value of 2 but with a slightly broader spread.
5Under review as submission to TMLR
0.00.51.0
0.0 2.5 5.0 7.510.0
xDensityPareto−distributed data
0246
1.92.02.12.22.3
αDensityPosterior of α ~ G(2,1)
0246
1.9 2.0 2.1 2.2 2.3
αDensityPosterior of α ~ G(4,2)
Figure 1: Pareto distribution and Bayesian posterior of α
Table 1 provides the summary measures for the Pareto-distributed data, extreme values, and the posterior
distributions derived under two priors. A comparison between the Posteriors with diﬀerent priors highlights
the diﬀerences that arise from adjusting the priors in Bayesian updating. This example demonstrates how
the BEL model functions when faced with data from a Pareto distribution.
Table 1: Summary statistics of Pareto-distributed data, extreme values, posteriors
Description Min 25% Median Mean 75% Max
Pareto-distributed data 1.00 1.15 1.38 1.90 1.93 29.66
Posterior of α∼G(2,1)1.88 2.05 2.10 2.10 2.14 2.33
Posterior of α∼G(4,2)1.89 2.06 2.10 2.10 2.14 2.37
Extreme values 3.01 3.54 4.36 5.69 7.17 29.66
The regularization term R(x)can be inﬂuenced by the choice of reference distribution g(x). The KL di-
vergence quantiﬁes the diﬀerence between two probability distributions. Reference distribution choices aﬀect
the BEL model. We can evaluate how diﬀerent reference distributions, namely the exponential, normal,
and uniform, would aﬀect the KL divergence and the subsequent loss. The KL divergence quantiﬁes the
diﬀerence between our empirical Pareto-distributed data and each reference distribution. Table 2 presents
the KL divergences and losses for each choice of reference. The uniform distribution exhibits the smallest
divergence, suggesting it mirrors the empirical data more closely than the other distributions. However, the
associated loss of 736.35 is only slightly lower than that of the exponential reference. In contrast, the normal
distribution, with the highest divergence of 3.77, results in the most signiﬁcant loss (763.71). These ﬁndings
underscore the role of reference distribution choice in BEL modeling.
Table 2: KL divergences and losses for diﬀerent reference distributions
ReferenceK(f;g)L(X,θ)
Exponential 1.19 737.67
Normal 3.77 763.71
Uniform 0.00 736.35
This framework employs a Bayesian updating mechanism and ensures the convergence of posterior distri-
butions. This method draws on established principles from the literature (Ghosal & Van der Vaart, 2017;
Schwartz, 1965; Tipping, 2001), adapting these techniques to meet the challenges posed by extreme val-
ues. Moreover, the integration of information-theoretic measures into the regularization term highlights its
capability to handle the sparsity and unpredictability of extreme events.
6Under review as submission to TMLR
2.2 Reference distributions and model regularization
The concept of admissible reference distributions is required in formulating the BEL model, particularly
in determining the model’s eﬃcacy and robustness. Reference distributions play a signiﬁcant role in the
regularization process. In recent literature, the choice and characteristics of reference distributions have
been scrutinized for their impact on model performance. For instance, Muller & Quintana (2004) emphasize
the signiﬁcance of using distributions that reﬂect the underlying data-generating process, ensuring that
statistical models remain sensitive to data patterns while avoiding overﬁtting. Similarly, Walker (2010)
highlights how reference distributions could be tailored to enhance the accuracy of Bayesian models in
high-dimensional spaces. These discussions underscore the importance of selecting reference distributions to
guarantee that the model remains aligned with empirical data characteristics. In this context, this section
adopts a deﬁnition of admissibility for reference distributions, encapsulating the characteristics required for
eﬀective model performance.
A reference distribution g(X)for a random variable Xis deﬁned as admissible if it satisﬁes the following
conditions to ensure its appropriateness and utility in statistical inference:
(a) The distribution g(X)assigns a positive probability to any event that has a non-zero probability
under the true data-generating distribution. Formally,
∀E⊆X,Ptrue(E)>0 =⇒g(E)>0, (16)
whereXisthesupportoftherandomvariable X, andPtruerepresentstheprobabilitymeasureunder
the true data-generating process, capturing the actual distribution from which data are sampled.
(b) The distribution g(X)is absolutely continuous with respect to the Lebesgue measure, meaning that
it has a density function h(x)that describes the probability in terms of an integral over X:
g(A) =/integraldisplay
Ah(x)dµ(x),∀A⊆X. (17)
(c) All moments of the distribution g(X)are ﬁnite, which is essential for the tractability and stability
of estimators derived from g(X). Speciﬁcally, for any positive integer k,
E[|X|k] =/integraldisplay
X|x|kh(x)dµ(x)<∞. (18)
Given admissible reference distribution, the loss function L(X,θ)in BEL possesses the following properties:
1. For any ﬁxed θ∈Θand for any admissible reference distribution g(X), the loss function Lis
bounded. Speciﬁcally,
∃Lmin,Lmax∈R,∀θ∈Θ,∀g(X)(admissible) , Lmin≤L(X,θ|g(X))≤Lmax.
2. The loss function L(X,θ)is a mapping from the Cartesian product of the space of datasets and the
parameter space Θto the real numbers, i.e., L:X× Θ→R. Furthermore,Lis continuous with
respect to its arguments. Formally, for any sequence (Xn,θn)→(X,θ)inX×Θ,
lim
n→∞L(Xn,θn) =L(X,θ).
Proposition 1 Consider the loss function L: Θ×G→ R, where Θrepresents the parameter space for a
statistical model, and Gis the set of all admissible reference distributions for a random variable X. There
exists an optimal reference distribution g∗(X)∈Gsuch that
L(X,θ|g∗(X))≤L(X,θ|g(X))∀g(X)∈G. (19)
This implies that g∗(X)minimizes the loss function over all admissible distributions in G.
7Under review as submission to TMLR
Proof.We deﬁneL(X,θ|g(X))as a real-valued function that is ﬁnite across all g(X)∈ Gdue to the
admissibility and properties of distributions in G. We then construct a sequence {gn(X)}⊂Gsuch that
lim
n→∞L(X,θ|gn(X)) = inf
g∈GL(X,θ|g(X)),
where each gn(X)is chosen to progressively reduce the loss function value. Utilizing the compactness prop-
erty ofGunder the topology induced by the weak convergence of measures ensures that every sequence of
distributions inGhas a convergent subsequence. This compactness can be argued based on the constraints
(like boundedness of moments) placed on distributions in G. AssumeLis continuous with respect to this
topology. Hence, a convergent subsequence {gnk(X)}of{gn(X)}converges to some limit g∗(X)∈G. By
continuity,
L(X,θ|g∗(X)) = lim
k→∞L(X,θ|gnk(X)).
This sequence of steps establishes that g∗(X), as the limit of a minimizing sequence, indeed satisﬁes
L(X,θ|g∗(X))≤L(X,θ|g(X))∀g(X)∈G.
The derivation of the optimal reference distribution is aligned with the principles of Bayesian statistics and
extremevaluetheory. Forareview, refertoBernardo&Smith(2009)andGelmanetal.(1995). Additionally,
theconceptofextremevalueanalysisdrawsfromwell-establishedtheoriesaselaboratedinColesetal.(2001).
Proposition 1 is formulated to enhance the BEL model’s applicability in practical scenarios.
Proposition 2 LetGdenote the set of all admissible reference distributions for a random variable X.
Assumeg∗(X)∈Gminimizes the loss function L. Then, for a ﬁnite number m∈N, there exists a set of
distributions{gi}m
i=1and associated mixing weights {wi}m
i=1, wherewi≥0for alliand/summationtextm
i=1wi= 1, such
that
g∗(X)≈m/summationdisplay
i=1wigi(X), (20)
where each gi(X)is a commonly known distribution (e.g., exponential, normal).
Proof.DeﬁneMas the set of all possible ﬁnite mixtures of the form
m(X) =k/summationdisplay
i=1wigi(X),
wherek∈N,wi≥0,/summationtextk
i=1wi= 1, and eachgi(X)is a member of a predeﬁned set of common distributions
that are elements of G. The setM, being a set of convex combinations of a ﬁnite number of ﬁxed distributions
withinG, is compact in the topology of weak convergence of probability measures. This compactness arises
from the Tychonoﬀ theorem for products of compact spaces and the compactness of the simplex deﬁned by
the weights. Assume Lis continuous with respect to the weak convergence topology on probability measures.
The Extreme Value Theorem for continuous functions over compact sets ensures that Lattains its minimum
overM. Therefore, there exists a mixture distribution m∗(X)∈Msuch that
L(X,θ|m∗(X))≤L(X,θ|m(X))∀m(X)∈M.
Given the minimization properties of g∗(X)inGand the proximity of Mto any distribution in Gthrough
ﬁnite mixtures, m∗(X)serves as an approximation to g∗(X). Speciﬁcally, the distance (in terms of L)
betweenm∗(X)andg∗(X)can be made arbitrarily small by appropriately choosing the components and
weights of the mixture, leveraging the density of the ﬁnite mixtures in the space of probability measures
under mild conditions. Thus, m∗(X)approximates g∗(X)closely, suggesting that ﬁnite mixtures of common
distributions can represent the optimal distribution in applications where g∗(X)is computationally infeasible.
Proposition 1 establishes the existence of an optimal reference distribution within the set of all admissible
distributions, which minimizes the loss function in the BEL model. Proposition 2, Builds on the prior result
to demonstrate that the identiﬁed optimal reference distribution can be closely approximated using a ﬁnite
mixture of common probability distributions, facilitating practical implementation and analysis.
8Under review as submission to TMLR
Example 2 Consider a BEL model operating on a synthetically generated dataset, D, a mixture of an
exponential and a normal distribution. The BEL model processes this high-dimensional dataset, detailed in
Deﬁnition 1. Here, a normal prior, speciﬁcally f(θ)∼N(0,1), is assumed for the parameters. The likelihood,
f(X|θ), is constructed based on the combined exponential and normal distributions intrinsic to the dataset.
Using the Bayesian formulation, the posterior distribution is calculated. An important step is extracting
extreme values, for which a ﬁlter function, F:X→Y, retains values exceeding the 95th percentile threshold.
Following iterative evaluations and adjustments within the BEL model, the optimal reference distribution,
g∗(X), resembles a mixture of exponential and normal distributions. The mixing weights, w1andw2, result
in the approximation g(X)≈.65E(.5) +.35N(5,1)for the datasetD.
Figure 2 presents data and extremes histogram along with the approximated optimal reference distribution.
The histogram shows the distribution of the raw data (in blue) and the extracted extreme values (in yellow).
A surge in the right tail exempliﬁes the extreme values. The optimal reference distribution illustrates the
density of the approximated optimal reference distribution, which is a mixture of the exponential and normal.
0.000.020.040.060.08
0 5 10
XDensityData Extremes
0.000.050.100.150.20
0 5 10
XDensityReference
Figure 2: Data, extreme values, and approximated optimal reference distribution
The eﬀectiveness of the BEL model in managing high-dimensional data sets it apart from conventional
Bayesian models, which often struggle with the “curse of dimensionality.” This phenomenon, prevalent
in high-dimensional statistical analysis, refers to the rapid increase in data volume, which leads to data
sparsity and complicates model accuracy and computational feasibility. Belloni & Hansen (2014) discuss
strategies for overcoming the curse of dimensionality in high-dimensional regression models. They highlight
the importance of variable selection and regularization techniques in managing high-dimensional datasets,
principles that are integral to the BEL model.
Furthermore, Fan & Lv (2010) explore variable selection methods for ultra-high-dimensional datasets, em-
phasizing the need for models to maintain computational eﬃciency while ensuring accuracy. Their insights
align with the BEL model’s approach, which leverages a ﬁltering mechanism and optimal reference distri-
butions to tackle high-dimensional challenges. Sang & Gelfand (2009) examine Bayesian approaches for
high-dimensional extremes. As demonstrated in the example, the BEL model’s ability to identify and iso-
late extreme values within a high-dimensional space mirrors the methodologies discussed by Heﬀernan &
Southworth (2013), illustrating its applicability in practical scenarios involving extreme events.
Proposition 3 Consider the BEL model operating on a dataset with dimensionality dand sample size n.
Deﬁne the rate of convergence of the posterior distribution in terms of the KL divergence as
ρ(d,n) =K(f(θ|Xn
1);f(θ)), (21)
wheref(θ)denotes the prior distribution. For suﬃciently large nrelative tod, the rate of convergence ρ(d,n)
can be bounded by constants 0<C 1<C 2such that:
C1dlog(n)≤ρ(d,n)≤C2dlog(n). (22)
Proof.Consider the sequence of posterior distributions {f(θ|Xn
1)}asnincreases. By the properties of the
BEL model, we assume that the posterior distributions concentrate around the true parameter value θ0asn
increases. From the consistency of KL divergence between consecutive posteriors (Lemma 1), it follows that:
lim
n→∞K/parenleftbig
f(θ|Xn
1);f(θ|Xn+1
1)/parenrightbig
= 0,
9Under review as submission to TMLR
indicating improved estimation of the posterior with larger sample sizes. Given the use of a regularization
term that potentially stabilizes variance and controls model complexity in high dimensions, the KL divergence
from the prior to the posterior decreases at a rate proportional to dlog(n). This rate reﬂects both the
increase in dimensionality and the logarithmic improvement due to the accumulation of more data. Using
concentration inequalities and properties of the KL divergence, we assert that for a large nand highd,
C1dlog(n)≤K(f(θ|Xn
1);f(θ))≤C2dlog(n),
whereC1andC2are constants derived from the regularization parameters and the dispersion measures of
the prior and likelihood functions.
The bounded divergence between the actual and estimated posteriors, irrespective of the increased dimen-
sionality, assures robust convergence in increased dimensionality to avoid the curse of dimensionality.
Proposition 4 Consider a dataset Dsampled from a distribution Pwith distinct modes {m1,m2,...,mk}
corresponding to diﬀerent extreme events. In the BEL framework, augmented with a Dirichlet Process (DP)
prior, the posterior modes {l1,l2,...,lk}converge to the true modes as the sample size increases. For any
/epsilon1>0, there exists a sample size N(/epsilon1)such that for all n>N (/epsilon1),
sup
1≤i≤k|li−mi|</epsilon1. (23)
This states that the estimated modes from Bayesian extreme learning approximate the actual distribution’s
modes within an error margin /epsilon1, with high probability as sample size increases.
Proof.The BEL framework employs a DP with base distribution G0and concentration parameter α, enabling
a ﬂexible, nonparametric prior that adjusts based on observed data:
H∼DP(G0,α).
Upon incorporating the dataset D, the posterior distribution of potential generating distributions evolves to
H|D∼DP(Gn,α+n),
whereGnrepresents a combination of the empirical distribution of Dand the prior distribution G0. Asn
increases, the inﬂuence of G0diminishes, allowing the empirical data to predominantly shape the posterior.
The properties of the Dirichlet Process, particularly its support for clustering around the sample data’s
modes, ensure that the posterior distribution’s modes {l1,l2,...,lk}increasingly align with the true modes
{m1,m2,...,mk}ofP. This alignment is substantiated by
P/parenleftbigg
sup
1≤i≤k|li−mi|>/epsilon1/parenrightbigg
→0asn→∞.
Hence, with a suﬃciently large sample size, represented by N(/epsilon1), the probability that the estimated modes
deviate from the true modes by more than /epsilon1becomes negligible.
Following Muller & Quintana (2004) and Walker (2010), this section has introduced admissible reference
distributions to enhance the robustness and accuracy of the BEL framework, focusing on how these distri-
butions support eﬀective estimations. By deﬁning conditions that ensure the reference distributions align
with the underlying data-generating processes, such as maintaining continuity and bounding moments, the
approach mitigates overﬁtting.
2.3 High-dimensionality and sparsity
We can integrate methodologies from sparse Bayesian learning to address the sparsity in the extremes of
datasetD. This approach utilizes sparsity-inducing priors, such as the Laplace prior, a concept extended
into the Bayesian framework by Park & Casella (2008) in their discussion of the Bayesian lasso. Such priors
10Under review as submission to TMLR
enable the model to maintain its interpretability and eﬀectiveness even with sparse data. Consider the
sparsity-inducing prior fs(θ), implemented to inﬂuence the parameters. This technique aligns with Tipping
(2001) introduction of the relevance vector machine in sparse Bayesian learning. With this, the model’s
sparse posterior distribution can be represented as
fs(θ|D)∝fs(θ)/productdisplay
x∈DP(x|θ). (24)
The posterior fs(θ)penalizes regions of the parameter space corresponding to non-extreme values in D. Con-
sequently, the model’s primary focus becomes the extreme values in the set ε, leading to a linear complexity
in terms of|ε|. Carvalho et al. (2010) and Bhattacharya et al. (2015) and have further developed such
sparsity-inducing techniques, with the horseshoe estimator and Dirichlet-Laplace priors, respectively, en-
hancing the capability of Bayesian models in high-dimensional settings. To determine the disparity between
the true posterior distribution over the extreme values and the BEL model’s approximated distribution, we
employ the L2norm. IfQ(ε)is the BEL model’s approximation, a constant /epsilon1can bound the diﬀerence. This
result leads to the following proposition.
Proposition 5 Consider a dataset Dsampled from a distribution Pwith dimensionality dand a subset
εrepresenting extreme values where |ε|/lessmuchd. Utilizing sparse Bayesian techniques, the BEL model can
approximate the posterior distribution over εwithin a tolerance /epsilon1, while the computational complexity remains
linear with respect to |ε|, rather than d. Formally,
/bardblP(ε)−Q(ε)/bardbl2</epsilon1, (25)
whereQ(ε)is the approximate posterior distribution and /epsilon1>0is a predetermined error margin.
Proof.Assume the implementation of a sparse Bayesian framework within the BEL model, focusing exclu-
sively on the subset εof the dataD. LetP(ε)denote the true posterior distribution conﬁned to ε, andQ(ε)
represent the sparse Bayesian approximation of this distribution. The goal is to minimize the Euclidean
norm (L2norm) of the error between P(ε)andQ(ε), deﬁned as
/bardblP(ε)−Q(ε)/bardbl2=/parenleftbigg/integraldisplay
ε(P(x)−Q(x))2dx/parenrightbigg1/2
.
Select a set of basis functions Φ ={φ1,φ2,...,φ|ε|}to capture the signiﬁcant characteristics of the distribu-
tion within ε. The approximation Q(ε)is then expressed as a linear combination of these basis functions
Q(ε) =|ε|/summationdisplay
i=1aiφi(x),
whereaiare the coeﬃcients determined through sparse Bayesian optimization techniques. Under the as-
sumption of well-chosen basis functions Φand adequate sample size, it is feasible to achieve
/bardblP(ε)−Q(ε)/bardbl2</epsilon1.
Moreover, the computational complexity of determining Q(ε)directly depends on the number of basis functions
|ε|, rather than the full dimensionality d, making the computation:
Complexity (ε) =O(|ε|).
Example 3 Suppose we are studying temperature anomalies in a region that exhibits multimodal extreme
behaviors. These modes might be caused by, for instance, heat waves, cold snaps, and storms. Let’s assume
the underlying data follows a mixture of three Gaussian distributions:
P(x) =w1N(x;µ1,σ2
1) +w2N(x;µ2,σ2
2) +w3N(x;µ3,σ2
3), (26)
wherewiis the weight of the i-th Gaussian component, and µiandσ2
iare its mean and variance. Here, µ1,
µ2, andµ3might represent temperature anomalies due to a heatwave, cold snap, and storm, respectively.
11Under review as submission to TMLR
Using the BEL model integrated with a DP, the model identiﬁes the underlying multimodal structure. As
per Proposition 4, as more data becomes available, the BEL model’s inferred modes, l1,l2,...,lk, converge
to the true modes m1,m2,...,mk. Given the sparsity of the extremes, we further equip the BEL model with
sparsity-inducing priors. Even though events like severe heatwaves and cold snaps are rare, they are learned
without necessitating computational complexity based on the entire dataset’s size. As per Proposition 5, the
model’s eﬃciency is optimized for the sparse extremes. Figure 3 presents the density of the entire dataset.
Vertical lines on this density plot indicate the true modes (in blue) and the BEL inferred modes (in red). The
demarcation between the true and inferred modes validates the model’s eﬃcacy in capturing the underlying
structure. The right plot focuses on the extremes of the dataset. Extreme values lie in the top 5%.
0.000.050.10
−4 0 4
ValueDensityBlue: True modes, Red: BEL inferred modesTrue and inferred modes
0.00.30.60.9
4.0 4.5 5.0 5.5 6.0
ValueDensityValues >  4.09Sparse extremes
Figure 3: Density plots of true and BEL-inferred modes, along with extreme values
As emphasized by Park & Casella (2008) and Tipping (2001), the integration of sparsity-inducing priors
enhances the model’s performance in high-dimensional spaces where data sparsity prevails. The ﬁndings
underscore the model’s capability to approximate the true distribution of these extreme values within a
predeﬁned error margin.
2.4 Robustness against outliers
Datasets can frequently contain outliers that can distort posterior inference. We can establish an upper
bound on the total variation distance between posteriors derived from datasets with and without outliers.
This bound, represented by a function of the fraction of outliers in the dataset, oﬀers a measure of the
sensitivity of the Bayesian model to outliers.
Proposition 6 LetDoandDnobe datasets with and without outliers, respectively, and let π(θ|X)denote
the posterior distribution derived using the BEL model for dataset X. Assume that a fraction φ∈[0,1]of
Doconsists of outliers and /epsilon1is an upper bound on the contribution of an individual outlier to the likelihood.
Then, the total variation distance between the posterior distributions π(θ|Do)andπ(θ|Dno)is bounded by
g(φ) =1
2φ·/epsilon1, that is,
∆(π(θ|Do),π(θ|Dno))≤g(φ), (27)
where ∆(P,Q)is the total variation distance between two distributions PandQdeﬁned as
∆(P,Q) =1
2/integraldisplay
|π(θ|X)−π(θ|Y)|dθ. (28)
Proof.Consider the likelihood functions L(θ;Do)andL(θ;Dno)for the datasets with and without outliers.
Given that the fraction φof the datasetDocontains outliers, each inﬂuencing the likelihood by at most /epsilon1,
the maximum impact on the combined likelihood can be modeled as
∆L=|L(θ;Do)−L(θ;Dno)|≤φ·/epsilon1.
Considering a uniform prior p(θ)for simplicity, the disparity in posterior distributions inﬂuenced by this
diﬀerence in likelihood translates to
∆π=|π(θ|Do)−π(θ|Dno)|≤φ·/epsilon1.
12Under review as submission to TMLR
By integrating ∆πover the parameter space Θand applying the total variation deﬁnition, we obtain
∆(π(θ|Do),π(θ|Dno)) =1
2/integraldisplay
|π(θ|Do)−π(θ|Dno)|dθ≤1
2φ·/epsilon1.
This result establishes an upper bound on the total variation distance between posterior distributions derived
from datasets that either include or exclude outliers. This bound is expressed as a function of the proportion
of outliers within the dataset, providing a measure of the model’s sensitivity to these anomalous data points.
The degree to which outliers can skew the posterior distribution is illustrated.
2.5 Universal approximation with BEL
The following proposition, inspired by the principles of universal approximation in neural networks Hornik
et al. (1989), demonstrates the model’s capability to approximate continuous extreme value distributions
within any given tolerance level. This is particularly signiﬁcant in EVT.
Proposition 7 LetFbe a continuous extreme value distribution deﬁned on a compact interval S⊆R. For
any/epsilon1 >0, there exists a set of prior distributions Pand a suﬃciently large dataset D⊂Ssuch that the
BEL model’s posterior mean, µ(D|P), satisﬁes
sup
x∈S|F(x)−µ(D|P)|</epsilon1. (29)
This statement ensures that the BEL model can universally approximate any continuous extreme value dis-
tribution over Swithin any desired precision /epsilon1.
Proof.Given a continuous extreme value distribution Fdeﬁned on the compact interval S, and for any
desired precision /epsilon1 >0, by the uniform continuity of FonS, there exists δ >0such that for all x,y∈S
with|x−y|<δ:
|F(x)−F(y)|</epsilon1
2.
PartitionSinto a ﬁnite number of subintervals {Si}, each with a diameter less than δ. This partition ensures
that within any subinterval Si, the ﬂuctuation of Fis bounded by /epsilon1/2. For each subinterval Si, choose a
representative point xiand select a prior PifromPsuch that the BEL model, when applied to a dataset Di
includingxi, produces a posterior mean µ(Di|Pi)approximating F(xi)within/epsilon1/2:
|F(xi)−µ(Di|Pi)|</epsilon1
2.
By the triangle inequality and the uniform continuity of F, for anyx∈Si, it follows that
|F(x)−µ(Di|Pi)|≤|F(x)−F(xi)|+|F(xi)−µ(Di|Pi)|</epsilon1.
Hence, for each x∈S, there is a corresponding subinterval Sicontainingxwhere the BEL model’s posterior
mean approximates F(x)within an/epsilon1margin.
Example 4 Consider the scenario of studying peak river ﬂow rates indicative of century-long ﬂood events
using the BEL model. We apply the BEL model to see how well it can approximate the true underlying extreme
value distribution, represented here by the Gumbel distribution. First, 100 years of data are simulated under
the Gumbel distribution, characterized by extreme events, assuming that the true extreme value distribution
of peak river ﬂow rates follows the Gumbel distribution. We can ﬁt the BEL model to the generated data,
using subsets of increasing size to observe how quickly the BEL model’s estimates converge towards the true
parameters as more data gets included.
The Gumbel distribution is an extreme value distribution for modeling the maximum or minimum of a number
of samples of various distributions. The CDF of the Gumbel distribution is expressed as
F(x;µ,β) =e−e−(x−µ
β)(30)
13Under review as submission to TMLR
whereµis the location parameter, and βis the scale parameter. The parameters µandβare real numbers,
andβ > 0. This example assumes that the yearly maximum ﬂow rates follow a Gumbel distribution with
µ= 50andβ= 10. These parameters could be estimated based on historical ﬂood data or through methods
like maximum likelihood estimation or Bayesian inference. Given these parameters, the CDF of our Gumbel
distribution models the probability that the peak river ﬂow rate is below a certain value. This modeling is
crucial for infrastructure planning and risk assessment in ﬂood-prone regions.
Figure 4 presents the true distribution and BEL’s approximations to visualize how the model performs as
more data is considered. The histogram shows the simulated data’s density, indicating infrequent peak river
ﬂow rates. The BEL model demonstrates convergence to the true extreme value distribution with increasing
data. The approximations of the mean parameter get progressively closer to the actual value. The dashed
line marks the actual parameter as a constant benchmark.
0.000.020.040.060.08
40 60 80
Flow rateDensitySimulated river flow rates
50.052.555.057.560.062.5
25 50 75 100
Sample sizeEstimated meanConvergence of BEL model estimates
Figure 4: BEL model estimations and true Gumbel distribution
As highlighted by the principles of universal approximation within neural networks (Hornik et al., 1989),
this result extends the BEL to universally approximate continuous extreme value distributions. This is
important in EVT, where accurately modeling tail behaviors inﬂuences risk assessment and prediction. For
any given tolerance, the BEL model can approximate any continuous extreme value distribution deﬁned over
a compact set. This is achieved through the selection of suitable priors and a suﬃciently large dataset.
3 Empirical study
This section outlines the implementation of Bayesian extreme learning and provides an empirical study
of extreme returns in diverse economic sectors during a period of external shocks. The methodology is
then compared against conventional statistical and machine learning techniques, including support vector
machines, random forest, and neural networks, to evaluate the BEL model’s predictive accuracy in modeling
extremities.
3.1 Implementation
Algorithm 1 outlines implementation by initializing parameters θbased on prior information and calculating
an initial regularization term R, given user-deﬁned priors f, a reference distribution g, and regularization
coeﬃcients αandβ. Iteratively, the BEL model reﬁnes its predictions by ﬁltering out extreme values Yand
consistently updating the posterior distribution using Bayesian rules. The main feature is the integration of
a regularization term R(Y)in the loss function L(Y,θ), constructed to balance between the expectation of
the model and a penalty term mitigating overﬁtting. The algorithm iterates until a predeﬁned convergence
criterion, such as a threshold for successive losses, is satisﬁed.
14Under review as submission to TMLR
Algorithm 1 Bayesian extreme learning implementation
1:procedure BEL(D,f,g,α,β,λ ) ⊿DatasetD, priorsf, reference distribution g
2:θ←initialize parameters based on prior information
3:X←extract data from D
4:Y←F(X) ⊿Filter extreme values
5: Compute initial regularization term R(Y)usingα,β,g (Y)
6:Lprev←∞ ⊿Initialize previous loss to a very high value
7: criteria←deﬁne convergence criteria based on BEL speciﬁcs
8: whilecriteria not met do ⊿Criteria include threshold for posterior convergence
9: Update f(θ|Y)using Bayesian updating rules
10: Compute H(Y)andK(f(Y|θ);g(Y))
11:R(Y)←αH(Y) +βK(f(Y|θ);g(Y)) ⊿Update regularization term
12:L(Y,θ)←E[f(Y|θ)]−λR(Y) ⊿Compute loss
13: if|L(Y,θ)−Lprev|<some threshold then
14: criteria ←met
15: end if
16:Lprev←L(Y,θ)
17:θ←update parameters based on L(Y,θ)
18: end while
19: returnθ,f(θ|Y),R(Y),L(Y,θ) ⊿Updated parameters, posterior, regularization, loss
20:end procedure
3.2 Extremities across diverse economic sectors
The empirical analysis leverages data from principal exchange-traded funds (ETFs) across various sectors,
including communication, energy, real estate, ﬁnancials, healthcare, industrials, technology, and utilities.
The dataset spans from July 3, 2018, to September 29, 2023, encompassing signiﬁcant market events such
as the COVID-19 pandemic, which introduced extreme volatility and distinctive economic challenges across
these sectors. Figure 5 presents the PDF and CDF of ETF returns. The plots facilitate a comparison
of return distributions for each sector. For instance, sectors with wider and ﬂatter PDFs indicate higher
volatility in returns, whereas steeper slopes in the CDF plots suggest quicker accumulation of returns.
010203040
−0.2 −0.1 0.0 0.1
ReturnsPDFSector
Communication
Energy
Financials
Healthcare
Industrials
Real Estate
Technology
Utilities0.000.250.500.751.00
−0.2 −0.1 0.0 0.1
ReturnsCDFSector
Communication
Energy
Financials
Healthcare
Industrials
Real Estate
Technology
Utilities
Figure 5: PDF and CDF plots of ETF returns across various sectors
Table 3 provides summary returns statistics for diﬀerent ETFs by sector. The Technology sector exhibits
a maximum return of 0.111 and a minimum of -0.149 with a standard deviation (SD) of 0.017, indicating
moderate volatility. By comparing standard deviations, we see a variation in volatility across sectors. Sectors
like Energy, with higher volatility, suggest a greater risk but also the potential for higher returns. Conversely,
sectors like Healthcare exhibit lower volatility, indicating more stable but potentially lower returns. Skewness
values across sectors are negative, suggesting returns are skewed to the left; mainly, the Real Estate sector
showsthehighestleft-skewness(-1.158). Kurtosisvaluesaresigniﬁcantlyhigherthan3(whichwouldindicate
a normal distribution) for all sectors, suggesting a propensity for extreme returns or “fat tails.” For instance,
the Financials sector displays a high kurtosis of 13.524, indicating a higher likelihood of extreme return values
than a normal distribution. The percentile rows capture the spread of the returns, where, for instance, the
10th percentile of Energy sector returns is -0.024, meaning that 10% of the returns fall below this value.
15Under review as submission to TMLR
Table 3: Summary measures of ETF returns
Min Max SD Skewness Kurtosis 10% 90% 99%
Communication -0.120 0.086 0.016 -0.514 6.106 -0.017 0.017 0.041
Energy -0.225 0.149 0.023 -0.895 13.252 -0.024 0.025 0.059
Financials -0.147 0.124 0.017 -0.552 13.524 -0.017 0.016 0.044
Healthcare -0.104 0.074 0.012 -0.383 10.399 -0.011 0.012 0.030
Industrials -0.120 0.119 0.015 -0.581 11.944 -0.015 0.015 0.038
Real Estate -0.174 0.084 0.015 -1.158 17.546 -0.015 0.015 0.038
Technology -0.149 0.111 0.017 -0.414 7.971 -0.018 0.019 0.042
Utilities -0.121 0.120 0.014 -0.204 15.647 -0.014 0.013 0.032
Figure 6 presents the PDF and CDF plots of extreme returns. The PDF plots on the left reveal the likelihood
of extreme return levels. For instance, sectors with heavier tails imply higher uncertainty and potential for
volatile extremes. The CDF plot on the right complements this by illustrating the probability accumulation
of returns, with steeper sections indicating a denser clustering of extreme events at certain return levels. The
bimodal distributions of ETF extreme returns highlight the nature of extreme events across various sectors.
This bimodality is informative for the implementation of the BEL framework. Bimodal distributions often
suggest the presence of multiple underlying processes or regimes. Traditional statistical models may not
capture such complexities, especially when dealing with extreme values. The BEL framework’s strength lies
in incorporating priors, including multiple modes.
0246
−0.2 −0.1 0.0 0.1
ExtremesPDFSector
Communication
Energy
Financials
Healthcare
Industrials
Real Estate
Technology
Utilities0.000.250.500.751.00
−0.2 −0.1 0.0 0.1
ExtremesCDFSector
Communication
Energy
Financials
Healthcare
Industrials
Real Estate
Technology
Utilities
Figure 6: PDF and CDF plots of extreme ETF returns across various sectors
Table 4 presents the results of applying the BEL model to various sectors. The θparameter indicates each
sector’s central characteristics or tendencies. The θvalues are uniform across sectors, which suggests that the
central tendencies of these sectors are closely aligned. The entropy H(X)varies slightly across sectors. For
instance, Healthcare exhibits a relatively higher entropy of 2.484, denoting a greater level of unpredictability
and complexity in its data patterns. In contrast, sectors like energy, with an entropy of 1.962, display lower
uncertainty, indicatingmorepredictabledatabehaviors. Theregularizationterm R(X)quantiﬁesthebalance
between ﬁtting the data and maintaining a degree of uncertainty to prevent overﬁtting. The consistently low
R(X)values across all sectors highlight the model’s eﬃciency in balancing data ﬁt and complexity without
overly conforming to the dataset’s idiosyncrasies.
The assessment of the predictive accuracy of the BEL framework involves a testing process. Initially, the
dataset is partitioned into a training set comprising 50% of the data and a testing set accounting for the
remaining. The root mean square error (RMSE) is then computed for each sector, providing a quantitative
measure of the model’s prediction errors. The empirical results are presented in Table 5. In the ﬁnancial and
technology sectors, BEL outperforms other models. Speciﬁcally, in the ﬁnancial sector, BEL achieves the
lowest RMSE of 0.0264. Similarly, with an RMSE of 0.0304 in the technology sector, it demonstrates superior
predictive accuracy, likely attributable to its treatment of high-dimensional and extreme-value data. This
16Under review as submission to TMLR
Table 4: BEL model statistics for each sector
Sector θH(X)R(X)L(X,θ)
Communication 0.009 2.231 0.001 243.061
Energy 0.012 1.962 0.002 206.879
Financials 0.010 2.355 0.001 236.428
Healthcare 0.010 2.484 0.001 256.712
Industrials 0.011 2.161 0.001 250.121
RealEstate 0.012 2.089 0.001 245.474
Technology 0.010 2.201 0.001 236.218
Utilities 0.011 2.297 0.001 252.280
θis the model parameter. H(X)denotes the entropy. R(X)
is the regularization term. L(X,θ)is the loss function.
contrasts with the energy and real estate sectors, where BEL ranks second, suggesting that these sectors’
data characteristics might align more closely with the methodologies employed in models like SVR and NN.
Table 5: RMSEs for BEL and conventional methods
Sector BEL SVR RF NN
Communication 0.0447(4)0.0497(3)0.0575(2)0.0439(1)
Energy 0.0342(2)0.0338(1)0.0379(3)0.0309(4)
Financials 0.0264(1)0.0273(2)0.0281(3)0.0235(4)
Healthcare 0.0326(3)0.0337(2)0.0406(4)0.0316(1)
Industrials 0.0342(2)0.0376(3)0.0384(4)0.0314(1)
Real Estate 0.0328(2)0.0315(1)0.0335(3)0.0300(4)
Technology 0.0304(1)0.0329(2)0.0340(3)0.0291(4)
Utilities 0.0347(4)0.0339(3)0.0358(2)0.0333(1)
The methods compared here include Support Vector Machines (SVR),
Random Forest (RF), and Neural Networks (NN), each applied to various
sectors to evaluate the Root Mean Square Error (RMSE).
4 Concluding remarks
This paper presents a framework for handling high-dimensional datasets characterized by extreme values.
This work highlights the eﬃcacy of integrating Bayesian inference, extreme value theory, and regularization
techniques rooted in information theory. These methods enhance the robustness of parameter estimation
and reduce overﬁtting. Convergence properties are derived, and universal approximation capabilities for
continuous extreme value distributions are illustrated. The empirical evaluation across diverse economic sec-
tors during the COVID-19 pandemic highlights the framework’s applicability. Further research can focus on
enhancing the framework’s computational eﬃciency and exploring integrating other Bayesian nonparametric
approaches to improve its applicability to larger datasets.
References
O. M. Ardakani. Option pricing with maximum entropy densities: The inclusion of higher-order moments.
Journal of Futures Markets , pp. 1–16, 2022.
O. M. Ardakani. Capturing information in extreme events. Economics Letters , 231:111301, 2023.
O. M. Ardakani and M. Saenz. On the comparison of inequality measures: evidence from the world values
survey.Applied Economics Letters , pp. 1–10, 2022.
17Under review as submission to TMLR
O. M. Ardakani, N. Ebrahimi, and E. S. Sooﬁ. Ranking forecasts by stochastic error distance, information
and reliability measures. International Statistical Review , 86(3):442–468, 2018.
O. M. Ardakani, M. Asadi, N. Ebrahimi, and E. S. Sooﬁ. MR plot: A big data tool for distinguishing
distributions. Statistical Analysis and Data Mining: The ASA Data Science Journal , 13(4):405–418, 2020.
T. Bayes. An essay towards solving a problem in the doctrine of chances. Biometrika , 45(3-4):296–315, 1958.
J. Beirlant, Y. Goegebeur, J. Segers, and J. L. Teugels. Statistics of extremes: theory and applications .
Wiley, 2006.
R. Bellman. Dynamic programming . Princeton University Press, Princeton, 1958.
A. Belloni and C. Hansen. High-dimensional methods and inference on structural and treatment eﬀects.
Journal of Economic Perspectives , 18(2):29–50, 2014.
J. M. Bernardo and A. F. Smith. Bayesian theory . Wiley, 2009.
S. M. Berry, B. P. Carlin, J. J. Lee, and P. Muller. Bayesian adaptive methods for clinical trials . CRC Press,
2010.
A. Bhattacharya, D. Pati, N. S. Pillai, and D. B. Dunson. Dirichlet–Laplace priors for optimal shrinkage.
Journal of the American Statistical Association , 110(512):1479–1490, 2015.
C. M. Bishop and N. M. Nasrabadi. Pattern recognition and machine learning . Springer, New York, 2006.
C. M. Carvalho, N. G. Polson, and J. G. Scott. The horseshoe estimator for sparse signals. Biometrika , 97
(2):465–480, 2010.
V. Chavez-Demoulin and A. C. Davison. Generalized additive modelling of sample extremes. Journal of the
Royal Statistical Society Series C: Applied Statistics , 54(1):207–222, 2005.
S. Coles, J. Bawa, L. Trenner, and P. Dorazio. An introduction to statistical modeling of extreme values .
Springer, London, 2001.
T. M. Cover and J. A. Thomas. Information theory and statistics: Elements of information theory . Wiley,
1991.
A. C. Davison and R. L. Smith. Models for exceedances over high thresholds. Journal of the Royal Statistical
Society Series B: Statistical Methodology , 52(3):393–425, 1990.
G. Ditzler, M. Roveri, C. Alippi, and R. Polikar. Learning in nonstationary environments: A survey. IEEE
Computational Intelligence Magazine , 10(4):12–25, 2015.
J. H. Einmahl and J. Segers. Maximum empirical likelihood estimation of the spectral measure of an
extreme-value distribution. The Annals of Statistics , pp. 2953–2989, 2009.
J. H. Einmahl, V. I. Piterbarg, and L. De Haan. Nonparametric estimation of the spectral measure of an
extreme value distribution. The Annals of Statistics , 29(5):1401–1423, 2001.
J. H. Einmahl, L. Haan, and C. Zhou. Statistics of heteroscedastic extremes. Journal of the Royal Statistical
Society Series B: Statistical Methodology , 78(1):31–51, 2016.
S. Engelke and A. S. Hitz. Graphical models for extremes. Journal of the Royal Statistical Society Series B:
Statistical Methodology , 82(4):871–932, 2020.
J. Fan and J. Lv. A selective overview of variable selection in high dimensional feature space. Statistica
Sinica, 20(1):101–148, 2010.
A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian data analysis . Chapman and Hall/CRC,
1995.
18Under review as submission to TMLR
S. Ghosal and A. Van der Vaart. Fundamentals of nonparametric Bayesian inference . Cambridge University
Press, 2017.
B. Gnedenko. Sur la distribution limite du terme maximum d’une serie aleatoire. Annals of Mathematics ,
pp. 423–453, 1943.
L. Haan and A. Ferreira. Extreme value theory: An introduction . Springer, New York, 2006.
T. Hastie, R. Tibshirani, J. H. Friedman, and J. H. Friedman. The elements of statistical learning: data
mining, inference, and prediction . Springer, New York, 2009.
J. E. Heﬀernan and H. Southworth. Extreme value modelling of dependent series using R. Journal of
Statistical Software , 54(16):1–25, 2013.
K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators.
Neural Networks , 2(5):359–366, 1989.
P. J. Huber. Robust statistics . Wiley, 1981.
A. Kiriliouk, R. Holger, J. Segers, and J. L. Wadsworth. Peaks Over Thresholds modeling with multivariate
generalized Pareto distributions. Technometrics , 61(1):123–135, 2019.
S. Kotz and S. Nadarajah. Extreme value distributions: theory and applications . World Scientiﬁc, 2000.
S. Kullback. Information theory and statistics . Wiley (reprinted in 1968 by Dover), 1959.
S. Kullback and R. Leibler. On information and suﬃciency. Annals of Mathematical Statistics , 22:79–86,
1951.
M. R. Leadbetter, L. Georg, and R. Holger. Extremes and related properties of random sequences and
processes . Springer Science & Business Media, 2012.
P. Muller and F. A. Quintana. Nonparametric Bayesian data analysis. Statistical Science , 19(1):95–110,
2004.
T. Park and G. Casella. The bayesian lasso. Journal of the American Statistical Association , 103(482):
681–686, 2008.
P. J. Rousseeuw and A. M. Leroy. Robust regression and outlier detection . Wiley, 2005.
H. Sang and A. E. Gelfand. Hierarchical modeling for extreme values observed over space and time. Envi-
ronmental and Ecological Statistics , 16(3):407–426, 2009.
L. Schwartz. On bayes procedures. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete , 4(1):
10–26, 1965.
C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal , 27:379 423,
1948.
R. L. Smith. Maximum likelihood estimation in a class of nonregular cases. Biometrika , 72(1):67–90, 1985.
E. S. Sooﬁ. A generalizable formulation of conditional logit with diagnostics. Journal of the American
Statistical Association , 87:812–816, 1992.
E. S. Sooﬁ, N. Ebrahimi, and M. Habibullah. Information distinguishability with application to analysis of
failure data. Journal of the American Statistical Association , 90:657–668, 1995.
M. E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning
Research , 1:211–244, 2001.
J. L. Wadsworth and J. A. Tawn. Dependence modelling for spatial extremes. Biometrika , 99(2):253–272,
2012.
S. G. Walker. Bayesian Nonparametrics. In N. L. Hjort, C. Holmes, P. Muller, and S. G. Walker (eds.),
Bayesian nonparametric methods: motivation and ideas , volume 28, pp. 22–34. 2010.
19