Under review as submission to TMLR
Robust Constrained Reinforcement Learning
Anonymous authors
Paper under double-blind review
Abstract
Constrained reinforcement learning is to maximize the reward subject to constraints on
utilities/costs. However, in practice it is often the case that the training environment is not
the same as the test one, due to, e.g., modeling error, adversarial attack, non-stationarity,
resulting in severe performance degradation and more importantly constraint violation in
the test environment. To address this challenge, we formulate the framework of robust
constrained reinforcement learning under model uncertainty, where the MDP is not fixed
but lies in some uncertainty set. The goal is two fold: 1) to guarantee that constraints
on utilities/costs are satisfied for all MDPs in the uncertainty set, and 2) to maximize the
worst-case reward performance over the uncertainty set. We design a robust primal-dual
approach, and further develop theoretical guarantee on its convergence, complexity and
robust feasibility. We then investigate a concrete example of δ-contamination uncertainty
set, design an online and model-free algorithm and theoretically characterize its sample
complexity.
1 Introduction
In many practical reinforcement learning (RL) applications, it is critical for an agent to meet certain constraints
on utilities/costs while maximizing the reward. This problem is usually modeled as the constrained Markov
decision processes (CMDPs) Altman (1999). Consider a CMDP with state space S, action space A, transition
kernel P={pa
s∈∆S1:s∈S,a∈A}, reward and utility functions: r,ci:S×A→[0,1],1≤i≤m, and
discount factor γ. The goal of CMDP is to find a stationary policy π:S→∆Athat maximizes the expected
reward subject to constraints on the utility:
max
π∈ΠEπ,P/bracketleftigg∞/summationdisplay
t=0γtr(St,At)|S0∼ρ/bracketrightigg
,s.t.Eπ,P/bracketleftigg∞/summationdisplay
t=0γtci(St,At)|S0∼ρ/bracketrightigg
≥bi,1≤i≤m, (1)
whereρis the initial state distribution, bi’s are some thresholds and Eπ,Pdenotes the expectation when the
agent follows policy πand the environment transits following P.
In practice, it is often the case that the environment on which the learned policy will deploy (the test
environment) possibly deviates from the training one, due to, e.g., modeling error of the simulator, adversarial
attack, and non-stationarity. This could lead to a significant performance degradation in reward, and more
importantly, constraints may not be satisfied anymore, which is severe in safety-critical applications. For
example, a drone may run out of battery and crash due to mismatch between training and test environments.
This hence motivates the study of robust constrained RL in this paper. In this paper, we take a pessimistic
approach in face of uncertainty. Specifically, consider a set of transition kernels P, which is usually constructed
in a way to include the test environment with high probability Iyengar (2005); Nilim & El Ghaoui (2004);
Bagnell et al. (2001). The learned policy should satisfy the constraints under all these environments in P, i.e.,
∀P∈P,
Eπ,P/bracketleftigg∞/summationdisplay
t=0γtci(St,At)|S0∼ρ/bracketrightigg
≥bi, (2)
1∆Xdenotes the probability simplex supported on the set X.
1Under review as submission to TMLR
which is equivalent to minP∈PEπ,P[/summationtext∞
t=0γtc(St,At)|S0∼ρ]≥bi. At the same time, we aim to optimize the
worst-case reward performance over P:
max
π∈Πmin
P∈PEπ,P/bracketleftigg∞/summationdisplay
t=0γtr(St,At)|S0∼ρ/bracketrightigg
,
s.t.min
P∈PEπ,P/bracketleftigg∞/summationdisplay
t=0γtci(St,At)|S0∼ρ/bracketrightigg
≥bi,1≤i≤m. (3)
On one hand, a feasible solution to eq. (3) always satisfies eq. (2), and on the other hand, the solution to
eq. (3) provides a performance guarantee for any P∈P. We note that our approach and analysis can also be
applied to the optimistic approach in face of uncertainty.
In this paper, we design and analyze a robust primal-dual algorithm for the problem of robust constrained
RL. In particular, the technical challenges and our major contributions are as follows.
•We take the Lagrange multiplier method to solve the constrained policy optimization problem. A first
question is that whether the primal problem is equivalent to the dual problem, i.e., whether the duality
gap is zero. For non-robust constrained RL, the Lagrange function has a zero duality gap Paternain et al.
(2019); Altman (1999). However, we show that this is not necessarily true in the robust constrained setting.
Note that the set of visitation distribution being convex is one key property to show zero duality gap of
constrained MDP Altman (1999); Paternain et al. (2019). In this paper, we constructed a novel counter
example showing that the set of robust visitation distributions for our robust problem is non-convex.
•In the dual problem of non-robust CMDPs, the sum of two value functions is actually a value function of
the combined reward. However, this does not hold in the robust setting, since the worst-case transition
kernels for the two robust value functions are not necessarily the same. Therefore, the geometry of our
Lagrangian function is much more complicated. In this paper, we formulate the dual problem of the robust
constrained RL problem as a minimax linear-nonconcave optimization problem, and show that the optimal
dual variable is bounded. We then construct a robust primal-dual algorithm by alternatively updating the
primal and dual variables. We theoretically prove the convergence to stationary points, and characterize
its complexity.
•In general, convergence to stationary points of the Lagrangian function does not necessarily imply that the
solution is feasible Lin et al. (2020); Xu et al. (2023). We design a novel proof to show that the gradient
belongs to the normal cone of the feasible set, based on which we further prove the robust feasibility of
the obtained policy.
•We apply and extend our results on an important uncertainty set referred to as δ-contamination model
Huber (1965). Under this model, the robust value functions are not differentiable and we hence propose a
smoothed approximation of the robust value function towards a better geometry. We further investigate
the practical online and model-free setting and design an actor-critic type algorithm. We also establish its
convergence, sample complexity, and robust feasibility.
We then discuss works related to robust constrained RL.
Robust constrained RL. In Russel et al. (2020), the robust constrained RL problem was studied, and a
heuristic approach was developed. The basic idea is to estimate the robust value functions, and then to use
the vanilla policy gradient method Sutton et al. (1999) with the vanilla value function replaced by the robust
value function. However, this approach did not take into consideration the fact that the worst-case transition
kernel is also a function of the policy (see Section 3.1 in Russel et al. (2020)), and therefore the "gradient"
therein is not actually the gradient of the robust value function. Thus, its performance and convergence cannot
be theoretically guaranteed. The other work Mankowitz et al. (2020) studied the same robust constrained RL
problem under the continuous control setting, and proposed a similar heuristic algorithm. They first proposed
a robust Bellman operator and used it to estimate the robust value function, which is further combined
with some non-robust continuous control algorithm to update the policy. Both approaches in Russel et al.
2Under review as submission to TMLR
(2020) and Mankowitz et al. (2020) inherit the heuristic structure of "robust policy evaluation" + "non-robust
vanilla policy improvement", which may not necessarily guarantee an improved policy in general. In this
paper, we employ a "robust policy evaluation" + " robustpolicy improvement" approach, which guarantees
an improvement in the policy, and more importantly, we provide theoretical convergence guarantee, robust
feasibility guarantee, and complexity analysis for our algorithms.
Constrained RL. The most commonly used method for constrained RL is the primal-dual method Altman
(1999); Paternain et al. (2019; 2022); Auer et al. (2008), which augments the objective with a sum of constraints
weighted by their corresponding Lagrange multipliers, and then alternatively updates the primal and dual
variables. Under the non-robust constrained MDP problems, it was shown that the strong duality holds, and
hence the primal-dual method has zero duality gap Paternain et al. (2019); Altman (1999). The convergence
rate of the primal-dual method was moreover investigated in Ding et al. (2020; 2021); Li et al. (2021b); Liu
et al. (2021); Ying et al. (2021).
Another class of method is the primal method, which does not introduce any dual variables or Lagrange
multipliers; Instead, it checks if the constraints are satisfied after each update, and enforces the constraints if
not Achiam et al. (2017); Liu et al. (2020); Chow et al. (2018); Dalal et al. (2018); Xu et al. (2021); Yang
et al. (2020).
The above studies, when directly applied to robustconstrained RL, cannot guarantee the constraints when
there is model deviation. Moreover, the objective and constraints in this paper take min over the uncertainty
set (see eq. (4)), and therefore have much more complicated geometry than the non-robust case.
Non-Constrained Robust RL under model uncertainty. The non-constrained robust RL was first
introduced and studied in Iyengar (2005); Nilim & El Ghaoui (2004), where the uncertainty set is assumed to
be known, and the problem can be solved using robust dynamic programming. It was further extended to the
model-free setting, where the learner assumes no knowledge of the environment, and can only get samples
from its centroid transition kernel Roy et al. (2017); Wang & Zou (2021; 2022); Zhou et al. (2021); Yang et al.
(2021); Panaganti & Kalathil (2021); Ho et al. (2018; 2021).
These works, however, mainly focus on robust RL without constraints, whereas in this paper we investigate
robust RL with constraints, which is more challenging. There is a related line of works on (robust) imitation
learning Ho & Ermon (2016); Fu et al. (2017); Torabi et al. (2018); Viano et al. (2022), which can be
formulated as a constrained problem. But their problem settings and approaches are fundamentally different
from ours.
2 Preliminaries
Constrained MDP. Consider the CMDP problem in eq. (1). Define the visitation distribution induced by
policyπand transition kernel P:dπ
ρ,P(s,a) = (1−γ)/summationtext∞
t=0γtP(St=s,At=a|S0∼ρ,π,P). It can be shown
that the set of the visitation distributions of all policies {dπ
ρ,P∈∆S×A:π∈Π}is convex Paternain et al.
(2022); Altman (1999). Based on this convexity, the strong duality of CMDP can be established Altman
(1999); Paternain et al. (2019) under a standard assumption referred as Slater’s condition: Bertsekas (2014);
Ding et al. (2021): there exists a constant ζ >0and a policy π∈Πs.t.∀i,Vπ
ci,P−bi≥ζ.
Robust MDP. In this paper, we focus on the (s,a)-rectangular uncertainty set Nilim & El Ghaoui (2004);
Iyengar (2005), i.e., P=/circlemultiplytext
s,aPa
s, where Pa
s⊆∆S. At each time step, the environment transits following a
transition kernel belonging to the uncertainty set Pt∈P. The robust value function of a policy πis then
defined as the worst-case expected accumulative discounted reward following policy πover all MDPs in the
uncertainty set Nilim & El Ghaoui (2004); Iyengar (2005):
Vπ
r,P(s)≜ min
κ=(P0,P1,...)∈/circlemultiplytext
t≥0PEκ/bracketleftigg∞/summationdisplay
t=0γtr(St,At)|S0=s,π/bracketrightigg
, (4)
where Eκdenotes the expectation when the state transits according to κ. It was shown that the robust
value function is the fixed point of the robust Bellman operator Nilim & El Ghaoui (2004); Iyengar (2005);
Puterman (2014): TπV(s)≜/summationtext
a∈Aπ(a|s)/parenleftbig
r(s,a) +γσPa
s(V)/parenrightbig
,whereσPas(V)≜minp∈Pasp⊤Vis the support
function of VonPa
s.
3Under review as submission to TMLR
Note that the minimizer of eq. (4), κ∗, is stationary in time Iyengar (2005), which we denote by κ∗=
{Pπ,Pπ,...}, and refer to Pπas the worst-case transition kernel. Then the robust value function Vπ
r,Pis actually
the value function under policy πand transition kernel Pπ. The goal of robust RL is to find the optimal robust
policyπ∗that maximizes the worst-case accumulative discounted reward: π∗= arg max πVπ
r,P(s),∀s∈S.
3 Robust Constrained RL
Recall the robust constrained RL formulated in eq. (3):
max
θ∈ΘVπθr(ρ),s.t.Vπθci(ρ)≥bi,1≤i≤m, (5)
where for simplicity we omit the subscript PinVπθ
⋄,Pand denote by Vπθci(ρ)andVπθr(ρ)the robust value
function for ciandrunderπθ. The goal of eq. (5) is to find a policy that maximizes the robust reward value
function among those feasible solutions. Here, any feasible solution to eq. (5) can guarantee that under any
MDP in the uncertainty set, its accumulative discounted utility is always no less than bi, which guarantees
robustness to constraint violation under model uncertainty. Furthermore, the optimal solution to eq. (5)
achieves the best "worst-case reward performance" among all feasible solutions. If we use the optimal solution
to eq. (5), then under any MDP in the uncertainty set, we have a guaranteed reward no less than the value of
eq. (5).
In this paper, we focus on the parameterized policy class, i.e., πθ∈ΠΘ, whereΘ⊆Rdis a parameter set and
ΠΘis a class of parameterized policies, e.g., direct parameterized policy, softmax or neural network policy.
For technical convenience, we adopt a standard assumption on the policy class.
Assumption 1. The policy class ΠΘisk-Lipschitz and l-smooth, i.e., for any s∈Sanda∈Aand for any
θ∈Θ, there exist universal constants k,l, such that∥∇πθ(a|s)∥≤k,and∥∇2πθ(a|s)∥≤l.
This assumption can be satisfied by many policy classes, e.g., direct parameterization Agarwal et al. (2021),
soft-max Mei et al. (2020); Li et al. (2021a); Wang & Zou (2020), or neural network with Lipschitz and
smooth activation functions Du et al. (2019); Neyshabur (2017); Miyato et al. (2018).
The problem eq. (5) is equivalent to the following max-min problem:
max
θ∈Θmin
λi≥0Vπθr(ρ) +m/summationdisplay
i=1λi(Vπθci(ρ)−bi). (6)
Unlike non-robust CMDP, strong duality for robust constrained RL may not hold. For robust RL, the robust
value function can be viewed as the value function for policy πunder its worst-case transition kernel Pπ,
and therefore can be written as the inner product between the reward (utility) function and the visitation
distribution induced by πandPπ(referred to as robust visitation distribution of π). The following lemma
shows that the set of robust visitation distributions may not be convex, and therefore, the approach used in
Altman (1999); Paternain et al. (2019) to show strong duality cannot be applied here.
Lemma 1. There exists a robust MDP, such that the set of robust visitation distributions is non-convex.
In the following, we focus on the dual problem of eq. (6). For simplicity, we investigate the case with one
constraint, and extension to the case with multiple constraints is straightforward:
min
λ≥0max
θ∈ΘVπθr(ρ) +λ(Vπθc(ρ)−b). (7)
We make an assumption of Slater’s condition, assuming there exists at least one strictly feasible policy
Bertsekas (2014); Ding et al. (2021), under which, we further show that the optimal dual variable of eq. (7) is
bounded.
Assumption 2. There exists ζ >0and a policy π∈ΠΘ, s.t.Vπ
c(ρ)−b≥ζ.
Lemma 2. Denote the optimal solution of eq. (7)by(λ∗,πθ∗). Then,λ∗∈/bracketleftig
0,2
ζ(1−γ)/bracketrightig
.
4Under review as submission to TMLR
Lemma 2 suggests that the dual problem eq. (7) is equivalent to a bounded min-max problem:
min
λ∈/bracketleftbig
0,2
ζ(1−γ)/bracketrightbigmax
θ∈ΘVπθr(ρ) +λ(Vπθc(ρ)−b). (8)
In general, the robust value functions Vπ
randVπ
care not differentiable. Although we can use sub-differential-
based approaches Wang & Zou (2022), it may bring huge difficulties in analysis. To solve this issue, we make
the following assumption.
Assumption 3. There exists some approximation function ˜Vπ
rand˜Vπ
c, and define the corresponding Lagrange
function ˜VL(θ,λ) =˜Vπθr(ρ) +λ(˜Vπθc(ρ)−b)such that Assumption 2 holds for ˜V, and the gradients of the
Lagrangian function are Lipschitz:
∥∇λ˜VL(θ,λ)|θ1−∇λ˜VL(θ,λ)|θ2∥≤L11∥θ1−θ2∥, (9)
∥∇λ˜VL(θ,λ)|λ1−∇λ˜VL(θ,λ)|λ2∥≤L12|λ1−λ2|, (10)
∥∇θ˜VL(θ,λ)|θ1−∇θ˜VL(θ,λ)|θ2∥≤L21∥θ1−θ2∥, (11)
∥∇θ˜VL(θ,λ)|λ1−∇θ˜VL(θ,λ)|λ2∥≤L22|λ1−λ2|. (12)
We will justify this assumption under a widely-used contamination uncertainty model in the next section.
Possible approaches for obtaining such approximation include using its Moreau envelope or a regularized
version of the robust value function, e.g., entropy regularization. If ˜Vapproximates the robust value function
V, the solution to ˜VLis also close to the one to VL. We hence aim to solve the smoothed problem instead.
We will also characterize this approximation error in the next section for the contamination model.
The problem in eq. (8) is a bounded linear-nonconcave optimization problem. We then propose our robust
primal-dual algorithm for robust constrained RL in Algorithm 1. The basic idea of Algorithm 1 is to perform
Algorithm 1 Robust Primal-Dual algorithm (RPD)
Input:T,αt,βt,bt
Initialization :λ0,θ0
fort= 0,1,...,T−1do
λt+1←/producttext
[0,Λ∗]/parenleftig
λt−1
βt/parenleftbig˜Vπθtc(ρ)−b/parenrightbig
−bt
βtλt/parenrightig
θt+1←/producttext
Θ/parenleftig
θt+1
αt/parenleftbig
∇θ˜Vπθtr(ρ) +λt+1∇θ˜Vπθtc(ρ)/parenrightbig/parenrightig
end for
Output:θT
gradient descent-ascent w.r.t. λandθalternatively. When the policy πviolates the constraint, the dual
variableλincreases such that λVπ
cdominatesVπ
r. Then the gradient ascent will update θuntil the policy
satisfies the constraint. Therefore, this approach is expected to find a feasible policy (as will be shown in
Lemma 5). Here,/producttext
X(x)denotes the projection of xto the set X, and{bt}is a non-negative monotone
decreasing sequence, which will be specified later. Algorithm 1 reduces to the vanilla gradient descent-ascent
algorithm in Lin et al. (2020) if bt= 0. However, btis critical to the convergence of Algorithm 1 Xu et al.
(2023). The outer problem of eq. (8) is actually linear, and after introducing bt, the update of λtcan be
viewed as a gradient descent of a strongly-convex function λ(Vc−b) +bt
2λ2, which converges more stable and
faster.
Denote that Lagrangian function by VL(θ,λ)≜Vπθr(ρ) +λ(Vπθc(ρ)−b), and further denote the gradient
mapping of Algorithm 1 by
Gt≜
βt/parenleftig
λt−/producttext
[0,Λ∗]/parenleftig
λt−1
βt/parenleftbig
∇λ˜VL(θt,λt)/parenrightbig/parenrightig/parenrightig
αt/parenleftig
θt−/producttext
Θ/parenleftig
θt+1
αt/parenleftbig
∇θ˜VL(θt,λt)/parenrightbig/parenrightig/parenrightig
. (13)
The gradient mapping is a standard measure of convergence for projected optimization approaches Beck
(2017). Intuitively, it reduces to the gradient (∇λ˜VL,∇θ˜VL), whenΛ∗=∞andΘ=Rd, and it measures
5Under review as submission to TMLR
the updates of θandλat time step t. If∥Gt∥→0, the updates of both variables are small, and hence the
algorithm converges to a stationary solution.
As will be shown in Section 4, Assumption 3 can be satisfied with a smoothed approximation of the robust
value function.
In the following theorem, we show that our robust primal-dual algorithm converges to a stationary point
of the min-max problem eq. (16), with a complexity of O(ϵ−4). The accurate statement can be found in
Appendix E.
Theorem 1. Under Assumption 3, if we set step sizes αt,βt,andbtas in Section E and T=O((Λ∗)4ϵ−4),
then min 1≤t≤T∥Gt∥≤2ϵ.
We note that this convergence rate matches the rate of general convex-nonconcave optimization problems in
Xu et al. (2023), which is State-of-the-Art according to our best knowledge.
The next proposition characterizes the feasibility of the obtained policy.
Proposition 1. Denote by W≜arg min 1≤t≤T∥Gt∥. IfλW−1
βW/parenleftbig
∇λ˜VL(θW,λW)/parenrightbig
∈[0,Λ∗), thenπW
satisfies the constraint with a 2ϵ-violation.
In general, convergence to stationary points of the Lagrangian function does not necessarily imply that the
solution is feasible. Proposition 1 shows that Algorithm 1 always return a policy that is robust feasible, i.e.,
satisfying the constraints in eq. (5). Intuitively, if we set Λ∗larger so that the optimal solution λ∗∈[0,Λ∗),
then Algorithm 1 is expected to converge to an interior point of [0,Λ∗]and therefore, πWis feasible. On the
other hand, Λ∗can’t be set too large. Note that the complexity in Theorem 1 depends on Λ∗(see eq. (54) in
the appendix), and a larger Λ∗means a higher complexity.
4 δ-Contamination Uncertainty Set
In this section, we investigate a concrete example of robust constrained RL with δ-contamination uncertainty
set. The method we developed here can be similarly extended to other types of uncertainty sets like
KL-divergence or total variation. The δ-contamination uncertainty set models the scenario where the
state transition of the MDP could be arbitrarily perturbed with a small probability δ. This model is
widely used to model distributional uncertainty in the literature of robust learning and optimization, e.g.,
Huber (1965); Du et al. (2018); Huber & Ronchetti (2009); Nishimura & Ozaki (2004; 2006); Prasad et al.
(2020a;b); Wang & Zou (2021; 2022). Specifically, let P={pa
s|s∈S,a∈A}be the centroid transition
kernel, then the δ-contamination uncertainty set centered at Pis defined as P≜/circlemultiplytext
s∈S,a∈APa
s, where
Pa
s≜{(1−δ)pa
s+δq|q∈∆S},s∈S,a∈A.
Under the δ-contamination setting, the robust Bellman operator can be explicitly computed: TπV(s) =/summationtext
a∈Aπ(a|s)/parenleftbig
r(s,a) +γ/parenleftbig
δmins′V(s′) + (1−δ)/summationtext
s′∈Spa
s,s′V(s′)/parenrightbig/parenrightbig
.In this case, the robust value function
is non-differentiable due to the minterm, and hence Assumption 3 does not hold. One possible approach
is to use sub-gradient, which, however, is less stable, and its convergence is difficult to characterize. In the
following, we design a differentiable and smooth approximation of the robust value function. Specifically,
consider a smoothed robust Bellman operator Tπ
σusing the LSE function:
Tπ
σV(s) =EA∼π(·|s)/bracketleftbigg
r(s,A) +γ(1−δ)/summationdisplay
s′∈SpA
s,s′V(s′) +γδLSE(σ,V)/bracketrightbigg
, (14)
whereLSE(σ,V) =log(/summationtextd
i=1eσV(i))
σforV∈Rdand someσ < 0. The approximation error |LSE(σ,V)−
minV|=O(σ−1)→0asσ→−∞, and hence the fixed point of Tπ
σ, denoted by Vπ
σ, is an approximation
of the robust value function Vπ(Theorem 4 in Wang & Zou (2022)). We refer to Vπ
σas the smoothed
robust value function and define the smoothed robust action-value function as Qπ
σ(s,a)≜r(s,a) +γ(1−
δ)/summationtext
s′∈Spa
s,s′Vπ
σ(s′) +γδLSE(σ,Vπ
σ). It can be shown that for any π, asσ→−∞,∥Vπ
r−Vπ
σ,r∥→ 0and
∥Vπ
c−Vπ
σ,c∥→0.
6Under review as submission to TMLR
The gradient of Vπθσcan be computed explicitly Wang & Zou (2022): ∇Vπθσ(s) =B(s,θ) +
γδ/summationtext
s∈SeσVπθσ(s)B(s,θ)
(1−γ)/summationtext
s∈SeσVπθσ(s),whereB(s,θ)≜1
1−γ+γδ/summationtext
s′∈Sdπθ
s,P(s′)/summationtext
a∈A∇πθ(a|s′)Qπθσ(s′,a), anddπθ
s,P(·)is the
visitation distribution of πθunder Pstarting from s. Denote the smoothed Lagrangian function by
VL
σ(θ,λ)≜Vπθσ,r(ρ) +λ(Vπθσ,c(ρ)−b). The following lemma shows that ∇VL
σis Lipschitz.
Lemma 3.∇VL
σis Lipschitz in θandλ. And hence Assumption 3 holds for VL
σ.
A natural idea is to use the smoothed robust value functions to replace the ones in eq. (7):
min
λ≥0max
π∈ΠΘVπ
σ,r(ρ) +λ(Vπ
σ,c(ρ)−b). (15)
As will be shown below in Lemma 6, this approximation can be arbitrarily close to the original problem in
eq. (7) asσ→−∞. We first show that under Assumption 2, the following Slater’s condition holds for the
smoothed problem in eq. (15).
Lemma 4. Letσ=O(ϵ−1)be sufficiently large such that ∥Vπ
σ,c−Vπ
c∥< ζfor anyπ, then the Slater’s
condition also holds for Vπ
σ,c, i.e., there exists ζ′>0and a policy π′∈ΠΘs.t.Vπ′
σ,c(ρ)−b≥ζ′.
The following lemma shows that the optimal dual variable for eq. (15) is also bounded.
Lemma 5. Denote the optimal solution of eq. (15)by(λ∗,πθ∗). Thenλ∗∈/bracketleftig
0,2Cσ
ζ′/bracketrightig
, whereCσis the upper
bound of smoothed robust value functions Vπ
σ,c.
Denote by Λ∗=max/braceleftig
2Cσ
ζ′,2
ζ(1−γ)/bracerightig
, then problems eq. (8) and eq. (15) are equivalent to the following
bounded ones: minλ∈[0,Λ∗]maxπ∈ΠΘVπ
r(ρ) +λ(Vπ
c(ρ)−b),and
min
λ∈[0,Λ∗]max
π∈ΠΘVπ
σ,r(ρ) +λ(Vπ
σ,c(ρ)−b). (16)
The following lemma shows that the two problems are within a gap of O(ϵ).
Lemma 6. Choose a large enough σ=O(ϵ−1)such that∥Vπ
r−Vπ
σ,r∥≤ϵand∥Vπ
c−Vπ
σ,c∥≤ϵ. Then
/vextendsingle/vextendsingle/vextendsingle/vextendsinglemin
λ∈[0,Λ∗]max
π∈ΠΘVπ
σ,r(ρ) +λ(Vπ
σ,c(ρ)−b)−min
λ∈[0,Λ∗]max
π∈ΠΘVπ
r(ρ) +λ(Vπ
c(ρ)−b)/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤(1 +Λ∗)ϵ.
In the following, we hence focus on the smoothed dual problem in eq. (16), which is an accurate approximation
of the original problem eq. (8). Denote the gradient mapping of the smoothed Lagrangian function VL
σby
Gt≜
βt/parenleftig
λt−/producttext
[0,Λ∗]/parenleftig
λt−1
βt/parenleftbig
∇λVL
σ(θt,λt)/parenrightbig/parenrightig/parenrightig
αt/parenleftig
θt−/producttext
Θ/parenleftig
θt+1
αt/parenleftbig
∇θVL
σ(θt,λt)/parenrightbig/parenrightig/parenrightig
. (17)
Applying our RPD algorithm in eq. (16), we have the following convergence guarantee.
Corollary 1. If we set step sizes αt,βt,andbtas in Section J and set T=O((Λ∗)8ϵ−12), then
min 1≤t≤T∥Gt∥≤2ϵ.
This corollary implies that our robust primal-dual algorithm converges to a stationary point of the min-max
problem eq. (16) under the δ-contamination model, with a complexity of O(ϵ−12).
5 Robust Primal-Dual Method for Model-Free Setting
In the previous sections, we assumed full knowledge of the environment, the smoothed robust value functions,
and their gradients. However, such information may not be readily available in real-world applications. To
address this challenge, we consider a more practical model-free setting, where we have no prior knowledge
7Under review as submission to TMLR
of the environment or robust value functions and can only obtain samples from the nominal transition
kernel. This setting presents new challenges in terms of designing effective algorithms for constrained robust
reinforcement learning. By extending our approach to the model-free setting, we aim to provide a more
practical and applicable solution to robust constrained RL problems.
Different from the non-robust value function which can be estimated using Monte Carlo, robust value functions
are the value function corresponding to the worst-case transition kernel from which no samples are directly
taken.
To solve this issue, we adopt the smoothed robust TD algorithm (Algorithm 2) from Wang & Zou (2022) to
estimate the smoothed robust value functions.
Algorithm 2 Smoothed Robust TD Wang & Zou (2022)
Input:Tinner,π,σ,c
Initialization :Q0,s0
fort= 0,1,...,T inner−1do
Chooseat∼π(·|st)and observe ct,st+1
Vt(s)←/summationtext
a∈Aπ(a|s)Qt(s,a)for alls∈S
Qt+1(st,at)←Qt(st,at) +αt/parenleftbig
ct+γ(1−δ)·Vt(st+1) +γδ·LSE(σ,Vt)−Qt(st,at)/parenrightbig
end for
Output:QTinner,c≜QTinner
It was shown that the smoothed robust TD algorithm converges to the smoothed robust value function with
a sample complexity of O(ϵ−2)Wang & Zou (2022) under the tabular case. We then construct our online and
model-free RPD algorithm as in Algorithm 3. We note that Algorithm 3 is for the tabular setting with finite
SandA. It can be easily extended to the case with large/continuous SandAusing function approximation.
Algorithm 3 Online Robust Primal-Dual algorithm
Input:T,σ,ϵest,βt,αt,bt,r,c
Initialization :λ0,θ0
fort= 0,1,...,T−1do
SetTinner =O/parenleftig
1
ϵ2
est/parenrightig
and run Algorithm 2 for randc, outputQTinner,r,QTinner,c
ˆVπθtσ,r(s)←/summationtext
aπθt(a|s)QTinner,r(s,a),ˆVπθtσ,c(s)←/summationtext
aπθt(a|s)QTinner,c(s,a)
ˆVπθtσ,r(ρ)←/summationtext
sρ(s)ˆVπθtσ,r(s),ˆVπθtσ,c(ρ)←/summationtext
sρ(s)ˆVπθtσ,c(s)
λt+1←/producttext
[0,Λ∗]/parenleftig
λt−1
βt/parenleftig
ˆVπθtσ,c(ρ)−b/parenrightig
−bt
βtλt/parenrightig
θt+1←/producttext
Θ/parenleftig
θt+1
αt/parenleftig
∇θˆVπθtσ,r(ρ) +λt+1∇θˆVπθtσ,c(ρ)/parenrightig/parenrightig
end for
Output:θT
Algorithm 3 can be viewed as a biased stochastic gradient descent-ascent algorithm. It is a sample-based
algorithm without assuming any knowledge of robust value functions and can be performed in an online
fashion. We further extend the convergence results in Theorem 1 to the model-free setting, and characterize
the following finite-time error bound of Algorithm 3. Similarly, Algorithm 3 can be shown to achieve a
2ϵ-feasible policy almost surely.
Under the online model-free setting, the estimation of the robust value functions is biased. Therefore, the
analysis is more challenging than the existing literature, where it is usually assumed that the gradients are
exact. We develop a new method to bound the bias accumulated in every iteration of the algorithm and
establish the final convergence results. The formal statement and discussion on the sample complexity can be
found in Theorem 3.
8Under review as submission to TMLR
Theorem 2. Set step sizes αt,βt,andbtas in Section J, and let ϵest=O(ϵ2
σ2t0.75),T=O(ϵ−12), then
min 1≤t≤T∥Gt∥≤(1 +√
2)ϵ.
6 Numerical Results
In this section, we numerically demonstrate the robustness of our algorithm in terms of both maximizing
robust reward value function and satisfying constraints under model uncertainty. We compare our RPD
algorithm with the heuristic algorithms in Russel et al. (2021); Mankowitz et al. (2020) and the vanilla
non-robust primal-dual method. Based on the idea of "robust policy evaluation" + "non-robust policy
improvement" in Russel et al. (2021); Mankowitz et al. (2020), we combine the robust TD algorithm 2 with
non-robust vanilla policy gradient method Sutton et al. (1999), which we refer to as the heuristic primal-dual
algorithm. Several environments, including Garnet Archibald et al. (1995), 8×8Frozen-Lake and Taxi
environments from OpenAI Brockman et al. (2016), are investigated.
We first run the algorithm and store the obtained policies πtat each time step. At each time step, we run
robust TD with a sample size 200 for 30 times to estimate the objective Vr(ρ)and the constraint Vc(ρ). We
then plot them v.s. the number of iterations t. The upper and lower envelopes of the curves correspond to
the 95 and 5 percentiles of the 30 curves, respectively. We deploy our experiments on the δ-contamination
model, and repeat for two different values of δ= 0.2,0.3.
Garnet problem. A Garnet problem can be specified by G(Sn,An), where the state space ShasSnstates
(s1,...,sSn)and action space has Anactions (a1,...,aAn). The agent can take any actions in any state, and
receives a randomly generated reward/utility signal generated from the uniform distribution on [0,1]. The
transition kernels are also randomly generated. The comparison results are shown in Fig.1.
8×8Frozen-Lake problem. We then compare the three algorithms under the 8×8Frozen-lake problem
setting in Fig.4. The Frozen-Lake problem involves a frozen lake of size 8×8which contains several "holes".
The agent aims to cross the lake from the start point to the end point without falling into any holes. The
agent receives r=−10andc= 30when falling in a hole, receives r= 20andc= 5when arriving at the
endpoint; At other times, the agent receives r= 2and a randomly generated utility caccording to the
uniform distribution on [0,10].
Taxi problem. We then compare the three algorithms under the Taxi problem environment. The taxi
problem simulates a taxi driver in a 5×5map. There are four designated locations in the grid world and a
passenger occurs at a random location of the designated four locations at the start of each episode. The goal
of the driver is to first pick up the passengers and then drop them off at another specific location. The driver
receivesr= 20for each successful drop-off and always receives r=−1at other times. We randomly generate
the utility according to the uniform distribution on [0,50] for each state-action pair. The results are shown in
Fig.3.
From the experiment results above, it can be seen that: (1) Both our RPD algorithm and the heuristic
primal-dual approach find feasible policies satisfying the constraint under the worst-case scenario, i.e., Vπ
c≥b.
However, the non-robust primal-dual method fails to find a feasible solution that satisfy the constraint under
the worst-case scenario. (2) Compared to the heuristic PD method, our RPD method can obtain more reward
and can find a more robust policy while satisfying the robust constraint. Note that the non-robust PD method
obtain more reward, but this is because the policy it finds violates the robust constraint. Our experiments
demonstrate that among the three algorithms, our RPD algorithm is the best one which optimizes the
worst-case reward performance while satisfying the robust constraints on the utility.
9Under review as submission to TMLR
(a)Vcwhenδ= 0.2.
 (b)Vrwhenδ= 0.2.
 (c)Vcwhenδ= 0.3.
 (d)Vrwhenδ= 0.3.
Figure 1: Comparison on Garnet problem G(20,10).
(a)Vcwhenδ= 0.2.
 (b)Vrwhenδ= 0.2.
 (c)Vcwhenδ= 0.3.
 (d)Vrwhenδ= 0.3.
Figure 2: Comparison on 4×4Frozen-Lake problem.
(a)Vcwhenδ= 0.2.
 (b)Vrwhenδ= 0.2.
 (c)Vcwhenδ= 0.3.
 (d)Vrwhenδ= 0.3.
Figure 3: Comparison on Taxi problem.
7 Conclusion
In this paper, we formulate the problem of robust constrained reinforcement learning under model uncertainty,
where the goal is to guarantee that constraints are satisfied for all MDPs in the uncertainty set, and to
maximize the worst-case reward performance over the uncertainty set. We propose a robust primal-dual
algorithm, and theoretically characterize its convergence, complexity and robust feasibility. Our algorithm
guarantees convergence to a feasible solution, and outperforms the other two heuristic algorithms. We
further investigate a concrete example with δ-contamination uncertainty set, and construct online and
model-free robust primal-dual algorithm. Our methodology can also be readily extended to problems with
other uncertainty sets like KL-divergence, total variation and Wasserstein distance. The major challenge lies
in deriving the robust policy gradient, and further designing model-free algorithm to estimate the robust
value function.
References
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In Proc.
International Conference on Machine Learning (ICML) , pp. 22–31. PMLR, 2017.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient
methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research , 22
(98):1–76, 2021.
Eitan Altman. Constrained Markov decision processes: stochastic modeling . Routledge, 1999.
10Under review as submission to TMLR
TW Archibald, KIM McKinnon, and LC Thomas. On the generation of Markov decision processes. Journal
of the Operational Research Society , 46(3):354–361, 1995.
Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. In
Proc. Advances in Neural Information Processing Systems (NIPS) , volume 21, 2008.
J Andrew Bagnell, Andrew Y Ng, and Jeff G Schneider. Solving uncertain Markov decision processes. 09
2001.
Amir Beck. First-order methods in optimization . SIAM, 2017.
Dimitri P Bertsekas. Constrained optimization and Lagrange multiplier methods . Academic press, 2014.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540 , 2016.
Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunov-based
approach to safe reinforcement learning. In Proc. Advances in Neural Information Processing Systems
(NeurIPS) , volume 31, 2018.
Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. Safe
exploration in continuous action spaces. arXiv preprint arXiv:1801.08757 , 2018.
Dongsheng Ding, Kaiqing Zhang, Tamer Basar, and Mihailo Jovanovic. Natural policy gradient primal-dual
method for constrained Markov decision processes. In Proc. Advances in Neural Information Processing
Systems (NeurIPS) , volume 33, pp. 8378–8390, 2020.
Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo Jovanovic. Provably efficient safe
exploration via primal-dual policy optimization. In Proc. International Conference on Artifical Intelligence
and Statistics (AISTATS) , pp. 3304–3312. PMLR, 2021.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of
deep neural networks. In Proc. International Conference on Machine Learning (ICML) , pp. 1675–1685.
PMLR, 2019.
Simon S Du, Yining Wang, Sivaraman Balakrishnan, Pradeep Ravikumar, and Aarti Singh. Robust
nonparametric regression under Huber’s ϵ-contamination model. arXiv preprint arXiv:1805.10406 , 2018.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement
learning. arXiv preprint arXiv:1710.11248 , 2017.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic
programming. Mathematical Programming , 156(1-2):59–99, 2016.
Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Fast Bellman updates for robust MDPs. In Proc.
International Conference on Machine Learning (ICML) , pp. 1979–1988. PMLR, 2018.
Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Partial policy iteration for l1-robust Markov decision
processes. Journal of Machine Learning Research , 22(275):1–46, 2021.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural information
processing systems , 29, 2016.
P. J. Huber. A robust version of the probability ratio test. Ann. Math. Statist. , 36:1753–1758, 1965.
PJ Huber and EM Ronchetti. Robust Statistics . John Wiley & Sons, Inc, 2009.
Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research , 30(2):257–280, 2005.
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Softmax policy gradient methods can take
exponential time to converge. arXiv preprint arXiv:2102.11270 , 2021a.
11Under review as submission to TMLR
Tianjiao Li, Ziwei Guan, Shaofeng Zou, Tengyu Xu, Yingbin Liang, and Guanghui Lan. Faster algorithm
and sharper analysis for constrained Markov decision process. arXiv preprint arXiv:2110.10351 , 2021b.
Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax
problems. In Proc. International Conference on Machine Learning (ICML) , pp. 6083–6093. PMLR, 2020.
Tao Liu, Ruida Zhou, Dileep Kalathil, PR Kumar, and Chao Tian. Fast global convergence of policy
optimization for constrained MDPs. arXiv preprint arXiv:2111.00552 , 2021.
Yongshuai Liu, Jiaxin Ding, and Xin Liu. Ipo: Interior-point policy optimization under constraints. In Proc.
Conference on Artificial Intelligence (AAAI) , volume 34, pp. 4940–4947, 2020.
Daniel J Mankowitz, Dan A Calian, Rae Jeong, Cosmin Paduraru, Nicolas Heess, Sumanth Dathathri, Martin
Riedmiller, and Timothy Mann. Robust constrained reinforcement learning for continuous control with
model misspecification. arXiv preprint arXiv:2010.10644 , 2020.
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of
softmax policy gradient methods. In Proc. International Conference on Machine Learning (ICML) , pp.
6820–6829. PMLR, 2020.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative
adversarial networks. In Proc. International Conference on Learning Representations (ICLR) , 2018.
Behnam Neyshabur. Implicit regularization in deep learning. arXiv preprint arXiv:1709.01953 , 2017.
Arnab Nilim and Laurent El Ghaoui. Robustness in Markov decision problems with uncertain transition
matrices. In Proc. Advances in Neural Information Processing Systems (NIPS) , pp. 839–846, 2004.
Kiyohiko G Nishimura and Hiroyuki Ozaki. Search and knightian uncertainty. Journal of Economic Theory ,
119(2):299–333, 2004.
Kiyohiko G. Nishimura and Hiroyuki Ozaki. An axiomatic approach to ϵ-contamination. Economic Theory ,
27(2):333–340, 2006.
Kishan Panaganti and Dileep Kalathil. Sample complexity of robust reinforcement learning with a generative
model.arXiv preprint arXiv:2112.01506 , 2021.
Santiago Paternain, Luiz Chamon, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained reinforcement
learning has zero duality gap. In Proc. Advances in Neural Information Processing Systems (NeurIPS) ,
volume 32, 2019.
Santiago Paternain, Miguel Calvo-Fullana, Luiz FO Chamon, and Alejandro Ribeiro. Safe policies for
reinforcement learning via primal-dual methods. IEEE Transactions on Automatic Control , 2022.
Adarsh Prasad, Vishwak Srinivasan, Sivaraman Balakrishnan, and Pradeep Ravikumar. On learning ising
models under Huber’s contamination model. Proc. Advances in Neural Information Processing Systems
(NeurIPS) , 33, 2020a.
Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust estimation via
robust gradient estimation. Journal of the Royal Statistical Society: Series B (Statistical Methodology) , 82
(3):601–627, 2020b.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley &
Sons, 2014.
Aurko Roy, Huan Xu, and Sebastian Pokutta. Reinforcement learning under model mismatch. In Proc.
Advances in Neural Information Processing Systems (NIPS) , pp. 3046–3055, 2017.
Reazul Hasan Russel, Mouhacine Benosman, and Jeroen Van Baar. Robust constrained-MDPs: Soft-
constrained robust policy optimization under model uncertainty. arXiv preprint arXiv:2010.04870 , 2020.
12Under review as submission to TMLR
Reazul Hasan Russel, Mouhacine Benosman, Jeroen Van Baar, and Radu Corcodel. Lyapunov robust
constrained-MDPs: Soft-constrained robustly stable policy optimization under model uncertainty. arXiv
preprint arXiv:2108.02701 , 2021.
Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy gradient methods for
reinforcement learning with function approximation. In Proc. Advances in Neural Information Processing
Systems (NIPS) , volume 99, pp. 1057–1063. Citeseer, 1999.
Faraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observation. arXiv
preprint arXiv:1807.06158 , 2018.
Luca Viano, Yu-Ting Huang, Parameswaran Kamalaruban, Craig Innes, Subramanian Ramamoorthy, and
Adrian Weller. Robust learning from observation with model misspecification. In Proceedings of the 21st
International Conference on Autonomous Agents and Multiagent Systems , pp. 1337–1345, 2022.
Yue Wang and Shaofeng Zou. Finite-sample analysis of Greedy-GQ with linear function approximation under
Markovian noise. In Proc. International Conference on Uncertainty in Artificial Intelligence (UAI) , pp.
11–20. PMLR, 2020.
Yue Wang and Shaofeng Zou. Online robust reinforcement learning with model uncertainty. In Proc. Advances
in Neural Information Processing Systems (NeurIPS) , 2021.
Yue Wang and Shaofeng Zou. Policy gradient method for robust reinforcement learning. In Proc. International
Conference on Machine Learning (ICML) , 2022.
Tengyu Xu, Yingbin Liang, and Guanghui Lan. Crpo: A new approach for safe reinforcement learning with
convergence guarantee. In Proc. International Conference on Machine Learning (ICML) , pp. 11480–11491.
PMLR, 2021.
Zi Xu, Huiling Zhang, Yang Xu, and Guanghui Lan. A unified single-loop alternating gradient projection algo-
rithm for nonconvex-concave and convex-nonconcave minimax problems. arXiv preprint arXiv:2006.02032 ,
2023.
Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. Projection-based constrained
policy optimization. arXiv preprint arXiv:2010.03152 , 2020.
Wenhao Yang, Liangyu Zhang, and Zhihua Zhang. Towards theoretical understandings of robust Markov
decision processes: Sample complexity and asymptotics. arXiv preprint arXiv:2105.03863 , 2021.
Donghao Ying, Yuhao Ding, and Javad Lavaei. A dual approach to constrained Markov decision processes
with entropy regularization. arXiv preprint arXiv:2110.08923 , 2021.
Zhengqing Zhou, Qinxun Bai, Zhengyuan Zhou, Linhai Qiu, Jose Blanchet, and Peter Glynn. Finite-sample
regret bound for distributionally robust offline tabular reinforcement learning. In Proc. International
Conference on Artifical Intelligence and Statistics (AISTATS) , pp. 3331–3339. PMLR, 2021.
13Under review as submission to TMLR
Appendix
A Additional Experiments
4×4Frozen Lake problem. The 4×4frozen lake is similar to the 8×8one but with a smaller map.
Similarly, we randomly generate the utility signal for each state-action pair. The results are shown in Fig.2.
(a)Vcwhenδ= 0.2.
 (b)Vrwhenδ= 0.2.
 (c)Vcwhenδ= 0.3.
 (d)Vrwhenδ= 0.3.
Figure 4: Comparison on 8×8Frozen-Lake problem.
N-Chain problem. We then compare three algorithms under the N-Chain problem environment. The
N-chain problem involves a chain contains Nnodes. The agent can either move to its left or right node.
When it goes to left, it receives a reward-utility signal (1,0); When it goes right, it receives a reward-utility
signal (0,2), and if the agent arrives the N-th node, it receives a bonus reward of 40. There is also a small
probability that the agent slips to the different direction of its action. In this experiment, we set N= 40.
The results are shown in Fig.5.
(a)Vcwhenδ= 0.2.
 (b)Vrwhenδ= 0.2.
 (c)Vcwhenδ= 0.3.
 (d)Vrwhenδ= 0.3.
Figure 5: Comparison on N-Chain problem.
B Proof of Lemma 1
Denote by Pπ={(pπ)a
s∈∆S:s∈S,a∈A}the worst-case transition kernel corresponding to the policy
π. We consider the δ-contamination uncertainty set defined in Section 4. We then show that under δ-
contamination model, the set of visitation distributions is non-convex. The robust visitation distribution set
can be written as follows:


d∈∆S×A:∃π∈Π,s.t.∀(s,a),

d(s,a) =π(a|s)/summationdisplay
bd(s,b),
γ/summationdisplay
s′,a′(pπ)a′
s′,sd(s′,a′) + (1−γ)ρ(s) =/summationdisplay
ad(s,a).



.(18)
14Under review as submission to TMLR
Under theδ-contamination model, Pπcan be explicated as (pπ)a
s,s′= (1−δ)pa
s,s′+δ 1{s′=arg minVπ}. Hence
the set in eq. (18) can be rewritten as


d∈∆S×A:∃π,s.t.∀(s,a),

d(s,a) =π(a|s)/parenleftigg/summationdisplay
bd(s,b)/parenrightigg
,
γ(1−δ)/summationdisplay
s′,a′pa′
s′,sd(s′,a′) +γδ 1{s=arg minVπ}
+ (1−γ)ρ(s) =/summationdisplay
ad(s,a).



. (19)
Now consider any two pairs (π1,d1),(π2,d2)of policy and their worst-case visitation distribution, to show
that the set is convex, we need to find a pair (π′,d′)such that∀λ∈[0,1]and∀s,a,
λd1(s,a) + (1−λ)d2(s,a) =d′(s,a), (20)
d′(s,a) =π′(a|s)/parenleftigg/summationdisplay
bd′(s,b)/parenrightigg
, (21)
/summationdisplay
a′d′(s,a′) =γ(1−δ)/summationdisplay
s′,a′pa′
s′,sd′(s′,a′) +γδ 1{s=arg minVπ′}+ (1−γ)ρ(s). (22)
eq. (22) firstly implies that ∀s,
λ 1{s=arg minVπ1}+ (1−λ) 1{s=arg minVπ2}= 1{s=arg minVπ′}, (23)
where from eq. (20) and eq. (21), π′should be
π′(a|s) =d′(s,a)/summationtext
bd′(s,b)=λd1(s,a) + (1−λ)d2(s,a)/summationtext
b(λd1(s,b) + (1−λ)d2(s,b)). (24)
We then construct the following counterexample, which shows that there exists a robust MDP, two policy-
distribution pairs (π1,d1),(π2,d2), andλ∈(0,1), such that λ 1{s=arg minVπ1}+ (1−λ) 1{s=arg minVπ2}̸=
1{s=arg minVπ′}, and therefore the set of robust visitation distribution is non-convex.
Consider the following Robust MDP. It has three states 1,2,3and two actions a,b. When the agent is at
state 1, if it takes action a, the system will transit to state 2and receive reward r= 0; if it takes action b,
the system will transit to state 3and receive reward r= 2. When the agent is at state 2/3, it can only take
actiona/b, the system can only transits back to state 1and the agent will receive reward r= 1. The initial
distribution is 1s=1.
1
32action=a r = 0
action=b r = 2action=a r = 1
action=b r = 1
Clearly all policy can be written as π= (p,1−p), wherepis the probability of taking action aat state 1.
We consider two policies, π1= (1,0)andπ2= (0,1).
15Under review as submission to TMLR
It can be verified that arg minVπ1= 1, and its robust visitation distribution, denoted by d1, is
d1(1,a) =1−γ
1−γ2, (25)
d1(1,b) = 0, (26)
d1(2,a) =γ(1−γ)
1−γ2, (27)
d1(2,b) = 0, (28)
d1(3,a) = 0, (29)
d1(3,b) = 0. (30)
Similarly, arg minVπ2= 2, and and its robust visitation distribution, denoted by d2, is
d2(1,a) = 0, (31)
d2(1,b) =1−γ
1−γ2, (32)
d2(2,a) = 0, (33)
d2(2,b) = 0, (34)
d2(3,a) = 0, (35)
d2(3,b) =γ(1−γ)
1−γ2. (36)
Hence according to eq. (24), π′should be as follows:
π′(a|1) =λ,π′(b|1) = 1−λ,π′(a|2) = 1,π′(b|3) = 1. (37)
We then show that there exists λ∈[0,1], such that λ 1{s=1}+ (1−λ) 1{s=2}̸= 1{arg minVπ′}.
Clearly eq. (23) holds only if Vπ′(1) =Vπ′(2) = minsVπ′(s). However, according to the Bellman equations
forπ′, we have that
Vπ′(1) =λ(γ(1−δ)Vπ′(2) +γδminVπ′) + (1−λ)(2 +γ(1−δ)Vπ′(3) +γδminVπ′),(38)
Vπ′(2) = 1 +γ(1−δ)Vπ′(1) +γδminVπ′, (39)
Vπ′(3) = 1 +γ(1−δ)Vπ′(1) +γδminVπ′. (40)
If we setλ=1
3,
Vπ′(1) =4
3+γδminVπ′+γ(1−δ)Vπ′(2), (41)
Vπ′(2) = 1 +γδminVπ′+γ(1−δ)Vπ′(1). (42)
Clearly,Vπ′(1)̸=Vπ′(2), and hence λ 1{arg minV1}+ (1−λ) 1{arg minV2}̸= 1{arg minVπ′}.
C Proof of Lemmas 2 and 5
Proof of Lemma 2
Proof.We first set C=Vπθ∗
r(ρ) +λ∗(Vπθ∗
c(ρ)−b), clearly maxπ∈ΠVπ
r(ρ) +λ∗(Vπ
c(ρ)−b) =C, and hence
C= max
π∈ΠVπ
r(ρ) +λ∗(Vπ
c(ρ)−b)≥Vπζ
r(ρ) +λ∗(Vπζ
c(ρ)−b)≥Vπζ
r(ρ) +λ∗ζ. (43)
16Under review as submission to TMLR
Thus we have that
λ∗≤C−Vπζ
r(ρ)
ζ. (44)
Note that
C= min
λ≥0max
π∈ΠVπ
r(ρ) +λ(Vπ
c(ρ)−b)(a)
≤max
π∈ΠVπ
r(ρ)≤1
1−γ, (45)
where (a)is because minλ≥0maxπ∈ΠVπ
r(ρ) +λ(Vπ
c(ρ)−b)is less than the optimal value of inner problem
whenλ= 0, i.e., maxπ∈ΠVπ
r(ρ), and1
1−γis the upper bound of robust value functions. Hence we have that
λ∗≤1
(1−γ)ζ, (46)
which completes the proof.
Proof of Lemma 5
Proof.SetC=Vθ∗
σ,r(ρ) +λ∗(Vθ∗
σ,c(ρ)−b), then
C= max
π∈ΠVπ
σ,r(ρ) +λ∗(Vπ
σ,c(ρ)−b)≥Vπζ′
σ,r(ρ) +λ∗(Vπζ′
σ,c(ρ)−b)≥Vπζ′
σ,r(ρ) +λ∗ζ′. (47)
Thus we have that
C≥Vπζ
σ,r(ρ) +λ∗ζ′, (48)
hence
λ∗≤C−Vπζ
σ,r(ρ)
ζ′. (49)
Note that
C= min
λ≥0max
π∈ΠVπ
σ,r(ρ) +λ(Vπ
σ,c(ρ)−b)≤max
π∈ΠVπ
σ,r(ρ)≤Cσ, (50)
whereCσistheupperboundof smoothedrobustvalue functions Wang &Zou(2022): Cσ=1
1−γ(1+2γRlog|S|
σ).
Hence we have that
λ∗≤Cσ
ζ′, (51)
which completes the proof.
D Proof of Lemma 6
Proof.For anyλ, denote the optimal value of the inner problems maxπ∈ΠΘVπ
σ,r(ρ) +λ(Vπ
σ,c(ρ)−b)and
maxπ∈ΠΘVπ
r(ρ) +λ(Vπ
c(ρ)−b)byVD(λ)andVD
σ(λ). It is then easy to verify that
|VD(λ)−VD
σ(λ)|≤(1 +λ)ϵ≤(1 +Λ∗)ϵ. (52)
Denote the optimal solutions of minλ∈[0,Λ∗]VD(λ)andminλ∈[0,Λ∗]VD
σ(λ)byλDandλD
σ. We thus conclude
that|VD
σ(λD
σ)−VD(λD)|≤(1 +Λ∗)ϵ, and this thus completes the proof.
17Under review as submission to TMLR
E Proof of Theorem 1
We restate Theorem 1 with all the specific step sizes as follows.
Setbt=19
20ξt0.25,µt=ξL2
21+16τL2
21
ξ(bt+1)2−2ν, βt=1
ξ,αt=ν+µt, whereξ <1
10L11,νis any positive number
andτ >max/braceleftig
2,192(L22+2ν−ξ(L21)2)
202∗16ξL2
21/bracerightig
, then
min
1≤t≤T∥Gt∥2≤2ϵ, (53)
when
T= max/braceleftigg
7(Λ∗)4
ξ4ϵ4,/parenleftbigg
2 +9ξ(τ−2)(L21)2D2D3
ϵ2/parenrightbigg2/bracerightigg
=O(ϵ−4). (54)
The definitions of the constants can be found in Section J.
Theorem 1 can be proved similarly as Theorem 2, and hence the proof is omitted here.
F Proof of Corollary 1
Corollary 1 follows directly from Theorem 1 and Lemma 3. More specifically, note that
L11=CV
σ=O(1), (55)
L21= (1 +Λ∗)Lσ=O(Λ∗σ), (56)
L12= 0, (57)
L22=CV
σ=O(1). (58)
Hence the corollary can be proved by plugging these constants in Theorem 1. Note that Lσ=O(Λ∗σ),
D2=O(L2
21),D3=O((Λ∗)2), therefore it implies the sample complexity to achieve an ϵ-stationary solution
isO((Λ∗)8ϵ−12).
G Proof of Proposition 1
Proof.Theλ-entry ofGWis smaller than 2ϵ, i.e.,
|(GW)λ|=/vextendsingle/vextendsingle/vextendsingle/vextendsingleβW/parenleftbigg
λW−/productdisplay
[0,Λ∗]/parenleftbig
λW−1
βW/parenleftbig
∇λ˜VL(θW,λW)/parenrightbig/parenrightbig/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle<2ϵ. (59)
Denoteλ+≜/producttext
[0,Λ∗]/parenleftig
λW−1
βW/parenleftbig
∇λ˜VL(θW,λW)/parenrightbig/parenrightig
. From Lemma 3 in Ghadimi & Lan (2016),
−∇λ˜VL(θW,λ+)can be rewritten as the sum of two parts: −∇λ˜VL(θW,λ+)∈N[0,Λ∗](λ+) + 4ϵB, where
NK(x)≜/braceleftbig
g∈Rd:⟨g,y−x⟩≤0 :∀y∈K/bracerightbig
is the normal cone, and Bis the unit ball.
This hence implies that for any λ∈[0,Λ∗],(λ−λ+)(VW
c−b)≥−4(λ−λ+)ϵ. By setting λ=Λ∗, we have
VW
c+ 4ϵ≥b, which means πWis feasible with a 4ϵ-violation.
H Proof of Lemma 3
Proof.Recall that VL
σ(θ,λ) =Vπθσ,r(ρ) +λ(Vπθσ,c(ρ)−b), hence we have that
∇λVL
σ(θ,λ) =Vπθσ,c(ρ)−b, (60)
∇θVL
σ(θ,λ) =∇θVπθσ,r(ρ) +λ∇θVπθσ,c(ρ). (61)
18Under review as submission to TMLR
Note that in Wang & Zou (2022), it has been shown that
∥Vπθ1σ,r−Vπθ2σ,r∥≤CV
σ∥θ1−θ2∥, (62)
∥∇θVπθ1σ,r−∇θVπθ2σ,r∥≤Lσ∥θ1−θ2∥, (63)
where the definition of constants CV
σandLσcan be found in Section J. Hence
∥∇λVL
σ(θ,λ)|θ1−∇λVL
σ(θ,λ)|θ2∥=∥Vπθ1σ,c(ρ)−Vπθ2σ,c(ρ)∥≤CV
σ∥θ1−θ2∥, (64)
∥∇λVL
σ(θ,λ)|λ1−∇λVL
σ(θ,λ)|λ2∥= 0. (65)
Similarly, we have that
∥∇θVL
σ(θ,λ)|θ1−∇θVL
σ(θ,λ)|θ2∥≤(1 +λ)Lσ∥θ1−θ2∥≤(1 +Λ∗)Lσ∥θ1−θ2∥, (66)
∥∇θVL
σ(θ,λ)|λ1−∇θVL
σ(θ,λ)|λ2∥≤| (λ1−λ2)|max
θ∈Θ∥∇θVπθσ,c(ρ)∥≤CV
σ|λ1−λ2|. (67)
This completes the proof.
I Proof of Theorem 2
We then prove Theorem 2. Our proof extends the one in Xu et al. (2023) to the biased setting.
To simplify notations, we denote the updates in Algorithm 3 by ˆf(θt)≜ˆVπθtσ,c(ρ)−b, and ˆg(θt,λt+1)≜
∇θˆVπθtσ,r(ρ) +λt+1∇θˆVπθtσ,c(ρ), and denote the update functions in Algorithm 1 by f(θt)≜Vπθtσ,c(ρ)−b, and
g(θt,λt+1)≜∇θVπθtσ,r(ρ) +λt+1∇θVπθtσ,c(ρ). Here ˆfandˆgcan be viewed as biased estimations of fandg.
In the following, we will first show several technical lemmas that will be useful in the proof of Theorem 2.
Lemma 7. Recall that the step size αt=ν+µt. Ifµt>(1 +Λ∗)Lσ,∀t≥0, then
VL
σ(θt+1,λt+1)−VL
σ(θt,λt+1)≥⟨θt+1−θt,−ˆg(θt,λt+1) +g(θt,λt+1)⟩
+/parenleftigµt
2+ν/parenrightig
∥θt+1−θt∥2. (68)
Proof.Note that from the update of θtand proposition of projection, it implies that
/angbracketleftbigg
θt+1
αtˆg(θt,λt+1)−θt+1,θt−θt+1/angbracketrightbigg
≤0. (69)
Hence
⟨ˆg(θt,λt+1)−αt(θt+1−θt),θt−θt+1⟩≤0. (70)
From Lemma 3, we have that
VL
σ(θt+1,λt+1)−VL
σ(θt,λt+1)≥⟨θt+1−θt,g(θt,λt+1)⟩−(1 +Λ∗)Lσ
2∥θt+1−θt∥2. (71)
Summing up the two inequalities implies
VL
σ(θt+1,λt+1)−VL
σ(θt,λt+1)
≥⟨θt+1−θt,−ˆg(θt,λt+1) +g(θt,λt+1) +αt(θt+1−θt)⟩−(1 +Λ∗)Lσ
2∥θt+1−θt∥2
≥⟨θt+1−θt,−ˆg(θt,λt+1) +g(θt,λt+1)⟩+/parenleftbigg
αt−Lσ(1 +Λ∗)
2/parenrightbigg
∥θt+1−θt∥2
≥⟨θt+1−θt,−ˆg(θt,λt+1) +g(θt,λt+1)⟩+/parenleftigµt
2+ν/parenrightig
∥θt+1−θt∥2, (72)
and hence completes the proof.
19Under review as submission to TMLR
Lemma 8. Recall that the step size βt=1
ξ, and setξ≤1
b0, then
VL
σ(θt+1,λt+1)−VL
σ(θt,λt)
≥(f(θt−1)−ˆf(θt−1))(λt+1−λt) +⟨θt+1−θt,−ˆg(θt,λt+1) +g(θt,λt+1)⟩−ξ(CV
σ)2
2∥θt−θt−1∥2
+/parenleftigµt
2+ν/parenrightig
∥θt+1−θt∥2+bt−1
2(λ2
t−λ2
t+1)−1
ξ(λt+1−λt)2−1
2ξ(λt−λt−1)2. (73)
Proof.For anyt>1, define ˜Vt(θ,λ)≜VL
σ(θ,λ) +bt−1
2λ2. Thus we have
|∇λ˜Vt(θt,λt+1)−∇λ˜Vt(θt,λt)|=bt−1|λt+1−λt|≤b0|λt+1−λt|, (74)
where that last inequality is due to bt−1≤b0. Note that ˜Vt(θ,λ)isbt−1-strongly convex in λ, hence we have
(∇λ˜Vt(θ,λt+1)−∇λ˜Vt(θ,λt))(λt+1−λt)
≥bt−1(λt+1−λt)2
≥bt−1/parenleftbiggbt−1+b0
bt−1+b0/parenrightbigg
(λt+1−λt)2
=bt−1b0
bt−1+b0(λt+1−λt)2+b2
t−1
bt−1+b0(λt+1−λt)2
≥bt−1b0
bt−1+b0(λt+1−λt)2+1
bt−1+b0(∇λ˜Vt(θt,λt+1)−∇λ˜Vt(θt,λt))2, (75)
where the last inequality is from eq. (74).
Recall the update of λtin Algorithm 3 which can be rewritten as
λt+1=/productdisplay
[0,Λ∗]/parenleftbigg
λt−1
βt∇λ˜Vt+1(θt,λt) +1
βt(f(θt)−ˆf(θt))/parenrightbigg
, (76)
This further implies that ∀λ∈[0,Λ∗]:
(βt(λt+1−λt) +∇λ˜Vt+1(θt,λt)−f(θt) +ˆf(θt))(λ−λt+1)≥0. (77)
Hence setting λ=λkimplies that
(βt(λt+1−λt) +∇λ˜Vt+1(θt,λt)−f(θt) +ˆf(θt))(λt−λt+1)≥0. (78)
Similarly, we have that
(βt(λt−λt−1) +∇λ˜Vt(θt−1,λt−1)−f(θt−1) +ˆf(θt−1))(λt+1−λt)≥0. (79)
Note that ˜Vtis convex, we hence have that
˜Vt(θt,λt+1)−˜Vt(θt,λt)
≥(∇λ˜Vt(θt,λt))(λt+1−λt)
= (∇λ˜Vt(θt,λt)−∇λ˜Vt(θt−1,λt−1))(λt+1−λt) + (∇λ˜Vt(θt−1,λt−1))(λt+1−λt)
(a)
≥(∇λ˜Vt(θt,λt)−∇λ˜Vt(θt−1)(λt−1),λt+1−λt)
+ (f(θt−1)−ˆf(θt−1)−βt(λt−λt−1))(λt+1−λt), (80)
where (a)is from eq. (79). The first term in the RHS of eq. (80) can be further bounded as follows.
(∇λ˜Vt(θt,λt)−∇λ˜Vt(θt−1,λt−1))(λt+1−λt)
20Under review as submission to TMLR
= (∇λ˜Vt(θt,λt)−∇λ˜Vt(θt−1,λt))(λt+1−λt)
+ (∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))(λt+1−λt)
= (∇λ˜Vt(θt,λt)−∇λ˜Vt(θt−1,λt))(λt+1−λt)
+ (∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))(λt−λt−1)
+mt+1(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1)), (81)
wheremt+1≜(λt+1−λt)−(λt−λt−1). Plug it in eq. (80) and we have that
˜Vt(θt,λt+1)−˜Vt(θt,λt)
≥(f(θt−1)−ˆf(θt−1)−βt(λt−λt−1))(λt+1−λt)
+ (∇λ˜Vt(θt,λt)−∇λ˜Vt(θt−1,λt))(λt+1−λt)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(a)
+ (∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))(λt−λt−1)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(b)
+ (∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))mt+1/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(c). (82)
We then provide bounds for each term in eq. (82) as follows.
Term (a)can be bounded as follows:
(∇λ˜Vt(θt,λt)−∇λ˜Vt(θt−1,λt))(λt+1−λt)
= (∇λVL
σ(θt,λt)−∇λVL
σ(θt−1,λt))(λt+1−λt)
≥−(λt+1−λt)2
2ξ−ξ
2(∇λVL
σ(θt,λt)−∇λVL
σ(θt−1,λt))2
≥−(λt+1−λt)2
2ξ−ξ(CV
σ)2
2∥θt−θt−1∥2, (83)
which is from Cauchy–Schwarz inequality and CV
σ-smoothness of VL
σ(θ,λ).
Term (b)can be bounded as follows:
(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))(λt−λt−1)
≥1
bt−1+b0(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))2, (84)
which is from eq. (75).
Term (c)can be bounded as follows by Cauchy–Schwarz inequality:
mt+1(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))
≥−ξ
2(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))2−1
2ξm2
t+1 (85)
Moreover, it can be shown that
1
ξ(λt+1−λt)(λt−λt−1) =1
2ξ(λt+1−λt)2+1
2ξ(λt−λt−1)2−1
2ξm2
t+1. (86)
Plug eq. (83) to eq. (86) in 82, and we have that
˜Vt(θt,λt+1)−˜Vt(θt,λt)
21Under review as submission to TMLR
≥(f(θt−1)−ˆf(θt−1))(λt+1−λt)−βt(λt−λt−1)(λt+1−λt)
+ (∇λ˜Vt(θt,λt)−∇λ˜Vt(θt−1,λt))(λt+1−λt) + (∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))(λt−λt−1)
+mt+1(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))
≥(f(θt−1)−ˆf(θt−1))(λt+1−λt)−1
2ξ(λt+1−λt)2−1
2ξ(λt−λt−1)2+1
2ξm2
t+1
−(λt+1−λt)2
2ξ−ξ(CV
σ)2
2∥θt−θt−1∥2+1
bt−1+b0(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))2
−ξ
2(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))2−1
2ξm2
t+1
≥(f(θt−1)−ˆf(θt−1))(λt+1−λt)−1
ξ(λt+1−λt)2−1
2ξ(λt−λt−1)2−ξ(CV
σ)2
2∥θt−θt−1∥2.(87)
From the definition of ˜Vt, we have that
˜Vt(θt,λt+1)−˜Vt(θt,λt)
=VL
σ(θt,λt+1) +bt−1
2λ2
t+1−VL
σ(θt,λt)−bt−1
2λ2
t. (88)
Then we have that
VL
σ(θt,λt+1)−VL
σ(θt,λt)
≥bt−1
2(λ2
t−λ2
t+1) + (f(θt−1)−ˆf(θt−1))(λt+1−λt)
−1
ξ(λt+1−λt)2−1
2ξ(λt−λt−1)2−ξ(CV
σ)2
2∥θt−θt−1∥2. (89)
Combining with Lemma 7, if ∀t,µt>(1 +Λ∗)Lσ, we then have that
VL
σ(θt+1,λt+1)−VL
σ(θt,λt)
≥(f(θt−1)−ˆf(θt−1))(λt+1−λt) +⟨θt+1−θt,−ˆg(θt,λt+1) +g(θt,λt+1)⟩−ξ(CV
σ)2
2∥θt−θt−1∥2
+/parenleftigµt
2+ν/parenrightig
∥θt+1−θt∥2+bt−1
2(λ2
t−λ2
t+1)−1
ξ(λt+1−λt)2−1
2ξ(λt−λt−1)2. (90)
Lemma 9. Define
Ft+1≜−8
ξ2bt+1(λt−λt+1)2−8
ξ/parenleftbigg
1−bt
bt+1/parenrightbigg
λ2
t+1+VL
σ(θt+1,λt+1) +bt
2λ2
t+1
+/parenleftbigg
−16(CV
σ)2
ξb2
t+1−ξ(CV
σ)2
2/parenrightbigg
∥θt+1−θt∥2+/parenleftbigg8
ξ−1
2ξ/parenrightbigg
(λt+1−λt)2, (91)
and if1
bt+1−1
bt≤ξ
5, then
Ft+1−Ft
≥St+/parenleftbiggµt
2+ν−16(CV
σ)2
ξb2
t+1−ξ(CV
σ)2
2/parenrightbigg
∥θt+1−θt∥2+bt−bt−1
2λ2
t+1
+9
10ξ(λt+1−λt)2+8
ξ/parenleftbiggbt
bt+1−bt−1
bt/parenrightbigg
λ2
t+1, (92)
whereSt≜16
btξ(f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1) + (f(θt−1)−ˆf(θt−1))(λt+1−λt) +⟨θt+1−
θt,−ˆg(θt,λt+1) +g(θt,λt+1)⟩.
22Under review as submission to TMLR
Proof.From eq. (78) and eq. (79), we have that
βtmt+1(λt−λt+1)≥(∇λ˜Vt+1(θt,λt)−∇λ˜Vt(θt−1,λt−1))(−λt+λt+1)
+ (f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1). (93)
The first term can be rewritten as
(∇λ˜Vt+1(θt,λt)−∇λ˜Vt(θt−1,λt−1))(λt+1−λt)
= (∇λ˜Vt+1(θt,λt)−∇λ˜Vt(θt−1,λt))(λt+1−λt) + (∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))(λt+1−λt)
= (∇λ˜Vt+1(θt,λt)−∇λ˜Vt(θt−1,λt))(λt+1−λt) + (∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))(λt−λt−1)
+mt+1(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1)). (94)
The first term in eq. (94) can be bounded as
(∇λ˜Vt+1(θt,λt)−∇λ˜Vt(θt−1,λt))(λt+1−λt)
= (∇λVL
σ(θt,λt)−∇λVL
σ(θt−1,λt))(λt+1−λt) + (btλt−bt−1λt)(λt+1−λt)
(a)
≥−1
2h(∇λVL
σ(θt,λt)−∇λVL
σ(θt−1,λt))2−h
2(λt+1−λt)2
+(bt−bt−1)
2(λ2
t+1−λ2
t)−(bt−bt−1)
2(λt+1−λt)2
(b)
≥−(CV
σ)2
2h∥θt−θt−1∥2−h
2(λt+1−λt)2
+(bt−bt−1)
2(λ2
t+1−λ2
t)−(bt−bt−1)
2(λt+1−λt)2, (95)
where (a)is from the Cauchy–Schwarz inequality and (b)is from the CV
σ-smoothness of VL
σ, for anyh>0.
Similar to eq. (75), the second term in eq. (94) can be bounded as
(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))(λt−λt−1)
≥bt−1b0
bt−1+b0(λt−λt−1)2+1
bt−1+b0(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))2. (96)
The third term in eq. (94) can be bounded as
mt+1(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))
≥−ξ
2(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))2−1
2ξm2
t+1. (97)
Hence combine eq. (95) to eq. (96) and plug in eq. (94), we have that
(∇λ˜Vt+1(θt,λt)−∇λ˜Vt(θt−1,λt−1))(λt+1−λt)
≥−(CV
σ)2
2h∥θt−θt−1∥2−h
2(λt+1−λt)2
+(bt−bt−1)
2(λ2
t+1−λ2
t)−(bt−bt−1)
2(λt+1−λt)2
+bt−1b0
bt−1+b0(λt−λt−1)2+1
bt−1+b0(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))2
−ξ
2(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))2−1
2ξm2
t+1. (98)
Hence eq. (93) can be further bounded as
(βtmt+1)(λt−λt+1)
23Under review as submission to TMLR
≥(f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1)
−(CV
σ)2
2h∥θt−θt−1∥2−h
2(λt+1−λt)2
+(bt−bt−1)
2(λ2
t+1−λ2
t)−(bt−bt−1)
2(λt+1−λt)2
+bt−1b0
bt−1+b0(λt−λt−1)2+1
bt−1+b0(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))2
−ξ
2(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))2−1
2ξm2
t+1. (99)
It can be directly verified that
mt+1(λt−λt+1) =1
2(λt−λt−1)2−1
2(λt−λt+1)2−m2
t+1
2. (100)
Recall that βt=1
ξ, hence
1
2ξ(λt−λt−1)2−1
2ξ(λt−λt+1)2−m2
t+1
2ξ
≥(f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1)
−(CV
σ)2
2h∥θt−θt−1∥2−h
2(λt+1−λt)2
+(bt−bt−1)
2(λ2
t+1−λ2
t)−(bt−bt−1)
2(λt+1−λt)2
+bt−1b0
bt−1+b0(λt−λt−1)2+1
bt−1+b0(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))2
−ξ
2(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))2−1
2ξm2
t+1. (101)
Fromξ≤1
b0≤2
b0+bt−1, we have1
bt−1+b0(∇λ˜Vt(θt−1,λt)−∇λ˜Vt(θt−1,λt−1))2−ξ
2(∇λ˜Vt(θt−1,λt)−
∇λ˜Vt(θt−1,λt−1))2≥0. Also, it can be shown thatbt−1b0
bt−1+b0≥bt−1b0
2b0=bt−1
2. Thus, it follows that
1
2ξ(λt−λt−1)2−1
2ξ(λt−λt+1)2−m2
t+1
2ξ
≥(f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1)
−(CV
σ)2
2h∥θt−θt−1∥2−h
2(λt+1−λt)2
+(bt−bt−1)
2(λ2
t+1−λ2
t)−(bt−bt−1)
2(λt+1−λt)2
+bt−1
2(λt−λt−1)2−1
2ξm2
t+1. (102)
Re-arrange the terms, it follows that
−1
2ξ(λt−λt+1)2−bt−bt−1
2λ2
t+1
≥(f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1)−(CV
σ)2
2h∥θt−θt−1∥2−h
2(λt+1−λt)2
−(bt−bt−1)
2λ2
t−(bt−bt−1)
2(λt+1−λt)2+bt−1
2(λt−λt−1)2−1
2ξ(λt−λt−1)2
≥−1
2ξ(λt−λt−1)2−(bt−bt−1)
2λ2
t+ (f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1)
24Under review as submission to TMLR
−(CV
σ)2
2h∥θt−θt−1∥2−h
2(λt+1−λt)2+bt−1
2(λt−λt−1)2, (103)
where the last inequality is from the fact that btis decreasing.
Now multiply2
ξbton both sides, we further have that
−1
ξ2bt(λt−λt+1)2−1
ξ/parenleftbigg
1−bt−1
bt/parenrightbigg
λ2
t+1
≥−1
ξ2bt(λt−λt−1)2−1
ξ/parenleftbigg
1−bt−1
bt/parenrightbigg
λ2
t+2
ξbt(f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1)
−(CV
σ)2
hξbt∥θt−θt−1∥2−h
ξbt(λt+1−λt)2+1
ξ(λt−λt−1)2. (104)
If we seth=bt
2, eq. (104) can be rewritten as
−1
ξ2bt(λt−λt+1)2−1
ξ/parenleftbigg
1−bt−1
bt/parenrightbigg
λ2
t+1
≥−1
ξ2bt(λt−λt−1)2−1
ξ/parenleftbigg
1−bt−1
bt/parenrightbigg
λ2
t+2
ξbt(f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1)
−2(CV
σ)2
ξb2
t∥θt−θt−1∥2−1
2ξ(λt+1−λt)2+1
ξ(λt−λt−1)2. (105)
Further we have that
−1
ξ2bt+1(λt−λt+1)2+/parenleftbigg1
ξ2bt+1−1
ξ2bt/parenrightbigg
(λt−λt+1)2−1
ξ/parenleftbigg
1−bt
bt+1/parenrightbigg
λ2
t+1+1
ξ/parenleftbiggbt−1
bt−bt
bt+1/parenrightbigg
λ2
t+1
≥−1
ξ2bt(λt−λt−1)2−1
ξ/parenleftbigg
1−bt−1
bt/parenrightbigg
λ2
t+2
ξbt(f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1)
−2(CV
σ)2
ξb2
t∥θt−θt−1∥2−1
2ξ(λt+1−λt)2+1
ξ(λt−λt−1)2. (106)
Re-arranging the terms in eq. (106) implies that
−1
ξ2bt+1(λt−λt+1)2−1
ξ/parenleftbigg
1−bt
bt+1/parenrightbigg
λ2
t+1−/parenleftbigg
−1
ξ2bt(λt−λt−1)2−1
ξ/parenleftbigg
1−bt−1
bt/parenrightbigg
λ2
t/parenrightbigg
≥−/parenleftbigg1
ξ2bt+1−1
ξ2bt/parenrightbigg
(λt−λt+1)2−1
ξ/parenleftbiggbt−1
bt−bt
bt+1/parenrightbigg
λ2
t+1
−2(CV
σ)2
ξb2
t∥θt−θt−1∥2−1
2ξ(λt+1−λt)2+1
ξ(λt−λt−1)2
+2
ξbt(f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1)
≥−7
10ξ(−λt+λt+1)2−2(CV
σ)2
ξb2
t∥θt−θt−1∥2+1
ξ(λt−λt−1)2+1
ξ/parenleftbiggbt
bt+1−bt−1
bt/parenrightbigg
λ2
t+1
+2
ξbt(f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1), (107)
where the last inequality is from1
bt+1−1
bt≤ξ
5. Recall in Lemma 8, we showed that
VL
σ(θt+1,λt+1)−VL
σ(θt,λt)
≥(f(θt−1)−ˆf(θt−1))(λt+1−λt) +⟨θt+1−θt,−ˆg(θt,λt+1) +g(θt,λt+1)⟩−ξ(CV
σ)2
2∥θt−θt−1∥2
+/parenleftigµt
2+ν/parenrightig
∥θt+1−θt∥2+bt−1
2(λ2
t−λ2
t+1)−1
ξ(λt+1−λt)2−1
2ξ(λt−λt−1)2. (108)
25Under review as submission to TMLR
Combine both inequality together, and we further have that
−8
ξ2bt+1(λt−λt+1)2−8
ξ/parenleftbigg
1−bt
bt+1/parenrightbigg
λ2
t+1−/parenleftbigg
−8
ξ2bt(λt−λt−1)2−8
ξ/parenleftbigg
1−bt−1
bt/parenrightbigg
λ2
t/parenrightbigg
+VL
σ(θt+1,λt+1)−VL
σ(θt,λt)
≥−28
5ξ(−λt+λt+1)2−16(CV
σ)2
ξb2
t∥θt−θt−1∥2+8
ξ(λt−λt−1)2+8
ξ/parenleftbiggbt
bt+1−bt−1
bt/parenrightbigg
λ2
t+1
+16
btξ(f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1)
+ (f(θt−1)−ˆf(θt−1))(λt+1−λt) +⟨θt+1−θt,−ˆg(θt,λt+1) +g(θt,λt+1)⟩−ξ(CV
σ)2
2∥θt−θt−1∥2
+/parenleftigµt
2+ν/parenrightig
∥θt+1−θt∥2+bt−1
2(λ2
t−λ2
t+1)−1
ξ(λt+1−λt)2−1
2ξ(λt−λt−1)2
=St+/parenleftbigg
−16(CV
σ)2
ξb2
t−ξ(CV
σ)2
2/parenrightbigg
∥θt−θt−1∥2+/parenleftbigg
−28
5ξ−1
ξ/parenrightbigg
(−λt+λt+1)2+bt−1
2(λ2
t−λ2
t+1)
+/parenleftbigg8
ξ−1
2ξ/parenrightbigg
(λt−λt−1)2+8
ξ/parenleftbiggbt
bt+1−bt−1
bt/parenrightbigg
λ2
t+1+/parenleftigµt
2+ν/parenrightig
∥θt+1−θt∥2, (109)
whereSt≜16
btξ(f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1) + (f(θt−1)−ˆf(θt−1))(λt+1−λt) +⟨θt+1−
θt,−ˆg(θt,λt+1) +g(θt,λt+1)⟩. Now
−8
ξ2bt+1(λt−λt+1)2−8
ξ/parenleftbigg
1−bt
bt+1/parenrightbigg
λ2
t+1−/parenleftbigg
−8
ξ2bt(λt−λt−1)2−8
ξ/parenleftbigg
1−bt−1
bt/parenrightbigg
λ2
t/parenrightbigg
+VL
σ(θt+1,λt+1)−VL
σ(θt,λt) +bt
2λ2
t+1−bt−1
2λ2
t
+/parenleftbigg
−16(CV
σ)2
ξb2
t+1−ξ(CV
σ)2
2/parenrightbigg
∥θt+1−θt∥2−/parenleftbigg
−16(CV
σ)2
ξb2
t−ξ(CV
σ)2
2/parenrightbigg
∥θt−θt−1∥2
+/parenleftbigg8
ξ−1
2ξ/parenrightbigg
(λt+1−λt)2−/parenleftbigg8
ξ−1
2ξ/parenrightbigg
(λt−λt−1)2
≥St+/parenleftbiggµt
2+ν−16(CV
σ)2
ξb2
t+1−ξ(CV
σ)2
2/parenrightbigg
∥θt+1−θt∥2+bt−bt−1
2λ2
t+1
+/parenleftbigg8
ξ−1
2ξ−28
5ξ−1
ξ/parenrightbigg
(λt+1−λt)2+8
ξ/parenleftbiggbt
bt+1−bt−1
bt/parenrightbigg
λ2
t+1
=St+/parenleftbiggµt
2+ν−16(CV
σ)2
ξb2
t+1−ξ(CV
σ)2
2/parenrightbigg
∥θt+1−θt∥2+bt−bt−1
2λ2
t+1
+9
10ξ(λt+1−λt)2+8
ξ/parenleftbiggbt
bt+1−bt−1
bt/parenrightbigg
λ2
t+1, (110)
which then completes the proof.
We now restate Theorem 2 with all the specific step sizes. The definitions of these constants can also be
found in Section J.
Theorem 3. (Restatement of Theorem 2) Set bt=19
20ξt0.25,µt=ξ(CV
σ)2+16τ(CV
σ)2
ξ(bt+1)2−2ν,βt=1
ξ,αt=ν+µt,
whereξ >2ν+(1+Λ∗)Lσ
(CVσ)2,νis any positive number and τis any number greater than 2. Moreover, set
ϵest=1
t0.5LΩ1
32t0.25Λ∗+2Λ∗+1
α1(1+Λ∗)CVσ192ϵ2
3200ξ(τ−2)(CVσ)2uLΩ=O(ϵ2
σ2t0.75), then
min
1≤t≤T∥Gt∥2≤(1 +√
2)ϵ, (111)
whenT= max/braceleftbigg
7(Λ∗)4
ξ4ϵ4,/parenleftig
2 +9ξ(τ−2)(CV
σ)2uK
ϵ2/parenrightig2/bracerightbigg
=O(ϵ−12).
26Under review as submission to TMLR
Proof.Denote by pt≜8(τ−2)(CV
σ)2
ξb2
t+1andM1≜16τ2
(τ−2)2+(ξ(CV
σ)2−ν)2
64(τ−2)2(CVσ)2ξ2. Then it can be verified that
ν+µt
2−ξ(CV
σ)2
2−16(CV
σ)2
ξb2
t+1=pt. Then eq. (110) can be rewritten as
Ft+1−Ft≥St+pt∥θt+1−θt∥2+bt−bt−1
2λ2
t+1
+9
10ξ(λt+1−λt)2+8
ξ/parenleftbiggbt
bt+1−bt−1
bt/parenrightbigg
λ2
t+1. (112)
From the definition, we have that
Gt=
βt/parenleftig
λt−/producttext
[0,Λ∗]/parenleftig
λt−1
βt/parenleftbig
∇λVL
σ(θt,λt)/parenrightbig/parenrightig/parenrightig
αt/parenleftig
θt−/producttext
Θ/parenleftig
θt+1
αt/parenleftbig
∇θVL
σ(θt,λt)/parenrightbig/parenrightig/parenrightig
, (113)
and denote by
˜Gt≜
βt/parenleftig
λt−/producttext
[0,Λ∗]/parenleftig
λt−1
βt/parenleftbig
∇λ˜Vt(θt,λt)/parenrightbig/parenrightig/parenrightig
αt/parenleftig
θt−/producttext
Θ/parenleftig
θt+1
αt/parenleftbig
∇θ˜Vt(θt,λt)/parenrightbig/parenrightig/parenrightig
. (114)
It can be verified that
∥Gt∥−∥ ˜Gt∥≤bt−1|λt|. (115)
From Theorem 4.2 in Xu et al. (2023), it can be shown that
∥˜Gt∥2≤2(µt+ν)2∥θt+1−θt∥2+/parenleftbigg
2(CV
σ)2+1
ξ2/parenrightbigg
(λt+1−λt)2, (116)
and
M1≥2(ν+µt)2
p2
t. (117)
Hence
∥˜Gt∥2≤M1p2
t∥θt+1−θt∥2+/parenleftbigg
2(CV
σ)2+1
ξ2/parenrightbigg
(λt+1−λt)2. (118)
Setut≜1
max/braceleftig
M1pt,10+20ξ2(CVσ)2
9ξ/bracerightig, then from eq. (112), we have that
ut∥˜Gt∥2≤Ft+1−Ft−St−bt−bt−1
2λ2
t+1−8
ξ/parenleftbiggbt
bt+1−bt−1
bt/parenrightbigg
λ2
t+1. (119)
Summing the inequality above from t= 1toT, then
T/summationdisplay
t=1ut∥˜Gt∥2≤FT+1−F1−T/summationdisplay
t=1St+8
ξ/parenleftbiggb0
b1λ2
2−bT
bT+1λ2
T+1/parenrightbigg
+/parenleftbiggb0−bT
2(Λ∗)2/parenrightbigg
≤FT+1−F1−T/summationdisplay
t=1St+8
ξb0
b1(Λ∗)2+/parenleftbiggb0−bT
2(Λ∗)2/parenrightbigg
, (120)
which is from btis decreasing and λt<Λ∗. Note that
max
t≥1max
θ∈Θ,λ∈[0,Λ∗]Ft= max/braceleftbigg
−8
ξ2bt+1(λt−λt+1)2−8
ξ/parenleftbigg
1−bt
bt+1/parenrightbigg
λ2
t+1+VL
σ(θt+1,λt+1) +bt
2λ2
t+1
27Under review as submission to TMLR
+/parenleftbigg
−16(CV
σ)2
ξb2
t+1−ξ(CV
σ)2
2/parenrightbigg
∥θt+1−θt∥2+/parenleftbigg8
ξ−1
2ξ/parenrightbigg
(λt+1−λt)2/bracerightbigg
≤1.6
ξ(Λ∗)2+ (1 +Λ∗)(2Cσ) +b1
2(Λ∗)2+15
2ξ(Λ∗)2
≜F∗, (121)
which is from the definition of bt, and 8(bt
bt+1−1)≤8((t+1)0.25
t0.25−1)≤8(20.25
1−1)<1.6. Then plugging in
the definition of btimplies that
T/summationdisplay
t=1ut∥˜Gt∥2≤F∗−F1−T/summationdisplay
t=1St+8
ξ(Λ∗)2+/parenleftbiggb0
2(Λ∗)2/parenrightbigg
. (122)
If moreover set u≜max/braceleftig
M1,10+20ξ2(CV
σ)2
9ξp2/bracerightig
, thenut≥1
upt, and hence
/summationtextT
t=11
pt∥˜Gt∥2
/summationtextT
t=11
pt≤u/summationtextT
t=11
pt/parenleftigg
F∗−F1−T/summationdisplay
t=1St+8
ξ(Λ∗)2+/parenleftbiggb0
2(Λ∗)2/parenrightbigg/parenrightigg
. (123)
Plug in the definition of ptthen we have that
/summationtextT
t=11
pt∥˜Gt∥2
/summationtextT
t=11
pt≤3200ξ(τ−2)(CV
σ)2d
192(√
T−2)/parenleftigg
F∗−F1−T/summationdisplay
t=1St+8
ξ(Λ∗)2+/parenleftbiggb0
2(Λ∗)2/parenrightbigg/parenrightigg
.(124)
We moreover have that
|St|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle16
btξ(f(θt−1)−ˆf(θt−1)−f(θt) +ˆf(θt))(−λt+λt+1) + (f(θt−1)−ˆf(θt−1))(λt+1−λt)
+⟨θt+1−θt,−ˆg(θt,λt+1) +g(θt,λt+1)⟩/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤32t0.25Λ∗(Ωt−1+Ωt) + 2Λ∗Ωt−1+1
αt(1 +Λ∗)CV
σΩt, (125)
whereΩt≜max/braceleftig
∥g(θt,λt+1)−ˆg(θt,λt+1)∥,|f(θt)−ˆf(θt)|/bracerightig
. Note that it has been shown in Wang & Zou
(2022) that Ωt≤LΩmax/braceleftig
∥Qσ,r−ˆQσ,r∥,∥Qσ,c−ˆQσ,c∥/bracerightig
=LΩϵest, and hence Ωtcan be controlled by
settingϵest.
Note that αt=ν+µtis increasing, hence1
αt≤1
α1. Hence if we set ϵest =
1
t0.5LΩ1
32t0.25Λ∗+2Λ∗+1
α1(1+Λ∗)CVσ192ϵ2
3200ξ(τ−2)(CVσ)2uLΩ=O(ϵ2
t0.75), then
|St|≤1
t0.5192ϵ2
3200ξ(τ−2)(CVσ)2uLΩ, (126)
and hence
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleT/summationdisplay
t=1St/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤√
T192ϵ2
3200ξ(τ−2)(CVσ)2uLΩ. (127)
Thus plug in eq. (124) and we have that
/summationtextT
t=11
pt∥˜Gt∥2
/summationtextT
t=11
pt≤3200ξ(τ−2)(CV
σ)2u
192(√
T−2)K+ϵ2, (128)
28Under review as submission to TMLR
whereK=F∗−F1+8
ξ(Λ∗)2+/parenleftbigb1
2(Λ∗)2/parenrightbig
. WhenT= (2 +3200ξ(τ−2)(CV
σ)2uK
192ϵ2 )2, we have that
/summationtextT
t=11
pt∥˜Gt∥2
/summationtextT
t=11
pt≤2ϵ2. (129)
Similarly to Theorem 4.2 in Xu et al. (2023), if t>194(Λ∗)4
2·104ξ4ϵ4, thenbt−1<ϵ
Λ∗andbt−1λt<ϵ. Hence combine
with eq. (115) we finally have that
min
1≤t≤T∥Gt∥≤(1 +√
2)ϵ, (130)
whenT= max/braceleftbigg
7(Λ∗)4
ξ4ϵ4,/parenleftig
2 +9ξ(τ−2)(CV
σ)2uK
ϵ2/parenrightig2/bracerightbigg
=O(ϵ−4).
J Constants
In this section, we summarize the definitions of all the constants we used in this paper.
¯f= (1 +Λ∗)1
1−γ,
F0=¯f+K0(Λ∗)2
ξ,
D1=16τ2
(τ−2)2+K2(ξL2
21−µ)2
(τ−2)2L4
21ξ2,
D2= max/braceleftbigg
D1,K2+ξ2L2
21
ξ,/bracerightbigg
D3=F0+(Λ∗)2
ξ,
LV=k|A|
(1−γ)2,
Cσ=1
1−γ(1 + 2γδlog|S|
σ),
CV
σ=1
1−γ|A|kCσ,
kB=1
1−γ+γδ/parenleftbig
|A|Cσl+|A|kCV
σ/parenrightbig
+2|A|2γ(1−δ)
(1−γ+γδ)2k2Cσ,
Lσ=kB+γδ
1−γ/parenleftbigg/radicalbig
|S|kB+ 2σ|S|CV
σ1
1−γ+γδk|A|Cσ/parenrightbigg
,
bt=19
20ξt0.25,
M1=16τ2
(τ−2)2+(ξ(CV
σ)2−ν)2
64(τ−2)2(CVσ)2ξ2,
u= max/braceleftbigg
M1,10 + 20ξ2(CV
σ)2
9ξp2/bracerightbigg
,
F∗=1.6
ξ(Λ∗)2+ (1 +Λ∗)(2Cσ) +b1
2(Λ∗)2+15
2ξ(Λ∗)2,
K=F∗−F1+8
ξ(Λ∗)2+/parenleftbiggb1
2(Λ∗)2/parenrightbigg
,
µt=ξ(CV
σ)2+16τ(CV
σ)2
ξ(bt+1)2−2ν,
29Under review as submission to TMLR
βt=1
ξ,
αt=ν+µt. (131)
30