Under review as submission to TMLR
General-Purpose In-Context Learning
by Meta-Learning Transformers
Anonymous authors
Paper under double-blind review
Abstract
Modern machine learning requires system designers to specify aspects of the learning
pipeline, such as losses, architectures, and optimizers. Meta-learning, or learning-to-learn,
instead aims to learnthose aspects, and promises to unlock greater capabilities with less
manual eﬀort. One particularly ambitious goal of meta-learning is to train general-purpose
in-context learning algorithms from scratch, using only black-box models with minimal in-
ductive bias . Such a model takes in training data, and produces test-set predictions across a
wide range of problems, without any explicit deﬁnition of an inference model, training loss,
or optimization algorithm. In this paper we show that Transformers and other black-box
models can be meta-trained to act as general-purpose in-context learners. We characterize
phase transitions between algorithms that generalize, algorithms that memorize, and algo-
rithms that fail to meta-train at all, induced by changes in model size, number of tasks, and
meta-optimization. We further show that the capabilities of meta-trained algorithms are
bottlenecked by the accessible state size (memory) determining the next prediction, unlike
standard models which are thought to be bottlenecked by parameter count. Finally, we
propose practical interventions such as biasing the training distribution that improve the
meta-training and meta-generalization of general-purpose in-context learning algorithms.
1 Introduction
Meta-learning is the process of automatically discovering new learning algorithms instead of designing them
manually (Schmidhuber, 1987). An important quality of human-engineered learning algorithms, such as
backpropagation and gradient descent, is their applicability to a wide range of tasks or environments. For
learning-to-learntoexceedthosecapabilities,themeta-learnedlearningalgorithmsmustbesimilarily general-
purpose. Recently, there has been signiﬁcant progress toward this goal (Kirsch et al., 2019; Oh et al., 2020).
The improved generality of the discovered learning algorithms has been achieved by introducing inductive
bias, such as by bottlenecking the architecture or by hiding information, which encourage learning over
memorization. Methods include restricting learning rules to use gradients (Metz et al., 2019; Kirsch et al.,
2019;Ohetal.,2020), symbolicgraphs(Realetal.,2020;Co-Reyesetal.,2021), orparametersharing(Kirsch
& Schmidhuber, 2020; Kirsch et al., 2021).
While enabling generalization, these inductive biases come at the cost of increasing the eﬀort to design these
systems and potentially restrict the space of discoverable learning algorithms. Instead, we seek to explore
general-purpose meta-learning systems with minimal inductive bias . Good candidates for this are black-box
sequence-models as meta-learners such as LSTMs (Hochreiter et al., 2001; Wang et al., 2016; Duan et al.,
2016) or Transformers (Vaswani et al., 2017). These memory-based or in-context learners take in training
data and produce test-set predictions without any explicit deﬁnition of an inference model, training loss,
or optimization algorithm. With recent advances of in-context learning in large language models (Brown
et al., 2020), neural networks can already learn many concepts from demonstrations. What are the necessary
conditions such that those models can learn from a wide range of demonstrations? To what extent can we
elicit in-context learning that generalizes to a wider range of problems, in a similar way how learning via
backpropagation and gradient descent can generalize?
1Under review as submission to TMLR
In this work, we investigate how such in-context meta-learners can be trained to (meta-)generalize and
learn on signiﬁcantly diﬀerent datasets than used during meta-training. For this we propose a Transformer-
basedGeneral-Purpose In-Context Learner (GPICL) which is described with an associated meta-training
task distribution in Section 3. In Section 4.1 we characterize phase transitions—induced by scaling the
number of tasks or the model size used for meta-training—between memorization, task identiﬁcation, and
general learning-to-learn. We further show in Section 4.2 that the capabilities of meta-trained algorithms
are bottlenecked by their accessible state (memory) size determining the next prediction (such as the hidden
state size in a recurrent network), unlike standard models which are thought to be bottlenecked by parameter
count. Finally, in Section 4.3, we propose practical interventions that improve the meta-training of general
purpose learning algorithms.
2 Background
What is a (supervised) learning algorithm? In this paper, we focus on the setting of meta-learning
supervised in-context learning algorithms. Consider a mapping
/parenleftBig
{xi,yi}ND
i=1,x/prime/parenrightBig
/mapsto→y/prime(1)
from the training (support) set D={xi,yi}ND
i=1and a query input x/primeto the query’s prediction y/primewhere
xi,x/prime∈RNx,yi,y/prime∈RNyandND,Nx,Ny∈N+. The subset of these functions that qualify as learning
algorithms are those that improve their predictions y/primegiven an increasingly larger training set D. Meta-
learning then corresponds to ﬁnding these functions via meta-optimization. As in other black-box meta-
learning models, we use a neural network to represent such functions. Such in-context learning is diﬀerent
from gradient-based meta-learning (such as MAML (Finn et al., 2017)) in that no explicit gradients are
computed at meta-test time. All required mechanisms for learning are implicitly encoded in the black-box
neural network.
What is a general-purpose learning algorithm? A learning algorithm can be considered general-
purpose if it learns on a wide range of possible tasks Dand their respective related queries x/prime,y/prime. In this
paper, we are interested in strong generalization across entirely diﬀerent datasets such as MNIST, Fashion
MNIST, and CIFAR10. Human-engineered learning algorithms such as gradient-descent on a suitable loss
function can be considered general-purpose learning algorithms that can be applied to any of these datasets
(where the gradient is obtained via backpropagation or other means). Meta-learners often don’t generalize
that well at meta-test time when we have an entirely new dataset that we want to learn on. We set out to
investigate under which conditions in-context learning generalizes well. In comparison to in-context learning,
gradient-based methods like MAML hard-code the human-engineered learning algorithm of gradient descent
and inherit its generalization properties.
3 General-Purpose In-Context Learning
LearningGeneralizingAlgorithm Description Seen Tasks Unseen Tasks
7 7 Task memorization
3 7 Task identiﬁcation
7 3 Zero-shot generalization
3 3General-purpose
learning algorithm
Performance
Examples seen
Table1: Analgorithmencodedinaneuralnetworkcan
be classiﬁed along two diﬀerent dimensions: To what
extent it learnsand to what extent it generalizes .Due to the small number of inductive biases in
black-box models, we can only expect (meta-
)generalization when meta-training with an appro-
priately broad data distribution. Thus, changes
in the data distribution aﬀect whether and how a
model meta-learns and meta-generalizes. We clas-
sify algorithms along two diﬀerent dimensions: To
what extent it learns (improving predictions given
increasingly larger training sets provided at infer-
ence time), and to what extent it generalizes (per-
forms well on instances, tasks, or datasets not seen
before). Algorithms can then be categorized as in
Table 1. In task memorization, the model immedi-
ately performs well on seen tasks but does not gen-
2Under review as submission to TMLR
eralize. In task identiﬁcation, the model identiﬁes the task and gets better on it at inference time as it sees
more examples but can only do so on tasks very similar to what it was trained on. In zero-shot generalization,
the model immediately generalizes to unseen tasks, without observing examples. Finally, a general-purpose
learning algorithm improves as it observes more examples both on seen and signiﬁcantly diﬀerent unseen
tasks. We demonstrate that sharp phase transitions occur between these learning modalities, and empirically
investigate these.
3.1 Generating Tasks for Learning-to-Learn
Neural networks are known to require datasets of signiﬁcant size to eﬀectively generalize. While in standard
supervised learning large quantities of data are common, meta-learning algorithms may require a similar
number of distinct tasksin order to learn and generalize. Unfortunately, the number of commonly available
tasks is orders of magnitudes smaller compared to the datapoints in each task.
Previous work has side-stepped this issue by building-in architectural or algorithmic structure into the
learning algorithm, in eﬀect drastically reducing the number of tasks required. For example, in Kirsch &
Schmidhuber (2020); Kirsch et al. (2021), the authors included symmetries into the black-box model in
the form of input and output permutation invariances. An alternative to this is the generation of new
tasks (Schmidhuber, 2013; Clune, 2019; Such et al., 2020; Parker-Holder et al., 2022). Unfortunately, it is
not easy to generate a wide range of tasks that are both diverse and contain structure as it can be found in
the real world.
General-Purpose In-Context Learning Transformerx10x2y1x/uni2032 3y2… y/uni2032 1y/uni2032 2… y/uni2032 3Third support setThird query set
Figure 1: Our General-Purpose In-Context Learner
(GPICL) is based on the vanilla Transformer which is
trained to make predictions for queries x/primegiven any preﬁx
of a dataset D:={xi,yi}ND
i=1as in Equation 2.In this work, we take an intermediate step by
augmenting existing datasets, in eﬀect increas-
ing the breadth of the task distribution based
on existing task regularities. We generate a
large number of tasks by taking existing su-
pervised learning datasets, randomly project-
ing their inputs and permuting their classiﬁca-
tion labels. While the random projection re-
moves spatial structure from the inputs, this
structure is not believed to be central to the
task (for instance, the performance of SGD-
trained fully connected networks is invariant
to projection by a random orthogonal matrix
(Wadia et al., 2021)). Task augmentation al-
lows us to investigate fundamental questions
about learning-to-learn in the regime of many
tasks without relying on huge amounts of existing tasks or elaborate schemes to generate those.
A task or dataset Dis then deﬁned by its corresponding base dataset ¯D={¯xi,¯yi}, (linear) projection
A∈RNx×Nx, withAij∼N/parenleftBig
0,1
Nx/parenrightBig
, and output permutation ρ,D={A¯xi,ρ(¯yi)}. Unless noted otherwise,
the distribution over output permutations p(ρ)is uniform.
3.2 Meta-learning and meta-testing
Meta-learning Given those generated tasks, we then meta-train jointly on a mini-batch sampled from the
whole distribution. First, we sample datasets Dfrom the augmented task distribution p(D)and then take
a random batch D1:NDfrom the training set. Second, we minimize J(θ), the sum of losses on the query
prediction after observing any preﬁx D1:j−1
J(θ) =ED∼p(D)
ND/summationdisplay
j=1l(fθ(D1:j−1,xj),yj)
, (2)
3Under review as submission to TMLR
2
4
8
16
32
64
128
256
512
1024
Hidden size (capacity)2022242628210212214216218220222224Number of tasksMLP: Accuracy on seen tasks
0.00.20.40.60.81.0
(a)
2
4
8
16
32
64
128
256
512
Transformer model size2022242628210212214216218220222224Number of tasksAccuracy on seen tasks
2
4
8
16
32
64
128
256
512
Transformer model sizeAccuracy on unseen tasks
0.00.20.40.60.81.0Transformer (b) (c)
Figure 2: GPICL is able to generalize to unseen tasks. Each cell is a separate meta-training run.
(a)An MLP classiﬁer trained in a multi-task fashion across various numbers of tasks (generated based on
MNIST) and network sizes is able to ﬁt linearly more tasks, the larger its capacity. (b)A sequence model
(here the GPICL Transformer) that observes a dataset Dof inputs and labels transitions into generalizing
to an seemingly unbounded number of tasks with an increase in model size. This is achieved by switching
from a memorization solution to a learning solution that (c)generalizes to unseen tasks. This generalization
does not occur with the MLP.
Algorithm 1 Meta-Training for General-Purpose In-Context Learning (GPICL) via Augmentation
Require: Dataset ¯D={¯xi,¯yi}, Number of tasks K∈N+
# Deﬁnep(D)by augmenting ¯D, here by:
{A(k)
ij}K
k=1∼N(0,1
Nx) ⊿Sample input projections
{ρ(k)}K
k=1∼p(ρ) ⊿Sample output permutations
D(k)={A(k)¯xi,ρ(k)(¯yi)}
p(D) := Uniform[{D(k)}K
k=1]
# Meta-Training on p(D)
whilenot converged do
θ←θ−α∇θJ(θ) ⊿Equation 2
where in the classiﬁcation setting, lis the cross entropy loss between the label yjand prediction y/prime=
fθ(D1:j−1,xj),fθis a neural network mapping to predictions y/primeas in Equation 1. During meta-training, we
take gradient steps in J(θ)by backpropagation and Adam (Kingma & Ba, 2014). To investigate the eﬀect
of the data distribution, we train on various numbers of tasks (Algorithm 1). Finally, we need to choose
a black-box model for the function fθ. We use a vanilla Transformer (Vaswani et al., 2017) with learned
positional embeddings, visualized in Figure 1. We call it the General-Purpose In-Context Learner (GPICL).
Each token corresponds to the concatenation of a transformed input xiand one-hot encoded label yi−1. The
model predicts the corresponding logits y/prime=yifor the current input x/prime=xi. When querying for the ﬁrst
x1, no label for the previous input is available, so we feed a zero vector.
Meta-testing Atmeta-testtime, nogradient-basedlearningisused. Instead, wesimplyobtainaprediction
y/primeby evaluating the neural network fθon a dataset Dand query point x/prime. The dataset Dis either derived
fromthesamebasedataset(egMNISTaftermeta-trainingonMNIST)oritisderivedfromadiﬀerentdataset
(eg Fashion MNIST or CIFAR10). In both cases a seen or unseen random projection is used. Datapoints
are taken only from the respective test split of the base dataset.
4 Experiments on the Emergence of General Learning-To-Learn
Multi-task training with standard classiﬁers Given a task distribution of many diﬀerent classiﬁcation
tasks, we ﬁrst ask under what conditions we expect “learning-to-learn” to emerge. We train a single model
4Under review as submission to TMLR
across many tasks where each task corresponds to a random transformation of the MNIST dataset, but
where the MLP only receives a single datapoint instead of a whole sequence as input. This corresponds to
ND= 1in Equation 2. We would expect such a non-sequential classiﬁer to be able to correctly predict for
more tasks as its number of parameters increases. When plotting the network capacity against the number
of tasks, we indeed observe a linear boundary where an increasing number of tasks can be ﬁt the larger the
network (Figure 2a). This is consistent with results in Collins et al. (2016), which found that a constant
number of bits about the data distribution can be stored per model parameter, across a variety of model
architectures and scales.
Learning-to-learn with large sequential models and data In contrast to the MLP classiﬁer, a se-
quence model that observes multiple observations and their labels from the same task, could exceed that
linear performance improvement by learning at inference time. Indeed, we observe that when switching
to a Transformer that can observe a sequence of datapoints before making a prediction about the query,
more tasks can be simultaneously ﬁt (Figure 2b). At a certain model size and number of tasks, the model
undergoes a phase transition, allowing to generalize to a seemingly unbounded number of tasks. We hypoth-
esize that this is due to switching the prediction strategy from memorization to learning-to-learn. Further,
when (meta-)testing the same trained models from the previous experiment on an unseen task (new random
transformation of MNIST), they generalize only in the regime of large numbers of tasks and model size
(Figure 2c). As an in-context learner, meta-testing does not involve any gradient updates but only running
the model in forward mode.
Insight 1: It is possible to learn-to-learn with black-box models Eﬀective learning algorithms can
be realized in-context using black-box models with few inductive biases, given suﬃcient meta-training task
diversity and large enough model sizes. To transition to the learning-to-learn regime, we needed at least
213= 8192tasks.
In the following, we study learning-to-learn from the perspective of the data distribution , thearchitecture ,
and the optimization dynamics . For the data distribution, we look at how the data diversity aﬀects the
emergence and phase transitions of learning-to-learn, generalization, and memorization. For architecture,
we analyze the role of the model and state size in various architectures. Finally, we observe challenges in
meta-optimizationanddemonstratehowmemorizationfollowedbygeneralizationisanimportantmechanism
that can be facilitated by biasing the data distribution.
4.1 Large Data: Generalization and Phase Transitions
0 20 40 60 80 100
Number of examples seen0.00.20.40.60.81.0AccuracyMeta-test learning curve on MNIST
0 20 40 60 80 100
Number of examples seenMeta-test learning curve on FashionMNIST
Trained on
mnist
fashion_mnist
(a) (b)
Figure 3: GPICL learns from examples at test time, and gen-
eralizes to unseen tasks and datasets. We meta-trained the Trans-
former on a set of tasks deﬁned by random transformations of either
MNIST (blue) or FashionMNIST (orange). We then meta-test on un-
seen tasks, and seen (ab) or unseen (ba) datasets. The plot shows the
accuracy averaged across multiple runs at each inner step, with shad-
ing indicating 95%conﬁdence intervals. The increase in performance at
each step suggests we have learned a learning algorithm.Simple data augmentations
lead to the emergence of
learning-to-learn To verify
whether the observed generalizing
solutions actually implement
learning algorithms (as opposed
to e.g. zero-shot generalization),
we analyze the meta-test time
behavior. We plot the accuracy
for a given query point for varying
numbers of examples in Figure 3.
As is typical for learning algo-
rithms, the performance improves
when given more examples (inputs
and labels).
Generalization Naturally, the
question arises as to what extent
these learning algorithms are gen-
eral. While we have seen general-
ization to unseen tasks consisting
5Under review as submission to TMLR
Table 2: Meta-test generalization to various datasets after meta-training on augmented MNIST and seeing
99 examples, predicting the 100th. We report the mean across 3 meta-training seeds, 16 sequences from each
task, 16 tasks sampled from each base dataset. GPICL is competitive to other approaches that require more
inductive bias.
Method / Dataset Inductive bias MNISTFashion
MNISTKMNIST Random CIFAR10 SVHN
SGD Backprop, SGD 70.31% 50.78% 37.89% 100.00% 14.84% 10.16%
MAML Backprop, SGD 53.71% 48.44% 36.33% 99.80% 17.38% 11.33%
VSML In-context, param sharing 79.04% 68.49% 54.69% 100.00% 24.09% 17.45%
LSTM In-context, black-box 25.39% 28.12% 18.10% 58.72% 12.11% 11.07%
GPICL Transformer (ours) In-context, black-box 73.70% 62.24% 53.39% 100.00% 19.40% 14.58%
of novel projections of the same dataset, do the learned algorithms also generalize to unseen datasets?
In Figure 3 we observe strong out-of-distribution performance on Fashion MNIST after having trained on
MNIST (b, blue), and there is no generalization gap compared to directly training on Fashion MNIST (b,
orange). Similarly, when meta training on Fashion MNIST and meta testing on MNIST (a, orange) we
observe that the learning algorithm generalizes, albeit with a larger generalization gap.
Comparison to other methods Other datasets and baselines are shown in Table 2. We aim to validate
whether methods with less inductive bias (such as our GPICL), can compete with methods that include
more biases suitable to learning-to-learn. This includes stochastic gradient descent (SGD), updating the
parameters online after observing each datapoint. MAML (Finn et al., 2017) proceeds like SGD, but uses
a meta-learned neural network initialization. Both methods that rely on backpropagation and gradient
descent learn more slowly than our Transformer. In the case of MAML, this may be due to the main
mechanism being feature reuse (Raghu et al., 2020) which is less useful when training across our wider
task distribution. For in-context learners (methods that do not hard-code gradient descent at meta-test
time), we test VSML (Kirsch & Schmidhuber, 2020) that discovered learning algorithms signiﬁcantly gen-
eralizing between tasks. Our GPICL comes surprisingly close to VSML without requiring the associated
inductive bias. GPICL generalizes to many datasets, even those that consist of random input-label pairs.
20232629212215218221224
Number of tasks0.1
0.00.10.20.30.40.50.60.7Accuracy improvement within sequenceT ask
memorizationT ask
identificationGeneral
learning to learn
Seen MNIST
Unseen MNIST
Unseen FashionMNIST
Unseen KMNIST
Figure 4: Transformers exhibit three diﬀerent
phases in terms of meta-learned behavior. (1)
When training on a small number of tasks, tasks are
memorized. (2) Tasks from the training distribution
areidentiﬁed, whichisevidentasawithin-sequencein-
crease of performance. (3) When training across many
tasks, wediscoveralearningalgorithmthatgeneralizes
to unseen tasks and unseen datasets.We also observe that learning CIFAR10 and SVHN
from only 99 examples with a general-purpose learn-
ing algorithm is diﬃcult, which we address in Sec-
tion 4.4. Training and testing with longer context
lengths improves the ﬁnal predictions (Appendix
A.2). Using LSTM-based in-context learners per-
forms worse, which we further discuss in Section 4.2
among other alternative architectures.
Insight 2: Simple data augmentations are ef-
fective for learning-to-learn The generality of
the discovered learning algorithm can be controlled
via the data distribution. Even when large task dis-
tributions are not (yet) naturally available, simple
augmentations are eﬀective.
Transitioning from memorization to task
identiﬁcation to general learning-to-learn
When do the learned models correspond to mem-
orizing, learning, and generalizing solutions? In
Figure 4, we meta-train across varying numbers of
tasks, with each point on the x-axis corresponding
to multiple separate meta-training runs. We plot
the accuracy diﬀerence between the last and ﬁrst
prediction (how much is learned at meta-test time)
for a seen task, an unseen task, and an unseen task
6Under review as submission to TMLR
with a diﬀerent base dataset. We observe three phases: In the ﬁrst phase, the model memorizes all tasks,
resulting in no within-sequence performance improvement. In the second phase, it memorizes and learns to
identify tasks, resulting in a within-sequence improvement conﬁned to seen task instances. In the ﬁnal and
third phase, we observe a more general learning-to-learn, a performance improvement for unseen tasks, even
diﬀerent base datasets (here FashionMNIST). This phenomenon applies to various other meta-training and
meta-testing datasets. The corresponding experiments can be found in Appendix A.5.
0k 2k 5k 8k 10k 12k 15k
Number of tasks0.60.81.01.21.41.6Training loss
Figure 5: Solutions found by GPICL
after meta-training are bi-modal,
with a memorization and generaliza-
tion mode. Each point represents the
training loss at the end of meta-training
for runs with diﬀerent seeds and for various
numbers of tasks that include the transition
boundary previously observed. Almost all
solutions are either in a memorization clus-
ter or in a generalization cluster.The phase transition to general learning-to-learn In
Figure 4 we observe a fairly discrete transition from task iden-
tiﬁcation to generalizing learning-to-learn (the second dashed
line) as a function of the number of tasks. Previously, Fig-
ure 2 (c) showed a similarly discrete phase transition from no
learning to learning on unseen tasks. What happens during
this transition and when do the found solutions correspond to
memorizing (task memorization or seen task identiﬁcation) vs
generalizing solutions? To analyze the transition from task
identiﬁcation to general learning to learn, we perform multi-
ple training runs with varying seeds and numbers of tasks on
MNIST. This is shown in Figure 5, reporting the ﬁnal training
loss. We ﬁnd that the distribution is bi-modal. Solutions at
the end of training are memorizing or generalizing. Memoriza-
tion cluster: The larger the number of tasks, the more diﬃcult
it is to memorize all of them with a ﬁxed model capacity (or
learn to identify each task). Generalization cluster: At a cer-
tain number of tasks (here 6000), a transition point is reached
where optimization sometimes discovers a lower training loss
that corresponds to a generalizing learning to learn solution.
For larger numbers of tasks the solutions always settle in the
generalizing cluster.
Insight 3: The meta-learned behavior has phase transi-
tionsWhen increasing the number of tasks, the meta-learned
behavior transitions from task memorization, to task identiﬁ-
cation, to general learning-to-learn. The last transition is discrete, with two unique clusters.
4.2 Architecture: Large Memory (State) is Crucial for Learning
2528211214
State size0.00.20.40.60.81.0AccuracyUnseen MNIST
LSTM
Transformer
Outer-product LSTM
VSML without symmetries
(a)
210212214
Parameter count0.00.20.40.60.81.0AccuracyUnseen MNIST (b)
Figure 6: The state size (accessible memory)
of an architecture most strongly predicts its
performance as a general-purpose learning al-
gorithm. (a) A large state is crucial for learning-to-
learn to emerge. (b)The parameter count correlates
less well with learning capabilities.In the previous experiments we observed that given
suﬃcient task diversity and model size, Transform-
ers can learn general-purpose learning algorithms.
This raises the question how essential the Trans-
former architecture is and whether other black-box
models could be used. We hypothesize that for
learning-to-learnthesizeofthememoryatmeta-test
time (or state more generally) is particularly impor-
tant in order to be able to store learning progress.
Through self-attention, Transformers have a partic-
ularly large state. We test this by training several
architectures with various state sizes in our meta-
learning setting. In Figure 6a, we observe that when
we vary the hyper-parameters which most inﬂuence
the state size, we observe that for a speciﬁc state
size we obtain similar performance of the discovered
learning algorithm across architectures. In contrast,
these architectures have markedly diﬀerent numbers of parameters (Figure 6b).
7Under review as submission to TMLR
0k 10k 20k 30k 40k 50k
step24loss
0k 10k 20k 30k 40k 50k
step
100% permuted labels
Training
Unseen FashionMNIST
Unseen MNIST
(a)
0k 20k 40k
step2.252.302.352.40lossTraining loss
0k 20k 40k
stepUnseen FashionMNIST
0k 20k 40k
stepUnseen MNIST (b)
Figure7:Meta-trainingdynamicsofteninvolveanextendedperiodwhereGPICL’sperformance
is stuck on a plateau. (a) Meta-loss vs. meta-training step, for a uniform distribution over meta-training
tasks. Training tasks are generated by random transformations of FashionMNIST. (b)A zoomed in view
of the plateau. The loss only decreases slightly and the model memorize small biases in the training data
(decreasing generalization) before the loss drops sharply.
What corresponds to state (memory) in various architectures? MemoryNSin the context of
recurrent neural networks corresponds to the hidden state or context vector of size NH, thusNS∈O(NH).
More generally, we can describe the state as the information bottleneck that the sequence has to pass
through before making predictions. In the context of learning-to-learn, this state has to hold information
about everything that has been learned so far. Standard learning algorithms such as neural networks trained
via SGD would have a state that corresponds to the neural network parameters, iteratively updated via
SGD. In transformers, self-attention allows for a particularly large state of NS∈O(NKNLNT)whereNK
is the size of key, value, and query, NLis the number of layers, and NTis the length of the sequence. In
addition to Figure 6, Figure 13 show meta-test performance on more tasks and datasets.
Insight 4: Large state is more crucial than parameter count This suggests that the model size
in terms of parameter count plays a smaller role in the setting of learning-to-learn and Transformers have
beneﬁted in particular from an increase in state size by self-attention. Beyond learning-to-learn, this likely
applies to other tasks that rely on storing large amounts of sequence-speciﬁc information.
4.3 Challenges in Meta-Optimization
Meta-optimization is known to be challenging. Meta gradients (Finn et al., 2017; Xu et al., 2018; Bechtle
et al., 2021) and works with parameter sharing or weight updates in their architecture (Kirsch & Schmidhu-
ber, 2020; Pedersen & Risi, 2021; Risi, 2021) observed various diﬃculties: Slower convergence, local minima,
unstable training, or loss plateaus at the beginning of training (see Appendix Figure 21). We show that
some of these problems also occur with black-box models and propose eﬀective interventions.
Loss plateaus when meta-learning with black-box models By training across a large number of
randomly transformed tasks, memorizing any task-speciﬁc information is diﬃcult. Instead, the model is
forced to ﬁnd solutions that are directly learning. We observe that this results in (meta-)loss plateaus during
meta-training where the loss only decreases slightly for long periods of time (Figure 7a). Only after a large
number of steps (here around 35 thousand) does a drop in loss occur. In the loss plateau, the generalization
loss increases on unseen tasks from both the same and a diﬀerent base dataset (Figure 7b). This suggests
that being able to ﬁrst memorize slightly enables the following learning-to-learn phase. Furthermore, we
observe that all gradients have a very small norm with exception of the last layer (Appendix Figure 17).
Intervention 1: Increasing the batch size High variance gradients appear to be one reason training
trajectories become trapped on the loss plateau (see Appendix Figures 15, 16). This suggests increasing the
meta-batch size as a straightforward solution. When plotting various batch sizes against numbers of tasks
we obtain three kinds of solutions at the end of meta-training (Figure 8a): (1) Solutions that generalize and
learn, (2) Solutions that memorize, and (3) Solutions that are still in the loss plateau (due to maximum of
50 thousand optimization steps). The larger the batch size, the more tasks we can train on without getting
stuck in a loss plateau. When plotting the length of the loss plateau against the task batch size (Figure 8b)
we observe a power-law relationship with increasing batch sizes decreasing the plateau length. At the same
8Under review as submission to TMLR
8
16
32
64
128
256
512
1024
2048
4096
T ask batch size2022242628210212214216218220222224Number of tasksPlateau, overfit, or generalize?
PlateauOverfitGeneralize
(a)
2526272829210211212
Task batch size213214215Plateau length# of tasks
216
218
220
222
224 (b)
217219221223225
Number of tasks213214215Plateau length Task batch size
25
27
29
211 (c)
Figure 8: Whether GPICL memorizes, generalizes, or remains trapped on a meta-loss plateau
depends on the number of meta-training tasks, and the meta-training batch size. (a) A phase
diagram showing GPICL’s behavior at the end of meta-training (50k steps). Solutions either memorize,
generalize and learn, or remain in the loss plateau. With additional training steps, conﬁgurations in the
plateau might eventually transition to memorization or generalization. Generalization only occurs with large
enough batch sizes and suﬃcient, but not too many, tasks. (b)This behavior is explained by the plateau
length decreasing with the increasing batch sizes (reducing the noise contribution), and (c)increasing with
larger numbers of tasks.
time, the batch size also increases the number of total tasks seen in the plateau (Appendix Figure 18). Thus,
this intervention relies on parallelizability. An increase in the number of tasks also increases the plateau
length (Figure 8c), possibly due to a larger number of tasks inhibiting the initial memorization phase.
24loss0% permuted labels 10% permuted labels
0k 10k 20k 30k 40k 50k
step24loss90% permuted labels
0k 10k 20k 30k 40k 50k
step100% permuted labels
Training
Unseen FashionMNIST
Unseen MNIST
(a) (b)
(c) (d)
Figure 9: Biasing the training distribution is an
eﬀective intervention which prevents a meta-
loss plateau. A uniform distribution over tasks leads
to a long plateau (d), while increasing the training
fraction that corresponds to a single task reduces the
plateau(abc).Intervention 2: Changes in the meta-
optimizer Given that many gradients in the loss
plateau have very small norm, Adam would rescale
those element-wise, potentially alleviating the issue.
In practice, we observe that the gradients are so
small that the /epsilon1in Adam’s gradient-rescaling de-
nominator (for numerical stability) limits the up-
scaling of small gradients. Using smaller /epsilon1results in
morethanhalvingtheplateaulength. Alternatively,
discarding the magnitude of the gradient entirely by
applying the sign operator to an exponential moving
average of the gradient (replacing Adam’s approx-
imate magnitude normalization with direct magni-
tude normalization) has a similar eﬀect while also
increasing the numerical stability over Adam with
small/epsilon1(Appendix Figure 19).
Intervention 3: Biasing the data distribution
/ Curricula GPICL mainly relies on the data dis-
tribution for learning-to-learn. This enables a dif-
ferent kind of intervention: Biasing the data distri-
bution. The approach is inspired by the observation
that before leaving the loss plateau the model mem-
orizes biases in the data. Instead of sampling label permutations uniformly, we bias towards a speciﬁc
permutation by using a ﬁxed permutation for a fraction of each batch. This completely eliminates the loss
plateau, enabling a smooth path from memorizing to learning (Figure 9). Surprisingly, even when heav-
ily biasing the distribution, memorization is followed by generalization. This biased data distribution can
be viewed as a curriculum, solving an easier problem ﬁrst that enables the subsequent harder learning-to-
learn. Further investigation is required to understand how this transition occurs. This may be connected
to grokking (Power et al., 2022) which we investigate in Appendix A.5. We hypothesize that many natu-
ral data distributions—including language—contain such sub-tasks that are easy to memorize followed by
generalization.
9Under review as submission to TMLR
4.4 Domain-speciﬁc and general-purpose learning
MNIST
(meta-trained)FashionMNIST
(unseen)KMNIST
(unseen)Random
(unseen)CIFAR10
(unseen)SVHN
(unseen)
Meta-test task0.00.20.40.60.81.0Meta-test acurracy of last predictionGPICL without feature embedding
GPICL with feature embedding
Figure 10: Using pre-trained networks allows
leveraging domain-speciﬁc knowledge while
still generalizing to other datasets GPICL is
meta-trained on MNIST either with the randomly
transformed raw inputs or randomly transformed pre-
trained features. Pre-training helps to accelerate
meta-test-time in-context learning on datasets that
have a matching domain, such as CIFAR10. With
only 100 examples, the learning algorithm can achieve
about 45%accuracy on CIFAR10. The learning al-
gorithms still generalize to a wide range of datasets.
Error bars are 95%conﬁdence intervals of the mean
across meta-training runs.We demonstrated the feasibility of meta-learning
in-context learning algorithms that are general-
purpose. An even more useful learning algorithm
would be capable of both generalizing, as well as
leveraging domain-speciﬁc information for learning
when it is available. This would allow for consid-
erably more eﬃcient in-context learning, scaling to
more diﬃcult datasets without very long input se-
quences. Toward this goal, we investigate a sim-
ple scheme that leverages pre-trained neural net-
works as features to learn upon. This could be from
an unsupervised learner or a frozen large language
model (Radford et al., 2021; Tsimpoukelli et al.,
2021). Here, we ﬁrst project the inputs ¯xiof a base-
dataset ¯Dinto some latent space using a pre-trained
network, and then proceed with meta-training and
meta-testing as before, randomly projecting these
alternative features. For the pre-trained network,
we use a ResNet trained on ImageNet and remove
its ﬁnal layer. In Figure 10 we have meta-trained
GPICL on MNIST either with the randomly trans-
formed raw inputs or randomly transformed embed-
ded features. At meta-test-time the learning algo-
rithm generalizes to a wide range of datasets, mea-
sured by the meta-test accuracy of the 100th exam-
ple. At the same time, the pre-trained ImageNet
helps to accelerate learning on datasets that have
a matching domain, such as CIFAR10. We observe
that with only 100 examples, the learning algorithm
meta-trained on MNIST, can achieve about 45%accuracy on CIFAR10.
5 Related work
Meta-learning: Inductive biases and general-purpose learning algorithms Meta-learning ap-
proaches exist with a wide range of inductive biases, usually inspired by existing human-engineered learning
algorithms. Some methods pre-wire the entire learning algorithm (Finn et al., 2017), pre-wire backpropaga-
tion and the structure of a gradient-based optimizer (Andrychowicz et al., 2016; Metz et al., 2019; 2020a),
or learn the loss function (Houthooft et al., 2018; Kirsch et al., 2019; Bechtle et al., 2021).
Many methods search over hyper-parameters that alter existing learning algorithms (Xu et al., 2018; Metz
et al., 2020b; Chen et al., 2022). Fast weight programmers or hyper-networks update the weights of the same
or another neural network (Schmidhuber, 1992; 1993a; Ha et al., 2017; Irie et al., 2021; Sandler et al., 2021;
Kirsch & Schmidhuber, 2022; Zhmoginov et al., 2022), frequently with various symmetries. There has been
growing interest in meta-learning more general-purpose learning algorithms. Such learning algorithms aim
to be general and reusable like other human-engineered algorithms (e.g. gradient descent). The improved
generality of the discovered learning algorithm has been achieved by introducing inductive bias, such as by
bottlenecking the architecture or by hiding information, encouraging learning over memorization. Methods
include enforcing learning rules to use gradients (Metz et al., 2019; Kirsch et al., 2019; Oh et al., 2020),
symbolic graphs (Real et al., 2020; Co-Reyes et al., 2021), or parameter sharing and symmetries (Kirsch &
Schmidhuber, 2020; Kirsch et al., 2021). Parameter sharing and symmetries have additionally been discussed
in the context of self-organization (Tang & Ha, 2021; Risi, 2021; Pedersen & Risi, 2022).
10Under review as submission to TMLR
In-context learning with black-box models Black-box neural networks can learn-to-learn purely in
their activations (in-context) with little architectural and algorithmic bias (Hochreiter et al., 2001; Wang
et al., 2016; Duan et al., 2016; Santoro et al., 2016; Garnelo et al., 2018). This requires a feedback or demon-
stration signal in the inputs that allows for learning such as the reward in reinforcement learning or label
in supervised learning (Schmidhuber, 1993b). While a frequently used architecture is the LSTM (Hochre-
iter & Schmidhuber, 1997; Gers et al., 2000), this mechanism has also seen substantial recent attention
in Transformer models (Brown et al., 2020; Chan et al., 2022) under the name of in-context learning. In
large language models (LLMs) demonstrations of a task in the input help solving language-based tasks at
inference (meta-test) time (Brown et al., 2020). This few-shot learning ability has been attributed to the
data-distributional properties of text corpora (Chan et al., 2022). In-context learning has also been inter-
preted from a Bayesian inference perspective (Ortega et al., 2019; Mikulik et al., 2020; Nguyen & Grover,
2022; Müller et al., 2022). Our method GPICL is in the class of these black-box in-context learners. The
number of model parameters has been at the core of scaling up LLMs to unlock greater capabilities and have
been formulated in scaling laws (Kaplan et al., 2020; Hoﬀmann et al., 2022). Our empirical study suggests
that for learning-to-learn, the amount of memory (model state) is even more predictive of in-context learning
capabilities than parameter count.
General-purpose in-context learning While in-context learning has been demonstrated with black-
box models, little investigation of general-purpose meta-learning with these models has been undertaken.
Generalization in LLMs has previously been studied with regards to reasoning and systematicity (Csordás
et al., 2021; Delétang et al., 2022; Wei et al., 2022; Zhou et al., 2022; Anil et al., 2022). In this work we
focus on meta-generalization instead, the extent to which in-context learning algorithms generalize. In con-
trast to previous methods, GPICL implements general-purpose learning algorithms. Independently, Garg
et al. (2022) recently studied generalization on synthetic functions compared to our augmented datasets.
VSML (Kirsch & Schmidhuber, 2020) also implements in-context learning with black-box LSTMs, but makes
use of parameter-sharing to aid generalization. PFNs (Müller et al., 2022) demonstrated learning to learn
on small tabular datasets when meta-training on synthetically generated problems. Experiments on more
complex classiﬁcation settings such as Omniglot relied on ﬁne-tuning. In comparison, our method inves-
tigated meta-generalization of learning algorithms directly to datasets such as MNIST, Fashion MNIST,
and CIFAR10 while studying fundamental questions about the conditions necessary for such generalization.
TabPFNs (Hollmann et al., 2022) extend PFNs to larger tabular datasets.
6 Discussion and Conclusion
By generating tasks from existing datasets, we demonstrated that black-box models such as Transformers
can meta-learn general-purpose in-context learning algorithms (GPICL). We observed that learning-to-learn
arises in the regime of large models and large numbers of tasks with several phase transitions from task
memorization, to task identiﬁcation, to general learning. The size of the memory or model state signiﬁcantly
determines how well any architecture can learn how to learn across various neural network architectures.
We identiﬁed diﬃculties in meta-optimization and proposed interventions in terms of optimizers, hyper-
parameters, and a biased data distribution acting as a curriculum. We demonstrated that in-context learning
algorithms can also be trained to combine domain-speciﬁc learning and general-purpose learning. We believe
our ﬁndings open up new possibilities of data-driven general-purpose meta-learning with minimal inductive
bias, including generalization improvements of in-context learning in large language models (LLMs).
An important subject of future work is the exploration of task generation beyond random projections, such as
augmentation techniques for LLM training corpora or generation of tasks from scratch. A current limitation
is the applicability of the discovered learning algorithms to arbitrary input and output sizes beyond random
projections. Appropriate tokenization to uniﬁed representations may solve this (Chowdhery et al., 2022).
Furthermore, learning algorithms often process millions of inputs before outputting the ﬁnal model. In the
current black-box setting, this is still diﬃcult to achieve and it requires new advances for in context length
of sequence models. Recurrency-based models may suﬀer from accumulating errors, whereas Transformer’s
computational complexity grows quadratically in sequence length.
11Under review as submission to TMLR
References
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoﬀman, David Pfau, Tom Schaul, Brendan
Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances
in Neural Information Processing Systems , pp. 3981–3989, 2016.
Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Venkatesh Ra-
masesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length general-
ization in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
(eds.),Advances in Neural Information Processing Systems , 2022.
Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav
Sukhatme, and Franziska Meier. Meta learning via learned loss. In 25th International Conference on
Pattern Recognition (ICPR) , pp. 4161–4168. IEEE, 2021.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165 , 2020.
Stephanie CY Chan, Adam Santoro, Andrew K Lampinen, Jane X Wang, Aaditya Singh, Pierre H
Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent in-context
learning in transformers. arXiv preprint arXiv:2205.05055 , 2022.
Yutian Chen, Xingyou Song, Chansoo Lee, Zi Wang, Qiuyi Zhang, David Dohan, Kazuya Kawakami, Greg
Kochanski, Arnaud Doucet, Marc’aurelio Ranzato, et al. Towards learning universal hyperparameter
optimizers with transformers. arXiv preprint arXiv:2205.13320 , 2022.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language
modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.
Jeﬀ Clune. Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artiﬁcial intelli-
gence.arXiv preprint arXiv:1905.10985 , 2019.
John D Co-Reyes, Yingjie Miao, Daiyi Peng, Esteban Real, Quoc V Le, Sergey Levine, Honglak Lee, and
Aleksandra Faust. Evolving reinforcement learning algorithms. In International Conference on Learning
Representations , 2021.
Jasmine Collins, Jascha Sohl-Dickstein, and David Sussillo. Capacity and trainability in recurrent neural
networks. arXiv preprint arXiv:1611.09913 , 2016.
Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. The devil is in the detail: Simple tricks improve
systematic generalization of transformers. In EMNLP, 2021.
Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Marcus
Hutter, Shane Legg, and Pedro A Ortega. Neural networks and the chomsky hierarchy. arXiv preprint
arXiv:2207.02098 , 2022.
Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast
reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779 , 2016.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In International Conference on Machine Learning , pp. 1126–1135. PMLR, 2017.
Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context?
a case study of simple function classes. arXiv preprint arXiv:2208.01066 , 2022.
Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan,
Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In International
Conference on Machine Learning , pp. 1704–1713. PMLR, 2018.
12Under review as submission to TMLR
Felix A Gers, Jürgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with lstm.
Neural computation , 12(10):2451–2471, 2000.
David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In International Conference on Learning
Representations , 2017.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–1780,
1997.
Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In
International Conference on Artiﬁcial Neural Networks , pp. 87–94. Springer, 2001.
Jordan Hoﬀmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal
large language models. arXiv preprint arXiv:2203.15556 , 2022.
Noah Hollmann, Samuel Müller, Katharina Eggensperger, and Frank Hutter. Tabpfn: A transformer that
solves small tabular classiﬁcation problems in a second. Table Representation Workshop at NeurIPS , 2022.
Rein Houthooft, Richard Y Chen, Phillip Isola, Bradly C Stadie, Filip Wolski, Jonathan Ho, and Pieter
Abbeel. Evolved policy gradients. arXiv preprint arXiv:1802.04821 , 2018.
Kazuki Irie, Imanol Schlag, Róbert Csordás, and Jürgen Schmidhuber. Going beyond linear transformers
with recurrent fast weight programmers. Advances in Neural Information Processing Systems , 34:7703–
7717, 2021.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeﬀrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361 , 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Louis Kirsch and Jürgen Schmidhuber. Meta learning backpropagation and improving it. arXiv preprint
arXiv:2012.14905 , 2020.
Louis Kirsch and Jürgen Schmidhuber. Self-referential meta learning. In Decision Awareness in Reinforce-
ment Learning Workshop at ICML 2022 , 2022.
Louis Kirsch, Sjoerd van Steenkiste, and Jürgen Schmidhuber. Improving generalization in meta reinforce-
ment learning using learned objectives. arXiv preprint arXiv:1910.04098 , 2019.
Louis Kirsch, Sebastian Flennerhag, Hado van Hasselt, Abram Friesen, Junhyuk Oh, and Yutian Chen.
Introducing symmetries to black box meta reinforcement learning. arXiv preprint arXiv:2109.10781 , 2021.
Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha Sohl-Dickstein. Under-
standing and correcting pathologies in the training of learned optimizers. In International Conference on
Machine Learning , pp. 4556–4565. PMLR, 2019.
Luke Metz, Niru Maheswaranathan, C Daniel Freeman, Ben Poole, and Jascha Sohl-Dickstein. Tasks,
stability, architecture, and compute: Training more eﬀective learned optimizers, and using them to train
themselves. arXiv preprint arXiv:2009.11243 , 2020a.
Luke Metz, Niru Maheswaranathan, Ruoxi Sun, C Daniel Freeman, Ben Poole, and Jascha Sohl-
Dickstein. Using a thousand optimization tasks to learn hyperparameter search strategies. arXiv preprint
arXiv:2002.11887 , 2020b.
Vladimir Mikulik, Grégoire Delétang, Tom McGrath, Tim Genewein, Miljan Martic, Shane Legg, and Pedro
Ortega. Meta-trained agents implement bayes-optimal agents. Advances in neural information processing
systems, 33:18691–18703, 2020.
13Under review as submission to TMLR
Samuel Müller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers
can do bayesian inference. In International Conference on Learning Representations , 2022.
Tung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta learning via se-
quence modeling. In International Conference on Machine Learning , pp. 16569–16594. PMLR, 2022.
Junhyuk Oh, Matteo Hessel, Wojciech M Czarnecki, Zhongwen Xu, Hado van Hasselt, Satinder Singh, and
David Silver. Discovering reinforcement learning algorithms. arXiv preprint arXiv:2007.08794 , 2020.
Pedro A Ortega, Jane X Wang, Mark Rowland, Tim Genewein, Zeb Kurth-Nelson, Razvan Pascanu, Nicolas
Heess, Joel Veness, Alex Pritzel, Pablo Sprechmann, et al. Meta-learning of sequential strategies. arXiv
preprint arXiv:1905.03030 , 2019.
Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefen-
stette, and Tim Rocktäschel. Evolving curricula with regret-based environment design. arXiv preprint
arXiv:2203.01302 , 2022.
Joachim Winther Pedersen and Sebastian Risi. Evolving and merging hebbian learning rules: increasing gen-
eralization by decreasing the number of rules. In Proceedings of the Genetic and Evolutionary Computation
Conference , pp. 892–900, 2021.
Joachim Winther Pedersen and Sebastian Risi. Minimal neural network models for permutation invariant
agents.arXiv preprint arXiv:2205.07868 , 2022.
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization
beyond overﬁtting on small algorithmic datasets. arXiv preprint arXiv:2201.02177 , 2022.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In International Conference on Machine Learning , pp. 8748–8763. PMLR,
2021.
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse? towards
understanding the eﬀectiveness of maml. In International Conference on Learning Representations , 2020.
Esteban Real, Chen Liang, David So, and Quoc Le. Automl-zero: evolving machine learning algorithms
from scratch. In International Conference on Machine Learning , pp. 8007–8019. PMLR, 2020.
Sebastian Risi. The future of artiﬁcial intelligence is self-organizing and self-assembling. sebastianrisi.com ,
2021. URL https://sebastianrisi.com/self_assembling_ai .
Mark Sandler, Max Vladymyrov, Andrey Zhmoginov, Nolan Miller, Andrew Jackson, Tom Madams, et al.
Meta-learning bidirectional update rules. arXiv preprint arXiv:2104.04657 , 2021.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning
withmemory-augmentedneuralnetworks. In International conference on machine learning , pp.1842–1850.
PMLR, 2016.
Jürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the
meta-meta-... hook . PhD thesis, Technische Universität München, 1987.
Jürgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent net-
works.Neural Computation , 4(1):131–139, 1992.
Jürgen Schmidhuber. Reducing the ratio between learning complexity and number of time varying variables
in fully recurrent nets. In International Conference on Artiﬁcial Neural Networks , pp. 460–463. Springer,
1993a.
Jürgen Schmidhuber. A ‘self-referential’weight matrix. In International conference on artiﬁcial neural net-
works, pp. 446–450. Springer, 1993b.
14Under review as submission to TMLR
Jürgen Schmidhuber. Powerplay: Training an increasingly general problem solver by continually searching
for the simplest still unsolvable problem. Frontiers in psychology , 4:313, 2013.
Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth Stanley, and Jeﬀrey Clune. Generative teaching
networks: Accelerating neural architecture search by learning to generate synthetic training data. In
International Conference on Machine Learning , pp. 9206–9216. PMLR, 2020.
Yujin Tang and David Ha. The sensory neuron as a transformer: Permutation-invariant neural networks for
reinforcement learning. Advances in Neural Information Processing Systems , 34:22574–22587, 2021.
Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal
few-shot learning with frozen language models. Advances in Neural Information Processing Systems , 34:
200–212, 2021.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems , pp.
5998–6008, 2017.
Neha Wadia, Daniel Duckworth, Samuel S Schoenholz, Ethan Dyer, and Jascha Sohl-Dickstein. Whitening
and second order optimization both make information in the dataset unusable during training, and can
reduce or prevent generalization. In International Conference on Machine Learning , pp. 10617–10629.
PMLR, 2021.
Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles
Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint
arXiv:1611.05763 , 2016.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of
thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 , 2022.
Zhongwen Xu, Hado P van Hasselt, and David Silver. Meta-gradient reinforcement learning. Advances in
neural information processing systems , 31, 2018.
Andrey Zhmoginov, Mark Sandler, and Maksym Vladymyrov. Hypertransformer: Model generation for
supervised and semi-supervised few-shot learning. In International Conference on Machine Learning , pp.
27075–27098. PMLR, 2022.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier
Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language
models.arXiv preprint arXiv:2205.10625 , 2022.
15Under review as submission to TMLR
A Appendix
A.1 Summary of Insights
Insight 1: It is possible to learn-to-learn with black-box models Eﬀective in-context learning
algorithms can be realized using black-box models with few inductive biases, given suﬃcient meta-training
task diversity and large enough model sizes. To transition to the learning-to-learn regime, we needed at least
213= 8192tasks.
Insight 2: Simple data augmentations are eﬀective for general learning-to-learn The generality
of the discovered learning algorithm can be controlled via the data distribution. Even when large task
distributions are not (yet) naturally available, simple augmentations that promote permutation and scale
invariance are eﬀective.
Insight 3: The meta-learned behavior has phase transitions When increasing the number of tasks,
the meta-learned behavior transitions from task memorization, to task identiﬁcation, to general learning-to-
learn. The last transition is discrete, with two unique clusters.
Insight 4: Large state is more crucial than parameter count The speciﬁc inductive biases of each
architecture matter to a smaller degree. The driving factor behind their ability to learn how to learn is
the size of their state. Furthermore, this suggests that the model size in terms of numbers of parameters
plays a smaller role in the setting of learning-to-learn and Transformers have beneﬁted in particular from an
increase in state size by self-attention. In non-meta-learning sequence tasks parameter count is thought to
be the performance bottleneck (Collins et al., 2016). Beyond learning-to-learn, this likely applies to other
tasks that rely on processing and storing large amounts of sequence-speciﬁc information.
A.2 Limitations
Varying input and output sizes Compared to many previous works in meta-learning (Andrychowicz
et al., 2016; Finn et al., 2017; Kirsch & Schmidhuber, 2020), the discovered learning algorithms are only
applicabletoanarbitraryinputandoutputsizebyusingrandomprojections. Thismaymakeitmorediﬃcult
to apply the learning algorithm to a new, unseen problem. This problem also applies to Transformers applied
to multiple tasks and modalities. Related work has solved this problem by tokenizing inputs to compatible,
uniﬁed representations (Chowdhery et al., 2022). We expect these techniques or others to be useful in the
learning-to-learn context too.
Processing large datasets Learning algorithms often process millions of inputs before outputting the
ﬁnal model. In the black-box setting, this is still diﬃcult to achieve. Recurrency-based models usually
suﬀer from accumulating errors, whereas Transformers computational complexity grows quadratically in the
sequence length. Additional work is required to build models capable of processing and being trained on
long sequences. Alternatively, parallel processing, similar to batching in learning algorithms, may be a useful
building block.
A.3 Architectural Details and Hyper-parameters
Transformer details By default, all Transformers have a key, value, and query size of 32,8heads, and 4
layers, and model size of NM= 256. The model size deﬁnes the dimensionality of each token, and the MLP
between layers scales this size up to a hidden representation of 4×NMwhereNMcorresponds to the model
size.
Outer-product LSTM We slightly modify an LSTM by replacing the context state with an outer-product
update and inner-product read-out.
x_and_h = jnp . concatenate ([ inputs , prev_state . hidden ] , axis= −1)
gated = hk. Linear(8 ∗size∗self .num_heads)(x_and_h)
gated = gated . reshape ((batch_size , self .num_heads, 8 ∗size ))
gated = checkpoint_name(gated , ’gated ’)
16Under review as submission to TMLR
# i = input , g = cell_gate , f = forget_gate ,
# q = query , o = output_gate
sizes = (3∗size , 3∗size , size , size )
indices = np.cumsum( sizes [: −1])
k1, k2, q, o = jnp . split (gated , indices , axis= −1)
scale = jax .nn. softplus (
hk. get_parameter( ’key_scale ’ , shape=(), dtype=k1.dtype ,
init=jnp . zeros ))
i , g, f = jnp .einsum( ’bhki ,bhkj −>kbhij ’ ,
jax .nn.tanh( split_axis (k1, (3 , size ))) ∗scale ,
jax .nn.tanh( split_axis (k2, (3 , size ))))
f = jax .nn. sigmoid( f + 1) # Forget bias
c = f∗prev_state . cell + jax .nn. sigmoid( i ) ∗g
read = jnp .einsum( ’bhij ,bhi −>bhj ’ , c , q)
h = hk. Flatten ()( jax .nn. sigmoid(o) ∗jnp .tanh(read))
VSML We use a version of VSML with a single layer and self-messages (Kirsch et al., 2021) of size 8. Each
LSTM has a hidden size of 16. For each LSTM update we use two micro-ticks. We train on 225tasks with a
90%biased permutation distribution. The task batch size is 8. All images are scaled to a size of 32×32×3
VSMLwithoutsymmetries BeforeactivationsarefedtoastandardinstantiationofVSML,allinputsare
projected using a learnable linear projection. Logits are generated using another linear projection, followed
by a softmax. We use a version of VSML with a single layer and self-messages (Kirsch et al., 2021) of size
8. The LSTMs are on a grid of k×kLSTMs, where k∈{1,2,4,8,16,24}. Each LSTM has a hidden size of
64. For each LSTM update we use two micro-ticks. We train on 225tasks with a 90%biased permutation
distribution. The task batch size is 128. All images are scaled to a size of 14×14.
LSTM For the results in Table 2, we used a hidden size of 256and105optimization steps. Larger hidden
sizes were harder to optimize. We train on 225tasks with a 90%biased permutation distribution. The task
batch size is 128. All images are scaled to a size of 32×32×3
A.4 Experimental Details
Most experiments can be run on a single GPU, some require 16GPUs due to sequence length and large
batch sizes, with suﬃcient GPU memory (around 16GB each). Some experiments, such as Figure 2, require
up to 1000runs of that kind to produce the ﬁnal heat-map.
Input normalization Each dataset is z-normalized by its mean and standard deviation across all examples
and pixels.
Number of seeds and shading If not noted otherwise, line plots use 8seeds for meta-training and at
least 512seeds for meta-testing. Shading indicates 95%conﬁdence intervals.
Random dataset To test the meta-learned learning algorithms on a synthetically generated problem, we
generate classiﬁcation datasets of 10datapoints where the input x∈R32×32×3is drawn from a uniform
distribution between 0and1. For each datapoint, labels yare drawn from a uniform categorical distribution
of10classes.
Figure 2 The MLP has two hidden layers of varying size with relu activations. The Transformer has the
default parameters as deﬁned above.
Figure 3 We use a transformer model with a model size of 256. We train on 225tasks with a 90%biased
permutation distribution. The task batch size is 128. All images are scaled to a size of 32×32×3Inputs
are z-normalized across the dataset and all input dimensions.
Table 2 The SGD baseline was obtained by sweeping over learning rates from 10−4to0.5, optimizers SGD,
Adam and Adam with weight decay, one or two layers, and hidden sizes of 32,64, or128on MNIST. The best
conﬁguration (most sample eﬃcient) corresponds to a learning rate of 10−3, Adam, and no hidden layers.
17Under review as submission to TMLR
20232629212215218221224
Number of tasks0.1
0.00.10.20.30.40.50.60.7Accuracy improvement within sequenceT ask
memorizationT ask
identificationGeneral
learning to learn
Seen MNIST
Unseen MNIST
Unseen FashionMNIST
Unseen KMNIST
20232629212215218221224
Number of tasks0.2
0.00.20.40.6Accuracy improvement within sequenceT ask
memorizationT ask
identificationGeneral
learning to learn
Seen FashionMNIST
Unseen MNIST
Unseen FashionMNIST
Unseen KMNIST
20232629212215218221224
Number of tasks0.1
0.00.10.20.30.40.50.6Accuracy improvement within sequenceT ask
memorizationT ask
identificationGeneral
learning to learn
Seen KMNIST
Unseen MNIST
Unseen FashionMNIST
Unseen KMNIST
Figure 11: Transformers exhibit three diﬀerent phases in terms of meta-learned behavior on
various meta training datasets. (1) When training on a small number of tasks, tasks are memorized.
(2) Tasks from the training distribution are identiﬁed, which is evident as a within-sequence increase of
performance. (3) When training across many tasks, we discover a learning algorithm that generalizes to
unseen tasks and unseen datasets.
20232629212215218221224
Number of tasks0.00.20.40.6Accuracy improvement within sequenceT ask
memorizationT ask
identificationGeneral
learning to learn
Seen MNIST
Unseen MNIST
Unseen FashionMNIST
Unseen KMNIST
Unseen CIFAR10
Unseen SVHN
20232629212215218221224
Number of tasks0.00.10.20.30.40.50.60.7Accuracy improvement within sequenceT ask
memorizationT ask
identificationGeneral
learning to learn
Seen FashionMNIST
Unseen MNIST
Unseen FashionMNIST
Unseen KMNIST
Unseen CIFAR10
Unseen SVHN
20232629212215218221224
Number of tasks0.1
0.00.10.20.30.40.50.60.7Accuracy improvement within sequenceT ask
memorizationT ask
identificationGeneral
learning to learn
Seen CIFAR10
Unseen MNIST
Unseen FashionMNIST
Unseen KMNIST
Unseen CIFAR10
Unseen SVHN
Figure 12: The phase transitions also happen when using the embeddings from Section 4.4. This
enables faster learning on datasets such as CIFAR10 with only 100 training examples while still generalizing
to various datasets.
SGD performs updates online on each one out of the 100 data points. MAML is equivalent to SGD up to
the diﬀerence that we meta-train the weight initialization according to Equation 2 where θare the initial
parameters of the classiﬁer that is then updated using SGD at meta-test time. All black-box approaches do
not use gradient descent at meta-test time. All meta-learning approaches where meta-trained and tuned via
grid search on MNIST.
Figure 4 Input normalization is disabled.
Figure 6 The Transformer uses a task batch size of 512.
Figure 7 Trained on 216tasks generated from FashionMNIST with labels fully permuted.
Figure 8 Trained on 216tasks generated from FashionMNIST with labels fully permuted.
Figure 9 Trained on 216tasks generated from FashionMNIST with label permutations varied.
Figure 5 We trained a Transformer with model size 64and32seeds for each number-of-tasks-conﬁguration.
A.5 Additional Experiments
Phase transitions on other meta training datasets In Figure 2 and Figure 4 we observe a fairly
discrete transition between task identiﬁcation and general learning-to-learn as a function of the number of
tasks. We show these phase transitions on more meta training datasets in Figure 11. When using ImageNet
embeddings as discussed in Section 4.4, we observe similar phase transitions also on CIFAR10 and other
datasets as shown in Figure 12.
18Under review as submission to TMLR
242628210212214
State size0.00.20.40.60.81.0AccuracySeen MNIST
(seen task & seen dataset)
242628210212214
State sizeUnseen MNIST
(unseen task, seen dataset)
242628210212214
State sizeUnseen FashionMNIST
(unseen task, unseen dataset)
LSTM
Transformer
Outer-product LSTM
VSML without symmetries
(a)
210212214
Parameter count0.00.20.40.60.81.0AccuracySeen MNIST
(seen task & seen dataset) (b)
Figure 13: The state size (accessible memory) of an architecture most strongly predicts its
performance as a general-purpose learning algorithm. (a) A large state is crucial for learning-to-
learn to emerge. (b)The parameter count correlates less well with learning capabilities.
50 100 200 400
sequence_length0.00.10.20.30.40.50.60.70.8accuracy
task
Seen FashionMNIST
Unseen FashionMNIST
Unseen MNIST
Figure 14: Increasing the sequence length during meta-training and meta-testing improves the predictive
performance of the ﬁnal query in the sequence. Error bars indicate 95%conﬁdence intervals.
Large State is Crucial for Learning We show that for learning-to-learn the size of the memory NS
at meta-test time (or state more generally) is particularly important in order to be able to store learning
progress. We test this by training several architectures with various NSin our meta-learning setting. In
addition to Figure 6, Figure 13 show meta-test performance on more tasks and datasets.
Sequence length In all experiments of the main paper we have meta-trained on a sequence length (number
of examples) of 100. This is a small training dataset compared to many human-engineered learning algo-
rithms. In general, as long as the learning algorithm does not overﬁt the training data, more examples should
increase the predictive performance. In Figure 14 we investigate how our model scales to longer sequence
lengths. We observe that the ﬁnal accuracy of the last query in the sequence consistently increases with
longer sequences. The generalization to longer sequences than those seen during meta-training is another
important direction for future work.
Gradient and update statistics To better understand the properties of the loss plateau, we visualize
diﬀerent statistics of the gradients, optimizer, and updates. In Figure 15, we track the exponential moving
average statistics of Adam before the loss plateau and after (dashed vertical line). In Figure 16 we investigate
how gradients diﬀer between settings with a plateau and settings with a biased distribution where the plateau
is avoided. We plot the cosine similarity between consecutive optimization steps, the gradient L2-norm, and
the similarity and norm of the weight updates after normalization with Adam. The statistics are plotted
cumulatively or smoothed with a Gaussian ﬁlter for better readability. The gradient and update cosine
similarity diﬀer only marginally between cases with a plateau and cases without. We observe that the
gradient L2-norm in the plateau is half as big as in the biased distribution case, although the updates that
19Under review as submission to TMLR
0 10000 20000 30000 40000 50000
step0.000.010.020.030.040.050.060.07L2 normnorm_type = MovAvg gradient norm
0 10000 20000 30000 40000 50000
step0.00000.00050.00100.00150.00200.00250.0030L2 normnorm_type = MovAvg gradient squared norm
Figure 15: L2-norms of the gradient and squared gradient exponential moving average in Adam. The dashed
line corresponds to the loss drop at the end of the loss plateau.
Adam applies are going towards zero. This also results in not moving far from parameter initialization
when in the plateau. We hypothesize this has to do with varying gradient norms when looking at individual
parameter tensors (Figure 17). We observe that the gradients have a small norm for most tensors, except
for the last layer.
Batch size and number of tasks inﬂuence on plateau length Instead of looking at the plateau length
in terms of the number of steps (Figure 8), we may also be concerned with the total number of tasks seen
within the plateau. This is relevant in particular when the task batch is not processed fully in parallel but
gradients are accumulated. Figure 18 shows the same ﬁgure but with the number of tasks in the plateau on
the y-axis instead. It can be observed that larger batch-sizes actually increase the data requirement to leave
the plateau, despite decreasing the plateau in terms of the number of optimization steps. Similarly, a larger
task training distribution requires a larger number of tasks to be seen within the plateau.
Adjusting Adam’s /epsilon1or changing the optimizer As discussed in the main paper and visualized in
Figure 19b, decreasing /epsilon1signiﬁcantly shortens the plateau. This is due to the rescaling of very small gradient
magnitudes being limited by /epsilon1. At the same time it incurs some instability. Directly normalizing the gradient
by applying the sign function element-wise (Figure 19a) to the exponential gradient average shortens the
plateau even further.
Whenmemorizationhappens, canweelicitgrokking? InFigure8awehaveseenthataninsuﬃciently
large task distribution can lead to memorization instead of general learning-to-learn. At the same time,
Figure 9 showed that biasing the data distribution is helpful to avoid loss plateaus. Power et al. (2022)
observed a phenomenon which they called “grokking” in which even after having converged in terms of
training loss, test loss may suddenly decrease. Large amounts of regularization, like weight decay with a
coeﬃcient of 1.0were found to facilitate this behavior. Is grokking connected to the optimization behavior
we observe, and if so, do similar interventions help in our setting? We look in particular at the boundary
of memorization and generalization ( 214= 16384) where doubling the number of tasks a few more times
would lead to generalization. Figure 20 shows three task settings, 210,214,216, and three diﬀerent weight
decay coeﬃcients, 0.01,0.1,1.0. The setting of 216tasks shows generalization by default and only serves as
a baseline for the weight decay coeﬃcient analysis. In the cases of memorization due to too few tasks, we
have not been able to produce grokking behavior.
Optimization diﬃculties in VSML Previous work has observed several optimization diﬃculties: Slower
convergence, local minima, unstable training, or loss plateaus at the beginning of training. Figure 21 shows
some of these diﬃculties in the context of VSML (Kirsch & Schmidhuber, 2020). Because VSML has
permutation invariance and parameter sharing built into the architecture as an inductive bias, changing the
20Under review as submission to TMLR
0.81.01.21.41.61.82.02.2training_losspermute_labels_prob
0.0
0.1
0.9
1.0
1.00
0.75
0.50
0.25
0.000.250.500.751.00smooth_grad_sim
0.00.10.20.30.40.5smooth_grad_norm
050100150200init_param_dist
020406080100cum_grad_sim
020406080100120140cum_grad_norm
1.00
0.75
0.50
0.25
0.000.250.500.751.00smooth_update_sim
0.000.050.100.150.200.250.30smooth_update_norm
0 10000 20000 30000 40000 50000
step0100200300400cum_update_sim
0 10000 20000 30000 40000 50000
step020406080100cum_update_norm
(a) (b) (c)
Figure 16: Gradient and Adam update statistics for diﬀerently biased data distributions. (a)Plateaus in
the loss are inﬂuenced by the bias in the data distribution. Plateaus result in moving away slowly from the
parameter initialization. (b)The cosine similarity of both gradients and updates in consecutive steps is only
marginally diﬀerent with or without a loss plateau. (c)While the gradient norm is about half as big when
a plateau exists, the updates are going towards zero.
21Under review as submission to TMLR
109
107
105
103
101
tensor_grad_normtransformer/ln_f/offset/grad_norm
transformer/h3_mlp/linear/b/grad_norm
transformer/h3_ln_2/offset/grad_norm
transformer/h3_mlp/linear_1/w/grad_norm
transformer/h3_ln_1/scale/grad_norm
transformer/h3_attn/value/b/grad_norm
transformer/h3_attn/query/w/grad_norm
transformer/h3_attn/query/b/grad_norm
transformer/h3_attn/linear/w/grad_norm
transformer/h3_attn/linear/b/grad_norm
transformer/h3_attn/key/b/grad_norm
transformer/h2_mlp/linear/w/grad_norm
transformer/h2_ln_2/offset/grad_norm
transformer/h2_attn/value/w/grad_norm
transformer/h2_attn/value/b/grad_norm
transformer/h2_attn/query/w/grad_norm
transformer/h2_attn/query/b/grad_norm
transformer/h2_attn/linear/w/grad_norm
transformer/h0_ln_1/scale/grad_norm
transformer/h3_mlp/linear/w/grad_norm
transformer/h1_mlp/linear_1/b/grad_norm
transformer/h1_mlp/linear/w/grad_norm
transformer/h2_attn/key/b/grad_norm
transformer/h1_mlp/linear/b/grad_norm
transformer/h1_ln_2/offset/grad_norm
transformer/h1_ln_1/scale/grad_norm
transformer/h2_mlp/linear_1/b/grad_norm
transformer/h1_mlp/linear_1/w/grad_norm
transformer/h1_ln_1/offset/grad_norm
transformer/h0_attn/query/b/grad_norm
transformer/h1_attn/query/b/grad_norm
transformer/h1_attn/key/w/grad_norm
transformer/h0_mlp/linear_1/b/grad_norm
transformer/h1_attn/value/b/grad_norm
transformer/h0_attn/value/w/grad_norm
transformer/h0_attn/value/b/grad_norm
transformer/h0_attn/query/w/grad_norm
transformer/h0_ln_2/offset/grad_norm
transformer/h1_attn/value/w/grad_norm
transformer/h0_attn/linear/w/grad_norm
transformer/h2_ln_1/offset/grad_norm
transformer/h0_attn/key/b/grad_norm
linear_out/w/grad_norm
linear_out/b/grad_norm
linear/w/grad_norm
transformer/h1_attn/key/b/grad_norm
transformer/h1_attn/linear/b/grad_norm
transformer/h0_attn/key/w/grad_norm
transformer/h2_attn/linear/b/grad_norm
transformer/h1_ln_2/scale/grad_norm
transformer/h0_ln_2/scale/grad_norm
transformer/h3_ln_1/offset/grad_norm
transformer/h1_attn/query/w/grad_norm
transformer/h3_attn/key/w/grad_norm
transformer/h2_ln_2/scale/grad_norm
transformer/h2_ln_1/scale/grad_norm
transformer/h0_mlp/linear_1/w/grad_norm
transformer/h0_mlp/linear/w/grad_norm
transformer/h0_attn/linear/b/grad_norm
transformer/h3_ln_2/scale/grad_norm
~/pos_embs/grad_norm
transformer/h3_mlp/linear_1/b/grad_norm
transformer/h2_mlp/linear/b/grad_norm
transformer/ln_f/scale/grad_norm
linear/b/grad_norm
transformer/h2_mlp/linear_1/w/grad_norm
transformer/h2_attn/key/w/grad_norm
transformer/h3_attn/value/w/grad_norm
transformer/h1_attn/linear/w/grad_norm
transformer/h0_ln_1/offset/grad_norm
transformer/h0_mlp/linear/b/grad_norm
tensor
0.1
 0.0 0.1 0.2 0.3
tensor_grad_sim~/pos_embs/grad_sim
transformer/ln_f/scale/grad_sim
transformer/ln_f/offset/grad_sim
transformer/h3_mlp/linear_1/b/grad_sim
transformer/h3_mlp/linear/w/grad_sim
transformer/h3_mlp/linear/b/grad_sim
transformer/h3_ln_2/scale/grad_sim
transformer/h3_attn/query/w/grad_sim
transformer/h3_attn/linear/w/grad_sim
transformer/h3_attn/key/w/grad_sim
transformer/h3_attn/key/b/grad_sim
transformer/h2_mlp/linear_1/w/grad_sim
transformer/h2_mlp/linear_1/b/grad_sim
transformer/h2_ln_2/scale/grad_sim
transformer/h3_mlp/linear_1/w/grad_sim
transformer/h2_ln_2/offset/grad_sim
transformer/h2_ln_1/scale/grad_sim
transformer/h2_attn/value/w/grad_sim
transformer/h2_attn/linear/b/grad_sim
transformer/h2_attn/key/w/grad_sim
transformer/h1_attn/value/w/grad_sim
transformer/h1_mlp/linear_1/w/grad_sim
transformer/h2_attn/linear/w/grad_sim
transformer/h1_mlp/linear_1/b/grad_sim
transformer/h0_mlp/linear_1/w/grad_sim
transformer/h3_attn/value/w/grad_sim
transformer/h2_ln_1/offset/grad_sim
transformer/h1_mlp/linear/b/grad_sim
transformer/h1_ln_2/offset/grad_sim
transformer/h1_ln_1/scale/grad_sim
transformer/h1_ln_2/scale/grad_sim
transformer/h1_attn/linear/b/grad_sim
transformer/h1_attn/key/w/grad_sim
transformer/h2_attn/key/b/grad_sim
transformer/h0_mlp/linear/w/grad_sim
transformer/h0_ln_2/scale/grad_sim
transformer/h0_ln_2/offset/grad_sim
transformer/h0_attn/query/w/grad_sim
transformer/h0_ln_1/scale/grad_sim
transformer/h0_ln_1/offset/grad_sim
transformer/h1_attn/key/b/grad_sim
transformer/h0_attn/query/b/grad_sim
transformer/h0_attn/linear/w/grad_sim
transformer/h0_attn/linear/b/grad_sim
transformer/h2_attn/query/w/grad_sim
linear/w/grad_sim
linear/b/grad_sim
transformer/h0_attn/key/w/grad_sim
transformer/h2_attn/query/b/grad_sim
transformer/h1_attn/query/w/grad_sim
transformer/h0_mlp/linear_1/b/grad_sim
transformer/h0_attn/value/w/grad_sim
transformer/h1_mlp/linear/w/grad_sim
transformer/h3_ln_2/offset/grad_sim
transformer/h3_attn/query/b/grad_sim
transformer/h1_attn/query/b/grad_sim
linear_1/b/grad_sim
transformer/h0_mlp/linear/b/grad_sim
transformer/h1_ln_1/offset/grad_sim
transformer/h2_attn/value/b/grad_sim
transformer/h3_attn/linear/b/grad_sim
transformer/h0_attn/value/b/grad_sim
linear_1/w/grad_sim
transformer/h2_mlp/linear/w/grad_sim
transformer/h3_ln_1/scale/grad_sim
transformer/h2_mlp/linear/b/grad_sim
transformer/h1_attn/linear/w/grad_sim
transformer/h3_attn/value/b/grad_sim
transformer/h0_attn/key/b/grad_sim
transformer/h3_ln_1/offset/grad_sim
transformer/h1_attn/value/b/grad_simtensor
Figure 17: Gradient L2 norms (left) and gradient cosine similarity for consecutive optimization steps (right)
for diﬀerent parameter tensors. The last (output) layer has the largest gradients. Most other gradients are
small.
22Under review as submission to TMLR
2526272829210211212
Task batch size220221222223224225226227Plateau number of tasks seen# of tasks
216
218
220
222
224
217219221223225
Number of tasks220221222223224225226227Plateau number of tasks seenTask batch size
25
27
29
211
Figure 18: Instead of plotting the loss plateau length in terms of optimization steps, we look at the total
number of tasks seen within the plateau as a function of the task batch size and the number of tasks in the
training distribution. An increase in the task batch size leads to more tasks to be processed to leave the
plateau.
2526272829210211212
Task batch size210211212213214215Plateau length# of tasks
216
218
220
222
224
Optimizer
Adam
Sign
(a)
2526272829210211212
Task batch size211212213214215Plateau length# of tasks
216
218
220
222
224
1024
1016
108
 (b)
2526272829210211212
Task batch size219221223225227Plateau number of tasks seen# of tasks
216
218
220
222
224
Optimizer
Adam
Sign
2526272829210211212
Task batch size220221222223224225226227Plateau number of tasks seen# of tasks
216
218
220
222
224
1024
1016
108
217219221223225
Number of tasks219221223225227Plateau number of tasks seenTask batch size
25
27
29
211
Optimizer
Adam
Sign
217219221223225
Number of tasks220221222223224225226227Plateau number of tasks seenTask batch size
25
27
29
211
1024
1016
108
Figure 19: (a)When replacing Adam with a sign normalization of the gradient or (b)reducing/epsilon1the plateau
length is signiﬁcantly shorter.
23Under review as submission to TMLR
0.51.01.52.02.53.03.54.0lossnum_tasks = 1024 | weight_decay = 0.01 num_tasks = 1024 | weight_decay = 0.1 num_tasks = 1024 | weight_decay = 1.0
0.51.01.52.02.53.03.54.0lossnum_tasks = 16384 | weight_decay = 0.01 num_tasks = 16384 | weight_decay = 0.1 num_tasks = 16384 | weight_decay = 1.0
0 25000 50000 75000 100000 125000 150000 175000 200000
step0.51.01.52.02.53.03.54.0lossnum_tasks = 65536 | weight_decay = 0.01
0 25000 50000 75000 100000 125000 150000 175000 200000
stepnum_tasks = 65536 | weight_decay = 0.1
0 25000 50000 75000 100000 125000 150000 175000 200000
stepnum_tasks = 65536 | weight_decay = 1.0task
Training loss
Seen FashionMNIST
Unseen FashionMNIST
Unseen MNIST
Figure 20: We investigate whether grokking as deﬁned in Power et al. (2022) can be produced when we
observe memorization on a smaller numbers of tasks. This would correspond to the test loss decreasing long
after the training loss has converged. We have not been able to elicit this behavior when looking at diﬀerent
numbers of tasks and weight decay coeﬃcients.
24Under review as submission to TMLR
0.500.751.001.251.501.752.002.25Training loss# of tasks = 20 | 1 layers # of tasks = 20 | 2 layers # of tasks = 20 | 3 layers # of tasks = 20 | 4 layers
0k 10k 20k 30k 40k 50k
Step0.500.751.001.251.501.752.002.25Training loss# of tasks = 225 | 1 layers
0k 10k 20k 30k 40k 50k
Step# of tasks = 225 | 2 layers
0k 10k 20k 30k 40k 50k
Step# of tasks = 225 | 3 layers
0k 10k 20k 30k 40k 50k
Step# of tasks = 225 | 4 layers0% permuted
10% permuted
100% permuted
Figure 21: Loss plateaus and slow convergence with deeper variants of VSML.
number of tasks has only a small eﬀect. We observe that in particular deeper architectures make meta-
optimization more diﬃcult.
25