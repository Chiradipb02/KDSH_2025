Under review as submission to TMLR
You Only Debias Once: Towards Flexible Accuracy-Fairness
Trade-offs at Inference Time
Anonymous authors
Paper under double-blind review
Abstract
Deep neural networks are prone to various bias issues, jeopardizing their applications for
high-stake decision-making. Existing fairness methods typically offer a fixed accuracy-fairness
trade-off at inference time To Reviewer
bMqY: Re-
quested
Change 1:
We revise
"the infer-
ence time"
to "infer-
ence time"
throughout
the paper.
And we only
highlight
this one., since the weight of the well-trained model is a fixed point
(fairness-optimum) in the weight space1. Nevertheless, more flexible accuracy-fairness trade-
offs at inference time are practically desired since: 1) stakes of the same downstream task
can vary for different individuals, and 2) different regions have diverse laws or regularization
for fairness. If using the previous fairness methods, we have to train multiple models, each
offering a specific level of accuracy-fairness trade-off. This is often computationally expensive,
time-consuming, and difficult to deploy, making it less practical for real-world applications.
To address this problem, we propose You Only Debias Once (YODO) to achieve in-situ flexible
accuracy-fairness trade-offs at inference time, using a single model that trained only once2.
Instead of pursuing one individual fixed point (fairness-optimum) in the weight space, we aim
to find a ‚Äúline‚Äù in the weight space that connects the accuracy-optimum and fairness-optimum
points using a single model. Points (models) on this line implement varying levels of accuracy-
fairness trade-offs. at inference time, by manually selecting the specific position of the learned
‚Äúline‚Äù, our proposed method can achieve arbitrary accuracy-fairness trade-offs for different
end-users and scenarios. Experimental results on tabular and image datasets show that YODO
achieves flexible trade-offs between model accuracy and fairness, at ultra-low overheads. Our
codes are anonymously available at https://anonymous.4open.science/r/yodo-BB81 .
1 Introduction
Deep neural networks (DNNs) are prone to be biased with respect to sensitive attributes (Mehrabi et al., 2021;
Duetal.,2020;Dworketal.,2012;Binns,2018;Caton&Haas,2020;Barocasetal.,2017), whichraisesconcerns
about the application of deep learning model on high-stake decision-making, such as credit scoring (Petrasic
et al., 2017), criminal justice (Berk et al., 2021), job market (Hu & Chen, 2018), healthcare (Rajkomar et al.,
2018; Ahmad et al., 2020; Bjarnad√≥ttir & Anderson, 2020; Grote & Keeling, 2022) and education B√∏yum
(2014); Brunori et al. (2012); Kizilcec & Lee (2022). Decisions made by these biased algorithms could have a
long-lasting even life-long impact on people‚Äôs lives and may affect underprivileged groups negatively. This
concern about biased models has aroused wide interest from both academic and industrial researchers in
analyzing and achieving fairness for DNNs.
Many studies have shown that achieving fairness in deep learning models involves a trade-off with model
accuracy (Bertsimas et al., 2011; Menon & Williamson, 2018; Zhao & Gordon, 2019; Bakker et al., 2019). This
has led to the development of various methods aimed at addressing fairness in deep learning models (Edwards
& Storkey, 2015; Louppe et al., 2017; Chuang & Mroueh, 2020), but typically these methods have been
designed to achieve a fixed level of accuracy-fairness trade-off. In real-world applications, however, the
1Theweight space of a model refers to the space of all possible values that the model‚Äôs trainable parameters can take. If a
neural network has mtrainable parameters, then the dimension of the weight space is m. Each point, a m-dimensional vector,
in the weight space corresponds to a specific model. For visualization purposes, the weight space is often reduced to a 2D space,
as shown in Figure 1.
2This is also what we mean by one-time training in this paper.
1Under review as submission to TMLR
HighFairnessLowAccuracy
HighAccuracyLowFairness2DWeightSpaceNetworkSubspacesLce+‚ÜµLfx
¬∞50050100150200¬∞50050100150
18181818192023271e+10
¬∞50050100150200¬∞50050100150
0.00690.0260.0610.160.441.23.5101e+10
Accuracy ( Error Rate ‚Üì)Fairness (          ‚Üì)Œîùê∑ùëÉ(a) The Proposed Model(b) The Loss Landscapeof YODO
Figure 1: (a):The overview of our proposed method. 2D Weight Space indicated the landscape of model
accuracy and fairness.
 indicates the accuracy-optimum weight with high accuracy but low fairness, and
indicates the fairness-optimum weight with low accuracy but high fairness. Network Subspaces shows the
different subspaces correspond with different objectives (i.e., accuracy Lceand fairnessLf).(b): The loss
landscape of the model accuracy (error rate) and fairness ( ‚àÜDP) in the same weight space of our proposed
method. The weight space is reduced to two dimensions (Garipov et al., 2018). The different points indicate
different objectives,
 indicates the accuracy‚Äìoptimum endpoint in the weight space, while
 indicates the
fairness-optimum endpoint in the weight space. The dataset is ACS-Iwith gender as sensitive attribute.
appropriate trade-off between accuracy and fairness may vary depending on the context and the needs of
different stakeholders/regions. Thus, it is important to have flexible trade-offs at inference time due to:
1) Downstream tasks with different stakes can have varying fairness requirements, depending
on the individuals involved. According to a survey by Srivastava et al. (2019), people prioritize accuracy
over fairness when the stakes are high in certain domains. For example, in healthcare (e.g., cancer prediction),
accuracy should be favored over fairness, as prioritizing fairness can lead to "fatal" consequences, such as
missing cancer diagnoses at higher rates (Chen et al., 2018; Srivastava et al., 2019). Since the stakes of
downstream tasks differ among individuals, the expected level of fairness may also vary significantly. Thus,
in high-stakes domains, the trade-off between accuracy and fairness should be flexible and controllable at
inference time.
2) Different regions have diverse laws or regulations for fairness. The use of decision-making systems
is regulated by local laws and policies. However, countries may exhibit differences in the importance of
fairness in various applications. For example, the labor market and employees may expect fairness to be much
stronger in Germany than in North America (Gerlach et al., 2008). Therefore, developers of machine learning
models should consider the varying fairness requirements when applying their algorithms in different regions.
Unfortunately, while urgently needed, flexible trade-offs between model accuracy and fairness at inference
time remain underexplored. Thus, we aim to answer the following question in this paper:
Can we achieve flexible accuracy-fairness trade-offs at inference time using a single model
that trained only once?
It is an open but challenging problem to achieve flexible accuracy-fairness trade-offs. One solution is to train
multiple models for different trade-offs, which limits its practical use due to the significant training time and
memory overhead. Another alternative solution is the post-processing method for fairness, which may lead to
suboptimal model accuracy and require sensitive attributes at inference time (Woodworth et al., 2017).
To address the above question, we propose You Only Debias Once (YODO) to achieve flexible fairness-accuracy
trade-offs via learning an objective-diverse neural network subspace that contains accuracy-optimum and
fairness-optimum points in the weight space. As illustrated in Figure 1(a), we design an objective-diverse
neural network subspace, which contains two endpoints (the red network and the green network in the weight
space.) The two endpoints3are encouraged to accuracy-optimum (
 in Figure 1) and fairness-optimum (
 in
Figure 1) solutions, respectively. During the training time, we encourage the two endpoints to converge to
accuracy-optimum and fairness-optimum points in the weight space, and also encourage the ‚Äúline‚Äù between
the two endpoints to achieve transitional solutions. Specifically, we combine the two endpoints with random
weight and then optimize them with a corresponding combination of accuracy and fairness objectives. at
inference time, we enable the model to achieve arbitrary accuracy-fairness trade-offs with the trained model
3One endpoint represents a specific model parameter.
2Under review as submission to TMLR
by manually selecting a trade-off coefficient to determine the model weight (namely, select a position of the
line). Our contributions are highlighted as follows:
‚Ä¢We propose a new target, achieving in-situ flexible accuracy-fairness trade-offs at inference time while
only trained once, which adaptively meets the requirements of flexible fairness in real-world applications.
‚Ä¢We achieve the above in-situ flexible accuracy-fairness trade-offs by introducing an objective-diverse
neural network subspace to achieve accuracy-fairness trade-offs. The subspace has two different endpoints
in weight space, which are optimized for accuracy-optimum and fairness-optimum at different endpoints
of subspaces and the ‚Äúline‚Äù between the two endpoints to achieve transitional solutions. Thus it can
achieve flexible trade-offs by customizing the endpoints at inference time, with ultra-low overheads.
‚Ä¢Experimental results validate the effectiveness and efficiency of the proposed method on both tabular
and image data. The result of experiments shows that our proposal achieves comparable performance
with only trained once with various neural network architectures when compared to trained models for
single use or fairness on a single attribute only. The visualization experiment provides the insight that
our method gradually learns fair representation, which guarantees fair prediction.
2 Related Works
To Reviewer
bMqY: Con-
cern 5: we
move litera-
ture review
earlier.In this section, we discuss three areas of research that are related to our work, namely group fairness,
accuracy-fairness trade-offs, and neural network subspace learning.
Fairness in Machine Learning. Recently, algorithmic fairness (Pogodin et al., 2023; Cruz et al., 2023; Guo
et al., 2023; Li et al., 2023; Pham et al., 2023; Jung et al., 2023; Han et al., 2023; Chung et al., 2023; Roh
et al., 2021; Guldogan et al., 2023; Roh et al., 2023; Schrouff et al., 2022a;b; Vogel et al., 2020; Coston et al.,
2020; Maheshwari et al., 2022) is required legally or morally in machine learning systems and various fairness
definitions in machine learning systems have been proposed to meet the requirement of fairness expectation.
The fairness can typically be classified into individual fairness orgroup fairness , which can be achieved via
pre/in/post-processing. In this paper, we focus on the in-processing group fairness, which measures the
statistical parity between subgroups defined by the sensitive attributes, such as gender or race (Zemel et al.,
2013; Louizos et al., 2015; Hardt et al., 2016; Chuang & Mroueh, 2020; Zafar et al., 2017; Madras et al., 2018;
Joseph et al., 2016). Nevertheless, these constraints are trained and satisfied during training, the model may
expect different accuracy-fairness trade-offs at inference time. In contrast, Our proposed method, YODO,
aims to address this issue by enabling flexible accuracy-fairness trade-offs at inference time.
Accuracy-Fairness Trade-offs. Many existing works investigate the trade-offs between the model accuracy
and fairness(Cooper et al., 2021; Kim et al., 2020; Bertsimas et al., 2011; Menon & Williamson, 2018; Liu
& Vicente, 2020; Kleinberg, 2018; Haas, 2019; Rothblum & Yona, 2021; Wei & Niethammer, 2020; Barlas
et al., 2021; Dutta et al., 2020; Maity et al., 2020; Chouldechova & Roth, 2018; Zhang et al., 2022; Vogel
et al., 2021). Maity et al. (2020) discusses the existence of the accuracy-fairness trade-offs. (Kim et al.,
2020; Dressel & Farid, 2018; Zliobaite, 2015; Dutta et al., 2020; Blum & Stangl, 2019; Wick et al., 2019)
shows that fairness and model accuracy conflict with each another, and achieved fairness often comes with a
necessary cost in loss of model accuracy. Cooper et al. (2021); Kim et al. (2020) re-examines the trade-offs
and concludes that unexamined assumptions may result in emergent unfairness. These studies emphasize the
need to develop models that balance accuracy and fairness while also considering the unique characteristics
of the data and the potential for emergent biases. Navon et al. (2021) proposed Pareto-Front Learning
(PFL), which employs Pareto HyperNetworks (PHNs) for efficient and scalable learning of entire Pareto fronts
in multi-objective optimization, enabling post-training selection of desired operating points. Mehta et al.
(2022) proposed a reinforcement learning approach (PA-Net) that efficiently approximates Pareto fronts in
bi-objective travelling salesperson problems, enhancing performance and application in robotic navigation
tasks. Menon & Williamson (2018) examines fairness tradeoffs in binary classifiers, linking fairness measures
to cost-sensitive risks and revealing that optimal classifiers use instance-dependent thresholding, with a
proposed simple approach for fairness-aware problems. Kim et al. (2020); Liu & Vicente (2022); Wick et al.
(2019); Dutta et al. (2020) discuss the trade-offs between accuracy and fairness. To Reviewer
21XQ: Re-
quested
Change 4:
discussion on
accuracy-
fairness
trade-off.3Under review as submission to TMLR
Neural Network Subspaces Learning. The idea of learning a neural network subspace is concurrently
proposed by Wortsman et al. (2021) and Benton et al. (2021). Multiple sets of network weights are treated as
the corners of a simplex, and an optimization procedure updates these corners to find a region in weight space
in which points inside the simplex correspond to accurate networks. Garipov et al. (2018) learning a connection
between two independently trained neural networks, considering curves and piecewise linear functions with
fixed endpoints. Wortsman et al. (2021) and Benton et al. (2021) concurrently proposed to learn a functionally
diverse subspace. Our proposed method YODOdiffers from these methods by specifying each subspace to the
different learning objectives and allowing flexible fairness levels at inference time.Dosovitskiy & Djolonga
(2020) trains a single model on distribution of losses, allowing it to generate outputs corresponding to any loss
from the training distribution, increasing efficiency at both training and inference times, as demonstrated in
beta-VAE, learned image compression, and fast style transfer tasks. Ha et al. (2017) proposed Hypernetworks,
which generate weights for another network, achieve competitive results in sequence modeling and image
recognition tasks with fewer learnable parameters, challenging traditional weight-sharing paradigms. To Reviewer
21XQ: Re-
quested
Change 4:
add more
discussion on
accuracy-
fairness
trade-off.3 Preliminaries
In this section, we introduce the notation used throughout this paper and provide an overview of the
preliminaries of our work, including algorithmic fairness and neural network subspace learning.
Notations. We use{x,y,s}to denote a data instance, where x‚ààRd,y‚àà{0, 1},s‚àà{0, 1}are feature,
label, sensitive attribute, respectively. ÀÜy‚àà[0, 1]denotes the predicted value by machine learning model
ÀÜy=f(x;Œ∏) :Rd‚Üí[0, 1]with trainable parameter Œ∏‚ààRmin them-dimensional weight space, which is
represented as a m-dimensional flatten vector. Ddenotes the data distribution of ( x,y) andD0/D1denotes
the distribution of the data with sensitive attribute 0/1. In this work, we consider the fair binary classification
(y‚àà{0, 1}) with binary sensitive attributes ( s‚àà{0, 1}).
3.1 Algorithmic Fairness
Forsimplicityof thepresentation, To Reviewer
21XQ: Re-
quested
Change 1:
We remove
‚Äúwithout loss
of general-
ity‚Äù.wefocus onthe groupfairnessdefinition ofdemographicparity (DP)(Dwork
et al., 2012). DP is a fairness metric that aims to ensure that the proportion of positive outcomes is equal
between different demographic groups in the population. To measures the DP between two demographic
groups, ‚àÜDPis defined as the absolute difference in the positive prediction rates of the machine learning
model between the two demographic groups. This definition is proposed to ensure that the model treats
different groups equally. Demographic parity (DP) is defined as follows:
Definition 3.1 (Demographic Parity) .Demographic Parity requires the predicted values ÀÜyto be independent
of the sensitive attribute s, that is,P(ÀÜy|s= 0) =P(ÀÜy|s= 1).
One relaxed metric to measure demographic parity has also been proposed by Edwards & Storkey (2015) and
widely used by Agarwal et al. (2018); Wei et al. (2019); Taskesen et al. (2020); Madras et al. (2018); Chuang
& Mroueh (2020). The relaxed metric ‚àÜDP of demographic parity is defined as follows:
‚àÜDP(f) =|Ex‚àºD0f(x)‚àíEx‚àºD1f(x)|, (1)
One practical and effective way to achieve demographic parity is to formulate a penalized optimization with
‚àÜDPas gap regularization. The overall objective for a fixed level of accuracy-fairness trade-offs is as follows:
L=Lce(f(x;Œ∏),y) +A¬∑Lf(f(x;Œ∏),y) =Lce+A¬∑Lf, (2)
whereLceis the objective function (e.g., cross-entropy) of the downstream task, Lf. is instantiated as the
demographic parity difference ‚àÜDP(f), To Reviewer
zQ7a: Con-
cern 3: ex-
plicitly intro-
duceLf.andAis a fixed hyperparameter to balance the model accuracy and
fairness. In the following, we set Ato1for simplicity. We also conducted experiments to explore the effect of
varyingAin Appendix C.3.
In addition to DP, we also consider the fairness metric of Equality of Opportunity (EO) and Equalized
Odds (Eodd) in our experiments, as described in Section 6.6 and Appendix C.2. Through the experiment on
multiple fairness definitions (i.e., DP, EO, Eodd), we can gain a more comprehensive understanding of the
performance of our approach and the trade-offs between fairness and accuracy. To Reviewer
DMiX:
Adding more
fairness crite-
ria. 4Under review as submission to TMLR
3.2 Neural Network Subspaces
The optimization of the neural network is to find a minimum (often a local minimum) in a high-dimensional
weight space. Wortsman et al. (2021) and Benton et al. (2021) proposed a method to learn a functionally
diverse neural network subspace, which is parameterized by two sets of network weights, œâ1andœâ2. At
training time, a network is sampled from the line defined by this pair of weights, Œ∏= (1‚àíŒ±)œâ1+Œ±œâ2for
Œ±‚àà[0, 1], and the two sets of weights œâ1andœâ2are optimized by one back-propagation simultaneously,
making the learning process more efficient. By parameterizing the neural network subspace using two sets of
weights, this method allows for greater control over the optimization of the neural network. This is because
the user can specify the values of Œ±to control the degree of mixing between œâ1andœâ2. Different from this
method that the learning objective of both œâ1andœâ2are model accuracy for the downstream task, our
proposed method lets the œâ1andœâ2learn different target, accuracy and fairness, respectively. The method
also allows for the exploration of the trade-offs between accuracy and fairness, as the two sets of weights can
be optimized to achieve different levels of accuracy and fairness.
4 Methodology
In this section, we introduce our method YODO, which learns an objective-diverse neural network subspace
targeting flexible trade-offs between accuracy and fairness at inference time while training the model once.
4.1 You Only Debias Once
Our goal is to find an objective-diverse subspace (the ‚Äúline‚Äù) in weight space comprised of accuracy-optimum
(
in Figure 1) and fairness-optimum (
 in Figure 1) neural networks, as illustrated in Figure 1. Specifically,
we first parameterize two sets of trainable weights œâ1‚ààRnandœâ2‚ààRnfor one neural network, and then
optimize weights œâ1,œâ2to achieve that f(x;œâ1)is high for model accuracy and f(x;œâ2)is high for model
fairness. Thus f(x; (1‚àíŒ±)œâ1+Œ±œâ2))achievesŒ±-controlled accuracy-fairness trade-offs. In other words, we
aim to learn a ‚Äúline‚Äù between œâ1andœâ2in the weight space to achieve flexible accuracy-fairness trade-offs at
inference time.
In the training process, we aim to optimize œâ1,œâ2with objective functions targeting accuracy and fairness
objectives, respectively. Thus two endpoints œâ1andœâ2correspond with objective function for downstream
taskLceand the objective function for fairness. Lce+Lf4. Thus the learned œâ1,œâ2will be accuracy-optimum
(
in Figure 1) and fairness-optimum (
 in Figure 1) points in the weight space. Based on the linear
combination of two endpoints, we try to minimize the loss as
L= (1‚àíŒ±)Lce/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
œâ1(
)+Œ±(Lce+Lf)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
œâ2(
)=Lce+Œ±Lf,
(3)
whereŒ∏= (1‚àíŒ±)œâ1+Œ±œâ2forŒ±‚àà[0, 1].Lceis instantiated as the cross-entropy loss for downstream tasks
(i.e., binary classification task), and Lfis instantiated as demographic parity difference ‚àÜDP[f(x;Œ∏)].
Since we seek to optimize the different levels of fairness constraints, for each Œ±‚àà[0, 1], we propose to minimize
E(x,y)‚àºD[Lce(f(x;Œ∏),y) +Œ±Lf(f(x;Œ∏),y)], thus our training objective is to minimize
EŒ±‚àºU/bracketleftbig
E(x,y)‚àºD[Lce(f(x;Œ∏),y) +Œ±Lf(f(x;Œ∏),y)]/bracketrightbig
,
s.t.Œ∏= (1‚àíŒ±)œâ1+Œ±œâ2,(4)
where Udenotes the uniform distribution between 0and1and the trade-off hyperparameter Œ±follows uniform
distribution, i.e., Œ±‚àºU.
Why Does YODOAchieve Flexible Accuracy-fairness Trade-offs in Terms of Objective Function?
For eachŒ±inU, the objective E(x,y)‚àºD[Lce(f(x;Œ∏),y) +Œ±Lf(f(x;Œ∏),y)]will be optimized to the minima,
leading to different accuracy-fairness trade-offs with different Œ±. In other words, our model can be regarded
4For simplicity, here we set the hyperparameters Ain Equation (2) to 1.
5Under review as submission to TMLR
as training infinite models with different fairness constraints in the training phase. Since our method aims to
find the "line" between the accuracy-optimum and fairness-optimum points in the weight space, we need to
ensure that any point on this line corresponds to a specific level of fairness. Therefore, we randomly sample a
Œ±during each epoch. In other words, each Œ±corresponds to one model with a specific level of fairness. Under
a wide range of different Œ±values, we train numerous models (infinite) throughout the training process. To Reviewer
21XQ: Con-
cern 3Such a mechanism guarantees that YODOcould achieve flexible trade-offs with one-pass training at inference
time. In the following, we analyze our model from the model optimization perspective.
4.2 Optimization Resulting in Objective-Diverse Subspace
In this section, we discuss the model optimization for YODO. In each training batch, we randomly sample
Œ±‚àºU[0,1], and then we use Œ∏= (1‚àíŒ±)œâ1+Œ±œâ2as the model parameters. We calculate the gradients for œâ1
andœâ2with respect to objective function (Equation (4)) as follows:
‚àÇL
‚àÇœâi=‚àÇL
‚àÇœâi=‚àÇL
‚àÇŒ∏‚àÇŒ∏
‚àÇœâi. (5)
Consider that Œ∏= (1‚àíŒ±)œâ1+Œ±œâ2, the gradients for the endpoints œâ1andœâ2are
‚àÇ(Lce+Œ±Lf)
‚àÇœâ1= (1‚àíŒ±)‚àÇ(Lce+Œ±Lf)
‚àÇŒ∏,
‚àÇ(Lce+Œ±Lf)
‚àÇœâ2=Œ±‚àÇ(Lce+Œ±Lf)
‚àÇŒ∏,(6)
To Reviewer
bMqY: Re-
quested
Change 1:
We revise
theœâ1and
œâ2to the
correct or-
der. We only
highlight
this one, and
we also the
following
content ac-
cordingly.From Equations (5) and (6), we can see that gradients for œâ1andœâ2are related to the LceandLfwith
different values of the scale coefficient (i.e., 1‚àíŒ±andŒ±). The optimization of the two endpoints œâ1and
œâ2only depends on the gradient‚àÇ(Lce+Œ±Lf)
‚àÇŒ∏, which are the most time-consuming operations in the back-
propagation during the model training. This indicates that our method does not require any extra cost in the
back-propagation phase since we only compute‚àÇ(Lce+Œ±Lf)
‚àÇŒ∏once.
Why Does YODOAchieve Flexible Accuracy-fairness Trade-offs in Terms of Gradient? Equation (6)
indicates the optimization of the weights œâ1andœâ2, as well as the objective function, are both controlled
by the hyperparameter Œ±. In the following analysis, we will examine the optimization of these weights with
different values of Œ±.
‚Ä¢WhenŒ±= 0, the gradients of œâ1in Equation (6) is calculated by the loss‚àÇ(Lce+Œ±Lf)
‚àÇœâ1= (1‚àí
0)‚àÇ(Lce+0‚àóLf)
‚àÇŒ∏=‚àÇ(Lce)
‚àÇŒ∏and the gradients of œâ2is0, indicating weight œâ1will be only optimized
by the accuracy objective.
‚Ä¢WhenŒ±= 1, the gradients in Equation (6) is calculated by the loss‚àÇ(Lce+Œ±Lf)
‚àÇœâ2= 1‚àó‚àÇ(Lce+1‚àóLf)
‚àÇŒ∏=
‚àÇ(Lce+Lf)
‚àÇŒ∏and the gradients of œâ1is0, indicating weight œâ2will be only optimized by the fairness
objective.
‚Ä¢When 0<Œ±< 1, the weights œâ1andœâ2will be only optimized by the linear combination of accuracy
objective and fairness objective.
From the analysis of gradient, we can conclude that the optimization tends to encourage the two endpoints œâ1
andœâ2to accuracy-optimum and fairness-optimum solutions in the weight space . As illustrated in Figure 1(b),
we plot the landscape of the error rate (accuracy) and the ‚àÜDP(fairness). The landscapes are depicted with
a trained YODO.
and
indicate the two endpoints, which are encouraged to learn accuracy and fairness
objectives, respectively. The landscape shows that our method learns an objective-diverse neural network
subspace and optimizes the endpoints to accuracy-optimum and fairness-optimum solutions.
Model Optimization. To promote the diversity of the neural network subspaces being learned, we follow
the approach proposed in (Wortsman et al., 2021) and aim to learn two distinct endpoints, i.e., the accuracy-
optimum and the fairness-optimum endpoint in the weight space. Specifically, we constrain the cosine
similarity of the pair of endpoints ( œâ1andœâ2) to be zero. To achieve this, we augment the original learning
6Under review as submission to TMLR
Algorithm 1 YODO
Require: Training setS, balance hyperparameters Œ≤
1:Initialize each œâiindependently.
2:forbatch inSdo
3:Randomly sample an Œ±
4:Calculate interpolated weight œâ=‚Üê(1‚àíŒ±)œâ1+Œ±œâ2
5:Calculate lossL=Lce+Œ±Lf+Œ≤Lreg
6:Back-propagateLto updateœâ1andœâ2using Equation (6).
7:end forobjective (Equation (4)) with a cosine
similarity regularization term Lreg, which
is defined asLreg=‚ü®œâ1,œâ2‚ü©2
|œâ1|2
2|œâ2|2
2, which en-
courages the two endpoints to be as dis-
similar as possible, effectively diversifying
the learned subspaces. Minimizing Lreg
(i.e., the cosine similarity of œâ1andœâ2)
promotes diversity between the two end-
points. Given that the cosine similarity
of two random high-dimensional vectors
is typically close to zero, and considering that the accuracy-optimum and fairness-optimum points To Reviewer
bMqY:
Concern 4:
weights to be
orthogonalin the
weight space are generally distinct, chasing Lreg=‚ü®œâ1,œâ2‚ü©2
|œâ1|2
2|œâ2|2
2= 0ensures that the two points will not be
identical. We note that accuracy-optimum and fairness-optimum points becoming identical may not happen
in experiments, we merely use this regularization term to prevent it from happening. Thus the final objective
function will beL=Lce+Œ±Lf+Œ≤Lreg To Reviewer
zQ7a: Re-
quested
Change 3:
analyze ne-
cessity of
Lreg.. The algorithm of YODOis presented in Algorithm 1.
4.3 Prediction Procedure
After training a model f(x; (1‚àíŒ±)œâ1+Œ±œâ2))with two sets of parameters œâ1andœâ2, the prediction procedure
for a test sample xcan be summarized as follows:
1.Choose the desired trade-off parameter Œ±, which con-
trols the balance between accuracy and fairness.
2.Compute the weighted combination of the two sets of
trained weights, (1‚àíŒ±)œâ1+Œ±œâ2, to obtain the model
parameters for the desired trade-off.
3.Ccompute the prediction function to the test sample
xasf(x; (1‚àíŒ±)œâ1+Œ±œâ2), to obtain the predicted
output.
xLce+‚ÜµLf
Figure 2: Prediction procedure of YODO
This prediction procedure offers flexible accuracy-fairness trade-offs at inference time, enabling users to choose
the desired level of accuracy and fairness for their specific application. To Reviewer
DMiX: Re-
quested
Changes 1:
we added
Section 4.3
for the pre-
diction pro-
cedure.5 Discussion
In this section, we provide further analysis of our proposed method, including the model complexity, and
model deployment. Additionally, we discuss the relationship between our work and related research.
Table 1: Comparing the running time for one epoch
of the fixed model and the YODO, the experiments are
conducted using an NVIDIA RTX A5000 GPU. The
results are the mean of 10trials The unit is second.
Datasets Fixed YODOExtra Time
UCI Adult 0.42 0.58 38%
KDD Census 3.78 4.99 32%
ACS-I 3.93 6.09 55%
ACS-E 3.53 4.25 20%
Average 2.91 3.97 36%Model Complexity. we analyze the time and space
complexity of YODO. Compared to the memory uti-
lization of the models with fixed accuracy-fairness
trade-off, our method utilizes twofold memory usage.
However, it can achieve in-situ flexible trade-offs
at inference time. In practice, if the downstream
task needs Nkinds of different accuracy-fairness
trade-offs, we need Ndifferent trained models with
different fixed fairness constraints, which also need
Ntimes training. In this sense, YODOonly requires
storing two sets of neural network weights and one-
time training. For time complexity, the extra com-
putational cost stems from the linear combination of
the two sets of neural network weights at inference time, which is time-saving and negligible. At the training
time, the computational cost is from gradient computation of œâ1/œâ2(Equation (6)). We also conducted
7Under review as submission to TMLR
experiments comparing the fixed model and YODOto evaluate the additional running time and presented
the results in Table 1. The results show that, on average, YODO only results in a 36% increase in training
time. When compared to an arbitrary accuracy-fairness trade-off, this extra time is negligible. For example,
if we require 100levels of trade-off on the ACS-E dataset, YODO takes 2.91seconds, while training 100fixed
models takes 397seconds. In this sense, the 2.91needed by YODO is considered negligible. To Reviewer
bMqY: Re-
quested
Change 3:
add time
complexity
analysisModel Deployment. We discuss the advantages of the proposed method in the model deployment phase.
YODOcan be recalibrated for accuracy-fairness configuration before/after deployment for a fixed/flexible
accuracy-fairness trade-off. For fixed accuracy-fairness trade-offs, we can set predefined Œ±forYODOwithout
retraining the model. For flexible accuracy-fairness trade-offs, we can define an API for users to input their Œ±
to meet their own requirements. Another advantage is that our method can output the varying predictive
values for individuals (experiment in Figure 7), which can provide a reference for the decision makers to
examine the realization degree of fairness.
Difference and Relation to Previous Works. Wortsman et al. (2021); Benton et al. (2021) proposed
to learn the neural network subspaces, which aims to find a flat subspace for high accuracy. The goal of
these two papers is to learn a functionally diverse neural network subspace in which the model accuracy is
all high at each endpoint. Our method creatively proposes to achieve flexible fairness at inference time via
learning a objective-diverse neural network subspace containing the optimum solutions for multiple objectives
(e.g., accuracy and fairness). Nunez et al. (2023) proposed a similar method to achieve accuracy-efficiency
trade-offs, which is similar to our proposed method. However, this paper proposed to train a degenerate
subspace with a single point in weight space. Instead, we target to train an objective-diverse neural network
subspace by training the linear combined weights with corresponding linear combinations of objectives (i.e.,
accuracy and fairness).
6 Experiments
We empirically evaluate the performance of our proposed method in this section. First, we evaluate the
effectiveness of the proposed method on both tabular and image datasets in Section 6.1. Then, we investigate
the in-situ flexible accuracy-fairness trade-offs of YODOin Section 6.2. Moreover, we investigate the
visualization of hidden representation and the distribution of the prediction values in Sections 6.3 and 6.4,
which gains insights into why our method can achieve flexible accuracy-fairness trade-offs. We also provide a
case study on CelebAdataset in Section 6.5. The major Observations of the experiments are highlighted in
boldface .
Experimental Setting. In our experiments, we evaluate the performance of our proposed method on
multiple datasets, including both tabular and image data. For tabular dataset, we adopt UCI Adult (Dua &
Graff, 2017), KDD Census (Dua & Graff, 2017), ACS-I(ncome) (Ding et al., 2021), ACS-E(mployment) (Ding
et al., 2021), which are widely used benchmark datasets in the fairness and machine learning community. For
image data analysis, we use the CelebAdataset, which is a large-scale face attributes dataset with more than
200K celebrity images. This dataset is commonly used in the field of computer vision and has been used in
various studies related to fairness in machine learning. The additional experimental setting is presented in
Appendix A.
Baselines. We included two kinds of baseline methods, namely Fixed Training andERM, in our
experiments to validate the effectiveness of our proposed method. Fixed Training, as an important baseline,
trains multiple models for different fixed accuracy-fairness trade-offs. Training the multiple fixed models
requires expensive time and memory costs. For each point in Figures 3 and 12, we trained one fixed model using
the objective function shown in Equation (2) with different values of A, which is set to (0, 1] To Re-
viewer zQ7a:
Concern 6:
Range ofA
andA= 0.with an interval
of0.05. Besides that, we also consider more baseline methods, including Prejudice Remover (Kamishima
et al., 2012), Adversarial Debiasing (Louppe et al., 2017) for demographic parity.5We provide more details
of the baselines in Appendix B.2. For our experimental setting, we have to train 20individual models from
5We note that all these baselines use fixed training (i.e., each model represents a single level of fairness), while our proposed
YODOtrains once to achieve a flexible level of fairness.
8Under review as submission to TMLR
234Fairness (¬¢DP)5.05.56.06.57.0Accuracy (Error Rate)051018202224Accuracy (Error Rate)
0.02.55.07.518202224Accuracy (Error Rate)0245.05.56.06.57.0Accuracy (Error Rate)0245.05.56.06.57.0Accuracy (Error Rate)
01020152025Accuracy (Error Rate)0510152025Accuracy (Error Rate)051017181920Accuracy (Error Rate)
0.02.55.07.5182022Accuracy (Error Rate)YODOFixed TraingingPrejudice RemoverAdversarial DebiasingERM
Figure 3: The Pareto frontier of the model accuracy and fairness. The first row is the fairness performance with
respect to gender sensitive attribute, while the second row is race sensitive attribute. The model performance
metric is Error Rate (lower is better), and the fairness metric is ‚àÜDP(lower is better, Equation (1)).
02040Fairness (¬¢DP)15202530Accuracy (Error Rate)2040Fairness (¬¢DP)203040Accuracy (Error Rate)51015Fairness (¬¢DP)78910Accuracy (Error Rate)12Fairness (¬¢DP)78910Accuracy (Error Rate)CelebA(Attractive-Gender)
234Fairness (¬¢DP)5.05.56.06.57.0Accuracy (Error Rate)CelebA(Attractive-Age)CelebA(Smiling-Gender)CelebA(Smiling-Age)YODOFixed TraingingERM
Figure 4: The Pareto frontier of the model performance and fairness on the CelebAdataset. The downstream
task is to predict whether a person is Attractive (Smiling) or not. The sensitive attribute we considered is
gender and age. The x-axis represents the difference in demographic parity ( ‚àÜDP) between the sensitive
groups, while the y-axis represents the error rate of the model. Our proposed one-time training model achieves
a comparable trade-off between performance and fairness, as indicated by the Pareto frontier when compared
to a set of fixed-trained models.
scratch for each trade-off hyperparameter. Another baseline is Empirical Risk Minimization (ERM), which is
to minimize the empirical risk of downstream task (marked as ‚ãÜin the result).
6.1 Will YODOAchieve Flexible Trade-offs only with Training Once?
In this section, we validate the effectiveness of our proposed YODOon real-world datasets, including tabular
data and image data. We present the results in Figures 3, 4 and 12 and use Pareto frontier (Emmerich &
Deutz, 2018) to evaluate our proposed method and the baseline. Pareto frontier is widely used to evaluate
the accuracy-fairness trade-offs by Kim et al. (2020); Liu & Vicente (2020); Wei & Niethammer (2020) and
characterizes a model‚Äôs achievable accuracy for given fairness conditions. Pareto frontier characterizes a
model‚Äôs achievable accuracy for given fairness conditions, as a measurement to understand the trade-offs
between model accuracy and fairness. The Pareto frontier indicates the set of models that achieve the best
trade-off between performance and fairness. The detail of the experiments can be found at Appendix B.4.
The results on the tabular dataset are presented in Figures 3 and 12. And the results on the image dataset
are presented in Figure 4. From these figures, we make the following major observations:
Obs.1: Even though YODOonly needs to be trained once, it performs similarly to baseline
(Fixed Training), or even better in some cases. After conducting experiments on real-world datasets,
we compared the Pareto frontier of YODOwith that of the Fixed Training baseline. We found that Pareto
frontier of YODOcoincides with that of Fixed Training in most figures, indicating it can achieve comparable
or even better performance than baseline. The result makes YODOreadily usable for real-world applications
9Under review as submission to TMLR
ACS-I (Gender)ACS-I (Race)CelebA(Gender)
0.00.51.0√Ü2025Error Rate1020¬¢DPgender0.00.51.0√Ü16.517.017.5Error Rate24¬¢DPgender0.00.51.0√Ü16.016.5Error Rate123¬¢DPgender
Figure 5: The accuracy-fairness trade-offs at inference time with respect to Œ±for three different datasets:
ACS-Idataset with gender as the sensitive attribute ( Left),ACS-Idataset with race as the sensitive attribute
(Middle),CelebAdataset with gender as the sensitive attribute ( Right). We observed that the fine-grained
accuracy-fairness trade-offs could be achieved by selecting different values of Œ±, providing more nuanced
accuracy-fairness trade-offs. Note that the results are obtained at inference time with a single trained model.
requiring flexible fairness. It is worth highlighting that our proposed method only needs one-time training to
obtain the Pareto frontier at inference time, making it computationally efficient.
Obs.2: YODOachieves better fairness performance than baseline (Fixed Training) in some cases.
From the results of UCI Adult ,KDD Census , and ACS-Idataset, we observed that our method achieves better
fairness performance in some cases. Specifically, the Pareto frontier represented in blue in the figures can
reach a lower ‚àÜDPvalue than the baseline methods. This observation suggests that our method is highly
effective in achieving flexible fairness and can be readily used in real-world applications where fairness is
crucial for the downstream task.
Obs.3: On some datasets (e.g., UCI Adult ,KDD Census ,CelebA),YODOeven outperforms the
ERM baseline. Upon comparing our results on the UCI Adult and KDD Census datasets, we observed
that the Pareto frontier of our method covers the point of ERM. This finding suggests that our approach
outperforms ERM in both model accuracy and fairness performance. It also demonstrates that our proposed
objective-diverse neural network subspace can achieve flexible accuracy-fairness trade-offs while potentially
improving model accuracy. This advantage is likely attributed to the powerful capacity of the proposed
objective-diverse subspace.
Obs.4: YODOachieves a more smooth Pareto frontier than baseline (Fixed Training). On most
datasets, especially on CelebAimage data, our proposed method demonstrates a smoother Pareto frontier
compared to the baselines. For example, on CelebAAttractive-Age and Attractive-Gender in Figure 4, the
Pareto frontier is notably smoother than baselines. A smooth Pareto frontier implies that our proposal is able
to achieve more fine-grained accuracy-fairness trade-offs, which can provide a more dependable prediction for
high-stakes decision-making scenarios.
6.2 Can the Accuracy-fairness Trade-Offs Be Flexible by Controlling Parameter Œ±?
To explore the effect of hyperparameter Œ±on the accuracy-fairness trade-offs, we conducted experiments
on the ACS-Iand CelebAdatasets with different values of Œ±.Œ±is a hyperparameter that controls the
trade-off between model accuracy and fairness in machine learning models. A higher value of Œ±emphasizes
more on fairness, while a lower value prioritizes accuracy. By adjusting the value of Œ±, we can make the
accuracy-fairness trade-off flexible and controllable. We evaluate the model accuracy and fairness performance
with the different values of Œ±at inference time. The results are presented in Figure 5 and we observed:
Obs.5: The accuracy-fairness trade-offs can be controlled by Œ±at inference time. Our experiments
on both tabular and CelebAdatasets showed that as we increase the value of hyperparameter Œ±, the error
rate (lower is better) gradually increases while ‚àÜDPgradually decreases. The solution at Œ±= 0prioritizes
accuracy-optimum (lowest error rate), while the solution at Œ±= 1prioritizes fairness-optimum (lowest ‚àÜDP).
These results demonstrate that our proposed method can achieve flexible accuracy-fairness trade-offs, with
distinct linear correlations between the trade-off and the value of Œ±. Based on these findings, our approach
can provide more reliable and controllable predictions by customizing Œ±at inference time.
10Under review as submission to TMLR
‚Üµ=0‚Üµ=10.250.50.75RepresentationDistributionFair
Density
Figure 6: The changing distribution of the prediction values ÀÜyand the representations as Œ±increases.
Blue indicates male and orange indicates female. Top: The distribution is estimated with kernel density
estimation (Terrell & Scott, 1992). The distributions are more polarized between males and females with Œ±= 0,
but become more similar with Œ±= 1. AsŒ±increases, the distributions for male and female groups become
more similar, indicating achieving demographic parity. Bottom : The visualization of the representation with
the values of Œ±are from 0to1. The figure at the far left ( Œ±= 0) shows that the representations for males
(
) and females (
 ) are distinctly separate, indicating that the representations contain more sensitive
information. The figure on the far right ( Œ±= 1) shows the representations of different groups are mixed
together, indicating that the representation contains little sensitive information.
6.3 How Does the Distribution of the Predictive Values Vary with Changing Œ±?
In this section, we examine the distribution of predictive values to investigate the varying trade-offs of our
method. We specifically plot the distribution of predictive values for different groups (male and female) to
verify the flexible accuracy-fairness trade-offs provided by our approach. The experiments are conducted on
ACS-Idataset and the results are presented in Figure 6. We have the following observation:
Obs.6: The distribution of predictive values for different groups becomes increasingly similar
as the value of Œ±increases , indicating that our model becomes more fair as the values of Œ±increase.
Additionally, we found that the distributions of the predictive values of different groups follow the same
distribution, showing the predictive values are independent of sensitive attributes. The varying distribution
of predictive values provides valuable insights into why our model can achieve flexible accuracy-fairness
trade-offs from a distributional perspective..
6.4 How Do the Representations Vary with Changing Œ±?
In this section, we visualize the hidden representation of different groups to provide further analysis and insight
into the proposed method, YODO. Demographic parity requires that model predictions be independent of
sensitive attributes (e.g., demographic groups). It is natural and rational to examine the independence between
the hidden representation (penultimate layer) and the sensitive attribute. To interpret this representation,
ifŒ±= 0(unfair model), the representations of the male and female groups would contain more sensitive
information. In contrast, if Œ±= 1(fair model), the representations would display a mix, signifying minimal
sensitive information. To Reviewer
bMqY: Con-
cern 3: ex-
plain hidden
representa-
tion.The visualization of representations is widely used to inspect the fairness issue of
DNNs from the manifold perspective (Du et al., 2021; Louizos et al., 2015). With the different combination
with different Œ±s, we visualize the hidden representation in Figure 6 and we observed that:
Obs.7: The disparity of the representation of different groups becomes smaller and smaller
with the increasing Œ±. The figure on the far left in Figure 6 shows that the representation of male and
female groups are distinctly separate, indicating that the representations contain more sensitive information.
The figure on the far right shows the representations of different groups mixed together, indicating that the
representations contain little sensitive information. The sensitive information embedded in the representation
11Under review as submission to TMLR
Figure 7: Case study on CelebAdataset. The sensitive attribute is gender. The downstream task is to
predict whether a person is attractive or not. The results show that the YODOcan provide the instance-level
prediction change for practitioners to examine the fairness performance of our proposed method. By providing
instance-level prediction changes, YODOenabled practitioners to better understand the model‚Äôs fairness
performance and make informed decisions.
indicates that the two endpoint biases correspond to model accuracy and fairness, respectively. This
phenomenon provides insight into YODO, that it can learn a fair representation to guarantee a fair prediction.
6.5 How Does the Prediction Value Vary at the Instance Level? A Case Study on Image Data
Figure 8: The mean of the prediction values for females
and males with respect to varying Œ±. The red line
indicates the predictive value ÀÜy= 0.5. The blue line
is the mean of predictive values of each group (i.e.,
male and female). The results illustrate that the mean
prediction values for males and females become more
similar asŒ±increases., indicating a fairer prediction.In this section, we experiment on the CelebAdataset
to investigate how the predicted values changes and
the varying prediction with the various Œ±in the
instance level. The sensitive attribute we consider is
Gender. The downstream task is to predict whether
a person is attractive or not. The female group has
more positive samples than the Male group, leading
the biased prediction. We first present the mean of
the prediction with respect to varying Œ±for each
group (i.e., male and female) in Figure 8. We also
present some cases to investigate the effect of the
change of the values of Œ±in Figure 7. Based on our
analysis of Figures 7 and 8, we make the following
observations:
Obs.8: The mean of the predictive values
of different groups (e.g., female, male) ap-
proaches 0.5with the increasing Œ±. Analysis of
Figure 7 shows that, overall, YODOtends to decrease
the predictive values for the Female group and in-
crease the predictive values for the Male group, resulting in lower ‚àÜDPvalues and indicating fairer predictions
overall. Our observations demonstrate that our proposed method can mitigate the unfairness caused by the
dataset‚Äôs gender imbalance and ensure that individuals from different groups are treated equitably.
Obs.9: YODOcan provide an instance-level explanation for group fairness with only one model,
while fixed training models need multiple models. To Reviewer
bMqY: Re-
quested
Change 4:
Rewrite ob-
servation 9.InCelebAdataset, the Female group has more
positive (attractive) samples than the other one, leading to a higher predictive value for the male group.
Figure 7 shows that YODOtends to lower the predictive values of the Female group while higher the predictive
values of the Male group. Such a tendency would result in fairer predictive results. Our proposed method
YODOoffers an instance-level explanation for individuals with only one model To Reviewer
bMqY: Re-
quested
Change 4. For example, Figure 7
demonstrates how YODOcan lower the predictive values for the Female group and increase the predictive
values for the Male group, resulting in a fairer outcome for individuals belonging to each group. Our method
can provide individualized explanations and a trustworthy model for end-users.
12Under review as submission to TMLR
6789Fairness (¬¢EO)171819Accuracy (Error Rate)UCIAdultKDDCensusACS-I-1YACS-I-5Y
YODOFixed TraingingERMGenderRace01020Fairness (¬¢EO)152025Accuracy (Error Rate)152025Fairness (¬¢EO)152025Accuracy (Error Rate)
2345Fairness (¬¢EO)5.05.56.06.57.0Accuracy (Error Rate)2345Fairness (¬¢EO)5.05.56.06.57.0Accuracy (Error Rate)6810Fairness (¬¢EO)171819Accuracy (Error Rate)
6810Fairness (¬¢EO)182022Accuracy (Error Rate)81012Fairness (¬¢EO)182022Accuracy (Error Rate)
6789Fairness (¬¢EO)171819Accuracy (Error Rate)
Figure 9: The Pareto frontier of the model accuracy and fairness. The first row is the fairness performance with
respect to gender sensitive attribute, while the second row is race sensitive attribute. The model performance
metric is Error Rate (lower is better), and the fairness metric is ‚àÜEO(lower is better, Equation (1).
CelebA(Attractive-Gender)CelebA(Attractive-Age)CelebA(Smiling-Gender)CelebA(Smiling-Age)
3540Fairness (¬¢EO)182022Accuracy (Error Rate)354045Fairness (¬¢EO)18202224Accuracy (Error Rate)121416Fairness (¬¢EO)678910Accuracy (Error Rate)123Fairness (¬¢EO)78910Accuracy (Error Rate)354045Fairness (¬¢EO)18202224Accuracy (Error Rate)YODOFixed TraingingERM
Figure 10: The Pareto frontier of the model performance and fairness on CelebAdataset. The downstream
task is to predict whether a person is Attractive (Smiling) or not. The sensitive attribute is gender and age.
6.6 How Does YODOPerform on Another Group Fairness Criterion, Equality of Opportunity?
In this section, we experiment on the fairness metric Equality of Opportunity (EO) (Hardt et al., 2016). EO
measures whether a classifier provides equal opportunities to individuals from different groups. By satisfying
this criterion, we can ensure that protected groups are not disproportionately negatively impacted by the
model‚Äôs predictions.
Definition 6.1 (Equality of opportunity) .A classifier satisfies this definition if both protected and unprotected
groups have an equal probability of a subject in a positive class having a negative predictive value.
For example, this implies that the probability of an applicant with an actual good credit score being incorrectly
assigned a bad predicted credit score should be the same for both male and female applicants. The formal
definition of EO is as the following:
‚àÜEO(f) =|Ex‚àºD0,y=1f(x)‚àíEx‚àºD1,y=1f(x)|. (7)
We conduct the experiment using ‚àÜEOas a fairness constraint and metric and present the results on tabular
and image data. The results are presented in Figures 9, 10 and 13. We have the following observations:
Obs.10: YODOperforms similarly to baseline (Fixed Training) in terms of Equality of Opportu-
nity, or even better in some cases. This observation indicates it can achieve comparable or even better
performance than the baseline. And this experiment also makes our method readily usable for real-world
applications, which require flexible fairness. The result shows the effectiveness of our proposal on other
fairness metrics, demonstrating its overall effectiveness in promoting fairness.
13Under review as submission to TMLR
0.00.51.0√Ü182022Error Rate024¬¢DPACS-I(Gender)
1.01.52.02.53.03.54.04.55.01.01.52.02.53.03.54.04.55.0A0.00.51.0√Ü16182022Error Rate0123¬¢DPACS-I(Race)
1.01.52.02.53.03.54.04.55.01.01.52.02.53.03.54.04.55.0A
Figure 11: The effect of the accuracy-fairness balance parameter A. We train YODOwith varying training
balance parameter values ( A) and report their performance with different Œ±. The x-axis represents a parameter
to control the accuracy-fairness trade-off for inference, while the y-axis shows both fairness (in blue) and
accuracy (in red). The strength of the color reflects the value of A. The optimal balance, achieving the best
performance in terms of accuracy and fairness, occurs at A= 1. We also present the results for each figure
corresponding to different values of Ain Figures 17 and 18.
6.7 How Does YODOPerform with Different the Balance Parameter A?
In this experiment, we evaluated the performance of the Yodomodel with varying balance parameter values
(A). The desirable balance parameter Afor the accuracy-fairness trade-off should have the following properties:
1. The error rate of the downstream task should be as low as possible.
2.The best performance for fairness should be achieved when we choose the hyperparameter Œ±at
inference time.
To investigate the impact of the value Aon the trade-off between accuracy and fairness, we tested the model
performance with varying A, ranging from 1to5, with increments of 0.5. We present the results in Figure 11.
The results show that YODOexhibited its best performance when A= 1, as this specific balance parameter
value resulted in higher accuracy and a larger demographic parity span compared to other values of A. This
observation indicates that setting A= 1effectively addresses the trade-off between model accuracy and
fairness, achieving an optimal balance. To Reviewer
21XQ: Con-
cern 4: vary-
ingA.As the value of Aincreases, although the fairness performance improves, the accuracy of the downstream
task deteriorates. When A= 5, the error rate of the downstream task is even worse than the highest error
rate of the model with A= 1. The poorer performance for the downstream task demonstrates that the model
withA= 5is inferior to that with A= 1. To Reviewer
bMqY: Con-
cern 1: vary-
ingA.7 Conclusion
In this paper, we proposed YODO, a novel method that achieves accuracy-fairness trade-offs at inference time
to meet the diverse requirements of fairness in real-world applications. Our approach is the first to achieve
flexible trade-offs between model accuracy and fairness through the use of an objective-diverse neural network
subspace. Our extensive experiments demonstrate the effectiveness and practical value of the proposed
approach, and we offer a detailed analysis of the underlying mechanisms by examining the distributions of
predictive values and hidden representations. By enabling in-situ flexibility, our approach can provide more
nuanced control over the trade-offs between accuracy and fairness, thereby advancing the state-of-the-art in
the field of algorithmic fairness.
14Under review as submission to TMLR
References
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions
approach to fair classification. In International Conference on Machine Learning , pp. 60‚Äì69, 2018.
Muhammad Aurangzeb Ahmad, Arpit Patel, Carly Eckert, Vikas Kumar, and Ankur Teredesai. Fairness in
machine learning for healthcare. In Proceedings of the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining , pp. 3529‚Äì3530, 2020.
Michiel A Bakker, Alejandro Noriega-Campero, Duy Patrick Tu, Prasanna Sattigeri, Kush R Varshney,
and AS Pentland. On fairness in budget-constrained decision making. In KDD Workshop of Explainable
Artificial Intelligence , 2019.
Pinar Barlas, Kyriakos Kyriakou, Olivia Guest, Styliani Kleanthous, and Jahna Otterbacher. To" see" is to
stereotype: Image tagging algorithms, gender recognition, and the accuracy-fairness trade-off. Proceedings
of the ACM on Human-Computer Interaction , 4(CSCW3):1‚Äì31, 2021.
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine learning. Nips tutorial , 1:2017,
2017.
Gregory Benton, Wesley Maddox, Sanae Lotfi, and Andrew Gordon Gordon Wilson. Loss surface simplexes
for mode connecting volumes and fast ensembling. In International Conference on Machine Learning , pp.
769‚Äì779. PMLR, 2021.
Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice
risk assessments: The state of the art. Sociological Methods & Research , 50(1):3‚Äì44, 2021.
Dimitris Bertsimas, Vivek F Farias, and Nikolaos Trichakis. The price of fairness. Operations research , 59(1):
17‚Äì31, 2011.
Reuben Binns. Fairness in machine learning: Lessons from political philosophy. In Conference on Fairness,
Accountability and Transparency , pp. 149‚Äì159. PMLR, 2018.
Margr√©t Vilborg Bjarnad√≥ttir and David Anderson. Machine learning in healthcare: Fairness, issues, and
challenges. In Pushing the Boundaries: Frontiers in Impactful OR/OM Research , pp. 64‚Äì83. INFORMS,
2020.
Avrim Blum and Kevin Stangl. Recovering from biased data: Can fairness constraints improve accuracy?
arXiv preprint arXiv:1912.01094 , 2019.
Steinar B√∏yum. Fairness in education‚Äìa normative analysis of oecd policy documents. Journal of Education
Policy, 29(6):856‚Äì870, 2014.
Paolo Brunori, Vito Peragine, and Laura Serlenga. Fairness in education: The italian university before and
after the reform. Economics of Education Review , 31(5):764‚Äì777, 2012.
Simon Caton and Christian Haas. Fairness in machine learning: A survey. arXiv preprint arXiv:2010.04053 ,
2020.
Irene Y Chen, Fredrik D Johansson, and David Sontag. Why is my classifier discriminatory? In Proceedings
of the 32nd International Conference on Neural Information Processing Systems , pp. 3543‚Äì3554, 2018.
AlexandraChouldechovaandAaronRoth. Thefrontiersoffairnessinmachinelearning. CoRR,abs/1810.08810,
2018. URL http://arxiv.org/abs/1810.08810 .
Ching-Yao Chuang and Youssef Mroueh. Fair mixup: Fairness via interpolation. In International Conference
on Learning Representations , 2020.
Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, and Noah
Constant. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. In
International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=
kXwdL1cWOAi .
15Under review as submission to TMLR
A Feder Cooper, Ellen Abrams, and NA NA. Emergent unfairness in algorithmic fairness-accuracy trade-off
research. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society , pp. 46‚Äì54, 2021.
Amanda Coston, Alan Mishler, Edward H. Kennedy, and Alexandra Chouldechova. Counterfactual risk
assessments, evaluation, and fairness. In FAT*, pp. 582‚Äì593, 2020. URL https://doi.org/10.1145/
3351095.3372851 .
Andr√© Cruz, Catarina G Bel√©m, Jo√£o Bravo, Pedro Saleiro, and Pedro Bizarro. FairGBM: Gradient
boosting with fairness constraints. In International Conference on Learning Representations , 2023. URL
https://openreview.net/forum?id=x-mXzBgCX3a .
Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine
learning. NeurIPS , 2021.
Alexey Dosovitskiy and Josip Djolonga. You only train once: Loss-conditional training of deep networks. In
International Conference on Learning Representations , 2020. URL https://openreview.net/forum?id=
HyxY6JHKwr .
Julia Dressel and Hany Farid. The accuracy, fairness, and limits of predicting recidivism. Science advances , 4
(1):eaao5580, 2018.
Mengnan Du, Fan Yang, Na Zou, and Xia Hu. Fairness in deep learning: A computational perspective. IEEE
Intelligent Systems , 2020.
Mengnan Du, Subhabrata Mukherjee, Guanchu Wang, Ruixiang Tang, Ahmed Awadallah, and Xia Hu.
Fairness via representation neutralization. Advances in Neural Information Processing Systems , 34, 2021.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/
ml.
Sanghamitra Dutta, Dennis Wei, Hazar Yueksel, Pin-Yu Chen, Sijia Liu, and Kush Varshney. Is there a trade-
off between fairness and accuracy? a perspective using mismatched hypothesis testing. In International
Conference on Machine Learning , pp. 2803‚Äì2813. PMLR, 2020.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Proceedings of the 3rd innovations in theoretical computer science conference , pp. 214‚Äì226,
2012.
Harrison Edwards and Amos Storkey. Censoring representations with an adversary. arXiv preprint
arXiv:1511.05897 , 2015.
Michael TM Emmerich and Andr√© H Deutz. A tutorial on multiobjective optimization: fundamentals and
evolutionary methods. Natural computing , 17(3):585‚Äì609, 2018.
Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, and Surya
Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time
evolution of the neural tangent kernel. Advances in Neural Information Processing Systems , 33:5850‚Äì5861,
2020.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity
and the lottery ticket hypothesis. In International Conference on Machine Learning , pp. 3259‚Äì3269. PMLR,
2020.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon Wilson. Loss
surfaces, mode connectivity, and fast ensembling of dnns. In Proceedings of the 32nd International
Conference on Neural Information Processing Systems , pp. 8803‚Äì8812, 2018.
Knut Gerlach, David Levine, Gesine Stephan, and Olaf Struck. Fairness and the employment contract: North
american regions versus germany. Cambridge Journal of Economics , 32(3):421‚Äì439, 2008.
16Under review as submission to TMLR
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks.
InProceedings of the thirteenth international conference on artificial intelligence and statistics , pp. 249‚Äì256.
JMLR Workshop and Conference Proceedings, 2010.
Thomas Grote and Geoff Keeling. Enabling fairness in healthcare through machine learning. Ethics and
Information Technology , 24(3):39, 2022.
Ozgur Guldogan, Yuchen Zeng, Jy yong Sohn, Ramtin Pedarsani, and Kangwook Lee. Equal improvability:
A new fairness notion considering the long-term impact. In The Eleventh International Conference on
Learning Representations , 2023. URL https://openreview.net/forum?id=dhYUMMy0_Eg .
Dongliang Guo, Zhixuan Chu, and Sheng Li. Fair attribute completion on graph with missing attributes. In
International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=
9vcXCMp9VEp .
David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In International Conference on Learning
Representations , 2017. URL https://openreview.net/forum?id=rkpACe1lx .
Christian Haas. The price of fairness-a framework to explore trade-offs in algorithmic fairness. In 40th
International Conference on Information Systems, ICIS 2019 . Association for Information Systems, 2019.
Xudong Han, Timothy Baldwin, and Trevor Cohn. Everybody needs good neighbours: An unsupervised
locality-based method for bias mitigation. In International Conference on Learning Representations , 2023.
URL https://openreview.net/forum?id=pOnhudsvzR .
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances in
neural information processing systems , 29:3315‚Äì3323, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770‚Äì778, 2016.
Lily Hu and Yiling Chen. A short-term intervention for long-term fairness in the labor market. In Proceedings
of the 2018 World Wide Web Conference , pp. 1389‚Äì1398, 2018.
Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth. Fairness in learning: classic and
contextual bandits. In Proceedings of the 30th International Conference on Neural Information Processing
Systems, pp. 325‚Äì333, 2016.
Sangwon Jung, Taeeon Park, Sanghyuk Chun, and Taesup Moon. Exact group fairness regularization via
classwise robust optimization. In International Conference on Learning Representations , 2023. URL
https://openreview.net/forum?id=Q-WfHzmiG9m .
Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-aware classifier with prejudice
remover regularizer. In Joint European conference on machine learning and knowledge discovery in databases .
Springer, 2012.
Joon Sik Kim, Jiahao Chen, and Ameet Talwalkar. Fact: A diagnostic for group fairness trade-offs. In
International Conference on Machine Learning , pp. 5264‚Äì5274. PMLR, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Ren√© F Kizilcec and Hansol Lee. Algorithmic fairness in education. In The ethics of artificial intelligence in
education , pp. 174‚Äì202. Routledge, 2022.
Jon Kleinberg. Inherent trade-offs in algorithmic fairness. In Abstracts of the 2018 ACM International
Conference on Measurement and Modeling of Computer Systems , pp. 40‚Äì40, 2018.
Puheng Li, James Zou, and Linjun Zhang. FaiREE: fair classification with finite-sample and distribution-free
guarantee. In International Conference on Learning Representations , 2023. URL https://openreview.
net/forum?id=shzu8d6_YAR .
17Under review as submission to TMLR
Suyun Liu and Luis Nunes Vicente. Accuracy and fairness trade-offs in machine learning: A stochastic
multi-objective approach. arXiv preprint arXiv:2008.01132 , 2020.
Suyun Liu and Luis Nunes Vicente. Accuracy and fairness trade-offs in machine learning: A stochastic
multi-objective approach. Computational Management Science , 19(3):513‚Äì537, 2022.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV) , December 2015.
Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. The variational fair autoencoder.
arXiv preprint arXiv:1511.00830 , 2015.
Gilles Louppe, Michael Kagan, and Kyle Cranmer. Learning to pivot with adversarial networks. NeurIPS ,
30, 2017.
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and transferable
representations. International Conference on Machine Learning , 2018.
Gaurav Maheshwari, Pascal Denis, Mikaela Keller, and Aur√©lien Bellet. Fair nlp models with differentially
private text encoders. CoRR, abs/2205.06135, 2022. URL https://doi.org/10.48550/arXiv.2205.
06135.
Subha Maity, Debarghya Mukherjee, Mikhail Yurochkin, and Yuekai Sun. There is no trade-off: enforcing
fairness can improve accuracy. arXiv preprint arXiv:2011.03173 , 2020.
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias
and fairness in machine learning. ACM Computing Surveys (CSUR) , 54(6):1‚Äì35, 2021.
Ishaan Mehta, Sharareh Taghipour, and Sajad Saeedi. Pareto frontier approximation network (pa-net) to
solve bi-objective tsp. In 2022 IEEE 18th International Conference on Automation Science and Engineering
(CASE), pp. 1198‚Äì1205, 2022. doi: 10.1109/CASE49997.2022.9926520.
Aditya Krishna Menon and Robert C Williamson. The cost of fairness in binary classification. In Conference
on Fairness, accountability and transparency , pp. 107‚Äì118. PMLR, 2018.
Aviv Navon, Aviv Shamsian, Ethan Fetaya, and Gal Chechik. Learning the pareto front with hypernetworks.
InInternational Conference on Learning Representations , 2021. URL https://openreview.net/forum?
id=NjF772F4ZZR .
Elvis Nunez, Maxwell Horton, Anish Prabhu, Anurag Ranjan, Ali Farhadi, and Mohammad Rastegari. Lcs:
Learning compressible subspaces for efficient, adaptive, real-time network compression at inference time.
InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pp. 3818‚Äì3827,
2023.
Kevin Petrasic, Benjamin Saul, James Greig, Matthew Bornfreund, and Katherine Lamberth. Algorithms
and bias: What lenders need to know. White & Case , 2017.
Thai-Hoang Pham, Xueru Zhang, and Ping Zhang. Fairness and accuracy under domain generalization. In
International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=
jBEXnEMdNOL .
Roman Pogodin, Namrata Deka, Yazhe Li, Danica J. Sutherland, Victor Veitch, and Arthur Gretton. Efficient
conditionally invariant representation learning. In The Eleventh International Conference on Learning
Representations , 2023. URL https://openreview.net/forum?id=dJruFeSRym1 .
Alvin Rajkomar, Michaela Hardt, Michael D Howell, Greg Corrado, and Marshall H Chin. Ensuring fairness
in machine learning to advance health equity. Annals of internal medicine , 169(12):866‚Äì872, 2018.
Yuji Roh, Kangwook Lee, Steven Whang, and Changho Suh. Sample selection for fair and robust training.
Advances in Neural Information Processing Systems , 34:815‚Äì827, 2021.
18Under review as submission to TMLR
Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh. Improving fair training under correlation
shifts.arXiv preprint arXiv:2302.02323 , 2023.
Guy N Rothblum and Gal Yona. Consider the alternatives: Navigating fairness-accuracy tradeoffs via
disqualification. arXiv preprint arXiv:2110.00813 , 2021.
Jessica Schrouff, Natalie Harris, Oluwasanmi Koyejo, Ibrahim Alabdulmohsin, Eva Schnider, Krista Opsahl-
Ong, Alex Brown, Subhrajit Roy, Diana Mincu, Christina Chen, Awa Dieng, Yuan Liu, Vivek Natarajan,
Alan Karthikesalingam, Katherine A. Heller, Silvia Chiappa, and Alexander D‚ÄôAmour. Maintaining fairness
across distribution shift: do we have viable solutions for real-world applications? CoRR, abs/2202.01034,
2022a. URL https://arxiv.org/abs/2202.01034 .
Jessica Schrouff, Natalie Harris, Oluwasanmi O Koyejo, Ibrahim Alabdulmohsin, Eva Schnider, Krista
Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Chrsitina Chen, Awa Dieng, Yuan Liu,
Vivek Natarajan, Alan Karthikesalingam, Katherine A Heller, Silvia Chiappa, and Alexander D‚ÄôAmour.
Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. In Alice H.
Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information
Processing Systems , 2022b. URL https://openreview.net/forum?id=K-A4tDJ6HHf .
Megha Srivastava, Hoda Heidari, and Andreas Krause. Mathematical notions vs. human perception of
fairness: A descriptive approach to fairness for machine learning. In Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining , pp. 2459‚Äì2468, 2019.
Bahar Taskesen, Viet Anh Nguyen, Daniel Kuhn, and Jose Blanchet. A distributionally robust approach to
fair classification. arXiv preprint arXiv:2007.09530 , 2020.
George R Terrell and David W Scott. Variable kernel density estimation. The Annals of Statistics , pp.
1236‚Äì1265, 1992.
Robin Vogel, Aur√©lien Bellet, and St√©phan Cl√©men√ßon. Learning fair scoring functions: Fairness definitions,
algorithms and generalization bounds for bipartite ranking. CoRR, abs/2002.08159, 2020. URL https:
//arxiv.org/abs/2002.08159 .
Robin Vogel, Aur√©lien Bellet, and St√©phan Cl√©men√ßon. Learning fair scoring functions: Bipartite ranking
under roc-based fairness constraints. In AISTATS , pp. 784‚Äì792, 2021. URL http://proceedings.mlr.
press/v130/vogel21a.html .
Dennis Wei, Karthikeyan Natesan Ramamurthy, and Flavio du Pin Calmon. Optimized score transformation
for fair classification. arXiv preprint arXiv:1906.00066 , 2019.
Susan Wei and Marc Niethammer. The fairness-accuracy pareto front. arXiv preprint arXiv:2008.10797 ,
2020.
Michael Wick, Jean-Baptiste Tristan, et al. Unlocking fairness: a trade-off revisited. Advances in neural
information processing systems , 32, 2019.
Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning non-discriminatory
predictors. In Conference on Learning Theory , pp. 1920‚Äì1953, 2017.
Mitchell Wortsman, Maxwell Horton, Carlos Guestrin, Ali Farhadi, and Mohammad Rastegari. Learning
neural network subspaces. arXiv preprint arXiv:2102.10472 , 2021.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fairness
constraints: Mechanisms for fair classification. In Artificial Intelligence and Statistics , pp. 962‚Äì970. PMLR,
2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 , 2016.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In
International conference on machine learning , pp. 325‚Äì333. PMLR, 2013.
19Under review as submission to TMLR
Guanhua Zhang, Yihua Zhang, Yang Zhang, Wenqi Fan, Qing Li, Sijia Liu, and Shiyu Chang. Fairness
reprogramming. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in
Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=Nay_rOB-dZv .
HanZhaoandGeoffGordon. Inherenttradeoffsinlearningfairrepresentations. Advances in neural information
processing systems , 32:15675‚Äì15685, 2019.
Indre Zliobaite. On the relation between accuracy and fairness in binary classification. arXiv preprint
arXiv:1505.05723 , 2015.
20Under review as submission to TMLR
In this appendix, we provide the two additional experiments in Appendix A, including the experiment on
ACS-Edataset and the distribution of the predictive values on CelebAdataset. We also provide a more
detailed experimental setting in Appendix B. We carried out supplementary experiments to delve deeper into
the analysis of our proposed model in Appendix C.
A Additional Experiments
A.1 Experiments on ACS-EDataset
In this appendix, we present the results of complementary experiments conducted on the ACS-Edataset with
gender as the sensitive attribute. The results for two-group fairness metrics, ‚àÜDP and ‚àÜEO, are presented
in Figures 12 and 13, respectively. These results are complementary to the ones presented in Figures 3 and 9.
The experimental results on the ACS-Edataset with gender as the sensitive attribute show that our proposed
method achieves comparable accuracy-fairness trade-offs to the baselines for both group fairness metrics
‚àÜDP and ‚àÜEO. This finding is consistent with the results of the experiments on other datasets with different
sensitive attributes presented in Figures 3 and 9.
0246Fairness (¬¢DP)18.018.519.019.520.0Accuracy (Error Rate)0246Fairness (¬¢DP)18.018.519.019.520.0Accuracy (Error Rate)GenderACS-E-1YACS-E-5Y
Figure 12: The Pareto frontier of the model performance and fairness on ACS-Edataset spanning 1 year or 5
years on ‚àÜDPmetric. The sensitive attribute is gender.
0246Fairness (¬¢EO)18.0018.2518.5018.7519.00Accuracy (Error Rate)0246Fairness (¬¢EO)18.0018.2518.5018.7519.00Accuracy (Error Rate)YODOFixed TraingingERMGenderACS-E-1YACS-E-5Y
Figure 13: The Pareto frontier of the model performance and fairness on ACS-Edata spanning 1 year or 5
years on ‚àÜEOmetric. The sensitive attribute is gender.
A.2 The Distribution of Predictive Values on Image Dataset
In this appendix, we provide the distribution of the predictive values on the image dataset, CelebA. The
distribution of the predictive values of different groups are more and more similar with the increase of the
values ofŒ±, indicating that our model is more and more fair with the increase of the values of Œ±. This result
shows that our proposed method encourages the distribution to follow the same distribution, and further
guarantees fair prediction.
21Under review as submission to TMLR
‚Üµ=0‚Üµ=10.250.50.75DistributionFair
Density
Figure 14: The distribution of the prediction values with different Œ±onCelebAdataset. The distribution
of the predictive values of different groups (i.e., Male, Female) becomes more and more similar with the
increasingŒ±. The distributions are more polarised between Male and Female with Œª= 0while the distribution
is nearly the same with Œª= 1.
Table 2: The summary of the datasets used in our experiment.
Dataset Data Type Task Sensitive Attributes #Instances
UCI Adult Tabular Income Gender, Race 48,842
KDD Census Tabular Income Gender, Race 299,285
ACS-Income (1 year) Tabular Income Gender, Race 265,171
ACS-Income (5 years) Tabular Income Gender, Race 1,315,945
ACS-Emploement (1 year) Tabular Employment Gender 136,965
ACS-Emploement (5 years) Tabular Employment Gender 665,137
CelebA Image Attractive Gender, Age 202,599
B Experiment Details
In this section, we provide a detailed description of our experiment setting, including the description of the
datasets, neural network architectures, and experiment settings used in our experiments.
B.1 Dataset
In the appendix, we provide more details about the datasets used in the experiments. These include tabular
datasets such as UCI Adult ,KDD Census ,ACS-I,ACS-E, as well as the image dataset CelebA. We provide
the statistics of the datasets in Table 2. In the following, we provide a comprehensive description of each
dataset used in our experiments:
‚Ä¢UCI Adult6(Dua & Graff, 2017): This dataset is extracted from the 1994 Census database. The
downstream task is to predict whether the personal income is over 50K a year. The sensitive attributes
in this dataset are gender and race.
‚Ä¢KDD Census7(Dua & Graff, 2017): This data set contains 299285census data extracted from the
1994 and 1995 by the U.S. Census Bureau. The data contains 41demographic and employment
related variables. The instance weight indicates the number of people in the population that each
record represents due to stratified sampling. The sensitive attributes are gender and race.
‚Ä¢ACS-I(ncome)8(Ding et al., 2021): The task is to predict whether the income of an individual is
above $50, 000. The source data was filtered to only include individuals above the age of 16, who
reported usual working hours of at least 1 hour per week in the past year, and an income of at least
$100. The threshold of the income is $50, 000. We use two data from this dataset which spans 1year
or5years. The sensitive attributes we consider for this dataset are gender and race.
6https://archive.ics.uci.edu/ml/datasets/adult
7https://archive.ics.uci.edu/ml/datasets/Census-Income+(KDD)
8https://github.com/zykls/folktables
22Under review as submission to TMLR
‚Ä¢ACS-E(mployment)9(Ding et al., 2021): The task is to predict whether an individual is employed
and the individuals are between the ages of 16 and 90. We use two data from this dataset which
spans 1year or 5years. The sensitive attribute we consider for this dataset is gender.
‚Ä¢CelebA10(Liu et al., 2015): The CelebFaces Attributes ( CelebA) dataset a large-scale face attributes
dataset consisting of more than 200K celebrity images and each image has 40face attributes. The
downstream task is to predict whether the person is attractive (smiling) or not, formulated as binary
classification tasks. We consider Male (gender) and Young (age) as sensitive attributes.
B.2 BaselinesTo Reviewer
DMiX: Re-
quested
change 2:
adding base-
lines.We provide the details of the baseline methods employed in our experiments. We note that all baselines use
fixed training (i.e., each model represents a single level of fairness), whereas our proposed YODOtrains once
to achieve a flexible level of fairness. The details of the baseline methods are as follows:
‚Ä¢Fixed Training (Dua & Graff, 2017) is an in-process technique that incorporates fairness constraints
as regularization term into the objective function (Chuang & Mroueh, 2020; Kamishima et al., 2012).
This approach enhances the model‚Äôs fairness by optimizing the regularization term during training.
The regularization term is represented as ‚àÜDP,‚àÜEO, and ‚àÜEodd, as shown in Equations (1), (7)
and (8).
‚Ä¢Prejudice Remover (Kamishima et al., 2012) introduces the prejudice remover as a regularization
term, ensuring independence between the prediction and the sensitive attribute. Prejudice Remover
uses mutual information to quantify the relationship between the sensitive attribute and the prediction,
thereby maintaining their independence.
‚Ä¢Adversarial Debiasing (Louppe et al., 2017) involves simultaneous training of the network for
the downstream tasks and an adversarial network. The adversarial network receives the classifier‚Äôs
output as input and is trained to differentiate between sensitive attribute groups in the output. The
classifier is trained to make accurate predictions for the input data while also training the adversarial
network not to identify groups based on the classifier‚Äôs output.
B.3 Neural Network Architectures.
The experiments were conducted using a Multilayer Perceptron (MLP) neural network architecture for tabular
data and a ResNet-18 architecture for image data.
‚Ä¢Tabular . Tabular data is structured data with a fixed number of input features. For our experiments,
we used a two-layer Multilayer Perceptron (MLP) with 256hidden neurons. The MLP architecture
is commonly used for tabular data.
‚Ä¢Image. We used a ResNet-18 architecture for image data in our experiments. The used ResNet-18
has been pre-trained on the ImageNet dataset. And we fine-tuned it for our task. The ResNet-18
architecture is widely used for image classification tasks, as it can handle the high dimensionality of
image data and learn hierarchical representations of the input features.
B.4 Experiment setting
In our experiments, we trained the neural network using the Adam optimizer (Kingma & Ba, 2014). The
optimization process took into account two important metrics: accuracy and fairness. To maintain focus on
these metrics, we trained the neural network for fixed numbers of epochs across different datasets. Specifically,
9https://github.com/zykls/folktables
10https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html
23Under review as submission to TMLR
Train One Model with Radom
0.00.51.0√Ü202530Error Rate510¬¢DPgender0.00.51.0√Ü16171819Error Rate1234¬¢DPgenderTrain Two Separate Models
0.00.51.0√Ü16.517.017.5Error Rate24¬¢DPgender
Figure 15: Comparison of Two Training Strategies. Left:Training a single model with two sets of parameters
œâ1andœâ2.Right:Training two separate models with œâ1andœâ2. The error rate in the right subfigure shows
that interpolating the weights œâ1andœâ2of the two well-trained models at inference time achieves no worse
accuracy, since the error rate is relatively high when Œ±= 0.5.
for the UCI Adult andKDD Census datasets, we trained for 2epochs, whereas the ACS-IandACS-Edatasets
required 8epochs of training. The CelebAdataset, on the other hand, demanded 30epochs for adequate
performance. We initialized the neural networks using the Xavier initialization (Glorot & Bengio, 2010).
As for the training parameters, we set the learning rate to 0.001. We also used different batch sizes for
the training process, with 512being the batch size for tabular datasets, and 128for image datasets. This
difference in batch size accommodates the varying computational requirements of the datasets. Notably, we
did not apply weight decay in our experiments. To Reviewer
21XQ: Re-
quested
Change 2:
Fully de-
scribed ex-
perimental
details.C Additional Experiments
We carried out supplementary experiments to delve deeper into the analysis of our proposed model. These
experiments encompass the following aspects: comparing the training of two separate models, examining
Equalized Odds, investigating the impact of varying the accuracy-fairness balance parameter A, and evaluating
the performance of the YOLO model with larger architectures.
C.1 Comparison to Training Two Separate Models
Training two models separately and interpolating them can not achieve our goal- flexible accuracy-fairness
trade-offs at inference time. As verified and studied by previous works Frankle et al. (2020); Wortsman et al.
(2021); Fort et al. (2020); Benton et al. (2021), interpolating the weights œâ1andœâ2of two well-trained models
at inference time has been shown to achieve no better accuracy than an untrained model. We also conduct
experiments to verify this point in our case with two settings: 1) training a single model with two sets of
parameters œâ1andœâ2. 2) Training two separate models with œâ1andœâ2. We present the results in Figure 15.
The error rate in the right subfigure shows that interpolating the weights œâ1andœâ2of the two well-trained
models at inference time achieves no worse accuracy since the error rate is relatively high when Œ±= 0.5.
C.2 Experiments on Equalized Odds
In this section, we experiment on the fairness metric Equalized Odds (Eodd) (Hardt et al., 2016). Eodd
assesses whether a classifier offers equal opportunities to individuals from diverse groups. A classifier adheres
to this criterion if it maintains equal true positive rates and false positive rates across all demographic groups.
We present a relaxed version of Eodd as follows:
‚àÜEodd (f) =|Ex‚àºD0,y=1f(x)‚àíEx‚àºD1,y=1f(x)|+|Ex‚àºD0,y=0f(x)‚àíEx‚àºD1,y=0f(x)|. (8)
We observed that YODOperforms similarly to baseline (Fixed Training) in terms of Equalized
Odds, or even better in some cases. This observation indicates our proposed method can achieve
24Under review as submission to TMLR
68Fairness (¬¢Eodd)171819Accuracy (Error Rate)UCI AdultKDD CensusACS-I-1YACS-I-5YGender
YODOFixed TraingingERMRace51015Fairness (¬¢Eodd)152025Accuracy (Error Rate)5101520Fairness (¬¢Eodd)152025Accuracy (Error Rate)
0123Fairness (¬¢Eodd)5.05.56.06.57.0Accuracy (Error Rate)1234Fairness (¬¢Eodd)5.05.56.06.57.0Accuracy (Error Rate)5.07.510.0Fairness (¬¢Eodd)182022Accuracy (Error Rate)
468Fairness (¬¢Eodd)182022Accuracy (Error Rate)6810Fairness (¬¢Eodd)171819Accuracy (Error Rate)
Figure 16: The Pareto frontier of the model accuracy and fairness. The first row is the fairness performance
with respect to gender sensitive attribute, while the second row is race sensitive attribute. The model
performance metric is Error Rate (lower is better), and the fairness metric is ‚àÜEodd(lower is better, as
shown in Equation (1)).
0.00.51.0√Ü16.517.017.5Error Rate24¬¢DPACS-I(Gender, A=1.0)
0.00.51.0√Ü16.517.017.518.0Error Rate123¬¢DPACS-I(Gender, A=1.5)
0.00.51.0√Ü1718Error Rate12¬¢DPACS-I(Gender, A=2.0)
0.00.51.0√Ü171819Error Rate12¬¢DPACS-I(Gender, A=2.5)
0.00.51.0√Ü17181920Error Rate0.51.01.5¬¢DPACS-I(Gender, A=3.0)
0.00.51.0√Ü17181920Error Rate0.51.01.5¬¢DPACS-I(Gender, A=3.5)
0.00.51.0√Ü1820Error Rate0.51.01.5¬¢DPACS-I(Gender, A=4.0)
0.00.51.0√Ü1820Error Rate0.51.01.5¬¢DPACS-I(Gender, A=4.5)
0.00.51.0√Ü182022Error Rate0.51.0¬¢DPACS-I(Gender, A=5.0)
Figure 17: The effect of the accuracy-fairness balance parameter AonACS-Idataset with Gender as the
sensitive attribute. The results show that our method can achieve flexible accuracy-fairness trade-offs at
inference time with all the values of A.
comparable or even better performance than the baseline. The result shows the effectiveness of our proposal
on other fairness metrics, demonstrating its overall effectiveness in promoting fairness.
25Under review as submission to TMLR
0.00.51.0√Ü16.016.5Error Rate123¬¢DPACS-I(Race, A=1.0)
0.00.51.0√Ü16.517.017.5Error Rate12¬¢DPACS-I(Race, A=1.5)
0.00.51.0√Ü16.517.017.518.0Error Rate0.51.01.5¬¢DPACS-I(Race, A=2.0)
0.00.51.0√Ü1718Error Rate0.51.0¬¢DPACS-I(Race, A=2.5)
0.00.51.0√Ü171819Error Rate0.51.01.5¬¢DPACS-I(Race, A=3.0)
0.00.51.0√Ü171819Error Rate0.51.0¬¢DPACS-I(Race, A=3.5)
0.00.51.0√Ü17181920Error Rate0.20.40.60.8¬¢DPACS-I(Race, A=4.0)
0.00.51.0√Ü1820Error Rate0.40.60.8¬¢DPACS-I(Race, A=4.5)
0.00.51.0√Ü182022Error Rate0.10.20.3¬¢DPACS-I(Race, A=5.0)
Figure 18: The effect of the accuracy-fairness balance parameter AonACS-Idataset with Race as the
sensitive attribute. The results show that our method can achieve flexible accuracy-fairness trade-offs at
inference time with all the values of A.
C.3 Impact of Different Values of Accuracy-fairness Balance Parameter A
In addition to the combined results for different Avalues presented in Figure 11, we also display the results
for each individual Avalue in separate figures, as shown in Figure 17 and Figure 18. The results demonstrate
that models with various Avalues can achieve a range of accuracy-fairness trade-offs. However, as Aincreases,
the overall accuracy of the downstream task declines while the span of the fairness metric ‚àÜDPexpands.
C.4 The Performance of YODOwith Larger ModelsTo Re-
viewer zQ7a:
Concern 2:
larger mod-
els.We conducted experiments on larger models using the CelebAdataset, specifically examining ResNet18,
ResNet34, ResNet50, ResNet101, WideResNet50, and WideResNet101 (He et al., 2016; Zagoruyko &
Komodakis, 2016). We investigated the accuracy-fairness trade-off behavior of these models while varying
theŒ±parameter. We note that all the results are referenced with one model at the inference time. Our
observations revealed that 1) All models were capable of achieving a flexible accuracy-fairness trade-off
during inference. 2) Larger models demonstrated greater stability, with smaller models like ResNet18 and
ResNet34 exhibiting more fluctuations, while larger models such as ResNet50, ResNet101, WideResNet50,
and WideResNet101 showed smoother performance.
26Under review as submission to TMLR
0.00.51.0√Ü2025Error Rate1020¬¢DPResNet-18
0.00.51.0√Ü1520Error Rate10203040¬¢DPResNet-34
0.00.51.0√Ü12.515.017.520.0Error Rate10203040¬¢DPResNet-50
0.00.51.0√Ü152025Error Rate02040¬¢DPResNet-101
0.00.51.0√Ü1520Error Rate102030¬¢DPWideResNet-50
0.00.51.0√Ü1520Error Rate102030¬¢DPWideResNet-101
Figure 19: Performance of YODOwith larger meodels.
27