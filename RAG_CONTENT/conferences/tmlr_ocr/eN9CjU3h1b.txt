Published in Transactions on Machine Learning Research (01/2024)
MMD-Regularized Unbalanced Optimal Transport
Piyushi Manupriya cs18m20p100002@iith.ac.in
Department of Computer Science and Engineering, IIT Hyderabad, INDIA.
J. SakethaNath saketha@cse.iith.ac.in
Department of Computer Science and Engineering, IIT Hyderabad, INDIA.
Pratik Jawanpuria pratik.jawanpuria@microsoft.com
Microsoft, INDIA.
Reviewed on OpenReview: https: // openreview. net/ forum? id= eN9CjU3h1b
Abstract
We study the unbalanced optimal transport (UOT) problem, where the marginal constraints
are enforced using Maximum Mean Discrepancy (MMD) regularization. Our work is moti-
vated by the observation that the literature on UOT is focused on regularization based on
ϕ-divergence (e.g., KL divergence). Despite the popularity of MMD, its role as a regular-
izer in the context of UOT seems less understood. We begin by deriving a specific dual of
MMD-regularized UOT (MMD-UOT), which helps us prove several useful properties. One
interesting outcome of this duality result is that MMD-UOT induces novel metrics, which
not only lift the ground metric like the Wasserstein but are also sample-wise efficient to es-
timate like the MMD. Further, for real-world applications involving non-discrete measures,
we present an estimator for the transport plan that is supported only on the given ( m)
samples. Under certain conditions, we prove that the estimation error with this finitely-
supported transport plan is also O(1/√m). As far as we know, such error bounds that
are free from the curse of dimensionality are not known for ϕ-divergence regularized UOT.
Finally, we discuss how the proposed estimator can be computed efficiently using acceler-
ated gradient descent. Our experiments show that MMD-UOT consistently outperforms
popular baselines, including KL-regularized UOT and MMD, in diverse machine learning
applications.
1 Introduction
Optimal transport (OT) is a popular tool for comparing probability measures while incorporating geometry
overtheirsupport. OThaswitnessedalotofsuccessinmachinelearningapplications(Peyré&Cuturi,2019),
where distributions play a central role. The Kantorovich’s formulation for OT aims to find an optimal plan
forthetransportofmassbetweenthesourceandthetargetdistributionsthatincurstheleastexpectedcostof
transportation. While classical OT strictly enforces the marginals of the transport plan to be the source and
target, one would want to relax this constraint when the measures are noisy (Frogner et al., 2015) or when
the source and target are un-normalized (Chizat, 2017; Liero et al., 2018). Unbalanced optimal transport
(UOT) (Chizat, 2017), a variant of OT, is employed in such cases, which performs a regularization-based
soft-matching of the transport plan’s marginals with the source and the target distributions.
Unbalanced optimal transport with Kullback Leibler (KL) divergence and, in general, with ϕ-
divergence (Csiszar, 1967) based regularization is well-explored in literature (Liero et al., 2016; 2018).
Entropy regularized UOT with KL divergence (Chizat et al., 2017; 2018) has been employed in applica-
tions such as domain adaptation (Fatras et al., 2021), natural language processing (Chen et al., 2020b),
and computer vision (De Plaen et al., 2023). Existing works (Piccoli & Rossi, 2014; 2016; Hanin, 1992;
Georgiou et al., 2009) have also studied total variation (TV)-regularization-based UOT formulations. While
1Published in Transactions on Machine Learning Research (01/2024)
MMD-based methods have been popularly employed in several machine learning (ML) applications (Gretton
et al., 2012; Li et al., 2017; 2021; Nguyen et al., 2021), the applicability of MMD-based regularization for
UOT is not well-understood. To the best of our knowledge, interesting questions like the following, have not
been answered in prior works:
•Will MMD regularization for UOT also lead to novel metrics over measures, analogous to the ones
obtained with the KL divergence (Liero et al., 2018) or the TV distance (Piccoli & Rossi, 2014)?
•What will be the statistical estimation properties of these?
•How can such MMD regularized UOT metrics be estimated in practice such that they are suitable
for large-scale applications?
In order to bridge this gap, we study MMD-based regularization for matching the marginals of the transport
plan in the UOT formulation (henceforth termed MMD-UOT).
We first derive a specific dual of the MMD-UOT formulation (Theorem 4.1), which helps further analyze its
properties. One interesting consequence of this duality result is that the optimal objective of MMD-UOT
is a valid distance between the source and target measures (Corollary 4.2), whenever the transport cost is
valid (ground) metric over the data points. Popularly, this is known as the phenomenon of lifting metrics to
measures. This result is significant as it shows that MMD-regularization in UOT can parallel the metricity-
preservation that happens with KL-regularization (Liero et al., 2018) and TV-regularization (Piccoli & Rossi,
2014). Furthermore, our duality result shows that this induced metric is a novel metric belonging to the
family of integral probability metrics (IPMs) with a generating set that is the intersection of the generating
sets of MMD and the Kantorovich-Wasserstein metric. Because of this important relation, the proposed
distance is always smaller than the MMD distance, and hence, estimating MMD-UOT from samples is at
least as efficient as that with MMD (Corollary 4.6). This is interesting as minimax estimation rates for MMD
can be completely dimension-free. As far as we know, there are no such results that show that estimation
with KL/TV-regularized UOT can be as efficient sample-wise. Thus, the proposed metrics not only lift the
ground metrics to measures, like the Wasserstein, but also are sample-wise efficient to estimate, like MMD.
However, like any formulation of optimal transport problems, the computation of MMD-UOT involves op-
timization over all possible joint measures. This may be challenging, especially when the measures are
continuous. Hence, we present a convex program-based estimator, which only involves a search over joints
supported at the samples. We prove that the proposed estimator is statistically consistent and converges to
MMD-UOT between the true measures at a rate O/parenleftig
m−1
2/parenrightig
, wheremis the number of samples. Such efficient
estimators are particularly useful in machine learning applications, where typically only samples from the
underlying measures are available. Such applications include hypothesis testing, domain adaptation, and
model interpolation, to name a few. In contrast, the minimax estimation rate for the Wasserstein distance is
itselfO/parenleftig
m−1
d/parenrightig
, wheredis the dimensionality of the samples (Niles-Weed & Rigollet, 2019). That is, even if
a search over all possible joints is performed, estimating Wasserstein may be challenging. Since MMD-UOT
can approximate Wasserstein arbitrarily closely (as the regularization hyperparameter goes ∞), our result
can also be understood as a way of alleviating the curse of dimensionality problem in Wasserstein. We
summarize the comparison between MMD-UOT and relevant OT variants in Table 1.
Finally, our result of MMD-UOT being a metric facilitates its application whenever the metric properties
of OT are desired, for example, while computing the barycenter-based interpolation for single-cell RNA
sequencing (Tong et al., 2020). Accordingly, we also present a finite-dimensional convex-program-based
estimator for the barycenter with MMD-UOT. We prove that this estimator is also consistent with an
efficient sample complexity. We discuss how the formulations for estimating MMD-UOT (and barycenter)
can be solved efficiently using accelerated (projected) gradient descent. This solver helps us scale well
to large datasets. We empirically show the utility of MMD-UOT in several applications including two-
sample hypothesis testing, single-cell RNA sequencing, domain adaptation, and prompt learning for few-
shot classification. In particular, we observe that MMD-UOT outperforms popular baselines such as KL-
regularized UOT and MMD in our experiments.
We summarize our main contributions below:
2Published in Transactions on Machine Learning Research (01/2024)
Table 1: Summarizing interesting properties of MMD and several OT/UOT approaches. ϵOT (Cuturi, 2013)
andϵKL-UOT (Chizat, 2017) denote the entropy-regularized scalable variants OT and KL-UOT (Liero
et al., 2018), respectively. MMD and the proposed MMD-UOT are shown with characteristic kernels. By
‘finite-parameterization bounds’ we mean results similar to Theorem 4.10.
Property MMD OT ϵOT TV-UOT KL-UOT ϵKL-UOT MMD-UOT
Metricity
Lifting of ground metric
No curse of dimensionality
Finite-parametrization bounds N/A
•Dual of MMD-UOT and its analysis. We prove that MMD-UOT induces novel metrics that not only
lift ground metrics like the Wasserstein but also are sample-wise efficient to estimate like the MMD.
•Finite-dimensionalconvex-program-basedestimatorsforMMD-UOTandthecorrespondingbarycen-
ter. We prove that the estimators are both statistically and computationally efficient.
•We illustrate the efficacy of MMD-UOT in several real-world applications. Empirically, we observe
that MMD-UOT consistently outperforms popular baseline approaches.
We present proofs for all our theory results in Appendix B. As a side-remark, we note that most of our
results not only hold for MMD-UOT but also for a UOT formulation where a general IPM replaces MMD.
Proofs in the appendix are hence written for general IPM-based regularization and then specialized to the
case when the IPM is MMD. This generalization to IPMs may itself be of independent interest.
2 Preliminaries
Notations. LetXbe a set (domain) that forms a compact Hausdorff space. Let R+(X),R(X)denote
the set of all non-negative, signed (finite) Radon measures defined over X; while the set of all probability
measures is denoted by R+
1(X). For a measure on the product space, π∈R+(X×X ), letπ1,π2denote
the first and second marginals, respectively (i.e., they are the push-forwards under the canonical projection
maps ontoX). LetL(X),C(X)denote the set of all real-valued measurable functions and all real-valued
continuous functions, respectively, over X.
IntegralProbabilityMetric(IPM): GivenasetG⊂L (X),theintegralprobabilitymetric(IPM)(Muller,
1997; Sriperumbudur et al., 2009; Agrawal & Horel, 2020) associated with G, is defined by:
γG(s0,t0)≡max
f∈G/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Xfds0−/integraldisplay
Xfdt0/vextendsingle/vextendsingle/vextendsingle/vextendsingle∀s0,t0∈R+(X). (1)
Gis called the generating set of the IPM, γG.
Maximum Mean Discrepancy (MMD) Letkbe a characteristic kernel (Sriperumbudur et al., 2011)
over the domainX, let∥f∥kdenote the norm of fin the canonical reproducing kernel Hilbert space (RKHS),
Hk, corresponding to k. MMDkis the IPM associated with the generating set: Gk≡{f∈Hk|∥f∥k≤1}.
Using a characteristic kernel k, MMD metric between s0,t0∈R+(X)is defined as:
MMDk(s0,t0)≡max
f∈Gk/vextendsingle/vextendsingle/integraltext
Xfds0−/integraltext
Xfdt0/vextendsingle/vextendsingle
=∥µk(s0)−µk(t0)∥k,(2)
whereµk(s)≡/integraltext
ϕk(x)ds(x), is the kernel mean embedding of s(Muandet et al., 2017), ϕkis the canonical
featuremapof k. Akernelkiscalledacharacteristickernelifthemap µkisinjective. MMDcanbecomputed
analytically using evaluations of the kernel k. MMDkis a metric when the kernel kis characteristic. A
continuous positive-definite kernel konXis called c-universal if the RKHS Hkis dense inC(X)w.r.t. the
3Published in Transactions on Machine Learning Research (01/2024)
sup-norm, i.e., for every function g∈C(X)and allϵ >0, there exists an f∈Hksuch that∥f−g∥∞≤ϵ.
Universal kernels are also characteristic. Gaussian kernel (RBF kernel) is an example of a universal kernel
over the continuous domain. Dirac delta kernel is an example of a universal kernel over the discrete domain.
Optimal Transport (OT) Optimal transport provides a tool to compare distributions while incorporating
the underlying geometry of their support points. Given a cost function, c:X×X∝⇕⊣√∫⊔≀→ R, and two probability
measuress0∈R+
1(X),t0∈R+
1(X), thep-Wasserstein Kantorovich OT formulation is given by:
¯Wp
p(s0,t0)≡ min
π∈R+
1(X×X )/integraldisplay
cpdπ,s.t.π1=s0, π2=t0, (3)
wherep≥1. An optimal solution of (3) is called an optimal transport plan. Whenever the cost is a metric,
d, overX×X(ground metric), ¯Wpdefines a metric over measures, known as the p-Wasserstein metric, over
R+
1(X)×R+
1(X).
Kantorovich metric ( Kc)Kantorovich metric also belongs to the family of integral probability metrics
associated with the generating set Wc≡/braceleftbigg
f:X∝⇕⊣√∫⊔≀→R|max
x∈X̸=y∈X|f(x)−f(y)|
c(x,y)≤1/bracerightbigg
, wherecis a metric over
X×X. The Kantorovich-Rubinstein duality result shows that the 1-Wasserstein metric is the same as the
Kantorovich metric when restricted to probability measures (refer for e.g. (5.11) in Villani (2009)):
¯W1(s0,t0)≡ min
π∈R+
1(X×X )/integraldisplay
cpdπ,= max
f∈G/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Xfds0−/integraldisplay
Xfdt0/vextendsingle/vextendsingle/vextendsingle/vextendsingle≡ Kc(s0,t0),
s.t.π1=s0, π2=t0
wheres0,t0∈R+
1(X).
3 Related Work
Given the source and target measures, s0∈R+(X)andt0∈R+(X), respectively, the unbalanced opti-
mal transport (UOT) approach (Liero et al., 2018; Chizat et al., 2018) aims to learn the transport plan
by replacing the mass conservation marginal constraints (enforced strictly in ‘balanced’ OT setting) by a
soft regularization/penalization on the marginals. KL-divergence and, in general, ϕ-divergence (Csiszar,
1967), (Sriperumbudur et al., 2009) based regularizations have been most popularly studied in UOT setting.
Theϕ-divergence regularized UOT formulation may be written as (Frogner et al., 2015), (Chizat, 2017):
min
π∈R+(X×X )/integraldisplay
cdπ+λDϕ(π1,s0) +λDϕ(π2,t0), (4)
wherecis the ground cost metric and Dϕ(·,·)denotes the ϕ-divergence (Csiszar, 1967; Sriperumbudur
et al., 2009) between two measures. Since in UOT settings, the measures s0, t0may be un-normalized,
following (Chizat, 2017; Liero et al., 2018) the transport plan is also allowed to be un-normalized. UOT
with KL-divergence-based regularization induces the so-called Gaussian Hellinger-Kantorovich metric (Liero
et al., 2018) between the measures whenever 0< λ≤1and the ground cost cis the squared-Euclidean
distance. Similar to the balanced OT setup (Cuturi, 2013), an additional entropy regularization in KL-UOT
formulation facilitates Sinkhorn iteration (Knight, 2008) based efficient solver for KL-UOT (Chizat et al.,
2017) and has been popularly employed in several machine learning applications (Fatras et al., 2021; Chen
et al., 2020b; Arase et al., 2023; De Plaen et al., 2023).
Total Variation (TV) distance is another popular metric between measures and is the only common member
of theϕ-divergence family and the IPM family. UOT formulation with TV regularization (denoted by |·|TV)
has been studied in (Piccoli & Rossi, 2014):
min
π∈R+(X×X )/integraldisplay
cdπ+λ|π1−s0|TV+λ|π2−t0|TV. (5)
UOT with TV-divergence-based regularization induces the so-called Generalized Wasserstein metric (Piccoli
& Rossi, 2014) between the measures whenever λ > 0and the ground cost cis a valid metric. As far as
4Published in Transactions on Machine Learning Research (01/2024)
we know, none of the existing works study the sample complexity of estimating these metrics from samples.
More importantly, algorithms for solving (5) with empirical measures that computationally scale well to ML
applications seem to be absent in the literature.
Besides the family of ϕ-divergences, the family of integral probability metrics is popularly used for comparing
measures. AnimportantmemberoftheIPMfamilyistheMMDmetric, whichalsoincorporatesthegeometry
oversupportsthroughtheunderlyingkernel. Duetoitsattractivestatisticalproperties(Grettonetal.,2006),
MMD has been successfully applied in a diverse set of applications including hypothesis testing (Gretton
et al., 2012), generative modelling (Li et al., 2017), self-supervised learning (Li et al., 2021), etc.
Recently, (Nath & Jawanpuria, 2020) explored learning the transport plan’s kernel mean embeddings in the
balanced OT setup. They proposed learning the kernel mean embedding of a joint distribution with the least
expected cost and whose marginal embeddings are close to the given-sample-based estimates of the marginal
embeddings. As kernel mean embedding induces MMD distance, MMD-based regularization features in
the balanced OT formulation of (Nath & Jawanpuria, 2020) as a means to control overfitting. To ensure
that valid conditional embeddings are obtained from the learned joint embeddings, (Nath & Jawanpuria,
2020) required additional feasibility constraints that restrict their solvers in scaling well to machine learning
applications. We also note that (Nath & Jawanpuria, 2020) neither analyze the dual of their formulation
nor study its metric-related properties and their sample complexity result of O(m−1
2)does not apply to our
MMD-UOT estimator as their formulation is different from the proposed MMD-UOT formulation (6).
In contrast, we bypass the issues related to the validity of conditional embeddings as our formulation involves
directly learning the transport plan and avoids kernel mean embedding of the transport plan. We perform
a detailed study of MMD regularization for UOT, which includes analyzing its dual and proving metric
properties that are crucial for optimal transport formulations. To the best of our knowledge, the metricity
of MMD-regularized UOT formulations has not been studied previously. The proposed algorithm scales
well to large-scale machine learning applications. While we also obtain O(m−1
2)estimation error rate, we
require a different proof strategy than (Nath & Jawanpuria, 2020). Finally, as discussed in Appendix B,
most of our theoretical results apply to a general IPM-regularized UOT formulation and are not limited to
the MMD-regularized UOT formulation. This generalization does not hold for (Nath & Jawanpuria, 2020).
Wasserstein auto-encoders (WAE) also employ MMD for regularization. However, there are some important
differences. The regularization in WAEs is only performed for one of the marginals, and the other marginal is
matched exactly. This not only breaks the symmetry (and hence the metric properties) but also brings back
the curse of dimensionality in estimation (for the same reasons as with unregularized OT). Further, their
work does not attempt to study any theoretical properties with MMD regularization and merely employs
it as a practical tool for matching marginals. Our goal is to theoretically study the metric and estimation
properties with MMD regularization. We present more details in Appendix B.18.
We end this section by noting key differences between MMD and OT-based approaches (including MMD-
UOT). A distinguishing feature of OT-based approaches is the phenomenon of lifting the ground-metric
geometry to that over distributions. One such result is visualized in Figure 2(b), where the MMD-based-
interpolate of the two unimodal distributions comes out to be bimodal. This is because MMD’s interpolation
is the (literal) average of the source and the target densities, irrespective of the kernel. This has been
well-established in the literature (Bottou et al., 2017). On the other hand, OT-based approaches obtain
a unimodal barycenter. This is a ‘geometric’ interpolation that captures the characteristic aspects of the
source and the target distributions. Another feature of OT-based methods is that we obtain a transport plan
between the source and the target points which can be used for various alignment-based applications, e.g.,
cross-lingual word mapping (Alvarez-Melis & Jaakkola, 2018; Jawanpuria et al., 2020), domain adaptation
(Courty et al., 2017; Courty et al., 2017; Gurumoorthy et al., 2021), etc. On the other hand, it is unclear
how MMD can be used to align the source and target data points.
5Published in Transactions on Machine Learning Research (01/2024)
4 MMD Regularization for UOT
We propose to study the following UOT formulation, where the marginal constraints are enforced using
MMD regularization.
Uk,c,λ 1,λ2(s0,t0)≡ min
π∈R+(X×X )/integraltext
cdπ+λ1MMDk(π1,s0) +λ2MMDk(π2,t0)
= min
π∈R+(X×X )/integraltext
cdπ+λ1∥µk(π1)−µk(s0)∥k+λ2∥µk(π2)−µk(t0)∥k,(6)
whereµk(s)is the kernel mean embedding of s(defined in Section 2) induced by the characteristic kernel k
used in the generating set Gk≡{f∈Hk|∥f∥k≤1}, andλ1,λ2>0are the regularization hyper-parameters.
We begin by presenting a key duality result.
Theorem 4.1. (Duality) Wheneverc,k∈C(X×X )andXis compact, we have that:
Uk,c,λ 1,λ2(s0,t0) = max
f∈Gk(λ1),g∈Gk(λ2)/integraltext
Xfds0+/integraltext
Xgdt0,
s.t.f(x) +g(y)≤c(x,y)∀x,y∈X.(7)
Here,Gk(λ)≡{g∈Hk|∥g∥k≤λ}.
The duality result helps us to study several properties of the MMD-UOT (6), discussed in the corollaries
below. The proof of Theorem 4.1 is based on an application of Sion’s minimax exchange theorem (Sion,
1958) and is detailed in Appendix B.1.
Applications in machine learning often involve comparing distributions for which the Wasserstein metric is a
popular choice. While prior works have shown metric-preservation happens under KL-regularization (Liero
et al., 2018) and TV-regularization (Piccoli & Rossi, 2016), it is an open question if MMD-regularization in
UOT can also lead to valid metrics. The following result answers this affirmatively.
Corollary 4.2. (Metricity) In addition to assumptions in Theorem (4.1), whenever cis a metric,Uk,c,λ,λ
belongs to the family of integral probability metrics (IPMs). Also, the generating set of this IPM is the
intersection of the generating set of the Kantorovich metric and the generating set of MMD. Finally, Uk,c,λ,λ
is a valid norm-induced metric over measures whenever kis characteristic. Thus, Uliftsthe ground metric
cto that over measures.
The proof of Corollary 4.2 is detailed in Appendix B.2. This result also reveals interesting relationships be-
tweenUk,c,λ,λ, the Kantorovich metric, Kc, and the MMD metric used for regularization. This is summarized
in the following two results.
Corollary 4.3. (Interpolant) In addition to assumptions in Corollary 4.2, if the kernel is c-universal
(continuous and universal), then ∀s0,t0∈ R+(X),limλ→∞Uk,c,λ,λ (s0,t0) =Kc(s0,t0). Fur-
ther, if the cost metric, c, dominates the characteristic kernel, k, induced metric, i.e., c(x,y)≥/radicalbig
k(x,x) +k(y,y)−2k(x,y)∀x,y∈ X, thenUk,c,λ,λ (s0,t0) =λMMDk(s0,t0)whenever 0< λ≤1.
Finally, when λ∈(0,1), MMD-UOT interpolates between the scaled MMD and the Kantorovich metric. The
nature of this interpolation is already described in terms of generating sets in Corollary 4.2.
We illustrate this interpolation result in Figure 1. Our proof of Corollary 4.3, presented in Appendix B.3,
also shows that the Euclidean distance satisfies such a dominating cost assumption when the kernel employed
is the Gaussian kernel and the inputs lie on a unit-norm ball. The next result presents another relationship
between the metrics in the discussion.
Corollary 4.4.Uk,c,λ,λ (s,t)≤min (λMMDk(s,t),Kc(s,t)).
TheproofofCorollary4.4isstraightforwardandispresentedinAppendixB.5. Thisresultenablesustoshow
properties like weak metrization and sample efficiency with MMD-UOT. For a sequence sn∈R+
1(X), n≥1,
we say that snweakly converges to s∈ R+
1(X)(denoted as sn⇀ s), if and only if EX∼sn[f(X)]→
EX∼s[f(X)]for all bounded continuous functions over X. It is natural to ask when is the convergence in
metric over measures equivalent to weak convergence on measures. The metric is then said to metrize the
6Published in Transactions on Machine Learning Research (01/2024)
0<𝜆≤1 ; Kernel-based cost.𝜆∈(1,∞)𝜆→∞  (a) MMD                                 (b) Kantorovich                    (c) New UOT metrics
Figure 1: For illustration, the generating set of Kantorovich-Wasserstein is depicted as a triangle, and the
scaled generating set of MMD is depicted as a disc. The intersection represents the generating set of the
IPM metric induced by MMD-UOT. (a) shows the special case when our MMD-UOT metric recovers back
the sample-efficient MMD metric, (b) shows the special case when our MMD-UOT metric reduces to the
Kantorovich-Wasserstein metric that lifts the ground metric to measures, and (c) shows the resulting family
of new UOT metrics which are both sample-efficient and can lift ground metrics to measures.
weak convergence of measures or is equivalently said to weakly metrize measures. The weak metrization
properties of the Wasserstein metric and MMD are well-understood (e.g., refer to Theorem 6.9 in (Villani,
2009) and Theorem 7 in (Simon-Gabriel et al., 2020)). The weak metrization property of Uk,c,λ,λfollows
from the above Corollary 4.4.
Corollary 4.5. (Weak Metrization) Uk,c,λ,λmetrizes the weak convergence of normalized measures.
The proof is presented in Appendix B.6. We now show that the metric induced by MMD-UOT inherits
the attractive statistical efficiency of the MMD metric. In typical machine learning applications, only
finite samples are given from the measures. Hence, it is important to study statistically efficient metrics
that alleviate the curse of dimensionality problem prevalent in OT (Niles-Weed & Rigollet, 2019). Sample
complexity result with the metric induced by MMD-UOT is presented as follows.
Corollary 4.6. (Sample Complexity) Let us denoteUk,c,λ,λ, defined in (6), by ¯U. Let ˆsm,ˆtmdenote the
empirical estimates of s0,t0∈R+(X)respectively with msamples. Then, ¯U(ˆsm,ˆtm)→¯U(s0,t0)at a rate
(apart from constants) same as that of MMDk(ˆsm,s0)→0.
Since the sample complexity of MMD with a normalized characteristic kernel is O(m−1
2)(Smola et al.,
2007), the same will be the complexity bound for the corresponding MMD-UOT. The proof of Corollary 4.6 is
presented in Appendix B.7. This is interesting because, though MMD-UOT can arbitrarily well approximate
Wasserstein (as λ→∞), its estimation can be far more efficient than O/parenleftig
m−1
d/parenrightig
, which is the minimax
estimation rate for the Wasserstein (Niles-Weed & Rigollet, 2019). Here, dis the dimensionality of the
samples. Further, in Lemma B4, we show that even when MMDq
k(q≥2∈N) is used for regularization, the
sample complexity again comes out to be O/parenleftig
m−1
2/parenrightig
. We conclude this section with a couple of remarks.
Remark 4.7. As a side result, we prove the following theorem (Appendix B.8) that relates our MMD-UOT to
the MMD-regularized Kantorovich metric. We believe this connection is interesting as it generalizes the pop-
ular Kantorovich-Rubinstein duality result on relating (unregularized) OT to the (unregularized) Kantorovich
metric.
Theorem 4.8. In addition to the assumptions in Theorem 4.1, if cis a valid metric, then
Uk,c,λ 1,λ2(s0,t0) = min
s,t∈R(X)Kc(s,t) +λ1MMDk(s,s0) +λ2MMDk(t,t0). (8)
Remark 4.9. It is noteworthy that most of our theoretical results presented in this section not only hold
with the MMD-UOT formulation (9) but also with a general IPM-regularized UOT formulation, which we
discuss in Appendix B. This generalization may be of independent interest for future work.
Finally, minor results on robustness and connections with spectral normalized GAN (Miyato et al., 2018)
are discussed in Appendix B.16 and Appendix B.17, respectively.
7Published in Transactions on Machine Learning Research (01/2024)
4.1 Finite-Sample-based Estimation
As noted in Corollary 4.6, MMD-UOT can be efficiently estimated from samples of source and target.
However, one needs to solve an optimization problem over all possible joint (un-normalized) measures. This
can be computationally expensive1(for example, optimization over the set of all joint density functions).
Hence, in this section, we propose a simple estimator where the optimization is only over the joint measures
supported at sample-based points. We show that our estimator is statistically consistent and that the
estimation is free from the curse of dimensionality.
Letmsamples be given from the source, target, s0, t0∈ R+(X)respectively2. We denoteDi=
{xi1,···xim},i= 1,2as the set of samples given from s0,t0respectively. Let ˆsm,ˆtmdenote the empiri-
cal measures using samples D1,D2. Let us denote the Gram-matrix of DibyGii. LetC12be them×m
cost matrix with entries as evaluations of the cost function over D1×D 2. Following the common practice
in OT literature (Chizat et al., 2017; Cuturi, 2013; Damodaran et al., 2018; Fatras et al., 2021; Le et al.,
2021; Balaji et al., 2020; Nath & Jawanpuria, 2020; Peyré & Cuturi, 2019), we restrict the transport plan
to be supported on the finite samples from each of the measures in order to avoid the computational issues
in optimizing over all possible joint densities. More specifically, let αbe them×m(parameter/variable)
matrix with entries as αij≡π(x1i,x2j)wherei,j∈{1,···,m}. With these notations and the mentioned
restricted feasibility set, Problem (6) simplifies to the following, denoted by ˆUm(ˆsm,ˆtm):
min
α≥0∈Rm×mTr/parenleftbig
αC⊤
12/parenrightbig
+λ1/vextenddouble/vextenddouble/vextenddoubleα1−σ1
m1/vextenddouble/vextenddouble/vextenddouble
G11+λ2/vextenddouble/vextenddouble/vextenddoubleα⊤1−σ2
m1/vextenddouble/vextenddouble/vextenddouble
G22, (9)
where Tr (M)denotes the trace of matrix M,∥x∥M≡√
x⊤Mx, andσ1,σ2are the masses of the source,
target measures, s0,t0, respectively. Since this is a Convex Program over a finite-dimensional variable, it
can be solved in a computationally efficient manner (refer Section 4.2).
However, as the transport plan is now supported on the given samples alone, Corollary 4.6 does not apply.
Thefollowingresultshowsthatourestimator(9)isconsistent, andtheestimationerrordecaysatafavourable
rate.
Theorem 4.10. (Consistency of the proposed estimator) Let us denoteUk,c,λ 1,λ2, defined in (6),
by¯U. Assume the domain Xis compact, ground cost is continuous, c∈C(X×X ), and the kernel kis
c-universal, normalized. Let the source measure ( s0), the target measure ( t0), as well as the corresponding
MMD-UOT transport plan be absolutely continuous. Also assume s0(x),t0(x)>0∀x∈X. Then, we
have w.h.p. and any (arbitrarily small) ϵ>0that/vextendsingle/vextendsingle/vextendsingleˆUm(ˆsm,ˆtm)−¯U(s0,t0)/vextendsingle/vextendsingle/vextendsingle≤O/parenleftig
λ1+λ2√m+g(ϵ)
m+ϵσ/parenrightig
. Here,
g(ϵ)≡minv∈Hk⊗Hk∥v∥ks.t.∥v−c∥∞≤ϵ, andσis the mass of the optimal MMD-UOT transport plan.
Further, if cbelongs toHk⊗Hk, then w.h.p./vextendsingle/vextendsingle/vextendsingleˆUm(ˆsm,ˆtm)−¯U(s0,t0)/vextendsingle/vextendsingle/vextendsingle≤O/parenleftig
λ1+λ2√m/parenrightig
.
We discuss the proof of the above theorem in Appendix B.9. Because kis universal, g(ϵ)<∞∀ϵ >0.
The consistency of our estimator as m→ ∞can be realized, if, for example, one employs the scheme
λ1=λ2=O(m1/4)andϵ→0at a slow enough rate such thatg(ϵ)
m→0. In Appendix B.9.1, we show that
even ifϵdecays as fast as O/parenleftbig1
m2/3/parenrightbig
, theng(ϵ)blows-up atmost as O/parenleftbig
m1/3/parenrightbig
. Hence, overall, the estimation
error still decays as O/parenleftbig1
m1/4/parenrightbig
. To the best of our knowledge, such consistency results have not been studied
in the context of KL-regularized UOT.
4.2 Computational Aspects
Problem (9) is an instance of a convex program and can be solved using the mirror descent algorithm detailed
in Appendix B.10. In the following, we propose to solve an equivalent optimization problem which helps us
leverage faster solvers for MMD-UOT:
min
α≥0∈Rm×mTr/parenleftbig
αC⊤
12/parenrightbig
+λ1/vextenddouble/vextenddouble/vextenddoubleα1−σ1
m1/vextenddouble/vextenddouble/vextenddouble2
G11+λ2/vextenddouble/vextenddouble/vextenddoubleα⊤1−σ2
m1/vextenddouble/vextenddouble/vextenddouble2
G22. (10)
1Note that this challenge is inherent to OT (and all its variants). It is not a consequence of our choice of MMD regularization.
2The no. of samples from source and target need not be the same, in general.
8Published in Transactions on Machine Learning Research (01/2024)
Algorithm 1 Accelerated Projected Gradient Descent for solving Problem (10).
Require: Lipschitz constant L, initialα0≥0∈Rm×m.
f(α) =Tr/parenleftbig
αC⊤
12/parenrightbig
+λ1/vextenddouble/vextenddoubleα1−σ1
m1/vextenddouble/vextenddouble2
G11+λ2/vextenddouble/vextenddoubleα⊤1−σ2
m1/vextenddouble/vextenddouble2
G22.
γ1= 1.
y1=α0.
i= 0.
whilenot converged do
αi=Project≥0/parenleftbig
yi−1
L∇f(yi)/parenrightbig
.
γi+1=1+√
1+4γ2
i
2.
yi+1=αi+γi−1
γi+1(αi−αi−1).
i=i+ 1.
end while
returnαi.
The equivalence between (9) and (10) follows from standard arguments and is detailed in Appendix B.11.
Our next result shows that the objective in (10) is L-smooth (proof provided in Appendix B.12).
Lemma 4.11. The objective in Problem (10) is L-smooth with L =
2/radicalbig
(λ1m)2∥G11∥2
F+ (λ2m)2∥G22∥2
F+ 2λ1λ2(1⊤mG111m+1⊤mG221m).
The above result enables us to use the accelerated projected gradient descent (APGD) algorithm (Nesterov,
2003; Beck & Teboulle, 2009) with fixed step-size τ= 1/Lfor solving (10). The detailed steps are pre-
sented in Algorithm 1. The overall computation cost for solving MMD-UOT (10) is O(m2
√ϵ), whereϵis the
optimality gap. In Section 5, we empirically observe that the APGD-based solver for MMD-UOT is indeed
computationally efficient.
4.3 Barycenter
A related problem is that of barycenter interpolation of measures (Agueh & Carlier, 2011), which has
interesting applications (Solomon et al., 2014; 2015; Gramfort et al., 2015). Given measures s1,...,snwith
total masses σ1,...,σnrespectively, and interpolation weights ρ1,...,ρn, the barycenter s∈ R+(X)is
defined as the solution of ¯B(s1,···,sn)≡mins∈R+(X)/summationtextn
i=1ρiUk,c,λ 1,λ2(si,s).
In typical applications, only sample sets, Di, fromsiare available instead of sithemselves. Let us denote
the corresponding empirical measures by ˆs1,..., ˆsn. One way to estimate the barycenter is to consider
¯B(ˆs1,···,ˆsn). However, this may be computationally challenging to optimize, especially when the measures
involved are continuous. So we propose estimating the barycenter with the restriction that the transport
planπicorresponding to Uk,c,λ 1,λ2(ˆsi,s)is supported onDi×∪n
i=1Di. And, letαi≥0∈Rmi×mdenote the
corresponding probabilities. Following (Cuturi & Doucet, 2014), we also assume that the barycenter, s, is
supported on∪n
i=1Di. Let us denote the barycenter problem with this support restriction on the transport
plans and the Barycenter as ˆBm(ˆs1,···,ˆsn). LetGbe the Gram-matrix of ∪n
i=1DiandCibe themi×m
matrix with entries as evaluations of the cost function.
Lemma 4.12. The barycenter problem ˆBm(ˆs1,···,ˆsn)can be equivalently written as:
minα1,···,αn≥0/summationtextn
i=1ρi/parenleftig
Tr/parenleftbig
αiC⊤
i/parenrightbig
+λ1∥αi1−σi
mi1∥2
Gii+λ2∥α⊤
i1−/summationtextn
j=1ρjα⊤
j1∥2
G/parenrightig
.(11)
We present the proof in Appendix B.14.1. Similar to Problem (10), the objective in Problem (11) is a smooth
quadratic program in each αiand is jointly convex in αi’s. In Appendix B.14.2, we also present the details
for solving Problem (11) using APGD as well as its statistical consistency in Appendix B.14.3.
9Published in Transactions on Machine Learning Research (01/2024)
ϵKL-UOT plan                  MMD-UOT plan    Barycenter
             (a)           (b)         (c)𝜖
Figure 2: (a) Optimal Transport plans of ϵKL-UOT and MMD-UOT; (b) Barycenter interpolating between
Gaussian measures. For the chosen hyperparameter, the barycenters of ϵKL-UOT and MMD-UOT overlap
and can be looked as smooth approximations of the OT barycenter; (c) Objective vs Time plot comparing
ϵKL-UOT solved using the popular Sinkhorn algorithm (Chizat et al., 2017; Pham et al., 2020) and MMD-
UOT (10) solved using APGD. A plot showing ϵKL-UOT’s progress at the initial phase is given in Figure 4.
5 Experiments
In Section 4, we examined the theoretical properties of the proposed MMD-UOT formulation. In this section,
we show that MMD-UOT is a good practical alternative to the popular entropy-regularized ϵKL-UOT. We
emphasize that our purpose is not to benchmark state-of-the-art performance. Our codes are publicly
available at https://github.com/Piyushi-0/MMD-reg-OT.
5.1 Synthetic Experiments
We present some synthetic experiments to visualize the quality of our solution. Please refer to Appendix C.1
for more details.
Transport Plan and Barycenter We perform synthetic experiments with the source and target as
Gaussian measures. We compare the OT plan of ϵKL-UOT and MMD-UOT in Figure 2(a). We observe
that the MMD-UOT plan is sparser compared to the ϵKL-UOT plan. In Figure 2(b), we visualize the
barycenter interpolating between the source and target, obtained with MMD, ϵKL-UOT and MMD-UOT.
While MMD barycenter is an empirical average of the measures and hence has two modes, the geometry
of measures is considered in both ϵKL-UOT and MMD-UOT formulations. Barycenters obtained by these
methods have the same number of modes (one) as in the source and the target. Moreover, they appear to
smoothly approximate the barycenter obtained with OT (solved using a linear program).
Visualizing the Level Sets Applications like generative modeling deal with optimization over the pa-
rameter (θ) of the source distribution to match the target distribution. In such cases, it is desirable that
the level sets of the distance function over the measures show a lesser number of stationary points that are
not global optima (Bottou et al., 2017). Similar to (Bottou et al., 2017), we consider a model family for
source distributions as F={Pθ=1
2(δθ+δ−θ) :θ∈[−1,1]x[−1,1]}and a fixed target distribution Q
asP(2,2)/∈F. We compute the distances between PθandQaccording to various divergences. Figure 3
presents level sets showing the set of distances {d(Pθ,Q) :θ∈[−1,1]x[−1,1]}where the distance d(.,.)is
measured using MMD, Kantorovich metric, ϵKL-UOT, and MMD-UOT (9), respectively. While all methods
correctly identify global minima (green arrow), level sets with MMD-UOT and ϵKL-UOT show no local
minima (encircled in red for MMD) and have a lesser number of non-optimal stationary points (marked with
black arrows) compared to the Kantorovich metric in Figure 3(b).
Computation Time In Figure 2(c), we present the objective versus time plot. The source and target
measures are chosen to be the same, in which case the optimal objective is 0. MMD-UOT (10) solved using
10Published in Transactions on Machine Learning Research (01/2024)
(a)                                                  (b)                                                  (c)         (d)   
Figure 3: Level sets of distance function between a family of source distributions and a fixed target distri-
bution with the task of finding the source distribution closest to the target distribution using (a) MMD, (b)
¯W2, (c)ϵKL-UOT, and (d) MMD-UOT. While all methods correctly identify global minima (green arrows),
level sets with MMD-UOT and ϵKL-UOT show no local minima (encircled in red for MMD) and have a
lesser number of non-optimal stationary points (marked with black arrows) compared to (b).
Table 2: Average Test Power (between 0 and 1; higher is better) on MNIST. MMD-UOT obtains the highest
average test power at all timesteps.
N MMD ϵKL-UOT MMD-UOT
100 0.137 0.099 0.154
200 0.258 0.197 0.333
300 0.467 0.242 0.588
400 0.656 0.324 0.762
500 0.792 0.357 0.873
10000.909 0.506 0.909
APGD (described in Section 4.2) gives a much faster rate of decrease in objective compared to the Sinkhorn
algorithm used for solving KL-UOT.
5.2 Two-Sample Hypothesis Test
Given two sets of samples {x1,...,xm}∼s0and{y1,...,ym}∼t0, the two-sample test aims to determine
whether the two sets of samples are drawn from the same distributions, viz., to predict if s0=t0. The
performance evaluation in the two-sample test relies on two types of errors. Type-I error occurs when
s0=t0, but the algorithm predicts otherwise. Type-II error occurs when the algorithm incorrectly predicts
s0=t0. The probability of Type-I error is called the significance level. The significance level can be
controlled using permutation test-based setups (Ernst, 2004; Liu et al., 2020). Algorithms are typically
compared based on the empirical estimate of their test power (higher is better), defined as the probability
of not making a Type-II error and the average Type-I error (lower is better).
Dataset and experimental setup. Following (Liu et al., 2020), we consider the two sets of samples, one
fromthetrueMNIST(LeCun&Cortes,2010)andanotherfromfakeMNISTgeneratedbytheDCGAN(Bian
et al., 2019). The data lies in 1024 dimensions. We take an increasing number of samples ( N) and compute
the average test power over 100 pairs of sets for each value of N. We repeat the experiment 10 times and
report the average test power in Table 2 for the significance level α= 0.05. By the design of the test, the
average Type-I error was upper-bounded, and we noted the Type-II error in our experiment. We detail
the procedure for choosing the hyperparameters and the list of chosen hyperparameters for each method in
Appendix C.2.
Results. In Table 2, we observe that MMD-UOT obtains the highest test power for all values of N. The
average test power of MMD-UOT is 1.5−2.4times better than that of ϵKL-UOT across N. MMD-UOT also
outperforms EMD and 2-Wasserstein, which suffer from the curse of dimensionality, for all values of N. Our
results match the sample efficient MMD metric’s result on increasing Nto 1000, but for lesser sample-size,
MMD-UOT is always better than MMD.
11Published in Transactions on Machine Learning Research (01/2024)
Table 3: MMD distance (lower is better) between computed barycenter and the ground truth distribution.
A sigma-heuristics based RBF kernel is used to compute the MMD distance. We observe that MMD-UOT’s
results are closer to the ground truth than the baselines’ results at all timesteps.
Timestep MMD ϵKL-UOT MMD-UOT
t1 0.375 0.391 0.334
t2 0.190 0.184 0.179
t3 0.125 0.138 0.116
Avg. 0.230 0.238 0.210
5.3 Single-Cell RNA Sequencing
We empirically evaluate the quality of our barycenter in the Single-cell RNA sequencing experiment. Single-
cell RNA sequencing technique (scRNA-seq) helps us understand how the expression profile of the cells
changes (Schiebingeretal.,2019). BarycenterestimationintheOTframeworkoffersaprincipledapproachto
estimate the trajectory of a measure at an intermediate timestep t(ti<t<tj) when we have measurements
available only at ti(source) and tj(target) time steps.
Dataset and experimental setup. We perform experiments on the Embryoid Body (EB) single-cell
dataset (Moon et al., 2019). The dataset has samples available at five timesteps ( tjwithj= 0,..., 4), which
were collected during a 25-day period of development of the human embryo. Following (Tong et al., 2020),
we project the data onto two-dimensional space and associate uniform measures to the source and the target
samples given at different timesteps. We consider the samples at timestep tiandti+2as the samples from
the source and target measures where 0≤i≤2and aim at estimating the measure at titimestep as their
barycenter with equal interpolation weights ρ1=ρ2= 0.5.
We compute the barycenters using MMD-UOT (11) and the ϵKL-UOT (Chizat et al., 2018; Liero et al.,
2018) approaches. For both, a simplex constraint is used to cater to the case of uniform measures. We also
compare against the empirical average of the source and target measures, which is the barycenter obtained
with the MMD metric. The computed barycenter is evaluated against the measure corresponding to the
ground truth samples available at the corresponding timestep. We compute the distance between the two
using the MMD metric with RBF kernel (Gretton et al., 2012). The hyperparameters are chosen based on
the leave-one-out validation protocol. More details and some additional results are in Appendix C.3.
Results. Table 3 shows that MMD-UOT achieves the lowest distance from the ground truth for all the
timesteps, illustrating its superior interpolation quality.
5.4 Domain Adaptation in JUMBOT framework
OT has been widely employed in domain adaptation problems (Courty et al., 2017; Courty et al., 2017; Seguy
et al., 2018; Damodaran et al., 2018). JUMBOT (Fatras et al., 2021) is a popular domain adaptation method
based onϵKL-UOT that outperforms OT-based baselines. JUMBOT’s loss function involves a cross-entropy
term andϵKL-UOT discrepancy term between the source and target distributions. We showcase the utility
of MMD-UOT (10) in the JUMBOT (Fatras et al., 2021) framework.
Dataset and experimental setup: We perform the domain adaptation experiment with and Digits
datasets comprising of MNIST (LeCun & Cortes, 2010), M-MNIST (Ganin et al., 2016), SVHN (Netzer
et al., 2011), USPS (Hull, 1994) datasets. We replace the ϵKL-UOT based loss with the MMD-UOT loss
(10), keeping the other experimental set-up the same as JUMBOT. We obtain JUMBOT’s result with
ϵKL-UOT with the best-reported hyperparameters (Fatras et al., 2021). Following JUMBOT, we tune
hyperparameters of MMD-UOT for the Digits experiment on USPS to MNIST (U ∝⇕⊣√∫⊔≀→M) domain adaptation
task and use the same hyperparameters for the rest of the domain adaptation tasks on Digits. More details
are in Appendix C.4.
12Published in Transactions on Machine Learning Research (01/2024)
Table 4: Target domain accuracy (higher is better) obtained in domain adaptation experiments. Results for
ϵKL-UOT are reproduced from the code open-sourced for JUMBOT in (Fatras et al., 2021). MMD-UOT
outperforms ϵKL-UOT in all the domain adaptation tasks considered.
Source Target ϵKL-UOT MMD-UOT
M-MNIST USPS 91.53 94.97
M-MNIST MNIST 99.35 99.50
MNIST M-MNIST 96.51 96.96
MNIST USPS 96.51 97.01
SVHN M-MNIST 94.26 95.35
SVHN MNIST 98.68 98.98
SVHN USPS 92.78 93.22
USPS MNIST 96.76 98.53
Avg. 95.80 96.82
Results: Table 4 reports the accuracy obtained on target datasets. We observe that MMD-UOT-based loss
performs better than ϵKL-UOT-based loss for all the domain adaptation tasks. In Figure 8 (appendix), we
also compare the t-SNE plot of the embeddings learned with the MMD-UOT and the ϵKL-UOT-based loss
functions. The clusters learned with MMD-UOT are better separated (e.g., red- and cyan-colored clusters).
5.5 More Results on Domain Adaptation
In Section 5.4, we compared the proposed MMD-UOT-based loss function with the ϵKL-UOT based loss
function in the JUMBOT framework (Fatras et al., 2021). It should be noted that JUMBOT has a ResNet-
50 backbone. Hence, in this section, we also compare with popular domain adaptation baselines having
ResNet-50 backbone. These include DANN (Ganin et al., 2015), CDANN-E (Long et al., 2017), DEEPJ-
DOT(Damodaranetal.,2018), ALDA(Chenetal.,2020a), ROT(Balajietal.,2020), andBombOT(Nguyen
et al., 2022). BombOT is a recent state-of-the-art OT-based method for unsupervised domain adaptation
(UDA). As in JUMBOT (Fatras et al., 2021), BombOT also employs ϵKL-UOT based loss function. We
also include the results of the baseline ResNet-50 model, where the model is trained on the source and is
evaluated on the target without employing any adaptation techniques.
Office-Home dataset: We evaluate the proposed method on the Office-Home dataset (Venkateswara
et al., 2017), popular for unsupervised domain adaptation. We use the backbone network of ResNet-50
following. The Office-Home dataset has 15,500 images from four domains: Artistic images (A), Clip Art (C),
Product images (P) and Real-World (R). The dataset contains images of 65 object categories common in
office and home scenarios for each domain. Following (Fatras et al., 2021; Nguyen et al., 2022), evaluation is
done in 12 adaptation tasks. Following JUMBOT, we validate the proposed method on the A →C task and
use the chosen hyperparameters for the rest of the tasks.
Table 5 reports the target accuracies obtained by different methods. The results of the BombOT method are
quoted from (Nguyen et al., 2022), and the results of other baselines are quoted from (Fatras et al., 2021).
We observe that the proposed MMD-UOT-based method achieves the best target accuracy in 11out of 12
adaptation tasks.
VisDA-2017 dataset: We next consider the next domain adaptation task between the training and
validation sets of the VisDA-2017 (Recht et al., 2018) dataset. We follow the experimental setup detailed in
(Fatras et al., 2021). The source domain of VisDA has 152,397 synthetic images, while the target domain
has 55,388 real-world images. Both the domains have 12 object categories.
Table 6 compares the performance of different methods. The results of the BombOT method are quoted from
(Nguyen et al., 2022), and the results of other baselines are quoted from (Fatras et al., 2021). The proposed
13Published in Transactions on Machine Learning Research (01/2024)
Table 5: Target accuracies (higher is better) on the Office-Home dataset in the UDA setting. The letters
denote different domains: ‘A’ for Artistic images, ‘P’ for Product images, ‘C’ for Clip art and ‘R’ for Real-
World images. The proposed method achieves the highest accuracy on almost all the domain adaptation
tasks and achieves the best accuracy averaged across the tasks.
Method A→C A→P A→R C→A C→P C→R P→A P→C P→R R→A R→C R→PAvg
ResNet-50 34.9 50.0 58.0 37.4 41.9 46.2 38.5 31.2 60.4 53.9 41.2 59.9 46.1
DANN44.3 59.8 69.8 48.0 58.3 63.0 49.7 42.7 70.6 64.0 51.7 78.3 58.3(Ganin et al., 2015)
CDAN-E52.5 71.4 76.1 59.7 69.9 71.5 58.7 50.3 77.5 70.5 57.9 83.5 66.6(Long et al., 2017)
DEEPJDOT50.7 68.7 74.4 59.9 65.8 68.1 55.2 46.3 73.8 66.0 54.9 78.3 63.5(Damodaran et al., 2018)
ALDA52.2 69.3 76.4 58.7 68.2 71.1 57.4 49.6 76.8 70.6 57.3 82.5 65.8(Chen et al., 2020a)
ROT47.2 71.8 76.4 58.6 68.1 70.2 56.5 45.0 75.8 69.4 52.1 80.6 64.3(Balaji et al., 2020)
ϵKL-UOT (JUMBOT)55.2 75.5 80.8 65.5 74.4 74.9 65.2 52.7 79.2 73.0 59.9 83.4 70.0(Fatras et al., 2021)
BombOT56.2 75.2 80.5 65.8 74.6 75.4 66.2 53.2 80.0 74.2 60.183.3 70.4(Nguyen et al., 2022)
Proposed 56.5 77.2 82.0 70.0 77.1 77.8 69.3 55.1 82.0 75.5 59.3 84.0 72.2
Table 6: Target accuracies (higher is better) on the VisDA-2017 dataset in the UDA setting. The proposed
MMD-UOT method achieves the highest accuracy.
Dataset CDAN-E ALDA DEEPJDOT ROT ϵKL-UOT (JUMBOT) BombOT Proposed
VisDA-2017 70.1 70.5 68.0 66.3 72.5 74.6 77.0
method achieves the best performance, improving the accuracy obtained by ϵKL-UOT based JUMBOT and
BombOT methods by 4.5%and2.4%, respectively.
5.6 Prompt Learning for Few-Shot Classification
The task of learning prompts (e.g. “a tall bird of [class]”) for vision-language models has emerged as a
promising approach to adapt large pre-trained models like CLIP (Radford et al., 2021) for downstream
tasks. The similarity between prompt features (which are class-specific) and visual features of a given image
can help us classify the image. A recent OT-based prompt learning approach, PLOT (Chen et al., 2023),
obtainedstate-of-the-artresultsonthe K-shotrecognitiontaskinwhichonly Kimagesperclassareavailable
during training. We evaluate the performance of MMD-UOT following the setup of (Chen et al., 2023) on
the benchmark EuroSAT (Helber et al., 2018) dataset consisting of satellite images, DTD (Cimpoi et al.,
2014) dataset having images of textures and Oxford-Pets (Parkhi et al., 2012) dataset having images of pets.
Results With the same evaluation protocol as in (Chen et al., 2023), we report the classification accuracy
averaged over three seeds in Table 7. We note that MMD-UOT-based prompt-learning achieves better results
than PLOT, especially when Kis less (more challenging case due to lesser training data). With the EuroSAT
dataset, the improvement is as high as 4% for a challenging case of K=1. More details are in Appendix C.5.
14Published in Transactions on Machine Learning Research (01/2024)
Table 7: Average and standard deviation (over 3 runs) of accuracy (higher is better) on the k-shot classifi-
cation task, shown for different values of shots ( k) in the state-of-the-art PLOT framework. The proposed
method replaces OT with MMD-UOT in PLOT, keeping all other hyperparameters the same. The results
of PLOT are taken from their paper (Chen et al., 2023).
Dataset Method 1 2 4 8 16
EuroSATPLOT 54.05 ±5.95 64.21±1.9072.36±2.2978.15±2.65 82.23±0.91
Proposed 58.47±1.37 66.0±0.93 71.97±2.2179.03±1.91 83.23±0.24
DTDPLOT 46.55 ±2.6251.24±1.9556.03±0.43 61.70±0.35 65.60±0.82
Proposed 47.27±1.46 51.0±1.7156.40±0.73 63.17±0.69 65.90±0.29
6 Conclusion
The literature on unbalanced optimal transport (UOT) has largely focused on ϕ-divergence-based regu-
larization. Our work provides a comprehensive analysis of MMD-regularization in UOT, answering many
open questions. We prove novel results on the metricity and the sample efficiency of MMD-UOT, propose
consistent estimators which can be computed efficiently, and illustrate its empirical effectiveness on several
machine learning applications. Our theoretical and empirical contributions for MMD-UOT and its corre-
sponding barycenter demonstrate the potential of MMD-regularization in UOT as an effective alternative
toϕ-divergence-based regularization. Interesting directions of future work include exploring applications of
IPM-regularized UOT (Remark 4.9) and the generalization of Kantorovich-Rubinstein duality (Remark 4.7).
7 Funding Disclosure and Acknowledgements
We thank Kilian Fatras for the discussions on the JUMBOT baseline, and Bharath Sriperumbudur (PSU)
and G. Ramesh (IITH) for discussions related to Appendix B.9.1. We are grateful to Rudraram Siddhi
Vinayaka. We also thank the anonymous reviewers for constructive feedback. PM and JSN acknowledge the
support of Google PhD Fellowship and Fujitsu Limited (Japan), respectively.
References
Rohit Agrawal and Thibaut Horel. Optimal bounds between f-divergences and integral probability metrics.
InICML, 2020.
Martial Agueh and Guillaume Carlier. Barycenters in the wasserstein space. SIAM Journal on Mathematical
Analysis, 43(2):904–924, 2011.
David Alvarez-Melis and Tommi Jaakkola. Gromov-Wasserstein alignment of word embedding spaces. In
EMNLP, 2018.
Yuki Arase, Han Bao, and Sho Yokoi. Unbalanced optimal transport for unbalanced word alignment. In
ACL, 2023.
Yogesh Balaji, Rama Chellappa, and Soheil Feizi. Robust optimal transport with applications in generative
modeling and domain adaptation. In NeurIPS , 2020.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences , 2(1):183–202, 2009.
A. Ben-Tal and A. Nemirovski. Lectures On Modern Convex Optimization, 2021.
Yuemin Bian, Junmei Wang, Jaden Jungho Jun, and Xiang-Qun Xie. Deep convolutional generative adver-
sarial network (dcgan) models for screening and design of small molecules targeting cannabinoid receptors.
Molecular Pharmaceutics , 16(11):4451–4460, 2019.
15Published in Transactions on Machine Learning Research (01/2024)
Alberto Bietti and Julien Mairal. Group invariance, stability to deformations, and complexity of deep
convolutional representations. Journal of Machine Learning Research , 20:25:1–25:49, 2017.
Alberto Bietti, Grégoire Mialon, Dexiong Chen, and Julien Mairal. A kernel perspective for regularizing
deep neural networks. In ICML, 2019.
Leon Bottou, Martin Arjovsky, David Lopez-Paz, and Maxime Oquab. Geometrical insights for implicit
generative modeling. Braverman Readings in Machine Learning 2017 , pp. 229–268, 2017.
Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang. Prompt learning
with optimal transport for vision-language models. In ICLR, 2023.
Minghao Chen, Shuai Zhao, Haifeng Liu, and Deng Cai. Adversarial-learned loss for domain adaptation. In
AAAI, 2020a.
Yimeng Chen, Yanyan Lan, Ruinbin Xiong, Liang Pang, Zhiming Ma, and Xueqi Cheng. Evaluating natural
language generation via unbalanced optimal transport. In IJCAI, 2020b.
Xiuyuan Cheng and Alexander Cloninger. Classification logit two-sample testing by neural networks for
differentiating near manifold densities. IEEE Transactions on Information Theory , 68:6631–6662, 2019.
L. Chizat, G. Peyre, B. Schmitzer, and F.-X. Vialard. Unbalanced optimal transport: Dynamic and kan-
torovich formulations. Journal of Functional Analysis , 274(11):3090–3123, 2018.
Lénaïc Chizat, Gabriel Peyré, Bernhard Schmitzer, and François-Xavier Vialard. Scaling algorithms for
unbalanced optimal transport problems. Math. Comput. , 87:2563–2609, 2017.
Lenaïc Chizat. Unbalanced optimal transport : Models, numerical methods, applications. Technical report,
Universite Paris sciences et lettres, 2017.
Kacper P. Chwialkowski, Aaditya Ramdas, D. Sejdinovic, and Arthur Gretton. Fast two-sample testing with
analytic representations of probability measures. In NIPS, 2015.
M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In CVPR,
2014.
Samuel Cohen, Michael Arbel, and Marc Peter Deisenroth. Estimating barycenters of measures in high
dimensions. arXiv preprint arXiv:2007.07105 , 2020.
N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation. IEEE
Transactions on Pattern Analysis and Machine Intelligence , 39(9):1853–1865, 2017.
Nicolas Courty, Rémi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal
transportation for domain adaptation. In NIPS, 2017.
I. Csiszar. Information-type measures of difference of probability distributions and indirect observations.
Studia Scientiarum Mathematicarum Hungarica , 2:299–318, 1967.
M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NIPS, 2013.
Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. In ICML, 2014.
Bharath Bhushan Damodaran, Benjamin Kellenberger, Rémi Flamary, Devis Tuia, and Nicolas Courty.
DeepJDOT: Deep Joint Distribution Optimal Transport for Unsupervised Domain Adaptation. In ECCV,
2018.
Henri De Plaen, Pierre-François De Plaen, Johan A. K. Suykens, Marc Proesmans, Tinne Tuytelaars, and
Luc Van Gool. Unbalanced optimal transport: A unified framework for object detection. In CVPR, 2023.
Michael D. Ernst. Permutation Methods: A Basis for Exact Inference. Statistical Science , 19(4):676 – 685,
2004.
16Published in Transactions on Machine Learning Research (01/2024)
Kilian Fatras, Thibault Séjourné, Nicolas Courty, and Rémi Flamary. Unbalanced minibatch optimal trans-
port; applications to domain adaptation. In ICML, 2021.
Rémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurélie Boisbunon, Stanislas Cham-
bon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Léo Gautheron, Nathalie T.H.
Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien
Seguy, DanicaJ.Sutherland, RomainTavenard, AlexanderTong, andTitouanVayer. Pot: Pythonoptimal
transport. Journal of Machine Learning Research , 22(78):1–8, 2021.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso Poggio. Learning with
a wasserstein loss. In NIPS, 2015.
Yaroslav Ganin, E. Ustinova, Hana Ajakan, Pascal Germain, H. Larochelle, François Laviolette, Mario
Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. In Journal of
Machine Learning Research , 2015.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette,
Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of
Machine Learning Research , 17(1):2096–2030, 2016.
Tryphon T. Georgiou, Johan Karlsson, and Mir Shahrouz Takyar. Metrics for power spectra: An axiomatic
approach. IEEE Transactions on Signal Processing , 57(3):859–867, 2009.
Alexandre Gramfort, Gabriel Peyré, and Marco Cuturi. Fast optimal transport averaging of neuroimaging
data. In Proceedings of 24th International Conference on Information Processing in Medical Imaging ,
2015.
Arthur Gretton. A simpler condition for consistency of a kernel independence test. arXiv: Machine Learning ,
2015.
Arthur Gretton, Karsten M. Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alexander J. Smola. A
kernel method for the two-sample-problem. In NIPS, 2006.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander Smola. A
kernel two-sample test. Journal of Machine Learning Research , 13(25):723–773, 2012.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved
training of wasserstein gans. In NIPS, 2017.
K. Gurumoorthy, P. Jawanpuria, and B. Mishra. SPOT: A framework for selection of prototypes using
optimal transport. In European Conference on Machine Learning and Knowledge Discovery in Databases
(ECML PKDD) , 2021.
Leonid G. Hanin. Kantorovich-rubinstein norm and its application in the theory of lipschitz spaces. In
Proceedings of the Americal Mathematical Society , volume 115, 1992.
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Introducing eurosat: A novel
dataset and deep learning benchmark for land use and land cover classification. In IGARSS 2018-2018
IEEE International Geoscience and Remote Sensing Symposium , pp. 204–207. IEEE, 2018.
J.J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 16(5):550–554, 1994.
P. Jawanpuria, M. Meghwanshi, and B. Mishra. Geometry-aware domain adaptation for unsupervised align-
ment of word embeddings. In Annual Meeting of the Association for Computational Linguistics , 2020.
Wittawat Jitkrittum, Zoltán Szabó, Kacper P. Chwialkowski, and Arthur Gretton. Interpretable distribution
features with maximum testing power. In NIPS, 2016.
17Published in Transactions on Machine Learning Research (01/2024)
Philip A. Knight. The sinkhorn–knopp algorithm: Convergence and applications. SIAM Journal on Matrix
Analysis and Applications , 30(1):261–275, 2008.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Khang Le, Huy Nguyen, Quang M Nguyen, Tung Pham, Hung Bui, and Nhat Ho. On robust optimal
transport: Computational complexity and barycenter computation. In NeurIPS , 2021.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. http://yann.lecun.com/exdb/mnist/,
2010.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabás Póczos. MMD GAN: Towards
Deeper Understanding of Moment Matching Network. In NIPS, 2017.
Yazhe Li, Roman Pogodin, Danica J. Sutherland, and Arthur Gretton. Self-supervised learning with kernel
dependence maximization. In NeurIPS , 2021.
Matthias Liero, Alexander Mielke, and Giuseppe Savaré. Optimal transport in competition with reaction:
The hellinger-kantorovich distance and geodesic curves. SIAM J. Math. Anal. , 48:2869–2911, 2016.
Matthias Liero, Alexander Mielke, and Giuseppe Savaré. Optimal entropy-transport problems and a new
hellinger–kantorovich distance between positive measures. Inventiones mathematicae , 211(3):969–1117,
2018.
Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, Arthur Gretton, and Danica J. Sutherland. Learning deep
kernels for non-parametric two-sample tests. In ICML, 2020.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Conditional adversarial domain
adaptation. In NIPS, 2017.
David Lopez-Paz and Maxime Oquab. evisiting classifier two-sample tests. In ICLR, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for gener-
ative adversarial networks. In ICLR, 2018.
Kevin R. Moon, David van Dijk, Zheng Wang, Scott Gigante, Daniel B. Burkhardt, William S. Chen,
Kristina Yim, Antonia van den Elzen, Matthew J. Hirn, Ronald R. Coifman, Natalia B. Ivanova, Guy
Wolf, and Smita Krishnaswamy. Visualizing structure and transitions for biological data exploration.
Nature Biotechnology , 37(12):1482–1492, 2019.
Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, and Bernhard Schölkopf. Kernel mean em-
bedding of distributions: A review and beyond. Foundations and Trends ®in Machine Learning , 10(1–2):
1–141, 2017.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in Applied
Probability , 29:429–443, 1997.
J. Saketha Nath and Pratik Kumar Jawanpuria. Statistical optimal transport posed as learning kernel
embedding. In NeurIPS , 2020.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course , volume 87. Springer Science
& Business Media, 2003.
Yuval Netzer, Tiejie Wang, Adam Coates, A. Bissacco, Bo Wu, and A. Ng. Reading digits in natural images
with unsupervised feature learning. In NeurIPS , 2011.
Khai Nguyen, Dang Nguyen, Quoc Nguyen, Tung Pham, Hung Bui, Dinh Phung, Trung Le, and Nhat Ho.
On transportation of mini-batches: A hierarchical approach. In ICML, 2022.
Thanh Tang Nguyen, Sunil Gupta, and Svetha Venkatesh. Distributional reinforcement learning via moment
matching. In AAAI, 2021.
18Published in Transactions on Machine Learning Research (01/2024)
Jonathan Niles-Weed and Philippe Rigollet. Estimation of Wasserstein distances in the spiked transport
model. In Bernoulli , 2019.
O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012.
Gabriel Peyré and Marco Cuturi. Computational optimal transport. Foundations and Trends ®in Machine
Learning , 11(5-6):355–607, 2019.
Khiem Pham, Khang Le, Nhat Ho, Tung Pham, and Hung Bui. On unbalanced optimal transport: An
analysis of sinkhorn algorithm. In ICML, 2020.
Benedetto Piccoli and Francesco Rossi. Generalized wasserstein distance and its application to transport
equations with source. Archive for Rational Mechanics and Analysis , 211:335–358, 2014.
Benedetto Piccoli and Francesco Rossi. On properties of the generalized wasserstein distance. Archive for
Rational Mechanics and Analysis , 222, 12 2016.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervision. In ICML, 2021.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do CIFAR-10 classifiers gener-
alize to CIFAR-10? arXiv, 2018.
Geoffrey Schiebinger, Jian Shu, Marcin Tabaka, Brian Cleary, Vidya Subramanian, Aryeh Solomon, Joshua
Gould, Siyan Liu, Stacie Lin, Peter Berube, Lia Lee, Jenny Chen, Justin Brumbaugh, Philippe Rigollet,
Konrad Hochedlinger, Rudolf Jaenisch, Aviv Regev, and Eric S. Lander. Optimal-transport analysis
of single-cell gene expression identifies developmental trajectories in reprogramming. Cell, 176(4):928–
943.e22, 2019.
Vivien. Seguy, Bharath B. Damodaran, Remi Flamary, Nicolas Courty, Antoine Rolet, and Mathieu Blondel.
Large-scale optimal transport and mapping estimation. In ICLR, 2018.
Carl-Johann Simon-Gabriel, Alessandro Barp, Bernhard Schölkopf, and Lester Mackey. Metrizing weak
convergence with maximum mean discrepancies. arXiv, 2020.
Maurice Sion. On general minimax theorems. Pacific Journal of Mathematics , 8(1):171 – 176, 1958.
Alexander J. Smola, Arthur Gretton, Le Song, and Bernhard Schölkopf. A hilbert space embedding for
distributions. In ALT, 2007.
Justin Solomon, Raif Rustamov, Leonidas Guibas, and Adrian Butscher. Wasserstein propagation for semi-
supervised learning. In ICML, 2014.
Justin Solomon, Fernando de Goes, Gabriel Peyré, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du,
and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal transportation on geometric
domains. ACM Trans. Graph. , 34(4), 2015.
L. Song. Learning via hilbert space embedding of distributions. In PhD Thesis , 2008.
Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions
classes from videos in the wild. CoRR, 2012.
BharathK.Sriperumbudur, KenjiFukumizu, ArthurGretton, BernhardSchölkopf, andGertR.G.Lanckriet.
On integral probability metrics, phi-divergences and binary classification. arXiv, 2009.
Bharath K. Sriperumbudur, Kenji Fukumizu, and Gert R. G. Lanckriet. Universality, characteristic kernels
and RKHS embedding of measures. Journal of Machine Learning Research , 12:2389–2410, 2011.
Ilya O. Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schölkopf. Wasserstein auto-encoders. In
ICLR, 2018.
19Published in Transactions on Machine Learning Research (01/2024)
Alexander Tong, Jessie Huang, Guy Wolf, David Van Dijk, and Smita Krishnaswamy. TrajectoryNet: A
dynamic optimal transport network for modeling cellular dynamics. In ICML, 2020.
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing
network for unsupervised domain adaptation. In CVPR, 2017.
Cédric Villani. Optimal Transport: Old and New . A series of Comprehensive Studies in Mathematics.
Springer, 2009.
A Preliminaries
A.1 Integral Probability Metric (IPM):
Given a setG⊂L (X), the integral probability metric (IPM) (Muller, 1997; Sriperumbudur et al., 2009;
Agrawal & Horel, 2020) associated with G, is defined by:
γG(s0,t0)≡max
f∈G/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Xfds0−/integraldisplay
Xfdt0/vextendsingle/vextendsingle/vextendsingle/vextendsingle∀s0,t0∈R+(X). (12)
Gis called the generating set of the IPM, γG.
In order that the IPM metrizes weak convergence, we assume the following (Muller, 1997):
Assumption A.1. G⊆C (X)and is compact.
Since the IPM generated by Gand its absolute convex hull is the same (without loss of generality), we
additionally assume the following:
Assumption A.2. Gis absolutely convex.
Remark A.3. We note that the assumptions A.1 and A.2 are needed only to generalize our theoretical results
to an IPM-regularized UOT formulation (Formulation 13). These assumptions are satisfied whenever the
IPM employed for regularization is the MMD (Formulation 6) with a kernel that is continuous and universal
(i.e., c-universal).
A.2 Classical Examples of IPMs
•Maximum Mean Discrepancy (MMD): Letkbe a characteristic kernel (Sriperumbudur et al.,
2011) over the domain X, let∥f∥kdenote the norm of fin the canonical reproducing kernel Hilbert
space (RKHS),Hk, corresponding to k. MMDkis the IPM associated with the generating set:
Gk≡{f∈Hk|∥f∥k≤1}.
MMDk(s0,t0)≡max
f∈Gk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Xfds0−/integraldisplay
Xfdt0/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
•Kantorovich metric ( Kc):Kantorovich metric also belongs to the family of integral probability
metrics associated with the generating set Wc≡/braceleftbigg
f:X∝⇕⊣√∫⊔≀→R|max
x∈X̸=y∈X|f(x)−f(y)|
c(x,y)≤1/bracerightbigg
, wherec
is a metric overX. The Kantorovich-Fenchel duality result shows that the 1-Wasserstein metric is
the same as the Kantorovich metric when restricted to probability measures.
•Dudley: This is the IPM associated with the generating set:
Dd≡ {f:X∝⇕⊣√∫⊔≀→R|∥f∥∞+∥f∥d≤1},wheredis a ground metric over X ×X. The
so-called Flatmetric is related to the Dudley metric. It’s generating set is: Fd≡
{f:X∝⇕⊣√∫⊔≀→R|∥f∥∞≤1,∥f∥d≤1}.
•Kolmogorov: LetX=Rn. Then, theKolmogorovmetricistheIPMassociatedwiththegenerating
set: ¯K≡/braceleftbig
1(−∞,x)|x∈Rn/bracerightbig
.
20Published in Transactions on Machine Learning Research (01/2024)
•Total Variation (TV): This is the IPM associated with the generating set: T ≡
{f:X∝⇕⊣√∫⊔≀→R|∥f∥∞≤1},where∥f∥∞≡max
x∈X|f(x)|. Total Variation metric over measures s0,t0∈
R+(X)is defined as:
TV(s,t)≡/integraltext
Yd|s−t|(y), where|s−t|(y)≡/braceleftbigg
s(y)−t(y)ifs(y)≥t(y)
t(y)−s(y)otherwise
B Proofs and Additional Theory Results
AsmentionedinthemainpaperandRemark4.9, mostofourproofsholdevenwithageneralIPM-regularized
UOT formulation (13) under mild assumptions. We restate such results and give a general proof
that holds for IPM-regularized UOT (Formulation 13), of which MMD-regularized UOT (For-
mulation 6) is a special case.
The proposed IPM-regularized UOT formulation is presented as follows.
UG,c,λ1,λ2(s0,t0)≡ min
π∈R+(X×X )/integraldisplay
cdπ+λ1γG(π1,s0) +λ2γG(π2,t0), (13)
whereγGis defined in equation (12).
We now present the theoretical results and proofs with IPM-regularized UOT (Formulation 13), of which
MMD-regularized UOT (Formulation 6) is a special case. To the best of our knowledge, such an analysis for
IPM-regularized UOT has not been done before.
B.1 Proof of Theorem 4.1
Theorem 4.1. (Duality) WheneverGsatisfies Assumptions A.1 and A.2, c,k∈C(X×X )andXis
compact, we have that:
UG,c,λ1,λ2(s0,t0) = max
f∈G(λ1),g∈G(λ2)/integraltext
Xfds0+/integraltext
Xgdt0,
s.t.f(x) +g(y)≤c(x,y)∀x,y∈X. (14)
Proof.We begin by re-writing the RHS of (13) using the definition of IPMs given in (12):
UG,c,λ1,λ2(s0,t0)≡ min
π∈R+(X×X )/integraldisplay
X×Xcdπ+λ1/parenleftbigg
max
f∈G/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Xfds0−/integraldisplay
Xfdπ1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
+λ2/parenleftbigg
max
g∈G/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Xgdt0−/integraldisplay
Xgdπ2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
∵(A.2)= min
π∈R+(X×X )/integraldisplay
X×Xcdπ+λ1/parenleftbigg
max
f∈G/integraldisplay
Xfds0−/integraldisplay
Xfdπ1/parenrightbigg
+λ2/parenleftbigg
max
g∈G/integraldisplay
Xgdt0−/integraldisplay
Xgdπ2/parenrightbigg
= min
π∈R+(X×X )/integraldisplay
X×Xcdπ+/parenleftbigg
max
f∈G(λ1)/integraldisplay
Xfds0−/integraldisplay
Xfdπ1/parenrightbigg
+/parenleftbigg
max
g∈G(λ2)/integraldisplay
Xgdt0−/integraldisplay
Xgdπ2/parenrightbigg
= max
f∈G(λ1),g∈G(λ2)/integraldisplay
Xfds0+/integraldisplay
Xgdt0+ min
π∈R+(X×X )/integraldisplay
X×Xcdπ−/integraldisplay
Xfdπ1−/integraldisplay
Xgdπ2
= max
f∈G(λ1),g∈G(λ2)/integraldisplay
Xfds0+/integraldisplay
Xgdt0+ min
π∈R+(X×X )/integraldisplay
X×Xc−¯f−¯gdπ
= max
f∈G(λ1),g∈G(λ2)/integraldisplay
Xfds0+/integraldisplay
Xgdt0+/braceleftbigg0iff(x) +g(y)≤c(x,y)∀x,y∈X,
−∞ otherwise.
= max
f∈G(λ1),g∈G(λ2)/integraldisplay
Xfds0+/integraldisplay
Xgdt0,
s.t.f(x) +g(y)≤c(x,y)∀x,y∈X.
(15)
Here, ¯f(x,y)≡f(x),¯g(x,y)≡g(y). The min-max interchange in the third equation is due to Sion’s
minimax theorem: (i) since R(X)is a topological dual of C(X)wheneverXis compact, the objective is
21Published in Transactions on Machine Learning Research (01/2024)
bilinear (inner-product in this duality), whenever c,f,gare continuous. This is true from Assumption A.1
andc∈C(X×X ). (ii)oneofthefeasibilitysetsinvolves G, whichisconvexcompactbyAssumptionsA.1,A.2.
The other feasibility set is convex (the closed conic set of non-negative measures).
RemarkB.1. Whenever the kernel, k, employed is continuous, the generating set of the corresponding MMD
satisfies assumptions A.2 and Gk⊆C(X). Hence, the above proof also works in our case of MMD-regularized
UOT (i.e., to prove Theorem 4.1 in the main paper).
B.2 Proof of Corollary 4.2
We first derive an equivalent re-formulation of 13, which will be used in our proof.
Lemma B1.
UG,c,λ1,λ2(s0,t0)≡ min
s,t∈R+(X)|s|W1(s,t) +λ1γG(s,s0) +λ2γG(t,t0), (16)
whereW1(s,t)≡/braceleftbigg¯W1(s
|s|,t
|t|)if|s|=|t|,
∞ otherwise., with ¯W1as the 1-Wasserstein metric.
Proof.
min
s,t∈R+(X)|s|W1(s,t) +λ1γG(s,s0) +λ2γG(t,t0)
= min
s,t∈R+(X);|s|=|t||s|min
¯π∈R+
1(X×X )/integraldisplay
cd¯π+λ1γG(s,s0) +λ2γG(t,t0)s.t.¯π1=s
|s|,¯π2=t
|t|
= min
η>0η min
¯π∈R+
1(X×X )/integraldisplay
cd¯π+λ1γG(η¯π1,s0) +λ2γG(η¯π2,t0)
= min
η>0min
¯π∈R+
1(X×X )/integraldisplay
c ηd¯π+λ1γG(η¯π1,s0) +λ2γG(η¯π2,t0)
= min
π∈R+(X×X )/integraldisplay
cdπ+λ1γG(π1,s0) +λ2γG(π2,t0)
The first equality holds from the definition of W1:W1(s,t)≡/braceleftbigg¯W1(s
|s|,t
|t|)if|s|=|t|,
∞ otherwise.. Eliminating
normalized versions sandtusing the equality constraints and introducing ηto denote their common mass
gives the second equality. The last equality comes after changing the variable of optimization to π∈
R+(X×X )≡η¯π. Recall thatR+(X)denotes the set of all non-negative Radon measures defined over X;
while the set of all probability measures is denoted by R+
1(X).
Corollary 4.2 in the main paper is restated below with the IPM-regularized UOT formulation (13), followed
by its proof.
Corollary 4.2. (Metricity) In addition to assumptions in Theorem (4.1), whenever cis a metric,UG,c,λ,λ
belongs to the family of integral probability metrics (IPMs). Also, the generating set of this IPM is the
intersection of the generating set of the Kantorovich metric and the generating set of the IPM used for
regularization. Finally, UG,c,λ,λis a valid norm-induced metric over measures whenever the IPM used for
regularization is norm-induced (e.g. MMD with a characteristic kernel). Thus, Uliftsthe ground metric c
to that over measures.
Proof.The constraints in dual, (7), are equivalent to: g(y)≤min
x∈Xc(x,y)−f(x)∀y∈X. The RHS is
nothing but the c-conjugate ( c-transform) of f. From Proposition 6.1 in (Peyré & Cuturi, 2019), whenever
cis a metric we have: min
x∈Xc(x,y)−f(x) =/braceleftbigg
−f(y)iff∈Wc,
−∞otherwise.Here,Wcis the generating set of the
Kantorovich metric lifting c. Thus the constraints are equivalent to: g(y)≤−f(y)∀y∈X,f∈Wc.
22Published in Transactions on Machine Learning Research (01/2024)
Now, since the dual, (7), seeks to maximize the objective with respect to g, and monotonically increases
with values of g; at optimality, we have that g(y) =−f(y)∀y∈X. Note that this equality is possible to
achieve as both g,−f∈G(λ)∩Wc(these sets are absolutely convex). Eliminating g, one obtains:
UG,c,λ,λ (s0,t0) = max
f∈G(λ)∩Wc/integraltext
Xfds0−/integraltext
Xfdt0,
Comparing this and the definition of IPMs 12, we have that UG,c,λ,λbelongs to the family of IPMs. Since
any IPM is a pseudo-metric (induced by a semi-norm) over measures (Muller, 1997), the only condition left
to be proved is positive definiteness with UG,c,λ,λ(s0,t0). Following Lemma B1, we have that for optimal
s∗,t∗in (16),UG,c,λ,λ(s0,t0) = 0⇐⇒ (i)W1(s∗,t∗) = 0,(ii)γG(s∗,s0) = 0,(iii)γG(t∗,t0) = 0as each term
in the RHS is non-negative. When the IPM used for regularization is a norm-induced metric (e.g. the MMD
metric or the Dudley metric), the conditions (i),(ii),(iii)⇐⇒s∗=t∗=s0=t0, which proves the positive
definiteness. Hence, we proved that UG,c,λ,λis a norm-induced metric over measures whenever the IPM used
for regularization is a metric.
Remark B.2. Recall that MMD is a valid norm-induced IPM metric whenever the kernel employed is char-
acteristic. Hence, our proof above also shows the metricity of the MMD-regularized UOT (as per corollary 4.2
in the main paper).
Remark B.3. IfGis the unit uniform-norm ball (corresponding to TV), our result specializes to that
in (Piccoli & Rossi, 2016), which proves that UG,c,λ,λcoincides with the so-called Flat metric (or the bounded
Lipschitz distance).
Remark B.4. If the regularizer is the Kantorovich metric3, i.e.,G=Wc, andλ1=λ2=λ≥1, then
UWc,c,λ,λcoincides with the Kantorovich metric. In other words, the Kantorovich-regularized OT is the same
as the Kantorovich metric. Hence providing an OT interpretation for the Kantorovich metric that is valid
for potentially un-normalized measures in R+(X).
B.3 Proof of Corollary 4.3
Proof.As discussed in Theorem 4.1 and Corollary 4.2, the MMD-regularized UOT (Formulation 6) is an
IPM with the generating set as an intersection of the generating sets of the MMD and the Kantorovich-
Wasserstein metrics. We now present special cases when MMD-regularized UOT (Formulation 6) recovers
back the Kantorovich-Wasserstein metric and the MMD metric.
Recovering Kantorovich. Recall thatGk(λ) ={λg|g∈Gk}. From the definition of Gk(λ),f∈Gk(λ) =⇒
f∈Hk,∥f∥k≤λ. Hence, as λ→∞,Gk(λ) =Hk. Using this in the duality result of Theorem 4.1, we have
the following.
lim
λ→∞Uk,c,λ,λ (s0,t0) = lim
λ→∞max
f∈Gk(λ)∩Wc/integraldisplay
fds0−/integraldisplay
fdt0= max
f∈Hk∩Wc/integraldisplay
fds0−/integraldisplay
fdt0
(1)= max
f∈C(X)∩Wc/integraldisplay
fds0−/integraldisplay
fdt0
(2)= max
f∈Wc/integraldisplay
fds0−/integraldisplay
fdt0
Equality (1)holds becauseHkis dense in the set of continuous functions, C(X). For equality (2), we use
thatWcconsists of only 1-Lipschitz continuous functions. Thus, ∀s0,t0∈R+(X),limλ→∞Uk,c,λ,λ (s0,t0) =
Kc(s0,t0).
Recovering MMD. We next show that when 0<λ1=λ2=λ≤1and the cost metric cis such that
c(x,y)≥/radicalbig
k(x,x) +k(y,y)−2k(x,y) =∥ϕ(x)−ϕ(y)∥k∀x,y(Dominating cost assumption discussed
in B.4), then∀s0,t0∈R+(X),Uk,c,λ,λ (s0,t0) =MMDk(s0,t0).
3The ground metric in UG,c,λ,λmust be the same as that defining the Kantorovich regularizer.
23Published in Transactions on Machine Learning Research (01/2024)
Letf∈Gk(λ) =⇒f=λgwhereg∈Hk,∥g∥≤1. This also implies that λg∈Hkasλ∈(0,1].
|f(x)−f(y)|=|⟨λg,ϕ (x)−ϕ(y)⟩|(RKHS property)
≤|⟨g,ϕ(x)−ϕ(y)⟩|(∵0<λ≤1)
≤∥g∥k∥ϕ(x)−ϕ(y)∥k(Cauchy Schwarz)
≤∥ϕ(x)−ϕ(y)∥k(∵∥g∥≤1)
≤c(x,y)(Dominating cost assumption, discussed in B.4)
=⇒f∈Wc
Therefore,Gk(λ)⊆WCand hence,Gk(λ)∩WC=Gk(λ). This relation, together with the metricity result
shown in Corollary 4.2, implies that Uk,c,λ,λ (s0,t0) =λMMDk(s0,t0). In B.4, we show that the Euclidean
distance satisfies the dominating cost assumption when the kernel employed is the Gaussian kernel and the
inputs lie on a unit-norm ball.
B.4 Dominating Cost Assumption with Euclidean cost and Gaussian Kernel
We present a sufficient condition for the Dominating cost assumption (used in Corollary 4.3) to be satisfied
while using a Euclidean cost and a Gaussian kernel based MMD. We consider the characteristic RBF kernel,
k(x,y) = exp (−s∥x−y∥2), and show that for the hyper-parameter, 0< s≤0.5, the Euclidean cost is
greater than the Kernel cost when the inputs are normalized, i.e., ∥x∥=∥y∥= 1.
∥x−y∥2≥k(x,x) +k(y,y)−2k(x,y)
⇐⇒∥x∥2+∥y∥2−2⟨x,y⟩≥2−2k(x,y)
⇐⇒⟨x,y⟩≤exp (−2s(1−⟨x,y⟩))(Assuming normalized inputs)(17)
From Cauchy Schwarz inequality, −∥x∥∥y∥≤⟨x,y⟩≤∥x∥∥y∥. With the assumption of normalized inputs,
we have that−1≤⟨x,y⟩≤1. We consider two cases based on this.
Case 1:⟨x,y⟩∈[−1,0]In this case, condition (17) is satisfied ∀s≥0becausek(x,y)≥0∀x,ywith a
Gaussian kernel.
Case 2:⟨x,y⟩∈(0,1]In this case, our problem in condition (17) is to find s≥0such that ln⟨x,y⟩≤
−2s(1−⟨x,y⟩). We further consider two sub-cases and derive the required condition as follows:
Case 2A:⟨x,y⟩∈(0,1
e/bracketrightbig
We re-parameterize ⟨x,y⟩=e−nforn≥1. With this, we need to find s≥0such
that−n≤−2s(1−e−n)⇐⇒n≥2s(1−e−n). This is satisfied when 0<s≤0.5becausee−n≥1−n.
Case 2B:⟨x,y⟩∈(1
e,∞)We re-parameterize ⟨x,y⟩=e−1
nforn >1. With this, we need to find s≥0
such that1
n/parenleftig
1−e−1
n/parenrightig≥2s. We consider the function f(n) =n/parenleftig
1−e−1
n/parenrightig
forn≥1. We now show that fis
an increasing function by showing that the gradientdf
dn= 1−/parenleftbig
1 +1
n/parenrightbig
e−1
nis always non-negative.
df
dn≥0
⇐⇒e1
n≥/parenleftbigg
1 +1
n/parenrightbigg
⇐⇒1
n−ln/parenleftbigg
1 +1
n/parenrightbigg
≥0
⇐⇒1
n−(ln (n+ 1)−ln (n))≥0
24Published in Transactions on Machine Learning Research (01/2024)
Applying the Mean Value Theorem on g(n) = lnn, we get
ln (n+ 1)−lnn= (n+ 1−n)1
z,wheren≤z≤n+ 1
=⇒ln/parenleftbigg
1 +1
n/parenrightbigg
=1
z≤1
n
=⇒df
dn=1
n−ln/parenleftbigg
1 +1
n/parenrightbigg
≥0
The above shows that fis an increasing function of n. We note that limn→∞f(n) = 1, hence,1
f(n)=
1
n/parenleftig
1−e−1
n/parenrightig≥1which implies that condition (17) is satisfied by taking 0<s≤0.5.
B.5 Proof of Corollary 4.4
Corollary 4.4 in the main paper is restated below with the IPM-regularized UOT formulation (13), followed
by its proof.
Corollary 4.4.UG,c,λ,λ(s,t)≤min (λγG(s,t),Kc(s,t)).
Proof.Theorem 4.1 shows that UG,c,λ,λis an IPM whose generating set is the intersection of the generating
sets of Kantorovich and the scaled version of the IPM used for regularization. Thus, from the definition of
max, we have that UG,c,λ,λ(s,t)≤λγG(s,t)andUG,c,λ,λ(s,t)≤Kc(s,t). This implies that UG,c,λ,λ(s,t)≤
min (λγG(s,t),Kc(s,t)). As a special case, Uk,c,λ,λ (s,t)≤min (λMMDk(s,t),Kc(s,t)).
B.6 Proof of Corollary 4.5
Corollary 4.5 in the main paper is restated below with the IPM-regularized UOT formulation (13), followed
by its proof.
Corollary 4.5. (Weak Metrization) UG,c,λ,λmetrizes the weak convergence of normalized measures.
Proof.For convenience of notation, we denote UG,c,λ,λbyU. From Corollary 4.4 in the main paper,
0≤U(βn,β)≤Kc(βn,β)
From Sandwich theorem, limβn⇀βU(βn,β)→0aslimβn⇀βKc(βn,β))→0by Theorem 6.9 in (Villani,
2009).
B.7 Proof of Corollary 4.6
Corollary 4.6 in the main paper is restated below with the IPM-regularized UOT formulation (13), followed
by its proof.
Corollary 4.6. (Sample Complexity) Let us denoteUG,c,λ,λ, defined in 13, by ¯U. Let ˆsm,ˆtmdenote the
empirical estimates of s0,t0∈R+(X)respectively with msamples. Then, ¯U(ˆsm,ˆtm)→¯U(s0,t0)at a rate
(apart from constants) same as that of γG(ˆsm,s0)→0.
Proof.We use metricity of ¯Uproved in Corrolary 4.2. From triangle inequality of the metric ¯Uand Corol-
lary 4.4 in the main paper, we have that
0≤|¯U(ˆsm,ˆtm)−¯U(s0,t0)|≤¯U(ˆsm,s0) +¯U(t0,ˆtm)≤λ/parenleftbig
γG(ˆsm,s0) +γG(ˆtm,t0)/parenrightbig
.
Hence, by Sandwich theorem, ¯U(ˆsm,ˆtm)→¯U(s0,t0)at a rate at which γG(ˆsm,s0)→0andγG(ˆtm,t0)→0. If
the IPM used for regularization is MMD with a normalized kernel, then MMD k(s0,ˆsm)≤/radicalig
1
m+/radicalig
2 log(1/δ)
m
with probability at least 1−δ(Smola et al., 2007).
Fromtheunionbound, withprobabilityatleast 1−δ,|¯U(sm,tm)−¯U(s0,t0)|≤2λ/parenleftbigg/radicalig
1
m+/radicalig
2 log(2/δ)
m/parenrightbigg
.
25Published in Transactions on Machine Learning Research (01/2024)
B.8 Proof of Theorem 4.8
We first restate the standard Moreau-Rockafellar theorem, which we refer to in this discussion.
Theorem B2. LetXbe a real Banach space and f,g:X∝⇕⊣√∫⊔≀→R∪{∞}be closed convex functions such that
dom(f)∩dom(g)is not empty, then: (f+g)∗(y) = min
x1+x2=yf∗(x1)+g∗(x2)∀y∈X∗. Here,f∗is the Fenchel
conjugate of f, andX∗is the topological dual space of X.
Theorem 4.8 in the main paper is restated below with the IPM-regularized UOT formulation 13, followed
by its proof.
Theorem 4.8. In addition to the assumptions in Theorem 4.1, if cis a valid metric, then
UG,c,λ1,λ2(s0,t0) = min
s,t∈R(X)Kc(s,t) +λ1γG(s,s0) +λ2γG(t,t0). (18)
Proof.Firstly, the result in the theorem is not straightforward and is not a consequence of Kantorovich-
Rubinstein duality. This is because the regularization terms in our original formulation (13, 16) enforce
closeness to the marginals of a transport plan and hence necessarily must be of the same mass and must
belong toR+(X). Whereas in the RHS of 18, the regularization terms enforce closeness to marginals that
belong toR(X)and more importantly, they could be of different masses.
We begin the proof by considering indicator functions FcandFGdefined overC(X)×C(X)as:
Fc(f,g) =/braceleftig0iff(x) +g(y)≤c(x,y)∀x,y∈X,
∞ otherwise.,FG,λ1,λ2(f,g) =/braceleftig0iff∈G(λ1),g∈G(λ2),
∞ otherwise.
Recall that the topological dual of C(X)is the set of regular Radon measures R(X)and the duality product
⟨f,s⟩≡/integraltext
fds∀f∈C(X),s∈R(X). Now, from the definition of Fenchel conjugate in the (direct sum)
spaceC(X)⊕C(X), we have: F∗
c(s,t) = max
f∈C(X),g∈C(X)/integraltext
fds+/integraltext
gdt,s.t.f(x) +g(y)≤c(x,y)∀x,y∈X,
wheres,t∈R(X). Under the assumptions that Xis compact and cis a continuous metric, Proposition 6.1
in (Peyré & Cuturi, 2019) shows that F∗
c(s,t) = max
f∈Wc/integraltext
fds−/integraltext
fdt=Kc(s,t).
On the other hand, FG,λ1,λ2(f,g) =/parenleftbigg
max
f∈G(λ1)/integraltext
fds+ max
g∈G(λ2)/integraltext
gdt/parenrightbigg
=λ1γG(s,0) +λ2γG(t,0). Now,
we have that the RHS of 18 is min
s,t,s 1,t1∈R(X):(s,t)+(s1,t1)=(s0,t0)F∗
c(s,t) +F∗
G,λ1,λ2(s1,t1). This is be-
causeγG(s0−s,0) =γG(s0,s). Now, observe that the indicator functions FG,λ1,λ2,Fcare closed, con-
vex functions because their domains are closed, convex sets. Indeed, Gis a closed, convex set by As-
sumptions A.1, A.2. Also, it is simple to verify that the set {(f,g)|f(x) +g(y)≤c(x,y)∀x,y∈X}
is closed and convex. Hence by applying the Moreau-Rockafellar formula (Theorem B2), we have that
the RHS of 18 is equal to (Fc+FG,λ1,λ2)∗(s0,t0). But from the definition of conjugate, we have that
(Fc+FG,λ1,λ2)∗(s0,t0)≡ max
f∈C(X),g∈C(X)/integraltext
Xfds0+/integraltext
Xgdt0−Fc(f,g)−FG,λ1,λ2(f,g).Finally, from the
definition of the indicator functions Fc,FG,λ1,λ2, this is same as the final RHS in 15. Hence Proved.
Remark B.5. Whenever the kernel, k, employed is continuous, the generating set of the corresponding
MMD satisfies assumptions A.1, A.2 and Gk⊆C(X). Hence, the above proof also works in our case of
MMD-UOT.
B.9 Proof of Theorem 4.10: Consistency of the Proposed Estimator
Proof.From triangle inequality,
|ˆUm(ˆsm,ˆtm)−¯U(s0,t0)|≤| ˆUm(ˆsm,ˆtm)−ˆUm(s0,t0)|+|ˆUm(s0,t0)−¯U(s0,t0)|, (19)
where ˆUm(s0,t0)is same as ¯U(s0,t0)except that it employs the restricted feasibility set, F(ˆsm,ˆtm), for the
transport plan: set of all joints supported using the samples in ˆsm,ˆtmalone i.e.,
F(ˆsm,ˆtm)≡/braceleftig/summationtextm
i=1/summationtextm
j=1αijδ(x1i,x2j)|αij≥0∀i,j= 1,...,m/bracerightig
. Here,δzis the Dirac measure at z. We
begin by bounding the first term in RHS of (19).
26Published in Transactions on Machine Learning Research (01/2024)
We denote the (common) objective in ˆUm(·,·),¯U(·,·)as a function of the transport plan, π, byh(π,·,·).
Then,
ˆUm(ˆsm,ˆtm)−ˆUm(s0,t0) = min
π∈F(ˆsm,ˆtm)h(π,ˆsm,ˆtm)− min
π∈F(ˆsm,ˆtm)h(π,s0,t0)
≤h(π0∗,ˆsm,ˆtm)−h(π0∗,s0,t0)/parenleftigg
whereπ0∗= arg min
π∈F(ˆsm,ˆtm)h(π,s0,t0)/parenrightigg
=λ1/parenleftbig
MMDk(π0∗
1,ˆsm)−MMDk(π0∗
1,s0)/parenrightbig
+λ2/parenleftbig
MMDk(π0∗
2,ˆtm)−MMDk(π0∗
2,t0)/parenrightbig
≤λ1MMDk(s0,ˆsm) +λ2MMDk(t0,ˆtm) (∵MMDksatisfies triangle inequality )
Similarly, one can show that ˆUm(s0,t0)−ˆUm(ˆsm,ˆtm)≤λ1MMDk(s0,ˆsm) +λ2MMDk(t0,ˆtm). Now, (Muan-
det et al., 2017, Theorem 3.4) shows that, with probability at least 1−δ, MMDk(s0,ˆsm)≤1√m+/radicalig
2 log(1/δ)
m,
wherekis a normalized kernel. Hence, the first term in inequality (19) is upper-bounded by (λ1+
λ2)/parenleftbigg
1√m+/radicalig
2 log 2/δ
m/parenrightbigg
, with probability at least 1−δ.
We next look at the second term in inequality (19): |ˆUm(s0,t0)−¯U(s0,t0)|. Let ¯πmbe the optimal transport
plan in definition of ˆUm(s0,t0). Letπ∗be the optimal transport plan in the definition of ¯U(s0,t0). Consider
another transport plan: ˆπm∈F(ˆsm,ˆtm)such that ˆπm(xi,yj) =η(xi,yj)
m2whereη(xi,yj) =π∗(xi,yj)
s0(xi)t0(yj)for
i,j∈[1,m].
|ˆUm(s0,t0)−¯U(s0,t0)|=ˆUm(s0,t0)−¯U(s0,t0)
=h(¯πm,s0,t0)−h(π∗,s0,t0)
≤h(ˆπm,s0,t0)−h(π∗,s0,t0) (∵¯πmis optimal, )
≤/integraldisplay
cdˆπm−/integraldisplay
cdπ∗+λ1∥µk(ˆπm
1)−µk(π∗
1)∥k+λ2∥µk(ˆπm
2)−µk(π∗
2)∥k
(∵Triangle inequality )
To upper bound these terms, we utilize the fact that the RKHS, Hk, corresponding to a c-universal kernel,
k, is dense inC(X)wrt. the supnorm (Sriperumbudur et al., 2011) and like-wise the direct-product space,
Hk⊗Hk, is dense inC(X×X )(Gretton, 2015). Given any f∈C(X)×C(X), and arbitrarily small ϵ>0,
we denote by fϵ,f−ϵthe functions inHk⊗Hkthat satisfy the condition:
f−ϵ/2≤f−ϵ≤f≤fϵ≤f+ϵ/2.
Such anfϵ∈Hk⊗Hkwill exist because: i) f+ϵ/4∈C(X)×C(X)and ii)Hk⊗Hk⊆C(X)×C(X)is
dense. So there must exist some fϵ∈Hk⊗Hksuch that|f(x,y) +ϵ/4−fϵ(x,y)|≤ϵ/4∀x,y∈X ⇐⇒
f(x,y)≤fϵ(x,y)≤f(x,y) +ϵ/2∀x,y∈X. Analogously, f−ϵexists. In other words, fϵ,f−ϵ∈Hk⊗Hkare
arbitrarily close upper-bound (majorant), lower-bound (minorant) of f∈C(X)×C(X).
We now upper-bound the first of the set of terms (denote s0(x)t0(y)byξ(x,y)and ˆξm(x,y)is the corre-
sponding empirical measure):
/integraldisplay
cdˆπm−/integraldisplay
cdπ∗≤/integraldisplay
cϵdˆπm−/integraldisplay
c−ϵdπ∗
=⟨cϵ,µk(ˆπm)⟩−⟨c−ϵ,µk(π∗)⟩
=⟨cϵ,µk(ˆπm)⟩−⟨cϵ,µk(π∗)⟩+⟨cϵ,µk(π∗)⟩−⟨c−ϵ,µk(π∗)⟩
=⟨cϵ,µk(ˆπm)−µk(π∗)⟩+⟨cϵ−c−ϵ,µk(π∗)⟩
≤⟨cϵ,µk(ˆπm)−µk(π∗)⟩+ϵσπ∗
(∵∥cϵ−c−ϵ∥∞≤ϵand defineσsas the mass of measure s)
≤∥cϵ∥k∥µk(ˆπm)−µk(π∗)∥k+ϵσπ∗
27Published in Transactions on Machine Learning Research (01/2024)
One can obtain the tightest upper bound by choosing cϵ≡arg minv∈Hk⊗Hk∥v∥ks.t.c≤v≤c+ϵ/2.
Accordingly, we replace ∥c∥kbyg(ϵ)in the theorem statement4. Further, we have:
∥µk(ˆπm)−µk(π∗)∥2
k=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplay
ϕk(x)⊗ϕk(y)dˆπm(x,y)−/integraldisplay
ϕk(x)⊗ϕk(y)dπ∗(x,y)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
k
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplay
ϕk(x)⊗ϕk(y)d(ˆπm(x,y)−π∗(x,y))/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
k
=/angbracketleftbigg/integraldisplay
ϕk(x)⊗ϕk(y)d(ˆπm(x,y)−π∗(x,y)),/integraldisplay
ϕk(x′)⊗ϕk(y′)d(ˆπm(x′,y′)−π∗(x′,y′))/angbracketrightbigg
=/angbracketleftbigg/integraldisplay
ϕk(x)⊗ϕk(y)η(x,y)d/parenleftig
ˆξm(x,y)−ξ(x,y)/parenrightig
,/integraldisplay
ϕk(x′)⊗ϕk(y′)η(x′,y′)d/parenleftig
ˆξm(x′,y′)−ξ(x′,y′)/parenrightig/angbracketrightbigg
=/integraldisplay /integraldisplay
⟨ϕk(x)⊗ϕk(y),ϕk(x′)⊗ϕk(y′)⟩η(x,y)η(x′,y′)d/parenleftig
ˆξm(x,y)−ξ(x,y)/parenrightig
d/parenleftig
ˆξm(x′,y′)−ξ(x′,y′)/parenrightig
=/integraldisplay /integraldisplay
⟨ϕk(x),ϕk(x′)⟩⟨ϕk(y),ϕk(y′)⟩η(x,y)η(x′,y′)d/parenleftig
ˆξm(x,y)−ξ(x,y)/parenrightig
d/parenleftig
ˆξm(x′,y′)−ξ(x′,y′)/parenrightig
=/integraldisplay /integraldisplay
k(x,x′)k(y,y′)η(x,y)η(x′,y′)d/parenleftig
ˆξm(x,y)−ξ(x,y)/parenrightig
d/parenleftig
ˆξm(x′,y′)−ξ(x′,y′)/parenrightig
Now, observe that ˜k:X×X×X×X defined by ˜k((x,y),(x′,y′))≡k(x,x′)k(y,y′)η(x,y)η(x′,y′)is a valid
kernel. This is because ˜k=kakbkc, whereka((x,y),(x′,y′))≡k(x,x′)is a kernel, kb((x,y),(x′,y′))≡
k(y,y′)is a kernel, and kc((x,y),(x′,y′))≡η(x,y)η(x′,y′)is a kernel (the unit-rank kernel), and product of
kernels is indeed a kernel. Let ψ(x,y)be the feature map corresponding to ˜k. Then, the final RHS in the
above set of equations is:
=/integraldisplay /integraldisplay
⟨ψ(x,y),ψ(x′,y′)⟩d/parenleftig
ˆξm(x,y)−ξ(x,y)/parenrightig
d/parenleftig
ˆξm(x′,y′)−ξ(x′,y′)/parenrightig
=/angbracketleftbigg/integraldisplay
ψ(x,y)d/parenleftig
ˆξm(x,y)−ξ(x,y)/parenrightig
,/integraldisplay
ψ(x′,y′)d/parenleftig
ˆξm(x′,y′)−ξ(x′,y′)/parenrightig/angbracketrightbigg
.
Hence, we have that: ∥µk(ˆπm)−µk(π∗)∥k=/vextenddouble/vextenddouble/vextenddoubleµ˜k(ˆξm)−µ˜k(ξ)/vextenddouble/vextenddouble/vextenddouble˜k. Again, using (Muandet et al., 2017,
Theorem 3.4), with probability at least 1−δ,/vextenddouble/vextenddouble/vextenddoubleµ˜k(ˆξm)−µ˜k(ξ)/vextenddouble/vextenddouble/vextenddouble˜k≤C˜k
m+√
2C˜klog(1/δ)
m, whereC˜k=
max
x,y,x′,y′∈X˜k((x,y),(x′,y′)). Note that C˜k<∞asXis compact and s0,t0are assumed to be positive
measures and kis normalized.
Now the MMD-regularizer terms can be bounded using a similar strategy. Recall that, ˆπm
1(xi) =/summationtextn
j=1π∗(xi,yj)
m2s0(xi)t0(yj), so we have the following.
∥µk(ˆπm
1)−µk(π∗
1)∥2
k=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplay
ϕk(x)dˆπm
1(x)−/integraldisplay
ϕk(x)dπ∗
1(x)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
k
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplay
ϕk(x)d(ˆπm
1(x)−π∗
1(x))/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
k
=/angbracketleftbigg/integraldisplay
ϕk(x)d(ˆπm
1(x)−π∗
1(x)),/integraldisplay
ϕk(x′)d(ˆπm
1(x′)−π∗
1(x′))/angbracketrightbigg
=/angbracketleftbigg/integraldisplay
ϕk(x)η(x,y)d/parenleftig
ˆξm(x,y)−ξ(x,y)/parenrightig
,/integraldisplay
ϕk(x′)η(x′,y′)d/parenleftig
ˆξm(x′,y′)−ξ(x′,y′)/parenrightig/angbracketrightbigg
=/integraldisplay /integraldisplay
⟨ϕk(x),ϕk(x′)⟩η(x,y)η(x′,y′)d/parenleftig
ˆξm(x,y)−ξ(x,y)/parenrightig
d/parenleftig
ˆξm(x′,y′)−ξ(x′,y′)/parenrightig
=/integraldisplay /integraldisplay
k(x,x′)η(x,y)η(x′,y′)d/parenleftig
ˆξm(x,y)−ξ(x,y)/parenrightig
d/parenleftig
ˆξm(x′,y′)−ξ(x′,y′)/parenrightig
.
4This leads to a slightly weaker bound, but we prefer it for ease of presentation
28Published in Transactions on Machine Learning Research (01/2024)
Now, observe that ¯k:X×X×X×X defined by ¯k((x,y),(x′,y′))≡k(x,x′)η(x,y)η(x′,y′)is a valid ker-
nel. This is because ¯k=k1k2, wherek1((x,y),(x′,y′))≡k(x,x′)is a kernel and k2((x,y),(x′,y′))≡
η(x,y)η(x′,y′)is a kernel (the unit-rank kernel), and product of kernels is indeed a kernel. Hence,
we have that:∥µk(ˆπm
1)−µk(π∗
1)∥k=/vextenddouble/vextenddouble/vextenddoubleµ¯k(ˆξm)−µ¯k(ξ)/vextenddouble/vextenddouble/vextenddouble¯k. Similarly, we have: ∥µk(ˆπm
2)−µk(π∗
2)∥k=/vextenddouble/vextenddouble/vextenddoubleµ¯k(ˆξm)−µ¯k(ξ)/vextenddouble/vextenddouble/vextenddouble¯k. Again, using (Muandet et al., 2017, Theorem 3.4), with probability at least 1−δ,
/vextenddouble/vextenddouble/vextenddoubleµ¯k(ˆξm)−µ¯k(ξ)/vextenddouble/vextenddouble/vextenddouble¯k≤C¯k
m+√
2C¯klog(1/δ)
m, whereC¯k= max
x,y,x′,y′∈X¯k((x,y),(x′,y′)). Note that C¯k<
∞asXis compact, s0,t0are assumed to be positive measures, and kis normalized. From the
union bound, we have:/vextendsingle/vextendsingle/vextendsingleˆUm(ˆsm,ˆtm)−¯U(s0,t0)/vextendsingle/vextendsingle/vextendsingle≤(λ1+λ2)/parenleftbigg
1√m+/radicalig
2 log (5/δ)
m+C¯k
m+√
2C¯klog(5/δ)
m/parenrightbigg
+
g(ϵ)/parenleftbigg
C˜k
m+√
2C˜klog (5/δ)
m/parenrightbigg
+ϵσπ∗, with probability at least 1−δ. In other words, w.h.p. we have:
/vextendsingle/vextendsingle/vextendsingleˆUm(ˆsm,ˆtm)−¯U(s0,t0)/vextendsingle/vextendsingle/vextendsingle≤O/parenleftig
λ1+λ2√m+g(ϵ)
m+ϵσπ∗/parenrightig
for anyϵ>0. Hence proved.
B.9.1 Bounding g(ϵ)
Let the target function to be approximated be h∗∈C(X)⊂L2(X), which is the set of square-integrable
functions (wrt. some measure). Since Xis compact, kbeing c-universal, it is also L2-universal.
Consider the inclusion map ι:Hk∝⇕⊣√∫⊔≀→L2(X), defined by ιg=g. Let’s denote the adjoint of ιbyι∗. Consider
the regularized least square approximation of h∗defined by ht≡(ι∗ι+t)−1ι∗h∗∈Hk, wheret>0. Now,
using standard results, we have:
∥ιht−h∗∥L2=∥/parenleftbig
ι(ι∗ι+t)−1ι∗−I/parenrightbig
h∗∥L2
=∥/parenleftbig
ι ι∗(ι ι∗+t)−1−I/parenrightbig
h∗∥L2
=∥/parenleftbig
ι ι∗(ι ι∗+t)−1−(ι ι∗+t)(ι ι∗+t)−1/parenrightbig
h∗∥L2
=t∥(ι ι∗+t)−1h∗∥L2
≤t∥(ι ι∗)−1h∗∥L2
The last inequality is true because the operator ι ι∗is PD andt >0. Thus, ift≡ˆt=ϵ
∥(ι ι∗)−1h∗∥L2, then
∥ιhˆt−h∗∥∞≤∥ιhˆt−h∗∥L2≤ϵ. Clearly,
g(ϵ)≤∥hˆt∥Hk
=/radicalig
⟨hˆt,hˆt⟩Hk
=/radicalig
⟨(ι∗ι+ˆt)−1ι∗h∗,(ι∗ι+ˆt)−1ι∗h∗⟩Hk
=/radicalig
⟨ι∗(ι ι∗+ˆt)−1h∗,ι∗(ι ι∗+ˆt)−1h∗⟩Hk
=/radicalig
⟨(ι ι∗+ˆt)−1ι ι∗(ι ι∗+ˆt)−1h∗,h∗⟩L2
=/radicalig
⟨(ι ι∗)1
2(ι ι∗+ˆt)−1h∗,(ι ι∗)1
2(ι ι∗+ˆt)−1h∗⟩L2
=∥(ι ι∗)1
2(ι ι∗+ˆt)−1h∗∥L2.
Now, consider the spectral function f(λ) =λ1
2
λ+ˆt. This is maximized when λ=ˆt. Hence,f(λ)≤1
2√
ˆt. Thus,
g(ϵ)≤∥h∗∥L2√
∥(ι ι∗)−1h∗∥L2
2√ϵ. Therefore, as ϵdecays as1
m2/3, then,g(ϵ)
m≤O/parenleftbig1
m2/3/parenrightbig
.
29Published in Transactions on Machine Learning Research (01/2024)
B.10 Solving Problem (9) using Mirror Descent
Problem (9) is an instance of a convex program and can be solved using Mirror Descent (Ben-Tal & Ne-
mirovski, 2021), presented in Algorithm 2.
Algorithm 2 Mirror Descent for solving Problem (9)
Require: Initialα1≥0, max iterations N.
f(α) =Tr/parenleftbig
αC⊤
12/parenrightbig
+λ1/vextenddouble/vextenddoubleα1−σ1
m1/vextenddouble/vextenddouble
G11+λ2/vextenddouble/vextenddoubleα⊤1−σ2
m1/vextenddouble/vextenddouble
G22.
fori←1toNdo
if∥∇f(αi)∦=0then
si= 1/∥∇f(αi)∥∞.
else
returnαi.
end if
αi+1=αi⊙e−si∇f(αi).
end for
returnαi+1.
B.11 Equivalence between Problems (9) and (10)
We comment on the equivalence between Problems (9) and (10) based on the equivalence of their Ivanov
forms:
Ivanov form for Problem (9) is
min
α≥0∈Rm1×m2Tr/parenleftbig
αC⊤
12/parenrightbig
s.t./vextenddouble/vextenddouble/vextenddouble/vextenddoubleα1−σ1
m11/vextenddouble/vextenddouble/vextenddouble/vextenddouble
G11≤r1,/vextenddouble/vextenddouble/vextenddouble/vextenddoubleα⊤1−σ2
m21/vextenddouble/vextenddouble/vextenddouble/vextenddouble
G22≤r2,
wherer1,r2>0.
Similarly, the Ivanov form for Problem (10) is
min
α≥0∈Rm1×m2Tr/parenleftbig
αC⊤
12/parenrightbig
s.t./vextenddouble/vextenddouble/vextenddouble/vextenddoubleα1−σ1
m11/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
G11≤¯r1,/vextenddouble/vextenddouble/vextenddouble/vextenddoubleα⊤1−σ2
m21/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
G22≤¯r2,
where ¯r1,¯r2>0.
As we can see, the Ivanov forms are the same with ¯r1=r2
1,¯r2=r2
2, the solutions obtained for Problems (9)
and (10) are the same.
B.12 Proof of Lemma 4.11
Proof.Letf(α)denote the objective of Problem (10), G11,G22are the Gram matrices over the source and
target samples, respectively and m1,m2as the number of source and target samples respectively.
∇f(α) =C12+ 2/parenleftigg
λ1G11/parenleftbigg
α1m2−σ1
m11m1/parenrightbigg
1⊤
m2+λ21m1/parenleftbigg
1⊤
m1α−1⊤
m2σ2
m2/parenrightbigg
G22/parenrightigg
We now derive the Lipschitz constant of this gradient.
∇f(α)−∇f(β) = 2/parenleftbig
λ1G11(α−β)1m21⊤
m2+1m11⊤
m1λ2(α−β)G22/parenrightbig
vec/parenleftbig
(∇f(α)−∇f(β))⊤/parenrightbig
= 2/parenleftig
λ1vec/parenleftbig
(G11(α−β)1m21⊤
m2/parenrightbig⊤) +λ2vec/parenleftbig
(1m11⊤
m1(α−β)G22)⊤/parenrightbig/parenrightig
= 2/parenleftbig
λ11m21⊤
m2⊗G11+λ2G22⊗1m11⊤
m1/parenrightbig
vec(α−β)
where⊗denotes Kronecker product.
30Published in Transactions on Machine Learning Research (01/2024)
∥vec(∇f(α)−∇f(β))∥F=∥vec/parenleftbig
(∇f(α)−∇f(β))⊤/parenrightbig
∥F
≤2∥λ11m21⊤
m2⊗G11+λ2G22⊗1m11⊤
m1∥F∥vec(α−β)∥F(Cauchy Schwarz).
This implies the Lipschitz smoothness constant
L= 2∥λ11m21⊤
m2⊗G11+λ2G22⊗1m11⊤
m1∥F
= 2/radicalig
(λ1m2)2∥G11∥2
F+ (λ2m1)2∥G22∥2
F+ 2λ1λ2/angbracketleftbig
1m21⊤m2⊗G11,G22⊗1m11⊤m1/angbracketrightbig
F
= 2/radicalig
(λ1m2)2∥G11∥2
F+ (λ2m1)2∥G22∥2
F+ 2λ1λ2(1⊤m1G111m1) (1⊤m2G221m2).
For the last equality, we use the following properties for Kronecker products-
Mixed product property: (A⊗B)⊤=A⊤⊗B⊤,(A⊗B)(C⊗D) = (AC)⊗(BD)and
Spectrum property: Tr ((AC)⊗(BD)) =Tr(AC)Tr(BD).
B.13 Solving Problem (10) using Accelerated Projected Gradient Descent
In Algorithm 1, we present the accelerated projected gradient descent (APGD) algorithm that we use to solve
Problem (10), as discussed in Section 4.2. The projection operation involved is Project≥0(x) = max( x,0).
B.14 More on the Barycenter problem
B.14.1 Proof of Lemma 4.12
Proof.Recall that we estimate the barycenter with the restriction that the transport plan πicorresponding
toˆU(ˆsi,s)is supported onDi×∪n
i=1Di. Letβ≥0∈Rmdenote the probabilities parameterizing the
barycenter, s. With ˆUmasdefinedinEquation(9), theMMD-UOTbarycenterformulation, ˆBm(ˆs1,···,ˆsn) =
min
β≥0/summationtextn
i=1ρiˆUm(ˆsi,s(β)), becomes
min
α1,···,αn,β≥0n/summationdisplay
i=1ρi/braceleftigg
Tr/parenleftbig
αiC⊤
i/parenrightbig
+λ1∥αi1−σi
mi1∥Gii+λ2∥α⊤
i1−β∥G/bracerightigg
. (20)
Following our discussion in Sections 4.2 and B.11, we present an equivalent barycenter formulation with
squared-MMD regularization. This not only makes the objective smooth, allowing us to exploit accelerated
solvers, but also simplifies the problem, as we discuss next.
B′
m(ˆs1,···,ˆsn)≡ min
α1,···,αn,β≥0n/summationdisplay
i=1ρi/braceleftigg
Tr/parenleftbig
αiC⊤
i/parenrightbig
+λ1∥αi1−σi
mi1∥2
Gii+λ2∥α⊤
i1−β∥2
G/bracerightigg
.(21)
The above problem is a least squares problem in terms of βwith a non-negativity constraint. Equating the
gradient wrt βas 0, we get G(β−/summationtextn
j=1ρjα⊤
j1) = 0. As the Gram matrices of universal kernels are full-rank
(Song, 2008, Corollary 32), this implies β=/summationtextn
j=1ρjα⊤
j1, which also satisfies the non-negativity constraint.
Substituting β=/summationtextn
j=1ρjα⊤
j1in 21 gives us the MMD-UOT barycenter formulation:
B′
m(ˆs1,···,ˆsn)≡ min
α1,···,αn,β≥0n/summationdisplay
i=1ρi/braceleftigg
Tr/parenleftbig
αiC⊤
i/parenrightbig
+λ1∥αi1−σi
mi1∥2
Gii+λ2∥α⊤
i1−n/summationdisplay
j=1ρjα⊤
j1∥2
G/bracerightigg
.(22)
31Published in Transactions on Machine Learning Research (01/2024)
B.14.2 Solving the Barycenter Formulation
The objective of 22, as a function of αi, has the following smoothness constant (derivation analogous to
Lemma 4.11 in the main paper).
Li= 2ρi/radicalig
(λ1m)2∥Gii∥2
F+ (ηimi)2∥G∥2
F+ 2λ1ηi(1⊤miGii1mi)(1⊤mG1m)
whereηi=λ2(1−ρi). We jointly optimize for αi’s using accelerated projected gradient descent with step-size
1/Li.
B.14.3 Consistency of the Barycenter estimator
Similar to Theorem 4.10, we show the consistency of the proposed sample-based barycenter estimator. Let
ˆsibe the empirical measure supported over msamples from si. From the proof of Lemma 4.12 and 22, recall
that,
B′
m(s1,···,sn) = min
α1,···,αn≥0n/summationdisplay
i=1ρi/parenleftig
Tr/parenleftbig
αiC⊤
i/parenrightbig
+λ1∥αi1−ˆsi∥2
Gii+λ2∥α⊤
i1−n/summationdisplay
j=1ρjα⊤
j1∥2
G/parenrightig
.
Now let us denote the true Barycenter with squared-MMD regularization by B(s1,···,sn)≡
min
s∈R+(X)/summationtextn
i=1ρiU(si,s)whereU(si,s)≡ min
πi∈R+(X)/integraltext
cdπi+λ1MMD2
k(πi
1,si) +λ2MMD2
k(πi
2,s). Let
π1∗,...,πn∗,s∗be the optimal solutions corresponding to B(s1,···,sn). It is easy to see that s∗=/summationtextn
j=1ρjπj∗
2(for e.g. refer (Cohen et al., 2020, Sec C)). After eliminating s, we have:B(s1,···,sn) =
min
π1,...,πn∈R+(X)/summationtextn
i=1ρi/parenleftig/integraltext
cdπi+λ1MMD2
k(πi
1,si) +λ2MMD2
k(πi
2,/summationtextn
j=1ρjπj
2)/parenrightig
.
Theorem B3. Letηi(x,z)≡πi∗(x,z)
si(x)s′(z)wheres′is the mixture density s′≡/summationtextn
i=11
nsi. Under
mild assumptions that the functions, ηi,c∈ Hk⊗Hk, we have that w.h.p., the estimation error,
|B′
m(ˆs1,···,ˆsm)−B(s1,···,sn)|≤O ( max
i∈[1,n]/parenleftbig
∥ηi∥k∥c∥k/parenrightbig
/m).
Proof.From triangle inequality,
|B′
m(ˆs1,···,ˆsn)−B(s1,···,sn)|≤|B′
m(ˆs1,···,ˆsn)−B′
m(s1,···,sn)|+|B′
m(s1,···,sn)−B(s1,···,sn)|,(23)
whereB′
m(s1,···,sn)is the same as B(s1,···,sn)except that it employs restricted feasibility sets,
Fi(ˆs1,···,ˆsn)for corresponding αias the set of all joints supported at the samples in ˆs1,···,ˆsnalone.
LetDi={xi1,···,xim}and the union of all samples, ∪Dn
i=1={z1,···,zmn}.
Fi(ˆs1,···,ˆsn)≡/braceleftig/summationtextm
l=1/summationtextmn
j=1αljδ(xil,zj)|αlj≥0∀l= 1,...,m ;j= 1,...,mn/bracerightig
. Here,δris the Dirac
measure at r. We begin by bounding the first term.
We denote the (common) objective in B′
m(·),B(·)as a function of the transport plans, (π1,···,πn), by
h(π1,···,πn,·).
B′
m(ˆs1,···,ˆsn)−B′
m(s1,···,sn) = min
πi∈Fi(ˆs1,···,ˆsn)h(π1,···,πn,ˆs1,···,ˆsn)− min
πi∈Fi(ˆs1,···,ˆsn)h(π1,···,πn,s1,···,sn)
≤h(¯π1∗,···,¯πn∗,ˆs1,···,ˆsn)−h(¯π1∗,···,¯πn∗,s1,···,sn)/parenleftig
where ¯πi∗= arg minπi∈Fi(ˆs1,···,ˆsn)h(π1,···,πn,s1,···,sn)fori∈[1,n]/parenrightig
=/summationtextn
i=1λ1ρi/parenleftbig
MMD2
k(¯πi∗
1,ˆsi)−MMD2
k(¯πi∗
1,si)/parenrightbig
=/summationtextn
i=1ρiλ1/parenleftbig
MMDk(¯πi∗
1,ˆsi)−MMDk(¯πi∗
1,si)/parenrightbig/parenleftbig
MMDk(¯πi∗
1,ˆsi) +MMDk(¯πi∗,si)/parenrightbig
(1)
≤2λ1M/summationtextn
i=1ρi/parenleftbig
MMDk(¯πi∗
1,ˆsi)−MMDk(¯πi∗
1,si)/parenrightbig
≤2λ1M/summationtextn
i=1ρiMMDk(ˆsi,si)(As MMD satisfies Triangle Inequality) ,
≤2λ1Mmax
i∈[1,n]MMDk(ˆsi,si)
32Published in Transactions on Machine Learning Research (01/2024)
where for inequality (1) we use that max
s,t∈R+
1(X)MMDk(s,t) =M <∞as the generating set of MMD is
compact.
As with probability at least 1−δ, MMDk(ˆsi,si)≤1√m+/radicalig
2 log(1/δ)
m(Smola et al., 2007), with union
bound, we get that the first term in inequality (23) is upper-bounded by 2λ1M/parenleftbigg
1√m+/radicalig
2 log 2n/δ
m/parenrightbigg
, with
probability at least 1−δ.
We next look at the second term in inequality (23): |B′
m(s1,···,sn)−B(s1,···,sn)|. Let (¯π1,···,¯πn)be
the solutions ofB′
m(s1,···,sn). Let (π1∗,···,πn∗)be the solutions of B(s1,···,sn). Recall that s′denotes
the mixture density s′≡/summationtextn
i=11
nsi. Let us denote the empirical distribution of s′byˆs′(i.e., uniform samples
from∪n
i=1Di). Consider the transport plans: ˆπim∈Fi(ˆs1,···,ˆsn)such that ˆπim(l,j) =ηi(xl,zj)
m2nwhere
33Published in Transactions on Machine Learning Research (01/2024)
ηi(xl,zj) =πi∗(xl,zj)
si(xl)s′(zj), forl∈[1,m];j∈[1,mn].
|B′
m(s1,···,sn)−B(s1,···,sn)|=B′
m(s1,···,sn)−B(s1,···,sn)
=h(¯π1m,···,¯πnm,s1,···,sn)−h(π1∗,···,πn∗,s1,···,sn)
≤h(ˆπ1m,···,ˆπnm,s1,···,sn)−h(π1∗,···,πn∗,s1,···,sn)
=n/summationdisplay
i=1ρi/braceleftigg
/angbracketleftbig
µk(ˆπim)−µk(πi∗),ci/angbracketrightbig
+ 2λ1M/parenleftig
∥µk(ˆπim
1)−µk(si)∥k
−∥µk(πi∗
1)−µk(si)∥k/parenrightig
+
2λ2M
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleµk(ˆπim
2)−µk
n/summationdisplay
j=1ρjˆπjm
2
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
k−/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleµk(πi∗
2)−µk
n/summationdisplay
j=1ρjπj∗
2
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
k
/bracerightigg
(Upper-bounding the sum of two MMD terms by 2M)
≤n/summationdisplay
i=1ρi/braceleftigg
/angbracketleftbig
µk(ˆπim)−µk(πi∗),ci/angbracketrightbig
+ 2λ1M/vextenddouble/vextenddoubleµk(ˆπim
1)−µk(πi∗
1)/vextenddouble/vextenddouble
k+
2λ2M
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleµk(ˆπim
2)−µk
n/summationdisplay
j=1ρjˆπjm
2
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
k−/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleµk(πi∗
2)−µk
n/summationdisplay
j=1ρjπj∗
2
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
k
/bracerightigg
(Using triangle inequality)
≤n/summationdisplay
i=1ρi/braceleftigg
/angbracketleftbig
µk(ˆπim)−µk(πi∗),ci/angbracketrightbig
+ 2λ1M∥µk(ˆπim
1)−µk(πi∗
1)∥k+
2λ2M
∥µk(ˆπim
2)−µk(πi∗
2)∥k+n/summationdisplay
j=1ρj∥µk(ˆπjm
2)−µk(πj∗
2)∥k
/bracerightigg
(Triangle Inequality and linearity of the kernel mean embedding)
≤n/summationdisplay
i=1ρi/braceleftigg
∥µk(ˆπim)−µk(πi∗)∥k∥ci∥k+ 2λ1M∥µk(ˆπim
1)−µk(πi∗
1)∥k+
2λ2M
∥µk(ˆπim
2)−µk(πi∗
2)∥k+n/summationdisplay
j=1ρj∥µk(ˆπjm
2)−µk(πj∗
2)∥k
/bracerightigg
(Cauchy Schwarz)
≤max
i∈[1,n]/braceleftigg
∥µk(ˆπim)−µk(πi∗)∥k∥ci∥k+ 2λ1M∥µk(ˆπim
1)−µk(πi∗
1)∥k+
2λ2M/parenleftbigg
∥µk(ˆπim
2)−µk(πi∗
2)∥k+ max
j∈[1,n]∥µk(ˆπjm
2)−µk(πj∗
2)∥k/parenrightbigg/bracerightigg
34Published in Transactions on Machine Learning Research (01/2024)
We now repeat the steps similar to B.9 (for bounding the second term in the proof of Theorem 4.10) and
get the following.
∥µk(ˆπim)−µk(πi∗)∥k= max
f∈Hk,∥f∥k≤1/vextendsingle/vextendsingle/vextendsingle/integraltext
fdˆπim−/integraltext
fdπi∗/vextendsingle/vextendsingle/vextendsingle
= max
f∈Hk,∥f∥k≤1/integraltext
fdˆπim−/integraltext
fdπi∗
= max
f∈Hk,∥f∥k≤1/summationtextm
l=1/summationtextmn
j=1f(xl,zj)πi∗(xl,zj)
m2nsi(xl)s′(zj)−/integraltext/integraltext
f(x,z)πi∗(x,z)
si(x)s′(z)si(x)s′(z)dxdz
= max
f∈Hk,∥f∥k≤1EX∼ˆsi−si,Z∼ˆs′−s′/bracketleftig
f(X,Z)πi∗(X,Z)
si(X)s′(Z)/bracketrightig
= max
f∈Hk,∥f∥k≤1EX∼ˆsi−si,Z∼ˆs′−s′/bracketleftbig
f(X,Z)ηi(X,Z)/bracketrightbig
= max
f∈Hk,∥f∥k≤1EX∼ˆsi−si,Z∼ˆs′−s′/bracketleftbig/angbracketleftbig
f⊗ηi,ϕ(X)⊗ϕ(Z)⊗ϕ(X)⊗ϕ(Z)/angbracketrightbig/bracketrightbig
= max
f∈Hk,∥f∥k≤1/angbracketleftbig
f⊗ηi,EX∼ˆsi−si,Z∼ˆs′−s′[ϕ(X)⊗ϕ(Z)⊗ϕ(X)⊗ϕ(Z)]/angbracketrightbig
≤ max
f∈Hk,∥f∥k≤1∥f⊗ηi∥k∥EX∼ˆsi−si,Z∼ˆs′−s′[ϕ(X)⊗ϕ(Z)⊗ϕ(X)⊗ϕ(Z)]∥k
(∵Cauchy Schwarz)
= max
f∈Hk,∥f∥k≤1∥f∥k∥ηi∥k∥EX∼ˆsi−si,Z∼ˆs′−s′[ϕ(X)⊗ϕ(X)⊗ϕ(Z)⊗ϕ(Z)]∥k
(∵properties of norm of tensor product)
= max
f∈Hk,∥f∥k≤1∥f∥k∥ηi∥k∥EX∼ˆsi−si[ϕ(X)⊗ϕ(X)]⊗EZ∼ˆs′−s′[ϕ(Z)⊗ϕ(Z)]∥k
≤∥ηi∥k∥EX∼ˆsi−si[ϕ(X)⊗ϕ(X)]∥k∥EZ∼ˆs′−s′[ϕ(Z)⊗ϕ(Z)]∥k
=∥ηi∥k∥µk2(ˆsi)−µk2(si)∥k2∥µk2(ˆs′)−µk2(s′)∥k2
(∵ϕ(·)⊗ϕ(·)is the feature map corresponding to k2.)
Similarly, we have the following for the marginals.
∥µk(ˆπim
1)−µk(πi∗
1)∥k= max
f∈Hk,∥f∥k≤1/vextendsingle/vextendsingle/vextendsingle/integraldisplay
fdˆπim
1−/integraldisplay
fdπi∗
1/vextendsingle/vextendsingle/vextendsingle
= max
f∈Hk,∥f∥k≤1/integraldisplay
fdˆπim
1−/integraldisplay
fdπi∗
1
= max
f∈Hk,∥f∥k≤1m/summationdisplay
l=1mn/summationdisplay
j=1f(xl)πi∗(xl,zj)
m2nsi(xl)s′(zj)−/integraldisplay /integraldisplay
f(x)πi∗(x,z)
si(x)s′(z)si(x)s′(z)dxdz
= max
f∈Hk,∥f∥k≤1EX∼ˆsi−si,Z∼ˆs′−s′/bracketleftbigg
f(X)πi∗(X,Z)
si(X)s′(Z)/bracketrightbigg
= max
f∈Hk,∥f∥k≤1EX∼ˆsi−si,Z∼ˆs′−s′/bracketleftbig
f(X)ηi(X,Z)/bracketrightbig
= max
f∈Hk,∥f∥k≤1EX∼ˆsi−si,Z∼ˆs′−s′/bracketleftbig/angbracketleftbig
f⊗ηi,ϕ(X)⊗ϕ(X)⊗ϕ(Z)/angbracketrightbig/bracketrightbig
= max
f∈Hk,∥f∥k≤1/angbracketleftbig
f⊗ηi,EX∼ˆsi−si,Z∼ˆs′−s′[ϕ(X)⊗ϕ(X)⊗ϕ(Z)]/angbracketrightbig
≤ max
f∈Hk,∥f∥k≤1∥f⊗ηi∥k∥EX∼ˆsi−si,Z∼ˆs′−s′[ϕ(X)⊗ϕ(X)⊗ϕ(Z)]∥k
(∵Cauchy Schwarz)
= max
f∈Hk,∥f∥k≤1∥f∥k∥ηi∥k∥EX∼ˆsi−si,Z∼ˆs′−s′[ϕ(X)⊗ϕ(X)⊗ϕ(Z)]∥k
(∵properties of norm of tensor product)
= max
f∈Hk,∥f∥k≤1∥f∥k∥ηi∥k∥EX∼ˆsi−si[ϕ(X)⊗ϕ(X)]⊗EZ∼ˆs′−s′[ϕ(Z)]∥k
≤∥ηi∥k∥EX∼ˆsi−si[ϕ(X)⊗ϕ(X)]∥k∥EZ∼ˆs′−s′[ϕ(Z)]∥k
=∥ηi∥k∥µk2(ˆsi)−µk2(si)∥k2∥µk(ˆs′)−µk(s′)∥k
(∵ϕ(·)⊗ϕ(·)is the feature map corresponding to k2.)
35Published in Transactions on Machine Learning Research (01/2024)
Thus, with probability at least 1−δ,|B′
m(s1,···,sn)−B(s1,···,sn)| ≤/parenleftbigg
max
i∈[1,n]/braceleftig
∥ηi∥k∥ci∥k+ 2λ1M∥ηi∥k+ 2λ2M(∥ηi∥k+ max
j∈[1,n]∥ηj∥k)/bracerightig/parenrightbigg/parenleftbigg
1√m+/radicalig
2 log (2n+2)/δ
m/parenrightbigg2
. Ap-
plying union bound again for the inequality in 23, we get that with probability at least 1−δ,
|B′
m(ˆs1,···,ˆsn)−B(s1,···,sn)| ≤/parenleftbigg
1√m+/radicalig
2 log (2n+4)/δ
m/parenrightbigg/parenleftbigg
2λ1M+ζ/parenleftbigg
1√m+/radicalig
2 log (2n+4)/δ
m/parenrightbigg/parenrightbigg
, where
ζ=/parenleftbigg
max
i∈[1,n]/braceleftig
∥ηi∥k∥ci∥k+ 2λ1M∥ηi∥k+ 2λ2M(∥ηi∥k+ max
j∈[1,n]∥ηj∥k)/bracerightig/parenrightbigg
.
B.15 More on Formulation (10)
Analogous to Formulation (10) in the main paper, we consider the following formulation where an IPM
raised to the qthpower with q>1∈Zis used for regularization.
UG,c,λ1,λ2,q(s0,t0)≡ min
π∈R+(X×X )/integraldisplay
cdπ+λ1γq
G(π1,s0) +λ2γq
G(π2,t0) (24)
Formulation (10) in the main paper is a special case of Formulation (24), when IPM is MMD and q= 2.
Following the proof in Lemma B1, one can easily show that
UG,c,λ1,λ2,q(s0,t0)≡ min
s,t∈R+(X)|s|W1(s,t) +λ1γq
G(s,s0) +λ2γq
G(t,t0). (25)
To simplify notations, we denote UG,c,λ,λ, 2byUin the following. It is easy to see that Usatisfies the
following properties by inheritance.
1.U≥0as each of the terms in the objective in Formulation (25) is greater than 0.
2.U(s0,t0) = 0⇐⇒s0=t0, whenever the IPM used for regularization is a norm-induced metric. As
W1,γGare non-negative terms, U(s0,t0) = 0⇐⇒s=t,γG(s,s0) = 0,γG(t,t0) = 0. If IPM used
for regularization is a norm-induced metric, the above condition reduces to s0=t0.
3.U(s0,t0) =U(t0,s0)as each term in Formulation (25) is symmetric.
We now derive sample complexity with Formulation (24).
Lemma B4. Let us denote UG,c,λ1,λ2,qdefined in Formulation (9) by U, whereq>1∈Z. Let ˆsm,ˆtmdenote
the empirical estimates of s0,t0∈R+
1(X)respectively with msamples. Then, U(ˆsm,ˆtm)→U(s0,t0)at a
rate same as that of γG(ˆsm,s0)→0.
Proof.
U(s0,t0)≡ min
π∈R+(X×X )h(π,s0,t0)≡/integraldisplay
cdπ+λγq
G(π1,s0) +λγq
G(π2,t0)
We have,
U(sm,tm)−U(s0,t0) = min
π∈R+(X×X )h(π,ˆsm,ˆtm)− min
π∈R+(X×X )h(π,s0,t0)
≤h(π∗,ˆsm,ˆtm)−h(π∗,s0,t0)/parenleftigg
whereπ∗= arg min
π∈R+(X×X )h(π,s0,t0)/parenrightigg
=λ/parenleftbig
γq
G(π∗
1,ˆsm)−γq
G(π∗
1,s0) +γq
G(π∗
2,ˆtm)−γq
G(π∗
2,t0)/parenrightbig
=λ/parenleftigg
(γG(π∗
1,ˆsm)−γG(π∗
1,s0))/parenleftiggq−1/summationdisplay
i=0γi
G(π∗
1,ˆsm)γq−1−i
G (π∗
1,s0)/parenrightigg/parenrightigg
+
λ/parenleftigg
/parenleftbig
γG(π∗
2,ˆtm)−γG(π∗
2,t0)/parenrightbig/parenleftiggq−1/summationdisplay
i=0γi
G(π∗
2,ˆtm)γq−1−i
G (π∗
2,t0)/parenrightigg/parenrightigg
36Published in Transactions on Machine Learning Research (01/2024)
≤λ/parenleftigg
γG(s0,ˆsm)/parenleftiggq−1/summationdisplay
i=0γi
G(π∗
1,ˆsm)γq−1−i
G (π∗
1,s0)/parenrightigg/parenrightigg
+
λ/parenleftigg
γG(t0,ˆtm)/parenleftiggq−1/summationdisplay
i=0γi
G(π∗
2,ˆtm)γq−1−i
G (π∗
2,t0)/parenrightigg/parenrightigg
(∵γGsatisfies triangle inequality )
≤λ/parenleftigg
γG(s0,ˆsm)q−1/summationdisplay
i=0/parenleftbigg/parenleftbiggq−1
i/parenrightbigg
γi
G(π∗
1,ˆsm)γq−1−i
G (π∗
1,s0)/parenrightbigg/parenrightigg
+
λ/parenleftigg
(γG(t0,ˆtm)/parenleftiggq−1/summationdisplay
i=0/parenleftbiggq−1
i/parenrightbigg
γi
G(π∗
2,ˆtm)γq−1−i
G (π∗
2,t0)/parenrightigg/parenrightigg
=λ/parenleftig
γG(s0,ˆsm) (γG(π∗
1,ˆsm) +γG(π∗
1,s0))q−1
+γG(t0,ˆtm)/parenleftbig
γG(π∗
2,ˆtm) +γG(π∗
2,t0)/parenrightbigq−1/parenrightig
≤λ(2M)q−1/parenleftbig
γG(s0,ˆsm) +γG(t0,ˆtm)/parenrightbig
.
For the last inequality, we use that max
a∈R+
1(X)max
b∈R+
1(X)γG(a,b) =M <∞as the domain is compact.
Similarly, one can show the other way inequality, resulting in the following.
|U(s0,t0)−U(sm,tm)|≤λ(2M)q−1/parenleftbig
γG(s0,ˆsm) +γG(t0,ˆtm)/parenrightbig
. (26)
The rate at which |U(sm,tm)−U(s0,t0)|goes to zero is hence the same as that with which either of the
IPM terms goes to zero. For example, if the IPM used for regularization is MMD with a normalized kernel,
then MMD k(s0,ˆsm)≤/radicalig
1
m+/radicalig
2 log(1/δ)
mwith probability at least 1−δ(Smola et al., 2007).
From the union bound, with probability at least 1−δ,|U(sm,tm)−U(s0,t0)| ≤
2λ(2M)q−1/parenleftbigg/radicalig
1
m+/radicalig
2 log(2/δ)
m/parenrightbigg
. Thus,O/parenleftig
1√m/parenrightig
is the common bound for the rate at which the
LHS as well as the MMD k(s0,ˆsm)decays to zero.
B.16 Robustness
We show the robustness property of IPM-regularized UOT 13 with the same assumptions on the noise model
as used in (Fatras et al., 2021, Lemma 1) for KL-regularized UOT.
Lemma B5. (Robustness) Lets0,t0∈R+
1(X). Consider sc=ρs0+ (1−ρ)δz(ρ∈[0,1]), a distribution
perturbed by a Dirac outlier located at some zoutside of the support of t0. Letm(z) =/integraltext
c(z,y)dt0(y).
We have that,UG,c,λ1,λ2(sc,t0)≤ρUG,c,λ1,λ2(s0,t0) + (1−ρ)m(z).
37Published in Transactions on Machine Learning Research (01/2024)
Proof.Letπbe the solution of UG,c,λ1,λ2(s0,t0). Consider ˜π=ρπ+ (1−ρ)δz⊗t0. It is easy to see that
˜π1=ρπ1+ (1−ρ)δzand˜π2=ρπ2+ (1−ρ)t0.
UG,c,λ1,λ2(sc,t0)≤/integraldisplay
c(x,y)d˜π(x,y) +λ1γG(˜π1,sc) +λ2γG(˜π2,t0)(Using the definition of min)
≤/integraldisplay
c(x,y)d˜π(x,y) +λ1(ργG(π1,s0) + (1−ρ)γG(δz,δz)) +λ2(ργG(π2,t0) + (1−ρ)γG(t0,t0))
(∵IPMs are jointly convex)
=/integraldisplay
c(x,y)d˜π(x,y) +ρ(λ1γG(π1,s0) +λ2γG(π2,t0))
=ρ/integraldisplay
c(x,y)dπ(x,y) +/integraldisplay
(1−ρ)c(z,y)d(δz⊗t0)(z,y) +ρ(λ1γG(π1,s0) +λ2γG(π2,t0))
=ρ/integraldisplay
c(x,y)dπ(x,y) +/integraldisplay
(1−ρ)c(z,y)dt0(y) +ρ(λ1γG(π1,s0) +λ2γG(π2,t0))
=ρUG,c,λ1,λ2(s0,t0) + (1−ρ)m(z).
We note that m(z)is finite ast0∈R+
1(X).
We now present robustness guarantees with a different noise model.
Corollary B6. We say a measure q∈R+(X)is corrupted with ρ∈[0,1]fraction of noise when q=
(1−ρ)qc+ρqn, whereqcis the clean measure and qnis the noisy measure.
Lets0,t0∈R+(X)be corrupted with ρfraction of noise such that |sc−sn|TV≤ϵ1and|tc−tn|TV≤ϵ2. We
have thatUG,c,λ,λ(s0,t0)≤UG,c,λ,λ(sc,tc) +ρβ(ϵ1+ϵ2), whereβ= max
f∈G(λ)∩Wc∥f∥∞.
Proof.We use our duality result of UG,c,λ,λ, from Theorem 4.1. We first upper-bound UG,c,λ,λ (sn,tn)which
is later used in the proof.
UG,c,λ,λ (sn,tn) = max
f∈G(λ)∩Wc/integraldisplay
fdsn−/integraldisplay
fdtn
= max
f∈G(λ)∩Wc/integraldisplay
fd(sn−sc) +/integraldisplay
fdsc−/integraldisplay
fd(tn−tc)−/integraldisplay
fdtc
≤ max
f∈G(λ)∩Wc/integraldisplay
fd(sn−sc) + max
f∈G(λ)∩Wc/integraldisplay
fd(tn−tc) + max
f∈G(λ)∩Wc/parenleftbigg/integraldisplay
fdsc−/integraldisplay
fdtc/parenrightbigg
≤β(|sc−sn|TV+|tc−tn|TV) +UG,c,λ,λ(sc,tc)
=β(ϵ1+ϵ2) +UG,c,λ,λ(sc,tc). (27)
We now show the robustness result as follows.
UG,c,λ,λ(s0,t0) = max
f∈G(λ)∩Wc/integraldisplay
fds0−/integraldisplay
fdt0
= max
f∈G(λ)∩Wc(1−ρ)/integraldisplay
fdsc+ρ/integraldisplay
fdsn−(1−ρ)/integraldisplay
fdtc−ρ/integraldisplay
fdtn
= max
f∈G(λ)∩Wc(1−ρ)/parenleftbigg/integraldisplay
fdsc−/integraldisplay
fdtc/parenrightbigg
+ρ/parenleftbigg/integraldisplay
fdsn−/integraldisplay
fdtn/parenrightbigg
≤ max
f∈G(λ)∩Wc(1−ρ)/parenleftbigg/integraldisplay
fdsc−/integraldisplay
fdtc/parenrightbigg
+ max
f∈G(λ)∩Wcρ/parenleftbigg/integraldisplay
fdsn−/integraldisplay
fdtn/parenrightbigg
= (1−ρ)UG,c,λ,λ (sc,tc) +ρUG,c,λ,λ (sn,tn)
≤(1−ρ)UG,c,λ,λ (sc,tc) +ρ(UG,c,λ,λ (sc,tc) +β(ϵ1+ϵ2)) (Using 27)
=UG,c,λ,λ (sc,tc) +ρβ(ϵ1+ϵ2).
38Published in Transactions on Machine Learning Research (01/2024)
We note that β= max
f∈G(λ)∩Wc∥f∥∞≤max
f∈Wc∥f∥∞<∞. Also, as β≤min/parenleftbigg
max
f∈Gk(λ)∥f∥∞,max
f∈Wc∥f∥∞/parenrightbigg
≤
min/parenleftbigg
λ,max
f∈Wc∥f∥∞/parenrightbigg
(for a normalized kernel).
B.17 Connections with Spectral Normalized GAN
We comment on the applicability of MMD-UOT in generative modelling and draw connections with the
Spectral Norm GAN (SN-GAN) (Miyato et al., 2018) formulation.
A popular approach in generative modelling is to define a parametric function gθ:Z ∝⇕⊣√∫⊔≀→X that takes a
noise distribution and generates samples from Pθdistribution. We then learn θto makePθcloser to the real
distribution, Pr. On formulating this problem with the dual of MMD-UOT derived in Theorem 4.1, we get
min
θmax
f∈Wc∩Gk(λ)/integraldisplay
fdPθ−/integraldisplay
fdPr (28)
We note that in the above optimization problem, the critic function or the discriminator fshould satisfy
∥f∥c≤1and∥f∥k≤λwhere∥f∥cdenotes the Lipschitz norm under the cost function c. Let the critic func-
tion befW, parametrized using a deep convolution neural network (CNN) with weights W={W1,···,WL},
whereLis the depth of the network. Let Fbe the space of all such CNN models, then Problem (28) can be
approximated as follows.
min
θmax
fW∈F;∥fW∥c≤1,∥fW∥k≤λ/integraldisplay
fWdPθ−/integraldisplay
fWdPr (29)
The constraint∥f∥c≤1is popularly handled using a penalty on the gradient, ∥∇fW∥(Gulrajani et al.,
2017). The constraint on the RKHS norm, ∥f∥k, is more challenging for an arbitrary neural network.
Thus, we follow the approximations proposed in (Bietti et al., 2019). (Bietti et al., 2019) use the result
derived in (Bietti & Mairal, 2017) that constructs a kernel whose RKHS contains a CNN, ¯f, with the same
architecture and parameters as fbut with activations that are smooth approximations of ReLU. With this
approximation, (Bietti et al., 2019) shows tractable bounds on the RKHS norm. We consider their upper
bound based on spectral normalization of the weights in fW. With this, Problem (29) can be approximated
with the following.
min
θmax
fW∈F/integraldisplay
fWdPθ−/integraldisplay
fWdPr+ρ1∥∇fW∥+ρ2L/summationdisplay
i=11
λ∥Wi∥2
sp, (30)
where∥.∥spdenotes the spectral norm and ρ1,ρ2>0. Formulations like (30) have been successfully applied
as variants of Spectral Normalized GAN (SN-GAN). This shows the utility of MMD-regularized UOT in
generative modelling.
B.18 Comparison with WAE
The OT problem in WAE (RHS in Theorem 1 in (Tolstikhin et al., 2018)) using our notation is:
min
π∈R+
1(X×Z )/integraldisplay
c(x,G(z))dπ(x,z),s.t.π1=PX, π2=PZ, (31)
whereX,Zare the input and latent spaces, Gis the decoder, and PX,PZare the probability measures
corresponding to the underlying distribution generating the given training set and the latent prior (e.g.,
Gaussian).
(Tolstikhin et al., 2018) employs a one-sided regularization. More specifically, (Tolstikhin et al., 2018, eqn.
(4)) in our notation is:
min
π∈R+
1(X×Z )/integraldisplay
c(x,G(z))dπ(x,z) +λ2MMDk(π2,PZ),s.t.π1=PX. (32)
39Published in Transactions on Machine Learning Research (01/2024)
However, in our work, the proposed MMD-UOT formulation corresponding to (31) reads as:
min
π∈R+
1(X×Z )/integraldisplay
c(x,G(z))dπ(x,z) +λ1MMDk(π1,PX) +λ2MMDk(π2,PZ). (33)
It is easy to see that the WAE formulation (32) is a special case of our MMD-UOT formulation (33). Indeed,
asλ1→∞, both formulations are the same.
The theoretical advantages of MMD-UOT over WAE are that MMD-UOT induces a new family of metrics
and can be efficiently estimated from samples at a rate O(1√m)whereas WAE is not expected to induce
a metric as the symmetry is broken. Also, WAE is expected to be cursed with dimensions in terms of
estimation, as a marginal is exactly matched, similar to unregularized OT.
We now present the details of estimating (33) in the context of VAEs. The transport plan πis factorized as
π(x,z)≡π1(x)π(z|x), whereπ(z|x)is the encoder. For the sake of fair comparison, we choose this encoder
and the decoder, G, to be exactly the same as that in (Tolstikhin et al., 2018). Since π1(x)is not modelled by
WAE, we fall back to the default parametrization in our paper of distributions supported over the training
points. More specifically, if D={x1,...,xm}is the training set (sampled from PX), then our formulation
reads as:
min
π(z|x),α∈∆mm/summationdisplay
i=1αi/integraldisplay
c(xi,G(z))dπ(z|xi) +λ1MMD2
k/parenleftbigg
α,1
m1/parenrightbigg
+λ2MMD2
k/parenleftiggm/summationdisplay
i=1αiπ(z|xi),PZ/parenrightigg
,(34)
whereGis the gram-matrix over the training set D. We solve (34) using SGD, where the block over the α
variables can employ accelerated gradient steps.
C Experimental Details and Additional Results
We present more experimental details and additional results in this section. We have followed standard
practices to ensure reproducibility. We will open-source the codes to reproduce all our experiments upon
acceptance of the paper.
C.1 Synthetic Experiments
We present more details for the experiments in Section 5.1, along with additional experimental results.
Transport Plan and Barycenter We use squared-Euclidean cost as the ground metric. We take points
[1,2,···,50]and consider Gaussian distribution over them with mean, and standard deviation as (15, 5)
and (35, 3), respectively. The hyperparameters for MMD-UOT are λas 100 and σ2in the RBF kernel
(k(x,y) = exp/parenleftig
−∥x−y∥2
2σ2/parenrightig
) as 1. The hyperparameters for ϵKL-UOT are λandϵas 1.
For the barycenter experiment, we take points [1,2,···,100]and consider Gaussian distribution over them
with mean, and standard deviation as (20, 5) and (60, 8), respectively. The hyperparameters for MMD-UOT
areλas 100 and σ2in the RBF kernel as 10. The hyperparameters for ϵKL-UOT are λas 100 and ϵas
10−3.
Visualizing the Level Sets For all OT variants squared-Euclidean is used as a ground metric. For the
level set with MMD, RBF kernel is used with σ2as 3. For MMD-UOT, λis 1 and RBF kernel is used with
σ2as 1. For plotting the level set contours, 20 lines are used for all methods.
Computation Time The source and target measures are Uniform distributions from which we sample
5,000 points. The dimensionality of the data is 5. The experiment is done with hyper-parameters as
squared-Euclidean distance, squared-MMD regularization with RBF kernel, sigma as 1 and lambda as 0.1.
ϵKL-UOT’s entropic regularization coefficient is 0.01, and lambda is 1. We choose entropic regularization
coefficient from the set {1e−3,1e−2,1e−1}and lambda from the set {1e−2,1e−1,1}. This hyper-
parameter resulted in the fastest convergence. This experiment was done on an NVIDIA-RTX 2080 GPU.
40Published in Transactions on Machine Learning Research (01/2024)
𝜖𝜖
Figure 4: Computation time: Convergence plots with m= 5000 for the case of the same source and
target measures where the optimal objective is expected to be 0. Left: MMD-UOT Problem (10) solved
with accelerated projected gradient descent. Right: ϵKL-UOT’s convergence plot is shown separately. We
observe that ϵKL-UOT’s objective plateaus in 0.3 seconds. We note that our convergence to the optimal
objective is faster than that of ϵKL-UOT.
Figure 5: Sample efficiency: Log-log plot of optimal objective vs number of samples. The optimal objective
values of MMD-UOT and ϵKL-UOT formulation are shown as the number of samples increases. The data
lies in 10 dimensions, and the source and target measures are both Uniform. MMD-UOT can be seen to
have a better rate of convergence.
Sample Complexity In Theorem 4.10 in the main paper, we proved an attractive sample complexity of
O/parenleftig
m−1
2/parenrightig
for our sample-based estimators. In this section, we present a synthetic experiment to show that
the convergence of MMD-UOT’s metric towards the true value is faster than that of ϵKL-UOT. We sample
10-dimensional sources and target samples from Uniform sources and target marginals, respectively. As the
marginals are equal, the metrics over measures should converge to 0 as the number of samples increases.
We repeat the experiment with an increasing number of samples. We use squared-Euclidean cost. For
ϵKL-UOT,λ= 1,ϵ= 1e−2. For MMD-UOT, λ= 1and RBF kernel with σ= 1is used. In Figure 5,
we plot MMD-UOT’s objective and the square root of the ϵKL-UOT objective on increasing the number of
samples. It can be seen from the plot that the MMD-UOT achieves a better rate of convergence compared
toϵKL-UOT.
EffectofRegularization InFigures7and6, wevisualizematchingthemarginalsofMMD-UOT’soptimal
transport plan. We show the results with both RBF kernel k(x,y) = exp/parenleftig
−∥x−y∥2
2∗10−6/parenrightig
and the IMQ kernel
k(x,y) =/parenleftbig
10−6+∥x−y∥2/parenrightbig−0.5. As we increase λ, the matching becomes better for unnormalized measures,
and the marginals exactly match the given measures when the measures are normalized. We have also shown
the unbalanced case results with KL-UOT. As the POT library (Flamary et al., 2021) doesn’t allow including
a simplex constraint for KL-UOT, we do not show this.
41Published in Transactions on Machine Learning Research (01/2024)
                MMD-UOT with RBF kernel                MMD-UOT with IMQ kernel
                KL-UOT
Figure 6: (With unnormalized measures) Visualizing the marginals of transport plans learnt by MMD-UOT
and KL-UOT, on increasing λ.
                MMD-UOT with IMQ kernel
                MMD-UOT with RBF kernel
Figure 7: (With normalized measures) Visualizing the marginals of MMD-UOT (solved with simplex con-
straints) plan on increasing λ. We do not show KL-UOT here as the Sinkhorn algorithm for solving KL-UOT
in the POT library (Flamary et al., 2021) does not incorporate the Simplex constraints on the transport
plan.
C.2 Two-sample Test
Following (Liu et al., 2020), we repeat the experiment 10 times, and in each trial, we randomly sample a
validation subset and a test subset of size Nfrom the given real and fake MNIST datasets. We run the
two-sample test experiment for type-II error on the test set for a given trial using the hyperparameters
chosen for that trial. The hyperparameters were tuned for N= 100for each trial. The hyperparameters
42Published in Transactions on Machine Learning Research (01/2024)
Table 8: Test power (higher is better) for the task of CIFAR-10.1 vs CIFAR 10. The proposed MMD-UOT
method achieves the best results.
ME SCF C2ST-S C2ST-L MMD ϵKL-UOT MMD-UOT
0.588 0.171 0.452 0.529 0.316 0.132 0.643
for a given trial were chosen based on the average empirical test power (higher is better) over that trial’s
validation dataset.
We use squared-Euclidean distance for MMD-UOT and ϵKL-UOT formulations. RBF kernel, k(x,y) =
exp/parenleftig
−∥x−y∥2
2σ2/parenrightig
, is used for MMD and for MMD-UOT formulation. The hyperparameters are chosen from
the following set. For the MMD-UOT and MMD, σwas chosen from {median, 40, 60, 80, 100} where the
median is the median-heuristic (Gretton et al., 2012). For the MMD-UOT an ϵKL-UOT,λis chosen from
{0.1, 1, 10}. For ϵKL-UOT,ϵwas chosen from {1, 10−1,10−2,10−3,10−4}. Based on validation, σas the
median is chosen for MMD at all trials. For ϵKL-UOT, the best hyperparameters (λ,ϵ)are (10, 0.001) for
trial number 3, (0.1, 0.1) for trial number 10 and (1, 0.1) for the remaining the 8 trials. For MMD-UOT,
the best hyperparameters (λ,σ2)are(0.1,60)for trial number 9 and (1,median2)for the remaining 9 trials.
Additional Results Following (Liu et al., 2020), we consider the task of verifying that the datasets
CIFAR-10 (Krizhevsky, 2009) and CIFAR-10.1 (Recht et al., 2018) are statistically different. We follow the
same experimental setup as given in (Liu et al., 2020). The training is done on 1,000 images from each
dataset, and the test is on 1,031 images. The experiment is repeated 10 times, and the average test power is
compared with the results shown in (Liu et al., 2020) with the popular baselines: ME (Chwialkowski et al.,
2015; Jitkrittum et al., 2016), SCF (Chwialkowski et al., 2015; Jitkrittum et al., 2016), C2ST-S (Lopez-Paz &
Oquab, 2017), C2ST-L (Cheng & Cloninger, 2019). We repeat the experiment following the same setup for
the MMD and ϵKL-UOT baselines. The chosen hyperparameters (λ,ϵ)for the 10 different experimen-
tal runsϵKL-UOT are (0.1,0.1),(1,0.1),(1,0.1),(1,0.01),(1,0.1),(1,0.1),(1,0.1),(0.1,0.1),(1,0.1),(1,0.1)
and (1,0.1). The chosen (λ,σ2)for the 10 different experimental runs of MMD-UOT are
(0.1,median ),(1,60),(10,100),(0.1,80),(0.1,40),(0.1,40),(0.1,40),(1,median ),(0.1,80)and(1,40). Table 8
shows that the proposed MMD-UOT obtains the highest test power.
C.3 Single-Cell RNA sequencing
scRNA-seq helps us understand how the expression profile of the cells changes over stages (Schiebinger
et al., 2019). A population of cells is represented as a measure of the gene expression space, and as they
grow/divide/die, and the measure evolves over time. While scRNA-seq records such a measure at a time
stamp, it does so by destroying the cells (Schiebinger et al., 2019). Thus, it is impossible to monitor how
the cell population evolves continuously over time. In fact, only a few measurements at discrete timesteps
are generally taken due to the cost involved.
We perform experiments on the Embryoid Body (EB) single-cell dataset (Moon et al., 2019). The Embryoid
Bodydatasetcomprisesdataat5timestepswithsamplesizesas2381,4163,3278,3665and3332,respectively.
The MMD barycenter interpolating between measures s0,t0has the closed form solution as1
2(s0+t0). For
evaluating the performance at timestep ti, we select the hyperparameters based on the task of predicting for
{t1,t2,t3}\ti. We use IMQ kernel k(x,y) =/parenleftig
1+∥x−y∥2
K2/parenrightig−0.5
. Theλhyperparameter for the validation of
MMD-UOT is chosen from {0.1,1,10}andK2is chosen from{1e−4,1e−3,1e−2,1e−1,median}, where
median denotes the median of { 0.5∥x−y∥2∀x,y∈Ds.t.x̸=y} over the training dataset ( D). The chosen
(λ,K2)for timesteps t1,t2,t3are (1, 0.1), (1, median) and (1, median), respectively. The λhyperparameter
for the validation of ϵKL-UOT is chosen from {0.1,1,10}andϵis chosen from{1e−5,1e−4,1e−3,1e−
2,1e−1}. The chosen (λ,ϵ)for timesteps t1,t2,t3are (10, 0.01), (1, 0.1) and (1, 0.1) respectively. In Table 9,
we compare against additional OT-based methods ¯W1,¯W2,ϵOT.
43Published in Transactions on Machine Learning Research (01/2024)
Table 9: Additional OT-based baselines for two-sample test: Average Test Power (between 0 and 1; higher
is better) on MNIST. MMD-UOT obtains the highest average test power at all timesteps even with the
additional baselines.
N ¯W1¯W2ϵOT MMD-UOT
100 0.111 0.099 0.108 0.154
200 0.232 0.207 0.191 0.333
300 0.339 0.309 0.244 0.588
400 0.482 0.452 0.318 0.762
500 0.596 0.557 0.356 0.873
1000 0.805 0.773 0.508 0.909
Figure 8: (Best viewed in color) The t-SNE plots of the source and target embeddings learnt for the M-
MNIST to USPS domain adaptation task. Different cluster colors imply different classes. The quality of
the learnt representations can be judged based on the separation between clusters. The clusters obtained by
MMD-UOT seem better separated (for example, the red and the cyan-colored clusters).
C.4 Domain Adaptation in JUMBOT framework
The experiments are performed with the same seed as used by JUMBOT. For the experiment on the Digits
dataset, the chosen hyper-parameters for MMD-UOT are K2in the IMQ kernel k(x,y) =/parenleftig
1+∥x−y∥2
K2/parenrightig−0.5
as10−2andλas 100. In Figure 8, we also compare the t-SNE plot of the embeddings learnt with the MMD-
UOT andϵKL-UOT-based loss. The clusters formed with the proposed MMD-UOT seem better separated
(for example, the red and the cyan-colored clusters). For the experiment on the Office-Home dataset, the
chosen hyperparameters for MMD-UOT are/parenleftig
λ= 100, IMQ kernel with K2= 0.1/parenrightig
. For the VisDA-2017
dataset, the chosen hyperparameters for MMD-UOT are/parenleftig
λ= 1, IMQ kernel with K2as 10/parenrightig
.
For the validation phase on the Digits and the Office-Home datasets, we choose λfrom the set{1,10,100}
andK2from the set{0.01,0.1,10,100,median}. For the validation phase on VisDA, we choose λfrom the
set{1,10,100}andK2from the set{0.1,10,100}.
44Published in Transactions on Machine Learning Research (01/2024)
   PLOT
      Proposed MMD-UOT-based Prompt LearningOriginal image
Figure 9: The attention maps corresponding to each of the four prompts for the baseline (PLOT) and
the proposed method. The prompts learnt using the proposed MMD-UOT capture diverse attributes for
identifying the cat (Oxford-Pets dataset): lower body, upper body, image background and the area near the
mouth.
Original image   PLOT
      Proposed MMD-UOT-based Prompt Learning
Figure 10: The attention maps corresponding to each of the 4 prompts for the baseline (PLOT) and the
proposed method. The prompts learnt using the proposed MMD-UOT capture diverse attributes for identi-
fying the dog (Oxford-Pets dataset): the forehead and the nose, the right portion of the face, the head along
with the left portion of the face, and the ear.
C.5 Prompt Learning
LetF={fm|M
m=1}denote the set of visual features for a given image and Gr={gn|N
n=1}denote the set of
textual prompt features for class r. Following the setup in the PLOT baseline, an OT distance is computed
between empirical measures over 49 image features and 4 textual prompt features, taking cosine similarity
cost. LetdOT(x,r)denote the OT distance between the visual features of image xand prompt features of
classr. The prediction probability is given by p(y=r|x) =exp ((1−dOT(x,r)/τ))/summationtextT
r=1exp ((1−dOT(x,r)/τ)),whereTdenotes the
total no. of classes and τis the temperature of softmax. The textual prompt embeddings are then optimized
with the cross-entropy loss. Additional results on Oxford-Pets (Parkhi et al., 2012) and UCF101 (Soomro
et al., 2012) datasets are shown in Table 11.
45Published in Transactions on Machine Learning Research (01/2024)
Table 10: Hyperparameters (kernel type, kernel hyperparameter, λ) for the prompt learning experiment.
Dataset 1 2 4 8 16
EuroSAT (imq2, 10−3,500) (imq1, 104,103) (imq1, 10−2, 500) (imq1, 104,500) (rbf, 1, 500)
DTD (imq1, 10−2, 10) (rbf, 100, 100) (imq2, 10−2, 10) (rbf, 10−2,10) (rbf, 0.1, 1)
Oxford-Pets (imq2, 0.01, 500) (rbf, 10−3,10) (imq, 1, 10) (imq1, 0.1, 10) (imq1, 0.01, 1)
UCF101 (rbf, 1, 100) (imq2, 10, 100) (rbf, 0.01, 1000) (rbf, 10−4, 10) (rbf, 100, 103)
Table 11: Additional Prompt Learning results. Average and standard deviation (over 3 runs) of accuracy
(higher is better) on the k-shot classification task, shown for different values of shots ( k) in the state-of-
the-art PLOT framework. The proposed method replaces OT with MMD-UOT in PLOT, keeping all other
hyperparameters the same. The results of PLOT are taken from their paper (Chen et al., 2023).
Dataset Method 1 2 4 8 16
EuroSATPLOT 54.05 ±5.95 64.21±1.9072.36±2.2978.15±2.65 82.23±0.91
Proposed 58.47±1.37 66.0±0.93 71.97±2.2179.03±1.91 83.23±0.24
DTDPLOT 46.55 ±2.6251.24±1.9556.03±0.43 61.70±0.35 65.60±0.82
Proposed 47.27±1.46 51.0±1.7156.40±0.73 63.17±0.69 65.90±0.29
Oxford-PetsPLOT 87.49 ±0.57 86.64±0.63 88.63±0.2687.39±0.7487.21±0.40
Proposed 87.60±0.65 87.47±1.04 88.77±0.4687.23±0.3488.27±0.29
UCF101PLOT 64.53±0.7066.83±0.43 69.60±0.67 74.45±0.50 77.26±0.64
Proposed 64.2 ±0.7367.47±0.82 70.87±0.48 74.87±0.33 77.27±0.26
Avg acc.PLOT 63.16 67.23 71.66 75.42 78.08
Proposed 64.38 67.98 72.00 76.08 78.67
Following the PLOT baseline, we use the last-epoch model. The authors empirically found that learning 4
prompts with the PLOT method gave the best results. In our experiments, we keep the number of prompts
and the other neural network hyperparameters fixed. We only choose λand the kernel hyperparameters
for prompt learning using MMD-UOT. For this experiment, we also validate the kernel type. Besides RBF,
we consider two kernels belonging to the IMQ family: k(x,y) =/parenleftig
1+∥x−y∥2
K2/parenrightig−0.5
(referred to as imq1) and
k(x,y) = (K2+∥x−y∥2)−0.5(referred to as imq2). We choose λfrom {10, 100, 500, 1000} and kernel
hyperparameter ( K2orσ2) from { 1e−3,1e−2,1e−1,1,10,1e+ 2,1e+ 3}. The chosen hyperparameters
are included in Table 10.
46