Under review as submission to TMLR
Continual Learning for Long-Tailed Recognition
Anonymous authors
Paper under double-blind review
Abstract
We propose Continual Learning for Long-Tailed Recognition (CLTR), a framework that em-
ploys standard off-the-shelf Continual Learning (CL) methods for addressing Long-Tailed
Recognition (LTR) problems, by first learning the majority classes (Head) followed by learn-
ingoftheminorityclasses(Tail), withoutforgettingthemajority. Toensurethatourmethod
is theoretically sound, we first prove that training a model on long-tailed data leads to
weights similar to training the same learner on the Head classes. This naturally necessitates
another step where the model learns the Tail after the Head in a sequential manner. We
then prove that employing CL can effectively mitigate catastrophic forgetting in this setup
and thus improve the model’s performance in addressing LTR. We evaluate the efficacy
of our approach using several standard CL methods on multiple datasets (CIFAR100-LT,
CIFAR10-LT, ImageNet-LT, and Caltech256), showing that CLTR achieves state-of-the-art
performance on all the benchmarks. Further, we demonstrate the effectiveness of CLTR in
the more challenging task of class-incremental LTR, surpassing the state-of-the-art methods
in this area by notable margins. Lastly, extensive sensitivity analyses and detailed discus-
sions are provided to further explore the underlying mechanisms of CLTR. Our work not
only bridges LTR and CL in a systematic way, but also paves the way for leveraging future
advances in CL methods to more effectively tackle LTR problems.
1 Introduction
Data in real-world scenarios often exhibits long-tailed distributions (Buda et al., 2018; Reed, 2001; Zhang
et al., 2023; Fu et al., 2022), where the number of samples in some classes (Head set) is significantly larger
than the number of samples in other classes (Tail set). This imbalance can lead to sub-optimal performance
in deep learning models. This problem is known as Long-Tailed Recognition (LTR), which can be described
as training a model on highly imbalanced data and attempting to achieve high accuracy on a balanced test
set (Zhang et al., 2023).
Given that the size of the Head set is substantially larger than the Tail set, samples from the Head generally
dominate the loss and determine the gradient. Consequently, samples from the Tail are less impactful,
leading to strong performance in Head classes but a significant decline in the performance of the Tail classes
(Alshammari et al., 2022). Numerous studies have sought to mitigate this issue by balancing training data
through over-sampling the Tail classes (sample-wise balancing) (Chawla et al., 2002; Estabrooks et al., 2004;
Fengetal.,2021). Alternatively, featureextractorshavebeentrainedusingtheHeadsetandadaptedthrough
transfer learning to be used for the Tail classes (Liu et al., 2019; Wang et al., 2017; Zhong et al., 2019; Jamal
et al., 2020). Another approach has been to regularize the loss or gradient selectively, depending on the size
of the class set (loss-wise balancing) (Cao et al., 2019; Cui et al., 2019; Tang et al., 2020). Weight balancing
has been proposed as a method for penalizing excessive weight growth during training, thus forcing per-class
weight norms to maintain more uniform magnitudes (Alshammari et al., 2022). However, both sample-wise
and loss-wise balancing methods lead to increased sensitivity to variations in the tail (Wang et al., 2021c).
It has also been shown that these methods may compromise the representational capability of the deep
features learned by the model (Zhou et al., 2020). To address these issues, multi-stage training has recently
been proposed as a viable approach to this problem (Zhou et al., 2020; Zhang et al., 2022). However, these
solutions often rely on an ensemble of multiple experts or backbones to allow effective training of both
1Under review as submission to TMLR
Head and Tail sets, as the use of a single model for multi-stage training would likely result in catastrophic
forgetting.
To address this, we propose a simple yet novel framework called ContinualLearning for Long- Tailed
Recognition (CLTR), which formulates LTR as a sequential learning problem where the Head classes are
learned first, followed by the Tail classes. In this framework, our method draws on the benefits of CL to
alleviate catastrophic forgetting and retain both Head and Tail information effectively. To ensure that our
approach is theoretically well-grounded, we first prove that training a model on an LTR dataset leads to sim-
ilar weights as training the same model solely on the Head. This naturally leads to the need for an additional
step where the Tail classes are further learned by the model, i.e., a sequential learning of the Head followed
by the Tail. Next, we prove that CL can effectively mitigate catastrophic forgetting in this setup and allow
for effective learning of the Tail without forgetting the Head. We validate our theory and the efficacy of
CLTR using five datasets, MNIST-LT, CIFAR100-LT, CIFAR10-LT, ImageNet-LT, and Caltech256. First,
we use the toy MNIST-LT dataset and show that the actual distance between weight vectors when trained on
either the Head or the entire dataset aligns closely with our theoretical predictions. Next, to further assess
the efficacy of CLTR, we employ a range of CL methods in our framework and evaluate the performance on
LTR benchmarks, namely CIFAR100-LT, CIFAR10-LT, and ImageNet-LT, with varying imbalance factors.
The results indicate that CLTR consistently achieves either the best or second-best performance across all
benchmarks, affirming its viability as a long-tailed classifier. We then compare the performance of our model
to recent works on Long-Tail class-incremental Learning (LT-CIL), and show that CLTR outperforms the
state-of-the-art methods. Finally, we offer a discussion on the implications of the proposed perspective for
LTR and the limitations of our study.
Our contributions are as follows: ( 1) We propose and prove a theorem that sets an upper bound on the
distance between weights obtained when training a learner on different partitions of an imbalanced dataset,
under the assumption of strong convexity of the loss function. This bound is inversely proportional to the
imbalance factor and proportional to the strong convexity of the loss function. ( 2) Building on this theorem,
we introduce a new approach that employs CL solutions for the LTR problem using a sequential learning
framework. To support this approach, we prove the effectiveness of CLTR in reducing the loss when learning
the Head and Tail sets sequentially. ( 3) We substantiate our method through comprehensive experiments
that show the effectiveness of our CLTR framework in addressing LTR and LT-CIL problems. Our results
indicate that using CLTR leads to state-of-the-art performances in both problem setups.
2 Related Work
Long-Tailed Recognition. Real-world datasets often exhibit imbalanced distributions, with some classes
appearing more frequently than others. Training a model on such imbalanced data can result in poor
performance on the rare classes. LTR addresses this issue by enabling models to perform well on both Head
and Tail classes (Cao et al., 2019). LTR approaches can be broadly categorized into three primary groups:
data distribution re-balancing ,class-balanced losses , andtransfer learning from Head to Tail (Kang et al.,
2019). Data distribution re-balancing techniques include over-sampling the Tail (Chawla et al., 2002; Han
et al., 2005), under-sampling the Head (Drummond et al., 2003), and class-balanced sampling (Shen et al.,
2016; Mahajan et al., 2018). Class-balanced loss approaches modify the loss function to treat each sample
differently, e.g., including class distribution-based loss (Cao et al., 2019; Cui et al., 2019; Huang et al., 2019),
focal loss (Lin et al., 2017), and Bayesian uncertainty (Khan et al., 2019). Additionally, transfer learning
techniques leverage features learned from the Head to improve learning on the Tail (Yin et al., 2019; Liu
etal.,2019). Morerecently, thelimitationsofclassre-balancinghavebeendiscussedandtheBilateral-Branch
Network (BBN) was proposed to improve representation learning (Zhou et al., 2020). This method addresses
thetrainingoftheencoderandclassifierseparatelythroughanovelcumulativelearningstrategythatinitially
focuses on universal patterns before progressively concentrating on the Tail. The RoutIng Diverse Experts
(RIDE) model is introduced to enhance LTR by reducing model variance (Wang et al., 2021c). Finally,
the assumption that the test set distribution is always uniform is challenged and test-agnostic long-tailed
recognition is introduced (Zhang et al., 2022). The authors discuss that self-supervised learning facilitates
universal feature learning, improving performance on test sets with unknown distribution. To this end,
they introduce a new method that trains multiple experts on a long-tailed dataset to manage various class
2Under review as submission to TMLR
distributions and uses self-supervision at test time to combine these experts for unknown class distributions.
Although numerous prior works have addressed LTR, few provide a mathematical analysis of the training
process using imbalanced data (Ye et al., 2021; Francazi et al., 2023). These works demonstrate that the
Head is learned more quickly than the Tail, primarily focusing on the training dynamics. In contrast, our
theoretical analysis studies the convergence point of training within the LTR framework.
As discussed, some LTR solutions fall into the category of multi-stage training (Zhou et al., 2020; Zhang
et al., 2022). Our work here extends this by first presenting a formal framework in which LTR is formulated
as a sequential problem. Along with the theoretical foundations that describe why sequential learning is
particularly well-suited for LTR, we also identify the key factors that influence the success of these methods.
Subsequently, we propose that CL be used as a viable and highly effective solution for LTR. This allows us
to draw from a rich pool of prior work on CL, which unlike existing multi-stage learning solutions to LTR,
use only a single network throughout the training.
Continual Learning. CL addresses the challenge of adapting a deep learning model to new tasks (e.g., new
classes or distributions) while maintaining performance on the previously learned tasks. The main challenge
to address by CL methods is the mitigation of catastrophic forgetting, i.e., forgetting the previous tasks
as the new tasks are learned. CL methods are typically grouped into three categories: expansion-based ,
regularization-based , andmemory-based approaches .Expansion-based CL methods utilize a distinct subset
of parameters for learning each task (Sarwar et al., 2019; Li et al., 2019; Yoon et al., 2020). Regularization-
basedtechniques penalize significant changes in crucial network parameters (relative to previous tasks) by
incorporating a regularization term in the loss function (Saha et al., 2020; 2021; Farajtabar et al., 2020;
Kirkpatrick et al., 2017; Li & Hoiem, 2017). Memory-based approaches employ a replay memory to store a
limited number of samples from previous tasks, which are then used in future training to minimize forgetting
(Riemer et al., 2018; Chaudhry et al., 2019; Shim et al., 2021). FOSTER uses a two-stage paradigm to
dynamically expand and compress modules when learning new tasks (Wang et al., 2022). Task-id Prediction
based on Likelihood Ratio (TPL) is proposed in (Lin et al., 2024a) for class-incremental Learning. This
method utilizes likelihood ratios for task-id prediction by leveraging available replay data and task-specific
models trained within a shared network. It thus overcomes the challenge of task identification in the absence
of explicit task identifiers at test time. More recently, gradient surgery has been employed for addressing
CL where the gradient from the new task is projected to the orthogonal direction of the previously learned
tasks to ensure learning the new task does not impact the previous task (Saha et al., 2020; Saha & Roy,
2023). These methods achieve state-of-the-art performance on CL benchmarks.
Long-Tailed Class-Incremental Learning. Few prior works have attempted to address the problem of
class-incremental learning when the data is heavenly imbalanced. A novel replay method called Partitioning
Reservoir Sampling (PRS) is proposed in (Kim et al., 2020). This method dedicates a sufficient amount of
memory to tail classes in order to avoid catastrophic forgetting in minority classes. In (Liu et al., 2022a),
this problem is addressed in two different setups, ordered and shuffled. In the ordered scenario the number
of samples in each new task is less than in previous tasks, while in the shuffled scenario, the size of classes
is completely random. They propose a two-stage learning method utilizing a learnable weight scaling layer
for reducing the bias due to data imbalance. Finally, in (Liu et al., 2022b), OLTR++ is proposed which
is a unified algorithm that integrates imbalanced classification, few-shot learning, open-set recognition, and
active learning through dynamic meta-embedding and memory association. Note that none of the above
works attempt to employ CL as a solution for LTR scenarios.
3 Proposed Approach
3.1 Training on Long-Tailed Distributions
In this section, we first define the LTR problem and then analyze the behavior of a model when trained on
long-tailed distributions to provide a theoretical basis for our proposed CLTR framework. Let’s consider
the input space to be Rd, where each input is represented by xi, and the label space is {1,...,k}, where
each label is denoted by yi. Let Ddenote the training set containing samples (xi,yi).Dcis a subset of
Dwhere Dc={(xi,yi)∈D∣yi=c}and∣Dc∣represents its cardinality. Without loss of generality, let the
3Under review as submission to TMLR
classes be ordered by their cardinalities such that ∣Di∣≥∣Dj∣for alli<j. Following (Hong et al., 2024),
letDHandDTrepresent the subsets of Dcorresponding to the Head set and Tail set, respectively as
DH={(xi,yi)∈D∶yi≤ck}andDT={(xi,yi)∈D∶yi>ck}, whereckdenotes how many classes belong to
each set. As a result, every class in the Head has more samples than any class in the Tail. The loss function
overDcis defined as L(Dc,θ)=1
∣Dc∣∑∣Dc∣
i=1ℓ((xi,yi),θ), where (xi,yi)∈Dc. Note that ℓ((xi,yi),θ)is the
loss of each individual sample. For brevity, we will henceforth use notations LDc=LDc(θ)=L(Dc,θ)and
ℓ((xi,yi))=ℓ(xi,yi)(θ)=ℓ((xi,yi),θ).
LTR aims to address the challenge of learning from highly imbalanced data. This occurs when the training
dataDcontains more samples in some classes (the Head set DH) and fewer in others (the Tail set DT). The
imbalance factor IFquantifies the severity of this issue in a dataset, defined as:
IF=∣Dcmax∣
∣Dcmin∣, (1)
wherecrepresents the class index, ∣Dc∣denotes the cardinality of each class, cmax=arg max ∣Dc∣, and
cmin=arg min ∣Dc∣, such that Dcmax∈DHandDcmin∈DT. Now we formally define a long-tailed dataset and
the LTR problem in the following:
Definition 3.1. A dataset is deemed long-tailed when∣Dcmax∣≫∣Dcmin∣or, in other words, IF≫1. When
a model is trained on such a dataset and its performance is assessed on a test set where each class chas the
same number of samples (i.e. ∣Dc∣=κfor each class cwithin the test set where κis a constant number), the
problem is referred to as Long-Tailed Recognition .
Assumption 3.2. We initially assume that all head classes are of size ∣DH∣, and all tail classes are of size
∣DT∣, with∣DH∣>>∣DT∣. These assumptions will be relaxed later in Assumption 3.7. The model is a logistic
regression classifier with parameters θtrained with regularized crossed-entropy loss which is a combination
of cross-entropy loss and an additional L2regularization termµ
2∥θ∥2that prevents weights from growing
excessively.
Assumption 3.2 helps simplify the derivation of the following theoretical analysis. However, our framework
shows strong performances even when all the constraints in this Assumption are relaxed, as presented in
Section 4.2. We now introduce Theorem 3.3 demonstrating the relationship between the weights of the model
when it is trained solely on the head as well as on the entire dataset.
Theorem 3.3. Given Assumption 3.2, if a model with parameter vector θis trained in an LTR setting
(Definition 3.1), then,
∥θ∗−θ∗
H∥2≤4δ
µH+µ, (2)
whereθ∗represents the parameter vector obtained after training, θ∗
Hdenotes the parameter vector when the
model is trained solely on the Head set, δis the maximum difference between the loss of the learner using
the entire dataset and the Head set ( ∣L(D)−L(DH)∣≤δ) for any value of θ, andµHandµare the strong
convexity parameters of the loss calculated on the Head set and the entire dataset, respectively.
To prove this theorem, we first introduce Lemma 3.4, which shows that when the difference between two
strongly convex functions is bounded, their minimizers also reside in a bounded neighborhood of each other.
Lemma 3.4. If∣f(x)−g(x)∣≤δand bothf(x)andg(x)are strongly convex, then:
∥xg−xf∥2≤4δ
µf+µg, (3)
wherexgandxfarearg minf(x)andarg ming(x), respectively.
For the full proof of Lemma 3.4, see Appendix A.1.
Proof of Theorem 3.3. The model is trained on the entire dataset Dby minimizing the loss function L
defined as:
L(D)=1
∣D∣⎛
⎝∑
(xi,yi)∈DHℓ((xi,yi))+∑
(xi,yi)∈DTℓ((xi,yi))⎞
⎠, (4)
4Under review as submission to TMLR
Using L(DH)=1
∣DH∣∑(xi,yi)∈DHℓ((xi,yi))andL(DT)=1
∣DT∣∑(xi,yi)∈DTℓ((xi,yi)), we can derive:
L(D)=∣DH∣
∣D∣L(DH)+∣DT∣
∣D∣L(DT). (5)
Now we define γ=∣DH∣
∣D∣. Since ∣D∣=∣DH∣+∣DT∣, we can derive that 1−γ=∣DT∣
∣D∣. Pluggingγinto Eq. 5 yields:
L(D)=γL(DH)+(1−γ)L(DT). (6)
Given that IF=∣DH∣
∣DT∣, henceγ=IF
1+IF, which falls within the range of [0.5,1). Since based on Definition 3.1,
IF≫0in LTR, we can conclude that the value of γapproaches one. Consequently, L(D)approaches L(DH)
for allθvalues. Let δbe defined as the maximum difference of the losses:
∣L(D)−L(DH)∣<δ. (7)
From Eq. 6, it follows that lim
IF→∞δ=0.
Following Assumption 3.2, the loss function can be formulated as:
L(D,θ)=−1
NN
∑
i=1yilog(P(f(θ,xi)))+µ
2∥θ∥2,(xi,yi)∈D. (8)
whereP(.)is the softmax function, f(θ,xi)is the output of the logistic regression with inputs xiand param-
etersθ, andµdenotes the coefficient of the regularization term. The value of this training hyper-parameter
is determined by the user, usually through hyper-parameter tuning, grid search, or similar approaches. This
loss function is employed because it is highly effective for the LTR problem, improving generalizability by re-
ducing overfitting and achieving state-of-the-art performance when dealing with LTR scenarios (Alshammari
et al., 2022). As our model is assumed to be a logistic regression classifier, the Hessian of the cross-entropy
loss,∇2
θLCE(D,θ), is positive semi-definite, where ∇2
θdenotes the Hessian with respect to θ. Adding a
regularization term with a coefficientµ
2results in a positive definite Hessian matrix with a lower bound of µ.
Therefore, the Hessian matrix satisfies ∇2
θL(D,θ)⪰µI. Consequently, the eigenvalues of the Hessian matrix
are bounded below by µ, ensuring that Lis a strongly convex loss function, where µrepresents the extent
of the convexity. From the definition of strong convexity (Sherman et al., 2021), it therefore follows that:
L(x1)≥L(x2)+∇L(x2)T(x1−x2)+µL
2∥x1−x2∥2, (9)
whereµLis the strong convexity parameter. A more detailed discussion of the strong convexity of the
loss function, its properties, and the relevance of the proposed theoretical analysis to the LTR problem is
provided in Appendix B.
Applying Lemma 3.4 to Eqs. 7 and 9 yields:
∥θ∗−θ∗
H∥2≤4δ
µH+µ, (10)
whereθ∗andθ∗
Harearg min Landarg min LH, respectively.
As a result, when the model is trained on a long-tailed dataset, the network parameter θconverges to a point
close to the weights of the model when it was only trained on the Head set θH. It is worth mentioning that
if the same coefficient for the regularization term µis used for both LandLH, the lower bound in Eq. 2
can be further simplified to2δ
µ. To further analyze the training of the model under the LTR scenario, let us
relax the assumption on the loss function and assume that the model is just using cross-entropy loss without
the regularization term. This leads to the following remark:
Remark 3.5. The upper bound of the distance between a learner’s parameters when trained on the entire
dataset, and the parameters of the same learner solely trained on the Head set, can be calculated as:
∥θ∗−θ∗
H∥2≤4δ
λ+λH, (11)
whereλandλHare the minimum eigenvalues of the hessian matrices of L(D)andL(DH), respectively
5Under review as submission to TMLR
Proof of Remark 3.5. We first show through Lemma 3.6 that if the difference between two convex functions
is bounded, then the distance between their minimizers can also be bounded.
Lemma 3.6. If∣f(x)−g(x)∣≤δand bothf(x)andg(x)are strictly convex, then:
∥xf−xg∥2≤4δ
λf+λg, (12)
wherexgandxfarearg minf(x)andarg ming(x), andλfandλgare the minimum eigenvalues of the
hessian matrices of f(x)andg(x), respectively.
The full proof of Lemma 3.6 is provided in Appendix A.2. Since the loss is now unregularized cross-entropy,
the loss function is strictly (but not strongly) convex (i.e. ∇2L(D,θ)≥0.) Hence, by applying Lemma 3.6
and Eq. 7 we conclude Eq. 11, which completes the proof.
To ensure that the upper bound expressed by Remark 3.5 is limited and approaches zero when δ→0, the
minimum eigenvalues of the Hessian of both loss functions should have lower bounds, which is again another
definition of strong convexity and verify our finding in theorem 3.3.
Theorem 3.3 assumes that there is only one Head and one Tail in the dataset, which is not the case in many
real-world datasets. So we are relaxing Assumption 3.2 as follows:
Assumption 3.7. Building upon Assumption 3.2, we modify the distribution of classes. Specifically, we no
longer assume that all classes are of equal size within the Head and Tail sets, respectively. Instead, the model
accommodates a scenario where the number of samples in the Head classes can differ from each other and
the same applies to the Tail, without specifying the relationship in size between ∣DH∣and∣DT∣.
Under the relaxed assumption where the size of the classes within Head and Tail sets differ, these sets can
be each further partitioned into their own distinct Head and Tail subsets. While each individual partition
remains imbalanced, we continue to subdivide them until: (1) ∣Di∣>>∣Dj∣fori<j, and (2) IFDi/≫1for all
partitions Di. In this scenario, there is no long-tailed partition of the data. Theorem 3.8 extends Theorem
3.3 to address this scenario for any number of partitions.
Theorem 3.8. Following Assumption 3.7, we divide the dataset Dintonpartitions. Let a subset of m≤n
partitions be ⋃m
i=1Di⊆D, with the largest partition being Da, i.e.a=arg maxi∣Di∣,i∈[1,m]. Then, the
weightsθ∗
⋃Diobtained from training the model on ⋃m
i=1Diwill always be in a bounded neighborhood of the
weightsθ∗
Daobtained from training on the largest subset Da.
Proof Sketch. We start by dividing the dataset into multiple partitions each substantially larger than the
previous one. We then apply Theorem 3.3 on the two largest subsets and find the upper bound for the weight
differences. We then consider the aggregation of these two subsets as the new ‘largest’ subset and apply
Theorem 3.3 to this ‘largest’ subset and the next largest partition to find a new upper bound. Repetitively
applying Theorem 3.3 allows us to calculate an ultimate upper bound for the weight difference when training
on the largest subset versus the entire dataset. The formal proof is provided in Appendix A.3.
3.2 Continual Learning for Long-Tailed Recognition
Let us assume an LTR problem and a learner with a set of parameters denoted as θ(recall definition 3.1).
Initially, the learner is trained on a highly imbalanced dataset D, as shown in Fig. 1, where θiis the
initialized model in the weight space. Owing to the larger number of Head samples in each iteration, they
dominate the evolution of the gradients (Eq. 5), resulting in a learner that performs significantly better on
the Head set than on the Tail set at the end of training. This process leads the parameters to converge
toθ∗. We showed in theorem 3.3 and 3.8 that under a strongly convex loss function, θ∗lies within a
bounded neighborhood of radius rof the learner’s weights θ∗
Hwhen trained exclusively on the Head set
DH, whereris proportional to the strong convexity of the loss function and inversely proportional to the
imbalance factor. This neighborhood falls in ψHwhich represents an area within the weight space where
the network performs well on the Head set. At this stage, the model should learn the Tail; however, if it
is simply fine-tuned on the Tail ( DT), then it results in moving towards θ∗
TinψTand will likely leave ψH.
6Under review as submission to TMLR
Continual Learning for Long -Tailed Recognition: 
Bridging the Gap in Theory and Practice
Motivation
TheLong -Tailed Recognition (LTR) problem arises inimbalanced datasets .
This paper bridges the theory -practice gap inthis context, providing
mathematical insights intothetraining dynamics ofLTR and proposing a
novel perspective ofusing Continual Learning (CL) foraddressing this
problem .
•Wepropose amathematical insight intotheoptimization dynamics inthe
LTR scenario byestablishing anupper bound onthedistance between
weights obtained when trained onthe fulldataset and the Head .
Furthermore, weextend thistheorem toapply toanynumber ofpartitions
withvarying class sizes .
•Using thisbound asabasis, weintroduce anew perspective onusing CL
solutions fortheLTR problem supported byanother theorem thatproves
theeffectiveness ofCLinreducing theloss when focusing onTail
classes .
•Wesubstantiate ourmethod through comprehensive experiments that
demonstrate theeffectiveness ofCLtechniques inaddressing LTR.Proposed Perspective
This diagram depicts theoverview oftheproposed solution where a
machine learning model istrained onanimbalanced dataset, where the
learner, initially at𝜃𝑖,tends tofavor themajority class, converging toapoint
𝜃∗near 𝜃𝐻∗ (optimal fortheHead butnottheTail).Ourproposed theorem
demonstrates thattheoptimal point 𝜃∗iswithin abounded neighborhood (𝑟)
of𝜃𝐻∗.Employing Continual Learning, wesequentially train onHead and
Tail, steering thelearner towards 𝜓𝐻𝑇,anreaintheweight space where the
model’s performance isbalanced forboth Head andTail.Mahdiyar Molahasani, Ali Etemad, 
Michael Greenspan
Smith School of Engineering and Ingenuity Labs 
Research Institute, Queen’s University, Canada
𝒟𝐻𝒟 
𝒟𝑇 
CL(𝒟𝑇) 
Head Tail𝜃𝐻∗
𝜃 ∗
𝜃𝐻𝑇∗
𝜓𝐻𝑇 𝜓𝑇 𝜓𝐻 𝜃𝑇∗𝜃𝑖 
𝑟 
Training on Long -Tailed Distribution
CL for LTRResults
Numerical Verification
Toverify thepredicted upper bound inTheorem 1,The actual distance
between 𝜃∗ and 𝜃𝐻∗ indifferent 𝐼𝐹and µarecompared with thecalculated
upper bound .The results  confirm  that CL methods  are effective  for LTR,  aligning  with our 
theorems . While  not outperforming  specialized  LTR methods,  CL shows  
significant  improvement  over baselines .
 We then utilize  the Caltech 256 dataset  to evaluate  the performance  of CL 
on a naturally  skewed  dataset  and demonstrate  that CL can outperform  
SOTA  methods .To validate  the efficacy  of CL in LTR,  we apply  five CL strategies  (LwF, 
EWC,  Modified  EWC,  GPM,  and SGP)  on CIFAR 100-LT (Table  1), 
CIFAR 10-LT (Table  2), and ImageNet -LT (Table  3). 
Conclusion
We advanced  a CL-based  approach  for LTR,  grounded  in the following  
three  theorems  that provide  insights  into optimization  dynamics  of models  in 
LTR scenarios : 1) an upper  bound  on weight distances  when  trained  on the 
Head  versus  the entire  dataset,  2) an extension  to multiple  subsets, and 3) 
a proof  that CL yields  lower  loss in LTR scenarios . Our empirical  validation  
on bench  marks  like MNIST -LT, CIFAR 100-LT, CIFAR 10-LT, and 
ImageNet -LT, as well as real-world  data via Caltech 256, corroborates  our 
theoretical  framework . Future  work  will delve  into non-convex loss 
landscapes  and refine  CL methods  for LTR,  aiming  for robust  solutions  in 
imbalanced  settings .
Acknowledgment
Theorem  3
Consider  a logistic  regression  model  with parameters  𝜃trained  using  
regularized  cross -entropy  loss in an LTR setting,  converging  to 𝜃𝑖. Then,  
ℒ𝒟,𝜃𝐸𝑊𝐶𝑖+1<ℒ𝒟,𝜃ℒ𝑖+1, where  𝜃𝐸𝑊𝐶𝑖+1 and 𝜃ℒ𝑖+1 denote  the weights  of the 
model  after a single  update  using  EWC  loss and regularized  cross -entropy  
loss, respectively .Theorem  1
Assume thatalogistic regression model withparameters 𝜃istrained using
regularized cross -entropy loss inanLTR setting .Then, 𝜃∗−𝜃𝐻∗2≤
4𝛿
𝜇𝐻+𝜇 ,where 𝜃∗ represents theparameter vector obtained after training,
𝜃𝐻∗ denotes theparameter vector when themodel istrained solely onthe
Head set, 𝛿 isthemaximum difference between theloss ofthelearner
using theentire dataset ortheHead setforanyvalue of𝜃,and 𝜇𝐻 and
𝜇 arethestrong convexity parameters oftheloss computed oneither the
Head setortheentire dataset .
Theorem  2
Let a logistic  regression  model  with parameters  𝜃be trained  using  
regularized  cross -entropy  loss in an LTR setting,  and let dataset  𝒟 be 
divided  into 𝑛 partitions . Further,  let a subset  of 𝑚<𝑛 partitions  be 
ڂ𝑖=1𝑚𝒟𝑖⊆𝒟, with the largest  partition  being 𝒟𝑎i.e. 𝑎=
argmax
𝑖𝒟𝑖,𝑖∈[1,𝑚].Then,  the weights 𝜃ڂ𝒟𝑖∗ obtained  from training  the 
model  on ڂ𝑖=1𝑚𝒟𝑖 will always  be in a bounded  neighborhood  of the weights  
𝜃𝒟𝑎∗ obtained  from training  on the largest  subset   𝒟𝑎.Inthissection, wederive theconditions inwhich CLcanbeapplied toa
long-tailed scenario byanalyzing theconvergence ofthemodel when
training data ishighly imbalanced .
Inorder toprove theeffectiveness ofemploying CLmethods foraddressing
LTR problems, the116following theorem isproposed .Contributions
Theorem 1
Theorem 2Theorem 3
Figure 1: Overview of learning under the
LTR scenario and our proposed CLTR ap-
proach (symbols described in the text).This phenomenon occurs in sequential learning and it is known
as catastrophic forgetting. To mitigate this problem and guide
the model to the intersection of ψHandψTdenoted as ψHT,
where the model performs well on both Head and Tail, the Tail
should be learned without forgetting the Head. To this end,
we propose using the standard CL methods for sequentially
learning the Tail after the Head while avoiding catastrophic
forgetting (converging to θ∗
HT).
Following (Prabhu et al., 2020), a general CL problem can be
formulated as a model exposed to a stream of Nincoming
trainingdatasets DYt={(xi,yi)∣yi∈Yt}for1≤t≤N, where Yt
is the corresponding set of labels. Up to the current timestep t,
the set of labels ⋃t
i=1Yiin dataset⋃t
i=1DYihas been previously
used in training of the network. The objective at the next
timestept+1is to find a mapping fθ∶x→ythat accurately
maps sample xto⋃t
i=1Yi∪Yt+1, where Yt+1is the set of new
unseen labels in the incoming new dataset DYt+1={(xi,yi)∣yi∈
Yt+1}. Therefore the ultimate objective of CL is to find an accurate mapping fθ∶x→yfor all(x,y)∈
⋃N
i=1DYi.
Considerdataset DundertheLTRsetup(Definition3.1)dividedinto Npartitionswithsubstantiallydifferent
sizes sorted based on cardinality such that D=⋃N
i=1Diand∣Di∣>>∣Di+1∣. We have shown in Theorem 3.3
and Theorem 3.8 that θ∗(the weights of the model when trained on the entire dataset) will be very close
toθ∗
1, which is the weights of the model when it is only trained on D1(the largest partition of the dataset).
As a result, the model after training on Dcan be considered as fθ∗
1∶x→yfor all(x,y)∈D1. On the other
hand, following Definition 3.1, the objective of LTR is to learn fθ∶x→yfor all(x,y)∈D=⋃N
i=1Di. Hence,
additional training steps are required for the model to further learn the rest of the partitions of the dataset
(⋃N
i=2Di). Thus, if we consider each of the partitions of the LTR dataset ( Difor1≤i≤N) as an incoming
CL dataset ( DYtfor1≤t≤N), the objective of the LTR problem would be equivalent to the objective of
CL, which is to estimate fθ:
fθ∶x→y s.t. (x,y)∈N
⋃
t=1DYtandDY1=D1,DY2=D2, ..., DYN=DN. (13)
Thus, our proposed approach unifies the two domains so that an LTR problem can be treated as a CL
problem. Algorithm 1 lays out the detailed procedure of our proposed framework. Due to the higher number
of samples in the Head, we start the sequence by learning the Head, a convention also followed in prior
multi-stage LTR methods (Zhou et al., 2020; Zhang et al., 2022).
Without loss of generality, we prove the effectiveness of employing a standard and simple CL method for
addressing LTR problems in the following theorem. We then extend this notion to other more powerful CL
methods empirically in the next section.
Theorem 3.9. Following Assumption 3.2, for the model trained on imbalance dataset Dforiepochs and
converging to θi, we have
L(D,θi+s
CL)<L(D,θi+s
L), (14)
for alls<S−i, wheresis the number of past training epochs in the second phase of training (training on
Tail),Sis the total number of epochs in both phases of the training, θi+s
CLandθi+s
Ldenote the weights of the
model after snumber of updates using CLand regularized cross-entropy loss, respectively.
Proof Sketch. (Formal proof in Appendix A.4) In this theorem, we use the simple EWC loss to represent CL
in general. It’s important to highlight that while EWC is not the most recent CL technique to be proposed,
it serves as a common baseline for comparison of all other CL methods. Furthermore, the mathematical
formulation of EWC is succinct and is amenable for use within Theorem 3.9. Moreover, we empirically
demonstrate the effectiveness of other and more sophisticated forms of CL for LTR in Section 4.2. We
7Under review as submission to TMLR
Algorithm 1 CLTR
1:Input:imbalanced data D, initialized model parameters θi, number of partitions N
2:Output:θ∗
HT
3:sort(D) in ascending order by cardinality of each class ∣Di∣≥∣Dj∣for alli<j
4:partition (D,L)whereL={l1,l2,...,lN,lN+1}denotes the partition boundaries, l1=0, andlN+1=k
5:initialize CL replay memory M
6:fori=1toNdo
7:Di=⋃li+1
j=liDj
8:end for
9:fort=1toNdo
10:θ∗
t=arg min LCL(θ,Dt,M)# CL training
11:update M
12:end for
13:returnθ∗
N
consider the updated weights after one iteration using both EWC loss and regularized cross-entropy loss. By
employing Taylor expansion, we approximate the losses for the new weights. We then show that the EWC
loss incorporates a regularization term that effectively constrains the weight updates. Leveraging the strong
convexity of the loss function and the positive nature of the Fisher information matrix, we prove that the
loss with EWC-updated weights is strictly less than that with regular cross-entropy updated weights. Note
that the loss on the entire dataset Din Eq. 14 is used as a theoretical upper bound, while this stage of
training is solely performed on the Tail set.
4 Experiments and Results
4.1 Experiment Setup
Datasets. First, weusethe MNIST-LT (LeCunetal.,1998)toydatasetwithdifferent IFvaluesandstrong
convexityparameterstostudythebehavioroftheupperbound(Eq.10)anditscompliancewithourtheorem.
Next, to evaluate the performance of CLTR in addressing the LTR problem, we employ three widely used
LTR datasets: CIFAR100-LT ,CIFAR10-LT (Cao et al., 2019), and ImageNet-LT (Liu et al., 2019).
These datasets represent long-tailed versions of the original CIFAR100, CIFAR10, and ImageNet datasets,
maintaining the same number of classes while the number of samples in each class decreases exponentially
according to the IF, where the first class has the maximum number of samples and the last class contains
the least number of samples, as illustrated in Appendix D. Finally, to further highlight the benefits of using
CLTR, we carry out additional experiments using the naturally skewed Caltech256 dataset (Griffin et al.,
2007).
Implementation Details. Following the experimental setup of (Alshammari et al., 2022) and (Fu et al.,
2022), we use ResNet-32 (He et al., 2016) and ResNeXt-50 (Xie et al., 2017) for CIFAR and ImageNet
benchmarks, respectively. The LTR methods selected for comparison are state-of-the-art solutions in the
area. We also employ various standard and state-of-the-art CL method in CLTR, namely LwF (Li & Hoiem,
2017), EWC (Kirkpatrick et al., 2017), Modified EWC (Molahasani et al., 2023), GPM (Saha et al., 2020),
FOSTER (Wang et al., 2022), SGP (Saha & Roy, 2023), and TPL (Lin et al., 2024a). We divided the dataset
into 2 partitions for LwF, EWC, Modified EWC, GPM, and SGP and 4 partitions for FOSTER and TPL.
All trainings were conducted using an NVIDIA RTX 3090 GPU with 24GB VRAM. More details on the
implementation specifics are provided in Appendix C.
Evaluation. For the LTR datasets (MNIST-LT, CIFAR100-LT, CIFAR10-LT, ImageNet-LT), we first train
the model on the long-tailed imbalanced training set and then evaluate it on the balanced test set, following
the evaluation protocol of (Alshammari et al., 2022). For Caltech256, we use the entire training set for
training and assess the model’s performance on the entire test set, retaining its original distribution. All
8Under review as submission to TMLR
0 20 40 60 80 100
IF0.00.51.01.52.02.53.03.5Distance
=0
=0.01
=0.05
=0.1
=0.5
Figure 2: The distance between θ∗and
θ∗
Hin different IFandµ.
0 50 100 150 200
IF51015Distance=0.01
Actual value
Upper bound
0 50 100 150 200
IF2345Distance=0.1
0 50 100 150 200
IF246Distance=0.05
0 50 100 150 200
IF12Distance=0.5
Figure 3: The actual distance between θ∗andθ∗
Hin different
IFandµcompared with the calculated upper bound.
reported values represent classification accuracy. The results of our proposed approach are highlighted in
the Tables.
4.2 Results
Empirical support for Theorem 3.3. To evaluate the validity of Theorem 3.3 on the upper bound for
the distance between the learner’s weights when trained on DandDH(∥θ∗−θ∗
H∥), we first train a logistic
regression model on MNIST-LT with varying IFandµvalues. Then we calculate the Euclidean distance
between the two sets of weights, as illustrated in Fig. 2. As expected from Eq. 10, increasing either the IF
or strong convexity ( µ) results in a reduced distance, indicating that the weights of the model trained using
Dapproach the weights when it is solely trained using DH. We also compared these actual distances with
the upper bound predicted by Theorem 3.3, as exhibited in Fig. 3. The results show that for all IFandµ
values, the measured distance is lower than the theoretical upper bound, which is aligned with our proposed
theorem. It is important to note that for this experiment, the upper bound is calculated using Eq. 5 in
Appendix A.1 which results in even a tighter neighborhood compared to Eq. 10.
Performance. We compare the performance of our CLTR framework with existing state-of-the-art LTR
solutions on three LTR benchmarks, CIFAR100-LT, CIFAR10-LT, and ImageNet-LT, as presented in Tables
1, 2, and 3. We also present two additional baselines where we train the backbone model on the imbalanced
data, with and without a class-balanced loss term. These results demonstrate that CLTR indeed provides
the best or the second-to-best performance across all benchmarks, as predicted by our proposed theorems.
Following the prior works such as (Alshammari et al., 2022), we avoid direct comparisons with solutions with
“bells and whistles” such as RIDE (Wang et al., 2021c), ACE (Cai et al., 2021), SSD (Li et al., 2021), and
PaCo (Cui et al., 2021), which employ aggressive data augmentations, ensembles learning, multi-expert and
self-supervised pretraining. It is worth mentioning that some previous LTR solutions like BBN (Zhou et al.,
2020) learn the Head and Tail separately in a multi-stage manner. They rely on various techniques to prevent
performance loss on the Head while learning the Tail. However, unlike these methods, our approach only
uses one model through the entire training process, and the results demonstrate that employing standard CL
methods designed to mitigate catastrophic forgetting yields the best performance in the LTR benchmarks.
To further explore the capabilities of our approach in more challenging settings, we compare the performance
of CLTR in addressing the LT-CIL problem with the prior state-of-the-art solutions in the area. Following
the experimental setup in (Liu et al., 2022a; Hou et al., 2019; Douillard et al., 2020), the models are first
trained on the largest 50 classes (Head), then, the remaining classes are learned incrementally in 5 or 10
consecutive tasks (Tail) with an equal number of new classes in each new task, from the largest subset
to the smallest subset of the dataset. We apply our method in this setting on the CIFAR100-LT dataset
and compare its performance with the prior works, as presented in Table 4. The results demonstrate that
9Under review as submission to TMLR
Table 1: LTR benchmarks for CIFAR100-LT.
ModelIF
100 50 10
Baseline (Cui et al., 2019) 38.3 43.9 55.7
Baseline + CB (Cui et al., 2019) 39.6 45.3 58.0
Focal loss (Lin et al., 2017) 38.4 44.3 55.8
Focal+CB (Cui et al., 2019) 39.6 45.2 58.0
τ-norm (Kang et al., 2019) 47.7 52.5 63.8
LDAM-DRW (Cao et al., 2019) 42.0 46.6 58.7
BBN∗(Zhou et al., 2020) 42.6 47.0 59.1
LogitAjust (Menon et al., 2020) 42.0 47.0 57.7
LDAM+SSP (Yang & Xu, 2020) 43.4 47.1 58.9
De-confound (Tang et al., 2020) 44.1 50.3 59.6
SSD (Li et al., 2021) 46.0 50.5 62.3
DiVE (He et al., 2021) 45.4 51.1 62.0
DRO-LT (Samuel & Chechik, 2021) 47.3 57.6 63.4
WD (Alshammari et al., 2022) 46.0 52.7 66.1
WD & Max (Alshammari et al., 2022) 53.457.7 68.7
CLTR (LwF) 45.149.358.7
CLTR (EWC) 44.450.358.8
CLTR (Modified EWC) 45.951.060.7
CLTR (GPM) 48.354.764.7
CLTR (FOSTER) 48.754.463.6
CLTR (SGP) 50.7 58.067.2
CLTR (TPL) 48.454.062.1Table 2: LTR benchmarks for CIFAR10-LT.
ModelIF
100 50
Baseline (Cui et al., 2019) 69.8 75.2
Baseline + CB (Cui et al., 2019) 74.7 79.3
Focal loss(Lin et al., 2017) 70.4 75.3
PG Re-sampling (Cui et al., 2018) 67.1 75.0
3LSSL (Díaz-Rodríguez et al., 2018) 85.2 88.2
Focal+CB(Cui et al., 2019) 74.6 79.3
LDAM-DRW(Cao et al., 2019) 77.0 79.3
BBN∗(Cao et al., 2019) 79.8 82.2
Manifold mixup (Cui et al., 2019) 73.0 78.1
CBA-LDAM (Cui et al., 2019) 80.3 82.2
ELF (LDAM)+DRW (Cui et al., 2019) 78.1 82.4
De-confound (Tang et al., 2020) 80.6 83.6
Hybrid-SC (Wang et al., 2021b) 81.4 85.4
MiSLAS (Zhong et al., 2021) 82.1 85.7
BCL (Zhu et al., 2022) 84.3 87.2
CLTR (LwF) 76.378.6
CLTR (EWC) 75.180.1
CLTR (Modified EWC) 77.881.3
CLTR (GPM) 81.284.8
CLTR (FOSTER) 81.785.9
CLTR (SGP) 83.085.5
CLTR (TPL) 84.7 87.6
Table 3: LTR benchmarks for ImageNet-LT.
ModelTop-1
accuracy
Baseline (Cui et al., 2019) 44.4
Baseline + CB (Cui et al., 2019) 33.2
KD (Hinton et al., 2015) 35.8
Focal (Lin et al., 2017) 30.5
SR Re-sampling (Mahajan et al., 2018) 46.8
OLTR (Liu et al., 2018) 35.6
cRT (Kang et al., 2019) 49.6
τ-norm (Kang et al., 2019) 49.4
LFME (Xiang et al., 2020) 37.5
De-confound (Tang et al., 2020) 51.8
Seasaw Loss (Wang et al., 2021a) 50.4
DiVE (He et al., 2021) 53.1
DisAlign (Zhang et al., 2021) 52.9
WD (Alshammari et al., 2022) 48.6
WD+Max (Alshammari et al., 2022) 53.9
CLTR (LwF) 47.6
CLTR (EWC) 48.9
CLTR (Modified EWC) 49.1
CLTR (GPM) 51.7
CLTR (FOSTER) 52.7
CLTR (SGP) 53.2
CLTR (TPL) 53.9Table 4: The performance of CLTR on Ordered LT-
CIL Benchmark for CIFAR100-LT .
MethodTasks
5 10
EEIL (Castro et al., 2018) 38.5 37.5
EEIL+2sLWS Liu et al. (2022a) 39.0 37.6
LUCIR (Hou et al., 2019) 42.7 42.2
PODNET (Douillard et al., 2020) 44.1 44.0
PODNET+2sLWS (Liu et al., 2022a) 44.4 44.4
LUCIR+2sLWS (Liu et al., 2022a) 45.9 45.7
CLTR (TPL) 48.4 47.3
Table 5: The performance of CLTR on Caltech256.
MethodBackbone
Inc.V4 Res.101
L2−FE(Li et al., 2018) 84.1 85.3
L2(Li et al., 2018) 85.8 87.2
L2−SP(Li et al., 2018) 85.3 87.2
DELTA (Li et al., 2018) 86.8 88.7
GBN (Liu et al., 2021) - 86.9
TransTailor (Liu et al., 2021) - 87.3
CLTR (SGP) 88.6 89.8
CLTR outperforms prior methods in both 5- and 10-task settings by considerable margins of 2.5% and 1.6%,
repsectively.
In LTR benchmarks, datasets are modified to exhibit a skewed distribution of samples among various classes.
However, such imbalanced class distributions are naturally observed in real-world data as well (Alshammari
et al., 2022). To evaluate the efficacy of CL techniques on non-LTR benchmark datasets, we utilize the
Caltech256 dataset (Griffin et al., 2007), which consists of 256 distinct classes representing everyday objects.
The largest class comprises 827 samples, while the smallest class contains only 80 samples, exhibiting an IF
of over 10. Here, we employ the CLTR and compare its performance to the state-of-the-art methods on this
10Under review as submission to TMLR
0 5 10 15 20 25 30
Training Step3040506070Accurcay (%)
1 steps
2 steps
3 steps
4 steps
6 steps12 steps
30 steps
Final Error
Baseline
0 1 2 3 4 5 6
Training Step3040506070Accurcay (%)
20 samples
80 samples100 samples
150 samples
Figure 4: The Error behavior for various (left) number of incremental steps and (right) replay memory ( M)
size.
dataset for objected classification. The results are presented in Table 5. We observe that CL outperforms the
previous method on this dataset, demonstrating the strong potential of using CL in dealing with long-tailed
real-world datasets.
4.3 Discussion
Multiple Incremental Steps. Recall that Theorem 3.8 highlights CLTR’s capability for extending beyond
twoincrementalsteps. Increasingthenumberofpartitionsleadstosmaller IFwithineachpartitionatthecost
of an increase in forgetting. To explore the effect of varying partition numbers on CLTR’s final performance,
we adhere to the experimental protocol outlined in (Liu et al., 2022a). Initially, the model is trained on
the first 60 classes, followed by sequential learning of the remaining classes, divided into different numbers
of partitions. The results of this experiment are presented in Fig. 4 (left). Our results reveal an optimal
value for CLTR (FOSTER) (4 steps), yet the performance margin remains slim even with up to 30 tasks.
This highlights the effectiveness of CL methods employed within CLTR. The optimal value for the number
of incremental steps for each CL algorithm can be found in Appendix C.
Replay Memory. Several CL algorithms incorporate mechanisms to retain partial information from the
previous task, aiming to mitigate catastrophic forgetting. For example, EWC maintains prior model param-
eters along with their Fisher values, whereas both GPM and SGP safeguard the Core Gradient Space of
previous tasks. FOSTER, on the other hand, utilizes a replay memory for this purpose. Within the LTR
context, the presence of a buffer memory doesn’t require additional storage, as access to the full dataset
is already available. Nonetheless, to prevent hindering the model’s capacity to learn Tail distributions, we
deliberately avoid replaying all Head samples when learning the Tail, as evidenced by Eq. 7. Accordingly,
our analysis extends to how the number of Head samples replayed while learning the Tail impacts the model’s
performance, as illustrated in Fig. 4 (right). The replay memory’s size serves as a mediator between for-
getting previous information and worsening class imbalance, e.g. a larger replay memory reduces forgetting
but increases imbalance. Therefore, identifying an optimal balance in this trade-off is crucial. Our results
demonstrate the significance of an appropriate replay memory size; however, there exists a threshold beyond
which additional samples per class do not further improve the performance and the performance levels off.
Backward/Forward Transfer and Catastrophic Forgetting. Prior works discuss three key concepts in
the context of CL: catastrophic forgetting, backward transfer, and forward transfer (Díaz-Rodríguez et al.,
2018). As mentioned earlier, catastrophic forgetting occurs when the performance of a class declines after
retraining. Despite the use of CL methods, which are designed to mitigate this forgetting, a certain degree
of forgetting is still inevitable. Forward transfer is the improvement in performance on a new task after
employing CL, which is the central aim of retraining in CL. Finally, backward transfer is a beneficial side-
effect where retraining on new samples can actually enhance the model’s performance on the previous tasks.
This interesting phenomenon in CL has been extensively discussed in previous works in theory and practice
(Lin et al., 2022). Now, let’s discuss Fig. 5, which presents the difference in per-class accuracy of the best
11Under review as submission to TMLR
apple
aquarium_fish
baby
bear
beaver
bed
bee
beetle
bicycle
bottle
bowl
boy
bridge
bus
butterfly
camel
can
castle
caterpillar
cattle
chair
chimpanzee
clock
cloud
cockroach
couch
crab
crocodile
cup
dinosaur
dolphin
elephant
flatfish
forest
fox
girl
hamster
house
kangaroo
keyboard
lamp
lawn_mower
leopard
lion
lizard
lobster
man
maple_tree
motorcycle
mountain
mouse
mushroom
oak_tree
orange
orchid
otter
palm_tree
pear
pickup_truck
pine_tree
plain
plate
poppy
porcupine
possum
rabbit
raccoon
ray
road
rocket
rose
sea
seal
shark
shrew
skunk
skyscraper
snail
snake
spider
squirrel
streetcar
sunflower
sweet_pepper
table
tank
telephone
television
tiger
tractor
train
trout
tulip
turtle
wardrobe
whale
willow_tree
wolf
woman
worm0.2
0.1
0.00.10.20.30.40.50.6Difference in Accuracy
Catastrophic ForgettingBackward Transfer Forward Transfer
Head classes T ail classesPositive Difference
Negative Difference
Per-Class Differences
Behavior Trend
Figure 5: The difference in per-class accuracy of CLTR (SGP) and the baseline model. /searc
CL method (CLTR (SGP)) versus the baseline network. The analysis is based on CIFAR100-LT with an IF
of 100. The figure is divided into three regions corresponding to the scenarios discussed above: catastrophic
forgetting (bottom), backward transfer (top-left), and forward transfer (top-right). The bottom region in the
figure represents classes that undergo catastrophic forgetting, while the top-right region represents the Tail
samples (with a class index larger than 60), which demonstrate improved performance, or forward transfer.
We observe that using SGP as a CL solution for LTR results in very effective improvements in the per-class
accuracy of the Tail (forward transfer). Interestingly, despite the absence of Head data in the retraining
process, 42 out of 60 Head classes see some level of improvement after the model is exposed to the Tail
samples (backward transfer). This result emphasizes the remarkable potential of CL methods in enhancing
the performance on both new and previous tasks.
Runtime. The inference runtime is identical between CLTR and LTR solutions, due to identical backbones
in both types of methods and the fact that CL does not affect inference. Regarding the training runtime,
when CLTR is used, the data is divided into Head and Tail sets. At each step of the training, only one
partition of data is involved, alongside a replay memory with a limited size. Since the backbone is consistent
among all LTR approaches for each benchmark, the runtime is determined by the amount of data fed to the
model. Dividing the learning into multiple steps and using CL therefore does not impact the total runtime,
nor does it increase the training time significantly.
Limitations. Strong convexity is a key assumption in our theorem, which determines an upper bound
for the distance between the weights of a learner trained on the full dataset and the weights of the same
learner trained solely on the Head. This assumption offers a solid theoretical foundation for our method,
showcasing the feasibility of using CL techniques to address the LTR problem. However, as many deep
learningmodelsinpracticeemploynon-convexlossfunctionsthatpotentiallylimitthetheorem’sapplicability
to specific cases, it is crucial to highlight that our experimental results are not strictly dependent on the
strong convexity condition. In fact, our method exhibits impressive performance even under more relaxed
conditions, indicating its robustness and adaptability.
5 Conclusion and Future Work
In this work, we propose CLTR, a novel framework that uses standard CL techniques to learn the Head
and Tail sets sequentially. To ensure that our proposed solution is theoretically grounded, we first prove
that learning a long-tailed dataset leads to weights similar to the case where the model is solely trained
on the Head. Relying on this finding, we propose CL for learning the Tail sequentially following the Head,
without forgetting the Head. Our experimental results on CIFAR100-LT, CIFAR10-LT, ImageNet-LTR,
and Clatech256 support our theoretical findings and demonstrate the viability of our approach in achieving
state-of-the-artperformancesinallLTRandLT-CILbenchmarks. Futureresearchdirectionsincluderelaxing
some of our theoretical assumptions, and employing few-shot learning alongside CL for addressing LTR.
12Under review as submission to TMLR
References
Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed recognition via weight
balancing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 6897–
6907, 2022.
Mehdi Abbana Bennani, Thang Doan, and Masashi Sugiyama. Generalisation guarantees for continual
learning with orthogonal gradient descent. arXiv preprint arXiv:2006.11942 , 2020.
Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem
in convolutional neural networks. Neural Networks , 106:249–259, 2018.
Jiarui Cai, Yizhou Wang, and Jenq-Neng Hwang. Ace: Ally complementary experts for solving long-tailed
recognition in one-shot. In IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 112–
121, 2021.
Xufeng Cai and Jelena Diakonikolas. Last iterate convergence of incremental methods and applications in
continual learning. arXiv preprint arXiv:2403.06873 , 2024.
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with
label-distribution-aware margin loss. Advances in Neural Information Processing Systems (NeurIPS) , 32,
2019.
Francisco M Castro, Manuel J Marín-Jiménez, Nicolás Guil, Cordelia Schmid, and Karteek Alahari. End-
to-end incremental learning. In European Conference on Computer Vision (ECCV) , pp. 233–248, 2018.
Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong
learning with a-GEM. In International Conference on Learning Representations (ICLR) , 2019.
Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority
over-sampling technique. Journal of Artificial Intelligence Research , 16:321–357, 2002.
JiequanCui, ZhishengZhong, ShuLiu, BeiYu, andJiayaJia. Parametriccontrastivelearning. In IEEE/CVF
International Conference on Computer Vision (ICCV) , pp. 715–724, 2021.
Yin Cui, Yang Song, Chen Sun, Andrew Howard, and Serge Belongie. Large scale fine-grained categorization
and domain-specific transfer learning. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 4109–4118, 2018.
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective
number of samples. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp.
9268–9277, 2019.
Natalia Díaz-Rodríguez, Vincenzo Lomonaco, David Filliat, Davide Maltoni, et al. Don’t forget, there is
more than forgetting: new metrics for continual learning. In Continual Learning Workshop at NeurIPS ,
pp. 1–7, 2018.
Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled
outputs distillation for small-tasks incremental learning. In European Conference on Computer Vision
(ECCV), pp. 86–102, 2020.
Chris Drummond, Robert C Holte, et al. C4. 5, class imbalance, and cost sensitivity: why under-sampling
beats over-sampling. In Workshop on learning from Imbalanced Datasets II , volume 11, pp. 1–8, 2003.
Andrew Estabrooks, Taeho Jo, and Nathalie Japkowicz. A multiple resampling method for learning from
imbalanced data sets. Computational Intelligence , 20(1):18–36, 2004.
Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for continual
learning. In International Conference on Artificial Intelligence and Statistics , pp. 3762–3773, 2020.
13Under review as submission to TMLR
Chengjian Feng, Yujie Zhong, and Weilin Huang. Exploring classification equilibrium in long-tailed object
detection. In IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 3417–3426, 2021.
Emanuele Francazi, Marco Baity-Jesi, and Aurelien Lucchi. A theoretical analysis of the learning dynamics
under class imbalance. International Conference on Machine Learning (ICML) , 2023.
Yu Fu, Liuyu Xiang, Yumna Zahid, Guiguang Ding, Tao Mei, Qiang Shen, and Jungong Han. Long-tailed
visual recognition with deep models: A methodological survey and evaluation. Neurocomputing , 2022.
Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. Technical report , 2007.
Hui Han, Wen-Yuan Wang, and Bing-Huan Mao. Borderline-smote: a new over-sampling method in imbal-
anced data sets learning. In Advances in Intelligent Computing: International Conference on Intelligent
Computing (ICIC) , pp. 878–887, 2005.
Jie Hao, Kaiyi Ji, and Mingrui Liu. Bilevel coreset selection in continual learning: A new formulation and
algorithm. Advances in Neural Information Processing Systems , 36, 2024.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 770–778, 2016.
Yin-Yin He, Jianxin Wu, and Xiu-Shen Wei. Distilling virtual examples for long-tailed recognition. In
IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 235–244, 2021.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015.
Yan Hong, Jianfu Zhang, Zhongyi Sun, and Ke Yan. Proaug: Prototype-based augmentation for long-
tailed image classification. In IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pp. 3035–3039, 2024.
SaihuiHou, XinyuPan, ChenChangeLoy, ZileiWang, andDahuaLin. Learningaunifiedclassifierincremen-
tally via rebalancing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) ,
pp. 831–839, 2019.
Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang. Deep imbalanced learning for face recognition
and attribute prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence , 42(11):2781–
2794, 2019.
Muhammad Abdullah Jamal, Matthew Brown, Ming-Hsuan Yang, Liqiang Wang, and Boqing Gong. Re-
thinking class-balanced methods for long-tailed visual recognition from a domain adaptation perspective.
InIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 7610–7619, 2020.
Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalan-
tidis. Decoupling representation and classifier for long-tailed recognition. In International Conference on
Learning Representations (ICLR) , 2019.
Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance
with uncertainty. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp.
103–112, 2019.
Chris Dongjoo Kim, Jinseo Jeong, and Gunhee Kim. Imbalanced continual learning with partitioning reser-
voir sampling. In European Conference on Computer Vision (ECCV) , pp. 411–428, 2020.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic
forgetting in neural networks. National Academy of Sciences , 114(13):3521–3526, 2017.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-
ment recognition. IEEE, 86(11):2278–2324, 1998.
14Under review as submission to TMLR
Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-tailed visual recognition.
InIEEE/CVF International Conference on Computer Vision (ICCV) , pp. 630–639, 2021.
Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: A continual
structure learning framework for overcoming catastrophic forgetting. In International Conference on
Machine Learning (ICML) , pp. 3925–3934, 2019.
Xingjian Li, Haoyi Xiong, Hanchao Wang, Yuxuan Rao, Liping Liu, and Jun Huan. Delta: Deep learning
transfer using feature map with attention for convolutional networks. In International Conference on
Learning Representations (ICLR) , 2018.
Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 40(12):2935–2947, 2017.
Haowei Lin, Yijia Shao, Weinan Qian, Ningxin Pan, Yiduo Guo, and Bing Liu. Class incremental learning
via likelihood ratio based task prediction. In The Twelfth International Conference on Learning Repre-
sentations (ICLR) , 2024a.
Sen Lin, Li Yang, Deliang Fan, and Junshan Zhang. Beyond not-forgetting: Continual learning with back-
ward knowledge transfer. Advances in Neural Information Processing Systems (NeurIPS) , 35:16165–16177,
2022.
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object
detection. In IEEE International Conference on Computer Vision (ICCV) , pp. 2980–2988, 2017.
WeichenLin, JiaxiangChen, RuominHuang, andHuDing. Aneffectivedynamicgradientcalibrationmethod
for continual learning. arXiv preprint arXiv:2407.20956 , 2024b.
Bingyan Liu, Yifeng Cai, Yao Guo, and Xiangqun Chen. Transtailor: Pruning the pre-trained model for
improved transfer learning. In AAAI Conference on Artificial Intelligence , volume 35, pp. 8627–8634,
2021.
Si Liu, Risheek Garrepalli, Thomas Dietterich, Alan Fern, and Dan Hendrycks. Open category detection
with pac guarantees. In International Conference on Machine Learning (ICML) , pp. 3169–3178, 2018.
Xialei Liu, Yu-Song Hu, Xu-Sheng Cao, Andrew D Bagdanov, Ke Li, and Ming-Ming Cheng. Long-tailed
class incremental learning. In European Conference on Computer Vision (ECCV) , pp. 495–512, 2022a.
Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-
tailed recognition in an open world. In IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition (CVPR) , pp. 2537–2546, 2019.
Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and X Yu Stella. Open long-tailed
recognition in a dynamic world. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022b.
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin
Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In
European Conference on Computer Vision (ECCV) , pp. 181–196, 2018.
Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv
Kumar. Long-tail learning via logit adjustment. In International Conference on Learning Representations
(ICLR), 2020.
Mahdiyar Molahasani, Ali Etemad, and Michael Greenspan. Continual learning for out-of-distribution pedes-
trian detection. In IEEE International Conference on Image Processing (ICIP) , pp. 2685–2689. IEEE,
2023.
PramudithaPereraandVishalMPatel. Learningdeepfeaturesforone-classclassification. IEEE Transactions
on Image Processing , 28(11):5450–5463, 2019.
15Under review as submission to TMLR
Marine Picot, Francisco Messina, Malik Boudiaf, Fabrice Labeau, Ismail Ben Ayed, and Pablo Piantanida.
Adversarial robustness via fisher-rao regularization. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 45(3):2698–2710, 2022.
Ameya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions our
progress in continual learning. In European Conference on Computer Vision (ECCV) , pp. 524–540, 2020.
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly
convex stochastic optimization. arXiv preprint arXiv:1109.5647 , 2011.
William J Reed. The pareto, zipf and other power laws. Economics letters , 74(1):15–19, 2001.
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro.
Learning to learn without forgetting by maximizing transfer and minimizing interference. In International
Conference on Learning Representations (ICLR) , 2018.
Gobinda Saha and Kaushik Roy. Continual learning with scaled gradient projection. arXiv preprint
arXiv:2302.01386 , 2023.
Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient projection memory for continual learning. In Inter-
national Conference on Learning Representations (ICLR) , 2020.
Gobinda Saha, Isha Garg, Aayush Ankit, and Kaushik Roy. Space: Structured compression and sharing of
representational space for continual learning. IEEE Access , 9:150480–150494, 2021.
Dvir Samuel and Gal Chechik. Distributional robustness loss for long-tail learning. In IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV) , pp. 9495–9504, 2021.
Syed Shakib Sarwar, Aayush Ankit, and Kaushik Roy. Incremental learning in deep convolutional neural
networks using partial network sharing. IEEE Access , 8:4615–4628, 2019.
Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convo-
lutional neural networks. In European Conference on Computer Vision (ECCV) , pp. 467–482, 2016.
Uri Sherman, Tomer Koren, and Yishay Mansour. Optimal rates for random order online optimization.
Advances in Neural Information Processing Systems (NeurIPS) , 34:2097–2108, 2021.
Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, and Jongseong Jang. Online
class-incremental continual learning with adversarial shapley value. In AAAI Conference on Artificial
Intelligence , volume 35, pp. 9630–9638, 2021.
Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-tailed classification by keeping the good and
removingthebadmomentumcausaleffect. Advances in Neural Information Processing Systems (NeurIPS) ,
33:1513–1524, 2020.
Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Foster: Feature boosting and compression
for class-incremental learning. In European Conference on Computer Vision (ECCV) , pp. 398–414, 2022.
Jiaqi Wang, Wenwei Zhang, Yuhang Zang, Yuhang Cao, Jiangmiao Pang, Tao Gong, Kai Chen, Ziwei Liu,
Chen Change Loy, and Dahua Lin. Seesaw loss for long-tailed instance segmentation. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 9695–9704, 2021a.
Peng Wang, Kai Han, Xiu-Shen Wei, Lei Zhang, and Lei Wang. Contrastive learning based hybrid networks
forlong-tailedimageclassification. In IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 943–952, 2021b.
XudongWang, LongLian, ZhongqiMiao, ZiweiLiu, andStellaYu. Long-tailedrecognitionbyroutingdiverse
distribution-aware experts. In International Conference on Learning Representations (ICLR) , 2021c.
Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learning to model the tail. Advances in Neural
Information Processing Systems (NeurIPS) , 30, 2017.
16Under review as submission to TMLR
Yichen Wu, Long-Kai Huang, Renzhen Wang, Deyu Meng, and Ying Wei. Meta continual learning revisited:
Implicitly enhancing online hessian approximation via variance reduction. In The Twelfth International
Conference on Learning Representations , 2024.
Liuyu Xiang, Guiguang Ding, and Jungong Han. Learning from multiple experts: Self-paced knowledge
distillation for long-tailed classification. In European Conference on Computer Vision (ECCV) , pp. 247–
263, 2020.
Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations
for deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp.
1492–1500, 2017.
Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced learning. Advances
in Neural Information Processing Systems (NeurIPS) , 33:19290–19301, 2020.
Han-Jia Ye, De-Chuan Zhan, and Wei-Lun Chao. Procrustean training for imbalanced deep learning. In
IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 92–102, 2021.
Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature transfer learning for
face recognition with under-represented data. In IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pp. 5704–5713, 2019.
JaehongYoon, SaehoonKim, EunhoYang, andSungJuHwang. Scalableandorder-robustcontinuallearning
with additive parameter decomposition. In Eighth International Conference on Learning Representations
(ICLR), 2020.
Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. Task-agnostic continual learning using online
variational bayes with fixed-point updates. Neural Computation , 33(11):3139–3177, 2021.
Songyang Zhang, Zeming Li, Shipeng Yan, Xuming He, and Jian Sun. Distribution alignment: A unified
framework for long-tail visual recognition. In IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pp. 2361–2370, 2021.
Yifan Zhang, Bryan Hooi, Lanqing Hong, and Jiashi Feng. Self-supervised aggregation of diverse experts for
test-agnostic long-tailed recognition. Advances in Neural Information Processing Systems (NeurIPS) , 35:
34077–34090, 2022.
Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and Jiashi Feng. Deep long-tailed learning: A
survey.IEEE Transactions on Pattern Analysis and Machine Intelligence , 2023.
Yaoyao Zhong, Weihong Deng, Mei Wang, Jiani Hu, Jianteng Peng, Xunqiang Tao, and Yaohai Huang.
Unequal-training for deep face recognition with long-tailed noisy data. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pp. 7812–7821, 2019.
Zhisheng Zhong, Jiequan Cui, Shu Liu, and Jiaya Jia. Improving calibration for long-tailed recognition. In
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 16489–16498, 2021.
Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with cumulative
learning for long-tailed visual recognition. In IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pp. 9719–9728, 2020.
JianggangZhu,ZhengWang,JingjingChen,Yi-PingPhoebeChen,andYu-GangJiang. Balancedcontrastive
learning for long-tailed visual recognition. In IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pp. 6908–6917, 2022.
17Under review as submission to TMLR
Appendix
A Proofs
A.1 Proof of Lemma 3.4
Proof.Sincef(x)is strongly convex:
f(x2)≥f(x1)+∇f(x1)T(x2−x1)+µf
2∥x2−x1∥2. (1)
Accordingly if x2=xg=arg ming(x)andx1=xf=arg minf(x), then:
f(xg)−f(xf)≥∇f(xf)T(xg−xf)+µf
2∥xg−xf∥2. (2)
Sincexfis the minimizer of f,∇f(xf)=0. Therefore:
f(xg)−f(xf)≥µf
2∥xg−xf∥2. (3)
Similarly, considering g(x), withx1=xg, andx2=xf, we can derive Equation 1 as follows:
g(xf)−g(xg)≥µg
2∥xf−xg∥2. (4)
By adding and rearranging Eqs. 3 and 4, we will have:
(g(xf)−f(xf))+(f(xg)−g(xg))≥(µf+µg)
2∥xg−xf∥2. (5)
Using∣f(x)−g(x)∣≤δ, we can maximize (g(xf)−f(xf))and(f(xg)−g(xg))to obtain:
2δ≥µf+µg
2∥xg−xf∥2. (6)
Hence:
∥xg−xf∥2≤4δ
µf+µg, (7)
which completes the proof.
A.2 Proof of Lemma 3.6
Proof.Using the second-order Taylor series expansion for multivariate functions, we can approximate f(xg)
andg(xf)as follows:
f(xg)≃f(xf)+∇f(xf)(xg−xf)+1
2(xg−xf)⊺Hf(xf)(xg−xf), (8)
g(xf)≃g(xg)+∇g(xg)(xf−xg)+1
2(xf−xg)⊺Hg(xg)(xf−xg), (9)
whereHf(xf)andHg(xg)are the Hessian matrices of fandgevaluated at xfandxg, respectively.
Since∇f(xf)=∇g(xg)=0, by adding Eq. 8 and Eq. 9 together, we obtain:
f(xg)−g(xg)+g(xf)−f(xf)≃1
2(xg−xf)⊺Hf(xf)(xg−xf)+1
2(xf−xg)⊺Hg(xg)(xf−xg),(10)
Using∣f(x)−g(x)∣≤δ, we can maximize (g(xf)−f(xf))and(f(xg)−g(xg)):
2δ≥1
2(xg−xf)⊺Hf(xf)(xg−xf)+1
2(xf−xg)⊺Hg(xg)(xf−xg), (11)
18Under review as submission to TMLR
Letλfandλgbe the minimum eigenvalues of Hf(xf)andHg(xg), respectively. By properties of the
minimum eigenvalues, we can say:
(xg−xf)⊺Hf(xf)(xg−xf)≥λf∥xg−xf∥2, (12)
(xf−xg)⊺Hg(xg)(xf−xg)≥λg∥xf−xg∥2. (13)
Using Eqs. 12 and 13, we can rewrite Eq. 11:
2δ≥1
2λf∥xg−xf∥2+1
2λg∥xf−xg∥2. (14)
Therefore:
∥xf−xg∥2≤4δ
λf+λg, (15)
which completes the proof.
A.3 Proof of Theorem 3.8
Proof.LetDbe a dataset divided into a sequence of partitions D1,D2,...,Dnsuch that the imbalance factor
between any two consecutive partitions DiandDi+1is significantly large, i.e.,∣Di∣
∣Di+1∣≫1.
Consider a random subset of Dsorted from largest to smallest denoted as Da,Db,Dc,...(where ∣Da∣≫
∣Db∣≫∣Dc∣).
FromTheorem3.3, weknowthatiftheimbalancefactorbetweentwopartitionsissignificantlylarge,∣D1∣
∣D2∣≫1,
then the distance between the optimal parameters when trained on D1andD1∪D2is bounded by ζ, i.e.,
∣∣θ∗
D1−θ∗
D1∪D2∣∣2≤ζwhereζis computed using Eq. 9 in the manuscript.
Applying this Theorem to DaandDb, we have:
∣∣θ∗
Da−θ∗
Da∪Db∣∣2≤ζ1
Next, considering the combination of Da∪DbandDc, given that∣Da∪Db∣
∣Dc∣≫1, we deduce:
∣∣θ∗
Da∪Db−θ∗
Da∪Db∪Dc∣∣2≤ζ2
Given that the weights reside in a metric space, and the distances are Euclidean, the triangle inequality
applies. Combining the above inequalities, we therefore get:
∣∣θ∗
Da−θ∗
Da∪Db∪Dc∣∣2≤(√
ζ1+√
ζ2)2
Extending this argument for all partitions, we can conclude:
∣∣θ∗
Da−θ∗
∑Di∣∣2≤(m
∑
i=1√
ζi)2
wheremis the number of subsets selected randomly.
A.4 Proof of Theorem 3.9
Proof.Define the updated weight vector after one iteration over the Tail using EWC loss as:
θi+s
EWC=θi+s−1−η∇LEWC(DT,θi+s−1) (16)
Similarly, for L:
θi+s
L=θi+s−1−η∇L(DT,θi+s−1) (17)
19Under review as submission to TMLR
From the Taylor series expansion, we can estimate the Lof the model with θi+s
EWCoverD:
L(D,θi+s
EWC)≃L(D,θi+s−1)−η∇LEWC(DT,θi+s−1)∇L(D,θi+s−1) (18)
Similarly, for the Lof the model with θi+s
LoverD:
L(D,θi+s−1
L)≃L(D,θi+s−1)−η∇L(DT,θi+s−1)∇L(D,θi+s−1) (19)
Subtracting Eq. 19 from 18, we derive:
L(D,θi+s
EWC)−L(D,θi+s
L)≃η∇L(D,θi+s−1)(∇L(DT,θi+s−1)−∇LEWC(DT,θi+s−1)) (20)
Elastic Weight Consolidation (EWC) loss is expressed as:
LEWC(θi+s−1)=L(θi+s−1)+λ
2∣θ∣
∑
iFi(θi+s−1−θ∗)2(21)
Thus, we can compute ∇LEWC(DT,θi+s−1)as:
∇LEWC(DT,θi+s−1)=∇L(DT,θi+s−1)+λdiag(F)(θi+s−1−θ∗) (22)
Substituting Eq. 22 into Eq. 20, we obtain:
L(D,θi+s
EWC)−L(D,θi+s
L)=−ηλdiag(F)∇L(D,θi+s−1)T(θi+s−1−θ∗) (23)
To determine the sign of ηλdiag(F)∇L(D,θi+s−1)T(θi+s−1−θ∗), we must investigate the sign of each factor.
The values of ηandλare positive by construction. To determine the sign of ∇L(D,θi+s−1)T(θi+s−1−θ∗),
based on the strong convexity of Lwith respect to θiandθ∗, we have:
L(D,θ∗)≥L(D,θi+s−1)+∇L(D,θi+s−1)T(θ∗−θi+s−1)+µL
2∣θi+s−1−θ∗∣2. (24)
Rearranging, we obtain:
∇L(D,θi+s−1)T(θ∗−θi+s−1)≤L(D,θ∗)−L(D,θi+s−1)−µL
2∥θi+s−1−θ∗∥2. (25)
Sinceθ∗minimizes L, the term L(D,θ∗)−L(D,θi+s−1)is always negative. Moreover, −µL
2∥θi−θ∗∥2is also
always negative, leading to:
∇L(D,θi+s−1)T(θ∗−θi+s−1)<0. (26)
Consequently, ∇L(D,θi+s−1)T(θi−θ∗)is positive definite.
Finally, the diag (F)term is determined to be positive valued, according to the following Lemma A.1.
Lemma A.1. Let a logistic regression model be characterized by parameters θand trained using regularized
cross-entropy loss. Then, the diagonal values of its Fisher information matrix (diag(F)) are strictly positive.
The full proof is presented in A.5. Relying on Lemma A.1, we have derived that the sign of
ηλdiag(F)∇L(D,θi+s−1)T(θi−θ∗)is positive, which from Eq. 23 we can conclude:
L(D,θi+s
EWC)−L(D,θi+s
L)<0, (27)
which completes the proof.
20Under review as submission to TMLR
A.5 Proof of Lemma A.1
Proof.The Fisher information matrix is the estimated value of the Hessian of the log-likelihood:
F=E[∇2(−logL(θ))] (28)
In logistic regression, we model the probability of a binary outcome ygiven input xas:
P(y=1∣x;θ)=1
1+e−θTx(29)
whereθis the vector of model parameters. For a dataset {(xi,yi)}N
i=1}, the negative log-likelihood is:
−logL(θ)=N
∑
i=1[−yilog(1
1+e−θTxi)−(1−yi)log(1−1
1+e−θTxi)] (30)
So the Hessian of the negative log-likelihood is:
∇2(−logL(θ))=⎡⎢⎢⎢⎢⎢⎢⎢⎣∂2(−logL)
∂θ2
1⋯∂2(−logL)
∂θ1∂θd
⋮ ⋱ ⋮
∂2(−logL)
∂θd∂θ1⋯∂2(−logL)
∂θ2
d⎤⎥⎥⎥⎥⎥⎥⎥⎦(31)
As a result:
∇2(−logL(θ))=∇2L(θ) (32)
wheredis the dimensionality of θ. Now since the model is logistic regression and loss is regularized cross-
entropy, from Eq. 9, we have:
L(x1)≥L(x2)+∇L(x2)T(x1−x2)+µL
2∥x1−x2∥2, (33)
Which is the condition of strong convexity. As a result:
∇2L≥µLI (34)
From Eq.32 and Eq. 34:
∇2(−logL(θ))=∇2L(θ)≥µI (35)
Hence:
E[∇2(−logL(θ))]≥µI (36)
consequently:
diag(F)>diag(D),whereDii>0,for alli (37)
which completes the proof.
B Strong Convexity of the Loss Function
The assumption of strong convexity has been widely used in prior theoretical analysis of CL algorithms
(Lin et al., 2024b; Wu et al., 2024; Hao et al., 2024; Cai & Diakonikolas, 2024; Zeno et al., 2021; Bennani
et al., 2020). Here we aim to investigate whether the sequence in which training on multiple datasets is
conducted affects the outcome when the loss abides by this assumption. We begin by illustrating how the
loss landscape and its minima are influenced by the training data. We then establish that when training the
model on multiple datasets, altering the sequence of training impacts the convergence point under the strong
convexity condition. This is then followed by simple numerical examples that empirically demonstrate how
different training orders yield different results.
21Under review as submission to TMLR
Following (Picot et al., 2022), we define the empirical loss landscape Eas the change of the loss function
with respect to the change in the parameters of the model ( θ) when training on a particular dataset Di.
Therefore the loss landscape can be formulated as:
EDi(θ)=L(θ,D)∣D=Di=1
∣D∣∣D∣
∑
k=1ℓ((xk,yk),θ)⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪(xk,yk)∈Di. (1)
Hence, the parameters of the model after training can be expressed as:
θ∗
i=arg min
θEDi(θ)=arg min
θ1
∣D∣∣D∣
∑
k=1ℓ((xk,yk),θ)⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪⌟⟨rro⟪⟪⟩r⟪(xk,yk)∈Di. (2)
Since the loss function is assumed to be strongly convex, there will be only one minima (the global minima),
and the convergence of SGD to this point, under proper selection of the learning rate, is guaranteed (Rakhlin
et al., 2011). As a result, regardless of the initialization, for each dataset, the model will converge to its
corresponding global minima in the weight space. Consequently, if the model is trained on a sequence of
different datasets, it will always converge to the global minimum corresponding to the final dataset. So the
order of training can significantly change the outcome. We further demonstrate this phenomenon in the
following simple numerical examples.
We consider a simple dataset D1consisting of two data points {(0,0),(1,1)}with labels {0,1}. We train
a logistic regression model with L2 regularization (following Assumption 3.2) on this dataset which falls
under the strong convexity assumption, following Eq. 8. We use randomly initialized parameters θinitand
compare the corresponding convergence points θ∗as shown in Table A1. This experiment shows that the
model converges to the same point in the weight space (global minimum of the loss landscape) regardless of
the initialization.
Table A1: Convergence point of the model with different initialization parameters.
MetricsInitializations
Exp 1 Exp 2 Exp 3
θinit [1.7640,0.4001 ] [0.4967,−0.1382 ] [−1.5062,−0.5786 ]
θ∗[0.3554,0.3554 ] [0.3554,0.3554 ] [ 0.3554,0.3554 ]
We now introduce the second dataset D2consisting of two data points {(0.5,0.5),(0.7,0.7)}with labels
{0,1}. We train the model using D1andD2in four setups: (1) training only on D1; (2) training only on D2;
(3) training on D1followed by D2; (4) training only on D2followed by D1. We present the results for this
experiment in Table A2, where we observe that changing the order of the training, under the assumption
of strong convexity of the loss function, changes the convergence point in the weight space, with the last
training step being the determining factor.
Table A2: Convergence point of the model when trained on different datasets.
MetricsDatasets
D1 D2 D1→D2 D2→D1
θ∗[0.3554,0.3554 ] [0.2264,0.2264 ] [0.2264,0.2264 ] [0.3554,0.3554 ]
Relying on these results and the above explanation on the loss landscape and convergence points, the concept
of forgetting in a strongly convex setup can be explained as follows. First, the model is trained on D1and
converges to its corresponding unique global minimum ( θ∗
t1=θ1), as the convergence of strongly convex loss
using SGD is guaranteed. Next, starting from θ1, the model is trained on D2. Consequently, the model
will converge to the unique global minimum of the second dataset’s loss landscape ( θ∗
t2=θ2). Since the
loss function is strongly convex, L(D1,θ)has one global minimum which happens in θ1. As a result, the
22Under review as submission to TMLR
loss value in all other points in the weight space is larger than its value at its minimizer θ1. Hence, it can
be concluded that L(D1,θ2)>L(D1,θ1)which means the second step of training increases L(D1,θ). This
increase in the loss of the first dataset in the sequential learning of these two datasets represents catastrophic
forgetting.
C Implementation Details
All our experiments were conducted utilizing the PyTorch framework. We use the original implementation
of Learning without Forgetting (LwF), Elastic Weight Consolidation (EWC), a modified version of EWC,
GradientProjectionMemory(GPM),ScaledGradientProjection(SGP),FOSTER,andTPL.1. Thespecifics
of each algorithm’s implementation are summarized in Table A3. The parameters for each algorithm such as
Learning Rate (LR), Optimizer, Momentum, LR Scheduler, CL Weight, and number of Epochs are detailed.
Table A3: Table A1: Implementation Details of the Considered Algorithms for LTR benchmark.
Algorithm LR Opt. Momentum LR Scheduler CL Loss Weight Epochs Steps
LwF 0.001 SGD 0.9 - 0.01 5 2
EWC 0.01 SGD 0.9 - 10 90 2
Modified EWC 0.01 SGD 0.9 - 1000 90 2
GPM 0.001 SGD 0 Cosine Anneal LR - 100 2
SGP 0.001 SGD 0 Cosine Anneal LR - 150 2
FOSTER phase1 0.1 SGD 0.9 Cosine Anneal LR - 170 4
FOSTER phase2 0.1 SGD 0 Cosine Anneal LR - 130 4
TPL 0.01 SGD 0.8 Cosine Anneal LR - 50 4
Note that for SGP, the algorithm-specific hyperparameters are acquired through grid search as follows:
gpmeps=0.96andgpmeps−inc=0.004.
D Datasets
Fig. A1 illustrates the distribution of samples among different classes and the division of the dataset into
the Head and Tail sections. In the case of CIFAR100-LT with IF=100, the initial partition is configured
such that 5% of the samples fall within the Tail and 95% in the Head section (Classes 60 to 100 are classified
as Tail). For comparison purposes, the rest of the datasets follow a similar partition threshold where 60% of
the classes are assigned to the Head section.
1The code for the algorithms was obtained and modified from various open-source repositories:
https://github.com/ngailapdi/LWF
https://github.com/shivamsaboo17/Overcoming-Catastrophic-forgetting-in-Neural-Networks
https://github.com/MahdiyarMM/Continual-pedestrian-detection
https://github.com/sahagobinda/GPM
https://github.com/sahagobinda/SGP
https://github.com/G-U-N/ECCV22-FOSTER
https://github.com/linhaowei1/TPL
23Under review as submission to TMLR
0 2 4 6 8
class ID0100200300400500#training examplesHead classes T ail classes
100
50
20
10
5
2
(a)
0 20 40 60 80 100
class ID0100200300400500#training examplesHead classes T ail classes
100
50
10 (b)
0 2 4 6 8
class ID010002000300040005000#training examplesHead classes T ail classes
100
50 (c)
0 200 400 600 800 1000
class ID sorted by cardinality025050075010001250#training examplesHead classes T ail classes
(d)
0 50 100 150 200 250
class ID sorted by cardinality200400600800#training examplesHead classes T ail classes (e)
Figure A1: Class cardinality of (a) MNIST-LT, (b) CIAFR100-LT, (c) CIFAR10-LT, (d) ImageNet-LT and
(e) Caltech256
An interesting phenomenon observed when training models on highly imbalanced data is the presence of
artificially large weights in neurons corresponding to the Head classes (Alshammari et al., 2022). The LTR
solution, WD, addresses this problem by penalizing weight growth using weight decay. One way to assess
the network’s ability to handle LTR is by analyzing the bias in per-class weight norms. To this end, we
present the per-class weight norms of the Baseline, WD, and CLTR (SGP) models in Fig. A2.
E Weight Imbalance
0 20 40 60 80 100
Class Index0.000.020.040.060.080.10Weight NormBaseline
WD
SGP
Figure A2: Per-class weight norms of the
baseline, SGP, and WD.The figure reveals a significant imbalance in the weight norms
of the Baseline model, which is naively trained on the imbal-
anced dataset. In contrast, the WD and CLTR (SGP) models
exhibit more uniform weight norms across different classes. In-
terestingly, although CLTR (SGP) starts with the heavily im-
balanced weights of the Baseline model, it converges towards a
more uniform weight distribution without any explicit penalty
on weight growth. Unlike many other CL methods that re-
strict the plasticity of crucial weights, SGP only constrains the
direction of the weight update in the weight space, enabling
the model to converge to a more balanced weight distribution.
This further demonstrates the effectiveness of CL in addressing
LTR problem.
F Imbalanced Binary Classification
In this work, we address the LTR problem, which inherently involves multi-class classification (Zhou et al.,
2020;Zhangetal.,2022). Consequently, binaryclassificationdoesnottypicallyfallundertheLTRframework
and is beyond the primary scope of our study. However, we demonstrate that our proposed method, CLTR,
is also applicable to binary imbalanced learning scenarios. To evaluate CLTR on this setup, we adopt a two-
stage training process. First, we train the model on the Head class using one-class classification inspired by
(Perera & Patel, 2019). Here the model is trained to detect the Head class samples among other unlabeled
samples from an external dataset. Then, the model is trained on the Tail class samples with a replay
24Under review as submission to TMLR
memory storing a few samples from the Head. We conduct these experiments on two datasets, MNIST-LT
and CIFAR10-LT. For the first dataset, we select a random class from the Head and a random class from the
Tail: class4with1,000samples(Head)andclass7with100samples(Tail). Wethentrainalogisticregression
model on the Head, alongside 1,000 unlabeled randomly selected samples from other classes excluding the
Tail as the external data. We then train the model on the Tail using CL. A replay memory with 100 samples
of the Head is also employed in this stage. For CIFAR10-LT, we randomly select 5,000 samples of class 3
(Head) and 500 samples of class 5 (Tail) and train a ResNet-18 model under the same settings. We compare
the performance of the model on the Tail, Head, and the balanced test set against the following models:
BCE as the naive baseline trained on the entire imbalanced dataset using Binary Cross-Entropy loss; BCE
(Balanced loss), where the loss associated with each class is weighted based on its size; and BCE (Balanced
dataset), where the model is trained on the balanced version of the dataset in which the Head is under-
sampled to ensure both classes are of the same size. We use two versions of CLTR for this experiment with
EWC and SGP as the CL method. The results of this experiment are presented in Table A4, demonstrating
that CLTR can effectively improve the overall performance and reduce the performance gap between the
Head and Tail classes in imbalanced binary classification problems.
Table A4: The performance of CLTR on binary imbalanced classification.
ModelMNIST-LT ( IF=10) CIFAR10-LT ( IF=10)
Acc. Minority Acc. Majority Acc. Overall Acc. Minority Acc. Majority Acc. Overall
BCE 0.0 99.9 50.0 10.3 98.3 54.3
BCE (Balanced loss) 91.5 99.2 95.2 14.5 95.9 55.2
BCE (Balanced dataset) 89.4 91.6 90.6 57.5 54.7 56.2
CLTR (EWC) 95.8 95.0 95.4 52.2 68.0 60.1
CLTR (SGP) 96.4 98.1 97.3 58.7 70.5 64.6
25