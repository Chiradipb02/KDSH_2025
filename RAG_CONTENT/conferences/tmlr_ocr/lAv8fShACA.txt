Published in Transactions on Machine Learning Research (07/2022)
Online Coresets for Parametric and Non-Parametric Breg-
man Clustering
Rachit Chhaya rachit_chhaya@daiict.ac.in
DAIICT- Gandhinagar, India
Jayesh Choudhari choudhari.jayesh@alumni.iitgn.ac.in
CUBE, England
Anirban Dasgupta anirbandg@iitgn.ac.in
IIT Gandhinagar, India
Supratim Shit∗supratim.shit@gmail.com
Technion, Israel
Reviewed on OpenReview: Review
Abstract
We present algorithms that create coresets in an online setting for clustering problems based
on a wide subset of Bregman divergences. Notably, our coresets have a small additive error,
similar in magnitude to the gap between expected and empirical loss Bachem et al. (2017a),
and take update time O(d)for every incoming point where dis the dimension of the point.
Our first algorithm gives online coresets of size ˜O(poly(k,d,ϵ,µ ))fork-clusterings according
toanyµ-similarBregmandivergence. Wefurtherextendthisalgorithmtoshowtheexistence
ofnon-parametriccoresets, wherethecoresetsizeisindependentof k, thenumberofclusters,
for the same subclass of Bregman divergences. Our non-parametric coresets also function as
coresets for non-parametric versions of the Bregman clustering like DP-Means. While these
coresetsprovideadditiveerrorguarantees, theyaresignificantlysmallerforhighdimensional
data than the (relative-error) coresets obtained in Bachem et al. (2015) for DP-Means— for
the input of size nour coresets grow as O(logn)while being independent of das opposed
toO(dd)for points in RdBachem et al. (2015). We also present experiments to compare
the performance of our algorithms with other sampling techniques.
1 Introduction
Clustering is a frequently used operation in data processing. A canonical definition of the clustering problem
is via thek-median, in which kpossible centers need to be proposed such that the sum of distances of
every point to its closest center is minimized. There has been a plethora of work, both theoretical and
practical, devoted to finding efficient and provable clustering algorithms in this k-median setting. Most of
this literature is devoted towards dissimilarity measures that are algorithmically easier to handle e.g. the
variousℓpnorms, especially Euclidean.
A mathematically elegant family of dissimilarity measures that have found wide use in statistics are the Breg-
man divergences which include the squared Euclidean distance, the Mahalanobis distance, Kullbeck-Leibler
divergence, Itakuro-Saito dissimilarity and many others. While being mathematically satisfying, the chief
drawback of working with Bregman divergences for clustering is algorithmic— most of these divergences
do not satisfy either symmetry or triangle inequality conditions. Hence, developing efficient clustering al-
gorithms for these divergences has been a much harder problem to tackle. Banerjee et al. (2005) did a
∗Corresponding author
1Published in Transactions on Machine Learning Research (07/2022)
systematic study of the k-median clustering problem under Bregman divergences, and proposed algorithms
that are generalizations of Lloyd’s iterative algorithm for the Euclidean k-means problem. However, scalabil-
ity remains a major issue. Given that there are no theoretical bounds on the quality of the solution obtained
via the Lloyd’s algorithm in the general Bregman setting, a decent solution is often achieved via running
enough iterations as well as by searching over multiple initializations. This is clearly expensive when the
number of data points is large. This problem is further aggravated when the dimension of the input points
is also high and the number of clusters is not known.
Coresets, small summaries of data to enable efficient optimization, have been successfully used in many prob-
lems in computational geometry and more recently in machine learning. A coreset is a judiciously selected
(and reweighted) set of points, often from the input points themselves, such that solving the optimization
problem on the coreset gives a guaranteed approximation to the solution of the optimization problem on the
full data (Lucic et al., 2016).
In this work we explore two specific goals in creating coresets for Bregman divergence based clustering. First,
we wish to create the coresets in an online setting, i.e. the decision about each point should be taken when
the point is first consumed by the algorithm from an online stream. Secondly, we show the existence of a
single coreset that works for all values of k, the number of clusters. We further give an algorithmic version of
it under certain assumption. It is not apriori clear that either of these goals are achievable. Coreset creation
strategies, e.g. (Lucic et al., 2016), often require a rough approximation in order to construct the importance
sampling distribution. This route would seem to preclude taking online decisions.
Yet another issue is the dependence of the coreset size on the number of clusters— k, the number of clusters
can be large, and more importantly, it can be unknown, to be determined only after exploratory analysis
with clustering. When the number of clusters is unknown, even the existence of a coreset of sublinear size
is unclear. Recent work by Huang & Vishnoi (2020) shows that for relative error coresets for Euclidean
k-means, a linear dependence of coreset size on kis both sufficient and inevitable.
In this work, we tackle these questions for Bregman divergences. We develop coresets with small additive
error guarantees . Such results have also been obtained in the Euclidean setting by Bachem et al. (2018a),
and in the online subspace embedding setting by Cohen et al. (2016). We next show that in the case of
non-parametric clustering, there exists a coreset whose size is independent of k, the parameter representing
number of cluster centers. We utilize the sensitivity framework of Feldman & Langberg (2011) jointly with
the barrier functions method of Batson et al. (2012) in order to achieve this. Using an empirical notion of
sensitivity (Baykal et al., 2018) we present an algorithmic version of this result under certain assumptions.
To the best of our knowledge this is the first non-parametric coreset for clustering. A non-parametric coreset
will be useful in problems such as DP-Means clustering (Bachem et al., 2015) and extreme clustering (Kobren
et al., 2017), where number of clusters may not be known apriori. We now formally describe the setup and
list our contributions.
Given A∈Rn×dwhere rows are input points in Rd. Letφbe the mean of A, i.e.,φ=/summationtext
i≤n(ai/n). For a
query Q⊂Rdthe clustering cost on Awith respect to Qis defined as,
fQ(A) =/summationdisplay
i≤nmin
q∈Qfq(ai).
wherefq(ai)is a chosen µ-similar Bregman divergence (Definition 2.2). We present algorithms which return
coreset Cand set of corresponding weights Ωsuch that,∀X⊂Rd,|X|=k,
|fX(C,Ω)−fX(A)|≤ϵ(fX(A) +fφ(A)) (1)
where,fX(C,Ω)is the weighted cost on points C, i.e.,fX(C,Ω) =/summationtext
ci∈CωifX(ci). The weight ωi∈Ω
corresponds to the point ci∈C. For points coming in streaming fashion we use Ai∈Ri×dto represent
the firstipoints that have arrived. Let φidenote the mean point of Ai, i.e.,φi=/summationtext
j≤i(aj/i). Notice that
the additive term fφ(A)can be understood in the following manner—when f(·)is the squared Euclidean
metric, then fφ(A) =/summationtext
i∥ai−φ∥2
2=n×avgi,j∥ai−aj∥2
2.
Our main contributions are as follows,
2Published in Transactions on Machine Learning Research (07/2022)
•We present an online algorithm called BregmanFilter (Algorithm (1) ) which ensures property
equation 1 for any X∈Rk×dwith at least 0.99probability. BregmanFilter takesO(d)both
in update time and working space to return a coreset (C,Ω)forA. The expected size of the
coreset isO/parenleftig
dklogk
ϵ2µ2/parenleftbig
logn+ log/parenleftbig
fφ(A)/parenrightbig
−log/parenleftbig
fφ2(a2)/parenrightbig/parenrightbig/parenrightig
(Theorem 4.6). For the special case of k-
means clustering, this implies a online coreset of size O/parenleftig
dklogk
ϵ2/parenleftbig
logn+log/parenleftbig
fφ(A)/parenrightbig
−log/parenleftbig
fφ2(a2)/parenrightbig/parenrightbig/parenrightig
(Corollary 4.2) .
•In a non-parametric clustering problem, the number of clusters are unknown. We first show the exis-
tence of coreset for non-parametric clustering based on Bregman divergence. Under a mild assump-
tion on the data we also present an algorithmic version, NonParametricFilter (Algorithm (2) ),
which creates a coreset (C,Ω)based on importance sampling for clustering that ensures equation 1,
∀X∈Ri×dwhere 1≤i≤n. The coreset has O/parenleftig
1
ϵ2µ2/parenleftbig
logn+ log/parenleftbig
fφ(A)/parenrightbig
−log/parenleftbig
fφ2(a2)/parenrightbig/parenrightbig/parenrightig
ex-
pected points (Theorem 5.1). This coreset can be used for DP-Means clustering (Theorem 5.12). For
dlog(d)>log(log(n)), the expected coreset size is smaller than O(ddk∗ϵ−2), the current best known
coreset size for DP-means obtained by Bachem et al. (2015), where k∗is the optimal number of cen-
ters for DP-Means clustering. Further for the special case of k-means clustering, the non-parametric
coreset (C,Ω)forAwill have an expected size of O/parenleftig
1
ϵ2/parenleftbig
logn+log/parenleftbig
fφ(A)/parenrightbig
−log/parenleftbig
fφ2(a2)/parenrightbig/parenrightbig/parenrightig
(Corol-
lary 5.2).
•We present experimental results and compare the performance of our coresets with other known
coresetbuildingtechniques. Thecomparisonisdoneonreal-worlddatasetstosupportourtheoretical
claims.
ForAi, the BregmanFilter maintains an online coreset Ciwith corresponding weights Ωi. The coreset
is online in the sense that for every incoming point aithe algorithm either samples or discards the point
before getting the next point. With this we can also ensure an online guarantee, i.e., |fX(Ci)−fX(Ai)|≤
ϵ(fX(Ai)+fφi(Ai)), with constant probability ∀i∈[n], by taking a union bound over all i. Note that this is
a stronger guarantee and in this case the expected sample size has an excess multiplicative factor of O(logn).
Outline: In section 2 we present all the notations and definitions that we use in the rest of the paper. In
section 3 we discuss the previous works related to the results in this paper. In section 4 we present our first
result, which is an online algorithm for building coresets clustering based on µ-similar Bregman divergences.
In the next section 5 we discuss coreset results for non-parametric clustering for same class of divergences.
In section 6 we show a bound on the uniform deviation for the same class of divergences. Finally we present
some experimental results in section 7 on real datasets.
2 Preliminary
Here we define the notation and the common terms that we use in rest of the paper. The set of the first n
natural number is represented by [n]. A bold lower case letter denotes a vector or a point for e.g. a, and a
bold upper case letter denotes a matrix or set of points as defined by the context for e.g. A. Unless it is
stated otherwise, the matrix Ais used to represent npoints each in Rd.aidenotes the ithrow of matrix A
andajdenotes its jthcolumn. We use the notation Aito denote the matrix or a set, formed by the first i
rows or points of Aseen till a time in the streaming setting. Given A, the smallest and the largest absolute
values are defined as ∥A∥min= mini,j|ai,j|and∥A∥max= maxi,j|ai,j|.
In a clustering problem, depending on the type of the input points a function is used from a wide range of
divergence measure called Bregman divergence .
Definition 2.1. Bregman divergence: For any strictly convex, differentiable function Φ :Z→R, the
Bregman divergence with respect to Φ,∀x,y∈Zis,
dΦ(y,x) = Φ( y)−Φ(x)−∇Φ(x)T(y−x)
3Published in Transactions on Machine Learning Research (07/2022)
We also denote fx(y) =dΦ(y,x). Throughout the paper for some set of centers XinRdand point a∈Rd
we consider fX(a)as a cost function based on Bregman divergence. Such Xare also called query set. We
define it as fX(a) = min x∈Xfx(a) = min x∈XdΦ(a,x), wheredΦ(·)is some Bregman divergence as defined
above. If the set of points in Ahave corresponding weights {wa}then∀a∈Awe definefx(a) =wadΦ(a,x).
Unlike squared euclidean distance, not all Bregman divergences follow metric properties. However there is a
wide sub class called µ-similar Bregman divergence which can relate to distance measure that follows metric
properties.
Definition 2.2. A Bregman divergence dΦon domainZis called aµ-similar Bregman divergence for some
µ>0iff there exists a positive definite matrix Msuch that, for each x,y∈Z
µdM(y,x)≤dΦ(y,x)≤dM(y,x)
wheredM(y,x) = (y−x)TM(y−x)is the squared Mahalanobis distance.
Going forward, we also denote fM
x(a) =dM(a,x), and hence, we have µfM
x(a)≤fx(a)≤fM
x(a),∀xand
∀a∈A. Due to this we say fx(·)andfM
x(·)areµ-similar. For Euclidean k-means clustering Mis just
an identity matrix and µ= 1. It is known that a large set of Bregman divergences is µ-similar, including
KL-divergence, Itakura-Saito, Relative Entropy, Harmonic etc (Ackermann & Blömer, 2009). In Table 1, we
list the most common µ-similar Bregman divergences, their corresponding Mand theµ. In each case the λ
andνrefer to the minimum and maximum values of all coordinates over all points, i.e. the input is a subset
of[λ,ν]d.
Table 1:µ-similar Bregman divergences (Lucic et al., 2016).
Divergence Domain µ M
Squared-Euclidean Rd1 Id
Mahalanobis N Rd1 N
Exponential-Loss [λ,ν]d⊂Rd
+e−(ν−λ) eν
2Id
Kullback-Leibler [λ,ν]d⊂Rd
+λ
ν1
2λId
Itakura-Saito [λ,ν]d⊂Rd
+λ2
ν21
2λ2Id
Harmonic α(α>0) [λ,ν]d⊂Rd
+λα+2
να+2α(1−α)
2λα+2Id
Norm-Like α(α>2) [λ,ν]d⊂Rd
+λα−2
να−2α(1−α)
2να−2Id
Hellinger-Loss [−ν,ν]d⊂(−1,1)d2(1−ν2)3/22(1−ν)−3/2Id
∗Mahalanobis distance is also a µ-similar Bregman divergence with µ= 1andMis the
inverse of the covariance matrix.
There are two types of clustering, hard and soft clustering for Bregman divergence (Banerjee et al., 2005).
In this work, by the term clustering, we only refer to the hard clustering problem.
Coresets: A coreset Har-Peled & Mazumdar (2004); Agarwal et al. (2005); Badoiu & Clarkson (2003) acts
as a small proxy for the original data in the sense that it can be used in place of the original data for a
given optimization problem in order to obtain a provably accurate approximate solution to the problem. Let
ϵ>0. For a non-negative cost function, say fX(a), where Xis a query and a∈A, a set of subsampled and
appropriately reweighted points (C,Ω)is anϵ-coreset if∀X,
|/summationdisplay
a∈AfX(a)−/summationdisplay
(˜a,ω˜a)∈(C,Ω)ω˜afX(˜a)|≤ϵ/summationdisplay
a∈AfX(a).
Typically, the samples that we will construct will satisfy this condition with a desired probability.
While coresets are typically defined for relative errors, additive error coresets can also be defined similarly.
Forϵ,γ > 0,(C,Ω)is an additive (ϵ,γ)coreset of AifCcontains points from Awith corresponding weights
inΩ, and∀X,|/summationtext
a∈AfX(a)−/summationtext
(˜a,ω˜a)∈(C,Ω)ω˜afX(˜a)|≤ϵ/summationtext
a∈AfX(a) +γ. The coresets that are presented
here satisfy such additive guarantees. For ease of representation, sometime we will just use fX(C)instead
of/summationtext
(˜a,ω˜a)∈(C,Ω)ω˜afX(˜a)andfX(A)instead of/summationtext
a∈AfX(a).
4Published in Transactions on Machine Learning Research (07/2022)
Sensitivity Score: Given an input and an optimization function, a sensitivity score for each input point
measures the importance of the point for that optimization function. For a dataset A, a query spaceXthat
denotes candidate solutions to an optimization problem, and a cost function fX(·), Langberg & Schulman
(2010)definesensitivityscoresasfollows—thesensitivityofapoint aisdefinedas sa= supX∈XfX(a)/summationtext
a′∈AfX(a′).
Note that for all points a,sa∈[0,1], and can be treated as a probability score. The sensitivity based
coresets Langberg & Schulman (2010) are created by sampling points according to these probabilities (or
their upper bounds).
While the above definition is standard, we also define the following variant of sensitivity scores as a useful
tool in our results.
Empirical Sensitivity Score: In certain cases, for the query space X, it will be challenging to compute
a reasonable upper bound to the sensitivity scores. In such cases, we will use empirical sensitivity scores
sa= max X∈YfX(a)/summationtext
a′∈AfX(a′)whereYis a finite set of queries such that Y⊂X.
Online Sensitivity Scores: For inputs coming in a streaming fashion, i.e., the point aiarriving at the
ithinstance and so far the algorithm has received Ai−1; now for a query space Xand a cost function fX(·)
we define the online sensitivity score for every such point aias,sai= supX∈XfX(ai)/summationtext
j≤ifX(aj).
We focus on creating coresets for clustering. Our first results are on k-median clustering, where the query
spaceXsatisfiesX ⊆Rk×d1. One can also define a clustering problem when the number of clusters are
unknown. We call this as Non-Parametric Clustering . We define (C,Ω)to be an (ϵ,γ)-additive error coreset
fornon-parametricclusteringifitssizeisindependentof k(numberofcentres)andensures |fX(C)−fX(A)|≤
ϵfX(A) +γfor allk≤nand for all query X∈Rk×d.
DP-Means: The DP-Means problem, studied in Kulis & Jordan (2012), formalizes the clustering problem
when the number of clusters is unknown. It can be considered to be a specific case of the well-known facility
location problem. Given a dataset A∈Rn×dand a parameter λ >0, the goal of the problem is to find a
k∈(0,n]and an X∈Rk×dthat minimizes the costDP(A,X), which is defined as follows,
costDP(A,X,k) =/summationdisplay
i≤nmin
x∈X∥ai−x∥2+λk.
In this paper we consider an obvious extension of the cost function that depends on the Bregman divergences.
Given a dataset A∈Rn×dand a parameter λ, the goal of the problem is to find X∈Rk×dthat minimizes
thecostDP(A,X), defined as follows,
costDP(A,X) =fX(A) +λk.
Uniform Deviation: Given a distribution D, input set A={ai,...,am}where each aiis an independent
and identically distributed sample from D, a query Xand a function fX(.), theuniform deviation is defined
as/vextendsingle/vextendsingle/vextendsingle/vextendsingleEa∈DfX(a)−1
mfX(A)/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
Definition 2.3 (Pseudo-dimension Haussler (1992)) .For a function family Fmapping from an arbitrary
input spaceXtoR≥0and a distribution PonX, the pseudo-dimension of F, denoted by Pdim (F), is the
largestdsuch there is a sequence x1,...,xdof domain elements from Xand a sequence r1,...,rdof reals
such that for each b1,...,bd∈{above,below}, there is an f∈Fsuch that for all i= 1,...,d, we have
f(xi)≥riif and only if bi=above.
1Certain Bregman divergences have non-negativity constraints on the input.
5Published in Transactions on Machine Learning Research (07/2022)
3 Related Work
The initial coresets were used for making the computational geometric algorithms more efficient Badoiu
& Clarkson (2003), as well as to improve the running times for various clustering problems Har-Peled
& Mazumdar (2004); Agarwal et al. (2005). Since then there has been a significant amount of work on
coresets. Interested readers can look at (Woodruff et al., 2014; Bachem et al., 2017b) and the references
therein. Using sensitivities to construct coresets was introduced in (Langberg & Schulman, 2010) and further
generalized by (Feldman & Langberg, 2011). Coresets for clustering problems such as k-means clustering has
been extensively studied (Har-Peled & Mazumdar, 2004; Cohen et al., 2015; Braverman et al., 2016; Feldman
et al., 2016; Bachem et al., 2018a;b; Barger & Feldman, 2020; Feldman et al., 2020). In (Cohen et al., 2015)
the authors reduce the k-means problem to a constrained low rank approximation problem. They show that
a constant factor approximation can be achieved by just O(ϵ−2logk)size coreset and for (1±ϵ)relative
error approximation they get coreset of size O(kϵ−2). In (Barger & Feldman, 2020; Feldman et al., 2016),
the authors discuss a deterministic algorithm for creating coresets for clustering problem which ensures a
relative error approximation. The streaming version of (Barger & Feldman, 2020) returns a coreset of size
O(kϵ−2ϵ−2logn)which ensures a (1±ϵlogn)relative error approximation. Feldman et al. (2016) reduce the
problem of k-means clustering to ℓ2frequent item approximation. The streaming version of the algorithm
returns a coreset of size O(k2ϵ−2log2n). In (Bachem et al., 2018b), the authors give an algorithm which
returns a one shot coreset for all ℓpk-clustering problem, where p∈[1,pmax]. Their algorithm creates a grid
over the range [1,pmax]and based on the sensitivity at each grid point the coreset is built. It returns a coreset
of size ˜O(16pmaxdk2)for which it takes ˜O(ndk)ensuring (1±ϵ)relative error approximation. In (Boutsidis &
Magdon-Ismail, 2013; Cohen et al., 2015), authors show that one can use spectral approximation technique
(Batson et al., 2012) for deterministic feature selection for k-means problem. In (Bachem et al., 2018a),
the authors give an algorithm to create a coreset which only takes O(nd)time and returns a coreset of size
O(dkϵ−2logk)at a cost of small additive error approximation. Their algorithm can further be extended for
clustering based Bregman divergences which are µ-similar to squared Mahalanobis distance. In (Lucic et al.,
2016) the authors give algorithm to create such coresets for both hard and soft clustering based on µ-similar
Bregman Divergence. In this paper we present an online algorithm which is returns just a ˜O(dkϵ−2logk)
size coreset with similar guarantees.
There are several online algorithms for k-means clustering (Liberty et al., 2016; Lattanzi & Vassilvitskii,
2017; Bhaskara & Rwanpathirana, 2020). These algorithms do not create coresets, rather focus on giving
online algorithms for the clustering problem. In (Liberty et al., 2016) the authors give an online algorithm
that maintains a set of centers such that the k-means cost on these centers is ˜O(W∗)whereW∗is the optimal
k-means cost. Lattanzi et.al., Lattanzi & Vassilvitskii (2017) improve this result and give a robust algorithm
that can also handle outliers in the dataset. We further explore the relation of these algorithms with our
online coresets empirically in the experimental section.
The coreset building methods can also be analyzed from the point of view of generalization of the resulting
clustering. There are several results on uniform bound deviation for k-means clustering problem (Biau et al.,
2008; Bachem et al., 2017a). The result in Biau et al. (2008) shows that the difference between the optimal
empirical loss and the optimal clustering loss (i.e., the expected excess risk) can be bounded in terms of
the radius of the input data and inverse of the input size. As our main contribution in this paper is about
building strong coresets, we extend the results in Bachem et al. (2017a), which ensure that the difference
between empirical loss and clustering loss for all possible queries are bounded.
We use Theorem 3.2 from Chhaya et.al., Chhaya et al. (2020b), where the authors show that the coreset built
using sensitivity framework has a sampling complexity that only depends on O(S)instead ofO(Slog(S))in
(Braverman et al., 2016), where Sis the sum of sensitivity scores. Our coreset size for clustering based on
µ-similar Bregman divergence only has a dependence of O(1/µ), unlike in (Lucic et al., 2016; Bachem et al.,
2018a) where the dependence is O(1/µ2).
6Published in Transactions on Machine Learning Research (07/2022)
Algorithm 1 BregmanFilter
Require: Streaming points ai,i= 1,2,...,n ;r>0
Ensure: (C,Ω)
C0= Ω 0=φ0=∅;S= 0
λ=∥a1∥min;ν=∥a1∥max
whilei≤ndo
λ= min{λ,∥ai∥min};ν= max{ν,∥ai∥max}
Updateµi=λ/ν; Update Mibased on (Table 1)
φi= ((i−1)φi−1+ai)/i;S=S+fMiφi(ai)
ifi= 1then
pi= 1
else
li=2fMiφi(ai)
µiS+8
µi(i−1);pi= min{1,rli}
end if
(ci,ωi) =/braceleftigg
(ai,1/pi) w. p.pi
(∅,0) else
(Ci,Ωi) = (Ci−1,Ωi−1)∪(ci,ωi)
end while
Return (C,Ω)
4 Online Coresets for Clustering
We consider that the input points are coming in a streaming manner, i.e., each point on arrival is either
selected to be in the coreset or discarded. We present our first algorithm called BregmanFilter . It creates a
coreset in an online manner for clustering based on Bregman divergences. The coreset is built via importance
sampling, which is based on sensitivity framework, i.e., we use sensitivity scores to define the sampling
probabilityofeachpoint. ThealgorithmstartswiththeknowledgeofBregmandivergence dΦ. Itisimportant
to note that for a fixed dΦif the domain of the input changes, then both the parameters Mandµalso change
(Ackermann & Blömer, 2009; Lucic et al., 2016) (table 1). This is exactly what happens in our case, because
we only have access to the stream of input. These parameters are important because they are used to
compute an upper bound on the sensitivity scores of every points.
Overview: Under a fixed µ-similar Bregman divergence, the input to the algorithm is a stream of points
and an user defined parameter rthat depends on ϵand the VC dimension of query space of the problem,
such that it returns (C,Ω)which is an ϵcoreset. Here on arrival of every input point say ai, the algorithm
first updates the smallest and largest absolute values as λandν, such that λ=∥Ai∥minandν=∥Ai∥max.
It also updates the mean φi. Next it computes both the Mahalanobis matrix Mias well asµi. Fortunately,
computing Miandµirequires maintaining only two simple statistic of the data (Table 1). Using these
terms it computes an upper bound for the sensitivity score, which is then used to decide whether aishould
be stored in the coreset. If selected, the point aiis stored with an appropriate weight ωi. At the end of
the stream we get (C,Ω). Note that this algorithm is online in nature because for every point its sampling
decision is taken before looking at the next incoming point. We present BregmanFilter as algorithm 1.
Now we present some supporting lemmas based on which we show the correctness and the corresponding
guarantees of the algorithm. Note that for Aiformed by first idata points. The algorithm BregmanFilter
maintains Miandµi(as per Table 1) such that, µifMix(aj)≤fx(aj)≤fMix(aj),∀xand∀aj∈Ai. Using
this we show a useful observation that is immediate, based on Mandµdefined in Table 1.
Lemma 4.1. For all Bregman divergences in Table 1, for j≤i,µj≥µiandMj⪯Mi.
Proof.At anyithpoint we have λ=∥Ai∥minandν=∥Ai∥max, i.e., the smallest and largest absolute values
inAi. Further we have ∥Aj∥min≥∥Ai∥minand∥Aj∥max≤∥Ai∥maxforj≤i. By using the formula for M
for all Bregman divergences given in Table 1 we have Mj⪯Miandµj≥µito be always true for j≤i.
7Published in Transactions on Machine Learning Research (07/2022)
For any Bergman divergence the mean of a set of points always minimizes the sum of Bergman divergences
between the set of points and any other point. Now recall that for all i∈[n], we useφito represent the
mean of the first ipoints, i.e., φi=/summationtext
j≤iaj/i. Hence we have the following important observation.
Lemma 4.2. For points arriving in streaming manner, ∀i>jwe have,fφi(Ai)≥fφj(Aj).
Proof.We have,
fφi(Ai) =/summationdisplay
r≤jfφi(ar) +/summationdisplay
j<r≤ifφi(ar)(i)
≥/summationdisplay
r≤jfφj(ar) +/summationdisplay
j<r≤ifφi(ar)≥fφj(Aj).
In(i)we use the fact that φjminimizes the sum of Bregman divergence between any point and first j
points.
Due to lemma 4.1 and lemma 4.2, we have the following lemma which is then used to upper bound the online
sensitivity scores with lias defined in BregmanFilter .
Lemma 4.3. For a fixed µ-similar Bregman divergence with streaming inputs, let φi=/summationtext
j≤i(aj/i)andMi
is the p.s.d. Mahalanobis matrix for Ai(as Table 1). If j≤i, thenfMiφi(Ai)≥/summationtext
j≤ifMjφj(aj)
Proof.
fMi
φi(Ai) =/summationdisplay
j≤i(aj−φi)TMi(aj−φi)
(i)
≥(aj−φi)TMi(aj−φi) +/summationdisplay
j≤i−1(aj−φi)TMi−1(aj−φi)
≥(aj−φi)TMi(aj−φi) +/summationdisplay
j≤i−1(aj−φi)TMj(aj−φi)
(ii)
≥(aj−φi)TMi(aj−φi) +/summationdisplay
j≤i−1(aj−φi−1)TMj(aj−φi−1)
≥(aj−φi)TMi(aj−φi) +/summationdisplay
j≤i−1(aj−φj)TMj(aj−φj)
=/summationdisplay
j≤ifMj
φj(aj)
In(i)we used the property that Mi−1⪯Mi. In(ii)we used the fact that for Ai−1, its meanφi−1minimizes
the cost.
As we uselifor building our coreset, the expected coreset size depends on the sum of li’s. In the next lemma
we prove that the scores li’s defined in BregmanFilter upper bound the online sensitivity scores of ai, i.e.,
supX∈Rk×dfX(ai)
fX(Ai−1)+fφi(Ai)and we also show that the sum of li’s is also bounded.
Lemma 4.4. For every incoming points aithelias defined in BregmanFilter , upper bounds the online
sensitivity score. I.e., ∀i∈[n],
sup
X∈Rk×dfX(ai)
fX(Ai−1) +fφi(Ai)≤li (2)
Furthermore,/summationdisplay
i≤nli≤/parenleftig
8 logn+ 4 log/parenleftbig
fM
φ(A)/parenrightbig
−4 log/parenleftbig
fM2
φ2(a2)/parenrightbig/parenrightig
/µ.
8Published in Transactions on Machine Learning Research (07/2022)
Proof.At stepi, let(µi,Mi)be the parameters such that, ∀ai∈Aand∀X,µifMix(aj)≤fx(aj)≤fMix(aj)
andφi=/summationtext
j≤iaj
i. Now for any query X∈Rk×d, each point aj∈Ai−1has some closest point xl∈X. Now
forsuchpair{aj,xl}, usingtheproperty (∥a+b∥)2≤2(∥a∥2+∥b∥2)wehavefMixl(φi)≤2fMixl(aj)+2fMiφi(aj).
Sobytakingintoaccountforallthepointsin Ai−1weget (i−1)fMi
X(φi)≤2/summationtext
aj∈Ai−1(fMi
X(aj)+fMiφi(aj)) =
2fMi
X(Ai−1)+2fMiφi(Ai−1). We use this triangle inequality in the following analysis, which holds ∀X∈Rk×d,
fX(ai)
fX(Ai−1) +fφi(Ai)(i)
≤fMi
X(ai)
fX(Ai−1) +fφi(Ai)
≤2fMiφi(ai) + 2fMi
X(φi)
fX(Ai−1) +fφi(Ai)
≤2fMiφi(ai) +4
i−1fMiφi(Ai−1) +4
i−1fMi
X(Ai−1)
fX(Ai−1) +fφi(Ai)
(ii)
≤2fMiφi(ai) +4
i−1fMiφi(Ai−1) +4
i−1fMi
X(Ai−1)
µi(fMi
X(Ai−1) +fMiφi(Ai))
=2fMiφi(ai) +4
i−1fMiφi(Ai−1)
µi(fMi
X(Ai−1) +fMiφi(Ai))+4
i−1fMi
X(Ai−1)
µi(fMi
X(Ai−1) +fMiφi(Ai))
≤2fMiφi(ai) +4
i−1fMiφi(Ai−1)
µi(fMi
X(Ai−1) +fMiφi(Ai))+4
µi(i−1)
≤2fMiφi(ai)
µifMiφi(Ai)+4
i−1fMiφi(Ai−1)
µifMiφi(Ai)+4
µi(i−1)
(iii)
≤2fMiφi(ai)
µifMiφi(Ai)+8
µi(i−1)
≤2fMiφi(ai)
µi/summationtext
j≤ifMjφj(aj)+8
µi(i−1)
The inequality (i)is due toµisimilarity, i.e., fX(ai)≤fMi
X(ai). Next couple of inequalities are by applying
triangle inequality on the numerator. In the (ii)inequality we use the µisimilarity to get a lower bound on
the denominator term. We get (iii)inequality by upper bounding the second and third term by 4/(µi(i−1)).
By the property of Bregman divergence we know that φi−1=argminxfx(Ai−1)and, so we have fφi(Ai) =
fφi(Ai−1) +fφi(ai)≥fφi−1(Ai−1) +fφi(ai). So by induction we get fφi(Ai)≥/summationtext
j≤ifφj(ai).
As,fMiφi(Ai)≥/summationtext
j≤ifMjφj(aj)which can be maintained as a running sum hence ∀i, the score lican be
computed in just one pass.
Next, in order to upper bound/summationtext
i≤nli, consider the denominator term of lias follows,
/summationdisplay
j≤ifMj
φj(aj) =/summationdisplay
j≤i−1fMj
φj(aj) +fMi
φi(ai)
=/summationdisplay
j≤i−1fMj
φj(aj)/parenleftbigg
1 +fMiφi(ai)
/summationtext
j≤i−1fMjφj(aj)/parenrightbigg
≥/summationdisplay
j≤i−1fMj
φj(aj)/parenleftbigg
1 +fMiφi(ai)
/summationtext
j≤ifMjφj(aj)/parenrightbigg
=/summationdisplay
j≤i−1fMj
φj(aj)(1 +qi)
(i)
≥exp(qi/2)/summationdisplay
j≤i−1fMj
φj(aj)
9Published in Transactions on Machine Learning Research (07/2022)
exp(qi/2)≤/summationtext
j≤ifMjφj(aj)
/summationtext
j≤i−1fMjφj(aj)
where for inequality (i)we used that qi=fMiφi(ai)/summationtext
j≤ifMj
φj(aj)≤1and hence we have (1 +qi)≥exp(qi/2). Now as
we know that/summationtext
j≤ifMjφj(aj)≥/summationtext
j≤i−1fMjφj(aj)hence the following product results into a telescopic product
and we get,
/productdisplay
2≤i≤nexp(qi/2)≤/summationtext
j≤nfMjφj(aj)
fM2φ2(a2)
So by taking logarithm of both sides we get/summationtext
2≤i≤nqi≤2 log/parenleftbig
fM
φ(A)/parenrightbig
−2 log/parenleftbig
fM2φ2(a2)/parenrightbig
. Further incor-
porating the terms8
µi(i−1)we haveli=2qi
µi+8
µi(i−1). Hence,/summationtext
2≤i≤nli≤4µ−1(logn+ log/parenleftbig
fM
φ(A)/parenrightbig
−
log/parenleftbig
fM2φ2(a2)/parenrightbig
). Whereµ=µn≤µiandM⪰Mn⪰Mifor alli≤n.
Note that the upper bounds and the sum are independent of k(#clusters). Now in the next Lemma we
show that by sampling enough points based on li, we get equation 1.
Lemma 4.5. InBregmanFilter , settingr=O/parenleftig
dk(logk) log(1/ϵ)
ϵ2/parenrightig
, the returned coreset (C,Ω)satisfies the
following guarantee with at least 0.99probability∀X∈Rk×d
|fX(C,Ω)−fX(A)|≤ϵ(fX(A) +fφ(A)) (3)
We proof this lemma by applying Bernstein’s inequality on the following random variables
wi=/braceleftigg
(1/pi−1)fX(ai)with probability pi
−fX(ai) with probability (1−pi).
A detailed proof is discussed in the appendix (A.1.1).
Now using the Lemmas 4.4 and 4.5, we have the following main theorem of this section.
Theorem 4.6. For points coming in streaming fashion, BregmanFilter returns a coreset (C,Ω)for the
clustering based on Bregman divergence such that for all X∈Rk×d, with at least 0.99probability it ensures
the following guarantee.
|fX(C,Ω)−fX(A)|≤ϵ(fX(A) +fφ(A))
Such a coreset has O/parenleftig
dk(logk) log(1/ϵ)
µϵ2/parenleftbig
logn+ log/parenleftbig
fM
φ(A)/parenrightbig
−log/parenleftbig
fM2φ2(a2)/parenrightbig/parenrightbig/parenrightig
expected samples.
BregmanFilter takesO(d)update time and uses O(d)working space.
The expected sample size of the coreset (C,Ω)returned by BregmanFilter is bounded by r/summationtext
i≤nli. Using
Lemma4.4andLemma4.5weobtaintheexpectedsamplesizetobe O/parenleftig
dk(logk) log(1/ϵ)
µϵ2/parenleftbig
logn+log/parenleftbig
fM
φ(A)/parenrightbig
−
log/parenleftbig
fM2φ2(a2)/parenrightbig/parenrightbig/parenrightig
. Further, by using the µ-similarity expected sample size is also O/parenleftig
dk(logk) log(1/ϵ)
µ2ϵ2/parenleftbig
logn+
log/parenleftbig
fφ(A)/parenrightbig
−log/parenleftbig
fφ2(a2)/parenrightbig/parenrightbig/parenrightig
.
The algorithm requires a working space of O(d)which is to maintain the mean(centre) φi,µi,Mi(sparse
matrix) and the running sum S. Further for every incoming point BregmanFilter only needs to compute the
distance between the point and the current mean hence the update time is just O(d). So the running time of
the entire algorithm is O(nd), which is why it is easy to scale for large n. Note that although Theorem 4.6
gives the guarantees at the last instance, but using the same analysis technique one can ensure an equivalent
guarantee at every ithinstance by taking an union bound– this requires an extra multiplicative factor of
log(n)in the sample size. By doing so, (Ci,Ωi)satisfies the following for Ai,∀i∈[n]and∀X∈Rk×dwith
at least 0.99probability,
10Published in Transactions on Machine Learning Research (07/2022)
|fX(Ci,Ωi)−fX(Ai)|≤ϵ(fX(Ai) +fφi(Ai)) (4)
Note that BregmanFilter returns a smaller coreset Ccompare to offline coresets Bachem et al. (2018b);
Lucic et al. (2016) but at a cost of additive factor approximation that depends on the structure of the
data. Further unlike Bachem et al. (2018a); Lucic et al. (2016) our sampling complexity only depends on
1/µ.BregmanFilter can be easily generalized to create coresets for weighted clustering where each point ai
has some weight wisuch thatfX(ai) =wiminx∈XdΦ(ai,x). While sampling point (say ai) the algorithm
BregmanFilter setsci=aiandωi=wi/piwith probability pi.
Notice that the additive term (second term) in the guarantee of Theorem 4.6 depends on the variance of the
input data. So it can be significantly large compare to the relative term (first term). One way to address this
issue is to use an user defined parameter τ∈(0,1)to reduce the effect of additive term. In the algorithm
BregmanFilter , one needs to update li=2fMiφi(ai)
µiτS+4
µi(i−1)(1
τ+ 1). In the following corollary we state our
guarantee.
Corollary 4.1. For points coming in streaming fashion and with the above change in BregmanFilter , the
algorithm returns a coreset (C,Ω)for the clustering based on Bregman divergence such that for all X∈Rk×d,
with at least 0.99probability it ensures the following guarantee.
|fX(C,Ω)−fX(A)|≤ϵfX(A) +ϵ′fφ(A)
whereϵ′=ϵ·τ. Such a coreset has O/parenleftig
dk(logk) log(1/ϵ)
µϵ2/parenleftbig
logn+log/parenleftbig
fM
φ(A)/parenrightbig
−log/parenleftbig
fM2φ2(a2)/parenrightbig
τ/parenrightbig/parenrightig
expected samples.
It takesO(d)update time and uses O(d)working space.
The proof is similar to the proof of Theorem 4.6. Here, an important change in the analysis is that instead
of equation 2 we use following sensitivity function,
sup
X∈Rk×dfX(ai)
fX(Ai−1) +τfφi(Ai). (5)
In the appendix we first present the upper on these scores followed by the proof of the above corollary. Note
that with an increase in the coreset size by a factor of1
τone can improve the additive error guarantee by a
factor ofτ.
4.1 Online Coresets for k-means Clustering
Ink-means clustering, most commonly used Bregman divergence measure is the squared euclidean distance.
Now for squared euclidean distance we know that, Mi=Idandµi= 1,∀i∈[n]. So in this case the algorithm
BregmanFilter does not need to maintain Miandµi. In the following corollary we state the guarantee of
BregmanFilter for k-means clustering.
Corollary 4.2. LetA∈Rn×dsuch that the points are coming in streaming manner and fed to
BregmanFilter , it returns a coreset (C,Ω)which ensures the guarantee equation 1 ∀X∈Rk×dwith prob-
ability at least 0.99for thek-means problem. Such a coreset has O/parenleftbigdk(logk) log(1/ϵ)
ϵ2/parenleftbig
logn+ log/parenleftbig
fφ(A)/parenrightbig
−
log/parenleftbig
fφ2(a2)/parenrightbig/parenrightbig/parenrightbig
expected samples. The update time is O(d)time and uses O(d)as working space.
Proof.The proof follows by combining Lemma 4.4 and Lemma 4.5. As k-means clustering has Mi=Idand
µi= 1for alli≤n, the upper bound on the sensitivity scores is just,
li=fφi(ai)/summationtext
j≤ifφj(aj)+8
i−1
It can be verified by a similar analysis as in the proof of Lemma 4.4. The proof of the second part of Lemma
4.4 and Lemma 4.5 will follow as it is. Further note that k-means is a hard clustering hence the ϵ-net
size isO(ϵ−dklogk). Hence the expected size of Creturned by BregmanFilter isO/parenleftig
dk(logk) log(1/ϵ)
ϵ2/parenleftig
logn+
log/parenleftbig
fφ(A)/parenrightbig
−log/parenleftbig
fφ2(a2)/parenrightbig/parenrightig/parenrightig
. Further, similar to other µ-similar Bregman divergences, here in this case
also the update time is O(d)and uses a working space of O(d).
11Published in Transactions on Machine Learning Research (07/2022)
5 Coresets for Non-Parametric Clustering
For data coming in an streaming fashion, it is extremely challenging to anticipate the value of kwith having
some domain knowledge. For such an input one requires to solve a non-parametric clustering problem, i.e.,
the problem is find a clustering as well as a value of k.
Our previous algorithm BregmanFilter requires such kand the size of the coreset returned by it depends
on suchk(#clusters). It is mainly due to the union bound we need to take over a set in the query space
(ϵ-net). Here we explore the possibility of building a coreset non-parametric clustering problem. Naturally
the size of such coresets are independent of any k. However, it ensures a provable guarantee for any query
Xwith at most ncenters.
Even showing the existence of such a coreset is not obvious. It is not difficult to realize that it is impossible
to get asublinear sized coreset for non-parametric clustering which ensures a relative error approximation.
The following example illustrates this. Let Abe set ofnpoints and let Cbe a set of size mwith weights Ω,
wherem=o(n). Now for (C,Ω)to be arelative error coreset for non-parametric clustering, it must ensure
the following for all Xwith at most nrows,
|fX(C,Ω)−fX(A)|≤ϵfX(A).
However, notice that no matter what the set Cis, the above claim is false when X=C. This is because for
such a query Xthe total cost fX(C,Ω) = 0, whereasfX(A)̸= 0sincem=o(n).
5.1 Existence of sublinear sized coresets for non-parametric clustering
In this subsection, we show the existence of coreset for non-parametric clustering that ensures an additive
error approximation guarantee. The proof for this will proceed via the probabilistic method – we will design a
set of sampling probabilities that will be used to show the existence of the claimed coreset. The main novelty
here is that the sampling scores are notthe usual sensitivity scores Feldman & Langberg (2011), which are
defined for all the data points all at once. In contrast, we define the scores iteratively— for every step i, we
define a score for the ithpoint, and this score depends on the coreset maintained by the algorithm till this
step, i.e., these scores are also random variables themselves. We will then show a bound on the expected
sample size.
At each step iwe define two sensitivity scores called barrier sensitivity scores . We start by defining barrier
sensitivity scores.
Definition 5.1 (Barrier Sensitivity Scores) .LetAi−1be the set first set of (i−1)points andφi=/summationtext
j≤iaj/i
be the mean of Ai. Let (Ci−1,Ωi−1)be a weighted subsample of Ai−1. At stepi, the upper barrier sensitivity
su
i= supXru
i(X)and the lower barrier sensitivity sl
i= supXrl
i(X), where:
ru
i(X) =fX(ai)
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +ϵfφi(Ai)(6)
rl
i(X) =fX(ai)
fX(Ci−1,Ωi−1)−(1−ϵ)fX(Ai−1) +ϵfφi(Ai)(7)
here supis over all Xwith at most ncenters.
Note that unlike the regular sensitivity scores, the above scores are neither guaranteed to be non-negative
nor bounded by 1, if the sets Ci−1and weights Ωi−1are arbitrary. Our algorithm will, however, maintain
an invariant (which is effectively the “coreset-property” of (Ci−1,Ωi−1)) that guarantees the non-negativity
of the scores.
Wewillsamplepointsbasedon the upperboundsontheabovebarriersensitivity, and showthattheresulting
set is a coreset for non-parametric clustering. A similar technique has been used in (Cohen et al., 2016) to
build coresets for spectral approximation for matrices. In case of general cost functions, especially Bregman
divergences, getting nontrivial upper bounds (i.e., bounds less than 1) onsu
iandsl
iis very challenging.
12Published in Transactions on Machine Learning Research (07/2022)
In order to show the existence result, we assume that we have access to an oracle that returns a upper
bound on these sensitivity scores. Utilizing these upper bounds, we show the existence of coreset for the
non-parametric clustering problem. We later show that under some assumption we can get an upper bound
on these scores and thereby getting an algorithm that returns a desired coreset the problem.
5.1.1 Algorithm Overview:
For every point ai, we assume that the oracle returns ˜su
iand˜sl
isuch that ˜su
i≥su
iand˜sl
i≥sl
i. Definepi
andωias follows– pi= min{1,cu˜su
i+cl˜sl
i}, wherecu=2
ϵ+ 1andcl=2
ϵ−1. Defineωi=1
pi. We sample ai
to be in the coreset with probability piand give it a weight ωiif included. Such a sampled set of points are
going to ensure the guarantee in the following theorem.
Theorem 5.1. LetA∈Rn×d, for every Bregman divergence dΦas in table (1) the above sampling technique
returns a coreset (C,Ω)such that∀Xwith at most ncenters it ensures.
/vextendsingle/vextendsinglefX(C,Ω)−fX(A)/vextendsingle/vextendsingle≤ϵ(fX(A) +fφ(A)). (8)
The expected size of such a coreset is O/parenleftig
1
µϵ2/parenleftig
logn+ log/parenleftbig
fM
φ(A)/parenrightbig
−log/parenleftbig
fM2φ2(a2)/parenrightbig/parenrightig/parenrightig
.
Note that the above guarantee equation 8 is deterministic, but the coreset size is a random variable. To
prove the above theorem we use the following supporting lemmas. We first show that for each point ai,
if˜su
iand˜sl
iupper bound the sensitivity scores su
iandsl
i, then the sampled coreset would guarantee that
equation 4 holds for all Xwith at most ncenters.
Lemma 5.2. Suppose the oracle returns ˜su
iand˜sl
isuch that,∀i∈[n].
sup
XfX(ai)
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +ϵfφi(Ai)≤˜su
i
sup
XfX(ai)
fX(Ci−1,Ωi−1)−(1−ϵ)fX(Ai−1) +ϵfφi(Ai)≤˜sl
i
Letϵ>0. The sampling probability for first point is p1= 1, and for the ithpoint ispi= min{˜si,1}where
˜si=cu˜su
i+cl˜sl
i, such that cu=2
ϵ+ 1andcl=2
ϵ−1.
It maintains,
(Ci,Ωi) =/braceleftigg
(Ci−1,Ωi−1)∪(ai,1
pi)if sampled,
(Ci−1,Ωi−1), else.
such that∀Xwith at most ncenters it guarantees
/vextendsingle/vextendsingle/vextendsinglefX(Ci,Ωi)−fX(Ai)/vextendsingle/vextendsingle/vextendsingle≤ϵ(fX(Ai) +fφi(Ai)).
Proof.We show this by induction. At i= 1this is true, as we have p1= 1. So we get,
(1−ϵ)fX(a1)≤fX(c1,ω1)≤(1 +ϵ)fX(a1) (9)
where c1=a1andω1= 1. Now consider that at i−1the tuple (Ci−1,Ωi−1)ensures
|fX(Ai−1)−fX(Ci−1,Ωi−1)|≤ϵ(fX(Ai−1) +fφi−1(Ai−1)) (10)
We show that (Ci,Ωi)also holds a similar guarantee for Ai. Recall that we need to show that upon creating
the sampling probability (and weight) by using an upper bound on the barrier sensitivity scores, no matter
whether the random process samples the point aior not, the tuple (Ci,Ωi)will ensure the desired guarantee.
Recall that the sampling probability pi= min{1,cu˜su
i+cl˜sl
i}. We first take the case when pi= 1, and hence
ci=aiandωi= 1. So we have,
|fX(Ai−1)−fX(Ci−1,Ωi−1)| ≤ϵ(fX(Ai−1) +fφi−1(Ai−1))
13Published in Transactions on Machine Learning Research (07/2022)
|fX(Ai)−fX(Ci,Ωi)|(i)
≤ϵ(fX(Ai−1) +fφi−1(Ai−1))
|fX(Ai)−fX(Ci,Ωi)|(ii)
≤ϵ(fX(Ai) +fφi(Ai))
Here, (i)is true because fX(ai) =fX(ci,ωi). In (ii)we only increase the RHS, because fφi(Ai)≥
fφi−1(Ai−1)(Observation 4.2) and fX(ai)≥0. So, finally we have,
(1−ϵ)fX(Ai)−ϵfφi(Ai)≤fX(Ci,Ωi)≤(1 +ϵ)fX(Ai) +ϵfφi(Ai)
Next when pi<1then for the upper bound we use the upper barrier sensitivity definition i.e., su
i,
pi≥˜su
i≥su
i
Using the definition of su
i, we have that∀X,
pi(i)
≥fX(ai)
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +ϵfφi(Ai)
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +ϵfφi(Ai)≥fX(ai)
pi
(1 +ϵ)fX(Ai−1) +ϵfφi(Ai)≥fX(Ci−1,Ωi−1) +fX(ai)
pi
Now, if aiis actually sampled, fX(Ci−1,Ωi−1) +fX(ai)
pi=fX(Ci,Ωi), and hence,
(1 +ϵ)fX(Ai−1) +ϵfφi(Ai)≥fX(Ci,Ωi)
(1 +ϵ)fX(Ai) +ϵfφi(Ai)≥fX(Ci,Ωi)
This shows that the upper bound claim is true with aiis sampled in (Ci,Ωi). In case that pi<1andaiis
not actually sampled, we have fX(Ci,Ωi) =fX(Ci−1,Ωi−1)and hence follows immediately from above. So,
whenpi<1, irrespective of aibeing sampled in (Ci,Ωi)or not, the desired upper bound claim for Aiholds.
For the lower bound we use the lower barrier sensitivity definition i.e., sl
iin a similar manner–
1>˜sl
i
1> sl
i
1≥fX(ai)
fX(Ci−1,Ωi−1)−(1−ϵ)fX(Ai−1) +ϵfφi(Ai)
fX(Ci−1,Ωi−1)−(1−ϵ)fX(Ai−1) +ϵfφi(Ai)≥fX(ai)
fX(Ci−1,Ωi−1)≥(1−ϵ)fX(Ai−1)−ϵfφi(Ai) +fX(ai)
fX(Ci−1,Ωi−1)≥(1−ϵ)fX(Ai)−ϵfφi(Ai)
When the point aiis not sampled in (Ci,Ωi)we have (Ci−1,Ωi−1) = ( Ci,Ωi)and hence we have our
inductive step. If aidoes get sampled than fX(Ci,Ωi)≥fX(Ci−1,Ωi−1). Hence in both the cases we have
that
fX(Ci,Ωi)≥(1−ϵ)fX(Ai)−ϵfφi(Ai)
This completes the proof of both the lower and upper bound for the ithstep.
There are two important points to note from the above lemma.
•Since, the oracle is giving an upper bound on the barrier sensitivity scores, so the coreset guarantees
are deterministic. That is when, pi<1then irrespective of aibeing sampled in the (Ci,Ωi)or not,
it always ensures the guarantee. It implies that if we run this process multiple times on the same
data stream, then each time we get a different coreset but each coreset ensures the desired guarantee
with probability 1.
14Published in Transactions on Machine Learning Research (07/2022)
•Due to the barrier sensitivity score based sampling, the coreset guarantee holds ∀X, and we do
not require the knowledge of pseudo dimension of the query space. This intuition is similar to the
deterministic spectral sparsification claim in (Batson et al., 2012).
Next we need to analyze the expected sample size. Note that, unlike the the online sensitivity scores in
BregmanFilter , the barrier sensitivity scores themselves are random variables, and depend both on the
order of the input stream as well as the previous sampling decisions while maintaining (Ci−1,Ωi−1). Next,
we show that the expectation of these scores can be bounded, hence giving a bound on the expected coreset
size. We first present a supporting lemma.
Lemma 5.3. Given non-negative scalars q,r,s,u,v andw, whereq,r,sandware positive, we define a
random variable tas,
t=/braceleftigg
q−u·r with probability p,
q−v·r with probability (1−p).
Ifr
q+w= 1then we get,
E/bracketleftbiggs
t+w−s
q+w/bracketrightbigg
=pu+ (1−p)v−uv
(1−u)(1−v)/parenleftbiggs
q+w/parenrightbigg
The proof is discussed in the appendix (A.2.1). Let Πi−1∈{0,1}i−1be the sampling decisions ( 0and1indi-
cating the not-sampled/sampled decision respectively) that the algorithm made while creating (Ci−1,Ωi−1),
based on which the sampling probability of the next point aiis being computed. The icoordinates of Πi
will be denoted as π1,...,πi.
Before bounding the expected barrier sensitivity scores we first show that the at any step i, the upper bound
on the expected sensitivity barrier score is independent of the sampling choice made by the algorithm in the
previous step for the point ai−1. Recall,cu=2
ϵ+ 1andcu=2
ϵ−1, for which we show a helpful lemma.
Lemma 5.4. For any step j≥1,
fX(ai)
pj+1((ϵ/2)fX(Ai−1) + (1 +ϵ/2)fX(Aj)−fX(Cj,Ωj))≤1
cu
fX(ai)
pj+1(fX(Cj,Ωj))−(ϵ/2)fX(Ai−1)−(1−ϵ/2)fX(Aj)≤1
cl.
Again the proof is delegated to the appendix (A.2.2). Next we show the following lemma which provides a
bound on the expected barrier sensitivity score. The detailed proof is discussed in the appendix (A.2.3).
Lemma 5.5. For points that are coming in a stream, let i≥j+ 1, and letπj+1denote the sampling choice
that the algorithm has made at (j+ 1)thstep. Then we have,
Eπj+1/bracketleftigg
fX(ai)
γu
i−1,j+1+ϵfφi(Ai)/bracketrightigg
≤fX(ai)
γu
i−1,j+ϵfφi(Ai)
Eπj+1/bracketleftigg
fX(ai)
γl
i−1,j+1+ϵfφi(Ai)/bracketrightigg
≤fX(ai)
γl
i−1,j+ϵfφi(Ai)
In the following lemma we bound the expected barrier sensitivity score with a term independent of any
sampling choice made by the algorithm until then. Finally, it yields the expected sample size.
Lemma 5.6. LetAbe a set of npoints each in Rd. For point aiand for all X∈Rk×dwe have following
bound∀i∈[n],
EΠi−1/bracketleftbiggfX(ai)
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +ϵfφi(Ai)/bracketrightbigg
≤2fMiφi(ai)
µiϵ/summationtext
j≤ifMjφj(aj)+12
µiϵ(i−1)
EΠi−1/bracketleftbiggfX(ai)
fX(Ci−1,Ωi−1)−(1−ϵ)fX(Ai−1) +ϵfφi(Ai)/bracketrightbigg
≤2fMiφi(ai)
µiϵ/summationtext
j≤ifMjφj(aj)+12
µiϵ(i−1)
15Published in Transactions on Machine Learning Research (07/2022)
Proof.Due to lemma 5.5 we show that expected sensitivity score for any aiis independent of the sampling
choice made by the algorithm for ai−1point. We show the result for the upper barrier sensitivity score. The
analysis for the lower barrier sensitivity score is very similar which can be found in the appendix A.2.4.
Let(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) =fX(Au
i−1)wherefX(Au
i−1) =/summationtext
j≤i−1fX(au
j). Here each term
fX(au
j) = (1 +ϵ−p−1
j)fX(aj)ifajis present in Ci−1elsefX(au
j) = (1 +ϵ)fX(aj). Now the expected upper
bound on the upper barrier sensitivity score can be bounded as follows.
EΠi−1/bracketleftbiggfX(ai)
fX(Au
i−1) +ϵfφi(Ai)/bracketrightbigg(i)
≤EΠi−1/bracketleftbiggfMi
X(ai)
fX(Au
i−1) +ϵfφi(Ai)/bracketrightbigg
(ii)
≤Eπi−1/bracketleftbigg/bracketleftig
2fMiφi(ai) +4
i−1/summationtext
aj∈Ai−1[fMiφi(aj) +fMi
X(aj)]/bracketrightig
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
=Eπi−1/bracketleftbigg/bracketleftig
2fMiφi(ai) +4
i−1/summationtext
aj∈Ai−1fMiφi(aj)/bracketrightig
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
+Eπi−1/bracketleftbigg 4
i−1fMi
X(Ai−1)
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
(iii)=Eπi−1/bracketleftbigg2fMiφi(ai) +4
i−1fMiφi(Ai−1)
γu
i−1,i−1+ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
+Eπi−1/bracketleftbigg 4
i−1fMi
X(Ai−1)
γu
i−1,i−1+ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
(iv)
≤Eπi−2/bracketleftbigg2fMiφi(ai) +4
i−1fMiφi(Ai−1)
γu
i−1,i−2+ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−3/bracketrightbigg
+Eπi−2/bracketleftbigg 4
i−1fMi
X(Ai−1)
γu
i−1,i−2+ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−3/bracketrightbigg
(v)
≤Eπ0/bracketleftbigg2fMiφi(ai) +4
i−1fMiφi(Ai−1)
γu
i−1,0+ϵfφi(Ai)/bracketrightbigg
+Eπ0/bracketleftbigg 4
i−1fMi
X(Ai−1)
γu
i−1,0+ϵfφi(Ai)/bracketrightbigg
=2fMiφi(ai) +4
i−1fMiφi(Ai−1)
ϵ/2fX(Ai−1) +ϵfφi(Ai)+4
(i−1)fMi
X(Ai−1)
ϵ/2fX(Ai−1) +ϵfφi(Ai)
(vi)
≤2fMiφi(ai) +4
i−1fMiφi(Ai−1)
µiϵ(0.5fMi
X(Ai−1) +fMiφi(Ai))+4
(i−1)fMi
X(Ai−1)
µiϵ(0.5fMi
X(Ai−1) +fMiφi(Ai))
≤2fMiφi(ai)
µiϵfMiφi(Ai)+4fMiφi(Ai−1)
µiϵ(i−1)fMiφi(Ai)+8fMi
X(Ai−1)
µiϵ(i−1)fMi
X(Ai−1)
(vii)
≤2fMiφi(ai)
µiϵfMiφi(Ai)+4
µiϵ(i−1)+8
µiϵ(i−1)
≤2fMiφi(ai)
µiϵ/summationtext
j≤ifMjφj(aj)+12
µiϵ(i−1)
The inequality (i)is by upper bounding Bregman divergence by squared Mahalanobis distance. The inequal-
ity(ii)is due to applying triangle inequality on the numerator, (a2+b2)≤2(a2+b2). The (iii)equality is
by replacing the denominator with the above definition. The (iv)inequality is by applying the supporting
Lemma 5.3. By recursively applying Lemma 5.3 we get the inequality (v)which is independent of the ran-
dom choices made by the algorithm. The inequality (vi)is by using the lower bound on the denominator.
The inequality (vii)an upper bound on the second and the third term. In the final inequality we use the
fact that for any µsimilar Bregman divergence from Ackermann & Blömer (2009); Lucic et al. (2016) we
16Published in Transactions on Machine Learning Research (07/2022)
haveMj⪯Miforj≤i(Lemma 4.1). Further by the property of Bregman divergence we know that
fφi(Ai−1)≥fφi−1(Ai−1). Hence we have fMiφi(Ai)≥/summationtext
j≤ifMjφj(aj). Notice that the above analysis is also
true for all X. Hence we have this upper bound for all Xwith at most ncenters. So we have the,
EΠi−1/bracketleftbiggfX(ai)
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +fφi(Ai)/bracketrightbigg
≤2fφi(ai)
ϵ/summationtext
j≤ifφj(aj)+12
ϵ(i−1)
Note that the analysis holds for all X. Hence we get the expected upper bound on both the barrier sensitivity
scores. Further, these upper bounds are independent of k, which is number of centers in Xor any bicreteria
approximation that usually depends on k. Now as the oracle returns a tight upper bound on the sensitivity
scores which is used to compute the sampling probability of a streaming point, so we can comment about
the expected sample size by bounding the sum of upper bounds of barrier sensitivity scores.
Lemma 5.7. The algorithm returns (C,Ω)such that the expected size of CisO/parenleftig
1
µϵ2/parenleftig
logn+log/parenleftbig
fM
φ(A)/parenrightbig
−
log/parenleftbig
fM2φ2(a2)/parenrightbig/parenrightig/parenrightig
.
The detailed proof is discussed in the appendix (A.2.5). Now we are ready to prove our main theorem.
Proof. of Theorem 5.1. Using lemma 5.2 and 5.6 we can now show that the any coreset created by the
algorithm satisfies the guarantees in Theorem 5.1. Since the coreset guarantees hold deterministically, there
must exist a coreset with size less than the expected bound that satisfies the guarantees. This completes the
existential guarantee stated by Theorem 5.1.
5.2 Coreset algorithm for non-parametric clustering based on empirical sensitivity
Now as it is not known how to get an oracle that upper bounds the barrier sensitivity scores, we present
a heuristic which works well in practice. We estimate the sensitivity scores using sampling, and call these
empirical sensitivity scores .
We first give an example to show that the sampling probabilities in the online setting need to be upper
bounds to the barrier sensitivity scores.
Example5.1. Fix anyϵ∈(0,1). Consider a simple setup where the first point is a∈Rd. Now the algorithm
samples the point in the coreset with probability 1.
Suppose the second point is b∈Rdand suppose that the upper barrier sensitivity for it, achieved by the query
X∗, equalss. That is,
s= sup
Xru
2(X) =ru
2(X∗)
Now, suppose we sample bwith some probability p<s. Thus,
p < ru
2(X∗) =fX∗(b)
(1 +ϵ)fX∗(a)−fX∗(a) +ϵ(fφ(a) +fφ(b))
ϵfX∗(a) +ϵ(fφ(a) +fφ(b))<fX∗(b)
p
ϵfX∗(a) +ϵ(fφ(a) +fφ(b))< f X∗(a) +fX∗(b)
p.,
which violates the coreset property. Hence, when we sample the second point, which happens with probability
p, we do not have a coreset with the desired guarantee.
Wenextpresentanexamplewhichshowsthedifficultyofestimatingthebarriersensitivityscoresbysampling
queries. This will motivate us to make some assumptions about our data.
17Published in Transactions on Machine Learning Research (07/2022)
Example 5.2. For a fixed point aiand a query space, let ru
i(X)be a random variable defined as equation 6,
where Xis chosen randomly from the query space. Consider that we do not have access to the sensitivity
scoresi= supXru
i(X). Our proposed algorithm is to sample a random set of queries Yuniformly at random
such that the maximum value of ru
i(·)inYwell approximates si.
Define the empirical upper sensitivity score to be ˜si= max X∈Yru
i(X). Note that if X∗belongs toY, then
˜si=si, but this is a very low probability event in general. Consider the set B={X|ru
i(X)≥si/K}for
someK > 1. If the sampled queries do not contain any element in B, then ˜si<si/K. Thus the probability
that this sampling based algorithm obtains an estimate ˜sithat satisfies ˜si≥si/Kis exactly the same as the
probability mass of the set B. Note that the first example shows that with si= 1, if˜si<1/K, then using
K˜sias sampling probabilities, we fail to have a coreset.
From this we conclude that, in order to use an empirically obtained upper bounds on the sensitivity scores
(called empirical sensitivity scores), we need additional assumptions to show that we get a valid coreset with
desired guarantees. We now state the assumption, originally in (Baykal et al., 2018).
LetXrepresent the query space where each query X∈Xhas at most ncenters. Now, for each point aiand
X, we consider the following two random variables, ru
i(X)andrl
i(X)defined as follows,
ru
i(X) =fX(ai)
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +ϵfφi(Ai)
rl
i(X) =fX(ai)
fX(Ci−1,Ωi−1)−(1−ϵ)fX(Ai−1) +ϵfφi(Ai),
where the randomness is over X∈X. We assume that the CDF of both ru
i(·)andrl
i(·)are bounded. The
assumption stated below is for ru
i(·). A similar assumption is also considered for rl
i(·).
Assumption 5.1. There is a pair of universal constants KandK′such that for each i∈[n], the CDF of
the random variable ru
i(X)forX∈Xdenoted by Gi()satisfies,
Gi(x∗/K)≤exp(−1/K′)
wherex∗= min{y∈[0,1] :Gi(y) = 1}.
Further, we consider the above assumption is true for all ai∈A. Now the following two lemmas are similar
to lemma 6 and 7 in (Baykal et al., 2018), using which we get the upper bounds on the sensitivity scores.
Here we state them for completeness. Lemma 5.8 is stated for all i≤n.
Lemma 5.8. LetK,K′>0be universal constants and let Xbe the query space as defined above with CDF
Gi(·)satisfying the assumption 5.1. Let Y={X1,X2,...,Xm}be a set ofmi.i.d. samples each drawn from
X. LetXm+1be an i.i.d. sample from Xthen,
P/parenleftbig
Kmax
X∈Yru
i(X)≤ru
i(Xm+1)/parenrightbig
≤exp(−m/K′)P/parenleftbig
Kmax
X∈Yrl
i(X)≤rl
i(Xm+1)/parenrightbig
≤exp(−m/K′)
Proof.LetXmax=argmaxX∈Yjru
i(X), then
P(Kmax
X∈Yru
i(X)≤ru
i(Xm+1)) =/integraldisplayx∗
0P(ru
i(Xmax)≤y/K|ru
i(Xm+1) =y)dP(y)
(i)=/integraldisplayx∗
0P(ru
i(Xmax)≤y/K)mdP(y)
≤/integraldisplayx∗
0Gi(y/K)mdP(y)
(ii)
≤Gi(x∗/K)m/integraldisplayx∗
0dP(y)
=Gi(x∗/K)m
≤exp(−m/K′)
Here (i)is because{X1,X2,...,Xm}are i.i.d. fromY. Further (ii)is due to the assumption 5.1.
18Published in Transactions on Machine Learning Research (07/2022)
Algorithm 2 NonParametricFilter
Require: ai,i= 1,...n ;t>1;ϵ∈(0,1);Y={X1,X2,...,XO(log(n/δ))}
Ensure: (C,Ω)
cu= 2/ϵ+ 1;cl= 2/ϵ−1;φ0=∅;S= 0;C1
0=...= Ωt
0=∅
λ=∥a1∥min;ν=∥a1∥max
whilei≤ndo
λ= min{λ,∥ai∥min};ν= max{ν,∥ai∥max}
Update Mi;µi=λ/ν
φi= ((i−1)φi−1+ai)/i;S=S+fMiφi(ai)
ifi= 1then
pi= 1
else
˜ru
i= max X∈Yru
i(X)
˜rl
i= max X∈Yrl
i(X)
pi= min{1,(cu˜ru
i+cl˜rl
i}
end if
(ci,ωi) =/braceleftigg
(ai,1/(pi)) ifaiis sampled
(∅,0) else
(Ci,Ωi) = (Ci−1,Ωi−1)∪(ci,ωi)
end while
Return (C,Ω)
Similarly, it is also proved for rl
i(). Let there is a finite set Y⊂Xfrom which we get empirical sensitivity
scores ˜ru
i= max X∈Yru
i(X)and˜rl
i= max X∈Yrl
i(X). Ouralgorithmusesthesescores. Nowwiththefollowing
lemma we establish that empirical sensitivity scores are good approximations to the true sensitivity scores
su
iandsl
i. It is also used to decide the size of the finite set Y.
Lemma 5.9. Letδ∈(0,1), consider the set Y⊂Xof size|Y|≥⌈K′log(n/δ)⌉, then
PX∈X/parenleftbig
∃i∈[n] :K˜ru
i≤ru
i(X)/parenrightbig
≤δ
PX∈X/parenleftbig
∃i∈[n] :K˜rl
i≤rl
i(X)/parenrightbig
≤δ
Proof.The proof follows from lemma 5.8. Let the event Eibe the event that K·maxX′∈Yru
i(X′)≤ru
i(X).
Now,
P(Ei) =PX∈X(Kmax
X′∈Yru
i(X′)≤ru
i(X))≤exp(−|Y|/K′)
Hence, by taking union bound over all i∈[n], we have that P[¬(∪iEi)]≥1−nexp(−|Y|/K′). By choosing
|Y|≥⌈K′log(n/δ)⌉, we get that P[¬(∪iEi)]≥1−δ. Hence with probability at least 1−δ, the score ˜ru
i
upper bounds the true score. We can show a similar claim for ˜rl
i.
So with high probability we have an upper bound on (su
i,sl
i)using empirical sensitivity scores (˜ru
i,˜rl
i). Al-
though the above two lemmas are stated for upper barrier sensitivity scores, they are also true for lower
barrier sensitivity scores. Now we present our second algorithm (2) called NonParametricFilter , which
returns a coreset for non-parametric clustering via Bregman divergence. Note that without the above as-
sumption 5.1 our algorithm acts as a heuristic. The algorithm requires the query set Ywhich hasO(log(n/δ))
queries.
The coreset from the above algorithm ensures the guarantee.
Theorem 5.10. LetA∈Rn×d, for every Bregman divergence dΦas in table (1) NonParametricFilter re-
turns a coreset (C,Ω)for non parametric clustering based on dΦsuch that∀Xwith at most ncentres in Rd
it ensures equation 1 with at least 1−δprobability.
/vextendsingle/vextendsinglefX(C)−fX(A)/vextendsingle/vextendsingle≤ϵ(fX(A) +fφ(A)). (11)
19Published in Transactions on Machine Learning Research (07/2022)
The size of such a coreset is O/parenleftig
1
µϵ2/parenleftig
logn+ log/parenleftbig
fM
φ(A)/parenrightbig
−log/parenleftbig
fM2φ2(a2)/parenrightbig/parenrightig/parenrightig
expected samples.
Proof.This is proved by combining the claims of Theorem 5.1 and Lemma 5.9.
Again the additive error factor can be further improved using an user defined parameter τ∈(0,1). In the
barrier sensitivity scores equation 6 and equation 7 we multiply τwith the additive term, i.e., ϵfφi(Ai)is
replaced with τϵfφi(Ai). We get the following corollary.
Corollary 5.1. LetA∈Rn×d, for every Bregman divergence dΦas in table (1) NonParametricFilter re-
turns a coreset (C,Ω)for non parametric clustering based on dΦsuch that∀Xwith at most ncentres in Rd
it ensures equation 1 with at least 1−δprobability.
/vextendsingle/vextendsinglefX(C)−fX(A)/vextendsingle/vextendsingle≤ϵfX(A) +ϵ′fφ(A), (12)
whereϵ′=ϵ·τ. The size of such a coreset is O/parenleftig
1
µϵ2/parenleftig
logn+log/parenleftbig
fM
φ(A)/parenrightbig
−log/parenleftbig
fM2φ2(a2)/parenrightbig
τ/parenrightig/parenrightig
expected samples.
This can be proved by a similar analysis as of Corollary 4.1.
5.2.1 Non-Parametric Clustering via Squared Euclidean Distance
AsNonParametricFilter returns a coreset for non-parametric clustering, hence it can also be used for
k-means clustering. In this case our algorithm (2) returns a coreset (C,Ω), for squared euclidean Bregman
divergence. The algorithm simply uses IdasMiand 1asµi. In the following corollary we state the
guarantees that NonParametricFilter ensures for k-means clustering that follows from Theorem 5.1.
Corollary 5.2. LetA∈Rn×dbe the points fed to NonParametricFilter for thek-means problem. It
returns a coreset (C,Ω)which ensures the guarantee equation 1 for any Xwith at most ncenters. The
coresets has O/parenleftig
1
ϵ2/parenleftig
logn+ log/parenleftbig
fφ(A)/parenrightbig
−log/parenleftbig
fφ2(a2)/parenrightbig/parenrightig/parenrightig
expected samples.
This directly follows from Theorem 5.1, and discussed in the appendix A.2.6.
5.2.2 Coresets for DP-Means Clustering
DP-Means clustering is a non-parametric clustering. Here we discuss how NonParametricFilter can also
be used to approximate DP-Means clustering Bachem et al. (2015). The problem was originally defined for
squared euclidean distances which has a valid generalization in other Bregman divergences. For an input A,
query Xwith at most ncost of DP-Means cost is defined as follows,
costDP(A,X) =fX(A) +λ|X|.
HerefX(A)is the cost on the entire Abased on some Bregman divergences introduced earlier. It is not
difficult to see that the coreset from NonParametricFilter ensures an additive error approximation for this
definition of DP-Means clustering based on µ-similar Bregman divergence. Now we claim that by allowing a
small additive error approximation our coreset size significantly improves upon the coresets for relative error
approximation for DP-Means clustering Bachem et al. (2015), as in practice O(dd)≫O(logn). We first the
state the following result.
Lemma 5.11. The coreset (C,Ω)from NonParametricFilter ensures the following for all Xwith at most
ncenters in Rd,
|costDP(C,X)−costDP(A,X)|≤ϵ(fX(A) +fφ(A))
Proof.Notice that if one applies DP-Means on the coreset from NonParametricFilter we get the following,
/vextendsingle/vextendsinglecostDP(C,X)−costDP(A,X)/vextendsingle/vextendsingle=/vextendsingle/vextendsinglefX(C)−fX(A)/vextendsingle/vextendsingle≤ϵ(fX(A) +fφ(A))
The last inequality is by Theorem 5.1.
20Published in Transactions on Machine Learning Research (07/2022)
Now due to Lemma 5.11 we have the following Theorem.
Theorem 5.12. Forϵ∈(0,1)the coreset (C,Ω)ensures, cost DP(A,XC)≤costDP(A,XA) +ϵ(fXC(A) +
fXA(A)+2fφ(A)). with high probability, where XCandXAare the optimal cluster centers for the DP-Means
clustering on (C,Ω)andA. The expected size of the coreset is O/parenleftig
1
µϵ2/parenleftig
logn+log/parenleftbig
fM
φ(A)/parenrightbig
−log/parenleftbig
fM2φ2(a2)/parenrightbig/parenrightig/parenrightig
.
Proof.LetXCandXAare the optimal centers for DP-Means clustering on CandArespectively. Now we
know that,
costDP(A,XC)−ϵ(fXC(A) +fφ(A))≤costDP(C,XC)
≤costDP(C,XA)
≤costDP(A,XA) +ϵ(fXA(A) +fφ(A))
costDP(A,XC)≤costDP(A,XA) +ϵ(fXC(A) +fXA(A) + 2fφ(A)).
6 Uniform Deviation
Our first result showed an additive error coreset for Bregman clustering. It is a natural question to ask
whether an additive error that is a function of fφ(A)is useful. In this section we provide evidence that when
looking at Bregman clustering from the generalization perspective, the generalization error obtained is also
a function of fφ(A). This implies that clustering by using coresets of the training data (rather than the full
data) does not increase the generalization gap.
For a given distribution of input sets and a learnt model, the uniform deviation is the difference between
the expected loss and the average empirical loss over a set of samples from the distribution of inputs. It is
used to understand the generalization error by a trained model. In this section we show that for clustering
based on any µ-similar Bregman divergence the uniform deviation is bounded. For a distribution Dover an
input space in Rdand someµ-similar Bregman divergence (Table 1), let ξ=Ea∈D[a]andσ2=Ea∈D[fξ(a)].
Bachem et al. (2017a) showed that for k-means clustering using Euclidean distance, ifEa∈D[fξ(a)2]
σ4≤t<∞,
then with sufficiently large number of samples A={a1,...,am}where each aiis an i.i.d. sample from D
andm= Ω/parenleftig
tkdlogk
ϵ2/parenrightig
we get the following ∀X∈Rk×dwith a constant probability,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
mm/summationdisplay
i=1fX(ai)−E[fX(a)]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤ϵ(σ2+E[fX(a)]) (13)
This essentially implies that there is an additive error approximation on the generalization error by a model
trainedonalargeinputset. Weshowthatasimilarresultisalsotrueforother µ-similarBregmandivergence.
The additive term (ϵ·σ2)in the generalization error is very similar to the additive term (ϵ·fφ(A))in our
coresets guarantees equation 1.
Given theD, the randomness is always over the samples from D. So we use E[·]instead of Ea∈D[·]. We
consider that for any µ-similar Bregman divergence we know the parameters Mandµ(1) forDsuch that
for all a∼Dandx∈Rd,µfM
x(a)≤fx(a)≤fM
x(a). We also consider the following assumption is true.
E/bracketleftig
fM
ξ(a)2/bracketrightig
µ2σ4
M+1
µ2<∞ (14)
Here,ξ=E[a],σ2
M=E[(a−ξ)TM(a−ξ)]and for all a∼D,fM
ξ(a) = (a−ξ)TM(a−ξ). Consider
A={a1,...,am}bemindependent samples from D.
We analyze a family of functions Gmapping from an input space RdtoR≥0that captures a measure between
the cost of a point a∼Dand a statistical cost of D. We first state the following lemma, whose proof is
discussed in the appendix A.3.1.
21Published in Transactions on Machine Learning Research (07/2022)
Lemma 6.1. Letk∈N. LetDbe a distribution on Rdwithµandσ2same as defined above. For a µ-similar
Bregman divergence, for any point a∼D, and any X∈Rk×dwe definegX(a)as,
gX(a) =fX(a)
σ2+E[fX(a)]. (15)
Lets(a) =2fξ(a)
µσ2
M+8
µ. Then we have gX(a)≤s(a)for all X∈Rk×dandE[s(a)2] =O(t).
For the family of function G={gX(·)|X∈Rk×d}, let Pdim (G)≤ρ<∞. Now due to above lemma we can
use the main framework of Bachem et al. (2017a) which is stated as follows,
Theorem 6.2. Letϵ∈(0,1)andGbe a family of functions from atoR≥0, where a∼D. Letρbe such
that Pdim (G)≤ρ<∞andtbe such that E[s(a)] =O(t). For every aandX∈Rk×d, lets(a)be a function
such that, supXgX(a)≤s(a). Let a1,...,a2mbe2mi.i.d. samples from Dwherem=O/parenleftbigtρ
ϵ2/parenrightbig
such that
4E[fM
ξ(a)2]
µ2σ4
M+96
µ2≤tandP(1
2m/summationtext2m
i=1s(ai)2>t)≤1/4. Then for all X∈Rk×dwith at least 0.99probability
we have,/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
mm/summationdisplay
i=1fX(ai)−E[fX(a)]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤ϵ(σ2+E[fX(a)]) (16)
The proof of can be found in the appendix A.3.2.
7 Experiments
We now show the empirical performance of BregFilter (our BregmanFilter ) as well as a heuristic version
ofNonParametricFilter (NP-Filter ). Since there are no existing onlinebaselines for this problem, we
compare it against the offlinecoreset algorithms on real datasets. It is important to note that theoretically
our coreset can never perform better than the offline coreset algorithm. Intuitively this simply is because an
offline algorithm has more knowledge from the complete data before deciding the sampling probability. Here
we investigate whether the performance of the BregFilter and the NP-Filter matches the performance of
the offline algorithms. We compare against the following coreset algorithms:
1)TwoPass: It is a two pass algorithm similar to BregmanFilter . Here the algorithm knows φfrom one
pass and in the second pass it computes the sampling probability by using φinstead ofφifor alli∈[n].
2)LWCS: It is the offline lightweight coreset algorithm as stated in Bachem et al. (2015).
3)RelCoreset : It is an Offline sampling method that uses bi-criteria approximation Lucic et al. (2016). The
resultant coreset ensures relative error approximation.
4)Leverage : It is just a heuristic online sampling algorithm. Here we do Online leverage score sampling
method as in Chhaya et al. (2020a). Note that leverage score sampling is only guaranteed to preserve the
rank of the matrix, not the cluster structure.
5)Online-K-Means : This is also a heuristic online sampling algorithm. We run the Algorithm-3 in Liberty
et al. (2016) which returns ˜O(k)forkcenters. We consider the returned set as our subset of data and
compare its performance with our coreset. In order to make this subset meaningful for the comparison we
reweigh the selected points with inverse of the sampling probability (just like other sampling techniques).
Notice that while the ˜O(k)points returned by the algorithm have a bicriteria approximation guarantee, there
is no theoretical guarantee that is available after doing a k-clustering of these points.
Please note that except for Leverage andOnline-K-Means , none of the other sampling method qualify as a
baseline for comparison with our method BregFilter . Further, note that even though any offline sampling
method can be treated as a streaming sampling method using merge-and-reduce technique, however as these
method allows the algorithm to discard a sample which might have been selected in the coreset at some
previous step, so these sampling methods do not qualify as baselines for our online sampling method. As
there are very limited baselines, so we compare with other offline methods such as LWCSandRelCoreset .
22Published in Transactions on Machine Learning Research (07/2022)
We compare the performance on the following datasets:
1)KDD(BIO-TRAIN) :145,751points with 74features. We consider k={100,200}and squared Euclidean as
the Bregman divergence (see Figure 1). We further consider that the data points are arriving in a streaming
fashion. On this dataset we compare the performance of BregFilter with other algorithms.
2)MNIST:60,000points in 784dimension digits dataset. Here we consider k={5,10,25,50}and rel-
ative entropy as Bregman divergence (see Figure 2). On this dataset we compare a heuristic version of
NonParametricFilter with Uniform andTwoPass sampling algorithms.
Using each of the above described algorithm, we first subsample coresets of different sizes. Once we have
the coreset, we run the weighted k-means++ Arthur & Vassilvitskii (2007) on them to obtain the centers.
We then use these centers and compute the quantization error ( Cs) on the full data set. We also compute
quantization error by running k-means++ on the full data set ( Cf). Finally we report the Relative-Error
η=|Cs−Cf|/Cf.
In the figure 1 the Y-axis represents the relative error ηand the X-axis represents the expected sample
size which is in terms of percentage of the full data. For every expected sample size, we run 10random
instances. Using a parameter that controls the sample size, we ensure that the expected sample size of the
10random instances are equal. Based on these we compute the average ηof these 10events and report
it in the plot. The figure shows the change in ηwith the increase in the coreset size for k={100,200}
onKDD(BIO-TRAIN) datasets for k-means clustering. As the coreset size increases the ηdecreases for all
the algorithms. As expected, the offline methods LWCSand RelCoreset perform relatively better than our
BregFilter .BregFilter clearly outperforms the baseline Online-K-Means in terms of relative error at all
sample sizes. We also compare with online version of Leverage score sampling which, empirically, appears to
be competitive with our method. However, recall that the coresets from Leverage do not have any provable
guarantee and it is not difficult to show a bad input where Leverage will fail to return reasonable clusters,
e.g. if the data spans a low rank space but has a large number of clusters. We present a toy example in the
appendix.
Alsonotethat Leverage andOnline-k-Means haveupdatetimesof O(d2)andO(kd)respectively, asopposed
to theO(d)update time of BregFilter . On KDD(BIO-TRAIN) fork= 100and expected coreset size as 1%
of the data, the average running time of BregFilter is 2 seconds, Online-k-Means is 379 seconds and
Leverage is 5264 seconds.
1 2 3
% of data0.10.20.30.4Relative Error 
KDD: BIO-TRAIN (k=100)
1 2 3
% of data0.10.20.30.40.5Relative Error 
KDD: BIO-TRAIN (k=200)TwoPass
LeverageLWCS
RelCoresetOnline-K-Means
BregFilter
Figure 1: Relative error v/s coreset size for squared Euclidean k-means clustering. We do not hope to beat
the offline methods. However, we are at par with them. We are one pass.
23Published in Transactions on Machine Learning Research (07/2022)
For comparing the performance of non-parametric coreset2. We run TwoPass,Uniform and NP-Filter
onMNIST. To the best of our knowledge there are no other baselines to compare with. In Uniform we
sample each point with probability r/n, whereris a parameter used to control the coreset size and n
is the number of input points. Now to capture the notion of coreset for non-parametric clustering we
runk-means++ on every coreset from each method for various values of k={5,10,25,50}. Finally we
compute the relative error ηas described above. The computation of empirical sensitivity scores makes
theNonParametricFilter computationaly expensive. The running time is ˜O(n2). So we run NP-Filter
(heuristicversionof NonParametricFilter )whereweusetheupperboundoftheexpectedbarriersensitivity
scores as in Lemma 5.6 to sample every point, i.e., ˜ru
i= ˜ru
i=2fMiφi(ai)
µiϵ/summationtext
j≤ifMj
φj(aj)+12
µiϵ(i−1)for alli∈[n]. Hence
therunningtimeof NP-Filter isjustO(nd)andthesamplingcomplexityofthereturnedcoresetiscontrolled
by the distortion parameter ϵ. Now for every value of ϵwe run 5instances of the algorithms and report the
averageηfor every value of k. Notice that as we increase ϵ, theηalso increases. This is due the fact that
the coreset size inversely depends on ϵ, so a highϵresults to a smaller coreset and as a result it incurs higher
η. It is evident from figure 2 that even our heuristic outperforms the Uniform and performs equivalent to
TwoPass.
10 20 30 40 50
Number of Centers (k)0.050.10Relative Error 
=0.25
Uniform
TwoPassNP-Filter
10 20 30 40 50
Number of Centers (k)0.00.10.2Relative Error 
=0.5
Uniform
TwoPassNP-Filter
10 20 30 40 50
Number of Centers (k)0.00.2Relative Error 
=0.75
Uniform
TwoPassNP-Filter
10 20 30 40 50
Number of Centers (k)0.250.500.75Relative Error 
=1
Uniform
TwoPassNP-Filter
Figure 2: Change in ηwith respect to #centers kfor various ϵ.
Please note here we compare the performance in terms of relative error approximation, which is stronger
than our actual additive error theoretical guarantees. The plot shows that even with small coreset sizes we
get tight relative error approximations and thus supporting the theoretical guarantees.
8 Conclusion
Here we presented online coreset for clustering based on Bregman divergences. We also present the first
algorithm for non-parametric coreset for the same problem. The algorithm leverages upon additive error
approximation, and uses barrier functions and empirical sensitivity scores.
Broader Impact Statement
We do not foresee any potential negative impact.
2coreset for non-parametric clustering.
24Published in Transactions on Machine Learning Research (07/2022)
Acknowledgments
Anirban would like to acknowledge the following grants and the corresponding funding agencies— Google
India Faculty Award, Cisco University grant, SERB-MATRICS grant, SERB-CORE research grant. Supra-
tim acknowledges the generous funding from the European Union’s Horizon 2020 research and innovation
programmed under grant agreement No 682203 -ERC-[ Inf-Speed-Tradeoff].
References
Marcel R Ackermann and Johannes Blömer. Coresets and approximate clustering for bregman divergences.
InProceedings of the twentieth annual ACM-SIAM symposium on Discrete algorithms , pp. 1088–1097.
SIAM, 2009. [4, 7, 16, and 33.]
Pankaj K Agarwal, Sariel Har-Peled, Kasturi R Varadarajan, et al. Geometric approximation via coresets.
Combinatorial and computational geometry , 52:1–30, 2005. [4 and 6.]
David Arthur and Sergei Vassilvitskii. k-means++ the advantages of careful seeding. In Proceedings of the
eighteenth annual ACM-SIAM symposium on Discrete algorithms , pp. 1027–1035, 2007. [23 and 37.]
Olivier Bachem, Mario Lucic, and Andreas Krause. Coresets for nonparametric estimation-the case of dp-
means. In ICML, pp. 209–217, 2015. [1, 2, 3, 20, and 22.]
Olivier Bachem, Mario Lucic, S Hamed Hassani, and Andreas Krause. Uniform deviation bounds for k-means
clustering. In International Conference on Machine Learning , pp. 283–291. PMLR, 2017a. [1, 6, 21, 22,
and 36.]
Olivier Bachem, Mario Lucic, and Andreas Krause. Practical coreset constructions for machine learning.
stat, 1050:4, 2017b. [6.]
Olivier Bachem, Mario Lucic, and Andreas Krause. Scalable k-means clustering via lightweight coresets. In
Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining ,
pp. 1119–1127, 2018a. [2, 6, 11, and 39.]
Olivier Bachem, Mario Lucic, and Silvio Lattanzi. One-shot coresets: The case of k-clustering. In Interna-
tional conference on artificial intelligence and statistics , pp. 784–792, 2018b. [6 and 11.]
Mihai Badoiu and Kenneth L Clarkson. Smaller core-sets for balls. In SODA, volume 3, pp. 801–802, 2003.
[4 and 6.]
Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with bregman
divergences. Journal of machine learning research , 6(Oct):1705–1749, 2005. [1 and 4.]
Artem Barger and Dan Feldman. Deterministic coresets for k-means of big sparse data. Algorithms , 13(4):
92, 2020. [6.]
Joshua Batson, Daniel A Spielman, and Nikhil Srivastava. Twice-Ramanujan Sparsifiers. SIAM Journal on
Computing , 41(6):1704–1721, 2012. [2, 6, and 15.]
CenkBaykal, LucasLiebenwein, IgorGilitschenski, DanFeldman, andDanielaRus. Data-dependentcoresets
for compressing neural networks with applications to generalization bounds. In International Conference
on Learning Representations , 2018. [2 and 18.]
Aditya Bhaskara and Aravinda Kanchana Rwanpathirana. Robust algorithms for online k-means clustering.
InAlgorithmic Learning Theory , pp. 148–173, 2020. [6.]
Gérard Biau, Luc Devroye, and Gábor Lugosi. On the performance of clustering in hilbert spaces. IEEE
Transactions on Information Theory , 54(2):781–790, 2008. [6.]
Christos Boutsidis and Malik Magdon-Ismail. Deterministic feature selection for k-means clustering. IEEE
Transactions on Information Theory , 59(9):6099–6110, 2013. [6.]
25Published in Transactions on Machine Learning Research (07/2022)
Vladimir Braverman, Dan Feldman, and Harry Lang. New frameworks for offline and streaming coreset
constructions. arXiv preprint arXiv:1612.00889 , 2016. [6.]
Rachit Chhaya, Jayesh Choudhari, Anirban Dasgupta, and Supratim Shit. Streaming coresets for symmetric
tensor factorization. In International Conference on Machine Learning , pp. 1855–1865. PMLR, 2020a. [22
and 38.]
Rachit Chhaya, Anirban Dasgupta, and Supratim Shit. On coresets for regularized regression. In Interna-
tional Conference on Machine Learning , pp. 1866–1876. PMLR, 2020b. [6 and 28.]
Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. Dimensionality
reduction for k-means clustering and low rank approximation. In Proceedings of the forty-seventh annual
ACM symposium on Theory of computing , pp. 163–172, 2015. [6.]
Michael B Cohen, Cameron Musco, and Jakub Pachocki. Online row sampling. In Approximation, Ran-
domization, and Combinatorial Optimization. Algorithms and Techniques (APPROX/RANDOM 2016) .
Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016. [2 and 12.]
Devdatt P Dubhashi and Alessandro Panconesi. Concentration of measure for the analysis of randomized
algorithms . Cambridge University Press, 2009. [28.]
Dan Feldman and Michael Langberg. A unified framework for approximating and clustering data. In
Proceedings of the forty-third annual ACM symposium on Theory of computing , pp. 569–578. ACM, 2011.
[2, 6, and 12.]
Dan Feldman, Mikhail Volkov, and Daniela Rus. Dimensionality reduction of massive sparse datasets using
coresets. In Advances in Neural Information Processing Systems , pp. 2766–2774, 2016. [6.]
Dan Feldman, Melanie Schmidt, and Christian Sohler. Turning big data into tiny data: Constant-size
coresets for k-means, pca, and projective clustering. SIAM Journal on Computing , 49(3):601–657, 2020.
[6.]
Sariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median clustering. In Proceedings
of the thirty-sixth annual ACM symposium on Theory of computing , pp. 291–300. ACM, 2004. [4 and 6.]
David Haussler. Decision theoretic generalizations of the pac model for neural net and other learning
applications. Information and computation , 100(1):78–150, 1992. [5.]
Lingxiao Huang and Nisheeth K. Vishnoi. Coresets for clustering in euclidean spaces: Importance sampling
is nearly optimal, 2020. [2.]
Ari Kobren, Nicholas Monath, Akshay Krishnamurthy, and Andrew McCallum. A hierarchical algorithm
for extreme clustering. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining , pp. 255–264, 2017. [2.]
Brian Kulis and Michael I Jordan. Revisiting k-means: new algorithms via bayesian nonparametrics. In
Proceedings of the 29th International Coference on International Conference on Machine Learning , pp.
1131–1138, 2012. [5.]
Michael Langberg and Leonard J Schulman. Universal ε-approximators for integrals. In Proceedings of the
twenty-first annual ACM-SIAM symposium on Discrete Algorithms , pp. 598–607. SIAM, 2010. [5 and 6.]
Silvio Lattanzi and Sergei Vassilvitskii. Consistent k-clustering. In International Conference on Machine
Learning , pp. 1975–1984, 2017. [6.]
Edo Liberty, Ram Sriharsha, and Maxim Sviridenko. An algorithm for online k-means clustering. In 2016
Proceedings of the eighteenth workshop on algorithm engineering and experiments (ALENEX) , pp. 81–89.
SIAM, 2016. [6 and 22.]
26Published in Transactions on Machine Learning Research (07/2022)
Mario Lucic, Olivier Bachem, and Andreas Krause. Strong coresets for hard and soft bregman clustering
with applications to exponential family mixtures. In Artificial intelligence and statistics , pp. 1–9, 2016.
[2, 4, 6, 7, 11, 16, 22, 29, and 33.]
Jack Sherman and Winifred J Morrison. Adjustment of an inverse matrix corresponding to a change in one
element of a given matrix. The Annals of Mathematical Statistics , 21(1):124–127, 1950. [30.]
David P Woodruff et al. Sketching as a tool for numerical linear algebra. Foundations and Trends ®in
Theoretical Computer Science , 10(1–2):1–157, 2014. [6 and 29.]
27Published in Transactions on Machine Learning Research (07/2022)
A Appendix
In this paper we use the following theorems in our analysis.
Theorem A.1 (Bernstein’s inequality 2009) .Let the scalar random variables x1,x2,···,xnbe independent
that satisfy∀i∈[n],|xi−E[xi]|≤b. LetX=/summationtext
ixiand letσ2=/summationtext
iσ2
ibe the variance of X. Then for any
t>0,
Pr/parenleftbig
X >E[X] +t/parenrightbig
≤exp/parenleftbigg
−t2
2σ2+bt/3/parenrightbigg
Theorem A.2 (2020b).LetAbe the dataset, Xbe the query space of dimension D, and for x∈X, let
fx(·)be the cost function. Let sjbe the sensitivity of the jthrow of A, and the sum of sensitivities be S.
Let(ϵ,δ)∈(0,1). Letrbe such that
r≥O/parenleftigS
ϵ2(Dlog1
ϵ+ log1
δ)/parenrightig
Cbe a matrix of rrows, each sampled i.i.d from Asuch that each ˜ai∈Cis chosen to be aj, with weight
S
rsj, with probabilitysj
S, forj∈[n]. Then Cis anϵ-coreset of Afor function f(), with probability at least
1−δ.
We use the above Theorem to bound our coreset size. Note that the Theorem considers a multinomial
sample where a point ˜aiin coreset Cisajand weightS
rsjforj∈[n]with probabilitysj
S. Instead in our
approach we get ˜aiasai, with weight 1/min{1,rsi}, with probability min{1,rsi}or it is∅, with weight 0,
with probability 1−min{1,rsi}. However, the same Theorem as above applies.
A.1 Online Coresets for Clustering
Here we discuss the proofs of the supporting lemmas to claim of the main theorem
A.1.1 Proof of Lemma 4.5
Proof.For some fixed (query) X∈Rk×dconsider the following random variable.
wi=/braceleftigg
(1/pi−1)fX(ai)with probability pi
−fX(ai) with probability (1−pi)
Note that E[wi] = 0and withpi= 1we get|wi|= 0. The algorithm uses the sampling probability
pi= min{rli,1}. Now we bound the term |wi|. In the case when pi<1andaiis sampled we have,
|wi| ≤1
pifX(ai)
=fX(ai)
rli
(i)
≤(fX(Ai−1) +fφi(Ai))fX(ai)
rfX(ai)
=(fX(Ai−1) +fφi(Ai))
r
≤(fX(A) +fφ(A))
r
(i)is by replacing liwith a smaller term,fX(ai)
fX(Ai−1+fφi(Ai)). In the last inequality is because fφ(A)≥fφi(Ai).
Hereφ=φnis the mean of the entire data A. Next if the point aiis not sampled then we know for sure
thatpi<1, hence we have that,
1> rli
28Published in Transactions on Machine Learning Research (07/2022)
≥rfX(ai)
(fX(Ai−1) +fφi(Ai))
∴fX(ai)≤(fX(A) +fφ(A))
r
Say,b= (fX(A) +fφ(A))/r, so|wi|≤b. Next we bound the var (/summationtext
i≤nwi) =/summationtext
i≤nE[w2
i]. Note that for a
single term when pi<1,E[w2
i]is,
E[w2
i] =/parenleftbig
pi(1/pi−1)2+ (1−pi)/parenrightbig
fX(ai)2
≤1
pifX(ai)2
=fX(ai)2
rli
≤(fX(Ai−1) +fφi(Ai))fX(ai)2
rfX(ai)
=(fX(Ai−1) +fφi(Ai))fX(ai)
r
≤fX(ai)(fX(A) +fφ(A))
r
So we get,
var/parenleftig/summationdisplay
i≤nwi/parenrightig
=/summationdisplay
i≤nE[w2
i]
≤/summationdisplay
i≤nfX(ai)(fX(A) +fφ(A))
r
=fX(A)(fX(A) +fφ(A))
r
≤(fX(A) +fφ(A))2
r
Now by applying Bernstein’s inequality (A.1) on/summationtext
i≤nwiwitht=ϵ(fX(A) +fφ(A))we bound the proba-
bility P=Pr/parenleftig
|fX(C,Ω)−fX(A)|≥ϵ(fX(A) +fφ(A))/parenrightig
as follows,
P≤exp/parenleftbigg−ϵ2(fX(A) +fφ(A))2
ϵ(fX(A) +fφ(A))2/3r+ 2(fX(A) +fφ(A))2/r/parenrightbigg
= exp/parenleftbigg−rϵ2
(ϵ/3 + 2)/parenrightbigg
So to get the above event with at least 0.99probability it is enough to set rto beO/parenleftig
1
ϵ2/parenrightig
. Note that the
above is guaranteed for a fixed X∈Rk×d.
Now we show that coreset (C,Ω)can be made strong coreset by taking a union bound over a set of queries.
We take a union bound over the ϵ/2-net of Rk×dWoodruff et al. (2014); Lucic et al. (2016). Such a net
will have at most O(ϵ−dklogk)queries. To ensure a strong a coreset guarantee it is enough to set ras
O/parenleftig
dk(logk) log(1/ϵ)
ϵ2/parenrightig
.
Intuition for log(fX(A)):Consider the following input stream — every ithat is a multiple of√nhas the
property that fX(ai)=fX(Ai−1), i.e. this point’s contribution to the current cost is more than the total
contributions of all the previous input points. For all other j,fX(aj)is, say, small. Any online algorithm
(i.e. one that makes irrevocable decisions without looking at the future) will need to assign a constant
probability to every ithat is a multiple of√n. Hence the resulting coreset size is at least√n. Note that
fX(A) =O(c√n), and hence log(fX(A))is a tight bound on the coreset size in this example.
29Published in Transactions on Machine Learning Research (07/2022)
Lemma A.3. For aµ-similar Bregman divergence, for all i∈[n]every incoming points aiwe have,
sup
X∈Rk×dfX(ai)
fX(Ai−1) +τfφi(Ai)≤2fMiφi(ai)
µiτ/summationtext
j≤ifMjφj(aj)+8
µiτ(i−1)(17)
Proof.
fX(ai)
fX(Ai−1) +τfφi(Ai)(i)
≤fMi
X(ai)
fX(Ai−1) +τfφi(Ai)
≤2fMiφi(ai) + 2fMi
X(φi)
fX(Ai−1) +τfφi(Ai)
≤2fMiφi(ai) +4
i−1fMiφi(Ai−1) +4
i−1fMi
X(Ai−1)
fX(Ai−1) +τfφi(Ai)
(ii)
≤2fMiφi(ai) +4
i−1fMiφi(Ai−1) +4
i−1fMi
X(Ai−1)
µi(fMi
X(Ai−1) +τfMiφi(Ai))
=2fMiφi(ai) +4
i−1fMiφi(Ai−1)
µi(fMi
X(Ai−1) +τfMiφi(Ai))+4
i−1fMi
X(Ai−1)
µi(fMi
X(Ai−1) +τfMiφi(Ai))
≤2fMiφi(ai) +4
i−1fMiφi(Ai−1)
µi(fMi
X(Ai−1) +τfMiφi(Ai))+4
µi(i−1)
≤2fMiφi(ai)
µiτfMiφi(Ai)+4
i−1fMiφi(Ai−1)
µiτfMiφi(Ai)+4
µi(i−1)
(iii)
≤2fMiφi(ai)
µiτfMiφi(Ai)+4
µi(i−1)([
1]τ+ 1)
≤2fMiφi(ai)
µiτ/summationtext
j≤ifMjφj(aj)+4
µiτ(i−1)([
1]τ+ 1)
Further with an analysis similar to second claim in lemma 4.4 we get/summationtext
i≤nli≤8 logn+
4 log/parenleftbig
fM
φ(A)/parenrightbig
−4 log/parenleftbig
fM2φ2(a2)/parenrightbig
µτ. Now the algorithm samples point based on its liscore. Then, applying Bern-
stein’s inequality on sum of all the random variables defined as,
wi=/braceleftigg
(1/pi−1)fX(ai)with probability pi
−fX(ai) with probability (1−pi).
Finally, by taking a union bound over an ϵ-net we can prove the claim in corollary 4.1.
A.2 Coresets for Non-Parametric Clustering
A.2.1 Proof of Lemma 5.3
Proof.The proof is fairly straight forward. Using simple algebra (similar to (Sherman & Morrison, 1950))
we have,
1
q+w−ur=1
q+w+ur(q+w)−2
1−ur(q+w)−1
=1
q+w+u
1−u(q+w)−1
30Published in Transactions on Machine Learning Research (07/2022)
1
q+w−vr=1
q+w+vr(q+w)−2
1−vr(q+w)−1
=1
q+w+v
1−v(q+w)−1
So we get,
E/bracketleftbiggs
t+w−s
q+w/bracketrightbigg
=pu+ (1−p)v−uv
(1−u)(1−v)/parenleftbiggs
q+w/parenrightbigg
A.2.2 Proof of Lemma 5.4
Proof.In order to show this we define the following notations. Let Airepresent the set of ipoints seen by
the algorithm so far. For some Xand for some j≤n, we define two scalars ζu
i,jandζl
i,jas follows,
ζu
i,j=ϵ
2fX(Ai) + (1 +ϵ
2)fX(Aj)andζl
i,j=−ϵ
2fX(Ai) + (1−ϵ
2)fX(Aj)
So we have ζu
i,i= (1 +ϵ)fX(Ai)andζl
i,i= (1−ϵ)fX(Ai). It is clear that for j≤i−1we haveζu
i−1,j≥ζu
j,j
andζl
i−1,j≤ζl
j,j. Further two more scalars γu
i,jandγl
i,jare defined as follows,
γu
i,j=ζu
i,j−fX(Cj,Ωj)andγl
i,j=fX(Cj,Ωj)−ζl
i,j
Note thatγu
i,i= (1 +ϵ)fX(Ai)−fX(Ci,Ωi)andγl
i,i=fX(Ci,Ωi)−(1−ϵ)fX(Ai). Forj≤i−1we get
γu
i−1,j≥γu
j,jandγl
i−1,j≥γl
j,j. Let,dj+1=fX(aj+1)
pj+1. Ifpj+1<1, then we have pj+1≥cu˜su
j+1≥cusu
j+1, and
hence we have the following for upper barrier,
pj+1≥cufX(aj+1)
γu
j,j+ϵfφj+1(Aj+1)≥cufX(aj+1)
γu
i−1,j+ϵfφj+1(Aj+1)≥cufX(aj+1)
γu
i−1,j+ϵfφi(Ai).
Therefore,
dj+1
γu
i−1,j+ϵfφi(Ai)≤1
cu.
Letdj+1
γu
i−1,j+ϵfφi(Ai)=hu
j+1, which is bounded by1
cu. Similarly for the lower barrier we have,
pj+1≥clfX(aj+1)
γl
j,j+ϵfφj+1(Aj+1)≥clfX(aj+1)
γl
i−1,j+ϵfφj+1(Aj+1)≥clfX(aj+1)
γl
i−1,j+ϵfφi(Ai).
So we get,
dj+1
γl
i−1,j+ϵfφi(Ai)≤1
cl.
A.2.3 Proof of Lemma 5.5
Proof.For brevity, going forward, we denote hl
j+1=dj+1
γl
i−1,j+ϵfφi(Ai). Recall that Lemma5.4 showed that
hl
j+1≤1
cl.
To apply Lemma 5.3 we set q=γu
i−1,j,r=dj+1/hu
j+1,s=fX(ai)andw=ϵfφi(Ai). Further let u=
hu
j+1(1−pj+1(1 +ϵ/2)),v=−hu
j+1pj+1(1 +ϵ/2)andp=pj+1. Note that from the above substitution we
getr
q+w= 1andt=γu
i−1,j+1. To prove the corollary we need the RHS of the lemmapu+(1−p)v−uv
(1−u)(1−v)≤0.
After substituting every term we we get,pj+1hu
j+1(hu
j+1(1+ϵ/2−pj+1(1+ϵ/2)2)−ϵ/2)
(1−u)(1−v). Now ifpj+1≥1/(1 +ϵ/2)
31Published in Transactions on Machine Learning Research (07/2022)
then this term is non positive, else when cu=2
ϵ+ 1the term remains non-positive. As πj+1∈{0,1}is the
sampling choice made by the algorithm for aj+1. So we have,
Eπj+1/bracketleftigg
fX(ai)
γu
i−1,j+1+ϵfφi(Ai)/bracketrightigg
≤fX(ai)
γu
i−1,j+ϵfφi(Ai)
A similar analysis also follows for the lower barrier. We use Lemma 5.3 by setting q=γl
i−1,j,r=
dj+1/hl
j+1,s=fX(ai)andw=ϵfφi(Ai). Further let u=−hl
j+1(1−pj+1(1−ϵ/2)),v=hl
j+1pj+1(1−ϵ/2))
andp=pj+1. By these substitution we have,r
q+w= 1and the random variable t=γl
i−1,j+1. Let
πj+1∈{0,1}is the random sampling choice made by the algorithm for aj+1. Now forcl=2
ϵ−1we get,
Eπj+1/bracketleftigg
fX(ai)
γl
i−1,j+1+ϵfφi(Ai)/bracketrightigg
≤fX(ai)
γl
i−1,j+ϵfφi(Ai)
A.2.4 Proof of Lemma 5.6
Proof.We use Lemma 5.5 to get the expected upper bound on the sensitivity scores i.e.,
sup
XfX(ai)
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +ϵfφi(Ai)
sup
XfX(ai)
fX(Ci−1,Ωi−1)−(1−ϵ)fX(Ai−1) +ϵfφi(Ai)
Let(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) =fX(Au
i−1)wherefX(Au
i−1) =/summationtext
j≤i−1fX(au
j). Here each term
fX(au
j) = (1 +ϵ−p−1
j)fX(aj)ifajis present in Ci−1elsefX(au
j) = (1 +ϵ)fX(aj). Now the expected upper
bound on the upper barrier sensitivity score can be bounded as follows.
EΠi−1/bracketleftbiggfX(ai)
fX(Au
i−1) +ϵfφi(Ai)/bracketrightbigg(i)
≤EΠi−1/bracketleftbiggfMi
X(ai)
fX(Au
i−1) +ϵfφi(Ai)/bracketrightbigg
(ii)
≤Eπi−1/bracketleftbigg/bracketleftig
2fMiφi(ai) +4
i−1/summationtext
aj∈Ai−1[fMiφi(aj) +fMi
X(aj)]/bracketrightig
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
=Eπi−1/bracketleftbigg/bracketleftig
2fMiφi(ai) +4
i−1/summationtext
aj∈Ai−1fMiφi(aj)/bracketrightig
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
+Eπi−1/bracketleftbigg 4
i−1fMi
X(Ai−1)
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
(iii)=Eπi−1/bracketleftbigg2fMiφi(ai) +4
i−1fMiφi(Ai−1)
γu
i−1,i−1+ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
+Eπi−1/bracketleftbigg 4
i−1fMi
X(Ai−1)
γu
i−1,i−1+ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
(iv)
≤Eπi−2/bracketleftbigg2fMiφi(ai) +4
i−1fMiφi(Ai−1)
γu
i−1,i−2+ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−3/bracketrightbigg
+Eπi−2/bracketleftbigg 4
i−1fMi
X(Ai−1)
γu
i−1,i−2+ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−3/bracketrightbigg
(v)
≤Eπ0/bracketleftbigg2fMiφi(ai) +4
i−1fMiφi(Ai−1)
γu
i−1,0+ϵfφi(Ai)/bracketrightbigg
+Eπ0/bracketleftbigg 4
i−1fMi
X(Ai−1)
γu
i−1,0+ϵfφi(Ai)/bracketrightbigg
32Published in Transactions on Machine Learning Research (07/2022)
=2fMiφi(ai) +4
i−1fMiφi(Ai−1)
ϵ/2fX(Ai−1) +ϵfφi(Ai)+4
(i−1)fMi
X(Ai−1)
ϵ/2fX(Ai−1) +ϵfφi(Ai)
(vi)
≤2fMiφi(ai) +4
i−1fMiφi(Ai−1)
µiϵ(0.5fMi
X(Ai−1) +fMiφi(Ai))+4
(i−1)fMi
X(Ai−1)
µiϵ(0.5fMi
X(Ai−1) +fMiφi(Ai))
≤2fMiφi(ai)
µiϵfMiφi(Ai)+4fMiφi(Ai−1)
µiϵ(i−1)fMiφi(Ai)+8fMi
X(Ai−1)
µiϵ(i−1)fMi
X(Ai−1)
(vii)
≤2fMiφi(ai)
µiϵfMiφi(Ai)+4
µiϵ(i−1)+8
µiϵ(i−1)
≤2fMiφi(ai)
µiϵ/summationtext
j≤ifMjφj(aj)+12
µiϵ(i−1)
The inequality (i)is by upper bounding Bregman divergence by squared Mahalanobis distance. The inequal-
ity(ii)is due to applying triangle inequality on the numerator, (a2+b2)≤2(a2+b2). The (iii)equality is
by replacing the denominator with the above definition. The (iv)inequality is by applying the supporting
Lemma 5.3. By recursively applying Lemma 5.3 we get the inequality (v)which is independent of the ran-
dom choices made by the algorithm. The inequality (vi)is by using the lower bound on the denominator.
The inequality (vii)an upper bound on the second and the third term. In the final inequality we use the
fact that for any µsimilar Bregman divergence from Ackermann & Blömer (2009); Lucic et al. (2016) we
haveMj⪯Miforj≤i(Lemma 4.1). Further by the property of Bregman divergence we know that
fφi(Ai−1)≥fφi−1(Ai−1). Hence we have fMiφi(Ai)≥/summationtext
j≤ifMjφj(aj). Notice that the above analysis is also
true for all X. Hence we have this upper bound for all Xwith at most ncenters. So we have the,
EΠi−1/bracketleftbiggfX(ai)
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +fφi(Ai)/bracketrightbigg
≤2fφi(ai)
ϵ/summationtext
j≤ifφj(aj)+12
ϵ(i−1)
Now let (fX(Ci−1,Ωi−1)−(1−ϵ)fX(Ai−1) =fX(Al
i−1)wherefX(Al
i−1) =/summationtext
j≤i−1fX(al
j). Here each term
fX(al
j) = (p−1
j−1+ϵ)fX(aj)ifajis present in Ci−1elsefX(al
j) = (−1+ϵ)fX(aj). Now the expected upper
bound on the lower sensitivity score is,
EΠi−1/bracketleftbiggfX(ai)
fX(Al
i−1) +ϵfφi(Ai)/bracketrightbigg(i)
≤EΠi−1/bracketleftbiggfMi
X(ai)
fX(Al
i−1) +ϵfφi(Ai)/bracketrightbigg
(ii)
≤Eπi−1/bracketleftbigg/bracketleftig
2fMiφi(ai) +4
i−1/summationtext
aj∈Ai−1[fMiφi(aj) +fMi
X(aj)]/bracketrightig
fX(Ci−1,Ωi−1)−(1−ϵ)fMi
X(Ai−1) +ϵfφii(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
=Eπi−1/bracketleftbigg/bracketleftig
2fMiφi(ai) +4
i−1fMiφi(Ai−1)/bracketrightig
fX(Ci−1,Ωi−1)−(1−ϵ)fX(Ai−1) +ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
+Eπi−1/bracketleftbigg 4
ϵ(i−1)fMi
X(Ai−1)
fX(Ci−1,Ωi−1)−(1−ϵ)fX(Ai−1) +ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
(iii)=Eπi−1/bracketleftbigg2fMiφi(ai) +4
i−1fMiφi(Ai−1)
γl
i−1,i−1+ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
+Eπi−1/bracketleftbigg 4
i−1fMi
X(Ai−1)
γl
i−1,i−1+ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−2/bracketrightbigg
(iv)
≤Eπi−2/bracketleftbigg2fMiφi(ai) +4
i−1fMiφi(Ai−1)
γl
i−1,i−2+ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−3/bracketrightbigg
+Eπi−3/bracketleftbigg 4
i−1fMi
X(Ai−1)
γl
i−1,i−2+ϵfφi(Ai)/vextendsingle/vextendsingle/vextendsingleΠi−3/bracketrightbigg
33Published in Transactions on Machine Learning Research (07/2022)
(v)
≤Eπ0/bracketleftbigg2fMiφi(ai) +4
i−1fMiφi(Ai−1)
γl
i−1,0+ϵfφi(Ai)/bracketrightbigg
+Eπ0/bracketleftbigg 4
i−1fMi
X(Ai−1)
γl
i−1,0+ϵfφi(Ai)/bracketrightbigg
=2fMiφi(ai) +4
i−1fMiφi(Ai−1)
ϵ/2fX(Ai−1) +ϵfφi(Ai)+4
(i−1)fMi
X(Ai−1)
ϵ/2fX(Ai−1) +ϵfφi(Ai)
(vi)
≤2fMiφi(ai) +4
i−1fMiφi(Ai−1)
µiϵ(0.5fMi
X(Ai−1) +fMiφi(Ai))+4
(i−1)fMi
X(Ai−1)
µiϵ(0.5fMi
X(Ai−1) +fMiφi(Ai))
≤2fMiφi(ai)
µiϵ/summationtext
j≤ifMjφj(aj)+12
µiϵ(i−1)
The inequality (i)is by upper bounding Bregman divergence by squared Mahalanobis distance. The in-
equality (ii)is due to applying triangle inequality on the numerator. The (iii)equality is by replacing the
denominator with the above definition. The (iv)inequality is by applying the supporting Lemma 5.3. By
recursively applying Lemma 5.3 we get the inequality (v)which is independent of the random choices made
by the algorithm. The inequality (vi)is by using the lower bound on the denominator and from here the
analysis is same as the upper bound analysis. So we have the following,
EΠi−1/bracketleftbiggfX(ai)
(1 +ϵ)fX(Ai−1)−fX(Ci−1,Ωi−1) +fφi(Ai)/bracketrightbigg
≤2fMiφi(ai)
µiϵ/summationtext
j≤ifMjφj(aj)+12
µiϵ(i−1)
A.2.5 Proof of Lemma 5.7
Proof.First we bound the expected sampling probability of each aii.e.,EΠi−1[pi].
EΠi−1[pi] =cuEΠi−1[˜su
i] +clEΠi−1[˜sl
i]
(i)
≤2cufMiφi(ai)
µiϵ/summationtext
j≤ifMjφj(aj)+12cu
µiϵ(i−1)+2clfMiφi(ai)
ϵ/summationtext
j≤ifMjφj(aj)+12cl
µiϵ(i−1)
≤8fMiφi(ai)
µiϵ2/summationtext
j≤ifMjφj(aj)+48
µiϵ2(i−1)
The inequality (i)is because the oracle returns a tight upper bound on the actual barrier sensitivity scores.
Now we bound the total expected sample size,
/summationdisplay
3≤i≤nE[pi]≤/summationdisplay
3≤i≤n/parenleftigg
8fMiφi(ai)
µiϵ2/summationtext
j≤ifMjφj(aj)+48
µiϵ2(i−1)/parenrightigg
≤48 logn
µiϵ2+/summationdisplay
3≤i≤n/parenleftigg
8fMiφi(ai)
µiϵ2/summationtext
j≤ifMjφj(aj)/parenrightigg
Because of the subtlety that E[p2]≥1, hence in the above analysis we bound sum from 3ton. Let the term
fMiφi(ai)/summationtext
j≤ifMj
φj(aj)=qi≤1. In the following analysis we bound summation of this term i.e.,/summationtext
i≤nqi. For that
consider the term/summationtext
j≤ifMjφj(aj)as follows,
/summationdisplay
j≤ifMj
φj(aj) =/summationdisplay
j≤i−1fMj
φj(aj)/parenleftbigg
1 +fMiφi(ai)
/summationtext
j≤i−1fMjφj(aj)/parenrightbigg
≥/summationdisplay
j≤i−1fMj
φj(aj)/parenleftbigg
1 +fMiφi(ai)
/summationtext
j≤ifMjφj(aj)/parenrightbigg
34Published in Transactions on Machine Learning Research (07/2022)
=/summationdisplay
j≤i−1fMj
φj(aj)(1 +qi)
(i)
≥exp(qi/2)/summationdisplay
j≤i−1fMj
φj(aj)
exp(qi/2)≤/summationtext
j≤ifMjφj(aj)
/summationtext
j≤i−1fMjφj(aj)
In(i)we use the fact that, for qi≤1,(1 +qi)≥exp(qi/2). Now as we know that/summationtext
j≤ifMjφj(aj)≥
/summationtext
j≤i−1fMjφj(aj)hence following product results into a telescopic product and we get,
/productdisplay
3≤i≤nexp(qi/2)≤/summationtext
j≤nfMjφj(aj)
fM2φ2(a2)
≤fM
φ(A)
fM2φ2(a2)
Now taking login both sides we get/summationtext
3≤i≤nqi≤2 log/parenleftbig
fM
φ(A)/parenrightbig
−2 log/parenleftbig
fM2φ2(a2)/parenrightbig
. Now with p1=p2= 1
we have the following bound on the expected samples.
/summationdisplay
1≤i≤nE[pi]≤2 +32
µϵ2/parenleftig
3 logn+ log/parenleftbig
fM
φ(A)/parenrightbig
−log/parenleftbig
fM2
φ2(a2)/parenrightbig/parenrightig
Here we consider that ∀i∈[n]we haveµ=µn≤µiandM=Mn⪰Mias theµ-similar Bregaman
divergence parameters for A. Note that the coreset size is independent of kandd. Hence the resultant
coreset ensures the desired guarantee equation 1 for all Xwith at most ncenters in Rd. The expected size
of the coreset is O/parenleftbigg
1
µϵ2/parenleftig
logn+ log/parenleftbig
fM
φ(A)/parenrightbig
−log/parenleftbig
fM2φ2(a2)/parenrightbig/parenrightig/parenrightbigg
.
A.2.6 Proof of Lemma 5.2: k-means Clustering
Proof.We prove it using the Lemmas 5.2, 5.6, 5.9 and 5.8. As for k-means clustering we have Mi=Idand
µi= 1for eachi≤n, hence∀i∈[n]the expected upper bound on both lower and upper barrier sensitivity
scores are,
2fφi(ai)
ϵ/summationtext
j≤ifφj(aj)+12
ϵ(i−1)
It can be verified by a similar analysis as in the proof A.2.1 and A.2.4 of Lemma 5.3 and 5.6. The rest of
the lemma’s proof follows as it is and we get a required guarantee. The NonParametricFilter returns a
coreset ofO/parenleftbigg
1
ϵ2/parenleftig
logn+ log/parenleftbig
fφ(A)/parenrightbig
−log/parenleftbig
fφ2(a2)/parenrightbig/parenrightig/parenrightbigg
expected samples.
A.3 Uniform Deviation
A.3.1 Proof of Lemma 6.1
Proof.For any a∈RdandX∈Rk×dwe have,
fX(a)
σ+E[fX(a)](i)
≤fM
X(a)
µ(σM+E[fM
X(a)])
(ii)
≤2fM
ξ(a) + 2fM
X(ξ)
µ(σM+E[fM
X(a)])
=2fM
ξ(a) + 2E[fM
X(ξ)]
µ(σM+E[fM
X(a)])
35Published in Transactions on Machine Learning Research (07/2022)
(iii)
≤2fM
ξ(a) + 4E[fM
ξ(a)] + 4E[fM
X(a)]
µ(σM+E[fM
X(a)])
=2fM
ξ(a) + 4σM+ 4E[fM
X(a)]
µ(σM+E[fM
X(a)])
≤2fM
ξ(a)
µσM+8
µ
Here (i)is by using upper bound (on the numerator) and lower bound (on the denominator) of the Bregman
divergence using squared Mahalanobis distance. In (ii)and(iii)we use the fact that (a+b)2≤2(a2+b2).
Next we bound E[s(a)2].
E[s(a)2] = E
/parenleftigg
2fM
ξ(a)
µσM+8
µ/parenrightigg2

=E/bracketleftigg/parenleftigg
4fM
ξ(a)2
µ2σ2
M+64
µ2+32fM
ξ(a)
µ2σM/parenrightigg/bracketrightigg
=E[4fM
ξ(a)2]
µ2σ2
M+64
µ2+32E[fM
ξ(a)]
µ2σM
≤4E[fM
ξ(a)2]
µ2σ2
M+96
µ2<t
We get the last equality because E[fM
ξ(a)] =σMand by the assumption equation 14.
A.3.2 Proof of Theorem 6.2
Proof.The proof of is same as the proof of Theorem 5 in Bachem et al. (2017a). Hence, here we only present
proof sketch.
AsE[s(a)]≤t, hence on 2mi.i.d. samples{ai,...,a2m}by Markov we get1
2m/summationtext2m
i=1s(ai)2<O(t)with at
least a constant probability.
The rest of the proof is based on a double sampling approach. Let am+1,am+2,...,a2mbe an additional
mindependent samples from Dand leth1,h2,...,hmbe independent random variables uniformly sampled
from{−1,1}. IfE[s(a)2]≤t, the probability of equation 16 not holding can be bounded by the probability
that there exists a gX(·)∈Gsuch that
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
m/summationdisplay
i≤mhi·(gX(ai)−gX(am+i))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥ϵ (18)
We first provide the intuition for some function g∈ Gand then show how we extend it to all g∈ G.
While the function g(a)is not bounded, for a given sample a1,a2,...,a2m, eachg(ai)is contained within
[0,s(ai)]. Given the sample a1,a2,...,a2m, the random variable hi·(g(ai)−g(ai+m)is bounded in 0±
max{s(ai),s(ai+m)}and has zero mean. Hence, given independent samples a1,a2,...,a2m, the probability
of equation 18 occurring for a single g∈Gcan be bounded using Hoeffding’s inequality by,
P
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
m/summationdisplay
i≤mhi·(gX(ai)−gX(am+i))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤ϵ
≤2 exp/parenleftigg
−2mϵ2
1
m/summationtext
i≤mmax{s(ai),s(ai+m)}/parenrightigg
≤2 exp/parenleftigg
−mϵ2
1
2m/summationtext
i≤2ms(ai)/parenrightigg
36Published in Transactions on Machine Learning Research (07/2022)
From lemma 6.1 as we know E[s(a)2]≤t, so form∈Ω/parenleftbigt
ϵ2/parenrightbig
we can ensure above event with at least 0.99
probability. Finally using Pdim (G)≤ρwe take a union to ensure that the above event with at least 0.99
probability. So we get m∈Ω/parenleftbigtρ
ϵ2/parenrightbig
.
A.4 Experiments on MNIST and Song Data
Here we discuss some more experimental results. We run our algorithms to compare with other baseline
coresetcreationalgorithms. Oncethecoresetisobtainedfromeachofthesamplingmethods, werunweighted
k-means++ clustering (Arthur & Vassilvitskii, 2007) on them for various values of kand get the centers.
These centers are considered as initial centres while running k-means clustering on the coreset and finally
obtain the centres. Once these centers are obtained, we compute the quantization error on the entire dataset
Cs, with respect to the corresponding centers. We also run a similar k-means clustering on the entire data
for the same values of kand get the quantization error from those centres, i.e., Cf. Finally we report the
relative error η, i.e.,η=|Cs−Cf|
Cf.
A.4.1 BregmanFilter
We compare the performance of BregFilter (our BregmanFilter ) with Uniform and TwoPass on the fol-
lowing datasets.
1.MNIST:60,000points in 784dimension digits dataset.
2.KDD(BIO-TRAIN) :145,751points with 74features.
3.SONGS:515,345songs from the Million song dataset with 90features.
In the figure 3 we show the change in relative error ( η) with respect to the change in the coreset size ( %of
data). Here we consider relative entropy (or KL divergence) as Bregman divergence and run the sampling
methods on the MNISTdataset. As the data has a natural clustering of 10digits, hence we use k={5,10}.
We run the sampling algorithms for various coreset sizes. We run 5random instances for each each coreset
size and here we report the average of their η.
0.5 1.0 1.5 2.0 2.5 3.0
% of data0.0050.0100.0150.0200.0250.0300.0350.040Relative Error 
MNIST (K=5)
Uniform
TwoPass
ParaFilter
0.5 1.0 1.5 2.0 2.5 3.0
% of data0.020.040.060.080.10
MNIST (K=10)
Uniform
TwoPass
ParaFilter
Breg
Breg
Figure 3: Relative error v/s coreset size for KL divergence.
We also run the sampling methods on KDD(BIO-TRAIN) andSONGS, considering squared Euclidean distance
as the Bregman divergence. In the figure 4 we report the average ηof the 5runs, fork={100,200}. The
plot shows the change in relative error ( η) with change in coreset size ( %of data).
37Published in Transactions on Machine Learning Research (07/2022)
Breg Breg
Breg Breg
Figure 4: Relative error v/s coreset size. Squared Euclidean Distance as Bregman Divergence.
In all these cases, as per the expectation we do see an improvement in ηas coreset size increases. Fur-
ther we also notice that the performance of the BregFilter is equivalent to TwoPass and outperforms the
performance of Uniform.
Bad example for Leverage:In the datasets considered, the empirical performance of the leverage score
is similar to that of the provable sampling strategy that we propose. It is useful to recall that this is not
always the case. The following example shows when leverage score can perform really badly.
Consider 4 points in R1as(1000),(1000),(1000)and(1)coming in this order. On these we are interested
in a 2-means clustering, i.e., k= 2. Clearly the cluster centers are (1000)and(1). Now the online leverage
scores (as defined in Algorithm 1 in Chhaya et al. (2020a)) of these points will be10002
10002,10002
2·10002,10002
3·10002, and
1
3·10002+1. Letrbe the parameter that controls the expected coreset size. So the points are sampled with
probability r·1,r·0.5,r·0.33andr
3·10002+1respectively. Now based on this, if we build a coreset, say with
r= 3, then with a very high probability we will not sample any representative from the (1)cluster (this
example can easily be generalized to coresets of a general size). Now with our sensitivity upper bound (refer
liin algorithm 1) depends on how much a current point is far from the previous points. In this example, the
online sensitivity score of the first point (1000)will be 1and point will be sampled with probability r·1.
The second point (1000)is just a copy of the first point, and hence the mean of the first two points is equal
to the second point itself. As a result its online sensitivity will be 1 (which is due to the second term in the
algorithm) and even this point will be sampled with probability r·1. For the next point (1000), also the
mean does not move, so its online sensitivity score will be 1/2and its sampling probability will be r·0.5.
Now for the final point (1)the mean moves from (1000)to(750.25). So its online sensitivity scores will be
749.25
749.25+1
2and its sampling probability be r/parenleftbig749.25
749.25+1
2/parenrightbig
, which is bigger than 1. So for any r the final point
will be sampled in our coreset with higher probability than the third point in the stream. As a result our
coreset, even if it is of size three, will have representatives from each cluster.
38Published in Transactions on Machine Learning Research (07/2022)
The main insight from the above toy example is that leverage score sampling only tries to preserve the rank
of the data, and not the cluster structure. Hence, if the data spans only a low rank space but has a large
number of clusters then Leverage sampling will likely perform worse compared to our BregFilter .
A.4.2 NonParametricFilter
Now we compare the performance of a heuristic version of NonParametricFilter called NP-Filter with
Uniform,Offline and TwoPass onKDD(BIO-TRAIN) dataset. The Offline is the lightweight coreset from
(Bachem et al., 2018a). Here for the comparison we do not consider the assumption 5.1. Instead use simply
use the expected upper bounds as shown in lemma 5.6. Further for NP-Filter the coreset size is controlled
byϵ. Now once we get a coreset from NP-Filter , we set the desired parameters of other sampling methods
to get similar coreset size. Here once the coreset is obtained from each of the sampling methods, we get
the relative error ηas described above. Now on each coreset, we run k-clustering for various values of
k={50,100,200,300}to capture the non-parametric nature of the coreset.
50 100 150 200 250 300
k = Number of Centers01234Relative Error 
=1.00
Uniform
Offline
TwoPass
NP-Filter
50 100 150 200 250 300
k = Number of Centers01234Relative Error 
=0.75
Uniform
Offline
TwoPass
NP-Filter
50 100 150 200 250 300
k = Number of Centers0.20.40.60.81.0Relative Error 
=0.50
Uniform
Offline
TwoPass
NP-Filter
50 100 150 200 250 300
k = Number of Centers0.050.100.150.200.250.30Relative Error 
=0.25
Uniform
Offline
TwoPass
NP-Filter
Figure 5: Change in Relative Error ηwith respect to number of centers kfor various values of ϵ.
For each of the algorithms and for each value of ϵwe run 5random instances, compute η=|CS−CF|
CFand
report the average ηvalue. We consider ϵ={1.0,0.75,0.5,0.25}, for which we have {500,850,1650,5500}
expected samples. Figure 5 shows the change in the value of Relative Error ηwith respect to the change
in number of centers k, for various values of ϵ. With decrease in ϵwe can note that the value of ηalso
decreases. This is because as the ϵincreases, the coreset size reduces, which results to a high η. Now an
interesting point to note is that the ηremains significantly smaller than ϵfor the same coreset across various
values ofk. Even though our algorithm is heuristic, but this plot reflects the non-parametric nature of our
39Published in Transactions on Machine Learning Research (07/2022)
coreset from importance sampling. Note that, even though Offline outperforms NP-Filter , but they are
very close.
40