Under review as submission to TMLR
Flow Map Matching
Anonymous authors
Paper under double-blind review
Abstract
Generative models based on dynamical transport of measure, such as diﬀusion models, ﬂow
matching models, and stochastic interpolants, learn an ordinary or stochastic diﬀerential
equation whose trajectories push initial conditions from a known base distribution onto
the target. While training is cheap, samples are generated via simulation, which is more
expensive than one-step models like GANs. To close this gap, we introduce ﬂow map
matching – an algorithm that learns the two-time ﬂow map of an underlying ordinary
diﬀerential equation. The approach leads to an eﬃcient few-step generative model whose
step count can be chosen a-posteriori to smoothly trade oﬀ accuracy for computational
expense. Leveraging the stochastic interpolant framework, we introduce losses for both direct
training of ﬂow maps and distillation from pre-trained (or otherwise known) velocity ﬁelds.
Theoretically, we show that our approach uniﬁes many existing few-step generative models,
including consistency models, consistency trajectory models, progressive distillation, and
neural operator approaches, which can be obtained as particular cases of our formalism.
With experiments on CIFAR-10 and ImageNet 32x32, we show that ﬂow map matching leads
to high-quality samples with signiﬁcantly reduced sampling cost compared to diﬀusion or
stochastic interpolant methods.
1 Introduction
In recent years, diﬀusion models ( Song et al. ,2020;Ho et al. ,2020;Sohl-Dickstein et al. ,2015;Song and Ermon ,
2020a ;b) have achieved state of the art performance across diverse modalities, including image ( Dhariwal
and Nichol ,2021;Rombach et al. ,2022;Esser et al. ,2024), audio ( Popov et al. ,2021;Jeong et al. ,2021;
Huang et al. ,2022;Lu et al. ,2022a ), and video ( Ho et al. ,2022a ;b;Blattmann et al. ,2023;Wu et al. ,2023)
data. Diﬀusion models, along with related techniques such as ﬂow matching ( Lipman et al. ,2022), rectiﬁed
ﬂow ( Liu et al. ,2022a ), and stochastic interpolants ( Albergo and Vanden-Eijnden ,2022;Albergo et al. ,
2023a ), construct a path in the space of measures between a base and a target distribution by specifying an
explicit connection between samples from each ( Albergo et al. ,2023a ). This deﬁnes a time-dependent family
of probability distributions that describes the dynamical transport of measure along the path. Critically,
this construction reduces the generative modeling problem to that of learning the corresponding velocity
ﬁeld ( Song et al. ,2020;Albergo et al. ,2023a ;Lipman et al. ,2022;Liu et al. ,2022a ), which leads to an eﬃcient
and stable paradigm for training. At sample generation time, however, models in this class generate data
by iteratively converting samples from the base into samples from the target through numerical integration
of an ordinary or stochastic diﬀerential equation. The number of integration steps required to produce
high-quality samples incurs a high cost that can limit real-time applications ( Chi et al. ,2024). Comparatively,
one-step models such as GANs ( Goodfellow et al. ,2014;2020;Creswell et al. ,2018) are notoriously diﬃcult
to train ( Metz et al. ,2017;Arjovsky et al. ,2017), but can be hundreds or thousands of times more eﬃcient to
sample, because they only require a single network evaluation. As a result, there has been signiﬁcant recent
interest in keeping the learning paradigm of diﬀusion-type models while reducing the number of steps needed
for sample generation ( Karras et al. ,2022).
Towards this goal, here we introduce a new class of generative models known as ﬂow map matching models ,
which learn the two-time ﬂow map of a probability ﬂow equation for an arbitrary diﬀusion or stochastic
interpolant. As shown in Figure 1, the learned ﬂow map can be used as a single-step model or as an iterative
model with a sample quality that we ﬁnd smoothly increases with the number of steps. We develop both
1Under review as submission to TMLR
Figure 1: Overview of ﬂow map matching. Our approach learns the two-time ﬂow map Xs,tthat transports
along the trajectory of an ordinary diﬀerential equation from time sto time t. The map is bidirectional, and can be
used to build an integrator with an arbitrary discretization. This integrator is exact in theory, and its number of steps
can be adjusted post-training to reduce inaccuracies due to estimation errors. The map can be distilled from a known
velocity ﬁeld or learned directly, and can be trained with an arbitrary, non-Gaussian base distribution, as illustrated
here with image-to-image translation.
distillation- and direct training-based approaches that can be used to convert a velocity into a ﬂow map or to
learn a ﬂow map from scratch. Overall, our main contributions can be summarized as:
•We introduce a framework for learning the two-time ﬂow map of an arbitrary ordinary diﬀerential equation,
and we relate our approach to recent work on consistency models ( Song et al. ,2023;Song and Dhariwal ,
2023) and consistency trajectory models ( Kim et al. ,2024).
•We show that, in contrast to previous approaches, learning the two-time ﬂow map leads to an eﬃcient
few-step generative model whose step count can be adjusted after training, to tradeoﬀ accuracy for cost.
•We introduce a new Lagrangian loss function for distillation of a ﬂow map from a pre-trained velocity ﬁeld,
which we show outperforms a related Eulerian loss function that can be obtained as the continuous-time
limit of the consistency distillation objective ( Song et al. ,2023). These Lagrangian and Eulerian loss
functions are shown to control the Wasserstein distance between the densities pushed forward by the
exact and the learned maps.
•We show that our Lagrangian loss, when coupled with the stochastic interpolant framework ( Albergo
and Vanden-Eijnden ,2022;Albergo et al. ,2023a ), leads to a new loss function for direct training of ﬂow
map models that can be used to stably and eﬃciently train few-step generative models without needing a
pre-trained velocity ﬁeld.
•We introduce a map distillation objective inspired by progressive distillation ( Salimans and Ho ,2022) and
neural operator approaches ( Zheng et al. ,2023) that can eﬃciently convert a few-step map model into a
single-step map.
To highlight the eﬃcacy and generality of our formalism, we illustrate our approach with numerical experiments
on CIFAR-10 and ImageNet 32×32.
2 Related Work
Dynamical transport of measure. Our approach is built upon the modern perspective of generative
modeling based on dynamical transport of measure. Grounded in the theory of optimal transport ( Villani ,
2009;Benamou and Brenier ,2000;Santambrogio ,2015), these models originate at least with ( Tabak and
Vanden-Eijnden ,2010;Tabak and Turner ,2013), but have been further developed by the machine learning
community in recent years ( Rezende and Mohamed ,2015;Dinh et al. ,2017;Grathwohl et al. ,2018;Chen
et al. ,2019). A breakthrough in this area originated with the appearance of score-based diﬀusion models ( Song
et al. ,2020;Song and Ermon ,2020a ;b), along with related denoising diﬀusion probabilistic models ( Ho et al. ,
2020;Sohl-Dickstein et al. ,2015). These methods generate samples by learning to time-reverse a stochastic
2Under review as submission to TMLR
diﬀerential equation with stationary density given by a Gaussian. More recent approaches such as ﬂow
matching ( Lipman et al. ,2022), rectiﬁed ﬂow ( Liu et al. ,2022a ;b), and stochastic interpolants ( Albergo
and Vanden-Eijnden ,2022;Albergo et al. ,2023a ;Ma et al. ,2024;Chen et al. ,2024) similarly construct
connections between the base density and the target, but allow for arbitrary base densities and provide a
greater degree of ﬂexibility in the construction of the connection.
Reducing simulation costs. There has been signiﬁcant recent interest in reducing the cost associated
with solving an ODE or an SDE for a generative model based on dynamical transport of measure. One
such approach, pioneered by rectiﬁcation ( Liu et al. ,2022a ;b), is to try to straighten the paths of the
probability ﬂow, so as to enable more eﬃcient adaptive integration. In the limit of optimal transport, the
paths become straight lines and the integration can be performed in a single step. A second approach is to
introduce couplings between the base and the target, such as by computing the optimal transport over a
minibatch ( Pooladian et al. ,2023;Tong et al. ,2023), or by using data-dependent couplings ( Albergo et al. ,
2023b ), which can simplify both training and sampling. A third approach has been to design hand-crafted
numerical solvers tailored for diﬀusion models ( Karras et al. ,2022;Zhang and Chen ,2023;Jolicoeur-Martineau
et al. ,2021;Liu et al. ,2022c ;Lu et al. ,2022b ), or to learn these solvers directly ( Watson et al. ,2021;2022;
Nichol and Dhariwal ,2021) to maximize eﬃciency. Instead, we propose to learn the ﬂow map directly, which
avoids estimating optimal transport maps and can overcome the inherent limitations of numerical integration.
Distillation and consistency techniques. Most related to our approach is a class of one-step models
based on distillation orconsistency ; we give an explicit mapping between these techniques and our own
in Appendix C. Consistency models ( Song et al. ,2023) have been introduced as a new class of generative
models that can either be distilled from a pre-trained diﬀusion model or trained directly, and are related to
several notions of consistency of the score model that have appeared in the literature ( Lai et al. ,2023a ;b;
Shen et al. ,2022;Boﬃ and Vanden-Eijnden ,2023;Daras et al. ,2023). These models learn a one-step map
from noise to data, and can be seen as learning a single-time ﬂow map. While they can perform very well,
consistency models do not beneﬁt from multistep sampling, and exhibit training diﬃculties that mandate
delicate hyperparameter tuning ( Song and Dhariwal ,2023).By contrast, we learn a two-time ﬂow map, which
enables us to smoothly beneﬁt from multistep sampling. Moreover, we introduce new loss functions that are
easier to train. Similarly, neural operator approaches ( Zheng et al. ,2023) learn a one-time ﬂow map from
noise to data, but do so by ﬁrst generating a dataset of trajectories from the probability ﬂow. Consistency
trajectory models ( Kim et al. ,2024) were later introduced to improve multistep sampling and to enable the
student to surpass the performance of the teacher. Similar to our approach, these models learn a two-time
ﬂow map, but do so using a very diﬀerent loss function that incorporates challenging adversarial training.
Progressive distillation ( Salimans and Ho ,2022) and knowledge distillation ( Luhman and Luhman ,2021)
techniques aim to convert a diﬀusion model into an equivalent model with fewer samples by matching several
steps of the original diﬀusion model. This approach is related to our ﬂow map distillation scheme, though the
object we distill is fundamentally diﬀerent.
3 Flow Map Matching
The central object in our method is the ﬂow map of a diﬀerential equation, which is a function that maps
points along trajectories. Our focus in this work is on diﬀerential equations obtained from generative models
based on dynamical transport of measure, but our deﬁnitions and some of our results also apply in a more
general context, as we now show. All proofs of the statements made in this section are provided in Appendix A,
with some additional theoretical results given in Appendix B.
3.1 Setup and deﬁnitions
We consider problems that involve ordinary diﬀerential equations (ODEs) deﬁned on Rdover an ensemble of
initial conditions,
˙xt=bt(xt), x t=0=x0∼ρ0, (3.1)
where b: [0,∞)×Rd→Rdis the time-dependent velocity ﬁeld and where ρ0∈P(Rd)is a base probability
density function (PDF), which we assume positive everywhere. Throughout the paper we make the assumption
that:
3Under review as submission to TMLR
Assumption 3.1. The velocity ﬁeld satisﬁes the one-sided Lipschitz condition
∃Ct∈L1[0, T] : ( bt(x)−bt(y))·(x−y)6Ct|x−y|2for all (x, y)∈Rd×Rd. (3.2)
Under this assumption, the classical Cauchy-Lipschitz theory ( Hartman ,2002) guarantees that solutions
of(3.1)exist and are unique for all x0∈Rdand for all t∈[0, T]. The PDF ρt>0ofxtsatisﬁes the transport
equation associated with ( 3.1)
∂tρt(x) +∇ ·(bt(x)ρt(x)) = 0 , ρ t=0(x) =ρ0(x) (3.3)
where ∇denotes a gradient with respect to x. Assuming that we can draw samples from ρ0, the generative
modeling problem is to generate samples from ρtat some t∈[0, T]. When bis known, these samples can be
obtained by numerically integrating (3.1). In practice, however, this can be costly, particularly when bis
given by an expressive neural network that is expensive to evaluate.
Here, we bypass this numerical integration by estimating the two-time ﬂow map, which we now deﬁne.
Deﬁnition 3.2 (Flow Map) .The ﬂow map Xs,t:Rd→Rdfor(3.1)is the unique map such that
Xs,t(xs) =xtfor all (s, t)∈[0, T]2, (3.4)
where (xt)t∈[0,T]is a solution to the ODE (3.1).
The ﬂow map in Deﬁnition 3.2can be seen as an integrator for (3.1)where the step size t−smay be chosen
arbitrarily. In addition to the deﬁning condition ( 3.4), we now highlight some of its useful properties1.
Proposition 3.3. The ﬂow map Xs,t(x)is the unique solution to the Lagrangian equation
∂tXs,t(x) =bt(Xs,t(x)), X s,s(x) =x, (3.5)
for all (s, t, x )∈[0, T]2×Rd. In addition, it satisﬁes
Xt,τ(Xs,t(x)) =Xs,τ(x) (3.6)
for all (s, t, τ, x )∈[0, T]3×Rd; in particular Xs,t(Xt,s(x)) = xfor all (s, t, x )∈[0, T]2×Rd, i.e. the ﬂow
map is invertible.
Proposition 3.3shows that if we can draw samples x0∼ρ0, then we can use the ﬂow map to generate one-step
samples from ρtfor any t∈[0, T]viaxt=X0,t(x0)∼ρt. Using the semigroup relation (3.6), we can also
generate samples in multiple steps using xtk=Xtk−1,tk(xtk−1)∼ρtkfor any discretization points (t0, . . . , t K)
with tk∈[0, T]andK∈N.
3.2 Distillation of a known velocity ﬁeld bt(x)
The diﬀerential characterization of the ﬂow map given by Proposition 3.3leads to a distillation loss that can
be used to learn an integrator for any diﬀerential equation with known right-hand side b, as we now show.
Corollary 3.4 (Lagrangian map distillation) .The ﬂow map is the global minimizer over ˆXof the loss
LLMD (ˆX) =/integraldisplay
[0,T]2/integraldisplay
Rdws,t/vextendsingle/vextendsingle∂tˆXs,t(x)−bt(ˆXs,t(x))/vextendsingle/vextendsingle2ρs(x)dxdsdt, (3.7)
subject to the boundary condition that ˆXs,s(x) =xfor all x∈Rdands∈[0, T]. In(3.7),ws,t>0is a weight
function and ρsis the solution to (3.3).
We remark that we can use any density in (3.7); nevertheless, it will be convenient to use ρsas it guarantees
that we learn the ﬂow map at values of xwhere we typically need to evaluate it. Moreover, we will show
in Section 3.4how the stochastic interpolant framework will enable us to evaluate (3.7)empirically without
having to solve ( 3.1) by providing access to alternative samples from ρs.
1We refer to (3.5)as the “Lagrangian equation” because it is deﬁned in a frame of reference that moves with Xs,t(x). Later,
we write down an alternative “Eulerian” relation that is deﬁned at any ﬁxed point x∈Rd.
4Under review as submission to TMLR
Algorithm 1 Lagrangian map distillation (LMD)
1:Input : Interpolant coeﬃcients αt, βt, γt; velocity model bt; weight function ws,t; batch size M.
2:repeat
3: Draw batch (si, ti, Isi)M
i=1.
4: Compute ˆLLMD =1
M/summationtextM
i=1wsi,ti|∂tˆXsi,ti(Isi)−bti(ˆXsi,ti(Isi))|2.
5: Take gradient step on ˆLLMD to update ˆX.
6:until converged
7:Return : Flow map ˆX.
The weight ws,tcan be adjusted to learn the map for diﬀerent pairs (s, t)of interest. For example, if we want
to estimate the map Xs,tand its inverse Xt,s, we can set ws,t= 1. If we only want to estimate the map going
forward with s6t, then we can set ws,t= 1ifs6tandws,t= 0otherwise.
When applied to a pre-trained model, such as the bof the probability ﬂow equation of a ﬂow matching or
diﬀusion model, Corollary 3.4can be used to train a new, few-step generative model with performance that
matches that of the teacher. When ˆXs,tis parameterized by a neural network, the partial derivative with
respect to tcan be computed eﬃciently using forward-mode automatic diﬀerentiation in most modern deep
learning packages. This procedure is summarized in Algorithm 1.
Equation (3.7)is based on the Lagrangian equation (3.5). The following result shows that the ﬂow map Xs,t
also obeys an Eulerian equation involving a derivative with respect to s.
Proposition 3.5. The ﬂow map Xs,tis the unique solution of the Eulerian equation
∂sXs,t(x) +bs(x)· ∇Xs,t(x) = 0 , X t,t(x) =x, (3.8)
for all (s, t, x )∈[0, T]2×Rd.
By squaring the left hand-side of ( 3.8), we may construct a second loss function for distillation.2
Corollary 3.6 (Eulerian map distillation) .The ﬂow map is the global minimizer over ˆXof the loss
LEMD (ˆX) =/integraldisplay
[0,T]2/integraldisplay
Rdws,t/vextendsingle/vextendsingle∂sˆXs,t(x) +bs(x)· ∇ˆXs,t(x)/vextendsingle/vextendsingle2ρs(x)dxdsdt, (3.9)
subject to the boundary condition ˆXs,s(x) =xfor all x∈Rdand for all s∈R. In (3.9),ws,t>0is a weight
function and ρsis the solution to (3.3).
While the Jacobian-vector product bs(x)· ∇ˆXs,t(x)can be computed using forward-mode automatic diﬀeren-
tiation, the high-dimensionality of xin most applications incurs an additional computational expense, so that
the loss in Corollary 3.4may be preferred in practice. In our numerical experiments reported below, we also
observed that the loss (3.7)is better behaved and converges faster than (3.9). Nevertheless, we summarize a
training procedure based on Corollary 3.6in Algorithm 2.
In Appendix C, we demonstrate how the preceding results connect with existing distillation-based approaches.
In particular, when bt(x)is the velocity of the probability ﬂow ODE associated with a diﬀusion model,
Corollary 3.6recovers the continuous-time limit of the loss functions used for consistency distillation ( Song
et al. ,2023;Song and Dhariwal ,2023) and consistency trajectory models ( Kim et al. ,2024), while Corollary 3.4
is new.
3.3 Wasserstein control
In this section, we show that the Lagrangian and Eulerian distillation losses (3.7)and(3.9)control the
Wasserstein distance between the density in (3.3)and the density of the pushforward of ρ0under the learned
ﬂow map. Combined with the Wasserstein bound in Albergo and Vanden-Eijnden (2022), the following results
2In (3.8), the term (bs(x)· ∇Xs,t(x))i=/summationtextd
j=1[bs(x)]j∂xj[Xs,t(x)]i= [∇Xs,t(x)·bs(x)]icorresponds to a Jacobian-vector
product that can be computed eﬃciently using forward-mode automatic diﬀerentiation.
5Under review as submission to TMLR
Algorithm 2 Eulerian map distillation (EMD)
1:Input : Interpolant coeﬃcients αt, βt, γt; velocity model bt; weight function ws,t; batch size M.
2:repeat
3: Draw batch (si, ti, Isi)M
i=1.
4: Compute ˆLEMD =1
M/summationtextM
i=1wsi,ti|∂sˆXsi,ti(Isi) +∇ˆXsi,ti(Isi)bti(Isi)|2.
5: Take gradient step on ˆLEMD to update ˆX.
6:until converged
7:Return : Flow map ˆX.
imply a bound on the Wasserstein distance between the target density and the pushforward density for the
learned ﬂow map in the case where bis a pre-trained stochastic interpolant or diﬀusion model. We begin by
stating our result for Lagrangian distillation.
Proposition 3.7 (Lagrangian error bound) .LetXs,t:Rd→Rddenote the ﬂow map for b, and let
ˆXs,t:Rd→Rddenote an approximate ﬂow map. Let ˆρ1=ˆX0,1/sharpρ0andρb
1=X0,1/sharpρ0. Then,
W2
2(ρb
1,ˆρ1)6e1+2/integraltext1
0|Ct|dt/integraldisplay1
0E/bracketleftbig/vextendsingle/vextendsinglebt(ˆX0,t(x0))−∂tˆX0,t(x0)/vextendsingle/vextendsingle2/bracketrightbig
dt6e1+2/integraltext1
0|Ct|dtLLMD (ˆX). (3.10)
where Ctis the constant appearing in Assumption 3.1.
The proof is given in Appendix A. We now state an analogous result for the Eulerian case.
Proposition 3.8 (Eulerian error bound) .LetXs,t:Rd→Rddenote the ﬂow map for b, and let ˆXs,t:Rd→
Rddenote an approximate ﬂow map. Denote by ˆρ1=ˆX0,1/sharpρ0andρb
1=X0,1/sharpρ0. Then,
W2
2(ρb
1,ˆρ1)6e/integraldisplay1
0E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle∂sˆXs,1(Is) +bs(Is)· ∇ˆXs,1(Is)/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
ds6eLEMD (ˆX). (3.11)
The proof is also given in Appendix A. The result in Proposition 3.8appears stronger than the result
in Proposition 3.7, because it is independent of any Lipschitz constant. Nevertheless, in our numerical
experiments we ﬁnd best performance when using the Lagrangian distillation loss, rather than the Eulerian
distillation loss. We hypothesize and provide numerical evidence that this originates due to the spatial
gradient present in the Eulerian distillation loss; in several cases of interest, the learned map can be singular or
nearly singular, so that the spatial gradient is not well-deﬁned everywhere. This leads to training diﬃculties
that manifest as fuzzy boundaries on the checkerboard dataset and blurry images on image datasets.
3.4 Direct training with stochastic interpolants
The stochastic interpolant framework leads to a new loss function for direct training of ﬂow maps that does
not require a pre-trained b. We ﬁrst give the deﬁnition of a stochastic interpolant.
Deﬁnition 3.9 (Stochastic Interpolant) .The stochastic interpolant Itbetween probability densities ρ0and
ρ1is the stochastic process given by
It=αtx0+βtx1+γtz, (3.12)
where α, β, γ2∈C1([0,1])satisfy α0=β1= 1,α1=β0= 0, and γ0=γ1= 0. In (3.12),(x0, x1)is drawn
from the coupling (x0, x1)∼ρ(x0, x1)satisfying the marginal constraints/integraltext
Rdρ(x0, x1)dx0=ρ1(x1)and/integraltext
Rdρ(x0, x1)dx1=ρ0(x0). Moreover, z∼N(0,Id)with z⊥(x0, x1).
Theorem 3.6 of ( Albergo et al. ,2023a ) shows that the stochastic interpolant given in Deﬁnition 3.9speciﬁes
an underlying probability ﬂow, as we now recall.
Proposition 3.10 (Probability ﬂow) .The probability density function ρt=Law(It)satisﬁes the transport
equation (3.3)with velocity ﬁeld given by
bt(x) =E[˙It|It=x]. (3.13)
6Under review as submission to TMLR
Algorithm 3 Flow map matching (FMM)
1:Input : Interpolant coeﬃcients αt, βt, γt; weight function ws,t; batch size M.
2:repeat
3: Draw batch (si, ti, Iti,˙Iti)M
i=1.
4: Compute the loss function
ˆLFMM =1
M/summationtextM
i=1wsi,ti/parenleftBig
|∂tˆXsi,ti(ˆXti,si(Iti))−˙Iti|2+|ˆXsi,ti(ˆXti,si(Iti))−Iti|2/parenrightBig
.
5: Take gradient step on ˆLFMM to update ˆX.
6:until converged
7:Return : Flow map ˆX.
Algorithm 4 Progressive ﬂow map matching (PFMM)
1:Input : Interpolant coeﬃcients αt, βt, γt; weight ws,t;K-step ﬂow map ˆX; batch size M.
2:repeat
3: Draw batch (si, ti, Isi)M
i=1and compute ti
k=si+ (k−1)(ti−si)fork= 1, . . . , K .
4: Compute ˆLPFMM =1
M/summationtextM
i=1wsi,ti/parenleftBig
|ˇXsi,ti(Isi)−/parenleftbigˆXti
K−1,ti
K◦ ··· ◦ ˆXti
1,ti
2/parenrightbig
(Isi)|2/parenrightBig
.
5: Take gradient step on ˆLPFMM to update ˇX.
6:until converged
7:Return : One-step ﬂow map ˇX.
In(3.13),E[·|It=x]denotes an expectation over the coupling (x0, x1)∼ρ(x0, x1)andz∼N(0, I)conditioned
on the event It=x.
The stochastic interpolant in Deﬁnition 3.9deﬁnes a path in the space of measures between an arbitrary
base density ρ0(which may be a simple Gaussian) and a target density ρ1for which there is an underlying
well-deﬁned transport. Moreover, the corresponding drift ﬁeld bcan be learned eﬃciently in practice by
solving a square loss regression problem ( Albergo et al. ,2023a )
b= argmin
ˆb/integraldisplay1
0E/bracketleftbig
|ˆbt(It)−˙It|2/bracketrightbig
dt, (3.14)
where Edenotes an expectation over the coupling (x0, x1)∼ρ(x0, x1)andz∼N(0,Id).
A canonical choice when ρ0=N(0,Id)considered in ( Albergo and Vanden-Eijnden ,2022) corresponds to
αt= 1−t,βt=t, and γt= 0, which recovers ﬂow matching ( Lipman et al. ,2022) and rectiﬁed ﬂow ( Liu
et al. ,2022a ). The choice αt= 0,βt=tandγt=√
1−t2corresponds to a variance-preserving diﬀusion
model with the identiﬁcation t=−logτwhere τ∈[0,∞)is the usual diﬀusion time3. A variance-exploding
diﬀusion model may be obtained by taking αt= 0,βt= 1, and γt=T−twith t∈[0, T]and where τ=T−t
is the usual diﬀusion time, though this violates the boundary conditions in Deﬁnition 3.9.
Given a pre-trained b, we can use the stochastic interpolant framework to evaluate the expectations in the
losses (3.7)and(3.9)by leveraging the fact that It∼ρt. Alternatively, the following result shows how a ﬂow
map may be learned directly from Itwithout the need of a pre-trained model.
Proposition 3.11 (Flow map matching) .The ﬂow map is the global minimizer over ˆXof the loss
LFMM [ˆX] =/integraldisplay
[0,1]2ws,t/parenleftBig
E/bracketleftbig
|∂tˆXs,t(ˆXt,s(It))−˙It|2/bracketrightbig
+E/bracketleftbig
|ˆXs,t(ˆXt,s(It))−It|2/bracketrightbig/parenrightBig
dsdt. (3.15)
In(3.15),ws,t>0andEis taken over the coupling (x0, x1)∼ρ(x0, x1)andz∼N(0,Id).
In the loss (3.15), we are free to adjust the weight factor ws,t. However, since we need to learn both the map
Xs,tand its inverse Xt,s, it is necessary to enforce the symmetry property wt,s=ws,t. If we learn the map for
3Note that γ0= 1in this case, so that I0=z
7Under review as submission to TMLR
all(s, t)∈[0,1]2using, for example, ws,t= 1, then we can generate samples from ρ1in one step via X0,1(x0)
with x0∼ρ0. We note that the second term enforcing invertibility comes at no additional cost, because
ˆXs,t(ˆXt,s(It))can be computed at the same time as ∂tˆXs,t(ˆXt,s(It))with standard Jacobian-vector product
functionality in modern deep learning packages. A summary of the ﬂow map matching procedure is given
in Algorithm 3. Empirically, we found learning a one-step map to be challenging in practice. Convergence was
signiﬁcantly improved by taking ws,t=wt,s=I(|t−s|61/K)for some K∈Nwhere Idenotes an indicator
function. Given such a K-step model, it can be converted into a one-step model using a map distillation loss
that is similar to progressive distillation ( Salimans and Ho ,2022) and neural operator approaches ( Zheng
et al. ,2023).
Lemma 3.12 (Progressive ﬂow map matching) .The unique minimizer over ˇXof the loss
LPFMM [ˇX] =/integraldisplay
[0,1]2ws,tE/bracketleftBig/vextendsingle/vextendsingleˇXs,t(Is)−/parenleftbigˆXtK−1,tK◦ ··· ◦ ˆXt1,t2/parenrightbig
(Is)/vextendsingle/vextendsingle2/bracketrightBig
dsdt, (3.16)
produces the same output in one step as the K-step iterated map ˆX. In (3.16),ws,t>0,Eis taken over the
coupling (x0, x1)∼ρ(x0, x1)andz∼N(0,Id), and tk=s+ (k−1)(t−s)fork= 1, . . . , K .
ts0011δ=1/KX0,δXδ,2δ∘X0,δ…
ws,t>0
Figure 2: Schematic illustrating the
weight ws,tin the FMM loss. The
resulting map needs at least Ksteps,
but trains faster and can be distilled
into a one-step map via PFMM.We note that ˆXis ﬁxed in (3.16)and serves as the teacher, so we only
need to compute the gradient with respect to the parameters of ˇX. In
practice, we may train ˆXusing (3.15)over a class of neural networks
and then freeze its parameters. We may then use (3.16)to distill ˆXinto
a more eﬃcient model ˇX, which can be initialized from the parameters
ofˆXfor an eﬃcient warm start.
If the Kevaluations of ˆXare expensive, we may iteratively mini-
mize (3.16)with some number M < K evaluations of ˆXand then
replace ˆXbyˇX, similar to progressive distillation ( Salimans and Ho ,
2022). For example, we may take M= 2 and then minimize (3.16)
⌈log2K⌉times to obtain a one-step map. Alternatively, we can ﬁrst
generate a dataset of (s, t, I s,(ˆXtK−1,tK◦ ··· ◦ ˆXt1,t2)(Is))in a parallel
oﬄine phase, which converts (3.16)into a simple least-squares problem.
Finally, if we are only interested in using the map forward in time, we
can set ws,t= 1ifs6tandws,t= 0otherwise. The resulting procedure
is summarized in Algorithm 4.
4 Numerical Realizations
In this section, we study the eﬃcacy of the four methods introduced in Section 3: the Lagrangian map
distillation discussed in Corollary 3.4, the Eulerian map distillation discussed in Corollary 3.6, the direct
training approach of Proposition 3.11, and the progressive ﬂow map matching approach of Lemma 3.12. We
consider their performance on a two-dimensional checkerboard dataset, as well as in the high-dimensional
setting of image generation, to highlight diﬀerences in their training eﬃciency and performance.
To ensure that the boundary conditions on the ﬂow map ˆXs,tdeﬁned in (3.5)are enforced, in all experiments,
we parameterize the map using the ansatz
ˆXs,t(x) = (1 −t+s)x+ (t−s)fθ
s,t(x), (4.1)
where fθ
s,t(x) : [0, T]2×Rd→Rdis a neural network with parameters θ.
4.1 2D Illustration
As a simple illustration of our method, we consider learning the ﬂow map connecting a two-dimensional
Gaussian distribution to the checkerboard distribution presented in Figure 3. Note that this example is
challenging because the target density is supported on a compact set, and it is discontinuous at the edge
of this set. This mapping can be achieved, as discussed in Section 3, in various ways: (a) implicitly, by
solving (3.1)with a learned velocity ﬁeld using stochastic interpolants (or a diﬀusion model), (b) directly,
8Under review as submission to TMLR
4
04DKL: 0.02 DKL: 0.02 DKL: 0.02 DKL: 0.02 DKL: 0.02 DKL: 0.02 DKL: 0.02 DKL: 0.02SI, N=80
DKL: 0.107 DKL: 0.107 DKL: 0.107 DKL: 0.107 DKL: 0.107 DKL: 0.107 DKL: 0.107 DKL: 0.107FMM, N=1
DKL: 0.045 DKL: 0.045 DKL: 0.045 DKL: 0.045 DKL: 0.045 DKL: 0.045 DKL: 0.045 DKL: 0.045FMM, N=4
DKL: 0.043 DKL: 0.043 DKL: 0.043 DKL: 0.043 DKL: 0.043 DKL: 0.043 DKL: 0.043 DKL: 0.043PFMM, N=1
DKL: 0.043 DKL: 0.043 DKL: 0.043 DKL: 0.043 DKL: 0.043 DKL: 0.043 DKL: 0.043 DKL: 0.043LMD, N=1
DKL: 0.079 DKL: 0.079 DKL: 0.079 DKL: 0.079 DKL: 0.079 DKL: 0.079 DKL: 0.079 DKL: 0.079EMD, N=1
4
 0 44
04
4
 0 4
 4
 0 4
 4
 0 4
 4
 0 4
 4
 0 4
Figure 3: Two-dimensional results. Comparison of the various map-matching procedures on the 2D checkerboard
dataset, with the results from the probability ﬂow ODE of a stochastic interpolant as reference (top, ﬁrst panel from
left). The one-step map obtained by FMM when learning on (s, t) = [0 ,1]2(top, second panel) performs less well
than the FMM map learned on |t−s|<0.25iterated 4 times (top, third panel); this 4-step map can be accurately
distilled into a one-step map via PFMM (top, fourth panel). The one-step map obtained by distilling the pre-trained
bvia LMD (top, ﬁfth panel) also performs better than the one-step map obtained by distilling the same bvia EMD
(top, sixth panel). A KL-divergence between the model distributions and the target are provided to quantify the
performance, indicating that FMM, its progressive distillation, and LMD are the closest performers to the probability
ﬂow ODE baseline. The bottom row indicates, by color, how points from the Gaussian base are assigned by each of
the respective maps. The yellow dots are points that mistakenly land outside the checkerboard. These results indicate
that the primary source of error in each case is handling the discontinuity of the optimal map at the edges of the
checker. See Appendix Dfor more details.
using the ﬂow map matching objective in (3.15), (c) progressively matching the ﬂow map using (3.16), or
(d/e) distilling the map using the Eulerian (3.9)or Lagrangian (3.7)losses. In each case, we use a fully
connected neural network with 512neurons per hidden layer and 6layers to parameterize either a velocity
ﬁeld ˆbt(x)or a ﬂow map ˆXs,t(x). We optimize each loss using the Adam ( Kingma and Ba ,2017) optimizer for
5×104training iterations. The results are presented in Figure 3, where we observe that using the one-step
ˆX0,1(x)directly learned by minimizing (3.15)over the entire interval (s, t)∈[0,1]2performs worse than
learning with |t−s|<0.25and sampling with 4 steps. With this in mind, we use the 4-step map as a teacher
to minimize the PFMM loss, which produces a performant distilled one-step map. We also note that the
EMD loss performs worse than the LMD loss when distilling the map from a learned velocity ﬁeld for a
stochastic interpolant.
4.2 Image Generation
Motivated by the above results, we consider a series of image generation experiments on the CIFAR-10 and
ImageNet- 32×32datasets. For comparison, we benchmark the method against alternative techniques that
seek to lower the number of steps needed to produce samples with stochastic interpolant models, e.g. by
straightening the ODE trajectories using minibatch OT ( Pooladian et al. ,2023;Tong et al. ,2023). We train
all of our models from scratch, so as to control the design space of the comparison. For clarity, we label when
benchmark numbers are quoted from the literature.
For learning of the ﬂow map, we use a U-Net architecture following ( Dhariwal and Nichol ,2021). For LMD
and EMD that require a pre-trained velocity ﬁeld to distill, we also use a U-Net architecture for b. Because the
ﬂow map Xs,tis a function of two times, we modify the architecture. Both sandtare embedded identically
totin the original architecture. The result is concatenated and treated like tin the original architecture for
downstream layers. We benchmark the performance of the methods using the Frechet Inception Distance
(FID), which computes a measure of similarity between real images from the dataset and those generated by
our models. In addition, we compute what we denote the Teacher-FID (T-FID), which computes a measure
of similarity between images generated by the teacher model and those generated by the distilled model.
This measure allows us to directly benchmark the convergence of the distillation method, as it captures
9Under review as submission to TMLR
MethodN=2 N=4Baseline
FID T-FID FID T-FID
SI 112.42 - 34.84 - 5.53
EMD 48.32 34.19 44.35 30.74 5.53
LMD 7.13 1.27 6.04 1.05 5.53
PFMM 18.35 7.02 11.14 1.52 8.44
Table 1: Comparison of various distillation methods using FID and Teacher-FID metrics on the CIFAR-10 dataset.
Note that for PFMM, no velocity model (e.g. from a stochastic interpolant) is needed. It relies solely on the
minimization of (3.15)and(3.16). Baseline indicates the FID of the teacher model (a velocity ﬁeld for EMD and
LMD, and a ﬂow map for PFMM) against the true data.
discrepancies between the distribution of samples generated by the teacher and the distribution of samples
generated by the student. In addition, this allows us to benchmark accuracy independent of the overall
performance of the teacher, as our teacher models were trained with limited compute.
NDDPM BatchOT FMM (Ours)
20 63.08 7.71 9.68
8 232.97 15.64 12.61
6 275.28 22.08 14.48
4 362.37 38.86 16.90
Table 2: FID scaling with number of function evaluations
Nto produce a sample on ImageNet- 32×32. Compares
DDPM ( Ho et al. ,2020) and multi-sample Flow Matching
using the BatchOT method ( Pooladian et al. ,2023) to
ﬂow map matching. The ﬁrst two columns are quoted
from Pooladian et al. (2023). Note that no distillation is
used here, but rather direct minimization of (3.15), using
|t−s|<0.25.Sampling eﬃciency In Table 1, we compute
the FID and T-FID for the stochastic interpolant,
Eulerian, Lagrangian, and progressive distillation
models on 2 and 4-step generation for CIFAR-10.
The stochastic interpolant was trained to a baseline
FID (sampling with an adaptive solver) of 5.53, and
was used as the teacher for EMD and LMD. The
teacher for PFMM was an FMM model trained with
|t−s|<0.25to an FID of 8.44 using 8-step sam-
pling. We observe that LMD and EMD methods
can eﬀectively distill their teachers and obtain low
T-FID scores. In addition, the 2 and 4-step samples
from these methods far outperform the stochastic
interpolant. This sampling eﬃciency is also apparent
May 21, 2024
1
SILMDEMDPFMM<latexit sha1_base64="xs8X4xjj7oqQObuVxG/Wx1o9QpY=">AAAD1XichZLNbtNAEMe3NR8lfDSFIxeLCAmhENlJE8IBqQIkuAAFkaZSEkXr9cRZZb1r7a5LjOUb4soDcIUr78PbME5diSQIVlrrr5nfzI5nJkgEN9bzfu3sOpcuX7m6d612/cbNW/v1g9snRqWawYApofRpQA0ILmFguRVwmmigcSBgGCyel/7hGWjDlfxgswQmMY0kn3FGLZqm9f2xhaUNZvkb96nr94ppveG1vNVxt4VfiQapzvH0YPfnOFQsjUFaJqgxI99L7CSn2nImoKiNUwMJZQsawSg844mRNAYzyZer4gv3PvpDd6Y0XmndlfXPoJzGxmRxgGRM7dxs+krjX32BpguwawXkjEoGovgfN0rtrD/JuUxSC5KdFzlLhWuVW3bRDbkGZkWGgjLN8VddNqeaMou9Xq9QRAqBebz+qOWLT9uWR50wEaqqpTQIjtXpLA+BKb0amWnFVC+4jEyx+U5iIMVRqBC7Xhu/AByHhtfYl7cJYLDSD/Mx1VFMlwWOJxo3S/UvkMsLEBWmDGGGa7aaUE7TINXo11FQ5F7rsNPENWn75dcv1tE4C0QKRf7+5TNEm37vSbPd2YYyEEJ9rLB2F8F+t+l3t7hIA8gK65fp+ni3MQ3hRa4O5vG8Zh+hWg033N/c521x0m75vVb33WHjqF3t+h65S+6RB8Qnj8kReUWOyYAwkpJv5Dv54QydwvnsfDlHd3eqmDtk7ThffwO8tUMK</latexit>N = 16<latexit sha1_base64="myhtRxCyBr8dQkz0MwtvAIWLjO4=">AAAD1XichZLNbtNAEMe3CR8lfDSFIxeLCAkhE9lJU8IBqQIkuAAFkaZSHEXr9cRZZe21dtclxvINceUBuMKV9+FtGKeuRGIEK63118xvZscz4yeCa+M4v3YazUuXr1zdvda6fuPmrb32/u0TLVPFYMSkkOrUpxoEj2FkuBFwmiigkS9g7C+fl/7xGSjNZfzBZAlMIxrGfM4ZNWiatfc8Ayvjz/M31lOr3ytm7Y7TddbHqgu3Eh1SnePZfuOnF0iWRhAbJqjWE9dJzDSnynAmoGh5qYaEsiUNYRKc8UTHNAI9zVfr4gvrPvoDay4V3thYa+ufQTmNtM4iH8mImoXe9pXGv/p8RZdgNgrIGY0ZiOJ/3CQ18+E053GSGojZeZHzVFhGWmUXrYArYEZkKChTHH/VYguqKDPY680KRSgRWESbjxq+/FS3POoHiZBVLaVBcKxOZXkATKr1yHQ3omrJ41AX2+8kGlIchQyw6y3vBeA4FLzGvrxNAIOleph7VIURXRU4ntCzS/UvkMcXICpMGcAc12w9oZymfqrQr0K/yJ3uQd/GNem55dctNtEo80UKRf7+5TNEbffwid3r16EMhJAfK6w3QHA4sN1BjQsVQFxhwzLdEG8dUxBc5OpjHsexhwi1Wrjh7vY+18VJr+sedgfvDjpHvWrXd8ldco88IC55TI7IK3JMRoSRlHwj38mP5rhZND83v5yjjZ0q5g7ZOM2vvwG180MI</latexit>N = 32<latexit sha1_base64="LWeqg0tLZGGn8MHntWPM6i+wCfs=">AAAD1XichZJLj9MwEMe9DY+lPLYLRy4RFRJCoUr6ohyQVoAEF2BBdLtSW1WOM02tOnFkO0tLlBviygfgCle+D9+GSTcr0QaBJUd/zfxmPJkZPxFcG9f9tVezLl2+cnX/Wv36jZu3DhqHt0+0TBWDIZNCqlOfahA8hqHhRsBpooBGvoCRv3xe+EdnoDSX8QezTmAa0TDmc86oQdOscTAxsDL+PHtjP7X73XzWaLotd3PsqvBK0STlOZ4d1n5OAsnSCGLDBNV67LmJmWZUGc4E5PVJqiGhbElDGAdnPNExjUBPs9Wm+Ny+j/7AnkuFNzb2xvpnUEYjrdeRj2REzULv+grjX32+okswWwVkjMYMRP4/bpya+WCa8ThJDcTsvMh5Kmwj7aKLdsAVMCPWKChTHH/VZguqKDPY6+0KRSgRWETbjxq+/FS1POoEiZBlLYVBcKxOrbMAmFSbkelWRNWSx6HOd99JNKQ4Chlg1+uTF4DjUPAa+/I2AQyW6mE2oSqM6CrH8YQTp1D/Anl8AaLClAHMcc02E8po6qcK/Sr088xtdTsOrknbK75evo1Ga1+kkGfvXz5D1PH6T5x2pwqtQQj5scTaPQQHPcfrVbhQAcQlNijSDfBWMQXBRa4O5nFdZ4BQvY4b7u3uc1WctFtev9V7120etctd3yd3yT3ygHjkMTkir8gxGRJGUvKNfCc/rJGVW5+tL+doba+MuUO2jvX1N8bgQw0=</latexit>N = 64
FMM
A
103105
Training Step101102103Loss
LMD, ImageNet-32x32
EMD, ImageNet-32x32LMD, CIFAR10
EMD, CIFAR10
100000 200000
Training Step20406080100
1-step FID
 B
Figure 4: (A) Qualitative comparison between the standard stochastic interpolant approach (SI), Lagrangian map
distillation (LMD), Eulerian map distillation (EMD), and progressive ﬂow map matching (PFMM). SI produces good
images for a suﬃciently large number of steps, but performs poorly for few steps. LMD performs well in the very-few
step regime, and outperforms EMD signiﬁcantly. PFMM performs well at any number of steps, though performs
slightly worse than LMD in the very-few step regime. (B) Quantitative comparison between EMD and LMD on both
CIFAR-10 and ImageNet 32×32. Despite both having the same minimizer, LMD trains faster, and attains a lower
loss value and a lower FID for a ﬁxed number of training steps.
10Under review as submission to TMLR
in the left side of Figure 4, in which with just 1 to 4 steps, the LMD and PFMM methods can produce
eﬀective samples, particularly when compared to the ﬂow matching approach.
Without any distillation, FMM can also produce eﬀective few-step maps. Training an FMM model on the
ImageNet- 32×32dataset, we observe (Table 2) that FMM achieves much better few-step FID when compared
to denoising diﬀusion models (DDPM), and better FID than mini-batch OT interpolant methods ( Pooladian
et al. ,2023). In the higher-step regime, the interpolant methods perform marginally better.
Eulerian vs Lagrangian distillation Remarkably, we ﬁnd a stark performance gap between the Eulerian
and Lagrangian distillation schemes. This is evident in both parts of Figure 4, where we see that higher-step
sampling with EMD only marginally improves image quality, and where the LMD loss for both CIFAR10 and
ImageNet- 32×32converges an order of magnitude faster than the EMD loss. The same holds for FIDs over
training, given in the right-most plot in the ﬁgure. Note that both LMD and EMD loss functions have a
global minimum at 0, so that the loss plots suggest continued training will improve distillation quality, but at
very diﬀerent rates.
5 Conclusion
In this work, we developed several ways to learn a two-time ﬂow map for generative modeling: either by
distilling a pre-trained velocity model with Eulerian or Lagrangian losses, or by directly training with the
stochastic interpolant framework. We empirically observe that while using more steps with the learned map
improves sample quality, a substantially lower number is needed when compared to other generative models
built on dynamical transport. Future work will investigate how to improve the training and the neural
network architecture so as to further reduce the number of steps without sacriﬁcing accuracy, and to improve
convergence for direct training of one-step maps.
References
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-based generative modeling through stochastic diﬀerential equations. arXiv:2011.13456 , 2020.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diﬀusion probabilistic models. In Advances in neural
information processing systems , volume 33, pages 6840–6851, 2020.
Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning
using nonequilibrium thermodynamics. arXiv:1503.03585 , 2015.
Yang Song and Stefano Ermon. Improved Techniques for Training Score-Based Generative Models.
arXiv:2006.09011 , October 2020a.
Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution.
arXiv:1907.05600 , 2020b.
Prafulla Dhariwal and Alex Nichol. Diﬀusion Models Beat GANs on Image Synthesis. arXiv:2105.05233 ,
2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diﬀusion models. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 10684–10695, 2022.
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi,
Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey,
Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectiﬁed ﬂow transformers for high-resolution
image synthesis. arXiv:2403.03206 , 2024.
Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-TTS: A
Diﬀusion Probabilistic Model for Text-to-Speech. In Proceedings of the 38th International Conference on
Machine Learning , pages 8599–8608. PMLR, July 2021.
11Under review as submission to TMLR
Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, and Nam Soo Kim. Diﬀ-TTS: A
Denoising Diﬀusion Model for Text-to-Speech. arXiv:2104.01409 , 2021.
Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, and Yi Ren. ProDiﬀ: Progressive Fast
Diﬀusion Model for High-Quality Text-to-Speech. In Proceedings of the 30th ACM International Conference
on Multimedia , MM ’22, pages 2595–2605, New York, NY, USA, October 2022. Association for Computing
Machinery.
Yen-Ju Lu, Zhong-Qiu Wang, Shinji Watanabe, Alexander Richard, Cheng Yu, and Yu Tsao. Conditional
Diﬀusion Probabilistic Model for Speech Enhancement. In ICASSP 2022 - 2022 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 7402–7406, May 2022a.
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma,
Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen Video: High Deﬁnition Video
Generation with Diﬀusion Models. arXiv:2210.02303 , 2022a.
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet.
Video Diﬀusion Models. Advances in Neural Information Processing Systems , 35:8633–8646, December
2022b.
Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and
Karsten Kreis. Align Your Latents: High-Resolution Video Synthesis With Latent Diﬀusion Models.
arXiv:2304.08818 , 2023.
Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying
Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-A-Video: One-Shot Tuning of Image Diﬀusion Models for
Text-to-Video Generation. arXiv:2212.11565 , 2023.
Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for
generative modeling. In The Eleventh International Conference on Learning Representations , 2022.
Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer
data with rectiﬁed ﬂow. In The Eleventh International Conference on Learning Representations , 2022a.
Michael S Albergo and Eric Vanden-Eijnden. Building normalizing ﬂows with stochastic interpolants. In The
Eleventh International Conference on Learning Representations , 2022.
Michael S Albergo, Nicholas M Boﬃ, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework
for ﬂows and diﬀusions. arXiv preprint arXiv:2303.08797 , 2023a.
Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchﬁel, Russ Tedrake, and
Shuran Song. Diﬀusion Policy: Visuomotor Policy Learning via Action Diﬀusion. arXiv:2303.04137 , 2024.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neural Information Processing
Systems , volume 27. Curran Associates, Inc., 2014.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM , 63(11):
139–144, October 2020.
Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A. Bharath.
Generative Adversarial Networks: An Overview. IEEE Signal Processing Magazine , 35(1):53–65, January
2018.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled Generative Adversarial Networks.
arXiv:1611.02163 , 2017.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein GAN. arXiv:1701.07875 , 2017.
12Under review as submission to TMLR
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the Design Space of Diﬀusion-Based
Generative Models. arXiv:2206.00364 , 2022.
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency Models. arXiv:2303.01469 , 2023.
Yang Song and Prafulla Dhariwal. Improved Techniques for Training Consistency Models. arXiv:2310.14189 ,
2023.
Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong
He, Yuki Mitsufuji, and Stefano Ermon. Consistency Trajectory Models: Learning Probability Flow ODE
Trajectory of Diﬀusion. arXiv:2310.02279 , 2024.
Tim Salimans and Jonathan Ho. Progressive Distillation for Fast Sampling of Diﬀusion Models.
arXiv:2202.00512 , 2022.
Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast Sampling
of Diﬀusion Models via Operator Learning. arXiv:2211.13449 , 2023.
Cédric Villani. Optimal transport: old and new , volume 338. Springer, 2009.
Jean-David Benamou and Yann Brenier. A computational ﬂuid mechanics solution to the monge-kantorovich
mass transfer problem. Numerische Mathematik , 84(3):375–393, 2000.
Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, NY , 55(58-63):94, 2015.
Esteban G. Tabak and Eric Vanden-Eijnden. Density estimation by dual ascent of the log-likelihood.
Communications in Mathematical Sciences , 8(1):217–233, 2010.
E. G. Tabak and Cristina V. Turner. A family of nonparametric density estimation algorithms. Communications
on Pure and Applied Mathematics , 66(2):145–164, 2013.
Danilo Rezende and Shakir Mohamed. Variational Inference with Normalizing Flows. In International
Conference on Machine Learning , pages 1530–1538. PMLR, June 2015.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density Estimation Using Real NVP. In International
Conference on Learning Representations , page 32, 2017.
Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. FFJORD:
Free-form Continuous Dynamics for Scalable Reversible Generative Models. arXiv:1810.01367 , 2018.
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural Ordinary Diﬀerential
Equations. arXiv:1806.07366 , 2019.
Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us build bridges: Understanding and extending
diﬀusion generative models. arXiv preprint arXiv:2208.14699 , 2022b.
Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M. Boﬃ, Eric Vanden-Eijnden, and Saining
Xie. SiT: Exploring Flow and Diﬀusion-based Generative Models with Scalable Interpolant Transformers.
arXiv:2401.08740 , 2024.
Yifan Chen, Mark Goldstein, Mengjian Hua, Michael S. Albergo, Nicholas M. Boﬃ, and Eric Vanden-Eijnden.
Probabilistic Forecasting with Stochastic Interpolants and Föllmer Processes. arXiv:2403.13724 , 2024.
Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and
Ricky Chen. Multisample ﬂow matching: Straightening ﬂows with minibatch couplings. arXiv preprint
arXiv:2304.14772 , 2023.
Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks,
Guy Wolf, and Yoshua Bengio. Improving and generalizing ﬂow-based generative models with minibatch
optimal transport. Transactions on Machine Learning Research , 2023.
13Under review as submission to TMLR
Michael S. Albergo, Mark Goldstein, Nicholas M. Boﬃ, Rajesh Ranganath, and Eric Vanden-Eijnden.
Stochastic interpolants with data-dependent couplings. arXiv:2310.03725 , 2023b.
Qinsheng Zhang and Yongxin Chen. Fast Sampling of Diﬀusion Models with Exponential Integrator.
arXiv:2204.13902 , 2023.
Alexia Jolicoeur-Martineau, Ke Li, Rémi Piché-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta Go
Fast When Generating Data with Score-Based Models. arXiv:2105.14080 , 2021.
Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo Numerical Methods for Diﬀusion Models on Manifolds.
arXiv:2202.09778 , 2022c.
Cheng Lu, Yuhao Zhou, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver: A Fast ODE Solver for
Diﬀusion Probabilistic Model Sampling in Around 10 Steps. arXiv:2206.00927 , 2022b.
Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to Eﬃciently Sample from
Diﬀusion Probabilistic Models. arXiv:2106.03802 , 2021.
Daniel Watson, William Chan, Jonathan Ho, and Mohammad Norouzi. Learning Fast Samplers for Diﬀusion
Models by Diﬀerentiating Through Sample Quality. arXiv:2202.05830 , 2022.
Alex Nichol and Prafulla Dhariwal. Improved Denoising Diﬀusion Probabilistic Models. arXiv:2102.09672 ,
2021.
Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Naoki Murata, Yuki Mitsufuji, and Stefano Ermon.
On the Equivalence of Consistency-Type Models: Consistency Models, Consistent Diﬀusion Models, and
Fokker-Planck Regularization. arXiv:2306.00367 , 2023a.
Chieh-Hsin Lai, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon.
Improving Score-based Diﬀusion Models by Enforcing the Underlying Score Fokker-Planck Equation.
arXiv:2210.04296 , 2023b.
Zebang Shen, Zhenfu Wang, Satyen Kale, Alejandro Ribeiro, Aim Karbasi, and Hamed Hassani. Self-
Consistency of the Fokker-Planck Equation. arXiv:2206.00860 , 2022.
Nicholas M. Boﬃ and Eric Vanden-Eijnden. Probability ﬂow solution of the Fokker–Planck equation. Machine
Learning: Science and Technology , 4(3):035012, July 2023.
Giannis Daras, Yuval Dagan, Alexandros G. Dimakis, and Constantinos Daskalakis. Consistent Diﬀusion
Models: Mitigating Sampling Drift by Learning to be Consistent. arXiv:2302.09057 , 2023.
Eric Luhman and Troy Luhman. Knowledge Distillation in Iterative Generative Models for Improved Sampling
Speed. arXiv:2101.02388 , January 2021.
Philip Hartman. Ordinary Diﬀerential Equations . Society for Industrial and Applied Mathematics, second
edition, 2002. doi: 10.1137/1.9780898719222.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980 , 2017.
M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: A deep learning framework
for solving forward and inverse problems involving nonlinear partial diﬀerential equations. Journal of
Computational Physics , 378:686–707, 2019.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diﬀusion Implicit Models. arXiv:2010.02502 ,
2022.
14Under review as submission to TMLR
A Proofs for Section 3
Proposition 3.3. The ﬂow map Xs,t(x)is the unique solution to the Lagrangian equation
∂tXs,t(x) =bt(Xs,t(x)), X s,s(x) =x, (3.5)
for all (s, t, x )∈[0, T]2×Rd. In addition, it satisﬁes
Xt,τ(Xs,t(x)) =Xs,τ(x) (3.6)
for all (s, t, τ, x )∈[0, T]3×Rd; in particular Xs,t(Xt,s(x)) = xfor all (s, t, x )∈[0, T]2×Rd, i.e. the ﬂow
map is invertible.
Proof. Taking the derivative with respect to tofXs,t(xs) =xt, we deduce
∂tXs,t(xs) = ˙xt=bt(xt) =bt(Xs,t(xs)) (A.1)
where we used the ODE (3.1)to obtain the second equality. Evaluating this expression at xs=xgives Equa-
tion ( 3.5). Also, for all (s, τ, t )∈[0, T]3, we have
Xτ,t(Xs,τ(xs)) =Xτ,t(xτ) =xt=Xs,t(xs). (A.2)
Evaluating this expression at xs=xgives Equation ( 3.6).
Corollary 3.4 (Lagrangian map distillation) .The ﬂow map is the global minimizer over ˆXof the loss
LLMD (ˆX) =/integraldisplay
[0,T]2/integraldisplay
Rdws,t/vextendsingle/vextendsingle∂tˆXs,t(x)−bt(ˆXs,t(x))/vextendsingle/vextendsingle2ρs(x)dxdsdt, (3.7)
subject to the boundary condition that ˆXs,s(x) =xfor all x∈Rdands∈[0, T]. In(3.7),ws,t>0is a weight
function and ρsis the solution to (3.3).
Proof. Equation (3.7)is a physics-informed neural network (PINN) ( Raissi et al. ,2019) loss that is minimized
only when the integrand is zero, i.e., when ( 3.5) holds.
Proposition 3.5. The ﬂow map Xs,tis the unique solution of the Eulerian equation
∂sXs,t(x) +bs(x)· ∇Xs,t(x) = 0 , X t,t(x) =x, (3.8)
for all (s, t, x )∈[0, T]2×Rd.
Proof. Taking the derivative with respect to sofXs,t(Xt,s(x)) =xand using the chain rule, we deduce that
0 =d
dsXs,t(Xt,s(x)) =∂sXs,t(Xt,s(x)) +∂sXt,s(x)· ∇Xs,t(Xt,s(x))
=∂sXs,t(Xt,s(x)) +bs(Xt,s(x))· ∇Xs,t(Xt,s(x))(A.3)
where we used Equation ( 3.5) to get the last equality. Evaluating this expression at Xt,s(x) =y, then
changing yintox, gives Equation ( 3.8).
Corollary 3.6 (Eulerian map distillation) .The ﬂow map is the global minimizer over ˆXof the loss
LEMD (ˆX) =/integraldisplay
[0,T]2/integraldisplay
Rdws,t/vextendsingle/vextendsingle∂sˆXs,t(x) +bs(x)· ∇ˆXs,t(x)/vextendsingle/vextendsingle2ρs(x)dxdsdt, (3.9)
subject to the boundary condition ˆXs,s(x) =xfor all x∈Rdand for all s∈R. In (3.9),ws,t>0is a weight
function and ρsis the solution to (3.3).
15Under review as submission to TMLR
Proof. Equation (3.9)is a PINN loss that is minimized only when the integrand is zero, i.e, when (3.8)
holds.
Proposition 3.11 (Flow map matching) .The ﬂow map is the global minimizer over ˆXof the loss
LFMM [ˆX] =/integraldisplay
[0,1]2ws,t/parenleftBig
E/bracketleftbig
|∂tˆXs,t(ˆXt,s(It))−˙It|2/bracketrightbig
+E/bracketleftbig
|ˆXs,t(ˆXt,s(It))−It|2/bracketrightbig/parenrightBig
dsdt. (3.15)
In(3.15),ws,t>0andEis taken over the coupling (x0, x1)∼ρ(x0, x1)andz∼N(0,Id).
Proof. We start by noticing that
E/bracketleftbig
|∂tˆXs,t(ˆXt,s(It))−˙It|2,
=E/bracketleftbig
|∂tˆXs,t(ˆXt,s(It))|2−2˙It·∂tˆXs,t(ˆXt,s(It)) +|˙It|2/bracketrightbig
,
=E/bracketleftbig
|∂tˆXs,t(ˆXt,s(It))|2−2E[˙It|It]·∂tˆXs,t(ˆXt,s(It)) +|˙It|2/bracketrightbig
,
=E/bracketleftbig
|∂tˆXs,t(ˆXt,s(It))|2−2bt(It)·∂tˆXs,t(ˆXt,s(It)) +|˙It|2/bracketrightbig
,(A.4)
where we used the tower property of the conditional expectation to get the third equality and the deﬁnition
ofbt(x)in (3.13) to get the last. This means that the loss ( 3.15) can be written as
LFMM [ˆX]
=/integraldisplay
[0,1]2/integraldisplay
Rdws,t/bracketleftbig
|∂tˆXs,t(ˆXt,s(x))−bt(x)|2+|ˆXs,t(ˆXt,s(x))−x|2/bracketrightbig
ρt(x)dxdsdt
+/integraldisplay
[0,1]2ws,tE/bracketleftbig
|˙It|2− |bt(It)|2/bracketrightbig
dsdt,(A.5)
where ρt=Law(It). The second integral does not depend on ˆX, so it does not aﬀect the minimization of
LFMM [ˆX]. Assuming that ws,t>0, the ﬁrst integral is minimized if and only if we have
∀(s, t, x )∈[0,1]2×Rd:∂tˆXs,t(ˆXt,s(x)) =bt(x)and ˆXs,t(ˆXt,s(x)) =x. (A.6)
From the second of these equations it follows that: (i) ˆXs,s(x) =x, and (ii) if we evaluate the ﬁrst equation
aty=ˆXt,s(x), this equation reduces to
∀(s, t, y )∈[0,1]2×Rd:∂tˆXs,t(y) =bt(ˆXs,t(y)) (A.7)
which recovers ( 3.5).
Lemma 3.12 (Progressive ﬂow map matching) .The unique minimizer over ˇXof the loss
LPFMM [ˇX] =/integraldisplay
[0,1]2ws,tE/bracketleftBig/vextendsingle/vextendsingleˇXs,t(Is)−/parenleftbigˆXtK−1,tK◦ ··· ◦ ˆXt1,t2/parenrightbig
(Is)/vextendsingle/vextendsingle2/bracketrightBig
dsdt, (3.16)
produces the same output in one step as the K-step iterated map ˆX. In (3.16),ws,t>0,Eis taken over the
coupling (x0, x1)∼ρ(x0, x1)andz∼N(0,Id), and tk=s+ (k−1)(t−s)fork= 1, . . . , K .
Proof. Equation ( 3.16) is a PINN loss whose unique minimizer satisﬁes
∀(s, t, x )∈[0, T]2×Rd:ˇXs,t(x) =/parenleftbigˆXtK−1,tK◦ ··· ◦ ˆXt1,t2/parenrightbig
(x), (A.8)
which establishes the claim.
16Under review as submission to TMLR
Proposition 3.7 (Lagrangian error bound) .LetXs,t:Rd→Rddenote the ﬂow map for b, and let
ˆXs,t:Rd→Rddenote an approximate ﬂow map. Let ˆρ1=ˆX0,1/sharpρ0andρb
1=X0,1/sharpρ0. Then,
W2
2(ρb
1,ˆρ1)6e1+2/integraltext1
0|Ct|dt/integraldisplay1
0E/bracketleftbig/vextendsingle/vextendsinglebt(ˆX0,t(x0))−∂tˆX0,t(x0)/vextendsingle/vextendsingle2/bracketrightbig
dt6e1+2/integraltext1
0|Ct|dtLLMD (ˆX). (3.10)
where Ctis the constant appearing in Assumption 3.1.
Proof. First observe that, by the one-sided Lipschitz condition ( 3.2),
∂t|Xs,t(x)−Xs,t(y)|2= 2(Xs,t(x)−Xs,t(y))·(bt(Xs,t(x))−bt(Xs,t(y))),
62Ct|Xs,t(x)−Xs,t(y)|2.(A.9)
Equation (A.9)highlights that (3.2)gives a bound on the spread of trajectories. We note that we allow for
Ct<0, which corresponds to globally contracting maps. Given ( A.9), we now deﬁne
Es,t=E/bracketleftbig/vextendsingle/vextendsingleXs,t(Is)−ˆXs,t(Is)/vextendsingle/vextendsingle2/bracketrightbig
, (A.10)
where we recall that Xs,t(x)satisﬁes ∂tXs,t(x) =bt(Xs,t(x))andXs,s(x) =x. Taking the derivative with
respect to tof (A.10), we deduce
∂tEs,t= 2E/bracketleftbig/parenleftbig
Xs,t(Is)−ˆXs,t(Is)/parenrightbig
·/parenleftbig
bt(Xs,t(Is))−∂tˆXs,t(Is)/parenrightbig/bracketrightbig
,
= 2E/bracketleftbig/parenleftbig
Xs,t(Is)−ˆXs,t(Is)/parenrightbig
·/parenleftbig
bt(ˆXs,t(Is))−∂tˆXs,t(Is)/parenrightbig/bracketrightbig
+ 2E/bracketleftbig/parenleftbig
Xs,t(Is)−ˆXs,t(Is)/parenrightbig
·/parenleftbig
bt(Xs,t(Is))−bt(ˆXs,t(Is))/parenrightbig/bracketrightbig
,
6E/bracketleftbig/vextendsingle/vextendsingleXs,t(Is)−ˆXs,t(Is)/vextendsingle/vextendsingle2/bracketrightbig
+E/bracketleftbig/vextendsingle/vextendsinglebt(ˆXs,t(Is))−∂tˆXs,t(Is)/vextendsingle/vextendsingle2/bracketrightbig
+ 2E/bracketleftbig/parenleftbig
Xs,t(Is)−ˆXs,t(Is)/parenrightbig
·/parenleftbig
bt(Xs,t(Is))−bt(ˆXs,t(Is))/parenrightbig/bracketrightbig
,
≡Es,t+δLMD
s,t + 2E/bracketleftbig/parenleftbig
Xs,t(Is)−ˆXs,t(Is)/parenrightbig
·/parenleftbig
bt(Xs,t(Is))−bt(ˆXs,t(Is))/parenrightbig/bracketrightbig
.(A.11)
Above, we deﬁned the two-time Lagrangian distillation error,
δLMD
s,t =E/bracketleftbig/vextendsingle/vextendsinglebt(ˆXs,t(Is))−∂tˆXs,t(Is)/vextendsingle/vextendsingle2/bracketrightbig
. (A.12)
By deﬁnition, the LMD loss can be expressed as LLMD(ˆX) =/integraltext
[0,1]2ws,tδLMD
s,tdsdt. Using (3.2)in(A.11), we
obtain the relation
∂tEs,t6(1 + 2 Ct)Es,t+δLMD
s,t, (A.13)
which implies that
∂t/parenleftbig
e−t−2/integraltextt
sCuduEs,t/parenrightbig
6e−t−2/integraltextt
sCuduδLMD
s,t. (A.14)
Since Es,s= 0this implies that
Es,t6/integraldisplayt
se(t−u)+2/integraltextt
uCτdτδLMD
s,udu6e1+2/integraltextt
s|Cτ|dτ/integraldisplayt
sδLMD
s,udu. (A.15)
Above, we used that (t, u)∈[0,1]2so that (t−u)61. This bound shows that E0,16e1+2/integraltext1
0|Cτ|dτ/integraltext1
0δLMD
0,udu,
which can be written explicitly as (using tinstead of uas dummy integration variable)
E/bracketleftbig/vextendsingle/vextendsingleX0,1(x0)−ˆX0,1(x0)/vextendsingle/vextendsingle2/bracketrightbig
6e1+2/integraltext1
0|Cτ|dτ/integraldisplay1
0E/bracketleftbig/vextendsingle/vextendsinglebt(ˆX0,t(x0))−∂tˆX0,t(x0)/vextendsingle/vextendsingle2/bracketrightbig
dt, (A.16)
Now, observe that by deﬁnition,
W2
2(ρb,ˆρ)6E/bracketleftbig/vextendsingle/vextendsingleX0,1(x0)−ˆX0,1(x0)/vextendsingle/vextendsingle2/bracketrightbig
, (A.17)
because the left-hand side is the inﬁmum over all couplings and the right-hand side corresponds to a speciﬁc
coupling that pairs points from the same initial condition. This completes the proof.
17Under review as submission to TMLR
Proposition 3.8 (Eulerian error bound) .LetXs,t:Rd→Rddenote the ﬂow map for b, and let ˆXs,t:Rd→
Rddenote an approximate ﬂow map. Denote by ˆρ1=ˆX0,1/sharpρ0andρb
1=X0,1/sharpρ0. Then,
W2
2(ρb
1,ˆρ1)6e/integraldisplay1
0E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle∂sˆXs,1(Is) +bs(Is)· ∇ˆXs,1(Is)/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
ds6eLEMD (ˆX). (3.11)
Proof. We ﬁrst deﬁne the error metric
Es,t=E/bracketleftBig/vextendsingle/vextendsingleXs,t(Is)−ˆXs,t(Is)/vextendsingle/vextendsingle2/bracketrightBig
. (A.18)
It then follows by direct diﬀerentiation that
∂sEs,t=E/bracketleftBig
2/parenleftBig
Xs,t(Is)−ˆXs,t(Is)/parenrightBig
·/parenleftBig
∂sXs,t(Is) +˙Is· ∇Xs,t(Is)−/parenleftBig
∂sˆXs,t(Is) +˙Is· ∇ˆXs,t(Is)/parenrightBig/parenrightBig/bracketrightBig
,
=E/bracketleftBig
2/parenleftBig
Xs,t(Is)−ˆXs,t(Is)/parenrightBig
·/parenleftBig
∂sXs,t(Is) +bs(Is)· ∇Xs,t(Is)−/parenleftBig
∂sˆXs,t(Is) +bs(Is)· ∇ˆXs,t(Is)/parenrightBig/parenrightBig/bracketrightBig
,
>−Es,t−E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle∂sXs,t(Is) +bs(Is)· ∇Xs,t(Is)−/parenleftBig
∂sˆXs,t(Is) +bs(Is)· ∇ˆXs,t(Is)/parenrightBig/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
,
=−Es,t−δEMD
s,t.
Above, we used the tower property of the conditional expectation, the Eulerian equation ∂sXs,t(Is) +bs(Is)·
∇Xs,t(Is) = 0 , and deﬁned the two-time Eulerian distillation error,
δEMD
s,t =E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle∂sˆXs,t(Is) +bs(Is)· ∇ˆXs,t(Is)/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
. (A.19)
This implies that
∂s(−esEs,t)6esδEMD
st. (A.20)
Using that Et,t= 0for any t∈[0,1]and integrating with respect to sfrom stot,
−etEt,t+esEs,t6/integraldisplayt
seuδEMD
u,tdu. (A.21)
It then follows that
Es,t6/integraldisplayt
seu−sδEMD
u,tdu, (A.22)
and hence, using that u−s∈[0,1]and that δEMD
u,t>0,
E/bracketleftbig/vextendsingle/vextendsingleX0,1(x0)−ˆX0,1(x0)/vextendsingle/vextendsingle2/bracketrightbig
6e/integraldisplay1
0E/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle∂sˆXs,1(Is) +bs(Is)· ∇ˆXs,1(Is)/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
ds. (A.23)
The proof is completed upon noting that
W2
2(ρ1,ˆρ1)6E/bracketleftbig/vextendsingle/vextendsingleX0,1(x0)−ˆX0,1(x0)/vextendsingle/vextendsingle2/bracketrightbig
, (A.24)
because the left-hand side is the inﬁmum over all couplings and the right-hand side corresponds to a particular
coupling.
B Additional theoretical results
B.1 Flow maps and denoisers
Since Law( Xt,s(It)) = Law( Is), it is tempting to replace Xt,s(It)byIsin the loss ( 3.15) and use instead
Ldenoise [ˆX] =/integraldisplay
[0,1]2ws,tE/bracketleftbig
|∂tˆXs,t(Is)−˙It|2/bracketrightBig
dsdt, (B.1)
18Under review as submission to TMLR
minimized over all ˆXsuch that ˆXs,s(x) =x. However, the minimizer of this objective is notthe ﬂow map
Xs,t, but rather the denoiser
Xdenoise
s,t (x) =E[It|Is=x]. (B.2)
This can be seen by noticing that the minimizer of ( B.1) is the same as the minimizer of
L/prime
denoise [ˆX] =/integraldisplay
[0,1]2ws,tE/bracketleftbig/vextendsingle/vextendsingle∂tˆXs,t(Is)−E[˙It|Is]/vextendsingle/vextendsingle2/bracketrightBig
dsdt,
=/integraldisplay
[0,1]2/integraldisplay
Rdws,t/bracketleftBig/vextendsingle/vextendsingle∂tˆXs,t(x)−E[˙It|Is=x]/vextendsingle/vextendsingle2/bracketrightBig
ρs(x)dxdsdt,(B.3)
which follows from an argument similar to the one used in the proof of Proposition 3.11. The minimizer
of (B.3) satiﬁes
∂tˆXs,t(x) =E[˙It|Is=x] =∂tE[It|Is=x], (B.4)
which implies (B.2)by the boundary condition ˆXs,s(x) =x. The denoiser (B.2)may be useful, but it is not
a consistent generative model. For instance, if x0∼ρ0andx1∼ρ1are independent in the deﬁnition of It,
since I0=x0andI1=x1by construction, for s= 0andt= 1we have
Xdenoise
0,1 (x) =E[x1] (B.5)
i.e. the one-step denoiser only recovers the mean of the target density ρ1.
B.2 Eulerian estimation or Eulerian distillation?
In light of the proof of Proposition 3.11, the reader may wonder whether we could also perform direct
estimation in the Eulerian setup, using as loss
LE(ˆX) =/integraldisplay
[0,T]2ws,tE/bracketleftBig/vextendsingle/vextendsingle∂sˆXs,t(Is) +˙Is· ∇ˆXs,t(Is)/vextendsingle/vextendsingle2/bracketrightBig
dsdt. (B.6)
This loss is obtained from (3.9)by taking the expectation over Is, using Law(Is) =ρs, and replacing bs(Is)
by˙Is. Unfortunately, ( B.6) is not equivalent to ( 3.9). To see why, we can expand the expectation in ( B.6):
E/bracketleftBig/vextendsingle/vextendsingle∂sˆXs,t(Is) +˙Is· ∇ˆXs,t(Is)/vextendsingle/vextendsingle2/bracketrightBig
=E/bracketleftBig/vextendsingle/vextendsingle∂sˆXs,t(Is)/vextendsingle/vextendsingle2+ 2( ˙Is· ∇ˆXs,t(Is))·∂sˆXs,t(Is) +/vextendsingle/vextendsingle˙Is· ∇ˆXs,t(Is)/vextendsingle/vextendsingle2/bracketrightBig
.(B.7)
For the cross term (which is linear in ˙Is), we can use the tower property of the conditional expectation to see
that
E/bracketleftbig
(˙Is· ∇ˆXs,t(Is))·∂sˆXs,t(Is)/bracketrightbig
=E/bracketleftbig
(E[˙Is|Is]· ∇ˆXs,t(Is))·∂sˆXs,t(Is)/bracketrightbig
,
=E/bracketleftbig
(bs(Is)· ∇ˆXs,t(Is))·∂sˆXs,t(Is)/bracketrightbig
.(B.8)
However, the tower property cannot be applied to the last term in ( B.7) since it is quadratic in ˙It, i.e.
E/bracketleftBig/vextendsingle/vextendsingle˙Is· ∇ˆXs,t(Is)/vextendsingle/vextendsingle2/bracketrightBig
/negationslash=E/bracketleftBig/vextendsingle/vextendsinglebs(Is)· ∇ˆXs,t(Is)/vextendsingle/vextendsingle2/bracketrightBig
. (B.9)
Since this term depends on ˆX, it cannot be neglected in the minimization, and the minimizer of (B.6)is
not the same as that of (3.9). Recognizing this diﬃculty, consistency models ( Song et al. ,2023;Song and
Dhariwal ,2023;Kim et al. ,2024) use a time-discretized variant of (B.6), and place a stopgrad on the term
˙Is· ∇ˆXs,t(Is)when computing the gradient of the loss. The resulting iterative scheme used to update ˆX
then has a ﬁxed point at ˆX=X, but it is hard to guarantee that this ﬁxed point is stable and attractive as
the iteration is not a gradient descent scheme.
C Relation to existing consistency and distillation techniques
In this section, we recast consistency models and several distillation techniques in the language of our two-time
ﬂow map Xs,tto clarify their relation with our work.
19Under review as submission to TMLR
C.1 Relation to consistency models
Noising process. Following the recommendations in Karras et al. (2022) (which are followed by both Song
et al. (2023) and Song and Dhariwal (2023)), we consider the variance-exploding process4
˜xt=a+tz, t ∈[0, tmax], (C.1)
where a∼ρ1(data from the target density) and z∼N(0, I). In practice, practitioners often set tmax= 80.
In this section, because we follow the score-based diﬀusion convention, we set time so that t= 0recovers ρ1
and so that a Gaussian is recovered as t→ ∞ . The corresponding probability ﬂow ODE is given by
˙˜xt=−t∇logρt(˜xt), ˜xt=0=a∼ρ1 (C.2)
where ρt(x) =Law(˜xt). In practice, (C.2)is solved backwards in time from some terminal condition ˜xtmax.
To make contact with our formulation where time goes forward, we deﬁne xt= ˜xtmax−t, leading to
˙xt= (tmax−t)∇logρtmax−t(xt), x t=0∼N(x0, t2
maxI). (C.3)
To make touch with our ﬂow map notation, we then deﬁne
∂tXs,t(x) = (tmax−t)∇logρtmax−t(Xs,t(x)), X s,s(x) =x. (C.4)
Consistency function. By deﬁnition ( Song et al. ,2023), the consistency function ft:Rd→Rdis such
that
ft(˜xt) =a, (C.5)
where ˜xtdenotes the solution of (C.2)anda∼ρ1. To make a connection with our ﬂow map formulation, let
us consider ( C.5) from the perspective of xt,
ft(xtmax−t) =xtmax, (C.6)
which is to say that
ft(x) =Xtmax−t,tmax(x). (C.7)
Note that only one time is varied here, i.e. ft(x), cannot be iterated upon: by its deﬁnition (C.5), it always
maps the observation ˜xtonto a sample a∼ρ1.
Discrete-time loss function for distillation. In practice, consistency models are typically trained in
discrete-time, by discretizing [tmin, tmax]into a set of Npoints tmin=t1< t2< . . . < t N=tmax. According
toKarras et al. (2022), these points are chosen as
ti=/parenleftbigg
t1/η
min+i−1
N−1/parenleftBig
t1/η
max−t1/η
min/parenrightBig/parenrightbiggη
, (C.8)
with η= 7. Assuming that we have at our disposal a pre-trained estimate st(x)of the score ∇logρt(x), the
distillation loss for the consistency function ft(x)is then given by
LN
CD(ˆf) =N−1/summationdisplay
i=1E/bracketleftBig/vextendsingle/vextendsingleˆfti+1(˜xti+1)−ˆfti(ˆxti)/vextendsingle/vextendsingle2/bracketrightBig
,
˜xti+1=a+ti+1z
ˆxti= ˜xti+1−(ti−ti+1)ti+1sti+1(xti+1),(C.9)
where Eis taken over the data a∼ρ1andz∼N(0, I). The term ˆxtiis an approximation of ˜xticomputed by
taking a single step of (C.2)with the approximate score model st(x). In practice, the square loss in (C.9)can
be replaced by an arbitrary metric d:Rd→Rd→R>0, such as a learned metric like LPIPS or the Huber
loss.
4Oftentimes t= 0is set to t=tmin>0for numerical stability, choosing e.g. tmin = 2×10−3.
20Under review as submission to TMLR
Continuous-time limit. In continuous-time, it is easy to see via Taylor expansion that the consistency
loss reduces to
L∞
CD(ˆf) = lim
N→∞NLN
CD(ˆf) =/integraldisplaytmax
tmin/integraldisplay
Rdw2
t/vextendsingle/vextendsingle∂tft(x)−tst(x)· ∇ft(x)/vextendsingle/vextendsingle2ρt(x)dxdt, (C.10)
where wt=η(t1/η
max−t1/η
min)t1−1/ηis a weight factor arising from the nonuniform time-grid. This is a
particular case of our Eulerian distillation loss (3.9)applied to the variance-exploding setting (C.1)with the
identiﬁcation ( C.7).
Estimation vs distillation of the consistency model. If we approximate the exact
∇logρt(x) =−E/bracketleftbigg˜xt−a
t2/vextendsingle/vextendsingle/vextendsingle˜xt=x/bracketrightbigg
, (C.11)
by a single-point estimator
∇logρt(x)≈a−˜xt
t2, (C.12)
we may use the expression
ˆxti≈a+tiz, (C.13)
in (C.9) to obtain the estimation loss,
LN
CT(ˆf) =N−1/summationdisplay
i=1E/bracketleftBig/vextendsingle/vextendsingleˆfti+1(˜xti+1)−ˆf−
ti(˜xti)/vextendsingle/vextendsingle2/bracketrightBig
,
˜xti=a+tiz.(C.14)
This expression does not require a previously-trained score model. Notice, however, that (C.14)must be used
with a stopgrad onf−
ti(˜xti)so that the gradient is taken over only the ﬁrst ˆfti+1(˜xti+1). This is because (C.9)
and(C.14)are diﬀerent objectives with diﬀerent minimizers, even at leading order after expansion in 1/N,
for the same reason that ( 3.9) diﬀers from ( B.6). To see this, observe that to leading order,
ˆf−
ti(˜xti) = ˆf−
ti+1(˜xti+1) +/parenleftbig
∂tˆf−
ti+1(˜xti+1) +z· ∇f−
ti+1(˜xti+1)/parenrightbig
(ti−ti+1) +O/parenleftbig
(ti−ti+1)2/parenrightbig
, (C.15)
which gives the continuous-time limit
L∞
CT(ˆf) = lim
N→∞LN
CD(ˆf) =/integraldisplaytmax
tmin/integraldisplay
Rdwt/vextendsingle/vextendsingle∂tft(x) +z· ∇f−
t(x)/vextendsingle/vextendsingle2ρt(x)dxdt. (C.16)
Observing that z=∂t˜xtshows that (C.16)recovers the Eulerian estimator described in Appendix B.2, which
does not lead to a gradient descent iteration.
C.2 Relation to neural operators
In our notation, neural operator approaches for fast sampling of diﬀusion models ( Zheng et al. ,2023) also
estimate the ﬂow map X0,tvia the loss
LFNO(ˆX) =/integraldisplay1
0/integraldisplay
Rd/vextendsingle/vextendsingleˆX0,t(x)−X0,t(x)/vextendsingle/vextendsingle2ρ0(x)dxdt, (C.17)
where ˆX0,tis parameterized by a Fourier Neural Operator and where X0,tis the ﬂow map obtained by
simulating the probability ﬂow ODE associated with a pre-trained (or given) bt(x). To avoid simulation at
learning time, they pre-generate a dataset of trajectories, giving access to X0,t(x)for many initial conditions
x∼ρ0. Much of the work focuses on the architecture of the FNO itself, which is combined with a U-Net.
21Under review as submission to TMLR
DKL(ρ1||ˆρ1)W2
2(ρ1,ˆρ1)W2
2(ˆρb
1,ˆρ1)L2error
SI 0.020 0.026 0.0 0.000
LMD 0.043 0.059 0.032 0.085
EMD 0.079 0.029 0.010 0.011
FMM, N= 1 0.104 0.021 – 0.026
FMM, N= 4 0.045 0.014 – 0.024
PFMM, N= 1 0.043 0.014 – 0.023
Table 3: Comparison of DKL(ρ||ˆρ)andW2
2(ρ,ˆρ), where ˆρis the pushforward density from the maps ˆX0,1(x0)for
the methods listed above. Additionally included is a comparison of L2expected error of the distillation methods
against their teacher ˆXSI
0,1given as E[|ˆXSI
0,1(x)−ˆX0,1(x)|2]. Intriguingly, LMD performs better in being distributionally
correct, as measured by the KL-divergence, but worse in preserving the coupling of the teacher model. The roles are
ﬂipped for EMD. This may highlight KL as a more informative metric in our case, as our aims are to sample correctly
in distribution. See Figure 5for a visualization.
C.3 Relation to progressive distillation
Progressive distillation ( Salimans and Ho ,2022) takes a DDIM sampler ( Song et al. ,2022) and trains a new
model to approximate two steps of the old sampler with one step of the new model. This process is iterated
repeatedly to successively halve the number of steps required. In our notation, this corresponds to minimizing
L∆t
PD(ˆX) =/integraldisplay1−2∆t
0/integraldisplay
Rd/vextendsingle/vextendsingleˆXt,t+2∆ t(x)−/parenleftbig
Xt+∆t,t+2∆ t◦Xt,t+∆t/parenrightbig
(x)/vextendsingle/vextendsingle2ρt(x)dxdt (C.18)
where Xis a pre-trained map from the previous iteration. This is then iterated upon, and ∆tis increased,
until what is left is a few-step model.
D Additional Experimental Details
D.1 2D checkerboard
Here, we provide further discussion and analysis of our results for generative modeling on the 2D checkerboard
distribution (Figure 3). Our KL-divergence estimates clearly highlight that there is a hierarchy of performance.
Of particular interest is the large discrepancy in performance between the Eulerian and Lagrangian distillation
techniques.
As noted in Figure 3and Table 3, LMD substantially outperforms its Eulerian counterpart in terms of
minimizing the KL-divergence between the target checkerboard density ρ1and model density ˆρ1=ˆX0,1/sharpρ0.
Interestingly, while LMD is more correct in distribution, EMD better preserves the original coupling
(x0,ˆXSI
0,1(x0))of the teacher model ˆXSI
0,1, as measured by the W2
2distance and the expected L2reconstruction
error, deﬁned as E[|ˆXSI
0,1(x)−ˆX0,1(x)|2]. Where this coupling is signiﬁcantly notpreserved is visualized in
Figure 5. For each model, we color code points for which |ˆXSI
0,1(x)−ˆX0,1(x)|2>1.0, highlighting where the
student map diﬀered from the teacher. We notice that the LMD map pushes initial conditions to an opposing
checker edge (purple) than where those initial conditions are pushed by the interpolant (blue). This is much
less common for the EMD map, but its performance is overall worse in matching the target distribution.
D.2 Image experiments
Here we include more experimental details for reproducing the results provided in Section 4. We use the U-Net
from the diﬀusion OpenAI paper ( Dhariwal and Nichol ,2021) with code given at https://github.com/ope-
nai/guided-diﬀusion . We use the same architecture for both CIFAR10 and ImageNet- 32×32experiments.
The architecture is also the same for training a velocity ﬁeld and for training a ﬂow map, barring the
augmentation of the time-step embedding in the U-Net to handle two times ( s, t) instead of one. Details of
the training conditions are presented in Table 4.
22Under review as submission to TMLR
SI, N=80
 FMM, N=1
 FMM, N=4
 PFMM, N=1
LMD, N=1
 EMD, N=1
Figure 5: Visualization of the diﬀerence in assignment of the maps ˆX0,1(x0)for the various models as compared
to the teacher/ground truth model ˆXSI
0,1(x0)for the same initial conditions from the base distribution x0. Points
that lie in the region |ˆXSI
0,1(x0)−ˆX0,1(x0)|2>1.0are colored as compared to the blue points, which represent where
the stochastic interpolant teacher mapped the same red initial conditions. This gives us an intuition for how well
each method precisely maintains the coupling (x0,ˆXSI
0,1(x0))from the teacher. Note that we are treating XSI
0,1as the
ground truth map here, as it is close to the exact map. The models based on FMM either don’t have a teacher or
have FMM , N= 4as a teacher, but all should have the same coupling at the minimizer.
CIFAR-10 ImageNet 32 ×32
Dimension 32 ×32 32 ×32
# Training point 5×1041,281,167
Batch Size 256 256
Training Steps (Lagrangian distillation) 1.5 ×1052.5×105
Training Steps (Eulerian distillation) 1.2 ×1052.5×105
Training Steps (Flow map matching) N/A 1 ×105
Training Steps (Progressive ﬂow map matching) 1.3 ×105N/A
U-Net channel dims 256 256
Learning Rate (LR) 0.0001 0 .0001
LR decay (every 1k epochs) 0.992 0.992
U-Net dim mult [1,2,2,2] [1,2,2,2]
Learned time embedding Yes Yes
#GPUs 4 4
Table 4: Hyperparameters and architecture for image datasets.
23