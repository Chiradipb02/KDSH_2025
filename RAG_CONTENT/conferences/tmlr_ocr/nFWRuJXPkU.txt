Published in Transactions on Machine Learning Research (08/2023)
Using Confounded Data in Latent Model-Based
Reinforcement Learning
Maxime Gasse maxime.gasse@servicenow.com
ServiceNow Research
Montréal QC, Canada
Damien Grasset damien.grasset@irt-saintexupery.com
IRT Saint Exupéry Canada
Montréal QC, Canada
Guillaume Gaudron guillaume.gaudron@ubisoft.com
Ubisoft La Forge
Bordeaux, France
Pierre-Yves Oudeyer pierre-yves.oudeyer@inria.fr
Inria Bordeaux Sud-Ouest
Bordeaux, France
Reviewed on OpenReview: https: // openreview. net/ forum? id= nFWRuJXPkU
Abstract
In the presence of confounding, naively using oﬀ-the-shelf oﬄine reinforcement learning
(RL) algorithms leads to sub-optimal behaviour. In this work, we propose a safe method to
exploit confounded oﬄine data in model-based RL, which improves the sample-eﬃciency
of an interactive agent that collects and learns from online, unconfounded data. First, we
import ideas from the well-established framework of do-calculus to express model-based RL
as a causal inference problem, thus bridging the gap between the ﬁelds of RL and causality.
Then, we propose a generic method for learning a causal transition model from oﬄine and
online data, which captures and corrects the confounding eﬀect using a hidden latent variable.
We demonstrate that our method is correct and eﬃcient, in the sense that it attains better
generalization guarantees thanks to the confounded oﬄine data (in the asymptotic case),
regardless of the confounding eﬀect (the oﬄine expert’s behaviour). We showcase our method
on a series of synthetic experiments, which demonstrate that a) using confounded oﬄine data
naively degrades the sample-eﬃciency of an RL agent collecting and learning from online
data; b) using confounded oﬄine data correctly improves its sample-eﬃciency.
1 Introduction
As human beings, understanding cause and eﬀect is crucial to successfully navigate our environment. If
I take aspirin, will my headache go away? Should I better lie down for a while? Should I do both? Two
key ingredients in our learning process are observation (we passively contemplate our environment) and
experimentation (we perform actions, and we measure their outcomes). While it is well-known that passive
observation is not suﬃcient to infer cause and eﬀect1, it is hard to believe that it can provide no learning
signal at all. The ﬁeld of cosmology draws models of the universe by exploiting experiments on earth (particle
physics) and passive observations of the sky (astronomy). In our everyday lives, the actions of others (e.g.,
a coworker taking aspirin) can bring some insight into the eﬀects of our own actions. A key question is
1Simply put, correlation does not imply causation. Or, citing Pearl [29], “behind every causal conclusion there must lie some
causal assumption that is not testable in observational studies”.
1Published in Transactions on Machine Learning Research (08/2023)
then: which role does observation play when learning cause and eﬀect? This question is at the core of the
burgeoning ﬁeld of causality and reinforcement learning (RL) [2; 40; 16; 41; 42; 43; 26]2.
In this paper we consider the role of confounded data in the generic setting of model-based RL. Imagine
an agent trying to solve a sequential decision-making problem, such as a bot in a videogame. The agent
can rely on observational data, collected from the passive observation of other agents (e.g., a dataset of
oﬄine traces collected from other bots or humans), and experimental data, collected through the agent’s own
interactions (e.g., a dataset of online traces collected during learning). We are interested in scenarios where
the observational data is confounded, that is, when a hidden variable has been the cause of both actions and
their eﬀects in the traces (e.g., when the observed agent had access to privileged information). Our goal is
then to understand if and how the use of confounded observational data can improve the sample-eﬃciency of
an online model-based RL agent learning from experimental data.
In the Markov Decision Process (MDP) setting the entire state of the environment is available to the learning
agent at each time step, hence there can be no hidden confounder. Because the issue of confounding does not
exist, straightforward solutions can leverage large oﬄine datasets safely, leading to the fast-growing ﬁeld of
oﬄine reinforcement learning [ 20;21]. In the more general Partially-Observable MDP (POMDP) setting,
however, oﬄine data must be considered with more care, because of the potential presence of confounding. A
typical example is in the context of medicine, when oﬄine data is collected from physicians who may rely on
information absent from their patient’s medical records, such as their wealthiness or their lifestyle. Suppose
that wealthy patients in general get prescribed speciﬁc treatments by their physicians, because they can
aﬀord it, while being less at risk to develop severe conditions regardless of their treatment, because they can
also aﬀord a healthier lifestyle. This creates a spurious correlation called confounding, and will cause a naive
recommender system to wrongly infer that a treatment has positive health eﬀects. Another example is in the
context of autonomous driving, when oﬄine data is collected from human drivers who have a wider ﬁeld
of vision than the camera on which the robot driver relies. Suppose human drivers push the brakes when
they see a person waiting to cross the street, and only when the person walks in front of the car it enters the
camera’s ﬁeld of vision. Then, again, a naive robot might wrongly infer from its observations that whenever
the brakes are pushed, a person appears in front of the car. Suppose now that the robot’s objective is to
avoid collisions with pedestrians, then it might get regrettably reluctant to push the brakes. Of course, in
both those situations, the learning agent can infer the right causal model by disregarding the (confounded)
oﬄine data altogether, and by relying only on online data instead, collected from its own direct interactions.
However, in both those situations also, collecting online data can be expensive, impractical, or even unethical,
while collecting oﬄine data by observing the behaviour of human agents is much more aﬀordable.
In this paper we study the question of combining confounded oﬄine data with non-confounded, online data
in model-based RL, in the general Partially-Observable Markov Decision Process (POMDP) setting. Our
contribution is three-fold:
1.We formalize model-based RL as a causal inference problem using the framework of do-calculus [ 30],
which allows us to reason about confounding in the online and oﬄine scenarios in a formal and
intuitive manner (Section 3).
2.We present a generic method for combining online and oﬄine data in model-based RL (Section 4),
which we demonstrate is correct even when the oﬄine policy relies on privileged hidden information
(confounding), and is eﬃcient in the asymptotic case (inﬁnite oﬄine data).
3.We illustrate the eﬀectiveness of our method with a practical implementation for the tabular setting,
and three experiments on synthetic toy problems (Section 5).
While our proposed method can be formulated outside of the framework of do-calculus, in this paper we hope
to demonstrate that it oﬀers a principled and intuitive tool to reason about causality in model-based RL.
By relating common concepts from RL and causality, we wish that our contribution will ultimately help to
bridge the gap between the two communities.
2See section 6 for a discussion of related works.
2Published in Transactions on Machine Learning Research (08/2023)
S0pinit
Stptrans
St+1ptrans
O0pprv
Otpprv
Ot+1pprv
At
πstdAt−1
πstd
Figure 1: Standard POMDP regime.S0pinit
Stptrans
St+1ptrans
O0pprv
Otpprv
Ot+1pprv
At
πprvAt−1
πprv
Figure 2: Privileged regime (confounding).
2 Background
2.1 Notation
In this paper, upper-case letters in italics denote random variables (e.g. X,Y), while their lower-case
counterpart denote their value (e.g. x,y) and their calligraphic counterpart their domain (e.g., x∈X).
For simplicity we consider only discrete random variables. To keep our notation uncluttered, with a slight
abuse of notations we use p(x)to denote sometimes the event probability p(X=x), and sometimes the
whole probability distribution of X, which should be clear from the context. In sequential models we also
distinguish random variables with a temporal index t, which might be ﬁxed (e.g., o0,o1), or undeﬁned
(e.g.,p(st+1|st,at)denotes at the same time the distributions p(s1|s0,a0)andp(s2|s1,a1)). We also adopt
a compact notation for sequences of contiguous variables (e.g., s0→T= (s0,...,sT)∈ST+1), and for
summations over sets (/summationtext
x∈X⇐⇒/summationtextX
x). We assume the reader is familiar with the concepts of conditional
independence ( X⊥ ⊥Y|Z) and probabilistic graphical models based on directed acyclic graphs (DAGs),
which can be found in most introductory textbooks, e.g. Pearl [27]; Studeny [36]; Koller and Friedman [18].
2.2 Partially-Observable Markov Decision Process
We consider episodic Partially-Observable Markov Decision Processes (POMDPs) of the form M=
(S,O,A,pinit,ptrans,pprv,r), with hidden states s∈S, observations o∈O, actionsa∈A, initial and
transition state distributions pinit(s0)andptrans(st+1|st,at), observation distribution pprv(ot|st), and reward
functionr:O→R. For simplicity we assume episodes τ= (o0,a0,...,oT)of ﬁnite length|τ|=T > 0, and
we introduce the concept of a history at time t,ht= (o0,a0,...,ot). The control mechanism is represented as
a stochastic policy π, which together with the POMDP dynamics pinit,ptransandpprvdeﬁnes a probability
distribution over trajectories, p(τ). In this work we consider two types of control policies, which result in two
distinct data-generation regimes.
Deﬁnition 1 (Standard POMDP regime) .In thestandard POMDP regime , actions are decided based only
on the visible information from the past, Ht, according to a standard policy πstd(at|ht). This results in the
data-generation process depicted in ﬁgure 1, and trajectory distributions that decompose as
pstd(τ) =S|τ|+1/summationdisplay
s0→|τ|pinit(s0)pprv(o0|s0)|τ|−1/productdisplay
t=0πstd(at|ht)ptrans(st+1|st,at)pprv(ot+1|st+1).
This standard regime is that of the regular POMDP control problem, which formulates as:
π⋆
std= arg max
πstdE
τ∼pstd
|τ|/summationdisplay
t=0r(ot)
. (1)
Deﬁnition 2 (Privileged POMDP regime) .In theprivileged POMDP regime , actions can be decided based on
the hidden state Stas well, according to a privileged policy πstd(at|ht,st). This results in the data-generation
3Published in Transactions on Machine Learning Research (08/2023)
process depicted in ﬁgure 2, with trajectory distributions that decompose as
pprv(τ) =S|τ|+1/summationdisplay
s0→|τ|pinit(s0)pprv(o0|s0)|τ|−1/productdisplay
t=0πprv(at|ht,st)ptrans(st+1|st,at)pprv(ot+1|st+1).
This privileged regime allows us to consider situations where trajectories are collected by observing an external
agent who uses privileged information, in the extreme case the entire POMDP hidden state. Such a privileged
agent can be for example a human driver in the context of autonomous driving, who has access to privileged
information not accessible to the learning robot, such as the weather forecast. There lies the origin of the
confounding problem in oﬄine RL.
2.3 Causality and do-calculus
Several frameworks exist in the literature for reasoning about causality [ 28;14;7]. Here we follow the
framework of Judea Pearl, whose concept of ladder of causation is particularly relevant to answer RL
questions. The ﬁrst level of the ladder, association , relates to the passive observation of an external
agent acting in the environment, while the second level, intervention , relates to the question of what will
happen to the environment as a result of the observer’s own actions. The tool of do-calculus [ 30], presented
in appendix A, acts as a bridge between these two levels, and is typically used to answer whether and
interventional distribution, such as p(y|do(x),z), can be identiﬁed from an observational distribution, such as
p(x,y,z ). In a nutshell, in causal systems that can be expressed as DAGs, an intervention do(x)forces the
variables in Xto take the speciﬁc value X=xregardless of their causal ancestors in the graph, and queries
of formp(y|do(x),z)measure the eﬀect of an intervention do(X=x)on an outcome event Y=y, in the
context where another event Z=zis also observed. In this paper, we will use do-calculus to reason formally
about model-based RL in diﬀerent POMDP data-collection regimes, which entail diﬀerent causal graphs.
3 Model-based RL as causal inference
Decision-making problems are inherently causal [ 10;7]. In POMDPs, model-based RL relies on measuring the
causal eﬀect of immediate interventions, do(at), on the next observation, ot+1, given that past observations,
o0→t, and past interventions, do(a0→t−1), have already happened. Such causal queries are embodied in the
causal transition model p(ot+1|o0→t,do(a0→t))3, which depends only on the POMDP dynamics in M, and
not on the control policy π.
p(ot+1|o0→t,do(a0→t)) =pstd(ot+1|o0→t,do(a0→t)),∀πstd
=pprv(ot+1|o0→t,do(a0→t)),∀πprv.
Together with the initial distribution p(o0), this causal model allows for the evaluation of any standard control
policyπstd(at|ht). Model-based RL then decomposes the control problem equation (1) into two sub-problems:
1. learning: given a dataset D, estimate a model ˆq(ot+1|ht,at)≈p(ot+1|o0→t,do(a0→t));
2. planning: given a history htand the model ˆq, derive an optimal action at.
In this work we consider only the ﬁrst problem, that is, learning the causal transition model from data. Next,
we show using do-calculus that this problem can be either trivial or impossible, depending on whether the
data is collected using a standard or a privileged control policy.
Guiding example. Consider the door problem illustrated in ﬁgure 3. You are sitting in a room with a door,
a light that can be red or green, and two buttons that will open the door depending on the light color. You can
collect data samples in two ways, either from interventions, i.e., you get up and press the buttons (expensive),
or from observations, i.e., you watch someone else press the buttons (cheap). A key detail: you’re colorblind
3Such a notation can be found also in [26]
4Published in Transactions on Machine Learning Research (08/2023)
Light
Button Door
Causal DAG
Figure 3: The door problem.
and can’t distinguish red from green. Your goal is to ﬁnd which button is more likely to open the door. The
mechanism responsible for opening the door works as follows: when the light is red, button A opens the door,
when the light is green, button B opens the door. The light is red 60% of the time, and green the rest of the
time. You are told nothing about the door’s mechanism, except that it depends on both the light color and the
button pressed ( Light→Door←Button). Since you are colorblind you cannot use the light color to make
decisions, and the question you are interested in is simply, which button is more likely to open the door? In
thedo-calculus framework, this question translates to
arg max
button∈{A,B}p(door=open|do(button )).
You have to estimate two causal queries: p(door=open|do(button =A))andp(door=open|do(button =B)).
3.1 In the standard POMDP regime
In the standard POMDP regime, we assume access to a dataset Dstd∼pstd(τ)of episodes τcollected using
an arbitrary standard policy πstd(at|ht). A key characteristic in this setting is that At⊥ ⊥St|Htis always
true, that is, every action is independent of the current hidden state given the current history. By applying
do-calculus on the causal graph from ﬁgure 1, the causal model can be shown to be trivially identiﬁable as
p(ot+1|o0→t,do(a0→t)) =pstd(ot+1|ht,at). (2)
Because of this property, any trajectory τ∼pstd(τ)can be interpreted as an interventional trajectory, where
the learning agent itself could have decided on each of the action atinτ. Thus, in the remainder of the paper
we will interchangeably call the standard POMDP regime the interventional regime , and any dataset Dstd
collected in this regime an interventional dataset .
Assuming suﬃcient exploration, which is achieved if the control policy is strictly positive ( πstd(at|ht)>0,
∀at,ht), an estimator of the POMDP causal model can be obtained from Dstdvia log-likelihood maximization,
ˆq= arg max
q∈QDstd/summationdisplay
τ|τ|−1/summationdisplay
t=0logq(ot+1|ht,at). (3)
This corresponds to the simplest and most common form of model learning via supervised learning [ 24], which
eﬀectively solves our causal inference problem.
Guiding example. Consider again our door example. If you collect the result of your own (or another
colorblind person’s) interactions with the door, then you know that the light color can not cause which button
is pressed (Light/negationslash→Button). Then, you can directly estimate the causal eﬀect of the button on the door,
p(door=open|do(button )) =pstd(door=open|button ).
In this regime, regardless of which policy is used to collect (button,door)samples, eventually you realize that
button A has more chances of opening the door (60%) than button B (40%), and thus is the optimal action4.
4One assumption though is strict positivity, πstd(button )>0∀button, which ensures that both buttons are pressed.
5Published in Transactions on Machine Learning Research (08/2023)
3.2 In the privileged POMDP regime
In the privileged POMDP regime, we assume access to a dataset Dprv∼pprv(τ)of episodes τcollected using
an arbitrary privileged policy πprv(at|ht,st). In this setting, actions might not be independent of the current
hidden state given the current history, i.e., At⊥ ⊥St|Htmight not hold. Because each hidden state Sthas a
causal eﬀect on both the current action Atand the next observation Ot+1, it acts as a hidden confounder in
the POMDP causal transition model. This confounding eﬀect can not be adjusted for without observing
the hidden states of the POMDP, and applying do-calculus on the causal graph from ﬁgure 2 results in the
causal model p(ot+1|o0→t,do(a0→t))being non-identiﬁable from pprv(τ). In particular,
p(ot+1|o0→t,do(a0→t))/negationslash=pprv(ot+1|ht,at).
Because of this, trajectories τ∼pprv(τ)cannot be interpreted as interventional. To better relate to the
causality literature, we will interchangeably call the privileged POMDP regime the observational regime , and
any datasetDprvcollected in this regime an observational dataset .
Note that, as a consequence of this non-identiﬁability, naively applying any oﬀ-the-shelf oﬄine RL algorithm
[20;21] on an observational dataset such as Dprvis a risky endeavour, and might result in biased transition
models and value functions, and sub-optimal policies.
Guiding example. Take again the door example in ﬁgure 3, and assume that you observe someone else
interacting with the door. You do not know whether that person is colorblind or not ( Light→Buttonis possible).
In this regime, without additional knowledge, you cannot recover the causal queries p(door=open|do(button ))
from the observed distribution p(door,button ). In thedo-calculus framework, the queries are said non
identiﬁable . However, if that person was to tell you the light color they see before they press A or B, then you
could recover those queries via deconfounding ,
p(door=open|do(button )) =/summationdisplay
light∈{red,green}pprv(light)pprv(door=open|light,button ).
This formula eventually yields the correct causal transition probabilities regardless of the observed policy, given
that enough (light,button,door)samples are collected5.
3.3 Connection to online and oﬄine RL
To relate the concepts of standard (interventional) and privileged (observational) POMDP data to online and
oﬄine RL, the key question to ask is, when the samples were collected, could the control policy have used
privileged information besides the history ht? Or, more formally, can we guarantee that At⊥ ⊥St|Htdid
hold in the data-generating process?
In online RL , the learning agent explicitly controls the data-collection policy, so by design it can not rely
on privileged information, hence At⊥ ⊥St|Htalways holds. Therefore, data collected in an online RL setting
can be safely treated as interventional, and the causal transition model can be directly estimated using
equation (3).
In oﬄine RL , the learning agent might have limited knowledge about the data-collection policy, sometimes
no knowledge at all. In some settings, it can be shown that the oﬄine policy could not have used any
privileged information, and oﬄine data can be treated as interventional. For example, with human replays
from Atari video games, it is hard to imagine a human player having access to more information from the
machine’s internal state than the regular video and audio outputs from the game. But in more general oﬄine
RL settings, access to privileged information can not be dismissed. This is particularly true with human
demonstrations collected in the wild, such as in the context of autonomous driving, medical recommender
systems (examples in Section 1), or question answering systems [ 26]. In that case, the oﬄine trajectories can
not be considered interventional, and the oﬄine dataset must be treated as observational.
5The strict positivity condition here is πprv(button |light )>0∀button,light.
6Published in Transactions on Machine Learning Research (08/2023)
S0pinit
Stptrans
St+1ptrans
O0pobs
Otpobs
Ot+1pobs
At
πAt−1
π
Iπ(at|ht,st,i= 0) :=πprv(at|ht,st)
π(at|ht,st,i= 1) :=πstd(at|ht)
Figure 4: Augmented POMDP setting, with a policy regime indicator Itaking values in{0,1}(1=inter-
ventional regime, no confounding, 0=observational regime, potential confounding), enforcing the contextual
independence At⊥ ⊥St|Ht,I= 1.
4 Combining observational and interventional data
Given enough online data, RL agents can learn optimal policies. But in some situations collecting a large
online (interventional) dataset can be expensive (recording a robot driver in the wild), while collecting a
large oﬄine (observational) dataset from demonstrations is relatively cheap (recording human drivers in the
wild). Is it possible then to leverage such oﬄine data to improve the sample-eﬃciency of an online RL agent,
even in the presence of confounding?6
4.1 Problem statement
We consider two datasets of POMDP trajectories, DstdandDprv, sampled respectively in the standard
(interventional) and the privileged (observational) POMDP regime. We then ask the following question: can
the observational dataset Dprvbe used in combination to the interventional dataset Dstd, to improve the
POMDP causal transition model p(ot+1|o0→t,do(a0→t))that would be obtained from equation (3) using
Dstdonly? As we will see, answering this question will require to go beyond the identiﬁability framework of
do-calculus.
4.2 The augmented POMDP
Since both datasets DstdandDprvare sampled from the same POMDP ( pinit,ptrans,pprv) controlled in two
diﬀerent ways, we introduce a regime indicator variable [ 7]I∈{0,1}that controls an augmented control
policyπ. This results in the augmented data-generating process depicted in ﬁgure 4, such that
Dprv∼p(τ|i= 0) :=pprv(τ), and
Dstd∼p(τ|i= 1) :=pstd(τ).
Note that the augmented control policy induces the contextual conditional independence At⊥ ⊥St|Ht,I= 1,
which is not implied by the DAG factorization. As a direct consequence of equation (2), in this augmented
POMDP the causal POMDP transition model can be extracted as
p(ot+1|o0→t,do(a0→t)) =pstd(ot+1|ht,at) =p(ot+1|ht,at,i= 1). (4)
4.3 The augmented learning problem
Inordertolearnthecausaltransitionmodel p(ot+1|o0→t,do(a0→t)weproposethefollowingtwo-stepprocedure,
which relies on ﬁtting a latent probabilistic model ˆqthat explains both DstdandDprv. Our latent model is
constrained to respect the structure of our augmented POMDP, with a latent variable zt∈Zthat substitutes
the true hidden state st∈S.
6Note that we consider this question in its broadest, without further assumptions about the observed oﬄine agent. The
oﬄine agent might act sub-optimally, or optimally according to a diﬀerent reward function than the learning agent.
7Published in Transactions on Machine Learning Research (08/2023)
Learning. Our learning problem formulates as standard likelihood maximization7,
ˆq= arg max
q∈QDprv/summationdisplay
(τ)logq(τ|i= 0) +Dstd/summationdisplay
(τ)logq(τ|i= 1), (5)
withQthe family of latent probabilistic models that respect the augmented POMDP structure,
q(τ|i= 0) =Z|τ|+1/summationdisplay
z0→|τ|qinit(z0)qobs(o0|z0)|τ|−1/productdisplay
t=0qprv(at|ht,zt)qtrans(zt+1|at,zt)qobs(ot+1|zt+1), and
q(τ|i= 1) =Z|τ|+1/summationdisplay
z0→|τ|qinit(z0)qobs(o0|z0)|τ|−1/productdisplay
t=0qstd(at|ht)qtrans(zt+1|at,zt)qobs(ot+1|zt+1).
Note that the recovered model ˆqconveniently decomposes into a series of simpler components: the initial latent
model ˆq(z0), the observation model ˆq(ot|zt), the latent transition model ˆq(zt+1|zt,at), and the behaviour model
ˆq(at|zt,ht,i). In practice, the behaviour model in the interventional regime ˆq(at|zt,ht,i= 1) = ˆq(at|ht,i= 1)
can be safely ignored during learning, since it does not impact the recovered latent variable nor the causal
transition model. Also, the agent behavior model in the observational regime ˆq(at|zt,ht,i= 0)can be
substituted for a simpler model ˆq(at|zt,i= 0), which further simpliﬁes the model architecture to be used
when solving (5). This leaves only four learnable components: ˆq(z0),ˆq(ot|zt),ˆq(zt+1|zt,at)andˆq(at|zt,i= 0),
each of which can be approximated using any black-box model, such as a feed-forward neural network.
Inference. We recover the causal transition model ˆq(ot+1|o0→t,do(a0→t)) = ˆq(ot+1|ht,at,i= 1)by applying
do-calculus on the augmented DAG from ﬁgure 4, with ztinstead ofst. The procedure conveniently unrolls
as a forward algorithm at test time, and relies on the recurrent computation of ˆq(zt|ht,i= 1), a.k.a. the
agent’s belief state at time t[5; 35]. First, the initial belief state at t= 0is recovered as
ˆq(z0|h0,i= 1) =ˆqinit(z0)ˆqobs(o0|z0)/summationtextZ
z0ˆqinit(z0)ˆqobs(o0|z0).
Then, for every 0≤t<T, the causal transition model is recovered as
ˆq(zt+1,ot+1|ht,at,i= 1) =/summationtextZ
ztˆq(zt|ht,i= 1)ˆqtrans(zt+1|zt,at)ˆqobs(ot+1|zt+1),
ˆq(ot+1|ht,at,i= 1) =/summationtextZ
zt+1ˆq(zt+1,ot+1|ht,at,i= 1),
and the next belief state is updated to
ˆq(zt+1|ht+1,i= 1) =ˆq(zt+1,ot+1|ht,at,i= 1)/summationtextZ
zt+1ˆq(zt+1,ot+1|ht,at,i= 1).
Since the observational distribution pprvdoes not appear in the expression of p(ot+1|o0→t,do(a0→t))in equa-
tion (4), how does the observational dataset Dprvinﬂuence the causal transition model ˆq(ot+1|o0→t,do(a0→t))?
The intuition is as follows. The learned model ˆqmust ﬁt both observational and interventional data by
sharing the same latent variables Zt, and the same building blocs ˆqinit(z0),ˆqobs(ot|zt)andˆqtrans(zt+1|zt,at).
The privileged behaviour model ˆqprv(at|zt)is the only component that can allow for discrepancies between
the two regimes, and it oﬀers a limited ﬂexibility. As a result, the observational distribution ˆq(τ|i= 0)
estimated fromDprvacts as a regularizer for the interventional distribution ˆq(τ|i= 1)estimated fromDstd.
This regularization helps prevent overﬁtting when learning from limited interventional data, and improves
the generalization performance of the estimated causal transition model. As a side comment, note that our
method does not rely on the identiﬁability of the latent transition model ptrans(st+1|st,at), which remains in
general non-identiﬁable from observational data, interventional data, or any of their combinations.
7Note that, while the problem of learning structured latent variable models is known to be hard in general, there also exists a
wide range of tools and algorithms available in the literature for solving it approximately, such as the EM algorithm or the
method of ELBO maximization.
8Published in Transactions on Machine Learning Research (08/2023)
4.4 Theoretical analysis
In this section we analyse the two-step approach described in the previous section, and we demonstrate that
it is 1) correct, in the sense that it yields a consistent estimator of the standard POMDP causal transition
model and 2) eﬃcient, in the sense that it yields a better estimator than the one based on interventional data
only (asymptotically in the number of observational data).
First, let us demonstrate how our approach is correct. An important assumption here is that the latent space
of the model is suﬃciently large ( |Z|≥|S| ), which ensures enough expressivity to learn the true augmented
POMDP distribution, i.e., p∈Q. Under this condition, with enough data ˆqconverges to p, and in particular
ˆq(τ|i= 1)→p(τ|i= 1)whenDstd→∞. Then, the standard POMDP causal model ˆq(ot+1|ht,at,i= 1)being
a marginal distribution of ˆq(τ|i= 1), it also converges to p(ot+1|ht,at,i= 1). Hence, solving equation (5)
with a suﬃciently large interventional dataset Dstdand a suﬃciently large latent space Zconverges to the
true standard POMDP transition model.
Second, let us demonstrate intuitively how our approach is eﬃcient asymptotically. Our key assumption is
that we have a big enough observational dataset, |Dprv|→∞, which makes it act as a strong regularizer
in equation (5). Our key result is theorem 1, which generalizes a famous result in econometrics known as
Manski’s bounds [23], from the contextual bandit setting ( T= 1) to the POMDP setting ( T > 1).
Theorem 1. Assuming|Dprv|→∞, for anyDstdthe recovered causal model is bounded as follows:
T−1/productdisplay
t=0ˆq(ot+1|o0→t,do(a0→t))≥T−1/productdisplay
t=0p(at|ht,i= 0)p(ot+1|ht,at,i= 0), and
T−1/productdisplay
t=0ˆq(ot+1|o0→t,do(a0→t))≤T−1/productdisplay
t=0p(at|ht,i= 0)p(ot+1|ht,at,i= 0) + 1−T−1/productdisplay
t=0p(at|ht,i= 0),
∀hT−1,aT−1,T≥1wherep(hT−1,aT−1,i= 0)>0.
Proof.See appendix D.
Let us denote the family of candidate causal models when solving equation (5) as H0={q(ot+1|o0→t,a0→t,i=
1)|q∈Q}when|Dprv|= 0, andH∞={q(ot+1|o0→t,a0→t,i= 1)|q∈Q∧q(τ|i= 0) =p(τ|i= 0)}when
|Dprv|→∞. Because there exists at least one episode τ= (o0,ao,...,oT)withp(τ|i= 0)>0, theorem 1
implies the non-trivial lower bound q(ot+1|o0→t,a0→t,i= 1)>0for every 0≤t≤T−1, at least for the
speciﬁc values in τ. Therefore, candidate models qsuch thatq(ot+1|o0→t,a0→t,i= 1) = 0 are allowed in
H0but forbidden in H∞, and henceH∞⊂H 0. Because this new hypothesis space is a strict subset of the
original one, it oﬀers better generalization bounds for a ﬁxed |Dprv|, or equivalently a better sample-eﬃciency
with respect to|Dprv|.
Guiding example. Let us now examine our door example in light of Theorem 1. Assume this time that
you observe many (button,door)interactions from a non-colorblind person (privileged, i= 0), who’s policy is
π(button=A|light=red ) = 0.9andπ(button=A|light=green ) = 0.4. Then you can already infer from Theorem 1
thatp(door=open|do(button=A ))∈[0.54,0.84]andp(door=open|do(button=B ))∈[0.24,0.94]. You now get a
chance to interact with the door ( i= 1), and you decide to press A10 times and B10 times. You are unlucky,
and your interventional study results in the following probabilities: q(door=open|do(button=A )) = 0.5and
q(door=open|do(button=B )) = 0.5. This does not coincide with your (reliable) observational study, and
therefore you adjust q(door=open|do(button=A ))to its lower bound 0.54. You now believe that pressing Ais
more likely to be your optimal strategy.
9Published in Transactions on Machine Learning Research (08/2023)
4.5 Limitations of the provided analysis
We would like to acknowledge two limitations of the theoretical results we provide in the previous section.
First, it is fairly easy to see that the upper bound in theorem 1 is not tight. For example,
T−1/productdisplay
t=0ˆq(ot+1|o0→t,do(a0→t))≤T−2/productdisplay
t=0ˆq(ot+1|o0→t,do(a0→t))∀T≥2,
is always true, and therefore
T−1/productdisplay
t=0ˆq(ot+1|o0→t,do(a0→t))≤ min
K∈{0,...,T−1}K/productdisplay
t=0p(at|ht,i= 0)p(ot+1|ht,at,i= 0) + 1−K/productdisplay
t=0p(at|ht,i= 0)
which is a tighter bound and also a generalization of Manski’s bounds [ 23]. Still, it is likely that this upper
bound is not tight either. The purpose of theorem 1 is merely to serve as a building block in the argument
“observational data creates bounds (in the asymptotic regime) which restrict the hypothesis space for learning”.
Providing tighter bounds for augmented POMDPs would give valuable insight, and is left for future work.
Second, the results in section 4.4 are restricted to the asymptotic regime |Dprv|→∞, and do not provide any
practical guarantee in the ﬁnite-sample regime. Our intuition is that these hard bounds would translate into
some kind of soft prior over the hypothesis space, which would also improve generalization. Proving this idea
formerly and deriving proper generalization bounds in the ﬁnite-sample regime is left for future work.
5 Experiments
We run experiments on the three synthetic toy problems described in ﬁgure 6, each expressing a diﬀerent
level of complexity and a diﬀerent form of privileged information. In order to answer the question raised in
section 4.1, we compare our augmented method against two baselines: no obswhich discards the observational
dataset, and naivewhich naively combines observational and interventional data as if there was no confounding.
We expect two things: 1) in the presence of privileged information (confounding), the observational dataset
decreases the performance of the naiveagent, compared to the no obsagent; and 2) our augmented agent
beneﬁts from the observational dataset, and outperforms both the naiveand theno obsagent. The code to
reproduce these experiments is available online8.
5.1 Experimental setup
RL procedure. Ouraugmented model-based RL procedure is depicted in algorithm 1. We start from
a pre-existing dataset of privileged POMDP trajectories (observational data), Dprv, an empty dataset of
standard POMDP trajectories (interventional data), Dstd, and a random exploration policy, ˆπstd. By ﬁxed
increments (e.g., 0,10,50,..., 1000), the learning agent collects new interventional trajectories by exploring
the environment, which complement the interventional dataset Dstd. After each increment a new latent-based
model ˆqis obtained by solving equation (5), and a new (near)-optimal policy ˆπstdis derived from the model
ˆqusing an actor-critic algorithm. The newly obtained policy ˆπstdis then used to collect the next increment
of interventional trajectories, supplemented with an /epsilon1-random noise for exploration.
Model and agent training. We train all three model-based methods, augmented ,no obsandnaive, using
the same model architecture and training procedure. Each building bloc ˆqinit,ˆqtrans,ˆqobsandˆqprvconsists in
a tabular logistic model, and equation (5) is solved via mini-batch stochastic gradient descent using Adam [ 17].
Once the POMDP dynamics are recovered we extract ˆq(o0)andˆq(ot+1|o0→t,do(a0→t))to train a "dreamer"
agent [11] via actor-critic, implemented as a feed-forward neural network that takes as input the recovered
POMDP belief state, ˆq(zt|o0→t,do(a0→t−1)).
Evaluation. We evaluate on the real test environment 1) the quality of the causal transition model
ˆq(ot+1|o0→t,do(a0→t))in terms of its likelihood on new interventional data (collected via random exploration),
8Code for the experiments: https://github.com/gasse/causal-rl-tmlr
10Published in Transactions on Machine Learning Research (08/2023)
Algorithm 1 Augmented model-based RL pseudocode
Require: observational dataset Dprv(potentially privileged), training method ( augmented ,no_obsornaive),
training steps N(e.g.,N= (0,10,50,..., 1000)), exploration noise /epsilon1
Ensure: estimated POMDP dynamics ˆqinit(z0),ˆqobs(ot|zt),ˆqtrans(zt+1|zt,at), and optimal policy ˆπstd
InitializeDstd←∅,ˆπstd←random exploration policy
iftraining method is no_obsthendiscard observations ( Dprv←∅)
iftraining method is naivethenconsider observations as interventions ( Dstd←D prvandDprv←∅)
nprev←0
for alln∈Ndo
Collectn−nprevnew interventional samples using ˆπstd+/epsilon1-random noise, and add them to Dstd
Obtain a new model ˆqusingDstdandDprv(equation (5), supervised learning)
Obtain a new policy ˆπstdusing the model ˆq(actor-critic in “dream” environment)
nprev←n
random noisy good perfect good perfect badmodel
likelihoodagent
performance
0 500 1000
|std|
0.0000.0250.0500.0750.1000.125Likelihood
no obs
naive
augmented
0 500 1000
|std|
0.0000.0250.0500.0750.1000.125Likelihood
0 500 1000
|std|
0.0000.0250.0500.0750.1000.125Likelihood
0 500 1000
|std|
0.0000.0250.0500.0750.1000.125Likelihood
0 500 1000
|std|
40
20
0Cumulated reward
0 500 1000
|std|
40
20
0Cumulated reward
0 500 1000
|std|
40
20
0Cumulated reward
0 500 1000
|std|
40
20
0Cumulated reward
Figure 5: Robustness to diﬀerent degrees of confounding on the tigerproblem. Top: model likelihood on
new data (higher is better). Bottom : agent performance in terms of cumulated reward (higher is better).
Columns : the diﬀerent privileged agents used to collect the observational dataset Dobs. Therandomagent
amounts to no confounding, while the noisy good ,perfect good andperfect bad agents introduce diﬀerent
degrees of confounding. We report the mean ±standard deviation over 10 random seeds. Our augmented
method performs best regardless of the degree of confounding, both in terms of model likelihood and agent
performance, and is on par with the naivemethod when there is no confounding (random).
and 2) the performance of the resulting policy ˆπstdin terms of its cumulated reward. We evaluate each model
and agent over 10K new trajectories, and we repeat each experiment 10 times with diﬀerent random seeds to
account for variability. We defer the reader to appendix B for the complete experimental details.
5.2 Robustness to diﬀerent degrees of confounding in the tiger problem
Tigeris a classic small-scale POMDP from Cassandra et al. [4]with|S|= 6hidden states and time
horizonT= 10. To measure the robustness of our method to diﬀerent degrees of confounding, we
consider diﬀerent privileged policies, described in detail in appendix B. For each experiment we col-
lect|Dprv|= 1500 (confounded) observational trajectories, and we train and evaluate at increments
|Dstd|∈(0,50,100,150,200,300,400,500,600,700,800,900,1000).
Results are reported in ﬁgure 5. The ﬁrst policy, random, amounts to a degree 0 of confounding, because
the privileged information is eﬀectively not used to generate Dprv. In this setting our method performs on
par with the naivemethod, which also leverages the observational data without bias due to the absence of
11Published in Transactions on Machine Learning Research (08/2023)
confounding. The noisy good policy mostly opens the correct door with the treasure, but sometimes also
decides to listen or to open the wrong door at random. This degree of confounding, although rather mild
in terms of the bias in the non-causal transition probabilities pprv(ot+1|ht,at), is particularly hurtful to the
naivemethod. The perfect good policy always opens the correct door, which induces non-causal transition
probabilities that are completely oﬀ the causal ones, and also hurts the performance of the naivemethod as
the learned model will tend to be over-optimistic. The perfect bad policy always opens the wrong door, which
induces transition probabilities that are completely oﬀ in the other direction, but this time the eﬀect on the
performance of the naivemethod is not as bad. Indeed, the learned naivemodel this time will tend to be
over-pessimistic, which is not such a bad prior in the case of the tigerproblem. Our augmented method,
always performs better than (or on par with) both no obsandnaive, eﬀectively leveraging the observational
dataset|Dprv|even in the presence of confounding.
5.3 Performance on the gridworld problems
Hidden treasures is a 3x3 grid-world problem inspired from Sutton et al. [37], with|S|= 36hidden states
and a time horizon T= 10.Sloppy dark room is a 5x5 grid-world inspired from Alt et al. [1], with|S|= 21
hidden states and a time horizon T= 30. In both problems, the privileged agent has complete information
at each time step, and uses a shortest path algorithm to decide on its next action. For each experiment we
collect|Dprv|= 8000(confounded) observational trajectories, and we train and evaluate each method at
increments|Dstd|∈(25,50,75,100,150,200,300,400,600,800,1000,1500,2000,3000,4000,5000,6000).
Results are reported in ﬁgure 7, where a similar trend can be observed in the two experiments. Initially,
when few interventional samples have been collected, the observational data seems to beneﬁt the naive
method, despite the presence of confounding, and it exhibits gains both in terms of model likelihood and
agent performance compared to the no obsmethod. But as more samples are collected, the untreated
confounding eﬀect in the observational data starts becoming hurtful to the naivemethod, and eventually
better performances are obtained by disregarding the observational data, using the no obsmethod. In both
cases our augmented method makes the best use of the available data, and exhibits a better convergence rate
than both naiveandno obs, in terms of model quality and agent performance, despite the confounding eﬀect
present in the observational data. In ﬁgure 7, right side, we report the density of tiles visited by the agent of
each method, at a chosen time step where our augmented method has almost converged, while the two other
methods haven’t. In hidden treasures we see that our method has learned to cycle through all 4 corners,
which is the optimal strategy, while the two other methods still struggle and focus only on a single side or a
single corner of the grid. In sloppy dark room , our method succeeds in quickly escaping the upper section
and reaching the target, while the two other methods struggle more in the upper section, and reach the target
less often. It is interesting to observe a very similar outcome in both experiments, while the type of privileged
information, hidden to the learning agent, is quite diﬀerent. In hidden treasures the learning agent knows
its position at all times, but is missing information about where the target is located, thus it needs to explore
the space at test time, while the privileged agent just has to go straight to the target. In sloppy dark room ,
the target is ﬁxed hence no exploration of the space is needed at test time, but the agent’s location is hidden
most of the time and the agent has to learn how to best navigate while being half-blind.
6 Related work
Causal RL. A whole body of work exists around the question of merging interventional and observational
data in RL in the presence of confounding. Bareinboim et al. [2]study a sequential decision problem similar
to ours, but assume that expert intentions are observed both in the interventional and the observational
regimes, i.e., prior to doing interventions the learning agent can ask “what would the expert do in my
situation?” This artiﬁcially introduces an intermediate, observed variable ˆat=f(ot)with the property that
pprv(at=ˆat|ˆat) = 1, which eﬀectively removes any confounding ( At⊥ ⊥St|Ht). Zhang and Bareinboim
[40;43]relax this assumption in the context of binary bandits, and later on in the more general context of
dynamic treatment regimes [ 41;42]. They derive causal bounds similar to ours (Theorem 1), and propose a
two-step approach which ﬁrst extracts causal bounds from observational data, and then uses these bounds
in an online RL algorithm. While their method nicely tackles the question of leveraging observational data
12Published in Transactions on Machine Learning Research (08/2023)
tiger hidden treasures sloppy dark room
Figure 6: Our three synthetic toy problems. In tiger, the learning agent receives a noisy signal of the tiger’s
position (roar left or roar right). It can wait and listen to a new roar at the cost of -1 reward, or decide to
move left or right. Treasure gives +10 reward, and tiger gives -100 reward. The privileged agent knows the
exact location of the tiger. The game stops when treasure or tiger is found, or after a maximum horizon of
T= 10. Inhidden treasures the agent must collect a treasure (+1 reward), which is randomly located in
one of the four corners. The privileged agent knows the treasure’s position at all times, the learning agent
doesn’t. The treasure is reset to a new location when found, and the game stops after a ﬁxed horizon of
T= 10. Insloppy dark room the agent must reach a treasure (+1 reward) located behind a wall, and slips
to a random adjacent tile instead of moving to the chosen direction 50% of the time. The privileged agent
knows its position at all times, while the learning agent is only revealed its position with 20% chances at each
time step, and is blind otherwise (a dummy position is revealed). The time horizon is ﬁxed to T= 30.
model
likelihoodagent
performancedensity of agent trajectories
(at speciﬁc time step ⋆)
no obs naive augmented
no obs naive augmentedhidden treasures sloppy dark room
|Dstd|= 1000 |Dstd|= 2000
0 2000 4000 6000
|std|
0.000.010.020.03Likelihood
no obs
naive
augmented
0 2000 4000 6000
|std|
0.51.01.52.02.5Cumulated reward⋆ ⋆
0 2000 4000 6000
|std|
0.00.51.01.51e11
 Likelihood
1e-11
0 2000 4000 6000
|std|
01234Cumulated reward ⋆ ⋆
Figure 7: Performance on the hidden treasures (top row ) and sloppy dark room (bottom row ) grid-
world problems. First column : model likelihood on new data (higher is better). Second column : agent
performance in terms of cumulated reward (higher is better). Third column : density of the agent trajectories
obtained by each method at a speciﬁc time step ⋆. In both setups, our augmented method outperforms both
the unbiased no obsmethod and the biased naivemethod, and displays a better sample-eﬃciency in terms of
interventional (online) data due to its correct use of the observational (oﬄine, confounded) data. While the
naivemethod can sometimes provide some initial gains compared to the no obsmethod, at some point the
confounding eﬀect in the observational dataset (collected from a privileged agent) starts aﬀecting the agent’s
performance negatively, and one is better oﬀ not using the confounded data at all ( no obsmethod).
13Published in Transactions on Machine Learning Research (08/2023)
for online exploration, it does not account for uncertainty in the bounds estimated from the observational
data. In comparison, our latent-based approach is more ﬂexible, as it never computes explicit bounds, but
rather lets the learning agent balance through equation (5) how data from both regimes will inﬂuence the
ﬁnal transition model, depending of the number of samples available. Kallus et al. [16]also propose a
two-step procedure to combine observational and interventional data, which however requires a series of
strong parametric assumptions (strong one-way overlap, linearity, non-singularity etc.), and only works in the
context of binary contextual bandits.
Causal confusion. In the context of imitation learning, the problem of causal misidentiﬁcation , that is,
ascribing the actions of the expert to the wrong explanatory variables, is attributed to confounding by
de Haan et al. [8]. Spencer et al. [34]argue instead that it is a manifestation of covariate shift , which appears
more plausible to us. Indeed, it can be shown that in the experiments of de Haan et al. [8]the experts don’t
use privileged information, which theoretically circumvents any confounding.
Exploiting oﬄine RL data. Combining online and oﬄine data in RL also raises additional challenges, such
as the value function initialisation problem [ 9] or the bootstrapping error accumulation problem [ 19;25]. While
these challenges could be combined with and ampliﬁed by confounding, they originate from fundamentally
diﬀerent issues and require orthogonal treatments. Oﬀ-policy evaluation, which is about estimating the
performance of a policy πusing observational data only, can be seen as a speciﬁc instantiation of the framework
we present in this paper. It corresponds to the particular setting |Dint|= 0, where it can be shown that the
causal transition model is in general not recoverable in the presence of confounding. Still, a growing body of
literature studies the question of learning purely from oﬄine data in the presence of confounding, under the
assumption that the data-generating process respects speciﬁc structural or parametric constraints [ 22;38;3].
Large sequence models. A recent trend in RL is to apply large sequence models to estimate the
environment’s dynamics in a model-based fashion [ 32;15], or to parameterize a goal-conditioned policy in a
model-free fashion [ 6;44]. While large sequence models appear a promising tool for eﬃciently combining
oﬄine and online datasets, they remain vulnerable to confounding, as pinpointed by Ortega et al. [26].
Because our proposed method follows a generic model-based approach, in theory it could be easily combined
with a large sequence model to address large-scale RL scenarios, while remaining robust to confounding.
7 Discussions
In this paper we have presented a simple, generic method for combining interventional and observational
(potentially confounded) POMDP trajectories in model-based reinforcement learning. We have demonstrated
theoretically that our method is correct and eﬃcient in the asymptotic case (inﬁnite observational data), and
we have illustrated its eﬀectiveness empirically on three synthetic toy problems. We have also highlighted the
dangers of naively using oﬄine data collected under privileged information in RL, which can eﬀectively hurt
the performance of an online RL agent, and reduce its sample-eﬃciency. The main limitation of our method
is that it adds an additional challenge on top of model-based RL, that of learning a latent-based POMDP
transition model, which can become problematic in high-dimensional RL settings. Still, the recent success of
discrete latent-based models for solving complex RL tasks [ 11;39;12] or tasks in high-dimensional domains
[31] lets us envision that this diﬃculty can be overcome in practice. A ﬁrst extension of this work would be to
develop a similar approach using latent-free transition models, which would remove the challenge of learning
a latent variable model. This seems doable at least in the case T= 1. A second potential extension to this
work could be to consider a setting where several privileged agents are observed, each with its own distinct
policy, leading to multiple observational datasets. This would lead, in the asymptotic case, to a stronger
regularizer for the causal POMDP transition model, as the implicit bounds implied by theorem 1 would
combine into tighter ones. A third, obvious extension would be to develop a similar approach within the
framework of model-free RL, which could take the form of an explicit or implicit value-function regularizer.
A fourth direction to investigate is that of scaling, by extending our method to RL tasks with continuous
observation spaces (e.g., pixel-based) and continuous latent spaces, so that it can be applied to a broader
range of problems. Finally, we hope that this work will help bridge the gap between the ﬁelds of RL and
causality, and will convince the RL community that causality is an adequate tool to reason about oﬄine,
observational data, which is plentiful in the world.
14Published in Transactions on Machine Learning Research (08/2023)
8 Acknowledgements
We thank the anonymous TMLR reviewers for their comments which helped us improve the quality of this
work. We thank David Berger for early discussions and for pointing us to relevant bodies of work. This work
was supported by the Canada First Research Excellence Fund (CFREF), Canada Excellence Research Chairs
(CERC), and the DEpendable Explainable Learning (DEEL) transatlantic research program.
Broader Impact Statement
Confounding is a prevalent issue in human-generated data, and can be an important source of bias in the
design of decision policies, if not dealt with properly. This paper makes a humble step towards more robustness
and fairness in AI-based decision systems, by combining causality and statistical learning to address the
confounding problem in reinforcement learning from oﬄine data. As such this work has potentially important
societal implications, in particular in critical systems where lives are at stake, such as medicine or self-driving
cars, and where human-generated data is prevalent.
References
[1]Bastian Alt, Matthias Schultheis, and Heinz Koeppl. Pomdps in continuous time and discrete spaces.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural
Information Processing Systems , volume 33, pages 13151–13162. Curran Associates, Inc., 2020.
[2]Elias Bareinboim, Andrew Forney, and Judea Pearl. Bandits with unobserved confounders: A causal
approach. In NIPS, 2015.
[3]Andrew Bennett, Nathan Kallus, Lihong Li, and Ali Mousavi. Oﬀ-policy evaluation in inﬁnite-horizon
reinforcement learning with latent confounders. In AISTATS , 2021.
[4]Anthony R. Cassandra, Leslie P Kaelbling, and Michael L. Littman. Acting optimally in partially
observable stochastic domains. In AAAI, 1994.
[5]Anthony Rocco Cassandra. Exact and approximate algorithms for partially observable Markov decision
processes . PhD thesis, Brown University, 1998.
[6]Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via
sequence modeling. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 15084–
15097. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/
7f489f642a0ddb10272b5c31057f0663-Paper.pdf .
[7]Philip Dawid. Decision-theoretic foundations for statistical causality. Journal of Causal Inference , 9(1):
39–77, 2021. doi: doi:10.1515/jci-2020-0008.
[8]Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. In NeurIPS,
2019.
[9] Sylvain Gelly and David Silver. Combining online and oﬄine knowledge in uct. In ICML, 2007.
[10]Samuel J Gershman. Reinforcement learning and causal models. In Michael R. Waldmann, editor, The
Oxford handbook of causal reasoning , chapter 10, pages 295–306. Oxford University Press, 2017.
[11]Danijar Hafner, Timothy P. Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete
world models. In ICLR, 2021.
[12]Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through
world models, 2023.
15Published in Transactions on Machine Learning Research (08/2023)
[13] Yimin Huang and Marco Valtorta. Pearl’s calculus of intervention is complete. In UAI, 2006.
[14]Guido W. Imbens and Donald B. Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences:
An Introduction . Cambridge University Press, 2015.
[15]Michael Janner, Qiyang Li, and Sergey Levine. Oﬄine reinforcement learning as one big sequence
modeling problem. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan,
editors,Advances in Neural Information Processing Systems , volume 34, pages 1273–1286. Curran
Associates, Inc., 2021.
[16]Nathan Kallus, Aahlad Manas Puli, and Uri Shalit. Removing hidden confounding by experimental
grounding. In NeurIPS , 2018.
[17] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
[18]Daphne Koller and Nir Friedman. Probabilistic Graphical Models - Principles and Techniques. MIT
Press, 2009.
[19]Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing oﬀ-policy q-learning via
bootstrapping error reduction. In NeurIPS , 2019.
[20]Sascha Lange, Thomas Gabel, and Martin A. Riedmiller. Batch reinforcement learning. In Reinforcement
Learning , volume 12 of Adaptation, Learning, and Optimization , pages 45–73. Springer, 2012.
[21]Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Oﬄine reinforcement learning: Tutorial,
review, and perspectives on open problems, 2020.
[22]Chaochao Lu, Bernhard Schölkopf, and José Miguel Hernández-Lobato. Deconfounding reinforcement
learning in observational settings, 2018.
[23]Charles F Manski. Nonparametric bounds on treatment eﬀects. The American Economic Review , 80(2):
319–323, 1990.
[24]Thomas M. Moerland, Joost Broekens, and Catholijn M. Jonker. Model-based reinforcement learning: A
survey, 2020.
[25]AshvinNair, AbhishekGupta, MurtazaDalal, andSergeyLevine. Awac: Acceleratingonlinereinforcement
learning with oﬄine datasets, 2021.
[26]Pedro A. Ortega, Markus Kunesch, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Joel Veness,
Jonas Buchli, Jonas Degrave, Bilal Piot, Julien Perolat, Tom Everitt, Corentin Tallec, Emilio Parisotto,
Tom Erez, Yutian Chen, Scott Reed, Marcus Hutter, Nando de Freitas, and Shane Legg. Shaking the
foundations: delusions in sequence models for interaction and control, 2021.
[27]Judea Pearl. Probabilistic reasoning in intelligent systems - networks of plausible inference. Morgan
Kaufmann, 1989.
[28] Judea Pearl. Causality . Cambridge University Press, Cambridge, UK, 2 edition, 2009.
[29] Judea Pearl. Causal inference in statistics: An overview. Statistics Surveys , 3:96 – 146, 2009.
[30] Judea Pearl. The do-calculus revisited. In UAI, 2012.
[31]Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang, editors,
Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of
Machine Learning Research , pages 8821–8831. PMLR, 18–24 Jul 2021.
[32]Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou,
and David Silver. Online and oﬄine reinforcement learning by planning with a learned model. In
M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in
Neural Information Processing Systems , volume 34, pages 27580–27591. Curran Associates, Inc., 2021.
16Published in Transactions on Machine Learning Research (08/2023)
[33]Ilya Shpitser and Judea Pearl. Identiﬁcation of joint interventional distributions in recursive semi-
markovian causal models. In AAAI, 2006.
[34]Jonathan Spencer, Sanjiban Choudhury, Arun Venkatraman, Brian Ziebart, and J. Andrew Bagnell.
Feedback in imitation learning: The three regimes of covariate shift, 2021.
[35]CharlotteStriebel. Suﬃcientstatisticsintheoptimumcontrolofstochasticsystems. Journal of Mathemati-
cal Analysis and Applications , 12(3):576–592, 1965. ISSN 0022-247X. doi: 10.1016/0022-247X(65)90027-2.
[36] Milan Studeny. Probabilistic Conditional Independence Structures . Springer, 2005.
[37]Richard S. Sutton, Doina Precup, and Satinder P. Singh. Between mdps and semi-mdps: A framework
for temporal abstraction in reinforcement learning. Artif. Intell. , 112(1-2):181–211, 1999.
[38]Guy Tennenholtz, Uri Shalit, and Shie Mannor. Oﬀ-policy evaluation in partially observable environments.
InAAAI, 2020.
[39]Philipp Wu, Alejandro Escontrela, Danijar Hafner, Ken Goldberg, and Pieter Abbeel. Daydreamer:
World models for physical robot learning. In Conference on Robot Learning , 2022.
[40] Junzhe Zhang and Elias Bareinboim. Transfer learning in multi-armed bandits: A causal approach. In
IJCAI, 2017.
[41]Junzhe Zhang and Elias Bareinboim. Near-optimal reinforcement learning in dynamic treatment regimes.
InNeurIPS , 2019.
[42]Junzhe Zhang and Elias Bareinboim. Designing optimal dynamic treatment regimes: A causal reinforce-
ment learning approach. In Hal Daumé III and Aarti Singh, editors, ICML, volume 119 of Proceedings
of Machine Learning Research , 2020.
[43]Junzhe Zhang and Elias Bareinboim. Bounding causal eﬀects on continuous outcomes. In AAAI, 2021.
[44]Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In Kamalika Chaudhuri,
Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the
39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning
Research , pages 27042–27059. PMLR, 17–23 Jul 2022.
17Published in Transactions on Machine Learning Research (08/2023)
Appendices
A Introduction to do-calculus 18
A.1 The three rules of do-calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.2 Note on ignorability and exogeneity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B Experimental details 19
B.1 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.2 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.3 Tiger experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C Additional empirical results 20
D Proof of Theorem 1. 23
A Introduction to do-calculus
The framework of do-calculus [ 30] was proposed as an intuitive tool to answer identiﬁability questions given a
causal graphG, such as, can the interventional distribution p(y|do(x),z)be recovered from the observational
distributions p(y,x,z )?
A.1 The three rules of do-calculus
Do-calculus relies on three graphical rules, which depend solely on the existence of speciﬁc structural
constraints in G:
•R1: insertion/deletion of observations, p(y|do(x),z,w ) =p(y|do(x),w)ifYandZared-separated
byX∪WinG⋆, the graph obtained from Gby removing all arrows pointing into variables in X.
•R2: action/observation exchange, p(y|do(x),do(z),w) =p(y|do(x),z,w )ifYandZared-separated
byX∪WinG†, the graph obtained from Gby removing all arrows pointing into variables in Xand
all arrows pointing out of variables in Z.
•R3: insertion/deletion of actions, p(y|do(x),do(z),w) =p(y|do(x),w)ifYandZared-separated by
X∪WinG‡, the graph obtained from Gby ﬁrst removing all the arrows pointing into variables in
X(thus creatingG⋆) and then removing all of the arrows pointing into variables in Zthat are not
ancestors of any variable in WinG⋆.
This set of rules has been shown to be complete [ 13;33], and results in an algorithm polynomial in the number
of nodes inGto answer identiﬁability questions, which either outputs "no" or "yes" along with an estimate
(a recovery formula) based on observational quantities. We refer the reader to Pearl [30]for a thorough
introduction to do-calculus.
A.2 Note on ignorability and exogeneity
In this paper we use at great length the concept of confounding, which is a core idea in Judea Pearl’s
do-calculus framework. For readers who are more familiar with the framework of potential outcomes from
Donald Rubin [ 14], the concept of confounding closely relates to the concepts of ignorability and exogeneity,
which can be shown to be equivalent to the unconfoundedness (no confounding) assumption [28].
18Published in Transactions on Machine Learning Research (08/2023)
B Experimental details
B.1 Training
In all our experiments we use tabular logistic models for each of the components in ˆq. That is, each building
blocq(z0),q(ot|zt),q(zt+1|zt,at), andq(at|ht,zt,i= 0)is parameterized using a set of softmax-normalized
scalars vectors. We train ˆqvia gradient descent using the Adam optimizer [ 17], by directly minimizing the
negative log likelihood of the model (equation (5)) on random mini-batches of trajectories sampled from
Dstd∪D prv. Agents are trained using the learned model as a “dream” environment (by sampling imaginary
trajectories τ∼ˆq(τ|i= 1)), with a simple actor-critic algorithm (REINFORCE with a state-value baseline)
for a ﬁxed number of iterations, also using the Adam optimizer. Both the actor and critic consists of a
2-layers perceptron (MLP) with the same hidden layer size, which take as input the belief state recovered
from the model. The training hyperparameters we used in each experiment are displayed in table 1.
tigerhidden sloppy
treasures dark room
Latent model
latent space size|Z| 32 256 128
learning rate 10−310−310−3
number of epochs (max) 500 500 500
number of gradient steps per epoch 50 100 100
minibatch size (trajectories τ) 32 64 64
Actor-critic agent
exploration noise /epsilon1 0.5 0.2 0.2
hidden layer size 256 512 256
learning rate 5×10−45×10−45×10−4
number of epochs 200 400 200
number of gradient steps per epoch 50 50 50
minibatch size (trajectories τ) 32 64 64
minibatch return scaling yes no no
entropy bonus 10−310−310−3
discount factor γ 1 1 1
Table 1: Training hyperparameters we used in each experiment. When learning the model, we divide the
learning rate by 10 after 10 epochs without loss improvement (reduce on plateau), and we stop training after
20 epochs without improvement (early stopping). We use all available data for training, and we monitor the
training loss for early stopping (no validation set).
B.2 Evaluation
Model quality (likelihood). To evaluate the general quality of the recovered POMDP model, we compute
the likelihood of ˆqon a new interventional dataset Dtestobtained from the true environment pwith a uniformly
random policy πrand,
Eτ∼pinit,ptrans,pobs,πrand
ˆq(o0)|τ|/productdisplay
t=1ˆq(ot+1|ht,i= 1)
.
We report an empirical estimate of this measure using 10000 trajectories.
Agent performance (cumulated reward). To evaluate quality of the agent obtained from the model ˆq
for solving the standard POMDP control task, we compute the expected cumulated reward of the policy ˆπ⋆
19Published in Transactions on Machine Learning Research (08/2023)
on the true environment p,
Eτ∼pinit,ptrans,pprv,ˆπ⋆
|τ|/summationdisplay
t=1r(ot)
.
We report an empirical estimate of this measure using 10000 trajectories.
B.3 Tiger experiment
We present the (compact) POMDP dynamics of the tigerproblem in table 2. After conversion to the
notation in the paper, the observations become ot= (roart,rewardt), the actions remain at=actiont, and
the hidden states are st= (tigert,rewardt). The privileged policies used in the experiments (section 5.2) are
reported in table 3.
Table 2: Compact POMDP dynamics in the tigerproblem.
tiger0
leftright
0.50.5
p(tiger0)
tigert+1
tigertactiontleftright
leftlisten 1.00.0
open left 0.50.5
open right 0.50.5
rightlisten 0.01.0
open left 0.50.5
open right 0.50.5
p(tigert+1|tigert,actiont)roart
tigertleftright
left0.850.15
right0.150.85
p(roart|tigert)
rewardt+1
tigertactiont-1-100+10
leftlisten 1.00.00.0
open left 0.01.00.0
open right 0.00.01.0
rightlisten 1.00.00.0
open left 0.00.01.0
open right 0.01.00.0
p(rewardt+1|tigert,actiont)
Table 3: Privileged policies πprv(action|tiger)used in the tigerexperiment.
actiont
privileged policy tigertlisten left right
randomleft0.33 0.33 0.33
right0.33 0.33 0.33
noisy goodleft0.05 0.30 0.65
right0.05 0.80 0.15
perfect goodleft0.00 0.00 1.00
right0.00 1.00 0.00
perfect badleft0.00 1.00 0.00
right0.00 0.00 1.00
C Additional empirical results
20Published in Transactions on Machine Learning Research (08/2023)
|Dstd| no obs naive augmented
100
400
1000
2000
4000
6000
Figure 8: Evolution of the test-time agent trajectories in the hidden treasures experiment. We report a
heatmap of the tiles visited by each agent ( no obs,naive,augmented ) at diﬀerent time steps (number of
interventional samples collected) during a single RL run (single seed). Eventually all methods converge to the
optimal strategy, which is to cycle through the 4 corners. Our augmented method converges to this behaviour
earlier on during training.
21Published in Transactions on Machine Learning Research (08/2023)
|Dstd| no obs naive augmented
100
400
1000
2000
4000
6000
Figure 9: Evolution of the test-time agent trajectories in the sloppy dark room experiment. We report
a heatmap of the tiles visited by each agent ( no obs,naive,augmented ) at diﬀerent time steps (number
of interventional samples collected), averaged over 10 RL runs (10 seeds). Eventually all methods manage
to consistently overcome the obstacle and reach the target tile. Our augmented method converges to this
behaviour earlier on during training.
22Published in Transactions on Machine Learning Research (08/2023)
D Proof of Theorem 1.
Theorem 1. Assuming|Dprv|→∞, for anyDstdthe recovered causal model is bounded as follows:
T−1/productdisplay
t=0ˆq(ot+1|o0→t,do(a0→t))≥T−1/productdisplay
t=0p(at|ht,i= 0)p(ot+1|ht,at,i= 0), and
T−1/productdisplay
t=0ˆq(ot+1|o0→t,do(a0→t))≤T−1/productdisplay
t=0p(at|ht,i= 0)p(ot+1|ht,at,i= 0) + 1−T−1/productdisplay
t=0p(at|ht,i= 0),
∀hT−1,aT−1,T≥1wherep(hT−1,aT−1,i= 0)>0.
Proof of Theorem 1. Considerq(τ,i)∈Qany distribution that follows our augmented POMDP constraints.
As an intermediary step, we will start by proving the following
T−1/productdisplay
t=0q(ot+1|ht,at,i= 1) =ZT+1/summationdisplay
z0→Tq(z0|h0,i= 0)T−1/productdisplay
t=0q(zt+1,ot+1|zt,at,ht,i= 0). (6)
First, for any 0≤t≤T−1, we can write the following factorization
q(zt,zt+1,ot+1|ht,at,i= 1) =q(zt|ht,at,i= 1)q(zt+1,ot+1|zt,ht,at,i= 1).
Because of the augmented POMDP constraints, the independences Zt⊥ ⊥At|Ht,I= 1andZt+1,Ot+1⊥ ⊥I|
Zt,At,Hthold inq, which further allows us to write
q(zt,zt+1,ot+1|ht,at,i= 1) =q(zt|ht,i= 1)q(zt+1,ot+1|zt,ht,at,i= 0). (7)
Then, we directly get
q(ot+1|ht,at,i= 1) =Z×Z/summationdisplay
zt,zt+1q(zt|ht,i= 1)q(zt+1,ot+1|zt,ht,at,i= 0). (8)
Now, let us consider the special case where T= 1. We can use the constraint Z0⊥ ⊥I|H0to write
q(o1|h0,a0,i= 1) =Z2/summationdisplay
z0→1q(z0|h0,i= 0)q(z1,o1|z0,h0,a0,i= 0),
which is equation (6), the desired result, for T= 1. In the case where T≥2, we can reuse equation (8) to
write
q(oT|hT−1,aT−1,i= 1) =Z2/summationdisplay
zT−1→Tq(zT−1|hT−2,aT−2,oT−1,i= 1)q(zT,oT|zT−1,hT−1,aT−1,i= 0)
=Z2/summationdisplay
zT−1→Tq(zT−1,oT−1|hT−2,aT−2,i= 1)
q(oT−1|hT−2,aT−2,i= 1)q(zT,oT|zT−1,hT−1,aT−1,i= 0)
T−1/productdisplay
t=T−2q(ot+1|ht,at,i= 1) =Z2/summationdisplay
zT−1→Tq(zT−1,oT−1|hT−2,aT−2,i= 1)q(zT,oT|zT−1,hT−1,aT−1,i= 0).
Then, we can introduce variable ZT−2and use equation (7) again to obtain
T−1/productdisplay
t=T−2q(ot+1|ht,at,i= 1) =Z3/summationdisplay
zT−2→Tq(zT−2,zT−1,oT−1|hT−2,aT−2,i= 1)q(zT,oT|zT−1,hT−1,aT−1,i= 0)
=Z3/summationdisplay
zT−2→Tq(zT−2|hT−2,i= 1)T−1/productdisplay
t=T−2q(zt+1,ot+1|zt,ht,at,i= 0).
23Published in Transactions on Machine Learning Research (08/2023)
In the case where T= 2, we can use Z0⊥ ⊥I|H0again to obtain equation (6), the desired result for T= 2.
In the case where T≥3, we can apply the same steps again to obtain
T−1/productdisplay
t=T−3q(ot+1|ht,at,i= 1) =Z4/summationdisplay
zT−3→Tq(zT−3|hT−3,i= 1)T−1/productdisplay
t=T−3q(zt+1,ot+1|zt,ht,at,i= 0).
Now, either T= 3and we can use Z0⊥ ⊥I|H0to obtain equation (6), or T≥4and we can continue the
decomposition by introducing ZT−4. By following this recursive approach we eventually reach Z0and prove
equation (6) for any T.
Let us now re-express equation (6) as follows
T−1/productdisplay
t=0q(ot+1|ht,at,i= 1) =ZT+1/summationdisplay
z0→Tq(z0|h0,i= 0)/parenleftBiggT−1/productdisplay
t=0q(zt+1,ot+1|zt,ht,at,i= 0)/parenrightBigg/parenleftBiggT−1/productdisplay
t=0q(at|zt,ht,i= 0)/parenrightBigg
+ZT+1/summationdisplay
z0→Tq(z0|h0,i= 0)/parenleftBiggT−1/productdisplay
t=0q(zt+1,ot+1|zt,ht,at,i= 0)/parenrightBigg/parenleftBigg
1−T−1/productdisplay
t=0q(at|zt,ht,i= 0)/parenrightBigg/parenrightBigg
T−1/productdisplay
t=0q(ot+1|ht,at,i= 1) =T−1/productdisplay
t=0q(at|ht,i= 0)q(ot+1|ht,at,i= 0)
+ZT+1/summationdisplay
z0→Tq(z0|h0,i= 0)/parenleftBiggT−1/productdisplay
t=0q(zt+1,ot+1|zt,ht,at,i= 0)/parenrightBigg/parenleftBigg
1−T−1/productdisplay
t=0q(at|zt,ht,i= 0)/parenrightBigg
.
By assuming probabilities are positive, we can substitute the second term by 0 to obtain our lower bound
T−1/productdisplay
t=0q(ot+1|ht,at,i= 1)≥T−1/productdisplay
t=0q(at|ht,i= 0)q(ot+1|ht,at,i= 0).
Then by assuming probabilities are upper bounded by 1, we can substitute q(ot+1|zt+1,zt,ht,at,i= 0)by 1
to obtain our upper bound
T−1/productdisplay
t=0q(ot+1|ht,at,i= 1)≤T−1/productdisplay
t=0q(at|ht,i= 0)q(ot+1|ht,at,i= 0)
+ZT+1/summationdisplay
z0→Tq(z0|h0,i= 0)/parenleftBiggT−1/productdisplay
t=0q(zt+1|zt,ht,at,i= 0)/parenrightBigg/parenleftBigg
1−T−1/productdisplay
t=0q(at|zt,ht,i= 0)/parenrightBigg
≤T−1/productdisplay
t=0q(at|ht,i= 0)q(ot+1|ht,at,i= 0) + 1−T−1/productdisplay
t=0q(at|ht,i= 0).
Finally, with ˆqsolution of (5) and |Dprv|→∞we have that DKL(p(τ|i= 0)/bardblˆq(τ|i= 0)) = 0 , and thus
ˆq(at|ht,i= 0) =p(at|ht,i= 0)and in particular ˆq(ot+1|ht,at,i= 0) =p(ot+1|ht,at,i= 0), which allows us
to conclude.
24