Published in Transactions on Machine Learning Research (01/2024)
Are you using test log-likelihood correctly?
Sameer K. Deshpande∗sameer.deshpande@wisc.edu
University of Wisconsin–Madison
Soumya Ghosh∗ghoshso@us.ibm.com
MIT-IBM Watson AI Lab
IBM Research
Tin D. Nguyen∗tdn@mit.edu
MIT-IBM Watson AI Lab
Massachusetts Institute of Technology
Tamara Broderick tbroderick@mit.edu
MIT-IBM Watson AI Lab
Massachusetts Institute of Technology
Reviewed on OpenReview: https: // openreview. net/ forum? id= n2YifD4Dxo
Abstract
Test log-likelihood is commonly used to compare diﬀerent models of the same data or
diﬀerent approximate inference algorithms for ﬁtting the same probabilistic model. We
present simple examples demonstrating how comparisons based on test log-likelihood can
contradict comparisons according to other objectives. Speciﬁcally, our examples show that
(i) approximate Bayesian inference algorithms that attain higher test log-likelihoods need
not also yield more accurate posterior approximations and (ii) conclusions about forecast
accuracy based on test log-likelihood comparisons may not agree with conclusions based on
root mean squared error.
1 Introduction
Test log-likelihood, also known as predictive log-likelihood or test log-predictive, is computed as the log-
predictive density averaged over a set of held-out data. It is often used to compare diﬀerent models of the
same data or to compare diﬀerent algorithms used to ﬁt the same probabilistic model. Although there are
compelling reasons for this practice (Section 2.1), we provide examples that falsify the following, usually
implicit, claims:
•Claim: The higher the test log-likelihood, the more accurately an approximate inference algorithm
recovers the Bayesian posterior distribution of latent model parameters (Section 3).
•Claim: The higher the test log-likelihood, the better the predictive performance on held-out data
according to other measurements, like root mean squared error (Section 4).
Our examples demonstrate that test log-likelihood is not always a good proxy for posterior approximation
error. They further demonstrate that forecast evaluations based on test log-likelihood may not agree with
forecast evaluations based on root mean squared error.
We are not the ﬁrst to highlight discrepancies between test log-likelihood and other analysis objectives. For
instance, Quiñonero-Candela et al. (2005) and Kohonen & Suomela (2005) showed that when predicting
*These authors contributed equally to this work.
1Published in Transactions on Machine Learning Research (01/2024)
discrete data with continuous distributions, test log-likelihood can be made arbitrarily large by concentrating
probability into vanishingly small intervals. Chang et al. (2009) observed that topic models with larger test
log-predictive densities can be less interpretable. Yao et al. (2019) highlighted the disconnect between test
log-likelihood and posterior approximation error in the context of Bayesian neural networks. Our examples,
however, reveal more fundamental discrepancies between test log-likelihood and other evaluation metrics. In
particular, we show how comparisons based on test log-likelihood can contradict comparisons based on other
objectives even in simple models like linear regression.
After introducing our notation, we precisely deﬁne test log-likelihood and review arguments for its use
in Section 2. In Sections 3.1–3.3, we present several examples showing that across a range of posterior
approximations, those with higher test log-likelihoods may nevertheless provide worse approximation quality.
Then, in Section 3.4, we provide some intuition about why this phenomenon can occur when the model is
severely misspeciﬁed (Section 3.1); when using sophisticated posterior approximation methods (Section 3.2);
and even when there is little or no model misspeciﬁcation (Section 3.3). In Section 4, we show examples in
both complex and simple models where test log-likelihood is higher but root mean squared error on held-out
data is worse. Our examples in Section 4 do depend on model misspeciﬁcation, but we note that model
misspeciﬁcation is unavoidable in practice. We conclude in Section 5 with a reﬂection on when we should use
test log-likelihood in practice.
2 Background
We assume we have access to training and testing data such that all data points are independently and
identically distributed (i.i.d.) from an unknown probability distribution P. LetD={yn}N
n=1denote the
training data. In many standard analyses, practitioners will have access to a predictive density of a future
data point y⋆given the observed D:π(y⋆|D). For instance, consider the following three cases.
•Case A: Practitioners often model the observed data by introducing a parameter θand specifying
that the data are i.i.d. from a conditional distribution Π(Y|θ)with density π(y|θ).In a non-Bayesian
analysis, one usually computes a point estimate ˆθof the unknown parameter (e.g. by maximum
likelihood). Given a point estimate ˆθ,the predictive density π(y⋆|D)is justπ(y⋆|ˆθ).
•Case B: A Bayesian analysis elaborates the conditional model from Case A by specifying a prior
distribution Π(θ)and formally computes the density π(θ|D)of the posterior distribution Π(θ|D)
from the assumed joint distribution Π(D,θ).The Bayesian posterior predictive density is given by
π(y⋆|D) =/integraldisplay
π(y⋆|θ)π(θ|D)dθ. (1)
•Case C: An approximate Bayesian analysis proceeds as in Case B but uses an approximation in
place of the exact posterior. If we let Π(θ|D)represent an approximation to the exact posterior,
Equation (1) yields the approximate Bayesian posterior predictive density π(y⋆|D). Sometimes,
due the diﬃculty of the integral in Equation (1), a further approximation may be used to yield a
predictive density π(y⋆|D).
In all of these cases, we will refer to the practitioner as having access to a model Πthat determines the
predictive distribution Π(y⋆|D); in particular, we allow “model” henceforth to encompass ﬁtted models and
posterior approximations. One can ask how well the resulting Π(y⋆|D)predicts new data generated from P.
Practitioners commonly assess how well their model predicts out-of-sample using a held-out set of testing
dataD⋆={y⋆
n}N⋆
n=1,which was not used to train the model. To compute test log-likelihood, they average
evaluations of the log-predictive density function over the testing set:
TLL(D⋆; Π) :=1
N⋆N⋆/summationdisplay
n=1logπ(y⋆
n|D), (2)
where our notation makes explicit the dependence of the test log-likelihood ( TLL) on testing dataD⋆and the
chosen model Π.In particular, researchers commonly use test log-likelihood to select between two models of
2Published in Transactions on Machine Learning Research (01/2024)
the data, say Πand˜Π;that is, they select model Πover ˜Πwhenever TLL(D⋆; Π)is higher than TLL(D⋆;˜Π).
Note that the abbreviation NLPD (negative log predictive density) is also commonly used in the literature for
the negative TLL (Quiñonero-Candela et al., 2005; Kohonen & Suomela, 2005). In Appendix C, we brieﬂy
discuss some alternative metrics for model comparison.
2.1 The case for test log-likelihood
In what follows, we ﬁrst observe that, if we wanted to choose a model whose predictive distribution is closer to
the true data distribution in a certain KL sense, then it is equivalent to choose a model with higher expected
log-predictive density (elpd). Second, we observe that TLLis a natural estimator of elpdwhen we have access
to a ﬁnite dataset.
The unrealistic case where the true data-generating distribution is known. The expected log-
predictive density is deﬁned as
elpd(Π) :=/integraldisplay
logπ(y⋆|D)dP(y⋆).
Our use of the abbreviation elpdfollows the example of Gelman et al. (2014, Equation 1). If we ignore an
additive constant not depending on Π,elpd(Π)is equal to the negative Kullback–Leibler divergence from the
predictive distribution Π(y⋆|D)to the true data distribution P(y⋆). Speciﬁcally, if we assume Phas density
p(y⋆),we have
KL(P(y⋆)/bardblΠ(y⋆|D)) =/integraldisplay
p(y⋆) logp(y⋆)dy⋆−elpd(Π).
Thus,elpd(Π)>elpd(˜Π)if and only if the predictive distribution Π(y⋆|D)is closer, in a speciﬁc KL sense, to
the true data distribution than the predictive distribution ˜Π(y⋆|D)is.
Test log-likelihood as an estimator. Since we generally do not know the true generating distribution P,
computing elpd(Π)exactly is not possible. By assumption, though, the test data are i.i.d. draws from P.
SoTLL(D⋆; Π)is a computable Monte Carlo estimate of elpd(Π).If we assume elpd(Π)is ﬁnite, it follows
that a Strong Law of Large Numbers applies: as N⋆→∞,TLL(D⋆; Π)converges almost surely to elpd(Π).
Therefore, with a suﬃciently high amount of testing data, we might compare the estimates TLL(D⋆; Π)and
TLL(D⋆;˜Π)in place of the desired comparison of elpd(Π)andelpd(˜Π). Note that the Strong Law follows
from the assumption that the y⋆
nvalues are i.i.d. under P; it does not require any assumption on the model
Πand holds even when the model Πis misspeciﬁed.
2.2 Practical concerns
SinceTLL(D⋆; Π)is an estimate of elpd(Π),it is subject to sampling variability, and a careful comparison
would ideally take this sampling variability into account. We ﬁrst elaborate on the problem and then
describe one option for estimating and using the sampling variability in practice; we take this approach in
our experiments below.
Tostart, supposewehadanothersetof N⋆testingdatapoints, ˜D⋆. Thengenerally TLL(D⋆; Π)/negationslash=TLL(˜D⋆; Π).
So it is possible, in principle, to draw diﬀerent conclusions using the TLLbased on diﬀerent testing datasets.
We can more reasonably express conﬁdence that elpd(Π)is larger than elpd(˜Π)if the lower bound of a
conﬁdence interval for elpd (Π)exceeds the upper bound of a conﬁdence interval for elpd (˜Π).
We next describe one way to estimate useful conﬁdence intervals. To do so, we make the additional (mild)
assumption that
σ2
TLL(Π) :=/integraldisplay
[logπ(y⋆|D)−elpd(Π)]2dP(y⋆)<∞.
Then, since the y⋆
nare i.i.d. draws from P,a Central Limit Theorem applies: as N⋆→∞,
√
N⋆(TLL(D⋆; Π)−elpd(Π))d→N (0,σ2
TLL(Π)).
Although we cannot generally compute σTLL(Π),we can estimate it with the sample standard deviation
ˆσTLL(Π)of the evaluations {logπ(y⋆
n|D)}N⋆
n=1. The resulting approximate 95% conﬁdence interval for elpd(Π)
3Published in Transactions on Machine Learning Research (01/2024)
is TLL (D⋆; Π)±2ˆσTLL/√
N⋆.In what follows, then, we will conclude elpd (Π)>elpd(˜Π)if
TLL(D⋆; Π)−2ˆσTLL(Π)/√
N⋆>TLL(D⋆;˜Π) + 2ˆσTLL(˜Π)/√
N⋆. (3)
For the sake of brevity, we will still write TLL (D⋆; Π)>TLL(D⋆;˜Π)in place of Equation (3) below.
To summarize: for a suﬃciently large test dataset D⋆, we expect predictions made from a model with
larger TLL to be closer (in the KL sense above) to realizations from the true data-generating process. In
our experiments below, we choose large test datasets so that we expect TLL comparisons to reﬂect elpd
comparisons. Our experiments instead illustrate that closeness between Π(y⋆|D)andP(in the KL sense
above) often does not align with a diﬀerent stated objective.
3 Claim: higher test log-likelihood corresponds to better posterior approximation
In this section, we give examples where test log-likelihood is higher though the (approximation) quality of
an approximate posterior mean, variance, or other common summary is lower. We start with examples in
misspeciﬁed models and then give a correctly speciﬁed example. We conclude with a discussion of the source
of the discrepancy: even in the well-speciﬁed case, the Bayesian posterior predictive need not be close to the
true data-generating distribution.
Practitioners often use posterior expectations to summarize the relationship between a covariate and a
response. For instance, the posterior mean serves as a point estimate, and the posterior standard deviation
quantiﬁes uncertainty. However, as the posterior density π(θ|D)is analytically intractable, practitioners must
instead rely on approximate posterior computations. There are myriad approximate inference algorithms –
e.g. Laplace approximation, Hamiltonian Monte Carlo (HMC), mean-ﬁeld variational inference, to name just
a few. All these algorithms aim to approximate the same posterior Π(θ|D).Test log-likelihood is often used
to compare the quality of diﬀerent approximations, with higher TLLvalues assumed to reﬂect more accurate
approximations, e.g. in the context of variational inference (see, e.g., Hoﬀman et al., 2013; Ranganath et al.,
2014; Hernández-Lobato et al., 2016; Liu & Wang, 2016; Shi et al., 2018) or Bayesian deep learning (see, e.g.,
Hernández-Lobato & Adams, 2015; Gan et al., 2016; Li et al., 2016; Louizos & Welling, 2016; Sun et al., 2017;
Ghosh et al., 2018; Mishkin et al., 2018; Wu et al., 2019; Izmailov et al., 2020; 2021; Ober & Aitchison, 2021).
Formally, suppose that our exact posterior is Π(θ|D)and that we have two approximate inference algorithms
that produce two approximate posteriors, respectively ˆΠ1(θ|D)and ˆΠ2(θ|D).The exact posterior and its
approximations respectively induce predictive distributions Π(y⋆|D),ˆΠ1(y⋆|D),andˆΠ2(y⋆|D).For instance,
ˆΠ1(θ|D)could be the empirical distribution of samples drawn using HMC and ˆΠ2(θ|D)could be a mean-ﬁeld
variational approximation. Our ﬁrst example demonstrates that it is possible that (i) TLL(D⋆;ˆΠ1)>
TLL(D⋆; Π)but (ii) using ˆΠ1could lead to diﬀerent inference about model parameters than using the exact
posterior Π.Our second example demonstrates that it is possible that (i) TLL(D⋆;ˆΠ1)>TLL(D⋆;ˆΠ2)but
(ii)ˆΠ1(θ|D)is a worse approximation to the exact posterior Π(θ|D)than ˆΠ2(θ|D).
3.1TLLand downstream posterior inference
Relying on TLLfor model selection can lead to diﬀerent inferences than we would ﬁnd by using the exact
posterior. To illustrate, suppose we observe D100={(xn,yn)}100
n=1drawn from the following heteroscedastic
model:
xn∼N(0,1), yn|xn∼N(xn,1 + log(1 + exp( xn))). (4)
Further suppose we model these data with a misspeciﬁed homoscedastic model:
θ∼N([0,0]/latticetop,[1,0; 0,1]), yn|θ,φn∼N(θTφn,1), (5)
whereφn= [xn,1]/latticetop, andθ= [θ1,θ2]. Figure 1 shows the posterior mean and the 95%predictive interval of
the misspeciﬁed regression line θ/latticetopφfrom (A) the exact Bayesian posterior; (B) the mean ﬁeld variational
approximation restricted to isotropic Gaussians; and (C)–(F) variational approximations with re-scaled
marginal variances. Each panel includes a scatter plot of the observed data, D100. We also report the
2-Wasserstein distance between the exact posterior and each approximation and the TLLaveraged over
4Published in Transactions on Machine Learning Research (01/2024)
Figure 1: (Left). Predictive distributions under the Bayesian posterior and mean ﬁeld variational approx-
imations. The two numbers in the title of each plot are the 2-Wasserstein distance to the exact posterior
and test log-likelihood computed on 104test set observations. Two standard errors in the test log-likelihood
estimate are (A) 0.03, (B) 0.03, (C) 0.02, (D) 0.02, (E) 0.02, (F) 0.02. (Right). The relationship between
2-Wasserstein distance to the posterior and test log-likelihood.
N∗= 104test data points drawn from Equation (4); note that the 2-Wasserstein distance can be used to
bound diﬀerences in means and variances (Huggins et al., 2020). The variational approximation (panel (B) of
Figure 1) is quite accurate: the 2-Wasserstein distance between the approximation and the exact posterior is
∼10−4.See also Figure 2, which shows the contours of the exact and approximate posterior distributions. As
we scale up the variance of this approximation, we move away from the exact posterior over the parameters
but the posterior predictive distribution covers more data, yielding higher TLL. The left panel of Figure 11
in Appendix B.3 shows the same pattern using the KL divergence instead of the 2-Wasserstein distance.
TLLand a discrepancy in inferences. Researchers are often interested in understanding whether there
is a relationship between a covariate and response; a Bayesian analysis will often conclude that there is
no relationship if the posterior on the corresponding eﬀect-size parameter places substantial probability on
an interval not containing zero. In our example, we wish to check whether θ1= 0. Notice that the exact
posterior distribution (panel (A) in Figures 1 and 2) is concentrated on positive θ1values. The 95%credible
interval of the exact posterior1is[0.63,1.07].Since the interval does not contain zero, we would infer that
θ1/negationslash= 0. On the other hand, as the approximations become more diﬀuse (panels (B)–(F)), TLLincreases,
and the approximations begin to place non-negligible probability mass on negative θ1values. In fact, the
approximation with highest TLL(panel (F) in Figures 1 and 2) yields an approximate 95% credible interval
of [-0.29,1.99], which covers zero. Had we used this approximate interval, we would have failed to conclude
θ1/negationslash= 0.That is, in this case, we would reach a diﬀerent substantive conclusion about the eﬀect θ1if we (i)
use the exact posterior or (ii) use the approximation selected by highest TLL.
3.2TLLin the wild
Next, we examine a more realistic scenario in which the diﬀerence between the quality of the posterior
approximation and the exact posterior distribution TLLarises naturally, without the need to artiﬁcially
increase the marginal variance of the variational approximations. To explore this situation, we will ﬁrst
introduce another example of misspeciﬁcation and repeat the type of analysis described in Section 3.1.
1Throughout we used symmetric credible intervals formed by computing quantiles: the 95%interval is equal to the 2.5%–97.5%
interquantile range.
5Published in Transactions on Machine Learning Research (01/2024)
Figure 2: Contours of (A) the exact posterior, (B) the mean ﬁeld variational approximation restricted to
isotropic Gaussians, and (C)–(F) re-scaled mean ﬁeld approximations. The line θ1= 0is highlighted in red.
Figure 3: (Left). Predictive distributions under the Bayesian posterior (A) and the SWAG posterior with
SWAG learning rate of (B) 10−3, (C) 10−2, (D) 10−1, (E) 1, and (F) 10. The two numbers in the title of
each plot are the 2-Wasserstein distance to the exact posterior and test log-likelihood computed on 104test
set observations. Two standard errors in the test log-likelihood estimates are (A) 0.16, (B) 0.15, (C) 0.14, (D)
0.13, (E) 0.05, (F) 0.01. (Right). Contours of the (A) exact posterior, and (B)–(F) SWAG approximations
with diﬀerent learning rates. The line θ1= 0is highlighted in red.
Consider the following case: we observe 500 observations D500={(xn,yn)}500
n=1drawn from a non-linear
model:
θ∗= [−2,−1]/latticetop, xn∼N(0,1), yn|θ∗,φn∼N(θ/latticetop
∗φn+x2
n,0.5), (6)
whereφn= [xn,1]/latticetop. Further suppose we modeled these data with a misspeciﬁed linear model
θ∼N([0,0]/latticetop[1,0; 0,1]), yn|θ,φn∼N(θ/latticetopφn,0.5). (7)
While the misspeciﬁcation here might appear egregious, linear models are widely used in practice for modeling
non-linear phenomena when one is primarily interested in inferring whether the covariates are positively
correlated, negatively correlated, or are uncorrelated with the responses (Berk et al., 2014; 2018; Blanca
et al., 2018; Vowels, 2023). Next, we use SWAG (Maddox et al., 2019), an oﬀ-the-shelf approximate inference
algorithm, to approximate the posterior Π(θ|D500). We also repeat the re-scaled variational inference
experiment from Section 3.1 with this set of data and models (Equations (6) and (7)); see Appendix B.2.
6Published in Transactions on Machine Learning Research (01/2024)
SWAG uses a gradient-based optimizer with a learning rate schedule that encourages the optimizer to oscillate
around the optimal solution instead of converging to it. Then, a Gaussian distribution is ﬁt to the set of
solutions explored by the optimizer around the optimum using moment matching. In general, one must select
the learning rate schedule in a heuristic fashion. One might be tempted to use TLLto tune the learning rate
schedule. We use this heuristic and run SWAG for a thousand epochs, annealing the learning rate down to a
diﬀerent constant value after 750epochs. Although used pedagogically here, similar heuristics have been
used in practice (di Langosco et al., 2022), where the learning rate is tuned based on the accuracy achieved
on held-out data. We vary this constant value over the set {10−3,10−2,10−1,1,10}. In Figure 3, we show
the resulting posterior mean and the 95%predictive interval of the misspeciﬁed regression line θ/latticetopφfrom (A)
the Bayesian posterior; (B)–(F) the SWAG posteriors using diﬀerent learning rate schedules. In each plot, we
overlay the observed data D500(black dots) with the true data generating function in dashed black. We also
report the 2-Wasserstein distance between the exact posterior and each approximation and the TLLaveraged
overN∗= 104test data points drawn from Equation (6). In all cases, SWAG overestimates the posterior
variance, with predictive distributions that better cover the data and consequently lead to a higher TLL.
However, these SWAG posterior approximations are fartherfrom the exact posterior. In fact, we found that
a learning rate of 10(Figure 3, Left, panel (F)) maximized TLLbut led to the worst approximation of the
exact posterior.
As in the previous section, next suppose we ﬁt this misspeciﬁed linear model to understand whether there is
a relationship between the covariates and the responses, i.e., whether θ1= 0. Notice that the exact posterior
distribution (Figure 3, Right, panel (A)) is concentrated on negative θ1values, with the 95%posterior credible
interval being [−1.96,−1.79].Since the interval is to the left of zero, we would infer that θ1<0and that
the covariate and the response are negatively correlated. In contrast, if we select the SWAG approximation
with the highest TLL, we select the posterior approximation in panel (F) on the right side of Figure 3. The
corresponding 95%posterior credible interval is [−4.46,0.74], which places non-negligible probability mass on
θ1>0. In this case, we would not conclude that the response and the covariate are negatively correlated –
by contrast to the conclusion using the exact posterior.
Figure 4: (Left). Contours of (A) the exact posterior, (B) the mean ﬁeld variational approximation restricted
to isotropic Gaussians, and (C)–(F) re-scaled mean ﬁeld approximations. The two numbers in the title of
each plot are the 2-Wasserstein distance to the exact posterior and test log-likelihoods computed on 104test
set observations. Two standard errors in the test log-likelihood estimates are (A) 0.019, (B) 0.020, (C) 0.014,
(D) 0.013, (E) 0.011, (F) 0.009. (Right). The non-monotonic relationship between distance to posterior and
test log-likelihood. Observe that the exact posterior does not achieve highest test log-likelihood.
3.3TLLand well-speciﬁed models
The examples above demonstrated that TLLis not a reliable proxy to posterior approximation quality when
the model is misspeciﬁed. Though misspeciﬁed models are the norm in practice, we now demonstrate that a
7Published in Transactions on Machine Learning Research (01/2024)
distribution with higher TLLmay not provide a more accurate posterior approximation even when the model
is correctly speciﬁed.
To this end, consider the following Bayesian linear model:
θ∼N([0,0]/latticetop,[1,0.9; 0.9,1]), yn|θ,φn∼N(θ/latticetopφn,0.252), (8)
whereφn= [xn,1]/latticetop. Now, suppose we observe ten data points D10={(xn,yn)}10
n=1sampled as
θ∗= [−2,−1]/latticetop, xn∼N(0,1), yn|θ∗,φn∼N(θ/latticetop
∗φn,0.252). (9)
The left panel of Figure 4 plots the contours of (A) the exact posterior distribution Π(φ|D10); (B) the
mean ﬁeld variational approximation constrained to the isotropic Gaussian family; and (C)–(F) variational
approximations with re-scaled marginal variances. In each panel, we report the 2-Wasserstein distance
between the approximate and exact posterior and the test log-predictive averaged over N⋆= 104test data
points drawn from Equation (9).
Although we have correctly speciﬁed the conditional model of y|(θ,φ),the exact posterior has a lower TLL
than some of the approximate posteriors; in particular, the 95% conﬁdence intervals for (C) and (D) are
disjoint from the 95% conﬁdence interval for the exact posterior, shown in (A). The left panel of Figure 4
suggests that the more probability mass an approximate posterior places around the true data-generating
parameter, the higher the TLL.Eventually, as the approximation becomes more diﬀuse, TLLbegins to
decrease (Figure 4 (right)). The non-monotonicity demonstrates that an approximate posterior with larger
impliedTLLcan in fact be further away from the exact posterior in a 2-Wasserstein sense than an approximate
posterior with smaller implied TLL.The right panel of Figure 11 in Appendix B.3 demonstrates the same
pattern using the KL divergence instead of the 2-Wasserstein distance. And Figure 9 in Appendix B.3 shows
that, in the well-speciﬁed case, a distribution with larger TLLcan provide a worse approximation of the
posterior standard deviation than a distribution with smaller TLL .
3.4 What is going on?
We next discuss why we should not expect TLLto closely track posterior approximation quality, or posterior-
predictive approximation quality. Essentially the issue is that, even in the well-speciﬁed case, the Bayesian
posterior predictive distribution need not be close to the true data-generating distribution.
We illustrate these distinctions in Figure 5. The lower surface represents the space of distributions over a
latent parameter θ. The upper surface represents the space of distributions over an observable data point y⋆.
Each dot in the ﬁgure represents a distribution. The two dots in the lower surface are the exact posterior
Π(θ|D)(left, green dot) and an approximate posterior ˆΠ(θ|D)(right, red dot). The three dots in the upper
surface are the posterior predictive distribution Π(y⋆|D)(left, green dot), the approximate posterior predictive
ˆΠ(y⋆|D)(lower right, red dot), and the true data-generating distribution P(y⋆)(upper right, black dot). The
gray lines on the left and right indicate that the distribution in the upper surface can be obtained from the
corresponding (connected) distribution in the lower surface via Equation (1).
The remaining three (non-gray) lines represent three diﬀerent discrepancies. Recall from Section 2 that
TLL(D⋆;ˆΠ)captures how close the approximate posterior predictive ˆΠ(y⋆|D)is to the true data-generating
processP(y⋆)in a particular KL sense:
TLL(D⋆,ˆΠ)≈−KL/parenleftBig
P(y⋆)/bardblˆΠ(y⋆|D)/parenrightBig
+constant.
To illustrate this notion of closeness, or equivalently discrepancy, in Figure 5, we draw a pink line between
P(y⋆)andˆΠ(y⋆|D). We observe that the TLLimportantly does notapproximate (even up to a constant) the
analogous discrepancy from the approximate posterior predictive ˆΠ(y⋆|D)to the exact posterior predictive
Π(y⋆|D)(blue line in the upper surface); that is, it does not capture how close the posterior predictive
approximation is to the exact posterior predictive. The TLLlikewise does not approximate (even up to
a constant) the corresponding discrepancy from the approximate posterior ˆΠ(θ|D)to the exact posterior
8Published in Transactions on Machine Learning Research (01/2024)
P(y⋆)
Π(y⋆|D)
ˆΠ(y⋆|D)TLLData space
Π(θ|D)ˆΠ(θ|D)
Parameter space
Figure 5: Cartoon illustration highlighting the diﬀerence between three diﬀerent discrepancies explored in
Section 3.4. The surfaces are spaces of distributions over a latent parameter (lower surface) or an observable
data pointy⋆(upper surface). The pink line indicates that TLL(D⋆;ˆΠ)estimates a discrepancy between the
approximate posterior predictive ˆΠ(y⋆|D)(upper surface, lower right, red dot) and the true data-generating
distributionP(y⋆)(upper surface, upper right, black dot). The blue line represents a diﬀerent discrepancy
between the exact posterior predictive (upper surface, left, green dot) and the approximate posterior predictive
(upper surface, lower right, red dot). The yellow line represents another diﬀerent discrepancy between the
exact posterior (lower surface, left, green dot) and the approximate posterior (lower surface, right, red dot).
Gray lines connect distributions over parameters with their corresponding predictive distributions.
Π(θ|D)(yellow line in the lower surface); that is, it does not capture how close the posterior approximation
is to the exact posterior.
The pink and blue lines would (nearly) align if the posterior predictive were very close to the true data-
generating distribution. For a misspeciﬁed model, the posterior predictive need not be close to the true
data-generating distribution. For a well-speciﬁed model, the posterior predictive and true data-generating
distribution may still be far for a ﬁnite dataset. On that view, as suggested by an anonymous referee, we
might expect the observed phenomenon to disappear asymptotically in the well-speciﬁed setting if suﬃcient
regularity conditions hold. The argument, essentially, is that (i) the actual posterior Π(θ|D)converges to
a point-mass at the true data generating parameter; this convergence implies (ii) that the actual posterior
predictive Π(y⋆|D)converges to the true data distribution P(y⋆),from which it follows that for large enough
training datasets (iii) KL/parenleftBig
Π(y⋆|D)/bardblˆΠ(y⋆|D)/parenrightBig
≈KL/parenleftBig
P(y⋆)/bardblˆΠ(y⋆|D)/parenrightBig
.However, we emphasize ﬁrst that
essentially every real data analysis is misspeciﬁed. And second, if a practitioner is in a setting where they
are conﬁdent there is no uncertainty in the unknown parameter value, there may be little reason to take a
Bayesian approach or go to the sometimes-considerable computational burden of approximating the Bayesian
posterior.
4 Claim: higher test log-likelihood corresponds to lower predictive error
As noted in Sections 2.1 and 3.4, TLLestimates how close a predictive distribution is from the true data-
generating process in a speciﬁc KL sense. On that view and analogous to Section 3, we would not expect
conclusions made by TLLto match conclusions made by comparing other predictive losses. Rather than
9Published in Transactions on Machine Learning Research (01/2024)
focus on more esoteric losses in our experiments, we note that TLLand RMSE are often reported as default
measures of model ﬁt quality in papers. If conclusions made between TLLand RMSE do not always agree
(as we expect and reinforce experimentally next), we should not expect TLLto always reﬂect performance
according to other predictive losses beyond RMSE. If the TLLis of fundamental interest, this observation is
of little consequence; if TLLis a convenient stand-in for a potential future loss of interest, this observation
may be meaningful.
Misspeciﬁed Gaussian process regression. We next construct two models Πand ˜Πsuch that
TLL(D⋆; Π)<TLL(D⋆;˜Π)but˜Πyields larger predictive RMSE. Suppose we observe D100={(xn,yn)}100
n=1
from the following data generating process:
xn∼U(−5,+5)yn|xn∼N(sin(2xn),0.1). (10)
Further suppose we model this data using a zero-mean Gaussian process (GP) with Gaussian noise,
f∼GP(0,k(x,x/prime)), yn|fn∼N(fn,σ2), (11)
wherefnis shorthand for f(xn). First consider the case where we employ a periodic kernel,2constrain
the noise nugget σ2to1.6, and ﬁt all other hyper-parameters by maximizing the marginal likelihood. The
resulting ﬁt is shown in Figure 6 (A). Next, consider an alternate model where we use a squared-exponential
kernel and ﬁt all hyper-parameters including the noise nugget via maximum marginal likelihood. The resulting
ﬁt is displayed in Figure 6 (B). The squared exponential model fails to recover the predictive mean and
reverts back to the prior mean ( RMSE = 0.737, 95% conﬁdence interval [0.729,0.745]), while the periodic
model recovers the predictive mean accurately, as measured by RMSE = 0.355(95%conﬁdence interval
[0.351,0.360]). Despite the poor mean estimate provided by the squared exponential model, it scores a
substantially higher TLL.
Figure 6: The plots display two Gaussian processes trained on the same set of data (represented by black plus
symbols). The dashed red line shows the mean of the posterior Gaussian process, while the red highlighted
region represents the 95%predictive interval. The subplot titles display the TLL(±2standard error) attained
by each Gaussian process. Although the Gaussian process in panel (A) achieves a better mean ﬁt compared
to panel (B), it has a worse TLL when evaluated on 104test instances (represented by black dots).
2PeriodicMatern32 inhttps://github.com/SheffieldML/GPy
10Published in Transactions on Machine Learning Research (01/2024)
In this example, we see that, even with an accurate point estimate, TLL can be reduced by, for instance,
inﬂating the predictive uncertainty. And this discrepancy between TLL and RMSE is not necessarily removed
by optimizing the parameters of a model.
Misspeciﬁed linear regression. Our next example illustrates that even when all parameters in a model
are ﬁt with maximum likelihood, a comparison based on TLL may still disagree with a comparison based on
RMSE. It also illustrates that the discrepancy between TLL and RMSE can arise even in very simple and
low-dimensional models and even when the training dataset is very large.
Speciﬁcally, suppose that we observe D={(xn,yn)}100,000
n=1generated according to
xn∼U(0,25), yn|xn∼Laplace (xn,1/√
2), (12)
which we model using one of the following misspeciﬁed conditional linear models:
Π :yn|xn∼N(θxn,σ2)
or
˜Π :yn|xn∼Laplace (0.45 +θxn,λ).(13)
Both Πand˜Πdepend on two unknown parameters. Πdepends on a slope θand a residual variance σ2and
˜Πdepends on a slope θand a residual scale λ. The kind of misspeciﬁcation is diﬀerent across models; while
Πhas the correct mean speciﬁcation but incorrect noise speciﬁcation, ˜Πhas incorrect mean speciﬁcation but
correct noise speciﬁcation.
We computed the maximum likelihood estimates (MLEs) (ˆθΠ,ˆσΠ)and(ˆθ˜Π,ˆλ˜Π)for both models. The two
ﬁtted models induce the following predictive distributions of y⋆|x⋆:
Π(y⋆|x⋆,D) :y⋆|x⋆∼N(ˆθΠx⋆,ˆσ2
Π)
and
˜Π(y⋆|x⋆,D) :y⋆|x⋆∼Laplace (0.45 + ˆθ˜Πx⋆,ˆλ˜Π).(14)
The means of these predictive distributions are natural point estimates of the output y⋆at inputx⋆.
Using a test set of size N⋆= 395,000,we observed TLL(D⋆; Π) =−1.420<−1.389 =TLL(D⋆;˜Π).The
standard error of either TLL estimate is only 0.002. Hence, based on sample mean and standard error, we
conclude that ˜Πhas better elpdthan Π. These values suggest that on average over inputs x⋆,˜Π(y⋆|x⋆,D)
is closer toP(y⋆|x⋆)than Π(y⋆|x⋆,D)in a KL sense. However, using the same test set, we found that Π
yielded more accurate point forecasts, as measured by root mean square error (RMSE):
/parenleftBigg
1
N⋆N⋆/summationdisplay
n=1(y⋆
n−ˆθΠx⋆
n)2/parenrightBigg1/2
= 1.000<1.025 =/parenleftBigg
1
N⋆N⋆/summationdisplay
n=1(y⋆
n−0.45−ˆθ˜Πx⋆
n)2/parenrightBigg1/2
. (15)
In addition, the 95%conﬁdence intervals for the RMSE do not overlap: the interval for Π’s RMSE is
[0.997,1.005]and that for ˜Π’s RMSE is [1.022,1.029]. The comparison of RMSEs suggests that on average
over inputs x⋆,the predictive mean of Π(y⋆|x⋆,D)is closer to the mean of P(y⋆|x⋆)than the predictive mean
of˜Π(y⋆|x⋆,D).In other words, the model with larger TLL– whose predictive distribution is ostensibly closer
toP– makes worse point predictions than the model with smaller TLL .
5 Discussion
Our paper is neither a blanket indictment nor recommendation of test log-likelihood. Rather, we hope
to encourage researchers to explicitly state and commit to a particular data-analysis goal – and recognize
that diﬀerent methods may perform better under diﬀerent goals. For instance, when the stated goal is to
approximate (summary statistics of) a Bayesian posterior, we argue that it is inappropriate to rely on test
log-likelihood to compare diﬀerent approximation methods. We have produced examples where a model
11Published in Transactions on Machine Learning Research (01/2024)
can provide a better test log-likelihood but yield a (much) poorer approximation to the Bayesian posterior
– in particular, leading to fundamentally diﬀerent inferences and decisions. We have described why this
phenomenon occurs: test log-likelihood tracks closeness of approximate posterior predictive distributions to
the data-generating process and not to the posterior (or posterior predictive) distribution. At the same time,
we recognize that evaluating posterior approximation quality is a fundamentally diﬃcult problem and will
generally necessitate the use of a proxy. It may be useful to consider multiple of the available options; a full
accounting is beyond the scope of this paper, but they include using conjugate models where exact posterior
summary statistics are available; comparing to established MCMC methods on models where a suﬃciently
large compute budget might be expected to yield a reliable approximation; simulation-based calibration
(Talts et al., 2018); sample-quality diagnostics (Gorham & Mackey, 2015; Chwialkowski et al., 2016; Liu et al.,
2016); and a host of visual diagnostics (Gabry et al., 2019). A careful investigation to understand how a
particular method struggles or succeeds may be especially illuminating.
On the other hand, in many data analyses, the goal is to make accurate predictions about future observables
or identify whether a treatment will help people who receive it. In these cases and many others, using a
Bayesian approach is just one possible means to an end. And many of the arguments for using the exact
Bayesian posterior in decision making assume correct model speciﬁcation, which we cannot rely upon in
practice. In predictive settings in particular, test log-likelihood may provide a compelling way to assess
performance. In addition to being essentially the only strictly proper local scoring rule (Bernardo & Smith,
2000, Proposition 3.13), TLLis sometimes advertised as a “non-informative” choice of loss function (Robert,
1996). Importantly, however, non-informative does not mean all-encompassing: as our examples in Section 4
show, test log-likelihood does not necessarily track with other notions of predictive loss. As we discuss
in Section 2.1, test log-likelihood quantiﬁes a predictive discrepancy only in a particular Kullback–Leibler
sense. It is important to note, however, that just because two distributions are close in KL, their means
and variances need not be close; in fact, Propositions 3.1 & 3.2 of Huggins et al. (2020) show that the
means and variances of distributions that are close in KL can be arbitrarily far apart. So even in settings
where prediction is of interest, we recommend users clearly specify their analytic goals and use evaluation
metrics tailored to those goals. If there is a quantity of particular interest in the data-generating process,
such as a moment or a quantile, a good choice of evaluation metric may be an appropriate scoring rule.
Namely, one might choose a scoring rule whose associated divergence function is known to quantify the
distance between the forecast’s quantity of interest and that of the data-generating process. For instance,
when comparing the quality of mean estimates, one option is using the squared-error scoring rule, whose
divergence function is the integrated squared diﬀerence between the forecast’s mean estimate and the mean
of the data-generating process. Another option is the Dawid–Sebastiani score (Dawid & Sebastiani, 1999),
which prioritizes accurately estimating predictive means and variances. See Gneiting & Raftery (2007) for a
list of commonly used scoring rules and their associated divergences.
Acknowledgments
We are grateful to Will Stephenson for helping us ﬁnd examples of discrepancies between posterior approxi-
mation quality and TLL .
This work was supported in part by the MIT-IBM Watson AI Lab, an NSF Career Award, an ONR Early
Career Grant, the DARPA I2O LwLL program, an ARPA-E project with program director David Tew, and
the Wisconsin Alumni Research Foundation.
References
Richard Berk, Lawrence Brown, Andreas Buja, Edward George, Emil Pitkin, Kai Zhang, and Linda Zhao.
Misspeciﬁed mean function regression: Making good use of regression models that are wrong. Sociological
Methods & Research , 43(3):422–451, 2014.
Richard Berk, Lawrence Brown, Andreas Buja, Edward George, and Linda Zhao. Working with misspeciﬁed
regression models. Journal of Quantitative Criminology , 34:633–655, 2018.
12Published in Transactions on Machine Learning Research (01/2024)
Robert H. Berk. Limiting behavior of posterior distributions when the model is incorrect. Annals of
Mathematical Statistics , 37(1):51–58, 1966.
José M Bernardo and Adrian F.M. Smith. Bayesian Theory . Wiley, 2000.
María J Blanca, Rafael Alarcón, and Roser Bono. Current practices in data analysis procedures in psychology:
What has changed? Frontiers in Psychology , 9:2558, 2018.
Jonathan Chang, Sean Gerrish, Chong Wang, Jordan Boyd-Graber, and David Blei. Reading tea leaves: How
humans interpret topic models. Advances in Neural Information Processing Systems , 22, 2009.
Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of ﬁt. In Proceedings
of the 33rdInternational Conference on Machine Learning . 2016.
A. Philip Dawid and Paola Sebastiani. Coherent dispersion criteria for optimal experimental design. Annals
of Statistics , 27(1):65–81, 1999.
Lauro Langosco di Langosco, Vincent Fortuin, and Heiko Strathmann. Neural variational gradient descent.
InFourth Symposium on Advances in Approximate Bayesian Inference , 2022.
Jonah Gabry, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. Visualization in
Bayesian workﬂow. Journal of the Royal Statistical Society Series A , 182(2):389–402, 2019.
Zhe Gan, Chunyuan Li, Changyou Chen, Yunchen Pu, Qinliang Su, and Lawrence Carin. Scalable Bayesian
learning of recurrent neural networks for language modeling. arXiv pre-print arXiv:1611.08034 , 2016.
Andrew Gelman, Jessica Hwang, and Aki Vehtari. Understanding predictive information criteria for Bayesian
models.Statistics and Computing , 24:997–1016, 2014.
Soumya Ghosh, Jiayu Yao, and Finale Doshi-Velez. Structured variational learning of Bayesian neural
networks with horseshoe priors. In Proceedings of the 35thInternational Conference on Machine Learning .
2018.
Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules, prediction, and estimation. Journal
of the American Statistical Association , 102:359–378, 3 2007.
Jackson Gorham and Lester Mackey. Measuring sample quality with Stein’s method. Advances in Neural
Information Processing Systems , 28, 2015.
José Miguel Hernández-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of
Bayesian neural networks. In Proceedings of the 32ndInternational Conference on Machine Learning . 2015.
José Miguel Hernández-Lobato, Yingzhen Li, Mark Rowland, Daniel Hernández-Lobato, and Richard Turner.
Black-boxα-divergence minimization. In Proceedings of the 33rdInternational Conference on Machine
Learning . 2016.
Matthew D. Hoﬀman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal
of Machine Learning Research , 14:1303–1347, 2013.
Jonathan H. Huggins and Jeﬀrey W. Miller. Reproducible model selection using bagged posteriors. Bayesian
Analysis, 18(1):79–104, 2023.
Jonathan H. Huggins, Mikołaz Kasprzak, Trevor Campbell, and Tamara Broderick. Validated variational
inference via practical posterior error bounds. In Proceedings of the 23rdInternational Conference on
Artiﬁcial Intelligence and Statistics , 2020.
Pavel Izmailov, Wesley J Maddox, Polina Kirichenko, Timur Garipov, Dmitry Vetrov, and Andrew Gordon
Wilson. Subspace inference for Bayesian deep learning. In Uncertainty in Artiﬁcial Intelligence . 2020.
13Published in Transactions on Machine Learning Research (01/2024)
Pavel Izmailov, Sharad Vikram, Matthew D. Hoﬀman, and Andrew Gordon Wilson. What are Bayesian
neural network posteriors really like? In Proceedings of the 38thInternational Conference on Machine
Learning . 2021.
Jukka Kohonen and Jukka Suomela. Lessons learned in the challenge: making predictions and scoring them.
InMachine Learning Challenges Workshop , pp. 95–116. Springer, 2005.
Chunyuan Li, Changyou Chen, Kai Fan, and Lawrence Carin. High-order stochastic gradient thermostates for
Bayesian learning of deep models. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcal Intelligence .
2016.
Qian Liu and Dilin Wang. Stein variaitonal gradient descent: A general purpose Bayesian inference algorithm.
InAdvances in Neural Informational Processing Systems . 2016.
Qiang Liu, Jason Lee, and Michael Jordan. A kernelized Stein discrepancy for goodness-of-ﬁt tests. In
Proceedings of the 33rdInternational Conference on Machine Learning . 2016.
Sanae Lofti, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew Gordon Wilson. Bayesian model
selection, the marginal likelihood, and generalization. In Proceedings of the 39thInternational Conference
on Machine Learning . 2022.
Christos Louizos and Max Welling. Structured and eﬃcient variational deep learning with matrix Gaussian
posteriors. In Proceedings of the 33rdInternational Conference on Machine Learning . 2016.
David J.C. MacKay. Information Theory, Inference, and Learning Algorithms . Cambridge University Press,
2003.
Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple
baseline for Bayesian uncertainty in deep learning. Advances in Neural Information Processing Systems , 32,
2019.
Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt, and Mohammad Emtiyaz Khan. SLANG:
Fast structured covariance approximations for Bayesian deep learning with natural gradient. In Advances
in Neural Informational Processing Systems . 2018.
Sebastian W Ober and Laurence Aitchison. Global inducing point variational posteriors for Bayesian neural
networks and deep Gaussian processes. In Proceedings of the 38thInternational Conference on Machine
Learning . 2021.
Joaquin Quiñonero-Candela, Carl Edward Rasmussen, Fabian Sinz, Olivier Bousquet, and Bernhard Schölkopf.
Evaluating predictive uncertainty challenge. In Machine Learning Challenges Workshop , pp. 1–27. Springer,
2005.
Rajesh Ranganath, Sean Gerrish, and David M Blei. Black box variational inference. In Proceedings of the
17thInternational Conference on Artiﬁcial Intelligence and Statistics , 2014.
Christian P Robert. Intrinsic losses. Theory and Decision , 40:191–214, 1996.
Jiaxin Shi, Shengyang Sun, and Jun Zhu. Kernel implicit variational inference. In International Conference
on Learning Representations . 2018.
Shengyang Sun, Changyou Chen, and Lawrence Carin. Learning structured weight uncertaitny in Bayesian
neural networks. In Proceedings of the 20thInternational Conference on Artiﬁcial Intelligence and Statistics .
2017.
Sean Talts, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. Validating Bayesian
inference algorithms with simulation-based calibration. arXiv preprint arXiv:1804.06788 , 2018.
Matthew J Vowels. Misspeciﬁcation and unreliable interpretations in psychology and social science. Psycho-
logical Methods , 28(3):507, 2023.
14Published in Transactions on Machine Learning Research (01/2024)
Anqi Wu, Sebatian Nowozin, Edward Meeds, Richard E Turner, José Miguel Hernández-Lobato, and
AlexanderLGaunt. DeterministicvariationalinferenceforrobustBayesianneuralnetworks. In International
Conference on Learning Representations . 2019.
Jiayu Yao, Weiwei Pan, Soumya Ghosh, and Finale Doshi-Velez. Quality of uncertainty quantiﬁcation for
Bayesian neural network inference. arXiv pre-print arXiv:1906.09686 , 2019.
15Published in Transactions on Machine Learning Research (01/2024)
A Variational Approximations
In Section 3 we formed isotropic Gaussian approximations to the exact posterior. In our illustrative examples,
the exact posterior itself is a Gaussian distribution, N(µ,Σ). In Sections 3.1 and 3.3 we use variational
approximations that share the same mean as the exact posterior and are isotropic, N(µ,ρI), where Iis a
two-dimensional identity matrix and ρ>0is a scalar. In this family of distributions, the optimal variational
approximation is N(µ,ρ∗I), where,
ρ∗=argmin
ρ∈R+KL(N(µ,ρI)/bardblN(µ,Σ)),
=2
tr(Σ−1).(16)
The result follows from setting the gradient ∇ρKL(N(µ,ρI)/bardblN(µ,Σ)))to zero and rearranging terms,
∇ρKL(N(µ,ρI)/bardblN(µ,Σ))) = 0,=⇒ ∇ρtr(ρΣ−1)
2−∇ρlnρ= 0,
=⇒1
ρ=tr(Σ−1)
2,=⇒ρ=2
tr(Σ−1).(17)
Note thatρ∗is guaranteed to be positive since Σ−1is positive deﬁnite and thus tr(Σ−1)>0. This optimal
variational approximation, N(µ,ρ∗I)is used in Panel (B) of Figure 1, Figure 2, and Figure 4. The other
panels useN(µ,λρ∗I), withλ∈[1,5,10,15,30]for Figure 1 and Figure 2. For Figure 4 (Left), λtakes values
in[4,5,7,9], and in [1,2,3,4,5,6,7,8,9,10,11]for Figure 4 (Right).
B Experimental details and additional experiments
B.1 Conﬁdence Intervals
An additional note on conﬁdence intervals for TLL. Suppose we are comparing two models Πand
˜Π. Although TLL(D⋆; Π)(respectively, ˆσTLL(Π))will generally be correlated with TLL(D⋆;˜Π)(respectively,
ˆσTLL(˜Π)), we do not expect a more careful treatment of that correlation to change our substantive conclusions.
Conﬁdence intervals for RMSE. To compute the RMSE conﬁdence interval, we ﬁrst compute the mean of
the squared errors (MSE, m) and its associated standard error of the mean ( s). Since we have a large number
of data points and the MSE takes the form of a mean, we assume the sampling distribution of the MSE is
well-approximated by a normal distribution. We use [m−2s,m+ 2s]as the 95%conﬁdence interval for the
MSE. We use [√m−2s,√m+ 2s]as the 95%conﬁdence interval for the RMSE. Note that the resulting
RMSE conﬁdence interval will generally not be symmetric.
B.2 Additional TLL in the wild experiments
SWAG with higher learning rates. In Figure 7 we continue the experiment described in Section 3.2
but using higher learning rates of 12,15, and 20. Despite moving further from the exact posterior the test
log-likelihood remains higher than those achieved by SWAG approximations with lower learning rates (panels
(B) through (E) of Figure 3).
Mean ﬁeld variational inference. Next, we reproduce the experimental setup described in Section 3.2,
but instead of using SWAG to approximate the posterior, we use mean ﬁeld variational inference and examine
the relationship between TLLand posterior approximation quality under diﬀerent re-scalings of the marginal
variance of the optimal variational approximation. Figure 8 shows the posterior mean and the 95%predictive
interval of the misspeciﬁed regression line θ/latticetopφfrom (A) the Bayesian posterior; (B) the mean ﬁeld variational
approximation restricted to isotropic Gaussians; and (C)–(F) several re-scaled variational approximations.
In each plot, we overlaid the observed data D500, the true data generating function in dashed black, and
also report the 2-Wasserstein distance between the true posterior and each approximation and the TLL
16Published in Transactions on Machine Learning Research (01/2024)
Figure 7: (Left). Predictive distributions under the SWAG posterior with SWAG learning rate of (G) 12, (H)
15, (I) 20. The two numbers in the title of each plot are the 2-Wasserstein distance to the exact posterior
and test log-likelihood computed on 104test set observations. Two standard errors in the test log-likelihood
estimates are (G) 0.01, (H) 0.009, (I) 0.08. (Right). Contours of the SWAG approximations with diﬀerent
learning rates. The line θ1= 0is highlighted in red.
Figure 8: (Left). Predictive distributions under the Bayesian posterior and mean ﬁeld variational approxima-
tions. The two numbers in the title of each plot are the 2-Wasserstein distance to the true posterior and test
log-likelihoods computed on 104test set observations. Two standard errors in the test log-likelihood estimates
are (A) 0.16, (B) 0.16, (C) 0.03, (D) 0.02, (E) 0.02, (F) 0.01. (Right). The relationship between distance to
posterior and test log-predictive density. Observe the log scale of the horizontal axis and the non-monotonic
relationship between test log-predictive density and 2-Wasserstein distance to the Bayesian posterior.
averaged over N∗= 104test data points drawn from Equation (4) Like in our previous example, the mean
ﬁeld approximation (panel (B) of Figure 8) is very close to the exact posterior. Further, as we scale up
the marginal variance of the approximate posteriors, the posterior predictive distributions cover more data,
yielding higher TLL,while simultaneously moving away from the exact posterior over the model parameters
in a 2-Wasserstein sense. Interestingly, when the approximation is diﬀuse enough, TLLdecreases, again
highlighting its non-monotonic relationship with posterior approximation quality. In this example of a
misspeciﬁed model, the non-monotonic relationship between TLLand 2-Wasserstein distance means that
TLL is, at best, a poor proxy of posterior approximation quality.
B.3The highest TLLdoes not match the best estimate of a posterior summary statistic or the lowest
KL
We ﬁrst reproduce the experimental setup that produced Figure 4, but now in Figure 9, we plot TLLagainst
the error in estimating the posterior standard deviation. In particular, the horizontal axis shows the absolute
value of the diﬀerence between (a) the marginal standard deviation of the parameters of interest under the
approximation and (b) the marginal standard deviation under the exact posterior. As in the right panel of
Figure 4, we observe that the highest (best) TLLdoes not correspond to the lowest (best) error in estimating
the posterior standard deviation.
17Published in Transactions on Machine Learning Research (01/2024)
Figure 9: The non-monotonic relationship between diﬀerence in marginal standard deviations and TLLin a
well-speciﬁed case. (Left)The horizontal axis reports the absolute diﬀerence in the standard deviation of
the weightθ1between an approximation and the posterior. (Right)The horizontal axis reports the absolute
diﬀerence in the standard deviation of the bias θ2between an approximation and the posterior.
To create Figure 10, we reproduce the experimental setup from Figure 8. Relative to the right panel of
Figure 8, we change only what is plotted on the horizontal axis; for Figure 10, we plot the log of the absolute
value of the diﬀerence between marginal standard deviations. Again, we see that the highest (best) TLLdoes
not correspond to the lowest (best) error in estimating the posterior standard deviation.
Figure 10: The non-monotonic relationship between diﬀerence in marginal standard deviations and TLLin a
misspeciﬁed case. The meaning of horizontal axis is similar to that of Figure 9.
Finally, Figure 11 reproduces analyses from the main text but uses KL divergence instead of 2-Wasserstein
distance to measure posterior approximation quality. In particular, the left panel of Figure 11 recreates the
right panel of Figure 1; as in Figure 1, we see that the highest (best) TLLdoes not correspond to the lowest
(best) divergence value. Likewise, the right panel of Figure 11 recreates the right panel of Figure 4; as in
Figure 4, we see the highest TLL again does not correspond to the lowest divergence value.
18Published in Transactions on Machine Learning Research (01/2024)
Figure 11: The smallest KL divergence does not correspond to the largest TLL, in a misspeciﬁed case ( left)
and a well-speciﬁed case ( right). The left panel reproduces the experimental results presented in the right
panel of Figure 1, but uses the reverse KL divergence to measure discrepancy with the exact posterior instead
of the 2-Wasserstein distance. The right panel reproduces the results in the right panel of Figure 4.
C Alternative model comparison metrics
In this work, we focused on one model comparison metric, test log-likelihood. However, there are many other
model comparison metrics. In what follows, we consider marginal likelihood and the joint log predictive
density as potential model comparison metrics.
C.1 Marginal likelihood
Instead of choosing the model with higher test log-likelihood, one might instead choose the model with higher
marginal likelihood, where the marginal likelihood for model Πisπ(D) =/integraltext
π(D|θ)π(θ)dθ. That is, given
models Πand˜Π,one might choose Πwheneverπ(D)>˜π(D);
As an anonymous referee pointed out, the marginal likelihood is a well-established Bayesian model selection
criterion; see, e.g., pg. 348 of MacKay (2003). However, the marginal likelihood criterion has several well-
known limitations. First, marginal likelihood comparisons can be unreliable when the models being compared
are misspeciﬁed; see §2 and references within Huggins & Miller (2023), and see also Berk (1966). Beyond
this concern, marginal likelihood quantiﬁes only how well a model ﬁts the available training data. It is
otherwise silent or “peripherally related” (Lofti et al., 2022) to the predictive quality of the model. And so,
just because a model Πyields a higher marginal likelihood than ˜Π, it does not necessarily follow that Π
produces better predictions of future data than ˜Π– even when the training and testing data are i.i.d. from
the same distribution.
These limitations notwithstanding, one might still try to use marginal likelihood to assess posterior approxi-
mation. Though we have not explicitly constructed examples, we anticipate similar phenomena as reported
above; namely, we expect it is possible for one posterior approximation to achieve higher marginal likelihood
than another while providing a worse approximation to the exact posterior. And we would expect these
phenomena to occur for reasons analogous to the behavior we saw for TLL; what marginal likelihood measures
is not directly related to posterior approximation quality.
19Published in Transactions on Machine Learning Research (01/2024)
C.2 The logarithm of the joint predictive density
Lofti et al. (2022) consider the logarithm of the joint predictive density, logπ(y⋆
1,...,y⋆
N⋆|D),as an alternative
metric for assessment of predictive quality. Analogous to Equation (1),
π(y⋆
1,...,y⋆
N⋆|D) =/integraldisplay
π(y⋆
1,...,y⋆
N⋆|θ)π(θ|D)dθ. (18)
If the training data are strictly independent from the test data under the speciﬁed model Π(not just
conditionally independent given a latent parameter), then the log joint predictive density will equal the TLL
– but such a case would generally be uninteresting in Bayesian modeling. In general, the log joint predictive
density need not equal the TLL.
We have already seen that the TLLis a Monte Carlo estimate of the elpd; that is, the TLLis an estimate of
“how well is a model expected to predict a single held-out observation, on average ?” The number of samples
in that estimate (i.e., the number of test data points) will dictate the Monte Carlo noise of that estimate.
There are analogously (at least) two perspectives on the log joint predictive density. One is that the log joint
predictive measures “how well does a model predict a speciﬁc collection of held-out data?” A second is that
the log joint predictive is an estimate (from just a single noisy Monte Carlo draw and therefore very high
in variance) of “how well is a model expected to predict the next N⋆held-out observations, on average?”
TheTLLand log joint predictive can be seen as two extremes of a spectrum, as we describe next. Suppose
one could divide a test set of size N⋆evenly into Mmini-batches of test data. Then one could make a
Monte Carlo estimate with Msamples of “how well is a model expected to predict the next N⋆/Mheld-out
observations, on average?” As Mincreases, the Monte Carlo noise decreases and the size of the held-out set
of interest decreases.
Depending on the application, any of these questions may be of interest to a practitioner. While the object
of study in this paper has been the TLL, the arguments in Section 3.4 suggest that the log joint predictive
density (or any other point on the spectrum above) would exhibit a similar issue to the one described in
this paper. After all, just like TLL, log joint predictive density does not directly track discrepancies between
distributions of latent model parameters.
20