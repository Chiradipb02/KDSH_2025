Published in Transactions on Machine Learning Research (06/2023)
Undersampling is a Minimax Optimal Robustness Interven-
tion in Nonparametric Classiﬁcation
Niladri S. Chatterji∗niladri@cs.stanford.edu
Department of Computer Science
Stanford University
Saminul Haque∗saminulh@stanford.edu
Department of Computer Science
Stanford University
Tatsunori B. Hashimoto thashim@stanford.edu
Department of Computer Science
Stanford University
Reviewed on OpenReview: https: // openreview. net/ forum? id= r6oHDYOZ6p
Abstract
While a broad range of techniques have been proposed to tackle distribution shift, the simple
baseline of training on an undersampled balanced dataset often achieves close to state-of-
the-art-accuracy across several popular benchmarks. This is rather surprising, since under-
sampling algorithms discard excess majority group data. To understand this phenomenon,
we ask if learning is fundamentally constrained by a lack of minority group samples. We
prove that this is indeed the case in the setting of nonparametric binary classiﬁcation. Our
results show that in the worst case, an algorithm cannot outperform undersampling unless
there is a high degree of overlap between the train and test distributions (which is unlikely to
be the case in real-world datasets), or if the algorithm leverages additional structure about
the distribution shift. In particular, in the case of label shift we show that there is always an
undersampling algorithm that is minimax optimal. In the case of group-covariate shift we
show that there is an undersampling algorithm that is minimax optimal when the overlap
between the group distributions is small. We also perform an experimental case study on a
label shift dataset and ﬁnd that in line with our theory, the test accuracy of robust neural
network classiﬁers is constrained by the number of minority samples.
1 Introduction
A key challenge facing the machine learning community is to design models that are robust to distribution
shift. When there is a mismatch between the train and test distributions, current models are often brittle and
perform poorly on rare examples (Hovy & Søgaard, 2015; Blodgett et al., 2016; Tatman, 2017; Hashimoto
et al., 2018; Alcorn et al., 2019). In this paper, our focus is on group-structured distribution shifts. In the
training set, we have many samples from a majority group and relatively few samples from the minority
group, while during test time we are equally likely to get a sample from either group.
To tackle such distribution shifts, a naïve algorithm is one that ﬁrst undersamples the training data by
discarding excess majority group samples (Kubat & Matwin, 1997; Wallace et al., 2011) and then trains
a model on this resulting dataset (see Figure 1 for an illustration of this algorithm). The samples that
remain in this undersampled dataset constitute i.i.d. draws from the test distribution. Therefore, while
a classiﬁer trained on this pruned dataset cannot suﬀer biases due to distribution shift, this algorithm is
clearly wasteful, as it discards training samples. This perceived ineﬃciency of undersampling has led to
the design of several algorithms to combat such distribution shift (Chawla et al., 2002; Lipton et al., 2018;
1Published in Transactions on Machine Learning Research (06/2023)
No Undersampling
 Undersampling
Learned Boundary
Predicted MajorityPredicted Minority
Majority SamplesMinority Samples
True BoundaryDropped Samples
Figure 1: Example with linear models and linearly separable data. On the left we have the maximum
margin classiﬁer over the entire dataset, and on the right we have the maximum margin classiﬁer over the
undersampled dataset. The undersampled classiﬁer is less biased and aligns more closely with the true
boundary.
Sagawa et al., 2020; Cao et al., 2019; Menon et al., 2020; Ye et al., 2020; Kini et al., 2021; Wang et al., 2022).
In spite of this algorithmic progress, the simple baseline of training models on an undersampled dataset
remains competitive. In the case of label shift, where one class label is overrepresented in the training data,
this has been observed by Cui et al. (2019); Cao et al. (2019), and Yang & Xu (2020). While in the case of
group-covariate shift, a study by Idrissi et al. (2022) showed that the empirical eﬀectiveness of these more
complicated algorithms is limited.
For example, Idrissi et al. (2022) showed that on the group-covariate shift CelebA dataset the worst-group
accuracy of a ResNet-50 model on the undersampled CelebA dataset which discards 97% of the available
training data is as good as methods that use all of available data such as importance-weighted ERM (Shi-
modaira, 2000), Group-DRO (Sagawa et al., 2020) and Just-Train-Twice (Liu et al., 2021). In Table 1,
we report the performance of the undersampled classiﬁer compared to the state-of-the-art-methods in the
literature across several label shift and group-covariate shift datasets. We ﬁnd that, although undersampling
isn’t always the optimal robustness algorithm, it is typically a very competitive baseline and within 1–4%
the performance of the best method.
Table 1: Performance of undersampled classiﬁer compared to the best classiﬁer across several popular label
shift and group-covariate shift datasets. When reporting worst-group accuracy we denote it by a⋆. When
available, we report the 95%conﬁdence interval. We ﬁnd that the undersampled classiﬁer is always within
1–4%of the best performing robustness algorithm, except on the CIFAR100 and MultiNLI datasets. In
Appendix F we provide more details about each of the results in the table.
Shift Type Dataset/PaperTest/Worst-Group⋆Accuracy
Best Undersampled
LabelImb. CIFAR10 (step 10) (Cao et al., 2019) 87.81 84 .59
Imb. CIFAR100 (step 10) (Cao et al., 2019) 59.46 53 .08
CelebA (Idrissi et al., 2022) 86.9±1.1⋆85.6±2.3⋆
Waterbirds (Idrissi et al., 2022) 87.6±1.6⋆89.1±1.1⋆
Group-Covariate
MultiNLI (Idrissi et al., 2022) 78.0±0.7⋆68.9±0.8⋆
CivilComments (Idrissi et al., 2022) 72.0±1.9⋆71.8±1.4⋆
Inspired by the strong performance of undersampling in these experiments, we ask:
2Published in Transactions on Machine Learning Research (06/2023)
Is the performance of a model under distribution shift fundamentally
constrained by the lack of minority group samples?
To answer this question we analyze the minimax excess risk . We lower bound the minimax excess risk to
prove that the performance of anyalgorithm is lower bounded only as a function of the minority samples
(nmin). This shows that even if a robust algorithm optimally trades oﬀ between the bias and the variance, it
is fundamentally constrained by the variance on the minority group which decreases only with nmin.
Our contributions. In our paper, we consider the well-studied setting of nonparametric binary classiﬁ-
cation (Tsybakov, 2010). By operating in this nonparametric regime we are able to study the properties of
undersampling in rich data distributions, but are able to circumvent the complications that arise due to the
optimization and implicit bias of parametric models.
We provide insights into this question in the label shift scenario, where one of the labels is overrepresented
in the training data, Ptrain(y= 1)≥Ptrain(y=−1), whereas the test samples are equally likely to come
from either class. Here the class-conditional distribution P(x|y)is Lipschitz in x. We show that in
the label shift setting there is a fundamental constraint, and that the minimax excess risk of any robust
learning method is lower bounded by 1/nmin1/3. That is, minority group samples fundamentally constrain
performance under distribution shift. Furthermore, by leveraging previous results about nonparametric
density estimation (Freedman & Diaconis, 1981) we show a matching upper bound on the excess risk of
a standard binning estimator trained on an undersampled dataset to demonstrate that undersampling is
optimal.
Further, we experimentally show in a label shift dataset (Imbalanced Binary CIFAR10) that the accuracy
of popular classiﬁers generally follow the trends predicted by our theory. When the minority samples are
increased, theaccuracyoftheseclassiﬁersincreasesdrastically, whereaswhenthenumberofmajoritysamples
are increased the gains in the accuracy are marginal at best.
Wealsostudythecovariateshiftcase. Inthissetting, therehasbeenextensiveworkstudyingtheeﬀectiveness
of transfer (Kpotufe & Martinet, 2018; Hanneke & Kpotufe, 2019) from train to test distributions, often
focusing on deriving speciﬁc conditions under which this transfer is possible. In this work, we demonstrate
that when the overlap (deﬁned in terms of total variation distance) between the group distributions Paand
Pbis small, transfer is diﬃcult, and that the minimax excess risk of any robust learning algorithm is lower
bounded by 1/nmin1/3. While this prior work also shows the impossibility of using majority group samples
in the extreme case with no overlap, our results provide a simple lower bound that shows that the amount
of overlap needed to make transfer feasible is unrealistic. We also show that this lower bound is tight, by
proving an upper bound on the excess risk of the binning estimator acting on the undersampled dataset.
Taken together, our results underline the need to move beyond designing “general-purpose” robustness
algorithms (like importance-weighting (Cao et al., 2019; Menon et al., 2020; Kini et al., 2021; Wang et al.,
2022), g-DRO (Sagawa et al., 2020), JTT (Liu et al., 2021), SMOTE (Chawla et al., 2002), etc.) that are
agnostic to the structure in the distribution shift. Our worst case analysis highlights that to successfully
beat undersampling, an algorithm must leverage additional structure in the distribution shift.
2 Related work
On several group-covariate shift benchmarks (CelebA, CivilComments, Waterbirds), Idrissi et al. (2022)
showed that training ResNet classiﬁers on an undersampled dataset either outperforms or performs as well
as other popular reweighting methods like Group-DRO (Sagawa et al., 2020), reweighted ERM, and Just-
Train-Twice (Liu et al., 2021). They ﬁnd Group-DRO performs comparably to undersampling, while both
tend to outperform methods that don’t utilize group information.
One classic method to tackle distribution shift is importance weighting (Shimodaira, 2000), which reweights
the loss of the minority group samples to yield an unbiased estimate of the loss. However, recent work (Byrd
& Lipton, 2019; Xu et al., 2020) has demonstrated the ineﬀectiveness of such methods when applied to
overparameterized neural networks. Many followup papers (Cao et al., 2019; Ye et al., 2020; Menon et al.,
3Published in Transactions on Machine Learning Research (06/2023)
2020; Kini et al., 2021; Wang et al., 2022) have introduced methods that modify the loss function in various
ways to address this. However, despite this progress undersampling remains a competitive alternative to
these importance weighted classiﬁers.
Our theory draws from the rich literature on non-parametric classiﬁcation (Tsybakov, 2010). Apart from
borrowing this setting of nonparametric classiﬁcation, we also utilize upper bounds on the estimation error
of the simple histogram estimator (Freedman & Diaconis, 1981; Devroye & Györﬁ, 1985) to prove our upper
bounds in the label shift case. Finally, we note that to prove our minimax lower bounds we proceed by using
the general recipe of reducing from estimation to testing (Wainwright, 2019, Chapter 15). One diﬀerence
from this standard framework is that our training samples shall be drawn from a diﬀerent distribution than
the test samples used to deﬁne the risk.
Past work has established lower bounds on the minimax risk for binary classiﬁcation without distribution
shift for general VC classes (see, e.g., Massart & Nédélec, 2006). Note that, these bounds are not directly
applicable in the distribution shift setting, and consequently these lower bounds scale with the total number
ofsamplesn=nmaj+nminratherthanwiththeminoritynumberofsamples (nmin). Therearealsoreﬁnements
of this lower bound to obtain minimax lower bounds for cost-sensitive losses that penalize errors on the two
class classes diﬀerently (Kamalaruban & Williamson, 2018). By carefully selecting these costs it is possible
to apply these results in the label shift setting. However, these lower bounds remain loose and decay with
nandnmajin contrast to the tighter nmindependence in our lower bounds. We provide a more detailed
discussion about potentially applying these lower bounds to the label shift setting after the presentation of
our theorem in Section 4.1.
There is rich literature that studies domain adaptation and transfer learning under label shift (Maity et al.,
2020) and covariate shift (Ben-David et al., 2006; David et al., 2010; Ben-David et al., 2010; Ben-David
& Urner, 2012; 2014; Berlind & Urner, 2015; Kpotufe & Martinet, 2018; Hanneke & Kpotufe, 2019). The
principal focus of this line of work was to understand the value of unlabeled data from the target domain,
ratherthantocharacterizetherelativevalueofthenumberoflabeledsamplesfromthemajorityandminority
groups. Amongthesepapers, mostcloselyrelatedtoourworkarethoseinthecovariateshiftsetting(Kpotufe
& Martinet, 2018; Hanneke & Kpotufe, 2019). Their lower bound results can be reinterpreted to show that
under covariate shift in the absence of overlap, the minimax excess risk is lower bounded by 1/nmin1/3. We
provide a more detailed comparison with their results after presenting our lower bounds in Section 4.2.
Finally, we note that Arjovsky et al. (2022) recently showed that undersampling can improve the worst-class
accuracy of linear SVMs in the presence of label shift. In comparison, our results hold for arbitrary classiﬁers
with the rich nonparametric data distributions.
3 Setting
In this section, we shall introduce our problem setup and deﬁne the types of distribution shift that we
consider.
3.1 Problem setup
The setting for our study is nonparametric binary classiﬁcation with Lipschitz data distributions. We are
givenntraining datapoints S:={(x1,y1),..., (xn,yn)}∈([0,1]×{− 1,1})nthat are all drawn from a train
distribution Ptrain. During test time, the data shall be drawn from a diﬀerent distribution Ptest. Our paper
focuses on the robustness to this shift in the distribution from train to test time. To present a clean analysis,
we study the case where the features xare bounded scalars, however, it is easy to extend our results to the
high-dimensional setting.
Given a classiﬁer f: [0,1]→{− 1,1}, we shall be interested in the test error (risk) of this classiﬁer under
the test distribution Ptest:
R(f;Ptest) :=E(x,y)∼Ptest[1(f(x)/negationslash=y)].
4Published in Transactions on Machine Learning Research (06/2023)
3.2 Types of distribution shift
We assume that Ptrainconsists of a mixture of two groups of unequal size, and Ptestcontains equal numbers
of samples from both groups. Given a majority group distribution Pmajand a minority group distribution
Pmin, the learner has access to nmajmajority group samples and nminminority group samples:
Smaj∼Pnmaj
majandSmin∼Pnmin
min.
Herenmaj> n/ 2andnmin< n/ 2withnmaj+nmin=n. The full training dataset is S=Smaj∪S min=
{(x1,y1),..., (xn,yn)}. We assume that the learner has access to the knowledge whether a particular sample
(xi,yi)comes from the majority or minority group.
The test samples will be drawn from Ptest=1
2Pmaj+1
2Pmin, a uniform mixture over PmajandPmin. Thus, the
training dataset is an imbalanced draw from the distributions PmajandPmin, whereas the test samples are
balanced draws. We let ρ:=nmaj/nmin>1denote the imbalance ratio in the training data. We consider the
uniform mixture during test time since the resulting test loss is of the same order as the worst-group loss.
We focus on two-types of distribution shifts: label shift and group-covariate shift that we describe below.
3.2.1 Label shift
In this setting, the imbalance in the training data comes from there being more samples from one class over
another. Without loss of generality, we shall assume that the class y= 1is the majority class. Then, we
deﬁne the majority and the minority class distributions as
Pmaj(x,y) =P1(x)1(y= 1)and Pmin=P−1(x)1(y=−1),
where P1,P−1are class-conditional distributions over the interval [0,1]. We assume that class-conditional
distributions Pihave densities on [0,1]and that they are 1-Lipschitz: for any x,x/prime∈[0,1],
|Pi(x)−Pi(x/prime)|≤|x−x/prime|.
We denote the class of pairs of distributions (Pmaj,Pmin)that satisfy these conditions by PLS. We note that
such Lipschitzness assumptions are common in the literature (see Tsybakov, 2010).
3.2.2 Group-covariate shift
In this setting, we have two groups {a,b}, and corresponding to each of these groups is a distribution (with
densities) over the features Pa(x)andPb(x). We letacorrespond to the majority group and bcorrespond
to the minority group. Then, we deﬁne
Pmaj(x,y) =Pa(x)P(y|x)and Pmin(x,y) =Pb(x)P(y|x).
We assume that for y∈{− 1,1}, for allx,x/prime∈[0,1]:
/vextendsingle/vextendsingleP(y|x)−P(y|x/prime)/vextendsingle/vextendsingle≤|x−x/prime|,
that is, the distribution of the label given the feature is 1-Lipschitz, and it varies slowly over the domain.
To quantify the shift between the train and test distribution, we deﬁne a notion of overlap between the group
distributions PaandPbas follows:
Overlap (Pa,Pb) := 1−TV(Pa,Pb)
where TV(Pa,Pb) := supE⊆[0,1]|Pa(E)−Pb(E)|, denotes the total variation distance between Paand Pb.
Notice that when PaandPbhave disjoint supports, TV(Pa,Pb) = 1and therefore Overlap (Pa,Pb) = 0. On
the other hand when Pa=Pb,TV(Pa,Pb) = 0andOverlap (Pa,Pb) = 1. When the overlap is 1, the majority
and minority distributions are identical and hence we have no shift between train and test. Observe that
Overlap (Pa,Pb) =Overlap (Pmaj,Pmin)since P(y|x)is shared across PmajandPmin.
5Published in Transactions on Machine Learning Research (06/2023)
Given a level of overlap τ∈[0,1]we denote the class of pairs of distributions (Pmaj,Pmin)with overlap at
leastτbyPGS(τ). It is easy to check that, PGS(τ)⊆P GS(0)at any overlap level τ∈[0,1].
Considering a notion of overlap between the marginal distributions Pa(x)andPb(x)is natural in the group
covariate setting since the conditional distribution that we wish to estimate P(y|x)remains constant from
train to test time. Higher overlap between PaandPballows a classiﬁer to learn more about the underlying
conditional distribution P(y|x)when it sees samples from either group. In contrast, in the label shift setting
P(x|y)remains constant from train to test time and higher overlap between P(x|1)andP(x|−1)does
not help to estimate P(y|x).
4 Lower bounds on the minimax excess risk
In this section, we shall prove our lower bounds that show that the performance of any algorithm is con-
strained by the number of minority samples nmin. Before we state our lower bounds, we need to introduce
the notion of excess risk and minimax excess risk.
Excess risk and minimax excess risk. We measure the performance of an algorithm Athrough its
excess risk deﬁned in the following way. Given an algorithm Athat takes as input a dataset Sand returns
a classiﬁerAS, and a pair of distributions (Pmaj,Pmin)with Ptest=1
2Pmaj+1
2Pmin, theexpected excess risk is
given by
Excess Risk [A; (Pmaj,Pmin)] :=ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
R(AS;Ptest))−R(f⋆(Ptest);Ptest)/bracketrightbig
, (1)
wheref⋆(Ptest)is the Bayes classiﬁer that minimizes the risk R(·;Ptest). The ﬁrst term corresponds to the
expected risk for the algorithm when given nmajsamples from Pmajandnminsamples from Pmin, whereas the
second term corresponds to the Bayes error for the problem.
Excess risk does not let us characterize the inherent diﬃculty of a problem, since for any particular data
distribution (Pmaj,Pmin)the best possible algorithm Ato minimize the excess risk would be the trivial
mappingAS=f⋆(Ptest). Therefore, to prove meaningful lower bounds on the performance of algorithms we
need to deﬁne the notion of minimax excess risk (see Wainwright, 2019, Chapter 15). Given a class of pairs
of distributionsPdeﬁne
Minimax Excess Risk (P) := inf
Asup
(Pmaj,Pmin)∈PExcess Risk [A; (Pmaj,Pmin)], (2)
where the inﬁmum is over all measurable estimators A. The minimax excess risk is the excess risk of the
“best” algorithm in the worst case over the class of problems deﬁned by P.
4.1 Label shift lower bounds
We demonstrate the hardness of the label shift problem in general by establishing a lower bound on the
minimax excess risk.
Theorem 4.1. Consider the label shift setting described in Section 3.2.1. Recall that PLSis the class of
pairs of distributions (Pmaj,Pmin)that satisfy the assumptions in that section. The minimax excess risk over
this class is lower bounded as follows:
Minimax Excess Risk (PLS) = inf
Asup
(Pmaj,Pmin)∈P LSExcess Risk [A; (Pmaj,Pmin)]≥1
6001
nmin1/3. (3)
We establish this result in Appendix B. We show that rather surprisingly, the lower bound on the minimax
excess risk scales only with the number of minority class samples nmin1/3, and does not depend on nmaj.
Intuitively,thisisbecauseanylearnermustpredictwhichclass-conditionaldistribution( P(x|1)orP(x|−1))
assigns higher likelihood at that x. To interpret this result, consider the extreme scenario where nmaj→∞
butnminisﬁnite. Inthiscase, thelearnerhasfullinformationaboutthemajorityclassdistribution. However,
6Published in Transactions on Machine Learning Research (06/2023)
thelearningtaskcontinuestobechallengingsinceanylearnerwouldbeuncertainaboutwhethertheminority
class distribution assigns higher or lower likelihood at any given x. This uncertainty underlies the reason
why the minimax rate of classiﬁcation is constrained by the number of minority samples nmin.
We brieﬂy note that, applying minimax lower bounds from the transfer learning literature (Maity et al.,
2020, Theorem 3.1 with α= 1,β= 0andd= 1) to our problem leads to a more optimistic lower bound of
1/n1/3. Our lower bounds that scale as 1/nmin1/3, uncover the fact that only adding minority class samples
helps reduce the risk.
As noted above in the introduction, it is possible to obtain lower bounds for the label shift setting by applying
bounds from the cost-sensitive classiﬁcation literature. However, as we shall argue below they are loose and
predict the incorrect trend when applied in this setting. Consider the result (Kamalaruban & Williamson,
2018, Theorem 4) which is a minimax lower bound for cost sensitive binary classiﬁcation that applies to
VC classses (which does not capture the nonparameteric setting studied here but it is illuminating to study
how that bound scales with the imbalance ratio ρ=nmaj/nmin). Assume that the joint distribution during
training is a mixture distribution given by P=ρ
1+ρPmaj+1
1+ρPminso that on average the ratio of the number
of samples from the majority and minority class is equal to ρ. Then by applying their lower bound we
ﬁnd that it scales with 1/(nρ)(see Appendix E for a detailed calculation). This scales inversely with ρthe
imbalance ratio and incorrectly predicts that the problem gets easier as the imbalance is larger. In contrast,
our lower bound scales with 1/nmin= (1 +ρ)/n, which correctly predicts that as the imbalance is larger, the
minimax test error is higher.
4.2 Group-covariate shift lower bounds
Next, we shall state our lower bound on the minimax excess risk that demonstrates the hardness of the
group-covariate shift problem.
Theorem 4.2. Consider the group shift setting described in Section 3.2.2. Given any overlap τ∈[0,1]
recall thatPGS(τ)is the class of distributions such that Overlap (Pmaj,Pmin)≥τ. The minimax excess risk in
this setting is lower bounded as follows:
Minimax Excess Risk (PGS(τ)) = inf
Asup
(Pmaj,Pmin)∈P GS(τ)Excess Risk [A; (Pmaj,Pmin)]
≥1
200(nmin·(2−τ) +nmaj·τ)1/3≥1
200nmin1/3(ρ·τ+ 2)1/3, (4)
whereρ=nmaj/nmin>1.
We prove this theorem in Appendix C.
We see that in the low overlap setting (τ/lessmuch1/ρ), the minimax excess risk is lower bounded by 1/nmin1/3, and
we are fundamentally constrained by the number of samples in minority group. To see why this is the case,
consider the extreme example with τ= 0where Pahas support [0,0.5]andPbhas support [0.5,1]. Thenmaj
majority group samples from Paprovide information about the correct label predict in the interval [0,0.5]
(the support of Pa). However, since the distribution P(y|x)is1-Lipschitz in the worst case these samples
provideverylimitedinformationaboutthecorrectpredictionsin [0.5,1](thesupportof Pb). Thus, predicting
on the support of Pbrequires samples from the minority group and this results in the nmindependent rate.
In fact, in this extreme case (τ= 0)even ifnmaj→∞, the minimax excess risk is still bounded away from
zero. This intuition also carries over to the case when the overlap is small but non-zero and our lower bound
shows that minority samples are much more valuable than majority samples at reducing the risk.
On the other hand, when the overlap is high ( τ/greatermuch1/ρ) the minimax excess risk is lower bounded by
1/(nmin(2−τ) +nmajτ)1/3and the extra majority samples are quite beneﬁcial. This is roughly because the
supports of PaandPbhave large overlap and hence samples from the majority group are useful in helping
make predictions even in regions where Pbis large. In the extreme case when τ= 1, we have that Pa=Pb
and therefore recover the classic i.i.d. setting with no distribution shift. Here, the lower bound scales with
1/n1/3, as one might expect.
7Published in Transactions on Machine Learning Research (06/2023)
Previous work on transfer learning with covariate shift has considered other more elaborate notions of trans-
ferability (Kpotufe & Martinet, 2018; Hanneke & Kpotufe, 2019) than overlap between group distributions
considered here. In the case of no overlap (τ= 0), previous results (Kpotufe & Martinet, 2018, Theorem 1
withα= 1,β= 0andγ=∞) yield the same lower bound of 1/nmin1/3. On the other extreme, applying
their result (Kpotufe & Martinet, 2018, Theorem 1 with α= 1,β= 0andγ= 0) in the high transfer regime
yields a lower bound on 1/n1/3. This result is aligned with the high overlap τ= 1case that we consider
here.
Beyond these two edge cases of no overlap ( τ= 0) and high overlap ( τ= 1), our lower bound is key to
drawing the simple complementary conclusion that even when overlap between group distributions is small
as compared to 1/ρ, minority samples alone dictate the rate of convergence.
5 Upper bounds on the excess risk for the undersampled binning estimator
We will show that an undersampled estimator matches the rates in the previous section showing that under-
sampling is an optimal robustness intervention. We start by deﬁning the undersampling procedure and the
undersampling binning estimator.
Undersampling procedure. Given training data S:={(x1,y1),..., (xn,yn)}, generate a new undersam-
pled datasetSUSby
•including all nminsamples fromSminand,
•includingnminsamples fromSmajby sampling uniformly at random without replacement.
This procedure ensures that in the undersampled dataset SUS, the groups are balanced, and that |SUS|=
2nmin.
The undersampling binning estimator deﬁned next will ﬁrst run this undersampling procedure to obtain SUS
and just uses these samples to output a classiﬁer.
Undersampled binning estimator Theundersampledbinningestimator AUSBtakesasinputadataset S
and a positive integer Kcorresponding to the number of bins, and returns a classiﬁer AS,K
USB: [0,1]→{− 1,1}.
This estimator is deﬁned as follows:
1. First, we compute the undersampled dataset SUS.
2. Given this dataset SUS, letn1,jbe the number of points with label +1that lie in the interval
Ij= [j−1
K,j
K]. Also, deﬁne n−1,janalogously. Then set
Aj=/braceleftBigg
1ifn1,j>n−1,j,
−1otherwise.
3. Deﬁne the classiﬁer AS,K
USBsuch that if x∈Ijthen
AS,K
USB(x) =Aj. (5)
Essentially in each bin Ij, we set the prediction to be the majority label among the samples that
fall in this bin.
Whenever the number of bins Kis clear from the context we shall denote AS,K
USBbyAS
USB. Below we establish
upper bounds on the excess risk of this simple estimator.
8Published in Transactions on Machine Learning Research (06/2023)
5.1 Label shift upper bounds
We now establish an upper bound on the excess risk of AUSBin the label shift setting (see Section 3.2.1).
Below we let c,C > 0be absolute constants independent of problem parameters like nmajandnmin.
Theorem 5.1. Consider the label shift setting described in Section 3.2.1. For any (Pmaj,Pmin)∈P LSthe
expected excess risk of the Undersampling Binning Estimator (Eq. (5)) with number of bins with K=
c⌈nmin1/3⌉is upper bounded by
Excess Risk [AUSB; (Pmaj,Pmin)] =ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
R(AS
USB;Ptest)−R(f⋆;Ptest)/bracketrightbig
≤C
nmin1/3.
We prove this result in Appendix B. This upper bound combined with the lower bound in Theorem 4.1 shows
that an undersampling approach is minimax optimal up to constants in the presence of label shift.
Our analysis leaves open the possibility of better algorithms when the learner has additional information
about the structure of the label shift beyond Lipschitz continuity.
5.2 Group-covariate shift upper bounds
Next, we present our upper bounds on the excess risk of the undersampled binning estimator in the group-
covariate shift setting (see Section 3.2.2). In the theorem below, C > 0is an absolute constant independent
of the problem parameters nmaj,nminandτ.
Theorem 5.2. Consider the group shift setting described in Section 3.2.2. For any overlap τ∈[0,1]and for
any(Pmaj,Pmin)∈P GS(τ)the expected excess risk of the Undersampling Binning Estimator (Eq. (5)) with
number of bins with K=⌈nmin1/3⌉is
Excess Risk [AUSB; (Pmaj,Pmin)] =ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
R(AS
USB;Ptest))−R(f⋆;Ptest)/bracketrightbig
≤C
nmin1/3.
We provide a proof for this theorem in Appendix C. Compared to the lower bound established in Theorem 4.2
which scales as 1/((2−τ)nmin+nmajτ)1/3, the upper bound for the undersampled binning estimator always
scales with 1/nmin1/3since it operates on the undersampled dataset ( SUS).
Thus, we have shown that in the absence of overlap (τ/lessmuch1/ρ=nmin/nmaj)there is an undersampling
algorithm that is minimax optimal up to constants. However when there is high overlap (τ/greatermuch1/ρ)there is
a non-trivial gap between the upper and lower bounds:
Upper Bound
Lower Bound=c(ρ·τ+ 2)1/3.
6 Minority sample dependence in practice
Inspired by our worst-case theoretical predictions in nonparametric classiﬁcation, we ask: how does the
accuracy of neural network classiﬁers trained using robust algorithms evolve as a function of the majority
and minority samples?
To explore this question, we conduct a small case study using the imbalanced binary CIFAR10 dataset (Byrd
& Lipton, 2019; Wang et al., 2022) that is constructed using the “cat” and “dog” classes. The test set consists
of all of the 1000cat and 1000dog test examples. To form our initial train and validation sets, we take 2500
cat examples but only 500dog examples from the oﬃcial train set, corresponding to a 5:1 label imbalance.
We then use 80%of those examples for training and the rest for validation. In our experiment, we either
(a)add only minority samples; (b)add only majority samples; (c)add both majority and minority samples
in a 5:1 ratio. We consider competitive robust classiﬁers proposed in the literature that are convolutional
neural networks trained either by using (i)the importance weighted cross entropy loss, or (ii)the importance
weighted VS loss (Kini et al., 2021). We early stop using the importance weighted validation loss in both
cases. The additional experimental details are presented in Appendix G.
9Published in Transactions on Machine Learning Research (06/2023)
Figure 2: Convolutional neural network classiﬁers trained on the Imbalanced Binary CIFAR10 dataset with
a 5:1 label imbalance. (Top) Models trained using the importance weighted cross entropy loss with early
stopping. (Bottom) Models trained using the importance weighted VS loss (Kini et al., 2021) with early
stopping. We report the average test accuracy calculated on a balanced test set over 5 random seeds. We
start oﬀ with 2500cat examples and 500dog examples in the training dataset. We ﬁnd that in accordance
with our theory, for both of the classiﬁers adding only minority class samples (red) leads to large gain in
accuracy (∼6%), while adding majority class samples (blue) leads to little or no gain. In fact, adding
majority samples sometimes hurts test accuracy due to the added bias. When we add majority and minority
samples in a 5:1 ratio (green), the gain is largely due to the addition of minority samples and is only
marginally higher ( <2%) than adding only minority samples. The green curves correspond to the same
classiﬁers in both the left and right panels.
Our results in Figure 2 are generally consistent with our theoretical predictions. By adding only minority
class samples the test accuracy of both classiﬁers increases by a great extent (6%), while by adding only
majority class samples the test accuracy remains constant or in some cases even decreases owing to the added
bias of the classiﬁers. When we add samples to both groups proportionately, the increase in the test accuracy
appears to largely to be due to the increase in the number of minority class samples. We see this on the left
panels, where the diﬀerence between adding only extra minority group samples (red) and both minority and
majority group samples (green) is small. Thus, we ﬁnd that the accuracy for these neural network classiﬁers
is also constrained by the number of minority class samples. Similar conclusions hold for classiﬁers trained
using the tilted loss (Li et al., 2020) and group-DRO objective (Sagawa et al., 2020) (see Appendix D).
7 Discussion
We showed that undersampling is an optimal robustness intervention in nonparametric classiﬁcation in the
absence of signiﬁcant overlap between group distributions or without additional structure beyond Lipschitz
continuity. We worked in one dimension for the sake of clarity and it would be interesting to extend this
study to higher dimensions. We focused on Lipschitz continuous distributions here, but it is also interesting
to consider other forms of regularity such as Hölder continuity.
10Published in Transactions on Machine Learning Research (06/2023)
At a high level our results highlight the need to reason about the speciﬁc structure in the distribution
shift and design algorithms that are tailored to take advantage of this structure. This would require us to
step away from the common practice in robust machine learning where the focus is to design “universal”
robustness interventions that are agnostic to the structure in the shift. Alongside this, our results also dictate
the need for datasets and benchmarks with the propensity for transfer from train to test time.
Acknowledgments
We would like to thank Ke Alexander Wang for his useful comments and feedback in the early stages of this
project. We would also like to thank Shibani Santurkar and Dimitrios Tsipras for useful discussions and
encouragement. Finally, we would like to thank the anonymous reviewers whose many helpful comments
improved the paper. NC was supported by a SAIL Postdoctoral Fellowship and TH was supported by a gift
from Open Philanthropy.
References
Michael Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, and Anh Nguyen. Strike
(with) a pose: Neural networks are easily fooled by strange poses of familiar objects. In Computer Vision
and Pattern Recognition (CVPR) , 2019.
Martin Arjovsky, Kamalika Chaudhuri, and David Lopez-Paz. Throwing away data improves worst-class
error in imbalanced classiﬁcation. arXiv preprint arXiv:2205.11672 , 2022.
Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility of unlabeled target
samples. In Algorithmic Learning Theory (ALT) , 2012.
Shai Ben-David and Ruth Urner. Domain adaptation–can quantity compensate for quality? Annals of
Mathematics and Artiﬁcial Intelligence , 2014.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain
adaptation. In Advances in Neural Information Processing Systems (NeurIPS) , 2006.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from diﬀerent domains. Machine learning , 2010.
Christopher Berlind and Ruth Urner. Active nearest neighbors in changing environments. In International
Conference on Machine Learning (ICML) , 2015.
Su Lin Blodgett, Lisa Green, and Brendan O’Connor. Demographic dialectal variation in social media: A
case study of african-american english. In Empirical Methods in Natural Language Processing (EMNLP) ,
2016.
Jonathon Byrd and Zachary Lipton. What is the eﬀect of importance weighting in deep learning? In
International Conference on Machine Learning (ICML) , 2019.
Clément Canonne. A short note on an inequality between KL and TV. arXiv preprint arXiv:2202.07198 ,
2022.
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with
label-distribution-aware margin loss. In Advances in Neural Information Processing Systems (NeurIPS) ,
2019.
Nitesh Chawla, Kevin Bowyer, Lawrence Hall, and Philip Kegelmeyer. Smote: Synthetic minority over-
sampling technique. Journal of Artiﬁcial Intelligence Research , 2002.
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on eﬀective
number of samples. In Computer Vision and Pattern Recognition (CVPR) , 2019.
Shai Ben David, Tyler Lu, Teresa Luu, and Dávid Pál. Impossibility theorems for domain adaptation. In
International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) , 2010.
11Published in Transactions on Machine Learning Research (06/2023)
Luc Devroye and László Györﬁ. Nonparametric density estimation: the L1view. Wiley Series in Probability
and Mathematical Statistics, 1985.
David Freedman and Persi Diaconis. On the histogram as a density estimator: L2theory.Zeitschrift für
Wahrscheinlichkeitstheorie und verwandte Gebiete , 1981.
Steve Hanneke and Samory Kpotufe. On the value of target data in transfer learning. In Advances in Neural
Information Processing Systems (NeurIPS) , 2019.
Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demo-
graphics in repeated loss minimization. In International Conference on Machine Learning (ICML) , 2018.
Dirk Hovy and Anders Søgaard. Tagging performance correlates with author age. In Association for Com-
putational Linguistics (ACL) , 2015.
Badr Youbi Idrissi, Martín Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing
achieves competitive worst-group-accuracy. In Causal Learning and Reasoning , 2022.
Parameswaran Kamalaruban and Robert Williamson. Minimax lower bounds for cost sensitive classiﬁcation.
arXiv preprint arXiv:1805.07723 , 2018.
Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos Thrampoulidis. Label-
imbalanced and group-sensitive classiﬁcation under overparameterization. In Advances in Neural Infor-
mation Processing Systems (NeurIPS) , 2021.
Samory Kpotufe and Guillaume Martinet. Marginal singularity, and the beneﬁts of labels in covariate-shift.
InConference On Learning Theory (COLT) , 2018.
Miroslav Kubat and Stan Matwin. Addressing the curse of imbalanced training sets: one-sided selection. In
International Conference on Machine Learning (ICML) , 1997.
Tian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith. Tilted empirical risk minimization. In
International Conference on Learning Representations (ICLR) , 2020.
Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black
box predictors. In International Conference on Machine Learning (ICML) , 2018.
Evan Liu, Behzad Haghgoo, Annie Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang,
and Chelsea Finn. Just train twice: Improving group robustness without training group information. In
International Conference on Machine Learning (ICML) , 2021.
Subha Maity, Yuekai Sun, and Moulinath Banerjee. Minimax optimal approaches to the label shift problem.
arXiv preprint arXiv:2003.10443 , 2020.
Pascal Massart and Élodie Nédélec. Risk bounds for statistical learning. Annals of Statistics , 2006.
Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv
Kumar. Long-tail learning via logit adjustment. In International Conference on Learning Representations
(ICLR), 2020.
Shiori Sagawa, Pang Wei Koh, Tatsunori Hashimoto, and Percy Liang. Distributionally robust neural
networks. In International Conference on Learning Representations (ICLR) , 2020.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood
function. Journal of Statistical Planning and Inference , 2000.
Rachael Tatman. Gender and dialect bias in youtube’s automatic captions. In ACL Workshop on Ethics in
Natural Language Processing , 2017.
Alexandre Tsybakov. Introduction to Nonparametric Estimation . Springer, 2010.
12Published in Transactions on Machine Learning Research (06/2023)
Martin Wainwright. High-dimensional statistics: A non-asymptotic viewpoint . Cambridge University Press,
2019.
ByronWallace, KevinSmall, CarlaBrodley, andThomasTrikalinos. Classimbalance, redux. In International
Conference on Data Mining (ICDM) , 2011.
Ke Alexander Wang, Niladri Chatterji, Saminul Haque, and Tatsunori Hashimoto. Is importance weighting
incompatible with interpolating classiﬁers? In International Conference on Learning Representations
(ICLR), 2022.
Larry Wasserman. Lecture notes in nonparametric classiﬁcation, 2019. URL https://www.stat.cmu.edu/
~larry/=sml/nonparclass.pdf . [Online; accessed 12-May-2022].
Wikipedia contributors. Poisson binomial distribution — Wikipedia, the free encyclopedia,
2022. URL https://en.wikipedia.org/w/index.php?title=Poisson_binomial_distribution&
oldid=1071847908 . [Online; accessed 5-May-2022].
Da Xu, Yuting Ye, and Chuanwei Ruan. Understanding the role of importance weighting for deep learning.
InInternational Conference on Learning Representations (ICLR) , 2020.
Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced learning. Advances
in Neural Information Processing Systems (NeurIPS) , 2020.
Han-Jia Ye, Hong-You Chen, De-Chuan Zhan, and Wei-Lun Chao. Identifying and compensating for feature
deviation in imbalanced deep learning. arXiv preprint arXiv:2001.01385 , 2020.
A Technical tools
In this section we avail ourselves of some technical tools that shall be used in all of the proofs below.
A.1 Reduction to lower bounds over a ﬁnite class
The lower bound on the minimax excess risk will be established via the usual route of ﬁrst identifying a
“hard” ﬁnite set of problem instances and then establishing the lower bound over this ﬁnite class. One
diﬀerence from the usual setup in proving such lower bounds (see Wainwright, 2019, Chapter 15) is that
the training samples are drawn from an imbalanced distribution, whereas the test samples are drawn from
a balanced one.
LetPbe a class of pairs of distributions, where each element (Pmaj,Pmin)∈Pis a pair of distributions
over [0,1]×{− 1,1}. As before, we let Ptestdenote the uniform mixture over Pmajand Pmin. We letV
denote a ﬁnite index set. Corresponding to each element v∈Vthere is a Pv= (Pv,maj,Pv,min)∈Pwith
Pv,test= (Pv,maj+Pv,min)/2. Finally, also deﬁne a pair of random variables (V,S)as follows:
1.Vis a uniform random variable over the set V.
2.(S|V=v)∼Pnmaj
v,maj×Pnmin
v,min, is an independent draw of nmajsamples from Pv,majandnminsamples
from Pv,min.
We shall let Qdenote the joint distribution of the random variables (V,S), and let QSdenote the marginal
distribution of S.
With this notation in place, we now present a lemma that lower bounds the minimax excess risk in terms of
quantities deﬁned over the ﬁnite class of “hard” instances Pv.
13Published in Transactions on Machine Learning Research (06/2023)
Lemma A.1. Let the random variables (V,S)be as deﬁned above. The minimax excess risk is lower bounded
as follows:
Minimax Excess Risk (P) = inf
Asup
(Pmaj,Pmin)∈PES∼Pnmaj
maj×Pnmin
min/bracketleftbig
R(AS;Ptest)−R(f⋆(Ptest);Ptest)/bracketrightbig
≥RV−BV,
where RVand Bayes-error BVare deﬁned as
RV:=ES∼QS[inf
hPr
(x,y)∼/summationtext
v∈VQ(v|S)Pv,test(h(x)/negationslash=y)],
BV:=EV[R(f⋆(PV,test);PV,test))].
Proof.By the deﬁnition of Minimax Excess Risk ,
Minimax Excess Risk = inf
Asup
(Pmaj,Pmin)∈PES∼Pnmaj
maj×Pnmin
min[R(AS;Ptest)]−R(f⋆(Ptest);Ptest)
≥inf
Asup
v∈VES|v∼Pnmaj
v,maj×Pnmin
v,min[R(AS;Pv,test)]−R(f⋆(Pv,test);Pv,test)
≥inf
AEV/bracketleftBig
ES|V∼Pnmaj
V,maj×Pnmin
V,min[R(AS;PV,test)]−R(f⋆(PV,test);PV,test))/bracketrightBig
= inf
AEV[ES|V∼Pnmaj
V,maj×Pnmin
V,min[R(AS;PV,test)]]−EV[R(f⋆(PV,test);PV,test))]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=BV.
We continue lower bounding the ﬁrst term as follows
inf
AEV[ES|V∼Pnmaj
V,maj×Pnmin
V,min[R(AS;PV,test)]] = inf
AE(V,S)∼Q[ Pr
(x,y)∼PV,test(AS(x)/negationslash=y)]
= inf
AES∼QSEV∼Q(·|S)[ Pr
(x,y)∼PV,test(AS(x)/negationslash=y)]
(i)
≥ES∼QS[inf
hEV∼Q(·|S)[ Pr
(x,y)∼PV,test(h(x)/negationslash=y)]]
=ES∼QS[inf
hPr
(x,y)∼/summationtext
v∈VQ(v|S)Pv,test(h(x)/negationslash=y)]
=RV,
where (i)follows sinceASis a ﬁxed classiﬁer given the sample set S. This, combined with the previous
equation block completes the proof.
A.2 The hat function and its properties
In this section, we deﬁne the hat function and establish some of its properties. This function will be useful
in deﬁning “hard” problem instances to prove our lower bounds. Given a positive integer Kthe hat function
is deﬁned as
φK(x) =

/vextendsingle/vextendsinglex+1
4K/vextendsingle/vextendsingle−1
4Kforx∈/bracketleftbig
−1
2K,0/bracketrightbig
,
1
4K−/vextendsingle/vextendsinglex−1
4K/vextendsingle/vextendsingleforx∈/bracketleftbig
0,1
2K/bracketrightbig
,
0 otherwise.(6)
WhenKis clear from context, we omit the subscript.
14Published in Transactions on Machine Learning Research (06/2023)
−0.2−0.1 0.0 0.1 0.2−0.050.000.05Hat Function ( f4)
Figure 3: The hat function with K= 4.
We ﬁrst notice that this function is 1-Lipschitz and odd, so
/integraldisplay 1
2K
−1
2KφK(x) dx= 0.
We also compute some other key quantities for φ.
Lemma A.2. For any positive integer K,
/integraldisplay 1
2K
−1
2K|φK(x)|dx=1
8K2.
Proof.We suppress Kin the notation. We have that,
/integraldisplay 1
2K
−1
2K|φ(x)|dx=/integraldisplay0
−1
2K/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
4K−/vextendsingle/vextendsingle/vextendsingle/vextendsinglex+1
4K/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingledx+/integraldisplay 1
2K
0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglex−1
4K/vextendsingle/vextendsingle/vextendsingle/vextendsingle−1
4K/vextendsingle/vextendsingle/vextendsingle/vextendsingledx.
The integrand/vextendsingle/vextendsingle1
4K−/vextendsingle/vextendsinglex+1
4K/vextendsingle/vextendsingle/vextendsingle/vextendsingleoverx∈/bracketleftbig
−1
2K,0/bracketrightbig
deﬁnes a triangle with base1
2Kand height1
4K, thus it
has area1
16K2. Therefore,
/integraldisplay0
−1
2K/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
4K−/vextendsingle/vextendsingle/vextendsingle/vextendsinglex+1
4K/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingledx=1
16K2.
The same holds for the second term. Thus, by adding them up we get that/integraltext1
2K
−1
2K|φ(x)|dx=1
8K2.
Lemma A.3. For any positive integer K,
/integraldisplay 1
K
0log/parenleftbigg1 +φK(x−1
2K)
1−φK(x−1
2K)/parenrightbigg/parenleftbigg
1 +φK/parenleftbigg
x−1
2K/parenrightbigg/parenrightbigg
dx≤1
3K3
and
/integraldisplay 1
K
0log/parenleftbigg1−φK(x−1
2K)
1 +φK(x−1
2K)/parenrightbigg/parenleftbigg
1−φK/parenleftbigg
x−1
2K/parenrightbigg/parenrightbigg
dx≤1
3K3.
15Published in Transactions on Machine Learning Research (06/2023)
Proof.Let us suppress Kin the notation. We prove the ﬁrst bound below and the second bound follows by
an identical argument. We have that
/integraldisplay 1
K
0log/parenleftbigg1 +φ(x−1
2K)
1−φ(x−1
2K)/parenrightbigg/parenleftbigg
1 +φ/parenleftbigg
x−1
2K/parenrightbigg/parenrightbigg
dx
=/integraldisplay 1
2K
−1
2Klog/parenleftbigg1 +φ(x)
1−φ(x)/parenrightbigg
(1 +φ(x)) dx
=/integraldisplay 1
2K
0log/parenleftbigg1 +φ(x)
1−φ(x)/parenrightbigg
(1 +φ(x)) dx+/integraldisplay0
−1
2Klog/parenleftbigg1 +φ(x)
1−φ(x)/parenrightbigg
(1 +φ(x)) dx
=/integraldisplay 1
2K
0log/parenleftbigg1 +φ(x)
1−φ(x)/parenrightbigg
(1 +φ(x)) dx−/integraldisplay0
1
2Klog/parenleftbigg1 +φ(−x)
1−φ(−x)/parenrightbigg
(1 +φ(−x)) dx
=/integraldisplay 1
2K
0log/parenleftbigg1 +φ(x)
1−φ(x)/parenrightbigg
(1 +φ(x)) dx+/integraldisplay 1
2K
0log/parenleftbigg1−φ(x)
1 +φ(x)/parenrightbigg
(1−φ(x)) dx,
where the last equality follows since φis an odd function. Now, we may collect the integrands to get that,
/integraldisplay 1
K
0log/parenleftbigg1 +φ(x−1
2K)
1−φ(x−1
2K)/parenrightbigg/parenleftbigg
1 +φ/parenleftbigg
x−1
2K/parenrightbigg/parenrightbigg
dx
= 2/integraldisplay 1
2K
0log/parenleftbigg1 +φ(x)
1−φ(x)/parenrightbigg
φ(x) dx
= 2/integraldisplay 1
2K
0log/parenleftbigg
1 +2φ(x)
1−φ(x)/parenrightbigg
φ(x) dx
≤2/integraldisplay 1
2K
02φ(x)2
1−φ(x)dx,
where the last inequality follows since log(1 +x)≤xfor allx. Now we observe that φ(x)≤x≤1
2for
x∈[0,1
2K], and in particular,1
1−φ(x)≤2. Thus,
/integraldisplay 1
K
0log/parenleftbigg1 +φ(x−1
2K)
1−φ(x−1
2K)/parenrightbigg/parenleftbigg
1 +φ/parenleftbigg
x−1
2K/parenrightbigg/parenrightbigg
dx
≤8/integraldisplay 1
2K
0φ(x)2dx
≤8/integraldisplay 1
2K
0x2dx
=1
3K3.
This proves the ﬁrst bound. The second bound follows analogously.
B Proofs in the label shift setting
Throughout this section we operate in the label shift setting (Section 3.2.1).
First, in Appendix B.1 through a sequence of lemmas we prove the minimax lower bound Theorem 4.1. Next,
in Appendix B.2 we prove Theorem 5.1 which is an upper bound on the excess risk of the undersampled
binning estimator (see Eq. (5)) with ⌈nmin⌉1/3bins by invoking previous results on nonparametric density
estimation (Freedman & Diaconis, 1981; Devroye & Györﬁ, 1985).
B.1 Proof of Theorem 4.1
In this section, we provide a proof of the minimax lower bound in the label shift setting.
16Published in Transactions on Machine Learning Research (06/2023)
Wewillproceedbyconstructingaclassofdistributionswheretheseparationbetweenanytwodistributionsin
the class is small enough such that it is hard to distinguish between them with ﬁnite minority class samples.
In particular, we split the interval [0,1]into sub-intervals and each class distribution on each sub-interval
either has slightly more probability mass on the left side of the sub-interval, on the right, or completely
uniform. Since the minority class sample size is limited, no classiﬁer will be able to tell which distribution
the minority class is generated from, and hence will suﬀer high excess risk.
We construct the “hard” set of distributions as follows. Fix Kto be an integer that will be speciﬁed in
the sequel as a function of nmin. Let the index set be V={−1,0,1}K×{− 1,0,1}K. Forv∈V, we will
letv1∈{− 1,0,1}Kbe the ﬁrst Kcoordinates and v−1∈{− 1,0,1}Kbe the last Kcoordinates. That is,
v= (v1,v−1).
For everyv∈Pwe shall deﬁne pair of class-conditional distributions Pv,1andPv,−1as follows: for x∈Ij=
[j−1
K,j
K],
Pv,1(x) = 1 +v1,jφ/parenleftbigg
x−j+ 1/2
K/parenrightbigg
Pv,−1(x) = 1 +v−1,jφ/parenleftbigg
x−j+ 1/2
K/parenrightbigg
,
whereφis deﬁned in Eq. 6. Notice that Pv,1only depends on v1while Pv,−1only depends on v−1. We
continue to deﬁne
Pv,maj(x,y) =Pv,1(x)1(y= 1)
Pv,min(x,y) =Pv,−1(x)1(y=−1),
and
Pv,test(x,y) =Pv,maj(x,y) +Pv,min(x,y)
2=Pv,1(x)1(y= 1) + Pv,−1(x)1(y=−1)
2.
Observe that in the test distribution it is equally likely for the label to be +1or−1.
Recall that as described in Section A.1, Vshall be a uniform random variable over VandS|V∼Pnmaj
v,maj×
Pnmin
v,min. We shall let Qdenote the joint distribution of (V,S)and let QSdenote the marginal over S.
With this construction in place, we ﬁrst show that the minimax excess risk is lower bounded as follows.
Lemma B.1. For any positive integers K,n maj,nmin, the minimax excess risk is lower bounded as follows:
Minimax Excess Risk (PLS)
= inf
Asup
(Pmaj,Pmin)∈P LSES∼Pnmaj
maj×Pnmin
min/bracketleftbig
R(AS;Ptest)−R(f⋆;Ptest)/bracketrightbig
≥1
36K−1
2ES∼QS/bracketleftBigg
TV/parenleftBigg/summationdisplay
v∈VQ(v|S)Pv,1,/summationdisplay
v∈VQ(v|S)Pv,−1/parenrightBigg/bracketrightBigg
. (7)
Proof.By invoking Lemma A.1 we get that
Minimax Excess Risk (PLS)
≥ES∼QS[inf
hPr
(x,y)∼/summationtext
v∈VQ(v|S)Pv,test(h(x)/negationslash=y)]
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
=:RV−EV[R(f⋆(PV,test);PV,test))]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:BV.
We proceed by calculating alternate expressions for RVandBVto get our desired lower bound on the
minimax excess risk.
17Published in Transactions on Machine Learning Research (06/2023)
Calculation of RV:Immediately by Le Cam’s lemma (Wainwright, 2019, Eq. 15.13), we get that
RV=ES∼QS/bracketleftBigg
inf
hPr
(x,y)∼/summationtext
v∈VQ(v|S)Pv,test(h(x)/negationslash=y)/bracketrightBigg
=1
2ES∼QS/bracketleftBigg
1−TV/parenleftBigg/summationdisplay
v∈VQ(v|S)Pv,1,/summationdisplay
v∈VQ(v|S)Pv,−1/parenrightBigg/bracketrightBigg
. (8)
Calculation of BV:Again by invoking Le Cam’s lemma (Wainwright, 2019, Eq. 15.13), we get that for
any class conditional distributions P1,P−1,
R(f⋆;Ptest) =1
2−1
2TV(P1,P−1).
So by taking expectations, we get that
BV=EV[R(f⋆(PV,test);PV,test)] =EV/bracketleftbigg1
2−1
2TV(PV,1,PV,−1)/bracketrightbigg
. (9)
We now compute EV[TV( PV,1,PV,−1)]as follows:
EV[TV( PV,1,PV,−1)] =1
2EV/bracketleftbigg/integraldisplay1
x=0|PV,1(x)−PV,−1(x)|dx/bracketrightbigg
=1
2EV
K/summationdisplay
j=1/integraldisplayj
K
j−1
K|V1,j−V−1,j|/vextendsingle/vextendsingle/vextendsingle/vextendsingleφ/parenleftbigg
x−j+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingledx

=1
2K/summationdisplay
j=1EV/bracketleftBigg/integraldisplayj
K
j−1
K|V1,j−V−1,j|/vextendsingle/vextendsingle/vextendsingle/vextendsingleφ/parenleftbigg
x−j+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingledx/bracketrightBigg
(i)=1
16K2K/summationdisplay
j=1EV[|V1,j−V−1,j|],
where (i)follows by Lemma A.2. Observe that V1,j,V−1,jare independent uniform random variables on
{−1,0,1}, it is therefore straightforward to compute that
EV[|V1,j−V−1,j|] =8
9.
This yields that
EV[TV( PV,1,PV,−1)] =1
18K.
Plugging this into Eq. (9) allows us to conclude that
BV=EV[R(f⋆(PV,test);PV,test)] =1
2/parenleftbigg
1−1
18K/parenrightbigg
. (10)
Combining Eqs. (8) and (10) establishes the claimed result.
In light of this previous lemma we now aim to upper bound the expected total variation distance in Eq. (7).
Lemma B.2. Suppose that vis drawn uniformly from the set {−1,1}K, and that S|vis drawn from
Pnmaj
v,maj×Pnmin
v,minthen,
ES/bracketleftBigg
TV/parenleftBigg/summationdisplay
v∈VQ(v|S)Pv,1,/summationdisplay
v∈VQ(v|S)Pv,−1/parenrightBigg/bracketrightBigg
≤1
18K−1
144Kexp/parenleftBig
−nmin
3K3/parenrightBig
.
18Published in Transactions on Machine Learning Research (06/2023)
Proof.Letψ:=ES/bracketleftbig
TV/parenleftbig/summationtext
v∈VQ(v|S)Pv,1,/summationtext
v∈VQ(v|S)Pv,−1/parenrightbig/bracketrightbig
. Then,
ψ=ES/bracketleftBigg
TV/parenleftBigg/summationdisplay
v∈VQ(v|S)Pv,1,/summationdisplay
v∈VQ(v|S)Pv,−1/parenrightBigg/bracketrightBigg
=1
2ES/bracketleftBigg/integraldisplay1
x=0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
v∈VQ(v|S) (Pv,1(x)−Pv,−1(x))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingledx/bracketrightBigg
=1
2ES
K/summationdisplay
j=1/integraldisplayj
K
x=j−1
K/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
v∈VQ(v|S) (Pv,1(x)−Pv,−1(x))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingledx

=1
2ES
K/summationdisplay
j=1/integraldisplayj
K
x=j−1
K/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
v∈VQ(v|S)(v1,j−v−1,j)φ/parenleftbigg
x−j+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingledx
,
where the last equality is by the deﬁnition of Pv,1andPv,−1. Continuing we get that,
ψ=1
2K/summationdisplay
j=1/bracketleftBigg/integraldisplayj
K
x=j−1
K/vextendsingle/vextendsingle/vextendsingle/vextendsingleφ/parenleftbigg
x−j+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingledx/bracketrightBigg
ES/bracketleftBigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
v∈VQ(v|S)(v1,j−v−1,j)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightBigg
(i)=1
16K2ES
K/summationdisplay
j=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
v∈VQ(v|S)(v1,j−v−1,j)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle

=1
16K2K/summationdisplay
j=1/integraldisplay/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
v∈VQ(v|S)(v1,j−v−1,j)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingledQS(S)
=1
16K2K/summationdisplay
j=1/integraldisplay/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
v∈VQ(v,S)(v1,j−v−1,j)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingledS
(ii)=1
16K2|V|K/summationdisplay
j=1/integraldisplay/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
v∈VQ(S|v)(v1,j−v−1,j)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingledS,
where (i)follows by the calculation in Lemma A.2 and (ii)follows since vis a uniform random variable over
the setV.
The distributions Pv,1andPv,−1are symmetrically deﬁned over all intervals Ij= [j−1
K,j
K], and hence all of
the summands in the RHS above are equal. Thus,
ψ=1
16K|V|/integraldisplay/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
v∈VQ(S|v)(v1,1−v−1,1)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingledS. (11)
Before we continue further, let us deﬁne
V+={v∈V|v1,1>v−1,1}.
19Published in Transactions on Machine Learning Research (06/2023)
For everyv∈V+, let ˜v∈Vbe such that is the same as von all coordinates, except ˜v1,1=−v1,1and
˜v−1,1=−v−1,1. Then continuing from Eq. (11) we ﬁnd that,
ψ(i)=1
16K|V|/integraldisplay/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
v∈V+(v1,1−v−1,1)(Q(S|v)−Q(S|˜v))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingledS
(ii)
≤1
16K|V|/integraldisplay/summationdisplay
v∈V+(v1,1−v−1,1)|Q(S|v)−Q(S|˜v)|dS
=1
16K|V|/summationdisplay
v∈V+(v1,1−v−1,1)/integraldisplay
|Q(S|v)−Q(S|˜v)|dS
=1
8K|V|/summationdisplay
v∈V+(v1,1−v−1,1)TV( Q(S|v),Q(S|˜v))
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
=:Ξ, (12)
where (i)we use the deﬁnition of V+and˜v,(ii)follows since v1,1>v−1,1forv∈V+.
Now we further partition V+into 3 setsV(1,0),V(0,−1),V(1,−1)as follows
V(1,0)={v∈V|v1,1= 1,v−1,1= 0},
V(0,−1)={v∈V|v1,1= 0,v−1,1=−1},
V(1,−1)={v∈V|v1,1= 1,v−1,1=−1}.
Note that Q(S|v) =Pnmaj
v,maj×Pnmin
v,min, and therefore
Ξ =/summationdisplay
v∈V+(v1,1−v−1,1)TV/parenleftbig
Pnmaj
v,maj×Pnmin
v,min,Pnmaj
˜v,maj×Pnmin
˜v,min/parenrightbig
(i)=/summationdisplay
v∈V(1,0)TV/parenleftbig
Pnmaj
v,maj×Pnmin
v,min,Pnmaj
˜v,maj×Pnmin
˜v,min/parenrightbig
+/summationdisplay
v∈V(0,−1)TV/parenleftbig
Pnmaj
v,maj×Pnmin
v,min,Pnmaj
˜v,maj×Pnmin
˜v,min/parenrightbig
+ 2/summationdisplay
v∈V(1,−1)TV/parenleftbig
Pnmaj
v,maj×Pnmin
v,min,Pnmaj
˜v,maj×Pnmin
˜v,min/parenrightbig
, (13)
where (i)follows since v1,v−1∈{− 1,0,1}Kand by the deﬁnition of the sets V(1,0),V(0,1)andV(1,−1).
Now by the Bretagnolle–Huber inequality (see Canonne, 2022, Corollary 4),
TV/parenleftbig
Pnmaj
v,maj×Pnmin
v,min,Pnmaj
˜v,maj×Pnmin
˜v,min/parenrightbig
= TV/parenleftbig
Pnmaj
˜v,maj×Pnmin
˜v,min,Pnmaj
v,maj×Pnmin
v,min/parenrightbig
≤1−1
2exp/parenleftbig
−KL/parenleftbig
Pnmaj
˜v,maj×Pnmin
˜v,min/bardblPnmaj
v,maj×Pnmin
v,min/parenrightbig/parenrightbig
,
where we ﬂip the arguments in the ﬁrst step for simplicity later.
Next, by the chain rule for KL-divergence, we have that
KL(Pnmaj
˜v,maj×Pnmin
˜v,min/bardblPnmaj
v,maj×Pnmin
v,min) =nmajKL(P˜v,maj/bardblPv,maj) +nminKL(P˜v,min/bardblPv,min).
Usingthese, letusupperboundtheﬁrstterminEq.(13)correspondingto v∈V(0,−1). Forv∈V(0,−1), notice
that KL(P˜v,maj/bardblPv,maj) = 0sincev1,j= ˜v1,jfor allj∈{1,...,K}. For the second term, KL(P˜v,min/bardblPv,min),
20Published in Transactions on Machine Learning Research (06/2023)
onlyv1,1and˜v1,1diﬀer, so
KL(P˜v,min/bardblPv,min) =/integraldisplay1
0Pv,−1(x) log/parenleftbiggPv,−1(x)
P˜v,−1(x)/parenrightbigg
dx
=/integraldisplay 1
K
0log/parenleftbigg1 +φK(x−1
2K)
1−φK(x−1
2K)/parenrightbigg/parenleftbigg
1 +φK/parenleftbigg
x−1
2K/parenrightbigg/parenrightbigg
dx
≤1
3K3,
where the last inequality is a result of the calculation in Lemma A.3.
Therefore, we get
/summationdisplay
v∈V(0,−1)TV/parenleftbig
Pnmaj
v,maj×Pnmin
v,min,Pnmaj
˜v,maj×Pnmin
˜v,min/parenrightbig
≤9K−1/parenleftbigg
1−1
2exp/parenleftBig
−nmin
3K3/parenrightBig/parenrightbigg
.
For the terms in Eq. (13) corresponding to V(0,−1),V(1,−1), we simply take the trivial bound to get
/summationdisplay
v∈V(0,−1)TV/parenleftbig
Pnmaj
v,maj×Pnmin
v,min,Pnmaj
˜v,maj×Pnmin
˜v,min/parenrightbig
≤9K−1,
/summationdisplay
v∈V(1,−1)TV/parenleftbig
Pnmaj
v,maj×Pnmin
v,min,Pnmaj
˜v,maj×Pnmin
˜v,min/parenrightbig
≤9K−1.
Plugging these bounds into Eq. (13) we get that,
Ξ≤4·9K−1−9K−1
2exp/parenleftBig
−nmin
3K3/parenrightBig
.
Now using this bound on Ξin Eq. (12) and observing that |V|= 9K, we get that,
ψ=ES/bracketleftBigg
TV/parenleftBigg/summationdisplay
v∈VQ(v|S)Pv,1,/summationdisplay
v∈VQ(v|S)Pv,−1/parenrightBigg/bracketrightBigg
≤1
8·9KK/parenleftbigg
4·9K−1−9K−1
2exp/parenleftBig
−nmin
3K3/parenrightBig/parenrightbigg
=1
18K−1
144Kexp/parenleftBig
−nmin
3K3/parenrightBig
,
completing the proof.
Finally, we combine Lemma B.1 and Lemma B.2 to establish the minimax lower bound in this label shift
setting. We recall the statement of the theorem here.
Theorem 4.1. Consider the label shift setting described in Section 3.2.1. Recall that PLSis the class of
pairs of distributions (Pmaj,Pmin)that satisfy the assumptions in that section. The minimax excess risk over
this class is lower bounded as follows:
Minimax Excess Risk (PLS) = inf
Asup
(Pmaj,Pmin)∈P LSExcess Risk [A; (Pmaj,Pmin)]≥1
6001
nmin1/3. (3)
Proof.By Lemma B.1 we know that,
Minimax Excess Risk (PLS)≥1
36K−1
2ES∼QS/bracketleftBigg
TV/parenleftBigg/summationdisplay
v∈VQ(v|S)Pv,1,/summationdisplay
v∈VQ(v|S)Pv,−1/parenrightBigg/bracketrightBigg
.
21Published in Transactions on Machine Learning Research (06/2023)
Next by the calculation in Lemma B.2 we have that
Minimax Excess Risk (PLS)≥1
36K−1
2/parenleftbigg1
18K−1
144Kexp/parenleftBig
−nmin
3K3/parenrightBig/parenrightbigg
=1
288Kexp/parenleftBig
−nmin
3K3/parenrightBig
.
SettingK=⌈nmin1/3⌉yields the following
Minimax Excess Risk (PLS)≥1
288⌈nmin1/3⌉exp/parenleftbigg
−nmin
3⌈nmin1/3⌉3/parenrightbigg
≥exp/parenleftBig
−nmin
3⌈nmin1/3⌉3/parenrightBig
288nmin1/3
⌈nmin1/3⌉1
nmin1/3
(i)
≥0.7 exp/parenleftbig
−1
3/parenrightbig
2881
nmin1/3
≥1
6001
nmin1/3,
where (i)follows since nmin1/3/⌈nmin1/3⌉≥0.7fornmin≥1.
B.2 Proof of Theorem 5.1
In this section, we derive an upper bound on the excess risk of the undersampled binning estimator AUSB
(Eq. (5)) in the label shift setting. Recall that given a dataset Sthis estimator ﬁrst calculates the under-
sampled dataset SUS, where the number of points from the minority group ( nmin) is equal to the number
of points from the majority group ( nmin), and the size of the dataset is 2nmin. Throughout this section,
(Pmaj,Pmin)shall be an arbitrary element of PLS.
To bound the excess risk of the undersampling algorithm, we will relate it to density estimation.
Recall that n1,jdenotes the number of points in SUSwith label +1that lie in Ij, andn−1,jis deﬁned
analogously.
Given a positive integer K, forx∈Ij= [j−1
K,j
K], by the deﬁnition of the undersampled binning estimator
(Eq. (5))
AS
USB(x) =/braceleftBigg
1ifn1,j>n−1,j,
−1otherwise.
Recall that since we have undersampled,/summationtext
jn1,j=/summationtext
jn−1,j=nmin. Therefore, deﬁne the simple histogram
estimators for P1(x) =P(x|y= 1)andP−1(x) =P(x|y=−1)as follows: for x∈Ij,
/hatwidePS
1(x) :=n1,j
Kn minand/hatwidePS
−1(x) :=n−1,j
Kn min.
With this histogram estimator in place, we may deﬁne an estimator for η(x) :=Ptest(y= 1|x)as follows,
/hatwideηS(x) :=/hatwidePS
1(x)
/hatwidePS
1(x) +/hatwidePS
−1(x).
Observe that, for x∈Ij
/hatwideηS(x)>1/2⇐⇒n1,j>n−1,j⇐⇒ AS
USB(x) = 1.
Deﬁning an estimator /hatwideηSfor the Ptest(y= 1|x)in this way will allow us to relate the excess risk of AUSBto
the estimation error in /hatwidePS
1and/hatwidePS
−1.
Before proving the theorem we restate it here.
22Published in Transactions on Machine Learning Research (06/2023)
Theorem 5.1. Consider the label shift setting described in Section 3.2.1. For any (Pmaj,Pmin)∈P LSthe
expected excess risk of the Undersampling Binning Estimator (Eq. (5)) with number of bins with K=
c⌈nmin1/3⌉is upper bounded by
Excess Risk [AUSB; (Pmaj,Pmin)] =ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
R(AS
USB;Ptest)−R(f⋆;Ptest)/bracketrightbig
≤C
nmin1/3.
Proof.By the deﬁnition of the excess risk
Excess Risk [AUSB; (Pmaj,Pmin)] :=ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
R(AS
USB;Ptest))−R(f⋆;Ptest)/bracketrightbig
.
By invoking (Wasserman, 2019, Theorem 1) we may upper bound the excess risk given a draw of Sby
R(AS
USB;Ptest))−R(f⋆;Ptest)≤2/integraldisplay/vextendsingle/vextendsingle/hatwideηS(x)−η(x)/vextendsingle/vextendsinglePtest(x) dx.
Continuing using the deﬁnition of /hatwideηSabove and because η=P1/(P1+P−1)we have that,
R(AS
USB;Ptest))−R(f⋆;Ptest)
= 2/integraldisplay1
0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/hatwidePS
1(x)
/hatwidePS
1(x) +/hatwidePS
−1(x)−P1(x)
P1(x) +P−1(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenleftbiggP1(x) +P−1(x)
2/parenrightbigg
dx
=/integraldisplay1
0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenleftBigg
P1(x) +P−1(x)
/hatwidePS
1(x) +/hatwidePS
−1(x)/parenrightBigg
/hatwidePS
1(x)−P1(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingledx
(i)
≤/integraldisplay1
0/vextendsingle/vextendsingle/vextendsingle/hatwidePS
1(x)−P1(x)/vextendsingle/vextendsingle/vextendsingledx+/integraldisplay1
0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleP1(x) +P−1(x)
/hatwidePS
1(x) +/hatwidePS
−1(x)−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/hatwidePS
1(x) dx
=/integraldisplay1
0/vextendsingle/vextendsingle/vextendsingle/hatwidePS
1(x)−P1(x)/vextendsingle/vextendsingle/vextendsingledx+/integraldisplay1
0/vextendsingle/vextendsingle/vextendsingle/hatwidePS
1(x) +/hatwidePS
−1(x)−P1(x)−P−1(x)/vextendsingle/vextendsingle/vextendsingle/hatwidePS
1(x)
/hatwidePS
1(x) +/hatwidePS
−1(x)dx
≤2/integraldisplay1
0/vextendsingle/vextendsingle/vextendsingle/hatwidePS
1(x)−P1(x)/vextendsingle/vextendsingle/vextendsingledx+/integraldisplay1
0/vextendsingle/vextendsingle/vextendsingle/hatwidePS
−1(x)−P−1(x)/vextendsingle/vextendsingle/vextendsingledx
(ii)
≤2/radicalBigg/integraldisplay1
0/parenleftBig
/hatwidePS
1(x)−P1(x)/parenrightBig2
dx+/radicalBigg/integraldisplay1
0/parenleftBig
/hatwidePS
−1(x)−P−1(x)/parenrightBig2
dx,
where (i)follows by the triangle inequality, (ii)is by the Cauchy–Schwarz inequality.
Taking expectation over the samples Sand by invoking Jensen’s inequality we ﬁnd that,
Excess Risk (AS; (Pmaj,Pmin))
=ES/bracketleftbig
R(AS
USB;Ptest))−R(f⋆;Ptest)/bracketrightbig
≤2/radicalBigg
ES/bracketleftbigg/integraldisplay/parenleftBig
/hatwidePS
1(x)−P1(x)/parenrightBig2
dx/bracketrightbigg
+/radicalBigg
ES/bracketleftbigg/integraldisplay/parenleftBig
/hatwidePS
−1(x)−P−1(x)/parenrightBig2
dx/bracketrightbigg
.
We note that /hatwidePS
jonly depends on nmini.i.d. draws from class j. Thus by (Freedman & Diaconis, 1981,
Theorem 1.7), if K=c⌈nmin⌉1/3then
ES/bracketleftbigg/integraldisplay/parenleftBig
/hatwidePS
j(x)−Pj(x)/parenrightBig2
dx/bracketrightbigg
≤C
nmin2/3.
Plugging this into the previous inequality yields the desired result.
23Published in Transactions on Machine Learning Research (06/2023)
C Proof in the group-covariate shift setting
Throughout this section we operate in the group-covariate shift setting (Section 3.2.2).
We will proceed similarly to Section B. We shall construct a family of class-conditional distributions such
that it will be necessary for adequate samples in each sub-interval of [0,1]to be able to learn the maximally
likely label in that sub-interval. On the other hand, we will construct the group-covariate distributions to be
separated from one another. As a consequence, sub-intervals with high probability mass under the minority
group distribution will have low probability mass under the majority group distribution. Hence, these sub-
intervals will not have enough training sample points for any classiﬁer to be able to learn the maximally
likely label and as a result shall suﬀer high excess risk.
First in Appendix C.1, we prove Theorem 4.2, the minimax lower bound through a sequence of lemmas.
Second in Appendix C.2, we prove Theorem 5.2 that upper bound on the excess risk of the undersampled
binning estimator with ⌈nmin⌉1/3bins.
C.1 Proof of Theorem 4.2
In this section, we provide a proof of the minimax lower bound in the group shift setting.
We construct the “hard” set of distributions as follows. Let the index set be V={−1,1}K. For every v∈V
deﬁne a distribution as follows: for x∈Ij= [j−1
K,j
K],
Pv(y= 1|x) :=1
2/bracketleftbigg
1 +vjφ/parenleftbigg
x−j+ 1/2
K/parenrightbigg/bracketrightbigg
,
whereφis deﬁned in Eq. 6. Given a τ∈[0,1]we also construct the group distributions as follows:
Pa(x) =/braceleftBigg
2−τifx∈[0,0.5)
τ ifx∈[0.5,1],
and let
Pb(x) = 2−Pa(x).
We can verify that
Overlap (Pa,Pb) = 1−TV(Pa,Pb) = 1−1
2/integraldisplay1
x=0|Pa(x)−Pb(x)|dx=τ.
We continue to deﬁne
Pv,maj(x,y) =Pv(y|x)Pa(x)
Pv,min(x,y) =Pv(y|x)Pb(x),
and
Pv,test(x,y) =Pv(y|x)/parenleftbiggPa(x) +Pb(x)
2/parenrightbigg
.
Observe that (Pa(x) +Pb(x))/2 = 1, the uniform distribution over [0,1].
Recall that as described in Section A.1, Vshall be a uniform random variable over VandS|V∼Pnmaj
v,maj×
Pnmin
v,min. We shall let Qdenote the joint distribution of (V,S)and let QSdenote the marginal over S.
With this construction in place, we present the following lemma that lower bounds the minimax excess risk
by a sum of exp(−KL(Q(S|vj= 1)/bardblQ(S|vj=−1))over the intervals. Intuitively, KL(Q(S|vj= 1)/bardblQ(S|
vj=−1)is a measure of how diﬃcult it is to identify whether vj= 1orvj=−1from the samples.
24Published in Transactions on Machine Learning Research (06/2023)
Lemma C.1. For any positive integers K,n maj,nminandτ∈[0,1], the minimax excess risk is lower bounded
as follows:
Minimax Excess Risk (PGS(τ)) = inf
Asup
(Pmaj,Pmin)∈P GS(τ)ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
R(AS;Ptest)−R(f⋆;Ptest)/bracketrightbig
≥1
32K2K/summationdisplay
j=1exp(−KL(Q(S|vj= 1)/bardblQ(S|vj=−1))).
Proof.By invoking Lemma A.1, we know that the minimax excess risk is lower bounded by
Minimax Excess Risk (PGS(τ))
≥ES∼QS[inf
hPr
(x,y)∼/summationtext
v∈VQ(v|S)Pv,test(h(x)/negationslash=y)]
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
=RV−EV[R(f⋆(PV,test);PV,test)]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=BV,
whereVis a uniform random variable over the set V,S|V=vis a draw from Pnmaj
v,maj×Pnmin
v,min, and Qdenotes
the joint distribution over (V,S).
We shall lower bound this minimax risk in parts. First, we shall establish a lower bound on RV, and then
an upper bound on the Bayes risk BV.
Lower bound on RV.Unpacking RVusing its deﬁnition we get that,
RV=ES∼QS[inf
hPr
(x,y)∼/summationtext
v∈VQ(v|S)Pv,test(h(x)/negationslash=y)]
=ES∼QS/bracketleftBigg
inf
h/integraldisplay1
0Ptest(x) Pr
y∼/summationtext
v∈VQ(v|S)Pv(·|x)[h(x)/negationslash=y] dx/bracketrightBigg
(i)=ES∼QS/bracketleftBigg/integraldisplay1
0Ptest(x) min/braceleftBigg/summationdisplay
v∈VQ(v|S)Pv(1|x),/summationdisplay
v∈VQ(v|S)Pv(−1|x)/bracerightBigg
dx/bracketrightBigg
(ii)=1
2−ES∼QS/bracketleftBigg/integraldisplay1
0Ptest(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
2−/summationdisplay
v∈VQ(v|S)Pv(1|x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingledx/bracketrightBigg
(iii)=1
2−/integraldisplay1
0Ptest(x)ES∼QS/bracketleftBigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
2−/summationdisplay
v∈VQ(v|S)Pv(1|x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightBigg
dx, (14)
where (i)followsbytaking htobethepointwiseminimizerover x,(ii)followssince Pv(−1|x) = 1−Pv(1|x)
andmin{s,1−s}= (1−|1−2s|)/2for alls∈[0,1], and (iii)follows by Fubini’s theorem which allows us
to switch the order of the integrals.
Ifx∈Ij= [j−1
K,j
K]for somej∈{1,...,K}we letjxdenote the value of this index j. With this notation
in place let us continue to upper bound integrand in the second term in the RHS above as follows:
ES∼QS/bracketleftBigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
2−/summationdisplay
v∈VQ(v|S)Pv(1|x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightBigg
(i)=ES∼QS/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleφ/parenleftbigg
x−jx+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle|Q(vjx= 1|S)−Q(vjx=−1|S)|/bracketrightbigg
=/vextendsingle/vextendsingle/vextendsingle/vextendsingleφ/parenleftbigg
x−jx+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleES∼QS[|Q(vjx= 1|S)−Q(vjx=−1|S)|]
(ii)=/vextendsingle/vextendsingle/vextendsingle/vextendsingleφ/parenleftbigg
x−jx+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleES∼QS/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleQ(S|vjx= 1)QV(vjx= 1)
QS(S)−Q(S|vjx=−1)QV(vjx=−1)
QS(S)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightbigg
(iii)=1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingleφ/parenleftbigg
x−jx+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleTV(Q(S|vjx= 1),Q(S|vjx=−1)), (15)
25Published in Transactions on Machine Learning Research (06/2023)
where (i)follows since Pv(1|x) = (1 +vjxφ(x−(jx+ 1/2)/K))/2and by marginalizing Q(v|S)over the
indicesj/negationslash=jx,(ii)follows by using Bayes’ rule and (iii)follows since the total-variation distance is half the
/lscript1distance. Now by the Bretagnolle–Huber inequality (see Canonne, 2022, Corollary 4) we get that,
TV(Q(S|vjx= 1),Q(S|vjx=−1))
≤1−exp(−KL(Q(S|vjx= 1)/bardblQ(S|vjx=−1)))
2. (16)
Combining Eqs. (14)-(16) we get that
RV
≥1
2−1
2/integraldisplay1
0Ptest(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingleφ/parenleftbigg
x−jx+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingledx
+1
4/integraldisplay1
0Ptest(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingleφ/parenleftbigg
x−jx+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleexp(−KL(Q(S|vjx= 1)/bardblQ(S|vjx=−1))) dx. (17)
Upper bound on BV:The Bayes error is
BV=EV[R(f⋆(PV);PV)]
=EV/bracketleftbigg
inf
fE(x,y)∼Pv,test1(f(x)/negationslash=y)/bracketrightbigg
=EV
inf
f/integraldisplay1
x=0/summationdisplay
y∈{−1,1}Ptest(x)PV,test(y|x)1(f(x) =−y)

=EV/bracketleftbigg/integraldisplay1
x=0Ptest(x) min
y∈{−1,1}PV,test(y|x)/bracketrightbigg
(i)=EV/bracketleftbigg1
2/parenleftbigg
1−/integraldisplay1
x=0Ptest(x)|PV,test(1|x)−PV,test(−1|x)|dx/parenrightbigg/bracketrightbigg
(ii)=EV/bracketleftbigg1
2/parenleftbigg
1−/integraldisplay1
x=0Ptest(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingleφ/parenleftbigg
x−jx+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingledx/parenrightbigg/bracketrightbigg
=1
2−1
2/integraldisplay1
x=0Ptest(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingleφ/parenleftbigg
x−jx+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingledx, (18)
where (i)follows since Pv(1|x) = 1−Pv(−1|x)andmin{s,1−s}= (1−|1−2s|)/2for alls∈[0,1], and
(ii)follows by our construction of Pvabove along with the fact that Pv(1|x) = 1−Pv(−1|x).
Putting things together: Combining Eqs. (17) and (18) allows us to conclude that
Minimax Excess Risk (PGS(τ))
≥1
4/integraldisplay1
0Ptest(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingleφ/parenleftbigg
x−jx+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleexp(−KL(Q(S|vjx= 1)/bardblQ(S|vjx=−1))) dx
=1
4K/summationdisplay
j=1/integraldisplayj
K
j−1
KPtest(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingleφ/parenleftbigg
x−j+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleexp(−KL(Q(S|vj= 1)/bardblQ(S|vj=−1))) dx
=1
4K/summationdisplay
j=1exp(−KL(Q(S|vj= 1)/bardblQ(S|vj=−1)))/bracketleftBigg/integraldisplayj
K
j−1
KPtest(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingleφ/parenleftbigg
x−j+ 1/2
K/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingledx/bracketrightBigg
(i)=1
32K2K/summationdisplay
j=1exp(−KL(Q(S|vj= 1)/bardblQ(S|vj=−1))),
where (i)follows by using Lemma A.2 along with the fact that Ptest(x) = 1in our construction to show that
the integral in the square brackets is equal to 1/8K2. This proves the result.
26Published in Transactions on Machine Learning Research (06/2023)
The next lemma upper bounds the KL divergence between Q(S|vj= 1)and Q(S|vj=−1)for each
j∈{1,...,K}. It shows that the KL divergence between these two posteriors is larger when the expected
number of samples in that bin is larger.
Lemma C.2. Suppose that vis drawn uniformly from the set {−1,1}K, and that S|vis drawn from
Pnmaj
v,maj×Pnmin
v,min. Then for any j∈{1,...,K/ 2}and anyτ∈[0,1],
KL(Q(S|vj= 1)/bardblQ(S|vj=−1))≤nmaj(2−τ) +nminτ
3K3,
and for any j∈{K/2 + 1,...,K}
KL(Q(S|vj= 1)/bardblQ(S|vj=−1))≤nmajτ+nmin(2−τ)
3K3.
Proof.Let us consider the case when j= 1. The bound for all other j∈{2,...,K}shall follow analogously.
Given samples S, letS= (S1,¯S1)be a partition where S1are the samples that fall in the interval I1, and ¯S1
be the other samples. Similarly, given a vector v∈{− 1,1}, letv= (v1,¯v1), wherev1is the ﬁrst component
and¯v1denotes the other components ( 2,...,K) ofv.
First, we will show that
Q(S|v1) =Q(S1|v1)Q(¯S1).
To see this, observe that
Q(S|v1) =Q((S1,¯S1)|v1) =Q(S1|v1)Q(¯S1|v1,S1).
Further, ifvis chosen uniformly over the hypercube {−1,1}K, then
Q(¯S1|v1,S1) =/summationdisplay
¯v1Q(¯S1,¯v1|v1,S1)
=/summationdisplay
¯v1Q(¯S1|v1,¯v1,S1)Q(¯v1|v1,S1)
(i)=/summationdisplay
¯v1Q(¯S1|v1,¯v1,S1)Q(¯v1)
(ii)=/summationdisplay
¯v1Q(¯S1|v1,¯v1)Q(¯v1)
(iii)=/summationdisplay
¯v1Q(¯S1|¯v1)Q(¯v1)
=Q(¯S1),
where (i)follows since by Bayes’ rule
Q(¯v1|v1,S1) =Q(¯v1|v1)Q(S1|v1,¯v1)
Q(S1|v1)
=Q(¯v1)Q(S1|v1,¯v1)
Q(S1|v1)(since ¯v1is independent of v1)
=Q(¯v1)Q(S1|v1)
Q(S1|v1)=Q(¯v1) (the samples in S1depend only on v1).
Inequality (ii)follows since the samples are drawn independently given v= (v1,¯v1). Finally, (iii)follows
since ¯S1(the samples that lie outside the interval I1) only depend on ¯v1since the marginal distribution of x
is independent of vand the distribution of y|xdepends only on the value of vcorresponding to the interval
in whichxlies.
27Published in Transactions on Machine Learning Research (06/2023)
Thus since, Q(S|v1) =Q(S1|v1)Q(¯S1)we have that
KL(Q(S|v1= 1)/bardblQ(S|v1=−1)) = KL( Q(S1|v1= 1)/bardblQ(S1|v1=−1)). (19)
To bound this KL divergence, let us condition of the number of samples in S1from group a, (the majority
group)n1,aand the number of samples from group b(the minority group), n1,b. Now since n1,aandn1,bare
independent of v1(which only aﬀects the labels) we have that,
Q(S1|v1) =/summationdisplay
n1,a,n1,bQ(n1,a,n1,b|v1)Q(S1|v1,n1,a,n1,b)
=/summationdisplay
n1,a,n1,bQ(n1,a,n1,b)Q(S1|v1,n1,a,n1,b)
=En1,a,n1,b[Q(S1|v1,n1,a,n1,b)].
Therefore, by the joint convexity of the KL-divergence and by Jensen’s inequality we have that,
KL(Q(S1|v1= 1)/bardblQ(S1|v1=−1))
≤En1,a,n1,b[KL( Q(S1|v1= 1,n1,a,n1,b)/bardblQ(S1|v1=−1,n1,a,n1,b))]. (20)
Now conditioned on v1,n1,aandn1,b, samples in S1are composed of 2 groups of samples (S1,a,S1,b). The
samples in each group (S1,a,S1,b)are drawn independently from the distributions Pa(x|x∈I1)Pv(y|x)
andPb(x|x∈I1)Pv(y|x)respectively. Therefore,
KL(Q(S1|v1= 1,n1,a,n1,b)/bardblQ(S1|v1=−1,n1,a,n1,b))
(i)=n1,aKL(Pa(x|x∈I1)Pv1=1(y|x)/bardblPa(x|x∈I1)Pv1=−1(y|x))
+n1,bKL(Pb(x|x∈I1)Pv1=1(y|x)/bardblPb(x|x∈I1)Pv1=−1(y|x))
(ii)= (n1,a+n1,b)Ex∼Unif(I1)[KL( Pv1=1(y|x)/bardblPv1=−1(y|x))]
(iii)=n1,a+n1,b
2Ex∼Unif(I1)
/summationdisplay
y∈{−1,1}/parenleftbigg
1 +yφ/parenleftbigg
x−1
2K/parenrightbigg/parenrightbigg
log/parenleftBigg/parenleftbig
1 +yφ/parenleftbig
x−1
2K/parenrightbig/parenrightbig
/parenleftbig
1 +yφ/parenleftbig
x−1
2K/parenrightbig/parenrightbig/parenrightBigg

=n1,a+n1,b
2/summationdisplay
y∈{−1,1}Ex∼Unif(I1)/bracketleftBigg/parenleftbigg
1 +yφ/parenleftbigg
x−1
2K/parenrightbigg/parenrightbigg
log/parenleftBigg/parenleftbig
1 +yφ/parenleftbig
x−1
2K/parenrightbig/parenrightbig
/parenleftbig
1 +yφ/parenleftbig
x−1
2K/parenrightbig/parenrightbig/parenrightBigg/bracketrightBigg
=n1,a+n1,b
2K/summationdisplay
y∈{−1,1}/integraldisplay 1
K
x=0/bracketleftBigg/parenleftbigg
1 +yφ/parenleftbigg
x−1
2K/parenrightbigg/parenrightbigg
log/parenleftBigg/parenleftbig
1 +yφ/parenleftbig
x−1
2K/parenrightbig/parenrightbig
/parenleftbig
1 +yφ/parenleftbig
x−1
2K/parenrightbig/parenrightbig/parenrightBigg/bracketrightBigg
dx
(iv)
≤n1,a+n1,b
3K2, (21)
where in (i)we let Pv1denote the conditional distribution of yforx∈I1givenv1,(ii)follows since both
PaandPbare constant in the interval, (iii)follows by our construction of Pvabove, and ﬁnally (iv)follows
by invoking Lemma A.3 that ensures that the integral is bounded by 1/3K2.
Using this bound in Eq. (20), along with Eq. (19) we get that
KL(Q(S|v1= 1)/bardblQ(S|v1=−1))≤E[n1,a+n2,b]
3K2.
Now there are nmajsamples from group ainSandnminsamples from group b. Therefore,
E[n1,a] =nmajPa(x∈I1) =nmaj(2−τ)
K,
E[n1,b] =nminPb(x∈I1) =nminτ
K.
28Published in Transactions on Machine Learning Research (06/2023)
Plugging this bound into Eq. (21) completes the proof by the ﬁrst interval. An identical argument holds for
j∈{2,...,K/ 2}. Forj∈{K/2 + 1,...,K}the only change is that
E[nj,a] =nmajPa(x∈Ij) =nmajτ
K,
E[nj,b] =nminPb(x∈Ij) =nmin(2−τ)
K.
Next, we combine the previous two lemmas to establish our stated lower bound. We ﬁrst restate it here.
Theorem 4.2. Consider the group shift setting described in Section 3.2.2. Given any overlap τ∈[0,1]
recall thatPGS(τ)is the class of distributions such that Overlap (Pmaj,Pmin)≥τ. The minimax excess risk in
this setting is lower bounded as follows:
Minimax Excess Risk (PGS(τ)) = inf
Asup
(Pmaj,Pmin)∈P GS(τ)Excess Risk [A; (Pmaj,Pmin)]
≥1
200(nmin·(2−τ) +nmaj·τ)1/3≥1
200nmin1/3(ρ·τ+ 2)1/3, (4)
whereρ=nmaj/nmin>1.
Proof.First, by Lemma C.1 we know that
Minimax Excess Risk (PGS(τ))≥1
32K2K/summationdisplay
j=1exp(−KL(Q(S|vj= 1)/bardblQ(S|vj=−1))).
Next, by invoking the bound on the KL divergences in the equation above by Lemma C.2 we get that
Minimax Excess Risk (PGS(τ))
≥1
64K/bracketleftbigg
exp/parenleftbigg
−nmaj(2−τ) +nminτ
3K3/parenrightbigg
+ exp/parenleftbigg
−nmin(2−τ) +nmajτ
3K3/parenrightbigg/bracketrightbigg
≥1
64K/bracketleftbigg
exp/parenleftbigg
−nmin(2−τ) +nmajτ
3K3/parenrightbigg/bracketrightbigg
SettingK=⌈(nmin(2−τ) +nmajτ)1/3⌉and recalling that τ≤1we get that
Minimax Excess Risk (PGS(τ))
≥1
64⌈(nmin(2−τ) +nmajτ)1/3⌉/bracketleftbigg
exp/parenleftbigg
−nmin(2−τ) +nmajτ
3⌈(nmin(2−τ) +nmajτ)1/3⌉3/parenrightbigg/bracketrightbigg
(i)
≥exp(−1/3)
64(nmin(2−τ) +nmajτ)1/3
⌈(nmin(2−τ) +nmajτ)1/3⌉1
(nmin(2−τ) +nmajτ)1/3
(ii)
≥0.7 exp(−1/3)
641
(nmin(2−τ) +nmajτ)1/3
≥1
2001
(nmin(2−τ) +nmajτ)1/3,
where (i)follows since nmin(2−τ) +nmajτ/⌈(nmin(2−τ) +nmajτ)1/3⌉3≤1, and (ii)follows since 0≤τ≤1
andnmin≥1and hence(nmin(2−τ)+nmajτ)1/3
⌈(nmin(2−τ)+nmajτ)1/3⌉≥0.7.
29Published in Transactions on Machine Learning Research (06/2023)
C.2 Proof of Theorem 5.2
In this section, we derive an upper bound on the excess risk of the undersampled binning estimator AUSB
(Eq. (5)). Recall that given a dataset Sthis estimator ﬁrst calculates the undersampled dataset SUS, where
the number of points from the minority group ( nmin) is equal to the number of points from the majority group
(nmin), and the size of the dataset is 2nmin. Throughout this section, (Pmaj,Pmin)shall be an arbitrary element
ofPGS(τ)for anyτ∈[0,1]. In this section, whenever we shall often denote Excess Risk (A; (Pmaj,Pmin))by
simply Excess Risk (A).
Before we proceed, we introduce some additional notation. For any j∈{1,...,K}andIj= [j−1
K,j
K]let
qj,1:=Ptest(y= 1|x∈Ij) =/integraldisplay
x∈IjP(y= 1|x)Ptest(x|x∈Ij) dx, (22a)
qj,1:=Ptest(y= 1|x∈Ij) =/integraldisplay
x∈IjP(y= 1|x)Ptest(x|x∈Ij) dx. (22b)
For the undersampled binning estimator AUSB(deﬁned above in Eq. (5)), deﬁne the excess risk in an interval
Ijas follows:
Rj(AS
USB) :=p/parenleftbig
y=−AS
j|x∈Ij/parenrightbig
−min{Ptest(y= 1|x∈Ij),Ptest(y=−1|x∈Ij)}
=qj,−AS
j−min{qj,1,qj,−1}.
The proof of the upper bound shall proceed in steps. First, in Lemma C.3 we will show that the excess risk
is equal to sum the excess risk over the intervals up to a factor of 2/Kon account of the distribution being
1-Lipschitz. Next, in Lemma C.4 we upper bound the risk over each interval. We put these two together
and to upper bound the risk.
Lemma C.3. The expected excess risk of undersampled binning estimator AUSBcan be decomposed as follows
Excess Risk (AUSB)≤K−1/summationdisplay
j=0ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
Rj(AS
USB)/bracketrightbig
·Ptest(Ij) +2
K,
where Ptest(Ij) :=/integraltext
x∈IjPtest(x) dx.
Proof.Recall that by deﬁnition, the expected excess risk is
ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
R(AS;Ptest)−R(f⋆;Ptest)/bracketrightbig
.
Let us ﬁrst decompose the Bayes risk R(f⋆),
R(f⋆) = inf
fE(x,y)∼Ptest[1(f(x)/negationslash=y)]
= inf
f/integraldisplay1
x=0/summationdisplay
y∈{−1,1}1(f(x)/negationslash=y)Ptest(y|x)Ptest(x) dx
=/integraldisplay1
x=0inf
f(x)∈{−1,1}/summationdisplay
y∈{−1,1}1(f(x)/negationslash=y)Ptest(y|x)Ptest(x) dx
=/integraldisplay1
x=0inf
f(x)∈{−1,1}Ptest(y=−f(x)|x)Ptest(x) dx
=/integraldisplay1
x=0min{Ptest(y= 1|x),Ptest(y=−1|x)}Ptest(x) dx. (23)
30Published in Transactions on Machine Learning Research (06/2023)
The risk of the undersampled binning algorithm AUSBis given by
R(AS
USB) =/integraldisplay1
x=0/summationdisplay
y∈{−1,1}1(AS
USB(x)/negationslash=y)Ptest(y|x)Ptest(x) dx
=/integraldisplay1
x=0Ptest(y=−AS
USB(x)|x)Ptest(x) dx.
Next, recall that the undersampled binning estimator is constant over the intervals Ijforj∈{1,...,K}
where it takes the value AS
j(to ease notation let us simply denote it by Ajbelow), and therefore
R(AS
USB) =K−1/summationdisplay
j=0/integraldisplay
x∈IjPtest(y=−Aj|x)Ptest(x) dx.
This combined with Eq. (23) tells us that
R(AS
USB)−R(f⋆)
=K−1/summationdisplay
j=0/integraldisplay
x∈Ij/parenleftbig
Ptest(y=−Aj|x)−min{Ptest(y= 1|x),Ptest(y=−1|x)}/parenrightbig
Ptest(x) dx. (24)
Recall the deﬁnition of qj,1andqj,−1from Eqs. (22a)-(22b) above. For any x∈Ij= [j−1
K,j
K],|Ptest(y|
x)−qj,y|≤1/K, since the distribution Ptest(y|x)is1-Lipschitz and qj,yis its conditional mean. Therefore,
R(AS
USB)−R(f⋆)
≤K−1/summationdisplay
j=0/integraldisplay
x∈Ij/parenleftbig
qj,−Aj−min{qj,1,qj,−1}/parenrightbig
Ptest(x) dx+2
KK−1/summationdisplay
j=0/integraldisplay
x∈IjPtest(x) dx
=K−1/summationdisplay
j=0/integraldisplay
x∈IjRj(AS
USB)Ptest(x) dx+2
K.
Taking expectation over the training samples S(wherenminsamples are drawn independently from Pminand
nmajsamples are drawn independently from Pmaj) concludes the proof.
Next we provide an upper bound on the expected excess risk is an interval Rj(AS
USB).
Lemma C.4. For anyj∈{1,...,K}withIj= [j−1
K,j
K],
ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
Rj(AS
USB)/bracketrightbig
≤c/radicalbig
nminPtest(Ij)+c
K,
wherecis an absolute constant, and Ptest(Ij) :=/integraltext
x∈IjPtest(x) dx.
Proof.Consider an arbitrary bucket j∈{1,...,K}.
Let us introduce some notation that shall be useful in the remainder of the proof. Analogous to qj,1and
qj,−1deﬁned above (see Eqs. (22a)-(22b)), deﬁne qa
j,1andqb
j,1as follows:
qa
j,1:=Pa(y= 1|x∈Ij) =/integraldisplay
x∈IjP(y= 1|x)Pa(x|x∈Ij) dx, (25a)
qb
j,1:=Pb(y= 1|x∈Ij) =/integraldisplay
x∈IjP(y= 1|x)Pb(x|x∈Ij) dx. (25b)
Essentially, qa
j,1is the probability that a sample is from group aand has label 1, conditioned on the event
that the sample falls in the interval Ij. Since
Ptest(x|x∈Ij) =1
2[Pa(x|x∈Ij) +Pb(x|x∈Ij)],
31Published in Transactions on Machine Learning Research (06/2023)
therefore
|qj,1−qa
j,1|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
x∈IjP(y= 1|x)Ptest(x|x∈Ij) dx−/integraldisplay
x∈IjP(y= 1|x)Pa(x|x∈Ij) dx/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
K. (26)
This follows since P(y|x)is1-Lipschitz and therefore can ﬂuctuate by at most 1/Kin the interval Ij. Of
course the same bound also holds for |qj,1−qb
j,1|.
With this notation in place let us present a bound on the expected value of Rj(AS
USB). By deﬁnition
Rj(AS
USB) =qj,−AS
j−min{qj,1,qj,−1}.
First, note that qj,1:=Ptest(y= 1|x∈Ij) = 1−qj,−1. Suppose that qj,1<1/2and therefore qj,−1>1/2
(the same bound shall hold in the other case). In this case, risk is incurred only when AS
j= 1. That is,
ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
Rj(AS
USB)/bracketrightbig
=|qj,−1−qj,1|Pr
S[AS
j= 1]
=|1−2qj,1|Pr
S[AS
j= 1]. (27)
Now by the deﬁnition of the undersampled binning estimator (see Eq. (5)), AS
j= 1only when there are more
samples in the interval Ijwith label 1than−1. However, we can bound the probability of this happening
sinceqj,1is smaller than qj,−1.
Letnjbe the number of samples in the undersampled sample set SUSin the interval Ij. Letn1,jbe the
number of these samples with label 1, andn−1,j=nj−n1,jbe the number of samples with label −1.
Further, let na,jbe the number of samples in from group asuch that they fall in the interval Ij, and deﬁne
mb,janalogously.
The probability of incurring risk is given by
P[Aj= 1] =2nmin/summationdisplay
s=1P[Aj= 1|nj=s]P[nj=s], (28)
where the sum is up to 2nminsince the size of the undersample dataset |SUS|is equal to 2nmin.
Conditioned on the event that nj=sthe probability of incurring risk is
P[Aj= 1|nj=s] =P[m1,j>n−1,j|nj=s] =P[n1,j>nj/2|nj=s]
=P[n1,j>s/2|nj=s]. (29)
Now, note that nj=na,j+nb,j. Thus continuing, we have that
P[n1,j>s/2|nj=s] =/summationdisplay
s/prime≤sP[n1,j>s/2|nj=s,nb,j=s/prime]P[nb,j=s/prime]
=/summationdisplay
s/prime≤sP[n1,j>s/2|na,j=s−s/prime,nb,j=s/prime]P[nb,j=s/prime].
In light of this previous equation, we want to control the probability that the number of samples with label
1in the interval Ijconditioned on the event that the number of samples from group ain this interval is
s−s/primeand the number of samples from group bin this interval is s/prime. Recall that qa
j,1andqb
j,1the probabilities
of the label of the sample being 1conditioned the event that sample is in the interval Ijwhen it is group a
andbrespectively. So we deﬁne the random variables:
za[s−s/prime]∼Bin(s−s/prime,qa
j,1), zb[s/prime]∼Bin(s/prime,qb
j,1), z[s]∼Bin(s,max/braceleftbig
qa
j,1,qb
j,1/bracerightbig
).
32Published in Transactions on Machine Learning Research (06/2023)
Then,
P[n1,j>s/2|nj=s]
=/summationdisplay
s/prime≤sP[n1,j>s/2|nj,a=s−s/prime,nj,b=s/prime]P[nj,b=s/prime]
=/summationdisplay
s/prime≤sP[za[s−s/prime] +zb[s/prime])>s/2|na,j=s−s/prime,nb,j=s/prime]P[nb,j=s/prime]
≤/summationdisplay
s/prime≤sP[z[s]>s/2|na,j=s−s/prime,nb,j=s/prime]P[nb,j=s/prime]
=/summationdisplay
s/prime≤sP[z[s]>s/2]P[nb,j=s/prime]
=P[z[s]>s/2]
(i)
≤exp/parenleftBig
−s
2(1−2 max/braceleftbig
qa
j,1,qb
j,1/bracerightbig
)2/parenrightBig
, (30)
where (i)follows by invoking Hoeﬀding’s inequality(Wainwright, 2019, Proposition 2.5). Combining this
with Eqs. (28) and (29) we get that
P[Aj= 1]≤2nmin/summationdisplay
s=1exp/parenleftBig
−s
2(1−2 max/braceleftbig
qa
j,1,qb
j,1/bracerightbig
)2/parenrightBig
P[nj=s].
Nownj, which is the number of samples that lands in the interval Ijis equal to na,j+nb,j. Now each
ofna,jandnb,j(the number of samples in this interval from each of the groups) are random variables
with distributions Bin(nmin,Pa(Ij))and Bin(nmin,Pb(Ij)), where Pa(Ij) =/integraltext
x∈IjPa(x) dxand Pb(Ij) =/integraltext
x∈IjPa(x) dx. Therefore, njis distributed as a sum of two binomial distribution and is therefore Poisson
binomiallydistributed(Wikipediacontributors,2022). Usingtheformulaforthemomentgeneratingfunction
(MGF) of a Poisson binomially distributed random variable we infer that,
P[Aj= 1]≤/parenleftBigg
1−Pa(Ij) +Pa(Ij) exp/parenleftBigg
−(1−2 max/braceleftbig
qa
j,1,qb
j,1/bracerightbig
)2
2/parenrightBigg/parenrightBiggnmin
×
/parenleftBigg
1−Pb(Ij) +Pb(Ij) exp/parenleftBigg
−(1−2 max/braceleftbig
qa
j,1,qb
j,1/bracerightbig
)2
2/parenrightBigg/parenrightBiggnmin
.
Plugging this into Eq. (28) we get that,
ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
Rj(AS
USB)/bracketrightbig
≤|1−2qj,1|/bracketleftBigg
1−Pa(Ij) +Pa(Ij) exp/parenleftBigg
−(1−2 max/braceleftbig
qa
j,1,qb
j,1/bracerightbig
)2
2/parenrightBigg/bracketrightBiggnmin
×
/bracketleftBigg
1−Pb(Ij) +Pb(Ij) exp/parenleftBigg
−(1−2 max/braceleftbig
qa
j,1,qb
j,1/bracerightbig
)2
2/parenrightBigg/bracketrightBiggnmin
=|1−2qj,1|/bracketleftBigg
1−Pa(Ij)/parenleftBigg
1−exp/parenleftBigg
−(1−2 max/braceleftbig
qa
j,1,qb
j,1/bracerightbig
)2
2/parenrightBigg/parenrightBigg/bracketrightBiggnmin
×
/bracketleftBigg
1−Pb(Ij)/parenleftBigg
1−exp/parenleftBigg
−(1−2 max/braceleftbig
qa
j,1,qb
j,1/bracerightbig
)2
2/parenrightBigg/parenrightBigg/bracketrightBiggnmin
.
Since|1−2 max/braceleftbig
qa
j,1,qb
j,1/bracerightbig
|≤1,
1−exp/parenleftBigg
−(1−2 max/braceleftbig
qa
j,1,qb
j,1/bracerightbig
)2
2/parenrightBigg
≥(1−2 max/braceleftbig
qa
j,1,qb
j,1/bracerightbig
)2
4,
33Published in Transactions on Machine Learning Research (06/2023)
and therefore
ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
Rj(AS
USB)/bracketrightbig
≤|1−2qj,1|/bracketleftBigg
1−Pa(Ij)(1−2 max/braceleftbig
qa
j,1,qb
j,1/bracerightbig
)2
2/bracketrightBiggnmin
×
/bracketleftBigg
1−Pb(Ij)(1−2 max/braceleftbig
qa
j,1,qb
j,1/bracerightbig
)2
2/bracketrightBiggnmin
(i)
≤|1−2qj,1|/bracketleftbigg
1−Pa(Ij)(1−2qj,1−2γ)2
2/bracketrightbiggnmin
×
/bracketleftbigg
1−Pb(Ij)(1−2qj,1−2γ)2
2/bracketrightbiggnmin
(ii)
≤|1−2qj,1|exp/parenleftbigg
−nmin(Pa(Ij) +Pb(Ij))(1−2qj,1−2γ)2
2/parenrightbigg
,
where (i)follows since|max{qa
j,1,qb
j,1}−qj,1|≤1/Kby Eq. (26) and γis such that|γ|≤1/K, and (ii)
follows since (1+z)b≤exp(bz). Now the RHS above is maximized when (1−2qj,1−2γ)2=c
nmin(Pa(Ij)+Pb(Ij)),
for some constant c. Plugging this into the equation above we get that
ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
Rj(AS
USB)/bracketrightbig
≤c/prime
/radicalbig
nmin(Pa(Ij) +Pb(Ij))+c/prime|γ|
≤c/prime
/radicalbig
nmin(Pa(Ij) +Pb(Ij))+c/prime
K.
Finally, noting that Ptest(Ij) = ( Pa(Ij) +Pb(Ij))/2completes the proof.
By combining the previous two lemmas we can now prove our upper bound on the risk of the undersampled
binning estimator. We begin by restating it.
Theorem 5.2. Consider the group shift setting described in Section 3.2.2. For any overlap τ∈[0,1]and for
any(Pmaj,Pmin)∈P GS(τ)the expected excess risk of the Undersampling Binning Estimator (Eq. (5)) with
number of bins with K=⌈nmin1/3⌉is
Excess Risk [AUSB; (Pmaj,Pmin)] =ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
R(AS
USB;Ptest))−R(f⋆;Ptest)/bracketrightbig
≤C
nmin1/3.
Proof.First by Lemma C.3 we know that
Excess Risk [AUSB]≤K−1/summationdisplay
j=0ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
Rj(AS
USB)/bracketrightbig
·Ptest(Ij) +2
K.
Next by using the bound on ES∼Pnmaj
maj×Pnmin
min/bracketleftbig
Rj(AS
USB)/bracketrightbig
established in Lemma C.4 we get that,
Excess Risk (AUSB)≤cK−1/summationdisplay
j=01/radicalbig
nminPtest(Ij)Ptest(Ij) +c
K
=c√nminK−1/summationdisplay
j=0/radicalBig
Ptest(Ij) +c
K
(i)
≤c√nmin√
KK−1/summationdisplay
j=0Ptest(Ij) +c
K
=c/radicalbigg
K
nmin+c
K.
34Published in Transactions on Machine Learning Research (06/2023)
where (i)follows since for any vector z∈RK,/bardblz/bardbl1≤√
K/bardblz/bardbl2. Maximizing over Kyields the choice
K=⌈nmin1/3⌉, completing the proof.
D Additional simulations
Figure 4: Convolutional neural network classiﬁers trained on the Imbalanced Binary CIFAR10 dataset
with a 5:1 label imbalance. (Top) Models trained using the tilted loss (Li et al., 2020) with early stopping.
(Bottom) Models trained using group-DRO (Sagawa et al., 2020) with early stopping. We report the average
test accuracy calculated on a balanced test set over 5 random seeds. We start oﬀ with 2500cat examples
and500dog examples in the training dataset. We ﬁnd similar trends to those obtained in Figure 2 even
with these losses that are designed to optimize for the worst group accuracy.
E Discussion about minimax lower bounds for cost-sensitive losses applied to the
label shift setting
We add a more detailed discussion about applying minimax cost-sensitive losses to obtain a lower bound in
the presence of label shift.
Assume that Pmajis distribution of the covariates x|y= 1, and Pminis the distribution of the covariates
x|y=−1. The training samples are drawn from the distribution:
P(x,y) =P(y= 1)Pmaj+P(y=−1)Pmin,
where
P(y= 1) =ρ
1 +ρand P(y=−1) =1
1 +ρ
for some imbalance ratio ρ>1. On average the ratio between the number of points from the majority class
to the number of points from the minority class is equal to ρ.
35Published in Transactions on Machine Learning Research (06/2023)
We set the cost of getting an incorrectly predicting the majority class label to be equal to
c1=1
1 +ρ
and the cost of incorrectly predicting the minority class label to be equal to
c−1=ρ
1 +ρ.
Note that the costs c1+c−1= 1and thatc1<c−1.
The expected cost-sensitive loss is therefore equal to
E(x,y)∼P[cy1[f(x)/negationslash=y]] =ρ
1 +ρE(x)∼P[c11[f(x)/negationslash= 1]] +1
1 +ρE(x)∼P[c−11[f(x)/negationslash=−1]]
=ρ
(1 +ρ)2Ex∼Pmaj[1[f(x)/negationslash= 1]] +ρ
(1 +ρ)2Ex∼Pmin[1[f(x)/negationslash=−1]]
=2ρ
(1 +ρ)2Ey∼Unif(−1,1),x∼P(x|y)[1[f(x)/negationslash=y]].
Now if we invoke the minimax lower bound (Kamalaruban & Williamson, 2018, Theorem 4) we get that
min
fmax
P2ρ
(1 +ρ)2Ey∼Unif(−1,1),x∼P(x|y)[1[f(x)/negationslash=y]]≥C
1 +ρmin/braceleftBigg/radicalBigg
V
(1 +ρ)n,1
1 +ρV
nh/bracerightBigg
,
where the minimum over fis over all measurable functions from the training data to binary labels, the
maximum is over a data distribution that can be correctly classiﬁed with a classiﬁer from a VC class with
VC dimension at most Vandhis the Massart noise margin. For more thorough deﬁnitions we urge the
reader to see (Kamalaruban & Williamson, 2018). With this lower bound we get that
min
fmax
PEy∼Unif(−1,1),x∼P(x|y)[1[f(x)/negationslash=y]]≥C(1 +ρ)
2ρmin/braceleftBigg/radicalBigg
V
(1 +ρ)n,1
1 +ρV
nh/bracerightBigg
≥C
2min/braceleftBigg/radicalBigg
V
(1 +ρ)n,1
1 +ρV
nh/bracerightBigg
.
Therefore we ﬁnd that this lower bound gets smaller as the imbalance ratio ρgets larger, predicting the
wrong trend for the label shift problem.
F Details about results in Table 1
InTable1, welistedresultsregardingtheperformanceofundersampledalgorithmstoothersthatarereported
in the literature. Here we provide detailed references to these results.
Label shift. The results for label shift are from the paper by Cao et al. (2019). The results are reported in
Table 2 of that paper. For Imb CIFAR 10 (step 10), the undersampling result corresponds to the entry CB
RS from that table with accuracy 84.59%(error 15.41%), while the best method corresponds to the method
LDAM-DRW with accuracy 87.81%(error 12.19%). For Imb CIFAR100 (step 10), the undersampling result
again corresponds to CB RS with accuracy 53.08%(error 46.92%) while the best method corresponds to the
method LDAM-DRW with accuracy 59.46%(error 40.54%).
Group-covariate shift. The results for the group-covariate shift are from Table 2 in Idrissi et al. (2022).
For the CelebA dataset, the undersampled accuracy corresponds to the method SUBG and the best accuracy
is for gDRO. For the Waterbirds dataset, the undersampled method is SUBG and the best competitor is
RWG. For the MultiNLI dataset, the undersampled accuracy corresponds to the method SUBG and the best
accuracy is for gDRO. Finally, for the CivilComments dataset, the undersampled method is SUBG and the
best method is RWG.
36Published in Transactions on Machine Learning Research (06/2023)
G Experimental details for Figures 2 and 4
We construct our label shift dataset from the original CIFAR10 dataset. We create a binary classiﬁcation
task using the “cat” and “dog” classes. We use the oﬃcial test examples as the balanced test set with 1000
cats and 1000dogs. To form the initial train and validation sets, we use 2500cat examples (half of the
training set) and 500dog examples, corresponding to a 5:1 label imbalance. We use 80%of those examples
for training and the rest for validation. We are left with 2500additional cat examples and 4500dog examples
from the original train set which we add into our training set to generate Figure 2.
We use the same convolutional neural network architecture as (Byrd & Lipton, 2019; Wang et al., 2022) with
random initializations for this dataset. We train this model using SGD for 800epochs with batchsize 64, a
constant learning rate 0.001and momentum 0.9. The importance weights used upweight the minority class
samples in the training loss and validation loss is calculated to be#Cat Train Examples
#Dog Train Examples. We note that all of
the experiments were performed on an internal cluster on 8 GPUs.
VS loss: Given a dataset{xi,yi}n
i=1, the VS loss (Kini et al., 2021) is deﬁned as follows
LVS(f) :=n/summationdisplay
i=1log/parenleftbigg
1 + exp/parenleftbigg
−/parenleftbiggngi
nmax/parenrightbiggγ
yif(xi)−τngi
n/parenrightbigg/parenrightbigg
,
wheregidenotes the group label, ngicorresponds to the number of samples from the group, nmaxis the
number of samples in the largest group and nis the total number of samples. We set τ= 3andγ= 0.3, the
best hyperparameters identiﬁed by Wang et al. (2022) on this dataset for this neural network architecture.
Tilted loss: The tilted loss (Li et al., 2020) is deﬁned as
LTilted(f) :=1
tlog/bracketleftBiggn/summationdisplay
i=1exp (t/lscript(yif(xi)))/bracketrightBigg
,
where we take /lscriptto be the logistic loss. In our experiments we set t= 2.
Group-DRO: We run group-DRO (Sagawa et al., 2020, Algorithm 1) with the logistic loss. We set
adversarial step-size ηq= 0.05which was the best hyperparameter identiﬁed by Wang et al. (2022).
37