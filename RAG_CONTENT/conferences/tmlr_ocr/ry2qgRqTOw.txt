Published in Transactions on Machine Learning Research (09/2023)
Fast Kernel Methods for Generic Lipschitz Losses
viap-Sparsified Sketches
Tamim El Ahmad tamim.elahmad@telecom-paris.fr
LTCI, Télécom Paris,
IP Paris, France
Pierre Laforgue pierre.laforgue@unimi.it
Department of Computer Science,
University of Milan, Italy
Florence d’Alché-Buc florence.dalche@telecom-paris.fr
LTCI, Télécom Paris,
IP Paris, France
Reviewed on OpenReview: https: // openreview. net/ forum? id= ry2qgRqTOw
Abstract
Kernel methods are learning algorithms that enjoy solid theoretical foundations while
suffering from important computational limitations. Sketching, which consists in looking for
solutions among a subspace of reduced dimension, is a well-studied approach to alleviate
these computational burdens. However, statistically-accurate sketches, such as the Gaussian
one, usually contain few null entries, such that their application to kernel methods and their
non-sparse Gram matrices remains slow in practice. In this paper, we show that sparsified
Gaussian (and Rademacher) sketches still produce theoretically-valid approximations while
allowing for important time and space savings thanks to an efficient decomposition trick . To
support our method, we derive excess risk bounds for both single and multiple output kernel
problems, with generic Lipschitz losses, hereby providing new guarantees for a wide range of
applications, from robust regression to multiple quantile regression. Our theoretical results
are complemented with experiments showing the empirical superiority of our approach over
state-of-the-art sketching methods.
1 Introduction
Kernel methods hold a privileged position in machine learning, as they allow to tackle a large variety of
learning tasks in a unique and generic framework, that of Reproducing Kernel Hilbert Spaces (RKHSs),
while enjoying solid theoretical foundations (Steinwart & Christmann, 2008b; Scholkopf & Smola, 2018).
From scalar-valued to multiple output regression (Micchelli & Pontil, 2005; Carmeli et al., 2006; 2010),
these approaches play a central role in nonparametric learning, showing a great flexibility. However, when
implemented naively, kernel methods raise major issues in terms of time and memory complexity, and are
often thought of as limited to “fat data”, i.e., datasets of reduced size but with a large number of input
features. One way to scale up kernel methods are the Random Fourier Features (Rahimi & Recht, 2007; Rudi
& Rosasco, 2017; Sriperumbudur & Szabó, 2015; Li et al., 2021), but they mainly apply to shift-invariant
kernels. Another popular approach is to use sketching methods, first exemplified with Nyström approximations
(Williams & Seeger, 2001; Drineas et al., 2005; Bach, 2013; Rudi et al., 2015). Indeed, sketching has recently
gained a lot of interest in the kernel community due to its wide applicability (Yang et al., 2017; Lacotte et al.,
2019; Kpotufe & Sriperumbudur, 2020; Lacotte & Pilanci, 2022; Gazagnadou et al., 2022) and its spectacular
successes when combined to preconditioners and GPUs (Meanti et al., 2020).
1Published in Transactions on Machine Learning Research (09/2023)
Sketching as a random projection method (Mahoney et al., 2011; Woodruff, 2014) is rooted in the Johnson-
Lindenstrauss lemma (Johnson & Lindenstrauss, 1984), and consists in working in reduced dimension
subspaces while benefiting from theoretical guarantees. Learning with sketched kernels has mostly been
studied in the case of scalar-valued regression, in particular in the emblematic case of Kernel Ridge Regression
(Alaoui & Mahoney, 2015; Avron et al., 2017; Yang et al., 2017; Chen & Yang, 2021a). For several identified
sketching types (e.g., Gaussian, Randomized Orthogonal Systems, adaptive sub-sampling), the resulting
estimators come with theoretical guarantees under the form of the minimax optimality of the empirical
approximation error. However, an important blind spot of the above works is their limitation to the square
loss. Few papers go beyond Ridge Regression, and usually exclusively with sub-sampling schemes (Zhang
et al., 2012; Li et al., 2016; Della Vecchia et al., 2021). In this work, we derive excess risk bounds for sketched
kernel machines with generic Lipschitz-continuous losses, under standard assumption on the sketch matrix,
solving an open problem from Yang et al. (2017). Doing so, we provide theoretical guarantees for a wide
range of applications, from robust regression, based either on the Huber loss (Huber, 1964) or ϵ-insensitive
losses (Steinwart & Christmann, 2008a), to quantile regression, tackled through the minimization of the
pinball loss (Koenker, 2005). Further, we address this question in the general context of single and multiple
output regression. Learning vector-valued functions using matrix-valued kernels (Micchelli & Pontil, 2005)
has been primarily motivated by multi-task learning. Although equivalent in functional terms to scalar-valued
kernels on pairs of input and tasks (Hein & Bousquet, 2004, Proposition 5), matrix-valued kernels (Álvarez
et al., 2012) provide a way to define a larger variety of statistical learning problems by distinguishing the
role of the inputs from that of the tasks. The computational and memory burden is naturally heavier in
multi-task/multi-output regression, as the dimension of the output space plays an inevitable role, making
approximation methods for matrix-valued kernel machines a crucial issue. To our knowledge, this work is the
first to address this problem under the angle of sketching. It is however worth mentioning Baldassarre et al.
(2012), who explored spectral filtering approaches for multiple output regression, and the generalization of
Random Fourier Features to operator-valued kernels by Brault et al. (2016).
An important challenge when sketching kernel machines is that the sketched items, e.g., the Gram matrix, are
usually dense. Plain sketching matrices, such as the Gaussian one, then induce significantly more calculations
than sub-sampling methods, which can be computed by applying a mask over the Gram matrix. Sparse
sketching matrices (Clarkson & Woodruff, 2017; Nelson & Nguyên, 2013; Cohen, 2016; Derezinski et al.,
2021) constitute an important line of research to reduce complexity while keeping good statistical properties
when applied to sparse matrices (e.g., matrices induced by graphs), which is not the case of a Gram matrix.
Motivated by these considerations, we analyze a family of sketches, unified under the name of p-sparsified
sketches, that achieve interesting trade-offs between statistical accuracy (Gaussian sketches can be recovered
as a particular case of p-sparsified sketches) and computational efficiency. The p-sparsified sketches are
also memory-efficient, as they do not require computing and storing the full Gram matrix upfront. Besides
theoretical analysis, we provide extensive experiments showing the superiority of p-sparsified sketches over
SOTA approaches such as accumulation sketches (Chen & Yang, 2021a).
Contributions. Our goal is to provide a framework to speed-up both scalar and matrix-valued kernel
methods which is as general as possible while maintaining good theoretical guarantees. For that purpose, we
present three contributions, which may be of independent interest.
•We derive excess risk bounds for sketched kernel machines with generic Lipschitz-continuous losses,
both in the scalar and multiple output cases. We hereby solve an open problem from Yang et al.
(2017), and provide a first analysis to the sketching of vector-valued kernel methods.
•We show that sparsified Gaussian and Rademacher sketches provide valid approximations when
applied to kernel methods. They maintain theoretical guarantees while inducing important space
and computation savings, as opposed to plain sketches.
•We discuss how to learn these new sketched kernel machines, by means of an approximated feature
map. We finally present experiments using Lipschitz-continuous losses, such as robust and quantile
regression, on both synthetic and real-world datasets, supporting the relevance of our approach.
2Published in Transactions on Machine Learning Research (09/2023)
Notation. For any matrix A∈Rm×p,A†is its pseudo-inverse, ∥A∥opits operator norm, Ai:∈Rpitsi-th
row, andA:j∈Rmitsj-th column. The identity matrix of dimension disId. For a couple of random
variables (X,Y)∈X×Y with distribution P,PXis the marginal distribution of X. Forf:X −→Y ,
we use E[f]=EPX[f(X)],E[ℓf]=EP[ℓ(f(X),Y)]andEn[ℓf] =1
n/summationtextn
i=1ℓ(f(xi),yi)for any function
ℓ:Y×Y−→ R.
2 Sketching Kernels Machines with Lipschitz-Continuous Losses
In this section, we derive excess risk bounds for sketched kernel machines with generic Lipschitz losses, for
both scalar and multiple output regression.
2.1 Scalar Kernel Machines
We consider a general regression framework, from an input space Xto some scalar output space Y⊆R.
Given a loss function ℓ:Y×Y→ Rsuch thatz∝⇕⊣√∫⊔≀→ℓ(z,y)is proper, lower semi-continuous and convex for
everyy, our goal is to estimate f∗=arg inff∈HE(X,Y)∼P[ℓ(f(X),Y)], whereH⊂YXis a hypothesis set,
andPis a joint distribution over X×Y. SincePis usually unknown, we assume that we have access to a
training dataset{(xi,yi)}n
i=1composed of i.i.d. realisations drawn from P. We recall the definitions of a
scalar-valued kernel and its RKHS (Aronszajn, 1950).
Definition 1 (Scalar-valued kernel) .A scalar-valued kernel is a symmetric function k:X×X→ Rsuch
that for all n∈N, and any (xi)n
i=1∈Xn,(αi)n
i=1∈Rn, we have/summationtextn
i,j=1αik(xi,xj)αj≥0.
Theorem 1 (RKHS).Letkbe a kernel onX. Then, there exists a unique Hilbert space of functions Hk⊂RX
such thatk(·,x)∈Hkfor allx∈X, and such that we have h(x) =⟨h,k(·,x)⟩Hkfor any (h,x)∈Hk×X.
A kernel machine computes a proxy for f∗by solving
min
f∈Hk1
nn/summationdisplay
i=1ℓ(f(xi),yi) +λn
2∥f∥2
Hk, (1)
whereλn>0is a regularization parameter. By the representer theorem (Kimeldorf & Wahba, 1971; Schölkopf
et al., 2001), the solution to Problem (1) is given by ˆfn=/summationtextn
i=1ˆαik(·,xi), with ˆα∈Rnthe solution to
min
α∈Rn1
nn/summationdisplay
i=1ℓ([Kα]i,yi) +λn
2α⊤Kα, (2)
whereK∈Rn×nis the kernel Gram matrix such that Kij=k(xi,xj).
Definition 2 (Regularized Kernel-based Sketched Estimator) .Given a matrix S∈Rs×n, withs≪n,
sketching consists in imposing the substitution α=S⊤γin the empirical risk minimization problem stated in
Equation (2). We then obtain an optimisation problem of reduced size on γ, that yields the sketched estimator
˜fs=/summationtextn
i=1[S⊤˜γ]ik(·,xi), where ˜γ∈Rsis a solution to
min
γ∈Rs1
nn/summationdisplay
i=1ℓ([KS⊤γ]i,yi) +λn
2γ⊤SKS⊤γ. (3)
In practice, one usually obtains the matrix Sby sampling it from a random distribution. The literature is
rich in examples of distributions that can be used to generate the sketching matrix S. For instance, the
sub-sampling matrices, where each line of Sis sampled from In, have been widely studied in the context of
kernel methods. They are computationally efficient from both time and space perspectives, and yield the
so-called Nyström approach (Williams & Seeger, 2001; Rudi et al., 2015). More complex distributions, such
as Randomized Orthogonal System (ROS) sketching or Gaussian sketch matrices, have also been considered
(Yang et al., 2017). In this work, we first give a general theoretical analysis of regularized kernel-based
3Published in Transactions on Machine Learning Research (09/2023)
sketched estimators for any K-satisfiable sketch matrix (Definition 3). Then, we introduce the p-sparsified
sketches and prove their K-satisfiablity, as well as their relevance for kernel methods in terms of statistical
and computational trade-off.
Works about sketched kernel machines usually assess the performance of ˜fsby upper bounding its squared
L2(PN)error, i.e., (1/n)/summationtextn
i=1(˜fs(xi)−fHk(xi))2, wherefHkistheminimizerofthetrueriskover Hk, supposed
to be attained (Yang et al., 2017, Equation 2), or through its (relative) recovery error ∥˜fs−ˆfn∥Hk/∥ˆfn∥Hk,
seeLacotte & Pilanci (2022, Theorem 3) . In contrast, we focus on the excess risk of ˜fs, the original quantity
of interest. As revealed by the proof of Theorem 2, the approximation error of the excess risk can be controlled
in terms of the L2(PN)error, and we actually recover the results from Yang et al. (2017) when we particularize
to the square loss with bounded outputs (second bound in Theorem 2). Furthermore, studying the excess
risk allows to better position the performances of ˜fsamong the known off-the-shelf kernel-based estimators
available for the targeted problem. To achieve this study, we rely on the key notion of K-satisfiability for a
sketch matrix (Yang et al., 2017; Liu et al., 2019; Chen & Yang, 2021a).
LetK/n =UDU⊤be the eigendecomposition of the Gram matrix, where D=diag (µ1,...,µn)stores the
eigenvalues of K/nin decreasing order. Let δ2
nbe the critical radius of K/n, i.e., the lowest value such that
ψ(δn) = (1
n/summationtextn
i=1min(δ2
n,µi))1/2≤δ2
n. The existence and uniqueness of δ2
nis guaranteed for any RKHS
associated with a positive definite kernel (Bartlett et al., 2006; Yang et al., 2017). Note that δ2
nis similar to
the parameter ˜ε2used in Yang et al. (2012) to analyze Nyström approximation for kernel methods. We define
the statistical dimension of Kasdn=min/braceleftbig
j∈{1,...,n}:µj≤δ2
n/bracerightbig
, withdn=nif no such index jexists.
Definition 3 (K-satisfiability, Yang et al. 2017) .Letc >0be independent of n,U1∈Rn×dnandU2∈
Rn×(n−dn)be the left and right blocks of the matrix Upreviously defined, and D2=diag (µdn+1,...,µn). A
matrixSis said to be K-satisfiable for cif we have
/vextenddouble/vextenddouble/vextenddouble(SU1)⊤SU1−Idn/vextenddouble/vextenddouble/vextenddouble
op≤1/2,and/vextenddouble/vextenddouble/vextenddoubleSU2D1/2
2/vextenddouble/vextenddouble/vextenddouble
op≤cδn. (4)
Roughly speaking, a matrix is K-satisfiable if it defines an isometry on the largest eigenvectors of K, and has
a small operator norm on the smallest eigenvectors. For random sketching matrices, it is common to show
K-satisfiability with high probability under some condition on the sketch size s, see e.g., Yang et al. (2017,
Lemma 5) for Gaussian sketches, Chen & Yang (2021a, Theorem 8) for Accumulation sketches. In Section 3,
we show similar results for p-sparsified sketches.
To derive our excess risk bounds, we place ourselves in the framework of Li et al. (2021), see Sections 2.1 and 3
therein. Namely, we assume that the true risk is minimized over HkatfHk:=arg minf∈HkE[ℓ(f(X),Y)].
The existence of fHkis standard in the literature (Caponnetto & De Vito, 2007; Rudi & Rosasco, 2017; Yang
et al., 2017), and implies that fHkhas bounded norm, see e.g., Rudi & Rosasco (2017, Remark 2). Similarly
to Li et al. (2021), we also assume that estimators returned by Empirical Risk Minimization have bounded
norms. Hence, all estimators considered in the present paper belong to some ball of finite radius R. However,
we highlight that our results do not require prior knowledge on R, and hold uniformly for all finite R. As a
consequence, we consider without loss of generality as hypothesis set the unit ball B(Hk)inHk, up to an a
posteriori rescaling of the bounds by Rto recover the general case.
Assumption 1. The true risk is minimized at fHk.
Assumption 2. The hypothesis set considered is B(Hk).
Assumption 3. For ally∈Y,z∝⇕⊣√∫⊔≀→ℓ(z,y)isL-Lipschitz.
Assumption 4. For allx,x′∈X, we havek(x,x′)≤κ.
Assumption 5. The sketch SisK-satisfiable with constant c>0.
Note that we discuss some directions to relax Assumption 2 in Appendix B. Many loss functions satisfy
Assumption 3, such as the hinge loss ( L= 1), used in SVMs (Cortes & Vapnik, 1995), the ϵ-insensitive ℓ1
(Drucker et al., 1997), the κ-Huber loss, known for robust regression (Huber, 1964), the pinball loss, used in
quantile regression (Steinwart & Christmann, 2011), or the square loss with bounded outputs. Assumption 4
is standard (e.g., κ= 1for the Gaussian kernel). Under Assumptions 1 to 5 we have the following result.
4Published in Transactions on Machine Learning Research (09/2023)
Theorem 2. Let˜fsas in Definition 2, suppose that Assumptions 1 to 5 hold, and let C= 1 +√
6c, withc
the constant from Assumption 5. Then, for any δ∈(0,1)with probability at least 1−δwe have
E/bracketleftbig
ℓ˜fs/bracketrightbig
≤E/bracketleftbig
ℓfHk/bracketrightbig
+LC/radicalbig
λn+δ2n+λn
2+ 8L/radicalbiggκ
n+ 2/radicalbigg
8 log (4/δ)
n. (5)
Furthermore, if ℓ(z,y) = (z−y)2/2andY⊂ [0,1], with probability at least 1−δwe have
E/bracketleftbig
ℓ˜fs/bracketrightbig
≤E/bracketleftbig
ℓfHk/bracketrightbig
+/parenleftbigg
C2+1
2/parenrightbigg
λn+C2δ2
n+ 8κ+√κ√n+ 2/radicalbigg
8 log (4/δ)
n. (6)
Proof sketch. The proof relies on the decomposition of the excess risk into two generalization error terms and
an approximation error term, i.e.,
E[ℓ˜fs]−E[ℓfHk] =E[ℓ˜fs]−En[ℓ˜fs] +En[ℓ˜fs]−En[ℓfHk] +En[ℓfHk]−E[ℓfHk]. (7)
The two generalization errors (of ˜fsandfHk) can be bounded using Bartlett & Mendelson (2003, Theorem 8)
together with Assumptions 1 to 4. For the last term, we can use Jensen’s inequality and the Lipschitz
continuity of the loss to upper bound this approximation error by the square root of the sum of the square
residuals of the Kernel Ridge Regression with targets the fHk(xi). The latter can in turn be upper bounded
using Assumptions 1 and 5 and Lemma 2 from Yang et al. (2017). When considering the square loss, Jensen’s
inequality is not necessary anymore, leading to the improved second term in the right-hand side of the last
inequality in Theorem 2.
Recall that the rates in Theorem 2 are incomparable as is to that of Yang et al. (2017, Theorem 2), since we
focus on the excess risk while the authors study the squared L2(PN)error. Precisely, we recover their results
as a particular case with the square loss and bounded outputs, up to the generalization errors. Instead, note
that we do recover the rates of Li et al. (2021, Theorem 1), based on a similar framework. Our bounds feature
two different terms: a quantity related to the generalization errors, and a quantity governed by δn, deriving
from theK-satisfiability analysis. The behaviour of the critical radius δncrucially depends on the choice of
the kernel. In Yang et al. (2017), the authors compute its decay rate for different kernels. For instance, we
haveδ2
n=O(/radicalbig
log (n)/n)for the Gaussian kernel, δ2
n=O(1/n)for polynomial kernels, or δ2
n=O(n−2/3)
for first-order Sobolev kernels. Note finally that by setting λn∝1/√nwe attain a rate of O(1/√n), that is
minimax for the kernel ridge regression, see Caponnetto & De Vito (2007).
Remark 1. Note that a standard additional assumption on the second order moments of the functions in
Hk(Bartlett et al., 2005) allows to derive refined learning rates for the generalization errors. These refined
rates are expressed in terms of ˆr⋆
Hk, the fixed point of a new sub-root function ˆψn. In order to make the
approximation error of the same order, it is then necessary to prove the K-satisfiability of Swith respect to
ˆr⋆2
Hkinstead ofδ2
n. Whether it is possible to prove such a K-satisfiability for standard sketches is however a
nontrivial question, left as future work.
2.2 Matrix-valued Kernel Machines
In this section, we extend our results to multiple output regression, tackled in vector-valued RKHSs. Note
that the output space Yis now a subset of Rd, withd≥2. We start by recalling important notions about
Matrix-Valued Kernels (MVKs) and vector-valued RKHSs (vv-RKHSs).
Definition 4 (Matrix-valued kernel) .A MVK is an application K:X×X→L (Rd), whereL(Rd)is the set
of bounded linear operators on Rd, such thatK(x,x′)=K(x′,x)⊤for all (x,x′)∈X2, and such that for all
n∈Nand any (xi,yi)n
i=1∈(X×Y )nwe have/summationtextn
i,j=1y⊤
iK(xi,xj)yj⩾0.
Theorem 3 (Vector-valued RKHS) .LetKbe a MVK. There is a unique Hilbert space HK⊂F(X,Rd),
the vv-RKHS of K, such that for all x∈X,y∈Rdandf∈HKwe havex′∝⇕⊣√∫⊔≀→K (x,x′)y∈HK, and
⟨f,K(·,x)y⟩H=f(x)⊤y.
5Published in Transactions on Machine Learning Research (09/2023)
Note that we focus in this paper on the finite-dimensional case, i.e., Y⊂Rd, such that for all x,x′∈X, we have
K(x,x′)∈Rd×d. For a training sample {x1,...,xn}, we define the Gram matrix as K=(K(xi,xj))1≤i,j≤n∈
Rnd×nd. A common assumption consists in considering decomposable kernels: we assume that there
exist a scalar kernel kand a positive semidefinite matrix M∈Rd×dsuch that for all x,x′∈Xwe have
K(x,x′) =k(x,x′)M. The Gram matrix can then be written K=K⊗M, whereK∈Rn×nis the scalar Gram
matrix, and⊗denotes the Kronecker product. Decomposable kernels are widely spread in the literature as
they provide a good compromise between computational simplicity and expressivity —note that in particular
they encapsulate independent learning, achieved with M=Id. We now discuss two examples of relevant
output matrices.
Example 1. In joint quantile regression, one is interested in predicting ddifferent conditional quantiles of
an outputygiven the input x. If(τi)i≤d∈(0,1)denote the ddifferent quantile levels, it has been shown in
Sangnier et al. (2016) that choosing Mij=exp(−γ(τi−τj)2)favors close predictions for close quantile levels,
while limiting crossing effects.
Example 2. In multiple output regression, it is possible to leverage prior knowledge on the task relationships
to design a relevant output matrix M. For instance, let Pbe thed×dadjacency matrix of a graph in which
the vertices are the tasks and an edge exists between two tasks if and only if they are (thought to be) related.
Denoting by LPthe graph Laplacian associated to P, Evgeniou et al. (2005) and Sheldon (2008) have proposed
to useM=(µLP+ (1−µ)Id)−1, withµ∈[0,1]. Whenµ= 0, we haveM=Idand all tasks are considered
independent. When µ= 1, we only rely on the prior knowledge encoded in P.
Given a sample (xi,yi)n
i=1∈/parenleftbig
X,Rd/parenrightbignand a decomposable kernel K=kM(its associated vv-RKHS is HK),
the penalized empirical risk minimisation problem is
min
f∈HK1
nn/summationdisplay
i=1ℓ(f(xi),yi) +λn
2∥f∥2
HK, (8)
whereℓ:Rd×Rd→Ris a loss such that z∝⇕⊣√∫⊔≀→ℓ(z,y)is proper, lower semi-continuous and convex for all
y∈Rd. By the vector-valued representer theorem (Micchelli & Pontil, 2005), we have that the solution
to Problem (8) writes ˆfn=/summationtextn
j=1K(·,xj)ˆαj=/summationtextn
j=1k(·,xj)Mˆαj, where ˆA=(ˆα1,..., ˆαn)⊤∈Rn×dis the
solution to the problem
min
A∈Rn×d1
nn/summationdisplay
i=1ℓ/parenleftig
[KAM ]⊤
i:,yi/parenrightig
+λn
2Tr/parenleftbig
KAMA⊤/parenrightbig
.
In this context, sketching consists in making the substitution A=S⊤Γ, whereS∈Rs×nis a sketch matrix
andΓ∈Rs×dis the parameter of reduced dimension to be learned. The solution to the sketched problem is
then ˜fs=/summationtextn
j=1k(·,xj)M/bracketleftbig
S⊤˜Γ/bracketrightbig
j:, with ˜Γ∈Rs×dminimizing
1
nn/summationdisplay
i=1ℓ/parenleftbig/bracketleftbig
KS⊤ΓM/bracketrightbig
i:,yi/parenrightbig
+λn
2Tr/parenleftbig
SKS⊤ΓMΓ⊤/parenrightbig
.
Theorem 4. Suppose that Assumptions 1 to 5 hold, that K=kMis a decomposable kernel with Minvertible,
and letCas in Theorem 2. Then for any δ∈(0,1)with probability at least 1−δwe have
E/bracketleftbig
ℓ˜fs/bracketrightbig
≤E/bracketleftbig
ℓfHK/bracketrightbig
+LC/radicalig
λn+∥M∥opδ2n+λn
2+ 8L/radicalbigg
κTr (M)
n+ 2/radicalbigg
8 log (4/δ)
n. (9)
Furthermore, if ℓ(z,y) =∥z−y∥2
2/2andY⊂B/parenleftbig
Rd/parenrightbig
, with probability at least 1−δwe have that
E/bracketleftbig
ℓ˜fs/bracketrightbig
≤E/bracketleftbig
ℓfHk/bracketrightbig
+/parenleftbigg
C2+1
2/parenrightbigg
λn+C2∥M∥opδ2
n+ 8 Tr (M)1/2κ∥M∥1/2
op+κ1/2
√n+ 2/radicalbigg
8 log (4/δ)
n.(10)
Proof sketch. The proof follows that of Theorem 2. The main challenge is to adapt Yang et al. (2017,
Lemma 2) to the multiple output setting. To do so, we leverage that Kis decomposable, such that the
K-satisfiability of Sis sufficient, where Kthe scalar Gram matrix.
6Published in Transactions on Machine Learning Research (09/2023)
Note that for M=Id(independent prior), the third term of the right-hand side of both inequalities becomes
of order/radicalbig
d/n, that is typical of multiple output problems. If moreover we instantiate the bound for d= 1,
we recover exactly Theorem 2. Finally, similarly to the scalar case in Theorem 2, looking at the least square
case (Equation (10)), by setting λn∝1/√n, we attain the minimax rate of O(1/√n), as stated in Caponnetto
& De Vito (2007) and Ciliberto et al. (2020, Theorem 5). To the best of our knowledge, Theorem 4 is the
first theoretical result about sketched vector-valued kernel machines. We highlight that it applies to generic
Lipschitz losses and provides a bound directly on the excess risk.
2.3 Algorithmic details
We now discuss how to solve single and multiple output optimization problems. Let {(˜µi,˜vi),i∈[s]}be the
eigenpairs of SKS⊤in descending order, ˜U= [˜Uij]s×s=(˜v1,..., ˜vs),r=rank(SKS⊤), and ˜Kr=˜Ur˜D−1/2
r,
where ˜Dr= diag(˜µ1,..., ˜µr), and ˜Ur= (˜v1,..., ˜vr).
Proposition 1. Solving Problem (3)is equivalent to solving
min
ω∈Rr1
nn/summationdisplay
i=1ℓ/parenleftbig
ω⊤zS(xi),yi/parenrightbig
+λn
2∥ω∥2
2, (11)
where zS(x) =˜K⊤
rS(k(x,x1),...,k (x,xn))⊤∈Rr.
Problem (11)thus writes as a linear problem with respect to the feature maps induced by the sketch,
generalizing the results established in Yang et al. (2012) for sub-sampling sketches. When considering multiple
outputs, it is also possible to derive a linear feature map version when the kernel is decomposable. These
feature maps are of the form zS⊗M1/2, yielding matrices of size nd×rdthat are prohibitive in terms of
space, see Appendix E. Note that an alternative way is to see sketching as a projection of the k(·,xi)into
Rr(Chatalic et al., 2021). Instead, we directly learn Γ. For both single and multiple output problems, we
consider losses not differentiable everywhere in Section 4 and apply ADAM Stochastic Subgradient Descent
(Kingma & Ba, 2015) for its ability to handle large datasets.
Remark 2. In the previous sections, sketching is always leveraged in primal problems. However, for some of
the loss functions we consider, dual problems are usually more attractive (Cortes & Vapnik, 1995; Laforgue
et al., 2020). This naturally raises the question of investigating the interplay between sketching and duality
on the algorithmic level. More details can be found in Appendix F.
3p-Sparsified Sketches
We now introduce the p-sparsified sketches, and establish their K-satisfiability. The p-sparsified sketching
matrices are composed of i.i.d. Rademacher or centered Gaussian entries, multiplied by independent Bernoulli
variables of parameter p(the non-zero entries are scaled to ensure that Sdefines an isometry in expectation).
The sketch sparsity is controlled by p, and when the latter becomes small enough, Scontains many columns
full of zeros. It is then possible to rewrite Sas the product of a sub-Gaussian and a sub-sampling sketch of
reduced size, which greatly accelerates the computations.
Definition 5. Lets<n, andp∈(0,1]. Ap-Sparsified Rademacher ( p-SR) sketching matrix is a random
matrixS∈Rs×nwhose entries Sijare independent and identically distributed (i.i.d.) as follows
Sij=

1√spwith probabilityp
2
0with probability 1−p
−1√spwith probabilityp
2(12)
Ap-Sparsified Gaussian ( p-SG) sketching matrix is a random matrix S∈Rs×nwhose entries Sijare i.i.d.
as follows
Sij=/braceleftigg
1√spGijwith probability p
0with probability 1−p(13)
7Published in Transactions on Machine Learning Research (09/2023)
where theGijare i.i.d. standard normal random variables. Note that standard Gaussian sketches are a special
case ofp-SG sketches, corresponding to p= 1.
Several works partially addressed p-SR sketches in the past literature. For instance, Baraniuk et al. (2008)
establish that p-SR sketches satisfy the Restricted Isometry Property (based on concentration results
from Achlioptas (2001)), but only for p= 1andp= 1/3. In Li et al. (2006), the authors consider generic
p-SR sketches, but do not provide any theoretical result outside of a moment analysis. The i.i.d. sparse
embedding matrices from Cohen (2016) are basically m/s-SR sketches, where m≥1, leading each column to
have exactly mnonzero elements in expectation. However, we were not able to reproduce the proof of the
Johnson-Linderstrauss property proposed by the author for his sketch (Theorem 4.2 in the paper, equivalent
to the first claim of K-satisfiability, left-hand side of (4)). More precisely, we think that the assumptions
considering “each entry is independently nonzero with probability m/s” and “each column has a fixed number
of nonzero entries” ( mhere) are conflicting. As far as we know, this is the first time p-SG sketches are
introduced in the literature. Note that both (12)and(13)can be rewritten as Sij= (1/√sp)BijRij, where the
Bijare i.i.d. Bernouilli random variables of parameter p, and theRijare i.i.d. random variables, independent
from theBij, such that E[Rij] = 0andE[RijRi′j′] = 1ifi=i′andj=j′, and 0otherwise. Namely, for
p-SG sketches Rij=Gijis a standard Gaussian variable while for p-SR sketches it is a Rademacher random
variable. It is then easy to check that p-SR andp-SG sketches define isometries in expectation. In the next
theorem, we show that p-sparsified sketches are K-satisfiable with high probability.
Theorem 5. LetSbe ap-sparsified sketching matrix. Then, there are some universal constants C0,C1>0
and a constant c(p), increasing with p, such that for s≥max/parenleftbig
C0dn/p2,δ2
nn/parenrightbig
and with a probability at least
1−C1e−sc(p), the sketch SisK-satisfiable for c=2√p/parenleftig
1 +/radicalbig
log (5)/parenrightig
+ 1.
Proof sketch. To prove the left-hand side of (4), we use Boucheron et al. (2013, Theorem 2.13), which shows
that any i.i.d. sub-Gaussian sketch matrix satisfies the Johnson-Lindenstrauss lemma with high probability.
To prove the right-hand side of (4), we work conditionally on a realization of the Bij, and use concentration
results of Lipschitz functions of Rademacher or Gaussian random variables (Tao, 2012). We highlight that
such concentration results do not hold for sub-Gaussian random variables in general, preventing from showing
K-satisfiability of generic sparsified sub-Gaussian sketches. Note that having Sij∝BijRijis key, and that
sub-sampling uniformly at random non-zero entries instead of using i.i.d. Bernoulli variables would make
the proof significantly more complex. We highlight that Theorem 5 strictly generalizes Yang et al. (2017,
Lemma 5), recovered for p= 1, and extends the results to Rademacher sketches.
Computational property of p-sparsified sketches. In addition to be statistically accurate, p-sparsified
sketches are computationally efficient. Indeed, recall that the main quantity one has to compute when
sketching a kernel machine is the matrix SKS⊤. With standard Gaussian sketches, that are known to be
theoretically accurate, this computation takes O(sn2)operations. Sub-sampling sketches are notoriously
less precise, but since they act as masks over the Gram matrix K, computing SKS⊤can be done inO(s2)
operations only, without having to store the entire Gram matrix upfront. Now, let S∈Rs×nbe ap-sparsified
sketch, and s′=/summationtextn
j=1I{S:j̸= 0s}be the number of columns of Swith at least one nonzero element. The
crucial observation that makes Scomputationally efficient is that we have
S=SSGSSS, (14)
whereSSG∈Rs×s′is obtained by deleting the null columns from S, andSSS∈Rs′×nis a sub-Sampling
sketch whose sampling indices correspond to the indices of the columns in Swith at least one non-zero
entry1. We refer to (14)as thedecomposition trick . This decomposition is key, as we can apply first a fast
sub-sampling sketch, and then a sub-Gaussian sketch on the sub-sampled Gram matrix of reduced size. Note
thats′is a random variable. By independence of the entries, each column is null with probability (1−p)s.
Then, by the independence of the columns we have that s′follows a Binomial distribution with parameters n
and1−(1−p)s, such that E[s′] =n(1−(1−p)s).
1Precisely,SSSis the identity matrix Is′, augmented with n−s′null columns inserted at the indices of the null columns of S.
8Published in Transactions on Machine Learning Research (09/2023)
Hence, the sparsity of the p-sparsified sketches, controlled by parameter p, is an interesting degree of freedom
to add: it preserves statistical guarantees (Theorem 5) while speeding-up calculations (14). Of course, there
is no free lunch and one looses on one side what is gained on the other: when pdecreases (sparser sketches),
the lower bound to get guarantees s≳dn/p2increases, but the expected number of non-null columns s′
decreases, thus accelerating computations (note that for p= 1we exactly recover the lower bound and number
of non-null columns for Gaussian sketches).
Corollary 1. LetS∈Rs×nbe ap-sparsified sketching matrix, and C0,C1andc(p)as in Theorem 5. Then,
settingp≈0.7ands=C0dn/(0.72),SisK-satisfiable for c= 9, with a probability at least 1−C1e−sc(0.7).
These values of pandsminimize computations while maintaining the guarantees.
Proof.By substituting s=C0dn/p2intoE[s′], one can show that it is optimal to set p≈0.7, independently
fromC0anddn.
Corollary 1 gives the optimal values of pandsthat ensure K-satisfiability of a p-sparsified sketching matrix
while having some complexity reduction. However, the lower bound in Theorem 5 is a sufficient condition,
that might be conservative. Looking at the problem of setting sandpfrom the practitioner point of view, we
also provide more aggressive empirical guidelines. Indeed, although this regime is not covered by Theorem 5,
experiments show that setting sas for the Gaussian sketch and psmaller than 1/syield very interesting
results, see Figure 1(c). Overall, p-sparsified sketches ( i) generalize Gaussian sketches by introducing sparsity
as a new degree of freedom, ( ii) enjoy a regime in which theoretical guarantees are preserved and computations
(slightly) accelerated, ( iii) empirically yield competitive results also in aggressive regimes not covered by
theory, thus achieving a wide range of intesting accuracy/computations tradeoffs.
Related works. Sparse sketches have been widely studied in the literature, see Clarkson & Woodruff
(2017); Nelson & Nguyên (2013); Derezinski et al. (2021). However these sketches are well-suited when
applied to sparse matrices (e.g., matrices induced by graphs). In fact, given a matrix A, computing SAwith
these types of sketching has a time complexity of the order of nnz (A), the number of nonzero elements of A.
Besides, these sketches usually are constructed such that each column has at least one nonzero element (e.g.
CountSketch, OSNAP), hence no decomposition trick is possible. Regarding kernel methods, since a Gram
matrix is typically dense (e.g., with the Gaussian kernel, nnz (K)=n2), and since no decomposition trick can
be applied, one has to compute the whole matrix Kand store it, such that time and space complexity implied
by such sketches are of the order of n2. In practice, we show that we can set psmall enough to computationally
outperform classical sparse sketches and still obtain similar statistical performance. Note that an important
line of research is devoted to improve the statistical performance of Nyström’s approximation, either by
adaptive sampling (Kumar et al., 2012; Wang & Zhang, 2013; Gittens & Mahoney, 2013), or leverage scores
(Alaoui & Mahoney, 2015; Musco & Musco, 2017; Rudi et al., 2018; Chen & Yang, 2021b). We took the
opposite route, as p-SG sketches are accelerated but statistically degraded versions of the Gaussian sketch.
4 Experiments
We now empirically compare the performance of p-sparsified sketches against state-of-he-art approaches,
namely Nyström approximation (Williams & Seeger, 2001), Gaussian sketch (Yang et al., 2017), Accumulation
sketch (Chen & Yang, 2021a), CountSketch (Clarkson & Woodruff, 2017) and Random Fourier Features
(Rahimi & Recht, 2007). We chose not to benchmark ROS sketches as CountSketch has equivalent statistical
accuracy while being faster to compute. Results reported are averaged over 30 replicates.
4.1 Scalar regression
Robust regression. We generate a dataset composed of n= 10,000training datapoints: 9,900input
points drawn i.i.d. from U([010, 110])and100other drawn i.i.d. from N(1.5 110,0.25I10). The outputs are
generated as y=f⋆(x) +ϵ, whereϵ∼N (0,1)and
f⋆(x) = 0.1e4x1+4
1 +e−20(x2−0.5)+ 3x3+ 2x4+x5,
9Published in Transactions on Machine Learning Research (09/2023)
MSE
s
(a) Test relative MSE w.r.t. swithκ-Huber.
Training time (in seconds)
s (b) Training time (seconds) w.r.t. swithκ-Huber.
MSE
Training time (in seconds)
(c) Test relative MSE w.r.t. training times with κ-Huber.
Figure 1: Trade-off between Accuracy and Efficiency for p-SR sketches with κ-Huber loss on synthetic dataset.
as introduced in Friedman (1991). We generate a test set of nte= 10,000points in the same way. We use the
Gaussian kernel and select its bandwidth —as well as parameters λnandκ(andϵforϵ-SVR)— via 5-folds
cross-validation. We solve this 1D regression problem using the κ-Huber loss, described in Appendix G. We
learn the sketched kernel machines for different values of s(from 40 to 140) and several values of p, the
probability of being non-null in a p-SR sketch. Figure 1(a) presents the test error as a function of the sketch
sizes. Figure 1(b) shows the corresponding computational training time. All methods reduce their test
error, measured in terms of the relative Mean Squared Error (MSE) when sincreases. Note that increasing p
increases both the precision and the training time, as expected. This behaviour recalls the Accumulation
sketches, since we observe a form of interpolation between the Nyström and Gaussian approximations. The
behaviour of all the different sketched kernel machines is shown in Figure 1(c), where each of them appears
as a point (training time, test MSE). We observe that p-SR sketches attain the smallest possible error
(MSE≤0.05) at the lowest training time budget (mostly around 5.6<time< 6.6). Moreover, p-SR sketches
obtain a similar precision range as the Accumulation sketches, but for smaller training times (both approaches
improve upon CountSketch and Gaussian sketch in that respect). Nyström sketching, which similarly to
our approach does not need computing the entire Gram matrix, is fast to compute. The method is however
known to be sensitive to the non-homogeneity of the marginal distribution of the input data (Yang et al.,
2017, Section 3.3). In contrast, the sub-Gaussian mixing matrix SSGin(14)makesp-sparsified sketches more
robust, as empirically shown in Figure 1(c). See Appendix H.1 for results on p-SG sketches.
10Published in Transactions on Machine Learning Research (09/2023)
4.2 Vector-valued regression
Table 1: Test pinball and crossing loss and training times (in seconds) with and without sketching ( s= 50).
Dataset Metrics w/o Sketch 20/ntr-SR 20/ntr-SG Acc. m= 20 CountSketch
BostonPinball loss 51.28±0.67 54.75±0.74 54 .78±0.72 54 .73±0.75 54 .60±0.72
Crossing loss 0.34±0.13 0 .26±0.08 0 .11±0.07 0 .15±0.07 0.10±0.05
Training time 6.97±0.25 1 .43±0.07 1 .38±0.08 1 .48±0.05 1.23±0.07
otolithsPinball loss 2.78 2 .66±0.02 2.64±0.02 2.67±0.03 2 .65±0.02
Crossing loss 5.18 5.46±0.06 5 .43±0.05 5 .46±0.06 5 .44±0.05
Training time 606.8 20 .4±0.5 20.0±0.3 22.1±0.4 20 .9±0.3
Joint quantile regression. We choose the quantile levels as follows τ= (0.1,0.3,0.5,0.7,0.9). We apply a
subgradient algorithm to minimize the pinball loss described in Appendix G with ridge regularization and a
kernelK=kMwithMdiscussed in Example 1, and ka Gaussian kernel. We select regularisation parameter
λnand bandwidth of kernel σ2via a 5-fold cross-validation. We showcase the behaviour of the proposed
algorithm for Joint Sketched Quantile Regression on two datasets: the Boston Housing dataset (Harrison Jr
& Rubinfeld, 1978), composed of 506data points devoted to house price prediction, and the Fish Otoliths
dataset (Moen et al., 2018; Ordoñez et al., 2020), dedicated to fish age prediction from images of otoliths
(calcium carbonate structures), composed of a train and test sets of size 3780and165respectively. The
results are averages over 10 random 70%−30%train-test splits for Boston dataset. For the Otoliths dataset
we kept the initial given train-test split. The results are reported in Table 1. Sketching allows for a massive
reduction of the training times while preserving the statistical performances. As a comparison, according to
the results of Sangnier et al. (2016), the best benchmark result for the Boston dataset in terms of test pinball
loss is 47.4, while best test crossing loss is 0.48, which shows that our implementation does not compete in
terms of quantile prediction but preserves the non-crossing property.
Table 2: ARRMSE and training times (in sec) with square loss and s= 100when using Sketching.
Dataset Metrics w/o Sketch 20/ntr-SR 20/ntr-SG Acc. m= 20 CountSketch
rf1ARRMSE 0.575 0.584±0.003 0 .583±0.003 0 .592±0.001 0.575±0.0005
Training time 1.73 0.22±0.025 0.25±0.005 0 .60±0.0004 0 .66±0.013
rf2ARRMSE 0.578 0.671±0.009 0 .656±0.006 0 .796±0.006 0 .715±0.011
Training time 1.77 0 .28±0.003 0.27±0.003 0.82±0.003 0 .62±0.001
scm1dARRMSE 0.418 0.422±0.002 0 .423±0.001 0 .423±0.001 0 .420±0.001
Training time 9.36 0.45±0.022 0.45 ±0.019 0.86±0.006 2 .49±0.035
scm20dARRMSE 0.755 0 .754±0.003 0 .754±0.003 0.753±0.001 0.754±0.002
Training time 6.16 0.38±0.016 0.38 ±0.017 0.70±0.032 1 .91±0.047
Multi-output regression. We finally conducted experiments on multi-output kernel ridge regression. We
used decomposable kernels, and took the largest datasets introduced in Spyromitros-Xioufis et al. (2016).
They consist in four datasets, divided in two groups: River Flow (rf1 and rf2) both composed of 4108
training data, and Supply Chain Management (scm1d and scm20d) composed of 8145and7463training data
respectively (more details and additional results can be found in Appendix H.2). We compare our non-sketched
decomposable matrix-valued kernel machine with the sketched version. For the sake of conciseness, we only
report here the Average Relative Root Mean Squared Error (ARRMSE), see Table 2 and Appendix H.2.
For all datasets, sketching shows strong computational improvements while maintaining the accuracy of
non-sketched approaches.
Note that for both joint quantile regression and multi-output regression the results obtained after sketching
(no matter the sketch chosen) are almost the same as that attained without sketching. It might be explained
by two factors. First, the datasets studied have relatively small training sizes (from 354training data for
Boston to 8145for scm1d). Second, predicting jointly multiple outputs is a complex task, so that it appears
more natural to obtain less differences and variance using various types of sketches (or no sketch). However,
in all cases sketching induces a huge time saver.
11Published in Transactions on Machine Learning Research (09/2023)
5 Conclusion
We proposed excess-risk bounds for sketched kernel machines in the context of Lipschitz-continuous losses,
with results valid for both scalar and matrix-valued kernels. We introduced a novel sketching scheme that
leverages the good empirical statistical guarantees of the Gaussian Sketching while combining them with the
low cost of Nyström sketching. Numerical experiments show that this novel scheme opens the door to many
applications beyond the squared loss. Improvements on multi-output regression can certainly be obtained by
applying low-rank considerations in the output space as well.
Acknowledgments
The authors thank Olivier Fercoq for insightful discussions. This work was supported by the Télécom Paris
research chair on Data Science and Artificial Intelligence for Digitalized Industry and Services (DSAIDIS)
and by the French National Research Agency (ANR) through the ANR-18-CE23-0014 APi project.
References
Dimitris Achlioptas. Database-friendly random projections. In Proceedings of the twentieth ACM SIGMOD-
SIGACT-SIGART symposium on Principles of database systems , pp. 274–281, 2001.
Ahmet Alacaoglu, Olivier Fercoq, and Volkan Cevher. Random extrapolation for primal-dual coordinate
descent. In Proc of the International Conference on Machine Learning (ICML) , pp. 191–201. PMLR, 2020.
Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with statistical guarantees.
In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information
Processing Systems (NeurIPS) , volume 28, 2015.
Mauricio A. Álvarez, Lorenzo Rosasco, and Neil D. Lawrence. Kernels for vector-valued functions: a review.
Foundations and Trends in Machine Learning , 4(3):195–266, 2012.
Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society , pp.
337–404, 1950.
Haim Avron, Kenneth L Clarkson, and David P Woodruff. Faster kernel ridge regression using sketching and
preconditioning. SIAM Journal on Matrix Analysis and Applications , 38(4):1116–1138, 2017.
Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In Proc. of the 26th annual Conference
on Learning Theory , pp. 185–209. PMLR, 2013.
Luca Baldassarre, Lorenzo Rosasco, Annalisa Barla, and Alessandro Verri. Multi-output learning via spectral
filtering. Machine Learning , 87(3):259–301, 2012.
Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin. A simple proof of the restricted
isometry property for random matrices. Constructive Approximation , 28(3):253–263, 2008.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural
results.J. Mach. Learn. Res. , 3:463–482, 2003.
Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. Ann. Statist. , 33
(4):1497–1537, 2005.
Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds. Journal
of the American Statistical Association , 101(473):138–156, 2006.
Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory
of independence . Oxford university press, 2013.
Romain Brault, Markus Heinonen, and Florence Buc. Random fourier features for operator-valued kernels.
InAsian Conference on Machine Learning , pp. 110–125. PMLR, 2016.
12Published in Transactions on Machine Learning Research (09/2023)
Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Founda-
tions of Computational Mathematics , 7(3):331–368, 2007.
Claudio Carmeli, Ernesto De Vito, and Alessandro Toigo. Vector valued reproducing kernel hilbert spaces of
integrable functions and mercer theorem. Analysis and Applications , 4(04):377–408, 2006.
Claudio Carmeli, Ernesto De Vito, Alessandro Toigo, and Veronica Umanitá. Vector valued reproducing
kernel hilbert spaces and universality. Analysis and Applications , 8(01):19–61, 2010.
Antonin Chambolle, Matthias J Ehrhardt, Peter Richtárik, and Carola-Bibiane Schonlieb. Stochastic primal-
dual hybrid gradient algorithm with arbitrary sampling and imaging applications. SIAM Journal on
Optimization , 28(4):2783–2808, 2018.
Antoine Chatalic, Luigi Carratino, Ernesto De Vito, and Lorenzo Rosasco. Mean nyström embeddings for
adaptive compressive learning, 2021.
Yifan Chen and Yun Yang. Accumulations of projections—a unified framework for random sketches in kernel
ridge regression. In International Conference on Artificial Intelligence and Statistics , pp. 2953–2961. PMLR,
2021a.
Yifan Chen and Yun Yang. Fast statistical leverage score approximation in kernel ridge regression. In
International Conference on Artificial Intelligence and Statistics , pp. 2935–2943. PMLR, 2021b.
Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi. A general framework for consistent structured
prediction with implicit loss embeddings. J. Mach. Learn. Res. , 21(98):1–67, 2020.
Kenneth L. Clarkson and David P. Woodruff. Low-rank approximation and regression in input sparsity time. J.
ACM, 63(6), jan 2017. ISSN 0004-5411. doi: 10.1145/3019134. URL https://doi.org/10.1145/3019134 .
Michael B. Cohen. Nearly tight oblivious subspace embeddings by trace inequalities. In Proceedings of the
2016 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) , pp. 278–287, 2016.
Laurent Condat. A primal–dual splitting method for convex optimization involving lipschitzian, proximable
and linear composite terms. Journal of optimization theory and applications , 158(2):460–479, 2013.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning , 20(3):273–297, 1995.
Andrea Della Vecchia, Jaouad Mourtada, Ernesto De Vito, and Lorenzo Rosasco. Regularized erm on random
subspaces. In International Conference on Artificial Intelligence and Statistics , pp. 4006–4014. PMLR,
2021.
Michal Derezinski, Zhenyu Liao, E. Dobriban, and Michael W. Mahoney. Sparse sketches with small inversion
bias. InCOLT, 2021.
Petros Drineas, Michael W Mahoney, and Nello Cristianini. On the nyström method for approximating a
gram matrix for improved kernel-based learning. JMLR, 6(12), 2005.
Harris Drucker, Christopher JC Burges, Linda Kaufman, Alex J Smola, and Vladimir Vapnik. Support vector
regression machines. In Advances in neural information processing systems , pp. 155–161, 1997.
Theodoros Evgeniou, Charles A. Micchelli, and Massimiliano Pontil. Learning multiple tasks with kernel
methods. Journal of Machine Learning Research , 6(21):615–637, 2005.
Olivier Fercoq and Pascal Bianchi. A coordinate-descent primal-dual algorithm with large step size and
possibly nonseparable functions. SIAM Journal on Optimization , 29(1):100–134, Jan 2019.
Jerome H Friedman. Multivariate adaptive regression splines. The annals of statistics , pp. 1–67, 1991.
Nidham Gazagnadou, Mark Ibrahim, and Robert M. Gower. Ridgesketch: A fast sketching based solver for
large scale ridge regression. SIAM Journal on Matrix Analysis and Applications , 43(3):1440–1468, 2022.
doi: 10.1137/21M1422963. URL https://doi.org/10.1137/21M1422963 .
13Published in Transactions on Machine Learning Research (09/2023)
Alex Gittens and Michael Mahoney. Revisiting the nystrom method for improved large-scale machine learning.
Proceedings of the 30th International Conference on Machine Learning , 28(3):567–575, 17–19 Jun 2013.
William Groves and Maria Gini. On optimizing airline ticket purchase timing. ACM Transactions on
Intelligent Systems and Technology (TIST) , 7(1):1–28, 2015.
David Harrison Jr and Daniel L Rubinfeld. Hedonic housing prices and the demand for clean air. Journal of
environmental economics and management , 5(1):81–102, 1978.
Matthias Hein and Olivier Bousquet. Kernels, associated structures and generalizations. Technical Report
127, Max Planck Institute for Biological Cybernetics, Tübingen, Germany, July 2004.
Peter J Huber. Robust estimation of a location parameter. The Annals of Mathematical Statistics , pp. 73–101,
1964.
William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space 26.
Contemporary mathematics , 26:28, 1984.
George Kimeldorf and Grace Wahba. Some results on tchebycheffian spline functions. Journal of mathematical
analysis and applications , 33(1):82–95, 1971.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and
Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015.
Roger Koenker. Quantile regression . Cambridge university press, 2005.
Samory Kpotufe and Bharath K. Sriperumbudur. Gaussian sketching yields a J-L lemma in RKHS. In Silvia
Chiappa and Roberto Calandra (eds.), AISTATS 2020 , volume 108 of Proceedings of Machine Learning
Research , pp. 3928–3937. PMLR, 2020.
Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar. Sampling methods for the nyström method. J. Mach.
Learn. Res. , 13:981–1006, 2012.
JonathanLacotteandMertPilanci. Adaptiveandobliviousrandomizedsubspacemethodsforhigh-dimensional
optimization: Sharpanalysisandlowerbounds. IEEE Transactions on Information Theory , 68(5):3281–3303,
2022. doi: 10.1109/TIT.2022.3146206.
Jonathan Lacotte, Mert Pilanci, and Marco Pavone. High-dimensional optimization in adaptive random
subspaces. In Proc. of the 33rd International Conference on Neural Information Processing Systems , pp.
10847–10857, 2019.
Pierre Laforgue, Alex Lambert, Luc Brogat-Motte, and Florence d’Alché Buc. Duality in rkhss with infinite
dimensional outputs: Application to robust losses. In International Conference on Machine Learning , pp.
5598–5607. PMLR, 2020.
Ping Li, Trevor J Hastie, and Kenneth W Church. Very sparse random projections. In Proceedings of the
12th ACM SIGKDD international conference on Knowledge discovery and data mining , pp. 287–296, 2006.
Zhe Li, Tianbao Yang, Lijun Zhang, and Rong Jin. Fast and accurate refined nyström-based kernel svm. In
Thirtieth AAAI Conference on Artificial Intelligence , 2016.
Zhu Li, Jean-Francois Ton, Dino Oglic, and Dino Sejdinovic. Towards a unified analysis of random fourier
features. Journal of Machine Learning Research , 22(108):1–51, 2021.
Meimei Liu, Zuofeng Shang, and Guang Cheng. Sharp theoretical analysis for nonparametric testing under
random projection. In Alina Beygelzimer and Daniel Hsu (eds.), Proceedings of the Thirty-Second Conference
on Learning Theory , volume 99 of Proceedings of Machine Learning Research , pp. 2175–2209. PMLR, 25–28
Jun 2019.
14Published in Transactions on Machine Learning Research (09/2023)
Michael W Mahoney et al. Randomized algorithms for matrices and data. Foundations and Trends ®in
Machine Learning , 3(2):123–224, 2011.
Jiří Matoušek. Lectures on Discrete Geometry . Graduate Texts in Mathematics. Springer, 2013.
Andreas Maurer. A vector-contraction inequality for rademacher complexities. In International Conference
on Algorithmic Learning Theory , pp. 3–17. Springer, 2016.
Giacomo Meanti, Luigi Carratino, Lorenzo Rosasco, and Alessandro Rudi. Kernel methods through the roof:
Handling billions of points efficiently. Advances in Neural Information Processing Systems (NeurIPS) , 33,
2020.
Charles A Micchelli and Massimiliano Pontil. On learning vector-valued functions. Neural computation , 17
(1):177–204, 2005.
Endre Moen, Nils Olav Handegard, Vaneeda Allken, Ole Thomas Albert, Alf Harbitz, and Ketil Malde.
Automatic interpretation of otoliths using deep learning. PLoS One , 13(12):e0204713, 2018.
Cameron Musco and Christopher Musco. Recursive sampling for the nyström method. Advances in Neural
Information Processing Systems , 2017:3834–3846, 2017.
Jelani Nelson and Huy L. Nguyên. Osnap: Faster numerical linear algebra algorithms via sparser subspace
embeddings. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science , pp. 117–126,
2013. doi: 10.1109/FOCS.2013.21.
Alba Ordoñez, Line Eikvil, Arnt-Børre Salberg, Alf Harbitz, Sean Meling Murray, and Michael C Kampffmeyer.
Explaining decisions of deep neural networks used for fish age prediction. PloS one , 15(6):e0235013, 2020.
Ali Rahimi and B. Recht. Random features for large scale kernel machines. NIPS, 20:1177–1184, 01 2007.
Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In
Advances on Neural Information Processing Systems (NeurIPS) , pp. 3215–3225, 2017.
Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. Less is more: Nyström computational
regularization. Advances in Neural Information Processing Systems , 28, 2015.
Alessandro Rudi, Daniele Calandriello, Luigi Carratino, and Lorenzo Rosasco. On fast leverage score sampling
and optimal learning. In NeurIPS , 2018.
Maxime Sangnier, Olivier Fercoq, and Florence d’Alché Buc. Joint quantile regression in vector-valued
RKHSs. In Advances in Neural Information Processing Systems (NeurIPS) , Barcelona, France, December
2016.
Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines, regularization,
optimization, and beyond . Adaptive Computation and Machine Learning Series, 2018.
Bernhard Schölkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In Computational
Learning Theory: 14th Annual Conference on Computational Learning Theory, COLT 2001 and 5th
European Conference on Computational Learning Theory, EuroCOLT 2001 Amsterdam, The Netherlands,
July 16–19, 2001 Proceedings 14 , pp. 416–426. Springer, 2001.
Daniel Sheldon. Graphical multi-task learning, 2008.
Eleftherios Spyromitros-Xioufis, Grigorios Tsoumakas, William Groves, and Ioannis Vlahavas. Multi-target
regression via input space expansion: treating targets as inputs. Machine Learning , 104(1):55–98, 2016.
Bharath K Sriperumbudur and Zoltán Szabó. Optimal rates for random fourier features. In NIPS, 2015.
Ingo Steinwart and Andreas Christmann. Sparsity of svms that use the epsilon-insensitive loss. In Daphne
Koller, Dale Schuurmans, Yoshua Bengio, and Léon Bottou (eds.), Advances in Neural Information
Processing Systems 21 (NeurIPS) , pp. 1569–1576. Curran Associates, Inc., 2008a.
15Published in Transactions on Machine Learning Research (09/2023)
Ingo Steinwart and Andreas Christmann. Support vector machines . Springer Science & Business Media,
2008b.
Ingo Steinwart and Andreas Christmann. Estimating conditional quantiles with the help of the pinball loss.
Bernoulli , 17(1):211–225, 2011.
Michel Talagrand. Concentration of measure and isoperimetric inequalities in product spaces. Publications
Mathématiques de l’Institut des Hautes Etudes Scientifiques , 81(1):73–205, 1995.
Terence Tao. Topics in random matrix theory , volume 132. American Mathematical Soc., 2012.
Bang Cong Vu. A splitting algorithm for dual monotone inclusions involving cocoercive operators, 2011.
Shusen Wang and Zhihua Zhang. Improving cur matrix decomposition and the nyström approximation via
adaptive sampling. J. Mach. Learn. Res. , 14(1):2729–2769, jan 2013.
Christopher Williams and Matthias Seeger. Using the nyström method to speed up kernel machines. In
T. Leen, T. Dietterich, and V. Tresp (eds.), Advances in Neural Information Processing Systems , volume 13,
pp. 682–688. MIT Press, 2001.
David P. Woodruff. Sketching as a tool for numerical linear algebra. Found. Trends Theor. Comput. Sci. , 10
(1-2):1–157, 2014. doi: 10.1561/0400000060. URL https://doi.org/10.1561/0400000060 .
Tianbao Yang, Yu-feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nyström method vs random
fourier features: A theoretical and empirical comparison. In F. Pereira, C. J. C. Burges, L. Bottou,
and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems , volume 25. Curran
Associates, Inc., 2012.
Yun Yang, Mert Pilanci, Martin J Wainwright, et al. Randomized sketches for kernels: Fast and optimal
nonparametric regression. The Annals of Statistics , 45(3):991–1023, 2017.
Kai Zhang, Liang Lan, Zhuang Wang, and Fabian Moerchen. Scaling up kernel svm on limited resources:
A low-rank linearization approach. In Neil D. Lawrence and Mark Girolami (eds.), Proceedings of the
Fifteenth International Conference on Artificial Intelligence and Statistics , volume 22, pp. 1425–1434, 2012.
16Published in Transactions on Machine Learning Research (09/2023)
A Technical Proofs
In this section are gathered all the technical proofs of the results stated in the article.
Notation. We recall that we assume that training data (xi,yi)n
i=1are i.i.d. realisations sampled from a
joint probability density P(x,y). We define
En[ℓf] =1
nn/summationdisplay
i=1ℓ(f(xi),yi),
E[ℓf] =EP[ℓ(f(X),Y].
For a class of functions F, the empirical Rademacher complexity (Bartlett & Mendelson, 2003) is defined as
ˆRn(F) =E/bracketleftigg
sup
f∈F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
nn/summationdisplay
i=1σif(xi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle|x1,...,xn/bracketrightigg
,
whereσ1,...,σnare independent Rademacher random variables such that P{σi= 1}=P{σi=−1}= 1/2.
The corresponding Rademacher complexity is then defined as the expectation of the empirical Rademacher
complexity
Rn(F) =E/bracketleftig
ˆRn(F)/bracketrightig
.
A.1 Proof of Theorem 2
We first prove the first inequality in Theorem 2 for generic Lipschitz losses.
Proof.The proof follows that of Li et al. (2021, Theorem 3). We decompose the expected learning risk as
E[ℓ˜fs]−E[ℓfHk] =E[ℓ˜fs]−En[ℓ˜fs] +En[ℓ˜fs]−En[ℓfHk] +En[ℓfHk]−E[ℓfHk]. (7)
We then use Bartlett & Mendelson (2003, Theorem 8) to bound E[ℓ˜fs]−En[ℓ˜fs]andEn[ℓfHk]−E[ℓfHk].
Lemma 1. (Bartlett & Mendelson, 2003, Theorem 8) Let {xi,yi}n
i=1be i.i.d samples from Pand letH
be the space of functions mapping from XtoR. Denote a loss function with l:Y×R→[0,1]and recall
the learning risk function for all f∈HisE[lf], together with the corresponding empirical risk function
En[lf]= (1/n)/summationtextn
i=1l(yi,f(xi)). Then, for a sample of size n, for allf∈Handδ∈(0,1), with probability
1−δ/2, we have that
E[lf]≤En[lf] +Rn(l◦H) +/radicalbigg
8 log(4/δ)
n
wherel◦H={(x,y)→l(y,f(x))−l(y,0)|f∈H}.
Thus, since ˜fslies in the unit ball B(Hk)ofHkby Assumption 2, we obtain thanks to the above lemma,
with a probability at least 1−δ
E/bracketleftbig
ℓ˜fs/bracketrightbig
−En/bracketleftbig
ℓ˜fs/bracketrightbig
≤Rn/parenleftbig
ℓ◦B(Hk)/parenrightbig
+/radicalbigg
8 log(2/δ)
n.
Then, by the Lipschitz continuity of ℓ(Assumption 3) and point 4 of Theorem 12 from Bartlett & Mendelson
(2003), we have that
Rn(ℓ◦B(Hk))≤2LRn(B(Hk)).
Finally, Assumption 4 combined with Lemma 22 from Bartlett & Mendelson (2003) then yields
Rn(B(Hk))≤2
n/radicaltp/radicalvertex/radicalvertex/radicalbtn/summationdisplay
i=1k(xi,xi)≤2/radicalbiggκ
n.
17Published in Transactions on Machine Learning Research (09/2023)
As a consequence, we obtain
E/bracketleftig
ℓ˜fs/bracketrightig
−En/bracketleftig
ℓ˜fs/bracketrightig
≤4L√κ√n+/radicalbigg
8 log(4/δ)
n, (15)
and the exact same result applies to En[ℓfHk]−E[ℓfHk], by Assumption 2 and the opposite side of Lemma 1.
We now focus on the last quantity to bound. Let HS=/braceleftbig
f=/summationtextn
i=1/bracketleftbig
S⊤γ/bracketrightbig
ik(·,xi)|γ∈Rs/bracerightbig
. By Assump-
tions 2 and 3 and Jensen’s inequality we have
En/bracketleftig
ℓ˜fs/bracketrightig
−En/bracketleftig
ℓfHk/bracketrightig
=1
nn/summationdisplay
i=1ℓ/parenleftbig˜fs(xi),yi/parenrightbig
−1
nn/summationdisplay
i=1ℓ(fHk(xi),yi)
≤1
nn/summationdisplay
i=1ℓ/parenleftbig˜fs(xi),yi/parenrightbig
+λn
2/vextenddouble/vextenddouble˜fs/vextenddouble/vextenddouble2
Hk−1
nn/summationdisplay
i=1ℓ(fHk(xi),yi)
= inf
f∈HS
∥f∥Hk≤11
nn/summationdisplay
i=1ℓ(f(xi),yi)−1
nn/summationdisplay
i=1ℓ(fHk(xi),yi) +λn
2∥f∥2
Hk
≤inf
f∈HS
∥f∥Hk≤1L
nn/summationdisplay
i=1|f(xi)−fHk(xi)|+λn
2
≤Linf
f∈HS
∥f∥Hk≤1/radicaltp/radicalvertex/radicalvertex/radicalbt1
nn/summationdisplay
i=1|f(xi)−fHk(xi)|2+λn
2
=L/radicaltp/radicalvertex/radicalvertex/radicalbtinf
f∈HS
∥f∥Hk≤11
n/vextenddouble/vextenddoublefX−fX
Hk/vextenddouble/vextenddouble2
2+λn
2,
where, for any f∈Hk,fX=(f(x1),...,f (xn))∈Rn. Let ˜fR
s=/summationtextn
i=1/bracketleftbig
S⊤˜γR/bracketrightbig
ik(·,xi), where ˜γRis a
solution to
inf
γ∈Rs1
n/vextenddouble/vextenddoubleKS⊤γ−fX
Hk/vextenddouble/vextenddouble2
2+λnγ⊤SKS⊤γ.
It is easy to check that ˜fR
sis also a solution to
inf
f∈HS
∥f∥Hk≤∥˜fRs∥Hk1
n/vextenddouble/vextenddoublefX−fX
Hk/vextenddouble/vextenddouble2
2.
Since we have∥˜fR
s∥Hk≤1by Assumption 2, it holds
inf
f∈HS
∥f∥Hk≤11
n/vextenddouble/vextenddoublefX−fX
Hk/vextenddouble/vextenddouble2
2≤ inf
f∈HS
∥f∥Hk≤∥˜fRs∥Hk1
n/vextenddouble/vextenddoublefX−fX
Hk/vextenddouble/vextenddouble2
2
= inf
γ∈Rs1
n/vextenddouble/vextenddoubleKS⊤γ−fX
Hk/vextenddouble/vextenddouble2
2+λnγ⊤SKS⊤γ.
As a consequence,
En/bracketleftig
ℓ˜fs/bracketrightig
−En/bracketleftig
ℓfHk/bracketrightig
≤L/radicalbigg
inf
γ∈Rs1
n/vextenddouble/vextenddoubleKS⊤γ−fX
Hk/vextenddouble/vextenddouble2
2+λnγ⊤SKS⊤γ+λn
2.
Finally, since Sis aK-satisfiable sketch matrix, using Lemma 2 from Yang et al. (2017),
En/bracketleftig
ℓ˜fs/bracketrightig
−En/bracketleftig
ℓfHk/bracketrightig
≤LC/radicalbig
λn+δ2n+λn
2, (16)
whereC= 1 +√
6candcis a universal constant coming from K-satisfiable property. The desired bound is
obtained by combining Equations (7), (15) and (16).
18Published in Transactions on Machine Learning Research (09/2023)
We now give the proof of the second claim i.e., the excess risk bound for kernel ridge regression.
Proof.We now assume that the outputs are bounded, hence, without loss of generality, Y ⊂ [0,1].
First, we prove Lipschitz-continuity of the square loss under Assumptions 2 and 4. Let Hk(X) =
{f(x) :f∈Hk, x∈X}andg:z∈Hk(X)∝⇕⊣√∫⊔≀→1
2(z−y)2, fory∈[0,1]. Hence,g′(z) =z−yand by
Assumptions 2 and 4
|g′(z)|≤|z|+|y|≤|f(x)|+ 1≤|⟨f,k(·,x)⟩Hk|+ 1≤∥f∥Hkκ1/2+ 1 =κ1/2+ 1,
for somef∈Hkandx∈Xsincez∈Hk(X). As a consequence, we obtain that gis/parenleftbig
κ1/2+ 1/parenrightbig
-Lipschitz, i.e.
|ℓ(f(x),y)−ℓ(f′(x′),y)|≤/parenleftig
κ1/2+ 1/parenrightig
|f(x)−f′(x′)|.
We can then obtain the same generalisation bounds as above. Finally, looking at the approximation term,
En/bracketleftig
ℓ˜fs/bracketrightig
−En/bracketleftig
ℓfHk/bracketrightig
=1
2n/vextenddouble/vextenddouble˜fX
s−Y/vextenddouble/vextenddouble2
2−1
2n/vextenddouble/vextenddoublefX
Hk−Y/vextenddouble/vextenddouble2
2
≤1
2n/vextenddouble/vextenddouble˜fX
s−fX
Hk/vextenddouble/vextenddouble2
2
≤inf
f∈HS
∥f∥Hk≤11
2n/vextenddouble/vextenddoublefX−fX
Hk/vextenddouble/vextenddouble2
2+λn
2
≤inf
γ∈Rs1
n/vextenddouble/vextenddoubleKS⊤γ−fX
Hk/vextenddouble/vextenddouble2
2+λnγ⊤SKS⊤γ+λn
2
≤/parenleftbigg
C2+1
2/parenrightbigg
λn+C2δ2
n.
We obtain the same bound as above without the square root since we do not need to invoke Jensen inequality
as the studied loss is the square loss. Gathering all arguments, we obtain last inequality in Theorem 2.
A.2 Refined analysis in the scalar case
As said in Remark 1, and similarly to Li et al. (2021), we can conduct a refined analysis, leading to faster
convergence rates for the generalization errors, with the following additional assumption.
Assumption 6. There is a constant Bsuch that, for all f∈Hkwe have
E[f−fHk]2≤BE/bracketleftig
ℓf−ℓfHk/bracketrightig
. (17)
It has be shown that many loss functions satisfy this assumption such as Hinge loss (Steinwart & Christmann,
2008b; Bartlett et al., 2006), truncated quadratic or sigmoid loss (Bartlett et al., 2006). Under Assumptions 1
to 5 and 6, the following result holds:
Theorem 6. We define, for δ∈(0,1), the following sub-root function ˆψn
ˆψn(r) = 2LC1/parenleftigg
2
nn/summationdisplay
i=1min{b2r,µi}/parenrightigg1/2
+C2
nlog1
δ,
and let ˆr∗
Hkbe the fixed point of ˆψn, i.e., ˆψn/parenleftbig
ˆr∗
Hk/parenrightbig
=ˆr∗
Hk. Then, we have for all D> 1andδ∈(0,1)with
probability greater than 1−δ,
E/bracketleftig
ℓ˜fs/bracketrightig
≤E/bracketleftig
ℓfHk/bracketrightig
+D
D−1/parenleftbigg
LC/radicalbig
λn+δ2n+λn
2/parenrightbigg
+12D
Bˆr⋆
Hk+2C3
nlog1
δ, (18)
whereCis as in Theorem 2 and C1,C2,C3andb2are some constants and ˆr∗
Hkcan be upper bounded by
ˆr⋆
Hk≤min
0≤h≤n
b0h
n+/radicaligg
1
n/summationdisplay
i>hµi
,
whereBandb0are some constants.
19Published in Transactions on Machine Learning Research (09/2023)
Hence, we see that, in order to obtain faster learning rates than Theorem 2 as Li et al. (2021), we need to
replaceδ2
nbyˆr⋆2
Hk. However, according to the expression of ˆψnand its dependencies to non-explicit constants,
it appears very difficult to prove that/parenleftig
1
n/summationtextn
i=1min(ˆr⋆2
Hk,µi)/parenrightig1/2
≤ˆr⋆2
Hk, which is a necessary condition to
prove that a sketch matrix SisK-satisfiable. We still prove the above result following the proof of Theorem
4 in Li et al. (2021), and leave as an open problem to find faster rates than δn.
Proof.We use the decomposition of the expected learning risk from Li et al. (2021)
E[ℓ˜fs]−E[ℓfHk] =E[ℓ˜fs]−D
D−1En[ℓ˜fs]
+D
D−1/parenleftig
En[ℓ˜fs]−En[ℓfHk]/parenrightig
+D
D−1En[ℓfHk]−E[ℓfHk],
forD> 1. The generalization errors can be bounded as in Li et al. (2021) by Theorem 6. The approximation
error is bounded using (16).
A.3 Proof of Theorem 4
Here, the proof uses the same decomposition of the excess risk (Equation (7)) as in single output settings.
Since some works (Maurer, 2016) exist to easily extend generalisation bounds of functions in scalar-valued
RKHS to functions in vector-valued RKHS, the main challenge here is to derive an approximation error for
the multiple output settings. Hence, let us first state a needed intermediate results that we will prove later.
Lemma 2. For allf∈ HK, such that∥f∥HK≤1, we have z⊤/parenleftbig
K−1⊗M−1/parenrightbig
z≤1, wherez=/parenleftbig
f(x1)⊤,...,f (xn)⊤/parenrightbig⊤∈Rnd.
We are now equipped to state the main result that generalises Lemma 2 from Yang et al. (2017).
Lemma 3. LetZ⋆=(f⋆(x1),...,f⋆(xn))⊤∈Rn×dfor anyf⋆∈HKsuch that∥f⋆∥HK≤1, whereK=kM,
andS∈Rs×naK- satisfiable matrix. Then we have
inf
Γ∈Rs×d1
n∥KS⊤ΓM−Z⋆∥2
F+λnTr/parenleftbig
KS⊤ΓMΓ⊤S/parenrightbig
≤C2/parenleftbig
∥M∥opδ2
n+λn/parenrightbig
, (19)
whereC= 1 +√
6candcis the universal constant from Definition 3.
Proof.We adapt the proof of Lemma 2 from Yang et al. (2017) to the multidimensional case. If we are able
to find a Γ∈Rs×dsuch that
1
n∥KS⊤ΓM−Z⋆∥2
F+λnTr/parenleftbig
KS⊤ΓMΓ⊤S/parenrightbig
≤C2/parenleftbig
∥M∥opδ2
n+λn/parenrightbig
,
then in particular it also holds true for the minimizer. We recall the eigendecompositions1
nK=Knorm =
UDU⊤andM=V∆V⊤. Then the above problem rewrites as
∥D˜S⊤ΓV∆−Θ⋆∥2
F+λnTr/parenleftbig˜SD˜S⊤ΓMΓ/parenrightbig
≤C2/parenleftbig
∥M∥opδ2
n+λn/parenrightbig
,
where ˜S=SUandΘ⋆=1
n1/2U⊤Z⋆V. We can rewrite θ⋆=(Θ⋆
1:,..., Θ⋆
n:)⊤=1
n1/2/parenleftbig
U⊤⊗V⊤/parenrightbig
z⋆, hence
∥/parenleftbig
D−1/2⊗∆−1/2/parenrightbig
θ⋆∥2
2=z⋆⊤/parenleftbig
K−1⊗M−1/parenrightbig
z⋆, withz⋆=(Z⋆
1:,...,Z⋆
n:)⊤=/parenleftbig
f⋆(x1)⊤,...,f⋆(xn)⊤/parenrightbig⊤. By
Lemma 2, we have that ∥/parenleftbig
D−1/2⊗∆−1/2/parenrightbig
θ⋆∥2≤1, and using the notation γ=(Γ1:,..., Γs:)⊤∈Rsd, we
can rewrite the above problem as finding a γsuch that
∥θ⋆−/parenleftbig
D˜S⊤⊗∆V⊤/parenrightbig
γ∥2
2+λnγ⊤/parenleftbig˜SD˜S⊤⊗M/parenrightbig
γ≤C2/parenleftbig
∥M∥opδ2
n+λn/parenrightbig
.
20Published in Transactions on Machine Learning Research (09/2023)
As in (Yang et al., 2017), we partition vector θ⋆∈Rndinto two sub-vectors, namely θ⋆
1∈Rdndand
θ⋆
2∈R(n−dn)d, the diagonal matrix Dinto two blocks D1∈Rdn×dnandD2∈R(n−dn)×(n−dn)and finally,
under the condition s > dn, we let ˜S1∈Rs×dnand ˜S2∈Rs×(n−dn)denote the left and right block of ˜S
respectively. By the K-satisfiablility of Swe have
∥˜S⊤
1˜S1−Idn∥op≤1
2and∥˜S2D1/2
2∥op≤cδ2
n. (20)
By the first inequality, we have that ˜S⊤
1˜S1is invertible. In fact, assuming that there exists x∈Rdnsuch that
∥x∥2= 1and ˜S⊤
1˜S1X= 0, then/vextenddouble/vextenddouble(˜S⊤
1˜S1−Idn)x/vextenddouble/vextenddouble= 1>1
2. Then, we can define
ˆγ=/parenleftig
˜S1/parenleftbig˜S⊤
1˜S1/parenrightbig−1D−1
1⊗V∆−1/parenrightig
θ⋆
1. (21)
Hence,/vextenddouble/vextenddoubleθ⋆−(D˜S⊤⊗∆V⊤)ˆγ/vextenddouble/vextenddouble2
2=/vextenddouble/vextenddoubleθ⋆
1−(D1˜S⊤
1⊗∆V⊤)ˆγ/vextenddouble/vextenddouble2
2+/vextenddouble/vextenddoubleθ⋆
2−(D2˜S⊤
2⊗∆V⊤)ˆγ/vextenddouble/vextenddouble2
2,
and we have
/vextenddouble/vextenddoubleθ⋆
1−(D1˜S⊤
1⊗∆V⊤)ˆγ/vextenddouble/vextenddouble2
2=/vextenddouble/vextenddoubleθ⋆
1−/parenleftbig
D1˜S⊤
1⊗∆V⊤/parenrightbig/parenleftbig˜S1(˜S⊤
1˜S1)−1D−1
1⊗V∆−1/parenrightbig
θ⋆
1/vextenddouble/vextenddouble2
2
=/vextenddouble/vextenddoubleθ⋆
1−/parenleftig
D1˜S⊤
1˜S1(˜S⊤
1˜S1)−1D−1
1⊗∆V⊤V∆−1/parenrightig
θ⋆
1/vextenddouble/vextenddouble2
2
=∥θ⋆
1−θ⋆
1∥2
2
= 0,
and
/vextenddouble/vextenddoubleθ⋆
2−/parenleftbig
D2˜S⊤
2⊗∆V⊤/parenrightbig
ˆγ/vextenddouble/vextenddouble
2
=/vextenddouble/vextenddouble/vextenddoubleθ⋆
2−/parenleftig
D2˜S⊤
2˜S1/parenleftbig˜S⊤
1˜S1/parenrightbig−1D−1
1⊗Ip/parenrightig
θ⋆
1/vextenddouble/vextenddouble/vextenddouble
2
≤∥θ⋆
2∥2+/vextenddouble/vextenddouble/vextenddouble/parenleftig
D2˜S⊤
2˜S1/parenleftbig˜S⊤
1˜S1/parenrightbig−1D−1/2
1D−1/2
1⊗∆1/2∆−1/2/parenrightig
θ⋆
1/vextenddouble/vextenddouble/vextenddouble
2
=∥θ⋆
2∥2+/vextenddouble/vextenddouble/vextenddouble/parenleftig
D2˜S⊤
2˜S1/parenleftbig˜S⊤
1˜S1/parenrightbig−1D−1/2
1⊗∆1/2/parenrightig/parenleftig
D−1/2
1⊗∆−1/2/parenrightig
θ⋆
1/vextenddouble/vextenddouble/vextenddouble
2
≤∥θ⋆
2∥2+/vextenddouble/vextenddouble/vextenddouble/parenleftig
D2˜S⊤
2˜S1/parenleftbig˜S⊤
1˜S1/parenrightbig−1D−1/2
1⊗∆1/2/parenrightig/vextenddouble/vextenddouble/vextenddouble
op/vextenddouble/vextenddouble/vextenddouble/parenleftig
D−1/2
1⊗∆−1/2/parenrightig
θ⋆
1/vextenddouble/vextenddouble/vextenddouble
2
=∥θ⋆
2∥2+/vextenddouble/vextenddouble/vextenddoubleD2˜S⊤
2˜S1/parenleftbig˜S⊤
1˜S1/parenrightbig−1D−1/2
1/vextenddouble/vextenddouble/vextenddouble
op/vextenddouble/vextenddouble/vextenddouble∆1/2/vextenddouble/vextenddouble/vextenddouble
op/vextenddouble/vextenddouble/vextenddouble/parenleftig
D−1/2
1⊗∆−1/2/parenrightig
θ⋆
1/vextenddouble/vextenddouble/vextenddouble
2
≤∥θ⋆
2∥2+/vextenddouble/vextenddouble/vextenddoubleD1/2
2/vextenddouble/vextenddouble/vextenddouble
op/vextenddouble/vextenddouble/vextenddouble˜S2D1/2
2/vextenddouble/vextenddouble/vextenddouble
op/vextenddouble/vextenddouble˜S1/vextenddouble/vextenddouble
op/vextenddouble/vextenddouble/vextenddouble/parenleftbig˜S⊤
1˜S1/parenrightbig−1/vextenddouble/vextenddouble/vextenddouble
op/vextenddouble/vextenddouble/vextenddoubleD−1/2
1/vextenddouble/vextenddouble/vextenddouble
op/vextenddouble/vextenddouble/vextenddouble∆1/2/vextenddouble/vextenddouble/vextenddouble
op/vextenddouble/vextenddouble/vextenddouble/parenleftig
D−1/2
1⊗∆−1/2/parenrightig
θ⋆
1/vextenddouble/vextenddouble/vextenddouble
2.
(22)
We now bound all terms involved in (22). Since∥/parenleftbig
D−1/2⊗∆−1/2/parenrightbig
θ⋆∥2≤1, then∥/parenleftig
D−1/2
1⊗∆−1/2/parenrightig
θ⋆
1∥2≤
1and,
∥θ⋆
2∥2
2=d/summationdisplay
i=1n/summationdisplay
j=dn+1/parenleftig
θ⋆
2ji/parenrightig2
≤δ2
n∥M∥opd/summationdisplay
i=11
∆iin/summationdisplay
j=dn+1/parenleftig
θ⋆
2ji/parenrightig2
µj
≤δ2
n∥M∥opd/summationdisplay
i=1n/summationdisplay
j=1/parenleftig
θ⋆
2ji/parenrightig2
µj∆ii
=δ2
n∥M∥op/vextenddouble/vextenddouble/vextenddouble/parenleftig
D−1/2⊗∆−1/2/parenrightig
θ⋆/vextenddouble/vextenddouble/vextenddouble2
2
≤δ2
n∥M∥op,
21Published in Transactions on Machine Learning Research (09/2023)
sinceµj≤δ2
n, for allj≥dn+ 1and∆ii≤∥M∥opfor all 1≤i≤d. Moreover, since ∥˜S⊤
1˜S1−Idn∥op≤1
2,
∥˜S⊤
1˜S1∥op≤3
2, then∥˜S1∥op≤/radicalig
3
2. Besides, for all x∈Rdnsuch that∥x∥2= 1, we have
|∥˜S⊤
1˜S1x∥2−1|=|∥˜S⊤
1˜S1x∥2−∥x∥2|≤/vextenddouble/vextenddouble/parenleftbig˜S⊤
1˜S1−Idn/parenrightbig
x/vextenddouble/vextenddouble
2≤1
2,
Then, we obtain that ∥˜S⊤
1˜S1x∥2−1≥−1
2and then∥˜S⊤
1˜S1x∥2≥1
2, takingxthe eigenvector of ˜S⊤
1˜S1
corresponding to its smallest eigenvalue, we obtain that ∥/parenleftbig˜S⊤
1˜S1/parenrightbig−1∥−1
op≥1
2, and finally∥/parenleftbig˜S⊤
1˜S1/parenrightbig−1∥op≤2.
Moreover we have
∥D−1/2
1∥op≤1
δn,
∥D1/2
2∥op≤δn,
∥˜S2D1/2
2∥op≤cδn.
Thus,
/vextenddouble/vextenddoubleθ⋆
2−/parenleftbig
D2˜S⊤
2⊗∆V⊤/parenrightbig
ˆγ/vextenddouble/vextenddouble
2≤/parenleftbig
δ2
n∥M∥op/parenrightbig1/2+δncδn/parenleftbigg3
2/parenrightbigg1/2
21
δn∥M∥1/2
op
=/parenleftbig
δ2
n∥M∥op/parenrightbig1/2/parenleftig
1 +c√
6/parenrightig
Finally,/vextenddouble/vextenddoubleθ⋆−/parenleftbig
D˜S⊤⊗∆V⊤/parenrightbig
ˆγ/vextenddouble/vextenddouble2
2≤δ2
n∥M∥op/parenleftig
1 +c√
6/parenrightig2
. (23)
Furthermore, looking into the second term,
ˆγ⊤/parenleftbig˜SD˜S⊤⊗M/parenrightbig
ˆγ=/vextenddouble/vextenddouble/vextenddouble/parenleftig
D1/2˜S⊤⊗∆1/2V⊤/parenrightig
ˆγ/vextenddouble/vextenddouble/vextenddouble2
2
=/vextenddouble/vextenddouble/vextenddouble/parenleftig
D1/2
1˜S⊤
1⊗∆1/2V⊤/parenrightig
ˆγ/vextenddouble/vextenddouble/vextenddouble2
2+/vextenddouble/vextenddouble/vextenddouble/parenleftig
D1/2
2˜S⊤
2⊗∆1/2V⊤/parenrightig
ˆγ/vextenddouble/vextenddouble/vextenddouble2
2
=/vextenddouble/vextenddouble/vextenddouble/parenleftig
D−1/2
1⊗∆−1/2/parenrightig
θ⋆
1/vextenddouble/vextenddouble/vextenddouble2
2+/vextenddouble/vextenddouble/vextenddouble/parenleftig
D1/2
2˜S⊤
2˜S1/parenleftbig˜S⊤
1˜S1/parenrightbig−1D−1
1⊗∆−1/2/parenrightig
θ⋆
1/vextenddouble/vextenddouble/vextenddouble2
2
≤1 +/vextenddouble/vextenddouble/vextenddouble˜S2D1/2
2/vextenddouble/vextenddouble/vextenddouble2
op/vextenddouble/vextenddouble˜S1/vextenddouble/vextenddouble2
op/vextenddouble/vextenddouble/vextenddouble/parenleftbig˜S⊤
1˜S1/parenrightbig−1/vextenddouble/vextenddouble/vextenddouble2
op/vextenddouble/vextenddouble/vextenddoubleD−1/2
1/vextenddouble/vextenddouble/vextenddouble2
op/vextenddouble/vextenddouble/vextenddouble/parenleftig
D−1/2
1⊗∆−1/2/parenrightig
θ⋆
1/vextenddouble/vextenddouble/vextenddouble2
2
≤1 +c2δ2
n3
241
δ2n
= 1 + 6c2
=/parenleftig
1 +√
6c/parenrightig2
−2√
6c
≤/parenleftig
1 +√
6c/parenrightig2
.
Finally, we obtain that
/vextenddouble/vextenddoubleθ⋆−/parenleftbig
D˜S⊤⊗∆V⊤/parenrightbig
ˆγ/vextenddouble/vextenddouble2
2+λnˆγ⊤/parenleftbig˜SD˜S⊤⊗M/parenrightbig
ˆγ≤/parenleftig
1 +√
6c/parenrightig2/parenleftbig
∥M∥opδ2
n+λn/parenrightbig
, (24)
and as a conclusion
inf
Γ∈Rs×d1
n∥KS⊤ΓM−Z⋆∥2
F+λnTr/parenleftbig
KS⊤ΓMΓ⊤S/parenrightbig
≤C2/parenleftbig
∥M∥opδ2
n+λn/parenrightbig
, (25)
whereC= 1 +√
6c.
Now, as for the proof of Theorem 2, let us prove equation first inequality in Theorem 4.
22Published in Transactions on Machine Learning Research (09/2023)
Proof.For any function in B(HK) ={f∈HK:∥f∥HK≤1}, Lemma 1 still holds, then
E[ℓf]≤En[ℓf] +Rn(l◦B(HK)) +/radicalbigg
8 log(2/δ)
n. (26)
Then, using Corollary 1 from Maurer (2016), we have that:
Rn(ℓ◦B(HK))≤√
2LRn(B(HK)), (27)
where
Rn(F) =E
sup
f∈F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
nn/summationdisplay
i=1d/summationdisplay
j=1σijf(xi)j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle|x1,...,xn

=E/bracketleftigg
sup
f∈F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
nn/summationdisplay
i=1⟨σi,f(xi)⟩Rd/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle|x1,...,xn/bracketrightigg
,
whereσ11,...,σnparendindependent Rademacher variables, and for all 1≤i≤n,σi=(σi1,...,σid)⊤.
Hence
Rn(B(HK)) =E/bracketleftigg
sup
∥f∥HK≤1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
nn/summationdisplay
i=1⟨σi,f(xi)⟩Rd/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle|x1,...,xn/bracketrightigg
=E
sup
∥f∥HK≤1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/angbracketleftigg
2
nn/summationdisplay
i=1Kxiσi,f/angbracketrightigg
HK/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle|x1,...,xn

≤2
nE
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1Kxiσi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
HK|x1,...,xn
1/2
=2
nE
n/summationdisplay
i,j=1⟨σi,K(xi,xj)σj⟩Rd|x1,...,xn
1/2
=2
nE
n/summationdisplay
i,j=1k(xi,xj)⟨σi,Mσj⟩Rd|x1,...,xn
1/2
=2
n
n/summationdisplay
i,j=1k(xi,xj)d/summationdisplay
i′,j′=1E[Mi′j′σii′σjj′|x1,...,xn]
1/2
=2
n/parenleftiggn/summationdisplay
i=1k(xi,xi)d/summationdisplay
i′=1Mi′i′/parenrightigg1/2
=2
n(Tr (K⊗M))1/2
Rn(B(HK))≤2
n1/2κ1/2Tr (M)1/2.
Finally, for any function f∈B(HK), for allδ∈(0,1), we have for a probability at least 1−δ,
|E[ℓf]−En[ℓf]|≤4L/radicalbigg
2κ
nTr (M) + 2/radicalbigg
8 log(2/δ)
n. (28)
23Published in Transactions on Machine Learning Research (09/2023)
Now, for the approximation error term, we proceed as in the proof of Theorem 2. Let HS=/braceleftbig
f=/summationtextn
i=1k(·,xi)M/bracketleftbig
S⊤˜Γ/bracketrightbig
i|γ∈Rs×d/bracerightbig
. By Assumptions 2 and 3 and Jensen’s inequality,
En/bracketleftig
ℓ˜fs/bracketrightig
−En/bracketleftig
ℓfHK/bracketrightig
=1
nn/summationdisplay
i=1ℓ/parenleftbig˜fs(xi),yi/parenrightbig
−1
nn/summationdisplay
i=1ℓ(fHK(xi),yi)
≤1
nn/summationdisplay
i=1ℓ/parenleftbig˜fs(xi),yi/parenrightbig
+λn
2/vextenddouble/vextenddouble˜fs/vextenddouble/vextenddouble2
HK−1
nn/summationdisplay
i=1ℓ(fHK(xi),yi)
= inf
f∈HS
∥f∥HK≤11
nn/summationdisplay
i=1ℓ(f(xi),yi)−1
nn/summationdisplay
i=1ℓ(fHK(xi),yi) +λn
2∥f∥2
HK
≤inf
f∈HS
∥f∥HK≤1L
nn/summationdisplay
i=1∥f(xi)−fHK(xi)∥2+λn
2
≤Linf
f∈HS
∥f∥HK≤1/radicaltp/radicalvertex/radicalvertex/radicalbt1
nn/summationdisplay
i=1∥f(xi)−fHK(xi)∥2
2+λn
2
=L/radicaltp/radicalvertex/radicalvertex/radicalbtinf
f∈HS
∥f∥HK≤11
n/vextenddouble/vextenddoublefX−fX
HK/vextenddouble/vextenddouble2
F+λn
2,
where, for any f∈HK,fX=(f(x1),...,f (xn))⊤∈Rn×d. Let ˜fR
s=/summationtextn
i=1k(·,xi)M/bracketleftbig
S⊤˜ΓR/bracketrightbig
i, where ˜ΓRis
a solution to
inf
Γ∈Rs×d1
n/vextenddouble/vextenddoubleKS⊤ΓM−fX
HK/vextenddouble/vextenddouble2
F+λnTr/parenleftbig
KS⊤ΓMΓ⊤S/parenrightbig
.
It is easy to check that ˜fR
sis also a solution to
inf
f∈HS
∥f∥HK≤∥˜fRs∥HK1
n/vextenddouble/vextenddoublefX−fX
HK/vextenddouble/vextenddouble2
F.
Since we have∥˜fR
s∥HK≤1by Assumption 2, it holds
inf
f∈HS
∥f∥HK≤11
n/vextenddouble/vextenddoublefX−fX
HK/vextenddouble/vextenddouble2
F≤ inf
f∈HS
∥f∥HK≤∥˜fRs∥HK1
n/vextenddouble/vextenddoublefX−fX
HK/vextenddouble/vextenddouble2
F
= inf
Γ∈Rs×d1
n/vextenddouble/vextenddoubleKS⊤ΓM−fX
HK/vextenddouble/vextenddouble2
F+λnTr/parenleftbig
KS⊤ΓMΓ⊤S/parenrightbig
.
As a consequence, we have
En[ℓ˜fs]−En[ℓfHk]≤L/radicalbigg
inf
Γ∈Rs×d1
n∥KS⊤ΓM−fX
Hk∥2
F+λnTr (KS⊤ΓMΓ⊤S) +λn
2.
Finally, by Lemma 3 and Equation (28), we obtain the result stated.
Furthermore, we give the proof of the second claim, i.e. the excess risk bound for kernel ridge multi-output
regression.
Proof.We now assume that the outputs are bounded, hence, without loss of generality, Y⊂B/parenleftbig
Rd/parenrightbig
. First, we
prove Lipschitz-continuity of the square loss under Assumptions 2 and 4. Let g:z∈HK(X)∝⇕⊣√∫⊔≀→1
2∥z−y∥2
2.
24Published in Transactions on Machine Learning Research (09/2023)
We have that∇g(z) =z−y, and hence∥∇g(z)∥2≤∥f(x)∥2+ 1, for some f∈HKandx∈X. By
Assumptions 2 and 4 and Cauchy-Schwartz inequality, it is easy to check that
∥f(x)∥2
2≤/parenleftig
κ∥M∥op∥f(x)∥2
2/parenrightig1/2
,
which gives us that ∥f(x)∥2≤κ1/2∥M∥1/2
opand then∥∇g(z)∥2≤κ1/2∥M∥1/2
op+ 1. We finally obtain that
|ℓ(f(x),y)−ℓ(f′(x′),y)|≤/parenleftig
κ1/2∥M∥1/2
op+ 1/parenrightig
||f(x)−f′(x′)∥2.
We can then obtain the same generalisation bounds as above. Finally, looking at the approximation term,
En/bracketleftig
l˜fs/bracketrightig
−En/bracketleftig
lfHk/bracketrightig
=1
2n/vextenddouble/vextenddouble˜fX
s−Y/vextenddouble/vextenddouble2
2−1
2n/vextenddouble/vextenddoublefX
Hk−Y/vextenddouble/vextenddouble2
2
≤1
2n/vextenddouble/vextenddouble˜fX
s−fX
Hk/vextenddouble/vextenddouble2
2
≤inf
f∈HS
∥f∥Hk≤11
2n/vextenddouble/vextenddoublefX−fX
Hk/vextenddouble/vextenddouble2
2+λn
2
≤inf
γ∈Rs1
n/vextenddouble/vextenddoubleKS⊤γ−fX
Hk/vextenddouble/vextenddouble2
2+λnγ⊤SKS⊤γ+λn
2
≤/parenleftbigg
C2+1
2/parenrightbigg
λn+C2δ2
n.
Here again, as in second claim of Theorem 2, we can directly use bound (19)and then, in combination with
(28), we obtain the stated second claim in Theorem 4.
Finally, we here prove Lemma 2.
Proof.Letf∈HKsuch that∥f∥HK≤1andz=/parenleftbig
f(x1)⊤,...,f (xn)⊤/parenrightbig⊤∈Rnd. We define the linear
operatorSX:HK→Rndsuch thatSX(f) =/parenleftbig
f(x1)⊤,...,f (xn)⊤/parenrightbig⊤for allf∈HK. Then for all f∈HK
andz=/parenleftbig
z⊤
1,...,z⊤
n/parenrightbig⊤∈Rndwe have
⟨SX(f),z⟩Rnd=n/summationdisplay
i=1⟨f(xi),zi⟩Rd=n/summationdisplay
i=1⟨f,Kxizi⟩HK=/angbracketleftigg
f,n/summationdisplay
i=1Kxizi/angbracketrightigg
HK=⟨f,S⋆
X(z)⟩HK.
Hence
z⊤/parenleftbig
K−1⊗M−1/parenrightbig
z=/angbracketleftig
(K⊗M)−1SX(f),Sx(f)/angbracketrightig
Rnd=/angbracketleftig
S⋆
X/parenleftig
(K⊗M)−1SX(f)/parenrightig
,f/angbracketrightig
HK.
We recall the eigendecompositions of KandM
K=U(nD)U⊤=n/summationdisplay
i=1nµiuiu⊤
i
M=V∆V⊤=d/summationdisplay
j=1µivjv⊤
j.
25Published in Transactions on Machine Learning Research (09/2023)
Then,
K⊗M=/parenleftiggn/summationdisplay
i=1nµiuiu⊤
i/parenrightigg
⊗
d/summationdisplay
j=1µivjv⊤
j

=n/summationdisplay
i=1d/summationdisplay
j=1nµiµi/parenleftbig
uiu⊤
i/parenrightbig
⊗/parenleftbig
vjv⊤
j/parenrightbig
=n/summationdisplay
i=1d/summationdisplay
j=1nµiµi(ui)⊗(vj)/parenleftbig
u⊤
i/parenrightbig
⊗/parenleftbig
v⊤
j/parenrightbig
=n/summationdisplay
i=1d/summationdisplay
j=1nµiµi(ui)⊗(vj) ((ui)⊗(vj))⊤,
and for all 1≤i,i′≤nand1≤j,j′≤d, if(i,i′)̸=(j,j′), then (ui)⊗(vj)⊤((ui′)⊗(vj′))= 0and otherwise
(ui)⊗(vj)⊤((ui′)⊗(vj′)) = 1. Then, this allows to show that the operator norm of a Kronecker product is
the product of the operator norms, and that
(K⊗M)−1=n/summationdisplay
i=1d/summationdisplay
j=1(nµiµi)−1(ui)⊗(vj) ((ui)⊗(vj))⊤. (29)
We define, for all 1≤i≤nand1≤j≤d,
φij=1√nµiµjn/summationdisplay
l=1uilKxlvj. (30)
By definition, span/parenleftig
(φij)1≤i≤n,1≤j≤d/parenrightig
⊂span/parenleftig
(Kxivj)1≤i≤n,1≤j≤d/parenrightig
and we show that the φijs are orthonor-
mal,
⟨φij,φi′j′⟩HK=/angbracketleftigg
1√nµiµjn/summationdisplay
l=1uilKxlvj,1√nµi′µj′n/summationdisplay
l′=1ui′
l′Kxl′vj′/angbracketrightigg
HK
=1√nµiµj1√nµi′µj′n/summationdisplay
l,l′uilui′
l′/angbracketleftbig
Kxlvj,Kxl′vj′/angbracketrightbig
HK
=1√nµiµj1√nµi′µj′n/summationdisplay
l,l′uilui′
l′/angbracketleftbig
vj,Kxl,xl′vj′/angbracketrightbig
Rd
=1√nµiµj1√nµi′µj′n/summationdisplay
l,l′uilui′
l′k(xl,xl′)⟨vj,Mvj′⟩Rd
=1√nµiµj1√nµi′µj′n/summationdisplay
l,l′uilui′
l′k(xl,xl′)µj′⟨vj,vj′⟩Rd
= 0ifj̸=j′.
Otherwise, if j=j′,
⟨φij,φi′j′⟩HK=1√nµi1√nµi′n/summationdisplay
l,l′uilui′
l′k(xl,xl′)
=1√nµi1√nµi′⟨Kui,ui′⟩Rn
=1√nµi1√nµi′nµi⟨ui,ui′⟩Rn
= 0ifi̸=i′.
26Published in Transactions on Machine Learning Research (09/2023)
Hence,⟨φij,φi′j′⟩HK= 0if(i,i′)̸= (j,j′)and if (i,i′) = (j,j′),
⟨φij,φi′j′⟩HK= 1.
Finally, span/parenleftig
(φij)1≤i≤n,1≤j≤d/parenrightig
⊂span/parenleftig
(Kxivj)1≤i≤n,1≤j≤d/parenrightig
and dim/parenleftig
(φij)1≤i≤n,1≤j≤d/parenrightig
=nd=
dim/parenleftig
(Kxivj)1≤i≤n,1≤j≤d/parenrightig
, hence (φij)1≤i≤n,1≤j≤dyields an orthonormal basis of span/parenleftig
(Kxivj)1≤i≤n,1≤j≤d/parenrightig
.
As a consequence, all f∈HKcan be decomposed as f=f1+f2, withf1∈span/parenleftig
(Kxivj)1≤i≤n,1≤j≤d/parenrightig
and
f2∈span/parenleftig
(Kxivj)1≤i≤n,1≤j≤d/parenrightig⊥
. Thus, for all y∈Rd,ycan be written as y=/summationtextd
j=1yjvjand
⟨SX(f),z⟩Rnd=n/summationdisplay
i=1⟨f(xi),zi⟩Rd
=n/summationdisplay
i=1d/summationdisplay
j=1zij⟨f(xi),vj⟩Rd
=n/summationdisplay
i=1d/summationdisplay
j=1zij⟨f,Kxivj⟩HK
=n/summationdisplay
i=1d/summationdisplay
j=1zij⟨f1,Kxivj⟩HK+n/summationdisplay
i=1d/summationdisplay
j=1zij⟨f2,Kxivj⟩HK
=n/summationdisplay
i=1d/summationdisplay
j=1zij⟨f1,Kxivj⟩HK
=⟨SX(f1),z⟩Rnd.
Hence, let f∈HKsuch that∥f∥HK≤1, written as f=/summationtextn
i=1/summationtextd
j=1fijφij+f⊥, withfij∈Rfor all
1≤i≤nand1≤j≤dand such that/summationtextn
i=1/summationtextd
j=1f2
ij≤1andf⊥∈span/parenleftig
(Kxivj)1≤i≤n,1≤j≤d/parenrightig⊥
such that
∥f⊥∥HK≤1(since∥f∥HK=/summationtextn
i=1/summationtextd
j=1f2
ij+∥f⊥∥HK≤1, we have that
SX(f) =n/summationdisplay
i=1d/summationdisplay
j=1fijSX(φij),
and, for all 1≤l≤n,
φij(xl) =1√nµiµjn/summationdisplay
l′=1uil′k(xl′,xl)Mvj
=/radicalbiggµj
nµiK⊤
l:uivj,
and then
SX(φij) =/radicalbiggµj
nµi(Kui)⊗vj=√nµiµjui⊗vj. (31)
Finally,
SX(f) =n/summationdisplay
i=1d/summationdisplay
j=1fij(nµiµj)1/2ui⊗vj. (32)
27Published in Transactions on Machine Learning Research (09/2023)
Besides,
(K⊗M)−1SX(f) =
n/summationdisplay
i=1d/summationdisplay
j=1(nµiµi)−1(ui)⊗(vj) ((ui)⊗(vj))⊤

×
n/summationdisplay
i′=1d/summationdisplay
j′=1fi′j′(nµi′µj′)1/2ui′⊗vj′

=n/summationdisplay
i=1d/summationdisplay
j=1fij(nµiµi)−1/2ui⊗vj.
Then,
S⋆
X/parenleftig
(K⊗M)−1SX(f)/parenrightig
=n/summationdisplay
i=1d/summationdisplay
j=1fij(nµiµi)−1/2S⋆
X(ui⊗vj)
=n/summationdisplay
i=1d/summationdisplay
j=1fij(nµiµi)−1/2n/summationdisplay
i′=1Kxi(uii′vj),
and finally,
/angbracketleftig
S⋆
X/parenleftig
(K⊗M)−1SX(f)/parenrightig
,f/angbracketrightig
HK=n/summationdisplay
i,i′=1d/summationdisplay
j,j′=1n/summationdisplay
l=1fijfi′j′(nµiµi)−1/2uil⟨Kxlvj,φi′j′⟩HK
=n/summationdisplay
i,i′=1d/summationdisplay
j,j′=1fijfi′j′/angbracketleftigg
(nµiµi)−1/2n/summationdisplay
l=1uilKxlvj,φi′j′/angbracketrightigg
HK
=n/summationdisplay
i,i′=1d/summationdisplay
j,j′=1fijfi′j′⟨φij,φi′j′⟩HK
=n/summationdisplay
i=1d/summationdisplay
j=1f2
ij
/angbracketleftig
S⋆
X/parenleftig
(K⊗M)−1SX(f)/parenrightig
,f/angbracketrightig
HK≤1.
Thus, we do have the ellipse constraint
∥/parenleftig
K−1/2⊗M−1/2/parenrightig
z∥2≤1. (33)
A.4 Proof of Theorem 5
A.4.1 First claim of K-satisfiability
Let us now prove the first claim (l.h.s. of equation (4)) of the K-satisfiability for p-SR andp-SG sketches. It
is articulated around the following two lemmas.
Lemma 4. LetM∈Rd×dbe a symmetric matrix, ε∈(0,1), andCεbe anε-cover ofBd. Then we have
∥M∥op≤1
1−2ε−ε2sup
v∈Cε/vextendsingle/vextendsingle⟨v,Mv⟩/vextendsingle/vextendsingle.
Proof.LetM,εandCεas in Lemma 4. Let u∈Bd. By definition, there exist v∈Cεandw∈Bdsuch that
u=v+εw. We thus have
⟨u,Mu⟩=⟨v,Mv⟩+ 2ε⟨v,Mw⟩+ε2⟨w,Mw⟩. (34)
28Published in Transactions on Machine Learning Research (09/2023)
Taking the supremum on both sides of (34) we obtain
sup
u∈Bd|⟨u,Mu⟩|= sup
v∈Cε,w∈Bd/parenleftig
|⟨v,Mv⟩|+ 2ε|⟨v,Mw⟩|+ε2|⟨w,Mw⟩|/parenrightig
≤sup
v∈Cε|⟨v,Mv⟩|+ 2εsup
v∈Cε,w∈Bd|⟨v,Mw⟩|+ε2sup
w∈Bd|⟨w,Mw⟩|
≤sup
v∈Cε|⟨v,Mv⟩|+ 2ε sup
v′∈Bd,w∈Bd|⟨v′,Mw⟩|+ε2∥M∥op
= sup
v∈Cε|⟨v,Mv⟩|+/parenleftbig
2ε+ε2/parenrightbig
∥M∥op,
or again
∥M∥op≤1
1−2ε−ε2sup
v∈Cε/vextendsingle/vextendsingle⟨v,Mv⟩/vextendsingle/vextendsingle.
Lemma 5. LetS∈Rs×nbe ap-SR or ap-SG sketch. Let v∈Bn, then for every t>0, we have
P/braceleftigg
/vextendsingle/vextendsingle∥Sv∥2
2−∥v∥2
2/vextendsingle/vextendsingle>4
p/radicalbigg
2t
s+4t
sp/bracerightigg
≤2e−t.
Proof.The proof of Lemma 5 is largely adapted from the proof of Theorem 2.13 in Boucheron et al. (2013).
LetS∈Rs×nbe ap-SR or ap-SG sketch, and v∈Bn. It is easy to check that for all i≤swe have
E/bracketleftig
[Sv]2
i/bracketrightig
=1
s∥v∥2
2, such that
/vextendsingle/vextendsingle∥Sv∥2
2−∥v∥2
2/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingles/summationdisplay
i=1/parenleftbigg
[Sv]2
i−1
s∥v∥2
2/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
The proof then consists in applying Bernstein’s inequality (Boucheron et al., 2013, Theorem 2.10) to the
random variables [Sv]2
i. We now have to find some constants νandcsuch that/summationtexts
i=1E/bracketleftig
[Sv]4
i/bracketrightig
≤νand
s/summationdisplay
i=1E/bracketleftig
[Sv]2q
i/bracketrightig
≤q!
2νcq−2for allq≥3.
From(12)and(13), it is easy to check that the Sijare independent and 1/(sp)sub-Gaussian. Then, for all
λ∈R, we have
E/bracketleftbig
exp (λ[Sv]i)/bracketrightbig
=E
exp
λn/summationdisplay
j=1Sijvj


=n/productdisplay
j=1E/bracketleftbig
exp (λSijvj)/bracketrightbig
≤exp/parenleftbiggλ2
2sp∥v∥2
2/parenrightbigg
≤exp/parenleftbiggλ2
2sp/parenrightbigg
.
The random variable [Sv]iis therefore 1/(sp)sub-Gaussian, and Theorem 2.1 from Boucheron et al. (2013)
yields that for every integer q≥2it holds
E/bracketleftig
[Sv]2q
i/bracketrightig
≤q!
24/parenleftbigg2
sp/parenrightbiggq
≤q!
2/parenleftbigg4
sp/parenrightbiggq
.
29Published in Transactions on Machine Learning Research (09/2023)
Choosingq= 2, we obtain
s/summationdisplay
i=1E/bracketleftig
[Sv]4
i/bracketrightig
≤s/summationdisplay
i=1/parenleftbigg4
sp/parenrightbigg2
=16
sp2,
such that we can choose ν= 16/(sp2)andc= 4/(sp). Applying Theorem 2.10 from Boucheron et al. (2013)
to the random variables [Sv]2
ifinally gives that for any t>0it holds
P/braceleftigg
/vextendsingle/vextendsingle∥Sv∥2
2−∥v∥2
2/vextendsingle/vextendsingle>4
p/radicalbigg
2t
s+4t
sp/bracerightigg
≤2e−t.
Proof of the first claim of the K-satisfiability. LetK∈Rn×nbe a Gram matrix, and S∈Rs×nbe a
p-SR or ap-SG sketch. Recall that we want to prove that there exists c0>0such that
P/braceleftbigg/vextenddouble/vextenddoubleU⊤
1S⊤SU1−Idn/vextenddouble/vextenddouble
op>1
2/bracerightbigg
≤2e−c0s,
whereK/n =UDU⊤is the SVD of K, andU1∈Rn×dncontains the left part of U. Letε∈(0,1), and
Cε={v1,...,vNε}be anε-cover ofBdn. We know that such a covering exists with cardinality Nε≤/parenleftbig
1 +2
ε/parenrightbigdn,
see e.g., Matoušek (2013). Let Q=U⊤
1S⊤SU1−Idn, applying Lemma 4, we have
P/braceleftbigg
∥Q∥op>1
2/bracerightbigg
≤P/braceleftbigg
sup
i≤Nε/vextendsingle/vextendsingle⟨vi,Qvi⟩/vextendsingle/vextendsingle>1−2ε−ε2
2/bracerightbigg
≤/summationdisplay
i≤NεP/braceleftbigg/vextendsingle/vextendsingle⟨vi,Qvi⟩/vextendsingle/vextendsingle>1−2ε−ε2
2/bracerightbigg
=/summationdisplay
i≤NεP/braceleftbigg/vextendsingle/vextendsingle∥Swi∥2
2−∥wi∥2
2/vextendsingle/vextendsingle>1−2ε−ε2
2/bracerightbigg
, (35)
wherewi=U1vi∈Bn. Now, by Lemma 5, for any w∈Bn, we have
P/braceleftigg
/vextendsingle/vextendsingle∥Sw∥2
2−∥w∥2
2/vextendsingle/vextendsingle>4
p/radicalbigg
2t
s+4t
sp/bracerightigg
≤2e−t.
Lets≥32t/(α2p2), for someα≤1. Then, we have4
p/radicalig
2t
s+4t
sp≤α+α2p
8≤2α, and therefore
P/braceleftbig/vextendsingle/vextendsingle∥Sw∥2
2−∥w∥2
2/vextendsingle/vextendsingle>2α/bracerightbig
≤2e−t.
If we takeα= (1−2ε−ε2)/4, we obtain
P/braceleftbigg/vextendsingle/vextendsingle∥Sw∥2
2−∥w∥2
2/vextendsingle/vextendsingle>1−2ε−ε2
2/bracerightbigg
≤2e−t
as long ass≥512t
p2(1−2ε−ε2)2. Now, lett=p2(1−2ε−ε2)2
1024s+ log(Nε), ands≥1024log(1+2/ε)
p2(1−2ε−ε2)2dn. We do have
512t
p2(1−2ε−ε2)2=s
2+512
p2(1−2ε−ε2)2log(Nε)≤s
2+s
2=s,
such that
P/braceleftbigg/vextendsingle/vextendsingle∥Sw∥2
2−∥w∥2
2/vextendsingle/vextendsingle>1−2ε−ε2
2/bracerightbigg
≤2e−t=2e−c0s
Nε,
wherec0=p2(1−2ε−ε2)
1024. Plugging this result into (35), we get that as soon as s≥1024log(1+2/ε)
p2(1−2ε−ε2)2dnit holds
P/braceleftbigg
∥Q∥op>1
2/bracerightbigg
≤2e−c0s.
Finally, we can tune εto optimize the lower bound on s. If we take ε= 0.1, we obtain s≥5120dn/p2, and
c0≥p2/2560.
30Published in Transactions on Machine Learning Research (09/2023)
A.4.2 Second claim of K-satisfiability
We now turn to the proof of the second claim (r.h.s. of equation (4)) of the K-satisfiability for p-SR and
p-SG sketches. It builds upon the following two intermediate results, about the concentration of Lipschitz
functions of Rademacher or Gaussian random variables.
Lemma 6. LetK > 0, and letX1,...,Xnbe independent real random variables with |Xi|≤Kfor all
1≤i≤n. LetF:Rn→Rbe aL-Lipschitz convex function. Then, there exist C,c> 0such that for any λ
one has
P{|F(X)−EF(X)|≥Kλ}≤C′exp/parenleftbig
−c′λ2/L2/parenrightbig
.
Lemma 7. LetX1,...,Xnbe i.i.d. standard Gaussian random variables. Let F:Rn→Rbe aL-Lipschitz
function. Then, there exist C,c> 0such that for any λone has
P{|F(X)−EF(X)|≥λ}≤C′exp/parenleftbig
−c′λ2/L2/parenrightbig
.
The above two lemmas are taken from Tao (2012), see Theorems 2.1.12 and 2.1.13 therein, but are actually well
known results in the literature. In particular, Lemma 6 is adaptated from Talagrand’s inequality (Talagrand,
1995), while Lemma 7 is stated as Theorem 5.6 in Boucheron et al. (2013), with explicit constants. We
however choose the writing by Tao (2012) in order to be consistent with the Rademacher case.
Remark 3. Note that thanks to Lemma 6, we are even able to prove K-satisfiability for any sketch matrix S
whose entries are i.i.d. centered and reduced bounded random variables.
Proof of the second claim of the K-satisfiability. LetK∈Rn×nbe a Gram matrix, and S∈Rn×sbe
ap-SR or ap-SG sketch. Recall that we want to prove that there exist positive constants c,c1,c2>0such
that
P/braceleftig/vextenddouble/vextenddoubleSU2D1/2
2/vextenddouble/vextenddouble
op>cδn/bracerightig
≤c1e−c2s,
whereK/n =UDU⊤is the SVD of K,U2∈Rn×(n−dn)is the right part of U, andD2∈R(n−dn)×(n−dn)
is the right bottom part of D. Note that we have SU2D1/2
2=SU¯D1/2, where ¯D=diag (0dn,D2)∈Rn×n.
Following Yang et al. (2017), we have
/vextenddouble/vextenddoubleSU¯D1/2/vextenddouble/vextenddouble
op= sup
u∈Bs,v∈E|⟨u,Sv⟩|,
whereE=/braceleftbig
v∈Rn:∃w∈Sn−1,v=U¯D1/2w/bracerightbig
. Now, let u1,...uNbe a 1/2-cover ofBs. We know that
such a covering exists with cardinality N≤ 5s. We then have
/vextenddouble/vextenddouble/vextenddoubleSU¯D1/2/vextenddouble/vextenddouble/vextenddouble
op= sup
u∈Bs,v∈E|⟨u,Sv⟩|
≤max
i≤Nsup
v∈E/vextendsingle/vextendsingle/angbracketleftbig
ui,Sv/angbracketrightbig/vextendsingle/vextendsingle+1
2sup
u∈Bs,v∈E|⟨u,Sv⟩|
= max
i≤Nsup
v∈E/vextendsingle/vextendsingle/angbracketleftbig
ui,Sv/angbracketrightbig/vextendsingle/vextendsingle+1
2/vextenddouble/vextenddouble/vextenddoubleSU¯D1/2/vextenddouble/vextenddouble/vextenddouble
op,
and rearranging implies that/vextenddouble/vextenddouble/vextenddoubleSU¯D1/2/vextenddouble/vextenddouble/vextenddouble
op≤2 max
i≤Nsup
v∈E/vextendsingle/vextendsingle/angbracketleftbig
ui,Sv/angbracketrightbig/vextendsingle/vextendsingle.
Hence, for every c>0we have
P/parenleftbigg/vextenddouble/vextenddouble/vextenddoubleSU2D1/2
2/vextenddouble/vextenddouble/vextenddouble
op>cδn/parenrightbigg
≤P/parenleftbigg
max
i≤Nsup
v∈E/vextendsingle/vextendsingle/angbracketleftbig
ui,Sv/angbracketrightbig/vextendsingle/vextendsingle>c
2δn/parenrightbigg
≤/summationdisplay
i≤NP/braceleftbigg
sup
v∈E/vextendsingle/vextendsingle/angbracketleftbig
ui,Sv/angbracketrightbig/vextendsingle/vextendsingle>c
2δn/bracerightbigg
. (36)
31Published in Transactions on Machine Learning Research (09/2023)
Now, recall that
S=1√spB◦R,
whereB∈Rn×sis filled with i.i.d. Bernoulli random variables with parameter p,R∈Rn×sis filled with
i.i.d. Rademacher or Gaussian random variables for p-SR andp-SG sketches respectively, and ◦denotes the
Hadamard (termwise) matrix product. The next step of the proof consists in controlling the right-hand side
of(36)by showing that, conditionally on B, we have Lipschitz functions of Rademacher or Gaussian random
variables, whose deviations can be bounded using Lemmas 6 and 7. Therefore, from now on we assume Bto
be fixed, and only consider the randomness with respect to R. Letu∈Bs, and define F:Rn×s→Ras
F(R) =1√spsup
v∈E/vextendsingle/vextendsingle⟨u,(B◦R)v⟩/vextendsingle/vextendsingle.
It is direct to check that Fis a convex function. Moreover, we have
√spF(R) = sup
v∈E|⟨u,(B◦R)v⟩|
= sup
v∈Sn−1|⟨u,(B◦R)U¯D1/2v⟩|
= sup
v∈Sn−1|⟨¯D1/2U⊤(B◦R)⊤u,v⟩|
=/vextenddouble/vextenddouble/vextenddouble¯D1/2U⊤(B◦R)⊤u/vextenddouble/vextenddouble/vextenddouble
2.
Thus, for any R,R′we have
√sp/vextendsingle/vextendsingleF(R)−F(R′)/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextenddouble/vextenddouble/vextenddouble¯D1/2U⊤(B◦R)⊤u/vextenddouble/vextenddouble/vextenddouble
2−/vextenddouble/vextenddouble/vextenddouble¯D1/2U⊤(B◦R′)⊤u/vextenddouble/vextenddouble/vextenddouble
2/vextendsingle/vextendsingle/vextendsingle
≤/vextenddouble/vextenddouble/vextenddouble¯D1/2U⊤/parenleftbig
B◦(R−R′)/parenrightbig⊤u/vextenddouble/vextenddouble/vextenddouble
2
≤/vextenddouble/vextenddouble/vextenddouble¯D1/2/vextenddouble/vextenddouble/vextenddouble
op/vextenddouble/vextenddoubleU⊤/vextenddouble/vextenddouble
op∥B◦(R−R′)∥op∥u∥2
≤δn∥B◦(R−R′)∥F
≤δn∥R−R′∥F, (37)
such thatFis/radicalbig
δ2n/(sp)-Lipschitz. Moreover, we have
√spE[F(R)] =E/bracketleftig/vextenddouble/vextenddouble/vextenddoubleD1/2
2U⊤
2(B◦R)⊤u/vextenddouble/vextenddouble/vextenddouble
2/bracketrightig
≤/radicalig
E/bracketleftbig
u⊤(B◦R)U2D2U⊤
2(B◦R)⊤u/bracketrightbig
=/radicaltp/radicalvertex/radicalvertex/radicalbts/summationdisplay
k,k′=1ukuk′E/bracketleftbig/bracketleftbig
(B◦R)U2D2U⊤
2(B◦R)⊤/bracketrightbig
kk′/bracketrightbig
=/radicaltp/radicalvertex/radicalvertex/radicalbts/summationdisplay
k,k′=1n/summationdisplay
l,l′=1ukuk′[U2D2U⊤
2]ll′E/bracketleftbig
[B◦R]kl[B◦R]k′l′/bracketrightbig
=/radicaltp/radicalvertex/radicalvertex/radicalbts/summationdisplay
k=1n/summationdisplay
l=1B2
klu2
k[U2D2U⊤
2]ll
≤/radicalbig
Tr(D2),
which implies
E[F(R)]≤/radicalbiggn
sp/radicaligg/summationtextn
j=dn+1µj
n≤/radicalbiggn
sp/radicaltp/radicalvertex/radicalvertex/radicalbt1
nn/summationdisplay
j=dn+1min(µj,δ2n)≤/radicaligg
δ2n
p, (38)
32Published in Transactions on Machine Learning Research (09/2023)
where we have used the definition of δ2
nand the assumption s≥δ2
nn. Coming back to (36), we obtain
P/braceleftig/vextenddouble/vextenddoubleSU2D1/2
2/vextenddouble/vextenddouble
op>cδn/bracerightig
≤5sE/bracketleftbigg
P/braceleftbigg
sup
v∈E/vextendsingle/vextendsingle⟨u,Sv⟩/vextendsingle/vextendsingle>c
2δn/vextendsingle/vextendsingleB/bracerightbigg/bracketrightbigg
= 5sE/bracketleftig
P/braceleftig
F(R)>c
2δn/bracerightig/bracketrightig
≤5sE/bracketleftbigg
P/braceleftbigg
F(R)−E[F(R)]>δn/parenleftbiggc
2−1√p/parenrightbigg/bracerightbigg/bracketrightbigg
(39)
≤C5sexp/parenleftigg
−c′/parenleftbiggc
2−1√p/parenrightbigg2
δ2
nsp
δ2n/parenrightigg
(40)
≤Cexp/parenleftigg
−c′/parenleftigg/parenleftbiggc
2−1√p/parenrightbigg2
p−log(5)/parenrightigg
s/parenrightigg
,
where(39)comes from the upper bound on E[F(R)]we derived in (38), and(40)derives from Lemmas 6
and 7 applied to the function Fwhose Lipschitz constant has been established in (37). Therefore, taking
c=2√p/parenleftig
1 +/radicalbig
log (5)/parenrightig
+ 1, we have
P/braceleftig/vextenddouble/vextenddoubleSU2D1/2
2/vextenddouble/vextenddouble
op>cδn/bracerightig
≤c1e−c2s
withc1=C′andc2=c′/parenleftig/radicalbig
plog (5) +p
4/parenrightig
.
A.5 Proof of Proposition 1
We prove Proposition 1 thanks to duality properties.
Proof.Since problems (3)and(11)are convex problems under Slater’s constraints, strong duality holds and
we will show that they admit the same dual problem
min
ζ∈Rnn/summationdisplay
i=1ℓ⋆
i(−ζi) +1
2λnnζ⊤KS⊤(SKS⊤)†SKζ. (53)
First, we compute dual problem of (3), that can be rewritten
min
γ∈Rs,u∈Rnn/summationdisplay
i=1ℓi(ui) +λnn
2γ⊤SKS⊤γ
s.t.u=KS⊤γ.
Therefore the Lagrangian writes
L(γ,u,ζ ) =n/summationdisplay
i=1ℓi(ui) +λnn
2γ⊤SKS⊤γ+n/summationdisplay
i=1ζi(ui−[KS⊤γ]i)
=n/summationdisplay
i=1ℓi(ui) +λnn
2γ⊤SKS⊤γ+n/summationdisplay
i=1ζiui−ζ⊤KS⊤γ.
Differentiating with respect to γand using the definition of the Fenchel-Legendre transform, one gets
g(ζ) = inf
γ∈Rs,u∈RnL(γ,u,ζ )
=n/summationdisplay
i=1inf
ui∈R{ℓi(ui) +ζiui}+ inf
γ∈Rs/braceleftbiggλnn
2γ⊤SKS⊤γ−ζ⊤KS⊤γ/bracerightbigg
=n/summationdisplay
i=1−ℓ⋆
i(−ζi)−1
2λnnζ⊤KS⊤(SKS⊤)†SKζ
33Published in Transactions on Machine Learning Research (09/2023)
together with the equality SKS⊤˜γ=1
λnnSK˜ζ, implying ˜γ=1
λnn(SKS⊤)†SK˜ζ, where ˜ζ∈Rnis the solution
of the following dual problem
min
β∈Rnn/summationdisplay
i=1ℓ⋆
i(−βi) +1
2λnnβ⊤KS⊤(SKS⊤)†SKβ. (53)
Then, we compute dual problem of (11), that can be rewritten
min
ω∈Rr,u∈Rnn/summationdisplay
i=1ℓ(ui,yi) +λnn
2∥ω∥2
2
s.t.u=KS⊤˜Krω.
Therefore the Lagrangian writes
L(ω,u,ζ ) =n/summationdisplay
i=1ℓi(ui) +λnn
2∥ω∥2
2+n/summationdisplay
i=1ζi(ui−[KS⊤˜Krω]i)
=n/summationdisplay
i=1ℓi(ui) +λnn
2∥ω∥2
2+n/summationdisplay
i=1ζ⊤
iui−ω⊤˜K⊤
rSKζ.
Differentiating with respect to ωand using the definition of the Fenchel-Legendre transform, one gets
g(ζ) = inf
ω∈Rr,u∈RnL(ω,u,ζ )
=n/summationdisplay
i=1inf
ui∈R{ℓi(ui) +ζiui}+ inf
ω∈Rr/braceleftbiggλnn
2∥ω∥2
2−ω⊤˜K−1/2⊤SKζ/bracerightbigg
.
We have that
∂
∂ω/parenleftbig
∥ω∥2
2/parenrightbig
= 2ω
∂
∂ω/parenleftbig
ω⊤˜K⊤
rSKζ/parenrightbig
=˜K⊤
rSKζ,
Then, setting the gradient to zero, we obtain
˜ω=1
λnn˜K⊤
rSK˜ζ. (41)
Hence, putting it into the Lagrangian,
−1
λnnζ⊤KS⊤˜K−1/2˜K⊤
rSKζ =−1
λnnKS⊤/parenleftbig
SKS⊤/parenrightbig†SKζ,
and
1
2λnnζ⊤KS⊤˜Kr˜K⊤
rSKζ =1
2λnnKS⊤/parenleftbig
SKS⊤/parenrightbig†SKζ.
Hence, ˜ζ∈Rnis the solution to the following dual problem
min
β∈Rnn/summationdisplay
i=1ℓ⋆
i(−βi) +1
2λnnβ⊤KS⊤(SKS⊤)†SKβ. (53)
Finally, since both problems are convex and strong duality holds, we obtain through KKT conditions
˜ω=/parenleftbig
SKS⊤/parenrightbig−1/2⊤/parenleftbig
SKS⊤/parenrightbig
˜γ=/parenleftig
˜D1/2
r0r×s−r/parenrightig
˜V⊤˜γ
and
min
γ∈Rs1
nn/summationdisplay
i=1ℓ([KS⊤γ]i,yi) +λn
2γ⊤SKS⊤γ= min
ω∈Rrn/summationdisplay
i=1ℓ/parenleftbig
ω⊤zS(xi),yi/parenrightbig
+λn
2∥ω∥2
2.
34Published in Transactions on Machine Learning Research (09/2023)
B On relaxing Assumption 2
In this section, we detail the discussion about relaxing Assumption 2, i.e. the restriction of the hypothesis
set to the unit ball of the RKHS. Assumption 2 is a classical assumption in kernel literature to apply
generalisation bounds based on Rademacher complexity of a bounded ball of a RKHS. Moreover, it is also
useful in our case to derive an approximation error bound, describing how K-satisfiability of a sketch matrix
allows to obtain a good approximation of the minimiser of the risk. However, let us discuss the consequences
of relaxing this assumption. Indeed, all we need is a bound on the norm of the estimators ˜fs– minimiser
of the regularised ERM sketched problem – and ˜fR
s– minimiser of the regularised ERM sketched denoised
KRR problem. By definition, noting HS=/braceleftbig
f=/summationtextn
i=1/bracketleftbig
S⊤γ/bracketrightbig
ik(·,xi)|γ∈Rs/bracerightbig
, we have that
˜fs= arg min
f∈HS1
nn/summationdisplay
i=1ℓ(f(xi),yi) +λn
2∥f∥2
Hk.
Hence,
λn
2∥˜fs∥2
Hk≤1
nn/summationdisplay
i=1ℓ(˜fs(xi),yi) +λn
2∥˜fs∥2
Hk≤1
nn/summationdisplay
i=1ℓ(0,yi)≤1,
if we assume that max 1≤i≤nℓ(0,yi)≤1to simplify the derivations. As a consequence, we obtain that
∥˜fs∥Hk≤/radicalbigg
2
λn. (42)
Similarly, we have that
˜fR
s= arg min
f∈HS1
n/vextenddouble/vextenddoublefX−fX
Hk/vextenddouble/vextenddouble2
2+λn
2∥f∥2
Hk,
that gives
∥˜fR
s∥Hk≤/parenleftbigg1
λnn∥fHk∥2
Hk/parenrightbigg1/2
.
By Assumptions 2 and 4,
1
n∥fHk∥2
Hk=1
nn/summationdisplay
i=1⟨fHk,k(·,xi)⟩Hk
≤1
nn/summationdisplay
i=1∥fHk∥2
Hkk(xi,xi)
≤κ,
and finally
∥˜fR
s∥Hk≤/radicalbiggκ
λn. (43)
Remark 4. Note that in the multiple output settings, we obtain
∥˜fR
s∥Hk≤/radicaligg
κTr(M)
λn. (44)
We are now equipped to derive the generalisation error bound E/bracketleftig
ℓ˜fs/bracketrightig
−En/bracketleftig
ℓ˜fs/bracketrightig
and the approximation error
bound En/bracketleftig
ℓ˜fs/bracketrightig
−En/bracketleftig
ℓfHk/bracketrightig
. We first focus on the generalisation bound, and following the proof given in
Appendix A.1 and given the new norm upper bound/radicalig
2
λn, for anyδ∈(0,1), with probability 1−δ/2, we
have that
E/bracketleftig
ℓ˜fs/bracketrightig
−En/bracketleftig
ℓ˜fs/bracketrightig
≤4L√
2κ√λnn+/radicalbigg
8 log(4/δ)
n. (45)
35Published in Transactions on Machine Learning Research (09/2023)
This dependence in 1/√λnshows that, as expected by a regularisation penalty, with a fixed n, whenλn
increases, the generalisation bound decreases and then we obtain a better generalisation performance. However,
this behaviour does not reflect completely the role of λn, since there exists a tradeoff between overfitting and
underfitting, and then it should not be set too large. We now focus on the approximation error bound. As in
Appendix A.1, we obtain that
En/bracketleftig
ℓ˜fs/bracketrightig
−En/bracketleftig
ℓfHk/bracketrightig
=1
nn/summationdisplay
i=1ℓ/parenleftbig˜fs(xi),yi/parenrightbig
−1
nn/summationdisplay
i=1ℓ(fHk(xi),yi) (46)
= inf
f∈HS
∥f∥Hk≤∥˜fs∥Hk1
nn/summationdisplay
i=1ℓ(f(xi),yi)−1
nn/summationdisplay
i=1ℓ(fHk(xi),yi)
≤ inf
f∈HS
∥f∥Hk≤∥˜fs∥HkL
nn/summationdisplay
i=1|f(xi)−fHk(xi)|
≤L inf
f∈HS
∥f∥Hk≤∥˜fs∥Hk/radicaltp/radicalvertex/radicalvertex/radicalbt1
nn/summationdisplay
i=1|f(xi)−fHk(xi)|2
=L/radicaltp/radicalvertex/radicalvertex/radicalbt inf
f∈HS
∥f∥Hk≤∥˜fs∥Hk1
n/vextenddouble/vextenddoublefX−fX
Hk/vextenddouble/vextenddouble2
2, (47)
where, for any f∈Hk,fX=(f(x1),...,f (xn))∈Rn. Let ˜fR
s=/summationtextn
i=1/bracketleftbig
S⊤˜γR/bracketrightbig
ik(·,xi), where ˜γRis a
solution to
inf
γ∈Rs1
n/vextenddouble/vextenddoubleKS⊤γ−fX
Hk/vextenddouble/vextenddouble2
2+λnγ⊤SKS⊤γ.
It is easy to check that ˜fR
sis also a solution to
inf
f∈HS
∥f∥Hk≤∥˜fRs∥Hk1
n/vextenddouble/vextenddoublefX−fX
Hk/vextenddouble/vextenddouble2
2. (48)
Now, comparing (47)and(48), as done in Appendix A.1, essentially boils down to comparing/vextenddouble/vextenddouble˜fs/vextenddouble/vextenddouble
Hkand/vextenddouble/vextenddouble˜fR
s/vextenddouble/vextenddouble
Hk, which is a highly nontrivial question. In particular, the upper bounds (42)and(43)are not
informative enough. Another solution could consist in addingλn
2/vextenddouble/vextenddouble˜fs/vextenddouble/vextenddouble
Hkto(46). However, the upper bound
(42)then transforms this term into a constant bias. This can be explained as (42)is very crude. Instead,
having/vextenddouble/vextenddouble˜fs/vextenddouble/vextenddouble
Hkbounded by λα
nforα≥−1/2would be enough to exhibit a bias term that vanishes as λn
goes to 0. Note that it would still degrade the tradeoff with the genealisation term. Hence, if generalization
errors can be dealt with when removing Assumption 2, it is much more complex to control (46).
C Some background on statistical properties when using a p-sparsified sketch
In this section, we focus on p-sparsified sketches, and give, according to different standard choices of kernels
(Gaussian, polynomial and first-order Sobolev), the different learning rates obtained for the excess risk as
well as the condition on s.
We can first derive the following corollaries for the excess risk of p-sparsified sketched estimator in both single
and multiple output settings.
Corollary 2. For ap-sparsified sketch matrix Swiths≥max/parenleftbig
C0dn/p2,δ2
nn/parenrightbig
, we have with probability
greater than 1−C1e−sc(p)in the single output setting for a generic Lipschitz loss,
E/bracketleftbig
ℓ˜fs/bracketrightbig
≤E/bracketleftbig
ℓfHk/bracketrightbig
+LC/radicalbig
λn+δ2n+λn
2+ 8L/radicalbiggκ
n+O/parenleftbigg/radicalbiggs
n/parenrightbigg
, (49)
36Published in Transactions on Machine Learning Research (09/2023)
and ifℓ(z,y) = (z−y)2/2andY⊂ [0,1], with probability at least 1−δwe have that
E/bracketleftbig
ℓ˜fs/bracketrightbig
≤E/bracketleftbig
ℓfHk/bracketrightbig
+/parenleftbigg
C2+1
2/parenrightbigg
λn+C2δ2
n+ 8κ+√κ√n+O/parenleftbigg/radicalbiggs
n/parenrightbigg
. (50)
Corollary 3. For ap-sparsified sketch matrix Swiths≥max/parenleftbig
C0dn/p2,δ2
nn/parenrightbig
, we have with probability
greater than 1−C1e−sc(p)in the multiple output setting for a generic Lipschitz loss,
E/bracketleftbig
ℓ˜fs/bracketrightbig
≤E/bracketleftbig
ℓfHk/bracketrightbig
+LC/radicalig
λn+∥M∥opδ2n+λn
2+ 8L/radicalbigg
κTr (M)
n+O/parenleftbigg/radicalbiggs
n/parenrightbigg
. (51)
and ifℓ(z,y) =∥z−y∥2
2/2andY⊂B/parenleftbig
Rd/parenrightbig
, with probability at least 1−δwe have that
E/bracketleftbig
ℓ˜fs/bracketrightbig
≤E/bracketleftbig
ℓfHk/bracketrightbig
+/parenleftbigg
C2+1
2/parenrightbigg
λn+C2∥M∥opδ2
n+ 8 Tr (M)1/2κ∥M∥1/2
op+κ1/2
√n+O/parenleftbigg/radicalbiggs
n/parenrightbigg
.(52)
We summarize in Table 3 the different behaviours of δ2
nanddnin the different spectrum regimes considered,
in order to explicit the exact condition on sin each case. More specifically, for a Dth-order polynomial
kernel,dn, for anynis at mostD+ 1, leading to sof orderD+ 1to be sufficient. Finally, we can derive the
learning rate obtained as well as the exact condition on sfor each scenario, see Table 3. Compared with
Random Fourier Features (Li et al., 2021), we see that we obtain slightly degraded learning rates for Gaussian
and first-order Sobolev kernels, in comparison with the O(1/√n)rate the authors obtain. Our rates remain
however very close.
Table 3: Statistical dimension, lower bound obtained on s, and learning rate obtained for excess risk with
p-sparsified sketches for different kernels.
Kernel δ2
n dn s Learning rate
GaussianO/parenleftbigg√
log(n)
n/parenrightbigg
∝/radicalbig
log(n) Ω/parenleftig/radicalbig
log(n)/p2/parenrightig
O/parenleftig
(log(n))1/4
n1/2/parenrightig
Polynomial O/parenleftbig1
n/parenrightbig
∝1 Ω/parenleftig
1
p2/parenrightig
O/parenleftig
1√n/parenrightig
SobolevO/parenleftbig1
n2/3/parenrightbig
∝n1/3Ω/parenleftbig
n1/3/p2/parenrightbig
O/parenleftbig1
n1/3/parenrightbig
DDetailed algorithm of the generation and the decomposition of a p-sparsified sketch
In this section, we detail the process of generating a p-sparsified sketch and decomposing it as a product of a
sub-Gaussian sketch SSGand a sub-Sampling sketch SSS.
Algorithm 1 Generation of a p-sparsified sketch
input:s,nandp
Generate a s×nmatrixBwhose entries are i.i.d. Bernouilli random variables of parameter p.
indices←−indices of non-null columns of B.
B′←−Bwhere all null columns have been deleted.
Generate a matrix MSGof the same size as B′whose entries are either i.i.d. Gaussian or Rademacher
random variables.
SSG←−MSG◦B′, where◦denotes the component-wise Hadamard matrix product.
returnSSGandindices
37Published in Transactions on Machine Learning Research (09/2023)
E Some background on complexity for single and multiple output regression
In this section, we detail the complexity in time and space of various matrix operations and iterations of
stochastic subgradient descent in both single and multiple output settings for various sketching types.
We first recall the time and space complexities for elementary matrix products. The main advantage of using
Sub-Sampling matrices is that computing SKis equivalent to sampling straining inputs and construct a
s×nGram matrix, hence we gain huge time complexity since we do not compute a matrix multiplication,
as well as space complexity since we do not compute a n×nGram matrix. As a consequence, the main
advantage of our p-sparsified sketches is their ability to be decomposed as S=SSGSSS, whereSSG∈Rs×s′
is a sparse sub-Gaussian sketch and SSS∈Rs′×nis a sub-Sampling sketch, as explained in Section 3 and
Appendix D. This decomposition trick is particularly interesting when pis small, and since s′follows a
Binomial distribution of parameters nand1−(1−p)sand we assume in our settings that nis large, hence
we have that s′≈E[s′]∼
p→0nsp. In the following, we take s′=nsp. We recall that Accumulation matrices
from Chen & Yang (2021a) writes as S=/summationtextm
i=1S(i), where the S(i)s are sub-sampling matrices whose each
row is multiplied by independent Rademacher variables. Hence, both p-sparsified and Accumulation sketches
are interesting since it completely benefits from computational efficiency of sub-sampling matrices. See table
4 for complexity analysis of matrix multiplications.
Going into the complexity of the learning algorithms, the main difference between single and multiple output
settings are the computation of feature maps, relying on the construction of SKS⊤and the computation of
the square root of its pseudo-inverse for the single output setting which is not present in the multiple output
settings. We assume in our framework that dand evend2are typically very small in comparison with n.
Hence, we have that the complexity in the single output case is dominated by the complexity of the operation
SKS⊤, whereas in the multiple output case it is dominated by the complexity of the operation SK. We see
that from a time complexity perspective, p-sparsified sketches outperform Accumulation sketches in single
output settings as long as p≤m/n√s, and in multiple output settings as long as p≤m/ns. From a space
complexity perspective, Accumulation is always better as nspis typically greater than s, otherwise it shows
poor performance. However, pis usually chosen such that nspis not very large compared with s.
Table 4: Complexity of matrix operations for each sketching type.
Sketching type Complexity type SK SKS⊤
GaussiantimeO/parenleftbig
n2s/parenrightbig
O/parenleftbig
n2s/parenrightbig
spaceO/parenleftbig
n2/parenrightbig
O/parenleftbig
n2/parenrightbig
p-sparsifiedtimeO/parenleftbig
n2s2p/parenrightbig
O/parenleftbig
n2s3p2/parenrightbig
spaceO/parenleftbig
n2sp/parenrightbig
O/parenleftbig
n2s2p2/parenrightbig
AccumulationtimeO(nsm)O/parenleftbig
s2m2/parenrightbig
spaceO(ns)O/parenleftbig
s2/parenrightbig
CountSkethtimeO/parenleftbig
n2/parenrightbig
O/parenleftbig
n2/parenrightbig
spaceO/parenleftbig
n2/parenrightbig
O/parenleftbig
n2/parenrightbig
Sub-SamplingtimeO(ns)O/parenleftbig
s2/parenrightbig
spaceO(ns)O/parenleftbig
s2/parenrightbig
F Discussion with dual implementation
In this section we detail the discussion about duality of kernel machines when using sketching. A first idea
consists in computing the dual problem to the sketched problem (3). It writes
min
ζ∈Rnn/summationdisplay
i=1ℓ⋆
i(−ζi) +1
2λnnζ⊤KS⊤(SKS⊤)†SKζ, (53)
38Published in Transactions on Machine Learning Research (09/2023)
whereℓi=ℓ(·,yi), andf⋆denotes the Fenchel-Legendre transform of f, such that f⋆(θ) =supx⟨θ,x⟩−f(x).
First note that sketching with a subsampling matrix in the primal is thus equivalent to using a Nyström
approximation in the dual. This remark generalizes for any loss function the observation made in Yang et al.
(2017) for the kernel Ridge regression. However, although the ℓ⋆
imight be easier to optimize, solving (53)
seems not a meaningful option, as duality brought us back to optimizing over Rn, what we initially intended
to avoid. The natural alternative thus appears to use duality first, and then sketching. The resulting problem
writes
min
θ∈Rsn/summationdisplay
i=1ℓ⋆
i/parenleftbig
−[S⊤θ]i/parenrightbig
+1
2λnnθ⊤SKS⊤θ. (54)
It is interesting to note that (54)is also the sketched version of Problem (53), which we recall is itself the
dual to the sketched primal problem. Hence, sketching in the dual can be seen as a double approximation.
As a consequence, the objective value reached by minimizing (54)is always larger than that achieved by
minimizing (3), and theoretical guarantees for such an approach are likely to be harder to obtain. Another
limitation of (54)regards the ℓ⋆
i(−[S⊤θ]i). Indeed, these terms generally contain the non-differentiable part
of the objective function (for the ϵ-insensitive Ridge regression we have/summationtext
iℓ⋆
i(θi) =1
2∥θ∥2
2+⟨θ,y⟩+ϵ∥θ∥1for
instance), and are usually minimized by proximal gradient descent. However, using a similar approach for
(54)is impossible, since the proximal operator of ℓ⋆
i(S⊤·)is only computable if S⊤S=In, which is never
the case. Instead, one may use a primal-dual algorithm (Chambolle et al., 2018; Vu, 2011; Condat, 2013),
which solves the saddle-point optimization problem of the Lagrangian, but maintain a dual variable in Rn.
Coordinate descent versions of such algorithms (Fercoq & Bianchi, 2019; Alacaoglu et al., 2020) may also be
considered, as they leverage the possible sparsity of Sto reduce the per-iteration cost. In order to converge,
these algorithms however require a number of iteration that is of the order of n, making them hardly relevant
in the large scale setting we consider.
For all the reasons listed above, we thus believe that minimizing (53)or(54)is not theoretically relevant nor
computationally attractive, and that running stochastic (sub-)gradient descent on the primal problem, as
detailed at the beginning of the section, is the best way to proceed algorithmically despite the possibly more
elegant dual formulations. Finally, we highlight that although the condition S⊤S=Inis almost surely not
verified (we have S∈Rs×nwiths<n), we still have E[S⊤S] =Infor most sketching matrices. An interesting
research direction could thus consist in running a proximal gradient descent assuming that S⊤S=In, and
controlling the error incurred by such an approximation.
G Examples of Lipschitz-continuous losses
Robust losses for multiple output regression: Forp≥1,Y⊂Rp, and for all y,y′∈Y,ℓ(y,y′) =
g(y−y′), wheregis:
Forκ-Huber: Forκ>0:
∀y∈Y,g(y) =/braceleftigg
1
2∥y∥2
Yif∥y∥Y≤κ
κ/parenleftbig
∥y∥Y−κ
2/parenrightbig
otherwise.
Forϵ-SVR(i.e.ϵ-insensitive ℓ1loss): Forϵ>0:
∀y∈Y,g(y) =/braceleftigg
∥y∥Y−ϵif∥y∥Y≥ϵ
0otherwise.
The pinball loss (Koenker, 2005) for joint quantile regression: Fordquantile levels, τ1< τ2<
...<τdwithτi∈(0,1), we define:
ℓτ(f(x),y) =Lτ(f(x)−y1d),
39Published in Transactions on Machine Learning Research (09/2023)
with the following definition for Lτthe extension of pinball loss to Rd(Sangnier et al., 2016):
Forr∈Rd:
Lτ(r) =d/summationdisplay
j=1/braceleftigg
τjrj ifrj≥0,
(τj−1)rjifrj<0.
H Additional experiments
In this section, we bring some additional experiments and details.
H.1 Simulated dataset for single output regression
First, we report the plots obtained with κ-Huber forp-SG sketches (see Figure 2) and note that we observe a
behaviour similar to p-SR sketches when varying pand in comparison to other types of sketching and RFFs.
However, we see that the MSE obtained is slightly worse than p-SR sketches. An explanation might be that,
in a very sparse regime, i.e. very low p, ap-SG sketch is too different than a Gaussian sketch, making it lose
some good statistical properties of Gaussian sketches. We however observe that the larger pis, the smaller is
the statistical performance between p-SR andp-SG sketches.
We then report in the following the corresponding plots obtained with ϵ-SVR, that witnesses the same
phenomenon observed earlier with κ-Huber about the interpolation between Nyström method and Gaussian
sketching while varying the probability of being different than 0 p, withp-SR sketches (see Figure 3) and
p-SG sketches (see Figure 4).
H.2 Multi-Output Regression on real datasets
We here first a brief presentation on River Flow and Supply Chain Management:
1.River Flow datasets aim at predicting the river network flows for 48 hours in the future at specific
locations. These locations are 8 sites in the Mississippi River network in the United States and were
obtained from the US National Weather Service. Dataset rf2 extends rf1 since it contains additional
precipitation forecast information for each of the 8 sites.
2.The datasets scm1d and sm20d come from the Trading Agent Competition in Supply Chain Manage-
ment (TAC SCM) tournament from 2010. More details about data preprocessing can be found in
Groves & Gini (2015). The dataset contains prices of products at specific days, and the task is to
predict to the next day mean price (scm1d) or mean price for 20-days in the future (scm20d) for
each product in the simulation.
To conduct these experiments, the train-test splits used are the ones available at http://mulan.sourceforge.
net/datasets-mtr.html . Besides, we used multi-output Kernel Ridge Regression framework and an input
Gaussian kernel and an operator M=Id. We selected regularisation parameter λnand bandwidth of kernel
σ2via a 5-fold cross-validation. Results are averages over 30 replicates for sketched models.
Table 5: Numbers of training samples ( ntr), test samples ( nte), features ( q) and targets ( d).
Dataset ntr nte q d
Boston 354 152 13 5
otoliths 3780 165 4096 5
rf1 4108 5017 64 8
rf2 4108 5017 576 8
scm1d 8145 1658 280 16
scm20d 7463 1503 61 16
40Published in Transactions on Machine Learning Research (09/2023)
MSE
s
(a) Test relative MSE w.r.t. swithκ-Huber.
sTraining time (in seconds) (b) Training time (seconds) w.r.t. swithκ-Huber.
MSE
Training time (in seconds)
(c) Test relative MSE w.r.t. training times with κ-Huber.
Figure 2: Trade-off between Accuracy and Efficiency for p-SG sketches with κ-Huber loss on synthetic dataset.
We compare our non-sketched framework with the sketched one, and we furthermore compare our p-sparsified
sketches with Accumulation sketch from Chen & Yang (2021a) and CountSketch from Clarkson & Woodruff
(2017). Moreover, we report the range of results obtained by SOTA methods available at Spyromitros-Xioufis
et al. (2016). All results in terms of Test RRMSE are reported in Table 6, we see that our p-sparsified sketches
allow to ally statistical and computational performance, since we maintain an accuracy of the same order as
without sketching, and these sketches outperform Accumulation in terms of training times (see Table 2). In
comparison to SOTA, our framework does not compete with the best results obtained in Spyromitros-Xioufis
et al. (2016), but almost always remains within the range of results obtained with SOTA methods.
41Published in Transactions on Machine Learning Research (09/2023)
Table 6: Test RRMSE and ARRMSE for different methods on the MTR datasets. For decomposable
kernel-based models, loss here is square loss and s= 100when performing Sketching.
Dataset Targets w/o Sketch 20/ntr-SR 20/ntr-SG Acc. m= 20 CountSketch SOTA
rf1Mean 0.575 0.584±0.003 0.583±0.003 0.592±0.001 0.575 ±0.0005 [0.091, 0.983]
chsi2 0.351 0.356±0.005 0.357±0.004 0.361±0.002 0.350 ±0.002 [0.033, 0.797]
nasi2 1.085 1.124±0.003 1.124±0.003 1.082 ±0.0004 1.110±0.0003 [0.376, 1.951]
eadm7 0.388 0.397±0.004 0.398±0.003 0.387 ±0.001 0.387 ±0.001 [0.039, 1.019]
sclm7 0.660 0.659±0.008 0.661±0.005 0.681±0.002 0.648 ±0.002 [0.047, 1.503]
clkm7 0.283 0.281 ±0.001 0.282±0.001 0.293±0.0005 0.281 ±0.0004 [0.031, 0.587]
vali2 0.614 0.633±0.008 0.635±0.010 0.656±0.003 0.611 ±0.003 [0.037, 0.571]
napm7 0.593 0.628±0.020 0.614±0.016 0.627±0.003 0.601±0.003 [0.038, 0.909]
dldi4 0.629 0.597 ±0.004 0.597 ±0.003 0.646±0.001 0.614±0.002 [0.029, 0.534]
rf2Mean 0.578 0.671±0.009 0.656±0.006 0.796±0.006 0.715±0.011 [0.095, 1.103]
chsi2 0.318 0.382±0.016 0.358±0.010 0.478±0.006 0.426±0.013 [0.034, 0.737]
nasi2 1.099 1.084±0.005 1.092±0.006 1.018 ±0.003 1.036±0.002 [0.384, 3.143]
eadm7 0.342 0.390±0.013 0.369±0.007 0.456±0.004 0.417±0.010 [0.040, 0.737]
sclm7 0.610 0.719±0.030 0.672±0.021 0.948±0.014 0.852±0.030 [0.049, 0.970]
clkm7 0.311 0.328±0.009 0.330±0.009 0.614±0.005 0.436±0.006 [0.041, 0.891]
vali2 0.712 0.960±0.044 0.894±0.043 0.890±0.017 0.939±0.028 [0.047, 0.956]
napm7 0.589 0.812±0.014 0.831±0.017 1.110±0.023 0.856±0.032 [0.039, 0.617]
dldi4 0.646 0.696±0.010 0.701±0.011 0.855±0.004 0.761±0.007 [0.032, 0.770]
scm1dMean 0.418 0.422±0.002 0.423±0.001 0.423±0.001 0.420±0.001 [0.330, 0.457]
lbl 0.358 0.365±0.003 0.364±0.002 0.367±0.001 0.363±0.001 [0.294, 0.409]
mtlp2 0.352 0.360±0.003 0.362±0.003 0.362±0.001 0.358±0.001 [0.308, 0.436]
mtlp3 0.409 0.419±0.003 0.416±0.002 0.417±0.001 0.416±0.002 [0.315, 0.442]
mtlp4 0.417 0.427±0.002 0.426±0.003 0.426±0.001 0.423±0.002 [0.325, 0.461]
mtlp5 0.495 0.491 ±0.006 0.492±0.006 0.502±0.002 0.492±0.003 [0.349, 0.530]
mtlp6 0.534 0.524 ±0.008 0.527±0.006 0.537±0.002 0.527±0.002 [0.347, 0.540]
mtlp7 0.531 0.519 ±0.008 0.523±0.006 0.534±0.002 0.523±0.003 [0.338, 0.526]
mtlp8 0.542 0.536 ±0.010 0.540±0.008 0.547±0.002 0.537±0.003 [0.345, 0.504]
mtlp9 0.385 0.395±0.003 0.395±0.002 0.390±0.001 0.390±0.002 [0.323, 0.456]
mtlp10 0.389 0.398±0.003 0.397±0.003 0.394±0.002 0.394±0.001 [0.339, 0.456]
mtlp11 0.424 0.430±0.003 0.429±0.003 0.426±0.001 0.426±0.001 [0.327, 0.445]
mtlp12 0.420 0.422±0.003 0.421±0.004 0.423±0.001 0.421±0.002 [0.350, 0.466]
mtlp13 0.349 0.358±0.004 0.354±0.004 0.351±0.001 0.351±0.001 [0.322, 0.419]
mtlp14 0.347 0.364±0.004 0.363±0.003 0.350±0.001 0.355±0.002 [0.356, 0.472]
mtlp15 0.361 0.371±0.004 0.370±0.003 0.363±0.001 0.364±0.001 [0.314, 0.406]
mtlp16 0.376 0.382±0.003 0.384±0.003 0.376 ±0.001 0.378±0.001 [0.322, 0.407]
scm20dMean 0.755 0.754±0.003 0.754±0.003 0.753 ±0.001 0.754±0.002 [0.394, 0.763]
lbl 0.613 0.618±0.002 0.618±0.002 0.614±0.001 0.613 ±0.001 [0.356, 0.678]
mtlp2a 0.628 0.635±0.002 0.634±0.003 0.632±0.001 0.631±0.002 [0.352, 0.688]
mtlp3a 0.603 0.608±0.002 0.608±0.003 0.607±0.001 0.605±0.002 [0.363, 0.683]
mtlp4a 0.635 0.645±0.002 0.645±0.003 0.644±0.001 0.638±0.002 [0.374, 0.730]
mtlp5a 0.974 0.977±0.008 0.977±0.007 0.978±0.003 0.975±0.006 [0.413, 0.846]
mtlp6a 0.981 0.986±0.009 0.992±0.008 1.002±0.004 0.989±0.008 [0.424, 0.843]
mtlp7a 0.996 1.001±0.008 1.004±0.007 1.005±0.006 1.000±0.009 [0.404, 0.833]
mtlp8a 0.995 0.997±0.010 0.997±0.011 1.008±0.005 0.994 ±0.005 [0.407, 0.851]
mtlp9a 0.708 0.704±0.003 0.702±0.003 0.698 ±0.001 0.705±0.002 [0.382, 0.737]
mtlp10a 0.718 0.722±0.004 0.722±0.004 0.716 ±0.001 0.723±0.003 [0.413, 0.753]
mtlp11a 0.729 0.730±0.003 0.729±0.003 0.725 ±0.001 0.728±0.002 [0.402, 0.769]
mtlp12a 0.720 0.718±0.004 0.717±0.004 0.712 ±0.002 0.716±0.003 [0.429, 0.787]
mtlp13a 0.711 0.703±0.005 0.699±0.004 0.697 ±0.001 0.705±0.003 [0.400, 0.751]
mtlp14a 0.683 0.673±0.004 0.670±0.003 0.668 ±0.001 0.675±0.002 [0.411, 0.779]
mtlp15a 0.684 0.674±0.004 0.671±0.004 0.666 ±0.001 0.678±0.002 [0.384, 0.727]
mtlp16a 0.689 0.677±0.005 0.676±0.005 0.672 ±0.001 0.682±0.003 [0.386, 0.754]
42Published in Transactions on Machine Learning Research (09/2023)
MSE
s
(a) Test relative MSE w.r.t. swithϵ-SVR.
sTraining time (in seconds) (b) Training time (seconds) w.r.t. swithϵ-SVR.
Training time (in seconds)MSE
(c) Test relative MSE w.r.t. training times with ϵ-SVR.
Figure 3: Trade-off between Accuracy and Efficiency for p-SR sketches with ϵ-SVR on synthetic dataset.
43Published in Transactions on Machine Learning Research (09/2023)
MSE
s
(a) Test relative MSE w.r.t. swithϵ-SVR.
sTraining time (in seconds) (b) Training time (seconds) w.r.t. swithϵ-SVR.
Training time (in seconds)MSE
(c) Test relative MSE w.r.t. training times with ϵ-SVR.
Figure 4: Trade-off between Accuracy and Efficiency for p-SG sketches with ϵ-SVR on synthetic dataset.
44