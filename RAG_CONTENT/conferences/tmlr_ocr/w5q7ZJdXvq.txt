Under review as submission to TMLR
Stochastic Submodular Bandits with Delayed Composite
Anonymous Bandit Feedback
Anonymous authors
Paper under double-blind review
Abstract
This paper investigates the problem of combinatorial multiarmed bandits with stochastic
submodular (in expectation) rewards and full-bandit delayed feedback, where the delayed
feedback is assumed to be composite and anonymous. In other words, the delayed feedback
is composed of components of rewards from past actions, with unknown division among
the sub-components. Three models of delayed feedback: bounded adversarial, stochastic
independent, and stochastic conditionally independent are studied, and regret bounds are
derived for each of the delay models. Ignoring the problem dependent parameters, we show
that regret bound for all the delay models is ˜O(T2/3+T1/3ν)for time horizon T, whereνis a
delayparameterdefineddifferentlyinthethreecases, thusdemonstratinganadditivetermin
regret with delay in all the three delay models. The considered algorithm is demonstrated to
outperform other full-bandit approaches with delayed composite anonymous feedback. We
also demonstrate the generalizability of our analysis of the delayed composite anonymous
feedback in combinatorial bandits as long as there exists an algorithm for the offline problem
satisfying a certain robustness condition.
1 Introduction
Many real world sequential decision problems can be modeled using the framework of stochastic multi-armed
bandits (MAB), such as scheduling, assignment problems, ad-campaigns, and product recommendations. In
these problems, the decision maker sequentially selects actions and receives stochastic rewards from an
unknown distribution. The objective is to maximize the expected cumulative reward over a time horizon.
Such problems result in a trade-off between trying actions to learn the system ( exploration ) and taking the
action that is empirically the best seen so far ( exploitation ).
Combinatorial MAB (CMAB) involves the problem of finding the best subset of Kout ofNitems to optimize
a possibly nonlinear function of reward of each item. Such a problem has applications in cloud storage (Xiang
et al., 2014), cross-selling item selection (Wong et al., 2003), social influence maximization (Agarwal et al.,
2022), etc. The key challenge in CMAB is the combinatorial N-choose-Kdecision space, which can be very
large. This problem can be converted to standard MAB with an exponentially large action space, although
needing an exponentially large time horizon to even explore each action once. Thus, the algorithms for
CMAB aim to not have this exponential complexity while still providing regret bounds. An important class
of combinatorial bandits is submodular bandits; which is based on the intuition that opening additional
restaurants in a small market may result in diminishing returns due to market saturation. A set function
f: 2Ω→Rdefined on a finite ground set Ωis said to be submodular if it satisfies the diminishing return
property: for all A⊆B⊆Ω, andx∈Ω\B, it holds that f(A∪{x})−f(A)≥f(B∪{x})−f(B)(Nemhauser
et al., 1978). Multiple applications for CMABs with submodular rewards have been described in detail in
(Nie et al., 2022), including social influence maximization, recommender systems, and crowdsourcing. In
these setups, the function is also monotone (adding more restaurants give better returns, adding more seed
users give better social influence), where for all A⊆B⊆Ω,f(A)≤f(B), and thus we also assume
monotononicity in the submodular functions.
1Under review as submission to TMLR
Feedback plays an important role in how challenging the CMAB problem is. When the decision maker
only observes a (numerical) reward for the action taken, that is known as bandit or full-bandit feedback.
When the decision maker observes additional information, such as contributions of each base arm in the
action, that is semi-bandit feedback. Semi-bandit feedback greatly facilitates learning. Furthermore, there
are two common formalizations depending on the assumed nature of environments: the stochastic setting
and the adversarial setting. In the adversarial setting, the reward sequence is generated by an unrestricted
adversary, potentially based on the history of decision maker’s actions. In the stochastic environment, the
reward of each arm is drawn independently from a fixed distribution. For CMAB with submodular and
monotone rewards, stochastic setting is not a special case of the adversarial setting since in the adversarial
setting, the environment chooses a sequence of monotone and submodular functions {f1,···,fT}, while
the stochastic setup assumes ftto be monotone and submodular in expectation (Nie et al., 2022). In the
adversarial setting, even if we limit ourselves to MAB instead of CMAB, the effect of composite anonymous
delay appears as a multiplicative factor in the literature (e.g. (Cesa-Bianchi et al., 2018)). In this paper, we
study the impact of full-bandit feedback in the stochastic setting for CMAB with submodular rewards and
cardinality constraints. In this case, the regret analysis with full-bandit feedback has been studied in the
adversarial setting in (Niazadeh et al., 2021), and in stochastic setting in (Nie et al., 2022).
In the prior works on CMAB as mentioned earlier, the feedback is available immediately after the action is
taken. However, this may not always be the case. Instead of receiving the reward in a single step, it can
be spread over multiple number of time steps after the action was chosen. Following each action choice, the
player receives the cumulative rewards from all prior actions whose rewards are due at this specific step. The
difficulty of this setting is due to the fact that the agent does not know how this aggregated reward has been
constituted from the previous actions chosen. This setting is called delayed composite anonymous feedback.
Such feedback arise in multiple practical setups. As an example, we consider a social influence maximization
problem. Consider a case of social network where a company developed an application and wants to market
it through the network. The best way to do this is selecting a set of highly influential users and hope they
can love the application and recommend their friends to use it. Influence maximization is a problem of
finding a small subset (seed set) in a network that can achieve maximum influence. This subset selection
problem in social networks is commonly modeled as an offline submodular optimization problem (Domingos
& Richardson, 2001; Kempe et al., 2003; Chen et al., 2010). However, when the seed set is selected, the
propagation of influence from one person to another may incur a certain amount of time delay and is not
immediate (Chen et al., 2012). The time-delay phenomena in information diffusion has also been explored in
statistical physics (Iribarren & Moro, 2009; Karsai et al., 2011). The spread of influence diffusion, and that
at each time we can only observe the aggregate reward limits us to know the composition of the rewards into
the different actions in the past. Further, the application developer, in most cases, will only be able to see
the aggregate reward leading to this being a bandit feedback. This motivates our study of stochastic CMAB
with submodular rewards and delayed composite anonymous bandit feedback.
To the best of our knowledge, this is the first work on stochastic CMAB with delayed composite anonymous
feedback. Inthispaper, weconsiderthreemodelsofdelays. Thefirstmodelofdelayis‘UnboundedStochastic
Independent Delay’. In this model, different delay distributions can be chosen at each time, and these delay
distributions are independent of each other. The second model is ‘Unbounded Stochastic Conditionally
Independent Delay’. In this model, the delay distribution does not only depend on time, but also on the
set chosen. The third model is ‘Bounded Adversarial Delay’. In this model, the maximum delay at each
time can be chosen arbitrarily as long as it is bounded. We note that in stochastic cases, the delay is not
bounded, while is governed by the tight family of distributions.1In the adversarial case, there is a bound on
the maximum delay, and the process generating this delay does not need to satisfy any other assumptions.
Thus, the results of stochastic and adversarial setups do not follow from each other. In particular, this is the
first work where the delay distribution is allowed to change over time. This gives new models for delayed
composite anonymous feedback which are more general than that considered in the literature. In each of the
three models of delay, this paper derives novel regret bounds.
1See Section 2 for a detailed description.
2Under review as submission to TMLR
In our analysis, we define the notion of upper tail bounds, which measures the tightness of a family of
distributions2, and use it to bound the regret. This notion allows us to reduce the complexity of considering
a family of delay distributions to considering only a single delay distribution. Then we use Bernstein
inequality to control the effect of past actions on the observed reward of the current action that is being
repeated. This allows us to obtain a regret upper bound in terms of the expected value of the upper tail
bound. The use of upper tail bounds for studying regret in bandits with delayed feedback is novel and has
not been considered in the literature earlier, to the best of our knowledge.
The main contributions of this paper can be summarized as follows
1.We introduce regret bounds for a stochastic CMAB problem with expected monotone and submodular
rewards, a cardinality constraint, and composite anonymous feedback. Notably, this paper marks the first
study of the regret bound any CMAB problem with composite delayed feedback, including CMAB with
submodular rewards.
2.We investigate the ETCG algorithm from (Nie et al., 2022), detailing its performance in three feedback
delay models: bounded adversarial delay, stochastic independent delay, and stochastic conditional indepen-
dent delay. Specifically, this is the first study where the distribution of stochastic delay is permitted to vary
over time. This introduces novel models for stochastic delayed composite anonymous feedback, which are
more general than those previously explored in existing literature.
3.Our analysis reveals the cumulative (1−1/e)-regret of ETCG under specific bounds for each delay
model. When comparing stochastic independent and conditional independent delays, the former showcases
better regret bounds. Generalizing beyond specific parameters, our findings suggest a regret bound of
˜O(T2/3+T1/3ν)across delay models.
4.Lastly, we showcase the adaptability of our analysis for delayed feedback in combinatorial bandits, given
certain algorithmic conditions. Building on (Nie et al., 2022), we derive regret bounds for a meta-algorithm,
highlightingitsapplicabilitytootherCMABproblemssuchassubmodularbanditswithknapsackconstraints
(See (Nie et al., 2023)).
On the technical side, we define new generalized notions of delay and introduce the notion of upper tail
bounds, which measures the tightness of a family of distributions. As discussed in Appendix A.2, algorithms
designed for composite anonymous feedback, including those in our study, rely on the concept of repeating
actions a sufficient number of times to minimize the impact of delay on the observed reward. We employ
Bernstein’s inequality to control the effect of previous actions on the observed reward of the current action
that is being repeated. This approach enables us to establish an upper bound on regret, expressed in terms
of the expected value of the upper tail bound.
Through simulations with synthetic data, we demonstrate that ETCG outperforms other full-bandit methods
in the presence of delayed composite anonymous feedback.
2 Problem Statement
LetTbe the time horizon, Ωbe the ground set of base arms, and n:=|Ω|. Also letTbe a family of
probability distributions on non-negative integers. At each time-step t≥1, the agent chooses an action St
from the setS={S|S⊆Ω,|S|≤k}, wherekis the a given positive integer.
The environment chooses a delay distribution δt∈T. The observation xtwill be given by the formula
xt=t/summationdisplay
i=1fi(Si)δi(t−i), (1)
whereft(S)is sampled from Ft(S), the stochastic reward function taking its values in [0,1]. Moreover, we
assume that E[Ft(S)] =f(S), wheref: 2Ω→[0,1]a monotone and submodular function. We will use Xt
to denote the random variable representing the observation at time t.
2See Assumption 1 and Lemma 1 for more details.
3Under review as submission to TMLR
Forα∈(0,1], theα-pseudo-regret is defined by
Rα:=T/summationdisplay
t=1(αf(S∗)−f(St)),
whereS∗:= argmaxS∈Sf(S)is the optimal action. Note that the choice of α= 1corresponds to the classical
notion of pseudo-regret. When there is no ambiguity, we will simply refer to Rαas theα-regret or regret. In
the offline problem with deterministic f, finding the optimal action S∗is NP-hard. In fact, for α>1−1/e,
(Feige, 1998) showed that finding an action which is at least as good as αf(S∗)is NP-hard. However, the
standard greedy approach obtains a set which is at least as good as (1−1/e)f(S∗)(Nemhauser et al., 1978).
Therefore, throughout this paper, we will focus on minimizing (1−1/e)-regret and drop the subscript when
there is no ambiguity.
We consider three settings: bounded adversarial delay and unbounded stochastic independent delay, and
unbounded stochastic conditionally independent delay, described next.
Example 1. To elaborate on the nature of the delay, let us ignore the combinatorial aspect of the problem
for the moment and consider the following setting. A retailer, that sells both food and computer products,
can buy an advertisement slot on an E-commerce platform, e.g., Amazon or eBay. This is a 2-armed bandit
where we assume that the retailer buys an ad slot for a product at each time-step. We further assume that
each time-step is a single day and the only information revealed to the retailer every day is the total added
revenue as a result of the advertisements.
A delay distribution is a sequence of real numbers that add to one, e.g., δ= (0.9,0.05,0.05,0,···). Such a
delay means that 90%of the reward (increase in revenue as a result of the ads) is received immediately, while
5%of the reward is received in each of the next 2 time-steps. Clearly it is not enough to consider a fixed
delay distribution. Therefore we consider a situation where ∆is a random variable where δis a realization
of∆.
It is reasonable to assume that the effect of an ad for food is more immediately seen in the revenue compared
to the effect of an ad for computer products. Therefore we may consider a setting where ∆Fis a random
delay distribution corresponding to food and ∆Ccorrespond to computer products and ∆F̸= ∆C. This
corresponds to the setting considered in (Wang et al., 2021) and (Garg & Akash, 2019).
Now assume that a sale for computer products, but not food, is going to start next week. Modeling this
scenario means that ∆should change over time, but should also depend on the action, since only one of the
actions is affected by the sales. This corresponds to Unbounded Stochastic Conditionally Independent Delay
considered in our paper.
If we instead assume that the delay changes over time, but does not depend on the arm (for example if the
retailer is selling different types of computer products), then this will be Unbounded Stochastic Independent
Delay.
Finally, if delay is too complicated to be covered by previous settings, then we consider Bounded Adversarial
Delay. For example, consider a scenario where different retailers pay the E-commerce platform for adver-
tisement slots, but when the ad is shown depends on the buyers and the actions of other retailers, which can
not be known in advance. The boundedness assumption guarantees that for each ad slot purchased, the effect
on the revenue of the retailer will be limited to a fixed time, e.g. one month, from the purchase of the ad.
2.1 Unbounded Stochastic Independent Delay
In the unbounded stochastic independent delay case, we assume that there is a sequence of random delay
distributions (∆t)∞
t=1that is pair-wise independent, such that
Xt=t/summationdisplay
i=1Fi(Si)∆i(t−i).
In other words, at each time-step t, the observed reward is based on all the actions that have been taken in
the past and the action taken in time-step i≤tcontributes to the observation proportional to the value of
4Under review as submission to TMLR
the delay distribution at time i,∆i, evaluated at t−i. We call this feedback model composite anonymous
unbounded stochastic independent delay feedback .
Todefine ∆t,let(δi)i∈Jbedistributionschosenfrom T,whereJisafiniteindexsetandeach δiisrepresented
by a vector of its probability mass function. Thus, δi(x) =P(δi=x), for allx≥0. LetPtbe a random
variables taking values in J, wherePt(i) =P(Pt=i). Further, we define ∆t(x) :=/summationtext
i∈JPt(i)δi(x), for all
x≥0. Finally, ∆tis defined as a vector (∆t(0),∆t(1),···). Note that/summationtext∞
i=0∆t(i) = 1. The expectation of
∆tover the randomness of Ptis denoted by ET(∆t)which is a distribution given δi’s are distributions.
More generally, we may drop the assumption that Jis finite and define ∆tmore directly as follows. Each ∆t
is a random variable taking values in the set T. In other words, for all x≥0, the value of ∆t(x) = ∆t({x})
is a random variable taking values in [0,1]such that/summationtext∞
i=0∆t(i) = ∆t({0,1,2,···}) = 1. We define ET(∆t)
as the distribution over the set of non-negative integers for which we have
∀x≥0,ET(∆t)({x}) =ET(∆t({x}))∈[0,1].
Wewillalsoexplainthesedefinitionsbyanexample. Let Tbeafamilyofdistributionssupportedon {0,1,2}.
We chooseJ={1,2}, withδias the uniform distribution over {0,1}andδ2as the uniform distribution
over{0,2}. Then, we have δ1(0) =δ1(1) = 1/2andδ2(0) =δ2(2) = 1/2. Further, let P1be a random
variable such that P1(1) +P1(2) = 1. Then, ∆1(x) =/summationtext
i=1,2P1(i)δi(x)gives ∆1(0) =P1(1)/2 +P1(2)/2,
∆1(1) =P1(1)/2and∆1(2) =P1(2)/2.
Note that the independence implies that ∆tcan not depend on the action St, as this action depends on the
history of observations, which is not independent from (∆j)t−1
j=1.
Without any restriction on the delay distributions, there may not be any reward within time Tand thus
no structure of the rewards can be exploited. Thus, we need to have some guarantee that the delays do
not escape to infinity. An appropriate formalization of this idea is achieved using the following tightness
assumption.
Assumption 1. The family of distributions (ET(∆t))∞
t=1is tight.
Recall that a family (δi)i∈Iis called tight if and only if for every positive real number ϵ, there is an integer
jϵsuch thatδi({x≥jϵ})≤ϵ, for alli∈I. (See e.g. (Billingsley, 1995))
Remark1.IfTis tight, then (ET(∆t))∞
t=1is trivially tight. Note that if Tis finite, then it is tight. Similarly,
if(ET(∆t))∞
t=1is constant and therefore only takes one value, then it is tight. As a special case, if (∆t)∞
t=1
is identically distributed, then (ET(∆t))∞
t=1is constant and therefore tight.
To quantify the tightness of a family of probability distribution, we define the notion of upper tail bound .
Definition 1. Let(δi)i∈Ibe a family of probability distributions over the set of non-negative integers. We
sayδis anupper tail bound for this family if
δi({x≥j})≤δ({x≥j}),
for alli∈Iandj≥0.
In the following result (with proof in Appendix B), we show that the tightness and the existence of upper
tail bounds are equivalent.
Lemma 1. Let(δi)i∈Ibe a family of probability distributions over the set of non-negative integers. Then
this family is tight, if and only if it has an upper tail bound.
A tail upper bound allows us to estimate and bound the effect of past actions on the current observed reward.
More precisely, given an upper tail bound τfor the family (ET(∆t))∞
t=1, the effect of an action taken at time
ion the observer reward at tis proportional to ∆i(t−i), which can be bounded in expectation by τ.
ET(∆i(t−i))≤ET(∆i({x≥t−i}))≤τ({x≥t−i}).
As we will see, only the expected value of the upper tail bound appears in the regret bound.
5Under review as submission to TMLR
2.2 Unbounded Stochastic Conditionally Independent Delay
Intheunboundedstochasticconditionallyindependentdelaycase, weassumethatthereisafamilyofrandom
delay distributions {∆t,S}t≥1,S∈Ssuch that for any S∈S, the sequence (∆t,S)∞
t=1is pair-wise independent
and
Xt=t/summationdisplay
i=1Fi(Si)∆i,Si(t−i).
We call this feedback model composite anonymous unbounded stochastic conditionally independent delay
feedback.
In this case the delay ∆t= ∆t,Stcan depend on the action St, but conditioned on the current action, it is
independent of (some of the) other conditional delays. Similar to the stochastic independent delay setting,
we assume that the sequence {ET(∆t,S)}t≥1,S∈Sis tight.
Remark 2.In previously considered stochastic composite anonymous feedback models (e.g., (Wang et al.,
2021; Garg & Akash, 2019)), the delay distribution is independent of time. In other words, every action Shas
a corresponding random delay distribution ∆S, and the sequence (∆t,S)∞
t=1is independent and identically
distributed. Therefore, the number of distributions in the set {ET(∆t,S)}t≥1,S∈Sis less than or equal to the
number of arms, which is finite. Hence the family {ET(∆t,S)}t≥1,S∈Sis tight.
2.3 Bounded Adversarial DelayAlgorithm 1 ETCG algorithm
Input:Set of base arms Ω, horizonT, cardi-
nality constraint k
Assumption: n≤T
1:S(0)←∅,n←|Ω|
2:m←⌈(T/n)2/3⌉
3:forphasei∈{1,2,···,k}do
4:forarma∈Ω\S(i−1)do
5:PlayS(i−1)∪{a}armmtimes
6:Calculate the empirical mean ¯xi,a
7:end for
8:ai←argmaxa∈Ω\S(i−1)¯xi,a
9:S(i)←S(i−1)∪{ai}
10:end for
11:forremaining time do
12:Play action S(k)
13:end forIn the bounded adversarial delay case, we assume that
there is an integer d≥0such that for all δ∈T, we have
δ({x>d}) = 0. Here we have
Xt=t/summationdisplay
i=max{1,t−d}Fi(Si)δi(t−i),
where (δt)∞
t=1is a sequence of distributions in Tchosen by
the environment. Here we used δinstead of ∆to empha-
size the fact that these distributions are not chosen accord-
ing to some random variable with desirable properties. In
fact, the environment may choose δtnon-obliviously, that
is with the full knowledge of the history up to the time-
stept. We call this feedback model composite anonymous
bounded adversarial delay feedback .
3 Regret Analysis with Delayed Feedback
For analyzing the impact of delay, we use the algorithm Explore-Then-Commit-Greedy (ETCG) algorithm,
as proposed in (Nie et al., 2022). We start with S(0)=∅in phasei= 0. In each phase i∈{1,···,k},
we go over the list of all base arms Ω\S(i−1). For each such base arm, we take the action S(i−1)∪{a}
form=⌈(T/n)2/3⌉times and store the empirical mean in the variable ¯Xi,a. Afterwards, we let aito be
the base arm which corresponded to the highest empirical mean and define S(i):=S(i−1)∪{ai}. After the
end of phase k, we keep taking the action S(k)for the remaining time. The algorithm is summarized in
Algorithm 1.
We now provide the main results of the paper that shows the regret bound of Algorithm 1 with delayed
composite feedback for different feedback models. We define two main events that control the delay and the
randomness of the observation. Let I={(i,a)|1≤i≤k,a∈Ω\Si−1}, and define
E:=/braceleftbig
|¯Fi,a−f(Si−1∪{a})|≤rad|(i,a)∈I/bracerightbig
,andE′
d:=/braceleftbigg
|¯Fi,a−¯Xi,a|≤2d
m|(i,a)∈I/bracerightbigg
,
6Under review as submission to TMLR
where rad,d> 0are real numbers that will be specified later. We may drop the subscript dwhen it is clear
from the context. When Ehappens, the average observed reward associated with each arm stays close its
expectation, which is the value of the submodular function. When E′
dhappens, the average observed reward
for each arm remains close to the average total reward associated with playing that arm. The next result
bounds the regret as:
Theorem 1. For alld>0, we have
E(R)≤mnk + 2kTrad +4kTd
m+ 2nkTexp(−2mrad2) +T(1−P(E′
d)).
See Appendix C for a detailed proof. To obtain the regret bounds for different settings, we need to find
lower bounds for P(E′
d)and use Theorem 1.
Theorem 2 (Bounded Adversarial Delay) .If the delay is uniformly bounded by d, then we have
E(R) =O(kn1/3T2/3(log(T))1/2) +O(kn2/3T1/3d).
Proof.The detailed proof is provided in Appendix D. Here, we describe the proof outline. In this setting,
there is an integer d≥0such thatδt({x>d}) = 0, for allt≥1. Therefore, for any mconsecutive time-steps
ti,a≤t≤t′
i,a, the effect of delay may only be observed in the first dand the last dtime-steps. It follows
that/vextendsingle/vextendsingle/vextendsingle/summationtextt′
i,a
t=ti,aXt−/summationtextt′
i,a
t=ti,aFt/vextendsingle/vextendsingle/vextendsingle≤2d,for all (i,a)∈I. Therefore, in this case, we have P(E′
d) = 1. Note that
we are not making any assumptions about the delay distributions. Therefore, the delay may be chosen by
an adversary with the full knowledge of the environment, the algorithm used by the agent and the history
of actions and rewards. Plugging this in the bound provided by Theorem 1 completes the proof.
We note that /tildewideO(T2/3)is the best known bound for the problem in the absence of the delayed feedback, and
the result here demonstrate an additive impact of the delay on the regret bounds.
Theorem 3 (Stochastic Independent Delay) .If the delay sequence is stochastic and independent and tight
in expectation, then we have
E(R) =O(kn1/3T2/3log(T)) +O(kn2/3T1/3E(τ)),
whereτis an upper tail bound for {ET(∆t)}∞
t=1.
Proof.The detailed proof is provided in Appendix E. Here, we describe the proof outline. We start by
defining the random variables Ci,a=/summationtextt′
i,a
j=1∆j({x > t′
i,a−j}),for all (i,a)∈I. This random variable
measure the effect of actions taken up to t′
n,ion the observed rewards after t′
n,i. In fact, we will see
thatm|¯Xi,a−¯Fi,a|may be bounded by the sum of two terms. One Ci,awhich bounds the amount of
reward that “escapes" from the time interval [ti,a,t′
i,a]. The second one Ci′,a′, where (i′,a′)corresponds
to the action taken before Si−1∪{a}. This bound corresponds to the total of reward of the past actions
that is observed during [ti,a,t′
i,a]. Therefore, in order for the event E′
dto happen, it is sufficient to have
Ci,a≤d, for all (i,a)∈I. SinceCi,ais a sum of independent random variables, we may use Bernstein’s
inequality to see that P(Ci,a>E(Ci,a) +λ)≤exp/parenleftig
−λ2
2(E(τ)+λ/3)/parenrightig
.It follows from the definition that
E(Ci,a)≤E(τ). Therefore, by setting d=E(τ) +λ, and performing union bound on the complement of E′
d
givesP(E′
d)≥1−nkexp/parenleftig
−λ2
2(E(τ)+λ/3)/parenrightig
. Plugging this in Theorem 1 and choosing appropriate λgives us
the desired result.
Theorem 4 (Stochastic Conditionally Independent Delay) .If the delay sequence is stochastic, conditionally
independent and tight in expectation, then we have
E(R) =O(k2n4/3T2/3log(T)) +O(k2n5/3T1/3E(τ))
whereτis an upper tail bound for (ET(∆t,S))t≥1,S∈S.
7Under review as submission to TMLR
Proof.The detailed proof is provided in Appendix E and is similar to the proof of Theorem 3. The main
difference is that here we define C′
i,a=/summationtextt′
i,a
j=ti,a∆j({x>t′
i,a−j}),instead ofCi,a. Note that the sum here is
only over the time-steps where the action Si−1∪{a}is taken. Therefore C′
i,ais the sum of mindependent
term. On the other hand, when we try to bound m|¯Xi,a−¯Fi,a|, we decompose it into the amount of total
reward that “escapes" from the time interval [ti,a,t′
i,a]and the contribution of all the time intervals of the
form [ti′,a′,t′
i′,a′]in the past. Since the total number of such intervals is bounded by nk, here we find the
probability that C′
i,a≤2d
nkinstead ofCi,a≤das we did in the proof of Theorem 3. This is the source of
the multiplicative factor of nkwhich appears behind the regret bound of this setting when compared to the
stochastic independent delay setting.
4 Beyond Monotone Submodular Bandits
We note that (Nie et al., 2023) provided a generalized framework for combinatorial bandits with full bandit
feedback, where under a robustness guarantee, explore-then-commit (ETC) based algorithm have been used
to get provable regret guarantees. More precisely, let Abe an algorithm for the combinatorial optimization
problem of maximizing a function f:S→Rover a finite domain S⊆2Ωwith the knowledge that fbelongs
to a known class of functions F. for any function ˆf:S→R, letSA,ˆfdenote the output of Awhen it is
run with ˆfas its value oracle. The algorithm Acalled (α,δ)-robust if for any ϵ>0and any function ˆfsuch
that|f(S)−ˆf(S)|<ϵfor allS∈S, we have
f(SA,ˆf)≥αf(S∗)−δϵ.
It is shown in (Nie et al., 2023) that if Ais(α,δ)-robust, then the C-ETC algorithm achieves α-regret bound
ofO(N1/3δ2/3T2/3(log(T))1/2), whereNis an upper-bound for the number of times Aqueries the value
oracle (the detailed result and algorithm is given in Appendix G). In this work, we show that the result
could be extended directly with delayed composite anonymous bandit feedback. The proof requires small
changes, and are detailed in Appendix G. If Ais(α,δ)-robust, then the results with bandit feedback are as
follows.
Theorem 5. If the delay is uniformly bounded by d, then we have
E(Rα) =O(N1/3δ2/3T2/3(log(T))1/2) +O(N2/3δ1/3T1/3d).
Theorem 6. If the delay sequence is stochastic, then we have
E(Rα) =O(N1/3δ2/3T2/3log(T)) +O(N2/3δ1/3T1/3E(τ)),
whereτis an upper tail bound for (ET(∆t))∞
t=1.
Theorem 7. If the delay sequence is stochastic and conditionally independent, then we have
E(Rα) =O(N4/3δ2/3T2/3log(T)) +O(N5/3δ1/3T1/3E(τ)),
whereτis an upper tail bound for (ET(∆t))∞
t=1.
This shows that the proposed approach in this paper that deals with feedback could be applied on wide
variety of problems. The problems that satisfy the robustness guarantee include submodular bandits with
knapsack constraints and submodular bandits with cardinality constraints (considered earlier).
5 Experiments
Inourexperiments, weconsidertwoclassesofsubmodularfunctions(Linear(F1)andWeightCover(F2))and
and six types of delay (No Delay (D1), two setups of Stochastic Independent Delay (D2, D3), two setups of
StochasticConditionallyIndependentDelay(D4, D5), andAdversarialDelay(D6)). Forlinearfunction(F1),
we chooseF(S) :=1
k/summationtext
a∈Sg(a)+Nc(0,0.1)whereNc(0,0.1)is the truncated normal distribution with mean
8Under review as submission to TMLR
102103104105106
T101102103104105Cumulative Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB
(a) (F1)-(D1)
102103104105106
T101102103104105Cumulative Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB (b) (F1)-(D2)
102103104105106
T101102103104105Cumulative Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB (c) (F1)-(D3)
102103104105106
T101102103104105Cumulative Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB
(d) (F1)-(D4)
102103104105106
T101102103104105Cumulative Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB (e) (F1)-(D5)
102103104105106
T101102103104105Cumulative Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB (f) (F1)-(D6)
102103104105106
T101102103104105Cumulative Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB
(g) (F2)-(D1)
102103104105106
T101102103104105Cumulative Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB (h) (F2)-(D2)
102103104105106
T101102103104105Cumulative Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB (i) (F2)-(D3)
102103104105106
T101102103104105Cumulative Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB
(j) (F2)-(D4)
102103104105106
T101102103104105Cumulative Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB (k) (F2)-(D5)
102103104105106
T101102103104105Cumulative Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB (l) (F2)-(D6)
Figure 1: This plot shows the average cumulative 1-regret over horizon for each setting in the log-log scale.
The dashed lines are y=aT2/3fora∈{0.1,1,10}. Note that (F1) is a linear function and (D1) is the setting
with no delay. Moreover, (D2) corresponds to a delay setting where delay distributions are concentrated
near zero and decay exponentially.
0andstandarddeviation 0.1,truncatedtotheinterval [−0.1,1.0],n= 20andk= 4andchoose g(a)uniformly
from [0.1,0.9], for alla∈Ω. For weight cover function (F2), we choose ft(S) :=1
k/summationtext
j∈Jwt(j) 1S∩Cj̸=∅,
wherewt(j) =U([0,j/5])be samples uniformly from [0,j/5]forj∈1,2,3,4,n= 20andk= 4,(Cj)j∈Jis a
partition of Ωwhere Ωis divided into 4 categories of sizes 6,6,6,2. Stochastic set cover may be viewed as a
simple model for product recommendation, and more details on the function choices is in Appendix H. For
9Under review as submission to TMLR
the delay types, (D2) assumes ∆t(i) = (1−Xt)Xi
t, where (Xt)∞
t=1is an i.i.d sequence of random variables
with the uniform distribution U([0.5,0.9])for allt≥1andi≥0. (D3) assumes ∆tis a distribution over
[10,30]is sampled uniformly from the probability simplex using the flat Dirichlet distribution for all t≥0.
(D4) assumes ∆t(i) = (1−Yt)Yi
t, whereYt= 0.5 +ft∗0.4∈[0.5,0.9]for allt≥1andi≥0. (D5) assumes
∆tas deterministic taking value at lt=⌊20ft⌋+ 10for allt. (D6) ssumes ∆tas deterministic taking value
atlt=⌊20xt−1⌋+ 10withl1= 15for allt>1. The setups are detailed in Appendix H.
For comparisons, we use the baselines of CMAB-SM (Agarwal et al., 2022), DART(Agarwal et al., 2021),
OGo(Streeter & Golovin, 2008), and ARS-UCB (Wang et al., 2021), with details in Appendix H. We use
n= 20base arms and cardinality constraint k= 4. We run each experiment for different time horizons
T={102,103,104,105,106}. For each horizon, we run the experiment 10 times. In these experiments, ETCG
outperforms all other baselines for the weighted cover function by almost an order of magnitude. The linear
submodular function satisfies the conditions under which DART and CMAB-SM were designed. However,
the weighed cover function does not satisfy such conditions and therefore more difficult for those algorithms
to run. In both cases, we see that any kind of delay worsens the performance of DART and CMAB-SM
compared to ETCG. OGoexplores actions (including those with cardinality smaller then k) with a constant
probability, which could account for its lower performance compared to ETCG, DART, and CMAB-SM.
While ARS-UCB does not perform well in these experiments, it should be noted that, given enough time,
it should outperform ETCG. Also note that ARS-UCB has a linear storage complexity with respect to its
number of arms. This translates to an O(/parenleftbign
k/parenrightbig
)storage complexity in the combinatorial setting. Therefore,
even forn= 50andk= 25, it would require hundreds of terabytes of storage to run. In these experiments,
we haven= 20andk= 4, so it has only/parenleftbig20
4/parenrightbig
= 4845arms.
6 Conclusion
This paper considered the problem of combinatorial multiarmed bandits with stochastic submodular (in
expectation) rewards and delayed composite anonymous bandit feedback and provides first regret bound
results for this setup. Three models of delayed feedback: bounded adversarial, stochastic independent, and
stochastic conditionally independent are studied, and regret bounds are derived for each of the delay models.
The regret bounds demonstrate an additive impact of delay in the regret term.
Limitations: This paper demonstrates an additive impact of delay in the regret term, where the non-delay
term is the state-of-the-art regret bound. We note that this state-of-the-art regret bound is ˜O(T2/3), while
there is no matching lower bound. Further, our result shows ˜O(T1/3)dependence in the additive delay term,
while exploring optimality of such dependence is open.
References
Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. Advances in neural infor-
mation processing systems , 24, 2011.
Mridul Agarwal, Vaneet Aggarwal, Abhishek Kumar Umrawal, and Chris Quinn. Dart: Adaptive accept
reject algorithm for non-linear combinatorial bandits. Proceedings of the AAAI Conference on Artificial
Intelligence , 35(8):6557–6565, May 2021.
Mridul Agarwal, Vaneet Aggarwal, Abhishek K Umrawal, and Christopher J Quinn. Stochastic top k-subset
bandits with linear space and non-linear feedback with applications to social influence maximization.
ACM/IMS Transactions on Data Science (TDS) , 2(4):1–39, 2022.
P. Billingsley. Probability and Measure . Wiley Series in Probability and Statistics. Wiley, 1995. ISBN
9780471007104.
Nicolò Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. Nonstochastic bandits with composite anony-
mousfeedback. InSébastienBubeck, VianneyPerchet, andPhilippeRigollet(eds.), Proceedings of the 31st
Conference On Learning Theory , volume 75 of Proceedings of Machine Learning Research , pp. 750–773.
PMLR, 06–09 Jul 2018.
10Under review as submission to TMLR
Lin Chen, Christopher Harshaw, Hamed Hassani, and Amin Karbasi. Projection-free online optimization
with stochastic gradient: From convexity to submodularity. In International Conference on Machine
Learning , pp. 814–823. PMLR, 2018a.
Lixing Chen, Jie Xu, and Zhuo Lu. Contextual combinatorial multi-armed bandits with volatile arms and
submodular reward. Advances in Neural Information Processing Systems , 31, 2018b.
Wei Chen, Yifei Yuan, and Li Zhang. Scalable influence maximization in social networks under the linear
threshold model. In 2010 IEEE international conference on data mining , pp. 88–97. IEEE, 2010.
Wei Chen, Wei Lu, and Ning Zhang. Time-critical influence maximization in social networks with time-
delayed diffusion process. In Twenty-Sixth AAAI Conference on Artificial Intelligence , 2012.
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback.
InAnnual Conference Computational Learning Theory , 2008.
Thomas Desautels, Andreas Krause, and Joel W Burdick. Parallelizing exploration-exploitation tradeoffs in
gaussian process bandit optimization. Journal of Machine Learning Research , 15:3873–3923, 2014.
Pedro Domingos and Matt Richardson. Mining the network value of customers. In Proceedings of the seventh
ACM SIGKDD international conference on Knowledge discovery and data mining , pp. 57–66, 2001.
Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong
Zhang. Efficient optimal learning for contextual bandits. arXiv preprint arXiv:1106.2369 , 2011.
Uriel Feige. A threshold of ln n for approximating set cover. J. ACM, 45(4):634–652, jul 1998. ISSN
0004-5411. doi: 10.1145/285055.285059.
SiddhantGargandAdityaKumarAkash. StochasticBanditswithDelayedCompositeAnonymousFeedback,
October 2019. arXiv:1910.01161 [cs, stat].
José Luis Iribarren and Esteban Moro. Impact of human activity patterns on the dynamics of information
diffusion. Physical review letters , 103(3):038702, 2009.
Pooria Joulani, Andras Gyorgy, and Csaba Szepesvari. Online learning under delayed feedback. In Sanjoy
Dasgupta and David McAllester (eds.), Proceedings of the 30th International Conference on Machine
Learning , volume 28 of Proceedings of Machine Learning Research , pp. 1453–1461, Atlanta, Georgia, USA,
17–19 Jun 2013. PMLR.
Márton Karsai, Mikko Kivelä, Raj Kumar Pan, Kimmo Kaski, János Kertész, A-L Barabási, and Jari
Saramäki. Small but slow world: How network topology and burstiness slow down spreading. Physical
Review E , 83(2):025102, 2011.
David Kempe, Jon Kleinberg, and Éva Tardos. Maximizing the spread of influence through a social network.
InProceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data
mining, pp. 137–146, 2003.
Tian Lin, Jian Li, and Wei Chen. Stochastic online greedy learning with semi-bandit feedbacks. Advances
in Neural Information Processing Systems , 28, 2015.
Chris Mesterharm. On-line learning with delayed label feedback. In International Conference on Algorithmic
Learning Theory , pp. 399–413. Springer, 2005.
George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maxi-
mizing submodular set functions—i. Mathematical programming , 14:265–294, 1978.
Rad Niazadeh, Negin Golrezaei, Joshua R Wang, Fransisca Susan, and Ashwinkumar Badanidiyuru. Online
learning via offline greedy algorithms: Applications in market design and optimization. In Proceedings of
the 22nd ACM Conference on Economics and Computation , pp. 737–738, 2021.
11Under review as submission to TMLR
Guanyu Nie, Mridul Agarwal, Abhishek Kumar Umrawal, Vaneet Aggarwal, and Christopher John Quinn.
An explore-then-commit algorithm for submodular maximization under full-bandit feedback. In Proceed-
ings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence , pp. 1541–1551. PMLR,
August 2022.
Guanyu Nie, Yididiya Y Nadew, Yanhui Zhu, Vaneet Aggarwal, and Christopher John Quinn. A framework
for adapting offline algorithms to solve combinatorial multi-armed bandit problems with bandit feedback,
2023.
Ciara Pike-Burke, Shipra Agrawal, Csaba Szepesvari, and Steffen Grunewalder. Bandits with delayed,
aggregated anonymous feedback. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research ,
pp. 4105–4113. PMLR, 10–15 Jul 2018.
Idan Rejwan and Yishay Mansour. Top- kcombinatorial bandits with full-bandit feedback. In Algorithmic
Learning Theory , pp. 752–776. PMLR, 2020.
Matthew Streeter and Daniel Golovin. An online algorithm for maximizing submodular functions. In
D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou (eds.), Advances in Neural Information Processing
Systems, volume 21. Curran Associates, Inc., 2008.
Matthew Streeter, Daniel Golovin, and Andreas Krause. Online learning of assignments. In Y. Bengio,
D.Schuurmans, J.Lafferty, C.Williams, andA.Culotta(eds.), Advances in Neural Information Processing
Systems, volume 22. Curran Associates, Inc., 2009.
Maxim Sviridenko. A note on maximizing a submodular set function subject to a knapsack constraint.
Operations Research Letters , 32(1):41–43, 2004.
Sho Takemori, Masahiro Sato, Takashi Sonoda, Janmajay Singh, and Tomoko Ohkuma. Submodular bandit
problem under multiple constraints. In Conference on Uncertainty in Artificial Intelligence , pp. 191–200.
PMLR, 2020.
Siwei Wang, Haoyun Wang, and Longbo Huang. Adaptive algorithms for multi-armed bandit with composite
and anonymous feedback. Proceedings of the AAAI Conference on Artificial Intelligence , 35(11):10210–
10217, May 2021. doi: 10.1609/aaai.v35i11.17224.
Raymond Chi-Wing Wong, Ada Wai-Chee Fu, and Ke Wang. Mpis: Maximal-profit item selection with
cross-selling considerations. In Third IEEE International Conference on Data Mining , pp. 371–378. IEEE,
2003.
Yu Xiang, Tian Lan, Vaneet Aggarwal, and Yih Farn R Chen. Joint latency and cost optimization for
erasurecoded data center storage. ACM SIGMETRICS Performance Evaluation Review , 42(2):3–14, 2014.
Yisong Yue and Carlos Guestrin. Linear submodular bandits and their application to diversified retrieval.
Advances in Neural Information Processing Systems , 24, 2011.
Mingrui Zhang, Lin Chen, Hamed Hassani, and Amin Karbasi. Online continuous submodular maximization:
From full-information to bandit feedback. Advances in Neural Information Processing Systems , 32, 2019.
Junlong Zhu, Qingtao Wu, Mingchuan Zhang, Ruijuan Zheng, and Keqin Li. Projection-free decentral-
ized online learning for submodular maximization over time-varying networks. The Journal of Machine
Learning Research , 22(1):2328–2369, 2021.
12Under review as submission to TMLR
A Other Related Works
We note that this is the first work to derive regret bounds for CMAB with submodular and monotone
rewards and delayed feedback. Thus, the most related work can be divided into the results for CMAB with
submodular and monotone rewards, and that for MAB with delayed feedback, as will be described next.
A.1 Combinatorial Submodular Bandits
CMABs have been widely studied due to multiple applications. While the problem of CMAB is general and
there are multiple studies that do not use submodular rewards (Agarwal et al., 2021; 2022; Dani et al., 2008;
Rejwan & Mansour, 2020), we consider CMAB with monotone and submodular rewards. The assumption
of monotonicity and submodularity in reward functions is common in the literature (Streeter et al., 2009;
Niazadeh et al., 2021; Nie et al., 2022). For CMAB with monotone and submodular rewards, without any
further constraints, the optimal selection will be the entire set. Thus, additional assumptions are introduced
in the model, including cardinality constraint (Nemhauser et al., 1978) and knapsack constraints (Sviridenko,
2004). This paper considers CMAB with submodular and monotone rewards and cardinality constraint.
Further, we note that feedback pays an important role in CMAB decision making. CMAB with submodular
and monotone rewards and cardinality constraint has been studied with semi-bandit feedback (Lin et al.,
2015; Niazadeh et al., 2021; Zhang et al., 2019; Zhu et al., 2021; Chen et al., 2018a; Takemori et al., 2020).
The semi-bandit feedback setting provides more information as compared to the full-bandit setting. The
same is true for contextual bandit feedback (Yue & Guestrin, 2011; Chen et al., 2018b) as well. Here we
consider the full-bandit (or bandit) feedback without any additional feedback. CMAB with submodular
and monotone rewards, cardinality constraint, and full-bandit feedback has been studied in both adversarial
setting (Niazadeh et al., 2021) and in stochastic setting (Nie et al., 2022). This paper studies the stochastic
setting.
It is worth noting that for submodular bandits, the stochastic reward case is not a special case of the
adversarial reward case and the guarantees for the stochastic reward case are not necessarily better than the
adversarial reward case. In the adversarial setting, the environment chooses a sequence of monotone and
submodular functions {f1,···,fT}. This is incompatible with the stochastic reward setting since we only
require the set function ftto be monotone and submodular in expectation. Thus, the results on adversarial
submodular bandits will not lead to results for the stochastic submodular setting.
These works for CMAB do not study regret bound with delayed feedback, which is the focus of this paper.
A.2 Bandits With Delayed Rewards
The bandit problem with (non-anonymous) delayed feedback has been studied extensively (Mesterharm,
2005; Agarwal & Duchi, 2011; Desautels et al., 2014; Dudik et al., 2011; Joulani et al., 2013). In the non-
anonymous setting, the reward will be delayed and at each time-step, the agent observers a set of the form
{(t,rt)|t∈It}whereItis a set of time-steps in the past. In the aggregated anonymous setting, first studied
by (Pike-Burke et al., 2018), the reward for each arm is obtained at some point in the future, so that the
agent will receive the aggregated reward for some of the past actions at each time-step. (Cesa-Bianchi et al.,
2018) extended the reward model so that the reward of an action is not immediately observed by the agent,
but rather spread over at most dconsecutive steps in an adversarial way. However, they also assumed that
the bandit is adversarial. (Garg & Akash, 2019) considered the stochastic case and provided an algorithm
with a sub-linear regret bound of ˜O(n1/2T1/2) +O(nlog(T)d). In this setting, for each arm a, the there is a
random distribution ∆aover the set{0,1,···,d}and at each time-step, when the agent plays a, the delay
is sampled from ∆a. (Wang et al., 2021) also considers unbounded delay and proves the regret bound of
˜O(n1/2T1/2) +O(ν), whereνdepends on the delay distribution and nbut not on T. They also considered
the case with adversarial but bounded delay and proved a regret bound of ˜O(n1/2T2/3)+O(T2/3d). We note
in all of the works addressing composite anonymous feedback, including ours, a key idea is to repeat actions
enough times so that we can extract meaningful information. This is not always necessary in other types of
delay. In particular, if delay is not anonymous, there is no need to repeat actions since we will eventually
know the reward for each action. Except for ETCG of (Nie et al., 2022) and more generally, instances of
13Under review as submission to TMLR
the meta-algorithm C-ETC algorithm of (Nie et al., 2023), other algorithms discussed here for combinatoral
bandits do not repeat actions and therefore there is little hope of them achieving desirable results in the
presence of composite anonymous delay.
Note that, in the submodular setting, any algorithm that does not exploit the combinatorial structure of the
arms must take at least every action once which can be suboptimal since the number of arms is at least/parenleftbign
k/parenrightbig
which grows exponentially.
In this paper, we extend the delay model further by letting the random delay distribution also depend on
time (See Example 1 and Remark 2 for more details).
B Proof of Lemma 1
Proof.If an upper tail bound δexists, then we may simply define
jϵ:= min{j|δ({x≥j})≤ϵ},
to see that the family is tight. Next we assume that the family is tight and prove the existence of an upper
tail bound.
Letδbe the measure defined by
∀j≥0, δ(j) := sup
i∈Iδi({x≥j})−sup
i∈Iδi({x≥j+ 1}).
Clearly we have δ(j)≥0, for allj≥0. To show that δis a probability distribution, we sum the terms and
see that
δ({t|a≤t≤b}) =b/summationdisplay
t=aδ(t) =b/summationdisplay
t=a/parenleftbigg
sup
i∈Iδi({x≥t})−sup
i∈Iδi({x≥t+ 1})/parenrightbigg
= sup
i∈Iδi({x≥a})−sup
i∈Iδi({x≥b+ 1})
According to the definition of tightness, for all ϵ>0andb≥jϵ, we have
δ({t|a≤t≤b}) = sup
i∈Iδi({x≥a})−sup
i∈Iδi({x≥b+ 1})≥sup
i∈Iδi({x≥a})−ϵ.
Hence we have
δ({t≥0}) = lim
j→∞δ({t|0≤t≤j}) = 1−lim
j→∞sup
i∈Iδi({x≥j+ 1}) = 1.
Finally, to see that δis indeed an upper tail bound, we note that
δ({t|a≤t≤b}) = sup
i∈Iδi({x≥a})−sup
i∈Iδi({x≥b+ 1})≤sup
i∈Iδi({x≥a}).
Therefore
δ({t≥a}) = lim
b→∞δ({t|a≤t≤b})≤sup
i∈Iδi({x≥a}).
C Lemmas used in the proofs
In this section, we will provide the Lemmas that will be used in the proof of the main results. Let ti,adenote
the first time-step where the action S(i−1)∪{a}is played in the exploration phase and let t′
i,a:=ti,a+m−1
be the last such time-step. Therefore, we have
¯Fi,a=1
mt′
i,a/summationdisplay
t=ti,aFt,¯Xi,a=1
mt′
i,a/summationdisplay
t=ti,aXt.
14Under review as submission to TMLR
Similarly, the realized value of these random variables are
¯fi,a=1
mt′
i,a/summationdisplay
t=ti,aft,¯xi,a=1
mt′
i,a/summationdisplay
t=ti,axt.
For any phase iand arma∈Ω\Si−1, define the event
Ei,a:=/braceleftbig
|¯Fi,a−f(Si−1∪{a})|≤rad/bracerightbig
,
where radis a non-negative real number to be specified later. Using these events, we define
Ei:=/intersectiondisplay
a∈Ω\S(i−1)Ei,a,E:=k/intersectiondisplay
i=1Ei.
Lemma 2. We have
P(E)≥1−2nkexp(−2mrad2).
Proof.We haveFt∈[0,1]. Therefore, using Hoeffding’s inequality, we have
P(|¯Fi,a−f(Si−1∪{a})|>rad) = P
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglet′
i,a/summationdisplay
ti,aFt−mf(Si−1∪{a})/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle>m rad

≤2 exp/parenleftbigg
−2(mrad)2
m/parenrightbigg
= 2 exp(−2mrad2).
Hence
P(E) =P
k/intersectiondisplay
i,aEi,a

= 1−P
k/uniondisplay
i,a(Ei,a)c

≥1−/summationdisplay
i,aP((Ei,a)c)
= 1−/summationdisplay
i,aP(|¯Fi,a−f(Si−1∪{a})|>rad)
≥1−/summationdisplay
i,a2 exp(−2mrad2)
≥1−2nkexp(−2mrad2).
Next we define another set of events where the delay is controlled. Let
E′
d,i,a:=/braceleftbigg
|¯Xi,a−¯Fi,a|≤2d
m/bracerightbigg
,
for somed>0which will be specified later. Similar to above, we use these events to define
E′
d,i:=/intersectiondisplay
a∈Ω\S(i−1)Ed,i,a,E′
d:=k/intersectiondisplay
i=1E′
d,i.
Note thatE′
dcan happen even if the delay is not bounded. Later in Lemmas 5 and 6, we will find lower
bounds on the probability of E′
din both the adversarial and the stochastic setting.
15Under review as submission to TMLR
Lemma 3. Under the eventE∩E′
d, for all 1≤i≤kandd>0, we have
f(S(i))−f(S(i−1))≥1
k/bracketleftig
f(S∗)−f(S(i−1))/bracketrightig
−2 rad−4d
m.
Proof.Recall that aiis the sole element in Si\Si−1. That is,
ai= argmaxa∈Ω\S(i−1)¯xi,a.
Define
a∗
i:= argmaxa∈Ω\S(i−1)f(Si−1∪{a}).
Then we have
f(Si) =f(Si−1∪{ai})
≥¯fi,ai−rad (definition ofE)
≥¯xi,ai−2d
m−rad (definition ofE′)
≥¯xi,a∗
i−2d
m−rad (definition of a∗
i)
≥¯fi,a∗
i−4d
m−rad (definition ofE′)
≥f(Si−1∪{a∗
i})−4d
m−2 rad. (definition ofE)
Hence we have
f(Si)−f(Si−1)≥f(Si−1∪{a∗
i})−f(Si−1)−4d
m−2 rad.
Therefore
f(Si)−f(Si−1)≥f(Si−1∪{a∗
i})−f(Si−1)−4d
m−2 rad
= maxa∈Ω\S(i−1)f(Si−1∪{a})−f(Si−1)−4d
m−2 rad (definition of a∗
i)
≥maxa∈S∗\S(i−1)f(Si−1∪{a})−f(Si−1)−4d
m−2 rad ( S∗⊆Ω)
≥1
|S∗\Si−1|/summationdisplay
a∈S∗\S(i−1)f(Si−1∪{a})−f(Si−1)−4d
m−2 rad (maximum≥mean)
=1
|S∗\Si−1|/summationdisplay
a∈S∗\S(i−1)/bracketleftbig
f(Si−1∪{a})−f(Si−1)/bracketrightbig
−4d
m−2 rad
≥1
k/summationdisplay
a∈S∗\S(i−1)/bracketleftbig
f(Si−1∪{a})−f(Si−1)/bracketrightbig
−4d
m−2 rad ( |S∗\Si−1|≤|S∗|=k)
≥1
k/bracketleftbig
f(S∗)−f(Si−1)/bracketrightbig
−4d
m−2 rad,
where the last line follows from a well-known inequality for submodular functions.
Corollary 1. Under the eventE∩E′
d, for alld>0, we have
f(S(k))≥(1−1
e)f(S∗)−2krad−4kd
m.
Proof.Using Lemma 3, we have
f(Si)≥f(Si−1) +1
k(f(S∗)−f(Si−1))−2 rad−4d
m=/bracketleftbigg1
kf(S∗)−2 rad−4d
m/bracketrightbigg
+ (1−1
k)f(Si−1).
16Under review as submission to TMLR
Applying this inequality recursively, we get
f(Sk)≥/bracketleftbigg1
kf(S∗)−2 rad−4d
m/bracketrightbigg
+ (1−1
k)f(Sk−1)
≥/bracketleftbigg1
kf(S∗)−2 rad−4d
m/bracketrightbigg
+ (1−1
k)/parenleftbigg/bracketleftbigg1
kf(S∗)−2 rad−4d
m/bracketrightbigg
+ (1−1
k)f(Sk−2)/parenrightbigg
=/bracketleftbigg1
kf(S∗)−2 rad−4d
m/bracketrightbigg1/summationdisplay
l=0(1−1
k)l+ (1−1
k)2f(Sk−2)
...
≥/bracketleftbigg1
kf(S∗)−2 rad−4d
m/bracketrightbiggk−1/summationdisplay
l=0(1−1
k)l+ (1−1
k)kf(S0)
=/bracketleftbigg1
kf(S∗)−2 rad−4d
m/bracketrightbiggk−1/summationdisplay
l=0(1−1
k)l
Note that we have
k−1/summationdisplay
l=0(1−1
k)l=1−(1−1
k)k
1−(1−1
k)=k/parenleftbigg
1−(1−1
k)k/parenrightbigg
.
Hence
f(Sk)≥/bracketleftbigg1
kf(S∗)−2 rad−4d
m/bracketrightbigg
k/parenleftbigg
1−(1−1
k)k/parenrightbigg
=/parenleftbigg
1−(1−1
k)k/parenrightbigg
f(S∗)−/parenleftbigg
2krad−4kd
m/parenrightbigg/parenleftbigg
1−(1−1
k)k/parenrightbigg
≥/parenleftbigg
1−(1−1
k)k/parenrightbigg
f(S∗)−2krad−4kd
m.
Using the well known inequality (1−1
k)k≤1
e, we get
f(Sk)≥/parenleftbigg
1−1
e/parenrightbigg
f(S∗)−2krad−4kd
m.
Lemma 4. For alld>0, we have
E(R|E∩E′
d)≤mnk + 2kTrad +4kTd
m.
Proof.LetR=Rexploration +Rexploitation . The exploration phase is at most mnksteps, therefore we always
have
Rexploration≤mnk.
At each time-step in the exploitation phase, (1−1
e)f(S∗)−f(Sk)to is added to the expected regret. Hence
we have
E(R|E∩E′
d) =E(Rexploration|E∩E′
d) +E(Rexploitation|E∩E′
d)
≤mnk +T/bracketleftbigg
(1−1
e)f(S∗)−f(Sk)/bracketrightbigg
≤mnk + 2kTrad +4kTd
m,
where we used Corollary 1 in the last inequality.
17Under review as submission to TMLR
Theorem 8 (Theorem 1 in the main text) .For alld>0, we have
E(R)≤mnk + 2kTrad +4kTd
m+ 2nkTexp(−2mrad2) +T(1−P(E′
d)).
Proof.Using Lemmas 4 and 2, we have
E(R) =E(R|E∩E′
d)P(E∩E′
d) +E(R|(E∩E′
d)c)P((E∩E′
d)c)
≤E(R|E∩E′
d) +TP((E∩E′
d)c)
=E(R|E∩E′
d) +TP(Ec∪(E′
d)c)
≤E(R|E∩E′
d) +TP(Ec) +TP((E′
d)c)
≤(mnk + 2kTrad +4kTd
m) + 2nkTexp(−2mrad2) +T(1−P(E′
d)).
D Uniformly Bounded Delay
Lemma 5. If delay is uniformly bounded by d, then P(E′
d) = 1.
Proof.For allt≤0, letFt= 0and letδtbe any distribution over non-negative integers. We have
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglet′
i,a/summationdisplay
t=ti,aXt−t′
i,a/summationdisplay
t=ti,aFt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglet′
i,a/summationdisplay
j=ti,a−dFjδj({ti,a−j≤x≤t′
i,a−j})−t′
i,a/summationdisplay
t=ti,aFt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleti,a−1/summationdisplay
j=ti,a−dFjδj({ti,a−j≤x≤t′
i,a−j}) +t′
i,a/summationdisplay
j=ti,aFjδj({x≤t′
i,a−j})−t′
i,a/summationdisplay
t=ti,aFt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleti,a−1/summationdisplay
j=ti,a−dFjδj({ti,a−j≤x≤t′
i,a−j})−t′
i,a/summationdisplay
j=ti,aFjδj({x>t′
i,a−j})/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤ti,a−1/summationdisplay
j=ti,a−dδj({ti,a−j≤x≤t′
i,a−j}) +t′
i,a/summationdisplay
j=ti,aδj({x>t′
i,a−j})
≤d+t′
i,a/summationdisplay
j=ti,aδj({x>t′
i,a−j}).
Note that for j≤ti,a+m−d−1, we have
δj({x>t′
i,a−j}) =δj({x>ti,a+m−1−j})≤δj({x>d}) = 0.
Therefore we have
t′
i,a/summationdisplay
j=ti,aδj({x>t′
i,a−j}) =ti,a+m−1/summationdisplay
j=max{ti,a,ti,a+m−d}δj({x>t′
i,a−j})
≤(ti,a+m−1)−max{ti,a,ti,a+m−d}+ 1
= min{m,d}≤d.
Hence
|¯Xi,a−¯Fi,a|=1
m/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglet′
i,a/summationdisplay
t=ti,aXt−t′
i,a/summationdisplay
t=ti,aFt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤2d
m.
18Under review as submission to TMLR
Theorem 9 (Theorem 2 in the main text) .If the delay is uniformly bounded by d, then we have
E(R) =O(kn1/3T2/3(log(T))1/2) +O(kn2/3T1/3d).
Proof.Using Theorem 1 and Lemma 5, we see that
E(R)≤mnk + 2kTrad +4kTd
m+ 2nkTexp(−2mrad2).
Letrad :=/radicalig
log(T)
m. Then
exp(−2mrad2) =T−2,
and
E(R)≤mnk + 2kTrad +4kTd
m+ 2nkTexp(−2mrad2)
=mnk + 2kT/radicalbigg
log(T)
m+4kTd
m+ 2nk/T
≤mnk + 2kT/radicalbigg
log(T)
m+4kTd
m+ 2k
where we used T≥nin the last inequality. Since m=⌈(T/n)2/3⌉, we have (T/n)2/3≤m≤(T/n)2/3+ 1.
Therefore
E(R)≤mnk + 2kT/radicalbigg
log(T)
m+4kTd
m+ 2k
≤((T/n)2/3+ 1)nk+ 2kT/radicalig
log(T)/(T/n)2/3+4kTd
(T/n)2/3+ 2k
=kn1/3T2/3+nk+ 2kn1/3T2/3(log(T))1/2) + 4kn2/3T1/3d+ 2k
≤4kn1/3T2/3(log(T))1/2) + 4kn2/3T1/3d+ 2k
=O(kn1/3T2/3(log(T))1/2) +O(kn2/3T1/3d).
E Unbounded Stochastic Independent Delay
Lemma 6. If(∆j)∞
j=1is independent and (ET(∆j))∞
j=1is tight, then we have
P(E′
d)≥1−nkexp/parenleftbigg
−λ2
2(E(τ) +λ/3)/parenrightbigg
,
whered>0is a real number, τis a tail upper bound for the family (ET(∆j))∞
j=1andλ= max{0,d−E(τ)}.
Proof.Ifλ= 0, then the statement is trivially true. So we will assume that λ>0andd=E(τ) +λ. Define
Ci,a=t′
i,a/summationdisplay
j=1∆j({x>t′
i,a−j}).
Using the fact that τis an upper tail bound for (ET(∆j))∞
j=1, we can see that
E(Ci,a) =t′
i,a/summationdisplay
j=1E(∆j({x>t′
i,a−j}))≤t′
i,a/summationdisplay
j=1τ({x>t′
i,a−j})
=t′
i,a/summationdisplay
j=1τ({x≥j})≤∞/summationdisplay
j=0τ({x≥j}) =E(τ).
19Under review as submission to TMLR
Using Bernstein’s inequality, we have
P(Ci,a>d) =P(Ci,a>E(τ) +λ)
≤P(Ci,a>E(Ci,a) +λ)
≤exp
−λ2
2(/summationtextt′
i,a
j=1E(∆j({x>t′
i,a−j})2) +λ/3)

≤exp
−λ2
2(/summationtextt′
i,a
j=1E(∆j({x>t′
i,a−j})) +λ/3)

= exp/parenleftbigg
−λ2
2(E(Ci,a) +λ/3)/parenrightbigg
≤exp/parenleftbigg
−λ2
2(E(τ) +λ/3)/parenrightbigg
.
We haveXt=/summationtextt
j=1Fj(Sj)∆j(t−j). Therefore
m|¯Xi,a−¯Fi,a|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglet′
i,a/summationdisplay
t=ti,aXt−t′
i,a/summationdisplay
t=ti,aFt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglet′
i,a/summationdisplay
j=1Fj∆j({ti,a−j≤x≤t′
i,a−j})−t′
i,a/summationdisplay
t=ti,aFt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleti,a−1/summationdisplay
j=1Fj∆j({ti,a−j≤x≤t′
i,a−j}) +t′
i,a/summationdisplay
j=ti,aFj∆j({x≤t′
i,a−j})−t′
i,a/summationdisplay
t=ti,aFt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleti,a−1/summationdisplay
j=1Fj∆j({ti,a−j≤x≤t′
i,a−j})−t′
i,a/summationdisplay
j=ti,aFj∆j({x>t′
i,a−j})/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤ti,a−1/summationdisplay
j=1∆j({ti,a−j≤x≤t′
i,a−j}) +t′
i,a/summationdisplay
j=ti,a∆j({x>t′
i,a−j})
≤ti,a−1/summationdisplay
j=1∆j({x≥ti,a−j}) +t′
i,a/summationdisplay
j=1∆j({x>t′
i,a−j})
=ti,a−1/summationdisplay
j=1∆j({x>ti,a−1−j}) +Ci,a.
Ifti,a= 1, then the first sum will be zero and we have
m|¯Xi,a−¯Fi,a|≤Ci,a.
Otherwise, there exists (i′,a′)such thatt′
i′,a′=ti,a−1and
m|¯Xi,a−¯Fi,a|≤Ci′,a′+Ci,a.
LetE∗
i,abe the event that Ci,a≤dand define
E∗:=/intersectiondisplay
i,aE∗
i,a.
20Under review as submission to TMLR
Our discussion above shows that we have
P(E′
d)≥P(E∗).
On the other hand, we have
P(E∗) =P
/intersectiondisplay
i,aE∗
i,a

= 1−P
/uniondisplay
i,a(E∗
i,a)c

= 1−P
/uniondisplay
i,a{Ci,a>d}

≥1−/summationdisplay
i,aP({Ci,a>d})
≥1−/summationdisplay
i,aexp/parenleftbigg
−λ2
2(E(τ) +λ/3)/parenrightbigg
≥1−nkexp/parenleftbigg
−λ2
2(E(τ) +λ/3)/parenrightbigg
,
which completes the proof.
Theorem 10 (Theorem 3 in the main text) .If the delay sequence is stochastic, then we have
E(R) =O(kn1/3T2/3log(T)) +O(kn2/3T1/3E(τ)),
whereτis an upper tail bound for (ET(∆t))∞
t=1.
Proof.Letrad :=/radicalig
log(T)
m. Then, using T≥n, we have
2nkTexp(−2mrad2) = 2nkTexp(−2 log(T)) = 2nkT−1≤2k.
We choose d:=E(τ) + max{6E(τ),2 log(T)}andλ=d−E(τ) = max{6E(τ),2 log(T)}. Then we have
exp/parenleftbigg
−λ2
2(E(τ) +λ/3)/parenrightbigg
≤exp/parenleftbigg
−λ2
2(λ/6 +λ/3)/parenrightbigg
= exp(−λ)≤exp(−2 log(T)) =T−2.
Therefore
nkTexp/parenleftbigg
−λ2
2(E(τ) +λ/3)/parenrightbigg
≤nkT−1≤k.
So, using Theorem 1 and Lemma 6, we have
E(R)≤mnk + 2kTrad +4kTd
m+ 2nkTexp(−2mrad2) +T(1−P(E′
d))
≤mnk + 2kTrad +4kTd
m+ 2nkTexp(−2mrad2) +nkTexp/parenleftbigg
−λ2
2(E(τ) +λ/3)/parenrightbigg
≤mnk + 2kTrad +4kTd
m+ 3k
=mnk + 2kT/radicalbigg
log(T)
m+ 4kT
m(E(τ) + max{6E(τ),2 log(T)}) + 3k
≤mnk + 2kT/radicalbigg
log(T)
m+ 4kT
m(7E(τ) + 2 log(T)) + 3k.
21Under review as submission to TMLR
Sincem=⌈(T/n)2/3⌉, we have (T/n)2/3≤m≤(T/n)2/3+ 1. Therefore
E(R)≤mnk + 2kT/radicalbigg
log(T)
m+4kT
m(7E(τ) + 2 log(T)) + 3k
≤((T/n)2/3+ 1)nk+ 2kT/radicaligg
log(T)
(T/n)2/3+4kT
(T/n)2/3(7E(τ) + 2 log(T)) + 3k
=kn1/3T2/3+kn+ 2kn1/3T2/3log(T)1/2+ 28kn2/3T1/3E(τ) + 8kn2/3T1/3log(T) + 3k
≤12kn1/3T2/3log(T) + 28kn2/3T1/3E(τ) + 3k
=O(kn1/3T2/3log(T)) +O(kn2/3T1/3E(τ)).
F Unbounded Stochastic Conditionally Independent Delay
Lemma 7. If(∆j,S)∞
j=1is pairwise independent for all S∈Sand{E(∆j,S)}j≥1,S∈Sis tight, then we have
P(E′
d)≥1−nkexp/parenleftbigg
−λ2
2(E(τ) +λ/3)/parenrightbigg
,
whered > 0is a real number, τis a tail upper bound for the family {ET(∆j,S)}j≥1,S∈Sandλ=
max/braceleftbig
0,2d
nk−E(τ)/bracerightbig
.
Proof.Ifλ= 0, then the statement is trivially true. So we will assume that λ >0andd=nk
2(E(τ) +λ).
Define
C′
i,a=t′
i,a/summationdisplay
j=ti,a∆j({x>t′
i,a−j}).
Note that the sum is only over the time-steps where the action Si−1∪{a}is taken. Therefore C′
i,ais the
sum ofmindependent term. Using the fact that τis an upper tail bound for {ET(∆j,S)}j≥1,S∈S, we can
see that
E(C′
i,a) =t′
i,a/summationdisplay
j=ti,aE(∆j({x>t′
i,a−j}))≤t′
i,a/summationdisplay
j=ti,aτ({x>t′
i,a−j})
≤t′
i,a/summationdisplay
j=1τ({x>t′
i,a−j}) =t′
i,a/summationdisplay
j=1τ({x≥j})≤∞/summationdisplay
j=0τ({x≥j}) =E(τ).
Using Bernstein’s inequality, we have
P/parenleftbigg
C′
i,a>2d
nk/parenrightbigg
=P(C′
i,a>E(τ) +λ)
≤P(C′
i,a>E(C′
i,a) +λ)
≤exp
−λ2
2(/summationtextt′
i,a
j=ti,aE(∆j({x>t′
i,a−j})2) +λ/3)

≤exp
−λ2
2(/summationtextt′
i,a
j=ti,aE(∆j({x>t′
i,a−j})) +λ/3)

= exp/parenleftigg
−λ2
2(E(C′
i,a) +λ/3)/parenrightigg
≤exp/parenleftbigg
−λ2
2(E(τ) +λ/3)/parenrightbigg
.
22Under review as submission to TMLR
We haveXt=/summationtextt
j=1Fj(Sj)∆j(t−j). Therefore
m|¯Xi,a−¯Fi,a|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglet′
i,a/summationdisplay
t=ti,aXt−t′
i,a/summationdisplay
t=ti,aFt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglet′
i,a/summationdisplay
j=1Fj∆j({ti,a−j≤x≤t′
i,a−j})−t′
i,a/summationdisplay
t=ti,aFt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleti,a−1/summationdisplay
j=1Fj∆j({ti,a−j≤x≤t′
i,a−j}) +t′
i,a/summationdisplay
j=ti,aFj∆j({x≤t′
i,a−j})−t′
i,a/summationdisplay
t=ti,aFt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleti,a−1/summationdisplay
j=1Fj∆j({ti,a−j≤x≤t′
i,a−j})−t′
i,a/summationdisplay
j=ti,aFj∆j({x>t′
i,a−j})/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤ti,a−1/summationdisplay
j=1∆j({ti,a−j≤x≤t′
i,a−j}) +t′
i,a/summationdisplay
j=ti,a∆j({x>t′
i,a−j})
≤ti,a−1/summationdisplay
j=1∆j({x>ti,a−1−j}) +C′
i,a.
Define
Ii,a={(i′,a′)|ti′,a′<ti,a}.
Then we have
ti,a−1/summationdisplay
j=1∆j({x>ti,a−1−j}) =/summationdisplay
(i′,a′)∈Ii,at′
i′,a′/summationdisplay
j=ti′,a′∆j({x>ti,a−1−j})
≤/summationdisplay
(i′,a′)∈Ii,at′
i′,a′/summationdisplay
j=ti′,a′∆j({x>t′
i′,a′−j}) =/summationdisplay
(i′,a′)∈Ii,aC′
i′,a′.
Therefore, we have
m|¯Xi,a−¯Fi,a|≤/summationdisplay
i,aC′
i,a≤nkmax
i,a{C′
i,a}.
LetE∗∗
i,abe the event that Ci,a≤2d
nkand define
E∗∗:=/intersectiondisplay
i,aE∗∗
i,a.
Our discussion above shows that we have
P(E′
d)≥P(E∗∗).
23Under review as submission to TMLR
On the other hand, we have
P(E∗∗) =P
/intersectiondisplay
i,aE∗∗
i,a

= 1−P
/uniondisplay
i,a(E∗∗
i,a)c

= 1−P
/uniondisplay
i,a/braceleftbigg
C′
i,a>2d
nk/bracerightbigg

≥1−/summationdisplay
i,aP/parenleftbigg/braceleftbigg
C′
i,a>2d
nk/bracerightbigg/parenrightbigg
≥1−/summationdisplay
i,aexp/parenleftbigg
−λ2
2(E(τ) +λ/3)/parenrightbigg
≥1−nkexp/parenleftbigg
−λ2
2(E(τ) +λ/3)/parenrightbigg
,
which completes the proof.
Theorem11 (Theorem4inthemaintext) .If the delay sequence is stochastic and conditionally independent,
then we have
E(R) =O(kn1/3T2/3(log(T))1/2+k2n5/3T1/3log(T)) +O(k2n5/3T1/3E(τ))
=O(k2n4/3T2/3log(T)) +O(k2n5/3T1/3E(τ))
whereτis an upper tail bound for (ET(∆t))∞
t=1.
Proof.Letrad :=/radicalig
log(T)
m. Then, using T≥n, we have
2nkTexp(−2mrad2) = 2nkTexp(−2 log(T)) = 2nkT−1≤2k.
We choosed:=nk
2(E(τ) + max{6E(τ),2 log(T)})andλ=2d
nk−E(τ) = max{6E(τ),2 log(T)}. Then we have
exp/parenleftbigg
−λ2
2(E(τ) +λ/3)/parenrightbigg
≤exp/parenleftbigg
−λ2
2(λ/6 +λ/3)/parenrightbigg
= exp(−λ)≤exp(−2 log(T)) =T−2.
Therefore
nkTexp/parenleftbigg
−λ2
2(E(τ) +λ/3)/parenrightbigg
≤nkT−1≤k.
So, using Theorem 1 and Lemma 7, we have
E(R)≤mnk + 2kTrad +4kTd
m+ 2nkTexp(−2mrad2) +T(1−P(E′
d))
≤mnk + 2kTrad +4kTd
m+ 2nkTexp(−2mrad2) +nkTexp/parenleftbigg
−λ2
2(E(τ) +λ/3)/parenrightbigg
≤mnk + 2kTrad +4kTd
m+ 3k
=mnk + 2kT/radicalbigg
log(T)
m+2nk2T
m(E(τ) + max{6E(τ),2 log(T)}) + 3k
≤mnk + 2kT/radicalbigg
log(T)
m+2nk2T
m(7E(τ) + 2 log(T)) + 3k.
24Under review as submission to TMLR
Sincem=⌈(T/n)2/3⌉, we have (T/n)2/3≤m≤(T/n)2/3+ 1. Therefore
E(R)≤mnk + 2kT/radicalbigg
log(T)
m+2nk2T
m(7E(τ) + 2 log(T)) + 3k
≤((T/n)2/3+ 1)nk+ 2kT/radicaligg
log(T)
(T/n)2/3+2nk2T
(T/n)2/3(7E(τ) + 2 log(T)) + 3k
=kn1/3T2/3+kn+ 2kn1/3T2/3log(T)1/2+ 14k2n5/3T1/3E(τ) + 4k2n5/3T1/3log(T) + 3k
=O(kn1/3T2/3(log(T))1/2) +O(k2n5/3T1/3log(T)) +O(k2n5/3T1/3E(τ))
=O(k2n4/3T2/3log(T)) +O(k2n5/3T1/3E(τ)).
G Extension to general combinatorial bandits
The results of this paper could be generalized to settings beyond monotone submodular bandits with car-
dinality constraint. As we will see, instead of these assumptions, we only need a setting where we have an
algorithm for the offline problem satisfying a specific notion of robustness.
As before, let Ωbe the set of base arms and let Sbe a subset of 2Ω. LetFbe a class of functions from
S→ [0,1]where we know that f∈F. We useS∗to denote the optimal value of f.
Definition 2 ((Nie et al., 2023)) .LetAbe an algorithm for the combinatorial optimization problem of
maximizing a function f:S →Rover a finite domain S ⊆ 2Ωwith the knowledge that fbelongs to a
known class of functions F. for any function ˆf:S→R, letSA,ˆfdenote the output of Awhen it is run with
ˆfas its value oracle. The algorithm Acalled (α,δ)-robust if for any ϵ >0and any function ˆfsuch that
|f(S)−ˆf(S)|<ϵfor allS∈S, we have
f(SA,ˆf)≥αf(S∗)−δϵ.
In this setting, Nis an upper-bound for the number of A’s queries to the value oracle.
In the previous sections, the set Swas the set of all subsets of Ωwith size at most kandFwas the set of
monotone submodular functions on S. Corollary 1 simply states that the greedy algorithm is (1−1/e,2k)-
robust. If we choose Ato be the offline greedy algorithm, α= 1−1/e,δ= 2kandN=nk, then Algorithm 2
will reduce to Algorithm 1.
Algorithm 2 C-ETC algorithm ((Nie et al., 2023)
Input:Set of base arms Ω, horizonT, an offline (α,δ)-robust algorithm A, and an upper-bound Non the
number ofA’s queries to the value oracle
Assumption: N≤T
1:m←⌈(δT/N )2/3⌉
2:whileAqueries the value of some action Sdo
3:PlaySarmmtimes
4:Calculate the empirical mean ¯x
5:Return ¯xtoA
6:end while
7:forremaining time do
8:Play action SAoutput by algorithm A
9:end for
The proof only needs minor changes to adapt for Algorithm 2. Lemma 2 immediately generalizes to
P(E)≥1−2Nexp(−2mrad2),
wherenkis replaced by N. Instead of Corollary 1, we need the following statement.
25Under review as submission to TMLR
Corollary 2. Under the eventE∩E′
d, for alld>0, we have
f(SA)≥αf(S∗)−δ/parenleftbigg
rad +2d
m/parenrightbigg
.
Proof.Consider a time interval of length m, namelyt,t+ 1,···,t+m−1, where an action Sis repeated
and the empirical mean ¯xis observed. We have
m|¯x−f(S)|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglet+m−1/summationdisplay
i=t(xt−f(S))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglet+m−1/summationdisplay
i=t(xt−ft)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglet+m−1/summationdisplay
i=t(ft−f(S))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
Under the eventE, we have/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglet+m−1/summationdisplay
i=t(ft−f(S))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤mrad,
and under the event Ed, we have/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglet+m−1/summationdisplay
i=t(xt−ft)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤2d.
Therefore, we have
|¯x−f(S)|≤rad +2d
m.
Now the claim follows from the definition of (α,δ)-robustness ofA.
The proofs of Lemma 4 and Theorem 1 could be applied almost verbatim to give us
E(Rα)≤mN+δTrad +2δTd
m+ 2NTexp(−2mrad2) +T(1−P(E′
d)),
for alld>0. The results below follow.
Theorem 12. If the delay is uniformly bounded by d, then we have
E(Rα) =O(N1/3δ2/3T2/3(log(T))1/2) +O(N2/3δ1/3T1/3d).
Theorem 13. If the delay sequence is stochastic, then we have
E(Rα) =O(N1/3δ2/3T2/3log(T)) +O(N2/3δ1/3T1/3E(τ)),
whereτis an upper tail bound for (ET(∆t))∞
t=1.
Theorem 14. If the delay sequence is stochastic and conditionally independent, then we have
E(Rα) =O(N4/3δ2/3T2/3log(T)) +O(N5/3δ1/3T1/3E(τ)),
whereτis an upper tail bound for (ET(∆t))∞
t=1.
H Details on Function, Delay Settings, and Baselines in Evaluations
Submodular functions:
(F1) Linear:
Here we assume that fis a linear function of the individual arms. In particular, for a function
g: Ω→[0,1], we define
f(S) :=1
k/summationdisplay
a∈Sg(a).
More specifically, we let n= 20andk= 4and choose g(a)uniformly from [0.1,0.9], for alla∈Ω
and defineF(S) :=1
k/summationtext
a∈Sg(a) +Nc(0,0.1)whereNc(0,0.1)is the truncated normal distribution
with mean 0and standard deviation 0.1, truncated to the interval [−0.1,1.0].
26Under review as submission to TMLR
(F2) Weight Cover:
Here we assume that (Cj)j∈Jis a partition of Ωand there is a weight function wt:J→[0,1]. Then
ft(S)is the sum of the weights of the the indexes jwhereCj∩S̸=∅, divided by k. In other words,
if1is the indicator function, then
ft(S) :=1
k/summationdisplay
j∈Jwt(j) 1S∩Cj̸=∅.
More specifically, we let n= 20andk= 4. We divide Ωinto 4 categories of sizes 6,6,6,2and let
wt(j) =U([0,j/5])be samples uniformly from [0,j/5]forj∈1,2,3,4.
Stochastic set cover may be viewed as a simple model for product recommendation. Assume nis the
number of the products and each product belongs to exactly one of ccategories. Then the reward
will be equal to the sum of the weights of the categories that have been covered by the user divided
byk.
Delay settings:
(D1) No Delay
(D2) (Stochastic Independent Delay) For all t≥1andi≥0,∆t(i) = (1−Yt)Yi
t, where (Yt)∞
t=1is an i.i.d
sequence of random variables with the uniform distribution U([0.5,0.9]). The reward for time-step
twill be distributed over [t,∞)according to ∆t. In other words, at each time-step t, the agent
plays the action St, then the environment samples ft(St)according to the distribution of Ft(St)and
samplesytaccording to the distribution U([0.5,0.9]). Then we have δt(i) = (1−yt)yi
tfor alli≥0,
which is used in Equation 1 to determine the observation. In this example, the distribution of ∆t
does not depend on the action chosen by the agent and (∆t)∞
t=1is i.i.d.
(D3) (Stochastic Independent Delay) For all t≥0,∆tis a distribution over [10,30]is sampled uniformly
from the probability simplex using the flat Dirichlet distribution. The reward for time-step twill be
distributedover [t+10,t+30]accordingto ∆t. Inotherwords, ateachtime-step t, theagentplaysthe
actionSt, then the environment samples ft(St)according to the distribution of Ft(St)and samples
(β0,β2,···,β20)from the 20-dimensional probability simplex {(z0,···,z20)|zi≥0,/summationtextzi= 1},
according to the flat Dirichlet distribution. Then we have δt(i) =βi−10for all 10≤i≤30and
δt(i) = 0otherwise, which is used in Equation 1 to determine the observation. In this example, the
distribution of ∆tdoes not depend on the action chosen by the agent and (∆t)∞
t=1is i.i.d.
(D4) (Stochastic Conditionally Independent Delay) For all t≥1andi≥0, we have ∆t(i) = (1−Yt)Yi
t,
whereYt= 0.5 +ft∗0.4∈[0.5,0.9]. The reward for time-step twill be distributed over [t,∞)
according to ∆t. In other words, at each time-step t, the agent plays the action St, then the
environment samples ft(St)according to the distribution of Ft(St)and picksyt= 0.5 +ft(St)∗0.4.
Thenwehave δt(i) = (1−yt)yi
tforalli≥0, whichisusedinEquation1todeterminetheobservation.
Note that there is no more randomness in delay after the value of ft(St)is samples from Ft(St). Also
note that the value of ytdepends on the action of the agent. In this example (∆t,S)t≥1is pair-wise
independent for any S∈S.
(D5) (Stochastic Conditionally Independent Delay) At each time-step t, a numberltis chosen from [10,30]
according to the following formula.
lt=⌊20ft⌋+ 10.
The reward for time-step twill be observed at t+lt. More specifically, at each time-step t, the agent
plays the action St, then the environment samples ft(St)according to the distribution of Ft(St)and
pickslt=⌊20ft(St)⌋+ 10. Finally, we have δt(i) =1i=lt. In other words, the higher the reward,
the more it will be delayed. In this example, delay depends on the action chosen by the agent and
(∆t,S)t≥1is pair-wise independent for any S∈S.
27Under review as submission to TMLR
(D6) (Adversarial Delay) Let l1= 15and for allt>1, defineltaccording to the following formula.
lt=⌊20xt−1⌋+ 10.
As in the delay (D5), the value of ltdetermines the amount of delay, i.e. δt(i) =1i=lt. Note that
xt−1is the value of the previous observation as described in Equation 1. In other words, the higher
the previous observation, the more the current reward will be delayed.
Baselines:
We use three algorithms designed for CMAB with full-bandit feedback without delay and and algorithm
designed for MAB with composite anonymous feedback as the baseline.
•CMAB-SM (Agarwal et al., 2022) This algorithm assumes the expected reward functions are
Lipschitz continuous of individual base arm rewards. CMAB-SM has a theoretical 1-regret guarantee
of˜O(T2/3)with the assumption that if arm ais better than arm b, then replacing bbyain any set
not including awill give better reward function.
•DART(Agarwal et al., 2021) This algorithm assumes the expected reward functions are Lipschitz
continuous of individual base arm rewards and the reward functions have an additional property
relatedtothemarginalgainsofthebasearms. DARThasatheoretical 1-regretguaranteeof ˜O(T1/2)
with the assumption that if arm ais better than arm b, then replacing bbyain any set not including
awill give better reward function.
•OGo(Streeter & Golovin, 2008) This algorithm is designed for oblivious adversarial setting with
submodular rewards. Therefore the sequence of monotone and submodular functions is fixed in
advance. It has an (1−1/e)-regret guarantee of ˜O(T2/3).
•ARS-UCB (Wang et al., 2021) This algorithm is designed for MAB with composite anonymous
delayedfeedback. Thedelaymodelisaspecialcaseof unbounded stochastic conditionally independent
composite anonymous feedback delay that we described. However, they assume that ∆t,Sdoes not
depend on time. For our experiments, we use all subsets of Ωof sizekas the set of arms. ARS-
UCB has a 1-regret guarantee of ˜O(/parenleftbign
k/parenrightbig1/2T1/2)plus a constant term that depends on delay and the
number of its arms.
I Experiments with added regret
In Figure 2, we have considered the same functions and delay types as before. After fixing the function and
a delay type, we ran each experiment with and without delay 10 times and plotted the added regret when
delay is present. In these experiments, we see that the added regret for ETCG is consistently relatively
low with low variance. We note that one should be careful when interpreting these plots, since the regret
bounds are simply upper bounds and therefore the values shown here are the difference of two values that
are bounded from above. Specifically, having upper bounds Rdelay≤aT2/3+νT1/3andRno-delay≤aT2/3
do not implyRdelay−Rno-delay≤νT1/3.
28Under review as submission to TMLR
102103104105106
T0102103104105Cumulative Added Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB
(a) (F1)-(D2)
102103104105106
T0102103104105Cumulative Added Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB (b) (F1)-(D3)
102103104105106
T0102103104105Cumulative Added Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB
(c) (F1)-(D4)
102103104105106
T0102103104105106107Cumulative Added Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB (d) (F1)-(D5)
102103104105106
T0102103104105Cumulative Added Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB (e) (F1)-(D6)
102103104105106
T102
0102103104Cumulative Added Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB
(f) (F2)-(D2)
102103104105106
T104
103
102
0102103104105Cumulative Added Regret
 ETCG
CMAB-SM
DART
OGo
ARS-UCB (g) (F2)-(D3)
102103104105106
T105
104
103
102
0102103104105106Cumulative Added Regret
 ETCG
CMAB-SM
DART
OGo
ARS-UCB
(h) (F2)-(D4)
102103104105106
T104
103
102
0102103104105106107Cumulative Added Regret
 ETCG
CMAB-SM
DART
OGo
ARS-UCB (i) (F2)-(D5)
102103104105106
T010000200003000040000Cumulative Added Regret
ETCG
CMAB-SM
DART
OGo
ARS-UCB (j) (F2)-(D6)
Figure 2: This plot shows the average added cumulative regret over horizon for each setting in the symlog-log
scale over 10 runs. The scale of the y-axis is linear for |y|≤100and logarithmic for |y|>100. The gray
dashed lines are y=aT1/3fora∈{10,100,300}. The cyan dashed lines are y=νT1/3whereνis the
corresponding delay coefficient appearing in the regret bounds in Theorems 2, 3 and 4.
29