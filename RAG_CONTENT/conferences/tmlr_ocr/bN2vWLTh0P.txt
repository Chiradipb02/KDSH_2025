Published in Transactions on Machine Learning Research (11/2022)
Reinventing Policy Iteration under Time Inconsistency
Nixie S. Lesmana nixiesap001@e.ntu.edu.sg
School of Physical and Mathematical Sciences
Nanyang Technological University
Huangyuan Su huangyus@andrew.cmu.edu
School of Computer Science
Carnegie Mellon University
Chi Seng Pun cspun@ntu.edu.sg
School of Physical and Mathematical Sciences
Nanyang Technological University
Reviewed on OpenReview: https: // openreview. net/ forum? id= bN2vWLTh0P
Abstract
Policy iteration (PI) is a fundamental policy search algorithm in standard reinforcement
learning (RL) setting, which can be shown to converge to an optimal policy by policy
improvement theorems. However, the standard PI relies on Bellman’s Principle of Optimality,
which might be violated by some specifications of objectives (also known as time-inconsistent
(TIC) objectives), suchas non-exponentiallydiscountedreward functions. Theuse of standard
PI under TIC objectives has thus been marked with questions regarding the convergence of its
policy improvement scheme and the optimality of its termination policy, often leading to its
avoidance. In this paper, we consider an infinite-horizon TIC RL setting and formally present
an alternative type of optimality drawn from game theory, i.e., subgame perfect equilibrium
(SPE), that attempts to resolve the aforementioned questions. We first analyze standard PI
under the SPE type of optimality, revealing its merits and insufficiencies. Drawing on these
observations, we propose backward Q-learning (bwdQ), a new algorithm in the approximate
PI family that targets SPE policy under non-exponentially discounted reward functions.
Finally, with two TIC gridworld environments, we demonstrate the implications of our
theoretical findings on the behavior of bwdQ and other approximate PI variants.
1 Introduction
Policy iteration (PI) has enjoyed a long history of success in standard reinforcement learning (RL), which can
be attributed to standard PI that combines a dynamic programming (DP)-based policy evaluation1and a
greedy policy improvement; see Bellman (1957); Howard (1960). Standard PI has been the basis of many
classical RL algorithms, such as value iteration and the popular Q-learning (Watkins & Dayan (1992)), and it
still inspires the design of modern RL algorithms. Despite its prominence in standard RL setting, standard
PI has been deemed incompatible for time-inconsistent (TIC) objectives due to non-monotonicity and the
impliedviolation of Bellman’s principle of optimality (BPO) .
Time inconsistency (also abbreviated as TIC) is prevalent in dynamic choice problems and captures well
human’s tendency to deviate from their current plan at a future time; such deviation arises as a plan of
future course of actions that is optimalfor a human agent today, maynotbeoptimalfor the same agent
in thefuture. In the context of RL, TIC often arises as an effort to more closely model human preferences,
resulting in TIC objectives upon which an RL agent is built on, and has been investigated through several
major channels such as hyperbolic discounting and risk-sensitive RL.
1For a formal introduction, readers may refer to Eq (8).
1Published in Transactions on Machine Learning Research (11/2022)
The idea of questioning the validity of standard PI under TIC was pioneered in risk-sensitive RL by Sobel
(1982). In this seminal work, a counterexample to the monotonicity2property (also referred to as consistent
choice, temporal persistence, or stationarity across the literature) was posted and attention was raised in how
this property is commonly exploited to prove the convergence of standard policy improvement scheme to an
optimal policy. Two puzzles are then left for answers:
How optimal is the termination policy (the policy obtained at the end of an algorithmic
search) of standard PI?
and
Is it possible to guarantee the update monotonicity (a desirable algorithmic property that will
lead to convergence) of standard PI?
In this paper, we focus on infinite-horizon TIC RL problems and formally present the subgame perfect
equilibrium (SPE) notion of optimality that corresponds to how sophisticated, rational agent acts in the
face of TIC i.e., planning consistently in terms of solving optimizations that take into account the future
deviations. We will then revisit the two questions above to highlight standard PI’s merits and insufficiencies
in achieving the SPE notion of optimality.
The contribution of this paper can be summarized as follows:
•In terms of optimality, we establish that the termination policy of standard PI under TIC achieves
SPE.
•We study the failure of policy improvement theorem (Sutton & Barto (2018)) and highlight some
insufficiencies of standard PI update and the existing analysis tools, in the context of SPE policy
search.
•TIC-adjusted DP formula is established to compute nonexponentially-discounted Q-function, ad-
dressing the insufficiency of standard DP formula.
•Based on the aforementioned analyses, we devise a new PI paradigm for non-exponentially discounted
reward functions: backward Q-learning (bwdQ).
•We design toy Gridworld examples to demonstrate the implications of our findings on the behaviour
of bwdQ and other approximate PI variants under TIC.
•The analyses (in Section 5.1 and 4.3) relevant to the advantage of backward conditioning in bwdQ is of
independent interest: the characterization of its termination policy as SPE and its efficiency-related
desirability as an SPE policy learner extend beyond general-discounting objectives.
Note that some lengthy proofs/justifications of our results are deferred to Appendix.
2 Related Works
Non-monotonicity in risk-sensitive RL and solutions. In risk-related context, several follow-up works
since Sobel (1982) address the non-monotonicity issue following the line of reasoning that the search for a
globally-optimal policy in non-monotonic problems are computationally expensive (as one can only enumerate
over the whole policy space that is almost impossible in practice) and hence, new solutions are desired. For
instance, Mannor & Tsitsiklis (2011) formally compares between several policy classes to reduce the search
problem for globally-optimal policy (to a specific policy class) and proposes several practical approximation
algorithms. One important finding in their work is that randomization can improve control performance;
2A formal definition to monotonicity is provided in Eq (15). Intuitively, if a decision-making problem is monotonic, then
delaying the use of any two decision policies will preserve their ordering (in terms of the policies’ values from any states). When
this property holds, we can typically break our problem into subproblems as in BPO and attain computational efficiency.
2Published in Transactions on Machine Learning Research (11/2022)
this inspires Di Castro et al. (2012); Tamar & Mannor (2013); Prashanth & Ghavamzadeh (2013) to propose
gradient-based algorithms accustomed to mean-variance criteria, which highlighted parameterized stochastic
policy as a manner to deal with non-monotonicity. The latter works are relevant to our case as they also
use TIC adjustment terms to obtain temporal difference (TD)-based policy evaluation (PE) that resembles
the one used in extended DP theory Björk et al. (2014). To distinguish our approach, we note our focus
on using SPE policy itself to deal with non-monotonicity (by modifying our optimality type ) as opposed to
randomization or parameterization.
Non-monotonicity in hyperbolic-discounting RL and solutions. In hyperbolic-discounting context,
non-monotonicity have also appeared, independent of Sobel (1982)’s work; see Kurth-Nelson & Redish (2010)
for instance. In this work, several proposals towards computationally practical models are reviewed, with
varying action selection strategies drawn largely from behavioral or neuroscience point of view. A recent follow-
up work by Fedus et al. (2019) extends their distributed micro-agents model (i.e. µAgents) to handle larger
scale problems, utilizing deep neural network to model the different Q-values from a shared representation.
Though such modifications in action selection may have implicitly addressed the non-monotonicity underlying
PIT failure, to the best of our knowledge, an explicit connection between the two (as in Sobel (1982)) has
never been made.
Time-consistent Planning and Control. The idea of locally optimal, time-consistent planning under
TIC was pioneered by Strotz (1955); Pollak (1968). This type of planning corresponds to a sophisticated,
rational agent’s behavior who, when faced with TIC, compromises with their future selves by taking future
disobedience as a constraint in their decision-making. The solution concept is developed as a game-theoretic
framework that builds on backward inductive SPE search in games, thus coining the term SPE plan or
policy. This then leads to an intra-personal equilibria formalism by Björk & Murgoci (2014) which unifies
several task-specific TIC sources through extended DP theory and has attracted a wide array of literature
in TIC stochastic control. The rise of SPE policy as a major contending solution to the globally optimal
(precommitment) policy can then be attributed to two reasons: (i) as a controller, precommitment policy
may lead to some undesirable outcomes since it may lose its optimality as time evolves (for instance, due to
an unpredictable change in environment dynamics), (ii) computationally, there is lack of a pivotal tool to
identify a globally-optimal policy that generalizes naturally to different TIC tasks (for instance, due to its
disconnection to standard DP that requires BPO).
SPE Policy in TIC-RL. Some works in the general-discounting space have investigated TIC-RL from a
purelybehaviorallens, focusingparticularlyonthepropertyoftargetpolicyratherthanacomputationalaspect.
For instance, Lattimore & Hutter (2014) proposes rational agents that act according to history-dependent
SPE policies. In this work, the authors cover some theoretical aspects of policies such as characterization of
different policy types, existence results connecting discounting and policy types, and comparative study in
some example scenarios. In another work, Evans et al. (2016) proposes sophisticated agents that act according
to Markovian SPE policies and are modelled with delay-augmented Q-learning algorithms. Though relevant,
these algorithms are proposed in the context of generative models that aids human-like preference inferences;
thus, algorithmic properties are not covered. A recent work by Lesmana & Pun (2021) considers the search
of Markovian SPE policy under finite-horizon task-invariant TIC objectives. Drawing inspiration from the
extended DP theory, the authors propose Backward Policy Iteration (BPI), which has lex-monotonicity
guarantee in place of the standard policy improvement theorem. This work is the closest to ours where our
backward conditioning can be viewed as an infinite-horizon extension to BPI. We distinguish our contribution
by noting our main focus on analyzing standard PI , that motivates our infinite-horizon, Markovian SPE
policy formalism and the corresponding drop of time-dependency, shifting definition of players from times
to states. Relative to finite-horizon case, such formalism introduces technical challenges in both aspects of
policy evaluation and improvement, which we will remark on the respective sections of this paper.
3Published in Transactions on Machine Learning Research (11/2022)
3 Problem Formulation and the SPE Concept
In this section, we introduce the class of TIC RL problems of our interest and formally present the solution
concept of SPE policy. We then cast the general-discounting objective as a TIC RL problem and construct a
few examples in this context that we will quote frequently throughout the paper.
3.1 TIC RL Problem Formulation
We consider the policy search in an infinite-horizon TIC-MDP, which consists of the standard MDP tuple
(S,A,P,R)and a specific TIC source. The state space Sand action spaces As⊆A,∀s∈S,are assumed to
be discrete and finite with stationary probabilities pa
s(·):=P[Rt+1=·,St+1=·|St=s,At=a]governing the
transitions from a current state St=sto the next state St+1and reward Rt+1fors∈S, given a particular
actionAt=a. To define a stopping criterion, it is convenient to augment a so-called absorbing state, denoted
by¯svoid, which incurs no reward. Then, we define a stopping action ¯aas an action that drives a transition to
¯svoidfrom any states s∈Sandboundary states ¯s∈¯Sas rewarding states with specific action space A¯sor
¯A:={¯a}, i.e. once the boundary state is reached, we conclude with reward as there is only action ¯athat will
transit us from ¯sto¯svoidand then make us stay at ¯svoidforever. This setup is to complete the mathematical
framework of the environment for analyses, where the problem of interest has certain stopping criteria, e.g.,
after receiving a target reward.
Let us next denote by ΠMDthe set of all Markovian, deterministic policies π:={π(s) :s∈S}with
π:S→As. To aid presentation in subsequent sections, we define a·πas a policy that prescribes the use of
actiona∈Afor a current one-step decision and policy πfor all remaining decisions. Similarly, we denote
byδτ·πa policy that fixes the first τ >0steps decisions to δτ:={δ(Sw) :t≤w < t +τ}, with a map
δ:S→Asand a current time t, and follows πafterwards.
TIC reward structures and criterion We first note that by our assumption on stationary transitions,
we are limiting our TIC scope to those that arise from reward structures and criterion, described as follows.
Let us consider a general criterion Vπ(s)(with form not restricted at this point) for any π∈ΠMDand
s∈S. Given an initial state s0∈S, a standard notion of optimality aims to solve the globalproblem
P0,s0.= max πVπ(s0)and obtain the corresponding globally-optimal (precommitment) policy denoted by
π∗0. Next, let us define for each delay τ >0, thelocalproblemPτ,sτ.= max πVπ(sτ), wheresτrepresents
any realization of Sτfollowing the sequence of policies/braceleftbig
π∗0(St) : 0≤t<τ|S0=s0/bracerightbig
, and denote by π∗τ(sτ)
its solution. Bellman’s Principle of Optimality (BPO) then states that
∀τ,sτ, π∗τ(sτ) =π∗0(sτ) (1)
By the BPO definition above, the standard RL criteria belong to the time-consistent (TC) class that does not
violate (1). While in this paper, we consider criteria Vπ(s)that violate (1); these include general-discounting,
risk-related, and more (cf. Björk & Murgoci (2014)). Given any criteria Vπ(s), one can verify whether it
belongs to the TIC class through a counterexample (which will be illustrated with Example 3.7 below).
It is noteworthy that our way in defining TIC criterion is unlike most MDPs that specify the criterion Vπ(s)
up to the expectation of cumulative rewards. We aim to maintain generalities up to the formalism of SPE
optimality (in Section 3.2) that is valid for different forms of TIC reward structures and criterion. For
instance, while general-discounting objectives still admit an expectation form, risk-sensitive objectives involve
non-linearity in expectations. That said, to fully define a TIC-MDP, we need the exact specifications of
reward structures Rand the corresponding TIC sources. We will further discuss on this topic in Section 3.3
with general-discounting specifications.
3.2 SPE Notion of Optimality
In the previous subsection, we have established our focus on the objectives that violate BPO in (1). Once
BPO is violated, π∗0̸=π∗τfor someτ,sτand we will have a collection of competing optimization problems
{Pτ,sτ:∀τ∈[0,∞),sτ∈ S}to solve. We then have two options. Firstly, we can focus on only a
single agent corresponding to (0,s0), denoted by Agent- s0, and solve for a globally optimal or so-called
4Published in Transactions on Machine Learning Research (11/2022)
precommitment policy. However, as we have noted in Section 2, this is expensive to obtain in general and
more importantly, it has been known to suffer from the two puzzles in Section 1. Secondly, we can consider
{Pτ,sτ:∀τ∈[0,∞),sτ∈S}as a multi-agent problem, where each Agent- sτis associated with the problem
Pτ,sτ, and solve for the Nash equilibrium of this (sub-)game, i.e. SPE.
In this subsection, we will formalize such SPE notion of optimality in an infinite-horizon setting. Let us
consider any TIC criterion Vπ(s)and define the corresponding action-value or Q-function Qπ(s,a):=Va·π(s).
Our aim is then to find an (stationary) SPE policy ˆπ, defined as follows.
Definition 3.1 (SPE Policy) .A policy ˆπ∈ΠMDis an SPE policy if it satisfies
Qˆπ(s,ˆπ(s))≥Qˆπ(s,a),∀a∈As,∀s∈S (2)
In other words, our game consists of Players, indexed by the statess∈S, and we search for an SPE, where
Playerstakes into account the strategies of other Players s′∈S\{s}in its decision making as swill be
transited to an s′. In particular, Definition 3.1 implies that any state s′does not have incentive to deviate
from its strategy ˆπ(s′)when everyone else plays ˆπ; note that this is how SPE takes into account future
deviations, as we mentioned in Section 1.
Remark 3.2.It is important to note that in standard RL, the policy that realizes Definition 3.1 (or so-called
non-positive advantage) is the optimal policy. However, such equivalence draws on BPO, which does not apply
in TIC RL. Once BPO is violated, the optimal policy (one that solves P0,s0:=max πVπ(s0), that we refer to
as globally-optimal/precommitment policy π∗0) is no longer equivalent to the one that realizes non-positive
advantage (one that solves the game induced by competing local optimizations Pτ,sτ:=max πVπ(sτ), that
we refer to as SPE policy ˆπ). As we will see later in Section 3.3, an SPE policy (cf. Figure 1(c)) can be
neither equal or equivalent in value to the globally-optimal/precommitment policy (cf. Figure 1(b)).
Throughout this paper, we will adopt several technical assumptions to address the technical challenges of
suchinfinite-horizon SPE, particularly those that arise from the derivations of TIC-adjusted DP in Section
4.2 andbackward conditioning update in Section 5.1.
Assumption 3.3. ∀s∈S,∀ϵ>0,there exists a truncation step ¯T <∞s.t.∀π∈ΠMD,¨π:S→ ¯A,
/vextendsingle/vextendsingle/vextendsingleVπ(s)−Vπ¯T·¨π(s)/vextendsingle/vextendsingle/vextendsingle≤ϵ. (3)
Denote by ¯Ts,ϵthesmallest such ¯T.
Assumption 3.4. ∃s0∈Ss.t.∃ˆTs0<∞
∀s∈S\{s0},∃π∈ΠMD,ˆTs0/summationdisplay
t=0P[Sπ
t=s|S0=s0]>0.
Intuitively, Assumption 3.3 asserts that starting from any sand following any policy π, any rewards generated
after ¯Tsteps are negligible as the policy ¨πincurs 0rewards3. Then, Assumption 3.4 ensures the existence
of at least one state s0from which all other states s∈S\{s0}can be reached in finite time, with positive
probability. Combining these two, we fix s0and set ¯Tϵ:=max{ˆTs0+ 1,¯Ts0,ϵ}to obtain our last assumption.
Assumption 3.5. Let us defineSs0,ΠMD
0:={s0}and∀t∈(0,∞),
Ss0,ΠMD
t :={s∈S:∃τ∈[0,t),sτ∈Ss0,ΠMD
τs.t.∃π∈ΠMD,P[Sπ
t−τ=s|S0=sτ]>0} (4)
Then,∀ϵ>0,∀t∈[0,¯Tϵ],∀st∈Ss0,ΠMD
t,and∀π∈ΠMDwithP[Sπ
t=st|s0]>0,∃κ(ϵ) =κ(ϵ,t,st,π;s0)>0
s.t./vextendsingle/vextendsingle/vextendsingleVπ(st)−Vπ¯Tϵ−t·¨π(st)/vextendsingle/vextendsingle/vextendsingle≤ϵ
κ(ϵ)andlimϵ↓0ϵ
κ(ϵ)= 0.
3We note one important implication of Assumption 3.3: bounded value functions, which proof can be found in Appendix A.2.
5Published in Transactions on Machine Learning Research (11/2022)
Intuitively, Assumption 3.5 ensures the negligibility of rewards under πfrom any intermediate states
st∈Ss0,ΠMD
twhen there is a path connecting it to s0under the same policy π. In Appendix A.1, we show
that these assumptions are reasonable in practice through a set of sufficiency conditions (in general-discounting
context, in terms of restrictions on MDP and discounting function).
3.3 General-discounting Criterion
As a major concern of this paper, we consider the following infinite-horizon criterion
Vπ(sτ).=E/bracketleftigg∞/summationdisplay
t=τφ(t−τ)R(Sπ
t,π(Sπ
t))/vextendsingle/vextendsingle/vextendsingleSτ=sτ/bracketrightigg
(5)
defined for any τ≥0, with a general discounting function φ:N→(0,1]. The intermediate (possibly random)
reward function R:S×A→ R+follows the standard MDP formulation, with emphasis on its boundedness
and non-negativity. We further note our use of notation Sπ
tfor the (random) state visited at time ton a
trajectory generated by following policy πand initialized at Sτ=sτ.
Next, we define action-value or Q-function that relates to the value function in (5).
Definition 3.6 (Q-function) .For each state-action pair (s,a)∈S×A and a fixed policy π∈ΠMD, we
defineQ-function as
Qπ(s,a).=E/bracketleftigg∞/summationdisplay
t=0φ(t)R(Sa·π
t,π(Sa·π
t))/vextendsingle/vextendsingle/vextendsingleS0=s/bracketrightigg
(6)
Note that to emphasize on the stationarity of our problem, we have avoided any explicit appearance of τ
in the Definition 3.6 above by performing a simple change of variable drawn from the Markov property
P[Sπ
t=st|Sτ=sτ] =P[Sπ
t−τ=st|S0=sτ]. Later in Section 4.2, τwill be re-introduced as our RL agent’s
parameter that keeps track of nonstationarity changes.
We may now revisit the TIC concept by witnessing how criterion (5) violates BPO through a Gridworld
counterexample.
Example 3.7 (BPO Violation) .Consider a Gridworld environment as described in Figure 1(a) and a
hyperbolically-discounted criterion, i.e., setting φ(t−τ) = 1/(1 +k(t−τ))in (5). Given s0= 21, we can
compute (by trajectory enumeration) the globally-optimal, precommitment policy π∗0as in Figure 1(b). After
applying delay τ= 3and following the delaying policy δτ=π∗0, we reachsτ= 9, at which the locally-optimal
policy suggests π∗τ.={π∗τ(9)}={←}and accrues rewards Vπ∗τ(sτ) = 10/(1 + 1)>19/(1 + 3) =Vπ∗0(sτ);
see Figure 1(d). This violates BPO at τ= 3andsτ= 9.
Here onwards, we will use the general-discounting TIC value function and Q-function as defined in (5)-(6) for
any appearance of Vπ(s)andQπ(s,a), unless specified otherwise. We then denote by Vπ
TC(s)andQπ
TC(s,a)
the exponential-discounting (i.e., φ(t) =γt) time-consistent (TC) value function and Q-function to exemplify
any standard RL formulations in the subsequent sections.
4 An Analysis of Standard PI
In this section, we analyze standard PI under the SPE optimality type, revealing its merits and insufficiencies.
4.1 SPE Optimality of Termination Policy
We first present the standard PI update,
π′(s)←arg max
a∈AsQπ(s,a),∀s∈S (7)
6Published in Transactions on Machine Learning Research (11/2022)
(a)
 (b)
(c)
 (d)
Figure 1: (a) Deterministic, Hyperbolic ( k= 1) Gridworld.Scomprises 2 absorbing states ¯S={¯2,¯8}
emitting rewards R(¯2) = 19,R(¯8) = 10. Each action in A={↑,→,↓,←}drives transition through
deterministic P; transitions to WALL or outside the grid will spawn the agent back to its original location.
(b) Globally-optimal, precommitment policy π∗0and its corresponding path, accruing accumulated
rewardsVπ∗0(s0) =19
1+6. This path exhibits TIC at τ= 3andsτ= 9as shown in Example 3.7. (c) SPE
policy ˆπand its corresponding path, accruing rewards Vˆπ(s0) =19
1+8<Vπ∗0(s0). One could refer back to
Definition 3.1 and verify that no states sτon this path have the incentive to deviate from its current policy
ˆπ(sτ).(d) Delusional policy δτ·π∗τand its corresponding path, with τ,δτ,andπ∗τspecified in Example
3.7, accruing rewards of Vδτ·π∗τ(s0) =10
1+4<Vˆπ(s0). The term ’delusional’ is used to reflect how state 21
presumes 9will go up, unaware of the TIC issue.
where π′,πrepresent new(atcurrentiteration) and oldpolicies (at previous iteration) in any two consecutive
iterations. Next, we will show the merit of standard PI in Proposition 4.1: its termination policy achieves
SPE optimality.
Proposition 4.1. Ifπ′=πand update follows the rule in (7), then π,π′are SPE policy.
Proof.By (7) and π′=π, we obtain that∀a∈As,∀s∈S,
Qπ(s,π′(s))≥Qπ(s,a)⇒Qπ(s,π(s))≥Qπ(s,a).
Thus, by Definition 3.1, π,π′are SPE policy.
4.2 Policy Evaluation
The update rule (7) requires the computation of the true TIC Q-function Qπ(s,a), which is not straightforward.
In standard RL setting, there is a DP formula to efficiently compute TC Q-function,
Qπ
TC(st,at) =ER,S′∼patst[R(st,at) +γVπ
TC(S′)], (8)
whereVπ
TC(s)(iteratively) solves (8) after substituting π(st)intoat. Under general discounting as in (5), (8)
no longer holds. In this subsection, we present a recursive formula satisfied by our TIC Q-function (see (13)
below) by leveraging the extended DP theory (Björk et al. (2014)).
7Published in Transactions on Machine Learning Research (11/2022)
TIC-adjusted DP Noting that in Section 3.2 we have assumed access to a fixed ¯Tϵ<∞, we introduce a
reward adjustment (or r-function) that our agent will use it to track the nonstationary changes (due to TIC)
in Q-function.
Definition 4.2 (r-function) .For eachτ∈{0,... , ¯Tϵ},m∈{τ,... , ¯Tϵ},s∈S,a∈A, and a fixed policy
π∈ΠMD, we define r-function as
rπ,τ,m(s,a).=E[φ(m−τ)R(Sa·π
m,π(Sa·π
m))|Sτ=s] (9)
whereτandmare fixed parameters.
Next, we will use the adjustment function above to obtain a formula that recursively computes our Q-function.
Theorem 4.3. For any fixed π∈ΠMD,τ∈{0,... , ¯Tϵ},m∈{τ,... , ¯Tϵ},s∈S,a∈As,r-function satisfies
form=τ,
rπ,τ,τ(s,a) =ER∼pas[φ(0)R(s,a)] (10)
and form≥τ+ 1,
rπ,τ,m(s,a) =ES′∼pas/bracketleftbiggφ(m−τ)
φ(m−(τ+ 1))rπ,τ+1,m(S′,π(S′))/bracketrightbigg
. (11)
Moreover, for any fixed π∈ΠMD,t∈{0,... , ¯Tϵ},st∈S,at∈Ast, under some technical conditions (see
Assumption B.1), Q-function satisfies for st∈¯Sandat∈¯A,
Qπ(st,at) =ER∼patst[φ(0)R(st,at)] (12)
and forst∈S\ ¯Sandat∈Ast,
Qπ(st,at)≈ER∼patst[φ(0)R(st,at)] +ES′∼patst[Qπ(S′,π(S′))]−∆rπ
t, (13)
where ∆rπ
t.=/summationtext¯Tϵ
m=t+1/parenleftig
ES′∼patst/bracketleftbig
rπ,t+1,m(S′,π(S′))/bracketrightbig
−rπ,t,m(st,at)/parenrightig
.
Remark 4.4.Both the proof of Theorem 4.3 and the technical conditions for it are provided in Appendix B.
We note that Theorem 4.3 is an analog to Proposition 11 in Lesmana & Pun (2021) and our main technical
novelty lies in the approximation (’ ≈’) part of (13); here, a≈bdenotes the existence of κ(ϵ)s.t.|a−b|≤ϵ
κ(ϵ)
andlimϵ↓0ϵ
κ(ϵ)↓0. Intuitively, this result ensures that our approximation error can be made arbitrarily small
by choosing a sufficiently large ¯Tϵ.
Remark4.5.Theorem 4.3 has used the specific properties of general-discounting TIC source. For other types
of TIC sources, recursive formulas need to be re-derived. In risk-sensitive case, for instance, readers may refer
to Tamar & Mannor (2013); Sobel (1982).
Standard TD-based approximation algorithms such as Q-learning Watkins & Dayan (1992) are drawn from
the standard formula (8) and thus, are insufficient for general-discounting factor. Theorem 4.3 provides
a formula that addresses insufficiency of standard formula (8), which we will later use to reinvent a new
approximate PI algorithm for general-discounting objectives.
4.3 Policy Improvement (Update Monotonicity)
In this subsection, we will highlight some insufficiencies of the standard PI’s update and analysis tools in the
face of TIC by revisiting the unprovable policy improvement theorem. To start off, we present the following
proof (cf. Sutton & Barto (2018)) of the theorem: ∀s∈S,
Vπ
TC(s)≤Qπ
TC(s,π′(s)) =E/bracketleftig
Rπ′
t+1+γVπ
TC(Sπ′
t+1)|St=s/bracketrightig
≤E/bracketleftig
Rπ′
t+1+γQπ
TC(Sπ′
t+1,π′(Sπ′
t+1))|St=s/bracketrightig
=···≤···
≤Vπ′
TC(s). (14)
8Published in Transactions on Machine Learning Research (11/2022)
Note that in each alternating step of ’ =’ and ’≤’, two operations are performed: (i) a recursive expansion of
TC Q-function, and (ii) substituting the monotonicity relation:
∀s∈S, Vπ′·π
TC(s)≥Vπ·π
TC(s)⇒∀s∈S, Vδτ·π′·π
TC (s)≥Vδτ·π·π
TC (s) (15)
for all delays τ≥1andδτ={π′,π′,...}. Let us pay attention to the monotonicity relation, particularly
about how (15) fails under a TIC criterion. To this end, we recall Example 3.7 and focus on the states along
the precommitment path in Figure 1(b). We can then counter (15) as follows:
Setπ=π∗0,τ= 3,δτ=π∗0,π′(9) =←; then,19
1+3=Vπ·π(9)≤Vπ′·π(9) =10
1+1holds. However,
19
1 + 6=Vδ3·π·π(21) = Eδ3[Vπ·π(9)]>Eδ3[Vπ′·π(9)] =Vδ3·π′·π(21) =10
1 + 4(16)
showing that at s= 21, the monotonicity relation (15) does not hold.
We make two observations here: (i) defining improvement as in policy improvement theorem (i.e.,
∀s∈S,Vπ′(s)≥Vπ(s)) might be too strong as this definition targets optimal policies notSPE policies, (ii)
the counterexample (16) suggests the existence of priority ordering4overS(i.e. 9holdspriorityover 21)
such that unordered (i.e. ∀s∈S) update as in (7) may not suffice. To further probe on these issues, we will
consider the following example.
Example 4.6 (Inefficiency of Standard PI) .Let us refer back our Hyperbolic Gridworld in Figure 1(a). We
will keep our deterministic transition and reward functions, but restrict our state-space to:
˜S={1,2,3,5,7,8,9,11,13,15,17,19,21,22,23}
and action-spaces to:
˜A21={↑,→},˜A9={↑,←},
A3={←},
As={→},∀s∈{1,22},
As={↑},∀s∈{17,13,5,7,11,15,19,23},
As={¯a},∀s∈{2,8}.
Lettings0= 21,ϵ= 0,˜Π ={π∈ΠMD|π:˜S→ ˜As}, we have a priority-ordering on˜S,˜S0:¯T=˜Ss0,˜Π
0:¯T0:
˜S0={21},˜S1={17,22},˜S2={13,23},˜S3={9,19},˜S4={5,8,15},
˜S5={1,11},˜S6={2,7},˜S7={3},˜S8={2}. (17)
To apply standard PI for SPE policy search from s0= 21, we can choose an initial policy π(0)as illustrated
in Figure 2(a). Following the standard PI’s rule (7), policies at all states are updated conditional to the
policy at previous iteration. Let us now focus on the two important states s= 9,21, in which decisions need
to be made (i.e., |˜As|>1). First, note that after updated conditional to the previous policyπ(0)(9) =↑, we
obtainπ(1)(21) =↑that incurs a higher reward; see Figure 2(b). Following the same rule, we also obtain
π(1)(9) =←. When combined, the currentiteration ends up in π(1)that corresponds to a delusional path;
see Figure 2(c). We contend that such iterative update is inefficient as we would have found the desired SPE
path if state 21has anticipated π(1)(9) =←when making its update. We will see how we can achieve this in
Section 5.1, by leveraging a known priority-ordering as in (17).
5 Backward Q-learning Algorithm
Drawing on the analyses and observations in Section 4, we propose a new algorithm in the approximate PI
family that targets SPE policy under a general-discounting criterion.
4By either the sophisticated agent’s strategy Strotz (1955) or its SPE formalism Björk et al. (2014), higher priority here
corresponds to a later order of visitation in a trajectory.
9Published in Transactions on Machine Learning Research (11/2022)
(a)
 (b)
 (c)
Figure 2: 2-Layered Correction with Standard PI. (a) Policies at initialization. (b) Policies after state
21updates. (c) Policies after state 9(and all other states) updates.
5.1 Backward Conditioning
To mitigate the insufficiencies surrounding update monotonicity, we build on a recent result in Lesmana &
Pun (2021) and propose backward conditioning : to perform update backward fromS¯TtoS0andconditioning
the update of states with lower priority (happens earlier) on thenewpolicy π′of states with higher priority
(happens later). We formalize the above in the following update rule.
Definition 5.1 (Backward Conditioning Rule).For anyϵ>0, set ¯T:=¯Tϵand letS0:¯T:=Ss0,ΠMD
0:¯Tbe a
priority-ordering onS. Then, for t=¯T−1 : 0:
∀s∈St,π′(s)←arg max
a∈AsQ(π′)¯T−1−t·π(s,a) (18)
Remark 5.2.Note that in the Definition 5.1, we have assumed the existence of a priority-ordering : whatever
actions the states in S0:t−1are taking are assumed to have no effect on the choice of states in St. This
justifies (18): its backward order and conditioning the update of any s∈S ¯T−1(withhighestpriority) on
theoldpolicy π. We note however that even without such priority-ordering , the worst that can happen is
Ss0,ΠMD
t =S,∀t∈[0,¯T], which is equivalent to performing standard PI in (7) ¯Ttimes.
Next, we will show that the backward conditioning rule preserves the SPE optimality of termination policy.
Proposition 5.3. Ifπ′=πand update follows the rule in equation 18, then π,π′are SPE policy.
Proof.First, we will show that
∀s∈S,s∈Ss0,ΠMD
0:¯T−1(19)
Since ¯Tϵ−1≥ˆTs0, by definition of ˆTs0,∀s∈S,
∃π∈ΠMD,¯Tϵ−1/summationdisplay
t=0P[Sπ
t=s|s0]>0⇒∃t∈[0,¯Tϵ−1]s.t.∃π∈ΠMD,P[Sπ
t=s|s0]>0
⇒∃t∈[0,¯Tϵ−1]s.t.s∈St
⇒s∈S0:¯Tϵ−1.
Now, let us consider arbitrary s∈S. By (19),∃t∈[0,¯T−1]s.t.s∈St. Using such t, by (18), we have
∀a∈As,
Q(π′)¯T−t−1·π(s,π′(s))≥Q(π′)¯T−t−1·π(s,a)
which by π=π′, implies
Qπ′(s,π′(s))≥Qπ′(s,a).
10Published in Transactions on Machine Learning Research (11/2022)
Example 5.4 (Efficiency of Backward Conditioning) .To illustrate the difference between (18) and (7), let us
reconsider the setup in Example 4.6. Given the same initialization (see Figure 3(a)), backward conditioning
rule (18) asserts state 9∈˜S3to update earlierthan state 21∈˜S0, in onecurrentiteration. This necessarily
means that by the time state 21updates, it will have anticipated state 9’scurrentpolicyπ(1)(9) =←(see
Figure 3(b)) and directly obtain the desired π(1)(21) =→(see Figure 3(c)). When combined, such current
iteration ends up in π(1)that corresponds to the target SPE path ˆπ; note how the path extending from
s0 = 21in Figure 3(c) overlaps with the one in Figure 1(c). In contrast to (7), backward conditioning
imposes that the choice of later states are directly propagated to earlier states in each policy iteration and
correspondingly prevents inefficient movement of policies (i.e., away from an SPE policy as in Figure 2).
(a)
 (b)
 (c)
Figure 3: 2-Layered Correction with Backward Conditioning. (a) Initialization. (b) State 9updates
first. (c) State 21updates last, after 9,13,17make their updates.
Remark5.5.Through Example 4.6 and Example 5.4, we have concluded that relative to standard PI, backward
conditioning mitigates inefficient movement of policies (cf. Figure 2) by propagating information about future
players’ “optimal“ policies (as soon as it is found) directly to earlier players (cf. Figure 3). To see how
this phenomenon transfers to a TIC “Q-learning“ setting, readers may refer to Table 1 in Section 6.1 and
Appendix D.4.3, particularly to (i) the reduced delusionality in earlier episodes under backward conditioning
(cf. Figure 6(e)) as compared to standard forwardly-ordered version (cf. Figure 9(e)), and (ii) the improved
stability of Figures 6(e)-(f) relative to Figures 9(e)-(f). We would like to remark that our observations are
consistent with the results of related literature under non-TIC motivations, e.g., sample efficiency (Lin (1991);
Lee et al. (2019)), consistent uncertainty propagation (Bai et al. (2021)), where they have similarly concluded
backward update’s power in information propagation.
Remark 5.6.In Theorem 25 of Lesmana & Pun (2021), a finite-horizon analog to the update rule (18) has
been shown to exhibit lex-monotonicity (i.e. a weaker update monotonicity than PIT that reflects closer to
SPE), by leveraging a policy-independent ordering on time-extended state-space due to T <∞(i.e. players
are times). This prevents the cycling of policies, implying convergence. In T=∞, we lose this order (i.e.
players are states ) and resort to using visitation order on a trajectory. This results in a lex-mono analog: ∀t,
ifπ′
S>tiscloser to SPE ˆπS>tthan πS>t; so isπ′
St. It was discussed through Example 5.4 when St={21}
andS>t={17,13,9}. Convergence thus remains open as complications may arise when St∩St′̸=∅for some
t̸=t′.
5.2 Approximate Backward Conditioning
In this subsection, we are interested in deriving an approximate version to the backward conditioning rule in
Definition 5.1. This can be done in two steps. Firstly, we will replace the exact computation of Qπ′(s,a)
withprediction . To this end, we will use our results in Theorem 4.3 and derive TIC-adjusted TD targets for
predictingrπ′(st,at)from (10)-(11) and Qπ′(st,at)from (12)-(13),
ξr
t(m) =/braceleftigg
φ(0)R(st,at), m =t
γ(m−t)
γ(m−(t+1))rt+1,m(St+1,π′(St+1)), m>t(20)
11Published in Transactions on Machine Learning Research (11/2022)
ξQ
t=

φ(0)R(st,at) +Q(St+1,π′(St+1))
−max(0,∆rt), t≤T∗−1
φ(0)R(st,at), t=T∗andst∈¯S(21)
where ∆rt=/summationtextT∗
m=t+1rt+1,m(st+1,π′(st+1))−rt,m(st,at). Then, we can incorporate the backward conditioning
simply by reordering update from the end of a sampled trajectory t=T∗tot= 0. Note that here, we
have replaced ¯TwithT∗≤¯Tto account for the variable length of trajectories encountered in practice. We
summarize this section with a pseudocode in Algorithm 1, where lines 11,18−20capture our backward
conditioning and lines 12−17capture the TIC-adjusted TD evaluation5.
Algorithm 1 Backward Q-learning (bwdQ)
1:Parameters: exploration rate ϵ, episode length ¯T, learning rates αQ,αr
2:Init:
3:Q(s,a) = 0,∀s∈S\ ¯S,a∈A;
4:Q(s,a) =φ(0)R(s,a),∀s∈¯S,a∈¯A;
5:rτ,m(s,a) = 0,∀τ,m,s∈S,a∈A;
6:π′(s)←arg maxaQ(s,a),∀s∈S,π←∅
7:repeat
8: π←π′;
9:ChooseS0randomly;
10:SampleS0,A0,... ,ST∗−1,AT∗−1,ST∗,AT∗= ¯a∼πϵ;
11:fort←T∗to0do
12:form←ttoT∗do
13: Computeξr
t(m)according to (20);
14:rt,m(St,At)←rt,m(St,At) +αr(ξr
t(m)−rt,m(St,At));
15:end for
16:ComputeξQ
taccording to (21);
17:Q(St,At)←Q(St,At) +αQ(ξQ
t−Q(St,At));
18:ifQ(St,π(St))<maxaQ(St,a)then
19:π′(St)←arg maxaQ(St,a)
20:else
21:π′(St)←π(St)
22:end if
23:end for
24:untilstable ( π′̸=π)
Remark 5.7.While Algorithm 1 can be considered as a Q-learning’s variant, standard convergence analysis
such as in Bertsekas & Tsitsiklis (1996) does not apply to our case. Even more recent analysis techniques on
coupled iterations such as those done for double Q-learning (Hasselt (2010); Xiong et al. (2020); Zhao et al.
(2021)) do not apply to our case for the fully-coupled dynamics of the iterated r-functions and Q-function.
Formal convergence analysis of backward Q-learning is thus left for future study.
6 Learning Performance: An Illustration
In this section, we illustrate the behaviour of bwdQ in two TIC Gridworld environments: (i) Deterministic
(D), by reusing our motivating example in Figure 1, which has been shown to exhibit future deviations, and
(ii)Stochastic (S) , by injecting some random noise into state 9’s transition in (D). For our comparative study,
we consider as benchmarks two approximate PI variants that also target SPE policy under general-discounting
objectives, namely standard PI with Monte Carlo (MC) andsophisticated EU (sophEU) from Evans et al.
(2016). Pseudocodes and training specifications are provided in Appendices D.2-D.3.
5For detailed derivations of Algorithm 1, readers can refer to Appendix C.
12Published in Transactions on Machine Learning Research (11/2022)
6.1 Results
Our results and evaluation can be segregated into three components: efficiency, value prediction, and
termination policy, all of which are summarized into Table 1 and Figure 4.
Efficiency In Section 5.1, we have provided an intuition on the desirability of backward conditioning . From
Table 1, we can see its implication to actual learning instances with approximation. In particular, we can
observe that bwdQ demonstrates higher learning efficiency in both (D) and (S): it has significantly shorter
∆i∗in average (mean) with lower standard deviation compared to the others.
Table 1:Delusional period ∆i∗.=|i∗
21−i∗
9|statistics, presented as mean (stdev )(in thousands). This
metric relates to the 2-layered correction illustrated in Figures 2 and 3: ∆i∗quantifies how many iterations
21needs to reflect 9’s move to SPE. Episode indexes i∗
9andi∗
21represent the first overtaking episodes of
mean SPE Q-value at states 9 and 21, respectively; see Appendix D.4.1 for illustrative Q-value curves. For
each algorithm and environment, 10 experiments are conducted and each consists of 50 random seeds.
MC SophEU BwdQ
(D)15.39 (3.69)69.97 (1.81)2.37 (0.73)
(S)14.55 (4.83)97.56 (2.17)3.68 (0.51)
(a)
 (b)
Figure 4: (a) Value learning curves of s0= 21.(b) Termination policies . For each algorithm in (a),
the dark line and the shaded region each refers to the mean and standard deviation of 50 experiments, where
in one experiment, we record the values Vi(s0) =Qi(s0,πi(s0))at the end of each training episode i. In (b),
we record the termination policies πI(s),∀s∈Safter the policies at all states stabilize at training episode
I; note that Ihere may differ across algorithms and experiments as we only want to show the asymptotic
performance of each algorithm in learning SPE policy. Groundtruth ‘TRUE’ in (a) is then computed as the
value of analytical SPE policy in (D), and the value of termination policy in (S).
13Published in Transactions on Machine Learning Research (11/2022)
Value prediction From Figure 4(a), we can observe that in (D), the mean value of bwdQ matches closely
the groundtruth (manually computed) upon convergence. On the contrary, sophEU and MC both converge at
a value strictly smaller than the groundtruth. Similar conclusion can be drawn in (S), despite bwdQ produces
higher variance than the rest; see Appendices D.4.2-D.4.3 for more results and discussions on value biases.
Termination policy In both (D) and (S), all algorithms (i.e. MC, sophEU, and bwdQ) converge and the
termination policies are plotted in Figure 4(b). While all algorithms converge to the same policies in (S),
this is not true in (D): at s= 13,17, MC and sophEU converge to {↑,↑}when bwdQ converges to {↑,←}
or{←,←}. Thus, in Figure 4(b), we present together these three different termination policies. For the
termination policies in (D), we can verify that they correspond to the groundtruth SPE policies (by Definition
3.1 andQˆπcomputed manually from the reward specifications in Figure 1). This is consistent with our results
in Propositions 4.1 (MC) and 5.3 (bwdQ) that guarantee SPE optimality if converged. For the termination
policies in (S), we can see how the noise injected to 9affects the SPE policy: ˆπ(13)shifts from{←,↑}in (D)
to{←}in (S) asQˆπ(13,↑)in (S) is pulled down by random transitions of 9→5and9→13.
7 Conclusion and Future Works
Prior to this paper, it was unclear how policy iteration performs and whether it is sufficient in TIC RL
settings under which BPO or DP becomes inaccessible. Through this paper, we demonstrated how introducing
SPE optimality can shed lights on the two fundamental questions surrounding the use of PI in TIC RL
setting. While this paper on TIC RL is of theoretical nature, we managed to use a toy Gridworld example to
demonstrate our findings. In particular, we obtain positive results on PI in the sense of both standard PI and
backward conditioning’s capability to characterize SPE policies. Though we could not close the convergence of
either standard PI or backward conditioning, we have made progress towards it by verifying the importance of
orderedpolicy iteration and improvement criteria. From the perspective of policy evaluation, SPE optimality
recovers the use of DP-like formulas. This has resulted in familiar forms of algorithms, such as our backward
Q-learning, that is also important towards closing the analysis of SPE policy search.
From our current paper, closing the convergence analysis of PI, backward conditioning, and their approximate
variants under SPE optimality (i.e., in particular, addressing the fully-coupled iterations that arise with
TIC adjustments) is an important future direction. Secondly, noting our recovery of DP-like formulas and
Q-learning like algorithms, we see promise in adopting the use of function approximations and scaling up
to more general models such as linear MDP (Jin et al. (2020)) or more complex domains such as Atari
(Bellemare et al. (2013)). Finally, it will be interesting to apply the analysis of this paper to other TIC sources
such as risk-sensitive objectives and compare across different TIC sources the extent of future deviations,
TIC-adjustment techniques, and the control performance of PI (as a proxy for SPE policy). On a broader
extent, we hope that our first attempt can inspire more works on TIC RL towards closing the gap between
progress in standard RL and TIC RL.
14Published in Transactions on Machine Learning Research (11/2022)
References
Chenjia Bai, Lingxiao Wang, Lei Han, Jianye Hao, Animesh Garg, Peng Liu, and Zhaoran Wang. Principled
exploration via optimistic bootstrapping and backward induction. In International Conference on Machine
Learning , pp. 577–587. PMLR, 2021.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research , 47:253–279, 2013.
Richard Bellman. Dynamic Programming . Princeton University Press, Princeton, NJ, USA, 1 edition, 1957.
Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-dynamic programming. , volume 3 of Optimization and
neural computation series . Athena Scientific, 1996. ISBN 1886529108.
Tomas Björk and Agatha Murgoci. A theory of Markovian time-inconsistent stochastic control in discrete
time.Finance and Stochastics , 18(3):545–592, June 2014. doi: 10.1007/s00780-014-0234-y.
Tomas Björk, Agatha Murgoci, and Xun Yu Zhou. Mean-variance portfolio optimization with state-dependent
risk aversion. Mathematical Finance , 24(1):1–24, January 2014. doi: 10.1111/j.1467-9965.2011.00515.x.
Dotan Di Castro, Aviv Tamar, and Shie Mannor. Policy gradients with variance related risk criteria. arXiv
preprint arXiv:1206.6404 , 2012.
Owain Evans, Andreas Stuhlmüller, and Noah D. Goodman. Learning the preferences of ignorant, inconsistent
agents. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 30 of AAAI’16 , pp.
323–329, Phoenix, Arizona, February 2016. URL https://dl.acm.org/doi/10.5555/3015812.3015860 .
William Fedus, Carles Gelada, Yoshua Bengio, Marc G. Bellemare, and Hugo Larochelle. Hyperbolic
discounting and learning over multiple horizons. arXiv: 1902.06865 , February 2019. URL https://arxiv.
org/abs/1902.06865 .
Hado Hasselt. Double q-learning. Advances in neural information processing systems , 23, 2010.
Ronald A Howard. Dynamic programming and markov processes. 1960.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with
linear function approximation. In Conference on Learning Theory , pp. 2137–2143. PMLR, 2020.
Zeb Kurth-Nelson and A. David Redish. A reinforcement learning model of precommitment in decision
making.Frontiers in Behavioral Neuroscience , 4:184, 2010. ISSN 1662-5153. doi: 10.3389/fnbeh.2010.00184.
URL https://www.frontiersin.org/article/10.3389/fnbeh.2010.00184 .
Tor Lattimore and Marcus Hutter. General time consistent discounting. Theoretical Computer Science , 519:
140–154, 2014.
Su Young Lee, Choi Sungik, and Sae-Young Chung. Sample-efficient deep reinforcement learning via episodic
backward update. Advances in Neural Information Processing Systems , 32, 2019.
Nixie S Lesmana and Chi Seng Pun. A subgame perfect equilibrium reinforcement learning approach to
time-inconsistent problems. Available at SSRN 3951936 , 2021.
Long Ji Lin. Programming robots using reinforcement learning and teaching. In AAAI, pp. 781–786, 1991.
Shie Mannor and John N. Tsitsiklis. Mean-variance optimization in markov decision processes. CoRR,
abs/1104.5601, 2011. URL http://arxiv.org/abs/1104.5601 .
Robert A. Pollak. Consistent planning. The Review of Economic Studies , 35(2):201, April 1968. doi:
10.2307/2296548.
LA Prashanth and Mohammad Ghavamzadeh. Actor-critic algorithms for risk-sensitive mdps. In Advances
in neural information processing systems , pp. 252–260, 2013.
15Published in Transactions on Machine Learning Research (11/2022)
Matthew J Sobel. The variance of discounted Markov decision processes. Journal of Applied Probability , pp.
794–802, 1982.
Robert H. Strotz. Myopia and inconsistency in dynamic utility maximization. The Review of Economic
Studies, 23(3):165–180, December 1955. doi: 10.2307/2295722.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
Aviv Tamar and Shie Mannor. Variance adjusted actor critic algorithms. ArXiv, abs/1310.3697, 2013.
Christopher J.C.H. Watkins and Peter Dayan. Q-learning. Machine learning , 8(3-4):279–292, May 1992.
Huaqing Xiong, Lin Zhao, Yingbin Liang, and Wei Zhang. Finite-time analysis for double q-learning. Advances
in neural information processing systems , 33:16628–16638, 2020.
Lin Zhao, Huaqing Xiong, and Yingbin Liang. Faster non-asymptotic convergence for double q-learning.
Advances in Neural Information Processing Systems , 34:7242–7253, 2021.
16Published in Transactions on Machine Learning Research (11/2022)
A Additional Details on Assumption 3.3-3.5
A.1 MDP Examples under General-Discounting Criterion
In this section, we derive several sufficient conditions for our assumptions in Section 3.2, in the form of
restrictions on MDP (S,A,P,R)or discounting function φ(·).
Definition A.1 (Boundary-only Rewards).The reward function R:S×A→ R+isboundary-only if it is
non-zero only at boundary states, i.e.R(s,a)>0only ifs∈¯S.
Lemma A.2. Any MDP that has boundary-only rewards satisfies Assumption 3.3.
Proof.LetTπ
¯S|sdefines the minimum hitting time of any boundary states ¯s∈¯Swhen initiated at sand
following π. Thus,∀s∈S,∀π∈ΠMD,∀¯T <∞,
Vπ(s):=E/bracketleftigg∞/summationdisplay
t=0φ(t)Rπ
t|s/bracketrightigg
=¯T/summationdisplay
τ=0P[Tπ
¯S=τ|s]E/bracketleftiggτ/summationdisplay
t=0φ(t)Rπ
t|s/bracketrightigg
+∞/summationdisplay
τ=¯T+1P[Tπ
¯S=τ|s]E/bracketleftiggτ/summationdisplay
t=0φ(t)Rπ
t|s/bracketrightigg
,
Vπ¯T·¨π(s):=E/bracketleftigg∞/summationdisplay
t=0φ(t)Rπ¯T·¨π
t|s/bracketrightigg
=¯T/summationdisplay
τ=0P[Tπ¯T·¨π
¯S=τ|s]E/bracketleftiggτ/summationdisplay
t=0φ(t)Rπ¯T·¨π
t|s/bracketrightigg
+∞/summationdisplay
τ=¯T+1P[Tπ¯T·¨π
¯S=τ|s]E/bracketleftiggτ/summationdisplay
t=0φ(t)Rπ¯T·¨π
t|s/bracketrightigg
=¯T/summationdisplay
τ=0P[Tπ
¯S=τ|s]E/bracketleftiggτ/summationdisplay
t=0φ(t)Rπ
t|s/bracketrightigg
.(since∀τ≥¯T+ 1,Sτ= ¯svoid⇒P[Tπ¯T·¨π
¯S=τ|s] = 0.)
By bounded reward function,
Rmax:= max{|R(s,a)|:s∈S,a∈A} (22)
exists. Then,∀ϵ>0,∀s∈S, we can set ¯T <∞s.t.
|Rmax|φ(¯T+ 1)≤ϵ (23)
and the following holds,
sup
π∈ΠMD|Vπ¯T·π(s)−Vπ¯T·¨π(s)|= sup
π∈ΠMD/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∞/summationdisplay
τ=¯T+1P[Tπ
¯S=τ|s]E[φ(τ)Rπ
τ|s]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(byboundary-only rewards)
≤sup
π∈ΠMD|Rmax|∞/summationdisplay
τ=¯T+1P[Tπ
¯S=τ|s]φ(τ) (by (22))
≤sup
π∈ΠMD|Rmax|φ(¯T+ 1)P[Tπ
¯S>¯T|s] (byφ(·)decreasing)
≤|Rmax|φ(¯T+ 1)≤ϵ (by (23))
Lemma A.3. Suppose an MDP has boundary-only rewards, s0that satisfies Assumption 3.4 such that
¯Ts0,0<∞, and a discounting factor φ:N→(0,1]that satisfies
∀t≥0,φ(τ+t)
φ(τ)is increasing in τ,τ≥1. (24)
Then, Assumption 3.5 holds.
17Published in Transactions on Machine Learning Research (11/2022)
Proof.Suppose otherwise, ∃ϵ∗>0,t∗∈[0,¯Tϵ∗],s∗∈Ss0,ΠMD
t∗,π∗∈ΠMDs.t.
P[Sπ∗
t∗=s∗|s0]>0 (25)
and∀κ>0,
ϵ∗
κ<∥Vπ∗(s∗)−Vπ¯Tϵ∗
∗−t∗·¨π(st∗)∥=∞/summationdisplay
τ=¯Tϵ∗−t∗+1P[Tπ∗
¯S=τ|s∗]E[φ(τ)Rπ∗
τ|s∗] (26)
Let us fix π:=π∗and set
κ∗:=P[Sπ∗
t∗=s∗|s0]φ(¯Tϵ∗+ 1) (27)
Note thatκ∗>0by (25) and ¯Tϵ∗<∞(by Assumption 3.3, ¯Ts0,ϵ∗<∞, and by Assumption 3.4, ˆTs0<∞).
Then,
/vextendsingle/vextendsingle/vextendsingle/vextendsingleVπ∗(s0)−Vπ¯Ts0,ϵ∗
∗·¨π(s0)/vextendsingle/vextendsingle/vextendsingle/vextendsingle=/summationdisplay
τ=¯Ts0,ϵ∗+1P[Tπ∗
¯S=τ|s0]E[φ(τ)Rπ∗
τ|s0]
(by boundary-only rewards; see Lemma A.2’s proof)
≥/summationdisplay
τ=¯Tϵ∗+1P[Tπ∗
¯S=τ|s0]E[φ(τ)Rπ∗
τ|s0] (by¯Ts0,ϵ∗≤¯Tϵ∗)
=/summationdisplay
τ=¯Tϵ∗+1/summationdisplay
s∈SP[Sπ∗
t∗=s|s0]P[Tπ∗
¯S=τ−t∗|s]E[φ(τ)Rπ∗
τ−t∗|s]
≥P[Sπ∗
t∗=s∗|s0]/summationdisplay
τ=¯Tϵ∗−t∗+1P[Tπ∗
¯S=τ|s∗]E[φ(τ+t∗)Rπ∗
τ|s∗]
(by non-negative rewards and probabilities)
≥P[Sπ∗
t∗=s∗|s0]φ(¯Tϵ∗+ 1)/summationdisplay
τ=¯Tϵ∗−t∗+1P[Tπ∗
¯S=τ|s∗]E[φ(τ)Rπ∗
τ|s∗]
(by (24) and t∗∈[0,¯Tϵ∗])
>P[Sπ∗
t∗=s∗|s0]φ(¯Tϵ∗+ 1)ϵ∗
κ∗=ϵ∗(by (26) and (27))
This contradicts definition of ¯Ts0,ϵ∗(see Assumption 3.3), implying that our supposition is false.
Withκ(ϵ,t,st,π;s0):=P[Sπ
t=st|s0]φ(¯Tϵ+ 1), we will now show that
lim
ϵ→0ϵ
P[Sπ
t=st|s0]φ(¯Tϵ+ 1)= 0 (28)
For any fixed ϵ>0, let us define
G(¯Tϵ;s0):= min{P[Sπ
t=st|s0]>0 :t∈[0,¯Tϵ],st∈S,π∈ΠMD}. (29)
Then,∀π∈ΠMD,∀t∈[0,¯Tϵ],∀st∈Ss0,ΠMD
t,
0≤ϵ
P[Sπ
t=st|s0]φ(¯Tϵ+ 1)≤ϵ
G(¯Tϵ;s0)φ(¯Tϵ+ 1)(30)
Since ¯Ts0,0<∞, we have
¯T0.= max{ˆTs0,¯Ts0,0}<∞. (31)
18Published in Transactions on Machine Learning Research (11/2022)
Let us fix arbitrarily π∈ΠMD,t∈[0,¯T0],st∈Ss0,ΠMD
t. By (31), limϵ→0G(¯Tϵ;s0) =G(¯T0;s0)>0and
limϵ→0φ(¯Tϵ+ 1) =φ(¯T0+ 1)>0. Thus, we can take limϵ→0on the upper and lower bound in (30) and have
shown
lim
ϵ→0ϵ
P[Sπ
t=st|s0]φ(¯Tϵ+ 1)= 0
Finally, it’s straightforward to verify that our hyperbolic Gridworld in Figure 1(a) has boundary-only rewards
ands0= 21that satisfies Assumption 3.4. Moreover, due to the existence of τ∗:=max{Tπ
¯S<∞:π∈
ΠMD}<∞by its deterministic transition and |ΠMD|<∞, we have∀¯T≥τ∗,
sup
π∈ΠMD|Vπ(s0)−Vπ¯T·¨π(s0)|= sup
π∈ΠMD:Tπ
¯S<∞|Vπ(s0)−Vπ¯T·¨π(s0)|(by boundary-only rewards)
= 0 ( by∀τ >¯T≥τ∗,∀π∈ΠMDwith ¯Tπ
¯S<∞,P[Tπ
¯S=τ|s0] = 0)
and thus, ¯Ts0,0≤τ∗<∞. For a more concrete example, we can refer to the restricted Hyperbolic Gridworld
in Example 4.6, where we can compute manually ˆTs0= 7and ¯Ts0,0= 8.
A.2 Implied Bounded Value Functions
Through the following lemma, we can link Assumption 3.3 to the standard well-posedness condition of
bounded value functions that ensures the existence of optimal policy.
Lemma A.4. If Assumption 3.3 holds, then ∀s∈S,∀π∈ΠMD,Vπ(s)<∞.
Proof.Suppose∃π∗,s∗s.t.Vπ∗(s∗) =∞. Then, we can set s,π←s∗,π∗and arbitrary ϵ∗>0s.t.∀¯T <∞,
|Vπ¯T
∗·˜π(s∗)−Vπ∗(s∗)|>ϵ∗sinceVπ¯T
∗·˜π(s∗)<∞.
B Theorem 4.3
B.1 Technical Assumptions
Assumption B.1 ("Relevant at tunder π").Ift= 0,
P[Sπ
t=s0|s0] = 0,∀t≥1∧P[S1=s0|S0=s0,A0=a0] = 0 (32)
Ift>0,∃(st−1,at−1)"relevant at t−1under π" s.t.
P[St=st|St−1=st−1,At−1=at−1]>0∧at=π(st). (33)
Intuitively, for t > 0, (33) exhausts the use instances of (st,at)in PE updates Qπ(st−1,at−1)←
E[Qπ(st,π(st))] +...and thus, it must hold. Whereas for t= 0, some restrictions on the MDP
(e.g.,∀t′̸=t,Ss0,ΠMD
t∩Ss0,ΠMD
t′ =∅as in Example 4.6) can be imposed to ensure that (32) holds
∀π∈ΠMD,s0∈S\{ ¯S},a0∈As0. Note however that in actual use instances, (32) only need to hold
for the πencountered in the PI updates (instead of ∀π∈ΠMD). This may relax the need for such MDP
restrictions: as we can observe from our experiments (see Section 6), our algorithm still performs plausibly
well even when∀t′̸=t,Ss0,ΠMD
t∩Ss0,ΠMD
t′ =∅does not hold. In what follows, we present several intermediate
results that link the conditions in Assumption B.1 to the "approximation" (13) in Theorem 4.3.
Lemma B.2. At anyt≥0, if(st,at)is "relevant at tunder π", then∃s0,a0"relevant at 0 under π" s.t.
P[Sπs0,a0
t =st|s0]>0∧at=πs0,a0(st)
19Published in Transactions on Machine Learning Research (11/2022)
with πs0,a0defined as follows
πs0,a0(s) =/braceleftigg
a0,ifs=s0
π(s),otherwise(34)
Proof.(Base case: t= 0.) Note that for any (s0,a0)that is "relevant at 0under π", we have P[Sπs0,a0
0|s0] =
1>0. Moreover, a0=πs0,a0(s0)holds by definition in (34).
(t>0.) Proof by induction. Suppose that the relation holds at t=t′−1, we will show that it also holds at
t=t′. By (st′,at′)’s "relevance at t′under π",∃(st′−1,at′−1)"relevant at t′−1under π" s.t.
P[St′=st′|St′−1=st′−1,At′−1=at′−1]>0∧at′=π(st′) (35)
Moreover, by assumption (that at t=t′−1the relation holds), the above (st′−1,at′−1)satisfies:∃s0,a0
"relevant at 0under πs.t.
P[Sπs0,a0
t′−1=st′−1|s0]>0∧at′−1=πs0,a0(st′−1). (36)
Therefore,
P[Sπs0,a0
t′ =st′|s0]≥P[Sπs0,a0
t′ =st′|St′−1=st′−1]P[Sπs0,a0
t′−1=st′−1|s0]
=P[St′=st′|St′−1=st′−1,At′−1=at′−1]P[Sπs0,a0
t′−1=st′−1|s0]>0(by (36))
Moreover, by (s0,a0)’s "relevance at 0under π" andt′>0, we must have st′̸=s0which then implies
at′=πs0,a0(st′) (by (35))
Lemma B.3. For any π∈ΠMD,t≥0, and (st,at)"relevant at tunder π",∃κ>0s.t.
∀st+1∼pat
st,/vextendsingle/vextendsingle/vextendsingleVπ(st+1)−Vπ¯Tϵ−(t+1)·¨π(st+1)/vextendsingle/vextendsingle/vextendsingle≤ϵ
κ(37)
andlimϵ↓0ϵ
κ= 0.
Proof.Let us first fix arbitrarily (st,at,π). By Lemma B.2, ∃s0,a0and˜π:=πs0,a0s.t.
P[S˜π
t=st|s0]>0∧at= ˜π(st) (38)
Next, for any arbitrary choice of st+1∼patst, we have
P[St+1=st+1|St=st,At=at]>0 (39)
Therefore,
P[S˜π
t+1=st+1|s0]≥P[S˜π
t+1=st+1|St=st]P[S˜π
t=st|s0]
=P[St+1=st+1|St=st,At=at]P[S˜π
t=st|s0] (by (38))
>0 (by (38) and (39))
which by Assumption 3.5, implies
ϵ
κ(ϵ,t+ 1,st+1,˜π;s0)≥/vextendsingle/vextendsingle/vextendsingleV˜π(st+1)−V˜π¯Tϵ−(t+1)·¨π(st+1)/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingleVπ(st+1)−Vπ¯T−(t+1)·¨π(st+1)/vextendsingle/vextendsingle/vextendsingle
(by(s0,a0)’s "relevance at 0under π" andt+ 1>0,st+1̸=s0.)
20Published in Transactions on Machine Learning Research (11/2022)
and
lim
ϵ↓0ϵ
κ(ϵ,t+ 1,st+1,˜π;s0)= 0. (40)
Now, let us choose κ:=min{κ(ϵ,t+ 1,st+1,˜π;s0) :st+1∼patst}. Note that (37) directly holds. It thus
remains to show the following,
lim
ϵ↓0ϵ
κ= lim
ϵ↓0ϵ
min{κ(ϵ,t+ 1,st+1,˜π;s0) :st+1∼patst}
= lim
ϵ↓0max/braceleftbiggϵ
κ(ϵ,t+ 1,st+1,˜π;s0):st+1∼pat
st/bracerightbigg
= 0 (by (40) and at most finitely many choices of st+1∈S)
B.2 Proof of Theorem 4.3
For anym≥τ+ 1, we can derive r-function recursion as follows
rπ,τ,m(s,a).=E[φ(m−τ)R(Sa·π
m,π(Sa·π
m))|Sτ=s] (41)
=ESτ+1∼pas[E[φ(m−τ)R(Sπ
m,π(Sπ
m))|Sτ+1]] (42)
=ESτ+1∼pas/bracketleftbiggφ(m−τ)
φ(m−(τ+ 1))E[φ(m−(τ+ 1))R(Sπ
m,π(Sπ
m))|Sτ+1]/bracketrightbigg
(43)
=ESτ+1∼pas/bracketleftbiggφ(m−τ)
φ(m−(τ+ 1))rπ,τ+1,m(Sτ+1,π(Sτ+1)/bracketrightbigg
(44)
Form=τ, by Definition 4.2, we have
rπ,τ,τ(s,a).=E[φ(m−τ)R(Sa·π
m,π(Sa·π
m))|Sτ=s] (45)
=ER∼pas[φ(0)R(s,a)] (46)
Next, we derive Q-function recursion,
Qπ(st,at).=E/bracketleftbig
φ(0)R(st,at) +φ(1)R(Sat·π
t+1,π(Sat·π
t+1)) +...|St=st/bracketrightbig
(47a)
=ER∼patst[φ(0)R(st,at)] +E/bracketleftbig
φ(0)R(Sat·π
t+1,π(Sat·π
t+1)) +φ(1)R(Sat·π
t+2,π(Sat·π
t+2)) +...|St=st/bracketrightbig
−/braceleftbig
E/bracketleftbig
φ(0)R(Sat·π
t+1,π(Sat·π
t+1)) +φ(1)R(Sat·π
t+2,π(Sat·π
t+2)) +...|St=st/bracketrightbig
−E/bracketleftbig
φ(1)R(Sat·π
t+1,π(Sat·π
t+1)) +φ(2)R(Sat·π
t+2,π(Sat·π
t+2)) +...|St=st/bracketrightbig/bracerightbig
(47b)
=ER∼patst[φ(0)R(st,at)] +ESt+1∼patst[Qπ(St+1,π(St+1))]
−/braceleftig
ESt+1∼patst/bracketleftbig
E/bracketleftbig
φ(0)R(Sπ
t+1,π(Sπ
t+1)) +φ(1)R(Sπ
t+2,π(Sπ
t+2)) +...|St+1/bracketrightbig/bracketrightbig
−ESt+1∼patst/bracketleftbig
E/bracketleftbig
φ(1)R(Sπ
t+1,π(Sπ
t+1)) +φ(2)R(Sπ
t+2,π(Sπ
t+2)) +...|St+1/bracketrightbig/bracketrightbig/bracerightig
(47c)
=ER∼patst[φ(0)R(st,at)] +ESt+1∼patst[Qπ(St+1,π(St+1))]
−/braceleftigg
ESt+1∼patst/bracketleftigg
E/bracketleftigg∞/summationdisplay
m=t+1φ(m−(t+ 1))R(Sπ
m,π(Sπ
m))/vextendsingle/vextendsingle/vextendsingleSt+1/bracketrightigg/bracketrightigg
−ESt+1∼patst/bracketleftigg
E/bracketleftigg∞/summationdisplay
m=t+1φ(m−t)R(Sπ
m,π(Sπ
m))/vextendsingle/vextendsingle/vextendsingleSt+1/bracketrightigg/bracketrightigg/bracerightigg
(47d)
21Published in Transactions on Machine Learning Research (11/2022)
On the 2nd line, we apply ∀st+1∼patst,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleE/bracketleftigg∞/summationdisplay
m=t+1φ(m−(t+ 1))Rπ
m|St+1=st+1/bracketrightigg
−E
¯Tϵ/summationdisplay
m=t+1φ(m−(t+ 1))Rπ
m|St+1=st+1
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleE/bracketleftigg∞/summationdisplay
m=0φ(m)Rπ
m|S0=st+1/bracketrightigg
−E
¯Tϵ−(t+1)/summationdisplay
m=0φ(m)Rπ
m|S0=st+1
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=|Vπ(st+1)−Vπ¯Tϵ−(t+1)·¨π(st+1)|
≤ϵ
κ(by usingκfrom Lemma B.3)
On the 3rd line, we apply ∀st+1∼patst,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleE/bracketleftigg∞/summationdisplay
m=t+1φ(m−t)Rπ
m||St+1=st+1/bracketrightigg
−E
¯Tϵ/summationdisplay
m=t+1φ(m−t)Rπ
m|St+1=st+1
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleE/bracketleftigg∞/summationdisplay
m=1φ(m)Rπ
m−1|S0=st+1/bracketrightigg
−E
¯Tϵ−t/summationdisplay
m=1φ(m)Rπ
m−1|S0=st+1
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleE/bracketleftigg∞/summationdisplay
m=1φ(m−1)Rπ
m−1|S0=st+1/bracketrightigg
−E
¯Tϵ−t/summationdisplay
m=1φ(m−1)Rπ
m−1|S0=st+1
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(byφ(.)decreasing)
=/vextendsingle/vextendsingle/vextendsingleVπ(st+1)−Vπ¯Tϵ−(t+1)·¨π(st+1)/vextendsingle/vextendsingle/vextendsingle (by (5))
≤ϵ
κ(by usingκfrom Lemma B.3)
Therefore, continuing from (47d), we can perform approximation with ¯Tϵ<∞as follows,
≈ER∼patst[φ(0)R(st,at)] +ESt+1∼patst[Qπ(St+1,π(St+1))]
−

ESt+1∼patst
¯Tϵ/summationdisplay
m=t+1E[φ(m−(t+ 1))R(Sπ
m,π(Sπ
m))|St+1]

−ESt+1∼patst
¯Tϵ/summationdisplay
m=t+1φ(m−t)
φ(m−(t+ 1))E[φ(m−(t+ 1))R(Sπ
m,π(Sπ
m))|St+1]



(by setting κ(ϵ) =κ/2which directly implies limϵ↓0ϵ
κ(ϵ)= 0)
By applying (46), Definition 4.2, and (44) on the 1st, 2nd, and 3rd line, respectively, we can then obtain
=ER∼patst[φ(0)R(st,at)] +ESt+1∼patst[Qπ(St+1,π(St+1))]
−

¯Tϵ/summationdisplay
m=t+1/parenleftig
ESt+1∼patst/bracketleftbig
rπ,t+1,m(St+1,π(St+1))/bracketrightbig
−rπ,t,m(st,at)/parenrightig

(47e)
Finally, based on the Definition 3.6, we will set our boundary conditions (when we are at some boundary
states),
Qπ(st,at) =ER∼patst[φ(0)R(st,at)],∀st∈¯S,at∈¯A (48)
C Backward Q-learning Algorithm
In this section, we detail the derivations of our Backward Q-learning in Section 5.2 from Theorem 4.3.
22Published in Transactions on Machine Learning Research (11/2022)
C.1 r-table Update
Based on the r-function recursion, i.e. (44) and (46), we obtain bootstrap targets that corresponds to (20) in
the main paper,
ξr
t(m)←φ(0)R(st,at),form=t (49)
ξr
t(m)←φ(m−t)
φ(m−(t+ 1))rt+1,m(St+1,π′(St+1)),form=t+ 1 :T∗(50)
Then, updates to r-table are made as follows,
rt,m(st,at)←(1−αr)rt,m(st,at) +αrξr
t(m),form=t:T∗(51)
given learning rate αr>0.
C.2 Q-table Update
Based on the Q-function recursion, i.e. (47e) and (48), we obtain bootstrap targets that corresponds to (21)
in the main paper,
ξQ
t←γ(0)R(st,at),fort=T∗andst∈¯S (52)
ξQ
t←γ(0)R(st,at) +Q(St+1,π′(St+1))−max(0,∆rt),fort≤T∗−1 (53)
where ∆rt=/summationtextT∗
m=t+1rt+1,m(st+1,π′(st+1))−rt,m(st,at). Then, updates to Q-table can be done as follows,
Q(st,at)←(1−αQ)Q(st,at) +αQξQ
t,fort≤T∗(54)
given learning rate αQ>0.
Truncation from ¯TtoT∗.For our implementation, instead of keeping track of all values up to ¯T, we use
the variable length T∗of each trajectory sampled following a current policy π. However, we will still set a
sufficiently large ¯Tas a proxy for ¯Tϵto ensure that all trajectory terminates.
Clipping of adjustment terms. Let us denote by ∆rπ
tthe adjustment terms in the 2nd row of (47e) as
in the main paper. Referring to (53), we note that the clipped function max(0,∆rt)has been used in place of
∆rt. This is done to slow down the accumulation of error relevant to ∆rt≈∆rπ
t. In particular, we note that
∆rπ
t≥0:
∆rπ
t.=¯T−1/summationdisplay
m=t+1/parenleftig
ES′∼patst/bracketleftbig
rπ,t+1,m(S′,π(S′))/bracketrightbig
−rπ,t,m(st,at)/parenrightig
=¯T−1/summationdisplay
m=t+1/parenleftbigg
ES′∼patst/bracketleftbigg/parenleftbigg
1−φ(m−t)
φ(m−(t+ 1))/parenrightbigg
rπ,t+1,m(S′,π(S′)/bracketrightbigg/parenrightbigg
(by (44))
≥0 (byφ(·)discount factor and R:S×A→ R+s.t. (9) is non-negative)
But without clipping, ∆rt<0may happen in subsequent iterations, inflating Q-values at some states past a
certain threshold such that their neighboring states prefer transition to these inflated states than moving
towards goal states, when the latter clearly results in a fewer steps. This then creates a looping behaviour
which eventually lead to divergence.
C.3 Policy-table Update
We note our use of policy-table separate from the arg maxof a Q-table to represent greedypolicy. This is
due to the possibilities of non-unique actions realizing arg maxaQ(s,a)for somes∈Swhich may cause
non-unique r-function related values, i.e. the components in/summationtextT∗
m=t+1∆rm
t, after substituting different global
optima actions. Specifically, we follow the consistent tie-break rule proposed in Section 3.3 of Lesmana &
Pun (2021); see line 18-22 in Algorithm 1.
23Published in Transactions on Machine Learning Research (11/2022)
Algorithm 2 On-policy Monte Carlo Control (MC)
1:Input:Hyperbolic ( k= 1) Gridworld, Hyperparameters (ϵ,¯T)
2:Output: Approximate SPE Q-function Qˆπ(s,a)∀s∈S,a∈A
3:Initialize: Q(s,a)←0,∀s∈S\ ¯S,a∈A;Q(s,a)←φ(0)R(s,a),∀s∈¯S,a∈¯A;Returns (s,a)←∅,∀s∈
S,a∈A(s);π′(s)←arg maxaQ(s,a),∀s∈S;π←∅;
4:repeat
5:Update π←π′
6:ChooseS0randomly
7:Generate trajectoryω0:T∗.=S0,A0,... ,ST∗,AT∗following πϵ
8:SetG←0
9:fort←0toT∗do
10:ifthe pair (St,At)does not appear in ω0:t−1then
11: ComputeG←φ(T∗−t)R(ST∗,AT∗)
12: AppendGtoReturns (St,At)
13: UpdateQ(St,At)←average(Returns (St,At))
14: Updateπ′(St)←arg maxaQ(St,a)
15:end if
16:end for
17:untilstable ( π′̸=π)
D NUMERICAL EXAMPLES
This section provides some missing details on Section 6.
D.1 Environment Setup
We review 3 important considerations in our Gridworld designs: (i) existence of actual future deviations (i.e.
if states like (s0,sτ) = (21,9)exist, where the optimality of 21’s action is constrained by 9’s action such that
we have priority ordering onS), (ii)π∗0(s0)̸=ˆπ(s0)where the value of following SPE path ˆπis strictly
less than following precommitment path π∗0, and (iii) initialization to TIC, precommitment policy (that is
necessary to invoke the insufficiency of standard PI as illustrated in Example 4.6). For our stochastic (S)
example, we inject noise to the deterministic transitions pa
9(·)of state 9in (D) such that
∀a∈{←,↑,→,↓}, P(s′|s= 9,a) =/braceleftigg
.9,ifpa
9(s′) = 1in (D)
1−.9
3,else
D.2 Benchmark Algorithms
Following, we describe the two benchmark algorithms that we use in our experiments: MCandsophEU. For
ourMCimplementation, we use the fist-visit variant on-policy MC control6as described in Algorithm 2. For
thesophEU, we adapt the sophEUalgorithm proposed in Evans et al. (2016) by modifying the exploration
technique to ϵ-greedy; see Algorithm 3. This is done for fair comparison with the other two methods, i.e. MC
andbwdQ.
D.3 Training Setup
For each pair of algorithm and environment, hyperparameters are informally selected from the sets α,αQ∈
{.2,.3,.4,.5},αr∈{.7,.8,.9,1.0},ϵ∈{.01,.03,.05,.07,.1}with the following criteria in mind: (i) small
overtaking-mean i∗
21, (ii) small stdev-shade on the Q-value learning curves at s= 9,21, and (iii) identifiable
i∗
9,i∗
21(i.e. reducing the overlapping frequencies between two contending actions’ mean Q-value learning
6We refer to the sourcecode in https://github.com/dennybritz/reinforcement-learning prior to our hyperbolic-discounting
modification.
24Published in Transactions on Machine Learning Research (11/2022)
Algorithm 3 Sophisticated Expected-Utility Agent (sophEU)
1:Input:Hyperbolic ( k= 1) Gridworld, Hyperparameters (ϵ,¯T,α)
2:Output: Approximate SPE Q-function Qˆπ(s,a) =Q(s,a,0),∀s∈S,a∈A
3:Initialize: Q(s,a,d )←0,∀d,s∈S\ ¯S,a∈A;Q(s,a,d )←φ(0)R(s,a),∀d,s∈¯S,a∈A;π′
d(s)←
arg maxaQ(s,a,d ),∀d,s∈S,π←∅
4:repeat
5:Update π←π′
6:ChooseS0randomly
7:fort←0to¯T−1do
8:Sample action At∼πϵ
0(.|St)
9:Observe reward Rt+1.=R(St,At)and next state St+1
10:Setd←t
11:Compute utility U←φ(d)·R(St,At)
12:Compute expectation E←/summationtext
a′∼Aπϵ
0(a′|St+1)Q(St+1,a′,d+ 1)
13:UpdateQ(St,At,d)←Q(St,At,d) +α(U+E−Q(St,At,d))
14:Updateπ′
d(St)←arg maxaQ(St,a,d)
15:end for
16:untilstable ( π′
0̸=π0)
curves); see Figure 8(a)-(b) for relatively bad instances. For all environments and algorithms, we set ¯T= 100;
larger episode truncation does not affect much our experiment results. We summarize our final choice of
hyperparameters in Table 2.
Table 2: Hyperparameters
(ϵ,¯T,αQ/α,αr) MC sophEU bwdQ
(D) (.07, 100, -, -) (.07, 100, .4, -) (.07, 100, .4, 1.0)
(S) (.07, 100, -, -) (.07, 100, .4, -) (.07, 100, .4, .9)
D.4 Additional Results and Evaluation
This subsection expands the results and evaluation subsection in the main paper.
D.4.1 Q-value Learning Curves
To illustrate how we record the overtaking indexes i∗
9,i∗
21used to compute ∆i∗in Table 1, we plot in Figure
5-8 the Q-value learning curves that correspond to Figure 4.
D.4.2 Terminal Policies vs Groundtruth Value Comparisons
In Figure 4(b) of the main paper, we have shown that all algorithms will eventually terminate at SPE policy
ˆπ(s0)fors0= 21. However, Figure 4(a) shows that both MCandsophEUdo not flatten to the groundtruth
SPE value function Vˆπ(s0) =Qˆπ(s0,ˆπ(s0)). Now that we have Q-value learning curves in Figure 5-8, it
becomes clearer that the source of this discrepancy lies on the mis-evaluated Q-values; see Q(21,→)in Figure
5(a) for instance. This is explainable for a few reasons. Firstly, in the case of MC, the magnitude of exploratory
rateϵcauses Q-values to evaluate the exploratory policy πϵconsisting paths of extended lengths, which
correspondingly lead to an underestimated cumulative discounted reward. In the case of sophEU, similar
undervaluation of πhappens due to the action-taking probabilities being included in the Q-table updates;
see line 12-13 in Algorithm 3. While making ϵsmaller intuitively fixes this issue, learning performance
deteriorates (i.e. highly variable across seeds) once we decrease ϵup to certain threshold; our final choice
ofϵ=.07has taken this into consideration. Secondly, MCobserves some kind of smoothening effect across
updates, which if combined with the delayed reflecting of information (i.e. prolonged ∆i∗) exacerbate the
25Published in Transactions on Machine Learning Research (11/2022)
(a)
 (b)
(c)
 (d)
(e)
 (f)
Figure 5: Q-value Learning Curves for MC,sophEU, and bwdQS ats= 21,9in (D).
early flattening of policy values. Such smoothening concurrently explains how MCappears to have lesser
variance as compared to bwdQorsophEUat later iterations; see Figure 4(a)-Stochastic (S) in the main paper.
D.4.3 Ablation Study: Reversed Backward Q-learning
Since both benchmark algorithms suffer from similar undervaluation of policy issue, we construct an additional
benchmark: Reversed Backward Q-learning ( bwdQ-rev ), that is based on our own algorithm bwdQ. Here, we
only retain the extended DP-based policy evaluation component of bwdQ(that resembles TD-based methods
in standard RL literature) and apply standard conditioning by reversing the backward order of policy update
in line 11, Algorithm 1. This benchmarking can also be seen as an ablation study to see how backward
conditioning alone can reduce delusionality and improve learning performance. Figure 9 displays the value
and Q-value learning curves of bwdQ-rev against bwdQin both (D) and (S), under the same learning rates.
26Published in Transactions on Machine Learning Research (11/2022)
(a)
 (b)
(c)
 (d)
(e)
 (f)
Figure 6: Q-value Learning Curves for MC,sophEU, and bwdQats= 21,9in (S).
Value learning curves. From Figure 9(a), it can be seen that bwdQ-rev also manages to match the
groundtruth values at about the same speed of bwdQ. However, we can observe a large swing at earlier
iterations which indicate bwdQ-rev ’s degree of delusionality. In particular, such a swing is caused by 21’s late
update about 9’s strategy of going ’ ←’, resulting in delusional prediction targets Q(21,↑;π(9) =↑)and an
inflation of Q-values. BwdQ-rev ’s speed of correction towards the groundtruth here is then made possible by
its large learning rates7, which we will show to have some disadvantages next.
7Some comparisons can be made with MC’s degree of delusionality in Figure 4(a) of the main paper, that is milder for its
smaller (smoothened) learning rate. It is then natural to ask how sophEUdoes not seem to exhibit such (Q-)value inflation. This
can be explained by sophEU’sdelay-augmentation , in which the rate of value propagation from delays d > 0tod= 0may match
the speed of delusionality correction. To illustrate, we can observe how in Figure 5(c), sophEU’sQ(21,↑)climbs up slowly from 0
instead of jumping to near 2.0like most others.
27Published in Transactions on Machine Learning Research (11/2022)
(a)
 (b)
Figure 7: Q-value Learning Curves for sophEU(Ext.) ats= 21,9in (D).
(a)
 (b)
Figure 8: Q-value Learning Curves for sophEU(Ext.) ats= 21,9in (S).
Q-value learning curves. From Figure 9(e), while i∗
21ofbwdQ-rev seems to match bwdQ, we observe wide
stdev-shades for two contending actions ’ →’ and ’↑’ that overlap throughout training episodes, indicating
indecisive behaviour i.e. high variability of trained policy at convergence across random seeds. We note that
this phenomenon happens in 5 out of 10 bwdQ-rev experiments we conducted under (S) setup, while never
happening in bwdQ. Moreover, in Figure 9(f), the mean Q-curves of the two contending actions ’ ←’ and ’↑’
are relatively unstable and overlap frequently; see how bwdQbehaves in Figure 6(f) for comparison. These
evidences suggest that reversing backward conditioning to standard impedes learning, particularly impairing
bwdQ’s ability to handle larger learning rates. The results for both algorithms under (D) setup are largely
similar, except for bwdQ-rev ’s inflated Q-values at earlier iterations that has been covered previously.
28Published in Transactions on Machine Learning Research (11/2022)
(a)
 (b)
(c)
 (d)
(e)
 (f)
Figure 9:Value and Q-value Learning Curve at s= 21,9ofbwdQ-rev in (D) and (S). Experiment ID
forbwdQ-rev is similarly set to the one exhibiting slowest termination at 21 as indicated by the largest i∗
21.
29