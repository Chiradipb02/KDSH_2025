Under review as submission to TMLR
Reward Distance Comparisons Under Transition Sparsity
Anonymous authors
Paper under double-blind review
Abstract
Reward comparisons are vital for evaluating differences in agent behaviors induced by a
set of reward functions. Most conventional techniques employ optimized policies to derive
these behaviors; however, learning these policies can be computationally expensive and sus-
ceptible to safety concerns. Direct reward comparison techniques obviate policy learning
but suffer from transition sparsity, where only a small subset of transitions are sampled
due to data collection challenges and feasibility constraints. Existing state-of-the-art direct
reward comparison methods are ill-suited for these sparse conditions since they require high
transition coverage, where the majority of transitions from a given coverage distribution are
sampled. When this requirement is not satisfied, a distribution mismatch between sampled
and expected transitions can occur, which can introduce significant errors. This paper intro-
duces the Sparsity Resilient Reward Distance (SRRD) pseudometric, designed to eliminate
the need for high transition coverage by accommodating diverse sample distributions, which
are common under transition sparsity. We provide theoretical justifications for SRRD’s ro-
bustness and conduct empirical studies to demonstrate its practical efficacy across various
domains, namely Gridworld, Bouncing Balls, Drone Combat, and StarCraft II.
1 Introduction
In sequential decision problems, reward functions often serve as the most “succinct, robust, and transferable”
representationsofatask(Ng&Russell,2000),encapsulatingagentgoals,socialnorms,andintelligence(Silver
et al., 2021; Zahavy et al., 2021; Singh et al., 2009). For problems where a reward function is specified and
the goal is to find an optimal policy that maximizes cumulative rewards, Reinforcement Learning (RL) is
predominantly employed (Sutton & Barto, 2018). Conversely, when a reward function is complex or difficult
to specify, and past expert demonstrations (or policies) are available, the reward function can be learned via
Inverse Reinforcement Learning (IRL) (Ng & Russell, 2000).
In both RL and IRL contexts, reward functions govern agent decision-making, and reward comparisons can
help assess the similarity of these functions in terms of the behaviors that they induce. These comparisons
could be useful for: (1) Evaluating Agent Behaviors – By comparing how different reward functions align or
differ using specified similarity measures, agent rewards can be grouped through clustering or categorized via
classification, to reason and interpret the agents’ behaviors. This can be useful in IRL domains, where there
is need to extract meaning from inferred rewards computed to represent agent preferences and motivations
(Ng & Russell, 2000). For instance, in sport domains such as hockey, reward comparisons could be useful in
inferring player rankings and their decision-making strategies (Luo et al., 2020). (2) Initial Reward Screening
– In RL domains, direct reward comparisons (without computing policies) could serve as a preliminary step
to quickly identify rewards that will achieve a spectrum of desired behaviors before actual training. This
could be beneficial in scenarios where multiple possible reward configurations exist, but some might be more
efficient. For example, it might be important to distinguish rewards that support defensive versus offensive
strategies in military scenarios (Van Evera, 1998), or competitive versus cooperative behaviors in team
settings (Santos & Nyanhongo, 2019). (3) Addressing Reward Sparsity1– Reward comparisons could also
tackle issues such as reward sparsity, by identifying more informative, and easier-to-learn reward functions,
that might be similar in terms of optimal policies but more desirable than sparse reward functions.
1Transition sparsity arises when a minority of transitions are sampled. This is different from reward sparsity, which occurs
when rewards are infrequent or sparse, making RL tasks difficult.
1Under review as submission to TMLR
Figure 1: (Transition Sparsity in a 10×10Gridworld Domain) In this illustration, each transition starts
from a starting state sand ends in a destination state s′. For clarity in visualization, we consider action-
independent rewards such that actions can be omitted, R(s,s′). In (a), high transition coverage results
from a high rollout count (number of policy rollouts) in the absence of feasibility constraints, leading to the
majority of transitions being sampled (blue points). In ( b), low coverage results from a low rollout count in
the absence of feasibility constraints, leading to fewer sampled transitions (red points). In ( c), low coverage
results from feasibility constraints, such as movement restrictions that only allow actions to adjacent cells,
which can significantly reduce the space of sampled transitions (green points) irrespective of rollout count.
The task of reward comparisons aims to assess the similarity among a collection of reward functions. This
can be done through pairwise comparisons where the similarity distance D(RA,RB)between two reward
functions,RAandRB(vectors, not scalars), is computed. The similarity distance should reflect variations
not only in magnitude but also in the preferences and behaviors induced by the reward functions. This is
characterized by the property of policy invariance, which ensures that reward functions yielding the same op-
timal policies are considered similar even if their numerical reward values differ (Ng et al., 1999). This makes
direct reward comparisons via distance measures such as Euclidean or Kullback-Leibler (KL) divergence un-
favorable since these distances do not maintain policy invariance. To satisfy policy invariance, traditional
reward comparison techniques have adopted indirect approaches, which compare behaviors derived from
optimized policies generated from the reward functions under comparison (Arora & Doshi, 2021). However,
these indirect approaches pose the following challenges: (1) they can be slow and resource-intensive due to
iterative policy learning via RL, and (2) policy learning may not be favorable in critical online environments
such as healthcare or autonomous vehicles, where safety considerations are paramount (Amodei et al., 2016;
Thomas et al., 2021). Therefore, developing direct reward comparison methods that bypass the computa-
tionally expensive process of policy learning, while maintaining policy invariance is highly important.
To achieve policy invariance in direct reward comparisons, Gleave et al. (2020) introduced the Equivalent
Policy Invariant Comparison (EPIC) pseudometric. Given the task to compare two reward functions, EPIC
first performs reward canonicalization to express rewards in a standardized form by removing shaping, and
then computes the Pearson distance to measure the difference between the canonical reward functions.
Although theoretically rigorous, EPIC falls short in practice since it is designed to compare reward functions
underhightransitioncoverage,whenthemajorityoftransitionswithintheexploredstateandactionspaceare
sampled. In many practical scenarios, achieving high transition coverage can be impractical due to transition
sparsity, a condition where a minority of transitions are sampled. The remaining unsampled transitions may
have unknown or undefined reward values (especially if unrealizable), which can distort the computation of
rewardexpectationsduringcanonicalization. Transitionsparsitycanbeattributedto: (1) limitedsampling
- when data collection challenges result in fewer sampled transitions; and (2) feasibility constraints - when
environmental or agent-specific limitations restrict certain transitions. Consider, for instance, a standard
10×10Gridworld domain which has a total of 100states, each represented by an (x,y)coordinate. The total
number of possible transitions is at least 10,000(100states×number of actions ×100states) if at least one
action exists between every pair or states (see Figure 1). However, feasibility constraints such as movement
2Under review as submission to TMLR
restrictions might significantly limit the transitions that can be sampled. For example, an agent that can
only move to its neighboring states by taking single-step cardinal directions (movement actions: up, right,
down, left) in each state will explore fewer than 400transitions2, as shown in Figure 1c. To illustrate the
impact of limited sampling, consider a scenario where transitions are sampled via policy rollouts (trajectory
simulations from a given policy). Assuming that factors such as feasibility constraints, the transition model,
and the policy rollout method are kept constant, the extent of sampled transitions is directly influenced by
the number of policy rollouts. When the number of policy rollouts is low, fewer transitions are likely to be
sampled; and conversely, when the number of rollouts is high, a greater proportion of transitions are likely
to be sampled (see Figure 1a and 1b).
Contributions In this paper, we introduce the Sparsity Resilient Reward Distance (SRRD) pseudometric,
designed to improve direct reward comparisons in environments characterized by high transition sparsity.
SRRD demonstrates greater robustness compared to existing pseudometrics (such as EPIC), which require
high transition coverage. SRRD’s strength lies in its ability to integrate reward samples with diverse transi-
tion distributions, which are common in scenarios with low coverage. We provide a theoretical justification
for SRRD’s robustness and demonstrate its superiority through experiments in four domains of varying
complexity: Gridworld, Bouncing Balls, Drone Combat, and a StarCraft II environment. For the simpler
domains, Gridworld and Bouncing Balls, we evaluate SRRD against manually defined factors such as non-
linear reward functions and feasibility constraints, to fully understand its strengths and limitations under
controlled conditions. In the more complex domains, StarCraft II and Drone Combat, we assess SRRD in
battlefield scenarios characterized by large state and action spaces, to gauge how it is likely to perform in
realistic settings. Our final experiment explores a novel and practical application of these pseudometrics
as distance measures within a k-nearest neighbors algorithm, tailored to classify agent behaviors based on
reward functions computed via IRL. Empirical results highlight SRRD’s superior performance, as evidenced
by its ability to find higher similarity between rewards generated from the same agents and higher variation
between rewards from different agents. These results underscore the crucial need to account for transition
sparsity in direct reward comparisons.
2 Related Works
TheEPICpseudometricisthefirstdirectrewardcomparisontechniquethatcircumventspolicylearningwhile
maintaining policy invariance (Gleave et al., 2020). In practical settings, EPIC’s major limitation is that
it is designed to compare rewards under high transition coverage. In scenarios characterized by transition
sparsity, EPIC underperforms due to its high sensitivity to unsampled transitions, which can distort the
computation of reliable reward expectation estimates required during the canonicalization process. This
limitation has been observed by Wulfe et al. (2022), who introduced the Dynamics-Aware Reward Distance
(DARD)pseudometric. DARDimprovesonEPICbyrelyingontransitionsthatareclosertobeingphysically
realizable; however, it still remains highly sensitive to unsampled transitions.
Skalse et al. (2024) also introduced a family of reward comparison pseudometrics, known as Standardized
Reward Comparisons (STARC). These pseudometrics are shown to induce lower and upper bounds on worst-
case regret, implying that the metrics are tight, and differences in STARC distances between two reward
functions correspond to differences in agent behaviors. Among the different STARC metrics explored, the
Value-Adjusted Levelling (VAL) and the VALPotential functions are empirically shown to have a marginally
tighter correlation with worst-case regret compared to both EPIC and DARD. While an improvement,
a significant limitation of these metrics is their reliance on value functions, which can be computed via
policy evaluation—a process that incurs a substantially higher computational overhead than sample-based
approximations for both EPIC and DARD. In small environments, these metrics can be computed using
dynamic programming for policy evaluation, an iterative process with polynomial complexity relative to the
state and action spaces (Skalse et al., 2024). In larger environments, computing the exact value functions
becomes impractical hence the value functions need to be approximated via neural networks updated with
2Under the movement restriction, from each given state an agent can only transition to a maximum of four other states,
hence, the total number of transitions is less than 100states ×4 actions = 400transitions. The exact number is 358due to
boundary restrictions such as the inability to transition from (0,0)to(−1,0).
3Under review as submission to TMLR
Bellman updates (Skalse et al., 2024). Since the primary motivation for direct reward comparisons is to
eliminatethecomputationallyexpensiveprocessofpolicylearning, incorporatingvaluefunctionsissomewhat
contradictory since policy evaluation is iterative, and it can have comparable complexity with policy learning
techniques such as value iteration. Our work focuses on computationally scalable direct reward comparison
pseudometrics (such as EPIC and DARD), which do not involve iterative policy learning or evaluation.
The task of reward comparisons lies within the broader theme of reward evaluations, which aim to explain
or interpret the relationship between rewards and agent behavior. Some notable works tackling this theme,
include, Lambert et al. (2024), who developed benchmarks to evaluate reward models in Large Language
Models (LLMs), which are often trained using RL via human feedback (RLHF), to align the rewards with
human values. These benchmarks assess criteria such as communication, safety and reasoning capabilities
across a variety of reward models. In another line of work, Mahmud et al. (2023) presented a framework
leveraging human explanations to evaluate and realign rewards for agents trained via IRL on limited data.
Lastly, Russell & Santos (2019) proposed a method that examines the consistency between global and
local explanations, to determine the extent to which a reward model can capture complex agent behavior.
Similar to reward comparisons, reward evaluations can be influenced by shaping functions, thus necessitating
techniques such as canonicalization as preprocessing steps to eliminate shaping (Jenner & Gleave, 2022).
Reward shaping is a technique that transforms a base reward function into alternate forms (Ng et al., 1999).
This technique is mainly employed in RL for reward design where heuristics and domain knowledge are
integrated to accelerate learning (Mataric, 1994; Hu et al., 2020; Cheng et al., 2021; Gupta et al., 2022; Suay
et al., 2016). Several applications of reward shaping have been explored, and some notable examples include:
training autonomous robots for navigation (Tenorio-Gonzalez et al., 2010); training agents to ride bicycles
(Randløv & Alstrøm, 1998); improving agent behavior in multiagent contexts such as the Prisoner’s Dilemma
(Babes et al., 2008); and scaling RL algorithms in complex games (Lample & Chaplot, 2017; Christiano
et al., 2017). Among several reward shaping techniques, potential-based shaping is the most popular due to
its preservation of policy invariance, ensuring that the set of optimal policies remains unchanged between
different versions of reward functions (Ng et al., 1999; Wiewiora et al., 2003; Gao & Toni, 2015).
3 Preliminaries
This section provides the foundational concepts needed to understand the task of direct reward comparisons.
We begin by introducing the Markov Decision Process formalism, followed by a description of reward samples
andtheconceptofpolicyinvariance. Finally, wereviewtherelevantdirectrewardcomparisonpseudometrics,
along with their limitations, which SRRD aims to address.
3.1 Markov Decision Processes
A Markov Decision Process (MDP) is defined as a tuple (S,A,γ,T,R ), whereSandAare the state and
action spaces, respectively. The transition model T:S×A×S→ [0,1], dictates the probability distribution
of moving from one state, s∈S, to another state, s′∈S, under an action a∈A, and each given transition
is specified by the tuple (s,a,s′). The discount factor γ∈[0,1]reflects the preference for immediate over
future rewards. The reward function is denoted by R:S×A×S→ R, and for each transition, the reward
is represented as R(s,a,s′). A trajectory τ={(s0,a0),(s1,a1),···,(sn)},n∈Z+, is a sequence of states
and actions, with a total return: g(τ) =/summationtext∞
t=0γtR(st,at,st+1). The goal in an MDP is to find a policy
π:S×A→ [0,1](often via RL) that maximizes the expected return E[g(τ)].
Given the subsets Si⊆S,A⊆A, andSj⊆S, the tuple (Si,A,Sj)represents the set of transitions within
the cross-product Si×A×Sj. We define the set of rewards associated with these transitions as:
R(Si,A,Sj) ={R(s,a,s′)|(s,a,s′)∈Si×A×Sj}.
The expected reward over transitions in the cross-product Si×A×Sjis denoted by E[R(Si,A,Sj)].
4Under review as submission to TMLR
3.2 Reward Samples
In the standard MDP formulation, the reward function Ris fully specified for all possible transitions in-
cluding those that are unrealizable. However, in many practical scenarios, we may only have access to a
reward sample that is defined over a subset of transitions—typically realizable or have been observed. This
situation commonly arises in offline RL settings, where we might be restricted to datasets of reward samples
instead of the full reward function due to data collection challenges, feasibility constraints, privacy and safety
considerations in domains such as healthcare (Levine et al., 2020; Agarwal et al., 2020; Chen et al., 2024). In
this paper, we define a reward sample as a restriction of Rto a subset B⊆S×A×S , where,Bconsists
of sampled transitions under a specified policy. We assume that rewards are defined for transitions in B,
and are undefined or unknown for transitions not in B, since they are unsampled.
Given a batch of sampled transitions B, the coverage distribution D(s,a,s′)is defined as the probability
distribution over transitions used to generate the transition samples. The sampled state space and action
space are denoted by SD⊆SandAD⊆A, respectively. The sets of all possible distributions over AandS
are denoted by ∆Aand∆Srespectively, and the individual distributions over states and actions are denoted
byDS∈∆SandDA∈∆A, respectively.
3.3 Policy Invariance
Policy invariance is a condition where an optimal policy remains unchanged when a reward function is
modified typically through shaping (Ng et al., 1999; Jenner et al., 2022). In reward comparisons, policy
invariance is a key property to satisfy, since it ensures that equivalent rewards will yield the same optimal
policies (Gleave et al., 2020). Formally, any shaped reward can be represented by the additive relationship:
R′(s,a,s′) =R(s,a,s′) +F(s,a,s′), whereF(s,a,s′)is a shaping function. Potential shaping guarantees
policy invariance, and takes the form:
R′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s), (1)
whereRis the original reward function, and ϕis a state-potential function. Reward functions RandR′are
deemed equivalent since they yield the same optimal policies. To effectively compare reward functions that
may differ in numerical values but are equivalent in the optimal policies the induce, the use of pseudometrics
is highly important. Let Xbe a set, with x,y,z∈X, and letd:X×X→[0,∞)define a pseudometric.
This pseudometric adheres to the following axioms: (premetric) d(x,x) = 0for allx∈X; (symmetry)
d(x,y) =d(y,x)for allx,y∈X; and (triangular inequality) d(x,y)≤d(x,z) +d(z,y)for allx,y,z∈X.
Unlike a true metric, a pseudometric does not require that: d(x,y) = 0 =⇒x=y, making it ideal for
identifying equivalent reward functions that might have different numerical values.
3.4 Equivalent Policy Invariant Comparison (EPIC)
The EPIC pseudometric directly compares reward functions without computing policies, while maintaining
policy invariance (Gleave et al., 2020). EPIC’s reward comparison process involves two steps: first, reward
functionsarecanonicalizedintoastandardformwithoutshaping; andsecond, aPearsondistanceiscomputed
to differentiate the canonical rewards. The following definitions from Gleave et al. (2020) describe EPIC.
Definition 1. (Canonically Shaped Reward) Let R:S×A×S→ Rbe a reward function. Given distributions
DS∈∆SandDA∈∆Aover states and actions respectively, let SandS′be random variables distributed
asDSandAbe distributed asDA. The canonically shaped reward is:
CEPIC (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′)−R(s,A,S′)−γR(S,A,S′)]. (2)
Canonicalization expresses rewards in form free of shaping. Given a potentially shaped reward, R′(s,a,s′) =
R(s,a,s′) +γϕ(s′)−ϕ(s), canonicalization yields: CEPIC (R′)(s,a,s′) =CEPIC (R)(s,a,s′) +ϕres, where
ϕres=γE[ϕ(S)]−γE[ϕ(S′)]is the remaining residual shaping. EPIC assumes that SandS′are identically
distributed such that E[ϕ(S)] =E[ϕ(S′)]which results in ϕres= 0, leading to Proposition 1:
5Under review as submission to TMLR
Proposition 1. (The Canonically Shaped Reward is Invariant to Shaping) Let R:S×A×S→ Rbe a
reward function, ϕ:S→Ra state potential function, and R′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s)be the
shaped reward. Then: CEPIC (R) =CEPIC (R′).
Proposition 1 implies that the canonicalized reward functions RandR′are identical, allowing for comparison
in a standardized form without shaping. Finally, the EPIC pseudometric between two reward functions RA
andRBis computed as:
DEPIC(RA,RB) =Dρ(CEPIC (RA),CEPIC (RB)), (3)
where, for any random variables XandY, the Pearson distance, Dρ, is defined as:
Dρ(X,Y ) =/radicalbig
1−ρ(X,Y )/√
2. (4)
The Pearson correlation coefficient ρ(X,Y )is given by:
ρ(X,Y ) =E[(X−µX)(Y−µY)]
σXσY, (5)
whereµdenotes the mean, σthe standard deviation, and E[(X−µX)(Y−µY)] is the covariance between
XandY. The Pearson distance is defined over the range: 0≤Dρ(X,Y )≤1, whereDρ(X,Y ) = 0indicates
thatXandYare highly similar since ρ(X,Y ) = 1(perfect positive correlation), and Dρ(X,Y ) = 1indicates
thatXandYare maximally different since ρ(X,Y ) =−1. The Pearson distance is scale and shift invariant
sinceDρ(aX+c,bY +d) =Dρ(X,Y ), where,a,b,c,d∈Rare constants (Gleave et al., 2020). Therefore,
the EPIC pseudometric is scale, shift and shaping invariant, which are policy-preserving transformations.
Computing CEPICrequires access to all transitions in a reward function, making it feasible only for small en-
vironments. For reward functions with large or infinite state and action spaces, computing CEPIC(Equation
2) becomes nearly impractical since reward values are needed for all transitions. To address this challenge,
Gleave et al. (2020) introduced the sample-based EPIC approximation, denoted by ˆCEPIC, as follows:
Definition 2. (Sample-based EPIC) Given a batch BVofNVsamples from the coverage distribution D, and
a batchBMofNMsamples from the joint state and action distributions, DS×DA. For each (s,a,s′)∈BV,
the canonically shaped reward is approximated by taking the mean over BM:
ˆCEPIC (R)(s,a,s′) =R(s,a,s′) +γ
NM/summationdisplay
(x,u)∈BMR(s′,u,x)−1
NM/summationdisplay
(x,u)∈BMR(s,u,x )
−γ
N2
M/summationdisplay
(x,·)∈BM/summationdisplay
(x′,u)∈BMR(x,u,x′)(6)
Each term in ˆCEPICapproximates the corresponding expectation term in CEPIC(Equation 2). For example,
γ
NM/summationtext
(x,u)∈BMR(s′,u,x)estimates E[γR(s′,A,S′)], and−γ
N2
M/summationtext
(x,·)∈BM/summationtext
(x′,u)∈BMR(x,u,x′)estimates
−E[γR(S,A,S′)]. The approximation was primarily developed to achieve scalable computations without
transition sparsity considerations. In practical settings, ˆCEPIChas generally been applied in scenarios where
the full reward function is available, to retrieve reward values for all necessary transitions, irrespective of
their realizability. ˆCEPICuses two types of samples: BV, a collection of transitions sampled from a specified
policy; and BM, a collection of state-action pairs sampled from the joint state and action distributions.
Each transition in BVis canonicalized using state-action pairs in BM. Depending on how BMis sampled
(usually uniformly), it can help in reducing bias from the sampling policy used to generate BV, by including
state-actionpairsthatmaynotbepresentin BV; therebyprovidingabetterrepresentationoftheentirestate-
action space. As a result, BMcan help to provide broader coverage, and reduce bias and variance in results,
using relatively fewer state-action pairs compared to enumerating all possible combinations. Moreover, the
assumption that states in SandS′are identically distributed is satisfied since the transitions in the double
summation term,/summationtext
(x,·)∈BM/summationtext
(x′,u)∈BMR(x,u,x′), are sampled from the cross-product of state-action pairs
inBM.ˆCEPICgenerally works well when the full reward function is accessible, ensuring that each transition
fromBVis canonicalized with sufficient coverage dictated by BM. However, when the full reward function
is unavailable, and only a reward sample is available, some transitions that are needed in the approximation
might have undefined reward values which can degrade performance.
6Under review as submission to TMLR
3.5 Dynamics Aware Reward Distance (DARD)
The Dynamics Aware Reward Distance (DARD) pseudometric was introduced by Wulfe et al. (2022) to
address thechallenge of unrealizabletransitions in EPIC. They observed that EPIC oftenrelies on transitions
that are physically unrealizable, which can introduce errors in reward comparisons since rewards for these
transitionsareoftenarbitraryandunreliable. Tomitigatethischallenge, theyincorporatedtransitionmodels
to prioritize physically realizable transitions. The DARD pseudometric is given by:
CDARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′′)−R(s,A,S′)−γR(S′,A,S′′)], (7)
whereA∼ DA,S′∼T(s,A),S′′∼T(s′,A), andTis the transition model. DARD is invariant to
potential shaping and generally improves upon EPIC by distinguishing between the subsequent states to s
(denoted by S′) ands′(denoted by S′′). Transitions (s,A,S′)and(s′,A,S′′)are generally in-distribution
with respect to the transition dynamics TsinceS′is distributed conditionally based on (s,A), andS′′is
distributed conditionally based on (s′,A). Therefore, these transitions naturally align with the dynamics of
the sampled transitions, resulting in a lower likelihood of being unrealizable. However, transitions ( S′,A,S′′)
are more likely to be out-of-distribution with respect to the transition dynamics T, sinceS′′is not distributed
conditionally according to (S′,A), but rather to (s′,A). Consequently, DARD can be sensitive to out-of-
distribution transitions in (S′,A,S′′), which have a higher likelihood of being unrealizable. Nonetheless,
Wulfe et al. (2022) argue that these transitions are closer to being physically realizable since ( S′,A,S′′)
transitions are in close proximity to sands′, compared to transitions that are utilized in EPIC. The sample-
based DARD approximation, denoted by ˆCDARD, is computed as follows:
ˆCDARD (R)(s,a,s′) =R(s,a,s′) +γ
NANTNA/summationdisplay
i=0NT/summationdisplay
j=0R(s′,ui,x′′
j)−1
NANTNA/summationdisplay
i=0NT/summationdisplay
j=0R(s,ui,x′
j)
−γ
N2
AN2
TNA/summationdisplay
i=0NT/summationdisplay
j=0NA/summationdisplay
k=0NT/summationdisplay
l=0R(x′
ij,ukl,x′′
kl). (8)
Each term in ˆCDARDapproximates the corresponding expectation term in CDARD(Equation 7). For exam-
ple,γ
NANT/summationtextNA
i=0/summationtextNT
j=0R(s′,ui,x′′
j)estimates E[γR(s′,A,S′′)]. Transitions are sampled into a batch BVfrom
the coverage distribution, D. To compute E[R(s,A,S′)]andE[R(s′,A,S′′)],NAactions are sampled using
a specified policy (for example, a uniform policy), and NTsamples of S′orS′′are sampled conditionally
based onsors′. For the last expectation, the cross-product of all transitions between S′andS′′are used.
While ˆCDARDalleviates feasibility challenges by incorporating transition models, in practical settings, it has
generally been applied in problems where the full reward models are accessible. In this paper, we consider
scenarios where reward samples are available instead of the full reward functions.
3.6 Unsampled Transitions
Consider a reward function R:S×A×S→ R, whereSis the state space and Ais the action space. A
reward sample is generated according to a coverage distribution D, and it spans a state space SD⊆Sand
an action space AD⊆A. We define the following sets of transitions:
Full Coverage Transitions ( TD)- The set of all theoretically possible transitions within the reward
sample’s state-action space. This set is represented as TD=SD×AD×SD⊆S×A×S .
Sampled Transitions (TS) - The set of transitions that are actually present in the reward sample. Due to
feasibility constraints and limited sampling, this set is a subset of the full coverage transitions: TS⊆TD.
Unsampled Transitions (TU) - The set of full coverage transitions that are not explored in the reward
sample. These transitions can be both realizable and unrealizable, and TU=TD\TS.
A major limitation of pseudometrics such as EPIC (and to some extent, DARD), is that they are designed
to compare reward functions under high coverage, where |TS|≈|TD|. As|TU|→|TD|, the performance
of these pseudometrics significantly degrades due to the increasing number of unsampled transitions. To
7Under review as submission to TMLR
Figure 2: (Impact of unsampled transitions on canonicalizing R(s1,a1,s2)) Sampled transitions are those
explored in the reward sample, while expected transitions are those anticipated by ˆCEPICassuming full
coverage. As coverage decreases from ( a) to (c), due to a reduction in the number of sampled transitions, the
standard deviation of ˆCEPIC (R)(s1,a,s 2)increases, indicating ˆCEPIC’s increased instability to unsampled
transitions. For comparison, ˆCSRRDand ˆCDARDhave lower standard deviations, signifying higher stability.
illustrate this limitation, consider Equation 6 used to approximate CEPIC. To perform the computation,
we need to estimate: E[R(s′,A,S′)]by dividing the sum of rewards from s′toS′byNMtransitions;
E[R(s,A,S′)]by dividing the sum of rewards from stoS′byNMtransitions; and E[R(S,A,S′)]by dividing
the sum of all rewards from StoS′byNM2transitions, where NM≤|SD×AD|is the size of the state-
action pairs in the batch BM. Every state s∈Sis ideally expected to have NMtransitions to all other
statesS′, which can be impractical under transition sparsity when some transitions might be unsampled.
Since reward summations are divided by large denominators due to NM(see Equation 6), when coverage is
low, the number of sampled transitions needed to estimate the reward expectation terms will be fewer than
expected, introducing significant error.
Figure 2 illustrates an example showing the effect of unsampled transitions on canonicalization across three
reward samples spanning a state space S=S′={s1,...,s 8}and an action space, A={a1}, such that
NM= 8, under different levels of transition sparsity. Rewards are defined as R(si,a1,sj) = 1+γϕ(sj)−ϕ(si),
wherei,j∈{1,...,8}, and state potentials are randomly generated such that: |ϕ(s)|≤20, withγ= 0.5.
The task is to compute ˆCEPIC (R)(s1,a1,s2)over 1000simulations. For all reward samples, the mean
µ(ˆCEPIC (R)(s1,a1,s2))≈0, but the standard deviation σ(ˆCEPIC (R)(s1,a1,s2))varies based on coverage.
In Figure 2a, the reward sample has high coverage ( 100%), hence, the number of observed and expected
transitionsareequal. Inthisscenario, EPICishighlyeffectiveandallshapedrewardsaremappedtothesame
value (≈0), resulting in a standard deviation σ(ˆCEPIC (R)(s1,a1,s2)) = 0, highlighting consistent reward
canonicalization. In Figure 2b, the reward sample has moderate coverage and the fraction of unsampled
transitions is approximately 33%. As a result, σ(ˆCEPIC (R)(s1,a1,s2)) = 2.84, which is relatively high,
signifying EPIC’s sensitivity to unsampled transitions. In Figure 2c, the reward sample exhibits low coverage
and the fraction of unsampled transitions is approximately 77%, indicating a significant discrepancy between
thenumberofobservedandexpectedtransitions. Consequently, σ(ˆCEPIC (R)(s1,a1,s2)) = 7.36,highlighting
EPIC’s increased instability due to unsampled transitions. For comparison, we include the DARD and SRRD
estimates, which exhibit lower standard deviations, signifying greater stability. DARD reduces the effect of
unsampled transitions by relying mostly on transitions that are closer to sands′(S′andS′′), which typically
8Under review as submission to TMLR
comprise a smaller subset of states compared to those required by EPIC. However, this localized focus can
make DARD highly sensitive to variations in the composition of transitions between the reward samples
under comparison, since it might lack the context of transitions further from sands′, potentially limiting
its robustness. In this paper, we use DARD as an experimental baseline.
4 Approach: Sparsity Resilient Reward Distance (SRRD)
The motivation for SRRD is to develop a direct reward comparison technique that imposes minimal assump-
tionsaboutthestructureanddistributionoftransitionsinrewardsamples, therebyensuringrobustnessunder
transition sparsity. To derive SRRD, we integrate key characteristics of CDARDandCEPICas follows:
•CDARDeliminates the requirement that the set of states that can be reached from sands′must be
similar, thereby increasing the flexibility of transition sample distributions considered. We will refer
to states that can be reached from s′asS1, and states from sasS2.
•InCDARD,thetransitions (S′,A,S′′)aregenerallyincloseproximityto sands′becauseS′∼T(s,A)
andS′′∼T(s′,A). These transitions may not capture the states that are further away from s′and
s′, potentially lacking the overall context of the of the reward sample. To address this issue, similar
to howCEPICuses transitions for the entire sample, (S,A,S′), we utilize transitions for the entire
sample, which we denote as: (S3,A,S 4), whereS3encompasses all initial states from the sampled
transitions, and S4is the set of all subsequent states to S3.
These modifications reduce the impact of unsampled transitions, as reward expectations are computed based
on the observed structure of the sampled transitions, without relying on full coverage. With these consider-
ations, we derive the modified canonical equation as follows:
C1(R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)], (9)
where:S1andS2are subsequent states to s′ands, respectively; S3encompasses all initial states from
all sampled transitions; and S4are subsequent states to S3. Applying C1to a potentially shaped reward
R′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s), we get:
C1(R′)(s,a,s′) =C1(R)(s,a,s′) +ϕres1,
where,
ϕres1=E[γ2ϕ(S1)−γ2ϕ(S4) +γϕ(S3)−γϕ(S2)]. (10)
C1is not theoretically robust since it yields the residual shaping3ϕres1. To cancel E[ϕ(Si)],∀i∈{1,...,4}
inϕres1, we can add rewards R(Si,A,ki)to induce potentials γϕ(ki)−ϕ(Si); wherekican be any arbitrary
set of states. This results in the equation:
C2(R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,k 1)−γR(S2,A,k 2) +γR(S3,A,k 3)−γ2R(S4,A,k 4)].(11)
ApplyingC2to a potentially shaped reward R′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s), we get:
C2(R′)(s,a,s′) =C2(R)(s,a,s′) +ϕres2,
where,
ϕres2=E[γ3ϕ(k1)−γ3ϕ(k4) +γ2ϕ(k3)−γ2ϕ(k2)]. (12)
See Appendix A.2 for derivations of ϕres1andϕres2.
The canonical form C2is preferable to C1, since it enables the selection of kito eradicate ϕres2. A convenient
solution is to ensure that: k1=k4andk2=k3such that E[ϕ(k1)] =E[ϕ(k4)]andE[ϕ(k2)] =E[ϕ(k3)]which
results inϕres2= 0. We choose the solution: k1=k4=S5, andk2=k3=S6; whereS5are subsequent
states toS1, andS6are subsequent states to S2. This leads to the following SRRD definition:
3Residual shaping is the remaining shaping after canonicalization.
9Under review as submission to TMLR
Definition3 (SparsityResilientCanonicallyShapedReward) .LetR:S×A×S→ Rbe a reward function.
Given distributions DS∈∆SandDA∈∆Aover states and actions, let S3be the set of states sampled
according toDS, and letAbe the set of actions sampled according to DA. Furthermore, let T(S4|S3,A)be
a transition model governing the conditional distribution over next states, where, S4are subsequent states
toS3. For each s∈S3, ands′∈S4, letS1be the set of states sampled according to T(S1|s′,A). Let ˜S2be
the set of states sampled according to T(˜S2|s,A), and letS2represent non-terminal states in ˜S2. Similarly,
letS5andS6be set of states sampled according to T(S5|S1,A)andT(S6|S2,A), respectively. The Sparsity
Resilient Canonically Shaped Reward is defined as:
CSRRD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,S 5)−γR(S2,A,S 6) +γR(S3,A,S 6)−γ2R(S4,A,S 5)].(13)
Note that in CSRRD, we first sample ˜S2as subsequent states from s, and then derive S2as non-terminal
states in ˜S2. This modification ensures that (S2,A,S 6)⊆(S3,A,S 6), which is crucial for SRRD’s robustness
in Theorem 2. In practice though, the difference between ˆS2andS2is generally minimal, especially in
long-horizon problems where terminal states tend to be fewer compared to non-terminal states. The SRRD
canonicalization is invariant to potential shaping as described by Proposition 2.
Proposition 2. (The Sparsity Resilient Canonically Shaped Reward is Invariant to Shaping) Let R:
S×A×S → Rbe a reward function and ϕ:S →Rbe a state potential function. Applying CSRRDto
a potentially shaped reward R′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s)satisfies:CSRRD (R) =CSRRD (R′).
Proof.See Appendix A.3.
Given reward functions RAandRB, the SRRD pseudometric is computed as:
DSRRD (RA,RB) =Dρ(CSRRD (RA),CSRRD (RB)), (14)
whereDρis the Pearson distance (see Equation 4). The pseudometric DSRRDranges from 0whenRAis
equivalent to RB, to 1, indicating the maximal difference between RAandRB. We establish an upper
bound on regret showing that as DSRRD→0, the performance difference between the policies induced by
the reward functions under comparison, RAandRB, approaches 0. This is formalized as follows:
Theorem 1. LetRA,RB:S×A×S→ Rbe reward functions with respective optimal policies, π∗
A,π∗
B. Let
γbe a discount factor, Dπ(t,st,at,st+1)be the distribution over transitions S×A×S induced by policy πat
timet, andD(s,a,s′)be the coverage distribution used to compute DSRRD. Suppose there exists K > 0such
thatKD(st,at,st+1)≥Dπ(t,st,at,st+1)for all times t∈N, triples (st,at,st+1)∈S×A×S and policies
π∈{π∗
A,π∗
B}. Then the regret under RAfrom executing π∗
Binstead ofπ∗
Ais at most:
GRA(π∗
A)−GRA(π∗
B)≤32K∥RA∥2(1−γ)−1DSRRD (RA,RB),
whereGR(π)is the return of policy πunderR.
Proof.See Appendix A.7.
Depending on the transition model which dictates the composition of {S1,...,S 6},CSRRDcan be invariant
to shaping across various transition distributions without requiring full coverage, provided that, for each set
of transitions (Si,A,Sj)in the reward expectation terms, all transitions in the cross-product Si×A×Sjhave
defined reward values. This is guaranteed when the full reward function Ris available. In rare instances,
CSRRDcan be equivalent to CEPICandCDARDunder full coverage, as described below:
Proposition3. Given the state subset S⊆S, the SRRD, DARD, and EPIC canonical rewards are equivalent
under full coverage when: S=S1=...=S6for SRRD; S=S′=S′′for DARD, and S=S′for EPIC.
Proof.See Appendix A.5.
10Under review as submission to TMLR
In many practical scenarios, the full reward function may be unavailable, and CSRRDcan be approximated
fromrewardsamples, resultingin ˆCSRRD. Inthesesettings, thetransitionsneededineachrewardexpectation
term of ˆCSRRD, might be unsampled due to transition sparsity. Despite this challenge, approximations for
ˆCSRRDare robust due to the strategic choices of ki(see Equation 11 and 13), S5andS6, which ensure
that the approximations are ideal for the following two reasons: First, for any reward sample, we can
compute reliable expectation estimates for the first six terms, since for each set of transitions (Si,A,Sj),
Sjis distributed conditionally based on (Si,A); hence, these transitions naturally align with the transition
dynamics that dictate the nature of the reward sample. However, for transitions in the last two terms,
(S3,A,S 6)and(S4,A,S 5),S6is not distributed conditionally based on (S3,A)but on (S2,A), andS5is not
distributed conditionally based on (S4,A)but on (S1,A); hence these transitions may not align well with the
transition dynamics that dictate the structure of the reward sample, which makes these transitions highly
susceptible to being unsampled. Second, while there might be significant fractions of unsampled transitions
from (S4,A,S 5), and (S3,A,S 6), a minimal set of sampled transitions are likely to exist, because:
•Transitions (S1,A,S 5)⊆(S4,A,S 5):
SinceS1are subsequent states to s′, andS4are subsequent states for all sampled transitions. It
follows that, S1⊆S4(see example in Appendix A.10), hence, (S1,A,S 5)⊆(S4,A,S 5).
•Transitions (S2,A,S 6)⊆(S3,A,S 6).
S2is the set of non-terminal subsequent states from s. SinceS3encompasses all initial non-terminal
states from all sampled transitions, it follows that: S2⊆S3, hence, (S2,A,S 6)⊆(S3,A,S 6).
Therefore, we can get decent fractions of sampled transitions in (S3,A,S 6)and(S4,A,S 5)as well, which
reduces the extent and impact of unsampled transitions (see Section 4.1). To estimate SRRD in reward
samples, we present the sample-based SRRD approximation, denoted by ˆCSRRD, as follows:
Definition 4. (Sample-based SRRD) Given a set of transitions BVfrom a reward sample, let BMbe a batch
ofNMstate-action pairs sampled from the explored states and actions. From BM, derive sets Xi⊆BM, for
i∈{1,...,6}. EachXiis a set,{(x,u)}, wherexis a state and uis an action. The magnitude, |Xi|=Ni. We
defineX3={(x3,u)}, wherex3denotes all initial states for all transitions; X4={(x4,u)}, wherex4denotes
all subsequent states for all transitions. For each (s,a,s′)transition in BV, we define X1={(x1,u)}, where
x1denotes subsequent states for transitions starting from s′;X2={(x2,u)}, wherex2denotes non-terminal
subsequent states for transitions starting from s;X5={(x5,u)}, wherex5denotes subsequent states to X1;
andX6={(x6,u)}, wherex6denotes subsequent states to X2. For each (s,a,s′)inBV:
CSRRD (R)(s,a,s′)≈R(s,a,s′) +γ
N1/summationdisplay
(x1,u)∈X1R(s′,u,x 1)−1
N2/summationdisplay
(x2,u)∈X2R(s,u,x 2)
−γ
N3N4/summationdisplay
(x3,u)∈X3/summationdisplay
(x4,·)∈X4R(x3,u,x 4) +γ2
N1N5/summationdisplay
(x1,u)∈X1/summationdisplay
(x5,·)∈X5R(x1,u,x 5)
−γ
N2N6/summationdisplay
(x2,u)∈X2/summationdisplay
(x6,·)∈X6R(x2,u,x 6) +γ
N3N6/summationdisplay
(x3,u)∈X3/summationdisplay
(x6,·)∈X6R(x3,u,x 6)
−γ2
N4N5/summationdisplay
(x4,u)∈X4/summationdisplay
(x5,·)∈X5R(x4,u,x 5).(15)
The structure of the sample-based approximation is consistent with that of the EPIC and DARD approxi-
mations, to ensure that the approximation also works well in scenarios with very large reward samples4or
when the full reward functions are available. Each term in ˆCSRRDaims to estimate the corresponding ex-
pectation term in CSRRD(Equation 13). For example, −γ
N3N4/summationtext
(x3,u)∈X3/summationtext
(x4,·)∈X4R(x3,u,x 4)estimates
−γE[R(S3,A,S 4)]andγ
N1/summationtext
(x1,u)∈X1R(s′,u,x 1)estimatesγE[R(s′,A,S 1)]. As mentioned in Section 3.4,
4In this paper, we do not assume the availability of the full reward function in reward comparisons. Instead, transitions BV
are directly extracted from a finite reward sample R, which might have unsampled transitions. In situations where the reward
sample is too large, BVmust be sampled from R, similar to the case when full reward functions are available.
11Under review as submission to TMLR
utilizing both BVandBMmight help to provide broader state-action coverage and reduce the bias due to
the sampling policy of BV, which can be highly beneficial, especially in situations where transition sparsity
is not a major concern. However, in environments with high transition sparsity, some transitions resulting
from the combination of BVandBMmight have undefined reward values, as they might be absent from
the reward sample. When computing the reward expectation estimates, these undefined rewards are ignored
in the summation terms, which is equivalent to treating them as zero rewards. When all the necessary
transition terms are present (no unsampled transitions), ˆCSRRDcan satisfy Proposition 4 as follows:
Proposition 4. Given transition sets (Si,A,Sj)needed to compute each reward expectation term in ˆCSRRD,
if the transitions fully cover the cross-product Si×A×Sj, then for a shaped reward R′(s,a,s′) =R(s,a,s′)+
γϕ(s′)−ϕ(s), the sample-based SRRD approximation is invariant to shaping.
Proof.See Appendix A.4.
In summary, SRRD is designed to improve reward function comparisons under transition sparsity. To achieve
this, SRRD mitigates the impact of unsampled transitions, by canonicalizing rewards based on the observed
structure and distribution of sampled transitions. Regarding computational complexity, when employing the
double-batch sampling method that involves a batch BVofNVtransitions, and another batch BMofNM
state-action pairs for reward canonicaliztion. EPIC has the lowest complexity of O(max(NVNM,N2
M)). In
contrast, both DARD and SRRD have complexities of O(NVN2
M)even though SRRD generally has more
computations due to more reward expectation terms (see Appendix A.9). In this paper, we adopt the
sample-based approach that utilizes BVandBMto maintain consistency with approximations employed
in prior works. It is important to recognize that alternative sample-based approximations are also viable.
Specifically, Appendix B.3 explores a sample-based approximation method that employs unbiased estimates,
and Appendix B.4 explores an approximation method that utilizes regression to infer reward values for un-
sampled transitions. Across all these sample-based approximation variants, SRRD consistently outperforms
both DARD and EPIC under conditions of transition sparsity, confirming its robustness. Additionally, Ap-
pendix B.5 analyzes SRRD’s sensitivity to variations in the sampling policy, and Appendix A.8 presents a
generalized formula for potential SRRD extensions.
4.1 Relative Shaping Errors
This section provides a theoretical evaluation of the robustness of the sample-based approximation: ˆCSRRD,
ˆCDARD, and ˆCEPIC, to unsampled transitions. This task is inherently challenging due to the wide vari-
ability in the structure of reward samples, making it difficult to precisely assess the residual shaping effects
introduced by each unsampled transition. We first discuss the relevant definitions and assumptions for the
analysis, and then present Theorem 2, comparing the three methods. For a reward function R, the structure
of an arbitrary reward canonicalization method (such as CSRRD,CDARD, andCEPIC) takes the form:
CS(R)(s,a,s′) =R(s,a,s′) +En−1/summationdisplay
i=1[αiR(Si,A,S′
i)], (16)
where,αiis a constant,|αi|≤1, andi∈{1,...,n−1}are indices to denote the state subsets, Si,S′
i⊆S. The
sample-based approximation for CSis denoted by ˆCS. Given a non-zero reward sample R, we can bound
the maximum range of each canonical reward by defining the upper bound canonical reward as follows:
Definition 5. (Upper Bound Canonical Reward) Let Rbe a non-zero reward sample defined over a set
of transitions B, and let ˆCSbe an arbitrary sample-based canonicalization method. Furthermore, let Z=
max (s,a,s′)∈B(|R(s,a,s′)|)be the maximum absolute reward. The upper bound canonical reward is given by:
U(ˆCS(R)(s,a,s′)) =nZ (17)
The justification for Definition 5 is that CS(R)(s,a,s′)hasn-terms with absolute values bounded by Z(both
|R(s,a,s′)|≤Zand|αiE[R(Si,A,S′
i)]|≤Z), hence,U(ˆCS(R)(s,a,s′)) =nZ. The non-zero assumption
12Under review as submission to TMLR
onRguarantees that there exists at least one sampled transition with a non-zero reward, ensuring that
U(ˆCS(R)(s,a,s′))≥0. To quantify the performance bounds of the canonicalization methods, we define the
relative shaping error as follows:
Definition 6. (Relative Shaping Error (RSE)) Let R′be a shaped, non-zero reward sample defined over a
set of transitions B, and let ˆCSbe a sample-based reward canonicalization method, such that: ˆCS(R′) =
ˆCS(R) +ϕR, whereϕRis the residual shaping term. Suppose that ϕRcan be expressed as: ϕR=ϕ˜R+Kϕ,
whereKϕis a constant that does not vary with (s,a,s′), andϕ˜Ris the effective residual shaping term.
Furthermore, let U(ˆCS(R)(s,a,s′)) =nZrepresent the upper bound of the unshaped canonical reward, where,
Z= max (s,a,s′)∈B|R(s,a,s′)|. The relative shaping error is defined as:
RSE (ˆCS(R)(s,a,s′)) =|ϕ˜R(s,a,s′)|
U(ˆCS(R)(s,a,s′))=|ϕ˜R(s,a,s′)|
nZ. (18)
The relative shaping error (RSE) is designed to theoretically quantify the impact of residual shaping in
reward distance comparisons. The denominator, U(ˆCS(R)(s,a,s′)) =nZ, represents an upper bound on
the magnitude of the base (unshaped) canonical reward, and it serves to normalize the impact of shaping.
A low RSE value suggests that U(ˆCS(R)(s,a,s′))is substantially larger than |ϕ˜R(s,a,s′)|, indicating that
the impact of shaping is likely minimal. Conversely, a high RSE value implies that U(ˆCS(R)(s,a,s′))is
relatively small compared to |ϕ˜R(s,a,s′)|, highlighting a more likely significant influence of the effective
residual shaping. Note that in the RSE definition, the effective residual shaping term, ϕ˜R=ϕR−Kϕ, where
Kϕis a constant that does not impact the Pearson distance (since its shift invariant), and hence, the reward
distances. For a comprehensive discussion on the derivation of the RSE definition, refer to Appendix A.1.1.
With regards to reward samples, we define forward and non-forward transitions as follows:
Definition 7 (Forward transitions) .Given a reward sample that spans a state space S, and an action space
A. Consider the state subsets Si,Sj⊆S. Transitions (Si,A,Sj)are forward transitions if Sjis distributed
conditionally based on (Si,A), according to the underlying transition dynamics of the reward sample.
Definition 8 (Non-forward transitions) .Given a reward sample that spans a state space S, and an action
spaceA. Consider the state subsets Si,Sj,Sk⊆S. Transitions (Si,A,Sj)are non-forward transitions when
the states in Sjare not distributed conditionally based on (Si,A), but are instead based on (Sk,A), according
to the underlying transition dynamics of the reward sample.
In reward canonicalization methods, given transitions (Si,A,Sj)needed in computing reward expectations,
both forward and non-forward transitions can have unsampled transitions since canonicalization methods
typically require the cross-product of all transitions from SitoSj. However, forward transitions are gen-
erally more robust to being unsampled since they are usually in-distribution with the underlying transition
dynamics governing the reward samples, hence, they naturally align (or are consistent) with the progression
of rewards in the sample. In contrast, non-forward transitions are highly prone to being unsampled (and
also unrealizable) as they may include a significant fraction of transitions that are out-of-distribution with
the reward sample’s transition dynamics. Based on the rationale that forward transitions are more robust to
being unsampled compared to non-forward transitions, to make our analysis more tractable, we will assume
that the fraction of unsampled transitions in forward transitions is negligible, leading to Theorem 2:
Theorem 2. Consider the reward comparison task on two equivalent non-zero reward samples that differ
due to potential-based shaping, and share the same set of sampled transitions, B. During canonicalization,
consider the forward and non-forward transition sets needed to compute the reward expectation terms, and
assume that the fraction of unsampled forward transitions in both reward samples is negligible. Each reward
sample can be expressed as R′
i(s,a,s′) =R(s,a,s′) +γϕi(s′)−ϕi(s)fori∈{1,2}, where,Ris the unshaped
reward sample, γis a discount factor, and ϕi(s)is the potential shaping function for R′
i. Under transition
sparsity, the upper bound of the Relative Shaping Error (RSE) for ˆCSRRDis lower than that of ˆCDARDand
ˆCEPICrespectively, in the order:
RSE(ˆCSRRD )≤M
3Z;RSE(ˆCDARD )≤2M
3Z;RSE(ˆCEPIC )≤M
Z,
where,M= maxs∈S(|ϕi(s)|)is the maximum magnitude of potential shaping across all states for the reward
samples, and Z= max (s,a,s′)∈B(|R(s,a,s′)|), is the maximum absolute value of the unshaped reward sample.
13Under review as submission to TMLR
Proof.See Appendix A.1
In more precise terms, Theorem 2 theoretically evaluates the robustness ˆCSRRD,ˆCDARD, and ˆCEPICagainst
residual shaping introduced by unsampled transitions from non-forward transitions, which are more likely
to be out-of-distribution with the transition dynamics that generated the reward samples. The theorem
assesses the upper bounds of the RSE across the three canonicalization methods to determine their sensitiv-
ity to residual shaping during reward comparisons, under the assumptions that the reward samples under
comparison are equivalent in terms of the optimal policies they induce and they share the same set of transi-
tions. These assumptions ensure that we isolate the differences between the canonicalized reward samples to
variations in the residual shaping. A lower upper bound implies that a canonicalization method is likely to
be less sensitive to the effects of residual shaping and, hence, it is more likely to reveal the actual similarity
between the unshaped reward samples under comparison. As shown by the upper bounds of the relative
shaping errors, the approximation for ˆCSRRDis theoretically more robust, compared to those of ˆCEPICand
ˆCDARD, since it has the smallest upper bound.
5 Experiments
To empirically evaluate SRRD, we examine the following hypotheses:
H1:SRRD is a reliable reward comparison pseudometric under high transition sparsity.
H2:SRRD can enhance the task of classifying agent behaviors based on their reward functions.
In these hypotheses, we compare the performance of SRRD to both EPIC and DARD using sample-based
approximations. In H1, we analyze SRRD’s robustness under transition sparsity resulting from limited
sampling and feasibility constraints. In H2, we investigate a practical use case to classify agent behaviors
using their reward functions. Experiment 1 tests H1and Experiment 2 tests H2.
Domain Specifications To conduct Experiment 1, we need the capability to vary the number of sam-
pled transitions, since the goal is to test SRRD’s performance under different levels of transition sparsity.
Therefore, Experiment 1 is performed in the Gridworld and Bouncing Balls domains, as they provide the
flexibility for parameter variation to control the size of the state and action spaces5. These two domains
have also been studied in the EPIC and DARD papers, respectively. The Gridworld domain simulates agent
movement from a given initial state to a specified terminal state under a static policy. States are defined by
(x,y)coordinates where 0≤x<Nand0≤y <Mimplying|S|=NM. The action space consists of four
cardinal directions (single steps), and the environment is stochastic, with a probability ϵof transitioning
to any random state irrespective of the selected action. When ϵ= 0, a feasibility constraint is imposed,
preventing the agent from making random transitions. The Bouncing Balls domain, adapted from (Wulfe
et al., 2022), simulates a ball’s motion from a starting state to a target state while avoiding randomly mo-
bile obstacles. These obstacles add complexity to the environment since the ball might need to change its
strategy to avoid obstacles (at a distance, d= 3). Each state is defined by the tuple (x,y,d ), where (x,y)
indicates the ball’s current location, and dindicates the ball’s Euclidean distance to the nearest obstacle,
such that: 0≤x<N, and 0≤y<M. The action space includes eight directions (cardinals and ordinals),
and we also define the stochasticity-level parameter ϵfor choosing random transitions.
For Experiment 2, the objective is to test SRRD’s performance in near-realistic domain settings, where we
have no control over factors such as the nature of rewards and the level of transition sparsity. Therefore,
for domain settings, in addition to the Gridworld and the Bouncing Balls domains with the setup similar
to Experiment 1 but fixed parameters, we also examine the StarCraft II and Drone Combat domains which
both simulate battlefield environments where a controlled multiagent team aims to defeat a default AI
enemy team (Anurag, 2019; Vinyals et al., 2019). These domains resemble complex scenarios with large
state and action spaces (almost infinite for StarCraft II), enabling us to test SRRD’s (as well as the other
pseudometrics) generalization to near-realistic scenarios. Additional details about these domains, including
information about the state and action features are described in Appendix C.2.
5Experiment 1 excludes the Drone Combat and StarCraft II environments because these domains have very large state and
action spaces that hinder effective coverage computation.
14Under review as submission to TMLR
Reward Functions Extrinsic reward functions are manually defined using a combination of state and
action features. For the StarCraft II and Drone Combat domains, we use the default game engine scores
(also based on state and action features), as the reward function (see Appendix C.1.1). For the Gridworld
and Bouncing Balls domains, in each reward function, the reward values are derived from the decomposition
of state and action features, where, (sf1,...,sfn)is from the starting state s;(af1,...,afm)is from the action
a; and (s′
f1,...,s′
fn)is from the subsequent state s′. For the Gridworld domain, these features are the (x,y)
coordinates, and for the Bouncing Balls domain, these include (x,y,d ), wheredis the distance of the obstacle
nearest to the ball. For each unique transition, using randomly generated constants: {u1,...,un}for incoming
state features;{w1,...,wm}for action features; {v1,...vn}for subsequent state features, we create polynomial
and random rewards as follows:
Polynomial: R(s,a,s′) =u1sα
f1+...+unsα
fn+w1aα
f1+...+wmaα
fm+v1s′α
f1+...+vns′α
fn,
whereαis randomly generated from 1-10,denoting the degree of the polynomial.
Random: R(s,a,s′) =β,
whereβis a randomly generated reward for each unique transition.
Forthepolynomialrewards, αisthesameacrosstheentiresample, butotherconstantsvarybetweendifferent
transitions. The same reward relationships are used to model potential shaping functions. In addition, we
also explore linear and sinusoidal reward models as described in Appendix C.1.1.
For complex environments such as StarCraft II and the Drone Combat domain, specifying reward functions
can be challenging, hence we also incorporate IRL to infer rewards from demonstrated behavior. For our
experiments, we consider the following IRL rewards: Maximum Entropy IRL (Maxent) (Ziebart et al., 2008);
Adversarial IRL (AIRL) (Fu et al., 2018); and Preferential-Trajectory IRL (PTIRL) (Santos et al., 2021).
The full descriptions for these algorithms is described in Appendix C.2.3.
Sample-based Approximations The sample-based approximations used in the experiments adopt the
double-batch sampling approach introduced in EPIC to ensure consistency with established methods. This
approach utilizes two batches: BVcontaining NVtransitions and BMcontaining NMstate-action pairs to
canonicalize rewards in BV. For ˆCDARD, the original approximation separately samples NAactions and
NTstates (as shown in Equation 8) to obtain state-action pairs needed for canonicalization. To ensure
consistency across methods, we modify ˆCDARDto use the batch BMinstead of separate action and state
batches. Empirical results indicate that this modification does not significantly impact the outcomes, as
samplingBMis equivalent to separately sampling NAactions and NTstates, given that NM=NANT.
Additionally, it is unclear how duplicate samples are handled in both ˆCEPICand ˆCDARD. To address
this, our experiments implement all batches as sets, thereby eliminating errors due to duplicate samples. A
key distinction between prior works and our current study is the assumption regarding the availability of
reward functions. While previous studies assumed the existence of the full reward functions encompassing all
possible transitions, whether realizable or not, our work acknowledges that the full reward functions might
not be available due to transition sparsity, hence, all reward canonicalization computations are based on the
provided reward samples. Specifically, transitions in BVare directly extracted from these reward samples,
andBMis sampled from the joint state-action distribution spanned by the reward samples.
5.1 Experiment 1: Transition Sparsity
Objective: The goal of this experiment is to test SRRD’s ability to identify similar reward samples under
transition sparsity as a result of limited sampling and feasibility constraints.
Relevance: The EPIC and DARD pseudometrics struggle in conditions of high transition sparsity since
they are designed to compare reward functions under high coverage. SRRD is developed to be resilient
under transition sparsity and this experiment tests SRRD’s performance relative to both EPIC and DARD,
on varying levels of transition coverage due to feasibility constraints and limited sampling.
Approach: This experiment is conducted on a 20×20Gridworld domain and a 20×20Bouncing Balls
domain. Forallsimulations, manualrewardsareusedsincetheyenabletheflexibilitytovarythenatureofthe
15Under review as submission to TMLR
Figure 3: (Transition Sparsity). The figure illustrates the performance of reward comparison pseudometrics
in identifying the similarity between potentially shaped reward functions under two conditions: (a) limited
sampling and (b) feasibility constraints. A more accurate pseudometric yields a Pearson distance Dρclose to
0, indicating a high degree of similarity between shaped reward functions, while a less accurate pseudometric
results inDρclose to 1. In both experiments, transition coverage is calculated as the ratio of sampled
transitions to the set of all theoretically possible transitions |S×A×S|, including both feasible and unfeasible
transitions. Each coverage data point represents an average over 200simulations at a constant policy rollout
count, with coverage data points generated by varying the number of policy rollouts from 1to2000(see
Appendix C.1.4). In panel (a), EPICandDARD lag behind SRRDat low transition coverage due to
limited sampling, but their performance gradually improves as coverage increases with higher rollout counts.
In panel (b), movement restrictions significantly reduce transition coverage, regardless of rollout sampling
frequency, which negatively impacts EPIC’s performance (approaching DIRECT ).
relationship between reward values and features, enabling us to test the performance of the pseudometrics on
diverse reward values, which include polynomial and random reward relationships. We also vary the shaping
potentials such that |R(s,a,s′)|≤|γϕ(s′)−ϕ(s)|≤5|R(s,a,s′)|.
For each domain, a ground truth reward function ( GT) and an equivalent potentially shaped reward function
(SH) are generated, both with full coverage ( 100%). Using rollouts from a uniform policy, rewards Rand
R′are sampled from GTandSHrespectively, and they might differ in transition composition. After sample
generation, RandR′arecanonicalizedandrewarddistancesarecomputedusingcommontransitionsbetween
them, under varying levels of coverage, due to limited sampling and feasibility constraints. The SRRD,
DARD, and EPIC reward distances are computed, as well as DIRECT, which is the Pearson distance of
rewards without canonicalization. Since RandR′are drawn from equivalent reward functions, an accurate
pseudometric should yield distances close to the minimum Pearson distance, Dρ= 0; and the least accurate
should yield a distance close to the maximum, Dρ= 1. DIRECT serves as a worst-case performance baseline,
since it computes reward distances without canonicalization (needed to remove shaping). We perform 200
simulation trials for each comparison task, and record the mean reward distances.
Simulations and Results: Limited Sampling: Using rollouts from a uniform policy, we sample R
andR′fromGTandSH, respectively, under a stochasticity-level parameter, ϵ= 0.1. The number of
transitions sampled is controlled by varying the number of policy rollouts (rollout counts) from 1up to
2000. The corresponding coverage is computed as the number of sampled transitions over the number of all
theoretically possible transitions ( =|S×A×S| ). Figure 3a summarizes the variation of reward distances
to transition coverage due to different levels of transition sampling in the Gridworld, and Bouncing Balls
domains. As shown, SRRD outperforms other baselines since it converges towards Dρ= 0faster, even when
coverage is low. DARD generally outperforms EPIC, however, it is highly prone to shaping compared to
16Under review as submission to TMLR
SRRD since it is more sensitive to unsampled transitions. All pseudometrics generally outperform DIRECT,
illustrating the value of removing shaping via canonicalization. No significant difference in the general trends
of results are observed between the two domains, and additional simulations are presented in Appendix C.1.
In conclusion, the proposed SRRD consistently outperforms both EPIC and DARD, especially under limited
sampling when coverage is low.
Simulations and Results: Feasibility Constraints: Using rollouts (similar range from 1 to 2000) from
a uniform policy, we sample RandR′fromGTandSH, respectively. To impose feasibility constraints, we
set the stochasticity-level parameter, ϵ= 0, to restrict random transitions between states such that only
movement to adjacent states is permitted. These movement restrictions ensure that coverage is generally
low (<10%), even though the number of rollouts is similar to those in the first experiment. Figure 3b
summarizes the results for the variation of reward distances to transition coverage under the movement
restrictions. As shown, SRRD significantly outperforms all the baselines indicating its high robustness
under feasibility constraints.
5.2 Experiment 2: Classifying Agent Behaviors
Objective: ThegoalofthisexperimentistoassessSRRD’seffectivenessasadistancemeasureinclassifying
agent behaviors based on their reward functions. If SRRD is robust, it should identify similarities among
reward functions from the same agents while differentiating reward functions from distinct agents.
Relevance: This experiment6demonstrates a practical use-case for incorporating reward comparison pseu-
dometrics to interpret reward functions by relating them to agent behavior. In many real-world situations,
samples of agent behaviors are available, and there is a need to interpret the characteristics of the agents
that produced these behaviors. For example, several works have attempted to predict player rankings and
strategies using past game histories (Luo et al., 2020; Liu & Schulte, 2018; Yanai et al., 2022). This exper-
iment takes a similar direction by attempting to classify the identities of agents from their unlabeled past
trajectories using reward functions. The reliance on reward functions rather than the original trajectories
is based on the premise that reward functions are “succinct” and “robust”, hence a preferable means to
interpret agent behavior (Abbeel & Ng, 2004; Michaud et al., 2020).
Approach: In this experiment, we train a k-nearest neighbors ( k-NN) classifier to classify unlabeled agent
trajectories by indirectly using computed rewards, to identify the agents that produced these trajectories.
We examine the k-NN algorithm since it is one of the most popular distance-based classification techniques.
The experiment is conducted across all domains, and since we want to maximize classification accuracy, we
consider different IRL rewards, including: Maxent, AIRL, PTIRL as well as manual (extrinsic) rewards.
For manual rewards, we utilize the default game score for the StarCraft II and Drone Combat domains,
and polynomial rewards for the Gridworld and Bouncing Balls domains, where we induce random potential
shaping. For each domain, we examine SRRD, DIRECT, EPIC, and DARD as distance measures for a k-NN
reward classification task. The steps for the approach are as follows:
1. Create agents X={x1,...,xm}with distinct behaviors.
2. For each agent xi∈X, generate a collection of trajectory sets {ζxi
1,...,ζxip}, where each ζxi
j=
{τxi
j,1,...,τxi
j,q}is aq-sized set of trajectories. Compute reward functions {Rxi
1,...,Rxip}using IRL
or manual specification based on each corresponding ζxi
j.
3. Randomly shuffle all the computed reward functions R(from all agents), and split into the training
Rtrainand testing Rtestsets.
6Experiment 2 is related to prior works, Gleave et al. (2020) and Wulfe et al. (2022), where reward distances are computed
for the ground truth, regressed, and IRL-generated reward functions. Their results show that distances from each reward type
are relatively similar, reflecting some form of reward grouping or clustering. However, these works do not explore real-world use-
cases of this ‘reward grouping’ phenomenon, and our work presents a k-NN classification algorithm that utilizes reward distance
similarity (between reward sample vectors) to classify agent behaviors, without the need for additional reward preprocessing.
17Under review as submission to TMLR
4. Use each reward pseudometric as a distance measure to train a k-NN classifier with Rtrainand test
it withRtest.
Table 1: The accuracy (%) of different reward comparison distances in k−NN reward classification.
DOMAIN REWARDS DIRECT EPIC DARD SRRD
GridworldManual 69.8 69.3 70.0 75.8
Maxent 57.4 57.5 68.9 70.0
AIRL 82.3 84.9 85.0 86.2
PTIRL 82.2 84.2 83.4 86.0
Bouncing BallsManual 46.5 47.3 52.0 55.2
Maxent 39.7 46.0 50.8 49.9
AIRL 41.2 46.1 49.8 56.3
PTIRL 70.3 71.1 69.5 72.4
Drone CombatManual 67.1 67.2 66.2 73.9
Maxent 70.3 77.7 73.2 76.7
AIRL 90.1 90.7 92.3 93.8
PTIRL 52.5 63.7 65.1 78.3
StarCraft IIManual 65.5 67.4 69.5 76.5
Maxent 72.3 74.1 73.9 74.8
AIRL 75.1 75.3 78.1 77.0
PTIRL 77.2 78.1 77.6 79.8
Across all the four domains, in step 1 and 2, default parameters are set as follows: m- the number of
distinct agent policies is equal to 10;p- the number of trajectory sets is equal to 100; andq- the number of
trajectories per set (used in each IRL run) is equal to 5. In step 1, different agent behaviors are controlled
by varying the agents’ policies (See Appendix C.2.2). In step 4, to train the classifier, grid-search is used
to identify candidate values for kandγ, and twofold cross-validation (using Rtrain) is used to optimize
hyper-parameters based on accuracy. Since we assume potential shaping, the value of γis unknown, hence
its a hyper-parameter. To classify an arbitrary reward function Ri∈Rtest, we traverse reward functions
Rj∈Rtrain, and compute the distance, Dρ(Ri,Rj)using the reward pseudometrics. We then identify the
topk-closest rewards to Ri, and choose the label of the most frequent class. We select a training to test set
ratio of 70 : 30, and repeat this experiment 200times.
Simulations and Results: Table 1 summarizes experimental results. As shown, SRRD generally achieves
higher accuracy compared to DIRECT, EPIC and DARD across all domains, indicating SRRD effectiveness
at discerning similarities between rewards produced by the same agents, and differences between those
generated by different agents. This trend is more pronounced with manual rewards where SRRD significantly
outperforms other baselines. This can be attributed to potential shaping, which is intentionally induced in
manual rewards that SRRD is specialized to tackle. Therefore, SRRD proves to be a more effective distance
measure at classifying rewards subjected to potential shaping. For IRL-based rewards such as Maxent, AIRL,
and PTIRL, while we assume potential shaping, non-potential shaping could be present. This explains the
reduction in SRRD’s performance gap over EPIC and DARD, as well as the few instances where EPIC
and DARD outperform SRRD, though SRRD is still generally dominant. We also observe that all the
pseudometrics tend to perform better on AIRL rewards compared to other IRL-based rewards. This result
is likely due to the formulation of the AIRL algorithm, which is designed to effectively mitigate the effects
of unwanted shaping in reward approximation (Fu et al., 2018), thus providing more consistent rewards.
Overall, SRRD, EPIC, and DARD outperform DIRECT, emphasizing the importance of canonicalization at
reducing the impact of shaping.
To verify the validity of results, Welch’s t-tests for unequal variances are conducted across all domain
and reward type combinations, to test the null hypotheses: (1) µSRRD≤µDIRECT, (2)µSRRD≤µEPIC,
and (3)µSRRD≤µDARD; against the alternative: (1) µSRRD> µ DIRECT, (2)µSRRD> µ EPIC, and (3)
18Under review as submission to TMLR
µSRRD> µ DARD, whereµrepresents the sample mean. We reject the null when the p-value <0.05(level
of significance), and conclude that: (1) µSRRD> µ DIRECTfor all instances; (2) µSRRD> µ EPICfor11out
of12instances, and (3) µSRRD> µ DARDfor10out of 12instances. These tests are performed assuming
normality as per central limit theorem, since the number of trials is 200. For additional details about the
tests and accuracy metrics such as F1-scores, refer to Appendix C.2. In summary, we conclude that SRRD
is a more effective distance measure for classifying reward samples compared to its baselines.
6 Conclusion and Future Work
This paper introduces SRRD, a reward comparison pseudometric designed to address transition sparsity, a
significant challenge encountered when comparing reward functions without high transition coverage. Con-
ducted experiments, and theoretical analysis, demonstrate SRRD’s superiority over state-of-the-art pseudo-
metrics, such as EPIC and DARD, under limited sampling and feasibility constraints. Additionally, SRRD
proves effective as a distance measure for k-NN classification using reward functions to represent agent be-
havior. This implies that SRRD can find higher similarities between reward functions generated by the same
agent and higher differences between reward functions that are generated from different agents.
Most existing studies, including ours, have primarily focused on potential shaping, as it is the only additive
shaping technique that guarantees policy invariance (Ng et al., 1999; Jenner et al., 2022). Future research
should consider the effects of non-potential shaping on SRRD (see Appendix B.1) or random perturbations,
as these might distort reward functions that would otherwise be similar. This could help to standardize
and preprocess a wider range of rewards that might not necessarily be potentially shaped. In computing
reward distances, the Pearson distance is employed for its shift and scale invariance properties. However, this
distance measure is highly sensitive to outliers, especially in the case of small reward samples. Future work
should explore modifications such as Winsorization to mitigate the impact of outliers. Future studies should
also explore applications of reward distance comparisons in scaling reward evaluations in IRL algorithms. For
example, iterative IRL approaches such as MaxentIRL, often perform policy learning to assess the quality of
the updated reward in each training trial. Integrating direct reward comparison pseudometrics to determine
if rewards are converging, could help to skip the policy learning steps, thereby speeding up IRL. Finally, the
development of reward comparison metrics has primarily aimed to satisfy policy invariance. A promising
area to examine in the future is multicriteria policy invariance, where invariance might be conditioned to
different criteria. For example, in the context of reward functions in Large Language Models (LLMs), it
might be important to compute reward distance pseudometrics that consider different criteria such as bias,
safety, or reasoning, to advance interpretability, which could be beneficial for applications such as reward
fine-tuning and evaluation (Lambert et al., 2024).
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship Learning via Inverse Reinforcement Learning. In Proceed-
ings of the Twenty-First International Conference on Machine Learning , pp. 1, 2004.
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline rein-
forcement learning. In International conference on machine learning , pp. 104–114. PMLR, 2020.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete
problems in ai safety. arXiv preprint arXiv:1606.06565 , 2016.
Koul Anurag. Ma-gym: Collection of Multi-agent Environments based on OpenAI gym. https://github.
com/koulanurag/ma-gym , 2019.
Saurabh Arora and Prashant Doshi. A Survey of Inverse Reinforcement Learning: Challenges, Method and
Progress. Artificial Intelligence , 297(1), 2021.
Monica Babes, Enrique Munoz de Cote, and Michael Littman. Social Reward Shaping in the Prisoner’s
Dilemma (Short Paper). In Proc. of 7th Int. Conf. on Autonomous Agents and Multiagent Systems
(AAMAS) , 2008.
19Under review as submission to TMLR
Xiaocong Chen, Siyu Wang, Julian McAuley, Dietmar Jannach, and Lina Yao. On the opportunities and
challenges of offline reinforcement learning for recommender systems. ACM Transactions on Information
Systems, 42(6):1–26, 2024.
Ching-An Cheng, Andrey Kolobov, and Adith Swaminathan. Heuristic-guided Reinforcement Learning. In
Advances in Neural Information Processing Systems , volume 34, 2021.
Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep Rein-
forcement Learning from Human Preferences. Advances in Neural Information Processing Systems , 30,
2017.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided Cost Learning: Deep Inverse Optimal Control via
Policy Optimization. In International Conference on Machine Learning , pp. 49–58, 2016.
Justin Fu, Katie Luo, and Sergey Levine. Learning Robust Rewards with Adverserial Inverse Reinforcement
Learning. In International Conference on Learning Representations , 2018.
Yang Gao and Francesca Toni. Potential Based Reward Shaping for Hierarchical Reinforcement Learning.
InTwenty-Fourth International Joint Conference on Artificial Intelligence , 2015.
AdamGleave, MichaelDennis, ShaneLegg, StuartRussell, andJanLeike. QuantifyingDifferencesInReward
Functions. In International Conference on Learning Representations , 2020.
Adam Gleave, Mohammad Taufeeque, Juan Rocamonde, Erik Jenner, Steven Wang, Sam Toyer, Maxim-
ilian Ernestus, Nora Belrose, Scott Emmons, and Stuart Russell. Imitation: Clean imitation learning
implementations. arXiv:2211.11972v1 [cs.LG], 2022. URL https://arxiv.org/abs/2211.11972 .
Abhishek Gupta, Aldo Pacchiano, Yuexiang Zhai, Sham Kakade, and Sergey Levine. Unpacking Reward
Shaping: Understanding the Benefits of Reward Engineering on Sample Complexity. In Advances in
Neural Information Processing Systems , volume 35, 2022.
Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, and Changjie
Fan. Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping. In Advances in Neural
Information Processing Systems , volume 33, 2020.
Erik Jenner and Adam Gleave. Preprocessing Reward Functions for Interpretability. preprint
arXiv:2203.13553, 2022.
Erik Jenner, Herke van Hoof, , and Adam Gleave. Calculus on MDPs: Potential Shaping as a Gradient.
preprint arXiv:2208.09570, 2022.
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha
Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language
modeling. arXiv preprint arXiv:2403.13787 , 2024.
Guillaume Lample and Devendra Singh Chaplot. Playing FPS Games with Deep Reinforcement Learning.
InProceedings of the AAAI Conference on Artificial Intelligence , volume 31, 2017.
SergeyLevine, AviralKumar, GeorgeTucker, andJustinFu. Offlinereinforcementlearning: Tutorial, review,
and perspectives on open problems. arXiv preprint arXiv:2005.01643 , 2020.
Guiliang Liu and Oliver Schulte. Deep reinforcement learning in ice hockey for context-aware player evalu-
ation.arXiv preprint arXiv:1805.11088 , 2018.
Yudong Luo, Oliver Schulte, and Pascal Poupart. Inverse Reinforcement Learning for Team Sports: Valuing
Actions and Players. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial
Intelligence (IJCAI-20) , 2020.
Saaduddin Mahmud, Saisubramanian Sandhya, and Zilberstein Shlomo. Explanation-Guided Reward Align-
ment. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence , pp.
473–482, 2023.
20Under review as submission to TMLR
Maja J Mataric. Reward Functions for Accelerated Learning. Machine Learning Proceedings 1994 , pp.
pp. 181 – 189, 1994. doi: https://doi.org/10.1016/B978-1-55860-335-6.50030-1. URL https://www.
sciencedirect.com/science/article/abs/pii/B9781558603356500301 .
Eric J Michaud, Adam Gleave, and Stuart Russell. Understanding learned reward functions. arXiv preprint
arXiv:2012.05862 , 2020.
Andrew Y Ng and Stuart Russell. Algorithms for Inverse Reinforcement Learning. In International Confer-
ence on Machine Learning (ICML) , volume 2, 2000.
Andrew Y. Ng, Daishi Harada, and Stuart Russell. Policy Invariance under Reward Transformations: Theory
and Application to Reward Shaping. In International Conference on Machine Learning , pp. 278–287, 1999.
Jette Randløv and Preben Alstrøm. Learning to Drive a Bicycle Using Reinforcement Learning and Shaping.
InInternational Conference on Machine Leaning , volume 98, 1998.
Jacob Russell and Eugene Santos. Explaining Reward Functions in Markov Decision Processes. In The
Thirty-Second International Flairs Conference , 2019.
Eugene Santos and Clement Nyanhongo. A Contextual-Based Framework for Opinion Formation. In The
Thirty-Second International Flairs Conference , 2019.
Eugene Santos, Clement Nyanhongo, Hien Nguyen, Keum Joo Kim, and Gregory Hyde. Contextual Evalua-
tion of Human–Machine Team Effectiveness. Systems Engineering and Artificial Intelligence , pp. 283–307,
2021.
DavidSilver, SatinderSingh, DoinaPrecup, andRichardS.Sutton. RewardisEnough. Artificial Intelligence ,
299, 2021.
Satinder Singh, Richard Lewis, and Andrew G Barto. Where Do Rewards Come From. In Proceedings of the
Annual Conference of the Cognitive Science Society , pp. pp. 2601 – 2606. Cognitive Science Society, 2009.
Joar Max Viktor Skalse, Lucy Farnik, Sumeet Ramesh Motwani, Erik Jenner, Adam Gleave, and Alessandro
Abate. Starc: A general framework for quantifying differences between reward functions. In The Twelfth
International Conference on Learning Representations , 2024.
Halit Bener Suay, Tim Brys, Matthew E. Taylor, and Sonia Chernova. Learning from Demonstration for
Shaping through Inverse Reinforcement Learning. In Proceedings of the 2016 International Conference on
Autonomous Agents Multiagent Systems , pp. 429–437, 2016.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction . MIT Press, 2018.
AnaTenorio-Gonzalez, EduardoF.Morales, andLuisVillasenor-Pineda. DynamicRewardShaping: Training
a Robot by Voice. In Advances in Artificial Intelligence–IBERAMIA 2010 , pp. 483–492, 2010.
Garrett Thomas, Yuping Luo, and Tengyu Ma. Safe reinforcement learning by imagining the near future.
Advances in Neural Information Processing Systems , 34:13859–13869, 2021.
Stephen Van Evera. Offense, defense, and the causes of war. International Security , 22(4):5–43, 1998.
OriolVinyals, IgorBabuschkin, WojciechM.Czarnecki, MichaëlMathieu, AndrewDudzik, JunyoungChung,
David H. Choi, T. Ewalds R. Powell, and J. Oh P. Georgiev. Grandmaster Level in StarCraft II using
Multi-agent Reinforcement Learning. Nature, 575:350–354, 2019.
Eric Wiewiora, Garrison Cottrell, and Charles Elkan. Principled Methods for Advising Reinforcement Learn-
ing Agents. In Proceedings of the 20th International Conference on Machine Learning , pp. 792–799, 2003.
Blake Wulfe, Logan Michael Ellis, Jean Mercat, Rowan Thomas McAllister, and Adrien Gaidon. Dynamics-
AwareComparisonofLearnedRewardFunctions. In International Conference on Learning Representations
(ICLR), 2022.
21Under review as submission to TMLR
Chen Yanai, Adir Solomon, Gilad Katz, Bracha Shapira, and Lior Rokach. Q-ball: Modeling basketball
games using deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence ,
volume 36, pp. 8806–8813, 2022.
Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is Enough for
Convex MDPs. In Advances in Neural Information Processing Systems , volume 34, 2021.
Brian D. Ziebart, Andrew Maas, J.Andrew Bagnell, and Anind K. Dey. Maximum Entropy Inverse Rein-
forcmeent Learning. In Association for the Advancement of Artificial Intelligence (AAAI) , volume 8, pp.
1433–1438, 2008.
22Under review as submission to TMLR
A Derivations, Theorems and Proofs
A.1 Relative Shaping Error Comparisons
The section provides the proof for Theorem 2 which compares the upper bounds of the relative shaping
errors for ˆCSRRD,ˆCDARD, and ˆCEPIC. We organize the presentation as follows:
1. Discuss the motivation for the relative shaping errors in quantifying the variation of the residual
shaping normalized by the upper bound base (unshaped) canonical rewards (Appendix A.1.1).
2. Proof of Theorem 2 (Appendix A.1.2).
A.1.1 Motivation for the Relative Shaping Error (RSE) Measure
Consider the task of comparing two equivalent reward samples, R′
1andR′
2, which induce similar policies,
share the same set of transitions B, but may differ due to potential shaping. To compare the reward samples,
we first eliminate shaping by applying a sample-based canonicalization method, ˆCS. After canonicalization,
we compute the Pearson distance to quantify the difference between the canonical rewards as follows:
Dρ(ˆCS(R′
1),ˆCS(R′
2)) =/radicalig
1−ρ(ˆCS(R′
1),ˆCS(R′
2))/√
2 (19)
whereρis the Pearson correlation. Since R′
1andR′
2are assumed to have the same set of transitions, and
only differ due to potential shaping, after canonicalization, we have:
ˆCS(R′
1) =ˆCS(R1) +ϕR1=ˆCS(R) +ϕR1,
ˆCS(R′
2) =ˆCS(R2) +ϕR2=ˆCS(R) +ϕR2,
whereϕR1andϕR2are residual shaping terms, and ˆCS(R)is thebase canonical reward , which is similar
between the reward samples since the set of explored transitions are similar7, hence, ˆCS(R1) = ˆCS(R2) =
ˆCS(R). When computing Dρ(Equation 19), the Pearson correlation can thus be expressed as:
ρ(ˆCS(R′
1),ˆCS(R′
2)) =ρ(ˆCS(R) +ϕR1,ˆCS(R) +ϕR2). (20)
Analyzing Equation 20:
•When|ϕRi|<<|ˆCS(R)|for alli∈{1,2}thenρ(ˆCS(R′
1),ˆCS(R′
2))≈ρ(ˆCS(R),ˆCS(R)) = 1, since
the residual shaping terms have negligible impact. Conversely, when either |ϕR1|>>|ˆCS(R)|or
|ϕR2|>>|ˆCS(R)|,then the Pearson correlation is predominantly influenced by the residual shaping
terms, and ρcan become low or even negative, especially when ϕR1andϕR2differ significantly.
•Since ˆCS(R)is the same across ˆCS(R′
1)and ˆCS(R′
2), variations in the Pearson correlation can be
primarily attributed to variations in the residual shaping terms.
•To assess the impact of residual shaping, it is essential to also consider the magnitude ˆCS(R)as a
normalizing factor. If |ˆCS(R)|is significantly larger than the residual shaping terms, the influence
of the shaping terms diminishes, resulting in a higher Pearson correlation closer to 1. Conversely,
if|ˆCS(R)|is significantly smaller than the residual shaping terms, these shaping terms will have a
higher influence on the correlation, potentially reducing it.
To generalize these insights, lets consider the canonicalization task on a reward sample R′such that:
ˆCS(R′) =ˆCS(R) +ϕR. (21)
7We assume that the reward samples under comparison are equivalent and share the same transitions, to simplify our analysis
by isolating the differences between the reward samples to potential shaping.
23Under review as submission to TMLR
To quantify the influence of the residual shaping term ϕRrelative to the base canonical reward ˆCS(R), lets
establish the Relative Shaping Error (RSE) as follows:
RSE (ˆCS(R)(s,a,s′)) =|ϕR(s,a,s′)|
U(ˆCS(R)(s,a,s′))=|ϕR(s,a,s′)|
nZ. (22)
A low RSE indicates that U(ˆCS(R)(s,a,s′))is substantially large relative to |ϕR(s,a,s′)|, suggesting that
the residual shaping has minimal impact on Dρ. Conversely, a high RSE implies that U(ˆCS(R)(s,a,s′))is
small relative to|ϕR(s,a,s′)|, highlighting a more significant influence of residual shaping. By normalizing
with the upper bound of the RSE, we obtain a conservative measure to quantify the impact of shaping
in extreme scenarios. This formulation helps assess the robustness of reward canonicalization methods at
mitigating shaping effects during reward comparisons. From Equation 21, suppose we can express:
ˆCS(R)(s,a,s′) =ˆCS(˜R)(s,a,s′) +KR,
ϕR(s,a,s′) =ϕ˜R(s,a,s′) +Kϕ,
whereKRandKϕare constants that do not vary with (s,a,s′), andϕ˜Ris theeffective residual shaping .
When comparing reward samples R′
1andR′
2, in computing Dρ, the constants KRandKϕdo not affect the
Pearson correlation since it is shift invariant. Therefore:
ρ(ˆCS(R′
1),ˆCS(R′
2)) =ρ(ˆCS(R1) +ϕR1,ˆCS(R2) +ϕR2)
=ρ(ˆCS(˜R1) +KR1+ϕ˜R1+Kϕ1,ˆCS(˜R2) +KR2+ϕ˜R2+Kϕ2)
=ρ(ˆCS(˜R) +ϕ˜R1,ˆCS(˜R) +ϕ˜R2) (23)
Relating this to the RSE in Equation 22, note that |ϕR|serves as a measure of reward variation due to
shaping, and U(ˆCS(R))acts as a normalizing term. For the term ϕR, we can omit Kϕto get:
ϕ˜R=ϕR−Kϕ, (24)
since,Kϕdoes not affect the variation of the Pearson correlation. However, for the denominator U(ˆCS(R)),
we cannot omit KR(hence no changes for the denominator) because the denominator serves to normalize
the residual shaping. When KRis large, even though it does not affect the Pearson correlation, it lowers the
impact of|ϕ˜R|and vice-versa. This leads to the final RSE equation below:
RSE (ˆCS(R)(s,a,s′)) =|ϕ˜R(s,a,s′)|
U(ˆCS(R)(s,a,s′))=|ϕ˜R(s,a,s′)|
nZ. (25)
A.1.2 Proof of Theorem 2
Theorem 2 aims to compare the upper bounds of the RSEs (Equation 25) for ˆCEPIC,ˆCDARD, and ˆCSRRD.
Proof.Assuming a finite reward sample that spans a state space SDand an action space AD, where,Dis
the coverage distribution. The following subsets are defined: for ˆCEPIC,S⊆SDandS′⊆SD; for ˆCDARD,
S′′⊆SDandS′⊆SD; and for ˆCSRRD, eachSi⊆SD, wherei∈{1,..., 6}. Let’s define:
M= max
s∈SD(|ϕ(s)|),
as the maximum absolute shaping term for states in SD, whereM∈R. Then, for all shaping expectations:
|E[ϕ(S)]|≤M,|E[ϕ(S′)]|≤M,|E[ϕ(S′′)]|≤M,and|E[ϕ(Si)]|≤Mfor alli∈{1,..., 6}.
In the following analysis, we impose the assumption that unsampled forward transitions are negligible based
on the rationale that forward transitions are generally consistent or in-distribution with the transition dy-
namics of reward samples, compared to non-forward transitions which have a higher chance of being out-
of-distribution with the reward samples’ transition dynamics. For the analysis, we find the upper bound of
24Under review as submission to TMLR
the RSEs under different scenarios of transition sparsity from best case (no unsampled transitions) to worst
case (high levels of unsampled non-forward transitions).
Analysis of EPIC:
Considering Definition 1 for CEPIC, the transitions (S,A,S′)are forward transitions since by definition, S′
is created based on (S,A). However, the transitions: (s,A,S′)and(s′,A,S′)are non-forward transitions
sinceS′is not created based on (s,A)or(s′,A), but on (S,A). Since we only have unsampled non-forward
transitions,letthefractionofrandomlysampledtransitionsin (s′,A,S′)and(s,A,S′)beuandvrespectively,
where 0≤u,v≤1. Note that uandvvary based on (s,a,s′), however, for visual clarity, we denote them
asuandv. Incorporating uandvinto ˆCEPIC:
ˆCEPIC (R)(s,a,s′) =R(s,a,s′) +E[uγR(s′,A,S′)−vR(s,A,S′)−γR(S,A,S′)], (26)
Applying ˆCEPIC(Equation 26) to a shaped reward R′(s,a,s′), we get the residual shaping:
ϕepic= (γ−γu)ϕ(s′) + (v−1)ϕ(s) +E[(γ2u−γ2)ϕ(S′)] +E[γϕ(S)]−E[γvϕ(S′)] (27)
Best Case: In this scenario there are no unsampled transitions such that: u,v= 1. Applying u,v= 1into
Equation 27, we get: ϕepic=E[γϕ(S)]−E[γϕ(S′)].
InˆCEPIC, all transitions are canonicalized using the same state subsets SandS′such that the shaping terms
ϕ(S)andϕ(S′)do not vary with changes in (s,a,s′). Therefore, in ϕepic, the constant Kϕ=E[γϕ(S)]−
E[γϕ(S′)]does not vary with changes in (s,a,s′), and based on Equation 24, ϕ˜epic= 0, such that:
RSE (ˆCEPIC (R)) = 0.
Average Case: In this scenario, 0<u,v< 1. Fromϕepic(Equation 27), we can extract the constant term
Kϕ=−E[γ2ϕ(S′)] +E[γϕ(S)], which does not vary with (s,a,s′). Therefore, the effective residual shaping
(see Equation 24) is given by:
ϕ˜epic= (γ−γu)ϕ(s′) + (v−1)ϕ(s) +E[(γ2u−γv)ϕ(S′)], (28)
such that:
|ϕ˜epic|≤(γ−γu)|ϕ(s′)|+ (1−v)|(−ϕ(s))|+|γ2u−γv||E[ϕ(S′)]|
≤M(γ−γu+ 1−v+|γ2u−γv|),/parenleftbig
sinceM= max
s∈SD|ϕ(s)|/parenrightbig
.
≤M(2−u−v+|u−v|) ( whenγ= 1)
≤2M
From Equation 26, ˆCEPIChas four reward terms. Following Definition 5, the upper bound of ˆCEPIC (R)is
thereforeU(ˆCEPIC (R)) = 4Z, hence:
RSE (ˆCEPIC (R)) =|ϕ˜epic|
U(ˆCEPIC (R))≤M
2Z.
Worst Case: In this scenario, u,v= 0. Applying uandvinto ˆCEPIC(Equation 26), we get:
ˆCEPIC (R)(s,a,s′) =R(s,a,s′)−E[γR(S,A,S′)], (29)
andϕepic=γϕ(s′)−ϕ(s)−E[γ2ϕ(S′)] +E[γϕ(S)]. SinceSandS′are the same for all transitions, Kϕ=
−E[γ2ϕ(S′)] +E[γϕ(S)]is a constant, hence, ϕ˜epic=γϕ(s′)−ϕ(s)such that:
|ϕ˜epic|≤|γϕ(s′)|+|(−ϕ(s))|≤|γM+M|≤2M
25Under review as submission to TMLR
From Equation 29, we see that ˆCEPIChas two reward terms. Following Definition 5, the upper bound of
ˆCEPIC (R)is therefore U(ˆCEPIC (R)) = 2Z, hence:
RSE (ˆCEPIC (R)) =|ϕ˜epic|
U(ˆCEPIC (R)≤M
Z.
∴Based on all the three cases, we have:
RSE (ˆCEPIC (R))≤M
Z. (30)
Analysis of DARD:
Considering Equation 7 for CDARD, the state subset, S′′is subsequent to s′, andS′is subsequent to s. For
approximations, the transitions (S′,A,S′′)are non-forward transitions since S′′is created based on (s′,A),
rather than (S′,A). Let the fraction of randomly sampled transitions in (S′,A,S′′)bew, where 0≤w≤1.
Note thatwalso depends on (s,a,s′), however, for visual clarity we denote it as simply w. Incorporating w
into ˆCDARD:
ˆCDARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′′)−R(s,A,S′)−wγR(S′,A,S′′)], (31)
Applying ˆCDARDto a shaped reward R′, we get the residual shaping:
ϕdard=E[(γ2−wγ2)ϕ(S′′) + (wγ−γ)ϕ(S′)] (32)
Best Case: In this scenario, w= 1, such that|ϕdard|= 0, hence, the RSE (ˆCDARD ) = 0.
Average Case: For this scenario, 0<w< 1:
|ϕdard|≤(γ2−wγ2)|E[ϕ(S′′)]|+ (γ−wγ)|(−E[ϕ(S′)])|≤(γ2−wγ2)M+ (γ−wγ)M≤2M
InˆCDARD, the state subsets, S′andS′′, vary due to (s,a,s′)henceKϕ= 0. Following Definition 6, and
applying it to ˆCDARD(Equation 31): U(ˆCDARD (R)) = 4Z,such that:
RSE (ˆCDARD (R)) =|ϕdard|
U(ˆCDARD (R))≤M
2Z.
Worst Case: In this scenario, w= 0, hence we eliminate terms with win Equation 31 to get:
ˆCDARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′′)−R(s,A,S′)], (33)
which corresponds to: ϕdard=E[γ2ϕ(S′′) +γϕ(S′)], such that:
|ϕdard|≤|E[γ2ϕ(S′′)|+|E[γϕ(S′)]|≤|γ2M|+|γM|≤2M
Following Definition 6, and applying it to ˆCDARD(Equation 33): U(ˆCDARD (R)) = 3Z,such that:
RSE (ˆCDARD (R)) =|ϕdard|
U(ˆCDARD (R))≤2M
3Z.
∴Based on all the three cases, we have:
RSE (ˆCDARD (R))≤2M
3Z. (34)
Analysis of SRRD
Considering CSRRDin Definition 3, the first six reward terms are forward transitions since for each set
of transitions (Si,A,Sj),Sjis distributed conditionally based on (Si,A). The last two terms violate this
condition, hence, they are non-forward transitions. As mentioned in Section 4, SRRD is designed such that:
26Under review as submission to TMLR
•Transitions (S1,A,S 5)⊆(S4,A,S 5):
SinceS1are subsequent states to s′, andS4are subsequent states for all sampled transitions. It
follows that S1⊆S4, hence, (S1,A,S 5)⊆(S4,A,S 5).
•Transitions (S2,A,S 6)⊆(S3,A,S 6).
S2is the set of non-terminal subsequent states to s. SinceS3encompasses all initial states from all
sampled transitions, it follows that S2⊆S3, hence, (S2,A,S 6)⊆(S3,A,S 6).
For ˆCSRRD, let the fraction of the randomly sampled transitions for (S4,A,S 5)and(S3,A,S 6)bepand
q, respectively. Considering that the number of unsampled forward transitions are negligible, we establish
minimal thresholds m1,m2>0such that:p=m1when (S4,A,S 5)only contains sampled transitions from
(S1,A,S 5), andq=m2when (S3,A,S 6)only contains sampled transitions from (S2,A,S 6). Therefore:
m1≤p≤1andm2≤q≤1. Note that pandqvaries based on (s,a,s′), but for visual clarity, we express
them aspandq. Incorporating pandq, we get:
ˆCSRRD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4) +γ2R(S1,A,S 5)
−γR(S2,A,S 6) +qγR(S3,A,S 6)−pγ2R(S4,A,S 5)](35)
Applying ˆCSRRD(Equation 35) to a shaped reward R′(s,a,s′), we get the residual shaping:
ϕsrrd=E[(γ−qγ)ϕ(S3) + (pγ2−γ2)ϕ(S4) + (γ3−pγ3)ϕ(S5) + (qγ2−γ2)ϕ(S6)] (36)
Best Case: In this scenario there are no unsampled transitions such that: p,q= 1. Applying pandqinto
Equation 36, we get: ϕsrrd= 0, hence,RSE (ˆCSRRD (R)) = 0.
Average Case: In this scenario, m1< p < 1andm2< q < 1. Note that in ˆCSRRD,S3andS4are the
same for all transitions, since S3encompasses all initial states, and S4includes all subsequent states to S3.
Therefore,ϕ(S3)andϕ(S4)do not vary with (s,a,s′). Fromϕsrrd(Equation 36), we can extract the constant
Kϕ=E[γϕ(S3)−γ2ϕ(S4)]. Based on Equation 24, the effective residual shaping is given by:
ϕ˜srrd=E[−qγϕ(S3) +pγ2ϕ(S4) + (γ3−pγ3)ϕ(S5) + (qγ2−γ2)ϕ(S6)]
such that:
ϕ˜srrd≤qγ|(−E[ϕ(S3)])|+pγ2|E[ϕ(S4)]|+ (γ3−pγ3)|E[ϕ(S5)]|+ (γ2−qγ2)|(−E[ϕ(S6)])|
≤M(qγ+pγ2+γ3−pγ3+γ2−qγ2)
≤M(q+p+ 1−p+ 1−q)(whenγ= 1)
≤2M
Since ˆCSRRDhas 8 terms, following Definition 6 and applying it to Equation 35, we get U(ˆCSRRD (R)) = 8Z,
Therefore:
RSE (ˆCSRRD (R)) =|ϕ˜srrd|
U(ˆCSRRD (R))≤M
4Z.
Worst Case: In this scenario, p=m1(when (S4,A,S 5)only has sampled transitions from (S1,A,S 5))
andq=m2(when (S3,A,S 6)only has sampled transitions from (S2,A,S 6)). Consider a situation where
|(S4,A,S 5)|>>|(S1,A,S 5)|, and|(S3,A,S 6)|>>|(S2,A,S 6)|, such that: m1→0andm2→0. Therefore:
ˆCSRRD (R)(s,a,s′)≈R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4) +γ2R(S1,A,S 5)
−γR(S2,A,S 6)] (37)
with the corresponding residual shaping:
ϕsrrd≈E[γϕ(S3)−γ2ϕ(S4) +γ3ϕ(S5)−γ2ϕ(S6)]. (38)
27Under review as submission to TMLR
Fromϕsrrd(Equation 38), we can extract the constant Kϕ=E[γϕ(S3)−γ2ϕ(S4)]since it does not vary
with (s,a,s′). Therefore, the effective residual shaping is given by:
ϕ˜srrd≈E[γ3ϕ(S5)−γ2ϕ(S6)],
hence:
|ϕ˜srrd|≤γ3|E[ϕ(S5)]|+γ2|(−E[ϕ(S6)])|/vextendsingle/vextendsingle≤/vextendsingle/vextendsingleγ3M+γ2M/vextendsingle/vextendsingle≤2M
Following Definition 6, and applying it to ˆCSRRD(Equation 37): U(ˆCSRRD (R)) = 6Z, such that:
RSE (ˆCSRRD (R)) =|ϕ˜srrd|
U(ˆCSRRD (R)≤M
3Z.
∴Based on all the three cases, we have:
RSE (ˆCSRRD (R))≤M
3Z. (39)
Conclusion
Based on the upper bounds of the RSE values aggregated from the best, average and worst case scenarios
of transition sparsity (Equation 39, 34, 30), we can conclude that:
RSE(ˆCSRRD )≤M
3Z;RSE(ˆCDARD )≤2M
3Z;RSE(ˆCEPIC )≤M
Z,
The RSE serves as a valuable theoretical measure to evaluate the robustness of the pseudometrics by quan-
tifying the influence of residual shaping errors relative to the rewards. Its important to acknowledge that
the RSE is a conservative measure, since the residual shaping is normalized by U(CS(R)). Therefore, in
practice, it may not guarantee the order of performance predicted in Theorem 2, as the impact of residual
shaping could be more pronounced when normalized by smaller reward values. Despite this limitation, the
RSE still offers a meaningful theoretical measure on the robustness of the pseudometrics.
A.2 Residual Shaping
Derivation of ϕres1:ApplyingC1(Equation 9) to a shaped reward R′(s,a,s′) =R(s,a,s′)+γϕ(s′)−ϕ(s):
C1(R′)(s,a,s′) =R′(s,a,s′) +E[γR′(s′,A,S 1)−R′(s,A,S 2)−γR′(S3,A,S 4)]
=R(s,a,s′) +γϕ(s′)−ϕ(s) +E[γ(R(s′,A,S 1) +γϕ(S1)−ϕ(s′))
−(R(s,A,S 2) +γϕ(S2)−ϕ(s))−γ(R(S3,A,S 4) +γϕ(S4)−ϕ(S3))]
=C1(R)(s,a,s′) +E[γ2ϕ(S1)−γ2ϕ(S4) +γϕ(S3)−γϕ(S2)]
Hence,C1(R)(s,a,s′)yields the residual shaping:
ϕres1=E[γ2ϕ(S1)−γ2ϕ(S4) +γϕ(S3)−γϕ(S2)].
Derivation of ϕres2:ApplyingC2(Equation 11) to shaped reward R′(s,a,s′) =R(s,a,s′)+γϕ(s′)−ϕ(s):
C2(R′)(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s) +E[γ(R(s′,A,S 1) +γϕ(S1)−ϕ(s′))
−(R(s,A,S 2) +γϕ(S2)−ϕ(s))−γ(R(S3,A,S 4) +γϕ(S4)−ϕ(S3))
+γ2(R(S1,A,k 1) +γϕ(k1)−ϕ(S1))−γ(R(S2,A,k 2) +γϕ(k2)−ϕ(S2))
+γ(R(S3,A,k 3) +γϕ(k3)−ϕ(S3))−γ2(R(S4,A,k 4) +γϕ(k4)−ϕ(S4))].
=C2(R)(s,a,s′) +E[γ3ϕ(k1)−γ3ϕ(k4) +γ2ϕ(k3)−γ2ϕ(k2)]
28Under review as submission to TMLR
Hence,C2(R)(s,a,s′)yields the residual shaping:
ϕres2=E[γ3ϕ(k1)−γ3ϕ(k4) +γ2ϕ(k3)−γ2ϕ(k2)].
A.3 The Sparsity Resilient Canonically Shaped Reward is Invariant to Shaping
Proposition 2. (The Sparsity Resilient Canonically Shaped Reward is Invariant to Shaping) Let R:
S×A×S be a reward function and ϕ:S→Rbe a state potential function. Applying CSRRDto a po-
tentially shaped reward R′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s)satisfies:CSRRD (R) =CSRRD (R′).
Proof.Let’s apply CSRRD, Definition 3, to a shaped reward R′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s):
CSRRD (R′)(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s) +E[γ[R(s′,A,S 1) +γϕ(S1)−ϕ(s′)]
−[R(s,A,S 2) +γϕ(S2)−ϕ(s)]−γ[R(S3,A,S 4) +γϕ(S4)−ϕ(S3)]
+γ2[R(S1,A,S 5) +γϕ(S5)−ϕ(S1)]−γ[R(S2,A,S 6) +γϕ(S6)−ϕ(S2)]
+γ[R(S3,A,S 6) +γϕ(S6)−ϕ(S3)]−γ2[R(S4,A,S 5) +γϕ(S5)−ϕ(S4)]],
Regrouping the reward terms and the potentials, this reduces to:
CSRRD (R′)(s,a,s′) =CSRRD (R)(s,a,s′) + (γϕ(s′)−γE[ϕ(s′)]) + (−ϕ(s) +E[ϕ(s)])
+E[γ2(ϕ(S1)−ϕ(S1))] +E[γ(−ϕ(S2) +ϕ(S2))] +E[γ(ϕ(S3)−ϕ(S3))]
+E[γ2(−ϕ(S4) +ϕ(S4))] +E[γ3(ϕ(S5)−ϕ(S5))] +E[γ2(−ϕ(S6) +ϕ(S6))]
SinceE[γϕ(s′)] =γϕ(s′)andE[ϕ(s)] =ϕ(s), this leads to:
CSRRD (R′)(s,a,s′) =CSRRD (R)(s,a,s′).
A.4 Invariance under the Sample-Based SRRD Approximation
For the sample-based SRRD canonical reward (Definition 4), under transition sparsity, some rewards may
be undefined since the associated transitions may be unsampled. However, when all the necessary rewards
are defined, policy invariance can be achieved. To show this, we will first derive Lemma 1, then present the
proof which assumes that all the necessary rewards are available.
Lemma 1. Letϕ:S→Rbe a potential function. Given states xi∈Sandxj∈S, then:
1
n1n2n1/summationdisplay
i=1n2/summationdisplay
j=1(γϕ(xi)−ϕ(xj)) =γ
n1n1/summationdisplay
i=1ϕ(xi)−1
n2n2/summationdisplay
j=1ϕ(xj).
Proof.
1
n1n2n1/summationdisplay
i=1n2/summationdisplay
j=1(γϕ(xi)−ϕ(xj)) =1
n1n2
γn1/summationdisplay
i=1
n2/summationdisplay
j=1ϕ(xi)
−n2/summationdisplay
j=1/parenleftiggn1/summationdisplay
i=1ϕ(xj)/parenrightigg

Notice that ϕ(xi)is independent of jandϕ(xj)is independent of i,thus,
=γ
n1n2n1/summationdisplay
i=1n2ϕ(xi)−1
n1n2n2/summationdisplay
j=1n1ϕ(xj)
=γ
n1n1/summationdisplay
i=1ϕ(xi)−1
n2n2/summationdisplay
j=1ϕ(xj).
29Under review as submission to TMLR
Proposition 4. Given transition sets (Si,A,Sj)needed to compute each reward expectation term in ˆCSRRD,
if the transitions fully cover the cross-product Si×A×Sj, then for a shaped reward R′(s,a,s′) =R(s,a,s′)+
γϕ(s′)−ϕ(s), the sample-based SRRD approximation is invariant to shaping.
Proof.
ˆCSRRD (R′)(s,a,s′)≈R(s,a,s′) +γϕ(s′)−ϕ(s)
+γ
N1/summationdisplay
(x1,u)∈X1[R(s′,u,x 1) +γϕ(x1)−ϕ(s′)]−1
N2/summationdisplay
(x2,u)∈X2[R(s,u,x 2) +γϕ(x2)−ϕ(s)]
−γ
N3N4/summationdisplay
(x3,u)∈X3/summationdisplay
(x4,·)∈X4[R(x3,u,x 4) +γϕ(x4)−ϕ(x3)]
+γ2
N1N5/summationdisplay
(x1,u)∈X1/summationdisplay
(x5,·)∈X5[R(x1,u,x 5) +γϕ(x5)−ϕ(x1)]
−γ
N2N6/summationdisplay
(x2,u)∈X2/summationdisplay
(x6,·)∈X6[R(x2,u,x 6) +γϕ(x6)−ϕ(x2)]
+γ
N3N6/summationdisplay
(x3,u)∈X3/summationdisplay
(x6,·)∈X6[R(x3,u,x 6) +γϕ(x6)−ϕ(x3)]
−γ2
N4N5/summationdisplay
(x4,u)∈X4/summationdisplay
(x5,·)∈X5[R(x4,u,x 5) +γϕ(x5)−ϕ(x4)].
Rearranging terms, the above equation can be written as:
ˆCSRRD (R′)(s,a,s′) =ˆCSRRD (R)(s,a,s′) +ϕresiduals,
where:
ϕresiduals=γϕ(s′)−ϕ(s) +γ
N1/summationdisplay
(x1,u)∈X1[γϕ(x1)−ϕ(s′)]−1
N2/summationdisplay
(x2,u)∈X2[γϕ(x2)−ϕ(s)]
−γ
N3N4/summationdisplay
(x3,u)∈X3/summationdisplay
(x4,·)∈X4[γϕ(x4)−ϕ(x3)] +γ2
N1N5/summationdisplay
(x1,u)∈X1/summationdisplay
(x5,·)∈X5[γϕ(x5)−ϕ(x1)]
−γ
N2N6/summationdisplay
(x2,u)∈X2/summationdisplay
(x6,·)∈X6[γϕ(x6)−ϕ(x2)] +γ
N3N6/summationdisplay
(x3,u)∈X3/summationdisplay
(x6,·)∈X6[γϕ(x6)−ϕ(x3)]
−γ2
N4N5/summationdisplay
(x4,u)∈X4/summationdisplay
(x5,·)∈X5[γϕ(x5)−ϕ(x4)].(40)
Applying Lemma 1 to Equation 40, and simplifying terms, we get:
ϕresiduals=γϕ(s′)−ϕ(s) +γ2
N1/summationdisplay
(x1,u)∈X1[ϕ(x1)]−γϕ(s′)−γ
N2/summationdisplay
(x2,u)∈X2[ϕ(x2)] +ϕ(s)
−γ2
N4/summationdisplay
(x4,u)∈X4[ϕ(x4)] +γ
N3/summationdisplay
(x3,u)∈X3[ϕ(x3)] +γ3
N5/summationdisplay
(x5,u)∈X5[ϕ(x5)]−γ2
N1/summationdisplay
(x1,u)∈X1[ϕ(x1)]
−γ2
N6/summationdisplay
(x6,u)∈X6[ϕ(x6)] +γ
N2/summationdisplay
(x2,u)∈X2[ϕ(x2)] +γ2
N6/summationdisplay
(x6,u)∈X6[ϕ(x6)]−γ
N3/summationdisplay
(x3,u)∈X3[ϕ(x3)]
−γ3
N5/summationdisplay
(x5,u)∈X5[ϕ(x5)] +γ2
N4/summationdisplay
(x4,u)∈X4[ϕ(x4)] = 0
30Under review as submission to TMLR
Therefore:
ˆCSRRD (R′) =ˆCSRRD (R).
A.5 Pseudometric Equivalence Under Full Coverage
Proposition3. Given the state subset S⊆S, the SRRD, DARD, and EPIC canonical rewards are equivalent
under full coverage when: S=S1=...=S6for SRRD; S=S′=S′′for DARD, and S=S′for EPIC.
Proof.
CEPIC (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′)−R(s,A,S′)−γR(S,A,S′)]
CDARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′′)−R(s,A,S′)−γR(S′,A,S′′)]
CSRRD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,S 5)−γR(S2,A,S 6) +γR(S3,A,S 6)−γ2R(S4,A,S 5)]
Under full coverage when every state s∈Sis connected by Aactions to every other state s′∈S. Then,
S=S′=S′′=S1=S2=S3=S4=S5=S6, such that:
CEPIC =CDARD =CSRRD =R(s,a,s′) +E[γR(s′,A,S )−R(s,A,S )−γR(S,A,S )].(41)
A.6 Repeated Canonicalization Under Full Coverage
Proposition 5. Given the state subset S⊆S, under full coverage when: S=S1=...=S6for SRRD;
S=S′=S′′for DARD, and S=S′for EPIC. Then: CSRRD,CEPIC, andCDARDcannot be further
canonicalized.
Proof.FromProposition3, weshowedthatunderfullcoverage, CS=CEPIC =CDARD =CSRRD. Applying
CSto canonicalize a previously canonicalized reward we get:
CS[CS(R)(s,a,s′)] =CS[R(s,a,s′) +E[γR(s′,A,S )−R(s,A,S )−γR(S,A,S )]]
=CS(R)(s,a,s′) +γE[CS(R(s′,A,S ))]−E[CS(R(s,A,S ))]−γE[CS(R(S,A,S ))]
=CS(R)(s,a,s′)
+γE[R(s′,A,S ) +E[γR(S,A,S )−R(s′,A,S )−γR(S,A,S )]]
−E[R(s,A,S ) +E[γR(S,A,S )−R(s,A,S )−γR(S,A,S )]]
−γE[R(S,A,S ) +E[γR(S,A,S )−R(S,A,S )−γR(S,A,S )]]
=CS(R)(s,a,s′)
+γE[R(s′,A,S )−R(s′,A,S )] +E[γR(S,A,S )−γR(S,A,S )]
−E[R(s,A,S )−R(s,A,S )] +E[γR(S,A,S )−γR(S,A,S )]
−γE[R(S,A,S )−R(S,A,S )] +E[γR(S,A,S )−γR(S,A,S )]
=CS(R)(s,a,s′)
31Under review as submission to TMLR
A.7 Regret Bound
In this section, we establish a regret bound in terms of the SRRD distance. The procedure for the analysis
is adapted from the related work on EPIC by Gleave et al. (2020). Given reward functions RAandRB
and their optimal policies π∗
Aandπ∗
B, we show that the regret of using policy π∗
Binstead of a policy π∗
A
is bounded by a function of DSRRD (RA,RB). We also show that as DSRRD (RA,RB)→0, the regret goes
towards 0suggesting that π∗
A≈π∗
B. The concept of regret bounds is important as it shows that differences
inDSRRDreflect differences between the optimal policies induced by the input rewards.
For our analysis, we will use the following Lemmas:
Lemma 2. Letfbe a one-dimensional vector of real numbers and fi⊆f. Then:
||fi||2≤||f||2 (42)
Proof.Supposefhasnelements and fihaskelements. Since fi⊆f, every element in fiis also inf, and
k≤n. Therefore,/summationtextf2≥/summationtextf2
i(Euclidean distance always positive) such that: ||fi||2≤||f||2.
Lemma 3. LetRA,RB:S×A×S→ Rbe reward functions with corresponding optimal policies π∗
Aand
π∗
B. LetDπ(t,st,at,st+1)denote the distribution over trajectories that policy πinduces at time step t. Let
D(s,a,s′)be the coverage distribution over transitions S×A×S . Suppose that there exists some K > 0
such thatKD(st,at,st+1)≥D(t,st,at,st+1)for all time steps t∈N, triplesst,at,st+1∈S×A×S and
policiesπ∈{π∗
A,π∗
B}. Then the regret under RAfrom executing π∗
Boptimal for RBinstead ofπ∗
Ais at most:
GRA(π∗
A)−GRA(π∗
B)≤2K
1−γDL1,D(RA,RB).
whereDL1,Dis a pseudometric in L1space, andGR(π)resembles the return of Runder a policy π.
Proof.See Gleave et al. (2020) Lemma A.11.
Lemma 4. LetRA,RB:S×A×S→ Rbe reward functions. Let π∗
Aandπ∗
Bbe policies optimal for reward
functionsRAandRB. Suppose the regret under the standardized reward RS
Afrom executing π∗
Binstead of
π∗
Ais upper bounded by some U∈R:
GRS
A(π∗
A)−GRS
A(π∗
B)≤U. (43)
Assuming that S3is identically distributed to S4inCSRRD, then the regret is bounded by:
GRA(π∗
A)−GRA(π∗
B)≤8U∥RA∥2. (44)
Proof.Following (Gleave et al., 2020), we can express the standardized reward as:
RS=CSRRD (R)
∥CSRRD (R)∥2, (45)
InCSRRD(Equation 13), the states S1andS5are based on s′, andS2andS6are based on s. Assuming
thatS3is identically distributed to S4, we can see that CSRRDis a potential function, where, ϕ(s′) =
E[R(s′,A,S 1)] +γR(S1,A,S 5)−γR(S4,A,S 5)],andϕ(s) =E[R(s,A,S 2)] +γR(S2,A,S 6)−γR(S3,A,S 6)].
Therefore,CSRRDis simplyRshaped by some potential Φ, such that:
GCSRRD (R)(π) =GR(π)−Es0∼d0[Φ(s0)]
Therefore, we can write:
GRS(π) =1
∥CSRRD (R)∥2GCSRRD (R)(π) =1
∥CSRRD (R)∥2(GR(π)−Es0∼d0[Φ(s0)]), (46)
32Under review as submission to TMLR
where,s0depends only on the initial state distribution d0, but notπ. Applying Equation 46 to π∗
Aandπ∗
B:
GRS(π∗
A)−GRS(π∗
B) =1
∥CSRRD (RA)∥2(GRA(π∗
A)−GRA(π∗
B)). (47)
Combining Equation 47 and 43:
GRA(π∗
A)−GRA(π∗
B)≤U∥CSRRD (RA)∥2. (48)
We now bound∥CSRRD (RA)∥2in terms of∥RA∥2. The SRRD canonical reward is expressed as:
CSRRD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,S 5)−γR(S2,A,S 6) +γR(S3,A,S 6)−γ2R(S4,A,S 5)]
Now, using the triangular inequality rule on the L2distance, and linearity of expectation:
||CSRRD (R)(s,a,s′)||2≤||R(s,a,s′)||2+E[γ||R(s′,A,S 1)||2+||−R(s,A,S 2)||2+γ||−R(S3,A,S 4)||2
+γ2||R(S1,A,S 5)||2+γ||−R(S2,A,S 6)||2+γ||R(S3,A,S 6)||2+γ2||−R(S4,A,S 5)||2]
Using Lemma 2, the L2norm of each reward subspace is such that:
||R(Si,Aj,Sk)||2≤||R(S,A,S′)||2=||R||2. (49)
Therefore,
||CSRRD (R)(s,a,s′)||2≤8||R||2 (50)
Combining Equation 50 and 48 we get:
GRA(π∗
A)−GRA(π∗
B)≤8U||R||2.
Lemma 5. Given a reward function R:S×A×S→R, then: E[CSRRD (R)(S,A,S′)] = 0, ifSandS′are
identically distributed.
Proof.Applying the transitions (S,A,S′)intoCSRRD(Equation 13), S′=S4=S1=S2=S5=S6, and
S=S3, such that:
CSRRD (R)(S,A,S′) =R(S,A,S′) +E[γR(S′,A,S′)−R(S,A,S′)−γR(S,A,S′)]
Therefore:
E[CSRRD (R)(S,A,S′)] =E[R(S,A,S′) +E[γR(S′,A,S′)−R(S,A,S′)−γR(S,A,S′)]
ifSis identically distributed to S′, then E[R(S′,A,S′) =E[R(S,A,S′)], hence:
E[CSRRD (R)(S,A,S′)] =E[R(S,A,S′) +E[γR(S,A,S′)−R(S,A,S′)−γR(S,A,S′)] = 0
Theorem 1. LetRA,RB:S×A×S→Rbe reward functions with respective optimal policies, π∗
A,π∗
B. Let
γbe a discount factor, Dπ(t,st,at,st+1)be the distribution over transitions S×A×S induced by policy πat
timet, andD(s,a,s′)be the coverage distribution. Suppose there exists K > 0such thatKD(st,at,st+1)≥
Dπ(t,st,at,st+1)for all times t∈N, triples (st,at,st+1)∈S×A×S and policies π∈{π∗
A,π∗
B}. Then the
regret under RAfrom executing π∗
Binstead ofπ∗
Ais at most:
GRA(π∗
A)−GRA(π∗
B)≤32K∥RA∥2(1−γ)−1DSRRD (RA,RB),
whereGR(π)is the return of policy πunder reward R.
33Under review as submission to TMLR
Proof.SinceCSRRDis mean zero for transition inputs (S,A,S′)(see Lemma 5), from Gleave et al. (2020)
[A.4], it follows that:
DSRRD (RA,RB) =1
2/vextenddouble/vextenddoubleRS
A(S,A,S′)−RS
B(S,A,S′)/vextenddouble/vextenddouble2
2. (51)
From Lemma A.10 in (Gleave et al., 2020), the L1norm of a function is upper bounded by its L2norm on
a probability space, such that:
DL1,D(RS
A,RS
B) =/vextenddouble/vextenddoubleRS
A(S,A,S′)−RS
B(S,A,S′)/vextenddouble/vextenddouble
1≤2DSRRD (RA,RB). (52)
Combining Lemma 3 and Equation 52:
GRSRRD
A(π∗
A)−GRSRRD
A(π∗
B)≤2K
1−γDL1,D(RSRRD
A,RSRRD
B )≤4K
1−γDSRRD (RA,RB).(53)
Applying Lemma 4, we get:
GRA(π∗
A)−GRA(π∗
B)≤32K∥RA∥2
1−γDSRRD (RA,RB). (54)
As shown, when DSRRD→0.
A.8 Generalized SRRD Extensions
The following steps result in the generalized formula for potential SRRD extensions.
1. To create SRRD, the first step is to adopt the desirable characteristics from both DARD and EPIC
(refer to Section 4), and derive C1as follows:
C1(R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)].
C1yields the residual shaping term: ϕres1=E[γ2ϕ(S1)−γ2ϕ(S4) +γϕ(S3)−γϕ(S2)].
2. To cancel E[ϕ(Si)],∀i∈{1,...,4}, we add rewards R(Si,A,k1
i)to induce potentials γϕ(k1
i)−ϕ(Si),
which results in C2:
C2(R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,k1
1)−γ2R(S4,A,k1
4) +γR(S3,A,k1
3)−γR(S2,A,k1
2)].
C2yields the residual shaping: ϕres2=E[γ3ϕ(k1
1)−γ3ϕ(k1
4) +γ2ϕ(k1
3)−γ2ϕ(k1
2)].
3. To cancel E[ϕ(k1
i)], we add rewards R(k1
i,A,k2
i)to induce potentials γϕ(k2
i)−ϕ(k1
i), yieldingC3:
C3(R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,k1
1)−γ2R(S4,A,k1
4) +γR(S3,A,k1
3)−γR(S2,A,k1
2)
+γ3R(k1
1,A,k2
1)−γ3R(k1
4,A,k2
4) +γ2R(k1
3,A,k2
3)−γ2R(k1
2,A,k2
2)]
C3yields the residual shaping: ϕres3=E[γ4ϕ(k2
1)−γ4ϕ(k2
4) +γ3ϕ(k2
3)−γ3ϕ(k2
2)].
4. As we can see, this process results in the generalized formula:
Cn(R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,k1
1)−γ2R(S4,A,k1
4) +γR(S3,A,k1
3)−γR(S2,A,k1
2)
+γ3R(k1
1,A,k2
1)−γ3R(k1
4,A,k2
4) +γ2R(k1
3,A,k2
3)−γ2R(k1
2,A,k2
2)
···
+γnR(kn−2
1,A,kn−1
1)−γnR(kn−2
4,A,kn−1
4) +γn−1R(kn−2
3,A,kn−1
3)
−γn−1R(kn−2
2,A,kn−1
2)],
34Under review as submission to TMLR
where,n≥3.Cnyields the residual shaping:
ϕn=E[γn+1ϕ(kn−1
1)−γn+1ϕ(kn−1
4) +γnϕ(kn−1
3)−γnϕ(kn−1
2)].
•Looking at ϕn, asnincreases, we generally multiply the state distributions, kiby (≈γn). Therefore,
the upper bound magnitude of ϕnsignificantly decreases since 0≤γ < 1, and each|ϕ(ki)|≤M,
whereMis the upper bound potential for all distributions ki⊆SD(see Appendix A.1.2). Therefore,
asnapproaches infinity, ϕnapproaches 0.
•The advantage of the generalized SRRD form is that ϕnapproaches 0asnincreases, without any
assumptions on the distribution of a reward sample. The challenge, however, is that many kiterms
need to be computed making the process very expensive and difficult to implement. Therefore, a
smallernis preferable. In SRRD, we choose n= 2, then use our intuition to select sets, ki, which
further reduces the residual shaping.
A.9 Computational Complexity
The pseudometrics discussed in this paper all utilize a double sampling method that uses the batches: BV
ofNVsampled transitions, and BMofNMstate-action pairs. The sample-based approximations for the
methods have the following computational complexities:
EPIC Complexity:
CEPIC (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′)−R(s,A,S′)−γR(S,A,S′)].
•For all transitions in BV, approximating E[R(S,A,S′)]fromBMtakes approximately O(N2
M)com-
plexity since we iterate BMin a double loop (see Equation 6). However, this expectation is the same
for all transitions, hence it can be computed once.
•For each transition (s,a,s′)∈BV, the reward expectations, E[R(s′,A,S′)]andE[R(s,A,S′)]can
be approximated in one iteration through BM, resulting in O(NM)time complexity per transition.
Since the computation varies based on (s,a,s′), the overall complexity is O(NVNM).
Therefore, the overall complexity for EPIC is O(max(NVNM,N2
M)). WhenNMis significantly larger com-
pared toNV, the complexity is approximately O(N2
M), and ifNVis significantly larger relative to NM, the
complexity is approximately O(NVNM).
DARD Complexity:
CDARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′′)−R(s,A,S′)−γR(S′,A,S′′)].
•For each transition (s,a,s′)∈BV, the reward expectations, E[R(s′,A,S′′)]andE[R(s,A,S′)]can be
approximated through a single iteration of BM, resulting in O(NM)time complexity per transition.
The reward expectation, E[R(S′,A,S′′)]can be computed via a double loop through BM, and it
varies based on (s,a,s′), resulting in O(N2
M)time complexity per transition.
Therefore, the overall complexity for DARD is O(NVN2
M).
SRRD Complexity:
CSRRD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,S 5)−γR(S2,A,S 6) +γR(S3,A,S 6)−γ2R(S4,A,S 5)].
35Under review as submission to TMLR
•For all transitions in BV, approximating E[R(S3,A,S 4)]fromBMtakes approximately O(N2
M)
complexity since we iterate BMin a double loop (see Equation 15). However, this expectation is
the same for all transitions, hence it can be computed once.
•For each transition (s,a,s′)∈BV, the reward expectations, E[R(s′,A,S 1)]andE[R(s,A,S 2)]can
be approximated in one iteration through BM, resulting in O(NM)time complexity per transition.
The reward expectations, E[R(S1,A,S 5)],E[R(S2,A,S 6)],E[R(S3,A,S 6)]andE[R(S4,A,S 5)], all
require double iterations through BM, and they vary based on (s,a,s′), hence, they take O(N2
M)
complexity per transition.
Therefore, the overall complexity for SRRD is O(NVN2
M).
A.10 SRRD State Definitions
Figure 4 is a graph showing transitions in a reward sample with 10statesSD={x0,...,x 9}, and a single
actionAD={a1}between state transitions. The goal here is to illustrate an example showing how states
{S1,...,S 6}are defined in SRRD, as well as the state relationships: (S1⊆S4)and(S2⊆S3), which make
SRRD robust to unsampled transitions.
Figure 4: A transition graph with 10states{x0,...x 9}, and a single action {a1}. State subsets are defined
based on the transition: (x0,a1,x1).
The Sparsity Resilient Canonically Shaped Reward is given by:
CSRRD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4) +γ2R(S1,A,S 5)
−γR(S2,A,S 6) +γR(S3,A,S 6)−γ2R(S4,A,S 5)],
where:S1are subsequent states to s′, andS2are subsequent non-terminal states to s.S3encompasses all
initial states from all transitions; S4,S5, andS6are subsequent states to S3,S1andS2, respectively.
36Under review as submission to TMLR
Following the SRRD definition, the states in Figure 4, are defined as follows:
s: statex0.
s′: statex1.
S1(subsequent to s′):{x2,x5,x6,x9}.
S2(subsequent non-terminal states to s):{x1,x3,x4,x5,x7}.
S3(initial states from all transitions): {x0,x1,x2,x3,x4,x5,x7,x9}. terminal states x6andx8not included
S4(subsequent states to S3):{x1,x2,x3,x4,x5,x6,x7,x8,x9}. starting state x0not included
S5(subsequent states to S1):{x2,x4,x6,x7,x8}
S6(subsequent states to S2):{x1,x2,x3,x4,x5,x6,x8,x9}
Transition Relationships (See Section 4 for Reference)
1.S1⊆S4, therefore, (S1,A,S 5)⊆(S4,A,S 5).
2.S2⊆S3, therefore, (S2,A,S 6)⊆S3,A,S 6).
B Additional Considerations
B.1 Deviations from Potential Shaping
Figure 5: (Non-Potential Shaping Effects): As the severity of randomly generated noise increases from part
(a) to (c), rewards deviate more from potential shaping, hence, all the pseudometrics degrade in performance.
In the end (part (c)), the pseudometrics perform similarly to DIRECT, showing that canonicalization does
not yield any additional advantages when the rewards significantly deviate from potential shaping.
Rewardcomparisonpseudometricsaregenerallydesignedtoeliminatepotentialshaping, andinthisstudy, we
examine how deviations from potential shaping can affect the performance of these pseudometrics. Within a
20×20Gridworld domain, we generate a ground truth ( GT) polynomial reward function and a corresponding
shaped reward function ( SH), both with full coverage ( 100%transitions). From GTandSH, we sample
rewardsRandR′using uniform policy rollovers. For both samples, we add additional noise, N, with the
following severity levels: None:N= 0,Mild:|N|≤max(|R′|), andHigh:|N|≤5 max(|R′|), whereN
is randomly generated from a uniform distribution within bounds defined by the severity levels. Thus, the
updated shaped reward is given by:
R′′=R′+N.
Figure 5 shows the performance variation of the reward comparison pseudometrics to different noise severity
levels. When N= 0(noise free), the difference between SRRD, EPIC, DARD, and DIRECT is the highest,
with a performance order: SRRD >DARD>EPIC>DIRECT, which demonstrates SRRD’s performance
37Under review as submission to TMLR
advantage over other pseudometrics under potential shaping. As the impact of Nincreases, the shaped
rewardR′′becomesalmostentirelycomprisedofnoise, andtheshapingcomponentsignificantlydeviatesfrom
potential shaping. As shown, SRRD’s performance gap over other pseudometrics significantly diminishes.
At high severity (Figure 5c), SRRD’s performance is nearly identical to all other pseudometrics, including
DIRECT, which does not involve any canonicalization. In conclusion, these results still demonstrate SRRD’s
superiority in canonicalizing rewards even with minor random deviations from potential shaping (Figure 5
b). However, as the rewards become non-potentially shaped, all pseudometrics generally become ineffective,
performing similarly to non-canonicalized techniques such as DIRECT.
B.2 Environments with Infinite or Continuous State and Action Spaces
As previously mentioned in Section 3 and 4, computing the exact reward comparison distances is only
feasible in small environments with finite discrete states and actions, where all the possible transitions
between states and actions can be enumerated. For complex environments with infinite or continuous states
and actions, computing the exact values for DEPIC,DDARDandDSRRDbecomes impractical, hence, sample-
based approximation methods have been developed. These approximations take transition inputs composed
of discrete states and actions, and when applied to continuous environments, it is essential to discretize the
state and action observations from the continuous signals as demonstrated in prior works (Gleave et al., 2020)
and Wulfe et al. (2022). In our experiments, these approximations are necessary in environments such as the
Drone Combat Scenario and StarCraft II. While not necessarily continuous in a strict mathematical sense,
StarCraft II operates in real-time with partial observability, multiagent decision-making, durative actions,
and asynchronous, parallel action execution. The game updates at approximately 16times per second, and
it effectively has infinite states and actions. This fluid, real-time interactions aligns it more with continuous
decision-making scenarios, and to manage the complexity, we perform feature preprocessing to discretize and
cluster state and action observations before applying the sample-based approximations.
An intriguing area for future research is the integration of function approximation to generalize reward
canonicalization to reward functions represented as neural networks. While function approximation might
notbenecessaryforstraightforwardrewardcomparisons—wherethegoalistoretrieveasimilaritydistance—
they become essential in applications requiring the canonicalization of the entire reward functions, such as
standardizing rewards during IRL for example. Our initial proposed approach involves training a neural
network to predict canonical rewards based on batches of transition observations. In the training process,
the canonicalized rewards for the batch can be approximated via sample-based methods, and then the neural
network aims to predict the canonicalized rewards from the input transition batch. The network iteratively
learns to predict the canonicalized rewards by minimizing the difference between its predictions and the
sample-based approximations.
B.3 Sample-based Approximations using Unbiased Estimates
In this paper, the sample-based SRRD approximation presented adopts a similar form to that of EPIC
and DARD, primarily to maintain consistency with well-established methods. This approach utilizes two
types of samples: BV, a batch of transitions of size NV, andBM, a batch of state-action pairs of size NM.
Each transition in BVis canonicalized using state-action pairs from BM. The advantage is that if BMis
representativeofthejointstate-actiondistributionoftherewardfunction(orsample),canonicalizationcanbe
highly effective even with smaller sizes of BM. Since each transition in BVis canonicalized using state-action
pairs fromBM, the computational load required during canonicalization can be reduced compared to using
all possible transitions. For example, suppose we have a reward function with 10,000states and 100actions.
Inthiscase, thetotalnumberoftransitions = 10,000×100×10,000 = 1010. However, usingthesample-based
SRRD approximation method, we could generate a sample BVwith 100transitions, and another sample
BM, with 100state-action pairs. Each transition in BVis then canonicalized using state-action pairs from
BM, and the total number of transitions needed is approximately NV∗NM2≈100∗1002= 106transitions
(see Appendix A.9); which is way less than the transitions needed for the full computation.
The double-batch sampling approach (using BVandBM)generally works well in environments where tran-
sition sparsity is not a major problem. However, when transition sparsity is a concern, a large fraction of
38Under review as submission to TMLR
transitions generated from the combination of BVandBMmight be unsampled, and can have undefined
reward values, which can introduce errors in canonicalization. Our proposed pseudometric, SRRD, is much
more robust under these conditions compared to both EPIC and DARD, even though it uses a similar sam-
pling form. An alternative approach to the double-batch sampling method is to use unbiased estimates. The
unbiased estimate approximations are described as follows:
ˆCEPIC (R)(s,a,s′)≈R(s,a,s′) +γ
N1/summationdisplay
R(s′,u,x′)−1
N2/summationdisplay
R(s,u,x′)−γ
N3/summationdisplay
R(x,u,x′),(55)
where:{(s′,u,x′)}is the set of sampled transitions that start from the state s′, with the total number of
transitions equal to N1;{(s,,u,x′)}is the set of sampled transitions that start from the state s, with the
total number of transitions equal to N2; and{(x,u,x′)}is the set of all the sampled transitions, with the
total number of transitions equal to N3.
ˆCDARD (R)(s,a,s′)≈R(s,a,s′) +γ
N1/summationdisplay
R(s′,u,x′′)−1
N2/summationdisplay
R(s,u,x′)−γ
N3/summationdisplay
R(x′,u,x′′),(56)
where:{(s′,u,x′′)}, is the set of all sampled transitions that start from the state s′, with the total number
of transitions equal to N1;{(s,,u,x′)}is the set of all sampled transitions that start from the state s, with
the total number of transitions equal to N2; and{(x′,u,x′′)}is the set of transitions that start from the
subsequent states of sto the subsequent states of s′, and have a total number of transitions equal to N3.
CSRRD (R)(s,a,s′)≈R(s,a,s′) +γ
N1/summationdisplay
R(s′,u,x 1)−1
N2/summationdisplay
R(s,u,x 2)−γ
N3/summationdisplay
R(x3,u,x 4)
+γ2
N4/summationdisplay
R(x1,u,x 5)−γ
N5/summationdisplay
R(x2,u,x 6) +γ
N6/summationdisplay
R(x3,u,x 6)−γ2
N7/summationdisplay
R(x4,u,x 5).
(57)
For SRRD, let X1represent the set of all subsequent states from s′;X2represent the set of all subsequent
states tos; andX3be the set of all initial states for transitions, while X4,X5, andX6denote the sets of
subsequent states from X3,X1, andX2, respectively. For all transitions, the set: {(x3,u,x 4)}contains all
sampled transitions where x3∈X3andx4∈X4, with the total number of transitions equal to N3. For each
sampled transition, (s,a,s′), the set:{(s′,u,x 1)}contains observed transitions starting from s′and ending
inx1∈X1, with a magnitude N1.{(s,u,x 2)}contains observed transitions starting from sand ending in
x2∈X2, with a magnitude N2.{(x1,u,x 5)}contains observed transitions starting from x1∈X1and ending
inx5∈X5, with a magnitude N4.{(x2,u,x 6)}contains observed transitions starting from x2∈X2and
ending inx6∈X6, with a magnitude N5.{(x3,u,x 6)}contains observed transitions starting from x3∈X3
and ending in x6∈X6, with a magnitude N6.{(x4,u,x 5)}contains observed transitions starting from
x4∈X4and ending in x5∈X5, with a magnitude N7.
Figure 6 shows results obtained for Experiment 1 (refer to Section 5.1), using sample-based approximations
relying on unbiased estimates, instead of the double-batch sampling approach. The obtained results show
a similar trend to the results obtained from the the double sampling approach, where, SRRD still yields
better results compared to both EPIC and DARD. In general, unbiased estimates tend to be more accurate
under transition sparsity, as shown in Figure 7, which shows results for both the double-sampling and the
unbiasedestimateapproaches. Overall, theunbiasedestimateapproachtendstooutperformthedouble-batch
approach, however the double-sampling approach catches up as the level of transition sparsity decreases. The
SRRD approach still outperforms both EPIC and SRRD especially when coverage is low (≤20%), and it also
performs well under the the double-batch sampling mechanism, highlighting its robustness in eliminating
potential shaping.
39Under review as submission to TMLR
Figure 6: (Unbiased Estimate Approximations). The figure shows results for Experiment 1 (refer to Section
5.1) which is conducted using unbiased estimates as approximations for the SRRD, DARD and EPIC. As
shown, results demonstrate a similar trend as the one obtained when using the double-batch sampling
approach reliant on BVandBM. The goal of the experiment is to compare the effectiveness of reward
comparison pseudometrics at identifying the similarity between potentially shaped reward functions under
two conditions: (a) limited sampling and (b) feasibility constraints. A more accurate pseudometric yields a
Pearson distance Dρclose to 0, indicating a high degree of similarity between shaped reward functions, while
a less accurate pseudometric results in Dρclose to 1. In (a), EPIC and DARD lag behind SRRD at low
coverage due to limited sampling, but their performance gradually improves as coverage increases. In (b),
movement restrictions significantly reduce transition coverage, negatively impacting both EPIC and DARD.
Figure 7: (Unbiased Estimates vs Double Batch Sampling). This figure presents the results for Experiment
1 under limited sampling for the Gridworld domain. It compares the performance of pseudometrics using
the double-sampling approach (with batches BvandBN) versus the unbiased estimate approach. For the
unbiased estimates, the results are plotted in bold lines and for the double-sampling approach, they are
plotted using broken lines. The initial ’u’ denotes the unbiased estimate methods, for example uSRRD.
The objective is to compare the difference in performance between unbiased estimates and the double-batch
sampling approach. As shown, the unbiased estimate approaches tend to outperform the double-batch
approaches. In both sampling approaches (double-sampling or unbiased estimates), SRRD also outperforms
both EPIC and DARD especially when coverage is low (≤20%). The SRRD approach also performs well
under the double-batch sampling approach, highlighting its robustness at eliminating potential shaping.
40Under review as submission to TMLR
B.4 Inferring Rewards for Unsampled Transitions via Regression
All the canonicalization methods discussed in this paper are susceptible to unsampled transitions, however,
CSRRDis more resilient since it primarily relies on forward transitions rather than non-forward transitions
(refer to Section 4.1). In this study, instead of relying solely on sampled transitions during canonicalization,
we explore the possibility of addressing transition sparsity by generalizing rewards from sampled transitions
to unsampled transitions via regression. Using a 20×20Bouncing Balls domain, we generate reward samples
using a uniform policy and compute the SRRD, DARD, and EPIC distances. To vary the coverage of the
samples, we adjust the number of trajectories generated from policy rollouts used to sample rewards, and
compute coverage as the fraction of the sampled transitions over the total number of transitions, |S×A×S|.
In this experiment, reward samples are only defined for feasible transitions and unfeasible transitions have
undefined reward values. However, during canonicalization, the reward values for the unsampled transitions,
are inferred via regression. Figure 8 shows the results obtained in this experiment:
Figure 8: (Incorporating Regressed Rewards) When the relationship between rewards and state-action fea-
tures is less complex, such as the case with polynomial rewards, learning the regression model is highly
effective. Consequently, incorporating rewards that are inferred via regression into canonicalization can be
beneficial, resulting in improved performance for regressDARD ,regressSRRD , andregressEPIC . However,
when the relationship between rewards and state-action features is complex, the learned regression model
might struggle to generalize well such as the case for random and sinusoidal rewards.
As shown in Figure 8, the success of incorporating regressed rewards depends on the nature of the reward
function. When the reward function is derived as a simple combination of state-action features, learning
the regression model is highly effective due to the less complex relationship between state-action features
and reward values. This is likely the case for polynomial rewards, where pseudometrics that incorporate
regressed rewards, generally outperform the original non-regression-based pseudometrics, especially as the
coverage increases. However, for sinusoidal and random reward functions, the relationship between state-
action features and rewards is much more complex, making it challenging to effectively learn a highly effective
model under transition sparsity. Consequently, the rewards learned via regression may not generalize well,
and the original sample-based approximations tend to outperform the regression-based approximations. In
general, the regressed SRRD approximation outperformed all other regressed-based approximations. This
superiority is attributed to the inherent nature of SRRD which relies more on forward transitions that are
likely in-distribution with the reward samples, than non-forward transitions that are more prone to being
out-of-distribution with the dynamics of the reward samples. Therefore, even when incorporating regression,
SRRD depends less on the regressed rewards compared to EPIC and DARD, resulting in more accurate
predictions. EPIC ideally requires full coverage, hence, under transition sparsity, it relies more heavily on
regressed rewards which makes it more susceptible to errors when the regression model cannot generalize
well. In this experiment, we compared linear regression, decision trees, and neural networks, and chose linear
regression since they yielded the best results in terms of accuracy. This could result from the lack of diverse
data, as rewards are only defined for feasible transitions, which are a small subset compared to the total
number of transitions that would be needed if no feasibility constraints were imposed.
41Under review as submission to TMLR
B.5 Sensitivity of SRRD to Sampling Policy
A crucial challenge in reward comparison tasks using reward samples is the fact that reward samples partially
represent the true reward function, and they might not fully capture the structure of the full reward function.
This section examines the performance of SRRD to variations in the policy used to extract reward samples.
The experiment is conducted within a 15×15Gridworld environment, where an agent’s objective is to
navigate from an initial state (0,0)to the target state (14,14). The agents selects actions from the set A=
{up,right,down,left}. The reward function is derived from predefined expert behaviors using Adversarial
Inverse Reinforcement Learning (AIRL). Figure 9 shows a reward function Rdiagonal, computed for an agent
with a diagonally oriented policy where for each state s:πdiagonal (up|s) = 0.1,πdiagonal (right|s) = 0.4,
πdiagonal (down|s) = 0.4andπdiagonal (left|s) = 0.1. Figure 10 shows a reward function Rtop, computed for an
agent with a policy oriented towards the upper segment of the grid where for each state s:πtop(up|s) = 0.7;
πtop(right|s) = 0.1,πtop(down|s) = 0.1, andπtop(left|s) = 0.1. In both figures, the reward for each transition
is represented by the triangular directional arrows, for example, in Figure 9, the reward from state (0,0)
to state (0,1)is2.5. The reward function is defined exclusively for feasible transitions, and the intensity
of rewards is depicted using three three colors: red for high rewards (>5), blue for moderate rewards
(2−5), and light brown for low rewards (<2). In this study, we compare the similarity between two shaped
reward samples derived from RdiagonalorRtopusing a specified reward sampling policy. We then analyze
the variation in DSRRDbased on different sampling policies. The experiment is repeated over 100trials.
Figure 9: ( Rdiagonal). The reward function is generated via AIRL from an agent that executes the policy
πdiagnoal, which favors movement along the grid’s main diagonal. High rewards are highlighted in red,
moderate rewards in blue, and low rewards in light brown.
42Under review as submission to TMLR
Figure 10: ( Rtop). The reward function is generated via AIRL from an expert agent that executes the policy
πtop, which favors movement towards the upper horizontal section of the grid.
Table 2: Sensitivity of SRRD to different reward sampling policies. The coverage is computed as the fraction
of sampled transitions, over the total number of feasible transitions.
Reward Sampling Policy
πuniform πdiagonal πtop πleft
# of Trajectories 50 100 50 100 50 100 50 100
Coverage (%) 33.2 43.4 56.9 74.6 11.91 14.3 10.86 13.03
DSRRDforRdiagonal 0.62 0.60 0.57 0.56 0.64 0.63 0.62 0.62
DSRRDforRtop 0.65 0.64 0.62 0.61 0.64 0.63 0.65 0.65
Table 2 summarizes results for testing SRRD’s sensitivity to different reward sampling policies πdiagonal,πtop
as well as: πuniform- utilizes a uniform policy across all four directions; and πleft- biased towards the left
segment of the grid. As shown, DSRRDvaries based on the reward sampling policy. In Rdiagonalwhen all
the feasible transitions are used in reward comparisons, DSRRD≈0.58, which is the benchmark value to
test the policy variations. This value closely matches the distance values obtained by πuniformandπdiagonal,
which both sample transitions that closely match the actual distribution of the benchmark rewards, Rdiagonal.
However, policies such as πtopandπleft, generally achieve distances that deviate from the benchmark score,
43Under review as submission to TMLR
since the reward distribution in the reward samples might be less representative of the structure of Rdiagonal.
Overall,DSRRDvaries between: [0.56−0.64], while the benchmark distance from Rdiagonalis0.58. InRtop,
when all feasible transitions are used in reward comparisons, DSRRD≈0.63. However, as shown in Table
2, the computed values for DSRRDvaries within the range [0.61−0.65], depending on the reward sampling
policy used. Specifically, πdiagonalyields lower distances than other sampling policies, since it likely missed
the high reward regions at the top of the grid and predominantly relied on regions with low reward variation.
As shown in the table, increasing the number of sampled trajectories from 50to100generally leads to lower
SRRD distances. However, this effect is less pronounced in this experiment because the reward function is
defined only for feasible transitions, limiting the variability and coverage inherently. In conclusion, SRRD is
sensitive to variations in the policies that generate the reward samples, since canonicalization relies on the
distribution of sampled transitions, which might not be representative of the actual transition distribution
under full coverage. In sampling rewards, it is desirable to ensure that, the sample has high coverage and
broad support over the distribution of the true reward function. We also performed this study with EPIC
and DARD, and they both yield higher distances within the range [0.63−0.7]for bothRdiagonalandRtop.
C Additional Experimental Details
C.1 Experiment 1
C.1.1 Reward Functions
Extrinsic reward values are manually defined using a combination of state and action features. For the
StarCraft II and Drone Combat domains, we use the default game score (also based on state and action
features), as the reward values. For the Gridworld and Bouncing Balls domains, in each reward function, the
reward value, R(s,a,s′)is derived from the decomposition of state and action features, where: (sf1,...,sfn)is
from the starting state, s;(af1,...,afm)is from the action, a; and (s′
f1,...,s′
fn)is from the subsequent state,
s′. For the Gridworld domain, these features are the (x,y)coordinates, and for the Bouncing Balls domain,
theseinclude (x,y,d ), wheredisthedistanceoftheobstaclenearesttotheball. Usingthefollowingrandomly
generated constants: {u1,...,un}for incoming state features; {w1,...,wm}for action features; {v1,...vn}for
subsequent state features; we created the following reward models:
•Linear:
R(s,a,s′) =u1sf1+...+unsfn+w1af1+...+wmafm+v1s′
f1+...+vns′
fn,
•Polynomial:
R(s,a,s′) =u1sα
f1+...+unsα
fn+w1aα
f1+...+wmaα
fm+v1s′α
f1+...+vns′α
fn,
where,αis randomly generated from 1−10, denoting the degree of the polynomial.
•Sinusoidal:
R(s,a,s′) =u1sin(sf1) +···+unsin(sfn) +w1sin(af1) +···+wmsin(afm)
+v1sin(s′
f1) +···+vnsin(s′
fn)
•Random
R(s,a,s′) =β,
where,βis a randomly generated reward for each given transition.
The same relationships are used to model potential functions, where: ϕ(s) =f(sf1,..,sfn), andfis the
relationship drawn from the set: {polynomial, sinusoidal, linear, random}. For the StarCraft II and Drone
Combat domains, we used the default game score provided by the game engine, as the reward function.
44Under review as submission to TMLR
For the StarCraft II domain, this score8focuses on the composition of unit and resource features as well as
actions within the domain. Since the Drone Combat environment is originally designed for a predator-prey
domain, we adapt the score9to essentially work for the Drone Combat scene (i.e instead of a predator being
rewarded for eating some prey, the reward is now an ally attacking an enemy).
C.1.2 Transition Sparsity:
Algorithm 1 summarizes the pseudocode for Experiment 1 to examine transition sparsity. To test the effect
of limited sampling, we run the algorithm with different number of rollouts (rollout count), dictated by
the arrayT. To test the effect of feasibility constraints, we run Algorithm 1 but with imposed movement
restrictions by setting ϵ= 0.
Algorithm 1 Analyzing the effect of limited sampling on reward distance
Input:
T- list of policy rollout counts,
E- number of experimental trials under same condition,
G- grid size,
RD - list to store reward distances at different coverages,
MC - maximum coverage ≈S×A×S.
Output: RD
1:generate GT - ground truth reward, SH - shaped reward from all possible transitions.
2:forrollout countinTdo
3:trial distance,trial coverage =list(),list()
4:fortrial inEdo
5:Bgt,Bsh=set(),set()
6:generate trajectories τgtandτshusing uniform policy rollouts.
7:for(s,a,s′)∈τgtdo
8:Bgt.add((s,a,s′))
9:end for
10:for(s,a,s′)∈τshdo
11:Bsh.add((s,a,s′))
12:end for
13:for(s,a,s′)∈Bgtand (s,a,s′)∈Bsh, retrieveR(s,a,s′)using GT and R′(s,a,s′)using SH,
respectively.
14:coverage =|Bgt∪Bsh|
MC
15:computedist(R,R′)using EPIC, SRRD, DARD, or DIRECT.
16:trial distance.append(dist(R,R′))
17:trial coverage.append( coverage)
18:end for
19:RD.append([mean(trial coverage), mean(trial distance)])
20:end for
8https://steemit.com/steemstem/@cpufronz/building-a-bot-for-starcraft-ii-2-the-starcraft-ii-environment
9https://github.com/koulanurag/ma-gym
45Under review as submission to TMLR
C.1.3 Transition Sparsity: Additional Results
In both the Gridworld and the Bouncing Balls domains, we did not see much difference in the structure of
results between the 10×10domain and the 20×20domains. Results were fairly consistent in that SRRD
tends to outperform DARD and EPIC, and feasibility constraints tend to limit coverage significantly.
Figure 11: 10×10Gridworld: Variation of reward relationships
Figure 12: 20×20Gridworld: Variation of reward relationships
46Under review as submission to TMLR
Figure 13: 10×10Bouncing Balls: Variation of reward relationships
Figure 14: 20×20Bouncing Balls: Variation of reward relationships
C.1.4 Experimental Parameters
A uniform policy in the Gridworld domain randomly selects one of the four actions, {north, east, south,
west}, at each timestep. For the Bouncing Balls domain, it randomly selects {north, north-east, east, east-
south, south, south-west, west, west-north, north}. The parameter ϵdictates the ratio of times in which
random transitions (instead of uniform policy) are executed. Table 3 and Table 4 shows the experimental
parameters used in Experiment 1 (Algorithm 1).
47Under review as submission to TMLR
Table 3: (Low Coverage): Parameters used to test the variation of coverage for the Gridworld and the
Bouncing Balls domain.
Parameter Values
Rollout Counts, T [1,2,3,4,5,6,7,8,9,10,15,20,30,40,50,75,100,200,300,400,500,1000,2000]
Epochs,E 200
Policy,π uniform,ϵ= 0.1
Discount,γ 0.7
Dimensions 20×20
Table 4: (Feasibility Constraints): Parameters used to test the variation of coverage in the presence of
movement restrictions, ϵ= 0, for the Gridworld and Bouncing Balls domain.
Parameter Values
Rollout Counts, T [1,2,3,4,5,6,7,8,9,10,15,20,30,40,50,75,100,200,300,400,500,1000,2000]
Epochs,E 200
Policy,π uniform,ϵ= 0
Discount,γ 0.7
Dimensions 20×20
C.2 Experiment 2
C.2.1 Reward Classification: Testbeds and IRL
Gridworld: The Gridworld domain simulates agent movement from a given initial state to a specified
terminal state under a static policy. Each state is defined by an (x,y)coordinate where 0≤x < N,
and0≤y < Mimplying|S|=NM. For Experiment 2, the action space only consists of four cardinal
directions {north, east, south, west}, and to define classes, we use static policies based on the action-selection
distribution (out of 100) per state. Table 6 shows the Gridworld parameters used for Experiment 2.
Bouncing Balls: The Bouncing Balls domain, adapted from (Wulfe et al., 2022), simulates a ball’s
motion from a starting state to a target state while avoiding randomly mobile obstacles. These obstacles
add complexity to the environment since the ball might need to change its strategy to avoid obstacles (at a
distance,d= 3). Each state is defined by the tuple (x,y,d ), where (x,y)indicates the ball’s current location,
anddindicates the ball’s Euclidean distance to the nearest obstacle, such that: 0≤x < N,0≤y < M,
andd≤max(M,N ). The action space includes eight directions (cardinals and ordinals), with the stochastic
parameterϵfor choosing random transitions. Table 5 describes the parameters for Experiment 2.
Figure 15: Bouncing Balls domain: The red ball starts at a randomly assigned state and aims to reach the
green state while avoiding being close to the black obstacles. The presence of the obstacles makes the domain
more complex than the simple Gridworld.
48Under review as submission to TMLR
Drone Combat: The Drone Combat domain is derived from the multi-agent gym environment, which
simulates a predator-prey interaction (Anurag, 2019). We adapt this testbed to simulate a battle between
two drone swarms; a blue swarm denoting the ally team; and a red swarm denoting a default AI enemy.
The goal is for the blue ally team to defeat the default AI team. This testbed offers discrete actions and
states within a fully observable environment, while also offering flexibility for unit creation, and obstacle
placement. However, the number of states and actions is still high such that we did not use the testbed in
Experiment 1. Each unit (blue and red squares) possesses a distinct set of parameters and possible actions.
Each team consists of drones and ships, and the team that wins either destroys the entire drones of the
opponent or its ship. This ship adds complexity to the decision-making process of the teams which need to
engage with the enemy, as well as safeguard their ships. Each drone is defined by the following attributes:
visibility range (VR) - the range a unit can see from its current position (partial observability); health (H)
- the number of firings a unit can sustain; movement range(MR) - the maximum distance that a unit can
move to; and shot strength(SS) - the probability of a shot hitting its target. All these attributes are drawn
from the set:
U={(VR,H,MR,SS)|VR∈{1,3,5},H∈{5,10,15},MR∈{1,2,3},SS∈{0.05,0.1}}
Table 7 summarizes the parameters used for Experiment 2.
Figure 16: Drone Combat: The drone combat domain describes a battlefield scene where the blue team aims
to attack the red AI team. The brown squares present movement obstacles; the green square represents the
enemy’s ship; and the purple square represents the team’s ship.
StarCraft II (SC2): The SC2 domain is a strategy game created by Blizzard Entertainment that features
real-time actions on a complex battlefield environment. The game involves planning, strategy, and quick
decision-making to control a multi-agent ally team, aiming to defeat a default AI team in a competitive,
challenging, and time-sensitive environment. SC2 serves not only as entertainment but also as a platform
for professional player competitions. Due to its complexity and the availability of commonly used interactive
Python libraries, the SC2 game is widely employed in Reinforcement Learning, serving as a testbed for
multi-agent research. The goal of the ally team is to gather resources, and build attacking units that are
used to execute a strategic mission to defeat the AI enemy; within an uncharted map, that gets revealed
after extensive exploration (introduces partial observability). The sheer size of the map and the multitude
of possible actions for each type of unit, as well as the number of units, contribute to the enormity of the
action and state spaces. During combat, each ally unit, moves in a decentralized manner and attacks an
enemy unit using an assigned cooperative strategy from the set: C={c1,c2,c3,c4}; wherec1- move towards
ally closest to enemy’s start base; c2- move towards a random enemy unit; c3- move towards ally closest
to an enemy unit; and c4- move towards the map’s center. We focus on attack-oriented ally units to reduce
the state space. Non-attacking units such as pylons are treated as part of the environment. The game state
records the number of ally units ( numally), and the total number of opponent units ( numenemy); as well as
the central coordinates of the ally and the enemy. The action records the number of ally units attacking the
enemy at an instance. Table 8 describes the StarCraft II parameters used for Experiment 2.
49Under review as submission to TMLR
Figure 17: (StarCraft II): The domain describes a multiagent team that aims to defeat a default AI enemy.
In this figure, the the red section denotes the team’s base where it builds resources and attacking units. In
the green section, it shows the enemy’s base, where the team is attacking the enemy.
C.2.2 Experimental Parameters:
Across all the four domains, default parameters were set as follows: m- the number of distinct agent policies
was set to 10;p- the number of trajectory sets was set to 100; andq- the number of trajectories per set
was set to 5. The optimal values for γ(discount factor) and k(the neighborhood size) are not fixed for each
independent trial. Therefore, for hyperparameter selection, we employ a grid search over the set defined by:
{(γ,k) :γ∈{0,0.1,..., 1},k∈{10,20,..., 100}}
The agent classes shown describe the policy that an agent takes in each given state. For example, in the
Gridworld domain, an agent with a policy [25, 25, 25, 25], randomly selects the cardinal direction to take
from a uniform distribution. For the Drone Combat and StarCraft II domains, the agent behaves based on
the combination of the defined attributes.
Table 5: Bouncing Balls Parameters for Experiment 2.
Parameter Values
Agent policies (10 classes) [[12,12,12,12,13,13,13,13],[5,5,25,25,25,5,5,5],
[25,25,25,5,5,5,5,5],[5,5,5,5,5,25,25,25],[5,5,65,5,5,5,5,5],
[5,5,5,65,5,5,5,5],[5,5,5,5,65,5,5,5],[5,25,5,25,5,25,5,5],
[20,5,20,5,20,5,20,5],[5,20,5,20,5,20,5,20]]
Trajectory sets per policy 100
Number of obstacles 5
Distance to obstacle (Manhattan) 3
Number of comparison trials 200
Actions move: {north, north-east, east, east-south, south, south-west, west,
west-north}
State Dimensions 20×20×3
50Under review as submission to TMLR
Table 6: Gridworld Parameters for Experiment 2.
Parameter Values
Agent policies (10 classes), [[25, 25, 25, 25], [5, 5, 5, 85], [85, 5, 5, 5], [5, 85, 5, 5], [5, 5, 85, 5], [5,
15, 30, 55], [55, 30, 15, 5], [15, 5, 55, 30], [5, 55, 30, 15], [15, 30, 5, 55]]
Trajectory sets per policy, 100
Number of comparison trials, 200
Actions, move: {north, west, south, east}
State Dimensions 20×20
Table 7: Drone Combat Parameters for Experiment 2.
Parameter Values
Agent policies 10classes, each consisting of 5agents. Each agent xhas attributes
randomly drawn from the set:
U={(VR,H,MR,SS )|VR∈ {1,3,5},H∈ {5,10,15},MR∈
{1,2,3},SS∈{0.05,0.1}}
Trajectory sets per policy 100
Number of agents per team 11 (1 ship, 10 drones)
Number of comparison trials 200
Actions,αis the movement range,
1≤α≤3{{leftα,upα,rightα,downα},attack}
Dimensions 40×25, with obstacles occupying ≈30% of the area
Table 8: StarCraft II Parameters for Experiment 2.
Parameter Values
Agent policies generated based on
resources and strategy10 classes, agents attributes randomly chosen from:
U={(c,u)|c∈{c1,c2,c3,c4},u∈{adept,voidray,phoenix,stalker}}.
Trajectory sets per policy 100
Comparison trials 200
Actions Number of attacking units per unit time
State representation (num ally,num enemy,(xally,yally),(xenemy,yenemy ))
C.2.3 Inverse Reinforcement Learning (IRL)
In Experiment 2, we utilize Inverse Reinforcement Learning (IRL) to compute agent rewards based on
demonstrated behavior. Specifically, we employ three IRL algorithms: Maximum Entropy IRL (Maxent-
IRL) (Ziebart et al., 2008); Adversarial IRL (Fu et al., 2018); and the Preferential Trajectory IRL (PT-IRL)
(Santos et al., 2021). In addition, we compute manual rewards that differ due to potential shaping.
Maxent IRL The objective of the Maxent IRL10algorithm is to compute a reward function that will
generate a policy (learner) πLthat matches the feature expectations of the trajectories generated by the
expert’s policy (demonstrations, assumed to be optimal) πE. Formally, this objective can be expressed as:
EπL[ϕ(τ)] =EπE[ϕ(τ)], (58)
whereϕτare trajectory features. Eπk=/summationtext
τ∈φkpπk(τ)·ϕ(τ), wherepπk(τ)is the probability distribution of
selecting trajectory τfromπk. The original Maxent-IRL algorithm modeled the relationship between state
10MaxentandAIRLimplementationsadaptedfrom: https://github.com/HumanCompatibleAI/imitation(Gleaveetal.,2022)
51Under review as submission to TMLR
features and agent rewards as linear, however, recent modifications now incorporate non-linear features via
neural networks. To resolve the ambiguity of having multiple optimal policies which can explain an agent’s
behavior, this algorithm applies the principle of maximum entropy to select rewards yielding a policy with
the highest entropy.
AIRL: The AIRL algorithm uses generative adversarial networks to train a policy that can mimic the
expert’s behavior. The IRL problem can be seen as training a generative model over trajectories, such that:
max
wJ(w) = max
wEτ∼D[logpw(τ)], (59)
wherepw(τ)∝p(s0)/producttextT−1
t=0P(st+1|st,at)eγt.Rw(st,at)and the parameterized reward is Rw(s,a). Using the
gradient of J(w), theentropy-regularized policy objective can be shown to reduce to:
max
πEπ/bracketleftiggT/summationdisplay
t=0(Rw(st,at)−logπ(at|st))/bracketrightigg
(60)
The discriminator is designed to take the form:
Dw(s,a) =exp{fw(s,a)}
exp{fw(s,a)}+π(a|s), (61)
and thetraining objective aims to minimize the cross-entropy loss between expert demonstrations and
generated samples:
L(w) =T/summationdisplay
t=0(−ED[logDw(st,at)]−Eπt[log(1−Dw(st,at))]) (62)
The policy optimization objective then uses the reward:
R(s,a) =log(Dw(s,a))−log(1−Dw(s,a)). (63)
The AIRL formulation can be seen as an extension of the Guided Cost Learning (GCL) by Finn et al. (2016),
with scalability modifications of analyzing data from a state-action level, rather than a trajectory-centric
formulation. The AIRL formulation can also be shown to be equivalent to the Maxent IRL formulation.
PTIRL: The PTIRL algorithm incorporates multiple agents, each with a set of demonstrated trajectories
φi. In order to compute the rewards for each agent, PTIRL considers target and non-target trajectories.
Target trajectories are demonstrated trajectories from a target agent, and non-target trajectories are demon-
strated trajectories from other agents. Denoting Pas the probability transition function for all the agents,
the linear expected reward for each trajectory τis defined as:
LER (τ) =m/summationdisplay
k=1P(sk′,ak,sk)·r(sk′,ak,sk).
For each trajectory set, there is a lower bound value lb(φ)and an upper bound value ub(φ), defined as:
lb(φ) = minτ∈φ(LER (τ))andub(φ) = maxτ∈φ(LER (τ)),respectively. From lb(φ)andub(φ), the spread δ
is defined as:
δ(φa,φb) =lb(φa)−ub(φb).
PTIRL defines a preferential ordering between any two trajectories φaandφbas a poset,≺, such that
ifφb≺φa, thenδ(φa,φb)>0. Given the above definitions, let φibe the set of target trajectories and
φnithe set of non-target trajectories. The PTIRL objective is to compute rewards such that φnt≺φi,
δ(φi,φnt)≥α, whereαis the minimum threshold for the spread. PT-IRL is generally fast because it
directly computes rewards via linear optimization.
52Under review as submission to TMLR
Table 9: Reward Learning Parameters Across Domains
AIRL MAXENT PTIRL
Trajectories/run: 5 Trajectories/run: 5 Target Trajectories/run: 5
RL Algorithm: PPO RL Algorithm: PPO Non-Target Trajectories/run: 10
Discount (γ): 0.9 Discount ( γ): 0.9 Max Reward Cap: +100
Reward Network - MLP
Hidden Size: [256, 128]Reward Network - MLP
Hidden Size: [256, 128]Min Reward Cap: -100
Learning Rate: 10−4Learning Rate: 10−4LP Solver: Cplex
Time Steps: 105
Generator Batch Size: 2048
Discriminator Batch Size: 256
C.2.4 Reward Classification: Significance Tests
Table 10: Experiment 2: Welch’s t-tests
SRRD_vs_DIRECT SRRD_vs_EPIC SRRD_vs_DARD
Domain Rewardst-statistic p-value t-statistic p-value t-statistic p-value
Manual 11.522 0 12.478 0 10.385 0
Maxent 28.496 0 28.142 0 2.593 0.005
AIRL 13.610 0 5.117 0 4.266 0 Gridworld
PTIRL 11.209 0 5.719 0 7.725 0
Manual 18.801 0 17.375 0 6.955 0
Maxent 32.341 0 12.104 0 -2.586 0.995
AIRL 45.020 0 28.226 0 19.488 0 Bouncing Balls
PTIRL 5.089 0 3.101 0.001 7.096 0
Manual 16.152 0 15.851 0 16.786 0
Maxent 17.829 0 -2.543 0.994 9.123 0
AIRL 9.772 0 8.023 0 3.935 0 Drone Combat
PTIRL 61.534 0 34.679 0 30.384 0
Manual 24.419 0 20.633 0 15.760 0
Maxent 6.171 0 1.717 0.043 2.233 0.013
AIRL 4.992 0 4.300 0 -2.913 0.998 StarCraft II
PTIRL 6.054 0 3.631 0 4.961 0
In Table 10, we show the comprehensive results for the Welch’s t-tests for unequal variances, which are
conducted across all domain and reward type combinations, to test the null hypotheses: (1) µSRRD≤
µDIRECT, (2)µSRRD≤µEPIC, and (3)µSRRD≤µDARD; against the alternative: (1) µSRRD> µ DIRECT,
(2)µSRRD> µ EPIC, and (3)µSRRD> µ DARD, whereµrepresents the sample mean. Generally, the tests
indicate that (1) µSRRD>µDIRECTfor all instances; (2) µSRRD>µEPICfor11out of 12instances, and (3)
µSRRD> µ DARDfor10out of 12instances. These tests are performed at a significant level of α= 0.05,
assuming normality as per central limit theorem, since the number of trials is 200. In summary, we conclude
that the SRRD pseudometric is more effective at classifying reward samples compared to its baselines. Full
details on accuracy scores collected are shown in Table 11 and Table 12.
53Under review as submission to TMLR
Table 11: Experiment 2: Accuracy results
Domain Rewards Statistic DIRECT EPIC DARD SRRD
GridworldManualmean 69.8 69.3 70.0 75.8
stdev 4.6 4.6 5.0 4.6
Maxentmean 57.4 57.5 68.9 70.0
stdev 4.5 4.5 4.5 4.4
AIRLmean 82.3 84.9 85.0 86.2
stdev 3.0 1.8 2.6 2.7
PTIRLmean 82.2 84.2 83.4 86.0
stdev 3.5 3.3 3.5 3.3
Bouncing BallsManualmean 46.5 47.3 52.0 55.2
stdev 4.8 4.6 4.8 4.5
Maxentmean 39.7 46.0 50.8 49.9
stdev 3.1 3.3 3.2 3.2
AIRLmean 41.2 46.1 49.8 56.3
stdev 3.4 3.9 3.3 3.3
PTIRLmean 70.3 71.1 69.5 72.4
stdev 4.2 4.3 4.1 4.0
Drone CombatManualmean 67.1 67.2 66.2 73.9
stdev 4.1 4.2 4.9 4.2
Maxentmean 70.3 77.7 73.2 76.8
stdev 3.7 3.8 4.2 3.5
AIRLmean 90.1 90.7 92.3 93.8
stdev 3.9 3.9 3.7 3.7
PTIRLmean 52.5 63.7 65.1 78.3
stdev 4.3 4.3 4.6 4.1
StarCraft IIManualmean 65.5 67.4 69.5 76.5
stdev 4.6 4.5 4.5 4.4
Maxentmean 72.3 74.1 73.9 74.8
stdev 4.1 4.1 4.2 4.1
AIRLmean 75.1 75.3 78.1 77.0
stdev 4.0 4.0 3.8 3.8
PTIRLmean 77.2 78.1 77.6 79.6
stdev 4.1 4.2 4.2 4.0
54Under review as submission to TMLR
Table 12: Experiment 2: Precision, Recall, F1-scoresDomain
Rewards
StatisticDIRECT EPIC DARD SRRDprecision
recall
f1-score
precision
recall
f1-score
precision
recall
f1-score
precision
recall
f1-scoreGridworldMaxentmean 64.757.656.864.857.756.972.968.867.876.170.169.6
stdev 3.54.14.13.44.04.03.74.94.83.64.34.2
Manualmean 74.470.068.873.669.468.174.070.168.478.776.173.2
stdev 4.14.44.74.54.44.84.54.54.94.23.34.2
AIRLmean 77.582.578.385.985.284.078.385.581.081.186.482.5
stdev 2.31.92.31.51.11.72.11.51.81.91.71.9
PTIRLmean 77.982.377.980.484.279.878.083.678.882.586.481.8
stdev 5.52.83.05.12.22.74.82.42.95.72.22.8Bouncing BallsManualmean 44.646.339.745.247.340.546.852.444.052.755.745.8
stdev 7.13.94.47.03.64.16.43.34.07.02.73.9
Maxentmean 33.539.734.141.145.940.951.150.647.342.949.842.4
stdev 2.62.32.03.12.62.45.63.03.13.42.82.8
AIRLmean 35.041.235.745.046.143.149.650.146.658.456.750.9
stdev 3.02.72.73.83.03.03.42.82.95.62.33.1
PTIRLmean 67.469.764.369.270.565.164.269.061.865.471.966.2
stdev 6.42.73.86.42.73.86.42.73.85.12.83.5Drone CombatManualmean 64.167.160.567.167.064.065.166.362.767.473.170.2
stdev 4.04.14.03.94.14.05.03.94.44.54.24.3
Maxentmean 65.170.367.674.177.672.874.673.571.074.076.671.3
stdev 3.83.63.73.63.83.74.53.23.84.03.53.7
AIRLmean 90.090.189.088.290.489.390.292.586.392.293.691.9
stdev 4.03.93.93.84.03.94.23.84.03.63.63.6
PTIRLmean 50.252.548.360.464.360.763.165.064.679.377.778.5
stdev 4.84.34.54.64.44.84.74.44.44.54.04.2StarCraft IIManualmean 60.265.462.764.167.364.770.169.267.976.176.974.5
stdev 4.54.64.54.84.44.64.53.53.94.74.34.5
Maxentmean 73.172.269.670.474.170.269.173.270.172.774.773.7
stdev 4.04.14.04.54.54.34.44.24.14.04.24.1
AIRLmean 73.875.070.473.973.869.876.777.973.776.878.676.7
stdev 3.74.03.84.33.94.13.93.93.93.94.03.9
PTIRLmean 72.177.172.577.678.677.178.177.075.780.079.576.7
stdev 3.94.14.04.24.34.44.54.34.24.03.94.0
55