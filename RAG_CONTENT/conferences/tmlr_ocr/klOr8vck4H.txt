Under review as submission to TMLR
Efficient Variable Selection Using Reinforcement Learning
for Big Data
Anonymous authors
Paper under double-blind review
Abstract
Efficient variable selection is crucial for optimizing the performance and interpretability of
machine learning models. However, as datasets expand in sample size and dimensionality,
traditional methods may encounter computational challenges and accuracy issues. To
tackle this problem in the realm of big data, we propose a novel approach referred to
asREinforcement learning for Variable Selection (REVS) within the Markov Decision
Process (MDP) framework. By prioritizing the long-term variable selection accuracy, we
propose a dynamic policy to adjust the candidate important variable set, guiding it toward
convergencetothetruevariableset. Toenhancecomputationalefficiency, wepresentanonline
policy iteration algorithm integrated with temporal difference learning for sequential policy
improvement. Our experiments demonstrate superior performance of the method, especially
in big data scenarios where it substantially reduces computation time compared to alternative
methods. Furthermore, inhigh-dimensionalfeaturesetswithstrongcorrelations, ourapproach
enhances variable selection accuracy by leveraging cumulative reward information from batch
data.
1 Introduction
The field of Reinforcement Learning (RL) is often concerned with the problem of how to make the optimal
sequential decisions in dynamic environments for agents, with the aim of maximizing the cumulative rewards
in long terms. In recent years, RL has made significant advances and has shown potential for various
applications in scientific domains, such as robotics (Silver et al., 2016; Kalashnikov et al., 2018; Li et al.,
2023; Nikkhoo et al., 2023), autonomous driving (Sallab et al., 2017; Chen et al., 2021b) and personalized
medicine (Weltz et al., 2022; Gao et al., 2022a). Recently, RL has demonstrated its ability to enhance the
efficiency of fundamental problems requiring extensive computational resources. For example, Fawzi et al.
(2022) introduced a deep RL approach based on AlphaZero1 that can tackle matrix multiplication of any
size efficiently. Wu et al. (2020) proposed model-based RL technique for hyperparameter tuning in complex
machine learning algorithms, particularly within the context of large scale datasets. Drawing inspiration from
the success of RL in optimizing large scale computational algorithms, our aim is to adapt these techniques
to address challenges in supervised learning within the realm of machine learning. In particular, our focus
centers on variable selection inbig data by leveraging the power of RL.
1.1 Motivations and Related Work
Variable selection plays a pivotal role in optimizing the performance and interpretability of machine learning
models. It can be achieved through regularization methods that induce sparsity by controlling the number of
non-zero coefficients in the model. In supervised learning, the optimization problem can be formulated as the
combination of loss+penalty, moderated by a regularization parameter to balance these two components.
Best subset selection, which penalizes the ℓ0-based norm of coefficients, is a notable method in this context
(Greenshtein, 2006; Raskutti et al., 2011; Zhang et al., 2014; Bertsimas & Van Parys, 2020). However, it is an
NP-hard problem and computationally challenging. To approximate solutions to the best subset selection,
various methods have been proposed. These include continuous proxies to the ℓ0norm, such as ℓ1norm
1Under review as submission to TMLR
(LASSO) (Tibshirani, 1996), mixture penalization of ℓ1andℓ2norm (Elastic Net) (Zou & Hastie, 2005),
and non-convex penalizations such as SCAD (Fan & Li, 2001) and MCP (Zhang, 2010). Nonetheless, in
scenarios with high-dimensional andhighly-correlated data, these methods may not effectively recover the
sparsity pattern, leading to biased estimations (Zhang & Huang, 2008) and suboptimal performance in
terms of accuracy and false discovery rates (Bertsimas et al., 2020). Furthermore, these regularization-based
approaches are sensitive to the tuning of hyperparameters. Systematic hyperparameter optimization, especially
when coupled with cross-validation in big data contexts with millions of observations, presents substantial
computational challenges (Yao & Allen, 2020). This computational complexity can significantly impedes the
practical deployment of these methods in real-world scenarios.
To address computational challenges in big data, one popular approach is data reduction through strategic
subsampling. The basic idea is to select the most informative data points to create a smaller dataset that
retains most information from the full dataset (Drineas et al., 2006; 2011; Wang et al., 2018). The effectiveness
of these approaches critically depends on the choice of sampling probabilities. Unlike uniform sampling,
empirical statistical leverage scores from the input covariate matrix are often employed to define non-uniform
subsampling probabilities, a technique known as algorithmic leveraging (Ma et al., 2015). Furthermore, an
information-based subsampling has been proposed to further improve the computation efficiency (Wang et al.,
2019). However, subsampling inevitably leads to certain information loss and the data-dependent sampling
process can introduce bias. Additionally, above methods primarily address the ordinary least squares problem
by using all the features, and are not suitable for variable selection.
In parallel, inspired by the divide-and-conquer strategy, distributed learning frameworks have been developed
to handle large-scale statistical optimization challenges (Jordan, 2012; Zhang et al., 2013; Chen & Zhou, 2020).
These frameworks break down complex tasks into smaller segments that are processed concurrently across
multiple computing units, with their results aggregated for the final outcome (Gao et al., 2022b). While this
strategy can reduce computation time, its inherent design of executing tasks independently, rather than in
a sequence, limits the ability to leverage insights gained from previous stages to enhance subsequent ones.
Recently, inspired by the multi-armed bandit and reinforcement learning problems, Yao & Allen (2020); Fan
et al. (2020; 2021); Liu et al. (2021) have proposed methods to adaptively select both observations and features,
sequentially adjusting the non-sparse variable set using batch data. These approaches have shown good
performance in datasets with moderate sample sizes. Our paper focuses on extending these methodologies to
scenarios involving large sample sizes, aiming to addressing the challenges from the scalability and efficiency
in more extensive data environments.
1.2 Research Question
In the context of the datasets with large sample sizes characterized by high-correlated features, how can
we efficiently and accurately identify the true non-sparse feature set? Is it feasible to employ a sequential
approach for gradually adjusting the non-sparse feature set using batch data, and how can this process be
optimized?
To address above question regarding efficient and accurate variable selection in large datasets, we turn to
the principles of RL. RL is a powerful machine learning technique that enables an agent to make sequential
decisions in order to maximize the long-term reward. RL problems are often structured under the Markov
Decision Process (MDP) framework (Puterman, 2014; Sutton & Barto, 2018). In MDP, the data are
collected in a sequential way and can be summarized as the triplet information of state, action and reward:
{(St,At,Rt)}0≤t≤T, whereTis denoted as the number of stages. One central objective is to learn the optimal
policy,π∗, a strategy enabling decision-makers to choose actions based on the current state at each stage,
with the goal of maximizing the expected discounted cumulative reward.
1.3 Major Contributions
We propose a novel approach referred to as REinforcement learning for efficient Variable Selection (REVS).
REVS integrates RL techniques to tackle the variable selection challenge in big data contexts. First, we
conceptualize the variable selection problem as a time-homogeneous MDP , effectively treating it as a policy-
driven process. This involves viewing each small data batch, sampled from the large dataset, as a stage in
2Under review as submission to TMLR
Figure 1: Formulate variable selection problem as MDP.
MDP. Within each batch t, the candidate non-sparse variable set represents our state, St. Our goal is to
dynamically adjust this set by adding or removing variables, or keeping it constant, guided by our evolving
estimated policy. Such adjustments lead to the transition to the next state, St+1. To evaluate the efficacy
of these changes, we fit the variable sets into a supervised learning model for each stage, with the model’s
performance serving as the reward. This performance feedback is then used to update our policy through
online policy iteration algorithm to iteratively update our policy until it converges. See Figure 1 as an
illustration. Our proposal leverages the iterative and adaptive nature of RL and applies these principles to
the variable selection process in large datasets.
Second, REVS leverages the crucial RL advantage of balancing exploration and exploitation. Compared with
traditional methods such as backward or forward selection (Derksen & Keselman, 1992; Mao, 2004; Zhang,
2008), which may get stuck at local optima due to their greedy nature, our approach employs strategies such
asϵ-greedy for policy updates. While we exploit the existing knowledge about variable sets that have shown
promise ( exploitation ), we also intermittently explore new variable combinations that haven’t been previously
considered ( exploration ). By incorporating the exploration-exploitation trade-off, REVS is designed not just
for local variable selection accuracy ( short-term ), but to enhance the overall accuracy of variable selection
over batch ( long-term ) in large datasets.
Third, REVS enhances computational efficiency in variable selection for large datasets. By fitting simpler
models on smaller data batches under the MDP framework, we reduce the computational burden commonly
associated with cross-validation in traditional regularization methods using the entire dataset. Furthermore,
within each batch, we propose the quantile navigation approach so that REVS concentrates on a smaller subset
of active variables, achieving additional computational efficiency compared to other mini-batch methods that
process all variables. Moreover, by utilizing cumulative reward information from each batch, this iterative
process of adjusting selected variables with varied training and validation batches inherently mitigates the
risk of overfitting and expedites the convergence. In our experiments, REVS demonstrates promising results,
showing enhanced efficiency and accuracy compared to traditional variable selection techniques using penalized
regression, particularly in scenarios involving big data and high-dimensional datasets with correlated features.
Fourth, compared with prior approaches for feature selection using reinforcement learning (Fard et al., 2013;
Rasoul et al., 2021), REVS introduces a more general and efficient framework to address the limitations in
big data: (1) We propose a refined MDP formulation that allows both the addition and removal of features
at each stage, offering greater flexibility and adaptability while adhering to the Markov assumption, while
the action space in previous works is limited to only adding features. (2) Our reward function, based on the
improvement in prediction loss, generalizes the framework’s applicability to a wide range of supervised learning
tasks, surpassing the simpler feature scoring metrics used in earlier methods. (3) To tackle computational
challenges, our approach decomposes the optimization problem into manageable steps, achieving significant
efficiency gains, particularly for large datasets. (4) The epsilon-greedy algorithm balances exploration and
exploitation, reducing overfitting in high-dimensional and correlated data settings. (5) We also provide
3Under review as submission to TMLR
theoretical guarantees, including convergence to optimal feature sets and a detailed analysis of the Bellman
equation, establishing a strong foundation for the method’s robustness and reliability.
2 Formulate Variable Selection with MDP
Suppose we have covariates X= (X1,X2,...,Xp)⊺of dimension p, and a response variable Y. Assuming
observationsareindependentlyandidenticallydrawnfromthepopulation (X⊺,Y), weconsidertheGeneralized
Linear Model (GLM, McCullagh & Nelder (1989)):
g(E[Y|X]) =β0+β⊺X, (1)
whereg(·)is a known link function that relates the expected value of Yto the linear predictor β⊺X,β0
is the intercept, and β⊺= (β1,...,βp). The GLM framework can cover a broad spectrum of supervised
learning tasks, each characterized by a specific link function. For example, when the identity link function
is used, it leads to the linear regression model Y=β0+β⊺X+ϵ, whereϵis the error term. For binary
classification tasks such as logistic regression, GLM utilizes the logit link function, resulting in the model
log/parenleftig
E[Y|X]
1−E[Y|X]/parenrightig
=β0+β⊺X. In a sparse GLM, it is typically assumed that most regression coefficients βj
are0. The main goal of variable selection is to identify significant variables with non-zero coefficients, and
accurately estimate them for predicting Y. In settings with complex feature structures and big data, we aim
to adapt RL to enhance computational efficiency and variable selection accuracy.
We propose to formulate the problem of variable selection with MDP. A ‘ stage’ in our context specifically
refers to a decision point in the iterative batch process to adjust variables. Each stage involves sampling a
small size of batch data from the big dataset. The primary objective is to use the information from each
batch of data to sequentially refine the selected variable set. For variable selection, we define the critical
elements of state,action, andrewardas follows:
2.1 State: Current Selected Variable Set
Let the state space Sbe the power set of features in X, i.e.,|S|= 2p. The optimal state, denoted as s∗,
corresponds to the true variable set with non-zero regression coefficients. The state at each stage t, represented
bySt, reflects the current selection of variables. At each stage t, we randomly sample a batch data with
small sample size from the large-scaled data. We use ¯Stto denote all historical triplet information up to that
point, along with the current state St. The goal is to sequentially assess and potentially adjust the selected
variable set through actions such as adding significant variables or removing redundant ones, thus guiding the
transition from one stage to the next, e.g., s1→s2→s3→..., and ultimately converging to the optimal
states∗across a series of batch data.
2.2 Action: Add/Remove One Variable
To ensure stability in the variable adjustment process, each stage is limited to either adding/removing one
variable or maintaining the existing set for the next stage. Specifically, the action space is defined as
A=/braceleftbigg
+1,+2,..., +p/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Add,−1,−2,...,−p/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Remove,0/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Invariant/bracerightbigg
.
Here, forj= 1,2,...,p, the action “ +j” means adding variable Xjto the current state, while the action
“−j” refers to removing variable Xjfrom the current state. The action “ 0” signifies maintaining the current
variable set for the next state. From the computational perspective, given a state s, the total number of
feasible actions is p+ 1, as we cannot add already existing variables in sor remove absent ones.
The transition probability Pt:¯St×A→ ∆Sgoverns the shift from the history information ¯st∈¯Stto the next
statest+1∈Swhen taking action at∈Aat timet. Our MDP formulation ensures that Ptis deterministic,
time-homogeneous, and stationary. Specifically, the transition depends solely on the current variable set st
4Under review as submission to TMLR
and actionat, without any time dependency. In particular, we use a single transition function Pto denote it:
Pt[st+1|¯st,at] =

I[st+1={st/uniontextXj}]ifat= +j,
I[st+1={st\Xj}]ifat=−j,
I[st+1=st] ifat= 0,
:=P[st+1|st,at].(2)
This deterministic and stationary nature aligns with the Markov assumption, affirming the consistency and
stationarity of state transitions in our MDP formulation.
2.3 Reward: Evaluation of Model Performance
We define the immediate reward Rtat each stage tto assess the performance of the current action. Intuitively,
a beneficial action is one that moves the current state progressively closer to the optimal state s∗, which
should provide the best prediction for the response variable Y. Our reward function R:St×A×St+1→R
consists of two components: a measure of change in model performance rperf, and a penalty term for the
current action rpento regulate the number of selected variables:
rt=R(st,at,st+1)
= f(st,st+1)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
rperf,t:Change in model performance−λsign(at)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
rpen,t:Penalty for action. (3)
In particular, for linear regression, the first term rperfcan be specified as the decline in mean square error ,
while for logistic regression, it can be the increase in log-likelihood , both measuring the improvement in
model performance when using variables transitioning from sttost+1due to action atto fit the model. The
second term is a penalty that decreases the reward by λwhen new variables are added ( overcoming potential
overfitting ) and increases it when variables are removed ( stimulating valid variable selection process ). When
at= 0, the reward rt= 0since the state remains unchanged. This penalty term effectively acts as a sequential
ℓ0-based regularization, penalizing the number of selected variables based on the action taken at each stage.
2.4 Policy: A Strategy Guiding the Adjustment of Variable Sets
In RL, a policy is a sequence of decision rules, {πt}t≥0, guiding the decision-maker on which action to choose
at each time t. In the context of variable selection, our policy determines how to intelligently adjust the
variables in the current state stat each stage to progressively move from sttowards the optimal state s∗as
stage progresses. Given that our defined MDP upholds the assumptions of stationary transition and reward,
we can only focus on a stationary and time-homogeneous policy class Π(Sutton & Barto, 2018). This class
is independent of time tand historical information preceding it. A policy πwithin this class satisfies the
conditionπ(a|St) =πt(a|¯St)for anyt, ensuring consistency in decision-making regardless of the stage. Under
a specific policy π∈Π, at each decision point t, the action At=ais chosen from the action space with a
probability dictated by π(a|s), given the current variable set Stass. Our policy πtakes the state value in S
as input and outputs a probability distribution over the action space A:
π(s) =/parenleftbig
π(+1|s),π(+2|s),...,π (+p|s),π(−1|s),
π(−2|s),...,π (−p|s),π(0|s)/parenrightbig⊺.(4)
Here,π(+j|s)andπ(−j|s)represent the probabilities of adding or removing the variable Xj, respectively.
Specifically, if Xj∈s, thenπ(+j|s) = 0. Conversely, if Xj/∈s, thenπ(−j|s) = 0. Meanwhile, π(0|s)
corresponds to the probability of maintaining the current state s. This policy framework enables the adaptive
and strategic navigation through the variable selection process, balancing the need to explore new variable
combinations with the aim of converging towards an optimal variable set for accurate prediction.
The main objective of RL is to identify the optimal policy that yields the highest discounted cumulative
reward. Similarly, for any given policy π∈Πand any initial state s∈S, our value function is defined as
Vπ(s) =Eπ/bracketleftbig/summationtext
t≥0γtR(St,At,St+1)|S0=s/bracketrightbig
, where Eπdenotes the expectation of the trajectory when the
5Under review as submission to TMLR
actions are selected according to π. Here,γ∈(0,1)is denoted as the fixed discounted factor that balances the
trade-off between the immediate and long-term rewards. The Q-function, denoted as Qπ(s,a), is defined as
the discounted cumulative reward where the initial state-action pair is (s,a)and then all subsequent actions
follow the policy π:Qπ(s,a) =Eπ/bracketleftbig/summationtext∞
t=0γtR(St,At,St+1)|S0=s,a0=a/bracketrightbig
.
With a substantial number of i.i.d. observations of covariate and response (xi,yi)in big data, our objective
is to estimate the optimal policy π∗∈arg maxVπ(s), which maximizes the expected discounted cumulative
reward for each state s∈S. The optimal policy π∗characterizes a specific way of sequentially adjusting
the variable set. To handle the large data size, we break the dataset into smaller and manageable batches.
This enables us to iteratively refine the policy in an online manner, using each batch of data to sequentially
update the policy based on insights gained at each stage. The reward function R, combining the change in
model performance and penalty for controlling redundant variables, guides selection towards the optimal set
of variabless∗.
Letπ(t)be the estimated optimal policy under each stage t. As the learning progresses, the estimated
policyπ(t)evolves, assigning higher probabilities to actions that effectively adjust the variable set towards s∗.
Specifically, when trajectory reaches s∗, the policy π(t)is expected to update such that the probability of
maintaining this state, π(t)(0|s∗), goes to 1 as t→∞. Conversely, when in states other than s∗, the policy
should actively seeks actions that bring the state closer to s∗, thus progressively improving the model’s fit to
the data. Specifically, this online computation process is executed using the policy iteration method with
Temporal Difference (TD) learning (Sutton & Barto, 2018).
3 Policy Iteration with TD Learning
In our variable selection framework, we employ a tailored policy iteration algorithm for progressive refinement
of variable adjustment strategies. This approach involves two alternating steps: (1) policy evaluation, and
(2) policy improvement. In the policy evaluation step, we estimate the action-value function Qπoldfor a
given policy πold, typically by solving the Bellman equation. This step assesses the efficacy of a current
policy in terms of how well it selects variables that contribute to an optimal performance in the GLM
model. In the policy improvement step, based on the estimated Qπold, we derive a new policy πnewby
choosing actions that maximize this function. This translates to adjusting the variable set by adding or
removing variables to enhance the regression model’s performance. This iterative process, symbolized as
π(0)evaluate−−−−−→Qπ(0)improve−−−−−→π(1)evaluate−−−−−→Qπ(1)improve−−−−−→π(2)→···→π∗→Q∗, continues until convergence
to the optimal policy π∗and optimal value Q∗.
A key aspect of REVS is balancing exploitation and exploration. Exploitation involves favoring actions that
previously resulted in significant rewards, i.e., effectively refining the variable set in the GLM. Conversely,
exploration entails trying new actions to potentially uncover better variable combinations, thereby accumulat-
ing more substantial long-term rewards. We incorporate this balance by using the ϵ-greedy algorithm (Yang
& Zhu, 2002; Chen et al., 2021a) in the policy improvement step, where the optimal action is chosen with
probability related to 1−ϵt, and other actions are explored with probability related to ϵt. The probability
distribution for actions in policy π(t)at stagetis defined as:
π(t)(a|s) =/braceleftbiggϵt/(p+ 1) + 1−ϵt,ifa∈arg maxQπ(t)(s,a),
ϵt/(p+ 1), otherwise.(5)
As the stage progresses ( t→∞),ϵtdecreases to 0, leading to the policy towards convergence.
We adapt the TD learning algorithm, specifically the State-Action-Reward-State-Action (SARSA) variant
(Zhao et al., 2016; Lee & Kim, 2022; Hu, 2023), to manage the policy iteration process. This on-policy
method is particularly suited for variable selection in large state spaces due to its efficiency in learning from
incomplete trajectories and updating policies online (Sutton & Barto, 2018). Our SARSA algorithm updates
theQ-function based on the observed transition from one state-action pair to the next. The update rule is:
Qt+1(st,at)←Qt(st,at) +αt/bracketleftbig
rt+γQt(st+1,at+1)−Qt(st,at)/bracketrightbig
, (6)
whereαtis the learning rate that decreases over time, ensuring convergence.
6Under review as submission to TMLR
3.1 Implementation with Big Data
For the practical implementation of our algorithm, we take linear regression as an illustration example. For
any staget, we randomly select differentntrainobservations for the training set and nvalidobservations
for the validation set from the large dataset, where ntrain≪nandnvalid≪n. We also ensure there’s
no overlap between the two sets. Let Xstdenote the subset of covariates contained in state st. Given
the training set{xi,yi}ntrain
i=1, we only use the variables in Xstto fit a simple linear regression model and
obtain the corresponding regression coefficients ζst. Then, the trajectory will transit to st+1after taking
actionatbased onπ(t). We fit another regression model with Xst+1and obtain the new coefficients
ζst+1. The change in model performance is calculated by the decline in mean square error rperf,t=
Envalid(Y−X⊺
stζst)2−Envalid(Y−X⊺
st+1ζst+1)2, where Envalidis the empirical mean with the sampled
validation data in stage t. The sampling process is implemented at each stage tso that each reward rtis
estimated with different subsets of data. Similarly, in the context of logistic regression, we can follow the
same procedure by replacing the decline of mean square error with the increase of log-likelihood function for
the logistic model. This maintains the adaptability of our approach across different types of models in GLM.
We refer more details for logistic regression in Appendix A.2.
In addressing the challenge of our potentially large state-action space, we recognize that many state-action
pairs may not be visited sufficiently, even with the ϵ-greedy approach. This can lead to a prolonged trajectory
withnumerousstagestogatheradequaterewardinformationforupdatingthe Q-function, potentiallyhindering
the efficiency of the TD learning algorithm. To mitigate this, we introduce a strategy utilizing quantiles of
evolving reward set to guide the trajectory towards more favorable states at each step.
3.2 Quantile-guided Navigation for Large Space
The process begins by generating a preliminary trajectory of T0stages using a uniformly random policy
πrandom, where actions for each state are selected with equal probability. During this phase, we record the
immediate rewards r0
tat each stage and compile them into an initial reward set R0={r0
t}T0
t=1. We then define
an ascending sequence of quantiles {τt}t≥0, whereτt→1ast→∞. As we proceed with TD learning and
policy iteration over a main trajectory, capped at a maximum of Tmaxstages, the reward set is continually
updated toRt=Rt−1/uniontextrt, incorporating the immediate rewards from each new stage. At each stage t,
transition to the next state is contingent on the current immediate reward rtsurpassing the upper τt-quantile
of the evolving reward set Rt. If this condition does not hold, the trajectory is maintained at the current
state, exploring alternative actions to transition to other variable sets. This quantile-guided approach guides
the trajectory progressively towards better states. By setting τt→1, the state transitions become increasingly
stringent, effectively guiding the trajectory towards better rewarding states as the process evolves.
From Theorem 1 in Section 4, the optimal policy would keep the trajectory stay at the optimal state. So, the
final estimation of non-sparse variable set is determined by the states where the action ‘0’ is optimal under the
final estimated policy at convergence. This is formalized as s∈S: 0∈arg maxa∈Aπfinal(a|s). Practically, if
the state of main trajectory stconverges to a specific state ˜s, it is set to be ˜s. The complete algorithm is
detailed in Appendix A.
3.3 Summarized Advantages of REVS
We summarize the following advantages of our proposed REVS for variable selection in large datasets:
(1)Computational efficiency. In scenarios with a large sample size n, by computing on small batches of
data (ntrain,nvalid≲logn) with MDP rather than solving the optimization problem using the entire dataset,
our method reduces the computational cost and hence is more efficient than fitting models on the full dataset.
In this way, REVS decomposes the whole variable selection process into a sequence of actions in each stage to
refine the variable set. Moreover, even within each small batch, REVS utilizes the quantile-guided navigation
technique and focuses on a reduced set of active variables p0≪p, which allows for further computational
savings compared to other mini-batch methods that operate over all pvariables.
7Under review as submission to TMLR
(2)Balance exploration and exploitation. Traditional variable selection techniques such as backward or
forward selection can get trapped in local optima due to their inherently greedy nature, especially in settings
with low signal-noise ratio and high-correlated features. By incorporating the ϵ-greedy algorithm, REVS does
not only focus on achieving immediate accuracy in variable selection. Instead, it aims to improve long-term
accuracy across batches with large datasets, effectively balancing the exploration of new variable sets with
the exploitation of known effective ones.
(3)Prevent overfitting with batch data. By continual adjusting selected variables with different sampled
training and validation batch data, and incorporating cumulative reward information, REVS inherently
overcomes overfitting. For high-dimensional setting with correlated features, REVS moderates the influence
of individual features that might appear overly predictive in specific subsets, and hence, improving variable
selection accuracy.
4 Theoretical Analysis
A key aspect in MDP is the use of the Bellman equation from dynamic programming, as highlighted by Sutton
& Barto (2018). This recursive equation connects the value of a state to values of its adjacent states, following
a specific policy throughout the trajectory. In variable selection, Bellman equations are appropriately adapted
to fit this framework. We state the following Bellman equations in our variable selection context. For any
given policy π∈Π, we have the Bellman equation for value function:
Vπ(s) =/summationdisplay
a∈Aπ(a|s) [R(s,a,s◦a) +γVπ(s◦a)],
and the Bellman equation for state-action function:
Qπ(s,a) =R(s,a,s◦a) +γ/summationdisplay
a′∈Aπ(a′|s◦a)Qπ(s◦a,a′),
whereRis defined in (3), and s◦ais denoted as the value of next state St+1when adjusting variables in
current state St=swith current action At=a.
It’s worth noting that our Bellman equations, differ from those in standard MDPs. Ours take a simplified
version, primarily because the transition probability in our model is a deterministic function dependent on
the current state sand action a. This means that the value of the subsequent state s◦ais predetermined
once we know sanda. Additionally, our model’s unique constraint of adding or removing only one variable
per stage leads to the value function and Q-function in the Bellman equation being specifically relevant to
states that differ by only one variable.
Next, we explore how the optimal policy π∗in RL guides the progression towards the optimal variable state
s∗. Traditionally, RL focuses on maximizing discounted cumulative rewards to learn the optimal policy π∗.
In our context, the goal is to identify an optimal adjustment strategy that leads us to the optimal state
for variable selection. We aim to establish a connection between these two goals. We use Pπ
t(s′,a′|s,a)to
denote thet-step visitation probability Prπ(St=s′,At=a′|S0=s,A0=a)on state-action pairs induced by
a stationary policy π∈Π. Let ∆a=R(s∗,a,s∗◦a)be the reward for taking a potentially redundant action
a̸= 0at the optimal state s∗. Intuitively, we expect ∆ato be negative, indicating an immediate decrease in
reward due to a transition to a less accurate state.
Assumption 1. The immediate reward drop near the optimal state s∗is greater than any future discounted
rewards: supπ∈Π/summationtext∞
t=2γt/summationtext
s′∈S,a′∈APπ
t(s′,a′|s∗,0)R(s′,a′,s′◦a′)<γinfa∈A\{ 0}|∆a|.
For any given sanda,(1−γ)/summationtext∞
t=0γtPπ
t(·,·|s,a)forms a probability mass distribution over state-action pairs.
It consists of a mixture of random pairs {St,At}t≥0with respective weights {(1−γ)γt}t≥0starting from
S0=s,A0=a. Therefore,/summationtext∞
t=0γt/summationtext
s′∈S,a′∈APπ
t(s′,a′|s,a)R(s′,a′,s′◦a′)can be seen as the expected
immediate reward under this transition probability. Assumption 1 implies that the immediate negative impact
of deviating from the optimal state s∗is more significant than any future benefits, reinforcing the importance
of staying close to s∗for optimal variable selection. Then, we present the following theorem for how optimal
policyπ∗guides the state transiting to s∗.
8Under review as submission to TMLR
Theorem 1. Supposeϵt>0for anyt≥0, andϵt→0ast→∞. Further assume supa>0,a∈A|f(s∗,a,s∗◦
a)|≤λ≤infa<0,a∈A|f(s∗,a,s∗◦a)|. Let the step size αt=O(t−c)where 1/2< c≤1. Then, under
Assumption 1, we have (I) Qt→Q∗,π(t)→π∗; (II) Under the optimal policy π∗,π∗(0|s∗) = 1, and for any
s̸=s∗,π∗(0|s) = 0.
Theorem 1 reveals that once the state reaches the optimal state s∗, it will remain there with no further
transitions under the optimal policy π∗. Essentially, this means that the optimal policy successfully identifies
and maintains the ideal variable set. From another perspective, if the current state is not optimal, i.e., either
missing important variables or containing redundant ones, the optimal policy will actively work to adjust it,
by continually refining the variable set until it aligns with the optimal state s∗. This theorem indicates that
the optimal policy effectively guides transitions towards achieving the best possible variable selection.
5 Experiments
We conduct several experiments to evaluate the performance of our REVS method. We concentrate on linear
regressioninthissection. TheresultsforotherGLMtypes, suchaslogisticregression, areincludedinAppendix
C.2. For linear regression, we simulate data based on the linear model Yi=β0+β⊺Xi+ϵi, whereϵii.i.d∼ N (0,2).
For logistic regression, we simulate data based on the logistic model logit(P(Yi= 1)) =β0+β⊺Xi, where
P(Yi= 1)represents the probability that the binary outcome Yiis 1, and logit(p) =log/parenleftig
p
1−p/parenrightig
forp∈(0,1).
The predictors Xiare the input features, and βrepresents the coefficients of the model. The dimension of
covariatespvaries from 200, 400, to 800. We ensure sparsity by setting p0= 25for linear regression, where p0
is the count of non-zero true regression coefficients. To characterize the complex correlation structures among
X, three following structures of the precision matrix Ω = Σ−1forXare considered. See the demonstration
of these structures in Figure 2.
Scenario 1 . (Ωis block diagonal). True regression coefficients β∗∈Rpare
( 1,1,1,1,1,0,0,0,0,0/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Block 1,−1,−1,−1,−1,−1,0,0,0,0,0/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Block 2,
1,1,1,1,1,0,0,0,0,0/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Block 3,−1,−1,−1,−1,−1,0,0,0,0,0/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Block 4,
1,1,1,1,1,0,0,0,0,0/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Block 5,0,0,..., 0)⊺.
We generate covariates with X5k+j=Zk+ϵx
5k+jwhereZki.i.d∼ N (0,1), for 0≤k≤4and1≤j≤5, and
ϵx
ji.i.d∼ N (0,1)for1≤j≤50. Remaining Xji.i.d∼ N (0,1)for51≤j≤p.
Scenario 2 . (Ωis banded). The setup for β∗∈Rpis identical to Scenario 1. The covarates are generated
withX∼N(0,Σ)where Σij= 0.8|i−j|.
Scenario 3 . (Ωis sparse).β∗∈Rpare the same as Scenario 1. The covarates are generated with
X∼N(0,Ω−1)where Ω =B+δI. Here,Iis an identity matrix, and Bhas off-diagonal entries, which equal
to0.5with probability 0.6, and 0 with probability 0.4.
We compare our proposed method with other state-of-art penalized regression methods for variable selection.
The following five methods are compared: (1) Lasso (Tibshirani, 1996); (2) Elastic net (Zou & Hastie, 2005);
(3) Smoothly Clipped Absolute Deviation (SCAD) based penalty (Fan & Li, 2001); (4) Minimax Concave
Penalty (MCP) (Zhang, 2010); (5) Our proposed REVS method with TD-learning.
We categorize variables with non-zero coefficients using positive labels, while those with zero coefficients are
assigned negative labels. Then, we employ the following criteria to assess the performance of comparison
methods: (1) Number of variables selected in the final model, with true value p0= 25; (2) True Negative Rate
(TNR) for selected variables; (3) Positive Predictive Value (PPV) for selected variables; (4) Mean Square
Error (MSE) for predicting Yon testing data.
9Under review as submission to TMLR
Figure 2: Three structures of precision matrix.
5.1 Big Data Setting
We set the number of observations n= 500,000. A dataset of size neval= 5,000is generated to assess model
performance. For big data settings, REVS streamlines the iterative process by sampling a small number
of observations, setting both ntrain =ntest= 200. We set preliminary trajectory length T0= 250. The
main trajectory extends up to Tmax= 2000stages, with the quantile sequence τt= 0.6 + 0.4t/Tmax→1as
t→Tmax. We use a discount factor γ= 0.99and a penalty term parameter λ= 0.2. To ensure convergence
of TD learning, we set the learning rate αt= 1/tand exploration probability ϵt= 1/t0.1, respectively. To
enhance computational efficiency in the big data scenario, we adopt the divide and conquer strategy (Smith,
1985) for penalized models. We split the data into 400 folds and aggregate the selected variables, by employing
a frequency table with a threshold cutting ratio of 0.8. The simulations for each scenario is replicated 50
times.
In our analysis, all methods exhibited similar results in terms of TNR, PPV, and MSE for variable selection
accuracy, largely attributed to the large sample sizes (see Figure 5 in Appendix C.1). However, the REVS
method distinguished itself by demonstrating superior computational efficiency. This advantage is due to the
method’s reliance on fitting simple linear models with smaller sample sizes at each stage. In Table 1, we
underscore this computational efficiency by presenting the mean computation time (in minutes) across 50
replications for linear regression, demonstrating REVS’s effectiveness in handling big data.
Table 1: Means of computation time (minutes) in big data setting under 50replications. Best values in bold.
Structure Method p= 200 p= 400 p= 800
Block ΩLasso 3.18 6.51 18.12
Elastic 6.88 17.02 52.65
SCAD 1.70 4.34 62.34
MCP 1.78 4.68 69.65
REVS 0.34 0.82 3.38
Banded ΩLasso 6.87 10.91 11.09
Elastic 14.95 26.05 31.25
SCAD 4.92 16.63 56.83
MCP 5.28 18.32 69.41
REVS 0.60 1.82 2.78
Sparse ΩLasso 5.28 6.78 19.74
Elastic 11.44 17.34 59.35
SCAD 3.89 9.09 69.33
MCP 4.05 9.52 78.90
REVS 0.40 0.81 4.04
10Under review as submission to TMLR
5.2 High-dimensional Setting
We explore our method in high-dimensional settings, where the sample size n= 200withntrain=ntest= 150.
The other parameter selection remain consistent with those in the big data setting. Figure 3(c) and Table
2 demonstrate that REVS not only closely aligns with the optimal variable selection, with the number of
variables selected being nearest to the true count p0= 25, but also exhibits the lowest variability for linear
regression. Furthermore, Figures 3(a)(b), and 4 highlight REVS’s superior performance in terms of TNR,
PPV, and MSE respectively. This is significant as it indicates a lower likelihood of REVS selecting redundant
variables compared to other penalized variable selection techniques. In addition to its strong performance in
linear regression, REVS also yields impressive results for logistic regression, as demonstrated in Table 3 and
Figure 6. This suggests that REVS effectively avoids redundant variable selection, maintaining high accuracy
and stability in both linear and logistic regression scenarios.
By iteratively updating selected variables using sampled training and validation subsets, we effectively harness
batch data to calculate the reward rt. This approach is able to reduce the risk of overfitting and inclusion of
redundant variables, and foster more informed model selection at each stage tthrough the integration of
cumulative reward insights. This feature of REVS underscores its robustness and adaptability, effectively
extending its range of applicability from large-scale data scenarios to more challenging high-dimensional
settings.
Block Banded Sparse
200 400 800 200 400 800 200 400 80070%80%90%100%
Dimension of Covariates (p)True Negative Rate (TNR)
Method Elastic Lasso MCP SCAD REVS
(a) True Negative Rate.
Block Banded Sparse
200 400 800 200 400 800 200 400 80020%40%60%80%100%
Dimension of Covariates (p)Positive Predictive Value (PPV)
Method Elastic Lasso MCP SCAD REVS (b) Positive Predictive Value.
Block Banded Sparse
200 400 800 200 400 800 200 400 800255075100125
Dimension of Covariates (p)Number of Selected Variables
Method Elastic Lasso MCP SCAD REVS
(c) Number of Selected Non-sparse Variables.
Figure 3: True Negative Rates (TNR), Positive Predictive Values (PPV), and numbers of selected non-sparse
variables of comparison methods in linear regression withhigh-dimensional data setting . The statistics in the
text show the mean and variance of above metrics across 50 replications.
11Under review as submission to TMLR
p= 200 p= 400 p= 800
# TNR(%) PPV(%) # TNR(%) PPV(%) # TNR(%) PPV(%)
Block Structure
Lasso 77.9 69.8% 32.9% 102.4 79.3% 25.2% 126.6 86.9% 20.3%
Elastic 81.0 68.0% 31.7% 104.7 78.8% 25.4% 127.1 86.8% 20.3%
SCAD 43.9 89.2% 57.6% 55.3 91.9% 46.1% 69.2 94.3% 36.5%
MCP 33.4 95.2% 75.9% 35.8 97.1% 71.0% 41.2 97.9% 61.4%
REVS 26.1 99.4% 96.0% 26.4 99.4% 94.6% 28.5 99.5% 86.5%
Banded Structure
Lasso 56.6 82.0% 46.2% 61.5 90.2% 44.8% 75.0 93.5% 36.5%
Elastic 54.4 83.2% 48.4% 60.0 90.8% 43.0% 78.4 93.1% 33.0%
SCAD 26.9 98.8% 98.7% 27.3 99.4% 97.5% 28.2 99.6% 95.6%
MCP 26.8 99.1% 99.2% 27.6 99.6% 98.8% 28.7 99.5% 98.3%
REVS 25.0 100% 100% 25.0 100% 100% 25.0 100% 100%
Sparse Structure
Lasso 79.8 68.7% 32.3% 101.0 79.7% 25.6% 129.8 86.5% 19.6%
Elastic 84.9 65.8% 30.0% 108.0 77.9% 24.1% 126.5 86.9% 20.6%
SCAD 29.2 97.6% 88.3% 30.6 98.5% 90.6% 31.2 99.2% 94.5%
MCP 26.7 99.3% 97.1% 26.9 99.5% 96.0% 27.2 99.7% 89.9%
REVS 25.6 99.6% 97.7% 26.1 99.7% 97.2% 26.5 99.8% 95.8%
Table 2: Means of number of selected variables (#) with p0= 25, Positive Predictive Value (PPV), and True
Negative Rate (TNR) in high-dimensional setting forlinear regression under 50replications. Best values in
bold.
6 Conclusions
In this paper, we adapted RL to address key challenges in supervised learning, particularly focusing on
feature selection in big data. We formulate the problem as a time-homogeneous MDP and develop an efficient
policy-iteration method using the TD learning algorithm to solve it. Our proposed REVS dynamically
adjusts the selection of variables in a step-wise manner, utilizing different sampled training and validation
subsets. By integrating the ϵ-greedy algorithm, REVS transcends beyond achieving immediate accuracy
in variable selection. It is designed to enhance long-term accuracy across large dataset batches, adeptly
balancing the exploration of new variable combinations with the exploitation of established effective ones.
In simulation studies, REVS shows significant computational efficiency for handling big data, surpassing
other state-of-the-art variable selection methods reliant on penalized regression. Furthermore, the utilization
of batch data for cumulative reward computation inherently safeguards against overfitting and continually
guides the variable selection process. REVS has also demonstrated superior performance in variable selection
accuracy, particularly in high-dimensional datasets with highly correlated covariates.
For future extensions, REVS currently utilizes the ϵ-greedy algorithm. The exploration of other RL algorithms,
suchastheUpperConfidenceBound(UCB)(Garivier&Moulines,2011;Kaufmannetal.,2012)andThompson
Sampling (Kang et al., 2024), presents another exciting research prospect. We leave these interesting directions
for future research.
Broader Impact Statement
The goal of this paper is to advance the problem of variable selection in the field of Machine Learning. We
believe that advancements in this area have the great potential to create widespread societal implications.
While our work opens up possibilities for positive change, it is beyond the scope of this paper to delve into
the specifics of these societal impacts.
12Under review as submission to TMLR
2.597(0.245)2.571(0.214)1.975(0.109)1.999(0.115)1.734(0.068)
3.046(0.446)3.07(0.402)2.021(0.134)2.073(0.149)1.812(0.106)
3.609(0.811)3.566(0.757)2.159(0.196)2.13(0.145)2.036(0.372)p = 200p = 400p = 800
2 3 4 5 6 7ElasticLassoMCPSCADREVS
ElasticLassoMCPSCADREVS
ElasticLassoMCPSCADREVS
log(MSE)
Method Elastic Lasso MCP SCAD REVSBlock Structure
(a) Block Structure
0.501(0.021)0.52(0.018)0.146(0.002)0.151(0.006)0.139(0.002)
0.556(0.012)0.561(0.029)0.143(0.002)0.14(0.002)0.132(0.002)
0.701(0.009)0.677(0.022)0.156(0.002)0.16(0.002)0.145(0.002)p = 200p = 400p = 800
0.25 0.50 0.75 1.00ElasticLassoMCPSCADREVS
ElasticLassoMCPSCADREVS
ElasticLassoMCPSCADREVS
log(MSE)
Method Elastic Lasso MCP SCAD REVSBanded Structure (b) Banded Structure
2.154(0.021)2.116(0.026)1.587(0.003)1.618(0.006)1.552(0.002)
2.334(0.029)2.288(0.027)1.594(0.006)1.601(0.007)1.587(0.004)
2.464(0.026)2.44(0.017)1.61(0.006)1.612(0.005)1.619(0.009)p = 200p = 400p = 800
1.5 2.0 2.5ElasticLassoMCPSCADREVS
ElasticLassoMCPSCADREVS
ElasticLassoMCPSCADREVS
log(MSE)
Method Elastic Lasso MCP SCAD REVSSparse Structure
(c) Sparse Structure
Figure 4: Mean Square Error (MSE) for predicting Yon testing data in linear regression withhigh-dimensional
data setting . The statistics in the text show the mean and variance of MSE across 50 replications.
References
Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms.
CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep , 32, 2019.
13Under review as submission to TMLR
D Bertsimas, J Pauphilet, and B Van Parys. Sparse regression: scalable algorithms and empirical performance.
Statistical Science , 35(4):555–578, 2020.
Dimitris Bertsimas and Bart Van Parys. Sparse high-dimensional regression: exact scalable algorithms and
phase transitions. The Annals of Statistics , 48(1):300–323, 2020.
Haoyu Chen, Wenbin Lu, and Rui Song. Statistical inference for online decision making: In a contextual
bandit setting. Journal of the American Statistical Association , 116(533):240–255, 2021a.
Jianyu Chen, Shengbo Eben Li, and Masayoshi Tomizuka. Interpretable end-to-end urban autonomous
driving with latent deep reinforcement learning. IEEE Transactions on Intelligent Transportation Systems ,
23(6):5068–5078, 2021b.
Lanjue Chen and Yong Zhou. Quantile regression in big data: A divide and conquer based strategy.
Computational Statistics & Data Analysis , 144:106892, 2020.
Shelley Derksen and Harvey J Keselman. Backward, forward and stepwise automated subset selection
algorithms: Frequency of obtaining authentic and noise variables. British Journal of Mathematical and
Statistical Psychology , 45(2):265–282, 1992.
Petros Drineas, Michael W Mahoney, and Shan Muthukrishnan. Sampling algorithms for ℓ2regression and
applications. In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete algorithm , pp.
1127–1136, 2006.
Petros Drineas, Michael W Mahoney, Shan Muthukrishnan, and Tamás Sarlós. Faster least squares approxi-
mation.Numerische Mathematik , 117(2):219–249, 2011.
Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle properties.
Journal of the American statistical Association , 96(456):1348–1360, 2001.
Wei Fan, Kunpeng Liu, Hao Liu, Pengyang Wang, Yong Ge, and Yanjie Fu. Autofs: Automated feature
selection via diversity-aware interactive reinforcement learning. In 2020 IEEE International Conference on
Data Mining (ICDM) , pp. 1008–1013. IEEE, 2020.
Wei Fan, Kunpeng Liu, Hao Liu, Yong Ge, Hui Xiong, and Yanjie Fu. Interactive reinforcement learning for
feature selection with decision tree in the loop. IEEE Transactions on Knowledge and Data Engineering ,
35(2):1624–1636, 2021.
Seyed Mehdi Hazrati Fard, Ali Hamzeh, and Sattar Hashemi. Using reinforcement learning to find an optimal
set of features. Computers & Mathematics with Applications , 66(10):1892–1904, 2013.
Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin
Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, et al.
Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610(7930):47–53,
2022.
Daiqi Gao, Yufeng Liu, and Donglin Zeng. Non-asymptotic properties of individualized treatment rules from
sequentially rule-adaptive trials. The Journal of Machine Learning Research , 23(1):11362–11403, 2022a.
Yuan Gao, Weidong Liu, Hansheng Wang, Xiaozhou Wang, Yibo Yan, and Riquan Zhang. A review of
distributed statistical inference. Statistical Theory and Related Fields , 6(2):89–99, 2022b.
Aurélien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit problems. In
International Conference on Algorithmic Learning Theory , pp. 174–188. Springer, 2011.
Eitan Greenshtein. Best subset selection, persistence in high-dimensional statistical learning and optimization
under l1 constraint. The Annals of Statistics , 34(5):2367–2386, 2006.
Michael Hu. Temporal difference learning. In The Art of Reinforcement Learning: Fundamentals, Mathematics,
and Implementations with Python , pp. 75–107. Springer, 2023.
14Under review as submission to TMLR
Michael I Jordan. Divide-and-conquer and statistical inference for big data. In Proceedings of the 18th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining , pp. 4–4, 2012.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen,
Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforcement learning for
vision-based robotic manipulation. In Conference on Robot Learning , pp. 651–673. PMLR, 2018.
Yue Kang, Cho-Jui Hsieh, and Thomas Lee. Online continuous hyperparameter optimization for generalized
linear contextual bandits. Transactions on Machine Learning Research , 2024.
Emilie Kaufmann, Olivier Cappé, and Aurélien Garivier. On bayesian upper confidence bounds for bandit
problems. In Artificial Intelligence and Statistics , pp. 592–600. PMLR, 2012.
Donghwan Lee and Do Wan Kim. Analysis of temporal difference learning: Linear system approach. arXiv
preprint arXiv:2204.10479 , 2022.
Zexin Li, Aritra Samanta, Yufei Li, Andrea Soltoggio, Hyoseung Kim, and Cong Liu. r3: On-device real-time
deep reinforcement learning for autonomous robotics. In 2023 IEEE Real-Time Systems Symposium (RTSS) ,
pp. 131–144. IEEE, 2023.
Kunpeng Liu, Yanjie Fu, Le Wu, Xiaolin Li, Charu Aggarwal, and Hui Xiong. Automated feature selection:
A reinforcement learning perspective. IEEE Transactions on Knowledge and Data Engineering , 35(3):
2272–2284, 2021.
Ping Ma, Michael W Mahoney, and Bin Yu. A statistical perspective on algorithmic leveraging. Journal of
Machine Learning Research , 16:861–911, 2015.
Kezhi Z Mao. Orthogonal forward selection and backward elimination algorithms for feature subset selection.
IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) , 34(1):629–634, 2004.
P McCullagh and JA Nelder. Generalized linear models. Generalized Linear Models , 1989.
ShahabNikkhoo, ZexinLi, AritraSamanta, YufeiLi, andCongLiu. Pimbot: Policyandincentivemanipulation
for multi-robot reinforcement learning in social dilemmas. In 2023 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS) , pp. 5630–5636. IEEE, 2023.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley &
Sons, 2014.
Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax rates of estimation for high-dimensional
linear regression over ℓq-balls.IEEE Transactions on Information Theory , 57(10):6976–6994, 2011.
Sali Rasoul, Sodiq Adewole, and Alphonse Akakpo. Feature selection using reinforcement learning. arXiv
preprint arXiv:2101.09460 , 2021.
Ahmad EL Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani. Deep reinforcement learning
framework for autonomous driving. arXiv preprint arXiv:1704.02532 , 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go
with deep neural networks and tree search. Nature, 529(7587):484–489, 2016.
Satinder Singh, Tommi Jaakkola, Michael L Littman, and Csaba Szepesvári. Convergence results for
single-step on-policy reinforcement-learning algorithms. Machine Learning , 38:287–308, 2000.
Douglas R Smith. The design of divide and conquer algorithms. Science of Computer Programming , 5:37–58,
1985.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
15Under review as submission to TMLR
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society:
Series B (Methodological) , 58(1):267–288, 1996.
HaiYing Wang, Rong Zhu, and Ping Ma. Optimal subsampling for large sample logistic regression. Journal
of the American Statistical Association , 113(522):829–844, 2018.
HaiYing Wang, Min Yang, and John Stufken. Information-based optimal subdata selection for big data linear
regression. Journal of the American Statistical Association , 114(525):393–405, 2019.
Justin Weltz, Alex Volfovsky, and Eric B Laber. Reinforcement learning methods in public health. Clinical
Therapeutics , 44(1):139–154, 2022.
Jia Wu, SenPeng Chen, and XiYuan Liu. Efficient hyperparameter optimization through model-based
reinforcement learning. Neurocomputing , 409:381–393, 2020.
Yuhong Yang and Dan Zhu. Randomized allocation with nonparametric estimation for a multi-armed bandit
problem with covariates. The Annals of Statistics , 30(1):100–121, 2002.
Tianyi Yao and Genevera I Allen. Feature selection for huge data via minipatch learning. arXiv preprint
arXiv:2010.08529 , 2020.
Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics ,
38(2):894–942, 2010.
Cun-Hui Zhang and Jian Huang. The sparsity and bias of the lasso selection in high-dimensional linear
regression. The Annals of Statistics , pp. 1567–1594, 2008.
Tong Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. Advances
in Neural Information Processing Systems , 21, 2008.
Yuchen Zhang, John Duchi, and Martin Wainwright. Divide and conquer kernel ridge regression. In Conference
on Learning Theory , pp. 592–617. PMLR, 2013.
Yuchen Zhang, Martin J Wainwright, and Michael I Jordan. Lower bounds on the performance of polynomial-
time algorithms for sparse linear regression. In Conference on Learning Theory , pp. 921–948. PMLR,
2014.
Dongbin Zhao, Haitao Wang, Kun Shao, and Yuanheng Zhu. Deep reinforcement learning with experience
replay based on sarsa. In 2016 IEEE Symposium Series on Computational Intelligence (SSCI) , pp. 1–6.
IEEE, 2016.
Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal
Statistical Society: Series B (Statistical Methodology) , 67(2):301–320, 2005.
16Under review as submission to TMLR
A Appendix A: Additional Implementation Details
A.1 Detailed Algorithm for REVS in Linear Regression
Algorithm 1 REVS for Linear Regression
1. Initialize Q0,{τt}t≥0.
2. Generate preliminary trajectory with randompolicy:
Initializes0;
Samplea0fromrandompolicyπrandom (·|s0);
Forstaget= 0,1,2,...,T 0,do:
Takeactionatandtransittost+1by (2);
Receive rewardr0
tdefined in (3):
Sample training set{xi,yi}ntrain
i=1and validation set {xj,yj}nvalid
j=1from whole data;
Estimateζstandζst+1by fittingY∼XstandY∼Xst+1with{xi,yi}ntrain
i=1;
Setr0
perf,t=Envalid(Y−X⊺
stζst)2−Envalid(Y−X⊺
st+1ζst+1)2;
Setr0
pen,t←−λI/bracketleftbig
at∈{+1,+2,..., +p}/bracketrightbig
+λI/bracketleftbig
at∈{− 1,−2,...,−p}/bracketrightbig
;
Obtainr0
t←r0
perf,t+r0
pen,t.
Sampleat+1fromπrandom (·|st+1);
UpdateQt+1(st,at)←Qt(st,at) +αt[rt+γQt(st+1,at+1)−Qt(st,at)];
Updatest←st+1,at←at+1, storer0
t;
ReturnR0={r0
t}T0
t=1andQT0.
3. Generate maintrajectory with ϵ-greedypolicy:
Initializes0, andsetQ0←QT0,R0←R 0obtained from previous step;
Samplea0fromϵ-greedypolicyπ(0)(·|s0)based onQ0by (5);
Forstaget= 0,1,2,...,Tmax,do:
Takeactionatandtransittost+1by (2);
Receive rewardrt←rperf,t+rpen,tin (3) with batch data as in preliminary trajectory;
Sampleat+1fromϵ-greedypolicyπ(t)(·|st+1)by (5) based on Qt;
UpdateQt+1(st,at)←Qt(st,at) +αt[rt+γQt(st+1,at+1)−Qt(st,at)];
Ifrt>quantile (Rt,τt):
Updatest←st+1,at←at+1;
UpdateRt+1←Rt/uniontextrt:
Untiltrajectory converges to ˜s.
4. Set final variable set to be ˜sifst→˜s, otherwise sample a variable set from {s∈ S : 0∈
arg maxa∈Aπ(Tmax)(a|s)}.
17Under review as submission to TMLR
A.2 REVS for Logistic Regression
In a manner akin to linear regression, for each stage t, we select a subset of observations from the larger
dataset for training ( ntrain) and validation ( nvalid), where both ntrainandnvalidare significantly smaller than
the total number of observations n. LetXstdenote the subset of covariates contained in state st. Given the
training set{xi,yi}ntrain
i=1, we fit a logistic regression model using only the variables in Xstand obtain the
regression coefficients ηst. After taking action ataccording to policy π(t), the state transitions to st+1, where
we fit a new logistic regression model with Xst+1and obtain coefficients ηst+1.
In the logistic regression framework, we modify the model performance metric. Instead of tracking the decline
in mean square error as in linear regression, we focus on the increase in the log-likelihood function. The
change in model performance, denoted as rperf,t, is defined by the increase in log-likelihood:
rperf,t=Envalid[YX⊺
st+1ηst+1−log(1 + exp( X⊺
st+1ηst+1))]−Envalid[YX⊺
stηst−log(1 + exp( X⊺
stηst))].
Here,Envalidrepresents the empirical mean calculated using the validation data sampled at stage t. This shift
in performance metric from MSE to log-likelihood is a key aspect of adapting REVS to logistic regression.
Algorithm 2 REVS for Logistic Regression
1. Initialize Q0,{τt}t≥0.
2. Generate preliminary trajectory with randompolicy:
Initializes0;
Samplea0fromrandompolicyπrandom (·|s0);
Forstaget= 0,1,2,...,T 0,do:
Takeactionatandtransittost+1by (2);
Receive rewardr0
tdefined in (3):
Sample training set{xi,yi}ntrain
i=1and validation set {xj,yj}nvalid
j=1from whole data;
Estimateηstandηst+1by fittingY∼XstandY∼Xst+1with{xi,yi}ntrain
i=1;
Setr0
perf,t=Envalid[YX⊺
st+1ηst+1−log(1 + exp(X⊺
st+1ηst+1))]−Envalid[YX⊺
stηst−log(1 +
exp(X⊺
stηst))];
Setr0
pen,t←−λI/bracketleftbig
at∈{+1,+2,..., +p}/bracketrightbig
+λI/bracketleftbig
at∈{− 1,−2,...,−p}/bracketrightbig
;
Obtainr0
t←r0
perf,t+r0
pen,t.
Sampleat+1fromπrandom (·|st+1);
UpdateQt+1(st,at)←Qt(st,at) +αt[rt+γQt(st+1,at+1)−Qt(st,at)];
Updatest←st+1,at←at+1, storer0
t;
ReturnR0={r0
t}T0
t=1andQT0.
3. Generate maintrajectory with ϵ-greedypolicy:
Initializes0, andsetQ0←QT0,R0←R 0obtained from previous step;
Samplea0fromϵ-greedypolicyπ(0)(·|s0)based onQ0by (5);
Forstaget= 0,1,2,...,Tmax,do:
Takeactionatandtransittost+1by (2);
Receive rewardrt←rperf,t+rpen,tin (3) with batch data as in preliminary trajectory;
Sampleat+1fromϵ-greedypolicyπ(t)(·|st+1)by (5) based on Qt;
UpdateQt+1(st,at)←Qt(st,at) +αt[rt+γQt(st+1,at+1)−Qt(st,at)];
Ifrt>quantile (Rt,τt):
Updatest←st+1,at←at+1;
UpdateRt+1←Rt/uniontextrt:
Untiltrajectory converges to ˜s.
4. Set final variable set to be ˜sifst→˜s, otherwise sample a variable set from {s∈ S : 0∈
arg maxa∈Aπ(Tmax)(a|s)}.
18Under review as submission to TMLR
B Appendix B: Proof of Theoretical Results
B.1 Proof of Bellman Equation under Variable Selection Context
The value function V(s)represents the expected reward when starting in state sand following a certain
policyπthereafter. The Bellman equation for the value function in a general RF problem is:
Vπ(s) =/summationdisplay
a∈Aπ(a|s)/summationdisplay
s′∈SP(s′|s,a) [R(s,a,s′) +γVπ(s′)].
Note that in our defined MDP, the transition probability is a deterministic function given sanda. Hence,
Vπ(s) =/summationdisplay
a∈Aπ(a|s)/summationdisplay
s′∈SI(s′=s◦a) [R(s,a,s′) +γVπ(s′)]
=/summationdisplay
a∈Aπ(a|s) [R(s,a,s◦a) +γVπ(s◦a)].
Similarly, the action-value function Q(s,a)represents the expected return after taking an action ain states
and then following policy π. From the general Bellman equation for the Q-function, we have
Qπ(s,a) =/summationdisplay
s′∈SP(s′|s,a)/bracketleftigg
R(s,a,s′) +γ/summationdisplay
a′∈Aπ(a′|s′)Qπ(s′,a′)/bracketrightigg
=/summationdisplay
s′∈SI(s′=s◦a)/bracketleftigg
R(s,a,s′) +γ/summationdisplay
a′∈Aπ(a′|s′)Qπ(s′,a′)/bracketrightigg
=R(s,a,s◦a) +γ/summationdisplay
a′∈Aπ(a′|s◦a)Qπ(s◦a,a′).
B.2 Useful Lemma for Proof of Theorem 1
We restate the following lemma (Corollary 1.5 and Lemma 1.6 in Agarwal et al. (2019)) for proof of Theorem
1. Let Pπto be the transition matrix on state-action pairs induced by a stationary policy π. Specifically,
Pπ
(s,a),(s′,a′):=P(s′|s,a)π(a′|s′). Then we have that:
[(1−γ)(I−γPπ)−1](s,a),(s′,a′)= (1−γ)∞/summationdisplay
t=0γtPπ(st=s′,at=a′|s0=s,a0=a),
and
Qπ= (I−γPπ)−1R.
B.3 Proof of Theorem 1
For (I), note that our formulation defines a valid MDP, and our choices of αtandϵtsatisfy the Robbins-Monro
conditions (Singh et al., 2000). Then, based on Theorem 1 in Singh et al. (2000), we have Qt→Q∗and
π(t)→π∗.
For (II), we firstly prove that, under the optimal policy π∗, we haveπ∗(0|s∗) = 1. We start from the
Q-function. Suppose we have a policy π0whereπ0(0|s∗) = 1. Then, from Bellman equation, we have
Qπ0(s∗,0) =R(s∗,0,s∗◦0) +γ/summationdisplay
a′∈Aπ0(a′|s∗◦0)Qπ0(s∗◦0,a′)
=R(s∗,0,s∗) +γ/summationdisplay
a′∈Aπ0(a′|s∗)Qπ(s∗,a′)
= 0 +γQπ0(s∗,0).
19Under review as submission to TMLR
So, we getQπ0(s∗,0) = 0. Based on the definition of the optimal policy π∗,Qπ∗(s∗,0)≥Qπ0(s∗,0) = 0.
Now, we consider any policy π1whereπ1(a̸= 0|s∗)>0. In other words, π1tends to adjust the variables
when the trajectory reaches the optimal states. From above Lemma in Appendix B.2, we can obtain a close
form to characterize the Q-function with Bellman Equation. In particular,
Qπ1(s∗,0) =/summationdisplay
s′∈S/summationdisplay
a′∈A∞/summationdisplay
t=0γtPπ1
t(s′,a′|s∗,0)R(s′,a′,s′◦a′)
= 0 +/summationdisplay
s′∈S/summationdisplay
a′∈A\{ 0}∞/summationdisplay
t=0γtPπ1
t(s′,a′|s∗,0)R(s′,a′,s′◦a′)
= 0 + I(s′=s∗,a′= 0)R(s∗,0,s∗◦0) +γ/summationdisplay
s′∈S/summationdisplay
a′∈A\{ 0}Pπ1
1(s′,a′|s∗,0)R(s′,a′,s′◦a′)
+/summationdisplay
s′∈S/summationdisplay
a′∈A\{ 0}∞/summationdisplay
t=2γtPπ1
t(s′,a′|s∗,0)R(s′,a′,s′◦a′)
= 0 + 0 +γ/summationdisplay
s′∈S/summationdisplay
a′∈A\{ 0}P(s′|s∗,0)π1(a′|s′)R(s′,a′,s′◦a′)
+/summationdisplay
s′∈S/summationdisplay
a′∈A\{ 0}∞/summationdisplay
t=2γtPπ1
t(s′,a′|s∗,0)R(s′,a′,s′◦a′)
= 0 +γ/summationdisplay
a′∈A\{ 0}π1(a′|s∗)R(s∗,a′,s∗◦a′)
+/summationdisplay
s′∈S/summationdisplay
a′∈A\{ 0}∞/summationdisplay
t=2γtPπ1
t(s′,a′|s∗,0)R(s′,a′,s′◦a′)
=γ/summationdisplay
a∈A\{ 0}π1(a|s∗)∆a+/summationdisplay
s′∈S/summationdisplay
a′∈A\{ 0}∞/summationdisplay
t=2γtPπ1
t(s′,a′|s∗,0)R(s′,a′,s′◦a′)
≤γsup
a∈A\{ 0}∆a+/summationdisplay
s′∈S/summationdisplay
a′∈A\{ 0}∞/summationdisplay
t=2γtPπ1
t(s′,a′|s∗,0)R(s′,a′,s′◦a′)
=−γinf
a∈A\{ 0}|∆a|+/summationdisplay
s′∈S/summationdisplay
a′∈A\{ 0}∞/summationdisplay
t=2γtPπ1
t(s′,a′|s∗,0)R(s′,a′,s′◦a′)
<0.
Here, the last equality is due to supa>0,a∈A|f(s∗,a,s∗◦a)|≤λ≤infa<0,a∈A|f(s∗,a,s∗◦a)|. Hence, we
have ∆a<0fora∈A\{ 0}. The last inequality holds by Assumption 1. Therefore, we have Qπ1(s∗,0)<
Qπ0(s∗,0) = 0, which means π1is not the optimal one. So, we have π∗(0|s∗) = 1. This completes the proof
for the first part.
For the second part, suppose we have a specific state s̸=s∗whereπ∗(0|s) = 1. Then, similar to the proof in
the first part, we have Vπ∗(s) =Qπ∗(s,0) = 0. Then, based on the definition of π∗,Vπ(s)≤Vπ∗(s) = 0
andQπ(s,0)≤Qπ∗(s,0) = 0hold for any π∈Π. Note that in our problem, the optimal state s∗is unique.
Hence, there exists a path Dwith finite length so that scan be transit to s∗, i.e.,s→···→s∗. We used0
to denote the length of the path. Following the path D, we construct a deterministic policy π′so thatscan
transit tos∗with probability equal to 1, and π′(0|s∗) = 1. Let ˜stbe thet-step state in this path. Now we
evaluateQπ′(s,0). Sinceπ′is a deterministic policy, we have
Qπ′(s,0) = 0 +d0/summationdisplay
t=1γtR(˜st,π′(˜st),˜st◦π′(st)) +γd0+1Vπ′(s∗)>0,
which is contradictory with Qπ′(s,0)≤0. This completes the proof.
20Under review as submission to TMLR
C Appendix C: Additional Experiment Results
C.1 Additional Experiment Results for Linear Regression
1.388(0.001)1.388(0.001)1.388(0.001)1.388(0.001)1.389(0.001)
1.386(0.001)1.386(0.001)1.386(0.001)1.386(0.001)1.387(0.001)
1.379(0)1.379(0)1.379(0)1.379(0)1.38(0)p = 200p = 400p = 800
1.35 1.40 1.45ElasticLassoMCPSCADREVS
ElasticLassoMCPSCADREVS
ElasticLassoMCPSCADREVS
log(MSE)
Method Elastic Lasso MCP SCAD REVSBlock Structure
(a) Block Structure
1.38(0)1.38(0)1.38(0)1.38(0)1.38(0)
1.395(0.001)1.395(0.001)1.395(0.001)1.395(0.001)1.394(0.001)
1.388(0.001)1.388(0.001)1.388(0.001)1.388(0.001)1.388(0.001)p = 200p = 400p = 800
1.36 1.40 1.44ElasticLassoMCPSCADREVS
ElasticLassoMCPSCADREVS
ElasticLassoMCPSCADREVS
log(MSE)
Method Elastic Lasso MCP SCAD REVSBanded Structure (b) Banded Structure
1.382(0)1.382(0)1.382(0)1.382(0)1.382(0)
1.385(0)1.385(0)1.385(0)1.385(0)1.385(0)
1.388(0)1.388(0)1.388(0)1.388(0)1.388(0)p = 200p = 400p = 800
1.36 1.38 1.40 1.42ElasticLassoMCPSCADREVS
ElasticLassoMCPSCADREVS
ElasticLassoMCPSCADREVS
log(MSE)
Method Elastic Lasso MCP SCAD REVSSparse Structure (c) Sparse Structure
Figure 5: Mean Square Errors (MSE) for predicting Yon testing data in linear regression withbig data
setting. The statistics in the text show the mean and variance of MSE across 50 replications.
21Under review as submission to TMLR
C.2 Additional Experiment Results for Logistic Regression
p= 200 p= 400 p= 800
# TNR(%) PPV(%) # TNR(%) PPV(%) # TNR(%) PPV(%)
Block Structure
Lasso 16.70 98.76% 78.60% 19.98 98.28% 67.56% 25.42 98.35% 49.57%
Elastic 45.06 83.64% 39.01% 48.26 91.28% 35.57% 72.12 92.60% 25.29%
MCP 16.98 98.13% 87.15% 18.40 98.97% 79.13% 21.36 98.94% 61.03%
SCAD 22.68 95.78% 67.54% 29.36 96.22% 51.98% 41.02 96.57% 35.15%
REVS 16.26 99.04% 89.73% 16.62 99.16% 81.16% 18.96 99.03% 61.29%
Banded Structure
Lasso 16.93 98.69% 77.42% 21.32 97.56% 64.78% 23.32 98.86% 51.67%
Elastic 42.16 85.94% 41.31% 47.74 91.87% 36.86% 69.13 93.35% 27.21%
MCP 24.73 94.76% 64.26% 26.31 97.02% 59.13% 26.78 96.68% 48.83%
SCAD 31.32 91.38% 57.56% 34.78 93.83% 50.12% 44.67 93.48% 33.64%
REVS 16.33 99.02% 91.73% 16.86 99.13% 80.72% 18.47 99.12% 61.34%
Sparse Structure
Lasso 18.56 97.13% 75.48% 21.89 97.04% 64.15% 24.92 98.54% 50.48%
Elastic 48.07 82.76% 36.84% 48.77 91.56% 36.08% 70.36 92.88% 26.92%
MCP 21.37 96.83% 72.15% 28.40 96.31% 60.72% 31.46 97.37% 43.12%
SCAD 30.76 91.89% 59.91% 39.36 91.07% 47.63% 47.02 92.05% 31.08%
REVS 16.29 99.02% 91.75% 18.97 99.12% 80.47% 19.96 98.79% 59.83%
Table 3: Means of numbers of selected variables (#) with p0= 15, Positive Predictive Values (PPV), and
True Negative Rates (TNR) in high-dimensional setting forlogistic regression under 50replications. Best
values in bold.
22Under review as submission to TMLR
Block Banded Sparse
200 400 800 200 400 800 200 400 80085.0%90.0%95.0%100.0%
Dimension of Covariates (p)True Negative Rate (TNR)
Method Elastic Lasso MCP SCAD REVS
(a) True Negative Rate.
Block Banded Sparse
200 400 800 200 400 800 200 400 80040%60%80%
Dimension of Covariates (p)Positive Predictive Value (PPV)
Method Elastic Lasso MCP SCAD REVS (b) Positive Predictive Value.
Block Banded Sparse
200 400 800 200 400 800 200 400 8002550
Dimension of Covariates (p)Number of Selected Variables
Method Elastic Lasso MCP SCAD REVS
(c) Number of Selected Non-sparse Variables.
Figure 6: True Negative Rates (TNR), Positive Predictive Values (PPV), and numbers of selected non-sparse
variables of comparison methods in logistic regression withhigh-dimensional data setting . The statistics in
the text show the mean and variance of above metrics across 50 replications.
23