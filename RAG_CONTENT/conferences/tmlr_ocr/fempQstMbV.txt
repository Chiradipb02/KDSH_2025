Published in Transactions on Machine Learning Research (04/2023)
Deep Double Descent via Smooth Interpolation
Matteo Gamba mgamba@kth.se
KTH Royal Institute of Technology
Erik Englesson engless@kth.se
KTH Royal Institute of Technology
Mårten Björkman celle@kth.se
KTH Royal Institute of Technology
Hossein Azizpour azizpour@kth.se
KTH Royal Institute of Technology
Reviewed on OpenReview: https: // openreview. net/ forum? id= fempQstMbV
Abstract
The ability of overparameterized deep networks to interpolate noisy data, while at the same
time showing good generalization performance, has been recently characterized in terms of
the double descent curve for the test error. Common intuition from polynomial regression
suggeststhatoverparameterizednetworksareabletosharplyinterpolatenoisydata, without
considerably deviating from the ground-truth signal, thus preserving generalization ability.
At present, a precise characterization of the relationship between interpolation and general-
ization for deep networks is missing. In this work, we quantify sharpness of ﬁt of the training
datainterpolatedbyneuralnetworkfunctions, bystudyingthelosslandscapew.r.t.tothein-
put variable locally to each training point, over volumes around cleanly- and noisily-labelled
training samples, as we systematically increase the number of model parameters and train-
ing epochs. Our ﬁndings show that loss sharpness in the input space follows both model-
and epoch-wise double descent, with worse peaks observed around noisy labels. While small
interpolating models sharply ﬁt both clean and noisy data, large interpolating models ex-
press a smooth loss landscape, where noisy targets are predicted over large volumes around
training data points, in contrast to existing intuition1.
1 Introduction
The ability of overparameterized deep networks to interpolate noisy data, while at the same time showing
good generalization performance (Belkin et al., 2018; Zhang et al., 2018), has been recently characterized
in terms of the double descent curve of the test error (Belkin et al., 2019; Geiger et al., 2019). Within this
framework, as model size increases, the test error follows the classical bias-variance trade-oﬀ curve (Geman
etal.,1992), peakingasmodelsbecomelargeenoughtoperfectlyinterpolatethetrainingdata, anddecreasing
as model size grows further (Belkin et al., 2019). This phenomenon, largely studied in the context of
regression (Bartlett et al., 2020; Muthukumar et al., 2020) and random features (Belkin et al., 2020), at
present lacks a precise characterization relating interpolation to generalization for deep networks.
Current intuition from linear and polynomial regression suggests that, under some hypothesis on the training
sample, large overparameterized models are able to perfectly interpolate both cleanly- and noisily-labeled
samples, without considerably deviating from the ground-truth signal, thus showing good performance de-
spite overﬁtting the training data (Muthukumar et al., 2020; Bartlett et al., 2020; Nakkiran et al., 2019a).
1Source code to reproduce our results available at https://github.com/magamba/double_descent
1Published in Transactions on Machine Learning Research (04/2023)
0
 2
x2
1
012sin(x)+
Prediction
Ground Truth
Observations
(a)
0
 2
x2
1
012sin(x)+
Prediction
Ground Truth
Observations (b)
x2
x1Losspathπ1
pathπ2
pathπ3K=1
K=2
K=3
training sample (c)
x2
x1Loss (d)
Figure 1: Intuition from overparameterized regression. a) Polynomial of large degree, trained with
gradient descent to ﬁt noisy scalar data, reproducing the polynomial regression experiment of Nakkiran
et al. (2019a), and reﬂecting common intuition on double descent, suggesting that the generalization ability
of large interpolating models is tied to sharply ﬁtting of noisy data, thus resulting in models that do not
deviate considerably from the ground truth signal. b) In this work we show that, contrary to intuition,
deep networks smoothly interpolate both clean and noisy data, and that improved generalization in the
interpolating regime is tied to smoothness of the loss w.r.t. the input variable. Geodesic MC integration.
For each base training point, we generate Pgeodesic paths by connecting a sequence of augmentations of
increasingstrength,whichweusetocovervolumesofincreasingsizeinthelosslandscapearoundeachtraining
point. We compare points that are c) sharply interpolated from those that are d) smoothly interpolated.
Figure 1a illustrates this phenomenon, showing a polynomial of large degree that perfectly ﬁts the training
data, with predictive function sharply interpolating noisy samples (intuitively corresponding to a spike at
each training point), while overall remaining close to the data-generating function.
In this work, we study the emergence of double descent for the test error of deep networks (Nakkiran et al.,
2019b) through the lens of smoothness of interpolation of the training data, as model size as well as the
number of training epochs vary, for models trained in practice. To quantify smoothness of interpolation,
we conduct an empirical exploration of the loss landscape w.r.t. the input variable, by providing explicit
measures of sharpness of the loss, focusing on image classiﬁcation.
Due to the inherently noisy nature of Euclidean estimators in pixel space, and following the manifold hy-
pothesisPope et al. (2020); Bengio (2013); Narayanan & Mitter (2010), postulating that natural data lies on
a combination of manifolds of lower dimension than the input data’s ambient dimension, we constrain our
measures to the support of the data distribution, locally to each training point.
Our empirical study shows that the polynomial intuition in Figure 1a does not hold in practice for deep
networks, which instead smoothly interpolate both clean and noisy data (Figure 1b). Speciﬁcally, smooth
interpolation – emerging both for large overparameterized networks and prolonged training – results in large
models conﬁdently predicting the (noisy) training targets over large volumes around each training point.
Contributions
•We present the ﬁrst systematic empirical study of smoothness of the loss landscape of deep networks
in relation to overparameterization and interpolation for natural image datasets.
•Starting from inﬁnitesimal smoothness measures from prior work, we introduce volumetric measures
that capture loss smoothness when moving away from training points.
•We develop a geodesic Monte Carlo integration method for constraining our measures to a local
approximation of the data manifold, in proximity of each training point.
2Published in Transactions on Machine Learning Research (04/2023)
•We present an empirical study of model-wise and epoch-wise double descent for neural networks
trained without confounders (explicit regularization, data augmentation, batch normalization), as
well as for commonly-found training settings. By decoupling smoothness from generalization, we
empirically show that overparameterization promotes input-space smoothness of the loss landscape.
Particularly, we produce practical examples in which smoothness of the learned function of deep
networks does not result in improved generalization, highlighting that the implicit regularization
eﬀectofoverparameterizationshouldbestudiedintermsofreducedvariationofthelearnedfunction.
2 Related work
Recent years have seen increased interest in the study of smoothness of deep networks in relationship to
generalization. For studies of learned representations , interpreting networks as functions of their parameters,
loss landscape smoothness has been related to improved generalization (Ma & Ying, 2021; Foret et al., 2020;
Rosca et al., 2020), increased stability to perturbations (Keskar et al., 2017), reduced minimum description
length (Hochreiter & Schmidhuber, 1997), as well as better model compression (Chang et al., 2021).
Additionally, for networks interpreted as functions of their input, for a ﬁxed parameterization , sensitivity
of the networks’ learned function has been connected to generalization performance (LeJeune et al., 2019;
Novak et al., 2018). Indeed mounting evidence, both empirical (Gamba et al., 2022; 2020; Novak et al., 2018)
as well as theoretical (Bubeck & Sellke, 2021; Neyshabur et al., 2018), suggests that large overparameterized
models achieve robust generalization (Ma & Ying, 2021) via smoothness of the learned function. While
overparameterization alone is not enough to guarantee strong robustness (Chen et al., 2021; Rice et al.,
2020), the large number of parameters of modern networks is thought to promote implicit regularization of
the network’s function (Gamba et al., 2022; Bubeck & Sellke, 2021; Neyshabur et al., 2018; 2015). In this
context, a direct study of interpolation via the parameter-space interpretation is limited by confounders, such
as symmetries of linearlayers(Singh & Jaggi,2020; Li etal., 2015), for which diﬀerentparameterizations may
yield the same equivalent interpolating function (Simsek et al., 2021). Thus, our work adopts the input-space
view of the loss landscape, to directly study sharpness of interpolation around each training point.
Our methodology builds upon input-space sensitivity analyses for neural networks, presenting a ﬁrst system-
atic study of the role of overparameterization in promoting smoothness of the network’s learned function.
The smoothness measures presented in section 3, are inspired by the vast body of work on the loss landscape
of neural networks in parameter space. Due to the extensive theoretical literature on double descent in
simpliﬁed controlled settings such as linear regression (Muthukumar et al., 2020; Bartlett et al., 2020; Belkin
et al., 2018), in the following we mainly draw connections to prior work targeting deep networks.
Deep Double Descent Double descent (Belkin et al., 2019; Geiger et al., 2019) was ﬁrst observed for
several machine learning algorithms for increasing model size (model-wise). Later, Nakkiran et al. (2019b)
showed a similar trend during training of deep networks (epoch-wise), as well as w.r.t. dataset size (sample-
wise). The phenomenon has been studied from various perspectives: bias-variance decomposition (Yang
et al., 2020; Neal et al., 2018), samples to parameters ratio (Belkin et al., 2020), parameter norms (Belkin
et al., 2019), and decision boundaries (Somepalli et al., 2022).
Inthiswork, westudymodel-wiseandepoch-wisedoubledescentintermsofsmoothnessofthelosslandscape
with respect to the input, and separate the analysis in terms of clean and noisily-labeled data points.
Importantly, in contrast to existing studies of double descent (Belkin et al., 2020), we focus on input space
and on the training loss – a quantity that does not follow double descent – and study sharpness metrics
based on training data, showing that they strongly correlate with the test error.
The most related work to ours is the concurrent one of Somepalli et al. (2022) studying decision boundaries
in terms of reproducibility and double descent. Our works diﬀer in that we study double descent in the loss
landscape. Furthermore, our study takes a closer look at the impact of clean and noisily-labeled samples,
and presents settings in which the emergence of regularity of the loss landscape does not result in improved
generalization. Finally, we focus on implicit regularization (by disabling batch norm, data augmentation)
and a simpler optimization procedure (constant learning rate SGD) to reduce confounding factors.
3Published in Transactions on Machine Learning Research (04/2023)
Loss Landscape of Neural Networks To understand the remarkable generalization ability of deep
networks (Xie et al., 2020; Geiger et al., 2019; Keskar et al., 2017), as well as to design better training
criteria (Foret et al., 2020), several works study the loss landscape of deep networks in parameter space ,
focusing on solutions obtained by SGD (Kuditipudi et al., 2019), as well as the optimization process (Arora
et al., 2022; Li et al., 2021). Inspired by such literature, we quantify smoothness of the loss landscape by
estimating the sharpness of the loss, as proposed by Foret et al. (2020) and Keskar et al. (2017) for the
parameter-space, but we perform our analysis in input-space . Importantly, in this work we focus on image
classiﬁcation tasks, and study smoothness of interpolation of training data points.
Input Space Sensitivity and Smoothness Novak et al. (2018) present an empirical sensitivity study of
fully-connectednetworkswithpiece-wiselinearactivationfunctionsthroughtheinput-outputJacobiannorm,
which is shown to strongly correlate to the generalization ability of the networks considered. Their study
proposes an inﬁnitesimal analysis of the Jacobian norm at training and validation points, as well as the use of
input-space trajectories in proximity of the data manifold to probe trained networks. LeJeune et al. (2019)
analysesecond-orderinformation(thetangentHessianofaneuralnetwork)byusingweakdataaugmentation
to constrain their measure to the proximity of the data manifold. Lastly, Gamba et al. (2022) introduce a
nonlinarity measure for piece-wise linear networks, that strongly correlates with the test error in the second
descent for large overparameterized models. Similar to the ﬁrst two works, we study smoothness of neural
networks, using the Jacobian and Hessian norm of neural networks trained in practice, and similar to the
latter work, we provide a systematic study of double descent, which we further extend to epoch-wise trends.
Finally, Rosca et al. (2020) postulate a connection between model-wise double descent and smoothness:
during the ﬁrst ascent models ﬁt the training data at the expense of smoothness, while the second descent
happens as the model size becomes large enough for smoothness to increase.
Later, Bubeck & Sellke (2021) theoretically prove a universal law of robustness highlighting a trade-oﬀ
between the model size and the Lipschitz constant of a learning algorithm w.r.t. its input variable. Our
work provides empirical evidence supporting the postulate of Rosca et al. (2020) and the law of robustness
of Bubeck & Sellke (2021).
3 Methodology
Our leading research question is to quantify smoothness of interpolation of training data for deep networks
trained on classiﬁcation tasks, as the number of model parameters is increased. We interpret a network
as a function with input variable x∈Rdand learnable parameter θ, incorporating all weights and biases.
Our study focuses on the landscape of the loss Lθ(x,y) :=L(θ,x,y)treated as a function of the input x,
with target y. Inspired by the literature on the loss landscape of neural networks in parameter space (Foret
et al., 2020; Dinh et al., 2017; Keskar et al., 2017), we quantify (the lack of) smoothness by devising explicit
measures of loss sharpness in a neighbourhood of training points (xn,yn), forn= 1,...,N. Crucially, for
any given network, we focus on sharpness w.r.t. the input variable x, keeping the parameter θﬁxed.
We begin by describing inﬁnitesimal sharpness in section 3.1, which we compute in proximity of the data
manifold local to each training point in section 3.2. Finally, we introduce a method for estimating sharp-
ness over data-driven volumes by exploiting data augmentation in section 3.3, and in section 3.4 we detail
the chosen data augmentation strategies. The proposed methodology enables us to measure sharpness of
interpolation of the training data, by restricting our study near the support of the data distribution.
3.1 Sharpness at Data Points
To estimate how sharply the loss changes w.r.t. inﬁnitesimal perturbations of the input variable x, we study
the Jacobian of the loss,
J(x,y) :=∂
∂xLθ(x,y) (1)
4Published in Transactions on Machine Learning Research (04/2023)
To measure sharpness at a point (xn,yn), we follow Novak et al. (2018), and compute the /lscript2norm of
J(xn,yn), which we take in expectation over the training set D={(xn,yn)}N
n=1,
J=ED/bardblJ(x,y)/bardbl2 (2)
assuming that the loss is diﬀerentiable one time at the points considered. Intuitively, sharpness is measured
by how fast the loss Lθ(x,y)changes in inﬁnitesimal neighbourhoods of the training data, and a network is
said to smoothly interpolate a data point xnif the loss is approximately ﬂat locally around the point and
the point is classiﬁed correctly according to the corresponding target yn. Throughout our experiments, the
Jacobian Jis computed using a backward pass w.r.t. the input variable x.
Equation 2 provides ﬁrst-order information about the loss landscape. To gain knowledge about curvature,
we also study the Hessian of the loss w.r.t. the input variable,
H(x,y) :=∂2
∂x∂xTLθ(x,y) (3)
whose Frobenius norm again we take in expectation over the training set
H=ED/bardblH(x,y)/bardbl2 (4)
The Hessian tensor in Equation 3 depends quadratically on the input space dimensionality d, providing a
noisy Euclidean estimator of loss curvature in proximity of the input data. Following the manifold hypothe-
sis(Bengio, 2013; Narayanan & Mitter, 2010), stating that natural data lies on subspaces of dimensionality
lower than the ambient dimension d, we restrict Hessian computation to the tangent subspace of each train-
ing point xn. Starting from Equation 1, throughout our experiments, Equation 3 is estimated by computing
the tangent Hessian, as outlined in the next section.
3.2 Tangent Hessian Estimation
To constrain Equation 3 to the support of the data distribution, we adapt the method by LeJeune et al.
(2019) and estimate the loss Hessian norm projected onto the data manifold local to each training point.
For any input data point (xn,yn)and corresponding Jacobian J(xn,yn), we generate Maugmented data
points xn+umby randomly sampling a displacement vector umusing weak data augmentation. For each
sampled um, we then estimate the Hessian H(xn,yn)projected along the direction xn+um, by computing
the ﬁnite diﬀerence1
δJ(xn,yn)−J(xn+δum,yn). Then, following Donoho & Grimes (2003) we estimate
the Hessian norm directly by computing
H=1
M2δ2ED/parenleftBigM/summationdisplay
m=1/bardblJ(xn,yn)−J(xn+δum,yn)/bardbl2
2/parenrightBig1
2(5)
which is equivalent to a rescaled version of the rugosity measure of LeJeune et al. (2019). Importantly,
diﬀerent from rugosity, we generate augmentations xn+umby using weak colour transformations in place
of aﬃne transformations (1-pixel shifts), since weak photometric transformations are guaranteed to be fully
on-manifold. Details about the speciﬁc colour transformations are presented in appendix C.
3.3 Sharpness over Data-Driven Volumes
The measures introduced in Equations 2 and 5, capture local sharpness over inﬁnitesimal neighbourhoods of
input data points. To study how diﬀerent networks ﬁt the training data, we devise a method for estimating
losssharpnessovervolumescenteredateachtrainingpoint xn, asonemovesawayfromthepoint. Essentially,
we exploit a variant of Monte Carlo (MC) integration to capture sharpness over data-driven volumes, by
applying two steps. First, we integrate the Jacobian and Hessian norms along geodesic paths πp⊂Rdbased
atxn, on the data manifold local to each training point, for p= 1,...,P. Second, we estimate sharpness
over the volume covered by the loss along the Ppaths via MC integration. The following details each step.
5Published in Transactions on Machine Learning Research (04/2023)
Sharpness along geodesic paths For each training point (xn,yn)∈D, we aim to estimate loss sharpness
as we move away from xn, while traveling on the support of the data distribution. To do so, we exploit a
sequence of weak data augmentations of increasing strength to generate Ppaths πp⊂Rdin the input space,
each formed by connecting augmentations of xnin order of increasing strength.
Formally, letTs:Rd→Rd, represent a family of smooth transformations (data augmentation) acting on
the input space and governed by parameter s, controlling the strength S=/bardbls/bardbl2as well as the direction of
the augmentation in Rd. In general, the parameter s, interpreted as a suitably distributed random variable,
models the randomness of the transformation. Randomly sampling s, yields a value sp,kcorresponding to
a ﬁxed transformation Tsp,kof strength Sk. For instance, for aﬃne translations, sp,kmodels a random
radial direction sampled from a hypersphere centered at xn, with strength Skdenoting the magnitude of the
translation (e.g. 4-pixel shift). For photometric transformations, sp,kmay model the change in brightness,
contrast, hue, and saturation, with total strength Sk.
To generate on-manifold paths πpstarting from xn, we proceed as follows. First, we ﬁx a sequence of
K+ 1strengthsS0< S1< ... < SK, withS0= 0denoting the identity transformation ∀p. Then, for
each strength Sk, withk≥1,prandom directions sp,kare sampled, each with respective ﬁxed magnitude
/bardblsp,k/bardbl2=Sk. This yields Psequences of transformations {Tsp,k}K
k=0, each producing augmented versions
xp,k
nofxn, ordered by strength, xp,1
n≺...≺xp,K
n, and forming a path πp⊂Rd. Speciﬁcally, each path πp
approximates an on-manifold trajectory by a sequence of Euclidean segments xp,k+1
nxp,k
n, fork= 0,...,K.
The maximum augmentation strength SKcontrols the distance traveled from xn, while the number Kof
strengths used controls how ﬁne-grained the Euclidean approximation is. Pseudocode for generating geodesic
paths is presented in section D.
Volume integration Once a sequence of paths {πp}P
p=1is generated for xn, volume-based sharpness is
computed by integrating over each path πp, and normalizing the measure by the length len(πp)of each path:
1
PP/summationdisplay
p=11
len(πp)/integraldisplay
πpσ(x,yn)dx (6)
where σrepresents an inﬁnitesimal sharpness measure, namely the Jacobian and tangent Hessian norms at
(xn,yn). The same method can also be applied to accuracy and crossentropy loss to evaluate consistency
and conﬁdence of the models predictions over volumes. Figures 1c and 1d illustrate geodesic MC integra-
tion. For each training point, Pgeodesic paths are generated, each anchored to the data manifold by K
augmentations. Integrating inﬁnitesimal measures over each path returns a MC sample of sharpness along
πp. Then, volumetric sharpness is estimated by MC integration over Psamples. Importantly, the number
Pof paths is ﬁxed throughout all experiments, representing the number of MC samples for volume-based
integration. Finally, we take a mean-ﬁled view by averaging over the training set D:
1
PEDP/summationdisplay
p=11
len(πp)/integraldisplay
πpσ(x,yn)dx=1
NPN/summationdisplay
n=1P/summationdisplay
p=11
len(πp)/integraldisplay
πpσ(x,yn)dx (7)
Importantly, extending LeJeune et al. (2019), we replace Euclidean integration by geodesic integration over
a local approximation of the data manifold, by generating augmentations of increasing strength.
Crucially, the proposed MC integration captures average-case sharpness in proximity of the training data and
is directly related to the generalization ability of the studied networks, as opposed to worst-case sensitivity,
as typically considered in adversarial settings (Moosavi-Dezfooli et al., 2019). In fact, the random sampling
performed in Equation 7 is unlikely to hit adversarial directions, which are commonly identiﬁed by searching
the input space through an optimization process (Goodfellow et al., 2014; Szegedy et al., 2013).
To conclude our methodology, in section 3.4 we present the family of transformations Tsused for generating
trajectories πpthroughout our experiments.
6Published in Transactions on Machine Learning Research (04/2023)
0 20 40 60
Base Width0.00.20.40.60.81.0ErrorTrain Error
T est Error
Train Loss
0.000.250.500.751.001.251.501.75
Loss
(a)
0 20 40 60
Base Width0.40.50.60.70.80.91.0accuracy
0 20 40 60
Base Width1234crossentropy
0 20 40 60
Base Width51015202530jacobian
0 20 40 60
Base Width20406080tangent hessian
Volume
0
1
2
34
5
6
7 (b)
Figure 2: a) Double descent curve for the test error for ConvNets trained on CIFAR-10 with 20%noisy labels.
b) Average metrics integrated over volumes of increasing size. Volumes are denoted by the number Kof weak
augmentations used to generate each geodesic path. From left to right: average training accuracy, training
loss, Jacobian norm and Hessian norm, each plotted against model size. The dotted vertical line marks
the model width that achieves zero train error (i.e. the interpolation threshold ). All models are trained for
4k epochs. We observe accuracy over volumes increases monotonically with model size, while crossentropy
follows double descent. Combined, the two observations suggest that large networks conﬁdently predict
the training targets over increasingly large volumes around the training data (for increasing model size).
Importantly, interpolation is sharpat the interpolation threshold, even inﬁnitesimally at each training point
(blue curves), while increasing overparameterization produces smoothinterpolation, contrary to existing
intuition. Shaded areas mark standard deviations over 3seeds.
3.4 Weak Data Augmentation Strategies
ComputingsharpnessofinterpolationviaEquation6foreachdatapoint xnrequiresgenerating Ptrajectories
πpcomposed of augmentations of xnof controlled increasing strength. Furthermore, the augmented data
points{xp,k
n}K
k=0should lie in proximity of the base point xnin order for the Euclidean approximation
to be meaningful. Finally, to correctly estimate correlation between smoothness and the generalization
ability of the networks considered, volume-based sharpness should not rely on validation data points, i.e. the
augmentations xp,k
nshould be strongly correlated to xn, for eachp,k.
To satisfy the above, we modify a weak data augmentation algorithm introduced by Yu et al. (2018), which
allows to eﬃciently generate augmentations that lie in close proximity to the base training point xn, for
image data. Speciﬁcally, each base image xn, consisting of Cinput channels (e.g. C= 3for RGB images)
andh×wspatial dimensions, is interpreted as Cindependent matrices xn[c,:,: ]∈Rh×w, each factorized
using Singular Value Decomposition (SVD), yielding a decomposition xn[c,:,: ] =UcΣcVcT, where Σcis
a diagonal matrix whose entries are the singular values of xn[c,:,: ]sorted by decreasing magnitude. In
the original method, Yu et al. (2018) produce weak augmentations by randomly erasing one singular value
from the smallest ones, thereby obtaining a modiﬁed matrix ˜Σc, and then reconstructing each channel of the
base sample via Uc˜ΣcVcT. In this work, in order to generate Prandom augmentations of strength k,˜Σcis
obtained by erasing ksingular values Σc
i,i, fori=w−k−p+1,...,w−p, andp= 0,...,P−12. Essentially,
the augmentation strength is given by the number kof singular values erased, and Paugmentations of similar
strength are generated by erasing Psubsets of size kfrom the smallest singular values, for each channel c.
We note that this method produces augmented images that are highly correlated with the corresponding
base training sample, and as such they do not directly amount to producing validation data points. We refer
the reader to appendix E for further details. In the next section, we present our empirical study of sharpness
of interpolation for neural networks in relationship to double descent.
4 Experiments
In this section, we present our empirical exploration of input-space smoothness of the loss landscape of deep
networks as model size and number of training epochs vary. Focusing on implicit regularization (Neyshabur
2Assuming square spatial dimensions h=w.
7Published in Transactions on Machine Learning Research (04/2023)
et al., 2015) promoted by optimization and model architecture, we evaluate our sharpness measures on net-
works with increasing number of parameters, trained without any form of explicit regularization (e.g. weight
decay, batch normalization, dropout). We extend our analysis to common training settings in section 4.4.
Experimental setup We reproduce deep double descent by following the experimental setup of Nakkiran
et al. (2019b). Speciﬁcally, we train a family of ConvNets formed by 4convolutional stages of controlled
base width [w,2w,4w,8w], forw= 1,..., 64, on the CIFAR-10 dataset with 20%noisy training labels and
on CIFAR-100. All models are trained for 4k epochs using SGD with momentum 0.9and ﬁxed learning
rate. Following Arpit et al. (2019), to stabilize prolonged training, we use a learning rate warmup schedule.
Furthermore, we extend our empirical results to training settings more commonly found in practice, and
validate our main ﬁndings on a series of ResNet18s (He et al., 2015) of increasing base width w= 1,..., 64,
with batch normalization, trained with the Adam optimizer for 4k epochs using data augmentation. We
refer the reader to section B for a full description of our experimental setting. In section G.1, we extend our
main results to Transformer networks trained on machine translation tasks.
We begin our experiments by reproducing double descent for the test error for the ConvNets (Figure 2a).
Starting with small models and by increasing model size, a U-shaped curve is observed whereupon small
models underﬁt the training data, as indicated by high train and test error. As model size increases,
the optimal bias/variance trade-oﬀ is reached (Geman et al., 1992). Mid-sized models increasingly overﬁt
training data – as shown by increasing test error for decreasing train error and loss – until zero training error
is achieved, and the training data is interpolated. The smallest interpolating model size is typically referred
to asinterpolation threshold (Belkin et al., 2019). Near said threshold, the test error peaks. Finally, large
overparameterized models achieve improved generalization, as marked by decreasing test error, while still
interpolating the training set.
4.1 Loss Landscape Smoothness Follows Double Descent
In this section, we establish a strong correlation between double descent of the test error and smooth
interpolation of noisy training data. Figure 2b studies ﬁtting of training data for models at convergence
(training for 4k epochs) as model size increases. Starting with (inﬁnitesimal) sharpness at training points
(blue curve), we observe that training accuracy at convergence monotonically increases with model size,
with 100%accuracy reached at the interpolation threshold and maintained therefrom. At the same time,
crossentropy loss over volumes follows double descent, with peak near the interpolation threshold, and
then decreasing as model size grows. Similarly, the Jacobian and Hessian norms peak at the interpolation
threshold and then rapidly decrease, showing that all training points become stationary for the loss, and
that the landscape becomes ﬂatter as model size grows past the interpolation threshold. When all measures
are integrated over volumes of increasing size (number Kof augmentations per path), we observe how large
overparameterized models are able to smoothly ﬁt the training data over large volumes. This ﬁnding suggests
that – in contrast to the polynomial intuition of Figure 1a) – overparameterized networks interpolate training
datasmoothly (as intuitively depicted in Figure 1b).
Ourﬁnding extendstheobservationsof Novak etal. (2018)and LeJeuneet al.(2019) fromﬁxed-sizenetworks
to a spectrum of model sizes, and establishes a clear correlation with the test error peak in double descent.
Finally, the results substantiate the universal law of robustness (Bubeck & Sellke, 2021), showing that at
the interpolation threshold highest sensitivity to input perturbations is observed, while overparameterization
beyond the threshold promotes smoothness. Intriguingly, our ﬁndings represent mean sharpness as opposed
to the worst case studied by Bubeck & Sellke (2021), showing that the observed regularity is much stronger
in practice. In the following section, we study this behaviour in proximity of cleanly- and noisily-labeled
training samples. We refer the reader to section 4.4 for analogous results on ResNets trained with Adam.
4.2 Smooth Interpolation of Noisy Labels
In this section, we break down the noisily labeled training set into two subsets: cleanly-labeled points, and
training points with corrupted labels, and explore how ﬁtting is aﬀected by the training labels.
8Published in Transactions on Machine Learning Research (04/2023)
0.20.40.60.81.0cleanaccuracy
12345678crossentropy
10203040jacobian
20406080100120140tangent hessian
Volume
0
1
2
34
5
6
7
0 20 40 60
Base Width0.20.40.60.81.0corrupted
0 20 40 60
Base Width12345678
0 20 40 60
Base Width10203040
0 20 40 60
Base Width20406080100120140
Figure 3: Average accuracy, crossentropy, Jacobian and Hessian norms integrated over volumes of increasing
size (augmentations per path) around clean (top) and noisy (bottom) subsets of the CIFAR-10 training set
with 20%noisy labels. For models near the interpolation threshold, we observe a large increase in the loss for
increasing neighborhood size. At the interpolation threshold, sharp interpolation is observed for both clean
and noisy samples, with crossentropy, sensitivity (Jacobian norm) and curvature peaking over all volumes
considered. Larger models present a smoother loss landscape around training points, with the largest models
expressing a locally ﬂat landscape around each point. This ﬁnding shows that large networks are conﬁdently
and smoothly predicting the noisy labels around data points whose label was corrupted, suggesting that
smoothness emerging from overparameterization in fact hinders generalization locally to those points.
0 20 40 60
Base Width0.00.20.40.60.81.0ErrorTrain Error
T est Error
Train Loss
0.00.51.01.52.02.53.03.5
Loss
(a)
0 20 40 60
Base Width0.20.40.60.8accuracy
0 20 40 60
Base Width1234crossentropy
0 20 40 60
Base Width10203040jacobian
0 20 40 60
Base Width20406080100120140tangent hessian
Volume
0
1
2
34
5
6
7 (b)
Figure 4: a) Double descent for the test error for ConvNets trained on CIFAR-100. b) Average metrics
integrated over volumes of increasing size (number Kof augmentations per path). From left to right:
average training accuracy, crossentropy, Jacobian, and Hessian norm, each plotted against model size. All
models are trained for 4k epochs. For relatively complex datasets (i.e. with few samples per class), our
ﬁndings hold even without artiﬁcially corrupted labels, suggesting that the trends reported in this work are
not caused by synthetic noise. Shaded areas depict standard deviations over 3seeds.
Figure 3 reports accuracy, crossentropy, as well as sharpness measures computed on the clean subset of
CIFAR-10 (top), as well as the corrupted subset (bottom), for volumes of increasing size. We begin by
noting that small models ﬁt mostly the cleanly labeled data points, and show close to zero accuracy on
the noisily labeled data points, showing a bias towards learning simple patterns. We hypothesize that most
cleanly labeled samples act as “simple examples”, while noisily labeled ones provide “hard examples”, akin
to support vectors, for small size models. This behaviour is aligned with prior observations, reporting that
9Published in Transactions on Machine Learning Research (04/2023)
Figure 5: (Left) Test error (Middle) Train crossentropy (Right) Jacobian norm for ConvNets trained on
CIFAR-10 with 20%noisy labels. The heatmaps show each metric for increasing training epochs (y-axis)
and base width (x-axis). Models past the interpolation threshold (base width w= 15) undergo epoch-wise
double descent for each metric. Similar trends are observed for curvature, as measured by the Hessian norm.
All metrics are computed on the training set only, without geodesic Monte Carlo integration.
deep networks admit support vectors (Toneva et al., 2018) and that deep networks share the order in which
samples are ﬁtted (Hacohen et al., 2020). We refer the reader to Hacohen et al. (2020) for details.
As model size grows toward the interpolation threshold, networks ﬁt both clean and noisy samples (as
marked by increasing accuracy on both subsets), with large models consistently predicting the clean and
noisy labels over large volumes. At the same time, crossentropy local to each training point (blue curve)
approaches zero past the interpolation threshold, while volume-based crossentropy undergoes double descent.
Interestingly, this trend is observed both around cleanly- and noisily-labeled training samples, with peaks at
the interpolation threshold which are considerably more marked for noisy labels.
Our sharpness measures follow double descent for all volumes considered, even when no Monte Carlo integra-
tion is performed (blue curve). Importantly, curvature as measured by the Hessian norm rapidly decreases
as model size grows, showing that large networks smoothly interpolate both clean and noisy samples. Im-
portantly, we observe how the second descent in test error corresponds to improved ﬁtting of cleanly-labeled
samples, while the network lose their generalization ability locally to noisy labeled points.
In Figure 4a we extend the observations to CIFAR-100, where model-wise double descent is observed even
on the standard dataset without artiﬁcially corrupted labels. Similarly to what observed on CIFAR-10,
Figure 4b shows the loss landscape peaking in sharpness at the interpolation threshold, and then rapidly
decreasing as model size grows, with large networks smoothly ﬁtting the training set over increasingly large
volumes. Thisﬁndingsuggeststhatdoubledescentistiedtodatasetcomplexity, andthatthetrendsreported
in this work are not caused by artiﬁcially corrupted labels.
4.3 Epochwise Double-Descent
We now turn our attention to epoch-wise double descent, ﬁrst reported for the test error of deep networks
by (Nakkiran et al., 2019b). Figure 5 shows the test error (left), train crossentropy (middle), as well
as Jacobian norm (right) for ConvNets trained on CIFAR-10 with 20%noisy labels. We consolidate our
observations for each metric with heatmaps, in which the y-axis represents training epochs, and the x-axis
denotesthemodels’basewidth. Weobservethatmodelspasttheinterpolationthreshold(basewidth w= 15)
undergo epoch-wise double descent for each metric. At the same time, models with base width w < 15are
unable to reduce their test error within 4k training epochs, and this is associated to non-decreasing training
loss as well as Jacobian norm. We hypothesize that the model size aﬀects a model’s ability to interpolate the
training data, and therefore aﬀects the training dynamics and the occurrence of epoch-wise double descent.
10Published in Transactions on Machine Learning Research (04/2023)
0 20 40 60
Base Width0.00.20.40.60.81.0ErrorTrain Error
T est Error
Train Loss
0.000.250.500.751.001.251.50
Loss
(a)
0 20 40 60
Base Width0.50.60.70.80.9accuracy
0 20 40 60
Base Width0.250.500.751.001.251.50crossentropy
0 20 40 60
Base Width246810jacobian
0 20 40 60
Base Width51015202530tangent hessian
Volume
0
1
2
34
5
6
7(b)
Figure 6: (a) Double descent for the test error the CIFAR-10 with 20%noisy labels for a family of ResNet18s
of increasing base width w, trained with data augmentation. (b) Accuracy, crossentropy, Jacobian and
Hessian norms over volumes. All models are trained for 4k epochs. Analogous trends as observed for the
ConvNets holds in this case. However, the largest integration volume considered ( K= 7augmentations per
path), now shows considerably increased sharpness and loss curvature, while still undergoing double descent.
4.4 Practical Training Settings
So far, the training setting included the least amount of confounders (e.g. adaptive learning rates, explicit
regularization, skip connections, normalization layers) and focused on implicit regularization. In Figure 6,
we extend our ﬁndings to ResNets trained on CIFAR-10 with 20%noisy labels, with the Adam optimizer,
data augmentation ( 4-pixel shifts and random horizontal ﬂips), as well as batch normalization layer (see
appendix B for details). Both model-wise and epoch-wise trends reported for the ConvNets also hold for this
setup, with the interpolation threshold occurring at base width w= 18. We consolidate our model-wise and
epoch-wise ﬁndings with heatmaps in Figure 12 and 14. Interestingly, data augmentation causes the peak in
test error to occur earlier than the interpolation threshold. We hypothesize that the mismatch – which can
also be observed in related works (Nakkiran et al., 2019b) – is due to a lack of ﬁne grained control over model
size as base width wvaries. Importantly, for large volumes around training points ( K= 7augmentations
per path), training accuracy degrades and loss sharpness increases. However, all sharpness metrics undergo
double descent as model size grows, conﬁrming the trends reported in simpler training settings.
4.5 Towards Decoupling Smoothness from Generalization
Our experimental results suggest that double descent in the test error is closely related to input-space
smoothness. One possible interpretation is that models at the interpolation threshold learn small and
irregular decision regions, marked by high loss sharpness, while large models learn more regular decision
regions with wider margins, supporting the observations of Jiang et al. (2019).
In fact, as consistently observed in our experiments, on the one hand, models near the interpolation threshold
fail to smoothly interpolate all clean samples, while on the other hand large models can smoothly interpolate
the entire training set. This eﬀectively enforces a trade-oﬀ for which large models lose generalization ability
around noisy samples, but can correctly classify all clean samples. Assuming the train and test distributions
are similar (i.e. excluding covariate shifts), this would in turn result in improved average test error past
the interpolation threshold, as indeed observed in practice. To assess the validity of our interpretation,
we decouple smoothness from generalization by studying training settings in which smooth training set
interpolation hurts generalization. In this setting, we expect smooth interpolation to consistently emerge
with overparameterization, but this time without producing double descent in the test error. To corroborate
our interpretation, in principle, one would need to construct a nearest neighbour classiﬁer (either in input or
in feature space), and test whether predictions for each test sample are aﬀected by proximity to corrupted
samples. In the following, we propose a simple experiment to decouple smoothness from generalization,
without requiring knowledge of proximity of test samples to train samples.
First, we corrupt 20%of the CIFAR-100 training set with asymmetric label noise, such that 80%samples
of20randomly selected classes are perturbed. At test time, this enables us to split the test set into (1)
samples whose classes have been corrupted, and (2) samples belonging to unperturbed classes. Figure 7a
11Published in Transactions on Machine Learning Research (04/2023)
0.20.40.60.8cleanaccuracy
12345crossentropy
51015202530jacobian
20406080tangent hessian
Volume
0
1
2
34
5
6
7
0 20 40 60
Base Width0.20.40.60.8corrupted
0 20 40 60
Base Width12345
0 20 40 60
Base Width51015202530
0 20 40 60
Base Width20406080
(a)
0 10 20 30 40 50 60
Base Width0.00.20.40.60.81.0ErrorTrain error
T est error: clean
T est error: corrupted
0 10 20 30 40 50 60
Base Width0.00.20.40.60.81.0Error
0 20 40 60
Base Width024681012jacobian
Volume
0
1
2
34
5
6
7 (b)
Figure7: Decoupling smoothness from generalization . Wepresentanexperimentinwhich 20randomly
selected classes of the CIFAR-100 training split are corrupted with asymmetric label noise, perturbing 80%
training labels within each class, for a total of 20%corrupted training samples. At test time, this enables
splitting the test set into classes that have been corrupted at train time, and unperturbed classes. (a)
Overparameterization promotes a smooth and ﬂat loss landscape around both cleanly-labeled as well as
noisy training samples under asymmetric label noise. (b, top) Conﬁrming our hypothesis, double descent for
the test error can still be observed for the unperturbed classes, while the trend disappears for the corrupted
classes. This ﬁnding shows that overparameterization promotes smoothness in the input variable, which is
aligned with generalization only around cleanly labeled points. (b, bottom) For networks trained on 100%
noisy labels, smooth interpolation still follows double descent over volumes of increasing size around training
points, but such property in this case is not aligned with generalization.
showsthat, evenunderstrongasymmetricnoise, overparameterizationpromotesinput-spacesmoothnessover
increasingly large volumes around both clean and noisy training samples. Perhaps surprisingly, in Figure 7b
(top) at test time double descent is still observed for test samples belonging to unperturbed classes, while the
trend disappears for the corrupted classes. This conﬁrms our interpretation and shows that double descent
should be understood in terms of input-space smoothness, and its relation to generalization.
Second, we train ResNets on CIFAR-10 with all training labels corrupted (Figure 7b, bottom). Also in this
setting, loss sharpness over volumes follows double descent, peaking near the interpolation threshold, and
decreasing with increasing model size. Trivially, all networks in this setting lose their generalization ability,
with performance close to random chance. This ﬁnding shows that overparameterization promotes smooth
interpolation of the training data, and that such property is not necessarily aligned with generalization.
5 Conclusions
In this work, we present geodesic Monte Carlo integration tools to study the input space of neural networks,
providing intuition – built on extensive experiments – on how neural networks ﬁt training data. We present a
strong correlation between epoch-wise as well as model-wise double descent for the test error and smoothness
of the loss landscape in input space. Our experiments show that overparameterization promotes input space
regularity via smooth interpolation of clean and noisy training data, which is aligned with improved gener-
alization for datasets with relatively low ratio of label noise. Crucially, contrary to intuitions in polynomial
regression, deep networks uniformly predict noisy training targets over volumes around noisily-labeled train-
ing samples – a behaviour which may have severe negative impact in practical applications with imbalanced
training sets or with covariate shifts of the population distribution.
12Published in Transactions on Machine Learning Research (04/2023)
Consistently in our experiments, we observe a peak in test error and loss sharpness near the interpolation
threshold, which decreases for better generalizing models. Finally, for increasing volumes around each
training point, we observe that overparametrization promotes ﬂatter minima of the loss in input space ,
providing initial clues as to why large overparameterized models generalize better, and corroborating the
ﬁndings of Somepalli et al. (2022) on regularity of decision boundaries of overparameterized classiﬁers, as
well as Gamba et al. (2022) on input-space regularity.
Our analysis substantiates thelaw of robustness of Bubeck& Sellke (2021), and extendsthe ﬁndingsof Novak
et al. (2018) to experimental settings with controlled model size. We hypothesize that overparameterization
aﬀects the dynamics of optimization and interpolation, promoting a smooth loss landscape. An interesting
open problem is characterizing the impact of individual layers on interpolation, as model size grows.
Finally, our analysis opens the question of whether increased interpolation smoothness is to be attributed
to the model architecture, the optimizer, or a combination of both. First, increased network width, as
controlled in our experiments, has been recently connected to the existence of paths connecting critical
points for the optimizer (Simsek et al., 2021), suggesting that model width plays an important role in
aﬀecting the dynamics of the optimizer. Particularly, one or mode connected manifold of minima may allow
wider networks to retain interpolation while at the same time optimizing for input-space smoothness (Li
et al., 2021). Second, understanding the existence of implicit regularization promoted by the optimizer is at
present an active area of research. On the one hand, several studies argue that stochastic optimization, and
the potential implicit regularization eﬀect of mini-batch noise, are not required for generalization (Chiang
et al., 2023; Paquette et al., 2022; Geiping et al., 2020). On the other hand, current models of double descent
hypothesize that stochastic noise is an important component in explaining implicit regularization and double
descent in deep learning (Li et al., 2021; Blanc et al., 2020).
Acknowledgments
This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program
(WASP) funded by the Knut and Alice Wallenberg Foundation. Scientiﬁc computation was enabled by the
supercomputing resource Berzelius provided by National Supercomputer Centre at Linköping University and
the Knut and Alice Wallenberg foundation. The work was partially funded by Swedish Research Council
project 2017-04609.
References
Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on edge of stability in
deep learning. arXiv preprint arXiv:2205.09745 , 2022.
Devansh Arpit, Víctor Campos, and Yoshua Bengio. How to initialize your network? robust initialization
for weightnorm & resnets. Advances in Neural Information Processing Systems , 32, 2019.
Peter L. Bartlett, Philip M. Long, Gábor Lugosi, and Alexander Tsigler. Benign overﬁtting in lin-
ear regression. Proceedings of the National Academy of Sciences , 117(48):30063–30070, 2020. doi:
10.1073/pnas.1907378117.
Mikhail Belkin, Daniel J Hsu, and Partha Mitra. Overﬁtting or perfect ﬁtting? risk bounds for classiﬁcation
and regression rules that interpolate. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 31. Curran
Associates, Inc., 2018.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice
and the classical bias–variance trade-oﬀ. Proceedings of the National Academy of Sciences , 116(32):15849–
15854, 2019.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM Journal on
Mathematics of Data Science , 2(4):1167–1180, 2020.
13Published in Transactions on Machine Learning Research (04/2023)
Yoshua Bengio. Deep learning of representations: Looking forward. In International conference on statistical
language and speech processing , pp. 1–37. Springer, 2013.
Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural networks
driven by an ornstein-uhlenbeck like process. In Conference on learning theory , pp. 483–513. PMLR, 2020.
Sébastien Bubeck and Mark Sellke. A universal law of robustness via isoperimetry. Advances in Neural
Information Processing Systems , 34, 2021.
Mauro Cettolo, Christian Girardi, and Marcello Federico. Wit3: Web inventory of transcribed and translated
talks. In Proceedings of the Conference of European Association for Machine Translation (EAMT) , pp.
261–268, 2012.
Xiangyu Chang, Yingcong Li, Samet Oymak, and Christos Thrampoulidis. Provable beneﬁts of overparam-
eterization in model compression: From double descent to pruning neural networks. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence , volume 35, pp. 6974–6983, 2021.
Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust overﬁtting may be
mitigated by properly learned smoothening. In International Conference on Learning Representations ,
2021.
P. Chiang, R. Ni, D. ˜Y. Miller, A. Bansal, J. Geiping, M. Goldblum, and T. Goldstein. Gradient-based opti-
mization is not necessary for generalization in neural networks. In International Conference on Learning
Representations , 2023.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep
nets. InInternational Conference on Machine Learning , pp. 1019–1028. PMLR, 2017.
David L. Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding techniques for high-
dimensional data. Proceedings of the National Academy of Sciences , 100(10):5591–5596, 2003. doi: 10.
1073/pnas.1031596100.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for
eﬃciently improving generalization. In International Conference on Learning Representations , 2020.
Matteo Gamba, Stefan Carlsson, Hossein Azizpour, and Mårten Björkman. Hyperplane arrangements of
trained convnets are biased. arXiv preprint arXiv:2003.07797 , 2020.
Matteo Gamba, Adrian Chmielewski-Anders, Josephine Sullivan, Hossein Azizpour, and Mårten Björkman.
Are all linear regions created equal? In International Conference on Artiﬁcial Intelligence and Statistics ,
pp. 6573–6590. PMLR, 2022.
Mario Geiger, Stefano Spigler, Stéphane d’Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio Biroli, and
Matthieu Wyart. Jamming transition as a paradigm to understand the loss landscape of deep neural
networks. Physical Review E , 100(1):012115, 2019.
Jonas Geiping, Micah Goldblum, Phil Pope, Michael Moeller, and Tom Goldstein. Stochastic training is not
necessary for generalization. In International Conference on Learning Representations , 2020.
Stuart Geman, Elie Bienenstock, and René Doursat. Neural Networks and the Bias/Variance Dilemma.
Neural Computation , 4(1):1–58, 01 1992. ISSN 0899-7667. doi: 10.1162/neco.1992.4.1.1.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572 , 2014.
Guy Hacohen, Leshem Choshen, and Daphna Weinshall. Let’s agree to agree: Neural networks share
classiﬁcation order on real datasets. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research ,
pp. 3950–3960. PMLR, 13–18 Jul 2020.
14Published in Transactions on Machine Learning Research (04/2023)
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-
level performance on imagenet classiﬁcation. In Proceedings of the IEEE International Conference on
Computer Vision , pp. 1026–1034, 2015.
Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural computation , 9(1):1–42, 1997.
Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the generalization gap in deep
networks with margin distributions. In International Conference on Learning Representations , 2019.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang.
On large-batch training for deep learning: Generalization gap and sharp minima. In International Con-
ference on Learning Representations , 2017.
Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Rong Ge, and Sanjeev Arora.
Explaininglandscapeconnectivityof low-costsolutions formultilayernets. Advances in neural information
processing systems , 32, 2019.
DanielLeJeune,RandallBalestriero, HamidJavadi, andRichardGBaraniuk. Implicitrugosityregularization
via data augmentation. arXiv preprint arXiv:1905.11639 , 2019.
Yixuan Li, Jason Yosinski, Jeﬀ Clune, Hod Lipson, and John Hopcroft. Convergent learning: Do diﬀerent
neural networks learn the same representations? arXiv preprint arXiv:1511.07543 , 2015.
Zhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after sgd reaches zero loss?–a mathematical
framework. In International Conference on Learning Representations , 2021.
Chao Ma and Lexing Ying. On linear stability of sgd and input-smoothness of neural networks. In M. Ran-
zato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural
Information Processing Systems , volume 34, pp. 16805–16817. Curran Associates, Inc., 2021.
Matouš Macháček and Ondřej Bojar. Results of the wmt14 metrics shared task. In Proceedings of the Ninth
Workshop on Statistical Machine Translation , pp. 293–301, 2014.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robustness via
curvature regularization, and vice versa. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , June 2019.
Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless interpolation of
noisy data in regression. IEEE Journal on Selected Areas in Information Theory , 1(1):67–83, 2020.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double
descent, 2019a. URL https://windowsontheory.org/2019/12/05/deep-double-descent/ .
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learning
Representations , 2019b.
Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis. Advances
in neural information processing systems , 23, 2010.
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-Julien,
and Ioannis Mitliagkas. A modern take on the bias-variance tradeoﬀ in neural networks. arXiv preprint
arXiv:1810.08591 , 2018.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. In International Conference on Learning Representations
Workshop Track , 2015.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role of
over-parametrization in generalization of neural networks. In International Conference on Learning Rep-
resentations , 2018.
15Published in Transactions on Machine Learning Research (04/2023)
RomanNovak, YasamanBahri, DanielAAbolaﬁa, JeﬀreyPennington, andJaschaSohl-Dickstein. Sensitivity
and generalization in neural networks: an empirical study. In International Conference on Learning
Representations , 2018.
Courtney Paquette, Elliot Paquette, Ben Adlam, and Jeﬀrey Pennington. Implicit regularization or implicit
conditioning? exact risk trajectories of sgd in high dimensions. In Advances in Neural Information
Processing Systems , 2022.
Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension
of images and its impact on learning. In International Conference on Learning Representations , 2020.
Leslie Rice, Eric Wong, and Zico Kolter. Overﬁtting in adversarially robust deep learning. In Hal Daumé
III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning , volume
119 ofProceedings of Machine Learning Research , pp. 8093–8104. PMLR, 13–18 Jul 2020.
Mihaela Rosca, Theophane Weber, Arthur Gretton, and Shakir Mohamed. A case for new neural network
smoothness constraints. NeurIPS Workshops , 2020.
Berﬁn Simsek, François Ged, Arthur Jacot, Francesco Spadaro, Clement Hongler, Wulfram Gerstner, and
Johanni Brea. Geometry of the loss landscape in overparameterized neural networks: Symmetries and
invariances. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference
on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp. 9722–9732. PMLR,
18–24 Jul 2021.
Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. Advances in Neural Information
Processing Systems , 33:22045–22055, 2020.
Gowthami Somepalli, Liam Fowl, Arpit Bansal, Ping Yeh-Chiang, Yehuda Dar, Richard Baraniuk, Micah
Goldblum, and Tom Goldstein. Can neural nets learn the same model twice? investigating reproducibility
and double descent from the decision boundary perspective. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 13699–13708, 2022.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 , 2013.
Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Ge-
oﬀrey J Gordon. An empirical study of example forgetting during deep neural network learning. In
International Conference on Learning Representations , 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30,
2017.
Zeke Xie, Issei Sato, and Masashi Sugiyama. A diﬀusion theory for deep learning dynamics: Stochastic gra-
dient descent exponentially favors ﬂat minima. In International Conference on Learning Representations ,
2020.
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance trade-oﬀ for
generalization of neural networks. In International Conference on Machine Learning , pp. 10767–10777.
PMLR, 2020.
Tao Yu, Huan Long, and John E Hopcroft. Curvature-based comparison of two neural networks. In 2018
24th International Conference on Pattern Recognition (ICPR) , pp. 441–447. IEEE, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. International Conference on Learning Representations , 2018.
16Published in Transactions on Machine Learning Research (04/2023)
A Appendix
Section B summarizes our experimental setup, while Section C, D, and E respectively detail the tangent
Hessian computation method, the geodesic path generation algorithm, and the weak data augmentation
strategy used for geodesic Monte Carlo integration. Finally, in section F we extend our discussion of related
works. Additional experiments are reported in section G.
B Network Architectures and Training Setup
Network Architectures The ConvNets and ResNets used follow the experimental settings of Nakkiran
et al. (2019b), with the only diﬀerence that we disable batch normalization in order to focus our study on
implicit regularization. In summary, the ConvNets are composed of 4convolutional stages (each with a
single conv + ReLU block) with kernel size 3×3, stride 1, padding 1, each followed by maxpooling of stride
2and kernel size 2×2. Finally, a max pooling layer of stride 2and kernel size 2×2is applied, followed
by a linear layer. The Residual networks used in this study are ResNet18s (He et al., 2015) without batch
normalization.
Both ConvNets and ResNets are formed by 4convolutional stages at which the number of learned feature
maps doubles, i.e. the base width of each stage follows the progression [w,2w,4w,8w], withw= 64denoting
a standard ResNet18. To control the number of parameters in each network, the base width wvaries from
1to64.
Throughout our experiments, augmentations ˜xnof a sample (xn,yn)are labelled with their respective
(potentially noisy) training target yn.
Dataset Splits To tune the training hyperparameters of all networks, a validation split of 1000samples
was drawn uniformly at random from the training split of CIFAR-10 and CIFAR-100.
ConvNet Training Setup The training settings are the same for CIFAR-10 and CIFAR-100. All Con-
vNets are trained for 4k epochs with SGD with momentum 0.9, ﬁxed learning rate 1e−3, batch size 128,
and no weight decay. All learned layers are initialized with Pytorch’s default weight initialization (version
1.11.0). To stabilize prolonged training in the absence of batch normalization, we use learning rate warmup:
starting from a base value of 1e−4the learning rate is linearly increased to 1e−3during the ﬁrst 5epochs
of training, after which it remains constant at 1e−3.
ResNet Training Setup All ResNets are trained for 4k epochs using Adam with base learning rate 1e−4,
batch size 128, and no weight decay. All learned layers are initialized with Pytorch’s default initialization
(version 1.11.0). All residual networks are trained with data augmentation, consisting of 4−pixelrandom
shifts, and random horizontal ﬂips.
Computational Resources Our experiments are conducted on a local cluster equipped with NVIDIA
TeslaA100s with 40GB onboard memory. For each dataset and architecture, we train 64diﬀerent networks
for4000epochs with 3diﬀerent seeds. The total time for computing our experiments, excluding training
networks and hyperparameter ﬁnetuning, amounts to approximately 6GPU years. Furthermore, computing
our statistics requires evaluating per-sample Jacobians for each training point and corresponding augmenta-
tions, for increasing volumes around each point. For each training setting, this was performed for 72model
checkpoints collected during training, to produce the heatmaps in Figures 5, 11, 12, 13 and 14.
C Tangent Hessian Computation
To estimate the tangent Hessian norm at a point xnthrough Equation 5, we approximate the tangent space
to the data manifold local to xnby using a set of random weak augmentations of xn. To guarantee that all
augmentations xn+um, as well as the displacements xn+δumlie on the data manifold, we use weak colour
augmentations as follows.
17Published in Transactions on Machine Learning Research (04/2023)
Color augmentations
0 5 10 15 20 22 25 28 30
Erased SVD values
Figure 8: (Left) Visualization of random colour augmentations used to estimate the tangent Hessian norm.
Each row represents a set of random augmentation, with the ﬁrst image per-row showing the corresponding
base sample. (Right) Each row represents SVD augmentations of increasing strength. Also in this case, the
ﬁrst column represents the base sample used to generate the corresponding augmentations in each row.
For each sample xn, we apply in random order the following photometric transformations:
•random brightness transformation in the range [0.9,1.1], with 1. denoting the identity transforma-
tion.
•random contrast transformation in [0.9,1.1], with 1. denoting the identity transformation.
•random saturation transformation in [0.9,1.1], with 1. denoting the identity transformation.
•random hue transformation in [−0.05,0.05], with 0. denoting the identity transformation.
Furthermore, a step size δ= 0.1is used for computing the ﬁnite diﬀerences in Equation 5. 4augmentations
xn+umare sampled for each point. All randomness is controlled to ensure reproducibility. Figure 8 (left)
shows a visualization of the colour augmentations used.
D Geodesic Paths Generation
In this section, we provide pseudocode for the algorithm used for generating geodesic paths, used for Monte
Carlo integration. Let x0∈Rddenote a training point, which we use as the starting point of geodesic paths
πpemanating from x0. LetTs:Rd→Rddenote a family of smooth transformations (data augmentation),
dependent on a parameter scontrolling the magnitude and direction of the transformation (e.g. radial
direction and displacement for pixel shifts). Let S:={s1,...,sK}denote a sequence of parameters for the
familyTs, each with strength Sk=/bardblsk/bardbl2fork= 1,...,K, such that S1< ... < SK. Then, Algorithm 1
returns a geodesic path π: [0,1]→Rd, based at x0, i.e.π(0) = x0, which is anchored to the data manifold
local to x0by a sequence of augmentations of increasing strength, for k= 1,...,K.
Particularly, Algorithm 1 can be applied Ptimes to generate paths πpemanating from x0. Finally, by
integrating metrics of interest (e.g. Jacobian and tangent Hessian norms) along each path πp, we obtain
18Published in Transactions on Machine Learning Research (04/2023)
Algorithm 1 Generate a geodesic path πemanating from a training point x0.
1:function Geodesic Path (x0,Ts,S:={s1,...,sK})
2:P←{ x0} ⊿Set of on-manifold points.
3: forsk∈Sdo
4: sample s∼sk⊿Sample augmentation of strength Sk=/bardbls/bardbl2.
5: xk=Ts(x0) ⊿Generate weak data augmentation.
6:P←P∪{ xk}
7: end for
8: returnP ⊿Set of data augmentations forming a path
π, with points sorted by distance from x0.
9:end function
estimates of sharpness of the loss along each path, which we use as Monte Carlo samples in Equation 7 for
estimating volume-sharpness. We recall that the size of the volume considered is controlled by the maximum
augmentation strength SKused for generating weak augmentations, which is proportional to the distance
travelled away from x0in input space.
E SVD Augmentation
The SVD augmentation method presented in section 3.4 allows for generating images that lie in close prox-
imity to the base sample xn. Figure 8 shows an illustration of the original image (ﬁrst column) and several
augmented images, as the augmentation strength (number of erased singular values) increases. Figure 9
shows the average (over the CIFAR-10 training set) Euclidean distance of augmented samples from their
respective base sample, as well as the length of the polygonal path formed by connecting augmentations of
increasing strength. We note that for k <30, in expectation, augmentations lie in close proximity to the
original base sample in Euclidean space.
5 10 15 20 25 30
Augmentation strength05101520253035L2 distanceDist. from base sample
Path length
Figure 9: Average L2 distance from the base samples, for augmentations of increasing strength.
F Extended Related Works
In this section, we extend the related work discussion of section 2 to contextualize our ﬁndings in relationship
to linear models.
In linear regression, the model-wise double descent phenomenon has been studied in terms of harmless inter-
polation (Muthukumar et al., 2020) or benign overﬁtting of noisy data (Bartlett et al., 2020), by controlling
the number of input features θconsidered. Particularly, for the least squares solution to a noisy linear
19Published in Transactions on Machine Learning Research (04/2023)
regression problem with random input and features, the impact of noise on generalization is mitigated by
the abundance of weak features (Belkin et al., 2020). In this context, interpolation is studied for data whose
populationisdescribedbynoisyobservationsfromalineargroundtruthmodel. Inthefollowing, wedelineate
the main diﬀerences between linear regression and the experimental setting considered in our study.
We begin by noting that, since the model function of linear models has zero curvature (both w.r.t. model
input xand parameters θ), the only source of nonlinearity and curvature in linear regression is the error
function (MSE). To see this, let f(x,θ) =θTxdenote a linear regression model, estimated by minimizing
the mean squared error L(θ,x,y) =1
2NN/summationtext
n=1(f(xn,θ)−yn)2, whereynis a noisy target∀n. Then, the error
functionLhas constant curvature H=/bardbl∂2L
∂x∂xT/bardbl2=/bardblθθT/bardbl2, independent of x.
In contrast, we study the case of nonlinear classiﬁcation problems and nonlinear models, which have notable
diﬀerences from the linear case. First, there is no a priori closed form solution of the learning problem, thus
providing relevance to empirical studies. Second, curvature of the model function is non-constant, and the
function may oscillate arbitrarily outside of the training data (this is known as the Runge phenomenon).
Third, studies that rely exclusively on the test error suggest that interpolation is harmless also in overpa-
rameterized nonlinear models. Finally, the model function of convolutional architectures is independent of
input-data dimensionality, and the relationship between complexity of the model function and its underlying
parameterization is therefore implicit.
In this setting, we experimentally show that, in the interpolating regime, (1) curvature at training points
depends non-monotonically on model size; (2) oscillations occur especially for small interpolating models,
whichareworstaﬀectedbynoise; (3)largemodelsachievelow-curvatureinterpolationofbothcleanandnoisy
samples (in contrast with the polynomial intuition), and such property is observed over large volumes (non-
zero measure) around each training point (in contrast with the Runge phenomenon, thus providing evidence
ofimplicit regularization); (4)Interpolationof noiseimpactsgeneralizationeven forlargemodels(contraryto
the overparameterized linear regression case); (5) Double descent observed for input space curvature occurs
even when ﬁtting 100%noisy data, more clearly pinpointing properties that are consistently promoted by
overparameterization in deep nonlinear networks.
Our methodology enables the study of sharpness of ﬁt of training data for nonlinear models, providing a
comparative study of the regularity with which diﬀerent parameterizations achieve interpolation and (in
some cases) generalization.
G Additional Experiments
G.1 Transformers
0 100 200 300 400 500
Embedding Dimension0100200300400500Erroriwslt14_de_en
wmt14_en_fr
(a)
0 100 200 300 400 500
Embedding Dimension0.0050.0100.0150.0200.0250.030 Loss Jacobian normiwslt14_de_en
wmt14_en_fr (b)
Figure 10: a) Double descent of the test error for transformers trained on translation tasks, as the embed-
ding dimension and model width vary. b) Average Jacobian norm .
20Published in Transactions on Machine Learning Research (04/2023)
We consider multi-head attention-based Transformers (Vaswani et al., 2017) for neural machine translation
tasks. We vary model size by controlling the embedding dimension de, as well as the width hof all fully
connected layers, which we set to h= 4defollowing the architecture described in Vaswani et al. (2017). We
train the transformer networks on the WMT’14 En-Fr task (Macháček & Bojar, 2014), as well as ISWLT’14
De-En (Cettolo et al., 2012). The training set of WMT’14 is reduced by randomly sampling 200k sentences,
ﬁxed for all models. The networks are trained for 80k gradient steps, to optimize per-token perplexity, with
10%label smoothing, and no dropout, gradient clipping or weight decay.
For both datasets, Figure 10a shows the double descent curve for the test error for both datasets considered.
Figure 10b extends our main result beyond vision models, showing that loss sharpness at each training point,
as measured by the Jacobian norm, follows double descent for the test error.
G.2 ConvNets
Figures 11 and 13 summarize our main ﬁndings with heatmaps showing modelwise and epochwise trends for
the test error, train loss, as well as our sharpness metrics, individually computed over the clean and noisy
subsets of CIFAR-10.
Figure 11: Test error (left), crossentropy loss over cleanly-labelled training samples (middle) and corrupted
training samples (right) over epochs (y-axis) for diﬀerent model sizes (x-axis), for ConvNets on CIFAR-10.
G.3 ResNets
Figures 12 and 14 present heatmaps showing modelwise and epochwise trends for the test error, train loss,
as well as our sharpness metrics, individually computed over the clean and noisy subsets of CIFAR-10.
Figure 12: Test error (left), crossentropy loss over cleanly-labelled training samples (middle) and corrupted
training samples (right) over epochs (y-axis) for diﬀerent model sizes (x-axis), for ResNets on CIFAR-10.
21Published in Transactions on Machine Learning Research (04/2023)
Figure13: (Leftcolumn)MetricsevaluatedonthetrainingsetwithoutMonteCarlointegrationonConvNets.
(Right column) Monte Carlo integration over a neighborhood with paths consisting of 7augmentations.
22Published in Transactions on Machine Learning Research (04/2023)
Figure 14: (Left column) Metrics evaluated on the training set without Monte Carlo integration on ResNets.
(Right column) Monte Carlo integration over a neighborhood with paths consisting of 7augmentations.
23