Under review as submission to TMLR
Auditor Fairness Evaluation via Learning Latent Assessment
Models from Elicited Human Feedback - Rebuttal (ver.2)
Anonymous authors
Paper under double-blind review
Abstract
Algorithmic fairness literature presents numerous mathematical notions and metrics, and
also points to a tradeoﬀ between them while satisﬁcing some/all of them simultaneously.
Furthermore, the contextual nature of fairness notions makes it diﬃcult to automate bias
evaluation in diverse algorithmic systems. Therefore, in this paper, we propose a novel
model called latent assessment model (LAM) to characterize binary feedback provided by
humanauditors,byassumingthattheauditorcomparestheclassiﬁer’soutputtohis/herown
intrinsicjudgmentforeachinput. Weprovethatindividualand/orgroupfairnessnotionsare
guaranteed as long as the auditor’s intrinsic judgments inherently satisfy the fairness notion
at hand, and are relatively similar to the classiﬁer’s evaluations. We also demonstrate this
relationship between LAM and traditional fairness notions on three well-known datasets,
namely COMPAS, German credit and Adult Census Income datasets. Furthermore, we also
derive the minimum number of feedback samples needed to obtain probably approximately
correct (PAC) learning guarantees to estimate LAM for black-box classiﬁers. Moreover, we
propose a novel multi-attribute reputation measure to evaluate auditor’s preference towards
various fairness notions as well as sensitive groups. These guarantees are also validated
using standard machine learning algorithms, which are trained on real binary feedback
elicited from 400 human auditors regarding COMPAS.
1 Introduction
Several machine learning (ML) based systems have recently been reported as being discriminatory with
respect to the sensitive attributes (e.g. race and gender) in a variety of application domains, such as recom-
mender systems in criminal justice (Angwin et al., 2016), e-commerce services in online markets (Fisman &
Luca, 2016), and life insurance premiums (Waxman, 2018). Most of these applications have outcomes that
depend on subjective human judgements which vary drastically across diﬀerent people. Typically, these out-
come labels are obtained on a case-by-case basis from diﬀerent types of individuals (typically subject matter
experts who have rich experience in their respective ﬁelds.). However, experts also exhibit biases especially
when it comes to subjective judgements. For instance, in criminal justice applications, one may argue that
a ‘just’ outcome for a parole application by a Black applicant is to deny the bail since he/she has a high
chance to recidivate back to crime since the applicant lives in an unsafe neighborhood. On the other hand,
the same applicant could be presented with a bail if he/she has no prior counts and has a clean record in the
past. This idea of justice where everyone should be treated fairly based on their own features/merit, and
independent of how others are treated, is called non-comparative fairness (Feinberg, 1974). In a practical
data collection process, outcome labels are elicited from a non-comparative fairness standpoint since each
person is presented with one case at a time (for example, readers may refer to (Goel & Faltings, 2019)).
However, these judgements are usually subjective and even experts may not always arrive at a consensus on
one correct verdict. Since the labelers suﬀer from inherent biases, their labels are often subject to inquisition
(Cooper et al., 2021). This dearth of correct labels invalidates the quantiﬁcation of ML performance using
accuracy, which measures the alignment of classiﬁer’s outcome with the correct label found in training data.
On the contrary, algorithmic fairness literature do not question the ﬁdelity of training data and discusses the
quantiﬁcation of biases and tradeoﬀs with accuracy using comparative fairness notions (Mehrabi et al., 2021).
1Under review as submission to TMLR
Comparative fairness notions primarily focus on issues such as equality, similarity and proportionality, which
questions whether an individual is treated/serviced ‘justly’ in comparison to that received by others. Biases
measured by comparative notions can be broadly classiﬁed into two types, namely disparate treatment , where
diﬀerentgroupsofpeoplearetreateddiﬀerently; and disparate impact , wherediﬀerentgroupsofpeopleobtain
diﬀerent proportions of outcomes. Individual fairness (Dwork et al., 2012) quantiﬁes disparate treatment
via comparing the similarity between two individuals based on a task-speciﬁc distance metric. On the other
hand, group fairness notions (Caton & Haas, 2020; Chouldechova & Roth, 2018; Hardt et al., 2016b) quantify
disparate impact by comparing the diﬀerences in outcome proportions (e.g. true positive, false positive rates,
predictive positive value) across diﬀerent sensitive groups. In addition to measuring comparative fairness
in ML systems, various mitigation techniques have been proposed to alleviate these biases in the literature.
These mechanisms are typically categorized into three types: (i) pre-processing techniques that modiﬁes
the input to the ML model (Kamiran & Calders, 2012), (ii) in-processing techniques that retrain the ML
model (Kamishima et al., 2012; Zafar et al., 2017), and (iii) post-processing techniques that modify the
output labels (Hardt et al., 2016b; Corbett-Davies et al., 2017). Each of these approaches either maximize
accuracy (or some loss function) under prescribed comparative fairness constraints, or maximize comparative
fairness under tolerable constraints on accuracy loss. Finally, a fundamental tradeoﬀ between comparative
fairness and accuracy has also been investigated based on which, a compromise in accuracy was proposed
to obtain fairness improvements in the system. However, comparative fairness notions cannot be completely
automated and demands human feedback to identify appropriate fairness notions for system evaluation.
Heterogeneous stakeholders compete against each other to enforce their preferred fairness notions, all of
which cannot be satiated simultaneously due to a fundamental tradeoﬀ that exists between comparative
fairness notions (Chouldechova, 2017; Kleinberg et al., 2016). This dogﬁght between various stakeholders
regardingtheselectionofanappropriatecomparativefairnessmetricisalsocontext-dependent(Binns,2018),
due to diﬀerences in protected groups across applications. Therefore, several researchers have attempted to
explore human perception of fairness to identify the appropriate comparative fairness notion. For more
details, the reader may refer to Section 1.2.
The inability of comparative notions to enable a reliable algorithmic fairness analysis, combined with the
desertion of non-comparative fairness perspective in the past, leads to a major predicament in the design of
fair ML algorithms. To the best of our knowledge, this paper makes the ﬁrst attempt to address this gap via
modeling human auditors from a non-comparative fairness perspective, and develop a scalable method to
evaluate high-dimensional ML algorithms. Furthermore, this paper also assumes that the auditor provides a
binary feedback regarding system fairness since the auditor’s intrinsic evaluation of labels is a subconscious
phenomenon and is practically diﬃcult to quantify in most applications.
1.1 Technical Contributions
The main contributions of the paper are four-fold. Firstly, a novel latent assessment model (LAM) was
proposed to represent binary human feedback regarding the fairness of the system, from a noncomparative
perspective. Unlike most of the past literature on human perception of fairness, the proposed model allows
human auditors to provide their feedback at will, without enforcing any speciﬁc fairness notion artiﬁcially
(Gillen et al., 2018). Secondly, we emphasize that the availability of non-comparative fairness does not
necessarily dispense with the need of comparative fairness. We prove that a system/entity satisﬁes individual
and/or group fairness notions if the auditor exhibits LAM in his/her fairness evaluations. We also show that
converse holds true in the case of individual fairness. Since both the system and auditor rules are hidden, we
compute PAC learning guarantees on algorithmic auditing based on binary feedback obtained from human
auditors. Third, we introduce a novel multi-attribute reputation measure to evaluate auditor’s inherent
biases based on various comparative fairness notions as well as sensitive attributes. Lastly, we validate
the relationships with comparative fairness notions on three real datasets, namely COMPAS, Adult Income
Census and German credit datasets. Using the feedback data of 400 crowd workers collected by (Dressel &
Farid, 2018), we compare various learning frameworks such as logistic regression, support vector machines
(SVM) and decision trees to estimate auditor’s intrinsic judgements and their feedback. We also measure
the reputation of the crowd workers to evaluate auditor’s preference towards various comparative fairness
notions.
2Under review as submission to TMLR
Figure 1: A Map of Technical Contributions
1.2 Related Work on Human Perception of Fairness and Worker Reputation
In the past, several researchers have attempted to model human perception of fairness, but have always tried
to ﬁt their revealed feedback to one of the existing fairness notions. For instance, task-based similarity metric
usedinindividualfairnessnotion(Dworketal.,2012)wereestimatedby(Jungetal.,2019)basedonfeedback
elicited from auditors regarding how a given pair of individuals have been treated. Similarly, the work of
(Gillen et al., 2018) assumes the existence of an auditor who is capable of identifying unfairness given pair of
individuals when the underlying similarity metric is Mahalanobis distance. Saxena investigated how people
perceive the ﬁtness of three diﬀerent individual fairness notions in the context of loan decisions (Saxena
et al., 2019). The three notions are: (1) treat similar individuals similarly (Dwork et al., 2012), (2) Never
favor a worse individual over a better one (Joseph et al., 2016), and (3) the probability of approval should
be equal to the reward being the greatest (Liu et al., 2017). They show that people exhibit a preference for
the last fairness deﬁnition.
From the perspective of group fairness notions, an experiment performed by (Srivastava et al., 2019) asks
participants to choose among two diﬀerent models to identify which notion of fairness (demographic parity
or equalized odds) best captures people’s perception in the context of both risk assessment and medical
applications. Likewise, another team surveyed 502 workers on Amazon’s Mturk platform and observed a
preference towards equal opportunity in (Harrison et al., 2020). Note that both papers asked participants
to reveal their analysis concerning a speciﬁc fairness notion in the context of given sensitive attributes (e.g.
race, gender) which was clearly pointed out as a limitation of their work. On the contrary, in this paper,
we impose no such restrictions on the auditor in constructing their feedback with respect to satiating any
speciﬁc fairness notion. Instead, we assume that the expert auditor employs an intrinsic fair decision rule
(which is unknown) to evaluate a given data tuple. Dressel and Farid in (Dressel & Farid, 2018) showed
that COMPAS is as accurate and fair as that of untrained human auditors regarding predicting recidivism
scores. On the other hand, (Yaghini et al., 2021) proposed a novel fairness notion, equality of opportunity
(EOP), which requires that the distribution of utility should be the same for individuals with similar desert.
Based on eliciting human judgments, they learned the proposed EOP notion in terms of criminal risk as-
sessment context. Results show that EOP performs better than existing notions of algorithmic fairness in
terms of equalizing utility distribution across groups. Another interesting work is by (Grgic-Hlaca et al.,
2018), who discovered that people’s fairness concerns are typically multi-dimensional (relevance, reliability,
and volitionality), especially when binary feedback was elicited. This means that modeling human feedback
should consider several factors beyond social discrimination. A major drawback of these approaches is that
3Under review as submission to TMLR
the demographics of the participants involved in the experiments (Yaghini et al., 2021; Grgic-Hlaca et al.,
2018; Harrison et al., 2020; Saxena et al., 2019) are not evenly distributed. For instance, the conducted ex-
periments ask how models treated Caucasians and African-Americans. However, non-Caucasian participants
were insuﬃcient to assess the relationship between the participant’s own demographics and the group that
was disadvantaged by the model. Moreover, the participants are presented with multiple questions in the
existing literature which cannot be scaled for larger decision-based models (Yaghini et al., 2021).
The problem of evaluating the biases of auditors/workers is also studied in crowdsourcing literature. Typ-
ically, auditors’ reliability is also measured by comparing their responses with majority vote (Jamaludeen
et al., 2019) or ground truth (Le et al., 2010). Since majority vote assumes that every auditor has same ex-
pertise, it cannot be applied to our framework as the biases of the auditors vary from one another. Moreover,
due to the absence of ground truth, it is not possible to compare auditor responses. Recently, the work of
Ghai et al. (2020) introduced a novel method of measuring worker’s biases based on counterfactual fairness -
a worker is considered to be fair if he/she provides the same label for an individual and its counterfactual. On
the other hand, this paper proposed a novel reputation mechanism accounting various comparative fairness
notions across diﬀerent sensitive attributes.
2 Preliminaries: Traditional Fairness Notions
In most practical systems, two types of discrimination exist: (i) disparate treatment , where an individual is
intentionally treated diﬀerently based on his/her membership in a protected class; and (ii) disparate impact ,
where members of a protected class are more negatively impacted than others. However, algorithmic fairness
literature has studied a diﬀerent set of fairness notions (ref. (Caton & Haas, 2020; Chouldechova & Roth,
2018; Mehrabi et al., 2021; Pessach & Shmueli, 2020)). Let g(·)be a ML system which predicts an outcome
y=g(x), wherexis the (multi-attribute) input variable and y∗be the true label.
2.1 Group Fairness Notions
The notion of group fairness seek for parity of some statistical measure across all the protected attributes
present in the data. Diﬀerent versions of group-conditional metrics led to diﬀerent group deﬁnitions of
fairness. Let Abe the set of protected attributes, where a∈Ais the privileged group and a/prime∈Ais the
underprivileged group.
Statistical Parity: This measures seeks to compute the probability diﬀerence of individuals who are
predicted to be positive across diﬀerent sensitive groups. Formally, it can be deﬁned as followed.
P[y= 1|A=a]−P[y= 1|A=a/prime]≤δ (1)
Sincestatisticalparityaimsforequalproportionsofpositiveoutcomes,theidealvalueofprobabilitydiﬀerence
should be 0. Speciﬁcally, if the diﬀerence is greater than 0, privileged group is beneﬁted. Whereas, if the
diﬀerence is less than 0, the underprivileged group is beneﬁted. A major disadvantage of statistical parity is
when the base rates (ratio of actual positive outcomes) are signiﬁcantly diﬀerent for various groups.
Equal Opportunity: To overcome the drawbacks in statistical parity, (Hardt et al., 2016a) introduced
the notion of equalized odds which computes the diﬀerence between the true positive rates (TPR) of two
protected groups.
P[y= 1|y∗= 1,A=a]−P[y= 1|y∗= 1,A=a/prime]≤δ. (2)
Smaller diﬀerences between groups indicate better fairness. Since this notion considers the true label y∗, it
assumes that the base rates of the two groups are representative and were not obtained in a biased manner.
Calibration: The measures computed the diﬀerence between positive predictive value of two groups. Pos-
itive predictive value represents the probability of an individual with a positive prediction actually experi-
encing a positive outcome. This notion is mathematically formulated as follows.
P[y∗= 1|y= 1,A=a]−P[y∗= 1|y= 1,A=a/prime]≤δ. (3)
4Under review as submission to TMLR
Figure 2: Latent Assessment Model of the Expert Auditor
Although in some cases equal calibration may be the desired measure, it has been shown that it is incompat-
ible with equalized odds (Pleiss et al., 2017) and is insuﬃcient to ensure accuracy (Corbett-Davies & Goel,
2018).
Equal Accuracy: This requires similar accuracy across groups (Berk et al., 2018).
P[y=y∗|A=a]−P[y=y∗|A=a/prime]≤δ. (4)
2.2 Individual Fairness
Individual fairness assessments, rather than measuring discrimination across diﬀerent sensitive groups, con-
sider fairness for each individual, with the assumption that similar individuals should be treated as similarly
as feasible (Dwork et al., 2012). Formally, given any two individuals xi,xj∈Xand for some κ,δ≥0, the
ML system gis(κ,δ)-individually fair if
d(yi,yj)≤δ,ifD(xi,xj)≤κ, (5)
whereyi=g(xi)andyj=g(xj)are the system’s outputs for the inputs xiandxjrespectively.
Note that the above deﬁnition contains a slack in the bound presented in Equation 5. Since the original
deﬁnition of individual fairness directly compares the two similarlity metrics d(yi,yj)andD(xi,xj), it be-
comes very challenging to ﬁnd appropriate distance metrics in practice. Furthermore, this relaxed deﬁnition
enables us to evaluate a relationship between our fairness model and individual fairness. Such relaxations
have also been considered in the past for rigorous analysis John et al. (2020).
Unfortunately, in most practical applications, the similarity metric D(xi,xj)is task-speciﬁc and is typically
unknown, which causes a severe restraint on our ability to ensure individual fairness. As a solution, metric
learning was proposed to discern a task-speciﬁc similarity metric by evaluating the relative distance between
human judgements (Ilvento, 2019) for any given pair of inputs. On the other hand, (Mukherjee et al., 2020)
utilizes Mahalanobis distance as a fair metric and proposed EXPLORE, an algorithm to learn similarity
between individuals from pairs of comparable and incomparable samples. It learns similarity such that the
logistic regression predicts “comparable" when the fair distance is short, and “incomparable" when the fair
distance is large. Interested readers can refer to (Fleisher, 2021) which discussed various ineﬃciencies of
individual fairness in detail.
3 Latent Assessment Model and Non-Comparative Evaluation of Human Auditors
Consider an expert auditor who is presented with a data tuple (x,y), wherex∈Xis the input given to ML
modelgandy=g(x)∈Yis the output label as shown in Figure 2. Let fdenote the expert auditor’s latent
decision rule, z=f(x)be the latent subjective label for the input x, ands(x,y,z )represent the auditor’s
binary feedback regarding the input-output pair (x,y). In this paper, we model auditor’s judgments from a
non-comparative perspective as follows:
Deﬁnition 1 (/epsilon1-Latent Assessment Model) .An auditor is said to satisfy /epsilon1-LAM if there exists a tuple
(X,Y,d,f,/epsilon1 )such that the auditor compares the system’s output y=g(x)with the latent subjective label
5Under review as submission to TMLR
Figure 3: Auditor Evaluation Framework based on /epsilon1-LAM
z=f(x)using a distance metric dand reveal his/her binary feedback as
s/parenleftbig
x,y,z/parenrightbig
=/braceleftBigg
1,ifd/parenleftbig
y,z/parenrightbig
≥/epsilon1,
0,otherwise.(6)
Furthermore, we say the auditor fis/epsilon1-LAM with respect to the system gifd/parenleftbig
y,z/parenrightbig
</epsilon1for allx∈X.
We also say that the auditor fis/epsilon1-LAM with respect to the system gwith high probability, if P/parenleftBig
d/parenleftbig
y,z/parenrightbig
<
/epsilon1/parenrightBig
≥1−δ, for some small /epsilon1>0andδ>0.
In other words, if the distance between the output label y=g(x)and the latent subjective label z=f(x)
is greater than /epsilon1, the auditor will deem the system label y=g(x)asunfair. For the sake of illustration,
consider an individual who committed felony and has multiple prior oﬀences, received low recidivism score
from a risk assessment tool. The expert auditor evaluates the individual intrinsically and may decide that
he/she should receive a higher recidivism score. Then, the auditor judges the tool’s output as unfair. In
this paper, we assume the fair relation femployed by the expert auditor is unknown. Therefore, we need to
learn the proposed /epsilon1-LAM using statistical learning techniques. This will be discussed in Section 4 in detail.
Although/epsilon1-LAM comprises of three unknowns in practice, namely d,fand/epsilon1, we assume that the distance
metricdused by the auditor is known.
Remark 1. In the case of binary classiﬁcation, the Hamming distance between gandf, denoted as dH/parenleftbig
y=
g(x),z=f(x)/parenrightbig
, takes a binary value of 0 or 1. Therefore, for any 0< /epsilon1 < 1,/epsilon1-LAM model in Deﬁnition
1 reduces to s=y⊕z, where⊕denotes the XOR operation. Since the XOR function is reversible, we can
infer the latent subjective label z=y⊕sexactly from elicited feedback.
Note that auditors exhibit diﬀerent types of biases in the real-world. Examples include conﬁrmation bias,
hindsight bias, anchoring bias, racial and gender bias. Every auditor is susceptible to biases which are
shaped by their prior experiences and/or knowledge about various social groups within the community.
Consequently, it is not reasonable to accept any auditor’s feedback blindly without evaluating their inherent
biases. Our proposed model, LAM, captures auditor’s biases through the latent subjective label z=f(x)in
Equation 6. However, in order to quantify auditor biases, we need to investigate the relationship between
LAM and comparative fairness notions. By doing so, we can estimate any given auditor’s performance in
terms of diverse fairness notions that do not necessarily align with each other.
In this paper, we propose a novel auditor evaluation framework for high-dimensional ML systems in Figure
3 to quantify his/her biases across diverse comparative fairness notions. Since it is impractical to elicit
feedback that spans entirely across the high-dimensional input space, there is a need to estimate LAM for
a given auditor to predict his/her fairness evaluation for any input-output pair (x,y)∈X×Y . In this
paper, we assume that the distance function d(·)and/epsilon1are known and restrict our attention to learning
6Under review as submission to TMLR
the auditor’s latent decision rule ffrom limited elicited feedback. In practice, human auditors are typically
incapable of expressing their intrinsic evaluations fformally in functional form. Therefore, we predict
auditor’s latent subjective labels ˆz=ˆf(x)on any input x∈Xusing a learning algorithm A(e.g. logistic
regression, random forests, and support vector machines), which is trained using elicited feedback samples/braceleftbig
(x1,y1,s1),···,(xn,yn,sn)/bracerightbig
. Furthermore, most ML-based systems are not revealed to fairness evaluators,
andneedtobeassessedasblack-boxmodels. Forexample, themodelusedtodevelopCOMPASisproprietary
and is not revealed to any criminal justice evaluator. In other words, it is also essential to estimate the system
model ˆgfrom available data/braceleftbig
(x1,y1),···,(xn,yn)/bracerightbig
, and predict its output label ˆy= ˆg(x).
Using the two predictions ˆyandˆz, we can now predict the binary feedback
ˆs/parenleftbig
x,ˆy,ˆz/parenrightbig
=/braceleftBigg
1,ifd/parenleftbig
ˆy,ˆz/parenrightbig
≥/epsilon1,
0,otherwise.(7)
Then, we measure the overall bias in the system from a noncomparative fairness perspective as deﬁned below.
Deﬁnition 2 (Fairness Measure) .The direct empirical fairness measure (DEFM) from a noncomparative
fairness perspective is deﬁned formally as the average feedback obtained directly from the original auditor,
i.e.
µ=E(s) =1
nn/summationdisplay
i=1si. (8)
On the other hand, the indirect empirical fairness measure (IEFM) from a noncomparative fairness perspec-
tive is given by the empirical estimate of the true bias, i.e.
ˆµ=E(ˆs) =1
nn/summationdisplay
i=1ˆsi, (9)
where ˆsis computed according to Equation 7.
The following section evaluates theoretical guarantees in terms of sample complexity and fundamental limits
of the proposed auditor evaluation framework due to the two predictors ˆfandˆg.
4 PAC Guarantees
Most ML systems map high-dimensional input spaces to low-dimensional outcomes. For example, in criminal
justice or banking applications, input space spans across the entire population of individuals serviced by the
ML system, while the outcome space is ﬁnite (e.g. binary decisions such as approval/disapproval of bails
or loans.). In such a scenario, feedback elicitation from human auditors is extremely challenging due to
the large number of feasible input-output pairs, which cannot be evaluated by any one human auditor.
Therefore, there is a need for human-system teaming for a practically scalable fairness evaluation, wherein
human auditors give feedback for a small subset of input-output pairs and the system learns their LAM
from elicited feedback and scales it across all other possibilities for a wholesome evaluation. This section
investigates thefundamentallimits ofsuch fairness evaluationsystems intermsof samplecomplexity (i.e. the
minimum number of feedback samples needed to eﬀectively learn auditor’s LAM with prescribed guarantees)
as well as the evaluation performance.
LetCbe a class of auditor’s decision rules f:X → Y whereXis the set of all possible individuals
serviced by the ML system and Yis the set of output labels of the ML system. Let Adenote a learning
algorithm that identiﬁes a hypothesis ˆfn∈Hthat minimizes the loss d(f,ˆfn), based on nobservations
{(xi,yi,si)}n
i=1∈Xn×Yn×{0,1}n, which follow some data distribution D. Note that the hypothesis class
His not necessarily equal to the original class of decision rules Cdue to our inability to mathematically
characterizeCexactly, especiallyduetothecomplexitiesinhumandecisionmaking. ThePACmodelprovides
guarantees for minimum number of such instances needed to obtain a hypothesis ˆfnwith error no more than
/epsilon1with probability 1−δ. Formally,
7Under review as submission to TMLR
Deﬁnition 3 (PAC Learnability of f).We say that the auditor’s latent decision rule fis PAC-learnable (in
both realizable and non-realizable settings) if there exists numbers /epsilon1f>0,0< δf<1,Nf(/epsilon1f,δf)>0, and
an algorithmAfwhich receives n≥Nf(/epsilon1f,δf)i.i.d. samples (drawn from some distribution D) as input,
and outputs an estimated classiﬁer ˆfnsuch that
P/parenleftBig
d(f,ˆfn)≤/epsilon1f/parenrightBig
≥1−δf.
Similarly, the ML-based system is unavailable to the evaluation platform i.e. gin unknown.
Deﬁnition4 (PACLearnabilityof g).We say that the ML-based system gis PAC-learnable (in both realizable
and non-realizable settings) if there exists numbers /epsilon1g>0,0< δg<1,Ng(/epsilon1g,δg)>0, and an algorithm
Agwhich receives n≥Ng(/epsilon1g,δg)i.i.d. samples (drawn from some distribution D) as input, and outputs an
estimated classiﬁer ˆgnsuch that
P/parenleftBig
d(g,ˆgn)≤/epsilon1g/parenrightBig
≥1−δg.
As discussed earlier, the auditor’s intrinsic rule fis not directly revealed, while eliciting his/her feedback
regarding biases in the system-of-interest. Therefore, we need to compute the intrinsic rule ˆfnin order to
reproduce auditor’s judgements for other input possibilities. At the same time, the classiﬁer is typically
unavailable to the bias-evaluation platform, i.e., gis also unknown to the bias-evaluation platform. In other
words, a practical bias-evaluation platform is expected to learn (f,g)to obtain a model (ˆfn,ˆgn), and identify
an appropriate fairness notion for the given context so that the bias evaluation platform can algorithmically
evaluate bias in a system with a large input space. In the following theorem, we provide guarantees for the
algorithmic /epsilon1−LAMevaluations, based on estimated rules ˆfnandˆgn.
Theorem 1. LetNdenote the minimum number of samples needed to guarantee /epsilon1−LAMempirically, i.e.
P/parenleftBig
d(ˆgn,ˆfn)</epsilon1/parenrightBig
>1−δfor some/epsilon1>0andδ >0. Then, for any auditor’s intrinsic rule fand classiﬁer
g, there exists some 0<Nf,Ng<N,/epsilon1g,/epsilon1f,/epsilon1,˜δ,δg,δf>0such that/epsilon1g+/epsilon1f+/epsilon1<˜/epsilon1andδg+δf+δ<2 +˜δ,
and an algorithm Athat receives i.i.d. samples {(xi,yi,zi)}n
i=1as input, and outputs rules ˆfnandˆgnwith a
probability of d(f(x),g(x))</epsilon1being at least 1−˜δ, only when
n≥N,min
/epsilon11,/epsilon12,δ1,δ2(max{Ng,Nf}), (10)
whereNfandNgsatisfy PAC learning bounds for fandg.
Proof.In this proof, we will show that fis/epsilon1-LAM with respect to gwith high probability, i.e.
P/parenleftBig
d(f(x),g(x))</epsilon1/parenrightBig
≥1−δ (11)
for some small /epsilon1 >0andδ >0. Assuming that both fandgare PAC-learnable functions (as stated in
Deﬁnitions 3 and 4 respectively), we evaluate the triangle inequality between the labels z=f(x),y=g(x),
ˆz=ˆfn(x)andˆy= ˆgn(x), as shown below:
d(g(x),f(x))≤d(g(x),ˆgn(x)) +d(ˆgn(x),f(x)) (due to ∆ineq. between y,ˆyandz)
≤d(g(x),ˆgn(x)) +d(ˆgn(x),ˆfn(x)) +d(ˆfn(x),f(x))(due to ∆ineq. between ˆy,ˆzandz).
(12)
Note thatfandgare PAC-learnable functions with parameters (/epsilon1f,δf,Nf)and(/epsilon1g,δg,Ng)respectively.
Letˆfbeˆ/epsilon1-LAM with respect to ˆgwith high probability, i.e.
P/parenleftBig
d(ˆf(x),ˆg(x))<ˆ/epsilon1/parenrightBig
≥1−ˆδ, (13)
8Under review as submission to TMLR
Figure 4: The interaction between the learning algorithms of fandg.
for some ˆ/epsilon1 >0andˆδ >0. Then, we can always ﬁnd some number /epsilon1 >0such that/epsilon1g+ ˆ/epsilon1+/epsilon1f≤/epsilon1. In
other words, if we deﬁne three events: Eg.={d(g(x),ˆgn(x))< /epsilon1g},ELAM.={d(ˆgn(x),ˆfn(x))<ˆ/epsilon1}and
Ef.={d(f(x),ˆfn(x))</epsilon1f}, the intersection of these events will guarantee
d(g(x),f(x))≤d(g(x),ˆgn(x)) +d(ˆgn(x),ˆfn(x)) +d(ˆfn(x),f(x))
≤/epsilon1g+ ˆ/epsilon1+/epsilon1f
< /epsilon1.(14)
Therefore, from Frechet’s lower bound on the probability of intersection of events, we obtain
P/parenleftBig
d(g(x),f(x))</epsilon1/parenrightBig
≥P/parenleftBig
Eg∧ELAM∧Ef/parenrightBig
≥P(Eg) +P(ELAM) +P(Ef)−2,(due to Frechet’s lower bound)
≥(1−δg) + (1−ˆδ) + (1−δf)−2
>1−δ,(15)
whereδis some number such that δg+ˆδ+δf<δ.
LetNg(/epsilon1g,δg)andNf(/epsilon1f,δf)denote the minimum samples needed to guarantee PAC learnability at gand
frespectively. In other words, the maximum of the two numbers will guarantee PAC learnability of both
gandf, i.e.N(/epsilon1g,/epsilon1f,δg,δf) = max{Ng(/epsilon1g,δg),Nf(/epsilon1f,δf)}. However, there are several possible ways to
split the prescribed tolerance /epsilon1andδacross the terms (/epsilon1f,/epsilon1g,ˆ/epsilon1)and(δf,δg,ˆδ)such that the two inequalities
/epsilon1g+ ˆ/epsilon1+/epsilon1f</epsilon1andδg+ˆδ+δf<δhold true. In other words, the minimum number of samples needed to
achieve prescribed guarantees (/epsilon1,δ)as stated in Equation 11 is given by Equation 10.
Note that IEFM ˆµcannot be better than DEFM µdue to the estimation errors in ˆfnandˆgn. In other
words, there is no free lunch in terms of fairness measurement when auditor and/or system are replaced by
digital twins. However, if the learning errors in both ˆfnandˆgnare small, we demonstrate that there is a
tolerable gap between the direct and indirect fairness measurement in Section 8 (ref. to Figure 6). Since the
problem of ﬁnding the gap between DEFM and IEFM remained intractable, we present an upper bound on
the distance d(ˆfn(x),ˆgn(x))in terms of d(f(x),g(x)), as a surrogate analysis.
Theorem 2. IfAis a learning algorithm which receives a training set of size n≥N,
min
/epsilon11,/epsilon12,δ1,δ2(max{Ng,Nf}), there always exists a distribution DoverXsuch that
9Under review as submission to TMLR
d(ˆfn(x),ˆgn(x))≤/epsilon1f+/epsilon1g+d(f(x),g(x)),
for anyx∈X.
Proof.We illustrate this result based on the following scenario. Consider that the true LAM and the
estimated LAM have opposing opinion regarding an input x, i.e.,s(x,y,ˆy) = 1andˆs(x,z,ˆz) = 0. In other
words, we assume that d(f(x),g(x))≥/epsilon1andd(ˆfn(x),ˆgn(x))</epsilon1. The objective is to measure the deviation
between the auditor’s true LAM and the estimated auditor twin’s LAM. Figure 4 depicts this scenario, where
FandGdenote the realizable function spaces from which we choose the estimates ˆfn∈Fandˆgn∈Gof
the auditor’s intrinsic rule fand the black-box classiﬁer grespectively. Since fandgare unknown, it is
reasonable to assume that the two functions do not belong to the function spaces FandGrespectively. Let
/epsilon1fand/epsilon1gdenote the errors in estimating the functions ˆfnandˆgnrespectively.
Based on triangle inequality, the following can be inferred.
d(ˆfn(x),ˆgn(x))≤d(f(x),ˆfn(x)) +d(f(x),ˆgn(x))≤/epsilon1f+d(f(x),ˆgn(x)), (16)
since the learning algorithm used to estimate fcomes with a bias of /epsilon1f, i.e. we have d(f,ˆfn)≤/epsilon1f. Similarly,
we have
d(ˆfn(x),ˆgn(x))≤d(g(x),ˆgn(x)) +d(ˆfn(x),g(x))≤/epsilon1g+d(ˆfn(x),g(x)). (17)
Adding above two equations we obtain
d(ˆfn(x),ˆgn(x))≤1
2/parenleftBig
/epsilon1f+/epsilon1g+d(ˆfn(x),g(x)) +d(f(x),ˆgn(x))/parenrightBig
. (18)
Furthermore, we have
d(f(x),ˆgn(x))≤/epsilon1g+d(f(x),g(x))
d(ˆfn(x),g(x))≤/epsilon1f+d(f(x),g(x)).(19)
Substituting Equation 19 in Equation 18, we obtain
d(ˆfn(x),ˆgn(x))≤1
2(/epsilon1f+/epsilon1g+/epsilon1g+d(f(x),g(x)) +/epsilon1f+d(f(x),g(x)))≤/epsilon1f+/epsilon1g+d(f(x),g(x)).(20)
5 Relationship Between Non-Comparative and Comparative Fairness Notions
Although comparative and non-comparative fairness notions are fundamentally diﬀerent from each other,
this section discusses their mutual relationship formally.
5.1 Relation with Individual Fairness
In the following proposition, we show how the system gcan be evaluated based on the notion of (κ,δ)-
individual fairness, when the auditor fis/epsilon1-LAM with respect to the system g.
Proposition 1. A systemgis(κ,2/epsilon1+δ)-individually fair, if its auditor fis(κ,δ)-individually fair and
satisﬁes/epsilon1-LAM with respect to the system g.
Proof.Given (xi,yi)and(xj,yj)∈X×Y such thatD(xi,xj)≤κ(the two individuals are κ-similar), then
fis(κ,δ)-individually fair if d/parenleftBig
f(xi),f(xj)/parenrightBig
<δ. However, note that if gis/epsilon1-LAM with respect to f, then
10Under review as submission to TMLR
d/parenleftBig
g(xi),f(xi)/parenrightBig
< /epsilon1andd/parenleftBig
g(xj),f(xj)/parenrightBig
< /epsilon1. Therefore, by applying a chain of triangle inequalities, we
obtain
d/parenleftBig
g(xi),g(xj)/parenrightBig
≤d/parenleftBig
g(xi),f(xi)/parenrightBig
+d/parenleftBig
f(xi),f(xj)/parenrightBig
+d/parenleftBig
f(xj),g(xj)/parenrightBig
<2/epsilon1+δ,(21)
for allxi,xjsuch thatD(xi,xj)≤κ.
Remark 2. In the case of binary classiﬁers, if the auditor fisκ-individually fair, we have f(xi) =f(xj)
for allxi,xjwheneverD(xi,xj)≤κ. Furthermore, if the auditor fis/epsilon1-LAM with respect to the system
gfor any/epsilon1∈(0,1), we haveg(x) =f(x)for allx∈X. Combining the above two properties, we get
g(xi) =f(xj) =f(xi) =g(xj)for allxi,xjsuch thatD(xi,xj)≤κ.
We illustrate this result using the following example from the banking domain. Consider two individuals
who are looking to apply for a loan. A banking system would evaluate both the applications via collecting
information such as gender, race, address, credit history, collateral, and his/her ability to pay back. At the
same time, consider an auditor who makes fairness judgements based on the rule: "If he/she has cleared
all the debts and possesses reasonably valued collateral, the loan must be granted". Given that the auditor
treats any two similar individuals similarly, the auditor satisﬁes individual fairness. Hence, if the evaluation
of the banking system is relatively similar to the auditor’s fair relation, from Proposition 1, the banking
system is also individually fair.
Proposition 2. If an auditor fis not (κ,δ)-individually fair, but satisﬁes /epsilon1-LAM with respect to the system
g, then the system is not (κ,δ−2/epsilon1)-individually fair as well.
Proof.Iffis not individually fair, then for some input pair (xi,xj)such thatD(xi,xj)< κ, we have
d(f(xi),f(xj))>δforallκ,δ∈R. However, notethatif gis/epsilon1-LAMwithrespectto f, thend(g(xi),f(xi))<
/epsilon1andd(g(xj),f(xj))</epsilon1. Therefore, by applying a chain of triangle inequalities, we have
d(f(xi),f(xj))≤d(g(xi),f(xi)) +d(g(xj),f(xj)) +d(g(xi),g(xj)) (22)
Substituting the bounds of d(g(xj),f(xj))andd(g(xi),g(xj))we get
2/epsilon1+d(g(xi),g(xj))> d (g(xi),f(xi)) +d(g(xj),f(xj)) +d(g(xi),g(xj))
≥d(f(xi),f(xj))
> δ(23)
for allδ∈R. Therefore, we also have d(g(xi),g(xj))>δ−2/epsilon1.
Remark 3. For binary classiﬁcation, Proposition 2 can be reduced as follows. Note that if fis notκ-
individually fair, we have f(xi)/negationslash=f(xj)even thoughD(xi,xj)≤κ. Furthermore, if gis/epsilon1−LAMwith
respect toffor any/epsilon1∈(0,1), we haveg(x) =f(x)for allx∈X. Combining the above two properties, we
getg(xi) =f(xj)/negationslash=f(xi) =g(xj)for allxi,xjsuch thatD(xi,xj)≤κ.
Consider the earlier example of banking where, there are two individuals, AandB, who possess the same
degree of merit. Imagine that the bank approves A’s loan application and denies B. This outcome remains
the same as per the auditor’s fair relation. Imagine further that neither AnorBmerits the outcome.
Though both banking’s evaluation and auditor’s judgements seem to be similar, they violate the precept,
"treat similar individuals similarly". Moreover, Ais treated in a way that Adoes not merit. Hence, we can
assert that banking evaluation does not satisfy individual fairness.
11Under review as submission to TMLR
5.2 Relation with Group Fairness Notions
As discussed earlier, group fairness notions compare certain probabilistic measure across two protected
groups. In the remaining section, we will focus on the relationship between group fairness notions and our
proposed LAM. For the sake of convenience, let us denote px,y(g,a) =P[g(x) =y|A=a].
Proposition 3. Given that the probability distributions are M-Lipschitz continuous over all possible fandg
functions,gsatisﬁes (2M/epsilon1+δ)-statistical parity, if gis/epsilon1-LAM with respect to f, andfsatisﬁesδ-statistical
parity.
Proof.Given the set of protected attributes A, sincefsatisﬁesδ-statistical parity, we have ||px,y(f,a)−
px,y(f,a/prime)||<δfor alla,a/prime∈A. Then, we have
px,y(g,a)−px,y(g,a/prime) = [px,y(g,a)−px,y(f,a)] + [px,y(g,a/prime)−px,y(f,a/prime)]
+ [px,y(f,a)−px,y(f,a/prime)](24)
AssumingM-Lipschitz continuity over all f(x),g(x), we have||px,y(g,a)−px,y(f,a)||< M·/epsilon1, since
d(g(x),f(x))</epsilon1. Combining all the inequalities, we have
||px,y(g,a)−px,y(g,a/prime)||<2M/epsilon1+δ. (25)
Again, consider the earlier example of loan approvals to illustrate the above proposition. Consider that there
exists two groups which are classiﬁed based income - low and high. The banking system builds a credit model
based purely. Moreover, the system may decide to use diﬀerent requirement levels - low interest or default
to low income group, so that the percentage of people getting a loan in low-income group is equal to the
percentage of people getting a loan in high-income group. Now, suppose an auditor presents fair judgements
based on the rule: “If Group A has a FICO credit score of 550 and cleared all the debts, the loan must be
granted. If Group B has a FICO score of 700 and has valuable collateral, grant the loan”. Note that, the
auditor’s fair relation is somewhat similar to that of the bank’s policy. Since the bank’s policy is known to
be statistically fair, the auditor is also unbiased from a group fairness perspective.
Similarly, the following three propositions identify the relationship between our proposed /epsilon1−LAMand three
other group fairness notions, namely equal opportunity, calibration, and equal accuracy.
Proposition 4. Given that the probability distributions are M-Lipschitz continuous over all possible fand
gfunctions,gsatisﬁes (2M/epsilon1+δ)-equal opportunity, if gis/epsilon1-LAM with respect to f, andfsatisﬁesδ-equal
opportunity.
Proof.The proof is similar to that of Proposition 3. Therefore, for the sake of brevity, the proof is not
included.
Proposition 5. Given that the probability distributions are M-Lipschitz continuous over all possible fand
gfunctions,gsatisﬁes (2M/epsilon1+δ)-calibration, if gis/epsilon1-LAM with respect to f, andfsatisﬁesδ-calibration.
Proof.The proof is similar to that of Proposition 3. Therefore, for the sake of brevity, the proof is not
included.
Proposition 6. Given that the probability distributions are M-Lipschitz continuous over all possible fand
gfunctions,gsatisﬁes (2M/epsilon1+δ)-equal accuracy, if gis/epsilon1-LAM with respect to f, andfsatisﬁesδ-equal
accuracy.
Proof.The proof is similar to that of Proposition 3. Therefore, for the sake of brevity, the proof is not
included.
12Under review as submission to TMLR
6 Comprehensive Reputation of Human Auditors
Given the auditor’s intrinsic labels {ˆz1,···,ˆzK}, we can compute the auditor’s performance for a given
sensitive/protected group in terms of various fairness notions such as statistical parity (ref. Equation equa-
tion 1), equal opportunity (ref. Equation equation 2), calibration (ref. Equation equation 3) and individual
fairness (ref. Section 2.2). Furthermore, note that this multi-dimensional fairness evaluation is diﬀerent for
diﬀerent sensitive groups. For example, in the United States, protected groups are typically deﬁned based
on race, gender, religion or any combination of these attributes. Such multi-attribute fairness evaluations
across diﬀerent sensitive/protected groups naturally steers us towards deﬁning a multi-attribute reputation
matrix
R(ˆz1,···,ˆzK) =
r1,1(ˆz1,···,ˆzK)···r1,L(ˆz1,···,ˆzK)
.........
rM,1(ˆz1,···,ˆzK)···rM,L(ˆz1,···,ˆzK)
, (26)
whereMis the total number of fairness notions and Lis the total number of sensitive groups. For example,
if we are evaluating the auditor based on statistical parity (SP) with respect to the sensitive attribute race,
thenrSP, race =P[f(x) = 1|race =a]−P[f(x) = 1|race =a/prime].
Althoughweproposeamulti-dimensionalfairnessevaluationinamatrixformat R, itisnecessarytorepresent
auditor’s biases as a one-dimensional score νsignifying his/her overall performance with respect to various
fairness notions. Therefore, we apply Frobenius norm of the reputation matrix Rto compute the auditor’s
scalar reputation score as follows.
ν(ˆz1,···,ˆzK) =||R(ˆz1,···,ˆzK)||F=/radicaltp/radicalvertex/radicalvertex/radicalbtM/summationdisplay
i=1L/summationdisplay
j=1/vextendsingle/vextendsingleri,j(ˆz1,···,ˆzK)/vextendsingle/vextendsingle2. (27)
We choose this reputation score due to the following axiomatic properties:
•Perfect Fairness: A utopian auditor satisﬁes all the fairness notions, i.e. every entry in Rbecomes
zero. Consequently, ||R||F→0.
•Lipschitz-Boundedness: Consider any deviation ∆fromR. Then, we have||R+ ∆||F≤||R||F+
||∆||F, due to triangle inequality. In other words, the Frobenius norm based score satisﬁes Lipschitz
property, since
||R+ ∆||F−||R||F
||∆||F≤1.
Lipschitz property is a particularly important since there is a bound to the change in score, even
though the auditor exhibits dynamic preferences regarding fairness notions.
•Equal Treatment of Fairness Notions: Frobenius norm of the matrix Rcan also be represented
as follows.
||R||F=/radicalBig
Tr(RTR) (28)
LetR,UR, andRVbe the reputation matrices of three diﬀerent auditors. Then, we have
||UR||F=||RV||F=/radicalBig
Tr(RTR) =||R||F. (29)
In other words, the three auditors with reputation matrices which diﬀer by orthogonal transforma-
tions have the reputation score. For the sake of illustration, consider two auditors: (i) one who
complies with statistical parity, but not with calibration, and (ii) another who satisﬁes calibration
but not statistical parity. Assuming that both (i) and (ii) treats all the remaining fairness notions
identically, the Frobenius norm of their reputation matrices would be same. In other words, our
reputation score treats all fairness notions equally.
13Under review as submission to TMLR
7 Evaluation Methodology
In this section, we discuss diﬀerent methodologies used to evaluated our proposed /epsilon1-LAM based on simulation
as well as real human audit data.
7.1 Datasets
We validate our theoretical ﬁndings using the following datasets, each of which are pre-processed as follows:
1.ProPublica’s COMPAS dataset (Larson et al., 2016): In this paper, we perform same preprocess-
ing procedure as done by ProPublica in their analysis in (Larson et al., 2016). The races in the dataset
are only restricted to African-American, Caucasian, and other. We consider femalesandCaucasians as
privileged groups (Bellamy et al., 2019). We consider the binary attribute called two-year recidivism
(with labels being most likely orleast likely ) within the dataset as the output label in our analysis. If
thetwo-year recidivism attribute is least likely , we deem the outcome as a favorable one. Furthermore,
we discretize ageinto three age-groups (i.e. <25, 25-45,>45) and relabel this feature as age category .
Similarly, prior count feature is also discretized into three discrete bins (i.e. 0, 1-3, >3) as well.
2.German credit data (Merz & Murphy, 1996): In this dataset, we consider credit history, savings,
employment, sex, and age as input features. Moreover, we categorize ageinto two groups: young ( <26)
and old (>=26). We assume that males and older individuals as privileged groups and credit risk value
of 1 (i.e. good credit risk) as the favourable outcome.
3.Adult income dataset (Kohavi & Becker, 1994): The objective is to predict whether the income
of an individual is >$50Kor<$50K. The input features include age, sex, race, and education. In the
pre-processing phase, the continuous feature ageis transformed into diﬀerent groups of ages (0-10, 11-20,
and so on). For the feature race, we limited the labels to binary by mapping ‘White’ to 1 and all other
races to 0. We have 32561 data tuples in total.
4.Real human feedback data (Dressel & Farid, 2018): This data acquisition experiment consists of
a short description of the defendant (gender, age, race, and previous criminal history) is provided to the
human auditor. A total of 1000 defendant descriptions are used that are drawn randomly from the original
ProPublica’s COMPAS dataset. Furthermore, these descriptions were divided into 20 subsets of 50 each.
The experiment consisted of 400 diﬀerent crowd workers and each one of them was randomly assigned to
see one of these 20 subsets. The participants predicted whether a particular individual would recidivate
within 2 years of their most recent crime. The original data consists whether a crowd worker predicted
correctly or not compared to the original classiﬁcation in the COMPAS dataset. We preprocessed this
dataset and obtained the true prediction given by the crowd workers.
7.2 Evaluation of Comparative and Noncomparative Fairness Notions from Data
Comparative fairness notions provide a benchmark for fairness evaluation in today’s ML-based systems. The
evaluation of these comparative fairness notions, be it group fairness or individual fairness, have been studied
extensively in the literature. This paper uses state-of-the-art approaches to evaluate comparative fairness of
both ML-based system as well as the auditor in our simulation experiments and our analysis on real datasets.
Furthermore, non-comparative fairness is also evaluated using the proposed LAM on the same datasets, and
is compared with the empirical comparative fairness evaluations as demonstrated in Section 5.
The evaluation of individual fairness notion relies on a similarity metric Dthat quantiﬁes the (dis)similarity
between input proﬁles representing diﬀerent individuals. The exact choice of similarity metric Dis highly
contextual to the ML-based system and the environment in which it is deployed. However, in a complex
application setting, it is impractical to characterize Dformally due to limited knowledge about the system
and its environment. Nonetheless, one can learn the similarity metric Dfrom observed participants in this
system. Inspired from prior work (Zemel et al., 2013; Lahoti et al., 2019; John et al., 2020), we adopt
an unsupervised learning algorithm based on clustering, where we construct context-dependent clusters of
similar individuals using non-sensitive attribute features from respective datasets. For illustrative purposes,
14Under review as submission to TMLR
Sex Age Race Prior Oﬀenses Charge Degree
Female 25-45 Caucasian 1 to 3 Misdemeanor
Male 25-45 Other 0 Felony
Female Greater than 45 African-American 0 Felony
Male 25 - 45 Other More than 3 Misdemeanor
Male Greater than 45 Other 1 to 3 Misdemeanor
Table 1: Example of 5 diﬀerent clusters present in COMPAS dataset
Table 1 presents 5 diﬀerent clusters based on various attributes available within the COMPAS dataset. A
family ofsimilarity metrics basedon Mahalanobisdistance D(C) =/radicalbig
(xi−xj)TC−1(xi−xj)is used, where
xi,xjare input proﬁles in a dataset. The parameter Cis a positive semi-deﬁnite covariance matrix that is
learned from data. For COMPAS, the features juvenile counts (both misdemeanor and felony counts), prior
oﬀencesandcharge degree are used to construct clusters based on estimated Mahalanobis distance. Similarly,
for German credit dataset, we consider the following features credit history ,employment andsavings. Lastly,
for the Adult income dataset, we use education years andageto measure similarity. A cluster is labeled
as being violated by a given entity (be it the ML-based system, or the auditor) if all output labels are not
identical. Therefore, we say an entity is individually fair if it has no cluster violations. Furthermore, in
binary classiﬁcation settings, the number of mismatches between the output labels at the ML-based system
and the auditor can be evaluated within each cluster. We say that the auditor is /epsilon1-LAM with respect to the
ML-based system (i.e., a measure for non-comparative fairness evaluation) if there exists at most /epsilon1input
proﬁles across all clusters, whose outcome labels at both the ML-based system and the auditor mismatches.
Groupfairnessnotionsrelyonthediﬀerencebetweentheprobabilityoffavorableoutcomesattheunprivileged
group to that of the privileged group. This paper evaluates the group fairness notions for the digital twins of
both ML-based system ( ˆgn) and the auditor ( ˆfn) which are designed using standard ML algorithms - logistic
regression, random forest, and support vector machine (SVM). However, in the context of equalized odds
and calibration, the probabilities are evaluated conditioned on the true outcome labels. This paper assumes
that these true labels are the original outcomes from the ML-based system ( g) and the human auditor ( f)
respectively that are available from the simulated entities or real datasets.
7.3 Simulated Auditor Models
Realdatasetsaretypicallysmallinsizerangingaround50feedbacksamplesperauditor, collectedfromabout
100-500 auditors in total. Such data collection exercises are extremely expensive and are not always available
in a timely manner. As a result, validation on real data does not provide a comprehensive evaluation of
the proposed methodology. In order to address this issue, this paper also considers three simulated auditors
based on decision trees to mimic human auditor’s behavior on COMPAS, German credit, and Adult income
datasets respectively. Although these decision trees are manually chosen by the authors and cannot be
guaranteed to exist in real-world, their designs are inspired from relevant and contextual features that are
not sensitive in terms of any protected attributes.
For COMPAS, the decision rule s1is deﬁned below, using three input features, namely number of prior
oﬀences anddegree of the oﬀence (felony or misdemeanor), and a binary output label based on the attribute
two year recidivism (most likely or least likely):
s1/parenleftbig
x,y,z/parenrightbig
=

0,ifx.priors-count∈[1,3]andx.charge-degree = Felony
OR
ifx.priors-count >3andx.charge-degree = Misdemeanor
1,otherwise.(30)
15Under review as submission to TMLR
Figure 5: Accuracy of respective learning models while predicting the simulated auditor responses across
diﬀerent training set sizes.
Since the task of the Adult income dataset is to predict whether yearly income of an individual is >50K or
<=50K, the decision rule s2is constructed based on the feature education as shown below:
s2/parenleftbig
x,y,z/parenrightbig
=/braceleftBigg
0,ifx.education∈[Bachelors, Masters, School Professor, Doctorate]
1,otherwise.(31)
Similarly, for German credit dataset, the decision rule s3is deﬁned based on input features savings, credit
historyandemployment are considered while designing the auditor’s relation.
s3/parenleftbig
x,y,z/parenrightbig
=/braceleftBigg
0,ifx.savings>500,x.credit-history = Paid, and x.employment >2years
1,otherwise.(32)
8 Experimental Results and Discussion
This section evaluates the proposed fairness evaluation methodology using three diﬀerent experiments.
Firstly, standard learning algorithms (e.g. logistic regression, support vector machine, and random for-
est) trained to mimic the auditor responses are evaluated in terms of learning accuracy and absolute error
|µ−ˆµ|inSection8.1. Secondly, thedigitaltwinsofboththeML-basedsystemandtheauditordesignedbased
on standard learning algorithms are evaluated in terms of both comparative and non-comparative fairness
notions in Section 8.2. Lastly, the empirical distribution of the Frobenius norm based auditor reputation
score is depicted and compared with other scoring functions on real human feedback data in Section 8.3.
8.1 Auditor Twin Evaluation
Auditor twins are evaluated in two ways, namely learning performance in terms of accuracy and fairness
evaluation error in terms of absolute error |µ−ˆµ|, on the four datasets listed in Section 7.1 in the context
of binary classiﬁcation. While the labels y=g(x),z=f(x)ands(x,y,z )are available in simulation
experiments, the real human feedback data (Dressel & Farid, 2018) does not contain the latent subjective
labelz=f(x)explicitly. However, as stated in Remark 1, the auditor’s latent subjective label zcan be
inferred from the system’s true label yand elicited feedback susing the XOR relation z=y⊕sin binary
classiﬁcation settings. In each experiment, 25 random train-test splits are performed on all four datasets
and average performance metrics are used to gain insights into the performance evaluation of the proposed
auditor twin algorithms.
Simulated Auditor on COMPAS, Adult and German Credit Datasets: The training set size is varied from
10 to 100 data samples and the accuracy in predicting the simulated auditor’s responses is recorded for
16Under review as submission to TMLR
Figure 6: Absolute error between DEFM and IEFM across varied training set sizes.
Figure 7: Number of auditor twins achieving a prescribed accuracy as a function of training set size, for
diﬀerent learning models.
each training size. Figure 5 shows that the accuracy in predicting ˆz=ˆf(x)increases with training set
size. More speciﬁcally, the random forest algorithm outperforms both logistic regression and SVM in terms
of both accuracy and absolute error |µ−ˆµ|. This is because the three simulated auditors s1,s2ands3
resemble that of a decision tree which makes random forest algorithm into a realizable learning framework.
In addition, the absolute error between DEFM and IEFM (ref. to Figure 6) is close to zero across all three
learning algorithms. This signiﬁes that auditor’s responses can be eﬃciently learned even with small number
of samples. Note that evaluators typically incur a very high price for eliciting a large number of feedback
samples. For all practical purposes, since eliciting more than 100 feedback samples per auditor is infeasible,
the x-axis in Figures 5 and 6 is limited to at most 100 training samples.
Real Human Feedback Data: Similar to the earlier experiment, the accuracies in predicting real human
responses is recorded for diﬀerent training set sizes varying from 10 to 40 samples. Figure 7 shows that the
number of crowd auditors whose twins’ responses have an accuracy of at least 90%(blue bars) increases
with the training set size across all three learning algorithms. Although random forest outperformed other
learning algorithms in reliably mimicking the simulated auditors, logistic regression outperforms random
forest based twin in real human feedback data. However, SVM recorded low accuracy performance in a
majority of real human auditors. Figure 8 depicts the histogram of the absolute error |µ−ˆµ|observed on
real human feedback data, as well as the best ﬁtting gamma distribution that minimizes the negative log-
likelihood function (for more details, refer to Virtanen et al. (2020)). Regardless of the learning algorithm,
Figure 8 illustrates the absolute error between DEFM and IEFM to be smaller than 0.2 with high probability
amongst real crowd auditors.
17Under review as submission to TMLR
Figure 8: Distribution of crowd auditors in terms of error between DEFM and IEFM.
Dataset# Test
SamplesTotal
# Clusters# Clusters
Violated
by System# Clusters
Violated
by Auditor# LAM-based
Mismatches
COMPAS 1543 70 30 0 57
German credit 250 25 21 0 25
Adult income 8141 62 55 0 59
Table 2: Comparison between individual fairness evaluations at system and auditor, and their LAM mis-
matches for three simulated auditor twins across various datasets.
8.2 Demonstrating the Relationship between Non-Comparative and Comparative Notions
In this section, the digital twins of both ML-based system and the auditor are evaluated in terms of com-
parative and non-comparative fairness notions. In terms of comparative fairness, both individual fairness
and group fairness are evaluated as per the methodology discussed in Section 7.2. The labels of both the
ML-based system and the auditor are learned using standard learning algorithms (e.g. logistic regression,
SVM, and random forest). The results are averaged across 25 random train-test splits.
Simulated Auditor on COMPAS, Adult and German Credit Datasets: The training set size is considered as
50 samples, since it incurs a very high price for eliciting a large number of samples. The respective entity’s
(the ML-based system or the auditor) biases are evaluated on remaining test set predictions. In terms
ofindividual fairness , Table 2 shows that the simulated auditor is individually fair (0 violated clusters).
However, the auditor does not satisfy /epsilon1-LAM with respect to all three ML-based systems due to a large
number of LAM-based output label mismatches between the twins of the simulated auditor ˆfn(x)and that
of the system ˆg(x)shown in Table 2. In other words, even though the simulated auditor is individually
fair, the system is not individually fair in practice, if the auditor is not /epsilon1-LAM with respect to the system.
However, the fairness of the simulated auditor is heteroskedastic in terms of group fairness notions. We
measure the simulated auditor’s biases with respect to three group fairness notions (statistical parity, equal
opportunity and calibration) based on diﬀerent sensitive attributes (gender and race/age). Based on our
insights from Section 8.1, the fairness evaluations due to random forest classifer are treated as a benchmark
for auditor performance. Note that both COMPAS and German credit datasets exhibit biases in fairness
evaluation in both logistic regression as well as SVM models. However, as observed in Figure 9b, the Adult
income dataset seems to be robust to ML models in terms of any group fairness notion since all ML models
have similar learning performance, as shown in Figure 5. Such behavior is consistent with Theorem 2.
Furthermore, note that if the auditor is fair with respect to any group fairness notion (e.g. refer to equalized
opportunity value for the auditor’s twin in COMPAS dataset in Figure 9a), the system is not fair if the
auditor’s twin is not /epsilon1-LAM with respect to the system’s twin.
18Under review as submission to TMLR
(a) COMPAS
(b) Adult income
(c) German credit
Figure 9: Evaluation of real-world classiﬁers and the simulated auditor twin for statistical parity (SP), equal
opportunity (EO) and calibration (C), with respect to diﬀerent protected attributes.
Real Human Feedback Data: Based on earlier results (Figure 7), the choice of ML algorithm varies from one
auditor to another, as well as the application context. Therefore, we train each crowd auditor’s responses
using all three ML algorithms (logistic regression, SVM, and random forest) and select the one which yields
in highest accuracy in predicting their responses. Speciﬁcally, logistic regression predicted 57.5% of crowd
auditors’ responses with high accuracy. Whereas, SVM and random forest classiﬁers predicted 25% and
17.5% of crowd auditor’s responses with high accuracy respectively. In terms of individual fairness , Figure
10 shows that only 20 auditors (in a total of 400 auditors) are individually fair (0 violated clusters). The
majority of the crowd auditors violate 5% to 20% of the clusters present in the subset given to them. Figure
10 presents the best-ﬁt gamma distribution of individually fair auditors whose parameters are estimated as
α= 12.75andβ= 0.0187. Furthermore, we evaluate the crowd auditors by varying the threshold δin the
deﬁnition of group fairness notions (statistical parity, equalized odds, calibration, and equal accuracy), as
stated in Section 2.1. Each crowd auditor’s performance is assessed based on the four group fairness notions
19Under review as submission to TMLR
Figure10: Thedistributionofcrowdauditorsasafunctionoffractionofclustersviolatingindividualfairness.
Figure 11: Number of crowd auditors satisfying diﬀerent group fairness notions across varied probabilistic
bias with respect to race and gender.
20Under review as submission to TMLR
Figure 12: Reputation Matrices for the Three Simulated Auditors s1,s2ands3on COMPAS, Adult Income
and German Credit Datasets respectively
by varying the threshold δfrom 0 to 0.5. Figure 11 shows that crowd auditors are typically more fair with
respect to race than gender based on the shift in distribution.
8.3 Reputation Scores Interpretation and its Relation with Learning Performance
In this section, auditor reputation scores are interpreted using simulation experiments and then evaluated
on real experiments. The results are discussed below in the following two subsections respectively. Note that
lower reputation scores correspond to lower biases, which can be attributed to higher audit reliability.
Simulated Auditor on COMPAS, Adult and German Credit Datasets: As discussed in simulation results in
Section 8.2, random forest classiﬁer is considered the benchmark in this analysis. Figure 12 shows reputation
matrices of the simulated auditors on COMPAS, Adult income and German credit across three learning
algorithms. The rows in the reputation matrix Rrepresent diﬀerent comparative fairness notions in the
following top-down order: statistical parity, equal opportunity, calibration, and individual fairness. Whereas,
the columns represent diﬀerent sensitive attributes as shown in Figure 12. Note that the null attribute φ
is included in the attribute-set since individual fairness is agnostic to sensitive attributes. The reputation
scores of all twins for the Adult dataset is similar due to their identical learning performance. However, this
is not the same for other datasets. For instance, random forest classiﬁer outperforms logistic regression and
SVM in COMPAS and German credit datasets, even in terms of their reputation scores as well. Another
interesting observation to note is that learning errors do not increase biases identically across all fairness
notions. For example, in the case of R3, statistical parity and equal opportunity notions are similar, but the
logistic regression model registered a signiﬁcant chance in calibration in logistic regression and SVM models.
This is automatically reﬂected in their reputation scores as well.
21Under review as submission to TMLR
Figure 13: The Frobenius reputation score distribution of 400 crowd workers.
(a) Distribution of crowd auditors’ Frobenius
reputation vs. Eigenvalues reputation
(b) Distribution of crowd auditors’ Frobenius norm
reputation vs. spectral norm reputation
Figure 14: A comparison of diﬀerent reputation scoring functions
Real Human Feedback Data: The histogram of reputation scores based on real data elicited from 400 crowd
workers is plotted in Figure 13. A best-ﬁt gamma distribution is also evaluated via minimizing the negative
log-likelihood function, whose parameters were found to be α= 31.25andβ= 0.04. Note that the repu-
tation scores range between 0.16 and 1.67, with a signiﬁcant majority of them lying between 0.5 and 1.0.
Furthermore, we compare the proposed Frobenius norm reputation of the auditor with two diﬀerent scoring
functions i.e., eigenvalues and spectral norm of the reputation matrix R. On the ﬁnal note, Figure 14 de-
picts a monotonic relationship between Frobenius norm based reputation score and eigenvalue/spectral-norm
based scoring mechanisms. However, the other scoring mechanisms do not necessarily follow the axiomatic
properties of fairness as stated in Section 6.
22Under review as submission to TMLR
9 Conclusion and Future Work
We developed a novel latent assessment model to characterize human auditor feedback and demonstrated
its relationship with traditional fairness notions both theoretically and on real datasets. We obtained PAC
learning guarantees on learning auditor’s intrinsic fairness assessments, and demonstrated the learning per-
formance of three learning algorithms on a real human feedback dataset. Consequently, this paper enabled us
to accomplish two important challenges in the design of a crowd-auditing platform: (i) we can learn/mimic
auditor’s intrinsic evaluations using little elicited feedback and automate the evaluation on the remaining
possibilities especially in high-dimensional learning algorithms, and (ii) we can also evaluate auditor biases
with respect to diverse traditional fairness notions. In addition, we use the relationship between LAM and
traditional fairness notions to identify reliable auditors for feedback elicitation based on their reputation
scores.
In future, we will address all the other challenges in the design of crowd-auditing platforms. Since feedback
elicitation is an expensive process, we will improve our LAM model to account for feedback for data bundles,
as opposed to our current feedback model for singleton data tuples. Furthermore, we will also investigate
appropriate fusion rules to aggregate feedback collected from multiple auditor with heterogeneous opinions
based on their reputation.
References
J. Angwin, J. Larson, S. Mattu, and L. Kirchner. Machine Bias. ProPublica , May 23 2016.
Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoﬀman, Stephanie Houde, Kalapriya Kannan,
Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilović, et al. Ai fairness 360: An
extensibletoolkitfordetectingandmitigatingalgorithmicbias. IBM Journal of Research and Development ,
63(4/5):4–1, 2019.
Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice
risk assessments: The state of the art. Sociological Methods & Research , 50(1):3–44, 2018.
Reuben Binns. Fairness in machine learning: Lessons from political philosophy. In Conference on Fairness,
Accountability and Transparency , pp. 149–159. PMLR, 2018.
Simon Caton and Christian Haas. Fairness in machine learning: A survey. arXiv preprint arXiv:2010.04053 ,
2020.
Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction
instruments. Big data, 5(2):153–163, 2017.
Alexandra Chouldechova and Aaron Roth. The frontiers of fairness in machine learning. arXiv preprint
arXiv:1810.08810 , 2018.
A Feder Cooper, Ellen Abrams, and Na Na. Emergent unfairness in algorithmic fairness-accuracy trade-oﬀ
research. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society , pp. 46–54, 2021.
Sam Corbett-Davies and Sharad Goel. The measure and mismeasure of fairness: A critical review of fair
machine learning. arXiv preprint arXiv:1808.00023 , 2018.
Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making
and the cost of fairness. In Proceedings of the 23rd acm sigkdd international conference on knowledge
discovery and data mining , pp. 797–806, 2017.
Julia Dressel and Hany Farid. The accuracy, fairness, and limits of predicting recidivism. Science advances ,
4(1):eaao5580, 2018.
C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness Through Awareness. In Proceedings
of the 3rd innovations in theoretical computer science conference , pp. 214–226. ACM, 2012.
23Under review as submission to TMLR
Joel Feinberg. Noncomparative justice. The philosophical review , 83(3):297–338, 1974.
Ray Fisman and Michael Luca. Fixing Discrimination in Online Marketplaces. In Harvard Business Review ,
December 2016.
Will Fleisher. What’s fair about individual fairness? In Proceedings of the 2021 AAAI/ACM Conference on
AI, Ethics, and Society , pp. 480–490, 2021.
Bhavya Ghai, Q Vera Liao, Yunfeng Zhang, and Klaus Mueller. Measuring social biases of crowd workers
using counterfactual queries. arXiv preprint arXiv:2004.02028 , 2020.
Stephen Gillen, Christopher Jung, Michael Kearns, and Aaron Roth. Online learning with an unknown
fairness metric. In Advances in Neural Information Processing Systems , pp. 2600–2609, 2018.
Naman Goel and Boi Faltings. Crowdsourcing with fairness, diversity and budget constraints. In Proceedings
of the 2019 AAAI/ACM Conference on AI, Ethics, and Society , pp. 297–304, 2019.
Nina Grgic-Hlaca, Elissa M Redmiles, Krishna P Gummadi, and Adrian Weller. Human perceptions of
fairness in algorithmic decision making: A case study of criminal risk prediction. In Proceedings of the
2018 World Wide Web Conference , pp. 903–912, 2018.
M. Hardt, E. Price, and N. Srebro. Equality of Opportunity in Supervised Learning. In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Pro-
cessing Systems 29 , pp. 3315–3323. Curran Associates, Inc., 2016a.
Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of Opportunity in Supervised Learning. In Advances
in Neural Information Processing Systems , pp. 3315–3323, 2016b.
Galen Harrison, Julia Hanson, Christine Jacinto, Julio Ramirez, and Blase Ur. An empirical study on the
perceived fairness of realistic, imperfect machine learning models. In Proceedings of the 2020 Conference
on Fairness, Accountability, and Transparency , pp. 392–402, 2020.
Christina Ilvento. Metric learning for individual fairness. arXiv preprint arXiv:1906.00250 , 2019.
Noor Jamaludeen, Vishnu Unnikrishnan, Maya S Sekeran, Majed Ali, Le Anh Trang, and Myra Spiliopoulou.
Assessing the reliability of crowdsourced labels via twitter. In LWDA, pp. 115–126, 2019.
Philips George John, Deepak Vijaykeerthy, and Diptikalyan Saha. Verifying individual fairness in machine
learning models. In Conference on Uncertainty in Artiﬁcial Intelligence , pp. 749–758. PMLR, 2020.
Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and
contextual bandits. Advances in neural information processing systems , 29, 2016.
ChristopherJung, MichaelKearns, SethNeel, AaronRoth, LoganStapleton, andZhiweiStevenWu. Eliciting
and enforcing subjective individual fairness. arXiv preprint arXiv:1905.10660 , 2019.
Faisal Kamiran and Toon Calders. Data preprocessing techniques for classiﬁcation without discrimination.
Knowledge and Information Systems , 33(1):1–33, 2012.
ToshihiroKamishima, ShotaroAkaho, HidekiAsoh, andJunSakuma. Fairness-awareclassiﬁerwithprejudice
remover regularizer. In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases , pp. 35–50. Springer, 2012.
Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-oﬀs in the fair determination
of risk scores. arXiv preprint arXiv:1609.05807 , 2016.
Ronny Kohavi and Barry Becker. UCI machine learning repository, 1994. URL http://archive.ics.uci.
edu/ml.
24Under review as submission to TMLR
Preethi Lahoti, Krishna P Gummadi, and Gerhard Weikum. ifair: Learning individually fair data represen-
tations for algorithmic decision making. In 2019 ieee 35th international conference on data engineering
(icde), pp. 1334–1345. IEEE, 2019.
Jeﬀ Larson, Julia Angwin, Lauren Kirchner, and Surya Mattu. How we analyzed the compas recidivism
algorithm. ProPublica , May 23 2016.
John Le, Andy Edmonds, Vaughn Hester, and Lukas Biewald. Ensuring quality in crowdsourced search rele-
vance evaluation: The eﬀects of training question distribution. In SIGIR 2010 workshop on crowdsourcing
for search evaluation , volume 2126, pp. 22–32, 2010.
Yang Liu, Goran Radanovic, Christos Dimitrakakis, Debmalya Mandal, and David C Parkes. Calibrated
fairness in bandits. arXiv preprint arXiv:1707.01875 , 2017.
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on
bias and fairness in machine learning. ACM Computing Surveys (CSUR) , 54(6):1–35, 2021.
C. J. Merz and P. Murphy. UCI machine learning repository, 1996. URL http://archive.ics.uci.edu/ml .
Debarghya Mukherjee, Mikhail Yurochkin, Moulinath Banerjee, and Yuekai Sun. Two simple ways to learn
individual fairness metrics from data. In International Conference on Machine Learning , pp. 7097–7107.
PMLR, 2020.
Dana Pessach and Erez Shmueli. Algorithmic fairness. arXiv preprint arXiv:2001.09784 , 2020.
Geoﬀ Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness and cali-
bration. In Advances in Neural Information Processing Systems , pp. 5680–5689, 2017.
Nripsuta Ani Saxena, Karen Huang, Evan DeFilippis, Goran Radanovic, David C Parkes, and Yang Liu.
How do fairness deﬁnitions fare? examining public attitudes towards algorithmic deﬁnitions of fairness.
InProceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society , pp. 99–106, 2019.
Megha Srivastava, Hoda Heidari, and Andreas Krause. Mathematical notions vs. human perception of
fairness: A descriptive approach to fairness for machine learning. In Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining , pp. 2459–2468, 2019.
Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Ev-
geni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew
Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert
Kern, Eric Larson, C J Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde,
Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald,
Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0:
Fundamental Algorithms for Scientiﬁc Computing in Python. Nature Methods , 17:261–272, 2020. doi:
10.1038/s41592-019-0686-2.
Andrew Waxman. BankThink AI Can Help Banks Make Better Decisions, But it Doesn’t Remove Bias.
American Banker , June 05 2018.
Mohammad Yaghini, Andreas Krause, and Hoda Heidari. A human-in-the-loop framework to construct
context-aware mathematical notions of outcome fairness. In Proceedings of the 2021 AAAI/ACM Confer-
ence on AI, Ethics, and Society , pp. 1023–1033, 2021.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness be-
yond disparate treatment & disparate impact: Learning classiﬁcation without disparate mistreatment. In
Proceedings of the 26th International Conference on World Wide Web , pp. 1171–1180. International World
Wide Web Conferences Steering Committee, 2017.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In Pro-
ceedings of the 30th International Conference on Machine Learning , volume 28 of Proceedings of Machine
Learning Research , pp. 325–333, 17–19 Jun 2013.
25