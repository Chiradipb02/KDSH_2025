Published in Transactions on Machine Learning Research (01/2023)
Target Propagation via Regularized Inversion
for Recurrent Neural Networks
Vincent Roulet∗vroulet@uw.edu
Department of Statistics
University of Washington
Zaid Harchaoui zaid@uw.edu
Department of Statistics
University of Washington
Reviewed on OpenReview: https: // openreview. net/ forum? id= Q5vdEJyhA8
Abstract
Target Propagation (TP) algorithms compute targets instead of gradients along neural net-
works and propagate them backward in a way that is similar to yet diﬀerent than gradient
back-propagation (BP). The idea initially appeared as a perturbative alternative to BP that
may improve gradient evaluation accuracy when training multi-layer neural networks (Le-
Cun, 1985) and has gained popularity as a biologically plausible counterpart of BP. However,
there have been many variations of TP, and a simple version of TP still remains worthwhile.
Revisiting the insights of LeCun (1985) and Lee et al. (2015), we present a simple version
of TP based on regularized inversions of layers of recurrent neural networks. The proposed
TP algorithm is easily implementable in a diﬀerentiable programming framework. We illus-
trate the algorithm with recurrent neural networks on long sequences in various sequence
modeling problems and delineate the regimes in which the computational complexity of TP
can be attractive compared to BP.
1 Introduction
Target Propagation (TP) algorithms can be seen as perturbative learning alternatives to the gradient back-
propagation algorithm, where virtual targets are propagated backward instead of gradients (LeCun, 1985;
Rohwer, 1989; Mirowski & LeCun, 2009; Bengio, 2014; Lee et al., 2015; Meulemans et al., 2020; Manchev
& Spratling, 2020). A high-level summary is presented in Fig. 1: while gradient back-propagation con-
siders storing intermediate gradients in a forward pass, TP algorithms proceed by computing and storing
approximate inverses. The approximate inverses are then used to pass targets backward along the graph of
computations to ﬁnally yield a weight update for stochastic learning.
TPaimstotakeadvantageoftheavailabilityofapproximateinversestocomputebetterdescentdirectionsfor
the objective at hand. Bengio et al. (2013) and Bengio (2020) argued that the approach could be relevant for
problems involving multiple compositions such as the training of Recurrent Neural Networks (RNNs), which
generally suﬀer from the phenomenon of exploding or vanishing gradients (Hochreiter, 1998; Bengio et al.,
1994; Schmidhuber, 1992). This perspective was studied theoretically for Diﬀerence Target Propagation
(DTP), a modern variant of TP, that was related to an approximate Gauss-Newton method, suggesting
interesting venues to explain the beneﬁts of TP (Bengio, 2020; Meulemans et al., 2020; 2021; Ernoult et al.,
2022; Fairbank et al., 2022).
Another motivation for TP comes from neuroscience to propose biologically plausible learning processes in
the brain and overcome the biological implausibility of gradient Back-Propagation (BP) (Ernoult et al.,
∗Now at Google.
1Published in Transactions on Machine Learning Research (01/2023)
Gradient Back-Propagation Target Propagation
Forward  
Pass
Backward  
Pass
Fig. 1: Our implementation of target propagation uses the linearization of gradient inverses instead of gradients in
a backward pass akin to gradient back-propagation.
2022). The biological plausibility of TP required its actual implementation to use several approximations
such as learning inverses with reverse layers (Lee et al., 2015; Manchev & Spratling, 2020). However, it is
unclear whether such reverse layers learn layer inverses during the training process, which hinders the study
of the original idea of using approximate inverses.
To promote the original idea of TP and guide its implementation from both a biological and an optimization
viewpoint, we develop here a simpliﬁed implementation of TP using analytical formulations. The proposed
approach formalizes several heuristics used in previous implementations to highlight the key components
that can make TP successful to, e.g., learning from long sequences. In our implementation of TP for re-
current neural networks, we consider approximating inverses via their variational formulation with analytic
expressions. The analytic expression of our approximate inverse sheds light on the relevance of a regular-
ization term which can be interpreted as inverting perturbed layers, an insight studied earlier by Lee et al.
(2015) and Manchev & Spratling (2020) when learning reversed layers. In addition, by using analytical ex-
pressions, we can directly test recent interpretations of TP as an approximate Gauss-Newton method whose
approximation comes from the learned reverse layers (Bengio, 2020; Meulemans et al., 2020; Ernoult et al.,
2022).
Inthisspirit, wealso use theinterpretationoftheDTPformula(Leeet al., 2015)asaﬁnitediﬀerenceapprox-
imation of a linearized regularized inverse to propose a smoother formula that can directly be integrated into
a diﬀerentiable programming framework. We detail the computational complexity of the proposed implemen-
tation of TP and compare it to the one of BP, showing that the additional cost of computing inverses can be
eﬀectively amortized for very long sequences. Following the benchmark of Manchev & Spratling (2020), we
observe that the proposed implementation of TP can perform better than classical gradient-based methods
on several tasks involving RNNs. The code is made publicly available at https://github.com/vroulet/tpri .
Relatedwork. TheuseofthegradientBack-Propagation(BP)algorithmtotraindeepnetworkswaspopu-
larized by Rumelhart et al. (1986), though the algorithm had been used in several other ﬁelds before (Werbos,
1974; 1994; Griewank & Walther, 2008; Goodfellow et al., 2016). At the same time, alternatives to BP were
proposed by, e.g., (LeCun, 1985, Section 2.2), by using “ideal states" propagated through the transpose of
the weights and initialized at the desired outputs to deﬁne local learning objectives at each layer of the
network. This idea was further explored in two diﬀerent ways: either by redeﬁning the training problem
through the optimization of ideal states called “targets” as initiated by Rohwer (1989) or by redeﬁning the
backward dynamics of BP to back-propagate the desired outputs through approximate inverses as developed
by, e.g., Bengio (2014) and Lee et al. (2015).
Optimization in target space. The training of deep networks can be seen as an optimization problem over
both the weights of the layers and the intermediate states computed by the network, the two diﬀerent sets
of variables being constrained by the dynamics of the network. For a given set of weights, the intermediate
states are entirely determined by the input of the network; that is the viewpoint adopted by BP by using
forward-backward passes on the network (Rumelhart et al., 1986). Rohwer (1989) proposed to consider the
intermediate states as the main variables of the network, which he called “targets”, by computing for a ﬁxed
set of targets the optimal weights such that the dynamics are approximately satisﬁed while the input and
2Published in Transactions on Machine Learning Research (01/2023)
output of the network are ﬁxed at the values given by the training set. The overall training objective depends
only on targets that can be optimized by gradient descent on the error of reconstruction of the dynamics.
Such an approach has been re-proposed under diﬀerent names by Atiya & Parlos (2000) and Castillo et al.
(2006) and later cast as penalized formulation of the dynamical constraints by Carreira-Perpinan & Wang
(2014) who exploited the resulting unconstrained formulation to decouple the training of each layer by
alternate optimization. Taylor et al. (2016) and Gotmare et al. (2018) considered tackling the dynamical
constraints by using a Lagrangian formulation of the problem and using an Alternative Direction Method
of Multipliers (ADMM). In this spirit, Wiseman et al. (2017) used an ADMM-like algorithm for language
modeling and reported disappointing experimental results.
On the other hand, Fairbank et al. (2022) revisited the original approach of Rohwer (1989) by considering
a reconstruction error of the dynamics only based on its linear part and reported successful experimental
results on several architectures. The main diﬃculty of the optimization in target space is that the total
number of variables is a priori equal to the number of samples times the width and depth of the network.
This diﬃculty was overcome by Fairbank et al. (2022) by considering optimizing targets deﬁned on a subset
of the training set, then propagating the information given by new samples by the chain rule. Taylor et al.
(2016) overcame this issue by simplifying the optimization of the targets as one coordinate descent pass on
the Lagrangian formulation which then resembles a back-propagation algorithm. In our work, we consider
back-propagating targets to update weights rather than considering directly the targets as variables.
Back-propagating targets or gradient surrogates. The original idea of LeCun (1985) to use back-propagation
mechanisms that could back-propagate ideal states through the network was revisited a few decades later
by Bengio (2014) and Lee et al. (2015). Although directly using approximate inverses of the layers to
back-propagate targets appeared unsuccessful, a slight variant called Diﬀerence Target Propagation (DTP),
which stabilizes the inverses by means of a ﬁnite diﬀerence scheme explained in Sec. 2.2, matched roughly
the performance of BP. We formalize this variant by using directly the Jacobian of the approximate inverse
to test the relevance of the approach from an optimization viewpoint. DTP has been recently interpreted
theoretically as an approximate Gauss-Newton method (Bengio, 2020; Meulemans et al., 2020; 2021), a
viewpoint that we discuss in detail in Appendix C. On a practical side, DTP was successfully implemented
for recurrent neural networks by Manchev & Spratling (2020). We follow the experimental benchmark done
by Manchev & Spratling (2020) while formalizing some of the mechanisms of DTP in our work. Recently,
Ahmad et al. (2020) and Dalm et al. (2021) considered using analytical inverses to implement TP and blend
it with what they called a gradient-adjusted incremental formula. Yet, an additional orthogonality penalty
is critical for their approach to work, while our approach can dispense of such additional penalty.
The back-propagation of gradients has also been bypassed by approximating the sensitivity of the output of
the network to small perturbations by Le Cun et al. (1988) in order to update weights according to these
sensitivities. Alternatively, gradient surrogates were approximated with “synthetic gradients” by Jaderberg
et al. (2017) and Czarnecki et al. (2017) to decouple the backward pass of feed-forward deep networks and
speed up the training process. Roulet & Harchaoui (2022) considered computing gradients of the Moreau
envelope through the structure of the network and cast several previous works as instances of approximate
computations of the gradients of the Moreau envelope. Rather than modifying the backward operations in
the layers, one can also modify the weight updates for deep forward networks by using a regularized inverse
as successfully implemented by Frerix et al. (2018). Recently, Amid et al. (2022) generalized the approach
of Frerix et al. (2018) by considering alternative reconstruction losses, that is, alternative objectives for the
weights to minimize for given targets by using matching losses on pre-activations or post-activations targets
deﬁned by a gradient step on these quantities.
Biological plausibility. A recurrent motivation for the development of TP algorithms has been the biological
implausibility of BP (Crick, 1989). We summarize here some of the arguments against the biological plau-
sibility of BP and refer the interested reader to, e.g., Bengio (2020) and Manchev & Spratling (2020) for
a detailed discussion. First BP requires each neuron to emit two distinct signals (forward and backward)
which has not been observed neurophysiologically (Roelfsema & Ooyen, 2005). Another issue is that BP
solves the credit assignment problem (Minsky, 1961; Hinton et al., 1984) by using weight transport, imply-
ing a symmetric backward connectivity pattern, which is thought impossible in the brain (Lillicrap et al.,
3Published in Transactions on Machine Learning Research (01/2023)
2016). On the other hand, DTP may circumvent some of these issues such as weight transport by using
diﬀerent dynamics forward and backward. The credit assignment problem and other biological constraints to
model learning schemes motivated several variations of TP or other learning schemes such as direct feedback
alignment (Nøkland, 2016; Bartunov et al., 2018; Crafton et al., 2019; Lansdell et al., 2019; Akrout et al.,
2019; Kunin et al., 2020; Ernoult et al., 2022). As mentioned earlier, our objective here is to complement
the biological viewpoint on TP by taking an optimization viewpoint to understand its relevance in training
performance and use these ﬁndings to serve as guidelines for biologically plausible implementations of TP.
Notation. Forf:Rp×Rq→Rd, the partial derivative of fw.r.t.xon(x,y)∈Rp×Rqis denoted
∂xf(x,y) =/parenleftbig
∂fj(x,y)/∂xi/parenrightbig
i,j∈Rp×d.
2 Target Propagation with Linearized Regularized Inverses
While TP was initially developed for multi-layer neural networks, we focus on its implementation for Re-
current Neural Networks (RNNs), as we shall follow the benchmark of Manchev & Spratling (2020) in the
experiments. RNNs are also a canonical family of neural networks in which interesting phenomena arise in
back-propagation algorithms.
Problem setting. A simple RNN parameterized by θ= (Whh,Wxh,bh,Why,by)maps a sequence of inputs
x1:τ= (x1,...,xτ)to an output ˆy=Fθ(x1:τ)by computing hidden states ht∈Rdhcorresponding to the
inputsxt∈Rdx. Formally, the output ˆyand the hidden states htare computed as an output operation
following transition operations
ˆy=gθ(hτ) :=s(Whyhτ+by),
ht=fθ,t(ht−1) :=a(Wxhxt+Whhht−1+bh)fort∈{1,...,τ},
wheresis, e.g., thesoft-maxfunctionforclassiﬁcationtasks, aisanon-linearoperationsuchasthehyperbolic
tangent function, and the initial hidden state is generally ﬁxed as h0= 0. Given samples of sequence-output
pairs (x1:τ,y), the RNN is trained to minimize the error /lscript(y,Fθ(x1:τ))of predicting ˆy=Fθ(x1:τ)instead ofy.
As one considers longer sequences, RNNs face the challenge of exploding/vanishing gradients
∂Fθ(x1:τ)/∂ht(Bengio & Frasconi, 1995); see Appendix A for more discussion. We acknowledge that speciﬁc
parameterization-based strategies have been proposed to address this issue of exploding/vanishing gradients,
such as orthonormal parameterizations of the weights (Arjovsky et al., 2016; Helfrich et al., 2018; Lezcano-
Casado & Martınez-Rubio, 2019). The focus here is to simplify and understand TP as a back-propagation-
type algorithm using RNNs as a workbench. Indeed, training RNNs is an optimization problem involving
multiple compositions for which approximate inverses can easily be available. The framework could also be
applied to, e.g., time-series or control models (Roulet et al., 2019).
Giventheparameters Whh,Wxh,bhofthetransitionoperations, wecangetapproximateinversesof fθ,t(ht−1)
for allt∈{1,...,τ}, that yield optimization surrogates that can be better performing than the ones cor-
responding to regular gradients. We present below a simple version of TP based on regularized analytic
inverses andinverse linearizations .
Back-propagating targets. The idea of TP is to compute virtual targets vtfor each layer t=τ,..., 1
such that if the layers were able to match their corresponding target at time t, i.e.,fθ,t(ht−1)≈vt, the
objective would decrease. The ﬁnal target vτis computed as a gradient step on the loss w.r.t. hτ. The
targets are back-propagated using an approximate inverse f†
θ,toffθ,tat each time step. detailed in Sec. 2.1.
Formally, consider an RNN that computed τstatesh1,...,hτfrom a sequence x1,...,xτwith associated
outputy. For a given stepsize γh>0, we propose to back-propagate targets by computing
vτ=hτ−γh∂h/lscript(y,gθ(hτ)), (1)
vt−1=ht−1+∂hf†
θ,t(ht)/latticetop(vt−ht),fort∈{τ,..., 1}. (2)
4Published in Transactions on Machine Learning Research (01/2023)
The update rule (2) blends two ideas: i) regularized analytic inversions to approximate the inverse f−1
θ,tby
f†
θ,t; ii) linear approximation, i.e., using the Jacobian of the approximate inverse instead of the diﬀerence
target propagation formula proposed by Lee et al. (2015, Eq. 15). We shall also show that the update (2)
puts in practice an insight from Bengio (2020) suggesting to use the inverse of the gradients in the spirit of
a Gauss-Newton method. Once all targets are computed, the parameters of the transition operations are
updated such that the outputs of fθ,tat each time step move closer to the given target.
Formally, the update consists of a gradient step with stepsize γθon the squared error between the targets
and the current outputs, i.e., for θh∈{Whh,Wxh,bh},
θnext
h=θh−γθτ/summationdisplay
t=1∂θh/bardblfθ,t(ht−1)−vt/bardbl2
2/2. (3)
As for the parameters θy= (Why,by), they are updated by a simple gradient step on the loss with the
stepsizeγθ.
2.1 Regularized Analytic Inversion
To explore further the ideas of LeCun (1985) and Lee et al. (2015), we consider the variational deﬁnition of
the inverse,
f−1
θ,t(vt) = argmin
vt−1∈Rdh/bardblfθ,t(vt−1)−vt/bardbl2
2= argmin
vt−1∈Rdh/bardbla(Wxhxt+Whhvt−1+bh)−vt/bardbl2
2. (4)
As long asvtbelongs to the image fθ,t(Rdh)offθ,t, this deﬁnition recovers exactly the inverse of vtbyfθ,t.
More generally, if vt/negationslash∈fθ,t(Rdh), Eq. (4) computes the best approximation of the inverse in the sense of the
Euclidean projection. For an injective activation function aandθh= (Whh,Wxh,bh), the solution of (4) can
easily be computed. Formally, for the sigmoid, the hyperbolic tangent, or the ReLU, their inverse can be
obtained analytically for any vt∈a(Rdh). So forvt∈a(Rdh)andWhhfull rank, we get
f−1
θ,t(vt) = (W/latticetop
hhWhh)−1W/latticetop
hh(a−1(vt)−Wxhxt−bh).
Ifvt/negationslash∈a(Rdh), the minimizer of (4) is obtained by ﬁrst projecting vtontoa(Rdh), before inverting the linear
operation. To account for non-invertible matrices Whh, we also add a regularization in the computation of
the inverse. Overall we consider approximating the inverse of the layer by a regularized inverse of the form
f†
θ,t(vt) = (W/latticetop
hhWhh+rI)−1W/latticetop
hh(a−1(π(vt))−Wxhxt−bh),
with a regularization parameter r>0and a projection πontoa(Rdh)detailed in Appendix B.
Analytic inversion vs. parameterized inversion. Bengio (2014) and Manchev & Spratling (2020)
parameterize the inverse as a reverse layer such that
f†
θ,t(vt) =ψθ/prime,t(vt) :=a(Wxhxt+Vvt+c),
and learn the parameters θ/prime= (V,c)for this reverse layer to approximate the inverse of the forward com-
putations. Learning good approximations comes with a computational cost that can be better controlled
by using analytic inversions presented above. The approach based on parameterized inverses may lack the-
oretical grounding, as pointed out by Bengio (2020), as we do not know how close the learned inverse is to
the actual inverse throughout the training process. In contrast, the analytic inversion (4) is less ad hoc, it
enables us to test directly the main idea of TP and it leads to competitive performance on real datasets.
The analytic formulation of the inverse gives simple insights into an approach with parameterized inverses.
Namely, the analytic formula suggests parameterizing the reverse layer such that (i) the reverse activation
is deﬁned as the inverse of the activation, (ii) the layer uses a non-linear operation followed by a linear one
instead of a linear operation followed by a non-linear one as used by, e.g., Manchev & Spratling (2020).
5Published in Transactions on Machine Learning Research (01/2023)
Regularized inversion vs noise injection. A potential issue with learning parameterized inverses as
done by Manchev & Spratling (2020) and Lee et al. (2015) is that the inverse depends a priori on the current
set of parameters of the transition function, which, in turn, changes along the training process. Manchev &
Spratling (2020) and Lee et al. (2015) considered then stabilizing the learning process of the parameterized
inverse by letting it minimize perturbed versions of the layers, an idea reminiscent of the early work of Le Cun
et al. (1988).
Formally, the parameterized inverses are learned by performing gradient steps over θ/primeon/summationtextτ
t=1/bardblψθ/prime,t(fθ,t(ht−1+εt−1))−(ht−1+εt−1)/bardbl2
2for/epsilon1t−1∼N (0,σ2I)andh0,...,hτ−1given by a forward
pass on the RNN for a given sample and ﬁxed parameters θ. Such a strategy led to performance gains in
the overall training (Manchev & Spratling, 2020, Section 3.1.2, Figure 7).
In our formulation, the regularization can be seen as a counterpart of the noise injection. Namely, consider
the inverse of averaged perturbed layers, i.e, the minimizer in vt−1ofEz∼N(0,σ2I)/bardblfθ+z,t(vt−1)−vt/bardbl2
2. As
shown in Appendix C, the regularized inverse f†
θ,tminimizes an upper-bound of this objective, namely, for
σ2=r/dhandvt∈a(Rdh),
f†
θ,t(vt) = argmin
vt−1∈RdhEz∼N(0,σ2I)/bardbla−1(fθ+z,t(vt−1))−a−1(vt)/bardbl2
2,
where Ez∼N(0,σ2I)/bardbla−1(fθ+z,t(vt−1))−a−1(vt)/bardbl2
2≥/lscriptaEz∼N(0,σ2I)/bardblfθ+z,t(vt−1)−vt/bardbl2
2for a/lscripta-Lipschitz-
continuous activation aand the approximation error of using the upper-bound is quantiﬁed in Appendix C.
The regularization rcan then be interpreted as a hyper-parameter that ensures that the inverses computed
for the current parameters are also approximately valid for the updated parameters as long as /bardblθnext
h−θh/bardbl2
2≤
σ2=r/dh, which hints that the regularization rmay be chosen such that ris larger than a constant times
γθ.
2.2 Linearized Inversion
Earlier instances of TP used directly approximate inverses of the network layers such that the TP update
formula would read vt−1=f†
θ,t(vt)in (2). Yet, we are unaware of a successful implementation of TP using
directly the inverses. To circumvent this issue, Lee et al. (2015) proposed the Diﬀerence Target Propagation
(DTP) formula
vt−1=ht−1+f†
θ,t(vt)−f†
θ,t(ht).
If the inverses were exact, the DTP formula would reduce to vt−1=f−1
θ,t(vt). Lee et al. (2015) introduced
the DTP formula to mitigate the approximation error of the inverses by parameterized layers. The DTP
formula was recently interpreted as an approximate Gauss-Newton method (Meulemans et al., 2020; Bengio,
2020) by observing that it can be approximated by the linearization of the inverse as
f†
θ,t(vt)−f†
θ,t(ht) =∂hf†
θ,t(ht)/latticetop(vt−ht) +O(/bardblvt−ht/bardbl2
2).
Our implementation puts in practice this insight. We show in Appendix D that the linearization we pro-
pose (2) leads to slightly better training curves than the ﬁnite-diﬀerence approximation. Moreover, our
linearized formula enables a simple implementation of TP in a diﬀerentiable programming framework and
simpliﬁes its comparison with gradient back-propagation from a computational viewpoint.
3 Gradient Back-Propagation versus Target Propagation
Graph of computations. Gradient back-propagation and target propagation both compute an update
direction for the objective at hand. The diﬀerence lies in the oracles computed and stored in the forward
pass, while the graph of computations remains the same. To clarify this view, we reformulate TP in terms
6Published in Transactions on Machine Learning Research (01/2023)
... ...
 
...
......
...  
Fig. 2: The graph of computations of target propagation is the same as the one of gradient back-propagation except
thatf†needs to be computed and Jacobian of the inverses, ∂hf† /latticetopare used instead of gradients ∂hfin the transition
operations.
of displacements λt=vt−htsuch that Eq. (1), (2) and (3) read
λτ=−γh∂h/lscript(y,gθ(hτ)), λ t−1=∂hf†
θ,t(ht)/latticetopλt,fort∈{τ,..., 1},
uθh=τ/summationdisplay
t=1∂θhfθ,t(ht−1)λt, θnext
h=θh+γhuθh.
TP amounts then to computing an update direction uθhfor the parameters θhwith a graph of computations,
illustrated in Fig. 2, analogous to that of BP illustrated in Appendix A. The diﬀerence lies in the use of the
Jacobian of the inverse
∂hf†
θ,t(ht)/latticetopinstead of ∂hfθ,t(ht−1).
The implementation of TP with the formula (2) can be done in a diﬀerentiable programming framework,
where, rather than computing the gradient of the layer, one evaluates the inverse and keep the Jacobian of
the inverse. With the precise graph of computation of TP and BP, we can compare their computational
complexity explicitly and bound the diﬀerence in the directions they output.
Arithmetic complexity. Clearly, the space complexities of BP and our implementation of TP are the
same since the Jacobians of the inverse and the original gradients have the same size. In terms of time
complexity, TP appears at ﬁrst glance to introduce an important overhead since it requires the computation
of some inverses. However, a close inspection of the formula of the regularized inverse reveals that a matrix
inversion needs to be computed only once for all time steps. Therefore the cost of the inversion may be
amortized if the length of the sequence is particularly long.
Formally, thetimecomplexityoftheforward-backwardpassofgradientback-propagationisessentiallydriven
by matrix-vector products, i.e.,
TBP=τ/summationdisplay
t=1
T(fθ,t) +T(∂hfθ,t) +T(∂θhfθ,t)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Forward+T(∂hfθ,t(ht−1)) +T(∂θhfθ,t(ht−1))/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Backward

≈τ(dxdh+dh2) +τdh2,
wheredxis the dimension of the inputs xt,dhis the dimension of the hidden states ht, for a function fwe
denote byT(f)the time complexity to evaluate fand we consider, e.g., ∂θfθ,t(ht−1)as the linear function
λ→∂θfθ,t(ht−1))λ.
7Published in Transactions on Machine Learning Research (01/2023)
On the other hand, the time complexity of target propagation is
TTP=τ/summationdisplay
t=1
T(fθ,t)+T(f†
θ,t)+T(∂θhfθ,t) +T(∂hf†
θ,t)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Forward+T(∂hf†
θ,t(ht)/latticetop)+T(∂θhfθ,t(ht−1))
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Backward
+P(f†
θ,t),
whereP(f†
θ,t)is the cost of encoding the inverse, which, in our case, amounts to the cost of encoding
cθ:z→(W/latticetop
hhWhh+rI)−1W/latticetop
hhz,suchthatourregularizedinversecanbecomputedas f†
θ,t(vt) =cθ(a−1(vt)−
Wxhxt+bh). Encoding cθcomes at the cost of inverting one matrix of size dh. Therefore, the time-complexity
of TP can be estimated as
TTP≈dh3+τ(dxdh+dh2) +τdh2≈TBPifτ≥dh.
So for long sequences whose length is larger than the dimension of the hidden states, the cost of TP with
regularized inverses is approximately the same as the cost of BP. If a parameterized inverse was used rather
than a regularized inverse, the cost of encoding the inverse would correspond to the cost of updating the
reverse layers by, e.g., a stochastic gradient descent. This update has a cost similar to BP. However, it is
unclear whether these updates get us close to the actual inverses.
Bounding the diﬀerence between target propagation and gradient back-propagation. As the
computational graphs of BP and TP are the same, we can bound the diﬀerence between the oracles returned
by both methods. First, note that the updates of the parameters of the output functions are the same since,
in TP, gradient steps of the loss are used to update these parameters. The diﬀerence between TP and BP
lies in the updates with respect to the parameters of the transition operations.
For BP, the update direction is computed by chain rule as
∂θh/lscript(y,Fθ(x1:τ)) =τ/summationdisplay
t=1∂θhfθ,t(ht−1)∂hτ
∂ht∂hτ/lscript(y,gθ(hτ)),
where the term ∂hτ/∂htdecomposes along the time steps as ∂hτ/∂ht=/producttextτ
s=t+1∂hs−1fθ,s(hs−1). The
direction computed by TP has the same structure, namely it can be decomposed for γh= 1as
uθh=τ/summationdisplay
t=1∂θhfθ,t(ht−1)ˆ∂hτ
ˆ∂ht∂hτ/lscript(y,gθ(hτ)),
where ˆ∂hτ/ˆ∂ht=/producttextτ
s=t+1∂hsf†
θ,s(hs)/latticetop. We can then bound the diﬀerence between the directions given by
BP or TP as formally stated in the following lemma.
Lemma 3.1. The diﬀerence between the oracle returned by gradient back-propagation ∂θh/lscript(y,Fθ(x1:τ))and
the oracle returned by target propagation uθhcan be bounded as
/bardbl∂θh/lscript(y,Fθ(x1:τ))−uθh/bardbl≤τ/summationdisplay
t=1ct/bardbl∂θhfθ,t(ht−1)/bardbl/bardbl∂hτ/lscript(y,gθ(hτ))/bardblsup
t=1,...,τ/bardbl∂ht−1fθ,t(ht−1)−∂htf†
θ,t(ht)/latticetop/bardbl,
wherect=/summationtextt−1
s=0asbt−1−switha= supt=1,...τ/bardbl∂ht−1fθ,t(ht−1)/bardbl,b= supt=1,...τ/bardbl∂htf†
θ,t(ht)/latticetop/bardbl.
For regularized inverses, we have, denoting zt=Wxhxt+Whhht−1+bh,
/bardbl∂ht−1fθ,t(ht−1)−∂htf†
θ,t(ht)/latticetop/bardbl≤/bardblW/latticetop
hh/bardbl/parenleftBig
/bardbl∇a(zt)−∇a(zt)−1/bardbl+/bardblI−(W/latticetop
hhWhh+rI)−1/bardbl/bardbl∇a(zt)−1/bardbl/parenrightBig
.
For the two oracles to be close, we need the preactivation zt=Wxhxt+Whhht−1+bhto lie in the region
of the activation function that is close to being linear s.t. ∇a(zt)≈I. We also need (W/latticetop
hhWhh+rI)−1to
be close to the identity which can be the case if, e.g., r= 0and the weight matrices Whhwere orthonormal.
By initializing the weight matrices as orthonormal matrices, the diﬀerences between the two oracles can be
close in the ﬁrst iterations. However, in the long term, target propagation appears to give potentially better
oracles, as shown in the experiments below. From a theoretical standpoint, the above lemma can be used
to obtain convergence guarantees to stationary points of TP up to the approximation error of BP by TP as
shown in Appendix C.
8Published in Transactions on Machine Learning Research (01/2023)
0 2 4
Iterations×104020004000 Train Loss
0 2 4
Iterations×104020004000
0 1 2
Iterations×104050100
0 2 4
Iterations×1040255075100 Accuracy
0 2 4
Iterations×1040255075100
0 1 2
Iterations×1040255075100TP BP
Fig. 3: Temporal order problem T= 60, Temporal order problem T= 120, Adding problem T= 30.
Testing the interpretation of TP as a Gauss-Newton method. Recently target propagation has
been interpreted as an approximate Gauss-Newton (GN) method, by considering that the DTP formula
approximates the linearization of the inverse, which itself is a priori equal to the inverse of the gradi-
ents (Bengio, 2020; Meulemans et al., 2020; 2021). Namely, provided that f†
θ,t(fθ,t(ht−1))≈ht−1such that
∂ht−1fθ,t(ht−1)∂htf†
θ,t(ht)≈I, we have
∂htf†
θ,t(ht)≈/parenleftbig
∂ht−1fθ,t(ht−1)/parenrightbig−1.
Bycomposingtheinversesofthegradients, wegetanupdatesimilartotheoneofGN.Namely, recallthatif n
invertible functions f1,...,fnwere composed to solve a least square problem of the form /bardblfn◦...◦f1(x)−y/bardbl2
2,
aGNupdatewouldtaketheform x(k+1)=x(k)−∂x0f1(x0)−/latticetop...∂xn−1f(xn−1)−/latticetop(xn−y), wherextisdeﬁned
iteratively as x0=x(k),xt+1=ft(xt). In other words, GN and TP share the idea of composing the inverse
of gradients. However, numerous diﬀerences remain as detailed in Appendix C such as the fact that the
gradient of the layer with respect to its parameters is not inverted in the usual implementation of TP and
that the approximation error of GN by TP may grow exponentially with the length of the network. Note that
even if TP was approximating GN, it is unclear whether GN updates are adapted to stochastic problems.
In any case, by using an analytical formula for the inverse, we can test this interpretation by using non-
regularized inverses, which would amount to directly using the inverses of the gradients as in a GN method.
If the success of TP could be explained by its interpretation as a GN method, we should observe eﬃcient
training curves when no regularization is added.
4 Experiments
In the following, we compare our simple TP approach, which we shall refer to as TP, to gradient Back-
Propagation referred to as BP. We follow the experimental benchmark of Manchev & Spratling (2020) to
whichweaddresultsforRNNsonCIFARandforGRUnetworksonFashionMNIST.Additionalexperimental,
details on the initialization and the hyper-parameter selection can be found in Appendix D.
Data. We consider two synthetic datasets generated to present training diﬃculties for RNNs and several
real datasets consisting of scanning images pixel by pixel to classify them (Hochreiter & Schmidhuber, 1997;
Le et al., 2015; Manchev & Spratling, 2020).
9Published in Transactions on Machine Learning Research (01/2023)
0 2 4
Iterations×1042000400060008000 Train Loss
0 2 4
Iterations×1042000400060008000
0 5
Iterations×103700071007200
0 5
Iterations×103600070008000
0 1 2
Iterations×104400600
0 2 4
Iterations×1040255075100 Accuracy
0 2 4
Iterations×1040255075100
0 5
Iterations×10301020
0 5
Iterations×103010203040
0 1 2
Iterations×10401020BP TP
Fig. 4: Image classiﬁcation pixel by pixel and word prediction. From left to right: MNIST, MNIST with
permuted images, CIFAR10, FashionMNIST with GRU, Penn Treebank with RNN.
Temporal order problem. A sequence of length Tis generated using a set of randomly chosen sym-
bols{a,b,c,d}. Two additional symbols XandYare added at positions t1∈[T/10,2T/10]and
t2∈[4T/10,5T/10]. The network must predict the correct order of appearance of XandYout of four
possible choices{XX,XY,YX,YY }.
Adding problem. The input consists of two sequences: one is made of randomly chosen numbers from [0,1],
and the other one is a binary sequence full of zeros except at positions t1∈[1,T/10]andt2∈[T/10,T/2].
The second position acts as a marker for the time steps t1andt2. The goal of the network is to output the
mean of the two random numbers of the ﬁrst sequence (Xt1+Xt2)/2.
Image classiﬁcation pixel by pixel. The inputs are images of (i) grayscale handwritten digits given in the
database MNIST (LeCun & Cortes, 1998), (ii) colored objects from the database CIFAR10 (Krizhevsky,
2009) or (iii) grayscale images of clothes from the database FashionMNIST (Xiao et al., 2017). The images
are scanned pixel by pixel and channel by channel for CIFAR10, and fed to a sequential network such as a
simple RNN or a GRU network (Cho et al., 2014). The inputs are then sequences of 28×28 = 784 pixels
for MNIST or FashionMNIST and 32×32×3 = 3072 pixels for CIFAR with a very long-range dependency
problem. We also consider permuting the images of MNIST by a ﬁxed permutation before feeding them into
the network, which gives potentially longer dependencies in the data.
Word prediction. Given a sequence of twords from 10000 sentences of the Penn Treebank
dataset (Marcinkiewicz, 1994), the task is to predict the t+ 1thword among a vocabulary of 10 000 words.
We consider sequences of length 64 and a ﬁxed embedding of size 1024 of the words.
Model. In both synthetic settings, we consider randomly generated mini-batches of size 20, a simple RNN
with hidden states of dimension 100, and hyperbolic tangent activation. For the temporal order problem,
the output function uses a soft-max function on top of a linear operation, and the loss is the cross-entropy.
For the adding problem, the output function is linear, the loss is the mean-squared error, and a sample is
considered to be accurately predicted if the mean squared error is less than 0.04 as done by (Manchev &
Spratling, 2020).
For the classiﬁcation of images with sequential networks, we consider mini-batches of size 16 and a cross-
entropy loss. For MNIST and CIFAR, we consider a simple RNN with hidden states of dimension 100,
hyperbolic tangent activation, and a softmax output. For FashionMNIST, we consider a GRU network and
adapted our implementation of target propagation to that case while using hidden states of dimension 100
and a softmax output, see Appendix B for the detailed implementation of TP for GRU networks.
10Published in Transactions on Machine Learning Research (01/2023)
0 2 4
Iterations×1040255075100 AccuracyTP DTP-RI DTP-PI
Fig. 5: Comp. of TP implem.
Region of parameters  
with convergence  Fig. 6: Conv. w.r.t. stepsize & reg.
4 14 49 196 392 784
Length163264128256512 Width
BPTP Fig. 7: Perf. vs width & length.
For the prediction of words, we consider an RNN that outputs a prediction at each time step. Our imple-
mentation of TP, in this case, consists in replacing the usual gradient back-propagation rule used in the
computational scheme of gradient back-propagation by the Jacobian of the regularized inverse as explained
in Sec. 3 and detailed in Appendix B. We use a simple RNN architecture with hyperbolic tangent activations
for the transitions of hidden states of size 256, an output function consisting of a soft-max layer on top of a
linear layer, and a cross-entropy loss.
Performance comparisons. In Fig. 3, we observe that TP performs better than BP on the temporal
ordering problem: it is able to reach 100% accuracy in fewer iterations than BP for sequences of length
60 and, for sequences of length 120, it is still able to reach 100% accuracy in fewer than 40 000 iterations
while BP is not. On the other hand, for the adding problem, TP performs less well than BP. The contrast
in performance between the two synthetic tasks was also observed by (Manchev & Spratling, 2020) using
diﬀerence target propagation with parameterized inverses. The main diﬀerence between these tasks is the
nature of the outputs, which are binary for the temporal problem and continuous for the adding problem.
In Fig. 4, we observe that TP generally performs better or as well as BP for image classiﬁcation tasks. For the
MNIST dataset, it reaches around 74% accuracy after 4·104iterations. This phenomenon is also observed
with permuted images, where the optimization appears smoother, and TP obtains around 86% accuracy
after 4·104iterations and is still slightly faster than BP. On the CIFAR dataset, no algorithms appear
to reach a signiﬁcant accuracy, though TP is still faster. On the FashionMNIST dataset, where a GRU
network is used, our implementation of TP performs on par with BP, which shows that our approach can be
generalized to more complex networks than a simple RNN. For the word prediction task, our implementation
of TP is still able to optimize well the objective in terms of training loss, outperforming BP in that regard,
and performing slightly worse in terms of test accuracy.
Comparison of diﬀerent implementations of TP. We evaluate the impact of using regularized in-
verses as opposed to parameterized inverses and linearized propagation as opposed to ﬁnite-diﬀerence-based
propagation. The variant of target propagation with parameterized inverse and ﬁnite-diﬀerence propaga-
tion corresponds to the approach of Lee et al. (2015) recently implemented by Manchev & Spratling (2020)
and referred to in Fig. 5 as DTP-PI. The variant of target propagation with regularized inverse and ﬁnite-
diﬀerence propagation is referred to in Fig. 5 as DTP-RI. Recall that our approach involves regularized
inverses and linearized propagation, referred to as TP. In Fig. 5, we observe that both TP and DTP-RI
outperform DTP-PI, demonstrating the beneﬁts of using regularized inverses. On the other hand, both TP
and DTP-RI perform on par overall, with the former being slightly better for the given parameters. The
linearized formula has the advantage to be easily adapted to other architectures such as a GRU network or an
RNN with intermediate outputs as explained in Appendix B. Additional comparisons of our implementation
of TP as opposed to the implementation of TP by Manchev & Spratling (2020) are presented in Appendix D.
11Published in Transactions on Machine Learning Research (01/2023)
0 2 4
Iterations×10202040 Accuracyγh
1e-06
1e-05
1e-04
1e-03
1e-02
0 2 4
Iterations×10202040 Accuracyγθ
1e-04
1e-03
1e-02
1e-01
1e0
0 2 4
Iterations×10202040 Accuracyr
1e-01
1e0
1e1
1e2
1e3
Fig. 8: Sensitivity analyses of the hyper-parameters of TP on MNIST pixel by pixel. From left to right:
varyingγhforγθ=10−1,r=1, varyingγθforγh=10−4,r=1, varyingrforγh=10−4,γθ=10−1.
Impact of the regularization term. As mentioned in Sec. 3, by using an analytical formula to compute
the inverse of the layers, we can question the interpretation of TP as a Gauss-Newton method, which would
amount to TP without regularization. To understand the eﬀect of the regularization term, we computed
the area under the training loss curve of TP for 400 iterations on a log10grid of varying step-sizes γθand
regularizations rforaﬁxedstepsize γh= 10−3. TheresultsarepresentedinFig.6,wherethesmallerthearea,
thebrighterthepointandtheabsenceofdotsinthegridmeansthatthealgorithmdiverged. Fig.6showsthat
without regularization we were not able to obtain convergence of the algorithm. Simply using the gradients
of the inverse as in a Gauss-Newton method may not directly work for RNNs. Additional modiﬁcations
of the method could be added to make target propagation closer to Gauss-Newton, such as inverting the
layers with respect to their parameters as proposed by Bengio (2020). For now, the regularization appears
to successfully handle the rationale of target propagation.
Performance of TP and BP in terms of the length of the sequence. In Fig. 7, we compare the
performance of BP and TP in terms of accuracy after 400 iterations on the MNIST problem for various
widths determined by the size of the hidden states and various lengths determined by the size of the inputs
(i.e., we feed the RNN with kpixels at a time, which gives a length 784/k). Fig. 7 shows that TP is generally
appropriate for long sequences, while BP remains more eﬃcient for short sequences. TP may then be seen
as an interesting alternative for dynamical problems which involve many discretization steps as in RNNs and
related architectures.
Sensitivity analyses. In Fig. 8, we observe that the algorithm is mostly sensitive to the learning rate γθ
while both the stepsize γhused for the ﬁrst target and the regularization radmit a larger range of values
ensuring fact increase of the accuracy. Note that for the regularization, while most values above r= 1ensure
a similar increase of accuracy, smaller values than 10−1in the MNIST pixel-by-pixel experiment would not
lead to convergent algorithms as explained previously in Fig. 6.
Conclusion
We proposed a simple target propagation approach grounded in two important computational components,
regularized inversion, and linearized propagation. The proposed approach also sheds light on previous
insights and successful rules for target propagation. The code is made publicly available at https://github.
com/vroulet/tpri . We have used target propagation within a stochastic gradient outer loop to train neural
networks for a fair comparison to a stochastic gradient descent using gradient backpropagation. Developing
adaptive stochastic gradient algorithms in the spirit of Adam that lead to boosts in performance when using
target propagation instead of gradient backpropagation is an interesting avenue for future work. Continuous
counterparts of target propagation in a neural ODE spirit is also an interesting avenue for future work.
Acknowledgments. This work was supported by NSF CCF-1740551, NSF DMS-1839371, the CIFAR
program “Learning in Machines and Brains”, and faculty research awards. We thank Nikolay Manchev for
all the details he provided on his code. We also thank the reviewers and the action editor of TMLR for their
insightful feedback that helped improve the content and the presentation of the paper.
12Published in Transactions on Machine Learning Research (01/2023)
References
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,
Andy Davis, Jeﬀrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoﬀrey
Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,
Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens,
BenoitSteiner,IlyaSutskever,KunalTalwar,PaulTucker,VincentVanhoucke,VijayVasudevan,Fernanda
Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.
org/.
NasirAhmad, MarcelAvanGerven, andLucaAmbrogioni. GAIT-prop: Abiologicallyplausiblelearningrule
derived from backpropagation of error. In Advances in Neural Information Processing Systems , volume 33,
2020.
Mohamed Akrout, Collin Wilson, Peter Humphreys, Timothy Lillicrap, and Douglas B Tweed. Deep learning
without weight transport. In Advances in neural information processing systems , volume 32, 2019.
EhsanAmid,RohanAnil, andManfredWarmuth. Locoprop: Enhancingbackpropvialocallossoptimization.
InInternational Conference on Artiﬁcial Intelligence and Statistics , pp. 9626–9642. PMLR, 2022.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In Proceed-
ings of the 33rd International Conference on Machine Learning , 2016.
Amir F Atiya and Alexander G Parlos. New results on recurrent network training: unifying the algorithms
and accelerating convergence. IEEE transactions on neural networks , 11(3):697–709, 2000.
Sergey Bartunov, Adam Santoro, Blake Richards, Luke Marris, Geoﬀrey E Hinton, and Timothy Lillicrap.
Assessing the scalability of biologically-motivated deep learning algorithms and architectures. In Advances
in neural information processing systems , volume 31, 2018.
Yoshua Bengio. How auto-encoders could provide credit assignment in deep networks via target propagation.
arXiv preprint arXiv:1407.7906 , 2014.
Yoshua Bengio. Deriving diﬀerential target propagation from iterating approximate inverses. arXiv preprint
arXiv:2007.15139 , 2020.
Yoshua Bengio and Paolo Frasconi. Diﬀusion of context and credit information in markovian models. Journal
of Artiﬁcial Intelligence Research , 3:249–270, 1995.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent
is diﬃcult. IEEE transactions on neural networks , 5(2):157–166, 1994.
Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013.
Miguel Carreira-Perpinan and Weiran Wang. Distributed optimization of deeply nested systems. In Pro-
ceedings of the 17th International Conference on Artiﬁcial Intelligence and Statistics , 2014.
Enrique Castillo, Bertha Guijarro-Berdinas, Oscar Fontenla-Romero, Amparo Alonso-Betanzos, and Yoshua
Bengio. A very fast learning method for neural networks based on sensitivity analysis. Journal of Machine
Learning Research , 7(7), 2006.
Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statis-
tical machine translation. arXiv preprint arXiv:1406.1078 , 2014.
Brian Crafton, Abhinav Parihar, Evan Gebhardt, and Arijit Raychowdhury. Direct feedback alignment with
sparse connections for local learning. Frontiers in neuroscience , 13:525, 2019.
13Published in Transactions on Machine Learning Research (01/2023)
Francis Crick. The recent excitement about neural networks. Nature, 337:129–132, 1989.
Wojciech Marian Czarnecki, Grzegorz Świrszcz, Max Jaderberg, Simon Osindero, Oriol Vinyals, and Koray
Kavukcuoglu. Understanding synthetic gradients and decoupled neural interfaces. In Proceedings of the
34th International Conference on Machine Learning , 2017.
SanderDalm, Nasir Ahmad, LucaAmbrogioni, and Marcel vanGerven. Scaling uplearning with GAIT-prop.
arXiv preprint arXiv:2102.11598 , 2021.
Olivier Devolder, François Glineur, and Yurii Nesterov. First-order methods of smooth convex optimization
with inexact oracle. Mathematical Programming , 146(1-2):37–75, 2014.
Maxence Ernoult, Fabrice Normandin, Abhinav Moudgil, Sean Spinney, Eugene Belilovsky, Irina Rish, Blake
Richards, and Yoshua Bengio. Towards scaling diﬀerence target propagation by learning backprop targets.
arXiv preprint arXiv:2201.13415 , 2022.
Michael Fairbank, Spyridon Samothrakis, and Luca Citi. Deep learning in target space. Journal of Machine
Learning Research , 23(8):1–46, 2022.
Thomas Frerix, Thomas Möllenhoﬀ, Michael Moeller, and Daniel Cremers. Proximal backpropagation. In
Proceedings of the 6th International Conference on Learning Representations , 2018.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning . The MIT Press, 2016.
Akhilesh Gotmare, Valentin Thomas, Johanni Brea, and Martin Jaggi. Decoupling backpropagation using
constrained optimization methods. In Credit Assignment in Deep Learning and Reinforcement Learning
Workshop (ICML 2018 ECA) , 2018.
Andreas Griewank and Andrea Walther. Evaluating derivatives: principles and techniques of algorithmic
diﬀerentiation , volume 105. Siam, 2008.
Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled Cayley
transform. In Proceedings of the 35th International Conference on Machine Learning , 2018.
Geoﬀrey E Hinton, Terrence J Sejnowski, and David H Ackley. Boltzmann machines: Constraint satisfaction
networks that learn. Technical report, Computer Science Department, Carnegie Mellon University, 1984.
Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem so-
lutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems , 6(02):107–116,
1998.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–1780,
1997.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David Silver, and
Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. In Proceedings of the 34th
International Conference on Machine Learning , 2017.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of
Toronto, 2009.
Daniel Kunin, Aran Nayebi, Javier Sagastuy-Brena, Surya Ganguli, Jonathan Bloom, and Daniel Yamins.
Tworoutestoscalablecreditassignmentwithoutweightsymmetry. In Proceedings of the 37th International
Conference on Machine Learning , 2020.
Benjamin James Lansdell, Prashanth Ravi Prakash, and Konrad Paul Kording. Learning to solve the credit
assignment problem. In Proceedings of the 7th International Conference on Learning Representations ,
2019.
Quoc V Le, Navdeep Jaitly, and Geoﬀrey E Hinton. A simple way to initialize recurrent networks of rectiﬁed
linear units. arXiv preprint arXiv:1504.00941 , 2015.
14Published in Transactions on Machine Learning Research (01/2023)
Yann Le Cun, Conrad C Galland, and Geoﬀrey E Hinton. GEMINI: gradient estimation through matrix
inversion after noise injection. In Advances in Neural Information Processing Systems , volume 1, 1988.
Yann LeCun. Une procédure d’apprentissage pour reseau à seuil asymétrique. Proceedings of Cognitiva 85 ,
pp. 599–604, 1985.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. http://yann.lecun.com/exdb/mnist/,
1998.
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Diﬀerence target propagation. In
Machine Learning and Knowledge Discovery in Databases . Springer, 2015.
Mario Lezcano-Casado and David Martınez-Rubio. Cheap orthogonal constraints in neural networks: A
simple parametrization of the orthogonal and unitary group. In Proceedings of the 36th International
Conference on Machine Learning , 2019.
Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synaptic feedback
weights support error backpropagation for deep learning. Nature communications , 7(1):1–10, 2016.
Nikolay Manchev and Michael Spratling. Target propagation in recurrent neural networks. Journal of
Machine Learning Research , 21(7):1–33, 2020.
Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. Using Large
Corpora, 273, 1994.
Alexander Meulemans, Francesco Carzaniga, Johan Suykens, João Sacramento, and Benjamin F. Grewe.
A theoretical framework for target propagation. In Advances in Neural Information Processing Systems ,
volume 33, 2020.
Alexander Meulemans, Matilde Tristany Farinha, Javier Garcia Ordonez, Pau Vilimelis Aceituno, João
Sacramento, and Benjamin F Grewe. Credit assignment in neural networks through deep feedback control.
InAdvances in Neural Information Processing Systems , volume 34, 2021.
Marvin Minsky. Steps toward artiﬁcial intelligence. Proceedings of the IRE , 49(1):8–30, 1961.
Piotr Mirowski and Yann LeCun. Dynamic factor graphs for time series modeling. In Machine Learning and
Knowledge Discovery in Databases . Springer, 2009.
Arild Nøkland. Direct feedback alignment provides learning in deep neural networks. In Advances in neural
information processing systems , volume 29, 2016.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient problem. arXiv
preprint arXiv:1211.5063 , 2012.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in
Neural Information Processing Systems , volume 32, 2019.
Pieter R Roelfsema and Arjen van Ooyen. Attention-gated reinforcement learning of internal representations
for classiﬁcation. Neural computation , 17(10):2176–2214, 2005.
Richard Rohwer. The “moving targets" training algorithm. In Advances in Neural Information Processing
Systems, volume 2, 1989.
Vincent Roulet and Zaid Harchaoui. Diﬀerentiable programming a la moreau. In International Conference
on Acoustics, Speech and Signal Processing (ICASSP) , 2022.
15Published in Transactions on Machine Learning Research (01/2023)
Vincent Roulet, Siddhartha Srinivasa, Dmitriy Drusvyatskiy, and Zaid Harchaoui. Iterative linearized con-
trol: stable algorithms and complexity guarantees. In Proceedings of the 36th International Conference on
Machine Learning , 2019.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning Internal Representations by Error Propagation .
MIT Press, Cambridge, MA, USA, 1986.
Jürgen Schmidhuber. Learning complex, extended sequences using the principle of history compression.
Neural Computation , 4(2):234–242, 1992.
Ilya Sutskever, James Martens, and Geoﬀrey E Hinton. Generating text with recurrent neural networks. In
Proceedings of the 28th International Conference on Machine Learning , 2011.
Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training
neural networks without gradients: A scalable ADMM approach. In Proceedings of the 33rd International
Conference on Machine Learning , 2016.
P. J. Werbos. Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences . PhD
thesis, Harvard University, 1974.
Paul Werbos. The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political
Forecasting . Wiley-Interscience, 1994.
Sam Wiseman, Sumit Chopra, Marc-Aurelio Ranzato, Arthur Szlam, Ruoyu Sun, Soumith Chintala, and
Nicolas Vasilache. Training language models using target-propagation. arXiv preprint arXiv:1702.04770 ,
2017.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking
machine learning algorithms. https://github.com/zalandoresearch/fashion-mnist, 2017.
16Published in Transactions on Machine Learning Research (01/2023)
Appendix Outline
The Appendix is organized as follows.
1. Sec. A recalls how gradient back-propagation works for RNNs.
2. Sec. B details the implementation of target propagation.
3. Sec. C details (i) the interpretation of the regularization as a counterpart of the noise injection, (ii)
the diﬀerences between TP and BP, (iii) the interpretation of TP as a Gauss-Newton method.
4. Sec. D details the parameters used in our experiments and presents additional experiments.
A Gradient Back-Propagation in Recurrent Neural Networks
Given diﬀerentiable activation functions a, the training of recurrent neural networks is amenable to opti-
mization by gradient descent. The gradients can be computed by gradient back-propagation implemented
in modern diﬀerentiable programming software (Rumelhart et al., 1986; Werbos, 1994; Paszke et al., 2019;
Abadi et al., 2015). The gradient back-propagation algorithm is illustrated in Fig. 9. Formally, the gradients
are computed by the chain rule such that, for a sample (y,x1:τ),θh= (Whh,Wxh,bh), and ˆy=Fθ(x1:τ)the
predicted output,
∂/lscript(y,ˆy)
∂θh=τ/summationdisplay
t=1∂ht
∂θh∂hτ
∂ht∂ˆy
∂hτ∂/lscript
∂ˆy.
The term∂hτ/∂htdecomposes along the time steps as
∂hτ
∂ht=τ/productdisplay
s=t+1∂hs
∂hs−1.
Asτgrows, the norm of the term ∂hτ/∂htmay then either increase to inﬁnity ( exploding gradients ) or
exponentially decrease to 0 ( vanishing gradients ). This phenomenon may prevent the RNN from learning
from dependencies between temporally distant events (Hochreiter, 1998). Several solutions were proposed to
tackle this issue, including changing the network architecture (Hochreiter & Schmidhuber, 1997), Hessian-
free optimization (Sutskever et al., 2011), gradient clipping and regularization (Pascanu et al., 2012), or
orthonormal parametrizations (Arjovsky et al., 2016; Helfrich et al., 2018; Lezcano-Casado & Martınez-
Rubio, 2019). We consider here propagating targets instead of gradients as ﬁrst presented by LeCun (1985)
and recently revisited by Bengio (2014); Lee et al. (2015).
B Detailed Implementation
B.1 Target Propagation for RNNs with ﬁnal outputs
As detailed in Sec. 3, target propagation with linearized regularized inverses amounts to move along an
update direction computed by a forward-backward algorithm akin to gradient propagation. The iterations
of linearized target propagation are then summarized in Algo. 1 and Algo. 2.
In the implementation of the regularized inverses, since the inverses of activation functions such as the
sigmoid or the tangent hyperbolic are numerically unstable, we consider projecting on a subset of a(Rdh).
For the hyperbolic tangent, we clip the target to [−1 +ε,1−ε]forε= 10−3. Concretely, for an hyperbolic
tangent activation function, the projection is then π(v) = (min(max( vi,−1 +ε),1−ε))dh
i=1forv∈Rdh. To
read Algo. 2, we recall our notations for θ= (Whh,Wxh,bh,Why,by):
gθ(hτ) =s(Whyhτ+by), (5)
fθ,t(ht−1) =a(Wxhxt+Whhht−1+bh), (6)
f†
θ,t(vt) = (W/latticetop
hhWhh+rI)−1W/latticetop
hh(a−1(π(vt))−Wxhxt−bh). (7)
Note that Algo. 2 can also be used for mini-batches of sequence-output pairs since all operations are either
element-wise or linear with respect to the sample of sequence-output pair.
17Published in Transactions on Machine Learning Research (01/2023)
... ...
 
...
......
...  
Fig. 9: Gradient back-propagation for RNNs.
Algorithm 1 Stochastic learning with target propagation
1:Inputs: Initial parameters θ(0)= (Whh,Wxh,bh,Why,by)of an RNN deﬁned by Eq. (5) and (6), stepsize
γθ, total number of iterations K
2:fork= 1...Kdo
3:Draw a sample or a mini-batch of sequences-output pairs (x1:τ,y).
4:Compute
uθ= (uθh,uθy) =TP(θ(k−1),x1:τ,y),
where TP is Algo. 2
5:Update the parameters as θ(k)=θ(k−1)+γθuθ.
6:end for
B.2 Target Propagation for GRU Networks
B.2.1 Formulation
Starting from h0= 0, given an input sequence x1,...,xτ, the GRU network (as implemented in Py-
torch1(Paszke et al., 2019)), iterates for t= 1,...,τ,
mt=fm,t(ht−1) :=σ(Wimxt+Whmht−1+bm) (8)
zt=fz,t(ht−1) :=σ(Wizxt+Whzht−1+bz) (9)
nt=fn,t(ht−1,mt) := tanh(Winxt+bin+mt⊙(Whnht−1+bhn)) (10)
ht=fh,t(ht−1,zt,nt) := (1−zt)⊙ht−1+zt⊙nt, (11)
where⊙is the Hadamard product, σis the sigmoid function. In the following, we will denote simply
θ= (θm,θz,θn)the parameters of the network with
θm= (Wim,Whm,bm), θz= (Wiz,Whz,bz), θn= (Win,bin,Whn,bhn).
The output of the network is, e.g., a soft-max operation on the hidden state computed at the last step (if
applied to an image scanned pixel by pixel for example). See the main paper for the expression of the output
in that case.
B.2.2 Modifying the Chain Rule
The underlying idea of our implementation of target propagation in a diﬀerentiable programming framework
is to mix classical gradients and Jacobians of the inverse of the functions. Denote for a given output loss L
1Compared to https://pytorch.org/docs/stable/generated/torch.nn.GRU.html , we used a single variable bm=
bim+bhm, same for bz.
18Published in Transactions on Machine Learning Research (01/2023)
Algorithm 2 Proposed target propagation algorithm
1:Parameters: πa projection onto a susbet of a(Rdh), stepsizeγh, regularization r.
2:Inputs: Current parameters θ= (θh,θy)withθh= (Whh,Wxh,bh),θy= (Why,by)of the RNN, sample
of sequences-output pairs (x1:τ,y).
3:Forward Pass:
4:Compute and store V= (W/latticetop
hhWhh+rI)−1W/latticetop
hhgiving access to f†
θ,t(vt)deﬁned in Eq. (7).
5:Initializeh0= 0.
6:fort= 1,...,τdo
7:Compute and store ht=fθ,t(ht−1), ∂θhfθ,t(ht−1), ∂htf†
θ,t(ht).
8:end for
9:Compute and store /lscript(y,gθ(hτ)), ∂∂hτ/lscript(y,gθ(hτ)), ∂θy/lscript(y,gθ(hτ)).
10:Backward Pass:
11:Deﬁneλτ=−γh∂hτ/lscript(y,gθ(hτ)), uθy=−∂θy/lscript(y,gθ(hτ)).
12:fort=τ,..., 1do
13:Computeλt−1=∂htf†
θ,t(ht)/latticetopλt.
14:end for
15:Outputs: Update directions for θh,θy:
uθh=τ/summationdisplay
t=1∂θhfθ,t(ht−1)λt, uθy=−∂θy/lscript(y,gθ(hτ)).
computed on a given mini-batch with the current parameters θ,ˆ∂L/ˆ∂htthe direction back-propagated by
our implementation of target propagation until the step ht. The directions for the parameters of the network
can be output as
ˆ∂L
ˆ∂θ=τ/summationdisplay
t=1∂ht
∂θˆ∂L
ˆ∂ht.
The main task is to deﬁne ˆ∂L/ˆ∂ht−1given ˆ∂L/ˆ∂htand appropriate regularized inverses. For that, we start
with the chain rule for ∂ht/∂ht−1and we will replace some of the gradients by Jacobians of regularized
inverses at some places.
Classical chain rule. We have
∂ht
∂ht−1=/parenleftbigg
−∂zt
∂ht−1/parenrightbigg
diag(ht−1) + I diag(1−zt) +∂zt
∂ht−1diag(nt) +∂nt
∂ht−1diag(zt) (12)
= diag(1−zt) +∂zt
∂ht−1(diag(nt)−diag(ht−1)) +∂nt
∂ht−1diag(zt). (13)
Now for∂nt/∂ht−1, we further decompose the function fn,t(ht−1)asfn,t(ht−1) =gt(mt⊙at),withgt(u) =
tanh(Winxt+bin+u)andat=/lscript(ht−1) :=Whnht−1+bhn. We then have, denoting p=mt⊙at
∂nt
∂ht−1=/parenleftbigg∂mt
∂ht−1diag(at) +∂at
∂ht−1diag(mt)/parenrightbigg
∇gt(p), (14)
with∇gt(p) = diag(tanh/prime(Winxt+bin+p)).
Inverses. Now, the variables zt,mtandatare functions of htthat incorporate a linear operation and that
can be inverted. Namely, we can deﬁne the following regularized inverses
f†
m,t(vt) = (W/latticetop
hmWhm+rI)−1W/latticetop
hm(σ−1(vt)−Wirxt−bm)
f†
z,t(vt) = (W/latticetop
hzWhz+rI)−1W/latticetop
hz(σ−1(vt)−Wizxt−bz)
/lscript†(vt) = (W/latticetop
hnWhn+rI)−1W/latticetop
hn(vt−bhn).
19Published in Transactions on Machine Learning Research (01/2023)
We can then do the following substitutions in Eq.(12) and (14),
∂mt
∂ht−1←ˆ∂mt
ˆ∂ht−1=∂f†
m,t(mt)/latticetop,∂zt
∂ht−1←ˆ∂zt
ˆ∂ht−1=∂f†
z,t(zt)/latticetop,∂at
∂ht−1←ˆ∂at
ˆ∂ht−1=∂/lscript†(at)/latticetop,
to deﬁne the quantity back-propagated by target propagation.
Note that by taking the gradient of the inverse we can ignore the biases and the inputs. Namely, we have
for example ∂f†
m,t(mt) = diag((σ−1)/prime(mt))Whm(W/latticetop
hmWhm+rI)−1,hence
∂f†
m,t(mt)/latticetop= (W/latticetop
hmWhm+rI)−1W/latticetop
hmdiag((σ−1)/prime(mt)).
The expression for ∂f†
z,t(zt)is identical. Since /lscriptis aﬃne, we have simply ∂/lscript†(at)/latticetop= (W/latticetop
hnWhn+rI)−1W/latticetop
hn.
Summary. Combined together, we get, denoting dt=ˆ∂L
ˆ∂ht,
ˆ∂L
ˆ∂ht−1= (1−zt)⊙dt+∂f†
z,t(zt)/latticetop((nt−ht−1)⊙dt)
+∂f†
m,t(mt)/latticetop(at⊙tanh/prime(Winxt+bin+u)⊙zt⊙dt)
+∂/lscript†(at)/latticetop(mt⊙tanh/prime(Winxt+bin+u)⊙zt⊙dt)
= (1−zt)⊙dt+ (W/latticetop
hzWhz+rI)−1W/latticetop
hz/parenleftbig
(σ−1)/prime(zt)⊙(nt−ht−1)⊙dt/parenrightbig
+ (W/latticetop
hmWhm+rI)−1W/latticetop
hm/parenleftbig
(σ−1)/prime(mt)⊙at⊙tanh/prime(Winxt+bin+u)⊙zt⊙dt/parenrightbig
+ (W/latticetop
hnWhn+rI)−1W/latticetop
hn(mt⊙tanh/prime(Winxt+bin+u)⊙zt⊙dt).
This provides a rule to propagate targets through linearized regularized inverses.
B.3 Target Propagation for RNNs for Word Prediction
For the word prediction task, we consider a RNN that outputs a prediction at each time-step. Namely, the
input-output samples consist in two sequences of words (x1:τ,y1:τ)whereytis the word following xtin a
sequence of words. The RNN consists in ﬁrst embedding the input words xtinto a ﬁnite dimensional vector
˜xt=DxtwithD∈Rdx×vwherevis the vocabulary size and dxis the embedding dimension given that x
is represented by a one-hot vector of size v. The RNN treats the embedded input as described in Sec. 2 and
outputs a prediction at each time-step. Namely, the RNN can be summarized as outputting a sequence of
predictions ˆy1:τfrom a sequence of inputs x1:τas
ˆyt=gθ(ht) =s(Whyht+by)fort∈{1,...τ}
ht=fθ,t(ht−1) =a(WxhDxt+Whhht−1+bh)fort∈{1,...τ},
wheresis, e.g., the soft-max function and ais a nonlinear operation such as the hyperbolic tangent function
and we ﬁxed h0= 0.
To implement TP in this case, we consider making one step of gradient descent on the losses of the prediction,
namely, computing
ut=ht−γh∂h/lscript(yt,gθ(ht))fort∈{1,...,τ}.
Then we consider back-propagating the targets as
vt−1=ht−1+∂hf†
θ(ht)/latticetop(vt−ht+ut−ht)fort∈{τ,..., 1}.
starting from vτ=hτsuch that if only hτwas used to predict the output we retrieve the implementation of
TP presented previously. The parameters of the network are then updated as presented in Sec. 2.
From a diﬀerentiable programming viewpoint, the computational scheme of our implementation of TP is
the same as the one of gradient back-propagation once considering the displacements ut=ut−htand
λt=vt−ht, and replacing ∂hfθ,t(ht−1)by∂hf†
θ,t(ht)/latticetopas presented in Fig. 10.
20Published in Transactions on Machine Learning Research (01/2023)
... 
...
......
......
 ... ...
Fig. 10: Computational scheme of TP implemented for RNNs with intermediate outputs.
21Published in Transactions on Machine Learning Research (01/2023)
C Theoretical Insights
C.1 Regularization and Noise Injection
In Section 2, we mentioned how the regularization used in the analytic formulation of the inverse layers
could be interpreted as a counterpart of the noise injection heuristic used to learn parameterized inverses.
We formalize this statement below. Recall that the transition functions take the form
fθ,t(ht) =a(Wxhxt+Whhht+bh). (15)
Perturbed versions of this transition function read
fθ+z,t(ht) =a((Wxh+Zxh)xt+ (Whh+Zhh)ht−1+bh+zh),
forz∼N(0,σ2I)decomposed into independent Gaussian random matrices Zxh,Zhhand random vector zh,
all of which having independent Gaussian random entries with mean zero and variance σ2.
Lemma C.1. Forvt∈a(Rdh)and provided that the activation function ais/lscripta-Lipschitz-continuous, the
regularized inversion f†
θ,t(vt) = (W/latticetop
hhWhh+rI)−1W/latticetop
hh(a−1(vt)−Wxhxt−bh)offθ,tdeﬁned in (15)min-
imizes an upper-bound on the variational formulation of the inverse subject to random perturbations, i.e.,
minvt−1Ez∼N(0,σ2I)/bardblfθ+z,t(vt−1)−vt/bardbl2
2forσ2=r/dh.
Proof.We have for vt∈a(Rdh)anda,/lscripta-Lipschitz-continuous,
/bardblfθ+z,t(vt−1)−vt/bardbl2
2=EZxh,Zhh,zh/bardbla((Wxh+Zxh)xt+ (Whh+Zhh)vt−1+bh+zh)−vt/bardbl2
2
≤/lscriptaEZxh,Zhh,zh/bardbl(Wxh+Zxh)xt+ (Whh+Zhh)vt−1+bh+zh−a−1(vt)/bardbl2
2,
The minimizer of the above upper bound is then
ˆvt−1= argmin
vt−1∈RdhEZxh,Zhh,zh/bardbl(Wxh+Zxh)xt+ (Whh+Zhh)vt−1+bh+zh−a−1(vt)/bardbl2
2
= argmin
vt−1∈RdhEZxh,Zhh,zh/bracketleftBig1
2v/latticetop
t−1(Whh+Zhh)/latticetop(Whh+Zhh)vt−1
−v/latticetop
t−1(Whh+Zhh)/latticetop(a−1(vt)−(Wxh+Zxh)xt−bh−zh)/bracketrightBig
= argmin
vt−1∈Rdh/braceleftbigg1
2v/latticetop
t−1(W/latticetop
hhWhh+E[Z/latticetop
hhZhh])vt−1−v/latticetop
t−1W/latticetop
hh(a−1(vt)−Wxhxt−bh)/bracerightbigg
= (W/latticetop
hhWhh+dhσ2I)−1W/latticetop
hh(a−1(vt)−Wxhxt−bh) =f†
θ,t(vt),
forf†
θ,t(vt)the regularized inversion used in our implementation with r=dhσ2.
If the activation function is the identity, the regularized inversion f†
θ,tis the exact minimizer of the variational
formulation of the inverse subject to random perturbations. If the activation function is not the identity, we
can still quantify the approximation used by using regularized inverses as shown below.
Lemma C.2. Consider an injective and /lscripta-Lipschitz continuous activation function. For vt∈a(Rdh),
denote the objective of the variational formulation of the inverse subject to random perturbations as F(v) =
Ez∼N(0,σ2I)/bardblfθ+z,t(v)−vt/bardbl2
2and denote ˆv=f†
θ,t(vt)the approximate minimizer given by the regularized
inversion for r=dhσ2. We have that
F(ˆv)−min
v∈RdhF(v)≤/parenleftbigg
1−ma,δ
/lscripta/parenrightbigg
F(ˆv) +ma,δεδ,
where, forv∗∈argminv∈RdhF(v), denotingφθ,t(v) =Wxhxt+Whhv+bh, we deﬁned for any δ>0,
ma,δ= inf
u=φθ+z,t(v∗),/bardblz/bardbl2≤δ
w=a−1(vt)/bardbla(u)−a(w)/bardbl2
/bardblu−w/bardbl2, εδ=/integraldisplay
/bardblz/bardbl2≥δ/bardblφθ+z,t(v∗)−a−1(vt)/bardbl2
2e−/bardblz/bardbl2
2/2σ2dz.
such thatma,δ>0sinceais injective and εδvanishing for δ→+∞.
22Published in Transactions on Machine Learning Research (01/2023)
Proof.Decompose the transition function fθ,tasfθ,t=a◦φθ,t, whereφθ,t(v) =Wxhxt+Whhv+bhand
φθ+z,t(v) = (Wxh+Zxh)xt+ (Whh+Zhh)ht−1+bh+zh. Denotev∗∈argminv∈RdhF(v). We have
F(v∗) =/integraldisplay
/bardblz/bardbl2≤δ/bardbla(φθ+z,t(v∗))−vt/bardbl2
2e−/bardblz/bardbl2
2/2σ2dz+/integraldisplay
/bardblz/bardbl2≥δ/bardbla(φθ+z,t(v∗))−vt/bardbl2
2e−/bardblz/bardbl2
2/2σ2dz
≥/integraldisplay
/bardblz/bardbl2≤δ/bardbla(φθ+z,t(v∗))−vt/bardbl2
2e−/bardblz/bardbl2
2/2σ2dz
≥/integraldisplay
/bardblz/bardbl2≤δma,δ/bardblφθ+z,t(v∗)−a−1(vt)/bardbl2
2e−/bardblz/bardbl2
2/2σ2dz
≥ma,δ/integraldisplay
/bardblφθ+z,t(v∗)−a−1(vt)/bardbl2
2e−/bardblz/bardbl2
2/2σ2dz−ma,δεδ
(i)
≥ma,δ/integraldisplay
/bardblφθ+z,t(ˆv)−a−1(vt)/bardbl2
2e−/bardblz/bardbl2
2/2σ2dz−ma,δεδ
≥ma,δ/lscript−1
aF(ˆv)−ma,δεδ,
where in (i)we used that ˆv= argminvEz/bardblφθ+z,t(v)−a−1(vt)/bardbl2
2and we deﬁned
ma,δ= inf
u=φθ+z,t(v∗),/bardblz/bardbl2≤δ
w=a−1(vt)/bardbla(u)−a(w)/bardbl2
/bardblu−w/bardbl2, εδ=/integraldisplay
/bardblz/bardbl2≥δ/bardblφθ+z,t(v∗)−a−1(vt)/bardbl2
2e−/bardblz/bardbl2
2/2σ2dz.
Lemma C.2 shows that if the solution of the variational formulation of the inverse subject to random pertur-
bations is in a region of the activation function where the activation function is nearly the identity, i.e., such
thatma,δ≈1for, e.g.,δ= 3σsuch thatεδ/lessmuch1, then the regularized inversion of the transition function
we propose nearly minimizes the objective of the variational formulation of the inverse subject to random
perturbations.
C.2 Gradient Back-Propagation vs Target Propagation
Lemma 3.1. The diﬀerence between the oracle returned by gradient back-propagation ∂θh/lscript(y,Fθ(x1:τ))and
the oracle returned by target propagation uθhcan be bounded as
/bardbl∂θh/lscript(y,Fθ(x1:τ))−uθh/bardbl≤τ/summationdisplay
t=1ct/bardbl∂θhfθ,t(ht−1)/bardbl/bardbl∂hτ/lscript(y,gθ(hτ))/bardblsup
t=1,...,τ/bardbl∂ht−1fθ,t(ht−1)−∂htf†
θ,t(ht)/latticetop/bardbl,
wherect=/summationtextt−1
s=0asbt−1−switha= supt=1,...τ/bardbl∂ht−1fθ,t(ht−1)/bardbl,b= supt=1,...τ/bardbl∂htf†
θ,t(ht)/latticetop/bardbl.
For regularized inverses, we have, denoting zt=Wxhxt+Whhht−1+bh,
/bardbl∂ht−1fθ,t(ht−1)−∂htf†
θ,t(ht)/latticetop/bardbl≤/bardblW/latticetop
hh/bardbl/parenleftBig
/bardbl∇a(zt)−∇a(zt)−1/bardbl+/bardblI−(W/latticetop
hhWhh+rI)−1/bardbl/bardbl∇a(zt)−1/bardbl/parenrightBig
.
Proof.The ﬁrst claim is a direct application of Lemma C.3 and the second claim follows from the formulation
of the regularized inverse, using that ∇a−1(ht) =∇a(a−1(ht))−1=∇a(zt)−1.
Lemma C.3. GivenA1,...,An,B1,...,Bn∈Rp×p, for any sub-multiplicative matrix norm /bardbl·/bardbl, and any
1≤t≤n,/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublet/productdisplay
i=1Ai−t/productdisplay
i=1Bi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤δt−1/summationdisplay
i=0aibt−1−i,
wherea= supi=1,...,n/bardblAi/bardbl,b= supi=1,...n/bardblBi/bardblandδ= supi=1,...,n/bardblAi−Bi/bardbl.
23Published in Transactions on Machine Learning Research (01/2023)
Proof.Deﬁne fort≥1,δt=/bardbl/producttextt
i=1Ai−/producttextt
i=1Bi/bardbl, we have
δt≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleAt/parenleftBiggt−1/productdisplay
i=1Ai−t−1/productdisplay
i=1Bi/parenrightBigg
+ (At−Bt)t−1/productdisplay
i=1Bi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤aδt−1+δbt−1≤δt−1/summationdisplay
i=0aibt−1−i.
A convergence to a stationary point for TP can be derived from classical results on an approximate gradient
descent detailed below (the proof is akin to the results of Devolder et al. (2014)).
Corollary C.4 (Corollary of Lemma C.5) .Denoteεka bound on the diﬀerence between the oracle returned
by gradient back-propagation and by target-propagation both applied to the whole dataset. Provided that the
objective is L-smooth and the stepsizes of TP are chosen such that γ=γhγy<1/L, afterKiterations, we
get
min
k∈{0,...,K−1}/bardbl∇L(θ(k))/bardbl2
2≤2/parenleftbig
L(θ(0))−minθ∈RdL(θ)/parenrightbig
γK) +1
KK−1/summationdisplay
k=0ε2
k.
whereL(θ) =1
n/summationtextn
i=1/lscript(Fθ(x1:τ,i),yi)withFθ(x1:τ,i)the output of the RNN on a sample (x1:τ,i,yi)and/lscriptthe
chosen loss.
Lemma C.5. Letf:Rp→Rbe anL-smooth function. Consider an approximate gradient descent on f
with step size 0≤γ≤1/L, i.e.,θ(k+1)=θ(k)−γ/hatwide∇f(θ(k)),where/bardbl/hatwide∇f(θ(k))−∇f(θ(k))/bardbl2≤εk. AfterK
iterations, this method satisﬁes,
min
k∈{0,...,K−1}/bardbl∇f(θ(k))/bardbl2
2≤2(f(θ(0))−minθ∈Rpf(θ))
γK+1
KK−1/summationdisplay
k=0ε2
k.
Proof.Denoteg(k)=/hatwide∇f(θ(k))−∇f(θ(k))for allk≥0. ByL-smoothness of the objective, the iterations of
the approximate gradient descent satisfy, using in (i)thatLγ≤1,
f(θ(k+1))≤f(θ(k)) +∇f(θ(k))/latticetop(θ(k+1)−θ(k)) +L
2/bardblθ(k+1)−θ(k)/bardbl2
2
=f(θ(k))−γ/bardbl∇f(θ(k))/bardbl2
2−γ∇f(θ(k))/latticetopg(k)+Lγ2
2/bardbl∇f(θ(k)) +g(k)/bardbl2
2
=f(θ(k))−γ/parenleftbigg
1−Lγ
2/parenrightbigg
/bardbl∇f(θ(k))/bardbl2
2+Lγ2
2/bardblg(k)/bardbl2
2+γ(Lγ−1)∇f(θ(k))/latticetopg(k)
(i)
≤f(θ(k))−γ/parenleftbigg
1−Lγ
2/parenrightbigg
/bardbl∇f(θ(k))/bardbl2
2+Lγ2
2/bardblg(k)/bardbl2
2+γ(1−Lγ)/bardbl∇f(θ(k))/bardbl2/bardblg(k)/bardbl2
≤f(θ(k))−γ/parenleftbigg
1−Lγ
2/parenrightbigg
/bardbl∇f(θ(k))/bardbl2
2+Lγ2
2/bardblg(k)/bardbl2
2+γ(1−Lγ)
2(/bardbl∇f(θ(k))/bardbl2
2+/bardblg(k)/bardbl2
2)
≤f(θ(k))−γ
2/bardbl∇f(θ(k))/bardbl2
2+γ
2/bardblg(k)/bardbl2
2.
Summing from k= 0toK−1and rearranging the terms, we get
K−1/summationdisplay
k=0/bardbl∇f(θ(k))/bardbl2
2≤2(f(θ(0))−minθ∈Rpf(θ))
γ+K−1/summationdisplay
k=0ε2
k.
Taking the minimum of /bardbl∇f(θ(k))/bardbl2
2and dividing by Kwe get the result.
24Published in Transactions on Machine Learning Research (01/2023)
C.3 Target Propagation vs Gauss-Newton updates
We discuss the interpretation of Target Propagation (TP) as a Gauss-Newton (GN) method which was
proposed by Bengio (2020) and Meulemans et al. (2020). As already mentioned in Sec. 3, the main similarity
between TP and GN is the fact that both TP and GN use the inverse of the gradients or approximations
thereof. In this section, we shall discuss this interpretation for feed-forward networks to follow the claims
of Meulemans et al. (2020). Namely, we consider here a network deﬁned by LweightsW1,...,WLandL
activation functions a1,...,aLwhich transform an input x0into an output xLby computing (no biases were
considered by Meulemans et al. (2020)),
xt=ft(xt−1) =at(Wtxt−1)fort∈{1,...,L}
Denotingφ(x,θ)the output of the network for an input x=x0, withθ= (W1,...,WL)being the parameters
of the network, the objective consists in minimizing the loss between the outputs of the network and the
sample outputs, i.e., minimizing L(y,φ(x,θ))for pairs of input-output samples (x,y).
GN step. Recall ﬁrst the rationale of a GN step for such feed-forward networks with a squared-loss, which
amount to solving
min
θ∈Rp1
nn/summationdisplay
i=1/bardblφ(xi,θ)−yi/bardbl2
2.
A GN step amounts to linearize the non-linear function φaround a current set of parameters θ(k)and solve
the corresponding least-square problems to deﬁne the next set of parameters, i.e,
θ(k+1)= argmin
θ1
nn/summationdisplay
i=1/bardblφ(xi,θ(k)) +∂θφ(xi,θ(k))/latticetop(θ−θ(k))−yi/bardbl2
2
=θ(k)−/parenleftBiggn/summationdisplay
i=1∂θφ(xi,θ(k))∂θφ(xi,θ(k))/latticetop/parenrightBigg−1/parenleftBiggn/summationdisplay
i=1∂θφ(xi,θ(k))/parenleftBig
φ(xi,θ(k))−yi/parenrightBig/parenrightBigg
.
To consider TP as an approximate GN method we need the following considerations.
1. Consider the iteration on a mini-batch of size 1, s.t.
θ(k+1)=θ(k)−/parenleftBig
∂θφ(xi,θ(k))∂θφ(xi,θ(k))/latticetop/parenrightBig−1/parenleftBig
∂θφ(xi,θ(k))/parenleftBig
φ(xi,θ(k))−yi/parenrightBig/parenrightBig
.
2. Consider that the gradients of the networks are invertible, s.t.
θ(k+1)=θ(k)−/parenleftBig
∂θφ(xi,θ(k))/parenrightBig−/latticetop/parenleftBig
φ(xi,θ(k))−yi/parenrightBig
.
3. Consider updating only one set of parameters θl=Wl, s.t.,
θ(k+1)
l=θ(k)
l−/parenleftBig
∂θlφ(xi,θ(k))/parenrightBig−/latticetop/parenleftBig
φ(xi,θ(k))−yi/parenrightBig
.
with
∂θlφ(xi,θ(k)) =∂θlfl(xl−1)∂xfl+1(xl)...∂xfL(xL−1)
so that, provided that all matrices inside the matrix multiplication are invertible, we get
∂θlφ(xi,θ(k))−T=∂θlfl(xl−1)−T∂xfl+1(xl)−T...∂xfL(xL−1)−T
4. Finally, ignore the last inversion and replace it by the gradient on the parameters θl, then we get an
iteration similar to TP, with
θ(k+1)
l=θ(k)
l−∂θlfl(xl−1)∂xfl+1(xl)−T...∂xfL(xL−1)−T∂xLL(y,xL)
forLa squared loss. Namely, we keep the inversion of the gradients of the intermediate functions.
Our objective here is to question whether viewing TP as a GN step with the approximations explained above
is meaningful or not.
25Published in Transactions on Machine Learning Research (01/2023)
Does the original TP formulation approximate GN? Meulemans et al. (2020) start by considering
the original TP formulation, i.e., targets computed as vt=ψt(vt+1)forψtan approximate inverse of ftand
withvL=xL−η∂xL/lscript(y,xL). Meulemans et al. (2020, Lemma 1) show then that, provided that we use the
exact inverse, ψt=f−1
t,
∆xt=vt−xt=−ηL−1/productdisplay
s=t∂xsfs+1(xs)−/latticetop∂xL/lscript(y,xL) +O(η2).
(Meulemans et al., 2020, Theorem 2) conclude that (i) for mini-batches of size 1, (ii) for a squared loss, (iii)
for invertible ft, asη→0, TP uses a Gauss-Newton optimization with block diagonal approximation to
compute the targets in the sense that as η→0,
∆xt≈−η∂xt(ft+1◦...◦fL)−/latticetop(xt).
As the stepsize of any optimization algorithm tends to 0, they all are the same, since the update would be 0
in all cases. To make the claim of Meulemans et al. (2020) more precise, the constants hidden in O(η2)need
to be detailed in order to understand in which regimes of the stepsize the approximation is meaningful.
Assuming the inverses ψtto be/lscriptψLipschitz continuous and Lψ-smooth (i.e. with Lψ-Lipschitz continuous
gradients), a quick look at the proof of Lemma 1 of Meulemans et al. (2020) shows that
vt−xt=−ηL−1/productdisplay
s=t∂xsfs+1(xs)−/latticetop∂xL/lscript(y,xL) +ξt
/bardblξt/bardbl2≤δt
δs≤Lψδ2
s+1+/lscriptψδs+1+Lψ/lscript2
ψη2/bardbl∂xL/lscript(y,xL)/bardbl2
2fors∈{t,...,L−1}
δL≤Lψ
2η2/bardbl∂xL/lscript(y,xL)/bardbl2
2.
Hence the approximation error is then of the order of /bardblξt/bardbl2≤Lψ/bardbl∂xL/lscript(y,xL)/bardbl2
2(/summationtextL−t
s=0/lscripts
ψ)η2+o(η2), that is
the approximation error may be valid for η≤(Lψ/bardbl∂xL/lscript(y,xL)/bardbl2
2(/summationtextL−t
s=0/lscripts
ψ))−1/2. Yet, in practice, TP does
not appear to use very small stepsizes. Moreover, if the similarity of TP with GN could explain its eﬃciency,
then by the reasoning of Meulemans et al. (2020), the original TP formulation should be eﬃcient. Yet, the
original TP formulation has never been shown to produce satisfying results.
Does TP with the diﬀerence target propagation approximate GN? Meulemans et al. (2020) make
a similar claim for TP with the Diﬀerence Target Propagation formula, i.e., vt=xt+ψt(vt+1)−ψt(xt+1).
Namely, Meulemans et al. (2020, Lemma 3) show that
∆xt=vt−xt=−ηL−1/productdisplay
s=t∂xsψs(xs)/latticetop∂xL/lscript(y,xL) +O(η2).
Once again, for the claim to be meaningful beyond inﬁnitesimal stepsizes, the terms in O(η2)need to be
detailed. If we use a linearized version of the diﬀerence target propagation formula as presented in (2),
namelyvt−xt=∂xt+1ψt(xt+1)/latticetop(vt+1−xt+1), then we have the equality
∆xt=vt−xt=−ηL−1/productdisplay
s=t∂xsψs(xs)/latticetop∂xL/lscript(y,xL)
and the idea that TP could be seen as an approximate GN method may be pursued in a meaningful way.
However the error of approximation of the inverse of the gradients must be taken into account in order to
understand the validity of the approach.
26Published in Transactions on Machine Learning Research (01/2023)
Propagating the approximation error of the gradient inverses. We compute the approximation
error incurred by composing gradients of the inverse instead of inverses of gradients. Formally, the approxi-
mation error for one layer can be estimated under the assumption that
ψt(ft(xt−1)) =xt−1+e(xt−1), (16)
witheanε-Lipschitzcontinuousfunctionandtheassumptionthattheminimalsingularvalue σof∂xtft(xt−1)
is positive.
The function ea priori depends on the parameters of the layer; we ignore this dependency and simply
considereto beε-Lipschitz continuous for all θ. For a function e, its Lipschitz continuity constant is
ε= supxsup/bardblλ/bardbl2≤1/bardbl∂xe(x)/latticetopλ/bardbl2= supx/bardbl∂xe(x)/bardbl,where/bardbl·/bardbldenotes the spectral norm. By diﬀerentiating
both sides of Eq. (16), we get
∂xft(xt−1)∂xψt(xt) = I +∂xe(xt−1).
By assuming the minimal singular value σof∂xft(xt−1)to be positive, we get that ∂xft(xt−1)is invertible
and so
∂xψt(xt) =∂xft(xt−1)−1(I +∂xe(xt−1)).
Hence∂xψt(xt)isσ−1(1 +ε)Lipschitz-continuous and
/bardbl(∂xft(xt−1))−1−∂xψt(xt)/bardbl≤ε
σ. (17)
Now for multiple compositions, using Lemma C.3, we get
/bardbl(∂xf1(x0))−1...(∂xfL(xL−1))−1−∂xψ1(x1)...∂xψL(xL)/bardbl≤(1 +ε)L
σL.
Therefore the accumulation error diverges with the length Lof the network as soon as ε≥σ−1.
Testing the hypothesis that TP could be interpreted as using GN updates directions. Here
we come back to the setting of RNNs presented in the paper. In this case the length of the compositions
of layers is τand according to the previous discussion, the error of approximation of the product of the
inverse of the gradients by the product of the gradients of the approximate inverses could easily diverge as τ
grows (long sequences). Nevertheless, by using analytical formulas for the inverses, we can ensure that the
approximation error is zero, which would correspond then to the ideal setting where TP uses GN update
directions for the hidden states.
Formally, inthecontextofRNNs, aGauss-Newtonupdatedirectionforthehiddenstatesisgivenas(ignoring
the inverse of the output function)
−γhτ−1/productdisplay
s=t+1(∂hft+1,θ(ht))−/latticetop∂h/lscript(y,gθ(hτ)),
If no regularization is used in the deﬁnition of the regularized inverse, i.e., if we use
f−1
θ,t(ht) = (W/latticetop
hhWhh)−1W/latticetop
hh(a−1(ht)−Wxhxt−bh),
which requires the inverse of Whhto be well deﬁned, we would get
∂f−1
θ,t(ht) =∂hft+1,θ(ht)−1.
The updates of TP using the formula (2) would then be exactly the ones of a GN on the hidden states as
presented by Meulemans et al. (2020), i.e.,
vt−ht=−γhτ−1/productdisplay
s=t+1(∂hft+1,θ(ht))−/latticetop∂h/lscript(y,gθ(hτ)).
So by considering our implementation without regularization, we can test whether the interpretation of TP
as an approximate GN method is meaningful in terms of optimization convergence. As shown in Fig. 7, it
appears that regularizing the inverses is necessary to obtain convergence, hence the interpretation of TP as
GN may not be suﬃcient to explain why TP can converge.
27Published in Transactions on Machine Learning Research (01/2023)
BP TP
γ γhγθκ
Temporal order problem length 60 10−510−210−110
Temporal order problem length 120 10−510−210−21
Adding problem 10−310−110−11
MNIST pixel by pixel 10−610−410−11
MNIST pixel by pixel permuted 10−410−410−11
CIFAR 10−310−210−210
FashionMNIST with GRU 10−210−110−21
Penn Treebank with RNN 10−210−310−11
Table 1: Hyper-parameters chosen for Fig. 3 and 4.
D Experimental Details
D.1 Initialization and Hyper-Parameters
Initializationanddatageneration. Inallexperiments, theweightsoftheRNNareinitializedasrandom
orthogonal matrices, and the biases are initialized as 0 as presented by Le et al. (2015) and Manchev &
Spratling (2020). For all experiments, the data was not normalized, as done by Manchev & Spratling (2020).
We kept a setting as similar as possible as the one of Manchev & Spratling (2020) to be able to compare
target propagation with regularized or parameterized inverses.
Hyper-parameters. In the synthetic tasks, for BP we used a momentum of 0.9with Nesterov accelerated
gradient scheme as done by Manchev & Spratling (2020). Otherwise, we did not use any momentum for
the experiment on MNIST pixel by pixel presented in the main paper. The learning rates of BP and the
parameters of TP were found by a grid-search on a log10basis and are presented in Table 1. We did not add
a regularization term in the training of the RNNs.
For Fig. 7, we used batch sizes of size 512 and performed a grid search for the stepsizes of BP and for the
stepsizesγhof TP while keeping the same regularization rand stepsize γθto the parameters found for the
length 784.
Software. We used Python 3.8 and PyTorch 1.6. The RNN was coded using the cuDNN implementation
available in PyTorch that is highly optimized for computing forward-backward passes on RNNs for gradient
back-propagation.
Hardware. All experiments were performed on GPUs using Nvidia GeForce GTX 1080 Ti (12G memory).
Each experiment only used one gpu at a time (clock speed 1.5 Ghz).
Time evaluation. On our GPU, we observed that for the MNIST pixel by pixel experiment, 200 iterations
(each iteration considering 16 samples) were taking approximately 60s for BP and 800s for TP. Note that
with larger batch-sizes the cost of the regularized inversion would be amortized by the fact that more samples
are treated simultaneously. We kept the setting of Manchev & Spratling (2020) for ease of comparison.
28Published in Transactions on Machine Learning Research (01/2023)
0 2 4
Iterations×104020004000 Train Loss
0 2 4
Iterations×10420004000
0 1 2
Iterations×104050100
0 2 4
Iterations×1042000400060008000
0 2 4
Iterations×1042000400060008000
0 2 4
Iterations×1040255075100 Accuracy
0 2 4
Iterations×1040255075100
0 1 2
Iterations×1040255075100
0 2 4
Iterations×1040255075100
0 2 4
Iterations×1040255075100TP BP DTP-PI
Fig. 11: Comparison of our implementation of TP denoted TP, against TP with a Diﬀerence Target Propa-
gation formula and Parameterized Inverses, denoted DTP-PI and gradient Back-Propagation, denoted BP.
From Left to right: Temporal order task with T= 60, Temporal order task with T= 120, Adding problem
withT= 30, MNIST pixel by pixel and MNIST pixel by pixel with permuted images.
D.2 Additional Experiments
Comparison of TP with regularized or parameterized inverses. In Fig. 11, we evaluate the perfor-
mance of our implementation of TP using regularized inverses and linearized propagation as opposed to the
implementation of Manchev & Spratling (2020) using the diﬀerence target propagation formula and parame-
terized inverses on all datasets studied by Manchev & Spratling (2020) using the reported hyper-parameters
chosen by Manchev & Spratling (2020). We observe that our implementation generally outperforms the one
of Manchev & Spratling (2020) except for the addition task where they perform on-par. As observed in
Fig. 5, the diﬀerence of performance can be explained by the use of regularized inverses while the linearized
formulation used in our implementation has the advantage to easily adapt TP for other architectures.
Visualization of norms of targets. On Fig. 12, we observe that the norm of the displacements λt=
vt−htdeﬁned in Sec. 3 propagated by TP do not necessarily enjoy more stable norms than the gradients
∂/lscript/∂htpropagated by BP as explained in Appendix A for tvarying along the length of the sequence. This
phenomenon is in contrast with the better performance of TP over BP for this task, namely, predicting
images from MNIST with RNNs and suggests interesting avenues for future research.
Gradient norms and spectral radius analyses. On Fig. 13, we present, as done by Manchev &
Spratling (2020, Figure 3) for TP with parameterized inverses, the evolution of (i) the norm of the ora-
cle directions computed by either TP or BP for all transition parameters θh, (ii) the spectral radius of the
transition matrix Whh, on the MNIST pixel by pixel experiment. We observe that TP provides smaller
oracle direction norms which allowed for larger stepsizes with slightly less variance of the gradients. On the
other hand the spectral radius of the transition matrix appears to grow for both oracles with TP having a
larger growth at the start.
TP vs BP in terms of time. To account for the additional cost of inversion for each mini-batch, we
consider the convergence of the algorithms in time rather than in iterations. We found that, on average,
1 iteration of BP takes approximately 13 times less time than one iteration of TP in our implementation
(note that BP beneﬁts from highly optimized implementations for GPU machines, and TP could potentially
also beneﬁt from the same optimized implementations). Therefore we ran BP for 13 times more iterations
than TP and multiplied the number of iterations by the approximate time needed for each iteration for all
29Published in Transactions on Machine Learning Research (01/2023)
756
714
672
630
588
546
504
462
420
378
336
294
252
210
168
126
84
42
0
Sequence Depth0
400
800
1200
1600
2000
2400
2800
3200
3600
4000Iterations
10−1210−1010−810−610−410−2TP
756
714
672
630
588
546
504
462
420
378
336
294
252
210
168
126
84
42
0
Sequence Depth0
400
800
1200
1600
2000
2400
2800
3200
3600
4000Iterations
0.050.100.150.20BP
Fig. 12: Norms of displacements or gradients along the sequence of pixels analyzed by an RNN to predict
MNIST digits.
0 5000 10000
Iterations10−1101Oracle Dir. NormTP BP
0 5000 10000
Iterations1.001.021.041.06Spectral RadiusTP BP
Fig. 13: Norms of oracle directions and spectral radii along iterations.
0 2 4
Time in s×1040255075100 AccuracyTP BP Fig.14: MNISTpixelbypixelintime.
algorithms. In Fig. 14, we observe that in time too, TP performs better than BP, which stays stuck at an
accuracy of approximately 22%.
30