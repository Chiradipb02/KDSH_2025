Under review as submission to TMLR
Approximation, Estimation and Optimization Errors for a
Deep Neural Network
Anonymous authors
Paper under double-blind review
Abstract
The error of supervised learning is typically split into three components: approximation,
estimation and optimization errors. While all three have been extensively studied in the
literature, a unified treatment is less frequent, in part because of conflicting assumptions.
Current approximation results rely on carefully hand crafted weights or practically unavail-
able information, which are difficult to achieve by gradient descent. Optimization theory is
best understood in over-parametrized regimes with more weights than samples, while clas-
sical estimation errors require the opposite regime with more samples than weights. This
paper contains two results which bound all three error components simultaneously for deep
fully connected networks. The first uses a regular least squares loss and shows convergence
in the under-parametrized regime. The second uses a kernel based loss function and shows
convergence in both under and over-parametrized regimes.
1 Introduction
In this paper, we consider supervised learning of fully connected neural networks without bias: For network
fθwith weights θand normalized training samples (xi,yi)on thed-dimensional sphere, we minimize the loss
ℓ(θ) =1
2NN/summationdisplay
i=1|fθ(xi)−yi|2
by gradient descent. We also consider alternative losses, which allow more flexibility with regard to the num-
ber of samples and network size. The main results provide a complete error analysis including approximation
errors,estimation errors andoptimization errors (Shalev-Shwartz & Ben-David, 2014).
Overview
•Approximation Error: If the data points yi=f(xi)are generated by some unknown target function
f, how well can the network approximate it, i.e. how large is the error infθ∥fθ−f∥L2(D)on some
domainD, ignoring error contributions from sampling and optimization algorithms? Typical results
establish bounds
inf
θ∥fθ−f∥L2(D)≤cn(θ)−r, f ∈K, (1)
for some rate r >0, with an asymptotic rate n(θ)−r, wheren(θ)is a complexity measure of the
network, e.g. width, depth or total number of weights. Any quantifiable rate requires some prior
conditions on f, here given by membership in a compact set K, which typically consists of functions
with bounded Sobolev, Besov, Barron or other smoothness norm.
First results show universal approximation properties (Cybenko, 1989; Hornik et al., 1989; Barron,
1993; Zhou, 2020; Lu et al., 2017; Hanin & Sellke, 2017) and that neural networks can reproduce
classical approximation rates for targets with Sobolev and Besov regularity (Gribonval et al., 2022;
1Under review as submission to TMLR
Gühring et al., 2020; Opschoor et al., 2020; Li et al., 2019; Suzuki, 2019). More recent papers pro-
vide super-convergence results, where networks outperform classical methods, as well as optimality
benchmarks like manifold width, for the price of discontinuous weight assignments (Yarotsky, 2017;
2018; Yarotsky & Zhevnerchuk, 2020; Daubechies et al., 2022; Shen et al., 2019; Lu et al., 2021).
Approximation results with smoothness requirements more tailored to neural networks use Barron
and related spaces (Bach, 2017; Klusowski & Barron, 2018; Weinan et al., 2022; Li et al., 2020; Siegel
& Xu, 2020; 2022a; Bresler & Nagaraj, 2020). Several surveys are given in (Pinkus, 1999; DeVore
et al., 2021; Weinan et al., 2020; Berner et al., 2022).
In the above results, the weights are not trained by practical optimizers, but rather hand-picked
or sampled from practically unknown distributions. Practical networks do not always achieve the
theoretical bounds Adcock & Dexter (2021); Grohs & Voigtlaender (2023) The papers Jentzen &
Riekert (2022); Ibragimov et al. (2022); Gentile & Welper (2022); Welper (2024b;a) are closely
related to the results in this paper and provide an analysis of approximation errors in combination
with gradient descent training. Finally Siegel & Xu (2022b); Siegel et al. (2023); Beck et al. (2022);
Herrmann et al. (2022) consider approximation or estimation with alternative optimizers like greedy
algorithms.
•Generalization and Estimation errors: Practically, one can neither evaluate nor optimize the L2(D)
error directly, and therefore trains the sample or empirical loss, resulting in the empirical risk mini-
mizer ˆθ. Theresultingexpectedlossiscalledthe generalization error andsplitintothe approximation
errorand a remaining estimation error , (Shalev-Shwartz & Ben-David, 2014):
∥fˆθ−f∥2
L2(D)= inf
θ∥fθ−f∥2
L2(D)+/parenleftbigg
∥fθ−f∥2
L2(D)−inf
θ∥fθ−f∥2
L2(D)/parenrightbigg
.
Therearemanytechniquestoboundgeneralizationandestimationerrors,forexamplebyestablishing
uniform bounds between the expected and sample loss
sup
θ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∥fθ−f∥2
L2(D)−1
2NN/summationdisplay
i=1|fθ(xi)−f(xi)|2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≲C+N−1/2(2)
for some complexity measure Cof the neural networks like VC-dimension or Rademacher complexity.
VC-dimension bounds of the form VC-dim ≲˜O(Ln(θ))for total number of weights n(θ)and depth
Lare in Neyshabur et al. (2017); Bartlett et al. (1998); Harvey et al. (2017). Bounds for the
Rademachercomplexitytendtobeindependentofthesizeofthenetworkase.g. O/parenleftbigg√
L/producttextL
ℓ=1∥Wℓ∥F√
N/parenrightbigg
for weightWℓin the Frobenius norm from Golowich et al. (2018). Similar bounds are in Neyshabur
etal.(2015);Liangetal.(2019);Tuetal.(2020). Whilethenorm ∥Wℓ∥Fmaygrowforwidenetworks
with standard scaling, newer results depend on the difference ∥Wℓ−Wℓ
0∥to some reference or initial
weightsWℓ
0in Frobenius and other matrix norms, which tends to be small in the over-parametrized
limit and leads to some combined generalization and gradient descent convergence results (Cao &
Gu, 2020). Other Rademacher complexity bounds rely on smoothness (Weinan et al., 2022; 2019).
Further techniques include margin theory (Jakubovitz et al., 2019; Neyshabur et al., 2018; Bartlett
et al., 2017) mutual information (Asadi et al., 2018; Steinke & Zakynthinou, 2020) compression
(Arora et al., 2018) and Besov regularity (Suzuki, 2019). Empirical observations show that against
conventional wisdom neural networks generalize well in over-parametrized regimes, with enough
weights to fit random data, (Neyshabur et al., 2017; Zhang et al., 2017; Geiger et al., 2019).
•Optimization Error: Both the empirical loss minimizer ˆθ= argminℓ(θ)and the approximation error
infθ∥fθ−f∥2
L2(D)build on an optimization problem. Since these are non-convex, can we practically
compute the minimizers, or at least a sufficiently good substitute? This question is addressed in the
optimization literature for neural networks. The approach in this paper relies on the neural tangent
kernel (NTK) coined in Jacot et al. (2018) and introduced simultaneously in Li & Liang (2018);
Allen-Zhu et al. (2019); Du et al. (2019b;a). The concept is further developed in Zou et al. (2020);
Arora et al. (2019a;b); Su & Yang (2019); Lee et al. (2019); Song & Yang (2019); Zou & Gu (2019);
2Under review as submission to TMLR
Kawaguchi & Huang (2019); Chizat et al. (2019); Oymak & Soltanolkotabi (2020); Ji & Telgarsky
(2020); Nguyen & Mondelli (2020); Bai & Lee (2020); Chen et al. (2021); Song et al. (2021); Lee
et al. (2022); Gentile & Welper (2022); Welper (2024b;a). In this paper we use lower bounds for
the NTK that originate from Bietti & Mairal (2019); Geifman et al. (2020); Ji et al. (2020); Chen
& Xu (2021). The optimization literature contains may other approaches that we only mention
briefly: Landscape analysis (Nguyen & Hein, 2017; Ge et al., 2018; Du & Lee, 2018; Soltanolkotabi
et al., 2019; Venturi et al., 2019), Wasserstein gradient flow (Soudry & Carmon, 2016; Safran &
Shamir, 2018; Chizat & Bach, 2018; Mei et al., 2018; Rotskoff & Vanden-Eijnden, 2018; Sirignano
& Spiliopoulos, 2020), as well as several overviews (Weinan et al., 2020; Berner et al., 2022; Roberts
et al., 2022).
•Generalization and Optimization: The definition of stochastic gradient descent intertwines opti-
mization with sampling and the literature provides positive effects on generalization. This can be
analyzed in abstract settings, e.g. the paper Hardt et al. (2016) bounds the generalization error
between empirical and expected risk of SGD iterates of general convex and non-convex objective
functions. In addition, it bounds the expected loss between SGD and empirical risk minimizers in
the convex case.
Other papers are more specific to neural networks, i.e. Cao & Gu (2020); Nitanda & Suzuki (2021)
consider a combination of estimation and optimization in over-parametrized regimes. The latter
proves generalization errors for stochastic gradient descent by comparing its dynamics with an NTK
idealization, yielding minimax optimal SGD convergence rates. In addition, it does not require any
lower bounds on the NTK eigenvalues, similar to Welper (2024b) and the current paper. The papers
Wang & Ma (2023); Liu et al. (2022); Park et al. (2022); Neu et al. (2021) find improved estimation
bounds by explicitly incorporating gradient descent dynamics. Another closely related set of results
is Drews & Kohler (2022); Kohler & Krzyzak (2022), which consider all three error contributions
and control the optimization error based on the contributions of the final layer.
Since the approximation question considers continuous norms, corresponding to infinite sample limits, they
are under-parametrized and therefore difficult to reconcile with the optimization literature. Indeed, the
majority of the approximation references above uses hand-picked weights or practically unavailable infor-
mation and is not trained by practical optimizers. The optimization and related generalization literature,
usually considers severely over-parametrized regimes, which aids their analysis. E.g., in this regime gradient
descent dynamics is comparable to linearised models like the NTK. Practical networks tend to be much less
over-parametrized and therefore allow more nonlinear behaviour.
New Contributions This paper provides a unified analysis of all three error components and extends the
NTK theory from the well studied over-parametrized regime to under-parametrized regimes for one result
and the full spectrum from under- to over-parametrized for another. For all results, we train the second but
last (non-convex) layer of fully connected deep networks without bias of constant depth and varying width
m. For this problem, the prior work Gentile & Welper (2022); Welper (2024b;a) establishes approximation
and optimization error bounds. The new contribution is a corresponding analysis of the estimation errors.
Estimation errors require us to bound some complexity measure of the neural networks fθ, for which we
consider two alternative approaches.
1. The most common complexity measures are VC-dimension and Rademacher complexity, which in
turn can be bounded by chaining techniques, i.e. by Dudley’s inequality (Shalev-Shwartz & Ben-
David, 2014). In our case, it is convenient to skip the Rademacher complexity and use Dudley’s
inequality directly because it has already been used to establish NTK concentration inequalities for
the approximation and optimization error bounds (Welper, 2024b).
We minimizes the sample loss
ℓ(θ) =1
2N/summationdisplay
i=1|fθ(xi)−f(xi)|2(3)
3Under review as submission to TMLR
forNuniformly random normalized samples xion the unit sphere Sd−1with gradient descent. We
show that as long as the error does not satisfy the approximation and estimation error estimate
∥fθ−f∥2
L2(Sd−1)≲m−a+mbN−c,
with network width m, the gradient descent error decreases exponentially. The rates a,bandcare
specified in Theorem 2.2 below and depend on the Sobolev smoothness of the target function f.
Although we optimize the discrete sample loss, we bound the error in the continuous L2(Sd−1)norm
and therefore obtain the expected or generalization error. The result is comparable to standard
machine learning theory. In particular, the second term requires that the number of samples Nis
larger than the width mof the network (up to some power).
2. While requiring more samples Nthan width mmatches common wisdom in machine learning the-
ory, it does not explain the empirical observation that neural networks generalize well in over-
parametrized regimes. To establish generalization error bounds in this regime, we rely on a different
complexity measure: The approximation and optimization results in Welper (2024b;a) establish that
the Sobolev norm of the gradient descent iterates ∥fθn−f∥Hs(Sd−1)remain uniformly bounded, in-
dependent of the size of the network. If s>1 +d/2, Sobolev embedding theorems imply that fθis
uniformly Lipschitz and therefore the estimation error bound (2) can be proven by uniform laws of
large numbers (Vershynin, 2018, Section 8.2).
Unfortunately, the current theory provides only bounds for s <1/2, insufficient for the argument.
We may, however, proceed with the kernel loss
ℓ(θ) =1
2N/summationdisplay
i=1⟨k(xi,·),fθ(xi)−f(xi)⟩2, (4)
withuniformlyrandom xi, whichprobestheresidual fθ−fwithanintegralkernel k(x,y),x,y∈Sd−1
in theL2inner product⟨·,·⟩and is easier to bound in low regularity settings. Moreover, for common
kernels like the heat kernel, Gaussian kernel e−|x−y|2/σ2or Laplacian kernel e−|x−y|/σ, this loss
converges to the standard mean squared loss (3) for σ→0and proper normalization.
Although our interest in this kernel loss is of theoretical nature, to explore new arguments for
generalization in over-parametrized regimes, it is similar to variational losses in VPINNs (Kharazmi
et al., 2019; 2021), used to solve PDEs with neural networks. In this application, it is common
that PDE solutions do not admit continuous point evaluations, and instead one probes the residual
⟨fθ−f,v⟩with test functions vfrom some linear subspace, for which the given kernels would be one
example.
The kernel loss also bears a resemblance with randomized smoothing (Cohen et al., 2019): In order
to mitigate adversarial attacks on a classifier fθforYclasses, these methods choose the class that
is most likely under normal perturbations ϵ∼N(0,δ2)
g(x) = min
c∈YPr [fθ(x+ϵ) =c].
In comparison, for a Gaussian kernel with variance δ2, the kernel loss is identical to the mean squares
loss of the averaged network
gθ(x) =E[fθ(x+ϵ)] =⟨k(x,·),fθ)⟩.
The second main result shows that for the kernel loss, gradient descent decreases exponentially until
it reaches the approximation and estimation error
∥fθ−f∥2
L2(Sd−1)≲m−a+N−c,
again with rates aandcdependent on the Sobolev regularity of fas specified in Theorem 2.3. The
two error contributions on the left hand side are decoupled and we achieve the worst case of the
approximation error m−aand the sample error N−c. Contrary to the first result and conventional
machine learning theory, this allows meaningful generalization errors even in over-parametrized
regimes with more samples Nthan width m.
4Under review as submission to TMLR
To obtain the results, we do not bound the difference between empirical and expected loss (2) directly.
Instead, we compare the gradient descent evolution to an idealized method trained on the continuous L2
loss:
θn: trained by gradient descent on loss (3) or (4).
¯θn: trained by gradient descent on loss1
2∥f¯θ−f∥2
L2(Sd−1).
Convergence of the latter is established in Welper (2024a). From this we prove convergence for the former
based on perturbation analysis and sample errors for the respective gradients (not the loss as in standard
analysis (2)). This approach is reminiscent of Cohen et al. (2002), which analyzes adaptive PDE solvers
by comparing them with idealized infinite dimensional ones. The sample errors are established by Dudley’s
inequality for the sample loss (3) and by matrix Bernstein inequalities for the kernel loss (4).
2 Main Results
This section contains the main results of the paper.
2.1 Setup
The setup is almost identical to Welper (2024b;a), with the major difference that the references train on an
idealized continuous L2(Sd−1)loss, whereas we train on practical sample losses.
Notations We denote generic constants by c, which may be different in each occurrence, but do not depend
on the width m, number of samples Nor input dimension d. Alternatively, we use the shorthand a≲b,
a≳b,a∼bto denotea≤cb,a≥cb,a≲b≲a, respectively.
For integer s, Sobolev spaces Hs(Sd−1)consist of all functions on Sd−1withL2(Sd−1)bounded weak deriva-
tives of order s. For non-integer s, these spaces can be defined by the decay of their expansion
∥f∥2
Hα(Sd−1)=∞/summationdisplay
l=0ν(l)/summationdisplay
j=1/parenleftig
1 +l1/2(l+d−2)1/2/parenrightig2α/vextendsingle/vextendsingle/vextendsingle/angbracketleftig
Yj
l,f/angbracketrightig/vextendsingle/vextendsingle/vextendsingle2
(5)
in spherical harmonics
Yj
ℓ, ℓ = 0,1,2,..., 1≤j≤ν(ℓ), (6)
for suitable numbers ν(ℓ), see e.g. Barceló et al. (2020). We denote the corresponding inner product by
⟨·,·⟩Hs(Sd−1).
Network We consider fully connected networks without bias
f1(x) =W0x,
fℓ+1(x) =Wℓm−1/2
ℓσ/parenleftbig
fℓ(x)/parenrightbig
, ℓ= 1,...,L(7)
of depthL, with normalized inputs in the unit sphere x∈D:=Sd−1and standard scaling. We summarize
all trainable weights in the parameter θand abbreviate the network by fθ(x) :=fL−1(x). To obtain a simple
non-convex model problem, we optimize the second but last layer WL−1and initialize all weights randomly
WL∈{− 1,+1}1×mL+1i.i.d. Rademacher not trained ,
WL−1∈Rmℓ+1×mℓ, ℓ∈[L] i.i.d.N(0,1) trained
Wℓ∈Rmℓ+1×mℓ, ℓ∈[L−2] i.i.d.N(0,1) not trained
W1∈Rm1×d, ℓ∈[L] i.i.d.N(0,1) not trained .
All hidden layers are of comparable size, the input d-dimensional and the output scalar:
m:=mL−1, 1 =mL+1≤mL∼···∼m1≥d.
5Under review as submission to TMLR
Activation Functions We require smooth activation functions with no more than linear growth and
derivatives bounded as follows:
|σ(x)|≲|x|,|σ(i)(x)|≲1i= 1,2,|σ(j)(x)|≤p(x), j = 3,4,(8)
for some polynomial p(x).
Training All networks are trained by gradient descent
θn+1=θn−γ∇θℓ(θn), (9)
with learning rate γ >0. We use different losses ℓ(θ)for the main results and define them in the respective
sections.
Neural Tangent Kernel The main results require coercivity of the neural tangent kernel, which has been
shown in Welper (2024b) for ReLUactivations based on Bietti & Mairal (2019); Geifman et al. (2020); Chen
& Xu (2021), but remains open for smoother activations (8) used in this paper. Since we only train the
second but last layer, in our case the neural tangent kernel (NTK) is informally defined as
Γ(x,y) = lim
width→∞R/summationdisplay
r=1∂rfθ(x)∂rfθ(y), (10)
with partial derivatives ∂WL−1
ijabbreviated by a single index ∂rwithr= 1,...,R :=mLmL−1. The
coercivity condition is then stated as
⟨f,Hf⟩HS(Sd−1)≳∥f∥HS−β(Sd−1), Hf :=/integraldisplay
DΓ(·,y)f(y)dy (11)
for someβ >0, allS∈{0,s}, some smoothness level 0≤s≤β
2and allf∈Hs(Sd−1). Again, for networks
with ReLUactivations, bias and all layers trained this is true with β=d/2, see Welper (2024b).
In addition, the main results require
Σk(1)̸= 0, k = 1,...,L, (12)
for the Gaussian process that describes the forward evaluation of the random initial network in the infinite
width limit (Jacot et al., 2018). Its correlation matrices Σ(x,y) = Σ(xTy)only depend on the angle between
x,y∈Sd−1and are, defined by
Σℓ+1(x,y) :=Eu,v∼N(0,A)[σ(u),σ(v)], A =/bracketleftbiggΣℓ(x,x) Σℓ(x,y)
Σℓ(y,x) Σℓ(y,y)/bracketrightbigg
, Σ0(x,y) =xTy,
As for coercivity, this property is known for ReLUactivations, where Σ(1) = 1 , see Chen & Xu (2021), and is
expected to be a minor technical assumption for smoother activations (8). The condition is directly related
to the NTK coercivity and with it left for future work.
2.2 Result I: Pointwise Sampling
For the first result, we use the standard least squares loss
ℓ(θ) =1
21
NN/summationdisplay
i=1[fθ(xi)−f(xi)]2, (13)
withNindependent uniform samples xi∈Sd−1. We first collect all major assumptions.
Assumption 2.1. Assume:
6Under review as submission to TMLR
1. The neural network (7)-(8)is trained by gradient descent (9).
2. The NTK satisfies coercivity (11)for0≤2s≤βand the forward process satisfies (12).
3. All hidden layers are of similar size: m0∼···∼mL−1=:m.
4. Smoothness is bounded by 0<s< 1/2.
5. Definehandτas follows and choose learning rate γand an arbitrary αso that
h=chm−1
21
1+α, τ =h2αm, γ ≲h√m, 0≤α<1−s.
for some constant chthat may depend on the initial error ∥fθ0−f∥L2(Sd−1).
The following result is similar to Welper (2024a, Theorem 2.2), which only considers approximation and
optimization errors. While the reference trains on the continuous L2(Sd−1)loss, we train on the discrete
sample loss and therefore also include estimation errors.
Theorem2.2. Assume we train the sampleloss (13), let Assumption2.1 be satisfied, let ∥f∥L∞(Sd−1)≲m1/2,
and define
∆sample (m,N ) =c∆m3/2
N1/2h1−αs
2β∥fθ0−f∥−1
Hs(Sd−1)
for some sufficiently large c∆. Then with residual κn:=fθn−fand probability at least 1−cL(e−m+e−τ),
while the gradient descent error exceeds the final approximation and estimation error
∥κk∥2
L2(Sd−1)≥ca/parenleftig
m−1
2α
1+α+ ∆sample (m,N )/parenrightigs
β∥κ0∥2
Hs(Sd−1), k<n, (14)
we have
∥κn∥2
L2(Sd−1)≤Ce−γ[hα+∆sample (m,N)]n∥κ0∥2
L2(Sd−1),∥κn∥2
Hs(Sd−1)≤C∥κ0∥2
Hs(Sd−1).
for sufficiently large constants ca,candCindependent of m,κ0andκn.
The proof is in Section B.2. The assumptions relate the smoothness, the size of the network and number of
samples. The only major assumption is the coercivity (11), (12), which is open for our activations but can
be easily inferred from the literature (Bietti & Mairal, 2019; Geifman et al., 2020; Chen & Xu, 2021) for
ReLUactivations. In the latter case, βdepends on the input dimension dand therefore all other bounds are
also dimension dependent, although this is not explicit in the stated results. See Welper (2024b) for details,
and numerical verification for smoother activations required in the theorem.
The result shows that gradient descent converges exponentially fast until the error is sufficiently small (14)
and we have
∥κn∥2
L2(Sd−1)<ca/parenleftig
m−1
2α
1+α+ ∆sample (m,N )/parenrightigs
β∥κ0∥2
Hs(Sd−1),
The first summand, m−1
2α
1+αtogether with the smoothness ∥κ0∥2
Hs(Sd−1)provides a typical approximation
error bound of the form (1). The second term ∆sample, bounds the sample error and has the typical N−1/2
dependence on the number of samples together with some factors of mandhthat measure the complexity
of the network. In particular, for the overall error to be small, we must have more samples Nthan width m.
2.3 Result II: Kernel Sampling
Motivation Generalization errors can be derived from bounds of the form
sup
θ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∥fθ−f∥L2(Sd−1)−1
2NN/summationdisplay
i=1|fθ(xi)−f(xi)|/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤C+N−1/2,
7Under review as submission to TMLR
whichcontrolsthedifferencebetweentheexpectedandempiricalloss, uniformlyinallparameters θcontained
in a set Θof all relevant parameters. The bound on the right hand side depends on some complexity measure
Csuch as Rademacher complexity. By the supremum in the estimate, the complexity bound usually depends
on the size of the hypothesis class {fθ|θ∈Θ}. In Theorem 2.2, this gives rise to the mdependent
∆sample (m,N )and therefore to the requirement of more samples than network size (although we technically
apply the argument to the gradient, not the loss).
In order to decouple the size of the network form the number of samples, we use the observation from Welper
(2024b) or Theorem 2.2 that in the initial NTK regime the Sobolev norm Hs(Sd−1)of the residual κ:=fθ−f
does not grow, i.e. ∥κn∥Hs(Sd−1)≤∥κ0∥Hs(Sd−1). Hence, we may replace the sample error with a bound of
the form
sup
∥κ∥Hs(Sd−1)≤∥κ0∥Hs(Sd−1)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∥κ∥2
L2(Sd−1)−1
2NN/summationdisplay
i=1|κ(xi)|2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤C+N−1/2, (15)
whichisindependent of the network size . Ifsissufficientlylargesothat Hs(Sd−1)isembeddedinto L∞(Sd−1)
with some margin, this estimate can be shown by a uniform law of large numbers, as e.g. in Vershynin (2018,
Chapter 8.2). Such an embedding also ensures a bounded loss function, which is assumed for standard VC
dimension and Rademacher complexity bounds of the generalization error. For small s, a favorable right
hand side cannot be expected. Indeed, the map κ→∥κ∥2
∗:=1
2N/summationtextN
i=1|κ(xi)|2fromHstoRmust be
continuous, otherwise one can find a perturbation ˜κso that∥κ∥L2−∥˜κ∥L2≤∥κ−˜κ∥L2≤∥κ−˜κ∥Hsis small
and∥κ∥∗−∥˜κ∥∗is large and as a result the left hand side of (15) must be large for either κor˜κ. By the
Sobolev embedding theorem (Adams & Fournier, 2008) the continuity holds for s≥d/2and fails otherwise.
Unfortunately, our results provide low Sobolev regularity s<1/2, insufficient for this embedding.
For a first theoretical exploration of alternative complexity measures and in order to stay compatible with
earlier approximation and optimization results, we change the point evaluation to localized integrals
ℓk(θ) :=1
2NN/summationdisplay
i=1⟨k(xi,·),fθ−f⟩2, (16)
with uniformly random xiand some integral kernel k:Sd−1×Sd−1→Rthat is smoothing and allows
continuous evaluation for all s >0. Note that many standard kernels, like heat, Gaussian and Laplacian
kernels, converge to the Dirac delta for their “width” going to zero. As a result, the loss ℓkconverges to the
regular mean square loss ℓin (13). See Section 2.4 for more details.
Kernels Before we state the main result, we need some properties of the kernel. First, we assume it is
zonal, i.e. that k(x,y) =k(xTy). As a result, by the Funk-Hecke formula (Atkinson & Han, 2012) the
eigenfunctions are spherical harmonics Yj
l(6) and for the corresponding eigenvalues λlj, we require
1≲λlj≲1, l≤L,1≤j≤ν(l),
λlj≲1, l>L, 1≤j≤ν(l),(17)
so that up to a limiting level L > 0the eigenvalues are of unit size and falling thereafter. In addition, the
kernels are bounded
sup
x∈D∥k(x,·)∥L2(Sd−1)≤Ck. (18)
We defer a more thorough discussion of kernels with the given properties to Section 2.4 and for the time
being consider a convolutional kernel on the line instead of zonal kernel on the sphere, to avoid technicalities.
In this case, a natural example is the Gaussian kernel kG(x−y) =1√
2πte−|x−y|2
t2. It is diagonalized by the
Fourier transform with continuous eigenvalues
λ(ω) =ˆkG(ω) =1
2√πe−ω2t2
4
for which we obtain (17) by the simple observation
λ(ω)≲1, ω∈R, λ (ω)≳1, ω2t2≤1⇔|ω|≤1
t,
8Under review as submission to TMLR
Similar kernels on the sphere are more technical and discussed in Section 2.4.
Result Unlike more traditional complexity measures in machine learning, the smoothness ∥κn∥Hs(Sd−1)≤
∥κ0∥Hs(Sd−1)is a byproduct of the gradient descent method and independent of the size of the network . This
yields error bounds with decoupled approximation and sampling error in the following theorem.
Theorem 2.3. Assume we train the kernel loss (16)with conditions (17),(18)and corresponding constants
CkandL. Let Assumption 2.1 be satisfied and for arbitrary τN≲Ndefine
∆sample (m,N ) =c∆/bracketleftigg
C2
k/parenleftigτN
N/parenrightig1/2
+C−2
k/parenleftbiggN
τN/parenrightbigg1/2
L−s+L−s/bracketrightigg
.
for some sufficiently large c∆. Then with κn:=fθn−fand probability at least 1−ce−τ−2τN[eτN−τN−1]−1
while the gradient descent error exceeds the final approximation and estimation error
∥κk∥2
L2(Sd−1)≥ca/parenleftig
m−1
2α
1+α+ ∆sample (m,N )/parenrightigs
β∥κ0∥2
Hs(Sd−1), k<n,
we have
∥κn∥2
L2(Sd−1)≤Ce−γ[hα+∆sample (m,N)]n∥κ0∥2
L2(Sd−1),∥κn∥2
Hs(Sd−1)≤C∥κ0∥2
Hs(Sd−1).
for sufficiently large constants ca,candCindependent of m,κ0andκn.
As for Theorem 2.2 the error decays exponentially until the final sum of approximation and sample error
is reached. Unlike Theorem 2.2, the sample error ∆sample (m,N )does not depend on the network size m.
Hence, the final error is the worse of the approximation and the sample error and provides meaningful error
bounds both in under- and over-parametrized regimes.
If we choose the best possible ratioN
τN=C4
kLsbetween the number of samples Nand the success probability
parameterτN, given the parameters LandCkof the kernel, we obtain
∆sample (m,N )≤(1 +c∆)L−s/2, (19)
converging to zero for large Lcorresponding to locally concentrated kernels, see Section 2.4.
The theorem contains the inequality ∥κn∥2
Hs(Sd−1)≤C∥κ0∥2
Hs(Sd−1)and therefore κn=fθn−fremains
bounded in the Sobolev norm. Hence, for the generalization error, we consider the hypothesis class of
bounded Sobolev functions, instead of neural networks with bounded weights as in typical Rademacher or
margin bounds.
2.4 Kernels
In this section, we consider kernels that meet our assumptions (17) and (18).
Heat Kernel We first consider the heat kernel kt(x,y)on the sphere Sd−1, defined as the solution of the
heat equation with Dirac delta as initial condition (Zhao & Song, 2018)
∂tkt(·,y)−∆kt(·,y) = 0, k 0(·,y) =δ(·,y),
where ∆is the Laplace-Beltrami operator on the sphere and δ(x,y)the Dirac delta distribution on the
sphere. On the flat space Rd, this kernel is identical to the Gaussian kernel (2π)−1/2t−1e−|x−y|2/t2, while
on the sphere they differ. We use the heat kernel because it allows a particularly simple verification of our
kernel assumptions.
Indeed, since the Laplace-Beltrami operator’s eigenfunctions are spherical harmonics Yj
lwith eigenvalues
−l(l+d−2)(Atkinson & Han, 2012), the heat equation has the explicit solution
kt(x,y) =/summationdisplay
l,je−l(l+d−2)tYj
l(x)/angbracketleftig
Yj
l,δ(·,y)/angbracketrightig
=/summationdisplay
l,je−l(l+d−2)tYj
l(x)Yj
l(y) (20)
9Under review as submission to TMLR
in its eigenbasis. Therefore, the eigenvalues of the kernel are λlj=e−l(l+d−2)tand withl(l+d−2)t≤1⇔
l≲t−1/2we have
e−1≤λlj≤1, l ≲t−1/2,
λlj≤1, l ≳t−1/2
so that the kernel assumption (17) is satisfied with L=ct−1/2. Since the eigenvalues decay exponentially,
the Sobolev norms of the kernel are bounded. More concretely, Lemma D.1 in the supplementary material
shows that
∥kt(·,y)∥2
L2(Sd−1)≤C2
k=:ct−d+3/2.
In conclusion, the heat kernel satisfies all assumptions of Theorem 2.3 and the sample error (19) simplifies
to
∆sample (m,N )≤(1 +c∆)ts/4,N
τN∼t−1
2s−4d+6
for the given number of samples. By construction, for t→0the kernel converges to the Dirac delta and
therefore the kernel loss (16) converges to the sample loss (13).
Gaussian and Laplace Kernels In order to obtain some further insight into permissible kernels, we
consider the Gaussian and Laplacian kernels
kG(x−y) =1√
2πte−|x−y|2
t2, k L(x−y) =1
2te−|x−y|
t
on the real line R. Fors<1/2, these are clearly bounded in Hs(R). Moreover, since these are convolutional
kernels, the eigenvalues correspond to the Fourier coefficients, given by
ˆkG(ω) =1
2√πe−ω2t2
4, ˆkL(ω) =/radicalbigg
2
π1
1 +ω2t2.
Thus, we easily obtain the analogues of the eigenvalue bounds (17):
ˆkG(ω)≲1, ω∈R, ˆkG(ω)≳1, ω2t2≤1⇔|ω|≤1
t,
ˆkL(ω)≲1, ω∈R, ˆkL(ω)≳1, ω2t2≤1⇔|ω|≤1
t.
Similar results on the sphere are significantly more involved and beyond the scope of this paper. See e.g.
Geifman et al. (2020, Appendix C) for an analysis of the Laplace kernel, without the fine grained dependence
ontrequired for our purposes.
2.5 Sketch of Proof
Overview This section contains a short overview over the proofs of Theorems 2.2 and 2.3. We start by
introducing a scale of continuous loss functions
ℓS(θ) :=1
2∥fθ−f∥2
HS(D), S ∈{0,s}.
ForS= 0, this loss is the generalization or L2(D)error. ForS=s>0, the loss is used to control the Sobolev
smoothness of the neural networks later in the proof. For convenience, we abbreviate ⟨·,·⟩s:=⟨·,·⟩Hs(D)and
κ:=fθ−f. A simple calculation and the mean value theorem yield that the loss evolves as
ℓS(θn+1)−ℓS(θn) =−γ/summationdisplay
r⟨κ,∂rf¯θ⟩S∂rℓ(θn),
10Under review as submission to TMLR
for some ¯θon the line segment between θnandθn+1. On the left hand side, we use the ℓSloss, i.e. the
generalization error or the smoothness, because these are the quantities we ultimately want to control in the
theorems. On the right hand side, we have the discrete loss ℓthat we actually train on. By a perturbation
argument, we replace the discrete loss with the continuous one:
ℓS(θn+1)−ℓS(θn)
=−γ/summationdisplay
r⟨κn,∂rf¯θ⟩S∂rℓ0(θn)−γ/summationdisplay
r⟨κn,∂rf¯θ⟩S[∂rℓ(θn)−∂rℓ0(θn)].
=:GD0+Permutation
The permutation term will be bounded by
Permutation≤γ∆sample (m,N )∥κk∥0∥κk∥S,
giving rise to the terms ∆sample (m,N )in the main theorems.
Convergence of GD0We first consider the case that the permutation term Permutation is zero. This
corresponds to training on the continuous L2(D)lossℓ0directly and has been studied in Welper (2024b;a).
The convergence analysis is based on the observation that
ℓ0(θn+1)−ℓ0(θn)≤−γ⟨κn,Hκn⟩0+permutation ,
ℓs(θn+1)−ℓs(θn)≤−γ⟨κn,Hκn⟩s+permutation ,(21)
whereHis the linear integral operator induced by the neural tangent kernel
Γ(x,y) = lim
width→∞R/summationdisplay
r=1∂rfθ0(x)∂rfθ0(y).
In short, the terms −γ⟨κn,Hκn⟩are a linearization of the gradient −γ∥∇θfθ∥2, usually found in gradient
descent analysis. This linearization remains accurate because of the crucial observation that the weights
θ0−θndo not move far from their initial during training.
Ignoring the linearization error, the left hand sides of (21) are bilinear forms and therefore allow simple
convergence proofs if the eigenvalues of Hare lower bounded. While this is true in over-parametrized
regimes, for the L2(D)loss, the NTK is a compact operator and the eigenvalues converge to zero. Therefore,
weconsideracoupledevolutionofthe ℓ0andℓslosses: Thelatterensuresthatthenetworksremainuniformly
bounded in the Sobolev norms Hs(D). This implies that the residual κnis concentrated in low frequencies
or equivalently eigenspaces corresponding to large eigenvalues. Therefore, the system is comparable to ones
with lower bounded eigenvalues and allows us to prove convergence of the ℓ0loss.
Since this theory already contains permutation terms, the new terms Permutation from sampling the
gradient can be added to the theory with only minimal changes, shown in Theorem A.1.
Bounds for Permutation The main new contribution of the paper is to show that Permutation , i.e.
the difference between the gradients of the continuous loss ℓ0and the discrete loss ℓremain small. The
arguments are similar to standard generalization error bounds, which address the difference between ℓ0−ℓ
directly.
1. ClassicaltechniquestoboundgeneralizationerrorsincludeVC-dimensionorRademachercomplexity,
which, in turn, are sometimes bounded by Dudley’s inequality (Shalev-Shwartz & Ben-David, 2014).
In Section B, we use the latter directly to bound Permutation for the least squares loss (13).
This is convenient because similar techniques are used in the prior work Welper (2024b) to show
concentration of the NTK. The argument is reminiscent of the uniform law of large numbers as
shown in Vershynin (2018, Chapter 8.2).
11Under review as submission to TMLR
2. For the kernel loss (16), we take a different route in Section C. First, in the limit of infinite samples,
heat, Gauss and Laplace kernels do not converge to the L2(D)-loss. They rather converge to a dual
norm∥fθ−f∥H′for primal norm∥·∥H=∥·∥2
L2(D)+t∥·∥2
Hα(D), someα>0and a parameter tfor
the width of the kernel.
In the NTK convergence analysis, we have already shown that the residual fθn−fis concentrated
in low frequencies and as a result for carefully chosen t, theL2(D)part of the above norms dominate
theHα(D)part. Together with concentration results for the kernels, this time shown by matrix
Bernstein inequalities, this implies gradient descent convergence in the L2(D)norm.
Remark on Generalization Errors and Optimization Many generalization error bounds in the liter-
ature involve terms like N−1/2∥Wℓ−Wℓ
0∥F(or other norms) that include the distance of the weights from
their initial, which stays small in NTK gradient descent convergence proofs. Concretely, if we use the bound
from the first equation in the proof of Theorem A.1, which underlies all main results, we obtain on the
trained layer
∥WL−1−WL−1
0∥F≤m1/2∥WL−1−WL−1
0∥=m∥θ−θ0∥∗≤mh=m1−1
21
1+α≤m1/2
for someα≥0. Therefore, the generalization error bounds can be further bounded by N−1/2∥WL−1−
WL−1
0∥F≤N−1/2m1/2. Hence, to obtain small generalization error, we must be in an under-parametrized
regime with m≤N. This conflicts with over-parametrization assumptions in most NTK convergence results.
While the generalization bounds and NTK convergence may be reconciled with a finer analysis, we rely on
the NTK results in Welper (2024b;a), which also work in under-parametrized regimes.
3 Numerical Experiments
In this section, we supplement the theoretical findings with numerical convergence rates with respect to
network width and number of samples. We consider the following test case:
•Network Architecture: Fully connected with bias and ReLUactivation.
•Input data: Uniformly distributed on the cube [−1,1]d.
•Labels:Gaussian density function of the input samples, i.e. yi=e−|xi|2/2.
•Test Loss: In order to approach the L2(D)error, the test loss is computed on a large number of
1000uniformly sampled xiwith mean squared loss, no matter the loss function used for training.
•Kernel Loss: Toapproximatethekernelloss, weuniformlysample Npointszi∈[−1,1]dandthenfor
eachiwe sampleNk= 10samples from the normal distribution N(zi,0.01)to obtainxij∈RN×Nk.
Then, we approximate the kernel by Monte Carlo integration
ℓk(θ)≈N/summationdisplay
i=1
Nk/summationdisplay
j=1fθ(xij)−f(xij)
2
.
•Training: 20000 gradient descent steps with learning rate 0.05.
•Repetition: Since the randomness of data and initialization is important for the theoretical results,
all reported losses are an average of 20trials.
Figure 1 and Table 1 contain the estimated convergence rates of the test loss for training with mean squared
loss (MSE) (13) and kernel loss (16). For comparison, the expected convergence rate for piecewise linear
approximation of smooth functions is m−2/dfor the error and m−4/dfor the loss or squared error. Typical
convergence rates with respect to number of samples is N−1/2. These theoretical rates are better than the
actual ones reported in the table. The practical rates have significant variance despite being averaged over
12Under review as submission to TMLR
32
40
48
56
64Width50
100
150200250Samples 6.49e-05 0.000113 0.000197 0.000343 0.000597 0.00104 Loss
(a) MSE Loss, dim= 3, depth= 5.
32
40
48
56
64Width50
100
150200250Samples 5.52e-05 9e-05 0.000147 0.00024 0.000391 0.000638Loss (b) Kernel Loss, dim= 3, depth= 5.
Figure 1: Test loss for training with mean squared loss (13) (left) and kernel loss (16) (right). All axes are
log-scaled so that the slope corresponds to convergence rates.
Dimension 3 and Depth 5 – Trained with MSE Loss
dof rate Nrate
m/N100 150 200 250 100 150 200 250
400.331 1.33 1.33 1.19 1.9 1.36 0.743 0.896
480.833 0.328 0.628 1.06 1.86 1.13 0.933 1.25
561.13 0.969 1.25 0.434 2.2 1.07 1.08 0.684
64-1.03 2.05 0.967 1.47 1.56 2.08 0.582 0.985
Dimension 3 and Depth 5 – Trained with Kernel Loss
dof rate Nrate
m/N100 150 200 250 100 150 200 250
401.08 0.676 0.748 1.07 1.65 0.939 0.797 0.482
480.229 0.517 0.433 1.05 1.23 1.07 0.744 0.987
561.69 1.77 1.75 1.11 2.02 1.1 0.734 0.539
64-0.54 -0.218 0.898 1.19 1.71 1.21 1.25 0.713
Table 1: Estimated convergence rates between neighbouring losses for the given m/N. Left: Rate along the
column, i.e. with respect to m. Right: Rate along rows, i.e. with respect to number of samples N. The first
table is trained with mean squared loss (MSE) (13) and the second with kernel loss (16).
20separate runs. Individual runs are even more noisy. All plots are in log scale so that slopes along the
xoryaxes correspond to convergence rates. Despite severe over-parametrization, the loss decreases with
respect toN, although with slowing rate. This matches the theoretical results for the kernel loss, which
provides bounds in over-parametrized regimes, but does not decrease below the approximation error. In
conclusion, the experiments confirm the results of the paper, but also indicate that the theoretical findings
are pessimistic.
More detailed results, including the loss and shallow networks, are contained in Appendix E.
References
Robert A. Adams and John J F. Fournier. Sobolev spaces . Number 140 in Pure and applied mathematics.
Acad. Press, Amsterdam, 2. ed., [nachdr.]. edition, 2008. ISBN 9780120441433.
13Under review as submission to TMLR
Ben Adcock and Nick Dexter. The gap between theory and practice in function approximation with deep
neural networks. SIAM Journal on Mathematics of Data Science , 3(2):624–655, 2021.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Re-
search, pp. 242–252, Long Beach, California, USA, 09–15 Jun 2019. PMLR. Full version available at
https://arxiv.org/abs/1811.03962 .
SanjeevArora, RongGe, BehnamNeyshabur, andYiZhang. Strongergeneralizationboundsfordeepnetsvia
a compression approach. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pp. 254–263.
PMLR, 10–15 Jul 2018.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization
and generalization for overparameterized two-layer neural networks. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning , volume 97 of
Proceedings of Machine Learning Research , pp. 322–332, Long Beach, California, USA, 09–15 Jun 2019a.
PMLR.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact
computation with an infinitely wide neural net. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32.
Curran Associates, Inc., 2019b.
Amir Asadi, Emmanuel Abbe, and Sergio Verdu. Chaining mutual information and tightening generalization
bounds. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),
Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018.
KendallAtkinsonandWeiminHan. Spherical harmonics and approximations on the unit sphere: an introduc-
tion. Number 2044 in Lecture notes in mathematics. Springer-Verlag, Berlin, 2012. ISBN 9783642259821.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine
Learning Research , 18(19):1–53, 2017.
Yu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural
networks. In International Conference on Learning Representations , 2020.
Juan Antonio Barceló, Teresa Luque, and Salvador Pérez-Esteva. Characterization of sobolev spaces on the
sphere.Journal of Mathematical Analysis and Applications , 491(1):124240, 2020. ISSN 0022-247X.
A.R.Barron. Universalapproximationboundsforsuperpositionsofasigmoidalfunction. IEEE Transactions
on Information Theory , 39(3):930–945, 1993.
Peter Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear vc dimension bounds for piecewise polynomial
networks. In M. Kearns, S. Solla, and D. Cohn (eds.), Advances in Neural Information Processing Systems ,
volume 11. MIT Press, 1998.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neu-
ral networks. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 30. Curran Associates,
Inc., 2017.
Christian Beck, Arnulf Jentzen, and Benno Kuckuck. Full error analysis for the training of deep neural
networks. Infinite Dimensional Analysis, Quantum Probability and Related Topics , 25(02):2150020, June
2022. ISSN 0219-0257, 1793-6306.
Julius Berner, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. The Modern Mathematics of Deep
Learning. In Philipp Grohs and Gitta Kutyniok (eds.), Mathematical Aspects of Deep Learning , pp. 1–111.
Cambridge University Press, 1 edition, December 2022. ISBN 9781009025096 9781316516782.
14Under review as submission to TMLR
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Infor-
mation Processing Systems , volume 32. Curran Associates, Inc., 2019.
GuyBreslerandDheerajNagaraj. SharprepresentationtheoremsforReLUnetworkswithprecisedependence
on depth. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems , volume 33, pp. 10697–10706. Curran Associates, Inc., 2020.
Yuan Cao and Quanquan Gu. Generalization Error Bounds of Gradient Descent for Learning Over-
Parameterized Deep ReLU Networks. Proceedings of the AAAI Conference on Artificial Intelligence ,
34(04):3349–3356, April 2020. ISSN 2374-3468, 2159-5399.
Lin Chen and Sheng Xu. Deep neural tangent kernel and laplace kernel have the same rkhs. In International
Conference on Learning Representations , 2021.
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is sufficient to
learn deep re{lu} networks? In International Conference on Learning Representations , 2021.
Lénaïc Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 31. Curran Associates,
Inc., 2018. https://arxiv.org/abs/1805.09545 .
Lénaïc Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché Buc, E. Fox, and R. Garnett (eds.), Advances in
Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.
A. Cohen, W. Dahmen, and R. DeVore. Adaptive Wavelet Methods II—Beyond the Elliptic Case. Founda-
tions of Computational Mathematics , 2(3):203–202, August 2002. ISSN 16153375.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing.
In KamalikaChaudhuri andRuslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pp. 1310–1320. PMLR,
09–15 Jun 2019.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Math. Control Signal Systems , 2:
303–314, 1989.
I. Daubechies, R. DeVore, S. Foucart, B. Hanin, and G. Petrova. Nonlinear Approximation and (Deep) ReLU
Networks. Constructive Approximation , 55(1):127–172, February 2022. ISSN 0176-4276, 1432-0940.
Ronald DeVore, Boris Hanin, and Guergana Petrova. Neural network approximation. Acta Numerica , 30:
327–444, 2021.
Selina Drews and Michael Kohler. On the universal consistency of an over-parametrized deep neural network
estimate learned by gradient descent, 2022. https://arxiv.org/abs/2208.14283 .
SimonDuandJasonLee. Onthepowerofover-parametrizationinneuralnetworkswithquadraticactivation.
In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine
Learning , volume 80 of Proceedings of Machine Learning Research , pp. 1329–1338. PMLR, 10–15 Jul 2018.
https://arxiv.org/abs/1803.01206 .
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of
deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research ,
pp. 1675–1685, Long Beach, California, USA, 09–15 Jun 2019a. PMLR.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-
parameterized neural networks. In International Conference on Learning Representations , 2019b.
15Under review as submission to TMLR
Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design.
InInternational Conference on Learning Representations , 2018. https://arxiv.org/abs/1711.00501 .
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Basri Ronen. On the
similarity between the laplace and neural tangent kernels. In H. Larochelle, M. Ranzato, R. Hadsell, M.F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 1451–1461.
Curran Associates, Inc., 2020.
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stéphane d’Ascoli, Giulio Biroli,
Clément Hongler, and Matthieu Wyart. Scaling description of generalization with number of parameters
in deep learning. CoRR, abs/1901.01608, 2019.
R. Gentile and G. Welper. Approximation results for gradient descent trained shallow neural networks in
1d, 2022. https://arxiv.org/abs/2209.08399 .
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural net-
works. In Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings of the 31st Confer-
ence On Learning Theory , volume 75 of Proceedings of Machine Learning Research , pp. 297–299. PMLR,
06–09 Jul 2018.
Rémi Gribonval, Gitta Kutyniok, Morten Nielsen, and Felix Voigtlaender. Approximation Spaces of Deep
Neural Networks. Constructive Approximation , 55(1):259–367, February 2022. ISSN 0176-4276, 1432-0940.
Philipp Grohs and Felix Voigtlaender. Proof of the Theory-to-Practice Gap in Deep Learning via Sampling
Complexity bounds for Neural Network Approximation Spaces. Foundations of Computational Mathemat-
ics, July 2023. ISSN 1615-3375, 1615-3383.
Ingo Gühring, Gitta Kutyniok, and Philipp Petersen. Error bounds for approximations with deep ReLU
neural networks in ws,p norms. Analysis and Applications , 18(05):803–859, 2020.
Boris Hanin and Mark Sellke. Approximating continuous functions by ReLU nets of minimal width. https:
//arxiv.org/abs/1710.11278 , 2017.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient
descent. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International
Conference on Machine Learning , volume 48 of Proceedings of Machine Learning Research , pp. 1225–1234,
New York, New York, USA, 20–22 Jun 2016. PMLR.
Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for piecewise
linear neural networks. In Satyen Kale and Ohad Shamir (eds.), Proceedings of the 2017 Conference on
Learning Theory , volume 65 of Proceedings of Machine Learning Research , pp. 1064–1068. PMLR, 07–10
Jul 2017.
Lukas Herrmann, Joost A. A. Opschoor, and Christoph Schwab. Constructive deep ReLU neural network
approximation. Journal of Scientific Computing , 90(2):75, 2022.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal
approximators. Neural Networks , 2(5):359–366, 1989. ISSN 0893-6080.
Daniel Hsu, Sham Kakade, and Tong Zhang. Tail inequalities for sums of random matrices that depend on
the intrinsic dimension. Electronic Communications in Probability , 17:1 – 13, 2012.
Shokhrukh Ibragimov, Arnulf Jentzen, and Adrian Riekert. Convergence to good non-optimal critical points
inthetrainingofneuralnetworks: Gradientdescentoptimizationwithonerandominitializationovercomes
all bad non-global local minima with high probability, 2022. https://arxiv.org/abs/2212.13111 .
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.),Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018.
16Under review as submission to TMLR
Daniel Jakubovitz, Raja Giryes, and Miguel R. D. Rodrigues. Generalization Error in Deep Learning. In
Holger Boche, Giuseppe Caire, Robert Calderbank, Gitta Kutyniok, Rudolf Mathar, and Philipp Petersen
(eds.),Compressed Sensing and Its Applications , pp. 153–193. Springer International Publishing, Cham,
2019. ISBN 9783319730738 9783319730745.
Arnulf Jentzen and Adrian Riekert. A proof of convergence for the gradient descent optimization method
with random initializations in the training of neural networks with relu activation for piecewise linear
target functions. Journal of Machine Learning Research , 23(260):1–50, 2022.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily small
test error with shallow ReLU networks. In International Conference on Learning Representations , 2020.
Ziwei Ji, Matus Telgarsky, and Ruicheng Xian. Neural tangent kernels, transportation mappings, and
universal approximation. In International Conference on Learning Representations , 2020.
Kenji Kawaguchi and Jiaoyang Huang. Gradient descent finds global minima for generalizable deep neural
networks of practical sizes. In 2019 57th Annual Allerton Conference on Communication, Control, and
Computing (Allerton) , pp. 92–99, 2019.
E. Kharazmi, Z. Zhang, and G. E. Karniadakis. Variational physics-informed neural networks for solving
partial differential equations, 2019. https://arxiv.org/abs/1912.00873 .
EhsanKharazmi,ZhongqiangZhang, andGeorgeE.M.Karniadakis. hp-vpinns: Variationalphysics-informed
neural networks with domain decomposition. Computer Methods in Applied Mechanics and Engineering ,
374:113547, 2021. ISSN 0045-7825.
Jason M. Klusowski and Andrew R. Barron. Approximation by combinations of ReLU and squared ReLU
ridge functions with ℓ1andℓ0controls. IEEE Transactions on Information Theory , 64(12):7649–7656,
2018.
Michael Kohler and Adam Krzyzak. Analysis of the rate of convergence of an over-parametrized deep neural
network estimate learned by gradient descent, 2022. https://arxiv.org/abs/2210.01443 .
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in
Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.
Jongmin Lee, Joo Young Choi, Ernest K Ryu, and Albert No. Neural tangent kernel analysis of deep narrow
neural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and
Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning , volume 162
ofProceedings of Machine Learning Research , pp. 12282–12351. PMLR, 17–23 Jul 2022.
Bo Li, Shanshan Tang, and Haijun Yu. Better approximations of high dimensional smooth functions by deep
neural networks with rectified power units. Communications in Computational Physics , 27(2):379–411,
2019. ISSN 1991-7120.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent
onstructureddata. InS.Bengio, H.Wallach, H.Larochelle, K.Grauman, N.Cesa-Bianchi, andR.Garnett
(eds.),Advances in Neural Information Processing Systems 31 , pp. 8157–8166. Curran Associates, Inc.,
2018.
Zhong Li, Chao Ma, and Lei Wu. Complexity measures for neural networks with general activation functions
using path-based norms, 2020. https://arxiv.org/abs/2009.06132 .
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geometry, and
complexity of neural networks. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings of the
Twenty-Second International Conference on Artificial Intelligence and Statistics , volume 89 of Proceedings
of Machine Learning Research , pp. 888–896. PMLR, 16–18 Apr 2019.
17Under review as submission to TMLR
Fusheng Liu, Haizhao Yang, Soufiane Hayou, and Qianxiao Li. From optimization dynamics to generalization
bounds via Łojasiewicz gradient inequality. Transactions on Machine Learning Research , 2022. ISSN 2835-
8856.
George G. Lorentz, Manfred v. Golitschek, and Yuly Makovoz. Constructive Approximation: Advanced
Problems . Springer-Verlag Berlin Heidelberg, 1996.
Jianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation for smooth
functions. SIAM Journal on Mathematical Analysis , 53(5):5465–5506, 2021.
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural
networks: A view from the width. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30 , pp.
6231–6239. Curran Associates, Inc., 2017.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer
neural networks. Proceedings of the National Academy of Sciences , 115(33):E7665–E7671, 2018. ISSN
0027-8424. https://arxiv.org/abs/1804.06561 .
Stanislav Minsker. On some extensions of bernstein’s inequality for self-adjoint operators. Statistics &
Probability Letters , 127:111–119, 2017. ISSN 0167-7152.
Gergely Neu, Gintare Karolina Dziugaite, Mahdi Haghifam, and Daniel M. Roy. Information-theoretic
generalization bounds for stochastic gradient descent. In Mikhail Belkin and Samory Kpotufe (eds.),
Proceedings of Thirty Fourth Conference on Learning Theory , volume 134 of Proceedings of Machine
Learning Research , pp. 3526–3545. PMLR, 15–19 Aug 2021.
Behnam Neyshabur, Ryota Tomioka, and Srebro Nathan. Norm-based capacity control in neural networks.
In Peter Grünwald, Elad Hazan, and Satyen Kale (eds.), Proceedings of The 28th Conference on Learning
Theory, volume 40 of Proceedings of Machine Learning Research , pp. 1376–1401, Paris, France, 03–06 Jul
2015. PMLR.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Srebro Nati. Exploring generalization in
deep learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 30. Curran Associates,
Inc., 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Srebro Nathan. A PAC-bayesian approach to spectrally-
normalized margin bounds for neural networks. In International Conference on Learning Representations ,
2018.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Doina Precup and
Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning , volume 70
ofProceedings of Machine Learning Research , pp. 2603–2612. PMLR, 06–11 Aug 2017.
Quynh N Nguyen and Marco Mondelli. Global convergence of deep networks with one wide layer followed by
pyramidal topology. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems , volume 33, pp. 11961–11972. Curran Associates, Inc., 2020.
Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under neural
tangent kernel regime. In International Conference on Learning Representations , 2021.
Joost A. A. Opschoor, Philipp C. Petersen, and Christoph Schwab. Deep ReLU networks and high-order
finite element methods. Analysis and Applications , 18(05):715–770, 2020.
S. Oymak and M. Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees
for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory , 1(1):84–105,
2020.
18Under review as submission to TMLR
Sejun Park, Umut Simsekli, and Murat A Erdogdu. Generalization bounds for stochastic gradient descent
via localized $\varepsilon$-covers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun
Cho (eds.), Advances in Neural Information Processing Systems , 2022.
Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta Numerica , 8:143–195, 1999.
Daniel A. Roberts, Sho Yaiday, and Boris Hanin. The Principles of Deep Learning Theory: An Effective
Theory Approach to Understanding Neural Networks . Cambridge University Press, 2022.
Grant M. Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems: Asymptotic
convexity of the loss landscape and universal scaling of the approximation error. CoRR, abs/1805.00915,
2018. https://arxiv.org/abs/1805.00915 .
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer ReLU neural networks. In
Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine
Learning , volume 80 of Proceedings of Machine Learning Research , pp. 4433–4441, Stockholmsmässan,
Stockholm Sweden, 10–15 Jul 2018. PMLR.
S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms . Un-
derstanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014. ISBN
9781107057135.
ZuoweiShen, HaizhaoYang, andShijunZhang. Nonlinearapproximationviacompositions. Neural Networks ,
119:74–84, 2019. ISSN 0893-6080.
Jonathan W. Siegel and Jinchao Xu. Approximation rates for neural networks with general activation
functions. Neural Networks , 128:313–321, 2020. ISSN 0893-6080.
Jonathan W. Siegel and Jinchao Xu. High-order approximation rates for shallow neural networks with cosine
and ReLUkactivation functions. Applied and Computational Harmonic Analysis , 58:1–26, 2022a. ISSN
1063-5203.
Jonathan W. Siegel and Jinchao Xu. Optimal convergence rates for the orthogonal greedy algorithm. IEEE
Transactions on Information Theory , 68(5):3354–3361, 2022b.
Jonathan W. Siegel, Qingguo Hong, Xianlin Jin, Wenrui Hao, and Jinchao Xu. Greedy training algorithms
for neural networks and applications to PDEs. Journal of Computational Physics , 484:112084, July 2023.
ISSN 00219991.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of large
numbers. SIAM Journal on Applied Mathematics , 80(2):725–752, 2020.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimization land-
scape of over-parameterized shallow neural networks. IEEE Transactions on Information Theory , 65(2):
742–769, 2019. https://arxiv.org/abs/1707.04926 .
ChaeHwan Song, Ali Ramezani-Kebrya, Thomas Pethick, Armin Eftekhari, and Volkan Cevher. Sub-
quadratic overparameterization for shallow neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin,
P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , vol-
ume 34, pp. 11247–11259. Curran Associates, Inc., 2021.
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound, 2019.
https://arxiv.org/abs/1906.03593 .
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for
multilayer neural networks, 2016. https://arxiv.org/abs/1605.08361 .
Elias M. Stein and Guido Weiss. Introduction to Fourier Analysis on Euclidean Spaces (PMS-32) . Princeton
University Press, December 1972. ISBN 9781400883899.
19Under review as submission to TMLR
Thomas Steinke and Lydia Zakynthinou. Reasoning About Generalization via Conditional Mutual Informa-
tion. In Jacob Abernethy and Shivani Agarwal (eds.), Proceedings of Thirty Third Conference on Learning
Theory, volume 125 of Proceedings of Machine Learning Research , pp. 3437–3452. PMLR, 09–12 Jul 2020.
Lili Su and Pengkun Yang. On learning over-parameterized neural networks: A functional approximation
perspective. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.),
Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.
Taiji Suzuki. Adaptivity of deep reLU network for learning in besov and mixed smooth besov spaces: optimal
rate and curse of dimensionality. In International Conference on Learning Representations , 2019.
Joel A. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends ®in Machine
Learning , 8(1-2):1–230, 2015. ISSN 1935-8237.
Zhuozhuo Tu, Fengxiang He, and Dacheng Tao. Understanding generalization in recurrent neural networks.
InInternational Conference on Learning Representations , 2020.
Luca Venturi, Afonso S. Bandeira, and Joan Bruna. Spurious valleys in one-hidden-layer neural network
optimization landscapes. Journal of Machine Learning Research , 20(133):1–34, 2019. https://arxiv.
org/abs/1802.06384 .
Roman Vershynin. High-dimensional probability: an introduction with applications in data science . Num-
ber 47 in Cambridge series in statistical and probabilistic mathematics. Cambridge University Press,
Cambridge ; New York, NY, 2018. ISBN 9781108415194.
Mingze Wang and Chao Ma. Generalization error bounds for deep neural networks trained by sgd, 2023.
https://arxiv.org/abs/2206.03299 .
E Weinan, Chao Ma, and Lei Wu. A priori estimates of the population risk for two-layer neural networks.
Communications in Mathematical Sciences , 17(5):1407–1425, 2019. ISSN 1945-0796.
E Weinan, Ma Chao, Wu Lei, and Stephan Wojtowytsch. Towards a mathematical understanding of neural
network-based machine learning: What we know and what we don’t. CSIAM Transactions on Applied
Mathematics , 1(4):561–615, 2020. ISSN 2708-0579.
E Weinan, Chao Ma, and Lei Wu. The Barron Space and the Flow-Induced Function Spaces for Neural
Network Models. Constructive Approximation , 55(1):369–406, February 2022. ISSN 0176-4276, 1432-0940.
G. Welper. Approximation and gradient descent training with neural networks, 2024a. https://arxiv.
org/abs/2405.11696 .
Gerrit Welper. Approximation results for gradient flow trained neural networks. Journal of Machine Learn-
ing, 3(2):107–175, 2024b.
Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks , 94:103–114,
2017. ISSN 0893-6080.
Dmitry Yarotsky. Optimal approximation of continuous functions by very deep ReLU networks. In Sébastien
Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings of the 31st Conference On Learning
Theory, volume 75 of Proceedings of Machine Learning Research , pp. 639–649. PMLR, 06–09 Jul 2018.
Dmitry Yarotsky and Anton Zhevnerchuk. The phase diagram of approximation rates for deep neural
networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems , volume 33, pp. 13005–13015. Curran Associates, Inc., 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. In International Conference on Learning Representations ,
2017.
20Under review as submission to TMLR
Chenchao Zhao and Jun S. Song. Exact Heat Kernel on a Hypersphere and Its Applications in Kernel SVM.
Frontiers in Applied Mathematics and Statistics , 4:1, January 2018. ISSN 2297-4687.
Ding-Xuan Zhou. Universality of deep convolutional neural networks. Applied and Computational Harmonic
Analysis, 48(2):787–794, 2020. ISSN 1063-5203.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in
Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-parameterized
deep ReLU networks. Machine Learning , 109(3):467 – 492, 2020.
21Under review as submission to TMLR
A Gradient Descent Convergence
A.1 Convergence Result
In this section, we prove an abstracted convergence result that is the foundation for Theorems 2.2 and 2.3.
To this end, let Θ⊂RRbe a set of admissible weights, and Hs,s∈Ra set of Hilbert spaces. Then we
consider
f·: Θ→H0, θ →fθ, (22)
ℓs:Hs→R, θ →ℓs(θ) =:1
2∥fθ−f∥2
s, (23)
ℓ:H0→R, θ →ℓ(θ), (24)
whereℓ0corresponds to the continuous loss, or generalization error, and ℓto the discrete loss. The Hilbert
spaces have norms ∥·∥s=∥·∥Hsand are related by the interpolation inequality
∥·∥b≲∥·∥c−b
c−aa∥·∥b−a
c−ac (25)
for for alla,b,c∈R. Typically, we choose Sobolev spaces Hs=Hs(D), the neural network θ→fθ(·)∈
L2(D) =H0, and some discrete loss ℓ, but this is not important thought this section. The statement of the
following result relies on the empirical NTK and NTK
Hθ,¯θ:=R/summationdisplay
r=1(∂rfθ)(∂rf¯θ)∗, H := lim
width→∞R/summationdisplay
r=1∂rfθ0∂rf∗
θ0,
wheref∗is theH0-adjoint of f. Note that the adjoint, contained in the dual space (H0)′, applied to a
functionvisf∗(v) =⟨f,v⟩and as a result, Hcorresponds to the integral operator induced by the integral
kernel in (11).
The following convergence result is almost identical to Welper (2024a, Theorem 3.1), up to a new error term
for the difference between the Hilbert space loss ℓ0and the discrete sample loss ℓin (31).
Theorem A.1. Assume we minimize the loss ℓof the parametrized function θ→fθ∈H0in(22), with
gradient descent (9)and Hilbert spaces (25). Define the residual κk=fθk−f. Letmbe an indicator for the
network size that satisfies the inequalities below. With α>0from(29)below, assume that
1.His coercive for S= 0andS=sand someβ >2s>0
∥v∥2
S−β≲⟨v,Hv⟩S, v ∈HS−β. (26)
2. For some norm ∥·∥∗, the distance of the weights from their initial value is bounded by
/vextenddouble/vextenddoubleθk−θ0/vextenddouble/vextenddouble
∗≲1, k= 1,...,n⇒/vextenddouble/vextenddoubleθn+1−θ0/vextenddouble/vextenddouble
∗≲γ√mn/summationdisplay
k=0∥κk∥0.(27)
3. The learning rate γis sufficiently small so that
γ∥∇θℓ(θn)∥∗≲chm−1
21
1+α=:h (28)
for some constant chthat may depend on the initial error ∥κ0∥0.
4. ForS= 0andS=s, initial value θ0, any ¯θ,˜θ∈Θand any ¯h>0, the bounds/vextenddouble/vextenddoubleθ0−¯θ/vextenddouble/vextenddouble
∗≤¯hand/vextenddouble/vextenddoubleθ0−˜θ/vextenddouble/vextenddouble
∗≤¯himply
∥H˜θ,θ0−H˜θ,¯θ∥S,0≤c¯hα, ∥Hθ0,˜θ−H¯θ,˜θ∥S,0≤c¯hα. (29)
22Under review as submission to TMLR
5. ForS= 0andS=s, we have
∥H−Hθ0,θ0∥S,0≤chm−1
2α
1+α=hα. (30)
6. There is a bound ∆sample (m,N )and sufficiently large constant cAso that
sup
∥θ0−θ∥∗≤cAh
∥θ0−¯θ∥∗≤cAh−R/summationdisplay
r=1/angbracketleftbig
κk,∂rf¯θ/angbracketrightbig
S[∂rℓ(θ)−∂rℓ0(θ)]≤∆sample (m,N )∥κk∥0∥κk∥S(31)
for all gradient descent iterates κkwithk≤n.
Then, while the gradient descent error exceeds the final approximation and estimation error
∥κk∥2
0≥ca/parenleftig
m−1
2α
1+α+ ∆sample (m,N )/parenrightigs
β∥κ0∥2
s, k ≤n, (32)
we have
∥κn+1∥2
0≤Ce−γ[hα+∆sample (m,N)](n+1)∥κ0∥2
0,∥κn+1∥2
s≤C∥κ0∥2
s
for sufficiently large constants ca,candCindependent of m,κ0andκn+1.
The theorem has a long list of assumptions, which we verify for the proof of the main theorems in Sections
B and C below. The coercivity (26) is the only major assumption that remains in the main theorems. The
second assumption (27) is used to show that the weights do not move far from their initial and the third
(28) provides bounds for the learning rate. The next two assumptions (29) and (30) are major components
of NTK analysis and require that the NTK is Hölder continuous and that the empirical NTK concentrates
close to the infinite width limit. Finally, assumption (31) bounds the difference between the gradient of the
continuous L2loss and the discrete loss ℓ. The last assumption is the major concern of this paper, while all
others have been established in Welper (2024b;a).
The proof is identical to Welper (2024a, Theorem 3.1), with one difference: The error reduction lemma
Welper (2024a, Lemma 3.2) is replaced with Lemma A.2 below. This introduces a new error term form the
sample loss. While this extra error term only requires minimal changes in the proof, for the convenience of
the reader, we include it in Section A.3.
A.2 Gradient Descent Error Reduction
The first step in our convergence proof establishes an error decay in every gradient descent step. It matches
Welper (2024a, Lemma 3.2) up to an additional error term ∆sample (m,J)for the difference between the
continuous and discrete losses.
Lemma A.2. Assume that (28),(29),(30)and(31)hold. Assume that/vextenddouble/vextenddoubleθ0−θn/vextenddouble/vextenddouble
∗≤h. Then
ℓS(θn+1)−ℓS(θn)≤−γ⟨κ,Hκ⟩S+cγhα∥κ∥0∥κ∥S+γ∆sample (m,N )∥κ∥0∥κ∥S.
Proof.Applying the mean value theorem to the gradient descent step θn+1=θn−∆nwith ∆n:=γ∇θℓ(θn),
we obtain
ℓS(θn+1)−ℓS(θn) =ℓS(θn−∆n)−ℓS(θn)
=−ℓ′
S(θn−ξ∆n)∆n,
for someξ∈(0,1). Abbreviating κ=κnand plugging in the derivatives ℓ′
S(θ)T= [⟨κ,∂rfθ⟩S]R
r=1and
∆n=γ[∂rℓ(θ)]R
r=1, yields
ℓS(θn+1)−ℓS(θn) =−γ/summationdisplay
r⟨κ,∂rfθn−ξ∆n⟩S∂rℓ(θn).
23Under review as submission to TMLR
Next, we replace the derivative ∂rℓ(θ)of the discrete loss by the corresponding derivative of the continuous
loss∂rℓ0(θ) =⟨κ,∂rfθ⟩
ℓS(θn+1)−ℓS(θn) =−γ/summationdisplay
r⟨κ,∂rfθn−ξ∆n⟩S[∂rℓ0(θn) + [∂rℓ(θn)−∂rℓ0(θn)]]
=−γ/summationdisplay
r⟨κ,∂rfθn−ξ∆n⟩S⟨κ,∂rfθn⟩
−γ/summationdisplay
r⟨κ,∂rfθn−ξ∆n⟩S[∂rℓ(θn)−∂rℓ0(θn)]
=: (I) + (II).
Let us first estimate (I). To this end, we express ⟨v,κ⟩by theH0dualv∗κ:=⟨v,κ⟩and obtain
(I) =−γ/summationdisplay
r⟨κ,∂rfθn−ξ∆n⟩S∂r(fθn)∗(κ)
=−γ/angbracketleftigg
κ,/bracketleftigg/summationdisplay
r(∂rfθn−ξ∆n)(∂rfθn)∗/bracketrightigg
κ/angbracketrightigg
S,
=−γ⟨κ,Hθn−ξ∆n,θnκ⟩S.
Adding and subtracting terms to compare fθn−ξ∆nandfθnwith the initial fθ0, we obtain
(I) =−γ/angbracketleftbig
κ,Hθ0,θ0κ/angbracketrightbig
S
+γ/angbracketleftbig
κ,Hθ0,θ0−Hθ0,θnκ/angbracketrightbig
S
+γ/angbracketleftbig
κ,Hθ0,θn−Hθn−ξ∆n,θnκ/angbracketrightbig
S.
Assumption (30) implies
−/angbracketleftbig
κ,Hθ0,θ0κ/angbracketrightbig
S=−⟨κ,Hκ⟩S+/angbracketleftbig
κ,H−Hθ0,θ0κ/angbracketrightbig
S≤−⟨κ,Hκ⟩S+hα∥κ∥0∥κ∥S
and Assumption (29), with ¯h=hand¯h=h+∥∆n∥∗implies
∥Hθ0,θ0−Hθ0,θn∥S,0≤chα,
∥Hθ0,θn−Hθn−ξ∆n,θn∥S,0≤c(h+∥∆n∥∗)α.
Combining these inequalities, yields
(I)≤−γ⟨κ,Hκ⟩S+ 3cγ/bracketleftbig
h+∥∆n∥∗/bracketrightbigα∥κ∥0∥κ∥S,
≤−γ⟨κ,Hκ⟩S+cγhα∥κ∥0∥κ∥S,
where in the last step we have used that ∥∆n∥∗=γ∥∇θℓ(θn)∥∗≲hby assumption (28).
It remains to bound (II). Again, using∥∆n∥∗≲hand the assumptions of this lemma, we have ∥θ0−(θn+
ξ∆n)∥≲hfor allξ∈[0,1]. Thus, with Assumption (31) and our abbreviation κ=κn, we obtain
(II) =−γ/summationdisplay
r⟨κ,∂rfθn−ξ∆n⟩S[∂rℓ(θn)−∂rℓ0(θn)]
≤γ∆sample (m,N )∥κ∥0∥κ∥S.
Combining (I)and(II)yields the lemma.
A.3 Proof of Theorem A.1
The following proof is identical to Welper (2024a, Theorem 3.1) up to the additional error term
∆sample (m,N ). We include the proof to trace the minor modification and keep the paper self contained.
24Under review as submission to TMLR
Proof of Theorem A.1. The proof is based on the gradient descent error reduction in Lemma A.2. All of its
assumptions are given, except the weight distance/vextenddouble/vextenddoubleθn−θ0/vextenddouble/vextenddouble
∗, which we include in the induction hypothesis:
We assume
∥κk∥2
0≲e−γ[hα+∆]k∥κk∥2
0,
hk:= max
l≤k/vextenddouble/vextenddoubleθl−θ0/vextenddouble/vextenddouble
∗≲chm−1
21
1+α=:h
for allk≤nand∆ = ∆sample (m,N )and prove the case k=n+ 1by induction. With the induction
hypothesis and assumptions (28), (29), (30) and (31), Lemma A.2 together with coercivity (1) implies
∥κn+1∥2
0−∥κ0∥2
0≤−γ∥κn∥2
−β+cγhα∥κn∥2
0+γ∆∥κn∥2
0.
∥κn+1∥2
s−∥κ0∥2
s≤−γ∥κn∥2
s−β+cγhα∥κn∥0∥κn∥s+γ∆∥κn∥0∥κn∥s,
or shorter
∥κn+1∥2
0−∥κ0∥2
0≤−γ∥κn∥2
−β+γ[chα+ ∆]∥κn∥2
0.
∥κn+1∥2
s−∥κ0∥2
s≤−γ∥κn∥2
s−β+γ[chα+ ∆]∥κn∥0∥κn∥s.
This is not a closed iteration of the ∥κn∥2
0and∥κ∥2
sresiduals because of the ∥·∥−βand∥·∥s−βnorms. We
eliminate them with the interpolation inequalities 25
∥κ∥0≤∥κ∥s
β+s
−β∥κ∥β
β+ss ⇒ −∥ κ∥2
−β≤−∥κ∥2+2β
s
0∥κ∥−2β
ss,
∥κ∥0≤∥κ∥s
β
s−β∥κ∥β−s
βs ⇒ −∥ κ∥2
s−β≤−∥κ∥2β
s
0∥κ∥2−2β
ss
so that
∥κn+1∥2
0−∥κ0∥2
0≲−γ∥κn∥2+2β
s
0∥κn∥−2β
ss+γ[hα+ ∆]∥κn∥2
0,
∥κn+1∥2
s−∥κ0∥2
s≲−γ∥κn∥2β
s
0∥κn∥2−2β
ss +γ[hα+ ∆]|κn∥0∥κn∥s.
Error bounds for this iteration are given in Lemma D.5 with xn+1:=∥κn+1∥2
0,yn+1:=∥κ∥2
s,ρ=β/s,
a=c= 1andb=d=hα+ ∆. To show the lemma’s assumption (48), we use 2s≤βso that
/parenleftbigg
2−s
β/parenrightbigg
≤2⇔s
β≤2s
β
2−s
β⇔1
ρ≤2
2ρ−1.
Hence, assumption (32) implies
xk=∥κk∥2
0≥/parenleftig/parenleftig
m−1
21
1+α/parenrightigα
+ ∆/parenrightigs
β∥κ0∥2
s= (hα+ ∆)s
β∥κ0∥2
s≳/parenleftbigg
2b
a/parenrightbigg1
ρ
y0≳/parenleftbiggd
c/parenrightbigg 2
2ρ−1
y0.
and Lemma D.5 is applicable. Therefore, we obtain
∥κn+1∥2
0≲e−γ[hα+∆](n+1)∥κ0∥2
0, ∥κn+1∥2
s≲∥κ0∥2
s,
which shows the first induction hypothesis.
It remains to bound hn+1to show the second induction hypothesis. To this end note that
hn+1= max
k≤n+1/vextenddouble/vextenddoubleθk−θ0/vextenddouble/vextenddouble
∗≲γ√mn/summationdisplay
k=1∥κk∥0≲γ√mn/summationdisplay
k=1e−γ[hα+∆]k∥κ0∥0,
where in the second step we have used assumption (27) and in the third step the induction hypothesis. We
bound the latter sumn/summationdisplay
k=1e−γ[hα+∆]k≤/integraldisplay∞
0e−γ[hα+∆]kdk=1
γ[hα+ ∆],
25Under review as submission to TMLR
to conclude that
hn+1≤cγ√m1
γ[hα+ ∆]∥κ0∥0≤cγ√m1
γhα∥κ0∥0≤c√mh−α∥κ0∥0=h.
In the last step we have used that the definition of himplies
h=chm−1
21
1+α=ch√mm1
2α
1+α=c√mm1
2α
1+α∥κ0∥0=c√mh−α∥κ0∥0
for constant ch=c∥κ0∥0dependent on∥κ0∥0. Together with the first induction hypothesis this concludes
the proof.
B Sampling
In this section, we prove bounds for the error ∆sample (m,N )between continuous and sample loss (Lemma
B.4) and then Theorem 2.2. Throughout the section, we use the following regularity conditions on the
activation function
|σ(x)|≲|x|, (33)
|σ(x)−σ(¯x)|≲|x−¯x| (34)
|˙σ(x)|≲1. (35)
Moreover, for the time being, we assume that the weight matrices are bounded
/vextenddouble/vextenddoubleWℓ/vextenddouble/vextenddoublen−1/2
ℓ≲1,/vextenddouble/vextenddouble¯Wℓ/vextenddouble/vextenddoublen−1/2
ℓ≲1,∥x∥≲1∀x∈D. (36)
TypicallyWℓwill be the initial weight and ¯Wℓthe weight of a later gradient descent step. Likewise, we
denote by ¯θ,f¯θand ¯fℓ, etc. the weights, networks and layers based on the perturbed ¯Wℓ. For the main
theorems, these bounds follow from properties of random matrices at the initial weights and the observation
that weights do not move far from their initial in (27).
Throughout the section, we abbreviate D=Sd−1,L2=L2(D),Hs=Hs(D)and⟨·,·⟩Hs=⟨·,·⟩Hs(D), when
convenient.
B.1 Concentration
In this section, for the sample loss ℓ(θ) =1
2N/summationtextN
i=1|fθ(xi)−f(xi)|2, we bound the sample error
sup
θ,¯θ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleR/summationdisplay
r=1/angbracketleftbig
κk,∂rf¯θ/angbracketrightbig
HS[∂rℓ(θ)−∂rℓ0(θ)]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
This establishes assumption (31) in the abstract convergence Theorem A.1 and leads to the proof of the first
main result.
Let us abbreviate
Yθ:=R/summationdisplay
r=1⟨κ,∂rf¯θ⟩HS[∂rℓ(θ)−∂rℓ0(θ)],
which is a random variable with respect to the sample points xiimplicitly contained in the discrete loss ℓ(θ).
Since theℓ0loss is the expectation of the sample loss ℓ, it is easy to see that E[Yθ] = 0and is suffices to show
concentration results. These follow from Dudley’s inequality, for which we proof that Yθhas sub-gaussian
increments, i.e.
∥Yθ−Yϑ∥ψ2≲∥θ−ϑ∥∗.
26Under review as submission to TMLR
Throughout the section, we use Orlicz and weight norms
∥X∥ψ2= inf/braceleftbig
t>0 :E/bracketleftbig
exp(X2/t2)/bracketrightbig
≤2/bracerightbig
,
∥θ∥∗:=∥WL−1∥m−1/2
L−1,
where we have used that θ=WL−1because we only train the second but last layer. ∥·∥denotes the
Euclidean norm for vectors and the induced matrix norm for matrices. As in the introduction, we abbreviate
D:=Sd−1.
Before we prove sub-gaussian increments, we bound several components involved in Yθseparately. We start
with the factor⟨κ,∂rfθ⟩Hs.
Lemma B.1. Let0≤s<1and assume σsatisfies (33),(34), the weights satisfy (36)and are of equivalent
sizem0∼···∼mL−1. Then
R/summationdisplay
r=1⟨κ,∂rfθ⟩2
Hs≲∥κ∥2
Hs.
Proof.By Lemma D.2 cited from Welper (2024b), with ϵsufficiently small so that s+ϵ <1the bilinear
form
B(u,v) =/integraldisplay
D/integraldisplay
Du(x)/parenleftiggR/summationdisplay
r=1∂rfθ(x)∂rfθ(y)/parenrightigg
v(y)dxdy
=R/summationdisplay
r=1⟨u,∂rfθ⟩⟨∂rfθ,v⟩
foruandvinL2is bounded by
B(u,v)≲∥u∥H−s∥v∥H−s
and can therefore be extended to all u,v∈H−s. In this case the L2inner product⟨·,·⟩turns into the
H−s×Hsdual pairing. Denoting by R:Hs→H−sthe Riesz map, we obtain
R/summationdisplay
r=1⟨κ,∂rfθ⟩2
Hs=R/summationdisplay
r=1⟨κ,∂rfθ⟩Hs⟨∂rfθ,κ⟩Hs
=R/summationdisplay
r=1⟨Rκ,∂rfθ⟩⟨∂rfθ,Rκ⟩=B(Rκ,Rκ )≲∥Rκ∥2
H−s=∥κ∥2
Hs,
which proves the lemma
Next, we consider the loss ∂rℓ(θ) =1
N/summationtextN
i=iκ(xi)∂rfθ(xi)and show sub-gaussian increments for the sum-
mands. As usual, we abbreviate L∞(D) =L∞.
Lemma B.2. Let0<s< 1and assume∥f∥L∞≲m1/2,σsatisfies (33),(34),(35)and the weights satisfy
(36). LetXbe a uniform random variable with values in Dand set
Xθ:=κ(X)∇fθ(X)−E[κ(X)∇fθ(X)]
Then
/vextenddouble/vextenddouble∥Xθ−Xϑ∥/vextenddouble/vextenddouble
ψ2≲m1/2∥θ−ϑ∥∗,/vextenddouble/vextenddouble∥Xθ∥/vextenddouble/vextenddouble
ψ2≲m1/2.
Note thatκ(X)is a scalar and∇fθ(X) =∇WL−1fθ(X)is a matrix. With the convention ∂WL−1
ij=∂rfor
suitabler, we may also regard ∇fθ(X)as a vector and thus ∥κ(X)∇fθ(X)∥as the Euclidean vector norm,
which we do in the following.
27Under review as submission to TMLR
Proof.We splitXθ=Zθ−E[Zθ]into a random part and an expectation, with
Zθ:=κ(X)∇fθ(X)
and estimate both terms separately.
1.Estimate/vextenddouble/vextenddouble∥Zθ−Zϑ∥/vextenddouble/vextenddouble
ψ2:First, we upper bound the ψ2-norm by the L∞-norm and separate the
factorκ:/vextenddouble/vextenddouble∥Zθ−Zϑ∥/vextenddouble/vextenddouble2
ψ2≤/vextenddouble/vextenddouble∥Zθ−Zϑ∥/vextenddouble/vextenddouble2
L∞=/vextenddouble/vextenddouble∥κ∇fθ−κ∇fϑ∥/vextenddouble/vextenddouble2
L∞
= sup
x∈DR/summationdisplay
r=1[κ(x) [∂rfθ(x)−∂rfϑ(x)]]2
≤∥κ∥2
L∞∥∥∇fθ−∇fϑ∥∥2
L∞.(37)
The first factor is bounded by
∥κ∥L∞≤∥f∥L∞+∥f¯θ∥L∞≲m1/2,
where we have used that κ=f−f¯θfor some weights ¯θthat satisfy (36), that ∥f∥L∞≲m−1/2by
assumption and∥f¯θ∥L∞≲m−1/2by Lemma D.3, cited from Welper (2024b).
To bound the second factor, recall that the derivatives ∂rforr∈{1,...,R}are shorthand for the
derivatives ∂WL−1
ijof the second but last layer. A short calculation shows that
∂WL−1
ijfθ=WL
im−1/2
Lm−1/2
L−1/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
:=wi˙σ/parenleftbig
fL
i/parenrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:ui(θ)σ/parenleftbig
fL−1
j/parenrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:vj, (38)
see e.g. Welper (2024b, Proof of Lemma 6.18) for details. The weights WLof the last layer have
only one index because the network is scalar valued. The layer fL−1does not depend on WL−1and
thereforevjdoes not depend on θ=WL−1. Then, for all x∈D, we have
∥∇fθ(x)−∇fϑ(x)∥2=/summationdisplay
ij[wiui(θ)vj−wiui(ϑ)vj]2
=/summationdisplay
ijw2
i[ui(θ)−ui(ϑ)]2v2
j
=/bracketleftigg/summationdisplay
iw2
i[ui(θ)−ui(ϑ)]2/bracketrightigg
/summationdisplay
jv2
j

≤∥w∥2
ℓ∞∥u(θ)−u(ϑ)∥2∥v∥2.
SinceWL
i=±1, we have
∥w∥ℓ∞≲m−1
and from Lemma D.3 cited from Welper (2024b), together with the Lipschitz continuity of σand ˙σ,
we have
∥u(θ)−u(ϑ)∥≲m1/2∥θ−ϑ∥∗, ∥v∥≲m1/2.
Thus, we obtain
∥∇fθ(x)−∇fϑ(x)∥≲∥θ−ϑ∥∗
for allx∈Dand therefore together with (37) that
/vextenddouble/vextenddouble∥Zθ−Zϑ∥/vextenddouble/vextenddouble
ψ2≲m1/2∥θ−ϑ∥∗.
28Under review as submission to TMLR
2.Estimate∥E[Zθ]−E[Zϑ]∥:First, using that Xis uniform, we factor out the residual κ:
∥E[Zθ]−E[Zϑ]∥2≤R/summationdisplay
r=1/bracketleftbigg1
|D|/integraldisplay
Dκ(x) [∂rfθ(x)−∂rfϑ(x)]dx/bracketrightbigg2
=1
|D|∥κ∥2
L2R/summationdisplay
r=11
|D|/integraldisplay
D|∂rfθ(x)−∂rfϑ(x)|2dx
≤∥κ∥2
L∞/vextenddouble/vextenddouble∥∂rfθ−∂rfϑ∥/vextenddouble/vextenddouble2
L∞
where in the first inequality we have used Cauchy-Schwarz and in the last one exchanged the order
of sum and integral. The right hand side is identical to (37) and bounded the same way.
Combining the estimates for Zθand its expectation and using that the ψ2-norm of a non-negative constant
is upper bounded by the constant itself, we obtain
/vextenddouble/vextenddouble∥Xθ−Xϑ∥/vextenddouble/vextenddouble
ψ2≤/vextenddouble/vextenddouble∥Zθ−Zϑ∥/vextenddouble/vextenddouble
ψ2+/vextenddouble/vextenddouble∥E[Zθ]−E[Zϑ]∥/vextenddouble/vextenddouble
ψ2≲m1/2∥θ−ϑ∥∗,
which shows the first part of the lemma.
The proof of the second bound/vextenddouble/vextenddouble∥Xθ∥/vextenddouble/vextenddouble
ψ2≲m1/2is identical upon replacing XϑandZϑwith 0 throughout
the proof. Using ˙σ(x)≲1, we obtain∥u(θ)∥≲m1/2instead of∥u(θ)−u(ϑ)∥≲m1/2∥θ−ϑ∥∗by Lemma
D.3. Omitting the factor ∥θ−ϑ∥∗in the left hand side throughout the rest of the proof shows the second
part of the lemma.
The next lemma establishes the sub-gaussian increments of the random variable Yθ.
Lemma B.3. Let0<s< 1and assume∥f∥L∞≲m1/2,σsatisfies (33),(34),(35)and the weights satisfy
(36). LetXbe a uniform random variable on Dand forθ,¯θ∈Θset
Yθ:=R/summationdisplay
r=1⟨κ,∂rf¯θ⟩HS[∂rℓ(θ)−∂rℓ0(θ)].
Then
/vextenddouble/vextenddoubleYθ−Yϑ/vextenddouble/vextenddouble
ψ2≲/parenleftigm
N/parenrightig1/2
∥κ∥HS∥θ−ϑ∥∗,/vextenddouble/vextenddoubleYθ/vextenddouble/vextenddouble
ψ2≲/parenleftigm
N/parenrightig1/2
∥κ∥HS.
Proof.Plugging in the definitions of the loss ℓand its continuum limit ℓ0, we have
∂rℓ(θ) =1
NN/summationdisplay
i=1κ(xi)∂rfθ(xi), ∂ rℓ0(θ) =⟨κ,∂rfθ⟩=E[κ∂rfθ]
and therefore
Yθ=1
NN/summationdisplay
i=1R/summationdisplay
r=1⟨κ,∂rf¯θ⟩HS/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:ur[κ(xi)∂rfθ(xi)−E[κ(X)∂rfθ(X)]]/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
=:(Xi
θ)r=1
NN/summationdisplay
i=1uTXi
θ.
With Hoeffding’s inequality, we estimate the ψ2-norm by
/vextenddouble/vextenddoubleYθ−Yϑ/vextenddouble/vextenddouble2
ψ2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
NN/summationdisplay
i=1uT(Xi
θ−Xi
ϑ)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
ψ2≲1
N2N/summationdisplay
i=1/vextenddouble/vextenddoubleuT(Xi
θ−Xi
ϑ)/vextenddouble/vextenddouble2
ψ2.
29Under review as submission to TMLR
The argument of the ψ2-norm is bounded by |uT(Xi
θ−Xi
ϑ)|≤∥u∥∥Xi
θ−Xi
ϑ∥and thus
/vextenddouble/vextenddoubleYθ−Yϑ/vextenddouble/vextenddouble2
ψ2≤1
N2N/summationdisplay
i=1∥u∥2/vextenddouble/vextenddouble∥Xi
θ−Xi
ϑ∥/vextenddouble/vextenddouble2
ψ2
≲1
N2N/summationdisplay
i=1∥κ∥2
HSm∥θ−ϑ∥2
∗
≲m
N∥κ∥2
HS∥θ−ϑ∥2
∗,
where the last inequalities follows from
∥u∥≲∥κ∥HS,/vextenddouble/vextenddouble∥Xi
θ−Xi
ϑ∥/vextenddouble/vextenddouble
ψ2≲m1/2∥θ−ϑ∥∗,
by Lemmas B.1 and B.2, respectively. Taking the square root completes the proof of the first inequality.
The second follows analogously by replacing YϑandXϑwith zero throughout the proof.
Lemma B.4. Let0<s< 1and assume∥f∥L∞≲m1/2,σsatisfies (33),(34),(35)and the weights satisfy
(36). Let Θh:={θ∈Θ|/vextenddouble/vextenddoubleθ−θ0/vextenddouble/vextenddouble
∗≤h}forh≲1and some initial (trained) weight θ0that satisfies (36).
Then with probability at least 1−2 exp(−m2h2)
sup
θ∈Θh/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleR/summationdisplay
r=1⟨κ,∂rf¯θ⟩HS[∂rℓ(θ)−∂rℓ0(θ)]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≲/parenleftigm
N/parenrightig1/2
mh∥κ∥HS.
Proof.We abbreviate
Yθ:=R/summationdisplay
r=1⟨κ,∂rf¯θ⟩HS[∂rℓ(θ)−∂rℓ0(θ)]
and prove the lemma with Dudley’s inequality. We have established in Lemma B.3 that Yθhas sub-gaussian
increments/vextenddouble/vextenddoubleYθ−Yϑ/vextenddouble/vextenddouble
ψ2≲/parenleftigm
N/parenrightig1/2
∥κ∥HS∥θ−ϑ∥∗.
Next, weestimatethecoveringnumbersinDudley’sinequality. Thesetofeligibleparameters Θhiscontained
in the ball of radius hinRRin the∥·∥∗-norm. Hence, for every ϵ≥0there is an ϵ-covering of at most
N(ϵ)≤/parenleftbigg3h
ϵ/parenrightbiggR
ϵ-balls, see e.g. Lorentz et al. (1996, Chapter 15, Proposition 1.3). Then, using logx≤x−1≤x,
/integraldisplay∞
0/radicalbig
logN(ϵ)dϵ=R1/2/integraldisplayh
0/radicaligg
log/parenleftbigg3h
ϵ/parenrightbigg
dϵ≤R1/2/integraldisplayh
0/radicaligg/parenleftbigg3h
ϵ/parenrightbigg
dϵ=1
2(3hR)1/2h1/2≲mh,
where in the last step we have used that R1/2∼m. Hence, Dudley’s inequality (with tail bounds) implies
that for all u≥0
sup
θ,ϑ∈Θh|Yθ−Yϑ|≲/parenleftigm
N/parenrightig1/2
∥κ∥HS/bracketleftbigg/integraldisplay∞
0/radicalbig
logN(ϵ)dϵ+uh/bracketrightbigg
holds with probability at least 1−2 exp(−u2). Choosing u=myields
sup
θ,ϑ∈Θh|Yθ−Yϑ|≲/parenleftigm
N/parenrightig1/2
mh∥κ∥HS,
30Under review as submission to TMLR
with probability at least 1−2 exp(−m2). The sub-gaussian bound/vextenddouble/vextenddoubleYθ/vextenddouble/vextenddouble
ψ2≲/parenleftbigm
N/parenrightbig1/2∥κ∥HSfrom Lemma
B.3 implies
|Yθ0|≲/parenleftigm
N/parenrightig1/2
mh∥κ∥HS.
with probability at least 1−2 exp(−m2h2). Then, the lemma follows from supθ∈Θh|Yθ|≤supθ∈Θh|Yθ−
Yθ0|+|Yθ0|.
B.2 Convergence: Proof of Theorem 2.2
Proof of Theorem 2.2. The theorem follows from Theorem A.1 with Hs=HsandH0=L2, for which we
have to verify all assumptions. Most of them have been established in the proof of Welper (2024a, Theorem
2.2), which is identical to the one of this paper, except that it uses a continuous L2loss instead of a sample
loss. This results in the extra assumption (31), which is the only one left to verify.
By Lemma B.4, with probability at least 1−2 exp(−m2h2)we have
sup
θ∈Θh/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleR/summationdisplay
r=1/angbracketleftbig
κk,∂rf¯θ/angbracketrightbig
HS[∂rℓ(θ)−∂rℓ0(θ)]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≲m3/2
N1/2h∥κk∥HS,
which provides an estimate for ∆sample (m,N )up to a missing factor ∥κk∥L2. To insert this factor, we use
the lower bound from Assumption (32): For k≤n
∥κk∥2
L2≥ca(hα+ ∆sample (m,N ))s
β∥κ0∥2
Hs≥cahαs
β∥κ0∥2
Hs,
which implies
1≲h−αs
2β∥κ0∥−1
Hs∥κk∥L2.
Inserting this into the application of Lemma B.4 above, we obtain
sup
θ∈Θh/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleR/summationdisplay
r=1/angbracketleftbig
κk,∂rf¯θ/angbracketrightbig
HS[∂rℓ(θ)−∂rℓ0(θ)]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≲/bracketleftbiggm3/2
N1/2h1−αs
2β∥κ0∥−1
Hs/bracketrightbigg
∥κk∥L2∥κk∥HS.
Comparing to the definition of ∆sample (m,N )in assumption (31), we obtain
∆sample (m,N )≲m3/2
N1/2h1−αs
2β∥κ0∥−1
Hs,
which shows the assumption.
It remains to consider the success probability. From the parts of the proof shown in Welper (2024a),
the bounds fail with probability cL(e−m+e−τ)and from above with probability e−cm2h2≤ecmbecause
h=chm−1
21
1+α≥m−1/2for anyα>0from (28).
This completes the proof, together with an index shift n+ 1→nbetween the statements of Theorems A.1
and 2.2.
C Kernels
In this section, we prove Theorem 2.3 based on bounds for the error ∆sample (m,N )between the L2(D)loss
and the kernel loss
ℓk(θ) :=1
2NN/summationdisplay
i=1⟨k(xi,·),fθ−f⟩2
31Under review as submission to TMLR
for kernelsk(x,y)withx,y∈D=Sd−1andxiuniformly and independently sampled on the domain D. We
denote the corresponding expected loss by
¯ℓk(θ) :=E/bracketleftbig
ℓk(θ)/bracketrightbig
:=1
2|D|/integraldisplay
D⟨k(x,·),fθ−f⟩2dx.
We consider this expectation in Section C.1, which is generally not identical to1
2∥fθ−f∥2
L2(D). Then, we
show corresponding concentration inequalities based on matrix Bernstein inequalities in Section C.2. The
proof of Theorem 2.3 is in Section C.3.
Throughout this section, we abbreviate D=Sd−1,L2=L2(D),Hs=Hs(D)and⟨·,·⟩Hs=⟨·,·⟩Hs(D), when
convenient.
C.1 Expectation
Define the inner product and norm that give rise to the expected kernel loss as
⟨u,v⟩k=E/bracketleftigg
1
NN/summationdisplay
i=1⟨u,k(xi,·)⟩⟨k(xi,·),v⟩/bracketrightigg
,∥·∥2
k:=⟨·,·⟩k, ¯ℓk(θ) :=1
2∥fθ−f∥2
k.(39)
Unfortunately, the norm ∥·∥kis not equivalent to the L2norm and therefore the expected loss ¯ℓk(θ)is not
equivalent to the L2lossℓ0(θ). To bridge this gap, in this section, we construct a modified inner product
and corresponding norm and loss
⟨u,v⟩♯∥·∥2
♯:=⟨·,·⟩♯, ¯ℓ♯(θ) :=1
2∥fθ−f∥2
♯, (40)
with the following properties:
1. The two inner products ⟨·,·⟩kand⟨·,·⟩♯are close for smooth functions.
2. The norm∥·∥2
♯and loss ¯ℓ♯(θ)are equivalent to ∥·∥L2andℓ0(θ), respectively.
To this end, we first characterize the inner product ⟨·,·⟩k.
Lemma C.1. Assume the symmetric kernel k:D×D→RhasL2-orthogonal eigenfunctions ψjwith
eigenvalues λj. Then for any u,v∈L2
⟨u,v⟩k=E/bracketleftigg
1
NN/summationdisplay
i=1⟨u,k(·,xi)⟩⟨k(xi,·),v⟩/bracketrightigg
=∞/summationdisplay
r=1λ2
j⟨u,ψr⟩⟨ψr,v⟩.
Proof.We denote the expectation by E. By the law of large numbers, we have
E:=E/bracketleftigg
1
NN/summationdisplay
i=1⟨u,k(·,xi)⟩⟨k(xi,·),v⟩/bracketrightigg
=1
|D|/integraldisplay
D⟨u,k(·,x)⟩⟨k(x,·),v⟩dx.
Plugging in u,vin eigenbasis
u=∞/summationdisplay
s=1⟨u,ψs⟩ψs, v =∞/summationdisplay
t=1⟨u,ψt⟩ψt,
we obtain
E=∞/summationdisplay
s,t=1⟨u,ψs⟩⟨v,ψt⟩/integraldisplay
D⟨ψs,k(·,x)⟩⟨k(x,·),ψt⟩dx
=∞/summationdisplay
s,t=1⟨u,ψs⟩⟨v,ψt⟩λsλt1
|D|/integraldisplay
Dψs(x)ψt(x)dx
=∞/summationdisplay
s=1λ2
s⟨u,ψs⟩⟨ψs,v⟩,
32Under review as submission to TMLR
where in the second step we have used eigenvalue and vector definition ⟨ψs,k(·,x)⟩=λsψs(x)and in the
last step we have used that ψsis a orthonormal basis, with normalized squared expectation.
In the next lemma, we define the modified inner product ⟨·,·⟩♯and prove the properties from the introduction
of this section.
Lemma C.2. Assume the symmetric kernel k:D×D→RhasL2-orthogonal eigenfunctions ψjand eigen-
valuesλjwith
1≲λj≲1, j≤J,
λj≲1, j >J.
Define a lower bounded perturbation ¯λjof the eigenvalue λjand the corresponding inner product
¯λj:=/braceleftbiggλj j≤J
max{λj,1}j≥J.⟨u,v⟩♯:=∞/summationdisplay
r=1¯λ2
j⟨u,ψr⟩⟨ψr,v⟩.
For some increasing weights µj≥1let
∥v∥2
¯Hs:=J/summationdisplay
j=1µ2s
j⟨ψj,v⟩2
be the norm of a Hilbert space ¯Hs. Then for all u,v∈L2ands≥0
/vextendsingle/vextendsingle/vextendsingle⟨u,v⟩k−⟨u,v⟩♯/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∞/summationdisplay
j=1/bracketleftbig
λ2
j−¯λ2
j/bracketrightbig
⟨u,ψj⟩⟨ψj,v⟩/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤µ−s
J∥u∥L2∥v∥¯Hs,
and the induced norm ∥·∥2
♯:=⟨·,·⟩♯is equivalent to the L2norm.
The weights µjand ¯Hsdefine a smoothness space, which is left generic for now but will be replaced by
Sobolev spaces below.
Proof.We have
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∞/summationdisplay
j=1/bracketleftbig
λ2
j−¯λ2
j/bracketrightbig
⟨u,ψj⟩⟨ψj,v⟩/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j>J/bracketleftbig
λ2
j−¯λ2
j/bracketrightbig
⟨u,ψj⟩⟨ψj,v⟩/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤
/summationdisplay
j>J/vextendsingle/vextendsingleλ2
j−¯λ2
j/vextendsingle/vextendsingle⟨u,ψj⟩2
1/2
/summationdisplay
j>J/vextendsingle/vextendsingleλ2
j−¯λ2
j/vextendsingle/vextendsingle⟨v,ψj⟩2
1/2
≤
/summationdisplay
j>J⟨u,ψj⟩2
1/2
/summationdisplay
j>J⟨v,ψj⟩2
1/2
≤µ−s
J∥u∥L2∥v∥¯Hs,
where the first equality follows from λj=¯λjforj≤J, the second from Cauchy-Schwarz and the third from
0≤¯λ2
j−λ2
j≤1by definition of ¯λjforj >J. The last inequality follows from
/summationdisplay
j>J⟨u,ψj⟩2≤∥u∥2
L2
and/summationdisplay
j>J⟨u,ψj⟩2=µ−2s
J/summationdisplay
j>Jµ2s
J⟨u,ψj⟩2≤µ−2s
J/summationdisplay
j>Jµ2s
j⟨u,ψj⟩2=µ−2s
J∥u∥2
¯Hs.
Finally, by construction, the eigenvalues ¯λj∼1are upper and lower bounded by one and therefore the norm
∥·∥♯is equivalent to the L2norm.
33Under review as submission to TMLR
C.2 Concentration
We show concentration for
R/summationdisplay
r=1⟨κ,∂rf¯θ⟩Hs/bracketleftbig
∂rℓk(θ)−∂r¯ℓk(θ)/bracketrightbig
which matches Assumption (31), except for the wrong expected loss ¯ℓk(θ)instead of ℓ0(θ), which will
be considered in the next section. Throughout this section, we denote the adjoint of u, byu∗, so that
⟨f,u⟩=f∗v. For weights θ,¯θ∈Θ, we use the abbreviation
Fs
θ,¯θκ:=R/summationdisplay
r=1∂rfθ⟨∂rf¯θ,κ⟩Hs (41)
which shows up repeatedly in the following proofs. We first bound its norm.
Lemma C.3. Assume that σsatisfies the growth and Lipschitz conditions (33),(34)and may be different
in each layer. Assume all weights in the networks, θ,¯θand the domain are bounded (36). Then for
0≤S≤s<1
∥FS
θ,¯θκ∥Hs≤∥κ∥Hs.
Proof.Let⟨·,·⟩be theH−S×HSdual pairing and R:HS→H−Sthe corresponding Riesz map. Since the
Hs-norm is the dual of H−s, we have
∥FS
θ,¯θκ∥Hs=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleR/summationdisplay
r=1∂rfθ⟨∂rf¯θ,κ⟩HS/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Hs
= sup
∥v∥H−s≤1/angbracketleftigg
v,R/summationdisplay
r=1∂rfθ⟨∂rf¯θ,κ⟩HS/angbracketrightigg
= sup
∥v∥H−s≤1/angbracketleftigg
v,/parenleftiggR/summationdisplay
r=1∂rfθ∂rf∗
¯θ/parenrightigg
Rκ/angbracketrightigg
where in the last step we have used the definition of the adjoint functional f∗
¯θ∈H−Sso that⟨f¯θ,κ⟩HS=
⟨f¯θ,Rκ⟩=f∗
¯θ(Rκ). Formally, in this argument ⟨·,·⟩is theHS×H−Sdual pairing, which reduces to the L2
inner product in case Rκis inL2, see the proof of Lemma B.1 for more details. By Lemma D.2 cited from
Welper (2024b), with ϵsufficiently small so that s+ϵ<1, we obtain
∥Fs
θ,¯θκ∥Hs≲ sup
∥v∥H−s≤1∥v∥H−s∥Rκ∥H−s≤∥Rκ∥H−s≤∥Rκ∥H−S=∥κ∥HS≤∥κ∥Hs,
where we have used the embeddings ∥·∥H−s≤∥·∥H−Sand∥·∥HS≤∥·∥HsbecauseS≤s.
The next lemma shows the main concentration result.
Lemma C.4. Assume that σsatisfies the growth and Lipschitz conditions (33),(34)and may be different
in each layer. Assume all weights in the networks, θ,¯θand the domain are bounded (36). Assume the kernel
is bounded by supx∈D∥k(x,·)∥L2≤Ck. Then for 0≤s<1with probability at least 1−2t(et−t−1)−1we
have
R/summationdisplay
r=1⟨κ,∂rf¯θ⟩Hs/bracketleftbig
∂rℓk(θ)−∂r¯ℓk(θ)/bracketrightbig
≲C2
k/parenleftigg/radicalbigg
t
N+t
N/parenrightigg
∥κ∥L2∥κ∥Hs.
for allκ∈Hs.
Proof.The lemma states that the gradient of the sample loss is close to its average, with high probability,
which we show by the matrix Bernstein inequality. To this end, we first unravel the definition of the loss to
34Under review as submission to TMLR
define appropriate random matrices:
R/summationdisplay
r=1⟨κ,∂rf¯θ⟩Hs∂rℓk(θ) =R/summationdisplay
r=1⟨κ,∂rfθ⟩Hs/parenleftigg
1
NN/summationdisplay
i=1⟨k(xi,·),κ⟩⟨k(xi,·),∂rfθ⟩/parenrightigg
=1
NN/summationdisplay
i=1R/summationdisplay
r=1⟨κ,k(xi,·)⟩⟨k(xi,·),∂rfθ⟩⟨κ,∂rf¯θ⟩Hs
=/angbracketleftigg
κ,1
NN/summationdisplay
i=1k(xi,·)k(xi,·)∗R/summationdisplay
r=1∂rfθ⟨κ,∂rf¯θ⟩Hs/angbracketrightigg
=/angbracketleftig
κ,MFs
θ,¯θκ/angbracketrightig
withFs
θ,¯θκdefined in (41) and the integral kernel
M:=1
NN/summationdisplay
i=1k(xi,·)k(xi,·)∗.
With an analogous computation, we obtain the expectation with respect to the samples xi:
E/bracketleftiggR/summationdisplay
r=1⟨κ,∂rf¯θ⟩Hs∂rℓk(θ)/bracketrightigg
=/angbracketleftig
κ,¯MFs
θ,¯θκ/angbracketrightig
,
with
¯M:=E/bracketleftigg
1
NN/summationdisplay
i=1k(xi,·)k(xi,·)∗/bracketrightigg
=/integraldisplay
Dk(x,·)k(xi,·)∗dx.
It follows that
R/summationdisplay
r=1⟨κ,∂rf¯θ⟩Hs/bracketleftbig
∂rℓk(θ)−∂r¯ℓk(θ)/bracketrightbig
=/angbracketleftig
κ,[M−¯M]Fs
θ,¯θκ/angbracketrightig
≤ ∥κ∥L2∥M−¯M∥Hs→L2∥Fs
θ,¯θκ∥Hs(42)
The last term
∥Fs
θ,¯θκ∥Hs≲∥κ∥Hs (43)
is bounded by Lemma C.3 so that it remains to bound ∥M−¯M∥Hs→L2. To this end note that Mis a
sum of rank one operators, each depending on one independent sample xi. Hence, concentration follows
from dimension free matrix Bernstein inequalities (Hsu et al., 2012; Tropp, 2015; Minsker, 2017). We use a
corollary shown in Gentile & Welper (2022) and stated in the supplementary material Lemma D.4 for which
we only need to bound
∥k(x,·)∗∥(Hs)∗=∥k(x,·)∥H−s≤∥k(x,·)∥L2≤Ck
for allx∈D, provided by the assumptions of the lemma. With Lemma D.4 this provides
Pr/bracketleftigg
/vextenddouble/vextenddoubleM−¯M/vextenddouble/vextenddouble
Hs→L2≳C2
k/parenleftigg/radicalbigg
t
N+t
N/parenrightigg/bracketrightigg
≤2t/parenleftbig
et−t−1/parenrightbig−1.
Together with (42) and (43) this proves the lemma.
C.3 Convergence: Proof of Theorem 2.3
We first bound ∆sample (m,N )based on the results in the last two sections.
35Under review as submission to TMLR
Lemma C.5. Assume that σsatisfies the growth and Lipschitz conditions (33),(34)and may be different
in each layer. Assume all weights in the networks, θ,¯θand the domain are bounded (36). Assume the kernel
k:D×D→Ris zonal, i.e. k(x,y) =k(xTy), and has eigenvalues λljwith
1≲λlj≲1, l≤L,1≤j≤ν(l),
λlj≲1, l>L, 1≤j≤ν(l)
and index structure and ν(l)matching spherical harmonics (6)and
sup
x∈D∥k(x,·)∥L2≤Ck.
Then for 0≤S≤s<1with probability at least 1−2t[et−t−1]−1for allκ∈Hswe have
R/summationdisplay
r=1⟨κ,∂rf¯θ⟩HS/bracketleftbig
∂rℓk(θ)−∂r¯ℓ♯(θ)/bracketrightbig
≲C2
k/bracketleftigg/radicalbigg
t
N+t
N/bracketrightigg
∥κ∥L2∥κ∥HS+L−s∥κ∥L2∥κ∥Hs,
with loss ¯ℓ♯(θ)defined in (40).
Note that the conclusion of the lemma contains both Sands. This allows S=s, but also the choice S= 0
for which the last summand in the right hand side does not provide a meaningful bound if we insist that
S=s= 0.
Proof.We first split the difference into expectation and concentration components:
R/summationdisplay
r=1⟨κ,∂rf¯θ⟩HS/bracketleftbig
∂rℓk(θ)−∂r¯ℓ♯(θ)/bracketrightbig
=R/summationdisplay
r=1⟨κ,∂rf¯θ⟩HS/bracketleftbig
∂rℓk(θ)−∂r¯ℓk(θ)/bracketrightbig
+R/summationdisplay
r=1⟨κ,∂rf¯θ⟩HS/bracketleftbig
∂r¯ℓk(θ)−∂r¯ℓ♯(θ)/bracketrightbig
:= (I) + (II).
The first part (I)is bounded by Lemma C.4:
(I)≲C2
k/parenleftigg/radicalbigg
t
N+t
N/parenrightigg
∥κ∥L2∥κ∥HS,
with probability at least 1−2t(et−t−1)−1.
To estimate the expectation part (II), first note that the Funk-Hecke formula (Atkinson & Han, 2012)
implies that the eigenfunctions of the zonal kernel k(·,·)are spherical harmonics Yj
land thusL2orthogonal.
Hence, we obtain the loss
¯ℓ♯(θ) :=1
2∥fθ−f∥2
♯:=1
2/summationdisplay
l,j¯λ2
lj/angbracketleftig
Yj
l,fθ−f/angbracketrightig2
, ¯λj:=/braceleftbigg
λj j≤J,
max{λj,1}j≥J,
by Lemma C.2. The partial derivatives of the modified and expected kernel loss are given by
∂r¯ℓ♯(θ) =/summationdisplay
l,j¯λ2
lj/angbracketleftig
κ,Yj
l/angbracketrightig/angbracketleftig
Yj
l,∂rfθ/angbracketrightig
,
and
∂r¯ℓk(θ) =E/bracketleftigg
1
NN/summationdisplay
i=1⟨κ,k(xi,·)⟩⟨k(xi,·),∂rfθ⟩/bracketrightigg
=/summationdisplay
l,jλ2
lj/angbracketleftig
κ,Yj
l/angbracketrightig/angbracketleftig
Yj
l,∂rfθ/angbracketrightig
,
36Under review as submission to TMLR
by Lemma C.1, respectively. It follows that
(II) =R/summationdisplay
r=1⟨κ,∂rf¯θ⟩HS
/summationdisplay
l,j/bracketleftbig
λ2
lj−¯λ2
lj/bracketrightbig/angbracketleftig
κ,Yj
l/angbracketrightig/angbracketleftig
Yj
l,∂rfθ/angbracketrightig

=/summationdisplay
l,j/bracketleftbig
λ2
lj−¯λ2
lj/bracketrightbig/angbracketleftig
κ,Yj
l/angbracketrightig/angbracketleftigg
Yj
l,∂rfθR/summationdisplay
r=1⟨κ,∂rf¯θ⟩HS/angbracketrightigg
=/summationdisplay
l,j/bracketleftbig
λ2
lj−¯λ2
lj/bracketrightbig/angbracketleftig
κ,Yj
l/angbracketrightig/angbracketleftig
Yj
l,FS
θ,¯θκ/angbracketrightig
,
withFS
θ,¯θκdefined in (41). Bounding the latter with Lemma C.3 and using Lemma C.2, we conclude that
(II)≲µ−s
L∥κ∥L2∥FS
θ,¯θκ∥Hs≲µ−s
L∥κ∥L2∥κ∥Hs,
whereµlare the weights in the definition of the Sobolev space Hsvia spherical harmonics (5) and therefore
µl∼l. Together with the bounds for (I)this completes the proof.
Proof of Theorem 2.3. The theorem follows from Theorem A.1. It shows gradient descent convergence in
arbitrary scales of Hilbert spaces, for which the natural choice is Hs=Hs. However, we replace the L2=H0
norm with the equivalent ∥·∥♯norm to utilize our concentration result in Lemma C.5. This choice does
not alter any assumptions or conclusions, except coercivity which uses the H0inner product instead of the
corresponding norm. We show that coercivity of the L2inner product implies coercivity of the ∥·∥♯inner
product as well as the sample assumption (31). All other assumptions are proven in Welper (2024b, Theorem
2.2) for an analogous result without sample error.
1.Coercivity (26):Since the neural tangent kernel is zonal, by the Funk-Hecke formula (Atkinson &
Han, 2012), it has spherical harmonics as eigenfunctions with some eigenvalues µlj, so that we have
⟨v,Hv⟩♯=/summationdisplay
l,j¯λ2
lµljv2
lj∼/summationdisplay
l,jµljv2
lj=⟨v,Hv⟩,
withvlj=/angbracketleftig
v,Yj
l/angbracketrightig
and the∥·∥♯norm from (40). Hence, ∥·∥♯-coercivity is equivalent to the regular
L2-coercivity.
2.Assumption (31):We show that with high probability for all θ,¯θwith∥·∥∗distance to the initial
θ0smaller than ≲handS∈{0,s}we have the bound
R/summationdisplay
r=1⟨κ,∂rf¯θ⟩HS/bracketleftbig
∂rℓk(θ)−ℓ♯(θ)/bracketrightbig
≤∆sample (m,N )∥κ∥L2∥κ∥HS (44)
with
∆sample (m,N )≲C2
k/parenleftigτN
N/parenrightig1/2
+C−2
k/parenleftbiggN
τN/parenrightbigg1/2
L−s+L−s, (45)
which provides Assumption (31). To this end, by Lemma C.5 with t=τNand probability at least
1−2τN[eτN−τN−1]−1for allκ∈Hswe have
R/summationdisplay
r=1⟨κ,∂rf¯θ⟩HS/bracketleftbig
∂rℓk(θ)−¯ℓ♯(θ)/bracketrightbig
≲C2
k/parenleftigτN
N/parenrightig1/2
∥κ∥L2∥κ∥HS+L−s∥κ∥L2∥κ∥Hs.(46)
37Under review as submission to TMLR
This yields the claimed bounds (44), (45) for S=s. ForS= 0the last summand has the wrong
norm:L−s∥κ∥L2∥κ∥Hsinstead ofL−s∥κ∥L2∥κ∥HS=L−s∥κ∥2
L2. To replace the Hsnorm with an
L2norm note that assumption (32) yields
∥κk∥2
L2≥ca/parenleftig
m−1
2α
1+α+ ∆sample (m,N )/parenrightigs
β∥κ0∥2
Hs≳C2
k/parenleftigτN
N/parenrightig1/2
∥κ0∥2
Hs
fork≤n. Moreover, by induction, from Theorem A.1 we have ∥κk∥Hs≲∥κ0∥Hsand therefore
arrive at
∥κk∥2
Hs≲C−2
k/parenleftbiggN
τN/parenrightbigg1/2
∥κk∥L2.
Together with (46) this yields the claimed bounds (44) and (45) for the case S= 0. Together with
the caseS=sabove, this establishes assumption (31).
This completes the proof, together with an index shift n+ 1→nbetween the statements of Theorems A.1
and 2.2.
D Supplementary Material
D.1 Technical Lemmas
Lemma D.1. Letkt(x,y)be the heat kernel defined in (20). Then for all y∈Sd−1
∥kt(·,y)∥2
Hs(Sd−1)≲t−s−d+3/2.
Proof.Plugging the definition of the heat kernel (20) into the definition of Sobolev norms (5), we obtain
∥kt(·,y)∥2
Hs(Sd−1)=∞/summationdisplay
l=0ν(l)/summationdisplay
j=1/parenleftig
1 +l1/2(l+d−2)1/2/parenrightig2s/vextendsingle/vextendsingle/vextendsinglee−l(l+d−2)tYj
l(y)/vextendsingle/vextendsingle/vextendsingle2
≲1 +∞/summationdisplay
l=0l2se−2l2tν(l)/summationdisplay
j=1/vextendsingle/vextendsingle/vextendsingleYj
l(y)/vextendsingle/vextendsingle/vextendsingle2
.
Since|Yj
l(y)|2≲ν(l)andν(l)≲ld−2, see Stein & Weiss (1972, Chapter 4.2, Corollary 2.9), we obtain
∥kt(·,y)∥2
Hs(Sd−1)≲1 +∞/summationdisplay
l=0l2sl2d−4e−2l2t≲1 +/integraldisplay∞
0l2s+2d−4e−2l2tdl
= 1 +t−s−d+3/2/integraldisplay∞
0x2s+2d−4e−2x2dx≲t−s−d+3/2,
were we have substituted x=l√
tand used that the latter integral is bounded.
D.2 Results from Gentile & Welper (2022); Welper (2024b;a)
To keep the paper self contained, this section contains several results from Gentile & Welper (2022); Welper
(2024b;a).
Lemma D.2. Assume that σand ˙σsatisfy the growth and Lipschitz conditions (33),(34)and may be
different in each layer. Assume the weights, perturbed weights and domain are bounded (36)andmL∼
mL−1∼···∼m1. Then for 0<s< 1andm0:=m1
/integraldisplay/integraldisplay
D×Df(x)/parenleftiggR/summationdisplay
r=1∂rfθ(x)∂rfθ(y)/parenrightigg
g(y)dxdy≲∥f∥H−s(Sd−1)∥g∥H−s(Sd−1).
38Under review as submission to TMLR
Proof.This is a direct consequence of Welper (2024b, Lemma 7.16) and Welper (2024b, Lemma 6.7). For ϵ
sufficiently small so that s+ϵ<1, the former shows that
/integraldisplay/integraldisplay
D×Df(x)k(x,y)g(y)dxdy≤∥f∥H−s(Sd−1)∥g∥H−s(Sd−1)∥k∥C0;s+ϵ,s+ϵ(Sd−1),
fork(x,y) =/summationtextR
r=1∂rfθ(x)∂rfθ(y)and where∥·∥C0;s+ϵ,s+ϵ(Sd−1)is a Hölder norm of order s+ϵin the two
variablesxandy. Technically, the reference does not include the case s= 0, which follows directly from
asup-norm bound and the fact that the domain is bounded. The second reference Welper (2024b, Lemma
6.7) shows that
∥k∥C0;s+ϵ,s+ϵ≲1.
wherek(x,y) =/summationtextR
r=1∂rfθ(x)∂rfθ(y)is denoted by¯ˆΓ. Combining the two inequalities yields the result.
Lemma D.3 (Welper (2024b, Lemma 6.2)) .Assume that∥x∥≲1.
1. Assume that σsatisfies the growth condition (33)and may be different in each layer. Assume the
weights are bounded (36). Then
/vextenddouble/vextenddoublefℓ(x)/vextenddouble/vextenddouble≲m1/2
0ℓ−1/productdisplay
k=0/vextenddouble/vextenddoubleWk/vextenddouble/vextenddoublem−1/2
k.
2. Assume that σsatisfies the growth and Lipschitz conditions (33)and(34)and may be different in
each layer. Assume the weights and perturbed weights are bounded (36). Then
/vextenddouble/vextenddoublefℓ(x)−¯fℓ(x)/vextenddouble/vextenddouble≲m1/2
0ℓ−1/summationdisplay
k=0/vextenddouble/vextenddoubleWk−¯Wk/vextenddouble/vextenddoublem−1/2
kℓ−1/productdisplay
j=0
j̸=kmax/braceleftbig/vextenddouble/vextenddoubleWj/vextenddouble/vextenddouble,/vextenddouble/vextenddouble¯Wj/vextenddouble/vextenddouble/bracerightbig
m−1/2
j.
Lemma D.4 (Gentile & Welper (2022, Corollary 6.4)) .Letξi,i= 1,...,nbe independent random variables,
U,VHilbert spaces and Xi=Xi(ξi) =vi(ξi)ui(ξi)∗=viu∗
ibe Bochner integrable rank one operators with
vi∈Vandu∗
i∈U∗. Assume there are µ>0andν >0such that for all i= 1,...,n
∥u∗∥U∗≤µ, ∥v∥V≤ν,
almost surely. Then, for any t>0,
Pr/bracketleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1Xi−E[Xi]/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble>/radicalbigg
8µ2ν2t
n+2µνt
3n/bracketrightigg
≤2t/parenleftbig
et−t−1/parenrightbig−1.
Proof.The only difference to the reference is that we assume ∥u∗∥U∗≤µinstead of∥u∥U≤µ, which is
equivalent due to the Riesz representation theorem.
Lemma D.5 (Welper (2024a, Lemma 3.3)) .Leta,b,c,d> 0andρ>1/2. Letxnandynbe two sequences
that satisfy
xn+1−xn≤−γax1+ρ
ny−ρ
n+γbxn,
yn+1−yn≤−γcxρ
ny1−ρ
n+γd√xnyn.(47)
Furthermore, assume that
xk≥/parenleftbiggd
c/parenrightbigg 2
2ρ−1
y0, x k≥/parenleftbigg
2b
a/parenrightbigg1
ρ
y0, for allk= 0,...,n−1. (48)
Then
xn≤e−γbnx0, y n≤y0.
39Under review as submission to TMLR
32
40
48
56
64Width50
100
150200250Samples 0.000151 0.00022 0.000321 0.000469 0.000685 0.001Loss
(a) MSE Loss, dim= 3, depth= 2.
32
40
48
56
64Width50
100
150200250Samples 0.000136 0.000193 0.000275 0.000392 0.000558 0.000794Loss (b) Kernel Loss, dim= 3, depth= 2.
Figure 2: Test loss for training with mean squared loss (13) (left) and (16) (right). All axes are log-scaled
so that the slope corresponds to convergence rates.
E Extra Numerical Experiments
This appendix contains some extra numerical results for the setup in Section 3.
•Section 3 does not report any losses. They are contained in the extended Table 2.
•This section also contains experiments for shallow networks in three dimensions, shown in Figure 2
and Table 2.
•Figure 3 contains results for the shallow network with MSE loss in 7 dimensions and with larger
range for samples and width.
The observations from Section 3 remain unchanged. The deep networks performs slightly better than the
shallow ones, but have significantly more weights in total.
40Under review as submission to TMLR
32
64
128
256Width50
250
500750Samples 0.000597 0.00108 0.00196 0.00356 0.00645 0.0117Loss
Figure 3: Test loss for training with mean squared loss (13) for dimension 7and depth 2. All axes are
log-scaled so that the slope corresponds to convergence rates.
41Under review as submission to TMLR
Dimension 3 and Depth 2 – Trained with MSE Loss
Test Loss dof rate Nrate
m/N100 150 200 250 100 150 200 250 100 150 200 250
400.00043 0.00025 0.000238 0.000212 0.719 1.3 0.659 0.955 0.832 1.34 0.169 0.524
480.00033 0.000256 0.000203 0.000165 1.45 -0.119 0.883 1.36 1.08 0.631 0.805 0.917
560.000279 0.00019 0.000171 0.000163 1.1 1.92 1.1 0.0938 1.6 0.943 0.367 0.222
640.000252 0.000197 0.000153 0.000151 0.768 -0.273 0.859 0.58 1.22 0.601 0.893 0.0546
Dimension 3 and Depth 5 – Trained with MSE Loss
Test Loss dof rate Nrate
m/N100 150 200 250 100 150 200 250 100 150 200 250
400.000268 0.000155 0.000125 0.000102 0.331 1.33 1.33 1.19 1.9 1.36 0.743 0.896
480.00023 0.000146 0.000111 8.44e-05 0.833 0.328 0.628 1.06 1.86 1.13 0.933 1.25
560.000194 0.000126 9.19e-05 7.89e-05 1.13 0.969 1.25 0.434 2.2 1.07 1.08 0.684
640.000222 9.55e-05 8.08e-05 6.49e-05 -1.03 2.05 0.967 1.47 1.56 2.08 0.582 0.985
Dimension 3 and Depth 2 – Trained with Kernel Loss
Test Loss dof rate Nrate
m/N100 150 200 250 100 150 200 250 100 150 200 250
400.000317 0.000223 0.000208 0.00019 1.29 1.43 1.47 0.677 1.26 0.871 0.246 0.388
480.000275 0.000227 0.000187 0.000162 0.784 -0.11 0.563 0.898 1.2 0.469 0.672 0.662
560.000271 0.000199 0.000169 0.00015 0.103 0.858 0.658 0.474 1.16 0.756 0.565 0.536
640.000233 0.000173 0.000143 0.000136 1.12 1.05 1.26 0.756 1.49 0.732 0.664 0.232
Dimension 3 and Depth 5 – Trained with Kernel Loss
Test Loss dof rate Nrate
m/N100 150 200 250 100 150 200 250 100 150 200 250
400.00019 0.00013 0.000103 9.29e-05 1.08 0.676 0.748 1.07 1.65 0.939 0.797 0.482
480.000183 0.000118 9.56e-05 7.67e-05 0.229 0.517 0.433 1.05 1.23 1.07 0.744 0.987
560.000141 9.01e-05 7.29e-05 6.47e-05 1.69 1.77 1.75 1.11 2.02 1.1 0.734 0.539
640.000151 9.27e-05 6.47e-05 5.52e-05 -0.54 -0.218 0.898 1.19 1.71 1.21 1.25 0.713
Table 2: Loss and estimated convergence rates between neighbouring losses for the given m/N. Left: Rate
along the column, i.e. with respect to m. Right: Rate along rows, i.e. with respect to number of samples
N. The first table is trained with mean squared loss (MSE) (13) and the second with kernel loss (16).
42