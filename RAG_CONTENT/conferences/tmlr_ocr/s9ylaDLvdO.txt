Published in Transactions on Machine Learning Research (MM/2024)
Exploiting Hankel–Toeplitz Structures
for Fast Computation of Kernel Precision Matrices
Frida Viset† f.m.viset@tudelft.nl
Delft Center for Systems and Control
Delft University of Technology, The Netherlands
Anton Kullberg † anton.kullberg@liu.se
Department of Electrical Engineering
Linköping University, Sweden
Frederiek Wesel f.wesel@tudelft.nl
Delft Center for Systems and Control
Delft University of Technology, The Netherlands
Arno Solin arno.solin@aalto.fi
Department of Computer Science
Aalto University, Finland
Reviewed on OpenReview: https: // openreview. net/ forum? id= s9ylaDLvdO
Abstract
TheHilbert–space Gaussian Process (hgp) approach offers a hyperparameter-independent
basis function approximation for speeding up Gaussian Process (gp) inference by projecting
thegpontoMbasisfunctions. Thesepropertiesresultinafavorabledata-independent O(M3)
computational complexity during hyperparameter optimization but require a dominating
one-time precomputation of the precision matrix costing O(NM2)operations. In this
paper, we lower this dominating computational complexity to O(NM)withno additional
approximations . We can do this because we realize that the precision matrix can be split into a
sum of Hankel–Toeplitz matrices, each having O(M)unique entries. Based on this realization
we propose computing only these unique entries at O(NM)costs. Further, we develop two
theorems that prescribe sufficient conditions for the complexity reduction to hold generally
for a wide range of other approximate gpmodels, such as the Variational Fourier Feature
(vff) approach. The two theorems do this with no assumptions on the data and no additional
approximations of the gpmodels themselves. Thus, our contribution provides a pure speed-up
of several existing, widely used, gpapproximations, without further approximations .
1 Introduction
Gaussian Processes (gps, Rasmussen & Williams, 2006) provide a flexible formalism for modeling functions
which naturally allows for the incorporation of prior knowledge and the production of uncertainty estimates in
the form of a predictive distribution. Typically a gpis instantiated by specifying a prior mean and covariance
(kernel) function, which allows for incorporation of prior knowledge. When data becomes available, the
gpcan then be conditioned on the observations, yielding a new gpwhich can be used for predictions and
uncertainty quantification.
†-These authors contributed equally to this work. A reference implementation built on top of GPJax is available at
https://github.com/AOKullberg/hgp-hankel-structure.
1Published in Transactions on Machine Learning Research (MM/2024)
0 10 20 30 40 5010−1101103
Number of bfs,M(×103)←Wall-clock time (s)
hgp OursO(M2)O(M)
Figure 1: An order of magnitude speed-up without any additional approximations: Wall-clock
time to compute the precision matrix for an increasing number Mof basis functions.
While all these operations have closed-form expressions for regression, the computations of the mean and
covariance of the predictive gprequire instantiating and inverting the kernel (Gram) matrix, which encodes
pair-wise similarities between all data. These operations require respectively O(N2)andO(N3)computations,
whereNis the number of data points. Furthermore, if one wishes to optimize the hyperparameters of the
kernel function, which in gps is typically accomplished by optimizing the log-marginal likelihood, the kernel
matrix needs to be instantiated and inverted multiple times, further hindering the applicability of gps to
large-scale datasets.
The ubiquitous strategy to reduce this computational complexity consists in approximating the kernel matrix
in terms of Basis Functions (bfs), yielding what is essentially a low-rank or “sparse” approximation of
the kernel function (see Rasmussen & Williams, 2006; Quiñonero-Candela & Rasmussen, 2005; Snelson &
Ghahramani, 2005). Computational savings can then be achieved by means of the matrix inversion lemma,
which requires instantiating and inverting the precision matrix (sum of the outer products of the bfs)
instead of the kernel matrix at prediction, thereby lowering the computational complexity of inference to
O(NM2+M3), whereMis the number of bfs. If the number of bfs is chosen smaller than the number
of data points in the training set ( M <N), computational benefits arise and the computational costs for
hyperparameter optimization and inference are dominated by O(NM2).
It is less widely known that this cost can be improved further. A notable bfframework is the hgp(Solin
& Särkkä, 2020) which projects the gponto a dense set of orthogonal, hyperparameter-independent bfs.
This deterministic approximation is particularly attractive as it exhibits fast convergence guarantees to the
fullgpin terms of the number of bfs for smooth shift-invariant kernels compared to other approximations.
Furthermore, the fact that the bfs are hyperparameter-independent speeds up gphyperparameter
optimization considerably, which in the hgprequires onlyO(M3)operations after a one-time precomputation
of the precision matrix, costing O(NM2). These favorable properties have ensured a relatively wide usage of
thehgp(see e.g. Svensson et al., 2016; Berntorp, 2021; Kok & Solin, 2018), and it is available in, e.g., PyMC
(Abril-Pla et al., 2023) and Stan (Riutort-Mayol et al., 2022). However, as argued by Lindgren et al. (2022),
a high number of bfs may be required for a faithful approximation of the full model. Thus, the hgpis
typically employed in applications where forming the initial O(NM2)projection does not become too heavy.
In this paper, we reduce this complexity to O(NM)withno additional approximations (see Fig. 1), by
exploiting structural properties of the precision matrix. We remark that these structural properties arise
from thebfs that are used to approximate the kernel, notthe kernel itself. Our contributions are as follows.
•We show that the hgpprecision matrix using basis functions defined on a (hyper-)cubical domain
can be split in a sum of multilevel block-Hankel and a multilevel block-Toeplitz matrix (Fig. 2),
where each summand only has O(M)unique entries instead of O(M2). This allows us to determine
the elements of the full precision matrix at O(NM)instead ofO(NM2).
•This method does not only hold for hgpbasis functions. We provide sufficient conditions on
thebfs for this reduction in computational complexity to hold. Examples of other bfs where the
2Published in Transactions on Machine Learning Research (MM/2024)
1D
H(1)
n2D
H(1)
n⊗H(2)
n3D
H(1)
n⊗H(2)
n⊗H(3)
n
Figure 2: The precision matrix for polynomial basis functions has a nested Hankel structure. The visualization
of the matrix is proportionally darker as the logarithm of each entry increases. The matrices are computed as
the sum of all entries Hnforn={1,...,N}, where the expression for Hnis given below each matrix.
conditions hold, and that therefore can be sped up using the same technique are polynomial, (complex)
exponential and (co)sinusoidal basis functions. We, therefore, enable the speed-up of other bf-based
gpapproximations such as variational Fourier features (Hensman et al., 2017).
In the experiments, we demonstrate in practice that our approach lowers the computational complexity of
thehgpby an order of magnitude on simulated and real data.
2 Background
Agpis a collection of random variables, any finite number of which have a joint Gaussian distribution (Ras-
mussen & Williams, 2006). We denote a zero–mean gpbyf∼GP (0,κ(·,·)), whereκ(x,x′) :RD×RD→R
is the kernel, representing the covariance between inputs xandx′. Given a dataset of input–output pairs
{(xn,yn)}N
n=1,gps are used for non-parametric regression and classification by coupling the latent functions
fwith observations through a likelihood model p(y|f) =/producttextN
i=1p(yi|f(xi)).
For notational simplicity, we will in the following focus on gpmodels with a Gaussian (conjugate) likelihood,
yi∼N(f(xi),σ2). The posterior gp,GP(µ⋆(·),Σ⋆(·,·))can be written down in closed form by
µ⋆(x⋆) =k⊤
⋆(K+σ2I)−1y, (1a)
Σ⋆(x⋆,x′
⋆) =k(x⋆,x′
⋆)−k⊤
⋆(K+σ2I)−1k⋆′, (1b)
where K∈RN×Nandk⋆∈RNare defined element-wise as Ki,j:=κ(xi,xj)and[k⋆]i:=κ(xi,x⋆)for
i,j∈1,2,...,N, and k⋆′is defined similarly. Observations are collected into y∈RN. Due to the inverse of
(K+σ2I)in the posterior mean and covariance, the computational cost of a standard gpscales asO(N3),
which hinders applicability to large datasets.
2.1 Basis Function Approximations
The prevailing approach in literature to circumvent the O(N3)computational bottleneck is to approximate
thegpwith a sparse approximation, using a finite number of either inducing points or Basis Functions (bfs)
(e.g., Rasmussen & Williams, 2006; Quiñonero-Candela & Rasmussen, 2005; Hensman et al., 2017). The bf
representation is commonly motivated by the approximation
κ(x,x′)≈ϕ(x)⊤Λϕ(x′). (2)
We use the notation from Solin & Särkkä (2020) to align with the next section. Here, ϕ(·) :RD→RMare the
bfs,ϕ(·):= [ϕ1(·),ϕ2(·),...,ϕ M(·)]⊤. Further, Λ∈RM×Mare the corresponding bfweights. Combining
this approximation with the posterior gp, Eq. (1), and applying the Woodbury matrix inversion lemma yields
µ⋆(x⋆) =ϕ(x⋆)⊤/parenleftbig
Φ⊤Φ+σ2Λ−1/parenrightbig−1Φ⊤y, (3a)
Σ⋆(x⋆,x′
⋆) =σ2ϕ(x⋆)⊤/parenleftbig
Φ⊤Φ+σ2Λ−1/parenrightbig−1ϕ(x′
⋆). (3b)
3Published in Transactions on Machine Learning Research (MM/2024)
Here,Φ∈RN×Mis commonly referred to as the regressor matrix and is defined as Φi,::=ϕ(xi)⊤. Further,
Φ⊤Φ∈RM×Mis theprecision matrix which is a central component of the following section. Computing this
precision matrix requires O(NM2)operations. With this approximation, computing the posterior mean and
covariance in Eq. (3) requires instantiating and inverting/parenleftbig
Φ⊤Φ+σ2Λ−1/parenrightbig−1which can be performed with
O(NM2+M3)operations. If we have more samples than bfs (which is the required condition for bfs to
give computational savings), i.e., N≥M, the overall computational complexity is then of O(NM2), i.e., the
inversion costs are negligible as compared to the cost of computing the precision matrix.
3 Methods
Our main findings are in the form of two theorems (Theorems 3.1 and 3.4). These theorems prescribe the
necessary conditions that bfexpansions need to fulfill to be able to reduce the computational complexity
of computing the precision matrix Φ⊤ΦfromO(NM2)toO(NM), applicable to multiple previous works
that rely on parametric basis functions (incl. Lázaro-Gredilla et al., 2010; Hensman et al., 2017; Solin &
Särkkä, 2020; Tompkins & Ramos, 2018; Dutordoir et al., 2020). Since our contribution reduces the cost of
computing the precision matrix to O(NM), it follows that the computational complexity of computing the
posterior mean or covariance is also of O(NM). Further, both of the theorems reduce the memory scaling
fromO(M2)toO(M). Note that these reductions are withoutapproximations, only relying on the structural
properties of the considered models.
In the following, we assume that the kernel is a tensor product kernel (Rasmussen & Williams, 2006),
i.e.,κ(x,x′) =/producttextD
d=1κ(d)(x(d),x(d)′), whereκ(d)(·,·)is the kernel along the dthdimension. Then, if each
component of the kernel is approximated with mdbfs such that
κ(d)(x(d),x(d)′)≈ϕ(d)(x(d))⊤Λ(d)ϕ(d)(x(d)′), (4)
where ϕ(d)(·) :R→Rmdare thebfs[ϕ(d)
1,ϕ(d)
2,...,ϕ(d)
md]⊤along thedthdimension and Λ(d)∈Rmd×md
contains the associated weights. The full kernel can then be approximated as
κ(x,x′)≈/producttextD
d=1ϕ(d)(x(d))⊤Λ(d)ϕ(d)(x(d)′), (5)
where in this case ϕ(·) :RD→RMandΛ∈RM×Mare
ϕ(x) =⊗D
d=1ϕ(d)(x(d)), (6a)
Λ=⊗D
d=1Λ(d). (6b)
Here,M:=/producttextD
d=1mdis the total number of bfs. Given this decomposition, the precision matrix can be
expressed as
Φ⊤Φ=/summationtextN
n=1ϕ(xn)ϕ(xn)⊤=/summationtextN
n=1⊗D
d=1ϕ(d)(x(d)
n)/bracketleftig
ϕ(d)(x(d)
n)/bracketrightig⊤
. (7)
This decomposition of the precision matrix is key in the following and we will primarily study the individual
products ϕ(d)(x(d)
n)[ϕ(d)(x(d)
n)]⊤where certain structure may appear that is exploitable to our benefit. To
provide some intuition, we consider the precision matrix for polynomial bfs in 1D, 2D, and 3D (see Fig. 2).
The 1D case (left) has a Hankelstructure and the 2D (middle) and 3D (right) cases have 2-level and 3-level
block Hankel structure, respectively. It is these types of structures that allow a reduction in complexity without
approximations. Next, we provide clear technical definitions of the matrix structures and then proceed to state
our main findings. All of the discussed matrix structures are also visually explained in Table A1 in App. E.
3.1 Hankel and Toeplitz Matrices
Anm×mmatrix Hhas Hankel structure iff it can be expressed using a vector γ∈R(2m−1)containing all
the unique entries, according to
H=
γ1γ2... γm
γ2γ3...γm+1
............
γmγm+1...γ2m−1
. (8)
4Published in Transactions on Machine Learning Research (MM/2024)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
ϕ(d)ϕ(d)⊤=
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright/summationtext
nH(1)+
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright/summationtext
nT(1).
Figure 3: The precision matrix for sinusoidal basis functions in one dimension has neither Hankel nor Toeplitz
structure. However, it can be decomposed into a sum of two matrices, where one has a Hankel structure, and
one has Toeplitz structure. Here, 49 bfs are placed along one dimension.
In other words, each element of the Hankel matrix on the ithrow and the jthcolumn is given by γi+j−1.
Similarly, an m×mmatrix has Toeplitz structure iff it can be expressed using a vector γ∈R(2m−1)such
that each element on the ithrow and the jthcolumn is given by γi−j+m. See Fig. 3 for an example of what a
Hankel and a Toeplitz matrix visually looks like.
We define a matrix H(D)as aD-level block Hankel matrix if it can be expressed as
H(D)=
H(D−1)
1 H(D−1)
2...H(D−1)
mD
H(D−1)
2 H(D−1)
3...H(D−1)
mD+1
............
H(D−1)
mD H(D−1)
mD+1...H(D−1)
2mD−1
, (9)
where H(D−1)
kis a block Hankel matrix, and H(1)
kis a simple Hankel matrix. Each submatrix H(d)
kfor
d∈{1,...,D}can be indexed by row and column id,jd∈{1,...,m d}which again yields a submatrix
H(d−1)
id+jd−1which can be indexed in the same manner. Therefore, an equivalent definition of a block Hankel
matrix is that each individual entry of H(D), given by a set of indices i1,j1,...,i D,jD, is given by the
entry γk1,...,kDof a tensor γ∈RK1×K2×...×KD, wherekd=id+jd−1, andKd= 2md−1. Similarly,
T(D−1)is a block Toeplitz matrix if each entry i1,j1,...,i D,jDis given by the entry γk1,...,kDof a tensor
γ∈RK1×K2×...×KD, wherekd=md+id−jd, andKd= 2md−1.
3.2 Block Hankel–Toeplitz Matrices
In addition to block Hankel and block Toeplitz structures, we require a slightly more general but highly
related structure. We call this structure block Hankel–Toeplitz structure and define it as follows. A matrix
G(D)has aD-level block Hankel–Toeplitz structure, if the matrix is defined either as
G(D)=
G(D−1)
1 G(D−1)
2...G(D−1)
mD
G(D−1)
2 G(D−1)
3...G(D−1)
mD+1
............
G(D−1)
mD G(D−1)
mD+1...G(D−1)
2mD−1
, (10)
if levelDis Hankel, or as
G(D)=
G(D−1)
mD...G(D−1)
2 G(D−1)
1
G(D−1)
mD+1...G(D−1)
3 G(D−1)
2
............
G(D−1)
2mD−1...G(D−1)
mD+1G(D−1)
mD
, (11)
if levelDis Toeplitz. Further, G(D−1)
jareD−1level block Hankel–Toeplitz matrices if D−1>2, and a
simple Hankel or Toeplitz matrix if D−1 = 1. Each submatrix G(d)
kford∈{1,...,D}can be indexed by
5Published in Transactions on Machine Learning Research (MM/2024)
row and column id,jd∈{1,...,m d}which again yields a submatrix defined either as G(d−1)
id+jd−1orG(d−1)
md+id−jd
(depending on whether the dth-level has Hankel structure as in Eq. (10) or Toeplitz structure as in Eq. (11)).
Each entry in a block Hankel–Toeplitz matrix can also be expressed by the entry γk1,k2,...,kDof a tensor
γ∈RK1×K2×...×KD, whereKd= 2md−1, and
kd=/braceleftigg
id+jd−1,if leveldis Hankel
md+id−jd,if leveldis Toeplitz.(12)
A crucial property of the block Hankel–Toeplitz structure is the preservation of structure under addition.
Assume that AandBare two block Hankel–Toeplitz matrices and that they are structurally identical, in
the sense that they have the same number of levels, the same number of entries in each block, and each level
shares either Toeplitz or Hankel properties. Then, let each entry of AandBbe given by αk1,k2,...,kDfor
a tensorα∈Rm1,...,m Dandβk1,k2,...,kDforβ∈Rm1,...,m D, respectively, with kddefined in Eq. (15). Each
entry in A+Bis then given by the sum of the entries αk1,...,kD+βk1,...,kD. Thus, the sum of two block
Hankel–Toeplitz matrices with identical structure is also a block Hankel–Toeplitz matrix. By associativity of
matrix addition, a sum/summationtextN
n=1GnofNHankel–Toeplitz matrices {G1,...,GN}with identical structure is
therefore itself a Hankel–Toeplitz matrix.
3.3 Kronecker Products of Hankel–Toeplitz Matrices and Block Hankel–Toeplitz Matrices
A special case of a class of matrices Gwhich has block Hankel–Toeplitz structure are Kronecker products of
DHankel and Toeplitz matrices {G(1),...,G(D)}, i.e.,
G=D/circlemultiplydisplay
d=1G(d):=G(1)⊗G(2)⊗···⊗ G(D). (13)
An equivalent definition of the Kronecker product gives each entry on the ithrow andjthcolumn in the block
Hankel–Toeplitz matrix Gas an expression of the entries on the idthrow andjdthcolumn of each matrix
G(d)according to
Gi,j=/producttextD
d=1G(d)
id,jd. (14)
Note that there is a one-to-one map between each index i,jand the index sets {i1,...,i D}and{j1,...,j D}.
As each matrix G(d)has Hankel or Toeplitz structure, the entries can equivalently be defined by a vector γ(d)
with 2md−1entries. Each entry Gi,jis therefore given by
Gi,j=/producttextD
d=1γ(d)
kd=γk1,...,kD, (15)
wherekdis defined in Eq. (12), and where γ:=/circlemultiplytextD
d=1γ(d)is a rank-1 tensor with/producttextD
d=1(2md−1)elements.
3.4 Main Results
We are now ready to state our main findings. The following two theorems rely on the product decomposition
Eq. (5) and study each dimension dseparately, as is evidently possible from Eq. (7). Our first theorem
generalizesaresultbyGreengardetal.(2023)regardingcomplexexponentialbasisfunctions. Ourfirsttheorem
establishes that if the product ϕ(d)(x(d)
n)[ϕ(d)(x(d)
n)]⊤has Hankel or Toeplitz structure, the resulting precision
matrix only has/producttextD
d=1(2md−1)unique entries, reducing the computational complexity of instantiating it
fromO(NM2)toO(NM). We formalize this in the following theorem.
Theorem 3.1. If the matrix
G(d)(x(d)
n):=ϕ(d)(x(d))/bracketleftbig
ϕ(d)(x(d))/bracketrightbig⊤, (16)
is a Hankel or Toeplitz matrix for all x(d)∈Ralong each dimension d, the information matrix Φ⊤Φwill be
a multi-level block Hankel or Toeplitz matrix, and therefore have/producttextD
d=1(2md−1)unique entries.
6Published in Transactions on Machine Learning Research (MM/2024)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Φ⊤Φ=
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright/summationtext
nH(1)⊗H(2)+
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright/summationtext
nT(1)⊗H(2)+
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright/summationtext
nH(1)⊗T(2)+
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright/summationtext
nT(1)⊗T(2)
Figure 4: The precision matrix for sinusoidal bfs in two dimensions has neither Hankel nor Toeplitz structure.
However, it can be decomposed into 2D= 4matrices, which each have block Hankel–Toeplitz structure. Here,
7bfs are placed along each of the two dimensions, giving a total of 49 bfs.
Proof.Assume that the matrix G(d)(x(d)
n):=ϕ(d)(x(d)
n)[ϕ(d)(x(d)
n)]⊤is Hankel or Toeplitz. The precision
matrix can then be expressed as
Φ⊤Φ=/summationtextN
n=1⊗D
d=1ϕ(d)(x(d)
n)/bracketleftig
ϕ(d)(x(d)
n)/bracketrightig⊤
=/summationtextN
n=1⊗D
d=1G(d)(x(d)
n), (17)
where the matrix ⊗D
d=1G(d)(x(d)
n)is multi-level Hankel or Toeplitz by definition, see Sec. 3.3. Further, the sum
of several D-level block Hankel–Toeplitz matrices is itself a D-level block Hankel–Toeplitz matrix, see Sec. 3.2.
Since each matrix G(d)(x(d)
n)has at most (2md−1)unique entries, the matrix Φ⊤Φ=/summationtextN
n=1⊗D
d=1G(d)(x(d)
n)
therefore has at most M=/producttextD
d=1(2md−1)unique entries.
The preceding theorem holds true for, for instance, polynomial and complex exponential bfs, which we
establish in Corollaries 3.2 and 3.3.
Corollary 3.2. The precision matrix for polynomial bfsdefined by
ϕ(d)
id(x(d)) = (x(d))id−1, (18)
can be represented by a tensor with/producttextD
d=12md−1entries.
Proof.See App. A for a proof.
Corollary 3.3. The precision matrix for complex exponential bfsdefined by
ϕC
j(x) = exp(iπj⊤x) =/producttextD
d=1exp(iπjdxd), (19)
can be represented by a tensor with/producttextD
d=12md−1entries.
Proof.See App. B for a proof.
For some bfs, the structure of the product ϕ(d)(x(d)
n)[ϕ(d)(x(d)
n)]⊤is more intricate, but is still of a favorable,
exploitable nature. This is clearly evident from Fig. 4, where the precision matrix for sinusoidal bfs in two
dimensions is visualized. In particular, for some bfs, the product is the sum of a Hankel and a Toeplitz
matrix, such that the precision matrix only has/producttextD
d=13mdunique entries, again reducing the computational
cost of computing it to O(NM). We formalize this in the following theorem.
Theorem 3.4. If the product ϕ(d)(x(d)
n)[ϕ(d)(x(d)
n)]⊤is the sum of a Hankel matrix denoted G(d),(1)(x(d)
n)
and a Toeplitz matrix G(d),(−1)(x(d)
n), and there exists a function g(d)(kd,x(d)
n)such that G(d),(1)
id,jd(x(d)
n) =
g(id+jd,x(d)
n)andG(d),(−1)
id,jd(x(d)
n) =−g(id−jd,x(d)
n), all entries in the precision matrix can be represented
by a tensor γ(k1,k2,...,k D)with/producttextD
d=13mdentries.
7Published in Transactions on Machine Learning Research (MM/2024)
Proof.The precision matrix can in this case be expressed as
:=C/bracehtipdownleft/bracehtipupright/bracehtipupleft/bracehtipdownright
Φ⊤Φ=/summationtextN
n=1⊗D
d=1ϕ(d)(x(d)
n)/bracketleftig
ϕ(d)(x(d)
n)/bracketrightig⊤
=/summationtextN
n=1⊗D
d=1/parenleftig
G(d),(1)(x(d)
n) +G(d),(−1)(x(d)
n)/parenrightig
=/summationtext2D
p=1/parenleftig/producttextD
d=1e(d)
p/parenrightig/summationtextN
n=1⊗D
d=1G(d),(e(d)
p)(x(d)
n), (20)
whereep={e(1)
p,...,e(D)
p}∈SDandSD={1,−1}Dis a set containing 2Delements. Each of the 2D
matrices/parenleftig/producttextD
d=1e(d)
p/parenrightig/summationtextN
n=1⊗D
d=1G(d),(e(d)
p)(x(d)
n)is now the Kronecker product between DHankel or Toeplitz
matrices. The entries of Ccan be expressed element-wise as
Ci,j=/summationtext2D
p=1/parenleftig/producttextD
d=1e(d)
p/parenrightig/summationtextN
n=1/producttextD
d=1g(id+e(d)
pjd,x(d)
n). (21)
If we define a tensor γas
γk1,...,kD=/summationtextN
n=1/producttextD
d=1g(kd,x(d)
n) (22)
for indiceskd=1−md,2−md,..., 2md−1,2md, each entry of the precision matrix Ci,jcan be expressed as
Ci,j=/summationtext2D
p=1/parenleftig/producttextD
d=1e(d)
p/parenrightig
γi1+e(1)
pj1,...,iD+e(D)
pjD. (23)
Aseachsum kd=id+e(d)
pjdisanintegerbetween 1−mdand2md, thetensor γwillhave/producttextD
d=13mdentries.
The preceding theorem applies to, for instance, the bfs in anhgp(Solin & Särkkä, 2020) defined on a
rectangular domain, formalized in Corollary 3.5. It further holds for multiple other works using similar bfs,
formalized in Corollary 3.6. We remark that for D= 1, we require md>3for any savings to take effect,
whereas for D> 1,md≥2suffices.
Corollary 3.5. The precision matrix in an hgpdefined on a rectangular domain [−L1,L1]×···× [−LD,LD]
can be represented by a tensor with/producttextD
d=13mdentries.
Proof.See App. C for a proof.
Corollary 3.6. The precision matrix in a gpapproximated by sinusoidal and cosine bfswith frequencies on
a grid (such as the regular Fourier features described in Hensman et al. (2017) and Wahls et al. (2014), the
Fourier approximations to periodic kernels described in Tompkins & Ramos (2018), the quadrature Fourier
features described in Mutný & Krause (2018), the equispaced-version of sparse spectrum bfsdescribed in
Lázaro-Gredilla et al. (2010), or the one-dimensional special case of Dutordoir et al. (2020)) can be represented
by a tensor with/producttextD
d=13mdentries.
Proof.See App. D for a proof.
3.5 Outlook and Practical Use
Both Theorems 3.1 and 3.4 reduce the computational complexity of calculating the entries of the precision
matrix Eq. (7) from O(NM2)toO(NM). This enables us to scale the number of bfs significantly more
than previously, before running into computational, or storage related, problems. For clarity, the standard
hgpis given in Alg. 1 with the original approach in red and our proposed approach in blue. As compared to
the standard (offline) hgp, the only change we make is the computation of the precision matrix.
We remark that computing the posterior mean and variance of the bfexpansion now costs O(NM +M3),
now possibly dominated by the M3term. This cost also appears in the hyperparameter optimization, as
we choose to optimize the mllas in Solin & Särkkä (2020). Remedies for this cost are out of scope of this
8Published in Transactions on Machine Learning Research (MM/2024)
Algorithm 1 Sketch of an algorithm for Hilbert gplearning and inference. The original approach by Solin
& Särkkä (2020) in red, our proposed approach in blue.
Input:Data as input–output pairs {(xi,yi)}N
i=1,
test inputs x⋆, number of basis functions M
Compute Φ⊤Φat costO(NM2) ▷Eq. (7)
Compute γat costO(NM) ▷Eq. (22)
Construct Φ⊤Φusing γat costO(M2)
repeat
Optimize Marginal Log-Likelihood (mll) w.r.t. hyperparameters at cost O(M3)
untilConvergence
Performgpinference using the pre-calculated matrices. This entails computing the posterior mean and
covariance, at a computational cost of O(M3).
7.457.57.557.67.6573.4273.4473.46
Latitude (◦)Longitude (◦)
20×20bfs
7.457.57.557.67.65
Latitude (◦)
40×40bfs
0 2 4 6 −2
7.457.57.557.67.65
Latitude (◦)
80×80bfs
(a) Predictive means indicated by color intensity with transparency
proportional to the predictive variance. Increasing number of bfs
from left to right.−0.1 0 0.1 0.2102104900 3,500 6,400
←nlpd←Wall-clock time (s)Number of bfs(M)hgp Ours
(b) Wall-clock time to compute precision matrix
by sequentially including each data point over
nlpd.
Figure 5: Our proposed computational scheme reduces the computation time for datasets with high-frequency
variations, as these require many bfs to achieve accurate reconstruction. This underwater magnetic field has
lowerNegative Log Predictive Density (nlpd) with a large amount ( 6400) compared to a smaller amount
(400) ofbfs. For 6400bfs, our computational scheme reduced the required time to compute the precision
matrix from 2.7 hours to 1.7 minutes.
paper, but could potentially be reduced through the use of efficient approximate matrix inverses and trace
estimators (see, e.g., Davies, 2015). Another consequence of these theorems is that multi-agent systems that
collaborate to learn the precision matrix such as Jang et al. (2020), Viset et al. (2023) or Pillonetto et al.
(2019) can do this by communicating O(M)bits instead ofO(M2)bits.
4 Experiments
We demonstrate the storage and computational savings of our structure exploiting scheme by means of three
numerical experiments. The experiments demonstrate the practical efficiency of our scheme using the hgp.
We reiterate that the savings are withoutadditional approximations and the posterior is therefore exactlyequal
to that of the standard hgp. Further, as hgps adhere to Theorem 3.4, this demonstration is a representative
example of the speedups that can be expected using, e.g., regular Fourier features (Hensman et al., 2017), or
bfexpansions of periodic kernels (Dutordoir et al., 2020). Since Theorem 3.1 requires computing and storing
only 2DMcomponents whereas Theorem 3.4 requires 3DM, our experiments demonstrate a practical upper
bound on the storage and computational savings, a “worst case”.
Our first experiment demonstrates the computational scaling of our scheme on a simulated 3D dataset.
Secondly, we consider a magnetic field mapping example with data collected by an underwater vessel, as an
application where the high-frequency content of the data requires a large amount of bfs to reconstruct the
9Published in Transactions on Machine Learning Research (MM/2024)
(a) Fullgp
0 1500 3000 [mm]
(b)hgp/ Ours1 2 3 40.1110100
Number of bfs,M(×103)←Wall-clock time (s)hgp OursO(M2)O(M)
(c) Time to compute the precision matrix for increasing M
Figure 6: These experiments recover the results from Solin & Särkkä (2020) exactly for predicting yearly
precipitation levels across the US, and measure the wall-clock time needed by our proposed computational
scheme. The hgpefficiently approximates the full gpsolution using m1=m2= 45, totalingM= 2025bfs.
field. Thirdly, a precipitation dataset is used, mirroring an example in Solin & Särkkä (2020), improving the
computational scaling in that particular application even further than the standard hgp. Our freely available
hgpreference implementation along with hyperparameter optimization is written for GPJax (Pinder & Dodd,
2022). All timing experiments are run on an HP Elitebook 840 G5 laptop (Intel i7-8550U CPU, 16GB RAM).
For fair comparison, we naively loop over data points, to avoid any possible low-level optimization skewing
the results.
Computational Scaling We compare the time necessary for computing the precision matrix for N= 500
data points for the standard hgpas well as for our structure exploiting scheme. The results are presented
in Fig. 1 for an increasing number of bfs. AfterM= 14000, thehgpis no longer feasible due to memory
constraints, whereasourformulationscaleswellabovethat, butstopat M= 64000 forclarityintheillustration.
Clearly, the structure exploitation gives a significantly lower computational cost than the standard hgp, even
for small quantities of bfs. Further, it drastically reduces the memory requirements, where for M= 14000,
thehgprequires roughly 1.5GBfor storing the precision matrix, while our formulation requires roughly
2.8MBusing 64-bit floats. This makes it possible for us to scale the number of bfs significantly more before
running into computational or storage restrictions. It is noteworthy that even though the implementation is
in a high-level language, we still see significant computational savings.
Magnetic Field Mapping Ashgps are commonly used to model and estimate the magnetic field for, e.g.,
mapping purposes (Solin et al., 2015; Kok & Solin, 2018), we consider a magnetic field mapping example and
demonstrate the ability of our computational scheme to scale hgps to spatially vast datasets. The data was
gathered sequentially in a lawn-mower path in a region approximately 7×7kmlarge by an underwater vessel
outside the coast of Norway ( d= 2andN= 1.39million). The data was split into a training set and test set
with roughly a 50/50split, deterministically split by a grid pattern, to ensure reasonable predictions in the
entire data domain, see App. F.2 for more details. We vary the amount of bfs and compare the time required
to sequentially include each new data point in the precision matrix as well as the Negative Log Predictive
Density(nlpd). As the underwater magnetic field covers a large area, a large number of bfs are required to
accurately represent the field, see Fig. 5a where the details of the predicted magnetic field is captured more
accurately for an increasing number of bfs. This is also apparent from the decreasing nlpdas the number of
bfs increases, see Fig. 5b. At 6400bfs, the necessary computation time is several orders of magnitude lower
for our approach compared to the standard hgp.
U.S. Precipitation Data We consider a standard precipitation data set containing US annual precipitation
summaries for year 1995 ( d= 2andN= 5776) (Vanhatalo & Vehtari, 2008). We exactly mimic the setup in
Solin & Särkkä (2020) and primarily focus our evaluation on the calculation of the precision matrix. The
time for computing the precision matrix is visualized in Fig. 6c, where our approach clearly outperforms the
standard hgp. The predictions on a dense grid over the continental US can be found in Figs. 6a and 6b,
where the hgpmanages to capture both the large-scale as well as the small-scale variations well.
10Published in Transactions on Machine Learning Research (MM/2024)
5 Related Work
The computational scheme that we detail here can be used to speed up a range of approximate gpand kernel
methods with bfs that satisfy the Hankel–Toeplitz structure we use (see e.g. Tompkins & Ramos, 2018; Solin
& Särkkä, 2020; Hensman et al., 2017). It is also applicable to Lázaro-Gredilla et al. (2010) in the special
case where the considered frequencies of the bfs are equidistant, as well as Dutordoir et al. (2020) when the
input space is one-dimensional. However, there is also a wide range of gpapproximations that do not have
the Hankel–Toeplitz structure required for Theorem 3.1 or Theorem 3.4 to apply. While the structure we
exploit in Theorem 3.4 is apparent in the quadrature Fourier feature approach of Mutný & Krause (2018)
when the frequencies are on a structured Cartesian grid, other quadrature-like methods are not possible to
speed-up in similar ways. The most well-known method is the random Fourier feature approach (Rahimi
& Recht, 2007), where speed-up is not possible as the frequencies are sampled at random. Similarly, the
Gaussian quadrature approaches of Dao et al. (2017); Shustin & Avron (2022) and the random quadrature
approach of Munkhoeva et al. (2018) do not constrain frequencies to a regular grid and therefore do not have
the Hankel–Toeplitz structure required by Theorem 3.4.
As was pointed out by Quiñonero-Candela & Rasmussen (2005), inducing point approaches can also be viewed
asbfapproximations. The inducing variable approaches essentially summarize the data in a set of inducing
variables (Snelson & Ghahramani, 2005; Seeger et al., 2003; Csató & Opper, 2002), which most commonly
represent function values at a certain set of inputs, even though other choices are possible (Lázaro-Gredilla
& Figueiras-Vidal, 2009). Another closely related approach which also uses inducing points, with similar
computational complexity of O(NM2), is the variational gppioneered by Titsias (2009). In light of the bf
viewpoint, all the aforementioned approaches therefore also involve computing a precision matrix, but even for
ordered inducing points on a grid, the particular Hankel–Toeplitz structure we exploit does not exist in general.
A range of related work stacks additional approximations on top of the sparse basis function/inducing
point approximations (Yadav et al., 2021; Izmailov et al., 2018; Hensman et al., 2013). The purpose of
these additional approximations is typically to either reduce the computational complexity of computing
the precision matrix for example by using bfs with compact support, or by implementing an approximate
algorithm for inverting the precision matrix. A well-known structure exploiting method is Structured Kernel
Interpolation (ski) (Wilson & Nickisch, 2015; Yadav et al., 2021; Izmailov et al., 2018), which approximates
the precision matrix through cubic interpolation between inducing points on a regular grid. An alternative
approach that improves the computational complexity of the variational gp, is theStochastic Variational
gp(svgp) (Hensman et al., 2013). The svgpis thede facto standard approach for large–scale gps, due to
the possibility of utilizing mini-batching for training, greatly speeding up hyperparameter (and variational
parameter) learning (Hensman et al., 2013; 2015), with implementation available in, e.g., GPyTorch and
GPflow (Gardner et al., 2018; de G. Matthews et al., 2017).
While this paper focuses on bfapproximations to gps, there is a lot of work on approximating exact gp
regression through means of, e.g., Conjugate Gradient (cg) descent (Gibbs & MacKay, 1996; Artemev
et al., 2021; Gardner et al., 2018). These methods view gpregression as the solution to a linear system of
equations and seek to solve this approximately. This typically reduces the cost of exact gpregression from
O(N3)toO(IN2), whereIis the number of cgiterations. Davies (2015) uses cgas the driving scalability
mechanism, but further reduces the computational complexity to O(NMI )through use of M-efficient kernels ,
which essentially constitute the approaches from Quiñonero-Candela & Rasmussen (2005) as well as compact
kernels (see, e.g., Kullberg et al., 2021; Buhmann, 2003; Wu, 1995). cghas also been combined with ski
in thekiss-gp framework of Wilson & Nickisch (2015) to reduce the computational complexity of (mean)
inference toO(N+MlogM). Pleiss et al. (2018) extended kiss-gp with the Lanczos algorithm to reduce
the complexity of computing the predictive variance to O(k)after pre-computation, where kis the number of
Lanczos iterations. Recently, Stochastic Gradient Descent (sgd) was introduced as an alternative to cg(Lin
et al., 2023a;b), potentially with better performance for ill-conditioned datasets. Technically, any of these
ideas are straightforward to include in the hgpas well and could potentially be used to reduce the O(M3)
complexity of inference and hyperparameter learning.
Other previous work has discovered that functions of difference matrices (a special case of Toeplitz matrices)
are also sums of Hankel and Toeplitz matrices in the one-dimensional case, and Kronecker products of these
11Published in Transactions on Machine Learning Research (MM/2024)
in the two-dimensional case (Strang & MacNamara, 2014). Since our matrices are also Kronecker products
of Hankel and Toeplitz matrices, we end up exploiting similar structures as Strang & MacNamara (2014)
arising in a different situation.
6 Conclusion
Our contribution details a computational approach for exploiting Hankel and Toeplitz structures that
appear in multiple bfapproximation schemes to kernels for gps. These structures allow us to reduce the
computational complexity of computing the corresponding precision matrix from O(NM2)toO(NM)without
further approximations. Further, our approach reduces the storage requirement for containing all necessary
information about the posterior to make predictions from O(M2)toO(M). The Hankel and Toeplitz
structures appear because of the particular bfs that are used to approximate the kernel, notthe kernel itself.
The reduced computational and storage requirements are particularly beneficial in the hgpwhere more bfs
allow us to capture higher frequencies of the kernel spectrum, otherwise unattainable without significant
computational resources. We foresee that our contribution will allow hgps to tackle larger problems without
the need for extensive specialized hardware, opening up approximate gplearning and inference for a wider
audience. Future work could investigate if the results can be generalized to wider ranges of bfs, which is
easily verified through Theorems 3.1 and 3.4. Another potential is to investigate ways of approximately
decomposing an already known precision matrix into Hankel–Toeplitz matrices which does not admit an
analytical such decomposition. This would yield the same computational benefits but with some potential
loss of accuracy.
A reference implementation built on top of GPJax is available at https://github.com/AOKullberg/hgp-hankel-
structure.
Impact Statement
This work develops numerical methods for the wide field of machine learning, where the goal is to make
existing methods more compute-efficient and open new avenues for developing future methods. There are
many potential uses and therefore societal consequences of such methods, none of which we see the need to
specifically highlight here.
Acknowledgements
We would like to express our gratitude to the supervisors (Gustaf Hendeby, Isaac Skog, Kim Batselier,
Manon Kok and Rudy Helmons) of the three PhD candidates (Anton Kullberg, Frederiek Wesel and Frida
Viset) involved in this project. They secured the funding that made this research possible. Their support in
providing the necessary resources and their encouragement for our development as independent researchers
have been invaluable. Their contributions have thus indirectly shaped this work, and we are grateful for
their continued guidance and support. We would like to thank the anonymous reviewers for their numerous
suggestions which have greatly improved the quality of this paper. Frederiek Wesel, and thereby this work, is
supported by the Delft University of Technology AI Labs program. Arno Solin acknowledges funding from
the Research Council of Finland (grant id 339730). The underwater magnetic field data used were collected
by MARMINE/NTNU research cruise funded by the Research Council of Norway (Norges Forskningsråd,
NFR) Project No. 247626/O30 and associated industrial partners. Ocean Floor Geophysics provided the
magnetometer that was used for magnetic data acquisition and pre-processed the magnetic data. The authors
declare no competing interests.
References
Oriol Abril-Pla, Virgile Andreani, Colin Carroll, Larry Dong, Christopher J Fonnesbeck, Maxim Kochurov,
Ravin Kumar, Junpeng Lao, Christian C Luhmann, Osvaldo A Martin, et al. PyMC: a modern, and
comprehensive probabilistic programming framework in Python. PeerJ Computer Science , 9:e1516, 2023.
12Published in Transactions on Machine Learning Research (MM/2024)
Artem Artemev, David R. Burt, and Mark van der Wilk. Tighter bounds on the log marginal likelihood of
Gaussian process regression using conjugate gradients. In Proceedings of the 38th International Conference
on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp. 362–372. PMLR, 2021.
Karl Berntorp. Online Bayesian inference and learning of Gaussian–process state–space models. Automatica ,
129:109613, July 2021.
Martin D. Buhmann. Radial Basis Functions: Theory and Implementations . Cambridge University Press,
2003.
Zhongming Chen, Kim Batselier, Johan A. K. Suykens, and Ngai Wong. Parallelized tensor train learning of
polynomial classifiers. IEEE Transactions on Neural Networks and Learning Systems , 29(10):4621–4632,
2018.
Lehel Csató and Manfred Opper. Sparse on-line Gaussian processes. Neural Computation , 14(3):641–668,
March 2002.
Tri Dao, Christopher M De Sa, and Christopher Ré. Gaussian quadrature for kernel features. In Advances in
Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017.
Alexander James Davies. Effective Implementation of Gaussian Process Regression for Machine Learning .
PhD thesis, University of Cambridge, 2015.
Alexander G. de G. Matthews, Mark van der Wilk, Tom Nickson, Keisuke Fujii, Alexis Boukouvalas, Pablo
León-Villagrá, Zoubin Ghahramani, and James Hensman. GPflow: A Gaussian process library using
TensorFlow. Journal of Machine Learning Research , 18(40):1–6, 2017.
Vincent Dutordoir, Nicolas Durrande, and James Hensman. Sparse Gaussian processes with spherical
harmonic features. In Proceedings of the 37th International Conference on Machine Learning , volume 119
ofProceedings of Machine Learning Research , pp. 2793–2802. PMLR, 2020.
Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, and Andrew G Wilson. GPyTorch: Blackbox
matrix-matrix Gaussian process inference with GPU acceleration. In Advances in Neural Information
Processing Systems , volume 31. Curran Associates, Inc., 2018.
Mark N. Gibbs and David J. C. MacKay. Efficient implementation of Gaussian processes for interpolation.
Technical Report, University of Cambridge, 1996.
Philip Greengard, Manas Rachh, and Alex Barnett. Equispaced fourier representations for efficient gaussian
process regression from a billion data points. arXiv, 2023.
James Hensman, Nicolò Fusi, and Neil D. Lawrence. Gaussian processes for big data. In Proceedings of the
29th Conference on Uncertainty in Artificial Intelligence , UAI, pp. 282–290. AUAI Press, 2013.
James Hensman, Alexander Matthews, and Zoubin Ghahramani. Scalable variational Gaussian process
classification. In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics ,
volume 38 of Proceedings of Machine Learning Research , pp. 351–360. PMLR, 2015.
James Hensman, Nicolas Durrande, and Arno Solin. Variational Fourier features for Gaussian processes. The
Journal of Machine Learning Research , 18(1):5537–5588, January 2017.
Pavel Izmailov, Alexander Novikov, and Dmitry Kropotov. Scalable Gaussian processes with billions of
inducing inputs via tensor train decomposition. In Proceedings of the 21st International Conference on
Artificial Intelligence and Statistics , volume 84 of Proceedings of Machine Learning Research , pp. 726–735.
PMLR, 2018.
Dohyun Jang, Jaehyun Yoo, Clark Youngdong Son, Dabin Kim, and H. Jin Kim. Multi–robot active sensing
and environmental model learning with distributed Gaussian process. IEEE Robotics and Automation
Letters, 5(4):5905–5912, 2020.
13Published in Transactions on Machine Learning Research (MM/2024)
Diederik P. Kingma and Jimmy Ba. Adam. Adam: A method for stochastic optimization. In Proceedings of
the 3rd International Conference on Learning Representations (ICLR) , 2015.
Manon Kok and Arno Solin. Scalable magnetic field SLAM in 3D using Gaussian process maps. In Proceedings
of the 21st International Conference on Information Fusion (FUSION) , pp. 1353–1360, 2018.
Anton Kullberg, Isaac Skog, and Gustaf Hendeby. Online joint state inference and learning of partially
unknown state–space models. IEEE Transactions on Signal Processing , 69:4149–4161, 2021.
Miguel Lázaro-Gredilla and Aníbal Figueiras-Vidal. Inter-domain Gaussian processes for sparse inference
using inducing features. In Advances in Neural Information Processing Systems. , volume 22, 2009.
Miguel Lázaro-Gredilla, Joaquin Quiñonero-Candela, Carl Edward Rasmussen, and Aníbal R. Figueiras-Vidal.
Sparse spectrum Gaussian process regression. Journal of Machine Learning Research , 11(63):1865–1881,
2010.
Jihao Andreas Lin, Javier Antorán, Shreyas Padhy, David Janz, José Miguel Hernández-Lobato, and Alexander
Terenin. Sampling from Gaussian process posteriors using stochastic gradient descent. In Advances in
Neural Information Processing Systems , volume 37, 2023a.
Jihao Andreas Lin, Shreyas Padhy, Javier Antorán, Austin Tripp, Alexander Terenin, Csaba Szepesvári,
José Miguel Hernández-Lobato, and David Janz. Stochastic gradient descent for Gaussian processes done
right. arXiv:2310.20581, 2023b.
Finn Lindgren, David Bolin, and Håvard Rue. The SPDE approach for Gaussian and non-Gaussian fields: 10
Years and still running. Spatial Statistics , 50:100599, August 2022.
Marina Munkhoeva, Yermek Kapushev, Evgeny Burnaev, and Ivan Osele ˜dets. Quadrature-based features
for kernel approximation. In Advances in Neural Information Processing Systems , volume 31. Curran
Associates, Inc., 2018.
Mojmír Mutný and Andreas Krause. Efficient high dimensional Bayesian optimization with additivity and
quadrature Fourier features. In Advances in Neural Information Processing Systems , volume 32, pp.
9019–9030, 2018.
A. Novikov, M. Trofimov, and I. Oseledets. Exponential machines. Bulletin of the Polish Academy of Sciences
Technical Sciences , 66(6 (Special Section on Deep Learning: Theory and Practice)):789–797, 2018.
Gianluigi Pillonetto, Luca Schenato, and Damiano Varagnolo. Distributed multi–agent Gaussian regression
via finite–dimensional approximations. IEEE Transactions on Pattern Analysis and Machine Intelligence ,
41(9):2098–2111, 2019.
Thomas Pinder and Daniel Dodd. GPJax: A Gaussian process framework in JAX. Journal of Open Source
Software, 7(75):4455, 2022.
Geoff Pleiss, Jacob Gardner, Kilian Weinberger, and Andrew Gordon Wilson. Constant-time predictive
distributions for Gaussian processes. In Proceedings of the 35th International Conference on Machine
Learning , volume 80 of Proceedings of Machine Learning Research , pp. 4114–4123. PMLR, 2018.
Joaquin Quiñonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate Gaussian
process regression. Journal of Machine Learning Research , 6(65):1939–1959, 2005.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural
Information Processing Systems , volume 20. Curran Associates, Inc., 2007.
C.E. Rasmussen and C.K.I. Williams. Gaussian Processes for Machine Learning . Adaptive Computation and
Machine Learning series. MIT Press, 2006.
Gabriel Riutort-Mayol, Paul-Christian Bürkner, Michael R. Andersen, Arno Solin, and Aki Vehtari. Practical
Hilbert space approximate Bayesian Gaussian processes for probabilistic programming. Statistics and
Computing , 33(1):17, December 2022.
14Published in Transactions on Machine Learning Research (MM/2024)
Matthias W. Seeger, Christopher K. I. Williams, and Neil D. Lawrence. Fast forward selection to speed up
sparse Gaussian process regression. In International Workshop on Artificial Intelligence and Statistics ,
volume 9 of Proceedings of Machine Learning Research , pp. 254–261. PMLR, 2003.
Paz Fink Shustin and Haim Avron. Gauss-Legendre features for Gaussian process regression. Journal of
Machine Learning Research , 23(92):1–47, 2022.
Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in
Neural Information Processing Systems , volume 18. MIT Press, 2005.
Arno Solin and Simo Särkkä. Hilbert space methods for reduced-rank Gaussian process regression. Statistics
and Computing , 30(2):419–446, March 2020.
Arno Solin, Manon Kok, Niklas Wahlström, Thomas B Schön, and Simo Särkkä. Modeling and interpolation
of the ambient magnetic field by Gaussian processes. IEEE Transactions on Robotics , 34(4):1112–1127,
September 2015.
Gilbert Strang and Shev MacNamara. Functions of difference matrices are Toeplitz plus Hankel. SIAM
Review, 56(3):525–546, 2014.
Andreas Svensson, Arno Solin, Simo Särkkä, and Thomas Schön. Computationally efficient Bayesian learning
of Gaussian process state space models. In Proceedings of the 19th International Conference on Artificial
Intelligence and Statistics , volume 51, pp. 213–221. PMLR, 2016.
Michalis Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Proceedings of the
12th International Conference on Artificial Intelligence and Statistics , volume 5 of Proceedings of Machine
Learning Research , pp. 567–574. PMLR, 2009.
Anthony Tompkins and Fabio Ramos. Fourier feature approximations for periodic kernels in time-series
modelling. Proceedings of the AAAI Conference on Artificial Intelligence , 32(1), 2018.
Jarno Vanhatalo and Aki Vehtari. Modelling local and global phenomena with sparse Gaussian processes. In
Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence (UAI) , pp. 571 – 578. AUAI
Press, 2008.
Frida Viset, Rudy Helmons, and Manon Kok. Distributed multi-agent magnetic field norm slam with gaussian
processes. In 2023 26th International Conference on Information Fusion (FUSION) , pp. 1–8, 2023.
Sander Wahls, Visa Koivunen, H. Vincent Poor, and Michel Verhaegen. Learning multidimensional Fourier
series with tensor trains. In 2014 IEEE Global Conference on Signal and Information Processing (GlobalSIP) ,
pp. 394–398. IEEE, 2014.
Andrew Wilson and Hannes Nickisch. Kernel interpolation for scalable structured Gaussian processes
(KISS–GP). In Proceedings of the 32nd International Conference on Machine Learning , volume 37 of
Proceedings of Machine Learning Research , pp. 1775–1784. PMLR, 2015.
ZongminWu. Compactlysupportedpositivedefiniteradialfunctions. Advances in Computational Mathematics ,
4(1):283–292, 1995.
Mohit Yadav, Daniel Sheldon, and Cameron Musco. Faster kernel interpolation for Gaussian processes. In
Proceedings of The 24th International Conference on Artificial Intelligence and Statistics , volume 130 of
Proceedings of Machine Learning Research , pp. 2971–2979. PMLR, 2021.
15Published in Transactions on Machine Learning Research (MM/2024)
Appendix
The majoriy of the appendix covers proofs of the corollaries following from Theorems 3.1 and 3.4. The rest
is dedicated to further details on our empirical experiments as well as visual explanations of the structures
that are exploited in the main body of the paper. The appendix is organized as follows. App. A proves
Theorem 3.1 for polynomial Basis Functions (bfs). App. B proves Theorem 3.1 for complex exponential
bfs. App. C proves Theorem 3.4 for Hilbert space bfs defined on a rectangular domain. App. D proves
Theorem 3.4 for ordinary Fourier features. App. E contains visual representations of the structures explained
in the main body of the paper. Lastly, App. F provides a full description of all the included experiments and
the data used in them, with additional plots and results.
A Proof of Corollary 3.2 (use of Theorem 3.1 for Polynomial Basis Functions)
Polynomial bfss (as used in for example Chen et al. (2018)) are defined along each dimension as
ϕ(d)
id(x(d)) = (x(d))id−1(24)
By selecting gid+jd(x(d)) = (x(d))jd+id−1, the product of two component bfs becomes
ϕ(d)
id(x(d))ϕ(d)
jd(x(d)) = (x(d))id−1(x(d))jd−1= (x(d))jd+id−1=gid+jd, (25)
which shows that the conditions for Theorem 3.1 is satisfied.
B Proof of Corollary 3.3 (use of Theorem 3.1 for Complex Exponential Basis
Functions)
Complex exponential bfs (as used in for example Novikov et al. (2018)) are defined along each dimension as
ϕ(d)
id(x(d)) = exp(iπjdx(d)) (26)
By selecting gid+jd(x(d)) = exp(iπ(jd+id)x(d)), the product of two component bfs becomes
ϕ(d)
id(x(d))ϕ(d)
jd(x(d)) = exp(iπidx(d)) exp(iπjdx(d)) = exp(iπ(jd+id)x(d)) =gid+jd, (27)
which shows that the conditions for Theorem 3.1 is satisfied.
CProof of Corollary 3.5 (use of Theorem 3.4 for Hilbert Space Basis Functions on a
Rectangular Domain)
Hilbert space bfs on a rectangular domain are defined according to (Solin & Särkkä, 2020)
ϕi(x) =D/productdisplay
d=11√Ldsin/parenleftbiggπid(xd+Ld)
2Ld/parenrightbigg
=D/productdisplay
d=11√Ldcos/parenleftbiggπid(xd+Ld)
2Ld−π
2/parenrightbigg
, (28)
where the indices i=1,...,MDhas a one-to-one mapping with all possible combinations of the indices
i1,i2,...,i D, given that each index id={1,...,D}.
This corresponds to defining the bfs according to Eq. (6a), with each entry of ϕ(d)(x)defined as
ϕ(d)
id(x) =1√Ldsin/parenleftbiggπid(x+Ld)
2L/parenrightbigg
=1√Ldcos/parenleftbiggπid(x+Ld)
2Ld−π
2/parenrightbigg
,
whereid∈{1,...,m}. Define a linear function θid:R→Ras
θid(x) =πid(x+Ld)
2Ld−π
2.
16Published in Transactions on Machine Learning Research (MM/2024)
Hence, the bfs can be written as
ϕ(d)(x) =1√Ld
cos(θ1)
...
cos(θM)
.
The product ϕ(d)(x(d)
n)ϕ(d)(x(d)
n)⊤is then given by
ϕ(d)(x(d)
n)ϕ(d)(x(d)
n)⊤=1
LdN/summationdisplay
n=1
cos(θ1) cos(θ1)... cos(θ1) cos(θM)
.........
cos(θM) cos(θM)... cos(θM) cos(θM)
.
Then, note that
cos(u) cos(v) =cos(u+v) + cos(u−v)
2. (29)
When we apply this to each entry in the matrix, we get that
ϕ(d)(x(d)
n)ϕ(d)(x(d)
n)⊤=
1
2Ld/parenleftiggN/summationdisplay
n=1
cos(θ1+θ1)... cos(θ1+θM)
.........
cos(θM+θ1)... cos(θM+θM)

/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜Gd,(1)+N/summationdisplay
n=1
cos(θ1−θ1)... cos(θ1−θM)
.........
cos(θM−θ1)... cos(θM−θM)

/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜Gd,(−1)/parenrightigg
,
where Gd,(1)andGd,(−1)are a Hankel and a Toeplitz matrix, respectively. Then, since cos(θid±θjd) =
±sin(θid±jd)≜g(id±jd) =g(kd), there exists a function g(kd)that fulfills conditions for Theorem 3.4.
D Proof of Corollary 3.6 (use of Theorem 3.4 for Ordinary Fourier Features)
Ordinary Fourier features for separable kernels are defined slightly differently from Hilbert Space bfs (Hensman
et al., 2017), in that they consider both a sine and a cosine function for each considered frequency. The set of
bfs are given by
ϕ(d)(x) =/bracketleftig
ϕ(d)
sin(x)ϕ(d)
cos(x)/bracketrightig
=/bracketleftbigsin(∆x) sin(2∆x)... sin(md∆) cos(∆x) cos(2∆x)... cos(md∆)/bracketrightbig
,(30)
where ∆determines the spacing of the Fourier features in the frequency domain. The precision matrix can
therefore be expressed as
Φ⊤Φ=/bracketleftbiggΦ⊤
sinΦsin(Φ⊤
cosΦsin)⊤
Φ⊤
cosΦsinΦ⊤
cosΦcos/bracketrightbigg
, (31)
and we can apply Theorem 3.4 directly to entries Φ⊤
sinΦsinandΦ⊤
sinΦcosto prove that each of these have a
block Hankel–Toeplitz structure.
For the matrix Φ⊤
sinΦsin, the product ϕsin(x(d)
n)ϕsin(x(d)
n)⊤can be expanded as
{ϕsin(x(d)
n)ϕsin(x(d)
n)⊤}i,j= sin(i∆x(d)) sin(j∆x(d)) ={G(d),(−1)}i,j+{G(d),(1)}i,j, (32)
where
{G(d),(1)}i,j= cos(i∆x(d)+j∆x(d)), (33)
and
{G(d),(−1)}i,j= cos(∆x(d)−∆x(d)). (34)
The entry at row iand column jofG(d),(1)can therefore be defined by the function g(i+j) =−cos(i+
j)∆x(d), and the entry at row iand column jofG(d),(−1)is given by−g(i−j), satisfying the requirements
for Theorem 3.4.
17Published in Transactions on Machine Learning Research (MM/2024)
For the matrix Φ⊤
cosΦsin, the product ϕcos(x(d)
n)ϕsin(x(d)
n)⊤can be expanded as
{ϕcos(x(d)
n)ϕsin(x(d)
n)⊤}i,j= cos(i∆x(d)) sin(j∆x(d)) ={G(d),(−1)}i,j+{G(d),(1)}i,j; (35)
where
{G(d),(1)}i,j= sin(i∆x(d)+j∆x(d)), (36)
and
{G(d),(−1)}i,j= sin(i∆x(d)−j∆x(d)) (37)
The entry at row iand column jofG(d),(1)can therefore be defined by the function g(i+j) =−cos((i+
j)∆x(d)), and the entry at row iand column jofG(d),(−1)is given by−g(i−j), satisfying the requirements
for Theorem 3.4.
For the matrix Φ⊤
cosΦcos, the product ϕcos(x(d)
n)ϕcos(x(d)
n)⊤can be expanded as
{ϕcos(x(d)
n)ϕcos(x(d)
n)⊤}i,j= cos(i∆x(d)) cos(j∆x(d)) ={G(d),(−1)}i,j+{G(d),(1)}i,j; (38)
where
{G(d),(1)}i,j= cos(i∆x(d)+j∆x(d)), (39)
and
{G(d),(−1)}i,j= cos(i∆x(d)−j∆x(d)). (40)
The entry at row iand column jofG(d),(1)can therefore be defined by the function g(i+j) =cos((i+j)∆x(d)),
and the entry at row iand column jofG(d),(−1)is given byg(i−j). An important notion for this matrix which
makes it different from ΦsinΦ⊤
sinandΦcosΦ⊤
sinis that this does not exactly satisfy the criteria for Theorem 3.4.
The difference is that for the criteria to be exactly satisfied, entry {G(d),(−1)}i,jwould have to be equal to
−g(i−j)rather than g(i−j). However, by applying the proof of Theorem 3.4, but now noticing that the
entries of C=Φ⊤
cosΦcoscan be expressed element-wise as
Ci,j=2D/summationdisplay
p=1N/summationdisplay
n=1D/productdisplay
d=1g(id+e(d)
pjd), (41)
which allows us to use the tensor γas defined in Eq. (22) to express each entry of Caccording to
Ci,j=2D/summationdisplay
pγi1+e(1)
p,...,i D+e(D)
p. (42)
EOverview of Hankel, Toeplitz, and D-level Block Hankel–Toeplitz Matrix Structures
An overview of Hankel, Toeplitz and D-level Block Hankel–Toeplitz matrix structures are given in Table A1.
F Experiment Details
More specific details of the numerical experiments are provided here with additional visualizations and
explanations.
F.1 U.S. Precipitation Data
The precipitation data is two-dimensional with N= 5776data points first considered in (Vanhatalo & Vehtari,
2008). We perform regression in the lat/lon domain and first center the data (but do not perform scaling)
and use a simple squared-exponential kernel. We optimized the mllusing GPJax (Pinder & Dodd, 2022)
for both the hgpas well as the standard gpfor100iterations using Adam (Kingma & Adam, 2015). The
hyperparameters of the kernel and likelihood were initialized as l= 1,σ2
SE= 10andσe= 1, wherelis the
lengthscale, σ2
SEis the kernel variance and σethe noise standard deviation. Purely for visualization purposes,
the inputs were projected to a local coordinate system given by CRS 5070 which are used for all of the plots.
The original data is plotted in Fig. A7. The timing experiments were run using md= 5,10,..., 65bfs along
each dimension, totaling between M= 25andM= 4225bfs.
18Published in Transactions on Machine Learning Research (MM/2024)
Table A1: An overview of the matrix structure and tensor representation for Hankel, Toeplitz, block Hankel,
block Toeplitz and block Hankel matrices. The illustrations are examples of matrices with the property
described in each row. The illustrations contain one square for each matrix entry, where the color of the
square corresponds to the value.
Structure Definition Visual Domain
Hankel H=
γ1γ2... γ m
γ2γ3... γ m+1
............
γmγm+1... γ 2m−1

γ∈RK
Toeplitz T=
γm... γ 2γ1
γm+1... γ 3γ2
............
γ2m−1... γ m+1γm

γ∈RK
D-level block Han-
kelH(D)=
H(D−1)
1 H(D−1)
2...H(D−1)
mD
H(D−1)
2 H(D−1)
3...H(D−1)
mD+1
............
H(D−1)
mDH(D−1)
mD+1...H(D−1)
2mD−1

γ∈RK1×...×KD
D-level block
ToeplitzT(D)=
T(D−1)
mD...T(D−1)
2 T(D−1)
1
T(D−1)
mD+1...T(D−1)
2 T(D−1)
2
............
T(D−1)
2mD−1...T(D−1)
mD+1T(D−1)
mD

γ∈RK1×...×KD
D-level block
Hankel–Toeplitz,
level D is HankelG(D)=
G(D−1)
1 G(D−1)
2...G(D−1)
mD
G(D−1)
2 G(D−1)
3...G(D−1)
mD+1
............
G(D−1)
mDG(D−1)
mD+1...G(D−1)
2mD−1

γ∈RK1×...×KD
D-level block
Hankel–Toeplitz,
level D is ToeplitzG(D)=
G(D−1)
mD...G(D−1)
2 G(D−1)
1
G(D−1)
mD+1...G(D−1)
3 G(D−1)
2
............
G(D−1)
2mD−1...G(D−1)
mD+1G(D−1)
mD

γ∈RK1×...×KD
19Published in Transactions on Machine Learning Research (MM/2024)
0 1500 3000 [mm]
Figure A7: Raw data from precipitation data set. Each data point is visualized as a cross with color indicating
the precipitation. The data set contains mostly low frequency content with some high frequency content
apparent in the west coast as well as the south eastern parts.
F.2 Magnetic Field Mapping
The magnetic field data has N≈1.4milliondata points and is collected with an underwater vehicle outside
the Norwegian coast. The data used were collected by MARMINE/NTNU research cruise funded by the
Research Council of Norway (Norges Forskningsråd, NFR) Project No. 247626/O30 and associated industrial
partners. Ocean Floor Geophysics provided magnetometer that was used for magnetic data acquisition and
pre-processed the magnetic data. The data was later split into a training set and test set, at a roughly 50%
split. The nature of the data split is visualized in Fig. A8b. However, in practice, we selected the width
of the test squares and the training squares smaller than the one displayed in the illustration and they are
merely that big for visualization purposes. The illustration displays squares that are 0.01latitudinal degrees
wide and 0.03longitudinal degrees tall, corresponding to approximately 1kmin Cartesian coordinates in this
area. The split we actually used was squares which were 0.001latitudinal degrees wide and 0.003longitudinal
degrees tall, corresponding to approximately 100 min both directions in Cartesian coordinates in that area.
gpregression with a squared exponential kernel is able to extrapolate for approximately one lengthscale,
but will not necessarily give a very informative prediction one or two lengthscales away from the nearest
measurement. Although we do not know the lengthscale that would optimize the likelihood of the data before
using training data to find it, we see from a zoomed-in version of Fig. 5b approximately how fast the magnetic
field is changing across the spatial dimension and use this to make a reasonable guess at the distance we
expect a well-tuned gpto be able to extrapolate the learned magnetic field. We then project the data into
a local coordinate system using WGS84 and perform regression in Cartesian coordinates. We center and
standardize the data with the mean and standard deviation of the training data. A squared-exponential
kernel was used and we optimize the mllin GPJax (Pinder & Dodd, 2022) using Adam (Kingma & Adam,
2015) for 100iterations to find hyperparameters. The hyperparameters were initialized as l= 200 m,σ2
SE= 1
andσ2
y= 1. The resulting hyperparameters were lSE= 190,σ2
y= 0.0675,σ2
SE= 7.15.
20Published in Transactions on Machine Learning Research (MM/2024)
7.45 7.5 7.55 7.6 7.6573.4273.4473.46
Latitude (◦)Longitude (◦)
0 2 4 6 −2
(a) Raw magnetic field measurements for the underwater
magnetic field data. The plotted data is subsampled to
100thof the data for visualization purposes.
7.45 7.5 7.55 7.6 7.6573.4273.4473.46
Latitude (◦)Longitude (◦)Training data Test data(b) Data divided into training and test set. Determin-
istically split to ensure ensure that the lengthscale is
captured properly in the training data.
Figure A8: Magnetic field training data and test data. The data is roughly 50/50 split between training and
test set. Both training and test data are normalized only by the mean and standard deviation of the training
data.
21