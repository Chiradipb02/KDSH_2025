Published in Transactions on Machine Learning Research (07/2022)
Your Policy Regularizer is Secretly an Adversary
Rob Brekelmans brekelma@usc.edu
University of Southern California
Information Sciences Institute
Tim Genewein timgen@deepmind.com
Jordi Grau-Moya
Grégoire Delétang
Markus Kunesch
Shane Legg
Pedro Ortega pedro.ortega@gmail.com
DeepMind
Reviewed on OpenReview: https: // openreview. net/ forum? id= XXXX
Abstract
Policy regularization methods such as maximum entropy regularization are widely used in
reinforcement learning to improve the robustness of a learned policy. In this paper, we
unify and extend recent work showing that this robustness arises from hedging against
worst-case perturbations of the reward function, which are chosen from a limited set by
an implicit adversary. Using convex duality, we characterize the robust set of adversarial
reward perturbations under kl- andα-divergence regularization, which includes Shannon
and Tsallis entropy regularization as special cases. Importantly, generalization guarantees
can be given within this robust set. We provide detailed discussion of the worst-case reward
perturbations, and present intuitive empirical examples to illustrate this robustness and its
relationship with generalization. Finally, we discuss how our analysis complements previous
results on adversarial reward robustness and path consistency optimality conditions.
1 Introduction
Regularization plays a crucial role in various settings across reinforcement learning ( rl), such as trust-region
methods (Peters et al., 2010; Schulman et al., 2015; 2017; Bas-Serrano et al., 2021), offline learning (Levine
et al., 2020; Nachum et al., 2019a;b; Nachum & Dai, 2020), multi-task learning (Teh et al., 2017; Igl et al.,
2020), and soft Q-learning or actor-critic methods (Fox et al., 2016; Nachum et al., 2017; Haarnoja et al.,
2017; 2018; Grau-Moya et al., 2018). Various justifications have been given for policy regularization, such as
improved optimization (Ahmed et al., 2019), connections with probabilistic inference (Levine, 2018; Kappen
et al., 2012; Rawlik et al., 2013; Wang et al., 2021), and robustness to perturbations in the environmental
rewards or dynamics (Derman et al., 2021; Eysenbach & Levine, 2021; Husain et al., 2021).
In this work, we use convex duality to analyze the reward robustness which naturally arises from policy reg-
ularization in rl. In particular, we interpret regularized reward maximization as a two-player game between
the agent and an imagined adversary that modifies the reward function. For a policy π(a|s)regularized with
a convex function Ω(π) =Eπ[˙Ω(π)]and regularization strength 1/β, we investigate statements of the form
max
π(a|s)(1−γ)Eτ(π)/bracketleftigg∞/summationdisplay
t=0γt/parenleftbigg
r(at,st)−1
β˙Ω/parenleftbig
π(at|st)/parenrightbig/parenrightbigg/bracketrightigg
= max
π(a|s)min
r′(a,s)∈Rπ(1−γ)Eτ(π)/bracketleftigg∞/summationdisplay
t=0γtr′(at,st)/bracketrightigg
,(1)
wherer′(a,s)indicates a modified reward function chosen from an appropriate robust set Rπ(see Fig. 1-
2). Eq. (1) suggests that an agent may translate uncertainty in its estimate of the reward function into
1Published in Transactions on Machine Learning Research (07/2022)
Figure 1:Robust setRπ(red region) of perturbed reward functions to which a stochastic policy generalizes,
in the sense of Eq. (2). Red star indicates the worst-case perturbed reward r′
π∗=r−∆rπ∗(Prop. 2) chosen
by the adversary. The robust set also characterizes the set of reward perturbations ∆r(a,s)that are feasible
for the adversary, which differs based on the choice of regularization function, regularization strength β, and
reference distribution π0(see Sec. 4.1 and Fig. 2). We show the robust set for the optimal single-step policy
with value estimates Q(a,s) =r(a,s)andkldivergence regularization to a uniform π0, withβ= 1. Our
robust set is larger and has a qualitatively different shape compared to the robust set of Derman et al. (2021)
(dotted lines, see Sec. 5.2).
regularization of a learned policy, which is particularly relevant in applications such as inverse rl(Ng et al.,
2000; Arora & Doshi, 2021) or learning from human preferences (Christiano et al., 2017).
This reward robustness further implies that regularized policies achieve a form of ‘zero-shot’ generalization to
newenvironmentswheretherewardisadversariallychosen. Inparticular, foranygiven π(a|s)andamodified
rewardr′∈Rπwithin the corresponding robust set, we obtain the following performance guarantee
Eτ(π)/bracketleftigg∞/summationdisplay
t=0γtr′(at,st)/bracketrightigg
≥Eτ(π)/bracketleftigg∞/summationdisplay
t=0γt/parenleftbigg
r(at,st)−1
β˙Ω/parenleftbig
πt/parenrightbig/parenrightbigg/bracketrightigg
. (2)
Eq. (2) states that the expected modified reward under π(a|s), withr′∈Rπas in Fig. 1, will be greater
than the value of the regularized objective with the original, unmodified reward. It is in this particular sense
that we make claims about robustness and zero-shot generalization throughout the paper.
Our analysis unifies recent work exploring similar interpretations (Ortega & Lee, 2014; Husain et al., 2021;
Eysenbach & Levine, 2021; Derman et al., 2021) as summarized in Sec. 5 and Table 1. Our contributions
include
•A thorough analysis of the robustness associated with klandα-divergence policy regularization,
which includes popular Shannon entropy regularization as a special case. Our derivations for the
α-divergence generalize the Tsallis entropy rlframework of Lee et al. (2019).
•We derive the worst-case reward perturbations ∆rπ=r−r′
πcorresponding to any stochastic policy
πand a fixed regularization scheme (Prop. 2).
•Fortheoptimalregularizedpolicyinagivenenvironment, weshowthatthecorrespondingworst-case
reward perturbations match the advantage function for anyα-divergence. We relate this finding to
the path consistency optimality condition, which has been used to construct learning objectives in
(Nachum et al., 2017; Chow et al., 2018), and a game-theoretic indifference condition, which occurs
at a Nash equilibrium between the agent and adversary (Ortega & Lee, 2014).
•We visualize the set Rπof adversarially perturbed rewards against which a regularized policy is
robust in Fig. 1-2, with details in Prop. 1. Our use of divergence instead of entropy regularization
to analyze the robust set clarifies several unexpected conclusions from previous work. In particular,
similar plots in Eysenbach & Levine (2021) suggest that MaxEnt rlis not robust to the reward
functionofthetrainingenvironment, andthatincreasedregularizationstrengthmayhurtrobustness.
Our analysis in Sec. 5.1 and App. F.4 establishes the expected, opposite results.
•We perform experiments for a sequential grid-world task in Sec. 4 where, in contrast to previous
work, we explicitly visualize the reward robustness and adversarial strategies resulting from our
theory. We use the path consistency or indifference conditions to certify optimality of the policy.
2Published in Transactions on Machine Learning Research (07/2022)
Ortega & Lee (2014) Eysenbach & Levine (2021) Husain et al. (2021) Derman et al. (2021) Ours
Multi-Step Analysis ✗ ✓ ✓ ✓ ✓
Worst-Case ∆r(a,s) policy form policy form value form policy (via dual lpEq. (11)) policy &value forms
Robust Set ✗ ✓ (see our App. F.4) ✗ ✓ (flexible specification) ✓
Divergence Used KL ( α= 1) Shannon entropy (Sec. 5.1) any convex Ω derived from robust set any convex Ω,α-Div examples
µ(a,s)orπ(a|s)Reg.? π(a|s) π(a|s) Both π(a|s) Both
Indifference ✓ ✗ ✗ ✗ ✓
Path Consistency ✗ ✗ ✗ ✗ ✓
Table 1: Comparison to related work.
2 Preliminaries
In this section, we review linear programming ( lp) formulations of discounted Markov Decision Processes
(mdp) and extensions to convex policy regularization.
Notation For a finite setX, letRXdenote the space of real-valued functions over X, with RX
+indicating
restriction to non-negative functions. We let ∆|X|denote the probability simplex with dimension equal to
the cardinality of X. Forµ,q∈RX,⟨µ,q⟩=/summationtext
x∈Xµ(x)q(x)indicates the inner product in Euclidean space.
2.1 Convex Conjugate Function
We begin by reviewing the convex conjugate function, also known as the Legendre-Fenchel transform, which
will play a crucial role throughout our paper. For a convex function Ω(µ)which, in our context, has domain
µ∈RX
+, the conjugate function Ω∗is defined via the optimization
Ω∗(∆r) = sup
µ∈RX
+/angbracketleftbig
µ,∆r/angbracketrightbig
−Ω(µ), (3)
where ∆r∈RX. The conjugate operation is an involution for proper, lower semi-continuous, convex Ω
(Boyd & Vandenberghe, 2004), so that (Ω∗)∗= ΩandΩ∗is also convex. We can thus represent Ω(µ)via a
conjugate optimization
Ω(µ) = sup
∆r∈RX/angbracketleftbig
µ,∆r/angbracketrightbig
−Ω∗(∆r). (4)
Differentiating with respect to the optimization variable in Eq. (3) or (4) suggests the optimality conditions
µ∆r=∇Ω∗(∆r) ∆rµ=∇Ω(µ). (5)
Note that the above conditions also imply relationships of the form µ∆r= (∇Ω)−1(∆r). This dual corre-
spondence between values of µand∆rwill form the basis of our adversarial interpretation in Sec. 3.
2.2 Divergence Functions
We are interested in the conjugate duality associated with policy regularization, which is often expressed
using a statistical divergence Ω(µ)over a joint density µ(a,s) =µ(s)π(a|s)(see Sec. 2.3). In particular,
we consider the family of α-divergences (Amari, 2016; Cichocki & Amari, 2010), which includes both the
forward and reverse kldivergences as special cases. In the following, we consider extended divergences that
accept unnormalized density functions as input (Zhu & Rohwer, 1995) so that we may analyze function space
dualities and evaluate Lagrangian relaxations without projection onto the probability simplex.
KL Divergence The ‘forward’ kldivergence to a reference policy π0(a|s)is commonly used for policy
regularization in rl. Extending the input domain to unnormalized measures, we write the divergence as
Ωπ0(µ) =Eµ(s)/bracketleftig
DKL[π:π0]/bracketrightig
=/summationdisplay
s∈Sµ(s)/summationdisplay
a∈A/parenleftbigg
π(a|s) logπ(a|s)
π0(a|s)−π(a|s) +π0(a|s)/parenrightbigg
. (6)
Using a uniform reference π0(a|s) = 1∀(a,s), we recover the Shannon entropy up to an additive constant.
α-Divergence Theα-divergence Eµ(s)/bracketleftbig
Dα[π0:π]/bracketrightbig
over possibly unnormalized measures is defined as
Ω(α)
π0(µ) =1
α(1−α)/summationdisplay
s∈Sµ(s)/parenleftbigg
(1−α)/summationdisplay
a∈Aπ0(a|s) +α/summationdisplay
a∈Aπ(a|s)−/summationdisplay
a∈Aπ0(a|s)1−απ(a|s)α/parenrightbigg
(7)
3Published in Transactions on Machine Learning Research (07/2022)
Divergence Conjugate Conjugate Expression Optimizing Argument ( π∆rorµ∆r)
1
βDKL[π:π0]1
βΩ∗
π0,β(∆r)1
β/summationtext
aπ0(a|s) exp/braceleftbig
β·∆r(a,s)/bracerightbig
−1
βπ0(a|s) exp/braceleftbig
β·∆r(a,s)/bracerightbig
1
βDKL[µ:µ0]1
βΩ∗
µ0,β(∆r)1
β/summationtext
a,sµ0(a,s) exp/braceleftbig
β·∆r(a,s)/bracerightbig
−1
βµ0(a,s) exp/braceleftbig
β·∆r(a,s)/bracerightbig
1
βDα[π0:π]1
βΩ∗(α)
π0,β(∆r)1
β1
α/summationtext
aπ0(a|s) expα/braceleftbig
β·/parenleftbig
∆r(a,s)−ψ∆r(s;β)/parenrightbig/bracerightbigα−1
β1
α+ψ∆r(s;β)π0(a|s) expα/braceleftbig
β·(∆r(a,s)−ψ∆r(s;β))/bracerightbig
1
βDα[µ0:µ]1
βΩ∗(α)
µ0,β(∆r)1
β1
α/summationtext
a,sµ0(a,s) expα/braceleftbig
β·∆r(a,s)/bracerightbigα−1
β1
αµ0(a,s) expα/braceleftbig
β·∆r(a,s)/bracerightbig
Table 2: Conjugate Function expressions for klandα-divergence regularization of either the policy π(a|s)
or occupancy µ(a,s). See App. B.1-B.4 for derivations. The final column shows the optimizing argument
in the definition of the conjugate function1
βΩ∗(∆r), for example µ∆r:= arg maxµ⟨µ,∆r⟩−1
βΩµ0(µ). Note
that each conjugate expression for π(a|s)regularization also contains an outer expectation over µ(s).
Taking the limiting behavior, we recover the ‘forward’ kldivergence DKL[π:π0]asα→1or the ‘reverse’
kldivergence DKL[π0:π]asα→0.
To provide intuition for the α-divergence, we define the deformed α-logarithm as in Lee et al. (2019), which
matches Tsallis’s q-logarithm (Tsallis, 2009) for α= 2−q. Its inverse is the α-exponential, with
logα(u) =1
α−1/parenleftbig
uα−1−1/parenrightbig
, expα(u) = [1 + (α−1)u]1
α−1
+. (8)
where [·]+= max(·,0)ensures fractional powers can be taken and suggests that expα(u) = 0foru≤
1/(1−α). Using the α-logarithm, we can rewrite the α-divergence similarly to the kldivergence in Eq. (6)
Ω(α)
π0(µ) =1
α/summationdisplay
s∈Sµ(s)/parenleftbigg/summationdisplay
a∈Aπ(a|s) logαπ(a|s)
π0(a|s)−π(a|s) +π0(a|s)/parenrightbigg
.
For a uniform reference π0, theα-divergence differs from the Tsallis entropy by only the 1/αfactor and an
additive constant (see App. F.1).
2.3 Unregularized MDPs
A discounted mdpis a tuple{S,A,P,ν 0,r,γ}consisting of a state space S, action spaceA, transition
dynamicsP(s′|s,a)fors,s′∈S,a∈A, initial state distribution ν0(s)∈∆|S|in the probability simplex,
and reward function r(a,s) :S×A∝⇕⊣√∫⊔≀→ R. We also use a discount factor γ∈(0,1)(Puterman (1994) Sec 6).
We consider an agent that seeks to maximize the expected discounted reward by acting according to
a decision policy π(a|s)∈∆|A|for eachs∈ S. The expected reward is calculated over trajectories
τ∼π(τ) :=ν0(s0)/producttextπ(at|st)P(st+1|st,at), which begin from an initial s0∼ν0(s)and evolve according
to the policy π(a|s)andmdpdynamicsP(s′|s,a)
RL(r) := max
π(a|s)(1−γ)Eτ∼π(τ)/bracketleftbigg∞/summationdisplay
t=0γtr(st,at)/bracketrightbigg
. (9)
We assume that the policy is stationary and Markovian, and thus independent of both the timestep and
trajectory history.
Linear Programming Formulation We will focus on a linear programming ( lp) form for the objective
in Eq. (9), which is common in the literature on convex duality. With optimization over the discounted state-
action occupancy measure, µ(a,s):= (1−γ)Eτ∼π(τ)[/summationtext∞
t=0γtI(at=a,st=s)], we rewrite the objective as
RL(r):= max
µ/angbracketleftbig
µ,r/angbracketrightbig
subject to µ(a,s)≥0∀(a,s)∈A×S, (10)
/summationdisplay
aµ(a,s) = (1−γ)ν0(s) +γ/summationdisplay
a′,s′P(s|a′,s′)µ(a′,s′)∀s∈S.
We refer to the constraints in the second line of Eq. (10) as the Bellman flow constraints , which force µ(a,s)
to respect the mdpdynamics. We denote the set of feasible µasM⊂ RA×S
+. For normalized ν0(s)and
P(s|a′,s′), we show in App. A.2 that µ(a,s)∈Mimpliesµ(a,s)is normalized.
4Published in Transactions on Machine Learning Research (07/2022)
It can be shown that any feasible µ(a,s)∈Minduces a stationary π(a|s) =µ(a,s)/µ(s), whereµ(s):=/summationtext
a′µ(a′,s)andπ(a|s)∈∆|A|is normalized by definition. Conversely, any stationary policy π(a|s)induces
a unique state-action visitation distribution µ(a,s)(Syed et al. (2008), Feinberg & Shwartz (2012) Sec. 6.3).
Along with the definition of µ(a,s)above, this result demonstrates the equivalence of the optimizations in
Eq. (9) and Eq. (10). We will proceed with the lpnotation from Eq. (10) and assume µ(s)is induced by
π(a|s)whenever the two appear together in an expression.
Importantly, the flow constraints in Eq. (10) lead to a dual optimization which reflects the familiar Bellman
equations (Bellman, 1957). To see this, we introduce Lagrange multipliers V∈RSfor each flow constraint
andλ(a,s)∈RA×S
+for the nonnegativity constraints. Summing over s∈S, and eliminating µ(a,s)by
settingd/dµ (a,s) = 0yields the duallp
RL∗(r):= min
V,λ(1−γ)/angbracketleftbig
ν0,V/angbracketrightbig
subject to V(s) =r(a,s) +γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
+λ(a,s)∀(a,s)∈A×S,(11)
where we have used Es′
a,s/bracketleftbig
V(s′)/bracketrightbig
as shorthand for EP(s′|a,s)[V(s′)]and reindexed the transition tuple from
(s′,a′,s)to(s,a,s′)compared to Eq. (10). Note that the constraint applies for all (a,s)∈A×S and that
λ(a,s)≥0. By complementary slackness, we know that λ(a,s) = 0for(a,s)such thatµ(a,s)>0.
2.4 Regularized MDPs
We now consider regularizing the objective in Eq. (10) using a convex penalty function Ω(µ)with coefficient
1/β. We primarily focus on regularization using a conditional divergence Ωπ0(µ):=Eµ(s)π(a|s)[˙Ω(π)]between
the policy and a normalized reference distribution π0(a|s), as in Sec. 2.2 and (Ortega & Braun, 2013; Fox
et al., 2016; Haarnoja et al., 2017; 2018). We also use the notation Ωµ0(µ) =Eµ(a,s)[˙Ω(µ)]to indicate
regularization of the full state-action occupancy measure to a normalized reference µ0(a,s), which appears,
for example, in Relative Entropy Policy Search ( reps) (Peters et al., 2010; Belousov & Peters, 2019). The
regularized objective RLΩ,β(r)is then defined as
RLΩ,β(r) := max
µ∈M/angbracketleftbig
µ,r/angbracketrightbig
−1
βΩπ0(µ) (12)
where Ωπ0(µ)contains an expectation under µ(a,s)as in Eq. (6)-(7). We can also derive a dual version of
the regularized lp, by first writing the Lagrangian relaxation of Eq. (12)
max
µmin
V,λ(1−γ)/angbracketleftbig
ν0,V/angbracketrightbig
+⟨µ,r+γEs′
a,s/bracketleftbig
V/bracketrightbig
−V+λ⟩−1
βΩπ0(µ). (13)
Swapping the order of optimization under strong duality, we can recognize the maximization over µ(a,s)as
a conjugate function1
βΩ∗
π0,β, as in Eq. (3), leading to a regularized dual optimization
RL∗
Ω,β(r) = min
V,λ(1−γ)/angbracketleftbig
ν0,V/angbracketrightbig
+1
βΩ∗
π0,β/parenleftig
r+γEs′
a,s/bracketleftbig
V/bracketrightbig
−V+λ/parenrightig
(14)
which involves optimization over dual variables V(s)only and is unconstrained, in contrast to Eq. (11).
Dual objectives of this form appear in (Nachum & Dai, 2020; Belousov & Peters, 2019; Bas-Serrano et al.,
2021; Neu et al., 2017). We emphasize the need to include the Lagrange multiplier λ(a,s), withλ(a,s)>0
when the optimal policy has π∗(a|s) = 0, since an important motivation for α-divergence regularization is
to encourage sparsity in the policy (see Eq. (8), Lee et al. (2018; 2019); Chow et al. (2018)).
SoftValueAggregation Initerativealgorithmssuchas(regularized)modifiedpolicyiteration(Puterman
& Shin, 1978; Scherrer et al., 2015), it is useful to consider the regularized Bellman optimality operator (Geist
et al., 2019). For given estimates of the state-action value Q(a,s):=r(a,s) +γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
, the operator
T∗
Ωπ0,βupdatesV(s)as
V(s)←1
βΩ∗
π0,β(Q) = max
π∈∆|A|/angbracketleftbig
π,Q/angbracketrightbig
−1
βΩπ0(π). (15)
Note that this conjugate optimization is performed in each state s∈Sand explicitly constrains each π(a|s)
to be normalized. Although we proceed with the notation of Eq. (12) and Eq. (14), our later developments
are compatible with the ‘soft-value aggregation’ perspective above. See App. C for detailed discussion.
5Published in Transactions on Machine Learning Research (07/2022)
3 Adversarial Interpretation
In this section, we interpret regularization as implicitly providing robustness to adversarial perturbations
of the reward function. To derive our adversarial interpretation, recall from Eq. (4) that conjugate duality
yields an alternative representation of the regularizer
1
βΩ(α)
π0(µ) = max
∆r∈RA×S⟨µ,∆r⟩−1
βΩ∗(α)
π0,β(∆r). (16)
Using this conjugate optimization to expand the regularization term in the primal objective of Eq. (12),
RLΩ,β(r) = max
µ∈Mmin
∆r∈RA×S⟨µ,r−∆r⟩+1
βΩ∗(α)
π0,β/parenleftbig
∆r/parenrightbig
. (17)
We interpret Eq. (17) as a two-player minimax game between an agent and an implicit adversary, where
the agent chooses an occupancy measure µ(a,s)∈Mor its corresponding policy π(a|s), and the adversary
chooses reward perturbations ∆r(a,s)subject to the convex conjugate1
βΩ∗(α)
π0,β(∆r)as a penalty function
(Ortega & Lee, 2014).
To understand the limitations this penalty imposes on the adversary, we transform the optimization over
∆rin Eq. (17) to a constrained optimization in Sec. 3.1. This allows us to characterize the feasible set of
reward perturbations available to the adversary or, equivalently, the set of modified rewards r′(a,s)∈Rπ
to which a particular stochastic policy is robust. In Sec. 3.2 and 3.4, we interpret the worst-case adversarial
perturbations corresponding to an arbitrary stochastic policy and the optimal policy, respectively.
3.1 Robust Set of Modified Rewards
In order to link our adversarial interpretation to robustness and zero-shot generalization as in Eq. (1)-(2), we
characterize the feasible set of reward perturbations in the following proposition. We state our proposition
for policy regularization, and discuss differences for µ(a,s)regularization in App. D.2.
Proposition 1. Assume a normalized policy π(a|s)for the agent is given, with/summationtext
aπ(a|s) = 1∀s∈S.
Underα-divergence policy regularization to a normalized reference π0(a|s), the optimization over ∆r(a,s)in
Eq.(17)can be written in the following constrained form
min
∆r∈R∆π/angbracketleftbig
µ,r−∆r/angbracketrightbig
whereR∆
π:=/braceleftbigg
∆r∈RA×S/vextendsingle/vextendsingle/vextendsingle/vextendsingleΩ∗(α)
π0,β(∆r)≤0/bracerightbigg
, (18)
We refer toR∆
π⊂RA×Sas the feasible set of reward perturbations available to the adversary. This translates
to a robust setRπof modified rewards r′(a,s) =r(a,s)−∆r(a,s)for the given policy. These sets depend
on theα-divergence and regularization strength βvia the conjugate function.
Forkldivergence regularization, the constraint is
/summationdisplay
a∈Aπ0(a|s) exp/braceleftbig
β·∆r(a,s)/bracerightbig
≤1. (19)
See App. D.1 for proof, and Table 2 for the convex conjugate function1
βΩ∗(α)
π0,β(∆r)associated with various
regularization schemes. The proof proceeds by evaluating the conjugate function at the minimizing argument
∆rπin Eq. (17) (see Sec. 3.2), with Ω∗(α)
π0,β(∆rπ) = 0∀αfor normalized π(a|s)andπ0(a|s). The constraint
then follows from the fact that Ω∗(α)
π0,β(∆rπ)is convex and increasing in ∆r(Husain et al., 2021). We visualize
the robust set for a two-dimensional action space in Fig. 2, with additional discussion in Sec. 4.1.
As in Eq. (2), we can provide ‘zero-shot’ performance guarantees using this set of modified rewards. For any
perturbed reward in the robust set r′∈Rπ, we have⟨µ,r′⟩≥⟨µ,r⟩−1
βΩ(α)
π0(µ), so that the policy achieves
an expected modified reward which is at least as large as the regularized objective. However, notice that this
form of robustness is sensitive to the exact value of the regularized objective function. Although entropy
regularization and divergence regularization with a uniform reference induce the same optimal µ(a,s), we
highlight crucial differences in their reward robustness interpretations in Sec. 5.1.
6Published in Transactions on Machine Learning Research (07/2022)
3.2 Worst-Case Perturbations: Policy Form
From the feasible set in Prop. 1, how should the adversary select its reward perturbations? In the following
proposition, we use the optimality conditions in Eq. (5) to solve for the worst-case reward perturbations
∆rπ(a,s)which minimize Eq. (17) for an fixed but arbitrary stochastic policy π(a|s).
Proposition2. For a given policy π(a|s)or state-action occupancy µ(a,s), the worst-case adversarial reward
perturbations ∆rπor∆rµassociated with a convex function Ω(µ)and regularization strength 1/βare
∆rπ=∇µ1
βΩ(µ). (20)
See App. A.1 for proof. We now provide example closed form expressions for the worst-case reward per-
turbations under common regularization schemes. We emphasize that the same stochastic policy π(a|s)or
joint occupancy measure µ(a,s)can be associated with different adversarial perturbations depending on the
choice ofα-divergence and strength β.1
KL Divergence Forkldivergence policy regularization, the worst-case reward perturbations are
∆rπ(a,s) =1
βlogπ(a|s)
π0(a|s), (21)
which corresponds to the pointwise regularization ∆rπ(a,s) = ˙Ωπ0(π(a|s)for each state-action pair, with
Ωπ0(µ) =Eµ(a,s)[˙Ωπ0(π(a|s)]. See App. B.1. We show an analogous result in App. B.2 for state-action
occupancy regularization DKL[µ:µ0], where ∆rµ(a,s) =1
βlogµ(a,s)
µ0(a,s)=˙Ωµ0(µ(a,s)).
α-Divergence Forkldivergence regularization, the worst-case reward perturbations had a similar expres-
sion for conditional and joint regularization. However, we observe notable differences for the α-divergence
in general. For policy regularization to a reference π0,
∆rπ(a,s) =1
βlogαπ(a|s)
π0(a|s)+ψ∆r(s;β), (22)
where we define ψ∆r(s;β)as
ψ∆r(s;β):=1
β1
α/parenleftigg/summationdisplay
a∈Aπ0(a|s)−/summationdisplay
a∈Aπ0(a|s)1−απ(a|s)α/parenrightigg
. (23)
As we discuss in App. B.3, ψ∆r(s;β)plays the role of a normalization constant for the optimizing argument
π∆r(a|s)in the definition of1
βΩ∗(α)
π0,β(∆r)(see Eq. (3), Table 2). This term arises from differentiating Ω(α)
π0(µ)
with respect to µ(a,s)instead of from an explicit constraint. Assuming the given π(a|s)and reference
π0(a|s)are normalized, note that ψ∆r(s;β) =1
β(1−α)Dα[π0:π]. With normalization, we also observe that
ψ∆r(s;β) = 0forkldivergence regularization ( α= 1), which confirms Eq. (21) is a special case of Eq. (22).
For any given state-action occupancy measure µ(a,s)and jointα-divergence regularization to a reference
µ0(a|s), the worst-case perturbations become
∆rµ(a,s) =1
βlogαµ(a,s)
µ0(a,s), (24)
with detailed derivations in App. B.4. In contrast to Eq. (22), this expression lacks an explicit normalization
constant, as this constraint is enforced by the Lagrange multipliers V(s)andµ(a,s)∈M(App. A.2).
1One exception is that a policy with a′s.t.π(a′|s) = 0can only be represented using klregularization if π0(a′|s) = 0.
7Published in Transactions on Machine Learning Research (07/2022)
3.3 Worst-Case Perturbations: Value Form
In the previous section, we analyzed the implicit adversary corresponding to anystochastic policy π(a|s)for
a given Ω,π0,andβ. We now take a dual perspective, where the adversary is given access to a set of dual
variablesV(s)across states s∈Sand selects reward perturbations ∆rV(a,s). We will eventually show in
Sec. 3.4 that these perturbations match the policy-form perturbations at optimality.
Our starting point is Theorem 3 of Husain et al. (2021), which arises from taking the convex conjugate
(−RL Ω,β(r))∗of theentireregularized objective RLΩ,β(r), which is concave in µ(a,s). See App. E.1.
Theorem 1 (Husain et al. (2021)) .The optimal value of the regularized objective RLΩ,β(r)in Eq.(12), or
its dualRL∗
Ω,β(r)in Eq.(14), is equal to
inf
V,λinf
∆rV(1−γ)/angbracketleftbig
ν0,V/angbracketrightbig
+1
βΩ∗(α)
π0,β(∆rV) (25)
subject to V(s) =r(a,s) +γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
−∆rV(a,s) +λ(a,s)∀(a,s)∈A×S.
Rearranging the equality constraint to solve for ∆rV(a,s)and substituting into the objective, this opti-
mization recovers the regularized dual problem in Eq. (14). We can also compare Eq. (25) to the un-
regularized dual problem in Eq. (11), which does not include an adversarial cost and whose constraint
V(s) =r(a,s)+γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
+λ(a,s)implies an unmodified reward, or ∆rV(a,s) = 0. Similarly to Sec. 3.2,
the adversary incorporates the effect of policy regularization via the reward perturbations ∆rV(a,s).
3.4 Policy Form =Value Form at Optimality
In the following proposition, we provide a link between the policy and value forms of the adversarial reward
perturbations, showing that ∆rπ∗(a,s) = ∆rV∗(a,s)for the optimal policy π∗(a|s)and valueV∗(s). As
in Eysenbach & Levine (2021), the uniqueness of the optimal policy implies that its robustness may be
associated with an environmental reward r(a,s)for a given regularized mdp.
Proposition 3. For the optimal policy π∗(a|s)and value function V∗(s)corresponding to α-divergence policy
regularization with strength β, the policy and value forms of the worst-case adversarial reward perturbations
match, ∆rπ∗= ∆rV∗, and are related to the advantage function via
∆rπ∗(a,s) =Q∗(a,s)−V∗(s) +λ∗(a,s), (26)
where we define Q∗(a,s):=r(a,s)+γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
and recallλ∗(a,s)π∗(a|s) = 0by complementary slackness.
Note thatV∗(s)depends on the regularization scheme via the conjugate function1
βΩ∗(α)
π0,β(∆rV)in Eq.(25).
Proof.See App. A.3. We consider the optimal policy in an mdpwithα-divergence policy regularization
1
βΩ(α)
π0(µ), which is derived via similar derivations as Lee et al. (2019) or by eliminating µ(a,s)in Eq. (13).
π∗(a|s) =π0(a|s) expα/braceleftbig
β·/parenleftbig
Q∗(a,s)−V∗(s) +λ(a,s)−ψ∆rπ∗(s;β)/parenrightbig/bracerightbig
. (27)
We prove Prop. 3 by plugging this optimal policy into the worst-case reward perturbations from Eq. (22),
∆rπ∗(a,s) =1
βlogαπ∗(a|s)
π0(a|s)+ψ∆rπ∗(s;β). We can also use Eq. (26) to verify π∗(a|s)is normalized, since
ψ∆rπ∗ensures normalization for the policy corresponding to ∆rπ∗. In App. C.3, we also show ψQ∗(s;β) =
V∗(s) +ψ∆rπ∗(s;β), whereψQ∗(s;β)is a Lagrange multiplier enforcing normalization in Eq. (15).
Path Consistency Condition The equivalence between ∆rπ∗(a,s)and∆rV∗(a,s)at optimality matches
thepath consistency conditions from (Nachum et al., 2017; Chow et al., 2018) and suggests generalizations
to generalα-divergence regularization. Indeed, combining Eq. (22) and (26) and rearranging,
r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−1
βlogαπ∗(a|s)
π0(a|s)−ψ∆rπ∗(s;β) =V∗(s)−λ∗(a,s) (28)
8Published in Transactions on Machine Learning Research (07/2022)
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Feasible Set
Q(s,a)
Perturbed
(a)DKL,β= 0.1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Feasible Set
Q(s,a)
Perturbed(b)DKL,β= 1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Feasible Set
Q(s,a)
Perturbed (c)DKL,β= 10
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (d)α=−1,β= 10
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case(e)α= 3,β= 10
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Feasible Set
Q(s,a)
Perturbed
(f)DKL,β= 0.1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Feasible Set
Q(s,a)
Perturbed(g)DKL,β= 1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Feasible Set
Q(s,a)
Perturbed (h)DKL,β= 10
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (i)α=−1,β= 10
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case(j)α= 3,β= 10
Figure 2: Robust Set (red region) of perturbed reward functions to which a stochastic policy generalizes,
in the sense that the policy is guaranteed to achieve an expected modified reward greater than or equal to
the value of the regularized objective (Eq. (2)). The robust set characterizes the perturbed rewards which
are feasible for the adversary. Red stars indicate the worst-case perturbed reward r′
π∗=r−∆rπ∗(Prop. 2).
We show robust sets for the optimal π∗(a|s)with fixedQ(a,s) =r(a,s)values (blue star), where the optimal
policy differs based on the regularization parameters α,β,π 0(see Eq. (27)). The robust set is more restricted
with decreasing regularization strength (increasing β), implying decreased generalization. Importantly, the
slope of the robust set boundary can be linked to the action probabilities under the policy (see Sec. 4.1).
for alls∈Sanda∈A. This is a natural result, since path consistency is obtained using the kktoptimality
condition involving the gradient with respect to µof the Lagrangian relaxation in Eq. (13). Similarly, we
have seen in Prop. 2 that ∆rπ=∇µ1
βΩ(α)
π0(µ). See App. A.4.
Path consistency conditions were previously derived for the Shannon entropy (Nachum et al., 2017) and
Tsallis entropy with α= 2(Chow et al., 2018), but our expression in Eq. (28) provides a generalization to
α-divergences with arbitrary reference policies. We provide more detailed discussion in App. E.2.
Indifference Condition As Ortega & Lee (2014) discuss for the single step case, the saddle point of the
minmax optimization in Eq. (17) reflects an indifference condition which is a well-known property of Nash
equilibria in game theory (Osborne & Rubinstein, 1994). Consider Q(a,s) =r(a,s) +γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
to be the
agent’s estimated payoff for each action in a particular state. For the optimal policy, value, and worst-case
reward perturbations, Eq. (28) shows that the pointwise modified reward Q∗(a,s)−∆rπ∗(a,s) =V∗(s)is
equal to a constant.2Against the optimal strategy of the adversary, the agent becomes indifferent between
the actions in its mixed strategy. The value or conjugate function V∗(s) =1
βΩ∗
π0,β(Q∗)(see App. C) is known
asthecertainty equivalent (Fishburn,1988;Ortega &Braun,2013), which measures thetotalexpectedutility
for an agent starting in state s, in a two-player game against an adversary defined by the regularizer Ωwith
strengthβ. We empirically confirm the indifference condition in Fig. 3 and 8.
4 Experiments
In this section, we visualize the robust set and worst-case reward perturbations associated with policy
regularization, using intuitive examples to highlight theoretical properties of our adversarial interpretation.
4.1 Visualizing the Robust Set
In Fig. 2, we visualize the robust set of perturbed rewards for the optimal policy in a two-dimensional action
space for the klorα-divergence, various β, and a uniform or non-uniform prior policy π0. Since the optimal
policy can be easily calculated in the single-step case, we consider fixed Q∗(a,s) =r(a,s) ={1.1,0.8}and
show the robustness of the optimal π∗(a|s), which differs based on the choice of regularization scheme using
Eq. (27). We determine the feasible set of ∆rusing the constraint in Prop. 1 (see App. D.3 for details), and
plot the modified reward r′
π∗(a,s) =Q∗(a,s)−∆rπ∗(a,s)for each action.
Inspecting the constraint for the adversary in Eq. (19), note that both reward increases ∆r(a,s)<0and
reward decreases ∆r(a,s)>0contribute non-negative terms at each action, which either up- or down-weight
2This holds for actions with π∗(a|s)>0andλ(a,s) = 0. Note that we treat Q(a,s)as the reward in the sequential case.
9Published in Transactions on Machine Learning Research (07/2022)
(i)β= 1.0
a1a2a3a4a5a60.00.51.01.52.0r(a,s)Environment r(a,s) 
= Agent Q-Values
a1a2a3a4a5a60.00.51.0Policy (a|s)
a1a2a3a4a5a6-1.0-0.500.51.0r(a,s)
Perturbation r(a,s)
a1a2a3a4a5a60.00.51.01.52.0r/prime(a,s)
Perturbed r/prime=rr
(ii)β= 10
a1a2a3a4a5a60.00.51.0Policy (a|s)
a1a2a3a4a5a6-1.0-0.500.51.0r(a,s)
Perturbation r(a,s)
a1a2a3a4a5a60.00.51.01.52.0r/prime(a,s)
Perturbed r/prime=rr
(a) Optimal Policy(i)β= 1.0
a1a2a3a4a5a60.00.51.01.52.0Q(a,s)Agent Q(a,s) Values
a1a2a3a4a5a60.00.51.0Policy (a|s)
a1a2a3a4a5a6-1.0-0.500.51.0r(a,s)
Perturbation r(a,s)
a1a2a3a4a5a60.00.51.01.52.0r/prime(a,s)
Perturbed r/prime=rr
(ii)β= 10
a1a2a3a4a5a60.00.51.0Policy (a|s)
a1a2a3a4a5a6-1.0-0.500.51.0r(a,s)
Perturbation r(a,s)
a1a2a3a4a5a60.00.51.01.52.0r/prime(a,s)
Perturbed r/prime=rr
(b) Suboptimal Policy
Figure 3: Single-Step Reward Perturbations forklregularization to uniform reference policy π0(a|s).
Q-values in left columns are used for each βin columns 2-4. We report the worst-case −∆rπ∗(a,s)(Eq. (22)),
so negative values correspond to reward decreases. (a)Optimal policy ( Q∗(a,s) =r(a,s)) using the environ-
ment reward, where the perturbed r′(a,s) =c∀areflects the indifference condition. (b)Suboptimal policy
where indifference does not hold. In all cases, actions with high Q(a,s)are robust to reward decreases.
the reference policy π0(a|s). The constraint on their summation forces the adversary to trade off between
perturbations of different actions in a particular state. Further, since the constraints in Prop. 1 integrate
over the action space, the rewards for allactions in a particular state must be perturbed together. While it
is clear that increasing the reward in both actions preserves the inequality in Eq. (2), Fig. 2 also includes
regions where one reward decreases.
For high regularization strength ( β= 0.1), we observe that the boundary of the feasible set is nearly linear,
with the slope−π0(a1|s)
π0(a2|s)based on the ratio of action probabilities in a policy that matches the prior. The
boundary steepens for lower regularization strength. We can use the indifference condition to provide further
geometric insight. First, drawing a line from the origin with slope 1will intersect the feasible set at the
worst-case modified reward (red star) in each panel, with r′
∗(a1,s) =r′
∗(a2,s). At this point, the slope of
the tangent line yields the ratio of action probabilities in the regularized policy, as we saw for the β= 0.1
case. With decreasing regularization as β→∞, the slope approaches 0or−∞for a nearly deterministic
policy and a rectangular feasible region.
Finally, we show the α-divergence robust set with α∈{− 1,3}andβ= 10in Fig. 2 (d)-(e) and (i)-(j),
with further visualizations in App. H. Compared to the kldivergence, we find a wider robust set boundary
forα=−1. Forα= 3andβ= 10, the boundary is more strict and we observe much smaller reward
perturbations as the optimal policy becomes deterministic ( π(a1|s) = 1) for both reference distributions.
However, in contrast to the unregularized deterministic policy, the reward perturbations ∆rπ∗(a,s)̸= 0are
nonzero. We provide a worked example in App. G, and note that indifference does not hold in this case,
r′
π∗(a1,s)̸=r′
π∗(a2,s), due to the Lagrange multiplier λ∗(a2,s)>0.
4.2 Visualizing the Worst-Case Reward Perturbations
In this section, we consider kldivergence regularization to a uniform reference policy, which is equivalent
to Shannon entropy regularization but more appropriate for analysis, as we discuss in Sec. 5.1.
SingleStepCase InFig.3, weplotthe negative worst-caserewardperturbations −∆rπ(a,s)andmodified
reward for a single step decision-making case. For the optimal policy in Fig. 3(a), the perturbations match
the advantage function as in Eq. (26) and the perturbed reward for all actions matches the value function
V∗(s). While we have shown in Sec. 3.2 that any stochastic policy may be given an adversarial interpretation,
we see in Fig. 3(b) that the indifference condition does not hold for suboptimal policies.
The nearly-deterministic policy in Fig. 3(a)(ii) also provides intuition for the unregularized case as β→∞.
Although we saw in Sec. 3.3 that ∆rπ∗(a,s) = 0∀ain the unregularized case, Eq. (11) and (26) suggest that
λ(a,s) =V∗(s)−Q∗(a,s)plays a similar role to the (negative) reward perturbations in Fig. 3(a)(ii), with
λ(a1,s) = 0andλ(a,s)>0for all other actions.
10Published in Transactions on Machine Learning Research (07/2022)
(a) Environment
(uniformν0(s),
r(a,s) =−1for water,
r(a,s) = 5for goal)
0 1 2 3 4 50
1
2
3
4
5-0.0
-0.0
-0.0
-0.0
-0.1
-0.0-0.0
0.0
-0.1
0.0
-0.1
-0.00.0
0.1
-0.2
0.2
-0.2
-0.1-0.0
-0.4
-2.2
0.1
-0.7
-0.2-0.0
-1.3
2.7
0.4
0.1-0.0
-0.5
-1.2
1.2
0.7
0.30.0
0.0
-0.0
0.1
-0.0
-0.1-0.1
0.1
-0.0
0.1
0.0
-0.0-0.1
0.2
-0.2
0.2
0.1
-0.0-0.7
0.2
-2.1
-0.3
0.1
-0.00.3
2.7
-1.4
-0.2
-0.00.5
1.1
-1.2
-0.7
-0.3
-0.0-0.0
-0.0
-0.0
-0.0
-0.0
-0.0-0.1
-0.1
-0.1
-0.1
-0.1
-0.1-0.1
-0.0
-0.2
0.0
-0.1
-0.2-0.2
-0.7
-1.6
-0.7
-0.2
-0.3-0.9
-2.9
-2.8
-0.7
-0.4-0.5
-0.8
1.8
-0.7
-0.4
-0.30.1
0.0
0.1
-0.0
0.1
0.10.1
0.0
0.2
-0.0
0.1
0.10.2
-0.4
0.6
-0.4
0.2
0.20.8
0.8
3.4
0.8
0.7
0.40.5
-0.3
-0.3
0.4
0.3-0.0
-0.0
-0.0
-0.0
-0.0
-0.0
1.5
1.0
0.5
0.00.51.01.5 (b)β= 0.2(High Reg.)
0 1 2 3 4 50
1
2
3
4
5-0.0
-0.0
-0.1
-0.0
-0.0
-0.0-0.0
-0.0
-0.1
0.1
-0.0
-0.0-0.0
0.1
-0.3
0.3
-0.1
-0.1-0.0
-0.3
-2.3
0.2
-0.7
-0.1-0.0
-1.1
1.1
0.2
0.1-0.0
-0.4
-0.7
0.7
0.5
0.2-0.0
0.0
-0.1
-0.0
-0.0
-0.0-0.0
0.1
-0.1
-0.0
-0.0
-0.0-0.1
0.3
-0.3
0.1
0.0
-0.0-0.7
0.2
-2.3
-0.4
0.0
-0.00.1
1.1
-1.3
-0.2
-0.00.3
0.6
-0.7
-0.5
-0.3
-0.0-0.0
-0.1
-0.0
-0.1
-0.1
-0.0-0.1
-0.1
-0.2
-0.2
-0.2
-0.2-0.2
-0.1
-0.4
-0.1
-0.2
-0.2-0.3
-0.7
-1.7
-0.7
-0.3
-0.3-0.7
-2.5
-2.5
-0.6
-0.4-0.4
-0.7
0.7
-0.7
-0.4
-0.20.1
0.1
0.1
0.1
0.1
0.10.2
0.1
0.3
0.1
0.2
0.20.3
-0.3
0.6
-0.3
0.2
0.30.6
0.4
1.3
0.4
0.5
0.30.4
-0.4
-0.4
0.3
0.2-0.0
-0.0
-0.0
-0.0
-0.0
-0.0
1.5
1.0
0.5
0.00.51.01.5 (c)β= 1
0 1 2 3 4 50
1
2
3
4
5-0.0
0.0
0.0
-0.1
-0.1
-0.1-0.0
0.1
0.0
-0.1
-0.1
-0.0-0.0
0.1
0.1
-0.1
-0.2
-0.0-0.0
0.1
-2.1
-0.2
-1.2
-0.0-0.0
-0.5
0.1
-0.6
0.0-0.0
-0.2
-0.2
0.1
0.1
0.1-0.1
-0.1
-0.0
0.0
-0.0
-0.0-0.1
-0.1
0.0
0.1
-0.0
-0.0-0.2
-0.1
0.0
0.1
-0.0
-0.0-1.2
-0.3
-1.9
0.1
-0.1
-0.0-0.6
0.1
-0.5
-0.1
-0.00.1
0.1
-0.2
-0.2
-0.2
-0.0-0.0
-0.0
-0.0
-0.0
-0.0
-0.0-0.2
-0.1
-0.1
-0.1
-0.2
-0.1-0.2
-0.1
-0.1
-0.1
-0.2
-0.2-0.2
-0.2
-1.1
-0.2
-0.2
-0.2-0.2
-1.8
-1.9
-0.2
-0.2-0.2
-0.9
0.1
-0.9
-0.2
-0.10.1
0.0
0.0
0.0
0.1
0.10.1
0.0
0.0
0.0
0.1
0.10.1
-0.9
-0.0
-0.9
0.1
0.10.1
-0.3
0.1
-0.2
0.1
0.10.1
-0.2
-0.2
0.1
0.1-0.0
-0.0
-0.0
-0.0
-0.0
-0.0
1.5
1.0
0.5
0.00.51.01.5 (d)β= 10(Low Reg.)
Figure 4: Grid-World Reward Perturbations. (a) Sequential task. (b)-(d)Policies trained with Shan-
non entropy regularization of different strength. Action probabilities are indicated via relative arrow lengths;
the goal-state is gray and without annotations. Colors indicate worst-case adversarial reward perturbations
∆rπ(a,s) =1
βlogπ(a|s)
π0(a|s)for each state and action (up, down, left, right) against which the policy is robust.
Red (or positive ∆rπ(a,s)) implies that the policy is robust to reward decreases (up to the value shown)
imposed by the adversary. These decreases are balanced by adversarial reward increases (blue) for other
actions in the same state. We confirm the optimality of each policy using path consistency in App. H Fig. 8.
Sequential Setting In Fig. 4(a), we consider a grid world where the agent receives +5for picking up the
reward pill,−1for stepping in water, and zero reward otherwise. We train an agent using tabular Q-learning
and a discount factor γ= 0.99. We visualize the worst-case reward perturbations ∆rπ(a,s) =1
βlogπ(a|s)
π0(a|s)
in each state-action pair for policies trained with various regularization strengths in Fig. 4(b)-(d). While it
is well-known that there exists a unique optimal policy for a given regularized mdp, our results additionally
display the adversarial strategies and resulting Nash equilibria which can be associated with a regularization
scheme specified by Ω,π0,α, andβin a given mdp.
Each policy implicitly hedges against an adversary that perturbs the rewards according to the values and
colormap shown. For example, inspecting the state to the left of the goal state in panel Fig. 4(b)-(c), we
see that the adversary reduces the immediate reward for moving right (in red, ∆rπ∗>0). Simultaneously,
the adversary raises the reward for moving up or down towards the water (in blue). This is in line with the
constraints on the feasible set, which imply that the adversary must balance reward decreases with reward
increases in each state. In App. E.4 Fig. 8, we certify the optimality of each policy using the path consistency
conditions, which also confirms that the adversarial perturbations have rendered the agent indifferent across
actions in each state.
Although we observe that the agent with high regularization in Fig. 4(b) is robust to a strong adversary,
the value of the regularized objective is also lower in this case. As expected, lower regularization strength
reduces robustness to negative reward perturbations. With low regularization in Fig. 4(d), the behavior of
the agent barely deviates from the deterministic policy in the face of the weaker adversary.
5 Discussion
Our analysis in Sec. 3 unifies and extends several previous works analyzing the reward robustness of regular-
ized policies (Ortega & Lee, 2014; Eysenbach & Levine, 2021; Husain et al., 2021), as summarized in Table 1.
We highlight differences in the analysis of entropy-regularized policies in Sec. 5.1, and provide additional
discussion of the closely-related work of Derman et al. (2021) in Sec. 5.2.
5.1 Comparison with Entropy Regularization
As argued in Sec. 3, the worst-case reward perturbations preserve the value of the regularized objective
function. Thus, we should expect our robustness conclusions to depend on the exact form of the regularizer.
When regularizing with the Tsallis or Shannon ( α= 1) entropy, the worst-case reward perturbations become
∆rπ(a,s) =1
βlogαπ(a|s) +1
β1
α/parenleftig
1−/summationdisplay
a∈Aπ(a|s)α/parenrightig
. (29)
11Published in Transactions on Machine Learning Research (07/2022)
See App. F.2, we also show that for 0< α≤1, these perturbations cannot decrease the reward, with
−∆rπ(a,s)≥0andr′
π(a,s)≥r(a,s). In the rest of this section, we argue that this property leads to several
unsatisfying conclusions in previous work (Lee et al., 2019; Eysenbach & Levine, 2021), which are resolved
by using the klandα-divergence for analysis instead of the corresponding entropic quantities.3
First, this means that a Shannon entropy-regularized policy is only ‘robust’ to increases in the reward
function. However, for useful generalization, we might hope that a policy still performs well when the reward
function decreases in at least some states. Including the reference distribution via divergence regularization
resolves this issue, and we observe in Fig. 2 and Fig. 4 that the adversary chooses reward decreases in
some actions and increases in others. For example, for the kldivergence, ∆rπ∗(a,s) =1
βlogπ∗(a|s)
π0(a|s)=
Q∗(a,s)−V∗(s)implies robustness to reward decreases when π∗(a|s)>π0(a|s)orQ∗(a,s)>V∗(s).
Similarly, Lee et al. (2019) note that for any α,
1
βΩ∗(Hα)
β (Q) = max
π∈∆|A|⟨π,Q⟩+1
βHα(π)≥Q(amax,s)
whereamax= arg maxaQ(a,s)and the Tsallis entropy Hα(π)equals the Shannon entropy for α= 1. This
soft value aggregation yields a result that is largerthan any particular Q-value. By contrast, for the α-
divergence, we show in App. F.3 that for fixed βandα>0,
Q(amax,s) +1
β1
αlog2−απ(amax|s)≤1
βΩ∗(α)
π0,β(Q)≤Q(amax,s). (30)
This provides a more natural interpretation of the Bellman optimality operator V(s)←1
βΩ∗(α)
π0,β(Q)as a soft
maximum operation. As a function of β, we see in App. C.4 and F.3 that the conjugate ranges between
Eπ0[Q(amax,s)]≤1
βΩ∗(α)
π0,β(Q)≤Q(amax,s).
Finally, using entropy instead of divergence regularization also affects interpretations of the feasible set.
Eysenbach & Levine (2021) consider the same constraint as in Eq. (19), but without the reference π0(a|s)
/summationdisplay
a∈Aexp/braceleftbig
β·∆r(a,s)/bracerightbig
≤1∀s∈S. (31)
This constraint suggests that the original reward function ( ∆r= 0) is not feasible for the adversary. More
surprisingly, Eysenbach & Levine (2021) App. A8 argues that increasing regularization strength (with lower
β) may lead to lessrobust policies based on the constraint in Eq. (31). In App. F.4, we discuss how including
π0(a|s)in the constraint via divergence regularization (Prop. 1) avoids this conclusion. As expected, Fig. 2
shows that increasing regularization strength leads to more robust policies.
5.2 Related Algorithms
Several recent works provide algorithmic insights which build upon convex duality and complement or extend
our analysis. Derman et al. (2021) derive practical iterative algorithms based on a general equivalence
between robustness and regularization, which can be used to enforce robustness to bothreward perturbations
(through policy regularization) and changes in environment dynamics (through value regularization). For
policy regularization, Derman et al. (2021) translate the specification of a desired robust set into a regularizer
using the convex conjugate of the set indicator function. In particular, Derman et al. (2021) associate kl
divergence or (scaled) Tsallis entropy policy regularization with the robust set R∆
π:={∆r|∆r(a,s)∈
[1
β1
αlogαπ(a|s)
π0(a|s),∞)∀(a,s)∈A×S} . Our analysis proceeds in the opposite direction, from regularization
to robustness, using the conjugate of the divergence. While the worst-case perturbations result in the same
modified objective, our approach yields a larger robust set with qualitatively different shape (see Fig. 1).
Zahavy et al. (2021) analyze a general ‘meta-algorithm’ which alternates between updates of the occupancy
measureµ(a,s)and modified reward r′(a,s)in online fashion. This approach highlights the fact that the
modified reward r′
πor worst-case perturbations ∆rπchange as the policy or occupancy measure is optimized.
The results of Zahavy et al. (2021) and Husain et al. (2021) hold for general convex mdps, which encompass
common exploration and imitation learning objectives beyond the policy regularization setting we consider.
3Entropy regularization corresponds to divergence regularization with the uniform reference distribution π0(a|s).
12Published in Transactions on Machine Learning Research (07/2022)
As discussed in Sec. 3.4, path consistency conditions have been used to derive practical learning objectives
in (Nachum et al., 2017; Chow et al., 2018). These algorithms might be extended to general α-divergence
regularizationviaEq.(28), whichinvolvesanarbitraryreferencepolicy π0(a|s)thatcanbelearnedadaptively
as in (Teh et al., 2017; Grau-Moya et al., 2018).
Finally, previous work has used dual optimizations similar to Eq. (14) to derive alternative Bellman error
losses (Dai et al., 2018; Belousov & Peters, 2019; Nachum & Dai, 2020; Bas-Serrano et al., 2021), highlighting
how convex duality can be used to bridge between policy regularization and Bellman error aggregation
(Belousov & Peters, 2019; Husain et al., 2021).
6 Conclusion
In this work, we analyzed the robustness of convex-regularized rlpolicies to worst-case perturbations of
the reward function, which implies generalization to adversarially chosen reward functions from within a
particular robust set. We have characterized this robust set of reward functions for klandα-divergence
regularization, provided a unified discussion of existing works on reward robustness, and clarified apparent
differences in robustness arising from entropy versus divergence regularization. Our advantage function
interpretation of the worst-case reward perturbations provides a complementary perspective on how Q-values
appear as dual variables in convex programming forms of regularized mdps. Compared to a deterministic,
unregularized policy, a stochastic, regularized policy places probability mass on a wider set of actions and
requires state-action value adjustments via the advantage function or adversarial reward perturbations.
Conversely, a regularized agent, acting based on given Q-value estimates, implicitly hedges against the
anticipated perturbations of an appropriate adversary.
References
Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact
of entropy on policy optimization. In International Conference on Machine Learning . PMLR, 2019.
Shun-ichi Amari. Information geometry and its applications , volume 194. Springer, 2016.
Saurabh Arora and Prashant Doshi. A survey of inverse reinforcement learning: Challenges, methods and
progress. Artificial Intelligence , 297:103500, 2021.
Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, Joydeep Ghosh, and John Lafferty. Clustering with
bregman divergences. Journal of machine learning research , 6(10), 2005.
Joan Bas-Serrano, Sebastian Curi, Andreas Krause, and Gergely Neu. Logistic q-learning. In International
Conference on Artificial Intelligence and Statistics , pp. 3610–3618. PMLR, 2021.
Richard Bellman. A markovian decision process. Journal of mathematics and mechanics , 6(5):679–684, 1957.
Boris Belousov. Bregman divergence of alpha-divergence. Blog post , 2017. URL http://www.
boris-belousov.net/2017/04/16/bregman-divergence/ .
Boris Belousov and Jan Peters. Entropic regularization of markov decision processes. Entropy, 21(7), 2019.
Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.
Yinlam Chow, Ofir Nachum, and Mohammad Ghavamzadeh. Path consistency learning in tsallis entropy
regularized mdps. In International Conference on Machine Learning , pp. 979–988. PMLR, 2018.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforce-
ment learning from human preferences. Advances in neural information processing systems , 30, 2017.
Andrzej Cichocki and Shun-ichi Amari. Families of alpha-beta-and gamma-divergences: Flexible and robust
measures of similarities. Entropy, 12(6):1532–1568, 2010.
BoDai,AlbertShaw,LihongLi,LinXiao,NiaoHe,ZhenLiu,JianshuChen,andLeSong. Sbeed: Convergent
reinforcement learning with nonlinear function approximation. In International Conference on Machine
Learning , pp. 1125–1134. PMLR, 2018.
13Published in Transactions on Machine Learning Research (07/2022)
Esther Derman, Matthieu Geist, and Shie Mannor. Twice regularized mdps and the equivalence between
robustness and regularization. In Thirty-Fifth Conference on Neural Information Processing Systems ,
2021.
Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex optimiza-
tion.Journal of Machine Learning Research , 17(83):1–5, 2016.
Benjamin Eysenbach and Sergey Levine. Maximum entropy RL (provably) solves some robust RL problems.
InInternational Conference on Learning Representations , 2021. URL https://openreview.net/forum?
id=PtSAD3caaA2 .
Eugene A Feinberg and Adam Shwartz. Handbook of Markov decision processes: methods and applications ,
volume 40. Springer Science & Business Media, 2012.
Peter C Fishburn. Nonlinear preference and utility theory . Number 5. Johns Hopkins University Press, 1988.
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. In
Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence , pp. 202–211, 2016.
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision processes.
InICML 2019-Thirty-sixth International Conference on Machine Learning , 2019.
Jordi Grau-Moya, Felix Leibfried, and Peter Vrancx. Soft q-learning with mutual-information regularization.
InInternational Conference on Learning Representations , 2018.
TuomasHaarnoja,HaoranTang,PieterAbbeel,andSergeyLevine. Reinforcementlearningwithdeepenergy-
based policies. In Proceedings of the 34th International Conference on Machine Learning , volume 70 of
Proceedings of Machine Learning Research , pp. 1352–1361. PMLR, 06–11 Aug 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine
Learning , pp. 1861–1870. PMLR, 2018.
Hisham Husain, Kamil Ciosek, and Ryota Tomioka. Regularized policies are reward robust. International
Conference on Artificial Intelligence and Statistics , 2021.
Maximilian Igl, Andrew Gambardella, Jinke He, Nantas Nardelli, N Siddharth, Wendelin Böhmer, and
Shimon Whiteson. Multitask soft option learning. In Conference on Uncertainty in Artificial Intelligence ,
pp. 969–978, 2020.
Hilbert J Kappen, Vicenç Gómez, and Manfred Opper. Optimal control as a graphical model inference
problem. Machine learning , 87(2):159–182, 2012.
Kyungjae Lee, Sungjoon Choi, and Songhwai Oh. Sparse markov decision processes with causal sparse tsallis
entropy regularization for reinforcement learning. IEEE Robotics and Automation Letters , 3(3):1466–1473,
2018.
KyungjaeLee, SungyubKim, SungbinLim, SungjoonChoi, andSonghwaiOh. Tsallisreinforcementlearning:
A unified framework for maximum entropy reinforcement learning. arXiv preprint arXiv:1902.00137 , 2019.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv
preprint arXiv:1805.00909 , 2018.
SergeyLevine, AviralKumar, GeorgeTucker, andJustinFu. Offlinereinforcementlearning: Tutorial, review,
and perspectives on open problems. arXiv preprint arXiv:2005.01643 , 2020.
Ofir Nachum and Bo Dai. Reinforcement learning via fenchel-rockafellar duality. arXiv preprint
arXiv:2001.01866 , 2020.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and
policy based reinforcement learning. arXiv preprint arXiv:1702.08892 , 2017.
14Published in Transactions on Machine Learning Research (07/2022)
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted
stationary distribution corrections. Reinforcement Learning for Real Life Workshop (ICML) , 2019a.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: Policy
gradient from arbitrary experience. arXiv preprint arXiv:1912.02074 , 2019b.
Jan Naudts. Generalised thermostatistics . Springer Science & Business Media, 2011.
Gergely Neu, Anders Jonsson, and Vicenç Gómez. A unified view of entropy-regularized markov decision
processes. arXiv preprint arXiv:1705.07798 , 2017.
Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In International
Conference on Machine Learning , 2000.
Pedro Ortega and Daniel Lee. An adversarial interpretation of information-theoretic bounded rationality.
InProceedings of the AAAI Conference on Artificial Intelligence , volume 28, 2014.
Pedro A Ortega and Daniel A Braun. Thermodynamics as a theory of decision-making with information-
processing costs. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences ,
469(2153), 2013.
Martin J Osborne and Ariel Rubinstein. A course in game theory . 1994.
Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 24, 2010.
Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming . John Wiley
& Sons, Inc., 1994.
Martin L Puterman and Moon Chirl Shin. Modified policy iteration algorithms for discounted markov
decision problems. Management Science , 24(11), 1978.
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and reinforcement
learning by approximate inference. In International Joint Conference on Artificial Intelligence , 2013.
BrunoScherrer, MohammadGhavamzadeh, VictorGabillon, BorisLesner, andMatthieuGeist. Approximate
modified policy iteration and its application to the game of tetris. Journal of Machine Learning Research ,
16(49):1629–1676, 2015.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy
optimization. In International conference on machine learning , pp. 1889–1897. PMLR, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimiza-
tion algorithms. arXiv e-prints , pp. arXiv–1707, 2017.
Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear programming.
InProceedings of the 25th international conference on Machine learning , pp. 1032–1039, 2008.
Yee Whye Teh, Victor Bapst, Wojciech Marian Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell,
Nicolas Heess, and Razvan Pascanu. Distral: robust multitask reinforcement learning. In Proceedings of
the 31st International Conference on Neural Information Processing Systems , pp. 4499–4509, 2017.
Constantino Tsallis. Introduction to nonextensive statistical mechanics: approaching a complex world .
Springer Science & Business Media, 2009.
Ziyi Wang, Oswin So, Jason Gibson, Bogdan Vlahov, Manan S Gandhi, Guan-Horng Liu, and Evangelos A
Theodorou. Variational inference mpc using tsallis divergence. arXiv preprint arXiv:2104.00241 , 2021.
Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough for convex
mdps.Advances in Neural Information Processing Systems , 2021.
Huaiyu Zhu and Richard Rohwer. Information geometric measurements of generalisation. Technical report,
Aston University, 1995.15Published in Transactions on Machine Learning Research (07/2022)
Appendix
Table of Contents
A Implications of Conjugate Duality Optimality Conditions 17
A.1 Proof of Prop. 2 : Policy Form Worst-Case Reward Perturbations . . . . . . . . . . . . . 17
A.2 Optimal Policy in a Regularized MDP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.3 Proof of Prop. 3: Policy Form Worst-Case Perturbations match Value Form at Optimality 18
A.4 Path Consistency and KKT Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.5 Modified Rewards and Duality Gap for Suboptimal Policies . . . . . . . . . . . . . . . . . 19
B Convex Conjugate Derivations 20
B.1 KL Divergence Policy Regularization:1
βΩ∗
π0,β(∆r). . . . . . . . . . . . . . . . . . . . . . 20
B.2 KL Divergence Occupancy Regularization:1
βΩ∗
µ0,β(∆r). . . . . . . . . . . . . . . . . . . 21
B.3α-Divergence Policy Regularization:1
βΩ∗(α)
π0,β(∆r). . . . . . . . . . . . . . . . . . . . . . . 22
B.4α-Divergence Occupancy Regularization:1
βΩ∗(α)
µ0,β(∆r). . . . . . . . . . . . . . . . . . . . 24
C Soft Value Aggregation 24
C.1 KL Divergence Soft Value Aggregation:1
βΩ∗
π0,β(Q). . . . . . . . . . . . . . . . . . . . . . 25
C.2α-Divergence Soft Value Aggregation:1
βΩ∗
π0,β(Q). . . . . . . . . . . . . . . . . . . . . . . 26
C.3 Relationship between Normalization Constants ψ∆rπ∗,ψQ∗, and Value Function V∗(s). . 27
C.4 Plotting Value Function as a Function of Regularization Parameters α,β. . . . . . . . . . 29
D Robust Set of Perturbed Rewards 30
D.1 Proof of Prop. 1: Robust Set of Perturbed Rewards for Policy Regularization . . . . . . . 30
D.2 Robust Set for α-Divergence under µ(a,s)Regularization . . . . . . . . . . . . . . . . . . 31
D.3 Plotting the α-Divergence Feasible Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
E Value Form Reward Perturbations 32
E.1 Proof of Thm. 1 (Husain et al. (2021)) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
E.2 Path Consistency (Comparison with Nachum et al. (2017); Chow et al. (2018)) . . . . . . 33
E.3 Indifference Condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
E.4 Confirming Optimality using Path Consistency and Indifference . . . . . . . . . . . . . . . 35
F Comparing Entropy and Divergence Regularization 35
F.1 Tsallis Entropy and α-Divergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
F.2 Non-Positivity of ∆rπ(a,s)for Entropy Regularization . . . . . . . . . . . . . . . . . . . . 36
F.3 Bounding the Conjugate Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
F.4 Feasible Set for Entropy Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
G Worked Example for Deterministic Regularized Policy ( α= 2,β= 10) 39
H Additional Feasible Set Plots 40
16Published in Transactions on Machine Learning Research (07/2022)
A Implications of Conjugate Duality Optimality Conditions
In this section, we show several closely-related results which are derived from the conjugate optimality
conditions. We provide additional commentary in later Appendix sections which more closely follow the
sequence of the main text.
First, recall from Section 2.1 the definition of the conjugate optimizations for functions over X:=A×S.
We restrict µ∈RA×S
+to be a nonnegative function over X, so that
1
βΩ∗(∆r) = sup
µ∈RA×S
+/angbracketleftbig
µ,∆r/angbracketrightbig
−1
βΩ(µ),1
βΩ(µ) = sup
∆r∈RA×S/angbracketleftbig
µ,∆r/angbracketrightbig
−1
βΩ∗(∆r), (32)
and the implied optimality conditions are
∆r=1
β∇µΩ(µ) =/parenleftbigg
∇∆r1
βΩ∗/parenrightbigg−1
(µ)µ=1
β∇∆rΩ∗(∆r) =/parenleftbigg
∇µ1
βΩ/parenrightbigg−1
(∆r). (33)
A.1 Proof of Prop. 2 : Policy Form Worst-Case Reward Perturbations
Proposition2. For a given policy π(a|s)or state-action occupancy µ(a,s), the worst-case adversarial reward
perturbations ∆rπor∆rµassociated with a convex function Ω(µ)and regularization strength 1/βare
∆rπ=∇µ1
βΩ(µ). (20)
Proof.The reward perturbations are defined via conjugate optimization for Ω(µ)in Eq. (32), where ∆r∈
RA×S. The proposition follows directly from the optimality condition in Eq. (33), and we focus on the
∆r=1
β∇µΩ(µ)condition for convenience.
In App. B, we derive the explicit forms for the worst-case reward perturbations for klandα-divergence
regularization from Sec. 3.2 of the main text. See App. B Table 3 for references to particular derivations.
Note that we do not consider further constraints on µin the conjugate optimization. Instead, we view the
Bellman flow constraints µ(a,s)∈M(and normalization constraint µ(a,s)∈∆|A|×|S|) as arising from the
overall (regularized) mdpoptimization in Eq. (10) or (12), as we discuss in the next subsection.
A.2 Optimal Policy in a Regularized MDP
In Lemma 1 below, we show that the Bellman flow constraints Eq. (10), which are enforced by the optimal
Lagrange multipliers V∗(s), ensure that the optimal µ∗(a,s)is normalized. This suggests that an explicit
normalization constraint is not required. In Prop. 4, we then proceed to derive the optimal policy in a
regularized mdpusing the conjugate optimality conditions in Eq. (33).
Lemma 1 (Flow Constraints Ensure Normalization) .Assume that the initial state distribution ν0(s)and
transition dynamics P(s′|a,s)are normalized, with/summationtext
sν0(s) = 1and/summationtext
s′P(s′|a,s) = 1. If a state-occupancy
measure satisfies the Bellman flow constraints µ(a,s)∈M, then it is a normalized distribution µ(a,s)∈
∆|A|×|S|.
Proof.Starting from the Bellman flow constraints/summationtext
aµ(a,s) = (1−γ)ν0(s)+γ/summationtext
a′,s′P(s|a′,s′)µ(a′,s′), we
consider taking the summation over states s∈S,
/summationdisplay
a,sµ(a,s) = (1−γ)/summationdisplay
sν0(s) +γ/summationdisplay
s/summationdisplay
a′,s′P(s|a′,s′)µ(a′,s′)(1)= (1−γ) +γ/summationdisplay
sP(s|a′,s′)/summationdisplay
a′,s′µ(a′,s′)(2)= (1−γ) +γ/summationdisplay
a′,s′µ(a′,s′)
where (1)usesthenormalizationassumptionon ν0(s)andthedistributivelaw, and (2)usesthenormalization
assumption on P(s|a′,s′). Finally, we rearrange the first and last equality to obtain
(1−γ)/summationdisplay
a,sµ(a,s) = (1−γ) =⇒/summationdisplay
a,sµ(a,s) = 1 (34)
17Published in Transactions on Machine Learning Research (07/2022)
which shows that µ(a,s)is normalized as a joint distribution over a∈A,s∈S, as desired.
Proposition 4 (Optimal Policy in Regularized MDP) .Given the optimal value function V∗(s)and Lagrange
multipliers λ∗(a,s), the optimal policy in the regularized mdpis given by
µ∗=1
β∇Ω∗/parenleftig
r+Es′
a,s/bracketleftbig
V∗/bracketrightbig
−V∗+λ∗/parenrightig
=/parenleftig
∇µ1
βΩ/parenrightig−1/parenleftig
r+Es′
a,s/bracketleftbig
V∗/bracketrightbig
−V∗+λ∗/parenrightig
.
This matches the conjugate conditions in Eq. (33)using the arguments ∆r(a,s)←r(a,s) +Es′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−
V∗(s) +λ∗(a,s).
Proof.In Sec. 2.4, we moved from the regularized primal optimization (Eq. (12)) to the dual optimization
(Eq. (14)) via the regularized Lagrangian
min
V,λmax
µ(1−γ)/angbracketleftbig
ν0,V/angbracketrightbig
+⟨µ,r+γEs′
a,s/bracketleftbig
V/bracketrightbig
−V+λ⟩−1
βΩπ0(µ)
Note that the Lagrange multipliers λ(a,s)enforceµ(a,s)≥0whileV(s)enforces the flow constraints and
thus, by Lemma 1, normalization of µ(a,s). We recognized the final two terms as a conjugate optimization
1
βΩ∗
π0,β/parenleftig
r+γEs′
a,s/bracketleftbig
V/bracketrightbig
−V+λ/parenrightig
= max
µ/angbracketleftbig
µ,r+γEs′
a,s/bracketleftbig
V/bracketrightbig
−V+λ/angbracketrightbig
−1
βΩπ0(µ) (35)
to yield a dual optimization over V(s)andλ(a,s)only in Eq. (14). After solving the dual optimization for
the optimal V∗(s),λ∗(a,s), we can recover the optimal policy in the mdpusing the optimizing argument of
Eq. (35). Differentiating Eq. (35) and solving for µyields∇µ1
βΩ(µ) =r+γEs′
a,s/bracketleftbig
V/bracketrightbig
−V+λwhich we invert
to obtain Prop. 4. The other equality follows from the conjugate optimality conditions in Eq. (33).
Forα-divergence regularization, the optimal policy or state-action occupancy is given by the ‘optimizing
argument’ column of Table 2, up to reparameterization of ∆r(a,s)←r(a,s) +Es′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−V∗(s) +λ∗(a,s)
as the dual variable. In this case, note that the argument to the conjugate function accounts for the flow
and nonnegativity constraints via V∗(s)andλ∗(a,s). In particular, we have
Policy Reg., App. B.3 Eq. (63)
µ∗(a,s) =µ(s)π0(a|s) expα/braceleftig
β·/parenleftig
r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−V∗(s) +λ(a,s)−ψ∆rπ∗(s;β)/parenrightig/bracerightig
(36)
Occupancy Reg., App. B.4 Eq. (68)
µ∗(a,s) =µ0(a,s) expα/braceleftig
β·/parenleftig
r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−V∗(s) +λ(a,s)/parenrightig/bracerightig
(37)
whereψ∆rπ∗(s;β) =1
β1
α(1−/summationtext
aπ0(a|s)1−απ∗(a|s)α)appears from differentiating ∇1
βΩπ0(µ)as in Eq. (23)
or App. B.3. This means that the optimal policy is only available in self-consistent fashion, with the
normalization constant inside the expα, which can complicate practical applications (Lee et al., 2019; Chow
et al., 2018).
A.3 Proof of Prop. 3: Policy Form Worst-Case Perturbations match Value Form at Optimality
The substitution ∆r(a,s)←r(a,s) +Es′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−V∗(s) +λ∗(a,s)above already anticipates the result in
Prop. 3, which links the reward perturbations for the optimal policy ∆rπ∗or state-action occupancy ∆rµ∗
to the advantage function. See the proof of Thm. 1 in App. E.1 for additional context in relation to the
value-form reward perturbations ∆rV(a,s).
Proposition 3. For the optimal policy π∗(a|s)and value function V∗(s)corresponding to α-divergence policy
regularization with strength β, the policy and value forms of the worst-case adversarial reward perturbations
match, ∆rπ∗= ∆rV∗, and are related to the advantage function via
∆rπ∗(a,s) =Q∗(a,s)−V∗(s) +λ∗(a,s), (26)
where we define Q∗(a,s):=r(a,s)+γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
and recallλ∗(a,s)π∗(a|s) = 0by complementary slackness.
Note thatV∗(s)depends on the regularization scheme via the conjugate function1
βΩ∗(α)
π0,β(∆rV)in Eq.(25).
18Published in Transactions on Machine Learning Research (07/2022)
Proof.The result follows by combining Prop. 2, which states that ∆rπ=∇µ1
βΩ(µ), and Prop. 4,
which implies∇µ1
βΩ(µ∗) =r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−V∗(s) +λ∗(a,s)as a condition for optimality of
{µ∗(a,s),V∗(s),λ∗(a,s)}. Thus, for the optimal policy π∗(a|s)and Lagrange multipliers {V∗(s),λ∗(a,s)},
we have ∆rπ∗(a,s) =r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−V∗(s) +λ∗(a,s)and similarly for ∆rµ∗(a,s).
We can confirm this using the expression for the optimal policy in Eq. (36) and the worst-case reward
perturbations in Sec. 3.2. For example, recalling µ∗(a,s) =µ(s)π∗(a|s), we can write the α-divergence
policy regularization case as ∆rπ∗(a,s) =1
βlogαπ∗(a|s)
π0(a|s)+ψ∆rπ∗(s;β) =r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−V∗(s) +
λ(a,s)±ψ∆rπ∗(s;β).
A.4 Path Consistency and KKT Conditions
Finally, note that the kktoptimality conditions (Boyd & Vandenberghe, 2004) include the condition that
we have used in the proof of Prop. 3. At optimality, we have
r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−V∗(s) +λ∗(a,s)−∇1
βΩ(µ∗) = 0. (38)
Thiskktcondition is used to derive path consistency objectives in Nachum et al. (2017); Chow et al. (2018).
For general α-divergence policy regularization, we substitute ∇1
βΩ(α)
π0(µ∗) = ∆rπ∗(a,s) =1
βlogαπ∗(a|s)
π0(a|s)+
ψ∆rπ∗(s;β)using Eq. (22) (see App. B.3 for detailed derivations). This leads to the condition
r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−V∗(s) +λ∗(a,s)−1
βlogαπ∗(a|s)
π0(a|s)−ψ∆rπ∗(s;β) = 0, (39)
which matches Eq. (28). We compare our α-divergence path consistency conditions to previous work in
App. E.2.
A.5 Modified Rewards and Duality Gap for Suboptimal Policies
We can also use the conjugate duality of state-action occupancy measures and reward functions ( r(a,s)or
r′(a,s)) to express the optimality gap for a suboptimal µ(a,s). Consider the regularized primal objective as
a (constrained) conjugate optimization,
RLΩ,β(r) :=1
βΩ∗(r) = max
µ∈M/angbracketleftbig
µ,r/angbracketrightbig
−1
βΩ(µ) (40)
≥/angbracketleftbig
µr′,r/angbracketrightbig
−1
βΩ(µr′) (41)
where the inequality follows from the fact that any feasible µr′∈Mprovides a lower bound on the objective.
We use the notation µr′to anticipate the fact that, assuming appropriate domain considerations, we would
like to associate this occupancy measure with a modified reward function r′using the conjugate optimality
conditions in Eq. (33) (with r′as the dual variable). In particular, for a given Ω, we use the fact that
µr′=1
β∇Ω∗(r′)to recognize the conjugate duality gap as a Bregman divergence. Rearranging Eq. (41),
1
βΩ∗(r)−/angbracketleftbig
µr′,r/angbracketrightbig
+1
βΩ(µr′)≥0 (42)
1
βΩ∗(r)−/angbracketleftbig
µr′,r/angbracketrightbig
+/angbracketleftbig
µr′,r′/angbracketrightbig
−1
βΩ∗(r′)≥0 (43)
1
βΩ∗(r)−1
βΩ∗(r′)−/angbracketleftbig
r−r′,1
β∇Ω∗(r′)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
µr′/angbracketrightbig
≥0 (44)
DΩ∗[r:r′]≥0 (45)
where the last line follows from the definition of the Bregman divergence (Amari, 2016). For example, using
thekldivergence Ω(µ) =DKL[µ:µ0], one can confirm that the Bregman divergence generated by Ω∗is
also a kldivergence, DKL[µr′:µr∗](Belousov, 2017; Banerjee et al., 2005).
19Published in Transactions on Machine Learning Research (07/2022)
B Convex Conjugate Derivations
In this section, we derive the convex conjugate associated with klandα-divergence regularization of the
policyπ(a|s)or state-action occupancy µ(a,s). We summarize these results in Table 2, with equation
references in Table 3. In both cases, we treat the regularizer1
βΩ(µ)as a function of µ(a,s)and optimize
over all states jointly,
1
βΩ∗(·) = sup
µ∈RA×S
+/angbracketleftbig
µ,·/angbracketrightbig
−1
βΩ(µ). (46)
These conjugate derivations can be used to reason about the optimal policy via1
βΩ∗/parenleftbig
r+γEs′
a,s/bracketleftbig
V/bracketrightbig
−V−λ/parenrightbig
,
as argued in App. A.2, or the worst case reward perturbations using1
βΩ∗(∆r). We use ∆ras the argument
or dual variable throughout this section.
In App. C, we derive alternative conjugate functions which optimize over the policy in each state, where
π(a|s)∈∆|A|is constrained to be a normalized probability distribution. This conjugate arises in considering
soft value aggregation or regularized iterative algorithms as in Sec. 2.4. See Table 4 for equation references.
Divergence Ω1
βΩ∗(∆r) ∆rµ(a,s)µ∆r(a,s)
1
βDKL[π:π0]Eq. (47) Eq. (50) Eq. (51)
1
βDKL[µ:µ0]Eq. (55) Eq. (53) Eq. (54)
1
βDα[π0:π]Eq. (56) Eq. (62) Eq. (63)
1
βDα[µ0:µ]Eq. (66) Eq. (67) Eq. (68)
Table 3: Equations for ∆ror ‘mdpOptimality’ Conjugate
(µOptimization, No Normalization Constraint)Divergence Ω1
βΩ∗(Q)Qπ(a,s)πQ(a,s)
1
βDKL[π:π0]Eq. (78) Eq. (75) Eq. (76)
1
βDα[π0:π]Eq. (82) Eq. (81) Eq. (80)
Table 4: Equations for ‘Soft Value’ V∗(s)Conjugates
(πOptimization, Normalization Constraint)
B.1 KL Divergence Policy Regularization:1
βΩ∗
π0,β(∆r)
The conjugate function for kldivergence from the policy π(a|s)to a reference π0(a|s)has the following
closed form
1
βΩ∗
π0,β(∆r) =1
β/summationdisplay
sµ(s)/parenleftigg/summationdisplay
sπ0(a|s) exp/braceleftbig
β·∆r(a,s)/bracerightbig
−1/parenrightigg
. (47)
Proof.We start from the optimization in Eq. (3) or (32), using conditional kldivergence regularization
Ωπ0(µ) =Eµ(s)[DKL[π:π0]]as in Eq. (6).
1
βΩ∗
π0,β(∆r) = max
µ/angbracketleftbig
µ,∆r/angbracketrightbig
−1
β/summationdisplay
a,sµ(a,s) logµ(a,s)
µ(s)π0(a|s)+1
β/summationdisplay
a,sµ(a,s)−1
β/summationdisplay
a,sµ(s)π0(a|s)(48)
=⇒∆r=∇µ/parenleftigg
1
β/summationdisplay
a,sµ(a,s) logµ(a,s)
µ(s)π0(a|s)+1
β/summationdisplay
a,sµ(a,s)−1
β/summationdisplay
a,sµ(s)π0(a|s)/parenrightigg
(49)
Worst-Case Reward Perturbations ∆rπ(a|s)We can recognize Eq. (49) as an instance of Prop. 2.
Noting that the marginal µ(s)depends on µ(a,s), we make use of the identity/summationtext
s′∂
∂µ(a,s)µ(s′) =/summationtext
s′,a′∂
∂µ(a,s)µ(a′,s′) =/summationtext
s′,a′δ(a′,s′=a,s) = 1as in (Neu et al., 2017; Lee et al., 2019). Differentiat-
20Published in Transactions on Machine Learning Research (07/2022)
ing, we obtain
∆r(a,s) =1
βlogµ(a,s)
µ(s)π0(a|s)−1
β/summationdisplay
a,s∂µ(a,s)
∂µ(a′,s′)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
1+1
β/summationdisplay
a,sµ(a,s)
µ(s)∂/summationtext
a′′µ(a′′,s)
∂µ(a′,s′)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
δ(s=s′)+1
β−1
β/summationdisplay
a,s∂/summationtext
a′′µ(a′′,s)
∂µ(a′,s′)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
δ(s=s′)π0(a|s)
=1
βlogµ(a,s)
µ(s)π0(a|s)+1
β/summationdisplay
aµ(a,s)
µ(s)−1
β/summationdisplay
aπ0(a|s)
=1
βlogµ(a,s)
µ(s)π0(a|s). (50)
In the last line, we assume/summationtext
aπ0(a|s) = 1and note that/summationtext
aµ(a,s)
µ(s)=/summationtext
aµ(a,s)
µ(s)=µ(s)
µ(s)= 1.
OptimizingArgument π∆r(a|s)Wederivetheconjugatefunctionbysolvingfortheoptimizingargument
µ(a,s)in terms of ∆r(a,s)and substituting back into Eq. (48). Defining π∆r(a|s) =µ∆r(a,s)
µ∆r(s)as the policy
induced by the optimizing argument µ∆r(a,s)in Eq. (50), we can solve for π∆rto obtain
π∆r(a|s) =π0(a|s) exp/braceleftbig
β·∆r(a,s)/bracerightbig
(51)
Conjugate Function1
βΩ∗
π0,β(∆r)We plug this back into the conjugate optimization Eq. (48), with
µ∆r(a,s) =µ(s)π∆r(a,s). Assuming π0(a|s)is normalized, we also have/summationtext
a,sµ(s)π0(a|s) = 1and
1
βΩ∗
π0,β(∆r) =/summationdisplay
a,sµ(s)π∆r(a|s)·∆r(a,s)−1
β/parenleftbig/summationdisplay
a,sµ(s)π∆r(a|s)·logπ0
π0exp{β∆r(a,s)}−µ(s)π0(a|s) exp/braceleftbig
β∆r(a,s)/bracerightbig
+µ(s)π0(a|s)/parenrightbig
=1
β/summationdisplay
sµ(s)/parenleftigg/summationdisplay
sπ0(a|s) exp/braceleftbig
β·∆r(a,s)/bracerightbig
−1/parenrightigg
(52)
as desired. Note that the form of the conjugate function also depends on the regularization strength β.
Finally, we verify that our other conjugate optimality condition ∆rπ=/parenleftbig
∇∆r1
βΩ∗
π0,β/parenrightbig−1(µ∆r), orµ∆r=
∇∆r1
βΩ∗
π0,β(∆rπ)holds for this conjugate function. Indeed, differentiating with respect to ∆r(a,s)above, we
see that∂
∂∆r(a,s)1
βΩ∗
π0,β(∆r) =µ(s)π0(a|s) exp{β·∆r(a,s)}matchesµ∆r(a,s) =µ(s)π∆r(a|s)via Eq. (51).
Although our regularization Ωπ0(µ) =Eµ(s)[DKL[π:π0]]applies at each π0(a|s), we saw that performing
the conjugate optimization over µ(a,s)led to an expression for a policy π∆r(a|s) =µ∆r(a,s)/µ(s)that is
normalized by construction/summationtext
aπ∆r(a|s) =/summationtext
aµ∆r(a,s)
µ(s)= 1. Conversely, for a given normalized π(a|s), the
above conjugate conditions yield ∆rπ(a,s)such that Eq. (51) is also normalized.
B.2 KL Divergence Occupancy Regularization:1
βΩ∗
µ0,β(∆r)
Nearly identical derivations as App. B.1 apply when regularizing the divergence Ωµ0(µ) =DKL[µ:µ0]
between the joint state-action occupancy µ(a,s)and a reference µ0(a,s). This leads to the following results
Worst-Case Perturbations: ∆rµ(a,s) =1
βlogµ(a,s)
µ0(a,s)(53)
Optimizing Argument: µ∆r(a,s) =µ0(a,s) exp{β·∆r(a,s)}. (54)
Conjugate Function:1
βΩ∗
µ0,β(∆r) =1
β/summationdisplay
a,sµ0(a,s) exp/braceleftbig
β·∆r(a,s)/bracerightbig
−1
β/summationdisplay
a,sµ0(a,s)(55)
Such regularization schemes appear in reps(Peters et al., 2010), while Bas-Serrano et al. (2021) consider
both policy and occupancy regularization.
21Published in Transactions on Machine Learning Research (07/2022)
B.3α-Divergence Policy Regularization:1
βΩ∗(α)
π0,β(∆r)
The conjugate for α-divergence regularization of the policy π(a|s)to a reference π0(a|s)takes the form
1
βΩ∗(α)
π0,β(∆r) =1
β1
α/summationdisplay
a,sµ(s)/parenleftbigg
π0(a|s)/bracketleftbigg
1 +β(α−1)/parenleftbig
∆r(a,s)−ψ∆r(s;β)/parenrightbig/bracketrightbiggα
α−1
−1/parenrightbigg
+/summationdisplay
sµ(s)ψ∆r(s;β).(56)
whereψ∆r(s;β)is a normalization constant for the optimizing argument π∆r(a|s)corresponding to ∆r(a,s).
We provide explicit derivations of the conjugate function instead of leveraging f-divergence duality (Belousov
& Peters, 2019; Nachum & Dai, 2020) in order to account for the effect of optimization over the joint
distribution µ(a,s). We will see in App. C.2 see that the conjugate in Eq. (56) takes a similar to form
as the conjugate with restriction to normalized π(a|s)∈∆|A|, where this constraint is not captured using
f-divergence function space duality.
Proof.We begin by writing the α-divergence Ω(α)
π0(µ) =Eµ(s)[Dα[π0:π]]as a function of the occupancy
measureµ, withπ(a|s) =µ(a,s)
µ(s). As in Prop. 2, the conjugate optimization implies an optimality condition
for∆r(a,s).
1
βΩ∗(α)
π0,β(∆r) = max
µ/angbracketleftbig
µ,∆r/angbracketrightbig
−1
β1
α(1−α)/parenleftigg
(1−α)/summationdisplay
a,sµ(s)π0(a|s) +α/summationdisplay
a,sµ(a,s)−/summationdisplay
a,sµ(s)π0(a|s)1−α/parenleftigµ(a,s)
µ(s)/parenrightigα/parenrightigg
(57)
=⇒∆r=∇µ1
β1
α(1−α)/parenleftigg
(1−α)/summationdisplay
a,sµ(s)π0(a|s) +α/summationdisplay
a,sµ(a,s)−/summationdisplay
a,sµ(s)π0(a|s)1−α/parenleftigµ(a,s)
µ(s)/parenrightigα/parenrightigg
(58)
Worst-CaseRewardPerturbations ∆rπ(a|s)Wenowdifferentiatewithrespectto µ(a,s), usingsimilar
derivations as in Lee et al. (2019). While we have already written Eq. (58) using µ(a,s) =µ(s)π(a|s), we
again emphasize that µ(s)depends on µ(a,s). Differentiating, we obtain
∆r(a,s) =∇µ1
βΩ(α)
π0(µ) (59)
=1
β1
α(1−α)/summationdisplay
a′,s′∂
∂µ(a,s)/parenleftig
(1−α)µ(s′)π0(a′|s′) +αµ(a′,s′)−µ(s′)1−απ0(a′|s′)1−αµ(a′,s′)α/parenrightig
=1
β1
α(1−α)/parenleftig
(1−α)/summationdisplay
s′∂/summationtext
a′µ(a′,s′)
∂µ(a,s)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
δ(s=s′)/summationdisplay
a′π0(a′|s) +α/summationdisplay
a′,s′∂µ(a′,s′)
∂µ(a,s)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
δ(a′,s′=a,s)−
−/summationdisplay
a′,s′αµ(a′,s′)α−1∂µ(a′,s′)
∂µ(a,s)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
δ(a′,s′=a,s)µ(s′)1−απ0(a′|s′)1−α−(1−α)/summationdisplay
a′,s′µ(s′)−α∂/summationtext
a′µ(a′,s′)
∂µ(a,s)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
δ(s=s′)π0(a′|s′)1−αµ(a′,s′)α/parenrightig
=1
β1
α(1−α)/parenleftig
(1−α)/summationdisplay
aπ0(a|s) +α−α/parenleftigµ(a,s)
µ(s)π0(a|s)/parenrightigα−1
−(1−α)/summationdisplay
a′π0(a|s)1−α/parenleftigµ(a,s)
µ(s)/parenrightigα/parenrightig
(1)=1
β1
α+1
β1
1−α−1
β1
1−α/parenleftigπ(a|s)
π0(a|s)/parenrightigα−1
−1
β1
α/summationdisplay
aπ0(a|s)1−απ(a|s)α
=1
β1
α−1/parenleftbigg/parenleftigπ(a|s)
π0(a|s)/parenrightigα−1
−1/parenrightbigg
+1
β1
α/parenleftigg
1−/summationdisplay
aπ0(a|s)1−απ(a|s)α/parenrightigg
(60)
where we have rewritten (1)in terms of the policy π(a|s) =µ(a,s)
µ(s)and assumed π0(a|s)is normalized.
Lettingπ∆r(a|s)indicate the policy which is in dual correspondence with ∆r(a,s), we would eventually like
to invert the equality in Eq. (60) to solve for π(a|s)in each (a,s). However, the final term depends on a
sum over all actions. To handle this, we define
ψ∆r(s;β) =1
β1
α/parenleftbig/summationdisplay
aπ0(a|s)−/summationdisplay
aπ0(a|s)1−απ∆r(a|s)α/parenrightbig
. (61)
22Published in Transactions on Machine Learning Research (07/2022)
Sinceπ∆r(a|s) =µ∆r(a,s)
µ(s)is normalized by construction, the constant ψ∆r(s;β)with respect to actions has
appeared naturally when optimizing with respect to µ(a,s). In App. C.2-C.3, we will relate this quantity to
the Lagrange multiplier used to enforce normalization when optimizing over π(a|s)∈∆|A|.
Finally, we use Eq. (60) to write ∆rπ(a,s)as
∆rπ(a,s) =1
βlogαπ(a|s)
π0(a|s)+ψ∆r(s;β). (62)
Optimizing Argument π∆r(a|s)Solving for the policy in Eq. (62) and denoting this as π∆r(a|s),
π∆r(a|s) =π0(a|s) expα/braceleftbig
β·/parenleftbig
∆r(a,s)−ψ∆r(s;β)/parenrightbig/bracerightbig
=π0(a|s)/bracketleftbig
1 +β(α−1) (∆r(a,s)−ψ∆r(s;β))/bracketrightbig1
α−1
+.(63)
Note that ∆rπ(a|s)is defined in self-consistent fashion due to the dependence of ψ∆r(s;β)onπ∆r(a|s)in
Eq. (61). Further, ψ∆r(s;β)does not appear as a divisive normalization constant for general α, which is
inconvenient for practical applications (Lee et al., 2019; Chow et al., 2018).
ConjugateFunction1
βΩ∗(α)
π0,β(∆r)Finally,weplugthisintotheconjugateoptimizationEq.(57). Although
weeventuallyneedtoobtainafunctionof ∆r(a,s)only, wewrite π∆r(a|s)ininitialstepstosimplifynotation.
1
βΩ∗(α)
π0,β(∆r) =/summationdisplay
a,sµ(s)π∆r(a|s)·∆r(a,s)−1
β1
α(1−α)/parenleftig
(1−α)/summationdisplay
a,sµ(s)π0(a|s) +α/summationdisplay
a,sµ(s)π∆r(a|s)−/summationdisplay
a,sµ(s)π∆r(a|s)/parenleftigπ∆r(a|s)
π0(a|s)/parenrightigα−1/parenrightig
=/summationdisplay
a,sµ(s)π∆r(a|s)·∆r(a,s)−1
β1
α/summationdisplay
a,sµ(s)π0(a|s)−1
β1
1−α/summationdisplay
a,sµ(s)π∆r(a|s) (64)
+1
β1
α(1−α)/summationdisplay
a,sµ(s)π∆r(a|s)/parenleftig
1 +β(α−1)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
−1/parenleftbig
∆r(a,s)−ψ∆r(s;β)/parenrightbig/parenrightig
(1)=/parenleftig
1−1
α/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
α−1
α/parenrightig/summationdisplay
a,sµ(s)π∆r(a|s)·∆r(a,s)−1
β1
α/summationdisplay
a,sµ(s)π0(a|s)−/parenleftig1
β1
1−α−1
β1
α(1−α)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
−1
β1
α/parenrightig/summationdisplay
a,sµ(s)π∆r(a|s) +1
αψ∆r(s;β)
(2)=1
β1
α/summationdisplay
a,sµ(s)π∆r(a|s) +β
βα−1
α/summationdisplay
a,sµ(s)π∆r(a|s)·∆r(a,s)±β
βα−1
αψ∆r(s;β) +1
αψ∆r(s;β)−1
β1
α/summationdisplay
a,sµ(s)π0(a|s)
where in (1)we note that1
β1
α(1−α)·β(α−1) =−1
α. In(2), we add and subtract the term in blue, which will
allow to factorize an additional term of [1 +β(α−1)(∆r−ψ∆r(s;β))]and obtain a function of ∆r(a,s)only
1
βΩ∗(α)
π0,β(∆r) =1
β1
α/summationdisplay
a,sµ(s)π∆r(a|s)/parenleftig
1 +β(α−1)/parenleftbig
∆r(a,s)−ψ∆r(s;β)/parenrightbig/parenrightig
+/parenleftigα−1
α+1
α/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
1/parenrightig
ψ∆r(s;β)−1
β1
α/summationdisplay
a,sµ(s)π0(a|s)
(1)=1
β1
α/summationdisplay
a,sµ(s)/parenleftig
π0(a|s)/bracketleftig
1 +β(α−1)/parenleftbig
∆r(a,s)−ψ∆r(s;β)/parenrightbig/bracketrightigα
α−1−1/parenrightig
+/summationdisplay
sµ(s)ψ∆r(s;β)⟩ (65)
where in (1)we have used Eq. (63) and1
α−1+ 1 =α
α−1, along with/summationtext
aπ0(a|s) = 1.
Confirming Conjugate Optimality Conditions Finally, we confirm that differentiating Eq. (65) with
respect to ∆r(a,s)yields the conjugate condition π∆r=∇1
βΩ∗(α)
π0,β(∆r). Noting thatα
α−1−1 =1
α−1,
∇1
βΩ∗(α)
π0,β(∆r) =β(α−1)
βαα
α−1/summationtext
sµ(s)/summationtext
aπ0(a|s)/bracketleftbig
1 +β(α−1)/parenleftbig
∆r(a,s)−ψ∆r(s;β)/parenrightbig/bracketrightbig1
α−1/parenleftbig∂∆r(a,s)
∂∆r(a′,s′)−∂ψ∆r(s;β)
∂∆r(a′,s′)/parenrightbig
+/summationtext
sµ(s)∂ψ∆r(s;β)
∂∆r(a′,s′)
which simplifies to π∆r(a|s) =∂
∂∆r(a,s)1
βΩ∗(α)
π0,β(∆r) =π0(a|s)/bracketleftbig
1 +β(α−1)/parenleftbig
∆r(a,s)−ψ∆r(s;β)/parenrightbig/bracketrightbig1
α−1and
matches Eq. (63).
23Published in Transactions on Machine Learning Research (07/2022)
B.4α-Divergence Occupancy Regularization:1
βΩ∗(α)
µ0,β(∆r)
The conjugate function1
βΩ∗(α)
µ0,β(∆r)forα-divergence regularization of the state-action occupancy µ(a,s)to
a reference µ0(a,s)can be written in the following form
1
βΩ∗(α)
µ0,β(∆r) =1
β1
α/summationdisplay
a,sµ0(a,s)/bracketleftbig
1 +β(α−1)∆r(a,s)/bracketrightbigα
α−1−1
β1
α(66)
Note that this conjugate function can also be derived directly from the duality of general f-divergences, and
matches the form of conjugate considered in (Belousov & Peters, 2019; Nachum & Dai, 2020).
Proof.Worst-Case Reward Perturbations ∆rµ(a|s)
∆r(a,s) =1
β1
α(1−α)∇µ/parenleftigg
(1−α)/summationdisplay
a,sµ0(a,s) +α/summationdisplay
a,sµ(a,s)−/summationdisplay
a,sµ0(a,s)1−αµ(a,s)α/parenrightigg
(67)
=1
β1
1−α−1
β1
1−αµ0(a,s)1−αµ(a,s)α−1
Optimizing Argument µ∆r(a,s).Solving for µ∆r(a,s),
µ∆r(a,s) =µ0(a,s) expα{β·∆r(a,s)}=µ0(a,s)[1 +β(1−α)∆r(a,s)]1
α−1
+ (68)
Conjugate Function1
βΩ∗(α)
µ0,β(∆r).Plugging this back into the conjugate optimization, we finally obtain
1
βΩ∗(α)
µ0,β=/summationdisplay
a,sµ∆r(a,s)·∆r(a,s)−1
β1
α(1−α)/parenleftbigg
(1−α)/summationdisplay
a,sµ0(a,s) +α/summationdisplay
a,sµ∆r(a,s)−/summationdisplay
a,sµ∆r(a,s)/parenleftbigg
µ∆r(a,s)
µ0(a,s)/parenrightbiggα−1
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=1+β(α−1)∆r(a,s)(Eq.(68))/parenrightbigg
=/parenleftig
1−1
α/parenrightig/summationdisplay
a,sµ∆r(a,s)·∆r(a,s)−1
β1
α/summationdisplay
a,sµ0(a,s) +/parenleftbigg
1
β1
α(1−α)−1
β1
1−α/parenrightbigg/summationdisplay
a,sµ∆r(a,s) (69)
=α−1
α/summationdisplay
a,sµ0(a,s)/bracketleftbig
1 +β(α−1)∆r(a,s)/bracketrightbig1
α−1·∆r(a,s) +1
β1
α/summationdisplay
a,sµ0(a,s)/bracketleftbig
1 +β(α−1)∆r(a,s)/bracketrightbig1
α−1−1
β1
α/summationdisplay
a,sµ0(a,s)
=/summationdisplay
a,sµ0(a,s)/bracketleftbig
1 +β(α−1)∆r(a,s)/bracketrightbig1
α−1·1
β1
α/parenleftbig
1 +β(α−1)∆r(a,s)/parenrightbig
−1
β1
α/summationdisplay
a,sµ0(a,s) (70)
=1
β1
α/summationdisplay
a,sµ0(a,s)/bracketleftbig
1 +β(α−1)∆r(a,s)/bracketrightbigα
α−1−1
β1
α/summationdisplay
a,sµ0(a,s) (71)
where, to obtain the exponent in the last line, note that1
α−1+ 1 =α
α−1.
C Soft Value Aggregation
Soft value aggregation (Fox et al., 2016; Haarnoja et al., 2017) and the regularized Bellman optimality
operator (Neu et al., 2017; Geist et al., 2019) also rely on the convex conjugate function, but with a slightly
different setting than our derivations for the optimal regularized policy or reward perturbations in App. B. In
particular, in each state we optimize over the policy π(a|s)∈∆|A|using an explicit normalization constraint
(Eq. (74)).
We derive the regularized Bellman optimality operator from the primal objective in Eq. (12). Factorizing
µ(a,s) =µ(s)π(a|s), we can imagine optimizing over µ(s)andπ(a|s)∈∆|A|separately,
max
µ(s)→Mmax
π(a|s)∈∆|A|min
V(s)(1−γ)/angbracketleftbig
ν0(s),V(s)/angbracketrightbig
+/angbracketleftbig
µ(a,s),r(a,s) +γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
−V(s)/angbracketrightbig
−1
βΩπ0(µ).(72)
24Published in Transactions on Machine Learning Research (07/2022)
Eliminating µ(s)(by setting d/dµ (s) = 0) leads to a constraint on the form of V(s), since both may be
viewed as enforcing the Bellman flow constraints.
V(s) =/angbracketleftbig
π(a|s), r(s,a) +γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig/angbracketrightbig
−1
βΩπ0(π). (73)
We defineQ(s,a):=r(s,a) +γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
and writeV(s) =⟨π, Q⟩−1
βΩπ0(π)moving forward.
As an operator for iteratively updating V(s), Eq. (73) corresponds to the regularized Bellman operator
TΩπ0,βand may be used to perform policy evaluation for a given π(a|s)(Geist et al., 2019). The regularized
Bellman optimality operatorT∗
Ωπ0,β, which can be used for value iteration or modified policy iteration (Geist
et al., 2019), arises from including the maximization over π(a|s)∈∆|A|from Eq. (72)
V(s)←1
βΩ∗
π0,β(Q) = max
π∈∆|A|/angbracketleftbig
π,Q/angbracketrightbig
−1
βΩπ0(π). (74)
ComparisonofConjugateOptimizations Eq.(74)hastheformofaconjugateoptimization1
βΩ∗
π0,β(Q)
(Geist et al., 2019). However, in contrast to the setting of App. A.2 and App. B, we optimize over the
policy in each state, rather than the state-action occupancy µ(a,s). Further, we must include normalization
and nonnegativity constraints π(a|s)∈∆|A|, which can be enforced using Lagrange multipliers ψQ(s;β)and
λ(a,s). We derive expressions for this conjugate function for the kldivergence in App. C.1 and α-divergence
in App. C.2, and plot the value V∗(s)as a function of αandβin App. C.4.
Compared with the optimization for the optimal policy in Eq. (35), note that the argument of the conjugate
function does not include the value function V(s)in this case. We will highlight relationship between the
normalization constants ψQ∗(s;β),ψ∆rπ∗(s;β), andV∗(s)in App. C.3, where ψQ∗(s;β) =V∗(s)+ψ∆rπ∗(s;β)
as in Lee et al. (2019) App. D.
C.1 KL Divergence Soft Value Aggregation:1
βΩ∗
π0,β(Q)
We proceed to derive a closed form for the conjugate function of the kldivergence Ωπ0(π)as a function of
π(a|s)∈∆|A|, which we write using the Q-values as input
1
βΩ∗
π0,β(Q) = max
π(a|s)⟨π,Q⟩−1
β/parenleftigg/summationdisplay
aπ(a|s) logπ(a|s)
π0(a|s)+/summationdisplay
aπ(a|s)−/summationdisplay
aπ0(a|s)/parenrightigg
−ψQ(s;β)/parenleftigg/summationdisplay
a∈Aπ(a|s)−1/parenrightigg
+/summationdisplay
a∈Aλ(a,s)
=⇒Q(a,s) =1
βlogπ(a|s)
π0(a|s)+ψQ(s;β)−λ(a,s) (75)
Optimizing Argument Solving for πyields the optimizing argument
πQ(a|s) =π0(a|s) exp/braceleftbig
β·/parenleftbig
Q(a,s)−ψQ(s;β))/parenrightbig/bracerightbig
(76)
where we can ignore the Lagrange multiplier for the nonnegativity constraint λ(a,s)since exp{·}≥ 0ensures
πQ(a|s)≥0.We can pull the normalization constant out of the exponent to solve for
ψQ(s;β) =1
βlog/summationdisplay
aπ0(a|s) exp/braceleftbig
β·Q(a,s)/bracerightbig
. (77)
Plugging Eq. (76) into the conjugate optimization,
1
βΩ∗
π0,β(Q) =/angbracketleftbig
πQ,Q/angbracketrightbig
−1
β/parenleftig/summationdisplay
aπQ(a|s)·log
π0(a|s)
π0(a|s)exp/braceleftbig
β·/parenleftbig
Q(a,s)−ψQ(s;β)/parenrightbig/bracerightbig
+/summationdisplay
aπQ(a|s)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
1−1/parenrightig
−ψQ(s;β)/parenleftig/summationdisplay
a∈AπQ(a|s)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
1−1/parenrightig
.
25Published in Transactions on Machine Learning Research (07/2022)
Conjugate Function We finally recover the familiar log-mean-exp form for the kl-regularized value
function
V(s)←1
βΩ∗
π0,β(Q) =ψQ(s;β) =1
βlog/summationdisplay
aπ0(a|s) exp/braceleftbig
β·Q(a,s)/bracerightbig
. (78)
Notice that the conjugate or value function V(s)←1
βΩ∗
π0,β(Q)is exactly equal to the normalization constant
of the policy ψQ(s;β). We will show in App. C.2 that this property does not hold for general α-divergences,
with example visualizations in App. C.3 Fig. 5.
C.2α-Divergence Soft Value Aggregation:1
βΩ∗
π0,β(Q)
We now consider soft value aggregation using the α-divergence, where in contrast to App. B.3, we perform
the conjugate optimization over π(a|s)∈∆|A|in each state, with Lagrange multipliers ψQ(s;β)andλ(a,s)
to enforce normalization and nonnegativity.
1
βΩ∗
π0,β(Q) = max
π(a|s)/angbracketleftbig
π,Q/angbracketrightbig
−1
β1
α/summationdisplay
aπ0(a|s)−1
β1
1−α/summationdisplay
aπ(a|s) +1
β1
α(1−α)/summationdisplay
aπ0(a|s)1−απ(a|s)α(79)
−ψQ(s;β)/parenleftigg/summationdisplay
aπ(a|s)−1/parenrightigg
+/summationdisplay
aλ(a,s)
=⇒Q(a,s) =1
βlogαπ(a|s)
π0(a|s)+ψQ(s;β)−λ(a,s) (80)
Optimizing Argument Solving for πyields the optimizing argument for the soft value aggregation con-
jugate,
πQ(a|s) =π0(a|s) expα/braceleftbig
β·/parenleftbig
Q(a,s) +λ(a,s)−ψQ(s;β))/parenrightbig/bracerightbig
. (81)
Unlike the case of the standard exponential, we cannot easily derive a closed-form solution for ψQ(s;β).
Note that the expressions in Eq. (80) and Eq. (81) are similar to the form of the worst-case reward perturba-
tions ∆rπ(a|s)in Eq. (62) and optimizing policy π∆r(a|s)in Eq. (63), except for the fact that ψQ(s;β)arises
as a Lagrange multiplier and does not have the same form as ψ∆r(s;β) =1
β(1−α)Dα[π0:π]as in Eq. (23)
and Eq. (61). We will find that ψQ(s;β)andψ∆r(s;β)differ by a term of V∗(s)in App. C.3 (Eq. (86)).
Conjugate Function Plugging Eq. (81) into the conjugate optimization, we use similar derivations as in
Eq. (64)-Eq. (65) to write the conjugate function, or regularized Bellman optimality operator as
V(s)←1
βΩ∗(α)
π0,β(Q) =1
β1
α/summationdisplay
aπ0(a|s)/bracketleftig
1 +β(α−1)/parenleftbig
Q(a,s) +λ(a,s)−ψQ(s;β)/parenrightbig/bracketrightigα
α−1
+−1
β1
α+ψQ(s;β)
(82)
=1
β1
α/summationdisplay
aπ0(a|s) expα/braceleftig
β·/parenleftbig
Q(a,s) +λ(a,s)−ψQ(s;β)/parenrightbig/bracerightigα
−1
β1
α+ψQ(s;β)
Comparison with KL Divergence Regularization Note that for general α, the conjugate or value
functionV(s) =1
βΩ∗
π0,β(Q)in Eq. (82) is notequal to the normalization constant of the policy ψQ(s;β).
We discuss this further in the next section.
We also note that the form of the conjugate function is similar using two different approaches: optimizing
overπwith an explicit normalization constraint, as in Eq. (82), or optimizing over µwith regularization
ofπbut no explicit normalization constraint, as in App. B.3 or Table 2. This is in contrast to the kl
divergence, where the normalization constraint led to a log-mean-exp conjugate in Eq. (75) which is different
from App. B Eq. (47).
26Published in Transactions on Machine Learning Research (07/2022)
C.3 Relationship between Normalization Constants ψ∆rπ∗,ψQ∗, and Value Function V∗(s)
In this section, we analyze the relationship between the conjugate optimizations that we have considered
above, either optimizing over µ(a,s)as in deriving the optimal policy, or optimizing over π(a|s)∈∆|A|as in
the regularized Bellman optimality operatoror soft-valueaggregation. Using Q(a,s) =r(a,s)+γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
,
Optimal Policy (or Worst-Case Reward Perturbations) (App. B.3) (83)
1
βΩ∗(α)
π0,β/parenleftig
r+γEs′
a,s/bracketleftbig
V/bracketrightbig
−V+λ/parenrightig
= max
µ(a,s)∈F/angbracketleftbig
µ,r+γEs′
a,s/bracketleftbig
V/bracketrightbig
−V+λ/angbracketrightbig
−1
βΩ(α)
π0(µ)
Soft Value Aggregation (App. C.2), (84)
V(s)←1
βΩ∗(α)
π0,β/parenleftig
r+γEs′
a,s/bracketleftbig
V/bracketrightbig/parenrightig
= max
π∈∆|A|/angbracketleftbig
µ,r+γEs′
a,s/bracketleftbig
V/bracketrightbig/angbracketrightbig
−1
βΩ(α)
π0(π)
Note that the arguments differ by a term of V(s). We ignore the apparent difference in the λ(a,s)term,
which can be considered as an argument of the conjugate in Eq. (84) since a linear term of ⟨µ,λ⟩appears
when enforcing π∈∆|A|. Evaluating the optimizing arguments,
Optimal Policy (or Worst-Case Reward Perturbations) (App. B.3, Eq. (63), Table 2)
π(a|s) =π0(a|s) expα/braceleftig
β·/parenleftbig
Q(a,s) +λ(a,s)−V(s)−ψ∆r(s;β)/parenrightbig/bracerightig
(85)
Soft Value Aggregation (App. C.2, Eq. (82)),
π(a|s) =π0(a|s) expα/braceleftig
β·/parenleftbig
Q(a,s) +λ(a,s)−ψQ(s;β)/parenrightbig
For the optimal V∗(s)andQ∗(a,s) =r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−V∗(s), the two policies match. This can be
confirmed using similar reasoning as in Lee et al. (2019) App. D-E or (Geist et al., 2019) to show that
iterating the regularized Bellman optimality operator leads to the optimal policy and value.
Relationship between V∗(s)andψQ∗(s;β)This implies the condition which is the main result of this
section.
ψQ∗(s;β) =V∗(s) +ψ∆rπ∗(s;β). (86)
In Fig. 5 we empirically confirm this identity and inspect how each quantity varies with βandα(App. C.4)4
Eq. (86) highlights distinct roles for the value function V∗(s)and the Lagrange multiplier ψQ∗(s;β)enforcing
normalization of π(a|s)in soft-value aggregation ( Eq. (79) or Eq. (84)). It is well known that these coincide
in the case of kldivergence regularization, with V∗(s) =ψQ∗(s;β)as in App. C.1. We can also confirm
thatψ∆rπ∗(s;β) =1
β1
α/parenleftbig/summationtext
aπ0(a|s)−/summationtext
aπ0(a|s)1−απ∗(a|s)α/parenrightbig
= 0vanishes for klregularization ( α= 1)
and normalized π0and the normalized optimal policy π∗.
However, in the case of α-divergence regularization, optimization over the joint µ(a,s)in Eq. (83) introduces
the additional term ψ∆rπ∗(s;β), which is not equal to 0 in general.
Relationship between Conjugate Functions We might also like to compare the value of the conjugate
functions in Eq. (83) and Eq. (84), in particular to understand how including V∗(s)as an argument and
optimizing over πversusµaffect the optima. We write the expressions for the conjugate function in each
4Note thatψ∆rπ∗(s;β) =1
β1
α/parenleftbig/summationtext
aπ0(a|s)−/summationtext
aπ0(a|s)1−απ∗(a|s)α/parenrightbig
appears from differentiating1
βΩ(α
π0)(µ)with respect
toµ(App. B.3 Eq. (61)). We also write this as ψ∆rπ∗(s;β) =1
β(1−α)Dα[π0:π∗]for normalized π0andπ∗.
27Published in Transactions on Machine Learning Research (07/2022)
-3 -2 -1 0 1 2 3
0.940.950.960.970.98
1*()
0,
Value Fn V*(s) by 
-3 -2 -1 0 1 2 3
0.81.01.21.4
(s;)
Normalization (s;) by 
-3 -2 -1 0 1 2 3
0.2
0.00.20.4
1(1 )D
1(1 )D[0:*] by 
-3 -2 -1 0 1 2 3
-0.00100.001
Diff(s;)V*(s)1(1 )D[0:*]=0
 = 0.5
(a)β= 0.5
-3 -2 -1 0 1 2 3
1.021.041.061.081.10
1*()
0,
Value Fn V*(s) by 
-3 -2 -1 0 1 2 3
0.751.001.251.501.75
(s;)
Normalization (s;) by 
-3 -2 -1 0 1 2 3
0.25
0.000.250.50
1(1 )D
1(1 )D[0:*] by 
-3 -2 -1 0 1 2 3
-0.00100.001
Diff(s;)V*(s)1(1 )D[0:*]=0
 = 1.0
 (b)β= 1
-3 -2 -1 0 1 2 3
1.201.251.30
1*()
0,
Value Fn V*(s) by 
-3 -2 -1 0 1 2 3
1.01.5
(s;)
Normalization (s;) by 
-3 -2 -1 0 1 2 3
0.5
0.00.5
1(1 )D
1(1 )D[0:*] by 
-3 -2 -1 0 1 2 3
-0.00100.001
Diff(s;)V*(s)1(1 )D[0:*]=0
 = 2
 (c)β= 2
-3 -2 -1 0 1 2 3
1.41.51.6
1*()
0,
Value Fn V*(s) by 
-3 -2 -1 0 1 2 3
1.001.251.501.752.00
(s;)
Normalization (s;) by 
-3 -2 -1 0 1 2 3
0.50
0.25
0.000.250.50
1(1 )D
1(1 )D[0:*] by 
-3 -2 -1 0 1 2 3
-0.00100.001
Diff(s;)V*(s)1(1 )D[0:*]=0
 = 5
 (d)β= 5
Figure 5: Value Function V∗(s) =1
βΩ∗(α)
π0,β(Q∗)(first row) and Normalization Constant ψQ∗(s;β)(second
row) as a function of αfor various regularization strengths β. We use the same rewards as in Fig. 3 and
Fig. 7 and a uniform reference. We plot ψ∆r(s;β) =1
β(1−α)Dα[π0:π∗]in the third row, and confirm the
identityV∗(s) =ψQ∗(s;β)−ψ∆r(s;β)from Eq. (86) and (90) in the last row. We find that this equality
holds for all αup to small optimization errors on the order of 10−3.
0.001 0.01 0.1 1 10 100
1/
1.01.52.0
V(s)=
1*()
0,
maxar(a,s)
0[r(a,s)]
maxar(a,s)
0[r(a,s)]
maxar(a,s)
0[r(a,s)]
maxar(a,s)
0[r(a,s)]
maxar(a,s)
0[r(a,s)]
maxar(a,s)
0[r(a,s)]
=-1
KL[0||]
=0.5
KL[||0]
=2
=3
ai00.20.4Prior 0(a|s)
ai012Q(a,s) Values
(a) Uniform π0(a|s)
0.001 0.01 0.1 1 10 100
1/
1.52.0
V(s)=
1*()
0,
maxar(a,s)
0[r(a,s)]
maxar(a,s)
0[r(a,s)]
maxar(a,s)
0[r(a,s)]
maxar(a,s)
0[r(a,s)]
maxar(a,s)
0[r(a,s)]
maxar(a,s)
0[r(a,s)]
=-1
KL[0||]
=0.5
KL[||0]
=2
=3
ai00.20.4Prior 0(a|s)
ai012Q(a,s) Values (b)π0(a|s)∝r(a,s)
Figure 7: Value function V(s) =1
βΩ∗(α)
µ0,β(Q)as a function of β(x-axis) and α(colored lines), using Q(a,s)
andπ0(a|s)from the left inset. See Eq. (78) and Eq. (82) for closed forms.
28Published in Transactions on Machine Learning Research (07/2022)
case, highlighting the terms from Eq. (86) in blue.
Optimal Policy (or Worst-Case Reward Perturbations) (App. B.3, Eq. (56), Table 2)
1
βΩ∗(α)
π0,β/parenleftig
r+γEs′
a,s/bracketleftbig
V∗/bracketrightbig
−V∗+λ∗/parenrightig
(87)
=1
β1
α/summationdisplay
aπ0(a|s) expα/braceleftig
β·/parenleftbig
Q∗(a,s) +λ∗(a,s)−V∗(s)−ψ∆r(s;β)/parenrightbig/bracerightigα
−1
β1
α+ψ∆rπ∗(s;β)
Soft Value Aggregation (App. C.2, Eq. (82)) ,
V∗(s)←1
βΩ∗
π0,β/parenleftig
r+γEs′
a,s/bracketleftbig
V∗/bracketrightbig/parenrightig
(88)
=1
β1
α/summationdisplay
aπ0(a|s) expα/braceleftig
β·/parenleftbig
Q∗(a,s) +λ∗(a,s)−ψQ(s;β)/parenrightbig/bracerightigα
−1
β1
α+ψQ(s;β)
Note that we have rewritten V∗(s)←1
βΩ∗
π0,β(r+γEs′
a,s/bracketleftbig
V∗/bracketrightbig
)directly asV∗(s). To further simplify, note that
the optimal policy matches as in Eq. (85), with
π∗(a|s) =π0(a|s) expα{β·(Q∗(a,s) +λ∗(a,s)−ψQ∗(s;β))}
=π0(a|s) expα{β·(Q∗(a,s) +λ∗(a,s)−V∗(s)−ψ∆rπ∗(s;β))}.
Sinceπ∗(a|s) =π0(a|s) expα{·}, we can write terms of the form π0(a|s) expα{·}αin Eq. (87)-(88) as
π0(a|s)1−απ∗(a|s)α, where the exponents of π0(a|s)add to 1. Finally, we use this expression to simplify
the value function expression in Eq. (88), eventually recovering the equality in Eq. (86)
1
βΩ∗(α)
π0,β/parenleftig
r+γEs′
a,s/bracketleftbig
V∗/bracketrightbig/parenrightig
=V∗(s) =1
β1
α/summationdisplay
aπ0(a|s)1−απ∗(a|s)α−1
β1
α+ψQ(s;β) (89)
=ψQ(s;β)−ψ∆rπ∗(s;β) (90)
In the second line, we use the fact that ψ∆rπ∗(s;β) =1
β1
α/summationtext
aπ0(a|s)−1
β1
α/summationtext
aπ0(a|s)1−απ∗(a|s)αfrom
Eq. (61). We can use the same identity to show that the conjugate in Eq. (87) evaluates to zero,
1
βΩ∗(α)
π0,β/parenleftig
r+γEs′
a,s/bracketleftbig
V∗/bracketrightbig
−V∗+λ∗/parenrightig
=1
β1
α/summationdisplay
aπ0(a|s)1−απ∗(a|s)α−1
β1
α
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
ψ∆rπ∗(s;β)+ψ∆rπ∗(s;β) = 0 (91)
In Lemma 2, we provide a more detailed proof and show that this identity also holds for suboptimal policies
andtheirworst-caserewardperturbations,1
βΩ∗(α)
π0,β(∆rπ) = 0, whereEq.(91)isaspecialcasefor ∆rπ∗(a,s) =
r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−V∗(s) +λ∗(a,s).
Finally, note that the condition in Eq. (91) implies that for the optimal V∗(s), the regularized dual objective
RL∗
Ω,β(r) = minV,λ(1−γ)⟨ν0,V⟩+1
βΩ∗
π0,β/parenleftig
r+γEs′
a,sV−V+λ/parenrightig
in Eq. (14) reduces to the value function
averaged over initial states, RL∗
Ω,β(r) = (1−γ)⟨ν0,V∗⟩. This is intuitive since V∗(s)measures the regularized
objective attained from running the optimal policy for infinite time in the discounted mdp.
C.4 Plotting Value Function as a Function of Regularization Parameters α,β
Confirming Relationship between Normalization ψ∆rπ∗(s;β),ψQ∗(s;β)and Value Function V∗(s)
In Fig. 5, we plot both V∗(s)andψQ∗(s;β)for various values of α(x-axis) and β(in each panel). We also
plotψ∆rπ∗(s;β) =1
β(1−α)Dα[π0:π∗]in the third row, and confirm the identity in Eq. (86) in the fourth
row.
As we also observe in Fig. 7, the soft value function or certainty equivalent V∗(s) =1
βΩ∗(α)
π0,β(Q)is not
monotonic in αfor this particular set of single-step rewards (the same r(a,s)as in Fig. 3 or Fig. 7). Note
the small scale of the y-axis in the top row of Fig. 5.
While it can be shown that ψ∆r(s;β)is convex as a function of β, we see that ψ∆r(s;β)is not necessarily
convex inαand appears to be monotonically decreasing in α. Finally, we find that the identity in Eq. (86)-
(90) holds empirically, with only small numerical optimization issues.
29Published in Transactions on Machine Learning Research (07/2022)
ValueV∗(s)as a Function of β,αIn Fig. 7, we visualize the optimal value function V∗(s) =1
βΩ∗
π0,β(Q),
forklorα-divergence regularization and different choices of regularization strength 1/β. The choice of
divergence particularly affects the aggregated value at low regularization strength, although we do not
observe a clear pattern with respect to α.5In all cases, the value function ranges between maxaQ(a,s)for
an unregularized deterministic policy as β→∞, and the expectation under the reference policy Eπ0[Q(a,s)]
for strong regularization as β→0. We also discuss this property in Sec. 5.1.
D Robust Set of Perturbed Rewards
In this section, we characterize the robust set of perturbed rewards to which a given policy π(a|s)orµ(a,s)
is robust, which also provides performance guarantees as in Eq. (2) and also describes the set of strategies
available to the adversary. For proving Prop. 1, we focus our discussion on policy regularization with klor
α-divergence regularization and compare with state-occupancy regularization in App. D.2.
D.1 Proof of Prop. 1: Robust Set of Perturbed Rewards for Policy Regularization
We begin by stating two lemmas, which we will use to characterize the robust set of perturbed rewards. All
proofs are organized under paragraph headers below the statement of Prop. 1.
Lemma 2. For the worst-case reward perturbation ∆rπ(a,s)associated with a given, normalized policy
π(a|s)andα- orkl-divergence regularization, the conjugate function evaluates to zero,
1
βΩ∗(α)
π0,β(∆rπ) = 0. (92)
Lemma 3. The conjugate function1
βΩ∗(α)
π0,β(∆r)is increasing in ∆r. In other words, if ∆˜r(a,s)≥∆r(a,s)
for all (a,s)∈A×S, then1
βΩ∗(α)
π0,β(∆˜r)≥1
βΩ∗(α)
π0,β(∆r).
Proposition 1. Assume a normalized policy π(a|s)for the agent is given, with/summationtext
aπ(a|s) = 1∀s∈S.
Underα-divergence policy regularization to a normalized reference π0(a|s), the optimization over ∆r(a,s)in
Eq.(17)can be written in the following constrained form
min
∆r∈R∆π/angbracketleftbig
µ,r−∆r/angbracketrightbig
whereR∆
π:=/braceleftbigg
∆r∈RA×S/vextendsingle/vextendsingle/vextendsingle/vextendsingleΩ∗(α)
π0,β(∆r)≤0/bracerightbigg
, (18)
We refer toR∆
π⊂RA×Sas the feasible set of reward perturbations available to the adversary. This translates
to a robust setRπof modified rewards r′(a,s) =r(a,s)−∆r(a,s)for the given policy. These sets depend
on theα-divergence and regularization strength βvia the conjugate function.
Forkldivergence regularization, the constraint is
/summationdisplay
a∈Aπ0(a|s) exp/braceleftbig
β·∆r(a,s)/bracerightbig
≤1. (19)
Proof.Recall the adversarial optimization in Eq. (17) for a fixed µ(a,s) =µ(s)π(a|s)
min
∆r⟨µ, r−∆r⟩+1
βΩ∗(α)
π0,β/parenleftbig
∆r/parenrightbig
, (93)
which we would like to transform into a constrained optimization. From Lemma 2, we know that
1
βΩ∗(α)
π0,β(∆rπ) = 0for the optimizing argument ∆rπin Eq. (93), but it is not clear whether this should
appear as an equality or inequality constraint. We now show that the constraint1
βΩ∗(α)
π0,β(∆r)≥0changes
5See Belousov & Peters (2019); Lee et al. (2018; 2019), or Appendix C.4 for additional discussion of the effect of α-divergence
regularization on learned policies.
30Published in Transactions on Machine Learning Research (07/2022)
the value of the objective, whereas the constraint1
βΩ∗(α)
π0,β(∆r)≤0does not change the value of the opti-
mization.
≥Inequality First, consider the optimization min∆r⟨µ,r−∆r⟩subject to1
βΩ∗(α)
π0,β(∆r)≥0. From the
optimizing argument ∆rπ, consider an increase in the reward perturbations ∆˜r(a,s)≥∆rπ(a,s)∀(a,s)
where∃(a,s)s.t.µ(a,s)>0and∆˜r(a,s)>∆rπ(a,s). By Lemma 3, we have1
βΩ∗(α)
π0,β(∆r)≥1
βΩ∗(α)
π0,β(∆rπ) =
0. However, the objective now satisfies ⟨µ,r−∆˜r⟩<⟨µ,r−∆rπ⟩for fixedµ(a,s), which is a contradiction
since ∆rπprovides a global minimum of the convex objective in Eq. (93).
≤Inequality We would like to show that this constraint does not introduce a different global minimum
of Eq. (93). Assume there exists ∆˜r(a,s)with1
βΩ∗(α)
π0,β(∆˜r)<0and⟨µπ,r−∆˜r⟩<⟨µπ,r−∆rπ⟩for
the occupancy measure µπassociated with the given policy π. By convexity of1
βΩ∗(α)
π0,β(∆r), we know
that a first-order Taylor approximation around ∆rπeverywhere underestimates the function,1
βΩ∗(α)
π0,β(∆˜r)≥
1
βΩ∗(α)
π0,β(∆rπ) +⟨∆˜r−∆rπ,∇1
βΩ∗(α)
π0,β(∆rπ)⟩. Noting that µπ=∇1
βΩ∗(α)
π0,β(∆rπ)by the conjugate optimality
conditions (Eq. (5), App. A), we have1
βΩ∗(α)
π0,β(∆˜r)−1
βΩ∗(α)
π0,β(∆rπ)≥ ⟨µπ,∆˜r⟩−⟨µπ,∆rπ⟩. This now
introduces a contradiction, since we have assumed both that1
βΩ∗(α)
π0,β(∆˜r)−1
βΩ∗(α)
π0,β(∆rπ)<0, and that
∆˜r(a,s)provides a global minimum, where ⟨µπ,r−∆˜r⟩<⟨µπ,r−∆rπ⟩implies⟨µπ,∆˜r⟩−⟨µπ,∆rπ⟩>0.
Thus, including the inequality constraint1
βΩ∗(α)
π0,β(∆˜r)≤0cannot introduce different minima.
This constraint is consistent with the constrained optimization and generalization guarantee in Eq. (1)-
(2), where it is clear that increasing the modified reward away from the boundary of the robust set (i.e.
decreasing ∆r(a,s)and1
βΩ∗(α)
π0,β(∆r)) is feasible for the adversary and preserves our performance guarantee.
See Eysenbach & Levine (2021) A2 and A6 for alternative reasoning.
Proof of Lemma 2 Forα-divergence policy regularization and a given π(a|s), we substitute the worst-case
reward perturbations ∆rπ(a,s) =1
βlogαπ(a|s)
π0(a|s)+ψ∆r(s;β)(Eq. (22) or Eq. (62)) in the conjugate function
1
βΩ∗(α)
π0,β(∆rπ)(Eq. (56) or Table 2). Assuming/summationtext
aπ(a|s) =/summationtext
aπ0(a|s) = 1, we have
1
βΩ∗(α)
π0,β(∆rπ) =1
β1
α/summationdisplay
aπ0(a|s) expα/braceleftbig
β·/parenleftbig
∆rπ(a,s)−ψ∆r(s;β)/parenrightbig/bracerightbigα−1
β1
α+ψ∆r(s;β)
=1
β1
α/summationdisplay
aπ0(a|s)/bracketleftig
1 +β(α−1)/parenleftbigg
1
β1
α−1/parenleftbigπ(a|s)
π0(a|s)α−1
−1/parenrightbig
+ψ∆r(s;β)−ψ∆r(s;β)/parenrightig/bracketrightigα
α−1
+−1
β1
α+ψ∆r(s;β)
=1
β1
α/summationdisplay
aπ0(a|s)1−απ(a|s)α−1
β1
α+ψ∆r(s;β)
= 0.
In the last line, we recall that ψ∆r(s;β) =1
β1
α/summationtext
aπ0(a|s)−1
β1
α/summationtext
aπ0(a|s)1−απ(a|s)αfrom Eq. (23) or (61).
Forklregularization, we plug ∆rπ(a,s) =1
βlogπ(a|s)
π0(a|s)(Eq. (21),(50)) into the conjugate in Eq. (47) or
Table 2,
1
βΩ∗
π0,β(∆rπ) =1
β/summationdisplay
aπ0(a|s) exp/braceleftbig
β∆rπ(a,s)/bracerightbig
−1
β=1
β/summationdisplay
aπ0(a|s) exp/braceleftbig
β1
βlogπ(a|s)
π0(a|s)/bracerightbig
−1
β=1
β/summationdisplay
aπ(a|s)−1
β= 0.
Proof of Lemma 3 See Husain et al. (2021) Lemma 3.
D.2 Robust Set for α-Divergence under µ(a,s)Regularization
For state-action occupancy regularization and kldivergence, Lemma 2 holds with1
βΩ∗
µ0,β(∆rµ) = 0for
normalized µ(a,s)and∆rµ(a,s) =1
βlogµ(a,s)
µ0(a,s). However, the reasoning in App. D no longer holds for α-
divergence regularization to a reference µ0(a,s). Substituting the worst-case reward perturbations (Eq. (24)
31Published in Transactions on Machine Learning Research (07/2022)
or (67)) into the conjugate function (Eq. (66) or Table 2)
1
βΩ∗(α)
µ0,β(∆rµ) =1
β1
α/summationdisplay
aµ0(a,s) expα/braceleftbig
β·∆rµ(a,s)/bracerightbigα−1
β1
α(94)
=1
β1
α/summationdisplay
aµ0(a,s)/bracketleftig
1 +β(α−1)/parenleftbigg1
β1
α−1/parenleftbigµ(a,s)
µ0(a,s)α−1
−1/parenrightbig/parenrightig/bracketrightigα
α−1
+−1
β1
α
=1
β1
α/summationdisplay
aµ0(a,s)1−αµ(a,s)α−1
β1
α
whose value is not equal to 0in general and instead is a function of the given µ(a,s). This may result in the
original environmental reward not being part of the robust set, since substituting ∆r(a,s) = 0into Eq. (94)
results in1
βΩ∗(α)
µ0,β(∆r) = 0.
D.3 Plotting the α-Divergence Feasible Set
To plot the boundary of the feasible set in the single step case, for the kldivergence regularization in two
dimensions, we can simply solve for the ∆r(a2,s)which satisfies the constraint/summationtext
aπ0(a|s) exp{β∆r(a|s)}=
1for a given ∆r(a1,s)
∆r(a2,s) =1
βlog1
π0(a2|s)(1−π0(a1|s) exp/braceleftbig
β·∆r(a1,s)/bracerightbig
). (95)
The interior of the feasible set contains ∆r(a1,s)and∆r(a2,s)that are greater than or equal to these values.
However, we cannot analytically solve for the feasible set boundary for general α-divergences, since the
conjugate function1
βΩ∗(α)
π0,β(∆r)depends on the normalization constant of π∆r(a,s). Instead, we perform
exhaustive search over a range of ∆r(a1,s)and∆r(a2,s)values. For each pair of candidate reward per-
turbations, we use cvx-py (Diamond & Boyd, 2016) to solve the conjugate optimization and evaluate
1
βΩ∗(α)
π0,β(∆r). We terminate our exhaustive search and record the boundary of the feasible set when we find
that1
βΩ∗(α)
π0,β(∆r) = 0within appropriate precision.
E Value Form Reward Perturbations
E.1 Proof of Thm. 1 (Husain et al. (2021))
We rewrite the derivations of Husain et al. (2021) for our notation and setting, where Ω/parenleftbig
µ/parenrightbig
represents a
convex regularizer. Starting from the regularized objective in Eq. (12),
max
µ∈MRLΩ,β(µ) = max
µ∈M/angbracketleftbig
µ,r⟩−1
βΩ/parenleftbig
µ/parenrightbig
, (96)
note that the objective is concave, as the sum of a linear term and the concave −Ω. Since the conjugate
is an involution for convex functions, we can rewrite RLΩ,β(r) =−(−RL Ω,β(r)) =−((−RL Ω,β)∗)∗, which
32Published in Transactions on Machine Learning Research (07/2022)
yields
RLΩ,β(r) = sup
µ∈M−((−RL Ω,β)∗)∗
(1)= sup
µ∈M−/parenleftbigg
sup
r′∈RA×S⟨µ,r′⟩−(−RL Ω,β)∗(r′)/parenrightbigg
= sup
µ∈Minf
r′∈RA×S⟨µ,−r′⟩+ (−RL Ω,β)∗(r′)
(2)= sup
µ∈Minf
r′∈RA×S⟨µ,r′⟩+ (−RL Ω,β)∗(−r′)
(3)= sup
µ∈Minf
r′∈RA×S⟨µ,r′⟩+/parenleftbigg
sup
µ′⟨µ′,−r′⟩+RLΩ,β(µ′)/parenrightbigg
(4)= sup
µ∈Minf
r′∈RA×S⟨µ,r′⟩+/parenleftbigg
sup
µ′⟨µ′,−r′⟩+⟨µ′,r⟩−1
βΩ(µ′)/parenrightbigg
= sup
µ∈Minf
r′∈RA×S⟨µ,r′⟩+1
β/parenleftbigg
sup
µ′⟨µ′,β·/parenleftbig
r−r′/parenrightbig
⟩−Ω(µ′)/parenrightbigg
(5)= sup
µ∈Minf
r′∈RA×S⟨µ,r′⟩+1
βΩ∗/parenleftbig
β·(r−r′)/parenrightbig
(6)= inf
r′∈RA×Ssup
µ∈M⟨µ,r′⟩+1
βΩ∗/parenleftbig
β·(r−r′)/parenrightbig
(97)
where (1)applies the definition of the conjugate of (−RL Ω,β)∗,(2)reparameterizes the optimization in terms
ofr′→−r′,(3)is the conjugate for (−RL Ω,β), and (4)uses the definition of the regularized RL objective
for occupancy measure µ′(a,s)with the reward r(a,s). Finally, (5)recognizes the inner maximization as
the conjugate function for a modified reward and (6)swaps the order of infandsupassuming the problem
is feasible.
Note that Eq. (97) is a standard unregularized rlproblem with modified reward r′(a,s). As in Sec. 2,
introducing Lagrange multipliers V(s)to enforce the flow constraints and λ(a,s)for the nonnegativity
constraint,
RLΩ,β(r) = inf
r′inf
V,λsup
µ/angbracketleftbig
µ,r′+γEs′
a,s/bracketleftbig
V/bracketrightbig
−V+λ/angbracketrightbig
+1
βΩ∗/parenleftbig
β·(r−r′)/parenrightbig
+ (1−γ)/angbracketleftbig
ν0,V/angbracketrightbig
(98)
Now, eliminating µ(a,s)yields the condition
r′(a,s) +γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
−V(s) +λ(a,s) = 0 =⇒V(s) =r′(a,s) +γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
+λ(a,s).(99)
Letting ∆rV(a,s) =r(a,s)−r′(a,s), we can consider Eq. (99) as a constraint and rewrite Eq. (98) as
RLΩ,β(r) = inf
∆rVinf
V,λ(1−γ)/angbracketleftbig
ν0,V/angbracketrightbig
+1
βΩ∗/parenleftbig
β·∆rV/parenrightbig
(100)
subj. toV(s) =r(a,s) +γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
−∆rV(a,s) +λ(a,s)
which matches Theorem 1. See Husain et al. (2021) for additional detail.
See App. A.3 for the proof of Prop. 3, which equates the form of ∆rV(a,s)and∆rπ(a,s)at optimality in
the regularized mdp.
E.2 Path Consistency (Comparison with Nachum et al. (2017); Chow et al. (2018))
We have seen in Sec. 3.3 and App. E.2 that the path consistency conditions arise from the kktconditions.
Forkldivergence regularization, Nachum et al. (2017) observe the optimal policy π∗(a|s)and valueV∗(s)
33Published in Transactions on Machine Learning Research (07/2022)
satisfy
r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−1
βlogπ∗(a|s)
π0(a|s)=V∗(s), (101)
wheretheLagrangemultiplier λ∗(a,s)isnotnecessarysincethe π∗(a|s)>0unlessπ0(a|s) = 0ortherewards
or values are infinite. This matches our condition in Eq. (39), where we can also recognize ∆rπ∗(a,s) =
1
βlogπ∗(a|s)
π0(a|s)=r(a,s)+γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
−V∗(s)−λ∗(a,s)as the identity from Prop. 3. Nachum et al. (2017) use
Eq. (101) to derive a learning objective, with the squared error Ea,s,s′/bracketleftbig/parenleftbig
r(a,s)+γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
−1
βlogπ(a|s)
π0(a|s)−
V(s)/parenrightbig2/bracketrightbig
used as a loss for learning π(a|s)andV(s)(or simplyQ(a,s), Nachum et al. (2017) Sec. 5.1) using
function approximation.
Similarly, Chow et al. (2018) consider a (scaled) Tsallis entropy regularizer,1
βΩ(π) =1
β1
α(α−1)(/summationtext
aπ(a|s)−/summationtext
aπ(a|s)α). Forα= 2, the optimal policy and value function satisfy
r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
+λ(a,s) +1
β1
α(α−1)−1
β1
α−1π(a|s)α−1=V∗(s) + Λ(s) (102)
where Λ(s)is a Lagrange multiplier whose value is learned in Chow et al. (2018). However, inspecting
the proof of Theorem 3 in Chow et al. (2018), we see that this multiplier is obtained via the identity
Λ(s) :=ψQ∗(s;β)−V∗(s) =ψ∆rπ∗(s;β)(see Eq. (23), App. C.3). Our notation in Eq. (102) differs from
Chow et al. (2018) in that we use1
βas the regularization strength (compared with their α). We have also
written Eq. (102) to explicitly include the constant factors appearing in the α-divergence.
In generalizing the path consistency equations, we will consider the α-divergence, with Ω(π) =1
α(α−1)((1−
α)/summationtext
aπ0(a|s) +α/summationtext
aπ(a|s)−/summationtext
aπ0(a|s)1−απ(a|s)α). Note that this includes an additional αfactor which
multipliesthe/summationtext
aπ(a|s)term,comparedtotheTsallisentropyconsideredinChowetal.(2018). Inparticular,
this scaling will change the1
β1
α(α−1)additive constant term in Eq. (102), to a term of1
β1
α−1.
Ourexpressionfor α-divergencepathconsistency, derivedusingtheidentity r(a,s)+γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
+λ∗(a,s)−
1
βlogαπ∗(a|s)
π0(a|s)=V∗(s) +ψ∆rπ∗(s;β)in Eq. (39), becomes
r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
+λ(a,s)−1
βlogαπ∗(a|s)
π0(a|s)=V∗(s) +ψ∆rπ∗(s;β) (103)
where we have rearranged terms from Eq. (28) in the main text to compare with Eq. (102). Note that we can
recognize ∆rπ(a,s) =1
βlogαπ∗(a|s)
π0(a|s)+ψ∆rπ∗(s;β)and we substitute Λ(s) :=ψQ∗(s;β)−V∗(s) =ψ∆rπ∗(s;β)
compared to Eq. (102).
E.3 Indifference Condition
In the single step setting with kldivergence regularization, Ortega & Lee (2014) argue that the perturbed
reward for the optimal policy is constant with respect to actions
r(a)−∆rπ∗(a) =c∀a∈A, (104)
when ∆rπ∗(a)are obtained using the optimal policy π∗(a|s). Ortega & Lee (2014) highlight that this
is a well-known property of Nash equilibria in game theory where, for the optimal policy and worst-case
adversarial perturbations, the agent obtains equal perturbed reward for each action and thus is indifferent
between them.
In the sequential setting, we can consider Q(a,s) =r(a,s) +γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
as the analogue of the single-step
reward or utility function. Using our advantage function interpretation for the optimal policy in Prop. 3, we
directly obtain an indifference condition for the sequential setting
Q∗(a,s)−∆rπ∗(a,s) =V∗(s) (105)
34Published in Transactions on Machine Learning Research (07/2022)
(a) Environment
(r(a,s) =−1for water,
r(a,s) = 5for goal)
0 1 2 3 4 50
1
2
3
4
5-0.3
-0.2
-0.2
-0.2
-0.2
-0.2-0.2
-0.2
-0.1
-0.2
-0.1
-0.1-0.1
-0.2
0.0
-0.2
0.0
0.10.1
0.4
1.6
0.5
0.2
0.31.0
2.3
2.3
0.9
0.71.5
2.0
3.2
2.0
1.3
1.0-0.3
-0.2
-0.2
-0.2
-0.2
-0.2-0.2
-0.2
-0.1
-0.2
-0.1
-0.1-0.1
-0.2
0.0
-0.2
0.0
0.10.1
0.4
1.6
0.5
0.2
0.31.0
2.3
2.3
0.9
0.71.5
2.0
3.2
2.0
1.3
1.0-0.3
-0.2
-0.2
-0.2
-0.2
-0.2-0.2
-0.2
-0.1
-0.2
-0.1
-0.1-0.1
-0.2
0.0
-0.2
0.0
0.10.1
0.4
1.6
0.5
0.2
0.31.0
2.3
2.3
0.9
0.71.5
2.0
3.2
2.0
1.3
1.0-0.3
-0.2
-0.2
-0.2
-0.2
-0.2-0.2
-0.2
-0.1
-0.2
-0.1
-0.1-0.1
-0.2
0.0
-0.2
0.0
0.10.1
0.4
1.6
0.5
0.2
0.31.0
2.3
2.3
0.9
0.71.5
2.0
3.2
2.0
1.3
1.0
012345(b)β= 0.2(High Reg.)
0 1 2 3 4 50
1
2
3
4
51.5
1.5
1.6
1.5
1.5
1.51.7
1.7
1.7
1.6
1.7
1.71.9
1.8
2.1
1.8
1.9
1.92.2
2.5
3.7
2.4
2.1
2.22.8
3.9
3.9
2.7
2.53.2
3.6
4.3
3.6
3.0
2.81.5
1.5
1.6
1.5
1.5
1.51.7
1.7
1.7
1.6
1.7
1.71.9
1.8
2.1
1.8
1.9
1.92.2
2.5
3.7
2.4
2.1
2.22.8
3.9
3.9
2.7
2.53.2
3.6
4.3
3.6
3.0
2.81.5
1.5
1.6
1.5
1.5
1.51.7
1.7
1.7
1.6
1.7
1.71.9
1.8
2.1
1.8
1.9
1.92.2
2.5
3.7
2.4
2.1
2.22.8
3.9
3.9
2.7
2.53.2
3.6
4.3
3.6
3.0
2.81.5
1.5
1.6
1.5
1.5
1.51.7
1.7
1.7
1.6
1.7
1.71.9
1.8
2.1
1.8
1.9
1.92.2
2.5
3.7
2.4
2.1
2.22.8
3.9
3.9
2.7
2.53.2
3.6
4.3
3.6
3.0
2.8
012345 (c)β= 1
0 1 2 3 4 50
1
2
3
4
53.8
3.8
3.7
3.7
3.8
3.84.0
3.9
3.8
3.8
3.9
3.94.1
3.9
3.8
3.9
4.1
4.14.3
4.1
4.9
4.0
4.2
4.24.4
4.9
4.9
4.4
4.34.6
4.7
4.9
4.7
4.6
4.43.8
3.8
3.7
3.7
3.8
3.84.0
3.9
3.8
3.8
3.9
3.94.1
3.9
3.8
3.9
4.1
4.14.3
4.1
4.9
4.0
4.2
4.24.4
4.9
4.9
4.4
4.34.6
4.7
4.9
4.7
4.6
4.43.8
3.8
3.7
3.7
3.8
3.84.0
3.9
3.8
3.8
3.9
3.94.1
3.9
3.8
3.9
4.1
4.14.3
4.1
4.9
4.0
4.2
4.24.4
4.9
4.9
4.4
4.34.6
4.7
4.9
4.7
4.6
4.43.8
3.8
3.7
3.7
3.8
3.84.0
3.9
3.8
3.8
3.9
3.94.1
3.9
3.8
3.9
4.1
4.14.3
4.1
4.9
4.0
4.2
4.24.4
4.9
4.9
4.4
4.34.6
4.7
4.9
4.7
4.6
4.4
012345 (d)β= 10(Low Reg.)
Figure 8: Confirming Optimality of the Policy. We show the perturbed rewards r′(a,s) =Q(a,s)−
∆r(a,s)for policies trained with kldivergence regularization. The indifference condition holds in all cases,
withr′(a,s) =c(s)for each state-action pair, with c(s) =V(s)forklregularization. This confirms that the
policy is optimal (Ortega & Lee, 2014; Nachum et al., 2017).
for actions with λ∗(a,s) = 0and nonzero probability under π∗(a|s). Observe that V∗(s)is a constant with
respect toa∈Afor a given state s∈S. Recall that our notation for V∗(s)omits its dependence on βandα.
This indifference condition indeed holds for arbitrary regularization strengths βand choices of α-divergence,
since our proof of the advantage function interpretation in App. A.3 is general. Finally, we emphasize that
the indifference condition holds only for the optimal policy with a given reward r(a,s)(see Fig. 3).
E.4 Confirming Optimality using Path Consistency and Indifference
In Fig. 4, we plotted the regularized policies and worst-case reward perturbations for various regularziation
strengthβandkldivergence regularization. In Fig. 8, we now seek to certify the optimality of each policy
using the path consistency or indifference conditions. In particular, we confirm the following equality holds
r(a,s) +γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
−1
βlogπ(a|s)
π0(a|s)=V(s)∀(a,s)∈A×S (106)
which aligns with the path consistency condition in (Nachum et al., 2017). Compared with Eq. (28)
or Eq. (103), Eq. (106) uses the fact that λ(a,s) = 0andψ∆r(s;β) = 0in the case of kldiver-
gence regularization. This equality also confirms the indifference condition since the right hand side
V∗(s)is a constant with respect to actions. Finally, we can recognize our advantage function interpre-
tationQ∗(a,s)−∆rπ∗(a,s) =V∗(s)in Eq. (106), by substituting Q∗(a,s) =r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
and
∆rπ∗(a,s) =1
βlogπ∗(a|s)
π0(a|s)forkldivergence regularization.
In Fig. 8, we plot r(a,s) +γEs′
a,s/bracketleftbig
V(s′)/bracketrightbig
−1
βlogπ(a|s)
π0(a|s)=Q(a,s)−∆rπ(a,s)for each state-action pair to
confirm that it yields a constant value and conclude that the policy and values are optimal. Note that
this constant is different across states based on the soft value function V∗(s), which also depends on the
regularization strength.
F Comparing Entropy and Divergence Regularization
In this section, we provide proofs and discussion to support our observations in Section 5.1 on the benefits
of divergence regularization over entropy regularization.
F.1 Tsallis Entropy and α-Divergence
To show a relationship between the Tsallis entropy and the α-divergence, we first recall the definition of the
q-exponential function logq(Tsallis, 2009). We also define logα(u), withα= 2−q, so that our use of logα(u)
35Published in Transactions on Machine Learning Research (07/2022)
matches Lee et al. (2019) Eq. (5)
logq(u) =1
1−q/parenleftbigg
u1−q−1/parenrightbigg
logα(u):= log2−q(u) =1
α−1/parenleftbigg
uα−1−1/parenrightbigg
(107)
The Tsallis entropy of order q(Tsallis, 2009; Naudts, 2011) can be expressed using either logqorlogα
HT
q[π(a)] =1
q−1/parenleftbigg
1−/summationdisplay
a∈Aπ(a)q/parenrightbigg
=/summationdisplay
a∈Aπ(a) logq/parenleftbig1
π(a)/parenrightbig
(108)
=−/summationdisplay
a∈Aπ(a)·log2−q/parenleftbig
π(a)/parenrightbig
(109)
Eq. (108) and Eq. (109) mirror the two equivalent ways of writing the Shannon entropy for q= 1. In
particular, we have q= 2−qandH1[π(a)] =/summationtextπ(a) log1
π(a)=−/summationtextπ(a) logπ(a). See Naudts (2011) Ch. 7
for discussion of these two forms of the deformed logarithm.
To connect the Tsallis entropy and the α-divergence in Eq. (7), we can consider a uniform reference measure
π0(a) = 1∀a. For normalized/summationtext
aπ(a) = 1,
Dα[π0(a) :π(a)] =1
α(1−α)/parenleftigg
(1−α)/summationdisplay
a∈Aπ0(a) +α/summationdisplay
a∈Aπ(a)−/summationdisplay
a∈Aπ0(a)1−απ(a|s)α/parenrightigg
(110)
=1
α(1−α)/parenleftigg
α+(1−α)−/summationdisplay
a∈Aπ(a|s)α/parenrightigg
+1
α(1−α)/parenleftigg
(1−α)/summationdisplay
a∈Aπ0(a)−(1−α)/parenrightigg
(111)
=−1
αHT
α[π(a)] +c (112)
which recovers the negative Tsallis entropy of order α, up to an multiplicative factor1
αand additive constant.
Note that including this constant factor via α-divergence regularization allows us to avoid an inconvenient
1
αfactor in optimal policy solutions (Eq. (27)) compared with Eq. 8 and 10 of Lee et al. (2019).
F.2 Non-Positivity of ∆rπ(a,s)for Entropy Regularization
We first derive the worst-case reward perturbations for entropy regularization, before analyzing the sign of
these reward perturbations for various values of αin Prop. 5. In particular, we have ∆rπ(a,s)≤0for entropy
regularization with 0< α≤1, which includes Shannon entropy regularization at α= 1. This implies that
the modified reward r′
π(a,s)≥r(a,s)for all (a,s).
Lemma 4. The worst-case reward perturbations for Tsallis entropy regularization correspond to
∆rπ(a,s) =1
βlogαπ(a|s) +1
β1
α/parenleftbig
1−/summationdisplay
a∈Aπ(a|s)α/parenrightbig
. (113)
with limiting behavior of ∆rπ(a,s) =1
βlogπ(a|s)for Shannon entropy regularization as α→1.
Proof.WecanwritetheTsallisentropyusinganadditionalconstant k,withk=1
αmirroringthe α-divergence
HT
α[π] =k
α−1/parenleftbigg/summationdisplay
a∈Aπ(a|s)−/summationdisplay
a∈Aπ(a|s)α/parenrightbigg
. (114)
Note that we use negative Tsallis entropy for regularization since the entropy is concave. Thus, the worst
case reward perturbations correspond to the condition ∆rπ(a,s) =∇1
βΩ(Hα)
π0(µ) =−∇Eµ(s)/bracketleftbig
HT
α[π]/bracketrightbig
. Differ-
entiating with respect to µ(a,s)using similar steps as in App. B.3 Eq. (59)-(60), we obtain
∆rπ(a,s) =k·1
βαlogαπ(a|s) +k·1
β(α−1)HT
α[π] (115)
Fork=1
αand(α−1)HT
α[π] = (/summationtext
aπ(a|s)−/summationtext
aπ(a|s)α), we obtain Eq. (113).
36Published in Transactions on Machine Learning Research (07/2022)
Proposition 5. For0< α≤1andβ > 0, the worst-case reward perturbations with Tsallis entropy
regularization from Lemma 4 are non-positive, with ∆rπ(a,s)≤0. This implies that the entropy-regularized
policy is robust to only pointwise reward increases for these values of α.
Proof.We first show logαπ(a|s)≤0for0<π(a|s)≤1and anyα. Note that we may write
logαπ(a|s) =/integraldisplayπ(a|s)
1uα−2du=1
α−1uα−1/vextendsingle/vextendsingle/vextendsingle/vextendsingleπ(a|s)
1=1
α−1(π(a|s)α−1−1). (116)
Sinceuα−2is a non-negative function for 0≤π(a|s)≤1, then/integraltextπ(a|s)
1uα−2du≤0.
To analyze the second term, consider 0< α≤1. We know that π(a|s)α≥π(a|s)for0< π(a|s)≤1,
so that/summationtext
aπ(a|s)α≥/summationtext
aπ(a|s) = 1. Thus, we have/summationtext
aπ(a|s)−/summationtext
aπ(a|s)α≤0andα > 0implies
1
α(/summationtext
aπ(a|s)−/summationtext
aπ(a|s)α)≤0. Since both terms are non-positive, we have ∆rπ(a,s)≤0for0<α≤1as
desired.
However, for α >1orα <0, we cannot guarantee the reward perturbations are non-positive. Writing the
second term in Eq. (113) asα−1
αHT
α[π], we first observe that that HT
α[π]≥0. Now,α>1orα<0implies
thatα−1
α>0, so that the second term is non-negative, compared to the first term, which is non-positive.
F.3 Bounding the Conjugate Function
Conjugate for a Fixed α,β:We follow similar derivations as Lee et al. (2019) to bound the value function
for general α-divergence regularization instead of entropy regularization. We are interested in providing a
bound for1
βΩ∗(α)
π0,β(Q)with fixedα,β. To upper bound the conjugate, consider the optimum over each term
separately
1
βΩ∗(α)
π0,β(Q) = max
π∈∆|A|/angbracketleftbig
π,Q/angbracketrightbig
−1
βDα[π0:π]
≤max
π∈∆|A|/angbracketleftbig
π,Q/angbracketrightbig
−min
π1
βDα[π0:π]
= max
π∈∆|A|/angbracketleftbig
π,Q/angbracketrightbig
−0
=Q(amax,s).
where we let amax:= arg maxaQ(a,s).
We can also lower bound the conjugate function in terms of maxaQ(a,s). Noting that any policy π(a|s)
provides a lower bound on the value of the maximization objective, we consider πmax(a|s) =δ(a=amax).
For evaluating πmax(a|s)α, we assume 0α= 0forα > 0and undefined otherwise. Thus, we restrict our
attention to α>0to derive the lower bound
1
βΩ∗(α)
π0,β(Q) = max
π∈∆|A|/angbracketleftbig
π,Q/angbracketrightbig
−1
βDα[π0:π]
≥/angbracketleftbig
πmax,Q/angbracketrightbig
−1
βDα[π0:πmax]
(1)=Q(amax,s)−1
β/parenleftig1
α(1−α)−1
α(1−α)π0(amax|s)1−α1α/parenrightig
=Q(amax,s) +1
β1
α1
1−α/parenleftbig
π0(amax|s)1−α−1/parenrightbig
=Q(amax,s) +1
β1
αlog2−απ0(amax|s).
where (1)usesπmax(a|s) =δ(a=amax)and simplifies terms in the α-divergence. One can confirm that
1
αlog2−απ0(amax|s) =1
α1
1−α(π0(amax|s)1−α−1)≤0forα>0. Combining these bounds, we can write
Forα>0,Q(amax,s) +1
β1
αlog2−απ0(amax|s)≤1
βΩ∗(α)
π0,β(Q)≤Q(amax,s).(117)
37Published in Transactions on Machine Learning Research (07/2022)
(a) Fig. 2 or Fig. 9 (left) of
Eysenbach & Levine (2021)
Constraint:/summationtext
aexp{β∆r(a)}≤1
(b) Shifted r′=r−∆r−ln(2)
Constraint:/summationtext
aexp{β∆r(a)}≤1
(c) With prior π0(a|s) =u(a) =1
2
Constraint/summationtext
aπ0(a) exp{β∆r(a)}≤1
Figure 9: Analyzing the feasible set for entropy regularization (a) versus divergence regularization (b,c).
Conjugate as a Function of β:Finally, we can analyze the conjugate as a function of β. Asβ→0
and 1/β→ ∞, the divergence regularization forces π(a|s) =π0(a|s)for anyαand the conjugate
1
βΩ∗(α)
π0,β(Q) =⟨π(a|s),Q(a,s)⟩−1
βΩ(π)→⟨π(a|s),Q(a,s)⟩. Asβ→∞and1/β→0, the unregularized
objective yields a deterministic optimal policy with π(a|s) = max aQ(a,s). In this case, the conjugate
1
βΩ∗(α)
π0,β(Q)→maxaQ(a,s). Thus, treating the conjugate as a function of β, we obtain
Eπ0(a|s)[Q(a,s)]≤1
βΩ∗(α)
π0,β(Q)≤max
aQ(a,s) (118)
For negative values of β, the optimization becomes a minimization, with the optimal solution approaching a
deterministic policy with π(a|s) = minaQ(a,s)asβ→−∞,1/β→0.
F.4 Feasible Set for Entropy Regularization
In this section, we compare feasible sets derived from entropy regularization (Fig. 9a) with those derived
from the kldivergence (Fig. 2, Fig. 9b,Fig. 9c), and argue that entropy regularization should be analyzed
as a special case of the kldivergence to avoid misleading conclusions.
IntheirApp. A8,Eysenbach&Levine(2021)makethesurprisingconclusionthatpolicieswith higherentropy
regularization are lessrobust, as they lead to smaller feasible or robust sets than for lower regularization.
This can be seen in the robust set plot for entropy regularization in Fig. 9a, where higher βindicates lower
regularization strength ( 1/β). The left panels in Eysenbach & Levine (2021) Fig. 2 or Fig. 9 match our
Fig. 9a. See their App. A8 for additional discussion.
To translate from Shannon entropy to kldivergence regularization, we include an additional additive con-
stant of1
βlog|A|corresponding to the (scaled) entropy of the uniform distribution, with −1
βDKL[π:π0] =
1
βH(π)−1
βlog|A|. We obtain Fig. 9b by shifting each curve in Fig. 9a by this scaled constant . This
highlights the delicate dependence on the feasible set on the exact form of the objective function, as the
constant shifts the robust reward set by (r′(a1),r′(a2))←(r′(a1)−1
βlog 2,r′(a2)−1
βlog 2). For kldiver-
gence regularization, the feasible set now includes the original reward function. As expected, we can see that
policies with higher regularization strength are morerobust with largerfeasible sets.
An alternative approach to correct the constraint set is to include the uniform reference distribution as in
Prop. 1 and Eq. (19), so that we calculate/summationtext
aπ0(a|s) exp/braceleftbig
β·∆r/bracerightbig
≤1. Similarly, the constraint in Eysen-
bach & Levine (2021) can be modified to have/summationtext
aexp/braceleftbig
β·∆r/bracerightbig
≤|A|in the case of entropy regularization.
Our modifications to the feasible set constraints clarify interpretations of how changing regularization
strength affects robustness. We do not give detailed consideration to the other question briefly posed in
Eysenbach & Levine (2021) App. A8, of ‘if a reward function r′is included in a robust set, what other reward
38Published in Transactions on Machine Learning Research (07/2022)
functions are included in that robust set?’, whose solution is visualized in Eysenbach & Levine (2021) Fig. 9
(right panel) without detailed derivations. However, this plot matches our Fig. 9b and 9c. This suggests that
the constraint arising from kldivergence regularization with strength β,/summationtextπ0(a|s) exp/braceleftbig
β·∆r(a,s)/bracerightbig
= 1,
is sufficient to define both the robust set for a given reward function and to characterize the other reward
functions to which an optimal policy is robust. The key observation is that the original reward function
is included in the robust set when explicitly including the reference distribution π0(a|s)as in divergence
regularization.
G Worked Example for Deterministic Regularized Policy ( α= 2,β= 10)
We consider the single-step example in Sec. 4.1 Fig. 2 or App. H Fig. 10-11, with a two-dimensional action
space, optimal state-action value estimates, Q∗(a,s) =r(a,s) ={1.1,0.8}, and uniform prior π0(a|s).
The case of policy regularization with α= 2andβ= 10is particularly interesting, since the optimal policy is
deterministic with π∗(a1|s) = 1.6First, we solve for the optimal policy for Q∗(a,s) =r(a,s)as in App. C.2,
1
βΩ∗
π0,β(Q∗) = max
π∈∆|A|/angbracketleftbig
π,Q∗/angbracketrightbig
−1
βΩ(α)
π0(π)−ψQ(s;β)/parenleftigg/summationdisplay
aπ(a|s)−1/parenrightigg
+/summationdisplay
aλ(a,s)
=⇒π∗(a|s) =π0(a|s)/bracketleftig
1 +β(α−1)/parenleftbig
Q∗(a,s) +λ∗(a,s)−V∗(s)−ψ∆rπ∗(s;β)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=ψQ∗(s;β)(see App. C.3 )/parenrightbig/bracketrightig1
α−1.
where forα= 2, we obtain π∗(a|s) =π0(a|s)/bracketleftbig
1 +β/parenleftbig
Q∗(a,s) +λ∗(a,s)−V∗(s)−ψ∆rπ∗(s;β)/parenrightbig/bracketrightbig
.
Using cvx-py(Diamond & Boyd, 2016) to solve this optimization for α= 2,β= 10,π0(a|s) =1
2∀a, and
the givenQ∗(a,s), we obtain
Q∗(a1,s) = 1.1 Q∗(a2,s) = 0.8 λ∗(a1,s) = 0
π∗(a1|s) = 1 π∗(a2|s) = 0 λ∗(a2,s) = 0.1
V∗(s) = 1.05 ψ∆rπ∗(s;β) =−0.05 ψQ∗(s;β) = 1.0. (119)
Our first observation is that, although the policy is deterministic with π∗(a1|s) = 1, the value function
V∗(s) = 1.05is not equal to maxaQ∗(a,s) = 1.1as it would be in the case of an unregularized policy.
Instead, we still need to subtract the α-divergence regularization term, which is nonzero. With α= 2and
1−α=−1, we have
V∗(s) =⟨π∗(a|s),Q∗(a,s)⟩−1
β1
α1
1−α/parenleftigg
1−/summationdisplay
aπ0(a|s)1−απ∗(a|s)α/parenrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
1
βDα[π0:π∗]= 1.1−1
101
21
−1/parenleftbig
1−.5−1·12−.5−1·02/parenrightbig
= 1.1 +.05·(1−2) = 1.05
Recall that for normalized π0,π, we haveψ∆rπ∗(s;β) =−1
β(1−α)Dα[π0(a|s) :π∗(a|s)] =−0.05, so that we
can confirm Eq. (119) for α= 2.
Finally, we confirm the result of Prop. 3 by calculating the reward perturbations in two different ways. For
a1, we have
∆rπ∗(a1,s) =1
β1
α−1/parenleftbigg
π∗(a1|s)
π0(a1|s)α−1
−1/parenrightbigg
+ψ∆rπ∗(s;β) =1
101
1/parenleftbigg
1
.51
−1/parenrightbigg
−.05 =.05
=Q∗(a1,s)−V∗(s) +λ∗(a1,s) = 1.1−1.05 + 0 =.05,
and fora2,
∆rπ∗(a2,s) =1
β1
α−1/parenleftbigg
π∗(a2|s)
π0(a2|s)α−1
−1/parenrightbigg
+ψ∆rπ∗(s;β) =1
101
1/parenleftbigg
0
.51
−1/parenrightbigg
−.05 =−.15
=Q∗(a2,s)−V∗(s) +λ∗(a2,s) = 0.8−1.05 + 0.1 =−.15
6We useα= 2instead ofα= 3in Fig. 2 for simplicity of calculations. See App. H Fig. 10 for α= 2robust set plots.
39Published in Transactions on Machine Learning Research (07/2022)
so that we have ∆rπ∗(a1,s) = 0.05and∆rπ∗(a2,s) =−0.15.
We can observe that the indifference condition does not hold, sinceQ∗(a1,s)−∆rπ∗(a1,s) = 1.1−0.05 = 1.05
does not match Q∗(a2,s)−∆rπ∗(a2,s) = 0.8−(−0.15) = 0.95.
However, adding the Lagrange multiplier λ∗(a2,s) = 0.1accounts for the difference in these values. This
allows us to confirm the path consistency condition (Eq. (28)),
r(a,s) +γEs′
a,s/bracketleftbig
V∗(s′)/bracketrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Q∗(a,s)−1
βlogαπ∗(a|s)
π0(a|s)−ψ∆rπ∗(s;β)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
∆rπ∗(a,s)=V∗(s)−λ∗(a,s)∀(a,s)∈A×S (120)
withQ∗(a1,s)−∆rπ∗(a1,s)−V∗(s) +λ∗(a1,s) = 1.1−0.05−1.05 + 0 = 0 andQ∗(a2,s)−∆rπ∗(a2,s)−
V∗(s) +λ∗(a2,s) = 0.8−(−0.15)−1.05 + 0.1 = 0.
H Additional Feasible Set Plots
In Fig. 10 and 11, we provide additional feasible set plots for the α-divergence with α∈{− 1,1,2,3}and
β∈{0.1,1.0,5,10}withr(a,s) = [1.1,0.8]. As in Fig. 2, we show the feasible set corresponding to the
single-step optimal policy for Q∗(a,s) =r(a,s)for various regularization schemes. Since each policy is
optimal, we can confirm the indifference condition for the kldivergence, with Q∗(a,s)−∆rπ∗(a,s) =V∗(s)
constant across actions and equal to the soft value function, certainty equivalent, or conjugate function
V∗(s) =1
βΩ∗(α)
π0,β(Q). When the indifference condition holds, we can obtain the ratio of action probabilities
in the regularized policy by taking the slope of the tangent line to the feasible set boundary at r′
π∗(a,s), as
in Sec. 4.1.
However, indifference does not hold in cases where the optimal policy sets π∗(a2|s) = 0, which occurs for
(α= 2,β= 10),(α= 3,β∈{5,10}) for a uniform reference policy and additionally for ( α= 2,β= 5) with
the nonuniform reference in Fig. 11. In these cases, we cannot ignore the Lagrange multiplier in Eq. (28),
r′
π∗(a,s) =Q∗(a,s)−∆rπ∗(a,s) =V∗(s)−λ∗(a,s), andλ∗(a2,s)>0results in a different perturbed reward
r′
π∗(a1,s)̸=r′
π∗(a2,s).
Forα=−1and low regularization strength ( β= 10), we observe a wider feasible set boundary than for kl
divergence regularization. For α= 2andα= 3, the boundary is more restricted and the worst-case reward
perturbations become notably smaller when the policy is deterministic. For example, we can compare β= 5
versusβ= 10forα= 2. However, as in Fig. 7, we do not observe notable differences in the robust sets at
lower regularization strengths based on the choice of α-divergence.
40Published in Transactions on Machine Learning Research (07/2022)
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case
(a)α=−1,β= 0.1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case(b)α=−1,β= 1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (c)α=−1,β= 5
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (d)α=−1,β= 10
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Feasible Set
Q(s,a)
Perturbed
(e)DKL,β= 0.1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Feasible Set
Q(s,a)
Perturbed (f)DKL,β= 1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (g)DKL,β= 5
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Feasible Set
Q(s,a)
Perturbed (h)DKL,β= 10
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case
(i)α= 2,β= 0.1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (j)α= 2,β= 1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (k)α= 2,β= 5
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (l)α= 2,β= 10
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case
(m)α= 3,β= 0.1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case(n)α= 3,β= 1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (o)α= 3,β= 5
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (p)α= 3,β= 10
Figure 10: Reference distribution π0= (1
2,1
2). See caption of Fig. 11.
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case
(a)α=−1,β= 0.1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case(b)α=−1,β= 1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (c)α=−1,β= 5
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (d)α=−1,β= 10
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Feasible Set
Q(s,a)
Perturbed
(e)DKL,β= 0.1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Feasible Set
Q(s,a)
Perturbed (f)DKL,β= 1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (g)DKL,β= 5
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Feasible Set
Q(s,a)
Perturbed (h)DKL,β= 10
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case
(i)α= 2,β= 0.1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (j)α= 2,β= 1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (k)α= 2,β= 5
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (l)α= 2,β= 10
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case
(m)α= 3,β= 0.1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case(n)α= 3,β= 1
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (o)α= 3,β= 5
0.6 0.8 1.0 1.2 1.4 1.6
r/prime(a1,s)0.50.60.70.80.91.01.11.2r/prime(a2,s)
Robust Set
Q(s,a)
Worst-Case (p)α= 3,β= 10
Figure 11: Reference distribution π0= (2
3,1
3). Feasible Set (red region) of perturbed rewards available
to the adversary, for kl(α= 1) andα-divergence ( α={−1,2,3}) regularization, various β, and fixed
Q∗(a,s) =r(a,s)values (blue star). We consider the optimal π∗(a|s)with regularization parameters α,β,π 0
and the given Q-values. Red star indicates worst-case perturbed reward r′
π∗=r−∆rπ∗for optimal policy.
41