Under review as submission to TMLR
Policy Optimization with Distributional Constraints:
An Optimal Transport View
Anonymous authors
Paper under double-blind review
Abstract
We consider constrained policy optimization in Reinforcement Learning, where the con-
straints are in form of marginals on state visitations and global action executions. Given
these distributions, we formulate policy optimization as unbalanced optimal transport over
the space of occupancy measures. We propose a general purpose RL objective based on
Bregman divergence and optimize it using Dykstra’s algorithm when the transition model
is known. The approach admits an actor-critic algorithm for when the state or action space
is large, and only samples from the marginals are available. We discuss applications of our
approach and provide demonstrations to show the eﬀectiveness of our algorithm.
1 Introduction
In reinforcement learning (RL), policy optimization seeks an optimal decision making strategy, known as a
policy (Bertsekas, 1995; Williams, 1992; Sutton et al., 2000). Policies are typically optimized in terms of
accumulated rewards with or without constraints on actions and/or states associated with an environment
(Altman, 1999).
Policy optimization has many challenges; perhaps the most basic is the constraint on ﬂow of state-action
visitations called occupancy measures . Indeed, formulating RL as a linear programming problem, occupancy
measures appear as an explicit constraint on the optimal policy (Puterman, 2014). The constraint-based
formulation suggests the possibility of implementing a broader set of objectives and constraints, such as
entropy regularization (Peters et al., 2010; Neu et al., 2017; Nachum & Dai, 2020) and cost-constrained
MDPs (Altman, 1999).
Considering the reward function as negative cost of assigning an action to a state, we view RL as a stochastic
assignment problem. We formulate policy optimization as an unbalanced optimal transport on the set
of occupancy measures. Where Optimal Transport (OT) (Villani, 2008) is the problem of adapting two
distributions on possibly diﬀerent spaces via a cost function, unbalanced OT relaxes the marginal constraints
on OT to arbitrary measures through penalty functions (Liero et al., 2017; Chizat et al., 2017).
We therefore deﬁne distributionally-constrained reinforcement learning as a problem of optimal transport.
Given baseline marginals over states and actions, policy optimization is unbalanced optimal transport adapt-
ing the state marginal to action marginal via the reward function. Built upon mathematical tools of OT,
we generalize the RL objective to the summation of a Bregman divergence DΓand any number of arbitrary
lower-semicontinuous convex functions φ/prime
is and an initial distribution ξas
min
µ∈∆DΓ(µ|ξ) +N/summationdisplay
iφi(µ),
where ∆is the set of occupancy measures (joint distributions on states and actions generated from policies).
Weoptimizethisobjectivewith Dykstra’s algorithm (Dykstra,1983)whichisamethodofiterativeprojections
onto general closed convex constraint sets. Under Fenchel duality, this algorithm allows decomposition of
the objective into Bregman projections on the subsets corresponding to each function.
1Under review as submission to TMLR
As particular case, we can regularize over the state space distribution and/or the global action execution
distribution of the desired occupancy measures. Given the reward function rand divergence functions Dψ1
andDψ2, our formulation allows constraints on the policy optimization problem in terms of distributions on
state visitations ρ/primeand/or action executions η/primeas:
max
µ∈∆Eµ[r]−/epsilon11Dψ1(µ1|ρ/prime)−/epsilon12Dψ2/parenleftbig
µT1|η/prime/parenrightbig
.
Marginal constraints of ρ/primeandη/primecan arise in various settings in policy optimization: problem constraints of
budget on actions or safety constraints on state visitations (Altman, 1999; Zhang et al., 2020a), empirical
state and action distributions from an expert demonstrations or for regularization purposes in an iterative
policy optimization algorithm (Neu et al., 2017)
We also propose an actor-critic algorithm with function approximation for large scale RL, for when we have
access to samples from a baseline policy (oﬀ-policy sampling or imitation learning) and samples from the
constraint marginals.
The structure of the paper is as follows: In the Section 2 we brieﬂy present the preliminaries on (unbalanced)
optimal transport and policy optimization in reinforcement learning. In Section 3 we introduce a general
objective with Bregman divergence for policy optimization and provide Dykstra iterations as a general primal
algorithm for optimizing this objective. Section 4 discusses distributionally constrained policy optimization
with unbalanced OT and its applications. In this section, we also provide an actor critic algorithm for large
scale RL. We conclude with demonstrations of the distributional constraints in Section 5 and discussion on
related works in Section 6.
2 Notation and Preliminaries
For any ﬁnite set X, letM(X)be the set of probability distributions on X. We denote the indicator
function on setXbyδX(x) =/braceleftBigg
0ifx∈X
∞otherwise. Forp∈M (X), we deﬁne the entropymapH(p) =
Ex∼p[logp(x)−1], and denote the Kullback-Leibler (KL) divergence between two positive functions p,qby
KL(p|q) =/summationtext
x∈Xp(x) log/parenleftBig
p(x)
q(x)/parenrightBig
−p(x) +q(x). Ifp,q∈M (X), for a given convex function ψ:X →R
withψ(1) = 0, we deﬁne ψ-divergence: Dψ(p|q) =/summationtext
xψ/parenleftBig
p(x)
q(x)/parenrightBig
q(x). In particular, for ψ(x) =xlog(x),
Dψ(p|q) =KL(p|q). We also use/angbracketleft·,·/angbracketrightas the natural inner product on X. Through out the paper by 1Xwe
denote a vector with elements one over set Xor just 1if the context is clear.
2.1 Optimal Transport
Givenmeasures a∈M (X),b∈M (Y)ontwosetsXandY, withacostfunction C:X×Y→ R, Kantorovich
Optimal Transportation is the problem of ﬁnding stochastic optimal plan µ∈M (X×Y ):
min
µEµ[C(x,y)] +δ{a}(µ1Y) +δ{b}/parenleftbig
µT1X/parenrightbig
.
WhenX=YandCis derived from a metric on X, this optimization deﬁnes a distance function on measure
spaceM(X), called Wasserstein distance (Villani, 2008).
Unbalanced Optimal Transport replaces hard constraints δ{a}andδ{b}, with penalty functions
min
µEµ[C(x,y)] +/epsilon11Dψ1(µ1|a) +/epsilon12Dψ2(µT1|b),
where/epsilon11,/epsilon12are positive scalars. This formulation also extends OT to measures of arbitrary mass. As
/epsilon11,/epsilon12→∞, the unbalanced OT approaches Kantorovich OT problem (Liero et al., 2017; Chizat et al., 2017).
To speed up the daunting computational costs of standard algorithms, an entropy term H(µ)is usually
added to the (U)OT objective to apply scaling algorithms (Cuturi, 2013; Chizat et al., 2017). When Xand
2Under review as submission to TMLR
Yare large or continuous spaces, we usually have access to samples from a,binstead of the actual measures.
Stochastic approaches usually add a relative entropy KL (µ|a⊗b), instead ofH(µ)in order to take advantage
of the Fenchel dual of the (U)OT optimization and estimate the objective from samples out of a,b(Aude
et al., 2016; Seguy et al., 2018).
2.2 Reinforcement Learning
Consider a discounted MDP (S,A,P,r,γ,p 0), with ﬁnite state space S, ﬁnite action space A, transition
modelP:S×A→M (S), initial distribution p0∈M (S), deterministic reward function r:S×A→ R
and discount factor γ∈[0,1). Letting Πbe the set of stationary policies on the MDP, for any policy
π:S→M (A), we denote Pπ:S→M (S)to be the induced Markov chain on S. In policy optimization,
the objective is
max
π∈Π/summationdisplay
s,aρπ(s)π(a|s)r(s,a), (1)
whereρπ(s) = (1−γ)/summationtext∞
t=0γtPr(st=s|π)is the discounted stationary distribution of Pπ. For a policy π,
we deﬁne its occupancy measure µπ∈M (S×A ), asµπ(s,a) =ρπ(s)π(a|s). Let ∆ :={µπ:π∈Π}be the
set of occupancy measures of Π, the following lemma bridges the two sets Πand∆:
Lemma 2.1. (Syed et al., 2008)[Theorem 2, Lemma2]
(i)∆ ={µ∈M (S×A ) :/summationtext
aµ(s,a) = (1−γ)p0(s) +γ/summationtext
s/prime,a/primeP(s|s/prime,a/prime)µ(s/prime,a/prime)∀s}
(ii)πµ(a|s) :=µ(s,a)//summationtext
aµ(s,a)is a bijection from ∆toΠ.
By multiplying π=πµto both sides of the equation in (i), one can obtain ∆ ={µ:µ≥0, Aµµ=bµ}
whereAµ=I−γPµTandbµ= (1−γ)πp0. In the rest of paper, we may drop the superscripts µandπ,
when the context is clear. Rewriting the policy optimization objective equation 1 in terms of µ, we get
max
µ∈∆Eµ[r] = max
µEµ[r]−δ{bµ}(Aµµ). (2)
The entropy-regularized version of objective equation 2, relative to a given baseline µ/prime∈∆, is also studied
(Peters et al., 2010; Neu et al., 2017):
max
µ∈∆Eµ[r]−/epsilon1KL(µ|µ/prime)≡min
µ∈∆KL/parenleftBig
µ|µ/primeer//epsilon1/parenrightBig
, (3)
where/epsilon1is the regularization coeﬃcient.
By lemma 2.1, one can decompose the regularization term in equation 3 as
KL(µ|µ/prime) =KL/parenleftBigg/summationdisplay
aµ|/summationdisplay
aµ/prime/parenrightBigg
+Eρµ[KL(π|π/prime)], (4)
with the ﬁrst term penalizing the shift on state distributions and the second penalty is over average shift of
policies for every state. Since the goal is to optimize for the best policy, one might consider only regularizing
relative toπ/primeas in (Schulman et al., 2015; Neu et al., 2017)
max
µ∈∆Eµ[r]−/epsilon1Eρµ[KL(π|π/prime)]. (5)
One can also regularize objective equation 2 by H(µ)as
max
µ∈∆Eµ[r]−/epsilon1H(µ)≡min
µ∈∆KL(µ|er//epsilon1), (6)
which encourages exploration and avoids premature convergence (Haarnoja et al., 2017; Schulman et al.,
2018; Ahmed et al., 2019).
3Under review as submission to TMLR
3 A General RL Objective with Bregman Divergence
Inthis section, weproposeageneralRL objective based onBregmandivergenceand optimizeusingDykstra’s
algorithmwhenthetransitionmodelisknown. Let Γbeastrictlyconvex, smoothfunctiononrelint (dom(Γ)),
the relative interior of its domain, with convex conjugate Γ∗. For any (µ,ξ)∈dom(Γ)×int(dom(Γ)), we
deﬁne the Bregman divergence
DΓ(µ|ξ) = Γ(µ)−Γ(ξ)−/angbracketleft∇ Γ(ξ),µ−ξ/angbracketright.
Givenξ, we consider the optimization
min
µDΓ(µ|ξ) +N/summationdisplay
iφi(µ), (7)
whereφ/prime
is are proper, lower-semicontinuous convex functions satisfying
∩N
irelint (dom(φi))∩relint (dom(Γ))/negationslash=∅. (8)
Let dom (Γ)be the simplex on S×A, regularized RL algorithms in Section 2.2 can be observed as instances
of optimization equation 7:
•Given a baseline µ/prime, setting Γ =H,φ1(µ) =δ{bµ}(Aµµ),ξ=µ/primeer//epsilon1, recovers objective equation 3.
•Similarly, as discussed in (Neu et al., 2017) ,Γ(µ) =/summationtext
s,aµ(s,a) logµ(s,a)//summationtext
aµ(s,a),φ1=Eµ[r//epsilon1],
φ2(µ) =δ{bµ}(Aµµ), andξ=µ/primerecovers obj. equation 5.
•Further, Γ =H,φ1(µ) =δ{bµ}(Aµµ), andξ=er//epsilon1, entropy-regularizes the occupancy measure in objective
equation 6.
The motivation behind using Bregman divergence is to generalize the KL divergence regularization usually
used in RL algorithms. Moreover, one may replace the Bregman divergence term in equation 7 with a
ψ-Divergence and attempt deriving similar arguments for the rest of the paper.
3.1 Dykstra’s Algorithm
In this section, we use Dykstra’s algorithm (Dykstra, 1983) optimize objective equation 7. Dykstra is a
method of iterative projections onto general closed convex constraint sets, which is well suited because the
occupancy measure constraint is on a compact convex polytope ∆.
Deﬁning the Proximal map of a convex function φ, with respect to DΓ, as
ProxDΓ
φ(µ) = arg min
˜µDΓ(˜µ|µ) +φ(˜µ),
for anyµ∈dom(Γ), we present the following proposition which is the generalization of Dykstra algorithm
in (Peyré, 2015):
Proposition 3.1 (Dykstra’s algorithm) .1For iteration l>0,
µ(l)=ProxDΓ
φ[l]N/parenleftBig
∇Γ∗/parenleftBig
∇Γ(µ(l−1))/parenrightBig
+ν(l−N)/parenrightBig
(9)
ν(l)=ν(l−N)+∇Γ(µ(l−1))−∇Γ(µ(l)), (10)
with [l]N=/braceleftBigg
N iflmodN= 0
lmodNotherwise,converges to the solution of optimization equation 7, with µ(0)=ξ
andν(i)=0for−N+ 1≤i≤0.
1proofs and derivations are in supplementary material.
4Under review as submission to TMLR
Intuitively, at step l, algorithm equation 9 projects µ(l−1)into the convex constraint set corresponding to
the function φ[l]N.
Corollary 3.2. Taking Γ =H, the iteration equation 9 is
µ(l)=ProxKL
φ[l]N/parenleftbig
µ(l−1)⊙z(l−N)/parenrightbig
, z(l)=z(l−N)⊙µ(l−1)
µ(l), (11)
where⊙,·
·are the element-wise product and division, µ(0)=ξandz(i)=1, for−N+ 1≤i≤0.
Given probability measures a,b, for Γ =H,N= 2,φ1(µ) =δ{a}(µ1),φ2(µ) =δ{b}(µT1), optimization
equation7isentropicregularizedOTproblemandalgorithm11, isthewellknownSinkhorn-Knoppalgorithm
(Cuturi, 2013). Similarly one can apply equation 11 to solve the regularized UOT problem (Chizat et al.,
2017; 2019).
As aforementioned RL objectives in Section 2.2 can be viewed as instances of optimization equation 7,
Dykstra’s algorithm can be used to optimize the objectives. In particular, as the constraint φN(µ) =
δ{bµ}(Aµµ)occurs in each objective, each iteration of Dykstra requires
ProxDΓ
φN(µ) = arg min
˜µ∈∆DΓ(˜µ|µ), (12)
which is the Bregman projection of µonto the set of occupancy measures ∆.
Althoughµ(themeasurefromthepreviousstepofDykstra)doesnotnecessarilylieinside ∆, stepequation12
of Dykstra could be seen as a Bregman divergence policy optimization resulting in dual formulation over
value functions. (See details of dual optimization in Supplementary Material.) This dual formulation is
similar to REPS algorithm (Peters et al., 2010).
In the next section we apply Dykstra to solve unbalanced optimal transport on ∆.
4 Distributionally-Constrained Policy Optimization
A natural constraint in policy optimization is to enforce a global action execution allotment and/or state
visitation frequency. In particular, given a positive baseline measure η/prime, withη/prime(a)being a rough execution
allotment of action aover whole state space, for a∈A, we can consider Dψ1(Eρπ[π]|η/prime)as a global penalty
constraint of policy πunder its natural state distribution ρπ. Similarly, the penalty Dψ2(ρπ|ρ/prime)enforces
a cautious or exploratory constraint on the policy behavior by avoiding or encouraging visitation of some
states according to a given positive baseline measure ρ/primeonS.
So, given baseline measures ρ/primeonSandη/primeonA, we deﬁne distributionally-constrained policy optimization:
max
µ∈∆Eµ[r]−/epsilon11Dψ1(µ1|ρ/prime)−/epsilon12Dψ2/parenleftbig
µT1|η/prime/parenrightbig
. (13)
WhenDψ1=Dψ2=KL, objective equation 13 looks similar to equation 3 (considering expansion in
equation 4), but they are diﬀerent. In equation 13, if ρ/prime=µ/prime1andη/prime=µ/primeT1, for some baseline µ/prime∈∆, then
the third term is KL/parenleftBig
Eρπ[π]|Eρπ/prime[π/prime]/parenrightBig
which is a global constraint on center of mass of πover the whole
state space, whereas Eρπ[KL(π|π/prime)]in equation 4 is a stronger constraint on closeness of policies on every
single state. The bottom line is that equation 13 generally constrains the projected marginals of µoverS
andA, and equation 3 constrains µelement wise.
Forregularizationpurposesiniterativepolicyoptimizationalgorithm(e.g., usingmirrordescent), onenatural
choice of the state and action marginals is to take ρ/prime=/summationtext
aµk−1,η/prime=/summationtext
sµk−1at thek’th iteration. In the
Supplementary Material, we discuss the policy improvement and convergence of such an algorithm. Another
source for the marginals ρ/prime,η/primeis the empirical visitation of states and actions sampled out of an expert policy
in imitation learning.
5Under review as submission to TMLR
Formulation of Objective equation 13 is in form of UOT on the set of occupancy measures. So, for applying
Dykstra algorithm, we can add an entropy term /epsilon1H(µ)to transform equation 13 into
max
µ∈∆−KL(µ|ξ)−/epsilon11Dψ1(µ1|ρ/prime)−/epsilon12Dψ2/parenleftbig
µT1|η/prime/parenrightbig
, (14)
which means setting N= 3,φ1=/epsilon11Dψ1,φ2=/epsilon12Dψ2,φ3(µ) =δbµ(Aµµ),ξ=er//epsilon1in Objective equation 72.
Hence, the algorithm equation 11 can be applied with following proximal functions:
ProxKL
φ1(µ) =diag/parenleftBigg
ProxKL
/epsilon11Dψ1(µ1)
µ1/parenrightBigg
µ, (15)
ProxKL
φ2(µ) =µdiag/parenleftBigg
ProxKL
/epsilon12Dψ2(µT1)
µT1/parenrightBigg
, (16)
ProxKL
φ3(µ) = arg min
˜µ∈∆KL(˜µ|µ). (17)
In general, for appropriate choices of φ1,φ2(e.g.,Dψ1=Dψ2=KL) the proximal operators in equation 15
have closed form solutions. However, as discussed, φ3in equation 17 has no closed form solution (see
SupplementaryMaterial). Wecanalsoconsiderotherfunctionsfor φ1,φ2inthisscenario, forexample, setting
φ2(µ) =δ{η/prime}(µT1), changes the problem into ﬁnding a policy πwhich globally matches the distribution η/prime
under its natural state distribution ρπ, i.e.,Es∼ρπ[π(a|s)] =η(a)for anya∈A3.
In the next section we propose an actor-critic algorithm for large scale reinforcement learning.
4.1 Large Scale RL
WhenS,Aare large, policy optimization via Dykstra is challenging because tabular updating of µ(s,a)is
time consuming or sometimes even impossible. In addition, it requires knowledge of reward function rfor
the initial distribution ξand transition model Pfor projection onto ∆in equation 17. Model estimation
usually requires large number of state-action samples. Also, we might only have oﬀ-policy samples or in
imitation learning scenarios, only access the marginals ρ/prime,η/primethrough observed samples by the expert. In this
section, we derive an oﬀ-policy optimization scheme with function approximation to address these problems.
Replacing the last two terms of obj. equation 13 with their convex conjugates by dual variables u,v,Q, we
get
max
µmin
u,v,QEµ/bracketleftbig
r−Aµ∗Q−/epsilon11u1T
A−/epsilon121SvT/bracketrightbig
+Ebµ[Q(s,a)] +/epsilon11Eρ/prime[ψ∗
1(u(s))] +/epsilon12Eη/prime[ψ∗
2(v(a))],(18)
whereAµ∗is the convex conjugate (transpose) of Aµ.
Having access to samples from a baseline µ/prime∈∆, both helps regularize the objective equation 18 into an
easier problem to solve, and allows oﬀ-policy policy optimization or imitation learning (Nachum & Dai,
2020). Yet, by the discussion in Section 4, regularizing with the term Dψ(µ|µ/prime)in equation 18 might make
marginal penalties redundant, in particular when ρ/prime=/summationtext
aµ/primeandη/prime=/summationtext
sµ/prime.
Here, we discuss the case where ρ/prime/negationslash=/summationtext
aµ/primeorη/prime/negationslash=/summationtext
sµ/prime. See the supplemental for ρ/prime=/summationtext
aµ/primeand
η/prime=/summationtext
sµ/prime. Without the loss of generality, assume marginals ρ/prime/negationslash=/summationtext
aµ/primeandη/prime/negationslash=/summationtext
sµ/prime. In this case,
regularizingequation18with Dψ(µ|µ/prime)andunderFenchelduality, wegettheoﬀ-policyoptimizationobjective
max
πmin
u,v,QEµ/prime[ψ∗(r+γPπQ−Q−/epsilon11u−/epsilon12v) (s,a)]
+ (1−γ)Eπ,p0[Q(s,a)] +/epsilon11Eρ/prime[ψ∗
1(u(s))] +/epsilon12Eη/prime[ψ∗
2(v(a))],(19)
whereu(s,a) :=u(s)andv(s,a) :=v(a). Now, the ﬁrst term is based on expectation of baseline µ/primeand can
be estimated from oﬄine samples. In a special case, when Dψ1=Dψ2=KL in objective equation 13 and
2Coeﬃcients /epsilon11and/epsilon12in equation equation 14 are diﬀerent from those in equation equation 13.
3As a constraint, πρπwould be a feasible solution, when π(a|s) =η/prime(a).
6Under review as submission to TMLR
we takeDψ=KL as well, similar derivations yield
max
πmin
u,v,QL(π,u,v,,Q ) := log Eµ/prime[exp (r+γPπQ−Q−/epsilon11u−/epsilon12v) (s,a)] +
(1−γ)Ep0,π[Q(s,a)] +/epsilon11logEρ/prime[exp(u(s))] +/epsilon12logEη/prime[exp(v(a))].(20)
In this objective note how the /epsilon11uand/epsilon12vare subtracted in the ﬁrst term under distribution µ/primeand the
last two term are added in the objective with respect to distributions ρ/primeandη/primeto adapt the distribution of
baseline samples µ/primeto the constraint distributions ρ/primeandη/prime.
Also, the policy evaluation problem for a ﬁxed policy πcorresponding to this objective 20 (i.e.,
minu,v,QL(u,v,,Q ;π)) corresponds to the dual form update of the Dykstra’s algorithm when Γ =H.
This connects the Dykstra’s algorithm in U(OT) literature to the minimax optimization of Reinforcement
Learning here and work of (Nachum & Dai, 2020). Please see the equation 25 in proof of Dykstra’s in the
appendix A.
Now, gradients of Lcan be computed with respect to u,v,Q:
∇uL=−/epsilon11E(s,a)∼µ/prime[Fµ/prime◦hπ
u,v,Q(s,a)∇u(s)] +/epsilon11Es∼ρ/prime[Fρ/prime◦u(s)∇u(s)], (21)
∇vL=−/epsilon12E(s,a)∼µ/prime[Fµ/prime◦hπ
u,v,Q(s,a)∇v(a)] +/epsilon12Ea∼η/prime[Fη/prime◦v(a)∇v(a)], (22)
∇QL=E(s,a)∼µ/prime[Fµ/prime◦hπ
u,v,Q(s,a) (γPπ∇Q−∇Q) (s,a)] + (1−γ)Es∼p0
a∼π(·|s)[∇Q(s,a)].(23)
And, the gradient with respect to policy πis
∇πL=γEa/prime∼π(·|s/prime)
(s,a,s/prime)∼µ/prime[Fµ/prime◦hπ
u,v,Q(s,a)Q(s/prime,a/prime)·∇logπ(a/prime|s/prime)] + (1−γ)Es∼p0
a∼π(·|s)[Q(s,a)∇logπ(a|s)],(24)
where,hπ
u,v,Q(s,a) :=r(s,a) +γPπQ(s,a)−Q(s,a)−/epsilon11u(s)−/epsilon12v(a)andFp◦h(z) :=Fp(h)(z) =
exp(h(z))/E˜z∼p[exp(h(˜z))]is the softmax operator for any h,z,p. Note that as in the ﬁrst term in ob-
jective equation 20, hπ
u,v,Q(s,a)could be viewed as the adjusted temporal diﬀerence error with respect to the
adjusted reward function r(s,a)−/epsilon11u(s)−/epsilon12v(a), seeingu(s)andv(a)as the cost functions associated to
states and actions respectively and equation 24 is similar to regular policy gradient update for this adjusted
reward function.
Finally,u,v,Q,π can be approximated by some functions (e.g., Neural networks) and one can apply primal-
dual stochastic gradient algorithm on πandu,v,Q. TreatingQas a vector of size S×A, algorithm 1 shows
the pseudo-code for optimizing 20.
Algorithm 1: Large Scale RL when ρ/prime/negationslash=/summationtext
aµ/primeorη/prime/negationslash=/summationtext
sµ/prime
Input:samples{(si,ai,ri)}n
i=1∼µ/prime∈∆,{s/prime
j}n
j=1∼ρ/prime,{a/prime
j}n
j=1∼η/prime,/epsilon11,/epsilon12, learning rates β1,β2
Replace Expectations in equation 21,equation 22,equation 23 and equation 24 by empirical
averages over samples.
[u,v,Q ]T←[u,v,Q ]T−β1[∇uL,∇vL,∇QL]T
π←π+β2∇πL
Output:π
Seethesupplementaltextforthegradientderivations. Also, notethatsetting uandvto0, inourupdates, we
can recover the dice-method optimization in Nachum & Dai (2020) with similar update for Qin equation 23,
for which the gradient calculation suﬀers from double sampling because of multiplication of two expectations
with respect to Pπ.
5 Demonstrations
In this section, we demonstrate the eﬀectiveness of distributionally-constrained policy optimization with
Dykstra. The purpose of our experiments is to answer how distributional penalization on ρ/primeandη/primeaﬀect
7Under review as submission to TMLR
the behavior of the policy and study the Dykstra’s empirical rate of convergence. We test these questions
at a granular level for a grid-world example ﬁrst and next we test it for the cart-pole problem as a large RL
task.
5.1 Grid-world example
We look into these questions on a grid world with its optimal policy out of equation 6 shown in Fig. 1.
Due to the simplicity, this environment is suitable for studying the eﬀect of distributional constraints on the
policy behavior. For the sake of focusing on the eﬀects of distributional constraints, we set the coeﬃcient of
the entropy term fairly low ( /epsilon1=.01) in optimizations equation 6 and equation 14.4
Figure 1: Left: The grid world MDP. The reward entering each state is shown on the grid. There is a block
at state (1,1)andA={up, down, left, right }. Every action succeeds to its intended eﬀect with probability
0.8and fails to each perpendicular direction with probability 0.1. Hitting the walls means stay and γ=.95.
An episode starts in (2,0)(bottom left corner) and any action in (0,3)or(1,3)transitions into (2,0).Right:
Optimal Policy out of optimization equation 6 for /epsilon1=.01.
1)
 2)
 3)
 4)
5)
 6)
 7)
 8)
Figure 2: (1-7): Optimal policy out of Dykstra’s algorithm, with η/prime
i= [α,.5−α,.5−α,α]forαi∈
{e−10,.1,.2,.25,.3,.4,.5−e−10}fori∈{1,..., 7}.(8): corresponding convergence of the Dykstra in (1-7),
horizontal axis is the number of iterations tand the vertical axis is /bardbl/summationtext
sµt−η/prime
i/bardbl.
We ﬁrst observe the independent eﬀect of η/primeon the policy, by setting /epsilon11= 0. We use,δη/prime(µT1)instead of
Dψ1(µT1|η/prime)as an extreme case when /epsilon12→∞to focus on the role η/prime. Figure 2(1-7), shows diﬀerences
among policies when the marginal on actions shifts from a distribution where only equiprobable actions down
andleftare allowed ( η/prime
1= (0,0.5,0.5,0)) towards the case where only upandrightare permitted with equal
probability ( η/prime
7= (0.5,0,0,0.5)).5
In Figure 2(1), under η/prime
1,downis the optimal action in state (0,2)because, this is the only way to get +1
reward (with luck). In 2(2), which changes to a .1probability on right, the policy eliminates the reliance on
change by switching state (0,2)to right.
4Supplementary Material provides the numerical settings in implementation of Dykstra.
5The optimal policies in this section aren’t necessarily deterministic (even though /epsilon1is set to be very small), because of the
constraintδη/prime(µ(a)). In general, the policies out of equation 14 are not necessarily deterministic either because of the nonlinear
objective.
8Under review as submission to TMLR
Note that the optimal policy in Figure 1(left) does not include a downmove. When downis forced to have
non-zero probability, Figures 2(1-6), the policy assigns it to state (2,3), towards the case where only upand
rightare permitted with equal probability ( η/prime
7= (0.5,0,0,0.5)).
Figures 2(7) shows the case where only upandrightare allowed. In state (2,3), this creates a quandary.
Rightis unlikely to incur the −1penalty, but will never allow escape from this state. For this reason, the
policy selects up, which with high probability will incur to the −1penalty, but has some probability of escape
toward the +1reward.
Figure 2(8), depicts the convergence of Dykstra towards various η/prime. Notably, in all cases the algorithm
converges, and the rate of convergence increases following the order of the subﬁgures.
Next, we test the extreme eﬀect of constraints on the state marginals on the policy via various ρ/prime, by setting
/epsilon12= 0and/epsilon11very high. We study the policy when ρ/prime(s) =.9for a single state s, and uniform distribution of
.1on the rest of states other than s. Figure 3(1-3) shows the policies when s∈{(0,2),(1,2),(2,3)}. Hitting
the wall seems to be viable strategy to stay and increase the visitation frequency of each of these states.
Figure 3(4) depicts the the convergence of Dykstra’s algorithm towards various ρ/prime. As shown, the error never
gets to zero. This is because by setting /epsilon11→∞, the objective is just to ﬁnd an occupancy measure with
closest state marginal to ρ/primeandDψ1(µ1|ρ/prime)can never be zero if ρ/primeis not from a valid occupancy measure.
1)
 2)
 3)
 4)
Figure 3: (1-3): Optimal policy out Dykstra, when ρ/prime
i(s) =.9fors∈{(0,2),(1,2),(2,3)}and uniform
distribution of .1on the rest of states other than sifori∈{1,2,3}.(4): corresponding convergence of the
Dykstra in (1-3), horizontal axis is the number of iterations tand the vertical axis is /bardbl/summationtext
aµt−ρ/prime
i/bardbl.
Wealsotesthowimitatingapolicywithdistributionalconstraintsaﬀectsthelearnedpolicy. Forthispurpose,
we create a new environment with reward −10at state (1,3). The optimal risk-averse policy π1out of this
environment is shown in Figure 4(1). Let η/prime
1=/summationtext
sµπ1be the action marginal corresponding to π1. Now
consider the RL problem with reward of −1in state (1,3)constrained by η/prime
1. Figure 4(2) shows the resulting
policyπ2. Notice that µπ2achieves the action marginal distribution η/prime
1, however,π2is quite diﬀerent from
π1, since the unconstrained optimal policy for the environment with reward of −1at state (1,3)is more
risk neutral. In contrast, distributionally constraining ρπ1=/summationtext
aµπ1(as in previous experiments) results in
the same policy of π1as in Figure 4(1). The diﬀerences are mostly in states (0,3)and(1,3), where actions
can be freely chosen (but not their visitation frequency) and contribution of state (2,3)which has a lower
visitation probability. Consider constraints on both η/primeandρ/prime. As explained earlier, the policy will then get as
1)
 2)
 3)
 4)
Figure 4: (1): Risk averse optimal policy learned by changing −1to−10in state (1,3).(2): Optimal policy
usingη/primeout of risk-averse policy as a distributional constraint. (3): Optimal policy with η/prime= [e−10,α,α,α ],
α= (1−e−10)/3.(4): Policy with the same η/primeandρ/prime
1.
close as possible to ρ/primewhile satisfying the action distribution η/prime. Figure 4-(3) shows the optimal policy for η/prime
with no constraint on ρ/prime.η/primeis a distribution where no upis allowed and the other three actions equiprobable.
9Under review as submission to TMLR
Figure 4-(4) depicts the policy under constraints on both η/primeandρ/primewhenρ/primeis the same distribution in Figure
3(1). The leftmost column and top row of this policy leads to (0,2) but in an attempt to satisfy ρ/prime, the policy
goes back to the left.
5.2 Cart-pole example
Figure 5: (Top row-left to right):Action distribution of expert policy, desired distribution η/prime, accumulated
reward during learning. (bottom row) Evolution of action distribution in Algorithm 1.
WealsoperformanablationstudyontheAlgorithm1(seeAppendix)inanimitationlearningscenarioonthe
cart-pole problem. In this test, the expert policy is learned by the policy gradient algorithm and we imitate
the learned policy (assuming r(s,a) = 0) with Algorithm 1 enforcing a given global action distribution η/prime
post-learning. In this setting the expert global action distribution out of a random run is shown in the
top-left corner of Figure 5 and η/primeis bimodal distribution forcing the cart to choose values around −1and
+1(Top-middle ﬁgure). The top-right ﬁgure shows the average return of the learned policy (under 3 seeds).
The second row of ﬁgure shows the shaping and splitting of the action marginal under primal-dual iterations
of Algorithm 1.
6 Related Works
•Objectives and Constraints in Reinforcement Learning. Posingpolicyoptimizationasaconstrained
linear programming on the set of occupancy measures has been long studied (Puterman, 2014). Recent
works have expanded linear programming view through a more general convex optimization framework.
For example, (Neu et al., 2017) uniﬁes policy optimization algorithms in literature for entropy regularized
average reward objectives. (Nachum et al., 2019b;a; Nachum & Dai, 2020) propose entropy-regularized policy
optimizations under Fenchel duality to incorporate the occupancy measure constraint. Unlike these works,
we looked at policy optimization from an OT point of view. To do so, we proposed a structured general
objective based on Bregman divergence that allows considering relaxations of entropy regularization using
marginals. (Zhang et al., 2020b) studies a general concave objective in RL and proposes a variational Monte
Carlo algorithm using the Fenchel dual of the objective function. Similar to these works we take advantage
of Fenchel duality. However, other than diﬀerent view point and structured objective, our work diﬀers in
solving the optimization by breaking the objective using Dykstra’s algorithm. (Zhang et al., 2020a) proposes
variouscaution penalty functions as the RL objective and a primal-dual approach to solve the optimization.
One of these objectives is a penalty on KL (·|ρ/prime), which is a part of our proposed unbalanced formulation.
Other than the formulation, in this work, we focused on distributional penalty on global action execution
which, to the best our knowledge, has not been studied before.
In constrained MDPs, a second reward c(s,a)<0is used to deﬁne a constrained value function Cπ(Altman,
1999; Geibel, 2006). Here Cπ(s) =Eπ[/summationtext∞
t=0γtc(s,a)]and the constraint is in form of Eπ[Cπ(s)]>A (∗),
whereAis a constant. Thus considering c(s,a)as the cost for taking action aat states, constrained MDP
optimizes the policy with a ﬁxed upper bound for the expected cost. Rather than introducing a ﬁxed scalar
10Under review as submission to TMLR
restriction (A), our formulation allows distributional constraints over both the action and state spaces (i.e. ρ/prime
andη/prime). The source of these distributional constraints may vary from an expert policy to the environmental
costs and we can apply them via penalty functions. In special cases, when an action can be identiﬁed by its
individual cost, constraint (∗)on expected cost can be viewed as a special case of marginal constraint on η/prime.
For instance, in the grid world of Fig. 1, if the cost for upandrightis signiﬁcantly higher than downand
left, then limited budget (small expected cost) is essentially equivalent to having a marginal constraint η/prime
1
supported on downandleft. However, in general, when c(s,a)varies for state per action, the expected cost
does not provide much guidance over global usage of actions or visitation of states as our formulation does.
•Reinforcement Learning and Optimal Transport OT in terms of Wasserstein distance has been
proposed in the RL literarture. (Zhang et al., 2018) views policy optimization as Wasserstein gradient
ﬂow on the space of policies. (Pacchiano et al., 2020) deﬁnes behavioral embedding maps on the space
of trajectories and uses an approximation of Wasserstein distance between measures on embedding space
as regularization for policy optimization. Marginals of occupancy measures can be viewed as embeddings
via state/action distribution extracting maps. Our work deﬁnes an additive structure on these embedding
functionals which is broken into Bregman projections using Dykstra.
•Imitation Learning and Optimal Transport In the imitation learning literature, (Xiao et al., 2019)
proposed an adversarial inverse reinforcement learning method which minimizes the Wasserstein distance to
the occupancy measure of the expert policy using the dual formulation of optimal transport. (Dadashi et al.,
2020) minimized the primal problem of Wasserstein minimization and (Papagiannis & Li, 2020) minimize
theSinkhorn Divergence to the expert’s occupancy measure. These works are fundamentally diﬀerent from
our approach as we are not solving the inverse RL problem and we view RL itself as a problem of stochastic
assignment of actions to states. The type of distributional constraints via unbalanced optimal transport
proposed in our work can be considered as relaxation of the idea of matching expert policy occupancy
measures. We consider matching the distribution of global action executions and state visitations of the
expert policy.
•Related works in Optimal Transport It is not the ﬁrst time (U) OT is considered constrained.
Martingale optimal transport imposes an extra constraint on the mean of the coupling (Beiglböck et al.,
2013) and using entropic regularization, Dykstra can be applied (Henry-Labordère, 2013).
•Other Settings In UOT formulation of equation 13 we used penalty Dψ. One can apply other functions
like the indicator function to enforce constraints on marginals like δη/prime(µT1)as discussed in Sec. 4. However,
using the constraint δρ/prime(µ1)in equation 13 could be problematic as it can easily be incompatible with the
occupancy measure constraint. If ρ/primeis not coming form a policy, then the optimization is infeasible. Despite
this, setting /epsilon12= 0,ρ/prime=ρπk−1in equation 13 for the k’th iteration of an iterative policy optimization
algorithm, equation 13 results in objective similar to TRPO(Schulman et al., 2015).
7 Conclusion
We have introduced distributionally-constrained policy optimization via unbalanced optimal transport. Ex-
tending prior work, we recast RL as a problem of unbalanced optimal transport via minimization of an
objective with a Bregman divergence which is optimized through Dykstra’s algorithm. We illustrate the the-
oretical approach through the convergence and policies resulting from marginal constraints on η/primeandρ/primeboth
individually and together. The result uniﬁes diﬀerent perspectives on RL and naturally allows incorporation
of a wide array of realistic constraints on policies. As discussed, we developed the general objectives using
Bregman divergence DΓandψ-divergence Dψand experimented on the case with DΓ=Dψ=KL, in this
paper. It is an interesting direction for future work to test for other various Γ’s andψ’s.
References
Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact
of entropy on policy optimization, 2019.
Eitan Altman. Constrained Markov decision processes , volume 7. CRC Press, 1999.
11Under review as submission to TMLR
Genevay Aude, Marco Cuturi, Gabriel Peyré, and Francis Bach. Stochastic optimization for large-scale
optimal transport, 2016.
Mathias Beiglböck, Pierre Henry-Labordère, and Friedrich Penkner. Model-independent bounds for option
prices—a mass transport approach. Finance and Stochastics , 17(3):477–501, 2013.
Dimitri P Bertsekas. Dynamic programming and optimal control , volume 1. Athena scientiﬁc Belmont, MA,
1995.
Dimitri P Bertsekas. Nonlinear programming . Athena Scientiﬁc, Belmont, MA, second edition, 1999.
Lenaic Chizat, Gabriel Peyré, Bernhard Schmitzer, and François-Xavier Vialard. Scaling algorithms for
unbalanced transport problems, 2017.
Lenaic Chizat, Gabriel Peyré, Bernhard Schmitzer, and François-Xavier Vialard. Unbalanced optimal trans-
port: Dynamic and kantorovich formulation, 2019.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transportation distances, 2013.
Robert Dadashi, Léonard Hussenot, Matthieu Geist, and Olivier Pietquin. Primal wasserstein imitation
learning, 2020.
Richard L Dykstra. An algorithm for restricted least squares regression. Journal of the American Statistical
Association , 78(384):837–842, 1983.
Peter Geibel. Reinforcement learning for mdps with constraints. In European Conference on Machine
Learning , pp. 646–653. Springer, 2006.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep
energy-based policies, 2017.
PierreHenry-Labordère. Automatedoptionpricing: Numericalmethods. International Journal of Theoretical
and Applied Finance , 16(08):1350042, 2013.
Matthias Liero, Alexander Mielke, and Giuseppe Savaré. Optimal entropy-transport problems and a new
hellinger–kantorovich distance between positive measures. Inventiones mathematicae , 211(3):969–1117,
Dec 2017. ISSN 1432-1297. doi: 10.1007/s00222-017-0759-8. URL http://dx.doi.org/10.1007/
s00222-017-0759-8 .
Oﬁr Nachum and Bo Dai. Reinforcement learning via fenchel-rockafellar duality, 2020.
Oﬁr Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted
stationary distribution corrections, 2019a.
Oﬁr Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: Policy
gradient from arbitrary experience, 2019b.
Gergely Neu, Anders Jonsson, and Vicenç Gómez. A uniﬁed view of entropy-regularized markov decision
processes. arXiv preprint arXiv:1705.07798 , 2017.
Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Anna Choromanska, Krzysztof Choromanski, and
Michael I. Jordan. Learning to score behaviors for guided policy optimization, 2020.
Georgios Papagiannis and Yunpeng Li. Imitation learning with sinkhorn distances, 2020.
Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence , volume 24, 2010.
Gabriel Peyré. Entropic approximation of wasserstein gradient ﬂows. SIAM Journal on Imaging Sciences , 8
(4):2323–2351, 2015.
12Under review as submission to TMLR
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley &
Sons, 2014.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy
optimization. In International conference on machine learning , pp. 1889–1897. PMLR, 2015.
John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning,
2018.
Vivien Seguy, Bharath Bhushan Damodaran, Rémi Flamary, Nicolas Courty, Antoine Rolet, and Mathieu
Blondel. Large-scale optimal transport and mapping estimation, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Advances in neural information processing
systems, pp. 1057–1063, 2000.
Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem of oﬀ-
policy temporal-diﬀerence learning. The Journal of Machine Learning Research , 17(1):2603–2631, 2016.
Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear programming.
InProceedings of the 25th international conference on Machine learning , pp. 1032–1039, 2008.
Cédric Villani. Optimal transport: old and new , volume 338. Springer Science & Business Media, 2008.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Machine learning , 8(3-4):229–256, 1992.
HuangXiao, MichaelHerman, JoergWagner, SebastianZiesche, JalalEtesami, andThaiHongLinh. Wasser-
stein adversarial imitation learning, 2019.
Junyu Zhang, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel. Cautious reinforcement learning via
distributional risk in the dual domain. arXiv preprint arXiv:2002.12475 , 2020a.
Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. Variational policy
gradient method for reinforcement learning with general utilities, 2020b.
RuiyiZhang,ChangyouChen,ChunyuanLi,andLawrenceCarin. Policyoptimizationaswassersteingradient
ﬂows. In International Conference on Machine Learning , pp. 5737–5746. PMLR, 2018.
A Fenchel Dual and Proofs in Section 3
For any function f:Dom (f)→R, its convex conjugate (or Fenchel dual) f∗is deﬁned as f∗(y) =
maxx/angbracketleftx,y/angbracketright−f(x). Iffis proper, convex and lower-semi continuous then f∗has the same properties
and one can write f(x) = maxy/angbracketleftx,y/angbracketright−f∗(y). Iffis strictly convex and smooth on int (domf), then∇fand
∇f∗are bijective maps between int (domf∗)and int (domf), i.e.,∇f∗=∇f−1. It is easy to verify that
•Forf(x) =δa(x),f∗(y) =/angbracketlefta,y/angbracketright.
•Considerfwith Dom (f) =M(Z), whereZis an underlying space. For a ﬁxed p∈M (Z), if
f(x) =Dψ(x|p),f∗(y) =Ep(ψ∗(y)). Also, ifψ(z) =zlogz, thenf(x) =KL(x|p)andf∗(y) =
logEp(exp(y)).
Proof of Proposition 3.1. Under condition equation 8, the Fenchel-Legendre duality holds and the solution
of optimization equation 7 can be recovered via
max
u1,···,uN−N/summationdisplay
i=1φ∗
i(ui)−D∗
Γ/parenleftBigg
−N/summationdisplay
i=1ui|ξ/parenrightBigg
= max
u1,···,uN−N/summationdisplay
i=1φ∗
i(ui)−Γ∗/parenleftBigg
∇Γ(ξ)−N/summationdisplay
i=1ui/parenrightBigg
−/angbracketleft∇ Γ(ξ),ξ/angbracketright+ Γ(ξ)
(25)
13Under review as submission to TMLR
with the primal-dual relationship
µ=∇Γ∗/parenleftBigg
−N/summationdisplay
i=1ui/parenrightBigg
.
Applying coordinate descent on equation 25, with initial condition (u(0)
1,···,u(0)
N) = (0,···,0), and setting
i= [l]N,J={1,···,N}\{i}, atl>0we get the iteration
u(l)
i= arg max
ui−φ∗
i(ui)−Γ∗(q−ui),
u(l)
j=u(l−1)
j,∀j∈J,(26)
whereq=∇Γ(ξ)−/summationtextJ
j=1u(l−1)
j. The primal problem of optimization in equation 26 is
arg min
µiΓ(µi)−/angbracketleftq,µi/angbracketright+φi(πi) = arg min
µiDΓ(µi|∇Γ∗(q)) +φi(µi) +const =ProxDΓ
φi(∇Γ∗(q)).
Hence, under the relation µi=∇Γ∗(q−ui), we can rewrite equation 26 as
u(l)
i=q−∇Γ/parenleftBig
ProxDΓ
φi(∇Γ∗(q))/parenrightBig
.
Hence, fori= [l]N, we haveµ(l)
i=∇Γ∗(q−u(l)
i)
µ(l)=∇Γ∗◦∇Γ/parenleftBig
ProxDΓ
φi(∇Γ∗(q))/parenrightBig
=ProxDΓ
φi(∇Γ∗(q)) =ProxDΓ
φi/parenleftBig
∇Γ∗/parenleftBig
q−u(l−N)
i +u(l−N)
i/parenrightBig/parenrightBig
=ProxDΓ
φi/parenleftBig
∇Γ∗/parenleftBig
∇Γ(µ(l−1)) +u(l−N)
i/parenrightBig/parenrightBig
.
Calculating the diﬀerence ∇Γ(µ(l−1))−∇Γ(µ(l))and change of variable ν(l)=−u(l)
[l]N, ends the proof.
Proof of Corollary 3.2. Peyré (2015) Setting Γ =H, then∇Γ = log and∇Γ∗= exp. Also,DΓ=KL and
by change of variable z(l)=∇Γ(v(l)), the corollary follows.
B Derivations in Section 4
Here we derive the Proximal operators in section equation 4.
B.1 Proximal Operator Calculations when DΓ= KL
WhenDΓ=KL (i.e., Γ =H), forφ3(µ) =δbµ(Aµµ)we have:
ProxKL
φ3(µ) = arg min
˜µ∈∆KL(˜µ|µ), (27)
lettingV,λto be dual variables, by the deﬁnition of ∆in lemma 2.1, the Lagrangian of equation 27 is
KL(˜µ|µ)−γ/summationdisplay
s,s/prime,a/primeP(s|s/prime,a/prime)µ(s/prime,a/prime)V(s)−(1−γ)/summationdisplay
sp0(s)V(s) +/summationdisplay
s,aµ(s,a)V(s) +λ/parenleftBigg/summationdisplay
s,aµ(s,a)−1/parenrightBigg
.
The derivative of the Lagrangian with respect to ˜µ(s,a)for equation 27 is
log(˜µ(s,a)/µ(s,a))−γ/summationdisplay
s/primeP(s/prime|s,a)V(s/prime) +V(s) +λ= 0.
So, the optimal solution is
˜µ(s,a) =e−λµ(s,a) exp(γ/summationdisplay
s/primeP(s/prime|s,a)V(s/prime)−V(s)).
14Under review as submission to TMLR
Since/summationtext
s,a˜µ(s,a) = 1, we have
λ= log/summationdisplay
s,aµ(s,a) exp/parenleftBigg
γ/summationdisplay
s/primeP(s/prime|s,a)V(s/prime)−V(s)/parenrightBigg
,
then
˜µ(s,a) =µ(s,a) exp (γ/summationtext
s/primeP(s/prime|s,a)V(s/prime)−V(s))/summationtext
s,aµ(s,a) exp(γ/summationtext
s/primeP(s/prime|s,a)V(s/prime)−V(s)), (28)
whereVis the solution of the dual problem
min
λ,Vλ+ (1−γ)/summationdisplay
sp0(s)V(s) =
min
Vlog/summationdisplay
s,aµ(s,a) exp(γ/summationdisplay
s/primeP(s/prime|s,a)V(s/prime)−V(s)) + (1−γ)/summationdisplay
sp0(s)V(s).(29)
By solving optimization equation 29, we can recover optimal ˜µby equation equation 28.
For other proximal operators we need the following lemma:
Lemma B.1. Peyré (2015) For any convex function h:
(i) For any φ(µ) =h(µ1)
ProxKL
φ(µ) =diag/parenleftBigg
ProxKL
h(µ1)
µ1/parenrightBigg
µ,
(ii) and, if φ(µ) =h(µT1)
ProxKL
φ(µ) =µdiag/parenleftBigg
ProxKL
h(µT1)
µT1/parenrightBigg
.
Proof.Let
˜µ:=ProxKL
φ(µ) = arg min
˜µKL(˜µ|µ) +h(˜µ1),
hence, by the ﬁrst order condition, at the optimal ˜µ, there exists Z∈∂h(˜µ1)such that we have log˜µ
µ+Z= 0.
Therefore,
˜µ=diag/parenleftbig
e−Z/parenrightbig
µ. (30)
Similarly, considering the ﬁrst order condition corresponding to ˜u:=ProxKL
h(µ1), we get ˜u=diag(e−Z)µ1,
and combining this with equation 30 proves part (i). Transposing (i), results in (ii).
Using this lemma, we calculate some of the proximal operators we used in Section 4 and 5.
•Letρ:=µ1. Givenρ/prime∈M (S), whenφ1(µ) =/epsilon11Dψ1(ρ|ρ/prime) =/epsilon11KL(ρ|ρ/prime),
ProxKL
/epsilon11KL(ρ) = arg min
˜ρKL(˜ρ|ρ) +/epsilon11KL(˜ρ|ρ/prime),
then, by the ﬁrst order condition, we have
log˜ρ
ρ+/epsilon11log˜ρ
ρ/prime= 0,
and ProxKL
/epsilon11KL(µ1) = (ρρ/prime/epsilon11)1/(1+/epsilon11). Applying lemma B.1(i), gives the proximal operator for φ1(µ).
•Whenφ1(µ) =δρ/prime(ρ), then ProxKL
φ1(ρ) =ρ/primeand by applying lemma B.1(i), we get
ProxKL
φ1(µ) =diag/parenleftbiggρ/prime
µ1/parenrightbigg
µ.
15Under review as submission to TMLR
•Similarly, given η/prime∈M (A). Ifη:=µT1andφ2=/epsilon12Dψ2(η|η/prime) =/epsilon12KL(η|η/prime),
ProxKL
/epsilon12KL(η) = arg min
˜ηKL(˜η|η) +/epsilon12KL(˜η|η/prime),
and ProxKL
/epsilon12KL(µT1) = (ηη/prime/epsilon12)1/(1+/epsilon12). Applying lemma B.1(ii) results in the proximal operator for
φ2.
•Also, when φ2(µ) =δη/prime(µT1), by lemma B.1(ii), we get
ProxKL
φ2(µ) =µdiag/parenleftbiggη/prime
µT1/parenrightbigg
.
B.2 Convergence of the iterative policy optimization for problems equation 13 and equation 14
We now show the convergence and policy improvement of optimization equation 13, and equation 14 in an
iterative policy optimization scenario.
Proposition B.2. Let(µk)k∈Nbe a sequence of occupancy measures such that for each k≥1,µkis the
solution to the optimization equation 13 with µ/prime=µk−1, then lim
k→∞µk=µ∗is the solution to max
µ∈∆Eµ[r].
Proof.Letµ0∈∆such thatµ0(s,a)>0for all (s,a). Letµk= Ψ(µk−1)for allk >0where Ψ(µ/prime)is the
solution to equation 14. Let Θ(µ) =Eµ[r], sinceµk−1is a feasible point for optimization equation 13, we
have monotonic improvement on the discounted accumulated rewards as
Θ(µk−1)≤max
µ∈∆Θ(µ)−/epsilon11Dψ1(µ1|µk−11)−/epsilon12Dψ2(µT1|µT
k−11)≤Θ(µk), (31)
with equality achieved only if µk−1=µk=µ∗.
Θ(µ)−/epsilon11Dψ1(/summationtext
aµ|/summationtext
aµ/prime)−/epsilon12Dψ2(/summationtext
sµ|/summationtext
sµ/prime)is strictly concave on µand smooth on µ/prime. Thus Ψ(µ/prime)is
continuous and so is Θ(Ψ(µ/prime)).
Since we assumed S,Aare ﬁnite, from monotone convergence theorem, we can conclude that
limk→∞Θ(µk) =:θ∞exists. Then we prove the theorem by contradiction. Suppose that θ∞<Θ(µ∗) =
max ∆ηΘ, then Θ−1(θ∞)is closed and bounded, thus compact in ∆, therefore there exists a c>0such that
Θ(Ψ(µ))−Θ(µ)≥conΘ−1(θ∞). By continuity and compactness we may choose a δ < c/ 2small enough
such that Θ(Ψ(µ))−Θ(µ)>c/2onΘ−1([θ∞−δ,θ∞]). There is such a δ(µ)>0for everyµ∈Θ−1(θ∞)and
we can choose δ= minδ(µ)>0by compactness.
Since limk→∞Θ(µk) =θ∞,thereexistsan n>0suchthat Θ(µn)∈[θ∞−δ,θ∞],thus Θ(µn+1) = Θ(Ψ(µn))>
Θ(µn) +c/2≥θ∞−δ+c/2>θ∞. This is an contradiction to the assumption that Θ(µk)converges to θ∞
increasingly.
Therefore,θ∞= Θ(µ∗), and from strict convexity of Θ, we have Θ−1(θ∞) ={µ∗}. So limk→∞µk=µ∗.
Similarly, for the optimization equation 14:
max
µ∈∆−KL(µ|ξ)−/epsilon11Dψ1(µ1|ρ/prime)−/epsilon12Dψ2/parenleftbig
µT1|η/prime/parenrightbig
,
we can have the following proposition:
Proposition B.3. Let(µk)k∈Nbe a sequence of occupancy measures such that for each k≥1,µkis the
solution to the optimization problem equation 14 with µ/prime=µk−1, then lim
k→∞µk=µ∗is the solution to
max
µ∈∆−KL(µ|ξ).
16Under review as submission to TMLR
C Large Scale Algorithm
C.1ρ/prime=/summationtext
aµ/primeandη/prime=/summationtext
sµ/prime
Following the approach in Sutton et al. (2016); Nachum & Dai (2020), assuming πis known, with the change
of variable ζ(s,a) :=µ
µ/prime(s,a), we can rewrite equation 18 with importance sampling weights as a policy
evaluation problem
min
u,v,Qmax
ζL(u,v,Q,ζ ;π) : =Eµ/prime[ζ(s,a) (r+γPπQ−Q−/epsilon11u−/epsilon12v) (s,a)]
+ (1−γ)Ep0,π[Q(s,a)] +/epsilon11Eρ/prime[ψ∗
1(u(s))] +/epsilon12Eη/prime[ψ∗
2(v(a))].(32)
Given the optimized Q,ζ, the gradient with respect to πis
∇πL(u,v,Q,ζ,π ) =E(s,a)∼µ/prime[ζ(s,a)Q(s,a)∇logπ(a|s)]. (33)
The gradients with respect to u,v,Q,ζ are as follows:
∇uL(u,v,Q,ζ ;π) =−/epsilon11Eµ/prime[ζ(s,a)∇u(s)] +/epsilon11Eρ/prime[∇uψ∗
1(u(s))], (34)
∇vL(u,v,Q,ζ ;π) =−/epsilon12Eµ/prime[ζ(s,a)∇v(a)] +/epsilon12Eη/prime[∇vψ∗
2(v(a))], (35)
∇QL(u,v,Q,ζ ;π) =Eµ/prime[(ζ+γPπ∇Q−∇Q) (s,a)] + (1−γ)Ep0,π[∇Q(s,a)], (36)
∇ζL(u,v,Q,ζ ;π) =Eµ/prime/bracketleftbig
∇ζ(s,a)hπ
u,v,Q(s,a)/bracketrightbig
. (37)
Wrapping maxπaround equation 32 gives the oﬀ-policy optimization. Given optimized Q,ζ, the gradient
with respect to πis
∇πL(u,v,Q,ζ,π ) =E(s,a)∼µ/prime[ζ(s,a)Q(s,a)∇logπ(a|s)] (38)
D Gradient Derivations
Here we derive optimization equation 18. Let’s ﬁx policy π, then following the policy optimization approach
in Nachum & Dai (2020), one might deﬁne the policy evaluation problem for πasmaxµh(µ)−δbπ(Aπµ)for
any arbitrary concave function has the problem is over-constrained and µis the unique solution of Aπµ=bπ.
So we deﬁne the policy evaluation problem for a ﬁxed πcorresponding to equation 13 as
max
µEµ[r]−/epsilon11Dψ1(µ1|ρ/prime)−/epsilon12Dψ2(µT1|η/prime)−δbπ(Aπµ)
(a)= max
µmin
u,v,Q/angbracketleftµ,r/angbracketright−/epsilon11/angbracketleftµ1,u/angbracketright+/epsilon11Eρ/prime[ψ∗
1(u(s))]−/epsilon12/angbracketleftµT1,v/angbracketright+/epsilon12Eη/prime[ψ∗
2(v(a))]−/angbracketleftAπµ,Q/angbracketright+Ebπ[Q(s,a)]
= max
µmin
u,v,Q/angbracketleftµ,r−Aπ∗Q−/epsilon11u1T
A−/epsilon121SvT/angbracketright+Ebπ[Q(s,a)] +/epsilon11Eρ/prime[ψ∗
1(u(s))] +/epsilon12Eη/prime[ψ∗
2(v(a))],
(39)
where (a) is obtained by replacing the last three terms by their convex conjugates and Aπ∗is transpose of Aπ.
This gives optimization equation 18. If we regularize objective equation 18 with Dψ(µ|µ/prime)regularization,
under Fenchel duality we get
max
µmin
u,v,Q/angbracketleftµ,r−Aπ∗Q−/epsilon11u1T
A−/epsilon121SvT/angbracketright−Dψ(µ|µ/prime) +Ebπ[Q(s,a)] +/epsilon11Eρ/prime[ψ∗
1(u(s))] +/epsilon12Eη/prime[ψ∗
2(v(a))]
= min
u,v,Q/braceleftbigg
−min
µ/angbracketleft−µ,r−Aπ∗Q−/epsilon11u1T
A−/epsilon121SvT/angbracketright+Dψ(µ|µ/prime)/bracerightbigg
+Ebπ[Q(s,a)]+/epsilon11Eρ/prime[ψ∗
1(u(s))] +/epsilon12Eη/prime[ψ∗
2(v(a))]
= min
u,v,QEµ/prime[ψ∗(r(s,a)−Aπ∗Q(s,a)−/epsilon11u(s)−/epsilon12v(a))] +Ebπ[Q(s,a)] +/epsilon11Eρ/prime[ψ∗
1(u(s))] +/epsilon12Eη/prime[ψ∗
2(v(a))],
(40)
17Under review as submission to TMLR
and wrapping equation 40 with maxπresults in equation 19. The gradient in equations equation 21,
equation 22 and equation 23 are basic calculus. Assuming Q∗,u∗,v∗are optimal functions out of the
policy evaluation in equation 40 for the given ﬁxed π, then using Danskin’s theorem Bertsekas (1999)
we have∂πminu,v,QL(u,v,Q ;π) =∂πL(u∗,v∗,Q∗;π)and gradient in equation 24 is derived using the
facts: (A)PπQ(s,a) =Es/prime∼P(·|s,a),a/prime∼π(·|s/prime)[Q(s/prime,a/prime)]and(B)for any distribution z∼p,∂pEp[h(z)] =
Ep[h(z)∇logp(z)].
In order to derive the objective of equation 32, ﬁrst we interchange minµandmaxu,v,Qin equation 39 by
minimax theorem. Fixing policy π, we rewrite the ﬁrst term in equation 39 as
/angbracketleftµ,r−Aπ∗Q−/epsilon11u1T
A−/epsilon121SvT/angbracketright=Eµ[r(s,a)−Aπ∗Q(s,a)−/epsilon11u(s)−/epsilon12v(a)]
=Eµ/prime[ζ(s,a) (r(s,a)−Aπ∗Q(s,a)−/epsilon11u(s)−/epsilon12v(a))],
whereζ(s,a) =µ(s,a)
µ/prime(s,a).
Equations equation 34–equation 37 are basic calculus derivations. For equation 38, given optimized Q,u,v
andζ, using (A)and(B), we can take the gradient of the ﬁrst two terms in equation 39, i.e.,
Eµ/prime[ζ(s,a) (r(s,a)−Aπ∗Q(s,a)−/epsilon11u(s)−/epsilon12v(a))] +Ebπ[Q(s,a)],
with respect to πand combine them under the relation µ(s,a) = (1−γ)p0(s)π(a|s) +
γπ(a|s)/summationtext
s/prime,a/primeP(s|s/prime,a/prime)µ(s/prime,a/prime)to get equation 38.
These two approaches in Section 4, are similar to policy gradient derivations in Nachum & Dai (2020), with
corrective terms on u(s)andv(a)in the softmax operator deﬁned in the main text.
E Demonstrations
E.1 GridWorld Setup
In Section 5, we set Dψ1=Dψ2=KL and we applied Dykstra in the gridworld. We used the Frobenius
norm on the diﬀerence of two consecutive matrices out of Dykstra until the error is less than 10−5.
Inordertoseetheextremeeﬀectof ρ/primeindependently(setting /epsilon12= 0),wecanenforceitas δρ/prime(µ1),eventhough
it might not be possible to ﬁnd a µsuchµ1=ρ/primeas discussed in Section 5. In this setting, we observed
Dykstra gets stuck switching back and forth between projection onto ρ/primeand projection onto occupancy
measures ∆and we get division by zero exception. In our experiments we observed similar outcome when
using the penalty function KL (µ1|ρ/prime)with high coeﬃcient /epsilon11(close to 20).
E.2 Cart-Pole setup
For this test we set γ=.99, withβ1= 0.005andβ2= 0.001, to compute the expectation with respect to
policy, we averaged Qvalues over 10 randomly sampled actions drawn from the policy at each state. To
better see the eﬀect of η/prime, we set a high value of /epsilon12= 1000. Policy in this example is a gaussian MLP network
of two hidden layers of 8 units and Qandvhave two hidden layers of 32units.
18