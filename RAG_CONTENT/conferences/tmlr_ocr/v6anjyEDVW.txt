Published in Transactions on Machine Learning Research (02/2023)
Costs and BeneÔ¨Åts of Fair Regression
Han Zhao hanzhao@illinois.edu
Department of Computer Science
University of Illinois Urbana-Champaign
Reviewed on OpenReview: https://openreview.net/forum?id=v6anjyEDVW
Abstract
Real-world applications of machine learning tools in high-stakes domains are often regulated to
be fair, in the sense that the predicted target should satisfy some quantitative notion of parity with
respect to a protected attribute. However, the exact tradeoff between fairness and accuracy with a
real-valued target is not entirely clear. In this paper, we characterize the inherent tradeoff between
statistical parity and accuracy in the regression setting by providing a lower bound on the error of
any attribute-blind fair regressor. Our lower bound is sharp, algorithm-independent, and admits a
simple interpretation: when the moments of the target differ between groups, any fair algorithm has
to make an error on at least one of the groups. We further extend this result to give a lower bound on
the joint error of any (approximately) fair algorithm, using the Wasserstein distance to measure the
quality of the approximation. With our novel lower bound, we also show that the price paid by a fair
regressor that does not take the protected attribute as input is less than that of a fair regressor with
explicit access to the protected attribute. On the upside, we establish the Ô¨Årst connection between
individual fairness, accuracy parity, and the Wasserstein distance by showing that if a regressor is
individually fair, it also approximately veriÔ¨Åes the accuracy parity, where the gap is again given by
the Wasserstein distance between the two groups. Inspired by our theoretical results, we develop
a practical algorithm for fair regression through the lens of representation learning, and conduct
experiments on a real-world dataset to corroborate our Ô¨Åndings.
1 Introduction
High-stakes domains, e.g., loan approvals, and credit scoring, have been using machine learning tools to help make
decisions. A central question in these applications is whether the algorithm makes fair decisions, in the sense that
certain sensitive data does not inÔ¨Çuence the outcomes or accuracy of the learning algorithms. For example, as regulated
by the General Data Protection Regulation (GDPR, Article 22 Paragraph 4) (gdp), ‚Äúdecisions which produce legal
effects concerning him or her or of similar importance shall not be based on certain personal data‚Äù, including race,
religious belief, etc. As a result, using sensitive data directly in algorithms is often prohibited. However, due to the
redundant encoding, redlining, and other problems, this ‚Äúfairness through blindness‚Äù is often not sufÔ¨Åcient to ensure
algorithmic fairness in automated decision-making processes.
Many works have produced methods aiming at reducing unfairness (Calmon et al., 2017; Chi et al., 2021; Hardt et al.,
2016; Agarwal et al., 2019; Feldman et al., 2015; Beutel et al., 2017; Lum & Johndrow, 2016) under various contexts.
However, the question of the price that we need to pay for enforcing various fairness deÔ¨Ånitions in terms of the accuracy
of these tools is less explored. In this paper, we attempt to answer this question by characterizing a tradeoff between
statistical parity and accuracy in the regression setting, where the regressor is prohibited to use the sensitive attribute
directly, dubbed as an attribute-blind regressor (predictor). Among many deÔ¨Ånitions of fairness (Verma & Rubin, 2018)
in the literature, statistical parity asks the predictor to be statistically independent of a predeÔ¨Åned protected attribute,
e.g., race, gender, etc. While empirically it has long been observed that there is an underlying tension between accuracy
and statistical parity (Calders et al., 2013; Zliobaite, 2015; Berk et al., 2017; Agarwal et al., 2019) in both classiÔ¨Åcation
and regression settings, theoretical understanding of this tradeoff in regression is limited. In the case of classiÔ¨Åcation,
Menon & Williamson (2018) explored such tradeoff in terms of the fairness frontier function under the context of
cost-sensitive binary classiÔ¨Åcation. Zhao & Gordon (2019) provided a characterization of such a etradeoff in binary
1Published in Transactions on Machine Learning Research (02/2023)
classiÔ¨Åcation. Recently, Chzhen et al. (2020a) and Le Gouic et al. (2020) concurrently with each other derived an
analytic bound to characterize the price of statistical parity in regression using Wasserstein barycentres when the learner
can take the sensitive attribute explicitly as an input.
In this paper, we derive the Ô¨Årst lower bound to characterize the inherent tradeoff between fairness and accuracy in the
regression setting under general `ploss when the regressor is prohibited to use the sensitive attribute directly during the
inference stage. Our main theorem can be informally summarized as follows:
Foranyfair algorithm satisfying statistical parity, it has to incur a large error on at least one of the
demographic subgroups when the moments of the target variable differ across groups. Furthermore,
if the population of the two demographic subgroups is imbalanced, the minorities could still suffer
from the reduction in accuracy even if the global accuracy does not seem to reduce.
We emphasize that the above result holds in the noiseless setting as well, where there exist (unfair) algorithms that are
perfect on both demographic subgroups. Hence it highlights the inherent tradeoff due to the coupling between statistical
parity and accuracy in general, not due to the noninformativeness of the input. We also extend this result to the general
noisy setting when only approximate fairness is required. Our bounds are algorithm-independent and do not make any
distributional assumptions. To illustrate the tightness of the lower bound, we also construct a problem instance where
the lower bound is attained. In particular, it is easy to see that in an extreme case where the group membership coincides
with the target task, a call for exact statistical parity will inevitably remove the perfect predictor. At the core of our
proof technique is the use of the Wasserstein metric and its contraction property under certain Lipschitz assumptions on
the regression predictors.
On the positive side, we establish the Ô¨Årst connection between individual fairness (Dwork et al., 2012), a more Ô¨Åne-
grained notion of fairness, and accuracy parity (Buolamwini & Gebru, 2018; Bagdasaryan et al., 2019; Chi et al.,
2021). Roughly speaking, an algorithm is said to be individually fair if it treats similar individuals similarly. We show
that if a regressor is individually fair, then it also approximately veriÔ¨Åes the accuracy parity. Interestingly, the gap in
this approximation is exactly given by the Wasserstein distance between the distributions across groups. Our proof
techniques are very simple but general, and we expect them to have broader applications in other learning scenarios
with real targets, e.g., domain adaptation for regression problems (Ganin et al., 2016; Courty et al., 2017; Zhao et al.,
2019b) and counter-factual inference (Johansson et al., 2016; Shalit et al., 2017; Johansson et al., 2020).
Although our main focus is to understand the costs and beneÔ¨Åts of fair regression, our analysis also naturally suggests
a practical algorithm to achieve statistical parity and accuracy parity simultaneously in regression by learning fair
representations. The idea is relatively simple and intuitive: it sufÔ¨Åces if we can ensure that the representations upon
which the regressor applies are approximately fair (measured by Wasserstein distance). Finally, we also conduct
experiments on a real-world dataset to corroborate our theoretical Ô¨Åndings. Our results highlight the role of the
Wasserstein distance in both the theoretical analysis and algorithm design of fair regression, which complements the
existing results for fair classiÔ¨Åcation using TV-distance (Zhao & Gordon, 2022).
2 Preliminaries
Notation We consider a general regression setting where there is a joint distribution mover the triplet T= (X,A,Y),
where X2X  Rdis the input vector, A2f0, 1gis the protected attribute, e.g., race, gender, etc., and Y2
Y[ 1, 1]is the target output.1Hence, the joint distribution mis deÔ¨Åned over the product space Xf 0, 1gY .
Lower case letters x,aandyare used to denote the instantiation of X,AandY, respectively. LetHbe a hypothesis
class of predictors from input to output space. Throughout the paper, we focus on the setting where the regressor
cannot directly use the sensitive attribute Ato form its prediction. However, note that even if the regressor does not
explicitly take the protected attribute Aas input, this fairness through blindness mechanism can still be biased due to the
redundant encoding issue (Barocas et al., 2017). To keep the notation uncluttered, for a2f0, 1g, we use mato mean the
conditional distribution of mgiven A=a. The zero-one entropy of A(Gr√ºnwald et al., 2004, Section 3.5.3) is denoted
asH0-1(A):=1 maxa2f0,1gPr(A=a). Furthermore, we use Fnto represent the cumulative distribution function
of a distribution noverR, i.e., for t2R,Fn(t):=Prn(( ¬•,t]). In this paper, we assume that the density of miand
1Our main results could be extended to the case where Acan take Ô¨Ånitely many values.
2Published in Transactions on Machine Learning Research (02/2023)
its corresponding pushforward under proper transformation (w.r.t. the Lebesgue measure l) is universally bounded
above, i.e.,kdmi/dlk¬•C,8i2f0, 1g. Given a feature transformation function g:X!Z that maps instances
from the input space Xto feature spaceZ, we deÔ¨Åne g]m:=mg 1to be the induced distribution (pushforward) of m
under g, i.e., for any measurable event E0Z ,Prg]m(E0):=Prm(g 1(E0)) = Prm(fx2Xj g(x)2E0g). We also
useY]mto denote the marginal distribution of Yfrom the joint distribution m, i.e., projection of monto the Ycoordinate.
Throughout the paper, we make the following assumption:
Assumption 2.1. There exists a constant Csuch that the density of every m0(w.r.t. the Lebesgue measure l) is
universally bounded above, i.e., kdm0/dlk¬•C.
Fairness DeÔ¨Ånition We mainly focus on group fairness where the group membership is given by the protected
attribute A. In particular, statistical parity asks that the predictor should be statistically independent of the protected
attribute. In binary classiÔ¨Åcation, this requirement corresponds to the notion of equality of outcome (Holzer & Neumark,
2006), and it says that the outcome rate should be equal across groups.
DeÔ¨Ånition 2.1 (Statistical Parity) .Given a joint distribution m, a classiÔ¨ÅerbY=h(X), satisÔ¨Åes statistical parity ifbYis
independent of A.
SincebYis continuous, the above deÔ¨Ånition implies that Prm0(bY2E) =Prm1(bY2E)for any measurable event ER.
Statistical parity has been adopted as a deÔ¨Ånition of fairness in a series of work (Calders et al., 2009; Edwards &
Storkey, 2015; Johndrow et al., 2019; Kamiran & Calders, 2009; Kamishima et al., 2011; Louizos et al., 2015; Zemel
et al., 2013; Madras et al., 2018), both under the classiÔ¨Åcation and regression settings.
Fair Regression Given a joint distribution m, the weighted `perror of a predictor bY=h(X)under mforp1is
deÔ¨Åned as
#p,m(bY):=√•
aPrm(A=a)#p,ma(bY) =√•
aPrm(A=a)
Emah
jbY Yjpi1/p
. (1)
Note that the deÔ¨Ånition above on weighted `perror is not the same as the `perror over the joint distribution m. Instead,
it is the weighted sum of `perrors on each sub-group, where the weighted combination is given by the ratio of
the sub-groups in the overall population. In fact, these two losses can be related by the following observation. Let
pa:=Prm(A=a), we have
kbY Yk`p(m):=
Emh
jbY Yjpi1/p
= 
√•
apaEmah
jbY Yjpi!1/p
√•
apa
Emah
jbY Yjpi1/p
=#p,m(bY),
where the inequality is due to Jensen‚Äôs inequality since g(t) =t1/pforp1is concave over t0. The above
inequality means that the `perror over the joint distribution mis an upper bound of our weighted `perror over subgroups,
which is the main focus of this paper. Conceptually, as we shall shortly see in Section 3.1, since we are more interested
in understanding the tradeoff between the so-called balanced error rate and statistical parity, where each sub-group has
the same weight towards the whole population, it is natural to treat each sub-group as a separate distribution and deÔ¨Åne
the corresponding error over it separately.
As two notable special cases, when p=2, the above deÔ¨Ånition reduces to the weighted sum of the square root
of the usual mean-squared-error (MSE) over each sub-group; when p=1,(1)becomes the weighted sum of the
mean-absolute-error (MAE) of the predictor overall each sub-group. To make the notation more compact, we may drop
the subscript mwhen it is clear from the context. The main departure from prior works on classiÔ¨Åcation is that both
YandbY(h(X))are allowed to be real-valued rather than just categorical. Under statistical parity, the problem of fair
regression can be understood as the following constrained optimization problem:
minimize
h2H#p,m(bY)
subject toPrm0(h(X)t) Prm1(h(X)t)e,8t2R.(2)
3Published in Transactions on Machine Learning Research (02/2023)
Note that since bY=h(X)2Ris a real-valued random variable and Ais binary, the constraint in the above optimization
formulation asks that the conditional cumulative distributions of bYare approximately equal across groups, which is an
additive approximation to the original deÔ¨Ånition of statistical parity. Formally, the constraint in (2)is known as the
Kolmogorov-Smirnov distance:
DeÔ¨Ånition 2.2 (Kolmogorov-Smirnov distance) .For two probability distributions mandm0overR, the Kolmogorov-
Smirnov distance K(m,m0)isK(m,m0):=supz2RjFm(z) Fm0(z)j.
With the Kolmogorov-Smirnov distance, we can deÔ¨Åne the e-statistical parity for a regressor h:
DeÔ¨Ånition 2.3 (e-Statistical Parity) .Given a joint distribution mand0e1, a regressorbY=h(X), satisÔ¨Åes
e-statistical parity ifK(h]m0,h]m1)e.
Clearly, the slack variable econtrols the quality of approximation and when e=0it reduces to asking exact statistical
parity as deÔ¨Åned in DeÔ¨Ånition 2.1.
Wasserstein Distance Given two random variables TandT0with the corresponding distributions mandm0, let
G(m,m0)denote the set of all couplings gofmandm0, i.e., gT=mandgT0=m0. The Wasserstein distance between
the pair of distributions mandm0is deÔ¨Åned as follows:
Wp(m,m0):=
inf
g2G(m,m0)Z
kT T0kpdg1/p
, (3)
where p1and throughout this paper we Ô¨Åx kk to be the `2norm. For the special case where both mandm0are
distributions over R, the Wasserstein distance Wp(m,m0)admits the following equivalent characterization (Kolouri
et al., 2017):
Wp(m,m0) =Z1
0jF 1
m(t) F 1
m0(t)jpdt1/p
, (4)
where F 1
m(t)denotes the generalized inverse of the cumulative distribution function, i.e., F 1
m(t) =infz2Rfz:F(z)
tg. The above closed-form formulation will be particularly useful in our later analysis. When p=1, the Wasserstein
distance is also called the Earth Mover distance , and it admits a dual representation in a variational form using sup
rather than inf:W1(m,m0) =supf:kfkL1R
f dm R
f dm0, wherekfkL:=supx6=x0jf(x) f(x0)j/jx x0jis
the Lipschitz seminorm of f. It is well-known that convergences of measures under the Wasserstein distance imply weak
convergence, i.e., convergence in distribution (Gibbs & Su, 2002). Furthermore, compared with other distance metrics
including total variation (TV), Jensen-Shannon distance, etc. that ignore the geometric structure of the underlying
space, Wasserstein distance often allows for more robust applications, e.g., the Wasserstein GAN (Arjovsky et al.,
2017), domain adaptation (Courty et al., 2017), etc., due to its Lipschitz continuous constraint in the dual representation.
Moreover, unlike the KL divergence, the Wasserstein distance between two measures is generally Ô¨Ånite even when
neither measure is absolutely continuous with respect to the other, a situation that often arises when considering
empirical distributions arising in practice. Furthermore, unlike the TV-distance, the Wasserstein distance inherently
depends on the geometry of the underlying space, whereas the TV distance is invariant under any bijective mapping.
3 Main Results
Recently, Agarwal et al. (2019) proposed a reduction-based approach to tackle (2)by solving a sequence of cost-sensitive
problems. By varying the slack variable e, the authors also empirically veriÔ¨Åed the unavoidable tradeoff between
statistical parity and accuracy in practice. However, to the best of our knowledge, a quantitative characterization of the
exact tradeoff between fairness and accuracy is still missing. In this section, we seek to answer the following intriguing
and important question:
In the setting of regression, what is the minimum error that any attribute-blind fair algorithm has to
incur, and how does this error depend on the coupling between the target and the protected attribute?
In what follows we shall Ô¨Årst provide a simple example to illustrate this tradeoff. This example will give readers a Ô¨Çavor
of the kind of impossibility result we are interested in proving. We then proceed to formally present our Ô¨Årst theorem
4Published in Transactions on Machine Learning Research (02/2023)
which exactly answers the above question, even if only approximate fairness is satisÔ¨Åed. We conclude this section with
some discussions on the implications of our results.
A Simple Example As a warm-up, let us consider an example to showcase the potential tradeoff between statistical
parity and accuracy. But before our construction, it should be noted that the error #p,m(bY)bears an intrinsic lower bound
for any deterministic predictor bY=h(X), i.e., the noise in the underlying data distribution m. Hence to simplify our
discussions, in this example, we shall construct distributions such that there is no noise in the data, i.e., for a2f0, 1g,
there exists a ground-truth labeling function h
asuch that Y=h
a(X)onma. Realize that such simpliÔ¨Åcation will only
make it harder for us to prove the lower bound on #p,masince there exist predictors that are perfect.
Example 3.1 (Target coincides with the protected attribute) .Fora2f0, 1g, let the marginal distribution X]mabe a
uniform distribution over f0, 1g. Let Y=abe a constant. Hence by construction, on the joint distribution, we have
Y=Ahold. Now for any fair predictor bY=h(X), the statistical parity asks bYto be independent of A. However, no
matter what value h(x)takes, we always have jh(x)j+jh(x) 1j1. Hence for any predictor h:X!R:
#1,m0(h) +#1,m1(h) =1
2jh(0) 0j+1
2jh(1) 0j+1
2jh(0) 1j+1
2jh(1) 1j1
2+1
2=1.
This shows that for any fair predictor h, the sum of `1errors of hon both groups has to be at least 1. On the other hand,
there exists a trivial unfair algorithm that makes no error on both groups by also taking the protected attribute into
consideration:8x2f0, 1g,h(x) =0ifA=0elseh(x) =1.
3.1 The Cost of Statistical Parity under Noiseless Setting
The example in the previous section corresponds to a worst case where Y=A. On the other hand, it is also clear that
when the target variable Yis indeed independent of the protected attribute A, there will be no tension between statistical
parity and accuracy. The following theorem exactly characterizes the tradeoff between fairness and accuracy by taking
advantage of the relationship between YandA:
Theorem 3.1. LetbY=h(X)be a predictor. If bYsatisÔ¨Åes statistical parity, then 8p1,
#p,m0(bY) +#p,m1(bY)Wp(Y]m0,Y]m1). (5)
We provide a proof by picture to illustrate the high-level idea of the proof in Fig. 1. For the special case of p=1and
p=2, Theorem 3.1 gives the following lower bounds on the sum of MAE and MSE on both groups respectively:
Corollary 3.1. IfbYsatisÔ¨Åes statistical parity, then #1,m0(bY) +#1,m1(bY) jEm0[Y] Em1[Y]jand#2
2,m0(bY) +
#2
2,m1(bY)1
2jEm0[Y] Em1[Y]j2.
Remark First of all, the lower bound Wp(Y]m0,Y]m1)corresponds to a measure of the distance between the marginal
distributions of Yconditioned on A=0andA=1respectively. Hence when Ais independent of Y, we will have
Y]m0=Y]m1so that the lower bound gracefully reduces to 0, i.e., no essential tradeoff between fairness and accuracy.
On the other extreme, consider Y=cA, where c>0. In this case, Afully describes Yand it is easy to verify that
Wp(Y]m0,Y]m1) =c, which means the lower bound also takes into account the magnitude of the target variable Y. For
a protected attribute Athat takes more than 2 values, we could extend Theorem 3.1 by considering all possible pairwise
lower bounds and averages over them. Furthermore, the lower bound is sharp, in the sense that for every p1, there
exist problem instances that achieve the above lower bound, e.g., Example 3.1. To see this, consider the `perror for any
predictor h, we have
#p,m0(h) +#p,m1(h) =1
2jh(0) 0jp+1
2jh(1) 0jp1/p
+1
2jh(0) 1jp+1
2jh(1) 1jp1/p
1
2jh(0) 0j+1
2jh(1) 0j+1
2jh(0) 1j+1
2jh(1) 1j
1
2+1
2=1=Wp(Y]m0,Y]m1),
5Published in Transactions on Machine Learning Research (02/2023)
where the Ô¨Årst inequality follows from Jensen‚Äôs inequality and the fact that g(t) =t1/pforp1is concave over
t0. On the other hand, it can be readily veriÔ¨Åed that the fair predictor h(X) =1/2 attains the lower bound. As
another example, consider the following Gaussian case for p=2:
Example 3.2 (Gaussian case) .Fora2f0, 1g, let the marginal distribution X]mabe a standard Gaussian distribution
N(0,Id)and assume A?X. Fix w2Rdwithkwk=1, and construct Y0=wTX 1andY1=wTX+1. Now
for any regressor bY=h(X), due to the data-processing inequality, bY?AsobYis fair. However, consider the `2error
ofhon both groups:
#2,m0(h) +#2,m1(h) =E1/2
X[(h(X) Y0)2] +E1/2
X[(h(X) Y1)2]
EX[jh(X) Y0j] +EX[jh(X) Y1j]EX[jY0 Y1j] =2,
where the Ô¨Årst inequality is due to Jensen‚Äôs inequality. On the other hand, note that the distributions of Y0andY1are
N( 1, 1)andN(1, 1), respectively. The analytic formula (Givens et al., 1984, Proposition 7) for the W2distance
between two Gaussians N(m0,S0)andN(m1,S1)is
W2
2(N(m0,S0),N(m1,S1)) =km0 m1k2+Tr
S0+S1 2
S1/2
0S1S1/2
01/2
,
which shows that W2(Y0,Y1) =j 1 1j=2. Further, consider bY=h(X) =wTX, then
#2,m0(h) +#2,m1(h) =E1/2
X[(h(X) Y0)2] +E1/2
X[(h(X) Y1)2] =1+1=2.
Hence hachieves the lower bound and the lower bound is veriÔ¨Åed.
It is worth pointing out that the lower bound in Theorem 3.1 is algorithm-independent and it holds on the population
distribution. That being said, by using recent tail bounds (Lei et al., 2020; Weed et al., 2019) on the expected Wasserstein
distance between the empirical distributions and its population counterpart, it is not hard to extend Theorem 3.1 to
obtain a Ô¨Ånite sample high probability bound of Theorem 3.1:
Theorem 3.2. LetbY=h(X)be the predictor and ÀÜmbe an empirical distribution induced from a sample of size n
drawn from m. IfbYsatisÔ¨Åes statistical parity, then there exists an absolute constant c1>0such that for 0<d<1,
with probability at least 1 dover the draw of the sample,
#2,m0(bY) +#2,m1(bY)#1,m0(bY) +#1,m1(bY)W1(Y]ÀÜm0,Y]ÀÜm1) 
2c1+q
2 log(2/d)r
1
n. (6)
Remark It is possible to obtain better lower bounds for the `2error in Theorem 3.2, but that requires making
more assumptions on the underlying distribution m, e.g., strongly log-concave density. The Ô¨Årst term in the lower
bound, W1(Y]ÀÜm0,Y]ÀÜm1), could be efÔ¨Åciently computed from the data by solving a linear program (Cuturi & Doucet,
2014, Problem (3)). Furthermore, it is worth pointing out that the lower bound in Theorem 3.2 applies to all the
predictorsbYand is insensitive to the marginal distribution of A. As a comparison, let pa:=Prm(A=a), then
#p,m(bY) =p0#p,m0(bY) +p1#p,m1(bY). In this case, if the group ratio is imbalanced, the overall error #p,m(bY)could still
be small even if the minority group suffers a large error. Using Theorem 3.1, we can also bound the joint error over all
the population:
Corollary 3.2. LetbY=h(X)be a predictor. If bYsatisÔ¨Åes statistical parity, then 8p1, the joint error has the
following lower bound:
#p,m(bY)H0-1(A)Wp(Y]m0,Y]m1). (7)
Compared with the one in Theorem 3.1, the lower bound of the joint error in Corollary 3.2 additionally depends on the
zero-one entropy of A. In particular, if the marginal distribution of Ais skewed, then H0-1(A)will be small, which
means that fairness will not reduce the joint accuracy too much. In this case, even if Wp(Y]m0,Y]m1)is large, it might
seem like that the joint error #p,m(bY)need not be large. However, this is due to the fact that the price in terms of the drop
in accuracy is paid by the minority group. Our observation here suggests that the joint error #p,m(bY)is not necessarily
6Published in Transactions on Machine Learning Research (02/2023)
Figure 1: Proof by picture. Under the constraint of statistical parity, the predictor hfair()onm0andm1induces the
same predictive distributions over Y. Applying a triangle inequality (with Wp(,)) to the triangle in the right circle
and using the fact that the Wasserstein distance is a lower bound of the regression error completes the proof.
the objective to look at in high-stakes applications, since it naturally encodes the imbalance between different subgroups
into account. Instead, a more appealing alternative to consider is the balanced error rate :
Balanced Error Rate of bY:=1
2
#p,m0(bY) +#p,m1(bY)
, (8)
which applies balanced weights to both groups in the objective function. Clearly, (8)could be reduced to the so-called
cost-sensitive loss , where data from group a2f0, 1gis multiplied by a positive weight that is reciprocal to the group‚Äôs
population level, i.e., 1/ Pr (A=a).
Comparisons with Related Lower Bounds It is instructive to compare the above lower bound for the population error
with the one of (Chzhen et al., 2020a, Theorem 2.3), where the authors use a Wasserstein barycenter characterization to
give a lower bound on the special case of squared `2error(p=2)when the regressor can explicitly take the protected
attribute as its input. As a comparison, our results apply to the general `ploss. To provide a more formal and detailed
comparison, we Ô¨Årst state the theorem for the mean-squared error in the setting where the regressor has explicit access
to the protected attribute Afrom Chzhen et al. (2020a) (using adapted notation for consistency):
Theorem 3.3 (Chzhen et al. (2020a) Theorem 2.3) .Assume, for each a2f0, 1g, that the univariate measure Y]mahas
a density and let pa:=Pr(A=a). Then,
min
gsatisÔ¨Åes statistical parityE[(f(X,A) g(X,A))2] =minn√•
a2f0,1gpaW2
2(Y]ma,n),
where f(X,A)is the Bayes optimal regressor and nis a distribution over R.
Remark First, the quantity of interest in Theorem 3.3 is the discrepancy between a fair predictor g(,)and the
Bayes optimal predictor f(,). On the other hand, the cost we are interested in this work is the excess risk (c.f.
DeÔ¨Ånition 3.1) of a fair predictor. These two terms are not the same in general. However, in the noiseless setting, we
know that Y=f(X,A)and the excess risk reduces to the error #2,m. In this case, the costs of fairness in Theorem 3.1
and Theorem 3.3 has the following relationship:
E[(Y g(X,A))2] =√•
a2f0,1gpa#2
2,ma(bY)
=0
@√•
a2f0,1gpa#2
2,ma(bY)1
A0
@√•
a2f0,1gpa1
A
0
@√•
a2f0,1gppappa#2,ma(bY)1
A2
=#2
2,m(bY).
7Published in Transactions on Machine Learning Research (02/2023)
Furthermore, although in both Theorem 3.1 and Theorem 3.3 we require the predictor to be fair in the sense of statistical
parity, the class of feasible predictors in Theorem 3.3 is still larger than that of Theorem 3.1. To see this, note that in
Theorem 3.1, beyond asking for h()to be fair, the same attribute-blind predictor hhas to be used in both groups. On
the other hand, although the attribute-aware predictor g(,)is constrained to be fair, different predictors g(,a)could
be applied over different groups indexed by A=a. Last but not least, in the noiseless case with p=2, one can use
Theorem 3.3 to obtain Theorem 3.1 as follows. Let W:=W2(Y]m0,Y]m1).
√•
a2f0,1gpa#2
2,ma(bY) min
hsatisÔ¨Åes statistical parityE[(h(X) Y)2]
 min
gsatisÔ¨Åes statistical parityE[(g(X,A) Y)2] (g(,)has additional access to A)
= min
gsatisÔ¨Åes statistical parityE[(f(X,A) g(X,A))2](Noiseless setting, so Y=f(X,A))
=minn√•
a2f0,1gpaW2
2(Y]ma,n) (Theorem 3.3)
=min
t2[0,W]p0t2+p1(W t)2(Barycenter property of W2)
=p0p1W2.
Now, since the above inequality holds for every p0,p1such that p0+p1=1, we can choose p0andp1as follows.
p0=# 1
2,m0(bY)
# 1
2,m0(bY) +# 1
2,m1(bY),p1=# 1
2,m1(bY)
# 1
2,m0(bY) +# 1
2,m1(bY).
Under this choice, from p0#2
2,m0(bY) +p1#2
2,m1(bY)p0p1W2, we immediately obtain #2,m0(bY) +#2,m1(bY)W=
W2(Y]m0,Y]m1)as desired. Note that the above relationship between Theorem 3.1 and Theorem 3.3 only holds for
p=2under the noiseless assumption. It is not clear such relationships continue to hold under the noisy setting and
general p1.
Another related result in the literature is Theorem 3.1 of Chi et al. (2021), which we restate as follows:
Theorem 3.4 (Chi et al. (2021) Theorem 3.1) .LetbY=h(X)be a predictor, then
#2
2,m0(bY) +#2
2,m1(bY)1
2
W1(Y]m0,Y]m1) W1(h]m0,h]m1)
+2
,
where [t]+:=maxf0,tg.
Remark When h()satisÔ¨Åes the statistical parity exactly, the above lower bound reduces to1
2W1(Y]m0,Y]m1)2. Again,
Theorem 3.4 is not directly comparable to Theorem 3.1 due to the same reason that Theorem 3.1 and Theorem 3.3
are not directly comparable. However, we can compare Theorem 3.4 and Theorem 3.3 directly since both costs are
measured by the mean-squared error. To do so, realize that for the special case of W2withkk 2as the underlying
metric, we know that the Wasserstein barycenter lies on the Wasserstein geodesic between Y]m0andY]m1(Villani,
2009). Let n=arg min √•a2f0,1gpaW2
2(Y]ma,n), i.e., nis the Wasserstein barycenter. Now since W2(,)is a
metric and nlies on the geodesic, we know
W2(Y]m0,n) +W2(Y]m1,n) =W2(Y]m0,Y]m1).
By using this fact, it is straightforward to verify that the following chain of inequalities holds:
W2
2(Y]m0,n) +W2
2(Y]m1,n)1
2 
W2(Y]m0,n) +W2(Y]m1,n)2=1
2W2
2(Y]m0,Y]m1)1
2W2
1(Y]m0,Y]m1),
where the Ô¨Årst inequality is due to the AM-GM inequality and the last one is due to the monotonicity of the Wp(,)
metric. Hence, in the special case of p=2with mean-squared error, the lower bound in Theorem 3.3 is tighter than the
one in Theorem 3.4.
8Published in Transactions on Machine Learning Research (02/2023)
3.2 Extension to Approximate Fairness under Noisy Setting
In the previous section, we show that there is an inherent tradeoff between statistical parity and accuracy when a
predictor exactly satisÔ¨Åes statistical parity, and in particular this holds even if there is a perfect (unfair) regressor in both
groups, i.e., there is no noise in the underlying population distribution. However, as formulated in (2), in practice, we
often only ask for approximate fairness where the quality of the approximation is controlled by the slack variable e.
Furthermore, even without the fairness constraint, in most interesting problems we often cannot hope to Ô¨Ånd perfect
predictors for the regression problem of interest. Hence, it is natural to ask what is the tradeoff between fairness and
accuracy when our predictor only approximately satisÔ¨Åes fairness ( e-SP, DeÔ¨Ånition 2.3) over general distribution m?
In this section, we shall answer this question by generalizing our previous results to prove lower bounds on both the
sum of conditional and the joint target errors that also take the quality of such approximation into account. Due to
potential noise in the underlying distribution, we Ô¨Årst deÔ¨Åne the excess risk rp,m(bY)of a predictorbY, which corresponds
to the reducible error:
DeÔ¨Ånition 3.1 (Excess Risk) .LetbY=h(X)2Rbe a predictor. The `pexcess risk of bYis deÔ¨Åned as rp,m(bY):=
#p,m(bY) #
p,m, where #
p,m:=inff#p,m(f(X))is the optimal error over all measurable functions.
Assuming the inÔ¨Åmum is achievable, we use f
ito denote an optimal regressor without fairness constraint over
mi,i2f0, 1g, i.e., f
i2arg minf#p,mi(f(X)). Then we have the following hold:
Proposition 3.1. LetbY=h(X)be a predictor. Under Assumption 2.1, for p1, if there exists e>0such that
Wp(h]m0,h]m1)e, then
rp,m0(bY) +rp,m1(bY) Wp(f
0]m0,f
1]m1)
|{z}
distance between optimal unfair predictors across groups 2(#
p,m0+#
p,m1) e, (9)
andbYsatisÔ¨Åes 2p
Ce-SP.
Remark It is easy to verify that Proposition 3.1 is a generalization of the lower bound presented in Theorem 3.1:
when f
iare perfect predictors, we have Y]mi=f
i]miand#
p,mi=0, fori2f0, 1g. Hence in this case the excess risk
rp,mi(bY)reduces to the error #p,mi(bY). Furthermore, if e=0, i.e.,bYsatisÔ¨Åes the exact statistical parity condition, then
the lower bound (9)recovers the lower bound (5). As a separate note, Proposition 3.1 also implies that one can use
the Wasserstein distance between the predicted distributions across groups as a proxy to ensure approximate statistical
parity. This observation has also been shown in Dwork et al. (2012, Theorem 3.3) in classiÔ¨Åcation. Finally, similar to
Corollary 3.2, the lower bound in Proposition 3.1 for the excess risks could also be extended to give a lower bound for
the weighted joint risk when approximate statistical parity ( e-SP) is enforced as a constraint. The proof is exactly the
same as the one for Corollary 3.2 so we omit it here.
3.3 Individual Fairness, Accuracy Parity and the Wasserstein Distance
In the previous section, we show that the Wasserstein distance between the output distributions across groups could
be used as a proxy to ensure approximate statistical parity. Nevertheless, Theorem 3.1 and Proposition 3.1 show that
statistical parity is often at odds with the accuracy of the predictor, and in many real-world scenarios SP is insufÔ¨Åcient
to be used as a notion of fairness (Dwork et al., 2012, Section 3.1). Alternatively, in the literature, a separate notion
of fairness, known as individual fairness , has been proposed in Dwork et al. (2012). Roughly speaking, under the
framework of individual fairness, for the classiÔ¨Åcation task Tof interest at hand, the learner will have access to a
(hypothetical) task-speciÔ¨Åc metric dT(,)for determining the degree to which individuals are similar w.r.t. the task
T. We emphasize here that the metric dT(,)should be task-speciÔ¨Åc, and in practice, it is often hard (or infeasible)
to determine this task-speciÔ¨Åc metric. Nevertheless, in this section, we are mainly interested in understanding the
relationship between the notion of individual fairness and its connection to accuracy parity, where we use the Wasserstein
distance as a bridge to connect these two. In particular, we say that a predictor his individually fair if it treats similar
individuals (measured by dT(,)) similarly:
DeÔ¨Ånition 3.2 (Individual Fairness, (Dwork et al., 2012)) .For the task Twith task-speciÔ¨Åc metric dT(,), a regressor
hsatisÔ¨Åes r-individual fairness if 8x,x02X ,jh(x) h(x0)jrdT(x,x0).
9Published in Transactions on Machine Learning Research (02/2023)
Essentially, individual fairness puts a Lipschitz continuity constraint on the predictor with respect to the task-speciÔ¨Åc
metric dT(,). Note that in the original deÔ¨Ånition (Dwork et al., 2012, DeÔ¨Ånition 2.1) the authors use a general metric
dT(,)as a similarity measure between individuals, and the choice of such similarity measure is at the center of related
applications. In this section, we use kk in DeÔ¨Ånition 3.2 mainly for the purpose of illustration, but the following
results can be straightforwardly extended for any metric dT(,)that is upper bounded by the norm kk, i.e., there
exists c>0, such that8x,x02X,dT(x,x0)ckx x0k. Another notion of group fairness that has gained increasing
attention (Buolamwini & Gebru, 2018; Bagdasaryan et al., 2019; Chi et al., 2021) is accuracy parity :
DeÔ¨Ånition 3.3 (e-Accuracy Parity) .Given a joint distribution mand0e1, a regressorbY=h(X)satisÔ¨Åes
e-accuracy parity ifj#1,m0(bY) #1,m1(bY)je.
Accuracy parity calls for approximately equalized performance of the predictor across different groups. The following
proposition states the relationship between individual fairness, accuracy parity and the W1distance between the
distributions m0andm1of different groups:
Proposition 3.2. Ifh()isr-individually fair, then hsatisÔ¨Åesp
r2+1W1(m0,m1)-accuracy parity.
Together with Lemma A.1 in the appendix, Proposition 3.2 suggests that in order to achieve approximate accuracy
parity, one can constrain the predictor to be Lipschitz continuous while at the same time try to decrease the W1distance
between the distributions across groups, via learning representations. In the case where the groups are similar and
the Wasserstein distance is small, individual fairness provides some guidance towards approximate accuracy parity.
However, in cases where the groups are different (disjoint), representation learning becomes more important.
3.4 Fair Representations with Wasserstein Distance
From the previous discussion, we know that the Wasserstein distance between the predicted distributions and the
input distributions can be used to control both statistical parity and accuracy parity, respectively. Is there a way to
simultaneously achieve both goals? In this section we shall provide an afÔ¨Årmative answer to this question via learning
fair representations. The high-level idea is quite simple and intuitive: given input variable X, we seek to learn a
representation Z=g(X)such that W1(g]m0,g]m1)is small. If furthermore the predictor hacting on the representation
Zis individually fair, we can then hope to have small statistical and accuracy disparity simultaneously.
One potential beneÔ¨Åt of learning fair representations for statistical parity over other methods based on constrained
optimization (Agarwal et al., 2019) is that we can release the learned fair representations so that different kinds
of downstream tasks could build on top of it without worrying about the fairness constraint anymore. Due to the
data-processing inequality, no matter what predictors for the downstream tasks are placed over the released fair
representations, we can guarantee that the predictors on top of the released fair representations are also going to satisfy
the statistical parity constraint. As a comparison, for constrained-based optimization methods, one often needs to
establish a different objective function when working with a different downstream task.
Concretely, the following proposition says if the Wasserstein distance between feature distributions from two groups,
W1(g]m0,g]m1), is small, then as long as the predictor is individually fair, it also satisÔ¨Åes approximate statistical parity:
Proposition 3.3. LetZ=g(X)be the features from input X. IfW1(g]m0,g]m1)eandbY=h(Z)isr-Lipschitz,
thenbY= (hg)(X)veriÔ¨Åes 2p
Cre-SP.
In practice since we only have Ô¨Ånite samples from the corresponding distributions, we will replace all the distributions
with their corresponding empirical versions. Furthermore, instead of using the joint error as our objective function, as
we discussed in the previous section, we propose to use the balanced error rate instead:
min
g,khkLrmax
kfkL11
2 
#2,m0(hg) +#2,m1(hg)+tEg]m0[f(Z)] Eg]m1[f(Z)], (10)
where t>0is a hyperparameter that trades off the `perror and the Wasserstein distance and ris the Lipschitz constant
of the predictor. The above problem could be optimized using the gradient descent-ascent algorithm (Edwards &
Storkey, 2015; Zhang et al., 2018). To implement the Lipschitz constraints, we apply weight clipping to the parameters
of both the adversary as well as the target predictor. More speciÔ¨Åcally, we use the projected gradient descent algorithm
to ensure the `2norm of the discriminator and the target predictor are bounded by the preset values.
10Published in Transactions on Machine Learning Research (02/2023)
Comparisons with Related Fair Representation Learning Method One closely related approach that uses the
Wasserstein distance as a penalty term in representation learning is from Chi et al. (2021), where the authors also
formulated a minimax approach to encourage accuracy disparity between different groups. Compared with Eq. (10),
from a model perspective, the main difference between our formulation and the one in Chi et al. (2021) is that
the discriminator in Eq. (10) only takes the features Zfrom different groups as input whereas the corresponding
discriminator in Chi et al. (2021) takes both the features Zand the label Yas input. Because of this difference, the
motivations of these two approaches are quite different. In our case, we use the Wasserstein distance to penalize the
discrepancy between the marginal feature distributions in order to (approximately) achieve statistical parity, whereas
the Wasserstein metric used in Chi et al. (2021) is used to align the conditional feature distributions (conditioned on
the label Y) between different groups, in order to minimize the accuracy disparity. To encourage accuracy parity, in
Eq. (10), we also constrain the predictor to be Lipschitz continuous, following Proposition 3.2.
4 Experiments
Our theoretical results imply that even if there is no signiÔ¨Åcant drop in terms of the overall population error when a
model is built to satisfy the statistical parity, the minority group can still suffer greatly from the reduction in accuracy.
On the other hand, by using the balanced error rate as the objective function, we can mitigate the disparate drops in
terms of accuracy between these two groups. Furthermore, by minimizing the Wasserstein distance of the feature
distributions across groups, we can hope to achieve both approximate statistical and accuracy parity. To verify these
implications, we conduct experiments on a real-world benchmark dataset, the Law School dataset (Wightman, 1998), to
present empirical results with various metrics. We refer readers to Appendix B for further details about the dataset, our
pre-processing pipeline and the models used in the experiments.
Experimental Setup To demonstrate the effect of using Wasserstein distance to regularize the representations with
adversarial training, we perform a controlled experiment by Ô¨Åxing the baseline model to be a three hidden-layer
feed-forward network with ReLU activations, denoted as MLP. To verify the effect of the balanced error rate on reducing
the accuracy disparity, we further study two variants of it, where one is using the weighted joint error rate (Ô¨Årst row
in Table 1) as the objective function whereas the other (second row in Table 1) uses the balanced error rate as the
objective function. We use W-MLP to denote the model with Wasserstein constraint for representation learning. In the
experiment, all the other factors are Ô¨Åxed to be the same across these two methods, including learning rate, optimization
algorithm, training epoch, and also batch size. To see how the Wasserstein regularization affects the joint error, the
conditional errors as well as the statistical parity and accuracy parity, we vary the coefÔ¨Åcient tfor the adversarial loss
between 0.1, 1.0, 5.0 and 10.0. For each setup, we repeat the experiment 5 times and report both the mean and the
standard deviation.
To further study the role of the Lipschitz constant rin achieving statistical parity as well as reducing the accuracy
disparity, in the second set of experiments we Ô¨Åx the coefÔ¨Åcient t=0.01 and vary the value of rby changing the
weight clipping value of the model parameters of both the adversary as well as the target predictor. More speciÔ¨Åcally,
we vary the clipping value for the parameters between 0.01, 0.1, 1.0 and 10.0. Again, for each setup, we repeat the
experiment 5 times and report both the mean and the standard deviation.
Results and Analysis The experimental results are listed in Table 1. In the table, we use K(bY0,bY1)to denote the
Kolmogorov-Smirnov distance of the predicted distribution across groups, which is also the value of approximate
statistical parity. From the table, it is then clear that with increasing t, both the statistical disparity and the accuracy
disparity are decreasing. Interestingly, the overall error #m(sensitive to the distribution of A) and the sum of group
errors #m0+#m1(insensitive to the imbalance of A) only marginally increase. In fact, for t=1.0, we actually observed
better accuracy. We conjecture that the improved performance stems from the implicit regularization via weight clipping
of the target predictor. With t=10.0, the last row shows that this method could effectively reduce both the statistical
and accuracy disparity to a value very close to 0, although at the cost of increasing errors. Comparing the two variants
of MLP with weighted joint error and the balanced error rate, we can see that the balanced error rate helps to reduce
both the Kolmogorov-Smirnov distance of the predicted distribution across groups as well as the accuracy disparity, at
the price of a higher joint error rate as well as the sum of group errors.
The impact of the Lipschitz constant ris shown in Table 2. As can be observed from the table, as rdecreases, both the
Kolmogorov-Smirnov distance of the predicted distribution across groups and the accuracy disparity also decreases. In
11Published in Transactions on Machine Learning Research (02/2023)
Table 1: Fair representations with Wasserstein regularization on the Law School dataset with different values of the
regularization coefÔ¨Åcient t. We report the overall error, group-wise error, statistical disparity, and accuracy disparity.
t # m #m0+#m1K(bY0,bY1)j#m0(bY) #m1(bY)j
MLP (weighted joint error) N/A 0.0340.005 0.0690.011 0.2960.044 0.0110.004
MLP (balanced error rate) N/A 0.0370.005 0.0720.010 0.2830.045 0.0100.003
W-MLP 0.1 0.0340.004 0.0670.008 0.2220.037 0.0110.003
W-MLP 1.0 0.0300.000 0.0590.001 0.1160.032 0.0110.002
W-MLP 5.0 0.0340.002 0.0670.004 0.0840.073 0.0080.002
W-MLP 10.0 0.0350.001 0.0690.003 0.0480.059 0.0060.000
Table 2: Fair representations with Wasserstein regularization on the Law School dataset with different values of the
Lipschitz constant r. We report the overall error, group-wise error, statistical disparity, and accuracy disparity.
r # m #m0+#m1K(bY0,bY1)j#m0(bY) #m1(bY)j
W-MLP 10.0 0.0360.003 0.0700.006 0.1590.032 0.0110.005
W-MLP 1.0 0.0360.003 0.0710.006 0.1550.034 0.0110.005
W-MLP 0.1 0.0350.002 0.0690.005 0.0920.044 0.0080.003
W-MLP 0.01 0.0370.002 0.0730.003 0.0030.005 0.0060.003
particular, when ris set to be 0.01, K(bY0,bY1)can be mitigated to 0.003, a value that is very close to 0. This means that
adjusting the Lipschitz constant value rcan potentially have a larger impact than tin controlling the (approximate)
statistical parity. Note, however, in this case, both the joint weighted error as well as the balanced error rate are also
larger, showing evidence of the trade-off between accuracy and statistical parity again. To conclude, all the empirical
results are consistent with our theoretical Ô¨Åndings.
5 Related Work
Fair Regression Two central notions of fairness have been extensively studied, i.e., individual fairness and group
fairness. Dwork et al. (2012) deÔ¨Åned individual fairness as a Lipschitz constraint of the underlying (randomized)
algorithm. However, this deÔ¨Ånition requires a priori a distance metric to compute the similarity between pairs of
individuals, which is often hard to construct or design in practice. Group fairness is a statistical deÔ¨Ånition, and it
includes a family of deÔ¨Ånitions which essentially ask some statistical scores to be equalized between different subgroups.
Typical examples include statistical parity, equalized odds (Hardt et al., 2016), and accuracy parity (Buolamwini &
Gebru, 2018). In this work we focus on an extension of statistical parity to regression problems and study its theoretical
tradeoff with accuracy when the regressor cannot directly take the protected attribute as input during both training and
inference stages. We also investigate the relationship between individual fairness and accuracy parity and provide a
bound through the Wasserstein distance. The line of work on fair regression through regularization techniques dates at
least back to Calders et al. (2013), where the authors enforce a Ô¨Årst-order moment requirement between the predicted
distributions. More recent works include (Komiyama et al., 2018) that use coefÔ¨Åcient of determination as a notion of
fairness when there are multiple sensitive attributes. When the sensitive attribute is continuous, generalized deÔ¨Ånition
using the R√©nyi correlation coefÔ¨Åcient exists (Mary et al., 2019). In a recent work, Agarwal et al. (2019) proposed
a reduction approach from fair regression to a sequence of cost-sensitive minimization problems. Other approaches
include a two-stage recalibration procedure (Chzhen et al., 2020b) and robust optimization techniques (Narasimhan
et al., 2020). Our deÔ¨Ånition of statistical parity in the regression setting is stronger than the one of Calders et al. (2013),
which proposed to use the difference of means as the metric. When the output dimension is one, our deÔ¨Ånition also
coincides with the one proposed by Agarwal et al. (2019), which amounts to the Kolmogorov-Smirnov distance.
Tradeoff between Fairness and Accuracy Although it has long been empirically observed that there is an inherent
tradeoff between accuracy and statistical parity in both classiÔ¨Åcation and regression problems (Calders et al., 2009; Zafar
et al., 2015; Zliobaite, 2015; Berk et al., 2017; Corbett-Davies et al., 2017; Zhao et al., 2019a), precise characterizations
12Published in Transactions on Machine Learning Research (02/2023)
on such tradeoffs are less explored. Berk et al. (2017) deÔ¨Åned the price of fairness (PoF) under their convex framework
for linear and logistic regression problems as the ratio between the loss of the optimal (approximately) fair regressor
and the Bayes optimal loss. Under this PoF deÔ¨Ånition, the authors then explored the accuracy-fairness frontier by
changing the approximate coefÔ¨Åcient of the fairness constraint. Menon & Williamson (2018, Proposition 8) explored
such tradeoff in terms of the fairness frontier function under the context of cost-sensitive binary classiÔ¨Åcation. Zhao
& Gordon (2022) proved a lower bound on the joint error that has to be incurred by any fair algorithm satisfying
statistical parity. Our negative result is similar to that of Zhao & Gordon (2022) in nature, and could be understood
as a generalization of their results from classiÔ¨Åcation to regression. Chzhen et al. (2020a) and Le Gouic et al. (2020)
concurrently with each other derived an analytic bound to characterize the price of statistical parity in regression when
the learner can take the sensitive attribute explicitly as an input for `2loss. In this case, the lower bound is given by the
optimal transport distance from two group distributions to a common one, characterized by the W2barycenter. Our
results differ in that in our setting the learner cannot use the sensitive attribute directly as an input, and our results hold
for the general `ploss. Note that this is signiÔ¨Åcant because it is not clear how to extend the results to space with Wpas
a metric, since the proof depends on the use of the Pythagoras‚Äô decomposition, which only holds under the `2distance.
On the upside, under certain data generative assumptions of the sampling bias, there is a line of recent works showing
that fairness constraints could instead improve the accuracy of the predictor (Dutta et al., 2020; Blum & Stangl, 2020).
In particular, Blum & Stangl (2020) prove that if the observable data are subject to labeling bias, then the Equality of
Opportunity constraint could help recover the Bayes optimal classiÔ¨Åer. Note that this does not contradict our results,
since in this work we do not make any assumptions about the underlying training distributions.
6 Conclusion
In this paper, we show that when the target distribution differs across different demographic subgroups, any attribute-
blind fair algorithm in the statistical parity sense has to achieve a large error on at least one of the groups. In particular,
we give a characterization of such a tradeoff using the Wasserstein distance. On the other hand, we also establish a
connection between individual fairness and accuracy parity, where again, the accuracy disparity gap is characterized by
the Wasserstein distance. Besides the theoretical contributions, our analysis using Wasserstein distance also suggests a
practical algorithm for fair regression through learning representations for different demographic subgroups that are
close in the sense of Wasserstein distance. Empirical results on a real-world dataset also conÔ¨Årm our Ô¨Åndings.
Acknowledgements
We would like to thank the anonymous reviewers for their helpful comments in improving the technical presentation of
the paper. The work of H.Z. was supported in part by a Facebook Research Award and Amazon AWS Cloud Credit.
References
General data protection regulation. URL https://gdpr-info.eu/art-22-gdpr/ . [Online; accessed 13-May-
2021].
Alekh Agarwal, Miroslav Dudik, and Zhiwei Steven Wu. Fair regression: Quantitative deÔ¨Ånitions and reduction-based
algorithms. In International Conference on Machine Learning , pp. 120‚Äì129, 2019.
Martin Arjovsky, Soumith Chintala, and L√©on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875 , 2017.
Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate impact on model
accuracy. Advances in Neural Information Processing Systems , 32:15479‚Äì15488, 2019.
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine learning. NIPS Tutorial , 2017.
Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and
Aaron Roth. A convex framework for fair regression. arXiv preprint arXiv:1706.02409 , 2017.
Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. Data decisions and theoretical implications when adversarially
learning fair representations. arXiv preprint arXiv:1707.00075 , 2017.
13Published in Transactions on Machine Learning Research (02/2023)
Avrim Blum and Kevin Stangl. Recovering from biased data: Can fairness constraints improve accuracy? In Symposium
on Foundations of Responsible Computing (FORC) , volume 1, 2020.
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classiÔ¨Åca-
tion. In Conference on fairness, accountability and transparency , pp. 77‚Äì91. PMLR, 2018.
Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. Building classiÔ¨Åers with independency constraints. In 2009
IEEE International Conference on Data Mining Workshops , pp. 13‚Äì18. IEEE, 2009.
Toon Calders, Asim Karim, Faisal Kamiran, Wasif Ali, and Xiangliang Zhang. Controlling attribute effect in linear
regression. In 2013 IEEE 13th International Conference on Data Mining , pp. 71‚Äì80. IEEE, 2013.
Flavio P Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R Varshney.
Optimized pre-processing for discrimination prevention. In Proceedings of the 31st International Conference on
Neural Information Processing Systems , pp. 3995‚Äì4004, 2017.
Sourav Chatterjee. Lecture notes in stein‚Äôs method and applications, August 2007.
Jianfeng Chi, Yuan Tian, Geoffrey J Gordon, and Han Zhao. Understanding and mitigating accuracy disparity in
regression. In International Conference on Machine Learning , 2021.
Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil. Fair regression with
wasserstein barycenters. arXiv preprint arXiv:2006.07286 , 2020a.
Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil. Fair regression via plug-in
estimator and recalibration with statistical guarantees. Advances in Neural Information Processing Systems , 33:
19137‚Äì19148, 2020b.
Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost
of fairness. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining , pp. 797‚Äì806. ACM, 2017.
Nicolas Courty, R√©mi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal transportation
for domain adaptation. In Advances in Neural Information Processing Systems , pp. 3730‚Äì3739, 2017.
Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. In International conference on machine
learning , pp. 685‚Äì693. PMLR, 2014.
Sanghamitra Dutta, Dennis Wei, Hazar Yueksel, Pin-Yu Chen, Sijia Liu, and Kush Varshney. Is there a trade-off
between fairness and accuracy? a perspective using mismatched hypothesis testing. In International Conference on
Machine Learning , pp. 2803‚Äì2813. PMLR, 2020.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In
Proceedings of the 3rd innovations in theoretical computer science conference , pp. 214‚Äì226. ACM, 2012.
Harrison Edwards and Amos Storkey. Censoring representations with an adversary. arXiv preprint arXiv:1511.05897 ,
2015.
Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying
and removing disparate impact. In proceedings of the 21th ACM SIGKDD international conference on knowledge
discovery and data mining , pp. 259‚Äì268, 2015.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran√ßois Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning
Research , 17(1):2096‚Äì2030, 2016.
Alison L Gibbs and Francis Edward Su. On choosing and bounding probability metrics. International statistical review ,
70(3):419‚Äì435, 2002.
Clark R Givens, Rae Michael Shortt, et al. A class of wasserstein metrics for probability distributions. The Michigan
Mathematical Journal , 31(2):231‚Äì240, 1984.
14Published in Transactions on Machine Learning Research (02/2023)
Peter D Gr√ºnwald, A Philip Dawid, et al. Game theory, maximum entropy, minimum discrepancy and robust bayesian
decision theory. Annals of statistics , 32(4):1367‚Äì1433, 2004.
Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportunity in supervised learning. In Advances in neural
information processing systems , pp. 3315‚Äì3323, 2016.
Harry J Holzer and David Neumark. AfÔ¨Årmative action: What do we know? Journal of Policy Analysis and Management ,
25(2):463‚Äì490, 2006.
Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual inference. In International
conference on machine learning , pp. 3020‚Äì3029, 2016.
Fredrik D Johansson, Uri Shalit, Nathan Kallus, and David Sontag. Generalization bounds and representation learning
for estimation of potential outcomes and causal effects. arXiv preprint arXiv:2001.07426 , 2020.
James E Johndrow, Kristian Lum, et al. An algorithm for removing sensitive information: application to race-
independent recidivism prediction. The Annals of Applied Statistics , 13(1):189‚Äì220, 2019.
Faisal Kamiran and Toon Calders. Classifying without discriminating. In 2009 2nd International Conference on
Computer, Control and Communication , pp. 1‚Äì6. IEEE, 2009.
Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning through regularization approach. In
2011 IEEE 11th International Conference on Data Mining Workshops , pp. 643‚Äì650. IEEE, 2011.
Soheil Kolouri, Se Rim Park, Matthew Thorpe, Dejan Slepcev, and Gustavo K Rohde. Optimal mass transport: Signal
processing and machine-learning applications. IEEE signal processing magazine , 34(4):43‚Äì59, 2017.
Junpei Komiyama, Akiko Takeda, Junya Honda, and Hajime Shimao. Nonconvex optimization for regression with
fairness constraints. In International conference on machine learning , pp. 2737‚Äì2746. PMLR, 2018.
Thibaut Le Gouic, Jean-Michel Loubes, and Philippe Rigollet. Projection to fairness in statistical learning. arXiv
e-prints , pp. arXiv‚Äì2005, 2020.
Jing Lei et al. Convergence and concentration of empirical measures under wasserstein distance in unbounded functional
spaces. Bernoulli , 26(1):767‚Äì798, 2020.
Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. The variational fair autoencoder. arXiv
preprint arXiv:1511.00830 , 2015.
Kristian Lum and James Johndrow. A statistical framework for fair predictive algorithms. arXiv preprint
arXiv:1610.08077 , 2016.
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and transferable
representations. In International Conference on Machine Learning , pp. 3381‚Äì3390, 2018.
J√©r√©mie Mary, Cl√©ment Calauzenes, and Noureddine El Karoui. Fairness-aware learning for continuous attributes and
treatments. In International Conference on Machine Learning , pp. 4382‚Äì4391. PMLR, 2019.
Aditya Krishna Menon and Robert C Williamson. The cost of fairness in binary classiÔ¨Åcation. In Conference on
Fairness, Accountability and Transparency , pp. 107‚Äì118, 2018.
Harikrishna Narasimhan, Andrew Cotter, Maya Gupta, and Serena Wang. Pairwise fairness for ranking and regression.
InProceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , volume 34, pp. 5248‚Äì5255, 2020.
Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: generalization bounds and
algorithms. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 , pp. 3076‚Äì3085.
JMLR. org, 2017.
Sahil Verma and Julia Rubin. Fairness deÔ¨Ånitions explained. In 2018 IEEE/ACM International Workshop on Software
Fairness (FairWare) , pp. 1‚Äì7. IEEE, 2018.
15Published in Transactions on Machine Learning Research (02/2023)
C√©dric Villani. Optimal transport: old and new , volume 338. Springer, 2009.
Jonathan Weed, Francis Bach, et al. Sharp asymptotic and Ô¨Ånite-sample rates of convergence of empirical measures in
wasserstein distance. Bernoulli , 25(4A):2620‚Äì2648, 2019.
Linda F Wightman. Lsac national longitudinal bar passage study. lsac research report series. 1998.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness constraints:
Mechanisms for fair classiÔ¨Åcation. arXiv preprint arXiv:1507.05259 , 2015.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In International
Conference on Machine Learning , pp. 325‚Äì333, 2013.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning. In
Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society , pp. 335‚Äì340. ACM, 2018.
Han Zhao and Geoffrey J Gordon. Inherent tradeoffs in learning fair representations. In Advances in neural information
processing systems , 2019.
Han Zhao and Geoffrey J. Gordon. Inherent tradeoffs in learning fair representations. Journal of Machine Learning
Research , 23(57):1‚Äì26, 2022. URL http://jmlr.org/papers/v23/21-1427.html .
Han Zhao, Amanda Coston, Tameem Adel, and Geoffrey J. Gordon. Conditional learning of fair representations. arXiv
preprint arXiv:1910.07162 , 2019a.
Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant representations for
domain adaptation. In International Conference on Machine Learning , pp. 7523‚Äì7532, 2019b.
Indre Zliobaite. On the relation between accuracy and fairness in binary classiÔ¨Åcation. arXiv preprint arXiv:1505.05723 ,
2015.
16Published in Transactions on Machine Learning Research (02/2023)
A Missing Proofs
In this section we provide all the missing proofs in the main text. For the ease of the readers, in what follows we shall
Ô¨Årst restate the theorems that appear in the main text and then provide the corresponding proofs.
A.1 Proofs of Theorem 3.1 and Corollary 3.1
Theorem 3.1. LetbY=h(X)be a predictor. If bYsatisÔ¨Åes statistical parity, then 8p1,
#p,m0(bY) +#p,m1(bY)Wp(Y]m0,Y]m1). (5)
Proof. First, realize that Wp(,)is a metric of probability distributions, the following chain of triangle inequalities
holds:
Wp(Y]m0,Y]m1)Wp(Y]m0,h]m0) +Wp(h]m0,h]m1) +Wp(h]m1,Y]m1).
Now due to the assumption that bY=h(X)is independent of A, the second term, Wp(h]m0,h]m1), is 0, leading to:
Wp(Y]m0,Y]m1)Wp(Y]m0,h]m0) +Wp(h]m1,Y]m1). (11)
Next, for a2f0, 1g, by deÔ¨Ånition of the Wasserstein distance,
Wp(Y]ma,h]ma) =
infgEg[jY bYjp]1/p

Ema[jY bYjp]1/p
=#p,ma(bY), (12)
where we use the fact that the pushforward distribution of maunder his a particular coupling between YandbY
to establish the above inequality. Applying the inequality (12) for both a=0anda=1and combining it with
inequality (11) completes the proof. 
Corollary 3.1. IfbYsatisÔ¨Åes statistical parity, then #1,m0(bY) +#1,m1(bY) jEm0[Y] Em1[Y]jand#2
2,m0(bY) +
#2
2,m1(bY)1
2jEm0[Y] Em1[Y]j2.
Proof. We Ô¨Årst prove the Ô¨Årst inequality in Corollary 3.1. Apply Theorem 3.1 by setting p=1. Let Id:Y!Y be the
identity map, i.e., Id(y) =y,8y2R. Clearly Id()is 1-Lipschitz. Using the sup characterization of the Wasserstein
distance, we have:
Em0[jbY Yj] +Em1[jbY Yj]W1(Y]m0,Y]m1) (Theorem 3.1)
=sup
kfkL1Z
f d(Y]m0) Z
f d(Y]m1)
Z
Idd(Y]m0) Z
Idd(Y]m1)(Id is 1-Lipschitz)
=Z
Y dm0 Z
Y dm1(Change of Variable)
=jEm0[Y] Em1[Y]j, (13)
where the second to last equation follows from the deÔ¨Ånition of pushforward distribution.
17Published in Transactions on Machine Learning Research (02/2023)
To prove the second inequality in Corollary 3.1, again set p=2in Theorem 3.1 and realize that for a,b, we have
a2+b2(a+b)2/2by the AM-GM inequality. Hence when p=2, we have
Em0[jbY Yj2] +Em1[jbY Yj2]E2
m0[jbY Yj] +E2
m1[jbY Yj] (Jensen‚Äôs inequality)
2 
Em0[jbY Yj] +Em1[jbY Yj]
2!2
(AM-GM inequality)
=1
2
Em0[jbY Yj] +Em1[jbY Yj]2
1
2jEm0[Y] Em1[Y]j2. (Eq. (13))
Note that for the last equation we apply the lower bound Em0[jbY Yj] +Em1[jbY Yj]jEm0[Y] Em1[Y]jwe
proved in the Ô¨Årst inequality. 
A.2 Proof of Theorem 3.2
Before we provide the proof of Theorem 3.2, we Ô¨Årst recall some useful results about the Wasserstein distance (Weed
et al., 2019; Lei et al., 2020).
Proposition A.1 (Proposition 20, (Weed et al., 2019)) .For all n0andp1, letÀÜmbe an empirical distribution
induced from mwith sample size n. Then,
Pr(Wp
p(m,ÀÜm)EWp
p(m,ÀÜmn) +t)exp( 2nt2). (14)
Proposition A.1 gives a concentration inequality of Wp
p(,)around its mean. Note that the expectation in (14) is over
the draw of the sample of size n. This inequality is particularly useful when p=1since it reduces to W1(,)and gives
a convergence rate of O(1/pn).
The following theorem is a special case of (Lei et al., 2020, Theorem 3.1), which bounds the rate of EWp(m,ÀÜmn):
Theorem A.1 (Theorem 3.1, (Lei et al., 2020)) .LetÀÜmbe an empirical distribution induced from mwith sample size n,
p1and recall thatY= [ 1, 1]. Then
EWp(Y]m,Y]ÀÜm)cpn 1
2p, (15)
where cpis a positive constant that only depends on p.
Again, the interesting case here is when p=1, which gives the same rate of O(1/pn)that coincides with the one in
Proposition A.1.
Theorem 3.2. LetbY=h(X)be the predictor and ÀÜmbe an empirical distribution induced from a sample of size n
drawn from m. IfbYsatisÔ¨Åes statistical parity, then there exists an absolute constant c1>0such that for 0<d<1,
with probability at least 1 dover the draw of the sample,
#2,m0(bY) +#2,m1(bY)#1,m0(bY) +#1,m1(bY)W1(Y]ÀÜm0,Y]ÀÜm1) 
2c1+q
2 log(2/d)r
1
n. (6)
Proof. We Ô¨Årst prove the Ô¨Ånite sample lower bound w.r.t. the `1error. Realize that W1(,)is a metric, the triangle
inequality gives us
W1(Y]ÀÜm0,Y]ÀÜm1)W1(Y]ÀÜm0,Y]m0) +W1(Y]m0,Y]m1) +W1(Y]m1,Y]ÀÜm1).
Combined with Theorem 3.1, the above inequality leads to
#1,m0(bY) +#1,m1(bY)W1(Y]ÀÜm0,Y]ÀÜm1)  
W1(Y]ÀÜm0,Y]m0) +W1(Y]m1,Y]ÀÜm1)
.
18Published in Transactions on Machine Learning Research (02/2023)
Hence it sufÔ¨Åces if we could provide high probability bound to further lower bound W1(Y]mi,Y]ÀÜmi), fori2f0, 1g.
To this end, we Ô¨Årst apply Proposition A.1 with p=1: let exp( 2nt2) = d/2and solve for t, we have t=p
log(2/d)/2n, which means that with probability at least 1 d/2,
W1(Y]ÀÜmi,Y]mi)EW1(Y]ÀÜmi,Y]mi) +r
log(2/d)
2n
cpr
1
n+r
log(2/d)
2n. (Theorem A.1)
Now apply the above inequality twice, one for i2f0, 1g. With a union bound, we have shown that w.p. 1 d,
#1,m0(bY) +#1,m1(bY)W1(Y]ÀÜm0,Y]ÀÜm1) 
2c1+q
2 log(2/d)r
1
n.
To prove the second lower bound w.r.t. the `2error, simply realize that #2,mi(bY)#1,mi(bY)fori2f0, 1g, which
completes the proof. 
A.3 Proof of Corollary 3.2
Corollary 3.2. LetbY=h(X)be a predictor. If bYsatisÔ¨Åes statistical parity, then 8p1, the joint error has the
following lower bound:
#p,m(bY)H0-1(A)Wp(Y]m0,Y]m1). (7)
Proof. To simplify the notation used in the proof, deÔ¨Åne #:=#p,m(bY),#0:=#p,m0(bY)and#1:=#p,m1(bY). Let
pa:=Prm(A=a). By Theorem 3.1, we know that #0+#1Wp(Y]m0,Y]m1). By deÔ¨Ånition of the joint error:
#=p0#0+p1#1minfp0,p1g(#0+#1)H0-1(A)Wp(Y]m0,Y]m1). 
A.4 Proof of Proposition 3.1
As a comparison to the Kolmogorov-Smirnov distance, the W1distance between distributions over Rcould be
equivalently represented as:
Proposition A.2 (Gibbs & Su (2002)) .For two distributions m,m0overR,W1(m,m0) =R
RjFm(z) Fm0(z)jdz.
Proposition A.2 was stated as a fact without proof in (Gibbs & Su, 2002), but it is not hard to see that it could be proved
using the equivalent characterization of W1in(4)by changing the integral variable. Furthermore, in regression if both
mandm0are continuous distributions, then the following well-known result (Chatterjee, 2007, Lemma 2) serves as a
bridge to connect the Wasserstein distance W1(,)and the Kolmogorov-Smirnov distance K(,):
Lemma A.1. If there exists a constant Csuch that the density of m0(w.r.t. the Lebesgue measure l) is universally
bounded above, i.e., kdm0/dlk¬•C, then K(m,m0)2p
CW1(m,m0).
Using Kolmogorov-Smirnov distance, the constraint in the optimization problem (2)could be equivalently expressed as
K(h]m0,h]m1)e. Now with Lemma A.1, we are ready to prove Proposition 3.1:
Proposition 3.1. LetbY=h(X)be a predictor. Under Assumption 2.1, for p1, if there exists e>0such that
Wp(h]m0,h]m1)e, then
rp,m0(bY) +rp,m1(bY) Wp(f
0]m0,f
1]m1)
|{z}
distance between optimal unfair predictors across groups 2(#
p,m0+#
p,m1) e, (9)
andbYsatisÔ¨Åes 2p
Ce-SP.
19Published in Transactions on Machine Learning Research (02/2023)
Proof. First, for a2f0, 1g, by deÔ¨Ånition of the Wasserstein distance, for any predictor bY=h(X):
Wp(Y]ma,h]ma) =
infgEg[jY bYjp]1/p

Ema[jY bYjp]1/p
=#p,ma(bY), (16)
Applying the above inequality to both handf
a, we have:
Wp(h]ma,Y]ma) +Wp(Y]ma,f
a]ma)#p,ma(bY) +#p,ma(f
a(X)) = #p,ma(bY) +#
p,ma,8a2f0, 1g. (17)
On the other hand, by the triangle inequality,
Wp(h]m0,h]m1) +√•
a2f0,1gWp(h]ma,Y]ma) +Wp(Y]ma,f
a]ma)Wp(f
0]m0,f
1]m1).
Now by the assumption Wp(h]m0,h]m1)eand Eq. (17), we have:
e+√•
a2f0,1g#p,ma(bY) +#
p,maWp(f
0]m0,f
1]m1).
By the deÔ¨Ånition of the excess risk, rearranging and subtracting 2√•a2f0,1g#
p,mafrom both sides of the inequality then
completes the proof of the Ô¨Årst part.
To show thatbYis2p
Ce-SP, Ô¨Årst note that bY=h(X)ist-SP iff K(h]m0,h]m1)t. Now apply Lemma A.1, under the
assumption that Wp(h]m0,h]m1)e:
K(h]m0,h]m1)2q
CW 1(h]m0,h]m1) (Lemma A.1)
2q
CW p(h]m0,h]m1) (Monotonicity of the Wp(,))
2p
Ce,
completing the proof. 
A.5 Proof of Proposition 3.2
Proposition 3.2. Ifh()isr-individually fair, then hsatisÔ¨Åesp
r2+1W1(m0,m1)-accuracy parity.
Proof. DeÔ¨Åne g(X,Y):=jbY Yj=jh(X) Yj. We Ô¨Årst show that if h(X)isr-Lipschitz, then g(X,Y)isp
r2+1-Lipschitz: for8x,y,x,x0:
jg(x,y) g(x0,y0)j=jh(x) yj jh(x0) y0j
jh(x) h(x0) y+y0j (Triangle inequality)
jh(x) h(x0)j+jy y0j
rkx x0k+jy y0j (hisr-Lipschitz)
q
r2+1q
kx x0k2+jy y0j2 (Cauchy-Schwarz)
=q
r2+1k(x,y) (x0,y0)k.
Letr0:=p
r2+1. Now consider the error difference:
j#1,m0(bY) #1,m1(bY)j=jEm0[jh(X) Yj] Em1[jh(X) Yj]j
=jEm0[g(X,Y)] Em1[g(X,Y)]j
sup
kg0kLr0jEm0[g0(X,Y)] Em1[g0(X,Y)]j
=r0sup
kg0kL1jEm0[g0(X,Y)] Em1[g0(X,Y)]j
=r0W1(m0,m1), (Kantorovich duality)
which completes the proof. 
20Published in Transactions on Machine Learning Research (02/2023)
A.6 Proof of Proposition 3.3
Proposition 3.3. LetZ=g(X)be the features from input X. IfW1(g]m0,g]m1)eandbY=h(Z)isr-Lipschitz,
thenbY= (hg)(X)veriÔ¨Åes 2p
Cre-SP.
Proof. We Ô¨Årst show that W1((hg)]m0,(hg)]m1)is small if hisr-Lipschitz. To simplify the notation, we deÔ¨Åne
m0
0:=g]m0andm0
1:=g]m1. Consider the dual representation of the Wasserstein distance:
W1(h]m0
0,h]m0
1) = sup
kf0kL1Z
f0d(h]m0
0) Z
f0d(h]m0
1)(Kantorovich duality)
=sup
kf0kL1Z
f0h dm0
0 Z
f0h dm0
1(Change of Variable formula)
sup
kfkLrZ
f dm0
0 Z
f dm0
1(hisr-Lipschitz)
=rW1(m0
0,m0
1)
=rW1(g]m0,g]m1)
re,
where the Ô¨Årst inequality is due to the fact that for kf0kL1,kf0hkLkf0kLkhkL=r. Applying Lemma A.1
toW1(h]m0
0,h]m0
1)then completes the proof. 
B Further Details about the Experiments
B.1 Dataset
The Law School dataset contains 1,823 records for law students who took the bar passage study for Law School
Admission.2The features in the dataset include variables such as undergraduate GPA, LSAT score, full-time status,
family income, gender, etc. In the experiment, we use gender as the protected attribute and undergraduate GPA as
the target variable. We use 80 percent of the data as our training set and the rest 20 percent as the test set. The data
distribution for different subgroups in the Law School dataset could be found in Figure 2. In the Law School dataset,
Pr(A=1) =0.452 , which is a quite balanced dataset. All the experiments are performed on a Titan 1080 GPU.
B.2 Network Architectures
We Ô¨Åx the baseline model to be a three hidden-layer feed-forward network with ReLU activations. The number of units
in each hidden layer is 50 and 20, respectively. The output layer corresponds to a linear regression model. This baseline
is denoted as MLP. To verify the effect of the balanced error rate on reducing the accuracy disparity, we further study
two variants of it, where one is using the weighted joint error rate (Ô¨Årst row in Table 1) as the objective function whereas
the other (second row in Table 1) uses the balanced error rate as the objective function. For learning with Wasserstein
regularization, the adversarial discriminator network takes the feature from the last hidden layer as input and connects it
to a hidden-layer with 10 units, followed by an auditor whose goal is to output a score function in distinguishing the
features from the two different groups. This model is denoted as W-MLP. Compared with MLP, the only difference of
W-MLP in terms of the objective function is that besides the `2loss for target prediction, the W-MLP also contains a
loss from the auditor to distinguish the sensitive attribute A.
B.3 Hyperparameters used in Experiments
In this section, we report the detailed hyperparameters used in our experiments to obtain the results in Table 1.
Throughout the experiments, we Ô¨Åx the learning rate to be 1.0 and use the same networks as well as random seeds. One
2We use the edited public version of the dataset which can be downloaded here: https://github.com/algowatchpenn/GerryFair/
blob/master/dataset/lawschool.csv
21Published in Transactions on Machine Learning Research (02/2023)
.050 .150 .250 .350 .450 .550 .650 .750 .850 .950
Y050100150200250300350# of ExamplesA=0
A=1
Figure 2: The data distributions of different groups in the Law School dataset.
important aspect in the implementation of the Wasserstein adversary is the choice of the clipping parameter for the
weights in the adversary network. The values used in our experiments are shown below in Table 3.
Table 3: Clipping parameters used in training the Wasserstein adversary.
t Clipping Value
W-MLP 0.1 0.1
W-MLP 1.0 1.0
W-MLP 5.0 5.0
W-MLP 10.0 10.0
22