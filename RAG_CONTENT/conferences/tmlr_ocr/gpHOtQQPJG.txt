Published in Transactions on Machine Learning Research (1/2025)
Optimization and Generalization Guarantees for
Weight Normalization
Pedro Cisneros-Velarde∗pacisne@gmail.com
VMware Research
Zhijie Chen lucmon@illinois.edu
University of Illinois Urbana-Champaign
Sanmi Koyejo sanmi@cs.stanford.edu
Stanford University
Arindam Banerjee arindamb@illinois.edu
University of Illinois Urbana-Champaign
Reviewed on OpenReview: https: // openreview. net/ forum? id= gpHOtQQPJG
Abstract
Weight normalization (WeightNorm) is widely used in practice for the training of deep neural
networks and modern deep learning libraries have built-in implementations of it. In this paper,
we provide the first theoretical characterizations of both optimization and generalization of
deep WeightNorm models with smooth activation functions. For optimization, from the form
of theHessian of the loss , we note that a small Hessian of the predictor leads to a tractable
analysis. Thus, we bound the spectral norm of the Hessian of WeightNorm networks and
show its dependence on the network width and weight normalization terms–the latter being
unique to networks without WeightNorm. Then, we use this bound to establish training
convergence guarantees under suitable assumptions for gradient decent. For generalization,
we use WeightNorm to get a uniform convergence based generalization bound, which is
independent from the width and depends sublinearly on the depth. Finally, we present
experimental results which illustrate how the normalization terms and other quantities of
theoretical interest relate to the training of WeightNorm networks.
1 Introduction
Weight normalization (WeightNorm) was first introduced by Salimans and Kingma (2016). The idea is to
normalize each weight vector—by dividing it by its Euclidean norm—in order to decouple its length from
its direction. Salimans and Kingma (2016) reported that WeightNorm is able to speed-up the training of
neural networks, as an alternative to the then recently introduced and still widely-used batch normalization
(BatchNorm) (Ioffe and Szegedy, 2015). WeightNorm is different from BatchNorm because it is independent
from the statistics of the batch sample being used at the gradient step. It is also different from another
class of normalization, called layer normalization (LayerNorm) (Ba et al., 2016) in that it does not entail the
normalization of any activation function of the neural network. Thus WeightNorm also has less computational
overhead because it does not require the computation and storage of additional mean and standard deviation
statistics as the other methods do. However, its empirical testing performance (or accuracy) has often been
found to not be as good as other normalization methods by itself, and thus various versions have been
formulated to improve its performance (Salimans and Kingma, 2016; Huang et al., 2017; Qiao et al., 2020).
Nevertheless, WeightNorm has been one of the most popular normalization methods since its introduction,
and as a result current machine learning libraries include built-in implementations of it, e.g., PyTorch 2.0.
∗Corresponding author.
1Published in Transactions on Machine Learning Research (1/2025)
Paralleltotheempiricaldevelopmentofnormalizationmethods, recentyearshaveseenadvancesintheoretically
understanding convergence of gradient descent (GD) and variants for deep learning models (Du et al., 2019;
Allen-Zhu et al., 2019; Zou et al., 2020; Nguyen, 2021; Liu et al., 2022; Banerjee et al., 2023). However, to
the best of our knowledge, no prior work has focused on providing formal optimization and generalization
guarantees for deep models with WeightNorm. In this paper, our contribution is to study the effect of
WeightNorm on both training and generalization from a theoretical perspective. In particular, we focus on
smooth networks—deep models with smooth activations—a setting which has become increasingly popular in
recent years (Du et al., 2019; Huang and Yau, 2020; Liu et al., 2022; Banerjee et al., 2023).
Training a neural network is a non-convex optimization problem, and establishing optimization guarantees
using GD requires analyzing the empirical loss used for training. Particularly, our analysis is based on the
second-order Taylor expansion of the empirical loss, for which we require bounds on its Hessian and gradient.
We observe that it is possible to bound the empirical loss Hessian by using a bound on the Hessian of the
network or predictor . Thus, our first technical contribution (Section 4) is a characterization of the spectral
norm of the Hessian of a smooth WeightNorm neural network. Similar to networks without WeightNorm (Liu
et al., 2020; Banerjee et al., 2023), our bound decreases with width, but unlike networks without WeightNorm,
our bound also decreases with the minimum weight vector norm across all hidden layers. A similar dependence
appears when bounding the norm of the empirical loss gradient, which follows from bounding the norm of
the predictor gradient (with respect to the network weights). Now, towards establishing the generalization of
WeightNorm networks, we obtain a bound on the output of the predictor which does not have any dependence
on the minimum weight vector norm. Similarly, the Lipschitz constant of the network does not have such a
dependency either. These results are used to subsequently establish a uniform convergence bound based on
the Rademacher complexity of WeightNorm networks (Section 6). Remarkably, all of our bounds used for
establishing both optimization and generalization guarantees have at most a polynomial dependence on the
depth of the network.
Our second technical contribution (Section 5) is to provide optimization guarantees for the training of smooth
deep networks with WeightNorm by GD under suitable assumptions. Using our previously derived bounds, we
use the restricted strong convexity (RSC) approach recently introduced by Banerjee et al. (2023) to establish
sufficient conditions that ensure a decrease on the value of the loss function with respect to its minimum
value. In particular, one such condition relies on an inequality (lower bound) on the quantity ∥∇L∥2
2/L,
whereLis the empirical loss and ∇Lits gradient. This inequality benefits from a larger width as well as
a larger minimum weight vector norm–as opposed to another condition for networks without WeightNorm
which only benefits from a larger width (Banerjee et al., 2023). We relate our condition to the so-called
restricted Polyak-Lojasiewicz (PL) condition, showing that ours is milder.
Our third technical contribution (Section 6) is to provide generalization guarantees to smooth networks with
WeightNorm. We iteratively use WeightNorm properties and our network scaling to obtain a generalization
gap which is independent of the network width—our bound is simply O(√
L/√n), whereLis the depth and n
the sample size. Thus, WeightNorm allows for a natural non-exponential dependence on the depth compared
to networks without WeightNorm (Bartlett et al., 2017; Neyshabur et al., 2018; Golowich et al., 2018).
Our generalization guarantee uses a uniform convergence approach based on the Rademacher complexity of
WeightNorm networks.
Fourth, we present experiments on the training of WeightNorm networks (Section 7). We present numerical
evidence that the convergence rate improves for larger values of (i) the ratio ∥∇L∥2
2/L, and (ii) the minimum
weight vector norm. According to our theoretical results, larger values of these two quantities strengthen the
RSC property of the empirical loss, which then improves the optimization convergence rate.
2 Related work
WeightNorm and Other Normalizations WeightNorm was first proposed by Salimans and Kingma
(2016), after a year from the introduction of BatchNorm by Ioffe and Szegedy (2015). LayerNorm was proposed
in the same year by Ba et al. (2016), and has also become very popular (Xu et al., 2019). Extensions and
variations of WeightNorm have been introduced since then, for example, centered weight normalization (Huang
et al., 2017), orthogonal weight normalization (Huang et al., 2018), and weight standardization (Qiao et al.,
2Published in Transactions on Machine Learning Research (1/2025)
2020). Theoretical analysis of normalization techniques in general has caught an increasing recent interest.
We focus mostly on WeightNorm, but mention that BatchNorm has some theoretical work, e.g. (Santurkar
et al., 2018; Lian and Liu, 2019). The work (Arpit et al., 2019) uses a theoretical approach to better come
up with initialization schemes tailored for WeightNorm networks with ReLU activations. In contrast, our
work focuses in understanding the optimization (training) and generalization of WeightNorm networks with
smooth activations. Wu et al. (2020) study the effect of WeightNorm on the convergence of overparameterized
least squares problem (which translates to a normalization of the parameters to estimate). We, in contrast,
study the non-convex problem of optimizing WeighNorm networks. Dukler et al. (2020) present exponential
convergence guarantees on shallow ReLU networks with WeightNorm, whereas we study smooth deep networks
and present generalization too. We conclude by remarking that the field of normalization is growing in
proposing new normalization schemes and improving their understanding and application—we refer the reader
to the recent survey (Huang et al., 2023).
Training Guarantees of Neural Networks Given the increasingly vast literature, we refer the readers
to the surveys (Fan et al., 2021; Bartlett et al., 2021) for an overview of the field of gradient descent training
of neural networks. Deep ReLU networks were analyzed by Zou and Gu (2019); Zou et al. (2020); Allen-Zhu
et al. (2019); Nguyen (2021); Nguyen et al. (2021), whereas deep networks with smooth activations—our
case—were analyzed by Du et al. (2019); Nguyen and Mondelli (2020); Liu et al. (2022). Regarding the
convergence analysis of gradient descent, Du et al. (2019); Allen-Zhu et al. (2019); Zou and Gu (2019);
Zou et al. (2020); Liu et al. (2022) used the Neural Tangent Kernel (NTK) approach (Jacot et al., 2018);
whereas Banerjee et al. (2023) used the RSC approach. All training guarantees hold over a neighborhood
around the initialization point of the network’s weights.
Generalization of Neural Networks To the best of our knowledge, the first work in studying the
Rademacher complexity for neural networks was (Neyshabur et al., 2015). Then, the seminal work (Bartlett
et al., 2017) provided generalization bounds for neural networks considering Lipschitz activation functions.
The bound, obtained using a covering approach, depends exponentially on the depth of the network. The
generalization bound was tightened for ReLU activations in (Neyshabur et al., 2018) using a PAC-Bayes
approach, but the same exponential dependency persisted. Then, the work (Golowich et al., 2018) avoided
some exponential dependence on the depth and established conditions on the norm of the network’s weight
matrices in order to obtain generalization bounds independent of depth. Unlike our work, all the cited works
do not study WeightNorm networks—indeed, we show, through a suitable adaptation of such existing results,
that WeightNorm allows the generalization bound to depend on√
L, whereLis the depth. Finally, we remark
that our generalization bound estimates how well finding a solution to the empirical loss approximates finding
a solution to the loss under the real data distribution. A different problem in the literature is inductive
bias, i.e., how a specific optimization method, while training the network, may converge to a solution that
generalizes well. This was studied for WeightNorm under GD in (Morwani and Ramaswamy, 2022) and under
stochastic GD in (Li et al., 2022).
3 Problem setup: deep learning with WeightNorm
Consider a training set {(xi,yi)}n
i=1,xi∈X ⊆ Rd,yi∈Y⊆ R. For a suitable loss function ℓ, the goal
is to minimize the empirical loss: L(θ) =1
n/summationtextn
i=1ℓ(yi,ˆyi) =1
n/summationtextn
i=1ℓ(yi,f(θ;xi)), where the prediction
ˆyi:=f(θ;xi)is from a deep neural network with parameter vector θ∈Rpfor some positive integer p.
In our setting fis a feed-forward multi-layer (fully-connected) neural network with weight normalization
(WeightNorm), having depth Land hidden layers with widths ml,l∈[L] :={1,...,L}; and it is given by
α(0)(x) =x, α(l)
i(x) =ϕ
1√ml(W(l)
i)⊤
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2α(l−1)(x)
, i= 1,...,ml, l= 1,...,L ,
f(θ;x) =v⊤α(L)(x),(1)
3Published in Transactions on Machine Learning Research (1/2025)
whereα(l)
i(x)∈Ris thei-th entry of the activation function α(l)(x)∈Rml,W(l)
i∈Rml−1is the weight vector
corresponding to the transpose of the i-th row of the layer-wise weight matrix W(l)∈Rml×ml−1,l∈[L],
v∈RmLis the last layer vector, ϕ(·)is the smooth (pointwise) activation function. The total set of parameters
is represented by the parameter vector
θ:= (vec(W(1))⊤,..., vec(W(L))⊤,v⊤)⊤∈R/summationtextL
k=1mkmk−1+mL, (2)
withm0=d. The pre-activation function ˜α(l)∈Rmlhas itsi-th entry defined by α(l)
i(x) =ϕ(˜α(l)
i(x)),
l∈[L]. For simplicity, we assume that the width of all hidden layers is the same, i.e., ml=m,l∈[L], and so
θ∈Rdm+(L−1)m2+m. We remark that fis also called the predictor since it is the predictive output of the
neural network.
Define the pointwise loss ℓi:=ℓ(yi,·) :R→R+and denote its first- and second-derivative as ℓ′
i:=dℓ(yi,ˆyi)
dˆyi
andℓ′′
i:=d2ℓ(yi,ˆyi)
dˆy2
i. The particular case of square loss is ℓ(yi,ˆyi) = (yi−ˆyi)2. We denote the gradient and
Hessian off(·;xi) :Rp→Rby∇if:=∂f(θ;xi)
∂θand∇2
if:=∂2f(θ;xi)
∂θ2respectively. The gradient and Hessian
of the empirical loss w.r.t. θare given by
∂L(θ)
∂θ=1
nn/summationdisplay
i=1ℓ′
i∇if ,∂2L(θ)
∂θ2=1
nn/summationdisplay
i=1/bracketleftbig
ℓ′′
i∇if∇if⊤+ℓ′
i∇2
if/bracketrightbig
. (3)
We denote the gradient with respect to the input data xi,i∈[n], by∇xf(θ;xi) :=∂f(θ;x)
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=xifor any
θ∈Rp. Let∥·∥ 2denote the spectral norm for matrices and L2-norm for vectors. Finally, we introduce the
following convenient notation:
Definition 3.1 (Minimum weight vector norm)./vextenddouble/vextenddouble¯W/vextenddouble/vextenddouble
2=mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2. As an example of the
notation, let{W(l)
i}m,L
i=1,l=1and{W′(l)
i}m,L
i=1,l=1be two sets of hidden weights and define f(W(l)
i,W′(l)
i) :=
αW(l)
i+βW′(l)
ifor someα,β∈R, then/vextenddouble/vextenddouble¯f(W,W′)/vextenddouble/vextenddouble
2= mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleαW(l)
i+βW′(l)
i/vextenddouble/vextenddouble/vextenddouble
2.
We state the following two assumptions:
Assumption 1 (Activation function). The activation ϕis 1-Lipschitz, i.e., |ϕ′|≤1, andβϕ-smooth, i.e.,
|ϕ′′
l|≤βϕ.
Assumption 2 (Ouput layer and input data). We initialize the vector of the output layer v0∈Rmas a
unit vector, i.e.,∥v0∥2= 1. Further, we assume the input data satisfies: ∥xi∥2= 1,i∈[n].
The assumption ∥x∥2= 1is for convenient scaling. Normalization assumptions are common in the litera-
ture (Allen-Zhu et al., 2019; Oymak and Soltanolkotabi, 2020; Nguyen et al., 2021). The unit value for the
last layer’s weight norm at initialization is also for convenience, since our results hold under appropriate
scaling for any other constant in O(1).
We introduce some additional notation. For two vectors π,¯π∈Rq,cos(π,¯π)denotes the cosine of the
angle between πand ¯π. We define the Euclidean ball around πwith radius ρ > 0byBEuc
ρ(π) :=
{ˆπ∈Rq|∥ˆπ−π∥2≤ρ}.
Finally, we point out that the original WeightNorm paper by Salimans and Kingma (2016) also has a scalar
multiplying the normalized weight vectors W(l)
i//vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2,l∈[L]. Their work considers optimizing over such
scalars whereas we consider a fixed scaling for our analysis.
4 Characterizing WeightNorm bounds on the loss and predictor
To characterize both the optimization with gradient descent (GD) and generalization of WeightNorm networks,
we need bounds on the empirical loss and predictor, as well as on their gradients, and the Hessian of the
4Published in Transactions on Machine Learning Research (1/2025)
latter. For example, the optimization guarantees in Section 5 make use of the second-order Taylor expansion
of the empirical loss Lfor which we need to bound its Hessian and gradient. As seen in equation (3), the loss
Hessian contains terms related to the gradient and the Hessian of the predictor f, which we then need to
bound. Now, regarding our generalization guarantees in Section 6, we need to obtain a bound on the values
of the predictor in order to bound the generalization gap. We obtain this bound by using intermediate results
derived from both the proofs of the Hessian and empirical loss Lbounds.
All of these bounds are presented in this section, which we believe could also be of independent interest. The
proofs are found in Section A and Section B of the appendix.
Theorem 4.1 (Hessian bound for WeightNorm). Under Assumptions 1 and 2, for any θ∈Rpwith
v∈BEuc
ρ1(v0), and any xi,i∈[n], we have
/vextenddouble/vextenddouble∇2
θf(θ;xi)/vextenddouble/vextenddouble
2≤O
(1 +ρ1)L3(1√m+L2max{|ϕ(0)|,|ϕ(0)|2})
min/braceleftig/vextenddouble/vextenddouble¯W/vextenddouble/vextenddouble
2,/vextenddouble/vextenddouble¯W/vextenddouble/vextenddouble2
2/bracerightig
. (4)
Corollary 4.1 (Hessian bound for WeightNorm under ϕ(0) = 0).Under Assumptions 1 and 2, and
assuming that the activation function satisfies ϕ(0) = 0, we have for any θ∈Rpwithv∈BEuc
ρ1(v0), and any
xi,i∈[n],
/vextenddouble/vextenddouble∇2
θf(θ;xi)/vextenddouble/vextenddouble
2≤O
(1 +ρ1)L3
√mmin/braceleftig/vextenddouble/vextenddouble¯W/vextenddouble/vextenddouble
2,/vextenddouble/vextenddouble¯W/vextenddouble/vextenddouble2
2/bracerightig
. (5)
Remark 4.1 (Comparison to networks without WeightNorm). The recent works (Liu et al., 2020;
Banerjeeetal.,2023)analyzedtheHessianspectralnormboundforfeedforwardnetworkswithoutWeightNorm.
(Liu et al., 2020) presented an exponential dependence on the network’s depth L, whereas (Banerjee et al.,
2023) avoided exponential dependence upon the choice of the initialization variance for the hidden layers’
weights. We show that WeightNorm eliminates such exponential dependence for at most L5independent
from any initialization of the weights, which improves to L3whenϕ(0) = 0. Another important difference is
that our bound also depends on the inverse of the minimum weight vector norm—large enough values on the
weight vectors decrease the upper bound. Finally, our bound is defined over any value on the weights, and
not just over a neighborhood around the initialization point.
Remark 4.2 (Dependence on the activation function). Corollary 4.1 shows that the Hessian bound
becomes tighter when ϕ(0) = 0because of an additional1√mscaling. In contrast, the Hessian bounds by Liu
et al. (2020); Banerjee et al. (2023) have the scaling1√mindependent from the value of ϕ(0). The reason for
this difference is the use of different scale factors: while those two other works introduce an extra scale factor
1√mon the linear output layer, we do not. In our paper, we follow the neural network scaling as done in the
seminal work on the Neural Tangent Kernel by Jacot et al. (2018).
Lemma 4.1 (Predictor gradient bounds). Under Assumptions 1 and 2, for any θ∈Rpwithv∈BEuc
ρ1(v0),
and any xi,i∈[n], we have
∥∇θf(θ;xi)∥2≤ϱθ≤O/parenleftigg
(1 +L|ϕ(0)|√m)/parenleftigg
1 +√
L(1 +ρ1)/vextenddouble/vextenddouble¯W/vextenddouble/vextenddouble
2/parenrightigg/parenrightigg
(6)
whereϱ2
θ:= (1 +L|ϕ(0)|√m)2+ 4(1 +ρ1)2/summationtextL
l=11
∥¯W∥2
2/parenleftig
1√m+ (l−1)|ϕ(0)|/parenrightig2
, and
∥∇xf(θ;xi)∥2≤1 +ρ1. (7)
Corollary 4.2 (Predictor gradient bounds under ϕ(0) = 0).Under Assumptions 1 and 2, and assuming
that the activation function satisfies ϕ(0) = 0, we have for any θ∈Rpwithv∈BEuc
ρ1(v0), and any xi,i∈[n],
∥∇θf(θ;xi)∥2≤ϱθ≤O/parenleftigg
1 +√
L(1 +ρ1)√m/vextenddouble/vextenddouble¯W/vextenddouble/vextenddouble
2/parenrightigg
(8)
whereϱ2
θ:= 1 +4L(1+ρ1)2
m∥¯W∥2
2.
5Published in Transactions on Machine Learning Research (1/2025)
Remark 4.3 (The Lipschitz constant is the same for networks of any depth). Surprisingly, the
Lipschitz constant of the network (7)—unlike networks without WeightNorm—does not explicitly depend on
the depthLof the network nor its width m. It depends on the radius ρ1which can be set to be a constant
O(1). This is a direct effect of WeightNorm: when computing ∇xf(θ;xi), we use a chain-rule argument
that depends on the product of Jacobians of the output α(l)at layerlwith respect to the previous output
α(l−1), and we show each Jacobian has at most unit norm due to WeightNorm. Thus, no matter how deep
the network is, the final effect this product of Jacobians has on the Lipschitz constant of the network is just a
constant one.
Proposition 4.1 (Empirical loss and empirical loss gradient bounds). Consider the square loss.
Under Assumptions 1 and 2, the following inequality holds for any θ∈Rpwithv∈BEuc
ρ1(v0),
L(θ)≤φ≤O((1 +ρ1)2(1 +L2|ϕ(0)|2m)), (9)
whereφ:=2
n/summationtextn
i=1y2
i+ 2(1 +ρ1)2(1 +L|ϕ(0)|√m)2. Moreover,
∥∇θL(θ)∥2≤2/radicalbig
L(θ)ϱθ≤2ϱθ√φ, (10)
withϱθas in Lemma 4.1.
Corollary 4.3 (Empirical loss and empirical loss gradient bounds under ϕ(0) = 0).Consider the
square loss. Under Assumptions 1 and 2, and assuming that the activation function satisfies ϕ(0) = 0, the
following inequality holds for any θ∈Rpwithv∈BEuc
ρ1(v0),
L(θ)≤φ≤O((1 +ρ1)2), (11)
whereφ:=2
n/summationtextn
i=1y2
i+ 2(1 +ρ1)2. Moreover, the bound in equation (10)holds withϱθas in Corollary 4.2.
Remark 4.4 (Comparison to networks without WeightNorm). As in Remark 4.1, our bounds are
polynomial on Lirrespective of weight values, whereas previous results were exponential on L.
5 Optimization guarantees for WeightNorm
In this section we prove our training guarantees for WeightNorm networks under the square loss. We first
introduce two technical lemmas that will be used for our convergence analysis, defining the restricted strong
convexity (RSC) property and a smoothness-like property respectively. These two properties use the bounds
derived in Section 4. All proofs are found in Section C of the appendix.
Definition 5.1 (Qθ
κsets).Givenθ∈Rpandκ∈(0,1], defineQθ
κ:={ˆθ∈Rp||cos(ˆθ−θ,∇θL(θ))|≥κ}.
Lemma 5.1 (RSC for WeightNorm under Square Loss). For square loss, under Assumptions 1 and 2,
for everyθ′∈Qθ
κ∩BEuc
ρ2(θ)withθ∈Rpandv,v′∈BEuc
ρ1(v0),
L(θ′)≥L(θ) +⟨θ′−θ,∇θL(θ)⟩+αθ,θ′
2∥θ′−θ∥2
2, (12)
with
αθ,θ′=κ2
2∥∇θL(θ)∥2
2
L(θ)
−O
/parenleftigg
1 +√
L/vextenddouble/vextenddouble¯W/vextenddouble/vextenddouble
2/parenrightigg(1 +ρ2)(1 +ρ1)3L3A(1√m,L2,|ϕ(0)|)B(1,L,|ϕ(0)|√m)
min/braceleftig/vextenddouble/vextenddouble¯f(W′,W)ξ/vextenddouble/vextenddouble
2,/vextenddouble/vextenddouble¯f(W′,W)ξ/vextenddouble/vextenddouble2
2/bracerightig
,(13)
wheref(W′,W)ξ:=ξW′+ (1−ξ)Wfor someξ∈[0,1],A(1√m,L2,|ϕ(0)|) :=1√m+L2max{|ϕ(0)|,|ϕ(0)|2}
andB(1,L,|ϕ(0)|√m) := 1 +L|ϕ(0)|√m. We say that the empirical loss Lsatisfies the RSC property
w.r.t. (Qθ
κ∩BEuc
ρ2(θ),θ)wheneverαθ,θ′>0.
6Published in Transactions on Machine Learning Research (1/2025)
Corollary 5.1 (RSC for WeightNorm under Square Loss and ϕ(0) = 0).Consider the square
loss. Under Assumptions 1 and 2 and assuming that the activation function satisfies ϕ(0) = 0, for every
θ′∈Qθ
κ∩BEuc
ρ2(θ)withθ∈Rpandv,v′∈BEuc
ρ1(v0), the inequality in (13)becomes
αθ,θ′=κ2
2∥∇θL(θ)∥2
2
L(θ)
−O
/parenleftigg
1 +√
L√m/vextenddouble/vextenddouble¯W/vextenddouble/vextenddouble
2/parenrightigg
(1 +ρ2)(1 +ρ1)3L3
√mmin/braceleftig/vextenddouble/vextenddouble¯f(W′,W)ξ/vextenddouble/vextenddouble
2,/vextenddouble/vextenddouble¯f(W′,W)ξ/vextenddouble/vextenddouble2
2/bracerightig
(14)
using the same notation as in Lemma 5.1.
Remark 5.1 (The RSC parameter αθand prior work). Our RSC characterization in the left-hand
side of(13)depends on∥∇θL(θ)∥2
2
L(θ), whereas (Banerjee et al., 2023)—the work that introduced the RSC-based
optimization analysis—depends on ∥1
n/summationtextn
i=1∇θf(θ,xi)∥2
2. Moreover, the minimum weight vector norm
appears in the right-hand side since we used the bounds on Section 4.
Remark 5.2 (Connection with the restricted Polyak-Lojasiewicz (PL) inequality). The RSC
condition has the form∥∇θL(θ)∥2
2
L(θ)≥2µθ,θ′whereµθ,θ′depends on the parameters θandθ′, which are in turn
restricted to a specific set. This is a milder condition than the so-called restricted PL condition (Oymak and
Soltanolkotabi, 2019; Banerjee et al., 2023), which is satisfied whenever µθ,θ′is a constant independent from
bothθandθ′. Our RSC condition becomes even milder when ϕ(0) = 0(see Corollary 5.1), since now the
parameterµθ,θ′has the term 1/√mwhich decreases with the width.
Remark 5.3 (The right-hand side of the RSC parameter αθ).Whenϕ(0) = 0, Corollary 5.1 shows that
the right-hand side of (13)will have the scaling parameter1√mdue to the Hessian bound (Corollary 4.1).
Lemma 5.2 (Smoothness-like property for WeightNorm under Square Loss). For square loss,
under Assumptions 1 and 2, for every θ,θ′∈Rpwithv,v′∈BEuc
ρ1(v0),
L(θ′)≤L(θ) +⟨θ′−θ,∇θL(θ)⟩+βθ,θ′
2∥θ′−θ∥2
2, (15)
with
βθ,θ′≤O/parenleftigg
L4A(1,L2,|ϕ(0)|)(1 +ρ1)2/parenleftigg
1 +1
min{/vextenddouble/vextenddouble¯f(W′,W)ξ/vextenddouble/vextenddouble
2,/vextenddouble/vextenddouble¯f(W′,W)ξ/vextenddouble/vextenddouble2
2}/parenrightigg/parenrightigg
,(16)
wheref(W′,W)ξ:=ξW′+ (1−ξ)Wfor someξ∈[0,1],A(1,L2,|ϕ(0)|) = (1 +L2max{|ϕ(0)|,|ϕ(0)|2}m)2.
Corollary 5.2 (Smoothness-like property for WeightNorm under Square Loss and ϕ(0) = 0).
Consider the square loss. Under Assumptions 1 and 2 and assuming that the activation function satisfies
ϕ(0) = 0, for every θ,θ′∈Rpwithv,v′∈BEuc
ρ1(v0), the inequality in (16)becomes
βθ,θ′≤O/parenleftigg
1 +L3(1 +ρ1)2
√mmin{/vextenddouble/vextenddouble¯f(W′,W)ξ/vextenddouble/vextenddouble
2,/vextenddouble/vextenddouble¯f(W′,W)ξ/vextenddouble/vextenddouble2
2})/parenrightigg
(17)
using the same notation as in Lemma 5.2.
Remark 5.4 (A stronger notion of convexity and smoothness). If the expressions in equations (12)
and(15)hold for any θ,θ′∈Rpso thatαθ,θ′≡α >0andβθ,θ′≡β >0, then we have the definitions of
strong convexity and smoothness, respectively—moreover, it immediately follows thatα
β<1.
The following assumptions are useful for establishing our convergence analysis—indeed, as we shortly show, a
decrease on the loss value is guaranteed as long as these assumptions are satisfied.
Assumption 3 (Iterates’ conditions). Consider the iterate θt∈Rpwithvt∈BEuc
ρ1(v0)and that: (A3.1)
gradient descent (GD) update θt+1=θt−ηt∇L(θt)with learning rate ηt>0chosen appropriately so that
vt+1∈BEuc
ρ1(v0);(A3.2)ρ2is chosen so that θt+1∈BEuc
ρ2(θt).
7Published in Transactions on Machine Learning Research (1/2025)
The first statement in Assumption 3 ensures that gradient descent has an appropriate learning rate. The
second statement defines a ball around the current iterate that should cover the next iterate—in principle,
its radius could be defined after defining the learning rate of GD of the first assumption; more details in
Remark 5.7.
Finally, we present our main optimization result: a reduction on the empirical loss towards its minimum
value (with the last layer in the Euclidean set of radius ρ1) using gradient descent. This is proved by using
Lemmas 5.1 and 5.2 and an adaptation of (Banerjee et al., 2023, Theorem 5.3); see Section C.2 of the
appendix.
Theorem 5.1 (Global Empirical Loss Reduction for WeightNorm under Square Loss, (Baner-
jee et al., 2023, Theorem 5.3)). LetBt:=Qθtκ∩BEuc
ρ2(θt)∩{θ∈Rp|v∈BEuc
ρ1(v0)},θ∗∈
arginfθ∈{θ∈Rp|v∈BEucρ1(v0)}L(θ),θt∈arginfθ∈BtL(θ), andγt:=L(θt)−L(θ∗)
L(θt)−L(θ∗). Letαθt,θt,βθtbe as in Lem-
mas 5.1 and 5.2 respectively with βθt≡βθt,θt+1=βθt,θt−ηt∇θL(θt). Consider Assumptions 1, 2, and 3, and
the RSC property αθt,θt>0. Then, we have γt∈[0,1); and ifL(θt)̸=L(θ∗), then for gradient descent with
step sizeηt=1
βθt, we haveαθt,θt
βθt∈(0,1]and
L(θt+1)−L(θ∗)≤/parenleftbigg
1−αθt,θt
βθt(1−γt)/parenrightbigg
(L(θt)−L(θ∗)). (18)
The result in (18)implies training convergence as the number of iterations increase. Since the rate at which
the loss decreases is time- and weight-dependent in (18), it is difficult to establish the number of steps needed
for convergence to a specific loss value; however, this shows that our framework may cover cases where the
convergence rate is heterogeneous across iterations. Finally, we note that the rate is always guaranteed to be
less than one.
Remark 5.5 (Important differences with respect to networks without WeightNorm). Theorem 5.1
differentiates from (Banerjee et al., 2023, Theorem 5.3) in that (i) it is deterministic , i.e., it is not stated
as holding with high probability; (ii) does not have all the weights of the network defined over a particular
neighborhood (with the exception of the output linear layer). Indeed, these differences also hold for all the
previously derived results in Section 4 and Section 5. We summarize the comparison between our results
and those for networks without WeightNorm by Banerjee et al. (2023) in Table 1 from Section D of the
appendix.
Remark 5.6 (About the radius ρ2in Assumption 3). We prove in Proposition C.2 of the appendix that,
for the case of ϕ(0) = 0,∥θt+1−θt∥2≤ηt·O/parenleftbigg
(1 +ρ1)2/parenleftbigg
1 +√
L√m∥¯Wt∥2/parenrightbigg/parenrightbigg
. Note that ρ2as in Assumption 3
could in principle depend on t, and thus we could set ρ2to be equal to the upper bound just shown for
∥θt+1−θt∥2. Remarkably, ηtas chosen in Theorem 5.1 can be upper bounded by one1and if/vextenddouble/vextenddouble¯Wt/vextenddouble/vextenddouble
2has a
uniform lower bound across t(note that our experiments in Section 7 provide evidence for/vextenddouble/vextenddouble¯Wt/vextenddouble/vextenddouble
2≥/vextenddouble/vextenddouble¯W0/vextenddouble/vextenddouble
2),
then the upper bound of ∥θt+1−θt∥2becomes a constant independent of t, and so does ρ2to satisfy (A3.2)
in Assumption 3.
Remark 5.7 (About gradient descent in Assumption 3). Regarding (A3.1)in Assumption 3, we
remark that we are onlyconsidering that the weight vector of the output layer is inside a neighborhood of its
initialization value across iterations—no constraint exist on the weights from the hidden layers. Moreover,
the radiusρ1of this neighborhood region can be set to be any arbitrary constant of order Θ(1)or even a
polynomial of L; thus,ρ1is not assumed to be arbitrarily close to zero and the neighborhood is not restricted
to be arbitrarily small. These arguments imply that our optimization guarantees are not restricted to be in
the lazy training regime (Liu et al., 2020). We also point out that (A3.1)is a necessary assumption so that
the linear output of the neural network, which has no normalization, remains bounded. Finally, we remark
that(A3.1)is not stronger than other existing works where assumptions of closeness around initialization
points are needed even on the hidden weights; e.g., see (Liu et al., 2020; 2022; Banerjee et al., 2023).
1We can set βθtso that 1≤βθtfrom (17) and so ηt=1
βθt≤1.
8Published in Transactions on Machine Learning Research (1/2025)
6 Generalization guarantees for WeightNorm
We establish generalization bounds for WeightNorm networks. Our bound is based on a uniform convergence
argument by bounding the Rademacher complexity of functions of the form (1)as long as the last layer
vector vstays within a ball of radius ρ1—the same condition we used in our optimization analysis. The core
of the analysis relies on the use of a contraction-like property across the network layers similar to (Golowich
et al., 2018) and uses WeightNorm and the network’s scaling to avoid exponential dependence on the depth
and avoid any dependence on the width. All proofs are found in Section E of the appendix.
Consider the training set S={(xi,yi)∼D,i∈[n]}, whereDdenotes the true but unknwon distribution.
The training and population losses are respectively defined as
LS(θ) :=1
nn/summationdisplay
i=1ℓ(yi,f(θ;xi))andLD(θ) :=E(X,Y)∼D[ℓ(Y,f(θ;X))].
We added the subscript Sto the empirical loss notation to explicitely denote its dependence on the training
setS.
Theorem 6.1 (Generalization Bound for WeightNorm under Square Loss). Consider the square
loss and the training set S={(xi,yi)i.i.d.∼ D,i∈[n]}and|y|≤1for anyy∼Dywith probability one. Under
Assumptions 1 and 2, assuming the activation function satisfies ϕ(0) = 0, with probability at least (1−δ)
over the choice of the training data xi∼D x,i∈[n], for any WeightNorm network f(θ;·)of the form (1)with
any fixedθ∈Rpandv∈BEuc
ρ1(v0), we have
LD(θ)−LS(θ)≤4(2 +ρ1)(1 +ρ1)/radicalbig
2 log(2)L+ 1√n+ 2(1 + (1 + ρ1)2)/radicalbig
2 log(2/δ)√n. (19)
Remark 6.1 (About the activation function). Theorem 6.1, unlike our previous results in the paper,
requires the smooth activation function to satisfy ϕ(0) = 0. As mentioned earlier in Remarks 4.2 and 5.3,
the case when ϕ(0) = 0improves our Hessian bounds and thus improves our optimization conditions by
introducing a better dependence on the width m. We point out that commonly used activation functions
such as the hyperbolic tangent function tanhand Gaussian Error Linear Unit ( GELU) satisfy both ϕ(0) = 0
and Assumption 1.
Remark 6.2 (The independence from the width mand the dependence on depth L).The
generalizaton bound (19)does not explicitly depend on the network’s width, but explicitly depends on its
depth. Setting ρ1= Θ(1), a choice which does not negatively affect our optimization results, results in a
generalization bound of O(/radicalig
L
n). Effectively, for deeper networks, we would need to increase the number
of samples at least linearly on the depth to improve our generalization bound—in practice, the number of
samples is usually much larger than the network’s depth. This is a benign dependence that WeightNorm allows
compared to an exponential one found in networks without WeightNorm (Bartlett et al., 2017; Neyshabur
et al., 2018; Golowich et al., 2018).
7 Experimental results
We show empirical results to support our theory. In our experiments, the WeightNorm networks are fully
connected with two hidden layers of equal width m,tanhactivation function, and linear output layer. We do
empirical evaluations on CIFAR-10 (Krizhevsky, 2009) and MNIST (Deng, 2012). Because of computational
efficiency, we apply mini-batch stochastic gradient descent (SGD) with batch size 512 to optimize the
WeightNorm networks under mean squared loss. In our experiments we are only concerned about the training
of WeightNorm networks from an optimization perspective—the fact that the training error minimizes across
epochs is enough for our purposes. Therefore, using a squared loss with a linear output in our experiments
suffices—as opposed to cross-entropy loss with a softmax output layer. Our experiments were conducted on a
computing cluster with AMD EPYC 7713 64-Core Processor and NVIDIA A100 Tensor Core GPU.
In our theory, the convergence rate as in Theorem 5.1 is directly proportional to the RSC parameter αθt,θt,
and it benefits from a larger positive αθt,θt. According to equation (13),αθt,θtbecomes larger under two
9Published in Transactions on Machine Learning Research (1/2025)
0 25 50 75 100 125 150 175 200
Epochs0.3890.3900.3910.3920.393mini[m],l[L]W(l)
i,t2
1024
2048
(a)
0 25 50 75 100 125 150 175 200
Epochs0.9920.9930.9940.9950.9960.9970.9980.999L(t+1)/L(t)
0.0040.0060.0080.0100.0120.0140.0160.0180.020
L(t)2 / L(t)
1024
2048 (b)
Figure 1: Training neural networks for two different widths m∈{1024,2048}on the CIFAR-10 dataset, with
two hidden layers L= 2of same width, with learning rate 0.001, and weights initialized independently from a
uniform distribution [−0.5√m,0.5√m]. Each subfigure plots: (a): mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i,t/vextenddouble/vextenddouble/vextenddouble
2; (b):∥∇L(θt)∥2
2/L(θt); where
trepresents the number of iterations.
conditions: as the ratio ∥∇L(θt)∥2
2/L(θt)increases and/or the minimum weight vector norm mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i,t/vextenddouble/vextenddouble/vextenddouble
2
increases. Then, the relevant question is: Can we provide empirical evidence of these two conditions benefiting
convergence during training?
We provide an affirmative answer in our simulations. We present our experiments on CIFAR-10 in Figure 1
and on MNIST in Figure 2.
We first focus on the effect of the ratio ∥∇L(θt)∥2
2/L(θt)on the RSC parameter. In Figure 1(a), we show
that the values for the minimum weight vector stay nearly constant across epochs for both networks (in
reality, they increase in value very slowly), i.e., the second term in the RSC parameter αθt(see equation (13))
basically remains stable. Therefore, according to our theory, the convergence speed should mostly depend
on the ratio∥∇L(θt)∥2
2/L(θt), i.e., the first term in the RSC parameter αθt(see equation (13)). Indeed,
this is expressed in Figure 1(b): the curve of the ratio ∥∇L(θt)∥2
2/L(θt)has a decreasing trend towards the
end of training for both networks (starting at around epoch 75) while the curve of the loss ratio between
two consecutive epochs L(θt+1)/L(θt)has an increasing trend. Therefore, since an increase in the ratio
L(θt+1)/L(θt)corresponds to a decrease on convergence speed, both networks have a slower convergence than
they had at the initial epochs.
Next, we focus on the effect of the minimum weight vector norm on the RSC parameter. According to our
theory, if the two networks in Figure 1 have the same value for the ratio ∥∇L(θt)∥2
2/L(θt), the network with
the larger minimum weight vector norm will have a larger RSC parameter (see equation (13)), and so we
would expect such network to have a faster convergence. Indeed, we observe this in our experiment: when
the red solid curve crosses the dashed red curve (around epochs 70 and 115) in Figure 1(b), the two networks
have the same value for ∥∇L(θt)∥2
2/L(θt). At this crossing, Figure 1(b) shows that the network with larger
width—and so with larger minimum weight vector norm—converges faster afterwards (i.e., the loss ratio
L(θt+1)/L(θt)for the green dashed curve has smaller values than the green solid curve).
We present similar results on the MNIST dataset in Figure 2. For example, similar to Figure 1, the minimum
weight vector norms for both networks in Figure 2 are practically stable, which results in the convergence
speed changing according to the changes in the ratio ∥∇L(θt)∥2
2/L(θt). Interestingly, the two curves of the
ratio∥∇L(θt)∥2
2/L(θt)do not cross, but they become very close on value around epoch 25. After this, we
10Published in Transactions on Machine Learning Research (1/2025)
0 100 200 300 400 500
Epochs1.20751.21001.21251.21501.21751.22001.22251.2250mini[m],l[L]W(l)
i,t2
512
1024
(a)
0 100 200 300 400 500
Epochs0.8250.8500.8750.9000.9250.9500.9751.000L(t+1)/L(t)
0.050.100.150.200.250.300.35
L(t)2 / L(t)
512
1024 (b)
Figure 2: Training neural networks for two different widths m∈{512,1024}on the MNIST dataset, with
two hidden layers L= 2of same width, with learning rate 0.001 and the weights are initialized with a
uniform distribution [−5√m,5√m]. Each subfigure plots: (a): mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i,t/vextenddouble/vextenddouble/vextenddouble
2; (b):∥∇L(θt)∥2
2/L(θt); where
trepresents the number of iterations.
observe that the wider network, which is the network with the larger minimum weight vector norm, keeps
achieving faster convergence for the rest of epochs despite having lower values of the ratio ∥∇L(θt)∥2
2/L(θt).
8 Conclusion
We formally analyzed the training and generalization of deep neural networks with WeightNorm under smooth
activations. For this we needed to characterize a series of bounds which highlighted the dependence of our
results on the depth, width, and minimum weight vector norm. Our simulations showed the possibility
that our training guarantees hold in practice. We hope our paper will elicit further work on the theoretical
foundation for other types of normalization whose connection to training and generalization are not well
explored.
Acknowledgments
Part of the work was done when Pedro Cisneros-Velarde was affiliated with the University of Illinois Urbana-
Champaign and was concluded during his current affiliation with VMware Research. The work was supported
in part by the National Science Foundation (NSF) through awards IIS 21-31335, OAC 21-30835, DBI 20-21898,
as well as a C3.ai research award. Sanmi Koyejo acknowledges support by NSF 2046795 and 2205329, NIFA
award 2020-67021-32799, the Alfred P. Sloan Foundation, and Google Inc.
References
Pytorch2.0. Pytorch2.0documentation. https://pytorch.org/docs/stable/generated/torch.nn.utils.
weight_norm.html . Accessed: 05-09-2023.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Proceedings of the 36th International Conference on Machine Learning , volume 97 of
Proceedings of Machine Learning Research , pages 242–252. PMLR, 2019.
Devansh Arpit, Víctor Campos, and Yoshua Bengio. How to initialize your network? robust initialization
for weightnorm & resnets. In Advances in Neural Information Processing Systems , volume 32. Curran
Associates, Inc., 2019.
11Published in Transactions on Machine Learning Research (1/2025)
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.
Arindam Banerjee, Pedro Cisneros-Velarde, Libin Zhu, and Misha Belkin. Restricted strong convexity of
deep learning models with smooth activations. In The Eleventh International Conference on Learning
Representations (ICLR) , 2023.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural
networks. In Advances in Neural Information Processing Systems (NeurIPS) , volume 30, 2017.
Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint. Acta
Numerica , 30:87–201, 2021.
Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration Inequalities: A Nonasymptotic
Theory of Independence . Oxford University Press, 02 2013. ISBN 9780199535255. doi: 10.1093/acprof:
oso/9780199535255.001.0001. URL https://doi.org/10.1093/acprof:oso/9780199535255.001.0001 .
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine , 29(6):141–142, 2012.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of
deep neural networks. In Proceedings of the 36th International Conference on Machine Learning , volume 97
ofProceedings of Machine Learning Research , pages 1675–1685. PMLR, 09–15 Jun 2019.
Yonatan Dukler, Quanquan Gu, and Guido Montufar. Optimization theory for ReLU neural networks trained
with normalization layers. In Proceedings of the 37th International Conference on Machine Learning ,
volume 119 of Proceedings of Machine Learning Research , pages 2751–2760, 2020.
Jianqing Fan, Cong Ma, and Yiqiao Zhong. A selective overview of deep learning. Statistical Science , 36(2):
264 – 290, 2021.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural
networks. In Proceedings of the 31st Conference On Learning Theory , volume 75 of Proceedings of Machine
Learning Research , pages 297–299, 2018.
Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural tangent hierarchy. In
International conference on machine learning , pages 4542–4551. PMLR, 2020.
Lei Huang, Xianglong Liu, Yang Liu, Bo Lang, and Dacheng Tao. Centered weight normalization in
accelerating training of deep neural networks. In Proceedings of the IEEE International Conference on
Computer Vision (ICCV) , Oct 2017.
Lei Huang, Xianglong Liu, Bo Lang, Adams Yu, Yongliang Wang, and Bo Li. Orthogonal weight normalization:
Solution to optimization over multiple dependent stiefel manifolds in deep neural networks. Proceedings of
the AAAI Conference on Artificial Intelligence , 32(1), Apr. 2018. doi: 10.1609/aaai.v32i1.11768.
Lei Huang, Jie Qin, Yi Zhou, Fan Zhu, Li Liu, and Ling Shao. Normalization techniques in training dnns:
Methodology, analysis and application. IEEE Transactions on Pattern Analysis and Machine Intelligence ,
pages 1–20, 2023. doi: 10.1109/TPAMI.2023.3250241.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning , volume 37
ofProceedings of Machine Learning Research , pages 448–456. PMLR, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. In Advances in Neural Information Processing Systems , volume 31. Curran Associates,
Inc., 2018.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
12Published in Transactions on Machine Learning Research (1/2025)
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces . Classics in Mathematics. Springer Berlin,
Heidelberg, 1991. doi: 10.1007/978-3-642-20212-4.
Zhiyuan Li, Tianhao Wang, and Dingli Yu. Fast mixing of stochastic gradient descent with normalization and
weight decay. In Advances in Neural Information Processing Systems , volume 35, pages 9233–9248, 2022.
Xiangru Lian and Ji Liu. Revisit batch normalization: New understanding and refinement via composition
optimization. In Proceedings of the Twenty-Second International Conference on Artificial Intelligence and
Statistics , volume 89 of Proceedings of Machine Learning Research , pages 3254–3263. PMLR, 16–18 Apr
2019.
C. Liu, L. Zhu, and M. Belkin. Loss landscapes and optimization in over-parameterized non-linear systems
and neural networks. Applied and Computational Harmonic Analysis , 59:85–116, 2022.
Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and why the
tangent kernel is constant. In Advances in Neural Information Processing Systems , 2020.
Depen Morwani and Harish G. Ramaswamy. Inductive bias of gradient descent for weight normalized smooth
homogeneous neural nets. In Proceedings of The 33rd International Conference on Algorithmic Learning
Theory, volume 167 of Proceedings of Machine Learning Research , pages 827–880, 2022.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks.
In Peter Grünwald, Elad Hazan, and Satyen Kale, editors, Proceedings of The 28th Conference on Learning
Theory, volume 40 of Proceedings of Machine Learning Research , pages 1376–1401, Paris, France, 03–06 Jul
2015. PMLR.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-
normalized margin bounds for neural networks. In International Conference on Learning Representations ,
2018.
Q. Nguyen and M. Mondelli. Global convergence of deep networks with one wide layer followed by pyramidal
topology. In Neural Information Processing Systems (NeurIPS) , 2020.
Quynh Nguyen. On the proof of global convergence of gradient descent for deep relu networks with linear
widths. In International Conference on Machine Learning (ICML) , 2021.
Quynh Nguyen, Marco Mondelli, and Guido Montufar. Tight bounds on the smallest eigenvalue of the neural
tangent kernel for deep relu networks. In International Conference on Machine Learning (ICML) , 2021.
S. Oymak and M. Soltanolkotabi. Towards moderate overparameterization: global convergence guarantees
for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory , 2020.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent takes the
shortest path? In International Conference on Machine Learning (ICML) , 2019.
Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Micro-batch training with batch-channel
normalization and weight standardization, 2020.
Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training
of deep neural networks. In Advances in Neural Information Processing Systems , volume 29, 2016.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization
help optimization? In Advances in Neural Information Processing Systems , volume 31, 2018.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms .
Cambridge University Press, 2014.
Xiaoxia Wu, Edgar Dobriban, Tongzheng Ren, Shanshan Wu, Zhiyuan Li, Suriya Gunasekar, Rachel Ward,
and Qiang Liu. Implicit regularization and convergence for weight normalization. In Advances in Neural
Information Processing Systems , volume 33, pages 2835–2847, 2020.
13Published in Transactions on Machine Learning Research (1/2025)
Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving
layer normalization. In Advances in Neural Information Processing Systems , volume 32, 2019.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks. In
Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-parameterized
deep relu networks. Machine Learning , 109:467–492, 2020.
A Spectral norm of the Hessian for WeightNorm
We establish the main theorem from Section 4.
Theorem 4.1 (Hessian bound for WeightNorm). Under Assumptions 1 and 2, for any θ∈Rpwith
v∈BEuc
ρ1(v0), and any xi,i∈[n], we have
/vextenddouble/vextenddouble∇2
θf(θ;xi)/vextenddouble/vextenddouble
2≤O
(1 +ρ1)L3(1√m+L2max{|ϕ(0)|,|ϕ(0)|2})
min/braceleftig/vextenddouble/vextenddouble¯W/vextenddouble/vextenddouble
2,/vextenddouble/vextenddouble¯W/vextenddouble/vextenddouble2
2/bracerightig
. (4)
A.1 Starting the Proof
For an order-3 tensor T∈Rd1×d2×d3we define the operator ∥·∥2,2,1as follows,
∥T∥2,2,1:= sup
∥x∥2=∥z∥2=1d3/summationdisplay
k=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingled1/summationdisplay
i=1d2/summationdisplay
j=1Tijkxizj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle,x∈Rd1,z∈Rd2. (20)
Let us consider any x∈Rdsuch that∥x∥2=√
d. Our analysis follows the general structure developed
in (Liu et al., 2020, Theorem 3.1). We first observe that
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(θ;x)
(∂θ)2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤L/summationdisplay
l1,l2=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(θ;x)
∂W(l1)∂W(l2)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2+ 2L/summationdisplay
l1=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(θ;x)
∂W(l1)∂W(L+1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2+/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(θ;x)
(∂W(L+1))2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
=L/summationdisplay
l1,l2=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(θ;x)
∂W(l1)∂W(l2)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2+ 2L/summationdisplay
l1=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(θ;x)
∂W(l1)∂W(L+1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,(21)
where the equality follows by noticing that, since W(L+1)=v,/parenleftig
∂2f(θ;x)
(∂W(L+1))2/parenrightig
i,j= 0for(i,j)∈[m]×[m], and
so/vextenddouble/vextenddouble/vextenddouble∂2f(θ;x)
(∂W(L+1))2/vextenddouble/vextenddouble/vextenddouble
2= 0.
From now on, for simplicity of notation, we will obviate the dependency on xwhen necessary.
Now, again from the proof of (Liu et al., 2020, Theorem 3.1), for 1≤l1≤l2≤L,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(θ)
∂W(l1)∂W(l2)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2α(l1)
(∂W(l1))2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,2,1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f
∂α(l1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞1[l1=l2]
+ 1[l2>l1]/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(11)
∂W(l1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2l2−1/productdisplay
l=l1+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l)
∂α(l−1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2α(l2)
∂α(l2−1)∂W(l2)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,2,1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f
∂α(l2)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞
+ 1[l2>l1]/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l1)
∂W(l1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l2)
∂W(l2)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2L/summationdisplay
l=l2+1l/productdisplay
l′=l1+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l′)
∂α(l′−1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2α(l)
(∂α(l−1))2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,2,1
×l/productdisplay
ˆl=l2+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(ˆl)
∂α(ˆl−1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f
∂α(l)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞(22)
14Published in Transactions on Machine Learning Research (1/2025)
We use the following conventional notation: if a product has the form/producttexta
i=bziwherea<bgiven some real
scalars{zi}n
i=awheren≥b, then/producttexta
i=bzi= 0.
Now we proceed to obtain upper bounds for all the required terms in the previous expression.
A.2 Upper bounds of the L2-norms ofα(l)
Lemma A.1. Consider Assumption 1. For any l∈[L], any x∈Rdsuch that∥x∥2= 1, and anyθ∈Rp, we
have
∥α(l)(x)∥2≤1 +l|ϕ(0)|√m .
Proof.Following (Allen-Zhu et al., 2019; Liu et al., 2020), we prove the result by induction. First, since
∥x∥2= 1by assumption, we have ∥α(0)(x)∥2= 1. Sinceϕis 1-Lipschitz,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleϕ

1√m(W(1)
i)⊤
/vextenddouble/vextenddouble/vextenddoubleW(1)
i/vextenddouble/vextenddouble/vextenddouble
2α(0)

i
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2−∥ϕ(0)∥2≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleϕ
1√m
(W(1)
i)⊤
/vextenddouble/vextenddouble/vextenddoubleW(1)
i/vextenddouble/vextenddouble/vextenddouble
2α(0)

i
−ϕ(0)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
1√m(W(1)
i)⊤
/vextenddouble/vextenddouble/vextenddoubleW(1)
i/vextenddouble/vextenddouble/vextenddouble
2α(0)

i/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,
so that
∥α(1)∥2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleϕ

1√m(W(1)
i)⊤
/vextenddouble/vextenddouble/vextenddoubleW(1)
i/vextenddouble/vextenddouble/vextenddouble
2α(0)

i
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
1√m(W(1)
i)⊤
/vextenddouble/vextenddouble/vextenddoubleW(1)
i/vextenddouble/vextenddouble/vextenddouble
2α(0)

i/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2+∥ϕ(0)∥2
=1√m/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1
(W(1)
i)⊤
/vextenddouble/vextenddouble/vextenddoubleW(1)
i/vextenddouble/vextenddouble/vextenddouble
2α(0)
2
+|ϕ(0)|√m
(a)
≤1√m/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1/vextenddouble/vextenddoubleα(0)/vextenddouble/vextenddouble2
2+|ϕ(0)|√m
= 1 +|ϕ(0)|√m ,
where (a) follows from Chauchy-Schwarz. For the inductive step, we assume that for layer l−1, we have
∥α(l−1)∥2≤(1 + (l−1)|ϕ(0)|√m).
Sinceϕis 1-Lipschitz, for layer l, we can similarly conclude that
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleϕ

1√m(W(l)
i)⊤
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2α(l−1)

i
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2−∥ϕ(0)∥2≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
1√m(W(l)
i)⊤
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2α(l−1)

i/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,
15Published in Transactions on Machine Learning Research (1/2025)
so that
∥α(l)∥2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleϕ

1√m(W(l)
i)⊤
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2α(l−1)

i
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
1√m(W(l)
i)⊤
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2α(l−1)

i/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2+∥ϕ(0)∥2
≤1√m/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble2
2+|ϕ(0)|√m
(a)
≤(1 + (l−1)|ϕ(0)|√m) +|ϕ(0)|√m
= 1 +l|ϕ(0)|√m,
where (a) follows from the inductive step. This completes the proof.
A.3 Upper bounds on the spectral norms of∂α(l)
∂W(l)and∂α(l)
∂α(l−1)
Lemma A.2. Consider Assumption 1. For any l∈{2,...,L}, any x∈Rdsuch that∥x∥2=√
d, and any
θ∈Rp, we have/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l)(x)
∂α(l−1)(x)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤1, (23)
and/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(1)(x)
∂α(0)(x)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(1)(x)
∂x/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤1. (24)
Additionally, under Assumption 2 with v∈BEuc
ρ1(v0),
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f(x)
∂α(L)(x)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤1 +ρ1. (25)
Proof.By definition, we have
/bracketleftbigg∂α(l)
∂α(l−1)/bracketrightbigg
i,j=1√mϕ′(˜α(l)
i)W(l)
ij/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2. (26)
Then, we have that for 2≤l≤L,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l)
∂α(l−1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2= sup
∥v∥2=11
mm/summationdisplay
i=1
ϕ′(˜α(l)
i)m/summationdisplay
j=1W(l)
ij/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2vj
2
(a)
≤1
msup
∥v∥2=1m/summationdisplay
i=1
(W(l)
i)⊤v/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2
2
≤1
msup
∥v∥2=1m/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2∥v∥2
2
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2
=1
m·m
= 1,
16Published in Transactions on Machine Learning Research (1/2025)
where (a) follows from ϕbeing 1-Lipschitz by Assumption 1. Finally, it is easy to follow the same proof and
show that/vextenddouble/vextenddouble/vextenddouble∂α(1)
∂α(0)/vextenddouble/vextenddouble/vextenddouble2
2≤1. This completes the proof for equation (24).
Finally, for proving equation (25), note that/vextenddouble/vextenddouble/vextenddouble∂f
∂α(L)/vextenddouble/vextenddouble/vextenddouble
2=∥v∥2≤∥v0∥2+∥v−v0∥2≤(1 +ρ1).
Lemma A.3. Consider Assumption 1. For any l∈[L], any x∈Rdsuch that∥x∥2= 1, and anyθ∈Rp, we
have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l)(x)
∂W(l)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤2
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2/parenleftbigg1√m+ (l−1)|ϕ(0)|/parenrightbigg
.(27)
Proof.Note that the parameter vector w(l)=vec(W(l))and can be indexed with j∈[m]andj′∈[d]when
l= 1andj′∈[m]whenl≥2. We will do the analysis for l≥2since a similar analysis holds for the case
l= 1. Thus, we have
/bracketleftbigg∂α(l)
∂W(l)/bracketrightbigg
i,jj′=/bracketleftbigg∂α(l)
∂w(l)/bracketrightbigg
i,jj′
=ϕ′(˜α(l)
i)∂
∂Wj,j′
1√m(W(l)
i)⊤
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2α(l−1)
 1[j=i]
=ϕ′(˜α(l)
i)1√m
α(l−1)
j′−((W(l)
i)⊤α(l−1))W(l)
ij′
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2
1/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
21[j=i]
=ϕ′(˜α(l)
i)√mα(l−1)
j′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
21[j=i]
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(I)−ϕ′(˜α(l)
i)√m((W(l)
i)⊤α(l−1))W(l)
ij′
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
21[j=i]
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(II).(28)
where we used the fact that
∂
∂Wjj′
1/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2
=−Wij′
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
21[j=i].
Defining the matrices A,B∈Rm×m2by[A]i,jj′:= (I)and[B]i,jj′:= (II), we observe that∂α(l)
∂W(l)=A−B.
17Published in Transactions on Machine Learning Research (1/2025)
Then, noting that∂α(l)
∂W(l)∈Rm×m2and∥V∥F=∥vec(V)∥2for any matrix V, we have
∥A∥2
2= sup
∥V∥F=1m/summationdisplay
i=1
m/summationdisplay
j,j′=1ϕ′(˜α(l)
i)√mα(l−1)
j′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
21[j=i]Vjj′
2
= sup
∥V∥F=1m/summationdisplay
i=1
m/summationdisplay
j′=1ϕ′(˜α(l)
i)√mα(l−1)
j′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2Vij′
2
≤1
msup
∥V∥F=1m/summationdisplay
i=1
m/summationdisplay
j′=1α(l−1)
j′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2Vij′
2
≤1
msup
∥V∥F=1m/summationdisplay
i=11/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble2
2∥Vi∗∥2
2
≤/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble2
2
m·mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2sup
∥V∥F=1m/summationdisplay
i=1∥Vi∗∥2
2
=/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble2
2
m·mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2
(a)
≤1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/parenleftbigg1√m+ (l−1)|ϕ(0)|/parenrightbigg2
where (a) follows from Lemma A.1.
18Published in Transactions on Machine Learning Research (1/2025)
Similarly,
∥B∥2
2= sup
∥V∥F=1m/summationdisplay
i=1
m/summationdisplay
j,j′=1ϕ′(˜α(l)
i)√m((W(l)
i)⊤α(l−1))W(l)
ij′
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
21[j=i]Vjj′
2
= sup
∥V∥F=1m/summationdisplay
i=1
ϕ′(˜α(l)
i)√m((W(l)
i)⊤α(l−1))/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2
2
m/summationdisplay
j′=1W(l)
ij′Vij′
2
≤1
msup
∥X∥F=1m/summationdisplay
i=1((W(l)
i)⊤α(l−1))2
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble6
2/parenleftig
(W(l)
i)⊤Xi∗/parenrightig2
≤1
msup
∥V∥F=1m/summationdisplay
i=1((W(l)
i)⊤α(l−1))2
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble6
2/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2∥Vi∗∥2
2
≤1
msup
∥V∥F=1m/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble6
2/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble2
2/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2∥Vi∗∥2
2
=1
msup
∥V∥F=1m/summationdisplay
i=1/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble2
2/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2∥Vi,∗∥2
2
≤/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble2
2
m·mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2
(a)
≤1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/parenleftbigg1√m+ (l−1)|ϕ(0)|/parenrightbigg2
where (a) follows from Lemma A.1.
Using the recent derived results, we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l)
∂W(l)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤∥A∥2+∥B∥2≤2
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2/parenleftbigg1√m+ (l−1)|ϕ(0)|/parenrightbigg
.
That completes the proof.
A.4 Bound on∥·∥2,2,1
Lemma A.4. Consider Assumption 1. For any x∈Rdsuch that∥x∥2= 1and anyθ∈Rp, each of the
following inequalities hold,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2α(l)(x)
(∂α(l−1)(x))2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,2,1≤βϕ, (29)
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2α(l)(x)
∂α(l−1)(x)∂W(l)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,2,1≤2(βϕ(1√m+ (l−1)|ϕ(0)|) + 1)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2(30)
forl= 2,...,L; and
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2α(l)(x)
(∂W(l))2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,2,1≤2(2βϕ(1√m+ (l−1)|ϕ(0)|) + 3)(1√m+ (l−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2, (31)
19Published in Transactions on Machine Learning Research (1/2025)
forl∈[L].
Proof.For the inequality (29), note that from (26) we obtain
/parenleftbigg∂2α(l)
(∂α(l−1))2/parenrightbigg
i,j,k=1
mϕ′′(˜α(l)
i)W(l)
ikW(l)
ij/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2,
and so
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2α(l)
(∂α(l−1))2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,2,1= sup
∥v1∥2=∥v2∥2=11
mm/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleϕ′′(˜α(l)
i)((W(l)
i)⊤v1)((W(l)
i)⊤v2)/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤βϕ
msup
∥v1∥2=∥v2∥2=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle((W(l)
i)⊤v1)((W(l)
i)⊤v2)/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤βϕ
msup
∥v1∥2=∥v2∥2=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2∥v1∥2·/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble∥v2∥
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=βϕ
m·m. (32)
For the inequality (30), carefully following the chain rule in (26) we obtain
/parenleftbigg∂2α(l)
∂α(l−1)∂W(l)/parenrightbigg
i,j,kk′=ϕ′′(˜α(l)
i)
W(l)
ij
√m/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2

α(l−1)
k′
√m/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
21[k=i]
−((W(l)
i)⊤α(l−1))W(l)
ik′
√m/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
21[k=i]

+ϕ′(˜α(l)
i)1√m1[k=i]
−W(l)
ik′W(l)
ij
√m/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2+1/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
21[k′=j]
.
20Published in Transactions on Machine Learning Research (1/2025)
Then, we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2α(l)
∂α(l−1)∂W(l)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,2,1
= sup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j=1m/summationdisplay
k′=1ϕ′′(˜α(l)
i)

W(l)
ij
√m/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2

α(l−1)
k′
√m/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2
−((W(l)
i)⊤α(l−1))W(l)
ik′
√m/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2

+ϕ′(˜α(l)
i)1√m
−W(l)
ik′W(l)
ij
√m/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2+1/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
21[k′=j]

v1,jV2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
= sup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j=1m/summationdisplay
k′=11
mϕ′′(˜α(l)
i)W(l)
ij/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2α(l−1)
k′v1,jV2,ik′
−m/summationdisplay
j=1m/summationdisplay
k′=11
mϕ′′(˜α(l)
i)W(l)
ij/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2((W(l)
i)⊤α(l−1))/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2W(l)
ik′v1,jV2,ik′
−m/summationdisplay
j=1m/summationdisplay
k′=11√mϕ′(˜α(l)
i)W(l)
ik′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2W(l)
ijv1,jV2,ik′+m/summationdisplay
j=11√mϕ′(˜α(l)
i)1/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2v1,jV2,ij/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤ sup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j=1m/summationdisplay
k′=11
mϕ′′(˜α(l)
i)W(l)
ij/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2α(l−1)
k′v1,jV2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+ sup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j=1m/summationdisplay
k′=11
mϕ′′(˜α(l)
i)W(l)
ij/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2((W(l)
i)⊤α(l−1))/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2W(l)
ik′v1,jV2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+ sup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j=1m/summationdisplay
k′=11√mϕ′(˜α(l)
i)W(l)
ik′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2W(l)
ijv1,jV2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+ sup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j=11√mϕ′(˜α(l)
i)1/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2v1,jV2,ij/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(33)
21Published in Transactions on Machine Learning Research (1/2025)
Now we upper bound each of the terms in the last inequality. We analyze the first term,
sup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j=1m/summationdisplay
k′=11
mϕ′′(˜α(l)
i)W(l)
ij/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2α(l−1)
k′v1,jV2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤βϕ
msup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
m/summationdisplay
j=1W(l)
ij/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2v1,j
/parenleftiggm/summationdisplay
k′=1α(l−1)
k′V2,ik′/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
(a)
≤βϕ
msup
∥v1∥2=∥V2∥F=1/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1∥v1∥2
2/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextenddouble/vextenddouble/vextenddoubleV2α(l−1)/vextenddouble/vextenddouble/vextenddouble
2
≤βϕ
msup
∥V2∥F=1/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=11/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2∥V2∥2/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble
2
(b)
≤βϕ
m/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=11/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble
2
(c)
≤βϕ√m(1√m+ (l−1)|ϕ(0)|)·1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2√m
=βϕ(1√m+ (l−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2
where (a) follows from applying Cauchy-Schwarz first with respect to the dot product described by the index
jand then on the index i; (b) follows from ∥V2∥2≤∥V2∥F; and (c) follows from Lemma A.1.
22Published in Transactions on Machine Learning Research (1/2025)
Now we analyze the second term,
sup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j=1m/summationdisplay
k′=11
mϕ′′(˜α(l)
i)W(l)
ij/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2((W(l)
i)⊤α(l−1))/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2W(l)
ik′v1,jV2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤βϕ
msup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1|(W(l)
i)⊤α(l−1)|/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
m/summationdisplay
j=1W(l)
ij/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2v1,j

m/summationdisplay
k′=1W(l)
ik′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2V2,ik′
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤βϕ
m/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble
2sup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
m/summationdisplay
j=1W(l)
ij/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2v1,j

m/summationdisplay
k′=1W(l)
ik′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2V2,ik′
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
(a)
≤βϕ
m/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble
2sup
∥v1∥2=∥V2∥F=1/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1∥v1∥2
2/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1∥V2,i∗∥2
2
=βϕ
m/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble
2sup
∥v1∥2=∥V2∥F=1/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1∥v1∥2
2/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2∥V2∥F
=βϕ
m/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble
2/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=11/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2
(b)
≤βϕ√m(1√m+ (l−1)|ϕ(0)|)·1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2√m
=βϕ(1√m+ (l−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2
where (a) follows from successive applications of the Cauchy-Schwarz inequality; (b) follows from Lemma A.1.
23Published in Transactions on Machine Learning Research (1/2025)
Now we analyze the third term,
sup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j=1m/summationdisplay
k′=11√mϕ′(˜α(l)
i)W(l)
ik′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2W(l)
ijv1,jV2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1√msup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
m/summationdisplay
j=1W(l)
ij/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2v1,j

m/summationdisplay
k′=1W(l)
ik′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2V2,ik′
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
(a)
≤1√msup
∥v1∥2=∥V2∥F=1/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1∥v1∥2
2/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1∥V2,i∗∥2
2
=1√msup
∥v1∥2=∥V2∥F=1/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1∥v1∥2
2/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2∥V2∥F
=1√m/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=11/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2
≤1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2
where (a) follows from successive applications of the Cauchy-Schwarz inequality.
Finally, for the fourth term,
sup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j=11√mϕ′(˜α(l)
i)1/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2v1,jV2,ij/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1√m1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2sup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j=1v1,jV2,ij/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1√m1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2sup
∥v1∥2=∥V2∥F=1m/summationdisplay
i=1∥V2,i∗∥2∥v1∥2
=1√m1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2sup
∥V2∥F=1m/summationdisplay
i=1∥V2,i∗∥2
≤1√m1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2sup
∥V2∥F=1√m/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1∥V2,i∗∥2
2
=1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2.
24Published in Transactions on Machine Learning Research (1/2025)
Then, taking all the analyzed terms back in (33), we obtain
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2α(l)
∂α(l−1)∂W(l)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,2,1≤2βϕ(1√m+ (l−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2+2
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2
=2(βϕ(1√m+ (l−1)|ϕ(0)|) + 1)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2,
which proves the inequality (30).
We now analyze the last inequality (31). Carefully following the chain rule in (28), we obtain
/parenleftbigg∂2α(l)
(∂W(l))2/parenrightbigg
i,jj′,kk′
=ϕ′′(˜α(l)
i)1√m
α(l−1)
j′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2−((W(l)
i)⊤α(l−1))W(l)
ij′
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2
 1[j=i]
×1√m
α(l−1)
k′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2−((W(l)
i)⊤α(l−1))W(l)
ik′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2
 1[k=i]
+ϕ′(˜α(l)
i)1√m1[j=i]
−α(l−1)
j′
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2W(l)
ik′ 1[k=i]
−
α(l−1)
k′Wij′
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2+((W(l)
i)⊤α(l−1))/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
21[k′=j′]−3((W(l)
i)⊤α(l−1))W(l)
ij′W(l)
ik′
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble5
2
 1[k=i]
.
25Published in Transactions on Machine Learning Research (1/2025)
Then, we have, after some careful calculation,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2α(l)
(∂W(l))2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,2,1
= sup
∥V1∥F=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1m/summationdisplay
k′=1ϕ′′(˜α(l)
i)1√m
α(l−1)
j/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2−((W(l)
i)⊤α(l−1))W(l)
ij′
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2

×1√m
α(l−1)
k′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2−((W(l)
i)⊤α(l−1))W(l)
ik′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2
V1,ij′V2,ik′
+m/summationdisplay
j′=1m/summationdisplay
k′=1ϕ′(˜α(l)
i)1√m
−α(l−1)
j′
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2W(l)
ik′
−
α(l−1)
k′Wij′
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2+((W(l)
i)⊤α(l−1))/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
21[k′=j′]−3((W(l)
i)⊤α(l−1))W(l)
ij′W(l)
ik′
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble5
2


×V1,ij′V2,ik′|
≤ sup
∥V1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1m/summationdisplay
k′=11
mϕ′′(˜α(l)
i)α(l−1)
j′α(l−1)
k′
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2V1,ij′V2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+ sup
∥V1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1m/summationdisplay
k′=11
mϕ′′(˜α(l)
i)((W(l)
i)⊤α(l−1))/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble4
2W(l)
ik′α(l−1)
j′V1,ij′V2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+ sup
∥V1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1m/summationdisplay
k′=11
mϕ′′(˜α(l)
i)((W(l)
i)⊤α(l−1))/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble4
2W(l)
ij′α(l−1)
k′V1,ij′V2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+ sup
∥V1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1m/summationdisplay
k′=11
mϕ′′(˜α(l)
i)((W(l)
i)⊤α(l−1))2
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble6
2W(l)
ij′W(l)
ik′V1,ij′V2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+ sup
∥V1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1m/summationdisplay
k′=11√mϕ′(˜α(l)
i)1/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2W(l)
ik′α(l−1)
j′V1,ij′V2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+ sup
∥V1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1m/summationdisplay
k′=11√mϕ′(˜α(l)
i)1/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2W(l)
ij′α(l−1)
k′V1,ij′V2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+ sup
∥V1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=11√mϕ′(˜α(l)
i)((W(l)
i)⊤α(l−1))/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2V1,ij′V2,ij′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+ sup
∥V1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1m/summationdisplay
k′=13√mϕ′(˜α(l)
i)((W(l)
i)⊤α(l−1))/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble5
2W(l)
ij′W(l)
ik′V1,ij′V2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(34)
26Published in Transactions on Machine Learning Research (1/2025)
Now we upper bound each of the terms in the last inequality. We analyze the first term,
sup
∥V1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1m/summationdisplay
k′=11
mϕ′′(˜α(l)
i)α(l−1)
j′α(l−1)
k′
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2V1,j′V2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤βϕ
msup
∥V1∥2=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(V1α(l−1))i(V2α(l−1))i/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤βϕ
m1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2sup
∥V1∥2=∥V2∥F=1/vextenddouble/vextenddouble/vextenddoubleV1α(l−1)/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddoubleV2α(l−1)/vextenddouble/vextenddouble/vextenddouble
2
≤βϕ
m1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble2
2sup
∥V1∥2=∥V2∥F=1∥V1∥2∥V2∥2
=βϕ
m1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble2
2
≤βϕ(1√m+ (l−1)|ϕ(0)|)2
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2.
Now we analyze the second term,
sup
∥V1∥F=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1m/summationdisplay
k′=11
mϕ′′(˜α(l)
i)((W(l)
i)⊤α(l−1))/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble4
2W(l)
ik′α(l−1)
j′V1,ij′V2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤βϕ
msup
∥V1∥F=∥V2∥F=1m/summationdisplay
i=1|(W(l)
i)⊤α(l−1)|/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
m/summationdisplay
j′=1α(l−1)
j′V1,ij′

m/summationdisplay
k′=1W(l)
ik′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2V2,ik′
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤βϕ
m/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble
21
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2sup
∥V1∥F=∥V2∥F=1/vextenddouble/vextenddouble/vextenddoubleV1α(l−1)/vextenddouble/vextenddouble/vextenddouble
2
×/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1
m/summationdisplay
k′=1W(l)
ik′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2V2,ik′
2
≤βϕ
m/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble2
21
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2sup
∥V2∥F=1/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1∥V2,i∗∥2
2
≤βϕ(1√m+ (l−1)|ϕ(0)|)2
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2.
The third term is analyzed very similarly as the second term and thus have the same upper bound.
27Published in Transactions on Machine Learning Research (1/2025)
Now, we analyze the fourth term,
sup
∥V1∥F=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1m/summationdisplay
k′=11
mϕ′′(˜α(l)
i)((W(l)
i)⊤α(l−1))2
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble6
2W(l)
ij′W(l)
ik′V1,ij′V2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤βϕ
msup
∥V1∥F=∥V2∥F=1m/summationdisplay
i=1|(W(l)
i)⊤α(l−1)|2
/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble4
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
m/summationdisplay
j′=1W(l)
ij′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2V1,ij′

m/summationdisplay
k′=1W(l)
ik′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2V2,ik′
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤βϕ
m/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble2
21
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2sup
∥V1∥F=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
m/summationdisplay
j′=1W(l)
ij′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2V1,ij′

×
m/summationdisplay
k′=1W(l)
ik′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2V2,ik′
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤βϕ
m/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble2
21
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2sup
∥V1∥F=∥V2∥F=1/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1∥V1,i∗∥2
2/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1∥V2,i∗∥2
2
=βϕ
m/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble2
21
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2
≤βϕ
m(1 + (l−1)|ϕ(0)|√m)2·1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2
=βϕ(1√m+ (l−1)|ϕ(0)|)2
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2.
Now, for the fifth term,
sup
∥V1∥F=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1m/summationdisplay
k′=11√mϕ′(˜α(l)
i)1/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2W(l)
ik′α(l−1)
j′V1,ij′V2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1√m1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2sup
∥V1∥F=∥V2∥F=1/vextenddouble/vextenddouble/vextenddoubleV1α(l−1)/vextenddouble/vextenddouble/vextenddouble
2/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1
m/summationdisplay
k′=1W(l)
ik′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2V2,ik′
2
≤1√m/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble
21
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2sup
∥V2∥F=1/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1∥V2,i∗∥2
2
≤(1√m+ (l−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2.
The sixth term is analyzed very similarly as the fifth term and thus have the same upper bound.
28Published in Transactions on Machine Learning Research (1/2025)
Now, for the seventh term,
sup
∥V1∥F=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=11√mϕ′(˜α(l)
i)((W(l)
i)⊤α(l−1))/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble3
2V1,ij′V2,ij′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1√m1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2sup
∥V1∥F=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1((W(l)
i)⊤α(l−1))/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2V1,ij′V2,ij′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1√m1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble
2sup
∥V1∥F=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1V1,ij′V2,ij′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1√m1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble
2sup
∥V1∥F=∥V2∥F=1m/summationdisplay
i=1∥V2,i∗∥2∥V1,i∗∥2
≤1√m1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble
2sup
∥V1∥F=∥V2∥F=1∥V2∥F∥V1∥F
=1√m1
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble
2
≤(1√m+ (l−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2.
Finally, for the eight term, following an analysis similar to the fourth term,
sup
∥V1∥F=∥V2∥F=1m/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
j′=1m/summationdisplay
k′=13√mϕ′(˜α(l)
i)((W(l)
i)⊤α(l−1))/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble5
2W(l)
ij′W(l)
ik′V1,ij′V2,ik′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤3√m/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble
2sup
∥V1∥2=∥V2∥F=1m/summationdisplay
i=11/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
m/summationdisplay
j′=1W(l)
ij′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2V1,ij′

×
m/summationdisplay
k′=1W(l)
ik′/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2V2,ik′
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤3√m/vextenddouble/vextenddouble/vextenddoubleα(l−1)/vextenddouble/vextenddouble/vextenddouble
21
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2sup
∥V1∥2=∥V2∥F=1/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1∥V1,i∗∥2
2/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1∥V2,i∗∥2
2
≤3(1√m+ (l−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2.
Then, taking all the analyzed terms back in (34), we obtain
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2α(l)
(∂W(l))2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,2,1≤4βϕ(1√m+ (l−1)|ϕ(0)|)2
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2+6(1√m+ (l−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2.
This finishes the proof.
29Published in Transactions on Machine Learning Research (1/2025)
A.5 Upper bound of the L∞-norm of∂f
∂α(l)
As seen before, to upper bound the Hessian, we need to upper bound the quantity/vextenddouble/vextenddouble/vextenddouble∂f
∂α(l)/vextenddouble/vextenddouble/vextenddouble
∞. For our purposes,
we will actually upper bound the L2-norm/vextenddouble/vextenddouble/vextenddouble∂f
∂α(l)/vextenddouble/vextenddouble/vextenddouble
2and use this to upper bound the L∞-norm.
Lemma A.5. Consider Assumptions 1 and 2. For any l∈[L], any x∈Rdsuch that∥x∥2= 1, and any
θ∈Rpwithv∈BEuc
ρ1(v0), we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f(θ;x)
∂α(l)(x)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤(1 +ρ1). (35)
Proof.From Lemma A.2, we know that/vextenddouble/vextenddouble/vextenddouble∂f
∂α(L)/vextenddouble/vextenddouble/vextenddouble
2≤1 +ρ1. Now, for the inductive step, assume/vextenddouble/vextenddouble/vextenddouble∂f
∂α(l)/vextenddouble/vextenddouble/vextenddouble
2≤
(1 +ρ1)for anyl<L. Then,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f
∂α(l−1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l)
∂α(l−1)∂f
∂α(l)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l)
∂α(l−1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f
∂α(l)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤1·(1 +ρ1)
where the last inequality follows again from Lemma A.2. We have finished the proof by induction.
A.6 Finishing the proof of Theorem 4.1
Replacing all the previously derived results back in (22), we obtain
30Published in Transactions on Machine Learning Research (1/2025)
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(θ)
∂W(l1)∂W(l2)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤2(2βϕ(1√m+ (l1−1)|ϕ(0)|) + 3)(1√m+ (l1−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l1)
i/vextenddouble/vextenddouble/vextenddouble2
2(1 +ρ1)
+2(1√m+ (l1−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l1)
i/vextenddouble/vextenddouble/vextenddouble
2/parenleftiggl2−1/productdisplay
l=l1+11/parenrightigg2(βϕ(1√m+ (l2−1)|ϕ(0)|) + 1)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l2)
i/vextenddouble/vextenddouble/vextenddouble
2
×(1 +ρ1)
+2(1√m+ (l1−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l1)
i/vextenddouble/vextenddouble/vextenddouble
22(1√m+ (l2−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l2)
i/vextenddouble/vextenddouble/vextenddouble
2L/summationdisplay
l=l2+1/parenleftiggl/productdisplay
l′=l1+11/parenrightigg
×βϕ
l/productdisplay
ˆl=l2+11
(1 +ρ1)
=2(2βϕ(1√m+ (l1−1)|ϕ(0)|) + 3)(1√m+ (l1−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l1)
i/vextenddouble/vextenddouble/vextenddouble2
2(1 +ρ1)
+2(1√m+ (l1−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l1)
i/vextenddouble/vextenddouble/vextenddouble
22(βϕ(1√m+ (l2−1)|ϕ(0)|) + 1)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l2)
i/vextenddouble/vextenddouble/vextenddouble
2(1 +ρ1)
+2(1√m+ (l1−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l1)
i/vextenddouble/vextenddouble/vextenddouble
22(1√m+ (l2−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l2)
i/vextenddouble/vextenddouble/vextenddouble
2·(L−l2)·βϕ(1 +ρ1)
≤2(2βϕ(1√m+L|ϕ(0)|) + 3)(1√m+L|ϕ(0)|)
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2(1 +ρ1)
+4(1√m+L|ϕ(0)|)(βϕ(1√m+L|ϕ(0)|) + 1)
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2(1 +ρ1)
+4Lβϕ(1√m+L|ϕ(0)|)2
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2(1 +ρ1)
≤O
L(1 +ρ1)(1√m+L2max{|ϕ(0)|,|ϕ(0)|2})
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2
(36)
Now, notice that∂2f(θ)
∂W(l1)=∂α(l1)
∂W(l1)/producttextL
l′=l1+1∂α(l′)
∂α(l′
1−1)·v, then
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(θ)
∂W(l1)∂W(L+1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l1)
∂W(l1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2L/productdisplay
l′=l1+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l′)
∂α(l′
1−1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤2(1√m+ (l1−1)|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l1)
i/vextenddouble/vextenddouble/vextenddouble
2≤2(1√m+L|ϕ(0)|)
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l1)
i/vextenddouble/vextenddouble/vextenddouble
2≤O
(1√m+L|ϕ(0)|)
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2
(37)
31Published in Transactions on Machine Learning Research (1/2025)
Replacing (36) along with (37) back in (21), we obtain
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(θ)
(∂θ)2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤L2O
L(1 +ρ1)(1√m+L2max{|ϕ(0)|,|ϕ(0)|2})
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2
+ 2LO
1√m+L|ϕ(0)|
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2

≤O
(1 +ρ1)L3(1√m+L2max{|ϕ(0)|,|ϕ(0)|2})
min/braceleftigg
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/bracerightigg
.(38)
This finishes the proof.
B Upper bounds on gradients of the predictor and empirical loss for WeightNorm
Lemma 4.1 (Predictor gradient bounds). Under Assumptions 1 and 2, for any θ∈Rpwithv∈BEuc
ρ1(v0),
and any xi,i∈[n], we have
∥∇θf(θ;xi)∥2≤ϱθ≤O/parenleftigg
(1 +L|ϕ(0)|√m)/parenleftigg
1 +√
L(1 +ρ1)/vextenddouble/vextenddouble¯W/vextenddouble/vextenddouble
2/parenrightigg/parenrightigg
(6)
whereϱ2
θ:= (1 +L|ϕ(0)|√m)2+ 4(1 +ρ1)2/summationtextL
l=11
∥¯W∥2
2/parenleftig
1√m+ (l−1)|ϕ(0)|/parenrightig2
, and
∥∇xf(θ;xi)∥2≤1 +ρ1. (7)
Proof.We first prove the bound on the gradient with respect to the weights. Using the chain rule,
∂f
∂W(l)=∂α(l)
∂W(l)L/productdisplay
l′=l+1∂α(l′)
∂α(l′−1)∂f
∂α(L)
and so
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f
∂W(l)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l)
∂W(l)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f
∂α(l)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2(a)
≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l)
∂W(l)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2·(1 +ρ1)
(b)
≤
2
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2(1√m+ (l−1)|ϕ(0)|)
·(1 +ρ1)
forl∈[L], where (a) follows from Lemma A.5, (b) follows from Lemma A.3. Similarly,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f
∂W(L+1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2=/vextenddouble/vextenddouble/vextenddoubleα(L)/vextenddouble/vextenddouble/vextenddouble
2≤(1 +L|ϕ(0)|√m),
where we used Lemma A.1 for the inequality. Now,
∥∇θf∥2
2=L+1/summationdisplay
l=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f
∂W(l)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2
≤(1 +L|ϕ(0)|√m)2+ 4(1 +ρ1)2L/summationdisplay
l=11
mini∈[m]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/parenleftbigg1√m+ (l−1)|ϕ(0)|/parenrightbigg2
=ϱ2
θ.
32Published in Transactions on Machine Learning Research (1/2025)
We further note that,
ϱ2
θ≤O
(1 +L2|ϕ(0)|2m) +L(1 +ρ1)2 1
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2(1
m+L2|ϕ(0)|2)

≤O
(1 +L2|ϕ(0)|2m)
1 +L(1 +ρ1)2
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble2
2


⇒ϱθ≤O
(1 +L|ϕ(0)|√m)
1 +√
L(1 +ρ1)
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2

,
where the implication follows from the property√
a+b≤√a+√
bfora,b≥0.
We now prove the bound on the gradient with respect to the input data. Again, using the chain rule,
∂f
∂x=∂f
∂α(0)=∂α(1)
∂α(0)/parenleftiggL/productdisplay
l′=2∂α(l′)
∂α(l′−1)/parenrightigg
∂f
∂α(L)
and so
/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f
∂x/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(1)
∂α(0)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftiggL/productdisplay
l′=2∂α(l′)
∂α(l′−1)/parenrightigg
∂f
∂α(L)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(1)
∂α(0)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/parenleftiggL/productdisplay
l′=2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂α(l′)
∂α(l′−1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/parenrightigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f
∂α(L)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
(a)
≤1 +ρ1
where (a) follows from Lemma A.2 and Lemma A.5. This completes the proof.
Proposition 4.1 (Empirical loss and empirical loss gradient bounds). Consider the square loss.
Under Assumptions 1 and 2, the following inequality holds for any θ∈Rpwithv∈BEuc
ρ1(v0),
L(θ)≤φ≤O((1 +ρ1)2(1 +L2|ϕ(0)|2m)), (9)
whereφ:=2
n/summationtextn
i=1y2
i+ 2(1 +ρ1)2(1 +L|ϕ(0)|√m)2. Moreover,
∥∇θL(θ)∥2≤2/radicalbig
L(θ)ϱθ≤2ϱθ√φ, (10)
withϱθas in Lemma 4.1.
Proof.We start by noticing that for θ∈Rp,
L(θ) =1
nn/summationdisplay
i=1(yi−f(θ;xi))2≤1
nn/summationdisplay
i=1(2y2
i+ 2|f(θ;xi)|2). (39)
Now, using the assumption on the weight of the output layer,
|f(θ;x)|=v⊤α(L)(x)
≤∥v∥2/vextenddouble/vextenddouble/vextenddoubleα(L)(x)/vextenddouble/vextenddouble/vextenddouble
2
(a)
≤(1 +ρ1)/vextenddouble/vextenddouble/vextenddoubleα(L)(x)/vextenddouble/vextenddouble/vextenddouble
2
(b)
≤(1 +ρ1)/parenleftbig
1 +L|ϕ(0)|√m/parenrightbig
,(40)
33Published in Transactions on Machine Learning Research (1/2025)
where (a) follows from ∥v∥2≤∥v0∥2+∥v−v0∥2≤1 +ρ1, and (b) follows from Lemma A.1. Now, replacing
this result back in (39)we obtainL(θ)≤1
n/summationtextn
i=1(2y2
i+ 2(1 +ρ1)2(1 +L|ϕ(0)|√m)2). This finishes the proof
for inequality (9).
Now, we observe that ∥∇θL(θ)∥2=/vextenddouble/vextenddouble1
n/summationtextn
i=1ℓ′
i∇θf/vextenddouble/vextenddouble
2≤1
n/summationtextn
i=1|ℓ′
i|∥∇θf∥2(a)
≤2ϱθ
n/summationtextn
i=1|yi−ˆyi| ≤
2ϱθ/radicalbig
L(θ)(b)
≤2√φϱθwhere (a) follows from Lemma 4.1 and (b) from inequality (9). This finishes the
proof for inequality (10). This completes the proof.
C Training using restricted strong convexity for WeightNorm
We establish the results from Section 5.
C.1 Restricted Strong Convexity and Smoothness
Lemma 5.1 (RSC for WeightNorm under Square Loss). For square loss, under Assumptions 1 and 2,
for everyθ′∈Qθ
κ∩BEuc
ρ2(θ)withθ∈Rpandv,v′∈BEuc
ρ1(v0),
L(θ′)≥L(θ) +⟨θ′−θ,∇θL(θ)⟩+αθ,θ′
2∥θ′−θ∥2
2, (12)
with
αθ,θ′=κ2
2∥∇θL(θ)∥2
2
L(θ)
−O
/parenleftigg
1 +√
L/vextenddouble/vextenddouble¯W/vextenddouble/vextenddouble
2/parenrightigg(1 +ρ2)(1 +ρ1)3L3A(1√m,L2,|ϕ(0)|)B(1,L,|ϕ(0)|√m)
min/braceleftig/vextenddouble/vextenddouble¯f(W′,W)ξ/vextenddouble/vextenddouble
2,/vextenddouble/vextenddouble¯f(W′,W)ξ/vextenddouble/vextenddouble2
2/bracerightig
,(13)
wheref(W′,W)ξ:=ξW′+ (1−ξ)Wfor someξ∈[0,1],A(1√m,L2,|ϕ(0)|) :=1√m+L2max{|ϕ(0)|,|ϕ(0)|2}
andB(1,L,|ϕ(0)|√m) := 1 +L|ϕ(0)|√m. We say that the empirical loss Lsatisfies the RSC property
w.r.t. (Qθ
κ∩BEuc
ρ2(θ),θ)wheneverαθ,θ′>0.
Proof.For anyθ′∈Qt
κ/2∩Bρ2(θ)withv′∈BEuc
ρ1(v0), by the second order Taylor expansion around θ, we
have
L(θ′) =L(θ) +⟨θ′−θ,∇θL(θ)⟩+1
2(θ′−θ)⊤∂2L(˜θ)
∂θ2(θ′−θ),
where ˜θ=ξθ′+ (1−ξ)θfor someξ∈[0,1]. We note that ˜v∈BEuc
ρ1(v0)since,∥˜v−v0∥2=
∥ξv′−ξv0+ (1−ξ)v−(1−ξ)v0∥2≤ξ∥v′−v0∥2+ (1−ξ)∥v−v0∥2≤ρ1.
Focusing on the quadratic form in the Taylor expansion, it can be shown that
(θ′−θ)⊤∂2L(˜θ)
∂θ2(θ′−θ)
=1
nn/summationdisplay
i=1ℓ′′
i/angbracketleftbigg
θ′−θ,∂f(˜θ;xi)
∂θ/angbracketrightbigg2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
I1+1
nn/summationdisplay
i=1ℓ′
i(θ′−θ)⊤∂2f(˜θ;xi)
∂θ2(θ′−θ)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
I2,
whereℓi=ℓ(yi,f(˜θ,xi)),ℓ′
i=∂ℓ(yi,z)
∂z/vextendsingle/vextendsingle/vextendsingle/vextendsingle
z=f(˜θ,xi), andℓ′′
i=∂2ℓ(yi,z)
∂z2/vextendsingle/vextendsingle/vextendsingle/vextendsingle
z=f(˜θ,xi). Likewise, we set ¯ℓi=
ℓ(yi,f(θ,xi)),¯ℓ′
i=∂ℓ(yi,z)
∂z/vextendsingle/vextendsingle/vextendsingle/vextendsingle
z=f(θ,xi), and ¯ℓ′′
i=∂2ℓ(yi,z)
∂z2/vextendsingle/vextendsingle/vextendsingle/vextendsingle
z=f(θ,xi)Then, followingasimilaranalysisto(Banerjee
et al., 2023),
34Published in Transactions on Machine Learning Research (1/2025)
I1=1
nn/summationdisplay
i=1ℓ′′
i/angbracketleftbigg
θ′−θ,∂f(˜θ;xi)
∂θ/angbracketrightbigg2
(a)
≥2
nn/summationdisplay
i=1/angbracketleftbigg
θ′−θ,∂f(θ;xi)
∂θ/angbracketrightbigg2
−4
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f(θ;xi)
∂θ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f(˜θ;xi)
∂θ−∂f(θ;xi)
∂θ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
×∥θ′−θ∥2
2
=2
nn/summationdisplay
i=11
(¯ℓ′
i)2/angbracketleftbigg
θ′−θ,¯ℓ′
i∂f(θ;xi)
∂θ/angbracketrightbigg2
−4
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f(θ;xi)
∂θ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f(˜θ;xi)
∂θ−∂f(θ;xi)
∂θ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
×∥θ′−θ∥2
2
≥2
nn/summationdisplay
i=11/summationtextn
j=1(¯ℓ′
j)2/angbracketleftbigg
θ′−θ,¯ℓ′
i∂f(θ;xi)
∂θ/angbracketrightbigg2
−4
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f(θ;xi)
∂θ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
×/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f(˜θ;xi)
∂θ−∂f(θ;xi)
∂θ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2∥θ′−θ∥2
2
(b)=1
2n2L(θ)n/summationdisplay
i=1/angbracketleftbigg
θ′−θ,¯ℓ′
i∂f(θ;xi)
∂θ/angbracketrightbigg2
−4
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f(θ;xi)
∂θ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂f(˜θ;xi)
∂θ−∂f(θ;xi)
∂θ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
×∥θ′−θ∥2
2
(c)
≥1
2L(θ)/angbracketleftigg
θ′−θ,1
nn/summationdisplay
i=1¯ℓ′
i∂f(θ;xi)
∂θ/angbracketrightigg2
−4
nn/summationdisplay
i=1ϱθ/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(¯θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2∥˜θ−θ∥2∥θ′−θ∥2
2
=1
2L(θ)⟨θ′−θ,∇θL(θ)⟩2−4
nn/summationdisplay
i=1ϱθ/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(¯θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2∥˜θ−θ∥2∥θ′−θ∥2
2
(d)
≥1
2L(θ)⟨θ′−θ,∇θL(θ)⟩2−4ϱθ∥θ′−θ∥3
2·1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(¯θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
(e)
≥κ2
2L(θ)∥∇θL(θ)∥2
2∥θ′−θ∥2
2−4ϱθ∥θ′−θ∥3
2·1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(¯θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
=/parenleftigg
κ2
2L(θ)∥∇θL(θ)∥2
2−4ϱθ∥θ′−θ∥2·1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(¯θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/parenrightigg
∥θ′−θ∥2
2,
where ¯θis some point in the segment that joins ˜θandθ; and where (a) is the inequality derived in the proof
of (Banerjee et al., 2023, Theorem 5.1); (b) follows by/summationtextn
i=1(¯ℓ′
i)2= 4/summationtextn
i=1(f(θ;xi)−yi)2= 4nL(θ); (c) follows
by Jensen’s inequality on the first term and on the second term by the use of Lemma 4.1 due to ˜v∈BSpec
ρ1(v0)
and the mean value theorem; (d) follows by/vextenddouble/vextenddouble˜θ−θ/vextenddouble/vextenddouble
2=∥ξθ′+ (1−ξ)θ−θ∥2=ξ∥θ′−θ∥2≤∥θ′−θ∥2; (e)
follows since θ′∈Qθ
κand from the fact that p⊤q= cos(p,q)∥p∥∥q∥for any vectors p,q.
35Published in Transactions on Machine Learning Research (1/2025)
RegardingI2, note that
I2=1
nn/summationdisplay
i=1ℓ′
i(θ′−θ)⊤∂2f(˜θ;xi)
∂θ2(θ′−θ)
(a)
≥−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1/parenleftbigg1√nℓ′
i/parenrightbigg/parenleftbigg1√n∥θ′−θ∥2
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(˜θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
(b)
≥−/parenleftbigg1
n∥[ℓ′
i]∥2
2/parenrightbigg1/2/parenleftigg
1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(˜θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2/parenrightigg1/2
∥θ′−θ∥2
2
(c)=−2/radicalig
L(˜θ)/parenleftigg
1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(˜θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2/parenrightigg1/2
∥θ′−θ∥2
2,
where (a) follows from the Cauchy-Schwartz inequality and the submultiplicative property of the induced
norm; (b) follows by Cauchy-Schwartz inequality again; and (c) from1
n∥[ℓ′
i]i∥2
2=4
n/summationtextn
i=1(˜yi−yi)2= 4L(˜θ).
Replacing the lower bounds on I1andI2, and using Lemma 4.1 and the fact that θ′∈BEuc
ρ2(θ), we finally
obtain,
(θ′−θ)⊤∂2L(˜θ)
∂θ2(θ′−θ)≥/parenleftigg
κ2
2∥∇θL(θ)∥2
2
L(θ)−4ϱθρ21
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(¯θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
−2√φ/parenleftigg
1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(˜θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2/parenrightigg1/2
∥θ′−θ∥2
2.
Now, we note that we can use Theorem 4.1 to upper bound the norm of the Hessian of the predictor because
¯vbelongs to the segment joining ˜vandv, two vectors which belong to the convex set BEuc
ρ1(v0), and so it
follows that ¯v∈BEuc
ρ1(v0).
36Published in Transactions on Machine Learning Research (1/2025)
Then, using Theorem 4.1, Lemma 4.1, and Proposition 4.1,
4ϱθρ21
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(¯θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2+ 2√φ/parenleftigg
1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(˜θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2/parenrightigg1/2
≤(1 +ρ1)L3(1√m+L2max{|ϕ(0)|,|ϕ(0)|2})O
4ϱθρ2
min/braceleftigg
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble¯W(l)
i/vextenddouble/vextenddouble/vextenddouble
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble¯W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/bracerightigg
+2√φ
min/braceleftigg
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble
2/bracerightigg

≤(4ϱθρ2+ 2√φ)O
(1 +ρ1)L3(1√m+L2max{|ϕ(0)|,|ϕ(0)|2})
min/braceleftigg
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble¯W(l)
i/vextenddouble/vextenddouble/vextenddouble
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble¯W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/bracerightigg

≤O
(ϱθρ2+√φ)(1 +ρ1)L3(1√m+L2max{|ϕ(0)|,|ϕ(0)|2})
min/braceleftigg
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble¯W(l)
i/vextenddouble/vextenddouble/vextenddouble
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble¯W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/bracerightigg

(a)
≤O

1 +√
L
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2

×max{(1 +ρ1),ρ2}(1 +ρ1)2L3(1√m+L2max{|ϕ(0)|,|ϕ(0)|2})(1 +L|ϕ(0)|√m)
min/braceleftigg
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble¯W(l)
i/vextenddouble/vextenddouble/vextenddouble
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble¯W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/bracerightigg

≤O

1 +√
L
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2

×(1 +ρ2)(1 +ρ1)3L3(1√m+L2max{|ϕ(0)|,|ϕ(0)|2})(1 +L|ϕ(0)|√m)
min/braceleftigg
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble¯W(l)
i/vextenddouble/vextenddouble/vextenddouble
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble¯W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2/bracerightigg

37Published in Transactions on Machine Learning Research (1/2025)
where (a) follows from
(ϱθρ2+√φ)≤O
ρ2(1 +L|ϕ(0)|√m)
1 +√
L(1 +ρ1)
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2
+ (1 +ρ1)(1 +L|ϕ(0)|√m)

≤O
max{ρ2,(1 +ρ1)}(1 +L|ϕ(0)|√m)
1 +√
L(1 +ρ1)
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddoubleW(l)
i/vextenddouble/vextenddouble/vextenddouble
2

.
That completes the proof.
Lemma 5.2 (Smoothness-like property for WeightNorm under Square Loss). For square loss,
under Assumptions 1 and 2, for every θ,θ′∈Rpwithv,v′∈BEuc
ρ1(v0),
L(θ′)≤L(θ) +⟨θ′−θ,∇θL(θ)⟩+βθ,θ′
2∥θ′−θ∥2
2, (15)
with
βθ,θ′≤O/parenleftigg
L4A(1,L2,|ϕ(0)|)(1 +ρ1)2/parenleftigg
1 +1
min{/vextenddouble/vextenddouble¯f(W′,W)ξ/vextenddouble/vextenddouble
2,/vextenddouble/vextenddouble¯f(W′,W)ξ/vextenddouble/vextenddouble2
2}/parenrightigg/parenrightigg
,(16)
wheref(W′,W)ξ:=ξW′+ (1−ξ)Wfor someξ∈[0,1],A(1,L2,|ϕ(0)|) = (1 +L2max{|ϕ(0)|,|ϕ(0)|2}m)2.
Proof.By the second order Taylor expansion about θ∈Rpwith ¯v∈BEuc
ρ1(v0), we haveL(θ′) =L(θ) +⟨θ′−
θ,∇θL(θ)⟩+1
2(θ′−θ)⊤∂2L(˜θ)
∂θ2(θ′−θ), where ˜θ=ξθ′+ (1−ξ)θfor someξ∈[0,1]. Similarly to what was
proved in Theorem 5.1, ˜v∈BEuc
ρ1(v0).
Now,
(θ′−θ)⊤∂2L(˜θ)
∂θ2(θ′−θ)
=1
nn/summationdisplay
i=1ℓ′′
i/angbracketleftbigg
θ′−θ,∂f(˜θ;xi)
∂θ/angbracketrightbigg2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
I1+1
nn/summationdisplay
i=1ℓ′
i(θ′−θ)⊤∂2f(˜θ;xi)
∂θ2(θ′−θ)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
I2,
whereℓi=ℓ(yi,f(˜θ,xi),ℓ′
i=∂ℓ(yi,z)
∂z/vextendsingle/vextendsingle/vextendsingle/vextendsingle
z=f(˜θ,xi), andℓ′′
i=∂2ℓ(yi,z)
∂z2/vextendsingle/vextendsingle/vextendsingle/vextendsingle
z=f(˜θ,xi). It is easy to note that I1≤
2ϱ2
˜θ∥θ′−θ∥2
2. ForI2, we have that
I2=1
nn/summationdisplay
i=1ℓ′
i(θ′−θ)⊤∂2f(˜θ;xi)
∂θ2(θ′−θ)
≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1/parenleftbigg1√nℓ′
i/parenrightbigg/parenleftbigg1√n∥θ′−θ∥2
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(˜θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
(a)
≤/parenleftbigg1
n∥[ℓ′
i]i∥2
2/parenrightbigg1/2/parenleftigg
1
nn/summationdisplay
i=1∥θ′−θ∥4
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(˜θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2/parenrightigg1/2
(b)
≤2/radicalig
L(˜θ)/parenleftigg
1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(˜θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2/parenrightigg1/2
∥θ′−θ∥2
2,
38Published in Transactions on Machine Learning Research (1/2025)
where (a) follows by Cauchy-Schwartz, and (b) from1
n∥[ℓ′
i]i∥2
2=4
n/summationtextn
i=1(˜yi−yi)2= 4L(˜θ). Replacing the
bounds onI1andI2on the original expression, and using Lemma 4.1,
(θ′−θ)⊤∂2L(˜θ)
∂θ2(θ′−θ)≤
2ϱ2
˜θ+ 2√φ/parenleftigg
1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(˜θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2/parenrightigg1/2
∥θ′−θ∥2
2.
Now, note that from the proof of Lemma 4.1,
ϱ2
˜θ≤O
(1 +L2|ϕ(0)|2m)
1 +L(1 +ρ1)2
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2


and so,
2ϱ2
˜θ+ 2√φ/parenleftigg
1
nn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂2f(˜θ;xi)
∂θ2/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2/parenrightigg1/2
≤O
(1 +L2|ϕ(0)|2m)
1 +L(1 +ρ1)2
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2


+O
(1 +ρ1)(1 +L|ϕ(0)|√m)·(1 +ρ1)L3(1√m+L2max{|ϕ(0)|,|ϕ(0)|2})
min{mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2}

≤O
L3(1 +L2max{|ϕ(0)|,|ϕ(0)|2}m)(1 +ρ1)2
1 +L
mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2
+(1√m+L2max{|ϕ(0)|,|ϕ(0)|2})
min{mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2}


≤O
L4(1 +L2max{|ϕ(0)|,|ϕ(0)|2}m)2(1 +ρ1)2
1 +1
min{mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble
2,mini∈[m]
l∈[L]/vextenddouble/vextenddouble/vextenddouble˜W(l)
i/vextenddouble/vextenddouble/vextenddouble2
2}

.
This completes the proof.
C.2 About the adaptation of (Banerjee et al., 2023, Theorem 5.3)
To adapt (Banerjee et al., 2023, Theorem 5.3) to our setting, we need to adapt the supporting lemma (Banerjee
et al., 2023, Lemma 5.1) to our setting as follows:
Lemma C.1 (Auxiliary result using RSC, (Banerjee et al., 2023, Lemma 5.1)). LetBt:=
Qθtκ∩BEuc
ρ2(θt)∩{θ∈Rp|v∈BEuc
ρ1(v0)}andθt∈arginfθ∈BtL(θ). Using the setting of Lemma 5.1, if
αθt,θt>0, then
L(θt)−inf
θ∈BtL(θ)≤1
2αθt,θt∥∇θL(θt)∥2
2. (41)
39Published in Transactions on Machine Learning Research (1/2025)
Proof.By Lemma 5.1 we have
L(θt)≥L(θt) +⟨θt−θt,∇θL(θt)⟩+αθt,θt
2∥θt−θt∥2
2. (42)
Now, let us take αθt,θtand define
ˆLt(θ) :=L(θt) +⟨θ−θt,∇θL(θt)⟩+αθt,θt
2∥θ−θt∥2
2.
Note that ˆLt(θ)is minimized at ˆθt+1:=θt−∇θL(θt)/αθt,θtand the minimum value is:
inf
θ∈RpˆLθt(θ) =ˆLt(ˆθt+1) =L(θt)−1
2αθt,θt∥∇θL(θt)∥2
2.
Then, we have
inf
θ∈BtL(θ) =L(θt)(a)
≥ˆLt(θt)≥inf
θ∈RpˆLt(θ) =L(θt)−1
2αθt,θt∥∇θL(θt)∥2
2,
where (a) follows from (42). Rearranging the terms completes the proof.
Finally, we note that Theorem 5.1 makes the claim thatαθt,θt
βθt∈(0,1](using the notation from the same
theorem). We now provide the proof for this result.
Proposition C.1 (The RSC to smoothness ratio). Consider the setting of Theorem 5.1. Then,
αθt,θt
βθt∈(0,1].
Proof.From Lemma C.1 we have
L(θt)−L(¯θt)≤1
2αθt,θt∥∇θL(θt)∥2
2. (43)
Now, using Lemma 5.2, we obtain
L(θt+1)≤L(θt) +⟨θt+1−θt,∇θL(θt)⟩+βθt
2∥θt+1−θt∥2
2
=L(θt)−ηt∥∇θL(θt)∥2
2+βθt
2η2
t∥∇θL(θt)∥2
2
=L(θt)−1
2βθt∥∇θL(θt)∥2
2(44)
where the last equality follows from ηt=1
βθt.
Now, we notice that θt+1∈Qθtκby Definition 5.1, θt+1∈BEuc
ρ2(θt)∩{θ∈Rp|v∈BEuc
ρ1(v0)}by Assumption 3,
and soθt+1∈Bt. This implies that L(¯θt)≤L(θt+1). Thus, using (44), we obtain
1
2βθt∥∇θL(θt)∥2
2≤L(θt)−L(¯θt).
Using this inequality along with (43)leads toαθt,θt
βθt≤1. Finally, by assumption we have αθt,θt>0and
by (17) we have βθt>0, and soαθt,θt
βθt>0. This completes the proof.
40Published in Transactions on Machine Learning Research (1/2025)
C.3 Distance between consecutive iterates
Proposition C.2 (Bound on the distance between consecutive iterates for WeightNorm under
ϕ(0) = 0).Under Assumptions 1 and 2, and assuming (A3.1)from Assumption 3 and that the activation
function satisfies ϕ(0) = 0, we have
∥θt+1−θt∥2≤ηt·O/parenleftigg
(1 +ρ1)2/parenleftigg
1 +√
L√m/vextenddouble/vextenddouble¯Wt/vextenddouble/vextenddouble
2/parenrightigg/parenrightigg
.
Proof.We have that∥θt+1−θt∥2=∥θt−ηt∇θL(θt)−θt∥2=ηt∥∇θL(θt)∥2(a)
≤ηt·2ϱθt√φ(b)
≤ηt·
O/parenleftbigg
(1 +ρ1)/parenleftbigg
1 +√
L(1+ρ1)√m∥¯Wt∥2/parenrightbigg/parenrightbigg
≤ηt·O/parenleftbigg
(1 +ρ1)2/parenleftbigg
1 +√
L√m∥¯Wt∥2/parenrightbigg/parenrightbigg
, where (a) follows from using (10)
from Proposition 4.1 and the definitions of ϱθtandφpresented therein; and (b) follows from Corollary 4.2
and Corollary 4.3.
D Comparison between networks with and without WeightNorm
We summarize in Table 1 the comparison between our bounds and optimization results and those for networks
without WeightNorm by Banerjee et al. (2023).
with WeightNorm without WeightNorm
Deterministic results High probability results
Hidden layer weights can take any value Hidden layer weights restricted to a neighborhood
around their initialization
Appearance of normalization terms, i.e., the mini-
mum weight vector norm, in the Hessian bounds
and the RSC and smoothness parametersNo appearance of such normalization terms
The polynomial dependence on the depth in our
bounds and parameters is independent from the
initialization variance of the weightsPolynomial dependence on the depth requires care-
ful choice of initialization variance
Lipschitz constant bound is independent from the
depthLipschitz constant bound exponentially grows with
the depth unless a careful choice of initialization
variance is done
Table 1: Comparison of results for neural networks with and without WeightNorm.
E Generalization bound for WeightNorm
We first state the following auxiliary technical result, which is based on (Golowich et al., 2018, Lemma 1).
We letA(l):={(W(1),W(2),...,W(l))∈Rm×d×Rm×m×···× Rm×m}.
Lemma E.1. Under Assumptions 1 and 2, assuming the activation function satisfies ϕ(0) = 0, consider any
WeightNorm network f(θ;·)of the form (1)with any fixed θ∈Rpandv∈BEuc
ρ1(v0), along with some input
data set{xi}n
i=1. Consider also any convex and monotonically increasing function g:R→[0,∞). For any
41Published in Transactions on Machine Learning Research (1/2025)
l∈{1,...,L−1},
E{ϵi}n
i=1
sup
¯W∈A(l)
w∈Rmg/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1ϵiϕ/parenleftbigg1√mw⊤
∥w∥α(l)(xi)/parenrightbigg/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle

≤2E{ϵi}n
i=1/bracketleftigg
sup
¯W∈A(l)g/parenleftigg
1√m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1ϵiα(l)(xi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/parenrightigg/bracketrightigg
(45)
Proof.Sincegis positive, g(|z|)≤g(z) +g(−z), and so
E{ϵi}n
i=1
sup
¯W∈A(l)
w∈Rmg/parenleftigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1ϵiϕ/parenleftbigg1√mw⊤
∥w∥α(l)(xi)/parenrightbigg/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle

≤E{ϵi}n
i=1
sup
¯W∈A(l)
w∈Rmg/parenleftiggn/summationdisplay
i=1ϵiϕ/parenleftbigg1√mw⊤
∥w∥α(l)(xi)/parenrightbigg/parenrightigg

+E{ϵi}n
i=1
sup
¯W∈A(l)
w∈Rmg/parenleftigg
−/parenleftiggn/summationdisplay
i=1ϵiϕ/parenleftbigg1√mw⊤
∥w∥α(l)(xi)/parenrightbigg/parenrightigg/parenrightigg

(a)= 2E{ϵi}n
i=1
sup
¯W∈A(l)
w∈Rmg/parenleftiggn/summationdisplay
i=1ϵiϕ/parenleftbigg1√mw⊤
∥w∥α(l)(xi)/parenrightbigg/parenrightigg

(b)= 2E{ϵi}n
i=1
g
sup
¯W∈A(l)
w∈Rmn/summationdisplay
i=1ϵiϕ/parenleftbigg1√mw⊤
∥w∥α(l)(xi)/parenrightbigg


(c)
≤2E{ϵi}n
i=1
g
sup
¯W∈A(l)
w∈Rmn/summationdisplay
i=1ϵi/parenleftbigg1√mw⊤
∥w∥α(l)(xi)/parenrightbigg


= 2E{ϵi}n
i=1
sup
¯W∈A(l)
w∈Rmg/parenleftiggn/summationdisplay
i=1ϵi/parenleftbigg1√mw⊤
∥w∥α(l)(xi)/parenrightbigg/parenrightigg

(d)
≤2E{ϵi}n
i=1
sup
¯W∈A(l)
w∈Rmg/parenleftigg
1√m/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1ϵi/parenleftbiggw⊤
∥w∥α(l)(xi)/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightigg

= 2E{ϵi}n
i=1/bracketleftigg
sup
¯W∈A(l)g/parenleftigg
1√m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1ϵiα(l)(xi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/parenrightigg/bracketrightigg
,
where (a) follows from the fact that ϵiand−ϵihave the same (symmetrical) distribution; (b) from gbeing
monotonically increasing; (c) from (Ledoux and Talagrand, 1991, equation (4.20)) which makes use of g
being convex and increasing, and of ϕbeing 1-Lipschitz and ϕ(0) = 0; and (d) follows from the increasing
monotonicity of g.
We now prove our generalization result.
Theorem 6.1 (Generalization Bound for WeightNorm under Square Loss). Consider the square
loss and the training set S={(xi,yi)i.i.d.∼ D,i∈[n]}and|y|≤1for anyy∼Dywith probability one. Under
42Published in Transactions on Machine Learning Research (1/2025)
Assumptions 1 and 2, assuming the activation function satisfies ϕ(0) = 0, with probability at least (1−δ)
over the choice of the training data xi∼D x,i∈[n], for any WeightNorm network f(θ;·)of the form (1)with
any fixedθ∈Rpandv∈BEuc
ρ1(v0), we have
LD(θ)−LS(θ)≤4(2 +ρ1)(1 +ρ1)/radicalbig
2 log(2)L+ 1√n+ 2(1 + (1 + ρ1)2)/radicalbig
2 log(2/δ)√n. (19)
Proof.Recall that for a class of functions F, the Rademacher complexity is given by
Rn(F) :=E{xi}n
i=1,{ϵi}n
i=1/bracketleftigg
sup
f∈F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1ϵif(xi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightigg
, (46)
where samples xi,i∈[n], are drawn i.i.d. from Dx, andϵi,i∈[n], are drawn i.i.d. from the Rademacher
distribution, i.e., +1or−1with probability1
2. Assuming∥f∥∞≤B,∀f∈F, a standard uniform convergence
argument implies
P/parenleftigg
sup
f∈F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1f(xi)−Ex∼Dx[f(x)]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥2Rn(F) +t/parenrightigg
≤2 exp/parenleftbigg
−nt2
2B2/parenrightbigg
. (47)
In other words, with probability at least (1−δ)over the draw of the samples, we have
∀f∈F,Ex∼Dx[f(x)]≤1
nn/summationdisplay
i=1f(xi) + 2Rn(F) +B/radicaligg
2 log2
δ
n. (48)
Now, consider a given loss function ℓso that, for a given constant ysuch that|y|≤1, it computes ℓ(y,f(x)).
Ifˆy∝⇕⊣√∫⊔≀→ℓ(y,ˆy)has a Lipschitz constant of λ, then by using the Rademacher contraction lemma (e.g., an
adaptation from (Shalev-Shwartz and Ben-David, 2014, Lemma 26.9)), the uniform convergence result can be
extended to the loss function: with probability at least (1−δ)over the draw of the samples, we have
∀f∈F,LD[f]≤LS[f] + 2λRn(F) +¯B/radicaligg
2 log2
δ
n. (49)
withLD[f] :=E(x,y)∼D[ℓ(y,f(x))],LS[f] :=1
n/summationtextn
i=1ℓ(yi,f(xi)), and supf∈F
|y|≤1|ℓ(y,f(·))|≤¯B.
Now we put everything according to the context of our setting. We let Fbe the set of all WeightNorm
networksf(θ;·)of the form (1)with any fixed θ∈Rpandv∈BEuc
ρ1(v0)under Assumptions 1 and 2, and
assuming the activation function satisfies ϕ(0) = 0. We also setLD(θ) :=LD[f(θ;·)]and
LS(θ) :=LS[f(θ;·)].
Consider any x∈Rdwith∥x∥2= 1. Now, using the assumption ϕ(0) = 0, we obtain
|f(θ;x)| ≤ (1 +ρ1)from equation (40), and soℓ(y,f(θ;x)) = (y−f(θ;x))2≤2|y|2+ 2|f(θ;x)|2≤
2(1 + (1 +ρ1)2), i.e.,
B= 2(1 + (1 + ρ1)2). (50)
Likewise, this allows us to obtain, for |y|≤1,
dℓ(y,ˆy)
dˆy/vextendsingle/vextendsingle/vextendsingle/vextendsingle
ˆy=f(θ;x)≤2(|y|+|f(θ,x)|)≤2(1 + (1 +ρ1)) = 2(2 +ρ1),
and so
λ= 2(2 +ρ1). (51)
43Published in Transactions on Machine Learning Research (1/2025)
Next, we focus on bounding Rn(F).
Rn(F) =1
nE{xi}n
i=1,{ϵi}n
i=1/bracketleftigg
sup
f∈F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1ϵif(xi)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightigg
=1
nE{xi}n
i=1,{ϵi}n
i=1
 sup
¯W∈A(L)
v∈Rm,∥v∥2≤1+ρ1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglev⊤/parenleftiggn/summationdisplay
i=1ϵiα(L)(xi)/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle

=(1 +ρ1)
nE{xi}n
i=1,{ϵi}n
i=1/bracketleftigg
sup
¯W∈A(L)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1ϵiα(L)(xi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/bracketrightigg
≤(1 +ρ1)
nE{xi}n
i=1,{ϵi}n
i=1/bracketleftigg
sup
¯W∈A(L)√m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1ϵiα(L)(xi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞/bracketrightigg
=(1 +ρ1)
nE{xi}n
i=1,{ϵi}n
i=1
sup
¯W∈A(L)max
k∈[m]√m/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1ϵiϕ
1√m(W(L)
k)⊤
/vextenddouble/vextenddouble/vextenddoubleW(L)
k/vextenddouble/vextenddouble/vextenddouble
2α(L−1)(xi)
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle

=(1 +ρ1)
nE{xi}n
i=1,{ϵi}n
i=1/bracketleftigg
sup
¯W∈A(L−1)sup
w∈Rm√m/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1ϵiϕ/parenleftbigg1√mw⊤
∥w∥2α(L−1)(xi)/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightigg
.(52)
Then,
=⇒n
(1 +ρ1)Rn(F)≤E{xi}n
i=1,{ϵi}n
i=1/bracketleftigg
sup
¯W∈A(L−1)sup
w∈Rm√m/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1ϵiϕ/parenleftbigg1√mw⊤
∥w∥2α(L−1)(xi)/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightigg
=1
λlog exp/parenleftigg
λE{xi}n
i=1,{ϵi}n
i=1/bracketleftigg
sup
¯W∈A(L−1)sup
w∈Rm√m/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1ϵiϕ/parenleftbigg1√mw⊤
∥w∥2α(L−1)(xi)/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightigg/parenrightigg
(a)
≤1
λlogE{xi}n
i=1,{ϵi}n
i=1
sup
¯W∈A(L−1)
w∈Rmexp/parenleftigg
λ√m/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1ϵiϕ/parenleftbigg1√mw⊤
∥w∥2α(L−1)(xi)/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightigg

(53)
where (a) follows from Jensen’s inequality. Now, using Lemma E.1 with the function g(z) = exp(λ√m·z),
1
λlogE{xi}n
i=1,{ϵi}n
i=1
sup
¯W∈A(L−1)
w∈Rmexp/parenleftigg
λ√m/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
i=1ϵiϕ/parenleftbigg1√mw⊤
∥w∥2α(L−1)(xi)/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightigg

≤1
λlog 2E{xi}n
i=1,{ϵi}n
i=1/bracketleftigg
sup
¯W∈A(L−1)exp/parenleftigg
λ√m·1√m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1ϵiα(L−1)(xi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/parenrightigg/bracketrightigg
=1
λlog 2E{xi}n
i=1,{ϵi}n
i=1/bracketleftigg
sup
¯W∈A(L−1)exp/parenleftigg
λ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1ϵiα(L−1)(xi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/parenrightigg/bracketrightigg
.
Now we realize the expression on the right-hand side of the above equality is very similar to the second
inequality of equation (53); therefore, we can iterate the same procedure using Lemma E.1 until we eventually
obtain,
=⇒n
(1 +ρ1)Rn(F)≤1
λlog/parenleftigg
2L·E{xi}n
i=1,{ϵi}n
i=1/bracketleftigg
exp/parenleftigg
λ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1ϵiα(0)(xi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/parenrightigg/bracketrightigg/parenrightigg
=1
λlog/parenleftigg
2L·E{xi}n
i=1,{ϵi}n
i=1/bracketleftigg
exp/parenleftigg
λ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1ϵixi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/parenrightigg/bracketrightigg/parenrightigg
.(54)
44Published in Transactions on Machine Learning Research (1/2025)
Now, we can closely follow the proof of (Golowich et al., 2018, Theorem 1) after the equation (9) from that
same paper. We first define the random variable Z=∥/summationtextn
i=1ϵixi∥2and obtain
1
λlog/parenleftig
2L·E{xi}n
i=1,{ϵi}n
i=1[exp(λZ)]/parenrightig
=Llog(2)
λ+log/parenleftig
E{xi}n
i=1,{ϵi}n
i=1/bracketleftig
exp/parenleftig
λ/parenleftig
Z−E{ϵi}n
i=1[Z]/parenrightig/parenrightig/bracketrightig/parenrightig
λ+log/parenleftig
E{xi}n
i=1/bracketleftig
exp/parenleftig
λE{ϵi}n
i=1[Z]/parenrightig/bracketrightig/parenrightig
λ.
(55)
Now,
E{ϵi}n
i=1[Z](a)
≤/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalbtE{ϵi}n
i=1
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1ϵixi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2

=/radicaltp/radicalvertex/radicalvertex/radicalvertex/radicalvertex/radicalvertex/radicalbtE{ϵi}n
i=1
n/summationdisplay
i=1ϵ2
ix⊤
ixi+n/summationdisplay
i=1n/summationdisplay
j=1
j̸=iϵiϵjx⊤
ixj

(b)=/radicaltp/radicalvertex/radicalvertex/radicalbtn/summationdisplay
i=1∥xi∥2
2
=⇒1
λlog/parenleftig
E{xi}n
i=1/bracketleftig
exp/parenleftig
λE{ϵi}n
i=1[Z]/parenrightig/bracketrightig/parenrightig
≤1
λlog
E{xi}n
i=1
exp
λ/radicaltp/radicalvertex/radicalvertex/radicalbtn/summationdisplay
i=1∥xi∥2
2



=1
λlog/parenleftbig
exp/parenleftbig
λ√n/parenrightbig/parenrightbig
=√n(56)
where (a) follows by Jensen’s inequality and (b) from the fact that ϵiis independent from ϵj,j̸=i.
If we consider{xi}n
i=1as constants, then we have that Zis a deterministic function of the i.i.d variables
(ϵi)n
i=1which satisfies
Z(ϵ1,...,ϵi,...,ϵn)−Z(ϵ1,...,−ϵi,...,ϵn)≤2∥xi∥2,
obtained by using the reverse triangle inequality. This inequality is a bounded-difference condi-
tion which implies that Z(considering{xi}n
i=1as constants) is sub-Gaussian with variance factor
v=1
4/summationtextn
i=14∥xi∥2
2=/summationtextn
i=1∥xi∥2
2; see (Boucheron et al., 2013, Theorem 6.2); which then implies
E{ϵi}n
i=1/bracketleftig
exp/parenleftig
λ/parenleftig
Z−E{ϵi}n
i=1[Z]/parenrightig/parenrightig/bracketrightig
≤exp/parenleftig
1
2λ2/summationtextn
i=1∥xi∥2
2/parenrightig
. Then, we obtain
log/parenleftig
E{xi}n
i=1,{ϵi}n
i=1/bracketleftig
exp/parenleftig
λ/parenleftig
Z−E{ϵi}n
i=1[Z]/parenrightig/parenrightig/bracketrightig/parenrightig
λ≤log/parenleftig
E{xi}n
i=1/bracketleftig
exp/parenleftig
λ2
2/summationtextn
i=1∥xi∥2
2/parenrightig/bracketrightig/parenrightig
λ=1
λ·λ2n
2=λn
2.
(57)
Replacing equations (56) and (57) back in (55) with λ=√
2 log(2)L√n, let us obtain
1
λlog/parenleftig
2L·E{xi}n
i=1,{ϵi}n
i=1[Z]/parenrightig
≤/radicalbig
2 log(2)Ln
2+/radicalbig
2 log(2)Ln
2+√n= (/radicalbig
2 log(2)L+ 1)√n(58)
which replacing back in (54) let us obtain,
n
(1 +ρ1)Rn(F) = (/radicalbig
2 log(2)L+ 1)√n
=⇒Rn(F)≤(1 +ρ1)√n(/radicalbig
2 log(2)L+ 1).
Plugging this bound back in (49), along with (50) and (51), completes the proof.
45