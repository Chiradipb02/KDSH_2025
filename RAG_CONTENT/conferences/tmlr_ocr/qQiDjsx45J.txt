Under review as submission to TMLR
Transfer learning with affine model transformation
Anonymous authors
Paper under double-blind review
Abstract
Supervised transfer learning (TL) has received considerable attention because of its potential
to boost the predictive power of machine learning in cases with limited data. In a conventional
scenario, cross-domain differences are modeled and estimated using a given set of source
models and samples from a target domain. For example, if there is a functional relationship
betweensourceandtargetdomains, onlydomain-specificfactorsareadditionallylearnedusing
target samples to shift the source models to the target. However, the general methodology
for modeling and estimating such cross-domain shifts has been less studied. This study
presents a TL framework that simultaneously and separately estimates domain shifts and
domain-specific factors using given target samples. Assuming consistency and invertibility of
the domain transformation functions, we derive an optimal family of functions to represent
the cross-domain shift. The newly derived class of transformation functions takes the same
form as invertible neural networks using affine coupling layers, which are widely used in
generative deep learning. We show that the proposed method encompasses a wide range of
existing methods, including the most common TL procedure based on feature extraction
using neural networks. We also clarify the theoretical properties of the proposed method,
such as the convergence rate of the generalization error, and demonstrate the practical
benefits of separately modeling and estimating domain-specific factors through several case
studies.
1 Introduction
Transfer learning (TL) is applied to improve the predictive performance of machine learning in a target domain
with limited data by reusing knowledge gained from training in related source domains. Its great potential
has already been demonstrated in various real-world problems, including computer vision (Krizhevsky et al.,
2012; Csurka, 2017), natural language processing (Ruder et al., 2019; Devlin et al., 2019), biology (Sevakula
et al., 2019), and materials science (Yamada et al., 2019; Wu et al., 2019; Ju et al., 2021). Notably, most of
the outstanding successes of TL to date have relied on naive analytic procedures using deep neural networks.
For example, a conventional method reuses feature representations encoded in an intermediate layer of a
pre-trained model as an input for the target task, or uses samples from the target domain to fine-tune the
parameters of the pre-trained source model (Yosinski et al., 2014). While such methods are operationally
plausible and intuitive, they lack methodological principles and remain theoretically unexplored in terms of
their learning capability for limited data. This study develops a principled methodology generally applicable
to various kinds of TL.
In this study, we focus on supervised TL settings. In particular, we deal with settings where, given feature
representations obtained from training in the source domain, we use samples from the target domain to
model and estimate their shift to the target. This procedure is called hypothesis transfer learning (HTL);
several methods have been proposed, such as using a linear transformation function (Kuzborskij & Orabona,
2013; 2017) or considering a general class of continuous transformation functions (Du et al., 2017). If the
transformation function appropriately captures the functional relationship between the source and target
domains, only the domain-specific factors need to be additionally learned, which can be done efficiently
even with a limited sample size. In other words, the performance of HTL depends strongly on whether the
transformation function appropriately represents the cross-domain shift. However, the general methodology
for modeling and estimating such domain shifts has been less studied.
1Under review as submission to TMLR
This study develops a TL methodology to estimate cross-domain shifts and domain-specific factors simulta-
neously and separately using given target samples. The HTL framework we employ considers two different
transformation functions: one to represent and estimate domain-specific factors and the other to adapt
them to the target domain in combination with the source features. For these transformation functions, we
derive a class of theoretically optimal transformation functions based on the assumptions of invertibility and
differentiability as well as consistency, i.e., that the optimal predictor remains unchanged through the two
transformations. The resulting function class takes the form of an affine coupling g1+g2·g3of three functions
g1,g2andg3, where the cross-domain shift is represented by the functions g1andg2, and the domain-specific
factors are represented by g3. These functions can be estimated simultaneously using conventional supervised
learning algorithms such as kernel methods or deep neural networks. Hereafter, we refer to this framework as
theaffine model transfer .
The affine coupling used in the affine model transfer is the basic model architecture of invertible neural
networks, which is widely used in several fields including generative modeling (Papamakarios et al., 2021; Dinh
et al., 2014; 2017; Kingma & Dhariwal, 2018). In spite of its simple architecture, invertible neural networks
with affine coupling layers are known to have universal approximation ability (Teshima et al., 2020), which
means that the proposed model class has the potential to represent a broad class of transformation functions.
Furthermore, when we use the intermediate layers of a source neural network as the feature representations
in the target domain, the affine model transfer is identical to the ordinary TL based on feature extractions.
As described, we can formulate a wide variety of TL algorithms within the affine model transfer, including
neural transfer as a special case.
To summarize, the contributions of our study are as follows:
•The affine model transfer is proposed to adapt source features to the target domain by separately
estimating cross-domain shift and domain-specific factors.
•Several existing methods of HTL are encompassed in the affine model transfer, including neural
network-based TL.
•The affine model transfer can work with any type of source model. For example, non-machine learning
models such as physical models can be used. It can also handle multiple source models without loss
of generality.
•For each of the three functions g1,g2, andg3, we provide an efficient and stable estimation algorithm
when modeled using the kernel method.
•Two theoretical properties of the affine transfer model are shown: the generalization bound and the
excess risk bound.
With several applications, we compare the affine model transfer with other TL algorithms, discuss its
strengths and weaknesses, and demonstrate the advantage of being able to estimate cross-domain shifts and
domain-specific factors separately.
2 Transfer learning via transformation function
2.1 Affine model transfer
This study considers regression problems with squared loss. We assume that the output of the target domain
y∈Y ⊂ Rfollowsy=ft(x) +ϵ, whereft:X →Ris the true model on the target domain, and the
observation noise ϵhas mean zero and variance σ2. We are given nsamples{(xi,yi)}n
i=1∈(X×Y )nfrom
the target domain and the feature representation fs(x)∈Fsfrom one or more source domains. Typically, fs
is given as a vector fs(x) = [f1(x),f2(x),...,fM(x)]⊤, including the output of the source models, observed
data in the source domains or learned features in a pre-trained source neural network, and so on, but it can
also be a non-vector feature such as a tensor, graph or text. Hereafter, fsis referred to as the source features.
Du et al. (2017) provided a TL framework using transformation functions as follows:
2Under review as submission to TMLR
1.With the source features, perform a variable transformation of the observed outputs as zi=
ϕ(yi,fs(xi)), using the data transformation function ϕ:Y×Fs→R.
2.Train an intermediate model ˆg(x)using the transformed sample set {(xi,zi)}n
i=1to predict the
transformed output zfor any given x.
3.Obtain a target model ˆft(x) =ψ(ˆg(x),fs(x))using the model transformation function ψ:R×Fs→Y
that combines ˆgandfsto define a predictor.
In particular, Du et al. (2017) considers the case where the model transformation function ψis equal to
the inverse of the data transformation function ϕ−1. We consider a more general case that eliminates this
constraint.
The proposed class of TL methods includes several methods proposed in previous studies. For example,
Kuzborskij & Orabona (2013; 2017) proposed a learning algorithm consisting of linear data transformation
and linear model transformation: ϕ=y−⟨θ,fs(x)⟩andψ=g(x) +⟨θ,fs(x)⟩with pre-defined coefficients θ.
In this case, factors unexplained by the linear combination of source features are learned with g, and the target
output is predicted additively with the common factor ⟨θ,fs(x)⟩and the additionally learned g. In Minami
et al. (2021), it is shown that a type of Bayesian TL is equivalent to the following transformation functions;
forFs⊂R,ϕ= (y−τfs(x))/(1−τ)andψ=ρg(x) + (1−ρ)fs(x)with two varying hyperparameters
τ <1and0≤ρ≤1. This includes TL using density ratio estimation (Liu & Fukumizu, 2016) and neural
network-based fine-tuning as special cases when the two hyperparameters belong to specific regions.
For simplicity, we denote the transformation functions as ϕfs(·) =ϕ(·,fs(x))andψfs(·) =ψ(·,fs(x)). To
derive the optimal class of ϕandψ, we make the following assumptions:
Assumption 1 (Differentiability) .The data transformation function ϕis differentiable with respect to the
first argument.
Assumption 2 (Invetribility) .The model transformation function ψis invertible with respect to the first
argument, i.e., its inverse ψ−1
fsexists.
Assumption 3 (Consistency) .For any distribution on the target domain pt(x,y), and for all x∈X,
ψfs(g∗(x)) =Ept[Y|X=x],
whereg∗(x) =Ept[ϕfs(Y)|X=x].
The regression function that minimizes the mean squared error is given by the conditional mean. In
Assumption 3, g∗is defined to be the best predictor function for the transformed variable z=ϕfs(y)in terms
of the mean squared error. Assumption 3 states that composing the optimal g∗with the model transformation
functionψfsleads to the best predictor Ept[Y|X=x]for the target domain. This assumption corresponds to
the unbiased condition of Du et al. (2017).
Under these assumptions, we derive the optimal class of the transformation functions which minimize the
mean squared error.
Theorem 1. Letg1,g2:Fs→Rdenote arbitrary functions. If Assumptions 1-3 hold, then the transformation
functionsϕandψsatisfy the following two properties:
(i)ψ−1
fs=ϕfs.
(ii)ψfs(g) =g1(fs) +g2(fs)·g.
Proof.According to Assumption 3, it holds that for any pt(y|x),
ψfs/parenleftbigg/integraldisplay
ϕfs(y)pt(y|x)dy/parenrightbigg
=/integraldisplay
ypt(y|x)dy. (1)
(i) Letδy0be the Dirac delta function supported on y0. Substituting pt(y|x) =δy0into Eq. (1), we have
ψfs(ϕfs(y0)) =y0(∀y0∈Y).
3Under review as submission to TMLR
Under Assumption 2, this implies the property (i).
(ii) For simplicity, we assume the inputs xare fixed and pt(y|x)>0. Applying the property (i) to Eq. (1)
yields/integraldisplay
ϕfs(y)pt(y|x)dy=ϕfs/parenleftbigg/integraldisplay
ypt(y|x)dy/parenrightbigg
.
We consider a two-component mixture pt(y|x) = (1−ϵ)q(y|x) +ϵh(y|x)with a mixing rate ϵ∈[0,1], whereq
andhdenote arbitrary probability density functions. Then, we have
/integraldisplay
ϕfs(y)/braceleftbig
(1−ϵ)q(y|x) +ϵh(y|x)/bracerightbig
dy=ϕfs/parenleftbigg/integraldisplay
y/braceleftbig
(1−ϵ)q(y|x) +ϵh(y|x)/bracerightbig
dy/parenrightbigg
.
Taking the derivative at ϵ= 0, we have
/integraldisplay
ϕfs(y)/braceleftbig
h(y|x)−q(y|x)/bracerightbig
dy=ϕ′
fs/parenleftbigg/integraldisplay
yq(y|x)dy/parenrightbigg/parenleftbigg/integraldisplay
y/braceleftbig
h(y|x)−q(y|x)/bracerightbig
dy/parenrightbigg
,
which yields/integraldisplay/braceleftbig
h(y|x)−q(y|x)/bracerightbig/braceleftbig
ϕfs(y)−ϕ′
fs/parenleftbig
Eq[Y|X]/parenrightbig
y/bracerightbig
dy= 0. (2)
For Eq. (2)to hold for any qandh,ϕfs(y)−ϕ′
fs/parenleftbig
Eq[Y|X=x]/parenrightbig
ymust be independent of y. Thus, the
functionϕfsand its inverse ψfs=ϕ−1
fsare limited to affine transformations.
Theorem 1 implies that the mean squared error is minimized when the data and model transformation
functions are given by an affine transformation and its inverse, respectively. In summary, the optimal class
for HTL is expressed as follows:
H=/braceleftbig
x∝⇕⊣√∫⊔≀→g1(fs(x)) +g2(fs(x))·g3(x)|g1∈G1,g2∈G2,g3∈G3/bracerightbig
,
whereG1,G2andG3are the arbitrarily function classes. Here, each of g1andg2is modeled as a function of
fsthat represents common factors across the source and target domains. g3is modeled as a function of x, in
order to capture the domain-specific factors unexplainable by the source features.
We have derived the optimal form of the transformation functions when the squared loss is employed. Even
for general convex loss functions, (i) of Theorem 1 still holds. However, (ii) of Theorem 1 does not generally
hold because the optimal transformation function depends on the loss function. Optimal models for various
convex loss functions are discussed in Appendix A.1.
Here, the affine transformation is found to be optimal in terms of minimizing the mean squared error. We
can also derive the same optimal function by minimizing the upper bound of the estimation error in the HTL
procedure, as discussed in Appendix A.2.
2.2 Relation to existing methods
The affine model transfer encompasses a learning scheme without transfer; i.e., by setting g1(fs) = 0and
g2(fs) = 1, the prediction model is estimated without using the source features. This corresponds to an
ordinary direct learning procedure.
In prior work, Kuzborskij & Orabona (2013) employs a two-step procedure where the source features are
combined with pre-defined weights, and then the auxiliary model is additionally learned for the residuals
unexplainable by the source features. The affine model transfer can represent this HTL as a special case
by setting g2(fs) = 1, but differs in the following two aspects. First, while the existing method models
only the difference (residuals) from the source domains, our model can also consider the cross-domain ratio
relationship, i.e., we also consider the function g2(fs). Another distinctive feature of affine model transfer lies
in the learning procedure that can estimate the data and model transformation functions simultaneously, as
described in Section 3.
4Under review as submission to TMLR
(a) Direct learning
 (b) Feature extraction
 (c) HTL–offset
 (d) Affine model transfer
Figure 1: Model architectures for the affine model transfer and related procedures. (a) Direct learning
predicts outputs using only the original inputs x, while (b) feature extraction-based neural transfer predicts
outputs using only the source features fs. (c) The HTL procedure proposed in Kuzborskij & Orabona
(2013) (HTL–offset) constructs the predictor as the sum of g1(fs)andg3(x). (d) The affine model transfer
encompasses these procedures, computing g1andg2as functions of the source features and constructing the
predictor as an affine combination with g3.
The affine model transfer can be naturally expressed as an architecture of network networks. This architecture,
called affine coupling layers, is widely used for invertible neural networks in flow-based generative modeling
(Dinh et al., 2014; 2017). Neural networks based on affine coupling layers have been proven to have universal
approximation ability (Teshima et al., 2020). This implies that the affine transfer model has the potential to
represent a wide range of function classes, despite its simple architecture based on the affine coupling of three
functions.
Whenapre-trainedsourcemodelisprovidedasaneuralnetwork, TLisusuallyperformedwiththeintermediate
layer as input to the model in the target domain. This is called a feature extractor or frozen featurizer and has
been experimentally and theoretically proven to have strong transfer capability as the de facto standard for
TL (Yosinski et al., 2014; Tripuraneni et al., 2020). The affine model transfer encompasses the neural feature
extractor as a special subclass, which is equivalent to setting g2(fs)g3(x) = 0. A performance comparison of
the affine model transfer with the neural feature extractor is presented in Section 5.2.
The affine model transfer can also be interpreted as generalizing the feature extractor by adding a product
termg2(fs)g3(x). This additional term allows for the inclusion of unknown factors in the transferred model
that are unexplainable by source features alone. Furthermore, this encourages the avoidance of a negative
transfer. The usual TL based only on g1(fs)attempts to explain and predict the data generation process in
the target domain using only features from the source domain. However, in the presence of domain-specific
factors, a negative transfer can occur owing to a lack of descriptive power. The additional term compensates
for this shortcoming. The comparison of behavior for the case with the non-relative source features is
described in Section 5.1.
3 Modeling and estimation
In this section, we focus on using kernel methods for the affine transfer model. Let H1,H2andH3be
reproducing kernel Hilbert spaces (RKHSs) with positive-definite kernels k1,k2andk3, which define the
feature mappings Φ1:Fs→H 1,Φ2:Fs→H 2andΦ3:X→H 3, respectively. For the proposed model class,
theℓ2-regularized empirical risk with the squared loss is given as follows:
Fα,β,γ =1
nn/summationdisplay
i=1/braceleftbig
yi−⟨α,Φ1(fs(xi))⟩H1−⟨β,Φ2(fs(xi))⟩H2⟨γ,Φ3(xi)⟩H3/bracerightbig2
+λ1∥α∥2
H1+λ2∥β∥2
H2+λ3∥γ∥2
H3,(3)
5Under review as submission to TMLR
Algorithm 1 Block relaxation algorithm (Zhou et al., 2013).
Initialize
a0,b0̸=0,c0̸=0
repeat
at+1=arg minaF(a, bt+1, ct+1)
bt+1=arg minbF(at+1, b, ct)
ct+1=arg mincF(at+1, bt+1, c)
untilconvergence
whereλ1,λ2,λ3>0are hyperparameters for the regularization. According to the representer theorem, the
minimizer of Fα,β,γwith respect to the parameters α∈H 1,β∈H 2, andγ∈H 3reduces to
α=n/summationdisplay
i=1aiΦ1(fs(xi)), β =n/summationdisplay
i=1biΦ2(fs(xi)), γ =n/summationdisplay
i=1ciΦ3(xi),
with then-dimensional unknown parameter vectors a,b,c∈Rn. Substituting this expression into Eq. (3), we
can obtain the objective function as
Fα,β,γ =1
n∥y−K1a−(K2b)◦(K3c)∥2
2+λ1a⊤K1a+λ2b⊤K2b+λ3c⊤K3c
=1
nn/summationdisplay
i=1/parenleftig
yi−k(i)⊤
1a−b⊤M(i)c/parenrightig2
+λ1a⊤K1a+λ2b⊤K2b+λ3c⊤K3c
:=F(a,b,c ).(4)
Here, the symbol ◦denotes the Hadamard product. KIis the Gram matrix associated with the kernel kI
forI∈{1,2,3}.k(i)
I= [kI(xi,x1)···kI(xi,xn)]⊤denotes the i-th column of the Gram matrix. The n×n
matrixM(i)is given by the tensor product M(i)=k(i)
2⊗k(i)
3ofk(i)
2andk(i)
3.
Because the model is linear with respect to parameter aand bilinear for bandc, the optimization of Eq. (4)
can be solved using well-established techniques for the low-rank tensor regression, such as CP-decomposition
(Harshman, 1970), Tucker decomposition (Tucker, 1966), and Tensor-Train decomposition (Oseledets, 2011).
In this study, we use the block relaxation algorithm (Zhou et al., 2013) as described in Algorithm 1. It
updatesa,b, andcby repeatedly fixing two of the three parameters and minimizing the objective function
for the remaining one. Fixing two parameters, the resulting subproblem can be solved analytically, because
the objective function is expressed in a quadratic form for the remaining parameter. Starting from arbitrary
initial values, the algorithm iteratively updates the parameters (at,bt,ct)at iteration tto(at+1,bt+1,ct+1)as
follows:
at+1= (K1+nλ1In)−1(y−(K2bt)◦(K3ct)),
bt+1= (diag(K3ct)2K2+nλ2In)−1diag(K3ct)(y−K1at+1),
ct+1= (diag(K2bt+1)2K3+nλ3In)−1diag(K2bt+1)(y−K1at+1),
whereyis a vector of nobserved outputs, Indenotes the identity matrix of size n, and diag(v)is the diagonal
matrix whose diagonal element is given by the vector v.
Algorithm 1 alternately estimates the parameters (a,b)of the transformation function and the parameters
cin the predictive model of the transformed output with the given transformed dataset {(xi,zi)}n
i=1. The
consistency and asymptotic normality of this estimator have been proven in Zhou et al. (2013).
4 Theoretical results
In this section, we present two theoretical properties, the generalization bound and excess risk bound.
6Under review as submission to TMLR
Let(Z,P)be an arbitrary probability space, and set {zi}n
i=1to be independent random variables distributed
according to P. For a function f:Z→R, define the expectation of fwith respect to Pand its empirical
counterpart as
Pf=EPf(z), P nf=1
nn/summationdisplay
i=1f(zi).
Letℓ(y,y′)be a non-negative loss bounded from above by L>0, such that for any fixed y′∈Y,y∝⇕⊣√∫⊔≀→ℓ(y,y′)
isµℓ-Lipschitz for some µℓ>0.
Recall that the function class proposed in this work is
H=/braceleftbig
x∝⇕⊣√∫⊔≀→g1(fs(x)) +g2(fs(x))·g3(x)|g1∈G1,g2∈G2,g3∈G3/bracerightbig
.
In particular, the following discussion in this section assumes that g1,g2, andg3are represented by linear
functions on the RKHSs.
4.1 Generalization bound
The optimization problem is expressed as follows:
min
α,β,γPnℓ/parenleftbig
y,⟨α,Φ1⟩H1+⟨β,Φ2⟩H2⟨γ,Φ3⟩H3/parenrightbig
+λα∥α∥2
H1+λβ∥β∥2
H2+λγ∥γ∥2
H3, (5)
where Φ1= Φ 1(fs(x)),Φ2= Φ 2(fs(x))andΦ3= Φ 3(x)denote the feature maps, and λα,λβ,λγ>0are
the regularization parameters. Without loss of generality, it is assumed that ∥Φ1∥2
H1≤1,∥Φ2∥2
H2≤1, and
∥Φ3∥2
H3≤1. Hereafter, we will omit the suffixes H1,H2andH3in the norms if there is no ambiguity.
Let(ˆα,ˆβ,ˆγ)be a solution of Eq. (5). For any α, we have
λα∥ˆα∥2≤Pnℓ(y,⟨ˆα,Φ1⟩+⟨ˆβ,Φ2⟩⟨ˆγ,Φ3⟩) +λα∥ˆα∥2+λβ∥ˆβ∥2+λγ∥ˆγ∥2
≤Pnℓ(y,⟨α,Φ1⟩) +λα∥α∥2.(6)
The first inequality holds because ℓ(·,·)and∥·∥are non-negative. For the second inequality, we use the fact
that the parameter set (ˆα,ˆβ,ˆγ)is the minimizer of Eq. (5). Denoting ˆRs=infα{Pnℓ(y,⟨α,Φ1⟩)+λα∥α∥2},we
obtain∥ˆα∥2≤λ−1
αˆRs.Because the same inequality as Eq. (6)holds forλβ∥ˆβ∥2,λγ∥ˆγ∥2andPnℓ(y,ˆh), we have
∥ˆβ∥2≤λ−1
βˆRs,∥ˆγ∥2≤λ−1
γˆRs,andPnℓ(y,ˆh)≤ˆRs.Moreover, we obtain Pℓ(y,ˆh) =E[Pnℓ(y,ˆh)]≤E[ˆRs].
Therefore, it is sufficient to consider the following hypothesis class Hand loss classL:
H=/braceleftbig
x∝⇕⊣√∫⊔≀→⟨α,Φ1⟩+⟨β,Φ2⟩⟨γ,Φ3⟩
| ∥α∥2≤λ−1
αˆRs,∥β∥2≤λ−1
βˆRs,∥γ∥2≤λ−1
γˆRs,Pℓ(y,h)≤E[ˆRs]/bracerightbig
,
L=/braceleftbig
(x,y)∝⇕⊣√∫⊔≀→ℓ(y,h)|h∈H/bracerightbig
.
Here, we show the generalization bound of the proposed model class. The following theorem is based on
Kuzborskij & Orabona (2017), showing that the difference between the generalization error and the empirical
error of this hypothesis class can be bounded using the magnitude of the relevance of the source and target
domains.
Theorem 2 (Generalization bound) .There exists a constant Cdepending only on λα,λβ,λγandLsuch
that, for any η>0andh∈H, with probability at least 1−e−η,
Pℓ(y, h)−Pnℓ(y, h) = ˜O/parenleftbigg/parenleftbigg/radicalbigg
Rs
n+µ2
ℓC2+√η
n/parenrightbigg/parenleftbigg√
LC+/radicalbig
Lη/parenrightbigg
+C2L+Lη
n/parenrightbigg
,
whereRs= infα{Pℓ(y,⟨α,Φ1⟩) +λα∥α∥2}.
7Under review as submission to TMLR
The proof is given in Appendix B.1.
Because Φ1is the feature map from the source feature space Fsonto the RKHSH1,Rscorresponds to
the true risk of training in the target domain using only the source features fs. If this is sufficiently small,
e.g.,Rs=˜O(n−1), the convergence rate indicated by Theorem 2 becomes n−1, which is an improvement
over the naive convergence rate n−1/2. This means that if training in the source domain yields feature
representations strongly related to the target domain, the convergence of training in the target domain is
accelerated. Theorem 2 measures this cross-domain relation using the metric Rs.
Theorem 2 is based on Theorem 11 of Kuzborskij & Orabona (2017) in which the function class g1+g3is
considered. Our work differs in the following two points: the source features are modeled not only additively
but also multiplicatively, i.e., we consider the function class g1+g2·g3, and we also consider the estimation
of the parameters for the source feature combination, i.e., the parameters of the functions g1andg2. In
particular, the latter affects the resulting rate in Theorem 2. Without estimating the source combination
parameters, the rate indicated by Theorem 2 improves only up to n−3/4. The details are discussed in
Appendix B.1.
4.2 Excess risk bound
In this section, we analyze the excess risk, which is the difference between the risk of the estimated function
and the smallest possible risk within the function class.
Recall that we consider the functions g1,g2andg3to be the elements of the RKHSs H1,H2andH3with
kernelsk1,k2andk3, respectively. Define the kernel k(1)=k1,k(2)=k2·k3andk=k(1)+k(2). Let
H(1),H(2)andHbe the RKHS with k(1),k(2)andkrespectively. For m= 1,2, consider the normalized Gram
matrixK(m)=1
n(k(m)(xi,xj))i,j=1,...,nand its eigenvalues (ˆλ(m)
i)n
i=1, arranged in nonincreasing order.
We prepare the following additional assumptions:
Assumption 4. There exists an h∗∈HsatisfyingP(y−h∗(x))2=infh∈HP(y−h(x))2. Similarly, there
exists anh(m)∗∈H(m)satisfyingP(y−h(m)∗(x))2= infh∈H(m)P(y−h(x))2form= 1,2.
Assumption 5. Form= 1,2, there exist positive real numbers am>0andsm∈(0,1)such that ˆλ(m)
j≤
amj−1/sm.
Assumption 4 is used in Bartlett et al. (2005) and is not overly restrictive as it holds for many regularization
algorithms and convex, uniformly bounded function classes.
In the analysis of kernel methods, Assumption 5 is standard (Steinwart & Christmann, 2008), and is known
to be equivalent to the classical covering or entropy number assumption (Steinwart et al., 2009). smmeasures
the complexity of the RKHS, with larger values corresponding to more complex function spaces.
Under Assumption 4, we obtain the following excess risk bound for the proposed model class. The proof is
based on Bartlett et al. (2005) and shown in Appendix B.2.
Theorem 3. Letˆhbe any element of HsatisfyingPnℓ(y,ˆh(x)) = infh∈HPnℓ(y,h(x)). Suppose that
Assumption 4 is satisfied, then there exists a constant cdepending only on µℓsuch that for any η>0, with
probability at least 1−5e−η,
P(y−ˆh(x))2−P(y−h∗(x))2≤c
 min
0≤κ1,κ2≤n

κ1+κ2
n+
1
nn/summationdisplay
j=κ1+1ˆλ(1)
j+n/summationdisplay
j=κ2+1ˆλ(2)
j
1
2

+η
n
.
Theorem 3 is a multiple-kernel version of Corollary 6.7 of Bartlett et al. (2005), and a data-dependent version
of Theorem 2 of Kloft & Blanchard (2011) which considers the eigenvalues of the Hilbert-Schmidt operators
onHandH(m). Theorem 3 concerns the eigenvalues of the Gram matrices K(m)computed from the data.
The following corollary follows from Theorem 3 and Assumption 5.
8Under review as submission to TMLR
Corollary 4. Letˆhbe any element of HsatisfyingPn(y−ˆh(x))2=infh∈HPn(y−h(x))2. Suppose that
Assumption 4 and 5 are satisfied, then for any η>0, with probability at least 1−5e−η,
P(y−ˆh(x))2−P(y−h∗(x))2=O/parenleftig
n−1
1+max{s1,s2}/parenrightig
.
Corollary 4 suggests that the convergence rate of the excess risk depends on the decay rates of the eigenvalues of
two Gram matrices K(1)andK(2).s1is the decay rate of eigenvalues of K(1)=1
n(k1(fs(xi),fs(xj)))i,j=1,...,n,
representing the learning efficiency using only the source features. s2is the decay rate of the eigenvalues of the
Hadamard product of the Gram matrices K2=1
n(k2(fs(xi),fs(xj))i,j=1,...,nandK3=1
n(k3(xi,xj))i,j=1,...,n.
The effect of combining the source features and the original inputs appears here. In general, it is difficult
to discuss the relationship between the spectra of two Gram matrices K2,K3and their Hadamard product
K2◦K3. Intuitively, the smaller the overlap between the space spanned by source features fsand by
the original input x, the smaller the overlap between H2andH3becauseH2is defined by the kernel
k2(fs(x),fs(x′))andH3is defined by the kernel k3(x,x′). In other words, as the source features fsand the
original input xhave different information, the tensor product H2⊗H 3will be more complex, and the decay
rates2is expected to be larger. In Appendix C, we experimentally confirm the relationship between the
decay rates2and the overlap of the space spanned by xandfs.
5 Experimental results
We demonstrate the potential of the affine model transfer through three different case studies: (i) the
prediction of feed-forward torque at seven joints of the SARCOS anthropomorphic robot arm (Williams
& Rasmussen, 2006), (ii) the prediction of lattice thermal conductivity of inorganic crystalline materials
(Yamada et al., 2019), (iii) TL for bridging the gap between experimental and theoretical values of specific
heat capacity for organic polymers (Hayashi et al., 2022). Experimental details are described in Appendix D.
The Python code is available at https://github.com/mshunya/AffineTL .
5.1 Kinematics of the robot arm
We experimentally investigated the learning performance of the affine model transfer, compared to several
naive methods. The objective is to predict the feed-forward torques, required to follow the desired trajectory,
at seven different joints of the SARCOS anthropomorphic robot arm (Williams & Rasmussen, 2006). Twenty-
one features representing the joint position, velocity, and acceleration were used as the input variable
x∈Rd(d= 21). The target task was to predict the torque value at one joint, and the source task was defined
as the prediction of torque at the other six joints. The experiments were conducted with seven different tasks
(denoted as Torque 1-7) corresponding to the seven different joints. The vector of the source torque values
was used as the source feature. For each target region, a training set of size n∈{5,10,15,20,30,40,50}was
randomly constructed 20 times, and the remaining samples were used as the test sets (Appendix D). The
experiment was designed for TL with a fairly small sample size.
For comparison, the following seven procedures were tested, including two existing HTL models (Kuzborskij
& Orabona, 2013; Du et al., 2017):
No transfer
Train a model using input xwith no transfer.
Only source
Train a model g1(f1)using only the source feature fsas input.
Input augmentation
Perform an ordinary regression with the augmented input vector concatenating xandfs.
HTL–offset (Kuzborskij & Orabona, 2013)
Calculate the transformed output zi=yi−g1(fs)whereg1(fs)is the model pre-trained using
Only source , and train an additional model with input xito predictzi.
HTL–scale (Du et al., 2017)
Calculate the transformed output zi=yi/g1(fs)whereg1(fs)is the model pre-trained using
Only source , and train an additional model with input xito predictzi.
9Under review as submission to TMLR
/uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000035/uni00000052/uni00000052/uni00000057/uni00000003/uni00000050/uni00000048/uni00000044/uni00000051/uni00000003/uni00000056/uni00000054/uni00000058/uni00000044/uni00000055/uni00000048/uni00000047/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000000b/uni00000031/uni00000050/uni0000000c/uni00000031/uni00000052/uni00000003/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000048/uni00000055
/uni00000032/uni00000051/uni0000004f/uni0000005c/uni00000003/uni00000056/uni00000052/uni00000058/uni00000055/uni00000046/uni00000048
/uni00000024/uni00000055/uni0000004a/uni00000058/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047
/uni0000002b/uni00000037/uni0000002f/uni00000010/uni00000052/uni00000049/uni00000049/uni00000056/uni00000048/uni00000057
/uni0000002b/uni00000037/uni0000002f/uni00000010/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048
/uni00000024/uni00000049/uni00000049/uni0000004c/uni00000051/uni00000048/uni00000037/uni0000002f/uni00000010/uni00000049/uni00000058/uni0000004f/uni0000004f
/uni00000024/uni00000049/uni00000049/uni0000004c/uni00000051/uni00000048/uni00000037/uni0000002f/uni00000010/uni00000046/uni00000052/uni00000051/uni00000056/uni00000057
(a) Target domain: Torque 1
/uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000035/uni00000052/uni00000052/uni00000057/uni00000003/uni00000050/uni00000048/uni00000044/uni00000051/uni00000003/uni00000056/uni00000054/uni00000058/uni00000044/uni00000055/uni00000048/uni00000047/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000000b/uni00000031/uni00000050/uni0000000c/uni00000031/uni00000052/uni00000003/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000048/uni00000055
/uni00000032/uni00000051/uni0000004f/uni0000005c/uni00000003/uni00000056/uni00000052/uni00000058/uni00000055/uni00000046/uni00000048
/uni00000024/uni00000055/uni0000004a/uni00000058/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047
/uni0000002b/uni00000037/uni0000002f/uni00000010/uni00000052/uni00000049/uni00000049/uni00000056/uni00000048/uni00000057
/uni0000002b/uni00000037/uni0000002f/uni00000010/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048
/uni00000024/uni00000049/uni00000049/uni0000004c/uni00000051/uni00000048/uni00000037/uni0000002f/uni00000010/uni00000049/uni00000058/uni0000004f/uni0000004f
/uni00000024/uni00000049/uni00000049/uni0000004c/uni00000051/uni00000048/uni00000037/uni0000002f/uni00000010/uni00000046/uni00000052/uni00000051/uni00000056/uni00000057
(b) Target domain: Torque 7
Figure 2: Box plots showing the distribution of the root mean squared errors (RMSE) for the seven analysis
procedures at (a) Torque 1 and (b) Torque 7. Each box plot showed the median and the first and third
quartiles.
Affine transfer (full)
Train the model g1+g2·g3.
Affine transfer (constrained)
Train the model g1+g3.
Kernel ridge regression with the radial basis function (RBF) kernel exp(−∥x−x′∥2/2ℓ2)was used to train
the model for each procedure. The scale parameter ℓwas fixed to the square root of the dimension of the
input. The regularization parameter in the kernel ridge regression and λα,λβandλγin the affine model
transfer were selected through five-fold cross-validation.
Table 1 summarizes the prediction performance of the seven different procedures for varying numbers of
training samples in the seven tasks. In most cases, the affine transfer model achieved the best prediction
performance in terms of the root mean squared error (RMSE). In several other cases, direct learning without
transfer produced the best results; in almost all cases, ordinary TL only using the source features and the two
existing HTL models showed no advantage over the affine transfer model. In this experiment, the performance
was evaluated in cases where the number of training samples was extremely small. The advantage of the
affine transfer model tended to be greater in these circumstances, such as with a sample size of n= 5or 10.
Figure 2 highlights the RMSE values for Torque 1 and Torque 7. The joint of Torque 1 is located closest
to the root of the arm. Therefore, the learning task for Torque 1 is less relevant to those for the other
joints, and the transfer from Torque 2-6 to Torque 1 would not work. In fact, as shown in Figure 2(a) and
Table 1, relying only on the source features ( Only source ) failed to acquire predictive ability. In addition,
HTL–offset andHTL–scale likewise showed poor prediction performance owing to the negative effect of
the failure in the variable transformation using the values of the other torques. In particular, the two HTL
models achieved lower predictive performance than direct learning ( No transfer ), resulting in the occurrence
of negative transfer.
Torque 7 was measured at the joint closest to the end of the arm. Therefore, Torque 7 strongly depends
on those at the other six joint positions, and the procedures based on the source features, including Only
10Under review as submission to TMLR
Target ModelNumber of training samples
n<d n ≈d n>d
5 10 15 20 30 40 50
Torque 1Direct 21.2±2.05 19.0±2.04 17.3±1.66 15.8 ±1.57 13.7 ±1.42 12.2±1.56 10.8±1.11
Only 24.9 ±11.4 20.3±1.77 19.5±1.42 19.3 ±1.35 18.3 ±1.92 18.0±1.76 17.5±1.63
Augmented 21.3 ±2.35 18.9±1.7116.8±1.27 15.3 ±1.97 12.7 ±1.56 11.0±1.58 9.93±1.08
Offset 24.6 ±11.5 19.1±2.02 17.7±1.58 17.1 ±2.15 15.2 ±3.37 14.3±3.82 12.8±2.97
Scale 24.9 ±8.53 20.3±3.32 20.0±7.54 19.1 ±3.19 17.8 ±2.83 18.5±3.25 18.1±4.44
AffineTL-full 21.3 ±2.10 18.9±2.09 17.4±1.93 15.6 ±1.68 13.4 ±1.95 11.6±1.48 10.4±0.925
AffineTL-const 21.4 ±1.9118.7±1.91 17.2±1.38 15.7 ±1.85 13.0 ±1.43 11.5±1.47 10.3±0.994
Torque 2Direct 15.8 ±2.38 13.0±1.42 11.5±0.966 10.4 ±0.821 9.39 ±0.978 8.38±0.767 7.72±0.753
Only 15.3 ±2.31 13.0±1.51 12.4±2.22 11.9 ±3.02 12.1 ±7.43 10.2±1.82 9.45±1.99
Augmented 15.7 ±2.48 12.7±1.4711.1±1.33 9.96 ±1.41 8.65 ±1.05 7.65±0.929 6.99±0.615
Offset 15.2 ±2.28 12.8±1.62 12.0±2.45 11.8 ±3.11 11.9 ±7.51 9.89±1.87 9.12±2.08
Scale 15.2 ±2.2912.6±1.59 12.1±2.32 11.7 ±3.12 11.9 ±7.52 9.95±1.86 9.15±2.05
AffineTL-full 14.4±1.60 12.7±1.82 11.5±1.93 10.8 ±1.68 9.58 ±1.97 7.96±1.04 7.52±0.674
AffineTL-const 14.6 ±1.86 12.8±1.45 11.4±1.48 10.5 ±1.64 9.39 ±1.79 8.17±1.06 7.58±0.870
Torque 3Direct 9.93 ±1.65 8.17±0.996 7.84±2.60 6.97 ±1.10 5.97 ±0.917 5.33±0.942 4.56±0.401
Only 8.99 ±2.98 7.62±2.35 6.91±1.65 6.45 ±1.20 5.66 ±0.908 5.31±0.968 4.95±0.964
Augmented 9.66 ±1.72 7.78±0.978 6.74±1.01 6.25 ±1.15 5.29 ±1.274.68±1.24 4.03±0.652
Offset 8.96 ±2.98 7.48±2.31 6.87±1.65 6.42 ±1.21 5.60 ±0.844 5.23±0.993 4.85 ±1.01
Scale 9.06 ±2.94 7.59±2.29 6.91±1.42 6.69 ±1.41 5.65 ±0.964 5.41±1.22 4.98±0.888
AffineTL-full 8.64±1.33 7.22±1.41 6.67±1.27 6.07 ±1.00 5.25 ±1.17 4.89±1.15 4.28±0.829
AffineTL-const 8.94 ±1.40 7.33±1.206.50±1.14 5.97 ±0.918 5.18 ±1.00 4.76±0.958 4.22±0.745
Torque 4Direct 14.2 ±2.30 11.1±2.39 9.49±2.18 7.80 ±0.978 6.91 ±0.778 6.06±0.630 5.47±0.653
Only 12.3 ±3.60 9.23±2.43 7.81±1.74 6.83 ±1.45 6.21 ±1.02 6.19±1.37 5.22±0.629
Augmented 13.8 ±2.83 9.69±1.64 8.52±1.80 7.06 ±1.03 5.97 ±0.9055.16±0.740 4.69±0.698
Offset 12.3 ±3.62 9.08±2.357.67±1.68 6.73±1.40 6.14 ±1.01 6.16±1.38 5.12±0.582
Scale 12.3 ±3.62 9.10±2.36 7.72±1.62 6.74 ±1.37 6.12 ±1.04 6.16±1.38 5.14±0.524
AffineTL-full 12.0±3.11 8.95±2.05 7.89±1.92 6.83 ±1.75 5.66±1.07 5.27±1.12 4.87±1.02
AffineTL-const 12.2 ±3.408.64±1.95 7.87±1.87 6.44±1.09 5.67±1.12 5.25±1.06 4.78±0.665
Torque 5Direct 1.08 ±0.169 0.986±0.0901 0.932±0.165 0.860 ±0.127 0.737 ±0.123 0.686±0.09370.608±0.0705
Only 1.11 ±0.155 1.01±0.0894 1.02±0.146 0.964 ±0.148 0.846 ±0.125 0.797±0.111 0.739±0.103
Augmented 1.08 ±0.160 0.985±0.0898 0.895±0.125 0.849±0.135 0.737 ±0.129 0.679±0.102 0.623±0.110
Offset 1.11 ±0.173 0.998±0.0949 0.982±0.163 0.944 ±0.152 0.806 ±0.113 0.738±0.110 0.693±0.0987
Scale 1.15 ±0.248 0.993±0.0933 0.970±0.151 0.939 ±0.124 0.806 ±0.0966 0.754±0.0842 0.776 ±0.211
AffineTL-full 1.03 ±0.1210.935±0.126 0.878±0.129 0.862±0.129 0.762 ±0.144 0.726±0.117 0.635±0.068
AffineTL-const 1.04 ±0.114 0.971±0.0999 0.897±0.113 0.888 ±0.129 0.739 ±0.122 0.702±0.092 0.629±0.0672
Torque 6Direct 1.86 ±0.246 1.67±0.194 1.50±0.167 1.36 ±0.156 1.21 ±0.143 1.11±0.088 1.07±0.0969
Only 1.95 ±0.250 1.88±0.407 1.79±0.206 1.80 ±0.378 1.61 ±0.216 1.58±0.173 1.55±0.200
Augmented 1.84 ±0.171 1.65±0.2001.48±0.183 1.33 ±0.207 1.17 ±0.200 1.03±0.117 0.964±0.115
Offset 1.92 ±0.257 1.84±0.426 1.72±0.262 1.71 ±0.421 1.44 ±0.271 1.39±0.245 1.39±0.289
Scale 1.91 ±0.256 1.89±0.425 1.81±0.326 1.84 ±0.398 1.68 ±0.300 1.59±0.248 1.59±0.242
AffineTL-full 1.82±0.229 1.64±0.191 1.58±0.224 1.41 ±0.248 1.24 ±0.212 1.13±0.307 0.996±0.0963
AffineTL-const 1.86 ±0.202 1.7±0.179 1.55±0.275 1.45 ±0.276 1.23 ±0.209 1.09±0.141 1.02±0.0923
Torque 7Direct 2.67 ±0.321 2.12±0.420 1.84±0.421 1.53 ±0.305 1.34 ±0.203 1.17±0.126 1.05±0.0960
Only 2.29 ±0.583 1.76±0.441 1.55±0.407 1.42 ±0.585 1.16 ±0.243 0.999±0.231 0.942±0.164
Augmented 2.55 ±0.408 1.90±0.433 1.68±0.417 1.39 ±0.367 1.20 ±0.236 1.01±0.142 0.901±0.112
Offset 2.29 ±0.588 1.71±0.405 1.55±0.408 1.41 ±0.586 1.15 ±0.249 0.995±0.233 0.935±0.167
Scale 2.32 ±0.58 1.75±0.428 1.59±0.395 1.42 ±0.569 1.21 ±0.249 1.06±0.249 0.967±0.161
AffineTL-full 2.29±0.533 1.75±0.4471.49±0.380 1.30±0.327 1.07 ±0.250 0.975±0.180 0.889±0.145
AffineTL-const 2.32 ±0.5521.71±0.419 1.49±0.373 1.26±0.257 1.06 ±0.220 0.950±0.163 0.885±0.156
Table 1: Performance on predicting the torque values at seven different joints of the SARCOS anthropomorphic
robot arm. The mean and standard deviation of the root mean square error with respect to 20 test sets are
reported for varying numbers of training samples and the seven different tasks. Seven different methods
were tested: No transfer (Direct),Only source (Only),Input augmentation (Augmented), HTL-offset
(Offset),HTL-scale (Scale),Affine transfer (full) (AffineTL-full), and Affine transfer (constrained)
(AffineTL-const), respectively.
source, were more effective than in the other tasks. In particular, the affine model transfer achieved the
best performance among the other methods. This is consistent with the theoretical result that the transfer
capability of the affine model transfer could be further improved when the risk of learning using only the
source features is sufficiently small.
11Under review as submission to TMLR
1st layer 2nd layer 3rd layer
Extracted layer253035404550Root mean squared error (W/mK)Feature extractor
Affine transfer
Without transfer
Fine-tuning
Figure 3: Change of RMSE values between the affine transfer model and the ordinary feature extractor when
using different levels of intermediate layers as the source features. The line plot shows the mean and 95%
confidence interval. As a baseline, RMSE values for direct learning without transfer and fine-tuned neural
networks are shown as dotted and dashed lines, respectively.
5.2 Lattice thermal conductivity of inorganic crystals
Here, we describe the relationship between the qualitative differences in source features and the learning
behavior of the affine model transfer, in contrast to ordinary feature extractors using neural networks. The
target task is to predict the lattice thermal conductivity (LTC) of inorganic crystalline materials, where
the LTC is the amount of vibrational energy propagated by phonons in a crystal. In general, LTC can be
calculated ab initio by performing many-body electronic structure calculations based on quantum mechanics.
However, it is quite time-consuming to perform the first-principles calculations for thousands of crystals,
which will be used as a training sample set to create a surrogate statistical model. Therefore, we perform
TL with the source task of predicting an alternative, computationally tractable physical property called
scattering phase space (SPS), which is known to be physically related to LTC.
We used the dataset from Ju et al. (2021) that records SPS and LTC for 320 and 45 inorganic compounds,
respectively. The input compounds were translated to 290-dimensional compositional descriptors using
XenonPy (Liu et al., 2021)1. For the preliminary step, neural networks with three hidden layers that predict
SPS were trained using 80% of the 320 samples. 100 models with different numbers of neurons were randomly
generated and the top 10 source models that showed the highest generalization performance in the source
domain were selected. Then, in the target task, an intermediate layer of a source model was used as the
feature extractor. A model was trained using 40 randomly chosen samples of LTC, and its performance
was evaluated with the remaining 5 samples. For each of the 10 source models, we performed the training
and testing 10 times with different sample partitions and compared the mean values of RMSE among four
different methods: (i) the affine model transfer using neural networks to model the three functions g1,g2and
g3, (ii) a neural network using the XenonPy compositional descriptors as input without transfer, (iii) a neural
network using the source features as input, and (iv) fine-tuning of the pre-trained neural networks. The width
of the layers of each neural network, the number of training epochs, and the dropout rate were optimized
during five-fold cross-validation looped within each training set. The modeling and learning procedures are
detailed in Appendix D.2.
Figure 3 shows the change in prediction performance of TL models using source features obtained from
different intermediate layers from the first to the third layers. The affine transfer model and the ordinary
feature extractor showed opposite patterns. The performance of the feature extractor improved when the
first intermediate layer closest to the input layer was used as the source features and gradually degraded
when layers closer to the output were used. When the third intermediate layer was used, a negative transfer
occurred in the feature extractor as its performance became worse than that of the direct learning. In contrast,
the affine transfer model performs better as the second and third intermediate layers closer to the output
1https://github.com/yoshida-lab/XenonPy
12Under review as submission to TMLR
/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000018/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013
/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000055/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000057/uni00000044/uni0000004f/uni0000004f/uni0000005c/uni00000010/uni00000052/uni00000045/uni00000056/uni00000048/uni00000055/uni00000059/uni00000048/uni00000047/uni00000003/uni0000004b/uni00000048/uni00000044/uni00000057/uni00000003/uni00000046/uni00000044/uni00000053/uni00000044/uni00000046/uni0000004c/uni00000057/uni0000005c/uni00000003/uni0000000b/uni0000004f/uni00000052/uni0000004a/uni00000003/uni0000002d/uni00000012/uni0000004e/uni0000004a/uni0000002e/uni0000000c
/uni00000018/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000016/uni00000018/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000017/uni00000018/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000030/uni00000027/uni00000010/uni00000046/uni00000044/uni0000004f/uni00000046/uni00000058/uni0000004f/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni0000004b/uni00000048/uni00000044/uni00000057/uni00000003/uni00000046/uni00000044/uni00000053/uni00000044/uni00000046/uni0000004c/uni00000057/uni0000005c/uni00000003/uni0000000b/uni0000004f/uni00000052/uni0000004a/uni00000003/uni0000002d/uni00000012/uni0000004e/uni0000004a/uni0000002e/uni0000000c
/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000001d/uni00000003/uni00000013/uni00000011/uni0000001a/uni0000001a/uni0000001b/uni0000001b
Figure 4: MD-calculated (vertical axis) and experimental values (horizontal axis) of the specific heat capacity
at constant pressure for various amorphous polymers.
were used. The affine transfer model using the third intermediate layer reached a level of accuracy slightly
better than fine-tuning, which intuitively uses more information to transfer than the extracted features.
In general, the features encoded in an intermediate layer of a neural network are more task-independent as
the layer is closer to the input, and the features are more task-specific as the layer is closer to the output
(Yosinski et al., 2014). Because the first layer does not differ much from the original input, using both x
andfsin the affine model transfer does not contribute much to performance improvement. However, when
using the second and third layers as the feature extractors, the use of both xandfscontributes to improving
the expressive power of the model, because the feature extractors have acquired different representational
capabilities from the original input. In contrast, a model based only on fsfrom a source task-specific feature
extractor could not account for data in the target domain, so its performance would become worse than
direct learning without transfer, i.e., a negative transfer would occur.
5.3 Heat capacity of organic polymers
We highlight the benefits of separately modeling and estimating domain-specific factors through a case study
in polymer chemistry. The objective is to predict the specific heat capacity at constant pressure CPof any
given organic polymer with its chemical structure in the repeating unit. Specifically, we conduct TL to
bridge the gap between experimental values and physical properties calculated from molecular dynamics
(MD) simulations.
For the target domain, experimental values of CP(Cexp
P) for 70 amorphous polymers were obtained from
the polymer properties database PoLyInfo (Otsuka et al., 2011). For the source domain, the properties
(CMD
P) for 1,077 polymers were calculated from all-atom classical MD simulations in Hayashi et al. (2022).
As shown in Figure 4, there was a large systematic bias between experimental and calculated values; the
MD-calculated properties CMD
Pexhibited an evident overestimation with respect to their experimental values.
As discussed in Hayashi et al. (2022), this observation is inevitable because classical MD calculations do not
reflect the presence of quantum effects in the real system: the vibrational energy of the classical harmonic
oscillator is significantly larger than that of the quantum harmonic oscillator at the same frequency. Hence,
the upward bias was observed in CMD
P, which mainly originated from the lack of quantum effects. According
to Einstein’s theory for the specific heat in physical chemistry, the logarithmic ratio between Cexp
PandCMD
P
13Under review as submission to TMLR
Table 2: Force field parameters that form the General AMBER force field (Wang et al., 2004) version 2
(GAFF2), and their detailed descriptions.
Parameter Description
mass Atomic mass
σ Equilibrium radius of van der Waals (vdW) interactions
ϵ Depth of the potential well of vdW interactions
charge Atomic charge of Gasteiger model
r0 Equilibrium length of chemical bonds
Kbond Force constant of bond stretching
polar Bond polarization defined by the absolute value of charge difference between atoms in a bond
θ0 Equilibrium angle of bond angles
Kangle Force constant of bond bending
Kdih Rotation barrier height of dihedral angles
Table 3: Comparison of three prediction models for experimental values of specific heat capacity with and
without using the MD-calculated properties as source features (mean and standard deviation of RMSE).
Model RMSE (log J/kg ·K)
y=α0+α1fs+ϵ 0.1392±0.04631
y=fs+x⊤γ+ϵ 0.1368±0.04265
y=α0+α1fs−(βfs+ 1)x⊤γ+ϵ0.1357±0.04173
can be calibrated by the following equation (Hayashi et al., 2022):
logCexp
P= logCMD
P+ 2 log/parenleftbiggℏω
kBT/parenrightbigg
+ logexp/parenleftbigℏω
kBT/parenrightbig
/bracketleftbig
exp/parenleftbigℏω
kBT/parenrightbig
−1/bracketrightbig2. (7)
wherekBis the Boltzmann constant, ℏis the Planck constant, ωis the frequency of molecular vibrations,
andTis the temperature. The bias is a monotonically decreasing function of frequency ω, which is described
as a black-box function of polymers with their molecular features. Hereafter, we consider the calibration of
this systematic bias using the affine transfer model.
The log-transformed value of Cexp
pis modeled as
y:= logCexp
P=α0+α1fs/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
g1−(βfs+ 1)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
g2·x⊤γ/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
g3+ϵ, (8)
whereϵrepresents observation noise, and α0,α1,βandγare the unknown parameters to be estimated. The
inputxwas given as a 190-dimensional descriptor vector, called the force field descriptor, wihch encodes
compositional, structural and physicochemical features of the chemical structure for a given polymer’s
repeating unit as detailed below. The source feature fswas given as the log-transformed value of CMD
P.
Therefore, fsis no longer a function of x; this modeling was intended for calibrating the MD-calculated
properties rather than for conventional TL. When α1= 1andβ= 0, Eq.(8)is consistent with the theoretical
equation in Eq. (7)in which the quantum effect is linearly modeled as α0+x⊤γ. In addition, for reference,
we estimated two partially constrained models: y=α0+α1fs+ϵandy=fs+x⊤γ+ϵ. The former uses the
MD-calculated properties alone. The latter corresponds to the theoretical model of Eq. (7)and is consistent
with the full model in Eq . (8)in whichα1= 1andβ= 0. Of 70 samples for which both MD-calculated and
experimental values of CPwere available, 60 samples were randomly sampled, and 20 predictive models were
trained with different sample splits.
The descriptor xrepresents the distribution of the ten different force field parameters ( t∈T=
{mass,σ,ϵ,charge,r0,Kbond,polar,θ0,Kangle,Kdih}that make up the empirical potential (i.e., the General
AMBER force field (Wang et al., 2004) version 2 (GAFF2)) of the classical MD simulation. The detailed
descriptions for each parameter are listed in Table 2. For each t, pre-defined values are assigned to their
14Under review as submission to TMLR
Value
Figure 5: Bar plot of regression coefficients γof linear calibrator filling the discrepancy between experimental
and MD-calculated specific heat capacity of amorphous polymers.
constituent elements in a polymer, such as individual atoms ( mass,charge,σ, andϵ), bonds (r0,Kbond, and
polar), angles (θ0andKangle), or dihedral angles ( Kdih), respectively. The probability density function of the
assigned values of tis then estimated and discretized into 10 points corresponding to 10 different element
species such as hydrogen and carbon for mass, and 20 equally spaced grid points for the other parameters.
Thus, the 190-dimensional descriptor vector xconsists of nine blocks with 10 or 20 elements corresponding to
the different types of force field parameters.
For each block in xdescribing the discretized density function of a force field parameter, the corresponding
parameters in γshould be estimated smoothly. To this end, fused regularization was introduced in the
objective function to be minimized:
λ1∥γ∥2
2+λ2/summationdisplay
t∈TJt/summationdisplay
j=2/parenleftbig
γt,j−γt,j−1/parenrightbig2,
whereJt= 10fort=massandJt= 20otherwise.γt,jdenotes the coefficient for the j-th grid point of the
force field parameter t. The hyperparameters λ1andλ2respectively penalize the increasing norm of γand
the increasing discrepancy between the regression coefficients of adjacent grid points. Both λ1andλ2were
optimized through five-fold cross-validation.
Table 3 summarizes the prediction performance (RMSE) of the three models. The ordinary linear model
y=α0+α1fs+ϵ, which ignores the force-field descriptors, exhibited the lowest prediction performance.
The other two calibration models y=fs+x⊤γ+ϵand the full model in Eq. (8)reached almost the same
accuracy, but the latter had achieved slightly better prediction accuracy. The estimated parameters of the full
model were α1≈0.889andβ≈−0.004. The model form is highly consistent with the theoretical equation in
Eq.(7)as well as the restricted model ( α1= 1,β= 0). This supports the validity of the theoretical model in
Hayashi et al. (2022) that explains the discrepancy between experimental and calculated values owing to the
presence or absence of quantum effects.
It is expected that physicochemical insights can be obtained by examining the estimated coefficient parameter
γin the term of x⊤γ, which would capture the contribution of the force field parameters to the quantum
effects yielding the systematic bias in the MD calculations. Figure 5 shows the mean values of the estimated
parameterγfor the full calibration model. The magnitude of the quantum effect is a monotonically increasing
function of the frequency of the harmonic oscillator ω, and is known to be highly related to the depth of
the potential well in van der Waals interaction ( ϵ) and in bond rotation ( Kdih), the force constants of bond-
stretching ( Kbond) and -bending ( Kangle), and the mass of the atoms ( mass). According to physicochemical
intuition, it is considered that as ϵ,Kbond,Kangle, andKdihdecrease, their potential energy surface becomes
shallow. This leads to the decrease of the frequency ω, resulting in the decrease of quantum effects for CP.
Further, theoretically, because the molecular vibration of light-weight atoms is faster than that of heavy
atoms,ωand quantum effects for CPshould increase with decreasing mass. These physical relationships could
be captured consistently with the estimated coefficients. The coefficients in lower regions of ϵ,Kbond,Kangle
15Under review as submission to TMLR
andKdihshowed large negative values, indicating that polymers containing more atoms, bonds, angles, and
dihedral angles with lower values of these force field parameters will have smaller quantum effects. Conversely,
the coefficients in lower regions of massshowed positive large values, meaning that polymers containing
more atoms with smaller masses will have larger quantum effects. Separately including the domain-common
and domain-specific factors in the transfer model, we could infer the features relevant to the cross-domain
differences.
6 Conclusions
In this study, we defined a general class of TL based on affine model transformations and clarified their
learning capability and applicability. We considered a procedure consisting of two stages: first, the source
features and target samples are given and transformed, and then the domain transfer model is estimated
using the transformed data. The affine model transformation was shown to minimize the expected squared
loss in the class of two-stage transfer learning. The affine transfer model is structurally common to a low-rank
tensor regression model and an invertible neural network model with affine-coupling layers. In the context
of TL, the model can be used to represent and estimate the cross-domain shift and domain-specific factors
simultaneously and separately.
Currently, the most widely applied methods of TL reuse features acquired by pre-trained neural networks in
the source domain. Such procedural approaches, including feature extractors and fine-tuning, are intuitively
plausible but lack a theoretical foundation. In addition, existing methods are designed to describe the target
domain using only features acquired in the source domain, and thus cannot adequately deal with cases
where domain-specific factors are present. Our affine model transfer is a principled methodology based on
the minimum expected square loss. It also has the ability to handle domain common and unique factors
simultaneously and separately.
The present methodology provides a general framework that can handle any model including neural networks
and any pre-defined features. As described, using intermediate layers of a pre-trained neural network as
the source features in the affine transfer model, we can represent ordinary TL based on feature extraction.
Furthermore, as shown in the case studies, the affine transfer model can be used as a calibrator between
computational models and real-world systems by defining predicted values from physics simulators as the
source features. By designing the source features and the three coupling functions that make up the optimal
form of transfer models, we expect to be able to formulate various kinds of TL.
References
Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. Annals of
Statistics , 33:1497–1537, 2005.
Gabriela Csurka. Domain adaptation for visual applications: A comprehensive survey. ArXiv, abs/1702.05374,
2017.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. ArXiv, abs/1810.04805, 2019.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation.
ArXiv, abs/1410.8516, 2014.
Laurent Dinh, Jascha Narain Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. Interna-
tional Conference on Learning Representations , 2017.
Simon S Du, Jayanth Koushik, Aarti Singh, and Barnabás Póczos. Hypothesis transfer learning via
transformation functions. Advances in Neural Information Processing Systems , 30, 2017.
Richard A. Harshman. Foundations of the parafac procedure: Models and conditions for an "explanatory"
multi-model factor analysis. UCLA Working Papers in Phonetics , 16:1–84, 1970.
16Under review as submission to TMLR
Yoshihiro Hayashi, Junichiro Shiomi, Junko Morikawa, and Ryo Yoshida. Radonpy: Automated physical
property calculation using all-atom classical molecular dynamics simulations for polymer informatics. ArXiv,
abs/2203.14090, 2022.
Sheng Ju, Ryo Yoshida, Chang Liu, Kenta Hongo, Terumasa Tadano, and Junichiro Shiomi. Exploring
diamond-like lattice thermal conductivity crystals via feature-based transfer learning. Physical Review
Materials , 5(5):053801, 2021.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,
2015.
Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances
in Neural Information Processing Systems , 31, 2018.
Marius Kloft and Gilles Blanchard. The local rademacher complexity of lp-norm multiple kernel learning.
Advances in Neural Information Processing Systems , 2011.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional
neural networks. Communications of the ACM , 60:84 – 90, 2012.
Ilja Kuzborskij and Francesco Orabona. Stability and hypothesis transfer learning. In International Conference
on Machine Learning , pp. 942–950. PMLR, 2013.
Ilja Kuzborskij and Francesco Orabona. Fast rates by transferring from auxiliary hypotheses. Machine
Learning , 106(2):171–195, 2017.
Chang Liu, Erina Fujita, Yukari Katsura, Yuki Inada, Asuka Ishikawa, Ryuji Tamura, Kaoru Kimura, and
Ryo Yoshida. Machine learning to predict quasicrystals from chemical compositions. Advanced Materials ,
33(36):2102507, 2021.
Song Liu and Kenji Fukumizu. Estimating posterior ratio for classification: Transfer learning from probabilistic
perspective. In Proceedings of the 2016 SIAM International Conference on Data Mining , pp. 747–755.
SIAM, 2016.
Shunya Minami, Song Liu, Stephen Wu, Kenji Fukumizu, and Ryo Yoshida. A general class of transfer
learning regression without implementation cost. Proceedings of AAAI Conference on Artificial Intelligence ,
35:8992–8999, 2021.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet S. Talwalkar. Foundations of Machine Learning . MIT
press, 2018.
Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing , 33(5):2295–2317,
2011.
Shingo Otsuka, Isao Kuwajima, Junko Hosoya, Yibin Xu, and Masayoshi Yamazaki. Polyinfo: Polymer
database for polymeric materials design. In 2011 International Conference on Emerging Intelligent Data
and Web Technologies , pp. 22–29. IEEE, 2011.
George Papamakarios, Eric T. Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshmi-
narayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning
Research , 22:57:1–57:64, 2021.
Sebastian Ruder, Matthew E. Peters, Swabha Swayamdipta, and Thomas Wolf. Transfer learning in natural
language processing. In Proceedings of the 2019 conference of the North American chapter of the association
for computational linguistics: Tutorials , 2019.
Rahul Kumar Sevakula, Vikas Singh, Nishchal Kumar Verma, Chandan Kumar, and Yan Cui. Transfer
learning for molecular cancer classification using deep neural networks. IEEE/ACM Transactions on
Computational Biology and Bioinformatics , 16:2089–2100, 2019.
17Under review as submission to TMLR
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. Advances in
Neural Information Processing Systems , 23, 2010.
Ingo Steinwart and Andreas Christmann. Support vector machines. In Information science and statistics ,
2008.
Ingo Steinwart, Don R. Hush, and Clint Scovel. Optimal rates for regularized least squares regression. In
Proceedings of the 22nd Annual Conference on Learning Theory , pp. 79—-93, 2009.
Takeshi Teshima, Isao Ishikawa, Koichi Tojo, Kenta Oono, Masahiro Ikeda, and Masashi Sugiyama. Coupling-
based invertible neural networks are universal diffeomorphism approximators. Advances in Neural Informa-
tion Processing Systems , 33:3362–3373, 2020.
Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance of task
diversity. Advances in Neural Information Processing Systems , 33:7852–7862, 2020.
Ledyard R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika , 31:279–311,
1966.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science , volume 47.
Cambridge university press, 2018.
Junmei Wang, Romain M. Wolf, James W. Caldwell, Peter A. Kollman, and David A. Case. Development
and testing of a general amber force field. Journal of Computational Chemistry , 25, 2004.
Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning , volume 2.
MIT press Cambridge, MA, 2006.
Stephen Wu, Yukiko Kondo, Masa-aki Kakimoto, Bin Yang, Hironao Yamada, Isao Kuwajima, Guillaume
Lambard, Kenta Hongo, Yibin Xu, Junichiro Shiomi, et al. Machine-learning-assisted discovery of polymers
with high thermal conductivity using a molecular design algorithm. npj Computational Materials , 5(1):
1–11, 2019.
Hironao Yamada, Chang Liu, Stephen Wu, Yukinori Koyama, Sheng Ju, Junichiro Shiomi, Junko Morikawa,
and Ryo Yoshida. Predicting materials properties with little data using shotgun transfer learning. ACS
Central Science , 5:1717–1730, 2019.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural
networks? Advances in Neural Information Processing Systems , 27, 2014.
Hua Zhou, Lexin Li, and Hongtu Zhu. Tensor regression with applications in neuroimaging data analysis.
Journal of the American Statistical Association , 108(502):540–552, 2013.
18Under review as submission to TMLR
A Other perspectives on affine model transfer
A.1 Transformation functions for general loss functions
Here we discuss the optimal transformation function for general loss functions.
Letℓ(y,y′)≥0be a convex loss function that returns zero if and only if y=y′, and letg∗(x)be the
optimal predictor that minimizes the expectation of ℓwith respect to the distribution ptfollowed by xandy
transformed by ϕ:
g∗(x) = arg min
gEpt/bracketleftbig
ℓ(g(x),ϕfs(y))/bracketrightbig
.
The function gthat minimizes the expected loss
Ept/bracketleftbig
ℓ(g(x),ϕfs(y))/bracketrightbig
=/integraldisplay/integraldisplay
ℓ(g(x),ϕfs(y))pt(x,y)dxdy
should be a solution to the Euler-Lagrange equation
∂
∂g(x)/integraldisplay
ℓ(g(x),ϕfs(y))pt(x,y)dy=/integraldisplay∂
∂g(x)ℓ(g(x),ϕfs(y))pt(y|x)dypt(x) = 0. (9)
Denote the solution of Eq. (9)byG(x;ϕfs). WhileGdepends on the loss ℓand distribution pt, we omit
those from the argument for notational simplicity. Using this function, the minimizer of the expected loss
Ex,y[ℓ(g(x),y)]can be expressed as G(x; id), where idrepresents the identity function.
Here, we consider the following assumption to hold, which generalizes Assumption 3:
Assumption3(b). For any distribution on the target domain pt(x,y)and allx∈X, the following relationship
holds:
ψfs(g∗(x)) = arg min
gEx,y[ℓ(g(x),y)].
This implies that the transformation functions ϕfsandψfssatisfy
ψfs/parenleftbig
G(x;ϕfs)/parenrightbig
=G(x; id). (10)
Assumption 3(b) states that if the best predictor G(x;ϕfs)for the data transformed by ϕis given to the
model transformation function ψ, it is consistent with the overall best predictor G(x;id)in the target region
in terms of the loss function ℓ. We consider all pairs of ψandϕthat satisfy this consistency condition.
Here, let us consider the following proposition:
Proposition 5. Under Assumption 1-2 and 3(b), it holds that ψ−1
fs=ϕfs.
Proof.The proof is analogous to that of Theorem 1. For any y0∈Y, letpt(y|x) =δy0. Combining this with
Eq. (9) leads to
∂
∂g(x)ℓ(g(x),ϕfs(y0)) = 0 (∀y0∈Y).
Becauseℓ(y,y′)returns the minimum value zero if and only if y=y′, we obtain G(x;ϕfs) =ϕfs(y0). Similarly,
we haveG(x;id) =y0. From these two facts and Assumption 3(b), we have ψfs(ϕfs(y0)) =y0, proving that
the proposition is true.
Proposition 5 indicates that the first statement of Theorem 1 holds for general loss functions. However, the
second claim of Theorem 1 generally depends on the type of loss function. Through the following examples,
we describe the optimal class of transformation functions for several loss functions.
Example 1 (Squared loss) .Letℓ(y,y′) =|y−y′|2. As a solution of Eq. (9), we can see that the optimal
predictor is the conditional expectation Ept[ϕfs(Y)|X=x]. As discussed in Section 2, the transformation
functionsϕfsandψfsshould be affine transformations.
19Under review as submission to TMLR
Example 2 (Absolute loss) .Letℓ(y,y′) =|y−y′|. Substituting this into Eq. (9), we have
0 =/integraldisplay∂
∂g(x)/vextendsingle/vextendsingleg(x)−ϕfs(y)/vextendsingle/vextendsinglept(y|x)dy
=/integraldisplay
sign/parenleftbig
g(x)−ϕfs(y)/parenrightbig
pt(y|x)dy
=/integraldisplay
ϕfs(y)≥g(x)pt(y|x)dy−/integraldisplay
ϕfs(y)<g(x)pt(y|x)dy.
Assuming that ϕfsis monotonically increasing, we have
0 =/integraldisplay
y≥ϕ−1
fs(g(x))pt(y|x)dy−/integraldisplay
y<ϕ−1
fs(g(x))pt(y|x)dy.
This yields,
/integraldisplay∞
ϕ−1
fs(g(x))pt(y|x)dy=/integraldisplayϕ−1
fs(g(x))
−∞pt(y|x)dy.
The same result is obtained even if ϕfsis monotonically decreasing. Consequently,
ϕ−1
fs(g(x)) = Median[ Y|X=x],
which results in
G(x;ϕfs) =ϕfs/parenleftbig
Median[Y|X=x]/parenrightbig
.
This implies that Eq. (10) holds for any ϕfsincluding an affine transformation.
Example 3 (Exponential-squared loss) .As an example where the affine transformation is not optimal,
consider the loss function ℓ(y,y′) =|ey−ey′|2. Substituting this into Eq. (9), we have
0 =/integraldisplay∂
∂g(x)/vextendsingle/vextendsingleexp(g(x))−exp(ϕfs(y))/vextendsingle/vextendsingle2pt(y|x)dy
= 2 exp(g(x))/integraldisplay/parenleftbig
exp(g(x))−exp(ϕfs(y))/parenrightbig
pt(y|x)dy.
Therefore,
G(x;ϕfs) = log E/bracketleftbig
exp(ϕfs(Y))|X=x/bracketrightbig
.
Consequently, Eq. (10) becomes
logE/bracketleftbig
exp(ϕfs(Y))/bracketrightbig
=ϕfs/parenleftbig
logE/bracketleftbig
exp(Y)/bracketrightbig/parenrightbig
.
Ifϕfsis an affine transformation, this equation does not generally hold.
A.2 Analysis of the optimal function class based on the upper bound of the estimation error
Here, we discuss the optimal class for the transformation function based on the upper bound of the estimation
error.
In addition to Assumptions 1 and 2, we assume the following in place of Assumption 3:
Assumption 6. The transformation functions ϕandψare Lipschitz continuous with respect to the first
argument, i.e., there exist constants µϕandµψsuch that,
ϕ(a,c)−ϕ(a′,c)≤µϕ∥a−a′∥2, ψ (b,c)−ψ(b′,c)≤µψ∥b−b′∥2,
for anya,a′∈Yandb,b′∈Rwith any given c∈Fs.
20Under review as submission to TMLR
Note that each Lipschitz constant is a function of the second argument fs, i.e.,µϕ=µϕ(fs)andµψ=µψ(fs).
Under Assumptions 1-2 and 6, the estimation error is upper bounded as follows:
E
x,y/bracketleftig
|ft(x)−ˆft(x)|2/bracketrightig
=E
x,y/bracketleftig/vextendsingle/vextendsingleψ(g(x),fs(x))−ψ(ˆg(x),fs(x))/vextendsingle/vextendsingle2/bracketrightig
≤E
x,y/bracketleftig
µψ(fs(x))2/vextendsingle/vextendsingleg(x)−ˆg(x)/vextendsingle/vextendsingle2/bracketrightig
≤3E
x,y/bracketleftig
µψ(fs(x))2/parenleftbig/vextendsingle/vextendsingleg(x)−ϕ(ft(x),fs(x))/vextendsingle/vextendsingle2
+/vextendsingle/vextendsingleϕ(ft(x),fs(x))−ϕ(y,fs(x))/vextendsingle/vextendsingle2
+/vextendsingle/vextendsingleϕ(y,fs(x))−ˆg(x)/vextendsingle/vextendsingle2/parenrightbig/bracketrightig
≤3E
x,y/bracketleftig
µψ(fs(x))2/vextendsingle/vextendsingleψ−1(ft(x),fs(x))−ϕ(ft(x),fs(x))/vextendsingle/vextendsingle2/bracketrightig
+ 3E
x,y/bracketleftig
µψ(fs(x))2µϕ(fs(x))2/vextendsingle/vextendsingleft(x)−y/vextendsingle/vextendsingle2/bracketrightig
+ 3E
x,y/bracketleftig
µψ(fs(x))2/vextendsingle/vextendsinglez−ˆg(x)/vextendsingle/vextendsingle2/bracketrightig
= 3E
x,y/bracketleftig
µψ(fs(x))2/vextendsingle/vextendsingleψ−1(ft(x),fs(x))−ϕ(ft(x),fs(x))/vextendsingle/vextendsingle2/bracketrightig
+ 3σ2E
x,y/bracketleftig
µψ(fs(x))2µϕ(fs(x))2/bracketrightig
+ 3E
x,y/bracketleftig
µψ(fs(x))2/vextendsingle/vextendsinglez−ˆg(x)/vextendsingle/vextendsingle2/bracketrightig
.
The derivation of this inequality is based on Du et al. (2017). We use the Lipschitz property of ψandϕ
for the first and third inequalities, and the second inequality comes from the numeric inequality (a−d)2≤
3(a−b)2+ 3(b−c)2+ 3(c−d)2fora,b,c,d∈R.
According to this inequality, the upper bound of the estimation error is decomposed into three terms: the
discrepancy between the two transformation functions, the variance of the noise, and the estimation error for
the transformed data. Although it is intractable to find the optimal solution of ϕ,ψ,ˆgthat minimizes all
these terms together, it is possible to find a solution that minimizes the first and second terms expressed as
the functions of ϕandψonly. Obviously, the first term, which represents the discrepancy between the two
transformation functions, reaches its minimum (zero) when ϕfs=ψ−1
fs. The second term, which is related to
the variance of the noise, is minimized when the differential coefficient∂
∂uψfs(u)is a constant, i.e., when ψfs
is a linear function. This is verified as follows. From ϕfs=ψ−1
fsand the continuity of ψfs, it follows that
µψ= max/vextendsingle/vextendsingle/vextendsingle∂
∂uψfs(u)/vextendsingle/vextendsingle/vextendsingle, µϕ= max/vextendsingle/vextendsingle/vextendsingle∂
∂uψ−1
fs(u)/vextendsingle/vextendsingle/vextendsingle=1
min|∂
∂uψfs(u)|,
and thus the product µϕµψtakes the minimum value (one) when the maximum and minimum of the differential
coefficient are the same. Therefore, we can write
ϕ(y,fs(x)) =y−g1(fs(x))
g2(fs(x)), ψ (g(x),fs(x)) =g1(fs(x)) +g2(fs(x))g(x),
whereg1,g2:Fs→Rare arbitrarily functions. Thus, the minimization of the third term in the upper bound
of the estimation error can be expressed as
min
g1,g2,gE
x,y|y−g1(fs(x)) +g2(fs(x))g(x)|2.
As a result, the suboptimal function class for the upper bound of the estimated function is given as
H=/braceleftbig
x∝⇕⊣√∫⊔≀→g1(fs(x)) +g2(fs(x))·g3(x)|g1∈G1,g2∈G2,g3∈G3/bracerightbig
.
This is the same function class derived in Section 2.
21Under review as submission to TMLR
B Proofs
B.1 Proof of Theorem 2
To bound the generalization error, we use the empirical and population Rademacher complexity ˆRS(F)and
R(F)of hypothesis class F, defined as:
ˆRS(F) =Eσsup
f∈F1
nn/summationdisplay
i=1σif(xi), R(F) =ESˆRS(F),
where{σi}n
i=1is a set of Rademacher variables that are independently distributed and each take one of the
values in{−1,+1}with equal probability, and Sdenotes a set of samples. The following proof is based on
the one of Theorem 11 shown in Kuzborskij & Orabona (2017).
Proof of Theorem 2. For any hypothesis class Fwith feature map Φwhere∥Φ∥2≤1, the following inequality
holds:
Eσsup
∥θ∥2≤Λ1
nn/summationdisplay
i=1σi⟨θ,Φ(xi)⟩≤/radicalbigg
Λ
n.
The proof is given, for example, in Theorem 6.12 of Mohri et al. (2018). Thus, the empirical Rademacher
complexity ofHis bounded as
ˆRS(H) =Eσ sup
∥α∥2
H1≤λ−1
αˆRs,∥β∥2
H2≤λ−1
βˆRs,∥γ∥2
H3≤λ−1
γˆRs1
nn/summationdisplay
i=1σi/braceleftbigg
⟨α,Φ1(fs(xi))⟩H1+⟨β,Φ2(fs(xi))⟩H2⟨γ,Φ(xi)⟩H3/bracerightbigg
≤Eσ sup
∥α∥2
H1≤λ−1
αˆRs1
nn/summationdisplay
i=1σi⟨α,Φ1(fs(xi))⟩H1
+ sup
∥β∥2
H2≤λ−1
βˆRs,∥γ∥2
H3≤λ−1
γˆRs1
nn/summationdisplay
i=1σi⟨β⊗γ,Φ2(fs(xi))⊗Φ(xi)⟩H2⊗H3
≤Eσ sup
∥α∥2
H1≤λ−1
αˆRs1
nn/summationdisplay
i=1σi⟨α,Φ1(fs(xi))⟩H1
+ sup
∥β⊗γ∥2
H2⊗H3≤λ−1
βλ−1
γˆR2s1
nn/summationdisplay
i=1σi⟨β⊗γ,Φ2(fs(xi))⊗Φ(xi)⟩H2⊗H3
≤/radicaligg
ˆRs
λαn+/radicaligg
ˆR2s
λβλγn(11)
≤/radicaligg
ˆRs
n/braceleftbigg/radicalbigg
1
λα+/radicaligg
L
λβλγ/bracerightbigg
.
The first inequality follows from the subadditivity of supremum. The last inequality follows from the fact
that ˆRs≤Pnℓ(y,⟨0,Φ1⟩) +λα∥0∥2≤L.
LetC=/radicalig
1
λα+/radicalig
L
λβλγ,and applying Talagrand’s lemma (Mohri et al., 2018) and Jensen’s inequality, we
obtain
R(L) =EˆRS(L)≤µℓEˆRS(H)≤CµℓE/radicaligg
ˆRs
n≤Cµℓ/radicaligg
EˆRs
n.
To apply Corollary 3.5 of Bartlett et al. (2005), we should solve the equation
r=Cµℓ/radicalbiggr
n, (12)
22Under review as submission to TMLR
and obtain r∗=µ2
ℓC2
n.Thus, for any η>0, with probability at least 1−e−η, there exists a constant C′>0
that satisfies
Pnℓ(y, h)≤C′/parenleftbigg
EˆRs+µ2
ℓC2
n+η
n/parenrightbigg
≤C′/parenleftbigg
Rs+µ2
ℓC2
n+η
n/parenrightbigg
. (13)
Note that, for the last inequality, because ˆRs≤Pnℓ(y,⟨α,Φ1⟩) +λα∥α∥2for anyα, taking the expectation of
both sides yields EˆRs≤Pℓ(y,⟨α,Φ1⟩) +λα∥α∥2, and this gives EˆRs≤infα{Pℓ(y,⟨α,Φ1⟩) +λα∥α∥2}=Rs.
Consequently, applying Theorem 1 of Srebro et al. (2010), we have
Pℓ(y, h(x))≤Pnℓ(y, h(x)) + ˜O/parenleftbigg/parenleftbigg/radicalbigg
Rs
n+µℓC+√η
n/parenrightbigg/parenleftbigg√
LC+/radicalbig
Lη/parenrightbigg
+C2L+Lη
n/parenrightbigg
.(14)
Here, we use ˆRS(H)≤C/radicalig
ˆRs
n≤C/radicalig
L
n.
Remark 1.As in Kuzborskij & Orabona (2017), without the estimation of the parameters αandβ, the
right-hand side of Eq. (11)becomes1√n/parenleftig
c1+c2/radicalbigˆRs/parenrightig
with some constant c1>0andc2>0, and Eq. (12)
becomes
r=1√n(c1+c2√r).
This yields the solution
r∗=/parenleftigg
c2
2√n+/radicaligg/parenleftbiggc2
2√n/parenrightbigg2
+c1√n/parenrightigg2
≤c2
2
n+c1√n,
where we use the inequality√x+√x+y≤√4x+ 2y. Thus, Eq. (13) becomes
Pnℓ(y, h)≤C′/parenleftbigg
Rs+c2
2
n+c1√n+η
n/parenrightbigg
.
Consequently, we have the following result:
Pℓ(y, h(x))≤Pnℓ(y, h(x)) + ˜O/parenleftbigg/parenleftbigg/radicalbigg
Rs
n+√c1
n3/4+c2+√η
n/parenrightbigg/parenleftbigg
c1+c2√
L+/radicalbig
Lη/parenrightbigg
+(c1+c2√
L)2+Lη
n/parenrightbigg
.
This means that even if Rs=˜O(n−1), the resulting rate only improves to ˜O(n−3/4).
B.2 Proof of Theorem 3
Recall that loss function ℓ(·,·)is assumed to be µℓ-Lipschitz for the first argument. In addition, we impose
the following assumption.
Assumption 7. There exists a constant B≥1such that for every h∈H,P(h−h∗)≤BP(ℓ(y,h)−ℓ(y,h∗)).
Because we consider ℓ(y,y′) = (y−y′)2in Theorem 3, Assumption 3 holds as stated in Bartlett et al. (2005).
First, we show the following corollary, which is a slight modification of Theorem 5.4 of Bartlett et al. (2005).
Corollary 6. Letˆhbe any element of HsatisfyingPnℓ(y,ˆh) =infh∈HPnℓ(y,h), and let ˆh(m)be any element
ofH(m)satisfyingPnℓ(y,ˆh(m)) = infh∈H(m)Pnℓ(y,h). Define
ˆψ(r) =c1ˆRS{h∈H: max
m∈{1,2}Pn(h(m)−ˆh(m))2≤c3r}+c2η
n,
wherec1,c2andc3are constants depending only on Bandµℓ. Then, for any η>0, with probability at least
1−5e−η,
Pℓ(y,ˆh)−Pℓ(y,h∗)≤705
Bˆr∗+(11µℓ+ 27B)η
n,
where ˆr∗is the fixed point of ˆψ.
23Under review as submission to TMLR
Proof.Define the function ψas
ψ(r) =c1
2R{h∈H:µ2
ℓmaxP(h(m)−h(m)∗)2≤r}+(c2−c1)η
n.
BecauseH,H(1)andH(2)are all convex and thus star-shaped around each of its points, Lemma 3.4 of Bartlett
et al. (2005) implies that ψis a sub-root. Also, define the sub-root function ψmas
ψm(r) =c(m)
1
2R{h(m)∈H(m):µ2
ℓP(h(m)−h(m)∗)2≤r}+(c2−c1)η
n.
Letr∗
mbe the fixed point of ψm(rm). Now, forrm≥ψm(rm), Corollary 5.3 of Bartlett et al. (2005) and the
condition on the loss function imply that, with probability at least 1−e−η,
µ2
ℓP(ˆh(m)−h(m)∗)2≤Bµ2
ℓP(ℓ(y,ˆh(m))−ℓ(y,ˆh(m)∗))≤705µ2
ℓrm+(11µℓ+ 27B)Bµ2
ℓη
n.
Denote the right-hand side by sm, and define r= maxrmands= maxsm. Becauses≥sm≥rm≥r∗
m, we
obtains≥ψm(s)according to Lemma 3.2 of Bartlett et al. (2005), and thus,
s≥10µ2
ℓR{h(m)∈H(m):µ2
ℓP(h(m)−h(m)∗)2≤s}+11µ2
ℓη
n.
Therefore, applying Corollary 2.2 of Bartlett et al. (2005) to the class µℓH(m), it follows that with probability
at least 1−e−η,
{h(m)∈H(m):µ2
ℓP(h(m)−h(m)∗)2≤s}⊆{h(m)∈H(m):µ2
ℓPn(h(m)−h(m)∗)2≤2s}.
This implies that with probability at least 1−2e−η,
Pn(ˆh(m)−h(m)∗)2≤2/parenleftbigg
705r+(11µℓ+ 27B)Bη
n/parenrightbigg
≤2/parenleftbigg
705 +(11µℓ+ 27B)B
n/parenrightbigg
r,
where the second inequality follows from r≥ψ(r)≥c2η
n. Define 2/parenleftig
705 +(11µℓ+27B)B
n/parenrightig
=c′. According to
the triangle inequality in L2(Pn), it holds that
Pn(h(m)−ˆh(m))2≤/parenleftig/radicalig
Pn(h(m)−h(m)∗)2+/radicalig
Pn(h(m)∗−ˆh(m))2/parenrightig2
≤/parenleftig/radicalig
Pn(h(m)−h(m)∗)2+√
c′r/parenrightig2
.
Again, applying Corollary 2.2 of Bartlett et al. (2005) to µℓH(m)as before, but now for r≥ψm(r), it follows
that with probability at least 1−4e−η,
{h∈H:µ2
ℓmaxP(h(m)−h(m)∗)2≤r}=2/intersectiondisplay
m=1{h(m)∈H(m):µ2
ℓP(h(m)−h(m)∗)2≤r}
⊆2/intersectiondisplay
m=1{h(m)∈H(m):µ2
ℓPn(h(m)−h(m)∗)2≤2r}
⊆2/intersectiondisplay
m=1{h(m)∈H(m):µ2
ℓPn(h(m)−ˆh(m))2≤(√
2r+√
c′r)2}
={h∈H:µ2
ℓmaxPn(h(m)−ˆh(m))2≤c3r},
24Under review as submission to TMLR
wherec3= (√
2+√
c′)2. Combining this with Lemma A.4 of Bartlett et al. (2005) leads to the following
inequality: with probability at least 1−5e−x
ψ(r) =c1
2R{h∈H:µ2
ℓmaxP(h(m)−h(m)∗)2≤r}+(c2−c1)η
n
≤c1ˆRS{h∈H:µ2
ℓmaxP(h(m)−h(m)∗)2≤r}+c2η
n
≤c1ˆRS{h∈H:µ2
ℓmaxPn(h(m)−ˆh(m))2≤c3r}+c2η
n
=ˆψ(r).
Lettingr=r∗and using Lemma 4.3 of Bartlett et al. (2005), we obtain r∗≤ˆr∗, thus proving the
statement.
Proof of Theorem 3. DefineR= maxmsuph∈H(m)Pn(y−h(x))2.For anyh∈H(m), we obtain
Pn(h(m)(x)−ˆh(m)(x))2≤2Pn(y−h(m)(x))2+ 2Pn(y−ˆh(m)(x))2≤4 sup
h∈H(m)Pn(y−h(x))2≤4R.
From the symmetry of the σiand the fact that H(m)is convex and symmetric, we obtain the following:
ˆRS{h∈H: maxPn(h(m)−ˆh(m))2≤4R}=Eσ sup
h(m)∈H(m)
Pn(h(m)−ˆh(m))2≤4R1
nn/summationdisplay
i=1σi2/summationdisplay
m=1h(m)(xi)
=Eσ sup
h(m)∈H(m)
Pn(h(m)−ˆh(m))2≤4R1
nn/summationdisplay
i=1σi2/summationdisplay
m=1(h(m)(xi)−ˆh(m)(xi))
≤Eσ sup
h(m),g(m)∈H(m)
Pn(h(m)−g(m))2≤4R1
nn/summationdisplay
i=1σi2/summationdisplay
m=1(h(m)(xi)−g(m)(xi))
= 2Eσsup
h(m)∈H(m)
Pnh(m)2≤R1
nn/summationdisplay
i=1σi2/summationdisplay
m=1h(m)(xi)
≤22/summationdisplay
m=1Eσsup
h(m)∈H(m)
Pnh(m)2≤R1
nn/summationdisplay
i=1σih(m)(xi)
≤22/summationdisplay
m=1

2
nn/summationdisplay
j=1min{R,ˆλ(m)
j}

1
2
≤

16
n2/summationdisplay
m=1n/summationdisplay
j=1min{R,ˆλ(m)
j}

1
2
.
The second inequality comes from the subadditivity of supremum and the third inequality follows from
Theorem 6.6 of Bartlett et al. (2005). To obtain the last inequality, we use√x+√y≤/radicalbig
2(x+y). Thus, we
have
2c1ˆRS{h∈H: maxPn(h(m)−ˆh(m))2≤4R}+(c2+ 2)η
n
≤4c1

16
n2/summationdisplay
m=1n/summationdisplay
j=1min/braceleftbig
R,ˆλ(m)
j/bracerightbig

1
2
+(c2+ 2)η
n,
25Under review as submission to TMLR
for some constants c1andc2. To apply Corollary 6, we should solve the following inequality for r
r≤4c1

16
n2/summationdisplay
m=1n/summationdisplay
j=1min/braceleftbig
r,ˆλ(m)
j/bracerightbigg

1
2
.
For any integers κm∈[0,n], the right-hand side is bounded as
4c1

16
n2/summationdisplay
m=1n/summationdisplay
j=1min/braceleftbig
r,ˆλ(m)
j/bracerightbigg

1
2
≤4c1

16
n2/summationdisplay
m=1
κm/summationdisplay
j=1r+n/summationdisplay
j=κm+1ˆλ(m)
j


1
2
=

/parenleftigg
256c2
1
n2/summationdisplay
m=1κm/parenrightigg
r+256c2
1
n2/summationdisplay
m=1n/summationdisplay
j=κm+1ˆλ(m)
j

1
2
,
and we obtain the solution r∗as
r∗≤128c2
1
n2/summationdisplay
m=1κm+
/braceleftigg
128c2
1
n2/summationdisplay
m=1κm/bracerightigg2
+256c2
1
n2/summationdisplay
m=1n/summationdisplay
j=κm+1ˆλ(m)
j
1
2
≤256c2
1
n2/summationdisplay
m=1κm+
256c2
1
n2/summationdisplay
m=1n/summationdisplay
j=κm+1ˆλ(m)
j
1
2
.
Optimizing the right-hand side with respect to κ1andκ2, we obtain the solution as
r∗≤ min
0≤κ1,κ2≤n

256c2
1
n2/summationdisplay
m=1κm+
256c2
1
n2/summationdisplay
m=1n/summationdisplay
j=κm+1ˆλ(m)
j
1
2

.
Furthermore, according to Corollary 6, there exists a constant csuch that with probability at least 1−5e−η,
P(y−ˆh(x))2−P(y−h∗(x))2≤c
 min
0≤κ1,κ2≤n

1
n2/summationdisplay
m=1κm+
1
n2/summationdisplay
m=1n/summationdisplay
j=κm+1ˆλ(m)
j
1
2

+η
n
.
Proof of Theorem 4. Using the inequality√x+y≤√x+√yforx≥0,y≥0, we have
P(y−ˆh(x))2−P(y−h∗(x))2=O/parenleftigg
min
0≤κ1,κ2≤n/braceleftigg
κ1+κ2
n+
1
nn/summationdisplay
j=κ1+1ˆλ(1)
j+1
nn/summationdisplay
j=κ2+1ˆλ(2)
j
1
2/bracerightigg
+η
n/parenrightigg
≤O
 min
0≤κ1,κ2≤n

κ1+κ2
n+
1
nn/summationdisplay
j=κ1+1j−1
s1+1
nn/summationdisplay
j=κ2+1j−1
s2
1
2

+η
n

≤O
 min
0≤κ1,κ2≤n

κ1+κ2
n+
1
nn/summationdisplay
j=κ1+1j−1
s1
1
2
+
1
nn/summationdisplay
j=κ2+1j−1
s2
1
2

+η
n
.
26Under review as submission to TMLR
Because it holds that
n/summationdisplay
j=κm+1j−1
sm</integraldisplay∞
κmx−1
smdx</bracketleftigg
1
1−1
smx1−1
sm/bracketrightigg∞
κm=sm
1−smκ1−1
smm,
form= 1,2, we should solve the following minimization problem:
min
0≤κ1,κ2≤n/braceleftigg
κ1+κ2
n+/parenleftbigg1
ns1
1−s1κ1−1
s1
1/parenrightbigg1
2
+/parenleftbigg1
ns1
1−s1κ1−1
s1
2/parenrightbigg1
2/bracerightigg
≡g(κ).
Taking the derivative, we have
∂g(κ)
∂κ1=1
n+1
2/parenleftbigg1
ns1
1−s1κ1−1
s1
1/parenrightbigg−1
2/parenleftbigg
−κ−1
s1
1
n/parenrightbigg
.
Setting this to zero, we find the optimal κ1as
κ1=/parenleftbiggs1
1−s14
n/parenrightbiggs1
1+s1
.
Similarly, we have
κ2=/parenleftbiggs2
1−s24
n/parenrightbiggs2
1+s2
,
and
P(y−ˆh(x))2−P(y−h∗(x))2
≤O/parenleftigg
1
n/parenleftbiggs1
1−s14
n/parenrightbiggs1
1+s1
+1
n/parenleftbiggs2
1−s24
n/parenrightbiggs2
1+s2
+ 21−s1
1+s1/parenleftbiggs1
1−s11
n/parenrightbigg 1
1+s1
+ 21−s2
1+s2/parenleftbiggs2
1−s21
n/parenrightbigg 1
1+s2
+η
n/parenrightigg
=O/parenleftig
n−1
1+s1+n−1
1+s2/parenrightig
=O/parenleftig
n−1
1+max{s1,s2}/parenrightig
.
C Eigenvalue decay of the Hadamard product of two Gram matrices
We experimentally investigated how the decay rate s2in Corollary 4 is related to the overlap degree in the
spaces spanned by the original input xand source features fs.
For the original input x∈R100, we randomly constructed a set of 10 orthonormal bases, and then generated
100 samples from their spanning space. For the source features fs∈R100, we selected dbases randomly from
the 10 orthonormal bases selected for xand the remaining 10−dbases from their orthogonal complement
space. We then generated 100 samples of fsfrom the space spanned by these 10 bases. The overlap number
dcan be regarded as the degree of overlap of two spaces spanned by the samples of xandfs. We generated
the 100 different sample sets of xandfs.
We calculated the Hadamard product of the Gram matrices K2andK3using the samples of xandfs,
respectively. For the computation of K2andK3, all combinations of the following five kernels were tested:
Linear kernel k(x,x′) =x⊤x
2γ2+ 1,
27Under review as submission to TMLR
Gaussian Matérn -1/2 Matérn -3/2 Matérn -5/2 LinearKernel function for 𝑘𝑘2Kernel function for 𝑘𝑘3
Gaussian Matérn -1/2 Matérn -3/2 Matérn -5/2 Linear
Figure 6: Decay rates of eigenvalues of K2(blue lines), K3(green lines) and K2◦K3(red lines) for all
combinations of the five different kernels. The vertical axis represents the decay rate, and the horizontal axis
represents the overlap dimension din the space where xandfsare distributed.
Matérn kernel k(x,x′) =21−ν
Γ(ν)/parenleftbigg√
2ν∥x−x′∥2
γ/parenrightbiggν
Kν/parenleftbigg√
2ν∥x−x′∥2
γ/parenrightbigg
forν=1
2,3
2,5
2,∞,
whereKν(·)is a modified Bessel function and Γ(·)is the gamma function. Note that for ν=∞, the
Matérn kernel is equivalent to the Gaussian RBF kernel. The scale parameter γof both kernels was set to
γ=/radicalbig
dim(x)=√
10. For a given matrix K, the decay rate of the eigenvalues was estimated as the smallest
value ofsthat satisfies λi≤∥K∥2
F·i−1
swhere∥·∥Fdenotes the Frobenius norm. Note that this inequality
holds for any matrices Kwiths= 1(Vershynin, 2018).
Figure 6 shows the change of the decay rates with respect to varying dfor various combinations of the kernels.
In all cases, the decay rate of K2◦K3showed a clear trend of monotonically decreasing as the degree of
overlapdincreases. In other words, the greater the overlap between the spaces spanned by xandfs, the
smaller the decay rate, and the smaller the complexity of the RKHS H2⊗H 3.
D Experimental details
The Python code to reproduce all experiments is available at https://github.com/mshunya/AffineTL .
Instructions for obtaining the datasets used in the experiments are described in the code.
D.1 Kinematics of the robot arm
D.1.1 Dataset
We used the SARCOS dataset in Williams & Rasmussen (2006). The task is to predict the feed-forward
torque required to follow the desired trajectory in the seven joints of the SARCOS anthropomorphic robot
arm. The twenty one features representing the joints’ position, velocity, and acceleration were used as x.
The observed values of six torques other than the torque at the joint in the target domain were given to
28Under review as submission to TMLR
Algorithm 2 Block relaxation algorithm for Affine transfer (full) .
Initialize
a0←(K1+λ1In)−1y,b0∼N(0,In),c0∼N(0,In),d0←0.5
repeat
a←(K1+λ1In)−1(y−(K2b+1)◦(K3c))
b←(Diag(K3c)2K2+λ2In)−1((K3c)◦(y−K1a−K3c))
c←(Diag(K2b+1) +λ3In)−1((K2b+1)◦(y−K1a))
d←⟨y−K1a−(K2b+1)◦(K3c),1⟩/n
untilconvergence
the source features fs. The dataset includes 44,484 training samples and 4,449 test samples. We selected
{5,10,15,20,30,40,50}samples randomly from the training set. The prediction performances of the trained
models were evaluated using the 4,449 test samples. Repeated experiments were conducted 20 times with
different independently sampled datasets.
D.1.2 Model definition and hyperparameter search
No transfer, Only source, Augmented, HTL–offset, HTL–scale For each procedure, we used kernel
ridge regression with the RBF kernel k(x,x′) =exp(−1
2ℓ2∥x−x′∥2
2).The scale parameter ℓwas set to the
square root of the input dimension as ℓ=√
21forNo transfer ,HTL–offset andHTL–scale ,ℓ=√
6
forOnly source andℓ=√
27forWith source . The regularization parameter λwas selected in five-fold
cross-validation in which the grid search was performed over 50 grid points in the interval [10−4,102].
Affine transfer (full), Affine transfer (constrained) We considered the following kernels:
k1(fs(x),fs(x′)) = exp/parenleftig
−1
2ℓ2∥fs(x)−fs(x′)∥2
2/parenrightig
(ℓ=√
6),
k2(fs(x),fs(x′)) = exp/parenleftig
−1
2ℓ2∥fs(x)−fs(x′)∥2
2/parenrightig
(ℓ=√
6),
k3(x,x′) = exp/parenleftig
−1
2ℓ2∥x−x′∥2
2/parenrightig
(ℓ=√
27),
forg1,g2andg3in the affine transfer model, respectively.
Hyperparameters to be optimized are the three regularization parameters λ1,λ2andλ3. We performed five-
fold cross-validation to identify the best hyperparameter set from the candidate points; [10−3,10−2,10−1,1]
forλ1and[10−2,10−1,1,10]for each of λ2andλ3.
To learn the Affine transfer (full) andAffine transfer (constrained) , we used the following objective
functions:
Affine transfer (full) ∥y−(K1a+ (K2b+1)◦(K3c) +d)∥2
2+λ1a⊤K1a+λ2b⊤K2b+λ3c⊤K3c,
Affine transfer (constrained)1
n∥y−(K1a+K3c+d)∥2
2+λ1a⊤K1a+λ3c⊤K3c.
Algorithm 2 summarizes the block relaxation algorithm for Affine transfer (full) . ForAffine transfer
(constrained) , we found the optimal parameters as follows:
/bracketleftiggˆa
ˆc
ˆd/bracketrightigg
=/parenleftigg/bracketleftiggK1
K3
1⊤/bracketrightigg
[K1K31] +/bracketleftiggλ1K1
λ3K3
0/bracketrightigg/parenrightigg−1/bracketleftiggK1
K3
1⊤/bracketrightigg
y
29Under review as submission to TMLR
The stopping criterion of the algorithm was set as
max
θ∈{a,b,c}maxi/vextendsingle/vextendsingleθ(new)
i−θ(old)
i/vextendsingle/vextendsingle
maxi/vextendsingle/vextendsingleθ(old)
i/vextendsingle/vextendsingle<10−4, (15)
whereθidenotes the i-th element of the parameter θ. This convergence criterion is employed in several
existing machine learning libraries, e.g., scikit-learn2.
D.2 Lattice thermal conductivity of inorganic crystals
D.2.1 Datasets
Two datasets were used, consisting of the lattice thermal conductivity (LTC) and scattering phase space (SPS)
of 45 and 320 inorganic crystals, respectively, that were obtained by performing first principle calculations
(Ju et al., 2021). To obtain the input descriptor x, XenonPy3was used to calculate 290 compositional and
physicochemical features of a given material (Liu et al., 2021).
D.2.2 Model definition and hyperparameter search
Fully connected neural networks were used for both the source and target models, with a LeakyReLu activation
function with α= 0.01. The model training was conducted using the Adam optimizer (Kingma & Ba, 2015).
Hyperparameters such as the width of the hidden layer, learning rate, number of epochs, and regularization
parameters were adjusted with five-fold cross-validation. For more details on the experimental conditions and
procedure, refer to the provided Python code.
Source model As a preliminary step, we trained 100 neural networks to predict SPS. The hidden layer
widthLwas randomly chosen from the range [50,100], and we trained a neural network with a structure of
(input)-L-L-L-1. Each of the three hidden layers of the source model was used as an input to the transfer
models, and we examined the difference in prediction performance for the three layers.
Affine transfer The functions g1,g2, andg3in the affine transfer model were modeled by neural networks.
We used neural networks with one hidden layer for g1,g2andg3.
D.3 Heat capacity of organic polymers
D.3.1 Dataset
The task is to bridge the specific heat capacity of a given polymeric material calculated by the classical MD
simulation, including bias and variance, to its experimental value. Experimental values of the specific heat
capacity of the 70 polymers were collected from PoLyInfo (Otsuka et al., 2011). The MD simulation was
also applied to calculate their heat capacities. For models to predict the log-transformed heat capacity, a
given polymer with its chemical structure was translated into the 190-dimensional force field descriptors,
using RadonPy4(Hayashi et al., 2022). We randomly sampled 60 training polymers and tested the prediction
performance of a trained model on the remaining 10 polymers 20 times. The PoLyInfo sample identifiers for
the selected polymers are listed in the code.
D.3.2 Model definition and hyperparameter search
As described in Section 5.3, the 190-dimensional force field descriptor consists of nine blocks corresponding to
different types of features. The Jtfeatures that make up block trepresent discretized values of the density
function of the force field parameters assigned to the atoms, bonds, or dihedral angles that constitute the
2https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html
3https://github.com/yoshida-lab/XenonPy
4https://github.com/RadonPy/RadonPy
30Under review as submission to TMLR
Algorithm 3 Block relaxation algorithm for the model in Eq. (16).
Initialize
α0←ˆα0,olr,α1←ˆα1,olr,β←0,γ←← ˆγdiff
repeat
α←arg minαFα,β,γ
β←arg minβFα,β,γ
γ←arg minγFα,β,γ
untilconvegence
given polymer. Therefore, the regression coefficients of the features within a block should be estimated
smoothly. To this end, we imposed fused regularization on the parameters as
λ1∥γ∥2
2+λ2/summationdisplay
t∈TJt/summationdisplay
j=2/parenleftbig
γt,j−γt,j−1/parenrightbig2,
whereT={mass,charge,ϵ,σ,K bond,r0,Kangle,θ,Kdih},andJt= 10fort=massandJt= 20otherwise.
The regression coefficient γt,jcorresponds to the j-th feature of block t.
Ordinary linear regression The experimental heat capacity y=logCexp
Pwas regressed on the MD-
calculated property, without regularization, as ˆy=α0+α1fswhere ˆydenotes the conditional expectation
andfs= logCMD
P.
Learning the log-difference We calculated the log-difference logCexp
P−logCMD
Pand trained the linear
model with the ridge penalty. The hyperparameters λ1andλ2for the scale- and smoothness-regularizers
were determined based on five-fold cross validation across 25 equally space grids in the interval [10−2,102]for
λ1and in{50,100,150}forλ2.
Affine transfer We used the affine transfer model as
ˆy=α0+α1fs(x)−(βfs(x) + 1)·x⊤γ, (16)
wherefs(x)is the log-transformed MD-calculated heat capacity logCMD
P. In the model training, the objective
function was given as follows:
Fα,β,γ =1
nn/summationdisplay
i=1/braceleftbig
yi−(α0+α1fs(xi)−(βfs(xi) + 1)x⊤γ)/bracerightbig2
+λββ2+λγ,1∥γ∥2
2+λγ,2/summationdisplay
t∈TJt/summationdisplay
j=2/parenleftbig
γt,j−γt,j−1/parenrightbig2,
whereα= [α0α1]⊤. With a fixed λβ= 1, the remaining hyperparameters λγ,1andλγ,2were optimized
through five-fold cross validation over 25 equally space grids in the interval [10−2,102]forλγ,1and in
{50,100,150}forλγ,2.
The algorithm to estimate the parameters α,βandγis described in Algorithm 3, where α0,olrandα1,olr
are the estimated parameters of the ordinary linear regression model, and ˆγdiffis the estimated parameter
of the log-difference model. For each step, the full conditional minimization of Fα,β,γwith respect to each
parameter can be made analytically as
arg min
αFα,β,γ = (F⊤
sFs)−1y⊤
s(y+ (βfs(X) + 1)◦(Xγ)),
arg min
βFα,β,γ =−(fs(X)⊤diag(Xγ)2fs(X) +nλ2)−1fs(X)⊤diag(Xγ)(y−Fsα+Xγ),
arg min
γFα,β,γ =−(X⊤diag(fs(X)β+ 1)2X+ Λ)−1X⊤diag(fs(X)β+ 1)(y−Fsα),
31Under review as submission to TMLR
whereXdenote the matrix in which the i-th row is xi,y= [y1···yn]⊤,fs(X) = [fs(x1)···fs(xn)]⊤,
Fs= [fs(X)1], andd= 190.Λis a matrix including the two regularization parameters λγ,1andλγ,2as
Λ =D⊤D,whereD=/bracketleftbigg
λγ,1Id
λγ,2M/bracketrightbigg
, M =
−1 1 0··· 0 0
0−1 1··· 0 0
..................
0 0 0··· 0 0
..................
0 0 0··· − 1 1
←m-th rows,
wherem∈{10,30,50,70,90,110,130,150,170}. Note that the matrix Mis the same as the matrix [0I189]−
[I1890]except that the m-th row is all zeros. Note also that M∈R189×190, and therefore D∈R279×190and
Λ∈R190×190.
The stopping criterion for the algorithm was the same as Eq. (15).
32