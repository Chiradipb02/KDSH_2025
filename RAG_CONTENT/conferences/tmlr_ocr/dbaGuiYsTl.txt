Under review as submission to TMLR
Wasserstein Modality Alignment Makes Your Multimodal
Transformer More Robust
Anonymous authors
Paper under double-blind review
Abstract
Multimodal fusion with a multimodal transformer is an effective method for both early
and late fusion paradigms. However, in a multimodal transformer, the modality fusion is
performed solely through the self-attention mechanism, which is originally designed for uni-
modal token sequences. To improve the self-attention mechanism for handling multimodal
input, a parametric adapter model, like the Q-former in BLIP-2, is often used to align tokens
fromdifferentmodalities. Ourempiricalstudyunveilsthatonlyusingtheself-attentionlayer
to perform the modality fusion makes the model less robust to missing modalities and input
noise, as the model will overly rely on one certain modality. To improve the robustness of the
transformer, our paper proposes an implicit approach based on Wasserstein distance that
aligns tokens from different modalities without using any additional trainable parameters.
Our empirical study shows that the implicit modality alignment improves the effectiveness
of the multimodal Transformer in discriminative tasks, as well as its robustness to input
noise and missing modalities. We conduct experiments on four downstream task datasets,
including 2-modalities and 3-modalities tasks. We also consider different fusion paradigms,
i.e., early and late fusion. The experimental results show that our proposed method has
a significant improvement in both performance and robustness over all baselines across all
datasets and fusion paradigms.
1 Introduction
Multimodal machine learning (MML) mimics human perception by integrating multiple modalities such as
text, audio, images, video, and sensor data to form a comprehensive understanding of the world. Many
multimodal models have been applied to various tasks like multimodal medical diagnostics Hayat et al.
(2022a), sentiment analysis Zadeh et al. (2018) and malicious speech detection Kiela et al. (2020).
Aligning heterogeneous data in multimodal learning is crucial since such data often exhibit distinct dis-
tributions, representations, and noise levels. Proper alignment enhances the uniform representation of
these diverse data types, leading to improved performance and robustness in multimodal tasks Ghahre-
mani Boozandani & Wachinger (2024); Liang et al. (2024); Kim et al. (2020). To achieve better modality
alignment, various strategies are applied in large-scale multimodal models, such as the Q-Former in BLIP-2
Li et al. (2023), contrastive learning in CLIP Radford et al. (2021) and Imagebind Girdhar et al. (2023).
Multimodal fusion based on a one-tower transformer, named as multimodal transformer (MT), by its flex-
ibility and simplicity, are widely used for a variety of multimodal learning tasks Lee et al. (2023); Nagrani
et al. (2021); Zhi et al. (2024); Ma et al. (2021). Although the multimodal transformer can handle multi-
modal tokens as the input due to the flexibility of self-attention layers, it lacks a mechanism for modality
alignment during the fine-tuning process. In other words, it is not optimal to rely on a pre-trained multi-
modal transformer to achieve the modal alignment Kim et al. (2021); Wang et al. (2021). Taking ViLT Kim
et al. (2021) as an example, tokens from different modalities are concatenated together and processed by
the multimodal transformer. Several learning tasks, such as image text matching and word patch alignment
are applied during the pre-training phase to ensure the modality alignment. However, the alignment process
1Under review as submission to TMLR
is not enforced in the fine-tuning stage if the input tokens are from multimodal sources, which leads to
deteriorated performance in a downstream task.
Multimodal Transformer
······
······
1
imx2
imxcls
iH1
imH2
imH
1meθ2meθ2m
ix1m
ixXfθcfθ ˆiy
Multimodal Transformer
······
······
1
imx2
imx1meθ2meθ2m
ix1m
ixˆiyAverage of all test cases(a) Fine-tu ning process of multimodal transformer (b) F ine-tuning process of multimodal transformer
with WMA regularization
0.644
0.428Xfθcls
iH1
imH2
imHWMA 
regularization
cfθ
0.6620.706AUROC
F1
AUROC
MAE0.663
0.4610.6750.671AUROC
F1
AUROC
MAEAverage of all test cases+3.0%
+7.7%+2.0%-2.1%
Figure 1: The overview of the proposed method (using early fusion structures as an example). ( a) Fine-
tuning the multimodal transformer on the downstream task. The image data xm1
iand text data xm2
iare
firstly processed by the embedding operation eθm1andeθm2(in late fusion structure, eθm1andeθm2refer to
the feature extraction by the pretrained encoders, i.e., Bert for text and ViT for image. ) to get the token
sequence¯xm1
iand¯xm2
i, which then are concatenated and processed by the multimodal transformer fθX.Hicls
in the multimodal transformer is input into the classifier fθcfor predicting the label ˆyi. Note that there is
no specific module to adjust modality alignment in (a). ( b) We propose the WMA regularization method
to search for the optimal modality alignment for the target task. WMA calculates the OT distance between
Him1andHim2to represent the alignment degree of two modalities. The target OT distance range is set to
achieve the optimal alignment. Our proposed WMA method effectively improves both the performance and
robustness of the model, as shown in the lower left corner of ( 1) and (2) ( ’Average of all test cases’ refers
to the average result of normal test, test with noise and the test with missing modality. See more details in
Table. 1 and Table. 2)
.
Toaddressthisissue, weproposeWassersteinModalityAlignment(WMA),animplicitregularizationmethod
toaligntheWassersteindistancebetweentwomodalitieswithinamultimodaltransformer,asshowninFig. 1.
To make the computation feasible, we use the popular optimal transport (OT) distance Peyré et al. (2019) as
theinstantiationofWassersteindistance. WeregularizethedegreeofmodalityalignmentbyadjustingtheOT
distance between different modalities’ feature distributions. Interestingly, our empirical study demonstrates
that directly minimizing the OT distance between two modalities often leads to inferior performance, so our
WMA aligns two modalities with a task-dependent modality distance. In the practical sense, the proposed
WMA is a plug-and-play method and does not introduce any additional trainable parameters. Our main
contributions are three-fold:
•Weproposetoperformthemodalityalignmentinthefine-tuningprocessofapre-trainedmultimodal
transformer without any additional adapter and design the Wasserstein Modality Alignment based
on the optimal transport distance to achieve lightweight modality alignment in the feature tokens
of the transformer.
•Instead of minimizing the OT distance between any two modalities, our WMA is a task-dependent
modality alignment method that can handle different requirements for the degree of modality align-
ment.
2Under review as submission to TMLR
•We evaluate our proposed WMA with three strong baselines on four datasets, including 2-modalities
and 3-modalities tasks. We also consider different fusion paradigms, including both early fusion
and late fusion. Our experimental results demonstrate significant improvements in performance
and especially robustness across all tasks and test cases. In early fusion paradigm, the averaged
improvement of our method compared with the best result from all baselines over all datasets are
1.4%, 1.5%, 1.4%, 0.7%and 1.4 %, respectively. For the late fusion paradigm, the improvement are
1.5%, 1.8%, 1.3%, 2.3%and 1.9 %.
The paper is organized as follows. Sec. 2 gives an overview of related research. Sec. 3 introduces the
proposed method. Sec. 4 shows the experiment results and the analysis. Finally, Sec. 5 summarizes the
paper, its limitations, and future work.
2 Related Work
Modality alignment in multimodal learning. Almost all large-scale multimodal models use specific
strategies for modal alignment. Contrastive loss is a popular approach that promotes related modality
alignment by boosting the similarity of positive sample pairs Li et al. (2021); Radford et al. (2021); Girdhar
et al. (2023). LLaVA trains an image-to-text adapter to align the two modalities Liu et al. (2024). Flamingo
achieves modality alignment by combining a pretrained vision encoder and a language model through a
series of gated cross-attention layers, allowing for effective interaction between visual and textual inputs
Alayrac et al. (2022). In Li et al. (2023), BLIP-2 employs a lightweight Querying Transformer to connect
frozen image encoders with large language models for modality alignment. In contrast, the multimodal
transformer, i.e., ViLT Kim et al. (2021) and SimVLMWang et al. (2021) only employ the agent task such
as image text matching and word patch alignment for aligning modality during pre-training, lacking such
approach at fine-tuning stage. To solve this problem, we propose an implicit regularization method to adjust
the alignment of the multimodal transformer during the fine-tuning process.
Robustness of multimodal learning. Modality noise and absence are two challenges to the robustness
of multimodal learning. In ML for healthcare, the patient may be missing the data such as an X-ray due
to economic/timing issues Zhi et al. (2024). In addition, some sensor data may be accompanied by a lot
of noise due to improper wear. A similar situation occurs in vision-language tasks. For example, some
online recommender models are unable to receive images uploaded by users or receive blurry images with
a lot of noise due to network issues. Sijie et al. Mai et al. (2022) propose the multimodal information
bottleneck to filter out noisy information in unimodal representations. Md et al. Islam & Iqbal (2022)
apply a cooperative multitask learning-based guided multimodal fusion approach to get robust performance
on noisy and misaligned sensor data. For the missing modality problem, Ma et al. (2021) reconstructs the
missing modalities using modality priors and Bayesian Meta-Learning during the inference phase. Lee et al.
(2023) propose the missing-aware prompts to learn the patterns of complete and incomplete samples. In
Zhi et al. (2024), an approach inspired by in-context learning is proposed to improve the data efficiency for
multimodal learning under missing modality and data scarcity. However, these methods require additional
parameters to enhance the incomplete samples. Differently, we employ the nonparametric regularization
approach to obtain robust multimodal representation.
3 Proposed Method
We first describe the problem definition, our motivation and the proposed method is elaborated on later.
3.1 Problem setting
We consider the multimodal transfer learning problem with a downstream dataset Dcontaining multi-
modal input samples. For notation simplicity, we assume there are two modalities in the dataset, i.e.,
D={xm1
i,xm2
i,yi}N
i=1whereyiis the label. Note that our framework can handle any number of modalities
in principle and We will describe how to extend it to 3-modalities tasks later. When we employ a pretrained
multimodal transformer for solving the target task, some embedding operations or feature extraction are
3Under review as submission to TMLR
MT-early-WMAMT-early
MT-early-RegBNmean std sum
MT-early-WMAMT-early
MT-early-RegBN0.03    1.59      43331.60     690.63NLL
0.03    1.52      41435.76     602.70
0.04    1.75      45527.80     811.38
mean std sum NLL
0.00    2.56     433552.72    113.28
0.03    1.26     293459.95    143.68
0.04    0.45     107685.24    702.77Text 
Image 
MT-early-WMAMT-early
MT-early-RegBNmean std sum
MT-early-WMAMT-early
MT-early-RegBN0.03    1.59      43331.60     690.63NLL
0.03    1.52      41435.76     602.70
0.04    1.75      45527.80     811.38
mean std sum NLL
0.00    2.56     433552.72    113.280.03    1.26     293459.95    143.68
0.04    0.45     107685.24    702.77Text 
Image (a) The histogram of integrated gradients value
(b) The statistics of integrated gradients value
Figure 2: The Integrated gradients value of input token under Hateful Memes dataset and early fusion
structure. We calculate the Integrated gradients value for every input token for all samples and put them
together to obtain the distribution analysis. The bigger Integrated gradients value refers to the bigger
contribution to the final classification. ( a) The histogram of the Integrated gradients values. ( b) The
statistics of the distribution of the Integrated gradients values. MT-early, MT-eary-RegBN and MT-early-
WMArefertotheoriginalMT,MTwithRegBNregularizationGhahremaniBoozandani&Wachinger(2024)
and MT with our proposed WMA regularization. The performance and robustness of three methods can be
find in Table 1. From the statistics, we find that MT-early heavy relies on text for decision due to the sum of
the Integrated gradients values of the text token 690.63 is much higher than that of the image token 113.28.
TheRegBNmethodalleviatestheproblemslightlywiththenumbers602.70and143.68. OurproposedWMA
method solves this problem – the contribution of text token and image token is almost identical–811.38 and
702.77. In WMA, the image modality has a more Gaussian-like attribution value distribution compared with
others, meaning that the model exploits the weak modality to learn the task.
4Under review as submission to TMLR
performed firstly performed on the input data xm1
iandxm2
i:
¯xm1
i=eθm1(xm1
i) = [m1cls;m11;···;m1Lm1], (1)
¯xm2
i=eθm2(xm2
i) = [m2cls;m21;···;m2Lm2], (2)
whereeθm1andeθm2refer to the embedding operation for two modalities such as linear projection, position
embedding and modality type embedding Kim et al. (2021) in early fusion structure. In late fusion structure,
eθm1andeθm2refer to the feature extraction by the pretrained encoders, i.e., Bert for text and ViT for image.
m1clsandm2clsare the added classification head token and Lm1andLm2are the number of embedded
tokens/feature. [; ]means the concatenate operation. Then, the multimodal transformer fθXinference the
output tokens Hiby
Hi=fθX([¯xm1
i;¯xm2
i]) = [Hicls;Him1;Him2], (3)
whereHim1∈RLm1×dandHim2∈RLm2×dare the processed features for two modalities, dis the em-
bedding dimension. Hicls∈R1×dis the final classification head token which can be input into an added
classifier/regressor fθcfor predicting the label and minimizing the loss:
ˆyi=fθc(Hicls),ℓ(i)
task=ℓcls(ˆyi,yi), (4)
whereℓclsis the task-dependent loss function such as cross-entropy and ℓ(i)
taskis the loss value for the ith
sample.
The issue with the fine-tuning process described above is that it lacks the approach for aligning xm1
iandxm2
i
or their representation in this multimodal transformer. We will introduce our proposed method for solving
this problem later.
3.2 Motivation
Although the strength of multimodal learning over unimodal learning has been proved both in theory Huang
et al. (2021); Lu (2023) and in practice Radford et al. (2021), it is plagued by missing modalities Zhi et al.
(2024); Lee et al. (2023) and input noise Papandreou et al. (2009). One of the reasons for the brittleness
of multimodal learning is that it heavily relies on one certain modality to make the prediction as a result
of the distributional gap between different modalities. In other words, some modalities are more easily by
the training process because those modalities are more learnable than others. The phenomenon is relevant
to shortcut learning Geirhos et al. (2020) where the model will use the shortcut features to complete the
given learning task and fail to generalize to out-of-distribution scenarios. To illustrate the phenomenon
in multimodal learning, we use the explanation tool Integrated Gradients Sundararajan et al. (2017) to
analyze a fine-tuned multimodal transformer on a two-modality dataset Hateful Memes under the early
fusion structure, as shown in Fig. 2. Integrated Gradients is an attribution method that computes the
contribution of each input feature to the output of a neural network by integrating gradients along a path
from a baseline input to the actual input. With this tool, we obtain the contribution of each input token to
the final classification.
It is observed from Fig. 2 that standard fine-tuning mainly uses the strong modality (text) to make the
prediction while the weak modality (image) is not sufficiently exploited. To address the observed shortcut
learning issue and make the model more robust to novel test environments, we propose to control the
distributional gap between different modalities. As the distribution of easily learned modalities is regularized
to be close to other modalities, the model is forced to avoid shortcut learning and use all modalities to learn
the task.
3.3 Wasserstein Modality Alignment
We propose the Wasserstein Modality Alignment (WMA), an implicit regularization for adjusting the align-
ment of different modalities in the multimodal transformer. For keeping computation efficient, we use the OT
distance as the instantiation of Wasserstein distance and represent the alignment degree of two modalities
5Under review as submission to TMLR
/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014 /uni00000014/uni00000011/uni00000015 /uni00000014/uni00000011/uni00000017 /uni00000014/uni00000011/uni00000019 /uni00000014/uni00000011/uni0000001b /uni00000015 /uni00000017 /uni00000014/uni00000013
/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003
/uni00000013/uni00000011/uni00000018/uni00000018/uni00000013/uni00000013/uni00000011/uni00000018/uni0000001a/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000013/uni00000011/uni00000019/uni00000015/uni00000018/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000013/uni00000011/uni00000019/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000015/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000013/uni00000024/uni00000038/uni00000035/uni00000032/uni00000026
/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000048/uni00000056/uni00000057 /uni0000004c/uni00000050/uni00000044/uni0000004a/uni00000048 noise/uni00000003 /uni00000057/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000051/uni00000052/uni0000004c/uni00000056/uni00000048
/uni0000004c/uni00000050/uni00000044/uni0000004a/uni00000048 miss /uni00000003 /uni00000057/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000050/uni0000004c/uni00000056/uni00000056
Figure3: TheperformanceofMT-early-WMAonHatefulMemesdatasetsinearlyfusionstructureunder α=
1with different value of β. Whenβ= 0, it equals to minimize the OT distance. The best overall performance
is gained at β= 1.8. The performance and robustness of the model are not exactly positively/negatively
correlated with the value of β, indicating that the alignment of the model should be task-dependent, instead
of minimizing the OT value. See more results in Table 5-8.
by it. Uniquely, WMA searches the task-dependent optimal alignment through two hyperparameters rather
than directly minimizing the OT distance.
For the feature Him1andHim2, the optimal transport problem is defined as
W(Hm1
i,Hm2
i) = min
T∈Σ(σ,δ)⟨C,T⟩, (5)
where C∈RLm1×Lm2is a manually defined cost matrix, with each element cpqrepresenting the distance
between the pth token of Hm1
iand theqth token of Hm2
i. The optimal solution T⋆is known as the optimal
transport plan. The set Σ(σ,δ)is defined as:
Σ(σ,δ) =/braceleftig
T∈RLm1×Lm2
+|T1Lm2=σ, T⊤1Lm1=δ/bracerightig
, (6)
whereσandδare the normalized distributions for Hm1
iandHm2
i, respectively, which are given by:
σ=1
Lm11Lm1, δ =1
Lm21Lm2, (7)
where 1Lm1and1Lm2are vectors of ones with lengths Lm1andLm2, respectively. To keep the computation
efficient, we apply the IPOT algorithm Xie et al. (2018) to solve this problem.
The optimal transport cost Di
m1m2is calculated as
Di
m1,m2=⟨C,T∗⟩. (8)
We useDi
m1,m2as the reference for the alignment degree and manually set the target value to regularize the
model parameters during the fine-tuning phase by modifying the loss function
ℓ(i)
task=ℓcls(ˆyi,yi) +α(Di
m1,m2−¯Dm1,m2)2. (9)
6Under review as submission to TMLR
We use the average ¯Dm1,m2of the first batch at model initialization (by applying the pretrained weight) as a
basisandsetthesearchrangebyacombinationofhyperparameters αandβ. Bythisstrategy, wedonotmin-
imizeDi
m1,m2, instead, we search for optimal alignment for the different modalities in the fine-tuning phase.
Fig. 3 shows the search results for the Hateful Memes dataset in early fusion structure under α= 1with
different values of β. The best overall performance is achieved at β= 1.8which demonstrates the superiority
of the proposed WMA over minimizing OT values. For 3-modalities tasks Hi= [Hicls;Him1;Him2;Him3],
we can easily modify Eq. 9 to
ℓ(i)
task=ℓcls(ˆyi,yi) +α((Di
m1,m2−β¯Dm1,m2)2
+(Di
m1,m3−β¯Dm1,m3)2
+(Di
m2,m3−β¯Dm2,m3)2). (10)
4 Experiment
We first introduce the experimental settings and then present the experimental results of our methods and
baselines on five datasets and two fusion paradigms, demonstrating the effectiveness of our method.
4.1 Experimental Setting
Datasets. We select two 2-modalities datasets and two 3-modalities datasets across different downstream
tasks to evaluate our proposed method.
•Hateful Memes Kiela et al. (2020). This is a binary classification task with two modalities, image
and text. The task is to detect the maliciousness of memes. The numbers of the samples in the
training/val/testing dataset are 8500, 500 and 1000.
•MM-IMDb Arevalo et al. (2017). This is a multi-label (25 labels) classification task with two
modalities, image and text. The task is to tag the film. The numbers of the samples in the
training/val/testing dataset are 32278, 5411 and 16120.
•UR-FUNNYHasanetal.(2019). Thisisabinaryclassificationtaskwiththreemodalities, text, video
and audio. The task is to detect humor in talks. The number of samples in the training/val/testing
dataset are 8074, 1034, 1058.
•MOSEI Zadeh et al. (2018). This is a regression task with three modalities, text, video and audio.
The task is to recognize the degree of sentiment. The number of samples in the training/val/testing
dataset are 16265, 1869, 4643.
•MedFuse-I Hayat et al. (2022b). This is a real-world dataset which contains EHR and X-ray data for
each patient. This binary classification task aims to predict in-hospital mortality after the first 48
hours spent in the ICU. The numbers of the samples in the training/val/testing dataset are 18845,
2138 and 5243.
Metrics. We set the metrics for each dataset according to the tasks. For Hateful Memes, UR-FUNNY
and Medfuse-I, we use the AUROC as evaluation metrics. For MM-IMDb and MOSEI, we use F1 score and
MAE, respectively.
Multimodal Transformer for different fusion paradigms. We consider two popular multimodal fusion
paradigms by multimodal transformer: early fusion and late fusion. For early fusion, we use a 12-layer and
12-head transformer and initialize it with the pretrained weight ViLT-B Kim et al. (2021). For late fusion,
we employ a 2-layer and 8-head transformer and randomly initialize it. All input tokens are added with
modality embedding and a cls token by following Kim et al. (2021).
Input data processing. In early fusion structure, for vision-language tasks Hateful Memes and MM-IMDb,
we follow the operation in ViLT Kim et al. (2021): the text is embedded by Bert and the image is split into
patches (same with ViT). For UR-FUNNY and MOSEI, we use MultiBench Liang et al. (2021) to get the
7Under review as submission to TMLR
embedded feature of three modalities. For Medfuse-I, we use a linear layer to embed the EHR data and the
same way with ViLT to process the X-ray image. In late fusion structure, for vision-language tasks Hateful
Memes and MM-IMDb, a ViT-base and a Bert-base are employed to extract the feature of two modalities,
while for UR-FUNNY and MOSEI, three GRU models are involved for feature extraction by following Liang
et al. (2021). For Medfuse-I, we follow the operation in Hayat et al. (2022b).
Baseline. We select the following strong baseline for comparison. These methods focus on efficiently
adjusting modal fusion to improve performance in multimodal learning.
•Standard transfer learning. We add a classifier/regressor for the target task and fine-tune all layers
of the models, which is denoted as MT-early and MT-late in the latter part.
•RegBN Ghahremani Boozandani & Wachinger (2024). Batch normalization of multimodal data with
regularization (RegBN) uses the Frobenius norm as a regularizer term to address the side effects
of confounders and underlying dependencies among different modality sources. RegBN provides a
plug-and-play approach to early and late fusion.
•GWMACGongetal.(2022). Gromov-Wassersteinmulti-modalalignmentandclustering(GWMAC)
learns the GromovWasserstein barycenter of their kernel matrices. GW barycenter is associated with
the GW distances between the different modalities to the clusters, and the optimal transport plans
corresponding to the GW distances help to achieve the alignment and the clustering of the multi-
modal data jointly. This method only provides a late fusion version, and we extend it to an early
fusion framework.
•MIB Mai et al. (2022). The multimodal information bottleneck (MIB) learns the minimal sufficient
representation for a given task by maximizing the mutual information between the representation
and the target and simultaneously constraining the mutual information between the representation
and the input data. The method is also compatible with early fusion and late fusion frameworks.
Hyperparametersettings. WesetthebatchsizeforHatefulMemes, MM-IMDb, UR-FUNNYandMOSEI
as 128, 64, 256, 256. The learning rate search range is [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]. The learning rate strategy
is linear decay with warm-up. The search range of αis set as [0.1, 0.2, 1.0, 5.0]. The search range of βis
[0.1, 0,2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 4.0, 10.0]. Early stopping with patience 5 is applied for
selecting the weight. Experiments are running on Tesla V100 GPUs.
Robustness test setting. We test the robustness of the model in two ways: test with modality noise
and test with missing modality. When simulating noise in a particular modality, we consider three kinds
of noise for image data: Gaussian noise, salt and pepper noise and Poisson noise. For text data, we use
swap adjacent letters, randomly permute middle section, keyboard typos, sticky keys and omission to create
the noisy input. For time series data (i.e., some features from MultiBench Liang et al. (2021) and EHR
data), we add the Gaussian Noise, uniform noise and Poisson noise. When simulating a missing modality,
we randomly remove a proportion of the samples for that modality by setting all pixels to 0 for the image,
replacing the text with an empty string and setting all values to 0 for time series. For Gaussian noise, the
noise level is set to the standard deviation of the noise. For the salt and pepper noise, the noise level refers to
the proportion of total pixels perturbed. For the uniform noise, the noise level refers to the noise range. For
the text data noise, the noise level is the probability of randomly applying noise to a word. We set the noise
level to be 0.25, 0.5, 0.75 to the image data, text data and time series data. The final result is the average
of the performance at different noise levels. For the missing modality test, We set 25 %, 50%and 75 %as
the missing proportion and calculate the average performance. Note that the real-world dataset Medfuse-I
lacks X-ray images for 74 %of samples and we do not manually set additional missing cases to simulate real
application scenarios.
Table 1 and 2 present the quantitative results of our proposed method WMA with the baselines across all
the datasets and test cases under early and late fusion, respectively. From Table 1 and 2, we summarize the
following observations:
•Our proposed WMA is more performant than the MT in all datasets and test cases under both
early and late fusion paradigms. Specifically, in the average performance of all test cases, the
8Under review as submission to TMLR
Table 1: Results of our proposed method with the baseline on all datasets and test cases for early fusion.
The bold numbers mean the best performance. The bigger AUROC and F1 and smaller MAE refer to better
performance.
normal test test with modality noise test with missing modality
Datasets Metric Methods normal image/video noise text/EHR noise audio noise image/video missing text missing audio missing average
Hateful Memes AUROC↑MT-early
MT-early-GWMAC
MT-early-RegBN
MT-early-MIB
MT-early-WMA0.730
0.732
0.736
0.743
0.7480.547
0.564
0.546
0.560
0.5830.666
0.680
0.680
0.661
0.682-
-
-
-
-0.626
0.626
0.620
0.619
0.6450.653
0.669
0.650
0.648
0.659-
-
-
-
-0.644
0.654
0.646
0.646
0.663
MM-IMDb F1↑MT-early
MT-early-GWMAC
MT-early-RegBN
MT-early-MIB
MT-early-WMA0.551
0.553
0.558
0.563
0.5630.251
0.353
0.216
0.209
0.3550.462
0.470
0.463
0.454
0.471-
-
-
-
-0.464
0.472
0.451
0.473
0.4770.412
0.422
0.429
0.423
0.437-
-
-
-
-0.428
0.454
0.423
0.424
0.461
UR-FUNNY AUROC↑MT-early
MT-early-GWMAC
MT-early-RegBN
MT-early-MIB
MT-early-WMA0.700
0.705
0.714
0.711
0.7120.645
0.656
0.613
0.641
0.6810.678
0.678
0.665
0.682
0.6860.635
0.631
0.616
0.63
0.6430.612
0.618
0.618
0.606
0.6240.670
0.673
0.645
0.670
0.6740.696
0.701
0.692
0.690
0.7030.662
0.666
0.652
0.666
0.675
MOSEI MAE↓MT-early
MT-early-GWMAC
MT-early-RegBN
MT-early-MIB
MT-early-WMA0.633
0.624
0.617
0.619
0.6150.691
0.679
0.714
0.679
0.6760.834
0.827
0.877
0.852
0.8240.672
0.657
0.665
0.666
0.6640.651
0.632
0.622
0.628
0.6190.826
0.817
0.848
0.831
0.8130.635
0.634
0.629
0.629
0.6230.706
0.696
0.704
0.701
0.691
Medfuse-I AUROC↑MT-early
MT-early-GWMAC
MT-early-RegBN
MT-early-MIB
MT-early-WMA0.840
0.844
0.855
0.852
0.8610.762
0.787
0.751
0.744
0.7960.723
0.749
0.722
0.715
0.755-
-
-
-
--
-
-
-
--
-
-
-
--
-
-
-
-0.763
0.793
0.776
0.770
0.804
Table 2: Results of our proposed method with the baseline on all datasets and test cases for late fusion. The
bold numbers mean the best performance. The bigger AUROC and F1 and smaller MAE refer to better
performance.
normal test test with modality noise test with missing modality
Datasets Metric Methods normal image/video noise text/EHR noise audio noise image/video missing text missing audio missing average
Hateful Memes AUROC↑MT-late
MT-late-GWMAC
MT-late-RegBN
MT-late-MIB
MT-late-WMA0.718
0.724
0.731
0.727
0.7280.652
0.663
0.649
0.665
0.6710.666
0.675
0.668
0.662
0.667-
-
-
-
-0.680
0.681
0.668
0.675
0.6920.622
0.622
0.623
0.640
0.663-
-
-
-
-0.668
0.673
0.668
0.674
0.684
MM-IMDb F1↑MT-late
MT-late-GWMAC
MT-late-RegBN
MT-late-MIB
MT-late-WMA0.602
0.603
0.612
0.612
0.6100.469
0.474
0.458
0.453
0.4890.488
0.495
0.505
0.500
0.510-
-
-
-
-0.553
0.559
0.550
0.549
0.5590.414
0.419
0.411
0.415
0.426-
-
-
-
-0.505
0.510
0.507
0.506
0.519
UR-FUNNY AUROC↑MT-late
MT-late-GWMAC
MT-late-RegBN
MT-late-MIB
MT-late-WMA0.700
0.700
0.709
0.712
0.7100.651
0.653
0.624
0.627
0.6600.686
0.692
0.694
0.694
0.6980.693
0.699
0.683
0.692
0.7020.654
0.658
0.645
0.643
0.6600.641
0.646
0.664
0.656
0.6720.676
0.680
0.671
0.676
0.6830.672
0.675
0.670
0.671
0.684
MOSEI MAE↓MT-late
MT-late-GWMAC
MT-late-RegBN
MT-late-MIB
MT-late-WMA0.638
0.627
0.610
0.610
0.6080.665
0.661
0.683
0.684
0.6430.826
0.811
0.829
0.844
0.8070.642
0.630
0.627
0.619
0.6150.638
0.633
0.629
0.624
0.6120.822
0.812
0.829
0.824
0.8010.640
0.633
0.637
0.626
0.6090.696
0.687
0.692
0.690
0.671
Medfuse-I AUROC↑MT-early
MT-early-GWMAC
MT-early-RegBN
MT-early-MIB
MT-early-WMA0.848
0.853
0.859
0.860
0.8640.781
0.794
0.785
0.774
0.8130.742
0.758
0.743
0.738
0.775-
-
-
-
--
-
-
-
--
-
-
-
--
-
-
-
-0.790
0.802
0.796
0.791
0.817
improvements are 3.0 %, 7.7%, 2.0%, 2.1%5.4%for five datasets in early fusion and 2.4 %, 2.8%,
1.8%, 3.6%, 3.4%in late fusion. Note that WMA in early fusion benefits the model more than in
late fusion, indicating that the distributional regularization is an essential component to make the
self-attention-based modality fusion more robust. Our proposed WMA method is also competitive
compared to the best results from all baselines. To be specific, MT-early-WMA surpasses the best
result at 1.4 %,1.5%,1.4%, 0.7%and 1.4 %in five datasets. For MT-late-WMA, the number comes to
1.5%, 1.8%, 1.3%, 2.3%and 1.9 %.
•GWMAC exhibits a similar trend as the proposed WMA method in most datasets, which shows
improvement in both normal test and robustness compared with the MT, but not as much as WMA.
What’s worse, GWMAC introduces more parameters and computing burden. It’s worth noting that
MIB and RegBN show the opposite trend with GWMAC and WMA, i.e., in most of the datasets,
the performance of the normal tests is improved (some of them even achieve the highest score),
9Under review as submission to TMLR
while the robustness is compromised. We attribute it to the fact that both MIB and RegBN aim to
learn a multimodal representation without redundant information, making the model mainly learn
from a certain modality. In our proposed WMA, the model adaptively achieves the proper modality
alignment to balance multiple modalities to improve both performance and robustness.
•In summary, our WMA method simultaneously improves the performance and robustness of the
multimodal transformer, outperforming all baselines across all datasets and fusion structures. In
addition, compared to most parametric methods, WMA does not introduce any additional trainable
parameters.
We also report all the searching results under various combinations of α(weight for the WMA loss) and β
(weight for the target distance) in Table 5-8 for early fusion and Table 9-12 for late fusion. We have the
following observations:
•The modality alignment can be effectively adjusted by using our proposed WMA method. The
obvious performance improvements can be achieved in almost half of the settings.
•The performance and robustness of the model are not exactly positively/negatively correlated with
the value of β. Different tasks and even different fusion structures require different degrees of
modality alignment. For example, Table 5 shows better performance and robustness at bigger target
OT values for the Hateful Memes dataset under early fusion and the opposite trend is observed from
Table 9 for Hateful Memes datasets under late fusion. Nevertheless, our proposed method can
achieve task-dependent optimal alignment.
4.2 Ablation study
Minimizing the OT distance. We compare minimizing the OT distance (used as an agent task in
many pre-train tasks Kim et al. (2021)) with our proposed WMA method. We simulate this strategy by
settingαto 1 andβto 0. The comparison of this method with our proposed method on the Hateful Memes
dataset is shown in Fig. 4. All results are shown in Table 13 and Table 14 for early fusion and late fusion,
respectively. Table 13 and Table 14 indicate that our proposed WMA outperforms this strategy in most of
the test cases. The average value of all test cases in WMA exceeds that in Minimizing the OT at 2.0 %, 5.3%,
1.2%and 2.3 %for all datasets under early fusion. For late fusion, the improvements are 1.2 %, 2.0%, 19.0 %
and 2.6 %. We assume that different tasks require different levels of modality heterogeneity and alignment,
and over-alignment could cause a loss of modality heterogeneity which might be important to the model
performance and robustness. Through only two hyperparameters, our proposed WMA method adaptively
achieves optimal modal alignment, guaranteeing both performance and robustness.
/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000048/uni00000056/uni00000057
/uni0000004c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000003
/uni00000051/uni00000052/uni0000004c/uni00000056/uni00000048
/uni00000057/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000051/uni00000052/uni0000004c/uni00000056/uni00000048 /uni0000004c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000050/uni0000004c/uni00000056/uni00000056/uni0000004c/uni00000051/uni0000004a   /uni00000057/uni00000048/uni0000005b/uni00000057/uni00000003
/uni00000050/uni0000004c/uni00000056/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000003/uni00000032/uni00000037
/uni0000003a/uni00000030/uni00000024
(a) early fusion
/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000048/uni00000056/uni00000057
/uni0000004c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000003
/uni00000051/uni00000052/uni0000004c/uni00000056/uni00000048
/uni00000057/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000051/uni00000052/uni0000004c/uni00000056/uni00000048 /uni0000004c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000050/uni0000004c/uni00000056/uni00000056/uni0000004c/uni00000051/uni0000004a   /uni00000057/uni00000048/uni0000005b/uni00000057/uni00000003
/uni00000050/uni0000004c/uni00000056/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000003/uni00000032/uni00000037
/uni0000003a/uni00000030/uni00000024 (b) late fusion
Figure 4: Results of our proposed method with minimizing OT distance on Hateful Memes datasets. See
the result for all datasets in Table 13 and Table 14.
Different distance metrics. We also explore different distance metrics in the proposed regularization
method. Specifically, weselectthecosinesimilarity(CS),Jensen–Shannon(JS)divergenceandtotalvariation
10Under review as submission to TMLR
/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000048/uni00000056/uni00000057
/uni0000004c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000003
 /uni00000051/uni00000052/uni0000004c/uni00000056/uni00000048
/uni00000057/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000051/uni00000052/uni0000004c/uni00000056/uni00000048 /uni0000004c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000050/uni0000004c/uni00000056/uni00000056/uni0000004c/uni00000051/uni0000004a   /uni00000057/uni00000048/uni0000005b/uni00000057/uni00000003
/uni00000050/uni0000004c/uni00000056/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000030/uni00000037/uni00000042/uni00000048/uni00000044/uni00000055/uni0000004f/uni0000005c
/uni00000030/uni00000037/uni00000042/uni00000048/uni00000044/uni00000055/uni0000004f/uni0000005c/uni00000042/uni00000026/uni00000036
/uni00000030/uni00000037/uni00000042/uni00000048/uni00000044/uni00000055/uni0000004f/uni0000005c/uni00000042/uni0000002d/uni00000036
/uni00000030/uni00000037/uni00000042/uni00000048/uni00000044/uni00000055/uni0000004f/uni0000005c/uni00000042/uni00000037/uni00000039
/uni00000030/uni00000037/uni00000042/uni00000048/uni00000044/uni00000055/uni0000004f/uni0000005c/uni00000042/uni0000003a/uni00000030/uni00000024
(a) early fusion
/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni00000003/uni00000057/uni00000048/uni00000056/uni00000057
/uni0000004c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000003
 /uni00000051/uni00000052/uni0000004c/uni00000056/uni00000048
/uni00000057/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000051/uni00000052/uni0000004c/uni00000056/uni00000048 /uni0000004c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000050/uni0000004c/uni00000056/uni00000056/uni0000004c/uni00000051/uni0000004a   /uni00000057/uni00000048/uni0000005b/uni00000057/uni00000003
/uni00000050/uni0000004c/uni00000056/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni00000019/uni00000018/uni00000013/uni00000011/uni0000001a/uni00000030/uni00000037/uni00000042/uni0000004f/uni00000044/uni00000057/uni00000048
/uni00000030/uni00000037/uni00000042/uni0000004f/uni00000044/uni00000057/uni00000048/uni00000042/uni00000026/uni00000036
/uni00000030/uni00000037/uni00000042/uni0000004f/uni00000044/uni00000057/uni00000048/uni00000042/uni0000002d/uni00000036
/uni00000030/uni00000037/uni00000042/uni0000004f/uni00000044/uni00000057/uni00000048/uni00000042/uni00000037/uni00000039
/uni00000030/uni00000037/uni00000042/uni0000004f/uni00000044/uni00000057/uni00000048/uni00000042/uni0000003a/uni00000030/uni00000024 (b) late fusion
Figure 5: Results of our proposed method with different distance metrics on Hateful Memes datasets. See
the result for all datasets in Table 15 and Table 16.
(TV) distance. The comparison of all methods on the Hateful Memes dataset is shown in Fig. 5. All results
under early fusion and late fusion are shown in Table 15 and Table 16, respectively. From Table 15 and Table
16, we find that while the WMA method performs best across almost all datasets and fusion paradigms,
some methods come close in certain datasets, i.e., cosine similarity in Hateful Memes datasets. This proves
the validity of our principle idea: obtaining task-dependent optimal fusion representations by adjusting the
modal alignment in the multimodal transformer.
Computational efficiency. To provide a more intuitive demonstration of the computational efficiency of
our proposed method, we compare the training times of all methods on the UR-FUNNY and MOSEI datasets
under early fusion and late fusion. Specifically, we report the time required for a training step, including
both the forward and backpropagation processes for a single batch of data. As shown in Table 4, our method
surpasses the strong baselines GWMAC, RegBN, and MIB in terms of training speed. This improvement can
be attributed to the fact that GWMAC, RegBN, and MIB introduce additional parameters, while our WMA
method operates without training any new parameters. Although the fast OT method Xie et al. (2018) used
in the paper increases the computation time for distance calculations, the overall impact on runtime remains
minimal. During inference, our method retains the same number of parameters and execution speed as the
basic MT. Therefore, the proposed WMA method demonstrates excellent computational efficiency.
Robustness to different random seeds. The WMA algorithm utilizes the average OT value of the first
batch of samples as the search basis. To verify the robustness of the method under different random seeds,
we conduct an ablation study using the UR-FUNNY dataset with early fusion. For each experiment, we use
10 different random seeds: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]. We report the average value along with the
standard deviation across all baselines in Table 3. From the table, we observe that the results of all methods
exhibit slight variations under different random seeds. However, the proposed WMA method consistently
demonstrates a clear advantage in terms of both performance and robustness. Additionally, we find that the
optimal values for αandβin the WMA method vary slightly depending on the seed (e.g., [ α=5,β=1.8] for
random seed = 0, and [ α=1,β=1.2] for random seed = 20). This adaptability highlights the WMA method’s
strength in achieving optimal modality alignment under varying conditions.
Table 3: The performance of different methods on UR-FUNNY dataset under early fusion with different
random seeds.
normal test test with modality noise test with missing modality
Datasets Metric Methods normal image/video noise text/EHR noise audio noise image/video missing text missing audio missing
UR-FUNNY AUROC↑MT-early
MT-early-GWMAC
MT-early-RegBN
MT-early-MIB
MT-early-WMA0.698 ±0.003
0.701 ±0.005
0.716 ±0.004
0.710 ±0.003
0.713 ±0.0030.642 ±0.004
0.653 ±0.004
0.613 ±0.009
0.643 ±0.008
0.681 ±0.0040.677 ±0.003
0.675 ±0.004
0.663 ±0.005
0.682 ±0.006
0.688 ±0.0040.634 ±0.004
0.628 ±0.006
0.617 ±0.011
0.632 ±0.005
0.646 ±0.0040.615 ±0.006
0.610 ±0.008
0.613 ±0.003
0.611 ±0.003
0.626 ±0.0030.673 ±0.003
0.671 ±0.004
0.648 ±0.002
0.666 ±0.003
0.675 ±0.0040.691 ±0.006
0.700 ±0.003
0.696 ±0.004
0.692 ±0.003
0.708 ±0.004
11Under review as submission to TMLR
Table 4: Computational time of a step for different methods on UR-FUNNY and MOSEI datasets under
early fusion and late fusion.
early fusion late fusion
models/datasets UR-FUNNY MOSEI UR-FUNNY MOSEI
MT-late
MT-late-GWMAC
MT-late-RegBN
MT-late-MIB
MT-late-WMA
MT-early-CS
MT-early-JS
MT-early-TV2.364s
3.290s
3.962s
3.476s
2.788s
2.381s
2.398s
2.409s2.288s
2.801s
3.287s
2.947s
2.570s
2.312s
2.472s
2.335s1.735s
2.172s
3.298s
2.786s
2.098s
1.778s
1.816s
1.983s1.506s
2.054s
2.990s
2.409s
1.920s
1.554s
1.609s
1.761s
5 Discussion
This paper primarily investigates the application of WMA to multimodal transformers in discriminative
tasks. Additionally, we explore the potential applications of WMA in generative tasks. Through a compre-
hensive literature survey, we observed that current generative multimodal large language models (MLLMs)
continue to rely on the transformer architecture, supplemented by connectors that bridge different modality
tokens—for example, the MLP used in LLAVA Liu et al. (2024) and the Q-Former used in BLIP2 Li et al.
(2023). We propose that our WMA method can be seamlessly integrated with these existing approaches for
the following reasons:
•Identical frameworks. Most generative MLLMs also use the transformer architecture, but involve a
decoder on the output side. There is no architectural conflict for implementing the WMA in such
MLLMs.
•Non-parametric approach. WMA is a non-parametric method that does not require additional
training, thereby avoiding any extra overhead in the training process of MLLMs.
•Token-level implementation. WMA operates at the token level within the input to the transformer,
ensuring compatibility with the connectors employed in MLLMs.
We believe that further regularization of MLLMs using WMA will enhance their performance as well as
robustness, representing a significant direction for our future work.
Multimodal systems can amplify biases in the data, i.e., focusing on a specific characteristic in hiring, or
credit scoring system Geirhos et al. (2020); Adewumi et al. (2024). Here We would like to discuss the impact
of the proposed methodology on this issue. From Fig. 2 we observe that MT-early heavily relies on text
for the decision due to the sum of the Integrated gradients values of the text token 690.63 is much higher
than that of the image token 113.28. The RegBN method alleviates the problem slightly with the numbers
602.70 and 143.68. Our proposed WMA method solves this problem – the contribution of text token and
image token is almost identical–811.38 and 702.77. In WMA, the image modality has a more Gaussian-like
attribution value distribution compared with others, meaning that the model exploits the weak modality
to learn the task. Through this case study, we demonstrate that the proposed WMA approach enables the
model to allocate more equitable attention to features across different modalities, rather than concentrating
on data from a single modality. Our proposed method effectively mitigates bias arising from amplified data
in multimodal systems.
6 Conclusion
This paper addresses a pivotal challenge in multimodal transformers: the absence of a modality alignment
approach during the fine-tuning phase. We introduce a Wasserstein distance-based regularization method to
adjust the modality alignment degree. The proposed method does not require training more parameters and
can be easily integrated into the multimodal transformer. The experimental results demonstrate significant
12Under review as submission to TMLR
improvements on performance and especially robustness on both 2-modalities and 3-modalities tasks, as
well as for early and late fusion paradigms. In early fusion paradigm, the averaged improvement of our
method compared with the best result from all baselines over all datasets are 1.4 %, 1.5%, 1.4%, 0.7%and
1.4%, respectively. For the late fusion paradigm, the improvements are 1.5 %, 1.8%, 1.3%, 2.3%and 1.9 %.
Meanwhile, our experimental results show that modality alignment needs to be task-dependent, rather than
forced alignment, i.e., minimizing the OT distance between modalities, which provides valuable insights for
related work. Our future work will focus on more theoretical analyses of our proposed method.
References
Tosin Adewumi, Lama Alkhaled, Namrata Gurung, Goya van Boven, and Irene Pagliai. Fairness and bias
in multimodal ai: A survey. arXiv preprint arXiv:2406.19097 , 2024.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,
Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for
few-shot learning. Advances in Neural Information Processing Systems , 35:23716–23736, 2022.
John Arevalo, Thamar Solorio, Manuel Montes-y Gómez, and Fabio A González. Gated multimodal units
for information fusion. arXiv preprint arXiv:1702.01992 , 2017.
Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias
Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelli-
gence, 2(11):665–673, 2020.
Morteza Ghahremani Boozandani and Christian Wachinger. Regbn: Batch normalization of multimodal
data with regularization. Advances in Neural Information Processing Systems , 36, 2024.
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin,
and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 15180–15190, 2023.
Fengjiao Gong, Yuzhou Nie, and Hongteng Xu. Gromov-wasserstein multi-modal alignment and clustering.
InProceedings of the 31st ACM International Conference on Information & Knowledge Management , pp.
603–613, 2022.
Md Kamrul Hasan, Wasifur Rahman, Amir Zadeh, Jianyuan Zhong, Md Iftekhar Tanveer, Louis-Philippe
Morency, et al. Ur-funny: A multimodal language dataset for understanding humor. arXiv preprint
arXiv:1904.06618 , 2019.
Nasir Hayat, Krzysztof J Geras, and Farah E Shamout. Medfuse: Multi-modal fusion with clinical time-series
data and chest x-ray images. In Machine Learning for Healthcare Conference , pp. 479–503. PMLR, 2022a.
Nasir Hayat, Krzysztof J. Geras, and Farah E. Shamout. Medfuse: Multi-modal fusion with clinical time-
series data and chest x-ray images, 2022b. URL https://arxiv.org/abs/2207.07027 .
Yu Huang, Chenzhuang Du, Zihui Xue, Xuanyao Chen, Hang Zhao, and Longbo Huang. What makes multi-
modal learning better than single (provably). Advances in Neural Information Processing Systems , 34:
10944–10956, 2021.
Md Mofijul Islam and Tariq Iqbal. Mumu: Cooperative multitask learning-based guided multimodal fusion.
InProceedings of the AAAI conference on artificial intelligence , volume 36, pp. 1043–1051, 2022.
Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and
Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. Advances
in neural information processing systems , 33:2611–2624, 2020.
Eun-Sol Kim, Woo Young Kang, Kyoung-Woon On, Yu-Jung Heo, and Byoung-Tak Zhang. Hypergraph
attention networks for multimodal learning. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pp. 14581–14590, 2020.
13Under review as submission to TMLR
Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or
region supervision. In International Conference on Machine Learning , pp. 5583–5594. PMLR, 2021.
Yi-Lun Lee, Yi-Hsuan Tsai, Wei-Chen Chiu, and Chen-Yu Lee. Multimodal prompting with missing modal-
ities for visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 14943–14952, 2023.
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong
Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances
in neural information processing systems , 34:9694–9705, 2021.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training
with frozen image encoders and large language models. In International conference on machine learning ,
pp. 19730–19742. PMLR, 2023.
Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Yufan Chen, Peter Wu,
Michelle A Lee, Yuke Zhu, et al. Multibench: Multiscale benchmarks for multimodal representation
learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks
Track (Round 1) , 2021.
Paul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard Chen, Zihao Deng, Nicholas
Allen, Randy Auerbach, Faisal Mahmood, et al. Quantifying & modeling multimodal interactions: An
information decomposition framework. Advances in Neural Information Processing Systems , 36, 2024.
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 26296–
26306, 2024.
Zhou Lu. A theory of multimodal learning. In Thirty-seventh Conference on Neural Information Processing
Systems, 2023. URL https://openreview.net/forum?id=7xlrdSOm3g .
Mengmeng Ma, Jian Ren, Long Zhao, Sergey Tulyakov, Cathy Wu, and Xi Peng. Smil: Multimodal learn-
ing with severely missing modality. In Proceedings of the AAAI Conference on Artificial Intelligence ,
volume 35, pp. 2302–2310, 2021.
Sijie Mai, Ying Zeng, and Haifeng Hu. Multimodal information bottleneck: Learning minimal sufficient
unimodal and multimodal representations. IEEE Transactions on Multimedia , 2022.
Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. Attention
bottlenecks for multimodal fusion. Advances in neural information processing systems , 34:14200–14213,
2021.
George Papandreou, Athanassios Katsamanis, Vassilis Pitsikalis, and Petros Maragos. Adaptive multimodal
fusion by uncertainty compensation with application to audiovisual speech recognition. IEEE Transactions
on Audio, Speech, and Language Processing , 17(3):423–435, 2009.
Gabriel Peyré, Marco Cuturi, et al. Computational optimal transport: With applications to data science.
Foundations and Trends ®in Machine Learning , 11(5-6):355–607, 2019.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In International conference on machine learning , pp. 8748–8763. PMLR,
2021.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In International
conference on machine learning , pp. 3319–3328. PMLR, 2017.
Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual
language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904 , 2021.
14Under review as submission to TMLR
Yujia Xie, Xiangfeng Wang, Ruijia Wang, and Hongyuan Zha. A fast proximal point method for computing
exact wasserstein distance. arXiv preprint arXiv:1802.04307 , 2018.
Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria, and Louis-Philippe Morency. Multi-
attentionrecurrentnetworkforhumancommunicationcomprehension. In Thirty-Second AAAI Conference
on Artificial Intelligence , 2018.
Zhuo Zhi, Ziquan Liu, Moe Elbadawi, Adam Daneshmend, Mine Orlu, Abdul Basit, Andreas Demosthenous,
and Miguel Rodrigues. Borrowing treasures from neighbors: In-context learning for multimodal learning
with missing modalities and data scarcity. arXiv preprint arXiv:2403.09428 , 2024.
15Under review as submission to TMLR
A More experimental results
We report all the searching results under various combination of αandβfor all datasets in Table 5-8 for
early fusion and 9-12 for late fusion.
Table 5: Results of different αandβon Hateful Memes dataset under early fusion. Bold indicates better or
equal performance than MT-early, and red font is the weight we select.
test case β= 0β= 0.1β= 0.2β= 0.4β= 0.6β= 0.8β= 1β= 1.2β= 1.4β= 1.6β= 1.8β= 2β= 4β= 10
α= 0.1normal 0.735 0.728 0.730 0.735 0.726 0.739 0.730 0.728 0.725 0.724 0.733 0.7210.732 0.714
image noise 0.573 0.564 0.560 0.565 0.557 0.553 0.565 0.555 0.570 0.568 0.564 0.582 0.565 0.568
text noise 0.686 0.682 0.686 0.686 0.676 0.683 0.679 0.683 0.669 0.665 0.681 0.668 0.683 0.661
image missing 0.611 0.618 0.614 0.618 0.625 0.631 0.627 0.621 0.618 0.617 0.612 0.602 0.606 0.649
text missing 0.671 0.665 0.666 0.668 0.657 0.664 0.664 0.666 0.655 0.654 0.657 0.6530.659 0.648
α= 0.2normal 0.726 0.731 0.732 0.731 0.738 0.735 0.736 0.732 0.736 0.740 0.735 0.728 0.717 0.721
image noise 0.571 0.572 0.561 0.554 0.553 0.563 0.568 0.563 0.564 0.571 0.5320.564 0.558 0.548
text noise 0.678 0.685 0.682 0.686 0.692 0.684 0.689 0.675 0.682 0.685 0.679 0.676 0.677 0.689
image missing 0.600 0.606 0.610 0.617 0.615 0.635 0.635 0.643 0.635 0.642 0.647 0.615 0.606 0.634
text missing 0.659 0.664 0.662 0.666 0.669 0.669 0.667 0.668 0.667 0.662 0.658 0.662 0.6480.654
α= 1normal 0.724 0.730 0.729 0.729 0.732 0.730 0.730 0.735 0.736 0.740 0.748 0.741 0.725 0.730
image noise 0.574 0.571 0.557 0.567 0.580 0.563 0.573 0.556 0.568 0.552 0.583 0.556 0.561 0.562
text noise 0.674 0.686 0.684 0.682 0.677 0.684 0.670 0.681 0.680 0.677 0.682 0.676 0.670 0.675
image missing 0.621 0.608 0.610 0.613 0.613 0.618 0.620 0.624 0.626 0.625 0.645 0.620 0.607 0.616
text missing 0.658 0.669 0.666 0.670 0.666 0.663 0.662 0.662 0.660 0.656 0.659 0.665 0.648 0.647
α= 5normal 0.728 0.721 0.727 0.733 0.732 0.734 0.738 0.733 0.735 0.721 0.741 0.737 0.731 0.728
image noise 0.577 0.578 0.575 0.561 0.571 0.574 0.569 0.546 0.564 0.562 0.575 0.559 0.577 0.546
text noise 0.670 0.664 0.663 0.680 0.686 0.689 0.676 0.681 0.684 0.675 0.689 0.673 0.6650.675
image missing 0.621 0.612 0.621 0.623 0.623 0.621 0.632 0.617 0.620 0.603 0.624 0.626 0.646 0.621
text missing 0.658 0.652 0.660 0.661 0.662 0.668 0.660 0.657 0.666 0.652 0.663 0.6520.655 0.667
α= 10normal 0.720 0.726 0.721 0.718 0.730 0.724 0.732 0.725 0.724 0.732 0.736 0.732 0.716 0.705
image noise 0.576 0.563 0.580 0.580 0.557 0.555 0.555 0.571 0.555 0.570 0.576 0.567 0.577 0.529
text noise 0.672 0.667 0.668 0.663 0.676 0.674 0.681 0.677 0.671 0.682 0.669 0.673 0.669 0.655
image missing 0.634 0.623 0.611 0.619 0.641 0.615 0.614 0.632 0.643 0.626 0.620 0.619 0.651 0.612
text missing 0.661 0.660 0.651 0.647 0.662 0.658 0.667 0.655 0.653 0.660 0.6520.659 0.658 0.636
Table 6: Results of different αandβon MM-IMDb dataset under early fusion. Bold indicates better or
equal performance than MT-early, and red font is the weight we select.
test case β= 0β= 0.1β= 0.2β= 0.4β= 0.6β= 0.8β= 1β= 1.2β= 1.4β= 1.6β= 1.8β= 2β= 4β= 10
α= 0.1normal 0.552 0.549 0.548 0.551 0.552 0.551 0.550 0.550 0.550 0.553 0.548 0.551 0.546 0.551
image noise 0.269 0.346 0.317 0.316 0.304 0.319 0.298 0.252 0.284 0.314 0.273 0.263 0.243 0.227
text noise 0.461 0.456 0.456 0.464 0.461 0.458 0.457 0.456 0.459 0.464 0.454 0.458 0.448 0.458
image missing 0.464 0.462 0.466 0.456 0.470 0.4630.469 0.472 0.454 0.477 0.4700.476 0.471 0.451
text missing 0.422 0.418 0.415 0.414 0.412 0.414 0.408 0.412 0.414 0.417 0.410 0.411 0.412 0.413
α= 0.2normal 0.549 0.552 0.552 0.550 0.550 0.551 0.550 0.551 0.548 0.551 0.548 0.550 0.550 0.548
image noise 0.326 0.329 0.271 0.292 0.321 0.2260.275 0.281 0.326 0.288 0.294 0.323 0.175 0.226
text noise 0.461 0.465 0.462 0.461 0.461 0.461 0.458 0.459 0.456 0.459 0.455 0.450 0.452 0.462
image missing 0.465 0.457 0.474 0.468 0.466 0.466 0.461 0.460 0.472 0.474 0.480 0.484 0.445 0.452
text missing 0.417 0.419 0.422 0.412 0.415 0.413 0.4120.418 0.412 0.417 0.410 0.407 0.406 0.421
α= 1normal 0.551 0.553 0.552 0.548 0.552 0.549 0.550 0.549 0.550 0.546 0.545 0.544 0.546 0.545
image noise 0.309 0.276 0.286 0.278 0.296 0.2370.329 0.247 0.270 0.269 0.272 0.235 0.227 0.291
text noise 0.462 0.459 0.458 0.458 0.462 0.459 0.457 0.457 0.459 0.449 0.445 0.449 0.456 0.452
image missing 0.453 0.444 0.453 0.454 0.470 0.466 0.482 0.469 0.461 0.480 0.472 0.466 0.459 0.444
text missing 0.415 0.415 0.415 0.409 0.421 0.415 0.416 0.410 0.415 0.412 0.413 0.4100.414 0.408
α= 5normal 0.555 0.558 0.553 0.553 0.548 0.550 0.551 0.549 0.545 0.544 0.545 0.545 0.544 0.544
image noise 0.263 0.287 0.226 0.279 0.276 0.335 0.344 0.257 0.250 0.278 0.271 0.2060.279 0.246
text noise 0.461 0.470 0.460 0.460 0.455 0.454 0.455 0.454 0.462 0.452 0.453 0.451 0.448 0.450
image missing 0.470 0.459 0.468 0.470 0.472 0.476 0.488 0.486 0.472 0.481 0.475 0.465 0.441 0.447
text missing 0.420 0.432 0.423 0.416 0.417 0.414 0.419 0.417 0.413 0.411 0.416 0.419 0.400 0.406
α= 10normal 0.555 0.563 0.553 0.554 0.554 0.548 0.552 0.551 0.548 0.544 0.548 0.550 0.549 0.540
image noise 0.282 0.355 0.254 0.332 0.243 0.239 0.238 0.250 0.219 0.298 0.206 0.222 0.284 0.215
text noise 0.469 0.471 0.463 0.461 0.464 0.457 0.456 0.462 0.461 0.453 0.451 0.453 0.456 0.433
image missing 0.473 0.477 0.481 0.473 0.469 0.469 0.474 0.470 0.470 0.476 0.473 0.479 0.469 0.435
text missing 0.430 0.437 0.429 0.421 0.420 0.4050.415 0.416 0.425 0.413 0.427 0.425 0.415 0.396
16Under review as submission to TMLR
Table 7: Results of different αandβon UR-FUNNY dataset under early fusion. Bold indicates better or
equal performance than MT-early, and red font is the weight we select.
test case β= 0β= 0.1β= 0.2β= 0.4β= 0.6β= 0.8β= 1β= 1.2β= 1.4β= 1.6β= 1.8β= 2β= 4β= 10
α= 0.1normal 0.702 0.701 0.701 0.700 0.698 0.700 0.702 0.704 0.704 0.699 0.699 0.699 0.698 0.697
image noise 0.646 0.648 0.643 0.649 0.647 0.654 0.652 0.653 0.652 0.651 0.648 0.654 0.656 0.657
text noise 0.680 0.682 0.681 0.681 0.680 0.679 0.684 0.683 0.686 0.681 0.680 0.6740.683 0.670
audio noise 0.630 0.633 0.630 0.635 0.630 0.633 0.626 0.631 0.633 0.629 0.627 0.635 0.643 0.634
image missing 0.608 0.608 0.609 0.606 0.609 0.608 0.613 0.614 0.614 0.608 0.612 0.597 0.611 0.601
text missing 0.673 0.682 0.684 0.669 0.674 0.6680.682 0.678 0.680 0.670 0.682 0.647 0.666 0.668
audio missing 0.700 0.699 0.697 0.696 0.694 0.697 0.697 0.699 0.701 0.696 0.695 0.691 0.696 0.692
α= 0.2normal 0.699 0.701 0.699 0.701 0.698 0.699 0.704 0.704 0.702 0.702 0.6990.702 0.705 0.700
image noise 0.644 0.648 0.653 0.653 0.653 0.648 0.6450.650 0.651 0.655 0.658 0.6430.646 0.662
text noise 0.680 0.682 0.678 0.683 0.683 0.684 0.681 0.683 0.680 0.6780.682 0.681 0.6720.681
audio noise 0.626 0.630 0.632 0.636 0.638 0.631 0.626 0.629 0.627 0.632 0.632 0.628 0.620 0.635
image missing 0.603 0.606 0.604 0.611 0.609 0.610 0.610 0.612 0.610 0.609 0.606 0.610 0.604 0.627
text missing 0.682 0.685 0.677 0.680 0.670 0.677 0.678 0.672 0.683 0.672 0.6690.675 0.6660.689
audio missing 0.695 0.698 0.695 0.697 0.695 0.695 0.699 0.699 0.697 0.699 0.6960.698 0.700 0.696
α= 1normal 0.705 0.700 0.702 0.699 0.699 0.701 0.701 0.695 0.702 0.705 0.702 0.705 0.704 0.704
image noise 0.661 0.665 0.653 0.648 0.656 0.651 0.649 0.644 0.638 0.660 0.649 0.657 0.666 0.671
text noise 0.679 0.683 0.679 0.681 0.679 0.683 0.679 0.676 0.680 0.667 0.686 0.688 0.6670.686
audio noise 0.633 0.643 0.634 0.635 0.627 0.634 0.631 0.629 0.634 0.636 0.6300.639 0.640 0.633
image missing 0.628 0.608 0.604 0.607 0.608 0.607 0.613 0.602 0.609 0.593 0.611 0.616 0.621 0.620
text missing 0.658 0.664 0.675 0.667 0.681 0.682 0.6530.676 0.675 0.582 0.671 0.680 0.631 0.647
audio missing 0.699 0.697 0.698 0.695 0.696 0.698 0.697 0.693 0.699 0.704 0.698 0.702 0.697 0.700
α= 5normal 0.704 0.695 0.699 0.697 0.637 0.706 0.702 0.705 0.701 0.702 0.712 0.6990.704 0.617
image noise 0.644 0.671 0.667 0.662 0.605 0.645 0.638 0.655 0.652 0.657 0.681 0.671 0.671 0.614
text noise 0.675 0.661 0.676 0.679 0.624 0.656 0.679 0.682 0.685 0.688 0.686 0.684 0.681 0.621
audio noise 0.631 0.635 0.641 0.644 0.616 0.623 0.624 0.637 0.633 0.632 0.643 0.637 0.634 0.598
image missing 0.604 0.606 0.611 0.608 0.555 0.619 0.604 0.609 0.620 0.623 0.624 0.6050.620 0.549
text missing 0.659 0.644 0.667 0.669 0.615 0.600 0.675 0.673 0.661 0.672 0.674 0.672 0.643 0.555
audio missing 0.703 0.693 0.696 0.695 0.618 0.702 0.697 0.702 0.698 0.698 0.703 0.6960.700 0.596
α= 10normal 0.698 0.704 0.698 0.699 0.701 0.700 0.696 0.703 0.699 0.701 0.704 0.696 0.698 0.709
image noise 0.665 0.675 0.667 0.653 0.669 0.659 0.642 0.662 0.668 0.671 0.666 0.671 0.668 0.663
text noise 0.676 0.681 0.685 0.666 0.675 0.679 0.680 0.683 0.683 0.690 0.683 0.682 0.674 0.664
audio noise 0.637 0.649 0.646 0.631 0.645 0.647 0.623 0.635 0.635 0.638 0.637 0.6340.661 0.620
image missing 0.605 0.629 0.613 0.611 0.604 0.618 0.6000.643 0.621 0.619 0.6120.626 0.639 0.602
text missing 0.659 0.667 0.673 0.652 0.657 0.665 0.675 0.662 0.658 0.692 0.651 0.662 0.626 0.646
audio missing 0.695 0.699 0.696 0.695 0.699 0.697 0.6930.699 0.696 0.696 0.700 0.692 0.687 0.706
17Under review as submission to TMLR
Table 8: Results of different αandβon MOSEI dataset under early fusion. Bold indicates better or equal
performance than MT-early, and red font is the weight we select.
test case β= 0β= 0.1β= 0.2β= 0.4β= 0.6β= 0.8β= 1β= 1.2β= 1.4β= 1.6β= 1.8β= 2β= 4β= 10
α= 0.1normal 0.668 0.628 0.647 0.627 0.624 0.626 0.639 0.647 0.622 0.654 0.635 0.634 0.635 0.769
image noise 0.725 0.697 0.688 0.700 0.678 0.725 0.709 0.731 0.741 0.708 0.724 0.739 0.737 0.784
text noise 0.849 0.863 0.844 0.813 0.828 0.832 0.833 0.839 0.822 0.857 0.826 0.825 0.827 0.903
audio noise 0.691 0.678 0.674 0.665 0.653 0.689 0.696 0.693 0.678 0.685 0.679 0.692 0.691 0.769
image missing 0.678 0.635 0.653 0.642 0.629 0.635 0.648 0.652 0.632 0.655 0.646 0.646 0.644 0.771
text missing 0.829 0.835 0.828 0.813 0.818 0.825 0.821 0.825 0.819 0.838 0.821 0.818 0.827 0.872
audio missing 0.669 0.630 0.651 0.628 0.628 0.623 0.640 0.649 0.624 0.653 0.638 0.633 0.650 0.792
α= 0.2normal 0.624 0.628 0.634 0.643 0.630 0.626 0.622 0.628 0.657 0.636 0.623 0.629 0.652 0.635
image noise 0.722 0.686 0.691 0.692 0.699 0.735 0.703 0.680 0.737 0.741 0.720 0.709 0.721 0.745
text noise 0.834 0.834 0.830 0.852 0.824 0.823 0.847 0.850 0.912 0.830 0.816 0.811 0.8470.830
audio noise 0.686 0.654 0.681 0.674 0.679 0.687 0.691 0.695 0.731 0.692 0.657 0.668 0.687 0.695
image missing 0.634 0.635 0.637 0.653 0.632 0.635 0.633 0.633 0.657 0.645 0.636 0.641 0.6560.644
text missing 0.822 0.821 0.817 0.830 0.820 0.817 0.832 0.827 0.868 0.821 0.820 0.821 0.8270.822
audio missing 0.628 0.649 0.640 0.650 0.629 0.627 0.628 0.636 0.656 0.641 0.634 0.638 0.668 0.652
α= 1normal 0.624 0.689 0.628 0.615 0.630 0.833 0.632 0.640 0.621 0.642 0.642 0.650 0.653 0.829
image noise 0.711 0.740 0.732 0.721 0.698 0.838 0.724 0.707 0.710 0.733 0.715 0.755 0.688 0.834
text noise 0.855 0.833 0.847 0.827 0.845 0.836 0.827 0.838 0.821 0.822 0.821 0.869 0.843 0.836
audio noise 0.676 0.710 0.684 0.662 0.655 0.839 0.679 0.664 0.671 0.664 0.679 0.712 0.676 0.834
image missing 0.629 0.694 0.635 0.621 0.640 0.8330.640 0.647 0.627 0.655 0.648 0.658 0.656 0.830
text missing 0.831 0.819 0.832 0.817 0.826 0.835 0.817 0.824 0.820 0.819 0.816 0.841 0.876 0.835
audio missing 0.623 0.690 0.627 0.621 0.637 0.837 0.634 0.646 0.634 0.646 0.658 0.649 0.718 0.990
α= 5normal 0.632 0.658 0.615 0.631 0.627 0.624 0.6430.624 0.627 0.658 0.640 0.642 0.633 0.832
image noise 0.743 0.710 0.676 0.710 0.691 0.687 0.697 0.714 0.707 0.716 0.743 0.736 0.686 0.834
text noise 0.818 0.865 0.824 0.835 0.849 0.841 0.839 0.837 0.852 0.860 0.871 0.855 0.823 0.834
audio noise 0.685 0.685 0.664 0.676 0.669 0.666 0.663 0.675 0.678 0.680 0.689 0.695 0.664 0.834
image missing 0.643 0.660 0.619 0.636 0.635 0.635 0.6530.631 0.637 0.667 0.652 0.652 0.643 0.833
text missing 0.812 0.842 0.813 0.821 0.829 0.824 0.8260.824 0.830 0.836 0.844 0.871 0.910 0.834
audio missing 0.630 0.654 0.623 0.639 0.641 0.629 0.6460.633 0.628 0.656 0.641 0.643 0.897 0.913
α= 10normal 0.629 0.626 0.655 0.619 0.627 0.627 0.6340.631 0.622 0.650 0.632 0.621 0.648 0.832
image noise 0.707 0.717 0.718 0.728 0.740 0.735 0.700 0.720 0.698 0.715 0.717 0.703 0.705 0.833
text noise 0.834 0.815 0.860 0.825 0.816 0.824 0.8400.820 0.829 0.911 0.834 0.831 0.823 0.833
audio noise 0.665 0.661 0.674 0.667 0.696 0.692 0.681 0.674 0.680 0.674 0.699 0.667 0.682 0.833
image missing 0.632 0.629 0.661 0.626 0.631 0.643 0.634 0.636 0.626 0.657 0.639 0.625 0.665 0.832
text missing 0.822 0.809 0.838 0.814 0.812 0.817 0.8280.816 0.820 0.869 0.829 0.838 0.904 0.832
audio missing 0.628 0.626 0.665 0.628 0.626 0.641 0.647 0.637 0.631 0.650 0.634 0.624 1.029 0.834
Table 9: Results of different αandβon Hateful Memes dataset under late fusion. Bold indicates better or
equal performance than MT-early, and red font is the weight we select.
test case β= 0β= 0.1β= 0.2β= 0.4β= 0.6β= 0.8β= 1β= 1.2β= 1.4β= 1.6β= 1.8β= 2β= 4β= 10
α= 0.1normal 0.685 0.724 0.725 0.722 0.722 0.723 0.680 0.680 0.681 0.710 0.707 0.721 0.709 0.686
image noise 0.661 0.666 0.666 0.663 0.662 0.662 0.658 0.658 0.659 0.661 0.659 0.672 0.654 0.665
text noise 0.636 0.670 0.672 0.670 0.671 0.671 0.633 0.632 0.633 0.654 0.663 0.668 0.652 0.635
image missing 0.674 0.680 0.682 0.680 0.674 0.687 0.671 0.672 0.672 0.687 0.685 0.692 0.678 0.673
text missing 0.583 0.636 0.636 0.633 0.632 0.610 0.581 0.579 0.580 0.594 0.594 0.614 0.606 0.576
α= 0.2normal 0.685 0.684 0.684 0.685 0.724 0.723 0.680 0.681 0.708 0.722 0.723 0.721 0.707 0.715
image noise 0.647 0.660 0.660 0.661 0.665 0.656 0.658 0.659 0.660 0.672 0.674 0.675 0.664 0.674
text noise 0.650 0.635 0.636 0.636 0.672 0.671 0.633 0.633 0.658 0.670 0.669 0.666 0.658 0.661
image missing 0.648 0.673 0.673 0.674 0.684 0.683 0.671 0.671 0.680 0.697 0.696 0.694 0.681 0.683
text missing 0.607 0.583 0.583 0.583 0.629 0.630 0.580 0.581 0.580 0.606 0.618 0.609 0.622 0.609
α= 1normal 0.719 0.712 0.717 0.722 0.684 0.680 0.726 0.723 0.716 0.718 0.688 0.722 0.589 0.594
image noise 0.664 0.652 0.657 0.665 0.661 0.659 0.671 0.672 0.669 0.663 0.664 0.675 0.569 0.571
text noise 0.666 0.662 0.667 0.673 0.635 0.633 0.675 0.670 0.662 0.670 0.637 0.664 0.567 0.574
image missing 0.678 0.675 0.673 0.680 0.674 0.672 0.697 0.694 0.693 0.684 0.6680.684 0.577 0.577
text missing 0.632 0.609 0.625 0.623 0.582 0.580 0.613 0.623 0.604 0.616 0.584 0.624 0.562 0.567
α= 5normal 0.716 0.717 0.717 0.713 0.717 0.712 0.709 0.722 0.723 0.719 0.676 0.589 0.599 0.601
image noise 0.659 0.671 0.674 0.661 0.659 0.664 0.660 0.676 0.666 0.656 0.659 0.570 0.569 0.560
text noise 0.658 0.659 0.660 0.658 0.663 0.669 0.661 0.666 0.664 0.664 0.626 0.567 0.580 0.584
image missing 0.678 0.691 0.695 0.680 0.684 0.687 0.685 0.695 0.678 0.685 0.667 0.577 0.570 0.549
text missing 0.626 0.626 0.631 0.614 0.617 0.599 0.603 0.615 0.597 0.625 0.583 0.562 0.575 0.581
α= 10normal 0.724 0.728 0.720 0.717 0.681 0.681 0.711 0.721 0.715 0.713 0.590 0.591 0.601 0.597
image noise 0.662 0.671 0.660 0.671 0.660 0.661 0.660 0.676 0.657 0.654 0.570 0.570 0.564 0.551
text noise 0.663 0.667 0.658 0.660 0.632 0.634 0.664 0.665 0.662 0.665 0.568 0.570 0.583 0.582
image missing 0.708 0.692 0.682 0.694 0.674 0.671 0.680 0.692 0.680 0.675 0.577 0.578 0.556 0.537
text missing 0.646 0.663 0.632 0.626 0.584 0.583 0.606 0.620 0.626 0.625 0.564 0.565 0.580 0.578
18Under review as submission to TMLR
Table 10: Results of different αandβon MM-IMDb dataset under late fusion. Bold indicates better or
equal performance than MT-early, and red font is the weight we select.
test case β= 0β= 0.1β= 0.2β= 0.4β= 0.6β= 0.8β= 1β= 1.2β= 1.4β= 1.6β= 1.8β= 2β= 4β= 10
α= 0.1normal 0.603 0.603 0.602 0.603 0.602 0.602 0.602 0.601 0.601 0.601 0.601 0.601 0.602 0.600
image noise 0.470 0.470 0.470 0.471 0.473 0.474 0.474 0.473 0.471 0.469 0.468 0.466 0.450 0.427
text noise 0.498 0.496 0.495 0.496 0.495 0.495 0.494 0.494 0.494 0.495 0.495 0.495 0.497 0.493
image missing 0.557 0.557 0.557 0.557 0.558 0.559 0.557 0.554 0.554 0.554 0.554 0.554 0.556 0.553
text missing 0.415 0.416 0.417 0.416 0.418 0.419 0.419 0.417 0.417 0.417 0.417 0.417 0.417 0.411
α= 0.2normal 0.603 0.603 0.603 0.603 0.600 0.602 0.602 0.601 0.602 0.600 0.600 0.601 0.601 0.599
image noise 0.472 0.470 0.472 0.471 0.471 0.474 0.475 0.472 0.469 0.467 0.464 0.462 0.438 0.428
text noise 0.498 0.497 0.498 0.495 0.492 0.494 0.494 0.495 0.495 0.496 0.496 0.496 0.496 0.486
image missing 0.555 0.555 0.556 0.557 0.555 0.558 0.558 0.554 0.554 0.553 0.553 0.553 0.560 0.556
text missing 0.416 0.413 0.416 0.417 0.416 0.419 0.419 0.417 0.418 0.416 0.417 0.417 0.417 0.420
α= 1normal 0.609 0.606 0.605 0.603 0.601 0.602 0.602 0.600 0.602 0.603 0.602 0.602 0.601 0.598
image noise 0.463 0.470 0.472 0.472 0.472 0.473 0.474 0.469 0.464 0.465 0.466 0.461 0.441 0.422
text noise 0.506 0.505 0.502 0.499 0.494 0.494 0.496 0.495 0.497 0.498 0.497 0.496 0.486 0.477
image missing 0.554 0.554 0.554 0.556 0.554 0.557 0.554 0.552 0.552 0.554 0.554 0.553 0.549 0.531
text missing 0.415 0.417 0.418 0.417 0.416 0.418 0.417 0.417 0.418 0.419 0.418 0.416 0.423 0.423
α= 5normal 0.602 0.610 0.609 0.605 0.600 0.601 0.601 0.599 0.600 0.599 0.599 0.595 0.592 0.591
image noise 0.411 0.489 0.463 0.474 0.474 0.475 0.473 0.465 0.461 0.463 0.455 0.442 0.414 0.400
text noise 0.482 0.510 0.509 0.502 0.496 0.493 0.495 0.496 0.497 0.496 0.494 0.493 0.475 0.464
image missing 0.555 0.559 0.556 0.553 0.552 0.555 0.554 0.551 0.550 0.551 0.550 0.542 0.515 0.507
text missing 0.397 0.426 0.419 0.420 0.416 0.416 0.416 0.416 0.415 0.413 0.412 0.404 0.411 0.413
α= 10normal 0.600 0.609 0.612 0.604 0.599 0.600 0.600 0.599 0.598 0.598 0.594 0.594 0.592 0.589
image noise 0.449 0.432 0.459 0.474 0.473 0.475 0.471 0.464 0.461 0.458 0.446 0.435 0.406 0.405
text noise 0.488 0.501 0.508 0.502 0.497 0.494 0.495 0.496 0.496 0.492 0.491 0.485 0.468 0.460
image missing 0.553 0.556 0.555 0.552 0.551 0.554 0.553 0.551 0.549 0.549 0.538 0.540 0.510 0.507
text missing 0.393 0.414 0.422 0.420 0.416 0.4140.416 0.415 0.413 0.411 0.401 0.408 0.413 0.410
Table 11: Results of different αandβon UR-FUNNY dataset under late fusion. Bold indicates better or
equal performance than MT-early, and red font is the weight we select.
test case β= 0β= 0.1β= 0.2β= 0.4β= 0.6β= 0.8β= 1β= 1.2β= 1.4β= 1.6β= 1.8β= 2β= 4β= 10
α= 0.1normal 0.631 0.623 0.590 0.701 0.701 0.701 0.701 0.701 0.702 0.702 0.700 0.699 0.702 0.696
image noise 0.506 0.506 0.497 0.671 0.663 0.655 0.652 0.644 0.632 0.619 0.611 0.607 0.578 0.578
text noise 0.635 0.626 0.588 0.695 0.695 0.693 0.692 0.691 0.689 0.686 0.686 0.690 0.686 0.682
audio noise 0.629 0.622 0.586 0.700 0.700 0.699 0.699 0.700 0.700 0.699 0.697 0.694 0.693 0.689
image missing 0.551 0.548 0.540 0.655 0.656 0.657 0.658 0.658 0.654 0.650 0.651 0.644 0.646 0.637
text missing 0.614 0.606 0.575 0.652 0.653 0.646 0.647 0.649 0.651 0.649 0.647 0.657 0.672 0.668
audio missing 0.601 0.595 0.579 0.684 0.683 0.680 0.680 0.680 0.678 0.676 0.677 0.674 0.659 0.654
α= 0.2normal 0.592 0.618 0.634 0.700 0.710 0.701 0.701 0.701 0.702 0.700 0.701 0.702 0.696 0.630
image noise 0.495 0.504 0.505 0.682 0.660 0.654 0.651 0.644 0.633 0.620 0.606 0.603 0.579 0.513
text noise 0.592 0.620 0.637 0.696 0.698 0.693 0.692 0.691 0.688 0.686 0.692 0.690 0.682 0.634
audio noise 0.585 0.614 0.630 0.701 0.702 0.699 0.700 0.699 0.699 0.698 0.696 0.694 0.689 0.631
image missing 0.541 0.551 0.554 0.648 0.660 0.657 0.657 0.659 0.656 0.647 0.644 0.644 0.638 0.549
text missing 0.580 0.612 0.625 0.658 0.672 0.647 0.648 0.649 0.653 0.647 0.661 0.660 0.667 0.624
audio missing 0.586 0.607 0.611 0.684 0.683 0.680 0.680 0.679 0.677 0.676 0.675 0.673 0.652 0.606
α= 1normal 0.604 0.617 0.628 0.616 0.701 0.703 0.700 0.704 0.700 0.702 0.706 0.702 0.603 0.596
image noise 0.492 0.492 0.492 0.493 0.660 0.655 0.651 0.644 0.632 0.619 0.604 0.590 0.510 0.508
text noise 0.611 0.624 0.637 0.621 0.694 0.693 0.695 0.690 0.687 0.693 0.693 0.687 0.603 0.596
audio noise 0.595 0.607 0.615 0.607 0.701 0.700 0.700 0.701 0.697 0.698 0.700 0.696 0.600 0.590
image missing 0.542 0.546 0.549 0.547 0.655 0.655 0.656 0.655 0.643 0.641 0.640 0.643 0.544 0.542
text missing 0.588 0.597 0.606 0.604 0.648 0.648 0.649 0.654 0.651 0.661 0.666 0.671 0.597 0.588
audio missing 0.594 0.604 0.614 0.615 0.680 0.678 0.681 0.677 0.679 0.675 0.666 0.656 0.605 0.597
α= 5normal 0.613 0.602 0.595 0.699 0.701 0.706 0.701 0.701 0.704 0.706 0.635 0.636 0.591 0.590
image noise 0.490 0.492 0.493 0.680 0.655 0.641 0.649 0.626 0.632 0.598 0.516 0.515 0.507 0.507
text noise 0.621 0.609 0.601 0.692 0.688 0.696 0.693 0.683 0.694 0.693 0.638 0.638 0.592 0.591
audio noise 0.605 0.594 0.587 0.699 0.700 0.703 0.701 0.697 0.700 0.700 0.629 0.631 0.584 0.582
image missing 0.554 0.550 0.545 0.637 0.652 0.658 0.654 0.651 0.651 0.637 0.546 0.547 0.540 0.539
text missing 0.606 0.596 0.587 0.630 0.631 0.650 0.645 0.638 0.667 0.665 0.614 0.620 0.583 0.583
audio missing 0.608 0.599 0.587 0.681 0.670 0.676 0.678 0.674 0.668 0.650 0.601 0.603 0.593 0.593
α= 10normal 0.586 0.594 0.616 0.595 0.702 0.702 0.702 0.704 0.704 0.638 0.631 0.597 0.591 0.590
image noise 0.487 0.488 0.490 0.496 0.649 0.626 0.647 0.621 0.621 0.502 0.515 0.508 0.506 0.506
text noise 0.592 0.599 0.624 0.601 0.686 0.685 0.694 0.683 0.692 0.644 0.634 0.597 0.591 0.591
audio noise 0.578 0.586 0.605 0.587 0.703 0.697 0.699 0.699 0.702 0.623 0.632 0.592 0.582 0.582
image missing 0.533 0.541 0.547 0.541 0.651 0.656 0.664 0.649 0.647 0.553 0.545 0.542 0.539 0.539
text missing 0.581 0.585 0.604 0.588 0.626 0.634 0.649 0.644 0.659 0.609 0.620 0.588 0.583 0.583
audio missing 0.582 0.588 0.606 0.586 0.665 0.671 0.682 0.668 0.658 0.631 0.602 0.593 0.593 0.593
19Under review as submission to TMLR
Table 12: Results of different αandβon MOSEI dataset under late fusion. Bold indicates better or equal
performance than MT-early, and red font is the weight we select.
test case β= 0β= 0.1β= 0.2β= 0.4β= 0.6β= 0.8β= 1β= 1.2β= 1.4β= 1.6β= 1.8β= 2β= 4β= 10
α= 0.1normal 0.748 0.624 0.652 0.780 0.702 0.617 0.628 0.777 0.724 0.634 0.621 0.778 0.740 0.620
image noise 0.775 0.642 0.673 0.874 0.718 0.649 0.663 0.876 0.764 0.655 0.663 0.807 0.819 0.715
text noise 0.811 0.823 0.814 0.843 0.819 0.8400.810 0.849 0.820 0.831 0.819 0.812 1.0070.809
audio noise 0.751 0.627 0.655 0.775 0.704 0.618 0.633 0.772 0.723 0.640 0.623 0.780 0.739 0.622
image missing 0.748 0.626 0.658 0.797 0.708 0.619 0.632 0.797 0.728 0.639 0.629 0.781 0.741 0.632
text missing 0.812 0.814 0.812 0.848 0.820 0.8320.810 0.856 0.825 0.827 0.817 0.813 1.0110.810
audio missing 0.753 0.625 0.651 0.796 0.707 0.620 0.627 0.795 0.725 0.632 0.623 0.773 0.742 0.624
α= 0.2normal 0.653 0.630 0.793 0.781 0.721 0.730 0.712 0.629 0.630 0.706 0.645 0.617 0.625 0.628
image noise 0.686 0.659 0.855 0.875 0.727 0.742 0.746 0.668 0.686 0.724 0.688 0.659 0.667 0.771
text noise 0.825 0.818 0.864 0.837 0.852 0.831 0.884 0.824 0.818 0.816 0.807 0.8270.821 0.817
audio noise 0.655 0.635 0.793 0.777 0.720 0.732 0.710 0.632 0.634 0.707 0.649 0.618 0.625 0.632
image missing 0.661 0.635 0.797 0.800 0.719 0.736 0.711 0.633 0.627 0.708 0.656 0.622 0.627 0.628
text missing 0.826 0.817 0.865 0.843 0.864 0.840 0.903 0.818 0.817 0.819 0.808 0.814 0.817 0.816
audio missing 0.656 0.629 0.808 0.799 0.727 0.729 0.718 0.637 0.632 0.711 0.643 0.622 0.628 0.628
α= 1normal 0.626 0.762 0.635 0.670 0.640 0.640 0.619 0.629 0.608 0.624 0.625 0.6430.635 0.781
image noise 0.675 0.786 0.663 0.684 0.672 0.660 0.673 0.667 0.643 0.665 0.658 0.6660.655 0.815
text noise 0.816 0.823 0.807 0.814 0.819 0.8350.815 0.815 0.807 0.818 0.822 0.8290.820 0.808
audio noise 0.629 0.764 0.637 0.674 0.641 0.6440.622 0.631 0.615 0.623 0.630 0.6470.640 0.781
image missing 0.626 0.765 0.640 0.672 0.643 0.639 0.623 0.629 0.612 0.629 0.625 0.6450.635 0.789
text missing 0.816 0.826 0.807 0.814 0.817 0.8280.814 0.809 0.801 0.812 0.820 0.8250.819 0.809
audio missing 0.633 0.759 0.638 0.671 0.642 0.642 0.623 0.638 0.609 0.627 0.628 0.6420.631 0.790
α= 5normal 0.775 0.832 0.621 0.625 0.621 0.625 0.708 0.643 0.720 0.635 0.707 0.718 0.790 0.810
image noise 0.779 0.852 0.670 0.662 0.677 0.648 0.732 0.678 0.816 0.666 0.721 0.754 0.842 0.832
text noise 0.849 0.833 0.810 0.821 0.813 0.814 0.812 0.852 1.006 0.815 0.819 0.806 0.809 0.811
audio noise 0.777 0.833 0.626 0.627 0.621 0.628 0.711 0.645 0.723 0.641 0.714 0.720 0.792 0.812
image missing 0.774 0.835 0.627 0.635 0.634 0.631 0.718 0.646 0.721 0.642 0.708 0.718 0.798 0.820
text missing 0.851 0.833 0.811 0.821 0.811 0.813 0.811 0.849 1.013 0.812 0.817 0.809 0.809 0.811
audio missing 0.782 0.862 0.621 0.628 0.632 0.626 0.713 0.646 0.728 0.632 0.708 0.720 0.794 0.814
α= 10normal 0.840 0.841 0.628 0.731 0.645 0.742 0.628 0.643 0.629 0.640 0.616 0.712 0.746 0.817
image noise 0.841 0.840 0.672 0.749 0.670 0.789 0.647 0.668 0.675 0.672 0.671 0.742 0.879 0.834
text noise 0.839 0.840 0.809 0.826 0.811 0.868 0.829 0.828 0.813 0.815 0.812 0.806 0.813 0.817
audio noise 0.840 0.842 0.631 0.736 0.646 0.745 0.631 0.648 0.635 0.647 0.618 0.713 0.744 0.819
image missing 0.855 0.856 0.632 0.738 0.647 0.742 0.626 0.642 0.631 0.637 0.621 0.716 0.760 0.825
text missing 0.839 0.840 0.811 0.838 0.814 0.8800.820 0.824 0.812 0.814 0.811 0.809 0.818 0.817
audio missing 0.842 0.846 0.631 0.731 0.647 0.750 0.629 0.643 0.629 0.640 0.618 0.716 0.749 0.820
Table 13: Results of our proposed method with minimizing OT distance on all datasets under early fusion.
The bold numbers mean the best performance. The bigger AUROC and F1 and smaller MAE refer to better
performance.
normal test test with modality noise test with missing modality
Datasets Metric Methods normal image/video noise text noise audio noise image/video missing text missing audio missing average
Hateful Memes AUROC↑minimize OT
WMA0.724
0.7480.574
0.5830.674
0.682-
-0.621
0.6450.658
0.659-
-0.650
0.663
MM-IMDb F1↑minimize OT
WMA0.551
0.5630.309
0.3550.462
0.471-
-0.453
0.4770.415
0.437-
-0.438
0.461
UR-FUNNY AUROC↑minimize OT
WMA0.705
0.7080.661
0.6810.679
0.6860.633
0.6430.628
0.6240.658
0.6740.699
0.7030.666
0.674
MOSEI MAE↓minimize OT
WMA0.624
0.6150.711
0.6760.855
0.8240.676
0.6640.629
0.6190.831
0.8130.623
0.6230.707
0.691
Table 14: Results of our proposed method with different distance metrics on all datasets under late fusion.
The bold numbers mean the best performance. The bigger AUROC and F1 and smaller MAE refer to better
performance.
normal test test with modality noise test with missing modality
Datasets Metric Methods normal image/video noise text noise audio noise image/video missing text missing audio missing average
Hateful Memes AUROC↑minimize OT
WMA0.719
0.7280.664
0.6710.666
0.667-
-0.678
0.6920.632
0.663-
-0.672
0.684
MM-IMDb F1↑minimize OT
WMA0.609
0.6100.463
0.4890.506
0.510-
-0.554
0.5590.415
0.426-
-0.509
0.519
UR-FUNNY AUROC↑minimize OT
WMA0.604
0.7100.492
0.6600.611
0.6980.595
0.7020.542
0.6600.588
0.6720.594
0.6830.575
0.684
MOSEI MAE↓minimize OT
WMA0.626
0.6080.675
0.6430.816
0.8070.629
0.6150.626
0.6120.816
0.8010.633
0.6090.689
0.671
20Under review as submission to TMLR
Table 15: Results of our proposed method with different distance metrics on all datasets and test cases for
MT-early. The bold numbers mean the best performance. The bigger AUROC and F1 and smaller MAE
refer to better performance.
normal test test with modality noise test with missing modality
Datasets Metric Methods normal image/video noise text noise audio noise image/video missing text missing audio missing average
Hateful Memes AUROC↑MT-early
MT-early-CS
MT-early-JS
MT-early-TV
MT-early-WMA0.730
0.742
0.743
0.750
0.7480.547
0.575
0.560
0.563
0.5830.666
0.674
0.673
0.680
0.682-
-
-
-
-0.626
0.656
0.635
0.615
0.6450.653
0.672
0.668
0.661
0.659-
-
-
-
-0.644
0.664
0.656
0.654
0.663
MM-IMDb F1↑MT-early
MT-early-CS
MT-early-JS
MT-early-TV
MT-early-WMA0.551
0.559
0.558
0.565
0.5630.251
0.342
0.336
0.308
0.3550.462
0.464
0.470
0.462
0.471-
-
-
-
-0.464
0.473
0.476
0.480
0.4770.412
0.417
0.417
0.414
0.437-
-
-
-
-0.428
0.451
0.451
0.446
0.461
UR-FUNNY AUROC↑MT-early
MT-early-CS
MT-early-JS
MT-early-TV
MT-early-WMA0.700
0.711
0.711
0.708
0.7120.645
0.656
0.660
0.665
0.6810.678
0.682
0.664
0.685
0.6860.635
0.624
0.639
0.630
0.6430.612
0.617
0.594
0.622
0.6240.670
0.679
0.663
0.680
0.6740.696
0.708
0.705
0.704
0.7030.662
0.668
0.662
0.671
0.675
MOSEI MAE↓MT-early
MT-early-CS
MT-early-JS
MT-early-TV
MT-early-WMA0.633
0.624
0.617
0.612
0.6150.691
0.677
0.680
0.723
0.6760.834
0.825
0.822
0.821
0.8240.672
0.671
0.665
0.668
0.6640.651
0.631
0.642
0.632
0.6190.826
0.821
0.814
0.822
0.8130.635
0.633
0.632
0.635
0.6230.706
0.697
0.696
0.702
0.691
Table 16: Results of our proposed method with different distance metrics on all datasets and test cases for
MT-late. The bold numbers mean the best performance. The bigger AUROC and F1 and smaller MAE
refer to better performance.
normal test test with modality noise test with missing modality
Datasets Metric Methods normal image/video noise text noise audio noise image/video missing text missing audio missing average
Hateful Memes AUROC↑MT-late
MT-late-CS
MT-late-JS
MT-late-TV
MT-late-WMA0.718
0.728
0.724
0.724
0.7280.652
0.675
0.666
0.665
0.6710.666
0.679
0.674
0.671
0.667-
-
-
-
-0.680
0.687
0.664
0.680
0.6920.622
0.621
0.639
0.633
0.663-
-
-
-
-0.668
0.678
0.673
0.675
0.684
MM-IMDb F1↑MT-late
MT-late-CS
MT-late-JS
MT-late-TV
MT-late-WMA0.602
0.612
0.608
0.609
0.6100.469
0.459
0.475
0.470
0.4890.488
0.478
0.500
0.490
0.510-
-
-
-
-0.553
0.555
0.550
0.549
0.5590.414
0.418
0.421
0.419
0.426-
-
-
-
-0.505
0.504
0.511
0.507
0.519
UR-FUNNY AUROC↑MT-late
MT-late-CS
MT-late-JS
MT-late-TV
MT-late-WMA0.700
0.702
0.710
0.708
0.7100.651
0.661
0.654
0.654
0.6600.686
0.695
0.692
0.693
0.6980.693
0.690
0.698
0.691
0.7020.654
0.655
0.640
0.651
0.6600.641
0.650
0.662
0.656
0.6720.676
0.680
0.677
0.678
0.6830.672
0.676
0.676
0.676
0.684
MOSEI MAE↓MT-late
MT-late-CS
MT-late-JS
MT-late-TV
MT-late-WMA0.638
0.612
0.613
0.613
0.6080.665
0.658
0.703
0.645
0.6430.826
0.822
0.811
0.809
0.8070.642
0.614
0.615
0.615
0.6150.638
0.621
0.620
0.619
0.6120.822
0.821
0.814
0.809
0.8010.640
0.616
0.618
0.620
0.6090.696
0.681
0.685
0.676
0.671
21