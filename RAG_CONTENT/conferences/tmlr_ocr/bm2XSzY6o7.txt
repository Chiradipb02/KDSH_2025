Under review as submission to TMLR
Identifying latent distances with Finslerian geometry
Anonymous authors
Paper under double-blind review
Abstract
Riemannian geometry provides powerful tools to explore the latent space of generative mod-
els while preserving the inherent structure of the data. Distance and volume measures can
be computed from a Riemannian metric deﬁned by pulling back the Euclidean metric from
the data to the latent manifold. With this in mind, most generative models are stochastic,
and so is the pullback metric. Yet, manipulating stochastic objects is at best impractical,
and at worst unachievable. To perform operations such as interpolations, or measuring the
distance between data points, we need a deterministic approximation of the pullback met-
ric. In this work, we deﬁne a new metric as the expected length derived from the stochastic
pullback metric. We show this metric deﬁnes a Finsler metric. We compare it with the
expected pullback metric. We show that in high dimensions, the metrics converge to each
other at a rate of O/parenleftbig1
D/parenrightbig
.
1 Introduction
Generative models provide a convenient way to learn low-dimensional latent variables zcorresponding to
data observations xthrough a smooth function f:Z ⊂Rq→ X ⊂ RD, such that x=f(z). In practice, this
function can be a Gaussian Process Latent Variable Model (GPLVM) (Lawrence, 2003) or similar.
Through this learnt manifold, one can generate new data or compare observations by interpolating or com-
puting distances. However, doing so by using the Euclidean distance in the latent space is misleading
(Hauberg, 2018a). If our observations are lying near a manifold (Feﬀerman et al., 2016), we want to equip
our latent space with a metric that preserves distance measures on it. Figure 1 (left panel) illustrates the
need for deﬁning geometric-aware distances on manifolds.
Distances on a manifold can be precisely deﬁned using a norm, which is a mathematical function that exhibits
several desirable properties such as non-negativity, homogeneity, and the triangle inequality. In particular,
a norm can be induced by an inner product (i.e., a quadratic function) that associates each pair of points
on the manifold with a scalar value.
Let us compute the inﬁnitesimal Euclidean norm in our data space. Using the Taylor expansion, we have:/vextenddouble/vextenddoublef(z+ ∆z)−f(z)/vextenddouble/vextenddouble2
2≈/vextenddouble/vextenddoublef(z) +J(z)∆z−f(z)/vextenddouble/vextenddouble2
2= ∆z/latticetopJ(z)/latticetopJ(z)∆z. As a ﬁrst approximation, the norm
deﬁned in the latent space locally preserves the Euclidean norm deﬁned in the data space. The curvature of
our data manifold is condensed in the Riemannian metric tensor Gz=J/latticetop(z)J(z), which serves as a proxy
to deﬁne the Riemannian metric: gz:(u, v)→u/latticetopGzv. In mathematical jargon, we say that the Riemannian
manifold ( Z,g) is obtained by pulling back the Euclidean metric through the map f.
Riemannian geometry enables the exploration of the latent space in precise geometric terms, and quantities of
interest such as the length, the energy or the volume can be directly derived from the pullback metric. These
geometric quantities are, by construction, known to be invariant to reparametrizations of the latent space Z,
and are thus statistically identiﬁable. Yet, we encounter another problem: while this geometric framework
exclusively handles deterministic objects, the decoding part of generative models is often stochastic. The
learnt map f, that mathematically describes those decoders, is stochastic too, and so is the Riemannian
metric pulled back through it. This is shown in the right panel of Figure 1. In conclusion, in order to
navigate our latent manifold, we need a deterministic approximation of our pullback metric.
1Under review as submission to TMLR
Previous research has approximated the stochastic pullback metric with the expected value of the Riemannian
metric tensor. Yet, the metric tensor serves as a surrogate quantity to deﬁne an induced norm on our
manifold. Instead of taking the expectation of the metric tensor, we propose to take the expectation of
the norm directly. In this paper , we compare our expected norm with the norm induced by the expected
metric tensor. The main ﬁndings are:
1.The expected norm deﬁnes a Finsler metric. Finsler geometry is a generalisation of Riemannian
geometry.
2.For Gaussian Processes, the stochastic norm obtained through the pullback metric follows a non-
central Nakagami distribution, so our Finsler metric has a closed-form expression.
3.In high dimensions, for Gaussian Processes, our Finsler metric and a previously studied Riemannian
metric converge to each other at a rate of O/parenleftbig1
D/parenrightbig
.
Observation space f = {f1, f2, f3}
Latent space
f
Observation space Latent space
Figure 1: Left ﬁgure : The Euclidean distance measure, in blue, in the latent space does not take into consideration
the geometry of the observational manifold, and therefore it is not identiﬁable and leads to misinterpretations. Instead,
the length derived from the pullback metric will follow the curvature of the manifold. Right ﬁgure : Generative
models often map the latent space to the data space using a stochastic process f={f1, f2, . . .}. A stochastic
Riemannian metric, whose realisations are represented by ellipses, are obtained when we pull back the Euclidean
metric, represented by a unit circle, through the stochastic process f.
1.1 Outline of the paper
The paper explores the latent spaces learned by generative models, which encode a latent low-dimensional
manifold that represents observed high-dimensional data. The latent manifold is noted Z ⊂Rqand the data
manifold is noted X ⊂RD.
Assuming the manifold hypothesis holds, we need to deﬁne a norm in the latent manifold to compute
distances that respect the underlying geometry of the data. Such a norm can be constructed by pulling back
the Euclidean distance through the smooth function that maps the latent manifold to the data manifold.
This map, f:Z → X , mathematically describes the decoder of a trained generative model. Those models
being often stochastic, we consider fbeing a stochastic process. It means that the pullback metric tensor,
G=J/latticetopJ, and its induced norm, /bardbl·/bardblG:u→√
u/latticetopGu, are also stochastic. In section 2, we mathematically
deﬁne the notion of stochastic pullback metric and stochastic manifolds.
To circumvent all the challenges posed by this stochastic component, a deterministic approximation of the
norm is needed. It can be deﬁned by taking the expectation of the metric tensor. This norm, that we will
note/bardbl·/bardblR:u→ /bardblu/bardblE[G], has been studied before by Tosi et al. (2014); Arvanitidis et al. (2018), and is
explained in section 2.2. In this paper, we propose instead to directly take the expectation of the stochastic
norm. This expected norm, noted /bardbl·/bardblF:u→E/bracketleftbig
/bardblu/bardblG/bracketrightbig
, is introduced in section 2.4. The norm /bardbl·/bardblRis
deﬁned by a Riemannian metric, and we show that the norm /bardbl·/bardblFdeﬁnes a Finsler metric. We explain the
general diﬀerence between Finsler and Riemannian geometry in section 3.
The aim of this paper is to compare those two norms. We ﬁrst draw absolute bounds in section 3.2, and
then relative bounds in 3.3. We also investigate the relative diﬀerence of the norms when the dimension of
the data space inscrease, in section 3.4. Finally, we perform some experiments in Section 4, that illustrates
the Riemannian and Finsler norm in the same latent space.
2Under review as submission to TMLR
1.2 Related works
Finsler geometry in Machine Learning
Our work crucially relies on Finslerian geometry, which has been well-studied mathematically, but has only
seen very limited use in machine learning and statistics. We point to two notable exceptions, which are quite
distinct from our work. Lopez et al. (2021) use symmetric spaces to represent graphs and endow these with
a Finsler metric to capture dissimilarity structure in the observational data. Ratliﬀ et al. (2021) discuss the
role of diﬀerential geometry in motion planning for robotics. Along the way, they touch upon Finslerian
geometry, but mostly as a neat tool to allow for generalizations. To the best of our knowledge, no prior work
has investigated the links between stochastic and Finslerian geometry.
Strategies to deal with stochastic Riemannian geometry
Tosi et al. (2014) and Arvanitidis et al. (2018) introduced approximation of the pullback metric by taking the
expectation of the metric tensor. In those two cases, the map fis respectively a trained Gaussian process, or
the decoder of a VAE. In this paper, the derivations only hold if fis a smooth stochastic process (Deﬁnition
2.2), which is not the case of the VAEs1, and hence, our results cannot be applied for those models.
In addition to the work of Tosi et al. (2014), a solution to circumvent the randomness of the metric tensor is
to consider that the data follows a speciﬁc probability distribution. Instead of looking at the shortest path
on the data manifold, Arvanitidis et al. (2021) borrow tools from information geometry and consider the
straightest paths on the manifold whose elements are probability distributions.
2 Expectation on random manifolds
The metric pulled back by a stochastic mapping is, de facto, stochastic and endows a random manifold.
Unfortunately, we are not yet equipped to derive geometric objects on a random manifold. Instead, we
dodge this problem by seeking a deterministic approximation of this stochastic metric.
As mentioned above, a common solution is to approximate such a metric by its expectation. In section 2.2,
we study the expected Riemannian metric and summarise the main ﬁndings of Eklund & Hauberg (2019).
The other solution suggested by this paper is to approximate the expectation of the lengths instead of
the random metric itself. In section 2.4, we show that this new metric is not Riemannian but Finslerian
(Proposition 2.2), and it has a closed-form expression when the map fis a Gaussian process (Proposition
2.3).
2.1 Random Riemannian geometry
The pullback metric is appropriately deﬁned as an Riemannian metric if and only if the mapping fis an
immersion, which is a diﬀerentiable function whose derivatives are injective everywhere on the manifold
(Lee, 2013, Proposition 13.9). A manifold equipped with a Riemannian metric is called a Riemannian
manifold.
Deﬁnition 2.1. The pullback of the Euclidean metric through the immersion f:Z → X is aRieman-
nian metric . It is deﬁned as the inner product gz:(TzZ,TzZ)→R+:(u, v)→u/latticetopGv, at a speciﬁc
point zin the manifold Z.uandvare vectors lying in the tangent plane TzZ(ie: the set of all tangent
vectors) of the manifold. G=J/latticetopJ, with Jthe Jacobian of f.
1the decoder of a VAE, while it decodes to a Gaussian, cannot be considered as a diﬀerentiable stochastic process. One
reason is because the independence of the probability of the data: p(x|z) =/producttextn
i=1p(xi|zi). Let us assume the opposite: the
decoder is a Gaussian process. The covariance of the Gaussian process would be a diagonal matrix because of the independence
of the probability of the data. The covariance would correspond to a dirac distribution: cov(xi, xj) =δij. However, a stochastic
process is diﬀerentiable only if the covariance is diﬀerentiable, which is not the case of the dirac distribution. Hence the decoder
of a VAE cannot be considered as a diﬀerentiable stochastic process.
3Under review as submission to TMLR
Since a Riemannian metric is an inner product, it induces a norm, noted /bardbl·/bardblG. We can use this norm to deﬁne
thecurve length andcurve energy on a manifold: LG(γ) =/integraltext1
0/vextenddouble/vextenddouble˙γ(t)/vextenddouble/vextenddouble
GdtandEG(γ) =/integraltext1
0/vextenddouble/vextenddouble˙γ(t)/vextenddouble/vextenddouble2
Gdt,
with γa curve deﬁned on Z, and ˙γits derivative. A locally length-minimising curve between two connecting
points is called a geodesic . To obtain a geodesic, we can minimise the curve length, but in practice
minimising the curve energy is more eﬃcient. On the manifold, we also may want to integrate probability
functions, and so we need to deﬁne a volume measure that can be used akin to the change of variable
formula for integrals: for U ⊂ Z ,/integraltext
f(U)h(x)dx=/integraltext
Uh(f(z))VRdz, with VR(z) =√Gzthe volume measure.
In addition, we are considering the case where the immersion fis a stochastic process. The outputs of our
trained model, x∈ X, which represent our data, are random variables.
Deﬁnition 2.2. Astochastic process is a collection of random variables {X(t, ω), t∈T}indexed
by an index set Tdeﬁned on a sample space Ω, which represents the set of all possible outcomes. An
outcome in Ωis denoted by ω, and a realisation of the stochastic process is the sequence of X(·, ω)that
depends on the outcome ω.
In this framework, our index set is our latent manifold T=Z, and our sample space Ωis deﬁned as the set
of the model evaluations. For every point z∈ Z, every time we execute our model, the output x=f(z)is
a random variable following a speciﬁc distribution. When the data xfollow a Gaussian distribution, the
stochastic process is called a Gaussian process . A GP-LVM is a model that learns how to map the data
from a latent space to a data space through a Gaussian process.
When fis a stochastic immersion, the metric tensor becomes a random matrix. In this paper, we call a
manifold equipped with the stochastic pullback metric a random manifold , noted (Z, g). As a consequence
of the stochastic aspect of the metric, all the functionals are stochastic themselves, and they are no longer
trivial to manipulate.
Deﬁnition 2.3. Arandom Riemannian metric tensor is a matrix-valued random ﬁeld (ie: a
collection of matrix-valued random variables {G(z, ω), z∈ Z} ), whose realisation for a speciﬁc evaluation
ω∈Ωis a Riemannian metric tensor. A random Riemannian metric is a metric induced by a
random Riemannian metric tensor: gz:(TzZ,TzZ)→R+:(u, v)→u/latticetopGv. For the rest of the paper,
the associated stochastic norm is noted:
/bardbl·/bardblG:TzZ →R+:u→/radicalbig
gz(u, u):=√
u/latticetopGu
If this stochastic norm is induced by fdeﬁned as a Gaussian process, then /bardbl·/bardblGfollows a non-central
Nakagami distribution. This is explained in the proof of Proposition 2.3.
2.2 Norm induced by the expected metric tensor
One way to approximate a random metric tensor is to take its expectation with respect to the collection of
random metrics induced by the stochastic process. This has been introduced before by Tosi et al. (2014)
GP-LVMs.
Deﬁnition 2.4. LetGbe a stochastic Riemannian metric tensor on the manifold Z. We refer to E[G]
as the expected metric tensor . It induces a Riemannian metric and a norm on Z. We will note the
norm induced by the expected metric tensor as:
/bardbl·/bardblR:TzZ →R+:u→ /bardblu/bardblE[G]:=/radicalBig
u/latticetopE[G]u
Like any Riemannian metric, we can deﬁne the following functionals: LR(γ) =/integraltext1
0/radicalbig
˙γ(t)/latticetopE[G] ˙γ(t)dt,
ER(γ) =/integraltext1
0˙γ(t)/latticetopE[G] ˙γ(t)dt=E[ER(γ)], and VR(z) =/radicalbig
detE[G].
4Under review as submission to TMLR
2.3 Expected paths on random manifolds
Approximating the stochastic metric by its expectation seems a natural but also ad-hoc solution. If we
want to explore a manifold, we might prefer to use a representative quantity, such as the lengths between
data points. This is the motivation of the work led by Eklund & Hauberg (2019). The expectation of the
lengths can give us an idea about how, on average, two points are connected on a random manifold. The
expected curve length , and its corresponding curve energy on the random manifold (Z, g)are deﬁned
as:LF(γ) =/integraltext1
0E/bracketleftBig/radicalbig
˙γ(t)/latticetopG˙γ(t)/bracketrightBig
dt=E[LR(γ)], and EF(γ) =/integraltext1
0E/bracketleftBig/radicalbig
˙γ(t)/latticetopG˙γ(t)/bracketrightBig2
dt.
One observation made by Eklund & Hauberg (2019) is that the length ( LR) derived from the expected
Riemannian metric is not equal to the expected curve length ( LF), and their respective energy curves diﬀer
by a variance term:
ER(γ)−EF(γ) =/integraldisplay1
0˙γ(t)/latticetopE[G] ˙γ(t)−E/bracketleftbigg/radicalBig
˙γ(t)/latticetopG˙γ(t)/bracketrightbigg2
dt
=/integraldisplay1
0E/bracketleftBig/vextenddouble/vextenddouble˙γ(t)/vextenddouble/vextenddouble2
G/bracketrightBig
−E/bracketleftBig/vextenddouble/vextenddouble˙γ(t)/vextenddouble/vextenddouble
G/bracketrightBig2
dt=/integraldisplay1
0Var/bracketleftBig/vextenddouble/vextenddouble˙γ(t)/vextenddouble/vextenddouble
G/bracketrightBig
dt
This term can be regarded as a regularisation term for the Riemannian energy curve: the curve energy
ERmight be penalised when the curve goes through regions with high-variance. In practice, for a Gaussian
process with a stationary kernel, this variance term is upper bounded by the posterior variance that is
relatively low next to the training points and is high outside of the support of the data. Later, we will also
see that the functionals agree in high dimensions, leading to the same geodesics (Section 3).
Eklund & Hauberg (2019) also noted that these quantities are bounded by the number of dimensions:
Proposition 2.1. (Eklund & Hauberg, 2019) Let f:Rq→RDbe a stochastic process such that the
sequence: {f/prime
1, f/prime
2, . . . , f/prime
D}has uniformly bounded moments. There is then a constant Csuch that:
0≤LR−LF
LR≤C
8D
2.4 Expected norm and Finsler geometry
Our work can be seen as an extension of Eklund & Hauberg (2019)’s research. We are interested in approxi-
mating the stochastic norm instead of the metric tensor, and by doing so, the derived curve length and curve
energy are the same ones studied by Eklund & Hauberg (2019). We go further as we not only compare curve
lengths, but the deterministic norms obtained with the stochastic metric.
Deﬁnition 2.5. LetGbe a stochastic Riemannian metric tensor on the manifold Z. It induces a
stochastic norm, /bardbl·/bardblGonZ. We will note the expected norm as:
/bardbl·/bardblF:TzZ →R+:u→E[/bardblu/bardblG]:=E/bracketleftBig√
u/latticetopGu/bracketrightBig
While it cannot be induced by an inner-product, it is suﬃciently convex to be deﬁned as a Finsler metric .
Deﬁnition 2.6. LetF:T Z → R+be a continuous non-negative function deﬁned on the tangent
bundle T Zof a diﬀerentiable manifold Z.
We say that Fis aFinsler metric if, for each point zofZandvonTzZ, we have (1) Positive
homogeneity :∀λ∈R+,F(λv) =λF(v). (2) Smoothness :Fis aC∞function on the slit tangent
bundle T Z \ { 0}. (3) Strong convexity criterion : the Hessian matrix gij(v) =1
2∂2F2
∂vivj(v)is positive
deﬁnite for non-zero v.
5Under review as submission to TMLR
A diﬀerentiable manifold Zequipped with a Finsler metric is called a Finsler manifold .
Finsler geometry can be seen as an extension of Riemannian geometry, since the requirements for deﬁning a
metric are less restrictive.
Proposition 2.2. LetGbe a stochastic Riemannian metric. Then, the function Fz:TzZ →R:u→
/bardblu/bardblFdeﬁnes a Finsler metric, but it is not induced by a Riemannian metric.
Proof. IfFwas induced by a Riemannian metric, then this metric would be deﬁned as:
fz:Rq×Rq→R+:(v1, v2)→E/bracketleftBig/radicalbig
v/latticetop
1Gv2/bracketrightBig2
. Since a Riemannian metric is an inner product, it
should be symmetric, positive, deﬁnite and bilinear. Here, we can see that fxis not bilinear, so fzis not a
Riemannian metric. However, we can prove that Fz:Rq→R:v→E/bracketleftBig√
v/latticetopGv/bracketrightBig
is positive, homogeneous,
smooth and strongly convex, and so Fzis a Finsler metric (Shen & Shen, 2016, Deﬁnition 2.1). For the full
proof, see Section B.1.
So far, we have assumed that fis an immersion and a stochastic process. If we consider fto be a Gaussian
Process in particular, the Finsler norm can be rewritten in a closed form expression.
Proposition 2.3. Letfbe a Gaussian process and Jits Jacobian, with J∼ N (E[J],Σ). The Finsler
norm can be written as:
Fz:TzZ →R+:/bardblv/bardblF:=v→√
2√
v/latticetopΣvΓ(D
2+1
2)
Γ(D
2)1F1/parenleftbigg
−1
2,D
2,−ω
2/parenrightbigg
,
with 1F1as the conﬂuent hypergeometric function of the ﬁrst kind and ω= (v/latticetopΣv)−1(v/latticetopE[J]/latticetopE[J]v).
Proof. We suppose that fis a Gaussian process, and so is its Jacobian. Gfollows a non-central Wishart
distribution: G=J/latticetopJ∼ W q(D,Σ,Σ−1E[J]/latticetopE[J]).v/latticetopGvis a scalar and also follows a non-central Wishart
distribution: v/latticetopGv∼ W 1(D, σ, ω ), with σ=v/latticetopΣvandω= (v/latticetopΣv)−1(v/latticetopE[J]/latticetopE[J]v)(Kent & Muir-
head, 1984, Deﬁnition 10.3.1). The square-root of a non-central Wishart distribution follows a non-central
Nakagami distribution (Hauberg, 2018b). Then, by construction, the stochastic norm /bardbl·/bardblGfollows a non-
central Nakagami distribution. The expectation of this distribution is known, and it has a closed-form
expression.
The conﬂuent hypergeometric function of the ﬁrst kind, also known as the Kummer function, is a special
function that is deﬁned as the solution of a speciﬁc second-order linear diﬀerential equation. The term ω
appears from the non-central Wishart distribution. When ωis non-zero, the distribution of the Jacobian
shifts away from the origin, and ωrepresents the magnitude and the direction of this shift, balanced by the
correlation between the variables. In Section 3.4, to prove our results in high-dimensions, we will assume
that our manifold Zis bounded, and so is ω.
3 Comparison of Riemannian and Finsler metrics
3.1 Theoretical comparison
In geometry, we need to deﬁne a metric (a norm) to compute functionals. In Riemannian geometry, the metric
is conveniently obtained by constructing an inner product. Because of its bilinearity, the inner product greatly
simpliﬁes subsequent computations, but it is also restrictive. A generalisation of Riemannian geometry can
be obtained by relaxing this assumption. Instead of deﬁning a metric as an inner product, we can deﬁne
6Under review as submission to TMLR
the metric as a norm2. Relaxing this assumption was studied by Finsler (1918), who gave his name to this
discipline.
Finsler geometry is similar to Riemannian geometry without the bilinear assumption. Most of the functionals
(curve length and curve energy) are deﬁned similarly to those obtained in Riemannian geometry. However,
the volume measure is diﬀerent, and there are at least two deﬁnitions of volume measure used in Finsler
geometry: the Busemann-Hausdorﬀ volume and the Holmes-Thomson volume measure (Wu, 2011). In this
paper, we decided to focus on the Busemann-Hausdorﬀ deﬁnition (Deﬁnition 3.1), which is more intuitive
and easier to derive. If the Finsler metric is a Riemannian metric, the deﬁnition of volume naturally coincides
with the Riemannian volume measure.
In Figure 3, a Busemann-Hausdorﬀ and the Riemannian volume measures have been computed for the same
set of data points. A Gaussian process has been trained to ﬁt data representing a pinwheel projected onto
a sphere.
Deﬁnition 3.1. For a given point zon the manifold, we deﬁne the Finsler indicatrix as the set
of vectors in the tangent space such that the Finsler metric is equal to: {v∈ TzZ|Fz(v) = 1}). We
callBn(1)the Euclidean unit ball, and vol (·)the standard Euclidean volume. In local coordinates
(e1,···, ed) on a Finsler manifold M, the Busemann-Hausdorﬀ volume form is deﬁned as dVF=
VF(z)e1∧ ··· ∧ ed, with:
VF(z) =vol(Bn(1))
vol({v∈ TzZ|Fz(v)<1}).
In the deﬁnition above, we introduce the notion of indicatrix . An indicatrix is a way to represent the
distortion induced by the metric on a unit circle. If our metric is euclidean, we will only have a linear
transformation between the latent and the observational spaces, and the indicatrix would still be a circle.
Because the Riemannian metric is quadratic, it will always generate an ellipse in the latent space. The
Finsler indicatrix, however, would have a convex, even asymmetrical, shape. This diﬀerence can be observed
in the indicatrix-ﬁeld represented in Figure 2: The Finsler indicatrices in purple can have almost rectangular
shape, while the Riemannian indicatrices, in orange, are ellipses.
There are also a few observations to note in Figure 2. First, in the area of low predictive variance (where
data points lie in the latent space), the Finsler and Riemannian indicatrices are alike. This follows from
the preceding comment that the metrics diverge by a variance term. If our mapping fwas deterministic,
both metrics would agree. Second, for every point, the Riemannian indicatrices are always contained by the
Finslerian ones, illustrating Proposition 3.1 on our absolute bounds in the following section.
3.2 Absolute bounds on the Finsler metric
The Finsler norm is upper bounded with the Riemannian norm obtained from the expected metric tensor.
It is also lower bounded:
Proposition 3.1. We deﬁne α= 2/parenleftbigg
Γ(D
2+1
2)
Γ(D
2)/parenrightbigg2
. The Finsler norm: /bardbl·/bardblFis bounded by two norms,
/bardbl·/bardblαΣand/bardbl·/bardblR, induced by the two respective Riemannian metric tensors: the covariance tensor αΣz
and the expected metric tensor E[Gz].
∀(z, v)∈ Z × T zZ:/bardblv/bardblαΣ≤ /bardblv/bardblF≤ /bardblv/bardblR
2A norm only needs to be deﬁnite and satisﬁes the triangular inequality, but is not necessary symmetric. This means that,
for a vector v, we can have a non reversible Finsler metric: Fx(v)/negationslash=Fx(−v). Intuitively, this means that the path used to
connect two points would be diﬀerent depending on the starting point. This asymmetric property becomes valuable when
studying the geometry of anisotropic media (Markvorsen, 2016), for example. In our case, our Finsler metric is reversible.
7Under review as submission to TMLR
A
B
Figure 2: Indicatrice ﬁeld over the latent space of the pinwheel data (in grey) representing the Riemannian (in orange)
and Finslerian (in purple) metrics (See Section C). (A) The indicatrices are computed over a grid in the latent space.
(B) The indicatrices are computed along a geodesic: the Riemannian and Finslerian metrics coincide.
Proof. The proof can be sketched as follows: the upper bound /bardblv/bardblF≤ /bardblv/bardblR, also rewritten as: E[√
v/latticetopGv]≤/radicalbig
v/latticetopE[G]v, is obtained by applying Jensen’s inequality, knowing that the square root x→√xis a concave
function. The lower bound /bardblv/bardblαΣ≤ /bardblv/bardblF, rewritten as√
v/latticetopαΣv≤E[√
v/latticetopGv], is obtained using the closed
form expression of the Finsler function.
The result is illustrated in Figure 4 (lower right). Four metric tensors ( G1, G2, G3, G4), each following a
non-central Wishart distribution with a speciﬁc mean and covariance matrix, have been computed. For each
of them, we have drawn the indicatrices ( {v∈ TzZ | /bardbl v/bardbl= 1}) induced by the norms: /bardbl·/bardblF,/bardbl·/bardblRand
/bardbl·/bardblαΣ. As expected, we can notice that the αΣ-indicatrix contains the Finsler indicatrix, itself containing
R-indicatrix.
By bounding the Finsler metric, we are able to bound their respective functionals:
Corollary 3.1. The length, the energy and the Busemann-Hausdorﬀ volume of the Finsler metric are
bounded respectively by the Riemannian length, energy and volume of the covariance tensor αΣ(noted
LαΣ, EαΣ, VαΣ) and the expected metric E[G](noted LR, ER, VR):
∀z∈ Z, LαΣ(z)≤LF(z)≤LR(z)
EαΣ(z)≤EF(z)≤ER(z)
VαΣ(z)≤VF(z)≤VR(z)
Proof. From Proposition 3.1, we need to integrate each term of the inequality to obtain the length and the
energy. The volume is less trivial, since we use the Busemann-Hausdorﬀ deﬁnition for measuring VF. We have
to place ourselves in hyperspherical coordinates, and show that the Finsler indicatrix is still bounded.
3.3 Relative bounds on the Finsler metric
8Under review as submission to TMLR
Proposition 3.2. Letfbe a stochastic immersion. finduces the stochastic norm /bardbl·/bardblG, deﬁned in
Section 2. The relative diﬀerence between the Finsler norm /bardbl·/bardblFand the Riemmanian norm /bardbl·/bardblRis:
0≤/bardblv/bardblR− /bardblv/bardblF
/bardblv/bardblR≤Var/bracketleftBig
/bardblv/bardbl2
G/bracketrightBig
2E/bracketleftBig
/bardblv/bardbl2
G/bracketrightBig2.
Proof. This proposition is a direct application of the Sharpened Jensen’s inequality (Liao & Berg, 2019).
The previous proposition is valid for any stochastic immersion. We can see that the metrics become equal
when the ratio of the variance over the expectation shrinks to zero. This happens in two cases: when the
variance converges to zero, which is similar to having a deterministic immersion, and when the number of
dimensions increases. The latter case is investigated below for a Gaussian process3.
Proposition 3.3. Letfbe a Gaussian process. We note ω= (v/latticetopΣv)−1(v/latticetopE[J]/latticetopE[J]v), with Jthe
jacobian of f, and Σthe covariance matrix of J.
The relative ratio between the Finsler norm /bardbl·/bardblFand the Riemmanian norm /bardbl·/bardblRis:
0≤/bardblv/bardblR− /bardblv/bardblF
/bardblv/bardblR≤1
D+ω+ω
(D+ω)2.
Proof. v/latticetopGvfollows a one-dimension non-central Wishart distribution: v/latticetopGzv∼ W 1(D, σ, ω ), with
σ=v/latticetopΣvandω= (v/latticetopΣv)−1(v/latticetopE[J]/latticetopE[J]v). We use the theorem of the moments to obtain both the
expectation and the variance, which leads us to the result.
Corollary 3.2. When fis a Gaussian Process, the relative ratio between the length, the energy and
the volume of the Finsler norm (noted LF, EF, VF) and the Riemannian norm (noted LR, ER, VR) is:
0≤LR(z)−LF(z)
LR(z)≤max
v∈TzZ/braceleftbigg1
D+ω+ω
(D+ω)2/bracerightbigg
0≤ER(z)−EF(z)
ER(z)≤max
v∈TzZ/braceleftBigg
2
D+ω+1 + 2 ω
(D+ω)2+2ω
(D+ω)3+ω2
(D+ω)4/bracerightBigg
0≤VR(z)−VF(z)
VR(z)≤1−/parenleftBigg
1−max
v∈TzZ/braceleftbigg1
D+ω+ω
(D+ω)2/bracerightbigg/parenrightBiggq
Proof. We directly use Proposition 3.3. To obtain the inequalities with the lengths and the energies, we
ﬁrst multiply all the terms by the Riemannian metric, and we integrate every term. To obtain the inequality
with the volume, similarly to Corollary 3.1, we place ourselves in hyperspherical coordinates and bound the
radius of the Finsler indicatrix.
In Figure 3, we can compare the volume measures obtained from the Riemannian and Finsler metrics, and
in particular, their ratio in the top right image. When the metrics are computed next to the data points
in area where the variance is very low, we can see that the ratio of the volume measure is at the order
of magnitude 10−4. Further away from the data points, the variance increases and so does the diﬀerence
between the Riemannian and Finsler volume measures.
3Interestingly, the term E[ν]2/Var[ν], when νfollows a central Nakagami distribution, is called a shape parameter. It has
been introduced by Nakagami himself to study the intensity of fading in radio wave propagation (Nakagami, 1960). When
ν:=/radicalbig
ξwith ξ∼ W 1(Σ, D), then m=D/2. This is a particular result obtained from Proposition 3.3, when E[J] = 0 .
9Under review as submission to TMLR
−0.6−0.4−0.20.00.20.40.6
A
B
−0.6−0.4−0.20.00.20.40.6
−4.0−3.5−3.0−2.5−2.0−1.5−1.0−0.5
D
−3.0−2.5−2.0−1.5−1.0−0.50.0
C
Figure 3: Diﬀerence of volume for data embedded in the latent space. (A) Riemannian volume measure, (B) Finslerian
(Busemann-Hausdorﬀ) volume measure, (C) Variance of the Gaussian process, (D) Ratio between the Riemannian
and Finslerian volume: (VR(z)−VF(z))/VR(z). All heatmaps are computed in logarithm scale.
3.4 Results in high dimensions
Proposition 3.3 and Corrollary 3.2 indicate that the metrics become similar when the dimension ( D) of the
observational space increases. If we assume that the latent space is a bounded manifold, the metrics converge
to each other at a rate of O/parenleftbig1
D/parenrightbig
, as do their functionals.
We assume that the latent manifold is bounded. Then, we can deduce that (1) the term ω, which represents
the non-centrality of the data, does not grow faster than the number of dimensions (See lemma B.2.2, in
Section B.2.3) and (2) we that the metrics are ﬁnite.
Corollary 3.3. Letfbe a Gaussian Process. In high dimensions, we have:
LR(z)−LF(z)
LR(z)=O/parenleftbigg1
D/parenrightbigg
ER(z)−EF(z)
ER(z)=O/parenleftbigg1
D/parenrightbigg
VR(z)−VF(z)
VR(z)=O/parenleftbiggq
D/parenrightbigg
And, when Dconverges toward inﬁnity: LR∼
+∞LF, ER∼
+∞EFandVR∼
+∞VF.
Proof. This result follows from Corollary 3.2, assuming the latent manifold is bounded.
Corollary 3.4. Letfbe a Gaussian Process. In high dimensions, the relative ratio between the Finsler
norm/bardbl·/bardblFand the Riemmanian norm /bardbl·/bardblRis:
/bardblv/bardblR− /bardblv/bardblF
/bardblv/bardblR=O/parenleftbigg1
D/parenrightbigg
And, when Dconverges toward inﬁnity: ∀v∈ TzZ,/bardblv/bardblR∼
+∞/bardblv/bardblF.
Proof. Similarly, from Proposition 3.3, in a bounded manifold, both metrics converge to each other in high
dimensions.
4 Experiments
We want to illustrate cases where these metrics could be useful in practice for real-world data. For this, we
use three datasets: a synthetic dataset (composed of data representing a pinwheel projected onto a sphere),
10Under review as submission to TMLR
dim: 3 dim: 5 dim: 10 dim: 50
Riemannian indicatrix Finslerian indicatrixLower bound metric tensor indicatrix
Figure 4: Left: Ratio of volumes (VR−VF)/VRdecreasing with respect to the number of dimensions. The results
were obtained from using a collection of matrices {Gi}following a non-central Wishart distribution. Upper right:
The Finsler and Riemannian indicatrices converge towards each other when increasing the number of dimensions.
Lower right: Illustration of the absolute bounds in Proposition 3.1 with the αΣ-indicatrices, Riemannian indicatrices
and Finsler indicatrices.
a font dataset Campbell & Kautz (2014), and a dataset representing single-cells stages Guo et al. (2010).
We trained a GP-LVM model to learn a 2d-manifold. From the optimised Gaussian process, we can access
the Riemannian and Finsler metric, and minimise their respective curve energies to obtain geodesics.
As we can see, the Finsler and Riemannian geodesics coincide in all cases. For all latent spaces (A.1. and
B.1. in Figure 5, and in Figure 6), the heatmap represents the Riemannian volume measure in logarithm
scale. The volume measure is low in the area of high density and high in the area of low density of data
points.
Observed data
Riemannian geodesic
Finslerian geodesic
A.1.
B.1.
A.2.
B.2.
Figure 5: Geodesics computed for latent (A.1., B.1.) and observational (A.2., B.2.) spaces. (A) The dataset used
was a pinwheel projected onto a sphere, as seen in A.2. (B) The dataset consists of the position of the markers
parametrising the contour of the letter f.
5 Discussion
Generative models are often used to reduce data dimension in order to better understand the mechanisms
behind the data generating process. We consider the general setting where the mapping from latent vari-
ables to observations is driven by a smooth stochastic process, and the sample mappings span Riemannian
manifolds. The Riemannian geometry machinery has already been used in the past to explore the latent
space.
11Under review as submission to TMLR
Figure 6: qPCR data
In this paper, we have shown how curves and volumes can be identiﬁed by deﬁning the length of a latent
curve as its expected length measured in the observation space. This is a natural extension of classical
diﬀerential geometric constructions to the stochastic realm. Surprisingly, we have shown that this does not
give rise to a Riemannian metric over the latent space, even if sample mappings do. Rather, the latent
representation naturally becomes equipped with a Finsler metric, implying that stochastic manifolds, such
as those spanned by Latent Variable Models (LVMs), are inherently more complex than their deterministic
counterparts.
The Finslerian view of the latent representation gives us a suitable general solution to explore a random
manifold, but it does not immediately translate into a practical computational tool. As Riemannian mani-
folds are better understood computationally than Finsler manifolds, we have raised the question: How good
an approximation of the Finsler metric can be achieved by a Riemannian metric? The answer turns out to
be: quite good. We have shown that as data dimension increases, the Finsler metric becomes increasingly
Riemannian. Since LVMs are most commonly applied to high-dimensional data (as this is where dimension-
ality reduction carries value), we have justiﬁcation for approximating the Finsler metric with a Riemannian
metric such that computational tools become more easily available. In practice we ﬁnd that geodesics under
the Finsler the Riemannian metric are near identical, except in regions of high uncertainty.
Notations
Z,X Smooth diﬀerentiable latent ( Z) and data ( X) manifold,
f A stochastic immersion f:Z ⊂Rq→ X ⊂ RD,
J Jacobian of the stochastic function f,
G Stochastic metric tensor deﬁned as the pullback metric through f:G=J/latticetopJ,
TzZ Tangent space of the manifold Zat a point z,
Σ IFfis a Gaussian process, then J∼/producttextD
i=1N(µi,Σ),
/bardbl·/bardblGStochastic induced norm: /bardblv/bardblG:=√
v/latticetopGv,
/bardbl·/bardblRRiemannian induced norm: /bardblv/bardblR:=/radicalbig
v/latticetopE[G]v:=/radicalbig
g(v, v),
/bardbl·/bardblFFinsler norm: /bardblv/bardblF:=E[√
v/latticetopGv] =F(v),
LR, ER, VRLength, energy and volume obatined from the Riemannian induced norm /bardbl·/bardblR,
LF, EF, VFLength, energy and Busemann Hausdorﬀ volume obtained from the Finsler norm /bardbl·/bardblF.
12Under review as submission to TMLR
References
Sumon Ahmed, Magnus Rattray, and Alexis Boukouvalas. Grandprix: scaling up the bayesian gplvm for
single-cell data. Bioinformatics , 35(1):47–54, 2019.
Georgios Arvanitidis, Lars Kai Hansen, and Søren Hauberg. Latent Space Oddity: on the Curvature of Deep
Generative Models. In International Conference on Learning Representations (ICLR) , 2018.
Georgios Arvanitidis, Miguel González-Duque, Alison Pouplin, Dimitris Kalatzis, and Søren Hauberg. Pulling
back information geometry, 2021. URL https://arxiv.org/abs/2106.05367 .
Eli Bingham, Jonathan P Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos,
Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D Goodman. Pyro: Deep universal probabilistic
programming. The Journal of Machine Learning Research , 20(1):973–978, 2019.
Neill D. F. Campbell and Jan Kautz. Learning a manifold of fonts. ACM Trans. Graph. , 33(4), jul 2014.
ISSN 0730-0301. doi: 10.1145/2601097.2601212. URL https://doi.org/10.1145/2601097.2601212 .
Nicki S. Detlefsen, Alison Pouplin, Cilie W. Feldager, Cong Geng, Dimitris Kalatzis, Helene Hauschultz,
Miguel González Duque, Frederik Warburg, Marco Miani, and Søren Hauberg. Stochman. GitHub. Note:
https://github.com/MachineLearningLifeScience/stochman/ , 2021.
David Eklund and Søren Hauberg. Expected path length on random manifolds. arXiv preprint
arXiv:1908.07377 , 2019.
Charles Feﬀerman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypothesis. Journal of
the American Mathematical Society , 29(4):983–1049, 2016.
Paul Finsler. Ueber kurven und Flächen in allgemeinen Räumen . Philos. Fak., Georg-August-Univ., 1918.
Guoji Guo, Mikael Huss, Guo Qing Tong, Chaoyang Wang, Li Li Sun, Neil D. Clarke, and Paul Robson.
Resolution of cell fate decisions revealed by single-cell gene expression analysis from zygote to blastocyst.
Developmental Cell , 18(4):675–685, 2010. ISSN 1534-5807. doi: https://doi.org/10.1016/j.devcel.2010.02.
012. URL https://www.sciencedirect.com/science/article/pii/S1534580710001103 .
Søren Hauberg. Only bayes should learn a manifold (on the estimation of diﬀerential geometric structure
from data). arXiv preprint arXiv:1806.04994 , 2018a.
Søren Hauberg. The non-central Nakagami distribution. Technical report, Technical University of Denmark,
2018b. URL http://www2.compute.dtu.dk/~sohau/papers/nakagami2018/nakagami.pdf .
John T. Kent and R. J. Muirhead. Aspects of Multivariate Statistical Theory. The Statistician , 1984. ISSN
00390526. doi: 10.2307/2987858.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Neil Lawrence. Gaussian process latent variable models for visualisation of high dimensional data. Advances
in neural information processing systems , 16, 2003.
John M Lee. Smooth manifolds. In Introduction to smooth manifolds , pp. 1–31. Springer, 2013.
J. G. Liao and Arthur Berg. Sharpening jensen’s inequality. The American Statistician , 73(3):278–281, 2019.
doi: 10.1080/00031305.2017.1419145.
Federico Lopez, Beatrice Pozzetti, Steve Trettel, Michael Strube, and Anna Wienhard. Symmetric spaces for
graph embeddings: A ﬁnsler-riemannian approach. In Marina Meila and Tong Zhang (eds.), Proceedings
of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learn-
ing Research , pp. 7090–7101. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/
lopez21a.html .
13Under review as submission to TMLR
Steen Markvorsen. A ﬁnsler geodesic spray paradigm for wildﬁre spread modelling. Nonlinear Analysis: Real
World Applications , 28:208–228, 2016.
Minoru Nakagami. The m-distributiona general formula of intensity distribution of rapid fading. In Statistical
methods in radio wave propagation , pp. 3–36. Elsevier, 1960.
Pyro. Gaussian process latent variable modelű, 2022. URL https://pyro.ai/examples/gplvm.html .
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning (Adaptive
Computation and Machine Learning) . The MIT Press, 2005. ISBN 026218253X.
Nathan D. Ratliﬀ, Karl Van Wyk, Mandy Xie, Anqi Li, and Muhammad Asif Rana. Generalized nonlinear
and ﬁnsler geometry for robotics. In 2021 IEEE International Conference on Robotics and Automation
(ICRA) , pp. 10206–10212, 2021. doi: 10.1109/ICRA48506.2021.9561543.
Yi-Bing Shen and Zhongmin Shen. Introduction to Modern Finsler Geometry . Co-published with HEP, 2016.
doi: 10.1142/9726. URL https://www.worldscientific.com/doi/abs/10.1142/9726 .
Alessandra Tosi, Søren Hauberg, Alfredo Vellido, and Neil D Lawrence. Metrics for probabilistic geometries.
arXiv preprint arXiv:1411.7432 , 2014.
J. G. Wendel. Note on the gamma function. The American Mathematical Monthly , 55(9):563–564, 1948.
ISSN 00029890, 19300972. URL http://www.jstor.org/stable/2304460 .
Bingye Wu. Volume form and its applications in ﬁnsler geometry. Publicationes Mathematicae , 78, 04 2011.
doi: 10.5486/PMD.2011.4998.
14Under review as submission to TMLR
A A primer on Geometry
The main purpose of the paper is to deﬁne and compare two legitimate metrics to compute the average length
between random points. Before going further, it’s important to formally deﬁne the two metrics (Riemannian
and Finsler metrics, respectively) which we do in sections A.2 and A.3. They are both constructed on
topological manifolds, the deﬁnition of which is recalled in section A.1. We ﬁnally introduce the notion of
random manifold in section A.4, which is the last notion needed to frame our problem of interest: which
metric should we use to compute the average distance on a random manifold?
A.1 Topological and diﬀerentiable manifolds
This section aims to deﬁne core concepts in diﬀerential geometry that will be used later to deﬁne Riemannian
and Finsler manifolds. Recall that two topological spaces are called homeomorphic if there is a continuous
bijection between them with continuous inverse.
Deﬁnition A.1. A d-dimensional topological manifold Mis a second-countable Hausdorﬀ topo-
logical space such that every point has an open neighbourhood homeomorphic to an open subset of
Rd.
LetMbe a topological manifold. This means that for any x∈ M there is an open neighbourhood Uxof
xand a homeomorphism φUx:Ux→Rdonto an open subset of Rd. Suppose that x, y∈ M are such that
Ux∩Uy/negationslash=∅, letU=Ux,V=Uyand consider the so-called coordinate change map
φV◦φ−1
U|φU(U∩V):φU(U∩V)→Rd.
We call Mtogether with an open cover {Ux}x∈Mas above a diﬀerentiable orsmooth manifold if the
coordinate maps are inﬁnitely diﬀerentiable.
Beyond these technical deﬁnitions, one can imagine a diﬀerentiable manifold as a well-behaved smooth
surface that possesses locally all the topological properties of a Euclidean space. All the manifolds in this
paper are assumed to be diﬀerentiable and connected manifolds.
Deﬁnition A.2. We also deﬁne, for a diﬀerentiable manifold M, the tangent space TxMas the set
of all the tangent vectors at x∈ M , and the tangent bundle T M the disjoint union of all the tangent
spaces: T M =∪
x∈MTxM.
So far, we have only deﬁned topological and diﬀerential properties of manifolds. In order to compute
geometric quantities, we need to equip those with a metric that helps us derive useful quantities such as
lengths, energies and volumes. A metric is a scalar valued function that is deﬁned for each point on the
topological manifold and takes as inputs one or two vectors (depending on the type of metric) from the
tangent space at the speciﬁc point. Such a function can either be deﬁned as a scalar product between two
vectors, this is the case of a Riemannian metric or, in the case of a Finsler metric, it is deﬁned similarly to
the norm of a vector. We will formally deﬁne these metrics and highlight their diﬀerences in the following
sections.
A.2 Riemannian manifolds
Deﬁnition A.3. LetMbe a manifold. A Riemannian metric is a map assigning at each point
x∈ M a scalar product G(·,·):TxM× TxM→R, with Ga positive deﬁnite bilinear map, which
is smooth with respect to x. A smooth manifold equipped with a Riemannian metric is called a
Riemannian manifold . We usually express the metric as a symmetric positive deﬁnite matrix G,
where we have for two vectors u, v∈ TxM:G(u, v) =/angbracketleftu, v/angbracketrightG=u/latticetopGv. We further deﬁne the induced
norm :v∈ TxM,/bardblv/bardblG=/radicalbig
G(v, v).
The Riemannian metric here can either refer to the scalar product Gitself, or the associated metric tensor
G.
15Under review as submission to TMLR
f
uvgx(u, v) = 1
MTxM
d f(v)
d f(u)
Figure 7: fis an immersion that maps a low dimensional manifold to a high dimensional manifold M. On M, a
tangent plane TxMis draw at x. The indicatrice of the Euclidean metric is plotted in blue. When this metric is
pulled-back through f, the low dimensional space is now equipped with the pullback metric g, which is a Riemannian
metric by deﬁnition. The vectors d f(u)andd f(v)are called the push-forwards of the vectors uandvthrough f.
Deﬁnition A.4. We consider a curve γ(t)and its derivative ˙γ(t)on a Riemannian manifold Mequipped
with the metric g. Then, we deﬁne the length of the curve :
LG(γ) =/integraldisplay/vextenddouble/vextenddouble˙γ(t))/vextenddouble/vextenddouble
Gdt=/integraldisplay/radicalbig
gt( ˙γ(t),˙γ(t))dt,
where gt=gγ(t). Locally length-minimising curves between two connecting points are called Geodesics .
Deﬁnition A.5. Thecurve energy is deﬁned as:
EG(γ) =/integraldisplay/vextenddouble/vextenddouble˙γ(t))/vextenddouble/vextenddouble2
Gdt=/integraldisplay
gt( ˙γ(t),˙γ(t))dt.
There are two interesting properties to note about the length of a curve and the curve energy. First, the
length is parametrisation invariant: for any bijective smooth function ηon the domain of γwe have that
LG(γ◦η) =LG(γ). We also say the Riemannian metric gives us intrinsic coordinates to compute the length.
Secondly, for a given curve γ, we have: LG(γ)2≤2EG(γ). Because of the invariance of the curve, when we
aim to minimise it, a solver can ﬁnd an inﬁnite number of solutions. On the other hand, the curve energy is
convex and will lead to a unique solution. Thus, to obtain a geodesic, instead of solving the corresponding
ODE equations, or directly minimising lengths, it is easier in practice to minimise the curve energy, as a
minimal energy gives a minimal length.
The Riemannian metric also provides us with an inﬁnitesimal volume element that relates our metric Gto
an orthonormal basis, the same way the Jacobian determinant accommodate for a change of coordinates in
the change of variables theorem.
Deﬁnition A.6. In local coordinates ( e1,···, ed), the volume form of the Riemannian manifold M,
equipped with the metric tensor G, is deﬁned as: dVG=VG(x)e1∧ ··· ∧ ed, with:
VG(x) =/radicalbig
det(G).
Remark. The symbol ∧represents the wedge product and it is used to manipulate diﬀerential k-forms. Here,
the basis vectors ( e1,···, ed) form a d-dimensional parallelepiped ( e1∧ ··· ∧ ed) with unit volume.
16Under review as submission to TMLR
f
M
VG(x)
γf(γ)
Figure 8: Once the low dimensional manifold is equipped with a metric that captures the inherent structure of the
high dimensional manifold, we can compute a geodesic γ, by minimising the energy functional between two points.
The geodesic f(γ)will be the shortest path between two points on the manifold M. The volume measure VGcan
also be used to integrate functions over regions of the manifold, as we would do in the Euclidean space. It can also
be linked to the density of the data: if the data points are uniformly distributed over the high-dimensional manifold,
in the low-dimensional manifold, a low volume would correspond to a high density of data. It is a useful way to give
more information about the distribution of the data.
A.3 Finsler manifolds
Finsler geometry is often described as an extension of Riemannian geometry, since the metric is deﬁned in
a more general way, lifting the quadratic constraint. In particular, the norm of a Riemmanian metric is a
Finsler metric, but the converse is not true.
Deﬁnition A.7. LetF:T M → R+be a continuous non-negative function deﬁned on the tangent
bundle T M of a diﬀerentiable manifold M. We say that Fis aFinsler metric if, for each point xof
MandvonTxM, we have:
1.Positive homogeneity: ∀λ∈R+,F(λv) =λF(v).
2.Smoothness: Fis aC∞function on the slit tangent bundle T M \ { 0}.
3.Strong convexity criterion: the Hessian matrix gij(v) =1
2∂2F2
∂vivj(v)is positive deﬁnite for non-
zerov.
A diﬀerentiable manifold Mequipped with a Finsler metric is called a Finsler manifold .
Here, it is worth noting that, for a given point in the manifold, the Finsler metric is deﬁned with only one
vector in the tangent space, while the Riemannian metric is deﬁned with two vectors. Moreover, from the
previous deﬁnition, we can deduce that the metric is:
1.Positive deﬁnite: for all x∈ M andv∈ TxM,F(v)≥0andF(v) = 0 if and only if v= 0.
2.Subadditive: F(v+w)≤F(v) +F(w)for all x∈ M andv, w∈ TxM.
We say that Fis a Minkowski norm on each tangent space TxM. Furthermore, if Fsatisﬁes the reversibility
property: F(v) =F(−v), it deﬁnes a norm on TxMin the usual sense.
Similarly to Riemannian geometry, lengths, energies and volumes can be deﬁned directly from the Finsler
metric:
Deﬁnition A.8. We consider a curve γand its derivative ˙γon a Finsler manifold Mequipped with
the metric F. We deﬁne the length of the curve as follows:
LF(γ) =/integraldisplay
F( ˙γ(t))dt.
17Under review as submission to TMLR
f
uFx(u) = 1
MTxM
Figure 9: fis an immersion that maps a low dimensional manifold to a high dimensional manifold M. On M, a
tangent plane TxMis draw at x. Compared to the Riemannian manifold, the Finsler indicatrix, which represents
the all the vectors u∈ TxMsuch that Fx(u) = 1 , is not necessarily an ellipse. It can be asymmetric if the metric is
asymmetric itself. It is always convex.
Deﬁnition A.9. Thecurve energy is deﬁned as: EF(γ) =/integraltext
F( ˙γ(t))2dt.
Not only are the deﬁnitions strikingly similar, they also share the same properties. The curve length is
also invariant under reparametrisation, and upper bounded by the curve energy. Computing geodesics on
a manifold is reduced to a variational optimisation problem. These propositions are proved in detail in
Lemmas B.1.4 and B.1.5, in the appendix.
In Riemannian geometry, the volume measure deﬁned by the metric is unique. In Finsler geometry, diﬀerent
deﬁnitions of the volume exist, and they all coincide with the Riemannian volume element when the metric
is Riemannian. The most common choices of volume forms are the Busemann-Hausdorﬀ measure and the
Holmes-Thompson measure. According Wu (2011), depending on the Finsler metric and the topological
manifold, some choices seem more legitimate than others. In this paper, we decided to only focus on the
Busemann-Hausdorﬀ volume, as its deﬁnition is the most commonly used and leads to easier derivations.
We will later show that in high dimensions, our Finsler metric converges to a Riemannian metric, and thus,
the results obtained for the Busemann-Hausdorﬀ volume measure are also valid for the Holmes-Thomson
volume measure.
Deﬁnition A.10. For a given point xon the manifold, we deﬁne the Finsler indicatrix as the set of
vectors in the tangent space such that the Finsler metric is equal to one: {v∈ TxM|F(v) = 1}). We
denote the Euclidean unit ball in RdbyBd(1)and for measurable subsets S⊆Rdwe use vol (S)to
denote the standard Eulcidean volume of S. In local coordinates ( e1,···, ed) on a Finsler manifold M,
theBusemann-Hausdorﬀ volume form is deﬁned as dVF=VF(x)e1∧ ··· ∧ ed, with:
VF(x) =vol(Bd(1))
vol({v∈ TxM|F(v)<1}).
We can interpret the volume as the ratio between the euclidean ball, and a convex ball whose radius is
deﬁned as a unit Finsler metric. If the Finsler metric is replaced by a Riemannian metric, the volume of
the indicatrix will be an ellipsoid whose semi-axis are equal to the inverse of the squareroot of the metric’s
eigenvalues. The Finsler volume then reduces to the deﬁnition of the Riemannian volume.
A.4 Random manifolds
So far, we have only considered deterministic data points lying on a manifold. If we consider our data to be
random variables, we will need to deﬁne the associated random metric and manifold.
18Under review as submission to TMLR
M
G1G2G3{ft}={f1, f2, f3,···}
Figure 10: Usually the immersion fwould be deterministic. In the case of most generative models, where fis
described by a GP-LVM, or the decoder of a VAE, the immersion is stochastic. The pullback metric is stochastic de
facto.
As said previously, if we have a function f:Rq→RDthat parametrises a manifold, then we can construct
a Riemannian metric G=J/latticetop
fJf, with Jfthe Jacobian of the function f. In the previous cases, we assumed
fto be a deterministic function, and so is the metric. We construct a stochastic Riemannian metric in the
same way, with fbeing a stochastic process. A stochastic process f:Rq→RDis a random map in the
sense that samples of the process are maps from RqtoRD(the so-called sample paths of the process).
Deﬁnition A.11. A stochastic process f:Rq→RDis smooth if the sample paths of fare smooth.
We call a smooth process f a stochastic immersion if the Jacobian matrix of its sample paths has full
rank everywhere. We can then deﬁne the stochastic Riemannian metric G=J/latticetop
fJf.
The terms stochastic andrandom are used interchangeably. The deﬁnition of the stochastic immersion is
fairly important, as it means that its Jacobian is full rank. Since the Jacobian is full rank, the random
metric Gis positive deﬁnite, a necessary condition to deﬁne a Riemannian metric. Another deﬁnition of a
stochastic Riemannian metric would be the following:
Deﬁnition A.12. Astochastic Riemannian metric onRqis a matrix-valued random ﬁeld on Rq
whose sample paths are Riemannian metrics. A stochastic manifold is a diﬀerentiable manifold
equipped with a stochastic Riemannian metric.
Any matrice drawn from this stochastic metric would be a proper Riemannian metric. When using the
random Riemannian metric on two vectors u, v∈ TxM,G(u, v) =u/latticetopGvis a random variable, but both u, v
are deterministic vectors. From this deﬁnition, it follows that the length, the energy and the also volume
are random variables.
Object Riemann Finsler
metric g:TxM× TxM→R F:TM→R+
length structure LG(γ) =/integraltext/radicalbig
gt( ˙γ(t),˙γ(t))dtLF(γ) =/integraltext
F( ˙γ(t))dt
energy structure EG(γ) =/integraltext
gt( ˙γ(t),˙γ(t))dt EF(γ) =/integraltext
F( ˙γ(t))2dt
volume element VG(x) =/radicalbig
|det{G}| VF(x) =vol(Bn(1))/vol({v∈ TxM|F(x, v)<1})
Busemann-Hausdorﬀ volume measure
Table 1: Comparison of Riemannian and Finsler metrics.
19Under review as submission to TMLR
B Proofs
One of the main challenges of this paper is to ﬁnd coherent notations while respecting the tradition of two
geometric ﬁelds. In Riemannian geometry, we principally use a metric, noted gp:TpM × T pM → R+,
that is deﬁned as an inner product and thus can induce a norm, but is not a norm. In Finsler geometry,
we call intercheangeably Finsler function, Finsler metric or Finsler norm, the norm traditionally noted
F:T M → R+, with Fp(u):=F(p, u)deﬁned at a point p∈ M for a vector u∈ TpM. We will assume that
all our metric are always deﬁned for a speciﬁc point z(orp) on our manifold Z(orM), and so we will just
drop this index. The following notations will be used:
Stochastic pullback metric tensor G=J/latticetop
fJf
Stochastic pullback metric ˜g:(u, v)→u/latticetopGu
Expected Riemannian metric g:(u, v)→u/latticetopE[G]v
Stochastic pullback induced norm /bardbl·/bardblG:u→√
u/latticetopGu
Expected Riemannian induced norm /bardbl·/bardblR:u→/radicalbig
u/latticetopE[G]u:=/radicalbig
g(u, u)
Finsler metric /bardbl·/bardblF:u→E[√
u/latticetopGu]:=F(u)
B.1 Finslerian geometry of the expected length
In this section, we will always let f:Rq→RDbe a stochastic immersion, Jfits Jacobian, and G=J/latticetop
fJf
a metric tensor. We will ﬁrst prove that the function F:TM→R:v→E/bracketleftBig√
v/latticetopGv/bracketrightBig
is a Finsler metric.
Then, for the speciﬁc case where Jffollows a non-central normal distribution, the Finsler metric Fde-
ﬁned as the expected length follows a non-central Nakagami distribution and can be expressed in closed form.
To prove that the function Fis indeed a Finsler metric, we will need to verify the criteria above, among them
the strong convexity criterion is less trivial to prove than the others. It will be detailed in Lemma B.1.3.
Strong convexity means that the Hessian matrix1
2Hess (F(v)2) =1
2∂2F2
∂vivj(v)is strictly positive deﬁnite for
non-negative v. This matrix, when Fis a Finsler function, is also called the fundamental form and plays an
important role in Finsler geometry. To prove the strong convexity criterion, we will need the full expression
of the fundamental form, detailed in Lemma B.1.1.
Lemma B.1.1. The Hessian matrix1
2Hess (F(v)2)of the function F(v) =E/bracketleftBig√
v/latticetopGv/bracketrightBig
is given by
1
2Hess (F(v)2) =E/bracketleftBig
(v/latticetopGv)1
2/bracketrightBig
E/bracketleftBig
(v/latticetopGv)−1
2G−(v/latticetopGv)−3
2Gvv/latticetopG/bracketrightBig
+E/bracketleftBig
(v/latticetopGv)−1
2G/bracketrightBig2
vv/latticetop.
Proof. LetGbe a random positive deﬁnite symmetric matrix and deﬁne g:Rq→R:v/mapsto→√
v/latticetopGv,
where vis considered a column vector. We would like to know the diﬀerent derivatives of gwith respect
tov. We name by default JgandHg, its Jacobian and Hessian matrix. Using the chain rule, we have:
Jg= (v/latticetopGv)−1
2v/latticetopGandHg= (v/latticetopGv)−1
2G−(v/latticetopGv)−3
2(Gvv/latticetopG).
For the rest of the proof, we need to show that derivatives and expectation values commute.
Using the Fubini theorem, we can show that tha derivatives and the expectation values commute.
ForF:Rq→R:v/mapsto→E[√
v/latticetopGv],
Hess (F) =E[Hg] =E/bracketleftBig
(v/latticetopGv)−1
2G−(v/latticetopGv)−3
2Gvv/latticetopG/bracketrightBig
∇F=E[Jg] =E[(v/latticetopGv)−1
2Gv].
We now consider the function h:Rq→R:v/mapsto→E[√
v/latticetopGv]2=F(v)2. Using the chain rule and changing
the order of expectation and derivatives, we have its Hessian
Hh= 2F·Hess [F] + 2∇F/latticetop∇F= 2E[g]E[Hg] + 2E[Jg]/latticetopE[Jg].
20Under review as submission to TMLR
Finally, replacing JgandHgpreviously obtained in this expression, we conclude:
1
2Hh(x, v) =E/bracketleftBig
(v/latticetopGv)1
2/bracketrightBig
E/bracketleftBig
(v/latticetopGv)−1
2G−(v/latticetopGv)−3
2Gvv/latticetopG/bracketrightBig
+E/bracketleftBig
(v/latticetopGv)−1
2G/bracketrightBig2
vv/latticetop.
Remark. Before going further, it’s important to note that G=J/latticetop
fJfis a random matrix that is positive
deﬁnite: it is symmetric by deﬁnition and has full rank. The later statement is justiﬁed by the assumption
that the stochastic process f:Rq→RDis an immersion, then Jfis full rank.
Lemma B.1.2. The function F(v) =E/bracketleftBig√
v/latticetopGv/bracketrightBig
is:
1.positive homogeneous: ∀λ∈R+,F(λv) =λF(v)
2.smooth: F(v)is aC∞function on the slit tangent bundle T M \ { 0}
Proof. 1) Let λ∈R, then we have: F(λv) =E/bracketleftBig√
λ2v/latticetopGv/bracketrightBig
=|λ|/bracketleftBig√
v/latticetopGv/bracketrightBig
.
2) The multivariate function: Rq\{0} →R∗
+:v→v/latticetopGvisC∞and strictly positive, since G=J/latticetop
fJfis
positive deﬁnite. The function R∗
+→R∗
+:x→√xis also C∞. Finally, R∗
+→R∗
+:x→E[x]is by deﬁnition
diﬀerentiable. By composition, F(v)is aC∞function on the slit tangent bundle T M \ { 0}.
Lemma B.1.3. The function F(v) =E/bracketleftBig√
v/latticetopGv/bracketrightBig
satisﬁes the strong convexity criterion.
Proof. Proving that F satisﬁes the strong convexity criterion is equivalent to show that the Hessian matrix
H=1
2Hess (F(v)2)is strictly positive deﬁnite. Thus, we need to prove that ∀w∈Rq\{0}, w/latticetopHw > 0.
According to Lemma B.1.1, because the expectation is a positive function, it’s straightforward to see that
∀w∈Rq\{0}, w/latticetopHw≥0. The tricky part of this proof is to show that w/latticetopHw > 0. This can be obtained if
one of the terms ( F·Hess (F)or∇F/latticetop∇F) is strictly positive.
First, let’s decompose Has the sum of matrices: H=FHess (F) +∇F/latticetop∇F(Lemma B.1.1), with:
F·Hess (F) =E/bracketleftBig
(v/latticetopGv)1
2/bracketrightBig
E/bracketleftbigg
(v/latticetopGv)−3
2/parenleftBig
(v/latticetopGv)G−Gv(Gv)/latticetop/parenrightBig/bracketrightbigg
,
∇F/latticetop∇F=E/bracketleftBig
(v/latticetopGv)−1
2G/bracketrightBig2
vv/latticetop.
We will study two cases: when w∈span (v), and when w /∈span (v). We will always assume that v/negationslash= 0, and
so by deﬁnition: F(v)>0.
Letw∈span (v). We will show that w/latticetop∇F/latticetop∇Fw > 0. We have w=αv, α ∈R. Because F is 1-
homogeneous and using Euler theorem, we have: ∇F(v)v=F(v). Then (αv)/latticetop∇F/latticetop∇F(αv) =α2F2, and
α2F(v)2>0.
Letw /∈span (v). F being a scalar function, we have: w/latticetopFHess [F]w=Fw/latticetopHess [F]w. We would like
to show that: w/latticetopHess [F]w > 0. The strategy is the following: if we prove that the kernel of Hess [F]
is equal to the span (v), then w /∈span (v)is equivalent to say that w /∈ker(Hess [F])and we can con-
clude that: w/latticetopHess [F]w > 0. Let’s prove span (v)∈ker(Hess (F)). We know that Hess (F)v= 0,
since F is 1-homogeneous, so we have span (v)∈ker(Hess (F)). To obtain the equality, we just need
to prove that the dimension of the kernel is equal to 1. Let z∈span (v/latticetopG)/latticetop, which is (Gv)Tz= 0.
We have dim(span (v/latticetopM)) = 1 , and thus: dim(span (v/latticetopG)/latticetop) = q−1. Furthermore, z/latticetopHess [F]z=
z/latticetopE/bracketleftBig
M(v/latticetopMv)−1
2/bracketrightBig
z > 0, so we can deduce that dim(im(Hess [F])) = q−1. Using the Rank-Nullity
theorem, we conclude that dim(ker( Hess (F))) = q−dim(im(Hess [F])) = 1 , which concludes the proof.
In conclusion, ∀w∈Rq\{0}, w/latticetop1
2Hess (F(v)2)w > 0. The function Fsatisﬁes the strong convexity criterion.
21Under review as submission to TMLR
Proposition 2.2. LetGbe a stochastic Riemannian metric. Then, the function Fz:TzZ →R:u→
/bardblu/bardblFdeﬁnes a Finsler metric, but it is not induced by a Riemannian metric.
Proof. Let’s deﬁne F as a Riemannian metric: F:Rq×Rq→R:(v1, v2)→E/bracketleftBig/radicalbig
v/latticetop
1Gv2/bracketrightBig
. IfFwere a
Riemannian metric, then it would be bilinear, which is clearly not the case. Thus, Fis not a Riemannian
metric. According to Lemma B.1.2 and Lemma B.1.2, Fis a Finsler metric.
Proposition 2.3. Letfbe a Gaussian process and Jits Jacobian, with J∼ N (E[J],Σ). The Finsler
norm can be written as:
Fz:TzZ →R+:/bardblv/bardblF:=v→√
2√
v/latticetopΣvΓ(D
2+1
2)
Γ(D
2)1F1/parenleftbigg
−1
2,D
2,−ω
2/parenrightbigg
,
with 1F1as the conﬂuent hypergeometric function of the ﬁrst kind and ω= (v/latticetopΣv)−1(v/latticetopE[J]/latticetopE[J]v).
Proof. The objective of the proof is to show that, if the Jacobian Jffollows a non-central normal
distribution, then, ∀v∈Rq, the expectation E[v/latticetopJ/latticetop
fJfv]will follow a non-central Nakagami distribution.
This is a particular case of the derivation of moments of non-central Wishart distributions, previously shown
and studied by Kent & Muirhead (1984); Hauberg (2018b).
By hypothesis, Jffollows a non-central normal distribution: Jf∼ N (E[J], ID⊗Σ). Then, G=J/latticetop
fJf
follows a non-central Wishart distribution: G∼ W d(D,Σ,Σ−1E[J]/latticetopE[J]). According to (Kent & Muirhead,
1984, Theorem 10.3.5.), v/latticetopGvwill also follow a non-central Wishart distribution: v/latticetopGv∼ W 1(D, v/latticetopΣv, ω),
with: ω= (v/latticetopΣv)−1(v/latticetopE[J]/latticetopE[J]v).
To compute E[√
v/latticetopGv], we shall look at the derivation of moments. (Kent & Muirhead, 1984,
Theorem 10.3.7.) states that: if X∼ W q(D,Σ,Ω/prime), with q≤D, then E[(det( X))k] =
(det{Σ})k2qkΓq(D
2+k)
Γq(D
2)1F1(−k,D
2,−1
2Ω/prime). We directly apply the theorem to our case, knowing that v/latticetopGv
is a scalar term, so det/parenleftbig
v/latticetopGv/parenrightbig
=v/latticetopGv,q= 1, and k=1
2:
/bardblv/bardblF:=E[√
v/latticetopGv] =√
2√
v/latticetopΣvΓ(D
2+1
2)
Γ(D
2)1F1(−1
2,D
2,−1
2ω)
Lemma B.1.4. The length of a curve using a Finsler metric is invariant by reparametrisation.
Proof. The proof is similar to the one obtained on a Riemannian manifold (Lee (2013), Proposition 13.25),
where we make use of the homogeneity property of the Finsler metric.
Let (M, F) be a Finsler manifold and γ:[a, b]→ M a piecewise smooth curve segment. We call ˜γa
reparametrisation of γ, such that ˜γ=γ◦φwith φ:[c, d]→[a, b]a diﬀeomorphism. We want to show that
LF(γ) =LF(˜γ).
LF(˜γ) =/integraldisplayd
cF(˙˜γ(t))dt=/integraldisplayd
cF(d
dt(γ◦φ(t)))dt
=/integraldisplayφ−1(b)
φ−1(a)|˙φ(t)|F(˙γ◦φ(t))dt=/integraldisplayb
aF(˙γ(t))dt=LF(γ)
Lemma B.1.5. If a curve globally minimizes its energy on a Finsler manifold, then it also globally minimizes
its length and the Finsler function Fof the velocity vector along the curve is constant.
22Under review as submission to TMLR
Proof. The curve energy and the curve length are deﬁned as: EF(γ) =/integraltext1
0F2( ˙γ(t))dtandLF(γ) =/integraltext1
0F( ˙γ(t))dt, with γ:[0,1]→Rd. Let’s deﬁne fandgtwo real-valued functions such that: f:R→R:t/mapsto→
F( ˙γ(t))andg:R→R:t/mapsto→1. Applying Cauchy-Schwartz inequality, we directly obtain:
/parenleftBigg/integraldisplay1
0F( ˙γ(t))dt/parenrightBigg2
≤/integraldisplay1
0F( ˙γ(t))2dt·/integraldisplay1
012dt, which means: LF(γ)2≤EF(γ).
The equality is obtained exactly when the functions fandgare proportional, hence, when the Finsler
function is constant.
B.2 Comparison of Riemannian and Finsler metrics
We have deﬁned both a Riemannian ( g:(v1, v2)→v/latticetop
1E[G]v2) and a Finsler ( F:(x, v)→E[√
v/latticetopGv])
metric, in the hope to compute the average length between two points on a random manifold created by
the random ﬁeld f:G=J/latticetop
fJf. The main idea of this section is to better compare those two metrics and
in what extend they diﬀer in terms of length, energy and volume. From now on, f:Rq→RDwill always
be deﬁned as a stochastic non-central gaussian process. Its Jacobian Jfalso follows a non-central gaussian
distribution, G=J/latticetop
fJfa non-central Wishart distribution, and F:(x, v) =E[√
v/latticetopGv]a non-central
Nakagami distribution (Proposition 2.3). The Finsler metric can be written in closed form.
In section B.2.1, we will see that the Finsler metric is upper and lower bounded by two Riemannian tensors
(Proposition 3.1), and we can deduce an upper and lower bound for the length, the energy and the volume
(Corollary 3.1). Then, in section B.2.2, we will show that the relative diﬀerence between the Finsler norm and
the Riemannian induced norm is always positive and upper bounded a term that is inversely proportional to
the number of dimensions D(Proposition 3.3). Similarly, we will deduce the same for the length, the energy
and the volume (Corollary 3.2). From this last results, we can directly conclude in section B.2.3 that both
metrics are equal in high dimensions (Corollary 3.4). A possible interpretation is that in high dimensions
the data distribution obtained on those manifolds becomes more and more concentrated around the mean,
reducing the variance term to zero. The manifold becoming deterministic, both metrics become equal.
Remark. Most of the following proofs will be a bit technical, as they rely on the closed form expression
of the non-central Nakagami distribution. Once proving the main propositions, obtaining the corollaries is
straightforward. While we do not have closed form expression of the indicatrix, we will show that it’s a
monotoneous function which can upper and lower bounded.
B.2.1 Bounds on the Finsler metric
Proposition 3.1. We deﬁne α= 2/parenleftbigg
Γ(D
2+1
2)
Γ(D
2)/parenrightbigg2
. The Finsler norm: /bardbl·/bardblFis bounded by two norms,
/bardbl·/bardblαΣand/bardbl·/bardblR, induced by the two respective Riemannian metric tensors: the covariance tensor αΣz
and the expected metric tensor E[Gz].
∀(z, v)∈ Z × T zZ:/bardblv/bardblαΣ≤ /bardblv/bardblF≤ /bardblv/bardblR
Proof. Let’s ﬁrst recall that the Finsler function can be written as:
/bardblv/bardblF:=F(v) =√
2√
v/latticetopΣvΓ(D
2+1
2)
Γ(D
2)·1F1(−1
2,D
2,−1
2ω).
The conﬂuent hypergeometric function is deﬁned as: 1F1(a, b, z ) =/summationtext∞
k=0(a)k
(b)kzk
k!,with (a)kand (b)kbeing
the Pochhammer symbols. Note that, despite their confusing notation, they are deﬁned as rising factorials.
By deﬁnition, we have:(a)k
(b)k=Γ(a+k)
Γ(b+k)Γ(b)
Γ(a). We can use the Kummer transformation to obtain:
23Under review as submission to TMLR
1F1(a, b,−z) =e−z1F1(b−a, b, z ). Replacing a=−1
2,b=D
2andz=1
2ω, we ﬁnally get:
F(v) =√
2√
v/latticetopΣv·e−z∞/summationdisplay
k=0Γ(D
2+1
2+k)
Γ(D
2+k)zk
k!.
1) Let’s show that: ∀v∈ TxM:√
v/latticetopαΣv≤F(v), with α= 2/parenleftbigg
Γ(D
2+1
2)
Γ(D
2)/parenrightbigg2
.
The Pochhammer symbole is deﬁned as (x)k=x(x+ 1). . .(x+k−1) =Γ(x+k)
Γ(x). For x∈R∗
+, we have:
(x)k≤(x+1
2)k. Thus,Γ(D
2+k)
Γ(D
2)≤Γ(D
2+1
2+k)
Γ(D
2+1
2). The Gamma function being strictly positive on R+, we obtain:
Γ(D
2+1
2)
Γ(D
2)≤Γ(D
2+1
2+k)
Γ(D
2+k)
√
2√
v/latticetopΣvΓ(D
2+1
2)
Γ(D
2)·e−z∞/summationdisplay
k=0zk
k!≤√
2√
v/latticetopΣv·e−z∞/summationdisplay
k=0Γ(D
2+1
2+k)
Γ(D
2+k)zk
k!
√
2Γ(D
2+1
2)
Γ(D
2)√
v/latticetopΣv≤√
2√
v/latticetopΣv·e−z∞/summationdisplay
k=0Γ(D
2+1
2+k)
Γ(D
2+k)zk
k!
√
v/latticetopαΣv≤F(v).
2) Let’s show that: ∀v∈ TxM:F(v)≤/radicalbig
v/latticetopE[G]v.
Wendel (1948) proved:Γ(x+y)
Γ(x)≤xy, for x > 0andy∈[0,1]. With x=D
2+k,y=1
2, we obtained
Γ(D
2+1
2+k)
Γ(D
2+k)≤/radicalBig
D
2+k, which leads to: F(v)≤√
2v/latticetopΣv·e−z/summationtext∞
k=0/radicalBig
D
2+kzk
k!.
Furthermore,/summationtext∞
k=0e−zzk
k!= 1 and the function x→/radicalBig
D
2+xis concave. Then by Jensen’s in-
equality: e−z/summationtext∞
k=0/radicalBig
D
2+kzk
k!≤/radicalBig
D
2+e−z/summationtext∞
k=0zk
k!k.Knowing that/summationtext∞
k=0zk
k!=zez, we have:
e−z/summationtext∞
k=0/radicalBig
D
2+kzk
k!≤/radicalBig
D
2+z.
And with z=Ω
2, we obtain: F(v)≤/radicalbig
v/latticetopΣ(D+ Ω)v.
From (Kent & Muirhead, 1984, p. 442), the expectation of a non-central Wishart distribution ( G∼
Wq(D,Σ,Ω)) is:E[G] =DΣ + ΣΩ . This ﬁnally leads to:
F(v)≤/radicalBig
v/latticetopE[G]v.
Remark. As a side note, the second part of the inequality F(v)≤/radicalbig
v/latticetopE[G]vcan be obtained using directly
Proposition 3.2.
Corollary 3.1. The length, the energy and the Busemann-Hausdorﬀ volume of the Finsler metric are
bounded respectively by the Riemannian length, energy and volume of the covariance tensor αΣ(noted
LαΣ, EαΣ, VαΣ) and the expected metric E[G](noted LR, ER, VR):
∀z∈ Z, LαΣ(z)≤LF(z)≤LR(z)
EαΣ(z)≤EF(z)≤ER(z)
VαΣ(z)≤VF(z)≤VR(z)
24Under review as submission to TMLR
Proof. From Proposition 3.1, we have ∀(x, v)∈ M × T xM:/radicalbig
h(v)≤F(v)≤/radicalbig
g(v), with h:v→v/latticetopαΣv
andg:v→v/latticetopE[G]vRiemannian metrics. We also deﬁne the parametric curve: ∀t∈R, γ(t) =xand
˙γ(t) =v.
1) Let’s show that LΣ(x)≤LF(x)≤LR(x). Because of the monotonicity of the Lebesgue integrals, we
directly have:/integraltext/radicalbig
h( ˙γ(t))dt≤/integraltext
F( ˙γ(t))dt≤/integraltext/radicalbig
g( ˙γ(t))dt.
2) Let’s show that EΣ(x)≤EF(x)≤ER(x). Since all the functions are positive, we can raise
them to the power two, and again, with the monotonicity of the Lebesgue integrals, we have:/integraltext
h( ˙γ(t))dt≤/integraltext
F2(γ(t),˙γ(t))dt≤/integraltext/radicalbig
g( ˙γ(t))dt.
3) Let’s show that VΣ(x)≤VF(x)≤VR(x). We write the vectors v∈ T xMin hyperspherical coordi-
nates: v=re, with r=/bardblv/bardblthe radial distance and ethe angular coordinates. With v=re, we have:
r·/radicalbig
h(e)≤r·F(e)≤r·g(e)⇐⇒/radicalbig
h(e)−1≥F(e)−1≥/radicalbig
g(e)−1.
We want to identify an inequality between the indicatrices, noted vol (Ih), vol (Ig), vol (IF), formed by the
functions h,gandF. Let’s deﬁne: rg/radicalbig
h(e) =rh/radicalbig
g(e) =rFF(e) = 1 . For every angular coordinate
e, we obtain: rh≥rF≥rg. Intuitively, this means that the ﬁnsler indicatrix will always be bounded
by the indicatrices formed by handg. The Busemann-Hausdorf volume of a function fis deﬁned as:
σB(f) = vol(Bn(1))/vol(If),with vol (Bn(1)) the volume of the unit ball and vol (If)the volume of the
indicatrix formed by f. The previous inequality and the deﬁnition of the Busemann-Hausdorﬀ volume
implies that: vol (Ih)≥vol(IF)≥vol(Ig)⇒σB(h)≤σB(F)≤σB(g). The functions gandhbeing
Riemannian, we have: σB(h) =/radicalbig
det(αΣ)andσB(g) =/radicalbig
det(E[G]), which concludes the proof.
B.2.2 Relative bounds between the Finsler and the Riemannian metric
Proposition 3.2. Letfbe a stochastic immersion. finduces the stochastic norm /bardbl·/bardblG, deﬁned in
Section 2. The relative diﬀerence between the Finsler norm /bardbl·/bardblFand the Riemmanian norm /bardbl·/bardblRis:
0≤/bardblv/bardblR− /bardblv/bardblF
/bardblv/bardblR≤Var/bracketleftBig
/bardblv/bardbl2
G/bracketrightBig
2E/bracketleftBig
/bardblv/bardbl2
G/bracketrightBig2.
Proof. We will directly use a sharpen version of Jensen’s inequality obtained by Liao & Berg (2019): Let
X be a one-dimensional random variable with mean µandP(X∈(a, b)) = 1 , where −∞ ≤ a≤b≤+∞.
Letφa twice derivable function on (a, b). We further deﬁne: h(x, µ) =φ(x)−φ(µ)
(x−µ)2−φ/prime(µ)
x−µ. Then:
inf
x∈(a,b){h(x, µ)}Var[X]≤E[φ(x)]−φ(E[x])≤sup
x∈(a,b){h(x, µ)}Var[X].
In our case, we will chose φ:z→√zwith za one-dimensional random variable deﬁned as z=v/latticetopGv.a= 0,
b= +∞andµ=E[z].h(z, µ) = (√z−√µ)(z−µ)−2−(2(z−µ)√µ)−1. Because its ﬁrst derivative φ/primeis
convex, the function x→h(x, µ)is monotonically increasing. Thus:
inf
z∈(0,+∞){h(x, µ)}= lim
z→0=−√µ
2µ2and sup
z∈(0,+∞){h(x, µ)}= lim
z→+∞= 0.
It ﬁnally gives:
−√µ
2µ2Var[z]≤E[√z]−/radicalbig
E[z]≤0.
25Under review as submission to TMLR
Replacing /bardblv/bardblF:=F(v) =E[√z]and/bardblv/bardblR:=/radicalbig
g(v) =/radicalbig
E[z] =√µconcludes the proof.
Lemma B.2.1. Letz∼ W 1(D, σ, Ω)following a one-dimensional non-central Wishart distribution. Then:
Var[z]
2E[z]2=1
D+ Ω+Ω
(D+ Ω)2
Proof. (Kent & Muirhead, 1984, Theorem 10.3.7.) states that if z∼ W 1(D, σ, ω )thenE[zk] =
σk2kΓ(D
2+k)
Γ(D
2)1F1(−k,D
2,−1
2Ω). In particular, for k= 1 andk= 2, we have 1F1(−1, b, c) = 1 −c
band
1F1(−2, b, c) = 1 −2c
b+c2
b(b+1). We also haveΓ(D
2+1)
Γ(D
2)=D
2andΓ(D
2+2)
Γ(D
2)=D
2/parenleftBig
D
2+ 1/parenrightBig
, which leads to:
E[z] =σ(D+ Ω) andE[z2] =σ2(2ω+ 2(D+ω) + (D+ω)2).Finally, we conclude:
Var[z]
E[z]2=E[z2]
E[z]2−1 =2ω
(D+ω)2+2
D+ω.
Proposition 3.3. Letfbe a Gaussian process. We note ω= (v/latticetopΣv)−1(v/latticetopE[J]/latticetopE[J]v), with Jthe
jacobian of f, and Σthe covariance matrix of J.
The relative ratio between the Finsler norm /bardbl·/bardblFand the Riemmanian norm /bardbl·/bardblRis:
0≤/bardblv/bardblR− /bardblv/bardblF
/bardblv/bardblR≤1
D+ω+ω
(D+ω)2.
Proof. The result is directly obtained using Proposition 3.2 and Lemma B.2.1.
Corollary 3.2. When fis a Gaussian Process, the relative ratio between the length, the energy and
the volume of the Finsler norm (noted LF, EF, VF) and the Riemannian norm (noted LR, ER, VR) is:
0≤LR(z)−LF(z)
LR(z)≤max
v∈TzZ/braceleftbigg1
D+ω+ω
(D+ω)2/bracerightbigg
0≤ER(z)−EF(z)
ER(z)≤max
v∈TzZ/braceleftBigg
2
D+ω+1 + 2 ω
(D+ω)2+2ω
(D+ω)3+ω2
(D+ω)4/bracerightBigg
0≤VR(z)−VF(z)
VR(z)≤1−/parenleftBigg
1−max
v∈TzZ/braceleftbigg1
D+ω+ω
(D+ω)2/bracerightbigg/parenrightBiggq
Proof. Let’s call M= max v∈TxM{ω
(D+ω)2+1
D+ω}.
From Proposition 3.3, we have:
0≤ /bardblv/bardblR− /bardblv/bardblF≤M/bardblv/bardblR,with M= max
v∈TxM/braceleftbigg1
D+ω+ω
(D+ω)2/bracerightbigg
1) By the monocity of the Lesbesgue integral, we can directly integrate the previous norms along a curve γ,
which immediately leads to: 0≤LR(x)−LF(x)≤ML R(x).
2) Since all the functions are positive: 0≤ /bardbl v/bardblF≤ /bardbl v/bardblR≤M/bardblv/bardblR+/bardblv/bardblFleads to:
/bardblv/bardbl2
F≤ /bardblv/bardbl2
R≤M2/bardblv/bardbl2
R+ 2M/bardblv/bardblF/bardblv/bardblR+/bardblv/bardbl2
F, and replacing /bardblv/bardblF≤ /bardblv/bardblRin the right hand
term: /bardblv/bardbl2
F≤ /bardblv/bardbl2
R≤(M2+ 2M)/bardblv/bardbl2
R+/bardblv/bardbl2
F, and ﬁnally: 0≤ /bardblv/bardbl2
R− /bardblv/bardbl2
F≤(M2+ 2M)/bardblv/bardbl2
R. Again,
by continuity of the Lebesgue integral, we directly obtain: 0≤ER(x)−EF(x)≤(M2+ 2M)ER(x).
26Under review as submission to TMLR
3) In order to compare the volume between the Finsler and the Riemannian metric, we need to compare
the volume of their indicatrices, noted: vol (Ig)and vol (IF)respectively. We write the vectors v∈ TxM
in hypershperical coordinates, with v=re,r=/bardblv/bardblthe radial distance and ethe angular coordinates.
The volume of the indicatrices obtained in dimension q(dimension of the latent space) can be written as:
dqV=rq−1drdΦ, with Φdeﬁning the diﬀerent angles. We will note rFandrgthe radial distances of the
Finsler and Riemann metrics such that: /bardblv/bardblF=rF/bardble/bardblF= 1and/bardblv/bardblR=rg/bardble/bardblR= 1obtained for a speciﬁc
angle e.
vol(IF)−vol(Ig) =/integraldisplay
Φ/parenleftbigg/integraldisplayrf
0rq−1dr−/integraldisplayrg
0rq−1dr/parenrightbigg
dΦ =/integraldisplay
Φrq
f
q
1−/parenleftBigg
rg
rf/parenrightBiggq
dΦ≤/integraldisplay
Φrq
f
qdΦ·
1−/parenleftBigg
rg
rf/parenrightBiggq
,
and by deﬁnition: vol (IF) =/integraltext
Φ(rq
f/q)dΦ. Furthermore, for a speciﬁc angle e, we have: rg/rF=/radicalbig
g(e)/F(e)≥1−M, from Proposition 3.3. We have:
0≤vol(IF)−vol(Ig)
vol(IF)≤ ·1−/parenleftBigg
rg
rf/parenrightBiggq
≤1−(1−M)q,
and by the deﬁnition of the Busemann Hausdorﬀ volume:VF(x)−VG(x)
VF(x)=vol(IF)−vol(Ig)
vol(IF), we conclude the
proof.
B.2.3 Implications in High Dimensions
In this section, we want to show that the diﬀerence between the Finsler norm and the Riemannian induced
norm, as well as their respective functionals, tend to zero at a rate of O(1
D). We need to be sure that ω
doesn’t grow faster than D, in other terms: ω=O(D). This can be obtained if we assume that every
element of the expectation of Jacobian is upper bounded ( ∃m∈R∗
+,∀i, jE[Jij]≤m). This happens
in at least two cases: (1) E[f]is somehow Lipschitz continuous; or (2) if fis a Gaussian Process and
its covariance is upper bounded. The latter case happens when the process is deﬁned over a bounded domain.
Lemma B.2.2. Our Finsler metric v→E[√
v/latticetopGv]is deﬁned with v/latticetopGv∼ W 1(D, v/latticetopΣv, ω), and ω=
(v/latticetopΣv)−1(v/latticetopE[J]/latticetopE[J]v).
If the Finsler manifold is bounded, then: ω≤DM, with M∈R+.
Proof. By deﬁnition, Σdoes not depend on D. We assume the manifold is bounded, which means that every
element of the expected Jacobian is upper bounded: E[J]ij≤m, with m∈R∗
+. We call σ=v/latticetopΣv∈R∗
+.
We have:
ω=σ−1D/summationdisplay
k=1q/summationdisplay
i=1q/summationdisplay
j=1viE[J]kiE[J]kjvj≤σ−1D/summationdisplay
k=1m2/bardblv/bardbl2≤DM,
with M=σ−1m2/bardblv/bardbl2∈R∗
+, and M does not depend on D.
Corollary 3.3. Letfbe a Gaussian Process. In high dimensions, we have:
LR(z)−LF(z)
LR(z)=O/parenleftbigg1
D/parenrightbigg
ER(z)−EF(z)
ER(z)=O/parenleftbigg1
D/parenrightbigg
VR(z)−VF(z)
VR(z)=O/parenleftbiggq
D/parenrightbigg
27Under review as submission to TMLR
And, when Dconverges toward inﬁnity: LR∼
+∞LF, ER∼
+∞EFandVR∼
+∞VF.
Proof. From Corollary 3.2, we directly obtained the results in high dimensions.
We assume that our latent space is bounded, then by B.2.2, we have: 0≤ω≤MD, with M∈R+.
For the length, we have:
LG(x)−LF(x)
LG(x)≤max
v∈TxM/braceleftbigg1
D+ω+ω
(D+ω)2/bracerightbigg
≤1 +M
D
For the energy functional, we have:
EG(x)−EF(x)
EG(x)≤max
v∈TxM/braceleftBigg
2
D+ω+1 + 2 ω
(D+ω)2+2ω
(D+ω)3+ω2
(D+ω)4/bracerightBigg
≤2 + 2 M
D+1 + 2 M+M2
D2
lim sup
D→∞D×EG(x)−EF(x)
EG(x)≤lim sup
D→∞2(1 + M) +M2+ 2M+ 1
D2→2(1 + M)
For the volume, we have:
VG(x)−VF(x)
VG(x)≤1−/parenleftBigg
1−max
v∈TxM/braceleftbigg1
D+ω+ω
(D+ω)2/bracerightbigg/parenrightBiggq
≤1−/parenleftbigg
1−1 +M
D/parenrightbiggq
Using Taylor series expansion, when x∼0, we have: 1−(1−x)q=qx+o(x2). Let’s call ε/lessmuch1, and rewrite
the Taylor series:
VG(x)−VF(x)
VG(x)≤q1 +M
D+εq1 +M
D
lim sup
D→∞D
q×VG(x)−VF(x)
VG(x)≤(1 +M)(1 + ε)
The diﬀerence between the functionals can converge to zero if they are similar in high dimensions, or if they
all diverge to inﬁnity. This latter case does not happen as we assume the latent manifold being bounded,
and so the metrics are then ﬁnite, which concludes the proof.
Corollary 3.4. Letfbe a Gaussian Process. In high dimensions, the relative ratio between the Finsler
norm/bardbl·/bardblFand the Riemmanian norm /bardbl·/bardblRis:
/bardblv/bardblR− /bardblv/bardblF
/bardblv/bardblR=O/parenleftbigg1
D/parenrightbigg
And, when Dconverges toward inﬁnity: ∀v∈ TzZ,/bardblv/bardblR∼
+∞/bardblv/bardblF.
Proof. Similar to the 3.3, assuming that our latent space is bounded, from B.2.2, we have 0≤ω≤MD.
From 3.3, we deduce:
28Under review as submission to TMLR
0≤/bardblv/bardblR− /bardblv/bardblF
/bardblv/bardblR≤1
D+ω+ω
(D+ω)2
≤1 +M
D
In a bounded manifold, the metric are ﬁnite. We can deduce that they converge to each other in high
dimensions.
C Experiments
C.1 Datasets
C.1.1 Font data
The dataset represents 46 diﬀerent font for each letter (upper and lower case) whose contour is parametrised
by a spline (or two splines, depending on the letter used) obtained from at least 500 points Campbell &
Kautz (2014).
In our case, we choose to learn the manifold of the letter f. The dataset is composed of 46 diﬀerent fonts,
each letter being drawn by 1024 points. We reduce this number from 1024 to 256 by sampling one point
every 4. The dimension of the observational space is then 256.
C.1.2 qPCR
The qPCR data, gathered from Guo et al. (2010), was used to illustrate the training of a GPLVM in Pyro
Pyro (2022) and is available at the Open Data Science repository Ahmed et al. (2019). It consists of 437
single-cell qPCR data for which the expression of 48 genes has been measured during 10 diﬀerent cell stages.
We then have 437 data points, 48 observations, and 10 classes. Before training the GP-LVM, the data is
grouped by the capture time, as illustrated in the Pyro documentation.
C.1.3 Pinwheel on a sphere
A pinwheel in 2-dimension is created and then projected onto a sphere using a stereographic projection
method. The ﬁnal dataset is composed of 1000 points with their coordinates in 3-dimensions.
C.2 GP-LVM training
We learn our two-dimensional latent space by training a GP-LVM Lawrence (2003) with Pyro Bingham et al.
(2019). The Gaussian Process used is a Sparse GP, deﬁned with a kernel (RBF, or Matern) composed of a
one-dimensional lengthscale and variance. The parameters are learnt with the Adam optimiser Kingma &
Ba (2014). The number of steps and the initialisation of the latent space vary with the dataset.
datasets pinwheel font data qPCR
Number of data points 500 46 437
Number of observations 3 256 48
initialisation PCA PCA custom
kernel RBF Matern52 Matern52
steps 17000 5000 5000
learning rate 1e-3 1e-4 1e-4
lengthscale 0.24 0.88 0.15
variance 0.95 0.30 0.75
noise 1e-4 1e-3 1e-3
Table 2: Description of the datasets trained with a GP-LVM.
29Under review as submission to TMLR
C.3 Computing indicatrices
An indicatrix of a function gat a point xis deﬁned such that: v∈ T xM|gx(v)<1. In other terms,
the indicatrix is the representation of a unit ball in our latent space. If we use an euclidean metric, our
indicatrix in our 2-dimensional latent space would be a unit ball, as we need to solve: v∈ TxM,/bardblv/bardbl<1.
For a Riemannian metric, our indicatrix is necessarily an ellipse, whose semi axis are the square-roots of the
eigenvalues of the metric tensor G::v∈ TxM,v/latticetopGv < 1. For our Finsler metric, we don’t have an analytical
solution, and so it’s diﬃcult to predict the shape of the convex polygon.
In this paper, the indicatrices are drawn the following way: for a single point in our latent space, we compute
the value of v/latticetopGvandF(x, v)forvvarying over the space. We then extract the contour when v/latticetopGvand
F(x, v)are equal to 1. Computing the area of the indicatrices will be used in the section C.4 to compute
the volume measures.
C.4 Computing the volume forms
For the ﬁgures used in this paper, by default, the background of the latent space represents the volume
measure of the expected Riemannian metric ( VG=/radicalbig
E[G]) on a logarithm scale. In ﬁgure 3, the volume
measure of the Finsler metric is also computed.
C.4.1 Finsler metric
To compute the volume measure of our Finsler metric, we choose the Busemann-Hausdorﬀ deﬁnition, which
is the ratio of a unit ball over the volume of its indicatrix: V=vol(Bn(1))/vol({v∈ TxM|F(x, v)<1}).
While our Finsler function has an analytical form, its expression doesn’t allow to directly solve the equation:
v∈ TxM, F (x, v)<1. Instead we approximate its indicatrix as describe in C.3, using a contour plot and
extracting the paths vertices. We can then compute the area of the obtained polygon, and divide with the
volume of a unit ball: vol (B2(1)) = π.
The volume measure can then be computed for each point over a grid (32 x 32, in ﬁgure 3), and we interpolate
all the other points. Note that this method can only be used when our latent space is of dimension 2.
C.4.2 Expected Riemannian metric
There is two ways to compute the volume measure of the expected Riemannian metric. One way is to directly
use the metric tensor: VG=/radicalbig
E[G]. Another one is to remember that any Riemannian metric is a Finsler
metric, and thus, the Busemann-Hausdorﬀ deﬁnition also applied for our metric: VG=vol(Bn(1))/vol({v∈
TxM|v/latticetopE[G]v <1}). Solving v/latticetopE[G]v <1forv∈ TxMis equivalent to solving the area of an ellipse.
For the ﬁrst method, we can either sample multiple times the metric, which is computationally expensive, or
use the fact that our metric tensor is a non-central Wishart matrix: G=J/latticetopJ∼ W q(D,Σ,Σ−1E[J]/latticetopE[J]),
with Σthe covariance of the Jacobian JandDthe dimension of the observational space. In this case, its
expectation is: E[G] =E[J]/latticetopE[J] +DΣ. We can access the derivatives of the function f(detailed in section
D.2), and compute both quantities E[J]and Σneeded to estimate the expected metric and its determinant.
For the second method, we can compute the area of the ellipse in the same way we compute the Finsler
volume measure.
C.5 Experiments when increasing the number of dimensions
In Figure 4, we computed the volume ratio and draw indicatrices while varying the number of dimensions,
to illustrate that both our Finsler metric and the expected Riemannian metric seem to converge when D
increases.
The main issue with this experiment is to vary only one factor, the number of dimensions D, while keeping
the other factors unchanged. This is diﬃcult for two reasons: 1) in real toy examples as seen in Figure 5, even
with a very low dimensional observational such as in the pinwheel on the sphere, both metrics are already
30Under review as submission to TMLR
very similar to each other. It would be diﬃcult to illustrate a convergence while increasing the number of
dimensions. 2) the function fneeds to be learnt again each time we increase the number of dimensions of
the observational space, and the parameters of the Gaussian Process will change too.
Instead, we try to illustrate our results by computing empirically the stochastic metric tensor G=J/latticetopJ,
using its Jacobian J∼ N (E[J],Σ), aD×qmatrix. The number of dimensions is modiﬁed by simply
truncatenating the Jacobian J. In Figure 4, the volume ratio is computed for 12 Jacobians obtained with
diﬀerent random parameters E[J]and Σ. The Finsler and Riemannian indicatrices (lower right) are drawn
for only one Jacobian selected randomly.
D Computations
D.1 Computing geodesics with Stochman and minimising the curve energy functionals
An essential task is to compute shortest paths, or geodesics, between data points in the latent space. Those
shortest paths can be obtained in two ways: either by solving a corresponding system of ODEs, or by
minimising the curve energy in the latent space. The former being computationally expensive, we favour
the second approach which consists in optimising a parameterised spline on the manifold. This method is
already implemented in Stochman Detlefsen et al. (2021), where we can easily optimise splines by redeﬁning
the curve energy function of a manifold class.
We need two curvge energy functional: one for the expected Riemannian metric and one for the Finsler
metric.
D.1.1 Curve energy for the Riemannian metric
We know that the stochastic metric tensor Gtdeﬁned on a point tfollows a non-central Whishart distribution.
Thus, we can compute its expectation E[Gt]knowing the Jacobian covariance and expectation: E[Jt]and Σ.
The next Section D.2 explains how to compute those quantities.
Assuming the spline is discretized into N points, we can compute the curve energy with:
EG(γ(t)) =/integraldisplay1
0˙γ(t)/latticetopE[Gt] ˙γ(t)dt≈N/summationdisplay
i=1˙γ/latticetop
i/parenleftBig
E[Ji]TE[Ji] +DΣi/parenrightBig
˙γi.
D.1.2 Curve energy for the Finsler metric
In order to compute of the curve energy E(γ), we must ﬁrst derive the expectation E[Jt]and covariance Σ
of the Jacobian of f, which should follow a normal distribution: Ji=∂fi∼ N/parenleftbig
E[J],Σ/parenrightbig
. We assume the
points ∂fiare independent samples with the same variance drawn from a normal distribution. We can then
compute the Finsler metric which follows a non-central Nakagami distribution (See Proposition 2.2):
E(γ(t)) =/integraldisplay1
0F(t,˙γ(t))2dt≈N/summationdisplay
i=12 ˙γ/latticetop
iΣi˙γi/parenleftBigg
Γ(D
2+1
2)
Γ(D
2)/parenrightBigg2
1F1/parenleftbigg
−1
2,D
2,−ωi
2/parenrightbigg2
,
with 1F1the conﬂuent hypergeometric function of the ﬁrst kind and ωi= ( ˙γ/latticetop
iΣi˙γi)−1( ˙γ/latticetop
iΩi˙γi)and Ωi=
Σ−1
iE[Ji]/latticetopE[Ji].
This function has been implemented in Pytorch using the known gradients for the hypergeometric function:
∂
∂x1F1(a, b, x ) =a
b1F1(a+ 1, b+ 1, x).
D.2 Accessing the posterior derivatives
We assume that the probabilitic mapping ffrom the latent variables Xto the observational variables Y
follows a normal distribution. We would like to obtain the posterior kernel Σ∗and expectation µ∗such that
p(∂tf|Y, X )∼ N (µ∗,Σ∗).
31Under review as submission to TMLR
We make the hypothesis the observed variables are modelled with a gaussian noise /epsilon1whose variance is the
same in every dimension. In particular, for the nthlatent ( x) and observed ( y) variable in the jthdimension:
yn,j=fj(xn,:) +/epsilon1n. Thus, the output variables have the same variance, and the posterior kernel Σ∗is then
isotropic with respect to the output dimensions: Σ∗=σ2
∗·ID.
There are two ways of obtaining the posterior variance and expectation:
•We use the gaussian processes to predict the derivative ( ∂cf) of the mapping function f, and
we multiply the obtained posterior kernel by the curve derivative ( ∂tc), following the chain rule:
d f(c(t))
dt=d f
dc·dc
dt(Section: D.2.1)
•We discretize the derivative of the mapping function as the diﬀerence of this function evaluated
at two close points. We use a linear operation to obtain the posterior variance and expectation:
∂tf(c(t))∼f(c(ti+1))−f(c(ti)). (Section: D.2.2)
D.2.1 Closed-form expressions
We assume that fis a Gaussian process. Hence, because the diﬀerentiation is a linear operation, the
derivative of a Gaussian Process is also a Gaussian Process Rasmussen & Williams (2005).
The data Y∈ RN×Dfollows a normal distribution, so we can infer the partial derivative of one data point
(JT)ji=∂yi
∂xj, with i= 1. . . D andj= 1. . . d. We have:
/bracketleftBigg
Y
(J)/latticetop/bracketrightBigg
=D/productdisplay
i=1N/parenleftBigg/bracketleftbiggµy
µ∂y/bracketrightbigg
,/bracketleftbiggK(x, x) ∂K(x, x∗)
∂K(x∗, x)∂2K(x∗, x∗)/bracketrightbigg/parenrightBigg
.
andJ/latticetopcan be predicted:
p(J/latticetop|Y, X ) =D/productdisplay
i=1N(µ∗,Σ∗),
with:
µ∗=∂K(x∗, x)·K(x, x)−1·(y−µy) +µ∂y
Σ∗=∂2K(x∗, x∗)−∂K(x∗, x)·K(x, x)−1·∂K(x, x∗).
Finally, ∂tfis obtained:
p(∂tf(c(t))|f(x), x) =D/productdisplay
i=1N/parenleftBig
˙cµ∗,˙c/latticetopΣ∗˙c·ID/parenrightBig
.
D.2.2 Discretization
One can notice that: ∂tf(c(t))∼f(c(ti+1))−f(c(ti)). We know that f(c(ti+1))andf(c(ti))both follows a
normal distribution.
/bracketleftbiggf(c(ti))
f(c(ti+1))/bracketrightbigg
=D/productdisplay
j=1N
/bracketleftbiggµi
µi+1/bracketrightbigg
,/bracketleftBigg
σ2
ii σ2
i,i+1
σ2
i+1,iσ2
i+1,i+1/bracketrightBigg
.
IfY=AXaﬃne transformation of a multivariate Gaussian X∼ N (µ, σ2), then Y is also a multivariate
Gaussian with: Y∼ N (Aµ, ATσ2A). In our case, we choose AT= [−1,1]. We have:
f(c(ti+1))−f(c(ti))∼ N (µ∗, σ2
∗·ID),
32Under review as submission to TMLR
with:
µ∗=µi+1−µi
σ2
∗=σ2
ii+σ2
i+1,i+1−2σ2
i,i+1
Figure 11: Illustration of the derivatives obtained with a trained GP on a simple parametrised function: both methods
give the correct derivatives if enough points are sampled.
33