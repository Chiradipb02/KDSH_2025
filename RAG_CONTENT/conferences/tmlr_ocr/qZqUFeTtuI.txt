Published in Transactions on Machine Learning Research (1/2025)
Statistical Mechanics of Min-Max Problems
Yuma Ichikawa ichikawa-yuma1@g.ecc.u-tokyo.ac.jp, ichikawa.yuma@fujitsu.com
Department of Basic Science, University of Tokyo
Fujitsu Limited
Koji Hukushima k-hukushima@g.ecc.u-tokyo.ac.jp
Department of Basic Science, University of Tokyo
Reviewed on OpenReview: https: // openreview. net/ forum? id= qZqUFeTtuI
Abstract
Min-max optimization problems, also known as saddle point problems, have attracted
significant attention due to their applications in various fields, such as fair beamforming,
generative adversarial networks (GANs), and adversarial learning. However, understanding
the properties of these min-max problems has remained a substantial challenge. This study
introduces a statistical mechanical formalism for analyzing the equilibrium values of min-
max problems in the high-dimensional limit, while appropriately addressing the order of
operations for min and max. As a first step, we apply this formalism to bilinear min-max
games and simple GANs, deriving the relationship between the amount of training data and
generalization error and indicating the optimal ratio of fake to real data for effective learning.
This formalism provides a groundwork for a deeper theoretical analysis of the equilibrium
properties in various machine learning methods based on min-max problems and encourages
the development of new algorithms and architectures.
1 Introduction
Min-max optimization problems, also known as saddle point problems, are well-known classical optimization
problems extensively studied in the context of zero-sum games (Wald, 1945; Von Neumann & Morgenstern,
1947). These problems have diverse applications across various fields, such as game theory, machine learning,
and signal processing. In game theory, min-max problems arise in zero-sum games where one player’s gain
corresponds to another’s loss. Several methods have been proposed to find the min-max value or equilibrium
points in these games (Dem’yanov & Pevnyi, 1972; Maistroskii, 1977; Bruck, 1977; Lions, 1978; Nemhauser
& Wolsey, 1988; Freund & Schapire, 1999). In machine learning, min-max games are relevant for training
generative adversarial networks (GANs) (Goodfellow et al., 2020; Arjovsky et al., 2017), Additionally, in
adversarial learning, these problems are employed to train models that are robust to adversarial attacks by
optimizing a worst-case perturbed loss function (Szegedy et al., 2013; Goodfellow et al., 2014b; Papernot
et al., 2016; Madry et al., 2017),
Despite the widespread application of min-max optimization problems, several challenges still need to be
addressed, including understanding the usefulness of these min-max formulations, evaluating the convergence
properties of the algorithms, and conducting sensitivity analyses of min-max values. A promising approach
to addressing these issues is to analyze the typical-case behavior of min-max problems by examining the
min-max value averaged over random instances drawn from distributions that capture realistic settings,
referred to as randomized instance ensembles . Statistical-mechanical approaches, which have demonstrated
their effectiveness in analyzing the typical-case behavior of randomized instance ensembles of optimization
and constraint-satisfaction problems (Mézard & Parisi, 1986; Fontanari, 1995), provide a powerful formalism
for such analyses. Extending this formalism to analyze the typical-case behavior of min-max values thus
presents a potential direction for further research, although this has not yet been fully explored.
1Published in Transactions on Machine Learning Research (1/2025)
This study applies the statistical mechanical formalism to min-max problems, modeling them as a virtual
two-temperature system. This formalism enables a sensitivity analysis of the typical-case min-max values
in the high-dimensional limit. Notably, this formalism properly addresses the order of min-max operations,
critical in non-convex scenarios where interchanging the order of min and max can lead to incorrect results
(Razaviyayn et al., 2020). Using this formalism, we analyze typical-case min-max values of bilinear min-max
games and simple GANs. In particular, we derive the relationship between the amount of training data and
generalization error and indicate the optimal ratio of fake data to real data for effective learning.
Our main contributions are as follows:
•We introduce a statistical-mechanical formalism developed for sensitivity analysis of equilibrium
values in high-dimensional min-max problems.
•Applying this approach, we conduct a detailed sensitivity analysis on a bilinear min-max game to
verify the theoretical validity of our approach.
•Building on this formalism, we analyze the generalization performance of GANs and determine the
optimal ratio between fake and real data for practical training.
2 Related Work
The replica method, which is employed in this study, is a non-rigorous but powerful heuristic approach in
statistical physics (Edwards & Anderson, 1975; Mézard et al., 1987; Mezard & Montanari, 2009). It has
been proven to be a valuable method for high-dimensional machine-learning problems. Previous studies have
investigated the relationship between dataset size and generalization error in supervised learning, including
single-layer (Gardner & Derrida, 1988; Opper & Haussler, 1991; Barbier et al., 2019; Aubin et al., 2020) and
multi-layer (Aubin et al., 2018) neural networks, as well as kernel methods(Dietrich et al., 1999; Bordelon
et al., 2020; Gerace et al., 2020). In unsupervised learning, the replica method has also been applied to
dimensionality reduction techniques such as the principal component analysis (Biehl & Mietzner, 1993; Hoyle
& Rattray, 2004; 2007), and to generative models such as energy-based models (Decelle et al., 2018; Ichikawa
& Hukushima, 2022) and denoising autoencoders (Cui & Zdeborová, 2023). However, the dataset-size
dependence of GANs has not been previously analyzed, which this study aims to address.
Related to our work, a statistical mechanical formalism for addressing min-max problems has been proposed
(Varga, 1998). However, the treatment of the inverse temperature limit differs from our approach, and it has
limitations in accurately handling the order of the min and max operations. In the context of adversarial
learning, which involves a non-convex and concave min-max problem, Tanner et al. (2024) analyzes a tractable
setting where the internal maximization can be solved. By reducing such cases to standard optimization
problems, they apply the replica method and approximate message passing to explore the core phenomenology
observed in the adversarial robustness. Even in cases where the internal maximization cannot be explicitly
solved, the formalism discussed here provides a basis for further analysis and potential extensions to more
complex scenarios.
Notation Here, we summarize the notations used in this study. We use the shorthand expression [N] =
{1,2,...,N}, whereN∈N.Id∈Rd×ddenotes ad×didentity matrix, and 1ddenotes the vector
(1,..., 1)⊤∈Rdand0ddenotes the vector (0,..., 0)⊤∈Rd. For a matrix A= (Aij)∈Rd×kand a vector
a= (ai)∈Rd, we use the shorthand expressions dA=∆/producttextd
i=1/producttextk
j=1dAijandda=∆/producttextd
i=1dai, respectively. The
notationOdx,dy(1)describes the asymptotic order of a function with respect to the parameters dxanddy.
Specifically, a function f(dx,dy)is said to beOdx,dy(1)if it remains bounded as dxanddygrow large (or
tend toward some specified limit), independently of dxanddy. The standard Gaussian measure is defined
asDz=∆dze−∥z∥2/2/(2π)n/2. The notation extrxf(x)represents the evaluation of a function f(x)at its
extremum with respect to the variable x. Specifically, this shorthand implies locating and evaluating f(x)at
points where its gradient ∇xf(x) =0.
2Published in Transactions on Machine Learning Research (1/2025)
3 Statistical Physics Formalism for Min-Max Optimization Problems
This section introduces a statistical-mechanical formalism that models min-max problems as a virtual two-
temperature system from a statistical mechanics perspective. Min-max problems are formally expressed
as
Ψ(A) = min
x∈Xmax
y∈YV(x,y;A),s.t.x∈X⊆ Rdx,y∈Y⊆ Rdy, (1)
whereV(·,·) :Rdx×Rdy→Ris a bivariate function; x∈Rdxandy∈Rdyare the optimization variables; X
andYare the feasible sets; Ais a parameter characterizing the problem, e.g., graph G. We introduce the
following Boltzmann distribution to analyze min-max problems for a given bivariate function V(x,y;A)in
Eq. (1), with virtual inverse temperatures βmin∈Randβmax∈R:
pβmin,βmax(x;A) =∆ 1
Z(βmin,βmax,A)e−βmin/parenleftbig
1
βmaxln/integraltext
YdyeβmaxV(x,y;A)/parenrightbig
,
whereZ(βmin,βmax,A)is the normalization constant, also known as the partition function. Hereafter, we refer
to it as the partition function. In this context, a two-temperature system is particularly important because it
allows us to distinguish between the opposing optimization objectives inherent in min-max problems, similar
to the different thermal behaviors in statistical mechanics.
By taking the limit βmax→+∞followed by βmin→+∞, the distribution lim
βmin→+∞lim
βmax→+∞pβmin,βmax(x;A)
concentrates on a uniform distribution over the min-max values, assuming that well-defined min-max values
exist forV(·,·)and the min-max value is bounded over feasible sets XandY. Note that the order of these
limits is crucial because the min and max operations cannot be interchanged in non-convex and non-concave
min-max problems (Razaviyayn et al., 2020), i.e., minx∈Xmaxy∈YV(x,y;A)̸=maxy∈Yminx∈XV(x,y;A).
While a similar formulation has been used in the previous work (Varga, 1998), they simultaneously take the
limits of both βminandβmaxwith a fixed ratio βmin/βmax=Oβmin,βmax(1), which does not fully capture the
distinct effects of min and max operations in non-convex settings. Such an approach generally does not yield
accurate results when the function V(x,y;A)is non-convex with respect to xandy.
Statistical-mechanical approaches have demonstrated their effectiveness in analyzing the typical-case behavior
specifically, the properties of the optimal value averaged over the instances that follow a distribution p(A)– for
optimization and constraint-satisfaction problems (Mézard & Parisi, 1986; Fontanari, 1995). These analyses
have succeeded in providing insights into different aspects of combinatorial optimization, unlike worst-case
analysis. This work also focuses on evaluating the typical cases of min-max problems characterized by a
random parameter A. Our main objective is to calculate the logarithm of Z(βmin,βmax,A)averaged over the
random variables Ain the limit βmax→∞followed by βmin→∞:
Ω = lim
βmin→∞lim
βmax→∞f(βmin,βmax),
where
f(βmin,βmax) =∆−1
βmindxEA[logZ(βmin,βmax,A)],
which is referred to as the free energy density. This free energy is a generating function with variables xand
y. Appendix C shows how to calculate the function of the optimal value xthrough this free energy.
Setting the ratio of the inverse temperatures as p=−βmin/βmax, this can be rewritten as
f(βmin,βmax) =−1
βmindxEA/bracketleftbigg
log/integraldisplay
Xdxe−βmin/parenleftbig
1
βmaxln/integraltext
dyeβmaxV(x,y;A)/parenrightbig/bracketrightbigg
,
=−1
βmindxEA/bracketleftbigg
log/integraldisplay
Xdx/parenleftbigg/integraldisplay
YdyeβmaxV(x,y;A)/parenrightbiggp/bracketrightbigg
. (2)
Although calculating the expectation value of the logarithm is generally difficult, we begin by using the
identity
EA[logf(A)] = lim
γ→+01
γlogEA[(f(A))γ] (3)
3Published in Transactions on Machine Learning Research (1/2025)
to expand the logarithmic form as follows:
Ω =−lim
βmin→∞lim
βmax→∞lim
γ→+01
βmindxγlog˜Zγ(βmin,βmax), (4)
where
˜Zγ(βmin,βmax) =∆EA/bracketleftbigg/parenleftbigg/integraldisplay
Xdx/parenleftbigg/integraldisplay
YdyeβmaxV(x,y;A)/parenrightbiggp/parenrightbiggγ/bracketrightbigg
. (5)
At this point, note that these transformations are purely algebraic identities without assuming the parameters
γorpto be integers.
Following the idea of the replica method (Edwards & Anderson, 1975; Parisi, 1979; 1983; Zdeborová &
Krzakala, 2016; Gabrié, 2020), we then proceed under the assumption that γandpare natural numbers.
Specifically, rather than addressing Eq (4) directly real values γandp, one calculates the average of the γ-th
andp-th powers for γ,p∈N, then performs an analytic continuation to γ,p∈Rfor this expression, and
finally takes the limits γ→+0,βmin→+∞andβmax→+∞. Based on this replica “trick”, the calculation
simplifies to the replicated partition function Zγ(βmin,βmax)as an approximation of ˜Zγ(βmin,βmax):
˜Zγ(βmin,βmax)≈Zγ(βmin,βmax) =∆EA/bracketleftiggγ/productdisplay
a=1/integraldisplay
Xadxap/productdisplay
l=1/integraldisplay
Yaldyaleβmax/summationtext
a,lV(xa,yal;A)/bracketrightigg
,(6)
up to the first order of γto take the γ→+0limit on the right- hand side of Eq. (4). This computation
is a standard procedure in the statistical physics of interaction systems including random variables, and is
generally accepted as exact, although rigorous proof has not yet been provided. Specifically, the mathematical
rigor of the method remains limited due to the unproven uniqueness of the analytic continuation, an issue
noted for the moment problem (Tanaka, 2007). As noted in Section 2, the replica method has provided
various results in high dimensional statistics and machine learning as well.
Additionally, before taking the limits, βmin→∞andβmax→∞, the concept of finite inverse temperatures
βminandβmaxcorresponds to scenarios where neither the minimum nor the maximum is fully achieved, a
common situation in the min-max algorithms. This approach provides valuable insights into cases where
neither extreme is fully realized or both are only partially optimized. Exploring novel algorithms based on
this finite-temperature generalization of min-max problems represents an intriguing direction for future work.
Furthermore, in game theory, this formalism can be interpreted as a framework for modeling games under
relaxed assumptions of complete rationality, where players xandyare assumed to behave with bounded
rationality rather than adhering strictly to classical models of fully rational behavior (Von Neumann &
Morgenstern, 1947).
In the following sections, we apply this formalism to a fundamental and significant bilinear min-max game,
demonstrating that the analytic continuation of pin the replica method is a rigorous operation. We then
analyze the minimal model of GANs as a more practical example.
4 Bilinear Min-max Games
This min-max formalism introduces two replica parameters: γ, associated with the randomness of A, andp,
related to the dual structure of min-max problems. The analytic continuation with respect to the replica
parameter γis widely recognized as effective and is frequently employed in the statistical mechanics of
optimization. However, the analytic continuation of the replica parameter phas not yet been explored.
While establishing its mathematical validity presents challenges, this study eliminates the influence of the
replica parameter γassociated with the randomness of Aand rigorously demonstrates that the analytic
continuation with respect to pholds for fundamental bilinear min-max games. Specifically, we show that
the free energy density derived using the replica trick in Eq. (6), as explained in Section 3, is equivalent to
the exact expression in Eq. (5) derived without analytic continuation of γandpfor bilinear min-max games
(Tseng, 1995; Daskalakis et al., 2017).
Bilinear games are regarded as a fundamental example for studying new min-max optimization algorithms
and techniques (Daskalakis et al., 2017; Gidel et al., 2019; 2018; Liang & Stokes, 2019). Mathematically,
4Published in Transactions on Machine Learning Research (1/2025)
bilinear zero-sum games can be formulated as the following min-max problem:
min
x∈{0,1}dxmax
y∈{0,1}dyV(x,y;W),
whereV(·,·)is given by
V(x,y;W) =1
2dxx⊤Wxxx+1
2dyy⊤Wyyy+1/radicalbig
dxdyx⊤Wxyy+x⊤bx+y⊤by,
where W= (Wxx,Wyy,Wxy,bx,by). For simplicity, we assume Wxx=wxx1dx×dx∈Rdx×dx,Wxy=
wxx1dx×dy∈Rdx×dy,Wyy=wyy1dy×dy∈Rdy×dy,bx=bx1dx∈Rdx, and by=by1dy∈Rdy. The following
results can be readily extended to the matrices Wxx,Wyy, andWxywith a limited number of eigenvalues of
Odx,dy(1). For a detailed discussion, refer to Appendix B.
In this setting, the analytically continued free energy density f(βmin,βmax;W)calculated using replicated
partition function Zγ(βmin,βmax)in Eq. (6) coincides with the exact free energy density ˜f(βmin,βmax;W)
from the partition function ˜Zγ(βmin,βmax)in Eq. (5).
Theorem 4.1 For anyβmin,βmax∈Randwxx,wxy,wyy,bx,by∈R, the following equality holds:
f(βmin,βmax;W) =˜f(βmin,βmax;W),
where
f(βmin,βmax;W) = extr
mx,my/bracketleftigg
wxx
2(mx)2+κwyy
2(my)2+wxyκ1/2mxmy
+bxmx+κbymy−1
βminH(mx) +κ
βmaxH(my)/bracketrightigg
,
whereκ=dy/dx,H(x) =−xlog(x)−(1−x)log(1−x)denotes binary cross entropy, and extrdenotes the
extremum operation.
This theorem establishes the validity of the analytic continuation for the replica parameter pusing Eq. (6)
for bilinear min-max games. The detailed proof of this theorem is provided in Appendix A.
5 Generative Adversarial Networks
Generative adversarial networks (GANs) (Goodfellow et al., 2020) aim to model high-dimensional probability
distributions based on training datasets. Despite significant progress in practical applications (Arjovsky
et al., 2017; Lucic et al., 2018; Ledig et al., 2017; Isola et al., 2017; Reed et al., 2016), several issues are
yet to be resolved, including how the amount of training data influences generalization performance and
how sensitive GANs are to specific hyperparameters. This section analyzes the relationship between the
amount of training data and generalization error. Additionally, we conduct a sensitivity analysis on the ratio
of fake data generated by the generator to the amount of training data, which is critical for the training of
GANs. Our analysis employs a minimal setup that captures the intrinsic structure and learning dynamics
of GANs (Wang et al., 2019). We consider the high-dimensional limit, where the number of real and fake
samples,nand˜n, respectively, and the dimension dare large while remaining comparable. Specifically, we
analyze the regime in which n,˜n,d→∞while maintaining a comparable ratio, i.e., α=n/d= Θ(d0)and
˜α=˜n/d= Θ(d0), commonly referred to as sample complexity.
5.1 Settings
Generative model for the dataset We consider that a training dataset D={xµ}n
µ=1, where each
xµ∈Rdis drawn from the following distribution:
xµ=1√
dw∗cµ+√ηnµ, (7)
5Published in Transactions on Machine Learning Research (1/2025)
where w∗∈Rdis a deterministic feature vector, cµ∈Ris random scalar drawn from a standard normal
distribution p(c) =N(c; 0,1),nµis a background noise vector whose components are i.i.d. from the standard
normal distribution N(n;0d,Id), andη∈Ris a scalar parameter to control the strength of the noise. We
also assume that ∥w∗∥2= 1. This generative model, known as the spiked covariance model (Wishart, 1928;
Potters & Bouchaud, 2020), has been studied in statistics to analyze the performance of unsupervised learning
methods such as PCA (Ipsen & Hansen, 2019; Biehl & Mietzner, 1993; Hoyle & Rattray, 2004), sparse PCA
(Lesieur et al., 2015), deterministic autoencoders (Refinetti & Goldt, 2022), and variational autoencoder
(Ichikawa & Hukushima, 2024; 2023).
GAN model Following Wang et al. (2019), we assume that the generator has the same linear structure as
the dataset generative model described in Eq. (7):
g(z;w) =∆1√
dwz+/radicalbig
˜η˜n, (8)
where w∈Rdis a learnable parameter, z∈Ris a latent variable drawn from a standard normal distribution
p(z) =N(z; 0,1),˜nis a noise vector whose components are i.i.d. from the standard normal distribution
N(˜n;0d,Id), and ˜η∈Ris a scalar parameter to control the strength of the noise.
We also define the linear discriminator as
ψ(x;v) =∆f/parenleftbigg1√
dv⊤x/parenrightbigg
, (9)
where xis an input vector, which can be either the real data xµfrom Eq. (7) or the fake one g(z˜µ;w)from
Eq. (8). The vector v∈Rdis a learnable parameter, and f:R→Rcan be any function.
Training algorithm The GAN is trained by solving the following min-max optimization problem:
min
w∈Rdmax
v∈RdV(w,v;D), (10)
where
V(w,v;D,˜D) =∆n/summationdisplay
µ=1ϕ(ψ(xµ;v))−˜n/summationdisplay
˜µ=1˜ϕ/parenleftbig
ψ(g(z˜µ;w);v)/parenrightbig
−λ
2∥v∥2+˜λ
2∥w∥2, (11)
and ˜D={zµ}˜n
˜µ=1are the latent values of the fake data. The last two terms are regularization terms, where
λand˜λcontrol the regularization strength. This value function defined in Eq. (11) is a general form that
includes various types of GANs. Specifically, when ϕ=˜ϕand∥ϕ∥L≤1, it represents a Wasserstein GANs
(WGANs) (Arjovsky et al., 2017) and, when ϕ(x) =logσ(x)and ˜ϕ(x) =−log(1−σ(x))withσbeing
the sigmoid function, it corresponds to the Vanilla GANs, which minimize the JS-divergence (Goodfellow
et al., 2014a). As we assumes a linear discriminator, V(w,v;D,˜D)can be expressed as a function of linear
combinations v⊤xµ/√
dand v⊤g(z˜µ;w)/√
das follows:
V(w,v;D,˜D) =n/summationdisplay
µ=1ϕ/parenleftbigg1√
dv⊤xµ/parenrightbigg
−˜n/summationdisplay
˜µ=1˜ϕ/parenleftbigg1√
dv⊤g(z˜µ;w)/parenrightbigg
−λ
2∥v∥2+˜λ
2∥w∥2, (12)
where, for clarity in the subsequent analysis, we redefined ϕand ˜ϕas functions of the linear combinations
v⊤xµ/√
dand v⊤g(z˜µ;w)/√
d.
Generalization error In the ideal case where the generator perfectly learns the underlying true probability
distribution, we have w∗=w. Therefore, we define the generalization error εgas
εg(¯w,w∗) =∆1
dED/bracketleftbig
∥¯w−w∗∥2/bracketrightbig
, (13)
where ¯wdenotes the min-max optimal value in Eq. (10). The generalization error, εg, quantifies the accuracy
of signal recovery from the training data.
6Published in Transactions on Machine Learning Research (1/2025)
5.2 Replica Calculation
We apply the replica formalism sketched in Section 3 to derive a set of deterministic equations characterizing
the typical behavior of GANs.
In this problem setting, the replicated partition function Zγin Eq. (6) can be expressed as
Zγ(βmin,βmax) =γ/productdisplay
a=1/integraldisplay
Rddwap/productdisplay
l=1/integraldisplay
Rddval/parenleftbigg
Ec,neβmax/summationtext
alϕ/parenleftbig
1
d(val)⊤w∗c+√η
d(val)⊤n/parenrightbig/parenrightbiggn
×/parenleftbigg
Eze−βmax/summationtext
al˜ϕ/parenleftbig
1
d(val)⊤waz+√˜η
d(val)⊤˜n/parenrightbig/parenrightbigg˜n
eβmax
2/summationtext
al(˜λ∥wa∥2−λ∥val∥2).
To take the average over nand˜n, we notice that since nand˜nfollow a multivariate normal distribution
N(˜n;0d,Id), thequantities u= ((val)⊤n/√
d)a,land˜u= ((val)⊤˜n/√
d)a,lalsofollowaGaussianmultivariate
distribution as
p(u) =p(˜u) =N(0γp,Q),
where
Q= (Qab
ls)∈Rγp×γp, Qab
ls=1
d(val)⊤vbs.
To conduct further computations, we introduce auxiliary variables through the following identities:
1 =/productdisplay
ablsd/integraldisplay
δ(dQab
ls−(val)⊤vbs)dQab
ls=/productdisplay
ald/integraldisplay
δ(dma
l−(val)⊤w∗)dma
l=/productdisplay
ald/integraldisplay
δ(dba
l−(val)⊤wa)dba
l.
The replicated partition function can then be expressed as
Zγ(βmin,βmax) =/integraldisplay
dQdmdbeβmind(S(Q,m,b)+T(Q,m,b)),
where the entropic term S(Q,m,b)and energetic term T(Q,m,b)are defined as follows:
S(Q,m,b) =∆ 1
dβminln/integraldisplay/productdisplay
aldwadval/productdisplay
ablsd/integraldisplay
δ(dQab
ls−(val)⊤vbs)
×/productdisplay
ald/integraldisplay
δ(dma
l−(val)⊤w∗)d/integraldisplay
δ(dba
l−(val)⊤wa)eβmax
2/summationtext
al(˜λ∥wa∥2−λ∥val∥2),
T(Q,m,b) =∆α
βminln/parenleftbigg/integraldisplay
Dc/integraldisplay
dup(u)eβmax/summationtext
alϕ/parenleftbig
1
d(val)⊤w∗c+√η
d(val)⊤n/parenrightbig/parenrightbigg
+˜α
βminln/parenleftbigg/integraldisplay
Dz/integraldisplay
d˜up(˜u)e−βmax/summationtext
al˜ϕ/parenleftbig
1
d(val)⊤waz+√˜η
d(val)⊤˜n/parenrightbig/parenrightbigg
.
Using the Fourier representation of the delta function, S(Q,m,b)is further expressed as
S(Q,m,b) =1
dβminlog/integraldisplay
d˜Qd˜md˜bed(1
2tr˜QQ−˜m⊤m−˜b⊤b)
/parenleftigg/integraldisplay/productdisplay
aldwadvale−1
2/summationtext
abls˜Qab
lsvalvbs+w∗/summationtext
al˜ma
lval+/summationtext
al˜ba
lwaval+βmax
2/summationtext
al(˜λ(wa)2−λ(val)2)/parenrightiggd
.(14)
Replica symmetric ansatz Here, we assume the following symmetric structure:
∀a,b∈[γ],∀l,s∈[p], Qab
ls=q+∆
βminδab+χ
βmaxδlsδab, (15)
∀a,b∈[γ],∀l,s∈[p],˜Qab
ls=βmaxˆqδlsδab−β2
max
βminˆ∆δab−β2
maxˆχ, (16)
∀a∈[γ],l∈[p], ma
l=m, ˜ma
l=βmaxˆm, (17)
∀a∈[γ],l∈[p], ba
l=b,ˆba
l=βmaxˆb. (18)
7Published in Transactions on Machine Learning Research (1/2025)
This replica symmetric (RS) structure restricts the integration of the replicated weight parameters {wa},
{val}across the entire R(d×γp)×R(d×γp)to a subspace that satisfies the constraints in Eq. (15)–(18). This
structure, along with scaling by the maximum and minimum beta values, is similar to the standard one-step
replica symmetry breaking (1RSB) (Mézard et al., 1987; Takahashi & Kabashima, 2022).
We now turn to the entropic term S(Q,m,b). The terms that exclude the integrals with respect to {val}
and{wa}can be expressed as
1
2tr˜QQ−˜m⊤m−˜b⊤b
=γβmin/parenleftigg
−1
2/parenleftigg
ˆq/parenleftbigg
q+∆
βmin+χ
βmax/parenrightbigg
−χ/parenleftigg
ˆχ+ˆ∆
βmin/parenrightigg
+ ˆχ∆ + ˆ∆q+∆ˆ∆
βmin/parenrightigg
+ ˆmm+ˆbb/parenrightigg
.(19)
The term that includes the integrals with respect to {val}and{wa}can be expressed as
Ez
/integraldisplay/productdisplay
aldwaDζadvale−1
2βmax(ˆq+λ)/summationtext
al(val)2+βmax/summationtext
al/parenleftig/radicalig
ˆ∆
βminζa+√
ˆχz+w∗ˆm+waˆb/parenrightig
val−˜λβmin
2/summationtext
a(wa)2
,
=Ez
/integraldisplay/productdisplay
adwaDζa
/integraldisplay
dvae−1
2βmax(ˆq+λ)(va)2+βmax/parenleftig/radicalig
ˆ∆
βminζa+√
ˆχz+w∗ˆm+waˆb/parenrightig
va
p
e−˜λβmin
2/summationtext
a(wa)2
,
=Ez
/integraldisplay/productdisplay
adwaDζae−˜λβmin
2/summationtext
a(wa)2−βmin
2(ˆq+λ)/summationtext
a/parenleftig/radicalig
ˆ∆
βminζa+√
ˆχz+w∗ˆm+waˆb/parenrightig2
,
=Ez

/integraldisplay
dwdζeβmin/parenleftig
−1
2ζ2−˜λ
2w2−(√
ˆ∆ζ+√
ˆχz+w∗ˆm+wˆb)2
2(ˆq+λ)/parenrightig
γ
.
This can be derived using the identity, for any a∈R+and anyx∈R,ea
2x2=/integraltext
Dze√azx. Summarizing
these results, the entropic term can be written as
S(Q,m,b,˜Q,˜m,˜b) =γ/parenleftigg
−1
2/parenleftigg
ˆq/parenleftbigg
q+∆
βmin+χ
βmax/parenrightbigg
−χ/parenleftigg
ˆχ+ˆ∆
βmin/parenrightigg
+ ˆχ∆ + ˆ∆q+∆ˆ∆
βmin/parenrightigg
+ ˆmm+ˆbb/parenrightigg
+1
βmin/integraldisplay
Dzlog/integraldisplay
dwdζeβmin/parenleftig
−1
2ζ2−˜λ
2w2−(√
ˆ∆ζ+√
ˆχz+w∗ˆm+wˆb)2
2(ˆq+λ)/parenrightig/parenrightigg
.
By taking the limit as βmax→∞followed by βmin→∞, we obtain
S(Q,m,b,˜Q,˜m,˜b) =−γ
2/parenleftigg
q(ˆq+ˆ∆)−(χ−∆)ˆχ−2mˆm−2bˆb+˜λ( ˆm2+ ˆχ)
ˆb2+ (ˆq+ˆ∆ +λ)˜λ/parenrightigg
.
We next turn to the energetic term T(Q,m,b). Under the RS ansatz, ufollows
ual=/radicalbiggχ
βmaxxal+/radicaligg
∆
βminya+√qξ,
8Published in Transactions on Machine Learning Research (1/2025)
where ˜xal,xal,˜yal,yal,ξ, and ˜ξfollow the standard normal distribution N(˜ξ; 0,1). Then, the energetic term
T(Q,m,b)can be expand as
T(Q,m,b) =∆α
βminln/parenleftbigg/integraldisplay
Dc/integraldisplay
dup(u)eβmax/summationtext
alϕ/parenleftbig
1
d(val)⊤w∗c+√η
d(val)⊤n/parenrightbig/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(a)
+˜α
βminln/parenleftbigg/integraldisplay
Dz/integraldisplay
d˜up(˜u)e−βmax/summationtext
al˜ϕ/parenleftbig
1
d(val)⊤waz+√˜η
d(val)⊤˜n/parenrightbig/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(b).
The term (a) can be simplified as
(a)=α
βminlnEc,ξ/bracketleftigg/integraldisplay/productdisplay
alDyaDxaleβmax/summationtext
alϕ/parenleftbig
mc+√η/parenleftbig√χ
βmaxxal+/radicalbig
∆
βminya+√qξ/parenrightbig/parenrightbig/bracketrightigg
,
=α
βminlnEc,ξ/bracketleftbigg/parenleftbigg/integraldisplay
Dy/parenleftbigg/integraldisplay
Dxeβmaxϕ/parenleftbig
mc+√η/parenleftbig√χ
βmaxx+/radicalbig
∆
βminy+√qξ/parenrightbig/parenrightbig/parenrightbiggp/parenrightbiggγ/bracketrightbigg
,
=α
βminγEc,ξ/bracketleftbigg
log/integraldisplay
Dy/parenleftbigg/integraldisplay
Dxeβmaxϕ/parenleftbig
mc+√η/parenleftbig√χ
βmaxx+/radicalbig
∆
βminy+√qξ/parenrightbig/parenrightbig/parenrightbiggp/bracketrightbigg
+Oγ(γ2),
=α
βminγEc,ξ/bracketleftbigg
log/integraldisplay
dye−βmin
2y2/parenleftbigg/integraldisplay
dxe−βmax
2x2+βmaxϕ(mc+√η(√χx+√
∆y+√qξ))/parenrightbiggp/bracketrightbigg
+Oγ(γ2).
Taking the limit as βmax→∞followed by βmin→∞, we obtain:
(a)=αγEc,ξ/bracketleftbigg
max
y/braceleftbigg
−1
2y2−max
x/braceleftbigg
−1
2x2+ϕ/parenleftig
mc+√η/parenleftig√χx+√
∆y+√qξ/parenrightig/parenrightig/bracerightbigg/bracerightbigg/bracketrightbigg
.
Similarly, the term (b) is also expressed as
(b)=˜α
βminγEz,˜ξ/bracketleftbigg
ln/integraldisplay
d˜ye−βmin
2y2/parenleftbigg/integraldisplay
d˜xe−βmax
2˜x2−βmax˜ϕ(bz+√η(√χ˜x+√
∆˜y+√q˜ξ))/parenrightbiggp/bracketrightbigg
+Oγ(γ2).
Taking the same limits, we find:
(b)= ˜αγEz,˜ξ/bracketleftbigg
max
˜y/braceleftbigg
−1
2y2−max
˜x/braceleftbigg
−1
2˜x2−˜ϕ/parenleftig
bz+/radicalbig
˜η/parenleftig√χ˜x+√
∆˜y+√q˜ξ/parenrightig/parenrightig/bracerightbigg/bracerightbigg/bracketrightbigg
.
Putting the entropic term and energetic term together, the free energy density is given by
f= extr
ˆq,ˆχ,ˆm,ˆb
q,δ,χ,m,b1
2/parenleftigg
qˆq−(χ−∆)ˆχ−2(mˆm+bˆb) +˜λ( ˆm2+ ˆχ)
ˆb2+ (ˆq+λ)˜λ−2(αΦ(q,∆,χ,m,b ) + ˜α˜Φ(q,∆,χ,m,b ))/parenrightigg
,
where
Φ(q,∆,χ,m,b ) =Ec,ξ/bracketleftbigg
max
y/braceleftbigg
−1
2y2−max
x/braceleftbigg
−1
2x2+ϕ/parenleftig
mc+√η/parenleftig√χx+√
∆y+√qξ/parenrightig/parenrightig/bracerightbigg/bracerightbigg/bracketrightbigg
,(20)
˜Φ(q,∆,χ,m,b ) =Ez,˜ξ/bracketleftbigg
max
˜y/braceleftbigg
−1
2˜y2−max
˜x/braceleftbigg
−1
2˜x2−˜ϕ/parenleftig
bz+/radicalbig
˜η/parenleftig√χ˜x+√
∆˜y+√q˜ξ/parenrightig/parenrightig/bracerightbigg/bracerightbigg/bracketrightbigg
.(21)
Note that the min and max operations are involved in the two-level optimization described in Eqs. (20) and
(21).
5.3 Results: Application to Simple GANs
In this subsection, following Wang et al. (2019), we apply the formulation derived above to the simple WGAN
to demonstrate its generalization properties and conduct a sensitivity analysis of the ratio rfake to real data.
9Published in Transactions on Machine Learning Research (1/2025)
Figure 1: (Left) Generalization error as a function of sample complexity αfor different values of the ratio r.
(Right) Asymptotic generalization error limα→∞ε(α)as a function of the the ratio r.
Self-consistent Equations We consider the case where the functions ϕ(x)and ˜ϕ(x)are both quadratic,
defined asϕ(x) =˜ϕ(x) =x2/2. This setting allows for an explicit calculation of the free energy density, which
is given by
f= extr
q,χ,m,b
ˆq,ˆχ,ˆm,ˆb/bracketleftigg
1
2/parenleftigg
qˆq−χˆχ−2mˆm−2bˆb+˜λ( ˆm2+ ˆχ)
ˆb2+ (ˆq+λ)˜λ−α(ηq+m2)
ηχ−1−˜α(˜ηq+b2)
˜ηχ+ 1/parenrightigg/bracketrightigg
.(22)
To find the extremum in Eq. (22), we require that the gradient with respect to each order parameter equals
zero. This results in the following set of self-consistent equations:
q=˜λ2( ˆm2+ ˆχ)
(ˆb2+˜λ(ˆq+λ))2, χ=˜λ
ˆb2+˜λ(ˆq+λ), m =ˆm˜λ
ˆb2+˜λ(ˆq+λ), b=−ˆb˜λ( ˆm2+ ˆχ)
(ˆb2+˜λ(ˆq+λ))2,
ˆq=αη
ηχ−1+˜α˜η
˜ηχ+ 1,ˆχ=αη(qη+m2ρ)
(ηχ−1)2+˜α˜η(q˜η+d2ρ)
(˜ηχ+ 1)2,ˆm=αm
ηχ−1,ˆb=−˜αb
˜ηχ+ 1.
Learning Curve For simplicity, we set ˜α=rαandλ=˜λ=η=˜η= 1. Our analysis focuses on how the
generalization error depends on αwhile varying the ratio r, as generating fake data from the generator is
generally much easier than collecting real data. Fig. 1 (Left) shows the dependence of generalization error
on sample complexity αfor various values of the ratio r. The results demonstrate a sharp decline in the
generalization error as the ratio rincreases. However, when rbecomes large, the generalization error increases
in the region where αis large, eventually leading to a phase where no learning occurs, and the generalization
error equals 1. This implies that as αincreases, the learning becomes dominated by only fake data.
In contrast, for smaller r, real data consistently dominates the objective function V(w,v;D,˜D), resulting in
a steady decrease in generalization error. However, the reduced influence of the fake data component in the
objective function, which drives the learning of the generator, requires a significantly larger amount of real
data for effective generator training.
Asymptotic Generalization Error We next analyze the asymptotic behavior of the generalization error
when the sample complexity αbecomes sufficiently large. The asymptotic behavior of the generalization error
as a function of αis given by
εg=

1−2√1−r
rr
r+2√
2/parenleftbig√1−r
rr+r−1/parenrightbig
(r−1)rα1/2 +Oα(α−1)r≤1,
1 +Oα(α−1) r>1.
10Published in Transactions on Machine Learning Research (1/2025)
The results for α→∞are shown in Fig. 1(Right). The optimal ratio is r=1/2, indicating that using fake
data approximately equal to half of the real data is effective when the dataset approaches infinity. At r= 1,
a phase transition occurs, suggesting that the model changes from a phase of effective learning phase to one
where fake data becomes dominant. Beyond this point, for r≥1, the model fails to learn any meaningful
signal w∗, and the generalization error is 1.
Furthermore, when r=1/2, the generalization error scales as εg∼α−1, which represents the optimal
asymptotic behavior for a model-matched scenario. These results demonstrate the critical role of the ratio
rin determining learning performance. Therefore, tuning the ratio raccording to the available real data
is crucial for achieving optimal performance. In practice, it is known that in training GANs, the stability
of learning can deteriorate depending on the ratio rof fake to real data. This theoretical analysis provides
insights into the importance of the ratio rand is expected to contribute to improving learning algorithms.
6 Conclusion
This study introduces a statistical mechanical formalism to analyze high-dimensional min-max optimization
problems, focusing on the critical order of min and max operations in non-convex scenarios. Our goal was
to perform a sensitivity analysis of equilibrium values, providing new insights into their properties and
generalization performance.
We applied this approach to a simple min-max game, evaluated the generalization performance of GANs,
and derived the optimal ratio of fake to real data for effective learning. This successful application not
only validates the approach but also opens the way for extending this formalism to more complex min-max
problems and broader applications, suggesting a promising direction for significant advancements in machine
learning and optimization.
Acknowledgments
We thank T. Takahashi, K. Okajima, Y. Nagano, and K. Nakaishi for useful discussions and suggestions.
This work was supported by JST Grant Number JPMJPF2221 and JPSJ Grant-in-Aid for Scientific Research
Number 23H01095. Additionally, YI was supported by the WINGS-FMSP program at the University of
Tokyo.
References
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In
International Conference on Machine Learning , pp. 214–223. PMLR, 2017.
Benjamin Aubin, Antoine Maillard, Florent Krzakala, Nicolas Macris, and Lenka Zdeborová. The committee
machine: Computational to statistical gaps in learning a two-layers neural network. Advances in Neural
Information Processing Systems , 31, 2018.
Benjamin Aubin, Florent Krzakala, Yue Lu, and Lenka Zdeborová. Generalization error in high-dimensional
perceptrons: Approaching bayes error with convex optimization. Advances in Neural Information Processing
Systems, 33:12199–12210, 2020.
Jean Barbier, Florent Krzakala, Nicolas Macris, Léo Miolane, and Lenka Zdeborová. Optimal errors and
phase transitions in high-dimensional generalized linear models. Proceedings of the National Academy of
Sciences, 116(12):5451–5460, 2019.
Michael Biehl and Andreas Mietzner. Statistical mechanics of unsupervised learning. Europhysics Letters , 24
(5):421, 1993.
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in kernel
regression and wide neural networks. In International Conference on Machine Learning , pp. 1024–1034.
PMLR, 2020.
11Published in Transactions on Machine Learning Research (1/2025)
Ronald E Bruck. On the weak convergence of an ergodic iteration for the solution of variational inequalities
for monotone operators in hilbert space. J. Math. Anal. Appl , 61(1):159–164, 1977.
Hugo Cui and Lenka Zdeborová. High-dimensional asymptotics of denoising autoencoders. arXiv preprint
arXiv:2305.11041 , 2023.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism.
arXiv preprint arXiv:1711.00141 , 2017.
Aurélien Decelle, Giancarlo Fissore, and Cyril Furtlehner. Thermodynamics of restricted boltzmann machines
and related learning dynamics. Journal of Statistical Physics , 172:1576–1608, 2018.
Vladimir Fedorovich Dem’yanov and Aleksandr Borisovich Pevnyi. Numerical methods for finding saddle
points.USSR Computational Mathematics and Mathematical Physics , 12(5):11–52, 1972.
Rainer Dietrich, Manfred Opper, and Haim Sompolinsky. Statistical mechanics of support vector networks.
Physical Review Letters , 82(14):2975, 1999.
Samuel Frederick Edwards and Phil W Anderson. Theory of spin glasses. Journal of Physics F: Metal Physics ,
5(5):965, 1975.
José Fernando Fontanari. A statistical analysis of the knapsack problem. Journal of Physics A: Mathematical
and General , 28(17):4751, 1995.
Yoav Freund and Robert E Schapire. Adaptive game playing using multiplicative weights. Games and
Economic Behavior , 29(1-2):79–103, 1999.
Marylou Gabrié. Mean-field inference methods for neural networks. Journal of Physics A: Mathematical and
Theoretical , 53(22):223002, 2020.
Elizabeth Gardner and Bernard Derrida. Optimal storage properties of neural network models. Journal of
Physics A: Mathematical and general , 21(1):271, 1988.
Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mézard, and Lenka Zdeborová. Generalisation
error in learning with random features and the hidden manifold model. In International Conference on
Machine Learning , pp. 3452–3462. PMLR, 2020.
Gauthier Gidel, Hugo Berard, Gaëtan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A variational
inequality perspective on generative adversarial networks. arXiv preprint arXiv:1802.10551 , 2018.
Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Rémi Le Priol, Gabriel Huang, Simon
Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics. In The 22nd
International Conference on Artificial Intelligence and Statistics , pp. 1802–1811. PMLR, 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. Advances in Neural Information Processing
Systems, 27, 2014a.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572 , 2014b.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM , 63(11):
139–144, 2020.
David C Hoyle and Magnus Rattray. Principal-component-analysis eigenvalue spectra from data with
symmetry-breaking structure. Physical Review E , 69(2):026124, 2004.
David C Hoyle and Magnus Rattray. Statistical mechanics of learning multiple orthogonal signals: asymptotic
theory and fluctuation effects. Physical Review E , 75(1):016101, 2007.
12Published in Transactions on Machine Learning Research (1/2025)
Yuma Ichikawa and Koji Hukushima. Statistical-mechanical study of deep boltzmann machine given weight
parameters after training by singular value decomposition. Journal of the Physical Society of Japan , 91
(11):114001, 2022.
Yuma Ichikawa and Koji Hukushima. Dataset size dependence of rate-distortion curve and threshold of
posterior collapse in linear vae. arXiv preprint arXiv:2309.07663 , 2023.
Yuma Ichikawa and Koji Hukushima. Learning dynamics in linear VAE: Posterior collapse threshold,
superfluous latent space pitfalls, and speedup with KL annealing. In Sanjoy Dasgupta, Stephan Mandt,
and Yingzhen Li (eds.), Proceedings of The 27th International Conference on Artificial Intelligence and
Statistics , volume 238 of Proceedings of Machine Learning Research , pp. 1936–1944. PMLR, 02–04 May
2024.
Niels Ipsen and Lars Kai Hansen. Phase transition in pca with missing data: Reduced signal-to-noise ratio,
not sample size! In International Conference on Machine Learning , pp. 2951–2960. PMLR, 2019.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional
adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pp. 1125–1134, 2017.
Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew
Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution
using a generative adversarial network. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pp. 4681–4690, 2017.
Thibault Lesieur, Florent Krzakala, and Lenka Zdeborová. Phase transitions in sparse pca. In 2015 IEEE
International Symposium on Information Theory (ISIT) , pp. 1635–1639. IEEE, 2015.
Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence of
generative adversarial networks. In The 22nd International Conference on Artificial Intelligence and
Statistics , pp. 907–915. PMLR, 2019.
Pierre-Louis Lions. Une méthode itérative de résolution d ’une inéquation variationnelle. Israel Journal of
Mathematics , 31:204–208, 1978.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created equal?
a large-scale study. Advances in Neural Information Processing Systems , 31, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017.
D Maistroskii. Gradient methods for finding saddle points. Matekon, 13(3):22, 1977.
Marc Mezard and Andrea Montanari. Information, physics, and computation . Oxford University Press, 2009.
Marc Mézard and Giorgio Parisi. A replica analysis of the travelling salesman problem. Journal de physique ,
47(8):1285–1296, 1986.
Marc Mézard, Giorgio Parisi, and Miguel Angel Virasoro. Spin glass theory and beyond: An Introduction to
the Replica Method and Its Applications , volume 9. World Scientific Publishing Company, 1987.
George Nemhauser and Laurence Wolsey. Wiley-interscience series in discrete mathematics and optimization.
Integer and Combinatorial Optimization , pp. 765–766, 1988.
Manfred Opper and David Haussler. Generalization performance of bayes optimal classification algorithm for
learning a perceptron. Physical Review Letters , 66(20):2677, 1991.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami.
The limitations of deep learning in adversarial settings. In 2016 IEEE European symposium on security
and privacy (EuroS&P) , pp. 372–387. IEEE, 2016.
13Published in Transactions on Machine Learning Research (1/2025)
Giorgio Parisi. Toward a mean field theory for spin glasses. Physics Letters A , 73(3):203–205, 1979.
Giorgio Parisi. Order parameter for spin-glasses. Physical Review Letters , 50(24):1946, 1983.
Marc Potters and Jean-Philippe Bouchaud. A First Course in Random Matrix Theory: For Physicists,
Engineers and Data Scientists . Cambridge University Press, 2020.
Meisam Razaviyayn, Tianjian Huang, Songtao Lu, Maher Nouiehed, Maziar Sanjabi, and Mingyi Hong.
Nonconvex min-max optimization: Applications, challenges, and recent theoretical advances. IEEE Signal
Processing Magazine , 37(5):55–66, 2020.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative
adversarial text to image synthesis. In International Conference on Machine Learning , pp. 1060–1069.
PMLR, 2016.
Maria Refinetti and Sebastian Goldt. The dynamics of representation learning in shallow, non-linear
autoencoders. In International Conference on Machine Learning , pp. 18499–18519. PMLR, 2022.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 , 2013.
Takashi Takahashi and Yoshiyuki Kabashima. Macroscopic analysis of vector approximate message passing
in a model-mismatched setting. IEEE Transactions on Information Theory , 68(8):5579–5600, 2022.
Toshiyuki Tanaka. Moment problem in replica method. Interdisciplinary information sciences , 13(1):17–23,
2007.
Kasimir Tanner, Matteo Vilucchio, Bruno Loureiro, and Florent Krzakala. A high dimensional model for
adversarial training: Geometry and trade-offs. arXiv preprint arXiv:2402.05674 , 2024.
Paul Tseng. On linear convergence of iterative methods for the variational inequality problem. Journal of
Computational and Applied Mathematics , 60(1-2):237–252, 1995.
Peter Varga. Minimax games, spin glasses, and the polynomial-time hierarchy of complexity classes. Physical
Review E , 57(6):6487, 1998.
John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior, 2nd rev. 1947.
Abraham Wald. Statistical decision functions which minimize the maximum risk. Annals of Mathematics , 46
(2):265–280, 1945.
Chuang Wang, Hong Hu, and Yue Lu. A solvable high-dimensional model of gan. Advances in Neural
Information Processing Systems , 32, 2019.
John Wishart. The generalised product moment distribution in samples from a normal multivariate population.
Biometrika , pp. 32–52, 1928.
Lenka Zdeborová and Florent Krzakala. Statistical physics of inference: Thresholds and algorithms. Advances
in Physics , 65(5):453–552, 2016.
14Published in Transactions on Machine Learning Research (1/2025)
A Derivation of Theorem 4.1 proof
In this section, we provide the derivation proof of Theorem 4.1. The derivation begins with the calculation of
the free energy density without the analytic continuation of p=−βmin/βmaxtop∈N. The free energy density
in Eq. (2) is connected to the effective Hamiltonian, Heff(x;W), which is defined through the relationship:
f(βmin,βmax;W) =−1
βmindxEWlog/summationdisplay
xexp (−βminHeff(x;W)).
The effective Hamiltonian is given by
Heff(x;W)
=1
βmaxlog/summationdisplay
yeβmaxV(x,y;W),
=1
βmaxlog/summationdisplay
yeβmax/parenleftig
wxxdx
2/parenleftbigx⊤1dx
dx/parenrightbig2
+dywyy
2/parenleftbigy⊤1dy
dy/parenrightbig2
+wxy√
dxdy/parenleftbigx⊤1dx
dx/parenrightbig/parenleftbigy⊤1dy
dy/parenrightbig/parenrightig
×eβmax/parenleftig
bxdx/parenleftbigx⊤1dx
dx/parenrightbig
+bydy/parenleftbigy⊤1dy
dy/parenrightbig/parenrightig
,
=1
βmaxlogeβmaxdx/parenleftig
wxx
2/parenleftbigx⊤1dx
dx/parenrightbig2
+bx/parenleftbigx⊤1dx
dx/parenrightbig/parenrightig
×/summationdisplay
yeβmaxdy/parenleftig
wyy
2/parenleftbigy⊤1dy
dy/parenrightbig2
+wxy/radicalig
dx
dy/parenleftbigx⊤1dx
dx/parenrightbig/parenleftbigy⊤1dy
dy/parenrightbig
+by/parenleftbigy⊤1dy
dy/parenrightbig/parenrightig
,
=βmaxdx/parenleftigg
wxxdx
2/parenleftbiggx⊤1dx
dx/parenrightbigg2
+bxdx/parenleftbiggx⊤1dx
dx/parenrightbigg/parenrightigg
×1
βmaxlog/integraldisplay
dˆmydmyeβmaxdy/parenleftig
wyy
2(my)2+wxy/radicalig
dx
dy/parenleftbigx⊤1dx
dx/parenrightbig
my+bymy−1
βmaxmyˆmy+1
βmaxSoftplus( ˆmy)/parenrightig
+o(dy)
,
where Softplus (x) =log(1 +ex). To evaluate the integral with respect to ˆmyandmy, we take the limit as
dy→∞and apply the saddle point approximation. The effective Hamiltonian can be expressed as follows:
Heff(x;W) =dx/parenleftigg
wxx
2/parenleftbiggx⊤1dx
dx/parenrightbigg2
+bx/parenleftbiggx⊤1dx
dx/parenrightbigg
+κextr
my,ˆmy/bracketleftbiggwyy
2(my)2+wxyκ−1/2/parenleftbiggx⊤1dx
dx/parenrightbigg
my+bymy−1
βmaxmyˆmy+1
βmaxSoftplus( ˆmy)/bracketrightbigg/parenrightigg
.
15Published in Transactions on Machine Learning Research (1/2025)
whereκ=dy/dx. By summing over x, the free energy density can be calculated as follows:
f(βmin,βmax;W)
=−1
βmindxlog/summationdisplay
xe−βmindx/parenleftig
wxx
2/parenleftig
x⊤1dx
dx/parenrightig2
+bx/parenleftig
x⊤1dx
dx/parenrightig/parenrightig
×e−βmindx/parenleftig
κextr
my,ˆmy/bracketleftig
wyy
2(my)2+wxyκ−1/2/parenleftbigx⊤1dx
dx/parenrightbig
my+bymy−1
βmaxmyˆmy+1
βmaxSoftplus( ˆmy)/bracketrightig/parenrightig
,
=−1
βmindxlog/parenleftbiggdx
2π/parenrightbigg/integraldisplay
dmxdˆmxe−βmindx/parenleftbigwxx
2(mx)2+bxmx+1
βminmxˆmx−1
βminSoftplus( ˆmx)/parenrightbig
×e−βmindx/parenleftig
κextr
my,ˆmy[wyy
2(my)2+wxyκ−1/2mxmy+bymy−1
βmaxmyˆmy+1
βmaxSoftplus( ˆmy)]/parenrightig
,
= extr
mx,ˆmx,my,ˆmy/parenleftigg
wxx
2(mx)2+bxmx+1
βminmxˆmx−1
βminSoftplus( ˆmx)
+κwyy
2(my)2+wxyκ1/2mxmy+κbymy−κ
βmaxmyˆmy+κ
βmaxSoftplus( ˆmy)/parenrightigg
.
The final equality is obtained by applying the saddle point method to evaluate the integral. From the saddle
point equations, the following expressions are
mx=σ( ˆmx), my=σ( ˆmy)
Further transformation of the equation yields the following expression:
f(βmin,βmax;W) = extr
mx,my/bracketleftigg
wxx
2(mx)2+κwyy
2(my)2+wxyκ1/2mxmy+bxmx+κbymy
−1
βminH(mx) +κ
βmaxH(my)/bracketrightigg
,(23)
whereH(x) =−xlogx−(1−x) log(1−x)represents the binary cross-entropy.
Next, we proceed to evaluate the free energy density under analytic continuation in the replica method, which
is expressed as
ˆf(βmin,βmax;W) =−1
βmindxlog/summationdisplay
x/summationdisplay
y1,...,ypeβmaxdx/parenleftbigg
wxxp
2/parenleftig
x⊤1dx
dx/parenrightig2
+κwyy
2/summationtext
l/parenleftig
y⊤
l1dy
dy/parenrightig2/parenrightbigg
×eβmaxdx/parenleftig
wxyκ1/2/parenleftig
x⊤1dx
dx/parenrightig/summationtext
l/parenleftig
y⊤
l1dy
dy/parenrightig
+bxp/parenleftig
x⊤1dx
dx/parenrightig
+κby/summationtext
l/parenleftig
y⊤
l1dy
dy/parenrightig/parenrightig
.
We introduce the order parameter through the Fourier transform representation of the delta function:
ˆf(βmin,βmax;W)
=−1
βmindxlog/parenleftbiggdxdp
y
(2π)p+1/parenrightbigg/integraldisplay
dmxdˆmx/productdisplay
ldmy
ldˆmy
leβmaxdx(wxxp
2(mx)2+κwyy
2/summationtext
l(my
l)2+wxyκ1/2mx/summationtext
lmy
l)
×eβmaxdx(bxpmx+κby/summationtext
lmy
l−1
βmaxˆmxmx−κ
βmax/summationtext
lˆmy
lmy
l)/summationdisplay
xeˆmx/summationtext
ixi/summationdisplay
y1,...,ype/summationtext
lˆmy
l/summationtext
jyj,
=−1
βmindxlog/parenleftbiggdxdp
y
(2π)p+1/parenrightbigg/integraldisplay
dmxdˆmx/productdisplay
ldmy
ldˆmy
leβmaxdx(wxxp
2(mx)2+κwyy
2/summationtext
l(my
l)2+wxyκ1/2mx/summationtext
lmy
l)
×eβmaxdx(bxpmx+κby/summationtext
lmy
l−1
βmaxˆmxmx−κ
βmax/summationtext
lˆmy
lmy
l+1
βmaxSoftplus( ˆmx)+κ
βmax/summationtext
lSoftplus( ˆmy
l)).
16Published in Transactions on Machine Learning Research (1/2025)
Under the assumption of replica symmetry, where ∀l∈[p],ˆmy
l=my,my
l=my, we can further reformulate
the expression as follows:
ˆf(βmin,βmax;W)
=−1
βmindxlog/integraldisplay
dmxdˆmxdmydˆmye−βmindx(wxx
2(mx)2+κwyy
2(my)2+wxyκ1/2mxmy)
×e−βmindx(bxmx+κbymy−1
βmaxpˆmxmx−κ
βmaxˆmymy+1
βmaxpSoftplus( ˆmx)+κ
βmaxSoftplus( ˆmy)+o(dx)+o(dy)),
= extr
mx,my,ˆmx,ˆmy/bracketleftigg
wxx
2(mx)2+κwyy
2(my)2+wxyκ1/2mxmy+bxmx+κbymy
−1
βmaxpˆmxmx−κ
βmaxˆmymy+1
βmaxpSoftplus( ˆmx) +κ
βmaxSoftplus( ˆmy)/bracketrightigg
.
The final equality is derived by handling the integral using the saddle point method. Consequently, the
following saddle point equation is obtained:
mx=σ( ˆmx), my=σ( ˆmy).
Substituting these results, the following expression for the free energy density is derived as
ˆf(βmin,βmax;W) = extr
mx,my/bracketleftigg
wxx
2(mx)2+κwyy
2(my)2+wxyκ1/2mxmy+bxmx+κbymy
−1
βminH(mx) +κ
βmaxH(my)/bracketrightigg
.(24)
This result coincides with the exact free energy density f(βmin,βmax;W), derived without the need for
analytic continuation.
B Generalization of Theorem 4.1 to Finite Eigenmodes
In this section, we generalize Theorem 4.1 to cases where Wxx,Wxy, andWyyare decomposed into a finite
number of eigenmodes as follows:
Wxx=L/summationdisplay
l=1αlala⊤
l, Wyy=M/summationdisplay
m=1βmbmb⊤
m, Wxy=N/summationdisplay
n=1γncnc⊤
n, αl,βm,γn,L,M,N =Odx,dy(1).
In this formulation, V(x,y;W)can be expanded as follows:
V(x,y;W) =dx
2L/summationdisplay
l=1αl/parenleftbigg1
dxx⊤al/parenrightbigg2
+dy
2M/summationdisplay
m=1βm/parenleftbigg1
dyy⊤bm/parenrightbigg2
+/radicalbig
dxdyN/summationdisplay
n=1γn/parenleftbigg1
dxx⊤cn/parenrightbigg/parenleftbigg1
dyc⊤
ny/parenrightbigg
+bxdx/parenleftbigg1
dxx⊤1dx/parenrightbigg
+bydy/parenleftbigg1
dyy⊤1dy/parenrightbigg
.
This expansion constitutes a direct extension of the calculations in Appendix A, with the terms x⊤1dx/dxand
y⊤1dy/dyaugmented by overlaps with the eigenvectors, {x⊤al/dx}l,{y⊤bm/dy}M
m=1, and{x⊤cn/dx,y⊤cn/dy}N
n=1.
By performing analogous calculations, we find that the free energy density, without assuming analytic
continuation, aligns with the analytically continued free energy density. Furthermore, this free energy density
is characterized by the saddle point condition involving L+M+N=Odx,dy(1)variables, as in Eq. (23). If
we assume L,M,N =Odx,dy(dx), then asdx→∞, the limit becomes trivial; Without appropriate scaling,
the free energy density diverges.
17Published in Transactions on Machine Learning Research (1/2025)
C Evaluation of Functions of the Optimal Value of Min-Max Problems
In this section, we present a method for evaluating the expected value EA[G(¯x(A))]over a set of problem
instancesA, where ¯x(A) =argminx∈Xmaxy∈YV(x,y;A)represents the min-max optimal solution. This
analysis provides insights into how Eq. (2) functions as a generating function. We also demonstrate that this
approach can be directly applied to evaluate the generalization error in Section 5, particularly Eq. (13).
The key idea is to expand EA[G(¯x(A))]as follows:
EA[G(¯x(A))] =EA/bracketleftbigg
lim
βmin→+∞lim
βmax→+∞/integraldisplay
pβmin,βmax(x;A)G(x)dx/bracketrightbigg
=dxEA/bracketleftbigg
lim
βmin→+∞lim
βmax→+∞∂
∂ωf(βmin,βmax;ωG(x))/vextendsingle/vextendsingle/vextendsingle/vextendsingle
ω=0/bracketrightbigg
,
where we extend the free energy from Eq. (2) by introducing a parameter ωas follows:
f(βmin,βmax;ωG(x)) =−1
βmindxEAlog/integraldisplay
dxexp/parenleftbigg
−βmin
βmaxlog/integraldisplay
dyexp (βmaxV(x,y;A)) +ωG(x)/parenrightbigg
.
We employ this technique to derive the generalization error in Section 5. Specifically, the generalization error
for GANs can be expressed as:
εg(¯w,w∗) =1
dED/bracketleftbig
∥¯w(D)−w∗∥2/bracketrightbig
.
To compute this, we augment the free energy calculation by adding the term ω(∥w∥2−2w⊤w∗). This
adjustment is incorporated into the calculation by modifying the term ˜λ(wa)2in the exponent of the d-th
power expression in Eq. (14) to (ω+˜λ)(wa)2−2ω(w∗wa). Since these terms are quadratic, the Gaussian
integration remains straightforward. The remaining calculation follows the same procedure in the main text
and is thus omitted for brevity. Eventually, we obtain the following form:
lim
d→+∞εg= 1−2A(ˆq,ˆχ,ˆm,ˆb) +A2(ˆq,ˆχ,ˆm,ˆb),
whereA(ˆq,ˆχ,ˆm,ˆb)is determined by Eq. (22), explicitly given as
A(ˆq,ˆχ,ˆm,ˆb) =
ˆb+/radicalig
˜λ(ˆq+λ)
ˆb2+˜λ(ˆq+λ)
/radicalbig
ˆm2+ ˆχ.
18