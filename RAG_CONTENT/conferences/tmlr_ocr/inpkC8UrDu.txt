Under review as submission to TMLR
What and How does In-Context Learning Learn? Bayesian
Model Averaging, Parameterization, and Generalization
Anonymous authors
Paper under double-blind review
Abstract
In-Context Learning (ICL) ability has been found efficient across a wide range of appli-
cations, where the Large Language Models (LLM) learn to complete the tasks from the
examples in the prompt without tuning the parameters. In this work, we conduct a com-
prehensive study to understand ICL from a statistical perspective. First, we show that
the perfectly pretrained LLMs perform Bayesian Model Averaging (BMA) for ICL under
a dynamic model of examples in the prompt. The average error analysis for ICL is then
built for the perfectly pretrained LLMs with the analysis of BMA. Second, we demonstrate
how the attention structure boosts the BMA implementation. With sufficient examples in
the prompt, attention is proven to perform BMA under the Gaussian linear ICL model,
which also motivates the explicit construction of the hidden concepts from the attention
heads values. Finally, we analyze the pretraining behavior of LLMs. The pretraining error
is decomposed as the generalization error and the approximation error, which are bounded
separately. Then the ICL average error of the pretrained LLMs is shown to be the sum of
O(T−1)and the pretraining error. In addition, we analyze the ICL performance of the pre-
trained LLMs with misspecified examples. The theoretical findings are corroborated with
the experimental results.
1 Introduction
With the ever-increasing sizes of model capacity and corpus, Large Language Models (LLM) have achieved
tremendous successes across a wide range of tasks [Dong et al., 2019; Wei et al., 2022c; Kojima et al.,
2022; Ouyang et al., 2022]. Recent studies have revealed that these LLMs possess immense potential,
as their large capacity allows for a series of emergent abilities [Wei et al., 2022b; Liu et al., 2023].
Figure 1: LLMs implement Bayesian Model Averaging
(BMA) for In-Context Learning (ICL). They estimate
the posterior of hidden concept zfrom examples and
use the posterior to mix the conditional probability of
response on the query and hidden concept z.One such ability is ICL, which enables an LLM
to learn from just a few examples, without chang-
ing the network parameters. Despite the tremen-
dous empirical successes, theoretical understanding
of ICL remains limited. Specifically, existing works
fail to explain why LLMs have the ability for ICL,
how the attention mechanism is related to the ICL
ability, and how pretraining influences ICL. Al-
though the optimality of ICL is investigated in Xie
et al. [2021] and Wies et al. [2023], these works
bothmakeunrealisticassumptionsonthepretrained
models, and their results cannot demystify the par-
ticular role played by the attention mechanism in
ICL.
In this work, we focus on the scenario where a trans-
former is first pretrained on a large dataset and then
prompted to perform ICL. Our goal is to rigorously
understand why the practice of “pretraining + prompting” unleashes the power of ICL. To this end, we
1Under review as submission to TMLR
0 10 20 30 40
Number of Examples036912Squared Errorcumulative squared error
squared error on each query
Figure 2: The cumulative
squared error of LLMs trained
for linear regression is bounded
by a constant. It verifies Pro-
postion 3.4, which is a result of
Propostion 3.3.
antonym country-capital present-past0.00.20.40.60.81.0Accuracyzs-hc
ICL-0
ICL-1ICL-5
ICL-10Figure 3: The constructed hid-
den concept provides sufficient in-
formation for LLM. Conditioned
on it in a zero-shot setting, LLMs
havecomparableperformancewith
ICL with several examples. It ver-
ifies Propostions 3.3 and 3.6.
1 1000 2000 3000 4000
Number of Examples2.02.53.03.54.0Ratio between attn and attn
d=2
d=3Figure 4: The ratios between
attn†and attnconverge to a
constant that depends on the
dimension when the example
numberTtends to infinity. It
verifies Propostion 3.6.
aim to answer the following three questions: (a)What type of ICL estimator is learned by LLMs? (b)
What are suitable performance metrics to evaluate ICL accurately and what are the error rates? (c)What
is the role played by the transformer architecture during the pretraining and prompting stages? The first
and the third questions demand scrutinizing the transformer architecture to understand how ICL happens
during transformer prompting. The second question then requires statistically analyzing the extracted ICL
process. Moreover, the third question necessitates a holistic understanding beyond prompting — we also
need to characterize the statistical error of pretraining and how this error affects prompting.
To address these questions, we adopt a Bayesian view and assume that the examples fed into a pretrained
LLM are sampled from a hidden variable model parameterized by a hidden concept z∗∈Z. Moreover,
the pretraining dataset contains sequences of examples from the same hidden variable model, but with the
concept parameter z∈Zitself randomly distributed according to a prior distribution. We mathematically
formulate ICL as the problem of predicting the response of the current covariate, where the prompt contains
texamples of covariate-response pairs and the current covariate.
Under such a setting, to answer (a), we show that the perfectly pretrained LLMs perform ICL in the form
of BMA under a dynamical data model. That is, LLM first computes a posterior distribution of z∗∈Z
given the first texamples, and then predicts the response of the (t+ 1)-th covariate by aggregating over the
posterior (Proposition 3.3), which is empirically verified in Section 3.2.
In addition, to answer (b), we adopt the online learning framework and define a notion called ICL average
error, which is the averaged prediction error of ICL on a sequence of covariate-response examples. We prove
that the ICL average error after prompting texamples isO(1/t)up to the statistical error of the pretrained
model (Theorem 5.2), which is validated by experiments in Section 3.2.
Finally, to answer (c), we elucidate the role played by the transformer architecture in prompting and
pretraining respectively. In particular, we show that a variant of attention mechanism encodes BMA in its
architecture under a linear Gaussian model, which enables the transformer to perform ICL via prompting.
Such an attention mechanism can be viewed as an extension of linear attention and coincides with the
standard softmax attention [Garnelo & Czarnecki, 2023] when the length of the prompt goes to infinity. Thus
we show that softmax attention Vaswani et al. [2017] approximately encodes BMA (Proposition 3.6), which
is empirically verified in Section 3.4. Besides, the transformer architecture enables a fine-grained analysis of
the statistical error incurred by pretraining. In particular, we prove that the error of the pretrained language
model, measured via total variation, is bounded by a sum of approximation error and generalization error
(Theorem 4.3). The approximation error decays to zero exponentially fast as the depth of the transformer
increases (Proposition 4.4), while the generalization error decays to zero sublinearly with the number of
tokensinthepretrainingdataset. Thisfeaturesthefirstpretraininganalysisoftransformersintotalvariation
distance that also takes the approximation error into account. Furthermore, as an interesting extension, we
also study the misspecified case where the response variables of the examples fed into the LLM are perturbed.
2Under review as submission to TMLR
Figure 5: To form the pretraining dataset, a hidden concept zis first sampled according to PZ, and a
document is generated from the concept. Taking the token sequence Stup to position t∈[T]as the input,
the LLM is pretrained to maximize the next token xt+1. During the ICL phase, the pretrained LLM is
prompted with several examples to predict the response of the query.
We provide sufficient conditions for ICL to be robust to the perturbations and establish the finite-sample
statistical error (Proposition G.5).
In sum, by addressing questions (a)–(c), we provide a unified understanding of the ICL ability of LLMs
and the particular role played by the attention mechanism. Our theory provides a holistic theoretical
understanding of the ICL average error, approximation, and generalization errors of ICL.
2 Preliminary
Attention and Transformers. Attention mechanism has been the most powerful and popular neural
networkmoduleinbothComputerVision(CV)andNaturalLanguageProcessing(NLP)communities, andit
is the backbone of the LLMs [Devlin et al., 2018; Brown et al., 2020]. Assume that we have a query vector q∈
Rdk. WithTkeyvectorsin K∈RT×dkandTvaluevectorsin V∈RT×dv, theattentionmechanismmapsthe
query vector qtoattn (q,K,V ) =V⊤softmax (Kq), where softmax normalizes a vector via the exponential
function, i.e., for x∈Rd,[softmax (x)]i= exp(xi)//summationtextd
j=1exp(xj)fori∈[d]. The output is a weighted sum of
V, and the weights reflect the closeness between Wandq. Fortquery vectors, we stack them into Q∈Rt×dk.
Attention maps these queries using the function attn (Q,K,V ) =softmax (QK⊤)V∈Rt×dv, where softmax
is applied row-wisely. In the practical design of transformers, practitioners usually use Multi-Head Attention
(MHA) instead of single attention to express sophisticated functions, which forwards the inputs through h
attention modules in parallel and outputs the sum of these sub-modules. Here h∈Nis a hyperparameter.
TakingX∈RT×das the input, MHA outputs mha(X,W ) =/summationtexth
i=1attn (XWQ
i,XWK
i,XWV
i), whereW=
(WQ
i,WK
i,WV
i)h
i=1is the parameters set of hattention modules, WQ
i∈Rd×dh,WK
i∈Rd×dh, andWV
i∈
Rd×dfori∈[h]are weight matrices for queries, keys, and values, and dhis usually set to be d/h[Michel
et al., 2019]. The transformer is the concatenation of the attention modules and the fully-connected layers,
which is widely adopted in LLMs [Brown et al., 2020].
Large Language Models and In-Context Learning. Many LLMs are autoregressive , such as GPT
[Brown et al., 2020]. It means that the model continuously predicts future tokens based on its own previous
values. For example, starting from a token x1∈X, where Xis the alphabet of tokens, a LLM Pθwith
parameter θ∈Θcontinuously predicts the next token according to xt+1∼Pθ(·|St)based on the past
St= (x1,···,xt)fort∈N. Here, each token represents a word and the position of the word [Ke et al.,
2020], and the token sequences Stfort∈Nlive in the sequences space X∗. LLMs are first pretrained on
a huge body of corpus, making the prediction xt+1∼Pθ(·|St)accurate, and then prompted to perform
downstream tasks. During the pretraining phase, we aim to maximize the conditional probability Pθ(x|S)
over the nominal next token x[Brown et al., 2020].
3Under review as submission to TMLR
Figure 6: The hidden concept z∗is “finer taxonomy” makes the hidden variables htevolve to finer level
according to equation 3.1. These hidden variables parameterize the relationship between /tildewidectandrtwith
equation 3.2.
After pretraining, LLMs are prompted to perform downstream tasks without tuning parameters. Different
from the finetuned models that learn the task explicitly [Liu et al., 2023], LLMs can implicitly learn from
the examples in the prompt, which is known as ICL [Brown et al., 2020]. Concretely, pretrained LLMs are
provided with a prompt ptt= (/tildewidec1,r1,...,/tildewidect,rt,/tildewidect+1)withtexamples and a query as inputs, where each pair
(/tildewideci,ri)∈X∗×Xis an example of the task, and /tildewidect+1is the query, as shown in Figure 5 in Appendix ??. For
example, the pttwitht= 2can be “Cats are animals, apples are plants, mushrooms are”. Here /tildewidec1∈X∗is
a token sequence “Cats are”, while r1is the response “animals”. The query /tildewidect+1is “mushrooms are”, and
the desired response is “fungi”. The prompts are generated from a hidden concept z∗∈Z, e.g.,z∗can be
the classification of biological categories, where Zis the concept space. The generation process is /tildewideci∼Pq
andri∼P(·|pti−1,z∗)for the nominal distribution Pandi∈[t], where Pqis the covariate distribution.
Thus, when performing ICL, LLMs aim to estimate the conditional distribution P(rt+1|ptt,z∗). It is widely
conjectured and experimentally found that the pretrained LLMs can implicitly identify the hidden concept
z∗∈Zfrom the examples, and then perform ICL by outputting from P(rt+1|ptt,z∗). In the following, we
will provide theoretical justifications for this claim. We note that delimiters are omitted in our work, and our
results can be generalized to handle this case. Since LLMs are autoregressive, the definition of the notation
P(·|S)withS∈X∗may be ambiguous because the length of the subsequent tokens is not specified. Unless
explicitly specified, we let P(·|S)denote the distribution of the next single token conditioned on S.
3 In-Context Learning via Bayesian Model Averaging
In this section, we show that LLMs perform ICL implicitly via BMA from two perspectives. First, we show
that the statistical model of ICL and the training loss enable LLMs to implement BMA for ICL under a
dynamical data model. Second, we show that the self-attention mechanism boosts the implementation of
the BMA algorithm in LLMs.
3.1 ICL Statistical Models Enables Bayesian Model Averaging
Given a sequence S={(/tildewidect,rt)}T
t=1withTexamples generated from a hidden concept z∗∈Z, we use
St={(/tildewideci,ri)}t
i=1to represent the first tICL examples in the sequence. Here /tildewidectandrtrespectively denote
the ICL covariate and response. During the ICL phase, a LLM is sequentially prompted with ptt= (St,/tildewidect+1)
fort∈[T−1], i.e., the first texamples and the (t+ 1)-th covariate. The prompted LLM aims to predict
the response rt+1based on ptt= (St,/tildewidect+1)whose true distribution is rt+1∼P(·|ptt,z∗). For the analysis
of ICL, we focus on the following hidden variable model
Assumption 3.1 (Dynamic Hidden Variable Model) .The hidden concept z∗∈Zparameterizes the distri-
butions of hidden variables {ht}T
t=1∈HTas
ht=gz∗(h1,...,ht−1,ζt), (3.1)
wheregz∗is a function parameterized by z∗, and{ζt}T
t=1are exogenous noises. These hidden variables
parameterize the covariates and responses for ICL as
rt=f(/tildewidect,ht,ξt),/tildewidect∼Pq∀t∈[T], (3.2)
4Under review as submission to TMLR
where Pqis the distribution of query, the hidden variable ht∈Hdetermines the relation between ctandrt,
ξt∈Ξfort∈[T]are i.i.d. random noises, and f:X∗×H× Ξ→Xis a function that relates response rtto
/tildewidect,ht, andξt.
In the data generation process, a hidden concept z∗∈Zis first generated from P(z). The covariates /tildewidect
are generated from Pqin an i.i.d. manner. Then the hidden variables {ht}T
t=1and responses{rt}T
t=1are
generated according to equation 3.1 and equation 3.2. The model in Assumption 3.1 essentially assumes
that the hidden concept z∗implicitly determines the transition of the conditional distribution P(rt=·|/tildewidect)
by affecting the evolution of the hidden variables {ht}t∈[T]. These hidden variables capture the relationship
between examples {(/tildewidect,rt)}T
t=1, since the examples may have some inferent relationship and share some
similarity when they are come up with by humans [Elman, 1995; Niyogi et al., 1997]. In other words, this
assumption assumes that the examples convey a main semantic z∗, e.g., ”finer taxonomy” in Figure. 6. Then
the response is generated from the hidden variable h, e.g., ”class”, and covariate, e.g., ”otter” in Figure. 6.
This model is quite general, and it subsumes the models in previous works.
Comparision with existing models The models in the existing works all assume that the examples are
i.i.d., i.e.,ht=gz∗(ζt)in the model of Assumption 3.1. For example, the Hiddn Markov Model (HMM)
model in Xie et al. [2021] assumes that the hidden variables for each example are independently generated.
In contrast, we allow them to depend on each other via equation 3.1. When the hidden variables ht=z∗for
t∈[T]degenerate to the hidden concept, the model in Assumption 3.1 recovers the topic model in Wang
et al. [2023] and the ICL model in Jiang [2023].
Assuming that the tokens follow the statistical model given in equation 3.2, during pretraining, we collect
Npindependent trajectories by sampling from equation 3.2 with concept zrandomly sampled from P(z).
Intuitively, during pretraining, by training in an autoregressive manner, the LLM approximates the condi-
tional distribution P(rt+1|ptt) =Ez∼P(z|ptt)[P(rt+1|ptt,z)], which is the conditional distribution of rt+1
given ptt, aggregated over the randomness of the concept z∗. Given an infinite number of samples and the
sufficiently large function class Θ, the pretrained LLMs can perfectly match the pretraining distribution.
Assumption 3.2 (Perfect Pretraining) .A LLM Pθis called perfectly pretrained if for all pt∈X∗and
r∈X, we have Pθ(r|pt) =P(r|pt), where Pis the distribution induced by the model in Assumption 3.1.
We will relax this assumption in Section 4 by analyzing the pretraining error.
Proposition 3.3 (LLMs Perform BMA) .Under Assumptions 3.1 and 3.2, LLMs perform BMA for ICL,
i.e.,
Pθ(rt+1|ptt)=/integraldisplay
P(rt+1|/tildewidect+1,St,z)P(z|St)dz, (3.3)
where Pis the distribution induced by Assumption 3.1.
We note that the left-hand side of equation 3.3 is the prediction of the pretrained LLM given a prompt
ptt. Meanwhile, the right-hand side is exactly the prediction given by the BMA algorithm that infers the
posterior belief of the concept z∗based onStand predicts rt+1by aggregating the likelihood in equation 3.2
with respect to the posterior P(z∗=·|St), as shown in Figure 1. Thus, this proposition shows that
perfectly pretrained LLMs are able to perform ICL because they implement BMA during prompting .
As mentioned, Proposition 3.3 is proved under a more general model than the previous works and thus serves
as a generalized result of some claims in the previous works. We note that the claim of Proposition 3.3 is
independent of the network structure. This partially explains why LSTMs demonstrate ICL ability in Xie
et al. [2021]. In Section 3.3, we will demonstrate how the attention mechanism helps to implement BMA.
The proof of Proposition 3.3 is in Appendix E.2.
Next, we study the performance of ICL from an online learning perspective. Recall that LLMs are continu-
ously prompted with Stand aim to predict the (t+ 1)-th covariate rt+1fort∈[T−1]. This can be viewed
as an online learning problem. For any algorithm that generates a sequence of density estimators {/hatwideP(rt)}T
t=1
for predicting{rt}t∈[T], we consider the following ICL average error as its performance metric:
AEt=1
tsup
zt/summationdisplay
i=1/parenleftig
logP(ri|pti−1,z)−log/hatwideP(ri)/parenrightig
. (3.4)
5Under review as submission to TMLR
This ICL average error measures the performance of the estimator /hatwidePcompared with the best hidden concept
in hindsight. For the perfectly trained LLMs, the estimator is exactly /hatwideP(rt) =P(rt+1|ptt). By building the
equivalence of pretrained LLM and BMA, we have the following proposition, which shows that predicting
{rt}t∈[T]by iteratively prompting the LLM incurs a O(1/T)average error.
Proposition 3.4 (ICL Average Error of Perfectly Pretrained Model) .Under Assumptions 3.1 and 3.2, we
have for any t∈[T]that
1
tt/summationdisplay
i=1logPθ(ri|pti−1)≥sup
z∈Z/parenleftig1
tt/summationdisplay
i=1logP(ri|pti−1,z) +logPZ(z)
t/parenrightig
.
HerePZis the prior of the hidden concept z∈Z. When the hidden concept space Zis finite and the prior
PZ(z)is the uniform distribution on Z, we have that AEt≤log|Z|/t.When the nominal concept z∗satisfies
that supz/summationtextt
i=1logP(ri|z,pti−1) =/summationtextt
i=1logP(ri|z∗,pti−1)for anyt∈[T], the average error is bounded
asAEt≤log(1/PZ(z∗))/t.
This theorem states that the ICL average error of the perfectly pretrained model is bounded by
log(1/PZ(z∗))/t. This is intuitive since the average error is relatively large if the concept z∗rarely appears
according to the prior distribution. This proposition shows that, when given sufficiently many examples,
predicting{rt}t∈[T]via ICL is almost as good as the oracle method which knows true concept z∗and the
likelihood function P(ri|pti−1,z∗). We state the result for the case where Zis finite, and these results can be
generalized to uncountable Zwith continuity assumptions. The proof of Proposition 3.4 and the extension is
in Appendix E.3. In Section 4, we characterize the deviation between the learned model and the underlying
true model.
3.2 Experimental validations for Propositions 3.3 and 3.4
To verify Propostion 3.3, we empirically construct the hidden concept and condition on it for inference.
We construct the hidden concept vector as the average sum over prompts of the values of twenty selected
attention heads, i.e., we compress the hidden concept into a vector with dimension 4096. To demonstrate
the effectiveness of the constructed hidden concepts, we add these hidden concept vectors at a layer of LLMs
when the model resolves the prompt with zero-shot. In Figure. 3, “zs-hc” refers to the results of LLMs that
infers with learned hidden concept vectors and zero-shot prompt, and “ICL- i” refers to the results of LLMs
prompted with iexamples. We consider the tasks of finding antonyms, finding the capitals of countries,
and finding the past tense of words, i.e., ht=z∗in Proposition 3.3. The results indicate that the LLMs
conditioned on the learned hidden concept vectors Pθ(rt+1|/tildewidect+1,z∗)have comparable performance with the
LLMs prompted with several examples Pθ(rt+1|ptt). This indicates that the learned hidden concept vectors
are indeed efficient compression of the hidden concepts, which proves that LLMs deduce hidden concepts for
ICL and corroborates with Proposition 3.3.
To verify Proposition 3.4, we will empirically show that t×AEt, i.e., the cumulative error, is upper bounded
by a constant. LLMs is trained for the linear regression task from scratch, which is a representative setting
studied in Garg et al. [2022]; Akyürek et al. [2022]. The examples in the prompt are {(xi,yi)}N
i=1, where
xi∈Rd,d= 20andyi=wTxifor somewsampled from Gaussian distribution. Given the Gaussian model,
we adopt the squared error to approximate the logarithm of the probability. Then t×AEtof the LLMs
can be well approximated by the sum of the squared error till time t. We note that there is pretraining
error in our experiments, which can be bounded by Theorems 4.3 and 5.2. The results in Figure 2 strongly
corroborate our theoretical findings. First, the results verify our claim in Proposition 3.4 that t·AEtcan be
upper bounded by a constant. Second, the line of squared error indicates that the ICL of LLMs only has a
significant error when T≤d, i.e., the cumulative error only increases in this region. Thus, the cumulative
error of the ICL by LLMs is at most linear in O(d/T). From the view of our theoretical result, discretizing
the set{z∈Rd|∥z∥2≤d}with approximation error δ > 0will result in a set with (C/δ)delements,
whereC > 0is an absolute constant. Proposition 3.4 implies that the cumulative error is the sum of the
log|Z|/T=dlog(C/δ)/Tand the pretraining error, which matches the simulation results. The experiment
details can be found in Appendix D. We also calculate the cumulative log-likelihoods in Proposition 3.4 on
time series data in C, which have constant-like upper bounds.
6Under review as submission to TMLR
3.3 Attention Parameterizes Bayesian Model Averaging
In the following, we explore the role played by the attention mechanism in ICL. To simplify the presentation,
we consider the case where the covariate /tildewidect∈X∗is a single token ct∈Xin this subsection. During the
ICL phase, pretrained LLMs are prompted with ptt= (St,ct+1)and tasked with predicting the (t+ 1)-th
responsert+1. The transformers first separately map the covariates /tildewideciand responses rifori∈[t]to the
corresponding feature spaces, which are usually realized by the fully connected layers. We denote these two
learnable mappings as k:Rd→Rdkandv:Rd→Rdv. Their nominal values are denoted as k∗andv∗,
respectively. The pretraining of the transformer essentially learns the nominal mappings v∗andk∗with
sufficiently many data points. After these transformations, the attention module will take vi=v∗(ri)and
ki=k∗(ci)fori∈[t]as the value and key vectors to predict the result for the query qt+1=kt+1=k∗(ct+1).
To elucidate the role played by attention, we consider a Gaussian linear simplification of equation 3.2.
Assumption 3.5 (Gaussian linear ICL model) .For the features vt=v∗(rt)andkt=k∗(ct)fort∈[t], we
have
vt=z∗ϕ(kt) +ξt,∀t∈[T], (3.5)
whereϕ:Rdk→Rdϕrefers to the feature mapping in some Reproducing Kernel Hilbert Space (RKHS),
z∗∈Rdv×dϕcorresponds to the hidden concept, and ξt∼N(0,σ2I),t∈[T]are i.i.d. Gaussian noises with
covarianceσ2I. The prior of z∗isP(z)is a Gaussian distribution N(0,λI).
The function in equation 3.5 is general. The generality comes from: (i) the feature mapping ϕand the
corresponding RKHS make Kernel Mean Embedding (KME) have sufficient expressiveness since all the
operations on distribution can be captured by some operations in KME space. (ii) Two learnable mappings
v∗(·)andk∗(·)are neural networks and are general enough to represent continuous functions. To specify the
relationship between Assumptions 3.1 and 3.5, note that equation 3.5 can be written as
rt=v−1
∗/parenleftig
z∗ϕ/parenleftbig
k∗(ct)/parenrightbig
+ξt/parenrightig
(3.6)
ifv−1
∗is reversible, which is a realization of equation 3.2 with ht=z,ξt=ϵt, andf(c,h,ξ ) =v−1
∗(hϕ(k∗(c))+
ξ). In other words, equation 3.5, or equivalently equation 3.6, specifies a specialization of equation 3.2 where
in the feature space, the hidden concept z∗represents a transformation between the value vand the key k.
Here, we simply take this as the transformation by a matrix, which can be easily generalized by building
a bijection between concepts zand complex transformations. In the following, to simplify the notation, let
K:Rdk×Rdk→R. denote the kernel function of the RKHS induced by ϕ. The stacks of the values and
keys are denoted as Kt= (k1,...,kt)⊤∈Rt×dkandVt= (v1,...,vt)⊤∈Rt×dv, respectively. Consequently,
the model in equation 3.5 implies that
P(vt+1|ptt)=/integraldisplay
P(vt+1|z,qt+1)P(z|St)dz∝exp/parenleftig
−/vextenddouble/vextenddoublevt+1−¯ztϕ(qt+1)/vextenddouble/vextenddouble2
Σ−1
t/slashbig
2/parenrightig
, (3.7)
where we denote by Σtthe covariance of vt+1∼P(·|St,qt+1), and the mean concept ¯ztis
¯zt=Vt/parenleftbig
K(Kt,Kt) +λI/parenrightbig−1ϕ(Kt). (3.8)
Combining equation 3.7 and equation 3.8, we can see that ¯ztϕ(qt+1)essentially measures the similarity
between the query and keys, which is quite similar to the attention mechanism defined in Section 2. However,
here the similarity is normalization according to equation 3.8, not by softmax. This motivates us to define a
new structure of attention and explore the relationship between the newly defined attention and the original
one. For any q∈Rdk,K∈Rt×dk, andV∈Rt×dv, we define a variant of the attention mechanism as follows,
attn†(q,K,V )=V⊤/parenleftbig
K(K,K ) +λI/parenrightbig−1K(K,q). (3.9)
7Under review as submission to TMLR
From equation 3.7, equation 3.8, and equation 3.9, it holds that the response vt+1for(t+ 1)-th query is
distributed as vt+1∼N(attn†(qt+1,Kt,Vt),Σt). We note that attn†bakes the BMA algorithm for the
Gaussian linear model in its architecture , by first estimating ¯ztvia equation 3.8 and deriving the final
estimatefromtheinnerproductbetween ¯ztandqt+1. Here attn†(·)isaninstanceofthe intention mechanism
studied in Garnelo & Czarnecki [2023] and can be viewed as a generalization of linear attention. Recall that
we define the softmax attention [Vaswani et al., 2017] for any q∈Rdk,K∈Rt×dk, andV∈Rt×dvas
attn (q,K,V ) =V⊤softmax (Kq). In the following proposition, we show that the attention in equation 3.9
coincides with the softmax attention as the sequence length goes to infinity.
Proposition 3.6. We assume that Assumption 3.5 holds for the feature mapping ϕof Gaussian RBF kernel
KRBF. In addition, we assume that ∥kt∥=∥vt∥= 1. Then, it holds for a constant C > 0that depends on dk
and anyq∈Rdkwith∥q∥= 1that limT→∞attn†(q,KT,VT) =C·limT→∞attn (q,KT,VT).
The proof is in Appendix E.4. Combined with the conditional probability of vt+1in equation 3.7, this propo-
sitionshowsthat softmax attention approximately encodes BMA inlongtokensequences[Wasserman,
2000], and thus is able to perform ICL when prompted after pretraining. This proposition also implies that
the output of the attention module contains the information of the hidden concept z∗, which will be verified
in experiments.
3.4 Experimental Validations For Proposition 3.6
We conduct two experiments to verify Proposition 3.6. The first experiment is same as the hidden concept
vectors construction experiments in Section 3.2. Proposition 3.6 and equation 3.8 imply that the heads of
attention contain the hidden concept information. Thus, we construct the hidden concept vector as the
average of the values of twenty selected attention heads. The effectiveness of the constructed hidden concept
is demonstrated in Figure 3. This result strongly corroborates with equation 3.8.
In the second experiment, we directly calculate the ratio between attn†and attn. We consider the case
dv= 1anddk=dfor somed > 0. The entries in Kof equation 3.9 are i.i.d. samples of Gaussian
distribution, and the i−th entry of Vis calculated as the inner product between a Gaussian vector and
thei−th column. Figure 4 shows the results for d= 2andd= 3. It shows that the ratio between attn†
and attnwill converge to a constant. This constant depends on the dimension d, which originates from
Proposition E.5.
4 Theoretical Analysis of Pretraining
4.1 Pretraining Algorithm
In this section, we describe the pretraining setting. We largely follow the transformer structures in Brown
et al. [2020]. The whole network is a composition of Dsub-modules, and each sub-module consists of a
MHA and a Feed-Forward (FF) fully connected layer. Here, D> 0is the depth of the network. The whole
network takes X(0)=X∈RL×das its input. In the t-th layer for t∈[D], it first takes the output X(t−1)of
the(t−1)-th layer as the input and forwards it through MHA with a residual link and a layer normalization
Πnorm(·)to outputY(t), which projects each row of the input into the unit ℓ2-ball. Here we take dh=din
MHA, and the generalization of our result to general cases is trivial. Then the intermediate output Y(t)is
forwarded to the FF module. It maps each row of the input Y(t)∈RL×dthrough the same single-hidden
layer neural network with dFneurons, that is ffn(Y(t),A(t)) =ReLU (Y(t)A(t)
1)A(t)
2, whereA(t)
1∈Rd×dF, and
A(t)
2∈RdF×dare the weight matrices. Combined with a residual link and layer normalization, it outputs
the output of layer tasX(t), that is
Y(t)=Πnorm/bracketleftbig
mha(X(t−1),W(t)) +γ(t)
1X(t−1)/bracketrightbig
,
X(t)=Πnorm/bracketleftbig
ffn(Y(t),A(t)) +γ(t)
2Y(t)/bracketrightbig
. (4.1)
Here we allocate weights γ(t)
1andγ(t)
2to residual links only for the convenience of theoretical analysis.
In the last layer, the network outputs the probability of the next token via a softmax module, that is
8Under review as submission to TMLR
Y(D+1)=softmax (I⊤
LX(D)A(D+1)/(Lτ))∈Rdy, where IL∈RLis the vector with all ones, A(D+1)∈Rd×dy
is the weight matrix, τ∈(0,1]is the fixed temperature parameter, and dyis the output dimension. The
parameters of each layer are denoted as θ(t)= (γ(t)
1,γ(t)
2,W(t),A(t))fort∈[D]andθ(D+1)=A(D+1), and
the parameter of the whole network is the concatenation of these parameters, i.e., θ= (θ(1),···,θ(D+1)).
We consider the transformers with bounded parameters. The set of parameters is
Θ =/braceleftig
θ|/vextenddouble/vextenddoubleA(D+1),⊤/vextenddouble/vextenddouble
1,2≤BA,max/braceleftbig/vextendsingle/vextendsingleγ(t)
1/vextendsingle/vextendsingle,/vextendsingle/vextendsingleγ(t)
2/vextendsingle/vextendsingle/bracerightbig
≤1,/vextenddouble/vextenddoubleA(t)
1/vextenddouble/vextenddouble
F≤BA,1,/vextenddouble/vextenddoubleA(t)
2/vextenddouble/vextenddouble
F≤BA,2,
/vextenddouble/vextenddoubleWQ,(t)
i/vextenddouble/vextenddouble
F≤BQ,/vextenddouble/vextenddoubleWK,(t)
i/vextenddouble/vextenddouble
F≤BK,/vextenddouble/vextenddoubleWV,(t)
i/vextenddouble/vextenddouble
F≤BV,t∈[D],i∈[h]/bracerightig
,
whereBA,BA,1,BA,2,BQ,BK, andBVare the bounds of parameter. Here we only consider the non-trivial
case where these bounds are larger than 1, otherwise, the magnitude of the output in Dthlayer decreases
exponentially with growing depth. The probability induced by the transformer with parameter θis denoted
asPθ.
The pretraining dataset consists of Npindependent trajectories. For the n-th trajectory with n∈[Np], a
hidden concept zn∼PZ(z)∈∆(Z)is first sampled, which is the hidden variable of the token sequence
to generate, e.g., the theme, the sentiment, and the style. Then the tokens are sequentially sampled from
the model in equation 3.2. We view this model as a Markov chain in the sequence space X∗induced by
zn, i.e.,xn
t+1∼P(·|Sn
t,zn)andSn
t+1= (Sn
t,xn
t+1), wherexn
t+1∈Xcan be either rtor/tildewidectin equation 3.2,
andSn
t,Sn
t+1∈X∗. This Markov chain is defined with respect to the state Sn
t, which obviously satisfies
the Markov property since Sn
ifori∈[t−1]are contained in Sn
t. The pretraining dataset is DNp,Tp=
{(Sn
t,xn
t+1)}Np,Tp
n,t=1where the concepts znis hidden from the context and thus unobserved. Here each token
sequence is divided into Tppieces{(Sn
t,xn
t+1)}Tp
t=1. We highlight that this pretraining dataset collecting
process subsumes those for GPT, and Masked AutoEncoders (MAE) [Radford et al., 2021]. For GPT, each
trajectory corresponds to a paragraph or an article in the pretraining dataset, and zn∼PZ(z)is realized
by the selection process of these contexts from the Internet. For MAE, we take Tp= 1, andSn
1andxn
2
respectively correspond to the image and the masked token.
To pretrain the transformer, we adopt the cross-entropy as the loss function, which is widely used in the
training of BERT and GPT. The pretraining algorithm is
/hatwideθ= argmin
θ∈Θ−1
NpTpNp/summationdisplay
n=1Tp/summationdisplay
t=1logPθ(xn
t+1|Sn
t). (4.2)
We first analyze the population version of equation 4.2. In the training set, the conditional distribu-
tion ofxn
t+1conditioned on Sn
tisP(xn
t+1|Sn
t) =/integraltext
ZP(xn
t+1|Sn
t,z)PZ(z|Sn
t)dz, where the unobserved
hidden concept is weighed via its posterior distribution. Thus, the population risk of equation 4.2 is
Et[ESt[KL(P(·|St)∥Pθ(·|St)) +H(P(·|St))]], wheret∼Unif ([Tp]),H(p) =−⟨p,logp⟩is the entropy, and
Stis distributed as the pertaining distribution. Thus, we expect that Pθwill converge to P. For MAE, the
network training adopts ℓ2-loss, and we defer the analysis of this case to Appendix F.4.
4.2 Performance Guarantee for Pretraining
We first state the assumptions for the pretraining setting.
Assumption 4.1. There exists a constant R > 0such that for any z∈ZandSt∼P(·|z), we have
∥S⊤
t∥2,∞≤Ralmost surely.
This assumption states that the ℓ2-norm of the magnitude of each token in the token sequence is upper
bounded by R > 0. It is satisfied in real applications, where the token is a finite-dimensional vector with
bounded components. For example, the tokenizers used in GPT-NeoX-20B [Black et al., 2022] and Llama2
[Touvron et al., 2023] both satisfy this assumption.
Assumption 4.2. Thereexistsa constant c0>0suchthat forany z∈Z,x∈XandS∈X∗,P(x|S,z)≥c0.
9Under review as submission to TMLR
This assumption states that the conditional probability of xconditioned on Sandzis lower bounded. This
comes from the ambiguity of language, that is, a sentence can take lots of words as its next word. It holds in
a wide range of problems, which is verified by the success of LLMs. Concretely, the transformers with finite
width, depth, and weights can only model the distribution that satisfies Assumption 4.2., since the last soft-
max layer of the transformer renders the probability of each token strictly larger than 0. Since transformers
have successfully learned a large range of tasks, it is reasonable to assume that those distributions satisfy
this assumption. Similar regularity assumptions are also widely adopted in ICL literature [Xie et al., 2021;
Wies et al., 2023]. The parameter c0depends on both the hidden concept set Zand the prompt. To state
our result, we respectively use ES∼DandPDto denote the expectation and the distribution of the average
distribution of Sn
tinDNp,Tp, i.e.,ES∼D[f(S)] =/summationtextTp
t=1ESt[f(St)]/Tpfor any function f:X∗→R.
Theorem 4.3. Let¯B=τ−1RhBABA,1BA,2BQBKBVand ¯D=D2d(dF+dh+d) +d·dy. Under Assump-
tions 4.1 and 4.2, the pretrained model P/hatwideθby the algorithm in equation 4.2 satisfies
ES∼D/bracketleftig
TV/parenleftbig
P(·|S),P/hatwideθ(·|S)/parenrightbig/bracketrightig
=O/parenleftbigg
inf
θ∗∈Θ/radicalig
ES∼DKL/parenleftbig
P(·|S)∥Pθ∗(·|S)/parenrightbig
+√
b∗t1/4
mixlog 1/δ
(NpTp)1/4
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
approximation error+√tmix/radicalbig
NpTp/parenleftig
¯Dlog(1+NpTp¯B)+log1
δ
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
generalization error/parenrightig/parenrightbigg
with probability at least 1−δ, whereb∗= log(max{c−1
0,1 +dyexp(BA/τ)}), andtmixis the mixing time of
the Markov chains induced by P, formally defined in Appendix F.1.
We define the right-hand side of the equation as ∆pre(Np,Tp,δ). The first and the second terms in the
bound are the approximation error . It measures the distance between the nominal distribution Pand
the distributions induced by transformers with respect to KL divergence. If the nominal model Pcan be
represented by transformers exactly, i.e., the realizable case, these two terms will vanish. The third term is
thegeneralization error , and it does not increase with the growing sequence length Tp. This is proved via
the PAC-Bayes framework.
This pretraining analysis is missing in most existing theoretical works about ICL. Xie et al. [2021], Wies
et al. [2023], and Jiang [2023] all assume access to an arbitrarily precise pretraining model. Although the
generalization bound in Li et al. [2023b] can be adapted to the pretraining analysis, the risk definition therein
can not capture the approximation error in our result. Furthermore, their analysis cannot fit the maximum
likelihood algorithm in equation 4.2. Concretely, their result can only show that the convergence rate of KL
divergence is O((NpTp)−1/2)with a realizable function class. Combined with Pinsker’s inequality, this gives
the convergence rate for total variation as O((NpTp)−1/4)even in the realizable case.
The deep neural networks are shown to be universal approximators for many function classes [Cybenko,
1989; Yarotsky, 2017]. Thus, the approximation error in Theorem 4.3 should vanish with the increasing size
of the transformer. To achieve this, we slightly change the structure of the transformer by admitting a bias
term in feed-forward modules, taking A(t)
2∈RdF×dF, and admitting dFto vary across layers. This mildly
affects the generalization error by replacing D·dFby the sum of dFof all the layers in Theorem 4.3. We
derive the approximation error bound when the dimension of each word is equal to one, i.e., X⊆R. Our
method can carry over the case d>1.
Proposition 4.4 (Informal) .Undercertainsmoothnessconditions, iftheparameterspaceofthetransformer
function class Θis large enough, then for some constant C > 0, we have
inf
θ∗∈Θmax
∥S⊤∥2,∞≤RKL/parenleftbig
P(·|S)∥Pθ∗(·|S)/parenrightbig
=O/parenleftbigg
c−1
0exp/parenleftbigg
−D1/4
C·B/parenrightbigg/parenrightbigg
,
whereBis the parameter related to the smoothness, which is specified in Appendix F.3.
The formal statement and proof are deferred to Appendix F.3. This proposition states that the approxima-
tion error decays exponentially with the increasing depth . Combined with this result, Theorem 4.3
provides the full description of the pretraining performance.
10Under review as submission to TMLR
5 ICL Error under Practical Settings
ICL Average Rrror with an Imperfectly Pretrained Model In Section 3, we study the ICL average
error with a perfect pretrained model. In what follows, we characterize the ICL average error when the
pretrained model has an error. Note that the distribution DICLof the prompts of ICL tasks can be different
from that of pretraining. We impose the following assumption on their relation.
Assumption 5.1. We assume that there exists an absolute constant κ>0such that for any ICL prompt
pt∈X∗with length less and equal to Tp, it holds that PDICL(pt)≤κ·PD(pt).
This assumption states the coverage of the prompt distribution by the pretraining distribution. We note
that there will be an information-theoretic barrier without this assumption. For example, if the pretraining
data does not contain any material about a specific mathematical symbol in the ICL prompt. In this case,
it will be extremely difficult for the LLM to derive the correct prediction, since the meaning of this math
symbol is unclear to LLMs. The parameter κscales with the size of the hidden space Zand the length of
the prompt. In the worst case, κ=O(|Z|∗Tp). Usually, we note that Npis substantially larger than κ.
Concretely, in the pretraining data generation process, each hidden concept will generate a large number of
sentences. The real-world data set contains a large number of sentences explaining the same concept, e.g.,
the data from Wikipedia. We then have the following theorem characterizing the ICL average error of the
pretrained model.
Theorem 5.2 (ICL average error of Pretrained Model) .We assume that the underlying hidden concept z∗
maximizes/summationtextt
i=1logP(ri|pti−1,z)for anyt∈[T](T≤Tp) and there exists an absolute constant β >0such
that log(1/p0(z∗))≤β. Under Assumptions 3.1, 4.1, 4.2, and 5.1, we have with probability at least 1−δ
that
T−1T/summationdisplay
t=1Ept∼D ICL/bracketleftig
logP(rt|z∗,ptt−1)−logP/hatwideθ(rt|ptt−1)/bracketrightig
≤O/parenleftbig
β/T+κ·b∗·∆pre(Np,Tp,δ)/parenrightbig
,
where we denote by ∆pre(Np,Tp,δ)the pretraining error in Theorem 4.3, and b∗= log max{c−1
0,1 +
dyexp(BA/τ)}.
We note that dyandBAare the parameters of transformers defined in Section 4.1. The requirement T≤Tp
originates from that the pertaining process can only guarantee the performance for prompt not longer than
Tp. Theorem 5.2 shows that the expected ICL average error for the pretrained model is upper bounded
by the sum of two terms: (a) the ICL average error for the underlying true model and(b) the
pretraining error . These two terms are separately bounded in Sections 3 and 4.
Prompting with Wrong Input-Output Mappings In the real-world implementations of ICL, the pro-
vided input-output examples may not conform to the nominal distribution induced by z∗, and the out-
puts in examples can be perturbed . The perturbed prompt is then denoted as pt′= (S′
t,/tildewidect+1), where
S′
t= (/tildewidec1,r′
1,···,/tildewidect,r′
t)∈X∗, andr′
ifori∈[t]is the modified output. Then we have that
Ept′/bracketleftig
KL/parenleftbig
P(·|/tildewidect+1,z∗)∥P/hatwideθ(·|S′
t,/tildewidect+1)/parenrightbig/bracketrightig
=O/parenleftbigg
κ∆pre(Np,Tp,δ)+exp/parenleftbigg
−√
t·∆KL
2(1 +l) log 1/c0/parenrightbigg/parenrightbigg
,
where ∆KLisadistinguishbilityparameter, and listhemaximallengthof /tildewidect. Thefirsttermisthepretraining
errorinTheorem4.3,whichisrelatedtothesizeofthepretrainingsetandthecapacityoftheneuralnetworks.
The second term is the ICL error. Intuitively, this term represents the concept identification error. If the
considered task z∗is distinguishable, i.e., satisfying Assumption G.3, this term decays to 0exponentially in√
t. The required assumptions and formal statement are in Appendix G.2.
6 Conclusion
In this paper, we investigated the theoretical foundations of ICL for the pretrained language models. We
proved that the perfectly pretrained LLMs implicitly implements BMA with regret O(1/t)over a general
response generation modeling, which subsumes the models in previous works. Based on this, we showed
11Under review as submission to TMLR
that the attention mechanism parameterizes the BMA algorithm. Analyzing the pretraining process, we
demonstrated that the total variation between the pretrained model and the nominal distribution consists of
the approximation error and the generalization error. The combination of the ICL regret and the pretraining
performance gives the full description of ICL ability of pretrained LLMs. We mainly focus on the prompts
that comprise several examples in this work and leave the analysis of instruction-based prompts for future
works.
References
A. Agarwal, S. Kakade, A. Krishnamurthy, and W. Sun. Flambe: Structural complexity and representation
learning of low rank MDPs. Advances in Neural Information Processing Systems , 33:20095–20107, 2020.
Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm
is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661 , 2022.
Martin Anthony, Peter L Bartlett, Peter L Bartlett, et al. Neural network learning: Theoretical foundations ,
volume 9. cambridge university press Cambridge, 1999.
Y. Bai, F. Chen, H. Wang, C. Xiong, and S. Mei. Transformers as statisticians: Provable in-context learning
with in-context algorithm selection. arXiv preprint arXiv:2306.04637 , 2023.
P. L Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized margin bounds for neural networks.
Neural Information Processing Systems , 2017.
M.I.Belghazi,A.Baratin,S.Rajeshwar,S.Ozair,Y.Bengio,A.Courville,andD.Hjelm. Mutualinformation
neural estimation. In International Conference on Machine Learning , pp. 531–540. PMLR, 2018.
S. Black, S. Biderman, . Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang,
et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745 ,
2022.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Neural Information Processing Systems , 2020.
Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foun-
dations of Computational Mathematics , 7:331–368, 2007.
Stephanie CY Chan, Adam Santoro, Andrew K Lampinen, Jane X Wang, Aaditya Singh, Pierre H
Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent few-shot learn-
ing in transformers. arXiv preprint arXiv:2205.05055 , 2022.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals
and systems , 2(4):303–314, 1989.
Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can GPT learn In-Context?
Language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559 ,
2022.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirec-
tional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and
Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation.
Advances in neural information processing systems , 32, 2019.
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang
Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234 , 2022.
John C Duchi. Information theory and statistics. Lecture Notes for Statistics , 311:304, 2019.
12Under review as submission to TMLR
Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation
in self-attention mechanisms. arXiv preprint arXiv:2110.10090 , 2021.
Dennis Elbrächter, Dmytro Perekrestenko, Philipp Grohs, and Helmut Bölcskei. Deep neural network ap-
proximation theory. IEEE Transactions on Information Theory , 67(5):2581–2623, 2021.
J. L. Elman. Language as a dynamical system. Mind as motion: Explorations in the dynamics of cognition ,
pp. 195–223, 1995.
Fabian Falck, Ziyu Wang, and Chris Holmes. Is in-context learning in large language models bayesian? a
martingale perspective. arXiv preprint arXiv:2406.00793 , 2024.
G. Feng, Y. Gu, B. Zhang, H. Ye, D. He, and L. Wang. Towards revealing the mystery behind chain of
thought: a theoretical perspective. arXiv preprint arXiv:2305.15408 , 2023.
Kenji Fukumizu. Nonparametric bayesian inference with kernel mean embedding. In Modern Methodology
and Applications in Spatial-Temporal Modeling , pp. 1–24. Springer, 2015.
Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context?
A case study of simple function classes. arXiv preprint arXiv:2208.01066 , 2022.
Marta Garnelo and Wojciech Marian Czarnecki. Exploring the space of key-value-query models with inten-
tion.arXiv preprint arXiv:2305.10203 , 2023.
N. Gruver, M. Finzi, S. Qiu, and A. G. Wilson. Large language models are zero-shot time series forecasters.
arXiv preprint arXiv:2310.07820 , 2023.
Michael Hahn and Navin Goyal. A theory of emergent in-context learning as implicit structure induction.
arXiv preprint arXiv:2303.07971 , 2023.
Or Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. Instruction induction: From few examples
to natural language task descriptions. arXiv preprint arXiv:2205.10782 , 2022.
Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: NNGP and NTK
for deep attention networks. In International Conference on Machine Learning , 2020.
Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt
Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. OPT-IML: Scaling language model instruction
meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017 , 2022.
H. J. Jeon, J. D. Lee, Q. Lei, and B. Van Roy. An information-theoretic analysis of in-context learning.
arXiv preprint arXiv:2401.15530 , 2024.
Hui Jiang. A latent space theory for emergent abilities in large language models. arXiv preprint
arXiv:2304.09960 , 2023.
Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. arXiv preprint
arXiv:2006.15595 , 2020.
Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. Self-
generated in-context learning: Leveraging auto-regressive language models as a demonstration generator.
arXiv preprint arXiv:2206.08082 , 2022.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language
models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 , 2022.
Antoine Ledent, Waleed Mustafa, Yunwen Lei, and Marius Kloft. Norm-based generalisation bounds for
deep multi-class convolutional neural networks. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 35, pp. 8279–8287, 2021.
13Under review as submission to TMLR
Y. Li, Y. Li, and A. Risteski. How do transformers learn topic structure: Towards a mechanistic under-
standing. arXiv preprint arXiv:2303.04245 , 2023a.
Yingcong Li, M Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms:
Generalization and stability in in-context learning. arXiv preprint arXiv:2301.07067 , 2023b.
Renjie Liao, Raquel Urtasun, and Richard Zemel. A pac-bayesian approach to generalization bounds for
graph neural networks. arXiv preprint arXiv:2012.07690 , 2020.
Shan Lin and Jingwei Zhang. Generalization bounds for convolutional neural networks. arXiv preprint
arXiv:1910.01487 , 2019.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes
good in-context examples for gpt- 3?arXiv preprint arXiv:2101.06804 , 2021.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,
prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM
Computing Surveys , 55(9):1–35, 2023.
YaoLu,MaxBartolo,AlastairMoore,SebastianRiedel,andPontusStenetorp. Fantasticallyorderedprompts
and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786 ,
2021.
Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. A kernel-based view of
language model fine-tuning. arXiv preprint arXiv:2210.05643 , 2022.
Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances in neural
information processing systems , 32, 2019.
Vladimir Mikulik, Grégoire Delétang, Tom McGrath, Tim Genewein, Miljan Martic, Shane Legg, and Pedro
Ortega. Meta-trained agents implement bayes-optimal agents. Advances in neural information processing
systems, 33:18691–18703, 2020.
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context.
arXiv preprint arXiv:2110.15943 , 2021.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint
arXiv:2202.12837 , 2022.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to spectrally-
normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564 , 2017.
P. Niyogi, R. C. Berwick, et al. A dynamical systems model for language change. Complex Systems , 11(3):
161–204, 1997.
Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi.
Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. arXiv preprint
arXiv:2206.03126 , 2022.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. Advances in Neural Information Processing Systems , 35:27730–27744, 2022.
Daniel Paulin. Concentration inequalities for markov chains by marton couplings and spectral methods.
Electronic Journal of Probability , 2015.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In International conference on machine learning , pp. 8748–8763. PMLR,
2021.
14Under review as submission to TMLR
Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning.
arXiv preprint arXiv:2112.08633 , 2021.
Le Song, Jonathan Huang, Alex Smola, and Kenji Fukumizu. Hilbert space embeddings of conditional
distributions with applications to dynamical systems. In International Conference on Machine Learning ,
2009.
E. Todd, M. L. Li, A. S. Sharma, A. Mueller, B. C. Wallace, and D. Bau. Function vectors in large language
models.arXiv preprint arXiv:2310.15213 , 2023.
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava,
S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 ,
2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems , 2017.
Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, An-
drey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint
arXiv:2212.07677 , 2022.
James Vuckovic, Aristide Baratin, and Remi Tachet des Combes. A mathematical theory of attention. arXiv
preprint arXiv:2007.02876 , 2020.
Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are implicitly topic models:
Explaining and finding good demonstrations for in-context learning. arXiv preprint arXiv:2301.11916 ,
2023.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Han-
naneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint
arXiv:2212.10560 , 2022.
Larry Wasserman. Bayesian model selection and model averaging. Journal of Mathematical Psychology , 44
(1):92–107, 2000.
Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on ap-
proximating turing machines with transformers. Advances in Neural Information Processing Systems , 35:
12071–12083, 2022a.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M
Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 ,
2021.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv
preprint arXiv:2206.07682 , 2022b.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of
thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 , 2022c.
Noam Wies, Yoav Levine, and Amnon Shashua. The learnability of in-context learning. arXiv preprint
arXiv:2303.07895 , 2023.
Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning
as implicit Bayesian inference. arXiv preprint arXiv:2111.02080 , 2021.
GregYang. TensorprogramsII:Neuraltangentkernelforanyarchitecture. arXiv preprint arXiv:2006.14548 ,
2020.
15Under review as submission to TMLR
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks , 94:103–114,
2017.
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are trans-
formers universal approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077 ,
2019.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexan-
der J Smola. Deep sets. Neural Information Processing Systems , 2017.
Fengzhuo Zhang, Boyi Liu, Kaixin Wang, Vincent YF Tan, Zhuoran Yang, and Zhaoran Wang. Rela-
tional reasoning via set transformers: Provable efficiency and applications to MARL. arXiv preprint
arXiv:2209.09845 , 2022a.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large
language models. arXiv preprint arXiv:2210.03493 , 2022b.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier
Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language
models.arXiv preprint arXiv:2205.10625 , 2022a.
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy
Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910 , 2022b.
16