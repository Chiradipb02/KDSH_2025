Under review as submission to TMLR
BONE: a unifying framework for Bayesian online learning in
non-stationary environments
Anonymous authors
Paper under double-blind review
Abstract
We propose a unifying framework for methods that perform Bayesian online learning in
non-stationary environments. We call the framework BONE, which stands for (B)ayesian
(O)nline learning in (N)on-stationary (E)nvironments. BONE provides a common structure to
tackle a variety of problems, including online continual learning, prequential forecasting, and
contextual bandits. The framework requires specifying three modelling choices: (i) a model
for measurements (e.g., a neural network), (ii) an auxiliary process to model non-stationarity
(e.g., the time since the last changepoint), and (iii) a conditional prior over model parameters
(e.g., a multivariate Gaussian). The framework also requires two algorithmic choices, which
we use to carry out approximate inference under this framework: (i) an algorithm to estimate
beliefs (posterior distribution) about the model parameters given the auxiliary variable,
and (ii) an algorithm to estimate beliefs about the auxiliary variable. We show how this
modularity allows us to write many different existing methods as instances of BONE; we
also use this framework to propose a new method. We then experimentally compare existing
methods with our proposed new method on several datasets; we provide insights into the
situations that make one method more suitable than another for a given task.
1 Introduction
Our goal in this paper is to study adaptive probabilistic methods that learn to make accurate predictions about
the next output yt+1based on the next input xt+1and a sequence of past inputs and outputs, (x1:t,y1:t),
wheretindexes time. This is often called prequential forecasting (Gama et al., 2008).
Many prediction methods assume that the data generating process (DGP) p(yt+1|xt+1,x1:t,y1:t)is static
through time. However, real-world data often comes from non-stationary distributions, where the underlying
data distribution changes, either gradually (e.g., rising mean temperature) or abruptly (e.g., price shocks
after a major news event). In this paper we propose a unified framework for tackling (one-step-ahead)
forecasts in such (potentially) non-stationary environments. Our framework encompasses many existing
lines of work, including online continual learning (Dohare et al., 2024), prequential forecasting (Liu, 2023),
test-time adaptation (Schirmer et al., 2024), and neural contextual bandits (Riquelme et al., 2018).
The framework we propose in this paper, which we call BONE — which stands for (B)ayesian (O)nline
learning in (N)on-stationary (E)nvironments — is based on a form of Bayesian inference in a hierarchical
model,1and is composed of three modelling choices and two algorithmic choices. The modelling choices
are: (M.1) a model for the measurements, (M.2) an auxiliary process to model non-stationarity, and (M.3) a
prior over model parameters conditioned on the auxiliary process and the past data. The algorithmic choices
1It is worth clarifying what we mean by “a form of Bayesian inference”. We focus on adaptive rules, which are defined
as “accumulating experience about the properties of the environment and leveraging this experience to improve the model’s
performance” (Peterka, 1981). While the adaptive rules that we consider in this work align with Bayes’ rule, this alignment
does not imply that we adopt the full set of assumptions required to be strictly Bayesian. By strictly Bayesian, we mean a
framework in which the properties of the data-generating process are fully known, well specified, and described within a formal
mathematical model with uncertainty quantified about all unknowns (Faden & Rausser, 1976; Peterka, 1981; Knoblauch et al.,
2022). Thus, our approach uses Bayes’ rule only to provide a rational basis for sequential decision-making while preserving
adaptability.
1Under review as submission to TMLR
are: (A.1) an algorithm to compute an approximate posterior over the model parameters given the auxiliary
variables, and (A.2) an algorithm to compute an approximate posterior —or more generally, a set of weights—
over the auxiliary variables. We show how these different axes of variation span a wide variety of existing and
new methods (see Table 2). We then perform an experimental comparison on a variety of tasks. Specifically,
we consider prequential forecasting, classification, bandits, and unsupervised time-series segmentation. An
easy-to-use library that implements these methods, written in Jax (Bradbury et al., 2018), is available at
[link-here-after-blind-review] .
To summarise, our contributions are threefold: (1) we provide an extensive literature review on methods
that tackle non-stationarity, and show that they can all be written as instances of our unified BONE
framework; (2) we use the BONE framework to develop a new method; and (3) we perform an experimental
comparison of many existing methods and the new method on environments with both abrupt changes and
gradually-changing distributions.
2 The framework
2.1 The stationary case
Consider a sequence of target measurements y1:t= (y1,...,yt)withyi∈Rd, and features x1:t= (x1,...,xt)
withxi∈Rq. LetDt= (xt,yt)be a datapoint and D1:t= (D1,...,Dt)the dataset at time t. We are
interested in methods that efficiently compute p(yt+1|xt+1,D1:t)in a recursive, online way. In our setting,
one observesxt+1just before observing yt+1; thus, to make a prediction about yt+1, we havex1:t+1andy1:t
at our disposal.2
Our estimate of the mean of p(yt+1|xt+1)is encoded in a parametric model h(θt;xt+1)(e.g., a neural
network), where θt∈Rmis the parameter vector. Since the parameters are unknown, we adopt a Bayesian
approach and write the prediction function as
E[h(θt;xt+1)|D1:t] =/integraldisplay
h(θt;xt+1)λ(θt;D1:t)dθt, (1)
whereλ(θt;D1:t)is a density over the model parameters θt∈Rm. In what follows we take λ(θt;D1:t)to be
the Bayesian posterior density (or an approximation)
λ(θt;D1:t) =p(θt|D1:t). (2)
Next, we modify the above static framework to account for non-stationarity.
2.2 BONE: the non-stationary case
The class of methods described above works well when the data-generating process is well-specified. In practice,
however, this is often not the case. Thus, to adapt to regime changes and other forms of non-stationarity, we
introduce an auxiliary random variable ψt∈Ψt, where Ψtis a set of possible values of the auxiliary variable
ψt. The purpose of ψtis to encode information about the current regime so this can be used to predict the
upcoming measurement. This can be achieved, for instance, by determining which past data points most
closely align with the most recent measurements or by selecting a model —from a set of candidate models—
that predicts the future observation. We describe the auxiliary variable in detail in Section 2.6.
GivenD1:t,xt+1, andψt, a prediction for yt+1is
ˆy(ψt)
t+1=Eλt[h(θt;xt+1)|ψt] :=/integraldisplay
h(θt;xt+1)λ(θt;ψt,D1:t)dθt. (3)
Here, we use the shorthand notation λtforλ(θt;ψt,D1:t), and we write Eλt[·|ψt]andˆy(ψt)
t+1to draw attention
to the dependence on ψt. Given that λtis the Bayesian posterior,
λ(θt;ψt,D1:t)∝τ(θt;ψt,D1:t−1)p(yt|θt,xt), (4)
2The input features xt+1and output measurements yt+1can correspond to different time steps. For example, xt+1can be
the state of the stock market at a fixed date and yt+1is the return on a stock some days into the future.
2Under review as submission to TMLR
whereτis theconditional prior density. This is a modelling choice that imposes an inductive bias on the
model parameters based on the data D1:t−1and the value ψt. The term p(yt|θt,xt)is the likelihood function,
which is assumed to have the property that for any t
E[yt|θt,xt] =/integraldisplay
ytp(yt|θt,xt) dyt=h(θt;xt). (5)
Finally, we introduce the function νt(ψt)that weights the importance of each ψt∈Ψtwhen making an
estimate ofyt+1. More precisely, we use νt(ψt)to make the estimation ˆyt+1as follows
ˆyt+1=/integraldisplay/parenleftbigg/integraldisplay
h(θt;xt+1)λt(θt;ψt,D1:t) dθt/parenrightbigg
νt(ψt) dψt
=:Eνt/bracketleftig
Eλt[h(θt;xt+1)|ψt]/bracketrightig
.(6)
In the Bayesian approach, we use νt(ψt) =p(ψt|D1:t), but we also consider other ways of defining this measure
weighting term in order to accommodate non-Bayesian approaches in the literature, as we discuss in Section
2.6.
As we show in this paper, many methods in the literature can be written as instances of (6). In particular,
these methods depend on the modelling choices of: (M.1: likelihood) the model for the measurements in the
form of the density p(yt|θt,xt)which in turns determines hthrough(5); (M.2: auxvar) an auxiliary variable
ψtthat modulates the predictions when non-stationarity is present; (M.3: prior) a conditional prior density τ
over model parameters conditioned on past information and the auxiliary variable ψ. Having specified (M.1 –
M.3) inference of the model is given by (A.1: posterior) a method to compute or approximate λtin(4), and
(A.2: weighting) a method to compute or approximate νtwhich weights possible values of ψt.
Algorithm 1 presents pseudocode for the prediction and update steps of a method within the BONE framework.
Notably, these components can be broadly divided into two categories: modeling and algorithmic. The
modelling components determine the inductive biases in the model, and correspond to h,ψt, andτ. The
algorithmic components dictate how operations are carried out to produce a final prediction — this corresponds
toλtandνt.
Algorithm 1 Generic predict and update step of BONE with discrete ψtat timet.
Require:D1:t// past data
Require:xt+1// optional inputs
Require:p(y|θ,xt)// Choice of (M.1: likelihood)
Require: Ψt// Choice of (M.2: auxvar)
1:forψt∈Ψtdo
2:τt(θt;ψt)←τ(θt|ψt,D1:t−1)// choice of (M.3: prior)
3:λt(θt;ψt)←λ(θt|ψt,D1:t)∝τt(θtψt)p(yt|θt,xt)// choice of (A.1: posterior)
4:νt(ψt)←ν(ψt|D1:t)// choice of (A.2: weighting)
5: ˆy(ψt)
t+1←Eλt[h(θt;xt+1)|ψt]// conditional prequential prediction
6:end for
7:ˆyt+1←/summationtext
ψtνt(ψt)ˆy(ψt)
t+1// weighted prequential prediction
2.3 Example tasks that can be solved with BONE
Before going into more detail about BONE, we give some concrete examples of tasks in which the BONE
framework can be applied. We group these examples into unsupervised tasks and supervised tasks.
2.3.1 Unsupervised tasks
Unsupervised tasks involve estimating unobservable quantities of interest from the data D1:t. Below, we
present three common tasks in this category.
3Under review as submission to TMLR
Segmentation Segmentation involves partitioning the data stream into contiguous subsequences or “blocks”
{D1:t1,Dt1+1:t2,...}, where the DGP for each block is governed by a sequence of unknown functions (Barry &
Hartigan, 1992). The goal is to determine the points in time when a new block begins, known as changepoints.
This is useful in many applications, such as finance, where detecting changes in market trends is critical. For
further reference, see e.g, Aminikhanghahi & Cook (2017); Gupta et al. (2024). In this setting, non-stationarity
is assumed to be abrupt and occurring at unknown points in time. We study an example in Section 4.3.1.
Filtering using state-space models (SSM )Filtering aims to estimate an underlying latent state θt
which evolves over time and often represents a meaningful concept. The posterior estimate of θtis computed
by applying Bayesian inference to the corresponding state space model (SSM), which determines the choice of
(M.1: likelihood), and how the state changes over time, through the choice of (M.3: prior). Examples include
estimating the state of the atmosphere (Evensen, 1994), tracking the position of a moving object (Battin,
1982), or recovering a signal from a noisy system (Basseville et al., 1993). In this setting, non-stationarity is
usually assumed to be continuous and occurring at possible time-varying rates.
Segmentation using Switching state-space models (SSSM) In this task, the modeller extends the
standard SSM with a set of discrete latent variables ψt∈{1,...,K}, which may change value at each time
step according to a state transition matrix. The parameters of the rest of the DGP depend on the discrete
stateψt. The objective is to infer the sequence of underlying discrete states that best “explains” the observed
data (Ostendorf et al., 1996; Ghahramani & Hinton, 2000; Beal et al., 2001; Fox et al., 2007; Van Gael et al.,
2008; Linderman et al., 2017). In this context, non-stationarity arises from the switching behaviour of the
underlying discrete process.
2.3.2 Supervised tasks
Supervised tasks involve predicting a measurable outcome yt. Unlike unsupervised tasks, this allows the
performance of the model to be assessed in an objective manner, since we can compare the prediction to
the actual observation. Supervised tasks have been the main focus of the machine learning community. We
present three common tasks in this category below.
Prequential forecasting Prequential (or one-step-ahead) forecasting (Gama et al., 2008) seeks to predict
the valueyt+1givenD1:tandxt+1. This is distinct from time-series forecasting, which typically does not
consider exogenous variables xt, and thus can forecast (or “roll out”) many steps into the future. We study
an example in Section 4.1.
Online continual learning (OCL) OCL is similar to prequential learning, in that the goal is to learn
a supervised model for regression or classification online. However, the objective is to train a model that
performs consistently across both past and future data, rather than just focusing on future forecasting (Cai
et al., 2021). The changepoints (corresponding to different “tasks”) may or may not be known. This setting
addresses the stability-plasticity dilemma, focusing on retaining previously learned knowledge while adapting
to new tasks. We study an example of OCL for classification, when the task boundaries are not known, in
Section 4.1.2.
(Non-stationary) contextual bandits In contextual bandit problems, the agent is presented with fea-
turesxt+1, and must choose an action (arm) that yields the highest expected reward (Li et al., 2010). We
letyt+1∈RAwhereA>2is the number of possible actions; this is a vector where the a-th entry contains
the reward one would have obtained had one chosen arm a. Lety(a)
tbe the observed reward at time tafter
choosing arm a, i.e., thea-th entry ofyt. A popular approach for choosing the optimal action (while tackling
the exploration-exploitation tradeoff) at each step is Thomson sampling (TS) (Thompson, 1933), which in
our setting works as follows: first, sample a parameter vector from the posterior, ˜θtfromλ(θt;ψt,D1:t);
then, greedily choose the best arm (the one with the highest expected payoff), at+1=arg maxaˆy(a)
t+1, where
ˆyt+1=h(˜θt;xt+1); and ˆy(a)
t+1is thea-th entry of ˆyt+1; finally, receive a reward y(at+1)
t+1. The goal is to select a
sequence of arms{a1,...,aT}that maximises the cumulative reward/summationtextT
t=1y(at)
t. When the mapping function
4Under review as submission to TMLR
his a neural network, this model is called a neural bandit. TS for neural bandits has been studied in many
papers, see e.g., Duran-Martin et al. (2022) and references therein. Non-stationary bandits have been studied
in Mellor & Shapiro (2013); Cartea et al. (2023a); Alami (2023); Liu et al. (2023). We study an example in
Section 4.2.
2.4 Details of BONE
In the following subsections, we describe each component of the BONE framework in detail, provide illustrative
examples, and reference relevant literature for further reading.
2.5 The measurement (likelihood) model (M.1)
Recall that h(θt;xt+1)is defined by
h(θt;xt+1) =E[yt+1|θt,xt+1]. (7)
For example, for linear measurement models, the expected value is given by:
h(θ,x) =

θ⊺x (regression) ,y∈R
σ(θ⊺x) (binary classification) ,y∈{0,1}
Softmax(θ⊺x)(multi-class classification) ,y∈{0,1}C(8)
whereσ(z) = (1 + exp(−z))−1is the sigmoid function, C∈Nis the number of classes, Softmax (z)k=
exp(zk)//summationtext
iexp(zi)represents the softmax function with z∈Rdandzithei-th element of z. In the machine
learning literature, the vector zis called the logits of the classifier. For non-linear measurement models, such
as neural networks, h(θ,x)represents the output of the network parameterised by θ. The best choice of h
will depend on the nature of the data, as well as the nature of the task, in particular, whether it is supervised
or unsupervised. We give some examples in Section 4.
2.6 The auxiliary variable (M.2)
The choice of auxiliary variable ψtis crucial to identify changes in the data-generating process, allowing our
framework to track non-stationarity. Below, we give a list of the common auxiliary variables used in the
literature.
RL(runlength): ψt=rt∈{0,...,t}is a scalar representing a lookback window , defined as the number of
steps since the last regime change. The value rt= 0indicates the start of a new regime at time t, while
rt≥1denotes the continuation of a regime with a lookback window of length rt. This choice of auxiliary
variable is common in the changepoint detection literature. See e.g., Adams & MacKay (2007); Knoblauch
et al. (2018); Alami et al. (2020); Agudelo-España et al. (2020); Altamirano et al. (2023); Alami (2023).
RLCC(runlength and changepoint count): ψt= (rt,ct)∈{0,...,t}×{0,...,t}is a vector that represents both
the runlength and the total number of changepoints, as proposed in Wilson et al. (2010). When rt=t, this
impliesct= 0, meaning no changepoints have occurred. Conversely, rt= 0indicates the start of a new regime
and implies ct∈{1,...,t}, accounting for at least one changepoint. For a given rt≥0, the changepoint
countctbelongs to the range {1,...,t−rt}. As with RL, this auxiliary variable assumes consecutive time
blocks, but additionally allows us to estimate the likelihood of entering a new regime by tracking the number
of changepoints seen so far.
CPT(changepoint timestep): ψt=ζt, withζt={ζ1,t,...,ζℓ,t}, is a set of size ℓ∈{0,...t}containing the ℓ
times at which there was a changepoint, with the convention that 0≤ζ1,t<ζ2,t<...<ζ ℓ,t≤t. This choice
of auxiliary variable was introduced in Fearnhead & Liu (2007) and has been studied in Fearnhead & Liu
(2011); Fearnhead & Rigaill (2019). Under mild assumptions, it can be shown that CPTis equivalent to RL,
see e.g., Knoblauch & Damoulas (2018).
CPL(changepoint location): ψt=s1:t∈{0,1}tis a binary vector. In one interpretation, si= 1indicates the
occurrence of a changepoint at time i, as in Li et al. (2021), while in another, it means that Dtbelongs to
the current regime, as in Nassar et al. (2022).
5Under review as submission to TMLR
CPV(changepoint probability vector): ψt=v1:t∈(0,1)tis at-dimensional random vector representing the
probability of each element in the history belonging to the current regime. This generalises CPLand was
introduced in Nassar et al. (2022) for online continual learning, allowing for a more fine-grained representation
of changepoints over time.
CPP(changepoint probability): ψt=υt∈(0,1)represents the probability of a changepoint. This is a special
case of CPVthat tracks only the most recent changepoint probability; this choice was used in Titsias et al.
(2024) for online continual learning.
ME(mixture of experts): ψt=αt∈{1,...,K}represents one of Kexperts. Each expert corresponds to
either a choice of model or one of Kpossible hyperparameters. This approach has been applied to filtering
(Chaer et al., 1997) and prequential forecasting (Liu, 2023; Abélès et al., 2024).
C:ψt=crepresents a constant auxiliary variable, where cis just a placeholder or dummy value. This is
equivalent to not having an auxiliary variable, or alternatively, to having a single expert that encodes all
available information; this choice recovers the stationary model in (1).
Space-time complexity There is a tradeoff between the complexity that ψis able to encode and the
computation power needed to perform updates. Loosely speaking, this can be seen in the cardinality of
the set of possible values of ψthrough time. Let Ψtbe the space of possible values for ψt. Depending on
the choice of ψt, the cardinality of Ψteither stay constant or increase over time, i.e., Ψt−1⊆Ψtfor all
t= 1,...,T. For instance, the possible values for RLincrease by one at each timestep; the possible values of
CPLdouble at each timestep; finally, the possible values for MEdo not increase. Table 1 shows the space of
values and cardinality that Ψttakes as a function of the choice of auxiliary variable.
name C CPT CPP CPL CPV ME RL RLCC
values {0}2{0,1,...,t}[0,1]{0,1}t(0,1)t{1, . . . , K } { 0,1, . . . , t } {{ 0, t}, . . . ,{t,0}}
cardinality 1 2t∞ 2tinf K t 2 +t(t+ 1)/2
Table 1: Design space for the auxiliary random variables ψt. Here,Tdenotes the total number of timesteps
andKdenotes a fixed number of candidates.
2.7 Conditional prior (M.3)
This component defines the prior predictive distribution over model parameters conditioned on the choice of
(M.2: auxvar) ψtand the datasetD1:t−1. In some cases, explicit access to past data is not needed.
For example, a common assumption is to have a Gaussian conditional prior over model parameters. In this
case, we assume that, given data D1:t−1and the auxiliary variable ψt, the conditional prior takes the form
τ(θt|ψt,D1:t−1) =N/parenleftbig
θt|gt(ψt,D1:t−1),Gt(ψt,D1:t−1)/parenrightbig
, (9)
withgt:Ψt×R(m+d)(t−1)→Rma function that returns the mean vector of model parameters,
E[θt|ψt,D1:t−1], andGt:Ψt×R(m+d)(t−1)→Rm×ma function that returns a m-dimensional covariance
matrix, Cov[θt|ψt,D1:t−1].
Below, we provide a non-exhaustive list of possible combinations of choices for (M.2: auxvar) and (M.3: prior)
of the form (9) that can be found in the literature, and we also introduce a new combination.
C-LSSM(constant linear with affine state-space model). We assume the parameter dynamics can be modeled
by a linear-Gaussian state space model (LSSM), i.e., E[θt|θt−1] =Ftθt−1+btandCov[θt|θt−1] =Qt, for
given (m×m)dynamics matrix Ft,(m×1)bias vectorbt, and (m×m)positive semi-definite matrix Qt.
We also assume ψt=cis a fixed (dummy) constant, which is equivalent to not having an auxiliary variable.
The characterisation of the conditional prior takes the form
gt(c,D1:t−1) =Ftµt−1+bt,
Gt(c,D1:t−1) =FtΣt−1F⊺
t+Qt.(10)
This is a common baseline model that we will specialise below.
6Under review as submission to TMLR
C-OU(constant with Ornstein-Uhlenbeck process). This is a special case of the C-LSSMmodel where Ft=γI,
bt= (1−γ)µ0,Qt= (1−γ2)Σ0,Σ0=σ2
0I,γ∈[0,1]is the fixed rate, and σ0≥0. The conditional prior
mean and covariance are a convex combination of the form
g(c,D1:t−1) =γµt−1+ (1−γ)µ0,
G(c,D1:t−1) =γ2Σt−1+ (1−γ2)Σ0.(11)
This combination is used in Kurle et al. (2019). Smaller values of the rate parameter γcorrespond to a faster
resetting, i.e., the distribution of model parameters revert more quickly to the prior belief (µ0,Σ0), which
means the past data will be forgotten.
CPP-OU(changepoint probability with Ornstein-Uhlenbeck process). Here ψt=υt∈[0,1]is the changepoint
probability that we use as the rate of an Ornstein-Uhlenbeck (OU) process, as proposed in Titsias et al.
(2024); Galashov et al. (2024). The characterisation of the conditional prior takes the form
g(υt,D1:t−1) =υtµt−1+ (1−υt)µ0,
G(υt,D1:t−1) =υ2
tΣt−1+ (1−υ2
t)Σ0.(12)
An example on how to compute υtusing an empirical Bayes procedure is given in (40).
C-ACI(constant with additive covariance inflation). This corresponds to a special case of C-LSSMin which
F=I,b=0, and Q=αIforα>0is the amount of noise added at each step. This combination is used in
Kuhl (1990); Duran-Martin et al. (2022); Chang et al. (2022; 2023) . The characterisation of the conditional
prior takes the form
g(c,D1:t−1) =µt−1,
G(c,D1:t−1) =Σt−1+Qt.(13)
This is similar to C-OUwithγ= 1, however, here we inject new noise at each step. Another variant of this
scheme, known as shrink-and-perturb (Ash & Adams, 2020), takes g(c,D1:t−1) =λµt−1andG(c,D1:t−1) =
Σt−1+Qt, where 0<λ< 1is the shrinkage parameters, and Qt=σ2
0I.
C-Static (constant with static parameters). Here ψt=c(withca dummy variable). This is a special case
of the C-ACIconfiguration in which α= 0. The conditional prior is characterised by
gt(c,D1:t−1) =µt−1,
Gt(c,D1:t−1) =Σt−1.(14)
ME-LSSM (mixture of experts with LSSM). Here ψt=αt∈{1,...,K}, and we have a bank of Kindependent
LSSM models; the auxiliary variable specifies which model to use at each step. The characterisation of the
conditional prior takes the form
gt(αt,D1:t−1) =F(αt)
tµ(αt)
t−1+b(αt)
t,
Gt(αt,D1:t−1) =F(αt)
tΣ(αt)
t−1F⊺
t+Q(αt)
t.(15)
The superscript (αt)denotes the conditional prior for the k-th expert. More precisely, µ(αt)
t−1,Σ(αt)
t−1are the
posterior at time t−1using F(αt)
t−1andQ(αt)
t−1from thek-th expert. This combination was introduced in
Chaer et al. (1997).
RL-PR(runlength with prior reset): for ψt=rt, this choice of auxiliary variable constructs a new mean and
covariance considering the past t−rtobservations. We have
gt(rt,D1:t−1) =µ01(rt= 0) +µ(rt−1)1(rt>0),
Gt(rt,D1:t−1) =Σ01(rt= 0) + Σ(rt−1)1(rt>0),(16)
whereµ(rt−1),Σ(rt−1)denotes the posterior belief computed using observations from indices t−rttot−1.
The casert= 0corresponds to choosing the initial pre-defined prior mean and covariance µ0andΣ0. This
7Under review as submission to TMLR
combination assumes that data from a single regime arrives in sequential blocksof time of length rt. This
choice of (M.3: prior) was first studied in Adams & MacKay (2007).
RL[1]-OUPR* (greedy runlength with OU and prior reset): This is a new combination we consider in this
paper, which is designed to accommodate both gradual changes and sudden changes. More precisely, we
assumeψt=rt, and we choose the conditional prior as either a hard reset to the prior, if νt(rt)>ε, or a
convex combination of the prior and the previous belief state (using an OUprocess), if νt(rt)≤ε. That is, we
define the conditional prior as
gt(rt,D1:t−1) =/braceleftigg
µ0(1−νt(rt)) +µ(rt)νt(rt)νt(rt)>ε,
µ0 νt(rt)≤ε,(17)
Gt(rt,D1:t−1) =/braceleftigg
Σ0(1−νt(rt)2) +Σ(rt)νt(rt)2νt(rt)>ε,
Σ0 νt(rt)≤ε.(18)
Hereνt(rt) =p(rt|D1:t), withrt=rt−1+ 1, is the probability we are continuing a segment, and νt(rt)with
rt= 0is the probability of a changepoint. For details on how to compute νt(rt), see(35). The value of the
threshold parameter εcontrols whether an abrupt change or a gradual change should take place. In the limit
whenε= 1, this new combination does not learn, since it is always doing a hard reset to the initial beliefs at
timet= 0. Conversely, when ε= 0, we obtain an OU-type update weighted by νt. Whenε= 0.5, we revert
back to prior beliefs when the most likely hypothesis is that a changepoint has just occurred. Finally, we
remark that the above combination allows us to make use of non-Markovian choices for (M.1: likelihood), as
we see in Section 4.3.1. This is, to the best of our knowledge, a new combination that has not been proposed
in the previous literature; for further details see Appendix A.3.
CPL-Sub (changepoint location with subset of data): for ψt=s1:t, this conditional prior constructs the mean
and covariance as
gt(s1:t,D1:t−1) =µ(s1:t−1),
Gt(s1:t,D1:t−1) =Σ(s1:t−1),(19)
whereµ(s1:t−1),Σ(s1:t−1)denotes the posterior belief computed using the observations for entries where s1:t−1
have value of 1. This combination assumes that data from the current regime is scattered from the past
history. That is, it assumes that data from a past regime could become relevant again at a later date. This
combination has been studied in Nguyen et al. (2017).
CPL-MCI (changepoint location with multiplicative covariance matrix): for ψt=s1:t, this choice of conditional
prior maintains the prior mean, but increases the norm of the prior covariance by a constant term β∈(0,1).
More precisely, we have that
gt(s1:t,D1:t−1) =µ(s1:t−1),
Gt(s1:t,D1:t−1) =/braceleftigg
β−1Σ(s1:t−1)st= 1,
Σ(s1:t−1)st= 0.(20)
This combination was first proposed in Li et al. (2021).
CPT-MMPR (changepoint timestep using moment-matched prior reset): for ψt=ζt, withζt={ζ1,t,...,ζℓ,t},
andζℓ,tthe position of the last changepoint, the work of Fearnhead & Liu (2011) assumes a dependence
structure between changepoints. That is, to build the conditional prior mean and covariance, they consider
the pastDζℓ,t:t−1datapoints whenever ζℓ,t≤t−1and a moment-matched approximation to the mixture
density over all possible subset densities since the last changepoint whenever ζℓ,t=t. For an example of MMPR
with RLchoice of (M.2: auxvar), see Appendix A.2.
2.8 Algorithm to compute the posterior over model parameters (A.1)
This section presents algorithms for estimating the density λ(θt;ψt,D1:t); we focus on methods that yield
Gaussian posterior densities. Specifically, we are interested in practical approaches for approximating the
conditional Bayesian posterior, as given in (4).
8Under review as submission to TMLR
There is a vast body of literature on methods for estimating the posterior over model parameters. Here,
we focus on three common approaches for computing the Gaussian posterior: conjugate updates ( Cj),
linear-Gaussian approximations ( LG), and variational Bayes ( VB).
2.8.1 Conjugate updates ( Cj)
Conjugate updates ( Cj) provide a classical approach for computing the posterior by leveraging conjugate prior
distributions. Conjugate updates occur when the functional form of the conditional prior τ(θt;ψt,D1:t−1)
matches that of the measurement model p(yt|θ,xt)(Robert et al., 2007, Section 3.3). This property allows
the posterior to remain within the same family as the prior, which leads to analytically tractable updates and
facilitates efficient recursive estimation.
A common example is the conjugate Gaussian model, where the measurement model is Gaussian with known
variance and the prior is a multivariate Gaussian. This results in closed-form updates for both the mean and
covariance. Another example is the Beta-Bernoulli pair, where the measurement model follows a Bernoulli
distribution with an unknown probability, and the prior is a Beta distribution. See e.g., Bernardo & Smith
(1994); West & Harrison (1997) for details.
The recursive nature of conjugate updates makes them particularly useful for real-time or sequential learning
scenarios, where fast and efficient updates are crucial.
2.8.2 Linear-Gaussian approximation ( LG)
The linear-Gaussian ( LG) method builds on the conjugate updates ( Cj) above. More precisely, the prior
is Gaussian and the measurement model is approximated by a linear Gaussian model, which simplifies
computations.
The prior over model parameters is assumed to be:
τt(θt;ψt,D1:t−1) =N/parenleftig
θt|µ(ψt)
t−1,Σ(ψt)
t−1/parenrightig
, (21)
whereµ(ψt)
t−1andΣ(ψt)
t−1are the mean and covariance, respectively. We use the measurement function hto
define a first-order approximation ¯htaround the prior mean which is given by
¯ht(θt,xt) =h/parenleftig
µ(ψt)
t−1,xt/parenrightig
+Ht/parenleftig
θt−µ(ψt)
t−1/parenrightig
. (22)
Here, Htis the Jacobian of h(θ,xt)with respect to θ, evaluated at µ(ψt)
t−1. The approximate posterior measure
is given by
λ(θt;ψt,D1:t)∝N(yt|¯ht(θt,xt),Rt)τt(θt;ψt,D1:t−1)
=N(yt|¯ht(θt,xt),Rt)N/parenleftig
θt|µ(ψt)
t−1,Σ(ψt)
t−1/parenrightig
∝N(θt|µ(ψt)
t,Σ(ψt)
t),(23)
where Rtis a known noise covariance matrix of the measurement yt. Under the LGalgorithmic choice, the
updated equations are
ˆy(ψt)
t=h/parenleftig
µ(ψt)
t−1,xt/parenrightig
,
S(ψt)
t=HtΣ(ψt)
t−1H⊺
t+Rt,
K(ψt)
t=Σ(ψt)
t−1H⊺
t/parenleftig
S(ψt)
t/parenrightig−1
,
µ(ψt)
t=µ(ψt)
t−1+K(ψt)
t/parenleftig
yt−ˆy(ψt)
t/parenrightig
,
Σ(ψt)
t=Σ(ψt)
t−1−/parenleftig
K(ψt)
t/parenrightig/parenleftig
S(ψt)
t/parenrightig/parenleftig
K(ψt)
t/parenrightig⊺
.(24)
This linear approximation enables efficient computation of the posterior in a Gaussian form. Examples include
the extended Kalman filter (EKF) (Haykin, 2004), which applies local linearisation to non-linear systems,
9Under review as submission to TMLR
the exponential family EKF (Ollivier, 2018), which approximates the measurement model as Gaussian by
matching the first two moments, and the low-rank Kalman filter (LoFi) method (Chang et al., 2023), which
assumes a diagonal-plus-low-rank (DLR) posterior precision matrix. See Särkkä & Svensson (2023) for more
details on such Gaussian filtering methods.
2.8.3 Variational Bayes ( VB)
Variational Bayes (VB) is a popular method for approximating a posterior distribution of model parameters
by choosing a parametric family (such as Gaussians) that is computationally tractable. The primary objective
of VB is to minimise the Kullback-Leibler (KL) divergence between a candidate Gaussian distribution and
the density λt. It can be shown that we can safely ignore the normalisation constant for λt, which is
often computationally expensive, so we can replace λtwith its unnormalised form. We have the following
optimisation problem for the posterior variational parameters:
(µt,Σt) = arg min
µ,ΣDKL(N(θt|µ,Σ)∥p(yt|θt,xt)τt(θt;ψt)), (25)
whereτt(θt;ψt)is the chosen prior distribution (M.3: prior).
An example of VB for neural network models is the Bayes-by-backpropagation method (BBB) of Blundell
et al. (2015), which assumes a diagonal posterior covariance (more expressive forms are also possible). Nguyen
et al. (2017) extended BBB to non-stationary settings. More recent approaches involve recursive estimation,
such as the recursive variational Gaussian approximation (R-VGA) method of Lambert et al. (2022) which
uses a full rank Gaussian variational approximation; the low-rank RVGA (L-RVGA) method of Lambert
et al. (2023), which uses a diagonal plus low-rank (DLR) Gaussian variational approximation; the Bayesian
online natural gradien (BONG) method of Jones et al. (2024), which combines the DLR approximation
with EKF-style linearisation for additional speedups; the natural gradient Gaussian approximation (NANO)
method of Cao et al. (2024), which uses a diagonal Gaussian approximation similar to VD-EKF in Chang
et al. (2022); and the projection-based unification of last-layer and subspace estimation (PULSE) method of
Cartea et al. (2023b), which targets different posterior densities for a subspace of the hidden layers and a
full-rank covariance over the final layer of a neural network.
2.8.4 Alternative methods
Alternative approaches for handling nonlinear or nonconjugate measurements have been proposed, such as
sequential Monte Carlo (SMC) methods (de Freitas et al., 2000), and ensemble Kalman filters (EnKF) (Roth
et al., 2017). These sample-based methods are particularly advantageous when the dimensionality of θis
large, or when a more exact posterior approximation is required, providing greater flexibility in non-linear
and non-Gaussian environments.
Generalised Bayesian methods, such as Mishkin et al. (2018); Knoblauch et al. (2022), generalise the VB
update of (25)by allowing the right-hand side to be a loss function. Alternatively, online gradient descent
methods like Bencomo et al. (2023) emulate state-space modelling via gradient-based optimisation.
2.9 Weighting function for auxiliary variable (A.2)
The termνt(ψt)defines the weights over possible values of the auxiliary variable (M.2: auxvar). We compute
it as the marginal posterior distribution νt(ψt) =p(ψt|D1:t)(see e.g., Adams & MacKay (2007); Fearnhead
& Liu (2007; 2011); Li et al. (2021)) or with ad-hocrules (see e.g., Nassar et al. (2022); Abélès et al. (2024);
Titsias et al. (2024)). In the former case, the weighting function takes the form
νt(ψt) =p(ψt|D1:t)
=p(yt|xt,ψt,D1:t−1)/integraldisplay
ψt−1∈Ψt−1p(ψt−1|D1:t−1)p(ψt|ψt−1,D1:t−1)dψt−1,(26)
10Under review as submission to TMLR
where one assumes that ytis conditionally independent of ψt−1, givenψt, andxtis an exogenous vector.
The first term on the right hand side of (26)is known as the conditional posterior predictive, and is given by
p(yt|xt,ψt,D1:t−1) =/integraldisplay
p(yt|θt,xt)τ(θt;ψt,D1:t−1)dθt. (27)
This integral over θtmay require approximations, as we discussed in Section 2.8. Furthermore, the integral
overψt−1in(26)may also require approximations, depending on the nature of the auxiliary variable ψt, and
the modelling assumptions for p(ψt|ψt−1,D1:t−1). We provide some examples below.
2.9.1 Discrete auxiliary variable (DA)
Here we assume the auxiliary variable takes values in a discrete space ψt∈Ψt. The weights for the discrete
auxiliary variable ( DA) can be computed with a fixed number of hypotheses K≥1or with a growing number
of hypotheses if the cardinality of Ψtincreases through time; we denote these cases by DA[K]andDA[inf]
respectively. Below, we provide three examples that estimate the weights under DA[inf] recursively.
RL(runlength with Markovian assumption): for ψt=rt, the work of Adams & MacKay (2007) takes
p(rt|rt−1,D1:t−1) =

1−H(rt−1)ifrt=rt−1+ 1,
H(rt−1)ifrt= 0,
0 otherwise,(28)
whereH:N0→(0,1)is the hazard function. A popular choice is to take H(r) =π∈(0,1)to be a fixed
constant hyperparameter known as the hazard rate. The choice RL[inf]-PR is known as the Bayesian online
changepoint detection model (BOCD).
CPL(changepoint location): for ψt=s1:t, the work of Li et al. (2021) takes
p(˜s1:t|s1:t−1,D1:t−1) =

πif([˜s1:t\˜st] =s1:t−1)and˜st= 1,
1−πif([˜s1:t\˜st] =s1:t−1)and˜st= 0,
0otherwise,(29)
i.e., the sequence of changepoints at time tcorrespond to the sequence of changepoints up to time t−1, plus
a newly sampled value for t. See Appendix A.4 for details on how to compute νt(s1:t).
CPT(changepoint timestep with Markovian assumption): for ψt=ζt, the work of Fearnhead & Liu (2007)
takes
p(ζt|ζt−1,D1:t−1) =p(ζℓ,t|ζℓ,t−1) =J(ζℓ,t−ζℓ,t−1), (30)
withJ:N0→(0,1)a probability mass function. Note that ζℓ,t−ζℓ,t−1is the distance between two
changepoints, i.e., a runlength. In this sense, ζℓ,t−ζℓ,t−1=rt, which relates CPTtoRL. See their paper for
details on how to compute νt(ζt).
Low-memory variants — from DA[inf]toDA[K]In the examples above, the number of computations
to obtain/summationtext
ψtνt(ψt)grows in time. To fix the computational cost, one can restrict the sum to be over a
subsetAtof the space of ψtwith cardinality|At|=K≥1. Each element in the set Atis called a hypothesis
and givenK≥1, we keep the Kmost likely elements —according to νt(ψt)— inAt. We then define the
normalised weighting function
ˆνt(ψt) =νt(ψt)/summationtext
ψ′
t∈Atνt(ψ′
t), (31)
which we use instead of νt(ψt). For example, in RLabove,At−1={r(k)
t−1:k= 1,...,K}are the unique K
most likely runlengths where the superscript represents the ranking according to νt−1(·). Then, at time t, the
augmented set ¯Atbecomes (At−1+ 1)∪{0}, where the sum is element-wise, and we then compute the Kmost
likely elements of ¯Atto defineAt. InCPL,At−1contains the Kmost likely sequences of changepoints, ¯Atis
defined as the collection of the 2Ksequences where each sequence of At−1has a zero or one concatenated at
11Under review as submission to TMLR
the end. Finally, the Kmost likely elements in ¯AtdefineAt. This style of pruning is common in segmentation
methods; see, e.g., Saatçi et al. (2010). However, other styles of pruning are also possible; see e.g., Li et al.
(2021).
Other choices for DA[K]Finally, some choices of weighting functions are derived using ad-hoc rules,
meaning that explicit or approximate solutions to the Bayesian posterior are not needed. One of the most
popular choices of ad-hoc weighting functions are mixture of experts, which weight different models according
to a given criterion.
ME(mixture of experts with algorithmic weighting): Consider ψt=αt. Letαt,k=kdenote the k-th
configuration over (M.3: prior). Next, denote by wt={wt,1,...,wt,K}a set of weights, where wt,k
corresponds to the weight for the k-th expert at time t. The work of Chaer et al. (1997) considers the
weighting function
νt(wt)k=exp(w⊺
t,kyt)
/summationtextK
j=1exp(w⊺
t,jyt), (32)
fork= 1,...,K. The set of weights wtare determined by maximising the surrogate gain function
Gt(wt) =p(yt|xt,D1:t−1) =K/summationdisplay
k=1p(yt|xt,αt,k,D1:t−1)νt(wt)k, (33)
with respect to wt,kfor allk= 1,...,Kat every timestep t.
We write DA[K], where Kis the number hypothesis, for methods that use Khypotheses at most. On the other
hand, we write DA[inf] when we do not impose a bound on the number of hypotheses used. Note that even
when the choice of (A.2: weighting) is built using DA[inf], one can modify it to make it DA[K].
Discrete auxiliary variable with greedy hypothesis selection ( DA[1])A special case of the above is
DA[1], where we employ a single hypothesis. In these scenarios, we set ν(ψt) = 1whereψtis the most likely
hypothesis.
RL(Greedy runlength): For ψt=rtandDA[1], we take
p(rt|rt−1,D1:t−1) =

1−πifrt=rt−1+ 1,
πifrt= 0,
0otherwise.(34)
Our choice of (A.2: weighting) is based on the marginal predictive likelihood ratio, which is derived from the
computation of p(rt|D1:t)under either either an increase in the runlength ( r(1)
t=rt−1+ 1)or a reset of the
runlength ( r(0)
t= 0). Under these assumptions, the form of νt(r(1)
t)is
νt(r(1)
t) =p(yt|r(1)
t,xt,D1:t−1) (1−π)
p(yt|r(0)
t,xt,D1:t−1)π+p(yt|r(1)
t,xt,D1:t−1) (1−π). (35)
For details on the computation of (A.2: weighting), see Appendix A.3. For a detailed implementation of
(M.2: auxvar) RL, (M.3: prior) OUPR, (A.2: weighting) DA[1], and (A.1: posterior) LG, see Algorithm 4 in the
Appendix.
For example, RL[1]is a runlength rtwith a single hypothesis. We provide another example next.
CPL(changepoint location with retrospective membership): for ψt=s1:t, the work of Nassar et al. (2022)
evaluates the probability of past datapoints belonging in the current regime. In this scenario,
p(s1:t|s1:t−1,D1:t−1) =p(s1:t|D1:t−1), (36)
so that
p(s1:t|D1:t)∝p(s1:t|D1:t−1)p(yt|xt,s1:t,D1:t−1). (37)
12Under review as submission to TMLR
This method allows for exact computation by summing over all possible 2telements. However, to reduce
the computational cost, they propose a discrete optimisation over possible values {νt(s1:t) :s1:t∈{0,1}t},
whereνt(s1:t) =p(s1:t|D1:t). Then, the hypothesis with highest probability is stored and gets assigned a
weight of one.
2.9.2 Continuous auxiliary variable ( CA)
Here, we briefly discuss continuous auxiliary variables (CA). For some choices of ψtand transition densities
p(ψt|ψt−1,D1:t−1), computation of (26)becomes infeasible. In these scenarios, we use simpler approximations.
We give an example below.
CPP(Changepoint probability with empirical Bayes estimate): for ψt=υt, consider
p(υt|υt−1,D1:t−1) =p(υt), (38)
so that
p(υt|D1:t)∝p(υt)p(yt|xt,υt). (39)
The work of Titsias et al. (2024) takes νt(υt) =δ(υt−υ∗
t), whereδis the Dirac delta function and υ∗
tis a
point estimate centred at the maximum of the marginal posterior predictive likelihood:
υ∗
t= arg max
υ∈[0,1]p(yt|xt,υ,D1:t−1). (40)
In practice, (40)is approximated by taking gradient steps towards the minimum. This is a form of empirical
Bayes approximation, since we compute the most likely value of the prior after marginalizing out θt. The
work of Galashov et al. (2024) considers a modified configuration with choice of (M.2: auxvar) υt∈(0,1)m.
3 Unified view of examples in the literature
Table 2 shows that many existing methods can be written as instances of BONE. Rather than specifying the
choice of (M.1), we instead write the task for which it was designed, as discussed in Section 2.3. We will
experimentally compare a subset of these methods in Section 4.
The methods presented in Table 2 can be directly applied to tackle any of the the problems mention in
Section 2.3. However, as choice of (M.1: likelihood), we specify the task under which the configuration was
introduced.
4 Experiments
In this section we experimentally evaluate different algorithms within the BONE framework on a number of
tasks.
Each experiment consists of a warmup period where the hyperparameters are chosen, and a deployperiod
where sequential predictions and updates are performed. In each experiment, we fix the choice of measurement
modelh(M.1: likelihood) and posterior inference method (A.1: posterior), and then compare different
methods with respect to their choice of (M.2: auxvar), (M.3: prior), and (A.2: weighting). For DAmethods,
we append the number of hypotheses in brackets to determine how many hypotheses are being considered.
For example, RL[1]-PR denotes one hypothesis, RL[K]-PR denotesKhypotheses, and RL[inf]-PR denotes
all possible hypotheses. In all experiments, unless otherwise stated, we consider a single hypothesis for choices
ofDA. See Table 3 for the methods we compare.
4.1 Prequential prediction
In this section, we give several examples of non-stationary prequential prediction problems.
13Under review as submission to TMLR
reference Task M.2: auxvar M.3: prior A.1: posterior A.2: weight
Kalman (1960) filtering C LSSM LG DA[1]
Magill (1965) filtering ME LSSM LG DA[K]
Chang & Athans (1978) filtering ME LSSM LG CA
Chaer et al. (1997) filtering ME LSSM LG DA[K]
Ghahramani & Hinton (2000) SSSM ME Static VB CA
Adams & MacKay (2007) seg. RL PR Cj DA[inf]
Fearnhead & Liu (2007) seg. & preq. CPT/ME PR Any DA[inf]
Wilson et al. (2010) seg. RLCC PR Cj DA[inf]
Fearnhead & Liu (2011) seg. CPT/ME MMPR Any DA[inf]
Mellor & Shapiro (2013) bandits RL PR Cj DA[inf]
Nguyen et al. (2017) OCL CPL Sub VB DA[1]
Knoblauch & Damoulas (2018) seg. RL/ME PR Cj DA[inf]
Kurle et al. (2019) OCL CPV Sub VB DA[1]
Li et al. (2021) OCL CPL MCI VB DA[inf]
Nassar et al. (2022) bandits & OCL CPV Sub LG DA[1]
Liu (2023) preq. ME C ,LSSM Any DA[K]
Titsias et al. (2024) OCL CPP OU LG CA
Abélès et al. (2024) preq. ME LSSM LG DA[K]
RL[1]-OUPR* (ours) any RL SPR Any DA[1]
Table 2: List of methods ordered by publication date. The tasks are discussed in Section 2.3. We use the
following abbreviations: SSSM means switching state space model; OCL means online continual learning;
seg. means segmentation; preq. means prequential. Methods that consider two choices of (M.2: auxvar) are
denoted by ‘ X/Y’. This corresponds to a double expectation in (6)—one for each choice of auxiliary variable.
4.1.1 Online regression for hour-ahead electricity forecasting
In this experiment, we consider the task of predicting the hour-ahead electricity load before and after the
Covid pandemic. We use the dataset presented in Farrokhabadi et al. (2022), which has 31,912 observations;
each observation contains 7 features xtand a single target variable yt. The 7 features correspond to pressure
(kPa), cloud cover (%), humidity (%), temperature (C) , wind direction (deg), and wind speed (KmH). The
target variable is the hour-ahead electricity load (kW). To preprocess the data, we normalise the target
variableytby subtracting an exponentially weighted moving average (EWMA) mean with a half-life of
20 hours, then dividing the resulting series by an EWMA standard deviation with the same half-life. To
normalise the features xt, we divide each by a 20-hour half-life EWMA.
Our choice of measurement model his a two-hidden layer multilayered perceptron (MLP) with four units per
layer and a ReLU activation function.
14Under review as submission to TMLR
M.2-M.3 Eq. A.2 Description Sections
static
C-Static (14) -This corresponds to the static case with a classical Bayesian update.
This method does not assume changes in the environment.4.3.1, 4.3.2
abrupt changes
RL-PR (16) DA[inf] This approach, commonly referred to as Bayesian online change-
point detection ( BOCD), assumes that non-stationarity arises
from independent blocks of time, each with stationary data. Esti-
mates are made using data from the current block. See Appendix
A.1 for more details.4.1.1, 4.1.2,
4.2, 4.3.1,
4.3.2,
WoLF+RL-PR* (16) DA[inf] Special case of RL-PRwith explicit choice of (A.1: posterior) which
makes it robust to outliers.4.3.2
gradual changes
CPP-OU (12) CA Updates are done using a discounted mean and covariance accord-
ing to the probability estimate that a change has occurred.4.1.1, 4.1.2,
4.2
C-ACI (13) -At each timestep, this method assumes that the parameters evolve
according to a linear map Ft, at a rate given by a known positive
semidefinite covariance matrix Qt.4.1.1, 4.1.2,
4.2,
abrupt & gradual changes
RL-MMPR (55) DA[inf] Modificationof CPT-MMPR thatassumesdependencebetweenanytwo
consecutive blocks of time and with choice of RL. This combination
employs a moment-matching approach when evaluating the prior
mean and covariance under a changepoint. See Appendix A.2 for
more details.4.3.1
RL-OUPR (17) DA[1] Depending on the threshold parameter, updates involve either (i) a
convex combination of the prior belief with the previous mean and
covariance based on the estimated probability of a change (given
the run length), or (ii) a hard reset of the mean and covariance,
reverting them to prior beliefs. See Appendix A.3 for more details.4.1.1, 4.1.2,
4.2, 4.3.1
Table 3: List of methods we compare in our experiments. The first column, M.2–M.3 , is defined by
the choices of (M.2: auxvar) and (M.3: prior). The second column, Eq., references the equation that
define M.2–M.3. The third column, A.2, determines the choice of (A.2: weighting). The fourth column,
Description , provides a brief summary of the method. The fifth column, Sections , shows the sections where
the method is evaluated. The choice of (M.1: likelihood) and (A.1: posterior) are defined on a per-experiment
basis. (The only exception being WolF+RL-PR ). For (M.2: auxvar) the acronyms are as follows: RLmeans
runlength, CPPmeans changepoint probability, Cmeans constant, and CPTmeans changepoint timestep.
For (M.3: prior) the acronyms are as follows: PRmeans prior reset, OUmeans Ornstein–Uhlenbeck, LSSM
means linear state-space model, Staticmeans full Bayesian update, MMRmeans moment-matched prior reset,
and OUPRmeans Ornstein–Uhlenbeck and prior reset. We use the convention in Hušková (1999) for the
terminology abrupt/gradual changes.
For this experiment, we consider RL[1]-OUPR* ,RL[1]-PR ,C-ACI, and CPP-OU. For computational convenience,
we plug in a point-estimate (MAP estimate) of the neural network parameters when making predictions
in(6). For a fully Bayesian treatment of neural network predictions, see Immer et al. (2021); we leave the
implementation of these approaches for future work.
The hyperparameters of each method are found using the first 300 observations (around 13 days) and deployed
on the remainder of the dataset. Specifically, during the warmup period we tune the value of the probability
of a changepoint for RL[1]-OUPR* andRL[1]-PR . For C-ACIwe tune Qt, and for CPP-OUwe tune the learning
rate. See the open-source notebooks for more details.
In the top panel of Figure 1 we show the evolution of the target variable ytbetween March 3 2020 and
March 10 2020. The bottom panel of Figure 1 shows the 12-hour rolling mean absolute error (MAE) of
predictions made by the methods. We see that there is a changepoint around March 7 2020 as pointed out
in Farrokhabadi et al. (2022). This is likely due to the introduction of Covid lockdown rules. Among the
methods considered, C-ACIandRL[1]-OUPR* adapt the quickest after the changepoint and maintain a low
rolling MAE compared to RL[1]-PR andCPP-OU.
15Under review as submission to TMLR
Figure 1: The top panel shows the target variable (electricity consumption) from March 1 2020 to March
12 2020. The bottom panel shows the twelve-hour rolling relative absolute error of predictions for the same
time window. The dotted black line corresponds to March 7 2020, when Covid lockdown began.
Next, Figure 2, shows the forecasts made by each method between March 4 2020 and March March 8 2020.
We observe a clear cyclical pattern before March 7 2020 but less so afterwards, indicating a change in daily
electricity usage from diurnal to constant.
Figure 2: One day ahead electricity forecasting results for Figure 1. The dotted black line corresponds to
March 7 2020.
We also observe that RL[1]-PR and CPP-OUslow-down their rate of adaptation. One possible explanation
of this behaviour is that the changes are not abrupt enough to be captured by the algorithms. To provide
evidence for this hypothesis, Figure 3 shows, on the left y-axis, the predictions for RL[1]-PR and the target
variableyt. On the right y-axis, we show the estimated runlength.
We see that RL[1]-PR resets approximately twice every day until the time of the changepoint. After that,
there is no evidence of a changepoint (as provided by the hyperparameters and the modelling choices), so
RL[1]-PR does not reset which translates to less adaptation for the period to the right of the changepoint.
Finally, we compare the error of predictions made by the competing methods. This is quantified in Figure 4,
which shows a box-plot of the five-day MAE for each of the competing methods over the whole dataset, from
March 2017 to November 2020. Our new RL[1]-OUPR* method has the lowest MAE.
4.1.2 Online classification with periodic drift
In this section we study the performance of C-ACI,CPP-OU,RL[1]-PR , and RL[1]-OUPR* for the classification
experiment of Section 6.2 in Kurle et al. (2019). More precisely, in this experiment xt,i∼Unif[−3,3]for
i∈{1,2},xt= (xt,1,xt,2)∈R2,yt∼Bernoulli (σ(θ⊺
txt))withθ(1)
t= 10 sin(5◦t)andθ(2)
t= 10 cos(5◦t).
16Under review as submission to TMLR
Figure 3: One day ahead electricity forecasting results for RL[1]-PR together with the target variable on the
left y-axis, and the value for runlength ( RL) on the right y-axis. We see that after the 7 March changepoint,
the runlength monotonically increases, indicating a stationary regime.
Figure 4: Distribution of the 5-day mean absolute error (MAE) for each of the competing methods on
electricity forecasting over the entire period. For this calculation we split the dataset into consecutive buckets
containing five days of data each, and for a given bucket we compute the average absolute error of the
predictions and observations that fall within the bucket.
Thus the unknown values of model parameters are slowly drifting deterministically according to sine and
cosine functions. The timesteps go from 0 to 720.
Figure 5: Misclassification rate of various methods on the online classification with periodic drift task.
Figure 5 summarises the results of the experiment where we show the misclassification rate (which is one
minus the accuracy) for the competing methods. Our RL[1]-OUPR* method works the best, and signifcantly
outperforms RL[1]-PR , since we use an OU drift process with a soft prior reset rather than assuming constant
parameter with a hard prior rset.
17Under review as submission to TMLR
We can improve the performance of RL[K]-PR if the number of hypotheses Kincreases, and if we vary the
changepoint probability threshold π, as shown in Figure 6. However, even then the performance of this
method still does not match our method.
Figure 6: Accuracy of predictions for RL[1]-PR as a function of the number of hypothesis and the prior
probability of a changepoint π. The black dotted line is the performance of RL[1]-OUPR* reported in Figure
5.
4.1.3 Online classification with drift and jumps
In this section we study the performance of C-ACI,CPP-OU,RL[1]-PR , and RL[1]-OUPR* for an experiment
with drift and sudden changes. More precisely, we assume that the parameters of a logistic regression problem
evolve according to
θt=/braceleftigg
θt−1+ϵtw.p. 1−pϵ,
U[−2,2]2w.p.pϵ,(41)
withpϵ= 0.01,θ0∼U[−2,2]2, andϵtis a zero-mean distributed random vector with isotropic covariance
matrix (0.01)2I2(where I2is a2×2identity matrix). Intuitively, this experiment has model parameters
that drift slowly with occasional abrupt changes (at a rate of 0.01).
Figure 7: Misclassification rate of various methods on the online classification with drift and jumps task.
Figure 7 shows the missclasification rate among the competing methods. We observe that C-ACI,CPP-OU, and
RL[1]-OUPR* have comparable performance, whereas RL[1]-PR is the method with highest misclassification
rate.
18Under review as submission to TMLR
Figure 8: Accuracy of predictions for RL[K]-PR as a function of the number of hypotheses ( K) and the
probability of a changepoint π. The black dotted line is the performance of RL[1]-OUPR* reported in Figure
7.
To explain this behaviour, Figure 8 shows the performance of RL[K]-PR as a function of number of hypotheses
and prior probability of a changepoint π. We observe that up to three hypotheses, the lowest misclassification
error of RL[K]-PR is higher than that of RL[1]-OUPR* , which only considers one hypothesis. However, as we
increase the number of hypotheses, the best performance for RL[K]-PR obtains a lower misclassification rate
than RL[1]-OUPR* . This is in contrast to the results in Figure 5. Here, we see that with more hypotheses
RL[K]-PR outperforms our new method at the expense of being more memory intensive.
4.2 Contextual bandits
In this section, we study the performance of C-ACI,CPP-OU,RL[1]-PR , and RL[1]-OUPR* for the simple
Bernoulli bandit from Section 7.3 of Mellor & Shapiro (2013). More precisely, we consider a multi-armed
bandit problem with 10 arms, 10,000 steps per simulation, and 100 simulations. The payoff of a given arm is
the outcome of a Bernoulli random variable with unknown probability θt=min{max{θt−1+ 0.03Zt,0},1}
for{Zt}t∈{1,2,...,10,000}independent and identically distributed standard normal random variables. We take
θ0∼Unif[0,1]and use the same formulation for all ten arms with independence across arms. The observations
are the rewards and there are no features (non-contextual).
The idea of using RL[1]-PR in multi-armed bandits problems was introduced in Mellor & Shapiro (2013).
With this experiment, we extend the concept to other members of the BONE framework. We use Thompson
sampling for each of the competing methods. Figure 9 shows the regret of using C-ACI,CPP-OU,RL[1]-PR ,
andRL[1]-OUPR* for the above multi-armed bandits problem. The results we obtain are similar to those of
Section 4.1.2. This is because both problems have a similar drift structure.
Figure 9: Regret of competing methods on the contextual bandits task. Confidence bands are computed
with one hundred simulations.
19Under review as submission to TMLR
4.3 Segmentation and prediction
In this section, we evaluate methods both in terms of their ability to “correctly” segment the observed output
signal, and to do one-step-ahead predictions. Note that by “correct segmentation”, we mean one that matches
the ground truth data generating process. This metric can only be applied to synthetic data.
4.3.1 Autoregression with dependence across the segments
In this experiment, we consider the synthetic autoregressive dataset introduced in Section 2 of Fearnhead &
Liu (2011), consisting of a set of one dimensional polynomial curves that are constrained to match up at
segmentation boundaries, as shown in the top left of Figure 10.
We compare the performance of the three methods in the previous subsection. For this experiment, we
employ a probability of a changepoint π= 0.01. Since this dataset has dependence of the parameters across
segments, we allow for the choice of (M.1: likelihood) to be influenced by the choice of (M.2: auxvar), i.e.,
the conditional expectation (5)can be written as h(θt;ψt,xt). For this experiment, we take (M.2: auxvar) to
beRLand our choice of (M.1: likelihood) becomes
h(θt;rt,x1:t) =θ⊺
th(x1:t,rt), (42)
withh(x1:t,rt) = [1,∆,∆2],∆ = (xt−xrt), andxrt≥xt. Intuitively this represents a quadratic curve fit
to the beginning xrtand end points xtof the current segment. Given the form of (M.1: likelihood) in (42),
here we do not consider C-ACInorCPP-OU. Instead, we use runlength with moment-matching prior reset, i.e.,
RL-MMPR (see Table 3) which was designed for segmentation with dependence.
Figure 10: The left panel shows a sample run of the piecewise polynomial regression with dependence across
segments. The x-axis is for the features, the (left) y-axis is for measurements together with the estimations
made by RL[1]-PR ,RL-MMPR, and RL[1]-OUPR* , the (right) y-axis is for the value of rtunder each model.
The orange line denotes the true data-generating process and the red line denotes the value of the hypothesis
RL. Theright panel shows the RMSE of predictions over 100 trials.
Figure 10 shows the results. On the right, we observe that RL[1]-OUPR* has the lowest RMSE. On the
left, we plot the predictions of each method, so we can visualise the nature of their errors. For RL[1]-PR ,
the spikes occur because the method has many false positive beliefs in a changepoint occurring, and this
causes breaks in the predictions due the explicit dependence of honrtand the hard parameter reset upon
changepoints. For RL-MMPR, the slow adaptation (especially when xt∈[1,5]) is because the method does not
adjust beliefs as quickly as it should. Our RL[1]-OUPR* method strikes a good compromise.
20Under review as submission to TMLR
Figure 11: Count of changepoints over an experiment for 100 trials. The orange line shows the true number
of changepoints for all trials.
Figure 11 shows the distribution (over 100 simulations) of the number of detected changepoints, i.e., instances
whereνt(rt)withrt= 0is the highest. We observe that superior predictive performance in Figure 10 does
not necessarily translate to a better segmentation capability. For example, the distribution produced by
RL-MMPR sits around the actual number of changepoints (better at segmenting) whereas RL[1]-OUPR* , which
is detecting far fewer changepoints, is the best performing prediction method. This reflects the discrepancy
between the objectives of segmentation and prediction. For a more thorough analysis and evaluation of
changepoint detection methods on time-series data, see Van den Burg & Williams (2020).
4.3.2 Non-stationary heavy-tailed regression with DA[inf]
It is well-known that the combination RL-PRis sensitive to outliers if the choice of (M.1: likelihood) is
misspecified, since an observation that is “unusual” may trigger a changepoint unnecessarily. As a consequence,
various works have proposed outlier-robust variants to the RL[inf]-PR for segmentation (Knoblauch et al.,
2018; Fearnhead & Rigaill, 2019; Altamirano et al., 2023; Sellier & Dellaportas, 2023) and for filtering
(Reimann, 2024). In what follows, we show how we can easily accomodate robust methods into the BONE
framework by changing the way we compute the likelihood and/or posterior. In particular, we consider the
WoLF-IMQ method of Duran-Martin et al. (2024).3We use WoLF-IMQ because it is a provably robust
algorithm and it is a straightforward modification of the linear Gaussian posterior update equations. We
denote RL[inf]-PR with (A.1: posterior) taken to be LGasLG+RL[inf]-PR and RL[inf]-PR with (A.1:
posterior) taken to be WoLF-IMQ as WoLF+RL[inf]-PR* .
To demonstrate the utility of a robust method, we consider a piecewise linear regression model with Student- t
errors, where the measurement are sampled according to xt∼U[−2,2],yt∼St/parenleftbig
ϕ(xt)⊺θt,1,2.01/parenrightbig
a Student-
tdistribution with location ϕ(xt)⊺θt, scale 1, degrees of freedom 2.01, andϕ(xt) = (1, x, x2). At every
timestep, the parameters take the value
θt=/braceleftigg
θt−1w.p. 1−pϵ,
U[−3,3]3w.p.pϵ,(43)
withpϵ= 0.01, andθ0∼U[−3,3]3. Intuitively, at each timestep, there is probability pϵof a changepoint,
and conditional on a changepoint occurring, the each of the entries of the new parameters θtare sampled
from a uniform in [−3,3]. Figure 12 shows a sample data generated by this process.
To process this data, our choice of (M.1: likelihood) is h(θt,xt) =θ⊺
txt.
The left panel in Figure 13 shows the rolling mean (with a window of size 10) of the RMSE for LG+RL[inf]-PR ,
WoLF+RL[inf]-PR* , and LG+C-Static . The right panel in Figure 13 shows the distribution of the RMSE for
all methods after 30 trials.
The left panel of Figure 13 shows that LG+C-Static has a lower rolling RMSE error than LG+RL[inf]-PR
up to first changepoint (around 100 steps). The performance of LG+C-Static significantly deteriorates
afterwards. Next, LG+RL[inf]-PR wrongly detects changepoints and resets its parameters frequently. This
3We set the soft threshold value to 4, representing four standard deviations of tolerance before declaring an outlier.
21Under review as submission to TMLR
Figure 12: Sample run of the heavy-tailed-regression process. Each box corresponds to the samples within a
segment.
Figure 13: The left panel shows the rolling RMSE using a window of the 10 previous observations. The
right panel shows the distribution of final RMSE over 30 runs. The vertical dotted line denotes a change in
the true model parameters.
results in periods of increased rolling RMSE. Finally, WoLF+RL[inf]-PR* has the lowest error among the
methods. After the regime change, its error increases at a similar rate to the other methods, however, it
correctly adapts to the regime and its error decreases soon after the changepoint.
Figure 14: Segmentation of the non-stationary linear regression problem. The left panel shows the
segmentation done by LG+RL[inf]-PR . The right panel shows the segmentation done by WoLF+RL[inf]-PR* .
Thex-axis is the timestep t, they-axis is the runlength rt(note that it is always the case that rt≤t), and the
color bar shows the value logp(rt|y1:t). The red line in either plot is the trajectory of the mode, i.e., the set
r∗
1:t={arg maxr1p(r1|D1),..., arg maxrtp(rt|D1:t)}. Note that the non-robust method (left) oversegments
the signal.
Figure14showstheposteriorbeliefofthevalueoftherunlengthusing LG+RL[inf]-PR andWoLF+RL[inf]-PR* .
The constant reaction to outliers in the case of LG+RL[inf]-PR means that the parameters keep reseting back
to the initial prior belief. As a consequence, the RMSE of LG+RL[inf]-PR deteriorates. On the other hand,
WoLF+RL[inf]-PR* resets less often, and accurately adjusts to the regime changes when they do happen.
This results in the lowest RMSE among the three methods.
22Under review as submission to TMLR
5 Conclusions
We introduced a unified Bayesian framework to perform online predictions in non-stationary environments, and
showed how it covers many prior works. We also used our framework to design a new method, RL[1]-OUPR* ,
which is suited to tackle prediction problems when the observations exhibit both abrupt and gradual changes.
We hope to explore other novel variants and applications in future work.
References
Baptiste Abélès, Joseph de Vilmarest, and Olivier Wintemberger. Adaptive time series forecasting with
markovian variance switching, 2024.
Ryan Prescott Adams and David J. C. MacKay. Bayesian online changepoint detection, 2007.
Diego Agudelo-España, Sebastian Gomez-Gonzalez, Stefan Bauer, Bernhard Schölkopf, and Jan Peters.
Bayesian online prediction of change points. In Conference on Uncertainty in Artificial Intelligence , pp.
320–329. PMLR, 2020.
Reda Alami. Bayesian change-point detection for bandit feedback in non-stationary environments. In Asian
Conference on Machine Learning , pp. 17–31. PMLR, 2023.
Réda Alami, Odalric Maillard, and Raphael Féraud. Restarted bayesian online change-point detector achieves
optimal detection delay. In International conference on machine learning , pp. 211–221. PMLR, 2020.
Matias Altamirano, François-Xavier Briol, and Jeremias Knoblauch. Robust and scalable bayesian online
changepoint detection, 2023.
Samaneh Aminikhanghahi and Diane J Cook. A survey of methods for time series change point detection.
Knowledge and information systems , 51(2):339–367, 2017.
Jordan T Ash and Ryan P Adams. On warm-starting neural network training. In NIPS, 2020. URL
http://arxiv.org/abs/1910.08475 .
Daniel Barry and John A Hartigan. Product partition models for change point problems. The Annals of
Statistics , pp. 260–279, 1992.
Michele Basseville, Igor V Nikiforov, et al. Detection of abrupt changes: theory and application , volume 104.
Prentice hall Englewood Cliffs, 1993.
Richard H Battin. Space guidance evolution-a personal narrative. Journal of Guidance, Control, and
Dynamics , 5(2):97–110, 1982.
Matthew Beal, Zoubin Ghahramani, and Carl Rasmussen. The infinite hidden markov model. Advances in
neural information processing systems , 14, 2001.
Gianluca M. Bencomo, Jake C. Snell, and Thomas L. Griffiths. Implicit maximum a posteriori filtering via
adaptive optimization, 2023.
J. Bernardo and A. Smith. Bayesian Theory . John Wiley, 1994.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural
network. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on
Machine Learning , volume 37 of Proceedings of Machine Learning Research , pp. 1613–1622, Lille, France,
07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/blundell15.html .
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,
George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable
transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax .
23Under review as submission to TMLR
Zhipeng Cai, Ozan Sener, and Vladlen Koltun. Online continual learning with natural distribution shifts: An
empirical study with visual data. In Proceedings of the IEEE/CVF international conference on computer
vision, pp. 8281–8290, 2021.
Wenhan Cao, Tianyi Zhang, Zeju Sun, Chang Liu, Stephen S-T Yau, and Shengbo Eben Li. Nonlinear
bayesian filtering with natural gradient gaussian approximation. arXiv [eess.SY] , October 2024. URL
http://arxiv.org/abs/2410.15832 .
Álvaro Cartea, Fayçal Drissi, and Pierre Osselin. Bandits for algorithmic trading with signals. Available at
SSRN 4484004 , 2023a.
Álvaro Cartea, Gerardo Duran-Martin, and Leandro Sánchez-Betancourt. Detecting toxic flow, 2023b.
Wassim S Chaer, Robert H Bishop, and Joydeep Ghosh. A mixture-of-experts framework for adaptive kalman
filtering. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) , 27(3):452–464,
1997.
Chaw-Bing Chang and Michael Athans. State estimation for discrete systems with switching parameters.
IEEE Transactions on Aerospace and Electronic Systems , AES-14(3):418–425, 1978.
Peter G Chang, Kevin Patrick Murphy, and Matt Jones. On diagonal approximations to the extended kalman
filter for online training of bayesian neural networks. In Continual Lifelong Learning Workshop at ACML
2022, 2022.
Peter G Chang, Gerardo Duran-Martin, Alex Shestopaloff, Matt Jones, and Kevin Patrick Murphy. Low-rank
extended kalman filtering for online learning of neural networks from streaming data. In Conference on
Lifelong Learning Agents , pp. 1025–1071. PMLR, 2023.
Joao FG de Freitas, Mahesan Niranjan, Andrew H. Gee, and Arnaud Doucet. Sequential monte carlo methods
to train neural network models. Neural computation , 12(4):955–993, 2000.
Shibhansh Dohare, J Fernando Hernandez-Garcia, Qingfeng Lan, Parash Rahman, A Rupam Mahmood, and
Richard S Sutton. Loss of plasticity in deep continual learning. Nature, 632(8026):768–774, 2024.
Gerardo Duran-Martin, Aleyna Kara, and Kevin Murphy. Efficient online bayesian inference for neural
bandits. In International Conference on Artificial Intelligence and Statistics , pp. 6002–6021. PMLR, 2022.
Gerardo Duran-Martin, Matias Altamirano, Alexander Y. Shestpaloff, Leandro Sánchez-Betancourt, Jeremias
Knoblauch, Matt Jones, Briol François-Xavier, and Kevin P. Murphy. Outlier-robust kalman filtering
through generalised bayes. In International Conference on Machine Learning . PMLR, 2024.
Geir Evensen. Sequential data assimilation with a nonlinear quasi-geostrophic model using monte carlo
methods to forecast error statistics. Journal of Geophysical Research: Oceans , 99(C5):10143–10162, 1994.
Arnold M Faden and Gordon C Rausser. Econometric policy model construction: the post-bayesian approach.
InAnnals of Economic and Social Measurement, Volume 5, number 3 , pp. 349–363. NBER, 1976.
Mostafa Farrokhabadi, Jethro Browell, Yi Wang, Stephen Makonin, Wencong Su, and Hamidreza Zareipour.
Day-ahead electricity demand forecasting competition: Post-covid paradigm. IEEE Open Access Journal
of Power and Energy , 9:185–191, 2022. doi: 10.1109/OAJPE.2022.3161101.
Paul Fearnhead and Zhen Liu. On-line inference for multiple changepoint problems. Journal of the Royal
Statistical Society Series B: Statistical Methodology , 69(4):589–605, 2007.
Paul Fearnhead and Zhen Liu. Efficient bayesian analysis of multiple changepoint models with dependence
across segments. Statistics and Computing , 21:217–229, 2011.
Paul Fearnhead and Guillem Rigaill. Changepoint detection in the presence of outliers. Journal of the
American Statistical Association , 114(525):169–183, 2019.
24Under review as submission to TMLR
Emily B Fox, Erik B Sudderth, Michael I Jordan, and Alan S Willsky. The sticky hdp-hmm: Bayesian
nonparametric hidden markov models with persistent states. Arxiv preprint , 2, 2007.
Alexandre Galashov, Michalis K Titsias, András György, Clare Lyle, Razvan Pascanu, Teh Yee Whye, and
Maneesh Sahani. Non-stationary learning of neural networks with automatic soft parameter reset. In NIPS,
November 2024. URL https://arxiv.org/abs/2411.04034 .
Joao Gama, Jesus Aguilar-Ruiz, and Ralf Klinkenberg. Knowledge discovery from data streams. Intelligent
Data Analysis , 12(3):251–252, 2008.
Zoubin Ghahramani and Geoffrey E Hinton. Variational learning for switching state-space models. Neural
computation , 12(4):831–864, 2000.
Muktesh Gupta, Rajesh Wadhvani, and Akhtar Rasool. Comprehensive analysis of change-point dynamics
detection in time series data: A review. Expert Systems with Applications , pp. 123342, 2024.
Simon Haykin. Kalman filtering and neural networks . John Wiley & Sons, 2004.
M Hušková. Gradual changes versus abrupt changes. Journal of Statistical Planning and Inference , 76(1-2):
109–125, 1999.
Alexander Immer, Maciej Korzepa, and Matthias Bauer. Improving predictions of bayesian neural nets via
local linearization. In International conference on artificial intelligence and statistics , pp. 703–711. PMLR,
2021.
Matt Jones, Peter Chang, and Kevin Murphy. Bayesian online natural gradient (BONG). In Advances in
Neural Information Processing Systems , May 2024. URL http://arxiv.org/abs/2405.19681 .
Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. Transactions of the
ASME–Journal of Basic Engineering , 82(Series D):35–45, 1960.
Jeremias Knoblauch and Theodoros Damoulas. Spatio-temporal bayesian on-line changepoint detection with
model selection. In International Conference on Machine Learning , pp. 2718–2727. PMLR, 2018.
Jeremias Knoblauch, Jack E Jewson, and Theodoros Damoulas. Doubly robust bayesian inference for
non-stationary streaming data with β-divergences. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N.Cesa-Bianchi, andR.Garnett(eds.), Advances in Neural Information Processing Systems , volume31.Cur-
ran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/
a3f390d88e4c41f2747bfa2f1b5f87db-Paper.pdf .
Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. An optimization-centric view on bayes’ rule:
Reviewing and generalizing variational inference. Journal of Machine Learning Research , 23(132):1–109,
2022. URL http://jmlr.org/papers/v23/19-1047.html .
Mark R Kuhl. Ridge regression signal processing. NASA, Langley Research Center, Joint University Program
for Air Transportation Research, 1989-1990 , 1990.
Richard Kurle, Botond Cseke, Alexej Klushyn, Patrick Van Der Smagt, and Stephan Günnemann. Continual
learning with bayesian neural networks for non-stationary data. In International Conference on Learning
Representations , 2019.
Marc Lambert, Silvère Bonnabel, and Francis Bach. The recursive variational gaussian approximation (r-vga).
Statistics and Computing , 32(1):10, 2022.
Marc Lambert, Silvère Bonnabel, and Francis Bach. The limited-memory recursive variational gaussian
approximation (l-rvga). Statistics and Computing , 33(3):70, 2023.
Aodong Li, Alex Boyd, Padhraic Smyth, and Stephan Mandt. Detecting and adapting to irregular distribution
shifts in bayesian online learning, 2021.
25Under review as submission to TMLR
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized
news article recommendation. In Proceedings of the 19th international conference on World wide web , pp.
661–670, 2010.
Scott Linderman, Matthew Johnson, Andrew Miller, Ryan Adams, David Blei, and Liam Paninski. Bayesian
learning and inference in recurrent switching linear dynamical systems. In Artificial intelligence and
statistics , pp. 914–922. PMLR, 2017.
Bin Liu. Robust sequential online prediction with dynamic ensemble of multiple models: A review. Neuro-
computing , pp. 126553, 2023.
Yueyang Liu, Benjamin Van Roy, and Kuang Xu. Nonstationary bandit learning via predictive sampling. In
International Conference on Artificial Intelligence and Statistics , pp. 6215–6244. PMLR, 2023.
David Magill. Optimal adaptive estimation of sampled stochastic processes. IEEE Transactions on Automatic
Control, 10(4):434–439, 1965.
Joseph Mellor and Jonathan Shapiro. Thompson sampling in switching environments with bayesian online
change point detection. arXiv preprint arXiv:1302.3721 , 2013.
Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt, and Mohammad Emtiyaz Khan. Slang:
Fast structured covariance approximations for bayesian deep learning with natural gradient. Advances in
neural information processing systems , 31, 2018.
Josue Nassar, Jennifer Brennan, Ben Evans, and Kendall Lowrey. Bam: Bayes with adaptive memory. arXiv
preprint arXiv:2202.02405 , 2022.
Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning. arXiv
preprint arXiv:1710.10628 , 2017.
Yann Ollivier. Online natural gradient as a Kalman filter. Electronic Journal of Statistics , 12(2):2930 – 2961,
2018. doi: 10.1214/18-EJS1468. URL https://doi.org/10.1214/18-EJS1468 .
Mari Ostendorf, Vassilios V Digalakis, and Owen A Kimball. From hmm’s to segment models: A unified view
of stochastic modeling for speech recognition. IEEE Transactions on speech and audio processing , 4(5):
360–378, 1996.
Václav Peterka. Bayesian approach to system identification. In Trends and Progress in System identification ,
pp. 239–304. Elsevier, 1981.
Hans Reimann. Towards robust inference for bayesian filtering of linear gaussian dynamical systems subject
to additive change. masterthesis, Universität Potsdam, 2024.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical
comparison of bayesian deep networks for thompson sampling, 2018.
Christian P Robert et al. The Bayesian choice: from decision-theoretic foundations to computational
implementation , volume 2. Springer, 2007.
Michael Roth, Gustaf Hendeby, Carsten Fritsche, and Fredrik Gustafsson. The ensemble kalman filter:
a signal processing perspective. EURASIP J. Adv. Signal Processing , 2017(1):56, 2017. URL https:
//doi.org/10.1186/s13634-017-0492-x .
Yunus Saatçi, Ryan D Turner, and Carl E Rasmussen. Gaussian process change point models. In Proceedings
of the 27th International Conference on Machine Learning (ICML-10) , pp. 927–934, 2010.
Simo Särkkä and Lennart Svensson. Bayesian filtering and smoothing , volume 17. Cambridge university
press, 2023.
Mona Schirmer, Dan Zhang, and Eric Nalisnick. Test-time adaptation with state-space models. arXiv preprint
arXiv:2407.12492 , 2024.
26Under review as submission to TMLR
Jeremy Sellier and Petros Dellaportas. Bayesian online change point detection with hilbert space approximate
student-t process. In International Conference on Machine Learning , pp. 30553–30569. PMLR, 2023.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika , 25(3-4):285–294, 1933.
Michalis K Titsias, Alexandre Galashov, Amal Rannen-Triki, Razvan Pascanu, Yee Whye Teh, and Jorg
Bornschein. Kalman filter for online classification of non-stationary data. In ICLR, 2024.
Gerrit JJ Van den Burg and Christopher KI Williams. An evaluation of change point detection algorithms.
arXiv preprint arXiv:2003.06222 , 2020.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and Zoubin Ghahramani. Beam sampling for the infinite
hidden markov model. In Proceedings of the 25th international conference on Machine learning , pp.
1088–1095, 2008.
Mike West and Jeff Harrison. Bayesian forecasting and dynamic models . Springer, 1997.
Robert C Wilson, Matthew R Nassar, and Joshua I Gold. Bayesian online learning of the hazard rate in
change-point problems. Neural computation , 22(9):2452–2476, 2010.
A Worked examples for BONE methods
In this section, we provide a detailed calculation of νt(ψt)for some choices of ψt. We consider a choice of
(M.1: likelihood) to be linear Gaussian with known observation variance Rt, i.e.,
p(yt|θ,xt) =N(yt|x⊺
tθt,Rt). (44)
A.1 Runlength with prior reset ( RL-PR)
A.1.1 Unbounded number of hypotheses RL[inf]-PR
The work in Adams & MacKay (2007) takes ψt=rtto be the runlength, with rt∈{0,1,...,t}, that that
counts the number of steps since the last changepoint. Assume the runlength follows the dynamics (28). We
considerνt(rt) =p(rt|D1:t)such that
p(rt|D1:t) =p(rt,D1:t)/summationtextt
ˆrt=0p(ˆrt,D1:t), (45)
forrt∈{0,...,t}. The RL-PRmethod estimates p(rt,D1:t)for allrt∈{0,...,t}at every timestep. To
estimate this value recursively, we sum over all possible previous runlengths as follows
p(rt,D1:t)
=t−1/summationdisplay
rt−1=0p(rt,rt−1,D1:t−1,Dt)
=t−1/summationdisplay
rt−1=0p(rt−1,D1:t−1)p(rt|rt−1,D1:t−1)p(yt|rt,rt−1,xt,D1:t−1)
=p(yt|rt,xt,D1:t−1)t−1/summationdisplay
rt−1=0p(rt−1,D1:t−1)p(rt|rt−1).(46)
In the last equality, there are two implicit assumptions, (i) the runlength at time tis conditionally independent
of the dataD1:t−1given the runlength at time t−1, and (ii) the model is Markovian in the runlength,
that is, conditioned on rt, the value of rt−1bears no information. Mathematically, this means that (i)
27Under review as submission to TMLR
p(rt|rt−1,D1:t−1) =p(rt|rt−1)and (ii)p(yt|rt,rt−1,D1:t−1) =p(yt|rt,D1:t−1). From(46), we observe there
are only two possible scenarios for the value of rt. Eitherrt= 0orrt=rt−1+ 1withrt−1∈{0,...,t−1}.
Thus,p(rt,D1:t)becomes
p(rt|D1:t) =p(yt|rt,xt,D1:t−1)p(rt−1,D1:t−1)p(rt|rt−1) ifrt≥1
p(rt|D1:t) =p(yt|rt,xt,D1:t−1)t−1/summationdisplay
rt−1=0p(rt−1,D1:t−1)p(rt|rt−1)ifrt= 0.(47)
The joint density (47)considers two possible scenarios: either we stay in a regime considering the past
rt≥1observations, or we are in a new regime, in which rt= 0. Finally, note that (47)depends on three
terms: (i) the transition probability p(rt|rt−1), which it is assumed to be known, (ii) the previous log-joint
p(rt−1,D1:t−1), withrt−1∈{0,1,...,t−1}, which is estimated at the previous timestep, and (iii) the prior
predictive density
p(yt|rt,xt,D1:t−1) =/integraldisplay
p(yt|θt,xt)p(θt|rt,D1:t−1)dθt. (48)
For a choice of (M.1: likelihood) given by (44)and a choice of (M.3: prior) given by (16), the posterior
predictive (48) takes the form.
p(yt|rt,xt,D1:t−1) =/integraldisplay
N(yt|x⊺
tθt,Rt)N/parenleftig
θt|µ(rt)
t−1,Σ(rt)
t−1/parenrightig
dθt
=N/parenleftig
yt|x⊺
tµ(rt)
t−1,x⊺
tΣ(rt)
t−1xt+Rt/parenrightig
,(49)
withrt∈{0,...,t−1}. Here,/parenleftig
µ(rt)
t−1,Σ(rt)
t−1/parenrightig
are the posterior mean and covariance at time t−1built using
the lastrt≥1observations. If rt= 0, then (µ(rt)
t−1,Σ(rt)
t−1) = (µ0,Σ0).
A.1.2 Bounded number of hypotheses RL[K]-PR
If we maintain a set of Kpossible hypotheses, then Ψt={r(1)
t−1,...,r(K)
t−1}∈{ 0,...,t−1}Kis a collection of
Kunique runlengths obtained at time t−1. Next, (46) takes the form
p(rt,D1:t) =p(yt|rt,xt,D1:t−1)p(rt−1,D1:t−1)p(rt|rt−1) ifrt≥1,(50)
p(rt,D1:t) =p(yt|rt,xt,D1:t−1)/summationdisplay
rt−1∈Ψt−1p(rt−1,D1:t−1)p(rt|rt−1) ifrt= 0.(51)
Here, we have that either rt=rt−1+ 1whenrt−1∈Ψt−1orrt= 0. After computing p(rt,D1:t)for allK+ 1
possibles values of rt, a choice is made to keep Khypotheses. For timesteps t≤K, we evaluate all possible
hypotheses until t>K.
Algorithm 2 shows an update step under this process when we maintain a set of Kpossible hypotheses.
A.2 Runlength with moment-matched prior reset ( RL-MMPR)
Here, we consider a modified version of the method introduced in Fearnhead & Liu (2011). We consider the
choice of RLand adjust the choice of (M.3: prior) for RL-PRintroduced in Appendix A.1 whenever rt= 0. In
this combination, for rt= 0, we takeτ(θt|rt,D1:t−1) =p(θt|rt,D1:t−1). Next
p(θt|rt,D1:t−1) =t−1/summationdisplay
rt−1=0p(θt,rt−1|rt,D1:t−1)
=t−1/summationdisplay
rt−1=0p(rt−1|D1:t−1)p(rt|rt−1)p(θt|rt,rt−1,y1:t−1)
=t−1/summationdisplay
rt−1=0p(rt−1|D1:t−1)p(rt|rt−1)N(θt|µ(rt−1)
t−1,Σ(rt−1)
t−1).(52)
28Under review as submission to TMLR
Because (52)is a mixture model, we choose a conditional prior to be Gaussian that approximates the first
two moments. We obtain
E[θt|rt,y1:t−1] =t−1/summationdisplay
rt−1=0p(rt−1|D1:t−1)p(rt|rt−1)µ(rt−1)
t−1 (53)
for the first moment, and
E[θtθ⊺
t|rt,y1:t−1]t−1/summationdisplay
rt−1=0p(rt−1|D1:t−1)p(rt|rt−1)/parenleftig
Σ(rt−1)
t−1+µ(rt−1)
t−1µ(rt−1)⊺
t−1/parenrightig
(54)
for the second moment. The conditional prior mean and prior covariance under rt= 0take the form
µ(0)
t=E[θt|rt,y1:t−1],
Σ(0)
t=E[θtθ⊺
t|rt,y1:t−1]−(E[θt|rt,y1:t−1]) (E[θt|rt,y1:t−1])⊺.(55)
Algorithm 3 shows an update step under this process when we maintain a set of Kpossible hypotheses.
A.3 Runlength with OU dynamics and prior reset ( RL[1]-OUPR* )
In this section, we provide pseudocode for the new hybrid method we propose. Specifically, our choices
in BONE are: RL[1]-OUPR* for (M.2: auxvar) and (M.3: prior), LGfor (A.1: posterior), and DA[1]for
(A.2: weighting). Because of our choice of (A.2: weighting), RL[1]-OUPR* considers a single hypothesis (or
runlength) which, at every timestep, is either increased by one or set back to zero, according to the probability
of a changepoint and a threshold ϵ∈(0,1).
In essence, RL[1]-OUPR* follows the logic behind RL[1]-PR introduced in Section A.1 with K= 1hypothesis
and different choice of (M.3: prior). To derive the algorithm for RL[1]-OUPR* at timet>1, supposert−1
is available (the only hypothesis we track). Denote by r(1)
tthe hypothesis of a runlength increase, i.e.,
rt=rt−1+ 1and denote by r(0)
tthe hypothesis of a runlenght reset, i.e., rt= 0. The probability of a
runlength increase under a single hypothesis takes the form:
νt(r(1)
t) =p(r(1)
t|D1:t)
=p(r(1)
t,D1:t)
p(r(1)
t,D1:t) +p(r(0)
t,D1:t)
=p(yt|r(1)
t,xt,D1:t−1)p(rt−1,D1:t−1) (1−π)
p(yt|r(0)
t,xt,D1:t−1)p(rt−1,D1:t−1)π+p(yt|r(1)
t,xt,D1:t−1)p(rt−1,D1:t−1) (1−π)
=p(yt|r(1)
t,xt,D1:t−1) (1−π)
p(yt|r(0)
t,xt,D1:t−1)π+p(yt|r(1)
t,xt,D1:t−1) (1−π).(56)
whereπ=p(rt|rt−1)withrt= 0is the prior probability of a changepoint and and 1−π=p(rt|rt−1with
rt=rt−1+ 1is the probability of continuation of the current segment.
Next, we use νt(rt)to decide whether to update our parameters or reset them according to a prior belief
according to some threshold ϵ. This implements our choice of (M.3: prior) given in (17)and(18). Because
we maintain a single hypothesis, the weight at the end of the update step is set to 1. Algorithm 4 shows an
update step for RL[1]-OUPR* under the choice of (M.1: likelihood) given by (44).
A.4 Changepoint location with multiplicative covariance inflation CPL-MCI
The work in Li et al. (2021) takes ψt=s1:tto be at-dimensional vector where the i-th element is a binary
vector that determines a changepoint at time t. Then, the sum of the entries of s1:trepresents the total
number of changepoints up to, and including, time t.
29Under review as submission to TMLR
We takeνt(s1:t) =p(s1:t|D1:t), which is recursively expressed as
p(s1:t|D1:t) =p(st,s1:t−1|yt,xt,D1:t−1)
=p(s1:t−1|D1:t−1)p(st|s1:t−1,xt,yt,D1:t−1).(57)
Here,p(s1:t−1|D1:t−1)is inferred at the previous timestep t−1. The estimate of a changepoint conditioned
on the past changes and the measurements is
p(st= 1|s1:t−1,D1:t)
=p(st= 1)p(yt|xt,s1:t−1,st= 1,D1:t−1)
p(st= 1)p(yt|st= 1,xt,s1:t−1,y1:t−1) +p(st= 0)p(yt|st= 0,xt,s1:t−1,D1:t−1)
=/parenleftbigg
1 + exp/parenleftbigg
−log/parenleftbiggp(st= 1)p(yt|st= 1,xt,s1:t−1,y1:t−1)
p(st= 0)p(yt|st= 0,xt,s1:t−1,D1:t−1)/parenrightbigg/parenrightbigg/parenrightbigg−1
=σ(mt),(58)
whereσ(x) = 1/(1 + exp(−x))and
mt= log/parenleftbiggp(yt|st= 1,xt,s1:t−1,D1:t−1)
p(yt|st= 0,xt,s1:t−1,D1:t−1)/parenrightbigg
+ log/parenleftbiggp(st= 1)
p(st= 0)/parenrightbigg
, (59)
and similarly,
p(st= 0|s1:t−1,D1:t) = 1−σ(mt). (60)
Finally, the transition between states is given by p(s1:t|s1:t−1) =p(st).
30Under review as submission to TMLR
B Algorithms
Algorithm 2 Implementation of RL[K]-PR . We consider an update at time tand one-step ahead forecasting
at timet+ 1under a Gaussian linear model with known observation variance.
Require: (µ0,Σ0)// default prior beliefs
Require:Dt= (xt,yt)// current observation
Require:{r(k)
t−1}K
k=1∈{0,...,t−1}K// bank of runlengths at time t−1
Require:{p(r(k)
t−1,D1:t)}K
k=1// joint from past hypotheses
Require:/braceleftig
(µ(k)
t−1,Σ(k)
t−1)/bracerightigK
k=1// beliefs from past hypotheses
Require:xt+1// next-step observation
Require:p(y|θ,x) =N(y|θ⊺x,Rt)// Choice of (M.1: likelihood)
1:// Evaluate hypotheses if there is no changepoint
2:fork= 1,...,K do
3:r(k)
t←r(k)
t−1+ 1
4:p(yt|r(k)
t,xt,D1:t−1)←N (yt|x⊺
tµ(k)
t−1,x⊺
tΣ(k)
t−1xt+Rt)// posterior predictive for k-th hypothesis
5:p(r(k)
t,D1:t)←p(yt|r(k)
t,xt,D1:t−1)p(r(k)
t−1,D1:t−1)p(r(k)
t|r(k)
t−1)// update joint density
6: (¯µ(k)
t,¯Σ(k)
t)←(µ(k)
t−1,Σ(k)
t−1)
7:τt(θt;r(k)
t)←N (θt|¯µt,¯Σt)// choice of (M.3: prior)
8:λt(θt;r(k)
t)∝τt(θt;r(k)
t)p(yt|θ⊺xt,Rt)∝N(θt|µ(k)
t,Σ(k)
t)// following (24)
9:end for
10:// Evaluate hypothesis under a changepoint
11:r(k+1)
t←0
12:p(yt|r(k+1)
t,xt,D1:t−1)←N (yt|x⊺
tµ0,x⊺
tΣ0xt+Rt)// posterior predictive for k-th hypothesis
13:p(r(k+1)
t,D1:t)←p(yt|r(k+1)
t,xt,D1:t−1)/summationtextK
k=1p(r(k)
t,D1:t)p(r(t+1)
t|r(k)
t−1)
14:// Extend number of hypotheses to K+ 1and keep top Khypotheses
15:I1:k= top.k({p(r(1)
t,D1:t),...,p (r(k+1)
t,D1:t)}, K)
16:{p(r(k)
t,D1:t)}K
k=1←slice.at({p(r(k)
t,D1:t)}K+1
k=1, I1:K)
17:{(µ(k)
t,Σ(k)
t)}K
k=1←slice.at({(µ(k)
t,Σ(k)
t)}K+1
k=1, I1:K)
18:// build weight and make prequential prediction
19:νt(r(k)
t)←p(r(k)
t,D1:t)/summationtextK
j=1p(r(j)
t,D1:t)fork= 1,...,K
20:ˆyt+1←x⊺
t+1/parenleftig/summationtextK
k=1νt(r(k)
t)µ(k)
t/parenrightig
// prequential prediction under a linear-Gaussian model
21:return{(µ(k)
t,Σ(k)
t,r(k)
t)}K
k=1,ˆyt+1
In Algorithm 2, the function top.k(A,K )returns the indices of the top K≥1elements of Awith highest
value. The function slice.at(A,B)returns the elements in Aaccording to the list of indices B. If|A|≤|B|,
we return all elements in A.
31Under review as submission to TMLR
Algorithm 3 Implementation of RL[K]-MMPR . We consider an update at time tand one-step ahead forecasting
at timet+ 1under a Gaussian linear model with known observation variance.
Require:Dt= (xt,yt)// current observation
Require:{r(k)
t−1}K
k=1∈{0,...,t−1}K// bank of runlengths at time t−1
Require:{p(r(k)
t−1,D1:t)}K
k=1// joint from past hypotheses
Require:/braceleftig
(µ(k)
t−1,Σ(k)
t−1)/bracerightigK
k=1// beliefs from past hypotheses
Require:xt+1// next-step observation
Require:p(y|θ,x) =N(y|θ⊺x,Rt)// Choice of (M.1: likelihood)
1:// Evaluate hypotheses if there is no changepoint
2:fork= 1,...,K do
3:r(k)
t←r(k)
t−1+ 1
4:p(yt|r(k)
t,xt,D1:t−1)←N (yt|x⊺
tµ(k)
t−1,x⊺
tΣ(k)
t−1xt+Rt)// posterior predictive for k-th hypothesis
5:p(r(k)
t,D1:t)←p(yt|r(k)
t,xt,D1:t−1)p(r(k)
t−1,D1:t−1)p(r(k)
t|r(k)
t−1)// update joint density
6: (¯µ(k)
t,¯Σ(k)
t)←(µ(k)
t−1,Σ(k)
t−1)
7:τt(θt;r(k)
t)←N (θt|¯µt,¯Σt)// choice of (M.3: prior)
8:λt(θt;r(k)
t)∝τt(θt;r(k)
t)p(yt|θ⊺xt,Rt)∝N(θt|µ(k)
t,Σ(k)
t)// following (24)
9:end for
10:// Evaluate hypothesis under a changepoint
11:r(k+1)
t←0
12:µ0←E[θt|rt,y1:t−1]// following (53)
13:Σ0←E[θtθ⊺
t|rt,y1:t−1]−(E[θt|rt,y1:t−1]) (E[θt|rt,y1:t−1])⊺// following (53) and (54)
14:p(yt|r(k+1)
t,xt,D1:t−1)←N (yt|x⊺
tµ0,x⊺
tΣ0xt+Rt)// posterior predictive for k-th hypothesis
15:p(r(k+1)
t,D1:t)←p(yt|r(k+1)
t,xt,D1:t−1)/summationtextK
k=1p(r(k)
t,D1:t)p(r(t+1)
t|r(k)
t−1)
16:// Extend number of hypotheses to K+ 1and keep top Khypotheses
17:I1:k= top.k({p(r(1)
t,D1:t),...,p (r(k+1)
t,D1:t)}, K)
18:{p(r(k)
t,D1:t)}K
k=1←slice.at({p(r(k)
t,D1:t)}K+1
k=1, I1:K)
19:{(µ(k)
t,Σ(k)
t)}K
k=1←slice.at({(µ(k)
t,Σ(k)
t)}K+1
k=1, I1:K)
20:// build weight and make prequential prediction
21:νt(r(k)
t)←p(r(k)
t,D1:t)/summationtextK
j=1p(r(j)
t,D1:t)fork= 1,...,K
22:ˆyt+1←x⊺
t+1/parenleftig/summationtextK
k=1νt(r(k)
t)µ(k)
t/parenrightig
// prequential prediction under a linear-Gaussian model
23:return{(µ(k)
t,Σ(k)
t,r(k)
t)}K
k=1,ˆyt+1
32Under review as submission to TMLR
Algorithm 4 Implementation of RL[1]-OUPR* , with update at time tand for one-step ahead forecasting at
timet+ 1, under a Gaussian linear model with known observation variance.
Require:Dt= (xt,yt)// current observation
Require:xt+1// next-step observation
Require:ϵ∈(0,1)// restart threshold
Require:rt−1∈{0,...,t−1}// runlength at time t−1
Require: (µ0,Σ0)// default prior beliefs
Require: (µt−1,Σt−1)// beliefs from prior step
Require:p(y|θ,x) =N(y|θ⊺x,Rt)// Choice of (M.1: likelihood)
1:(r(0)
t,r(1)
t)←(0,rt−1+ 1)// choice of (M.2: auxvar)
2:p(yt|r(0)
t,xt,D1:t−1)←N (yt|x⊺
tµ0,x⊺
tΣ0xt+Rt)// posterior predictive at changepoint
3:p(yt|r(1)
t,xt,D1:t−1)←N (yt|x⊺
tµt−1,x⊺
tΣt−1xt+Rt)// posterior predictive if no changepoint
4:νt(r(1))←p(yt|r(1)
t,xt,D1:t−1)(1−π)
p(yt|r(1)
t,xt,D1:t−1) (1−π)+p(yt|r(0)
t,xt,D1:t−1)π// probability of no-changepoint at timestep t
5:
6:ifν(r(1)
t)>ϵthen
7:rt←r(1)
t
8: ¯µ(rt)
t←µ(rt−1)
t−1ν(r(1)
t) +µ0/parenleftig
1−ν(r(1)
t)/parenrightig
9: ¯Σ(rt)
t←Σ(rt−1)
t−1ν(r(1)
t)2+Σ0/parenleftig
1−ν(r(1)
t)2/parenrightig
10:else ifν(r(1)
t)≤ϵthen
11:rt←r(0)
t
12: ¯µ(rt)
t←µ0
13: ¯Σ(rt)
t←Σ0
14:end if
15:τt(θt;rt)←N (θt|¯µt,¯Σt)// choice of (M.3: prior)
16:λt(θt;rt)∝N(θt|¯µt,¯Σt)p(yt|θ⊺xt,Rt)∝N(θt|µt,Σt)// choice of (A.1: posterior)— via (24)
17:ˆyt+1←x⊺
t+1µt// prequential prediction (given linear-Gaussian model)
18:return (µt,Σt,rt),ˆyt+1
33