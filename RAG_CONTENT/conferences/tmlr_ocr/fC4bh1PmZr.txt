Under review as submission to TMLR
Counterfactual Learning of Stochastic Policies
with Continuous Actions
Anonymous authors
Paper under double-blind review
Abstract
Counterfactual reasoning from logged data has become increasingly important for many
applications such as web advertising or healthcare. In this paper, we address the problem of
learning stochastic policies with continuous actions from the viewpoint of counterfactual risk
minimization (CRM). While the CRM framework is appealing and well studied for discrete
actions, the continuous action case raises new challenges about modelization, optimization,
and offline model selection with real data which turns out to be particularly challenging. Our
paper contributes to these three aspects of the CRM estimation pipeline. First, we introduce
a modelling strategy based on a joint kernel embedding of contexts and actions, which
overcomes the shortcomings of previous discretization approaches. Second, we empirically
showthattheoptimizationaspectofcounterfactuallearningisimportant, andwedemonstrate
the benefits of proximal point algorithms and smooth estimators. Finally, we propose an
evaluation protocol for offline policies in real-world logged systems, which is challenging since
policies cannot be replayed on test data, and we release a new large-scale dataset along with
multiple synthetic, yet realistic, evaluation setups.
1 Introduction
Logged interaction data is widely available in many applications such as drug dosage prescription (Kallus &
Zhou, 2018), recommender systems (Li et al., 2012), or online auctions (Bottou et al., 2013). An important
task is to leverage past data in order to find a good policyfor selecting actions ( e.g., drug doses) from available
features (or contexts), rather than relying on randomized trials or sequential exploration, which may be costly
to obtain or subject to ethical concerns.
More precisely, we consider offline logged bandit feedback data, consisting of contexts and actions selected by
a givenlogging policy , associated to observed rewards. This is known as bandit feedback , since the reward
is only observed for the action chosen by the logging policy. The problem of finding a good policy thus
requires a form of counterfactual reasoning to estimate what the rewards would have been, had we used a
different policy. When the logging policy is stochastic, one may obtain unbiased reward estimates under a
new policy through importance sampling with inverse propensity scoring (IPS, Horvitz & Thompson, 1952).
One may then use this estimator or its variants for optimizing new policies without the need for costly
experiments (Bottou et al., 2013; Dudik et al., 2011; Swaminathan & Joachims, 2015a;b), an approach also
known as counterfactual risk minimization (CRM). While this setting is not sequential, we assume that
learning a stochastic policy is required so that one may gather new exploration data after deployment.
In this paper, we focus on stochastic policies with continuous actions, which, unlike the discrete setting, have
received little attention in the context of counterfactual policy optimization (Demirer et al., 2019; Kallus &
Zhou, 2018; Chen et al., 2016). As noted by Kallus & Zhou (2018) and as our experiments confirm, addressing
the continuous case with naive discretization strategies performs poorly. Our first contribution is about data
modeling : we introduce a joint embedding of actions and contexts relying on kernel methods, which takes
into account the continuous nature of actions, leading to rich classes of estimators that prove to be effective
in practice.
1Under review as submission to TMLR
In the context of CRM, the problem of estimation is intrinsically related to the problem of optimization
of a non-convex objective function. In our second contribution, we underline the role of optimization
algorithms (Bottou et al., 2013; Swaminathan & Joachims, 2015b). We believe that this aspect was
overlooked, as previous work has mostly studied the effectiveness of estimation methods regardless of the
optimization procedure. In this paper, we show that appropriate tools can bring significant benefits. To that
effect, we introduce differentiable estimators based on soft-clipping the importance weights, which are more
amenable to gradient-based optimization than previous hard clipping procedures (Bottou et al., 2013; Wang
et al., 2017). We provide a statistical analysis of our estimator and discuss its theoretical performance with
regards to the literature. We also find that proximal point algorithms (Rockafellar, 1976) tend to dominate
simpler off-the-shelf optimization approaches, while keeping a reasonable computation cost.
Finally, an open problem in counterfactual reasoning is the difficult question of reliable evaluation of new
policies based on logged data only. Despite significant progress thanks to various IPS estimators, we believe
that this issue is still acute, since we need to be able to estimate the quality of policies and possibly select
among different candidate ones beforebeing able to deploy them in practice. Our last contribution is a small
step towards solving this challenge, and consists of a new offline evaluation benchmark along with a new
large-scale dataset, which we call CoCoA, obtained from a real-world system. The key idea is to introduce
importance sampling diagnostics (Owen, 2013) to discard unreliable solutions along with significance tests to
assess improvements to a reference policy. We believe that this contribution will be useful for the research
community as we are unaware of similar publicly available large-scale datasets for continuous actions.
Our findings, outlined below, are based on experimental results and validated through diverse tasks, demon-
strating the effectiveness of counterfactual learning for stochastic policies with continuous actions.
1.Discrete methods are not effective enough for continuous action problems. Discretization strategies
tend to underperform in applications involving continuous actions. To address this, we propose a
modeling approach that jointly embeds contexts and continuous actions, demonstrating superior
empirical performance compared to the linear models of Kallus & Zhou (2018); Chen et al. (2016)
and the tree-based method of (Majzoubi et al., 2020). This approach, termed the Counterfactual
Loss Predictor (CLP), is detailed in Section 3.2.
2.Optimization perspectives of counterfactual learning problems matter. Our work introduces a novel
differentiable estimator based on soft-clipping of importance weights. Empirically, the soft-clipping
strategy outperforms existing weight transformation methods (Wang et al., 2017; Su et al., 2019;
Metelli et al., 2021). Additionally, our findings underscore the effectiveness of proximal point methods
(Rockafellar, 1976), which have been shown to be particularly beneficial for optimizing nonconvex
objectivefunctions(Paquetteetal.,2018). Thesoft-clippingestimatorandtheassociatedoptimization
tools are introduced in Section 4. In Section 5, we analyze the excess risk of the proposed estimator
and compare it to existing methods in the literature.
3.Real-world counterfactual risk minimization requires a standard offline evaluation protocol. To
address this, we release CoCoA, a large-scale dataset designed for counterfactual learning with
continuous actions, and propose an offline evaluation procedure for model selection in counterfactual
risk minimization tasks. Our protocol is empirically validated for assessing and identifying unreliable
solutions, making it a crucial tool for real-world deployment of policies learned through counterfactual
risk minimization. The dataset and evaluation protocol are presented in Section 6.
2 Related Work
A large effort has been devoted to designing CRM estimators that have less variance than the IPS method,
through clipping importance weights (Bottou et al., 2013; Wang et al., 2017), variance regularization
(Swaminathan & Joachims, 2015a), or by leveraging reward estimators through doubly robust methods (Dudik
et al., 2011; Robins & Rotnitzky, 1995). In order to tackle an overfitting phenomenon termed “propensity
overfitting”, Swaminathan & Joachims (2015b) also consider self-normalized estimators (Owen, 2013). Such
estimation techniques also appear in the context of sequential learning in contextual bandits (Agarwal et al.,
2Under review as submission to TMLR
2014; Langford & Zhang, 2008), as well as for off-policy evaluation in reinforcement learning (Jiang & Li,
2016). In contrast, the setting we consider is not sequential. Moreover, unlike direct approaches (Dudik
et al., 2011) which learn a cost predictor to derive a deterministic greedy policy, our approach learns a model
indirectly by rather minimizing the policy risk. In discrete actions settings, several approaches have been
proposed for large action spaces (Majzoubi et al., 2020; Sachdeva et al., 2023; Saito et al., 2023).
While most approaches for counterfactual policy optimization tend to focus on discrete actions, few works
have tackled the continuous action case, again with a focus on estimation rather than optimization. In
particular, propensity scores for continuous actions were considered by Hirano & Imbens (2004). More
recently, evaluation and optimization of continuous action policies were studied in a non-parametric context
by Kallus & Zhou (2018), Lee et al. (2022), and by Demirer et al. (2019) in a semi-parametric setting. In
causal inference, recent works have addressed the question of treatment effect estimation with continuous
actions (Colangelo & Lee, 2023; Bockel-Rickermann et al., 2023).
In contrast to these previous methods, (i) we focus on stochastic policies while they consider deterministic
ones, even though the kernel smoothing approach of Kallus & Zhou (2018) and Lee et al. (2022) may be
interpreted as learning a deterministic policy perturbed by Gaussian noise. (ii) The terminology of kernels
used by Kallus & Zhou (2018) refers to a different mathematical tool than the kernel embedding used in our
work. We use positive definite kernels to define a nonlinear representation of actions and contexts in order
to model the reward function, whereas Kallus & Zhou (2018) use kernel density estimation to obtain good
importance sampling estimates and not model the reward. Chen et al. (2016) also use a kernel embedding of
contexts in their policy parametrization, while our method jointly models contexts and actions. Moreover,
their method requires computing an n×nGram matrix, which does not scale with large datasets; in principle,
it should be however possible to modify their method to handle kernel approximations such as the Nyström
method (Williams & Seeger, 2001). Besides, their learning formulation with a quadratic problem is not
compatible with CRM regularizers introduced by (Swaminathan & Joachims, 2015a;b) which would change
their optimization procedure. Eventually, we note that Krause & Ong (2011) use similar kernels to ours for
jointly modeling contexts and actions, but in the different setting of sequential decision making with upper
confidence bound strategies. (iii) While Kallus & Zhou (2018) and Demirer et al. (2019) focus on policy
estimation , our work introduces a new continuous-action data representation and encompasses optimization :
in particular, we propose a new contextual policy parameterization, which leads to significant gains compared
to baselines parametrized policies on the problems we consider, as well as further improvements related to
the optimization strategy. We also note that, apart from Demirer et al. (2019) that uses an internal offline
cross-validation for model selection, previous works did not perform offline model selection nor evaluation
protocols, which are crucial for deploying methods on real data.
Optimization methods for learning stochastic policies have been mainly studied in the context of reinforcement
learning through the policy gradient theorem (Ahmed et al., 2019; Sutton et al., 2000; Williams, 1992). Such
methods typically need to observe samples from the new policy at each optimization step, which is not
possible in our setting. Other methods leverage a form of off-policy estimates during optimization (Kakade &
Langford, 2002; Schulman et al., 2017), but these approaches still require to deploy learned policies at each
step, while we consider objective functions involving only a fixed dataset of collected data. In the context of
CRM, Su et al. (2019) introduce an estimator with a continuous clipping objective that achieves an improved
bias-variance trade-off over the doubly-robust strategy. Nevertheless, this estimator is non-smooth, unlike our
soft-clipping estimator.
3 Modeling of Continous Action Policies
We now review the CRM framework, originally introduced for discrete action spaces, and then present our
modeling approach for policies with continuous actions.
3.1 Background
For a stochastic policy πover a set of actions A, a contextual bandit environment generates i.i.d. context
featuresx∼PXinX, actionsa∼π(·|x)and lossesy∼PY(·|x,a), which may be seen as negative rewards,
3Under review as submission to TMLR
for some conditional probability distribution PY. We denote the resulting distribution over triplets (x,a,y )
byPπ. Then, the risk of a policy πis defined as:
L(π) =E(x,a,y )∼Pπ[y]. (1)
For the logged bandit problem, the environment provides an offline loggeddataset (xi,ai,yi)i=1,...,n, where
we assume (xi,ai,yi)∼Pπ0i.i.d. for a given stochastic logging1policyπ0, and we assume the propensities
π0,i:=π0(ai|xi)to be known.2Then, the task lies in using the logged data to determine a policy ˆπin a set
ofstochastic policies Πwith small risk.3Unlike reinforcement learning problems, where data can be gathered
iteratively, here the data is collected once using π0. As a result, the quality of a new policy ˆπcan only be
assessed counterfactually: “How would ˆπhave performed if it had been used during data collection?”
To minimize L, we typically have access to an empirical estimator ˆLallowing to perform counterfactual
reasoning and solve the following regularized problem:
ˆπ∈arg min
π∈Π/braceleftig
ˆL(π) + Ω(π)/bracerightig
, (2)
where Ωis a regularizer. With appropriate counterfactual estimators for ˆL, the framework of (2) has been
calledcounterfactual risk minimization (Swaminathan & Joachims, 2015a).
In this paper, we are motivated by real-world problems involving continuous actions, such as those presented
in Section 7, which requires addressing three aspects:
•Modeling: Which policy class Πis suitable for continuous actions, particularly when it is important
to capture potential nonlinear interactions between actions and contexts? To address this, we propose
the Counterfactual Loss Predictor (CLP), a method that serves as a smooth approximation of a
greedy policy, leveraging a kernel embedding of actions and contexts.
•Optimization : We propose a counterfactual loss functions ˆLthat is differentiable (Section 4.1). Then,
we discuss optimization strategies to deal with the non-convexity of ˆLandΩ(Section 4.2).
•Evaluation: In Section 6, design a new offline protocol to evaluate the performance of the solution
of (2) and to perform model selection on the choice (Π,ˆL,Ω).
Before, we present our contributions, we recall below existing choices for ˆLandΩthat we will evaluate later,
and another classical estimator ˆπ(Direct method) which will motivate our new policy class Π.
Counterfactual policy learning. The counterfactual4approach tackles the distribution mismatch between
the logging policy π0(·|x)and a policy πinΠvia importance sampling. The IPS method (Horvitz & Thompson,
1952) relies on correcting the distribution mismatch using the well-known relation
L(π) =E(x,a,y )∼Pπ0/bracketleftbigg
yπ(a|x)
π0(a|x)/bracketrightbigg
, (3)
assumingπ0has non-zero mass on the support of π. This allows us to derive an unbiased empirical estimate
ˆLIPS(π) =1
nn/summationdisplay
i=1yiπ(ai|xi)
π0,i. (4)
1The terminology associated to "logged" data and "logging" policy from Swaminathan & Joachims (2015a) is often referred to
as trajectory data and behavior policy in the importance sampling literature (Owen, 2013)
2In the continuous action case, we assume that the probability distributions admit densities and thus propensities denote the
density function of π0evaluated on the actions given a context.
3Note that this definition may also include deterministic policies by allowing Dirac measures, unless Πincludes a specific
constraint e.g., minimum variance, which may be desirable in order to gather data for future offline experiments.
4The "counterfactual" term comes from the causal inference literature and refers in this policy learning context to all estimators
using the inverse propensity scoring (IPS) method (Horvitz & Thompson, 1952)
4Under review as submission to TMLR
However, the empirical estimator ˆLIPS(π)has large variance and is subject to various overfitting phenomena.
First, this estimator can be prone to loss overfitting which refers to overweighting losses yiof rare sample
points which were highly unlikely under π0. For this, regularization strategies have been proposed, such
as clipping the importance sampling weights (cIPS, Bottou et al., 2013; Swaminathan & Joachims, 2015a).
Second, such estimators also suffer from propensity overfitting (Swaminathan & Joachims, 2015b) where the
minimization of the latter estimator can lead to learn policies that artificially overfit training data for negative
lossesyi(to maximize π(ai|xi)) or conversely avoid training data when losses are positives. Subsequently,
self-normalized estimators (SNIPS, Swaminathan & Joachims, 2015b; Owen, 2013) have been presented to
circumvent this issue, see Appendix A for a review and Appendix B.1 for a motivating example.
The direct method DM and the optimal greedy policy. For this class of methods, an important
quantity is the expected loss given actions and context, denoted by η∗(x,a) =E[y|x,a]. If this expected
loss was known, an optimal (deterministic) greedy policy π∗would simply select actions that minimize the
expected loss
π∗(x) = arg min
a∈Aη∗(x,a). (5)
It is then tempting to use the available data to learn an estimator ˆη(x,a)of the expected loss, for instance by
using ridge regression to fit yi≈ˆη(xi,ai)on the training data. Then, we may use the deterministic greedy
policy
ˆπDM(x) = arg min
a∈Aˆη(x,a).
This approach, termed direct method (DM), has the benefit of avoiding the high-variance problems of IPS-
based methods, but may suffer from large bias since it ignores the potential mismatch between ˆπDMandπ0.
Specifically, the bias is problematic when the logging policy provides unbalanced data samples ( e.g., only
samples actions in a specific part of the action space) leading to overfitting (Bottou et al., 2013; Dudik et al.,
2011; Swaminathan & Joachims, 2015b). Conversely, counterfactual methods re-balance these generated
data samples with importance weights and mitigate the distribution mismatch to better estimate reward
function on less explored actions (see explanations in Appendix C). Nevertheless, such loss estimators can be
sometimes effective in practice and may be used to improve IPS estimators in the so-called doubly robust
(DR) estimator (Dudik et al., 2011) by applying IPS to the residuals yi−ˆη(xi,ai), thus using ˆηas a control
variate to decrease the variance of IPS.
3.2 The Counterfactual Loss Predictor (CLP) for Continuous Actions Policies
We recall that our estimator ˆπis designed by optimizing (2) over a class of policies Π. In this subsection,
we discuss how to choose Πto model continuous actions problems. We emphasize that when considering
continuous action spaces, the choice of policies Πis more involved than in the discrete case. One may
indeed naively discretize the action space into buckets and leverage discrete action strategies, but then local
information within each bucket gets lost and it is non-trivial to choose an appropriate bucketization of the
action space based on logged data, which contains non-discrete actions.
We focus on stochastic policies belonging to certain classes of continuous distributions, such as Normal or
log-Normal with context-dependent means and variances. Specifically, we consider a set of policies of the form
Π =/braceleftig
πθs.t. for any x∈X, πθ(·|x) =D(µβ(x),σ2)withθ= (β,σ)∈Θ/bracerightig
, (6)
whereD(a,b)is a distribution of probability with mean aand variance b, such as the Normal distribution,
andΘis a parameter space. Here, the parameter space Θcan be written as Θ = Θβ×Θσ. The space Θσis
either a singleton (if σis considered as a fixed parameter specified by the user) or R∗
+(ifσis a parameter to
be optimized). The space Θβis the parameter space which models the contextual mean x∝⇕⊣√∫⊔≀→µβ(x), which
will be specified shortly. Note that in our policy set, the variance parameter σis a parameter that calibrates
the stochasticity of the policy. For applications where a learned policy needs to be deployed to collect data in
later rounds, a minimal variance can be enforced to ensure future exploration.
5Under review as submission to TMLR
3.2.1 Examples of simple context-dependent policies
Before introducing a more expressive model for Θβ, we consider the following simple counterfactual baselines
forΘβthat only consider contexts and that will be compared to our model in the experimental Section 7.
Given a context xinX⊂Rd:
–constant:µβ(x) =β(context-independent);
–linear:µβ(x) =⟨x,β1⟩+β0withβ= (β0,β1)∈Rd+1;
–poly:µβ(x) =⟨xx⊤,β1⟩+β0withβ= (β0,β1)∈Rd2+1.
These baselines require learning the parameters βby using the CRM approach (2). Intuitively, the goal is to
find a stochastic policy that is close to the optimal deterministic one from Eq. (5). Yet, these approaches do
not model potential interactions between actions and contexts. While these approaches, adopted by Chen
et al. (2016), Kallus & Zhou (2018) can be effective in simple problems, they may be limited in more difficult
scenarios where the expected loss η∗(x,a)has a complex behavior as a function of a. To address this issue,
we introduce the Counterfactual Loss Predictor (CLP) model in the next section.
3.2.2 The counterfactual loss predictor (CLP) model
The CLP model relies on two ideas: (i) setting µβ(x)to be a smooth approximation of the greedy policy (5),
and(ii)usingaparametricmodel ηβ(x,a)ofthelosswhichisexpressiveenoughtocapturecomplexinteractions
between actions and contexts. More precisely, assuming that we are given such a parametric model ηβ(x,a),
which we call loss predictor , we parametrize the mean of a stochastic policy by using a soft-argmin operator
with temperature γ >0:
CLP:µCLP
β(x) =m/summationdisplay
i=1aiexp(−γηβ(x,ai))/summationtextm
j=1exp(−γηβ(x,aj)), (7)
wherea1,...,am∈Aare anchor points ( e.g., a regular grid or quantiles of the action space). Then, µβmay
be viewed as a smooth approximation of the greedy policy µgreedy (x) =arg minaη(x,a). This allows CLP
policies to capture complex behavior of the expected loss as a function of a. The motivation for introducing
a soft-argmin operator over a finite number of anchor points is to build a policy that avoids the explicit
optimization over the continuous action set, while making the resulting CRM problem differentiable.
Next, we need of course to define the loss predictor ηβ(x,a). We choose it of the form
ηβ(x,a) =⟨β,ψ(x,a)⟩,
for some parameter β∈Rp, which norm controls the smoothness of ηβ, and a feature map ψ(x,a)∈Rp
that we detail in two parts: a joint kernel embedding between the actions and the contexts and a Nyström
approximation. The complete modeling of ηβ(x,a)is summarized in Figure 1.
1. Joint kernel embedding. In a continuous action problem, a reasonable assumption is that losses vary
smoothly as a function of actions. Thus, a good choice is to take ηin a space of smooth functions such as the
reproducing kernel Hilbert space H(RKHS) defined by a positive definite kernel (Schölkopf & Smola, 2002),
so that one may control the smoothness of ηthrough regularization with the RKHS norm. More precisely, we
consider kernels of the form
K((x,a),(x′,a′)) =⟨ψX(x),ψX(x′)⟩e−α
2∥a−a′∥2, (8)
where, for simplicity, ψX(x)is either a linear embedding ψX(x) =xor a quadratic one ψX(x) = (xxT,x),
while actions are compared via a Gaussian kernel, allowing to model complex interactions between contexts
and actions.
2. Nyström method and explicit embedding. Since traditional kernel methods lack scalability, we rely on
the classical Nyström approximation (Williams & Seeger, 2001) of the Gaussian kernel, which provides us
6Under review as submission to TMLR
contextsx∈X
actionsa∈AK((x,a),(x′,a′))Joint kernel embedding
ψ(x,a)∈RpNyström approximation
ηβ(x,a) =⟨β,ψ(x,a)⟩Loss predictor
Figure 1: Illustration of the joint kernel embedding for the counterfactual loss predictor (CLP) and loss
estimator.
a finite-dimensional approximate embedding ψA(a)inRmsuch thate−α
2∥a−a′∥2≈⟨ψA(a),ψA(a′)⟩for all
actionsa,a′. This allows us to build a finite-dimensional embedding
ψ(x,a) =ψX(x)⊗ψA(a), (9)
where⊗denotes the tensorial product, such that
K((x,a),(x′,a′))≈⟨ψX(x),ψX(x′)⟩⟨ψA(a),ψA(a′)⟩=⟨ψ(x,a),ψ(x′,a′)⟩.
More precisely, Nyström’s approximation consists in projecting each point from the RKHS to a m-dimensional
subspace defined as the span of manchor points, representing here the mapping to the RKHS of mactions
¯a1,¯a2,..., ¯amof the Nyström dictionary Z. In practice, we may choose ¯aito be equal to the aiin (7), since
in both cases the goal is to choose a set of “representative” actions. For one-dimensional actions ( A⊆R), it is
reasonable to consider a uniform grid, or a non-uniform ones based on quantiles of the empirical distribution
of actions in the dataset. In higher dimensions, one may simply use a K-means algorithms and assign anchor
points to centroids. The number of anchor points directly affects the quality of the approximation: a larger
number enhances representational power but reduces computational efficiency for large-scale applications,
and vice versa. In practice, this number is treated as a hyperparameter, selected using the offline protocol
described in Section 6. Its impact is further illustrated through a comparison between continuous and discrete
strategies in Appendix G.1.
From an implementation point of view, Nyström’s approximation considers the embedding ψA(a) =
K−1/2
ZZKZ(a), whereKZZ= [KA(¯ai,¯aj)]ijandKZ(a) = [KA(a,¯ai)]iandKAis the Gaussian kernel.
The whole procedure to design the CLP policy set ΠCLPis given in Algorithm 1.
Algorithm 1: Construction of the CLP Policy Set
Input:Temperature γ >0, kernelK, Nyström dictionary Z⊆A, parametric distribution D(such as
Normal or log-Normal).
Output: Policy set ΠCLP.
1. Define the d-dimensional feature map ψas in Eq. (9) by using KandZ.
2. For any β∈Rdand(x,a)∈X×A , define
ηβ(x,a) =⟨β,ψ(x,a)⟩andµCLP
β(x) =/summationdisplay
a∈Zexp(−γηβ(x,a))/summationtext
a′∈Zexp(−γηβ(x,a′)).
3. Define the policy set
ΠCLP=/braceleftbig
πs.t.∀x∈X, π(·|x) =D(µCLP
β(x),σ2),with (β,σ)∈Θ/bracerightbig
.
The anchor points that we use can be seen as the parameters of an interpolation strategy defining a smooth
function, similar to knots in spline interpolation. Naive discretization strategies would prevent us from
7Under review as submission to TMLR
exploiting such a smoothness assumption on the loss with respect to actions and from exploiting the structure
of the action space. Note that Section 7 provides a comparison with naive discretization strategies, showing
important benefits of the kernel approach. Our goal was to design a stochastic, computationally tractable,
differentiable approximation of the optimal (but unknown) greedy policy (5).
4 On Optimization Perspectives for CRM
Because our models yield non-convex CRM problems, it is crucial to study optimization aspects. Here, we
introduce a differentiable clipping strategy for importance weights and discuss optimization algorithms.
4.1 Soft Clipping IPS
The classical hard clipping estimator
ˆLcIPS(π) =1
nn/summationdisplay
i=1yimin{π(ai|xi)/π0,i,M} (10)
makes the objective function non-differentiable, and yields terms in the objective with clipped weights to
have zero gradient. In other words, a trivial stationary point of the objective function is that of a stochastic
policy that differs enough from the logging policy such that all importance weights are clipped. To alleviate
this issue, we propose a differentiable logarithmic soft-clipping strategy. Given a threshold parameter M≥0
and an importance weight wi=π(ai|xi)/π0,i, we consider the soft-clipped weights:
ζ(wi,M) =/braceleftigg
wi ifwi≤M
α(M) log (wi+α(M)−M)otherwise,(11)
whereα(M)is such that α(M)log(α(M)) =M, which yields a differentiable operator. We illustrate in Fig. 2
the expression of the logarithmic clipping
0 2 4 6 8 10
w0246810c(w)c(w) = min(w,M)
c(w)=(w,M)
c(w) = w
M
Figure 2: Soft Clipping of importance sampling weight w.
We give further explanations about the benefits of clipping strategies in Appendix B and compare our
soft-clipping strategy to other methods in Appendix G.2. Then, the soft clipping IPS (scIPS) estimator is
defined as:
ˆLscIPS(π) =1
nn/summationdisplay
i=1yiζ/parenleftbiggπ(ai|xi)
π0,i,M/parenrightbigg
. (12)
Note that our soft-clipping estimator differs from existing importance weight transformation strategies such
as the power-mean correction of importance sampling (PMCIS) (Metelli et al., 2021), the SWITCH (Wang
et al., 2017) and CAB (Su et al., 2019) estimators, which were not motivated by differentiability issuesas (see
an empirical comparison in Appendix G.2).
We now provide a similar generalization bound to that of Swaminathan & Joachims (2015a) (for the hard-
clipped version) for the variance-regularized objective of this soft-clipped estimator, justifying its use as a
8Under review as submission to TMLR
good optimization objective for minimizing the expected risk. Writing χi(π) =yiζ/parenleftig
π(ai|xi)
π0(ai|xi),M/parenrightig
, we recall
the empirical variance with scIPS that is used for regularization:
ˆVscIPS(π) =1
n−1n/summationdisplay
i=1(χi(π)−¯χ(π))2,with ¯χ(π) =1
nn/summationdisplay
i=1χi(π). (13)
We assume that costs yi∈[−1,0]almost surely, as in (Swaminathan & Joachims, 2015a), and make the
additional assumption that the importance weights π(ai|xi)/π0(ai|xi)are upper bounded by a constant W
almost surely for all π∈Π. This is satisfied, for instance, if all policies have a given compact support ( e.g.,
actions are constrained to belong to a given interval) and π0puts mass everywhere in this support.
Proposition 4.1 (Generalization bound for ˆLscIPS(π)).LetΠbe a policy class and π0be a logging policy,
under which an input-action-cost triple follows Dπ0. Assume that−1≤y≤0a.s. when (x,a,y )∼Dπ0and
that the importance weights are bounded by W. Then, with probability at least 1−δ, the IPS estimator with
soft clipping (12) on nsamples satisfies
∀π∈Π, L (π)≤ˆLscIPS (π) +O
/radicaligg
ˆVscIPS (π)/parenleftbig
Cn(Π,M) + log1
δ/parenrightbig
n+S/parenleftbig
Cn(Π,M) + log1
δ/parenrightbig
n
,
whereS=ζ(W,M ) =O(logW),ˆVscIPS (π)denotes the empirical variance of the cost estimates (20),
andCn(Π,M)is a complexity measure of the policy class defined in (24).
We prove Proposition 4.1 in Appendix E. This generalization error bound motivates the use of the empirical
variance penalization as in (Swaminathan & Joachims, 2015a) and shows that minimizing both the empirical
risk and penalization of the soft clipped estimator minimizes an upper bound of the true risk of the policy.
Note that while the bound requires importance weights bounded by a constant W, the bound only scales
logarithmically with WwhenW≫M, compared to a linear dependence for IPS (Swaminathan & Joachims,
2015a). However we gain significant benefits in terms of optimization by having a smooth objective.
Remark 4.1. If losses are in the range [−c,0], the constant Scan be replaced by cS, making the bound
homogeneous in the scale (indeed, the variance term is also scaled by c).
Remark 4.2. For a fixed parameter M, the scIPS estimator is less biased than the cIPS. Indeed, we can
bound the importance weights as min/braceleftig
π(a|x)
π0(a|x),M/bracerightig
≤ζ/parenleftig
π(a|x)
π0(a|x),M/parenrightig
≤π(a|x)
π0(a|x)and subsequently derive the
bound:/vextendsingle/vextendsingle/vextendsingleEx,a∼π0(·|x)/bracketleftbigg
ymin/braceleftbiggπ(a|x)
π0(a|x),M/bracerightbigg
−y/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle≥/vextendsingle/vextendsingle/vextendsingleEx,a∼π0(·|x)/bracketleftbigg
yζ/parenleftbiggπ(a|x)
π0(a|x),M/parenrightbigg
−y/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle≥0
We note however that the parameter Mmay have different optimal values for both methods, and that the main
motivation for such a clipping strategy is to provide a differentiable estimator which is not the case for cIPS
in areas where all point are clipped.
4.2 Proximal Point Algorithms
Non-convex CRM objectives have been optimized with classical gradient-based methods (Swaminathan
& Joachims, 2015a;b) such as L-BFGS (Liu & Nocedal, 1989), or the stochastic gradient descent ap-
proach (Joachims et al., 2018). Proximal point methods are classical approaches originally designed for convex
optimization (Rockafellar, 1976), which were then found to be useful for nonconvex functions (Fukushima &
Mine, 1981; Paquette et al., 2018). In order to minimize a function L, the main idea is to approximately solve
a sequence of subproblems that are better conditioned than L, such that the sequence of iterates converges
towards a better stationary point of L. More precisely, for our class of parametric policies, the proximal point
method consists of computing a sequence
θk≈arg min
θ/parenleftig
L(πθ) +κ
2∥θ−θk−1∥2
2/parenrightig
, (14)
9Under review as submission to TMLR
whereL(πθ) =ˆL(πθ) + Ω(πθ)andκ>0is a constant parameter. The regularization term Ωoften penalizes
the variance (Swaminathan & Joachims, 2015b), see Appendix F.2. The role of the quadratic function in (14)
is to make subproblems “less nonconvex” and for many machine learning formulations, it is even possible to
obtain convex sub-problems with large enough κ(see Paquette et al., 2018).
Inthispaper, weconsidersuchastrategy(14)withaparameter κ, whichwesettozeroonlyforthelastiteration.
Note that the effect of the proximal point algorithm differs from the proximal policy optimization (PPO)
strategy used in reinforcement learning (Schulman et al., 2017), even though both approaches are related.
PPO encourages a new stochastic policy to be close to a previous one in Kullback-Leibler distance. Whereas
the term used in PPO modifies the objective function (and changes the set of stationary points), the proximal
point algorithm optimizes and finds a stationary point of the original objective L, even with fixed κ.
The proximal point algorithm (PPA) introduces an additional computational cost as it leads to solving
multiple sub-problems instead of a single learning problem. In practice for 10 PPA iterations and with the
L-BFGS solver, the computational overhead was about 3×in comparison to L-BFGS without PPA. This
overhead seems to be the price to pay to improve the test reward and obtain better local optima, as we show
in the experimental section 7.2.2. Nevertheless, we would like to emphasize that computational time is often
not critical for the applications we consider, since optimization is performed offline.
5 Analysis of the Excess Risk
In the previous section, we have introduced a new counterfactual estimator ˆLscIPS(12) of the risk, which
satisfies good optimization properties. Motivated by the generalization bound in Proposition 4.1, for any policy
class Π, we associate ˆLscIPSwith the data-dependent regularizer and define the following CRM estimator
ˆπCRM= arg min
π∈Π/braceleftbigg
ˆLscIPS(π) +λ/radicaligg
ˆVscIPS(π)
n/bracerightbigg
, (15)
where ˆVscIPS(π)is the empirical variance defined in (13). In this section, we provide theoretical guarantees
on the excess risk of ˆπCRM, first for any general policy class Π, then for our newly introduced policy class
ΠCLP(Section 3.2). We provide the following high-probability upper-bound on the excess-risk.
Proposition 5.1 (Excess risk upper bound) .Consider the notations and assumptions of Proposition 4.1.
LetˆπCRMbe the solution of the CRM problem in Eq. (15) and π∗∈Πbe any policy. Then, with well chosen
parameters λandM, denoting the variance σ2
∗= Varπ0/bracketleftbig
π∗(a|x)/π0(a|x)/bracketrightbig
, with probability at least 1−δ:
L(ˆπCRM)−L(π∗)≲/radicaligg
(1 +σ2
∗) log(W+e)/parenleftbig
Cn(Π,M) + log1
δ/parenrightbig
n+log(W+e)(Cn(Π,M) + log1
δ)
n,
where≲hides universal multiplicative constants. In particular, assuming also that π0(x|a)−1are uniformly
bounded, the complexity of the class ΠCLPdescribed in Section 3.2, applied with a bounded kernel and
Θ =/braceleftbig
β∈Rm,s.t∥β∥≤C}×/braceleftbig
σ/bracerightbig
, is of order
Cn(ΠCLP)≤O(mlogn),
wheremis the size of the Nyström dictionary and O(·)hides multiplicative constants independent of nand
m.
The proof and the exact definition of Cn(Π,M)are provided in Appendix E. Our analysis relies on Theorem
15 of Maurer & Pontil (2009).
Comparison with related work The closest works are the ones of Chen et al. (2016) and Kallus & Zhou
(2018). Chen et al. (2016) analyze their method for Besov policy classes Bα
1,∞(Rd). Whenα→∞, they
obtain a rate of order O(n−1/4). In this case, their setting is parametric and their rate can be compared to our
O(n−1/2)whenmis finite. Kallus & Zhou (2018) provide bounds with respect to general deterministic classes
of functions, whose complexity is measured by their Rademacher complexity. For parametric classes, their
10Under review as submission to TMLR
excess risk is bounded (up to logs) by Rˆπ≲h−2n−1/2+h−1n−1/2+h2, wherehis a smoothing parameter.
By optimizing the bandwidth h=O(n−1/8), their method also yields a rate of order O(n−1/4).
Yet, a key difference between their setting and ours explains the gap between their rate O(n−1/4)and
O(n−1/2)of Proposition 5.1. Both consider deterministic policy classes, while we only consider stochastic
policies. Indeed, Wandσ∗would be unbounded for deterministic policies in Proposition 5.1. Therefore, to
leverage deterministic policies, they both need to smooth their predictions and suffer an additional bias that
we do not incur. This is why there is a difference between their rate and ours. For instance, for stochastic
classes with variance σ2, Kallus & Zhou (2018) would satisfy Rˆπ≲σ−2n−1/2+σ−1n−1/2forh≈σ, which
would also entail a rate of order O(n−1/2). Interestingly, on the other hand, our approach would satisfy a
rateO(n−1/3)for deterministic policies, i.e., σ2→0(see Appendix E.2). This may be explained by the fact
that, contrary to Kallus & Zhou (2018); Chen et al. (2016) who only use it in practice, we consider variance
regularization and clipping in our analysis.
Another related work is (Demirer et al., 2019). They obtain an excess risk rate of O(n−1/2)when learning
deterministic continuous action policies with a policy space of finite and small VC-dimension. Under a
margin condition, as in bandit problems, their rate may be improved to O(log(n)/n). However, their method
significantly differs from ours and Chen et al. (2016), Kallus & Zhou (2018) because it relies on a two steps
plug-in procedure: first estimate a nuisance function, then learn a policy using with a value function using this
estimate. Eventually, we note that Majzoubi et al. (2020) also enjoys a regret of O(n−1/2)(up to logarithmic
factors) but learns tree policies that are hardly comparable to ours. Both approaches turn out to perform
worse in all our benchmarks, as seen in Section. 7.2.2.
6 On Evaluation and Model Selection for Real-World Data
The CRM framework helps finding solutions when online experiments are costly, dangerous or raising ethical
concerns. As such it needs a reliable validation and evaluation procedure before rolling-out any solution
in the real world. In the continuous action domain, previous work have mainly considered semi-simulated
scenarios(Bertsimas&McCord,2018;Kallus&Zhou,2018), wherecontextsaretakenfromsuperviseddatasets
but rewards are synthetically generated. To foster research on practical continuous policy optimization,
we release a new large-scale dataset called CoCoA, which to our knowledge is the first to provide logged
exploration data from a real-world system with continuous actions. Additionally, we introduce a benchmark
protocol for reliably evaluating policies using off-policy evaluation.
6.1 The CoCoA Dataset
The CoCoA dataset comes from the Criteo online advertising platform which ran an experiment involving
a randomized, continuous policy for real-time bidding. Data has been properly anonymized so as to not
disclose any private information. Each sample represents a bidding opportunity for which a multi-dimensional
contextxinRdis observed and a continuous action ainR+has been chosen according to a stochastic policy
π0that is logged along with the reward −y(meaning cost y) inR. The reward represents an advertising
objective such as sales or visits and is jointly caused by the action and context (a,x). Particular care has been
taken to guarantee that each sample (xi,ai,π0(ai|xi),yi)is independent. The goal is to learn a contextual,
continuous, stochastic policy π(a|x)that generates more reward in expectation than π0, evaluated offline,
while keeping some exploration (stochastic part). As seen in Table 1, a typical feature of this dataset is
the high variance of the cost ( V[Y]), motivating the scale of the dataset Nto obtain precise counterfactual
estimates.
N d E[−Y]V[Y]V[A]P(Y̸= 0)
120.1063 11.37 9455 .01.07
Table 1: CoCoA dataset summary statistics.
11Under review as submission to TMLR
6.2 Evaluation Protocol for Logged Data
In order to estimate the test performance of a policy on real-world systems, off-policy evaluation is needed,
as we only have access to logged exploration data. Yet, this involves in practice a number of choices and
difficulties, the most documented being i) potentially infinite variance of IPS estimators (Bottou et al., 2013)
and ii) propensity over-fitting (Swaminathan & Joachims, 2015a;b) where actions in the logged data can be
overfitted or avoided (see Section 3.1). The former implies that it can be difficult to accurately assess the
performance of new policies due to large confidence intervals, while the latter may lead to estimates that
reflect large importance weights rather than rewards.
A proper evaluation protocol should therefore guard against such outcomes.
Algorithm 2: Evaluation Protocol
Input: 1−δ: confidence of statistical test (def: 0.95); ν: a max deviance ratio for effective sample size
(def: 0.01);
Output: counter-factual estimation of L(π)and decision to reject the null hypothesis {H0:
L(π)≥L(π0)}.
1. Split dataset D∝⇕⊣√∫⊔≀→Dtrain,Dvalid,Dtest
2. TrainπonDtrainand tune policy class and optimization hyper-parameters on Dvalid(for e.g by
internal cross-validation)
3. Estimate effective sample size neff(π)onDvalid
ifneff
n>νthen
Estimate ˆLSNIPS (π)onDtestand test ˆLSNIPS (π)<ˆL(π0)onDtestwith confidence 1−δ. If the test is
valid, reject H0, otherwise keep it.
else
KeepH0, consider the estimate to be invalid.
end
A first, structuring choice is the IPS estimator. While variants of IPS exist to reduce variance, such as clipped
IPS, we found Self-Normalized IPS (SNIPS, Swaminathan & Joachims, 2015b; Lefortier et al., 2016; Owen,
2013; Nedelec et al., 2017) to be more effective in practice. Indeed, it avoids the choice of a clipping threshold,
generally reduces variance and is equivariant with respect to translation of the reward.
A second component is the use of importance sampling diagnostics to prevent propensity over-fitting. Lefortier
et al. (2016) propose to check if the empirical average of importance weights deviates from 1. However, there
is no precise guideline based on this quantity to reject estimates. Instead, we recommend to use a diagnostic
on theeffective sample size neff=(/summationtextn
i=1wi)2//summationtextn
i=1w2
i, which measures how many samples are actually
usable to perform estimation of the counterfactual estimate; we follow Owen (2013), who recommends to
reject the estimate when the relative effective sample size neff/nis less than 1%. A third choice is a statistical
decision procedure to check if L(π)< L(π0). In theory, any statistical test against a null hypothesis H0:
L(π)≥L(π0)with confidence level 1−δcan be used.
Finally, we present our protocol in Algorithm 2. Since we cannot evaluate such a protocol on purely offline
data, we performed an empirical evaluation on synthetic setups where we could analytically design true
positive (L(π)< L(π0)) and true negative policies. We discuss in Section 7 the concrete parameters of
Algorithm 2 and their influence on false (non-)discovery rates in practice.
Model selection with the offline protocol In order to make realistic evaluations, hyperparameter
selection is always conducted by estimating the loss of a new policy πin a counterfactual manner. This
requires using a validation set (or cross-validation) with propensities obtained from the logging policy π0of
the training set. Such estimates are less accurate than online ones, which would require to gather new data
obtained from π, which we assume is not feasible in real-world scenarios.
To solve this issue, we have chosen to discard unreliable estimates that do not pass the effective sample size
test from Algorithm 2. When doing cross-validation, it implies discarding folds that do not pass the test, and
averaging estimates computed on the remaining folds. Although this induces a bias in the cross-validation
12Under review as submission to TMLR
procedure, we have found it to significantly reduce the variance and dramatically improve the quality of
model selection when the number of samples is small, especially for the Warfarin dataset in Section 7.
7 Experimental Setup and Evaluation
We now provide an empirical evaluation of the various aspects of CRM addressed in this paper such as
policy class modelling (CLP), estimation with soft-clipping, optimization with PPA, offline model selection
and evaluation. We conduct such a study on synthetic and semi-synthetic datasets and on the real-world
CoCoAdataset.
7.1 Experimental Validation of the Protocol
First, we study the ability of Algorithm 2 to accurately decide if a candidate policy πis better than a reference
logging policy π0(conditionL(π)≤L(π0)) on synthetic data. Here we simulate logging policy π0being a
lognormal distribution of known mean and variance, and an optimal policy π∗being a Gaussian distribution.
We generate a logged dataset by sampling actions a∼π0and trying to evaluate policies ˆπwith costs observed
under the logging policy. We compare the costs predicted using IPS and SNIPS offline metrics to the online
metric as the setup is synthetic, it is then easy to check that indeed they are better or worse than π0. We
compare the IPS and SNIPS estimates along with their level of confidences and the influence of the effective
sample size diagnostic.
Our first result is that SNIPS estimates are highly correlated to the true (online) reward (average correlation
ρ=.968, 30% higher than IPS, see plots in Appendix D.1). Then, we have shown that with a proper choice
of theνandδparameters, it is possible to control the False Discovery Rate (FDR) ( <10−4) and False
Non-Discovery Rate (FNDR) ( <5.10−4) on the same setup. We observed that on simple synthetic setups the
effective sample size criterion ν=neff/nis seldom necessary for policies close to the logging policy ( π≈π0).
For policies which were not close to the logging policy we found that standard statistical significance testing
at1−δlevel was by itself not enough to guarantee a low FDR which justified the use of ν. Adjusting the
effective sample size can therefore influence the performance of the protocol (see Appendix D.2.2 for detailed
results, D.3 for further illustrations of importance sampling diagnostics).
7.2 Experimental Evaluation of the Continuous Modelling and the Optimization Perspectives
In this section we introduce our empirical settings for evaluation and present our proposed CLP policy
parametrization, and the influence of optimization in counterfactural risk minimization problems.
7.2.1 Experimental Setup
We present the synthetic potential prediction setup, a semi-synthetic setup as well as our real-world setup.
Synthetic potentialprediction. We introducesimple synthetic environmentswith the following generative
process: an unobserved random group index ginGis drawn, which influences the drawing of a context xand
of an unobserved “potential” pinR, according to a joint conditional distribution PX,P|G. Intuitively, the
potentialpmay be compared to users a priori responsiveness to a treatment. The observed reward −yis
then a function of the context x, actiona, and potential p. The causal graph corresponding to this process is
given in Figure 3.
A YP X G
action context outcome group labelπ
Figure 3: Causal Graph of the synthetic setting. Adenotes action, Xcontext,Gunobserved group label, Y
outcome and Punobserved potentials. Unobserved elements are dotted.
13Under review as submission to TMLR
Then, we generate three datasets (“noisymoons, noisycircles, and anisotropic”, abbreviated respect. “moons,
circles, and GMM” in Table 2 and illustrated in Appendix F.1, with two-dimensional contexts on 2 or 3
groups and different choices of PX,P|G.
The goal is then to find a policy π(a|x)that maximizes reward by adapting to an unobserved potential. For
our experiments, potentials are normally distributed conditionally on the group index, p|g∼N(µg,σ2). As
many real-world applications feature a reward function that increases first with the action up to a peak and
finally drops, we have chosen a piecewise linear function peaked at a=p(see Appendix F.1, Figure 17),
that mimics reward over the CoCoAdataset presented in Section 6. In bidding applications, a potential may
represent an unknown true value for an advertisement, and the reward is then maximized when the bid
(action) matches this value. In medicine, increasing drug dosage may increase treatment effectiveness but if
dosage exceeds a threshold, secondary effects may appear and eclipse benefits (Barnes & Eltherington, 1966).
Semi-synthetic setting with medical data. We follow the setup of Kallus & Zhou (2018) using a dataset
on dosage of the Warfarin blood thinner drug (War, 2009). The dataset consists of covariates about patients
along with a dosage treatment prescription by a medical expert, which is a scalar value and thus makes the
setting useful for continuous action modelling. While the dataset is supervised, we simulate (see details in
Appendix F.1) a contextual bandit environment by using a hand-crafted reward function that is maximal for
actionsathat are within 10% of the expert’s therapeutic drug dosage, following Kallus & Zhou (2018).
Evaluation methodology For synthetic datasets, we generate training, validation, and test sets of size
10000 each. For the CoCoA dataset, we consider a 50%-25%-25% training-validation-test sets. We then
run each method with 5 different random intializations such that the initial policy is close to the logging
policy. Hyperparameters such as the regularization parameter λ, the number of anchor points m, etc. are
selected on a validation set with logged bandit feedback as explained in Algorithm 2. The variance σof
the policy classes that we consider are always set to the original variance of the logging policies. We use an
offline SNIPS estimate of the oat we consibtained policies, while discarding solutions deemed unsafe with
the importance sampling diagnostic. On the semi-synthetic Warfarin dataset we used a cross-validation
procedure to improve model selection due to the low dataset size. For estimating the final test performance
and confidence intervals on synthetic and on semi-synthetic datasets, we use an online estimate by leveraging
the known reward function and taking a Monte Carlo average with 100 action samples per context: this
accounts for the randomness of the policy itself over given fixed samples. For offline estimates we leverage
the randomness across samples to build confidence intervals: we use a 100-fold bootstrap and take percentiles
of the distribution of rewards. For the CoCoA dataset, we report SNIPS estimates for the test metrics.
7.2.2 Empirical Evaluation
We now evaluate our proposed CLP policy parametrization and the influence of optimization in counterfactural
risk minimization problems.
Continuous action space requires more than naive discretization. In Figure 4, we compare our
continuous parametrization to discretization strategies that bucketize the action space and consider stochastic
discrete-action policies on the resulting buckets, using IPS and SDM. We add a minimal amount of noise
to the deterministic DM in order to pass the neff/n > νvalidation criterion, and experimented different
hyperparameters and models which were selected with the offline evaluation procedure. On all synthetic
datasets, the CLP continuous modeling associated to the IPS perform significantly better than discrete
approaches (see also Appendix G.1), across all choices considered for the number of anchor points/buckets.
To achieve a reasonable performance, naive discretization strategies require a much finer grid, and are thus
also more computationally costly. The plots also show that our (stochastic) direct method strategy, where
we use the same parametrization is overall outperformed by the CLP parametrization combined to IPS,
highlighting a benefit of using counterfactual methods compared to a direct fit to observed rewards.
Counterfactual cost predictor (CLP) provides a competitive parameterization for continuous-
action policy learning. We compare our CLP modelling approach to other parameterized modelings
14Under review as submission to TMLR
0 10 20 30 40 50
Nb anchor points0.00.20.40.60.8Test reward
Test reward - dataset noisymoons
IPS Discrete
SDM Discrete
IPS Continuous
SDM Continuous
Figure 4: Continuous vs discretization strategies. Test rewards on NoisyMoons dataset with varying numbers
of anchor points for our continuous parametrization for IPS and SDM, versus naive discretization with
softmax policies. Note that few anchor points are sufficient to achieve good results on this dataset; this is not
the case for more complicated ones ( e.g., Warfarin requires at least 15 anchor points).
(constant, linear and non-linear described in Section 3.2) on our synthetic and semi-synthetic setups described
in Section 7.2.1 as well as the CoCoAdataset presented in Section 6.1.
Noisycircles NoisyMoons Anisotropic Warfarin CoCoA
Logging policy π0 0.5301 0.5301 0.4533 -13.377 11.34
scIPSConstant 0.6115±0.0000 0.6116±0.0000 0.6026±0.0000−8.964±0.001 11.36±0.13
Linear 0.6113±0.0001 0.7326±0.0001 0.7638±0.0005−12.857±0.002 11.35±0.02
Poly 0.6959±0.0001 0.7281±0.0001 0.7448±0.0008 - 10.36±0.11
CLP0.7674±0.00080.7805±0.00040.7703±0.0002-8.720±0.00111.44∗±0.10
SNIPSConstant 0.6115±0.0001 0.6115±0.0001 0.5930±0.0001−9.511±0.001 11.32±0.13
Linear 0.6115±0.0001 0.7360±0.0001 0.7103±0.0003−10.583±0.005 10.34±0.12
Poly 0.6969±0.0001 0.7370±0.0001 0.5801±0.0002 - 11.13±0.08
CLP0.6972±0.00010.74091±0.00040.7899±0.0002-9.161±0.00111.48∗±0.14
Table 2: Test rewards (higher the better) for several contextual modellings (see main text for details).
In Table 2, we show a comparison of test rewards for different contextual modellings (associated to different
parametric policy classes). We show the performance and the associated variance of the best policy obtained
with the offline model selection procedure (Section 6.2). Specifically, we consider a grid of hyperparameters
and optimized the associated CRM problem with the PPA algorithm (Section 4.2). We report here the
performances of scIPS and SNIPS estimators. For the Warfarin dataset, following Kallus & Zhou (2018),
we only consider the linear context parametrization baseline, since the dataset has categorical features
and higher-dimensional contexts. Overall, we find our CLP parameterization to improve over all other
contextual modellings, which highlights the effectiveness of the cost predictor at exploiting the continuous
action structure. As all the methods here have the same sample efficiency, the superior performance of our
method can be imputed to the richer policy class we use and which better models the dependency of contexts
and actions that may reduce the approximation error. We can also draw another conclusion: unlike synthetic
setups, it is harder to obtain policies that beat the logging policy with large statistical significance on the
CoCoAdataset where the logging policy already makes a satisfactory baseline for real-world deployment. Only
CLP passes the significance test on this dataset. This corroborates the need for offline evaluation procedures,
which were absent from previous works.
Soft-clipping improves performance of the counterfactual policy learning. Figure 5 shows the
improvements in test reward of our optimization-driven strategies for the soft-clipping estimator for the
synthetic datasets (see also Appendix G.3). The points correspond to different choices of the clipping
parameterM, models and initialization, with the rest of the hyper-parameters optimized on the validation
set using the offline evaluation protocol. This plot also shows that soft clipping provides benefits over hard
15Under review as submission to TMLR
clipping, perhaps thanks to a more favorable optimization landscape. Overall, these figures confirm that the
optimization perspective is important when considering CRM problems.
0.600 0.625 0.650 0.675 0.700 0.725 0.750 0.775 0.800
Classic clipping0.6000.6250.6500.6750.7000.7250.7500.7750.800Soft clipping
NoisyMoons Dataset
Modelling
clp
linear
piecewise-constant
non-linearClipping M
1.0
1.7
2.8
4.6
7.7
12.9
21.5
35.9
59.9
100.0
0.60 0.62 0.64 0.66 0.68 0.70
Classic clipping0.600.620.640.660.680.70Soft clipping
NoisyCircles Dataset
Modelling
clp
linear
piecewise-constant
non-linearClipping M
1.0
1.7
2.8
4.6
7.7
12.9
21.5
35.9
59.9
100.0
0.4 0.5 0.6 0.7 0.8
Classic clipping0.40.50.60.70.8Soft clipping
Anisotropic Dataset
Modelling
clp
linear
piecewise-constant
non-linearClipping M
1.0
1.7
2.8
4.6
7.7
12.9
21.5
35.9
59.9
100.0
Figure 5: Influence of soft-clipping. Relative improvements in the test performance for soft- vs hard-clipping
on synthetic datasets. The points correspond to different choices of the clipping parameter, models and
initialization.
In Appendix G.2, we also illustrate how soft-clipping improves or competes with other clipping strategies
(Metelli et al., 2021; Wang et al., 2017; Su et al., 2019).
Proximal point algorithm (PPA) influences optimization of non-convex CRM objective functions
and policy learning performance. We illustrate in Figure 6 the improvements in test reward and in
training objective of our optimization-driven strategies with the use of the proximal point algorithm (see also
Appendix G.3). Here, each point compares the test metric for fixed models as well as initialization seeds,
while optimizing the remaining hyperparameters on the validation set with the offline evaluation protocol.
Figure 6 (left) illustrates the benefits of the proximal point method when optimizing the (non-convex) CRM
objective in a wide range of hyperparameter configurations, while Figure 6 (center) shows that in many cases
this improves the test reward as well. In our experiments, we have chosen L-BFGS because it was performing
best among the solvers we tried (nonlinear conjugate gradient (CG) and Newton) and used 10 PPA iterations.
As for computational time, for 10 PPA iterations, the computational overhead was about 3×in comparison
to L-BFGS without PPA. This overhead seems to be the price to pay to improve the test reward and obtain
better local optima. Overall, these figures confirm that the proximal point algorithm improves performance
in CRM optimization problems.
2.0
 1.5
 1.0
 0.5
 0.0 0.5
Losses0.00.51.01.52.02.53.03.5
Better local minima 79%NoisyMoons - Histogram PROX NONPROX
|NONPROX|
0.50 0.55 0.60 0.65 0.70 0.75 0.80
Base optimizer0.500.550.600.650.700.750.80Proximal Point Algorithm
NoisyMoons Dataset
Modelling
clp
linear
piecewise-constant
non-linearEstimator
SNIPS
DR
IPS
cIPS
scIPS
Figure 6: Influence of proximal point optimization. Relative improvements in the training objective w and
w/o using the proximal point method (left) and relative improvements in the test performance w and w/o
using the proximal point method (right).
The scIPS estimator along with CLP parametrization and PPA optimization improves upon
previous state of the art methods. We also provide a baseline comparison to stochastic direct methods,
to (O-learn) of Chen et al. (2016) using their surrogate loss formulation for continous actions, to kernel
smoothing (KS) (Kallus & Zhou, 2018) who propose a counterfactual method using kernel density estimation.
16Under review as submission to TMLR
Their approach is based on an automatic kernel bandwidth selection procedure which did not perform well
on our datasets except Warfarin; instead, we select the best bandwidth on a grid through cross-validation
and selecting it through our offline protocol. We also investigate their self-normalized (SN) variant, which
is presented in their paper but not used in their experiments; it turned out to have lower performances in
practice. Moreover, we experimented using the generic doubly robust method from Demirer et al. (2019) but
could not reach satisfactory results using the parameters and feature maps that were used in their empirical
section and with the specific closed form estimators for their applications. Nevertheless, by adapting their
method with more elaborated models and feature maps, we managed to obtain performances beating the
logging policy; these modifications would make promising directions for future research venues. We eventually
also compare to (CATS) (Majzoubi et al., 2020) who propose an offline variant of their contextual bandits
algorithm for continuous actions. We used the code of the authors and obtained poor performances with
their offline variant. We do not provide results on their method for the CoCoAdataset as we could not access
the logged propensities to use the offline evaluation protocol. For the SDM on CoCoA, we did not manage to
simultaneously pass the ESS diagnostic and achieve statistical significance, probably due to the noise and
variance of the dataset.
Noisycircles NoisyMoons Anisotropic Warfarin CoCoA
Stochastic Direct Method 0.6205±0.0004 0.7225±0.0006 0.6383±0.0003−9.714±0.013 -
O-learn 0.608±0.0002 0.645±0.0003 0.754±0.0002−9.407±0.004 11.03±0.15
KS 0.612±0.0001 0.734±0.0001 0.785±0.0002−10.19∗11.38±0.07
SN-KS 0.609±0.0001 0.595±0.0001 0.652±0.0001−12.569±0.001 9.14±0.94
CATS offline 0.589±0.0011 0.592±0.0011 0.569±0.0012−12.236±0.2548 -
Ours 0.767±0.00080.781±0.00040.770±0.0002-8.720±0.00111.44∗±0.10
Table 3: Test rewards (higher the better) for previous methods for the logged bandit problem with continuous
actions
8 Conclusion and Discussion
In this paper, we address the problem of counterfactual learning of stochastic policies on real data with
continuous actions. This raises several challenges about different steps of the CRM pipeline such as (i)
modelization, (ii) optimization, and (iii) evaluation. First, we propose a new parametrization based on a
joint kernel embedding of contexts and actions, showing competitive performance. Second, we underline
the importance of optimization in CRM formulations with soft-clipping and proximal point methods. We
provide statistical guarantees of our estimator and the policy class we introduced. Third, we propose an
offline evaluation protocol and a new large-scale dataset, which, to the best of our knowledge, is the first
with real-world logged propensities and continuous actions. For future research directions, we would like to
investigate the doubly robust estimator with the CLP parametrization of stochastic policies with continuous
actions: this estimator achieves the best results in the discrete action case but requires further techniques to
be adapted to continuous actions.
References
Estimation of the Warfarin dose with clinical and pharmacogenetic data. New England Journal of Medicine ,
360(8):753–764, 2009.
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the
monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine
Learning (ICML) , 2014.
Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact of
entropy on policy optimization. In International Conference on Machine Learning (ICML) , 2019.
Charles Dee Barnes and Lorne George Eltherington. Drug dosage in laboratory animals: a handbook . Univ of
California Press, 1966.
17Under review as submission to TMLR
Dimitris Bertsimas and Christopher McCord. Optimization over continuous and multi-dimensional decisions
with observational data. In Advances in Neural Information Processing Systems (NeurIPS) , 2018.
Christopher Bockel-Rickermann, Toon Vanderschueren, Jeroen Berrevoets, Tim Verdonck, and
Wouter Verbeke. Learning continuous-valued treatment effects through representation balancing.
https://arxiv.org/abs/2309.03731 , 2023.
Léon Bottou, Jonas Peters, Joaquin Quiñonero Candela, Denis X. Charles, D. Max Chickering, Elon Portugaly,
Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The
example of computational advertising. Journal of Machine Learning Research (JMLR) , 14(1):3207–3260,
2013.
Guanhua Chen, Donglin Zeng, and Michael R. Kosorok. Personalized dose finding using outcome weighted
learning. Journal of the American Statistical Association , 111(516):1509–1521, 2016.
KyleColangeloandYing-YingLee. Doubledebiasedmachinelearningnonparametricinferencewithcontinuous
treatments. https://arxiv.org/abs/2004.03036 , 2023.
Mert Demirer, Vasilis Syrgkanis, Greg Lewis, and Victor Chernozhukov. Semi-parametric efficient policy
learning with continuous actions. In Advances in Neural Information Processing Systems (NeurIPS) , 2019.
Miroslav Dudik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. In International
Conference on Machine Learning (ICML) , 2011.
Masao Fukushima and Hisashi Mine. A generalized proximal point algorithm for certain non-convex
minimization problems. International Journal of Systems Science , 12(8):989–1000, 1981.
Keisuke Hirano and Guido W Imbens. The propensity score with continuous treatments. Applied Bayesian
modeling and causal inference from incomplete-data perspectives , 226164:73–84, 2004.
D. G. Horvitz and D. J. Thompson. A generalization of sampling without replacement from a finite universe.
Journal of the American Statistical Association , 47(260):663–685, 1952.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In International
Conference on Machine Learning (ICML) , 2016.
Thorsten Joachims, Adith Swaminathan, and Maarten de Rijke. Deep learning with logged bandit feedback.
InInternational Conference on Learning Representations (ICLR) , 2018.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Interna-
tional Conference on Machine Learning (ICML) , 2002.
Nathan Kallus and Angela Zhou. Policy evaluation and optimization with continuous treatments. In
International Conference on Artificial Intelligence and Statistics (AISTATS) , 2018.
Andreas Krause and Cheng S Ong. Contextual gaussian process bandit optimization. In Advances in Neural
Information Processing Systems (NIPS) , 2011.
John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information.
InAdvances in Neural Information Processing Systems (NIPS) , 2008.
Haanvid Lee, Jongmin Lee, Yunseon Choi, Wonseok Jeon, Byung-Jun Lee, Yung-Kyun Noh, and Kee-Eung
Kim. Local metric learning for off-policy evaluation in contextual bandits with continuous actions. In
Thirty-Sixth Conference on Neural Information Processing Systems , 2022.
Damien Lefortier, Adith Swaminathan, Xiaotao Gu, Thorsten Joachims, and Maarten de Rijke. Large-scale
validation of counterfactual learning methods: A test-bed. arXiv preprint arXiv:1612.00367 , 2016.
Lihong Li, Wei Chu, John Langford, Taesup Moon, and Xuanhui Wang. An unbiased offline evaluation of
contextual bandit algorithms with generalized linear models. In Proceedings of the Workshop on On-line
Trading of Exploration and Exploitation 2 , 2012.
18Under review as submission to TMLR
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical
programming , 45(1-3):503–528, 1989.
Maryam Majzoubi, Chicheng Zhang, Rajan Chari, Akshay Krishnamurthy, John Langford, and Aleksandrs
Slivkins. Efficient contextual bandits with continuous actions. In Advances in Neural Information Processing
Systems (NeurIPS) , 2020.
Andreas Maurer and Massimiliano Pontil. Empirical bernstein bounds and sample variance penalization. In
Conference on Learning Theory (COLT) , 2009.
Alberto Maria Metelli, Alessio Russo, and Marcello Restelli. Subgaussian and differentiable importance
sampling for off-policy evaluation and learning. In Advances in Neural Information Processing Systems
(NeurIPS) , 2021.
Thomas Nedelec, Nicolas Le Roux, and Vianney Perchet. A comparative study of counterfactual estimators.
arXiv preprint arXiv:1704.00773 , 2017.
Art Owen. Monte Carlo theory, methods and examples . 2013.
Courtney Paquette, Hongzhou Lin, Dmitriy Drusvyatskiy, Julien Mairal, and Zaid Harchaoui. Catalyst for
gradient-based nonconvex optimization. In International Conference on Artificial Intelligence and Statistics
(AISTATS) , 2018.
James Robins and Andrea Rotnitzky. Semiparametric efficiency in multivariate regression models with missing
data.Journal of the American Statistical Association , 90(429):122–129, 1995.
Ralph Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal on control
and optimization , 14(5):877–898, 1976.
Noveen Sachdeva, Lequn Wang, Dawen Liang, Nathan Kallus, and Julian McAuley. Off-policy evaluation for
large action spaces via policy convolution, 2023.
Yuta Saito, Qingyang Ren, and Thorsten Joachims. Off-policy evaluation for large action spaces via conjunct
effect modeling. In International Conference on Machine Learning (ICML) , 2023.
Bernhard Schölkopf and Alexander Smola. Learning with kernels: support vector machines, regularization,
optimization, and beyond . MIT press, 2002.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347 , 2017.
Yi Su, Lequn Wang, Michele Santacatterina, and Thorsten Joachims. Cab: Continuous adaptive blending for
policy evaluation and learning. In International Conference on Machine Learning (ICML) , 2019.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Advances in Neural Information Processing
Systems (NIPS) , 2000.
Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged bandit
feedback. In International Conference on Machine Learning (ICML) , 2015a.
Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual learning. In
Advances in Neural Information Processing Systems (NIPS) . 2015b.
Yu-Xiang Wang, Alekh Agarwal, and Miro Dudík. Optimal and adaptive off-policy evaluation in contextual
bandits. In International Conference on Machine Learning (ICML) , 2017.
Christopher KI Williams and Matthias Seeger. Using the nyström method to speed up kernel machines. Adv.
Neural Information Processing Systems (NIPS) , 2001.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Machine learning , 8(3-4):229–256, 1992.
19Under review as submission to TMLR
This appendix is organized as follows: in Appendix A, we present a review of off-policy estimators, then in
Appendix ??, we present discussions and toy experiments to motivate the need for clipping strategies on real
datasets. Next, we provide motivations in Appendix D for the offline evaluation protocol with experiments
justifying the need for appropriate diagnostics and statistical testing for importance sampling. Next, we
provide the details of our analysis and the proofs of Propositions 4.1, 5.1 in Appendix E. Then, Appendix F
is devoted to experimental details that were omitted from the main paper for space limitation reasons, and
which are important for reproducing our results (see also the code provided with the submission). Finally, in
Appendix G, we present additional experimental results to those in the main paper.
Note that the CoCoA dataset is available on an anonymous server here https://drive.google.com/open?
id=1GWcYFYNqx-TSvx1bbcbuunOdMrLe2733 .
A Review of Off-policy Estimators
Clipped estimator. In Equation (3), the counterfactual approach tackles the distribution mismatch between
the logging policy π0(·|x)and the evaluated policy πinΠvia importance sampling and uses inverse propensity
scoring (IPS, Horvitz & Thompson (1952)). However, the empirical (IPS) estimator has large variance and
may overfit negative feedback values yifor samples that are unlikely under π0(see motivation for clipped
estimators in B.1), resulting in higher variances. Clipping the importance sampling weights in Eq. (16)
as Bottou et al. (2013) mitigates this problem, leading to a clipped (cIPS) estimator
ˆLcIPS(π) =1
nn/summationdisplay
i=1yimin/braceleftbiggπ(ai|xi)
π0,i,M/bracerightbigg
. (16)
Smaller values of Mreduce the variance of ˆL(π)but induce a larger bias. Swaminathan & Joachims (2015a)
also propose adding an empirical variance penalty term controlled by a factor λto the empirical risk ˆL(π).
Specifically, they write νi(π) =yimin/parenleftig
π(ai|xi)
π0(ai|xi),M/parenrightig
and consider the empirical variance for regularization:
ˆVcIPS(π) =1
n−1n/summationdisplay
i=1(νi(π)−¯ν(π))2,with ¯ν(π) =1
nn/summationdisplay
i=1¯νi(π), (17)
which is subsequently used to obtain a regularized objective Lwith hyperparameters Mfor clipping and λ
for variance penalization, respectively, so that ΩcIPS(π) =λ/radicalig
ˆVcIPS(π)
nand:
L(π) =ˆLcIPS(π) +λ/radicaligg
ˆVcIPS(π)
n. (18)
The self-normalized estimator. Swaminathan & Joachims (2015b) also introduce a regularization mechanism
for tackling the so-called propensity overfitting issue, occuring with rich policy classes, where the method
would focus only on maximizing (resp. minimizing) the sum of ratios π(ai|xi)/π0,ifor negative (resp. positive)
costs. This effect is corrected through the following self-normalized importance sampling (SNIPS) estimator
(Owen, 2013, see also), which is equivariant to additive shifts in cost values:
ˆLSNIPS (π) =/summationtextn
i=1yiwπ
i/summationtextn
i=1wπ
i,withwπ
i=π(ai|xi)
π0,i. (19)
The SNIPS estimator is also associated to ΩSNIPS (π) =λ/radicalig
ˆVSNIPS (π)
nwhich uses an empirical variance
estimator that writes as:
ˆVSNIPS (π) =/summationtextn
i=1/parenleftig
wπ
i/parenleftig
yi−ˆLSNIPS (π)/parenrightig/parenrightig2
(/summationtextn
i=1wπ
i)2. (20)
20Under review as submission to TMLR
Direct methods. Such direct methods (DM) fit the loss values over contexts and actions in observed data
with an estimator ˆη(x,a), for instance by using ridge regression to fit yi≈ˆη(xi,ai), and to then use the
deterministic greedy policy ˆπDM(x) =arg minaˆη(x,a). These may suffer from large bias since it focuses on
estimating losses mainly near actions that appear in the logged data but have the benefit of avoiding the
high-variance problems of IPS-based methods. While such greedy deterministic policies may be sufficient
for exploitation, stochastic policies may be needed in some situations, for instance when one wants to still
encourage some exploration in a future round of data logs. Using a stochastic policy also allows us to obtain
more accurate off-policy estimates when performing cross-validation on logged data. Then, it may be possible
to define a stochastic version of the direct method by adding Gaussian noise with variance σ2:
ˆπSDM(·|x) =N(ˆπDM(x),σ2), (21)
In the context of offline evaluation on bandit data, such a smoothing procedure may also be seen as a form of
kernel smoothing for better estimation (Kallus & Zhou, 2018).
Doubly robust estimators. Additionally, such direct loss estimators can be effective when few samples are
available, and may be combined with IPS estimators in the so-called doubly robust estimator (DR, see,
e.g. Dudik et al. (2011)). This approach consists of correcting the bias of the DM estimator by applying IPS
to the residuals yi−ˆη(xi,ai), thus using ˆηas a control variate to decrease the variance of IPS. For discrete
actions, the DR estimator takes the form
ˆLDR(π) =1
nn/summationdisplay
i=1π(ai|xi)
π0,i(yi−ˆη(xi,ai)) +1
nn/summationdisplay
i=1/summationdisplay
a∈Aπ(a|xi)ˆη(xi,a).
We provide a more in depth discussion on a doubly robust adaptation of our estimator in Appendix G.4.
B Motivation for Clipped Estimators
In this section we provide an illustration of the logarithmic soft clipping and a motivation example for clipping
strategies in counterfactual systems.
B.1 A Toy Example for Soft Clipping
Here we provide a motivation example for clipping strategies in counterfactual systems in a toy example. In
Figure 7 we provide an example of large variance and loss overfitting problem.
We recall the data generation: a hidden group label ginGis drawn, and influences the associated context
distribution xand of an unobserved potential pinR, according to a joint conditional distribution PX,P|G
The observed reward ris then a function of the context x, actiona, and potential p. Here, we design one
outlier (big red dark dot on Figure 7 left). This point has a noisy reward r, higher than neighbors, and a
potentialphigh as its neighbors have a low potential. We artificially added a noise in the reward function f
that can be written as:
r=f(a,x,p ) +ε, ε∼N(0,1)
As explained in Section 7.2.1, the reward function is a linear function, with its maximum localized at the
pointx=p(x), i.e. at the potential sampled. The observability of the potential pis only through this reward
functionf. Hereafter, we compare the optimal policy computed, using different types of estimators.
The task is to predict the high potentials (red circles) and low potentials (blue circles) in the ground truth
data (left). Unfortunately, a rare event sample with high potential is put in the low potential cluster (big
dark red dot). The action taken by the logging policy is low while the reward is high: this sample is an
outlier because it has a high reward while being a high potential that has been predicted with a low action.
The resulting unclipped estimator is biased and overfits this high reward/low propensity sample. The rewards
of the points around this outlier are low as the diameter of the points in the middle figure show. Inversely,
clipped estimator with soft-clipping succeeds to learn the potential distributions, does not overfit the outlier,
and has larger rewards than the clipping policies as the diameter of the points show.
21Under review as submission to TMLR
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Ground truth data
pP|X
Low
Medium
HighrX|P
Low
High
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Actions sampled from unclipped estimator
aA|X
Low
Medium
HighrX|P
Low
High
0.0 0.2 0.4 0.6 0.8 1.0
x10.00.20.40.60.81.0x2Actions sampled from soft clipped estimator
aA|X
Low
Medium
HighrX|P
Low
High
Figure 7: High variance and loss overfitting. Unlikely ( π0,i≈0) sample (x1,x2) = (0.6,0.)with high reward
r(left) results in larger variance and loss overfitting for the unclipped estimator (middle) unlike clipped
estimator (right).
C Motivation for Counterfactual Methods
Direct methods (DM) learns a reward/cost predictor over the joint context-action space X×Abut ignore
the potential mismatch between the evaluated policy and the logging policy and π0. When the logged data
does not cover the joint context-action space X×Asufficiently, direct methods rather fit the region where
the data has been sampled and may therefore lead to overfitting (Bottou et al., 2013; Dudik et al., 2011;
Swaminathan & Joachims, 2015b). Counterfactual methods instead learn probability distributions directly
with a re-weighting procedure which allow them to fit the context-action space even with fewer samples.
In this toy setting we aim to illustrate this phenomenon for the DM and the counterfactual method. We
create a synthetic ’Chess’ environment of uni-dimensional contexts and actions where the logging policy
purposely covers only a small area of the action space, as illustrated in Fig. 8. The reward function is either
0, 0.5 or 1 in some areas which follow a chess pattern. We use a lognormal logging policy which is peaked in
low action values but still has a common support with the policies we optimize using the CRM or the DM.
0.0 0.2 0.4 0.6 0.8
Context0.00.20.40.60.8ActionChessboard
0.00.20.40.60.81.0
0.0 0.2 0.4 0.6 0.8 1.0
Actions012345Logging PDF
Figure 8: ’Chess’ toy synthetic setting (left) and lognormal logging policy (right).
Having set that environment and the logging policy, we illustrate in Fig. 9 the logging dataset, the actions
sampled by the policy learned by a Direct Method and eventually the actions sampled by a counterfactual
IPS estimator. To assess a fair comparison between the two methods, we use the same continuous action
modelling with the same parameters (CLP parametrization with m= 5anchor points).
This toy example illustrate the mentioned phenomenon in how the counterfactual estimator learns a re-
balanced distribution that maps the contexts to the actions generating higher rewards than the DM. The
latter only learns a mapping that is close to the actions sampled by the logging and only covers a smaller set
of actions.
22Under review as submission to TMLR
Figure 9: Logged data (left), action sampled by direct method (middle) and action sampled by counterfactual
policy.
D Motivation for the Offline Evaluation Protocol
In this part we demonstrate the offline/online correlation of the estimator we use for real-world systems and
for validation of our methods even in synthetic and semi-synthetic setups. We provide further explanations of
the necessity of importance sampling diagnostics and we perform experiments to empirically assess the rate
of false discoveries of our protocol.
D.1 Correlation of Self-Normalized Importance Sampling with Online Rewards
We show in Figures 10,12,11 comparisons of IPS and SNIPS against an on-policy estimate of the reward
for policies obtained from our experiments for linear and non-linear contextual modellings on the synthetic
datasets, where policies can be directly evaluated online. Each point represents an experiment for a model
and a hyperparameter combination. We measure the R2score to assess the quality of the estimation, and
find that the SNIPS estimator is indeed more robust and gives a better fit to the on-policy estimate. Note
also that overall the IPS estimates illustrate severe variance compared to their SNIPS estimate. While SNIPS
indeed reduces the variance of the estimate, the bias it introduces does not deteriorate too much its (positive)
correlation with the online evaluation.
These figures further justify the choice of the self-normalized estimator SNIPS (Swaminathan & Joachims,
2015b) for offline evaluation and validation to estimate the reward on held-out logged bandit data. The
SNIPS estimator is indeed more robust to the reward distribution thanks to its equivariance property to
additive shifts and does not require hyperparameter tuning.
Figure 10: Correlation between offline and online estimates on Anisotropic synthetic data . Linear
(left) and non-linear (right) contextual modellings. Ideal fit would be y=x.
D.2 Experimental validation of the protocol
We empirically evaluate our offline evaluation protocol on a toy setting where we simulate a logging policy
π0being a lognormal distribution of known mean and variance, and an optimal policy π∗being a Gaussian
distribution. We generate a logged dataset by sampling actions a∼π0and trying to evaluate policies ˆπwith
23Under review as submission to TMLR
Figure 11: Correlation between offline and online estimates on NoisyCircles synthetic data . Linear
(left) and non-linear (right) contextual modellings. Ideal fit would be y=x.
Figure 12: Correlation between offline and online estimates on NoisyMoons synthetic data . Linear
(left) and non-linear (right) contextual modellings. Ideal fit would be y=x.
costs observed under the logging policy. We compare the costs predicted using IPS and SNIPS offline metrics
to the online metric as the setup is synthetic, it is then easy to check that indeed they are better or worse
thanπ0. We compare the IPS and SNIPS estimates along with their level of confidences and the influence of
the effective sample size diagnostic. Offline evaluations of policies ˆπillustrated in Figure 13 are estimated
from logged data (xi,ai,yi,π0)i=1...nwhereai∼π0(·|xi)and where the policy risk would be optimal under
the oracle policy π∗.
0 2 4 6 8 10
Actions sampled0.00.10.20.30.40.50.60.70.8PDFLogging policy 0
Optimal policy *
Evaluated policy 
Figure 13: Illustration of policies: logging policy π0, optimalπ∗and example policy ˆπ.
While the goal of counterfactual learning is to find a policy ˆπwhich is as close a possible to the optimal
policyπ∗, based on samples drawn from a logging policy π0, it is in practice hard to assess the statistical
significance of a policy that is too "far" from the logging policy. Offline importance sampling estimates are
indeed limited when the distribution mismatch between the evaluated policy and the logging policy (in terms
of KL divergence DKL(π0||ˆπ)) is large. Therefore we create a setup where we evaluate the quality of offline
estimates for policies (i) "close" to the logging policy (meaning the KL divergence DKL(π0||ˆπ)is low) and
24Under review as submission to TMLR
(ii) "close" to the oracle optimal policy (meaning the KL divergence DKL(π∗||ˆπ)is low). In this experiment,
we focus on evaluating the ability of the offline protocol to correctly assess whether L(ˆπ)≤L(π0)or not by
comparing to online truth estimates. Specifically, for both setups (i) and (ii), we compare the number of
False Positives (FP) and False Negatives (FN) of the two offline protocols for N= 2000initializations, by
adding Gaussian noise to the parameters of the closed form policies. False negatives are generated when
the offline protocol keeps H0:L(π)≥L(π0)while the online evaluation reveals that L(π)≤L(π0), while
false positives are generated in the opposite case when the protocol rejects H0while it is true. We also
show histograms of the differences between online and offline boundary decisions for (L(ˆπ)<L(π0)), using
bootstrapped distribution of SNIPS estimates to build confidence intervals.
D.2.1 Validation of the use of SNIPS estimates for the offline protocol
To assess the performance of our evaluation protocol, we first compare the use of IPS and SNIPS estimates
for the offline evaluation protocol and discard solutions with low importance sampling diagnosticsneff
n<ν
with the recommended value ν= 0.01from Owen (2013). In Table 4, we provide an analysis of false positives
and false negatives in both setups. We first observe that for setup (i) the SNIPS estimates has both fewer
false positives and false negatives. Note that is setup is probably more realistic for real-world applications
where we want to ensure incremental gains over the logging policy. In setup (ii) where importance sampling
is more likely to fail when the evaluated policy is too "far" from the logging policy, we observe that the SNIPS
estimate has a drastically lower number of false negatives than the IPS estimate, though it slightly has more
false positives, thus illustrating how conservative this estimator is.
Offline ProtocolSetup (i) Setup (ii)
IPS SNIPS IPS SNIPS
ˆπ⪰π0KeepH0ˆπ⪰π0KeepH0ˆπ⪰π0KeepH0ˆπ⪰π0KeepH0
“Truth”ˆπ⪰π01282 24 1296 10 1565 67 1631 1
KeepH019 675 0 694 0 368 6 362
Table 4: Comparison of false positives and false negatives: Perturbation to the logging policy π0(setup (i))
and perturbation to the optimal policy (setup (ii)). The SNIPS estimator yields less FN and FP on setup (i),
while being more effective on setup (ii) as well by inducing a drastically lower FP rate than IPS and a low
FN rate. The effective sample size threshold is fixed at ν= 0.01
We then provide in Fig. 14 histograms of the differences of the upper boundary decisions between online
estimates and bootstrapped offline estimates over all samples for both setups (i, left) and (ii, right). Both
histograms illustrate how the IPS estimate underestimates the value of the reward with regard to the online
estimate, unlike the SNIPS estimates. In the setup (ii) in particular, the IPS estimate underestimates severely
the reward, which may explain why IPS has lower number of false positives when the evaluated policy is
far from the logging policy. However in both setups, IPS has a higher number of false negatives. We also
observed that our SNIPS estimates were highly correlated to the true (online) reward (average correlation
ξ=.968, 30% higher than IPS, see plots in Appendix D.1) for the synthetic setups presented in section 7.2.1,
which therefore confirms our findings.
D.2.2 Influence of the effective sample size criteria in the evaluation protocol
In this setup we vary the effective sample size (ESS) threshold and show in Fig. 15 how it influences the
performance of the offline evaluation protocol for the two previously discussed setups where we consider
evaluations of (i) perturbations of the logging policy (left) and (ii) perturbations of the optimal policy (right)
in our synthetic setup. We compute precision, recall and F1 scores for each threshold values between 0 and
1. One can see that for low threshold values where no policies are filtered, precision, recall and F1 scores
remain unchanged. Once the ESS raises above a certain threshold, undesirable policies start being filtered
but more false negatives are created when the ESS is too high. Overall, ESS criterion is relevant for both
setups. However, we observe that on simple synthetic setups the effective sample size criterion ν=neff/n
is seldom necessary for policies close to the logging policy ( π≈π0). Conversely, for policies which are not
close to the logging policy the standard statistical significance testing at 1−δlevel was by itself not enough
25Under review as submission to TMLR
Figure 14: Histogram of differences between online reward and offline lower confidence bound.
Perturbation to the logging policy π0(left), perturbation to the optimal policy π∗(right). Effective sample
size threshold ν= 0.01
to guarantee a low false discovery rate (FDR) which justified the use of ν. Adjusting the effective sample
size can therefore influence the performance of the protocol (see Appendix D.3 for further illustrations of
importance sampling diagnostics in what-if simulations).
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
ESS threshold0.60.70.80.91.0PerformanceEffective Sample Size influence on 0
Precision
Recall
F1
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
ESS threshold0.60.70.80.91.0PerformanceEffective Sample Size influence on 0
Precision
Recall
F1
Figure 15: Precision, recall and F1 score varying with the ESS threshold on synthetic setups (i)
and (ii). Setup (i) perturbation of the logging policy (left) and setup (ii) perturbation to the optimal policy
(right). The ESS threshold can maximize the F1 score.
D.3 Importance Sampling Diagnostics in What-If simulations
Importance sampling estimators rely on weighted observations to address the distribution mismatch for offline
evaluation, which may cause large variance of the estimator. Notably, when the evaluated policy differs too
much from the logging policy, many importance weights are large and the estimator is inaccurate. We provide
here a motivating example to illustrate the effect of importance sampling diagnostics in a simple scenario.
When evaluating with SNIPS, we consider an “effective sample size” quantity given in terms of the importance
weightswi=π(ai|xi)/π0(ai|xi)byne= (/summationtextn
i=1wi)2//summationtextn
i=1w2
i. When this quantity is much smaller than
the sample size n, this indicates that only few of the examples contribute to the estimate, so that the obtained
value is likely a poor estimate. Apart from that, we note also that IPS weights have an expectation of 1 when
summed over the logging policy distribution (that is E(x,a)∼π0[π(a|x)/π0(a|x)]= 1.). Therefore, another
sanity check, which is valid for any estimator, is to look for the empirical mean 1/n/summationtextn
i=1π(ai|xi)/π0,iand
compare its deviation to 1. In the example below, we illustrate three diagnostics: (i) the one based on effective
sample size described in Section 6; (ii) confidence intervals, and (iii) empirical mean of IPS weights. The
three of them coincide and allow us to remove test estimates when the diagnostics fail.
26Under review as submission to TMLR
Example D.1. What-if simulation: For xinRd, letmax(x) =max 1≤j≤dxj; we wish to estimate E(max(X))
forXi.i.d∼πµ=N(µ,σ)where samples are drawn from a logging policy π0=logN(λ0,σ0)(d=
3,(λ0,σ0) = (1,1/2)) and analyze parameters µaround the mode of the logging policy µπ0with fixed variance
σ= 1/2. In this parametrized policy example, we see in Fig. 16 that ne/n≪1, confidence interval range
increases and/summationtextn
i=1π(ai|xi)
π0,i̸= 1when the parameter µof the policy being evaluated is far away from the
logging policy mode µλ0.
0 1 2 3 4 5
104
103
102
101
100
Effective sample size ne/n
Threshold
0 mode
n*
e/n
0 1 2 3 4 5
0246
Estimation m and 95% C.I.s
0 mode
m
0 1 2 3 4 5
0.00.20.40.60.81.0
Importance sampling weights mean 1
nwi
Theoretical mean
0 mode
Empirical mean
Figure 16: Importance sampling diagnostics . Ideal importance sampling: i) effective sample ne/nclose
to1, ii) low confidence intervals (C.I.s) for ˆm, iii) empirical mean1
n/summationtext
iwiclose to 1. Note that when µ
differs too much from µπ0, importance sampling fails.
Note that in this example, the parameterized distribution that is learned (multivariate Gaussian) is not
the same as the parameterized distribution of the logging policy (multivariate Lognormal) which skewness
may explain the asymmetry of the plots. This points out another practical problem: even though different
parametrization of policies is theoretically possible, the probability density masses overlap is in practice
what is most important to ensure successful importance sampling. This observation is of utmost interest for
real-life applications where the initialization of a policy to be learned needs to be "close" to the logging policy;
otherwise importance sampling may fail from the very first iteration of an optimization in learning problems.
E Analysis of the Excess Risk
In this appendix, we provide details and proofs on the excess risk guarantees that are given in Section 4 and 5.
We start by recalling the definitions of an εcovering and the one of our soft-clipping operator ζprovided in
Eq. (11).
Definition E.1 (epsilon covering and metric entropy) .Anε-covering is a subset A0⊆Asuch thatAis
contained in the union of balls of radius εcentered in points in A0, in the metric induced by a norm ∥·∥. The
cardinality of the smallest ε-covering is denoted by H(ε,A,∥·∥)and its logarithm is called the metric entropy.
For any threshold parameter M≥0and importance weight w≥0, the soft-clip operator ζis defined by
ζ(w,M ) =/braceleftigg
w ifw≤M
α(M) log (w+α(M)−M)otherwise,
whereα(M)is such that α(M) log(α(M)) =M.
E.1 Ommitted Proofs
We start by defining our complexity measure Cn(Π,M), which will be upper-bounded by the metric entropy
in sup-norm at level ε= 1/nof the following function set,
FΠ,M:=/braceleftbigg
fπ: (x,a,y )∝⇕⊣√∫⊔≀→1 +y
Sζ/parenleftbiggπ(a|x)
π0(a|x),M/parenrightbigg
for someπ∈Π/bracerightbigg
, (22)
27Under review as submission to TMLR
whereS=ζ(W,M ). The function set corresponds to clipped prediction errors of policies πnormalized into
[0,1]. More precisely, to define rigorously Cn(Π,M), we denote for any n≥1andε>0, the complexity of a
classFby
H∞(ε,F,n) = sup
(xi,ai,yi)∈(X×A×Y )nH(ε,F/parenleftbig
{xi,ai,yi}/parenrightbig
,∥·∥∞), (23)
whereF/parenleftbig
{xi,ai,yi}/parenrightbig
=/braceleftbig/parenleftbig
f(x1,a1,y1),...,f (xn,an,yn)/parenrightbig
,f∈F/bracerightbig
⊆Rn. Then,Cn(Π,M)is defined by
Cn(Π,M) = logH∞(1/n,FΠ,M,2n). (24)
We are now ready to prove Proposition 4.1 that we restate below.
Proposition 4.1 (Generalization bound for ˆLscIPS(π)).LetΠbe a policy class and π0a logging policy, under
which an input-action-cost triple follows Dπ0. Assume that−1≤y≤0a.s. when (x,a,y )∼Dπ0and that
the importance weights are bounded by W. Then, with probability at least 1−δ, the IPS estimator with soft
clipping (12) on nsamples satisfies
∀π∈Π, L (π)≤ˆLscIPS (π) +O
/radicaligg
ˆVscIPS (π)/parenleftbig
Cn(Π,M) + log1
δ/parenrightbig
n+S/parenleftbig
Cn(Π,M) + log1
δ/parenrightbig
n
,
whereS=ζ(W,M ),ˆVscIPS (π)denotes the empirical variance of the cost estimates (20), and Cn(Π,M)is a
complexity measure (24) of the policy class.
Proof.LetΠbe a policy class, π0be a logging policy, and δ >0. LetM≥0be a threshold parameter,
W≥supa,x{π(a|x)/π0(a|x)}≥0a bound on the importance weights, and S=ζ(W,M ).
Let first consider the finite setting, in which case Cn(Π,M)≤log|Π|. Since all functions in FΠ,Mdefined in
Eq. (22) take values in [0,1], we can apply the concentration bound of Maurer & Pontil (2009, Corollary 5)
toFΠ,M, which yields that with probability at least 1−δ, for anyπ∈Π
E(x,a,y )∼Dπ0[fπ(x,a,y )]−1
nn/summationdisplay
i=1fπ(xi,ai,yi)≤/radicaligg
2ˆVscIPS(π) log(2|Π|/δ)
n+7 log(2|Π|/δ)
3(n−1),(25)
where ˆVscIPS(π)is the sample variance defined in (13). Furthermore, note that by construction of the fπ, for
anyπ∈Π,
E(x,a,y )∼Dπ0[fπ(x,a,y )] = 1 +LM(π)
Sand1
nn/summationdisplay
i=1fπ(xi,ai,yi) = 1 +ˆLscIPS(π)
S,
whereLM(π) =E(x,a,y )∼Dπ0/bracketleftbig
yζ/parenleftbig
π(a|x)/π0(a|x),M/parenrightbig/bracketrightbig
denotes the clipped expected risk of the policy πand
ˆLscIPS(π)is defined in (12). Thus, multiplying (25) by Sand using that L(π)≤LM(π)(sincey≤0
andζ(w,M )≤wfor allw), we get that with probability 1−δ,
L(π)≤ˆLscIPS(π) +/radicaligg
2ˆVscIPS(π) log(2|Π|/δ)
n+S7 log(2|Π|/δ)
3(n−1),∀π∈Π.
The finite setting may finally be extended to infinite policy classes by leveraging Maurer & Pontil (2009,
Theorem 6) as in (Swaminathan & Joachims, 2015a). This essentially consists in replacing |Π|above with
an empirical ℓ∞covering number of FΠ,Mof sizeH∞(1/n,FΠ,M,2n). Note that the number of empirical
samples 2nis due to the double-sample method used by Maurer & Pontil (2009).
We now state the excess risk upper-bound Proposition E.1 and provide the proof. The following proposition
is an intermediate result that will allow us to derive the Proposition 5.1.
28Under review as submission to TMLR
Proposition E.1. Consider the notations and assumptions of Proposition 4.1. Let ˆπCRMbe the solution of
the CRM problem in Eq. (15) and π∗∈Πbe any policy. Then, the choice λ= 3√
3/parenleftbig
Cn(Π,M) + log(30/δ)/parenrightbig1/2
implies with probability at least 1−δthe following upper-bound on the excess risk
L(ˆπCRM)−L(π∗)≤/radicaligg
32VM(π∗)/parenleftbig
Cn(Π,M) + log30
δ/parenrightbig
n+22S/parenleftbig
Cn(Π,M) + log30
δ/parenrightbig
n−1+hM(π∗),
whereV2
M(π∗)andhM(π∗)are the variance and bias of the is the clipped estimator of π∗and respectively
defined in (26) and (29).
Proof.We consider the notations of the proof of Proposition 4.1. Fix π∗∈Π. Applying, Theorem 15 of
Maurer & Pontil (2009)5to the function set FΠ,Mdefined in (22), we get w.p. 1−δ
E(x,a,y )∼Pπ0/bracketleftbig
fˆπCRM(x,a,y )/bracketrightbig
−E(x,a,y )∼Pπ0/bracketleftbig
fπ∗(x,a,y )/bracketrightbig
≤/radicaligg
32Var (x,a,y )∼Pπ0/bracketleftbig
fπ∗(x,a,y )/bracketrightbig/parenleftbig
Cn(Π,M) + log30
δ/parenrightbig
n+22/parenleftbig
Cn(Π,M) + log30
δ/parenrightbig
n−1.
Using the definition of fπ(x,a,y )(22), we have
E(x,a,y )∼Pπ0/bracketleftbig
fπ(x,a,y )/bracketrightbig
= 1 +LM(π)
SandVar(x,a,y )∼Pπ0/bracketleftbig
fπ(x,a,y )/bracketrightbig
=V2
M(π)
S2,
where
V2
M(π) = Var (x,a,y )∼Pπ0/parenleftbigg
yζ/parenleftbiggπ(a|x)
π0(a|x),M/parenrightbigg/parenrightbigg
. (26)
Substituting into the previous bound, this entails
LM(ˆπCRM)−LM(π∗)≤/radicaligg
32VM(π∗)/parenleftbig
Cn(Π,M) + log30
δ/parenrightbig
n+22S/parenleftbig
Cn(Π,M) + log30
δ/parenrightbig
n−1.(27)
To conclude the proof, it only remains to replace the clipped risk LMwith the true risk L. On the one hand,
since the costs ytake values into [−1,0], we haveyζ(π∗(a|x)/π0(a|x),M)≥yπ(a|x)/π0(a|x), which yields
L(ˆπCRM)≤LM(ˆπCRM). (28)
On the other-hand, by defining the bias
hM(π∗) =E(x,a,y )∼Pπ0/bracketleftbigg
yζ/parenleftbigg
M,π∗(a|x)
π0(a|x)/parenrightbigg
−yπ∗(a|x)
π0(a|x)/bracketrightbigg
(29)
we also have−L(π∗)−hM≤−LM(π∗), which together with (27) and (28) finally concludes the proof
L(ˆπCRM)−L(π∗)≤/radicaligg
32VM(π∗)/parenleftbig
Cn(Π,M) + log30
δ/parenrightbig
n+22S/parenleftbig
Cn(Π,M) + log30
δ/parenrightbig
n−1+hM(π∗).
We can now use the latter to prove Proposition 5.1 that is restated below.
5Note that in their notation, logMn(π)equalsCn(Π,M) +log(10),Xis the dataset{(xi,ai,yi)}1≤i≤nwhere
(xi,ai,yi)i.i.d.∼ Pπ0, andP(·,µ)is the expectation with respect to one test sample E(x,a,y )∼Pπ0[·].
29Under review as submission to TMLR
Proposition 5.1. Consider the notations and assumptions of Proposition 4.1. Let ˆπCRMbe the solution of
the CRM problem in Eq. (15) and π∗∈Πbe any policy. Then, with well chosen parameters λ, denoting the
varianceσ2
∗= Varπ0/bracketleftbig
π∗(a|x)/π0(a|x)/bracketrightbig
, with probability at least 1−δ:
L(ˆπCRM)−L(π∗)≲/radicaligg
(1 +σ2
∗) log(W+e)/parenleftbig
Cn(Π,M) + log1
δ/parenrightbig
n+log(W+e)(Cn(Π,M) + log1
δ)
n,
where≲hides universal multiplicative constants. In particular, assuming also that π0(x|a)−1are uniformly
bounded, the complexity of the class ΠCLPdescribed in Section 3.2, applied with a bounded kernel and
Θ =/braceleftbig
β∈Rm,s.t∥β∥≤C}×/braceleftbig
σ/bracerightbig
, is of order
Cn(ΠCLP,M)≤O(mlogn),
wheremis the size of the Nyström dictionary and O(·)hides multiplicative constants independent of nandm
(see (38)).
Proof.We first consider a general policy class Πand someπ∗∈Π. In this proof, to ease the notation, we write
Eπ0[·],Varπ0[·], andPπ0(·)to respectively refer to E(x,a,y )∼Pπ0[·],Var(x,a,y )∼Pπ0[·], andP(x,a,y )∼Pπ0(·).
We consider the notation of the proof of Proposition E.1 and start from its risk upper-bound
L(ˆπCRM)−L(π∗)≤/radicaligg
32VM(π∗)/parenleftbig
Cn(Π,M) + log30
δ/parenrightbig
n+22S/parenleftbig
Cn(Π,M) + log30
δ/parenrightbig
n−1+hM(π∗),(30)
where we recall, for any threshold M, the definitions of the bias and the variance of the clipped estimator
ofπ∗,
hM(π∗) =Eπ0/bracketleftbigg
y/parenleftbigg
ζ/parenleftigπ∗(a|x)
π0(a|x),M/parenrightig
−π∗(a|x)
π0(a|x)/parenrightbigg/bracketrightbigg
andV2
M(π∗) =Varπ0/bracketleftbigg
yζ/parenleftbiggπ∗(a|x)
π0(a|x),M/parenrightbigg/bracketrightbigg
.
Step 1: For any threshold M, upper-bound of the variance VM(π∗)and the bias hM(π∗).
By assumption, the (unclipped) variance of π∗/π0is bounded and we write
σ2
∗=Varπ0/bracketleftbiggπ∗(a|x)
π0(a|x)/bracketrightbigg
=E(x,a,y )∼Pπ0/bracketleftigg/parenleftbiggπ∗(a|x)
π0(a|x)−1/parenrightbigg2/bracketrightigg
.
First, we bound the clipped variance as
V2
M(π∗) =Varπ0/bracketleftbigg
yζ/parenleftbiggπ∗(a|x)
π0(a|x),M/parenrightbigg/bracketrightbigg
=Eπ0/bracketleftigg
y2ζ/parenleftbiggπ∗(a|x)
π0(a|x),M/parenrightbigg2/bracketrightigg
−Eπ0/bracketleftbigg
yζ/parenleftbiggπ∗(a|x)
π0(a|x),M/parenrightbigg/bracketrightbigg2
≤Eπ0/bracketleftbigg/parenleftigπ∗(a|x)
π0(a|x)/parenrightig2/bracketrightbigg
−LM(π∗)2=σ2
∗+ 1−LM(π∗)2≤σ2
∗+ 1.(31)
Then, by writing X=π∗(a|x)/π0(a|x), the bias may be upper-bounded as
hM(π∗)≤Eπ0/bracketleftbig
X−ζ(X,M )/bracketrightbig
≤Eπ0[(X−M)1{X >M}]
≤/integraldisplay∞
0Pπ0/parenleftig
(X−M)1{X >M}>t/parenrightig
dt
≤/integraldisplay∞
0Pπ0/parenleftbig
X−M >t/parenrightbig
dt=/integraldisplay∞
0Pπ0/parenleftig
(X−1)2>(t+M−1)2/parenrightig
dt
≤/integraldisplay∞
0Eπ0/bracketleftbig
(X−1)2/bracketrightbig
(t+M−1)2dt=Eπ0/bracketleftbig
(X−1)2/bracketrightbig
M−1=σ2
∗
M−1. (32)
30Under review as submission to TMLR
Furthermore, if W≤MthenS=ζ(W,M ) =W≤M, else, using α(M) =M/log(α(M))≤max{M,e}≤
M+e,
S=ζ(W,M ) =α(M) log/parenleftbig
W+α(M)−M/parenrightbig
≤(M+e) log(W+e). (33)
Therefore, substituting (31), (32), and (33) into (30), yields the following upper-bound on the excess risk
L(ˆπCRM)−L(π∗)
≤/radicaligg
32(1 +σ2∗)/parenleftbig
Cn(Π,M) + log30
δ/parenrightbig
n+22(M+e) log(W+e)/parenleftbig
Cn(Π,M) + log30
δ/parenrightbig
n−1+σ2
∗
M−1.
We now choose Msuch that
22(M−1) log(W+e)/parenleftbig
Cn(Π,M) + log30
δ/parenrightbig
n−1=σ2
∗
M−1(34)
which is possible since the left term grows from 0to infinity and the right term decreases from infinity to 0
forM > 1. Therefore, from the last two terms we eventually have
L(ˆπCRM)−L(π∗)≲/radicaligg
(1 +σ2
∗) log(W+e)/parenleftbig
Cn(Π,M) + log1
δ/parenrightbig
n+log(W+e)(Cn(Π,M) + log1
δ)
n,(35)
where≲hides universal multiplicative constants. This concludes the first part of the proof.
Step 2: Evaluating the policy class complexity Cn/parenleftbig
ΠCLP,M/parenrightbig
.
In this part, we provide a bound on the metric entropy Cn(ΠCLP,M) =logH∞(1/n,FΠCLP,2n). We recall
thatFΠCLPis defined in (22) and ΠCLPis described in Section 3.2. More precisely, let Z⊆Abe a Nyström
dictionary of size m≥1andγ >0. Since we use Gaussian distributions, we have
ΠCLP=/braceleftbig
πβs.t. for any x∈X, πβ(·|x) =N(µCLP
β(x),σ2),withβ∈Θβ/bracerightbig
,
where
Θβ=/braceleftbig
β∈Rm,s.t∥β∥≤C}
where
µCLP
β(x) =/summationdisplay
a∈Zexp(−γηβ(x,a))/summationtext
a′∈Zexp(−γηβ(x,a′))andηβ(x,a) =⟨β,ψ(x,a)⟩,
for some embedding ψdescribed in Section 3.2 which satisfies ∥ψ(x,a)∥≤υfor any (x,a). Fixx∈X. Let
us show that β∝⇕⊣√∫⊔≀→µCLP
β(x)is Lipschitz. Denote by Zβ(x) =/summationtext
a∈Zexp(−γηβ(x,a))the normalization factor.
We consider the gradient of µCLP
β(x)with regards to β
∂µCLP
β
∂β(x) =/summationdisplay
a∈Za/parenleftbiggψ(x,a) exp (⟨β,ψ(x,a)⟩)
Zβ(x)
−exp (⟨β,ψ(x,a)⟩)/summationtext
a∈Zψ(x,a) exp (⟨β,ψ(x,a)⟩)
Zβ(x)2/parenrightbigg
.
Taking the norm, and upper-bounding ∥ψ(x,a)∥≤υand∥a∥≤αZ, this yields
/vextenddouble/vextenddouble/vextenddouble∂µCLP
β
∂β(x)/vextenddouble/vextenddouble/vextenddouble≤2υαZ.
Therefore,β∝⇕⊣√∫⊔≀→µCLP
β(x)is2υαZ-Lipschitz, which implies that
β∝⇕⊣√∫⊔≀→πβ(a|x) =1
σ√
2πexp/parenleftigg
−1
2/parenleftbigga−µCLP
β(x)
σ/parenrightbigg2/parenrightigg
31Under review as submission to TMLR
are also Lipschitz with parameter/radicalbigg
2
eπυαZ
σ2. (36)
We recall that the metric entropy Cn,δ(ΠCLP) = logH∞(1/n,FΠCLP,2n)is applied to the function class
FΠCLP=/braceleftbigg
fβ: (x,a,y )∝⇕⊣√∫⊔≀→1 +y
Sζ/parenleftbiggπβ(a|x)
π0(a|x),M/parenrightbigg
for someπβ∈ΠCLP/bracerightbigg
.
By assumption, the inverse of the logging policy weights are bounded π0(a|x)−1≤M0for any (x,a)∈X×A
(as in Kallus & Zhou (2018)). Therefore, together with (36), for any (x,a,y )∈X×A×Y , the function
β∝⇕⊣√∫⊔≀→fβ(x,a,y )is Lipschitz with parameter
/radicalbigg
2
eπυαZM0
Sσ2. (37)
Letε >0. Because there exists an ε-covering of the ball {β∈Rd:∥β∥≤C}of size (C/ε)d, together
with (37), the latter provides a covering of FΠCLPin sup-norm with parameter
ε/radicalbigg
2
eπυαZM0
Sσ2.
Equalizing this with n−1and taking the log of the size of the covering entails
Cn(ΠCLP,M)≤dlog/parenleftigg/radicalbigg
2
eπCM 0υαZn
Sσ2/parenrightigg
.
Now, we recall that dis the dimension of the embedding ψ, which we model as
ψ(x,a) =ψX(x)⊗ψA(a)
whereψA(a)∈Rmis the embedding obtained by using the Nyström dictionary of size mon the action space
andψX(x)∈RdXis the embedding of the context space X⊆Rdx. Typically dX=d2
x+dx+ 1ordX=dx
respectively with the polynomial and linear maps considered in practice. Thus, d=mdX. Substituting the
latter into the complexity upper-bound and using 1≤SandM, we finally get
Cn(ΠCLP,M)≤mdXlog/parenleftigg/radicalbigg
2
eπCM 0υαZn
σ2/parenrightigg
, (38)
where we recall that dXis the dimension of the contextual feature map, Ca bound on the parameter norm β,
M0a bound on π0(a|x)−1,υ2a bound on the kernel, σ2the variance of the policies, and αZa bound on the
action norms∥a∥.
E.2 Discussion: on the Rate Obtained for Deterministic Classes
Consider the deterministic CLP class that assigns any input xto the action µCLP
β(x)defined in (7). Although
the paper focuses on stochastic policies, this appendix provides an excess-risk upper-bound with respect to
this deterministic class.
The latter corresponds to the choice σ= 0in the CLP policy set defined in Section 3.2 and therefore,
Proposition 5.1 cannot be applied directly. Fix some σ2>0to be optimized later. For any β∈Rm, we
denote byL(µCLP
β)the risk associated with the deterministic policy a=µCLP
β(x)and define πCLP
β(·|x)∼
N(µCLP
β(x),σ2). Then, let ˆπCRMbe the counterfactual estimator obtained by Proposition 5.1 on the class
ΠCLP ={πCLP
β(·|x)}, with probability 1−δ
ˆL(ˆπCRM)−L(µCLP
β)≤ˆL(ˆπCRM)−L(πCLP
β) +L(πCLP
β)−L(µCLP
β)
≤L(ˆπCRM)−L(πCLP
β) +L0σ/radicalbigg
2 log2
δ,
32Under review as submission to TMLR
where we assumed that the risk is L0-Lipschtitz and used that P/parenleftbig
|X|<σ/radicalbig
2 log(2/δ)/parenrightbig
≤δforX∼N(0,σ2).
From Proposition 5.1, this yields, with probability 1−2δ
ˆL(ˆπCRM)−L(µCLP
β)≲/radicaligg
(1 +σ2
∗) log(W+e)/parenleftbig
Cn(ΠCLP,M) + log1
δ/parenrightbig
n+L0σ/radicalbigg
2 log2
δ.
Now, note that Cn(Π,M)andlog(W)only yield logarithmic dependence on σ2andnand will thus not
impact the rate of convergence. The variance σ∗has a stronger dependence on σ2but can be upper-bounded
as follows
σ2
∗= Varπ0/bracketleftbiggπCLP
β(a|x)
π0(a|x)/bracketrightbigg
=/integraldisplay/parenleftbiggπCLP
β(a|x)−π0(a|x)
π0(a|x)/parenrightbigg2
π0(a|x)da
≤/integraldisplayπCLP
β(a|x)2
π0(a|x)da≤1
M0/integraldisplay
πCLP
β(a|x)2da=1
2σM0√π,
where the last equality is because πCLP
β(·|x)is a Gaussian distribution with variance σ2. Therefore, keeping
only the dependence on σandnand neglecting log-factors, we get the high-probability upper-bound
ˆL(ˆπCRM)−L(µCLP
β)≤˜O/parenleftig1√σn+σ/parenrightig
.
The choice σ=n−1/3entails a rate of order O(n−1/3).
F Details on the Experiment Setup and Reproducibility
In this section we give additional details on synthetic and semi-synthetic datasets, we provide details on the
evaluation methodology and information for experiment reproducibility.
F.1 Synthetic and Semi-Synthetic setups
Synthetic setups As many real-world applications feature a reward function that increases first with the
action, then plateaus and finally drops, we have chosen a piecewise linear function as shown in Figure 17 that
mimics reward buckets over the CoCoA dataset presented in Section 6.
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Action taken2
1
012Reward
Synthetic reward
p
r|a,p
0.6 0.8 1.0 1.2 1.4
Action taken2
1
012Reward
Mean reward over buckets
0
r|a
Figure 17: Synthetic reward engineering. The synthetic reward (left) is inspired from real-dataset reward
buckets (right).
We provide an illustration of the three synthetic datasets in Figure 18.
Semi-synthetic medical setup The semi-synthetic cost inputs prescriptions from medical experts to
obtainy(a,x) =max(|a−t∗|−0.1t∗,0), so as to mimic the expert prediction. The logging policy π0samples
actionsa∼π0contextually to a patient’s body mass index (BMI) score ZBMI =xBMI−µBMI
σBMIand can be
analytically written with i.i.d noise e∼N(0,1), moments of the therapeutic dose distribution µ∗
T,σ∗
Tsuch
thata=µ∗
T+σ∗
T√
θZBMI +σ∗
T√
1−θε(θ= 0.5in the setup of Kallus & Zhou (2018)). The logging
probability density function thus is a continuous density of a standard normal distribution over the quantity
a−µ∗
T+σ∗
T√
θZBMI
σ∗
T√1−θ.
33Under review as submission to TMLR
noisymoons
 noisycircles
 anisotropic
Figure 18: Contexts (points in R2), and potentials represented by a color map for the synthetic datasets.
Learned policies should vary with the context to adapt to the underlying potentials.
F.2 Reproducibility
We provide code for reproducibility and all experiments were run on a CPU cluster, each node consisting on
24 CPU cores (2 x Intel(R) Xeon(R) Gold 6146 CPU@ 3.20GHz), with 500GB of RAM.
Policy parametrization. In our experiments, we consider two forms of parametrizations: (i) a lognormal
distribution with θ= (θµ,σ),π(µ,σ)=logN(m,s)withs=/radicalbig
log (σ2/µ2+ 1);m=log(µ)−s2/2, so that
Ea∼π(µ,σ)[a] =µandVara∼π(µ,σ)[a] =σ2; (ii) a normal distribution π(µ,σ)=N(µ,σ). In both cases, the
meanµmay depend on the context (see Section 4), while the standard deviation σis a learned constant.
We add a positivity constraint for σand add an entropy regularization term to the objective in order to
encourage exploratory policies and avoid degenerate solutions.
Models. For parametrized distributions, we experimented both with normal and lognormal distributions
on all datasets, and different baseline parameterizations including constant, linear and quadratic feature
maps. We also performed some of our experiments on low-dimensional datasets with a stratified piece-wise
contextual parameterization, which partitions the space by bucketizing each feature by taking K(for e.g
K= 4) quantiles, and taking the cross product of these partitions for each feature. However this baseline is
not scalable for higher dimensional datasets such as the Warfarin dataset.
Hyperparameters. InTable5weshowthehyperparametersconsideredtoruntheexperimentstoreproduce
all the results. Note that the grid of hyperparameters is larger for synthetic data. For our experiments
involving anchor points, we validated the number of anchor points and kernel bandwidths similarly to other
hyperparameters.
Synthetic Warfarin CoCoA
Variance reg. λ {0.,0.001,0.01,0.1,1,10,100} {0.00010.0010.010.1} {0.,0.001,0.1}
ClippingM{1,1.7,2.8,4.6,7.7,12.9,21.5,35.9,59.9,100.0}{1,2.1,4.5,9.5,20}{1,2.1,4.5,9.5,10,20,100}
Prox.κ {0.001,0.01,0.1,1} {0.001,0.01,0.1} {0.001,0.01,0.1}
Reg. param. C{0.00001,0.0001,0.001,0.01,0.1}{0.00001,0.0001,0.001,0.01,0.1}{0.00001,0.0001,0.001,0.01,0.1}
Number of anchor points {2,3,5,7,10} {5,7,10,12,15,20} {2,3,5}
Softmaxγ {1,10,100} {1,5,10} {0.1,0.5,1,5}
Table 5: Table of hyperparameters for the Synthetic and CoCoA datasets
G Additional Results and Additional Evaluation Metrics
In this section we provided additional results on both contextual modeling and optimization driven approaches
of CRM.
G.1 Continuous vs Discrete strategies in Continuous-Action Space
We provide in Figure 19 additional plots for the continuous vs discrete strategies for the synthetic setups
described in Section 7.2.1.
34Under review as submission to TMLR
0 10 20 30 40 50
Nb anchor points0.00.20.40.60.8Test reward
Test reward - dataset noisycircles
IPS Discrete
SDM Discrete
IPS Continuous
SDM Continuous
0 10 20 30 40 50
Nb anchor points0.00.20.40.60.8Test reward
Test reward - dataset noisymoons
IPS Discrete
SDM Discrete
IPS Continuous
SDM Continuous
0 10 20 30 40 50
Nb anchor points0.00.20.40.60.8Test reward
Test reward - dataset anisotropic
IPS Discrete
SDM Discrete
IPS Continuous
SDM Continuous
Figure 19: Continuous vs discrete. Test rewards for CLP and (stochastic) direct method (DM) with Nyström
parameterization, versus a discrete approach, with varying numbers of anchor points. We add a minimal
amount of noise to the deterministic DM in order to pass the neffvalidation criterion.
G.2 Soft clipping improves or competes with other importance weighting transformation strategies
Soft-clipping improves or competes with other importance weighting transformation strategies.
We also experiment on the synthetic datasets comparing our soft clipping approach with other methods
which focus is to improve upon the classic clipping strategy for the same optimization purposes. Notably, we
consider the power-mean correction of importance sampling (PMCIS) (Metelli et al., 2021) method which we
adapt to our continuous modeling strategy to enable fair comparison. Moreover, we also added the SWITCH
(Wang et al., 2017) as well as the CAB (Su et al., 2019) methods. However, both methods use a direct
method term in their estimation, which is difficult to adapt for stochastic policies with continuous actions,
as explained in our discussions on doubly robust estimators (see Appendix G.4). Therefore, we considered
discretized strategies and compared them with soft clipped estimator applied to discretized policies. For
the discretized strategies, we have used the same anchoring strategies as described before, namely using
empirical quantiles of the logged actions (for 1D actions), and have optimized the number of anchor points
along with the other hyperparameters using the offline evaluation protocol. We see overall in Table 6 that our
soft-clipping strategy provides satisfactory performance or improves upon all weight transforming strategies
on the synthetic datasets.
Noisymoons Noisycircles Anisotropic
Logging policy π0 0.5301 0.5301 0.4533
SWITCH (discrete) 0.5786±0.0025 0.5520±0.0026 0.5741±0.0024
CAB (discrete) 0.5761±0.0024 0.5534±0.0025 0.5705±0.0021
scIPS (discrete) 0.5888±0.00220.5637±0.00240.5941±0.0019
PMCIS (CLP) 0.7244±0.0005 0.7189±0.00040.7739±0.0008
scIPS (CLP) 0.7674±0.00080.7805±0.0004 0.7703±0.0002
Table 6: Comparison of importance weight transformations on the synthetic datasets, for discretized strategies
and for continuous action policies.
G.3 Optimization Driven Approaches of CRM
In this part we provide additional results on optimization driven approaches of CRM for the Noisycircles,
Anisotropic, Warfarin and CoCoAdatasets.
Both Noisycircles and Anisotropic datasets in Figure 20 show the improvements in test reward and in training
objective of our optimization-driven strategies, namely the soft-clipping estimator and the use of the proximal
point algorithm. Overall we see that for most configurations, the proximal point method better optimizes
the objective function and provides better test performances, while the soft-clipping estimator performs
better than its hard-clipping variant, which may be attributed to the better optimization properties. For
semi-synthetic Warfarin and real-world CoCoA datasets in Figure 20 we also show the improvements in test
reward and in training objective of our optimization-driven strategies. More particularly we demonstrate the
35Under review as submission to TMLR
2.0
 1.5
 1.0
 0.5
 0.0 0.5
Losses0.00.51.01.52.02.53.03.5
Better local minima 76%NoisyCircles - Histogram PROX NONPROX
|NONPROX|
0.52 0.54 0.56 0.58 0.60 0.62 0.64 0.66 0.68 0.70
Base optimizer0.520.540.560.580.600.620.640.660.680.70Proximal Point Algorithm
NoisyCircles Dataset
Modelling
clp
linear
piecewise-constant
non-linearEstimator
SNIPS
DR
IPS
cIPS
scIPS
0.60 0.62 0.64 0.66 0.68 0.70
Classic clipping0.600.620.640.660.680.70Soft clipping
NoisyCircles Dataset
Modelling
clp
linear
piecewise-constant
non-linearClipping M
1.0
1.7
2.8
4.6
7.7
12.9
21.5
35.9
59.9
100.0
2.0
 1.5
 1.0
 0.5
 0.0 0.5
Losses0.00.51.01.52.02.53.03.5
Better local minima 72%Anisotropic - Histogram PROX NONPROX
|NONPROX|
0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80
Base optimizer0.450.500.550.600.650.700.750.80Proximal Point Algorithm
Anisotropic Dataset
Modelling
clp
linear
piecewise-constant
non-linearEstimator
SNIPS
DR
IPS
cIPS
scIPS
0.4 0.5 0.6 0.7 0.8
Classic clipping0.40.50.60.70.8Soft clipping
Anisotropic Dataset
Modelling
clp
linear
piecewise-constant
non-linearClipping M
1.0
1.7
2.8
4.6
7.7
12.9
21.5
35.9
59.9
100.0
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
Losses0.00.20.40.60.81.01.21.41.6Histogram PROX NONPROX
|NONPROX|  Dataset warfarin
30
 25
 20
 15
 10
 5
Base optimizer30
25
20
15
10
5
Proximal Point
Proximal Point Algorithm - Dataset warfarin
Modelling
clp
linear
constantEstimator
SNIPS
DR
IPS
cIPS
scIPS
20
 18
 16
 14
 12
 10
 8
 6
Hard clipping20
18
16
14
12
10
8
6
Soft clipping
Dataset warfarin
Modelling
clp
linear
constantClipping M
1
2.1
4.5
9.5
20
1.00
 0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
Losses012345CoCoA - Histogram PROX NONPROX
|NONPROX|
11.20 11.25 11.30 11.35 11.40 11.45
Base optimizer11.2011.2511.3011.3511.4011.45Proximal Point Algorithm
CoCoA Dataset
Modelling
clp
linear
piecewise-constant
non-linearEstimator
SNIPS
DR
IPS
cIPS
scIPS
11.30 11.32 11.34 11.36 11.38 11.40 11.42 11.44
Classic clipping11.3011.3211.3411.3611.3811.4011.4211.44Soft clipping
CoCoA Dataset
Modelling
clp
linear
piecewise-constant
non-linearClipping M
1
2.1
4.5
9.5
10.0
20
100.0
Figure 20: Optimization-driven approaches (NoisyCircles, Anisotropic, Warfarin and CoCoA datasets).
Relative improvements in the training objective from using the proximal point method (left), comparison of
test rewards for proximal point vs the simpler gradient-based method (center), and for soft- vs hard-clipping
(right).
effectiveness of proximal point methods on the Warfarin dataset where most proximal configurations perform
better than the base algorithm. Moreover, soft-clipping strategies perform better than its hard-clipping
variant on real-world dataset with outliers and noises, which demonstrate the effectiveness of this smooth
estimator for real-world setups.
36Under review as submission to TMLR
On further optimization perspectives and offline model selection As mentioned in Section 4, our
use of the proximal point algorithm differs from approaches that enhance policies to stay close to the logging
policies which modify the objective function as in (Schulman et al., 2017) in reinforcement learning. Another
avenue for future work would be to investigate distributionnally robust methods that do such modifications
of the objective function or add constraints on the distribution being optimized. The policy thereof obtained
would thus be closer to the logging policy in the CRM context. Moreover, as we showed in Section 6.2 with
importance sampling estimates and diagnostics, the offline decision becomes less statistically significant as
the evaluated policy is far from the logging policy. Investigating how the distributionally robust optimization
would yield better CRM solutions with regards to the offline evaluation protocol would make an interesting
future direction of research.
G.4 Doubly Robust Estimators
In this section we detail the discussion on doubly robust estimators and the difficulties that exist to obtain a
suitable estimator. In policy based methods for discrete actions, the DR estimator takes the form
ˆLDR(π) =1
nn/summationdisplay
i=1π(ai|xi)
π0,i(yi−ˆη(xi,ai)) +1
nn/summationdisplay
i=1/summationdisplay
a∈Aπ(a|xi)ˆη(xi,a)
Usually, the DR estimator should only improve on the vanilla IPS estimator thanks to the lower variance
induced by the outcome model ˆη. However, in a continuous-action setting with stochastic policies, the
second term becomes Ex∼PX,a∼π(·|x)[ˆη(x,a)], which is intractable to optimize in closed form since it involves
integrating over actions according to π(·|x). Thus, handling this term requires approximations (as described
hereafter), which may overall lead to poorer performance compared to an IPW estimator that sidesteps the
need for such a term.
The difficulty for stochastic policies with continuous actions is to derive an estimator of the term
Ex∼PX,a∼π(·|x)[ˆη(x,a)]. Unlike stochastic policies with discrete actions which allow to use a dis-
crete summation over the action set, we would need here to compute here an estimator of the form
1
n/summationtextn
i=1/integraltext
a∈Aπ(a|xi)ˆη(xi,a). We note that in the case of deterministic policy πlearning this direct method
term would easily boil down to1
n/summationtextn
i=1ˆη(xi,π(xi)), and the DR estimator would be built with smoothing
strategies for the IPW term as in (Kallus & Zhou, 2018).
In our experiments for stochastic policies with one dimensional actions A⊂R, we tried to approximate the
direct method term1
n/summationtextn
i=1/integraltext
a∈Aπ(a|xi)ˆη(xi,a)with a finite sum of CDFs differences over the manchor
pointsa1,...amby computing :
1
nn/summationdisplay
i=1m/summationdisplay
j=1/integraldisplayaj+1
a=ajπ(a|xi)ˆη(xi,aj)
We present a table below of some of the experiments we ran on the synthetic datasets we proposed, along
with an evaluation of the baselines that exist in the litterature for discrete actions. We see overall that our
model improves indeed upon the logging policy, but does not compare to the performances of the scIPS and
SNIPS estimators.
Demirer et al. (2019) provide a doubly robust (DR) estimator on continuous action using a semi-
parametric model of the policy value function. Their policy learning is performed in two stages (i)
estimate a doubly robust parameter θDR(x,a,r )in the semi-parametric model of the value function
E[y|a,x] =V(a,x) =⟨θ∗(x),ϕ(a,x)⟩and (ii) learn a policy in the empirical Monte Carlo estimate of
the policy value by solving
min
π∈Π/braceleftigg
ˆVDR(π) :=1
nn/summationdisplay
i=1⟨ˆθDR(xi,ai,ri),ϕ(π(xi),xi)⟩/bracerightigg
.
37Under review as submission to TMLR
Noisymoons Noisycircles Anisotropic
Logging policy π0 0.5301 0.5301 0.4533
Doubly Robust (discrete) 0.5756±0.0022 0.5500±0.0024 0.5593±0.0026
SWITCH (discrete) 0.5786±0.0025 0.5520±0.0026 0.5741±0.0024
CAB-DR (discrete) 0.5683±0.0023 0.5326±0.0025 0.5361±0.0028
Doubly Robust (ours) 0.6115±0.00010.6113±0.00020.5977±0.0001
Table 7: Comparison of doubly robust estimators, discretized strategies and our model which approximates
the direct method term.
The doubly robust estimation is performed with respect to the first parameter learned in (i) for the value
function, while we follow the CRM setting Swaminathan & Joachims (2015a) and directly derive estimators
of the policy value (risk) itself, which would correspond to the phase (ii). Instead, to derive an estimate a
DR estimator of such policy values, we tried extending the standard DR approach for discrete actions from
Dudik et al. (2011) to continuous actions by using our anchors points, but these worked poorly in practice.
38