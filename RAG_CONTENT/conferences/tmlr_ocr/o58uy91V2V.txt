Published in Transactions on Machine Learning Research (12/2024)
On the Detection of Reviewer-Author Collusion Rings From
Paper Bidding
Steven Jecmen sjecmen@gmail.com
Carnegie Mellon University
Nihar B. Shah nihars@cs.cmu.edu
Carnegie Mellon University
Fei Fang feifang@cmu.edu
Carnegie Mellon University
Leman Akoglu lakoglu@andrew.cmu.edu
Carnegie Mellon University
Reviewed on OpenReview: https: // openreview. net/ forum? id= o58uy91V2V
Abstract
Collusion rings pose a significant threat to peer review. In these rings, reviewers who are
also authors coordinate to manipulate paper assignments, often by strategically bidding on
each other’s papers. A promising solution is to detect collusion through these manipulated
bids, enabling conferences to take appropriate action. However, while methods exist for
detecting other types of fraud, no research has yet shown that identifying collusion rings is
feasible.
In this work, we consider the question of whether it is feasible to detect collusion rings from
the paper bidding. We conduct an empirical analysis of two realistic conference bidding
datasets and evaluate existing algorithms for fraud detection in other applications. We find
that collusion rings can achieve considerable success at manipulating the paper assignment
while remaining hidden from detection: for example, in one dataset, undetected colluders
are able to achieve assignment to up to 30% of the papers authored by other colluders.
In addition, when 10 colluders bid on all of each other’s papers, no detection algorithm
outputs a group of reviewers with more than 31% overlap with the true colluders. These
results suggest that collusion cannot be effectively detected from the bidding using popular
existing tools, demonstrating the need to develop more complex detection algorithms as well
as those that leverage additional metadata (e.g., reviewer-paper text-similarity scores).
1 Introduction
In scientific peer review, researchers are called upon to evaluate the work of other researchers in the same
scientific community and provide a recommendation as to whether each work should be published. Within
the field of computer science, academic conferences are some of the primary venues of publication. These
conferences receive a large pool of paper submissions from authors, and they rely on a peer review process
to accurately assess the quality of each submission and determine which to accept or reject. The resulting
acceptance decisions can have substantial impacts on the careers of the authors, as publication of a paper
in a competitive computer science conference is often used as an indication of research quality. Thus, it is
vital that the peer review processes of these conferences are fair to the authors.
However, the perceived prestige of a conference publication can incentivize authors and reviewers to act in
unethical ways. One such form of unethical behavior that has occurred in computer science conferences is
1Published in Transactions on Machine Learning Research (12/2024)
that ofcollusion rings (Vijaykumar, 2020; Littman, 2021). Conferences recruit many (if not all) qualified
authors of submitted papers to serve as reviewers, and consequently many reviewers at a conference are also
authors of one or more submissions to that same conference. In a collusion ring, a group of reviewers and
authors at a particular conference coordinate to manipulate the paper assignment such that they are assigned
to review each other’s papers. The most straightforward way that colluding reviewers can manipulate the
assignment is through “paper bidding,” a common phase of the paper assignment process. During the paper
bidding phase, each reviewer is asked to express their level of interest in reviewing each paper. These bids are
then taken into account when determining the paper assignment and often have a large influence (Jecmen
et al., 2020; Wu et al., 2021; Leyton-Brown et al., 2022). Thus, colluders can and do use strategic bidding
to achieve a desired paper assignment (Littman, 2021):
“The colluders hide conflicts of interest, then bid to review these papers, sometimes from
duplicate accounts, in an attempt to be assigned to these papers as reviewers.”
Once assigned, the colluders can give positive reviews to and push for the acceptance of other colluders’
papers.
Approaches to addressing academic fraud can be divided into mitigation-based and detection-based meth-
ods (Wilson, 2020). Several past works have proposed modifications to paper assignment algorithms that
aim at mitigating the impact of malicious bidding (Guo et al., 2018; Jecmen et al., 2020; Wu et al., 2021;
Boehmer et al., 2022; Jecmen et al., 2022). However, such approaches necessarily come with a cost in terms
of assignment quality, as they ignore some aspects of reviewer preferences expressed through bids. Moreover,
they do not identify which (if any) individuals were engaged in collusion. In contrast, detection methods have
received little attention in prior work. Ideally, an accurate detection method could identify any colluders so
that the conference can intervene to stop them. For example, a conference could decide to block the assign-
ment of detected colluders to each other’s papers, potentially leading to higher-quality assignments among
the honest reviewers as compared to mitigation methods. Reliable detection methods could also allow the
conference organizers to take more serious action against the colluding individuals, perhaps after a manual
investigation provides further evidence. In the words of Littman (2021):
“Better paper-assignment technology would help close one loophole that is being exploited.
But, without better investigative tools, we may never be able to hold the colluders to account.”
However, careless deployment of detection algorithms may result in false positives: honest reviewers and
authors being falsely identified as colluders. This is a serious danger in scientific peer review, since falsely-
accused reviewers and authors may have their reputations tarnished or their papers unfairly rejected. Es-
tablishing effective methods to detect collusion rings is thus a critical path for research on this problem.
A long line of research has applied anomaly-detection techniques to successfully detect various forms of fraud
in other settings. Many of these settings involve a network of people in which the fraud appears as a set of
anomalous interactions: for example, auction fraud on online platforms, fraudulent financial transactions,
fake reviews on sites such as Amazon, and fake accounts on social media (Akoglu et al., 2015). This raises
an important question of whether these techniques can be similarly applied to effectively detect fraudulent
interactions in the context of paper bidding for peer review. However, there is a vast range of approaches to
detection in the research literature that could potentially be applied to this problem. To facilitate a thorough
analysis, our work focuses on methods that attempt to detect collusion using only the paper bidding and
authorship data (i.e., the data that directly reflects the strategic bidding of colluders). Our focus on bidding
is additionally motivated by the fact that real-world investigations of collusion often begin with analysis of
the bidding data. Furthermore, many conferences use bidding-based heuristics to mitigate collusion (e.g.,
ignoring the bids of reviewers with too few positive bids; Jecmen et al. 2022). Concretely, we consider the
question: is it possible to effectively detect collusion rings from only the bids and authorships?
Answering this question alone requires an extensive empirical analysis. However, resolving this question
provides important direction for future research on detecting collusion rings. Our work does not attempt to
investigate various other questions of interest to the problem of collusion detection: for example, comparing
the effectiveness of detection- and mitigation-based approaches, conducting a game-theoretical analysis of
2Published in Transactions on Machine Learning Research (12/2024)
the detection problem, or gathering new data on the behavior of real colluders. As additional motivation
for our focus on using only bidding data, many conferences in practice use only bids to determine the paper
assignment (with the potential addition of some coarse subject-area matching); for instance, this is the
case for most conferences hosted on the popular HotCRP conference management system (where the first
instances of collusion were caught).
Within the realm of detection algorithms, we consider algorithms that attempt to accurately identify a single
group of colluders rather than those that identify a list of several potentially-colluding groups with lower
confidence. While an algorithm outputting multiple groups would be more likely to output the colluding
group, the aforementioned cost of false positives in our setting means that conferences should rightly be
hesitant to accuse a reviewer of collusion without a high degree of certainty. Detection algorithms with a high
false-positive rate could instead be applied as part of a collusion-mitigation approach in which a large number
of reviewer-paper pairs are flagged as potentially collusive and blocked from being assigned. However, we
view this class of algorithms as falling within the “mitigation” category rather than the “detection” category
we focus on, and as such should be compared to to the other collusion-mitigation approaches that have been
proposed in the literature.
One major challenge in the peer review setting is that there is no ground-truth data about how colluding
reviewers behave in practice. In this work, instead of assuming a specific form of collusive bidding behavior,
we address this challenge by considering a wide range of parameterized behavior for the collusion rings.
Specifically, since the purpose of a collusion ring is to achieve colluder-to-colluder paper assignments, we
vary the density of bidding between colluding reviewers and colluder-authored papers. Denser bidding
corresponds to a stronger attack, but also may allow the colluding group to be more easily detected. Thus,
our analysis aims to establish the regions of this parameterized space of colluder behavior at which collusion
can be effectively detected.
Our contribution is a set of empirical results on two different realistic bidding datasets concerning the
feasibility of detecting collusion rings. Our analysis consists of three parts. First, we characterize the
typical density of bidding found within groups of honest (non-colluding) reviewers, as high-density groups of
honest reviewers are potential false positives for detection algorithms. Second, we evaluate the performance
of several established algorithms at detecting injected collusion rings based on the anomalous density of
bidding within the ring. We consider a combination of fundamental approaches to finding dense subgraphs
and density-based fraud detection methods, including two algorithms that have been shown to effectively
detect fraud in other settings. Third, we evaluate the success of the injected collusion rings at achieving
their desired paper assignments, contextualizing the potential impact of fraud.
Overall, our results suggest that collusion rings cannot be effectively detected from only the bidding and
authorship data using popular existing algorithms. Our findings include the following:
•Using only the bidding and authorship data, all detection algorithms we analyze fail to detect some
injected collusion rings that are larger than any honest-reviewer groups with a similar density of
bidding. For example, when the collusion ring consists of 10 reviewers who bid on all of each other’s
authored papers, the output of the best-performing detection algorithm across both datasets has
only a 31% overlap (Jaccard similarity) with the true colluders on average.
•Colluders are able to achieve assignment to a substantial fraction of the papers authored by other
colluders while avoiding detection by all analyzed algorithms (30% and 24% in each of the two
datasets).
•A sizeable fraction of colluders can get at least one of their papers reviewed by another colluder
while avoiding detection (54% and 35% in each of the two datasets).
These results suggest that future research on detecting collusion rings must focus on more complex detection
methods, especially those that leverage other metadata (such as reviewer-paper text-similarity scores or
reviewer publication history).
The code and data we use in our analysis are available online at https://github.com/sjecmen/peer-
review-collusion-detection .
3Published in Transactions on Machine Learning Research (12/2024)
2 Related Work
Recent research has looked at improving various aspects of the peer review process (Shah, 2022). In this
line of work, various solutions have been proposed to the problem of reviewer-author collusion that we
consider (Jecmen et al., 2022). Jecmen et al. (2020) propose an algorithm for randomizing the paper
assignment such that the probability of a colluder achieving assignment to another colluder’s paper is limited.
Wu et al. (2021) propose learning a model of reviewer preferences from the bidding and using this model
to determine the paper assignment, thereby reducing the influence of collusive bidding. This work also
provides a semi-synthetic paper bidding dataset, which we use in our analysis. The works (Guo et al., 2018;
Boehmer et al., 2022) consider the problem of finding a cycle-free paper assignment. In the 2021 AAAI
Conference on Artificial Intelligence, a large AI conference, several precautions were taken to address the
problem of collusion: two-cycles in the paper assignment were disallowed and a constraint on the countries of
the reviewers assigned to the same paper was added (Leyton-Brown et al., 2022). Notably, these solutions all
aim to mitigate the impact of collusion rings on the assignment, rather than detecting them outright. While
the present work focuses on detecting attacks in the paper bidding phase, other works show that attacks on
the text-similarity algorithms used by conferences can be effective (Markwood et al., 2017; Tran & Jaiswal,
2019; Eisenhofer et al., 2023).
Most relevant to our work is the prior work (Jecmen et al., 2023), which observes paper bidding in a mock
conference setting and collects data on the strategies employed by participants acting as colluding reviewers.
In contrast, rather than attempting to precisely simulate the behavior of colluding reviewers, we consider an
intentionally simplified setting where malicious reviewer behavior can be easily parameterized, allowing us to
sweep out a wide range of potential colluder strategies. We note that the most common colluder strategies
observed by Jecmen et al. (2023) consist of injecting dense bidding between colluders, as is done by the
strategies considered in our analysis. Jecmen et al. (2023) also evaluate the performance of very simple
detection methods on the collected dataset, while we consider a set of more complex detection methods
based on dense-subgraph discovery.
Outside of the setting of scientific peer review, similar problems of fraud have been studied in the anomaly
detection literature. In online platforms such as Amazon or Yelp, several methods have been proposed to
detect products or sellers who purchase fraudulent reviews from users (Akoglu et al., 2013; Eswaran et al.,
2017; Kumar et al., 2018). Other works propose density-based methods for detecting groups of fraudsters
in these and similar online network settings (e.g., fraudulent transactions on eBay or fake followers on
Twitter) (Pandit et al., 2007; Prakash et al., 2010; Hooi et al., 2016); we evaluate the performance of (Hooi
etal.,2016)inouranalysis. However, oursetting(scientificpeerreview)isdistinctfromtheseonlineplatform
settings in a few ways. Most significantly, our work focuses on collusion in the paper bidding phase, where
the objective of colluders is to achieve a desired outcome in the subsequent paper assignment; in contrast, the
fraudulent reviews are themselves the objective of fraudsters on (e.g.) Amazon. As a result, the incentives
for peer-review colluders are not the same as those of frausters in the other settings: for example, since
each reviewer is assigned to review a limited number of papers, “camouflaging” by bidding positively on
non-colluder papers may result in those papers being assigned instead of the targeted papers. In addition,
fraudulent interactions on online platforms are often provided by large numbers of fake accounts specifically
used for fraud; since making fake accounts on common peer-review platforms is non-trivial, interactions
between colluders in peer review may be more often done under their real identities, leading to different
patterns of behavior. Numerous works have proposed other methods for graph-based anomaly detection,
including those that address online spam, cyber-attacks, and other forms of fraud (Akoglu et al., 2015).
Generic approaches for dense-subgraph detection (Goldberg, 1984; Tsourakakis et al., 2013; Hooi et al.,
2020) can also be applied to detect anomalies in graphs; we evaluate these methods in our analysis.
Aside from reviewer-author collusion rings, reviewers can attempt to dishonestly improve the chances of
acceptance of their own papers in other ways. One such form of malicious behavior occurs when reviewers
provide negative reviews to the papers they are assigned, thereby improving the relative standing of their
own authored work. In contrast to the issue of collusion rings, which require the cooperation of multiple
reviewers, this sort of “strategic reviewing” can be done by each reviewer alone. In a controlled experiment,
Balietti et al.(2016) found that people do exhibit strategic reviewing in acompetitive peer evaluation setting.
4Published in Transactions on Machine Learning Research (12/2024)
Several works have proposed methods to mitigate strategic reviewing with theoretical guarantees (Alon et al.,
2011; Xu et al., 2019; Dhull et al., 2022). On the detection side, Stelmakh et al. (2021) design a statistical test
for the presence of strategic reviewing and evaluate it on data collected from a peer evaluation experiment.
3 Preliminaries
In this section, we detail the setting of our analysis, the datasets we analyze, and the research questions we
answer.
3.1 Setting
We consider a conference peer review setting with a set of submitted papers Pand a set of reviewers R.
Conferences generally recruit the authors of the submitted papers to serve as reviewers, along with external,
non-author reviewers. The authorship set A⊂R×P contains all pairs (r,p)such that reviewer rauthored
paperp. Additionally, the conflict-of-interest set C⊂R×P contains all pairs (r,p)such that reviewer rhas
a conflict-of-interest with paper pand should not be assigned to review it ( A⊆C). Once submissions are
received, the conference asks each reviewer to indicate their level of interest on each submitted paper via
paper bidding. While conferences often allow each reviewer to choose from a number of discrete levels (e.g.,
“Eager”, “Willing”, “Not willing”), we consider a simplified setting with binary bids (positive or neutral);
note that allowing additional levels of bids can only give colluders more flexibility to manipulate the paper
assignment. The bid set B⊂R×P contains all pairs (r,p)such that reviewer rbid positively on paper p
(B∩C =∅). The conference also computes text-similarity scores between each reviewer and paper using a
functionT:R×P→ [0,1], whereT(r,p)indicates the level of similarity between the text of paper pand
the text of the past work of reviewer r.
The conference then computes the paper assignment in the following manner. First, the conference computes
similarity scores between each reviewer and paper using a function S:R×P→ [0,1]. In our experiments,
we useS(r,p) =1
2T(r,p)2I[(r,p)∈B]based on the function used in the 2016 Conference on Neural Information
Processing Systems (NeurIPS), a large machine-learning conference (Shah et al., 2018). The conference
then computes an assignment of papers to reviewers such that the total similarity of the assigned pairs is
maximized, subject to constraints that (i) each paper is assigned to exactly 3reviewers, (ii) each reviewer is
assigned at most 6papers, and (iii) no reviewer-paper pairs in Care assigned. While we use the stated values
in our experiments, the exact reviewer and paper loads vary between conferences. The maximum-similarity
assignment can be computed efficiently as a min-cost flow or as a linear program. This framework for paper
assignment has been used in numerous conferences and venues (Shah, 2022).
3.2 Datasets
We next provide details regarding the two datasets that we analyze in this work.
The first dataset, which we refer to as “AAMAS”, contains a subset of the real bidding from the 2021
International Conference on Autonomous Agents and Multiagent Systems, an AI conference. This dataset is
publicly available from PrefLib (Mattei & Walsh, 2013) and contains de-identified bids from reviewers that
did not opt-out from data collection. In this conference, bids were selected from {“Yes”, “Maybe”, “Conflict”,
“No response”}; we consider “Yes” and “Maybe” responses to be positive bids and include those reviewer-
paper pairs inB. We setCas the set of all reviewer-paper pairs with “Conflict” bids. Since the dataset does
not include authorship information, we reconstruct the authorships Aby subsampling 3 conflicts-of-interest
uniformly at random for each paper. The resulting dataset has 526 papers and 596 reviewers, 398 of whom
authored at least one paper. The dataset also does not contain text-similarity scores T(r,p). We generate
synthetic text-similarity scores using the procedure described in Appendix A, based on the text-similarities
from the 2018 International Conference on Learning Representations reconstructed by Xu et al. (2019).
Our second dataset, which we refer to as “S2ORC”, is the semi-synthetic dataset constructed and made
publicly available by Wu et al. (2021). This dataset contains synthetic bids between a large subset of
published computer science papers and authors from the Semantic Scholar Open Research Corpus (Ammar
5Published in Transactions on Machine Learning Research (12/2024)
et al., 2018), designed to match statistics from the NeurIPS 2016 conference (Shah et al., 2018). Bids were
chosen from values {0, 1, 2, 3}, where non-zero values indicate a positive bid (included in B). For the
authorship setA, we use the real authorships between the reviewers and papers in the dataset, discarding
the 90 bids placed on self-authored papers. We assume that the conflicts-of-interest are only the authorships
(C=A). The resulting data has 2446 papers and 2483 reviewers, 984 of whom authored at least one paper.
This dataset also contains real text-similarity scores between each reviewer and paper T(r,p), computed
using the popular TPMS algorithm (Charlin & Zemel, 2013). We compare the level of agreement between
these text-similarity scores and the bids to those found by Stelmakh et al. (2023) and find that they have a
realistic level of error; see Appendix A for details.
In our analysis, we assume that both of these datasets contain only bids from “honest” (non-colluding)
reviewers. The AAMAS dataset contains information from reviewers that did not opt-out from the data
collection, and we expect that any colluding reviewers in the conference would have done so. The S2ORC
bids are synthetic and do not model malicious reviewer behavior.
3.3 Problem Statement
Our goal is to detect collusion rings. We suppose that there exists a group of colluding reviewers M⊂Rwho
try to manipulate the paper assignment by altering their bids, with the aim of being assigned to review the
papers authored by other members of the colluding group. The objective of a collusion-detection algorithm
is to outputMgiven the set of bids B, along with the authorships Aand the other conflicts C. Note that the
text-similarities T(r,p)cannot be used for detection in our analysis–we consider the problem of detection
using only the bidding itself and the authorships.
As our original datasets do not contain colluding reviewers, we inject collusion into the datasets by choosing
a group of reviewers to be the colluders Mand modifying the bids of these reviewers (i.e., adding or removing
elements ofM×Pto/fromB). The strongest form of collusion would be to add bids between all reviewers in
Mand all papers authored by other reviewers in M, while additionally removing all other bids by reviewers
inM. However, malicious reviewers may not want to perform such an obvious manipulation out of fear
of being caught. Due to the lack of concrete evidence on colluding groups and the exact bidding strategy
that they might employ, we consider a wide range of possible colluding groups parameterized by both a size
parameter (i.e., the number of colluding reviewers) and a “density” parameter (roughly corresponding to the
attack strength).
As there are some modeling choices involved in concretely defining the bidding density parameter, we con-
sider two different notions of density, corresponding to two different graph representations of the bidding
relationships between the reviewers of the conference. The first representation (Section 4) is a unipartite
graph in which vertices represent reviewers. The second representation (Section 5) is a bipartite graph in
which vertices represent both reviewers and papers. We motivate and define each of these formulations in
their respective sections. For each graph formulation, we investigate the following three high-level research
questions:
•Q1:For what values of size and density do groups of honest reviewers already exist in the dataset
(without injected collusion)? (Sections 4.1 and 5.1)
•Q2:For what values of size and density can groups of injected colluders be accurately detected by
existing algorithms? (Sections 4.2 and 5.2)
•Q3:At each size and density, how successful are groups of injected colluders at achieving their
desired paper assignments? (Sections 4.3 and 5.3)
All of our experiments in these sections were conducted on a server with 515 GB RAM and 112 CPUs.
4 Unipartite Bidding Graph
In this section, we represent the reviewer bidding data in the form of a unipartite, directed graph where
each vertex corresponds to a reviewer. The graph contains a directed edge (r1,r2)if reviewer r1bid on at
6Published in Transactions on Machine Learning Research (12/2024)
2345678910111213
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
1113 885 455 121 13 0 0 0 0 0 0 0
1113 885 3608 1278 1412 3213 881 416 12 0 0 0
1113 6289 1.6E4 2.5E4 7.1E4 2.7E5 5.0E5 5.2E5 3.0E5
2.1E5
 4046
 0
1113 6289 6.1E4 6.4E5 2.9E6 7.1E6 2.1E7
5.6E7
 1.0E7
 5705
 41
 11
1113 3.9E4 2.1E5 4.4E6 3.1E7 1.2E8
4.1E8
 1.4E6
 3.0E6
 9.3E5
 5.1E4
 9.3E4
1.3E4 2.1E5 2.8E6 3.1E7 3.3E8
2.1E9
 3.7E7
 2.8E7
 3.1E7
 1.2E7
 1.9E4
 6.2E4
(a) Exact counts, AAMAS
2345678910111213
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
1652 1555 1234 882 520 218 54 6 0 0 0 0
1652 1555 4225 2976 4679 1.2E4 9596 1.1E4 4597 2474
1512
 331
1652 6015 1.2E4 1.9E4 5.3E4 1.7E5 3.4E5
4.8E5
 1.5E5
 897
 367
 0
1652 6015 4.3E4 2.5E5 8.3E5
2.2E6
 5.6E6
 7.7E4
 8.7E4
 178
 0
 0
1652 3.4E4 1.2E5 2.5E6
1.2E7
 2.7E7
 1.9E6
 2.9E4
 2.7E4
 0
 0
 0
1.9E4 1.7E5 2.6E6 1.9E7
1.2E8
 1.0E7
 5.0E5
 8.5E5
 3297
 0
 0
 0 (b) Exact counts, S2ORC
246810121416182022242628303234
number of reviewers (k)0.50.60.70.80.91.0edge density ()
(c) Groups found by greedy peeling, AAMAS
246810121416182022242628303234
number of reviewers (k)0.50.60.70.80.91.0edge density ()
 (d) Groups found by greedy peeling, S2ORC
Figure 1: Exact counts of honest-reviewer groups with varying size and edge density (Figures 1a-1b), and the
size and edge density of honest-reviewer groups found by a heuristic method (Figures 1c-1d). In Figures 1a-
1b, values in cells marked with “ ≥” represent lower bounds since exact counts were infeasible to compute;
the vertical axis represents lower bounds on the edge density. In Figures 1c-1d, each point corresponds to
an existing honest-reviewer group (found by a greedy peeling method), and the shaded area indicates the
region in which there exists at least one honest-reviewer group. The vertical axis represents exact values for
the edge density. Note that the vertical axis does not start at 0 for easier comparison with other figures.
least one paper authored by reviewer r2. Informally, the density of a group of reviewers in this graph is the
fraction of possible edges present between the reviewers. We formalize these definitions shortly. In Section 5,
we consider an alternative, bipartite graph representation of the bidding.
Our motivation for considering this graph representation is that it most naturally represents the reciprocal
behavior expected of a collusion ring. Several existing works (Guo et al., 2018; Boehmer et al., 2022; Leyton-
Brown et al., 2022) on reviewer-author collusion consider a similar reviewer-to-reviewer graph, aiming to
prevent cycles (i.e., “rings”) between reviewers in the paper assignment. For our purposes, the notion of
density implied by this graph has several useful properties. First, malicious reviewers may not attempt to
manipulate the assignment of every paper they author. For example, each colluder might have one bad paper
that they want their fellow colluders to review (e.g., because it is lower quality) and many good papers not
involved in the manipulation. Even if each colluder authors many good papers, collusion would still appear
as a dense subgraph. Additionally, collusion rings are based on a quid-pro-quo arrangement between the
colluders, where each colluder needs help getting some papers accepted to the conference. Each colluder
therefore should receive some benefit from the other colluders, reflected as density.
Formally, we denote the (directed) graph by G1= (V1,E1), whereV1=RandE1=∪p∈P{(r1,r2)∈R2:
(r1,p)∈B∧ (r2,p)∈A}. Given this graph representation, we characterize a potential group of colluding
reviewersS⊆Rin terms of two parameters. The first parameter kS=|S|is the size of the group. The
second parameter γSis the “edge density” of the group, defined as follows. For an arbitrary graph G= (V,E)
and any subset of vertices V′⊆V, we useE[V′]to denote the set of edges in the subgraph induced by V′.
We then define the edge density of Sto beγS=|E[S]|
2(|S|
2), the fraction of possible edges present. We henceforth
omit the subscript SfromkSandγSsince the subset in question will be clear from context.
We now analyze the problem of detecting collusion from G1. In the following three subsections, we provide
empirical analysis to answer each of the three research questions identified in Section 3.3.
7Published in Transactions on Machine Learning Research (12/2024)
4.1 Honest-Reviewer Groups (Q1)
In this subsection, we characterize the density of bidding among groups of honest (i.e., non-colluding)
reviewers in our datasets. For each value of the parameters (k,γ), we count the number of groups of size k
with edge density at least γ. Since we aim to detect collusion among authors, we consider only reviewers
that have authored at least one paper. This shows the frontier of identifiable detection: if honest-reviewer
groups exist at some (k,γ), then a colluding group with that same size and density cannot be identified as
suspicious with high confidence based on the bidding within the colluding group. Stated differently, this
analysis gives the number of potential false positives for a detection algorithm at each (k,γ). Recall that
false positives are a serious concern in collusion detection due to the danger to honest reviewers’ reputations.
InFigures1a-1b, weshowtheresultsofthisanalysis. Thesecountswereobtainedviaastandardbacktracking
search. Cells marked with “ ≥” were unable to be completed in 24 hours, as obtaining exact counts takes
worst-case time exponential in k. Instead, the values in these cells represent lower-bounds. We note that
these figures may be independently useful to conferences, since many conferences use heuristic checks for
collusion based on reviewer bidding patterns as part of the paper assignment (e.g., not assigning certain
reviewer-paper pairs or ignoring certain reviewers’ bids). The results in Figures 1a-1b may help these
conferences understand the false positive rates of such heuristics.
Since exact counts are infeasible for large values of k, we additionally use a heuristic method to find dense
subgraphs of larger sizes. We start with the entire graph and iteratively remove the vertex of smallest degree
to produce a sequence of subsets of decreasing size (commonly called “greedy peeling”). In Figures 1c-1d, we
plot the edge density γof these subsets against the size k. Thus, for all points (k,γ)in the shaded region,
at least one group of honest reviewers exists of size kand with edge density at least γ.
Overall, these results show that there is a large region of the space of (k,γ)where a colluding group could
not be detected based on its size and edge density due to the existence of many similar groups of honest
reviewers.
4.2 Detection Algorithm Evaluation (Q2)
For values of (k,γ)larger than those that exist in the honest bidding, we may hope that colluding groups
can be identified as suspicious by an appropriate algorithm. However, this may not always be possible: since
the size of the colluding group is unknown, a detection algorithm must identify that the colluding group
is more suspicious than the honest-reviewer groups of different sizes. In this section, we evaluate whether
several existing techniques for dense-subgraph discovery suffice to detect colluding groups with larger values
of(k,γ), including one algorithm demonstrated to effectively detect fraud on Twitter.
We consider two desiderata in selecting algorithms to evaluate. First, the algorithms should not require
explicitly specifying the size of the output subset. This is a requirement in our setting, since conference
program chairs have no evidence regarding the size of the colluding groups that they can use to direct the
detection algorithms. Second, our algorithms should be based on different notions of subgraph density that
implicitly balance the size and edge density of the groups in a different way. Since G1is unattributed, this
choice is the primary design decision made by a density-based detection algorithm. By considering a variety
of such choices, we hope to find one that detects the true colluders across the largest possible range of
injected size and density. As a result, we consider the following algorithms:
•Traditional densest-subgraph discovery (Goldberg, 1984; Charikar, 2000): Output the subset of
verticesSthat maximizes f(S) =|E1[S]|/|S|. This corresponds to the subgraph with highest
average degree. In our case, since we count all edges in both directions, this is a generalization of
the standard definition for undirected graphs. To solve this problem, we implement the LP-based
exact algorithm from Charikar (2000). We refer to this as “DSD”.
•Optimal quasi-clique discovery (Tsourakakis et al., 2013): For a given parameter α∈[0,1], output
the subset of vertices Sthat maximizes the “edge surplus” f(S) =|E1[S]|−2α/parenleftbig|S|
2/parenrightbig
. The second
term inf(S)corresponds to the expected number of edges in the subgraph if each edge occurs
independently with probability α. Since we count all edges in both directions, we include the factor
8Published in Transactions on Machine Learning Research (12/2024)
of2in the second term as compared to the standard definition for undirected graphs. To solve this
problem, we implement the two approximation algorithms proposed by Tsourakakis et al. (2013),
one based on greedy peeling and one based on local search. We refer to these algorithms as “OQC-
Greedy” and “OQC-Local” respectively. As recommended by Tsourakakis et al. (2013), we set
α= 1/3.
•TellTail(Hooietal.,2020): Thismethodfirstdefinesthe“adjustedmass”ofasubsetasthedifference
betweenthenumberofedgesinthesubsetandtheexpectednumberofedgesinthesubsetifedgesare
rewiredrandomly(preservingvertexdegrees). Subsetsarethenscoredbasedontheprobabilityofthe
adjusted mass under a fitted Generalized Pareto distribution. Most parameters of this distribution
are fixed as constants based on empirical observations across several datasets. Since this method
operates on undirected graphs and is non-trivial to adapt to a directed setting, we first map our
bidding graphG1to an undirected graph G= (V1,E)before inputting it to this algorithm. The
input graphGhas the same vertex set as G1and has an edge (r1,r2)iff both edges (r1,r2)∈E1
and(r2,r1)∈E1; that is,E={(r1,r2)∈R2: (r1,r2)∈E1∧(r2,r1)∈E1}). Denote the degree of
a vertexvinGbydeg(v)and the CDF of the Generalized Pareto distribution as FGP. Concretely,
the algorithm finds a subset of vertices S⊆Vthat approximately maximizes the objective function
f(S) =FGP/parenleftbigg
|E[S]|−/summationtext
v∈Sdeg(v)
4|E|/parenrightbigg
. Note that this algorithm implicitly takes into account the
connectivity between the chosen subset and the rest of the graph.
DSD, OQC-Greedy, and OQC-Local operate on pre-specified notions of density, without considering the
sparsity of subgraphs in the input graph. In contrast, TellTail defines density in a data-driven fashion by
identifying the distribution of masses that subgraphs of certain sizes follow, arguing that other notions of
density tend to be biased towards larger subgraphs. TellTail was also shown to detect fraudulent followers
on Twitter. These algorithms cover a representative set of the landscape of dense-subgraph mining solutions.
We evaluate the detection algorithms against the following collusion model. For each setting of (k,γ), we
choose a subset of kreviewers uniformly at random from among those reviewers that authored at least one
paper. We then add edges uniformly at random between colluding reviewers until the subgraph has edge
density at least γ. This modified graph is then passed as input to each detection algorithm. We repeat this
procedure for 50trials for each setting. We report the mean Jaccard similarity between the set of injected
colluders and the set of reviewers output by the detection algorithms.
For the detection algorithms that use local search to optimize their objective (OQC-Local and TellTail), we
return the result with highest objective value over 11 initializations. The first run is initialized according
to the triangle-counting heuristic suggested by Tsourakakis et al. (2013), and the remaining 10 runs are
initialized uniformly at random. We find that the detection performance of OQC-Local is significantly better
when only the heuristic initialization is used, since the random initializations often resulted in output with
a higher objective value but lower overlap with the true colluders; thus, we show the results with heuristic
initialization only in this section and defer the results with all initializations to Appendix B. However,
the poor performance of OQC-Local when all initializations are used indicates that the objective value of
OQC-Local is misaligned with the detection objective.
The results for DSD, OQC-Local, and TellTail are shown in Figure 2. Results for OQC-Greedy are shown in
Appendix B as it generally performs worse than OQC-Local. In both datasets, DSD performs very poorly;
this is because it consistently returns a overly large subset of reviewers. On the AAMAS dataset, OQC-Local
does somewhat better, achieving Jaccard similarity above 0.5 for values of k≥26andγ≥0.9. TellTail
performs by far the best, detecting colluders consistently for moderate-to-large values of (k,γ). However,
there exists a wide range of settings in which all algorithms fail to detect injected colluders: for example,
reviewer groups with 8≤k≤12andγ= 1.0do not exist in the honest bidding and yet are still not detected
by any algorithm. On the S2ORC dataset, OQC-Local and TellTail appear to perform equally well. There
is a similar region in which honest-reviewer groups do not exist and yet detection fails to identify the correct
group with high probability (e.g., 10≤k≤12withγ= 1.0), albeit smaller than in the AAMAS dataset.
9Published in Transactions on Machine Learning Research (12/2024)
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.00
±.000.01
±.000.02
±.000.02
±.000.03
±.000.03
±.000.04
±.000.04
±.000.05
±.000.06
±.000.06
±.000.07
±.000.07
±.000.08
±.000.09
±.000.09
±.000.10
±.00
0.00
±.000.01
±.000.01
±.000.02
±.000.03
±.000.03
±.000.04
±.000.04
±.000.05
±.000.06
±.000.06
±.000.07
±.000.07
±.000.08
±.000.08
±.000.09
±.000.10
±.00
0.00
±.000.01
±.000.02
±.000.02
±.000.03
±.000.03
±.000.04
±.000.04
±.000.05
±.000.06
±.000.06
±.000.07
±.000.07
±.000.08
±.000.08
±.000.09
±.000.10
±.00
0.00
±.000.01
±.000.01
±.000.02
±.000.03
±.000.03
±.000.04
±.000.04
±.000.05
±.000.05
±.000.06
±.000.07
±.000.07
±.000.08
±.000.08
±.000.09
±.000.10
±.00
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.04
±.000.04
±.000.05
±.000.05
±.000.06
±.000.07
±.000.07
±.000.08
±.000.08
±.000.09
±.000.10
±.00
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.04
±.000.04
±.000.05
±.000.05
±.000.06
±.000.06
±.000.07
±.000.08
±.000.08
±.000.09
±.000.09
±.00
(a) DSD, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.03
±.000.03
±.000.03
±.000.04
±.000.04
±.000.05
±.000.89
±.041.00
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.03
±.000.03
±.000.04
±.000.04
±.000.05
±.000.05
±.000.05
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.04
±.000.04
±.000.04
±.000.05
±.000.05
±.000.06
±.000.06
±.00
0.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.03
±.000.04
±.000.04
±.000.05
±.000.05
±.000.05
±.000.06
±.00
0.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.03
±.000.04
±.000.04
±.000.05
±.000.05
±.000.06
±.00
0.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.03
±.000.03
±.000.04
±.000.05
±.000.05
±.00 (b) DSD, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.00
±.000.01
±.000.02
±.000.02
±.000.02
±.000.03
±.000.05
±.000.08
±.010.18
±.030.45
±.060.46
±.050.61
±.050.73
±.050.82
±.040.83
±.040.92
±.020.90
±.03
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.04
±.000.04
±.000.06
±.010.08
±.010.22
±.040.24
±.040.41
±.050.52
±.060.62
±.050.77
±.050.79
±.040.84
±.04
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.04
±.000.04
±.000.07
±.010.08
±.010.10
±.010.15
±.020.31
±.040.41
±.050.47
±.050.56
±.050.61
±.05
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.04
±.000.04
±.000.05
±.010.06
±.010.07
±.010.10
±.010.12
±.010.17
±.020.22
±.030.23
±.030.33
±.04
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.04
±.000.04
±.000.05
±.000.05
±.000.07
±.010.07
±.010.08
±.010.11
±.010.13
±.010.14
±.010.16
±.01
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.03
±.000.04
±.000.05
±.000.05
±.000.05
±.010.07
±.010.07
±.010.08
±.010.08
±.010.10
±.010.10
±.01
(c) OQC-Local, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.00
±.000.01
±.000.01
±.000.01
±.000.03
±.010.27
±.060.69
±.060.87
±.040.94
±.031.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.03
±.020.32
±.060.67
±.060.85
±.040.97
±.020.96
±.020.99
±.010.99
±.011.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.06
±.030.23
±.050.66
±.060.83
±.050.91
±.030.94
±.030.97
±.021.00
±.000.97
±.021.00
±.001.00
±.00
0.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.06
±.030.18
±.050.37
±.060.68
±.050.67
±.050.77
±.050.92
±.030.88
±.040.97
±.020.97
±.02
0.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.01
±.000.07
±.030.10
±.040.19
±.050.48
±.060.63
±.050.63
±.050.81
±.040.81
±.04
0.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.02
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.05
±.010.13
±.040.18
±.050.32
±.050.38
±.05 (d) OQC-Local, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.010.85
±.050.96
±.031.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.36
±.071.00
±.001.00
±.001.00
±.000.98
±.021.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.30
±.060.94
±.031.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.01
±.000.02
±.000.01
±.000.01
±.000.02
±.000.03
±.000.03
±.000.03
±.000.03
±.000.03
±.000.03
±.000.12
±.040.79
±.060.99
±.000.97
±.020.99
±.00
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.03
±.000.03
±.000.03
±.000.03
±.000.03
±.000.04
±.000.04
±.000.04
±.000.03
±.000.03
±.000.04
±.00
0.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.03
±.000.03
±.000.03
±.000.03
±.000.04
±.000.03
±.000.04
±.000.04
±.000.05
±.000.04
±.000.04
±.00
(e) TellTail, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.00
±.000.00
±.000.00
±.000.01
±.000.03
±.010.33
±.050.78
±.050.96
±.020.99
±.010.99
±.011.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.01
±.000.01
±.000.00
±.000.00
±.000.01
±.010.03
±.010.31
±.060.65
±.050.94
±.030.99
±.010.98
±.010.99
±.011.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.01
±.000.00
±.000.01
±.000.01
±.000.02
±.010.05
±.020.18
±.040.66
±.050.84
±.040.94
±.020.99
±.010.97
±.011.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.010.06
±.020.24
±.040.55
±.060.72
±.040.85
±.030.95
±.020.98
±.010.99
±.011.00
±.00
0.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.010.03
±.010.08
±.020.11
±.030.27
±.050.53
±.060.73
±.050.75
±.050.82
±.05
0.00
±.000.01
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.03
±.010.05
±.020.05
±.020.14
±.030.15
±.030.20
±.04 (f) TellTail, S2ORC
Figure 2: Performance of detection algorithms on G1. Values indicate the mean Jaccard similarity between
the true set of colluders and the algorithm output, along with standard errors. Higher values correspond to
better detection performance.
4.3 Manipulation Success Evaluation (Q3)
In this subsection, we evaluate the impact of collusion in terms of the colluders’ success at achieving their
desired paper assignments, contextualizing the results of the previous two subsections. We conduct experi-
ments in which we inject colluding groups into G1for each setting of the parameters (k,γ)in the exact same
manner as in Section 4.2. We then fix this modified version of G1(i.e., the input to the detection algorithms
in Section 4.2) as the “target graph” and modify Bin order to realize this graph.
However, there are many possible strategies that these colluders could employ to modify their bids that
correspond to the addition of these edges, since each edge (r1,r2)denotes the presence of at least one bid
from reviewer r1on a paper authored by reviewer r2. Additionally, the relationship between bids and edges is
not one-to-one due to co-authorship between reviewers, which means that exactly realizing the target graph
via bidding may not be possible. Due to these challenges, we instead modify the bids of colluders to achieve
a modified version of G1that is “no more suspicious” than the target graph. Specifically, we consider each
edge (r1,r2)in the target graph such that r1is in the injected colluding group. If r2is also a colluder, we
add bids from r1on each paper authored by r2; otherwise, we choose one existing bid from r1on a paper
authored by r2uniformly at random and remove all other bids from r1on papers authored by r2. In this
way, we ensure that the edges within the injected colluder group are a subset of the edges in the target graph
and the edges outside the injected colluder group are a superset of the edges in the target graph. Subject to
these constraints, this procedure allows colluders to bid according to a worst-case strategy.
10Published in Transactions on Machine Learning Research (12/2024)
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.04
±.010.14
±.020.17
±.010.20
±.010.26
±.010.28
±.010.29
±.010.33
±.010.35
±.010.38
±.010.40
±.010.41
±.010.43
±.010.44
±.010.47
±.010.47
±.010.51
±.01
0.03
±.010.07
±.010.15
±.010.19
±.010.22
±.010.24
±.010.27
±.010.33
±.010.34
±.010.35
±.010.39
±.010.41
±.010.40
±.010.44
±.010.45
±.010.48
±.010.47
±.01
0.04
±.010.10
±.020.12
±.010.18
±.010.22
±.010.26
±.010.27
±.010.30
±.010.30
±.010.33
±.010.35
±.010.37
±.010.39
±.010.41
±.010.43
±.010.44
±.010.46
±.01
0.06
±.020.08
±.010.10
±.010.15
±.010.19
±.010.25
±.010.28
±.010.28
±.010.31
±.010.30
±.010.36
±.010.36
±.010.38
±.010.39
±.010.42
±.010.42
±.010.44
±.01
0.05
±.020.07
±.010.12
±.010.14
±.010.17
±.010.21
±.010.23
±.010.27
±.010.29
±.010.29
±.010.33
±.010.34
±.010.36
±.010.38
±.010.40
±.010.41
±.010.41
±.01
0.02
±.010.07
±.010.09
±.010.12
±.010.17
±.010.19
±.010.21
±.010.24
±.010.27
±.010.28
±.010.29
±.010.31
±.010.33
±.010.35
±.010.37
±.010.40
±.010.40
±.01
(a) Fraction of target papers with a colluder assigned,
AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.03
±.010.11
±.020.17
±.020.16
±.010.21
±.020.19
±.010.26
±.010.27
±.010.26
±.010.28
±.010.29
±.010.32
±.010.33
±.010.34
±.010.34
±.010.36
±.010.38
±.01
0.02
±.010.11
±.020.13
±.020.21
±.010.18
±.020.21
±.010.24
±.010.24
±.010.25
±.010.28
±.010.29
±.010.29
±.010.32
±.010.32
±.010.33
±.010.35
±.010.35
±.01
0.03
±.010.09
±.020.13
±.020.17
±.010.18
±.010.16
±.010.22
±.010.24
±.010.23
±.010.26
±.010.29
±.010.29
±.010.31
±.010.33
±.010.32
±.010.34
±.010.34
±.01
0.05
±.020.06
±.010.09
±.010.14
±.010.16
±.010.19
±.010.20
±.010.22
±.010.22
±.010.28
±.010.28
±.010.26
±.010.30
±.010.32
±.010.31
±.010.31
±.010.33
±.01
0.05
±.020.09
±.020.15
±.020.14
±.020.15
±.010.17
±.010.19
±.010.19
±.010.24
±.010.24
±.010.25
±.010.25
±.010.27
±.010.30
±.010.28
±.010.30
±.010.30
±.01
0.03
±.020.08
±.020.10
±.020.12
±.010.13
±.010.17
±.010.18
±.010.18
±.010.20
±.010.23
±.010.25
±.010.24
±.010.28
±.010.27
±.010.27
±.010.29
±.010.29
±.01(b) Fraction of target papers with a colluder assigned,
S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.11
±.030.32
±.030.34
±.030.41
±.020.50
±.020.50
±.020.52
±.020.57
±.020.61
±.020.61
±.020.62
±.010.64
±.010.68
±.010.66
±.010.70
±.010.69
±.010.73
±.01
0.11
±.040.17
±.020.32
±.030.38
±.020.41
±.020.46
±.020.51
±.020.56
±.010.57
±.020.60
±.020.60
±.010.64
±.010.64
±.010.66
±.010.68
±.010.69
±.010.71
±.01
0.11
±.030.21
±.030.25
±.030.36
±.020.42
±.020.47
±.020.48
±.020.53
±.020.51
±.020.56
±.020.58
±.010.60
±.010.63
±.010.64
±.010.67
±.010.66
±.010.69
±.01
0.15
±.030.22
±.030.24
±.020.30
±.030.41
±.020.43
±.020.50
±.020.49
±.020.54
±.020.54
±.020.58
±.010.58
±.010.61
±.010.63
±.010.66
±.010.62
±.010.66
±.01
0.11
±.030.19
±.030.25
±.020.31
±.020.37
±.020.43
±.020.45
±.020.48
±.020.52
±.020.52
±.020.58
±.020.59
±.010.58
±.010.59
±.010.62
±.010.65
±.010.65
±.01
0.06
±.020.17
±.030.22
±.020.27
±.030.34
±.020.38
±.020.40
±.020.46
±.020.49
±.020.52
±.010.54
±.020.53
±.010.56
±.010.58
±.010.60
±.010.64
±.010.62
±.01
(c) Fraction of colluders with a colluder assigned,
AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.07
±.020.17
±.020.26
±.020.25
±.020.32
±.020.30
±.020.39
±.020.39
±.020.41
±.020.43
±.020.43
±.010.48
±.010.46
±.010.49
±.010.47
±.010.52
±.010.54
±.01
0.06
±.030.20
±.030.23
±.030.32
±.020.29
±.020.34
±.020.35
±.020.38
±.020.39
±.020.43
±.020.42
±.010.44
±.010.46
±.010.46
±.010.47
±.010.50
±.010.50
±.01
0.09
±.030.15
±.020.20
±.020.28
±.020.26
±.020.26
±.020.35
±.020.36
±.020.36
±.020.39
±.020.43
±.010.42
±.010.45
±.010.47
±.010.45
±.010.47
±.010.50
±.01
0.08
±.030.11
±.020.16
±.020.23
±.020.28
±.020.29
±.020.30
±.020.34
±.020.35
±.020.41
±.020.42
±.010.40
±.010.46
±.010.45
±.020.46
±.010.45
±.010.47
±.01
0.07
±.020.14
±.020.21
±.030.23
±.020.25
±.020.28
±.020.31
±.020.31
±.020.35
±.020.37
±.020.38
±.010.38
±.010.41
±.010.43
±.010.43
±.010.43
±.010.43
±.01
0.05
±.030.11
±.020.17
±.030.20
±.020.22
±.020.26
±.020.28
±.020.28
±.010.34
±.020.36
±.020.36
±.010.37
±.020.41
±.010.40
±.010.40
±.010.41
±.010.43
±.01(d) Fraction of colluders with a colluder assigned,
S2ORC
Figure 3: Success of colluders in terms of kandγ. Values indicate the mean for each metric along with
standard errors.
Given the modified bids, we compute a paper assignment as detailed in Section 3.1. Since the exact objective
of colluders is not obvious, we evaluate the success of the malicious reviewers in two ways. First, we count the
fraction of “target” papers (i.e., those authored by at least one colluder) with at least one colluder assigned;
this indicates the extent to which colluders succeeded at influencing the acceptance decision for all of their
papers. This may be too harsh of a metric, since colluders may not aim to influence the decision for all of
their papers simultaneously. Second, we count the fraction of colluders who authored at least one paper that
has at least one other colluder assigned; this indicates the fraction of colluders who recieved some benefit
from participating in the collusion ring. We conduct 50trials for each setting of the parameters and report
the mean for each of the two metrics.
The results are shown in Figure 3. For the results on the AAMAS dataset, of particular interest are the
results with k≤20andγ≤0.8, since these were not detected by any of the algorithms in Section 4.2.
We see that at the extremes of this region, approximately one-third of colluder-authored papers have at
least one colluder assigned, and approximately one-half of colluders have at least one colluder assigned to
one of their papers. At (k= 10,γ= 0.8), where a larger number of honest-reviewer groups exist, both
success metrics are also reasonably high. The results on the S2ORC dataset are similar: for example, at
(k= 14,γ= 0.8),22%of target papers and 35%of colluders are successfully targeted while all detection
algorithms achieve low success rates. Similarly, 21%of papers and 34%of colluders are successfully targeted
when colluders are camouflaged among numerous honest-reviewer groups at (k= 12,γ= 0.9). Thus,
colluders can influence the acceptance decisions for a moderate fraction of their submitted papers without
being detected by the evaluated algorithms. The counts of honest-reviewer groups in Section 4.1 suggest
that a portion of the success of colluders may be unavoidable in this setting, regardless of the strength of the
detection algorithms. In practice, the implication of these results is that any group of reviewers could decide
to collude and unethically improve their chances of getting a paper accepted at the conference, tainting the
fairness of the peer review process, while remaining completely safe from bidding-based detection.
11Published in Transactions on Machine Learning Research (12/2024)
2 3 4 5
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
57 12 0 0
62 42 15 1
286 669 1101 1580
1567 9259 5.9E4 3.4E5
6670 2.4E5 5.4E6 1.0E8
(a) AAMAS
2 3 4
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
349 218 116
449 807 1164
1382 5590 1.8E4
5378 6.6E4 1.1E6
1.4E4 1.8E6 7.4E7 (b) S2ORC
Figure 4: Exact counts of honest-reviewer groups with varying size and bid density.
5 Bipartite Bidding Graph
In this section, we represent the reviewer bidding data as a bipartite graph, in which one set of vertices
corresponds to reviewers and the other set of vertices corresponds to papers. This graph contains three
types of undirected edges between a reviewer rand a paper p: bid edges, indicating that reviewer rbid
positively on paper p; authorship edges, indicating that reviewer rauthored paper p; and conflict-of-interest
edges, indicating that reviewer rhas a conflict-of-interest with paper pbut did not author it. Our motivation
for considering this graph is that it directly represents all data that the detection algorithms have access to.
Formally, we denote this graph as G2= (V2,(E(B)
2,E(A)
2,E(C)
2)), whereV2=R∪Pis the vertex set and
E(B)
2=B,E(A)
2=A, andE(C)
2=C\Aare the sets of bid, authorship, and conflict edges respectively. As in
Section 4, we characterize a (potentially colluding) group of reviewers S⊆Rin terms of a size parameter
kS=|S|and a density parameter ηS. InG2, we consider a new notion of density, which we call “bid density”.
For any subset of reviewers R′⊆R, defineP[R′] =∪r∈R′{p∈P : (r,p)∈A}to be the subset of papers
authored by at least one reviewer in R′. The bid density of Sis then defined as the total number of bids
made by reviewers in Son papers inP[S], divided by the maximum possible number of bids by reviewers
inSon papers inP[S]:
ηS=|E(B)
2[S∪P [S]]|
|S||P [S]|−|E(A)
2[S∪P [S]]|−|E(C)
2[S∪P [S]]|.
We again will omit the subscript SfromkSandηSsince it will be clear from context.
We now analyze the problem of detecting collusion from G2. As in Section 4, the following three subsections
present empirical analysis aimed at answering the three research questions identified in Section 3.3.
5.1 Honest-Reviewer Groups (Q1)
First, we count the number of honest-reviewer groups in G2for each value of size kand bid density η. We
consider only reviewers that have authored at least one paper. Since the bid density for a group is naturally
lower than the edge density in general, we consider values of bid density ranging between 0.2and1.0.
Thecounts areshownin Figure4. Due tothemore complicateddefinitionofbid density, wecannotefficiently
prune branches of the search tree and must enumerate all/parenleftbig|R|
k/parenrightbig
reviewer subsets. As a result, the range of
feasiblekare significantly more limited than in Section 4.1: counts for k≥6for AAMAS and k≥5for
S2ORC could not be completed in 24 hours. In S2ORC, we see that numerous honest-reviewer groups do
exist at every feasible setting, up to η= 1.0andk= 4. Honest-reviewer groups are less prevalent in AAMAS,
but still exist at k= 5andη≤0.8. Like the figures in Section 4.1, these figures may be independently useful
to conferences as indicators of the number of false positives for any heuristic defenses against collusion they
use.
As in Section 4.1, we also run a greedy peeling method to heuristically find high-density reviewer groups of
larger sizes. However, this method performs very poorly and so we relegate the results to Appendix B.
12Published in Transactions on Machine Learning Research (12/2024)
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.00
±.000.01
±.000.02
±.000.01
±.000.02
±.000.02
±.000.02
±.000.38
±.071.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.08
±.030.94
±.031.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.01
±.000.02
±.000.02
±.000.02
±.000.03
±.000.03
±.000.06
±.020.56
±.040.69
±.010.70
±.010.71
±.010.72
±.01
0.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.03
±.000.02
±.000.02
±.000.03
±.000.02
±.000.03
±.000.02
±.000.03
±.000.03
±.000.03
±.00
0.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.03
±.000.02
±.000.02
±.000.03
±.000.03
±.000.03
±.000.03
±.00
(a) OQC-Greedy, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.00
±.000.01
±.000.01
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.29
±.060.94
±.031.00
±.001.00
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.00 (b) OQC-Greedy, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.01
±.000.01
±.000.01
±.000.02
±.000.03
±.000.07
±.010.11
±.000.14
±.000.16
±.000.53
±.060.96
±.021.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.01
±.000.01
±.000.02
±.000.03
±.000.04
±.000.06
±.010.11
±.010.14
±.000.17
±.000.19
±.000.34
±.040.76
±.050.96
±.020.99
±.000.99
±.000.99
±.00
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.04
±.000.05
±.000.07
±.010.10
±.010.14
±.010.17
±.010.21
±.000.22
±.000.25
±.000.30
±.020.46
±.04
0.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.03
±.000.03
±.000.04
±.000.04
±.000.05
±.000.07
±.000.07
±.010.09
±.010.10
±.010.12
±.010.14
±.010.17
±.01
0.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.03
±.000.03
±.000.03
±.000.04
±.000.04
±.000.05
±.000.05
±.000.05
±.000.06
±.000.07
±.000.08
±.00
(c) Fraudar, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.10
±.020.58
±.060.97
±.021.00
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.15
±.030.50
±.040.85
±.03
0.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.03
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.00 (d) Fraudar, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.01
±.000.01
±.000.01
±.000.06
±.030.31
±.050.52
±.050.57
±.050.54
±.050.72
±.040.71
±.040.76
±.030.80
±.020.75
±.030.82
±.010.81
±.020.79
±.020.82
±.00
0.00
±.000.01
±.000.01
±.000.03
±.020.10
±.040.32
±.060.50
±.060.53
±.060.69
±.060.74
±.050.80
±.040.80
±.040.83
±.030.83
±.030.82
±.030.85
±.030.86
±.02
0.00
±.000.01
±.000.01
±.000.03
±.020.04
±.020.02
±.000.19
±.050.45
±.070.54
±.070.53
±.070.70
±.060.78
±.050.77
±.050.72
±.060.79
±.050.82
±.050.95
±.02
0.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.09
±.040.04
±.020.08
±.030.14
±.040.23
±.060.53
±.070.40
±.070.59
±.070.68
±.060.68
±.06
0.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.03
±.000.02
±.000.02
±.000.02
±.000.03
±.000.03
±.000.03
±.00
(e) OQC-Specialized, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.03
±.020.09
±.030.03
±.020.20
±.050.15
±.040.23
±.050.26
±.050.23
±.050.28
±.060.28
±.050.35
±.060.36
±.06
0.00
±.000.00
±.000.01
±.000.00
±.000.00
±.000.01
±.010.01
±.000.00
±.000.07
±.030.14
±.040.19
±.050.23
±.050.23
±.050.26
±.060.28
±.060.31
±.060.34
±.06
0.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.00
±.000.00
±.000.01
±.000.00
±.000.03
±.020.06
±.030.06
±.030.14
±.050.12
±.040.18
±.050.17
±.050.27
±.06
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.03
±.020.01
±.000.01
±.000.01
±.000.03
±.02
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.00 (f) OQC-Specialized, S2ORC
Figure 5: Performance of detection algorithms on G2. Values indicate the mean Jaccard similarity between
the true set of colluders and the algorithm output, along with standard errors. Higher values correspond to
better detection performance.
5.2 Detection Algorithm Evaluation (Q2)
Next, we evaluate the performance of a variety of dense-subgraph discovery algorithms at detecting injected
collusion inG2, including an influential fraud-detection algorithm for the online review setting. Specifically,
we consider the following algorithms:
•DSD, OQC-Greedy, OQC-Local, and TellTail: These algorithms are introduced in Section 4.2. As
input to each algorithm, we provide an unattributed bipartite graph G= (V2,E)with the same
vertex set asG2. We consider two variants for the edge set of the input graph: the edge set consists
of only the bid edges E=E(B)
2, and the edge set consists of the union of the bid and authorship
edgesE=E(B)
2∪E(A)
2. In the OQC algorithms, we use the original objective function for undirected
graphsf(S) =|E[S]|−(1/3)/parenleftbig|S|
2/parenrightbig
.
•Fraudar (Hooi et al., 2016): This algorithm is designed to detect fake product reviews (e.g., on
Amazon/Yelp) or fake followers (e.g., on Twitter) from a bipartite graph of users and products G=
(V,E), whereEcontains edges from users to products. Denote by deg(v)the degree of a vertex vin
G. The returned subset of vertices S⊂Vis chosen to approximately maximize the objective function
f(S) =1
|S|/summationtext
(u,v)∈E[S](log(5 +deg(v)))−1. This objective is “camouflage-resistent”, meaning that
fraudulent users cannot affect their own objective value by adding extra edges to honest products.
In our setting, we consider the reviewers to be users and papers to be products. We again input
an unattributed bipartite graph with the same vertex set as G2and two variants for the edge set:
E=E(B)
2andE=E(B)
2∪E(A)
2.
13Published in Transactions on Machine Learning Research (12/2024)
•OQC-Specialized: We additionally adapt the objective function of the optimal quasi-clique discovery
algorithm to our setting in a natural way. For a subset of reviewers S⊆R, we define the objective
function
f(S) =|E(B)
2[S∪P [S]]|−α/parenleftig
|S||P [S]|
−|E(A)
2[S∪P [S]]|−|E(C)
2[S∪P [S]]|/parenrightig
.
Analogous to the concept of “edge surplus” that motivates the optimal quasi-clique objective func-
tion, this corresponds to the number of excess bids above the expectation if each bid is made
independently with probability α. We use local search to optimize this objective. As in the other
OQC algorithms, we set α= 1/3.
Since we aim to detect a group of colluding reviewers, we discard any paper vertices in the subset output by
each algorithm, considering only the output reviewer vertices as the detected reviewers.
In addition to the algorithms from Section 4.2, Fraudar is an influential fraud-detection algorithm that
leverages a density-based signal for detection. It was designed to detect fraudulent reviews on platforms like
Amazon, a similar problem to our own. OQC-Specialized was not proposed in prior work, but naturally
generalizes the objective function of OQC-Local to operate directly on G2.
We now describe the collusion model against which we evaluate the detection algorithms. For each value
of(k,η), we inject a group of colluding reviewers into G2as follows. We first choose a set of kreviewers,
denoted byM, uniformly at random from among all reviewers who have authored at least one paper. We
then choose a reviewer-paper pair uniformly at random from M×P [M]and add this pair to E(B)
2if there
is no existing edge of any type between this pair. We repeat this until the bid density is at least η. This
modified graph is then used to construct the input to each detection algorithm. We repeat this procedure for
50trials for each setting of parameters. We report the mean Jaccard similarity between the set of injected
colluders and the output set of reviewers from the detection algorithms.
As in Section 4.2, for each local search algorithm (OQC-Local, TellTail, OQC-Specialized), we return the
result with highest objective value over 11 initializations. The first initial subset is chosen according to a
generalization of the heuristic suggested by Tsourakakis et al. (2013). For each vertex, we count the number
of bid-author-bid-author cycles that this vertex participates in and divide by the total bid- and authorship-
degree of the vertex; we then choose the initial subset to be the maximum vertex according to this metric
along with all of its neighbors. The remaining 10 runs are initialized uniformly at random.
We show results for the OQC-Greedy, Fraudar, and OQC-Specialized algorithms in Figure 5. Results for the
remaining algorithms, which generally performed worse, are in Appendix B. For all algorithms (other than
OQC-Specialized), we find that the performance is very similar when the input edge set is E(B)
2and when
it isE(B)
2∪E(A)
2; thus, all results shown are for the case with edge set E(B)
2. On AAMAS, we see that both
OQC-Greedy and Fraudar are perform very well for high values of (k,η)(e.g.,k≥20,η≥0.8). However,
both of these algorithms fail to detect colluders entirely at more moderate values of k(e.g.,k= 14,η= 1.0).
OQC-Specialized achieves some success for a much wider range of parameters, but fails to achieve Jaccard
similarities above 0.9even for extreme values of (k,η); this may indicate that the true colluding group
is not a local optimum for this algorithm’s objective function. On S2ORC, performance of all algorithms
is significantly worse. OQC-Greedy and Fraudar still identify the colluding groups for extreme values of
(k,η), but OQC-Specialized fails to consistently identify the colluders for any parameter values. Overall,
there exists a significant region of moderate parameter values at which no algorithm achieves good detection
performance.
5.3 Manipulation Success Evaluation (Q3)
Finally, we evaluate the success of the colluders at manipulating the assignment as a function of the parame-
ters(k,η). For each setting of these parameters, we inject a group of colluders as described in the preceding
section and compute a paper assignment using the procedure detailed in Section 3.1. As in Section 4.3, we
14Published in Transactions on Machine Learning Research (12/2024)
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.04
±.010.09
±.010.15
±.010.20
±.010.23
±.010.26
±.010.27
±.010.30
±.010.33
±.010.38
±.010.38
±.010.41
±.010.42
±.010.42
±.010.45
±.010.48
±.010.49
±.01
0.03
±.010.09
±.020.14
±.010.16
±.010.21
±.010.23
±.010.26
±.010.30
±.010.31
±.010.34
±.010.37
±.010.37
±.010.38
±.010.40
±.010.42
±.010.44
±.010.46
±.01
0.03
±.010.07
±.010.12
±.010.15
±.010.17
±.010.23
±.010.23
±.010.26
±.010.27
±.010.30
±.010.31
±.010.33
±.010.34
±.010.37
±.010.38
±.010.40
±.010.42
±.01
0.05
±.020.05
±.010.09
±.010.11
±.010.15
±.010.16
±.010.18
±.010.21
±.010.24
±.010.25
±.010.26
±.010.29
±.010.29
±.010.32
±.010.33
±.010.35
±.010.36
±.01
0.02
±.010.05
±.010.06
±.010.08
±.010.10
±.010.14
±.010.13
±.010.14
±.010.17
±.010.19
±.010.19
±.010.21
±.010.22
±.010.24
±.010.25
±.010.26
±.010.29
±.01
(a) Fraction of target papers with a colluder assigned,
AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.03
±.010.08
±.020.12
±.010.14
±.010.16
±.010.19
±.010.20
±.010.22
±.010.22
±.010.24
±.010.26
±.010.28
±.010.27
±.010.29
±.010.29
±.010.31
±.010.30
±.01
0.02
±.010.07
±.010.12
±.010.14
±.010.16
±.020.18
±.010.20
±.010.20
±.010.21
±.010.21
±.010.25
±.010.27
±.010.26
±.010.28
±.010.28
±.010.29
±.010.29
±.01
0.02
±.010.06
±.010.07
±.010.13
±.010.14
±.010.15
±.010.17
±.010.19
±.010.19
±.010.22
±.010.22
±.010.23
±.010.25
±.010.24
±.010.26
±.010.26
±.010.27
±.01
0.04
±.020.04
±.010.06
±.010.08
±.010.09
±.010.11
±.010.13
±.010.14
±.010.17
±.010.17
±.010.18
±.010.20
±.010.22
±.010.20
±.010.21
±.010.23
±.010.22
±.01
0.02
±.010.01
±.010.04
±.010.06
±.010.06
±.010.07
±.010.09
±.010.10
±.010.11
±.010.11
±.010.13
±.010.13
±.010.15
±.010.14
±.010.16
±.010.16
±.010.16
±.01(b) Fraction of target papers with a colluder assigned,
S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.09
±.030.23
±.030.31
±.030.41
±.020.43
±.020.48
±.020.52
±.020.56
±.020.55
±.020.61
±.020.60
±.020.65
±.010.64
±.010.65
±.010.67
±.010.71
±.010.72
±.01
0.12
±.030.21
±.030.31
±.030.34
±.020.44
±.020.46
±.020.50
±.020.54
±.020.52
±.020.59
±.020.63
±.010.60
±.010.62
±.020.63
±.020.67
±.010.69
±.010.68
±.01
0.08
±.030.18
±.030.27
±.030.30
±.020.36
±.020.45
±.020.44
±.020.48
±.020.48
±.020.51
±.020.54
±.010.55
±.020.59
±.010.62
±.010.62
±.010.63
±.010.66
±.01
0.10
±.030.14
±.030.20
±.020.28
±.030.32
±.020.34
±.020.36
±.020.43
±.020.50
±.020.45
±.020.48
±.010.52
±.010.55
±.010.55
±.010.59
±.010.58
±.010.59
±.01
0.04
±.020.13
±.020.15
±.020.18
±.020.25
±.020.31
±.020.32
±.020.32
±.020.35
±.020.40
±.010.40
±.010.42
±.020.47
±.020.48
±.010.48
±.010.49
±.010.53
±.01
(c) Fraction of colluders with a colluder assigned,
AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.05
±.020.17
±.030.23
±.020.24
±.020.28
±.020.31
±.020.33
±.020.33
±.020.34
±.010.37
±.010.39
±.010.43
±.010.42
±.010.41
±.010.43
±.010.47
±.010.44
±.01
0.04
±.020.13
±.020.21
±.020.23
±.020.27
±.020.28
±.020.32
±.020.34
±.010.34
±.020.33
±.010.39
±.020.42
±.010.42
±.010.42
±.010.43
±.010.43
±.010.44
±.01
0.02
±.010.09
±.020.13
±.020.22
±.020.24
±.020.24
±.020.27
±.020.32
±.010.31
±.020.33
±.020.36
±.010.36
±.010.38
±.010.38
±.010.39
±.020.41
±.010.41
±.01
0.06
±.020.08
±.020.09
±.010.14
±.020.16
±.020.20
±.020.22
±.020.25
±.020.28
±.020.27
±.010.28
±.010.33
±.010.33
±.020.30
±.010.33
±.010.36
±.010.35
±.01
0.03
±.020.04
±.010.07
±.010.10
±.020.10
±.010.14
±.010.17
±.010.17
±.010.19
±.020.19
±.010.21
±.010.22
±.010.26
±.010.23
±.010.27
±.010.28
±.010.27
±.01(d) Fraction of colluders with a colluder assigned,
S2ORC
Figure 6: Success of colluders in terms of kandη. Values indicate the mean for each metric along with
standard errors.
evaluate the success of the colluders in two ways: the fraction of colluder-authored papers with at least one
colluder assigned, and the fraction of colluders with at least one colluder assigned to one of their authored
papers. We conduct 50trials for each setting of the parameters and report the mean for each of the two
metrics.
TheresultsareshowninFigure6. OnAAMAS,weseethatat (k= 16,η= 0.8)wherenodetectionalgorithm
was able to detect colluders with high probability, the colluders can successfully achieve assignments to 30%
of their target papers and 54%of the colluders. Similar results are seen elsewhere on the frontier of the
undetected region. On S2ORC, although the detection algorithms performed poorly, the success values are
also lower than in the corresponding case on AAMAS. Still, in the cell (k= 26,η= 0.8)where the detection
algorithms performed poorly, colluders can achieve assignment to 26%of target papers and 42%of colluders,
a sizeable influence on the paper assignment. We note that these success rates are quite similar to those
found in Section 4.3 for the unipartite graph setting, which may provide some indication that the ability
of undetected colluders to manipulate the assignment is robust to the exact graph representation. Overall,
colluders are still able exert influence over the acceptance decisions for a non-trivial fraction of their own
papers while avoiding detection.
6 Discussion
We provide an empirical exploration of the problem of detecting reviewer-author collusion rings from manip-
ulated bidding, framing the problem as a dense-subgraph discovery problem in two different graph represen-
tations. Overall, we find that within our parametrized model of colluder behavior, malicious reviewers can
manipulate the paper assignment to moderate success while remaining within typical or difficult-to-detect
levels of density in the bidding graph (using popular existing anomaly-detection algorithms). This provides
evidence to support the conclusion that malicious reviewer behavior cannot be effectively detected using just
the bidding data. While our analysis cannot conclusively prove that bidding data is insufficient to detect
collusion, our methodology thoroughly explores this problem from several different analyses of realistic con-
ference data. Our work considers only a specific parametrized model of collusive behavior in which colluders
receive reciprocal benefits within the conference in question. However, our results indicate that even this
behavior cannot be detected. In reality, colluders may be more sophisticated in their attempts to avoid
detection, making detection more challenging.
15Published in Transactions on Machine Learning Research (12/2024)
One limitation of our work is that our analysis is performed on semi-synthetic datasets, since real datasets
containing bidding and authorship data are not publicly available. Thus, it is possible that the bidding
in our datasets is not fully representative of real conference bidding: for example, in the AAMAS dataset,
honest reviewers with abnormal bidding patterns may have opted-out of data collection. To further verify
our results, program chairs could replicate our methodology on their own conference’s real bidding data.
Another limitation of our work is that we consider an intentionally narrow research question: the feasibility
of detection from just the bidding, without the use of other metadata. As one of the first works on detecting
collusion, our analysis of this question provides a basis for future work to build on.
While our results do not immediately direct any specific detection algorithm to be deployed in practice,
caution should be taken to avoid negative societal impacts from the misuse of any detection algorithms
deployed in the future. The cost of misidentifying an honest reviewer as part of a collusion ring can have
enormous consequences for their reputation and career. To prevent such an instance, conference program
chairs should at least conduct an associated thorough manual investigation before pursuing any form of
action against a suspected collusion ring.
An important contribution of our work is to provide direction for future work. Most clearly, future work can
consider the problem of detection in richer-featured settings than just the binary bidding setting we consider
here and demonstrate if detection is feasible in such settings. Some examples of additional features that
may be helpful are the strengths of bids, text-similarity scores, and past co-authorships. In these settings,
future work can evaluate other types of anomaly detection methods beyond the dense-subgraph discovery
algorithms we consider. This analysis would be very helpful for further directing the scope of future research
on malicious bidding–if such research finds negative results even with additional metadata, research ought
to continue to focus on mitigation-based techniques to address collusion rings. One major challenge for this
line of research is the lack of availability of high-quality, fully-featured conference data. Future work can help
address this need for datasets by collecting and releasing paper bidding data; one approach is to conduct an
experiment from which (labeled) data can be collected, as done by Jecmen et al. (2023) for collusion and by
Stelmakh et al. (2021) for a different problem of malicious behavior in peer review. Our work also provides
some direction for research on collusion mitigation by establishing the region of colluding behavior that can
be easily detected. Mitigation efforts need not be concerned with collusion in this region and can instead
focus on mitigation within the undetectable region, particularly in the area where honest-reviewer groups
do not exist.
Acknowledgments
This work was supported in part by ONR N000142212181, NSF CAREER Award 1942124, NSF IIS 2200410,
and NSF IIS 2310482.
References
Leman Akoglu, Rishi Chandy, and Christos Faloutsos. Opinion fraud detection in online reviews by network
effects.Proceedings of the International AAAI Conference on Web and Social Media , 2013.
Leman Akoglu, Hanghang Tong, and Danai Koutra. Graph based anomaly detection and description: A
survey.Data Mining and Knowledge Discovery , 29(3):626–688, 2015.
Noga Alon, Felix Fischer, Ariel Procaccia, and Moshe Tennenholtz. Sum of us: Strategyproof selection from
the selectors. In Conference on Theoretical Aspects of Rationality and Knowledge , pp. 101–110. ACM,
2011.
Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason
Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, et al. Construction of the literature graph in
Semantic Scholar. arXiv preprint arXiv:1805.02262 , 2018.
Stefano Balietti, Robert L. Goldstone, and Dirk Helbing. Peer review and competition in the art exhibition
game.Proceedings of the National Academy of Sciences , 113(30):8414–8419, 2016.
16Published in Transactions on Machine Learning Research (12/2024)
Niclas Boehmer, Robert Bredereck, and André Nichterlein. Combating collusion rings is hard but possible.
InProceedings of the AAAI Conference on Artificial Intelligence , volume 36, pp. 4843–4850, 2022.
Moses Charikar. Greedy approximation algorithms for finding dense components in a graph. In International
Workshop on Approximation Algorithms for Combinatorial Optimization , pp. 84–95. Springer, 2000.
Laurent Charlin and Richard S. Zemel. The Toronto Paper Matching System: An automated paper-reviewer
assignment system. In ICML Workshop on Peer Reviewing and Publishing Models , 2013.
Komal Dhull, Steven Jecmen, Pravesh Kothari, and Nihar B. Shah. The price of strategyproofing peer
assessment. In The 9th AAAI Conference on Human Computation and Crowdsourcing , volume 2, 2022.
Thorsten Eisenhofer, Erwin Quiring, Jonas Möller, Doreen Riepel, Thorsten Holz, and Konrad Rieck. No
more reviewer #2: Subverting automatic paper-reviewer assignment using adversarial learning. arXiv
preprint arXiv:2303.14443 , 2023.
Dhivya Eswaran, Stephan Günnemann, Christos Faloutsos, Disha Makhija, and M. Kumar. ZooBP: Belief
propagation for heterogeneous networks. Proc. VLDB Endow. , 10:625–636, 2017.
Andrew V. Goldberg. Finding a maximum density subgraph. University of California, Berkeley EECS
Department Technical Report , 1984.
Longhua Guo, Jie Wu, Wei Chang, Jun Wu, and Jianhua Li. K-loop free assignment in conference review
systems. In 2018 International Conference on Computing, Networking and Communications (ICNC) , pp.
542–547. IEEE, 2018.
BryanHooi, HyunAhSong, AlexBeutel, NeilShah, KijungShin, andChristosFaloutsos. Fraudar: Bounding
graph fraud in the face of camouflage. In Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining , pp. 895–904, 2016.
Bryan Hooi, Kijung Shin, Hemank Lamba, and Christos Faloutsos. TellTail: Fast scoring and detection
of dense subgraphs. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pp.
4150–4157, 2020.
Steven Jecmen, Hanrui Zhang, Ryan Liu, Nihar Shah, Vincent Conitzer, and Fei Fang. Mitigating manip-
ulation in peer review via randomized reviewer assignments. Advances in Neural Information Processing
Systems, 33:12533–12545, 2020.
Steven Jecmen, Nihar B Shah, Fei Fang, and Vincent Conitzer. Tradeoffs in preventing manipulation in
paper bidding for reviewer assignment. arXiv preprint arXiv:2207.11315 , 2022.
Steven Jecmen, Minji Yoon, Vincent Conitzer, Nihar B Shah, and Fei Fang. A dataset on malicious paper
bidding in peer review. In Proceedings of the ACM Web Conference 2023 , pp. 3816–3826, 2023.
Srijan Kumar, Bryan Hooi, Disha Makhija, Mohit Kumar, Christos Faloutsos, and V. S. Subrahmanian.
Rev2: Fraudulent user prediction in rating platforms. Proceedings of the Eleventh ACM International
Conference on Web Search and Data Mining , 2018.
Kevin Leyton-Brown, Mausam, Yatin Nandwani, Hedayat Zarkoob, Chris Cameron, Neil Newman, and
Dinesh Raghu. Matching papers and reviewers at large conferences. arXiv preprint arXiv:2202.12273 ,
2022.
Michael L Littman. Collusion rings threaten the integrity of computer science research. Communications of
the ACM , 64(6):43–44, 2021.
Ian Markwood, Dakun Shen, Yao Liu, and Zhuo Lu. Mirage: Content masking attack against information-
based online services. In 26th USENIX Security Symposium , pp. 833–847, 2017.
Nicholas Mattei and Toby Walsh. PrefLib: A library for preferences http://www. preflib. org. In Algorithmic
Decision Theory: Third International Conference , pp. 259–270. Springer, 2013.
17Published in Transactions on Machine Learning Research (12/2024)
Shashank Pandit, Duen Horng Chau, Samuel Wang, and Christos Faloutsos. NetProbe: A fast and scalable
system for fraud detection in online auction networks. In Proceedings of the 16th International Conference
on World Wide Web , pp. 201–210, 2007.
B Aditya Prakash, Ashwin Sridharan, Mukund Seshadri, Sridhar Machiraju, and Christos Faloutsos. Eigen-
Spokes: Surprising patterns and scalable community chipping in large graphs. In Pacific-Asia Conference
on Knowledge Discovery and Data Mining , pp. 435–448. Springer, 2010.
Nihar Shah, Behzad Tabibian, Krikamol Muandet, Isabelle Guyon, and Ulrike Von Luxburg. Design and
analysis of the NIPS 2016 review process. Journal of Machine Learning Research , 19:1–34, 2018.
Nihar B Shah. Challenges, experiments, and computational solutions in peer review. Communications of
the ACM , 65(6):76–87, 2022.
Ivan Stelmakh, Nihar B. Shah, and Aarti Singh. Catch me if I can: Detecting strategic behaviour in peer
assessment. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pp. 4794–4802,
2021.
Ivan Stelmakh, John Wieting, Graham Neubig, and Nihar B Shah. A gold standard dataset for the reviewer
assignment problem. arXiv preprint arXiv:2303.16750 , 2023.
Dat Tran and Chetan Jaiswal. PDFPhantom: Exploiting PDF attacks against academic conferences’ paper
submission process with counterattack. In 2019 IEEE 10th Annual Ubiquitous Computing, Electronics &
Mobile Communication Conference (UEMCON) , pp. 0736–0743. IEEE, 2019.
Charalampos Tsourakakis, Francesco Bonchi, Aristides Gionis, Francesco Gullo, and Maria Tsiarli. Denser
than the densest subgraph: Extracting optimal quasi-cliques with quality guarantees. In Proceedings of
the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pp. 104–112,
2013.
T. N. Vijaykumar. Potential organized fraud in ACM/IEEE computer architecture con-
ferences. https://medium.com/@tnvijayk/potential-organized-fraud-in-acm-ieee-computer-
architecture-conferences-ccd61169370d , 2020. Accessed January 15, 2024.
Paul Wilson. Academic fraud: Solving the crisis in modern academia. Exchanges: The Interdisciplinary
Research Journal , 7(3):14–44, 2020.
Ruihan Wu, Chuan Guo, Felix Wu, Rahul Kidambi, Laurens Van Der Maaten, and Kilian Weinberger.
Making paper reviewing robust to bid manipulation attacks. In International Conference on Machine
Learning , pp. 11240–11250. PMLR, 2021.
Yichong Xu, Han Zhao, Xiaofei Shi, and Nihar B. Shah. On strategyproof conference peer review. In
Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence , pp. 616–622,
2019. doi: 10.24963/IJCAI.2019/87. URL https://doi.org/10.24963/ijcai.2019/87 .
A Text Similarity Details
In this section, we provide more details about the text-similarity scores in our datasets, introduced in
Section3.2. We synthetically generatedtheAAMAStext-similarity scorestosupplement theoriginalbidding
dataset. The S2ORC text-similarity scores were included in the original dataset by Wu et al. (2021), and
were computed by running the widely-used TPMS algorithm on the abstracts of the reviewer’s past papers
and the paper in question. Figure 7 shows the CDFs of the text-similarity scores among the reviewer-paper
pairs that did not have conflicts-of-interest.
Since our work analyzes the effect of bids on the paper assignment, it is important to validate that our text
similarity scores have a realistic level of agreement with the bids. We do this by comparing to the results of
Stelmakh et al. (2023), who evaluate the accuracy of commonly-used text-similarity algorithms at predicting
reviewer expertise using a ground-truth dataset. Specifically, Stelmakh et al. (2023) evaluate the accuracy of
18Published in Transactions on Machine Learning Research (12/2024)
the text-similarity scores by considering reviewer-paper-paper triples (r,p1,p2)∈R×P×P where reviewer
rreported greater expertise on paper p1than on paper p2. They find that the TPMS algorithm produces a
weakly greater text-similarity score for (r,p1)than for (r,p2)on 80% of “easy” triples (where the reviewer
reported high-expertise vs low-expertise) and on 62% of “hard” triples (where the reviewer reported high-
expertise vs higher-expertise). We use these results to generate and/or validate the text-similarity scores in
our datasets, considering the bids to be a proxy for reviewer expertise.
For the AAMAS dataset, text-similarity scores were sampled i.i.d. from one of three Gaussian distributions,
depending on the bid value for that reviewer-paper pair (from {“Yes”, “Maybe”, “No response”}). For the
“No response” pairs, the Gaussian distribution was fit to the dataset of text-similarities reconstructed from
the 2018 International Conference on Learning Representations by Xu et al. (2019). The variance of the
Gaussian distributions for the “Maybe” and “Yes” pairs were the same, but the means were chosen based
on the statistics from Stelmakh et al. (2023). Specifically, we set the means such that in expectation: (a)
80% of the triples (r,p1,p2)∈R×P×P whererbid “Yes” or “Maybe” on p1andrbid “No response” on
p2hadT(r,p1)≥T(r,p2); and (b) 62% of the triples (r,p1,p2)∈R×P×P whererbid “Yes” on p1andr
bid “Maybe” on p2hadT(r,p1)≥T(r,p2).
For the S2ORC dataset, we compare the text-similarity scores in the dataset to the statistics from Stelmakh
et al. (2023). Recall that the S2ORC dataset contains bid values for each reviewer-paper pair in {0,1,2,3}.
We find that 83% of the triples (r,p1,p2)∈R×P×P whererbid from {1, 2, 3} on p1andrbid 0 onp2
hadT(r,p1)≥T(r,p2). Additionally, we find that 65% of the triples (r,p1,p2)∈R×P×P whererbid 3
onp1andrbid {1, 2} on p2hadT(r,p1)≥T(r,p2). We see that the accuracy of the text-similarities in the
S2ORC dataset is similar to that of the ground-truth dataset.
B Omitted Experimental Results
In Figure 8, we provide evaluations of detection algorithms on G1omitted from Section 4.2. This includes the
OQC-Greedy algorithm, as well as the OQC-Local algorithm with all initializations (detailed in Section 4.2).
These algorithms generally perform worse than those with results shown in Section 4.2.
In Figure 9, we show the size and bid density of honest-reviewer groups detected by a greedy peeling method
onG2omitted from Section 5.1. In this method, we begin with the entire set of reviewers. At each iteration,
we greedily remove the reviewer whose removal results in the highest bid density within the set of remaining
reviewers. We then plot the bid density for each group size across the iterations. Clearly, this method
performs very poorly at finding groups of high density.
In Figure 10, we present the results of the detection algorithms on G2omitted from Section 5.2: DSD, OQC-
Local, and TellTail. These algorithms generally perform worse than those with results shown in Section 5.2.
0.00 0.02 0.04 0.06 0.08 0.10 0.12
text-similarity score0.00.20.40.60.81.0
(a) AAMAS
0.0 0.2 0.4 0.6 0.8 1.0
text-similarity score0.00.20.40.60.81.0 (b) S2ORC
Figure 7: CDF of the text-similarity scores for reviewer-paper pairs without a conflict-of-interest. The mean
text similarity score among these pairs is 0.030for AAMAS and 0.52for S2ORC.
19Published in Transactions on Machine Learning Research (12/2024)
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.03
±.000.04
±.000.04
±.000.04
±.000.05
±.000.40
±.040.60
±.040.70
±.040.85
±.030.89
±.030.90
±.03
0.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.03
±.000.03
±.000.03
±.000.04
±.000.04
±.000.04
±.000.05
±.000.11
±.020.41
±.040.80
±.040.87
±.030.97
±.02
0.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.03
±.000.03
±.000.03
±.000.04
±.000.04
±.000.04
±.000.06
±.000.06
±.000.06
±.000.07
±.020.65
±.060.92
±.03
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.03
±.000.04
±.000.04
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.06
±.000.06
±.000.07
±.00
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.03
±.000.03
±.000.03
±.000.04
±.000.04
±.000.04
±.000.05
±.000.06
±.000.05
±.000.07
±.000.07
±.00
0.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.03
±.000.03
±.000.04
±.000.04
±.000.04
±.000.04
±.000.05
±.000.05
±.000.06
±.000.06
±.000.06
±.00
(a) OQC-Greedy, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.92
±.030.99
±.011.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.07
±.030.93
±.030.99
±.011.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.01
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.50
±.070.99
±.011.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.01
±.000.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.01
±.000.01
±.000.01
±.000.10
±.040.66
±.060.97
±.021.00
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.01
±.000.01
±.000.05
±.030.45
±.070.98
±.001.00
±.00
0.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.00 (b) OQC-Greedy, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.00
±.000.01
±.000.02
±.000.02
±.000.02
±.000.03
±.000.04
±.000.04
±.000.04
±.000.05
±.000.05
±.000.05
±.000.06
±.010.09
±.010.75
±.050.93
±.020.97
±.01
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.03
±.000.04
±.000.04
±.000.05
±.010.05
±.000.05
±.000.06
±.000.07
±.010.08
±.010.54
±.060.95
±.02
0.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.03
±.000.04
±.000.04
±.000.04
±.000.04
±.000.05
±.000.06
±.000.05
±.000.06
±.000.07
±.000.10
±.02
0.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.03
±.000.04
±.000.04
±.000.05
±.000.04
±.000.05
±.000.06
±.000.06
±.000.06
±.000.06
±.000.08
±.01
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.03
±.000.03
±.000.04
±.000.04
±.000.05
±.000.05
±.000.05
±.000.06
±.000.06
±.000.06
±.000.06
±.00
0.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.03
±.000.04
±.000.04
±.000.04
±.000.04
±.000.05
±.000.05
±.000.06
±.000.06
±.000.06
±.000.07
±.00
(c) OQC-Local (including random initializations),
AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.03
±.020.07
±.030.43
±.070.37
±.071.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.03
±.020.07
±.030.15
±.050.41
±.070.35
±.071.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.05
±.030.13
±.050.29
±.060.47
±.071.00
±.000.98
±.020.98
±.021.00
±.001.00
±.00
0.00
±.000.00
±.000.01
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.09
±.040.17
±.050.25
±.060.27
±.060.90
±.040.88
±.050.92
±.040.94
±.03
0.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.03
±.020.03
±.020.13
±.050.29
±.060.23
±.060.69
±.070.73
±.06
0.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.04
±.020.05
±.030.04
±.020.05
±.03(d) OQC-Local (including random initializations),
S2ORC
Figure 8: Performance of additional detection algorithms on G1. Values indicate the mean Jaccard similarity
between the true set of colluders and the algorithm output, along with standard errors. Higher values
correspond to better detection performance.
2468101214161820222426283032340.00.20.40.60.81.0bid density ()
(a) AAMAS
2468101214161820222426283032340.00.20.40.60.81.0bid density ()
 (b) S2ORC
Figure 9: Size and bid density of honest-reviewer groups found by a heuristic method on G2. Each point
corresponds to an existing honest-reviewer group (found by a greedy peeling method), and the shaded area
indicates the region in which there exists at least one honest-reviewer group.
C Density of Detected Groups
One question relevant to the analyses in Sections 4.2 and 5.2 is why the detection algorithms fail to detect
the true colluders. Specifically, one may be interested in distinguishing the scenarios in which the detection
algorithms output a group of reviewers less-dense than the true colluders (indicating that the colluders could
potentially be detected if a better algorithm was developed) from the scenarios in which the detection algo-
rithms output a group of reviewers with higher-density (indicating that it will be difficult for any algorithm
to detect the colluders). In the following plots, we show the mean (edge or bid) density of the groups output
by the detection algorithms when colluding groups of varying size and density are injected. Note that the
detection algorithms are not restricted to outputting groups of the same size as the injected colluders.
The results for all algorithms are shown in Figure 11 (unipartite setting) and Figure 12 (bipartite setting).
Overall, we generally see that in the (lower-left) region of the plots in which the algorithms fail to detect
the true colluders (according to Figures 2 and 5), the algorithms are instead detecting reviewer groups with
low density. This indicates that their objective may not be aligned well with the profile of the true colluders
in this region. However, in the unipartite setting for the S2ORC dataset, TellTail detects a highly-dense
group of honest reviewers when the colluding group is small and less-dense. This provides some additional
20Published in Transactions on Machine Learning Research (12/2024)
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.00
±.000.01
±.000.01
±.000.02
±.000.03
±.000.04
±.000.11
±.000.14
±.000.19
±.020.61
±.050.97
±.021.00
±.000.99
±.000.99
±.000.99
±.001.00
±.001.00
±.00
0.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.06
±.010.09
±.010.15
±.000.17
±.000.20
±.000.36
±.040.85
±.040.98
±.000.99
±.000.99
±.000.99
±.00
0.00
±.000.01
±.000.01
±.000.02
±.000.03
±.000.03
±.000.03
±.000.05
±.000.06
±.010.12
±.010.17
±.010.19
±.000.22
±.000.24
±.000.27
±.000.35
±.020.58
±.04
0.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.03
±.000.04
±.000.04
±.000.06
±.000.06
±.010.08
±.010.14
±.010.20
±.010.23
±.000.24
±.01
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.03
±.000.03
±.000.04
±.000.03
±.000.04
±.000.05
±.000.05
±.000.05
±.000.05
±.000.06
±.000.06
±.00
(a) DSD, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.00
±.000.01
±.000.02
±.000.06
±.010.49
±.060.91
±.040.99
±.011.00
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.04
±.010.15
±.030.48
±.050.90
±.031.00
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.00
±.000.01
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.05
±.01
0.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.00 (b) DSD, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.00
±.000.01
±.000.01
±.000.01
±.000.11
±.040.19
±.050.33
±.060.39
±.060.75
±.060.51
±.070.74
±.060.82
±.050.86
±.040.91
±.030.81
±.050.96
±.020.98
±.02
0.00
±.000.00
±.000.01
±.000.02
±.000.01
±.000.07
±.030.16
±.050.34
±.060.21
±.050.40
±.070.54
±.070.47
±.070.51
±.070.64
±.060.75
±.050.77
±.050.84
±.05
0.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.01
±.000.01
±.000.02
±.000.05
±.020.07
±.030.11
±.030.10
±.030.16
±.040.22
±.040.21
±.040.23
±.040.35
±.05
0.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.01
±.000.03
±.000.03
±.000.02
±.000.03
±.000.02
±.000.03
±.000.03
±.000.03
±.00
0.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.01
±.000.02
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.03
±.000.03
±.000.03
±.000.03
±.00
(c) OQC-Local, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.04
±.030.06
±.030.04
±.030.11
±.040.10
±.040.19
±.050.12
±.050.10
±.040.16
±.050.15
±.05
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.03
±.020.01
±.000.05
±.030.01
±.000.13
±.050.18
±.05
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.00
±.000.00
±.000.01
±.000.00
±.000.00
±.000.01
±.000.00
±.000.01
±.000.01
±.000.00
±.000.01
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.00
±.000.01
±.000.00
±.00 (d) OQC-Local, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.03
±.010.09
±.030.84
±.051.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.03
±.000.03
±.000.03
±.000.02
±.000.08
±.030.68
±.060.92
±.040.98
±.021.00
±.001.00
±.001.00
±.00
0.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.03
±.000.03
±.000.03
±.000.03
±.000.03
±.000.04
±.000.03
±.000.06
±.020.43
±.070.94
±.03
0.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.03
±.000.03
±.000.03
±.000.03
±.000.04
±.000.04
±.000.04
±.000.04
±.000.04
±.00
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.02
±.000.03
±.000.03
±.000.03
±.000.03
±.000.04
±.000.04
±.000.04
±.000.04
±.000.05
±.000.05
±.00
(e) TellTail, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.00
±.000.01
±.000.01
±.000.01
±.000.02
±.000.03
±.010.09
±.030.10
±.030.15
±.040.31
±.060.37
±.05
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.000.02
±.000.02
±.000.04
±.010.10
±.030.17
±.05
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.02
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.01
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.000.01
±.000.00
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.000.01
±.00 (f) TellTail, S2ORC
Figure10: Performanceofadditionaldetectionalgorithmson G2. ValuesindicatethemeanJaccardsimilarity
between the true set of colluders and the algorithm output, along with standard errors. Higher values
correspond to better detection performance.
evidence that colluder detection in the region of ( k≤14,γ≤0.8) is not feasible, due to the presence of a
highly-dense group of honest reviewers that can mislead the detection algorithms.
D Experimental Results with Variant Datasets
As we detail in Section 3.2, the datasets we analyze in this work are semi-synthetic. The AAMAS dataset
does not contain authorships, and so we construct authorships by subsampling three conflicts-of-interest
for each paper. The S2ORC dataset contains bids that were synthetically constructed by Wu et al. (2021)
based on bidding statistics from NeurIPS 2016. In this section, we vary these synthetic aspects and conduct
additional experiments on these variant datasets.
In the variant AAMAS dataset, we construct authorships as follows. For each paper, instead of subsampling
three conflicts-of-interest uniformly at random, we choose an integer from {1,..., 5}uniformly at random
and subsample that many conflicts-of-interest to use as the authorships. In the variant S2ORC dataset,
we modify the original bids by removing 15,000 positive bids (over 10% of the positive bids) uniformly at
random and adding positive bids among 15,000 non-bidding pairs uniformly at random.
The results of the experiments on these variant datasets are shown in Figures 13-14 (unipartite setting)
and Figures 15-16 (bipartite setting). In general, we see that the results are very similar to those obtained
with the original datasets; for comparison, see Figures 2-3, Figures 5-6, and Figures 8-10. Notably, in the
unipartite setting, the TellTail, OQC-Greedy, and OQC-Local detection algorithms perform slightly better
on both variant datasets than on the original datasets. In the middle region where k= 15andγ≥0.8, these
algorithms are able to achieve moderate-to-high success rates, unlike on the original datasets.
21Published in Transactions on Machine Learning Research (12/2024)
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.12
±.000.12
±.000.12
±.000.12
±.000.12
±.000.12
±.000.12
±.000.12
±.00
0.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.12
±.000.12
±.000.12
±.000.12
±.000.12
±.000.12
±.000.12
±.00
0.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.12
±.000.12
±.000.12
±.000.12
±.000.12
±.000.12
±.000.12
±.00
0.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.12
±.000.12
±.000.12
±.000.12
±.000.12
±.00
0.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.12
±.000.12
±.000.12
±.000.12
±.00
0.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.11
±.000.12
±.000.12
±.00
(a) DSD, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.89
±.041.00
±.001.00
±.001.00
±.00
0.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.04
±.000.04
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.90
±.000.90
±.00
0.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.04
±.000.04
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.00
0.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.04
±.000.04
±.000.04
±.000.04
±.000.04
±.000.05
±.000.05
±.00
0.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.04
±.000.04
±.000.04
±.000.04
±.000.04
±.000.04
±.00
0.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.05
±.000.04
±.000.04
±.000.04
±.000.04
±.00 (b) DSD, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.010.31
±.030.53
±.050.51
±.050.63
±.050.74
±.050.82
±.040.82
±.040.91
±.030.88
±.03
0.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.26
±.000.33
±.030.34
±.030.42
±.040.51
±.040.59
±.040.71
±.040.72
±.040.75
±.03
0.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.26
±.010.25
±.010.25
±.010.34
±.030.40
±.040.43
±.040.49
±.040.51
±.04
0.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.28
±.000.27
±.000.27
±.000.27
±.000.28
±.000.27
±.000.26
±.010.25
±.010.26
±.010.28
±.020.29
±.020.32
±.02
0.27
±.000.27
±.000.27
±.000.27
±.000.28
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.26
±.000.27
±.010.26
±.010.25
±.010.24
±.01
0.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.27
±.000.26
±.000.26
±.01
(c) OQC-Local, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.42
±.000.42
±.000.42
±.000.42
±.000.43
±.010.55
±.040.78
±.050.90
±.030.94
±.031.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.42
±.000.42
±.000.42
±.000.42
±.000.42
±.000.43
±.010.56
±.030.72
±.040.78
±.030.87
±.020.86
±.020.89
±.010.89
±.010.90
±.000.90
±.000.90
±.000.90
±.00
0.42
±.000.42
±.000.42
±.000.42
±.000.42
±.010.42
±.000.43
±.010.45
±.030.61
±.030.68
±.030.74
±.020.76
±.020.77
±.020.80
±.000.77
±.020.80
±.000.80
±.00
0.42
±.000.42
±.000.42
±.000.42
±.000.42
±.000.42
±.000.42
±.010.43
±.010.46
±.020.46
±.030.52
±.030.50
±.030.54
±.030.64
±.020.62
±.020.67
±.020.67
±.02
0.42
±.000.42
±.000.42
±.000.42
±.000.42
±.000.42
±.000.42
±.000.42
±.010.42
±.000.43
±.010.42
±.010.39
±.020.42
±.020.42
±.030.40
±.020.49
±.020.49
±.02
0.42
±.000.42
±.000.42
±.000.42
±.000.42
±.000.42
±.000.42
±.000.42
±.000.42
±.000.41
±.010.41
±.010.41
±.010.40
±.010.41
±.010.39
±.010.37
±.020.34
±.02 (d) OQC-Local, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.93
±.020.98
±.011.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.68
±.020.90
±.000.90
±.000.90
±.000.89
±.010.90
±.000.90
±.000.90
±.000.90
±.000.90
±.00
0.58
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.63
±.010.79
±.010.80
±.000.80
±.000.80
±.000.80
±.000.80
±.000.80
±.00
0.57
±.000.57
±.000.58
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.58
±.000.58
±.010.67
±.010.70
±.000.70
±.000.70
±.00
0.57
±.000.57
±.000.57
±.000.58
±.000.58
±.000.57
±.000.58
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.00
0.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.58
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.58
±.000.57
±.00
(e) TellTail, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.84
±.000.84
±.000.84
±.000.84
±.000.79
±.020.70
±.040.82
±.040.95
±.020.99
±.010.99
±.011.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.84
±.000.84
±.000.84
±.000.84
±.000.82
±.010.76
±.030.78
±.030.70
±.030.86
±.020.89
±.010.88
±.010.89
±.010.90
±.000.90
±.000.90
±.000.90
±.000.90
±.00
0.84
±.000.84
±.000.84
±.000.84
±.000.84
±.000.82
±.010.75
±.030.60
±.040.64
±.030.71
±.020.75
±.020.79
±.010.78
±.010.80
±.000.80
±.000.80
±.000.80
±.00
0.84
±.000.84
±.000.84
±.000.84
±.000.84
±.000.84
±.000.84
±.000.80
±.020.68
±.040.47
±.040.55
±.030.52
±.030.60
±.020.66
±.010.68
±.010.69
±.010.70
±.00
0.84
±.000.84
±.000.84
±.000.84
±.000.84
±.000.84
±.000.84
±.000.84
±.000.82
±.010.75
±.030.68
±.030.55
±.040.45
±.040.45
±.030.51
±.020.55
±.020.60
±.02
0.84
±.000.84
±.000.84
±.000.84
±.000.84
±.000.84
±.000.84
±.000.84
±.000.84
±.000.82
±.010.84
±.000.82
±.010.73
±.030.68
±.040.55
±.040.56
±.040.38
±.04 (f) TellTail, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.52
±.030.64
±.040.72
±.040.85
±.030.89
±.030.90
±.03
0.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.49
±.010.54
±.020.74
±.030.79
±.030.87
±.01
0.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.49
±.010.67
±.020.75
±.02
0.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.00
0.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.00
0.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.00
(g) OQC-Greedy, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.92
±.030.99
±.011.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.54
±.010.88
±.010.89
±.010.90
±.000.90
±.000.90
±.000.90
±.000.90
±.000.90
±.00
0.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.67
±.020.80
±.010.80
±.000.80
±.000.80
±.000.80
±.000.80
±.00
0.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.55
±.010.65
±.010.70
±.000.70
±.000.70
±.000.70
±.00
0.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.53
±.000.59
±.010.61
±.000.60
±.00
0.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.000.52
±.00 (h) OQC-Greedy, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.47
±.000.47
±.000.47
±.000.47
±.000.48
±.000.48
±.000.47
±.000.48
±.000.48
±.000.47
±.000.48
±.000.47
±.000.47
±.000.47
±.000.81
±.040.93
±.020.97
±.01
0.47
±.000.47
±.000.47
±.000.48
±.000.48
±.000.47
±.000.48
±.000.48
±.000.47
±.000.47
±.000.48
±.000.48
±.000.48
±.000.47
±.000.47
±.000.66
±.030.85
±.02
0.47
±.000.48
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.48
±.000.47
±.000.48
±.000.47
±.000.47
±.000.48
±.000.48
±.000.48
±.01
0.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.48
±.000.47
±.000.48
±.000.48
±.000.48
±.000.48
±.000.47
±.00
0.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.48
±.000.48
±.000.47
±.000.48
±.000.47
±.000.47
±.000.48
±.000.48
±.000.48
±.00
0.47
±.000.47
±.000.47
±.000.48
±.000.48
±.000.48
±.000.48
±.000.48
±.000.47
±.000.47
±.000.47
±.000.48
±.000.47
±.000.47
±.000.48
±.000.48
±.000.48
±.00
(i) OQC-Local (including random initializations),
AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.9
0.8
0.7
0.6
0.5edge density ()
0.51
±.000.50
±.000.50
±.000.50
±.000.51
±.000.51
±.000.52
±.010.54
±.020.72
±.030.69
±.031.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.50
±.000.50
±.000.51
±.000.51
±.000.51
±.000.50
±.000.52
±.010.54
±.010.57
±.020.67
±.030.65
±.030.90
±.000.89
±.010.90
±.000.90
±.000.90
±.000.90
±.00
0.50
±.000.50
±.000.50
±.000.51
±.000.51
±.000.51
±.000.50
±.000.51
±.000.52
±.010.55
±.010.60
±.020.65
±.020.80
±.000.79
±.010.79
±.010.80
±.000.80
±.00
0.51
±.000.50
±.000.51
±.000.50
±.000.51
±.000.50
±.000.50
±.000.51
±.000.51
±.000.52
±.010.54
±.010.56
±.010.57
±.010.68
±.010.67
±.010.68
±.010.69
±.01
0.52
±.000.50
±.000.51
±.000.51
±.000.51
±.000.51
±.000.51
±.000.50
±.000.51
±.000.51
±.000.51
±.000.51
±.000.52
±.000.54
±.010.53
±.010.57
±.010.58
±.01
0.50
±.000.51
±.000.51
±.000.50
±.000.50
±.000.51
±.000.50
±.000.50
±.000.50
±.000.50
±.000.50
±.000.51
±.000.50
±.000.51
±.000.51
±.000.51
±.000.51
±.00(j) OQC-Local (including random initializations),
S2ORC
Figure 11: Density of detected groups on G1. Values indicate the mean edge density of the group of reviewers
output by the algorithm, along with standard errors.
22Published in Transactions on Machine Learning Research (12/2024)
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.38
±.000.38
±.000.38
±.000.37
±.000.38
±.000.38
±.000.37
±.000.59
±.041.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.38
±.000.38
±.000.38
±.000.38
±.000.37
±.000.37
±.000.38
±.000.37
±.000.38
±.020.78
±.020.81
±.000.82
±.000.82
±.000.82
±.000.82
±.000.82
±.000.82
±.00
0.38
±.000.38
±.000.38
±.000.38
±.000.38
±.000.38
±.000.37
±.000.37
±.000.37
±.010.37
±.010.36
±.010.37
±.010.57
±.020.64
±.000.63
±.000.63
±.000.64
±.00
0.38
±.000.38
±.000.38
±.000.38
±.000.38
±.000.38
±.000.38
±.000.37
±.010.38
±.000.37
±.000.36
±.010.36
±.010.37
±.000.36
±.010.36
±.010.35
±.010.35
±.01
0.38
±.000.38
±.000.38
±.000.38
±.000.38
±.000.38
±.000.38
±.000.38
±.000.38
±.000.38
±.000.38
±.000.37
±.000.38
±.000.37
±.000.37
±.000.38
±.000.37
±.01
(a) OQC-Greedy, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.53
±.030.78
±.010.81
±.000.81
±.00
0.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.00
0.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.00
0.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.000.41
±.00 (b) OQC-Greedy, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.14
±.000.51
±.060.96
±.021.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.12
±.000.12
±.000.13
±.000.14
±.000.14
±.000.26
±.030.61
±.040.79
±.020.81
±.000.82
±.000.82
±.00
0.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.12
±.000.10
±.010.09
±.010.10
±.010.12
±.000.14
±.000.14
±.000.15
±.000.18
±.010.28
±.03
0.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.12
±.000.10
±.010.08
±.010.07
±.000.07
±.000.08
±.000.08
±.010.09
±.01
0.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.11
±.000.09
±.01
(c) Fraudar, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.09
±.020.55
±.060.97
±.021.00
±.001.00
±.001.00
±.00
0.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.11
±.020.36
±.030.67
±.03
0.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.00
0.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.00
0.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.000.02
±.00 (d) Fraudar, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.47
±.000.47
±.000.47
±.000.49
±.010.59
±.020.67
±.020.70
±.020.68
±.020.76
±.020.75
±.020.77
±.010.79
±.010.77
±.010.79
±.010.79
±.010.77
±.010.78
±.01
0.47
±.000.47
±.000.47
±.000.48
±.010.50
±.010.56
±.020.61
±.020.61
±.020.67
±.020.68
±.010.69
±.010.69
±.010.70
±.010.70
±.010.70
±.010.71
±.010.71
±.01
0.47
±.000.47
±.000.47
±.000.48
±.000.48
±.000.47
±.000.50
±.010.52
±.010.54
±.010.54
±.010.56
±.010.57
±.010.57
±.010.57
±.010.58
±.010.59
±.010.61
±.00
0.47
±.000.47
±.000.47
±.000.48
±.000.47
±.000.47
±.000.47
±.000.46
±.000.47
±.000.47
±.000.47
±.000.47
±.000.45
±.000.45
±.000.45
±.000.44
±.000.44
±.00
0.48
±.000.47
±.000.47
±.000.48
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.000.47
±.00
(e) OQC-Specialized, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.57
±.000.57
±.000.57
±.000.58
±.000.58
±.000.57
±.010.58
±.010.59
±.010.63
±.020.61
±.010.62
±.010.65
±.020.63
±.010.65
±.020.64
±.020.67
±.020.66
±.02
0.57
±.000.57
±.000.57
±.000.58
±.000.57
±.000.57
±.000.57
±.000.57
±.000.58
±.010.59
±.010.59
±.010.61
±.010.59
±.010.60
±.010.61
±.010.61
±.010.62
±.01
0.57
±.000.58
±.000.58
±.000.57
±.000.57
±.000.58
±.000.58
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.010.58
±.000.57
±.000.57
±.000.58
±.000.57
±.00
0.57
±.000.58
±.000.58
±.000.57
±.000.57
±.000.58
±.000.58
±.000.57
±.000.57
±.000.58
±.000.57
±.000.56
±.000.57
±.010.56
±.010.58
±.000.57
±.000.57
±.00
0.57
±.000.57
±.000.57
±.000.57
±.000.58
±.000.58
±.000.57
±.000.57
±.000.58
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.000.57
±.01 (f) OQC-Specialized, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.14
±.000.17
±.020.59
±.060.96
±.021.00
±.001.00
±.000.99
±.000.99
±.001.00
±.001.00
±.00
0.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.14
±.000.15
±.000.28
±.030.69
±.030.81
±.000.81
±.000.81
±.000.81
±.00
0.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.14
±.000.15
±.000.16
±.000.21
±.020.35
±.03
0.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.12
±.00
0.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.000.13
±.00
(g) DSD, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.33
±.010.27
±.020.50
±.060.91
±.040.99
±.011.00
±.001.00
±.001.00
±.00
0.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.35
±.010.25
±.020.23
±.020.38
±.040.73
±.030.80
±.00
0.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.010.36
±.010.36
±.000.34
±.010.26
±.02
0.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.00
0.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.000.36
±.00 (h) DSD, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.28
±.010.30
±.010.30
±.010.28
±.010.36
±.030.43
±.040.51
±.040.51
±.050.81
±.040.61
±.050.79
±.040.84
±.030.88
±.030.90
±.020.84
±.040.95
±.010.99
±.01
0.30
±.010.28
±.010.29
±.010.31
±.010.29
±.010.32
±.020.37
±.030.48
±.030.38
±.030.49
±.030.58
±.040.51
±.040.53
±.040.60
±.030.65
±.030.67
±.030.72
±.03
0.28
±.010.29
±.010.27
±.010.27
±.010.28
±.010.29
±.010.28
±.010.28
±.010.32
±.010.31
±.010.34
±.020.32
±.020.35
±.020.38
±.020.39
±.020.39
±.020.45
±.02
0.30
±.010.28
±.010.28
±.010.29
±.010.29
±.010.28
±.010.29
±.010.28
±.010.28
±.010.29
±.010.28
±.010.29
±.010.28
±.010.29
±.010.29
±.010.28
±.010.28
±.01
0.30
±.010.29
±.010.27
±.010.29
±.010.28
±.010.28
±.010.27
±.010.28
±.010.27
±.010.28
±.010.29
±.010.28
±.010.27
±.010.28
±.010.30
±.010.29
±.010.28
±.01
(i) OQC-Local, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.29
±.020.31
±.010.30
±.010.29
±.020.29
±.020.31
±.020.28
±.020.32
±.030.33
±.030.31
±.020.36
±.030.35
±.030.41
±.040.37
±.040.33
±.030.40
±.040.38
±.04
0.30
±.020.29
±.020.31
±.010.29
±.020.29
±.020.30
±.020.30
±.020.30
±.020.28
±.020.30
±.010.28
±.020.28
±.020.27
±.020.33
±.020.29
±.020.31
±.030.36
±.03
0.31
±.020.29
±.020.28
±.010.30
±.010.30
±.020.25
±.010.27
±.020.27
±.020.32
±.020.29
±.020.30
±.020.31
±.020.25
±.020.26
±.020.29
±.020.28
±.020.28
±.02
0.29
±.010.29
±.020.27
±.020.28
±.020.27
±.020.28
±.010.29
±.010.28
±.020.29
±.020.29
±.020.27
±.020.31
±.020.26
±.020.31
±.010.30
±.010.30
±.020.27
±.02
0.29
±.020.28
±.020.31
±.020.27
±.020.28
±.020.26
±.020.27
±.020.29
±.020.31
±.010.28
±.020.28
±.020.28
±.020.27
±.020.28
±.020.28
±.020.30
±.020.29
±.02 (j) OQC-Local, S2ORC
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.21
±.030.85
±.051.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.001.00
±.00
0.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.20
±.020.59
±.040.76
±.030.81
±.010.82
±.000.82
±.000.82
±.00
0.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.17
±.010.35
±.030.61
±.02
0.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.00
0.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.000.16
±.00
(k) TellTail, AAMAS
246810121416182022242628303234
number of reviewers (k)1.0
0.8
0.6
0.4
0.2bid density ()
0.08
±.000.07
±.000.08
±.000.07
±.000.08
±.000.07
±.000.08
±.000.08
±.000.07
±.000.08
±.000.08
±.000.09
±.000.12
±.030.15
±.030.17
±.030.32
±.060.34
±.05
0.07
±.000.08
±.000.07
±.000.08
±.000.08
±.000.08
±.000.07
±.000.08
±.000.08
±.000.07
±.000.08
±.000.08
±.000.08
±.000.08
±.000.08
±.000.13
±.030.18
±.03
0.08
±.000.07
±.000.08
±.000.08
±.000.08
±.000.07
±.000.08
±.000.08
±.000.07
±.000.08
±.000.08
±.000.08
±.000.08
±.000.07
±.000.07
±.000.08
±.000.08
±.00
0.08
±.000.08
±.000.08
±.000.07
±.000.08
±.000.08
±.000.08
±.000.07
±.000.07
±.000.08
±.010.07
±.000.08
±.000.07
±.000.08
±.000.08
±.000.07
±.000.07
±.00
0.08
±.000.08
±.000.08
±.000.08
±.000.07
±.000.08
±.000.08
±.000.08
±.000.08
±.000.07
±.000.07
±.000.07
±.000.07
±.000.07
±.000.07
±.000.08
±.000.07
±.00 (l) TellTail, S2ORC
Figure 12: Density of detected groups on G2. Values indicate the mean bid density of the group of reviewers
output by the algorithm, along with standard errors.
23Published in Transactions on Machine Learning Research (12/2024)
5 10 15 20 25
number of reviewers (k)1.0
0.8
0.6edge density ()
0.01
±.000.03
±.000.04
±.000.06
±.000.07
±.00
0.01
±.000.03
±.000.04
±.000.06
±.000.07
±.00
0.01
±.000.03
±.000.04
±.000.06
±.000.07
±.00
(a) DSD, AAMAS
5 10 15 20 25
number of reviewers (k)1.0
0.8
0.6edge density ()
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.00
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.00
0.00
±.000.01
±.000.01
±.000.02
±.000.02
±.00 (b) DSD, S2ORC
5 10 15 20 25
number of reviewers (k)1.0
0.8
0.6edge density ()
0.01
±.000.04
±.010.44
±.130.71
±.110.88
±.05
0.01
±.000.03
±.010.04
±.010.20
±.030.54
±.13
0.02
±.000.03
±.000.04
±.010.06
±.020.12
±.05 (c) OQC-Local, AAMAS
5 10 15 20 25
number of reviewers (k)1.0
0.8
0.6edge density ()
0.00
±.000.07
±.050.93
±.071.00
±.001.00
±.00
0.00
±.000.02
±.010.54
±.150.71
±.120.97
±.03
0.01
±.000.02
±.010.05
±.020.41
±.140.77
±.09 (d) OQC-Local, S2ORC
5 10 15 20 25
number of reviewers (k)1.0
0.8
0.6edge density ()
0.01
±.000.11
±.101.00
±.001.00
±.001.00
±.00
0.01
±.000.01
±.000.34
±.151.00
±.001.00
±.00
0.01
±.010.02
±.010.04
±.020.03
±.010.32
±.15
(e) TellTail, AAMAS
5 10 15 20 25
number of reviewers (k)1.0
0.8
0.6edge density ()
0.01
±.000.05
±.031.00
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.72
±.111.00
±.001.00
±.00
0.01
±.000.00
±.000.01
±.000.30
±.070.93
±.04 (f) TellTail, S2ORC
5 10 15 20 25
number of reviewers (k)1.0
0.8
0.6edge density ()
0.01
±.000.04
±.010.04
±.000.39
±.140.99
±.01
0.01
±.000.04
±.010.03
±.010.05
±.010.23
±.12
0.01
±.000.03
±.010.04
±.010.04
±.010.05
±.01 (g) OQC-Greedy, AA-
MAS
5 10 15 20 25
number of reviewers (k)1.0
0.8
0.6edge density ()
0.00
±.000.01
±.011.00
±.001.00
±.001.00
±.00
0.01
±.000.01
±.000.01
±.001.00
±.001.00
±.00
0.00
±.000.01
±.010.01
±.000.01
±.010.67
±.14(h) OQC-Greedy, S2ORC
5 10 15 20 25
number of reviewers (k)1.0
0.8
0.6edge density ()
0.01
±.000.04
±.010.02
±.000.04
±.011.00
±.00
0.01
±.000.03
±.010.04
±.010.04
±.010.06
±.01
0.01
±.010.03
±.010.03
±.010.04
±.010.06
±.01
(i) OQC-Local (including
random initializations),
AAMAS
5 10 15 20 25
number of reviewers (k)1.0
0.8
0.6edge density ()
0.01
±.010.12
±.100.21
±.131.00
±.001.00
±.00
0.01
±.010.01
±.000.01
±.000.80
±.130.90
±.10
0.00
±.000.01
±.010.00
±.000.02
±.010.80
±.13(j) OQC-Local (including
random initializations),
S2ORC
Figure 13: Performance of detection algorithms on G1with variant datasets. Values indicate the mean
Jaccard similarity between the true set of colluders and the algorithm output, along with standard errors.
5 10 15 20 25
number of reviewers (k)1.0
0.8
0.6edge density ()
0.12
±.030.23
±.030.30
±.020.38
±.020.45
±.02
0.07
±.030.22
±.020.32
±.010.34
±.030.39
±.01
0.07
±.020.19
±.020.24
±.020.31
±.030.35
±.02
(a) Fraction of target pa-
pers with a colluder as-
signed, AAMAS
5 10 15 20 25
number of reviewers (k)1.0
0.8
0.6edge density ()
0.12
±.030.23
±.020.24
±.020.31
±.030.33
±.03
0.16
±.040.19
±.040.26
±.020.28
±.020.36
±.02
0.15
±.030.25
±.040.21
±.020.22
±.020.27
±.02(b) Fraction of target pa-
pers with a colluder as-
signed, S2ORC
5 10 15 20 25
number of reviewers (k)1.0
0.8
0.6edge density ()
0.18
±.040.42
±.060.49
±.040.59
±.040.63
±.03
0.16
±.060.45
±.060.53
±.020.53
±.030.60
±.03
0.18
±.060.35
±.030.42
±.040.49
±.030.56
±.04(c) Fraction of colluders
with a colluder assigned,
AAMAS
5 10 15 20 25
number of reviewers (k)1.0
0.8
0.6edge density ()
0.20
±.060.37
±.030.35
±.030.47
±.030.49
±.04
0.26
±.060.35
±.060.44
±.040.45
±.030.46
±.03
0.26
±.050.37
±.040.33
±.030.35
±.020.39
±.03(d) Fraction of colluders
with a colluder assigned,
S2ORC
Figure 14: Success of colluders in terms of kandγwith variant datasets. Values indicate the mean for each
metric along with standard errors.
24Published in Transactions on Machine Learning Research (12/2024)
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.00
±.000.01
±.000.02
±.011.00
±.001.00
±.00
0.00
±.000.01
±.010.01
±.010.04
±.010.23
±.11
0.00
±.000.02
±.010.01
±.010.02
±.010.02
±.01
(a) OQC-Greedy, AA-
MAS
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.00
±.000.00
±.000.00
±.001.00
±.001.00
±.00
0.00
±.000.00
±.000.00
±.000.01
±.010.01
±.01
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.00(b) OQC-Greedy, S2ORC
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.01
±.000.03
±.000.12
±.010.42
±.131.00
±.00
0.01
±.000.03
±.000.04
±.010.12
±.020.19
±.01
0.01
±.000.02
±.000.03
±.010.04
±.000.04
±.01 (c) Fraudar, AAMAS
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.00
±.000.00
±.000.01
±.000.01
±.001.00
±.00
0.00
±.000.00
±.000.01
±.000.01
±.000.01
±.00
0.00
±.000.00
±.000.01
±.000.01
±.000.01
±.00 (d) Fraudar, S2ORC
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.09
±.080.31
±.120.52
±.140.70
±.120.65
±.11
0.01
±.000.01
±.000.29
±.140.87
±.100.78
±.12
0.01
±.010.01
±.000.01
±.010.02
±.010.03
±.01
(e) OQC-Specialized,
AAMAS
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.07
±.070.00
±.000.08
±.080.16
±.110.17
±.11
0.00
±.000.01
±.000.01
±.000.19
±.130.39
±.16
0.00
±.000.00
±.000.00
±.000.00
±.000.01
±.00(f) OQC-Specialized,
S2ORC
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.01
±.000.04
±.010.11
±.020.52
±.131.00
±.00
0.01
±.000.02
±.000.03
±.000.06
±.010.20
±.00
0.01
±.000.02
±.000.02
±.000.03
±.010.05
±.01(g) DSD, AAMAS
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.00
±.000.00
±.000.01
±.000.41
±.161.00
±.00
0.00
±.000.00
±.000.01
±.000.01
±.000.01
±.00
0.00
±.000.00
±.000.01
±.000.01
±.000.01
±.00 (h) DSD, S2ORC
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.01
±.010.20
±.130.39
±.150.50
±.160.88
±.09
0.00
±.000.01
±.010.01
±.010.17
±.100.03
±.01
0.00
±.000.00
±.000.01
±.010.02
±.010.02
±.01
(i) OQC-Local, AAMAS
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.00
±.000.00
±.000.00
±.000.10
±.100.11
±.10
0.00
±.000.01
±.000.00
±.000.01
±.000.00
±.00
0.00
±.000.00
±.000.00
±.000.00
±.000.00
±.00 (j) OQC-Local, S2ORC
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.00
±.000.02
±.000.02
±.000.81
±.131.00
±.00
0.00
±.000.01
±.000.02
±.000.03
±.010.04
±.00
0.01
±.000.02
±.000.03
±.000.03
±.010.04
±.01 (k) TellTail, AAMAS
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.00
±.000.01
±.000.01
±.000.06
±.010.30
±.12
0.00
±.000.01
±.000.00
±.000.01
±.000.01
±.00
0.00
±.000.00
±.000.01
±.000.01
±.000.01
±.00 (l) TellTail, S2ORC
Figure 15: Performance of detection algorithms on G2with variant datasets. Values indicate the mean
Jaccard similarity between the true set of colluders and the algorithm output, along with standard errors.
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.15
±.040.24
±.020.33
±.030.37
±.020.46
±.02
0.09
±.020.16
±.020.24
±.030.31
±.020.33
±.02
0.06
±.030.13
±.030.16
±.020.18
±.010.21
±.02
(a) Fraction of target pa-
pers with a colluder as-
signed, AAMAS
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.12
±.040.18
±.020.26
±.030.28
±.020.30
±.02
0.05
±.020.07
±.020.16
±.030.23
±.020.26
±.02
0.02
±.020.07
±.020.08
±.020.10
±.020.16
±.02(b) Fraction of target pa-
pers with a colluder as-
signed, S2ORC
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.30
±.060.44
±.030.51
±.040.62
±.040.66
±.03
0.22
±.060.36
±.050.51
±.040.49
±.020.54
±.03
0.10
±.040.28
±.040.33
±.030.39
±.030.36
±.03(c) Fraction of colluders
with a colluder assigned,
AAMAS
510 15 20 25
number of reviewers (k)1.0
0.6
0.2bid density ()
0.22
±.060.32
±.040.39
±.040.45
±.030.48
±.03
0.10
±.030.13
±.040.26
±.040.38
±.030.39
±.03
0.02
±.020.12
±.030.15
±.040.20
±.040.27
±.03(d) Fraction of colluders
with a colluder assigned,
S2ORC
Figure 16: Success of colluders in terms of kandηwith variant datasets. Values indicate the mean for each
metric along with standard errors.
25