Under review as submission to TMLR
Reward Distance Comparisons Under Transition Sparsity
Anonymous authors
Paper under double-blind review
Abstract
Reward comparisons are vital for evaluating differences in agent behaviors induced by a
set of reward functions. Most conventional techniques employ optimized policies to derive
these behaviors; however, learning these policies can be computationally expensive and sus-
ceptible to safety concerns. Direct reward comparison techniques obviate policy learning
but suffer from transition sparsity, where only a small subset of transitions are sampled
due to data collection challenges and feasibility constraints. Existing state-of-the-art direct
reward comparison methods are ill-suited for these sparse conditions since they require high
transition coverage, where the majority of transitions from a given coverage distribution are
sampled. When this requirement is not satisfied, a distribution mismatch between sampled
and expected transitions can occur, introducing significant errors. This paper introduces the
Sparsity Agnostic Reward Distance (SARD) pseudometric, designed to eliminate the need
for high transition coverage by accommodating diverse sample distributions, likely com-
mon under transition sparsity. We provide theoretical justifications for SARD’s robustness
and conduct empirical studies to demonstrate its practical efficacy across various domains,
namely Gridworld, Bouncing Balls, Drone Combat, and StarCraft 2.
1 Introduction
In sequential decision problems, reward functions often serve as the most “succinct, robust, and transferable”
representationsofatask(Ng&Russell,2000),encapsulatingagentgoals,socialnorms,andintelligence(Silver
et al., 2021; Zahavy et al., 2021; Singh et al., 2009). For problems where a reward function is specified and
the goal is to find an optimal policy that maximizes cumulative rewards, Reinforcement Learning (RL) is
predominantly employed (Sutton & Barto, 2018). Conversely, when a reward function is complex or difficult
to specify, and past expert demonstrations (or policies) are available, the reward function can be learned via
Inverse Reinforcement Learning (IRL) (Ng & Russell, 2000).
In both RL and IRL contexts, reward functions govern agent decision-making, and reward comparisons can
help assess the similarity of these functions in terms of the behaviors that they induce. These comparisons
could be useful for: (1) Evaluating Agent Behaviors – By comparing how different reward functions align
or differ through specified similarity measures, agent rewards can be grouped (clustering) or categorized
(classification), to reason and interpret the agents’ behaviors. This can be useful in IRL domains, where there
is need to extract meaning from intrinsic rewards computed to represent agent preferences and motivations
(Ng & Russell, 2000). For instance, in sport domains such as hockey, reward comparisons could be useful in
inferring player rankings and their decision-making strategies (Luo et al., 2020). (2) Initial Reward Screening
– In RL domains, direct reward comparisons (without computing policies) could serve as a preliminary step
to quickly identify rewards that will achieve a spectrum of desired behaviors before actual training. This
could be beneficial in scenarios where multiple possible reward configurations exist, but some might be more
efficient. For example, it might be important to distinguish rewards that support defensive versus offensive
strategies in military scenarios (Van Evera, 1998), or competitive versus cooperative behaviors in team
settings (Santos & Nyanhongo, 2019). (3) Addressing Reward Sparsity1– Reward comparisons could also
tackle issues such as reward sparsity, by identifying more informative, and easier-to-learn reward functions,
that might be similar in terms of optimal policies but more desirable than sparse reward functions.
1Transition sparsity arises when a minority of transitions are sampled. This is different from the concept of reward sparsity,
which occurs when rewards are infrequent or sparse, making RL tasks difficult.
1Under review as submission to TMLR
Figure 1: (Transition Sparsity in a 10×10Gridworld Domain) In this illustration, each transition starts
from a starting state sand ends in a destination state s′. For clarity in visualization, we consider action-
independent rewards such that actions can be omitted, R(s,s′). In (a), high transition coverage results
from a high rollout count (number of policy rollouts) in the absence of feasibility constraints, leading to the
majority of transitions being sampled (blue points). In ( b), low coverage results from a low rollout count in
the absence of feasibility constraints, leading to fewer sampled transitions (red points). In ( c), low coverage
results from feasibility constraints, such as movement restrictions that only allow actions to adjacent cells;
which can significantly reduce the space of sampled transitions (green points) irrespective of rollout count.
The task of reward comparisons aims to identify the similarity among a collection of reward functions. This
can be done through pairwise comparisons where the similarity distance D(RA,RB)between two reward
functions,RAandRB(vectors, not scalars), is computed. The similarity distance should reflect variations
not only in magnitude but also in the preferences and behaviors induced by the reward functions. This is
characterized by the property of policy invariance, which ensures that reward functions yielding the same op-
timal policies are considered similar even if their numerical reward values differ (Ng et al., 1999). This makes
direct reward comparisons via distance measures such as Euclidean or Kullback-Leibler (KL) divergence un-
favorable since these distances do not maintain policy invariance. To satisfy policy invariance, traditional
reward comparison techniques have adopted indirect approaches, which compare behaviors derived from
optimized policies generated from the reward functions under comparison (Arora & Doshi, 2021). However,
these indirect approaches pose the following challenges: (1) they can be slow and resource-intensive due
to policy learning via RL, and (2) policy learning might not be favorable in critical environments such as
healthcare or autonomous vehicles, where safety considerations are important (Amodei et al., 2016; Thomas
et al., 2021). Therefore, the development of direct reward comparison methods that bypass policy learning
while maintaining policy invariance is highly important.
To achieve policy invariance in direct reward comparisons, Gleave et al. (2020) introduced the Equivalent
Policy Invariant Comparison (EPIC) pseudometric. Given the task to compare two reward functions, EPIC
first performs reward canonicalization to express the rewards in a standardized form by removing shaping;
and then computes the Pearson distance to calculate the difference between the canonical reward functions.
Although theoretically rigorous, EPIC falls short in practice since it is designed to compare reward functions
under high transition coverage, when the majority of transitions within the space of explored states and
actions, are sampled. In practical scenarios, this might be impractical due to transition sparsity—a condition
where only a minority of transitions are sampled. This sparsity makes EPIC vulnerable to unsampled
transitions, which can distort the computation of reward expectations in canonicalization (see Section 3.4).
Transition sparsity can be attributed to: (1) limited sampling - when challenges in data collection result
in few transitions being sampled; and (2) feasibility constraints - where environmental or agent-specific
limitations restrict certain transitions. Consider, for instance, a standard 10×10Gridworld domain where
each (x,y)coordinate represents a state. The total number of possible transitions is at least 10000if one or
more actions exist between any two states (see Figure 1). However, feasibility constraints such as movement
restrictions might significantly limit the transitions explored. For example, an agent that can only take four
2Under review as submission to TMLR
single-step cardinal actions per given state will explore fewer than 400 transitions (based on 100states×
4 directions; the exact number is 360 due to boundary limitations, such as the inability to transition from
(0,0)to(−1,0)), as shown in Figure 1c. To illustrate the impact of limited sampling, consider a scenario
where transitions are sampled via policy rollouts (trajectory simulations from a given policy). Assuming that
factors such as feasibility constraints, the transition model, and the policy rollout method are kept constant,
the extent of sampled transitions is directly influenced by the number of policy rollouts. When the number
of rollouts is low, fewer transitions will likely be sampled, and conversely, when the number of rollouts is
high, a greater proportion of transitions is likely to be sampled (see Figure 1a and 1b).
Contributions In this paper, we introduce the Sparsity Agnostic Reward Distance (SARD) pseudometric,
designed to improve reward comparisons in environments characterized by high transition sparsity. SARD
demonstrates greater robustness compared to existing pseudometrics (such as EPIC), which assume reward
samples with high transition coverage. SARD’s strength lies in its ability to integrate reward samples with
diverse transition distributions, which are common in scenarios with low coverage. We provide a theoretical
justification for SARD’s robustness and demonstrate its superiority through experiments in four domains of
varying complexity: Gridworld, Bouncing Balls, Drone Combat, and a StarCraft 2 environment. For the
simpler domains, Gridworld and Bouncing Balls, we evaluate SARD against manually defined factors such
as nonlinear reward functions and feasibility constraints, to fully understand its strengths and limitations
under controlled conditions. In the more complex domains, StarCraft 2 and Drone Combat, we assess SARD
in battlefield scenarios characterized by large state and action spaces, to gauge how it will likely to perform
in realistic settings. Our final experiment explores a novel and practical application of these pseudometrics
as distance measures within a k-nearest neighbors algorithm, tailored to classify agent behaviors based on
reward functions computed via IRL. Empirical results highlight SARD’s superior performance, evidenced
by its ability to find higher similarity between rewards generated from the same agents and higher variation
between rewards from different agents. These results emphasize the crucial need to accommodate transition
sparsity in reward comparisons.
2 Related Works
TheEPICpseudometricisthefirstdirectrewardcomparisontechniquethatcircumventspolicylearningwhile
maintaining policy invariance (Gleave et al., 2020). In practical settings, EPIC’s major limitation is that it is
designedtocomparerewardsunderhightransitioncoverage. Inscenarioscharacterizedbytransitionsparsity,
EPIC underperforms due to its high sensitivity to unsampled transitions, which can cause a distribution
mismatch between sampled and expected transitions, distorting the computation of reward expectations
(refer to Section 3.4). This limitation has been observed by Wulfe et al. (2022), who suggested the Dynamics-
Aware Reward Distance (DARD) pseudometric. DARD improves on EPIC by considering transitions that
are closer to being physically realizable; however, it remains sensitive to unsampled transitions, which limits
its efficacy. Additionally, DARD requires access to transition models, which might be inaccessible or difficult
to reliably estimate under transition sparsity.
Skalse et al. (2023) also introduced a family of reward comparison pseudometrics, known as Standardized
Reward Comparisons (STARC). These pseudometrics are shown to induce lower and upper bounds on worst-
case regret, implying that a small STARC difference between two reward functions corresponds to similar
behaviors. Among the different STARC canonical forms explored, the Value-Adjusted Levelling (VAL)
function is shown to have a higher correlation to regret compared to both EPIC and DARD. While an
improvement, the major flaw with VAL is that it involves the estimation of value functions, which carry
a significantly higher computational cost compared to both EPIC and DARD. To compute VAL in small
environments, value iteration can be performed, which has polynomial complexity in terms of the state and
action spaces (Skalse et al., 2023). For larger environments, value functions can be approximated via neural
networks updated with on-policy Bellman updates (Skalse et al., 2023). Since the primary motivation for
direct reward comparisons is to eliminate policy learning to lower computational costs, incorporating value
functions in these pseudometrics is somewhat contradictory. Therefore, we do not consider VAL as a viable
direct reward comparisons pseudometric.
3Under review as submission to TMLR
The task of reward comparisons lies within the broader theme of reward evaluations, which aim to explain
or interpret the relationship between rewards and agent behavior. Some notable works tackling this theme,
include, Lambert et al. (2024), who developed benchmarks to evaluate reward models in Large Language
Models (LLMs), which are often trained using RL via human feedback (RLHF), to align with human values.
These benchmarks assess criteria such as communication, safety and reasoning capabilities across a variety
of reward models. In another line of work, Mahmud et al. (2023) presented a framework leveraging human
explanations to evaluate and realign rewards for agents trained via IRL on limited data. Lastly, Russell
& Santos (2019) proposed a method that examines the consistency between global and local explanations,
to determine the extent to which a reward model captured complex agent behavior. Similar to reward
comparisons, reward evaluations can be influenced by shaping functions, thus necessitating techniques such
as canonicalization as preprocessing steps to eliminate shaping (Jenner & Gleave, 2022).
Reward shaping is a technique that transforms a base reward function into alternate forms (Ng et al., 1999).
This technique is mainly employed in RL for reward design where heuristics and domain knowledge are
integrated to accelerate learning (Mataric, 1994; Hu et al., 2020; Cheng et al., 2021; Gupta et al., 2022; Suay
et al., 2016). Several applications of reward shaping have been explored, and some notable examples include:
training autonomous robots for navigation (Tenorio-Gonzalez et al., 2010); training agents to ride bicycles
(Randløv & Alstrøm, 1998); improving agent behavior in multiagent contexts such as the Prisoner’s Dilemma
(Babes et al., 2008); and scaling RL algorithms in complex games (Lample & Chaplot, 2017; Christiano
et al., 2017). Among several reward shaping techniques, potential-based shaping is the most popular due to
its preservation of policy invariance, ensuring that the set of optimal policies remains unchanged between
different versions of reward functions (Ng et al., 1999; Wiewiora et al., 2003; Gao & Toni, 2015).
3 Preliminaries
This section establishes the necessary foundation for understanding the task of direct reward comparisons.
We review existing pseudometrics and discuss their limitations, which SARD aims to address.
3.1 Markov Decision Processes
A Markov Decision Process (MDP) is defined as a tuple (S,A,γ,T,R ), whereSandAare the state and
action spaces, respectively. The transition model T:S×A×S→ [0,1], dictates the probability distribution
of moving from one state, s∈S, to another state, s′∈S, under an action a∈A, and each given transition
is specified by the tuple (s,a,s′). The discount factor γ∈[0,1]reflects preference for immediate over future
rewards. The reward function is denoted by R:S×A×S → R, and for each transition, the reward is
denoted by R(s,a,s′). Given a batch of sampled transitions from a reward function R, we define the the
coverage distribution, D(s,a,s′), as the probability distribution over transitions from which the sample is
generated. The distributions over AandSare denoted byDAandDSrespectively; and the corresponding
sets of distributions are denoted by ∆Aand∆Srespectively. A trajectory τ={(s0,a0),(s1,a1),···,(sn)},
n∈Z+, is a sequence of states and actions, with a total return: g(τ) =/summationtext∞
t=0γtR(st,at,st+1). The goal in
an MDP is to find a policy π:S×A→ [0,1](often via RL) that maximizes the expected return E[g(τ)]. In
some situations, the rewards of an MDP are unknown, and IRL can be used to compute the rewards given
agent demonstrations (Abbeel & Ng, 2004; Ng & Russell, 2000; Wulfmeier et al., 2015).
3.2 Policy Invariance
Policy invariance is a condition where an optimal policy remains unchanged when a reward function is
modified typically through shaping (Ng et al., 1999; Jenner et al., 2022). In reward comparisons, policy
invariance is a key property to satisfy, since it ensures that equivalent rewards will yield the same optimal
policies (Gleave et al., 2020). Formally, any shaped reward can be represented by the additive relationship:
R′(s,a,s′) =R(s,a,s′) +F(s,a,s′), whereF(s,a,s′)is a shaping function. Potential shaping guarantees
policy invariance, and it takes the form: F(s,a,s′) =γϕ(s′)−ϕ(s), whereϕis a state potential function. A
potentially-shaped reward function, R′, is thus represented as:
R′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s). (1)
4Under review as submission to TMLR
whereRis the original reward function. Reward functions RandR′can be deemed equivalent since they
yield the same optimal policies.
To effectively compare reward functions that may differ in numerical values but are equivalent since they
yield the same optimal policies, the use of pseudometrics is highly important. Let Xbe a set, with (x,y,z )
elements of X, and letd:X×X→[0,∞)define a pseudometric. This pseudometric adheres to the following
axioms: (premetric) d(x,x) = 0for allx∈X; (symmetry) d(x,y) =d(y,x)for allx,y∈X; and (triangular
inequality) d(x,y)≤d(x,z) +d(z,y)for allx,y,z∈X. Unlike a true metric, a pseudometric does not
require that: d(x,y) = 0 =⇒x=y, making it ideal for identifying equivalent reward functions that might
have different numerical values.
3.3 Equivalent Policy Invariant Comparison (EPIC)
The EPIC pseudometric directly compares reward functions without computing policies, while maintaining
policy invariance (Gleave et al., 2020). EPIC’s reward comparison process involves two steps: first, reward
functionsarecanonicalizedintoastandardformwithoutshaping; andsecond, aPearsondistanceiscomputed
to differentiate the canonical rewards. The following definitions, by Gleave et al. (2020), describe EPIC.
Definition 1. (Canonically Shaped Reward) Let R:S×A×S→ Rbe a reward function. Given distributions
DS∈∆SandDA∈∆Aover states and actions respectively, let SandS′be random variables distributed
asDSandAbe distributed asDA. The canonically shaped reward is:
CEPIC (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′)−R(s,A,S′)−γR(S,A,S′)]. (2)
Canonicalization expresses rewards in form free of shaping. Given a potentially shaped reward, R′(s,a,s′) =
R(s,a,s′) +γϕ(s′)−ϕ(s), canonicalization yields: CEPIC (R′)(s,a,s′) =CEPIC (R)(s,a,s′) +ϕres, where
ϕres=γE[ϕ(S)]−γE[ϕ(S′)]is the remaining residual potential. EPIC assumes that SandS′are identically
distributed such that E[ϕ(S)] =E[ϕ(S′)], which results in ϕres= 0. This assumption leads to Proposition 1:
Proposition 1. (The Canonically Shaped Reward is Invariant to Shaping) Let R:S×A×S→ Rbe a
reward function,DS∈∆SandDA∈∆Abe distributions over states and actions, ϕ:S→Ra state potential
function, and R′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s)be the shaped reward. Then: CEPIC (R) =CEPIC (R′).
Proposition 1 means that reward functions RandR′, can be compared in their basis form, without the effect
of shaping. Finally, the EPIC pseudometric between two reward functions, RAandRB, is computed as:
DEPIC (RA,RB) =Dρ(CEPIC (RA),CEPIC (RB)), (3)
where:
Dρ(RA,RB) =/radicalbig
1−ρ(RA,RB)/√
2, (4)
is the Pearson distance. ρ(RA,RB)is the Pearson correlation between the reward functions, defined as:
ρ(RA,RB) =E[(RA−µRA)(RB−µRB)]
σRAσRB, (5)
whereµdenotes the mean, σdenotes the standard deviation, and E[(RA−µRA)(RB−µRB) is the covariance
betweenRAandRB. The Pearson distance ensures that EPIC is scale and shift invariant, whilst canoni-
calization makes EPIC invariant to shaping (Gleave et al., 2020). Computing the exact expectation terms
in EPIC is almost impractical because reward values are needed for all transitions. When a reward function
has a large (or infinite) state or action space, the sample-based EPIC can be computed as follows:
Definition 2. (Sample-based EPIC) Given transition samples from a coverage distribution Dand a batch
BMofNMsamples from the joint state and action distributions. The canonically shaped reward is:
CEPIC (R)(s,a,s′)≈R(s,a,s′) +γ
NM/summationdisplay
(x,u)∈BMR(s′,u,x)−1
NM/summationdisplay
(x,u)∈BMR(s,u,x )
−γ
N2
M/summationdisplay
(x,·)∈BM/summationdisplay
(x′,u)∈BMR(x,u,x′)(6)
5Under review as submission to TMLR
Figure 2: (Impact of unsampled transitions on CEPIC (R)(s1,a1,s2)) Sampled transitions are those explored
in the reward sample, while expected transitions are those anticipated by EPIC, assuming full coverage. As
coverage decreases from ( a) to (c), due to a reduction in the number of sampled transitions, the standard
deviation of CEPIC (R)(s1,a,s 2)increases, indicating EPIC’s increased instability to unsampled transitions.
For comparison, DARD and SARD have lower standard deviations, signifying higher stability.
3.4 Unsampled Transitions
Consider a reward function R:S×A×S , whereSis the state space and Ais the action space. Rewards are
sampled from a coverage distribution, D, and they span a state space SD⊆Sand an action space AD⊆A.
We define the following sets of transitions:
Full Coverage Transitions ( TD)- The set of all transitions (could be realizable or not) that span the
explored state and action space from the reward sample, TD=SD×AD×SD⊆S×A×S .
Sampled Transitions (TS) - The set of transitions that are actually present in the reward sample. Due
to feasibility constraints and limited sampling, this set is a subset of full coverage transitions: TS⊆TD.
Unsampled Transitions (TU) - The set of full coverage transitions that are not explored in the reward
sample. These transitions can be both realizable and unrealizable, and TU⊆TD\TS, whereTU⊆TD.
A major limitation of pseudometrics such as EPIC (and to some extend, DARD), is that they are designed
to compare reward functions under high transition coverage where |TS|≈|TD|. As|TU|→|TD|, the per-
formance of these pseudometrics significantly degrades due to unsampled transitions. To illustrate, consider
Equation 6, used to approximate EPIC (Equation 2). To perform the computation, we need to estimate:
E[R(s′,A,S′)]by dividing the sum of rewards from s′toS′byNMtransitions; E[R(s,A,S′)]by dividing
the sum of rewards from stoS′byNMtransitions; and E[R(S,A,S′)]by dividing the sum of all rewards
fromStoS′byNM2transitions, where NM=|SD×AD|. Every state s∈Sis expected to have NM
transitions to all other states S′, which is impractical under transition sparsity when a high number of
transitions remain unsampled. Since reward summations are divided by large denominators NMandN2
M
(see Equation 6), when coverage is low, the number of sampled transitions needed to estimate the reward
expectation terms will be fewer than expected, introducing significant error. Moreover, to eliminate residual
shaping,ϕres=γE[ϕ(S)]−γE[ϕ(S′)], EPIC assumes that SandS′are identically distributed, which favors
reward samples with high transition coverage where each state likely has a transition to every other state.
Figure 2 illustrates an example showing the effect of unsampled transitions on computing EPIC across
three reward samples spanning a state space S=S′={s1,...,s 8}, and an action space, A={a1}, such that
6Under review as submission to TMLR
NM= 8, under different levels of transition sparsity. Rewards are defined as R(si,a1,sj) = 1+γϕ(sj)−ϕ(si),
wherei,j∈{1,...,8}, and state potentials are randomly generated such that: |ϕ(s)|≤20, withγ= 0.5.
The task is to compute CEPIC (R)(s1,a1,s2)over 1000simulations. For all reward samples, the mean
µ(CEPIC (R)(s1,a1,s2))≈0, but the standard deviation, σ(CEPIC (R)(s1,a1,s2))varies based on coverage.
In Figure 2a, the reward sample has high coverage ( 100%), hence, the number of observed and expected
transitionsareequal. Inthisscenario, EPICishighlyeffectiveandallshapedrewardsaremappedtothesame
value (≈0), resulting in a standard deviation σ(CEPIC (R)(s1,a1,s2)) = 0, highlighting consistent reward
canonicalization. In Figure 2b, the reward sample has moderate coverage and the fraction of unsampled
transitions is approximately 33%. As a result, σ(CEPIC (R)(s1,a1,s2)) = 2.84is relatively high, signifying
EPIC’s sensitivity to unsampled transitions. In Figure 2c, the reward sample exhibits low coverage and the
fraction of unsampled transitions is approximately 77%, indicating a significant discrepancy between the
number of observed and expected transitions. Consequently, σ(CEPIC (R)(s1,a1,s2)) = 7.36, highlighting
EPIC’s increased instability due to unsampled transitions. For comparison, we have added the DARD and
SARD estimates, and these distances have lower standard deviations signifying greater stability.
EPIC’s limitations have also been acknowledged by Wulfe et al. (2022), who propose DARD, to only consider
transitions that are closer to being physically realizable. The DARD pseudometric is given by:
CDARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′′)−R(s,A,S′)−γR(S′,A,S′′)],
whereA∼DA,S′∼T(s,A),S′′∼T(s′,A), andTis the transition model. DARD is invariant to shaping
and generally improves upon EPIC by eliminating the identical distribution assumption and by separating
the states connected from s(denoted by S′) ands′(denoted by S′′). However, DARD can suffer from the
presence of unsampled transitions since it requires transitions from S′toS′′, which are not guaranteed to
exist. Furthermore, DARD can be highly sensitive to variations in transition distributions between reward
functions under comparison (Wulfe et al., 2022), since it relies on transitions that are very close to the reward
being canonicalized ( S′is close tos), and (S′′is close tos′); and might lack the context of transitions further
fromsands′. These factors make DARD a modest improvement over EPIC, but it still struggles to compare
reward functions under high transition sparsity. This paper uses DARD as an experimental baseline.
4 Approach: Sparsity Agnostic Reward Distance (SARD)
Our motivation in SARD is to develop a direct reward comparison technique that minimizes assumptions
on the structure and distribution of reward samples, ensuring robustness under high transition sparsity. To
derive SARD, we first modify CEPIC(Equation 2) to eradicate the need for high transition coverage. This is
achieved by eliminating the assumption that SandS′are identically distributed; and ensuring that reward
expectations computed for transitions from s,s′andSare different. This approach reduces the impact
of unsampled transitions, since reward expectations are computed based on the observed distribution of
sampled transitions. With these considerations, the modified canonical equation becomes:
C1(R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)], (7)
where the random variables: S1andS2are subsequent states to s′ands, respectively; S3encompasses all
initial states for all sampled transitions; and S4are subsequent states to S3. Applying C1to a shaped reward
R′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s), we get:
C1(R′)(s,a,s′) =C1(R)(s,a,s′) +ϕres1, (8)
where,
ϕres1=E[γ2ϕ(S1)−γ2ϕ(S4) +γϕ(S3)−γϕ(S2)]. (9)
C1is not theoretically robust since its prone to shaping due to residual potential ϕres1. To cancel E[ϕ(Si)],
∀i∈{1,...,4}, we can add rewards R(Si,A,ki)to induce potentials γϕ(ki)−ϕ(Si); wherekican be any
arbitrary distribution of states. This results in the equation:
C2(R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,k 1)−γR(S2,A,k 2) +γR(S3,A,k 3)−γ2R(S4,A,k 4)].(10)
7Under review as submission to TMLR
ApplyingC2to shaped reward R′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s), we get:
C2(R′)(s,a,s′) =C2(R)(s,a,s′) +ϕres2, (11)
where,
ϕres2=E[γ3ϕ(k1)−γ3ϕ(k4) +γ2ϕ(k3)−γ2ϕ(k2)]. (12)
See Appendix A.5 for derivations of ϕres1andϕres2.
The canonical form C2is preferable to C1, since it enables the selection of kito eradicate ϕres2. A convenient
solution is to ensure that: k1=k4andk2=k3such that E[ϕ(k1)] =E[ϕ(k4)]andE[ϕ(k2)] =E[ϕ(k3)] =⇒
ϕres2= 0. We choose the solution: k1=k4=S5, andk2=k3=S6; whereS5are states subsequent to S1,
andS6are states subsequent to S2. This leads to the following SARD definition:
Definition 3. (Sparsity Agnostic Canonically Shaped Reward) Let R:S×A×S→ Rbe a reward function.
Given distributions DS∈∆(S)andDA∈∆(A), letSbe a random variable distributed as DSandAbe
a random variable distributed as DA. Given a transition model T(S′|S,A)that governs the conditional
distribution from current states to next states. For each s∈Sands′∈S, let{S1,...,S 6}be random
variables distributed such that: S1andS2are subsequent states to s′ands, respectively; S3encompasses all
initial states from all sampled transitions; S4,S5, andS6are subsequent states to S3,S1andS2, respectively.
The Sparsity Agnostic Canonically Shaped Reward is defined as:
CSARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,S 5)−γR(S2,A,S 6) +γR(S3,A,S 6)−γ2R(S4,A,S 5)].(13)
The SARD canonicalization is also invariant to shaping, as described by Proposition 2.
Proposition 2. (The Sparsity Agnostic Canonically Shaped Reward is Invariant to Shaping) Let R:
S×A×S be a reward function, ϕ:S →R, a state potential function. Assuming that, for each reward
expectation term in CSARD, the cross-product for all transitions, (Si,A,Sj)can be computed, the shaped
rewardR′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s)satisfies:
CSARD (R) =CSARD (R′).
Proof.See Appendix A.2.
Given reward functions RAandRB, the SARD pseudometric is computed as follows:
DSARD (RA,RB) =Dρ(CSARD (RA),CSARD (RB)), (14)
where,Dpis the Pearson distance (see Equation 4).
The term ‘agnostic’ in CSARDunderscores its robustness and flexibility in accommodating variations in
transition distributions when canonicalizing reward samples that may lack full coverage but have enough
transitions to compute each reward expectation term in Equation 13. However, in many practical scenarios
with high transition sparsity, some transitions required for at least one of the reward expectation terms in
CSARDmight still remain unsampled. In these cases (with missing transitions), although the term ‘agnostic’
might seem somewhat optimistic, it still reflects SARD’s high robustness relative to other methods. This
robustness is facilitated by the strategic choices of kiinCSARD(Equation 13), S5andS6, ensuring that
sample-based approximations for SARD are ideal for the following two reasons: First, for any reward sample,
we are guaranteed to compute reliable expectation estimates for the first six terms, since for each set of
8Under review as submission to TMLR
transitions (SitoSj),Sjis created based on Si; hence, these transitions naturally align with any reward
sample distribution. However, for transitions in the last two terms, (S3toS6)and(S4toS5),S6andS5,
are not directly created based on S3andS4, as they were previously defined for S2andS1; therefore, these
terms are highly susceptible to unsampled transitions, since they might require transitions that may not be
present in the reward sample. Second, while there might be unsampled transitions from (S4toS5), and (S3
toS6), a minimal set of sampled transitions is likely to exist, because:
•Transitions (S1,A,S 5)⊆(S4,A,S 5).
SinceS1are subsequent states to s′, andS4are subsequent states for all sampled transitions. It
follows that, S1⊆S4(See Appendix A.10), hence, (S1,A,S 5)⊆(S4,A,S 5).
•Transitions (ˆS2,A,S 6)⊆(S3,A,S 6).
S2is the set of subsequent states from s, and it can contain non-terminal and terminal states (usually
fewer). Lets denote all non-terminal states in S2(excludes all terminal) by ˆS2, such that the states
inˆS2are initial states for some sampled transitions. Since S3encompasses all initial states from all
sampled transitions, it follows that: ˆS2⊆S3., hence, (ˆS2,A,S 6)⊆(S3,A,S 6).
Therefore, we are likely to get decent reward expectation estimates for the transitions: (S3toS6)and(S4to
S5). These factors ensure that under significant transition sparsity, the upper bound relative shaping error
(see Definition 4) for the approximation of CSARDis lower than that of CEPICandCDARD, as described
by Theorem 1.
Theorem 1. LetR1andR2be samples of similar reward functions under comparison, each potentially
shaped by a different shaping function, but sharing the same set of explored transitions. Under high transition
sparsity, where the fraction of sampled transitions is minimal, the upper bound of the Relative Shaping Error
(RSE) for the approximation of CSARDis lower than that of CEPICandCDARD.
Proof.See Appendix A.1.
The relative shaping error (RSE) aims to quantify the influence of the residual potentials when computing
the Pearson distance during reward comparisons, and it is defined as follows:
Definition 4. (Relative Shaping Error (RSE)) Given an arbitrary reward canonicalization method CS, and
a shaped reward sample R′(assumed to be non-zero), canonicalizing R′results in: CS(R′) =CS(R) +ϕr,
whereϕrrepresents the residual shaping term. Let U(CS(R))represent the upper bound of the base canonical
reward. We define the relative shaping error as:
RSE (CS) =|ϕr|
U(CS(R)). (15)
The denominator, U(CS(R)), represents an upper bound on the magnitude of the base (unshaped) canonical
reward, and it serves to normalize the impact of shaping. It is defined as follows:
Definition5. (Upper Bound Canonical Reward) Let Rbe an unshaped reward sample and CS(R)be a canon-
icalization function represented as the sum of nterms:CS(R)(s,a,s′) =R(s,a,s′)+E/summationtextn−1
i=1[αiR(Si,A,Sj)],
where|α|≤1. From all subsets of rewards from R, consider the subset that maximizes the absolute expecta-
tion|E[R(Sx,A,Sy)]|, whereSxorSycould also refer to single states. Let this maximum value be denoted
asZ∈Z+. The upper bound of the canonical reward is given by:
U(CS(R)) =nZ (16)
A low RSE value suggests that U(CS(R))is substantially larger than |ϕr|, indicating that the impact of
shaping is likely minimal. Conversely, a high RSE value implies that U(CS(R))is relatively small compared
to|ϕr|, highlighting a more significant influence of shaping. The RSE provides a measure of the worst-case
9Under review as submission to TMLR
impact of shaping in extreme scenarios, and it helps to assess the robustness of reward canonicalization
methods in mitigating shaping effects.
Computing the exact SARD canonicalization can be challenging for reward samples that might be too large
or have unsampled transitions. To approximate SARD under transition sparsity, the sample-based SARD
approximation, ˆCSARD, defined below is used:
Definition 6. (Sample-based SARD) Given transition samples from a coverage distribution Dand a batch
BMofNMsamples from the joint state and action distributions. From BM, we can derive sets Xi⊆BM,
fori∈{1,...,6}. EachXiis a set,{(x,u)}, wherexis a state and uis an action. The magnitude, |Xi|, is
denoted byNi. We define X1={(x1,u)}, wherex1denotes subsequent states for transitions starting from s′;
X2={(x2,u)}, wherex2denotes subsequent states for transitions that start from s;X3={(x3,u)}, where
x3denotes all initial states for all transitions; X4={(x4,u)}, wherex4denotes all subsequent states for
all transitions; X5={(x5,u)}, wherex5denotes subsequent states to X1;X6={(x6,u)}, wherex6denotes
subsequent states to X2.CSARDcan be approximated as:
ˆCSARD (R)(s,a,s′)≈R(s,a,s′) +γ
N1/summationdisplay
(x1,u)∈X1R(s′,u,x 1)−1
N2/summationdisplay
(x2,u)∈X2R(s,u,x 2)
−γ
N3N4/summationdisplay
(x3,u)∈X3/summationdisplay
(x4,·)∈X4R(x3,u,x 4) +γ2
N1N5/summationdisplay
(x1,u)∈X1/summationdisplay
(x5,.)∈X5R(x1,u,x 5)
−γ
N2N6/summationdisplay
(x2,u)∈X2/summationdisplay
(x6,·)∈X6R(x2,u,x 6) +γ
N3N6/summationdisplay
(x3,u)∈X3/summationdisplay
(x6,·)∈X6R(x3,u,x 6)
−γ2
N4N5/summationdisplay
(x4,u)∈X4/summationdisplay
(x5,·)∈X5R(x4,u,x 5).(17)
Proposition 3. (The sample-based SARD approximation can be invariant to shaping provided all transition
definitions are fully covered) Given a potentially-shaped reward sample R′that spans a state space SDand
an action space AD, whereDis the coverage distribution. If the SARD state definitions, Si⊆SDfor all
i∈{1,...6}, are derived from the sample, and all the necessary transition definitions are fully covered from
the sample, then:
ˆCSARD (R′) =ˆCSARD (R)
Proof.See Appendix A.3
Each term in the SARD approximation aims to estimate the corresponding term in the exact CSARD,
Equation 13. For example, −γ
N3N4/summationtext
(x3,u)∈X3/summationtext
(x4,·)∈X4R(x3,u,x 4)estimates−γE[R(S3,A,S 4)]and
γ
N1/summationtext
(x1,u)∈X1R(s′,u,x 1)estimatesγE[R(s′,A,S 1)]. The structure of the batch, BM(often sampled uni-
formly), ensures that we likely consider both underrepresented and overrepresented transitions, to reduce
bias in computations due to the transition model. Undefined reward triples (not present in reward sample
due to feasibility constraints or limited sampling), R(xi,u,xj), are ignored in the reward summation terms.
When all the necessary transition terms are present, the sample-based SARD approximation satisfies Propo-
sition 3, as shown in Appendix A.3. ˆCSARDcan fully canonicalize reward samples with significantly fewer
transitions, as long as all the necessary transitions for the reward expectation terms are available. However,
this is often not the case, as some transitions might be missing. In rare instances, when a reward sample
has full coverage, the three psuedometrics are equivalent as described in Proposition 4:
Proposition 4. The SARD, DARD, and EPIC canonical rewards are similar when a reward sample has
full coverage.
Proof.See Appendix A.6.
For the SARD psuedometric, we establish an upper bound for regret in terms of the SARD, showing that
whenDSARD→0, the difference between the policies induced by the reward functions under comparison is
close to 0, as described by Theorem 2:
10Under review as submission to TMLR
Theorem 1. LetRA,RB:S×A×S→Rbe reward functions with respective optimal policies, π∗
A,π∗
B. Let
Dπ(t,st,at,st+1)be the distribution over transitions S×A×Sinduced by policy πat timet, andD(s,a,s′)
be the coverage distribution. Suppose there exists K > 0such thatKD(st,at,st+1)≥Dπ(t,st,at,st+1)for
all timest∈N, triples (st,at,st+1)∈S×A×Sand policies π∈{π∗
A,π∗
B}. Then the regret under RAfrom
executingπ∗
Binstead ofπ∗
Ais at most:
GRA(π∗
A)−GRA(π∗
B)≤32K∥RA∥2(1−γ)−1DSARD (RA,RB),
whereGR(π)is the return of policy πunder reward R.
Proof.See Appendix A.8.
In summary, SARD is designed to improve reward comparisons under transition sparsity. To achieve this,
SARD eliminates the assumption that SandS′are identically distributed, and also reduces the impact of
missing transitions, by canonicalizing rewards solely based on sampled transitions. In terms of computational
complexity, the EPIC, DARD and SARD psuedometrics all have a complexity of O(N2
M)(see Appendix
A.9), where NMis the size of the batch of state-action pairs, BM. Empirically though, computing EPIC
and DARD is generally faster than SARD by a constant factor, since SARD has more expectation terms.
Lastly, in Appendix A.4, we present a generalized formula for possible SARD extensions.
5 Experiments
To evaluate SARD, we examine the following hypotheses:
H1:SARD is a reliable reward comparison pseudometric under high transition sparsity.
H2:SARD can enhance the task of classifying agent behaviors based on their reward functions.
In these hypotheses, we compare the performance of SARD to both EPIC and DARD using sample-based
approximations. In H1, we analyze SARD’s robustness under transition sparsity resulting from limited
sampling and feasibility constraints. In H2, we investigate a practical use case to classify agent behaviors
using their reward functions. Experiment 1 tests H1and Experiment 2 tests H2.
Domain Specifications To conduct Experiment 1, we need the capability to vary the number of sam-
pled transitions, since the goal is to test SARD’s performance under different levels of transition sparsity.
Therefore, Experiment 1 is performed in the Gridworld and Bouncing Balls domains, as they provide the
flexibility for parameter variation to control the size of the state and action spaces2. These two domains
have also been studied in the EPIC and DARD papers, respectively. The Gridworld domain simulates agent
movement from a given initial state to a specified terminal state under a static policy. States are defined by
(x,y)coordinates where 0≤x<Nand0≤y <Mimplying|S|=NM. The action space consists of four
cardinal directions (single steps), and the environment is stochastic, with a probability ϵof transitioning
to any random state irrespective of the selected action. When ϵ= 0, a feasibility constraint is imposed,
preventing the agent from making random transitions. The Bouncing Balls domain, adapted from (Wulfe
et al., 2022), simulates a ball’s motion from a starting state to a target state while avoiding randomly mobile
obstacles. These obstacles add complexity to the environment since the ball might need to change its strategy
to avoid obstacles (at a distance, d= 3). Each state is defined by the tuple (x,y,d ), where (x,y)indicates
the ball’s current location, and dindicates the ball’s Euclidean distance to the nearest obstacle, such that:
0≤x < N,0≤y < M, andd≤max(M,N ). The action space includes eight directions (cardinals and
ordinals), we also define the stochasticity-level parameter ϵfor choosing random transitions.
For Experiment 2, the objective is to test SARD’s performance in near-realistic domain settings, where we
have no control over factors such as the nature of rewards and the level of transition sparsity. Therefore,
for domain settings, in addition to the Gridworld and the Bouncing Balls domains with the setup similar
2Experiment 1 excludes the Drone Combat and StarCraft 2 environments because these domains have very large state and
action spaces that hinder effective coverage computation.
11Under review as submission to TMLR
to Experiment 1 but fixed parameters, we also examine the StarCraft 2 and Drone Combat domains which
both simulate battlefield environments where a controlled multiagent team aims to defeat a default AI
enemy team (Anurag, 2019; Vinyals et al., 2019). These domains resemble complex scenarios with large
state and action spaces (almost infinite for Starcraft2), enabling us to test SARD’s (as well as the other
pseudometrics) generalization to near-realistic scenarios. Additional details about these domains, including
information about the state and action features are described in Appendix C.1.
Reward Functions Extrinsic reward functions are manually defined using a combination of state and
action features. For the Starcraft2 and Drone Combat domains, we use the default game engine scores
(also based on state and action features), as the reward function (see Appendix B.1). For the Gridworld
and Bouncing Balls domains, in each reward function, the reward value R(s,a,s′)is derived from the
decomposition of state and action features, where, (sf1,...,sfn)is from the starting state s;(af1,...,afm)
is from the action a; and (s′
f1,...,s′
fn)is from the subsequent state s′. For the Gridworld domain, these
features are the (x,y)coordinates, and for the Bouncing Balls domain, these include (x,y,d ), wheredis
the distance of the obstacle nearest to the ball. For each unique transition, using randomly generated
constants:{u1,...,un}for incoming state features; {w1,...,wm}for action features; {v1,...vn}for subsequent
state features, we create polynomial and random rewards as follows:
Polynomial: R(s,a,s′) =u1sα
f1+...+unsα
fn+w1aα
f1+...+wmaα
fm+v1s′α
f1+...+vns′α
fn,
whereαis randomly generated from 1-10 ,denoting the degree of the polynomial.
Random: R(s,a,s′) =β,
whereβis a randomly generated reward for each unique transition.
Forthepolynomialrewards, αisthesameacrosstheentiresample, butotherconstantsvarybetweendifferent
transitions. The same reward relationships are used to model potential shaping functions. In addition, we
also explore linear and sinusoidal reward models as described in Appendix B.1.
For complex environments such as Starcraft2 and the Drone Combat domain, specifying reward functions
can be challenging, hence we also incorporate IRL to infer rewards from demonstrated behavior. For our
experiments, we consider the following IRL rewards: Maximum Entropy IRL (Maxent) (Ziebart et al., 2008);
Adversarial IRL (AIRL) (Fu et al., 2018); and Preferential-Trajectory IRL (PTIRL) (Santos et al., 2021).
The full descriptions for these algorithms is described in Appendix C.3.
5.1 Experiment 1: Transition Sparsity
Objective: The goal of this experiment is to test SARD’s ability to identify similar reward samples under
transition sparsity as a result of limited sampling and feasibility constraints.
Relevance: The EPIC pseudometric struggles under high transition sparsity since it is designed to com-
pare reward functions (as well as samples), under high coverage. SARD is developed to overcome EPIC’s
limitations, and this experiment tests SARD’s performance relative to EPIC and DARD, on varying levels
of transition coverage due to feasibility constraints and limited sampling.
Approach: This experiment is conducted on a 20×20Gridworld domain and a 20×20Bouncing Balls
domain. Forallsimulations, manualrewardsareusedsincetheyenabletheflexibilitytovarythenatureofthe
relationship between reward values and features, enabling us to test the performance of the pseudometrics on
diverse reward values, which include polynomial and random reward relationships. We also vary the shaping
potentials such that |R(s,a,s′)|≤|γϕ(s′)−ϕ(s)|≤5|R(s,a,s′)|.
For each domain, a ground truth reward function ( GT) and an equivalent potentially shaped reward function
(SH) are generated, both with full coverage ( 100%). Using rollouts from a uniform policy, rewards Rand
R′are sampled from GTandSHrespectively, and they might differ in transition composition. After sample
generation, RandR′arecanonicalizedandrewarddistancesarecomputedusingcommontransitionsbetween
them, under varying levels of coverage (to test limited sampling) and feasibility constraints. The SARD,
DARD, and EPIC reward distances are computed, as well as DIRECT, which is the Pearson distance of
12Under review as submission to TMLR
Figure 3: (Transition Sparsity) The performance of reward comparison pseudometrics in identifying the
similarity between potentially shaped reward functions under limited sampling and feasibility constraints.
For this task, a more accurate pseudometric yields a Pearson distance Dρclose to 0, indicating policy
equivalence between the shaped reward functions, and a less accurate pseudometric results in Dρclose to
1. In both experiments, transition coverage is calculated as the ratio of sampled transitions to the set of
all theoretically possible transitions |S×A×S| , including both feasible and infeasible transitions. Each
coverage data point is averaged over 200 simulations at a constant policy rollout count, with coverage data
points generated by varying the number of policy rollouts from 1 to 2000 (see Appendix B.3). In ( a), EPIC
and DARD lag behind SARD when transition coverage is low due to limited sampling from lower rollout
counts but gradually improve as transition coverage increases with higher rollout counts. In ( b), movement
restrictionssignificantlyreducetransitioncoverage, regardlessofrolloutsamplingfrequency, whichnegatively
impacts EPIC’s performance (almost similar to DIRECT).
rewards without canonicalization. Since RandR′are drawn from equivalent reward functions, an accurate
pseudometric should yield distances close to the minimum Pearson distance, Dρ= 0; and the least accurate
should yield a distance close to the maximum, Dρ= 1. DIRECT serves as a worst-case performance baseline,
since it computes reward distances without canonicalization (needed to remove shaping). We perform 200
simulation trials for each comparison task, and record the mean reward distances.
Simulations and Results: Limited Sampling: Using rollouts from a uniform policy, we sample Rand
R′fromGTandSH, respectively, under a stochasticity-level parameter, ϵ= 0.1. The number of transitions
sampled is controlled by varying the number of policy rollouts (rollout counts) from 1up to 2000. The
corresponding coverage is computed as the number of sampled transitions over the number of all theoretically
possible transitions ( =|S×A×S| ). Figure 3a summarizes the variation of reward distances to transition
coverage due to different levels of transition sampling in the Gridworld, and Bouncing Balls domains. As
shown, SARD generally outperforms other baselines since it converges towards Dρ= 0faster, even when
coverage is low. DARD generally outperforms EPIC; however, it’s highly prone to shaping compared to
SARD since it’s more sensitive to unsampled transitions. All pseudometrics generally outperform DIRECT,
illustrating the value of removing shaping via canonicalization. No significant difference in the general trends
of results are observed between the two domains, and additional simulations are presented in Appendix B.
In conclusion, the proposed SARD consistently outperforms both EPIC and DARD, especially under limited
sampling when coverage is low.
Simulations and Results: Feasibility Constraints: Using rollouts (similar range from 1 to 2000) from
a uniform policy, we sample RandR′fromGTandSH, respectively. To impose feasibility constraints,
we set the stochasticity-level parameter, ϵ= 0, to restrict random transitions between states such that
13Under review as submission to TMLR
only movement to adjacent states is permitted. Figure 3b summarizes the results for the variation of reward
distances to transition coverage under the movement restrictions. As shown, SARD significantly outperforms
all the baselines. The movement restriction ensures that coverage is generally low ( <10%), even though the
number of rollouts is similar to those in the first experiment. DARD still outperforms EPIC, which performs
relatively similar to DIRECT, indicating EPIC’s degraded performance under feasibility constraints.
5.2 Experiment 2: Classifying Agent Behaviors
Objective: ThegoalofthisexperimentistoassessSARD’seffectivenessasadistancemeasureinclassifying
agent behaviors based on their reward functions. If SARD is robust, it should identify similarities among
reward functions from the same agents while differentiating reward functions from distinct agents.
Relevance: This experiment3demonstrates a practical use-case for incorporating reward comparison pseu-
dometrics to interpret reward functions by relating them to agent behavior. In many real-world situations,
samples of agent behaviors are available, and there is a need to interpret the characteristics of the agents
that produced these behaviors. For example, several works have attempted to predict player rankings and
strategies using past game histories (Luo et al., 2020; Liu & Schulte, 2018; Yanai et al., 2022). This exper-
iment takes a similar direction by attempting to classify the identities of agents from their unlabeled past
trajectories using reward functions. The reliance on rewards rather than the original trajectories is based on
the premise that reward functions are "succinct" and "robust", hence a preferable means to interpret agent
behavior (Abbeel & Ng, 2004; Michaud et al., 2020).
Approach: In this experiment, we train a k-nearest neighbors ( k-NN) classifier to classify unlabeled agent
trajectories by indirectly using computed rewards, to identify the agents that produced these trajectories.
We examine the k-NN algorithm since it is one of the most popular distance-based classification techniques.
The experiment is conducted across all domains, and since we want to maximize classification accuracy, we
consider different IRL rewards, including: Maxent, AIRL, PTIRL as well as manual (extrinsic) rewards.
For manual rewards, we utilize the default game score for the Starcraft2 and Drone Combat domains, and
polynomial rewards for the Gridworld and Bouncing Balls domains, where we induce random potential
shaping. For each domain, we examine SARD, DIRECT, EPIC, and DARD as distance measures for a
k-NN reward classification task. The steps for the approach are as follows:
1. Create agents X={x1,...,xm}with distinct behaviors.
2. For each agent, xi∈X, generate trajectories {τxi
1,...,τxip}; and compute reward functions
{Rxi
1,...,Rxip}using IRL or manual specification.
3. Randomly shuffle all the computed reward functions R(from all agents), and split into the training
Rtrainand testingRtestsets.
4. Use each reward pseudometric as a distance measure to train a k-NN classifier with Rtrainand test
it withRtest.
In step 1, different agent behaviors are controlled by varying the agents’ policies (see Appendix C.2). In
step 4, to train the classifier, grid-search is used to identify candidate values for kandγ, and twofold cross-
validation (using Rtrain) is used to optimize hyper-parameters based on accuracy. Since we assume potential
shaping, the value of γis unknown, hence its a hyper-parameter. To classify an arbitrary reward function
Ri∈Rtest, we traverse reward functions Rj∈Rtrain, and compute the distance, Dρ(Ri,Rj)using the
reward pseudometrics. We then identify the top k-closest rewards to Ri, and choose the label of the most
frequent class. We select a training to test set ratio is 70 : 30, and repeat this experiment 200times.
3Experiment 2 is related to prior works, Gleave et al. (2020) and Wulfe et al. (2022), where reward distances are computed
for the ground truth, regressed, and IRL-generated reward types. Their results show that distances from each reward type are
relatively similar, reflecting some form of reward grouping or clustering. However, these works do not explore real-world use-
cases of this ’reward grouping’ phenomenon, and our work presents a k-NN classification algorithm that utilizes reward distance
similarity (between reward sample vectors) to classify agent behaviors, without the need for additional reward preprocessing.
14Under review as submission to TMLR
Table 1: The accuracy (%) of different reward comparison distances in k−NN reward classification.
DOMAIN REWARDS DIRECT EPIC DARD SARD
GridworldManual 69.8 69.3 70.0 75.8
Maxent 57.4 57.5 68.9 70.0
AIRL 82.3 84.9 85.0 86.2
PTIRL 82.2 84.2 83.4 86.0
Bouncing BallsManual 46.5 47.3 52.0 55.2
Maxent 39.7 46.0 50.8 49.9
AIRL 41.2 46.1 49.8 56.3
PTIRL 70.3 71.1 69.5 72.4
Drone CombatManual 67.1 67.2 66.2 73.9
Maxent 70.3 77.7 73.2 76.7
AIRL 90.1 90.7 92.3 93.8
PTIRL 52.5 63.7 65.1 78.3
StarCraft 2Manual 65.5 67.4 69.5 76.5
Maxent 72.3 74.1 73.9 74.8
AIRL 75.1 75.3 78.1 77.0
PTIRL 77.2 78.1 77.6 79.8
Simulations and Results: Table 1 summarizes experimental results. As shown, SARD generally achieves
higher accuracy compared to DIRECT, EPIC and DARD across all domains, indicating SARD effectiveness
at discerning similarities between rewards produced by the same agents, and differences between those
generated by different agents. This trend is more pronounced with manual rewards where SARD significantly
outperforms other baselines. This can be attributed to potential shaping, which is intentionally induced in
manual rewards that SARD is specialized to tackle. Therefore, SARD proves to be a more effective distance
measure at classifying rewards subjected to potential shaping. For IRL-based rewards such as Maxent, AIRL,
and PTIRL, while we assume potential shaping, non-potential shaping could be present. This explains the
reduction in SARD’s performance gap over EPIC and DARD, as well as the few instances where EPIC
and DARD outperform SARD, though SARD is still generally dominant. We also observe that all the
psuedometrics tend to perform better on AIRL rewards compared to other IRL-based rewards. This result
is likely due to the formulation of the AIRL algorithm, which is designed to effectively mitigate the effects
of unwanted shaping in reward approximation (Fu et al., 2018), thus providing more consistent rewards.
Overall, SARD, EPIC, and DARD outperform DIRECT, emphasizing the importance of canonicalization at
reducing the impact of shaping.
To verify the validity of results, Welch’s t-tests for unequal variances are conducted across all domain and
reward type combinations, to test the null hypotheses: (1) µSARD≤µDIRECT, (2)µSARD≤µEPIC, and
(3)µSARD≤µDARD; against the alternative: (1) µSARD> µDIRECT, (2)µSARD> µEPIC, and (3)
µSARD>µDARD, whereµrepresents the sample mean. We reject the null when the p-value < 0.05 (level
of significance), and conclude that: (1) µSARD>µDIRECT for all instances; (2) µSARD>µEPICfor11out
of12instances, and (3) µSARD> µDARDfor10out of 12instances. These tests are performed assuming
normality as per central limit theorem, since the number of trials is 200. For additional details about the
tests and accuracy metrics such as F1-scores, refer to Appendix C. In summary, we conclude that SARD is
a more effective distance measure for classifying reward samples compared to its baselines.
6 Conclusion and Future Work
This paper introduces SARD, a reward comparison pseudometric designed to address transition sparsity, a
significant challenge encountered when comparing reward functions without high transition coverage. Con-
ducted experiments demonstrate SARD’s superiority over state-of-the-art pseudometrics, such as EPIC and
DARD, under limited sampling and feasibility constraints. Additionally, SARD proves effective as a distance
measure for k-NN classification using reward functions to represent agent behavior. This implies that SARD
15Under review as submission to TMLR
can find higher similarities between reward functions generated by the same agent and higher differences
between reward functions that are generated from different agents.
Most existing studies, including ours, have primarily focused on potential shaping, as it is the only additive
shaping technique that guarantees policy invariance (Ng et al., 1999; Jenner et al., 2022). Future research
should consider the effects of non-potential shaping on SARD (see Appendix B.5) or random perturbations,
as these might distort reward functions that would otherwise be similar. This could help to standardize
and preprocess a wider range of rewards that might not necessarily be potentially shaped. In computing
reward distances, the Pearson distance is employed for its shift and scale invariance properties. However, this
distance measure is highly sensitive to outliers, especially in the case of small reward samples. Future work
should explore modifications such as Winsorization to mitigate the impact of outliers. Future studies should
also explore applications of reward distance comparisons in scaling reward evaluations in IRL algorithms.
For example, iterative IRL approaches such as MaxentIRL, often perform policy evaluations to assess the
quality of the updated reward in each training trial. Integrating direct reward comparison pseudometrics to
determine if rewards are converging, could help to skip the policy evaluation steps, thereby speeding up IRL.
Finally, the development of reward comparison metrics has primarily aimed to satisfy policy invariance.
A promising area to examine in the future is multicriteria policy invariance, where invariance might be
conditioned to different criteria. For example, in the context of reward functions in Large Language Models
(LLMs), it might be important to compute reward distance pseudometrics that consider different criteria
such as bias, safety, or reasoning, to advance interpretability, which could be beneficial for applications such
as reward fine-tuning and evaluation (Lambert et al., 2024).
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship Learning via Inverse Reinforcement Learning. In Proceed-
ings of the Twenty-First International Conference on Machine Learning , pp. 1, 2004.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete
problems in ai safety. arXiv preprint arXiv:1606.06565 , 2016.
Koul Anurag. Ma-gym: Collection of Multi-agent Environments based on OpenAI gym. https://github.
com/koulanurag/ma-gym , 2019.
Saurabh Arora and Prashant Doshi. A Survey of Inverse Reinforcement Learning: Challenges, Method and
Progress. Artificial Intelligence , 297(1), 2021.
Monica Babes, Enrique Munoz de Cote, and Michael Littman. Social Reward Shaping in the Prisoner’s
Dilemma (Short Paper). In Proc. of 7th Int. Conf. on Autonomous Agents and Multiagent Systems
(AAMAS) , 2008.
Ching-An Cheng, Andrey Kolobov, and Adith Swaminathan. Heuristic-guided Reinforcement Learning. In
Advances in Neural Information Processing Systems , volume 34, 2021.
Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep Rein-
forcement Learning from Human Preferences. Advances in Neural Information Processing Systems , 30,
2017.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided Cost Learning: Deep Inverse Optimal Control via
Policy Optimization. In International Conference on Machine Learning , pp. 49–58, 2016.
Justin Fu, Katie Luo, and Sergey Levine. Learning Robust Rewards with Adverserial Inverse Reinforcement
Learning. In International Conference on Learning Representations , 2018.
Yang Gao and Francesca Toni. Potential Based Reward Shaping for Hierarchical Reinforcement Learning.
InTwenty-Fourth International Joint Conference on Artificial Intelligence , 2015.
AdamGleave, MichaelDennis, ShaneLegg, StuartRussell, andJanLeike. QuantifyingDifferencesInReward
Functions. In International Conference on Learning Representations , 2020.
16Under review as submission to TMLR
Adam Gleave, Mohammad Taufeeque, Juan Rocamonde, Erik Jenner, Steven Wang, Sam Toyer, Maxim-
ilian Ernestus, Nora Belrose, Scott Emmons, and Stuart Russell. Imitation: Clean imitation learning
implementations. arXiv:2211.11972v1 [cs.LG], 2022. URL https://arxiv.org/abs/2211.11972 .
Abhishek Gupta, Aldo Pacchiano, Yuexiang Zhai, Sham Kakade, and Sergey Levine. Unpacking Reward
Shaping: Understanding the Benefits of Reward Engineering on Sample Complexity. In Advances in
Neural Information Processing Systems , volume 35, 2022.
Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, and Changjie
Fan. Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping. In Advances in Neural
Information Processing Systems , volume 33, 2020.
Erik Jenner and Adam Gleave. Preprocessing Reward Functions for Interpretability. preprint
arXiv:2203.13553, 2022.
Erik Jenner, Herke van Hoof, , and Adam Gleave. Calculus on MDPs: Potential Shaping as a Gradient.
preprint arXiv:2208.09570, 2022.
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha
Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language
modeling. arXiv preprint arXiv:2403.13787 , 2024.
Guillaume Lample and Devendra Singh Chaplot. Playing FPS Games with Deep Reinforcement Learning.
InProceedings of the AAAI Conference on Artificial Intelligence , volume 31, 2017.
Guiliang Liu and Oliver Schulte. Deep reinforcement learning in ice hockey for context-aware player evalu-
ation.arXiv preprint arXiv:1805.11088 , 2018.
Yudong Luo, Oliver Schulte, and Pascal Poupart. Inverse Reinforcement Learning for Team Sports: Valuing
Actions and Players. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial
Intelligence (IJCAI-20) , 2020.
Saaduddin Mahmud, Saisubramanian Sandhya, and Zilberstein Shlomo. Explanation-Guided Reward Align-
ment. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence , pp.
473–482, 2023.
Maja J Mataric. Reward Functions for Accelerated Learning. Machine Learning Proceedings 1994 , pp.
pp. 181 – 189, 1994. doi: https://doi.org/10.1016/B978-1-55860-335-6.50030-1. URL https://www.
sciencedirect.com/science/article/abs/pii/B9781558603356500301 .
Eric J Michaud, Adam Gleave, and Stuart Russell. Understanding learned reward functions. arXiv preprint
arXiv:2012.05862 , 2020.
Andrew Y Ng and Stuart Russell. Algorithms for Inverse Reinforcement Learning. In International Confer-
ence on Machine Learning (ICML) , volume 2, 2000.
Andrew Y. Ng, Daishi Harada, and Stuart Russell. Policy Invariance under Reward Transformations: Theory
and Application to Reward Shaping. In International Conference on Machine Learning , pp. 278–287, 1999.
Jette Randløv and Preben Alstrøm. Learning to Drive a Bicycle Using Reinforcement Learning and Shaping.
InInternational Conference on Machine Leaning , volume 98, 1998.
Jacob Russell and Eugene Santos. Explaining Reward Functions in Markov Decision Processes. In The
Thirty-Second International Flairs Conference , 2019.
Eugene Santos and Clement Nyanhongo. A Contextual-Based Framework for Opinion Formation. In The
Thirty-Second International Flairs Conference , 2019.
Eugene Santos, Clement Nyanhongo, Hien Nguyen, Keum Joo Kim, and Gregory Hyde. Contextual Evalua-
tion of Human–Machine Team Effectiveness. Systems Engineering and Artificial Intelligence , pp. 283–307,
2021.
17Under review as submission to TMLR
DavidSilver, SatinderSingh, DoinaPrecup, andRichardS.Sutton. RewardisEnough. Artificial Intelligence ,
299, 2021.
Satinder Singh, Richard Lewis, and Andrew G Barto. Where Do Rewards Come From. In Proceedings of the
Annual Conference of the Cognitive Science Society , pp. pp. 2601 – 2606. Cognitive Science Society, 2009.
Joar Max Viktor Skalse, Lucy Farnik, Sumeet Ramesh Motwani, Erik Jenner, Adam Gleave, and Alessandro
Abate. Starc: A general framework for quantifying differences between reward functions. In The Twelfth
International Conference on Learning Representations , 2023.
Halit Bener Suay, Tim Brys, Matthew E. Taylor, and Sonia Chernova. Learning from Demonstration for
Shaping through Inverse Reinforcement Learning. In Proceedings of the 2016 International Conference on
Autonomous Agents Multiagent Systems , pp. 429–437, 2016.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction . MIT Press, 2018.
AnaTenorio-Gonzalez, EduardoF.Morales, andLuisVillasenor-Pineda. DynamicRewardShaping: Training
a Robot by Voice. In Advances in Artificial Intelligence–IBERAMIA 2010 , pp. 483–492, 2010.
Garrett Thomas, Yuping Luo, and Tengyu Ma. Safe reinforcement learning by imagining the near future.
Advances in Neural Information Processing Systems , 34:13859–13869, 2021.
Stephen Van Evera. Offense, defense, and the causes of war. International Security , 22(4):5–43, 1998.
OriolVinyals, IgorBabuschkin, WojciechM.Czarnecki, MichaëlMathieu, AndrewDudzik, JunyoungChung,
David H. Choi, T. Ewalds R. Powell, and J. Oh P. Georgiev. Grandmaster Level in StarCraft II using
Multi-agent Reinforcement Learning. Nature, 575:350–354, 2019.
Eric Wiewiora, Garrison Cottrell, and Charles Elkan. Principled Methods for Advising Reinforcement Learn-
ing Agents. In Proceedings of the 20th International Conference on Machine Learning , pp. 792–799, 2003.
Blake Wulfe, Logan Michael Ellis, Jean Mercat, Rowan Thomas McAllister, and Adrien Gaidon. Dynamics-
AwareComparisonofLearnedRewardFunctions. In International Conference on Learning Representations
(ICLR), 2022.
Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum Entropy Deep Inverse Reinforcement
Learning. preprint arXiv:1507.04888, 2015.
Chen Yanai, Adir Solomon, Gilad Katz, Bracha Shapira, and Lior Rokach. Q-ball: Modeling basketball
games using deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence ,
volume 36, pp. 8806–8813, 2022.
Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is Enough for
Convex MDPs. In Advances in Neural Information Processing Systems , volume 34, 2021.
Brian D. Ziebart, Andrew Maas, J.Andrew Bagnell, and Anind K. Dey. Maximum Entropy Inverse Rein-
forcmeent Learning. In Association for the Advancement of Artificial Intelligence (AAAI) , volume 8, pp.
1433–1438, 2008.
18Under review as submission to TMLR
A Derivations, Theorems and Proofs
A.1 Relative Shaping Error Comparisons
The process to compare two reward samples, R1andR2, using a canonicalization method, CS(e.g.,CSARD
orCDARD), computes the Pearson distance between the canonicalized reward samples,
Dρ(CS(R1),CS(R2)) =/radicalbig
1−ρ(CS(R1),CS(R2)/√
2 (18)
where,ρis the Pearson correlation. Assuming that the reward samples under comparison are similar in terms
of optimal policies induced, but have different values due to shaping potentials ϕ1andϕ2, respectively, and
share also share a similar base canonical reward function4,CS(R), the Pearson correlation can be expressed
as:
ρ(CS(R1),CS(R2)) =ρ(CS(R) +ϕr1,CS(R) +ϕr2), (19)
where the residual shaping potentials, ϕr1andϕr2, account for the differences between the reward samples.
•In general, when the magnitude of the shaping residuals is significantly less than that of the base
canonical reward, |ϕri|<<|CS(R)|for alli∈{1,2}thenρ(CS(R1),CS(R2))≈ρ(CS(R),CS(R)) =
1, since the shaping terms have negligible impact. Conversely, when |ϕri|>>|CS(R)|, then
ρ(CS(R1),CS(R2))≈ρ(ϕr1,ϕr2), which can reach towards 0, whenϕr1andϕr2significantly differ.
•Since the base canonical reward CS(R)is the same across the compared rewards, the Pearson
correlation variation can be primarily attributed to the impact of the residual shaping potentials.
Therefore, our analysis focuses on quantifying the influence of these residual shaping terms on the
overall correlation.
•To estimate the impact of residual shaping potentials, it is essential to consider the magnitude of the
base canonical reward, CS(R). When|CS(R)|is significantly larger relative to the residual shaping
terms|ϕri|, wherei∈{1,2}, the influence of the residual potentials diminishes, resulting in a higher
Pearson correlation, closer to 1. Conversely, if|CS(R)|is significantly smaller compared to |ϕri|,
the shaping terms exert a greater impact, leading to a lower Pearson correlation, closer to 0. To
quantify this effect, we define the Relative Shaping Error (RSE) as outlined in Definition 4.
Definition 4. (Relative Shaping Error (RSE)) Given an arbitrary reward canonicalization method CS, and
a shaped reward sample R′(assumed to be non-zero), canonicalizing R′results in: CS(R′) =CS(R) +ϕr,
whereϕrrepresents the residual shaping term. Let U(CS(R))represent the upper bound of the base canonical
reward. We define the relative shaping error as:
RSE (CS) =|ϕr|
U(CS(R)). (20)
The denominator, U(CS(R)), represents an upper bound on the magnitude of the base (unshaped) canonical
reward, and it serves to normalize the impact of shaping. It is defined as follows:
Definition 5 .(Upper Bound Canonical Reward) Let Rbe an unshaped reward sample and CS, be an
arbitrary canonicalization function represented as the sum of nterms such that:
CS(R) =R(s,a,s′) +En−1/summationdisplay
i=1[αiR(Si,A,Sj)],
4The base canonical reward CS(R)of the similar but shaped rewards R1andR2could differ in practice when the transitions
for the reward samples under comparison are different. However, since the goal of canonicalization is to mitigate the impact of
shaping during reward comparisons, for our analysis, we assume that the base canonical rewards are similar, which is guaranteed
when the transitions between the two rewards are the same. The only difference between the rewards under comparison becomes
the shaping terms, ϕ1andϕ2.
19Under review as submission to TMLR
where|α|≤1. From all subsets of rewards from R, consider the subset that maximizes the absolute expectation
|E[R(Sx,A,Sy)]|, whereSxorSycould also refer to single states. Let this maximum value be denoted as
Z∈Z+. Then, the upper bound of the canonical reward is given by:
U(CS(R)) =nZ (21)
Explanation :
CS(R) =R(s,a,s′) +En−1/summationdisplay
i=1[αiR(Si,A,Sj)]
≤Z+ (n−1)Z
≤nZ.
This RSE aims to quantify the relative influence of the residual shaping potentials on the Pearson distance
normalized by the upper bound of the base (unshaped) canonical rewards (Definition 5). A low RSE value
suggests that U(CS(R))is substantially larger than |ϕr|, indicating that the impact of shaping is likely min-
imal. Conversely, a high RSE value implies that U(CS(R))is relatively small compared to |ϕr|, highlighting
a more significant influence of shaping. By normalizing with the upper bound, the RSE provides a measure
of the worst-case impact of shaping in extreme scenarios. This formulation helps assess the robustness of
reward canonicalization methods in mitigating shaping effects during reward comparisons.
Remark 1: (RSE Adjustment due to Constant Reward Expectation Terms): After canonicalizing a poten-
tially shaped reward sample, R′with a method CS, we getCS(R′). FromCS(R′), suppose at least one of the
expectation terms is a constant in the form, αE[R′(Si,A,Sj)], where,αis a constant, and E[R′(Si,A,Sj)]
does not depend on the variation of s,aors′. Then:
CS(R′) =CS(R) +ϕr=CS(˜R) +kR+ϕ˜r+kϕ, (22)
where,kR=αE[R(Si,A,Sj)],kϕ=α(γϕ(Sj)−ϕ(Si))are both constants. The base canonical reward and
shaping terms are: CS(R) =CS(˜R) +kR, andϕr=ϕ˜r+kϕ.When comparing shaped reward samples, R1
andR2. In computing the Pearson correlation, we have:
ρ(CS(R1),CS(R2)) =ρ(CS(˜R) +kR1+ϕ˜r1+kϕ1,CS(˜R) +kR2+ϕ˜r2+kϕ2)
=ρ(CS(˜R) +ϕ˜r1,CS(˜R) +ϕ˜r2) (23)
In computing the Pearson correlation (in the process of computing the Pearson distance), we can disregard
the presence of the k-terms since they are constants and the Pearson correlation is shift invariant.
Relating this to the RSE (Equation 20), note that |ϕr|primarily serves as a measure of reward variation
due to shaping, and U(CS(R))acts as a normalizing term. In computing |ϕr|, we can disregard kϕfor
the numerator since it won’t affect the variation of the Pearson correlation. However, for the denominator
U(CS(R)), we cannot disregard kRbecause the denominator serves to normalize the residual shaping terms.
WhenkRis large enough, even though it doesn’t affect the pearson correlation, it will lower the impact of
|ϕr|and vice-versa. Therefore, in computing the RSE under kRandkϕ, the refined RSE becomes:
RSE (CS) =|ϕr|
U(CS(R))
=|ϕ˜r+kϕ|
U(CS(˜R) +kR)
=|ϕ˜r|
U(CS(R))(24)
Now for the proof, we will compare the ratios of the upper bound RSE (Equation 24) values for ˆCEPIC,
ˆCDARD, and ˆCSARD.
20Under review as submission to TMLR
Theorem 1. LetR1andR2be samples of similar reward functions under comparison, each potentially
shaped by a different shaping function, but sharing the same set of explored transitions. Under high transition
sparsity, where the fraction of sampled transitions is minimal, the upper bound of the Relative Shaping Error
(RSE) for the approximation of CSARDis lower than that of CEPICandCDARD.
Proof.
Assuming a finite reward sample that spans a state space, SDand an action space, AD. The following subsets
are defined: for ˆCEPIC,S⊆SDandS′⊆SD; for ˆCDARD,S′′⊆SDandS′⊆SD; and for ˆCSARD, each
Si⊆SD, wherei∈{1,..., 6}. The shaping components are bounded by |E[ϕ(S)]|≤M,|E[ϕ(S′)]|≤M,
|E[ϕ(S′′)]|≤M, and|E[ϕ(Si)]|≤Mfor alli∈{1,..., 6}, where:
M= max{E[|ϕ(Sx)|]|for allSx⊆SD},
andM∈R+.
Analysis of EPIC:
The equation for CEPIC (R)is given by:
CEPIC (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S )−R(s,A,S′)−γR(S,A,S′)].
The term E[R(S,A,S′)]is an expectation over all transitions, where, Scontains initial states from all
transitions, and S′contains subsequent states from all transitions. Missing transitions can exist in (stoS′)
and(s′toS′)sinceS′is subsequent to Snotsors′. Let the fraction of sampled transitions in R(s′,A,S′)
andR(s,A,S′)be given by uandv, respectively, where, 0≤u,v≤1. Incorporating uandv, we get the
approximation, ˆCEPIC (R)as:
ˆCEPIC (R)(s,a,s′) =R(s,a,s′) +E[uγR(s′,A,S′)−vR(s,A,S′)−γR(S,A,S′)], (25)
Applying ˆCEPICto a shaped reward R′(s,a,s′), we get the residual potential:
ϕepic= (γ−vγ)ϕ(s′) + (v−1)ϕ(s) +E[(uγ2−γ2)ϕ(S′)] +E[(γ−vγ)ϕ(S)]
For thebest case scenario when all necessary transitions are available (full coverage), u,v≈1, and|ϕepic|≈
0, hence, the RSE (ˆCEPIC (R))≈0, highlighting effective canonicalization. For the worst case scenario :
u,v≈0, hence we can eliminate terms with u,vin Equation 25 such that:
ˆCEPIC (R)(s,a,s′)≈R(s,a,s′)−E[γR(S,A,S′)] (26)
Applying ˆCEPIC(Equation 26) to R′(s,a,s′), we obtain the shaping term:
ϕepic≈γϕ(s′)−ϕ(s)−E[γ2ϕ(S′)] +E[(γϕ(S)] (27)
InˆCEPIC (R′), theterm−E[γR′(S,A,S′)]isaconstantsinceitdoesnotvarywithchangesin s,aors′. Using
Remark 1, the shaping component from this constant is kϕ=E[γϕ(S)−γ2ϕ(S′)],and it can be diregrarded
when computing the Pearson distance in reward comparisons. Therefore: ϕepic≈γϕ(s′)−ϕ(s) +kϕ, and:
|ϕ˜epic|≈|γϕ(s′)−ϕ(s)|(disregarded kϕ)
≤|γM+M|(Since|ϕ(s′)|≤Mand|ϕ(s)|≤M)
≤2M (Since 0≤γ≤1).
Following Definition 5, U(ˆCEPIC (R))is given by:
U(ˆCEPIC (R)) =U(R(s,a,s′)−E[γR(S,A,S′)]) = 2Z.
21Under review as submission to TMLR
Therefore, the upper bound RSE for ˆCEPICis given by:
RSE (ˆCEPIC (R))≤M
Z(28)
Analysis of DARD:
The equation for CDARDis given by:
CDARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′′)−R(s,A,S′)−γR(S′,A,S′′)],
where,S′′are subsequent states to s′andS′are subsequent states to s. For approximations, we can have
missing transitions in computing E[R(S′,A,S′′)], sinceS′′is naturally subsequent to s′, notS′. Let the
fraction of sampled transitions in R(S′,A,S′′)bew, where, 0≤w≤1. Incorporating w, we get the
approximation, ˆCDARD, as follows:
ˆCDARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′′)−R(s,A,S′)−wγR(S′,A,S′′)], (29)
Applying ˆCDARD(Equation 29) to a shaped reward R′(s,a,s′), we get the residual potential:
ϕdard=E[(γ2−wγ2)ϕ(S′′) + (wγ−γ)ϕ(S′)]
For thebest case scenario without missing transitions, w≈1, and|ϕdard|≈0, hence, the RSE (ˆCDARD )≈0,
highlighting effective canonicalization. For the worst case scenario ,w≈0, hence we can eliminate terms
withwin Equation 29. Also, in ˆCDARD, every reward expectation term varies based on the value of sand
s′, hence, we do not have shaping constants kϕto disregard in computations, such that:
|ϕ˜dard|=/vextendsingle/vextendsingleE[γ2ϕ(S′′)−γϕ(S′)]/vextendsingle/vextendsingle
≤/vextendsingle/vextendsingleγ2M+γM/vextendsingle/vextendsingle(Since|ϕ(S′)|≤Mand|ϕ(S′′)|≤M)
≤2M (Since 0≤γ≤1).
Following Definition 5, U(ˆCDARD (R))is given by:
U(ˆCDARD (R)) =U(R(s,a,s′) +E[γR(s′,A,S′′)−R(s,A,S′)]) = 3Z.
Therefore,
RSE (ˆCDARD (R))≤2M
3Z(30)
Analysis of SARD
The equation for CSARDis given by:
CSARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4) +γ2R(S1,A,S 5)
−γR(S2,A,S 6) +γR(S3,A,S 6)−γ2R(S4,A,S 5)],
where:S1andS2are subsequent states to s′ands, respectively; S3encompasses all initial states from all
transitions; S4,S5, andS6are subsequent states to S3,S1andS2, respectively. As described in Section
4, while there might be unsampled transitions from (S4toS5), and (S3toS6), approximations for SARD
are robust since a minimal set of sampled transitions is likely to exist, because: transitions (S1,A,S 5)⊆
22Under review as submission to TMLR
(S4,A,S 5), and transitions (ˆS2,A,S 6)⊆(S3,A,S 6)., where, ˆS2denotes all non-terminal states in S2. For
the approximation, ˆCSARD, let the fraction for unsampled transitions for (S4,A,S 5)and(S3,A,S 6)in the
sample bepandq, respectively, where 0≤p,q≤1, such that:
ˆCSARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4) +γ2R(S1,A,S 5)
−γR(S2,A,S 6) +qγR(S3,A,S 6)−pγ2R(S4,A,S 5)](31)
Applying ˆCSARD(Equation 31) to a shaped reward R′(s,a,s′), we get the residual potential:
ϕsard=E[(γ−qγ)ϕ(S3) + (pγ2−γ2)ϕ(S4) + (γ3−pγ3)ϕ(S5) + (qγ2−γ2)ϕ(S6)]
In thebest case scenario without missing transitions, p,q≈1, and|ϕsard|≈0, hence, the RSE (ˆCSARD )≈0,
highlighting effective canonicalization. For the worst case scenario :p,q≈0, hence we can eliminate terms
with the coefficient p,qin Equation 31. Also since the transitions (S1,A,S 5)⊆(S4,A,S 5), and (ˆS2,A,S 6)⊆
(S3,A,S 6),where, ˆS2denotes all non-terminal states in S2. Ifp,q≈0, then E[R(S1,A,S 5)] = 0, and
E[R(ˆS2,A,S 6)] = 0 =⇒E[R(S2,A,S 6)] = 0(since terminal states in S2cannot transition to S6). Therefore:
ˆCSARD (R)(s,a,s′)≈R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)]
Applying ˆCSARDto a shaped reward R′(s,a,s′)we get the residual potential:
ϕsard≈E[γ2ϕ(S1)−γ2ϕ(S4) +γϕ(S3)−γϕ(S2)].
InCSARD (R′), the term−E[γR′(S3,A,S 4)]is a constant since it does not vary due to changes in s,a
ors′. From Remark 1, the resultant shaping component from the constant, kϕ=E[γϕ(S3)−γ2ϕS4],
can be disregarded when computing the Pearson distance in reward comparisons. Therefore: ϕsard =
E[γ2ϕ(S1)−γϕ(S2)] +kϕ, and:
|ϕ˜sard|=/vextendsingle/vextendsingleE[γ2ϕ(S1)−γϕ(S2)]/vextendsingle/vextendsingledisregarded kϕ
≤/vextendsingle/vextendsingleγ2M+γM/vextendsingle/vextendsingle(Since|ϕ(S1)|≤Mand|ϕ(S2)|≤M)
≤2M (Since 0≤γ≤1).
Following Definition 5, the upper bound U(ˆCSARD (R))is given by:
U(ˆCSARD (R)) =U(R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)]) = 4Z.
Therefore,
RSE (ˆCSARD (R))≤M
2Z(32)
Conclusion
For the worst case scenario under transition sparsity, we see that:
RSE (ˆCSARD (R))≤M
2Z
RSE (ˆCDARD (R))≤2M
3Z
RSE (ˆCEPIC (R))≤M
Z
Based on the upper bounds on the RSE values, we can conclude that:
RSE (ˆCSARD (R))<RSE (ˆCDARD (R))<RSE (ˆCEPIC (R)),
showing that in the worst case scenarios of transition sparsity, ˆCSARDis likely more robust than ˆCDARD
and ˆCEPIC.
23Under review as submission to TMLR
A.2 The Sparsity Agnostic Canonically Shaped Reward is Invariant to Shaping
Proposition 2: (The Sparsity Agnostic Canonically Shaped Reward is Invariant to Shaping) Let R:
S×A×S be a reward function, ϕ:S →R, a state potential function. Assuming that, for each reward
expectation term in CSARD, the cross-product for all transitions, (Si,A,Sj)can be computed, the shaped
rewardR′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s)satisfies:
CSARD (R) =CSARD (R′).
Proof.
Let’s apply CSARD, Definition 3, to a shaped reward R′(s,a,s′):
CSARD (R′)(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s) +E[γ[R(s′,A,S 1) +γϕ(S1)−ϕ(s′)]
−[R(s,A,S 2) +γϕ(S2)−ϕ(s)]−γ[R(S3,A,S 4) +γϕ(S4)−ϕ(S3)]
+γ2[R(S1,A,S 5) +γϕ(S5)−ϕ(S1)]−γ[R(S2,A,S 6) +γϕ(S6)−ϕ(S2)]
+γ[R(S3,A,S 6) +γϕ(S6)−ϕ(S3)]−γ2[R(S4,A,S 5) +γϕ(S5)−ϕ(S4)]],
Regrouping the reward terms and the potentials, this reduces to:
CSARD (R′)(s,a,s′) =CSARD (R)(s,a,s′) + (γϕ(s′)−γE[ϕ(s′)]) + (−ϕ(s) +E[ϕ(s)])
+E[γ2(ϕ(S1)−ϕ(S1))] +E[γ(−ϕ(S2) +ϕ(S2))] +E[γ(ϕ(S3)−ϕ(S3))]
+E[γ2(−ϕ(S4) +ϕ(S4))] +E[γ3(ϕ(S5)−ϕ(S5))] +E[γ2(−ϕ(S6) +ϕ(S6))]
SinceE[γϕ(s′)] =γϕ(s′)andE[ϕ(s)] =ϕ(s), this leads to:
CSARD (R′)(s,a,s′) =CSARD (R)(s,a,s′).
A.3 Sample-Based Approximation for SARD
Definition 6 (Sample-based SARD) Given transition samples from a coverage distribution Dand a batch
BMofNMsamples from the joint state and action distributions. From BM, we can derive sets Xi⊆BM,
fori∈{1,...,6}. EachXiis a set,{(x,u)}, wherexis a state and uis an action. The magnitude, |Xi|, is
denoted by Ni. We define X1={(x1,u)}, wherex1denotes subsequent states for transitions starting from
s′;X2={(x2,u)}, wherex2denotes subsequent states for transitions that start from s;X3={(x3,u), where
x3denotes all initial states for all transitions; X4={(x4,u)}, wherex4denotes all subsequent states for
all transitions; X5={(x5,u)}, wherex5denotes subsequent states to X1;X6={(x6,u)}, wherex6denotes
subsequent states to X2. The sample-based approximation, ˆCSARDcan be computed as:
ˆCSARD (R)(s,a,s′) =R(s,a,s′) +γ
N1/summationdisplay
(x1,u)∈X1R(s′,u,x 1)−1
N2/summationdisplay
(x2,u)∈X2R(s,u,x 2)
−γ
N3N4/summationdisplay
(x3,u)∈X3/summationdisplay
(x4,·)∈X4R(x3,u,x 4) +γ2
N1N5/summationdisplay
(x1,u)∈X1/summationdisplay
(x5,.)∈X5R(x1,u,x 5)
−γ
N2N6/summationdisplay
(x2,u)∈X2/summationdisplay
(x6,·)∈X6R(x2,u,x 6) +γ
N3N6/summationdisplay
(x3,u)∈X3/summationdisplay
(x6,·)∈X6R(x3,u,x 6)
−γ2
N4N5/summationdisplay
(x4,u)∈X4/summationdisplay
(x5,·)∈X5R(x4,u,x 5).(17)
24Under review as submission to TMLR
Lemma 1. Letϕ:S→Rbe a potential function defined on a state space Sstate-based potential function.
Given states xi∈Sandxj∈S, then:
1
n1n2n1/summationdisplay
i=1n2/summationdisplay
j=1(γϕ(xi)−ϕ(xj)) =γ
n1n1/summationdisplay
i=1ϕ(xi)−1
n2n2/summationdisplay
j=1ϕ(xj).
Proof.
1
n1n2n1/summationdisplay
i=1n2/summationdisplay
j=1(γϕ(xi)−ϕ(xj)) =γ
n1n2n1/summationdisplay
i=1n2/summationdisplay
j=1ϕ(xi)−1
n1n2n1/summationdisplay
i=1n2/summationdisplay
j=1ϕ(xj)
=γ
n1n2n1/summationdisplay
i=1
n2/summationdisplay
j=1ϕ(xi)
−1
n1n2n2/summationdisplay
j=1/parenleftiggn1/summationdisplay
i=1ϕ(xj)/parenrightigg
Notice that ϕ(xi)is independent of jandϕ(xj)is independent of i,thus,
=γ
n1n2n1/summationdisplay
i=1n2ϕ(xi)−1
n1n2n2/summationdisplay
j=1n1ϕ(xj)
Simplifying terms,
=γ
n1n1/summationdisplay
i=1ϕ(xi)−1
n2n2/summationdisplay
j=1ϕ(xj).
Proposition 3: (The sample-based SARD approximation can be invariant to shaping if all transition def-
initions are fully covered.) Given a potentially-shaped reward sample R′that spans a state space SDand
an action space AD, whereDis the coverage distribution. If the SARD state definitions, Si⊆SDfor all
i∈{1,...6}, are derived from the sample, and all the necessary transition definitions are fully covered from
the sample, then: ˆCSARD (R′) =ˆCSARD (R)
Proof.
ˆCSARD (R′)(s,a,s′)≈R(s,a,s′) +γϕ(s′)−ϕ(s)
+γ
N1/summationdisplay
(x1,u)∈X1[R(s′,u,x 1) +γϕ(x1)−ϕ(s′)]
−1
N2/summationdisplay
(x2,u)∈X2[R(s,u,x 2) +γϕ(x2)−ϕ(s)]
−γ
N3N4/summationdisplay
(x3,u)∈X3/summationdisplay
(x4,·)∈X4[R(x3,u,x 4) +γϕ(x4)−ϕ(x3)]
+γ2
N1N5/summationdisplay
(x1,u)∈X1/summationdisplay
(x5,·)∈X5[R(x1,u,x 5) +γϕ(x5)−ϕ(x1)]
−γ
N2N6/summationdisplay
(x2,u)∈X2/summationdisplay
(x6,·)∈X6[R(x2,u,x 6) +γϕ(x6)−ϕ(x2)]
+γ
N3N6/summationdisplay
(x3,u)∈X3/summationdisplay
(x6,·)∈X6[R(x3,u,x 6) +γϕ(x6)−ϕ(x3)]
−γ2
N4N5/summationdisplay
(x4,u)∈X4/summationdisplay
(x5,·)∈X5[R(x4,u,x 5) +γϕ(x5)−ϕ(x4)].
25Under review as submission to TMLR
Rearranging the terms, the above equation can be written as:
ˆCSARD (R′)(s,a,s′) =ˆCSARD (R)(s,a,s′) +ϕresiduals, (33)
where:
ϕresiduals=γϕ(s′)−ϕ(s) +γ
N1/summationdisplay
(x1,u)∈X1[γϕ(x1)−ϕ(s′)]−1
N2/summationdisplay
(x2,u)∈X2[γϕ(x2)−ϕ(s)]
−γ
N3N4/summationdisplay
(x3,u)∈X3/summationdisplay
(x4,·)∈X4[γϕ(x4)−ϕ(x3)] +γ2
N1N5/summationdisplay
(x1,u)∈X1/summationdisplay
(x5,·)∈X5[γϕ(x5)−ϕ(x1)]
−γ
N2N6/summationdisplay
(x2,u)∈X2/summationdisplay
(x6,·)∈X6[γϕ(x6)−ϕ(x2)] +γ
N3N6/summationdisplay
(x3,u)∈X3/summationdisplay
(x6,·)∈X6[γϕ(x6)−ϕ(x3)]
−γ2
N4N5/summationdisplay
(x4,u)∈X4/summationdisplay
(x5,·)∈X5[γϕ(x5)−ϕ(x4)].(34)
Applying Lemma 1 to Equation 34, and simplifying terms, we get:
ϕresiduals=γϕ(s′)−ϕ(s) +γ2
N1/summationdisplay
(x1,u)∈X1[ϕ(x1)]−γϕ(s′)−γ
N2/summationdisplay
(x2,u)∈X2[ϕ(x2)] +ϕ(s)
−γ2
N4/summationdisplay
(x4,u)∈X4[ϕ(x4)] +γ
N3/summationdisplay
(x3,u)∈X3[ϕ(x3)] +γ3
N5/summationdisplay
(x5,u)∈X5[ϕ(x5)]−γ2
N1/summationdisplay
(x1,u)∈X1[ϕ(x1)]
−γ2
N6/summationdisplay
(x6,u)∈X6[ϕ(x6)] +γ
N2/summationdisplay
(x2,u)∈X2[ϕ(x2)] +γ2
N6/summationdisplay
(x6,u)∈X6[ϕ(x6)]−γ
N3/summationdisplay
(x3,u)∈X3[ϕ(x3)]
−γ3
N5/summationdisplay
(x5,u)∈X5[ϕ(x5)] +γ2
N4/summationdisplay
(x4,u)∈X4[ϕ(x4)] = 0
Therefore,
ˆCSARD (R′) =ˆCSARD (R)
.
A.4 Generalized SARD Extensions
The following steps result in the generalized formula for potential SARD extensions.
1. To eliminate EPIC’s need for full coverage (Section 4), we first create C1to ensure that rewards are
canonicalized based on actual transition sample distributions:
C1(R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)].
C1yields a residual potential: ϕres1=E[γ2ϕ(S1)−γ2ϕ(S4) +γϕ(S3)−γϕ(S2)].
2. To cancel E[ϕ(Si)],∀i∈{1,...,4}, we add rewards R(Si,A,k1
i)to induce potentials γϕ(k1
i)−ϕ(Si),
which results in C2:
C2(R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,k1
1)−γ2R(S4,A,k1
4) +γR(S3,A,k1
3)−γR(S2,A,k1
2)].
26Under review as submission to TMLR
C2yields a residual potential: ϕres2=E[γ3ϕ(k1
1)−γ3ϕ(k1
4) +γ2ϕ(k1
3)−γ2ϕ(k1
2)].
3. To cancel E[ϕ(k1
i)], we add rewards R(k1
i,A,k2
i)to induce potentials γϕ(k2
i)−ϕ(k1
i), yieldingC3:
C3(R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,k1
1)−γ2R(S4,A,k1
4) +γR(S3,A,k1
3)−γR(S2,A,k1
2)
+γ3R(k1
1,A,k2
1)−γ3R(k1
4,A,k2
4) +γ2R(k1
3,A,k2
3)−γ2R(k1
2,A,k2
2)]
C3yields a residual potential: ϕres3=E[γ4ϕ(k2
1)−γ4ϕ(k2
4) +γ3ϕ(k2
3)−γ3ϕ(k2
2)].
4. As we can see, this process results in the generalized formula:
Cn(R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,k1
1)−γ2R(S4,A,k1
4) +γR(S3,A,k1
3)−γR(S2,A,k1
2)
+γ3R(k1
1,A,k2
1)−γ3R(k1
4,A,k2
4) +γ2R(k1
3,A,k2
3)−γ2R(k1
2,A,k2
2)
···
+γnR(kn−2
1,A,kn−1
1)−γnR(kn−2
4,A,kn−1
4) +γn−1R(kn−2
3,A,kn−1
3)−γn−1R(kn−2
2,A,kn−1
2)],
where,n≥3.Cnyields a residual potential:
ϕn=E[γn+1ϕ(kn−1
1)−γn+1ϕ(kn−1
4) +γnϕ(kn−1
3)−γnϕ(kn−1
2)].
•Looking at ϕn, asnincreases, we generally multiply the state distributions, kiby (≈γn). Therefore,
the upper bound magnitude of ϕnsignificantly decreases since 0≤γ < 1, and each|ϕ(ki)|≤M,
whereMis the upper bound potential for all distributions ki⊆SD(see Appendix A.1). Therefore,
asnapproaches infinity, ϕnapproaches 0.
•The advantage of the generalized SARD form is that ϕnapproaches 0asnincreases, without any
assumptions on the distribution of a reward sample. The challenge, however, is that many kiterms
need to be computed making the process very expensive and difficult to implement. Therefore, a
smallernis preferable. In SARD, we choose n= 2, then use our intuition to select sets, ki, which
further reduces the residual potential.
A.5 Residual Potentials
Residual potentials can be defined as the remaining sum of potentials after reward canonicalization.
Derivation of ϕres1:As shown in Section 4, the equation for C1(R)(s,a,s′)is:
C1(R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)],
ApplyingC1to a shaped reward R′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s):
C1(R′)(s,a,s′) =R′(s,a,s′) +E[γR′(s′,A,S 1)−R′(s,A,S 2)−γR′(S3,A,S 4)]
=R(s,a,s′) +γϕ(s′)−ϕ(s) +E[γ(R(s′,A,S 1) +γϕ(S1)−ϕ(s′))
−(R(s,A,S 2) +γϕ(S2)−ϕ(s))−γ(R(S3,A,S 4) +γϕ(S4)−ϕ(S3))]
=C1(R)(s,a,s′) +E[γ2ϕ(S1)−γ2ϕ(S4) +γϕ(S3)−γϕ(S2)]
Hence,C1(R)(s,a,s′)yields the residual potential:
ϕres1=E[γ2ϕ(S1)−γ2ϕ(S4) +γϕ(S3)−γϕ(S2)].
27Under review as submission to TMLR
Derivation of ϕres2:As shown in Section 4, the equation for C2(R)(s,a,s′)is:
C2(R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4) +γ2R(S1,A,k 1)
−γR(S2,A,k 2) +γR(S3,A,k 3)−γ2R(S4,A,k 4)].
ApplyingC2to shaped reward R′(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s):
C2(R′)(s,a,s′) =R(s,a,s′) +γϕ(s′)−ϕ(s) +E[γ(R(s′,A,S 1) +γϕ(S1)−ϕ(s′))
−(R(s,A,S 2) +γϕ(S2)−ϕ(s))−γ(R(S3,A,S 4) +γϕ(S4)−ϕ(S3))
+γ2(R(S1,A,k 1) +γϕ(k1)−ϕ(S1))−γ(R(S2,A,k 2) +γϕ(k2)−ϕ(S2))
+γ(R(S3,A,k 3) +γϕ(k3)−ϕ(S3))−γ2(R(S4,A,k 4) +γϕ(k4)−ϕ(S4))].
=C2(R)(s,a,s′) +E[γ3ϕ(k1)−γ3ϕ(k4) +γ2ϕ(k3)−γ2ϕ(k2)]
Hence,C2(R)(s,a,s′)yields the residual potential:
ϕres2=E[γ3ϕ(k1)−γ3ϕ(k4) +γ2ϕ(k3)−γ2ϕ(k2)].
A.6 Pseudometric Equivalence Under Full Coverage
Proposition 4. The SARD, DARD, and EPIC canonical rewards are similar when a reward sample has
full coverage.
Proof.
CEPIC (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′)−R(s,A,S′)−γR(S,A,S′)]
CDARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′)−R(s,A,S′)−γR(S′,A,S′)]
CSARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,S 5)−γR(S2,A,S 6) +γR(S3,A,S 6)−γ2R(S4,A,S 5)]
Under full coverage, every state s∈S, is connected by Aactions to every other state s′∈S. Thus,
S=S′=S′′=S1=S2=S3=S4=S5=S6, such that:
CEPIC =CDARD =CSARD =R(s,a,s′) +E[γR(s′,A,S )−R(s,A,S )−γR(S,A,S )].(35)
A.7 Repeated Canonicalization Under Full Coverage
Proposition 5. The SARD, DARD, or EPIC canonical reward cannot be further canonicalized if the reward
sample has full coverage.
From Proposition 4, under full coverage, Equation 35 states that:
CS=CEPIC =CDARD =CSARD =R(s,a,s′) +E[γR(s′,A,S )−R(s,A,S )−γR(S,A,S )].
Applying this equation to canonicalize a previously canonical reward we get:
28Under review as submission to TMLR
CS[CS(R)(s,a,s′)] =CS[R(s,a,s′) +E[γR(s′,A,S )−R(s,A,S )−γR(S,A,S )]]
=CS(R(s,a,s′)) +γE[CS(R(s′,A,S ))]−E[CS(R(s,A,S ))]−γE[CS(R(S,A,S ))]
=CS(R)(s,a,s′) +γE[CS[R(s′,a,S ) +E[γR(S,A,S )−R(s′,A,S )−γR(S,A,S )]]]
−E[CS[R(s,a,S ) +E[γR(S,A,S )−R(s,A,S )−γR(S,A,S )]]]
−γE[CS[R(S,a,S ) +E[γR(S,A,S )−R(S,A,S )−γR(S,A,S )]]]
=CS(R)(s,a,s′) +γE[CS[(R(s′,A,S )−E[R(s′,A,S )]) +E[γR(S,A,S )−γR(S,A,S )]]]
−E[CS[(R(s,A,S )−E[R(s,A,S )]) +E[γR(S,A,S )−γR(S,A,S )]]]
−γE[CS[R(S,a,S ) +E[γR(S,A,S )−R(S,A,S )−γR(S,A,S )]]]
After explicit cancellations, the above equation reduces to:
CS[CS(R)(s,a,s′)] =CS(R)(s,a,s′).
A.8 Regret Bound
In this section, we establish a regret bound in terms of the SARD distance. The procedure for the analysis
is adapted from related work on EPIC by Gleave et al. (2020).
Given reward functions RAandRBand their optimal policies π∗
Aandπ∗
B, we show that the regret of
using policy π∗
Binstead of a policy π∗
Ais bounded by a function of DSARD (RA,RB). We also show that
as the regret tends to be 0 suggesting that π∗
A≈π∗
B, the distance, DSARD (RA,RB)→0. The concept of
regret bounds is important as it shows that differences in DSARDreflect differences between optimal policies
induced by the input rewards.
For our analysis, we will use the following Lemmas:
Lemma 2. Letfbe a one-dimensional vector of real numbers and fi⊆f. Then:
||fi||2≤||f||2 (36)
Proof.Supposefhasnelements and fihaskelements. Since fi⊆f, every element in fiis also inf, and
k≤n. Therefore,/summationtextf2≥/summationtextf2
i(Euclidean distance always positive) such that: ||fi||2≤||f||2.
Lemma 3. LetRA,RB:S×A×S→Rbe reward functions with corresponding optimal policies π∗
Aand
π∗
B. LetDπ(t,st,at,st+1)denote the distribution over trajectories that policy πinduces at time step t. Let
D(s,a,s′)be the coverage distribution over transitions S×A×S. Suppose that there exists some K > 0such
thatKD(st,at,st+1)≥D(t,st,at,st+1)for all time steps t∈N, triplesst,at,st+1∈S×A×Sand policies
π∈{π∗
A,π∗
B}. Then the regret under RAfrom executing π∗
Boptimal for RBinstead ofπ∗
Ais at most:
GRA(π∗
A)−GRA(π∗
B)≤2K
1−γDL1,D(RA,RB).
where:DL1,Dis either a metric or pseudometric in L1space, andGR(π)resembles the return of Runder a
policyπ.
Proof.See Gleave et al. (2020)
29Under review as submission to TMLR
Lemma 4. LetRA,RB:S×A×S→Rbe reward functions. Let π∗
Aandπ∗
Bbe policies optimal for rewards
RAandRB. Suppose the regret under the standardized reward RSARD
Afrom executing π∗
Binstead ofπ∗
Ais
upper bounded by some U∈R:
GRSARD
A(π∗
A)−GRSARD
A(π∗
B)≤U. (37)
Then the regret under the original reward RAis bounded by:
GRA(π∗
A)−GRA(π∗
B)≤8U∥RA∥2. (38)
Proof.Let the standardized reward can be represented as:
RSARD=CSARD (R)
∥CSARD (R)∥2, (39)
It follows that:
GRSARD (π) =1
∥CSARD (R)∥2GCSARD (R)(π) =1
∥CSARD (R)∥2(GR(π)−Es0∼d0[Φ(s0)]),(40)
where,s0depends only on the initial state distribution d0, but notπ. Applying Equation 40 to π∗
Aandπ∗
B:
GRSARD (π∗
A)−GRSARD (π∗
B) =1
∥CSARD (RA)∥2(GRA(π∗
A)−GRA(π∗
B)). (41)
Combining Equation 41 and 37:
GRA(π∗
A)−GRA(π∗
B)≤U∥CSARD (RA)∥2. (42)
We now bound∥CSARD (RA)∥2in terms of∥RA∥2. The SARD canonical reward is expressed as:
CSARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,S 5)−γR(S2,A,S 6) +γR(S3,A,S 6)−γ2R(S4,A,S 5)]
Now, using the triangular equality rule on the L2distance, and linearity of expectation:
||CSARD (R)(s,a,s′)||2≤||R(s,a,s′)||2+E[γ||R(s′,A,S 1)||2+||−R(s,A,S 2)||2+γ||−R(S3,A,S 4)||2
+γ2||R(S1,A,S 5)||2+γ||−R(S2,A,S 6)||2+γ||R(S3,A,S 6)||2+γ2||−R(S4,A,S 5)||2]
Using Lemma 2, the L2norm of each reward subspace is such that:
||R(Si,Aj,Sk)||2≤||R(S,A,S′)||2=||R||2. (43)
therefore,
||CSARD (R)(s,a,s′)||2≤8||R||2 (44)
Combining Equation 44 and 42 we get:
GRA(π∗
A)−GRA(π∗
B)≤8U||R||2.
Theorem 2. LetRA,RB:S×A×S→Rbe reward functions with respective optimal policies, π∗
A,π∗
B. Let
Dπ(t,st,at,st+1)be the distribution over transitions S×A×Sinduced by policy πat timet, andD(s,a,s′)
be the coverage distribution. Suppose there exists K > 0such thatKD(st,at,st+1)≥Dπ(t,st,at,st+1)for
all timest∈N, triples (st,at,st+1)∈S×A×Sand policies π∈{π∗
A,π∗
B}. Then the regret under RAfrom
executingπ∗
Binstead ofπ∗
Ais at most:
GRA(π∗
A)−GRA(π∗
B)≤32K∥RA∥2(1−γ)−1DSARD (RA,RB),
whereGR(π)is the return of policy πunder reward R.
30Under review as submission to TMLR
Proof.Adapting Gleave et al. (2020) [A.4], we can write:
DSARD (RA,RB) =1
2/vextenddouble/vextenddoubleRSARD
A (S,A,S′)−RSARD
B (S,A,S′)/vextenddouble/vextenddouble2
2. (45)
such that, when considering the L1distance:
DL1,D(RSARD
A,RSARD
B ) =/vextenddouble/vextenddoubleRSARD
A (S,A,S′)−RSARD
B (S,A,S′)/vextenddouble/vextenddouble
1≤2DSARD (RA,RB).(46)
Combining Lemma 3 and Equation 46:
GRSARD
A(π∗
A)−GRSARD
A(π∗
B)≤2K
1−γDL1,D(RSARD
A,RSARD
B )≤4K
1−γDSARD (RA,RB).(47)
Applying Lemma 4, we get:
GRA(π∗
A)−GRA(π∗
B)≤32K∥RA∥2
1−γDSARD (RA,RB). (48)
As shown, when DSARD→0, the regret goes towards 0.
A.9 Computational Considerations
Given a batch BMofNMsamples from the joint state and action distribution. the computational complexity
of all the presented psuedometrics are approximately O(N2
M).
EPIC Complexity:
CEPIC (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′)−R(s,A,S′)−γR(S,A,S′)].
The most expensive computation for EPIC is calculating E[R(S,A,S′)], which takes approximately O(N2
M)
complexity, sinceweiterate BMintwoloopstoperformthecomputation. Othercomputations, E[R(s,A,S′)]
andE[R(s′,A,S′)]takeO(NM)complexity, since we iterate BMin a single loop. Hence, the overall com-
plexity is approximately O(N2
M).
DARD Complexity:
CDARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S′′)−R(s,A,S′)−γR(S′,A,S′′)].
The most expensive computation for DARD is calculating E[R(S′,A,S′′)], which takes approximately
O(N2
M)complexity, since we iterate BMin two loops to perform the computation. Other computations,
E[R(s′,A,S′′)]andE[R(s,A,S′)]takeO(NM)complexity, since we iterate BMin a single loop. Hence, the
overall complexity is approximately O(N2
M).
SARD Complexity:
CSARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4)
+γ2R(S1,A,S 5)−γR(S2,A,S 6) +γR(S3,A,S 6)−γ2R(S4,A,S 5)].
For the terms E[R(s′,A,S 1)]andE[R(s′,A,S 1)], the computational complexity is O(NM)since we iterate
BMin a single loop. For each of the terms, E[R(S3,A,S 4)],E[R(S1,A,S 5)],E[R(S2,A,S 6)],E[R(S3,A,S 6)],
E[R(S4,A,S 5)], the computational complexity is O(N2
M), since we iterate BMin two loops. Hence, the
overall complexity is approximately 5∗O(N2
M), which is asymptotically O(N2
M).
31Under review as submission to TMLR
A.10 SARD State Definitions
Figure 4 is a graph showing transitions in a reward sample with 10statesSD={x0,...,x 9}, and a single
actionAD={a1}between state transitions. The goal here is to illustrate an example showing how states
{S1,...,S 6}are defined in SARD, as well as the state relationships: (S1⊆S4)and(ˆS2⊆S3), which make
SARD robust to missing transitions.
Figure 4: A transition graph with 10states{x0,...x 9}, and a single action {a1}. State subsets are defined
based on the transition: (x0,a1,x1).
The Sparsity Agnostic Canonical Reward is given by:
CSARD (R)(s,a,s′) =R(s,a,s′) +E[γR(s′,A,S 1)−R(s,A,S 2)−γR(S3,A,S 4) +γ2R(S1,A,S 5)
−γR(S2,A,S 6) +γR(S3,A,S 6)−γ2R(S4,A,S 5)],
where:S1andS2are subsequent states to s′ands, respectively; S3encompasses all initial states from all
transitions; S4,S5, andS6are subsequent states to S3,S1andS2, respectively.
Following the SARD definition, the states in Figure 4, are defined as follows:
s: statex0.
s′: statex1.
S1(subsequent to s′):{x2,x5,x6,x9}.
S2(subsequent to s):{x1,x3,x4,x5,x7,x8}.x8is a terminal state
S3(initial states from all transitions): {x0,x1,x2,x3,x4,x5,x7,x9}. terminal states x6andx8not included
S4(subsequent states to S3):{x1,x2,x3,x4,x5,x6,x7,x8,x9}. starting state x0not included
S5(subsequent states to S1):{x2,x4,x6,x7,x8}
S6(subsequent states to S2):{x1,x2,x3,x4,x5,x6,x8,x9}
Transition Relationships (See Section 4 for Reference)
1.S1⊆S4, therefore, (S1,A,S 5)⊆(S4,A,S 5).
2.LetˆS2=S2\terminal states .excludes terminal state x8
ˆS2⊆S3, therefore, (ˆS2,A,S 6⊆S3,A,S 6).
32Under review as submission to TMLR
B Experiment 1: Additional Analysis
B.1 Reward Functions
Extrinsic reward values are manually defined using a combination of state and action features. For the
Starcraft2 and Drone Combat domains, we use the default game score (also based on state and action
features), as the reward values. For the Gridworld and Bouncing Balls domains, in each reward function, the
reward value, R(s,a,s′); is derived from the decomposition of state and action features, where, (sf1,...,sfn)
isfromthestartingstate, s;(af1,...,afm)isfromtheaction, a; and (s′
f1,...,s′
fn)isfromthesubsequentstate,
s′. For the Gridworld domain, these features are the (x,y)coordinates, and for the Bouncing Balls domain,
theseinclude (x,y,d ), wheredisthedistanceoftheobstaclenearesttotheball. Usingthefollowingrandomly
generated constants: {u1,...,un}for incoming state features; {w1,...,wm}for action features; {v1,...vn}for
subsequent state features; we created the following reward models:
•Linear:
R(s,a,s′) =u1sf1+...+unsfn+w1af1+...+wmafm+v1s′
f1+...+vns′
fn,
•Polynomial:
R(s,a,s′) =u1sα
f1+...+unsα
fn+w1aα
f1+...+wmaα
fm+v1s′α
f1+...+vns′α
fn,
where,αis randomly generated from 1−10, denoting the degree of the polynomial.
•Sinusoidal:
R(s,a,s′) =u1sin(sf1) +···+unsin(sfn) +w1sin(af1) +···+wmsin(afm)
+v1sin(s′
f1) +···+vnsin(s′
fn)
•Random
R(s,a,s′) =β,
where,βis a randomly generated reward for each given transition.
The same relationships are used to model potential functions, where: ϕ(s) =f(sf1,..,sfn), andfis the
relationship drawn from the set: {polynomial, sinusoidal, linear, random}. For the Starcraft 2 and Drone
Combat domains, we used the default game score provided the game engine, as the reward function. For
the Starcraft2 domain, this score5focuses on the composition of unit and resource features as well as actions
within the domain. Since the Drone Combat environment is originally designed for a predator-prey domain,
we adapt the score6to essentially work the same for the Drone Combat scene (i.e instead of a predator being
rewarded for eating some prey, the reward is now an ally attacking an enemy).
5https://steemit.com/steemstem/@cpufronz/building-a-bot-for-starcraft-ii-2-the-starcraft-ii-environment
6https://github.com/koulanurag/ma-gym/blob/master/ma gym/envs/
33Under review as submission to TMLR
B.2 Transition Sparsity Experiment
Algorithm 1 summarizes the pseudocode for Experiment 1 to examine transition sparsity. To test the effect
of limited sampling, we run the algorithm with different number of rollouts (rollout count), dictated by
the arrayT. To test the effect of feasibility constraints, we run Algorithm 1 but with imposed movement
restrictions by setting ϵ= 0.
Algorithm 1 Analyzing the effect of limited sampling on reward distance
Input:
T- list of policy rollout counts,
E- number of experimental trials under same condition,
G- grid size,
RD- list to store reward distances at different coverages,
MC- maximum coverage ≈S×A×S.
Output:RD
1:generateGT- ground truth reward, SH- shaped reward from all possible transitions.
2:forrollout_count inTdo
3:trial_distance ,trial_coverage = list(), list()
4:fortrialinEdo
5:Bgt,Bsh=set(),set()
6:generate trajectories τgtandτshusing uniform policy rollouts.
7:for(s,a,s′)∈τgtdo
8:Bgt.add(s,a,s′)
9:end for
10:for(s,a,s′)∈τshdo
11:Bsh.add(s,a,s′)
12:end for
13:for(s,a,s′)∈Bgt, and (s,a,s′)∈BshretrieveR(s,a,s′)usingGTandR′(s,a,s′)usingSH,
respectively.
14:coverage =|Bgt∪Bsh|/MC
15:computedist(R,R′)using EPIC, SARD, DARD, or other comparison metrics.
16:trial_distance .append(dist(R,R′))
17:trial_coverage .append( coverage)
18:end for
19:RD.append([mean( trial_coverage ), mean(trial_distance )])
20:end for
B.3 Experimental Parameters
A uniform policy in the Gridworld domain, would randomly select one of the four actions, {north, east,
south, west}, at each timestep. For the Bouncing Balls domain, it would select {north, north-east, east,
east-south, south, south-west, west, west-north, north}, randomly. The parameter ϵdictates the ratio of
times in which random transitions (instead of uniform policy), are executed. Table 2 and Table 3 shows the
experimental parameters used to run Experiment 1 (Algorithm 1).
34Under review as submission to TMLR
Table2: LowCoverage: ParametersusedtotestthevariationofcoveragefortheGridworldandtheBouncing
Balls domain.
Parameter Values
Rollout Counts, T [1,2,3,4,5,6,7,8,9,10,15,20,30,40,50,75,100,200,300,400,500,1000,2000]
Epochs,E 200
Policy,π uniform,ϵ= 0.1
Discount,γ 0.7
Dimensions 20×20
Table 3: Feasibility Constraints: Parameters used to test the variation of coverage in the presence of move-
ment restrictions, ϵ= 0.
Parameter Values
Rollout Counts, T [1,2,3,4,5,6,7,8,9,10,15,20,30,40,50,75,100,200,300,400,500,1000,2000]
Epochs,E 200
Policy,π uniform,ϵ= 0
Discount,γ 0.7
Dimensions 20×20
B.4 Transition Sparsity: Additional Results
Parameter Variation In both the Gridworld and the Bouncing Balls domains, we did not we did not see
much difference in the structure of results between the 10×10domain and the 20×20domains. Results
were fairly consistent in that SARD tends to outperform DARD and EPIC, and feasibility constraints tend
to limit coverage significantly.
Figure 5: 10×10Gridworld: Variation of reward relationships
35Under review as submission to TMLR
Figure 6: 20×20Gridworld: Variation of reward relationships
Figure 7: 10×10Bouncing Balls: Variation of reward relationships
36Under review as submission to TMLR
Figure 8: 20×20Bouncing Balls: Variation of reward relationships
Figure 9: Here we compare the ratio of the residual potentials to the actual rewards for reward samples
under comparison, on a 20×20Bouncing Balls domain. As shown, SARD is less prone to residual shaping
compared to DARD and EPIC.
37Under review as submission to TMLR
B.5 Deviations from Potential Shaping
Figure 10: Non-Potential Shaping Effect: As the severity of randomly generated noise increases, rewards
deviate more from potential shaping, hence all the pseudometrics degrade in performance. In the end, the
pseudometrics perform to the level of DIRECT, showing that canonicalization does not yield any additional
advantages at these levels.
Most reward comparison pseudometrics are designed with the goal of canonicalizing rewards that differ
due to potential shaping. In this experiment, we examine how deviations from non-potential shaping can
affect the performance of these pseudometrics. Within a 20×20Gridworld domain, we generate a ground
truth (GT) polynomial reward function and a corresponding shaped reward function ( SH), both with full
coverage ( 100%transitions). From GTandSH, we sample rewards RandR′using uniform policy rollovers.
For both samples, we add additional noise, N, with the following severity levels: None:N= 0,Mild:
|N|≤max(|R′|), andHigh:|N|≤5max(|R′|), whereNis randomly generated from a uniform distribution
within bounds defined by these severity levels. Thus, the updated shaped reward is given by:
R′′=R′+N.
Figure 10 shows the performance variation of the reward comparison pseudometrics to the severity levels,
dictated by N. As shown, when N= 0(noise free), the difference between SARD, EPIC, DARD, and DI-
RECT is the greatest, with a performance order: SARD >DARD>EPIC>DIRECT, which demonstrates
SARD’s performance advantage over other pseudometrics under potential shaping. As the impact of N
increases, the shaped reward, R′′, becomes almost entirely comprised of noise, and the shaping component
significantly deviates from potential shaping. As shown, SARD’s performance gap over other pseudometrics
significantly diminishes. At high severity (Figure 10c), SARD’s performance is nearly identical to all other
pseudometrics, including DIRECT, which does not involve any canonicalization. In conclusion, these results
still demonstrate SARD’s superiority in canonicalizing rewards even with minor deviations from potential
shaping (Figure 10 b). However, as the rewards become non-potentially shaped, all pseudometrics generally
become ineffective, performing similarly to non-canonicalized techniques such as DIRECT.
C Experiment 2: Additional Analysis
C.1 Reward Classification: Testbeds and IRL
Gridworld: The Gridworld domain simulates agent movement from a given initial state to a specified
terminal state under a static policy. Each state is defined by an (x,y)coordinate where 0≤x < N,
and0≤y < Mimplying|S|=NM. For Experiment 2, the action space only consists of four cardinal
directions {north, east, south, west}, and to define classes, we use static policies based on the action-selection
distribution (out of 100) per state. Table 1 shows the Gridworld parameters used for Experiment 2.
Bouncing Balls: The Bouncing Balls domain, adapted from (Wulfe et al., 2022), simulates a ball’s
motion from a starting state to a target state while avoiding randomly mobile obstacles. These obstacles
38Under review as submission to TMLR
add complexity to the environment since the ball might need to change its strategy to avoid obstacles (at a
distance,d= 3). Each state is defined by the tuple (x,y,d ), where (x,y)indicates the ball’s current location,
anddindicates the ball’s Euclidean distance to the nearest obstacle, such that: 0≤x < N,0≤y < M,
andd≤max(M,N ). The action space includes eight directions (cardinals and ordinals), with the stochastic
parameterϵfor choosing random transitions. Table 2 describes the parameters for Experiment 2.
Figure 11: Bouncing Balls domain: The red ball starts at a randomly assigned state and aims to reach the
green state while avoiding being close to the black obstacles. The presence of the obstacles makes the domain
more complex than the simple Gridworld.
DroneCombat: TheDroneCombatdomainisderivedfromthemulti-agentenvironment, whichsimulates
a predator-prey interaction (Anurag, 2019). We adapt this testbed to simulate a battle between two drone
swarms; a blue swarm denoting the ally team; and a red swarm denoting a default AI enemy. The goal is for
the blue ally team to defeat the default AI team. This testbed offers discrete actions and states within a fully
observable environment, while also offering flexibility for unit creation, and obstacle placement. However,
the number of states and actions is still high such that we didn’t use it in Experiment 1. Each unit (blue
and red squares) possesses a distinct set of parameters and possible actions. Each team consists of drones
and ships, and the team that wins either destroys the entire drones of the opponent or its ship. This ship
adds complexity to the decision-making process of the teams which need to engage with the enemy, as well
as safeguard their ships. Each drone is defined by the following attributes: visibility range (VR) - the range
a unit can see from its current position (partial observability); health (H) - the number of firings a unit can
sustain; movement range(MR) - the maximum distance that a unit can move to; and shot strength(SS) - the
probability of a shot hitting its target. All these attributes are drawn from the set:
U={(VR,H,MR,SS )|VR∈{1,3,5},H∈{5,10,15},MR∈{1,2,3},SS∈{0.05,0.1}}
Table 5 summarizes the parameters used for Experiment 2.
39Under review as submission to TMLR
Figure 12: Drone Combat: The drone combat domain describes a battlefield scene where the blue team aims
to attack the red AI team. The brown squares present movement obstacles; the green square represents the
enemy’s ship; and the purple square represents the team’s ship.
Starcraft 2 (SC2): The SC2 domain is a strategy game created by Blizzard Entertainment that features
real-time actions on a complex battlefield environment. The game involves planning, strategy, and quick
decision-making to control a multi-agent ally team, aiming to defeat a default AI team in a competitive,
challenging, and time-sensitive environment. SC2 serves not only as entertainment but also as a platform
for professional player competitions. Due to its complexity and the availability of commonly used interactive
Python libraries, the SC2 game is widely employed in Reinforcement Learning, serving as a testbed for
multi-agent research. The goal of the ally team is to gather resources, and build attacking units that are
used to execute a strategic mission to defeat the AI enemy; within an uncharted map, that gets revealed
after extensive exploration (introduces partial observability). The sheer size of the map and the multitude
of possible actions for each type of unit, as well as the number of units, contribute to the enormity of the
action and state spaces. During combat, each ally unit, moves in a decentralized manner and attacks an
enemy unit using an assigned cooperative strategy from the set: C={c1,c2,c3,c4}; wherec1- move towards
ally closest to enemy’s start base; c2- move towards a random enemy unit; c3- move towards ally closest
to an enemy unit; and c4- move towards the map’s center. We focus on attack-oriented ally units to reduce
the state space. Non-attacking units such as pylons are treated as part of the environment. The game state
records the number of ally units ( numally), and the total number of opponent units ( numenemy); as well as
the central coordinates of the ally and the enemy. The action records the number of ally units attacking the
enemy at an instance. Table 6 describes the Starcraft 2 parameters used for Experiment 2.
Figure 13: (Starcraft 2): The domain describes a multiagent team that aims to defeat a default AI enemy.
In this figure, the the red section denotes the team’s base where it builds resources and attacking units. In
the green section, it shows the enemy’s base, where the team is attacking the enemy.
40Under review as submission to TMLR
C.2 Testbed Parameters:
In all these domains, the optimal values for γ(discount factor) and k(the neighborhood size) are not fixed
for each independent trial. Therefore, for hyperparameter selection, we employ a grid search over the set
defined by:
{(γ,k) :γ∈{0,0.1,..., 1},k∈{5,10,15,..., 100}}
The agent classes shown describe the policy that an agent takes in each given state. For example, in the
Gridworld domain, an agent with a policy [25, 25, 25, 25], randomly selects the cardinal direction to take
from a uniform distribution. For the Drone Combat and Starcraft 2 domains, the agent behaves based on
the combination of attributes defined (refer to Appendix C.1).
Table 4: Bouncing Balls parameters for Experiment 2.
Parameter Values
Agent fixed policies (10 classes) [[12, 12, 12, 12, 13, 13, 13, 13], [5, 5, 25, 25, 25, 5, 5, 5], [25, 25, 25, 5,
5, 5, 5, 5], [5, 5, 5, 5, 5, 25, 25, 25], [5, 5, 65, 5, 5, 5, 5, 5], [5, 5, 5, 65,
5, 5, 5, 5], [5, 5, 5, 5, 65, 5, 5, 5], [5, 25, 5, 25, 5, 25, 5, 5], [20, 5, 20, 5,
20, 5, 20, 5], [5, 20, 5, 20, 5, 20, 5, 20]]
Trajectory sets per policy 100
Number of obstacles 5
Distance to deviate from obstacle
(Manhattan)3
Number of comparison trials 200
Actions move: {north, north-east, east, east-south, south, south-west, west,
west-north}
Dimensions 20×20
Table 5: Gridworld parameters for Experiment 2.
Parameter Values
Agent fixed policies (10 classes), [[25, 25, 25, 25], [5, 5, 5, 85], [85, 5, 5, 5], [5, 85, 5, 5], [5, 5, 85, 5], [5,
15, 30, 55], [55, 30, 15, 5], [15, 5, 55, 30], [5, 55, 30, 15], [15, 30, 5, 55]]
Trajectory sets per policy, 100
Number of comparison trials, 200
Actions, move: {north, west, south, east}
Dimensions 20×20 ( 400 states)
41Under review as submission to TMLR
Table 6: Drone Combat parameters for Experiment 2.
Parameter Values
Agent policies 10classes, each consisting of 5agents. Each agent xhas attributes
randomly drawn from the set:
U = {(VR, H, MR, SS) |VR∈ {1,3,5},H∈ {5,10,15},MR∈
{1,2,3},SS∈{0.05,0.1}}
Trajectory sets per policy 100
Number of agents per team 11 (1 ship, 10 drones)
Number of comparison trials 200
Actions,αdenotes movement
range, 1≤α≤3{{leftα,upα,rightα,downα},attack}
Dimensions 40×25, with obstacles occupying ≈30%of the area
Table 7: Starcraft 2 parameters for Experiment 2.
Parameter Values
Agent policies (description in Star-
craft2domain), generatedbasedon
resources and strategy10 classes, agents attributes randomly chosen from:
U={(c,u)|c∈{c1,c2,c3,c4},u∈{adept,voidray,phoenix,stalker}}.
Trajectory sets per policy 100
Comparison trials 200
Actions Number of attacking units per unit time
State representation (numally,numenemy,(xally,yally),(xenemy,yenemy ))
C.3 Inverse Reinforcement Learning (IRL)
In our experiments, we utilize Inverse Reinforcement Learning (IRL) to compute agent rewards based on
demonstrated behavior. Specifically, we employ three IRL algorithms: Maximum Entropy IRL (Maxent-
IRL) (Ziebart et al., 2008); Adversarial IRL; and Preferential Trajectory IRL (PT-IRL) (Santos et al., 2021).
In addition, we compute manual rewards that differ due to potential shaping.
A trajectory τj={(s0,a0),(s1,a1),...,(sd)}is a sequence of states and actions. A set of trajectories can be
written as:
φ={τ1,τ2,...τh},h∈Z+.
Maxent IRL The objective of the Maxent IRL7algorithm is to compute a reward function that will
generate a policy (learner) πLthat matches the feature expectations of the trajectories generated by the
expert’s policy (demonstrations, assumed to be optimal) πE. Formally, this objective can be expressed as:
EπL[ϕ(τ)] =EπE[ϕ(τ)], (49)
where Eπk=/summationtext
τ∈φkpπk(τ)·ϕ(τ), andpπk(τ)is the probability distribution of selecting trajectory τfromπk
generated by the set of trajectories φk. Maxent IRL assumes that the relationship between state features
and agent rewards is linear. To resolve the ambiguity of having multiple optimal policies which can explain
an agent’s behavior, this algorithm applies the principle of maximum entropy to select rewards yielding a
policy with the largest entropy.
7MaxentandAIRLimplementationsadaptedfrom: https://github.com/HumanCompatibleAI/imitation(Gleaveetal.,2022)
42Under review as submission to TMLR
AIRL: The AIRL algorithm uses generative adversarial networks to train a policy that can mimic the
expert’s behavior. The IRL problem can be seen as training a generative model over trajectories as:
max
wJ(w) =max
wEτ∼D[logpw(τ)], (50)
wherepw(τ)∝p(s0)/producttextT−1
t=0P(st+1|st,at)eγt.Rw(st,at)and the parameterized reward is Rw(s,a). Using the
gradient of J(w), theentropy-regularized policy objective can be shown to reduce to:
max
πEπ/bracketleftiggT/summationdisplay
t=0(Rw(st,at)−logπ(at|st))/bracketrightigg
(51)
The discriminator is designed to take the form:
Dw(s,a) =exp{fw(s,a)}
exp{fw(s,a)}+π(a|s), (52)
and thetraining objective aims to minimize the cross-entropy loss between expert demonstrations and
generated samples:
L(w) =T/summationdisplay
t=0(−ED[logDw(st,at)]−Eπt[log(1−Dw(st,at))]) (53)
The policy optimization objective then uses the reward:
R(s,a) =log(Dw(s,a))−log(1−Dw(s,a)) (54)
The AIRL formulation can be seen as an extension of the Guided Cost Learning (GCL) by Finn et al. (2016),
with scalability modifications of analyzing data from a state-action level, rather than a trajectory-centric
formulation. For example, in Finn et al. (2016), Equation 3.10 uses parameters Dw(τ)instead ofDw(s,a).
The AIRL formulation can also be shown to be equivalent to the Maxent IRL formulation.
PTIRL: The PTIRL algorithm incorporates multiple agents, each with a set of demonstrated trajectories
φi. In order to compute the rewards for each agent, PTIRL considers target and non-target trajectories.
Target trajectories are demonstrated trajectories from a target agent, and non-target trajectories are demon-
strated trajectories from other agents. Denoting Pwas the probability transition function for all the agents,
the linear expected reward for each trajectory τis defined as
LER (τ) =m/summationdisplay
k=1Pw(sk′,ak,sk)·r(sk′,ak,sk).
For each trajectory set, there is a lower bound value lb(φ)and an upper bound value ub(φ), respectively
defined as
lb(φ) =minτ∈φ(LER (τ))
and
ub(φ) =maxτ∈φ(LER (τ)).
Withlb(φ)andub(φ)definitions, the spread δis defined as
δ(φa,φb) =lb(φa)−ub(φb).
43Under review as submission to TMLR
Preferential ordering for any two trajectories φaandφbis denoted by a poset, ≺, such that if φb≺φa, then
δ(φa,φb)>0.
Given the above definitions, let φibe the set of target trajectories and φnithe set of non-target trajectories.
Assuming that the rewards of the target agent will ensure its behavior towards the target trajectories, the
PT-IRL objective is to find rewards such that φnt≺φi,δ(φi,φnt)≥α, whereαis the minimum threshold
for the spread. PT-IRL is generally fast because it directly computes rewards via linear optimization.
C.4 Reward Classification: Additional Results
Table 8: Experiment 2: Welch’s t-tests
SARD_vs_DIRECT SARD_vs_EPIC SARD_vs_DARD
Domain Rewardst-statistic p-value t-statistic p-value t-statistic p-value
Manual 11.522 0 12.478 0 10.385 0
Maxent 28.496 0 28.142 0 2.593 0.005
AIRL 13.610 0 5.117 0 4.266 0 Gridworld
PTIRL 11.209 0 5.719 0 7.725 0
Manual 18.801 0 17.375 0 6.955 0
Maxent 32.341 0 12.104 0 -2.586 0.995
AIRL 45.020 0 28.226 0 19.488 0 Bouncing Balls
PTIRL 5.089 0 3.101 0.001 7.096 0
Manual 16.152 0 15.851 0 16.786 0
Maxent 17.829 0 -2.543 0.994 9.123 0
AIRL 9.772 0 8.023 0 3.935 0 Drone Combat
PTIRL 61.534 0 34.679 0 30.384 0
Manual 24.419 0 20.633 0 15.760 0
Maxent 6.171 0 1.717 0.043 2.233 0.013
AIRL 4.992 0 4.300 0 -2.913 0.998 Starcraft 2
PTIRL 6.054 0 3.631 0 4.961 0
In Table 8, we show the comprehensive results for the Welch’s t-tests for unequal variances, which are
conducted across all domain and reward type combinations, to test the null hypotheses: (1) µSARD≤
µDIRECT, (2)µSARD≤µEPIC, and (3)µSARD≤µDARD; against the alternative: (1) µSARD>µDIRECT,
(2)µSARD>µEPIC, and (3)µSARD>µDARD, whereµrepresents the sample mean. Generally, the tests
indicate that (1) µSARD>µDIRECT for all instances; (2) µSARD>µEPICfor11out of 12instances, and
(3)µSARD>µDARDfor10out of 12instances. These tests are performed at a significant level of α= 0.05,
assuming normality as per central limit theorem, since the number of trials is 200. In summary, we conclude
that the SARD pseudometric is more effective at classifying reward samples compared to its baselines. Full
details on accuracy scores collected are shown in Table 9 and Table 10.
44Under review as submission to TMLR
Table 9: Experiment 2: Accuracy results
Domain Rewards Statistic DIRECT EPIC DARD SARD
GridworldManualmean 69.8 69.3 70.0 75.8
stdev 4.6 4.6 5.0 4.6
Maxentmean 57.4 57.5 68.9 70.0
stdev 4.5 4.5 4.5 4.4
AIRLmean 82.3 84.9 85.0 86.2
stdev 3.0 1.8 2.6 2.7
PTIRLmean 82.2 84.2 83.4 86.0
stdev 3.5 3.3 3.5 3.3
Bouncing BallsManualmean 46.5 47.3 52.0 55.2
stdev 4.8 4.6 4.8 4.5
Maxentmean 39.7 46.0 50.8 49.9
stdev 3.1 3.3 3.2 3.2
AIRLmean 41.2 46.1 49.8 56.3
stdev 3.4 3.9 3.3 3.3
PTIRLmean 70.3 71.1 69.5 72.4
stdev 4.2 4.3 4.1 4.0
Drone CombatManualmean 67.1 67.2 66.2 73.9
stdev 4.1 4.2 4.9 4.2
Maxentmean 70.3 77.7 73.2 76.8
stdev 3.7 3.8 4.2 3.5
AIRLmean 90.1 90.7 92.3 93.8
stdev 3.9 3.9 3.7 3.7
PTIRLmean 52.5 63.7 65.1 78.3
stdev 4.3 4.3 4.6 4.1
Starcraft 2Manualmean 65.5 67.4 69.5 76.5
stdev 4.6 4.5 4.5 4.4
Maxentmean 72.3 74.1 73.9 74.8
stdev 4.1 4.1 4.2 4.1
AIRLmean 75.1 75.3 78.1 77.0
stdev 4.0 4.0 3.8 3.8
PTIRLmean 77.2 78.1 77.6 79.6
stdev 4.1 4.2 4.2 4.0
45Under review as submission to TMLR
Table 10: Experiment 2: Precision, Recall, F1-scoresDomain
Rewards
StatisticDIRECT EPIC DARD SARDprecision
recall
f1-score
precision
recall
f1-score
precision
recall
f1-score
precision
recall
f1-scoreGridworldMaxentmean 64.757.656.864.857.756.972.968.867.876.170.169.6
stdev 3.54.14.13.44.04.03.74.94.83.64.34.2
Manualmean 74.470.068.873.669.468.174.070.168.478.776.173.2
stdev 4.14.44.74.54.44.84.54.54.94.23.34.2
AIRLmean 77.582.578.385.985.284.078.385.581.081.186.482.5
stdev 2.31.92.31.51.11.72.11.51.81.91.71.9
PTIRLmean 77.982.377.980.484.279.878.083.678.882.586.481.8
stdev 5.52.83.05.12.22.74.82.42.95.72.22.8Bouncing BallsManualmean 44.646.339.745.247.340.546.852.444.052.755.745.8
stdev 7.13.94.47.03.64.16.43.34.07.02.73.9
Maxentmean 33.539.734.141.145.940.951.150.647.342.949.842.4
stdev 2.62.32.03.12.62.45.63.03.13.42.82.8
AIRLmean 35.041.235.745.046.143.149.650.146.658.456.750.9
stdev 3.02.72.73.83.03.03.42.82.95.62.33.1
PTIRLmean 67.469.764.369.270.565.164.269.061.865.471.966.2
stdev 6.42.73.86.42.73.86.42.73.85.12.83.5Drone CombatManualmean 64.167.160.567.167.064.065.166.362.767.473.170.2
stdev 4.04.14.03.94.14.05.03.94.44.54.24.3
Maxentmean 65.170.367.674.177.672.874.673.571.074.076.671.3
stdev 3.83.63.73.63.83.74.53.23.84.03.53.7
AIRLmean 90.090.189.088.290.489.390.292.586.392.293.691.9
stdev 4.03.93.93.84.03.94.23.84.03.63.63.6
PTIRLmean 50.252.548.360.464.360.763.165.064.679.377.778.5
stdev 4.84.34.54.64.44.84.74.44.44.54.04.2Starcraft 2Manualmean 60.265.462.764.167.364.770.169.267.976.176.974.5
stdev 4.54.64.54.84.44.64.53.53.94.74.34.5
Maxentmean 73.172.269.670.474.170.269.173.270.172.774.773.7
stdev 4.04.14.04.54.54.34.44.24.14.04.24.1
AIRLmean 73.875.070.473.973.869.876.777.973.776.878.676.7
stdev 3.74.03.84.33.94.13.93.93.93.94.03.9
PTIRLmean 72.177.172.577.678.677.178.177.075.780.079.576.7
stdev 3.94.14.04.24.34.44.54.34.24.03.94.0
46