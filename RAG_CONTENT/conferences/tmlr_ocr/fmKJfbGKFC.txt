Published in Transactions on Machine Learning Research (12/2024)
Do Parameters Reveal More than Loss for
Membership Inference?
Anshuman Suri as9rw@virginia.edu
University of Virginia
Xiao Zhang xiao.zhang@cispa.de
CISPA Helmholtz Center for Information Security
David Evans evans@virginia.edu
University of Virginia
Reviewed on OpenReview: https: // openreview. net/ forum? id= fmKJfbGKFC
Abstract
Membershipinferenceattacksareusedasakeytoolfordisclosureauditing. Theyaimtoinfer
whether an individual record was used to train a model. While such evaluations are useful
to demonstrate risk, they are computationally expensive and often make strong assumptions
about potential adversaries’ access to models and training environments, and thus do not
provide tight bounds on leakage from potential attacks. We show how prior claims around
black-box access being sufficient for optimal membership inference do not hold for stochastic
gradient descent, and that optimal membership inference indeed requires white-box access.
Our theoretical results lead to a new white-box inference attack, IHA(InverseHessian
Attack), that explicitly uses model parameters by taking advantage of computing inverse-
Hessian vector products. Our results show that both auditors and adversaries may be able
to benefit from access to model parameters, and we advocate for further research into white-
box methods for membership inference.
1 Introduction
Models produced by using machine learning on private training data can leak sensitive information about
data used to train or tune the model (Salem et al., 2023). Researchers study these privacy risks by either
designing and evaluating attacks to simulate what motivated adversaries may be able to infer in particular
settings or by developing privacy methods that provide strong guarantees, often based on some notion of
differential privacy (Dwork et al., 2006), that bounds information disclosure from any attack. Although both
developing attacks and formal privacy proofs are important, conducting meaningful privacy audits is different
from both approaches. Empirical methods, usually in the form of attack simulations, are inherently limited
by the attacks considered and the uncertainty about the possibility of better attacks, while theoretical proofs
require many assumptions or result in loose bounds. Further, any claims based on theoretical results depend
on careful analysis that the system as implemented is consistent with the theory. If there is a theoretical
result that prescribes an optimal attack, then empirical results with that attack (or approximations of the
attack) can offer a more meaningful estimate of privacy risk than is possible with theory or experiments
alone. While the theory needs to cover all data distributions, experiments with an optimal attack focus on
the actual distribution and given model, resulting in tighter and more relevant privacy evaluations.
Privacy audits can also be important in adversarial contexts, where a regulator or external advocate conducts
them to test a released model. Auditors with elevated model access (such as associated training environments
or data) may be able to take advantage of more information to produce better estimates of what an adversary
could do without that information. Auditing is orthogonal to proofs that establish differential privacy
1Published in Transactions on Machine Learning Research (12/2024)
bounds or other privacy notions. As outlined by Cummings et al. (2024), theoretical bounds may be “too
conservative or inaccurate in some settings”, and it may not always be possible to come up with proofs or
theoretical bounds that ensure models do not “violate disclosure requirements in ways that are not captured
by differential privacy”. Empirical auditing can provide a more meaningful measure of privacy leakage for
these situations.
The most common disclosure auditing approach today is to conduct membership inference attacks (Kumar
& Shokri, 2020) and related attacks that attempt to extract specific data (Cummings et al., 2024). While
membership inference assumes the adversary already knows the full candidate record, it may still constitute
a direct privacy risk when revealing the inclusion of a known record in the training data itself, which leaks
sensitive information. In most scenarios, however, membership disclosure by itself is not a serious privacy
risk, but rather used as a proxy for understanding information leakage that may result in more serious
privacy violations. Membership inference is simple to define, relatively easy to measure, and aligns well with
differential privacy. This has resulted in it being widely used as a method for auditing disclosure risks for
machine learning (Kumar & Shokri, 2020; Yeom et al., 2020; Kazmi et al., 2024; Azize & Basu, 2024).
Prior results on membership inference attacks have largely focused on the black-box setting, where the
attacker only has input–output access to the target model. This focus has been reinforced by folklore and
results demonstrating negligible gains from parameter access (known as white-box attacks) (Nasr et al., 2018;
Carlinietal., 2022). Awell-knowntheoreticalresultby Sablayrollesetal.(2019) provesthatblack-boxaccess
is sufficient for optimal membership inference under certain conditions. This result has been the basis of
several subsequent works (Ye et al., 2022; Chaudhari et al., 2024). However, the assumptions made in its
derivation do not hold for most models, including ones trained with stochastic gradient descent (SGD). This
theoretical result has detered researchers from exploring more the potential for inference methods that utilize
parameter access, even though the theoretical result does not apply to common machine learning settings.
Contributions. In this work, we revisit previous assumptions surrounding the optimality of membership
inference attacks. Utilizing recent advances in discrete-time SGD-dynamics (Liu et al., 2021; Ziyin et al.,
2021), we provide a more accurate formulation of the optimal membership inference attack that demonstrates
the limitations of the results from Sablayrolles et al. (2019). In particular, we show that the claim that black-
box access is sufficient does not hold for models trained using SGD (Section 3). Our theoretical result also
prescribes an attack that exploits white-box access for auditing membership leakage, which we call the
Inverse Hessian Attack (IHA) (Section 3.3). We empirically demonstrate its effectiveness in simple settings,
showing that it outperforms state-of-the-art reference-model-based and prior white-box attacks (Section 4).
Our analyses suggest that the improved auditing performance can be directly attributed to access to the
model’s parameters. .
2 Preliminaries
The section provides background on membership inference (Section 2.1) and SGD dynamics (Section 2.2).
2.1 Membership Inference
Following the framework established by Sablayrolles et al. (2019), let Dbe a data distribution from which
nrecordsz1,z2,...,znare i.i.d. sampled with zi= (xi,yi)being thei-th record. Let w∈Rdbe the model
parameters produced by some machine learning algorithm on a training dataset D. Assumem1,m2,...,mn
follow a Bernoulli distribution with γ=P(mi= 1), wheremiis the membership indicator of zi(i.e.,mi= 1
ifzi∈D, andmi= 0otherwise). Given w, amembership inference attack aims to predict the unknown
membership mifor any given record zi.
Definition 2.1 (Membership Inference) .Letwbe the parameters of the target model and z1be a record.
Inferring the membership of z1to the training set of wis equivalent to computing:
M(w,z1) =P(m1= 1|w,z1).
2Published in Transactions on Machine Learning Research (12/2024)
LetP(w|z1,...,zn,m1,...,mn)be the posterior distribution of model parameters produced by some ran-
domized machine learning algorithm (i.e., stochastic gradient descent). Applying Bayes’ theorem, Sablay-
rolles et al. (2019) derived the following explicit formula for M(w,z1).
Lemma 2.1 (Sablayrolles et al. (2019)) .LetT={z2,...,zn,m2,...,mn}. Given model parameters w
and a record z1, the optimal membership inference is given by:
M(w,z1) =ET/bracketleftbigg
σ/parenleftbigg
ln/parenleftbiggp(w|m1= 1,z1,T)
p(w|m1= 0,z1,T)/parenrightbigg
+ ln/parenleftbiggγ
1−γ/parenrightbigg/parenrightbigg/bracketrightbigg
, (1)
whereσ(u) = (1 + exp(−u))−1is the Sigmoid function, and γ=P(m1= 1).
To use Lemma 2.1, one needs to characterize the posterior, P(w|z1,...,zn,m1,...,mn), to make explicit
the effect of the inferred record z1on the optimal membership inference M(w,z1). Recent advances in
discrete-time SGD dynamics (Liu et al., 2021; Ziyin et al., 2021) literature can help provide a connection
between the posterior and model parameters.
2.2 Discrete-time SGD Dynamics
Alineoftheoreticalwork(Welling&Teh,2011;Sato&Nakagawa,2014;Stephanetal.,2017;Liuetal.,2021;
Ziyin et al., 2021) has analyzed the continuous- and discrete-time dynamics of stochastic gradient methods
and provided insights for understanding deep learning generalization. Let Ltot(w) =L(w) +α
2∥w∥2
2be the
ℓ2-regularized total loss that we aim to optimize, where α≥0denotes the hyperparameter that controls the
regularization strength. Consider an SGD algorithm with the following update rule (for t= 1,2,3,...):


gt=∇Ltot(wt−1) +ηt−1;
ht=µht−1+gt;
wt=wt−1−λht.(2)
Here,µ∈[0,1)is the momentum, λ>0is the learning rate, and
ηt=1
S/summationdisplay
i∈Bt(∇ℓ(wt,zi) +αwt)−∇Ltot(wt) =1
S/summationdisplay
i∈Bt∇ℓ(wt,zi)−∇L(wt)
represents the unbiased mini-batch noise, where Btis a randomly sampled batch of examples with size S
from the training dataset D, andL(w) =1
n/summationtext
z∈Dℓ(w,z).
Assuming a model is trained using SGD according to the update rule defined by Equation 2 on a quadratic
lossand arrives at a stationary state , Liu et al. (2021) established a theoretical connection between the
Hessian matrix H, the asymptotic noise covariance C= limt→∞Ewt[cov(ηt,ηt)], and the asymptotic model
fluctuation Σ= limt→∞cov(wt,wt). We next lay out the two imposed assumptions.
Assumption 1 (Quadratic Loss) .The total loss function Ltot(w)is either globally quadratic or locally
quadratic close to a local minimum w∗. Specifically, the loss function can be approximated as:
Ltot(w) =Ltot(w∗) +1
2(w−w∗)⊤(H(w∗) +αId)(w−w∗) +o(∥w−w∗∥2
2), (3)
wherew∗is a local minimum, H(w∗)denotes the Hessian matrix at w∗with respect to the unregularized
loss function L(w), and Idstands for the d×didentity matrix.
Assumption 2 (Stationary-State) .After a sufficient number of iterations, models trained with SGD defined
by Equation 2 arrive at a stationary state, i.e., the asymptotic model fluctuation Σexists and is finite.
Undertheaboveassumptions,Liuetal.(2021)provedthefollowingtheoremthatdescribesmodelfluctuations
of discrete SGD in a quadratic potential with connections to the Hessian matrix and the noise covariance:
Theorem 2.2 (SGD Stationary distribution with momentum) .Letwbe updated with SGD defined by
the update rule in Equation 2 with momentum µ∈[0,1). Under Assumptions 1 and 2, if we additionally
suppose Ccommutes with H(w∗), then the asymptotic model fluctuation satisfies
Σ=/bracketleftbiggλ/parenleftbig
H(w∗) +αId/parenrightbig
1 +µ·/parenleftbigg
2Id−λ/parenleftbig
H(w∗) +αId/parenrightbig
1 +µ/parenrightbigg/bracketrightbigg−1λ2C
1−µ2.
3Published in Transactions on Machine Learning Research (12/2024)
Theorem 2.2 requires the existence of a finite stationary noise covariance and that the loss function is
quadratic close to a local minimum, which are both mild assumptions (see Liu et al. (2021) for detailed
discussions).
In a follow-up work, Ziyin et al. (2021) further derived the explicit dependence of the finite stationary noise
covariance Con the loss and Hessian around a local minimum w∗under certain assumptions:
Theorem 2.3 (SGD Noise Covariance) .LetLtot(w) =L(w)+α
2∥w∥2
2be the total loss with α≥0. Assume
the modelwis optimized with SGD defined by Equation 2 around a local minimum w∗. IfL(w∗)̸= 0, then
C=2L(w∗)
SH(w∗)−α2
Sw∗w∗⊤+O(S−2) +O(∥w−w∗∥2
2) +o(L(w∗)),
provided that Σis proportional to S−1and|L(w)−ℓ(w,zi)|is small (i.e., of order o(L(w)).
The first imposed assumption of Σ=O(S−1)has been justified by prior works (Liu et al., 2021; Xie
et al., 2021; Mori et al., 2022); the second assumption assumes that the current total training loss L(w)
approximates well the individual loss for each record ℓ(w,zi). Also, note that Theorem 2.3 directly implies
that the SGD noise covariance Ccommutes with the Hessian matrix H(w∗).
Based on the above two theorems and only considering the leading term in the noise covariance, we can
immediately derive the following formula for the stationary model fluctuation of SGD:
Σ=λ
S(1−µ)/parenleftbigg
2L(w∗)H(w∗)−α2w∗w∗⊤/parenrightbigg/parenleftbigg
H(w∗) +αId/parenrightbigg−1/parenleftbigg
2Id−λ
1 +µ/parenleftbig
H(w∗) +αId/parenrightbig/parenrightbigg−1
.(4)
We remark that if L(w∗) = 0(i.e.,w∗is a global minimum), then Σ=0. In addition, if the Hessian matrix
(H(w∗) +αId)has degenerate rank r < d, then (H(w∗) +αId)−1can be replaced by the corresponding
Moore-Penrose pseudo inverse. Accordingly, similar results to Equation 4 can be obtained by considering the
projection space spanned by eigenvectors with non-zero eigenvalues. Section 5 of Ziyin et al. (2021) provides
more detailed discussions of the imposed assumptions and the implications of the results.
3 Black-Box Access is not Sufficient
In this section, we examine previous assertions concerning optimal membership inference (Section 3.1) and
show, for models trained with SGD, that optimal membership inference requires parameter access (Sec-
tion 3.2). Our theory directly implies an attack (Section 3.3).
3.1 Limitations of Claims of Black-Box Optimality
Sablayrolles et al. (2019) proved the optimality of black-box membership inference under a Bayesian frame-
work. They assume (Equation 1 in Sablayrolles et al. (2019)) that the posterior distribution of model
parameterswtrained onz1,...,znwith membership m1,...,mnfollows
P(w|z1,...,zn)∝exp/parenleftbigg
−1
Tn/summationdisplay
i=1mi·ℓ(w,zi)/parenrightbigg
, (5)
whereTis a temperature parameter that captures the stochasticity of the learning algorithm. This as-
sumption makes subsequent derivations of optimal membership inference much easier, but oversimplifies the
trainingdynamicsoftypicalmachinelearningalgorithmssuchasSGD.Equation5assumesthattheposterior
ofwfollows a Boltzmann distribution that only depends on the training loss. This is desirable for Bayesian
posterior inference, where the goal is to provide a sampling strategy for an unknown data distribution given
a set of observed data samples. This can be achieved using stochastic gradient Langevin dynamics (SGLD)
(Welling & Teh, 2011) with shrinking step size λt(i.e., limt→∞λt= 0) and by injecting carefully-designed
Gaussian noiseN(0,λt·ID). However, this special SGLD design differs from the common practice of SGD
algorithms used to train neural networks in two key ways:
4Published in Transactions on Machine Learning Research (12/2024)
1. SGLD performs all analyses under continuous-time dynamics whereas actual SGD is performed with
discretesteps. WhilerelatedworksuchasStephanetal.(2017)castthecontinuous-timedynamicsof
SGD as a multivariate Ornstein-Uhlenbeck process (similar to SGLD) whose stationary distribution
is proven to be Gaussian (Equations 11 and 12 in Stephan et al. (2017)), they make additional
assumptions such as the noise covariance matrix being independent of model parameters.
2. SGLD assumes a vanishing learning rate until convergence, whereas SGD is performed with a non-
vanishing step size and for a finite number of iterations in practice. The learning rate of SGD is
often large, which can cause model dynamics to drift even further from SGLD (Ziyin et al., 2023),
especially in the discrete-time setting (Liu et al., 2021).
We thus characterize the analytical form of the posterior distribution with respect to model parameters
trained with SGD:
Theorem 3.1 (Posterior for SGD) .Assume the same assumptions as used in Theorems 2.2 and 2.3. Let w∗
be the local minimum that SGD (Equation 2) is converging towards. Then, the (conditional) log-probability
of observing parameters wis given by (up to constants and negligible terms):
−d
2lnL∗+d/summationdisplay
i=1ln/parenleftbigg/parenleftbig
2−λ
1+µ(σi(H∗) +α/parenrightbig/parenleftbig
σi(H∗) +α/parenrightbig
σi(H∗)/parenrightbigg
−S(1−µ)
2λ/parenleftbigg
1−λα
1 +µ/parenrightbigg
·∥w−w∗∥2
2
L∗
−S(1−µ)α
4λ·/parenleftbigg
2−λα
1 +µ/parenrightbigg
·∇L(w)⊤H−3
∗∇L(w)
L∗+S(1−µ)
2(1 +µ)·L(w)
L∗,
whereL∗=L(w∗),H∗=H(w∗)andσi(H∗)denotes the i-th largest eigenvalue of H∗.
A proof for Theorem 3.1 is given in Appendix B. Theorem 3.1 suggests that the posterior distribution of
model parameters learned by SGD not only relies on the training loss L(w)but is also crucially dependent
on other terms, such as the Hessian structure H∗, the gradient∇L(w)and theℓ2distance∥w−w∗∥2,
confirming that Equation 5 is insufficient to model the dynamics of a discrete-time SGD algorithm.
3.2 Optimal Membership Inference under Discrete-time SGD
We have explained why the assumption imposed by Sablayrolles et al. (2019) about the posterior distribution
ofwfollowing a Boltzmann distribution (Equation 5) does not hold for stochastic gradient methods typically
employed in practice. Next, we prove a theorem that gives an estimate of the optimal membership inference
scoring function by leveraging recent results on discrete-time SGD dynamics (Liu et al., 2021; Ziyin et al.,
2021). Our derivation is based on the assumptions that the loss achieved at the local minimum is unaffected
by removing a single training record and that the Hessian structure remains unchanged.
Assumption 3 (Similarity at local minimum) .For anyTandz1, letL0(w) =1
n/summationtextn
i=2miℓ(w,zi)and
L1(w) =1
n(ℓ(w,z1) +/summationtextn
i=2miℓ(w,zi)). When the training dataset differs only by a single data point z1,
assume that the Hessian matrix structure for models trained with and without the differing point share a
similar structure, and the loss function also achieves a similar value at the local minimum:
H∗=H0(w∗
0) =H1(w∗
1), L∗=L0(w∗
0) =L1(w∗
1), (6)
wherew∗
0is the local minimum that SGD with L0is converging towards, and H0denotes the Hessian matrix
with respect to L0(and likewise for w∗
1,L1, and H1).
As long as the size of the training dataset is sufficient and the excluded training record z1is not a low-
probabilityoutlierfromthedatadistribution D, weexpectAssumption3generallyholdsforSGDalgorithms.
Under Assumption 3 and a few other assumptions imposed in prior literature on discrete-time SGD dynamics
(Liu et al., 2021; Ziyin et al., 2021), we obtain a theorem (proof is in Appendix C) that describes the scoring
function for an optimal membership-inference adversary:
5Published in Transactions on Machine Learning Research (12/2024)
Theorem 3.2 (Optimal Membership-Inference Score) .Givenwproduced by an SGD algorithm defined by
Equation 2 and a record z1, the optimal membership inference M(w,z1)is given by:
ET/bracketleftbigg
σ/parenleftbiggS(1−µ)
2nL∗·/parenleftbiggℓ(w,z1)
1 +µ−1
λ(I1+I2+I3+I4)/parenrightbigg
+ ln/parenleftbiggγ
1−γ/parenrightbigg/parenrightbigg/bracketrightbigg
, (7)
whereI1,I2,I3, andI4are defined as follows:
I1:=1
n/parenleftbigg
1−λα
1 +µ/parenrightbigg
·∥H−1
∗∇ℓ(w,z1)∥2,
I2:= 2/parenleftbigg
1−λα
1 +µ/parenrightbigg
·/parenleftbig
H−1
∗∇L0(w)/parenrightbig⊤/parenleftbig
H−1
∗∇ℓ(w,z1)/parenrightbig
,
I3:=α
2n/parenleftbigg
2−λα
1 +µ/parenrightbigg
·/parenleftbig
H−1
∗∇ℓ(w,z1)/parenrightbig⊤/parenleftbig
H−1
∗/parenleftbig
H−1
∗∇ℓ(w,z1)/parenrightbig/parenrightbig
,
I4:=α/parenleftbigg
2−λα
1 +µ/parenrightbigg
·/parenleftbig
H−1
∗∇L0(w)/parenrightbig⊤/parenleftbig
H−1
∗/parenleftbig
H−1
∗∇ℓ(w,z1)/parenrightbig/parenrightbig
.
Here,L∗andH∗are defined in Assumption 3, and are dependent on T. Here,Trefers to the set of
both member and non-member records along with their corresponding membership indicators, as defined in
Lemma 2.1. Note that computing the optimal score requires access to the Hessian and model gradients, both
of which require access to the model parameters. In fact, knowledge of the learning rate λ, momentum µ,
and regularization parameter αare also required, thus requiring complete knowledge of the training setup
of the target model. Thus, black-box access is notsufficient for optimal membership inference.
The first two additional terms I1andI2can be interpreted as the magnitude and direction, respectively, of a
Newtonian step for the given record z1. The first term I1characterizes the influence magnitude in ℓ2-norm
of upweighting z1on the model parameters close to the local minimum (Koh & Liang, 2017), while the
second term captures the alignment between the influence of z1and the averaged influence of the remaining
trainingdata. Alarger influence magnitude ofz1oranincreased influence alignment suggestsahigherriskof
membership inference. We remark that the notion of a self-influence function introduced in Cohen & Giryes
(2024) naturally relates to I1, suggesting a similar insight to ours that better membership inference attacks
can be designed by leveraging the influence function of the inferred record. The last two additional terms, I3
andI4, originate from the extra L2regularization term imposed on the training loss of SGD (Section 2.2).
When the regularization parameter αis a very small positive constant, the effects of I3andI4on optimal
membership inference will be negligible, particularly compared to those of I1andI2.
3.3 Inverse Hessian Attack
While Theorem 3.2 directly prescribes an optimal membership inference adversary, computing the expecta-
tion over all possible models trained using the rest of the training data is infeasible. Our definition of optimal
membership inference corresponds to the true leakage of the model (as defined in Section 3.2 of Ye et al.
(2022)). It utilizes worst-case adversary knowledge (membership of all other training records) and white-box
access to estimate the influence of the target record, similar to how empirical attacks such as LiRA (Carlini
et al., 2022) and RMIA (Zarifzadeh et al., 2023) use reference models to account for atypical examples.
Specifically, making use of the insight of Theorem 3.2, we propose a scoring function based on the terms
inside the expectation in Equation 7:
IHA(z1) :=ℓ(w,z1)
1 +µ−1
λ/parenleftbig
I1+I2+I3+I4/parenrightbig
. (8)
Thescore,IHA (z1),forsomegivenrecord z1,canbeusedastheprobabilityof z1beingamember. Thisserves
directly as a useful attack for privacy auditing, without needing to train any reference models. Not having to
train reference models offers significant advantages. It helps auditors avoid additional computational costs
and, more importantly, eliminates the need for trainers to reserve hold-out data for reference model training.
6Published in Transactions on Machine Learning Research (12/2024)
This is particularly beneficial when data availability is a constraint for privacy auditing methods relying on
reference models. While the absence of a negative sign with the loss function (like in LOSS) in IHA (z1)may
seem counter-intuitive at first glance, it can be rewritten such that it is proportional to the negation of the
loss function (Appendix C.2).
While membership leakage is typically evaluated on a fixed dataset, the theoretical notion of optimal mem-
bership inference is defined for a much larger space. This space encompasses a broad distribution of possible
data (including both member and non-member records) and the corresponding models trained on various
splits of these datasets. In practice, it’s challenging to realize this larger space, but if we could define true
positive rates (TPRs) at low false positive rates (FPRs) with respect to this comprehensive space, they
would empirically correspond to the optimal membership inference attack. This theoretical framework pro-
vides a more robust understanding of membership inference, though its practical implementation remains a
significant challenge in privacy auditing.
The performance of our audit is also influenced by other factors, such as how efficiently and accurately
the inverse-Hessian vector products (iHVPs) can be computed and to what degree our assumptions hold
(particularly Assumption 3, which requires the Hessian and loss at local minima being unaffected by the
exclusion of a single datapoint).
4 Experiments
To evaluate IHA, we efficiently pre-compute ∇L1(w)to facilitate the computation of ∇L0(w)for any given
target record z1. For accurate Hessian matrix computations, we address the issue of ill-conditioning due
to near-zero and small negative eigenvalues by either damping or using low-rank approximations (damping-
based conditioning seems to perform best; see Appendix E for details). To support larger models where
direct Hessian computation is infeasible, we extend our method to use approximation methods based on
Conjugate Gradients for iHVP computation (Koh & Liang, 2017). Our implementation for reproducing all
the experiments is available as open-source code at https://github.com/iamgroot42/auditingmi .
Section 4.1 describes the baseline attacks, datasets, and models we use for our experiments. Section 4.2
summarizes our results, showing that IHA provides a robust privacy auditing baseline, matching or exceeding
the performance of current state-of-the-art attacks including attacks that use reference models. This is
notable since IHA does ont require training any reference models or the use of hold-out data.
For a given false positive rate (FPR), a threshold is computed using scores for non-members, which is then
used to compute the corresponding true positive rate (TPR). This is repeated for multiple FPRs to generate
the corresponding ROC curve, which is used to compute the AUC. This experimental design is commonly
used for membership-inference evaluations (Yeom et al., 2018; Carlini et al., 2022; Ye et al., 2022).
4.1 Setup
To evaluate IHA, we compare its performance to state-of-the-art baseline attacks with a representative set
of datasets and models.
Baseline Attacks. We include LOSS as a baseline that does not use reference models, SIF as it uses iHVP
similar to our audit, and LiRA as it is the current state-of-the-art for membership inference. While RMIA
(Zarifzadeh et al., 2023) uses fewer reference models, it achieves performance comparable to LiRA and thus
for the sake of performance comparison, it suffices to use LiRA with a large number of reference models. We
describe the underlying access assumptions for these attacks in Table 1.
LOSS(Yeom et al., 2018) .The negative loss is used in this attack as a direct signal for membership inference.
SIF(Cohen & Giryes, 2024) .Similar to ours, this attack employs the loss curvature of the target model
by computing its Hessian, which is then used to compute a self-influence score. The original attack assigns
0–1 scores to target records. It classifies a given record as a member if its self-influence score is within the
specified range and if its predicted class is correct. The latter rule can be ruled out as having many false
7Published in Transactions on Machine Learning Research (12/2024)
Attack Model Access Reference Models Used? Leave-one-out knowledge?
LOSS Black-box No No
LiRA Black-box Yes No
SIF White-box No No
L Black-box Yes Yes
LiRA-L Black-box Yes Yes
IHA (Ours) White-box No Yes
Table 1: Comparison of attacks based on level of model-access, use of reference models, and knowledge of
other training members.
positives/negatives. Instead of these steps, we choose to use the self-influence as membership scores directly.
While the authors used approximation methods for iHVP, we use the exact Hessian for fair comparison.
LiRA(Carlini et al., 2022) .There are two variants, LiRA-Offline , which uses “offline” models to estimate
a Gaussian distribution and then performs one-sided hypothesis testing using loss scores, and LiRA-Online ,
with additionally employs “online” models, i.e., models whose training data includes the target record. The
likelihood ratio for online/offline model score distributions is then used as the score for membership inference.
We use LiRA-Online, since it is the strongest of the two variants.
L-Attack (Ye et al., 2022) .The L-attack operates in a leave-one-out setting, training reference models on
D\{z}for any given record z. It uses loss as the target metric and computes attack thresholds for a desired
false positive rate (FPR) by leveraging the distribution of losses obtained from reference models.
LiRA-L. We propose combining the LiRA attack for the LOO-setting by utilizing reference models trained
under leave-one-out availability, followed by the offline variant of the LiRA attack (Carlini et al., 2022).
Datasets. Since we are limited by the computational constraints of computing iHVPs, we restrict our
experiments to datasets where small models can perform adequately.
Purchase-100(S). The task for this dataset (Shokri et al., 2017) is to classify a given purchase into one of
100 categories, given 600 features. We train 2-layer MLPs (32 hidden neurons) with cross-entropy loss, with
an average test accuracy of 84%. Experiments by Zarifzadeh et al. (2023) train larger (4-layer MLP) models
on 25K samples from Purchase-100, which is much smaller than the actual dataset, which is why we term
it Purchase-100(S) (Small). We also demonstrate results with a 4-layer MLP that achieves similar task
accuracy.
Purchase-100. For this version, we train models with 80K samples. We use the same 2-layer MLP archi-
tecture as Purchase-100(S) but achieve a higher test accuracy of 90%. Using more data increases the scope
for model performance. We report results for Purchase-100 in Table 2 as the corresponding models are less
prone to overfitting. For completeness, we report results for Purchase-100(S) in Appendix D.
MNIST-Odd. We consider the MNIST dataset (LeCun et al., 1998), with the modified task of classifying a
given digit image as odd or even. This modified task allows us to train models for binary classification using
the regression loss, and is thus highly likely to follow the assumptions made in our theory regarding quadratic
behavior for the loss function (Assumption 1). We train a logistic regression model with mean-squared error
loss, with an average test loss of .078.
FashionMNIST. We use the FashionMNIST (Xiao et al., 2017) dataset, where the task is to classify a given
clothing item image into one of ten categories. We train 2-layer MLPs (6 hidden neurons) with cross-entropy
loss, with an average test accuracy of 83%.
8Published in Transactions on Machine Learning Research (12/2024)
Table 2: Performance of various attacks, reported via attack AUC and true positive rate (TPR) at low false
positive rate (FPR). ROC curves for low FPR region are visualized in Figure 1 (Appendix).
AttackPurchase-100 MNIST-Odd FashionMNIST
AUC% TPR@FPR
AUC% TPR@FPR
AUC% TPR@FPR
1% 0.1% 1% 0.1% 1% 0.1%
LOSS .531 ±.0010.97 0.00 .500 ±.0030.97 0.09 .507 ±.0021.00 0.10
SIF .531 ±.0010.97 0.10 .500 ±.0020.97 0.10 .507 ±.0020.98 0.10
LiRA .644 ±.0044.70 0.98 .568±.0052.77 0.63 .578±.0202.98 0.63
IHA (Exact) .703±.00413.69 7.52 .542±.0042.61 0.51 .594±.0184.06 0.89
Models. We train 128 models in the same way as done in Carlini et al. (2022), where data from each model
is sampled at random from the actual dataset with a 50% probability. For each target model and target
record, there are thus 127 reference models available, half of which are expected to include the target record
in the training data. All of our models are trained with momentum ( µ= 0.9) and regularization ( α= 5e−4),
with a learning rate λ= 0.01. For a given false positive rate (FPR), a threshold is computed using scores
for non-members, which is then used to compute the corresponding true positive rate (TPR). This is then
repeated for multiple FPRs to generate the corresponding ROC curve, which is used to compute the AUC.
This experimental design is commonly used for membership-inference evaluations (Yeom et al., 2018; Carlini
et al., 2022; Ye et al., 2022).
4.2 Results
As summarized in Table 2, IHA provides a strong privacy auditing baseline that is competitive with current
state-of-the-art attacks that require reference models. This is especially useful, considering that IHA does
not require training any reference models and thus, does not require any hold-out data to train such reference
models. IHA performs much better than the baselines on tabular data (Purchase-100), and is competitive
with the baseline for image-based data (MNIST-Odd, Fashion MNIST). By extension, our method also
outperforms previous membership inference attacks that specifically utilize parameter access via white-box
access (Nasr et al., 2018; Cohen & Giryes, 2024), since such methods are outperformed by LiRA (Carlini
et al., 2022). While tabular data is a more realistic setting for membership inference and the improved
performance on Purchase-100 is promising, we leave to future work further investigation of these factors to
better understand the performance discrepancies.
Approximating iHVPs. In order to carry out IHA, an auditor needs to be able to calculate iHVPs and
gradients for all training data. While computing gradients is more computationally intensive than simply
calculating the loss, the difference is minimal. On the other hand, computing an iHVP involves calculating
the Hessian matrix and then inverting it, both of which are computationally expensive processes. Even
storing such an inverted Hessian can be problematic ( p×pmatrix for a model with pparameters). We thus
experiment with evaluating IHA using Conjugate Gradients (Koh & Liang, 2017) to approximate iHVP.
While such approximation does not require computing the Hessian directly, the time taken to compute this
term for each record is non-trivial. We thus evaluate this approximation-based method on a random sample
of 10000 records1and find that approximation methods retain most of the attack’s performance (Table 3).
We emphasize that the purpose of our comparisons is not to claim a better membership inference attack
for adversarial use; the threat models are not comparable, since our attack requires knowledge of all other
recordsD\{z1}for inferring a given target record z1(relaxing this assumption leads to severe performance
degradation, see Appendix F). Instead, IHA provides a way to empirically audit models for membership
leakage without training reference models, which is desirable in avoiding the need to reserve hold-out data
for training reference models. More importantly, our results suggest untapped opportunities in exploring
1For a direct comparison, we recompute metrics for the 10000 samples on which we use approximate-based variants.
9Published in Transactions on Machine Learning Research (12/2024)
Table 3: Performance of exact and approximation-based variants of IHA, reported via attack AUC and true
positive rate (TPR) at low false positive rate (FPR). Statistics are computed on 10000 samples.
DatasetExact CG
AUC%TPR@FPRAUC%TPR@FPR
1% 0.1% 1% 0.1%
Purchase-100 .701 ±.00913.74 7.56 .701 ±.00913.72 7.55
MNIST-Odd .541 ±.0092.76 0.43 .541 ±.0092.76 0.43
FashionMNIST .593 ±.0184.10 0.85 .592 ±.0184.09 0.86
Table 4: Performance of IHA on Purchase100-S (MLP-2) when only some of the terms corresponding to
Equation 8 are used. I1andI2seem to be responsible for most of the privacy auditing performance.
Terms Used AUC%TPR@FPR
1% 0.1%
I1 .591±.0031.04 0.09
I2 .704±.0042.63 0.70
I1,I2 .779±.00317.61 16.65
I1,I2,I3,I4.779±.00317.60 16.64
ℓ(w,z1),I1.594±.0031.12 0.12
ℓ(w,z1),I2.686±.0041.97 0.48
ℓ(w,z1),I1,I2.791±.00319.96 19.00
All .791 ±.00520.09 19.09
parameter access for stronger privacy audits as well as the possibility of new white-box inference attacks
from an adversarial lens.
4.3 Ablating over terms inside IHA
As described in Equation 8, calculating IHA requires computing the loss along with the four additional terms
I1,I2,I3andI4. However, the terms I3andI4are scaled by α(which is usually very small) and involve an
iHVP of an iHVP and thus may be much smaller compared to terms like I1,I2and the loss. We explore
variants of IHA, which ignore the terms I3andI4, to see how they impact auditing performance. We also
consider variants that use only the terms I1andI2to understand the importance of their contributions to the
performance of IHA, along with the inclusion or not of the loss term to understand the relative importance
of parameter-based signals.
The ablation study presented in Table 4 reveals several key insights about the performance of IHA. Excluding
I3andI4has negligible impact on auditing performance, even for low-FPR scenarios. The combination of I1
andI2aloneachievesanAUCof .779, andperformanceisidenticalwhentheterms I3andI4arealsoincluded.
Notably, the addition of the loss term ℓ(w,z1)toI1andI2results in a marginal improvement, increasing
the AUC to .791and slightly boosting the TPR at both 1% and 0.1% FPR. Interestingly, when examined
individually, I2(AUC.704) performs significantly better than I1(AUC.591), suggesting that I2captures
more relevant information for the auditing task. Including the loss term ℓ(w,z1)has little impact on I1but
harms performance when used wth I2. These findings indicate that the majority of the attack’s effectiveness
stems from the inverse Hessian vector products used in I1andI2, withI2being particularly important, while
the terms involving weight regularization and nested iHVPs ( I3andI4) contribute minimally to the overall
performance. While not as impactful as I2, the loss term still provides valuable information for the auditing
process. Based on these results, a simplified version of IHA using only ℓ(w,z1),I1, andI2could potentially
offer a favorable trade-off between computational efficiency and auditing effectiveness.
10Published in Transactions on Machine Learning Research (12/2024)
Table 5: Agreement rate between ground truth (GT) membership values, and various attacks for 500 training
and 500 testing data points. The upper triangle of the table corresponds to the agreement rates of members,
whereas the lower triangle corresponds to the agreement rates of non-members. The experimental setup is
Purchase-100(S), with effective FPR ≈0.05(a) and effective FPR ≈0.3(b).
LiRA L LiRA-L IHA GT
LiRA .794 .826 .718 .220
L .930 .908 .712 .234
LiRA-L .916 .938 .768 .154
IHA .912 .926 .916 .230
GT .954 .952 .954 .958
(a) Agreement between methods with FPR 0.05LiRA L LiRA-L IHA GT
LiRA .732 .658 .436 .652
L .724 .766 .492 .668
LiRA-L .588 .660 .462 .518
IHA .612 .660 .584 .640
GT .702 .702 .706 .706
(b) Agreement between methods with FPR 0.3
4.4 Comparison with Leave-One-Out Setting
When targeting a record for inference, IHA assumes knowledge of all the other n−1records in an n-sized
dataset. It is possible that the improved performance of IHA is due to this extra information rather than
inherent parameter access. To isolate and analyze these potential sources of increase in leakage, we also
assess the performance of a leave-one-out (LOO) membership inference test. We evaluate the L-attack (Ye
et al., 2022) on 1000 samples, training 100 reference models per record for score calibration. Since targeting
each record requires training multiple reference models for the L-attack, evaluating it on a larger sample of
data is computationally infeasible.
These results indicate that IHA outperforms the L-attack, achieving an AUC value of .791compared to
.737for the L-attack.2This suggests that even when controlling for the additional knowledge of all other
records in the dataset, the primary source of IHA’s superior performance stems from its access to model
parameters rather than just the leave-one-out setup. In a way, IHA uses parameter access to obviate the
need for reference models, as it directly aims to measure the influence of the given target record instead of
relying on reference models for score calibration.
Interestingly, in comparison, LiRA achieves an AUC of .767, which is lower than IHA but still higher than
the L-attack. This suggests that LiRA, even without the extensive reference model training, is more effective
than the L-attack, possibly due to its utilization of both “in” and “out” models as opposed to just “out”
models with the L-attack. To try and devise a stronger black-box attack for the LOO setting, we extend
LiRA to the LOO setting by using models trained on LOO data as reference models. LiRA-Offline under
the LOO setting achieves an AUC of .633, lower than the L-attack ( .737). Overall, these results demonstrate
the promise of IHA as a privacy auditing tool. It yields results comparable to that of techniques that train
hundreds of reference models (for each target record in the worst case, as in L-attack), without using a single
reference model.
4.5 Inter-Attack Agreement
Similar to Ye et al. (2022), we compute the agreement rate between ground-truth membership labels and
membership predicted by various attacks to understand the ability of our privacy audit to identify vulnerable
data, and demonstrate how it differs from existing attacks. Table 5 presents the agreement rate between
ground-truth membership labels and membership predicted by various attacks.
At a low FPR of ≈0.05, agreement in predictions for non-members is very high between attacks, with
agreement rates above 0.91for all pairs of attacks. On the other hand, agreement rates for member records
are expectedly lower. Interestingly, agreement between LiRA and LiRA-L is higher than IHA and any other
attack. This difference is especially pronounced for a higher FPR (Table 5b), where agreement rates are as
2Our results for the L-attack are lower than those reported by Ye et al. (2022). For instance, we observe a TPR of .668at0.3
FPR, while it was reported to be .968by (Ye et al., 2022). This discrepancy arises from our setting, which uses more data and
fewer model parameters. We verified our implementation through direct correspondence with the authors and by replicating
their results in the original setting, which used a smaller dataset and more parameters, resulting in a model prone to overfitting.
11Published in Transactions on Machine Learning Research (12/2024)
Attack Time (s) Memory (MB)
Precompute Time/Sample Precompute Runtime max(Precompute, Runtime)
IHA 43×60 0.16 4228 1324 4228
IHA (Approx) 0 85 0 1638 1638
LiRA 192×60 0.07 276 1228 1228
LOSS 0 0.004 0 906 906
Table 6: Runtime and memory statistics for various auditing techniques. Statistics are computed over 100
randomly-selected members for MLP-2 architecture models trained on Purchase-100 dataset.
low as≈0.4compared to 0.766for LiRA and LiRA-L. This is very interesting becaue the corresponding
TPRs for IHA are higher than LiRA and comparable to that of the L-attack, thus suggesting that the records
identified by IHA as being vulnerable are very different from those identified by LiRA or even the L-attack.
This also means that a combined (classifying a record as a member only when both attacks classify as a
member) attack would have some true positives with a very low FPR.
4.5.1 Runtime Comparison
To compare the computational costs of our proposed audit with existing auditing techniques, we analyze
runtime and memory usage statistics across different methods, aiming to evaluate efficiency and practicality
in real-world privacy audits (Table 6).
While the peak memory consumption of IHA is higher than that of LiRA, the approximate version of IHA
is not too far from LiRA in terms of memory consumption. Computing the total runtime is a function of
the number of samples used for the privacy audit, as a “precompute” step is required for LiRA (training
reference models) and exact IHA (computing Hessian). For instance, computing the audit for 1K samples
takes less overall time for IHA ( ≈1 hour) than it does for LiRA ( ≈3 hours). It should be noted that
the Hessian is too large to store on our GPU for IHA and is thus stored on the CPU, which is also why
it is slower. Although improvements may reduce the compute costs, the key advantage of such a privacy
audit comes from not having to reserve hold-out (or auxiliary) data. Our privacy audit, like the most trivial
LOSS attack, only requires member data and some non-member data, whereas other attacks in the literature
require shadow/reference models trained on comparable-sized datasets; the limiting factor here is reserving
data to train reference models, which a real-world model trainer may not want to do since it reduces the
amount of data available for training.
5 Conclusion
Our theoretical result proves that model parameter access is indeed necessary for optimal membership
inference, contrary to previous results derived under unrealistic assumptions and the common belief that
optimal membership inference can be achieved with only black-box model access. We propose the Inverse
Hessian Attack inspired by this theory that provides stronger privacy auditing than existing black-box
techniques.
Limitations. IHA is not yet practically realizable for most settings due to the computational expense of
calculating the Hessian, or even approximating iHVPs. This restriction poses challenges for real-world cases
where fixed compute budgets may be more crucial than the availability of auxiliary data. An auditor might
useasubsetofparameterstoreducecomputationalcostswhileperformingHessian-basedcomputations. This
aligns with model pruning (Liu et al., 2019), but understanding its impact on membership knowledge within
parameters is non-trivial (Yuan & Zhang, 2022). We also note that IHA’s performance can be sensitive to
the choice of the damping factor, which requires further investigation (Appendix D.1).
Our conclusion aligns well with recent calls in the literature to consider white-box access for rigorous auditing
(Casper et al., 2024). While our theory shows that parameter access is required for optimal membership
inference, it remains unclear how much better this is compared to the optimal membership inference attack
12Published in Transactions on Machine Learning Research (12/2024)
restricted to black-box access. Our empirical studies suggest the gap is non-trivial, but further study is
required to understand the theoretical limit of black-box attacks, which is a non-trivial but interesting
direction to explore. Exploring the accuracy of iHVP approximation methods to extend IHA to larger
models, along with multi-record inference, are both promising directions for future research.
Broader Impact Statement
The increasing integration of AI in sensitive domains like healthcare, finance, and personal data manage-
ment highlights the critical importance of privacy. Information leakage from AI models can have severe
consequences, making effective privacy auditing a necessary safeguard. Our work contributes to this field by
theoretically demonstrating that optimal membership inference attacks require white-box access to model
parameters, challenging the adequacy of black-box approaches. We also demonstrate with Inverse Hessian
Attack how this theory can be used to design empirical privacy audits that do not rely on reference models.
We advocate for the development of more sophisticated privacy auditing tools that fully leverage the elevated
access typically available to auditors, such as model parameters, to assess privacy leakage efficiently without
extensivedataandcomputeresources. Wehopeourtheoreticalandempiricalresultswillreinvigorateinterest
in the privacy research community to explore white-box attacks, for both adversarial and auditing purposes.
References
Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization for machine learning
in linear time. Journal of Machine Learning Research , 2017.
Achraf Azize and Debabrota Basu. How much does each datapoint leak your privacy? quantifying the
per-datum membership leakage. arXiv:2402.10065 , 2024.
Martin Bertran, Shuai Tang, Aaron Roth, Michael Kearns, Jamie H Morgenstern, and Steven Z Wu. Scalable
membership inference attacks via quantile regression. Advances in Neural Information Processing Systems ,
2024.
Stella Biderman, USVSN Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu
Purohit, and Edward Raff. Emergent and predictable memorization in large language models. Advances
in Neural Information Processing Systems , 2024.
Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership
inference attacks from first principles. In IEEE Symposium on Security and Privacy , 2022.
Stephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor Lynn Curtis, Benjamin Bucknall,
Andreas Haupt, Kevin Wei, Jérémy Scheurer, Marius Hobbhahn, et al. Black-box access is insufficient for
rigorous ai audits. In ACM Conference on Fairness, Accountability, and Transparency , 2024.
Harsh Chaudhari, Giorgio Severi, Alina Oprea, and Jonathan Ullman. Chameleon: Increasing label-only
membership leakage with adaptive poisoning. In International Conference on Learning Representations ,
2024.
Gilad Cohen and Raja Giryes. Membership inference attack using self influence functions. In Proceedings of
the IEEE/CVF Winter Conference on Applications of Computer Vision , 2024.
Rachel Cummings, Damien Desfontaines, David Evans, Roxana Geambasu, Yangsibo Huang, Matthew
Jagielski, PeterKairouz, GautamKamath, SewoongOh, OlgaOhrimenko, NicolasPapernot, RyanRogers,
Milan Shen, Shuang Song, Weijie Su, Andreas Terzis, Abhradeep Thakurta, Sergei Vassilvitskii, Yu-Xiang
Wang, Li Xiong, Sergey Yekhanin, Da Yu, Huanyu Zhang, and Wanrong Zhang. Advancing differential
privacy: Where we are now and future directions for real-world deployment. Harvard Data Science Review ,
2024.
Daniel DeAlcala, Aythami Morales, Gonzalo Mancera, Julian Fierrez, Ruben Tolosana, and Javier Ortega-
Garcia. Is my Data in your AI Model? Membership Inference Test with Application to Face Images.
arXiv:2402.09225 , 2024.
13Published in Transactions on Machine Learning Research (12/2024)
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private
data analysis. In Theory of Cryptography Conference . Springer, 2006.
Mishaal Kazmi, Hadrien Lautraite, Alireza Akbari, Mauricio Soroco, Qiaoyue Tang, Tao Wang, Sébastien
Gambs, and Mathias Lécuyer. PANORAMIA: Privacy Auditing of Machine Learning Models without
Retraining. In Advances in Neural Information Processing Systems , 2024.
PangWeiKohandPercyLiang. Understandingblack-boxpredictionsviainfluencefunctions. In International
Conference on Machine Learning , 2017.
Sasi Kumar and Reza Shokri. Ml privacy meter: Aiding regulatory compliance by quantifying the privacy
risks of machine learning. In Workshop on Hot Topics in Privacy Enhancing Technologies (HotPETs) ,
2020.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE , 86(11), 1998.
Marvin Li, Jason Wang, Jeffrey George Wang, and Seth Neel. Mope: Model perturbation based privacy
attacks on language models. In Conference on Empirical Methods in Natural Language Processing , 2023.
Kangqiao Liu, Liu Ziyin, and Masahito Ueda. Noise and fluctuation of finite learning rate stochastic gradient
descent. In International Conference on Machine Learning , 2021.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network
pruning. In International Conference on Learning Representations , 2019.
Takashi Mori, Liu Ziyin, Kangqiao Liu, and Masahito Ueda. Power-law escape rate of sgd. In International
Conference on Machine Learning , 2022.
Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning. In IEEE
Symposium on Security and Privacy , 2018.
Elre Talea Oldewage, Ross M Clarke, and José Miguel Hernández-Lobato. Series of hessian-vector products
for tractable saddle-free newton optimisation of neural networks. Transactions on Machine Learning
Research , 2024.
Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and Hervé Jégou. White-box vs
black-box: Bayes optimal strategies for membership inference. In International Conference on Machine
Learning , 2019.
Ahmed Salem, Giovanni Cherubin, David Evans, Boris Köpf, Andrew Paverd, Anshuman Suri, Shruti Tople,
and Santiago Zanella-Béguelin. SoK: Let the privacy games begin! A unified treatment of data inference
privacy in machine learning. In IEEE Symposium on Security and Privacy , 2023.
Issei Sato and Hiroshi Nakagawa. Approximation analysis of stochastic gradient langevin dynamics by using
fokker-planck equation and ito process. In International Conference on Machine Learning , 2014.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against
machine learning models. In IEEE Symposium on Security and Privacy , 2017.
Mandt Stephan, Matthew D Hoffman, David M Blei, and others. Stochastic gradient descent as approximate
bayesian inference. Journal of Machine Learning Research , 2017.
Jasper Tan, Blake Mason, Hamid Javadi, and Richard Baraniuk. Parameters or privacy: A provable tradeoff
between overparameterization and membership inference. Advances in Neural Information Processing
Systems, 2022.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In International
Conference on Machine Learning , 2011.
14Published in Transactions on Machine Learning Research (12/2024)
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking
machine learning algorithms, August 2017. arXiv: cs.LG/1708.07747.
Zeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for deep learning dynamics: Stochastic gra-
dient descent exponentially favors flat minima. In International Conference on Learning Representations ,
2021.
Jiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda, Vincent Bindschaedler, and Reza Shokri. Enhanced
membership inference attacks against machine learning models. In ACM Conference on Computer and
Communications Security , 2022.
Jiayuan Ye, Anastasia Borovykh, Soufiane Hayou, and Reza Shokri. Leave-one-out distinguishability in
machine learning. In International Conference on Learning Representations , 2024.
Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Ana-
lyzing the connection to overfitting. In Computer Security Foundations Symposium , 2018.
Samuel Yeom, Irene Giacomelli, Alan Menaged, Matt Fredrikson, and Somesh Jha. Overfitting, robustness,
and malicious algorithms: A study of potential causes of privacy risk in machine learning. Journal of
Computer Security , 2020.
Soma Yokoi and Issei Sato. Bayesian interpretation of SGD as Ito process. arXiv:1911.09011 , 2019.
Xiaoyong Yuan and Lan Zhang. Membership inference attacks and defenses in neural network pruning. In
USENIX Security Symposium , 2022.
Sajjad Zarifzadeh, Philippe Liu, and Reza Shokri. Low-cost high-power membership inference by boosting
relativity. arXiv:2312.03262 , 2023.
Liu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda. Strength of minibatch noise in SGD. In
International Conference on Learning Representations , 2021.
LiuZiyin, HongchaoLi, andMasahitoUeda. Lawofbalanceandstationarydistributionofstochasticgradient
descent. arXiv:2308.06671 , 2023.
A Related Work
This section reviews methods in membership inference (black-box and white-box), techniques for privacy
auditing to predict and mitigate data leakage, and the dynamics of stochastic gradient descent (SGD) along
with inverse Hessian vector products (iHVPs).
A.1 Membership Inference
Black-box Membership Inference. Early works on membership inference worked under black-box access,
utilizing the model’s loss (Shokri et al., 2017) on a given datapoint as a signal for membership. Since then
therehavebeenseveralworksfocusingondifferentformsofdifficultycalibration—accountingfortheinherent
“difficulty” of predicting on a record, irrespective of its presence in train data. This calibration has taken
several forms; direct score normalization with reference models (Sablayrolles et al., 2019), likelihood tests
based on score distributions (Carlini et al., 2022; Zarifzadeh et al., 2023; Ye et al., 2022), and additional
models for predicting difficulty (Bertran et al., 2024).
White-box Membership Inference. Nasr et al. (2018) explored white-box access to devise a meta-
classifier-based attack that additionally extracts intermediate model activations and gradients to increase
leakage but concluded that layers closer to the model’s output are more informative for membership inference
and report performance not significantly better than a black-box loss-based attack. Recent work by DeAlcala
et al. (2024), however, makes the opposite observation, with layers closer to the model’s input providing
15Published in Transactions on Machine Learning Research (12/2024)
noticeably better performance. Apart from these meta-classifier driven approaches, some works attempt
to utilize parameter access much more directly, often utilizing Hessian in one form or another. Cohen &
Giryes (2024) defined the self-influence of a datapoint zias (gi⊤H−1gi) as a signal for membership, using
LiSSA (Agarwal et al., 2017) to approximate the iHVP. This has similarities to our result since our optimal
membership inference score also involves computing iHVPs. Li et al. (2023) attempted to measure the
sharpness for a given model by evaluating fluctuations in model predictions after adding zero-mean noise to
the parameters, a step that is supposed to approximate the trace of the Hessian at the given point.
A.2 Privacy Auditing
Ye et al. (2024) proposed using efficient methods to “predict” memorization by not having to run computa-
tionally expensive membership inference attacks, with reported speedups of up to 140 x. They showed how
their proposed score (LOOD) correlates well with AUC, corresponding to an extremely strong MIA with
all-but-one access to records (L-attack (Ye et al., 2022)). However, it is unclear if this computed LOOD is
directly comparable across models, making it hard to calibrate these scores to compare the leakage from a
model relative to another (an important aspect of internal privacy auditing). Their derivations also involve
a connection with the Hessian. Biderman et al. (2024) studied the problem of forecasting memorization in
a model for specific training data and proposed using partially trained versions of the model (or smaller
models) as a proxy for their computation. While their results support the need for inexpensive auditing
methods, their focus is on predicting memorization early in the training process, while ours relates to audit-
ing fully trained models. More recently, Tan et al. (2022) studied the theory behind worst-case membership
leakage for the case of linear regression on Gaussian data and derived insights. While this is useful to make
an intuitive connection with overfitting, it does not provide a realizable attack or insights for the standard
case of models trained with SGD.
A.3 SGD Dynamics and iHVPs
SGD Dynamics. Stephan et al. (2017) approximated the SGD dynamics as an Ornstein-Uhlenbeck process,
while Yokoi & Sato (2019) provided a discrete-time weak-order approximation for SGD based on Itô process
and finite moment assumption. However, both works rely on strong assumptions about the gradient noises
and require a vanishingly small learning rate, largely deviating from the common practice of SGD. To address
the limitations of the aforementioned works, Liu et al. (2021) directly analyzed the discrete-time dynamics
of SGD and derived the analytic form of the asymptotic model fluctuation with respect to the asymptotic
gradient noise covariance and the Hessian matrix. Ziyin et al. (2021) further generalized the results of (Liu
et al., 2021) by deriving the exact minibatch noise covariance for discrete-time SGD, which is shown to vary
across different kinds of local minima. Our work builds on these advanced theoretical results of discrete-time
SGD dynamics but aims to enhance the understanding of optimal membership inference, particularly for
models trained with SGD.
iHVPs. Currentlyliteratureon approximatinginverse-Hessianvector productsrelies ononeoftwomethods:
conjugate gradients (Koh & Liang, 2017) or LiSSA (Agarwal et al., 2017). Both approximation methods rely
on efficient computation of exact Hessian-vector products, and use forward and backward propagation as
sub-routines. While these methods have utility in certain areas, such as influence functions (Koh & Liang,
2017) and optimization (Oldewage et al., 2024), approximation errors can be non-trivial. For instance,
I1in the formulation of our attack requires a low approximation error in the norm of an iHVP, while I2
simultaneously requires a low approximation error in the direction of the iHVP. Recent work on curvature-
aware minimization by Oldewage et al. (2024) proposes another method for efficient iHVP approximation as
a subroutine, but the authors observed high approximation errors based on both norm and direction.
16Published in Transactions on Machine Learning Research (12/2024)
B Proof for Theorem 3.1
Proof.Recall that H∗=H(w∗)andL∗=L(w∗). According to Theorem 2.2 and Theorem 2.3, we obtain
Σ=λ
S(1−µ)/parenleftbigg
2L∗H∗−α2w∗w∗⊤/parenrightbigg/parenleftbigg
H∗+αId/parenrightbigg−1/parenleftbigg
2Id−λ
1 +µ(H∗+αId)/parenrightbigg−1
, (9)
where we only consider the leading terms in Theorems 2.2 and 2.3. Note that Equation 9 holds when
the Hessian matrix H∗has full rank and L∗̸= 0. When the Hessian has degenerated rank such that
rank(H∗+αId) =r<d, the following more generalized result can be derived:
PrΣ=λ
S(1−µ)/parenleftbigg
2L∗H∗−α2w∗w∗⊤/parenrightbigg/parenleftbigg
H∗+αId/parenrightbigg+/parenleftbigg
2Id−λ
1 +µ/parenleftbig
H∗+αId/parenrightbig/parenrightbigg−1
,
where Pr= diag(1,...,1,0,...0)denotes the projection matrix onto non-zero eigenvalues, and +is the Moore-
Penrose inverse operator. If L∗= 0, meaningw∗is a global minimum, then the asymptotic model fluctuation
Σ=0. For ease of presentation, we assume the Hessian matrix has full rank in the following proof.
Then, we get:
Σ−1=S(1−µ)
λ/parenleftbigg
2Id−λ
1 +µ(H∗+αId)/parenrightbigg/parenleftbigg
H∗+αId/parenrightbigg/parenleftbigg
2L∗H∗−α2w∗w∗⊤/parenrightbigg−1
. (10)
Using the Sherman–Morrison formula, we obtain:
/parenleftbigg
2L∗H∗−α2w∗w∗⊤/parenrightbigg−1
=1
2L∗H−1
∗+α2
2L∗(2L∗−w∗⊤H−1
∗w∗)/parenleftbig
H−1
∗w∗w∗⊤H−1
∗/parenrightbig
=1
2L∗H−1
∗/parenleftbig
Id+O(α2)/parenrightbig
. (11)
Leaving out the second-order term O(α2)in Equation 11 (since the regularization parameter αis a typically
small constant in [0,1)) and plugging it back in Equation 10, we get:
Σ−1=S(1−µ)
2λL∗/parenleftbigg
2Id−λ
1 +µ(H∗+αId)/parenrightbigg/parenleftbigg
H∗+αId/parenrightbigg
H−1
∗
=S(1−µ)
2λL∗/parenleftbigg
2Id−λ
1 +µ(H∗+αId) + 2αH−1
∗−λα
1 +µ(Id+αH−1
∗)/parenrightbigg
=S(1−µ)
2λL∗/parenleftbigg
2/parenleftbigg
1−λα
1 +µ/parenrightbigg
Id−/parenleftbiggλ
1 +µ/parenrightbigg
H∗+α/parenleftbigg
2−λα
1 +µ/parenrightbigg
H−1
∗/parenrightbigg
. (12)
According to Laplace approximation, we can approximate the posterior distribution of wgivenw∗as
N(w∗,Σ). Therefore, making use of Equation 9, we can derive the explicit formula of the log-posterior
distribution as:
lnp(w|w∗) =−d
2ln(2π)−1
2ln det( Σ)−1
2(w−w∗)⊤Σ−1(w−w∗)
=1
2d/summationdisplay
i=1ln/parenleftbigg/parenleftbig
2−λ
1+µ(σi(H∗) +α)/parenrightbig/parenleftbig
σi(H∗) +α/parenrightbig
2L∗σi(H∗)/parenrightbigg
+const.
−S(1−µ)
4λL∗/bracketleftbigg
2/parenleftbigg
1−λα
1 +µ/parenrightbigg
∥w−w∗∥2
2+α/parenleftbigg
2−λα
1 +µ/parenrightbigg
(w−w∗)⊤H−1
∗(w−w∗)
−λ
1 +µ(w−w∗)⊤H∗(w−w∗)/bracketrightbigg
. (13)
Note that we assume the model parameterized by wis converging towards some local minima w∗using
stochastic gradient descent, where the loss landscape around the local minima w∗has a quadratic structure.
17Published in Transactions on Machine Learning Research (12/2024)
Therefore, we can perform second-order Taylor expansion of the loss function at w∗as follow:
L(w) =L(w∗) +1
2(w−w∗)⊤H∗(w−w∗) +o(∥w−w∗∥2
2), (14)
which further suggests that ∇L(w) =H∗(w−w∗)if taking gradient with respect to wfor both sides of
Equation 14. Plugging Equation 14 into Equation 13, we further obtain:
lnp(w|w∗) =1
2d/summationdisplay
i=1ln/parenleftbigg/parenleftbig
2−λ
1+µ(σi(H∗) +α)/parenrightbig/parenleftbig
σi(H∗) +α/parenrightbig
2L∗σi(H∗)/parenrightbigg
+const.
−S(1−µ)
2λL∗/parenleftbigg
1−λα
1 +µ/parenrightbigg
∥w−w∗∥2
2
−S(1−µ)α
4λL∗/parenleftbigg
2−λα
1 +µ/parenrightbigg
(w−w∗)⊤H−1
∗(w−w∗)
+S(1−µ)
4(1 +µ)L∗(w−w∗)⊤H∗(w−w∗)
=−d
2lnL∗+1
2d/summationdisplay
i=1ln/parenleftbigg/parenleftbig
2−λ
1+µ(σi(H∗) +α)/parenrightbig/parenleftbig
σi(H∗) +α/parenrightbig
σi(H∗)/parenrightbigg
+const.
−S(1−µ)
2λ/parenleftbigg
1−λα
1 +µ/parenrightbigg
·∥w−w∗∥2
2
L∗
−S(1−µ)α
4λ·/parenleftbigg
2−λα
1 +µ/parenrightbigg
·∇L(w)⊤H−3
∗∇L(w)
L∗
+S(1−µ)
2(1 +µ)·L(w∗)
L∗+o(∥w−w∗∥2
2). (15)
Omitting the constant and negligible terms, we thus complete the proof of Theorem 3.1. Note that we keep
theO(α2)term in Equation 13 and Theorem 3.1 for the sake of completeness but expect it to be negligible
compared with other terms, due to the fact that αis typically set as a very small constant within [0,1).
C Optimal Membership-Inference Score
C.1 Proof for Theorem 3.2
Proof.To derive the scoring function for an optimal membership inference, we need to compute the ratio
betweenp(w|w∗
1)andp(w|w∗
0), wherew∗
0(resp.w∗
1) denotes a local minimum (close to w) of the training
loss function with respect to {z2,...,zn}(resp.{z1,...,zn}). Note that we’ve obtained the posterior
distribution of win Theorem 3.1. Therefore, the remaining task is to analyze the following terms:
lnp(w|w∗
1)−lnp(w|w∗
0)
=−d
2/bracketleftbig
lnL1(w∗
1)−lnL0(w∗
0)/bracketrightbig
+1
2d/summationdisplay
i=1ln/parenleftbigg/parenleftbig
2−λ
1+µ(σi(H1(w∗
1)) +α)/parenrightbig/parenleftbig
σi(H1(w∗
1)) +α/parenrightbig
/parenleftbig
2−λ
1+µ(σi(H0(w∗
0)) +α)/parenrightbig/parenleftbig
σi(H0(w∗
0)) +α/parenrightbig·σi(H0(w∗
0))
σi(H1(w∗
1))/parenrightbigg
−S(1−µ)
2λ/parenleftbigg
1−λα
1 +µ/parenrightbigg/parenleftbigg∥w−w∗
1∥2
2
L1(w∗
1)−∥w−w∗
0∥2
2
L0(w∗
0)/parenrightbigg
+S(1−µ)
2(1 +µ)/parenleftbiggL1(w)
L1(w∗
1)−L0(w)
L0(w∗
0)/parenrightbigg
−S(1−µ)α
4λ/parenleftbigg
2−λα
1 +µ/parenrightbigg/parenleftbigg∇L1(w)⊤H1(w∗
1)−3∇L1(w)
L1(w∗
1)−∇L0(w)⊤H0(w∗
0)−3∇L0(w)
L0(w∗
0)/parenrightbigg
,(16)
where H0(w∗
0)(resp. H1(w∗
1)) denotes the Hessian of L0(resp.L1) atw∗
0(resp.w∗
1). Since both w∗
0and
w∗
1are close to parameters of the observed victim model w, so we can approximate the corresponding loss
using second-order Taylor expansion. Also, according to Assumption 3, we know H0(w∗
0) =H1(w∗
1) =H∗
18Published in Transactions on Machine Learning Research (12/2024)
andL0(w∗
0) =L1(w∗
1) =L∗. Thus, we can simplify Equation 16 and obtain the following form:
lnp(w|w∗
1)−lnp(w|w∗
0)
=−S(1−µ)
2λL∗/parenleftbigg
1−λα
1 +µ/parenrightbigg/parenleftbig
∥w−w∗
1∥2
2−∥w−w∗
0∥2
2/parenrightbig
+S(1−µ)
2(1 +µ)L∗/parenleftbig
L1(w)−L0(w)/parenrightbig
−S(1−µ)α
4λL∗/parenleftbigg
2−λα
1 +µ/parenrightbigg/parenleftbig
∇L1(w)⊤H−3
∗∇L1(w)−∇L0(w)⊤H−3
∗∇L0(w)/parenrightbig
=−S(1−µ)
2λL∗/parenleftbigg
1−λα
1 +µ/parenrightbigg/parenleftbig
∇L1(w)⊤H−1
∗H−1
∗∇L1(w)−∇L0(w)⊤H−1
∗H−1
∗∇L0(w)/parenrightbig
+S(1−µ)ℓ(w,z1)
2n(1 +µ)L∗
−S(1−µ)α
4nλL∗/parenleftbigg
2−λα
1 +µ/parenrightbigg/parenleftbigg
2∇L0(w)⊤H−3
∗∇ℓ(w,z1) +1
nℓ(w,z1)⊤H−3
∗∇ℓ(w,z1)/parenrightbigg
=−S(1−µ)
2nλL∗/parenleftbigg
1−λα
1 +µ/parenrightbigg/parenleftbigg
2∇L0(w)⊤H−1
∗H−1
∗∇ℓ(w,z1) +1
n∥H−1
∗∇ℓ(w,z1)∥2
2/parenrightbigg
+S(1−µ)ℓ(w,z1)
2n(1 +µ)L∗
−S(1−µ)α
4nλL∗/parenleftbigg
2−λα
1 +µ/parenrightbigg/parenleftbigg
2∇L0(w)⊤H−3
∗∇ℓ(w,z1) +1
nℓ(w,z1)⊤H−3
∗∇ℓ(w,z1)/parenrightbigg
, (17)
where the second equality holds because of the Taylor approximation ∇Li(w)−∇Li(w∗
i) =H∗(w−w∗
i)
fori∈{0,1}. Moreover, according to Lemma 2.1, we know the optimal membership inference is given by:
M(w,z1) =ET/bracketleftbigg
σ/parenleftbigg
ln/parenleftbiggp(w|m1= 1,z1,T)
p(w|m1= 0,z1,T)/parenrightbigg
+ ln/parenleftbiggγ
1−γ/parenrightbigg/parenrightbigg/bracketrightbigg
, (18)
whereσ(u) = (1 + exp(−u))−1is the Sigmoid function, T={z2,...,zn,m2,...,mn}, andγ=P(mi= 1).
Plugging Equation 17 into Equation 18, we obtain
M(w,z1) =ET/bracketleftbigg
σ/parenleftbiggS(1−µ)
2nL∗/parenleftbiggℓ(w,z1)
(1 +µ)−1
λ/parenleftbig
I1+I2+I3+I4/parenrightbig/parenrightbigg
+tγ/parenrightbigg/bracketrightbigg
,
whereI1,I2,I3,I4andtγare defined as:
I1:=1
n/parenleftbigg
1−λα
1 +µ/parenrightbigg
·∥H−1
∗∇ℓ(w,z1)∥2
2,
I2:= 2/parenleftbigg
1−λα
1 +µ/parenrightbigg
·/parenleftbig
H−1
∗∇L0(w)/parenrightbig⊤/parenleftbig
H−1
∗∇ℓ(w,z1)/parenrightbig
,
I3:=α
2n/parenleftbigg
2−λα
1 +µ/parenrightbigg
·/parenleftbig
H−1
∗∇ℓ(w,z1)/parenrightbig⊤/parenleftbig
H−1
∗/parenleftbig
H−1
∗∇ℓ(w,z1)/parenrightbig/parenrightbig
,
I4:=α/parenleftbigg
2−λα
1 +µ/parenrightbigg
·/parenleftbig
H−1
∗∇L0(w)/parenrightbig⊤/parenleftbig
H−1
∗/parenleftbig
H−1
∗∇ℓ(w,z1)/parenrightbig/parenrightbig
,
tγ:= ln/parenleftbiggγ
1−γ/parenrightbigg
.
Therefore, we complete the proof of Theorem 3.2.
C.2 Connection with LOSS attack
Note that while there are additional terms in our optimal membership-inference score, there is another
critical difference: the loss function has its sign flipped when compared to existing results (Yeom et al., 2018;
Sablayrolles et al., 2019). While this may seem counter-intuitive at first glance, we show below the addition
−(I1+I2+I3+I4)terms in Equation 7 are expected to be negatively correlated to the loss function, leading
to the proposed scoring function, in fact, aligns with the intuition of existing results. For simplicity, we
consider the setting without regularization (i.e., α= 0) and the Hessian matrix has full rank.
According to the assumption of quadratic loss around w∗, we have the following observations:
L(w)−L∗=1
2(w−w∗)⊤H∗(w−w∗) =1
2(w−w∗)⊤U⊤diag{σ1,...,σd}U(w−w∗),
19Published in Transactions on Machine Learning Research (12/2024)
where U⊤diag{σ1,...,σd}Uis the eigenvalue decomposition of H. Letv=U(w−w∗). Since Uis an
orthonormal matrix, we know ∥v∥2=∥w−w∗∥2. Thus, we obtain
σd·∥w−w∗∥2
2≤σj·∥v∥2
2= 2(L(w)−L∗) =d/summationdisplay
j=1σj·v2
j≤σ1·∥v∥2
2=σ1·∥w−w∗∥2
2,
which further suggests (provided the Hessian has full rank)
1
σ1≤∥w−w∗
i∥2
2
2(Li(w)−L∗)≤1
σdfor anyi∈{0,1}. (19)
Based on Assumption 3, we assume that the Hessian structure and the loss function value remain unchanged
with and without a single record z1. We hypothesize that the ratio1
k=∥w−w∗
i∥2
2
2(Li(w)−L∗)also remains similar for
i= 0andi= 1, wherek∈[σd,σ1]. Therefore, we have
2ℓ(w,z1)
nσ1≤∥w−w∗
1∥2
2−∥w−w∗
0∥2
2≤2ℓ(w,z1)
nσd. (20)
Note that the derivation from Equation 19 to Equation 20 is not mathematically rigorous, but as long as
the recordz1is not too deviated from the data distribution, we expect the above inequality holds. Plugging
Equation 20 into the log-likelihood term inside M(w,z1)(first equality in Equation 17 with α= 0), we get
lnp(w|w∗
1)−lnp(w|w∗
0) =S(1−µ)
2L∗/parenleftbigg
−1
λ(∥w−w∗
1∥2
2−∥w−w∗
0∥2
2) +ℓ(w,z1)
(1 +µ)n/parenrightbigg
=S(1−µ)
2L∗/parenleftbigg1
1 +µ−2
λk/parenrightbiggℓ(w,z1)
n, (21)
wherekis some real number that falls into [σd,σ1]. In addition, for the case of full-rank Hessian, it is easy
to see that the i-th eigenvalue of Σ(Equation (4)) can be written as:
S(1−µ)
2λL∗/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
positive/parenleftbigg
2−λσi
1 +µ/parenrightbigg−1
.
Since the covariance matrix Σis positive semi-definite and invertible, it must follow that all of its eigenvalues
are positive:
2−λσi
1 +µ>0⇒1
1 +µ−2
λσi<0,for anyi= 1,2,...,d.
With the above inequalities in mind, by looking at Equation (21), we get:
lnp(w|w∗
1)−lnp(w|w∗
0) =S(1−µ)
2nL∗/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
>0/parenleftbigg1
1 +µ−2
λk/parenrightbigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
<0ℓ(w,z1)
n. (22)
The upper limit on the score (hence the score itself) corresponding to IHA is thus proportional to the
negative of the loss function, aligning with intuition (lower loss indicative of overfitting, and thus the record
being a member). The score inside IHA can thus be interpreted (up to some approximation error) as
−f(w,z1)ℓ(w,z1)for somef(w,z1)>0that essentially accounts for SGD training dynamics, and is a
function dependent on parameter access to the target model.
D Purchase-100(S) v/s Purchase-100
Model trainers, under practical settings, would not want to produce sub-optimal models. Under the given
experimental settings (access to Purchase100 dataset), it is thus crucial to simulate model training setups
20Published in Transactions on Machine Learning Research (12/2024)
Table 7: Performance of various attacks on Purchase-100(S) and Purchase-100. There is a clear drop in
performance when shifting from Purchase-100(S) to Purchase-100. Statistics for IHA (CG) are computed on
10000 samples, not the entire dataset.
AttackPurchase-100Purchase-100(S)
MLP-2 MLP-4
AUC% TPR@FPR
AUC% TPR@FPR
AUC% TPR@FPR
1% 0.1% 1% 0.1% 1% 0.1%
LOSS .529 ±.0010.97 0.00 .589 ±.0031.04 0.00 .666 ±.0050.00 0.00
LiRA .634 ±.0034.70 0.98 .743 ±.0069.56 2.79 .843 ±.00425.17 9.09
IHA (Ours) .703 ±.00413.69 7.52 .791 ±.00319.95 18.99 — — —
IHA (CG) .701 ±.00913.72 7.55 .791 ±.00520.09 19.09 .691 ±.0071.14 0.16
that would maximize performance. The switch from Purchase-100(S) to Purchase-100 not only improves
model performance but also reduces the performance of MIA attacks (Table 7).
For instance, AUC values for LOSS drop from ∼0.59to∼0.53, and LiRA-Online from ∼0.74to∼0.63.
While the smaller version of the dataset has recently been argued not to be very relevant Carlini et al. (2022),
we believe this larger version is still interesting to study since such large datasets are practically relevant.
We hope that researchers will aim to use the larger version of the dataset and, in general, train target models
to maximize performance (as any model trainer would) within the constraints of their experimental design.
D.1 IHA performance on MLP-4
For the MLP-4 architecture on the Purchase-100(S) dataset, we observe a significant drop in performance
when using IHA compared to LiRA. We hypothesize that this performance degradation may be due to a
mismatch between the damping factor ϵused in our experiments ( 2e−1) and the optimal damping factor for
the eigenvalue distribution of this larger model. To test this hypothesis, we increased ϵto5e−1and repeated
the evaluation with IHA.
This adjustment led to a substantial improvement in performance: the AUC increased to 0.768, with TPRs
of 13.11% and 12.12% at 1% and 0.1% FPR, respectively. This result supports our hypothesis and highlights
a current limitation of IHA. Specifically, the damping factor ϵshould be scaled according to the model
architecture—ideally determined by some percentile of the eigenvalues. However, identifying the optimal
scaling method before conducting the audit remains an open question for future work.
It is important to note that further increasing ϵdoes not result in a linear performance improvement; in
fact, performance declines across architectures when ϵbecomes too large, which is expected. The remaining
performance gap is likely due to the absence of reference models or a violation of our assumptions about
Hessian behavior. However, the superior TPR at very low (0.1%) FPRs, compared to LiRA, suggests that
the former is more likely the cause.
E Implementing the Inverse Hessian Attack
For some given record z1,∇L0(w)can be computed by considering all data (except the target record) for
which membership is known. To make this step computationally efficient for an audit, we pre-compute
∇L1(w). Then, if the test record is indeed a member, we can compute ∇L0(w)as∇L1(w)−∇ℓ(w,z1)
n.
Note that this is equivalent to computing ∇L0(w)separately for each target record. The Hessian H∗is also
similarly pre-computed using the model’s training data.
Conditioning H∗.While computing Hessian matrices for our experiments, we notice the presence of near-
zero and small, negative eigenvalues (most of which are likely to arise from precision errors). Such eigenvalues
21Published in Transactions on Machine Learning Research (12/2024)
make the Hessian ill-conditioned and thus cannot be inverted directly. We explore two different techniques
to mitigate this: damping by adding a small constant ϵto all the eigenvalues or a low-rank approximation
where only eigenvalues (and corresponding eigenvectors) above a certain threshold ϵare used as a low-rank
approximation. We ablate over these two techniques for some candidate values of ϵ. Our results (Table 8)
suggest that damping with ϵ= 2e−1works best across all the datasets we test, which is the setting for which
we report our main results.
Table 8: Attack AUCs for various techniques to mitigate ill-conditioned Hessian matrix, with corresponding
ϵvalues.
DatasetLow-Rank Damping
ϵ= 1e−2ϵ= 1e−1ϵ= 2e−1ϵ= 1e−2ϵ= 1e−1ϵ= 2e−1
MNIST-Odd .521 .530 .500 .513 .535 .542
FashionMNIST .551 .557 .541 .533 .582 .594
F Approximating L0
Forauditingpurposes, experimentswhereallbutonememberisknownareuseful, butanadversaryisunlikely
to have this much knowledge of the training data. We experiment with the potential use of IHA where only
partial knowledge of the remaining n−1members may be available to approximate ∇L0, Approximating
L0with a fraction of the actual dataset could be useful in not only reducing the computational cost of the
audit, but also potentially enabling adversarial use of the attack in threat models where the attacker has
partial knowledge of the training data. We evaluate IHA for versions where L0is approximated using a
randomly-sampled fraction of the training data and report results in Table 9.
Table 9: Performance of approximation-based variant of IHA on Purchase100-S (MLP-2), when a fraction
of data from D\{z1}is used to approximate L0. Statistics are computed on 10000 samples.
Fraction AUC%TPR@FPR
1% 0.1%
0.2 .577 ±.0050.85 0.11
0.4 .607 ±.0071.79 0.28
0.6 .638 ±.0103.38 0.86
0.8 .692 ±.0126.47 1.59
0.9 .733 ±.01013.53 4.17
1.0 .791 ±.00520.09 19.09
We see a clear degradation in performance when only a subset of data is used–this is especially true for lower
fractions, where AUC can drop by as much as ≈0.2. More importantly, even when using 90% of the training
data, there is a significant gap in performance. The statistics we compute for IHA thus do completely utilize
knowledge of all other training records. While this result suggests that adversarial use of IHA would require
a very strong adversary (that posseses knowledge of nearly all training records), it also hints at how data
poisoning attacks could have a large impact on downstream membership inference. A poisoning adversary
could hypothetically craft data a way that interferes with L0(when relating to the optimal membership
adversary) and thus increase/decrease inference risk for other records.
22Published in Transactions on Machine Learning Research (12/2024)
G Additional Results
10−510−410−310−210−1100
FPR10−510−410−310−210−1100TPR
IHA (Ours)
LiRA
LOSS
SIF
(a) Purchase-100
10−510−410−310−210−1100
FPR10−510−410−310−210−1100TPR
IHA (Ours)
LiRA
LOSS
SIF (b) Purchase-100(S)
10−510−410−310−210−1100
FPR10−510−410−310−210−1100TPR
IHA (Ours)
LiRA
LOSS
SIF
(c) MNIST-Odd
10−510−410−310−210−1100
FPR10−510−410−310−210−1100TPR
IHA (Ours)
LiRA
LOSS
SIF (d) FMNIST
Figure 1: ROC curves for low-FPR region for various attacks and datasets.
23