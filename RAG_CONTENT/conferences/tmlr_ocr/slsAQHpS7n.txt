Published in Transactions on Machine Learning Research (04/2023)
Computationally-eﬃcient initialisation of GPs:
The generalised variogram method
Felipe Tobar ftobar@uchile.cl
Initiative for Data & AI
Universidad de Chile
Elsa Cazelles elsa.cazelles@irit.fr
CNRS, IRIT
Université de Toulouse
Taco de Wolﬀ tacodewolﬀ@gmail.com
Inria Chile
Reviewed on OpenReview: https: // openreview. net/ forum? id= slsAQHpS7n
Abstract
We present a computationally-eﬃcient strategy to initialise the hyperparameters of a Gaus-
sian process (GP) avoiding the computation of the likelihood function. Our strategy can be
used as a pretraining stage to ﬁnd initial conditions for maximum-likelihood (ML) training,
or as a standalone method to compute hyperparameters values to be plugged in directly
into the GP model. Motivated by the fact that training a GP via ML is equivalent (on aver-
age) to minimising the KL-divergence between the true and learnt model, we set to explore
diﬀerent metrics/divergences among GPs that are computationally inexpensive and provide
hyperparameter values that are close to those found via ML. In practice, we identify the GP
hyperparameters by projecting the empirical covariance or (Fourier) power spectrum onto a
parametric family, thus proposing and studying various measures of discrepancy operating
on the temporal and frequency domains. Our contribution extends the variogram method
developed by the geostatistics literature and, accordingly, it is referred to as the generalised
variogram method (GVM). In addition to the theoretical presentation of GVM, we pro-
vide experimental validation in terms of accuracy, consistency with ML and computational
complexity for diﬀerent kernels using synthetic and real-world data.
1 Introduction
Gaussian processes (GPs) are Bayesian nonparametric models for time series praised by their interpretability
and generality. Their implementation, however, is governed by two main challenges. First, the choice
of the covariance kernel, which is usually derived from ﬁrst principles or expert knowledge and thus may
result in complex structures that hinder hyperparameter learning. Second, the cubic computational cost of
standard, maximum-likelihood-based, training which renders the exact GP unfeasible for more than a few
thousands observations. The GP community actively targets these issues though the development of robust,
computationally-eﬃcient training strategies. Though these advances have facilitated the widespread use of
GP models in realistic settings, their success heavily depends on the initialisation of the hyperparameters.
In practice, initialisation either follows from expert knowledge or time-consuming stochastic search. This
is in sharp contrast with the main selling point of GPs, that is, being agnostic to the problem and able
to freely learn from data. To provide researchers and practitioners with an automated, general-application
and cost-eﬃcient initialisation methodology, we propose to learn the hyperparameters by approximating the
empirical covariance by a parametric function using divergences between covariances that are inexpensive
1Published in Transactions on Machine Learning Research (04/2023)
to compute. This approach is inspired by the common practice, such as when one computes some statistics
(e.g., mean, variance, discrete Fourier transform) to determine initial values for the kernel hyperparameters.
In the geostatistics literature, a method that follows this concept is the Variogram (Cressie, 1993; Chiles
& Delﬁner, 1999), which is restricted to particular cases of kernels and divergences. Therefore, we refer
to the proposed methodology as Generalised Variogram Method (GMV) in the sense that it extends the
application of the classic methodology to a broader scenario that includes general stationary kernels and
metrics, in particular, Fourier-based metrics. Though our proposal is conceptually applicable to an arbitrary
input/output dimension, we focus on the scalar-input scalar-output case and leave the extension to general
dimensions as future work.1
The contributions of our work are of theoretical and applied nature, and are summarised as follows:
•a novel, computationally-eﬃcient, training objective for GPs as an alternative to maximum likeli-
hood, based on a projection of sample covariance (or power spectrum) onto a parametric space;
•a particular instance of the above objective, based on the Wasserstein distance applied to the power
spectrum, that is convex and also admits a closed-form solution which can thus be found in a single
step;
•a study of the computational cost of the proposed method and its relationship to maximum likeli-
hood, both in conceptual and empirical terms;
•experimental validation of the proposed method assessing its stability with respect to initial con-
ditions, conﬁrming its linear computational cost, its ability to train GPs with a large number of
hyperparameters and its advantages as initialiser for maximum-likelihood training.
2 Preliminaries
2.1 Motivation
Let us consider y∼GP (0,K)and its realisation y= [y1,...,yn]∈Rnat times t= [t1,...,tn]∈Rn. The
kernelKis usually learnt by choosing a family {Kθ}θ∈Θand optimising the log-likelihood
l(θ) =−1
2Tr (K−1
θyy/latticetop)−1
2log|Kθ|−n
2log 2π, (1)
with respect to θ∈Θ, where we used the cyclic property of the trace, and deﬁned Kθdef=Kθ(t)according
to[Kθ]ij=Kθ(ti−tj),i,j∈{1,...,n}. Deﬁning Kdef=K(t) =Eyy/latticetop, we note that
El(θ) =−1
2Tr (K−1
θK)−1
2log|Kθ|−n
2log 2π. (2)
Noticethat,uptotermsindependentof θ,equation2isequivalenttothenegativeKullback-Leiblerdivergence
(NKL) between the (zero-mean) multivariate normal distributions N(0,K)andN(0,Kθ)given by
DNKL(N(0,K)||N(0,Kθ)) =−1
2/parenleftbigg
Tr/parenleftbig
K−1
θK/parenrightbig
+ log|Kθ|
|K|−n/parenrightbigg
, (3)
which, with a slight abuse of notation, can be expressed as a function of only the covariances as
DNKL(K,Kθ)def=DNKL(N(0,K)||N(0,Kθ)).
This reveals that learning a GP by maximising l(θ)in equation 1 can be understood (in expectation) as
minimising the KL between the t-marginalisations of the true process GP(0,K)and a candidate process
GP(0,Kθ). This motivates the following remark.
Remark 1. Since maximum-likelihood learning of GPs has a cubic computational cost but it is (on average)
equivalent to minimising a KL divergence, what other divergences or distances, computationally cheaper than
the likelihood, can be considered for learning GPs?
1For illustration and completeness, we incorporate a toy experiment featuring 5-dimensional input data in Sec. 5.7.
2Published in Transactions on Machine Learning Research (04/2023)
2.2 Divergences over covariance functions
We consider zero-mean stationary GPs.2The stationary requirement allows us to i) aggregate observations
in time when computing the covariance, and ii) compare covariances in terms of their (Fourier) spectral
content. Both perspectives will be present throughout the text, thus, we consider two types of divergences
over covariances: i) temporal ones, which operate directly to the covariances, and ii) spectral ones, which
operate over the the Fourier transform of the covariances, i.e., the GP’s power spectral density (PSD). As
our work relies extensively on concepts of spectral analysis and signal processing, a brief introduction to the
subject is included in Appendix A.
Though we can use most metrics (e.g., L1,L2) on both domains, the advantage of the spectral perspective is
that it admits density-speciﬁc divergences as it is customary in signal processing (Basseville, 1989). Bregman
divergences, which include the Euclidean, KL and Itakura-Saito (IS) (Itakura, 1968), are verticalmeasures,
i.e, they integrate the point-wise discrepancy between densities across their support. We also consider
horizontal spectral measures, based on the minimal-cost to transport the mass from one distribution—across
the support space—onto another. This concept, known as optimal transport (OT) (Villani, 2009) has only
recently been considered for comparing PSDs using, e.g., the 1- and 2-Wasserstein distances, denoted W1
andW2(Cazelles et al., 2021; Henderson & Solomon, 2019). See Appendix B for deﬁnitions of vertical and
horizontal divergences.
2.3 Related work
A classic notion of kernel dissimilarity in the machine learning community is that of kernel alignment , a
concept introduced by Cristianini et al. (2001) which measures the agreement between two kernels or a
kernel and a task; this method has been mostly applied for kernel selection in SVM-based regression and
classiﬁcation. ThisnotionofdissimilarityisbasedontheFrobeniusinnerproductbetweentheGrammatrices
of each kernel given a dataset—see (Cristianini et al., 2001, Def. 1). Though, in spirit, this concept is related
to ours in that the kernel is learnt by minimising a discrepancy measure, we take a signal-processing inspired
perspective and exploit the fact that, for stationary GPs, kernels are single-input covariance functions and
thus admit computationally-eﬃcient discrepancy measures. In addition to the reference above, the interested
reader is referred to (Cortes et al., 2012) for the centredkernel alignment method.
Within the GP community, two methodologies for accelerated training can be identiﬁed. The ﬁrst one
focuses directly on the optimisation procedure by, e.g., avoiding inverses (van der Wilk et al., 2020), or
derivatives (Rios & Tobar, 2018), or even by parallelisation; combining these techniques has allowed to
process even a million datapoints (Wang et al., 2019). A second perspective is that of sparse GP approxima-
tions using pseudo-inputs (Quinonero-Candela & Rasmussen, 2005), with particular emphasis on variational
methods (Titsias, 2009). The rates of convergence of sparse GPs has been studied by Burt et al. (2019)
and the hyperparameters in this approach have also been dealt with in a Bayesian manner by Lalchand
et al. (2022). Sparse GPs have allowed for ﬁtting GPs to large datasets (Hensman et al., 2013), training
non-parametric kernels (Tobar et al., 2015), and implementing deep GPs (Damianou & Lawrence, 2013;
Salimbeni & Deisenroth, 2017). Perhaps the work that is closest to ours in the GP literature is that of Liu
et al. (2020), which trains a neural network to learn the mapping from datasets to GP hyperparameters
thus avoiding the computation of the likelihood during training. However, the authors state that “ training
on very large datasets consumes too much GPU memory or becomes computationally expensive due to the
kernel matrix size and the cost of inverting the matrix ”. This is because they still need to compute the kernel
matrix during training of the net, while we bypass that calculation altogether.
The Wasserstein distance has been used to compare laws of GPs (Masarotto et al., 2019; Mallasto & Feragen,
2017), and applied to kernel design, in particular to deﬁne GPs (Bachoc et al., 2018) and deep GPs (Popescu
et al., 2022) over the space of probability distributions. Cazelles et al. (2021) proposed a distance between
time series based on the Wasserstein, termed the Wasserstein-Forier distance, and showed its application
2We do so for convenience in computation since non-zero-mean GPs can be understood as a mixture of a zero-mean GP
and a parametric regression model, where the GP models the residuals of the parametric part. See Section 2.7 in Rasmussen &
Williams (2005) for a discussion.
3Published in Transactions on Machine Learning Research (04/2023)
to GPs. However, despite the connection between GPs and the Wasserstein distance established by these
works, they have not been implemented to train (or initialise) GPs.
In geostatistics, the variogram function (Cressie, 1993; Chiles & Delﬁner, 1999) is deﬁned as the variance of
the diﬀerence of a process yat two locations t1andt2, that is,γ(t1−t2) =V[y(t1)−y(t2)]. The variogram is
computed by choosing a parametric form for γ(t)and then ﬁt it to a cloud of points (sample variogram) using
least squares. Common variogram functions in the literature include exponential and Gaussian ones, thus
drawing a natural connection with GP models. Furthermore, when the process yis stationary and isotropic
(or one-dimensional) as in the GP models considered here, the variogram and the covariance K(t)follow
the relationship γ(t) =K(0)−K(t), therefore, given a kernel function the corresponding variogram function
can be clearly identiﬁed (and vice versa). The way in which the variogram is ﬁtted in the geostatistics
literature is what inspires the methodology proposed here: we match parametric forms of the covariance (or
the PSD) to their corresponding sample approximations in order to ﬁnd appropriate values for the kernel
hyperparameters. Also, as we explore diﬀerent distances for the covariance and PSD beyond the Euclidean
one (least squares) we refer to our proposal as the Generalised Variogram Method (GVM).
GVM complements the literature in a way that is orthogonal to the above developments. We ﬁnd the
hyperparameters of a GP in a likelihood-free manner by minimising a loss function operating directly on the
sample covariance or its Fourier transform. As we will see, GVM is robust to empirical approximations of
the covariance or PSDs, admits arbitrary distances and has a remarkably low computational complexity.
3 A likelihood-free covariance-matching strategy for training GPs
Overview of the section. As introduced in Section 2.1, our objective is to approximate the ground-truth
kernelKwith a parametric kernel Kθ∗; to this end we will rely on an empirical data-driven estimate of
the kernel denoted ˆKn. We will proceed by matching ˆKnwith a parametric form Kθ∗nusing metrics in the
temporal domain, i.e., solving equation 7, or in the spectral domain matching the Fourier transform of ˆKn,
denoted ˆSn=FˆKn, with a parametric form Sθ∗n, i.e., solving equation 8. In the following, we present the
Fourier pairs KθandSθ, the estimators ˆKnand ˆSn, and the metrics considered for the matching. We then
present an explicit case for location-scale families, and we ﬁnally give theoretical arguments for the proposed
learning method.
3.1 Fourier pairs KθandSθand their respective estimators
Let us consider the zero-mean stationary process y∼GP (0,Kθ)with covariance Kθand hyperparameter
θ∈Θ. If the covariance Kθis integrable, Bochner’s Theorem (Bochner, 1959) states that Kθand the
process’ power spectral density (PSD), denoted Sθ, areFourier pairs , that is,
Sθ(ξ) =F{Kθ}def=/integraldisplay
RKθ(t)e−j2πξtdt, (4)
wherejis the imaginary unit and F{·}denotes the Fourier transform.
Since zero-mean stationary GPs are uniquely determined by their PSDs, any distance deﬁned on the space
of PSDs can be “lifted” to the space covariances and then to that of GPs. Therefore, we aim at learning the
hyperparameter θ∈Θby building statistics in either the temporal space (sample covariances) or in spectral
space (sample PSDs).
First, we consider the following statistic ˆKnin order to approximate the ground-truth kernel K.
Deﬁnition 1. Lety∈Rbe a zero mean stochastic process over Rwith observations y= [y1,...,yn]∈Rn
at times t= [t1,...,tn]∈Rn. The empirical covariance of yis given by
ˆKn(t) =n/summationdisplay
i,j=1yiyj1t=ti−tj
Card{t|t=ti−tj}. (5)
4Published in Transactions on Machine Learning Research (04/2023)
The estimator ˆSn(t)of the PSD Sthen simply corresponds to applying the Fourier transform to the empirical
kernel ˆKnin equation 5, that is
ˆSn(ξ) =/integraldisplay
RˆKn(t)e−j2πξtdt. (6)
Another usual choice for estimating the PSD is the Periodogram ˆSPer(Schuster, 1900). Though ˆSPeris
asymptotically unbiased ( ∀ξ,EˆSPer(ξ)→S(ξ)), it is inconsistent, i.e., its variance does not vanish when
n→∞(Stoica & Moses, 2005)[Sec. 2.4.2]. Luckily, the variance of ˆSPer(ξ)can be reduced via windowing
and the Welch/Bartlett techniques which produce (asymptotically) consistent and unbiased estimates of
S(ξ),∀ξ(Stoica & Moses, 2005).
3.2 Fourier-based covariance divergences
The proposed method builds on two types of (covariance) similarity. First, those operating directly on ˆKn
andKθ, known as temporal divergences, which include the L1andL2distances. Second, those operating
overtheFouriertransformsof ˆKnandKθ, thatis ˆSnandSθ, knownas spectral divergences. Inthetemporal
case, we aim to ﬁnd the hyperparameters of yby projecting ˆKn(t)in equation 5 onto the parametrised family
K={Kθ,θ∈Θ}. That is, by ﬁnding θ∗such thatKθ(·)isas close as possible toˆKn(t), i.e.,
θ∗
n= arg min
θ∈ΘD(ˆKn,Kθ), (7)
where the function D(·,·)is the chosen criterion for similarity. Akin to the strategy of learning the hy-
perparameters of the GP by matching the covariance, we can learn the hyperparameters by projecting an
estimator of the PSD, namely ˆSnin equation 6, onto a parametric family S={Sθ,θ∈Θ}, that is,
θ∗
n= arg min
θ∈ΘDF(ˆSn,Sθ), (8)
whereDF(·,·)is a divergence operating on the space of PSDs.
Remark 2. Since the map Kθ→Sθis one-to-one, equation 7 and equation 8 are equivalent when DF(·,·) =
D(F{·},F{·} )andS=F{K}.
We will consider parametric families Swith explicit inverse Fourier transform, since this way θparametrises
both the kernel and the PSD and can be learnt in either domain. These families include the Dirac delta,
Cosine, Square Exponential (SE), Student’s t, Sinc, Rectangle, and their mixtures; see Figure 10 in Appendix
A for an illustration of some of these PSDs and their associated kernels.
Remark 3 (Recovering kernel parameters from PSD parameters) .For a parametric Fourier pair ( K,S),
the Fourier transform induces a map between the kernel parameter space and the PSD parameter space.
For kernel/PSD families considered in this work this map will be bijective, which allows us to compute
the estimated kernel parameters from the estimated PSD parameters (and vice versa); see Table 1 for two
examples of parametric kernels and PSDs. Furthermore, based on this bijection we refer as θto both kernel
and PSD parameters.
3.3 A particular case with explicit solution: location-scale family of PSDs and 2-Wasserstein distance
Of particular relevance to our work is the 2-Wasserstein distance ( W2) andlocation-scale families of PSDs,
for which the solution of equation 8 is completely determined through ﬁrst order conditions.
Deﬁnition 2 (Location–scale) .A family of one-dimensional integrable PSDs is said to be of location-scale
type if it is given by/braceleftbigg
Sµ,σ(ξ) =1
σS0,1/parenleftbiggξ−µ
σ/parenrightbigg
,µ∈R,σ∈R+/bracerightbigg
, (9)
whereµ∈Ris the location parameter, σ∈R+is the scale parameter and S0,1is the prototype of the family.
For arbitrary prototypes S0,1, location-scale families of PSDs are commonly found in the GP literature. For
instance, the SE, Dirac delta, Student’s t, Rectangular and Sinc PSDs, correspond to the Exp-cos, Cosine,
5Published in Transactions on Machine Learning Research (04/2023)
Table 1: Location-scale PSDs (left) and their covariance kernel (right) used in this work. We have denoted
bytandξthe time and frequency variables respectively.
PSD Sµ,σ(ξ) PrototypeS0,1(ξ)Kernel Kµ,σ(t) K0,1(t)
Square-exp exp/parenleftBig
−/parenleftbigξ−µ
σ/parenrightbig2/parenrightBig
exp(−ξ2) Exp-cos√πσexp(−σ2π2t2) cos(2πµt)√πexp(−π2t2)
Rectangular rect/parenleftbigξ−µ
σ/parenrightbig
rect(ξ) Sinc σsinc(σt) cos(2πµt) sinc(t)
Laplace, Sinc, and Rectangular kernels respectively. Location-scale families do not, however, include kernel
mixtures, which are also relevant in our setting and will be dealt with separately. Though the prototype
S0,1might also be parametrised (e.g., with a shapeparameter), we consider those parameters to be ﬁxed
and only focus on θ:= (µ,σ)for the rest of this section. Table 1 shows the two families of location-scale
PSDs (and their respective kernels) that will be used throughout our work.
Remark 4. Let us consider a location-scale family of distributions with prototype S0,1and an arbitrary
memberSµ,σ. Their quantile (i.e., inverse cumulative) functions, denoted Q0,1andQµ,σrespectively, obey
Qµ,σ(p) =µ+σQ0,1(p). (10)
The linear expression in equation 10 is pertinent in our setting and motivates the choice of the 2-Wasserstein
distanceW2to compare members of the location-scale family of PSDs. This is because for arbitrary one-
dimensional distributions S1andS2,W2
2(S1,S2)can be expressed according to:
W2
2(S1,S2) =/integraldisplay1
0(Q1(p)−Q2(p))2dp, (11)
whereQ1andQ2denote the quantiles of S1andS2respectively. We are now in position to state the ﬁrst
main contribution of our work.
Theorem 1. IfSis a location-scale family with prototype S0,1, andSis an arbitrary PSD, the minimiser
(µ∗,σ∗)ofW2(S,Sµ,σ)is unique and given by
µ∗=/integraldisplay1
0Q(p)dpandσ∗=1/integraltext1
0Q2
0,1(p)dp/integraldisplay1
0Q(p)Q0,1(p)dp, (12)
whereQis the quantile function of S.
Proof Sketch. The proof follows from the fact that W2
2(S,Sµ,σ)is convex both on µandσ, which is shown by
noting that its Hessian is positive via Jensen’s inequality. Then, the ﬁrst order conditions give the solutions
in equation 12. The complete proof can be found in Appendix C.
Remark 5. Although integrals of quantile functions are not usually available in closed-form, computing
equation 12 is straightforward. First, Q0,1(p)is known for a large class of normalised prototypes, including
square exponential and rectangular functions. Second, µ∗=Ex∼S[x]and/integraltext1
0Q2
0,1(p)dp=Ex∼S[x2], wherex
is a random variable with probability density S. Third, both integrals are one-dimensional and supported on
[0,1], thus numerical integration is inexpensive and precise, specially when Shas compact support.
As pointed out by Cazelles et al. (2021), W2
2(S,Sθ)is in general non-convex in θ, however, for the particular
case of the location-scale family of PSDs and parameters (µ,σ), convexity holds. Since this family includes
usual kernel choices in the GP literature, Theorem 1 guarantees closed form solution for the optimisation
problem in equation 8 and thus can be instrumental for learning GPs without computing (the expensive)
likelihood function.
3.4 Learning from data
Learning the hyperparameters through the optimisation objective in equation 7 or equation 8 is possible
provided that the statistics ˆKnand ˆSnconverge to KandSrespectively. We next provide theoretical
6Published in Transactions on Machine Learning Research (04/2023)
results on the convergence of the optimal minimiser θ∗
n. The ﬁrst result, Proposition 1, focuses on the
particular case of the 2-Wasserstein distance and location-scale families, presented in Section 3.3.
Proposition 1. LetSθbe a location-scale family, Sthe ground truth PSD and DF=W2
2. Then
EW2
2(S,ˆSn)→0implies that the empirical minimiser θ∗
nin equation 8 converges to the true minimiser
θ∗= arg minθ∈ΘW2
2(S,Sθ), meaning that E|θ∗
n−θ∗|→0.
Proof Sketch. First, in the location-scale family we have θ= (µ,σ). Then, the solutions in Theorem 1 allow
us to compute upper bounds for |µ∗−µ∗
n|and|σ∗−σ∗
n|via Jensen’s and Hölder’s inequalities, both of which
converge to zero as EW2
2(S,ˆSn)→0. The complete proof can be found in Appendix C.
The second result, Proposition 2, deals with the more general setting of arbitrary parametric families, the
distancesL1,L2,W1,W2and either temporal and frequency based estimators. To cover both cases, we will
denote ˆfnto refer to either ˆKnorˆSn. Let us also consider the parametric function fθ∈{fθ|θ∈Θ}and the
ground-truth function fwhich denotes either the ground-truth covariance or ground-truth PSD.
Proposition 2. For general parametric families {fθ|θ∈Θ}, the empirical solution θ∗
nconverges a.s. to
the true solution θ∗under the following suﬃcient and stronger conditions: (i) D=WrorLr,r= 1,2, (ii)
D(ˆfn,f)a.s.−−−−→
n→∞0; (iii)θn−−−−→
n→∞θ⇐⇒D(fθn,fθ)→0; and (iv) the parameter space Θis compact.
Proof.This result follows as a particular case from Theorem 2.1 in Bernton et al. (2019), where the authors
study general r-Wasserstein distance estimators for parametric families of distributions for empirical mea-
sures, under the notion of Γ-convergence or, equivalently, epi-convergence. The proof for the Lr,r= 1,2
case is similar to that of Wr.
4 Practical considerations
This section is dedicated to the implementation of GVM. We ﬁrst discuss the calculation of the estimators
ˆKnand ˆSnand their complexity, we then present the special case of location-scale family and spectral
2-Wasserstein loss. The last subsection is dedicated to the noise variance parameter and the relationship
between GVM and ML solutions.
4.1 Computational complexity of the estimators ˆKnand ˆSn
First, for temporal divergences (operating on the covariance) we can modify the statistic in equation 5 using
binning, that is, by averaging values for which the lags are similar; this process is automatic in the case of
evenly-sampled data and widely used in discrete-time signal processing. This allows to reducing the amount
of summands in ˆKnfromn(n+1)
2to an order nor even lower if the range of the data grows beyond the length
of the correlations of interests.
Second, the Periodogram ˆSPercan be computed at a cost of O(nk)using bins, where nis the number of
observations and kthe number of frequency bins; in the evenly-sampled case, one could set k=nand apply
the fast Fourier transform at a cost O(nlogn). However, for applications where the number of datapoints
greatly exceeds the required frequency resolution, kcan be considered to be constant, which results in a cost
linear in the observations O(n).
4.2 The location-scale family and 2-Wasserstein distance
Algorithm 1 presents the implementation of GVM in the case of location-scale family of PSDs {Sθ}θ∈Θand
2-Wasserstein loss presented in Section 3.3.
7Published in Transactions on Machine Learning Research (04/2023)
Algorithm 1 GVM - Spectral loss W2&({Sθ}θ∈Θislocation-scale)
Require:ti,yi,i= 1,...,n
Require: location-scale PSD family {Sθ}θ∈Θ(e.g., Gaussians)
Deﬁne a grid over frequency space g={ξ0,ξ1,...,ξk}
Compute ˆSn=ˆSn(g), from eq. (6), over the chosen frequency grid
ComputeQnthe quantile function of ˆSn
Computeθ⋆
ngiven by eq. (12) with Q:=Qn
Linear computational complexity and quantiles. The cost of the 2-Wasserstein loss is given by
calculating i) the Periodogram ˆSPerof the estimator ˆSnin equation 6, ii) its corresponding quantile function,
and iii) the integrals in equation 12. Though the quantile function is available in closed form for some
families of PSDs (see Remark 5) they can also be calculated by quantisation in the general case: for a sample
distribution (or histogram) one can compute the cumulative distribution and then invert it to obtain the
quantile. Additionally, the integrals of the quantile functions also have a linear cost but only in the number
of frequency bins O(k)since they are frequency histograms, therefore, computing the solution has a linear
cost in the data.
4.3 Solving the general case: beyond the location-scale family
Recall that the proposed method has a closed-form solution when one considers PSDs in location-scale
family and the W2distance over the spectral domain. However, in the general case in equations 7 and 8,
the minimisation is not convex in θand thus iterative/numerical optimisation is needed. In particular, for
horizontal Fourier distances (e.g., Wasserstein distances) the derivative of the loss depends on the derivative
of the quantile function Qθwith respect to θ, which might not even be known in closed form in general,
specially for mixtures. However, in most cases the lags of the empirical covariance or the frequencies of
the Periodogram belong to a compact space and thus numerical computations are precise and inexpensive.
Therefore, approximate derivatives can be considered (e.g., in conjunction with BFGS) though in our ex-
periments the derivative-free Powell method also provide satisfactory results. Algorithms 2 and 3 present
the application of GVM using the temporal and spectral metrics respectively. Note that we will usually
considerL1,L2as temporal metrics Dand alsoW1,W2, Itakura- Saito and KL as spectral divergences DF.
Algorithms 2 and 3 are presented side to side for the convenience of the reader.
Algorithm 2 GVM - Temporal loss
Require:ti,yi,i= 1,...,n
Require: parametric family {Kθ}θ∈Θ
Require: temporal metric D(·,·)
Deﬁne temporal grid t={t1,t2,...,tn}
ComputeKθ=Kθ(t)
Compute ˆKn=ˆKn(t)as in eq. (5)
Construct loss θ/mapsto→D(ˆKn,Kθ)
Findθ⋆
nin eq. (7) using BFGS or PowellAlgorithm 3 GVM - Spectral loss
Require:ti,yi,i= 1,...,n
Require: parametric family {Sθ}θ∈Θ
Require: spectral metric DF(·,·)
Deﬁne frequency grid g={ξ0,ξ1,...,ξk}
ComputeSθ=Sθ(g)
Compute ˆSn=ˆSn(g), from eq. (6)
Construct loss θ/mapsto→DF(Sθ,ˆSn)
Findθ⋆
nin eq. (8) using BFGS or Powell
Linear computational complexity. For the general case of spectral losses using numerical optimisation
methods, we need to calculate ˆSnor its quantile (which are O(n)) only once, to then compute the chosen
distanceDF, which isO(k)for discrete measures deﬁned on a k-point frequency grid as many times as the
optimiser requires it. Therefore, the cost is O(k)but with a constant that depends on the complexity of
the parametric family {Sθ,θ∈Θ}and the optimiser of choice. For spatial losses, the complexity follows the
same reasoning for the chosen spatial distance Dand parametric family {Kθ,θ∈Θ}, and thus is also linear
in the datapoints.
8Published in Transactions on Machine Learning Research (04/2023)
4.4 Noise variance and relationship to maximum likelihood
Following the assumptions of the Fourier transform, the spectral divergences considered apply only to
Lebesgue-integrable PSDs, which rules out the relevant case of white-noise-corrupted observations. This
is because white noise, deﬁned by a Dirac delta covariance, implies a PSD given by a constant, positive,
inﬁnite-support, spectral ﬂoor that is non-integrable. These cases can be addressed with temporal diver-
gences, which are well suited (theoretically and in practice) to handle noise.
The proposed hyperparameter-search method is intended both as a standalone likelihood-free GP learning
technique and also as a initialisation approach to feed initial conditions to a maximum likelihood (ML)
routine. In this sense, we identify a relationship between the ML estimator ˆθMLand the proposed estimator
θ∗:= arg minθ∈ΘL2(ˆS,Sθ). From equation 4 and Plancherel’s theorem, we have /bardblS−Sθ/bardblL2=/bardblK−Kθ/bardblL2.
Then, by deﬁnition of the estimators and Lemma 2 in Hoﬀman & Ma (2020), we obtain the following
inequality
DKL(K||KˆθML)≤DKL(K||Kθ∗)≤1
2/bardblK−1/bardbl2/bardblK−1
θ∗/bardbl2/bardblK−Kθ∗/bardblF, (13)
where/bardbl·/bardblFdenotes the matrix Frobenius norm, DKL(A||B)denotes the KL divergence between zero-mean
multivariate normal distributions with covariances AandB, and recall that Kis the kernel of the ground
truth GP. Denoting the ball centred at 0 with radius MbyB(0,M), we present the following remark.
Remark 6. The inequality in equation 13 states that if the proposed estimator θ∗is such that/bardblS−Sθ∗/bardblL2∈
B(0,M), thenDKL(K||KˆθML)∈B(0,1
2/bardblK−1/bardbl2/bardblK−1
θ∗/bardbl2M). Therefore, under the reasonable assumption that
the function θ/mapsto→Kθonly produces well-conditioned matrices, the factor /bardblK−1/bardbl2/bardblK−1
θ∗/bardbl2is bounded and thus
both balls have radius of the same order.
5 Experiments
This section illustrates diﬀerent aspects of the proposed GVM through the following experiments (E):
E1: shows that GVM, unlike ML, is robust to diﬀerent initialisation values and subsets of observations,
E2: studies the sensibility of GVM to the calculation of ˆSnusing Periodogram, Welch and Bartlett,
E3: validates the linear computational complexity of GVM in comparison to the full and sparse GPs,
E4: compares spectral against temporal implementations of GVM on an audio time series,
E5: exhibits the results of learning a 20-component spectral mixture GP using GVM with diﬀerent
spectral metrics,
E6: assesses the ability of GVM to produce initial conditions which are then passed to an ML routine
that learns a spectral mixture GP with 4, 8, 12 and 16 components,
E7: presents a toy example where GVM is used to ﬁt a GP with an isotropic SE kernel to multi-input
(5-dimensional) data.
The code for GVM is available at https://github.com/GAMES-UChile/Generalised-Variogram-Method ,
a minimal working example of the code is presented in Appendix D. The benchmarks were implemented on
MOGPTK (de Wolﬀ et al., 2021).
5.1 E1: Stability with respect to initial conditions (spectral mixture kernel, L2, temporal)
This experiment assessed the stability of GVM with respect to random initial conditions and dif-
ferent realisations. We considered a GP with a 2-component spectral mixture kernel K(t) =/summationtext2
i=1σ2
iexp(−γiτ2) cos(2πµiτ) +σ2
noiseδτwith hyperparameters σ1= 2,γ1= 10−4,µ1= 2·10−2,
σ2= 2,γ2= 10−4,µ2= 3·10−2,σnoise = 1. We produced 4000-point realisations from the GP and 50
random initial conditions {θr}50
r=1according to [θr]i∼Uniform [1
2θi,3
2θi], whereθiis thei-th true hyperpa-
rameter.
9Published in Transactions on Machine Learning Research (04/2023)
We considered two settings: i) train from {θr}50
r=1using ML, and ii) compute GVM from {θr}50
r=1and
then perform ML starting from the value found by GVM. Each procedure was implemented using a single
realisation (to test stability wrt θr) and diﬀerent realisations (to test stability wrt the data). Our estimates
ˆθiwere assessed in terms of the NLL and the relative mean absolute error (RMAE) of the parameters/summationtext8
i=1|θi−ˆθi|/|θi|.
Fig. 1 shows the NLL (left) and RMAE (right) versus computation time, for the cases of ﬁxed (top) and
diﬀerent (bottom) observations; all times start from t= 1and are presented in logarithmic scale. First, in
all cases the GVM initialisation (in red) took about half a second and resulted in an NLL/RMAE virtually
identical to those achieved by ML initialised by GVM, this means that GVM provides reliable parameters
and not just initial conditions for ML. Second, for the ﬁxed observations (top), the GVM was stable wrt θr
unlike ML which in some cases diverged. Third, for the multiple observations (bottom) GVM-initialised ML
diverged in two (out of 50) runs, which is far fewer than the times that random-initialised ML diverged.
100101
time [seconds]2×1033×1034×1036×103NLL: fixed data & random initial condition (50 runs)
ML from random
ML from init
initialisation
100101
time [seconds]100101102relative absolute error: fixed data & random initial condition (50 runs)
ML from random ML from init initialisation
100101
time [seconds]2×1033×1034×1036×103NLL: random data & random initial condition (50 runs)
ML from random
ML from init
initialisation
100101
time [seconds]100101102relative absolute error: random data & random initial condition (50 runs)
ML from random ML from init initialisation
Figure 1: GVM as initialisation for ML starting form 50 random initial conditions: proposed GVM (red),
standard ML (green) and ML starting from GVM (blue). The top plots consider a single dataset, while the
bottom plots consider diﬀerent realisations for each initial condition. The L-BFGS-B optimizer was used
with the default gradient tolerance in order for the results to be comparable.
5.2 E2: Sensibility of GVM wrt the Periodogram (exact case: W2, location-scale)
We then considered kernels with location-scale PSDs and the W2metric over the PSDs. This case has a
unique solution but requires us to compute ˆSnin equation 8; this experiment evaluates diﬀerent ways of
doing so. We produced 4000 observations evenly-sampled in[0,1000]from GPs with square exponential
and rectangular PSDs (which correspond to the Exp-cos and Sinc kernels respectively—see Table 1), each
withlocation µ∼U[0.025,0.075]andscalel∼U[0.01,0.02]. Wecomputed ˆSviathePeriodogram, Welchand
Bartlett methods with diﬀerent windows. Table 2 shows the percentage relative error (PRE)3averaged over
50 runs. The estimates of both parameters are fairly consistent: their magnitude does not change radically
for diﬀerent windows and Periodogram methods. In particular, the estimates of the location parameter are
accurate with an average error in the order of 2% for both kernels. The scale parameter was, however, more
diﬃcult to estimate, this can be attributed to the spectral energy spread in the Periodogram, which, when
using the Wasserstein-2 metric, results in the scale parameter to be overestimated. For both kernels and
µ= 0.05,l= 0.01, the case of the (windowless) Periodogram is shown in Fig. 2 and the remaining cases are
all shown in Appendix E. In the light of these results, we considered the Periodogram (no window) for the
remaining experiments.
3PRE = 100|θ−ˆθ|
θ, where θis the true parameter and ˆθits estimate.
10Published in Transactions on Machine Learning Research (04/2023)
Table 2: Performance of GVM learning GPs with the Exp-cos and Sinc kernels under diﬀerent sampling
settings, Periodogram methods and windows. Each entry of the table shows the average percentage relative
error for location (left) and scale (right), with their standard deviations, separated by the symbol "/". True
parameters where µ∼U[0.025,0.075]andl∼U[0.01,0.02]; averages and standard deviations computed over
50 runs.
Kernel Window Periodogram Bartlett Welch
Exp-cosnone 2.30±1.61/33.41±13.27 2.17±1.91/20.88±11.69 2.05±1.64/24.87±14.35
hann 2.98±2.14/34.93±14.33 3.02±2.36/26.23±13.05 2.52±1.75/31.83±13.29
hamming 2.92±2.04/34.93±14.16 2.93±2.29/27.35±13.26 2.49±1.73/32.26±13.22
Sincnone 2.36±1.65/8.93±6.73 2.63±2.23/83.23±26.39 2.31±1.77/38.87±15.95
hann 2.68±2.02/10.03±9.22 2.59±1.77/58.86±17.48 2.44±1.78/19.52±11.32
hamming 2.60±2.01/9.62±8.93 2.56±1.75/50.62±15.79 2.43±1.76/16.51±10.75
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07
frequencies050100150Learnt parameters are loc: 0.0501, scale: 0.0089
Periodogram (w: None)
Learnt kernel
Ground truth Exp-cos kernel
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07
frequencies0100200Learnt parameters are loc: 0.0506, scale: 0.0108
Periodogram (w: None)
Learnt kernel
Ground truth Sinc kernel
Figure 2: GVM estimates for Exp-cos (left) and Sinc (right) kernels shown in red against ˆS(blue) and true
kernels (black). This case: Periodogram, no window.
5.3 E3: Linear complexity (exact case)
We then evaluated the computation time for the exact case of GVM ( W2distance and location-scale family)
for an increasing amount of observations. We considered unevenly-sampled observations from an single
component SM kernel ( µ= 0.05,σ= 0.01) in the range [0,1000]. We compared GVM against i) the ML
estimate starting from the GVM value (full GP, 100 iterations), and ii) the sparse GP using 200 pseudo
inputs (Snelson & Ghahramani, 2006). Fig. 3 shows the computing times versus the number of observations
thus validating the claimed linear cost of GVM and its superiority wrt to the rest of the methods. The (solid
line) interpolation in the plot is of linear order for GVM, linear for sparse GP since the number of inducing
points is ﬁxed, and cubic for the full GP.
Figure 3: Training times vs number of datapoints for the proposed GVM, full GP and sparse GP.
5.4 E4: Performance and cost of spectral and temporal metrics over the same time series
This experiment compares the performance and computational cost of the temporal and spectral implemen-
tations of GVM for a common time series and GP models of increasing complexity. We used a real-world
11Published in Transactions on Machine Learning Research (04/2023)
audio signal from the Free Spoken Digit Dataset4. GVM was implemented with the metrics L1andL2both
in the spectral and temporal domain to learn a sample from the above dataset, which was 4300 samples
long. The kernel considered was a spectral mixture (SM) (Wilson & Adams, 2013), Fig. 4 shows losses and
running times as a function of the number of components of the SM kernel; all runs use the Powell optimiser.
1234567891011121314151617181920
Number of components102
101
100101Loss
1234567891011121314151617181920
Number of components100101102Time [seconds]
Spectral L1
Spectral L2
Temporal L1
Temporal L2
Figure 4: Temporal and spectral implementation of GVM: Losses (left) and running times (right) as a
function of the order of the spectral mixture kernel.
From the left plot in Fig. 4 (losses) let us recall that each implementation has its own metric and thus
they are not comparable directly, however, notice that both spectral implementations are monotonic with
respect to the model order. Furthermore, the fact that the Temporal L2loss increases with the model order
suggests that the optimisation in the time domain is more challenging than in its spectral counterpart. In
terms of computational complexity, the right plot in Fig. 4 conﬁrms monotonicity of the computational cost
with the number of kernel parameters, however, notice that the spectral implementations reached a plateau
after 10 components; further suggesting the superiority of the spectral implementation in terms of ease of
optimisation.
Lastly, Fig. 5 shows the ﬁtting of both implementations and metrics in their respective domains. In the time
domain (top plots) we can see that the L1metric (top left) provides a generally acceptable ﬁt but misses
some peaks of the autocorrelation function (empirical kernel estimate ˆKn), while the L2metric (top right)
aims to reach the peaks of ˆKnat the cost of missing central parts of it. In the spectral domain (bottom
plots) we see only minor discrepancies for both metrics, with perhaps the most interesting feature being the
diﬀerence in the peak at around frequency 0.04, where L1matched the peak and L2provided a wider ﬁt.
0 100 200 300 400
time0.00.51.0T emporal distance: L1
Autocorrelation function
Learnt 10-comp Spectral Mix.
0 100 200 300 400
time0.00.51.0T emporal distance: L2
Autocorrelation function
Learnt 10-comp Spectral Mix.
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14
frequencies0100Spectral distance: L1
Periodogram (w: hann)
Learnt 10-comp Spectral Mix.
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14
frequencies0100Spectral distance: L2
Periodogram (w: hann)
Learnt 10-comp Spectral Mix.
Figure 5: Example of matching for the temporal (top) and spectral (bottom) implementation of GVM for
the 10-component SM using the L1(left) andL2(right) metrics.
4https://github.com/Jakobovski/free-spoken-digit-dataset
12Published in Transactions on Machine Learning Research (04/2023)
With this comparison, we validate the intuition that for kernels with concentrated spectral information (such
as the spectral mixture) the spectral implementation of GVM is more robust to the metric and faces a less
challenging optimisation task.
5.5 E5: Fitting a 20-component spectral mixture (diﬀerent spectral metrics)
This experiment shows the eﬀect of diﬀerent spectral distances in the GVM estimates, also using an audio
signal from the Free Spoken Digit Dataset (diﬀerent from Experiment 4). Based on the promising perfor-
mance of the spectral implementation of GVM, we trained a 20-component SM, a kernel known to be diﬃcult
to train, and considered the spectral distances L1,L2,W1andW2(spectral); Itakura-Saito and KL were
unstable and left our of the comparison. Fig. 6 shows the results of the GVM: observe that under almost all
metrics, the 20-component spectral mixture matches the Periodogram (considered to be the ground truth
PSD in this case). The exception is W2which struggles to replicate the PSD peaks due to its objective of
averaging mass horizontally .
0.00 0.02 0.04 0.06 0.08
frequencies0100200300Spectral distance: L1
Periodogram (w: None)
Learnt 20-comp Spectral Mix.
0.00 0.02 0.04 0.06 0.08
frequencies0100200300Spectral distance: L2
Periodogram (w: None)
Learnt 20-comp Spectral Mix.
0.00 0.02 0.04 0.06 0.08
frequencies0100200300Spectral distance: W1
Periodogram (w: None)
Learnt 20-comp Spectral Mix.
0.00 0.02 0.04 0.06 0.08
frequencies0100200300Spectral distance: W2
Periodogram (w: None)
Learnt 20-comp Spectral Mix.
Figure 6: GVM matching Periodogram with a 20-component SM under diﬀerent spectral metrics.
5.6 E6: Learning spectral mixtures and parameter initialisation ( L2, multiple orders)
0 500 1000 150001K2K3K
4 components
not initialised
BNSE-init
SL-GP-init
1400 1450 1500600
400
0 500 1000 150001K2K3K
8 components
not initialised
BNSE-init
SL-GP-init
1400 1450 1500600
400
0 500 1000 150001K2K3K
12 components
not initialised
BNSE-init
SL-GP-init
1400 1450 1500600
400
0 500 1000 150001K2K3K
16 components
not initialised
BNSE-init
SL-GP-init
1400 1450 1500600
400
Figure 7: NLLs for the SE spectral mixtures (4,8,12 and 16 components) with diﬀerent initialisation strate-
gies.
In this experiment, GVM was implemented to ﬁnd the initial conditions of a GP with SM kernel (4, 8,
12 and 16 components) to a real-world 1800-point heart-rate signal from the MIT-BIH database5. We
considered the L2metric (spectral) minimised with Powell and then passed the hyperparameters to an
ML routine for 1500 iterations (using Adam with learning rate = 0.1). This methodology was compared
against the random initialisation and provided by MOGPTK based on Bayesian nonparametric spectral
5http://ecg.mit.edu/time-series/
13Published in Transactions on Machine Learning Research (04/2023)
estimation (BNSE) (Tobar, 2018). Fig. 8 ﬁrst shows the GVM approximations to the heart-rate PSD using
16-component spectral mixtures, for both both kernels.
Table 3: Computation times (secs) for ﬁtting SMs.
4-comp 8-comp 12-comp 16-comp
GVM init 2.6 7.6 10.9 23.9
BNSE init 74.6 68.9 72.0 74.1
ML 420.3 464.6 529.2 582.6
Fig. 7 shows NLL for the cases considered. Observe that: i) the non-initialised ML training becomes trapped
in local minima in all four cases, ii) the initialisation provided by GVM provides a dramatic reduction of
the NLL, even wrt to the BNSE initialisation, iii) the “elbow” at the beginning of the GVM-initialised case
suggests that the ML training could have run for a a few iterations (e.g., 100) and still reach a sound solution.
Table 3 shows the execution times and reveals the superiority of GVM also in computational time.
0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.0200
frequencies0200400Training time = 23.18 [secs]
Periodogram (w: None)
Learnt 16-comp Spectral Mix.
0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.0200
frequencies0200400Training time = 14.55 [secs]
Periodogram (w: None)
Learnt 16-comp Sinc Mix.
Figure 8: GVM approximations of the PSD of a 1800-sample heart-rate signal using 16-components SE (top)
and rectangular (bottom) mixtures. Training time shown above each plot.
5.7 E7: Learning an isotropic SE kernel (standard variogram, 5-dimensional inputs, L2)
Thoughourworkfocusesonthesingle-input-dimensioncase, therationalebehindtheproposedGVMmethod
is applicable to datasets of arbitrary input dimension, therefore, to motivate the use of GVM for multi-
input GPs, we present a minimal multi-input example. We considered a 5-dimensional GP with SE kernel
K(τ) =σ2exp(−1
2l2||τ||2) +σ2
noiseδt, and considered four sets of values for the hyperparameters. Notice that
we assumed that all 5 dimensions had the same scale parameter, this is the usual setting of the Variogram
methodinGeostatistics(Cressie,1993;Chiles&Delﬁner,1999). Foreachsetofhyperparameters, wesampled
1000 points and then implemented GVM with the L2(temporal) distance to learn the GP. Table 4 shows the
estimate error and standard deviation for four sets of hyperparameters averaged over 100 runs, from which
the applicability of GVM to address the isotropic multi-input case can be conﬁrmed. For illustration and
resemblance to the standard variogram method, Fig. 9 shows the case σ2= 5,l= 1, andσnoise = 1, where
the learnt hyperparameters were ˆσ2= 5.85,ˆl= 1.09, and ˆσnoise= 1.02.
0 1 2 3 4 5
Lag magnitude012345
Variogram for a 5-dimensional isotropic SE kernel
Empirical variance
Learned kernel (GVM)
Figure 9: Empirical covariance and ﬁtted covariance function via GVM. The data consisted of 1000 5-
dimensional datapoints.
14Published in Transactions on Machine Learning Research (04/2023)
Table 4: GVM applied to multi-input data for four runs of synthetic data using diﬀerent hyperparameters,
i.e.,σ2,l,σ2
noise. For each set of hyperparameter, n= 100runs were executed to calculate the mean and
standard deviation of the estimates (i.e., ˆσ2,ˆl,ˆσ2
noise).
Runσ2l σ2
noise ˆσ2 ˆl ˆσ2
noise
1 5 2 1 5.20±2.07 0.50 ±0.09 0.91 ±0.68
2 15 2 2 14.12±13.14 2.07 ±1.09 2.04 ±1.51
3 3 3 0.5 2.68±3.33 3.24 ±1.65 0.97 ±0.71
40.5 5 0.5 0.45±0.62 3.96 ±2.50 0.56 ±0.30
6 Conclusions
Bydirectminimisationofthediscrepancyamongcovariancefunctions, wehaveproposedanovelmethodology
to pretrain stationary Gaussian processes, which avoids computation of the (cubic cost) likelihood function.
The found hyperparameter values can then be passed to an ML-based training routine as initial conditions
to conclude the training of the model. Our approach, termed Generalised Variogram Method (GVM),
represents a critical improvement in terms of computational complexity: we have shown, both theoretically
and empirically, that for the particular case of the 2-Wasserstein spectral distance and location-scale PSDs,
GVM is convex and its solution can be computed in a single step. In experimental terms, we have shown
the following properties of GVM in the general case: i) applicability to multi-input data, ii) stability wrt
to diﬀerent ways of computing the Periodogram, iii) consistency under diﬀerent realisations of the GP
unlike ML, iv) computational eﬃciency wrt ML and sparse GPs, v) a realistic alternative to compute initial
conditions for ML resulting in considerable reduction of ML iterations, and lastly, vi) ability to train kernels
of large number of components that are challenging to train from random initial conditions using ML.
In thegeneral formulation of theproposed GVM(i.e., using either a temporal or spectral divergence) theonly
requirementinoursetupisstationarity, however, particularresultsinourproposalhavespeciﬁcrequirements.
First, when using a spectral divergence (the main novel contribution of our work), it is needed that the
kernels’ Fourier transform (i.e., the PSD) can be computed and is Lebesgue integrable so that DF(ˆSn,Sθ)in
equation 8 can be calculated; this condition is rather general and admits most stationary kernels used in the
literature with the exception of the white noise variance—see Sec.4.4. Second, for the exact case outlined
in Thm. 1 we require that the PSD belongs to a location-scale family, this includes standard covariance
functions such as the (single-component) spectral mixture, square exponential, sinc, and cosine. PSDs that
are not location-scale, such as mixtures of the above kernels, can too be dealt with spectral divergences,
however, the solution is not exact and it has to be computed using numerical optimisation methods from
equation 8. Third, for all other covariances (including the white noise one) we can use GVM with temporal
divergences, which only requires the kernel to be stationary and evaluated pointwise, to ﬁnd the solution via
equation 7.
Spectral kernels are a large class of covariance functions, therefore, the GVM is widely applicable in real-
world scenarios in audio, seismology, astronomy, fault diagnosis, ﬁnance and any other ﬁelds where repetitive
temporal patterns arise. In this sense, we hope that our work paves the way for further research in conceptual
and applied ﬁelds. In theoretical terms, we envision extensions towards non-stationary data using, e.g., time-
frequency representations or mini-batches. In practical terms, we aim to address the general-input-dimension
case and that our developed companion software will help others make use of GVM as and initialisation
method for ML or to directly ﬁnd the required hyperparameters.
15Published in Transactions on Machine Learning Research (04/2023)
Acknowledgments
We would like to thank the anonymous referees for their valuable questions and comments, which allowed
us to clarify the presentation of the paper and reﬁne our results.
Part of this work was developed when Felipe Tobar was a Visiting Researcher at the Institut de Recherche
en Informatique de Toulouse (IRIT).
We thank ﬁnancial support from Google; ANR LabEx CIMI (grant ANR-11-LABX-0040) within the French
StateProgramme“Investissementsd’Avenir”; Fondecyt-Regular1210606; ANID-BasalCenterforMathemat-
ical Modeling FB210005; ANID-Basal Advanced Center for Electrical and Electronic Engineering FB0008;
and CORFO/ANID International Centers of Excellence Program 10CEII-9157 Inria Chile, Inria Challenge
OcéanIA, STICAmSud EMISTRAL, CLIMATAmSud GreenAI and Inria associated team SusAIn.
References
S. Amari. Information Geometry and its Applications , volume 194. Springer, 2016.
F. Bachoc, F. Gamboa, J.-M. Loubes, and N. Venet. A Gaussian process regression model for distribution
inputs.IEEE Transactions on Information Theory , 64(10):6620–6637, 2018.
M. Basseville. Distance measures for signal processing and pattern recognition. Signal processing , 18(4):
349–369, 1989.
E. Bernton, P. Jacob, M. Gerber, and C. Robert. On parameter estimation with the Wasserstein distance.
Information and Inference: A Journal of the IMA , 8(4):657–676, 2019.
S. Bochner. Lectures on Fourier integrals , volume 42. Princeton University Press, 1959.
D. Burt, C.E. Rasmussen, and M. Van Der Wilk. Rates of convergence for sparse variational Gaussian
process regression. In International Conference on Machine Learning , pp. 862–871. PMLR, 2019.
E. Cazelles, A. Robert, and F. Tobar. The Wasserstein-Fourier distance for stationary time series. IEEE
Transactions on Signal Processing , 69:709–721, 2021.
J. P. Chiles and P. Delﬁner. Geostatistics, Modelling Spatial Uncertainty . Wiley, 1999.
C. Cortes, M. Mohri, and A. Rostamizadeh. Algorithms for learning kernels based on centered alignment.
Journal of Machine Learning Research , 13(28):795–828, 2012.
N. Cressie. Statistics for Spatial Data . Wiley, 1993.
N. Cristianini, J. Shawe-Taylor, A. Elisseeﬀ, and J. Kandola. On kernel-target alignment. In Advances in
Neural Information Processing Systems , volume 14. MIT Press, 2001.
A. Damianou and N.D. Lawrence. Deep Gaussian processes. In Proc. of the International Conference on
Artiﬁcial Intelligence and Statistics , volume 31 of PMLR, pp. 207–215, 2013.
T. de Wolﬀ, A. Cuevas, and F. Tobar. MOGPTK: The multi-output Gaussian process toolkit. Neurocom-
puting, 424:49 – 53, 2021.
T. Henderson and J. Solomon. Audio transport: A generalized portamento via optimal transport. arXiv
preprint arXiv:1906.06763 , 2019.
J. Hensman, N. Fusi, and N.D. Lawrence. Gaussian processes for big data. In Proc. of the Conference on
Uncertainty in Artiﬁcial Intelligence , pp. 282–290, 2013.
M. Hoﬀman and Y.-A. Ma. Black-box variational inference as distilled Langevin dynamics. In Proc. of the
International Conference on Machine Learning , pp. 9267–9277, 2020.
16Published in Transactions on Machine Learning Research (04/2023)
F. Itakura. Analysis synthesis telephony based on the maximum likelihood method. In The 6th International
Congress on Acoustics , pp. 280–292, 1968.
V. Lalchand, W. Bruinsma, D.R. Burt, and C.E. Rasmussen. Sparse Gaussian process hyperparameters:
Optimize or integrate? In Advances in Neural Information Processing Systems , 2022.
S. Liu, X. Sun, P.J. Ramadge, and R.P. Adams. Task-agnostic amortized inference of Gaussian process
hyperparameters. In Advances in Neural Information Processing Systems , volume 33, pp. 21440–21452.
Curran Associates, Inc., 2020.
A. Mallasto and A. Feragen. Learning from uncertain curves: The 2-Wasserstein metric for Gaussian
processes. In Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc.,
2017.
V. Masarotto, V. M Panaretos, and Y. Zemel. Procrustes metrics on covariance operators and optimal
transportation of Gaussian processes. Sankhya A , 81(1):172–213, 2019.
G. Peyré and M. Cuturi. Computational optimal transport: With applications to data science. Foundations
and Trends ®in Machine Learning , 11(5-6):355–607, 2019.
S. Popescu, D. Sharp, J. Cole, and B. Glocker. Hierarchical Gaussian processes with Wasserstein-2 kernels,
2022. arXiv preprint 2010.14877.
J. Quinonero-Candela and C.E. Rasmussen. A unifying view of sparse approximate Gaussian process regres-
sion.The Journal of Machine Learning Research , 6:1939–1959, 2005.
C.E. Rasmussen and C.K.I. Williams. Gaussian Processes for Machine Learning . MIT Press, 2005.
G. Rios and F. Tobar. Learning non-Gaussian time series using the Box-Cox Gaussian process. In Interna-
tional Joint Conference on Neural Networks , pp. 1–8, 2018.
H. Salimbeni and M. Deisenroth. Doubly stochastic variational inference for deep Gaussian processes. In
Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017.
A. Schuster. The periodogram of magnetic declination as obtained from the records of the Greenwich
Observatory during the years 1871–1895. Trans. Cambridge Philos. Soc , 18:107–135, 1900.
E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural
Information Processing Systems , volume 18. MIT Press, 2006.
P. Stoica and R. Moses. Spectral analysis of signals . Pearson Prentice Hall Upper Saddle River, NJ, 2005.
M. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Proceedings of the
Twelth International Conference on Artiﬁcial Intelligence and Statistics , volume 5, pp. 567–574, 2009.
F. Tobar. Bayesian nonparametric spectral estimation. In Advances in Neural Information Processing
Systems 31 , pp. 10148–10158, 2018.
F. Tobar, T. Bui, and R. Turner. Learning stationary time series using Gaussian processes with nonpara-
metric kernels. In Advances in Neural Information Processing Systems 28 , pp. 3483–3491, 2015.
M. van der Wilk, ST John, A. Artemev, and J. Hensman. Variational Gaussian process models without
matrix inverses. In Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference ,
volume 118, pp. 1–9. PMLR, 08 Dec 2020.
M. Vetterli, J. Kovačević, and V.K. Goyal. Foundations of Signal Processing . Cambridge University Press,
2014.
C. Villani. Optimal transport: old and new , volume 338. Springer, 2009.
17Published in Transactions on Machine Learning Research (04/2023)
K. Wang, G. Pleiss, J. Gardner, S. Tyree, K.Q. Weinberger, and A.G. Wilson. Exact Gaussian processes
on a million data points. In Advances in Neural Information Processing Systems , volume 32. Curran
Associates, Inc., 2019.
A. Wilson and R. Adams. Gaussian process kernels for pattern discovery and extrapolation. In Proceedings
of the 30th International Conference on Machine Learning , volume 28, pp. 1067–1075. PMLR, 2013.
A Background: Fourier analysis of continuous-time stochastic processes
We present a brief description of the elements of Fourier analysis used in our proposal. For a more in-depth
introduction of the subject, the reader is referred to (Stoica & Moses, 2005) and (Vetterli et al., 2014).
Letusconsiderastochasticprocessover Rndenoted (yt)t∈Rn,n∈N. Sincetheusualconditionofstationarity
might be restrictive in some cases, we consider the following weaker version of stationarity.
Deﬁnition 3 (Wide-sense stationarity) .The stochastic process y is wide-sense stationary (WSS) if its mean
function is constant and its autocorrelation only depends on the temporal diﬀerence. That is,
E[yt] =µ(t) =µ (14)
E[yt1yt2] =c(t1,t2) =c(t1−t2). (15)
Though strict stationarity implies WSS, the implication does not hold in the opposite direction. However, for
Gaussian processes (GP), whose distribution is fully determined by the ﬁrst two moments, strict stationarity
and WSS are equivalent conditions.
Deﬁnition 4 (Power spectral density) .Letybe a WSS stochastic process with an absolutely integrable
correlation function c(τ) =E[ytyt−τ]. The Fourier transform of c(·)given by
S(ξ) =/integraldisplay∞
−∞c(τ)e−j2πξτdτ (16)
is called power spectral density .
Observe that, if both candSsatisfy the conditions for the inversion of the Fourier transform, then candS
areFourier pairs , meaning that we also have
c(τ) =/integraldisplay∞
−∞S(ξ)ej2πξτdξ. (17)
Equations 16 and 17 are a consequence of the Wiener-Khinchin Theorem (Vetterli et al., 2014, p. 292), which
relates autocorrelation structure of a WSS process with its distribution of energy across frequencies. This
result is instrumental in the construction of GP: under the observation that GPs are uniquely determined
by their autocorrelation (or covariance) function, the design of a GP can be conveniently performed in the
frequency domain by parametrising the PSD. Furthermore, recall that for zero-mean GPs the autocorrelation
and autocovariance functions coincide; thus, we refer to the latter as the covariance kernel.
There are well-known pairs of kernels and PSDs that follow from the above observation. Figure 10 illustrates
ﬁve cases, showing the covariance kernels, their PSD and a sample of a GP with the corresponding kernel.
In practice, we regard data as realisations of a stochastic process and we need to estimate the covariance
or the PSDs from the available datasets. In the signal processing community, these quantities are usually
estimated in a nonparametric fashion. For instance, the sample covariance in Deﬁnition 1 is an estimate of
the covariance, which in the case of evenly-spaced dataset {y∆,...,yN∆}takes the standard form
ˆc(k∆) =1
N−kN−k/summationdisplay
n=1yn∆y(n+k)∆. (18)
18Published in Transactions on Machine Learning Research (04/2023)
Time-10 -5 0 5 100.20.40.60.8Square exponential: KSE=σ2exp/parenleftbig−1
2l2t2/parenrightbig
Frequency-1 -0.5 0 0.5 1246PSDSE=|F{KSE}(ω)|2=/vextendsingle/vextendsingleσ2l√
2πexp/parenleftbig
−2l2π2ω2/parenrightbig/vextendsingle/vextendsingle2
Time-50 0 50-202y(t)∼N(0,KSE)
Time-10 -5 0 5 1000.51Rational quadratic: KRQ=σ2/parenleftbig
1+1
2l2αt2/parenrightbig−α
Frequency-1 -0.5 0 0.5 102040PSDRQ=|F{KRQ}(ω)|2- numerical
Time-50 0 50-10123y(t)∼N(0,KRQ)
Time-10 -5 0 5 100.511.52Periodic: KPER=σ2exp(kcos(ω0t))
Frequency-1 -0.5 0 0.5 1102030PSDPER=|F{KPER}(ω)|2- numerical
Time-15 -10 -5 0 5 10 15-4-202y(t)∼N(0,KPER)
Time-10 -5 0 5 10-101Cosine: Kcos=σ2cos(ω0t)
Frequency-1 -0.5 0 0.5 100.10.2PSDcos=|F{Kcos}(ω)|2=/vextendsingle/vextendsingle1
2σ2δ±ω0/vextendsingle/vextendsingle2
Time-15 -10 -5 0 5 10 15-101y(t)∼N(0,Kcos)
Time-10 -5 0 5 10-0.500.51Spectral mixture: KSM=/summationtextM
i=1σ2
ie−2π2t2l2
icos(2πtµi)
Frequency-1 -0.5 0 0.5 151015PSDSM=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtextM
i=1σ2
i√
2πl2
iexp/parenleftBig
−1
2l2
i(t±µi)2/parenrightBig/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
Time-15 -10 -5 0 5 10 15-1012y(t)∼N(0,KSM)
Figure 10: Illustration of the relationship between covariance and spectral representations of GPs. Left to
right: kernel, PSD and a GP sample. Top to bottom: square exponential (SE), rational quadratic (RQ),
periodic (Per), cosine (cos) and spectral mixture (SM) kernels. For the RQ and Per kernels, the PSD was
computed numerically using the discrete time Fourier transform.
TheproblemofrecoveringthePSDfromaﬁnitecollectionofobservations {yt1,...,ytN}isreferredtoas spec-
tral estimation and the classic nonparametric method to perform this estimation is called the Periodogram.
This technique builds on the observation that, under mild assumptions, the deﬁnition of the PSD in equation
16 is equivalent to
S(ω) = lim
T→∞E
1
2T/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplayT
−Tyte−jωtdt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
. (19)
Therefore, when only a ﬁnite dataset of observations is available, the limit and the expectation can be ignored
in the above expression thus yielding the natural sample estimate of the PSD given by
ˆS(ω) =1
N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleN/summationdisplay
i=1yte−jωt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
, (20)
whereNin the denominator replaced 2Tas normaliser.
Equation 20 is known as the Periodogram , a widely used method for estimating the PSD of a WSS stochastic
process introduced by Schuster (1900). Today, however, the concept of Periodogram refers to a wider class
of techniques for spectral estimation that build on the original formulation but at the same time address
some of the known drawbacks related to the biasedness and large variance of the Periodogram.
TwowidelyusedextensionsofthePeriodogramaretheBartlettandtheWelchmethods. Theformeroperates
by splitting the data into segments and computing the standard Periodogram over each one of them, to then
average over all computed Periodograms with the aim to reduce the noise in the estimate. The latter follows
the same concept but also multiplies each segment by a windowso as to mitigate the eﬀect of the border of
the segments in the estimation. Usual choices for the windows are the Hann and Hamming functions.
19Published in Transactions on Machine Learning Research (04/2023)
B Deﬁnition of distances and divergences considered
For two functions f1andf2, we have the following general distances:
•1-Euclidean : L1(f1,f2) =/integraltext
R|f1(ξ)−f2(ξ)|dξ
•2-Euclidean : L2(f1,f2) =/integraltext
R(f1(ξ)−f2(ξ))2dξ
Furthermore, when f1andf2are densities with quantile functions Q1andQ2respectively, we have the
additional divergences:
•1-Wasserstein (Villani, 2009; Peyré & Cuturi, 2019) :
W1(f1,f2) =/integraldisplay1
0|Q1(p)−Q2(p)|dp
•2-Wasserstein (Villani, 2009; Peyré & Cuturi, 2019) :
W2(f1,f2) =/integraldisplay1
0(Q1(p)−Q2(p))2dp
•Kullback-Leibler :
DKL(f1/bardblf2) =/integraldisplay
Rlog/parenleftbiggf1(ξ)
f2(ξ)/parenrightbigg
f1(ξ)dξ
•Itakura-Saito (Itakura, 1968) :
DIS(f1/bardblf2) =/integraldisplay
R/parenleftbiggf1(ξ)
f2(ξ)−logf1(ξ)
f2(ξ)−1/parenrightbigg
dξ
•Bregman divergences (Amari, 2016) : for a function G:R→Rthat is diﬀerentiable and strictly
convex,
DG(f1,f2) =G(f1)−G(f2)−/angbracketleft∇G(f2),f1−f2/angbracketright
Here, we have assumed that both f1andf2integrate unity, in the cases where this condition is not met, the
densities can be normalised before computing the distance.
C Proofs
C.1 Convexity of spectral loss for W2and location-scale family
Proof of Theorem 1. We recall that
W2
2(S,Sµ,σ) =/integraldisplay1
0(Qµ,σ(p)−Q(p))dp. (21)
From a direct application of the rule of diﬀerentiation under the integral sign, we obtain the gradient for the
location-scale family:
∇µ,σW2
2(S,Sµ,σ) = 2/integraldisplay1
0(Qµ,σ(p)−Q(p))∇µ,σQµ,σ(p)dp (22)
= 2/integraldisplay1
0(µ+σQ0,1(p)−Q(p))∇µ,σ(µ+σQ0,1(p))dp
= 2/integraldisplay1
0(µ+σQ0,1(p)−Q(p))/parenleftbigg1
Q0,1(p)/parenrightbigg
dp.
20Published in Transactions on Machine Learning Research (04/2023)
The Hessian for the location-scale family:
Hµ,σW2
2(S,Sµ,σ) = 2/integraldisplay1
0/parenleftbigg1Q0,1(p)
Q0,1(p)Q0,1(p)2/parenrightbigg
dp. (23)
The determinant of the Hessian (via Jensen’s inequality):
|H|/2 =/integraldisplay1
0Q0,1(p)2dp−/parenleftbigg/integraldisplay1
0Q0,1(p)dp/parenrightbigg2
>/integraldisplay1
0Q0,1(p)2dp−/integraldisplay1
0Q0,1(p)2dp= 0, (24)
where the inequality is strict due to the strict convexity of (·)2.
Therefore, the ﬁrst order conditions are given by:
/integraldisplay1
0(µ+σQ0,1(p)−Q(p))dp= 0⇐⇒µ=/integraldisplay1
0(Q(p)−σQ0,1(p))dp=/integraldisplay1
0Q(p)dp (25)
and/integraldisplay1
0(µ+σQ0,1(p)−Q(p))Q0,1(p)dp= 0 (26)
⇐⇒σ=/integraltext1
0(Q(p)−µ)Q0,1(p)dp
/integraltext1
0Q0,1(p)2dp=/integraltext1
0Q(p)Q0,1(p)dp
/integraltext1
0Q2
0,1(p)dp,(27)
where in the last expression we have used the fact that the location of the prototype S0,1is zero and so is
its mean, meaning that if x ∼S0,1we can write/integraltext1
0µQ0,1(p)dp=µEx∼S0,1[x] = 0.
C.2 Learning from data ( W2distance and location-scale family)
Proof of Proposition 1. First, recall that in the location-scale family θ= (µ,σ). We denote by Qand ˆQn
the respective quantile functions of Sand ˆSn. Then, following the solutions in Theorem 1 and Jensen’s
inequality we can compute the following upper bound for the location parameter µ∗:
(E|µ∗−µ∗
n|)2≤E|µ∗−µ∗
n|2=E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay1
0Q(p)dp−/integraldisplay1
0ˆQn(p)dp/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
≤E/bracketleftbigg/integraldisplay1
0|Q(p)−ˆQn(p)|2dp/bracketrightbigg
=E[W2
2(S,ˆSn)],
and by hypothesis EW2
2(S,ˆSn)→0.
Similarly, now using Hölder’s inequality, we obtain the following bound for the scale parameter σ∗:
E|σ∗−σ∗
n|=E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay1
0Q(p)Q0,1(p)dp−/integraldisplay1
0ˆQn(p)Q0,1(p)dp/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤E/bracketleftbigg/integraldisplay1
0|(Q(p)−ˆQn(p))Q0,1(p)|dp/bracketrightbigg
≤E/bracketleftBigg/parenleftbigg/integraldisplay1
0|Q(p)−ˆQn(p)|2dp/parenrightbigg1
2/bracketrightBigg/parenleftbigg/integraldisplay1
0|Q0,1(p)|2dp/parenrightbigg1
2
=E[W2(S,ˆSn)]/parenleftbigg/integraldisplay1
0|Q0,1(p)|2dp/parenrightbigg1
2
,
which tends to 0by again Jensen’s inequality, (E[W2(S,ˆSn)])2≤E[W2
2(S,ˆSn)]→0.
21Published in Transactions on Machine Learning Research (04/2023)
D Code
Wehavedevelopedashortself-containedtoolboxwhichisavailablein https://github.com/GAMES-UChile/
Generalised-Variogram-Method . The purpose of the code is to facilitate the use of the proposed method by
the community and in particular to replicate all our results. For the reader’s convenience, Jupyter Notebook
Exp0_minimal_ex.ipynb showing a minimal example of the toolbox implementation is presented here.
Minimal working example of provided code
Exp0 _minimal_ ex
June 3, 2021
[1]: #general imports
import numpy as np
import matplotlib .pyplot as plt
#our package
from waflgp import*
import utils
[2]: #load data
signal=np.loadtxt( 'Data/hr2.txt ')
[3]: #instantiate model, sum of 16 Gaussians
q=16
gp=waflgp(space_output =signal, aim ='learning ', kernel ='qSM')#Spectral Mix
#set frequencies (optional)
freqs=np.linspace( 0,0.02,2000)
gp.set_freqs(freqs)
[4]: #train with periodogram, L2 metric and q components
gp.train_WL(method ='periodogram ', metric ='L2', order=q)
#plot Periodogram and best PSD fit
gp.plot_psd(title =f'Minimal example ')
Optimization terminated successfully.
Current function value: 0.002876
Iterations: 33
Function evaluations: 21261
L2-ok
1
22Published in Transactions on Machine Learning Research (04/2023)
E Additional ﬁgures for E2
Experiment E2 showed the sensibility of GVM to the choice of Periodogram method and window for two
kernels. Figures 11 and 12 presents all the ﬁgures corresponding to the estimates in Table 2 for the Exp-cos
and Sinc kernel respectively.
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07
frequencies050100150Learnt parameters are loc: 0.0501, scale: 0.0089
Periodogram (w: None)
Learnt kernel
Ground truth Exp-cos kernel
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07
frequencies0100Learnt parameters are loc: 0.0515, scale: 0.0076
Periodogram (w: hann)
Learnt kernel
Ground truth Exp-cos kernel
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07
frequencies0100Learnt parameters are loc: 0.0514, scale: 0.0077
Periodogram (w: hamming)
Learnt kernel
Ground truth Exp-cos kernel
0.00 0.02 0.04 0.06 0.08 0.10
frequencies02040Learnt parameters are loc: 0.0500, scale: 0.0113
Bartlett (w: None, bins: 10)
Learnt kernel
Ground truth Exp-cos kernel
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07
frequencies02040Learnt parameters are loc: 0.0495, scale: 0.0101
Bartlett (w: hann, bins: 10)
Learnt kernel
Ground truth Exp-cos kernel
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07
frequencies02040Learnt parameters are loc: 0.0495, scale: 0.0099
Bartlett (w: hamming, bins: 10)
Learnt kernel
Ground truth Exp-cos kernel
0.00 0.02 0.04 0.06 0.08
frequencies02040Learnt parameters are loc: 0.0504, scale: 0.0100
Welch (w: None, bins: 10)
Learnt kernel
Ground truth Exp-cos kernel
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07
frequencies02040Learnt parameters are loc: 0.0506, scale: 0.0093
Welch (w: hann, bins: 10)
Learnt kernel
Ground truth Exp-cos kernel
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07
frequencies02040Learnt parameters are loc: 0.0506, scale: 0.0091
Welch (w: hamming, bins: 10)
Learnt kernel
Ground truth Exp-cos kernel
Figure 11: Exp-cos kernel.
23Published in Transactions on Machine Learning Research (04/2023)
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07
frequencies0100200Learnt parameters are loc: 0.0506, scale: 0.0108
Periodogram (w: None)
Learnt kernel
Ground truth Sinc kernel
0.00 0.01 0.02 0.03 0.04 0.05
frequencies0100200Learnt parameters are loc: 0.0505, scale: 0.0071
Periodogram (w: hann)
Learnt kernel
Ground truth Sinc kernel
0.00 0.01 0.02 0.03 0.04 0.05
frequencies0100200Learnt parameters are loc: 0.0505, scale: 0.0071
Periodogram (w: hamming)
Learnt kernel
Ground truth Sinc kernel
0.00 0.02 0.04 0.06 0.08 0.10 0.12
frequencies050100Learnt parameters are loc: 0.0506, scale: 0.0244
Bartlett (w: None, bins: 10)
Learnt kernel
Ground truth Sinc kernel
0.00 0.01 0.02 0.03 0.04 0.05 0.06
frequencies050100Learnt parameters are loc: 0.0504, scale: 0.0210
Bartlett (w: hann, bins: 10)
Learnt kernel
Ground truth Sinc kernel
0.00 0.01 0.02 0.03 0.04 0.05 0.06
frequencies050100Learnt parameters are loc: 0.0504, scale: 0.0195
Bartlett (w: hamming, bins: 10)
Learnt kernel
Ground truth Sinc kernel
0.00 0.02 0.04 0.06 0.08
frequencies050100Learnt parameters are loc: 0.0502, scale: 0.0168
Welch (w: None, bins: 10)
Learnt kernel
Ground truth Sinc kernel
0.00 0.01 0.02 0.03 0.04 0.05 0.06
frequencies050100Learnt parameters are loc: 0.0505, scale: 0.0135
Welch (w: hann, bins: 10)
Learnt kernel
Ground truth Sinc kernel
0.00 0.01 0.02 0.03 0.04 0.05 0.06
frequencies050100Learnt parameters are loc: 0.0505, scale: 0.0127
Welch (w: hamming, bins: 10)
Learnt kernel
Ground truth Sinc kernel
Figure 12: Sinc kernel.
24