Published in Transactions on Machine Learning Research (05/2023)
A Stochastic Proximal Polyak Step Size
Fabian Schaipp fabian.schaipp@tum.de
Department of Mathematics
Technical University of Munich
Robert M. Gower rgower@ﬂatironinstitute.org
Center for Computational Mathematics
Flatiron Institute, New York
Michael Ulbrich m.ulbrich@tum.de
Department of Mathematics
Technical University of Munich
Reviewed on OpenReview: https: // openreview. net/ forum? id= jWr41htaB3
Abstract
Recently, the stochastic Polyak step size ( SPS) has emerged as a competitive adaptive step
size scheme for stochastic gradient descent. Here we develop ProxSPS, aproximal variant
ofSPSthat can handle regularization terms. Developing a proximal variant of SPSis par-
ticularly important, since SPSrequires a lower bound of the objective function to work well.
When the objective function is the sum of a loss and a regularizer, available estimates of
a lower bound of the sum can be loose. In contrast, ProxSPS only requires a lower bound
for the loss which is often readily available. As a consequence, we show that ProxSPS is
easier to tune and more stable in the presence of regularization. Furthermore for image
classiﬁcation tasks, ProxSPS performs as well as AdamWwith little to no tuning, and results
in a network with smaller weight parameters. We also provide an extensive convergence
analysis for ProxSPS that includes the non-smooth, smooth, weakly convex and strongly
convex setting.
1 Introduction
Consider problems of the form
min
x∈Rnf(x), f(x) :=EP[f(x;S)] =/integraldisplay
Sf(x;s)dP(s), (1)
whereSis a sample space (or sample set). Formally, we can see Sas a random variable mapping to Sand
P(s)as the associated probability measure. Let us assume that for each s∈S, the function f(·;s) :Rn→R
is locally Lipschitz and hence possesses the Clarke subdiﬀerential ∂f(·;s)(Clarke, 1983). Problems of form
(1) arise in machine learning tasks where Sis the space of available data points (Bottou et al., 2018). An
eﬃcient method for such problems is stochastic (sub)gradient descent (Robbins & Monro, 1951; Bottou,
2010; Davis & Drusvyatskiy, 2019), given by
xk+1=xk−αkgk, gk∈∂f(xk;Sk),whereSk∼P. (SGD)
Moreover, we will also consider the composite problem
min
x∈Rnψ(x), ψ (x) :=f(x) +ϕ(x), (2)
1Published in Transactions on Machine Learning Research (05/2023)
whereϕ:Rn→R∪{∞}is a proper, closed, and convex regularization function. In practical situations,
the expectation in the objective function fis typically approximated by a sample average over N∈Ndata
points. We formalize this special case with
S={s1,...,sN}, P(si) =1
N, fi:=f(·;si)i= 1,...,N. (ER)
In this case, problem (1) becomes the empirical risk minimization problem
min
x∈Rn1
NN/summationdisplay
i=1fi(x).
1.1 Background and Contributions
Polyak step size. For minimizing a convex, possibly non-diﬀerentiable function f, Polyak (1987, Chapter
5.3) proposed
xk+1=xk−αkgk, αk=f(xk)−minf
/bardblgk/bardbl2, gk∈∂f(xk)\{0}.
This particular choice of αk, requiring the knowledge of minf, has been subsequently called the Polyak step
sizefor the subgradient method. Recently, Berrada et al. (2019); Loizou et al. (2021); Orvieto et al. (2022)
adapted the Polyak step size to the stochastic setting: consider the ( ER) case and assume that each fiis
diﬀerentiable and that a lower bound C(si)≤infxfi(x)is known for all i∈[N]. The method proposed by
(Loizou et al., 2021) is
xk+1=xk−min/braceleftBig
γb,fik(xk)−C(sik)
c/bardbl∇fik(xk)/bardbl2/bracerightBig
∇fik(xk), (SPSmax)
with hyper-parameters c,γb>0and where in each iteration ikis drawn from{1,...,N}uniformly at
random. It is important to note that the initial work (Loizou et al., 2021) used C(si) = inffi; later, Orvieto
et al. (2022) established theory for ( SPSmax) for the more general case of C(si)≤infxfi(x)and allowing for
mini-batching. Other works analyzed the Polyak step size in the convex, smooth setting (Hazan & Kakade,
2019) and in the convex, smooth and stochastic setting (Prazeres & Oberman, 2021). Further, the stochastic
Polyak step size is closely related to stochastic model-based proximal point (Asi & Duchi, 2019) as well as
stochastic bundle methods (Paren et al., 2022).
Contribution. We propose a proximal version of the stochastic Polyak step size, called ProxSPS, which
explicitly handles regularization functions. Our proposal is based crucially on the fact that the stochastic
Polyak step size can be motivated with stochastic proximal point for a truncated linear model of the ob-
jective function (we explain this in detail in Section 3.1). Our method has closed-form updates for squared
/lscript2-regularization. We provide theoretical guarantees for ProxSPS for any closed, proper, and convex regular-
ization function (including indicator functions for constraints). Our main results, Theorem 7 and Theorem 8,
alsogivenewinsightsfor SPSmax, inparticularshowingexactconvergenceforconvexandnon-convexsettings.
Lower bounds and regularization. Methods such as SPSmaxneed to estimate a lower bound C(s)for each
loss function f(·;s). Though infxf(x;s)can be precomputed in some restricted settings, in practice the lower
boundC(s) = 0is used for non-negative loss functions.1The tightness of the choice C(s)is further reﬂected
in the constant σ2:= minf−EP[C(S)], which aﬀects the convergence guarantees of SPSmax(Orvieto et al.,
2022).
Contribution. For regularized problems (2) and if ϕis diﬀerentiable, the current proposal of SPSmaxwould
addϕto every loss function f(·;s). In this case, for non-negative regularization terms, such as the squared
/lscript2-norm, the lower bound C(s) = 0is always loose. Indeed, if ϕ≥0, then infx∈Rn(f(x;s) +ϕ(x))≥
infx∈Rnf(x;s)and this inequality is strict in most practical scenarios. For our proposed method ProxSPS,
1See for instance https://github.com/IssamLaradji/sps .
2Published in Transactions on Machine Learning Research (05/2023)
wenowneedonlyestimatealowerboundfortheloss f(x;s)andnotforthecompositefunction f(x;s)+ϕ(x).
Further, ProxSPS decouples the adaptive step size for the gradient of the loss from the regularization (we
explain this in detail in Section 4.1 and Fig. 1).
Proximal and adaptive methods. The question on how to handle regularization terms has also been
posed for other families of adaptive methods. For Adam(Kingma & Ba, 2015) with /lscript2-regularization it
has been observed that it generalizes worse and is harder to tune than AdamW(Loshchilov & Hutter, 2019)
which uses weight decay. Further, AdamWcan be seen as an approximation to a proximal version of Adam
(Zhuang et al., 2022).2On the other hand, Loizou et al. (2021) showed that – without regularization –
default hyperparameter settings for SPSmaxgive very encouraging results on matrix factorization and image
classiﬁcation tasks. This is promising since it suggests that SPSmaxis anadaptive method, and can work
well across varied tasks without the need for extensive hyperparameter tuning.
Contribution. Weshowthatby handling /lscript2-regularizationusingaproximalstep, ourresulting ProxSPS isless
sensitive to hyperparameter choice as compared to SPSmax. This becomes apparent in matrix factorization
problems, where ProxSPS converges for a much wider range of regularization parameters and learning rates,
while SPSmaxis more sensitive to these settings. We also show similar results for image classiﬁcation over
theCIFAR10 andImagenet32 dataset when using a ResNetmodel, where, compared to AdamW, our method
is less sensitive with respect to the regularization parameter.
The remainder of our paper is organized as follows: we will ﬁrst recall how the stochastic Polyak step size,
in the case of ϕ= 0, can be derived using the model-based approach of (Asi & Duchi, 2019; Davis &
Drusvyatskiy, 2019) and how this is connected to SPSmax. We then derive ProxSPS based on the connection
to model-based methods, and present our theoretical results, based on the proof techniques in (Davis &
Drusvyatskiy, 2019).
2 Preliminaries
2.1 Notation
Throughout, we will write Einstead of EP. For any random variable X(s), we denote E[X(S)] :=/integraltext
SX(s)dP(s). We denote (·)+:= max{·,0}. We write ˜Owhen we drop logarithmic terms in the O-notation,
e.g. ˜O(1
K) =O(ln(1+K)
K).
2.2 General assumptions
Throughout the article, we assume the following:
Assumption 1. It is possible to generate inﬁnitely many i.i.d. realizations S1,S2,...fromS.
Assumption 2. For everys∈S,infxf(x;s)is ﬁnite and there exists C(s)satisfyingC(s)≤infxf(x;s).
In many machine learning applications, non-negative loss functions are used and thus we can satisfy the
second assumption choosing C(s) = 0for alls∈S.
2.3 Convex analysis
Leth:Rn→Rbe convex and α>0. The proximal operator is given by
proxαh(x) := arg min
yh(y) +1
2α/bardbly−x/bardbl2.
Further, the Moreau envelope is deﬁned by envα
h(x) := min yh(y) +1
2α/bardbly−x/bardbl2, and its derivative is
∇envα
h(x) =1
α(x−proxαh(x))(Drusvyatskiy & Paquette, 2019, Lem. 2.1). Moreover, due to the opti-
mality conditions of the proximal operator, if h∈C1then
ˆx= proxαh(x) =⇒/bardbl∇h(ˆx)/bardbl=α−1/bardblx−ˆx/bardbl=/bardbl∇envα
h(x)/bardbl. (3)
2ForSGDtreating/lscript2-regularization as a part of the loss can be seen to be equivalent to its proximal version (cf. Appendix C).
3Published in Transactions on Machine Learning Research (05/2023)
Davis & Drusvyatskiy (2019) showed how to use the Moreau envelope as a measure of stationarity: if
/bardbl∇envα
h(x)/bardblis small, then xis close to ˆxandˆxis an almost stationary point of h. Formally, the gradient
of the Moreau envelope can be related to the gradient mapping (cf. (Drusvyatskiy & Paquette, 2019, Thm.
4.5) and Lemma 11).
We say that a function h:Rn→RisL-smooth if its gradient is L–Lipschitz, that is
/bardbl∇h(x)−∇h(y)/bardbl ≤L/bardblx−y/bardbl,∀x,y∈Rn. (4)
IfhisL-smooth, then
h(y)≤h(x) +/angbracketleft∇h(x),y−x/angbracketright+L
2/bardbly−x/bardbl2for allx,y,∈Rn.
A functionh:Rn→Risρ–weakly convex for ρ≥0ifh+ρ
2/bardbl·/bardbl2is convex. Any L–smooth function is weakly
convex with parameter less than or equal to L(Drusvyatskiy & Paquette, 2019, Lem. 4.2). The above results
on the proximal operator and Moreau envelope can immediately be extended to hbeingρ–weakly convex if
α∈(0,ρ−1), since then h+ρ
2/bardbl·/bardbl2is convex.
If we assume that each f(·;s)isρs-weakly convex for ρs≥0, then applying (Bertsekas, 1973, Lem. 2.1)
to the convex function f(·;s) +ρs
2/bardbl·/bardbl2yields that f+ρ
2/bardbl·/bardbl2is convex and thus fisρ–weakly convex for
ρ:=E[ρS]. In particular, fis convex if each f(·;s)is assumed to be convex. For a weakly convex function h,
we denote with ∂hthe regular subdiﬀerential (cf. (Davis & Drusvyatskiy, 2019, section 2.2) and (Rockafellar
& Wets, 1998, Def. 8.3)).
3 The unregularized case
For this section, consider problems of form (1), i.e. no regularization term ϕis added to the loss f.
3.1 A model-based view point
Many classical methods for solving (1) can be summarized by model-based stochastic proximal point: in
each iteration, a model fx(·;s)is constructed approximating f(·;s)locally around x. WithSk∼Pbeing
drawn at random, this yields the update
xk+1= arg min
y∈Rnfxk(y;Sk) +1
2αk/bardbly−xk/bardbl2. (5)
The theoretical foundation for this family of methods has been established by Asi & Duchi (2019) and Davis
& Drusvyatskiy (2019). They give the following three models as examples:
(i)Linear:fx(y;s) :=f(x;s) +/angbracketleftg,y−x/angbracketrightwithg∈∂f(x;s).
(ii)Full:fx(y;s) :=f(y;s).
(iii)Truncated: fx(y;s) := max{f(x;s) +/angbracketleftg,y−x/angbracketright,infz∈Rnf(z;s)}whereg∈∂f(x;s).
It is easy to see that update (5) for the linear model is equal to ( SGD) while the full model results in the
stochastic proximal point method. For the truncated model , (5) results in the update
xk+1=xk−min/braceleftBig
αk,f(xk;Sk)−infz∈Rnf(z;Sk)
/bardblgk/bardbl2/bracerightBig
gk, gk∈∂f(xk,Sk). (6)
More generally, one can replace the term infx∈Rnf(x;Sk)with an arbitrary lower bound of f(·;Sk)(cf.
Lemma 10). The model-based stochastic proximal point method for the truncated model is given in Algo-
rithm 1. The connection between the truncated model and the method depicted in (6) is not a new insight
and has been pointed out in several works (including (Asi & Duchi, 2019; Loizou et al., 2021) and (Berrada
4Published in Transactions on Machine Learning Research (05/2023)
et al., 2019, Prop. 1)). For simplicity, we refer to Algorithm 1 as SPSthroughout this article. However, it
should be pointed out that this acronym (and variations of it) have been used for stochastic Polyak-type
methods in slightly diﬀerent ways (Loizou et al., 2021; Gower et al., 2021).
Algorithm 1 SPS
Require:x0∈Rn, step sizes αk>0.
fork= 0,1,2,...,K−1do
1. SampleSkand setCk:=C(Sk).
2. Choosegk∈∂f(xk;Sk). Ifgk= 0, setxk+1=xk. Otherwise, set
xk+1=xk−γkgk, γk= min/braceleftBig
αk,f(xk;Sk)−Ck
/bardblgk/bardbl2/bracerightBig
. (7)
returnxK
For instance consider again the SPSmaxmethod
xk+1=xk−min/braceleftBig
γb,fik(xk)−C(sik)
c/bardbl∇fik(xk)/bardbl2/bracerightBig
∇fik(xk), (SPSmax)
wherec,γb>0. Clearly, for c= 1andαk=γb, update (7) is identical to SPSmax. With this in mind, we
can interpret the hyperparameter γbinSPSmaxsimply as a step size for the model-based stochastic proximal
point step. For the parameter con the other hand, the model-based approach motivates the choice c= 1.
In this article, we will focus on this natural choice c= 1which also reduces the amount of hyperparameter
tuning. However, we should point out that, in the strongly convex case, c= 1/2gives the best rate of
convergence in (Loizou et al., 2021).
4 The regularized case
Now we consider regularized problems of the form (2), i.e.
min
x∈Rnψ(x), ψ (x) =f(x) +ϕ(x),
whereϕ:Rn→R∪{∞}is a proper, closed, λ-strongly convex function for λ≥0(we allowλ= 0). For
s∈S, denote by ψx(·;s)a stochastic model of the objective ψatx. We aim to analyze algorithms with the
update
xk+1= arg min
x∈Rnψxk(x;Sk) +1
2αk/bardblx−xk/bardbl2, (8)
whereSk∼Pandαk>0. Naively, if we know a lower bound ˜C(s)off(·;s) +ϕ(·), the truncated model
could be constructed for the function f(x;s) +ϕ(x), resulting in
ψx(y;s) = max{f(x;s) +ϕ(x) +/angbracketleftg+u,y−x/angbracketright,˜C(s)}, g∈∂f(x;s), u∈∂ϕ(x). (9)
In fact, Asi & Duchi (2019) and Loizou et al. (2021) work in the setting of unregularized problems and hence
their approaches would handle regularization in this way. What we propose instead, is to only truncate a
linearization of the loss f(x;s), yielding the model
ψx(y;s) =fx(y;s) +ϕ(y), fx(y;s) = max{f(x;s) +/angbracketleftg,y−x/angbracketright,C(s)}, g∈∂f(x;s).(10)
Solving (8) with the model in (10) results in
xk+1= arg min
y∈Rnmax{f(xk;Sk) +/angbracketleftgk,y−xk/angbracketright,C(Sk)}+ϕ(y) +1
2αk/bardbly−xk/bardbl2. (11)
5Published in Transactions on Machine Learning Research (05/2023)
−6−4−2 0 2 4 6−10123456
x0
x⋆ˆ x1
x1f(x;s) = ln(1 + exp(−0.5·x)),α= 10.0, λ= 0.1
f(·;s) +ϕ
ProxSPS model
SPSmodel
ProxSPS objective
SPSobjective
(a) Regularized logistic loss.
−2 0 2−3−2−10123ProxSPS
−2 0 2−3−2−10123SPS (b) Regularized squared loss with αk= 1, λ= 1.
Figure 1: a) SPSrefers to model (9) whereas ProxSPS refers to (10). We plot the corresponding model
ψx0(y;s)and the objective function of (8). x1(resp. ˆx1) denotes the new iterate for ProxSPS (resp. SPS),x⋆
is the minimizer of f(·;s) +ϕ. b) Streamlines of the vector ﬁeld V(xk) :=xk+1−xk, forf(x) =/bardblAx−b/bardbl2
and for the deterministic update, i.e. f(x;s) =f(x).ProxSPS refers to update (14) and SPSrefers to (13).
The circle marks the minimizer of f(x) +λ
2/bardblx/bardbl2.
The resulting model-based stochastic proximal point method is given in Algorithm 23. Lemma 12 shows
that, if proxϕis known, update (11) can be computed by minimizing a strongly convex function over a
compact one-dimensional interval. The relation to the proximal operator of ϕmotivates the name ProxSPS.
Further, the ProxSPS update (11) has a closed form solution when ϕis the squared /lscript2-norm, as we detail in
the next section.
Algorithm 2 ProxSPS
Require:x0∈Rn, step sizes αk>0.
fork= 0,1,2,...,K−1do
1. SampleSkand setCk:=C(Sk).
2. Choosegk∈∂f(xk;Sk).
Updatexk+1according to (11).
returnxK
4.1 The special case of /lscript2-regularization
Whenϕ(x) =λ
2/bardblx/bardbl2for someλ>0,ProxSPS (11) has a closed form solution as we show next in Lemma 1.
For this lemma, recall that the proximal operator of ϕ(x) =λ
2/bardblx/bardbl2is given by proxαϕ(x) =1
1+αλxfor all
α>0, x∈Rn.
Lemma 1. Letϕ(x) =λ
2/bardblx/bardbl2and letg∈∂f(x;s)andC(s)≤infz∈Rnf(z;s)hold for all s∈S. For
ψx(y;s) =fx(y;s) +ϕ(y)withfx(y;s) = max{f(x;s) +/angbracketleftg,y−x/angbracketright,C(s)}consider the update
xk+1= arg min
x∈Rnψxk(x;Sk) +1
2αk/bardblx−xk/bardbl2.
DenoteCk:=C(Sk)and letgk∈∂f(xk;Sk). Deﬁne
τ+
k:=

0 ifgk= 0,
min/braceleftbigg
αk,/parenleftBig
(1+αkλ)(f(xk;Sk)−Ck)−αkλ/angbracketleftgk,xk/angbracketright
/bardblgk/bardbl2/parenrightBig
+/bracerightbigg
else.
3Forϕ= 0, Algorithm 2 is identical to Algorithm 1.
6Published in Transactions on Machine Learning Research (05/2023)
Update(11)is given by
xk+1=1
1 +αkλ/parenleftBig
xk−τ+
kgk/parenrightBig
= proxαkϕ(xk−τ+
kgk). (12)
See Lemma 9 in the appendix for an extended version of the above lemma and its proof. The update (12)
can be naturally decomposed into two steps, one stochastic gradient step with an adaptive stepsize, that is
¯xk+1=xk−τ+
kgkfollowed by a proximal step xk+1= proxαkϕ(¯xk+1).This decoupling into two steps, makes
it easier to interpret the eﬀect of each step, with τ+
kadjusting for the scale/curvature and the following
proximal step shrinking the resulting parameters. There is no clear separation of tasks if we apply the SPS
method to the regularized problem, as we see next.
Algorithm 3 ProxSPS forϕ=λ
2/bardbl·/bardbl2
Require:x0∈Rn, step sizes αk>0.
fork= 0,1,2,...,K−1do
1. SampleSkand setCk:=C(Sk).
2. Choosegk∈∂f(xk;Sk). Ifgk= 0, setxk+1=1
1+αkλxk. Otherwise, set
xk+1=1
1 +αkλ/bracketleftBig
xk−min/braceleftBigg
αk,/parenleftbigg(1 +αkλ)(f(xk;Sk)−Ck)−αkλ/angbracketleftgk,xk/angbracketright
/bardblgk/bardbl2/parenrightbigg
+/bracerightBigg
gk/bracketrightBig
.
returnxK
4.2 Comparing the model of SPSand ProxSPS
For simplicity, assume again the discrete sample space setting ( ER) with diﬀerentiable loss functions fiand
letϕ=λ
2/bardbl·/bardbl2. Clearly, the composite problem (2) can be transformed to an instance of (1) by setting
/lscripti(x) :=fi(x) +λ
2/bardblx/bardbl2and solving minx/lscript(x)with/lscript(x) :=1
N/summationtextN
i=1/lscripti(x). Assume that a lower bound
/lscripti≤infx/lscripti(x)is known. In this case (9) becomes
ψx(y;si) = max/braceleftBig
fi(x) +λ
2/bardblx/bardbl2+/angbracketleft∇fi(x) +λx,y−x/angbracketright, /lscripti/bracerightBig
.
Due to Lemma 10, if ∇fik(xk) +λxk/negationslash= 0, the update (8) is given by
xk+1=xk−min/braceleftBig
αk,fik(xk) +λ
2/bardblxk/bardbl2−/lscriptik
/bardbl∇fik(xk) +λxk/bardbl2/bracerightBig
(∇fik(xk) +λxk). (13)
We refer to this method, which is using model (9), as SPS. On the other hand, using model (10) and if
∇fik(xk)/negationslash= 0, the update of ProxSPS (12) is
xk+1=1
1+αkλ/bracketleftBig
xk−min/braceleftbigg
αk,/parenleftBig(1+αkλ)(fik(xk)−C(sik))−αkλ/angbracketleft∇fik(xk),xk/angbracketright
/bardbl∇fik(xk)/bardbl2/parenrightBig
+/bracerightbigg
∇fik(xk)/bracketrightBig
.(14)
In Fig. 1a, we illustrate the two models (9) (denoted by SPS) and (10) (denoted by ProxSPS) for the logistic
loss with squared /lscript2-regularization. We can see that the ProxSPS model is a much better approximation of
the (stochastic) objective function as it still captures the quadratic behaviour of ϕ. Furthermore, as noted
in the previous section, ProxSPS decouples the step size of the gradient and of the shrinkage, and hence the
update direction depends on αk. In contrast, the update direction of SPSdoes not depend on αk, and the
regularization eﬀect is intertwined with the adaptive step size. Another way to see that the model (10) on
which ProxSPS is based on is a more accurate model as compared to the SPSmodel (9), is that the resulting
vector ﬁeld of ProxSPS takes a more direct route to the minimum, as illustrated in Fig. 1b.
Update (14) needs to compute the term /angbracketleft∇fik(xk),xk/angbracketrightwhile (13) needs to evaluate /bardblxk/bardbl2. Other than that,
the computational costs are roughly identical. For (14), a lower bound /lscriptiis required. For non-negative loss
7Published in Transactions on Machine Learning Research (05/2023)
functions, in practice both /lscriptiandC(si)are often set to zero, in which case (10) will be a more accurate
model as compared to (9).4
4.3 Convergence analysis
For the convergence analysis of Algorithm 2, we can work with the following assumption on ϕ.
Assumption 3. ϕ:Rn→R∪{∞}is a proper, closed, λ-strongly convex function with λ≥0.
Throughout this section we consider model (10), i.e. for g∈∂f(x;s), let
ψx(y;s) =fx(y;s) +ϕ(y), fx(y;s) = max{f(x;s) +/angbracketleftg,y−x/angbracketright,C(s)}.
Let us ﬁrst state a lemma on important properties of the truncated model:
Lemma 2. Considerfx(y;s) = max{f(x;s) +/angbracketleftg,y−x/angbracketright,C(s)}, whereg∈∂f(x;s)is arbitrary and C(s)≤
infz∈Rnf(z;s). Then, it holds:
(i) The mapping y/mapsto→fx(y;s)is convex.
(ii) For all x∈Rn, it holdsfx(x;s) =f(x;s). Iff(·;s)isρs–weakly convex for all s∈S, then
fx(y;s)≤f(y;s) +ρs
2/bardbly−x/bardbl2for allx,y∈Rn.
Proof. (i) The maximum over a constant and linear term is convex.
(ii) Recall that C(s)≤f(y;s)for ally∈Rn. Therefore, fx(x;s) = max{C(s),f(x;s)}=f(x;s).From
weak convexity of f(·;s)it followsf(x;s) +/angbracketleftg,y−x/angbracketright≤f(y;s) +ρs
2/bardbly−x/bardbl2and therefore
fx(y;s)≤max{C(s),f(y;s) +ρs
2/bardbly−x/bardbl2}=f(y;s) +ρs
2/bardbly−x/bardbl2for ally∈Rn.
4.3.1 Globally bounded subgradients
In this section, we show that the results for stochastic model-based proximal point methods in Davis &
Drusvyatskiy (2019) can be immediately applied to our speciﬁc model – even though this model has not
been explicitly analyzed in their article. This, however, requires assuming that the subgradients are bounded.
Proposition 3. Let Assumption 3 hold and assume that there is an open, convex set Ucontaining domϕ.
Letf(·;s)beρs–weakly convex for all s∈Sand letρ=E[ρS]. Assume that there exists Gs∈R+for all
s∈S, such that G:=/radicalbig
E[G2
S]<∞and
/bardblg(x;s)/bardbl≤Gs∀g(x;s)∈∂f(x;s),∀x∈U. (15)
Then,ψx(y;s)satisﬁes (Davis & Drusvyatskiy, 2019, Assum. B), in particular it holds
fx(x;s)−fx(y;s)≤Gs/bardblx−y/bardblfor alls∈Sand allx,y∈U. (16)
Remark 1. We state all four properties (B1)–(B4) of (Davis & Drusvyatskiy, 2019, Assum. B) explicitly in
the Appendix, see Proposition 14 which also contains the proof. The ﬁrst three properties follow immediately
in our setting. Only the last property (B4), stated in (16), requires the additional assumption (15).
Corollary 4 (Weakly convex case) .Let the assumptions of Proposition 3 hold with ρs>0for alls∈S.
Letρ=E[ρS]<∞and let ∆≥env1/(2ρ)
ψ(x0)−minψ. Let{xk}k=0,...,Kbe generated by Algorithm 2 for
constant step sizes αk=/parenleftBig
2ρ+/radicalBig
4ρG2K
∆/parenrightBig−1
. Then, it holds
E/bardbl∇env1/(2ρ)
ψ(xK
∼)/bardbl2≤8ρ∆
K+ 16G/radicalbigg
ρ∆
K,
wherexK
∼is uniformly drawn from {x0,...,xK−1}.
4For single element sampling, inf/lscriptican sometimes be precomputed (e.g. regularized logistic regression, see (Loizou et al.,
2021, Appendix D)). But even in this restricted setting it is not clear how to estimate inf/lscriptiwhen using mini-batching.
8Published in Transactions on Machine Learning Research (05/2023)
Proof.The claim follows from Proposition 3 and (Davis & Drusvyatskiy, 2019, Thm. 4.3), (4.16) setting
η= 0,¯ρ= 2ρ,T=K−1andβt=α−1
k.
Corollary 5 ((Strongly) convex case) .Let the assumptions of Proposition 3 hold with ρs= 0for alls∈S.
Letλ>0andx⋆= arg min xψ(x). Let{xk}k=0,...,Kbe generated by Algorithm 2 for step sizes αk=2
λ(k+1).
Then, it holds
E/bracketleftBig
ψ/parenleftBig
2
(K+1)(K+2)−2K/summationdisplay
k=1(k+ 1)xk/parenrightBig
−ψ(x⋆)/bracketrightBig
≤λ
(K+ 1)2/bardblx0−x⋆/bardbl2+8G2
λ(K+ 1).
Proof.Asρs= 0and henceρ= 0, we have that (Davis & Drusvyatskiy, 2019, Assum. B) is satisﬁed with
τ= 0(in the notation of (Davis & Drusvyatskiy, 2019), see Proposition 14). Moreover, by Lemma 2, (i)
andλ–strong convexity of ϕ, we haveλ–strong convexity of ψx(·;s). The claim follows from Proposition 3
and (Davis & Drusvyatskiy, 2019, Thm. 4.5) setting µ=λ,T=K−1andβt=α−1
k.
4.3.2 Lipschitz smoothness
Assumption (15), i.e. having globally bounded subgradients, is strong: it implies Lipschitz continuity of f
(cf. (Davis & Drusvyatskiy, 2019, Lem. 4.1)) and simple functions such as the squared loss do not satisfy
this. Therefore, we provide additional guarantees for the smooth case, without the assumption of globally
bounded gradients.
The following result, similar to (Davis & Drusvyatskiy, 2019, Lem. 4.2), is the basic inequality for the
subsequent convergence analysis.
Lemma 6. Let Assumption 3 hold. Let xk+1be given by (11)andψxkbe given in (10). For every x∈Rn
it holds
(1 +αkλ)/bardblxk+1−x/bardbl2≤/bardblxk−x/bardbl2−/bardblxk+1−xk/bardbl2+ 2αk/parenleftbig
ψxk(x;Sk)−ψxk(xk+1;Sk)/parenrightbig
.(17)
Moreover, it holds
ψxk(xk+1;Sk)≥f(xk;Sk) +/angbracketleftgk,xk+1−xk/angbracketright+ϕ(xk+1). (18)
Proof.The objective of (11) is given by Ψk(y) :=ψxk(y;Sk)+1
2αk/bardbly−xk/bardbl2.Using Lemma 2, (i) and λ-strong
convexity of ϕ,Ψk(y)is(λ+1
αk)–strongly convex. As xk+1is the minimizer of Ψk(y), for allx∈Rnwe have
Ψk(x)≥Ψk(xk+1) +1 +αkλ
2αk/bardblxk+1−x/bardbl2⇐⇒
(1 +αkλ)/bardblxk+1−x/bardbl2≤/bardblxk−x/bardbl2−/bardblxk+1−xk/bardbl2+ 2αk/parenleftbig
ψxk(x;Sk)−ψxk(xk+1;Sk)/parenrightbig
.
Moreover, by deﬁnition of fx(y;s)in (10) we have
ψxk(xk+1;Sk) =fxk(xk+1;Sk) +ϕ(xk+1)≥f(xk;Sk) +/angbracketleftgk,xk+1−xk/angbracketright+ϕ(xk+1).
We will work in the setting of diﬀerentiable loss functions with bounded gradient noise.
Assumption 4. The mapping f(·;s)is diﬀerentiable for all s∈Sand there exists β≥0such that
E/bardbl∇f(x;S)−∇f(x)/bardbl2≤βfor allx∈Rn. (19)
The assumption of bounded gradient noise (19) (in the diﬀerentiable setting) is indeed a weaker assumption
than (15) since E[∇f(x;S)] =∇f(x)and
E/bardbl∇f(x;S)−∇f(x)/bardbl2≤β⇐⇒E/bardbl∇f(x;S)/bardbl2≤/bardbl∇f(x)/bardbl2+β.
9Published in Transactions on Machine Learning Research (05/2023)
Remark 2. Assumption 4 (and the subsequent theorems) could be adapted to the case where f(·;s)is
weakly convex but non-diﬀerentiable: for ﬁxed x∈Rn, due to (Bertsekas, 1973, Prop. 2.2) and (Davis &
Drusvyatskiy, 2019, Lem. 2.1) it holds
E[∂f(x;S)] =E/bracketleftBig
∂/parenleftbig
f(x;S) +ρS
2/bardblx/bardbl2/parenrightbig
−ρSx/bracketrightBig
=∂f(x) +ρx−E[ρSx] =∂f(x),
where we used ρ=E[ρS]. Hence, for gs∈∂f(x;s)we have E[gS]∈∂f(x)and(19)is replaced by
E/bardblgS−E[gS]/bardbl2≤βfor allx∈Rn.
However, as we will still require that fis Lipschitz-smooth, we present our results for the diﬀerentiable
setting.
The proof of the subsequent theorems can be found in Appendix A.2 and Appendix A.3.
Theorem 7. Let Assumption 3 and Assumption 4 hold. Let f(·;s)be convex for all s∈Sand letfbe
L–smooth (4). Letx⋆= arg minx∈Rnψ(x)and letθ >1. Let{xk}k=0,...,Kbe generated by Algorithm 2 for
step sizesαk>0such that
αk≤1−1/θ
L. (20)
Then, it holds
(1 +αkλ)E/bardblxk+1−x⋆/bardbl2≤E/bardblxk−x⋆/bardbl2+ 2αkE[ψ(x⋆)−ψ(xk+1)] +θβα2
k. (21)
Moreover, we have:
a) Ifλ>0andαk=1
λ(k+k0)withk0≥1large enough such that (20)is fulﬁlled, then
E/bracketleftBig
ψ/parenleftBig
1
KK−1/summationdisplay
k=0xk+1/parenrightBig
−ψ(x⋆)/bracketrightBig
≤λk0
2K/bardblx0−x⋆/bardbl2+θβ(1 + lnK)
2λK. (22)
b) Ifλ= 0andαk=α√k+1withα≤1−1/θ
L, then
E/bracketleftBig
ψ/parenleftBig
1/summationtextK−1
k=0αkK−1/summationdisplay
k=0αkxk+1/parenrightBig
−ψ(x⋆)/bracketrightBig
≤/bardblx0−x⋆/bardbl2
4α(√
K+ 1−1)+θβα(1 + lnK)
4(√
K+ 1−1).(23)
c) Iffisµ–strongly convex with µ≥0,5andαk=αfulﬁlling (20), then
E/bardblxK−x⋆/bardbl2≤(1 +α(µ+ 2λ))−K/bardblx0−x⋆/bardbl2+θβα
µ+ 2λ. (24)
Remark 3. Ifλ > 0, for the decaying step sizes in item a) we get a rate of ˜O(1
K)ifλ > 0. In the
strongly convex case in item c), for constant step sizes, we get a linear convergence upto a neighborhood of
the solution. Note that the constant on the right-hand side of (24)can be forced to be small using a small
α. Further, the rate (24)has a 2λterm, instead of λ. This slight improvement in the rate occurs because we
do not linearize ϕin the ProxSPS model.
Theorem 8. Let Assumption 3 and Assumption 4 hold. Let f(·;s)beρs–weakly convex for all s∈Sand
letρ:=E[ρS]<∞. LetfbeL–smooth6and assume that infψ >−∞. Let{xk}k≥0be generated by
Algorithm 2. For θ>1, under the condition
η∈/braceleftBigg
(0,1
ρ−λ)ifρ>λ
(0,∞)else, α k≤1−θ−1
L+η−1, (25)
5Note that as f(·;s)is convex, so is f, and that we allow µ= 0here.
6Asfisρ–weakly convex, this implies ρ≤L.
10Published in Transactions on Machine Learning Research (05/2023)
it holds
K−1/summationdisplay
k=0αkE/bardbl∇envη
ψ(xk)/bardbl2≤2(envη
ψ(x0)−infψ)
1−η(ρ−λ)+βθ
η(1−η(ρ−λ))K−1/summationdisplay
k=0α2
k. (26)
Moreover, for the choice αk=α√k+1and withα≤1−θ−1
L+η−1, we have
min
k=0,...,K−1E/bardbl∇envη
ψ(xk)/bardbl2≤envη
ψ(x0)−infψ
α(1−η(ρ−λ))(√
K+ 1−1)+βθ
2η(1−η(ρ−λ))α(1 + lnK)
(√
K+ 1−1).
If instead we choose αk=α√
Kand withα≤√
K1−θ−1
L+η−1, we have
E/bardbl∇envη
ψ(xK
∼)/bardbl2≤2(envη
ψ(x0)−infψ)
α(1−η(ρ−λ))√
K+βθ
η(1−η(ρ−λ))α√
K,
wherexK
∼is uniformly drawn from {x0,...,xK−1}.
4.3.3 Comparison to existing theory
Recalling that Algorithm 1 is equivalent to SPSmaxwithc= 1andγb=αk, we can apply Theorem 7 and
Theorem 8 for the unregularized case where ϕ= 0and hence obtain new theory for ( SPSmax). We start
by summarizing the main theoretical results for SPSmaxgiven in (Loizou et al., 2021; Orvieto et al., 2022):
in the ( ER) setting, recall the interpolation constant σ2=E[f(x⋆;S)−C(S)] =1
N/summationtextN
i=1fi(x⋆)−C(si). If
fiisLi-smooth and convex, (Orvieto et al., 2022, Thm. 3.1) proves convergence to a neighborhood of the
solution, i.e. the iterates {xk}ofSPSmaxsatisfy
E[f(¯xK)−f(x⋆)]≤/bardblx0−x⋆/bardbl2
αK+2γbσ2
α, (27)
where ¯xK:=1
K/summationtextK−1
k=0xk,α:= min{1
2cLmax,γb}, andLmax:= maxi∈[N]Li.7For the nonconvex case, if fi
isLi-smooth and under suitable assumptions on the gradient noise, (Loizou et al., 2021, Thm. 3.8) states
that, for constants c1andc2, we have
min
k=1,...,KE/bardbl∇f(xk)/bardbl2≤1
c1K+c2. (28)
The main advantage of these results is that γbcan be held constant; furthermore in the convex setting (27),
the choice of γbrequires no knowledge of the smoothness constants Li. For both results however, we can
not directly conclude that the right-hand side goes to zero as K→∞as there is an additional constant.
Choosingγbsuﬃciently small does not immediately solve this as c1,αandc2all go to zero as γbgoes to
zero.
Ourresultscomplementthisbyshowingexactconvergenceforthe(weakly)convexcase,i.e.withoutconstants
on the right-hand side. This comes at the cost of an upper bound on the step sizes αkwhich depends on the
smoothness constant L. For exact convergence, it is important to use decreasing step sizes αk: Theorem 8
shows that the gradient of the Moreau envelope converges to zero at the rate O(1/√
K)for the choice of
αk=α√
K.8Another minor diﬀerence to (Loizou et al., 2021) is that we do not need to assume Lipschitz-
smoothness for all f(·;s)and work instead with the (more general) assumption of weak convexity. However,
we still need to assume Lipschitz smoothness of f.
Another variant of SPSmax, named DecSPS, has been proposed in (Orvieto et al., 2022): for unregularized
problems (1) it is given by
xk+1=xk−ˆγkgk,ˆγk=1
ckmin/braceleftBigf(xk;Sk)−Ck
/bardblgk/bardbl2,ck−1ˆγk−1/bracerightBig
(DecSPS)
7The theorem also handles the mini-batch case but, for simplicity, we state the result for sampling a single ikin each
iteration.
8Notice that αkthen depends on the total number of iterations Kand hence one would need to ﬁx Kbefore starting the
method.
11Published in Transactions on Machine Learning Research (05/2023)
where{ck}k≥0is an increasing sequence. In the ( ER) setting, if all fiare Lipschitz-smooth and strongly
convex, DecSPSconverges with a rate of O(1√
K), without knowledge of the smoothness or convexity constants
(cf. (Orvieto et al., 2022, Thm. 5.5)). However, under these assumptions, the objective fis strongly convex
and the optimal rate is O(1
K), which we achieve up to a logarithmic factor in Theorem 7, (22). Moreover,
forDecSPSno guarantees are given for nonconvex problems.
For regularized problems, the constant in (27) is problematic if σ2(computed for the regularized loss) is
moderately large. We refer to Appendix D.5 where we show that this can easily happen. For ProxSPS, our
theoretical results Theorem 7 and Theorem 8 are not aﬀected by this as they do not depend on the size of
σ2. To the best of our knowledge, this is the ﬁrst work to show theory for the stochastic Polyak step size in
a setting that explicitly considers regularization. Moreover, our results also cover the case of non-smooth or
non-real-valued regularization ϕwhere the theory in (Loizou et al., 2021) can not be applied.
5 Numerical experiments
Throughout we denote Algorithm 1 with SPSand Algorithm 3 with ProxSPS. For all experiments we use
PyTorch (Paszke et al., 2019)9.
5.1 General parameter setting
ForSPSandProxSPS we always use C(s) = 0for alls∈S. Forαk, we use the following schedules:
•constant : setαk=α0for allkand someα0>0.
•sqrt: setαk=α0√jfor all iterations kduring epoch j.
As we consider problems with /lscript2-regularization, for SPSwe handle the regularization term by incorporating
it into all individual loss functions, as depicted in (13). With ϕ=λ
2/bardbl·/bardbl2forλ≥0, we denote by ζkthe
adaptive step size term of the following algorithms:
•forSPSwe haveζk:=f(xk;Sk)+λ
2/bardblxk/bardbl2
/bardblgk+λxk/bardbl2(cf. (13) with /lscriptik= 0),
•forProxSPS we haveζk:=/parenleftBig
(1+αkλ)f(xk;Sk)−αkλ/angbracketleftgk,xk/angbracketright
/bardblgk/bardbl2/parenrightBig
+and thusτ+
k= min{αk,ζk}(cf. Lemma 1
withC(Sk) = 0).
5.2 Regularized matrix factorization
Problem description: ForA∈Rq×p, consider the problem
min
W1∈Rr×p,W2Rq×rEy∼N(0,I)/bardblW2W1y−Ay/bardbl2= min
W1∈Rr×p,W2Rq×r/bardblW2W1−A/bardbl2
F.
For the above problem, SPSmaxhas shown superior performance than other methods in the numerical experi-
ments of(Loizou et al.,2021). Theproblem can canbe turned into a (nonconvex)empirical risk minimization
problem by drawing Nsamples{y(1),...,y(N)}. Denoteb(i):=Ay(i). Adding squared norm regularization
withλ≥0(cf. (Srebro et al., 2004)), we obtain the problem
min
W1∈Rr×p,W2Rq×r1
NN/summationdisplay
i=1/bardblW2W1y(i)−b(i)/bardbl2+λ
2/parenleftbig
/bardblW1/bardbl2
F+/bardblW2/bardbl2
F/parenrightbig
. (29)
This ﬁts the format of (2), where x= (W1,W2), using a ﬁnite sample space S={s1,...,sN},f(x;si) =
/bardblW2W1y(i)−Ay(i)/bardbl2, andϕ=λ
2/bardbl·/bardbl2
F. Clearly, zero is a lower bound of f(·;si)for alli∈[N]. We investigate
ProxSPS for problems of form (29) on synthetic data. For details on the experimental procedure, we refer
9The code for our experiments and an implementation of ProxSPS can be found at https://github.com/fabian-sp/ProxSPS .
12Published in Transactions on Machine Learning Research (05/2023)
0 10 20 30 40
Epoch10−510−410−310−210−1ψ(xk)−minkψ(xk)constant
0 10 20 30 40
Epoch10−510−410−310−210−1sqrtα0
2.0
1.62
1.25
0.88
0.52.0
1.62
1.25
0.88
0.50.7
0.56
0.41
0.27
0.12
prox-sps
sps
sgd
Figure 2: Objective function for the Matrix Factorization problem (29), with constant (left) and sqrt
(right) step size schedule and several choices of initial values. Here minkψ(xk)is the best objective function
value found over all methods and all iterations.
to Appendix D.1.
Discussion: We discuss the results for the setting matrix-fac1 in Table 1 in the Appendix. We ﬁrst ﬁx
λ= 0.001and consider the three methods SPS,ProxSPS and SGD. Fig. 2 shows the objective function over
50 epochs, for both step size schedules sqrtandconstant , and several initial values α0. For the constant
schedule, we observe that ProxSPS converges quickly for all initial values while SPSis unstable. Note that
forSGDwe need to pick much smaller values for α0in order to avoid divergence ( SGDdiverges for large α0).
SPSfor largeα0is unstable, while for small α0we can expect similar performance to SGD(asγkis capped
byαk=α0). However, in the regime of small α0, convergence will be very slow. Hence, one of the main
advantages of SPS, namely that its step size can be chosen constant and moderately large (compared to SGD),
is not observed here. ProxSPS ﬁxes this by admitting a larger range of initial step sizes, all of which result
in fast convergence, and therefore is more robust than SGDandSPSwith respect to the tuning of α0.
For the sqrtschedule, we observe in Fig. 2 that SPScan be stabilized by reducing the values of αkover
the course of the iterations. However, for large α0we still see instability in the early iterations, whereas
ProxSPS does not show this behaviour. We again observe that ProxSPS is less sensitive with respect to the
choice ofα0as compared to SGD. The empirical results also conﬁrm our theoretical statement, showing exact
convergence if αkis decaying in the order of 1/√
k. From Fig. 3, we can make similar observations for the
validation error, deﬁned as1
Nval/summationtextNval
i=1/bardblW2W1y(i)−b(i)
val/bardbl2, whereb(i)
valare theNval=Nmeasurements from
the validation set (cf. Appendix D.1 for details).
We now consider diﬀerent values for λand only consider the sqrtschedule, as we have seen that for constant
step sizes, SPSwould not work for large step sizes and be almost identical to SGDfor small step sizes. Fig. 4
shows the objective function and validation error. Again, we can observe that SPSis unstable for large
initial values α0for allλ≥10−4. On the other hand, ProxSPS has a good performance for a wide range
ofα0∈[1,10]ifλis not too large. Indeed, ProxSPS convergence only starts to deteriorate when both α0
andλare very large. For α0= 1, the two methods give almost identical results. Finally, in Fig. 5a we plot
the validation error as a function of λ(taking the median over the last ten epochs). The plot shows that
the best validation error is obtained for λ= 10−4and for large α0. With SPSthe validation error is higher,
in particular for large α0andλ. Fig. 5b shows that ProxSPS leads to smaller norm of the iterates, hence a
more eﬀective regularization. Finally, we plot the actual step sizes for both methods in Fig. 6. We observe
that the adaptive step size ζk(Deﬁnition at end of Section 5.1) is typically larger and has more variance
forSPSthan ProxSPS, in particular for large λ. This increased variance might explain why SPSis unstable
whenα0is large: the actual step size is the minimum between αkandζkand hence both terms being large
could lead to instability. On the other hand, if α0= 1, the plot conﬁrms that SPSandProxSPS are almost
identical methods as ζk>αkfor most iterations.
13Published in Transactions on Machine Learning Research (05/2023)
0 10 20 30 40
Epoch10−410−2100102104Validation Errorconstant
0 10 20 30 40
Epoch10−410−310−210−1100sqrtα0
2.0
1.62
1.25
0.88
0.52.0
1.62
1.25
0.88
0.50.7
0.56
0.41
0.27
0.12
prox-sps
sps
sgd
Figure 3: Validation error for the Matrix Factorization problem (29), with constant (left) and sqrt(right)
step size schedule and several choices of initial values.
0 20 40
Epoch0.0000.0020.0040.0060.0080.010Objectiveψ(xk)λ= 1e−05
0 20 40
Epoch0.0020.0040.0060.0080.010λ= 0.0001
0 20 40
Epoch0.010.020.030.04λ= 0.001
0 20 40
Epoch0.1000.1250.1500.1750.2000.225λ= 0.01
0 20 40
Epoch1.01.52.02.5λ= 0.1
prox-sps, sqrt, α0=10.0
prox-sps, sqrt, α0=5.0
prox-sps, sqrt, α0=1.0sps, sqrt,α0=10.0
sps, sqrt,α0=5.0
sps, sqrt,α0=1.0
0 20 40
Epoch0.00000.00050.00100.00150.00200.0025Validation Errorλ= 1e−05
0 20 40
Epoch0.00000.00050.00100.00150.00200.00250.0030λ= 0.0001
0 20 40
Epoch0.0000.0020.0040.0060.0080.010λ= 0.001
0 20 40
Epoch0.000.020.040.060.08λ= 0.01
0 20 40
Epoch0.000.250.500.751.001.25λ= 0.1
prox-sps, sqrt, α0=10.0
prox-sps, sqrt, α0=5.0
prox-sps, sqrt, α0=1.0sps, sqrt,α0=10.0
sps, sqrt,α0=5.0
sps, sqrt,α0=1.0
Figure 4: Objective function value and validation error over the course of optimization. For the validation
error, we plot a rolling median over ﬁve epochs in order to avoid clutter.
We provide additional numerical results which conﬁrm the above ﬁndings in the Appendix: this includes the
results for the setting matrix-fac2 of Table 1 in Appendix D.2 as well as a matrix completion task on a
real-world dataset of air quality sensor networks (Rivera-Muñoz et al., 2022) in Appendix D.3.
5.3 Deep networks for image classiﬁcation
We train a ResNet56 and ResNet110 model (He et al., 2016) on the CIFAR10 dataset. We use the data
loading and preprocessing procedure and network implementation from https://github.com/akamaster/
pytorch_resnet_cifar10 . We do not use batch normalization. The loss function is the cross-entropy loss
of the true image class with respect to the predicted class probabilities, being the output of the ResNet56
network. We addλ
2/bardblx/bardbl2as regularization term, where xconsists of all learnable parameters of the model.
The CIFAR10 dataset consists of 60,000 images, each of size 32×32, from ten diﬀerent classes. We use
thePyTorch split into 50,000 training and 10,000 test examples and use a batch size of 128. For AdamW,
we set the weight decay parameter to λand set all other hyperparameters to its default. We use the
14Published in Transactions on Machine Learning Research (05/2023)
10−510−410−310−210−1
λ10−510−410−310−210−1100Validation Error
prox-sps, sqrt, α0=1.0
prox-sps, sqrt, α0=5.0
prox-sps, sqrt, α0=10.0sps, sqrt,α0=1.0
sps, sqrt,α0=5.0
sps, sqrt,α0=10.0
(a) Validation error
10−510−410−310−210−1
λ4.004.254.504.755.005.255.50/bardblxk/bardbl
prox-sps, sqrt, α0=1.0
prox-sps, sqrt, α0=5.0
prox-sps, sqrt, α0=10.0sps, sqrt,α0=1.0
sps, sqrt,α0=5.0
sps, sqrt,α0=10.0 (b) Model norm/radicalbig
/bardblW1/bardbl2+/bardblW2/bardbl2
Figure 5: Validation error and model norm as a function of the regularization parameter λ. Shaded area
is one standard deviation (computed over ten independent runs). For all values, we take the median over
epochs [40,50].
Figure 6: Adaptive step size selection for SPSandProxSPS. We plotζk(see deﬁnition in Section 5.1) as dots
for each iteration as well as their median over each epoch. For this plot, we use the results of only one of
the ten runs.
AdamW-implementation from https://github.com/zhenxun-zhuang/AdamW-Scale-free as it does not – in
contrast to the Pytorch implementation – multiply the weight decay parameter with the learning rate, which
leads to better comparability to SPSandProxSPS for identical values of λ. For SPSandProxSPS we use the
sqrt-schedule and α0= 1. We run each method repeatedly using (the same) three diﬀerent seeds for the
dataset shuﬄing.
Discussion: ForResnet56 , from the bottom plot in Fig. 7, we observe that both SPSandProxSPS work well
15Published in Transactions on Machine Learning Research (05/2023)
0 25 50 75 100
Epoch0.00.20.40.60.81.0Validation Accuracy
λ= 5e−06
0 25 50 75 100
Epoch0.00.20.40.60.81.0
λ= 5e−05
0 25 50 75 100
Epoch0.00.20.40.60.81.0
λ= 0.0005
prox-sps, sqrt, α0=1.0 sps, sqrt,α0=1.0 adamw, constant, α0=0.001
0 25 50 75 100
Epoch80100120/bardblxk/bardbl
λ= 5e−06
0 25 50 75 100
Epoch50607080
λ= 5e−05
0 25 50 75 100
Epoch102030405060
λ= 0.0005
prox-sps, sqrt, α0=1.0 sps, sqrt,α0=1.0 adamw, constant, α0=0.001
Figure 7: ResNet56 : (Top): Validation accuracy and model norm for three values of the regularization
parameterλ. Validation accuracy is deﬁned as the ratio of correctly labeled images on the validation set (i.e.
Top-1 accuracy ), plotted as ﬁve-epoch running median. (Bottom): With /bardblxk/bardblwe denote the norm of all
learnable parameters at the k-th iteration. Shaded area is two standard deviations over three independent
runs.
with ProxSPS leading to smaller weights. For λ= 5e−4, the progress of ProxSPS stagnates after roughly
25 epochs. This can be explained by looking at the adaptive step size term ζkin Fig. 9a: as it decays over
time we have τ+
k=ζk/lessmuchαk. Since every iteration of ProxSPS shrinks the weights by a factor1
1+αkλ, this
leads to a bias towards zero. This suggests that we should choose αkroughly of the order of ζk, for example
by using the values of ζkfrom the previous epoch.
For the larger model Resnet110 however, SPSdoes not make progress for a long time because the adaptive
step size is very small (see Fig. 8 and Fig. 9b). ProxSPS does not share this issue and performs well after a
few initial epochs. For larger values of λ, the training is also considerably faster than for AdamW. Generally,
we observe that ProxSPS (and SPSforResnet56 ) performs well in comparison to AdamW. This is achieved
without extensive hyperparameter tuning (in particular this suggests that setting c= 1inSPSmaxleads to
good results and reduces tuning eﬀort).
Furthermore, we trained a ResNet110 withbatch norm on the Imagenet32 dataset. The plots and exper-
imental details can be found in Appendix D.4. From Fig. 15, we conclude that SPSand ProxSPS perform
equally well in this experiment. Both SPSandProxSPS are less sensititve with respect to the regularization
parameterλthan AdamWand the adaptive step size leads to faster learning in the initial epochs compared to
SGD. We remark that with batch norm, the eﬀect of /lscript2-regularization is still unclear as the output of batch
norm layers is invariant to scaling and regularization becomes ineﬀective (Zhang et al., 2019).
6 Conclusion
We proposed and analyzed ProxSPS, a proximal version of the stochastic Polyak step size. We arrived
atProxSPS by using the framework of stochastic model-based proximal point methods. We then used this
framework to argue that the resulting model of ProxSPS is a better approximation as compared to the model
16Published in Transactions on Machine Learning Research (05/2023)
0 25 50 75 100
Epoch0.00.20.40.60.81.0Validation Accuracy
λ= 5e−06
0 25 50 75 100
Epoch0.00.20.40.60.81.0
λ= 5e−05
0 25 50 75 100
Epoch0.00.20.40.60.81.0
λ= 0.0005
prox-sps, sqrt, α0=1.0 sps, sqrt,α0=1.0 adamw, constant, α0=0.001
0 25 50 75 100
Epoch9095100105/bardblxk/bardbl
λ= 5e−06
0 25 50 75 100
Epoch405060708090
λ= 5e−05
0 25 50 75 100
Epoch20406080
λ= 0.0005
prox-sps, sqrt, α0=1.0 sps, sqrt,α0=1.0 adamw, constant, α0=0.001
Figure 8: ResNet110 : Validation accuracy as ﬁve-epoch running median (top) and model norm (bottom) for
three values of λ. Shaded area is two standard deviations over three independent runs.
used by SPSwhen using regularization. Our theoretical results cover a wide range of optimization problems,
including convex and nonconvex settings. We performed a series of experiments comparing ProxSPS,SPS,
SGDand AdamWwhen using /lscript2-regularization. In particular, we ﬁnd that SPScan be very hard to tune
when using /lscript2-regularization, and in contrast, ProxSPS performs well for a wide choice of step sizes and
regularization parameters. Finally, for our experiments on image classiﬁcation, we ﬁnd that ProxSPS is
competitive to AdamW, whereas SPScan fail for larger models. At the same time ProxSPS produces smaller
weights in the trained neural network. Having small weights may help reduce the memory footprint of the
resulting network, and even suggests which weights can be pruned.
Acknowledgments
We thank the Simons Foundation for hosting Fabian Schaipp at the Flatiron Institute. We also thank the
TUM Graduate Center for their ﬁnancial support for the visit.
References
Hilal Asi and John C. Duchi. Stochastic (approximate) proximal point methods: convergence, optimality,
and adaptivity. SIAM Journal on Optimization , 29(3):2257–2290, 2019. ISSN 1052-6234. doi: 10.1137/
18M1230323.
Amir Beck. First-order methods in optimization , volume 25 of MOS-SIAM Series on Optimization . Society
for Industrial and Applied Mathematics (SIAM), Philadelphia, PA; Mathematical Optimization Society,
Philadelphia, PA, 2017. ISBN 978-1-611974-98-0. doi: 10.1137/1.9781611974997.ch1.
Leonard Berrada, Andrew Zisserman, and M. Pawan Kumar. Training neural networks for and by interpo-
lation. June 2019.
Dimitri P. Bertsekas. Stochastic optimization problems with nondiﬀerentiable cost functionals. Journal of
Optimization Theory and Applications , 12:218–231, 1973. ISSN 0022-3239. doi: 10.1007/BF00934819.
17Published in Transactions on Machine Learning Research (05/2023)
(a)ResNet56
 (b)ResNet110
Figure 9: Adaptive step sizes for SPSandProxSPS. See deﬁnition of ζkin Section 5.1. For this plot, we use
the results of only one of the three runs.
Léon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP-
STAT’2010 , pp. 177–186. Physica-Verlag/Springer, Heidelberg, 2010.
Léon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning.
SIAM Review , 60(2):223–311, 2018. ISSN 0036-1445. doi: 10.1137/16M1080173.
Frank H. Clarke. Optimization and nonsmooth analysis . Canadian Mathematical Society Series of Mono-
graphs and Advanced Texts. John Wiley & Sons, Inc., New York, 1983. ISBN 0-471-87504-X. A Wiley-
Interscience Publication.
Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex functions.
SIAM Journal on Optimization , 29(1):207–239, 2019. ISSN 1052-6234. doi: 10.1137/18M1178244.
Dmitriy Drusvyatskiy and Courtney Paquette. Eﬃciency of minimizing compositions of convex functions
and smooth maps. Mathematical Programming , 178(1-2, Ser. A):503–558, 2019. ISSN 0025-5610. doi:
10.1007/s10107-018-1311-3.
Robert Gower, Othmane Sebbouh, and Nicolas Loizou. SGD for structured nonconvex functions: Learning
rates, minibatching and interpolation. In Arindam Banerjee and Kenji Fukumizu (eds.), Proceedings of
The 24th International Conference on Artiﬁcial Intelligence and Statistics , volume 130 of Proceedings of
Machine Learning Research , pp. 1315–1323. PMLR, 13–15 Apr 2021. URL https://proceedings.mlr.
press/v130/gower21a.html .
Elad Hazan and Sham Kakade. Revisiting the Polyak step size. May 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 770–778, 2016. doi:
10.1109/CVPR.2016.90.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and
Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015.
18Published in Transactions on Machine Learning Research (05/2023)
Nicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien. Stochastic Polyak step-
size for SGD: An adaptive learning rate for fast convergence. In Arindam Banerjee and Kenji Fukumizu
(eds.),Proceedings of The 24th International Conference on Artiﬁcial Intelligence and Statistics , volume
130 ofProceedings of Machine Learning Research , pp. 1306–1314. PMLR, 13–15 Apr 2021. URL https:
//proceedings.mlr.press/v130/loizou21a.html .
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference
on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019.
URL https://openreview.net/forum?id=Bkg6RiCqY7 .
Antonio Orvieto, Simon Lacoste-Julien, and Nicolas Loizou. Dynamics of SGD with stochastic Polyak
stepsizes: Truly adaptive variants and convergence to exact solution. May 2022.
Alasdair Paren, Leonard Berrada, Rudra P. K. Poudel, and M. Pawan Kumar. A stochastic bundle method
for interpolation. Journal of Machine Learning Research , 23(15):1–57, 2022. URL http://jmlr.org/
papers/v23/20-1248.html .
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf,
Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In Advances in Neural Information Processing Systems
32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf .
Boris T. Polyak. Introduction to optimization . Translations Series in Mathematics and Engineering. Opti-
mization Software, Inc., Publications Division, New York, 1987. ISBN 0-911575-14-6. Translated from the
Russian, With a foreword by Dimitri P. Bertsekas.
Mariana Prazeres and Adam M. Oberman. Stochastic gradient descent with Polyak’s learning rate. Journal
of Scientiﬁc Computing , 89(1):Paper No. 25, 16, 2021. ISSN 0885-7474. doi: 10.1007/s10915-021-01628-3.
L.M. Rivera-Muñoz, A.F. Giraldo-Forero, and J.D. Martinez-Vargas. Deep matrix factorization models for
estimation of missing data in a low-cost sensor network to measure air quality. Ecological Informatics , 71:
101775, 2022. ISSN 1574-9541. doi: https://doi.org/10.1016/j.ecoinf.2022.101775. URL https://www.
sciencedirect.com/science/article/pii/S1574954122002254 .
Herbert Robbins and Sutton Monro. A stochastic approximation method. Ann. Math. Statistics , 22:400–407,
1951. ISSN 0003-4851. doi: 10.1214/aoms/1177729586.
R. Tyrrell Rockafellar and Roger J.-B. Wets. Variational analysis , volume 317 of Grundlehren der math-
ematischen Wissenschaften [Fundamental Principles of Mathematical Sciences] . Springer-Verlag, Berlin,
1998. ISBN 3-540-62772-3. doi: 10.1007/978-3-642-02431-3.
Nathan Srebro, Jason Rennie, and Tommi Jaakkola. Maximum-margin matrix factorization. In
L. Saul, Y. Weiss, and L. Bottou (eds.), Advances in Neural Information Processing Sys-
tems, volume 17. MIT Press, 2004. URL https://proceedings.neurips.cc/paper/2004/file/
e0688d13958a19e087e123148555e4b4-Paper.pdf .
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger B. Grosse. Three mechanisms of weight decay regu-
larization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019 . OpenReview.net, 2019. URL https://openreview.net/forum?id=B1lz-3Rct7 .
Zhenxun Zhuang, Mingrui Liu, Ashok Cutkosky, and Francesco Orabona. Understanding AdamW through
proximal methods and scale-freeness. Transactions on Machine Learning Research , 2022. URL https:
//openreview.net/forum?id=IKhEPWGdwK .
19Published in Transactions on Machine Learning Research (05/2023)
Contents
1 Introduction 1
1.1 Background and Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2 Preliminaries 3
2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 General assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.3 Convex analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3 The unregularized case 4
3.1 A model-based view point . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
4 The regularized case 5
4.1 The special case of /lscript2-regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
4.2 Comparing the model of SPSandProxSPS . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.3 Convergence analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.3.1 Globally bounded subgradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.3.2 Lipschitz smoothness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.3.3 Comparison to existing theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
5 Numerical experiments 12
5.1 General parameter setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5.2 Regularized matrix factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5.3 Deep networks for image classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6 Conclusion 16
A Missing Proofs 21
A.1 Proofs of model-based update formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
A.2 Proof of Theorem 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.3 Proof of Theorem 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B Auxiliary Lemmas 26
C Model equivalence for SGD 27
D Additional information on numerical experiments 28
D.1 Matrix Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
D.2 Plots for matrix-fac2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
D.3 Matrix completion experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
20Published in Transactions on Machine Learning Research (05/2023)
D.4 Imagenet32 experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
D.5 Interpolation constant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
A Missing Proofs
A.1 Proofs of model-based update formula
Lemma 9. Forλ≥0, letϕ(x) =λ
2/bardblx/bardbl2and letg∈∂f(x;s)andC(s)≤infz∈Rnf(z;s)hold for all s∈S.
For
ψx(y;s) =fx(y;s) +ϕ(y), fx(y;s) = max{f(x;s) +/angbracketleftg,y−x/angbracketright,C(s)},
consider the update
xk+1= arg min
x∈Rnψxk(x;Sk) +1
2αk/bardblx−xk/bardbl2. (30)
DenoteCk:=C(Sk)and letgk∈∂f(xk;Sk). Deﬁne
τ+
k:=

0 ifgk= 0,
min/braceleftbigg
αk,/parenleftBig
(1+αkλ)(f(xk;Sk)−Ck)−αkλ/angbracketleftgk,xk/angbracketright
/bardblgk/bardbl2/parenrightBig
+/bracerightbigg
else.
Then, we have
xk+1=1
1 +αkλxk−τ+
k
1 +αkλgk=1
1 +αkλ/parenleftBig
xk−τ+
kgk/parenrightBig
= proxαkϕ(xk−τ+
kgk). (31)
Deﬁneτk:= 0ifgk= 0andτk:= min/braceleftBig
αk,(1+αkλ)(f(xk;Sk)−Ck)−αkλ/angbracketleftgk,xk/angbracketright
/bardblgk/bardbl2/bracerightBig
else. Then, it holds τk≤τ+
k
and
ψxk(xk+1;Sk) =f(xk;Sk)−αkλ
1+αkλ/angbracketleftgk,xk/angbracketright−τk
1+αkλ/bardblgk/bardbl2+ϕ(xk+1). (32)
Proof.Note that max{f(xk;Sk) +/angbracketleftgk,y−xk/angbracketright,Ck}is convex as a function of y. The update is therefore
unique. First, if gk= 0, then clearly xk+1= proxαkϕ(xk) =1
1+αkλxkand (32) holds true. Now, let gk/negationslash= 0.
The solution of (30) is either in {y|f(xk;Sk) +/angbracketleftgk,y−xk/angbracketright<Ck}, or in{y|f(xk;Sk) +/angbracketleftgk,y−xk/angbracketright>Ck}or
in{y|f(xk;Sk) +/angbracketleftgk,y−xk/angbracketright=Ck}. We therefore solve three problems:
(P1) Solve
y+= arg min
yCk+λ
2/bardbly/bardbl2+1
2αk/bardbly−xk/bardbl2.
Clearly, the solution is y+=1
1+αkλxk. Thisy+solves (30) if f(xk;Sk) +/angbracketleftgk,y+−xk/angbracketright<Ck.
(P2) Solve
y+= arg min
yf(xk;Sk) +/angbracketleftgk,y−xk/angbracketright+λ
2/bardbly/bardbl2+1
2αk/bardbly−xk/bardbl2.
Theoptimalityconditionis 0 =αkgk+αkλy++y+−xk. Thus, thesolutionis y+=1
1+αkλ(xk−αkgk).
Thisy+solves (30) if f(xk;Sk) +/angbracketleftgk,y+−xk/angbracketright>Ck.
(P3) Solve
y+= arg min
yλ
2/bardbly/bardbl2+1
2αk/bardbly−xk/bardbl2,s.t.f(xk;Sk) +/angbracketleftgk,y−xk/angbracketright=Ck.
21Published in Transactions on Machine Learning Research (05/2023)
The KKT conditions are given by
αkλy+y−xk+µgk= 0,
f(xk;Sk) +/angbracketleftgk,y−xk/angbracketright=Ck.
Taking the inner product of the ﬁrst equation with gk, we get
(1 +αkλ)/angbracketleftgk,y/angbracketright−/angbracketleftgk,xk/angbracketright+µ/bardblgk/bardbl2= 0.
From the second KKT condition we have /angbracketleftgk,y/angbracketright=Ck−f(xk;Sk) +/angbracketleftgk,xk/angbracketright, hence
(1 +αkλ)/parenleftbig
Ck−f(xk;Sk) +/angbracketleftgk,xk/angbracketright/parenrightbig
−/angbracketleftgk,xk/angbracketright+µ/bardblgk/bardbl2= 0.
Solving for µgivesµ=(1+αkλ)(f(xk;Sk)−Ck)−αkλ/angbracketleftgk,xk/angbracketright
/bardblgk/bardbl2 . From the ﬁrst KKT condition, we obtain
y+=1
1 +αkλ/parenleftbig
xk−µgk/parenrightbig
=1
1 +αkλ/parenleftbig
xk−(1 +αkλ)(f(xk;Sk)−Ck)−αkλ/angbracketleftgk,xk/angbracketright
/bardblgk/bardbl2gk/parenrightbig
.
Thisy+solves (30) if neither (P1) nor (P2) provided a solution.
For all three cases, the solution takes the form y+=1
1+αkλ[xk−tgk] =:y(t). As/bardblgk/bardbl2>0, the term
f(xk;Sk) +/angbracketleftgk,y(t)−xk/angbracketrightis strictly monotonically decreasing in t. We know f(xk;Sk) +/angbracketleftgk,y(t)−xk/angbracketright=Ck
fort=µ(from (P3)). Hence, f(xk;Sk) +/angbracketleftgk,y(t)−xk/angbracketright<Ck(>Ck)if and only if t>µ (t<µ ).
We conclude:
•Iff(xk;Sk) +/angbracketleftgk,y(0)−xk/angbracketright<Ck, then the solution to (P1) is the solution to (30). This condition
is equivalent to µ<0.
•Iff(xk;Sk) +/angbracketleftgk,y(αk)−xk/angbracketright>Ck, then the solution to (P2) is the solution to (30). This condition
is equivalent to αk<µ.
•If neither 0>µnorαk<µhold, i.e. if µ∈[0,αk], then the solution to (30) comes from (P3) and
hence is given by y(µ).
Altogether, we get that xk+1=1
1+αkλ[xk−τ+
kgk]withτ+
k= min{αk,(µ)+}.
Now, we prove (32). Note that if gk/negationslash= 0, thenτk= min{αk,µ}withµdeﬁned as in (P3). In the case of
(P1), we have ψxk(xk+1;Sk) =Ck+ϕ(xk+1). Moreover, it holds µ < 0and asαk>0we haveτk=µ.
Pluggingτk=µinto the right hand-side of (32), we obtain Ck+ϕ(xk+1).
In the case of (P2) or (P3), we have Ck≤f(xk;Sk) +/angbracketleftgk,xk+1−xk/angbracketright. Due tof(xk;Sk) +/angbracketleftgk,y(t)−xk/angbracketright=
f(xk;Sk)−1
1+αkλ/angbracketleftgk,xk/angbracketright+t
1+αkλ/bardblgk/bardbl2, we obtain (32) as xk+1=y(αk)andµ>αkin the case of (P2) and
xk+1=y(µ)andµ≤αkin the case of (P3).
Lemma 10. Consider the model fx(y;s) := max{f(x;s) +/angbracketleftg,y−x/angbracketright,C(s)}whereg∈∂f(x;s)andC(s)≤
infz∈Rnf(z;s)holds for all s∈S. Then, update (5)is given as
xk+1=xk−γkgk, γk=/braceleftBigg
0 ifgk= 0,
min/braceleftBig
αk,f(xk;Sk)−C(Sk)
/bardblgk/bardbl2/bracerightBig
else.
wheregk∈∂f(xk;Sk). Moreover, it holds
fxk(xk+1;Sk) = max{C(Sk),f(xk;Sk)−αk/bardblgk/bardbl2}, (33)
and therefore fxk(xk+1;Sk) =f(xk;Sk)−γk/bardblgk/bardbl2.
Proof.We apply Lemma 9 with λ= 0. Asf(xk;Sk)≥C(Sk), we have that τ+
k=τk=γk.
22Published in Transactions on Machine Learning Research (05/2023)
A.2 Proof of Theorem 7
From now on, denote with Fkthe ﬁltration that is generated by the history of all Sjforj= 0,...,k−1.
Proof of Theorem 7. In the proof, we will denote gk=∇f(xk;Sk). We apply Lemma 6, (17) with x=x⋆.
Due to Lemma 2 (ii) and convexity of f(·;s)it holds
ψxk(x⋆;Sk)≤f(x⋆;Sk) +ϕ(x⋆).
Together with (18), we have
(1 +αkλ)/bardblxk+1−x⋆/bardbl2≤/bardblxk−x⋆/bardbl2−/bardblxk+1−xk/bardbl2+ 2αk[ϕ(x⋆)−ϕ(xk+1)]
+ 2αk/bracketleftbig
f(x⋆;Sk)−f(xk;Sk)−/angbracketleftgk,xk+1−xk/angbracketright/bracketrightbig
.(34)
Smoothness of fyields
−f(xk)≤−f(xk+1) +/angbracketleft∇f(xk),xk+1−xk/angbracketright+L
2/bardblxk+1−xk/bardbl2.
Consequently,
−/angbracketleftgk,xk+1−xk/angbracketright=f(xk)−f(xk)−/angbracketleftgk,xk+1−xk/angbracketright
≤f(xk)−f(xk+1) +/angbracketleft∇f(xk)−gk,xk+1−xk/angbracketright+L
2/bardblxk+1−xk/bardbl2
≤f(xk)−f(xk+1) +θαk
2/bardbl∇f(xk)−gk/bardbl2+1
2θαk/bardblxk+1−xk/bardbl2+L
2/bardblxk+1−xk/bardbl2.
for anyθ>0, where we used Young’s inequality in the last step. Plugging into (34) gives
(1 +αkλ)/bardblxk+1−x⋆/bardbl2≤/bardblxk−x⋆/bardbl2+/bracketleftbig
αkL+1
θ−1/bracketrightbig
/bardblxk+1−xk/bardbl2+ 2αk[ϕ(x⋆)−ϕ(xk+1)]
+ 2αk/bracketleftbig
f(x⋆;Sk)−f(xk;Sk) +f(xk)−f(xk+1)/bracketrightbig
+θα2
k/bardbl∇f(xk)−gk/bardbl2.
Applying conditional expectation, we have E[f(x⋆;Sk)|Fk] =f(x⋆)and
E[−f(xk;Sk) +f(xk)|Fk] = 0,E[/bardbl∇f(xk)−gk/bardbl2|Fk]≤β.
Moreover, by assumption, αkL+1
θ−1≤0. Altogether, applying total expectation yields
(1 +αkλ)E/bardblxk+1−x⋆/bardbl2≤E/bardblxk−x⋆/bardbl2+ 2αkE[ψ(x⋆)−ψ(xk+1)] +θβα2
k
which proves (21).
Proof of a): letαk=1
λ(k+k0). Denote ∆k:=E/bardblxk−x⋆/bardbl2. Rearranging and summing (21), we have
K−1/summationdisplay
k=0E[ψ(xk+1)−ψ(x⋆)]≤K−1/summationdisplay
k=0/bracketleftBig
1
2αk∆k−1+αkλ
2αk∆k+1+θβαk
2/bracketrightBig
.
Plugging in αk, we have1+αkλ
2αk=λ(k+k0)
2+λ
2and thus
K−1/summationdisplay
k=0E[ψ(xk+1)−ψ(x⋆)]≤K−1/summationdisplay
k=0/bracketleftBig
λ(k+k0)
2∆k−λ(k+1+k0)
2∆k+1/bracketrightBig
+θβ
2K−1/summationdisplay
k=01
λ(k+k0).
Dividing by Kand using convexity of ψ10, we have
E/bracketleftBig
ψ/parenleftBig
1
KK−1/summationdisplay
k=0xk+1/parenrightBig
−ψ(x⋆)/bracketrightBig
≤λk0
2K/bardblx0−x⋆/bardbl2+θβ
2λKK−1/summationdisplay
k=01
k+k0.
10By assumption fis convex and therefore ψis convex.
23Published in Transactions on Machine Learning Research (05/2023)
Finally, ask0≥1, we estimate/summationtextK−1
k=01
k+k0≤/summationtextK−1
k=01
k+1≤1 + lnKby Lemma 13 and obtain (22).
Proof of b): Similar to the proof above, we rearrange and sum (21) from k= 0,...,K−1, and obtain
K−1/summationdisplay
k=0αkE[ψ(xk+1)−ψ(x⋆)]≤/bardblx0−x⋆/bardbl2
2+θβ/summationtextK−1
k=0α2
k
2.
We divide by/summationtextK−1
k=0αkand use convexity of ψin order to obtain the left-hand side of (23). Moreover, by
Lemma 13 we have
K−1/summationdisplay
k=0αk≥2α(√
K+ 1−1),K−1/summationdisplay
k=0α2
k≤α2(1 + lnK).
Plugging in the above estimates, gives
E/bracketleftBig
ψ/parenleftBig
1/summationtextK−1
k=0αkK−1/summationdisplay
k=0αkxk+1/parenrightBig
−ψ(x⋆)/bracketrightBig
≤/bardblx0−x⋆/bardbl2
4α(√
K+ 1−1)+θβα(1 + lnK)
4(√
K+ 1−1).
Proof of c): Iffisµ–strongly–convex, then ψis(λ+µ)–strongly convex and
ψ(x⋆)−ψ(xk+1)≤−µ+λ
2/bardblxk+1−x⋆/bardbl2.
From (21), with αk=α, we get
(1 +α(µ+ 2λ))E/bardblxk+1−x⋆/bardbl2≤E/bardblxk−x⋆/bardbl2+θβα2.
Doing a recursion of the above from k= 0,...,K−1gives
E/bardblxK−x⋆/bardbl2≤(1 +α(µ+ 2λ))−K/bardblx0−x⋆/bardbl2+θβα2K/summationdisplay
k=1(1 +α(µ+ 2λ))−k
Using the geometric series,/summationtextK
k=1(1 +α(µ+ 2λ))−k≤1+α(µ+2λ)
α(µ+2λ)−1 =1
α(µ+2λ), and thus
E/bardblxK−x⋆/bardbl2≤(1 +α(µ+ 2λ))−K/bardblx0−x⋆/bardbl2+θβα
µ+ 2λ.
A.3 Proof of Theorem 8
Proof of Theorem 8. In the proof, we will denote gk=∇f(xk;Sk). By assumption fisρ-weakly convex and
henceψis(ρ−λ)-weakly convex if ρ>λand convex if ρ≤λ. Hence, ˆxk:= proxηψ(xk)is well-deﬁned for
η <1/(ρ−λ)ifρ > λand for any η >0else. Note that ˆxkisFk–measurable. We apply Lemma 6, (17)
withx= ˆxk. Due to Lemma 2 (ii) it holds
ψxk(ˆxk;Sk) =fxk(ˆxk;Sk) +ϕ(ˆxk)≤f(ˆxk;Sk) +ρSk
2/bardblˆxk−xk/bardbl2+ϕ(ˆxk).
Together with (18), this gives
(1 +αkλ)/bardblxk+1−ˆxk/bardbl2≤(1 +αkρSk)/bardblxk−ˆxk/bardbl2−/bardblxk+1−xk/bardbl2
+ 2αk/parenleftBig
ϕ(ˆxk)−ϕ(xk+1) +f(ˆxk;Sk)−f(xk;Sk)−/angbracketleftgk,xk+1−xk/angbracketright/parenrightBig
Analogous to the proof of Theorem 7, due to Lipschitz smoothness, for all θ>0we have
−f(xk;Sk)−/angbracketleftgk,xk+1−xk/angbracketright≤−f(xk;Sk) +f(xk)
−f(xk+1) +θαk
2/bardbl∇f(xk)−gk/bardbl2+/bracketleftbig1
2θαk+L
2/bracketrightbig
/bardblxk+1−xk/bardbl2.
24Published in Transactions on Machine Learning Research (05/2023)
Plugging in gives
(1 +αkλ)/bardblxk+1−ˆxk/bardbl2≤(1 +αkρSk)/bardblxk−ˆxk/bardbl2+ 2αk/parenleftBig
ϕ(ˆxk)−ϕ(xk+1)/parenrightBig
+ 2αk/parenleftbig
f(ˆxk;Sk)−f(xk;Sk) +f(xk)−f(xk+1) +θαk
2/bardbl∇f(xk)−gk/bardbl2/parenrightbig
+/bracketleftbig1
θ+αkL−1/bracketrightbig
/bardblxk+1−xk/bardbl2.
It holds E[f(ˆxk;Sk)−f(xk;Sk)|Fk] =f(ˆxk)−f(xk)andE[ψ(ˆxk)|Fk] =ψ(ˆxk). By Assumption 4, we have
E[/bardblgk−∇f(xk)/bardbl2|Fk]≤β.Altogether, taking conditional expectation yields
(1 +αkλ)E[/bardblxk+1−ˆxk/bardbl2|Fk]≤(1 +αkρ)/bardblxk−ˆxk/bardbl2+ 2αkE/bracketleftbig
ψ(ˆxk)−ψ(xk+1)|Fk/bracketrightbig
+α2
kθβ+/bracketleftbig1
θ+αkL−1/bracketrightbig
E[/bardblxk+1−xk/bardbl2|Fk].
Next, the deﬁnition of the proximal operator implies that almost surely
ψ(ˆxk) +1
2η/bardblˆxk−xk/bardbl2≤ψ(xk+1) +1
2η/bardblxk+1−xk/bardbl2,
and hence
E/bracketleftbig
ψ(ˆxk)−ψ(xk+1)|Fk/bracketrightbig
≤E/bracketleftbig1
2η/bardblxk+1−xk/bardbl2−1
2η/bardblˆxk−xk/bardbl2|Fk/bracketrightbig
.
Altogether, we have
(1 +αkλ)E[/bardblxk+1−ˆxk/bardbl2|Fk]≤(1 +αk(ρ−η−1))/bardblxk−ˆxk/bardbl2
+α2
kθβ+/bracketleftbig1
θ+αkL+αkη−1−1/bracketrightbig
E[/bardblxk+1−xk/bardbl2|Fk].
From assumption (25), we can drop the last term. Now, we aim for a recursion in envη
ψ. Using that
1 +αk(ρ−η−1)
1 +αkλ=1 +αkλ−αkλ+αk(ρ−η−1)
1 +αkλ= 1 +αk(ρ−η−1−λ)
1 +αkλ≤1 +αk(ρ−η−1−λ),
we get
E[envη
ψ(xk+1)|Fk]≤E[ψ(ˆxk) +1
2η/bardblxk+1−ˆxk/bardbl2|Fk]
≤ψ(ˆxk) +1
2η/bardblxk−ˆxk/bardbl2
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=envη
ψ(xk)+1
2η/bracketleftbig
αk(ρ−η−1−λ)/bracketrightbig
/bardblxk−ˆxk/bardbl2+α2
k
2ηθβ.
Now using/bardblxk−ˆxk/bardbl=η/bardbl∇envη
ψ(xk)/bardblwe conclude
E[envη
ψ(xk+1)|Fk]≤envη
ψ(xk) +η
2/bracketleftbig
αk(ρ−η−1−λ)/bracketrightbig
/bardbl∇envη
ψ(xk)/bardbl2+α2
k
2ηθβ.
Due to (25), we have η−1+λ−ρ >0. Taking expectation and unfolding the recursion by summing over
k= 0,...,K−1, we get
K−1/summationdisplay
k=0αk
2(1−η(ρ−λ))E/bardbl∇envη
ψ(xk)/bardbl2≤envη
ψ(x0)−E[envη
ψ(xK)] +K−1/summationdisplay
k=0α2
k
2ηθβ.
Now using that envη
ψ(xK)≥infψalmost surely, we ﬁnally get
K−1/summationdisplay
k=0αkE/bardbl∇envη
ψ(xk)/bardbl2≤2(envη
ψ(x0)−infψ)
1−η(ρ−λ)+βθ
η(1−η(ρ−λ))K−1/summationdisplay
k=0α2
k, (35)
25Published in Transactions on Machine Learning Research (05/2023)
which proves (26). Now choose αk=α√k+1and divide (35) by/summationtextK−1
k=0αk. Using Lemma 13 for/summationtextK−1
k=0αk
and/summationtextK−1
k=0α2
k, we have
min
k=0,...,K−1E/bardbl∇envη
ψ(xk)/bardbl2≤envη
ψ(x0)−infψ
α(1−η(ρ−λ))(√
K+ 1−1)+βθ
2η(1−η(ρ−λ))α(1 + lnK)
(√
K+ 1−1).
Choosingαk=α√
Kinstead, we can identify the left-hand-side of (35) as α√
KE/bardbl∇envη
ψ(xK
∼)/bardbl2. Dividing by
α√
Kand using/summationtextK−1
k=0α2
k=α2, we obtain
E/bardbl∇envη
ψ(xK
∼)/bardbl2≤2(envη
ψ(x0)−infψ)
α(1−η(ρ−λ))√
K+βθ
η(1−η(ρ−λ))α√
K.
B Auxiliary Lemmas
Lemma 11 (Thm. 4.5 in (Drusvyatskiy & Paquette, 2019)) .LetfbeL-smooth and ϕbe proper, closed,
convex. For η>0, deﬁneGη(x) :=η−1/parenleftbig
x−proxηϕ(x−η∇f(x))/parenrightbig
. It holds
1
4/bardbl∇env1/(2L)
ψ(x)/bardbl≤/bardblG 1/L(x)/bardbl≤3
2(1 +1√
2)/bardbl∇env1/(2L)
ψ(x)/bardbl ∀x∈Rn.
Lemma 12. Letc∈R,a,x0∈Rnandβ > 0and letϕ:Rn→R∪{∞}be proper, closed, convex. The
solution to
y+= arg min
y∈Rn/parenleftbig
c+/angbracketlefta,y/angbracketright/parenrightbig
++ϕ(y) +1
2β/bardbly−x0/bardbl2(36)
is given by
y+=

proxβϕ(x0−βa),ifc+/angbracketlefta,proxβϕ(x0−βa)/angbracketright>0,
proxβϕ(x0), ifc+/angbracketlefta,proxβϕ(x0)/angbracketright<0,
proxβϕ(x0−βua)else, foru∈[0,1]such thatc+/angbracketlefta,proxβϕ(x0−βua)/angbracketright= 0.(37)
Remark 4. The ﬁrst two conditions can not hold simultaneously due to uniqueness of the solution. If neither
of the conditions of the ﬁrst two cases are satisﬁed, we have to ﬁnd the root of u/mapsto→c+/angbracketlefta,proxβϕ(x0−βua)/angbracketright
foru∈[0,1]. Due to strong convexity of the objective in (36), we know that there exists a root and hence
y+can be found eﬃciently with bisection.
Proof.The objective of (36) is strongly convex and hence there exists a unique solution. Due to (Beck, 2017,
Thm. 3.63), yis the solution to (36) if and only if it satisﬁes ﬁrst-order optimality, i.e.
∃u∈∂(·)+(c+/angbracketlefta,y/angbracketright) : 0∈ua+∂ϕ(y) +1
β(y−x0). (38)
Now, asy= proxβϕ(z)⇐⇒ 0∈∂ϕ(y) +1
β(y−z), it holds
(38)⇐⇒ ∃u∈∂(·)+(c+/angbracketlefta,y/angbracketright) : 0∈∂ϕ(y) +1
β(y−(x0−βua))
⇐⇒ ∃u∈∂(·)+(c+/angbracketlefta,y/angbracketright) :y= proxβϕ(x0−βua).
We distinguish three cases:
1. Let ¯y:= proxβϕ(x0−βa)and suppose that c+/angbracketlefta,¯y/angbracketright>0. Then∂(·)+(c+/angbracketlefta,¯y/angbracketright) ={1}and hence
¯ysatisﬁes (38) with u= 1. Hence,y+= ¯y.
26Published in Transactions on Machine Learning Research (05/2023)
2. Let ¯y:= proxβϕ(x0)and suppose that c+/angbracketlefta,¯y/angbracketright<0. Then∂(·)+(c+/angbracketlefta,¯y/angbracketright) ={0}and hence ¯y
satisﬁes (38) with u= 0. Hence,y+= ¯y.
3. If neither the condition of the ﬁrst nor of the second case of (37) are satisﬁed, then, as (38) is
a necessary condition for the solution y+, it must hold c+/angbracketlefta,y+/angbracketright= 0. Hence, there exists a
u∈∂(·)+(c+/angbracketlefta,y+/angbracketright) = [0,1]such that
c+/angbracketlefta,proxβϕ(x0−uβa)/angbracketright= 0.
Lemma 13. For anyK≥1it holds
K−1/summationdisplay
k=01
k+1= 1 +K−1/summationdisplay
k=11
k+1≤1 +/integraldisplayK−1
01
s+1ds= 1 + lnK,
K−1/summationdisplay
k=01√k+1≥/integraldisplayK
01√s+1ds= 2√
K+ 1−2.
The following is a detailled version of Proposition 3. We refer to Section 4.3 for context.
Proposition 14. Let Assumption 1 and Assumption 3 hold and assume that there is an open, convex set U
containing domϕ. Letf(·;s)beρs–weakly convex for all s∈Sand letρ=E[ρS]. Assume that there exists
Gs∈R+for alls∈S, such that G:=/radicalbig
E[G2
S]<∞and
/bardblg(x;s)/bardbl≤Gs∀g(x;s)∈∂f(x;s),∀x∈U. (39)
Then,ψx(y;s)(given in (10)) satisﬁes the following:
(B1) It is possible to generate inﬁnitely many i.i.d. realizations S1,S2,...fromS.
(B2) It holds E[fx(x;S)] =f(x)andE[fx(y;S)]≤f(y) +ρ
2/bardbly−x/bardbl2for allx,y∈Rn.
(B3) The mapping ψx(·;s) =fx(·;s) +ϕ(·)is convex for all x∈Rnand alls∈S.
(B4) For all x,y∈Uands∈S, it holdsfx(x;s)−fx(y;s)≤Gs/bardblx−y/bardbl.
Proof.The properties (B1)–(B4) are identical to (B1)–(B4) in (Davis & Drusvyatskiy, 2019, Assum. B),
settingr=ϕ,fx(·,ξ) =fx(·;s),η= 0,τ=ρ,L=G, andL(ξ) =Gs. (B1) is identical to Assumption 1.
(B2) holds due to Lemma 2, (ii), applying expectation and using the deﬁnition of f, i.e.f(x) =E[f(x;S)].
(B3) holds due to Lemma 2, (i) and convexity of ϕ. For (B4), taking g∈∂f(x;s)andx,y∈U, we have
fx(x;s)−fx(y;s)≤f(x;s)−f(x;s)−/angbracketleftg,y−x/angbracketright≤/bardblg/bardbl/bardbly−x/bardbl≤Gs/bardblx−y/bardbl.
C Model equivalence for SGD
In the unregularized case, the SGDupdate
xk+1=xk−αkgk, gk∈∂f(xk;Sk),
can be seen as solving (5) with the model
fx(y;s) =f(x;s) +/angbracketleftg,y−x/angbracketright, g∈∂f(x;s).
27Published in Transactions on Machine Learning Research (05/2023)
Now, consider again the regularized problem (2) with ϕ(x) =λ
2/bardblx/bardbl2and update (8) .
On the one hand, the model ψx(y;s) =f(x;s) +ϕ(x) +/angbracketleftg+λx,y−x/angbracketrightwithg∈∂f(x;s)yields
xk+1=xk−αk(gk+λxk) = (1−αkλ)xk−αkgk. (40)
On the other hand, the model ψx(y;s) =f(x;s) +/angbracketleftg,y−x/angbracketright+ϕ(y)withg∈∂f(x;s)results in
xk+1= proxαkϕ(xk−αkgk) =1
1 +αkλ/bracketleftbig
xk−αkgk/bracketrightbig
= (1−αk
1 +αkλλ)xk−αk
1 +αkλgk.(41)
Running (40) with step sizes αk=βkis equivalent to running (41) with step sizesαk
1+αkλ=βk⇐⇒αk=
βk
1−βkλ. In this sense, standard SGDcan be seen to be equivalent to proximal SGDfor/lscript2–regularized problems.
D Additional information on numerical experiments
D.1 Matrix Factorization
Synthetic data generation: We consider the experimental setting of the deep matrix factorization ex-
periments in (Loizou et al., 2021), but with an additional regularization. We generate data in the following
way: ﬁrst sample B∈Rq×pwith uniform entries in the interval [0,1]. Then choose υ∈R(which will be our
targeted inverse condition number) and compute A=DBwhereDis a diagonal matrix with entries from 1
toυ(equidistant on a logarithmic scale)11. In order to investigate the impact of regularization, we generate
a noise matrix Ewith uniform entries in [−ε,ε]and set ˜A:=A⊙(1+E). We then sample y(i)∼N(0,I)and
compute the targets b(i)=˜Ay(i). A validation set of identical size is created by the same mechanism, but
computing its targets, denoted by b(i)
val, via the original matrix Ainstead of ˜A. The validation set contains
Nval=Nsamples.
Name p q N υ r ε
matrix-fac1 6 10 1000 1e-54 0
matrix-fac2 6 10 1000 1e-510 0.05
Table 1: Matrix factorization synthetic datasets.
Model and general setup: Problem (29) can be interpreted as a two-layer neural network without acti-
vation functions. We train the network using the squared distance of the model output and b(i)(averaged
over a mini-batch) as the loss function. We run 50 epochs for diﬀerent methods, step size schedules and
values ofλ. For each diﬀerent instance, we do ten independent runs: each run has the identical training
set and initialization of W1andW2, but diﬀerent shuﬄing of the training set and diﬀerent samples y(i)for
the validation set. In order to allow a fair comparison, all methods have identical train and validation sets
across all runs. All metrics are averaged over the ten runs. We always use a batch size of 20.
D.2 Plots for matrix-fac2
In this section, we plot additional results for Matrix Factorization, namely for the setting matrix-fac2
of Table 1, see Fig. 10, Fig. 11, and Fig. 12. The results are qualitatively very similar to the setting
matrix-fac1 .
11Note that (Loizou et al., 2021) uses entries from 1toυon a linearscale which, in our experiments, did not result in large
condition numbers even if υis very small.
28Published in Transactions on Machine Learning Research (05/2023)
0 10 20 30 40
Epoch10−510−410−310−210−1ψ(xk)−minkψ(xk)constant
0 10 20 30 40
Epoch10−510−410−310−210−1sqrtα0
2.0
1.62
1.25
0.88
0.52.0
1.62
1.25
0.88
0.50.7
0.56
0.41
0.27
0.12
prox-sps
sps
sgd
Figure 10: Objective function for the Matrix Factorization problem (29), with constant (left) and sqrt
(right) step size schedule and several choices of initial values. Here minkψ(xk)is the best objective function
value found over all methods and all iterations.
0 10 20 30 40
Epoch10−210−1100Validation Errorconstant
0 10 20 30 40
Epoch10−210−1100sqrtα0
2.0
1.62
1.25
0.88
0.52.0
1.62
1.25
0.88
0.50.7
0.56
0.41
0.27
0.12
prox-sps
sps
sgd
Figure 11: Validation error for the Matrix Factorization problem (29), with constant (left) and sqrt(right)
step size schedule and several choices of initial values.
0 20 40
Epoch0.000250.000500.000750.001000.001250.00150Objectiveψ(xk)λ= 1e−05
0 20 40
Epoch0.00100.00150.00200.00250.00300.0035λ= 0.0001
0 20 40
Epoch0.0100.0150.0200.025λ= 0.001
0 20 40
Epoch0.100.150.200.25λ= 0.01
0 20 40
Epoch1.01.52.02.5λ= 0.1
prox-sps, sqrt, α0=10.0
prox-sps, sqrt, α0=5.0
prox-sps, sqrt, α0=1.0sps, sqrt,α0=10.0
sps, sqrt,α0=5.0
sps, sqrt,α0=1.0
0 20 40
Epoch0.00630.00640.00650.0066Validation Errorλ= 1e−05
0 20 40
Epoch0.00650.00700.00750.0080λ= 0.0001
0 20 40
Epoch0.0060.0080.0100.0120.0140.0160.018λ= 0.001
0 20 40
Epoch0.020.040.060.080.100.12λ= 0.01
0 20 40
Epoch0.000.250.500.751.001.25λ= 0.1
prox-sps, sqrt, α0=10.0
prox-sps, sqrt, α0=5.0
prox-sps, sqrt, α0=1.0sps, sqrt,α0=10.0
sps, sqrt,α0=5.0
sps, sqrt,α0=1.0
Figure 12: Objective function value and validation error over the course of optimization. For the validation
error, we plot a rolling median over ﬁve epochs in order to avoid clutter.
29Published in Transactions on Machine Learning Research (05/2023)
D.3 Matrix completion experiment
Consider an unknown matrix of interest W∈Rd1×d2. Factorizing W≈U/latticetopVwithU∈Rr×d1, V∈Rr×d2,
we can estimate the entries of matrix Was
ˆWij=u/latticetop
ivj+bU
i+bV
j, i∈[d1], j∈[d2], (42)
whereuiis thei-th column of Uandvjis thej-th column of V, andbU∈Rd1, bV∈Rd2are bias terms
(Rivera-Muñoz et al., 2022).
We can interpret this as an empirical risk minimization problem as follows: let Tbe the set of indices (i,j)
whereWijis known. With ˆWijas in (42) for trainable parameters (U,V,bU,bV), the (regularized) problem
is then given as
min
U,V,bU,bV1
|T|/summationdisplay
(i,j)∈T(Wij−ˆWij)2+λ
2/bardbl(U,V,bU,bV)/bardbl2.
We use a dataset containing air quality measurements of a sensor network over one month. This dataset has
been studied in Rivera-Muñoz et al. (2022).12The dataset contains measurements from 130 sensors over
720 timestamps, hence d1= 130, d2= 720. In total, there are 56158 nonzero measurements (the rest was
missing data or removed due to being an outlier). We split the nonzero measurements into a training set of
size|T|= 44926≈0.8·56158and the rest as a validation set. We standardize training and validation set
using mean and variance of the training set. We set r= 24and use batch size 128. The validation error is
deﬁned as the root mean squared error on the elements of the validation set (which is not used for training).
Discussion : The results are plotted in Fig. 13 and Fig. 14a. For all methods, we use a constant step size
αk.ProxSPS achieves the smallest error on the validation set for the two smaller values of λ. For the largest
λ,ProxSPS,SPSandSGDare almost identical for α0= 5, but SGDwithα0= 1is the best method. However,
over all tested values of λ, Fig. 14a shows that ProxSPS obtains the smallest error. Again, from the lower
plot in Fig. 13 we can observe that ProxSPS produces iterates with smaller norm.
D.4 Imagenet32 experiment
Imagenet32 contains 1,28 million training and 50,000 test images of size 32×32, from 1,000 classes. We train
the same ResNet110 as described in Section 5.3 with two diﬀerences: we exchange the output dimension of
the ﬁnal layer to 1,000 and activate batch norm. We use batch size 512. For this experiment we only run
one repetition.
Similar to the setup in Section 5.3, we run all methods for three diﬀerent values of λ. For AdamW, we use a
constant learning rate 0.001, for SGD,SPS, and ProxSPS we use the sqrt-schedule and α0= 1. The validation
accuracy and model norm are plotted in Fig. 15: we can observe that all methods perform similarly well
in terms of accuracy. However, AdamWis more sensitive with respect to the choice of λand the norm of its
iterates diﬀers signiﬁcantly from the other methods. Further, using an adaptive step size is advantageous:
from Fig. 16, we see that the adaptive step size is active in the initial iterations, which leads to a faster
learning of (Prox)SPS in the initial epochs compared to SGD.
D.5 Interpolation constant
We illustrate how the interpolation constant σ2behaves if it would be computed for the regularized loss
/lscripti(x) =fi(x) +λ
2/bardblx/bardbl2(cf. also Section 4.2). We do a simple ridge regression experiment. Let A∈RN×n
be a matrix with row vectors ai∈Rn, i∈[N]. We setN= 80, n= 100and generate ˆx∈Rnwith
entries drawn uniformly from [0,1]. We compute b=Aˆx. In this case, we have fi(x) =1
2(a/latticetop
ix−bi)2and
f(x) =1
N/summationtextN
i=1fi(x).
12Thedatasetcanbedownloadedfrom https://github.com/andresgiraldo3312/DMF/blob/main/DatosEliminados/Ventana_
Eli_mes1.csv .
30Published in Transactions on Machine Learning Research (05/2023)
0 25 50 75 100
Epoch0.650.700.750.800.85Validation Error
λ= 5e−05
0 25 50 75 100
Epoch0.650.700.750.800.85
λ= 0.0001
0 25 50 75 100
Epoch0.650.700.750.800.85
λ= 0.0005
prox-sps, constant, α0=10.0
prox-sps, constant, α0=5.0sps, constant, α0=10.0
sps, constant, α0=5.0sgd, constant, α0=5.0
sgd, constant, α0=1.0adamw, constant, α0=0.001
0 25 50 75 100
Epoch10203040/bardblxk/bardbl
λ= 5e−05
0 25 50 75 100
Epoch10203040
λ= 0.0001
0 25 50 75 100
Epoch1015202530
λ= 0.0005
prox-sps, constant, α0=10.0
prox-sps, constant, α0=5.0sps, constant, α0=10.0
sps, constant, α0=5.0sgd, constant, α0=5.0
sgd, constant, α0=1.0adamw, constant, α0=0.001
Figure 13: Matrix Completion : Validation error (top) and model norm (top) for three values of the regular-
ization parameter λ. Validation error is plotted as ﬁve-epoch running median. Shaded area is two standard
deviations over ten independent runs.
10−4
λ6.2×10−16.4×10−16.6×10−16.8×10−17×10−17.2×10−17.4×10−1Validation Error
adamw, constant, α0=0.001
prox-sps, constant, α0=5.0
prox-sps, constant, α0=10.0sgd, constant, α0=1.0
sgd, constant, α0=5.0sps, constant, α0=5.0
sps, constant, α0=10.0
(a)
10−610−410−2100102
λ10−610−510−410−310−210−1100101minψ−1
N/summationtextN
i=1/parenleftbig
infzfi(z) +λ
2/bardblz/bardbl2/parenrightbig
 (b)
Figure 14: (a) Matrix Completion : Validation error as a function of the regularization parameter λ. Shaded
area is one standard deviation (computed over ten independent runs). For all values, we take the median
over epochs [90,100]. (b) Interpolation constant for a ridge regression problem for varying regularization
parameterλ. See Appendix D.5 for details.
If one would apply the theory of SPSmaxfor the regularized loss functions /lscriptiwith estimates /lscripti= 0, the
constantσ2=/parenleftbig
minx∈Rnf(x) +ϕ(x)/parenrightbig
−1
N/summationtextN
i=1infz/lscripti(z)determines the size of the constant term in the
convergence results of (Loizou et al., 2021; Orvieto et al., 2022). We compute minx∈Rnf(x) +ϕ(x)by
solving the ridge regression problem. Further, the minimizer of /lscriptiis given by (aia/latticetop
i+λId)−1aibi. We plot
σ2for varying λin Fig. 14b to verify that σ2grows signiﬁcantly if λbecomes large (even if the loss could
31Published in Transactions on Machine Learning Research (05/2023)
0 20 40
Epoch0.00.20.40.60.81.0Validation Accuracyλ= 5e−06
0 20 40
Epoch0.00.20.40.60.81.0λ= 5e−05
0 20 40
Epoch0.00.20.40.60.81.0λ= 0.0005
adamw, constant, α0=0.001 sps, sqrt,α0=1.0 prox-sps, sqrt, α0=1.0 sgd, sqrt,α0=1.0
0 20 40
Epoch150200250300350/bardblxk/bardblλ= 5e−06
0 20 40
Epoch110115120125130135λ= 5e−05
0 20 40
Epoch4648505254λ= 0.0005
adamw, constant, α0=0.001 sps, sqrt,α0=1.0 prox-sps, sqrt, α0=1.0 sgd, sqrt,α0=1.0
Figure 15: ResNet110 forImagenet32 : Validation accuracy as ﬁve-epoch running median (top) and model
norm (bottom) for three values of λ.
be interpolated perfectly, i.e. infxf(x) = 0). We point out that the constant σ2does not appear in our
convergence results Theorem 7 and Theorem 8.
32Published in Transactions on Machine Learning Research (05/2023)
Figure 16: ResNet110 forImagenet32 : Adaptive step sizes for SPSand ProxSPS. See deﬁnition of ζkin
Section 5.1.
33