Published in Transactions on Machine Learning Research (03/2024)
Learning Sparse Graphs for Functional Regression using
Graph-induced Operator-valued Kernels
Akash Saha akashsaha@iitb.ac.in
IEOR, IIT Bombay
Mumbai, India
P. Balamurugan balamurugan.palaniappan@iitb.ac.in
IEOR, IIT Bombay
Mumbai, India
Reviewed on OpenReview: https: // openreview. net/ forum? id= f9l4eiPKpV
Abstract
A functional regression problem aims to learn a map F:Z/mapsto→Y, whereZis an appropriate
input space andYis a space of output functions. When Zis also a space of functions, the
learning problem is known as function-to-function regression. In this work, we consider the
problem of learning a map of the form F:Zp/mapsto→Y, a many-to-one function-to-function
regression problem, where the aim is to learn a suitable Fwhich maps pinput functions to
an output function. In order to solve this regression problem with pinput functions and a
corresponding output function, we propose a graph-induced operator-valued kernel (OVK)
obtained by imposing a graphical structure describing the inter-relationships among the p
input functions. When the underlying graphical structure is unknown, we propose to learn
anappropriateLaplacianmatrixcharacterizingthegraphicalstructure, whichwouldalsoaid
in learning the map F. We formulate a learning problem using the proposed graph-induced
OVK, and devise an alternating minimization framework to solve the learning problem. To
learnFalong with meaningful and important interactions in the graphical structure, a min-
imax concave penalty (MCP) is used as a sparsity-inducing regularization on the Laplacian
matrix. We further extend the alternating minimization framework to learn F, where each
of thepconstituent input functions as well as the output function are multi-dimensional.
To scale the proposed algorithm to large datasets, we design an eﬃcient sample-based ap-
proximation algorithm. Further, we provide bounds on generalization error for the map
obtained by solving the proposed learning problem. An extensive empirical evaluation on
both synthetic and real data demonstrates the utility of the proposed learning framework.
Our experiments show that simultaneous learning of Falong with sparse graphical struc-
ture helps in discovering signiﬁcant relationships among the input functions, and motivates
interpretability of such relationships driving the regression problem.
1 Introduction
Learning to predict functional output from a suitable input is characterized as a functional regression prob-
lem, which aims at learning a function-valued function F:Z→Y, whereZis an appropriate input space
andYis an output space of functions. In many scenarios, multiple inputs decide the value of an output,
which gives rise to functional regression problems of the form F:Zp→Y, wherepis the number of inputs
considered. Even more interesting is the case where interactions among the pinputs can be used in a precise
manner to predict y∈Y. In particular, we consider Zto be a space of functions, hence learning a map
F:Zp→Yis called a many-to-one function-to-function regression problem. Without loss of generality,
we refer to this many-to-one function-to-function regression problem as the functional regression problem
considered throughout this paper. Applications of this type of problems can be found in weather forecast-
1Published in Transactions on Machine Learning Research (03/2024)
Figure 1: Illustrative example of a functional regression problem, where z1,z2,z3represent atmospheric
pressure at 3 stations in a region, graph Gdepicts the inter-relationships among z1,z2,z3andyrepresents
the average temperature of the region. Fmapsz1,z2,z3toyincorporating G.
ing where diﬀerent weather parameters in stations measured at multiple timepoints across a month can be
characterized as functional inputs used to determine the average rainfall as a time-varying function in that
month. Similarly, emissions from a factory in a day can be predicted as a function of time, based on the
functional data obtained from readings of diﬀerent components involved in the manufacturing process at
diﬀerent timepoints in that day. In sports analytics, the movement data of diﬀerent players throughout the
game can let us know the inﬂuence of a particular strategy in ball possession/movement as a functional
output over the duration of the game. Thus in all these applications, we notice situations where a set of
input functions interact to produce an output function. Even though digital data is discrete, systems where
the inherent data produced is smooth and continuous by nature, can be modeled as functions over a suitable
domain (Ramsay, 1982) to leverage the variations based on that domain.
Consider a simple functional regression problem illustrated in Figure 1, where input functions z1,z2,z3∈Z
denote the atmospheric pressure measurements of 3 nearby weather stations and the output function y∈Y
denotes the average temperature of the region, throughout a particular day. For predicting y, considering
the input functions z1,z2andz3without any relation among them may be restrictive as inherent relations
betweentheinputfunctionsmaydictatethegenerationof y. Inordertocaptureinteractionsamong z1,z2,z3,
we introduce a graph structure Gbetweenz1,z2andz3in Figure 1, where the nodes of Grepresentzi’s and
the edges depict potential relations among them. The graph structure Gwill be useful in representing the
inﬂuences and inter-relations among zi’s, which can be useful in the prediction of y∈YusingF. We propose
a framework for combining the impact of z1,z2,...,zpwith the additional information of Gto predicty. In
determining the output function y, the graphical structure Gon the input functions may be known from
domain knowledge and can possibly be directly incorporated to learn F. A more interesting case is when G
is unknown and needs to be learned along with F. Learning the graph structure Gwould help to discover
interactions among zi’s which facilitate predicting y. When the number of input functions z1,z2,...,zp
grow larger, the associated graph Gmight also become dense with many edges and incorporating such dense
Gmight lead to computational diﬃculties and would also lead to spurious connections/edges which lack
interpretability. Thus, learning a sparse graphical structure Gon input functions becomes instrumental in
understanding the signiﬁcant relationships that drive the functional regression problem to predict the output
function.
2Published in Transactions on Machine Learning Research (03/2024)
In this work, we consider kernel methods to learn the map F:Zp→Y, either using a priori knowledge of
Gor by learning Galong with F. Kernel methods have been a popular class of methods that use kernel
functions to associate inputs to a higher dimensional feature space and ﬁnd applications in classiﬁcation,
clustering and regression (Shawe-Taylor et al., 2004). For a simple scalar-valued regression problem, the
aim is to learn f:X→R, whereXis an appropriate input space of vectors. A scalar-valued kernel
k:X×X→Rassociates two inputs in Xto a real number which provides a measure of similarity between
those inputs. Scalar-valued kernels which are positive semi-deﬁnite are associated to a (unique) space Hof
candidate functions fmapping input space XtoR,Hbeing referred to as a reproducing kernel Hilbert space
(RKHS). The function fto be learned resides in the aforementioned space Hof functions which enables
us to formulate a regularized loss minimization problem over H(Shawe-Taylor et al., 2004), whose solution
can be used to predict the desired output for input samples. On the other hand, for a functional regression
problem, an extension of scalar-valued kernel to operator-valued kernel (OVK) of the form K:Z×Z→L (Y)
associates two input functions to a bounded linear operator on the output space of functions instead of a
real number (Kadri et al., 2016; Saha & Palaniappan, 2020). An operator-valued kernel Kwhich is positive
semi-deﬁnite is associated to a (unique) space HKof candidate functions Fmapping input space ZtoY,HK
being called a function-valued reproducing kernel Hilbert space (Kadri et al., 2016). Similar to scalar-valued
kernel setting, in the operator-valued kernel setting too, a regularized loss minimization learning problem
can be formulated in the function-valued RKHS HK. The reproducing property of OVKs (Deﬁnition A.1)
helps in reformulating the learning problem in the output space Y, enabling algorithms to be developed for
solving the resultant learning problem.
In this paper, we consider another natural extension of this kernel-based framework which leads to a case
wherepinput functions and their interactions among themselves (captured by a graph G) can be used to
predicttheoutputfunction. Tolearn F:Zp→Y, weincorporate pinputfunctionsalongwiththeunderlying
graphical structure to create a graph-induced operator-valued kernel that induces a corresponding function-
valued RKHS to facilitate learning of F. In order to use graph-induced OVKs in the task of functional
regressionbasedon pinputfunctionswithunknowngraphicalstructure,weproposetolearn Fsimultaneously
along with the graphical structure. Predicting output function based on a graph-induced OVK constructed
using graphical structure Goverpinput functions, to our knowledge, is a novel problem and has not been
explored much in literature. In this context, we outline our major contributions below.
Contributions: We aim to address the following objectives in this work.
•We propose a graph-induced OVK for solving a functional regression problem with pinput functions
z1,z2,...,zpand their interactions represented using a graphical structure (known/unknown) to
predict a corresponding output function y. This is enabled by Proposition A.1 considering the
Laplacian matrix (L)of the underlying graph G.
•For practical scenarios where the underlying graphical structure is unknown, we provide a construc-
tion of graph-induced OVK and propose to jointly learn the functional regression map F, along with
the graphical structure Grepresented by LandD, whereDis a diagonal matrix with non-negative
entries signifying the impact of individual input functions on y. A regularized loss minimization
problem is formulated using L,DandFin the function-valued RKHS associated with the graph-
induced OVK.
•We propose an alternating minimization framework for solving the designed regularized loss mini-
mization problem to learn L,Dand the map F. The functional regression map Fis learned for a
ﬁxedLandDby adapting Operator based Minimum Residual (OpMINRES) algorithm (Saha &
Palaniappan, 2020) to solve a linear operator system associated with the graph-induced OVK. For
a ﬁxedF, matricesLandDare learned using projected gradient descent. To learn sparse L, we
introduce minimax concave penalty (MCP) (Ying et al., 2020) as a sparsity-inducing regularization
on the Laplacian matrix L.
•We further extend the proposed alternating minimization framework to solve a multi-dimensional
functional regression problem, where each input function zi∈Z,i∈{1,2,...,p}andy∈Yare
multi-dimensional.
3Published in Transactions on Machine Learning Research (03/2024)
•Inordertoscaletheproposedalternatingminimizationframeworktohandlelargedatasets,wedesign
an eﬃcient sample-based approximation algorithm which enables to solve the learning problem over
only a carefully chosen subset of training samples.
•We establish bounds on generalization error for the map Fobtained by solving the proposed learning
problem. Our generalization analysis also incorporates the learning of graph-induced OVK.
•An extensive empirical evaluation on both synthetic and real data has been carried out and the
comparison results demonstrate the eﬃcacy of the proposed learning framework. Further our exper-
iments show that simultaneous learning of sparse graphical structure along with the function-valued
regression map Festablishes interpretable relationships driving the functional regression problem.
2 Paper Organization
InSection3, relatedworksfromtheareasoffunctionaldataanalysis, functionalregressionandgraphlearning
are discussed. The notations used in this paper have been summarized in Section 4. The proposed framework
for solving a functional regression problem using a graphical structure is covered in Section 5. The graph-
induced OVK is introduced and a representer theorem for the learning problem with an unknown graph
structure is presented in Section 5.1. An alternating minimization framework is proposed in Section 5.1 for
jointly learning an unknown graphical structure on the functional inputs and the map F. Inducing sparsity
using a MCP regularization when learning the unknown graphical structure is also discussed in Section 5.1.
An extension of the proposed framework for solving multi-dimensional functional regression problems and
a sample-based approximation algorithm for scaling up to large training data are also presented in Section
5.1. In Section 6, the bounds on generalization error of the learned map Fare established by incorporating
learning of graph-induced OVK. Experiments using the proposed alternating minimization framework and
comparative results have been illustrated in Section 7. Section 8 provides the conclusion of the paper.
3 Related Work
As our work lies in the conﬂuence of various areas of research, we divide the related work based on the
diﬀerent areas below.
Functional Data Analysis : Continuous functions over a time interval have been explored as the central
part of functional data analysis (FDA) in (Ramsay, 1982) and (Ramsay & Dalzell, 2018). FDA techniques
have evolved signiﬁcantly with non-parametric approaches (Ferraty & Vieu, 2006) and functional principal
component analysis (FPCA) (Happ & Greven, 2018) becoming prevalent tools. These approaches have found
applications in sparse longitudinal data (Yao et al., 2005), classiﬁcation involving functional data (Rossi &
Villa, 2006) and clustering for multivariate functional data (Jacques & Preda, 2014).
Functional Regression : In the context of functional regression, Oliva et al. (2015) uses projections on
orthonormal basis systems for input and output spaces to estimate regression maps based on random basis
functionsfromrandomFourierfeatures(Rahimi&Recht,2007). Operator-valuedkernelmethodshavefound
applicationsforvector-valueddata(Micchelli&Pontil,2005)andfunction-valueddata(Kadrietal.,2016)for
solving corresponding vector-valued and functional regression problems. The construction of a positive semi-
deﬁniteoperator-valuedkernelusedtolearnafunction-valuedmappinginacorrespondingreproducingkernel
Hilbert space (RKHS) is considered in (Kadri et al., 2016), while Saha & Palaniappan (2020) uses indeﬁnite
operator-valuedkernelstolearnafunction-valuedfunctioninacorrespondingreproducingkernelKreinspace
(RKKS). Hullait et al. (2021) uses a robust functional linear regression model based on robust FPCA (Bali
et al., 2011) to predict a response function using a predictor function without considering multiple input
functional data and their graphical structure. Another approach in (Bouche et al., 2021) uses kernel-based
projection learning with a ﬁnite (not necessarily orthogonal) basis for the output space. High dimensional
functional data has been used in (Gahrooei et al., 2020) to perform function-to-function regression for a
functional response output using a linear combination of functional inputs considered as covariates. However,
associationsamongthefunctionalinputshavenotbeenconsideredin(Gahrooeietal.,2020). Functionaldeep
learning methods have been developed to solve regression problems using functional direct neural network
4Published in Transactions on Machine Learning Research (03/2024)
and functional basis neural network (Rao & Reimherr, 2023). In functional direct neural network, continuous
neurons interact with learned weight functions, whereas in functional basis neural network, basis functions
are used to encode the continuous neurons as well as weight functions. Both functional direct neural network
and functional basis neural network in (Rao & Reimherr, 2023) require large amount of data for training.
On the contrary, our work is related to a setting with limited number of training samples which is useful in
various practical scenarios where data availability is restricted.
Graph Learning : Learning graph structure has been an active area of research in machine learning.
In (Dong et al., 2016), a Laplacian matrix corresponding to the graph structure on the observed input
signals is learned by using a vectorized optimization problem with a smoothness assumption over the signals.
The graph Laplacian learning algorithm is based on an alternating minimization scheme for learning the
Laplacian matrix as well as the missing/noisy signals. Pu et al. (2021b) extends this idea by using a kernel-
based learning problem for determining the Laplacian matrix of a graph structure for smooth input signals.
Kernels are used to learn relationships between the input signals as well as the inter-relationships between
covariates such as timestamp of recording an observation. Another popular approach for graph structure
learning in (Qiao et al., 2019) is based on estimation of precision matrix (inverse of covariance matrix) for
functionaldatacorrespondingtothe pnodesinthegraph, assumingthatthedataarisesfroma p-dimensional
multivariate Gaussian process. A diﬀerential functional graphical model has been considered in (Zhao et al.,
2019) which learns a diﬀerential graph to characterize the diﬀerence between conditional dependencies of two
diﬀerent populations which is determined using their respective samples. Extending the idea of determining
precision matrix for capturing conditional dependence between the nodes, Qiao et al. (2020) proposes doubly
functional graphical models to capture the evolving conditional dependence relationship among the sampled
functions corresponding to the nodes. Instead of learning a single graph based on the data, Pu et al. (2021a)
learns a graph topology with topological diﬀerence variational autoencoder for graph learning.
A motivating work (Gómez et al., 2021) addresses function-to-function linear regression problem with both
known/unknown directed graph structure where the main focus is on root cause analysis and the node
representing output function is also a part of the graphical structure containing nodes corresponding to
the input functions. The graph structure considered is learned based on a neighborhood selection method
(Meinshausen&Bühlmann,2006)todeterminethesetofcandidateparentsforeachnodeinordertosolvethe
linear regression problem. Multivariate Gaussian processes have been used with basis functions in (Gómez
et al., 2021) to model a directed acyclic graph which enables solving a function-to-function linear regression
problem. However, in our approach, the goal of learning an undirected graph structure only on the pinput
functions is diﬀerent from the parent-based directed acyclic graph structure assumption in (Gómez et al.,
2021). Moreover, our OVK based approach helps to learn non-linear relations in comparison to the linear
model considered in (Gómez et al., 2021).
4 Notations
We consider a functional regression problem with the input space as X= (L2([0,1]))pand the output space
asY=L2([0,1]), wherep∈NandL2([a,b])denotes the space of equivalence classes of square integrable
functions on [a,b],a,b∈Randa < b. The notation [n]denotes the the set {1,2,...,n}, forn∈N. In
order to denote elements of the input space X, we use the notation x= (x1,x2,...,xp)∈X. We denote
the graphical structure over the input functions x1,x2,...,xpasG= (V,E), whereVis the set of pvertices
corresponding to the functional input variables x1,x2,...,xp. The degree of a vertex v∈Vis denoted by
deg(v).Krefers to an OVK mapping from X×XtoL(Y), whereL(Y)is the set of bounded linear operators
over the output space Y. For a matrix M∈Rk×k, wherek∈N,Mi,jdenotes the (i,j)-th element of Mand
Mi∈Rkdenotes the i-th column of M. Hence, we refer to matrix Mas[Mi,j]k
i,j=1or[M1,M2,...,Mk]. The
transpose of Mis denoted by M/latticetop. The notation diag (d1,d2,...,dp)denotes ap×pdiagonal matrix with
the diagonal entries as d1,d2,...,dp, wheredi∈R, fori∈[p]. For a matrix D∈Rp×p,Diag (D)denotes
thep×1vector containing the diagonal elements of D. Based on the context, other relevant notations will
be introduced in the paper as required and suitable descriptions will be provided for them.
5Published in Transactions on Machine Learning Research (03/2024)
5 Functional Regression based on a Graphical Structure
In this section, we introduce the functional regression problem with the aim of incorporating the graph
structure to aid the regression task. To model the graphical structure, we assume that G= (V,E)represents
an undirected graph where Vdenotes the node set with |V|(=p)nodes and Edenotes the edge set of
G. Letx(t) = (x1(t),x2(t),...,xp(t))denote the functional variables for a given domain t∈ Twhere
eachxiis represented by node vi∈V, fori∈[p]. Recall that L2([a,b])denotes the space of equivalence
classes of square integrable functions on [a,b], a,b∈Randa<b. For simplicity, we assume xi∈L2([0,1]),
hence/integraltext1
0x2
i(t)dt<∞,i∈[p]. We discuss the realistic case of functional regression with an unknown graph
structure here. For a related discussion on the case of a known graph structure, we refer the reader to
Appendix A.1.
5.1 Learning with Unknown Graph Structure
Consider a system where a set of input functions determines the output (or response) function. Let the
system be modeled based on pinput functional variables x1(t),x2(t),...,xp(t), wherexi∈L2([0,1]), i∈[p].
A functional response variable y(t)is used to model output of the system where y∈L2([0,1]). (Note that
[0,1]can be replaced with any closed time interval based on the application.)
The undirected graph structure of the functional input variables is represented by a suitable graph G=
(V,E), whereV={v1,v2,...,vp}andE={{vi,vj}|viis connected to vj,1≤i,j≤p}is the edge set which
characterizes the underlying relationship between the variables. Note that the notation for an edge uses
an unordered pair {vi,vj}which characterizes the undirected nature of the graph G. In order to model
the relation between functional input variables x1,x2,...,xpand functional response variable y, we use the
following map F:
y=F(x1,x2,...,xp,G). (1)
Note thatFnow depends explicitly on the graph Gin addition to the input functions x1,x2,...,xp.
For most problems in real life, the underlying graphical structure encoding the inter-relationships among
the input functions x1,x2,...,xpis not known. In such situations, the underlying graph on the input
functions has to be learned with simultaneous prediction of the functional response variable y∈Yusingx=
(x1,x2,...,xp)∈X. Consideringanundirectedsimplegraphstructureonthefunctionalvariablesmayresult
in encountering an integer programming based optimization problem which will lead to a computationally
harder problem in addition to the functional regression task. We consider a relaxation in this aspect by
allowing weighted undirected simple graphs in our approach. With a slight abuse of notation, let the graph
structure be given by G= (V,W ), where|V|=pandW∈Rp×pis symmetric with wi,j≥0, where
wi,j= 0whenever vertices viandvjare not connected and wi,j>0denotes the weight assigned to the
edge between viandvj. Hence the graph Laplacian matrix can be represented as L=D−W, where
D=diag(D1,1,D2,2,...,Dp,p)withDi,i=/summationtextp
j=1wi,j. It can be shown that Lis positive semi-deﬁnite by
virtue of being diagonally dominant with non-negative diagonal entries (Golub & Van Loan, 1996).
We consider the notations x= (x1,x2,...,xp)∈X(= (L2([0,1]))p)andy∈Y(=L2([0,1]))to represent
an arbitrary sample (x,y). To learn the mapping F, consider the training data of nsamples given as/braceleftbig
(x(i),y(i))/bracerightbign
i=1, wherex(i)= (x(i)
1,x(i)
2,...,x(i)
p)∈Xandy(i)∈Y. In order to learn F, we develop an
operator-valued kernel which can leverage the structural information of G.
Deﬁnition 5.1 (Graph-induced Operator-valued Kernel ).A graph-induced operator-valued kernel is
deﬁned as
(KG(x,x/prime)y)(t) =k1(x,x/prime;G)/integraldisplay1
0k2(s,t)y(s)ds, (2)
wherek2is a scalar-valued kernel on R2,Gis a graph associating the pinput functions in (x1,...,xp)∈X
andk1is deﬁned as
k1(x,x/prime;G) =e−γ(x−x/prime)/latticetop(L+D)(x−x/prime), γ > 0,
6Published in Transactions on Machine Learning Research (03/2024)
whereLis the Laplacian matrix of graph GandDis a diagonal matrix consisting of non-negative entries.
KGassociates a pair x,x/prime∈Xwith output function y∈YwhereGis the graph which incorporates the
interactionsof pconstituentinputfunctionsof xandx/prime.k2insidetheHilbert-SchmidtIntegral(HSI)operator/integraltext1
0k2(s,t)y(s)dsis a scalar-valued kernel on R×R. The radial basis function (RBF) type kernel construction
ofk1involvesL+D, where the addition of the Laplacian Lwith a diagonal matrix Dwith non-negative
entries results in a diagonally perturbed Laplacian matrix (Bapat et al., 2001) (see (Kurras et al., 2014;
Aliakbarisani et al., 2022) for applications of perturbed Laplacians). If k1is positive semi-deﬁnite and if k2
is positive semi-deﬁnite (implying that the HSI operator is positive semi-deﬁnite), then the construction in
(2) is known to be positive semi-deﬁnite (Kadri et al., 2016). The addition of diagonal matrix D(with non-
negative entries) to Lin (2) preserves the positive semi-deﬁniteness of the kernel k1. Note that the notation
of the form x/latticetopLx/primeused ink1denotes an inner product structure given by x/latticetopLx/prime=/summationtextp
i,j=1/integraltext1
0xi(t)Lijx/prime
j(t)dt
(see Appendix A.1 for details regarding this inner product structure).
Without loss of generality, henceforth we refer to the graph-induced operator-valued kernel KGasKfor
simplicity. The matrix L= [Li,j]p
i,j=1satisﬁes the conditions: L1= 0andLi,j=Lj,i≤0,∀i/negationslash=j. Note
that the graph-induced operator-valued kernel (K(x,x/prime)y)(t) =k1(x,x/prime;G)/integraltext1
0k2(s,t)y(s)dsas deﬁned in
(2) withk1(x,x/prime;G) =e−γ(x−x/prime)/latticetop(L+D)(x−x/prime), γ > 0,x,x/prime∈X,y∈Yis positive semi-deﬁnite (see proof
of Proposition A.1 in Appendix A.1), which ensures that there exists a unique function-valued RKHS HK
induced by K(Theorem 1 (Kadri et al., 2016)). Now to simultaneously learn Fincluding the graph structure
represented by LandD, the following optimization problem is formulated:
/tildewideF,/tildewideL,/tildewideD= arg min
F∈HK,L∈L,D∈Dn/summationdisplay
i=1/bardbly(i)−F(x(i))/bardbl2
Y+λ/bardblF/bardbl2
HK+ρLn/summationdisplay
i=1x(i)/latticetopLx(i)+ρD/bardblD/bardbl2
F,(3)
whereL={L∈Rp×p|L1= 0,Li,j=Lj,i≤0,∀i/negationslash=j}denotes the set of all matrices satisfying the
constraints associated with Laplacian matrices of the graph G= (V,W )withW= [wi,j]p
i,j=1. Note that
D={D∈Rp×p|Di,i≥0,Di,j= 0,∀i/negationslash=j}denotesthesetofalldiagonalmatriceswithnon-negativediagonal
entries,/bardbl./bardblFis the Frobenius norm and λ,ρL,ρD>0. The regularization of Din (3) using Frobenius norm
provides control on the values in D. Note the absence of a Frobenius norm based regularizer for Lin (3).
A diﬀerent smoothness term x/latticetopLx=1
2/summationtextp
i,j=1wi,j/bardblxi−xj/bardbl2
Yis considered (a similar term is considered in
(Humbert et al., 2021)), which provides an improved interaction-based data-oriented regularization instead
of using a simple matrix norm based regularization of L.
Using the representer theorem A.2 and the reproducing property of OVK K, the minimization problem (3)
is equivalently reduced to be in terms of u∈Yninstead ofF∈HKas follows (see Appendix A.2):
min
F,L,DJ(F,L,D ) =n/summationdisplay
i=1/bardbly(i)−F(x(i))/bardbl2
Y+λ/bardblF/bardbl2
HK+ρLn/summationdisplay
i=1x(i)/latticetopLx(i)+ρD/bardblD/bardbl2
F (4)
=⇒min
u,L,DJ(u,L,D ) =n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)−n/summationdisplay
j=1K(x(i),x(j))uj/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y+λn/summationdisplay
i,j=1/angbracketleftK(x(i),x(j))ui,uj/angbracketrightY (5)
+ρLn/summationdisplay
i=1x(i)/latticetopLx(i)+ρD/bardblD/bardbl2
F.
To solve (3) (or (5) equivalently) we now propose an alternating minimization framework where J(u,L,D )
is optimized alternatively with respect to u∈Yn,L∈LandD∈D. We now discuss the steps involved in
alternating minimization of J(u,L,D ).
5.1.1 Minimization with respect to ufor ﬁxedL,D
Assuming ﬁxed L,D, and from the reproducibility property of Kand representer theorem A.2, J(u,L,D )
from (5) simpliﬁes to the following system of linear operator equations in u(see Appendix A.8):
(K+λI)u=y, (6)
7Published in Transactions on Machine Learning Research (03/2024)
where Ki,ju=K(x(i),x(j))u=k1(x(i),x(j);G)¯k2(u),∀u∈Ywithk1(x,x/prime;G) =e−γ(x−x/prime)/latticetop(L+D)(x−x/prime),
¯k2(u)(t)is deﬁned using an exponential kernel in (2) as ¯k2(u)(t) =/integraltext1
0e−γop|s−t|u(s)ds,γop>0,s,t∈R,
u= [u1,u2,...,un]/latticetop∈Ynandy= [y(1),y(2),...,y(n)]/latticetop. In our framework, we consider a particular choice
of kernelk2onR2ask2(s,t) =e−γop|s−t|,γop>0. The OpMINRES algorithm (Saha & Palaniappan,
2020) solves the system (6) by using an iterative Krylov subspace minimal residual method. Consider
P:=K+λI, OpMINRES minimizes /bardbly−Pu/bardblYn, where the norm is deﬁned as /bardblξ/bardblYn=/radicalBig/summationtextn
i=1/integraltext1
0ξ2
i(t)dt,
forξ= (ξ1,ξ2,...,ξn)∈Yn. The steps involved in k-th iteration of OpMINRES algorithm are given below:
1. Transforming the linear operator system (K+λI)u=yinto a linear system in Rkusing a Lanczos-
based method (Lanczos, 1950), called Operator-valued Lanczos (or OpLanczos) scheme.
2. Solving the linear system of the previous step using QR decomposition.
3. Transforming the result obtained in step 2 appropriately to retrieve a solution in Yn.
Using the Krylov subspace Kk(P,y) =span{y,Py,P2y,...,Pk−1y}obtained at the k-th iteration, Op-
MINRES obtains an approximation ukto the original solution using the following:
uk= arg min
θ∈Kk(P,y)/bardbly−Pθ/bardblYn. (7)
The problem in (7) is transformed into a problem in Rkby using OpLanczos method. The OpLanczos
method at the k-th iteration, tridiagonalizes Pto get PQk=QkTk, whereTkhas a tridiagonal structure
given by
Tk=
α1β2 0
β2α2β3
β3α3...
......βk−2
βk−1αk−1βk
0 βkαk
, (8)
andQk= [q1,q2,...,qk], where the qi’s belonging toYnare orthonormal and q1is generally assumed to be
y//bardbly/bardblYn. Further, the relation PQk=Qk+1Tkis also satisﬁed for a suitably deﬁned Tk. UsingQk,θ∈Yn
can be written as θ=Qkϑfor an appropriate ϑ∈Rk. Hence we have:
min
θ∈Kk(P,y)/bardbly−Pθ/bardblYn= min
ϑ∈Rk/bardbly−PQkϑ/bardblYn= min
ϑ∈Rk/bardbly−Qk+/one.taboldstyleTkϑ/bardblYn (9)
= min
ϑ∈Rk/bardblQk+1(β1e1−Tkϑ)/bardblYn, (10)
(whereβ1=/bardbly/bardblYn,e1= [1,0,..., 0]/latticetopandq1=y//bardbly/bardblYn)
= min
ϑ∈Rk/bardblβ1e1−Tkϑ/bardbl2.(where/bardbl./bardbl2is the standard Euclidean norm.) (11)
Equation (10) reduces to (11) owing to the orthonormality of {q1,q2,...,qk+1}(columns of Qk+1). Solving
forϑk= arg min ϑ∈Rk/bardblβ1e1−Tkϑ/bardbl2is done using QR decomposition. Now, the transformation from Rk
back toYnto obtain ukis achieved using the following: uk=Qkϑk=Qk/parenleftbig
arg minϑ∈Rk/bardblβ1e1−Tkϑ/bardbl2/parenrightbig
. In
summary, we note that the minimization of Jwith respect to uis obtained by using OpMINRES for ﬁxed
LandD. Now, we proceed to the next step in the alternating minimization of J(u,L,D ).
5.1.2 Minimization with respect to Lfor ﬁxed u,D
The minimization of J(u,L,D )with respect to Lfor ﬁxed u,Dis simpliﬁed by considering the symmetry
ofL∈ L. To simplify the computations, we introduce vectorization of matrices and half-vectorization
8Published in Transactions on Machine Learning Research (03/2024)
of symmetric matrices (Henderson & Searle, 1979). For a matrix Z= [Zi,j]q
i,j=1∈Rq×qforq∈N, the
vectorization of Zis deﬁned as
vec(Z) = [Z1,1,...,Zq,1,Z1,2,...,Zq,2,...,Z 1,q,...,Zq,q]/latticetop.
The half-vectorization of a symmetric matrix Z= [Zi,j]q
i,j=1∈Rq×qis the vectorization of the lower trian-
gular part of Zgiven by
vech(Z) = [Z1,1,...,Zq,1,Z2,2,...,Zq,2,...,Zq−1,q−1,Zq,q−1,Zq,q]/latticetop.
By introducing vectorization and half-vectorization of L, we reduce the minimization of J(u,L,D )with
respect to matrix Linto minimization with respect to the vector vech (L). To tackle the constraint set L, we
can reduce it to a simpler form by using half-vectorization and vectorization of Lgiven by vech (L)∈Rp(p+1)
2
and vec (L)∈Rp2, respectively. The following relations are used to relate vech (L)and vec (L)using an
appropriate transformation matrix called duplication matrix M:
Mvech(L) =vec(L),whereM∈Rp2×p(p+1)
2.
The constraint set Lcan then be rewritten as Avech(L) = 0,Bvech(L)≤0, whereAandBare matrices
which handle L1= 0andLi,j≤0,i/negationslash=j, respectively (Dong et al., 2016; Pu et al., 2021b). The construction
and properties of A,BandMcan be found in Appendix A.5. For notational simplicity, we consider a
slight abuse of notations when referring to function J(u,L,D )to be equivalent to J(u,vech(L),vech(D))
as vech (L)and vech (D)can be used to represent LandD, respectively. Similarly, when considering ﬁxed
u,vech(L)or vech (D), we denote the function Jas a function of non-ﬁxed entities, without referring to the
ﬁxed variables. For example, in the current step, since vech (L)is the non-ﬁxed entity and u,vech(D)are
ﬁxed, we denote J(u,vech(L),vech(D))simply asJu,D(vech(L)). We employ a projected gradient descent
procedure to solve minJu,D(vech(L)). The (k+ 1)-th iterate vech (L)k+1is obtained from the k-th iterate
vech(L)kby the following projected gradient descent step :
vech(L)k+1= ΠL(vech(L)k−ηL∇vech(L)Ju,D(vech(L)k)), (12)
whereηL>0is the learning rate for the descent step, ΠLdenotes the projection operator onto the set L.
The expression for gradient term ∇vech(L)Jhas been derived in Appendix A.6. For a ﬁxed ˆz∈Rp(p+1)
2, the
projection operator ΠLis deﬁned as follows:
ΠL(ˆz) = arg min
z∈Rp(p+1)
2/bardblz−ˆz/bardbl2such thatAz= 0,Bz≤0. (13)
The projection operator deﬁned in (13) ensures that vech (L)obtained satisﬁes the constraints of a Laplacian
matrix of a graph. Projection operator ΠLis evaluated by solving the quadratic program (13) using well-
known interior point methods (Dikin, 1967; Andersen et al., 2012).
For a large matrix L, capturing meaningful relationships between input functions becomes important; oth-
erwise, elements of Lmay contain many non-zero values which are close to each other in magnitude and
may lead to lack of interpretability. Integrating sparsity-inducing regularizers on Lwould ensure that the
most important interactions get captured and can improve the predictions for output function. The learned
sparse graphs would then become useful for interpretation. In Section 5.1.4, we discuss about incorporating
sparsity-inducing regularizers in the L-based minimization problem minJu,D(vech(L)). Now that projected
gradient descent is proposed for minimization of Jwith respect to Lfor a ﬁxed uandD, we proceed with
a similar approach for the next step of the alternating minimization framework.
5.1.3 Minimization with respect to Dfor ﬁxed u,L
Similartotheminimizationof J(u,L,D )withrespectto Lforﬁxed u,D, asdiscussedintheprevioussection,
we proceed with the simpliﬁcation of D∈Dusing half-vectorization given as vech (D)∈Rp(p+1)
2to solve
minJu,L(vech(D)). We further use notation Diag (D)to denote the vector containing diagonal elements of
9Published in Transactions on Machine Learning Research (03/2024)
D. In order to deal with the constraint Di,i≥0,∀i∈[p]ofD, we construct a matrix C∈Rp×p(p+1)
2which
consists of 0’s and 1’s satisfying Cvech(D) =Diag (D). Construction of a suitable Chas been discussed in
Appendix A.5. For solving minJu,L(vech(D)), we use projected gradient descent steps given by:
vech(D)k+1= ΠD(vech(D)k−ηD∇vech(D)J(vech(D)k)), (14)
whereηD>0is learning rate for the descent step, ΠDdenotes the projection operator onto the set D, and
vech(D)kdenotes the k-th iterate for vech (D). The required expression for gradient ∇vech(D)Jhas been
derived in Appendix A.6. The projection operator ΠDis deﬁned for ˆd∈Rp(p+1)
2as
ΠD(ˆd) = arg min
d∈Rp(p+1)
2/bardbld−ˆd/bardbl2,such thatCd≥0. (15)
The explicit solution of (15) can be easily obtained for i∈[p(p+ 1)/2], as the following:
di=/braceleftBigg
0, ifi∈[p(p+ 1)/2]\J
max( ˆdi,0),otherwise,(16)
where the setJconsists of indices jsuch that (vech(C))j= 1(see Appendix A.5).
Thus, the proposed alternating minimization framework discussed in Sections 5.1.1-5.1.3 can be summarized
in the following steps:
1.Minimization with respect to F∈HK(oru∈Yn): Solving for uin(K+λI)u=y.
2.Minimization with respect to vech(L): Projected gradient descent of Jwith respect to vech (L)
such thatL∈L.
3.Minimization with respect to vech(D): Projected gradient descent of Jwith respect to vech (D)
such thatD∈D.
5.1.4 Sparsity Inducing Regularization
The functional regression problem aims at predicting the output function based on the interactions and
inﬂuences of the input functions. In a large-scale setting with numerous input functions inﬂuencing the
output function, providing each interaction of a pair of input functions with similar weights may hamper
the prediction capability as well as interpretability of the proposed graph-induced operator-valued kernel
method. Sparse graphs ensure a focus on picking more pivotal associations between the pairs of input
functions which drive the functional regression problem, as noisy interactions are given negligible weights
and signiﬁcant interactions contribute more to the prediction. A popular method to obtain sparsity on the
graph structure is to consider the trace of Las a regularizer (Qiao et al., 2018). But as Lis obtained based
on solving a constrained minimization problem (13), we introduce a constraint that can regularize the values
inL(Pu et al., 2021b). To facilitate this, we consider a vector c∈Rp(p+1)
2consisting of 0’s and 1’s, such
thatc/latticetopvech(L) =trace (L)(Appendix A.5). Let mtrace(>0)be a hyperparameter controlling the trace of
Lwhich is equivalent to controlling the oﬀ-diagonal entries of weight matrix Wcorresponding to the graph
learned. As our formulation requires solving a quadratic program in the projected gradient descent for L
(Section 5.1.2), a direct way to control the trace of Linvolves modifying the quadratic program (13) to the
following:
ΠL(ˆz) = arg min
z∈Rp(p+1)
2/bardblz−ˆz/bardbl2,such thatAz= 0,c/latticetopz=mtrace,Bz≤0,
where ˆz∈Rp(p+1)
2andmtrace>0. The constraint c/latticetopz=mtraceis appended with Az= 0to obtain the
transformed quadratic program for projection operator ΠL:
ΠL(ˆz) = arg min
z∈Rp(p+1)
2/bardblz−ˆz/bardbl2,such that ˜Az=˜0,Bz≤0, (17)
10Published in Transactions on Machine Learning Research (03/2024)
Figure 2: Comparison of h(w):=MCP (w)values for diﬀerent λmcpandγmcpvalues for varying w. [Best
viewed in color]
where ˜A=/bracketleftbiggA
c/latticetop/bracketrightbigg
and˜0 =/bracketleftbigg0
mtrace/bracketrightbigg
. For our experiments the value of mtraceis considered as p/2(discussed
in Section 7) to promote sparsity in the learned graph structure. Assigning mtracethe valuep/2, allows
a cumulative weight of p/2to be distributed among the edges in the learned graph, promoting sparsity.
Though we have used mtrace =p/2for all our experiments, we showcase the impact of diﬀerent values of
mtracein Appendix A.10.5 which illustrate that mtracecan be considered as a hyperparameter.
In addition to restricting the trace of Lwithmtrace, we also utilize a sparsity-inducing regularizer in
minu,DJ(vech(L)). Nonconvex regularizers have gained popularity in recent literature (Zhang et al., 2020;
Ying et al., 2020; Vargas Vieyra, 2022) for promoting sparsity in learned graphs. These nonconvex regulariz-
ers acting on weights on individual graph edges are more eﬀective than traditional /lscript1-norm based graphical
lasso regularization which is a classical approach in graph learning (Yuan & Lin, 2007; Banerjee et al., 2008;
d’Aspremont et al., 2008). A nonconvex regularizer, minimax concave penalty (MCP) (Zhang, 2010) denoted
byhis characterized by the deﬁnition of its derivative h/primegiven by:
h/prime(w) =/braceleftBigg
λmcp−w
γmcp, w∈[0,γmcpλmcp],
0, w∈[γmcpλmcp,∞),(18)
forλmcp,γmcp>0. The above deﬁnition of h/primewithh(0) = 0ensures that his monotonically increasing in
the interval [0,γmcpλmcp]. The impact of λmcpandγmcpon value of the regularizer hhas been compared in
Figure 2. The illustration in Figure 2 shows that MCP function hmagniﬁes the small values till γmcpλmcp
whereλmcpandγmcpcontrol the slope and curvature of the truncated concave quadratic function h. This
ensures that most smaller oﬀ-diagonal values are penalized by choosing proper λmcpand the diﬀerences
between large and small values are exaggerated which promotes sparsity. Though h/primein (18) (or equivalently
h) is deﬁned over R+, we overload the notation to deﬁne h(z) = [h(z)i]d
i=1wherez∈Rd
+. Ying et al.
(2020) proposes MCP to obtain a nonconvex penalized maximum likelihood estimation method for learning
sparse Laplacian matrices for graphs. However, here we introduce MCP regularizer in the projection step
considered for vech (L). Therefore, we reformulate the minimization problem with respect to Lfor ﬁxed u
11Published in Transactions on Machine Learning Research (03/2024)
andDas follows:
min
vech(L)∈˜L
vech(L)⊿MCPJu,D(vech(L)), (19)
where the constraint set is ˜L=/braceleftBig
z∈Rp(p+1)
2/vextendsingle/vextendsingle/vextendsingle˜Az=˜0,Bz≤0/bracerightBig
and the property vech (L)⊿MCPsigniﬁes
that vech (L)is sparse in the sense of MCP regularization.
The MCP regularizer operates on the oﬀ-diagonal entries of Lto induce sparsity. To solve (19), we consider
a two-level update procedure for vech (L). In the ﬁrst level, we use Ju,D(vech(L)), to perform an iteration
of gradient descent to obtain (q+ 1)-th iterate from q-th iterate, as follows:
vech(L)q+1=vech(L)q−ηL∇vech(L)Ju,D(vech(L)q), (20)
where∇vech(L)Jdenotes the gradient of Jwith respect to vech (L)andηL>0is the learning rate for the
descent step. In the second level, we ensure that vech (L)q+1obtained in (20) is projected onto set ˜Land
is also sparse in the sense of MCP regularization captured using the property vech (L)⊿MCP. To apply
the MCP regularizer on oﬀ-diagonal entries of Lfor vech (L), we construct a matrix H∈Rp(p+1)
2×p(p+1)
2
comprising 0’s and 1’s such that Hvech(L)produces a vector having the same structure as vech (L)with 0’s
corresponding to diagonal entries of Land the same oﬀ-diagonal entries as in L(see Appendix A.5 for the
construction). Then we aim to solve the following minimization problem:
min
z∈Rp(p+1)
2/bardblz−vech(L)q+1/bardbl2+p(p+1)
2/summationdisplay
i=1(h(H(−z)))i,such that ˜Az=˜0,Bz≤0, (21)
where the negative sign before zin MCP regularization is used since hoperates on non-negative entries and
Lcontains negative weights corresponding to oﬀ-diagonal entries. Further the product H(−z)eliminates the
positive diagonal entries of L. Note that the ﬁrst term in the objective function of (21) is convex in zand
the second term is concave in z. Hence we adopt a majorization-minimization iterative approach, similar
to (Ying et al., 2020) to solve (21). In each iteration lof this approach, we obtain a majorization of the
objective function in (21) using the linearization of concave function hatzlas:
/bardblz−vech(L)q+1/bardbl2+p(p+1)
2/summationdisplay
i=1(h/prime(H(−zl)))i(H(−z))i. (22)
Then we solve a minimization step, which when combined with the linearization in (22) leads to the following
quadratic program:
zl+1= arg min
z∈Rp(p+1)
2/bardblz−vech(L)q+1/bardbl2+p(p+1)
2/summationdisplay
i=1(h/prime(H(−zl)))i(H(−z))i,such that ˜Az=˜0,Bz≤0.(23)
We solve the problem (23) iteratively till convergence to obtain a sequence of zl’s. The procedure for MCP
based sparsity-inducing regularization has been summarized in Algorithm 1. The solution obtained from
Algorithm 1 is thus a valid Laplacian matrix of a graph, and satisﬁes trace constraint as well as MCP
based sparsity-inducing regularization property captured by vech (L)⊿MCP. Note that other nonconvex
regularizers such as smoothly clipped absolute deviation (SCAD) (Fan & Li, 2001; Ying et al., 2020) can
also be used instead of MCP in our approach which can provide comparable sparse graph structures.
Using the learned u,L,D, the graph-induced operator-valued kernel is used to predict the output function
corresponding to the pinput functions ˆx= (ˆx1,ˆx2,..., ˆxp)based on (114) given by
F(ˆx) =n/summationdisplay
i=1K(x(i),ˆx)ui,whereui∈Y. (24)
12Published in Transactions on Machine Learning Research (03/2024)
Algorithm 1 MCP Regularization of L
Input:vech(L)
Output: vech(Lmcp)
Initializez0=vech(L),H
l= 0
whilestopping criteria for zlnot satisﬁed do
Findh/prime(H(−zl))
Findzl+1as the solution of (23)
l←l+ 1
end while
vech(Lmcp) =zl
Algorithm 2 summarizes the entire alternating minimization procedure with sparsity-inducing regularization.
For our experiments, vech (L)is initialized using a Laplacian matrix LwithLi,j=−mtrace/(p(p−1)),for
i/negationslash=j∈[p]andLi,i=mtrace/p,fori∈[p], which satisﬁes the mtracecondition in (17). The initial vech (D)
is considered corresponding to D=Ip, identity matrix of order p. Note that the objective function Jin
problem (19) is not jointly convex with respect to u,vech(L)and vech (D). To prove the convergence of
alternating minimization of a non-convex function Jwith respect to a heterogeneous collection of three
variables where ui’s∈Yand vech (L),vech(D)∈Rp(p+1)
2requires development of fundamental results which
is out of scope of our current work, and we aim to take this up in future. However, we observed empirical
convergence of the proposed alternating minimization framework in our experiments.
5.1.5 Extension of Learning Graph Structure for Multi-dimensional Outputs
In many scenarios, a functional regression problem involves multi-dimensional input and output functions
which requires learning multiple functions corresponding to each dimension in the output space. Consider
X= (L2([0,1]))p,Y=L2([0,1])with{(x(i),y(i))}n
i=1as the training data, where x(i)∈Xr,y(i)∈Ys, for
i∈[n]. Notice that the input space is now Xrand output space is Ys, which are both multi-dimensional.
In this case, we have x(i)= (x(i)
1,x(i)
2,...,x(i)
r)wherex(i)
j= (x(i)
j1,x(i)
j2,...,x(i)
jp)∈X,∀j∈[r]andy(i)=
(y(i)
1,y(i)
2,...,y(i)
s)wherey(i)
j∈Y,∀j∈[s]. Consider a setting where the input space Xris mapped to
output spaceYsby learning distinct maps of the from Fi:Xr→Y,fori∈[s]. Note that to ﬁnd these maps
Fi, an extension to the framework developed in Sections A.1 and 5.1 can be used, as long as the proposed
graph-induced OVK framework is adapted to handle the multi-dimensional inputs x(i). For a motivating
example, consider the data of movement of players for a ﬁxed time interval in a basketball game comprising
thexandycoordinates of the playing area belonging to the input space X2. Similarly, movement of the ball
for the ﬁxed time interval is characterized by xand ycoordinates of the court which belong to the output
spaceY2. The corresponding regression problem involves learning F= [F1,F2]/latticetop, whereF1:X2/mapsto→Yand
F2:X2/mapsto→Y.
For the multi-dimensional setting, we propose the following extension of the scalar-valued kernel based on
LandDas
k1/parenleftBig
x(i),x(j);G/parenrightBig
=e−/summationtextr
k=1/bracketleftBig
γk/parenleftbig
x(i)
k−x(j)
k/parenrightbig/latticetop(L+D)/parenleftbig
x(i)
k−x(j)
k/parenrightbig/bracketrightBig
, (25)
whereγk>0,∀k∈[r]andk1maps pair of elements from the input space Xrto a real number. Now,
k1/parenleftbig
x(i),x(j);G/parenrightbig
can be appropriately used to create a graph-based operator-valued kernel in higher dimen-
sions using the construction of a suitable OVK as follows:

K1/parenleftbig
x(i),x(j)/parenrightbig
y1(t)
...
Ks/parenleftbig
x(i),x(j)/parenrightbig
ys(t)
=k1/parenleftBig
x(i),x(j);G/parenrightBig
/integraltextb
ak1
2(t/prime,t)y1(t/prime)dt/prime
.../integraltextb
aks
2(t/prime,t)ys(t/prime)dt/prime
, (26)
13Published in Transactions on Machine Learning Research (03/2024)
Algorithm 2 Alternating Minimization of J
Input:{(x(i),y(i))}n
i=1,x(i)= (x(i)
1,x(i)
2,...,x(i)
p)∈X,y(i)∈Y
Output: u,vech(L),vech(D)
Initialize vech(L)0∈L,vech(D)0∈D
y= [y(1),y(2),...,y(n)]/latticetop
k= 0
whileTruedo
Computek1(x(i),x(j);G) =e−γRij(vech(L)k+vech(D)k),∀i,j∈[n]using (137) in Appendix A.6
Solve for uin(K+λI)u=yto obtain ukusing OpMINRES
ifstopping criterion for ukis satisﬁed then
break from the outermost while loop //exit based on convergence of uiterates
end
q= 0
whilestopping criterion for vech (L)qnot satisﬁed do
//stopping criterion is based on convergence of vech (L)iterates
vech(L)q+1=vech(L)q−ηL∇vech(L)Ju,D(vech(L)q)
vech(L)mcpobtained based on MCP regularization using vech (L)q+1as input to Algorithm 1
vech(L)q+1←vech(L)mcp
q←q+ 1
end while
vech(L)k←vech(L)q
m= 0
whilestopping criterion for vech (D)mnot satisﬁed do
//stopping criterion is based on convergence of vech (D)iterates
vech(D)m=vech(D)k
vech(D)m+1= ΠD(vech(D)m−ηD∇vech(D)Ju,L(vech(D)m))using (14)
m←m+ 1
end while
vech(D)k←vech(D)m
k←k+ 1
end while
u=uk,vech(L) =vech(L)k,vech(D) =vech(D)k
whereki
2:R×R→R, fori∈[s]are scalar-valued kernels on R2. In order to use the output functions
y(i)= (y(i)
1,y(i)
2,...,y(i)
s)for learning smaps from input space Xrto the output space Y, the problem in (3)
is extended as follows:
/tildewideF,/tildewideL,/tildewideD= arg min
F=[F1,F2,...,Fs]/latticetop∈HK,L∈L,D∈Ds/summationdisplay
l=1/bracketleftBiggn/summationdisplay
i=1/bardbly(i)
l−Fl(x(i))/bardbl2
Y+λ/bardblFl/bardbl2
Hl
K/bracketrightBigg
+r/summationdisplay
k=1ρkx(i)
k/latticetopLx(i)
k+ρD/bardblD/bardbl2
F,(27)
whereλ,ρk,ρDare positive reals, HK=H1
K×H2
K×···×Hs
K,Fl:Xr→ Yforl∈[s]. Consider
the objective function of (27) as J(F,L,D ). Similar to Sections 5.1.1-5.1.4, on applying an alternating
minimization framework, the steps involved in L,Dminimization remain the same. In order to solve the
minimization problem in (27) (for ﬁxed L,D) an extension of the representer theorem A.2 is required which
follows based on the construction of graph-induced operator-valued kernel in (26).
Theorem 5.1 (Extended representer theorem ).LetKbe an operator-valued kernel as deﬁned in (26)
andHK=H1
K×H2
K×···×Hs
Kbe its corresponding function-valued reproducing kernel Hilbert space based
14Published in Transactions on Machine Learning Research (03/2024)
on kernelsk1,k1
2,...,ks
2. The solution /tildewideFλ∈HKof the regularized optimization problem:
/tildewideFλ= arg min
F=[F1,F2,...,Fs]/latticetop∈HKs/summationdisplay
l=1/parenleftBiggn/summationdisplay
i=1/bardbly(i)
l−Fl(x(i))/bardbl2
Y+λ/bardblFl/bardbl2
Hl
K/parenrightBigg
,
whereλ>0,F= [F1,F2,...,Fs]/latticetop∈HK=H1
K×H2
K×···×Hs
K, has the following form
/tildewideFλ(.) =
/tildewideF1
λ(.)
...
/tildewideFs
λ(.)
=
/summationtextn
i=1K1(x(i),.)u1
i
.../summationtextn
i=1Ks(x(i),.)us
i
,whereu1
i,u2
i,...,us
i∈Y. (28)
Proof.The proof follows as a consequence of the representer theorem proof in Appendix A.4.
In order to solve the minimization problem (27), we use the representer theorem and reproducibility property
of the OVKs Kl, forl∈[s]. The optimization problem in (27) is solved by using the alternating minimization
of the objective function with respect to F= [F1,F2,...,Fs]/latticetop∈HK,L∈LandD∈D. For a constant
LandD, we use the representer theorem (Theorem 5.1) to transform the objective function in terms of
F1∈H1
K,...,Fs∈Hs
Kto functions u1
i,u2
i,...,us
i∈Y,i∈[n], respectively. The objective function Jis
deﬁned as the following (see Appendix A.2):
J(F1,F2,...,Fs,L,D ) =s/summationdisplay
l=1/parenleftBiggn/summationdisplay
i=1/bardbly(i)
l−Fl(x(i))/bardbl2
Y+λ/bardblFl/bardbl2
Hl
K/parenrightBigg
+r/summationdisplay
k=1ρk/parenleftBiggn/summationdisplay
i=1x(i)
k/latticetopLx(i)
k/parenrightBigg
+ρD/bardblD/bardbl2
F
=⇒J(u1,u2,...,us,L,D ) =s/summationdisplay
l=1
n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)
l−n/summationdisplay
j=1Kl(x(i),x(j))ul
j/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y+λn/summationdisplay
i,j=1/angbracketleftKl(x(i),x(j))ul
i,ul
j/angbracketrightY

(29)
+r/summationdisplay
k=1/parenleftBigg
ρkn/summationdisplay
i=1x(i)
k/latticetopLx(i)
k/parenrightBigg
+ρD/bardblD/bardbl2
F.
For solving the multi-dimensional functional regression problem, the alternating minimization framework
discussed in Section 5.1 is extended with the major diﬀerence in the step concerning minimization with
respect toF= [F1,F2,...,Fs]/latticetop(oru1,u2,...,us) for ﬁxedLandD. The multi-dimensionality leads to
solving the following system of linear operator equations:
/bracketleftbig(K1+λI)u1... (Ks+λI)us/bracketrightbig
=/bracketleftbigΘ1... Θs/bracketrightbig
, (30)
where Kl
i,ju=Kl(x(i),x(j))u=k1(x(i),x(j);G)¯kk
2(u),∀u∈ Y,Θl= [y(1)
l,y(2)
l,...,y(n)
l]/latticetopwith ¯kl
2=/integraltext1
0e−γl
op|t/prime−t|u(t/prime)dt/prime,γl
op>0,∀l∈[s]andt/prime,t∈[0,1]. For allKl,l∈[s], the scalar-valued kernel k1
(given in (25)) used remains the same and is built on common LandD. Therefore, spossibly diﬀerent
graph-induced OVKs are obtained by using exponential kernels on R2withγl
op, forl∈[s].
In order to solve for u1,u2,...,usin (30), we use the OpMINRES algorithm discussed in Section 5.1.1 to
solve the systems (Kl+λI)u=yl,forl∈[s]. OpMINRES algorithm solves the ssystems in parallel with
a stopping criteria which combines srelative residuals.
Thus, for alternating minimization, the steps discussed can be summarized as:
1.Minimization with respect to F= [F1,F2,...,Fs]/latticetop∈HK(oru1,u2,...,us∈Yn): Solving
foru1,u2,...,usin
/bracketleftbig
(K1+λI)u1... (Ks+λI)us/bracketrightbig
=/bracketleftbig
Θ1... Θs/bracketrightbig
.
15Published in Transactions on Machine Learning Research (03/2024)
2.Minimization with respect to vech(L): Projected gradient descent of Jwith respect to vech (L)
inLwith sparsity inducing regularization.
3.Minimization with respect to vech(D): Projected gradient descent of Jwith respect to vech (D)
inD.
Using the learned graph-induced operator-valued kernel the output function is used to predict for input
functions ˆ x= (ˆx1,..., ˆxr), where ˆxj∈X, forj∈[r]as
F(ˆ x) =
/summationtextn
i=1K1(x(i),ˆ x)u1
i
.../summationtextn
i=1Ks(x(i),ˆ x)us
i
,whereu1
i,...,us
i∈Y,fori∈[s].
5.1.6 Sample-based Approximation for Functional Regression Problem
The kernel-based alternating minimization framework proposed earlier in this section helps to learn appro-
priate u,L, andDfor the prediction of functional output using (24). For a setting where the number of
training samples is large, the training can become computationally expensive as OpMINRES iteration scales
inO(n3), wherenis the number of training samples. This issue of scalability is a well-known problem in
kernel methods, which becomes more pronounced in an OVK-based framework. There are many popular
methods for handling scalability issues in kernel methods (Williams & Seeger, 2000; Meanti et al., 2020; Bach
& Jordan, 2005). In most cases, the approaches handling scalability issues for kernel methods are incorpo-
rated into the learning problem for approximating the kernel Gram matrices arising in large datasets, by
using low-rank Cholesky decomposition (Bach & Jordan, 2005), Nyström approximation (Williams & Seeger,
2000) and GPU-based acceleration and parallelization (Meanti et al., 2020). For vector-valued regression
problems, random Fourier features have been used for building OVKs (Brault et al., 2016; Brault, 2017)
which cannot be directly extended to functional regression problems due to the following reasons. Extension
of the random Fourier features to a functional setting requires developing a new theoretical framework for
a functional version of operator-valued Bochner’s theorem. Moreover, spectral decomposition of OVKs for
functional data is beyond the scope of our current work, hence we leave it for future work. In our approach,
we aim for a sample-based approximation heuristic algorithm which enables us to perform a greedy sample
selection procedure followed by the training with only those selected samples.
The motivation of the sample-based approximation lies in characterizing the action of the considered OVK
Koni-th sample (x(i),y(i))∈X×Y . Recall the learning problem discussed in Section 5.1 given by
/tildewideF,/tildewideL,/tildewideD= arg min
F∈HK,L∈L,D∈Dn/summationdisplay
i=1/bardbly(i)−F(x(i))/bardbl2
Y+λ/bardblF/bardbl2
HK+ρLn/summationdisplay
i=1x(i)/latticetopLx(i)+ρD/bardblD/bardbl2
F,
which requires solving for uin(K+λI)u=yin the ﬁrst step of alternating minimization with respect to F
(oru) for ﬁxedLandD. We consider the notations yiandKirespectively as equivalent to y(i)andK(x(i),·)
in this section for simplicity. Though approximating ui∈YinF(.) =/summationtextn
i=1Kiuicorresponding to sample
(x(i),y(i))may provide a better option for performing a sample-based approximation, we do not have the
luxury to perform the inversion required in (K+λI)u=y. One way to assess the importance of a training
sample (x(i),y(i))is to investigate the action of operator Kwithx(i)on the output function y(i). Towards
that we build ¯Ki:X →L (Y)by choosing samples which minimize the squared norm of the diﬀerence/summationtextn
i=1/bardblKiyi−¯Kiyi/bardbl2
HK, deﬁned over the RKHS HK. A working set of samples is constructed iteratively
from the training data to formulate ¯Kiyias a linear combination of Kijyij’s, whereij’s correspond to the
indices of a working set of samples in training data. Inspired by (Smola & Schölkopf, 2000), we propose the
following approach to construct ¯Ki’s iteratively. Consider indices in I={i1,i2,...,i|I|}⊂ [n]as the set
of indices for the working set IW={(x(i),y(i)) :i∈I}of samples from the training data. The aim is to
16Published in Transactions on Machine Learning Research (03/2024)
approximate the action of Kionyiusing samples in IWas
¯Kiyi=|I|/summationdisplay
j=1Ti,jKijyij,fori∈[n], (31)
=⇒
¯K1y1
¯K2y2
...
¯Knyn
=
T1,1T1,2... T 1,|I|
T2,1T2,2... T 2,|I|
............
Tn,1Tn,2... Tn,|I|

Ki1yi1
Ki2yi2
...
Ki|I|yi|I|
=:T
Ki1yi1
Ki2yi2
...
Ki|I|yi|I|
, (32)
whereT∈Rn×|I|. The values/vextenddouble/vextenddoubleKiyi−¯Kiyi/vextenddouble/vextenddouble
HKare treated as residuals of the approximation for the i-th
training sample. Approximations ¯Kicorresponding to each sample (x(i),y(i)), fori∈[n], created by the
working set IWof samples in (31) are bounded linear operators on the output space Y. For every sample
(x(i),y(i)), wherei∈[n], the action of operator Kionyiis approximated by using scalars Ti,1,Ti,2,...,Ti,|I|
withKijyij∈HK,forj∈[|I|]. Minimization of/summationtextn
i=1/bardblKiyi−¯Kiyi/bardbl2
HKensures that the working set of
samples can characterize closely the impact of Kiyi. The approximation of Ki,∀i∈[n]using the working
set of samples is described next.
Approximation of Operators using Samples: Initially, suppose I=∅and leti1∈[n]be the best
candidate index. Then the index set Iis updated as I={i1}, and the working set IWcontains only a single
sample corresponding to the index i1∈I. Now the optimization problem is to determine T1,1,T2,1,...,Tn,1
in¯Kiyi=Ti,1Ki1yi1,∀i∈[n]and is given by
arg min
T1,1,T2,1,...,T n,1n/summationdisplay
i=1/vextenddouble/vextenddoubleKiyi−¯Kiyi/vextenddouble/vextenddouble2
HK=n/summationdisplay
i=1/bardblKiyi−Ti,1Ki1yi1/bardbl2
HK. (33)
The solution of (33) is obtained as
Ti,1=/angbracketleftKiyi,Ki1yi1/angbracketrightHK
/angbracketleftKi1yi1,Ki1yi1/angbracketrightHK(34)
=/angbracketleftKii1yi,yi1/angbracketrightY
/angbracketleftKi1i1yi1,yi1/angbracketrightY,fori∈[n], (35)
whereKii1=K(x(i),x(i1))and (34) is obtained by diﬀerentiating the objective function in (33) with respect
toTi,1and equating it to 0, to obtain the minima for i∈[n]. The reproducing property of OVK is used to
obtain (35). This construction of Ti,1yields
/angbracketleftKiyi−¯Kiyi,Ki1yi1/angbracketrightHK= 0,∀i∈[n]. (36)
The equality in (36) denotes that the space span {(Kiyi−¯Kiyi),∀i∈[n]}is orthogonal to Ki1yi1(denoted
by span{(Kiyi−¯Kiyi)),∀i∈[n]}⊥Ki1yi1). Note that this orthogonality property holds for index i1. We
shall show later that a similar property indeed holds for all samples which will be added to the working set.
In general, the number of samples for the functional regression problem can be large and searching for the
best candidates in the complete training set may become costly and defeat the cause for developing a sample
based approximation. Hence we consider a random subset of Rsamples for an eﬃcient approximation, where
R<nand|I|<R.
For the iterative process to build the working set of indices Iand samples IW, let us assume that Ioldbe
the index set with |Iold|=k(say) andTold∈Rn×kbe the matrix formed based on (31) for obtaining
¯Kold
iyi=/summationtextk
j=1Ti,jKijyij,∀i∈[n]. Suppose ik+1be the index of next best sample to be added to get
Inew=Iold∪{ik+1}which provides ¯Knew
i=¯Kold
iyi+Ti,k+1Kik+1yik+1. The minimization problem as in
(33) is written as
arg min
T1,k+1,T2,k+1,...,T n,k+1n/summationdisplay
i=1/vextenddouble/vextenddoubleKiyi−¯Knew
iyi/vextenddouble/vextenddouble2
HK=n/summationdisplay
i=1/vextenddouble/vextenddoubleKiyi−¯Kold
iyi−Ti,k+1Kik+1yik+1/vextenddouble/vextenddouble2
HK,(37)
17Published in Transactions on Machine Learning Research (03/2024)
where the solution of (37) results in
Ti,k+1=/angbracketleftKiyi−¯Kold
iyi,Kik+1yik+1/angbracketrightHK
/angbracketleftKik+1yik+1,Kik+1yik+1/angbracketrightHK(38)
=/angbracketleftKiik+1yi,yik+1/angbracketrightY−/summationtextk
j=1Ti,j/angbracketleftKijik+1yij,yik+1/angbracketrightY
/angbracketleftKik+1ik+1yik+1,yik+1/angbracketrightY,fori∈[n]. (39)
Ti,k+1in (38) is obtained similar to the procedure for (34) by diﬀerentiating the objective function in (37)
with respect to Ti,k+1’s and equating it to 0, obtaining the minima for i∈[n]. Equation (39) follows from
properties of inner-product and reproducing property of OVK. The iterative construction ensures that the
following property holds:
/angbracketleftKiyi−¯Knew
iyi,Kijyij/angbracketrightHK= 0,∀i∈[n],∀j∈[k+ 1] (40)
=⇒span{Kiyi−¯Knew
iyi|i∈[n]}⊥span{Kijyij|j∈[k+ 1]}. (41)
Similar to (36), the iterative procedure ensures that the orthogonality property is extended to (41) in HK,
which will be used in the iterative selection process discussed below. For each iteration, it remains to ﬁnd
the best sample from the Rrandomly selected candidate set of samples, which is to be included in IW. We
discuss this next.
Selecting the Best Samples Iteratively: In order to ﬁnd the training sample which will minimize the
residuals most eﬀectively, let Cbe the candidate set of indices for training samples given by C⊆[n]\I.
Suppose for a particular iteration, let I={i1,i2,...,ik}and let the randomly selected candidate set of
indices beC={c1,c2,...,cM}. For each cr∈C, we calculate the improvement in the sum of residuals
which can result in including crinI. Assume ¯Kold
iyibe given by
¯Kold
iyi=k/summationdisplay
j=1Ti,jKijyij,fori∈[n], (42)
from which we obtain ¯Knew
iyias follows:
¯Knew
iyi=¯Kold
iyi+Ti,rKcrycr,forcr∈C. (43)
In order to select the best sample index from the candidate set C, we need to ﬁnd the index crinCwhich
best approximates/summationtextn
i=1/bardblKiyi−¯Knew
iyi/bardbl, whenInew=Iold∪{cr}is considered as the index set for the
new working set of samples. Let the improvement in the sum of residuals by adding crinIoldbe denoted
by Improvement (cr), given by
Improvement (cr) =n/summationdisplay
i=1/bardblKiyi−¯Kold
iyi/bardbl2
HK−n/summationdisplay
i=1/bardblKiyi−¯Knew
iyi/bardbl2
HK(44)
=/summationtextn
i=1/bracketleftbig
/angbracketleftKiyi−¯Kold
iyi,Kcrycr/angbracketrightHK/bracketrightbig2
/angbracketleftKcrycr,Kcrycr/angbracketrightHK(45)
=/summationtextn
i=1/bracketleftBig
/angbracketleftKicryi,ycr/angbracketrightY−/summationtextk
j=1Ti,j/angbracketleftKijcryij,ycr/angbracketrightY/bracketrightBig2
/angbracketleftKcrcrycr,ycr/angbracketrightY. (46)
Equation (44) quantiﬁes the reduction in the residual value by the addition of crto working set Iof indices.
Equation (45) is obtained from (44) by using the properties of inner product and ¯Kold
i, fori∈[n]and the
orthogonality property (41). Equation (46) follows from the reproducing property of operator-valued kernel
K. The sample which achieves the maximum improvement is considered to be the best sample to be added
to the working set. In terms of indices, this selection becomes
Best Index k= arg max
cr∈CImprovement (cr).
18Published in Transactions on Machine Learning Research (03/2024)
Algorithm 3 Sample-based Approximation
Input:{(x(i),y(i))}n
i=1,x(i)= (x(i)
1,x(i)
2,...,x(i)
p)∈X,y(i)∈Y
Output:I, the index set of working set of samples.
Initialize vech(L)0,vech(D)0
yi←y(i),Ki←K(x(i),.),Kij←K(x(i),x(j)),∀i,j∈[n]
Initializek= 0,I=∅,T=0
whilestopping criterion based on residual (49) is not satisﬁed do
ConstructCby drawing random subset of Melements from [n]\I,C={c1,c2,...,cM}
ComputeTi,k+1=/angbracketleftKiik+1yi,yik+1/angbracketrightY−/summationtextk
j=1Ti,j/angbracketleftKijik+1yij,yik+1/angbracketrightY
/angbracketleftKik+1ik+1yik+1,yik+1/angbracketrightY,fori∈[n]
Improvement (cm) =/summationtextn
i=1/bracketleftbig
/angbracketleftKicmyi,ycm/angbracketrightY−/summationtextk
j=1Ti,j/angbracketleftKijcmyij,ycm/angbracketrightY/bracketrightbig2
/angbracketleftKcmcmycm,ycm/angbracketrightY,form∈[M]
Best Index k= arg maxcm∈CImprovement (cm)
I=I∪{Best Index k}
k←k+ 1
end while
StoppingCriterion: Asisthecaseforanyiterativealgorithm, anappropriatestoppingcriterionisrequired
for ending the sample selection process which may be based on the number of iterations or accuracy. For
using accuracy-based stopping criterion, residual is calculated for the k-th iteration as
Residualk=n/summationdisplay
i=1/vextenddouble/vextenddouble(Ki−¯Ki)yi/vextenddouble/vextenddouble2
HK(47)
=n/summationdisplay
i=1
/angbracketleftKiyi,Kiyi/angbracketrightHK−2k/summationdisplay
j=1Tij/angbracketleftKiyi,Kijyij/angbracketrightHK+k/summationdisplay
j=1k/summationdisplay
l=1TijTil/angbracketleftKijyij,Kilyil/angbracketrightHK
(48)
=n/summationdisplay
i=1/angbracketleftKiiyi,yi/angbracketrightY−2n/summationdisplay
i=1k/summationdisplay
j=1Ti,j/angbracketleftKiijyi,yij/angbracketrightY+n/summationdisplay
i=1k/summationdisplay
j=1k/summationdisplay
l=1Ti,jTi,l/angbracketleftKijilyij,yil/angbracketrightY. (49)
Equation (48) follows from the properties of inner product of RKHS and (49) is obtained using the repro-
ducibility property of K. As the ﬁrst part of the summation in (49) remains constant for each iteration, a
threshold for residual value can be used to determine convergence of the last two terms. For a very large set
of training samples, a budget on the number of samples to consider can also be an eﬀective tool for approx-
imation. As the aim is to learn a kernel encapsulating the graphical structure between the input variables,
the sample approximation can still be costly. An eﬀective strategy is to start with an initial Lrepresenting a
fully connected graph and an initial Dwhich is the identity matrix. After the sample selection process, the
ﬁnal working set IWof samples indexed by Iare used throughout in the alternating minimization framework.
In our implementations, we used the residual calculation using a validation set instead of the training set
which provided a faster convergence and better generalization. Algorithm 3 illustrates the sample-based
approximation for functional regression problem.
6 Generalization Analysis
LetX= (L2([0,1]))pbe the input space and Y=L2([0,1])be the output space. Consider the training
samples given as z={(x(i),y(i)) :i∈[m]}⊆X×Y =:Zwherezi:= (x(i),y(i)),∀i∈[m]are drawn i.i.d.
from a probability distribution µ. The empirical error of a learned function-valued function Fon the data
zis given as the following:
Ez(F) =1
mm/summationdisplay
i=1L(y(i),F(x(i))), (50)
19Published in Transactions on Machine Learning Research (03/2024)
whereL:Y×Y→ R+is a loss function deﬁned on the output space Y. A typical learning problem involves
estimating a function-valued Fwhich is the solution of the following problem:
min
F∈HKEλ(F,K), (51)
whereEλ(F,K):=Ez(F) +λ/bardblF/bardbl2
HK. In our problem Kis parameterized by L,D,γ,γopand belongs to a
class of OVKsKand hence in this work, for the given data, we aim to learn the following:
(Kz,Fz):= arg min{Eλ(F,K) :K∈K,F∈HK}. (52)
The problem (52) can be reformulated as a regularized empirical error minimization problem. Our focus
is on the problem of bounding the generalization error of Fz, namely E(Fz)−E(F∗), where E(F)is the
expected error of Fgiven by E(F):=E[L(y,F(x))], the expectation Eis taken over the probability measure
µ, andF∗is the target function deﬁned as
F∗= arg min E(F), (53)
where the minimum is taken over all measurable functions F:X→Y.
6.1 Error Bounds
In this section, we introduce quantities which will be useful for our generalization bound analysis. We use
the approach in (Micchelli et al., 2016; Ying & Zhou, 2007; Wu & Zhou, 2006) and introduce sample error
as the following:
Sz(m,λ,F ) = [E(Fz)−Ez(Fz)] + [Ez(F)−E(F)]. (54)
The sample error Sz(m,λ,F )in (54) consists of two terms [E(Fz)−Ez(Fz)]and[Ez(F)−E(F)]. The ﬁrst
term [E(Fz)−Ez(Fz)]is the diﬀerence between the expected value of L(y,Fz(x))with respect to µand its
empirical mean over a ﬁxed random data set z⊆Z. To bound this term we use the notion of Rademacher
averages which enables us to control and analyze the random variables ziassociated with the data z⊆Z.
Similarly, the second term [Ez(F)−E(F)]denotes the diﬀerence between the empirical mean of L(y,F(x))
for a ﬁxedz⊆Zand its expectation with respect to µ. We follow the approach in (Micchelli et al., 2016) to
bound both the terms.
In addition to the sample error, we introduce another quantity known as the regularization error R(F)for
a functionF∈HKdeﬁned as
R(F) =E(F)−E(F∗) +λ/bardblF/bardbl2
HK, (55)
whereF∗is the target function. A regularized version of problem (53) is given by
(K∗
λ,F∗
λ):= arg min
K∈K,F∈HK{E(F) +λ/bardblF/bardbl2
HK:K∈K,F∈HK}. (56)
The regularization error of F∗
λis denoted byR∗(λ)as follows:
R∗(λ) = min
K∈Kmin
F∈HK/bracketleftbig
E(F)−E(F∗) +λ/bardblF/bardbl2
HK/bracketrightbig
. (57)
In order to determine a generalization bound, we use the following result which enables us to relate gener-
alization error using sample error and regularization error.
Proposition 6.1. For everyK∈K,F∈HK, the following inequality holds
E(Fz)−E(F∗)≤Sz(m,λ,F ) +R(F). (58)
Proof.In order to prove the inequality, we start with the generalization error,
E(Fz)−E(F∗) =Sz(m,λ,F ) +Ez(Fz)−Ez(F) +E(F)−E(F∗) +λ/bardblF/bardbl2
HK−λ/bardblF/bardbl2
HK(59)
=Sz(m,λ,F ) +R(F) +Ez(Fz)−Ez(F)−λ/bardblF/bardbl2
HK(60)
≤Sz(m,λ,F ) +R(F). (61)
20Published in Transactions on Machine Learning Research (03/2024)
Equation (59) is obtained by adding and subtracting Sz(m,λ,F )andλ/bardblF/bardbl2
HK. Equation (60) follows
from the deﬁnition of R(F)in (55). The inequality in (61) is obtained using the facts λ/bardblF/bardbl2
HK≥0and
Ez(Fz)−Ez(F)≤0, asFz= arg minF∈HKEz(F).
Now, we intend to bound the term E(Fz)−Ez(Fz)in (54). Towards this we deﬁne the following notion of
Rademacher average of a suitable class of functions.
Deﬁnition 6.1 (Rademacher Average ).LetFdenote a class of functions from XtoY. LetµXdenote
the marginal distribution over the input space X. Consider a m-tuple of samples from input space as
(x(1),x(2),...,x(m))∈Xm, wherex(i)∼µX,i∈[m]. Then the Rademacher average of class Fis deﬁned as
Rm;Y(F) =E/bracketleftBigg
sup
F∈F1
m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiF(x(i))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg
, (62)
whereεi’s are Rademacher random variables uniformly distributed over {+1,−1}andErepresents the
expectation over both i.i.d. Rademacher variables εi’s and i.i.d. variables x(i)’s based on µX.
Recall that for a class of functions Ffrom an input space XtoR, and a sample (x1,x2,...,xm)∈Xm, the
Radamacher average of Fis deﬁned as
Rm;R(F) =E/bracketleftBigg
sup
F∈F1
mm/summationdisplay
i=1εiF(xi)/bracketrightBigg
. (63)
Comparing this expression with that in Deﬁnition 6.1, we note that a suitable norm is used in Deﬁnition
6.1. Thus our deﬁnition of a suitable Radamacher average accommodates the nature of the function class F
which contains function-valued functions, unlike Fwhich is composed of simple real-valued functions.
In order to proceed with the upcoming proofs, we require some assumptions which we state next. Assume
∃β >0such that/bardbly/bardblY≤β,∀y∈Y, which provides a uniform upper bound on the norm of the outputs. In
addition, assume that the class Kis uniformly bounded, that is,
κ= sup
K∈Ksup
x∈Xsup
y∈Y/bardblK(x,.)y/bardblHK= sup
K∈Ksup
x∈Xsup
y∈Y/radicalbig
/angbracketleftK(x,x)y,y/angbracketrightY<∞. (64)
This assumption holds for OVKs which satisfy the trace class assumption (Kadri et al., 2016).
For anyK∈KandF∈HK, we deﬁne the following norm
/bardblF/bardbl∞= max
x∈Xmax
y∈Y|/angbracketleftF(x),y/angbracketrightY|= max
x∈Xmax
y∈Y|/angbracketleftF,K(x,.)y/angbracketrightHK|≤κ/bardblF/bardblHK, (65)
where we use the reproducing property, /angbracketleftF,K(x,.)y/angbracketrightHK=/angbracketleftF(x),y/angbracketrightYand the inequality in (65) follows from
Cauchy-Schwarz inequality. We deﬁne for t≥0,
Ξ(t):= sup
y∈Ysup
/bardbls/bardblY≤tL(y,s). (66)
The function Ξ(t)provides a bound on the loss function when the second argument sin the loss function has
restricted norm. Ξ(t)enables bounding the norm of the function Fvia the loss function Lby considering
t= 0. LetL:Y→Rbe deﬁned as the following:
L(t) = sup
y∈Ysup
/bardbls1/bardblY≤t,
/bardbls2/bardblY≤t|L(y,s1)−L(y,s2)|
/bardbls1−s2/bardblY. (67)
L(t)provides a Lipschitz constant for the loss function Lwith respect to the second argument when the
norm of the argument is bounded by t.
Lemma 6.2. LetFbe a class of functions from XtoY. Consider a m-tuple of samples from input space
as(x(1),x(2),...,x(m))∈Xm. Then the following hold:
21Published in Transactions on Machine Learning Research (03/2024)
1.E/bracketleftbig
supF∈F/bardbl1
m/summationtextm
i=1F(x(i))−EF/bardblY/bracketrightbig
≤2Rm;Y(F).
2. For every c∈R,Rm;Y(cF) =|c|Rm;Y(F).
3. Forφ:Y→R, ifφis a Lipschitz function with Lipschitz constant L, thenRm;R(φ◦F)≤LRm;Y(F).
Proof.The lemma has been proved as Lemma A.6 in Appendix A.9.
We further deﬁne the following constants:
ρ=/radicalbig
Ξ(0)/λ, τ =κρ, (68)
which will be useful in the upcoming results. The forthcoming results will use the class of kernels given by:
K0={K(x,.)y:K∈K,x∈X,y∈Y}. (69)
The following result provides a bound on the sample error which involves the Rademacher average of the
class of kernelsK0.
Theorem 6.3. IfF∈HK, then with conﬁdence 1−δ, whereδ∈(0,1), there holds
Sz(m,λ,F )≤2ρL(τ)β1/4(Rm;Y(K0))1/4+ (Ξ(τ) + Ξ(/bardblF/bardbl∞))/radicalBigg
log1
δ
2m. (70)
The proof will be provided later as a consequence of the results covered in the upcoming section. Theorem
6.3 provides a probabilistic upper bound for the sample error in terms of Rademacher average for the class
of graph-induced operator-valued kernels. Later in Section 6.3, we will consider Kto be the class of graph-
induced OVKs and derive a bound on Rm;Y(K0).
6.2 Estimating Sample Error
In this section, we derive results which aid in establishing the result in Theorem 6.3. We use Hoeﬀding
inequality for bounding the term Ez(F)−E(F)using random data set z⊆Z:=X×Y.
Lemma 6.4. LetFbe a bounded function. For every δ∈(0,1), with conﬁdence 1−δthere holds
Ez(F)−E(F)≤Ξ(/bardblF/bardbl∞)/radicalBigg
log1
δ
2m. (71)
Proof.Consider the random variable ζ=L(y,F(x)). Note that Ez=1
m/summationtextm
i=1ζ(zi), wherezi= (x(i),y(i))
andE=E(ζ). By our assumption, 0< ζ≤Ξ(/bardblF/bardbl∞)we have|ζ−E[ζ]|≤Ξ(/bardblF/bardbl∞). Using one-sided
Hoeﬀding inequality, we obtain
P/parenleftBigg
1
mm/summationdisplay
i=1(ζi−E[ζ])≥t/parenrightBigg
≤exp/parenleftbigg
−2mt2
Ξ2(/bardblF/bardbl∞)/parenrightbigg
. (72)
Consider,
δ= exp/parenleftbigg
−2mt2
Ξ2(/bardblF/bardbl∞)/parenrightbigg
=⇒log1
δ=/parenleftbigg2mt2
Ξ2(/bardblF/bardbl∞)/parenrightbigg
=⇒t= Ξ(/bardblF/bardbl∞)/radicalBigg
log1
δ
2m. (73)
Therefore, for every δ∈(0,1), with conﬁdence 1−δthe following holds:
Ez(F)−E(F)≤Ξ(/bardblF/bardbl∞)/radicalBigg
log1
δ
2m. (74)
22Published in Transactions on Machine Learning Research (03/2024)
Nowthatthesecondterminthesampleerror(54)hasbeenbounded, wefocusontheﬁrsttermbyconsidering
a union of unit balls in the space HKwhere the notion of Rademacher average can be deﬁned for obtaining
bounds. We deﬁne a function Θsuch that
E(Fz)−Ez(Fz)≤Θ(z):= sup
F∈ρBK(E(F)−Ez(F)), (75)
whereBKis the union of unit balls in HKoverK∈Kgiven by
BK=/uniondisplay
K∈K/braceleftBig
F∈HK:/bardblF/bardblHK≤1/bracerightBig
. (76)
The supremum is deﬁned over ρBKin (75) by the following reasoning: λ/bardblFz/bardbl2
HKz≤E(Fz)−E(F∗) +
λ/bardblFz/bardbl2
HKz≤Ez(0)−E(F∗)≤supy∈YL(y,0) = supy∈Ysups:/bardbls/bardblY≤0L(y,s) = Ξ(0). This gives a bound on
/bardblFz/bardblHKz, note that Fzis the minimizer in problem (52). In fact, using a similar approach, for any general
F∈HK, we have/bardblF/bardblHK≤/radicalbig
Ξ(0)/λ=:ρwhich leads to the set ρBKin (75).
Thus to bound E(Fz)−Ez(Fz), we ﬁnd a suitable upper bound on Θ(z)in the following lemma.
Lemma 6.5. Consider Θ(z) = supF∈ρBK(E(F)−Ez(F)), then for every δ∈(0,1), with conﬁdence 1−δ
the following holds
Θ(z)≤E[Θ(z)] + Ξ(τ)/radicalBigg
log1
δ
2m. (77)
Proof.Letz/prime
ibe the data set which is obtained by replacing i-th pairzi= (x(i),y(i))ofzwith (x/prime
i,y/prime
i). Then,
Θ(z)−Θ(z/prime
i) = sup
F∈ρBK(E(F)−Ez(F))−sup
F∈ρBK/parenleftBig
E(F)−Ez/prime
i(F)/parenrightBig
(78)
≤sup
F∈ρBK(Ez/prime
i(F)−Ez(F)) (79)
=1
msup
F∈ρBK(L(y/prime
i,F(x/prime
i))−L(y(i),F(x(i)))) (80)
≤1
mΞ(τ), (81)
where (79) is obtained by using properties of supremum and (81) follows from the deﬁnition of κ,τandΞ.
By interchanging zandz/prime
i, we obtain
|Θ(z)−Θ(z/prime
i)|≤1
mΞ(τ). (82)
Using McDiarmid’s inequality, we obtain
P(Θ(z)−E[Θ(z)]≥/epsilon1)≤exp/parenleftbigg
−2m/epsilon12
Ξ2(τ)/parenrightbigg
. (83)
Using a similar argument as in the proof of Lemma 6.4, the proof follows that for every δ∈(0,1), with
conﬁdence 1−δthe following holds:
Θ(z)−E[Θ(z)]≤Ξ(τ)/radicalBigg
log1
δ
2m. (84)
The next lemma helps to bound E[Θ(z)]using Rademacher average of the class K0.
23Published in Transactions on Machine Learning Research (03/2024)
Lemma 6.6. E[Θ(z)]is bounded above as follows:
E[Θ(z)]≤2ρL(τ)β1/4(Rm;Y(K0))1/4.
Proof.The lemma has been proved as Lemma A.7 in Appendix A.9.
Using Lemmas 6.4, 6.5 and 6.6, the result in Theorem 6.3 is proved.
6.3 Learning with Graph-Induced Operator-valued Kernels
In this section, we consider the following class of functions
K0={K(x,.)y:K∈K,x∈X,y∈Y}, (85)
whereKis deﬁned as the graph-induced OVK given by
K(x,x/prime)y=e−γ(x−x/prime)/latticetop(L+D)(x−x/prime)/integraldisplay1
0e−γop|s−t|y(s)ds, (86)
=g(x,x/prime)Ty, (87)
withγ,γop>0,L∈LandD∈D. Note that g(x,x/prime) =e−γ(x−x/prime)/latticetop(L+D)(x−x/prime)andTy=/integraltext1
0e−γop|s−t|y(s)ds.
Now, in order to bound the Rademacher average Rm;Y(K0), we follow an approach inspired by Maurer
(2016) and split the OVK Kusing properties on gandT. Consider the class of functions deﬁned as
G={g(x,.) =e−γ(x−.)/latticetop(L+D)(x−.)∈HG:/bardblg(x,.)/bardblHG≤RG,L∈L,D∈D,γ > 0}whereHGis the RKHS
corresponding to the scalar-valued kernel gonX×X.
Rm;Y(K0) =E/bracketleftBigg
sup
k∈Gsup
y∈Ysup
t∈X/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
mm/summationdisplay
i=1εik(x(i),t)Ty/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg
(88)
≤E/bracketleftBigg
sup
k∈Gsup
y∈Ysup
t∈X/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
mm/summationdisplay
i=1εik(x(i),t)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bardblTy/bardblY/bracketrightBigg
(89)
=/parenleftbigg
sup
y∈Y/bardblTy/bardblY/parenrightbigg
E/bracketleftBigg
sup
k∈Gsup
t∈X/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
mm/summationdisplay
i=1εik(x(i),t)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/bracketrightBigg
(90)
=/parenleftbigg
sup
y∈Y/bardblTy/bardblY/parenrightbigg
R+
m;R(G). (91)
In Equation (91), R+
m;R(G)denotes a Rademacher average involving absolute values of real-valued functions
inG. For bounding Rademacher average R+
m;R(G), we use the notion of covering numbers. Next, we provide
the deﬁnition of covering numbers.
Deﬁnition 6.2 (Covering Numbers ).Let(F,d)be a pseudo-metric space and Sbe a subset of F. For
every/epsilon1>0, the covering number of Sby balls of radius /epsilon1with respect to d, denoted byN(S,/epsilon1,d )is deﬁned
as the minimal number of balls of radius /epsilon1whose union covers S, namely,
N(S,/epsilon1,d ) = min

n∈N:∃{sj}n
j=1⊂Fsuch thatS⊆n/uniondisplay
j=1B(sj,/epsilon1)

,
whereB(sj,/epsilon1) ={s∈F:d(s,sj)≤/epsilon1}.
LetQbe a class of bounded real-valued functions deﬁned on X,x= (x(i):i∈[m])∈XmandQ|x=
{(Q(x(i)) :i∈[m]) :Q∈Q}⊆ Rm. For a norm induced by donX, we deﬁne the d-norm empirical covering
number ofQassociated with xasNd(Q,/epsilon1,m ) = supx∈XmN(Q|x,/epsilon1,d).
24Published in Transactions on Machine Learning Research (03/2024)
LetU= supg∈GE[g2]. Using a construction similar to (124) and (125) in Appendix A.3, we obtain
g(x,.) =e−γ(x−.)/latticetop(L+D)(x−.)=e−γ/bardblA(x−.)/bardbl2
p,
whereA=√
ΛV, for a diagonal matrix Λwith non-negative eigenvalues of L+Dand V is a orthonormal
matrix. Consider an appropriate bound as E[g2]≤a,∀g∈Gwhich is reasonable owing to the RBF-based
construction of scalar-valued kernels in G. Based on Corollary 2.2.8 in (Van Der Vaart & Wellner, 1996), we
can bound the Rademacher average using covering number as
R+
m;R(G)≤1√m/integraldisplayU
0/radicalbig
logNd(G,/epsilon1,m )d/epsilon1, (92)
≤1√m/integraldisplaya
0/radicalbig
logNd(G,/epsilon1,m )d/epsilon1. (93)
To boundNd(G,/epsilon1,m ), we use Remark 11 in (Cucker & Smale, 2002), to state that there exists C > 0and
q>0such that
logNd(G,/epsilon1,m )≤/parenleftbiggRGC
/epsilon1/parenrightbigg1
q
. (94)
Based on our assumptions, there exists K>0such that supy∈Y/bardblTy/bardblY≤K. Using (93) and (94) in (91), we
obtain
Rm;Y(K0)≤2aqK(RGC/a)1/2q
(2q−1)√m. (95)
Using the result in (95) with (70), we can establish the generalization bounds for the class of kernels con-
structed with graph-induced operator-valued kernels. For the problem considered in this work, the loss is
deﬁned as L(y,y/prime) =/integraltext1
0(y(t)−y/prime(t))2dtwith Ξ(t)≤(β+t)2andL(t)≤2(β+t). We obtain the following
forλ<1,δ∈(0,1), with conﬁdence 1−δas
Sz(m,λ,F∗
λ)≤4ρ(β+τ)β1/4/parenleftbigg2aqK(RGC/a)1/2q
(2q−1)√m/parenrightbigg1/4
+ ((β+τ)2+ (β+/bardblF∗
λ/bardbl∞)2)/radicalBigg
log1
δ
2m(96)
≤4β√
λ/parenleftbigg
β+κβ√
λ/parenrightbigg
β1/4/parenleftbigg2aqK(RGC/a)1/2q
(2q−1)√m/parenrightbigg1/4
+ 2/parenleftbigg
β+κβ√
λ/parenrightbigg2/radicalBigg
log1
δ
2m(97)
=4β9/4
λ(κ+√
λ)/parenleftbigg2aqK(RGC/a)1/2q
(2q−1)√m/parenrightbigg1/4
+2β2
λ(κ+√
λ)2/radicalBigg
log1
δ
2m(98)
<
β1/4/parenleftbigg2aqK(RGC/a)1/2q
(2q−1)/parenrightbigg1/4
+/radicalBigg
log1
δ
2
β2
λm1/8max{4(κ+ 1),2(κ+ 1)2}.(99)
The inequality (97) is obtained using ρ=/radicalbig
Ξ(0)/λ≤β/√
λ,τ=κρand/bardblF∗
λ/bardbl∞≤κρ. Now, a common
assumption for smooth kernels is of logarithmic decay of regularization error, i.e., R∗(λ)≤c/primeλη, where
η∈(0,1]andc/prime>0(Micchelli et al., 2016). Then the generalization error is bounded by
E(Fz)−E(F∗)≤c
λ+c/primeλη. (100)
Consider the function H(λ) =c
λ+c/primeλη, then the minimizer is obtained for λ∗= (c/ηc/prime)1/(1+η)with
H(λ∗) = (ηc/prime)1/(1+η)[1 + 1/η]cη/(1+η). Therefore, for δ∈(0,1), with conﬁdence 1−δwe obtain
E(Fz)−E(F∗)≤(ηc/prime)1/(1+η)(1 + 1/η)

β1/4/parenleftbigg2aqK(RGC/a)1/2q
(2q−1)/parenrightbigg1/4
+/radicalBigg
log1
δ
2
β2
m1/8A
η/(1+η)
,
(101)
25Published in Transactions on Machine Learning Research (03/2024)
where A= max{4(κ+ 1),2(κ+ 1)2}. (101) ensures an upper bound for the generalization error with the
help of a bound on the Rademacher average for the problem (52) of learning the OVK Kzand the functional
mapFzin the induced RKHS corresponding to Kzby using a ballBKin corresponding RKHS with a ﬁxed
radius (considered as 1). The task of establishing a bound on the regularization error R∗(λ)in (Micchelli
et al., 2016) considers an example prescribing value for ηbased on the hyperparameter in the RBF kernel.
A similar pursuit in our setting is not straightforward because of the functional nature of the input space.
Hence, we leave it for future work.
7 Experiments
Inordertoillustratetheeﬀectivenessofthedevelopedframework, wehaveusedfunctionalregressionproblem
with an unknown graph structure in the input data for both synthetic and real datasets. The task of
predicting output functions with the help of a Laplacian matrix denoting the relationship between the
set ofpinput functions has been illustrated in the experiments. As practical data is always available as
discrete observations corresponding to functions, standard FDA techniques can be used for the conversion
of functional data into vector representation using basis functions, e.g. Fourier basis, B-spline basis, etc.
LetX= (L2([a,b]))pandY=L2([c,d])be the input and output spaces, respectively. For our experiments,
the error metric used is residual sum of squares error (RSSE) (Kadri et al., 2016) deﬁned as RSSE =/summationtext
i/integraltextd
c{y(i)(t)−ˆy(i)(t)}2dt, wherey(i)is the actual output function and ˆy(i)is the predicted output function.
RSSE is better suited to compare functional outputs. The integrals involved have been approximated by
using numerical integration in our implementation. The quadratic programs involved in (23) and (15) are
solved by using CVXOPT (Andersen et al., 2023).
Experimental Setting : All methods were coded in Python 3.7 and the codes are made public.1All
experiments were run on a Linux box with 182 Gigabytes main memory and 28 CPU cores. As methods
to solve the problem of functional regression problem simultaneously with learning Land/orDare not
available, we use popular algorithms to ﬁrst determine L. Then for the learned L, we use our alternating
minimization framework to learn Dusing projected gradient descent and uusing OpMINRES. For the MCP-
basedLlearning and Dlearning in the proposed alternating minimization framework, we use a decaying
step-size in the projected gradient descent. The decaying step-size regime involves starting with an initial
step-size (e.g. 10−4) and reducing it by a ﬁxed factor (e.g. 2) after a set of iterations (e.g. 5) continuously
till a ﬁnal step-size (e.g. 10−9). In order to illustrate the eﬀectiveness, we consider the following methods
for comparison.
fglasso-OpMINRES-D :Lis determined using fglasso (Qiao et al., 2019), based on a Gaussian func-
tional model which provides a precision matrix (inverse of covariance matrix) corresponding to the
nodes with corresponding functional input data. The approach develops an extension of the glasso
criterion (Yuan & Lin, 2007) to fglasso for functional data. The learned Lis then used with our
alternating minimization regime for optimizing uandD. OpMINRES is used with k1(x,x/prime;G) =
e−γ(x−x/prime)/latticetop(L+D)(x−x/prime)andk2(s,t) =e−γop|s−t|, whereγ∈{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}
andγop∈{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}.
KGL-OpMINRES-D :Lis obtained by using Kernel Graph Learning (KGL) (Pu et al., 2021b) prob-
lem with respect to two kernel Gram matrices obtained for input signals and their timestamps which
have been used to establish the relationship between input functions. RBF kernels have been consid-
ered for the input functions. The hyperparameters for KGL are tuned using cross-validation in our im-
plementation. The learned Lis then used with our alternating minimization regime for optimizing u
andD. OpMINRES is used with k1(x,x/prime;G) =e−γ(x−x/prime)/latticetop(L+D)(x−x/prime)andk2(s,t) =e−γop|s−t|, where
γ∈{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}andγop∈{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}.
Sparse OpMINRES-L-D : This denotes our proposed method where we used Algorithm 2 to learn u,Land
D. Projected gradient descent is used in minimization with respect to LandDbased on a decaying step-size.
The sparsity is aided by the MCP regularization considered in learning of L. The graph-induced operator-
1Codes used for the experiments can be found at https://github.com/akashsaha06/graph-inducedOVK .
26Published in Transactions on Machine Learning Research (03/2024)
valued kernels are obtained using k1(x,x/prime;G) =e−γ(x−x/prime)/latticetop(L+D)(x−x/prime)andk2(s,t) =e−γop|s−t|, where
γ∈{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}andγop∈{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}.
Sparse Non-Pos-OpMINRES-L-D : As our framework is developed for OVKs, the proposed alternating
minimization is well adaptive to consider generalized non-positive semi-deﬁnite OVKs (Saha & Palaniappan,
2020) as graph-induced OVKs. We call this extension Sparse Non-Pos-OpMINRES-L-D. Here too, projected
gradient descent is used in minimization with respect to LandDusing the decaying step-size similar
to Sparse OpMINRES-L-D. The sparsity is aided by the MCP regularization considered in learning of L.
The graph-induced operator-valued kernels are obtained using k1(x,x/prime;G) =e−γ(x−x/prime)/latticetop(L+D)(x−x/prime)and
k2(s,t) =e−γop1|s−t|−e−γop2|s−t|, whereγ∈{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}andγop1,γop2∈
{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}. Note that k2is not necessarily a positive semi-deﬁnite kernel.
StoppingCriteria : Weelaborateonthestoppingcriteriaforthediﬀerentalgorithmsusedinthealternating
minimization framework in Algorithm 2.
•OpMINRES : The stopping criterion for OpMINRES is based on the following condition: the loop
exits if the value of relative residual norms between the current residual norm and the initial residual
norm is less than a threshold (e.g. 10−3was used in our implementation).
•Projected Gradient Descent : The projected gradient descent steps for both vech (L)and vech (D)
in Algorithm 2 use similar stopping criterion where the norm of diﬀerence between two consecutive
iterates is compared to be less than a threshold (e.g. 10−3was used in our implementation).
•MCP Regularization : The sparsity-inducing MCP regularization of vech (L)in Algorithm 1 com-
pares the norm of diﬀerence between two consecutive iterates against a threshold (considered as
10−3in our implementation) as the stopping criterion.
7.1 Experiments with synthetic data
Data Generation : For synthetic experiments, three sets of experiments have been considered with input
functions for graph structures having 3-nodes, 12-nodes and 25-nodes, respectively. The input functions are
generated based on weighted cosine functions and constant functions with random noise. The corresponding
output function is based on weighted sine functions sharing the weights between input functions and output
function (details are given in Appendix A.10).
For all the methods, a truncated trigonometric basis of L2([0,2π])with 30 basis functions has been consid-
ered for encoding the functional data. The experiments were run for three settings where the data has been
divided randomly into a training set, a validation set and a test set. The following data splits have been con-
sidered: (80/20/20), (160/40/40) and (320/80/80), representing the number of training samples/validation
samples/test samples.
The results for synthetic data with 12 nodes are summarized in Tables 1-2. From Table 2, we observe
that Sparse OpMINRES-L-D obtains comparable performance based on mean RSSE on the test data in all
three settings where 80 samples, 160 samples and 320 samples have been used for training. Both fglasso-
OpMINRES-D and KGL-OpMINRES-D essentially predict a graph structure ﬁrst and then use it for the
functional regression problem. Sparse OpMINRES-L-D and Sparse Non-Pos-OpMINRES-L-D present a
uniﬁed approach which incorporates sparse graph learning with the functional regression task. In Table
1, the learned graphs are illustrated where darker colors of edges indicate larger edge weights. Table 1
illustrates that fglasso-OpMINRES-D fails to diﬀerentiate between the interactions of input functions and
results in a fully connected graph consistently. Though, KGL-OpMINRES-L-D in comparison to fglasso-
OpMINRES-L-D produces a sparser graph structure, Sparse-OpMINRES-L-D learns sparse graph structures
exhibiting relations that can incorporate the associations enforced in the generation process. The learned
associations provide required correlations which can beneﬁt the functional regression task. Further details
of the synthetic data including experiments for 3 nodes and 25 nodes are given in Appendix A.10. Sparse
Non-Pos-OpMINRES-L-D also learns sparse graph structures which are informative of synthetic data used
for the functional regression task.
27Published in Transactions on Machine Learning Research (03/2024)
Table 1: Graphs corresponding to learned Lfor 12-node synthetic data. [Best viewed in color]
Train/
Val/
Test
samplesSparse
OpMINRES-L-Dfglasso-
OpMINRES-DKGL-
OpMINRES-DNon-Pos-
OpMINRES-L-D
80/20/20
160/40/40
320/80/80
Additional Experiments: Appendix A.10.5 contains the results of experiments conducted as an ablation
study in the 12-node setting.
7.2 Experiments on weather data
Weather data is dynamic and inter-relationships between diﬀerent parameters can be hard to predict. As our
problem solves a functional regression problem based on a relationship between a set of input functions, we
intend to showcase the eﬀectiveness of the proposed algorithm by predicting average dew-point temperature
28Published in Transactions on Machine Learning Research (03/2024)
Table 2: Mean RSSE results for 12-node synthetic data.
Train/Val/Test samples MethodsMean RSSE
Train Val Test
Sparse OpMINRES-L-D 1.140691 1.780445 1.583640
80/20/20 fglasso-OpMINRES-D 1.243734 1.821687 1.700265
KGL-OpMINRES-D 1.061473 1.775388 1.554853
Sparse Non-Pos-OpMINRES-L-D 1.167264 1.806093 1.618175
Sparse OpMINRES-L-D 0.888574 1.229568 1.385952
160/40/40 fglasso-OpMINRES-D 0.956907 1.285154 1.305025
KGL-OpMINRES-D 0.983432 1.260719 1.286481
Sparse Non-Pos-OpMINRES-L-D 1.154356 1.362239 1.417921
Sparse OpMINRES-L-D 1.062102 1.294110 1.239181
320/80/80 fglasso-OpMINRES-D 0.947426 1.336192 1.271646
KGL-OpMINRES-D 0.980995 1.299266 1.252706
Sparse Non-Pos-OpMINRES-L-D 1.073292 1.295140 1.243346
(F) across 12 weather stations based on their respective air temperatures (F). We consider 1 minute data
of Wyoming ASOS data collected from IEM ASOS One Minute Data (Iowa Environmental Mesonet, 2022).
The data has been collected for an interval of 2 hours for both input functions and output function from
January, 2022 to August, 2022. Data collected at one minute interval for diﬀerent 12 weather stations in
Wyoming was pre-processed to create 2 hour interval data by disregarding intervals where data was missing
in any of the 12 stations. A total of 718 samples have been collected after removing missing data.
For all the methods, a truncated trigonometric basis of L2([0,1])with 80 basis functions has been considered
for encoding the functional data. We segregate the weather data experiments into small weather data
experiments (Appendix A.10) by considering 120 samples and full weather data experiments. The following
randomdatasplitshavebeenconsidered: (80/20/20)and(472/123/123), representingthenumberoftraining
samples/validation samples/test samples in small weather data and full weather data settings, respectively.
Tables 3-4 showcase the performance of the algorithms for full weather data considering all 718 samples.
Sparse OpMINRES-L-D performs the best in terms of mean RSSE on the test data compared to fglasso-
OpMINRES-L-D and KGL-OpMINRES-L-D (Table 4). The maps in Table 3 describe the geographic posi-
tioning of the weather stations in Wyoming and the edges between them indicate potential inter-relations
between the stations. In Table 3, fglasso-OpMINRES-L-D and KGL-OpMINRES-L-D learn dense fully con-
nected graphs which do not provide much information regarding the impact of diﬀerent weather stations on
therelationshipofrespectiveairtemperatureto theaveragedewpointtemperature. SparseOpMINRES-L-D
learns a sparse Lwhere stations BPI(1) and CPR(2), P60(7) and SHR(10) along with RIW(8) and WRL(12)
are connected. BPI(1) (42.58507,−110.11115)and CPR(2) (42.908,−106.46442)are 300.7 km apart with an
elevation of 2124 m and 1612 m, respectively. P60(7) (44.54444,−110.42111)and SHR(10) (44.77,−106.97)
are 274.85 km apart with an elevation of 2368 m and 1209 m, respectively. RIW(8) (43.06423,−108.45984)
and WRL(12) (43.96571,−107.95083)are 108.28 km apart with an elevation of 1688 m and 1294 m. It can
be observed that the connections in the learned graph structure have been established between stations with
varying elevations lying in close proximity latitude-wise.
To illustrate the utility of our proposed sample-based approximation algorithm, we use the full weather data
and evaluate it to produce the results in Table 5 for all algorithms. The results in Table 5 show that the
sample-based approximation algorithm provides comparable results using only a few samples. In 5 runs,
out of 472 training samples, the number of samples in the working set of the sample-based approximation
algorithm varies between 123 to 200. Sparse OpMINRES-L-D performs the best in terms of the mean RSSE
on test data.
29Published in Transactions on Machine Learning Research (03/2024)
Table 3: Graphs corresponding to learned Lfor full weather (472/123/123) data. [Best viewed in color]
Sparse OpMINRES-L-D fglasso-OpMINRES-D KGL-OpMINRES-D
Table 4: Mean RSSE results for full weather data.
Train/Val/Test samples MethodsMean RSSE
Train Val Test
Sparse OpMINRES-L-D 0.002938 0.009891 0.010743
472/123/123 fglasso-OpMINRES-D 0.021094 0.013476 0.044216
KGL-OpMINRES-D 0.003474 0.010877 0.012797
Table 5: RSSE (mean ±standard deviation) results over 5 runs for sample-based approximation algorithm
using full weather data.
MethodsMean RSSE
Full Train Train subset Val Test
Sparse OpMINRES-L-D 0.083688±0.011229 0.010584±0.006664 0.013568±0.000914 0.097118±0.01392
fglasso-OpMINRES-D 0.112057±0.004981 0.011457±0.006908 0.014289±0.000710 0.130591±0.006171
KGL-OpMINRES-D 0.131076±0.016523 0.010166±0.005781 0.013702±0.000561 0.100619±0.017939
30Published in Transactions on Machine Learning Research (03/2024)
Table 6: Mean RSSE results for NBA data.
Train/Val/Test samples MethodsMean RSSE
Train Val Test
233/59/59 Sparse OpMINRES-L-D 0.025200 0.087748 0.106344
OpMINRES-D 0.023261 0.191459 0.265513
Table 7: RSSE (mean ±standard deviation) results for sample-based approximation algorithm for NBA
data.
MethodsMean RSSE
Full Train Train subset Val Test
Sparse OpMINRES-L-D 0.215943±0.002837 0.018371±0.001624 0.066631±0.003058 0.147214±0.004665
OpMINRES-D 9.070595±0.138830 0.005005±0.002031 0.180238±0.013534 0.452081±0.020617
7.3 Experiments on NBA data
The movement of basketball and 21 players involved on the court ( x-ycoordinates) in the Atlanta Hawks
(ATL) vs Utah Jazz (UTA) match on November 15, 2015 has been considered in this experiment. This
data is available in the Github repo NBA Movement Data (Seward, 2018). The data has been collected for
diﬀerent plays for both input functions of 21 players and output function denoting the position of the ball,
which includes missing data corresponding to some players in diﬀerent plays. As plays in a basketball game
are of diﬀerent time duration, we use a truncated trigonometric basis of L2([0,1])with 80 basis functions to
sample the functions at ﬁxed 100 points on [0,1]. A total of 351 samples have been collected after removing
missing data. A random data split of (233/59/59) representing the number of training samples/validation
samples/test samples has been considered. The problem requires solving a multi-dimensional functional
regression problem which is incompatible with fglasso and KGL algorithms, as both fglasso & KGL are based
on single dimensional input functions. Hence, we compare our method with the algorithm OpMINRES-D
where a ﬁxed Lis incorporated in our alternating minimization framework.
OpMINRES-D : A ﬁxedLis considered corresponding to a fully connected network of 21 nodes. This
decision was made as fglasso mostly learns a fully connected graph in earlier experiments. Thus, a ﬁxed L
(with no sparsity-inducing MCP) is used in the proposed alternating minimization regime for optimizing u
andD. OpMINRES is used with k1(x,x/prime;G) =e−γx(x−x/prime)/latticetop(L+D)(x−x/prime)−γy(x−x/prime)/latticetop(L+D)(x−x/prime)andk1
2(s,t) =
e−γ1
op|s−t|,k1
2(s,t) =e−γ2
op|s−t|, whereγx,γy∈{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}andγ1
op,γ2
op∈
{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}.
Sparse OpMINRES-L-D : We consider the graph-induced operator-valued kernels using k1(x,x/prime;G) =
e−γx(x−x/prime)/latticetop(L+D)(x−x/prime)−γy(x−x/prime)/latticetop(L+D)(x−x/prime)andk1
2(s,t) =e−γ1
op|s−t|,k2
2(s,t) =e−γ2
op|s−t|, whereγx,γy∈
{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}andγ1
op,γ2
op∈{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}.
Projected gradient descent is used in minimization with respect to LandDbased on a decaying step-size.
The sparsity is aided by the MCP regularization considered in learning of L.
The results are illustrated in Tables 6 and 8 where comparison method OpMINRES-D uses a fully connected
graph, however Sparse OpMINRES-L-D performs better with a sparse learned graph in terms of mean RSSE
on the test data. Observations for the match have been published in the match reports ESPN match recap
and ESPN match scoreboard (ESPN, 2015a;b). In Table 8, the depiction of a basketball court is provided
where the players have been arranged on the court with ATL players on the left and UTA players on the
right. The graphical structure corresponding to the learned Lin Table 8 illustrates strategic relationships
between players of both ATL and UTA. The connection between Derrick Favors—Trevor Booker (6—8) had
been pivotal for Utah Jazz. The performance of Al Horford in (4—7) and Kent Bazemore in (2—11) for
Atlanta Hawks has been captured. Though the partnership of Alec Burks—Trey Burke (9—18) for Utah
31Published in Transactions on Machine Learning Research (03/2024)
Jazz is not evident in the match reports, their ball carrying interactions may be the reason for being learned
inL.
Table 8: Graphs corresponding to learned Lfor NBA data (233/59/59) of ATL (left) vs UTA (right) match
using Sparse OpMINRES-L-D. [Best viewed in color]
The performance of sample-based approximation algorithm has been showcased on NBA data in Table 7.
Using the sample-based approximation algorithm, out of 233 training samples, the number of samples chosen
in the working set of samples in 5 runs varies between 57 to 62. Sparse OpMINRES-L-D performs the best
in terms of the mean RSSE on test data. The remaining results and details of experiments are in Appendix
A.10. The best hyperparameters for the experiments conducted have been listed in Appendix A.10.4.
32Published in Transactions on Machine Learning Research (03/2024)
8 Conclusion
In this work, we incorporate learning of a suitable graphical structure which drives a functional regression
problem where the output function depends on the input functions and also upon their inter-relationships
with each other. An alternating minimization based algorithm has been proposed to learn the Laplacian
matrixL, a non-negative diagonal matrix Dcharacterizing the graphical structure, along with the map
from input space to the output space. For a ﬁxed LandD, the functional regression learning problem is
formulated as an operator system of equations which is solved by using OpMINRES algorithm. Projected
gradient descent is used to learn the Laplacian matrix and the non-negative diagonal matrix in the alter-
nating minimization framework. A sparsity-inducing regularizer (e.g. MCP) in Lhas been incorporated
during the alternating minimization, which helps in learning a graphical structure and allows for improved
interpretability and can highlight interactions which are most relevant among input functions useful for the
prediction. To make the proposed algorithm scalable, a sample-based approximation algorithm has been
proposed which helps reduce the computations required for solving linear system of operator equations using
OpMINRES algorithm. An extension of the alternating minimization framework has also been proposed to
solve the multi-dimensional functional regression problem assuming a single graphical structure on the input
variables. The generalization analysis provides a bound on generalization error for learning a graph-induced
OVK. Experiments establish the utility of proposed graph-induced operator-valued kernels in functional
regression problems from diverse applications.
Broader Impact Statement
The framework and algorithms introduced in the paper with graph-induced operator-valued kernels aid in
learning a sparse graphical structure which drives a functional regression problem where the output function
depends on the input functions and their inter-relationships with each other. This will promote research in
investigating more sophisticated techniques for handling functional data with an inherent graphical structure
ingrained among them. To the best of our knowledge, our work does not have any negative impact.
Acknowledgments
We thank our anonymous reviewers for their insightful comments and suggestions. We declare no competing
interests.
References
Roya Aliakbarisani, Abdorasoul Ghasemi, and M. Ángeles Serrano. Perturbation of the normalized laplacian
matrix for the prediction of missing links in real networks. IEEE Transactions on Network Science and
Engineering , 9(2):863–874, 2022. doi: 10.1109/TNSE.2021.3137862.
M. S. Andersen, J. Dahl, Z. Liu, and L. Vandenberghe. Interior-point methods for large-scale cone program-
ming. In S. Sra, S. Nowozin, and S. J. Wright (eds.), Optimization for Machine Learning , volume 55-83.
MIT Press, 2012.
Martin S Andersen, Joachim Dahl, Lieven Vandenberghe, et al. Cvxopt: A python package for convex
optimization. version 1.3 , 2023. URL https://cvxopt.org .
FrancisR.BachandMichaelI.Jordan. Predictivelow-rankdecompositionforkernelmethods. In Proceedings
of the 22nd International Conference on Machine Learning , ICML ’05, pp. 33–40, New York, NY, USA,
2005. Association for Computing Machinery. ISBN 1595931805. doi: 10.1145/1102351.1102356. URL
https://doi.org/10.1145/1102351.1102356 .
Juan Lucas Bali, Graciela Boente, David E. Tyler, and Jane-Ling Wang. Robust functional principal
components: A projection-pursuit approach. The Annals of Statistics , 39(6):2852 – 2882, 2011. doi:
10.1214/11-AOS923. URL https://doi.org/10.1214/11-AOS923 .
33Published in Transactions on Machine Learning Research (03/2024)
Onureena Banerjee, Laurent El Ghaoui, and Alexandre d’Aspremont. Model selection through sparse maxi-
mumlikelihoodestimationformultivariategaussianorbinarydata. Journal of Machine Learning Research ,
9(15):485–516, 2008. URL http://jmlr.org/papers/v9/banerjee08a.html .
R. B. Bapat, S. J. Kirkland, and S. Pati. The perturbed laplacian matrix of a graph. Linear and Multi-
linear Algebra , 49(3):219–242, 2001. doi: 10.1080/03081080108818697. URL https://doi.org/10.1080/
03081080108818697 .
Dimitri Bouche, Marianne Clausel, François Roueﬀ, and Florence d’Alché Buc. Nonlinear functional output
regression: A dictionary approach. In Arindam Banerjee and Kenji Fukumizu (eds.), Proceedings of
The 24th International Conference on Artiﬁcial Intelligence and Statistics , volume 130 of Proceedings of
Machine Learning Research , pp. 235–243. PMLR, 13–15 Apr 2021. URL https://proceedings.mlr.
press/v130/bouche21a.html .
Romain Brault. Large-scale operator-valued kernel regression . Theses, Université Paris Saclay, July 2017.
URL https://hal.science/tel-01761768 .
Romain Brault, Markus Heinonen, and Florence Buc. Random fourier features for operator-valued ker-
nels. In Robert J. Durrant and Kee-Eung Kim (eds.), Proceedings of The 8th Asian Conference on
Machine Learning , volume 63 of Proceedings of Machine Learning Research , pp. 110–125, The University
of Waikato, Hamilton, New Zealand, 16–18 Nov 2016. PMLR. URL https://proceedings.mlr.press/
v63/Brault39.html .
Sou-Cheng Choi. Iterative methods for singular linear equations and least-squares problems . PhD thesis,
Stanford University, 2006.
Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of the American
mathematical society , 39(1):1–49, 2002.
Alexandre d’Aspremont, Onureena Banerjee, and Laurent El Ghaoui. First-order methods for sparse
covariance selection. SIAM Journal on Matrix Analysis and Applications , 30(1):56–66, 2008. doi:
10.1137/060670985. URL https://doi.org/10.1137/060670985 .
I.I. Dikin. Iterative solution of problems of linear quadratic programming. Doklady Akademiia Nauk SSSR ,
174:674–675, 1967.
Xiaowen Dong, Dorina Thanou, Pascal Frossard, and Pierre Vandergheynst. Learning laplacian matrix in
smooth graph signal representations. IEEE Transactions on Signal Processing , 64(23):6160–6173, 2016.
doi: 10.1109/TSP.2016.2602809.
ESPN. Jazz beat hawks 97-96 to end 3-game skid. https://www.espn.com/nba/recap/_/gameId/
400828035 , 2015a. Accessed: 15/10/2023.
ESPN. Atlanta hawks vs utah jazz box score. https://www.espn.com/nba/boxscore/_/gameId/
400828035 , 2015b. Accessed: 15/10/2023.
Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle prop-
erties. Journal of the American Statistical Association , 96(456):1348–1360, 2001. doi: 10.1198/
016214501753382273. URL https://doi.org/10.1198/016214501753382273 .
F. Ferraty and P Vieu. Nonparametric Functional Data Analysis, Theory and Practice. Springer Series in
Statistics, New York, 2006.
Mostafa Reisi Gahrooei, Kamran Paynabar, Massimo Pacella, and Jianjun Shi. Process modeling and
prediction with large number of high-dimensional variables using functional regression. IEEE Transactions
on Automation Science and Engineering , 17(2):684–696, 2020. doi: 10.1109/TASE.2019.2941167.
Gene H. Golub and Charles F. Van Loan. Matrix Computations . Johns Hopkins University Press, 3rd ed
edition, 1996.
34Published in Transactions on Machine Learning Research (03/2024)
Ana María Estrada Gómez, Kamran Paynabar, and Massimo Pacella. Functional directed graphical models
and applications in root-cause analysis and diagnosis. Journal of Quality Technology , 53(4):421–437, 2021.
doi: 10.1080/00224065.2020.1805380. URL https://doi.org/10.1080/00224065.2020.1805380 .
Clara Happ and Sonja Greven. Multivariate functional principal component analysis for data observed on
diﬀerent (dimensional) domains. Journal of the American Statistical Association , 113(522):649–659, 2018.
doi: 10.1080/01621459.2016.1273115. URL https://doi.org/10.1080/01621459.2016.1273115 .
Harold V Henderson and SR Searle. Vec and vech operators for matrices, with some uses in jacobians and
multivariate statistics. Canadian Journal of Statistics , 7(1):65–81, 1979.
Harjit Hullait, David S. Leslie, Nicos G. Pavlidis, and Steve King. Robust function-on-function regression.
Technometrics , 63(3):396–409, 2021. doi: 10.1080/00401706.2020.1802350. URL https://doi.org/10.
1080/00401706.2020.1802350 .
Pierre Humbert, Batiste Le Bars, Laurent Oudre, Argyris Kalogeratos, and Nicolas Vayatis. Learning
laplacian matrix from graph signals with sparse spectral representation. Journal of Machine Learning
Research , 22(195):1–47, 2021. URL http://jmlr.org/papers/v22/19-944.html .
Iowa State University Iowa Environmental Mesonet. Iem :: Asos one minute data. https://mesonet.
agron.iastate.edu/request/asos/1min.phtml , 2022. Accessed: 15/10/2023.
Julien Jacques and Cristian Preda. Model-based clustering for multivariate functional data. Computational
Statistics & Data Analysis , 71:92–106, 2014. ISSN 0167-9473. doi: https://doi.org/10.1016/j.csda.2012.
12.004. URL https://www.sciencedirect.com/science/article/pii/S0167947312004380 .
Hachem Kadri, Emmanuel Duﬂos, Philippe Preux, Stéphane Canu, Alain Rakotomamonjy, and Julien Au-
diﬀren. Operator-valued kernels for learning from functional response data. Journal of Machine Learning
Research , 17(20):1–54, 2016. URL http://jmlr.org/papers/v17/11-315.html .
Sven Kurras, Ulrike Luxburg, and Gilles Blanchard. The f-adjusted graph laplacian: a diagonal modiﬁca-
tion with a geometric interpretation. In Eric P. Xing and Tony Jebara (eds.), Proceedings of the 31st
International Conference on Machine Learning , volume 32 of Proceedings of Machine Learning Research ,
pp. 1530–1538, Bejing, China, 22–24 Jun 2014. PMLR. URL https://proceedings.mlr.press/v32/
kurras14.html .
Cornelius Lanczos. An iteration method for the solution of the eigenvalue problem of linear diﬀerential and
integral operators . United States Governm. Press Oﬃce Los Angeles, CA, 1950.
Heng Lian. Nonlinear functional models for functional responses in reproducing kernel hilbert spaces.
Canadian Journal of Statistics , 35(4):597–606, 2007. doi: https://doi.org/10.1002/cjs.5550350410. URL
https://onlinelibrary.wiley.com/doi/abs/10.1002/cjs.5550350410 .
Renjie Liao. Notes on rademacher complexity. https://www.cs.toronto.edu/~rjliao/notes/Notes_on_
Rademacher_Complexity.pdf , 2020. Accessed: 28/10/2023.
AndreasMaurer. Avector-contractioninequalityforrademachercomplexities. InRonaldOrtner, HansUlrich
Simon, and Sandra Zilles (eds.), Algorithmic Learning Theory , pp. 3–17. Springer International Publishing,
2016. ISBN 978-3-319-46379-7.
Giacomo Meanti, Luigi Carratino, Lorenzo Rosasco, and Alessandro Rudi. Kernel methods through the
roof: Handling billions of points eﬃciently. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 14410–14422. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
a59afb1b7d82ec353921a55c579ee26d-Paper.pdf .
Nicolai Meinshausen and Peter Bühlmann. High-dimensional graphs and variable selection with the Lasso.
The Annals of Statistics , 34(3):1436 – 1462, 2006. doi: 10.1214/009053606000000281. URL https:
//doi.org/10.1214/009053606000000281 .
35Published in Transactions on Machine Learning Research (03/2024)
Charles A. Micchelli and Massimiliano Pontil. On learning vector-valued functions. Neural Computation , 17
(1):177–204, 2005. doi: 10.1162/0899766052530802.
Charles A. Micchelli, Massimiliano Pontil, Qiang Wu, and Ding-Xuan Zhou. Error bounds for learning
the kernel. Analysis and Applications , 14(06):849–868, 2016. doi: 10.1142/S0219530516400054. URL
https://doi.org/10.1142/S0219530516400054 .
Junier Oliva, William Neiswanger, Barnabas Poczos, Eric Xing, Hy Trac, Shirley Ho, and Jeﬀ Schneider.
Fast Function to Function Regression. In Guy Lebanon and S. V. N. Vishwanathan (eds.), Proceedings of
the Eighteenth International Conference on Artiﬁcial Intelligence and Statistics , volume 38 of Proceedings
of Machine Learning Research , pp. 717–725, San Diego, California, USA, 09–12 May 2015. PMLR. URL
https://proceedings.mlr.press/v38/oliva15.html .
Xingyue Pu, Tianyue Cao, Xiaoyun Zhang, Xiaowen Dong, and Siheng Chen. Learning to learn
graph topologies. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan (eds.), Advances in Neural Information Processing Systems , volume 34, pp. 4249–4262. Cur-
ran Associates, Inc., 2021a. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/
21e4ef94f2a6b23597efabaec584b504-Paper.pdf .
Xingyue Pu, Siu Lun Chau, Xiaowen Dong, and Dino Sejdinovic. Kernel-based graph learning from smooth
signals: A functional viewpoint. IEEE Transactions on Signal and Information Processing over Networks ,
7:192–207, 2021b. doi: 10.1109/TSIPN.2021.3059995.
Lishan Qiao, Limei Zhang, Songcan Chen, and Dinggang Shen. Data-driven graph construction and
graph learning: A review. Neurocomputing , 312:336–351, 2018. ISSN 0925-2312. doi: https://
doi.org/10.1016/j.neucom.2018.05.084. URL https://www.sciencedirect.com/science/article/pii/
S0925231218306696 .
Xinghao Qiao, Shaojun Guo, and Gareth M. James. Functional graphical models. Journal of the American
Statistical Association , 114(525):211–222, 2019. doi: 10.1080/01621459.2017.1390466. URL https://doi.
org/10.1080/01621459.2017.1390466 .
Xinghao Qiao, Cheng Qian, Gareth M James, and Shaojun Guo. Doubly functional graphical models in high
dimensions. Biometrika , 107(2):415–431, 02 2020. ISSN 0006-3444. doi: 10.1093/biomet/asz072. URL
https://doi.org/10.1093/biomet/asz072 .
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt,
D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems , vol-
ume 20. Curran Associates, Inc., 2007. URL https://proceedings.neurips.cc/paper/2007/file/
013a006f03dbc5392effeb8f18fda755-Paper.pdf .
J. O. Ramsay and C. J. Dalzell. Some tools for functional data analysis. Journal of the Royal Statistical
Society: Series B (Methodological) , 53(3):539–561, 12 2018. ISSN 0035-9246. doi: 10.1111/j.2517-6161.
1991.tb01844.x. URL https://doi.org/10.1111/j.2517-6161.1991.tb01844.x .
James O. Ramsay. When the data are functions. Psychometrika , 47(4):379–396, 1982.
AniruddhaRajendraRaoandMatthewReimherr. Modernnon-linearfunction-on-functionregression. Statis-
tics and Computing , 33(6):1–12, 2023.
Fabrice Rossi and Nathalie Villa. Support vector machine for functional data classiﬁcation. Neurocomputing ,
69(7):730–742, 2006. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2005.12.010. URL https://
www.sciencedirect.com/science/article/pii/S0925231205003309 . New Issues in Neurocomputing:
13th European Symposium on Artiﬁcial Neural Networks.
Akash Saha and Balamurugan Palaniappan. Learning with operator-valued kernels in reproduc-
ing kernel krein spaces. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin
(eds.),Advances in Neural Information Processing Systems , volume 33, pp. 13856–13866. Curran
Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
9f319422ca17b1082ea49820353f14ab-Paper.pdf .
36Published in Transactions on Machine Learning Research (03/2024)
Neil Seward. Nba movement data github repo. https://github.com/sealneaward/nba-movement-data ,
2018. Accessed: 15/10/2023.
John Shawe-Taylor, Nello Cristianini, et al. Kernel methods for pattern analysis . Cambridge university press,
2004.
Alexander J. Smola and Bernhard Schölkopf. Sparse greedy matrix approximation for machine learning. In
Pat Langley (ed.), Proceedings of the Seventeenth International Conference on Machine Learning (ICML
2000), Stanford University, Stanford, CA, USA, June 29 - July 2, 2000 , pp. 911–918. Morgan Kaufmann,
2000.
Aad W Van Der Vaart and Jon A Wellner. Weak convergence and empirical processes: with applications to
statistics , volume 3. Springer, 1996.
Mariana Vargas Vieyra. Robust estimation of laplacian constrained gaussian graphical models with
trimmed non-convex regularization. In Antonio Salmerón and Rafael Rumí (eds.), Proceedings of The
11th International Conference on Probabilistic Graphical Models , volume 186 of Proceedings of Machine
Learning Research , pp. 85–96. PMLR, 05–07 Oct 2022. URL https://proceedings.mlr.press/v186/
vargas-vieyra22a.html .
Christopher Williams and Matthias Seeger. Using the nyström method to speed up kernel machines. In
T. Leen, T. Dietterich, and V. Tresp (eds.), Advances in Neural Information Processing Systems , vol-
ume 13. MIT Press, 2000. URL https://proceedings.neurips.cc/paper_files/paper/2000/file/
19de10adbaa1b2ee13f77f679fa1483a-Paper.pdf .
Qiang Wu and Ding-Xuan Zhou. Analysis of support vector machine classiﬁcation. Journal of Computational
Analysis & Applications , 8(2), 2006.
Fang Yao, Hans-Georg Müller, and Jane-Ling Wang. Functional data analysis for sparse longitudi-
nal data. Journal of the American Statistical Association , 100(470):577–590, 2005. doi: 10.1198/
016214504000001745. URL https://doi.org/10.1198/016214504000001745 .
Jiaxi Ying, José Vinícius de Miranda Cardoso, and Daniel Palomar. Nonconvex sparse graph learning
under laplacian constrained graphical model. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan,
and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 7101–7113. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
4ef42b32bccc9485b10b8183507e5d82-Paper.pdf .
Yiming Ying and Ding-Xuan Zhou. Learnability of gaussians with ﬂexible variances. Journal of Machine
Learning Research , 8(9):249–276, 2007. URL http://jmlr.org/papers/v8/ying07a.html .
Ming Yuan and Yi Lin. Model selection and estimation in the Gaussian graphical model. Biometrika , 94
(1):19–35, 03 2007. ISSN 0006-3444. doi: 10.1093/biomet/asm018. URL https://doi.org/10.1093/
biomet/asm018 .
Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics ,
38(2):894 – 942, 2010. doi: 10.1214/09-AOS729. URL https://doi.org/10.1214/09-AOS729 .
Yangjing Zhang, Kim-Chuan Toh, and Defeng Sun. Learning graph laplacian with mcp. arXiv preprint
arXiv:2010.11559 , 2020.
Boxin Zhao, Y. Samuel Wang, and Mladen Kolar. Direct estimation of diﬀerential functional
graphical models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d /quotesingle.ts1Alché-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran
Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/
7d6044e95a16761171b130dcb476a43e-Paper.pdf .
37Published in Transactions on Machine Learning Research (03/2024)
A Appendix
A.1 Functonal Regression with Known Graph Structure
In this section, we motivate functional regression problem with known graph structure. Consider a system
where a set of input functions determines the output (or response) function. Let the system be modeled
based onpinput functional variables x1(t),x2(t),...,xp(t), wherexi∈L2([0,1]), i∈[p]. A functional
response variable y(t)is used to model output of the system where y∈L2([0,1]). (Note that [0,1]can be
replaced with any closed time interval based on the application.)
The undirected graph structure of the functional input variables is represented by G= (V,E), whereV=
{v1,v2,...,vp}andE={{vi,vj}|viis connected to vj,1≤i,j≤p}is the edge set which characterizes the
underlying relationship between the variables. Note that the notation for an edge uses an unordered pair
{vi,vj}which characterizes the undirected nature of the graph G. In order to model the relation between
functional input variables x1,x2,...,xpand functional response variable y, we use the following map F:
y=F(x1,x2,...,xp,G). (102)
Note thatFnow depends explicitly on the graph Gin addition to the input functions x1,x2,...,xp. Here,
we consider a scenario where Gis known. Recall the example of a manufacturing factory, where the output of
emissions depends on the metrics of diﬀerent components involved in the manufacturing process. The graph
Gis determined in this case by understanding the components which are connected during the manufacturing
process.
We consider the notations x= (x1,x2,...,xp)∈X(= (L2([0,1]))p)andy∈Y(=L2([0,1]))to represent
an arbitrary sample (x,y). To learn the mapping F, consider the training data of nsamples given as/braceleftbig
(x(i),y(i))/bracerightbign
i=1, wherex(i)= (x(i)
1,x(i)
2,...,x(i)
p)∈Xandy(i)∈Y. In order to learn F, we develop an
operator-valued kernel which can leverage the structural information of G.
Towards this, we ﬁrst introduce an operator-valued kernel which maps the elements of X×Xto a set of
bounded linear operators over the output space Y, denoted byL(Y). We formally deﬁne OVK as follows.
Deﬁnition A.1 (Operator-valued Kernel ).(Kadri et al., 2016) An L(Y)-valued kernel KonX2is a
functionK(.,.) :X×X→L (Y), satisfying the following properties:
1.Kis Hermitian, that is ∀w,z∈X,K(w,z) =K(z,w)∗(∗denotes the adjoint operator),
2.Kis positive semi-deﬁnite on X2, that isKis Hermitian and for every natural number rand
all{(w(i),u(i))i∈[r]}∈X×Y , the matrix with (i,j)-th entry given by /angbracketleftK(w(i),w(j))u(i),u(j)/angbracketrightYis
positive semi-deﬁnite.
Constructing an operator-valued kernel based on Deﬁnition A.1 is a challenge as verifying both properties
of being Hermitian and positive semi-deﬁniteness becomes non-trivial. A construction of OVK that satisﬁes
both the properties in Deﬁnition A.1 has been proposed in (Lian, 2007; Kadri et al., 2016). The OVK
construction in (Lian, 2007; Kadri et al., 2016) uses a scalar-valued kernel k1onX×Xand a Hilbert-
Schmidt integral (HSI) operator deﬁned on the output space Y, and is given as follows:
(K(x,x/prime)y)(t) =k1(x,x/prime)/integraldisplay1
0k2(s,t)y(s)ds, (103)
wherek2inside the HSI operator/integraltext1
0k2(s,t)y(s)dsis a scalar-valued kernel on R×R. Ifk1is positive
semi-deﬁnite and if k2is positive semi-deﬁnite (implying that the HSI operator is positive semi-deﬁnite),
then the construction in (103) is known to be positive semi-deﬁnite (Kadri et al., 2016). We will now adapt
the OVK in (103) to include the graph structure information present in G. An obvious choice for using
the inﬂuence of graphical structure Gin the functional regression task is to use the adjacency matrix of G,
but the adjacency matrix of Gnot being necessarily positive semi-deﬁnite makes its utility restrictive. The
Laplacian matrix of a graph G, on the other hand is useful in this respect as it has the desired property of
38Published in Transactions on Machine Learning Research (03/2024)
being positive semi-deﬁnite which is useful in a kernel-based learning framework. The Laplacian matrix of
an undirected graph G= (V,E), withVas the node set and Eas edge set is deﬁned as L=D−A, where
D=diag(deg(v1),deg(v2),...,deg(vp))is the degree matrix and Ais the adjacency matrix of the graph G.
The elements of Lare given by
Li,j=

deg(vi),ifi=j,
−1,ifi/negationslash=jand{vi,vj}∈E,
0, otherwise.(104)
We propose to incorporate the graphical structure in Equation (103) within the scalar-valued kernel k1itself,
as follows:
k1(x,x/prime;G) =γx/latticetopLx/prime, γ > 0, (105)
=γp/summationdisplay
i,j=1/integraldisplay1
0xi(t)Lijx/prime
j(t)dt. (106)
The construction of k1in (105) involves capturing the graphical structure of GusingLinx/latticetopLx/prime. We use
an equivalent expression /angbracketleftx,Lx/prime/angbracketrightp:=x/latticetopLx/prime, where the inner product /angbracketleft·,·/angbracketrightp:X×X → Ris deﬁned as
/angbracketleftx,x/prime/angbracketrightp=/summationtextp
i=1/integraltext1
0xi(t)x/prime
i(t)dt. The inner product /angbracketleftx,x/prime/angbracketrightpmeasures the similarity between xandx/primeinX.
LetL= [L1,L2,...,Lp], whereLirepresents the i-th column of L, thenLx/primeis computed based on standard
matrix-vector multiplication where elements of Lare multiplied with x/primeby using scalar multiplication and
addition of functions as Lx/prime=/summationtextp
i=1Lix/prime
i. Equation (105) provides a tool to measure similarity between
x,x/prime∈Xwhere the interactions are encoded in the underlying graph Gusing the Laplacian matrix L. Ifk1
deﬁned in (105) can be proved to be a valid positive semi-deﬁnite scalar-valued kernel on X×X, then an
OVK can be deﬁned similar to the construction in Equation (103). Next, we prove that such a construction
is indeed possible.
Proposition A.1. For an underlying graph G= (V,E)with|V|=p, functional variables x=
(x1,x2,...,xp)∈X (= (L2([0,1]))p)and a functional response variable y∈Y(=L2([0,1])), consider an
operator-valued kernel K:X×X→L (Y)deﬁned as
(K(x,x/prime)y)(t) =k1(x,x/prime;G)/integraldisplay1
0k2(s,t)y(s)ds,
wherek1(x,x/prime;G) =γx/latticetopLx/prime,γ > 0andk2is a positive semi-deﬁnite scalar-valued kernel on R×R. Then
Kis positive semi-deﬁnite.
Proof.Please see Appendix A.3 for the proof.
Recall the construction of scalar-valued radial basis function (RBF) kernel deﬁned over Rd×Rd(d∈Z+)as
e−γ/bardbl x− x/prime/bardbl2, forγ >0, x, x/prime∈Rdbased on the kernel x/latticetopx. Similar to that construction, we now describe an
extension for kernel k1deﬁned in (105). The kernel notation k1(x,x/prime;G) =γx/latticetopLx/primeis overloaded to represent
the following RBF-type kernel:
k1(x,x/prime;G) =e−γ(x−x/prime)/latticetopL(x−x/prime), γ > 0. (107)
capturing the interaction of x,x/primeusingL(see Appendix A.3). The RBF-type kernel in (107) is an improved
version of the kernel in (105), as it can approximate higher dimensional relationships better owing to the
exponentialnatureandshiftinvariantpropertygivenby k1(x+h,x/prime+h;G) =k1(x,x/prime;G),∀x,x/prime,h∈X. Note
that when computing (x−x/prime)/latticetopL(x−x/prime), wherex= (x1,...,xp),x/prime= (x/prime
1,...,x/prime
p)∈X, the interactions
between the unlike pair (xi,x/prime
j),i/negationslash=jwould negate the inﬂuence of the like pair (xi,x/prime
i), because of the
structure of L(deﬁned in (104)). Therefore, we propose to use a diagonally perturbed Laplacian (Bapat
et al., 2001) to aid the functional regression task performance. Perturbed Laplacians have found applications
in spectral clustering, analysis of graphs (Kurras et al., 2014) and missing link prediction in networks
39Published in Transactions on Machine Learning Research (03/2024)
(Aliakbarisani et al., 2022). A natural perturbation of Laplacian for the scalar-valued kernel k1in (107)
involves the degree matrix Dleading to the following deﬁnition for k1:
k1(x,x/prime;G) =e−γ(x−x/prime)/latticetop(L+D)(x−x/prime), γ > 0,forx,x/prime∈X. (108)
Incorporating DwithLink1improves the representation of individual components of x,x/primein prediction
ofyas the like pair (xi,x/prime
i)gets weighed by Di,i+Li,iin the kernel expression which compensates for the
negation eﬀect described above. This enables us to deﬁne a family of operator-valued kernels discussed next,
which is induced by the graphical structure information.
Deﬁnition A.2 (Graph-induced Operator-valued Kernel for known G).A graph-induced operator-
valued kernel is deﬁned as
(KG(x,x/prime)y)(t) =k1(x,x/prime;G)/integraldisplay1
0k2(s,t)y(s)ds, (109)
wherek2is a scalar-valued kernel on R2,Gis a graph associating the pinput functions in (x1,...,xp)∈X
andk1is deﬁned as
k1(x,x/prime;G) =e−γ(x−x/prime)/latticetop(L+D)(x−x/prime),
forγ >0whereLis the Laplacian matrix and Dis the degree matrix of the graph G.
KGassociates a pair x,x/prime∈Xwith output function y∈YwhereGis the graph which incorporates the
interaction of pconstituent input functions of xandx/prime. The addition of DtoLin (108) preserves the positive
semi-deﬁniteness of the kernel k1asDis a diagonal matrix with positive entries. Using graph-induced OVK
for functional regression problem requires associating KGwith a function-valued reproducing kernel Hilbert
space where the map Ffrom (102) resides. The existence of a bijection between the set of positive semi-
deﬁnite (Mercer) operator-valued kernels and function-valued reproducing kernel Hilbert spaces has been
established in (Kadri et al., 2016). A function-valued reproducing kernel Hilbert space (RKHS) is deﬁned
as follows.
Deﬁnition A.3 (Function-valued RKHS ).(Kadri et al., 2016) A Hilbert space Hof functions from X
toYis called a reproducing kernel Hilbert space if there is a positive semi-deﬁnite L(Y)-valued kernel Kon
X2such that:
1. the function z/mapsto→K(w,z)gbelongs toH,∀w,z∈Xand∀g∈Y,
2. for every F∈H,w∈Xandg∈Y,/angbracketleftF,K(w,.)g/angbracketrightH=/angbracketleftF(w),g/angbracketrightY.(reproducing property)
Property 1 in Deﬁnition A.3 provides an association of OVK Kwith the spaceHwhich contains maps from
XtoY. The reproducing property in Deﬁnition A.3 helps to relate the inner product in Hto the inner
product inY. Using Proposition A.1 and Deﬁnition A.3, the positive semi-deﬁniteness of KGconstructed
using (109) ensures that there exists a unique RKHS HKGcorresponding to KG(Theorem 1 (Kadri et al.,
2016)). This enables us to formulate a learning problem in HKGas follows:
/tildewideFλ= arg min
F∈HKGn/summationdisplay
i=1/bardbly(i)−F(x(i))/bardbl2
Y+λ/bardblF/bardbl2
HKG, (110)
whereHKGis the function-valued RKHS induced by the graph-induced operator-valued kernel KGand
/bardbl·/bardblHKGdenotes the norm in HKG. In order to solve the optimization problem, we utilize the reproducing
property of operator-valued kernel (Deﬁnition A.3) given by
/angbracketleftF,KG(x,.)h/angbracketrightHKG=/angbracketleftF(x),h/angbracketrightY,∀x∈X,h∈Y. (111)
The minimization problem in (110) is not tractable using a search based procedure over HKG, hence the
reproducing property of operator-valued kernel KGin (111) can be leveraged to simplify the problem and
characterizethesolutionof(110)usingelementsoftheoutputspace Y. Wenowprovidearepresentertheorem
for the minimization problem (110) in the function-valued RKHS HKGcorresponding to the operator-valued
kernelKG.
40Published in Transactions on Machine Learning Research (03/2024)
Theorem A.2 (Representer theorem ).LetKGbe an operator-valued kernel and HKGbe its corre-
sponding function-valued reproducing kernel Hilbert space. The solution /tildewideFλ∈ HKGof the regularized
optimization problem: /tildewideFλ= arg minF∈HKG/summationtextn
i=1/bardbly(i)−F(x(i))/bardbl2
Y+λ/bardblF/bardbl2
HKG,whereλ>0,F∈HKG, has
the following form
/tildewideFλ(.) =n/summationdisplay
i=1KG(x(i),.)ui,whereui∈Y. (112)
Proof.Please ﬁnd the proof in Appendix A.4.
Using the representer theorem with reproducing property of operator-valued kernel, we provide below (Ap-
pendix A.8) the linear system of operators to determine ui,fori∈[n]:
(K+λI)u=y, (113)
where Kis a block operator matrix given by Ki,j=KG(x(i),x(j)),u= [u1,u2,...,un]/latticetopandy=
[y(1),y(2),...,y(n)]/latticetop. A simple inversion of K+λImay not be straightforward to obtain uin (113), for an
arbitrary choice of OVK KG. Saha & Palaniappan (2020) proposed an iterative operator minimum residual
(OpMINRES) algorithm which adapts a Krylov subspace minimal residual (MINRES) algorithm to solve
operator-based linear system of the form in (113). We delve deeper into the details of OpMINRES in Section
5.1.1. With the learned uobtained by solving (113), for any sample ˆx∈X, the prediction is given by
/tildewideFλ(ˆx) =n/summationdisplay
i=1KG(x(i),ˆx)ui,whereui∈Y. (114)
In (114), functions ui∈Yfori∈[n], can be considered as basis functions for the space Yand opera-
torsKG(x(i),ˆx)fori∈[n], correspond to operator-valued coeﬃcients of the basis functions. The term
KG(x(i),ˆx)uiamounts to the total contribution of sample x(i)in determining the prediction for ˆx.
A.2 Mathematical Derivations
We cover derivations which can transform the objective function where the optimization takes place over the
output spaceYinstead of the RKHS HKinduced by the OVK K. The following derivation has been referred
in Section 5.1 to obtain (5) from (4), where we simplify the expression J(F,L,D )to obtainJ(u,L,D ).
min
F,L,DJ(F,L,D ) =n/summationdisplay
i=1/bardbly(i)−F(x(i))/bardbl2
Y+λ/bardblF/bardbl2
HK+ρLn/summationdisplay
i=1x(i)/latticetopLx(i)+ρD/bardblD/bardbl2
F (115)
=⇒min
u,L,DJ(u,L,D ) =n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)−n/summationdisplay
j=1K(x(i),x(j))uj/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y+λ/angbracketleftBiggn/summationdisplay
i=1K(x(i),.)ui,n/summationdisplay
j=1K(x(j),.)uj/angbracketrightBigg
HK(116)
+ρLn/summationdisplay
i=1x(i)/latticetopLx(i)+ρD/bardblD/bardbl2
F
=n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)−n/summationdisplay
j=1K(x(i),x(j))uj/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y+λn/summationdisplay
i,j=1/angbracketleftK(x(i),x(j))ui,uj/angbracketrightY (117)
+ρLn/summationdisplay
i=1x(i)/latticetopLx(i)+ρD/bardblD/bardbl2
F.
Theexpression(116)isobtainedfrom(115)byusingtherepresentertheoremandthereproducibilityproperty
ofKis utilized to obtain (117).
Similarly, we obtain the objective function from (29) in Section 5.1.5 using representer theorem and the
reproducibility property of Kgiven as follows:
41Published in Transactions on Machine Learning Research (03/2024)
J(F1,F2,...,Fs,L,D ) =s/summationdisplay
l=1/parenleftBiggn/summationdisplay
i=1/bardbly(i)
l−Fl(x(i))/bardbl2
Y+λ/bardblFl/bardbl2
Hl
K/parenrightBigg
+r/summationdisplay
l=1ρl/parenleftBiggn/summationdisplay
i=1x(i)
l/latticetopLx(i)
l/parenrightBigg
+ρD/bardblD/bardbl2
F
=⇒J(u1,u2,...,us,L,D ) =
s/summationdisplay
l=1
n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)
l−n/summationdisplay
j=1Kl(x(i),x(j))ul
j/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y+λn/summationdisplay
i,j=1/angbracketleftKl(x(i),x(j))ul
i,Kl(x(j),.)ul
j/angbracketrightY
 (118)
+r/summationdisplay
k=1/parenleftBigg
ρkn/summationdisplay
i=1x(i)
k/latticetopLx(i)
k/parenrightBigg
+ρD/bardblD/bardbl2
F
=s/summationdisplay
l=1
n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)
l−n/summationdisplay
j=1Kl(x(i),x(j))ul
j/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y+λn/summationdisplay
i,j=1/angbracketleftKl(x(i),x(j))ul
i,ul
j/angbracketrightY
 (119)
+r/summationdisplay
k=1/parenleftBigg
ρkn/summationdisplay
i=1x(i)
k/latticetopLx(i)
k/parenrightBigg
+ρD/bardblD/bardbl2
F.
A.3 Positive semi-deﬁniteness of OVK
In this section, we cover the proof of Proposition A.1 in Section A.1 which helps in building a graph-induced
OVK. The major contribution is based on showing that the proposed scalar-valued kernel in graph-induced
OVK is a valid positive semi-deﬁnite kernel on X×X. We recall Proposition A.1 below.
Proposition A.3. For an underlying graph G= (V,E)with|V|=p, functional variables x=
(x1,x2,...,xp)∈ X(= (L([0,1]))p)and a functional response variable y∈ Y(=L2([0,1])), consider an
operator-valued kernel K:X×X→L (Y)deﬁned as
(K(x,x/prime)y)(t) =k1(x,x/prime;G)/integraldisplay1
0k2(s,t)y(s)ds,
wherek1(x,x/prime;G) =γx/latticetopLx/prime,γ > 0andk2is a positive semi-deﬁnite scalar-valued kernel on R×R. Then
Kis positive semi-deﬁnite.
Proof.In order to prove the positive semi-deﬁniteness of K, it is suﬃcient to prove the positive semi-
deﬁniteness of k1based on the construction followed in (Kadri et al., 2016). We focus on the term x/latticetopLx/prime
ink1. Let{x1,x2,...,xl}⊂Xbe a ﬁnite set of points. Consider any vector α∈RlandKbe al×lkernel
matrix given by K= [k1(xi,xj;G)]i,j.
Now, we recall that the space L2([0,1])has the following inner product and norm:
/angbracketleftf,g/angbracketright=/integraldisplay1
0f(t)g(t)dt, f,g∈L2([0,1]),
/bardblf/bardbl=/parenleftbigg/integraldisplay1
0f2(t)dt/parenrightbigg1/2
.
We deﬁne/angbracketleft.,./angbracketrightp:X×X→ Ras
/angbracketleftx,w/angbracketrightp=/angbracketleftx1,w1/angbracketright+/angbracketleftx2,w2/angbracketright+···+/angbracketleftxp,wp/angbracketright (120)
=p/summationdisplay
i=1/integraldisplay1
0xi(t)wi(t)dt, (121)
42Published in Transactions on Machine Learning Research (03/2024)
wherex= (x1,x2,...,xp)∈X,w= (w1,w2,...,wp)∈X. We can show that /angbracketleft.,./angbracketrightpis an inner product on
Xover the ﬁeld R.
•The symmetry of /angbracketleft.,./angbracketrightpfollows from the deﬁnition in equation (121) as:
/angbracketleftx,w/angbracketrightp=/angbracketleftw,x/angbracketrightp.
•Linearity:
/angbracketleftax+bw,z/angbracketrightp=/angbracketleftax1+bw1,z1/angbracketright+/angbracketleftax2+bw2,z2/angbracketright+···+/angbracketleftaxp+bwp,zp/angbracketright
=p/summationdisplay
i=1/integraldisplay1
0(axi(t) +bwi(t))z(t)dt
=ap/summationdisplay
i=1/integraldisplay1
0xi(t)zi(t)dt+bp/summationdisplay
i=1/integraldisplay1
0wi(t)zi(t)dt
=a/angbracketleftx,z/angbracketrightp+b/angbracketleftw,z/angbracketrightp.
•Positive semi-deﬁniteness: For any non-zero x∈X,
/angbracketleftx,x/angbracketrightp=/angbracketleftx1,x1/angbracketright+/angbracketleftx2,x2/angbracketright+···+/angbracketleftxp,xp/angbracketright
=p/summationdisplay
i=1/integraldisplay1
0x2
i(t)dt
=p/summationdisplay
i=1/bardblxi/bardbl2≥0.
The norm induced by /angbracketleft.,./angbracketrightponXis given by
/bardblx/bardblp=/radicalBig
/angbracketleftx,x/angbracketrightp
=/parenleftBiggp/summationdisplay
i=1/integraldisplay1
0x2
i(t)dt/parenrightBigg1/2
.
On considering xi,xj∈X,L∈Rp×p, the quantity xi/latticetopLxjcan be deﬁned as
xi/latticetopLxj=/parenleftbigxi
1xi
2... xi
p/parenrightbig
L1,1L1,2... L 1,p
L2,1L2,2... L 2,p
............
Lp,1Lp,2... Lp,p

xj
1
xj
2...
xj
p

=/parenleftbigxi
1xi
2... xi
p/parenrightbig
L1,1xj
1+L1,2xj
2+···+L1,pxj
p
L2,1xj
1+L2,2xj
2+···+L2,pxj
p
...
Lp,1xj
1+Lp,2xj
2+···+Lp,1xj
p

=/angbracketleftxi
1,L1,1xj
1+L1,2xj
2+···+L1,pxj
p/angbracketright
+/angbracketleftxi
2,L2,1xj
1+L2,2xj
2+···+L2,pxj
p/angbracketright
+···+/angbracketleftxi
p,Lp,1xj
1+Lp,2xj
2+···+Lp,pxj
p/angbracketright(122)
=/angbracketleftxi,Lxj/angbracketrightp. (123)
Note that from equations (122 and 123), Lxj∈X. Now based on the positive semi-deﬁniteness of L, we
can decompose L=V/latticetopΛV, whereVis an orthogonal matrix and Λis the diagonal matrix containing the
43Published in Transactions on Machine Learning Research (03/2024)
non-negative eigenvalues.
xi/latticetopLxj=/angbracketleftxi,Lxj/angbracketrightp
=/angbracketleftxi,V/latticetopΛVxj/angbracketrightp
=xi/latticetopV/latticetopΛVxj. (124)
ConsiderA=√
ΛV,
xi/latticetopLxj=xi/latticetopV/latticetopΛVxj
=xi/latticetopA/latticetopAxj
=/angbracketleftAxi,Axj/angbracketrightp. (125)
Note thatxi/latticetopLxj=/angbracketleftAxi,Axj/angbracketrightp=/angbracketleftφ(xi),φ(xj)/angbracketrightp(say) which is a characterization of kernels. For a ﬁnite
m∈N, letG∈Rm×mbe the Gram (kernel) matrix induced by using /angbracketleftAx,Aw/angbracketrightas a kernel. Let β∈Rm,
then we have:
β/primeGβ=m/summationdisplay
i,j=1βiGi,jβj
=m/summationdisplay
i,j=1βiβj/angbracketleftφ(xi),φ(xj)/angbracketrightp
=/angbracketleftBiggm/summationdisplay
i=1βiφ(xi),m/summationdisplay
j=1βjφ(xj)/angbracketrightBigg
p
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1βiφ(xi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
p.
This illustrates that xi/latticetopLxjis positive semi-deﬁnite and deﬁnes a scalar-valued kernel. Now, using the
properties of a scalar-valued kernel, γx/latticetopLx/prime, withγ >0is a valid positive semi-deﬁnite kernel. Thus the
kernel given by
k1(x,x/prime;G) =γx/latticetopLx/prime,∀x,x/prime∈X,γ > 0,
is a valid positive semi-deﬁnite scalar kernel on X×X. Therefore, by the construction of OVK used in
(Kadri et al., 2016),
(K(x,x/prime)y)(t) =k1(x,x/prime;G)/integraldisplay1
0k2(s,t)y(s)ds, (126)
deﬁnes an operator-valued kernel on X×X.
44Published in Transactions on Machine Learning Research (03/2024)
Now, as exponential of a scalar-valued kernel provides us another valid scalar-valued kernel (Shawe-Taylor
et al., 2004), we consider the following normalized version:
exp(γx/latticetopLx/prime)/radicalBig
exp(γx/latticetopLx) exp(γx/prime/latticetopLx/prime)= exp/parenleftBig
γx/latticetopLx/prime−γ
2x/latticetopLx−γ
2x/prime/latticetopLx/prime/parenrightBig
= exp/bracketleftBig
−γ
2/parenleftBig
x/latticetopLx+x/prime/latticetopLx/prime−2x/latticetopLx/prime/parenrightBig/bracketrightBig
= exp/bracketleftBig
−γ
2/parenleftbig
/bardblAx/bardbl2
p+/bardblAx/prime/bardbl2
p−2/angbracketleftAx,Ax/prime/angbracketrightp/parenrightbig/bracketrightBig
= exp/bracketleftBig
−γ
2/parenleftbig
/bardblAx−Ax/prime/bardbl2
p/parenrightbig/bracketrightBig
= exp/bracketleftBig
−γ
2(/angbracketleftA(x−x/prime),A(x−x/prime)/angbracketrightp)/bracketrightBig
= exp/bracketleftBig
−γ
2(/angbracketleftA(x−x/prime),A(x−x/prime)/angbracketrightp)/bracketrightBig
= exp/bracketleftBig
−γ
2(x−x/prime)/latticetopL(x−x/prime)/bracketrightBig
. (127)
As exponential of a scalar-valued kernel and normalization preserves the validity of a scalar-valued kernel,
we claim that the kernel in (127) deﬁnes a valid scalar-valued kernel. This illustrates that e−γ(x−x/prime)/latticetopL(x−x/prime)
is a valid positive semi-deﬁnite scalar-valued kernel on X×Xforγ > 0and an OVK with k1(x,x/prime;G) =
e−γ(x−x/prime)/latticetopL(x−x/prime),∀x,x/prime∈X,γ > 0in (126) provides a valid graph-induced OVK.
A.4 Proof of Representer theorem
We provide a proof for the Representer theorem (Theorem A.2) stated in Section A.1. In the proof we use the
Gateaux derivative in an associated function-valued reproducing kernel Hilbert space for an operator-valued
kernel. We recall the theorem statement ﬁrst and then provide a proof.
Theorem A.4 (Representer theorem ).LetKGbe an operator-valued kernel and HKGbe its corre-
sponding function-valued reproducing kernel Hilbert space. The solution /tildewideFλ∈ HKGof the regularized
optimization problem:
/tildewideFλ= arg min
F∈HKGn/summationdisplay
i=1/bardbly(i)−F(x(i))/bardbl2
Y+λ/bardblF/bardbl2
HKG, (128)
whereλ>0,F∈HKG, has the following form
/tildewideFλ(.) =n/summationdisplay
i=1KG(x(i),.)ui,whereui∈Y. (129)
Proof.We use the Gateaux derivative to obtain the condition for stationary point of the functional Jλ(F),
given by
Jλ(F) =n/summationdisplay
i=1/bardbly(i)−F(x(i))/bardbl2
Y+λ/bardblF/bardblHK,∀F∈HKG.
In order to ﬁnd the critical points in HKG, we use Gateaux derivative DGofJλwith respect to Fin the
directionH, which is deﬁned by
DGJλ(F,H) = lim
τ→0Jλ(F+τH)−Jλ(F)
τ.
Let/tildewideFbe the operator in HKGsuch that
/tildewideF= arg min
F∈HKGJλ(F) =⇒DGJλ(F,H) = 0,∀H∈HKG.
45Published in Transactions on Machine Learning Research (03/2024)
Jλcan be written as
Jλ(F) =n/summationdisplay
i=1Gi(F) +λL(F),
and asDGJλ(F,H) =/angbracketleftDGJλ(F),H/angbracketrightHKG,∀F,H∈HKG, we obtain the following.
noitemsep L(F) =/bardblF/bardbl2
HKG=/angbracketleftF,F/angbracketrightHKG. Therefore we have
lim
τ→0/angbracketleftF+τH,F +τH/angbracketrightHKG−/angbracketleftF,F/angbracketrightHKG
τ= 2/angbracketleftF,H/angbracketrightHKG
=⇒DGL(F) = 2F.
noiitemsep Gi(F) =/bardbly(i)−F(x(i))/bardbl2
Y. Then we have
lim
τ→0/bardbly(i)−F(x(i))−τH(x(i))/bardbl2
Y−/bardbly(i)−F(x(i))/bardbl2
Y
τ=−2/angbracketlefty(i)−F(x(i)),H(x(i))/angbracketrightY(130)
=−2/angbracketleftKG(x(i),.)(y(i)−F(x(i))),H/angbracketrightHKG
(131)
=−2/angbracketleftKG(x(i),.)ui,H/angbracketrightHKG,(132)
=⇒DGGi(F) =−2KG(x(i),.)ui.
We obtain equation (131) from equation (130) using the reproducibility property. In (131), we use ui=
y(i)−F(x(i))to get (132). Using 1, 2, and DGJλ1,λ2(/tildewideF) = 0, we obtain,/tildewideF(.) =1
λ/summationtextn
i=1KG(x(i),.)ui. The
constant1
λcan be absorbed in functions ui’s, such that/tildewideF(.) =/summationtextn
i=1KG(x(i),.)ui.
We provide a proof for the extended represeneter theorem (Theorem 5.1) based on the arguments used in
the earlier proof. We recall the theorem and then provide a proof.
Theorem A.5 (Extended representer theorem ).LetKbe an operator-valued kernel as deﬁned in (26)
andHK=H1
K×H2
K×···×Hs
Kbe its corresponding function-valued reproducing kernel Hilbert space based
on kernelsk1,k1
2,...,ks
2. The solution ˜Fλ∈HKof the regularized optimization problem.
/tildewideFλ= arg min
F=[F1,F2,...,Fs]/latticetop∈HKs/summationdisplay
l=1/parenleftBiggn/summationdisplay
i=1/bardbly(i)
l−Fl(x(i))/bardbl2
Y+λ/bardblFl/bardbl2
Hl
K/parenrightBigg
,
whereλ>0,F= [F1,F2,...,Fs]/latticetop∈HK=H1
K×H2
K×···×Hs
K, has the following form
/tildewideFλ(.) =
/tildewideF1
λ(.)
...
/tildewideFs
λ(.)
=
/summationtextn
i=1K1(x(i),.)u1
i
.../summationtextn
i=1Ks(x(i),.)us
i
,whereu1
i,u2
i,...,us
i∈Y. (133)
Proof.We use a similar argument as in case of the representer theorem proof. The Gateaux derivative is
used to obtain the condition for stationary points which minimize Jλ(F)written as a sum of Jl
λ/parenleftbig
Fl/parenrightbig
, for
l∈[s], given by
Jλ(F) =s/summationdisplay
l=1Jl
λ/parenleftbig
Fl/parenrightbig
=s/summationdisplay
l=1/parenleftBiggn/summationdisplay
i=1/bardbly(i)
l−Fl(x(i))/bardbl2
Y+λ/bardblFk/bardblHl
K/parenrightBigg
,∀F∈HK.
46Published in Transactions on Machine Learning Research (03/2024)
In order to ﬁnd the critical points in HK=H1
K×H2
K×···×Hs
K, we use Gateaux derivative DGofJl
λwith
respect toFlin the direction H∈Hl
K, which is deﬁned by
DGJl
λ(Fl,H) = lim
τ→0Jl
λ(Fl+τH)−Jl
λ(Fl)
τ.
Let/tildewideFlbe the operator in Hl
Ksuch that
/tildewideFl= arg min
Fl∈Hl
KJl
λ(Fl) =⇒DGJl
λ(Fl,H) = 0,∀H∈Hl
K,l∈[s].
Now,Jl
λcan be written as
Jl
λ(Fl) =n/summationdisplay
i=1Gl
i(Fl) +λLl(Fl),
and asDGJl
λ(Fl,H) =/angbracketleftDGJl
λ(Fl),H/angbracketrightHl
K,∀F,H∈Hl
K,l∈[s], we obtain the following.
noitemsep Ll(Fl) =/bardblFl/bardbl2
Hl
K=/angbracketleftFl,Fl/angbracketrightHl
K. Therefore we have
lim
τ→0/angbracketleftFl+τH,Fl+τH/angbracketrightHl
K−/angbracketleftFl,Fl/angbracketrightHl
K
τ= 2/angbracketleftFl,H/angbracketrightHl
K
=⇒DGLl(Fl) = 2Fl.
noiitemsep Gl
i(Fl) =/bardbly(i)
l−Fl(x(i))/bardbl2
Y. Then we have
lim
τ→0/bardbly(i)
l−F(x(i))−τH(x(i))/bardbl2
Y−/bardbly(i)
l−F(x(i))/bardbl2
Y
τ=−2/angbracketlefty(i)
l−F(x(i)),H(x(i))/angbracketrightY(134)
=−2/angbracketleftKl(x(i),.)(y(i)
l−F(x(i))),H/angbracketrightHl
K
(135)
=−2/angbracketleftKl(x(i),.)ul
i,H/angbracketrightHl
K, (136)
=⇒DGGl
i(Fl) =−2Kl(x(i),.)ul
i.
We obtain equation (135) from equation (134) using the reproducibility property. In (135), we use ul
i=
y(i)
l−Fl(x(i))to get (136). Using 1, 2, and DGJl
λ(/tildewideFl) = 0, we obtain,/tildewideFl(.) =1
λ/summationtextn
i=1Kl(x(i),.)ul
i. The
constant1
λcan be absorbed in functions ul
i’s, such that /tildewideFl(.) =/summationtextn
i=1Kl(x(i),.)ul
i,l∈[s]. Therefore, we
obtain the following:
/tildewideFλ(.) =
/tildewideF1
λ(.)
...
/tildewideFs
λ(.)
=
/summationtextn
i=1K1(x(i),.)u1
i
.../summationtextn
i=1Ks(x(i),.)us
i
,whereu1
i,u2
i,...,us
i∈Y.
A.5 Properties of A,B,C,H,c andMmatrices
This section deals with the construction and properties of matrices A,B,C,H,c andMwhich have been
used in the framework.
Determining A:
Ais a matrix which is constructed to represent the constraint L1= 0asAvech(L) = 0. For a given vech (L),
47Published in Transactions on Machine Learning Research (03/2024)
we can obtain the condition L1= 0withAvech(L) = 0based on the following construction:
A=
ep0p−10p−2... 0201
e2
pep−10p−2... 0201
e3
pe2
p−1ep−2... 0201
..................
ep
pep−1
p−1ep−2
p−2... e2
2e1
,
where 0k= [0,0,..., 0]∈R1×kdenotes a row vector containing kzeros,ek= [1,1,..., 1]∈R1×kdenotes a
row vector containing kones,ei
k= [0,..., 1,..., 0]∈R1×kdenotes a row vector with 1 in the i-th position
and zeros elsewhere, resulting in A∈Rp×p(p+1)
2.
Determining B:
Bis a matrix which is constructed to represent the constraint Li,j≤0,i/negationslash=jasBvech(L)≤0. For a given
vech(L), we reformulate the condition Li,j≤0,∀i/negationslash=jasBvech(L)≤0. This can be achieved with the
following construction:
B=
e2
p0p−10p−2... 030201
e3
p0p−10p−2... 030201
.....................
ep
p0p−10p−2... 030201
0pe2
p−10p−2... 030201
.....................
0pep−1
p−10p−2... 030201
.....................
0p0p−10p−2... e2
30201
0p0p−10p−2... e3
30201
0p0p−10p−2... 03e2
201
.
Note thatB∈Rp(p−1)
2×p(p+1)
2.
Determining C:
In section 5.1.3, Cis a matrix which is used to deal with the constraint Dii≥0,∀i∈[p]. We construct a
matrixC∈Rp×p(p+1)
2which consists of 0’s and 1’s satisfying Cvech(D) =Diag (D). For a given vech (L),
we formulate the matrix C∈Rp×p(p+1)
2using 0’s and 1’s as follows:
C=
←C1:→
←C2:→
←C3:→
.........
←Cp:→
,
where the row Ci:∈R1×p(p+1)
2, andCi:contains a 1 in the/parenleftBig
i(i+1)
2/parenrightBig
-th position and zeros elsewhere.
Determining H:
In section 5.1.4, we require a matrix H∈Rp(p+1)
2×p(p+1)
2comprising 0’s and 1’s such that Hvech(L)produces
a vector having the same structure as vech (L)with 0’s corresponding to diagonal entries of Land the same
oﬀ-diagonal entries as in L. For a given vech (L), we formulate the matrix H∈Rp(p+1)
2×p(p+1)
2using 0’s and
48Published in Transactions on Machine Learning Research (03/2024)
1’s as follows:
H=
←H1:→
←H2:→
←H3:→
.........
←Hp(p+1)
2:→
,
where the rows Hi:∈R1×p(p+1)
2, andHi:contains a 0 in the/parenleftBig
i(i+1)
2/parenrightBig
-th position and 1’s elsewhere.
Determining c:
In section 5.1.4, we consider a vector c∈Rp(p+1)
2consisting of 0’s and 1’s, such that c/latticetopvech(L) =trace (L).
The vector c∈Rp(p+1)
2is deﬁned for obtaining the diagonal entries of Dfrom vech (D)using 0’s and 1’s such
that
c/latticetopvech(D) =d1,1+d2,2+···+dp,p,
wherecis given as follows:
c= [c1,c2,...,cp−1,cp]/latticetop,
wherec∈Rp(p+1)
2such thatci∈R1×i(i+1)
2has all 0’s with 1 in/parenleftBig
i(i+1)
2/parenrightBig
-th element.
DeterminingM:
Mis a matrix which is constructed to transform vech (L)to vec (L)usingMvech(L) =vec(L). Let the
Laplacian of the graph of pnodes be denoted by Lwhich is symmetric,
L=
l1,1l2,1l3,1... lp,1
l2,1l2,2l3,2... lp,2
l3,1l3,2l3,3... lp,3
...............
lp,1lp,2lp,3... lp,p
.
Now, vec (L) = [l1,1,...,lp,1,l2,1,...,lp,2,...,lp,1,...,lp,p]/latticetopis obtained by stacking the columns and
vech(L) = [l1,1,...,lp,1,l2,2,...,lp,2,l3,3,...,lp,3,...,lp,p]/latticetop,
which is obtained by eliminating the super-diagonal elements and then stacking them up. As illustrated,
vec(L)∈Rp2and vech (L)∈Rp(p+1)/2. We can ﬁnd a matrix Mwhich satisﬁes
vec(L) =Mvech(L).
Hence observe that M∈Rp2×p(p+1)
2. Therefore,
M/latticetop=/summationdisplay
i≥jvi,j(vec(Ti,j))/latticetop,
wherevi,jis a vector of order1
2p(p+ 1)having the value 1 in the (j−1)n+i−1
2j(j−1)-th position and 0
elsewhere and Ti,jis ap×pmatrix with 1 in positions (i,j)and(j,i), and 0 elsewhere.
The following relations are used in order to write the expression of objective function in terms of vec (L).
(x(i)−x(j))/latticetop(L+D)(x(i)−x(j)) =vec((x(i)−x(j))(x(i)−x(j))/latticetop)/latticetopvec(L+D),
=vec((x(i)−x(j))(x(i)−x(j))/latticetop)/latticetop(vec(L) +vec(D)),
x(i)/latticetopLx(i)=vec(x(i)x(i)/latticetop)/latticetopvec(L),
/bardblD/bardblF=vec(D)/latticetopvec(D).
49Published in Transactions on Machine Learning Research (03/2024)
A.6 Derivation of Gradients
Inthis section, wecoverthederivationof ∇vech(L)Jand∇vech(D)Jwhich are requiredinAlgorithm 2. Weuse
(103) and (108) in the following expression to ﬁnd the gradients. Recall ¯k2(u)(t) =/integraltext1
0e−γop|s−t|u(s)ds,γop>
0,s,t∈Rin the following derivations.
J(u,L,D ) =n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)−n/summationdisplay
j=1K(x(i),x(j))uj/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y+λn/summationdisplay
i,j=1/angbracketleftK(x(i),x(j))ui,uj/angbracketrightY
+ρLn/summationdisplay
i=1x(i)/latticetopLx(i)+ρD/bardblD/bardbl2
F
=n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)−n/summationdisplay
j=1k1(x(i),x(j);G)¯k2(uj)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y+λn/summationdisplay
i=1,j=1/angbracketleftk1(x(i),x(j);G)¯k2(ui),uj/angbracketrightY
+ρLn/summationdisplay
i=1x(i)/latticetopLx(i)+ρD/bardblD/bardbl2
F
=n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)−n/summationdisplay
j=1e−γ(x(i)−x(j))/latticetop(L+D)(x(i)−x(j))¯k2(uj)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y
+λn/summationdisplay
i,j=1/angbracketlefte−γ(x(i)−x(j))/latticetop(L+D)(x(i)−x(j))¯k2(ui),uj/angbracketrightY+ρLn/summationdisplay
i=1x(i)/latticetopLx(i)+ρD/bardblD/bardbl2
F.
We consider the following variables for simplifying the computations:
Rij=vec((x(i)−x(j))(x(i)−x(j))/latticetop)/latticetopM, (137)
¯Ri=vec(x(i)x(i)/latticetop)/latticetopM.
Therefore,
J=n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)−n/summationdisplay
j=1e−γRij(vech(L)+vech(D))¯k2(uj)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y
+λn/summationdisplay
i,j=1/angbracketlefte−γRij(vech(L)+vech(D))¯k2(ui),uj/angbracketrightY+ρLn/summationdisplay
i=1¯Rivech(L) +ρDvech(D)/latticetopM/latticetopMvech(D)
=n/summationdisplay
i=1/angbracketleftBigg
y(i)−n/summationdisplay
j=1e−γRij(vech(L)+vech(D))¯k2(uj),y(i)−n/summationdisplay
j=1e−γRij(vech(L)+vech(D))¯k2(uj)/angbracketrightBigg
Y
+λn/summationdisplay
i,j=1e−γRij(vech(L)+vech(D))/angbracketleft¯k2(ui),uj/angbracketrightY+ρLn/summationdisplay
i=1¯Rivech(L) +ρDvech(D)/latticetopM/latticetopMvech(D)
=n/summationdisplay
i=1
/angbracketleftBig
y(i),y(i)/angbracketrightBig
Y−2n/summationdisplay
j=1e−γRij(vech(L)+vech(D))/angbracketleftBig
y(i),¯k2(uj)/angbracketrightBig
Y
+n/summationdisplay
j,k=1e−γ(Rij+Rik)(vech(L)+vech(D))/angbracketleftbig¯k2(uj),¯k2(uk)/angbracketrightbig
Y
+λn/summationdisplay
i,j=1e−γRij(vech(L)+vech(D))/angbracketleft¯k2(ui),uj/angbracketrightY
+ρLn/summationdisplay
i=1¯Rivech(L) +ρDvech(D)/latticetopM/latticetopMvech(D).
50Published in Transactions on Machine Learning Research (03/2024)
The expression above for Jcan be used to determine the gradient with respect to vech (L)and vech (D). The
gradient of Jwith respect to vech (L)for a ﬁxed uandDis given as follows:
∇vech(L)J= 2γn/summationdisplay
i,j=1R/latticetop
ije−γRij(vech(L)+vech(D))/angbracketleftBig
y(i),¯k2(uj)/angbracketrightBig
Y
−γn/summationdisplay
i,j,k=1(Rij+Rik)/latticetope−γ(Rij+Rik)(vech(L)+vech(D))/angbracketleftbig¯k2(uj),¯k2(uk)/angbracketrightbig
Y
−λγn/summationdisplay
i,j=1R/latticetop
ije−γRij(vech(L)+vech(D))/angbracketleft¯k2(ui),uj/angbracketrightY+ρLn/summationdisplay
i=1¯R/latticetop
i.
Similarly, for a ﬁxed uand vech (L)the gradient of Jwith respect to vech (D)can be written as
∇vech(D)J= 2γn/summationdisplay
i,j=1R/latticetop
ije−γRij(vech(L)+vech(D))/angbracketleftBig
y(i),¯k2(uj)/angbracketrightBig
Y
−γn/summationdisplay
i,j,k=1(Rij+Rik)/latticetope−γ(Rij+Rik)(vech(L)+vech(D))/angbracketleftbig¯k2(uj),¯k2(uk)/angbracketrightbig
Y
−λγn/summationdisplay
i,j=1R/latticetop
ije−γRij(vech(L)+vech(D))/angbracketleft¯k2(ui),uj/angbracketrightY+ 2ρDM/latticetopMvech(D).
For the experiments on NBA data, we use the approach in Section 5.1.5 with r= 2ands= 2. The inputs
x(i)= (x(i)
1,x(i)
2)∈X2and outputs y(i)= (y(i)
1,y(i)
2)∈Y2with (25) and (26), for i∈[n]. We consider the
following variables for simplifying the computations:
R1
ij=vec((x(i)
1−x(j)
1)(x(i)
1−x(j)
1)/latticetop)/latticetopM,
R2
ij=vec((x(i)
2−x(j)
2)(x(i)
2−x(j)
2)/latticetop)/latticetopM,
¯R1
i=vec(x(i)
1x(i)
1/latticetop)/latticetopM,
¯R2
i=vec(x(i)
2x(i)
2/latticetop)/latticetopM.
51Published in Transactions on Machine Learning Research (03/2024)
J(u1,u2,L,D ) =n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)
1−n/summationdisplay
j=1K(x(i),x(j))u1
j/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y+n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)
2−n/summationdisplay
j=1K(x(i),x(j))u2
j/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y
+λn/summationdisplay
i,j=1/angbracketleftK(x(i),x(j))u1
i,u1
j/angbracketrightY+λn/summationdisplay
i,j=1/angbracketleftK(x(i),x(j))u2
i,u2
j/angbracketrightY
+ρ1n/summationdisplay
i=1x(i)
1/latticetopLx(i)
1+ρ2n/summationdisplay
i=1x(i)
2/latticetopLx(i)
2+ρD/bardblD/bardbl2
F
=n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)
1−n/summationdisplay
j=1k1(x(i),x(j);G)¯k1
2(u1
j)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y+n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)
2−n/summationdisplay
j=1k1(x(i),x(j);G)¯k2
2(u2
j)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y
+λn/summationdisplay
i=1,j=1/angbracketleftk1(x(i),x(j);G)¯k1
2(u1
i),u1
j/angbracketrightY+λn/summationdisplay
i=1,j=1/angbracketleftk1(x(i),x(j);G)¯k2
2(u2
i),u2
j/angbracketrightY
+ρ1n/summationdisplay
i=1x(i)
1/latticetopLx(i)
1+ρ2n/summationdisplay
i=1x(i)
2/latticetopLx(i)
2+ρD/bardblD/bardbl2
F
=n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)
1−n/summationdisplay
j=1e−γ1/parenleftbig
x(i)
1−x(j)
1/parenrightbig/latticetop(L+D)/parenleftbig
x(i)
1−x(j)
1/parenrightbig
−γ2/parenleftbig
x(i)
2−x(j)
2/parenrightbig/latticetop(L+D)/parenleftbig
x(i)
2−x(j)
2/parenrightbig
¯k1
2(u1
j)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y
+n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubley(i)
2−n/summationdisplay
j=1e−γ1/parenleftbig
x(i)
1−x(j)
1/parenrightbig/latticetop(L+D)/parenleftbig
x(i)
1−x(j)
2/parenrightbig
−γ2/parenleftbig
x(i)
2−x(j)
2/parenrightbig/latticetop(L+D)/parenleftbig
x(i)
2−x(j)
2/parenrightbig
¯k2
2(u2
j)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y
+λn/summationdisplay
i,j=1/angbracketlefte−γ1/parenleftbig
x(i)
1−x(j)
1/parenrightbig/latticetop(L+D)/parenleftbig
x(i)
1−x(j)
1/parenrightbig
−γ2/parenleftbig
x(i)
2−x(j)
2/parenrightbig/latticetop(L+D)/parenleftbig
x(i)
2−x(j)
2/parenrightbig
¯k1
2(u1
j),u1
j/angbracketrightY
+λn/summationdisplay
i,j=1/angbracketlefte−γ1/parenleftbig
x(i)
1−x(j)
1/parenrightbig/latticetop(L+D)/parenleftbig
x(i)
1−x(j)
1/parenrightbig
−γ2/parenleftbig
x(i)
2−x(j)
2/parenrightbig/latticetop(L+D)/parenleftbig
x(i)
2−x(j)
2/parenrightbig
¯k2
2(u2
j),u2
j/angbracketrightY
+ρ1n/summationdisplay
i=1x(i)
1/latticetopLx(i)
1+ρ2n/summationdisplay
i=1x(i)
2/latticetopLx(i)
2+ρD/bardblD/bardbl2
F.
The gradient of Jwith respect to vech (L)for ﬁxed u1,u2andDis given by
∇vech(L)J= 2n/summationdisplay
i,j=1(γ1R1
ij+γ2R2
ij)/latticetope−(γ1R1
ij+γ2R2
ij)(vech(L)+vech(D))/parenleftbigg/angbracketleftBig
y(i)
1,¯k1
2(u1
j)/angbracketrightBig
Y+/angbracketleftBig
y(i)
2,¯k2
2(u2
j)/angbracketrightBig
Y/parenrightbigg
−n/summationdisplay
i,j,k=1(γ1R1
ij+γ2R2
ij+γ1R1
ik+γ2R2
ik)/latticetope−(γ1R1
ij+γ2R2
ij+γ1R1
ik+γ2R2
ik)(vech(L)+vech(D))/angbracketleftbig¯k1
2(u1
j),¯k1
2(u1
k)/angbracketrightbig
Y
−n/summationdisplay
i,j,k=1(γ1R1
ij+γ2R2
ij+γ1R1
ik+γ2R2
ik)/latticetope−(γ1R1
ij+γ2R2
ij+γ1R1
ik+γ2R2
ik)(vech(L)+vech(D))/angbracketleftbig¯k2
2(u2
j),¯k2
2(u2
k)/angbracketrightbig
Y
−λn/summationdisplay
i,j=1(γ1R1
ij+γ2R2
ij)/latticetope−(γ1R1
ij+γ2R2
ij)(vech(L)+vech(D))/angbracketleft¯k1
2(u1
i),u1
j/angbracketrightY
−λn/summationdisplay
i,j=1(γ1R1
ij+γ2R2
ij)/latticetope−(γ1R1
ij+γ2R2
ij)(vech(L)+vech(D))/angbracketleft¯k2
2(u2
i),u2
j/angbracketrightY+ρ1n/summationdisplay
i=1¯R1/latticetop
i+ρ2n/summationdisplay
i=1¯R2/latticetop
i.
52Published in Transactions on Machine Learning Research (03/2024)
Similarly, for ﬁxed u1,u2and vech (L)the gradient of Jwith respect to vech (D)can be written as
∇vech(D)J= 2n/summationdisplay
i,j=1(γ1R1
ij+γ2R2
ij)/latticetope−(γ1R1
ij+γ2R2
ij)(vech(L)+vech(D))/parenleftbigg/angbracketleftBig
y(i)
1,¯k1
2(u1
j)/angbracketrightBig
Y+/angbracketleftBig
y(i)
2,¯k2
2(u2
j)/angbracketrightBig
Y/parenrightbigg
−n/summationdisplay
i,j,k=1(γ1R1
ij+γ2R2
ij+γ1R1
ik+γ2R2
ik)/latticetope−(γ1R1
ij+γ2R2
ij+γ1R1
ik+γ2R2
ik)(vech(L)+vech(D))/angbracketleftbig¯k1
2(u1
j),¯k1
2(u1
k)/angbracketrightbig
Y
−n/summationdisplay
i,j,k=1(γ1R1
ij+γ2R2
ij+γ1R1
ik+γ2R2
ik)/latticetope−(γ1R1
ij+γ2R2
ij+γ1R1
ik+γ2R2
ik)(vech(L)+vech(D))/angbracketleftbig¯k2
2(u2
j),¯k2
2(u2
k)/angbracketrightbig
Y
−λn/summationdisplay
i,j=1(γ1R1
ij+γ2R2
ij)/latticetope−(γ1R1
ij+γ2R2
ij)(vech(L)+vech(D))/angbracketleft¯k1
2(u1
i),u1
j/angbracketrightY
−λn/summationdisplay
i,j=1(γ1R1
ij+γ2R2
ij)/latticetope−(γ1R1
ij+γ2R2
ij)(vech(L)+vech(D))/angbracketleft¯k2
2(u2
i),u2
j/angbracketrightY+ 2ρDM/latticetopMvech(D).
A.7 OpMINRES Algorithm
A.7.1 OpLanczos Step
OpLanczos in OpMINRES is used to trigiagonalize the operator matrix P. The vectors obtained from
OpLanczos form an orthonormal set. Using the OpLanczosStep Algorithm 5, we can obtain,
PQk=QkTk,whereTk=
α1β2 0
β2α2β3
β3α3...
......βk−2
βk−1αk−1βk
0 βkαk
,
andQk= [q1,q2,...,qk], whereqi’s are obtained using OpLanczosStep Algorithm. The columns of Qk
belonging toYnare orthonormal and the following equation is satisﬁed:
PQk=Qk+1Tk,whereTk=
α1β2 0
β2α2β3
β3α3...
......βk−2
βk−1αk−1βk
βkαk
0 βk+1
.
We intend to solve Au=yby obtaining a solution in the Krylov space Kk(P,y) =
span{y,Py,P2y,...,Pk−1y}. For each iteration k, we obtain the following equations using the trans-
formation θ=Qkϑ, where θ∈Yn,ϑ∈Rk.
min
θ∈Kk(P,y)/bardbly−Pϑ/bardblYn= min
ϑ∈Rk/bardbly−PQkϑ/bardblYn= min
ϑ∈Rk/bardbly−Qk+/one.taboldstyleTkϑ/bardblYn
= min
ϑ∈Rk/bardblQk+1(β1e1−Tkϑ)/bardblYn, (138)
(whereβ1=/bardbly/bardblYn,e1= [1 0...0]/latticetopandq1=y)
= min
ϑ∈Rk/bardblβ1e1−Tkϑ/bardbl2. (139)
53Published in Transactions on Machine Learning Research (03/2024)
Algorithm 4 OpMINRES (P,b,maxiter )
Input:P,b,maxiter
Output:ϑ,φ,ψ,χ
β1=/bardblb/bardblYn
q0= 0
q1=1
β1b
φ0=τ0=β1
χ0= 0
δ(1)
1= 0
c0=−1
s0= 0
d0=d−1=ϑ0= 0
k= 1
whilestopping criteria not satisﬁed do
OpLanczosStep (P,qk,qk−1,βk)→αk,βk+1,qk+1
//last left orthogonalization on middle two entries in last column of Tk+1,k
δ(2)
k=ck−1δ(1)
k+sk−1αk
γ(1)
k=sk−1δ(1)
k−ck−1αk
//last left orthogonalization to produce ﬁrst two entries of Tk+2,k+1ek+1
/epsilon1(1)
k+1=sk−1βk+1
δ(1)
k+1=−ck−1βk+1
//current left orthogonalization to zero out βk+1
SymOrtho (γ(1)
k,βk+1)→ck,sk,γ(2)
k
//right-hand side, residual norms
τk=ckφk−1
φk=skφk−1
ψk−1=φk−1/radicalBig
(γ(1)
k)2+ (δ(1)
k+1)2
//update solution
dk=1
γ(2)
k/parenleftBig
vk−δ(2)
kdk−1−/epsilon1(1)
kdk−2/parenrightBig
ϑk=ϑk−1+τkdk
χk=/bardblϑk/bardbl
k←k+ 1
end while
ϑ=ϑk,φ=φk,ψ=φk/radicalBig
(γ(1)
k+1)2+ (δ(1)
k+2)2,χ=χk
The change in norms /bardbl./bardblYnin (138) to/bardbl./bardbl2is obtained based on the following arguments. Let z=
[z1,z2,...,zk+1]/latticetop∈Rk+1andQk+1= [q1,q2,...,qk+1], whereqi∈Yn, fori= 1,2,...,k + 1, then we
have
/bardblQk+1zk+1/bardblYn=/bardblz1q1+z2q2+···+zk+1qk+1/bardblYn
=/radicalBigg
z2
1/integraldisplay
Ωyq2
1(t)dt+z2
2/integraldisplay
Ωyq2
2(t)dt+···+z2
k+1/integraldisplay
Ωyq2
k+1(t)dt (140)
=/radicalBig
z2
1+z2
2+···+z2
k+1(141)
=/bardblz/bardbl2.
Equation(140)reducesto(141)asthe qi’sareorthonormalin Yn. Solvingfor ϑk= arg min ϑ∈Rk/bardblβ1e1−¯Tkϑ/bardbl2
can be done using QR decomposition (Choi, 2006) which has been discussed in the next section. Now, the
54Published in Transactions on Machine Learning Research (03/2024)
Algorithm 5 OpLanczosStep (P,qk,qk−1,βk)
Input:A,qk,qk−1,βk
Output:αk,βk+1,qk+1
¯qk+1=Aqk−βkqk−1
αk=/angbracketleft¯qk+1,qk/angbracketrightYn
¯qk+1←¯qk+1−αkqk
βk+1=/bardbl¯qk+1/bardblYn
qk+1=1
βk+1¯qk+1
Algorithm 6 SymOrtho (a,b)
Input:a,b
Output:c,s,r
ifb== 0then
s= 0
r=|a|
ifa== 0then
c= 1
else
c= sign(a)
end
else ifa== 0then
c= 0
s= sign(b)
r=|b|
else if|b|>|a|then
τ=a/b
s= sign(b)/√
1 +τ2
c=sτ
r=b/s
else if|a|>|b|then
τ=b/a
c= sign(a)/√
1 +τ2
s=cτ
r=a/c
end
transformation from Rkback toYnto obtain ukis achieved using by the following:
uk=Qkϑk=Qk/parenleftbigg
arg min
ϑ∈Rk/bardblβ1e1−Tkϑ/bardbl2/parenrightbigg
.
55Published in Transactions on Machine Learning Research (03/2024)
A.7.2 QR Decomposition
InordertoapplyQRdecompositiononsymmetric Tk, weuseGivensrotation Sktoobtainaupper-triangular
system.
SkTk=/bracketleftbiggRk
0/bracketrightbigg
=
γ(1)
1δ(1)
2/epsilon1(1)
3 0
γ(2)
2δ(2)
3/epsilon1(1)
4
.........
γ(2)
k−2δ(2)
k−1/epsilon1(1)
k
γ(2)
k−1δ(2)
k
γ(2)
k
0 0
, S k(β1e1) =/bracketleftbiggtk
φk/bracketrightbigg
,
whereSk=Sk,k+1...S 2,3S1,2andSi,i+1are Givens rotations created to annihilate the βi’s in sub-diagonal
ofTk. TheSi,i+1’s involved in the product to obtain Skare given by,
Si,i+1=
Ii−1
cisi
si−ci
Ik−i
.
The matrices Si,i+1are obtained using the SymOrtho Algorithm 6. The sub-problem can be rewritten with
ϑk= arg min ϑ∈Rk/bardblβ1e1−Tkϑ/bardbl2as
ϑk= arg min
ϑ∈Rk/vextenddouble/vextenddouble/vextenddouble/vextenddouble/bracketleftbiggtk
φk/bracketrightbigg
−/bracketleftbiggRk
0/bracketrightbigg
ϑ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,wheretk= [τ1,τ2,...,τk]/latticetopand
/bracketleftbigg
tk
φk/bracketrightbigg
=β1Sk,k+1...S 2,3
c1
s1
0k−1
=β1Sk,k+1...S 3,4
c1
s1c2
s1s2
0k−2
=β1
c1
s1c2
...
s1...sk−1ck
s1...sk−1sk
.
A shorthand way to represent the action of Sk,k+1can be described as
/bracketleftbiggcksk
sk−ck/bracketrightbigg/bracketleftbigg
γ(1)
kδ(1)
k+10φk−1
βk+1αk+1βk+2 0/bracketrightbigg
=/bracketleftBigg
γ(2)
kδ(2)
k+1/epsilon1(1)
k+2τk
0γ(1)
k+1δ(1)
k+2φk/bracketrightBigg
.
OpMINRES computes ukinKk(P,y)as an approximate solution to the problem Pu=y:
uk=Qkϑk=QkR−1
ktk=Dk/bracketleftbiggtk−1
τk/bracketrightbigg
=/bracketleftbigDk−1dk/bracketrightbig/bracketleftbiggtk−1
τk/bracketrightbigg
=uk−1+τkdk.
The relation satisﬁed by dkis given by,
dk=1
γ(2)
k/parenleftBig
vk−δ(2)
kdk−1−/epsilon1(1)
kdk−2/parenrightBig
.
These details have been incorporated in OpMINRES Algorithm A.7. The OpMINRES Algorithm A.7 is
based on approximating an inﬁnite-dimensional problem in (150) by a ﬁnite-dimensional problem in (139).
As OpMINRES is based on MINRES algorithm (Choi, 2006), the convergence of OpMINRES follows from
the convergence of MINRES. The case of singular systems with OpMINRES needs more investigation. In
our experiments, the value of relative residual norms φk/φ0has been used as the stopping criteria for
OpMINRES.
56Published in Transactions on Machine Learning Research (03/2024)
A.8 Derivation of Linear system of operators
Derivation of (K+λI)u=y: We obtain a suﬃcient condition for stationary points for the optimization
problem in Theorem A.2.
Using the representer theorem, the minimization problem can be equivalently formulated as the following
problem:
˜ uλ= arg min
u∈Ynn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleyi−n/summationdisplay
j=1K(x(i),x(j))uj/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y+λ/angbracketleftbiggn/summationdisplay
i=1K(x(i),.)ui,n/summationdisplay
j=1K(x(j),.)uj/angbracketrightbigg
HKG.(142)
We have the following simpliﬁcation of the term/angbracketleftbigg/summationtextn
i=1K(x(i),.)ui,/summationtextn
j=1K(x(j),.)uj/angbracketrightbigg
HKGin problem
(142). We have
/angbracketleftbiggn/summationdisplay
i=1K(x(i),.)ui,n/summationdisplay
j=1K(x(j),.)uj/angbracketrightbigg
HKG=n/summationdisplay
i=1/angbracketleftbigg
K(x(i),.)ui,n/summationdisplay
j=1K(x(j),.)uj/angbracketrightbigg
HKG(143)
=n/summationdisplay
i=1n/summationdisplay
j=1/angbracketleftbigg
K(x(i),.)ui,K(x(j),.)uj/angbracketrightbigg
HKG(144)
=n/summationdisplay
i=1n/summationdisplay
j=1/angbracketleftbigg
K(x(i),x(j))ui,uj/angbracketrightbigg
Y. (145)
Note that Eq. (143) and Eq. (144) follow from the property of bilinear forms and Eq. (145) follows from
the reproducing property of K. Thus we have the following simpliﬁed formulation:
˜ uλ= arg min
u∈Ynn/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleyi−n/summationdisplay
j=1K(x(i),x(j))uj/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y+λn/summationdisplay
i=1,j=1/angbracketleftK(x(i),x(j))ui,uj/angbracketrightY.
To solve this problem, we ﬁrst construct the objective function Jλ(u)given by
Jλ(u) =n/summationdisplay
i=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleyi−n/summationdisplay
j=1K(x(i),x(j))uj/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
Y+λn/summationdisplay
i=1,j=1/angbracketleftK(x(i),x(j))ui,uj/angbracketrightY,u∈Yn.
LettingJλ(u) =/summationtextn
i=1Gi(u) +λL(u), we can ﬁnd the directional derivative of Jλ(u)with respect to the
direction vasDvJλ(u).
DvGi(u) = lim
τ→0Gi(u+τv)−Gi(u)
τ
=−2/angbracketleftbigg
yi−n/summationdisplay
j=1K(xi,xj)uj,n/summationdisplay
j=1K(x(i),x(j))vj/angbracketrightbigg
.
DvL(u) = lim
τ→0L(u+τv)−L(u)
τ
=λn/summationdisplay
i,j/angbracketleftK(x(i),x(j))ui,vj/angbracketright+λn/summationdisplay
i,j/angbracketleftK(x(i),x(j))vi,uj/angbracketright.
AsKis Hermitian from the deﬁnition of operator-valued kernel, we obtain
/angbracketleftK(x(i),x(j))ui,vj/angbracketright=/angbracketleftui,K(x(i),x(j))vj/angbracketright,∀i,j∈[n]. (146)
57Published in Transactions on Machine Learning Research (03/2024)
Therefore,
DvL(u) =λn/summationdisplay
i,j/angbracketleftK(x(i),x(j))ui,vj/angbracketright+λn/summationdisplay
i,j/angbracketleftK(x(i),x(j))vi,uj/angbracketright
=λn/summationdisplay
i,j/angbracketleftui,K(x(i),x(j))vj/angbracketright+λn/summationdisplay
i,j/angbracketleftK(x(i),x(j))vi,uj/angbracketright (147)
=λn/summationdisplay
i,j/angbracketleftui,K(x(i),x(j))vj/angbracketright+λn/summationdisplay
i,j/angbracketleftuj,K(x(j),x(i))vi/angbracketright (148)
= 2λn/summationdisplay
i,j/angbracketleftui,K(x(i),x(j))vj/angbracketright. (149)
Eq. (147) follows from Eq. (146) and in Eq. (147), we use symmetry of /angbracketleft·,·/angbracketrightto obtain Eq. (149). In order
to minimize Jλ(u), its directional derivative DvJλ(u) = 0,∀v∈Yn.
DvJλ(u) = 0
=⇒n/summationdisplay
i=1DvGi(u) +λDvL(u) = 0
=⇒ − 2n/summationdisplay
i=1/angbracketleftbigg
yi−n/summationdisplay
j=1K(x(i),x(j))uj,n/summationdisplay
j=1K(x(i),x(j))vj/angbracketrightbigg
+ 2λn/summationdisplay
i,j/angbracketleftui,K(x(i),x(j))vj/angbracketright= 0
=⇒n/summationdisplay
i=1/angbracketleftbiggn/summationdisplay
j=1K(x(i),x(j))uj−yi,n/summationdisplay
j=1K(x(i),x(j))vj/angbracketrightbigg
+n/summationdisplay
i,j/angbracketleftλui,K(x(i),x(j))vj/angbracketright= 0
=⇒n/summationdisplay
i=1/angbracketleftbiggn/summationdisplay
j=1K(x(i),x(j))uj−yi,n/summationdisplay
j=1K(x(i),x(j))vj/angbracketrightbigg
+n/summationdisplay
i=1/angbracketleftbigg
λui,n/summationdisplay
j=1K(x(i),x(j))vj/angbracketrightbigg
= 0
=⇒n/summationdisplay
i=1/angbracketleftbiggn/summationdisplay
j=1K(x(i),x(j))uj−yi+λui,n/summationdisplay
j=1K(x(i),x(j))vj/angbracketrightbigg
= 0,∀v∈Yn.
The above condition can be reduced to
(K+λI)u=y, (150)
where Kis a matrix of operators formed by using K.
A.9 Results for Generalization Bounds
We recall the lemma 6.2 and provide the proof next.
Lemma A.6. LetFbe a class of functions from XtoY. Consider a m-tuple of samples from input space
as(x(1),x(2),...,x(m))∈Xm. Then the following hold:
1.E/bracketleftbig
supF∈F/bardbl1
m/summationtextm
i=1F(x(i))−EF/bardblY/bracketrightbig
≤2Rm;Y(F).
2. For every c∈R,Rm;Y(cF) =|c|Rm;Y(F).
3. Forφ:Y→R, ifφis a Lipschitz function with Lipschitz constant L, thenRm;R(φ◦F)≤LRm;Y(F).
Proof.For part 1, we start with denoting S= (x(1),x(2),...,x(m))∈Xmand another independent sample
as¯S= (¯x(1),¯x(2),..., ¯x(m))∈Xm, we have
ES∼µm
X[F(x)] =E¯S∼µm
X/bracketleftBigg
1
mm/summationdisplay
i=1F(¯x(i))/bracketrightBigg
. (151)
58Published in Transactions on Machine Learning Research (03/2024)
We note that the Rademacher random variables (ε1,ε2,...,εm)are uniformly distributed over {+1,−1}and
every possible value they take has an equal probability of 1/2m. Without loss of generality, we can always
permute (ε1,ε2,...,εm)to obtainεP1= 1,...,εPk= 1,εPk+1=−1,...,εPm=−1, where 0≤k≤mand
{P1,...,Pm}is a permutation of [m]. Therefore,
ES∼µm
X/bracketleftBigg
E¯S∼µm
X/bracketleftBigg
sup
F∈F1
m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εi/parenleftBig
F(x(i))−F(¯x(i))/parenrightBig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg/bracketrightBigg
=ES∼µm
X
E¯S∼µm
X
sup
F∈F1
m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublek/summationdisplay
i=1/parenleftBig
F(x(i))−F(¯x(i))/parenrightBig
+m/summationdisplay
i=k+1/parenleftBig
F(x(i))−F(¯x(i))/parenrightBig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y


=ES∼µm
X/bracketleftBigg
E¯S∼µm
X/bracketleftBigg
sup
F∈F1
m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1/parenleftBig
F(x(i))−F(¯x(i))/parenrightBig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg/bracketrightBigg
. (152)
The expressions above hold as x(i)and¯x(i)are independent and symmetric. We obtain the following based
on the arguments made above.
ES∼µm
X/bracketleftBigg
sup
F∈F/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
mm/summationdisplay
i=1F(x(i))−Ex(i)∼µXF(x(i))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg
=ES∼µm
X/bracketleftBigg
sup
F∈F/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
mm/summationdisplay
i=1F(x(i))−E¯S∼µm
X/bracketleftbigg1
mF(¯x(i))/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg
(153)
=ES∼µm
X/bracketleftBigg
sup
F∈F/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleE¯S∼µm
X1
m/parenleftBiggm/summationdisplay
i=1F(x(i))−F(¯x(i))/parenrightBigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg
≤ES∼µm
X/bracketleftBigg
E¯S∼µm
X/bracketleftBigg
sup
F∈F/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
m/parenleftBiggm/summationdisplay
i=1F(x(i))−F(¯x(i))/parenrightBigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg/bracketrightBigg
(154)
=ES∼µm
X/bracketleftBigg
E¯S∼µm
X/bracketleftBigg
E/bracketleftBigg
sup
F∈F1
m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εi/parenleftBig
F(x(i))−F(¯x(i))/parenrightBig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg/bracketrightBigg/bracketrightBigg
(155)
≤ES∼µm
X/bracketleftBigg
E/bracketleftBigg
sup
F∈F1
m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiF(x(i))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg/bracketrightBigg
+E¯S∼µm
X/bracketleftBigg
E/bracketleftBigg
sup
F∈F1
m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiF(¯x(i))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg/bracketrightBigg
(156)
=2Rm;Y(F).
Equation (153) follows from (151). Jensen’s inequality is used to obtain (154) and (155) follows from (152)
by the fact that using Rademacher variables does not change the value of the expression in (223) (see proof of
Theorem 4.1 in (Liao, 2020)). The inequality (156) uses the fact that εiand−εifollow the same Rademacher
distribution and triangle inequality for norm /bardbl·/bardblYwith supremum.
For part 2,
Rm;Y(cF) =E/bracketleftBigg
sup
F∈F1
m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εicF(x(i))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg
=|c|E/bracketleftBigg
sup
F∈F1
m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiF(x(i))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg
=|c|Rm;Y(F).
59Published in Transactions on Machine Learning Research (03/2024)
For part 3, consider a m-tuple of samples from input space Xas(x(1),x(2),...,x(m))∈Xmand Rademacher
random variables εifori∈[m]. We assume φ:Y→Randφis a Lipschitz function with Lipschitz constant
L. Then
Rm;R(φ◦F) =E/bracketleftBigg
sup
F∈F1
mm/summationdisplay
i=1εi(φ◦F)(x(i))/bracketrightBigg
=Ex(i)∼µm
X1
m/bracketleftbigg
Eε\εm/bracketleftbigg
Eεm/bracketleftbigg
sup
F∈Fum(F) +εm(φ◦F)(x(m))/bracketrightbigg/bracketrightbigg/bracketrightbigg
, (157)
where um(F) =/summationtextm−1
i=1εi(φ◦F)(x(i)).
From the deﬁnition of the supremum, for any /epsilon1m>0(note that mis not an exponent in this notation),
there exists Fm
1,Fm
2∈Fsuch that
um(Fm
1) +εm(φ◦Fm
1)(x(m))≥(1−/epsilon1m)/bracketleftbigg
sup
F∈Fum(F) +εm(φ◦F)(x(m))/bracketrightbigg
(158)
um(Fm
2)−εm(φ◦Fm
2)(x(m))≥(1−/epsilon1m)/bracketleftbigg
sup
F∈Fum(F)−εm(φ◦F)(x(m))/bracketrightbigg
, (159)
otherwise it leads to a contradiction to the supremum assumption.
Therefore,
(1−/epsilon1m)Eεm/bracketleftbigg
sup
F∈Fum(F) +εm(φ◦F)(x(m))/bracketrightbigg
= (1−/epsilon1m)/bracketleftbigg1
2sup
F∈Fum(F) + (φ◦F)(x(m)) +1
2sup
F∈Fum(F)−(φ◦F)(x(m))/bracketrightbigg
(160)
≤1
2/bracketleftBig
um(Fm
1) +εm(φ◦Fm
1)(x(m)) +um(Fm
2)−εm(φ◦Fm
2)(x(m))/bracketrightBig
(161)
=1
2/bracketleftBig
um(Fm
1) +um(Fm
2) +εm/parenleftBig
(φ◦Fm
1)(x(m))−(φ◦Fm
2)(x(m))/parenrightBig/bracketrightBig
. (162)
≤sup
F∈Fum(F) +1
2/bracketleftBig
εm/parenleftBig
(φ◦Fm
1)(x(m))−(φ◦Fm
2)(x(m))/parenrightBig/bracketrightBig
. (163)
Equation (160) is obtained by using the deﬁnition of Rademacher random variable εm. The inequality (161)
is obtained using inequalities for Fm
1andFm
2in (158) and (159), respectively. Inequality (163) is obtained
by introducing supremum in the terms um(Fm
1) +um(Fm
2). As the inequality (163) holds for any /epsilon1m>0,
we claim
Eεm/bracketleftbigg
sup
F∈Fum(F) +εm(φ◦F)(x(m))/bracketrightbigg
≤sup
F∈Fum(F) +1
2/bracketleftBig
εm/parenleftBig
(φ◦Fm
1)(x(m))−(φ◦Fm
2)(x(m))/parenrightBig/bracketrightBig
.(164)
Using (164) in (157) we obtain
Rm;R(φ◦F) =Ex(i)∼µm
X1
m/bracketleftbigg
Eε\εm/bracketleftbigg
Eεm/bracketleftbigg
sup
F∈Fum(F) +εm(φ◦F)(x(m))/bracketrightbigg/bracketrightbigg/bracketrightbigg
≤Ex(i)∼µm
X1
m/bracketleftbigg
Eε\εm/bracketleftbigg
sup
F∈Fum(F) +1
2/bracketleftBig
εm/parenleftBig
(φ◦Fm
1)(x(m))−(φ◦Fm
2)(x(m))/parenrightBig/bracketrightBig/bracketrightbigg/bracketrightbigg
(165)
≤Ex(i)∼µm
X1
m/bracketleftbigg
Eε\{εm−1,εm}/bracketleftbigg
Eεm−1/bracketleftbigg
sup
F∈Fum−1+εm−1(φ◦F)(x(m−1))/bracketrightbigg
+1
2/bracketleftBig
εm/parenleftBig
(φ◦Fm
1)(x(m))−(φ◦Fm
2)(x(m))/parenrightBig/bracketrightBig/bracketrightbigg/bracketrightbigg
, (166)
60Published in Transactions on Machine Learning Research (03/2024)
where um−1= supF∈F/summationtextm−2
i=1εi(φ◦F)(x(i)). Now, for any /epsilon1m−1>0a similar approach is followed to obtain
Fm−1
1,Fm−1
2∈Fsuch that
um−1(Fm−1
1) +εm−1(φ◦Fm−1
1)(x(m−1))≥(1−/epsilon1m−1)/bracketleftbigg
sup
F∈Fum−1(F) +εm−1(φ◦F)(x(m−1))/bracketrightbigg
(167)
um−1(Fm−1
2)−εm−1(φ◦Fm−1
2)(x(m−1))≥(1−/epsilon1m−1)/bracketleftbigg
sup
F∈Fum−1(F)−εm−1(φ◦F)(x(m−1))/bracketrightbigg
.(168)
Using (167) and (168), similar arguments as made earlier help us to claim
Eεm−1/bracketleftbigg
sup
F∈Fum−1(F) +εm−1(φ◦F)(x(m−1))/bracketrightbigg
≤sup
F∈Fum−1(F) +1
2/bracketleftBig
εm−1/parenleftBig
(φ◦Fm−1
1)(x(m−1))−(φ◦Fm−1
2)(x(m−1))/parenrightBig/bracketrightBig
. (169)
Inequality (169) with (166) provides the following
Rm;R(φ◦F)≤Ex(i)∼µm
X1
m/bracketleftbigg
Eε\{εm−2,εm−1,εm}/bracketleftbigg
Eεm−2/bracketleftbigg
sup
F∈Fum−2+εm−2(φ◦F)(x(m−2))/bracketrightbigg
+1
2m/summationdisplay
i=m−1/bracketleftBig
εi/parenleftBig
(φ◦Fi
1)(x(i))−(φ◦Fi
2)(x(i))/parenrightBig/bracketrightBig/bracketrightBigg/bracketrightBigg
, (170)
where um−2= supF∈F/summationtextm−3
i=1εi(φ◦F)(x(i)). We iterate till the last step where
Rm;R(φ◦F)≤Ex(i)∼µm
X1
m/bracketleftbigg
Eε1/bracketleftbigg
sup
F∈Fε1(φ◦F)(x(1))/bracketrightbigg
+1
2m/summationdisplay
i=2/bracketleftBig
εi/parenleftBig
(φ◦Fi
1)(x(i))−(φ◦Fi
2)(x(i))/parenrightBig/bracketrightBig/bracketrightBigg
. (171)
For any/epsilon11>0, there exists F1
1,F1
2∈Fsuch that
ε1(φ◦F1
1)(x(1))≥(1−/epsilon11)/bracketleftbigg
sup
F∈Fε1(φ◦F)(x(1))/bracketrightbigg
(172)
−ε1(φ◦F1
2)(x(1))≥(1−/epsilon11)/bracketleftbigg
sup
F∈F−ε1(φ◦F)(x(1))/bracketrightbigg
. (173)
Using (172) and (173), we obtain
Eε1/bracketleftbigg
sup
F∈Fε1(φ◦F)(x(1))/bracketrightbigg
≤1
2/bracketleftBig
ε1/parenleftBig
(φ◦F1
1)(x(1))−(φ◦F1
2)(x(1))/parenrightBig/bracketrightBig
. (174)
Next, we simplify (171) using (174),
1
2m/summationdisplay
i=1/bracketleftBig
εi/parenleftBig
(φ◦Fi
1)(x(i))−(φ◦Fi
2)(x(i))/parenrightBig/bracketrightBig
≤1
2/bracketleftBiggm/summationdisplay
i=1εi/parenleftBig
(φ◦Fi
1)(x(i))−(φ◦Fi
2)(x(i))/parenrightBig/bracketrightBigg
(175)
=1
2/bracketleftBigg/parenleftBigg
(φ◦m/summationdisplay
i=1εiFi
1)(x(i))−(φ◦m/summationdisplay
i=1εiFi
2)(x(i))/parenrightBigg/bracketrightBigg
(176)
≤L/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1/parenleftBigεi
2Fi
1(x(i))−εi
2Fi
2(x(i))/parenrightBig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y(177)
≤LEsup
F∈F/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiF(x(i))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y. (178)
61Published in Transactions on Machine Learning Research (03/2024)
Inequality (175) is obtained by using Fi
1,Fi
2,∀i∈[m]which are obtained based on the procedure for Fm
1and
Fm
2. The deﬁnition of φis utilized to establish (176). Inequality (177) is obtained by using Lipschitz conti-
nuity of Φand (178) follows from the deﬁnition of supremum and expectation with respect to Rademacher
random variables. Hence using (178) with (174) and (171), we obtain
Rm;R(φ◦F)≤LRm;Y(F).
Next, we recall Lemma 6.6 and provide its proof.
Lemma A.7. E[Θ(z)]is bounded above as follows:
E[Θ(z)]≤2ρL(τ)β1/4(Rm;Y(K0))1/4.
Proof.
E[Θ(z)] =Esup
F∈ρBK[E(F)−Ez(F)] (179)
≤2Esup
F∈ρBK/bracketleftBigg
1
mm/summationdisplay
i=1εiL(y(i),F(x(i)))/bracketrightBigg
(180)
≤2L(τ)Rm;Y(ρBK) (181)
= 2ρL(τ)Rm;Y(BK). (182)
Equation (179) involves a supremum of F∈ρBKas/bardblF/bardbl∞is bounded by ρand inequality (180) is obtained
by using Lemma A.9 in Appendix A.9. Part 3 of Lemma 6.2 is used with φi(.) =L(y(i),.)which has a
Lipschitz constant L(τ)in order to obtain (181). (182) follows from Part 2 of Lemma 6.2. Now,
Rm;Y(BK) =E/bracketleftBigg
sup
F∈BK1
m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiF(x(i))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg
=E
sup
F∈BK1
m
m/summationdisplay
i,j=1εiεj/angbracketleftbigg
F(x(i)),F(x(j))/angbracketrightbigg
Y
1/2
 (183)
=E
sup
F∈BK1
m
m/summationdisplay
i,j=1εiεj/angbracketleftbigg
F,K(x(i),.)F(x(j))/angbracketrightbigg
HK
1/2
 (184)
≤E
sup
K∈Ksup
F∈BKsup
y∈Y1
m
/angbracketleftbigg
F,m/summationdisplay
i,j=1εiεjK(x(i),.)y/angbracketrightbigg
HK
1/2
 (185)
≤E
sup
K∈Ksup
F∈BK:
/bardblF/bardblHK≤1sup
y∈Y1
m
/bardblF/bardblHK/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i,j=1εiεjK(x(i),.)y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
HK
1/2
 (186)
≤E
sup
K∈Ksup
y∈Y1
m
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i,j=1εiεjK(x(i),.)y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
HK
1/2
 (187)
≤E
sup
K∈Ksup
y∈Y1√m
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiK(x(i),.)y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
HK
1/2
 (188)
62Published in Transactions on Machine Learning Research (03/2024)
=E
sup
K∈Ksup
y∈Y1√m
/angbracketleftbiggm/summationdisplay
i=1εiK(x(i),.)y,m/summationdisplay
j=1εjK(x(j),.)y/angbracketrightbigg
HK
1/4
 (189)
≤β1/4E
sup
K∈Ksup
y∈Ysup
x∈X1√m
m/summationdisplay
j=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiK(x(i),x)y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y
1/4
 (190)
=β1/4E
/parenleftBigg
sup
K∈Ksup
y∈Ysup
x∈X1
m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiK(x(i),x)y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/parenrightBigg1/4
 (191)
=β1/4(Rm;Y(K0))1/4. (192)
Thestepsinderiving Rm;Y(BK)≤β1/4(Rm;Y(K0))1/4usepropertiesofnorm, innerproductandreproducing
property of OVKs which have been discussed in Lemma A.8 (Appendix A.9). Therefore,
E[Θ(z)]≤2ρL(τ)β1/4(Rm;Y(K0))1/4.
We discuss the steps involved in deriving Rm;Y(BK)≤β1/4(Rm;Y(K0))1/4next.
Lemma A.8.
Rm;Y(BK)≤β1/4(Rm;Y(K0))1/4. (193)
Proof.
Rm;Y(BK) =E/bracketleftBigg
sup
F∈BK1
m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiF(x(i))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bracketrightBigg
=E
sup
F∈BK1
m
/angbracketleftbiggm/summationdisplay
i=1εiF(x(i)),m/summationdisplay
j=1εmF(x(j))/angbracketrightbigg
Y
1/2
 (194)
=E
sup
F∈BK1
m
m/summationdisplay
i,j=1εiεm/angbracketleftbigg
F(x(i)),F(x(j))/angbracketrightbigg
Y
1/2
 (195)
=E
sup
F∈BK1
m
m/summationdisplay
i,j=1εiεm/angbracketleftbigg
F,K(x(i),.)F(x(j))/angbracketrightbigg
HK
1/2
 (196)
=E
sup
F∈BK1
m
/angbracketleftbigg
F,m/summationdisplay
i,j=1εiεmK(x(i),.)F(x(j))/angbracketrightbigg
HK
1/2
 (197)
≤E
sup
K∈Ksup
F∈BK1
m
/angbracketleftbigg
F,m/summationdisplay
i,j=1εiεmK(x(i),.)F(x(j))/angbracketrightbigg
HK
1/2
 (198)
≤E
sup
K∈Ksup
F∈BKsup
y∈Y1
m
/angbracketleftbigg
F,m/summationdisplay
i,j=1εiεmK(x(i),.)y/angbracketrightbigg
HK
1/2
 (199)
63Published in Transactions on Machine Learning Research (03/2024)
≤E
sup
K∈Ksup
F∈BKsup
y∈Y1
m
/bardblF/bardblHK/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i,j=1εiεmK(x(i),.)y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
HK
1/2
 (200)
=E
sup
K∈Ksup
F∈BK:
/bardblF/bardblHK≤1sup
y∈Y1
m
/bardblF/bardblHK/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i,j=1εiεmK(x(i),.)y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
HK
1/2
 (201)
≤E
sup
K∈Ksup
y∈Y1
m
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i,j=1εiεmK(x(i),.)y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
HK
1/2
 (202)
=E
sup
K∈Ksup
y∈Y1
m
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
j=1εmm/summationdisplay
i=1εiK(x(i),.)y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
HK
1/2
 (203)
≤E
sup
K∈Ksup
y∈Y1
m
m/summationdisplay
j=1|εm|/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiK(x(i),.)y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
HK
1/2
 (204)
=E
sup
K∈Ksup
y∈Y1√m
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiK(x(i),.)y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
HK
1/2
 (205)
=E
sup
K∈Ksup
y∈Y1√m
/angbracketleftbiggm/summationdisplay
i=1εiK(x(i),.)y,m/summationdisplay
j=1εmK(x(j),.)y/angbracketrightbigg
HK
1/4
 (206)
=E
sup
K∈Ksup
y∈Y1√m
m/summationdisplay
i,j=1εiεm/angbracketleftK(x(i),.)y,K(x(j),.)y/angbracketrightHK
1/4
 (207)
=E
sup
K∈Ksup
y∈Y1√m
m/summationdisplay
i,j=1εiεm/angbracketleftK(x(i),x(j))y,y/angbracketrightY
1/4
 (208)
=E
sup
K∈Ksup
y∈Y1√m
/angbracketleftbiggm/summationdisplay
i,j=1εiεmK(x(i),x(j))y,y/angbracketrightbigg
Y
1/4
 (209)
≤E
sup
K∈Ksup
y∈Y1√m
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i,j=1εiεmK(x(i),x(j))y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/bardbly/bardblY
1/4
 (210)
≤β1/4E
sup
K∈Ksup
y∈Y1√m
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i,j=1εiεmK(x(i),x(j))y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y
1/4
 (211)
=β1/4E
sup
K∈Ksup
y∈Y1√m
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
j=1εmm/summationdisplay
i=1εiK(x(i),x(j))y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y
1/4
 (212)
≤β1/4E
sup
K∈Ksup
y∈Y1√m
m/summationdisplay
j=1|εm|/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiK(x(i),x(j))y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y
1/4
 (213)
64Published in Transactions on Machine Learning Research (03/2024)
=β1/4E
sup
K∈Ksup
y∈Y1√m
m/summationdisplay
j=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiK(x(i),x(j))y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y
1/4
 (214)
≤β1/4E
sup
K∈Ksup
y∈Ysup
x∈X1√m
m/summationdisplay
j=1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiK(x(i),x)y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y
1/4
 (215)
=β1/4E
/parenleftBigg
sup
K∈Ksup
y∈Ysup
x∈X1
m/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublem/summationdisplay
i=1εiK(x(i),x)y/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Y/parenrightBigg1/4
 (216)
=β1/4(Rm;Y(K0))1/4. (217)
Reproducing property of OVK Kis used to obtain (196) and (208). Inequalities (200) and 210are obtained
by using the Cauchy-Schwarz inequality. (202) is obtained by using the deﬁnition of BKwith/bardblF/bardblHK≤1.
The rest of the steps follow from the properties of inner-product and norm in HKandY.
Lemma A.9.
Esup
F∈ρBK[E(F)−Ez(F)]≤2E/bracketleftBigg
sup
F∈ρBK1
mm/summationdisplay
i=1εiL(y(i),F(x(i)))/bracketrightBigg
.
Proof.
Esup
F∈ρBK[E(F)−Ez(F)] =Ez∼Zmsup
F∈ρBK/bracketleftBigg
1
mm/summationdisplay
i=1E(x/prime,y/prime)∼ZL(y/prime
i,F(x/prime
i))−Ez(F)/bracketrightBigg
(218)
=Ez∼Zmsup
F∈ρBK/bracketleftBigg
Ez/prime∼Zm1
mm/summationdisplay
i=1L(y/prime
i,F(x/prime
i))−1
mm/summationdisplay
i=1L(y(i),F(x(i)))/bracketrightBigg
(219)
=Ez∼Zm/bracketleftBigg
sup
F∈ρBKEz/prime∼Zm/bracketleftBigg
1
mm/summationdisplay
i=1/parenleftBig
L(y/prime
i,F(x/prime
i))−L(y(i),F(x(i)))/parenrightBig/bracketrightBigg/bracketrightBigg
. (220)
Consider a function fdependent on two random variables X,Yin a class of functions F. Then
=⇒f(X,Y )≤sup
f∈Ff(X,Y )
=⇒EY[f(X,Y )]≤EY[sup
f∈Ff(X,Y )]
=⇒sup
f∈FEY[f(X,Y )]≤EY[sup
f∈Ff(X,Y )]
=⇒EXsup
f∈FEY[f(X,Y )]≤EXEY[sup
f∈Ff(X,Y )]. (221)
Therefore, using the property established in (221), we obtain
Esup
F∈ρBK[E(F)−Ez(F)] =Ez∼Zm/bracketleftBigg
sup
F∈ρBKEz/prime∼Zm/bracketleftBigg
1
mm/summationdisplay
i=1/parenleftBig
L(y/prime
i,F(x/prime
i))−L(y(i),F(x(i)))/parenrightBig/bracketrightBigg/bracketrightBigg
(222)
≤Ez∼ZmEz/prime∼Zm/bracketleftBigg
sup
F∈ρBK1
mm/summationdisplay
i=1/parenleftBig
L(y/prime
i,F(x/prime
i))−L(y(i),F(x(i)))/parenrightBig/bracketrightBigg
(223)
=Ez,z/prime∼Zm,ε/bracketleftBigg
sup
F∈ρBK1
mm/summationdisplay
i=1εi/parenleftBig
L(y/prime
i,F(x/prime
i))−L(y(i),F(x(i)))/parenrightBig/bracketrightBigg
(224)
65Published in Transactions on Machine Learning Research (03/2024)
=Ez,z/prime∼Zm,ε/bracketleftBigg
sup
F∈ρBK/parenleftBigg
1
mm/summationdisplay
i=1εiL(y/prime
i,F(x/prime
i))/parenrightBigg
+ sup
F∈ρBK/parenleftBigg
1
mm/summationdisplay
i=1(−εi)L(y(i),F(x(i)))/parenrightBigg/bracketrightBigg
(225)
=Ez/prime∼Zm,ε/bracketleftBigg
sup
F∈ρBK/parenleftBigg
1
mm/summationdisplay
i=1εiL(y/prime
i,F(x/prime
i))/parenrightBigg/bracketrightBigg
+Ez∼Zm,ε/bracketleftBigg
sup
F∈ρBK/parenleftBigg
1
mm/summationdisplay
i=1εiL(y(i),F(x(i)))/parenrightBigg/bracketrightBigg
(226)
=2E/bracketleftBigg
sup
F∈ρBK1
mm/summationdisplay
i=1εiL(y(i),F(x(i)))/bracketrightBigg
. (227)
(224) follows from (223) by the fact that using Rademacher variables does not change the value of the
expression in (223) (see proof of Theorem 4.1 in (Liao, 2020)). (225) follows from (224) as −εihas the same
distribution as εi. (227) is obtained from (226) as zandz/primefollow identical distribution.
A.10 Details of Experiments
Inordertoillustratetheeﬀectivenessofthedevelopedframework, wehaveusedfunctionalregressionproblem
with an unknown graph structure in the input data for both synthetic and real datasets. The task of
predicting output functions with the help of a Laplacian matrix denoting the relationship between the
set ofpinput functions has been illustrated in the experiments. As practical data is always available as
discrete observations corresponding to functions, standard FDA techniques can be used for the conversion
of functional data into vector representation using basis functions, e.g. Fourier basis, B-spline basis, etc.
LetX= (L2([a,b]))pandY=L2([c,d])be the input and output spaces, respectively. For our experiments,
the error metric used is residual sum of squares error (RSSE) (Kadri et al., 2016) deﬁned as RSSE =/integraltextd
c/summationtext
i{y(i)(t)−ˆy(i)(t)}2dt, wherey(i)is the actual output function and ˆy(i)is the predicted output function.
RSSE is better suited to compare functional outputs. The integrals involved have been approximated by
using numerical integration in our implementation. The quadratic programs involved in (23) and (15) are
solved by using CVXOPT (Andersen et al., 2023).
Experimental Setting : All methods were coded in Python 3.7. All experiments were run on a Linux
box with 182 Gigabytes main memory and 28 CPU cores. As methods to solve the problem of functional
regression problem simultaneously with learning Land/orDare not available, we use popular algorithms
to ﬁrst determine L. Then for the learned L, we use our alternating minimization framework to learn D
using projected gradient descent and uusing OpMINRES. For the MCP-based Llearning and Dlearning
in the proposed alternating minimization framework, we use a decaying step-size in the projected gradient
descent. The decaying step-size regime involves starting with an initial step-size (e.g. 10−4) and reducing
it by a ﬁxed factor (e.g. 2) after a set of iterations (e.g. 5) continuously till a ﬁnal step-size (e.g. 10−9).
Section 7 includes the details of the methods fglasso-OpMINRES-D, KGL-OpMINRES-D, Sparse Non-Pos-
OpMINRES-L-D and Sparse OpMINRES-L-D which we use in this section.
A.10.1 Experiments with synthetic data
Data Generation : For synthetic experiments, three sets of experiments have been considered with input
functions for graph structures having 3-nodes, 12-nodes and 25-nodes, respectively. Here, we discuss the
data generation for all three settings and results for 3-nodes and 25-nodes setting. For all the methods, a
truncated trigonometric basis of L2([0,2π])with 30 basis functions has been considered for encoding the
functional data. The experiments were run for three settings where the data has been divided randomly into
a training set, a validation set and a test set. The following data splits have been considered: (80/20/20),
(160/40/40) and (320/80/80), representing the number of training samples/validation samples/test samples.
The data generation is discussed below.
66Published in Transactions on Machine Learning Research (03/2024)
Figure 3: Samples from the 3-node based synthetic data.
For 3-node setting,
x1(t) =P/summationdisplay
i=1wicos(αit) x2(t) =P/summationdisplay
i=1wicos(αit) +/epsilon1i x3(t) =b,
y(t) =P/summationdisplay
i=1wisin(αit),
wheret∈[0,2π],wi,b∈U([−1,1]),αi∈U([0,1]),/epsilon1i∈N(0,σ2),fori∈[P],σ∈U([0,0.25]).The functions
are sampled at 100 points and normalization has been done after introducing Gaussian noise with 0.02
standard deviation for both input and output functions. Figure 3 includes some samples generated from the
dataset. For 12-node setting,
x1(t) =P/summationdisplay
i=1w1
icos(α1
it) x7(t) =b1+noise 1
x2(t) =P/summationdisplay
i=1w2
icos(α1
it) x8(t) =b2+noise 2
x3(t) =P/summationdisplay
i=1w3
icos(α1
it) x9(t) =b3+noise 3
x4(t) =P/summationdisplay
i=1w1
icos(α2
it) x10(t) =b4
x5(t) =P/summationdisplay
i=1w2
icos(α2
it) x11(t) =b5
x6(t) =P/summationdisplay
i=1w3
icos(α2
it) x12(t) =b6
67Published in Transactions on Machine Learning Research (03/2024)
y(t) =P/summationdisplay
i=1(w1
i+w2
i+w3
i) sin((α1
i+α2
i)t),
wheret∈[0,2π],wj
i,b1,b2,b3∈U([−1,1]),b4,b5,b6,αk
i∈U([0,1]),/epsilon1i∈N(0,σ2),fori∈[P],j= 1,2,3,k=
1,2,σ∈U([0,0.25])andnoisel∈N(0,0.252),l= 1,2,3forl-th partition of [0,2π]. The functions are
sampled at 100 points and normalized after Gaussian noise with 0.02 standard deviation being introduced
for both. Figure 4 includes some samples generated from the dataset. Note that results for 12 node case
have been discussed in the main paper.
Figure 4: Samples from the 12-node based synthetic data.
For 25-node setting,
x1(t) =P/summationdisplay
i=1w1
icos(α1
it) x2(t) =P/summationdisplay
i=1w2
icos(α1
it)
x3(t) =P/summationdisplay
i=1w3
icos(α1
it) x4(t) =P/summationdisplay
i=1w4
icos(α1
it)
x5(t) =P/summationdisplay
i=1w5
icos(α1
it) x6(t) =P/summationdisplay
i=1w1
icos(α2
it)
x7(t) =P/summationdisplay
i=1w2
icos(α2
it) x8(t) =P/summationdisplay
i=1w3
icos(α2
it)
x9(t) =P/summationdisplay
i=1w4
icos(α2
it) x10(t) =P/summationdisplay
i=1w5
icos(α2
it)
68Published in Transactions on Machine Learning Research (03/2024)
x11(t) =b1+noise 1 x12(t) =b2+noise 2
x13(t) =b3+noise 3 x14(t) =b4+noise 4
x15(t) =b5+noise 5 x16(t) =c1
x17(t) =c2 x18(t) =c3
x19(t) =c4 x20(t) =c5
x21(t) =d1x22(t) =d2x23(t) =d3x24(t) =d4x25(t) =d5
y(t) =P/summationdisplay
i=1(w1
i+w2
i+w3
i+w4
i) sin((α1
i+α2
i)t),
wheret∈[0,2π],wj
i,bj∈U([−1,1]),αk
i,cj∈U([0,1]),dj∈U([0,−1])/epsilon1i∈N(0,σ2),fori∈[P],j∈[5],k=
1,2,σ∈U([0,0.25])andnoisel∈N(0,0.252),l∈[5]forl-th partition of [0,2π].The functions are sampled at
100 points and normalization has been done after introducing Gaussian noise with 0.02 standard deviation
for both input and output functions. Figure 5 includes some samples generated from the dataset.
Figure 5: Samples from the 25-node based synthetic data.
TheresultsforsyntheticdataissummarizedinTables9-13whereSparseOpMINRES-L-Dattainscomparable
performance with learned sparse graphs illustrating important relationships driving the functional regression.
Table 9 shows that Sparse OpMINRES-L-D provides comparable results to other methods with respect to
the mean RSSE on test data for 3-nodes setting. Table 10 contains the Dvalues learned for experiments
with 3 nodes which improves the performance in functional regression task in a regularized manner. For
3-nodes setting, the data generation process involves similar information corresponding to node 1 and 2,
whereas node 3 involves random constants. Sparse OpMINRES-L-D captures relationship which includes
sparse relation between nodes 1, 2 and 3. fglasso-OpMINRES-L-D and KGL-OpMINRES-L-D learn fully
connected graphs in Table 12.
Table 11 showcases the mean RSSE results for the functional regression problem for 25-nodes experiment
where Sparse OpMINRES-L-D produces comparable results on the test data. In 25-nodes setting, the data
generation process involves varied information in nodes 1-10, whereas nodes 11-25 contain information which
does not impact the generation of the output function y. The graphs obtained for Sparse OpMINRES-L-D in
69Published in Transactions on Machine Learning Research (03/2024)
Table 9: Mean RSSE results for 3-node synthetic data.
Train/Val/Test samples MethodsMean RSSE
Train Val Test
Sparse OpMINRES-L-D 0.188184 0.117119 0.104051
80/20/20 fglasso-OpMINRES-D 0.109485 0.124759 0.124163
KGL-OpMINRES-D 0.086952 0.124781 0.183851
Sparse Non-Pos-OpMINRES-L-D 0.064445 0.158646 0.149415
Sparse OpMINRES-L-D 0.040655 0.211139 0.198398
160/40/40 fglasso-OpMINRES-D 0.062094 0.183526 0.193644
KGL-OpMINRES-D 0.07809 0.181575 0.201469
Sparse Non-Pos-OpMINRES-L-D 0.089997 0.19503 0.218885
Sparse OpMINRES-L-D 0.046926 0.153285 0.274924
320/80/80 fglasso-OpMINRES-D 0.095652 0.145256 0.281665
KGL-OpMINRES-D 0.057804 0.145889 0.271459
Sparse Non-Pos-OpMINRES-L-D 0.087422 0.150039 0.274598
Table 10:Dfor 3-node synthetic data.
Train/
Val/
Test
samplesSparse OpMINRES-L-D fglasso-OpMINRES-D KGL-OpMINRES-D
80/20/20 0.011319 0.036068 0.119428 1.001583 1.002949 1.000191 1.000672 1.002477 0.999940
160/40/40 1.104571 0.747521 1.229991 1.008645 1.007241 1.001036 1.010710 1.009007 1.000914
320/80/80 1.227095 0.761956 0.956403 1.019795 1.012624 1.001624 1.026910 1.019523 1.004683
Table 11: Mean RSSE results for 25-node synthetic data.
Train/Val/Test samples MethodsMean RSSE
Train Val Test
Sparse OpMINRES-L-D 0.754677 1.567458 1.822983
80/20/20 fglasso-OpMINRES-D 0.905085 1.527465 1.605906
KGL-OpMINRES-D 0.934007 1.478522 1.594960
Sparse Non-Pos-OpMINRES-L-D 0.922789 1.53816 1.564855
Sparse OpMINRES-L-D 0.662029 1.549598 1.215493
160/40/40 fglasso-OpMINRES-D 0.678837 1.602842 1.231550
KGL-OpMINRES-D 0.742796 1.571745 1.212629
Sparse Non-Pos-OpMINRES-L-D 0.600849 1.573893 1.215729
Sparse OpMINRES-L-D 0.767516 1.436166 1.436166
320/80/80 fglasso-OpMINRES-D 1.063051 1.385366 1.429937
KGL-OpMINRES-D 1.069962 1.356429 1.425034
Sparse Non-Pos-OpMINRES-L-D 0.668318 1.418141 1.437544
Table 13 show connections majorly between nodes 1-15. Though the input functions for nodes 11-15 contain
noisy random constant values, this information seems to be associated with input functions for nodes 5-10.
Sparse Non-Pos-OpMINRES-L-D also discovers relations in the clusters of nodes 1-10 and 11-25 majorly.
70Published in Transactions on Machine Learning Research (03/2024)
Table 12: Graphs corresponding to learned Lfor 3-node synthetic data.
Train/
Val/
Test
samplesSparse
OpMINRES-L-Dfglasso-
OpMINRES-DKGL-
OpMINRES-DSparse Non-Pos-
OpMINRES-L-D
80/20/20
160/40/40
320/80/80
A.10.2 Experiments on weather data
Weather data is dynamic and inter-relationships between diﬀerent parameters can be hard to predict. As our
problem solves a functional regression problem based on a relationship between a set of input functions, we
intend to showcase the eﬀectiveness of the proposed algorithm by predicting average dew-point temperature
(F) across 12 weather stations based on their respective air temperatures (F). We consider 1 minute data
of Wyoming ASOS data collected from IEM ASOS One Minute Data (Iowa Environmental Mesonet, 2022).
The data has been collected for an interval of 2 hours for both input functions and output function from
71Published in Transactions on Machine Learning Research (03/2024)
Table 13: Graphs corresponding to learned Lfor 25-node synthetic data. [Best viewed in color]
Train/
Val/
Test
samplesSparse
OpMINRES-L-Dfglasso-
OpMINRES-DKGL-
OpMINRES-DSparse Non-Pos-
OpMINRES-L-D
80/20/20
160/40/40
320/80/80
January, 2022 to August, 2022. Data collected at one minute interval for diﬀerent 12 weather stations in
Wyoming was pre-processed to create 2 hour interval data by disregarding intervals where data was missing
in any of the 12 stations. A total of 718 samples have been collected after removing missing data. The
following 12 weather stations in Wyoming have been considered: Big Piney (1), Casper/Natrona Intl (2),
Cheyenne/Warren AFB (3), Gillette (4), Laramie/Gen. Brees (5), Lander/Hunt Field (6), Yellowstone (7),
Riverton(8), RawlinsMunicipal(9), SheridanCo. Airport(10), TorringtonMunicipalAirport(11), Worland
Municipal (12).
72Published in Transactions on Machine Learning Research (03/2024)
Table 14: Mean RSSE results for small weather data.
Train/Val/Test samples MethodsMean RSSE
Train Val Test
Sparse OpMINRES-L-D 0.004302 0.041949 0.092553
80/20/20 fglasso-OpMINRES-D 0.001716 0.059419 0.082662
KGL-OpMINRES-D 0.002357 0.049899 0.097951
For all the methods, a truncated trigonometric basis of L2([0,1])with 80 basis functions has been considered
for encoding the functional data. We segregate the weather data experiments into small weather data
experiments by considering 120 samples and full weather data experiments. The following random data splits
havebeenconsidered: (80/20/20)and(472/123/123), representingthenumberoftrainingsamples/validation
samples/testsamplesinsmallweatherdataandfullweatherdatasettings,respectively. Wediscusstheresults
for the small weather data here. Note that results for full weather data related experimements are already
presented in the main paper.
Initially, we use a small dataset with 120 samples drawn at random from the considered 8 months. Tables 14-
15 showcase the performance of the algorithms for small weather data. Sparse OpMINRES-L-D performs the
best in terms of mean RSSE on the test data compared to fglasso-OpMINRES-L-D and KGL-OpMINRES-L-
D (Table 14). In Table 15, fglasso-OpMINRES-L-D and KGL-OpMINRES-L-D learn dense fully connected
graphs which do not provide much information regarding the impact of diﬀerent weather stations on the
relationship of respective air temperature to the average dew point temperature. The plots illustrate location
based relation between the 12 weather stations considered in Wyoming. Sparse OpMINRES-L-D learns a
sparseLwhere stations CYS(3) and TOR(11), GCC(4) and WRL(12), LND(6) and RIW(8) along with
P60(7)andRWL(9)areconnected. CYS(3) (41.15564,−104.81047)andTOR(11) (42.06472,−104.15278)are
114.89 km apart with an elevation of 1871 m and 1282 m, respectively. GCC(4) (44.34892,−105.53936)and
WRL(12) (43.96571,−107.95083)are 197.54 km apart with an elevation of 1230 m and 1294 m, respectively.
LND(6) (42.81524,−108.72984)and RIW(8) (43.06423,−108.45984)are 35.37 km apart with an elevation
of 1694 m and 1688 m. P60(7) (44.54444,−110.42111)and RWL(9) (41.8056,−107.19994)are 401.40 km
apart with an elevation of 2368 m and 2077 m. It can be observed that the connections in the learned graph
structure have been established between stations with varying distances lying in close proximity elevation-
wise (in 3 out of 4 cases) and latitude-wise.
A.10.3 Experiments on NBA data
The movement of basketball and 21 players involved on the court ( x-ycoordinates) in the Atlanta Hawks
(ATL) vs Utah Jazz (UTA) match on November 15, 2015 has been considered in this experiment. This
data is available in the Github repo NBA Movement Data (Seward, 2018). The data has been collected for
diﬀerent plays for both input functions of 21 players and output function denoting the position of the ball,
which includes missing data corresponding to some players in diﬀerent plays. The data corresponding to
the following players were used: Kyle Korver [ATL, G] (1), Thabo Sefolosha [ATL, G-F] (2), Paul Millsap
[ATL, F] (3), Al Horford [ATL, C-F] (4), Tiago Splitter [ATL, F-C] (5), Derrick Favors [UTA, F-C] (6),
Gordon Hayward [UTA, F] (7), Trevor Booker [UTA, F] (8), Alec Burks [UTA, G] (9), Shelvin Mack [ATL,
G] (10), Kent Bazemore [ATL, F-G] (11), Chris Johnson [UTA, F] (12), Justin Holiday [ATL, G] (13), Dennis
Schroder [ATL, G] (14), Jeﬀ Withey [UTA, C] (15), Mike Muscala [ATL, F-C] (16), Rudy Gobert [UTA, C]
(17), Trey Burke [UTA, G] (18), Raul Neto [UTA, G] (19), Rodney Hood [UTA, G] (20), Joe Ingles [UTA,
F] (21), where the team, position and number assigned for the experiments has been provided. As plays in
a basketball game are of diﬀerent time duration, we use a truncated trigonometric basis of L2([0,1])with
80 basis functions to sample the functions at ﬁxed 100 points on [0,1]. A total of 351 samples have been
collected based on removing missing data. A random data split of (233/59/59) representing the number
of training samples/validation samples/test samples has been considered. The problem requires solving a
multi-dimensional functional regression problem which is incompatible with fglasso and KGL algorithms, as
both fglasso & KGL are based on single dimensional input functions. Hence, we compare our method with
the algorithm OpMINRES-D where a ﬁxed Lis incorporated in our alternating minimization framework.
73Published in Transactions on Machine Learning Research (03/2024)
Table 15: Graphs corresponding to learned Lfor small weather data. [Best viewed in color]
Train/
Val/
Test
samplesSparse OpMINRES-L-D fglasso-OpMINRES-D KGL-OpMINRES-D
80/20/20
OpMINRES-D : A ﬁxedLis considered corresponding to a fully connected network of 21 nodes. This
decision was made as fglasso mostly learns a fully connected graph in earlier experiments. Thus, a ﬁxed L
(with no sparsity-inducing MCP) is used in the proposed alternating minimization regime for optimizing u
andD. OpMINRES is used with k1(x,x/prime;G) =e−γx(x−x/prime)/latticetop(L+D)(x−x/prime)−γy(x−x/prime)/latticetop(L+D)(x−x/prime)andk1
2(s,t) =
e−γ1
op|s−t|,k1
2(s,t) =e−γ2
op|s−t|, whereγx,γy∈{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}andγ1
op,γ2
op∈
{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}.
Sparse OpMINRES-L-D : We consider the graph-induced operator-valued kernels using k1(x,x/prime;G) =
e−γx(x−x/prime)/latticetop(L+D)(x−x/prime)−γy(x−x/prime)/latticetop(L+D)(x−x/prime)andk1
2(s,t) =e−γ1
op|s−t|,k2
2(s,t) =e−γ2
op|s−t|, whereγx,γy∈
{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}andγ1
op,γ2
op∈{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}.
Projected gradient descent is used in minimization with respect to LandDbased on a decaying step-size.
The sparsity is aided by the MCP regularization considered in learning of L.
Table 17: Mean RSSE results for NBA data.
Train/Val/Test samples MethodsMean RSSE
Train Val Test
233/59/59 Sparse OpMINRES-L-D 0.025200 0.087748 0.106344
OpMINRES-D 0.023261 0.191459 0.265513
The results are illustrated in Tables 16-17. where comparison method OpMINRES-D uses a fully connected
graph, however Sparse OpMINRES-L-D performs better with a sparse learned graph in terms of mean
RSSE on the test data. The following major relations are obtained for the game based on graph structure
corresponding to the learned Lin Table 16:
•Derrick Favors [UTA, F-C]—Trevor Booker [UTA, F] (6—8)
•Al Horford [ATL, C-F]—Gordon Hayward [UTA, F] (4—7)
•Alec Burks [UTA, G]—Trey Burke [UTA, G] (9—18)
74Published in Transactions on Machine Learning Research (03/2024)
Table 16: Graph corresponding to learned Lfor NBA data. [Best viewed in color]
Train/
Val/
Test
samplesSparse OpMINRES-L-D
233/59/59
•Thabo Sefolosha [ATL, G-F]—Kent Bazemore [ATL, F-G] (2—11)
•Dennis Schroder [ATL, G]—Rodney Hood [UTA, G] (14—20)
•Tiago Splitter [ATL, F-C]—Jeﬀ Withey [UTA, F-C] (5—15).
From the match report published in ESPN match recap and ESPN match scoreboard (ESPN, 2015a;b), it
is clear that the relation Derrick Favors—Trevor Booker (6—8) had been pivotal in the win of Utah Jazz.
The performance of Al Horford and Kent Bazemore for Atlanta Hawks was mentioned and captured by the
relations (4—7) and (2—11). Though the partnership of Alec Burks—Trey Burke (9—18) for Utah Jazz is
not evident in the match reports, their ball carrying interactions may be the reason for being learned in L.
A.10.4 Hyperparameters for Experiments
In this section, we list the hyperparameters used for Sparse OpMINRES-L-D for diﬀerent experiments
illustrated in this work.
Common hyperparameters: λreg= 0.5,γreg= 1(MCP), maxiter = 1000, tol = 10−3(OpMINRES). The
decaying step-size regime for projected gradient descent in LandD-based minimization involves starting
with an initial step-size 10−4and reducing it by a ﬁxed factor 2 after a set of 5 iterations continuously till a
ﬁnal step-size (or learning rate) 10−9.
As our proposed approach Sparse OpMINRES-L-D aggregates many components, we provide some ablation
studies to illustrate eﬀectiveness in the next section.
75Published in Transactions on Machine Learning Research (03/2024)
Table 18: Hyperparameters used for experiments with Sparse OpMINRES-L-D on diﬀerent data sets
Experiment No. of Training Samples γopλγρLρDmtrace
3-node80 10 10−510−410−3100 2
160 10 10−410−210−310 2
320 10 10−410−210−310 2
12-node80 1 10−510−610−21000 6
160 10 10−610−610−210 6
320 10 10−410−510−110−56
25-node80 10−110−310−410−11013
160 10 10−310−310−2100 13
320 100 10−210−210−1100 13
Weather Data80 10−310−210−110−416
472 1 10−210−110−410−16
NBA Data 233γ1
op= 100
γ2
op= 10010−2γx= 0.5
γy= 0.510210211
A.10.5 Experiments for Ablation Studies
In order to understand the impact of diﬀerent components of Sparse OpMINRES-L-D, we have run experi-
mentsfor12-nodesyntheticdatabyvaryingdiﬀerenthyperparametersandswitchingoﬀdiﬀerentcomponents
in our approach. In order to enforce sparsity of learned graphs, we introduce mtracebased constraint in Sec-
tion 5.1.4. In Table 19, we tabulate the graphs corresponding to learned Lwith Sparse OpMINRES-L-D for
12-node experiments using mtrace∈{0.1p,0.25p,0.5p,0.75p,0.9p}withp= 12. From Table 19, we observe
that most of the edges are being retained as mtracevalue is increased. The connections learned are illustra-
tive of the generation process of synthetic data in Section A.10.1 as mtraceis increased. Choice of mtracecan
be based on the error corresponding to the validation set as well as the desired number of connections to be
learned since the number of edges increases with increase in mtrace. Table 20 illustrates that the performance
with diﬀerent mtraceis comparable and a trade-oﬀ is expected when varying mtrace.
To illustrate the impact of choosing diﬀerent kernels in our framework, we utilize diﬀerent kernels as k2in
(2) by utilizing the following:
•ABS:k2(s,t) =e−γop|s−t|
•DIFFABS: k2(s,t) =e−γop1|s−t|−e−γop2|s−t|
•RBF:k2(s,t) =e−γop1|s−t|2
•EPAN:k2(s,t) = max(0,1−γop|s−t|2),
withγop,γop1,γop2∈{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}. Note that we use k1as deﬁned in (108)
which incorporates LandDof the learned graph. Table 21 showcases comparable performance of diﬀerent
kernels ask2, but Table 22 illustrates the graphs learned diﬀerent connections for ABS, DIFFABS. We see
that RBF and EPAN kernel choices learn diﬀerent graphs which contain some useful connections which can
be related to the generation of the synthetic data. It can be interpreted that most of the connections learned
by ABS, DIFFABS and RBF kernels better represent the generation of 12-node synthetic data.
Next, we study the impact of the mtraceconstraint in the sparsity regularization and the complete sparsity
regularization framework as proposed in Section 5.1.4. Table 23 illustrates comparable performance based
on mean RSSE error where mtraceconstraint is removed and LandD-based regularization is removed by
settingρLandρDas 0. Similarly, Table 24 illustrates comparable performance when MCP regularization is
76Published in Transactions on Machine Learning Research (03/2024)
Table 19: Graph corresponding to learned Lby considering mtracein{0.1p,0.25p,0.5p,0.75p,0.9p}in Sparse
OpMINRES-L-D for 12-node experiments ( p= 12). [Best viewed in color]
Train/
Val/
Test
samplesmtrace = 0.1p mtrace = 0.25p mtrace = 0.5p mtrace = 0.75p mtrace = 0.9p
80/20/20
160/40/40
320/80/80
completely removed and LandD-based regularization is removed by setting ρLandρDas 0. Although the
performanceiscomparableintermsofmeanRSSEerror, howeverwenotethatTables25and26illustratethe
failure to learn meaningful graphs since most of the graphs have connections with equal weights providing no
relevant information. Without the mtraceconstraint, Table 25 showcases negligible weights being assigned to
each connection, while without the complete MCP regularization framework Table 26 showcases uniformly
distributed weights across fully connected graphs.
77Published in Transactions on Machine Learning Research (03/2024)
Table 20: Mean RSSE results for mtracein{0.1p,0.25p,0.5p,0.75p,0.9p}using Sparse OpMINRES-L-D in
12-node synthetic data ( p= 12).
Train/Val/Test samples mtracein Sparse OpMINRES-L-DMean RSSE
Train Val Test
mtrace = 0.1p 1.250593 1.727192 1.532670
mtrace = 0.25p 1.234657 1.735397 1.587149
80/20/20 mtrace = 0.5p 1.140691 1.735397 1.583640
mtrace = 0.75p 1.092453 1.904231 1.636385
mtrace = 0.9p 1.075372 1.916426 1.640379
mtrace = 0.1p 0.873584 1.291735 1.380035
mtrace = 0.25p 0.886945 1.263236 1.368678
160/40/40 mtrace = 0.5p 0.888574 1.229568 1.385952
mtrace = 0.75p 0.849892 1.265577 1.412620
mtrace = 0.9p 0.831497 1.284316 1.425376
mtrace = 0.1p 1.073370 1.291374 1.237471
mtrace = 0.25p 1.071354 1.295419 1.238216
320/80/80 mtrace = 0.5p 1.062102 1.294110 1.239181
mtrace = 0.75p 1.056678 1.295300 1.242216
mtrace = 0.9p 1.053001 1.296809 1.241735
Table 21: Mean RSSE results for diﬀerent kernels as k2with 12-node synthetic data.
Train/Val/Test samples k2in Sparse OpMINRES-L-DMean RSSE
Train Val Test
ABS 1.140691 1.780445 1.583640
80/20/20 DIFFABS 1.167264 1.806093 1.618175
RBF 1.111369 2.092855 1.754655
EPAN 0.759214 2.035527 1.838925
ABS 0.888574 1.229568 1.385952
160/40/40 DIFFABS 1.154356 1.362239 1.417921
RBF 0.846649 1.236121 1.428687
EPAN 0.906656 1.260602 3.626625
ABS 1.062102 1.294110 1.239181
320/80/80 DIFFABS 1.073292 1.295140 1.243346
RBF 0.931760 1.330628 1.283005
EPAN 1.026490 1.304147 1.247868
Further, we perform experiments where we utilize a lasso based regularization to induce sparsity instead
of depending upon the proposed sparsity inducing framework. We introduce ρD/summationtextp
i=1|Dii|,ρD>0in (3)
instead ofρL/summationtextn
i=1x(i)/latticetopLx(i)+ρD/bardblD/bardbl2
Fand ignore the proposed MCP regularization framework. Table
28 illustrates comparable performance in terms of mean RSSE error but Table 27 showcases the failure to
distinguish meaningful interactions in the learned graphs.
78Published in Transactions on Machine Learning Research (03/2024)
Table22: Graphcorrespondingtolearned Lbyconsideringdiﬀerentkernelsfor k2inSparseOpMINRES-L-D
for 12-node experiments. [Best viewed in color]
Train/
Val/
Test
samplesABS DIFFABS RBF EPAN
80/20/20
160/40/40
320/80/80
79Published in Transactions on Machine Learning Research (03/2024)
Table 23: Mean RSSE results for 12-node synthetic data using Sparse OpMINRES-L-D with no mtrace
constraint and ablation of regularization with respect to LandD.
Train/Val/
Test samplesMethodsMean RSSE
Train Val Test
Sparse OpMINRES-L-D 1.140691 1.780445 1.583640
80/20/20 Sparse OpMINRES-L-D with no mtrace 1.274110 1.729524 1.530376
Sparse OpMINRES-L-D with no mtraceandρL= 01.273360 1.729269 1.530041
Sparse OpMINRES-L-D with no mtraceandρD= 00.790852 1.891594 1.407203
Sparse OpMINRES-L-D 0.888574 1.229568 1.385952
160/40/40 Sparse OpMINRES-L-D with no mtrace 0.896321 1.229568 1.371430
Sparse OpMINRES-L-D with no mtraceandρL= 00.896311 1.280351 1.371432
Sparse OpMINRES-L-D with no mtraceandρD= 00.833068 1.291457 1.410686
Sparse OpMINRES-L-D 1.062102 1.294110 1.239181
320/80/80 Sparse OpMINRES-L-D with no mtrace 1.078058 1.291549 1.237178
Sparse OpMINRES-L-D with no mtraceandρL= 01.078026 1.291574 1.237129
Sparse OpMINRES-L-D with no mtraceandρD= 01.078058 1.291550 1.237179
Table 24: Mean RSSE results for 12-node synthetic data using Sparse OpMINRES-L-D without MCP
regularization and ablation of regularization with respect to LandD.
Train/Val/
Test samplesMethodsMean RSSE
Train Val Test
Sparse OpMINRES-L-D 1.140691 1.780445 1.583640
80/20/20 Sparse OpMINRES-L-D with no MCP 0.911711 1.893550 1.481332
Sparse OpMINRES-L-D with no MCP and ρL= 00.911711 1.893550 1.481332
Sparse OpMINRES-L-D with no MCP and ρD= 00.755296 1.892366 1.399945
Sparse OpMINRES-L-D 0.888574 1.229568 1.385952
160/40/40 Sparse OpMINRES-L-D with no MCP 0.838759 1.282838 1.397084
Sparse OpMINRES-L-D with no MCP and ρL= 00.838759 1.282838 1.397084
Sparse OpMINRES-L-D with no MCP and ρD= 00.793097 1.289822 1.429700
Sparse OpMINRES-L-D 1.062102 1.294110 1.239181
320/80/80 Sparse OpMINRES-L-D with no MCP 1.062727 1.298913 1.240176
Sparse OpMINRES-L-D with no MCP and ρL= 01.062727 1.298913 1.240176
Sparse OpMINRES-L-D with no MCP and ρD= 01.062727 1.298926 1.240208
80Published in Transactions on Machine Learning Research (03/2024)
Table 25: Graph corresponding to learned Lby nomtraceconstraint and controlling ρLinL-based regular-
ization and ρDinD-based regularization for 12-node experiments. [Best viewed in color]
Train/
Val/
Test samples80/20/20 160/40/40 320/80/80
Nomtrace
ρL= 0, nomtrace
ρD= 0, nomtrace
81Published in Transactions on Machine Learning Research (03/2024)
Table 26: Graph corresponding to learned Lby switching oﬀ the MCP-based regularization and controlling
ρLinL-based regularization and ρDinD-based regularization for 12-node experiments. [Best viewed in
color]
Train/
Val/
Test samples80/20/20 160/40/40 320/80/80
No MCP
ρL= 0, no MCP
ρD= 0, no MCP
82Published in Transactions on Machine Learning Research (03/2024)
Table 27: Graph corresponding to learned Lby using only D-lasso based regularization without MCP-based
sparsity inducing regularization instead of Sparse OpMINRES-L-D for 12-node experiments. [Best viewed
in color]
Train/
Val/
Test samples80/20/20 160/40/40 320/80/80
Sparse OpMINRES-L-D
D-lasso without MCP
Table 28: Mean RSSE results for 12-node synthetic data using Sparse OpMINRES-L-D without MCP
regularization and lasso regularization for D.
Train/Val/
Test samplesMethodsMean RSSE
Train Val Test
80/20/20 Sparse OpMINRES-L-D 1.140691 1.780445 1.583640
Sparse OpMINRES-L-D with no MCP and D lasso 0.745433 1.891283 1.402357
160/40/40 Sparse OpMINRES-L-D 0.888574 1.229568 1.385952
Sparse OpMINRES-L-D with no MCP and D lasso 0.821067 1.284889 1.409910
320/80/80 Sparse OpMINRES-L-D 1.062102 1.294110 1.239181
Sparse OpMINRES-L-D with no MCP and D lasso 1.073490 1.297428 1.241109
83