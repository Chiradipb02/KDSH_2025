Under review as submission to TMLR
Explicit Regularization via Regularizer Mirror Descent
Anonymous authors
Paper under double-blind review
Abstract
Despite perfectly interpolating the training data, deep neural networks (DNNs) can often
generalize fairly well, in part due to the “implicit regularization” induced by the learning al-
gorithm. Nonetheless, various forms of regularization, such as “explicit regularization” (via
weight decay), are often used to avoid overﬁtting, especially when the data is corrupted.
There are several challenges with explicit regularization, most notably unclear convergence
properties. Inspired by the convergence properties of stochastic mirror descent (SMD) algo-
rithms, we propose a new method for training DNNs with regularization, called regularizer
mirror descent (RMD). In highly overparameterized DNNs, SMD simultaneously interpo-
latesthetrainingdataandminimizesacertainpotentialfunctionoftheweights. RMDstarts
with a standard cost which is the sum of the training loss and a convex regularizer of the
weights. Reinterpreting this cost as the potential of an “augmented” overparameterized net-
work and applying SMD yields RMD. As a result, RMD inherits the properties of SMD and
provably converges to a point “close” to the minimizer of this cost. RMD is computationally
comparable to stochastic gradient descent (SGD) and weight decay and is parallelizable in
the same manner. Our experimental results on training sets with various levels of corruption
suggest that the generalization performance of RMD is remarkably robust and signiﬁcantly
better than both SGD and weight decay, which implicitly and explicitly regularize the /lscript2
norm of the weights. RMD can also be used to regularize the weights to a desired weight
vector, which is particularly relevant for continual learning.
1 Introduction
1.1 Motivation
Today’s deep neural networks are typically highly overparameterized and often have a large enough capacity
to easily overﬁt the training data to zero training error (Zhang et al., 2016). Furthermore, it is now widely
recognized that such networks can still generalize well despite (over)ﬁtting (Bartlett et al., 2020; Belkin et al.,
2018; 2019; Nakkiran et al., 2021; Bartlett et al., 2021), which is, in part, due to the “implicit regularization”
(Gunasekar et al., 2018a; Azizan & Hassibi, 2019b; Neyshabur et al., 2015; Boﬃ & Slotine, 2021) property
of the optimization algorithms such as stochastic gradient descent (SGD) or its variants. However, in many
cases, especially when the training data is known to include corrupted samples, it is still highly desirable to
avoid overﬁtting the training data through some form of regularization (Goodfellow et al., 2016; Kukačka
et al., 2017). This can be done through, e.g., early stopping, or explicit regularization of the network
parameters via weight decay. However, the main challenge with these approaches is that their convergence
properties are in many cases unknown and they typically do not come with performance guarantees.
1.2 Contributions
The contributions of the paper are as follows.
1) We propose a new method for training DNNs with regularization, called regularizer mirror descent
(RMD), which allows for choosing any desired convex regularizer of the weights. RMD leverages the implicit
regularization properties of the stochastic mirror descent (SMD) algorithm. It does so by reinterpreting the
1Under review as submission to TMLR
explicit cost (the sum of the training loss and convex regularizer) as the potential function of an “augmented”
network. SMD applied to this augmented network and cost results in RMD.
2) Contrary to most existing explicit regularization methods, RMD comes with convergence guarantees ,
as a result of the connection to SMD. More speciﬁcally, for highly overparameterized models, it provably
converges to a point “close” to the minimizer of the cost.
3) RMD is computationally and memory-wise eﬃcient . It imposes virtually no additional overhead com-
pared to standard SGD, and can run in mini-batches and/or be distributed in the same manner.
4) We evaluate the performance of RMD using a ResNet-18 neural network architecture on the CIFAR-10
dataset with various levels of corruption. The results show that the generalization performance of RMD is
remarkably robust to data corruptions and signiﬁcantly better than both the standard SGD, which implicitly
regularizes the /lscript2norm of the weights, as well as weight decay, which explicitly does so. Further, unlike other
explicit regularization methods, e.g., weight decay, the generalization performance of RMD is very consistent
and not sensitive to the regularization parameter.
5) An extension of the convex regularizer can be used to guarantee the closeness of the weights to a desired
weight vector with a desired notion of distance. This makes RMD particularly relevant for continual learning .
Therefore, we believe that RMD provides a very viable alternative to the existing explicit regularization
approaches.
1.3 Related Work
Thereexista multitudeofregularizationtechniquesthatare usedinconjunctionwiththetraining procedures
of DNNs. See, e.g., Goodfellow et al. (2016); Kukačka et al. (2017) for a survey. While it is impossible to
discuss every work in the literature, the techniques can be broadly divided into the following categories based
on how they are performed: (i) via data augmentation , such as mixup (Zhang et al., 2018b), (ii) via the
network architecture , such as dropout (Hinton et al., 2012), and (iii) via the optimization algorithm , such as
early stopping (Li et al., 2020; Yao et al., 2007; Molinari et al., 2021), among others.
Our focus in this work is on explicit regularization, which is done through adding a regularization term to
the cost. Therefore, the most closely comparable approach is weight decay (Zhang et al., 2018a), which adds
an/lscript2-norm regularizer to the objective. However, our method is much more general, as it can handle any
desired strictly-convex regularizer.
Another related work is that of Hu et al. (2019), who proposed two diﬀerent forms of regularization with
convergence guarantees. Their ﬁrst regularizer (RDI) is based on distance to initialization. This is a special
case of our formulation (13) when ψ(w) =1
2/bardblw/bardbl2. Their second regularizer (AUX) is based on inserting an
auxiliary variable inside the loss the function, i.e.,1
2/summationtextn
i=1(f(w,xi) +αbi−yi)2. This is a diﬀerent kind of
regularizer from the standard augmented form and is thus not directly comparable with our approach.
As mentioned earlier, our algorithm for solving the explicitly-regularized problem leverages the “implicit
regularization” behavior of a family of optimization algorithms called stochastic mirror descent (Azizan
et al., 2021). We discuss this further in Section 2.3.
The rest of the paper is organized as follows. We review some preliminaries about explicit and implicit
regularization in Section 2. We present the main RMD algorithm and its various special cases in Section 3.
InSection4, weperformanexperimentalevaluationofRMDanddemonstrateitsgeneralizationperformance.
In Section 5, we show that RMD can be readily used for regularizing the weights to be close to any desired
weight vector, which is particularly important for continual learning. We present the convergence guarantees
of RMD in Section 6, and ﬁnally conclude in Section 7.
2 Background
We review some background about stochastic gradient methods and diﬀerent forms of regularization.
2Under review as submission to TMLR
2.1 Stochastic Gradient Descent
LetLi(w)denote the loss on the data point ifor a weight vector w∈Rp. For a training set consisting
ofndata points, the total loss is/summationtextn
i=1Li(w), which is typically attempted to be minimized by stochastic
gradient descent (Robbins & Monro, 1951) or one of its variants (such as mini-batch, distributed, adaptive,
with momentum, etc.). Denoting the model parameters at the t-th time step by wt∈Rpand the index of
the chosen data point by i, the update rule of SGD can be simply written as
wt=wt−1−η∇Li(wt−1), t≥1, (1)
whereηis the so-called learning rate, w0is the initialization, and ∇Li(·)is the gradient of the loss. When
trained with SGD, typical deep neural networks (which have many more parameters than the number of
data points) often achieve (near) zero training error (Zhang et al., 2016), or, in other words, “interpolate”
the training data (Ma et al., 2018).
2.2 Explicit Regularization
As mentioned earlier, it is often desirable to avoid (over)ﬁtting the training data to zero error, e.g., when
the data has some corrupted labels. In such scenarios, it is beneﬁcial to augment the loss function with a
(convex and diﬀerentiable) regularizer ψ:Rp→R, and consider
min
wλn/summationdisplay
i=1Li(w) +ψ(w), (2)
whereλ≥0is a hyper-parameter that controls the strength of regularization relative to the loss function.
A simple and common choice of regularizer is ψ(w) =1
2/bardblw/bardbl2. In this case, when SGD is applied to (2) it
is commonly referred to as weight decay. Note that the bigger λis, the more eﬀort in the optimization is
spent on minimizing/summationtextn
i=1Li(w). Since the losses Li(·)are non-negative, the lowest these terms can get is
zero, and thus, for λ→∞, the problem would be equivalent to the following:
min
wψ(w)
s.t.Li(w) = 0, i= 1,...,n.(3)
2.3 Implicit Regularization
Recently, it has been noted in several papers that, even withoutimposing any explicit regularization in
the objective, i.e., by optimizing only the loss function/summationtextn
i=1Li(w), there is still an implicit regularization
induced by the optimization algorithm used for training (Gunasekar et al., 2018a;b; Azizan & Hassibi, 2019b;
Lyu & Li, 2019; Poggio et al., 2020). Namely, when initialized at the origin, SGD with suﬃciently small
step size tends to converge to interpolating solutions with minimum /lscript2norm (Engl et al., 1996; Gunasekar
et al., 2018a), i.e.,1
min
w/bardblw/bardbl2
s.t.Li(w) = 0, i= 1,...,n.
More generally, it has been shown (Gunasekar et al., 2018a; Azizan et al., 2021) that SMD, whose update
rule is deﬁned for a diﬀerentiable strictly-convex “potential function” ψ(·)as
∇ψ(wt) =∇ψ(wt−1)−η∇Li(wt−1), (4)
with proper initialization and suﬃciently small learning rate tends to converge to the solution of2
min
wψ(w)
s.t.Li(w) = 0, i= 1,...,n.(5)
1See Section 6 for a more precise statement.
2See Section 6 and Theorem 6.3 for a more precise statement.
3Under review as submission to TMLR
Note that this is equivalent to the case of explicit regularization with λ→∞, i.e., problem (3).
3 Proposed Method: Regularizer Mirror Descent (RMD)
When it is undesirable to reach zero training error, e.g., due to the presence of corrupted samples in the
data, one cannot rely on the implicit bias of the optimization algorithm to avoid overﬁtting. That is because
these algorithms would interpolate the corrupted data as well. This suggests using explicit regularization
as in (2). Unfortunately, standard explicit regularization methods, such as weight decay, which is simply
employing SGD to (2), do not come with convergence guarantees. Here, we propose a new algorithm, called
Regularizer Mirror Descent (RMD), which, under appropriate conditions, provably regularizes the weights
for any desired diﬀerentiable strictly-convex regularizer ψ(·). In other words, RMD converges to a weight
vector close to the minimizer of (2).
We are interested in solving the explicitly-regularized optimization problem (2). Let us deﬁne an auxiliary
variablez∈Rnwith elements z[1],...,z [n]. The optimization problem (2) can be transformed into the
following form:
min
w,zλn/summationdisplay
i=1z2[i]
2+ψ(w)
s.t.z[i] =/radicalbig
2Li(w), i= 1,...,n.(6)
The objective of this optimization problem is a strictly-convex function
ˆψ(w,z) =ψ(w) +λ
2/bardblz/bardbl2,
and there are nequality constraints. We can therefore think of an “augmented” network with two sets of
weights,wandz. To enforce the constraints z[i] =/radicalbig
2Li(w), we can deﬁne a “constraint-enforcing” loss
ˆ/lscript/parenleftBig
z[i]−/radicalbig
2Li(w)/parenrightBig
, where ˆ/lscript(·)is a diﬀerentiable convex function with a unique root at 0(e.g., the square
lossˆ/lscript(·) =(·)2
2). Thus, (6) can be rewritten as
min
w,zˆψ(w,z)
s.t. ˆ/lscript/parenleftBig
z[i]−/radicalbig
2Li(w)/parenrightBig
= 0, i= 1,...,n.(7)
Note that (7) is similar to the implicitly-regularized optimization problem (5), which can be solved via
SMD. To do so, we need to follow (4) and compute the gradients of the potential ˆψ(·,·), as well as the loss
ˆ/lscript/parenleftBig
z[i]−/radicalbig
2Li(w)/parenrightBig
, with respect to wandz. We omit the details of this straightforward calculation and
simply state the result, which we call the RMD algorithm.
At timet, when the i-th training sample is chosen for updating the model, the update rule of RMD can be
written as follows:
∇ψ(wt) =∇ψ(wt−1) +ct,i/radicalbig
2Li(wt−1)∇Li(wt−1),
zt[i] =zt−1[i]−ct,i
λ,
zt[j] =zt−1[j],∀j/negationslash=i, (8)
wherect,i=ηˆ/lscript/prime/parenleftBig
zt−1[i]−/radicalbig
2Li(wt−1)/parenrightBig
,ˆ/lscript/prime(·)is the derivative of the constraint-enforcing loss function,
and the variables are initialized with w0= 0andz0= 0. Note that because of the strict convexity of
the regularizer ψ(·), its gradient∇ψ(·)is an invertible function, and the above update rule is well-deﬁned.
Algorithm 1 summarizes the procedure. As will be shown in Section 6, under suitable conditions, RMD
provably solves the optimization problem (2).
One can choose the constraint-enforcing loss as ˆ/lscript(·) =(·)2
2, which implies ˆ/lscript/prime(·) = (·), to simply obtain the
same update rule as in (8) with ct,i=η/parenleftBig
zt−1[i]−/radicalbig
2Li(wt−1)/parenrightBig
.
4Under review as submission to TMLR
Algorithm 1 Regularizer Mirror Descent (RMD)
Require:λ,η,w 0
1:Initialization: w←w0,z←0
2:repeat
3:Take a data point i
4:c←ηˆ/lscript/prime/parenleftBig
z[i]−/radicalbig
2Li(w)/parenrightBig
5:w←∇ψ−1/parenleftbigg
∇ψ(w) +c√
2Li(w)∇Li(w)/parenrightbigg
6:z[i]←z[i]−c
λ
7:untilconvergence
8:returnw
3.1 Special Case: q-norm Potential
An important special case of RMD is when the potential function ψ(·)is chosen to be the /lscriptqnorm, i.e.,
ψ(w) =1
q/bardblw/bardblq
q=1
q/summationtextp
k=1|w[k]|q, for a real number q >1. Let the current gradient be denoted by g:=
∇Li(wt−1). In this case, the update rule can be written as
wt[k] =/vextendsingle/vextendsingleξt,i/vextendsingle/vextendsingle1
q−1sign/parenleftbig
ξt,i/parenrightbig
,∀k
zt[i] =zt−1[i]−ct,i
λ,
zt[j] =zt−1[j],∀j/negationslash=i, (9)
forξt,i=|wt−1[k]|q−1sign(wt−1[k]) +ct,i√
2Li(wt−1)g[k], wherewt[k]denotes the k-th element of wt(the weight
vector at time t) andg[k]is thek-th element of the current gradient g. Note that for this choice of potential
function, the update rule is separable , in the sense that the update for the k-th element of the weight
vector requires only the k-th element of the weight and gradient vectors. This allows for eﬃcient parallel
implementation of the algorithm, which is crucial for large-scale tasks.
Even among the family of q-norm RMD algorithms, there can be a wide range of regularization eﬀects for
diﬀerent values of q. Some important examples are as follows:
/lscript1normregularization promotes sparsity in the weights. Sparsity is often desirable for reducing the storage
and/or computational load, given the massive size of state-of-the-art DNNs. However, since the /lscript1-norm is
neither diﬀerentiable nor strictly convex, one may use ψ(w) =1
1+/epsilon1/bardblw/bardbl1+/epsilon1
1+/epsilon1for some small /epsilon1 >0(Azizan &
Hassibi, 2019a).
/lscript∞normregularization promotes bounded and small range of weights. With this choice of potential, the
weights tend to concentrate around a small interval. This is often desirable in various implementations
of neural networks since it provides a small dynamic range for quantization of weights, which reduces the
production cost and computational complexity. However, since /lscript∞is, again, not diﬀerentiable, one can
choose a large value for qand useψ(w) =1
q/bardblw/bardblq
qto achieve the desirable regularization eﬀect of /lscript∞norm
(q= 10is used in Azizan et al. (2021)).
/lscript2normstill promotes small weights, similar to /lscript1norm, but to a lesser extent. The update rule is
wt[k] =wt−1[k] +ct,i/radicalbig
2Li(wt−1)g[k],∀k
zt[i] =zt−1[i]−ct,i
λ,
zt[j] =zt−1[j],∀j/negationslash=i. (10)
5Under review as submission to TMLR
3.2 Special Case: Negative Entropy Potential
One can choose the potential function ψ(·)to be the negative entropy, i.e., ψ(w) =/summationtextp
k=1w[k] log(w[k]). For
this particular choice, the associated Bregman divergence (Bregman, 1967; Azizan & Hassibi, 2019c) reduces
to the Kullback–Leibler divergence. Let the current gradient be denoted by g:=∇Li(wt−1). The update
rule would be
wt[k] =wt−1[k] exp/parenleftBigg
ct,i/radicalbig
2Li(wt−1)g[k]/parenrightBigg
∀k
zt[i] =zt−1[i]−ct,i
λ,
zt[j] =zt−1[j],∀j/negationslash=i, (11)
This update rule requires the weights to be positive.
4 Experimental Results
As mentioned in the introduction, there are many ways to regularize DNNs and improve their generalization
performance, including methods that perform data augmentation, a change to the network architecture, or
early stopping. However, since in this paper we are concerned with the eﬀect of the learning algorithm,
we will focus on comparing RMD with the standard SGD (which induces implicit regularization) and the
standard weight decay (which attempts to explicitly regularize the /lscript2norm of the weights). No doubt the
results can be improved by employing the aforementioned methods with these algorithms, but we leave that
study for the future since it will not allow us to isolate the eﬀect of the algorithm.
As we shall momentarily see, the results indicate that RMD outperforms both alternatives by a signiﬁcant
margin, thus making it a viable option for explicit regularization.
4.1 Setup
Dataset. To test the performance of diﬀerent regularization methods in avoiding overﬁtting, we need a
training set that does not consist entirely of clean data. We, therefore, took the popular CIFAR-10 dataset
(Krizhevsky & Hinton, 2009), which has 10classes and n= 50,000training data points, and considered
corrupting diﬀerent fractions of the data. In the ﬁrst scenario, we corrupted 25%of the data points, by
assigning them a random label. Since, for each of those images, there is a 9/10chance of being assigned a
wrong label, roughly 9/10×25% = 22.5%of the training data had incorrect labels. In the second scenario,
we randomly ﬂipped 10%of the labels, resulting in roughly 9%incorrect labels. For the sake of comparison,
in the third scenario, we considered the uncorrupted data set itself.
Network Architecture. We used a standard ResNet-18 (He et al., 2016) deep neural network, which is
commonly used for the CIFAR-10 dataset. The network has 18layers, and around 11million parameters.
Thus, it qualiﬁes as highly overparameterized. We did not make any changes to the network.
Algorithms. We use three diﬀerent algorithms for optimization/regularization.
1. Standard SGD (implicit regularization): First, we train the network with the standard (mini-batch)
SGD. While there is no explicit regularization, this is still known to induce an implicit regularization,
as discussed in Section 2.3.
2. Weight decay (explicit regularization): We next train the network with an /lscript2-norm regularization,
through weight decay. We ran weight decay with a wide range of regularization parameters, λ.
3. RMD (explicit regularization): Finally, we train the network with RMD, which is provably regu-
larizing with an /lscript2norm. For RMD we also ran the algorithm for a wide range of regularization
parameters, λ.
In all three cases, we train in mini batches—mini-batch RMD is summarized in Algorithm 2 in the Appendix.
6Under review as submission to TMLR
(a) RMD
 (b) Weight Decay
Figure 1: 25% corruption of the training set.
(a) RMD
 (b) Weight Decay
Figure 2: 10% corruption of the training set.
4.2 Results
The training and test accuracies for all three methods are given in Figs. 1-3. Fig. 1 shows the results when
the training data is corrupted by 25%, Fig. 2 when it is corrupted by 10%, and Fig. 3 when it is uncorrupted.
As expected, because the network is highly overparameterized, in all cases, SGD interpolates the training
data and achieves almost 100%training accuracy.
As seen in Fig. 1, at 25%data corruption SGD achieves 80%test accuracy. For RMD, as λvaries from 0.7 to
2.0, the training accuracy increases from 67%to82%(this increase is expected since RMD should interpolate
the training data as λ→∞). However, the test accuracy remains generally constant around 85%, with a
peak of 87%. This is signiﬁcantly better than the generalization performance of SGD. For weight decay, as
λincreases from 0.001 to 0.004, the training accuracy increases from 70%to98%(implying that there is no
need to increase λbeyond 0.004). The test accuracy, on the other hand, is rather erratic and varies from a
low of 67%to a peak of 80%.
As seen in Fig. 2, at 10%data corruption SGD achieves 87.5%test accuracy. For RMD, as λvaries from
0.7 to 2.0, the training accuracy increases from 82%to92.5%. The test accuracy remains generally constant
around and the peak of 88.5%is only marginally better than SGD. For weight decay, the training accuracy
increases from 86%to99%, while the test accuracy is erratic and peaks only at 80%.
Finally, for the sake of comparison, we show the results for the uncorrupted training data in Fig. 3. As
expected, since the data is uncorrupted, interpolating the data makes sense and SGD has the best test
7Under review as submission to TMLR
(a) RMD
(b) Weight Decay
Figure 3: Uncorrupted training set.
accuracy. Both RMD and weight decay approaches have higher test accuracy as λincreases, with RMD
having superior performance.
We should also mention that we have run experiments with 40%corruption in the data. Here SGD achieves
70%test accuracy, while RMD achieves a whopping 81.5%test accuracy with only 64%training accuracy.
See the Appendix for more details.
5 Regularization for Continual Learning
It is often desirable to regularize the weights to remainclose to a particular weight vector. This is particularly
useful for continual learning, where one seeks to learn a new task while trying not to “forget” the previous
task as much as possible (Lopez-Paz & Ranzato, 2017; Kirkpatrick et al., 2017; Farajtabar et al., 2020). In
this section, we show that our algorithm can be readily used for such settings by initializing w0to be the
desired weight vector and suitably choosing a notion of closeness.
Augmenting the loss function with a regularization term that promotes closeness to some desired weight
vectorwreg, one can pose the optimization problem as
min
wλn/summationdisplay
i=1Li(w) +/bardblw−wreg/bardbl2. (12)
Moregenerally,usingaBregmandivergence Dψ(·,·)correspondingtoadiﬀerentiablestrictly-convexpotential
functionψ:Rp→R, one can pose the problem as
min
wλn/summationdisplay
i=1Li(w) +Dψ(w,wreg). (13)
Note that Bregman divergence is deﬁned as Dψ(w,wreg) :=ψ(w)−ψ(wreg)−∇ψ(wreg)T(w−wreg), is
non-negative, and convex in its ﬁrst argument. Due to strict convexity of ψ, we also have Dψ(w,wreg) = 0
iﬀw=wreg. For the choice of ψ(w) =1
2/bardblw/bardbl2, for example, the Bregman divergence reduces to the usual
Euclidean distance Dψ(w,w 0) =1
2/bardblw−wreg/bardbl2.
Same as in Section 3, we can deﬁne an auxiliary variable z∈Rn, and rewrite the problem as
min
w,zλn/summationdisplay
i=1z2[i]
2+Dψ(w,wreg)
s.t.z[i] =/radicalbig
2Li(w), i= 1,...,n.(14)
8Under review as submission to TMLR
It can be easily shown that the objective of this optimization problem is a Bregman divergence, i.e.,
Dˆψ/parenleftbigg/bracketleftbigg
w
z/bracketrightbigg
,/bracketleftbigg
wreg
0/bracketrightbigg/parenrightbigg
, corresponding to a potential function ˆψ/parenleftbigg/bracketleftbigg
w
z/bracketrightbigg/parenrightbigg
=ψ(w) +λ
2/bardblz/bardbl2. As will be discussed
in Section 6, this is exactly in a form that an SMD algorithm with the choice of potential function ˆψ, initial-
izationw0=wregandz0= 0, and a suﬃciently small learning rate will solve. In other words, Algorithm 1
with initialization w0=wregprovably solves the regularized problem (13).
6 Convergence Guarantees
In this section, we provide convergence guarantees for RMD under certain assumptions, motivated by the
implicitregularizationpropertyofstochasticmirrordescent,recentlyestablishedinAzizan&Hassibi(2019b);
Azizan et al. (2021).
Let us denote the training dataset by {(xi,yi) :i= 1,...,n}, wherexi∈Rdare the inputs, and yi∈R
are the labels. The output of the model on data point iis denoted by a function fi(w) :=f(xi,w)of the
parameterw∈Rp. The loss on data point ican then be expressed as Li(w) =/lscript(yi−fi(w))with/lscript(·)being
convex and having a global minimum at zero (examples include square loss, Huber loss, etc.). Since we are
mainly concerned with highly overparameterized models (the interpolating regime), where p/greatermuchn, there are
(inﬁnitely) many parameter vectors wthat can perfectly ﬁt the training data points, and we can deﬁne
W={w∈Rp|fi(w) =yi, i= 1,...,n}
={w∈Rp|Li(w) = 0, i= 1,...,n}.
Letw∗∈Wdenote the interpolating solution that is closest to the initialization w0in Bregman divergence:
w∗= arg min
wDψ(w,w 0)
s.t.fi(w) =yi, i= 1,...,n.(15)
It has been shown that, for a linear model f(xi,w) =xT
iw, and for a suﬃciently small learning rate η >0,
the iterates of SMD (4) with potential function ψ(·), initialized at w0, converge to w∗(Azizan & Hassibi,
2019b).
When initialized at w0= arg minwψ(w)(which is the origin for all norms, for example), the convergence
point becomes the minimum-norm interpolating solution, i.e.,
w∗= arg min
wψ(w)
s.t.fi(w) =yi, i= 1,...,n.(16)
While for nonlinear models, the iterates of SMD do not necessarily converge to w∗, it has been shown that for
highly-overparameterized models, under certain conditions, this still holds in an approximate sense (Azizan
et al., 2021). In other words, the iterates converge to an interpolating solution w∞∈Wwhich is “close” to
w∗. More formally, the result from Azizan et al. (2021) along with its assumptions can be stated as follows.
Let us deﬁne DLi(w,w/prime) :=Li(w)−Li(w/prime)−∇Li(w/prime)T(w−w/prime),which is deﬁned in a similar way to a
Bregman divergence for the loss function. The diﬀerence though is that, unlike the potential function of
the Bregman divergence, due to the nonlinearity of fi(·), the loss function Li(·) =/lscript(yi−fi(·))need not be
convex (even when /lscript(·)is). Further, denote the Hessian of fibyHfi.3
Assumption 6.1. Denote the initial point by w0. There exists w∈ Wand a regionB={w/prime∈
Rp|Dψ(w,w/prime)≤/epsilon1}containingw0, such that DLi(w,w/prime)≥0,i= 1,...,n, for allw/prime∈B.
Assumption 6.2. Consider the region Bin Assumption 6.1. The fi(·)have bounded gradient and Hessian
on the convex hull of B, i.e.,/bardbl∇fi(w/prime)/bardbl≤γ, andα≤λmin(Hfi(w/prime))≤λmax(Hfi(w/prime))≤β,i= 1,...,n, for
allw/prime∈convB.
3We refrain from using ∇2fifor Hessian, which is typically used for Laplacian (divergence of the gradient).
9Under review as submission to TMLR
Theorem 6.3 (Azizan et al. (2021)) .Consider the set of interpolating solutions W={w∈Rp|f(xi,w) =
yi, i= 1,...,n}, the closest such solution w∗= arg minw∈WDψ(w,w 0), and the SMD iterates given in (4)
initialized at w0, where every data point is revisited after some steps. Under Assumptions 6.1 and 6.2, for
suﬃciently small step size, i.e., for any η >0for whichψ(·)−ηLi(·)is strictly convex on Bfor all i, the
following holds.
1. The iterates converge to w∞∈W.
2.Dψ(w∗,w∞) =o(/epsilon1).
In a nutshell, Assumption 6.1 states that the initial point w0is close to the set of global minima W, which
arguably comes for free in highly overparameterized settings (Allen-Zhu et al., 2019), while Assumption 6.2
states that the ﬁrst and second derivatives of the model are locallybounded. Motivated by the above result,
we now return to RMD and its corresponding optimization problem.
Let us deﬁne a learning problem over parameters/bracketleftbiggw
z/bracketrightbigg
∈Rp+nwith ˆfi/parenleftbigg/bracketleftbiggw
z/bracketrightbigg/parenrightbigg
=/radicalbig
2Li(w)−z[i],ˆyi= 0, and
ˆLi/parenleftbigg/bracketleftbiggw
z/bracketrightbigg/parenrightbigg
=ˆ/lscript/parenleftbigg
ˆyi−ˆfi/parenleftbigg/bracketleftbiggw
z/bracketrightbigg/parenrightbigg/parenrightbigg
=ˆ/lscript/parenleftBig
z[i]−/radicalbig
2Li(w)/parenrightBig
fori= 0,...,n. Note that in this new problem, we
now havep+nparameters and nconstraints/data points, and since p/greatermuchn, we havep+n/greatermuchn, and we are
still in the highly-overparameterized regime (even more so). Thus, we can also deﬁne the set of interpolating
solutions for the new problem as
ˆW=/braceleftbigg/bracketleftbigg
w
z/bracketrightbigg
∈Rp+n/vextendsingle/vextendsingle/vextendsingle/vextendsingleˆfi/parenleftbigg/bracketleftbigg
w
z/bracketrightbigg/parenrightbigg
= ˆyi, i= 1,...,n/bracerightbigg
. (17)
Let us deﬁne a potential function ˆψ/parenleftbigg/bracketleftbiggw
z/bracketrightbigg/parenrightbigg
=ψ(w) +λ
2/bardblz/bardbl2and a corresponding SMD
∇ˆψ/parenleftbigg/bracketleftbiggwt
zt/bracketrightbigg/parenrightbigg
=∇ˆψ/parenleftbigg/bracketleftbiggwt−1
zt−1/bracketrightbigg/parenrightbigg
−η∇ˆLi/parenleftbigg/bracketleftbiggwt−1
zt−1/bracketrightbigg/parenrightbigg
,
initialized at/bracketleftbiggw0
0/bracketrightbigg
. It is straightforward to verify that this update rule is equivalent to that of RMD, i.e.,
(8).
On the other hand, from (15), we have
ˆw∗= arg min
w,zDˆψ/parenleftbigg/bracketleftbiggw
z/bracketrightbigg
,/bracketleftbiggw0
0/bracketrightbigg/parenrightbigg
s.t. ˆfi/parenleftbigg/bracketleftbiggw
z/bracketrightbigg/parenrightbigg
= ˆyi, i= 1,...,n.(18)
PluggingDˆψ/parenleftbigg/bracketleftbiggw
z/bracketrightbigg
,/bracketleftbiggw0
0/bracketrightbigg/parenrightbigg
=Dψ(w,w 0) +λ
2/bardblz/bardbl2and ˆfi/parenleftbigg/bracketleftbiggw
z/bracketrightbigg/parenrightbigg
=/radicalbig
2Li(w)−z[i]into (18), it is easy to see
that it is equivalent to (14) for w0=wreg, and equivalent to (6) for w0= 0. The formal statement of the
theorem follows from a direct application of Theorem 6.3.
Assumption 6.4. Denote the initial point by/bracketleftbigg
w0
0/bracketrightbigg
. There exists/bracketleftbigg
w
z/bracketrightbigg
∈ˆWand a region ˆB=
/braceleftbigg/bracketleftbiggw/prime
z/prime/bracketrightbigg
∈Rp+n|Dˆψ/parenleftbigg/bracketleftbiggw
z/bracketrightbigg
,/bracketleftbiggw/prime
z/prime/bracketrightbigg/parenrightbigg
≤/epsilon1/bracerightbigg
containing/bracketleftbiggw0
0/bracketrightbigg
, such that DˆLi/parenleftbigg/bracketleftbiggw
z/bracketrightbigg
,/bracketleftbiggw/prime
z/prime/bracketrightbigg/parenrightbigg
≥0,i= 1,...,n, for
all/bracketleftbiggw/prime
z/prime/bracketrightbigg
∈ˆB.
10Under review as submission to TMLR
Assumption 6.5. Consider the region ˆBin Assumption 6.4. The ˆfi(·)have bounded gradient and Hessian
on the convex hull of ˆB, i.e.,/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇ˆfi/parenleftbigg/bracketleftbiggw/prime
z/prime/bracketrightbigg/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤γ, andα≤λmin/parenleftbigg
Hˆfi/parenleftbigg/bracketleftbiggw/prime
z/prime/bracketrightbigg/parenrightbigg/parenrightbigg
≤λmax/parenleftbigg
Hˆfi/parenleftbigg/bracketleftbiggw/prime
z/prime/bracketrightbigg/parenrightbigg/parenrightbigg
≤
β,i= 1,...,n, for all/bracketleftbiggw/prime
z/prime/bracketrightbigg
∈conv ˆB.
Theorem 6.6. Consider the set of interpolating solutions ˆWdeﬁned in (17), the closest such solution ˆw∗
deﬁned in (18), and the RMD iterates given in (8)initialized at/bracketleftbiggw0
0/bracketrightbigg
, where every data point is revisited
after some steps. Under Assumptions 6.4 and 6.5, for suﬃciently small step size, i.e., for any η > 0for
which ˆψ(·)−ηˆLi(·)is strictly convex on ˆBfor all i, the following holds.
1. The iterates converge to/bracketleftbiggw∞
z∞/bracketrightbigg
∈ˆW.
2.Dˆψ/parenleftbigg
ˆw∗,/bracketleftbiggw∞
z∞/bracketrightbigg/parenrightbigg
=o(/epsilon1).
Despite its somewhat complicated look, similar as in Assumption 6.1, Assumption 6.4 states the initial
point/bracketleftbiggw0
0/bracketrightbigg
is close to the (new) (p+n)-dimensional manifold ˆW, which is reasonable because the new
problem is even more overparameterized than the original p-dimensional one. Similar as in Assumption 6.2,
Assumption 6.5 requires the ﬁrst and second derivatives of the model to be locally bounded.
We should emphasize that while Theorem 6.6 states that we converge to the manifold ˆW, it does notmean
that it is ﬁtting the training data points or achieving zero training error. That is because ˆW∈Rp+nis a
diﬀerent (much higher-dimensional) manifold than W∈Rp, and interpolating it would translate to ﬁtting
the constraints deﬁned by the regularized problem.
7 Conclusion and Outlook
We presented Regularizer Mirror Descent (RMD), a novel eﬃcient algorithm for training DNN with any de-
sired strictly-convex regularizer. The starting point for RMD is a standard cost which is the sum of the train-
ing loss and a diﬀerentiable strictly-convex regularizer of the network weights. For highly-overparameterized
models, RMD provably converges to a point “close” to the minimizer of this cost. The algorithm can be
readily applied to any DNN and enjoys the same parallelization properties as SGD. We demonstrated that
RMD is remarkably robust to various levels of label corruption in data, and it outperforms both the implicit
regularization induced by SGD and the explicit regularization performed via weight decay, by a wide mar-
gin. We further showed that RMD can be used for continual learning, where regularization with respect to
a previously-learned weight vector is critical.
Given that RMD enables training any network eﬃciently with a desired regularizer, it opens up several
new avenues for future research. In particular, an extensive experimental study of the eﬀect of diﬀerent
regularizers on diﬀerent datasets and diﬀerent architectures would be instrumental to uncovering the role of
regularization in modern learning problems.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Proceedings of the 36th International Conference on Machine Learning . PMLR,
2019.
Navid Azizan and Babak Hassibi. A characterization of stochastic mirror descent algorithms and their
convergence properties. In IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP) , 2019a.
11Under review as submission to TMLR
Navid Azizan and Babak Hassibi. Stochastic gradient/mirror descent: Minimax optimality and implicit
regularization. In International Conference on Learning Representations (ICLR) , 2019b.
Navid Azizan and Babak Hassibi. A stochastic interpretation of stochastic mirror descent: Risk-sensitive
optimality. In 2019 58th IEEE Conference on Decision and Control (CDC) , pp. 3960–3965, 2019c.
Navid Azizan, Sahin Lale, and Babak Hassibi. Stochastic mirror descent on overparameterized nonlinear
models.IEEE Transactions on Neural Networks and Learning Systems , 2021. doi: 10.1109/TNNLS.2021.
3087480.
PeterLBartlett, PhilipMLong, GáborLugosi, andAlexanderTsigler. Benignoverﬁttinginlinearregression.
Proceedings of the National Academy of Sciences , 117(48):30063–30070, 2020.
Peter L Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint. arXiv
preprint arXiv:2103.09177 , 2021.
Mikhail Belkin, Daniel J Hsu, and Partha Mitra. Overﬁtting or perfect ﬁtting? risk bounds for classiﬁcation
and regression rules that interpolate. In Advances in Neural Information Processing Systems , volume 31,
2018.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice
and the classical bias–variance trade-oﬀ. Proceedings of the National Academy of Sciences , 116(32):15849–
15854, 2019.
Nicholas M Boﬃ and Jean-Jacques E Slotine. Implicit regularization and momentum algorithms in nonlin-
early parameterized adaptive control and prediction. Neural Computation , 33(3):590–673, 2021.
Lev M. Bregman. The relaxation method of ﬁnding the common point of convex sets and its application to
the solution of problems in convex programming. USSR Computational Mathematics and Mathematical
Physics, 7(3):200–217, 1967.
Rich Caruana, Steve Lawrence, and Lee Giles. Overﬁtting in neural nets: Backpropagation, conjugate
gradient, and early stopping. Advances in neural information processing systems , pp. 402–408, 2001.
Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems , volume 375.
Springer Science & Business Media, 1996.
Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for continual
learning. In International Conference on Artiﬁcial Intelligence and Statistics , pp. 3762–3773. PMLR,
2020.
Ian Goodfellow, Y Bengio, and A Courville. Regularization for deep learning. Deep learning , pp. 216–261,
2016.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of
optimization geometry. In International Conference on Machine Learning , pp. 1827–1836, 2018a.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear
convolutional networks. In Advances in Neural Information Processing Systems , volume 31, 2018b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Geoﬀrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Im-
proving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 ,
2012.
Wei Hu, Zhiyuan Li, and Dingli Yu. Simple and eﬀective regularization methods for training on noisily
labeled data with generalization guarantee. In International Conference on Learning Representations ,
2019.
12Under review as submission to TMLR
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic
forgetting in neural networks. Proceedings of the national academy of sciences , 114(13):3521–3526, 2017.
Alex Krizhevsky and Geoﬀrey Hinton. Learning multiple layers of features from tiny images. Technical
report, Citeseer, 2009.
Jan Kukačka, Vladimir Golkov, and Daniel Cremers. Regularization for deep learning: A taxonomy. arXiv
preprint arXiv:1710.10686 , 2017.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably
robust to label noise for overparameterized neural networks. In International conference on artiﬁcial
intelligence and statistics , pp. 4313–4324. PMLR, 2020.
David Lopez-Paz and Marc /quotesingle.ts1Aurelio Ranzato. Gradient episodic memory for continual learning. In Advances
in Neural Information Processing Systems , volume 30, 2017.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In
International Conference on Learning Representations , 2019.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the eﬀectiveness
of SGD in modern over-parametrized learning. In Proceedings of the 35th International Conference on
Machine Learning , volume 80, pp. 3325–3334. PMLR, 2018.
Cesare Molinari, Mathurin Massias, Lorenzo Rosasco, and Silvia Villa. Iterative regularization for convex
regularizers. In International Conference on Artiﬁcial Intelligence and Statistics , pp. 1684–1692. PMLR,
2021.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and
Experiment , 2021(12):124003, 2021.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of
implicit regularization in deep learning. In ICLR (Workshop) , 2015. URL http://arxiv.org/abs/1412.
6614.
Tomaso Poggio, Andrzej Banburski, and Qianli Liao. Theoretical issues in deep networks. Proceedings of
the National Academy of Sciences , 117(48):30039–30045, 2020.
Lutz Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade , pp. 55–69. Springer,
1998.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics , pp. 400–407, 1951.
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning.
Constructive Approximation , 26(2):289–315, 2007.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. arXiv preprint arXiv:1611.03530 , 2016.
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay regular-
ization. In International Conference on Learning Representations , 2018a.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
minimization. In International Conference on Learning Representations , 2018b.
13