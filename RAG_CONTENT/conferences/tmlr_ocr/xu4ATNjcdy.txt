Under review as submission to TMLR
Variational Stochastic Gradient Descent for Deep Neural
Networks
Anonymous authors
Paper under double-blind review
Abstract
Optimizing deep neural networks is one of the main tasks in successful deep learning. Current
state-of-the-art optimizers are adaptive gradient-based optimization methods such as Adam.
Recently, there has been an increasing interest in formulating gradient-based optimizers in a
probabilistic framework for better estimation of gradients and modeling uncertainties. Here,
we propose to combine both approaches, resulting in the Variational Stochastic Gradient
Descent ( VSGD) optimizer. We model gradient updates as a probabilistic model and utilize
stochastic variational inference ( SVI) to derive an efficient and effective update rule. Further,
we show how our VSGDmethod relates to other adaptive gradient-based optimizers like
Adam. Lastly, we carry out experiments on two image classification datasets and four deep
neural network architectures, where we show that VSGDoutperforms AdamandSGD.
1 Introduction
The driving force for deep learning success is efficient and effective optimization (Bottou, 2012; Sun, 2020).
Deep Neural Networks (DNNs) introduce multiple optimization challenges due to their complexity, size, and
loss landscape with multiple local minima, plateaus, and saddle points. Since the first attempt to train DNNs
with stochastic gradient descent ( SGD), there have been multiple approaches to speed up the learning process
and improve the final performance of DNNs.
Nowadays, adaptive gradient-based methods are the leading optimizers in deep learning. The first advance-
ments arrived with the idea of adding momentum to the SGDupdate rule, resulting in SGDwith momentum
(SGDM) (Sutskever et al., 2013). The next major breakthrough was Adam(Kingma & Ba, 2015), which
uses the first and second momenta to adapt the learning rate and gradients. Currently, Adamis probably
the most widely used optimizer in deep learning due to its relative insensitivity to hyperparameter values,
and much faster initial progress in training compared to SGD(Sun, 2020).
Recently, there has been an increasing interest in formulating SGDin a probabilistic framework. One example
is (Mandt et al., 2017) in which SGDis used as an approximate Bayesian inference algorithm analyzed by
relating it to the Ornstein-Uhlbeck process. A different approach utilizes a Bayesian perspective on SGDto
model the uncertainty for datastreams (Liu et al., 2024).
In this paper, we propose to combine these two approaches, and, as a result, we obtain a probabilistic
framework for adaptive gradient-based optimizers. First, we formulate a probabilistic model of SGD. Second,
we employ stochastic variational inference SVI(Hoffman et al., 2013) to derive an efficient adaptive gradient-
based update rule. We refer to our approach as Variational Stochastic Gradient Descent ( VSGD). We apply
our method to optimize overparameterized DNNs on image classification. Compared to AdamandSGD, we
obtain very promising results.
Our contributions can be summarized as follows:
1.We propose Variational Stochastic Gradient Descent ( VSGD), a novel optimizer that adopts a
probabilistic approach. In VSGD, we model the true gradient and the noisy gradient as latent and
observed random variables, respectively, within a probabilistic model. Additionally, this unique
1Under review as submission to TMLR
perspective allows us to manage gradient noise more effectively by adopting distinct noise models for
the true and noisy gradients, enhancing the optimizer’s robustness and efficiency.
2.We draw connections between VSGDand several established non-probabilistic optimizers, including
Normalized-SGD ,Adam, and SGDM. Our analysis reveals that many of these methods can be
viewed as specific instances or simplified adaptations of VSGDwhen a constant noise model is
assumed for both the true and observed gradients.
3.Lastly, we carry out an empirical evaluation of VSGDby comparing its performance against the
most popular optimizers, namely AdamandSGDin the context of large-scale image classification
tasks using overparameterized deep neural networks. Our results indicate that VSGDnot only
achieves lower generalization errors, but also converges at a competative rate compared to traditional
methods such as AdamandSGD. We believe that these findings underscore the practical advantages
ofVSGD, making it a compelling choice for complex deep learning challenges.
2 Background knowledge
Training Neural Nets with Gradient Descent Let us consider a supervised setup for deep neural
networks (DNNs) in which a DNN with weights θ∈RDpredicts a target variable y∈Yfor a given object
x∈X,fθ:X→Y. Here, we treat θas a vector to keep the notation uncluttered. Theoretically, finding best
values ofθcan be achieved by risk minimization :R(θ) =Ep(x,y)[ℓ(fθ(x),y)], whereℓis a loss function. In
practice, we do not have access to p(x,y), but we are given a dataset D. Then, training of fθcorresponds to
empirical risk minimization (Bottou, 2012; Sun, 2020): L(θ;D) =1
N/summationtextN
n=1ℓ(fθ(xn),yn). We assume that
the loss function is k-differentiable, ℓ∈Ck, at leastk= 1.
Many optimization algorithms for DNNs are based on gradient descent (GD). Due to computational
restrictions on calculating the full gradient (that is, over all training datapoints) for a DNN, the stochastic
version of GD,stochastic gradient descent (SGD), is employed. SGDresults in the following update rule:
θt=θt−1−ηtˆgt, (1)
whereηtis a learning rate, ˆgt=∇θL(θ;M)is a noisy version of the full gradient calculated over a mini-batch
ofMdata points,L(θ;M) =1
|M|/summationtext
n∈Mℓ(fθ(xn),yn), and we assume that E[ˆgt] =gt.
Related work Optimization of DNNs is a crucial component in deep learning research and, especially, in
practical applications. In fact, the success of deep learning models is greatly dependent on the effectiveness
of optimizers. The loss landscapes of DNNs typically suffer from multiple local minima, plateaus, and saddle
points (Du & Lee, 2018), making the optimization process challenging.
Since the introduction of backpropagation (Rumelhart et al., 1986; Schmidhuber, 2015), which is based on
SGDupdates, multiple techniques have been proposed to improve SGD. Some improvements correspond to
a better initialization of layers that results in a warm start of an optimizer (e.g., (Glorot & Bengio, 2010; He
et al., 2015; Sutskever et al., 2013)), or pre-defined learning schedules (e.g., (Loshchilov & Hutter, 2016)).
Another active and important line of research focuses on adaptive gradient-based optimizers. Multiple methods
are proposed to calculate adaptive learning rates for DNN, such as RMSprop (Graves, 2013; Tieleman &
Hinton, 2012), AdaDelta (Zeiler, 2012), and Adam(Kingma & Ba, 2015). Adamis probably the most
popular optimizer due to its relative insensitivity to hyperparameter values and rapid initial progress during
training (Keskar & Socher, 2017; Sun, 2020). However, it was pointed out that Adammay not converge
(Reddi et al., 2018), leading to an improvement of Adamcalled ADMSGrad (Reddi et al., 2018). There
are multiple attempts to understand Adamthat lead to new theoretical results and new variants of Adam
(e.g., (Barakat & Bianchi, 2019; Zou et al., 2019)). In this paper, we propose a new probabilistic framework
that treats gradients as random variables and utilizes stochastic variational inference ( SVI) (Hoffman et al.,
2013) to derive an estimate of a gradient. We indicate that our method is closely related to Adamand is an
adaptive gradient-based optimizer.
2Under review as submission to TMLR
... ...
... ...
Figure 1: A probabilistic graphical model of our proposed VSGD optimizer. White circles represent latent
variables, gray circles correspond to observable variables, filled small black circles denote control variables.
A different line of research focuses on 2nd-order optimization methods that require the calculation of Hessian
matrices. In the case of deep learning, it is infeasible to calculate Hessian matrices for all layers. Therefore,
many researchers focus on approximated 2nd-order optimizers. One of the most popular methods is KFAC
(Martens & Grosse, 2015). Recently, there have been new developments in this direction (e.g., (Eschenhagen
et al., 2023; Lin et al., 2021; 2023)). However, extending K-FAC introduces memory overhead that could
be prohibitive, and numerical instabilities might arise in low-precision settings due to the need for matrix
inversions or decompositions. Here, we focus on the 1st-order optimizer. However, we see great potential for
extending our framework to the 2nd-order optimization.
There are various attempts to treat SGDas a probabilistic framework. For instance, Mandt et al. (2017)
presented an interesting perspective on how SGDcan be used for approximate Bayesian posterior inference.
Moreover, they showed how SGDrelates to a stochastic differential equation (SDE), specifically, the Ornstein-
Uhlbeck process. The idea of perceiving SGDas an SDE was further discussed in, e.g., (Yokoi & Sato,
2019) with interesting connections to stochastic gradient Langevin dynamics (Welling & Teh, 2011). Another
perspective was on the phrasing SGDas a Bayesian method either for sampling (Mingard et al., 2021),
modeling parameter uncertainty for streaming data (Liu et al., 2024) or generalization (Smith & Le, 2017).
A different approach uses linear Gaussian models to formulate the generation process of the noisy gradients
and infer the hidden true gradient with Kalman filter or Bayesian filter, e.g., (Bittner & Pronzato, 2004;
Vuckovic, 2018; Yang, 2021). In this paper, we propose to model dependencies between truegradients and
noisygradients through the Bayesian perspective using a novel framework. Our framework treats both
quantities as random variables and allows efficient inference and incorporation of prior knowledge into the
optimization process. A similar line of work is the natural gradient-based methods for Bayesian Inference
where the optimizer uses natural gradients instead of Euclidean gradients (Khan et al., 2018; Osawa et al.,
2019). Our work can be considered orthogonal to this line of work, as they apply Adamto a Bayesian
model for learning the variational parameters. VSGD, on the other hand, assumes a Bayesian model for the
gradients themselves and may not necessarily be applied to a Bayesian inference problem.
3 Methodology
3.1 Our approach: VSGD
A probabilistic model of SGD There are several known issues with applying SGDto the training of
DNNs (Bottou, 2012; Sun, 2020), such as slow convergence due to gradient explosion/vanishing or too noisy
gradient, poor initialization of weights, and the need for a hyperparameter search for each architecture and
dataset. Here, we would like to focus on finding a better estimate of the truegradient. To achieve that, we
propose to perceive SGDthrough a Bayesian perspective, in which we treat noisy gradients as observed
variables and want to infer the posterior distributions over true gradients. Subsequently, we can then use the
expected value of the posterior distribution over the truegradientgtand plug it into the SGDupdate rule
(Eq. 1). In the following, we present how we can achieve that.
3Under review as submission to TMLR
In our considerations, we treat all quantities as random variables. Hence, the truegradientgtis a latent
variable, and we observe the noisygradient ˆgt. We propose to model gtandˆgtusing Gaussian distributions
with precision variables wgandwˆg, representing the information of systematic and observation noises,
respectively. Here, we use "observation noise" to indicate the sampling noise of the observed gradient and
"systematic noise" to indicate the introduced error of trying to approximate the real, local area of the gradient
surface with a simplified model. Both precision variables are treated as Gamma-distributed latent variables.
The advantage of treating precision variables as latent variables instead of hyperparameters is that this way,
we can incorporate prior knowledge over the systematic and observation noise as well as having uncertainty
over them.
We also introduce a control variate utto serve as the prior mean for gtand is essential for our approach. We
defineut=h(ˆg1:t−1)as a deterministic message passed from previously observed noisy gradients to time t.
Later (Eq. 7), we will discuss the specific formulation of the function h(·). In the case of ˆgt, we propose using
gtas the mean.
We assume that the gradients across each dimension are independent by modeling them independently. This
approach is adopted for computational efficiency. Moving forward, unless specified otherwise, all symbols
should be considered as scalars.
For the maximum number of iterations T, the joint distribution is then defined as follows (Figure 1):
p(wg,wˆg,g1:T,ˆg1:T;u1:T) =p(wg)p(wˆg)T/productdisplay
t=1p(gt|wg;ut)p(ˆgt|gt,wˆg), (2)
with the following distributions:
p(gt|wg;ut) =N(ut,w−1
g), (3)
p(ˆgt|gt,wˆg) =N(gt,w−1
ˆg), (4)
p(wg) = Γ(γ,γ), (5)
p(wˆg) = Γ(γ,Kgγ), (6)
where the Gamma distributions Γ(·)are parameterized by shape and rate. Kgis a hyperparameter designed
to reflect our prior belief about the variance ratio between ˆgiandgi, i.e.,Kg=E/bracketleftig
w−1
ˆg/bracketrightig
/E/bracketleftbig
w−1
g/bracketrightbig
. A
recommended value for Kgis greater than 1, e.g.Kg= 30, suggesting that the majority of observed variance
stems from observation noise w−1
ˆgrather than systematic noise w−1
g.γis a shared hyperparameter that
indicates the strength of the prior, essentially equating the prior knowledge with γobservations. A smaller
value forγ, such as 1e−8, is advised to represent our prior ignorance. The remaining key question is the
choice of the control variate utas a way to carry information. In this paper, we aggregate the information
overtby setting the prior mean for the current gtas the posterior mean of the previous ones:
ut=Ep(gt|ˆgt−1;ut−1)[gt], (7)
whereu0could be set to 0.
An alternative approach to using utto summarize the knowledge before tis to instead let gtexplicitly depend
ong1:t−1. However, incorporating the dependency of gtong1:t−1explicitly would lead to a non-scalable,
fully connected graphical model. A common alternative solution is to adopt a Markov structure, in which
gtdepends only on the most recent Lobservations gt−L:t−1. This would align the model with frameworks
similar to Kalman filters or Bayesian filters (Kalman, 1960). However, these models either require known
precision terms beforehand, which is often impractical or necessitate online learning of precisions through
computationally intensive methods such as nested loops in variational inference or Monte Carlo methods,
rendering them non-scalable for training deep neural networks. By introducing utand treating it as an
additional observation, we can utilize SVIto avoid nested loops and still guarantee convergence to the
estimated precisions (if any). As demonstrated in this paper, this strategy proves to be both scalable and
efficient.
4Under review as submission to TMLR
Stochastic Variational Inference Since the model defined by Eqs. 3-6 requires calculating intractable
integral over latent variables, to find a good estimate of gt, we aim to approximate the distribution
p(wg,wˆg,g1:T,ˆg1:T;u1:T)with a variational distribution q(wg,wˆg,g1:T;τ,ϕ1:T), parameterized by the global
and local variational parameters τand{ϕt}T
t=1respectively. We achieve this by maximizing the evidence
lower-bound (ELBO) defined by pandq
arg max
τ,ϕ1:TEq/bracketleftbigg
logp(wg,wˆg,g1:T,ˆg1:T;u1:T)
q(wg,wˆg,g1:T;τ,ϕ1:T)/bracketrightbigg
. (8)
We will demonstrate that this objective can be achieved concurrently with the original optimization objective
θ∗=argmaxθL(θ;D), allowing each SGDiteration of θto serve simultaneously as an SVI iteration for Eq. 8.
Following SVI(Hoffman et al., 2013), we employ the mean-field approach, assuming the variational posterior
is factorized as follows:
q(wg,wˆg,g1:T) =q(wg)q(wˆg)T/productdisplay
t=1q(gt). (9)
We define the marginal variational distributions as:
q(wg) = Γ(ag,bg), (10)
q(wˆg) = Γ(aˆg,bˆg), (11)
q(gt) =N(µt,g,σ2
t,g), (12)
fort= 1 :T, where the global and local variational parameters are τ= (ag,aˆg,bg,bˆg)Tandϕt= (µt,g,σ2
t,g)T,
respectively.
We now outline the complete SVIupdate process to meet the objective set in Eq. 8. For detailed derivations,
please see Appendix A.1. Let τt−1= (at−1,g,at−1,ˆg,bt−1,g,bt−1,ˆg)Tandϕt−1= (µt−1,g,σ2
t−1,g)Trepresent
the global and local variational parameters updated just before the t-thSVIiteration, and we set the control
variate:
ut=µt−1,g, (13)
as the variational version of utas in Eq. 7, and let u0= 0. At iteration t, we start by randomly drawing a
noisy gradient ˆgtwith the sampling algorithm of your choice (e.g. using a mini-batch of samples). Then,
conditioned on ˆgt,τt−1, andut, we update the local parameter ϕtas follows:
µt,g=bt−1,ˆg
bt−1,ˆg+bt−1gµt−1,g+bt−1,g
bt−1,ˆg+bt−1gˆgt, (14)
σ2
t,g=/parenleftigg
at−1,g
bt−1,g+at−1,ˆg
bt−1,ˆg/parenrightigg−1
. (15)
Note thatµt,grepresents a dynamic weighted average of the previous estimate µt−1,gand the noisy observation
ˆgt. Weights are dynamically determined based on the learned relative precision between Eq. 3 and Eq. 4.
µt,gthen acts as a control variate for the subsequent iteration. Recall thatat,g
bt,g=Eqt[wg]. Therefore, the
variance parameter σ2
t,gis equal to the inverse sum of the two precisions characterizing the systematic and
observation noise. For an alternative kernel smoothing view on Eq. 14, please refer to Appendix A.2.
Given local parameters ϕt, update equations for the global parameters τthave the form:
at,g=at,ˆg=γ+ 0.5, (16)
b′
t,g=γ+ 0.5/parenleftbig
σ2
t,g+ (µt,g−µt−1,g)2/parenrightbig
, (17)
b′
t,ˆg=Kgγ+ 0.5/parenleftbig
σ2
t,g+ (µt,g−ˆgt)2/parenrightbig
. (18)
5Under review as submission to TMLR
Algorithm 1 VSGD
Input: SVIlearning rate parameter {κ1,κ2}, learning rate η, prior strength γ, prior variance ratio Kg.
Initialize:
θ0,a0,g=γ;a0,ˆg=γ;b0,g=γ;b0,ˆg=Kgγ;µ0,g= 0
fort= 1toTdo
Compute ˆgtforL(θ;·)
ρt,1=t−κ1
ρt,2=t−κ2
Updateσ2
t,g,µt,g ▷Eq. 14, 15
Updateat,g,at,ˆg ▷Eq. 16
Updatebt,g,bt,ˆg ▷Eq. 19,20
Updateθt ▷Eq. 23
end for
In Eq. 16,at,gandat,ˆgremain constant throughout the iterations and depend purely on the prior strength
γ. For the sake of consistency, they are included in the update equations. Thus, the posterior belief of the
precision parameters wgandwˆgare determined purely by the rate parameters ( b′
t,g,b′
t,ˆg). Intuitively, b′
t,g
andb′
t,ˆgcharacterize the amount of systematic noise and observation noise accordingly. In both cases, the
prior value and the estimated variance of the true gradient additively contribute to the estimation. The main
difference between the two comes in the third term, which depends on how much the gradient mean changed
in time for the systematic noise and the difference between the observed gradient and the mean estimate for
the observation noise.
Finally, the SVI updates for the global parameters take the form:
bt,g= (1−ρt,1(t))bt−1,g+ρt,1(t)b′
t,g, (19)
bt,ˆg= (1−ρt,2(t))bt−1,ˆg+ρt,2(t)b′
t,ˆg. (20)
whereρt,1(t)andρt,2(t)denote the learning rates of SVIacross iterations t. A typical selection for ρcould
be as follows:
ρt,1(t) =t−κ1, (21)
ρt,2(t) =t−κ2. (22)
In this formulation, κ1andκ2are hyperparameters that influence the behavior of the SVI learning rates.
We typically limit κ1andκ2in the range (0.5,1]to letρt,1andρt,2meet the Robbins–Monro conditions
(Robbins & Monro, 1951), and we select κ2>κ1to allow for a slower or larger decaying learning rate for wg.
For example, setting κ1= 0.8andκ2= 0.9allowswgto adapt more rapidly to new developments compared
towˆg.
Note that when using a mini-batch of samples, ˆgtis calculated as the average of the sampled gradients in the
batch. While in VSGDit is possible to treat the samples in the mini-batch separately, see Appendix A.3 for
details.
Having concluded the t-thSVIiteration, we then incorporate the local parameter ϕtto finalize the t-thSGD
iteration:
θt=θt−1−η/radicalig
µ2
t,g+σ2
t,gµt,g, (23)
where/radicalig
µ2
t,g+σ2
t,g=/radicalbig
E[g2
t]represents an estimation of the local Lipschitz constant. A summary of the
steps of the VSGDalgorithm can be seen in Algorithm 1.
6Under review as submission to TMLR
3.2 VSGD ,Constant VSGD and their connections to other gradient-based deep learning optimizers
Our probabilistic framework offers a way of estimating gradients. A natural question to raise here is whether
this framework is related to other 1storder optimizers used in deep learning. Similarly to well-known methods
likeAdam,VSGDmaintains a cache of the first and second momenta of the gradient, calculated as weighted
moving averages. However, unlike other methods, where these variables remain constant, VSGDadjusts its
weights across iterations.
VSGD and Normalized-SGD First, we can show how our VSGDmethod relates to a version of SGD
called Normalized-SGD (Murray et al., 2019) that updates the gradients as follows:
θt=θt−1−η/radicalbig
ˆg2
tˆgt. (24)
If we takeγ→∞,Kg→0, such that limγ→∞Kg→0γKg= 0, then VSGDis approximately equivalent to
Normalized-SGD (Murray et al., 2019). We can show it by taking b0,g=γ,b0,ˆg=Kgγ. Then, for γ→∞
andKg→0, Eq. 14 yields the following:
µ1,g= 0·Kgγ
Kgγ+γ+ ˆg1γ
Kgγ+γ≈ˆg1. (25)
In a subsequent iteration, as γdominates all other values, we have: at,g≈γ,b′
t,g≈γ,b′
t,ˆg≈Kgγ. As a
result,bt,g≈γ,bt,ˆg≈Kgγ. By induction, the variational parameter for gtat iteration tis the following:
σ2
t,g=Kgγ2
γ(Kgγ+γ)=Kg
Kg+ 1≈0, (26)
µt,g=µt−1,g·Kgγ
Kgγ+γ+ ˆgtγ
Kgγ+γ≈ˆgt. (27)
As a result, the update rule for θis approximately the update rule of Normalize-SGD , namely:
θt≈θt−1−η/radicalbig
ˆg2
tˆgt. (28)
Constant VSGD Before we look at further connections between our VSGDand other optimizers, we
first introduce a simplified version of VSGDwith a constant variance ratio shared between gtandˆgt. This
results in the following model:
p(ω,g1:T,ˆg1:T;u1:T) =p(ω)T/productdisplay
t=1p(gt|ω;ut)p(ˆgt|gt,ω), (29)
with the following distributions:
p(gt|ω;ut) =N(ut,K−1
gω−1), (30)
p(ˆgt|gt,ω) =N(gt,ω−1), (31)
p(ω) = Γ(γ,γ). (32)
This approach is tantamount to imposing a strong prior on Eq. 2, dictating that the observation noise is the
systematic noise scaled by Kg. This strong prior affects only the ratio between systematic and observation
noises. Regarding the prior knowledge of the observation noise ωitself, we can maintain an acknowledgment
of our ignorance by setting γto a small value such as 10−8.
Given its characteristic of maintaining a constant variance ratio, we refer to the algorithm as Constant
VSGD. Since the derivation of Constant VSGD aligns with that of other variants of VSGD, we omit the
detailed derivation here and directly present the update equations.
7Under review as submission to TMLR
Algorithm 2 Constant VSGD
Input:SVI learning rate parameter κ, SGD learning rate η, prior strength γ, variance ratio Kg.
Initialize:
θ0,a0,ˆg=γ;b0,ˆg=γ;µ0,g= 0
fort= 1toTdo
Draw ˆgt
ρt=t−κ
Updateσ2
t,g,µt,g ▷Eq. 33, 34
Updateat,ˆg,bt,ˆg ▷Eq. 35,36
Updateθt ▷Eq. 37
end for
µt,g=µt−1,gKg
Kg+ 1+ ˆgt1
Kg+ 1, (33)
σ2
t,g=1
Kg+ 1bt−1,ˆg
at−1,ˆg, (34)
at,ˆg=γ+ 1, (35)
bt,ˆg=(1−ρt)bt−1,ˆg+ρt/bracketleftig
γ+ 0.5/parenleftbig
σ2
t,g+ (µt,g−ˆgt)2/parenrightbig
+ 0.5Kg/parenleftbig
σ2
t,g+ (µt,g−µt−1,g)2/parenrightbig/bracketrightig
,(36)
θt=θt−1−η/radicalig
µ2
t,g+σ2
t,gµt,g. (37)
The complete Constant VSGD algorithm is outlined in Algorithm 2, the updates are notably simpler
compared to VSGD, as outlined in Algorithm 1, yet they still offer sufficient flexibility to estimate the
uncertainties of the system. Summarizing the discussion above, we enumerate the key distinctions between
Constant VSGD andVSGDas follows:
1.InVSGD, two global hidden variables, wgandwˆg, are employed to represent systematic and
observation noise, respectively. On the contrary, Constant VSGD utilizesωfor observation noise
and constrains the systematic noise to a fraction1
Kgof the observation noise. Due to these reduced
degrees of freedom, all the update equations in Constant VSGD are simplified.
2.In the first momentum update Eq. 14 of VSGD, the weights are determined by the ratio between
systematic and observation noise. However, in Eq. 33 of Constant VSGD , these weights are fixed
at{Kg
Kg+1,1
Kg+1}, facilitating a direct comparison with other optimizers such as Adam.
To sum up, now we know how to simplify VSGDtoConstant VSGD . In the following, we will indicate how
Constant VSGD is connected to other optimizers, namely: Adam,SGDwith momentum, and AMSGrad .
3.2.1 Constant VSGD andAdam
First, we compare Constant VSGD with one of the strongest and most popular optimizers for DNNs,
Adam(Kingma & Ba, 2015). At the t-th iteration, Adamupdates the first two momenta of the gradient
through the following procedure:
mt=β1mt−1+ (1−β1)ˆgt, (38)
vt=β2vt−1+ (1−β2)ˆg2
t, (39)
whereβ1,β2are hyperparameters.
Then,θis updated as follows:
θt=θt−1−η√˜vt˜mt, (40)
8Under review as submission to TMLR
where ˜mtand˜vtare the bias-corrected versions of mtandvt, namely:
˜mt=mt
1−βt
1, (41)
˜vt=vt
1−βt
2. (42)
Now, the counterparts of mtandvtinConstant VSGD , i.e., the first and second momenta:
E[gt] =µt,g, (43)
E/bracketleftbig
g2
t/bracketrightbig
=µ2
t,g+σ2
t,g, (44)
whereµt,gis defined in Eq. 33. If we set Kg=β1
1−β1, then the first momentum updates in Eq. 33 and Eq. 38
become identical.
Next, we examine the relationship between Eq. 39 and Eq. 44. Intuitively, vt(Eq. 39) is an approximation of
the expected second momentum of the noisy gradient E/bracketleftbig
ˆg2
t/bracketrightbig
. While Eq. 44 attempts to quantify the expected
second momentum of the actual gradient E/bracketleftbig
g2
t/bracketrightbig
.
By plugging to Eq. 33 and Eq. 34 to Eq. 44, we obtain the following:
E/bracketleftbig
g2
t/bracketrightbig
=µ2
t−1,gK2
g
(Kg+ 1)2+ ˆg2
t1
(Kg+ 1)2(45)
+2Kg
(Kg+ 1)2µt−1,gˆgt+1
Kg+ 1bt−1,ˆg
at−1. (46)
For clarity, we divide the aforementioned equation into two components: Eq. 45and Eq. 46. The first
component aligns with Eq. 39, as both represent weighted sums of an estimated second momentum and ˆg2
t.
The distinction arises in the second component, where Constant VSGD introduces two additional factors
into the weighted sum: µt−1,gˆgtand1
Kg+1bt−1,ˆg
at−1. Specifically, µt−1,gˆgtapplies a penalty on Eq. 45whenµt−1,g
andˆgthave opposing signs. The factor1
Kg+1bt−1,ˆg
at−1represents the1
Kg+1fraction of observation noise learned
from the data, while a higher observation noise implies greater uncertainty in gt, leading to a higher expected
valueE/bracketleftbig
g2
t/bracketrightbig
.
In summary, Constant VSGD described in Algorithm 2 can be regarded as a variant of Adam. The first
momentum update in both algorithms is equivalent when Kgis set toβ1
1−β1. However, the second momentum
update in Constant VSGD incorporates an additional data-informed component that is dynamically
adjusted based on the observation noise learned from the data.
3.2.2 Constant VSGD andSGD with momentum ( SGDM )
The second optimizer to which we compare Constant VSGD isSGDwith the momentum term ( SGDM).
SGDMis quite often used for DNN training due to its simplicity and improved performance compared
toSGD(Liu et al., 2020b). At the t-th iteration, SGDMcalculates an increment to θt−1, denoted by vt,
expressed as a weighted sum between the previous increment vt−1and the latest noisy gradient ˆgt, namely:
vt=λvt−1+ηˆgt, (47)
θt=θt−1−vt, (48)
whereλis a hyperparameter. After rearranging Eq. 47, we get the following:
vt=λvt−1+ηˆgt= (λ+η)/bracketleftbiggλ
λ+ηvt−1+η
λ+ηˆgt/bracketrightbigg
. (49)
That is, the increment vtinSGDMis a weighted average of vt−1andˆgt, the counterpart of vtinConstant
VSGDisµt,gupdated by Eq. 33. Therefore, by choosing Kgsuch thatKg=γ
η, the update equations 49and
33become identical except for a constant scaling factor.
9Under review as submission to TMLR
3.2.3 Constant VSGD andAMSGrad
Another optimizer related to Constant VSGD isAMSGrad (Reddi et al., 2018), a variant of Adam.
AMSGrad was introduced to address a specific issue of Adamin estimating the second momentum. To
correct the problem, a factor contributing to Adam’s suboptimal generalization performance was introduced.
AMSGrad rectifies it by incorporating the long-term memory of the past gradients.
Specifically, the momentum accumulation process in AMSGrad remains identical to Adam, as defined
in Eq. 38and Eq. 39. However, for the parameter update step, AMSGrad differs from AMSGrad (as
in Eq. 40, whether bias-corrected or not) by incorporating the maximum of the past second momentum
estimates rather than relying solely on the most recent value, that is:
ˆvt=max(ˆvt−1,vt), (50)
θt=θt−1−η√˜vt˜mt. (51)
Here, the long-term memory is carried out by calculating the maximum of all historical second momentum
estimations.
Similarly to AMSGrad ,Constant VSGD incorporates a form of long-term memory regarding past gradient
information in its second momentum update equations 45and46. However, this is not achieved by computing
the maximum of historical values. Notably, the factor1
Kg+1bt−1,ˆg
at−1in Eq. 46represents the expected observation
noise, estimated using data up to the (t−1)-th iteration. In this expression, while Kgandat−1remain
constant for t≥2, the variable component is bt−1,ˆg, which is determined by Eq. 36, for clarity we restate it
as:
bt,ˆg=(1−ρt)bt−1,ˆg+ρts, (52)
where:
s=γ+ 0.5/parenleftbig
σ2
t,g+ (µt,g−ˆgt)2/parenrightbig
+ 0.5Kg/parenleftbig
σ2
t,g+ (µt,g−µt−1,g)2/parenrightbig
. (53)
It represents the combined squared systematic and observation noises after considering ˆgt. Thus, Eq. 52
suggeststhat bt−1,ˆgisabalancedsummationofthehistoricalestimateof bˆgandthenewlysquareddisturbances.
The forgetting rate decreases over t,ρt=t−κ, promoting longer memory retention in successive iterations. In
Constant VSGD , the magnitude of the memory retention can be modulated through the hyperparameter
κ.
4 Experiments
In this Section, we evaluate the performance of the VSGDcompared to other optimizers on high-dimensional
optimization problems The task we focus on in our work is training DNNs to solve classification tasks.
4.1 Setup
DataWe used three benchmark datasets: CIFAR100 (Krizhevsky et al., 2009), TinyImagenet-200
(Deng et al., 2009a)1, and Imagenet-1k (Deng et al., 2009b). The CIFAR100 dataset contains 60000 small
(32×32) RGB images labeled into 100 different classes, 50000 images are used for training, and 10000 are left
for testing. In the case of TinyImagenet-200 , the models are trained on 100000 images from 200 different
classes and tested on 10000 images. For a large-scale experiment, we use the Imagenet-1k dataset which
contains 1,281,167 images from 1000 classes.
Architectures We evaluate VSGDon various optimization tasks and use three different neural network
architectures: VGG(Simonyan&Zisserman,2015), ResNeXt (Xieetal.,2017), and ConvMixer (Trockman
& Kolter, 2023). We keep the model hyperparameters fixed for all the optimizers in our experiments. We
provide a detailed description of each architecture in Appendix B.1.
1We use Tiny Imagenet(Stanford CS231N) provided on www.image-net.org
10Under review as submission to TMLR
Hyperparameters We conducted a grid search over the following hyperparameters: Learning rate (all
optimizers); Weight decay ( AdamW ,VSGD); Momentum coefficient ( SGD). For each set of hyperparameters,
we trained the models with three different random seeds and chose the best one based on the validation
dataset. The complete set of hyperparameters used in all experiments is reported in Table 3.
Furthermore, we apply the learning rate scheduler, which halves the learning rate after each 10000 training
iterationsfor CIFAR100 andevery20000iterationsfor TinyImagenet-200 . Wetrain VGGandConvMixer
using batch size 256 for CIFAR100 and batch size 128 for TinyImagenet-200 . We use a smaller batch size
(128 for CIFAR100 and 64 for TinyImagenet-200 ) with the ResNeXt architecture to fit training on a
single GPU.
4.2 Results
In Table 1, we compare the top-1 test accuracy for Adam,AdamW ,SGD(with momentum), VSGD,VSGD
with weight decay for the different architectures and datasets. The accuracy is averaged across three random
seeds. We observe that VSGDalmost always converges to a better solution compared to AdamandSGD,
outperforming Adamby an average of 2.6% for CIFAR100 and 0.9% for TinyImagenet-200 . Additionally,
In Figures 2 and 3, we show how the top-1 test accuracy progresses during training. We plot the average
values (solid line) as minimum/maximum values (shaded area). We observe that VSGD’s convergence is
often the same rate or faster compared to AdamW . We present training curves for all models in Appendix
B.4.
Table 1: Final Average test accuracy, over three random seeds.
VSGD VSGD Adam AdamW SGD
(w/ L2) (w/o L2) (w/o L2) (w/ L2) (w/ mom)
CIFAR100
VGG16 70.1 70.0 66.8 66.6 67.9
ConvMixer 69.8 69.1 66.5 67.0 65.4
ResNeXt-18 71.4 71.2 68.2 69.7 68.5
TinyImagenet-200
VGG19 51.2 52.0 47.6 49.0 50.9
ConvMixer 53.1 52.6 51.9 52.4 52.4
ResNeXt-18 48.7 47.2 48.8 48.9 47.0Table2: AveragetrainingtimeonGeForce
RTX 2080 Ti (seconds per training itera-
tion) on CIFAR100 dataset.
Model Training Time
Name# ParamsAdam VSGD(×106)
VGG16 14.8 0.38 0.41
ConvMixer 0.6 0.36 0.38
ResNeXt-18 11.1 0.84 0.87
Runtime & Stability As can be observed in Algorithm 1, VSGDrequires additional operation at each
gradient update step compared to the Adamoptimizer. However, we did not observe a large computational
overhead compared to Adamwhen training on the GPU. To illustrate this, we provide the average training
time (per iteration) in Table 2.
Adamis known to perform well on deep learning tasks without the need for extensive hyperparameter
tuning. We observed that our approach also demonstrates stable performance without the need to tune the
hyperparameters for different architectures and datasets separately. Therefore, we believe that VSGDis
generally applicable to various DNN architectures and tasks, as is the case with Adam.
Imagenet In order to evaluate VSGD’s performance in overparameterized domains, we performed an
experiment on the Imagenet-1k dataset (Deng et al., 2009b) where we trained a ResNet-50 with both VSGD
andAdam. Due to limited computational resources, we did not perform any hyperparameter tuning for
this experiment. For both optimizers, we used a learning rate of 0.1 with a OneCycleLR learning rate
scheduler (Smith & Topin, 2019) and a batch size of 256. Same as our previous experiments, we set γto 5e-8
forVSGD. Both models were trained for 50 epochs. The top-1 accuracy of the validation set for VSGDwas
72.7%, exceeding Adam’s 69.8% by 2.9%. We also notice slight improvement in convergence speed (Figure 6).
11Under review as submission to TMLR
10000 20000 30000
Iteration0.50.60.7VGG16-bn
AdamW
SGD (mom)
VSGD
10000 20000 30000
Iteration0.50.60.7ConvMixer-256/8
AdamW
SGD (mom)
VSGD
10000 20000 30000
Iteration0.50.60.7ResNeXt-18
AdamW
SGD (mom)
VSGD
Figure 2: Top-1 test accuracy on CIFAR100 dataset.
20000 40000 60000
Iteration0.30.40.5VGG19-bn
AdamW
SGD (mom)
VSGD
20000 40000 60000
Iteration0.30.40.5ConvMixer-256/8
AdamW
SGD (mom)
VSGD
20000 40000 60000
Iteration0.30.40.5ResNeXt-18
AdamW
SGD (mom)
VSGD
Figure 3: Top-1 test accuracy on TinyImagenet-200 dataset.
5 Conclusion
In this work, we introduced VSGD, a novel optimization framework that combines gradient descent with
probabilistic modeling by treating the true gradients as a hidden random variable. This approach allows for a
more principled approach to model noise over gradients. Not only does this framework open a new path to
optimizing DNNs, but one can establish links with other popular optimization methods by certain specific
modeling choices in our method. Lastly, we evaluated VSGDon large-scale image classification tasks using a
variety of deep learning architectures, where we demonstrated that VSGDconsistently outperformed the
baselines and achieved a competitive convergence rate.
In conclusion, we outline two main directions for extending our framework. The first direction is to make
stronger dependency assumptions in VSGD. For example, we can model the dependencies between different
gradients by introducing covariates between gradients of different parameters in Eq. 3, or considering the
second-order momentum of gradients in the VSGDupdate rules. Second, we advocate for the application
ofVSGDto a broader spectrum of machine learning challenges beyond classification tasks such as deep
generative modeling, representation learning, and reinforcement learning.
References
Anas Barakat and Pascal Bianchi. Convergence analysis of a momentum algorithm with adaptive step size
for non convex optimization. arXiv preprint arXiv:1911.07596 , 2019.
Barbara Bittner and Luc Pronzato. Kalman filtering in stochastic gradient algorithms: construction of
a stopping rule. In 2004 IEEE International Conference on Acoustics, Speech, and Signal Processing ,
volume 2, pp. ii–709. IEEE, 2004.
Léon Bottou. Stochastic Gradient Descent Tricks , pp. 421–436. Springer Berlin Heidelberg, Berlin, Heidelberg,
2012. ISBN 978-3-642-35289-8. doi: 10.1007/978-3-642-35289-8_25. URL https://doi.org/10.1007/
978-3-642-35289-8_25 .
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image
Database. In CVPR09 , 2009a.
12Under review as submission to TMLR
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition , pp. 248–255,
2009b. doi: 10.1109/CVPR.2009.5206848.
Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic activation.
InInternational conference on machine learning , pp. 1329–1338. PMLR, 2018.
Runa Eschenhagen, Alexander Immer, Richard E Turner, Frank Schneider, and Philipp Hennig. Kronecker-
Factored Approximate Curvature for Modern Neural Network Architectures. In Thirty-seventh Conference
on Neural Information Processing Systems , 2023.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks.
InProceedings of the thirteenth international conference on artificial intelligence and statistics , pp. 249–256.
JMLR Workshop and Conference Proceedings, 2010.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 , 2013.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-
level performance on imagenet classification. In Proceedings of the IEEE international conference on
computer vision , pp. 1026–1034, 2015.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal
of Machine Learning Research , 2013.
Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from adam to
sgd.arXiv preprint arXiv:1712.07628 , 2017.
Mohammad Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivastava. Fast and
scalable Bayesian deep learning by weight-perturbation in Adam. In Jennifer Dy and Andreas Krause
(eds.),Proceedings of the 35th International Conference on Machine Learning , volume 80 of Proceedings
of Machine Learning Research , pp. 2611–2620. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.
press/v80/khan18a.html .
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference
on Learning Representations (ICLR) , San Diega, CA, USA, 2015.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Wu Lin, Frank Nielsen, Mohammad Emtiyaz Khan, and Mark Schmidt. Structured second-order methods
via natural gradient descent. 2021.
Wu Lin, Valentin Duruisseaux, Melvin Leok, Frank Nielsen, Mohammad Emtiyaz Khan, and Mark Schmidt.
Simplifying momentum-based positive-definite submanifold optimization with applications to deep learning.
InInternational Conference on Machine Learning , pp. 21026–21050. PMLR, 2023.
Haitao Liu, Yew-Soon Ong, Xiaobo Shen, and Jianfei Cai. When gaussian process meets big data: A review
of scalable gps. IEEE transactions on neural networks and learning systems , 31(11):4405–4423, 2020a.
Tianyi Liu, Yifan Lin, and Enlu Zhou. Bayesian stochastic gradient descent for stochastic optimization with
streaming input data. SIAM Journal on Optimization , 34(1):389–418, 2024.
Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent with momentum.
Advances in Neural Information Processing Systems , 33:18261–18271, 2020b.
Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. arXiv preprint
arXiv:1608.03983 , 2016.
Stephan Mandt, Matthew D Hoffman, David M Blei, et al. Stochastic gradient descent as approximate
bayesian inference. Journal of Machine Learning Research , 18(134):1–35, 2017.
13Under review as submission to TMLR
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature.
InInternational conference on machine learning , pp. 2408–2417. PMLR, 2015.
Chris Mingard, Guillermo Valle-Pérez, Joar Skalse, and Ard A Louis. Is sgd a bayesian sampler? well, almost.
The Journal of Machine Learning Research , 22(1):3579–3642, 2021.
Ryan Murray, Brian Swenson, and Soummya Kar. Revisiting normalized gradient descent: Fast evasion of
saddle points. IEEE Transactions on Automatic Control , 64(11):4818–4824, 2019.
KazukiOsawa, SiddharthSwaroop, MohammadEmtiyazEKhan, AnirudhJain, RunaEschenhagen, RichardE
Turner, and Rio Yokota. Practical deep learning with bayesian principles. Advances in neural information
processing systems , 32, 2019.
Joaquin Quinonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate gaussian
process regression. The Journal of Machine Learning Research , 6:1939–1959, 2005.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International
Conference on Learning Representations , 2018.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics , pp. 400–407, 1951.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating
errors.nature, 323(6088):533–536, 1986.
Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks , 61:85–117, 2015.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
InInternational Conference on Learning Representations , 2015.
Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large
learning rates. In Artificial intelligence and machine learning for multi-domain operations applications ,
volume 11006, pp. 369–386. SPIE, 2019.
Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient descent.
arXiv preprint arXiv:1710.06451 , 2017.
Ruo-Yu Sun. Optimization for deep learning: An overview. Journal of the Operations Research Society of
China, 8(2):249–294, 2020.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and
momentum in deep learning. In International conference on machine learning , pp. 1139–1147. PMLR,
2013.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for machine learning.
University of Toronto, Technical Report , 6, 2012.
Asher Trockman and J Zico Kolter. Patches are all you need? Transactions on Machine Learning Research ,
2023.
James Vuckovic. Kalman gradient descent: Adaptive variance reduction in stochastic optimization. arXiv
preprint arXiv:1810.12273 , 2018.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of
the 28th international conference on machine learning (ICML-11) , pp. 681–688. Citeseer, 2011.
Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations
for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pp. 1492–1500, 2017.
14Under review as submission to TMLR
Xingyi Yang. Kalman optimizer for consistent gradient descent. In ICASSP 2021-2021 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 3900–3904. IEEE, 2021.
Soma Yokoi and Issei Sato. Bayesian interpretation of SGD as Ito process. arXiv preprint arXiv:1911.09011 ,
2019.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 , 2012.
Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for convergences
of adam and rmsprop. In Proceedings of the IEEE/CVF Conference on computer vision and pattern
recognition , pp. 11127–11135, 2019.
15Under review as submission to TMLR
A Derivations
A.1 Derivation of VSGD
Here, we derive Eq. 14 to Eq. 18 following the SVI technique introduced by (Hoffman et al., 2013). First, we
provide a brief overview of the SVIprocedure before diving into the details.
In thet-thSVIiteration, we realize the following steps:
1. Sample ˆgfrom ˆg1:T, where the sample is denoted by ˆgt.
2. Update the local parameters ϕt,1:Twithτt−1andˆgt, such that
ϕt,k= arg max
ϕkEq(wg,wˆg;τt−1)/bracketleftbigg
logp(gk|wg;ut)p(ˆgt|gk,wˆg)
q(gk;ϕk)/bracketrightbigg
k= 1 :T, (54)
wherep(gk|wg;ut)andp(ˆgt|gk,wˆg)are defined in Eq. 3and Eq. 4, respectively. Note that here we
use the same gtfor eachk∈1 :T, as if the same ˆgtis observed Ttimes.
3. Calculate the intermediate global parameters τ′
twithϕt,1:T, such that:
τ′
t= arg max
τEq(g1:T;ϕt,1:T)/bracketleftigg
logp(wg)p(wˆg)/producttextT
k=1p(gk|wg;ut)p(ˆgt|gk,wˆg)
q(wg,wˆg;τ)/bracketrightigg
, (55)
wherep(wg)andp(wˆg)are defined in Eq. 5and Eq. 6, respectively.
4. Update the global parameters τtusing:
τt= (1−ρ)τt−1+ρτ′
t, (56)
whereρt= (ρt,1,ρt,2)Tis defined in Eq. 21and Eq. 22.
5.Pick anyϕt,k= (µt,k,σ2
t,k)T∈ϕt,1:T, and setut+1=µt,k, whereut+1serves as the control variate
for the next iteration. We can choose any ϕt,kbecause by the definition (Eq. 54),ϕt,kandϕt,jare
identical for any i,j∈1 :T.
Since allϕt,kare identical for k= 1 :T, for notational ease, we will use ϕt= (µt,g,σ2
t,g)Tto represent the
unique local parameter updated in the t-thSVIiteration. Distributions defined in Eq. 2are in the conjugate
exponential family, therefore, we can derive the closed-form update rule for ϕtaccording to Eq. 54:
logq(gk;µt,g,σ2
t,g)≡Eq(wg,wˆg;τt−1)[logp(gk|wg;ut) + logp(ˆgt|gk,wˆg)]. (57)
Next, by matching the coefficients of the natural parameters, we get:
µt,g=utbt−1,ˆg
bt−1,ˆg+bt−1g+ ˆgtbt−1,g
bt−1,ˆg+bt−1g, (58)
σ2
t,g=bt−1,ˆgbt−1g
at−1,g(bt−1,ˆg+bt−1g), (59)
where Eq. 59corresponds to Eq. 15. Putting the control variate ut=µt−1,gin Eq. 58results in Eq. 14. Note
that the shape parameters at−1,gandat−1,ˆgare cancelled out while deriving Eq. 58, reasons being that the
shapes are all equal to the same constant over the iterations, as later we will show in Eq. 64.
16Under review as submission to TMLR
Similarly for τ′
t= (a′
t,g,a′
t,ˆg,b′
t,g,b′
t,ˆg), according to Eq. 55:
logq(wg;a′
t,g,b′
t,g)≡Eq(ˆg1:T;ϕt)/bracketleftigg
logp(wg) +T/summationdisplay
k=1logp(gk|ut,wg)/bracketrightigg
, (60)
=Eq(ˆg1:T;ϕt)[logp(wg) +Tlogp(g1|ut,wg)], (61)
logq(wˆg;a′
t,ˆg,b′
t,ˆg)≡Eq(ˆg1:T;ϕt)/bracketleftigg
logp(wˆg) +T/summationdisplay
k=1logp(ˆgt|gk,wˆg)/bracketrightigg
, (62)
=Eq(ˆg1:T;ϕt)[logp(wˆg) +Tlogp(ˆgt|g1,wˆg)]. (63)
Again, by matching the coefficients of the natural parameters:
a′
t,g=a′
t,ˆg=γ+ 0.5T (64)
b′
t,g=γ+ 0.5T(σ2
t,g+µ2
t,g−2µt,gµt−1,g+µt−1,g2) (65)
b′
t,ˆg=Kgγ+ 0.5T(σ2
t,g+µ2
t,g−2µt,gˆgt+ ˆg2
t) (66)
In Eq. 64,a′
t,ganda′
t,ˆgequal the same constant over the iterations. As a result, at,gandat,ˆgare also equal
constants, according to Eq . 56.
Eq.64to Eq. 66reveals that during the posterior updates of wgandwˆg, choosing a larger value for T
increases the emphasis on the information conveyed by ˆgt, consequently diminishing the relative influence of
the priors. In VSGD, we have already made the prior strength, γ, a free parameter, then we can replace
Twith a constant and still pertain the ability to adjust the strength between prior and new observation
through values of γ.
That means we use a constant value, such as 1, to replace Tin Eq. 64, Eq. 65and Eq. 66, which result in
Eq.17and Eq. 18. Eq. 19and Eq. 20are derived by simply following Eq. 56. Eq. 16is derived from the fact
that Eq. 64is a constant, and interpolation between two equal constants results in the same constant.
A.2 A Kernel Smoothing View on VSGD
If we look at VSGDfrom a kernel smoothing perspective, making use of utserves as a cheap approximation
to conditioning on all of the historical data. Namely, VSGDseeks to smooth the local gradient function in a
similar way with the following kernel smoothing task.
There are two noisy observations of the gradient function at θ=θt. They are (θt,ut)and(θt,ˆgt). We can
define two Gaussian kernels for each of the noisy observations, governed by w−1
gandw−1
ˆg. By denoting the
Gaussian kernels κ1andκ2, the smoothed gradient value at θtshould be as follows:
gt=κ1(θt,θt)
κ1(θt,θt) +κ2(θt,θt)ut+κ2(θt,θt)
κ1(θt,θt) +κ2(θt,θt)ˆgt. (67)
After taking the expectation step of SVI (expect out wgandwˆg), the above equation simplifies to Eq (15).
Note thatutinVSGDis introduced as a learned summary of the noisy observations before tand we only need
to smooth on utas a cheap approximation to smoothing on all of the historical data. Similar simplification
technique is commonly seen in scalable Gaussian Process solutions, as discussed in Quinonero-Candela &
Rasmussen (2005); Liu et al. (2020a). In these references, the learned summaries are called the "inducing
points".
If we look at VSGDas an approximation to the local areas of the gradient function, then the precision term
wgcan be seen as a smoothness measure of the gradient surface. In VSGD we assume a global wg, which is
equivalent to assuming that the smoothness of the whole gradient function is governed by a constant. This
assumption is not necessarily ideal, but we still treat it as a global parameter for simplicity and practical
reasons.
17Under review as submission to TMLR
A.3 Treating Samples Separately in a Mini-bach
Assuming the size of the mini-batch is M, denote the sample gradients {ˆg(i)
t}i=1:M. According to Hoffman
et al. (2013), we would need to calculate Eq. 14Mtimes and average them in Eq. 17and Eq. 18. Specifically,
we would need to replace Eq. 14, Eq. 17and Eq. 18with:
µ(i)
t,g=µt−1,gbt−1,ˆg
bt−1,ˆg+bt−1g+ ˆg(i)
tbt−1,g
bt−1,ˆg+bt−1gi= 1 :M (68)
b′
t,g=/summationtextM
i=1γ+ 0.5/parenleftig
σ2
t,g+ (µ(i)
t,g−µt−1,g)2/parenrightig
M(69)
b′
t,ˆg=/summationtextM
i=1Kgγ+ 0.5/parenleftig
σ2
t,g+ (µ(i)
t,g−ˆg(i)
t)2/parenrightig
M(70)
And introduce an additional step
µt,g=/summationtextM
i=1µ(i)
t,g
M(71)
before Eq. 23.
Based on the equations presented, the computational cost grows linearly with the batch size M. Given
the importance of efficiency in contemporary optimization techniques, above equations were omitted from
the main paper. Nonetheless, it is conceivable that the increased computational expense could be offset by
enhanced convergence rates, a hypothesis reserved for future investigation.
18Under review as submission to TMLR
B Experimental Details
B.1 Architecture
We have used open-source implementations of all three models.
VGGWe use VGG16 and VGG192with batch normalization. We add adaptive average pooling before the
final linear layer to make the architecture suitable for both 32×32and64×64images.
ConvMixer We use ConvMixer3with 256 channels and set the depth to 8.
ResNeXt ResNeXt4we set the cardinality (or number of convolution groups) to 8 and the depth to 18
layers. The widen factor is set to 4, resulting in channels being [64, 256, 512, 1024] . We add adaptive
average pooling before the final linear layer to make the architecture suitable for both 32×32and64×64
images.
B.2 Hyperparameters
The full set of hyperparameters used in all experiments is reported in Table 3.
Table 3: Hyperparameter values used in allthe experiments.
Adam VSGD
(β1,β2) (0.9, 0.999) —
(κ1,κ2) — (0.9, 0.81)
γ — 1e-8
Kg — 30
learning rate {0.001, 0.005, 0.01, 0.02}
weight deacy {0, 0.01}
momentum coefficient {0.9, 0.99}
B.3 Data Augmentations
We use data augmentations during training to improve the generalizability of the models. These augmentations
were the same for the optimizers and only differ between datasets.
Table 4: Data augmentations.
CIFAR100 tiny ImageNet-200
RandomCrop(32, padding=4) RandomCrop(64, padding=4)
RandomHorizontalFlip RandomHorizontalFlip
RandomAffine(degrees=45, translate=(0.1, 0.1), scale=(0.9, 1.1))
ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)
2github.com/alecwangcq/KFAC-Pytorch/blob/master/models/cifar/vgg.py
3github.com/locuslab/convmixer-cifar10
4github.com/prlz77/ResNeXt.pytorch
19Under review as submission to TMLR
B.4 Training Curves
10000 20000 30000
Iteration0.00.51.01.52.0VGG16-bn
AdamW
SGD (mom)
VSGD
10000 20000 30000
Iteration0.00.51.01.52.0ConvMixer-256/8
AdamW
SGD (mom)
VSGD
10000 20000 30000
Iteration0.00.51.01.52.0ResNeXt-18
AdamW
SGD (mom)
VSGD
Figure 4: Training loss on CIFAR100 dataset.
20000 40000 60000
Iteration024VGG19-bn
AdamW
SGD (mom)
VSGD
20000 40000 60000
Iteration24ConvMixer-256/8
AdamW
SGD (mom)
VSGD
20000 40000 60000
Iteration24ResNeXt-18
AdamW
SGD (mom)
VSGD
Figure 5: Training loss on TinyImagenet-200 dataset.
0 50000 100000 150000 200000 250000
Iteration0255075
AdamW
VSGD
Figure 6: Top-1 Accuracy on the Imagenet dataset trained with ResNet-50.
C Hyperparameter Sensitivity Analysis
In this Section, we investigate the shape parameter γ’s sensitivity to VSGD’s performance. Higher γimplies
a stronger weight on the prior while a lower γimplies a stronger weight on the current observation. We train
VSGDon a wide range of γvalues on CIFAR100 using the ConvMixer architecture. The results can be
observed in Figures 7, 8 and Table 5. Inspecting Figure 7 (Right) more closely, we observe that while there is
a statistically significant difference between accuracies obtained by different γs, they all perform relatively
well as long as γis not too large.
20Under review as submission to TMLR
108
106
104
0.9940.9960.9981.000Train Accuracy
108
106
104
0.620.640.660.680.70T est Accuracy
Figure 7: Analysis of the hyperparameter γ’sensitivity on the VSGD’s performance in terms of accuracy.
024LossTrain
024T est
0 5000 10000 15000 20000 25000 30000
t0.80.91.0Accuracy
0 5000 10000 15000 20000 25000 30000
t0.6000.6250.6500.6750.700
1e-09
5e-09
1e-08
5e-08
1e-07
0.001
Figure 8: CIFAR100 learning curves trained with ConvMixer architecture for different γvalues.
γAccuracy
1e-9 67.75
5e-9 68.59
1e-8 69.03
5e-8 69.77
1e-7 69.71
1e-3 60.97
Table 5: Test accuracy on CIFAR100 trained with ConvMixer for different γvalues.
D Constant VSGD
In this Section, we offer a comparison between VSGDandConstant VSGD . From the modeling point of
view, VSGDhas more free parameters that allow our model to learn more complicated distributions. While
Constant VSGD imposes a constraint on VSGD, which is less flexible but may lead to better out-of-sample
performance. To investigate this, we ran additional experiments on CIFAR100 withConvMixer architecture.
The final accuracy for VSGDwas69.04±0.4, while the accuracy for Constant VSGD was68.49±0.19.
The learning curves can be observed in Figure 9.
21Under review as submission to TMLR
024LossTrain
024T est
0 5000 10000 15000 20000 25000 30000
t0.70.80.91.0Accuracy
0 5000 10000 15000 20000 25000 30000
t0.6000.6250.6500.6750.700Optim
VSGD
Constant VSGD
Figure 9: Comparison of Constant-VSGD andVSGDlearning curves trained on CIFAR100 with
ConvMixer architecture. The final accuracy for VSGDis69.04±0.4, while the accuracy for Constant-
VSGDis68.49±0.19.
22