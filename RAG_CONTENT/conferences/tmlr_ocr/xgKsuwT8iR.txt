Under review as submission to TMLR
Personalised Federated Learning On Heterogeneous Feature
Spaces
Anonymous authors
Paper under double-blind review
Abstract
Personalised federated learning (FL) approaches assume that raw data of all clients are
defined in a common space i.e.all clients store their data according to the same schema. For
real-world applications, this assumption is restrictive as clients, having their own systems to
collect and then store data, may use heterogeneous data representations. To bridge the gap
between the assumption of a shared subspace and the more realistic situation of client-specific
spaces, we propose a general framework coined FLICthat maps client’s data onto a common
feature space via local embedding functions, in a federated manner. Preservation of class
information in the latent space is ensured by a distribution alignment with respect to a learned
reference distribution. We provide the algorithmic details of FLICas well as theoretical
insights supporting the relevance of our methodology. We compare its performances against
FL benchmarks involving heterogeneous input features spaces. Notably, we are the first to
present a successful application of FL to Brain-Computer Interface signals acquired on a
different number of sensors.
1 Introduction
Federated learning (FL) is a machine learning paradigm where models are trained from multiple isolated
data sets owned by individual agents/clients, where raw data need not be transferred to a central server, nor
even shared in any way (Kairouz et al., 2021a). FL ensures data ownership, and structurally incorporates
the principle of data exchange minimization by only transmitting the required updates of the models being
learned. Recently, FL works have focused on personalised FL to tackle statistical data heterogeneity and used
local models to fit client-specific data (Tan et al., 2022; Jiang et al., 2019; Khodak et al., 2019; Hanzely &
Richtárik, 2020). However, most existing personalised FL works assume that the raw data on allclients share
the same structure and are defined on a common feature space. Yet, in practice, data collected by clients
may use differing structures: they may not capture the same information, some features may be missing or
not stored, or some might have been transformed ( e.g.via normalization, scaling, or linear combinations).
An illustrative example of this, related to Brain-Computer Interfaces (Yger et al., 2016; Lv et al., 2021)
and tackled in this paper, is the scenario where electroencephalography signals are recorded from different
subjects, with varying numbers of electrodes and a diverse range of semantic information ( e.g.motor imagery
tasks and resting state). To tackle the challenge of making FL possible in situations where clients have
heterogeneous feature spaces – such as disparate dimensionalities or differing semantics of vector coordinates
– we present the first personalised FL framework specifically designed to address this learning scenario.
Proposed Approach. The key idea of our proposal is driven by two objectives: (i) clients’ data have to
be embedded in a common latent space, and (ii) data related to the same semantic information ( e.g.label)
have to be embedded in the same region of this latent space. The first objective is a prior necessary step
before FL since it allows to define a relevant aggregation scheme on the central server for model parameters
(e.g.via weighted averaging). The second one is essential for a proper federated learning of the model
parameters as FL approaches are known to struggle when data across clients follow different probability
distributions (Kairouz et al., 2021b). As shown later, this second objective is not guaranteed by performing
client-independent learning of embedding functions, such as via low-dimensional embeddings or autoencoders.
To cope with this issue, we align clients’ embedded feature distributions with a latent anchor distribution that
1Under review as submission to TMLR
is shared across clients. The learning of the anchor distribution happens in a federated way, which means it
is updated locally on each client and then combined on a central server through barycenter computation
(Veldhuis, 2002; Banerjee et al., 2005). Then, we seamlessly integrate this distribution alignment mechanism,
utilizing local embedding functions and anchor distribution, into a personalised federated learning framework
that is similar to the approach proposed by Collins et al. (2021), without any loss of generality.
Contributions. To help the reader better grasp the differences of our approach with respect to the existing
literature, we here spell out our contributions:
1.We arethe first to formalize the problem of personalised FL on heterogeneous client’s feature spaces.
In contrast to existing approaches, the proposed general framework, referred to as FLIC, allows each
client to leverage other clients’ data even though they do not have the same raw representation.
2.We introduce a distribution alignment framework and an algorithm that learns the feature embedding
functions along with the latent anchor distribution in a local and global federated manner, respectively.
We also show how those essential algorithmic components are integrated into a personalised FL
algorithm, easing adoption by practitioners.
3.We provide algorithmic and theoretical support to the proposed methodology. In particular, we show
that for a simpler but insightful learning scenario, FLICis able to recover the true latent subspace
underlying the FL problem.
4.Beyond competitive experimental analyses on toy and real-world problems, we stand out as a pioneer
in Brain-Computer Interfaces (BCI) by being the first to learn from heterogeneous BCI datasets
using federated learning. The proposed methodology can handle data with different sensor counts
and class numbers, a feat not achieved by any other methodology to our knowledge, and can have a
strong impact on other medical domains with similar data heterogeneity.
Conventions and Notations. The Euclidean norm on Rdis∥·∥.|S|denotes the cardinality of the set S
andN∗=N\{0}. Forn∈N∗, we refer to{1,...,n}with [n].N(m,Σ)is the Gaussian distribution with
mean vector mand covariance matrix ΣandX∼νmeans that the random variable Xis drawn from the
probability distribution ν. The Wasserstein distance of order 2between any probability measures µ,νon
Rdwith finite 2-moment is W2(µ,ν) = (infζ∈T(µ,ν)/integraltext
Rd×Rd∥θ−θ′∥2dζ(θ,θ′))1/2, whereT(µ,ν)is the set of
transference plans of µandν.
2 Related works
As far as our knowledge goes, the proposed methodology is the first one to tackle the problem of FL from
heterogeneous feature spaces. However, some related ideas have been proposed in the literature. The idea of
usingdistributionalignementhasbeenconsideredintheFLliteraturebutonlyforaddressingdistributionshifts
on clients (Zhang et al., 2021b; Ye et al., 2022). Other methodological works on autoencoders (Xu et al., 2020),
word embeddings (Alvarez-Melis & Jaakkola, 2018; Alvarez-Melis et al., 2019) or FL under high statistical
heterogeneity (Makhija et al., 2022; Luo et al., 2021; Zhou et al., 2022) use similar ideas of distribution
alignment for calibrating feature extractors and classifiers. Comparing distributions from different spaces
has also been considered in a (non-FL) centralised manner using approaches like the Gromov-Wasserstein
distance or related distances (Mémoli, 2011; Bunne et al., 2019; Alaya et al., 2022).
Several other works can also be broadly related to the proposed methodology. Loosely speaking, we can
divide these related approaches into three categories namely (i) heterogeneous-architecture personalised FL,
(ii) vertical FL and (iii) federated transfer learning.
Compared to traditional horizontal personalised FL (PFL) approaches, so-called heterogeneous-architecture
ones are mostly motivated by local heterogeneity regarding resource capabilities of clients e.g.computation a
nd storage (Zhang et al., 2021a; Diao et al., 2021; Collins et al., 2021; Shamsian et al., 2021; Hong et al.,
2022; Makhija et al., 2022). Nevertheless, they never consider features defined on heterogeneous subspaces,
which is our main motivation. In vertical federated learning (VFL), clients hold disjoint subsets of features.
2Under review as submission to TMLR
Table 1: Related works. PFL refers to horizontal personalised FL, VFL to vertical FL and FTL to federated
transfer learning.
method type ̸=feature spaces multi-party no shared ID no shared feature
(Zhang et al., 2021a) PFL ✗ ✓ ✓ ✗
(Diao et al., 2021) PFL ✗ ✓ ✓ ✗
(Collins et al., 2021) PFL ✗ ✓ ✓ ✗
(Shamsian et al., 2021) PFL ✗ ✓ ✓ ✗
(Hong et al., 2022) PFL ✗ ✓ ✓ ✗
(Makhija et al., 2022) PFL ✗ ✓ ✓ ✓
FLIC (this paper) PFL ✓ ✓ ✓ ✓
(Hardy et al., 2017) VFL ✓ ✗ ✗ ✓
(Yang et al., 2019) VFL ✓ ✗ ✗ ✓
(Gao et al., 2019) FTL ✓ ✓ ✓ ✗
(Sharma et al., 2019) FTL ✗ ✗ ✓ ✗
(Liu et al., 2020) FTL ✓ ✗ ✗ ✓
(Mori et al., 2022) FTL ✓ ✓ ✗ ✗
However, a restrictive assumption is that a large number of users are common across the clients (Yang et al.,
2019; Hardy et al., 2017; Angelou et al., 2020; Romanini et al., 2021). In addition, up to our knowledge,
no vertical personalised FL approach has been proposed so far, which is restrictive if clients have different
business objectives and/or tasks. Finally, some works have focused on adapting standard transfer learning
approaches with heterogeneous feature domains under the FL paradigm. These federated transfer learning
(FTL) approaches (Gao et al., 2019; Mori et al., 2022; Liu et al., 2020; Sharma et al., 2019) stand for FL
variants of heterogeneous-feature transfer learning where there are bsourceclients and 1 target client with a
target domain. However, these methods do not consider the same setting as ours and assume that clients
share a common subset of features. We compare the most relevant approaches among the previous ones in
Table 1.
3 Proposed Methodology
Problem Formulation. We consider the problem where b∈N∗clients want to solve a learning task
within the centralised personalised FL paradigm (Yang et al., 2019; Kairouz et al., 2021a), where a central
entity orchestrates the collaborative solving of a common machine learning problem by the bclients, without
requiring raw data exchanges. The clients are assumed to possess local data sets {Di}i∈[b]such that, for any
i∈[b],Di={(x(j)
i,y(j)
i)}j∈[ni]wherex(j)
istands for a feature vector, y(j)
iis a label and ni=|Di|. In contrast
to existing FL approaches, we assume that the raw input features {x(j)
i}j∈[ni]of clients live in heterogeneous
spacesi.e.for anyi∈[b],x(j)
i∈XiwhereXiis a client-specific measurable space. More precisely, for any
i∈[b]andj∈[ni], we assume that x(j)
i∈Xi⊆Rkisuch that{Xi}i∈[b]are not part of a common ground
metric. This setting is challenging since standard FL approaches (McMahan et al., 2017a; Li et al., 2020)
and even personalised FL ones (Collins et al., 2021; Hanzely et al., 2021) cannot be directly applied. For
simplicity, we assume that all clients want to solve a multi-class classification task with C∈N∗classes. We
discuss later how regression tasks can be encompassed in the proposed framework.
Methodology. The goal of the proposed methodology, coined FLIC, is to learn a personalised model for each
client while leveraging the information stored by other clients’ data sets despite the heterogeneity issue. To
address this feature space heterogeneity, we propose to map client’s features into a fixed-dimension common
subspace Φ⊆Rkby resorting to learnt localembedding functions {ϕi:Xi→Φ}i∈[b]. In order to preserve
semantical information (such as the class associated to a feature vector) from the original data distribution,
we seek at learning the functions {ϕi}i∈[b]such that they are aligned with ( i.e.minimise their distance to)
some learnable latent anchor distributions that are shared across all clients. These anchor distributions act
3Under review as submission to TMLR
Figure 1: Illustration of part of the proposed methodology for b= 3clients with heterogeneous digit images
coming from three different data sets namely MNIST (Deng, 2012), USPS (Hull, 1994) and SVHN (Netzer
et al., 2011). The circles with digits inside stand for a group of samples, of a given class, owned by a client
and the size of the circles indicates their probability mass. In the subspace Φ,{µi}i∈[b](and their level
sets) refer to some learnable reference measures to which we seek to align the transformed version νϕiofνi.
Personalised FL then occurs in the space Φand aims at learning local models {θi= (α,βi)}i∈[b]for each
client as well as{ϕi,µi}i∈[b]. Non-personalised FL could also be considered and naturally embedded in the
proposed distribution alignement framework.
as universal “calibrators” for clients, preventing similar semantic information from different clients from
scattering across the subspace Φ. This scattering would otherwise impede proper subsequent federated
learning of the classification model. As depicted in Figure 1, local embedding functions are learnt by aligning
the mapped probability distributions, denoted as ν(c)
ϕi, conditioned on the class c∈[C], withClearnable
anchor measures {µc}c∈[C]. This alignment is achieved by minimising their distance.
Remark 1. We want to stress the significance of aligning the class-conditional probability distributions ν(c)
ϕ
with respect to the anchor distributions µc. Local and independent learning of embedding functions ϕiby each
client does not guarantee alignment of resulting probability distributions in the common subspace Φ. As in
unsupervised multilingual embeddings (Grave et al., 2019), alignments are crucial for preserving semantic
similarity of class information. Misalignment occurs when projecting class-conditionals in a lower-dimensional
space using an algorithm, like t-sne, that seeks at preserving only local similarities. Indeed, data from
different clients are projected in a subspace in which different class-conditionals may overlap. This is also the
case when using a neural network with random weights as a projector or an auto-encoder. Examples of such
phenomena are illustrated in Figure 3 Notably, the figure shows that the alignment with respect to the anchor
distribution is crucial to ensure that the class-conditional distributions are aligned in the common subspace Φ.
Once data from the heterogeneous spaces are embedded in the same latent subspace Φ, we can deploy a
federated learning methodology for training from this novel representation space. While any standard FL
approach e.g.FedAvg(McMahan et al., 2017a) can be used, we consider a personalised FL where each client
has a local model tailored to its specific data distribution as statistical heterogeneities that are still present in
Φ(Tan et al., 2022). Hence, given the aforementioned local embedding functions {ϕi}, the model parameters
{θi∈Rdi}and some non-negative weights associated to each client {ωi}i∈[b]such that/summationtextb
i=1ωi= 1, we
consider the following empirical risk minimisation problem:
min
θ1:b,ϕ1:bf(θ1:b,ϕ1:b) =b/summationdisplay
i=1ωifi(θi,ϕi), (1)
and for any i∈[b],
fi(θi,ϕi) =1
nini/summationdisplay
j=1ℓ/parenleftig
y(j)
i,g(i)
θi/bracketleftig
ϕi/parenleftig
x(j)
i/parenrightig/bracketrightig/parenrightig
. (2)
4Under review as submission to TMLR
whereℓ(·,·)stands for a classification loss function between the true label y(j)
iand the predicted one
g(i)
θi[ϕi(x(j)
i)]whereg(i)
θiis the local model that admits a personalised architecture parameterised by θiand
taking as input an embedded feature vector ϕi(x(j)
i)∈Φ.
Objective Function. At this stage, we are able to integrate the FL paradigm and the local embedding
function learning into a global objective function we want to optimise, see (1). Remember that we want
to learn the parameters {θi}i∈[b]of personalised FL models, in conjuction with some local embedding
functions{ϕi}i∈[b]and shared anchor distributions {µc}. In particular, the latter have to be aligned with
class-conditional distributions {ν(c)
ϕi}. We enforce this alignment via a Wasserstein regularisation term leading
us to a regularised version of the empirical risk minimisation problem defined in (1), namely
θ⋆
1:b,ϕ⋆
1:b,µ⋆
1:C= arg min
θ1:b,ϕ1:b,µ1:Cb/summationdisplay
i=1Fi(θi,ϕi,µ1:C),
and for any i∈[b],
Fi(θi,ϕi,µ1:C) =ωifi(θi,ϕi) +λ1ωi/summationdisplay
c∈YiW2
2/parenleftig
µc,ν(c)
ϕi/parenrightig
+λ2ωi/summationdisplay
c∈Yi1
JJ/summationdisplay
j=1ℓ/parenleftig
c,g(i)
θi/bracketleftig
Z(j)
c/bracketrightig/parenrightig
,(3)
where{Z(j)
c;j∈[J]}c∈[C]stand for samples drawn from {µc}c∈[C], andλ1,λ2>0are regularisation
parameters. The second term in (3)aims at aligning the conditional probability distributions of the
transformed features to the anchors. The third one is an optional term aspiring to calibrate the reference
measures with the classifier in cases where two or more classes are still ambiguous after mapping onto the
common feature space; it has also some benefits to tackle covariate shift in standard FL (Luo et al., 2021).
DesignChoicesandJustifications. Inthesequel, weconsidertheGaussiananchormeasures µc= N(vc,Σc)
wherevc∈Rkandc∈[C]. One of the key advantages of this Gaussian assumption is that, under mild
assumptions, it guarantees the existence of a transport map T(i)such thatT(i)
#(νi) =µ, owing to Brenier’s
theorem (Santambrogio, 2015) since a mixture of Gaussians admits a density with respect to the Lebesgue
measure. Hence, in our case, learning the local embedding functions boils down to approximating this
transport map T(i)
#byϕi. We also approximate the conditional probability measures {ν(c)
ϕi;c∈Yi}i∈[b]by
using Gaussian measures {ˆν(c)
ϕi= N( ˆm(c)
i,ˆΣ(c)
i);c∈Yi}i∈[b]such that for any i∈[b]andc∈[C],ˆm(c)
iand
ˆΣ(c)
istand for empirical mean vector and covariance matrix. The relevance of this approximation is detailed
in Appendix S1.2.
These two Gaussian choices (for the anchor distribution and the class-conditional distributions) allow us to
have a closed-form expression for the Wasserstein distance of order 2 which appears in (3), seee.g.Gelbrich
(1990); Dowson & Landau (1982). More precisely, we have for any i∈[b]andc∈[C],
W2
2/parenleftig
µc,ν(c)
ϕi/parenrightig
=/vextenddouble/vextenddouble/vextenddoublevc−m(c)
i/vextenddouble/vextenddouble/vextenddouble2
+B2/parenleftig
Σc,Σ(c)
i/parenrightig
,
whereB(·,·)denotes the Bures distance between two positive definite matrices (Bhatia et al., 2019).
Remark 2. Instead of Gaussian distribution approximations, we can consider more complex probability
distributions. For instance, we can use a Gaussian mixture model (GMM) and still be able to compute cheaply
the Wasserstein distance (Chen et al., 2018). We can even make no hypothesis on the data distribution and
compute the distance for alignment using the linear programming based OT-formulation or use any other
IPM such as the maximum mean discrepancy (MMD) (Gretton et al., 2012). However, in practice, we found
that the closed-form Wasserstein distance achieves slightly better performance than MMD.
Reference Distribution for Regression. For a regression problem, except the change in the loss function,
we need also to define properly the reference distribution. Since our goal is to map all samples for all clients
into a common latent subspace, in which some structural information about regression problem is preserved.
As such, in order to reproduce the idea of using a Gaussian mixture model as a anchor distribution, we propose
to use an infinite number of Gaussian mixtures in which the distribution of xassociated to a response yis
5Under review as submission to TMLR
Table 2: Current personalised FL techniques that can be embedded in the proposed framework. The
parameters α,βistand for model weights while ω∈[0,1].
Algorithm Local model Local weights
FedAvg-FT g(i)
θi=gθi θi
L2GD g(i)
θi=gθiθi=ωα+ (1−ω)βi
FedRepg(i)
θi=g(i)
βi◦gαθi= [α,βi]
going to be mapped on a unit-variance Gaussian distribution whose mean depends uniquely on y. Formally,
we define the anchor distribution as
µy= N(m(y),I)
where m(y)is a vector of dimension dthat is uniquely defined. In practice, we consider as m(y)=ya+ (1−y)b
whereaandbare two distinct vectors in Rd.
When training FLIC, this means that for a client i, we can compute W2
2/parenleftig
µy,ν(y)
ϕi/parenrightig
based on the set of training
samples{x,y}. In practice, if for a given batch of samples we have a single sample of value x, then the
Wasserstein distance boils to ∥ϕi(x)−m(y)∥2
2, which means that we are going to map xto its corresponding
vector on the segment [a,b].
4 Algorithm
As detailed in (3), we perform personalisation under the FL paradigm by considering local model architectures
{g(i)
θi}i∈[b]and local weights θ1:b. As an example, we could resort to federated averaging with fine-tuning ( e.g.
FedAvg-FT (Collins et al., 2022)), model interpolation ( e.g.L2GD(Hanzely & Richtárik, 2020; Hanzely et al.,
2020)) or partially local models ( e.g. FedRep(Collins et al., 2021) or the works of Oh et al. (2022); Singhal
et al. (2021)). Table 2 details how these methods can be embedded into the proposed methodology.
In Algorithm 1, we detail the pseudo-code associated to a specific instance of the proposed methodology when
FedRepis resorted to learn model parameters {θi= [α,βi]}i∈[b]under the FL paradigm. In this setting, α
stand for the shared weights associated to the shared layers of a neural network architecture and βifor local
ones aiming at performing personalised classification. Besides these two learnable parameters, the algorithm
also learns the local embedding functions ϕ1:band the anchor distributions µ1:C. In practice, at a given epoch
tof the algorithm, a subset At+1⊆[b]of clients are selected to participate to the training process.
Those clients receive the current latent anchor distribution µ(t)
1:Cand the current shared representation
α(t). Then, each client locally updates ϕi,βiand her local versions of α(t)andµ(t)
1:C. Afterwards, clients
send back to the server an updated version of α(t)andµ(t)
1:C. Updated global parameters α(t+1)andµ(t+1)
1:C
are then obtained by weighted averaging of client updates on appropriate manifolds. The use of the
Wasserstein loss in (3)naturally leads to perform averaging of the local anchor distributions via a Wasserstein
barycenter; algorithmic details are provided in the next paragraph. In Algorithm 1, we use for the sake of
simplicity the notation DescStep (F(t,m)
i,·)to denote a (stochastic) gradient descent step on the function
F(t,m)
i =Fi(β(t,m)
i,ϕ(t,m)
i,α(t),µ(t)
1:C)with respect to a subset of parameters in (θi,ϕi,µ1:C). This subset is
specified in the second argument of DescStep . A fully detailed version of Algorithm 1 is provided in the
supplementary material, see Algorithm S2.
Note that we take into account key inherent challenges to federated learning namely partial participation and
communication bottleneck . Indeed, we cope with the client/server upload communication issue by allowing
each client to perform multiple steps (here M∈N∗) so that communication is only required every Mlocal
steps. This allows us to consider updating global parameters, locally, via only one stochastic gradient descent
step and hence avoiding the client drift phenomenon (Karimireddy et al., 2020).
6Under review as submission to TMLR
Algorithm 1 FLIC
Require: initialisation α(0),µ(0)
1:C,ϕ(0,0)
1:b,β(0,0)
1:b.
1:fort= 0toT−1do
2:Sample a set of At+1of active clients.
3:fori∈At+1do
4:The central server sends α(t)andµ(t)
1:CtoAt+1.
5:// Update local parameters
6:form= 0toM−1do
7:ϕ(t,m+1)
i←DescStep/parenleftig
F(t,m)
i,ϕ(t,m)
i/parenrightig
.
8:β(t,m+1)
i←DescStep/parenleftig
F(t,m)
i,β(t,m)
i/parenrightig
.
9:ϕ(t+1,0)
i =ϕ(t,M)
i.
10:β(t+1,0)
i =β(t,M)
i.
11:// Update global parameters
12:α(t+1)
i←DescStep/parenleftig
F(t,M)
i,α(t)/parenrightig
.
13:µ(t+1)
i,1:C←DescStep/parenleftig
F(t,M)
i,µ(t)
1:C/parenrightig
.
14:// Communication with the server
15:Sendα(t+1)
iandµ(t+1)
i,1:Cto central server.
16:// Averaging global parameters
17:α(t+1)=b
|At+1|/summationtext
i∈At+1wiα(t+1)
i
18:µ(t+1)
1:C←WassersteinBarycenter ({µ(t+1)
i,1:C})
Ensure: parameters α(T),µ(T)
1:C,ϕ(T,0)
1:b,β(T,0)
1:b.
Averaging Anchor Distributions. In this paragraph, we provide algorithmic details regarding steps
14 and 20 in Algorithm 1. For any c∈[C], the anchor distribution µcinvolves two learnable parameters
namely the mean vector vcand the covariance matrix Σc. Regarding the former, step 14 stands for a
(stochastic) gradient descent step aiming to obtain a local version of vcdenoted by v(t+1)
i,cand step 20 boils
down to compute v(t+1)
c = (b/|At+1|)/summationtext
i∈At+1ωiv(t+1)
i,c. To enforce the positive semi-definite constraint of the
covariance matrix, we rewrite it as Σc=LcL⊤
cwhereLc∈Rk×kand optimise in step 14 with respect to the
factorLcinstead of Σc. We can handle the gradient computation of the Bures distance in step 14 using the
work of Muzellec & Cuturi (2018); and obtain a local factor L(t+1)
i,cat iteration t. In step 20, we compute
L(t+1)
c = (b/|At+1|)/summationtext
i∈At+1ωiL(t+1)
i,cand set Σ(t+1)
c =L(t+1)
c[L(t+1)
c]⊤. Whenλ2= 0in(3), these mean vector
and covariance matrix updates exactly boil down to perform one stochastic (because of partial participation)
gradient descent step to solve the Wasserstein barycenter problem arg minµc/summationtextb
i=1ωiW2
2(µc,ν(c)
ϕi).
Pre-training ϕ1:C.Owing to the introduction of a reference distribution which carries semantical information,
each local feature transformation function can be pretrained by optimising the loss/summationtext
c∈YiW2
2/parenleftig
µc,ν(c)
ϕi/parenrightig
.
While in theory, pre-training may not be necessary, we believe that it helps reaching a better solution of the
federated learning problem as parameters αandβiare optimised starting from a better latent representation
ofνϕi. This is a phenomenon that has been observed in the context of fine-tuning (Kumar et al., 2022) or
domain generalisation (Rame et al., 2022).
Remark 3. Regarding privacy, FL ensures that raw data never leaves the client device. In our case, the
only information that is sent to the server is the local anchor distribution µ(t+1)
i,1:Cand the local version global
parameters α(t+1)
i. So we believe that the risk of privacy leakage of FLICis similar to the one of FedAvg
(McMahan et al., 2017b).
7Under review as submission to TMLR
Figure 2: Red dashed line indicates that the two embedded features ϕ⋆
i(x(j)
i)and ˆϕi(x(j)
i)come from the
same initial raw feature x(j)
i. On test data, mean prediction errors for both FedRepoperating on ϕ⋆
i(x(j)
i)
and Algorithm S3 (referred to as FLIC-FedRep ) are similar (≈4.98×10−5).
5 Non-Asymptotic Convergence Guarantees in a Simplified Setting
Deriving non-asymptotic convergence bounds for Algorithm 1 in the general case is challenging since the
considered C-class classification problem leads to jointly solving personalised FL and federated Wasserstein
barycenter problems. Regarding the latter, obtaining non-asymptotic convergence results is still an active
research area in the centralised learning framework (Altschuler et al., 2021). As such, we propose to analyse
a simpler regression framework where the anchor distribution is known beforehand and not learnt under the
FL paradigm. While we acknowledge that this theoretical analysis is based on a simplified setting of our
approach, it still offers insightful perspective and we leave the general case for future work.
More precisely, we assume that x(j)
i∼N(mi,Σi)withmi∈RkiandΣi∈Rki×kifori∈[b],j∈[ni].
In addition, we consider that the continuous scalar labels are generated via the oracle model y(j)
i=
(A⋆β⋆
i)⊤ϕ⋆
i(x(j)
i)whereA⋆∈Rk×d,β⋆
i∈Rdandϕ⋆
i(·)are ground-truth parameters and feature transformation
function, respectively. We make the following assumptions on the ground truth, which are inherited from
those of FedRed.
H1.(i) For any i∈[b],j∈[ni], embedded features ϕ⋆
i(x(j)
i)are distributed according to N(0k,Ik).
(ii) Ground-truth model parameters satisfy ∥β⋆
i∥2=√
dfori∈[b]andA⋆has orthonormal columns.
(iii)For anyt∈{0,...,T−1},|At+1|=b′with 1≤b′≤b, and if we select b′clients, their ground-truth
head parameters{β⋆
i}i∈At+1spanRd.
(iv) In(2),ℓ(·,·)is theℓ2norm,ωi= 1/b,θi= [A,βi]andg(i)
θi(x) = (Aβi)⊤xforx∈Rk.
UnderH1-(i), (Delon et al., 2022, Theorem 4.1) show that ϕ⋆
ican be expressed as a non-unique affine map
with closed-form expression. To align with the true latent anchor distribution µ= N(0k,Ik), we propose to
estimate ˆϕiby leveraging this closed-form mapping between N(mi,Σi)andµ. Because of the non-unicity of
ϕ⋆
i, we show in Theorem 1 that we can only recover it up to a matrix multiplication. Interestingly, Theorem 1
also proves that the global representation A(T)learnt via FedRep(see Algorithm S3 in Appendix) is able to
correct this feature mapping indetermination. Associated convergence behavior is illustrated in Figure 2 on a
toy example whose details are postponed to Appendix S2.
Theorem 1. Assume H1. Then, for any xi∈Rki, we have ˆϕi(xi) =Qϕ⋆
i(xi)whereQ∈Rk×kis of the form
diagk(±1). Under additional technical assumptions detailed in Appendix S2, we have for any t∈{0,...,T−1}
and with high probability,
dist(A(t+1),QA⋆)≤(1−κ)(t+1)/2dist(A(0),QA⋆),
whereκ∈(0,1)is detailed explicitly in Theorem S3 and distdenotes the principal angle distance.
8Under review as submission to TMLR
6 Numerical Experiments
For numerically validating the benefits associated to the proposed methodology FLIC, we consider toy
problems with different characteristics of heterogeneity; as well as experiments on real data, namely (i) a
digit classification problem from images of different sizes, (ii) an object classification problem from either
images or text captioning on clients, and (iii) a Brain-Computer Interfaces problem.
Baselines. Since the problem we are addressing is novel, no FL competitor in the literature can serve as a
baseline beyond local learning. However, we propose to modify the methodology proposed by Makhija et al.
(2022) and Collins et al. (2022) to make them applicable to clients with heterogeneous feature spaces. The
method proposed by Makhija et al. (2022) handles local representation models with different architectures.
Since their key idea is to align the latent representations of fixed-dimensionality inputs shared by the server
to all clients, we propose an alternative approach called FedHeNN, that works for clients with different feature
spaces, where we build a Representation Alignment Dataset (RAD) based on the largest feature space and
then prune it to obtain a lower-dimensional RAD for each client. We can also adapt the FedRepapproach
(Collins et al., 2022) to our setting by considering a local feature transformation followed by a shared global
representation model and a local classification head. This approach, denoted as HetFedRep , maps the input
data to fixed dimensionality before the shared global representation model. We can understand this approach
as a special case of FLiCwhere the local feature transformation are not enforced to align with the anchor
distributions. We adapted our network architecture to match the baselines by considering two variants.
Following Makhija et al. (2022), we treated all layers except the last as the representation learning module for
a fair comparison. Therefore, in our approach, the alignment applies to the penultimate layer, and the last
layer is the classifier layer. We call this model FLIC-Class. Additionally, we introduced another model, called
FLIC-HL, similar to FedRep, but with an extra trainable global hidden layer with αandβias parameters for
respectively the shared representation and classification layers.
Data Sets. We consider four different classification problems to assess the performances of our approach.
For all simulations, we assume prior probability shift e.geach client will have access to data of only specific
classes. The first problem is a toy problem with 20 classes and Gaussian class-conditional distributions, where
we conduct two sub-experiments: adding random spurious features and applying a random linear mapping,
both of random dimensionality on each client. Section S3.1 in the supplementary material provides more
details. The second problem involves digit classification using MNIST and USPS datasets, with dimensions
of28×28and16×16, respectively. Each client hosts a subset of either MNIST or USPS dataset. The third
experiment addresses a multimodal problem using a subset of the TextCaps dataset (Sidorov et al., 2020),
an image captioning dataset. We converted it into a 4-class classification problem with 12,000and3,000
examples for training and testing, respectively, based on caption text or image. We used pre-trained models
(Bert and ResNet) to embed the caption and image into 768-dimensional and 512-dimensional vectors. To
create heterogeneity, we randomly pruned 10% of features on each client. Each client hosts either image or
text embeddings. Finally, the fourth problem is a real medical problem denoted as Brain-Computer Interface
(BCI) which consists in classifying mental imagery EEG datasets into five classes. The datasets we have
considered is based on six datasets from the mental imagery MOABB data repository (Jayaram & Barachant,
2018) (details are given in Section S3.1 in the supplement). Each of those EEG datasets have been acquired
on different subjects, have different number of channels and classes. We used a vectorized channel-dependent
covariance matrices representations of each EEG signals as a feature (Yger et al., 2016; Barachant et al., 2022).
Hence, the dimensionality of the feature space is different for each dataset. We have considered each subject
in all the experiments as a client owning his own dataset. In practice, the number of training examples on
client ranges from 30 to 600 while the dimensionality of the features goes from 6 to 1,830.
Illustrating the need for a reference distribution. The main bottleneck for applying FL algorithms to
heterogeneous feature spaces is the lack of a common space. However, one can argue that this common space
can be created by projecting the data onto a joint common space. As we have claimed, we illustrated here
that this is not sufficient. To do so, we have considered 10 different classification problems with Gaussian
class-conditionals of random dimensionality ranging from 3to10. We have projected those class-conditionals
onto a common space using different projection algorithms, namely t-sne(Van der Maaten & Hinton, 2008)
and multi-dimensional scaling ( MDS) (Kruskal, 1964). The results are shown in Figure 3. We can note that
9Under review as submission to TMLR
Figure 3: Projecting Gaussian class-conditionals of 10 different classification problems, with random di-
mensionality ranging from 3to10. (left)t-sneprojection. (right) Multi Dimensional Scaling projection.
Different tones of a color represent the same class-conditionals of a given problem. From this figure, we
remark the overlapping of classes regardless of the algorithm used for projection ( t-sneuses a random
initialization and while MDSuses a deterministic ones).This emphasizes the need for a proper alignement of
the class-conditionals during projection.
jsut projecting into a common subspace without taking into account the semantic of the class-conditionals
leads to overlapping of classes. This emphasizes the need for a proper alignment of the class-conditionals
during projection, based on reference distribution as we propose in FLIC.
Experimental Setting. For the experimental analysis, we use the codebase of Collins et al. (2021) with
some modifications to meet our setting. For all experiments, we consider T= 50communication rounds
for all algorithms; and at each round, a client participation rate of r= 0.1. The number of local epochs
for training has been set to M= 10. As optimisers, we have used an Adam optimiser with a learning rate
of0.001for all problems and approaches. Further details are given in Section S3.3 in the supplement. For
each component of the latent anchor distribution, we consider a Gaussian with learnable mean vectors and
fixed Identity covariance matrix. As such, the Wasserstein barycenter computation boils down to simply
average the mean of client updates and for computing the third term in (3), we just sample from the Gaussian
distribution. Accuracies are computed as the average accuracy over all clients after the last epoch in which
all local models are trained.
Results on Toy Data Sets. Figure 4 depicts the performance, averaged over 5runs, of the different
algorithms with respect to the number of clients and when only 3classes are present in each client. For
both data sets, we can note that for the noisy feature setting, FLICimproves on FedHeNN of about 3%of
accuracy across the setting, performs better than local learning and is comparable to HetFedRep. For the
linear mapping setting, while HetFedRep fails, FLICachieves better than other approaches with a gain of
performance of about 4%while the gap tends to decrease as the number of clients increases. Interestingly,
FLIC-HL performs slightly better than FLIC-Class showing the benefit of using a shared representation
layerα. Figure 5 also illustrates how samples embedded with ϕievolve during training towards the anchor
distribution µ1:C. At start, they are clustered client-wise and then converge towards the relevant classwise
anchor distribution.
Results on Digits and TextCaps Data Sets. Performance, averaged over 3runs, of all algorithms on
the real-word problems are reported in Table 3. For the Digitsdata set problem, we first remark that in all
situations, FL algorithms performs a bit better than local learning. In addition, both variants of FLICachieve
better accuracy than competitors. Difference in performance in favor our FLICreaches 3%for the most
difficult problem. For the TextCaps data set, gains in performance of FLIC-HL reach about 4%across settings.
While FedHeNN andFLICalgorithms follow the same underlying principle (alignment of representation in a
latent space), our framework benefits from the use of the latent anchor distributions, avoiding the need of
10Under review as submission to TMLR
Figure 4: Performance of FLICand competitors on the toy data sets with respect to the number of clients.
(left) Gaussian classes in dimension k= 5with added noisy feature. (right) Gaussian classes in dimension
k= 30, transformed by a random linear mapping. Only 3classes are present on each client among the 20
possible ones.
Figure 5: . 2D t-sneprojection of 5classes partially shared by 3clients for the toy linear mapping dataset
after learning the local transformation functions for (left) 10 epochs, (middle) 50 epochs, (right) 100 epochs.
The three different markers represent the different clients while classes are represented by different color tones.
The⋆marker represents the class-conditional mean of the reference distribution. We note that training set
converges towards those means.
sampling from the original space. Instead, FedHeNN may fail as the sampling strategy of their RAD approach
suffers from the curse of dimensionality and does not properly lead to a successful feature alignment.
Results on Brain-Computer interfaces. For the BCI problem, performances are in Table 3. In this case,
theLocalmodel performance corresponds also to the usual BCI performance measure as models are usually
subject-specific. We can note that FLIC-HL achieves better performance than all competitors with a gain of
about 3%of accuracy compared to BCI baseline. In addition, we pave the way to learning BCI models with
heterogeneous datasets.
Preprocessing datasets to same dimensionality. Some preprocessing can be applied to the above
datasets so that standard "same dimensionality" FL methods can be considered. We can apply a simple
resizing of an image. For the TextCaps classification problem we can extract features from multimodal
embedder such as CLIP Radford et al. (2021b) in which embeddings between text and images are aligned
and of dimension 512. For the BCI problem, we used EEGs acquired from the set of common electrodes. We
have run the same experiments as above but using these preprocessings and also compared to plain FedRep
and report the results in Table 4. FLICstill achieves slightly better performance than competitors on 7 out
11Under review as submission to TMLR
Table 3: Performance over 3 runs of our FLICmodel and the competitors on some real-data problems ( Digits
andTextCaps data set).
Data sets (setting) Local FedHeNN HetFedRep FLIC -Class FLIC-HL
Digits (b= 100 , 3 Classes/client) 97.49 ±0.4 97.45±0.5 57.85±1.4 97.83±0.3 97.70±0.2
Digits (b= 100 , 5 Classes/client) 96.15 ±0.3 96.15±0.2 54.87±5.0 96.46±0.6 96.54±0.7
Digits (b= 200 , 3 Classes/client) 93.33 ±0.2 93.40±0.4 67.99±2.2 94.50±0.3 94.51±0.3
Digits (b= 200 , 5 Classes/client) 87.48 ±1.5 87.22±1.8 48.88±3.0 91.11±0.6 91.10±0.7
TextCaps ( b= 100 , 2 Classes/client) 84.19 ±0.8 83.99±0.7 87.05±0.7 89.14±1.1 89.68±0.7
TextCaps ( b= 100 , 3 Classes/client) 76.04 ±0.8 75.39±0.9 77.99±0.6 81.27±0.2 81.50±0.2
TextCaps ( b= 200 , 2 Classes/client) 83.78 ±1.8 83.89±1.7 85.48±1.5 87.73±0.8 87.74±1.3
TextCaps ( b= 200 , 3 Classes/client) 74.95 ±1.1 74.77±1.0 75.73±0.8 79.08±0.7 78.49±0.7
BCI (b= 54) 73.51 ±0.8 70.84±1.0 75.03±0.6 75.17±0.9 76.27±0.2
BCI (b= 40) 73.98 ±0.2 71.48±0.6 74.23±0.7 75.09±1.0 75.82±0.3
Table 4: Performance over 3 runs of our FLICmodel and the competitors on the same real-world problems
when processed so as to have same input sizes.
Data sets (setting) Local FedHeNN HetFedRep FedRep FLIC -Class FLIC-HL
Digits-Resize ( b= 100, 3 Classes) 97.70 97.62 92.63 95.76 98.14 98.05
Digits-Resize ( b= 100, 5 Classes) 96.36 96.37 94.90 96.07 96.94 96.91
Digits-Resize ( b= 200, 3 Classes) 93.62 93.56 69.21 92.93 94.73 94.54
Digits-Resize ( b= 200, 5 Classes) 87.74 87.49 69.27 94.57 91.40 91.16
TextCaps-Clip ( b= 100, 2 Classes) 96.55 96.44 96.45 82.31 96.59 96.65
TextCaps-Clip ( b= 100, 3 Classes) 94.38 94.21 94.13 76.47 94.34 94.21
TextCaps-Clip ( b= 200, 2 Classes) 96.27 96.16 96.36 83.33 96.55 96.44
TextCaps-Clip ( b= 200, 3 Classes) 93.78 93.55 93.74 73.23 94.04 93.88
BCI-common (b= 40) 71.24 70.63 72.09 71.50 72.17 72.12
of 9 settings, but more importantly, one should highlight the gain in performance on BCI problems when
considering all sensors at disposal.
7 Conclusion
We have introduced a novel and general framework, referred to as FLIC, for personalised FL when clients
have heterogeneous feature spaces. Under this framework, we proposed a FL algorithm involving two key
components: (i) a local feature embedding function; and (ii) a latent anchor distribution which allows to
match similar semantical information from each client. Experiments on relevant data sets have shown that
FLICachieves better performances than competing approaches. Finally, we provided theoretical support to
the proposed methodology, notably via a non-asymptotic convergence result.
Limitations and Broader impacts of FLIC.One main limitation of FLICis that it requires a common
feature space to be defined with an ad-hocdimensionality. While this dimensionality can be chosen by the
user, it is not clear how to select it in practice and has to be set to default value (in our case 64). In addition,
the proposed approach has a computational overhead due to the need of learning the local embedding
functions. FLIChas the potential to widen the scope of privacy-aware FL applications by allowing clients
to have heterogeneous feature spaces. This is particularly relevant for medical applications where data are
collected from different sources and may have different formats.
12Under review as submission to TMLR
References
Mokhtar Z Alaya, Maxime Bérar, Gilles Gasso, and Alain Rakotomamonjy. Theoretical guarantees for
bridging metric measure embedding and optimal transport. Neurocomputing , 468:416–430, 2022.
Jason Altschuler, Sinho Chewi, Patrik Robert Gerber, and Austin J Stromme. Averaging on the Bures-
Wasserstein manifold: dimension-free convergence of gradient descent. In Advances in Neural Information
Processing Systems , 2021.
David Alvarez-Melis and Tommi Jaakkola. Gromov-Wasserstein Alignment of Word Embedding Spaces. In
Conference on Empirical Methods in Natural Language Processing , pp. 1881–1890, 2018.
David Alvarez-Melis, Stefanie Jegelka, and Tommi S. Jaakkola. Towards Optimal Transport with Global
Invariances. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), International Conference on Artificial
Intelligence and Statistics , volume 89, pp. 1870–1879, 2019.
Nick Angelou, Ayoub Benaissa, Bogdan Cebere, William Clark, Adam James Hall, Michael A Hoeh, Daniel
Liu, Pavlos Papadopoulos, Robin Roehm, Robert Sandmann, et al. Asymmetric private set intersection
with applications to contact tracing and private vertical federated machine learning. arXiv preprint
arXiv:2011.09350 , 2020.
Arindam Banerjee, Srujana Merugu, Inderjit S. Dhillon, and Joydeep Ghosh. Clustering with Bregman
Divergences. Journal of Machine Learning Research , 6(58):1705–1749, 2005.
Alexandre Barachant, Quentin Barthélemy, Jean-Rémi King, Alexandre Gramfort, Sylvain Chevallier, Pedro
L. C. Rodrigues, Emanuele Olivetti, Vladislav Goncharenko, Gabriel Wagner vom Berg, Ghiles Reguig,
Arthur Lebeurrier, Erik Bjäreholt, Maria Sayu Yamamoto, Pierre Clisson, and Marie-Constance Corsi.
pyriemann/pyriemann: v0.3, July 2022. URL https://doi.org/10.5281/zenodo.7547583 .
Rajendra Bhatia, Tanvi Jain, and Yongdo Lim. On the Bures–Wasserstein distance between positive definite
matrices. Expositiones Mathematicae , 37(2):165–191, 2019.
Charlotte Bunne, David Alvarez-Melis, Andreas Krause, and Stefanie Jegelka. Learning Generative Models
across Incomparable Spaces. In International Conference on Machine Learning , volume 97, pp. 851–861,
2019.
Wei-Ning Chen, Christopher A Choquette Choo, Peter Kairouz, and Ananda Theertha Suresh. The
Fundamental Price of Secure Aggregation in Differentially Private Federated Learning. In International
Conference on Machine Learning , volume 162, pp. 3056–3089, 2022.
Yongxin Chen, Tryphon T Georgiou, and Allen Tannenbaum. Optimal transport for gaussian mixture models.
IEEE Access , 7:6269–6278, 2018.
Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting Shared Representations
for Personalized Federated Learning. In International Conference on Machine Learning , pp. 2089–2099,
2021.
Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. FedAvg with Fine Tuning: Local
Updates Lead to Representation Learning. In Advances in Neural Information Processing Systems , 2022.
Julie Delon, Agnes Desolneux, and Antoine Salmona. Gromov–Wasserstein distances between Gaussian
distributions. Journal of Applied Probability , 59(4):1178–1198, 2022.
Li Deng. The MNIST Database of Handwritten Digit Images for Machine Learning Research. IEEE Signal
Processing Magazine , 29(6):141–142, 2012.
Enmao Diao, Jie Ding, and Vahid Tarokh. HeteroFL: Computation and Communication Efficient Federated
Learning for Heterogeneous Clients. In International Conference on Learning Representations , 2021.
D.C Dowson and B.V Landau. The Fréchet distance between multivariate normal distributions. Journal of
Multivariate Analysis , 12(3):450–455, 1982.
13Under review as submission to TMLR
Paul-Ambroise Duquenne, Hongyu Gong, and Holger Schwenk. Multimodal and Multilingual Embeddings
for Large-Scale Speech Mining. In Advances in Neural Information Processing Systems , volume 34, pp.
15748–15761, 2021.
Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy. Foundations and
Trends in Theoretical Computer Science , 9(3–4):211–407, 2014.
Dashan Gao, Yang Liu, Anbu Huang, Ce Ju, Han Yu, and Qiang Yang. Privacy-preserving Heterogeneous
Federated Transfer Learning. In IEEE International Conference on Big Data (Big Data) , pp. 2552–2559,
2019.
Matthias Gelbrich. On a Formula for the L2 Wasserstein Metric between Measures on Euclidean and Hilbert
Spaces.Mathematische Nachrichten , 147(1):185–203, 1990.
Edouard Grave, Armand Joulin, and Quentin Berthet. Unsupervised alignment of embeddings with wasserstein
procrustes. In The 22nd International Conference on Artificial Intelligence and Statistics , pp. 1880–1890.
PMLR, 2019.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander Smola. A kernel
two-sample test. Journal of Machine Learning Research , 13(25):723–773, 2012. URL http://jmlr.org/
papers/v13/gretton12a.html .
Filip Hanzely and Peter Richtárik. Federated learning of a mixture of global and local models. arXiv preprint
arXiv:2002.05516 , 2020.
Filip Hanzely, Slavomír Hanzely, Samuel Horváth, and Peter Richtárik. Lower bounds and optimal algorithms
for personalized federated learning. arXiv preprint arXiv:2010.02372 , 2020.
Filip Hanzely, Boxin Zhao, and Mladen Kolar. Personalized Federated Learning: A Unified Framework and
Universal Optimization Techniques. arXiv preprint arXiv: 2102.09743 , 2021.
Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume Smith, and
Brian Thorne. Private federated learning on vertically partitioned data via entity resolution and additively
homomorphic encryption. arXiv preprint arXiv:1711.10677 , 2017.
Junyuan Hong, Haotao Wang, Zhangyang Wang, and Jiayu Zhou. Efficient Split-Mix Federated Learning for
On-Demand and In-Situ Customization. In International Conference on Learning Representations , 2022.
J. J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 16(5):550–554, 1994.
Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-Rank Matrix Completion Using Alternating
Minimization. In ACM Symposium on Theory of Computing , pp. 665–674, 2013.
Vinay Jayaram and Alexandre Barachant. Moabb: trustworthy algorithm benchmarking for bcis. Journal of
neural engineering , 15(6):066011, 2018.
YihanJiang, JakubKonevcn` y, KeithRush, andSreeramKannan. Improvingfederatedlearningpersonalization
via model agnostic meta learning. arXiv preprint arXiv:1909.12488 , 2019.
Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
K. A. Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G.L. D’Oliveira, Salim El
Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip B. Gibbons,
Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu,
Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konevcný, Aleksandra Korolova, Farinaz
Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock,
Ayfer Özgür, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song,
Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth
Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao.
Advances and Open Problems in Federated Learning. Foundations and Trends in Machine Learning , 14
(1–2):1–210, 2021a.
14Under review as submission to TMLR
Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Hubert
Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi,
Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson,
Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecný, Aleksandra Korolova,
Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard
Nock, Ayfer Özgür, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn
Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth
Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances
and open problems in federated learning. Foundations and Trends in Machine Learning , 14(1–2):1–210,
2021b. ISSN 1935-8237. doi: 10.1561/2200000083.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning. In Interna-
tional Conference on Machine Learning , pp. 5132–5143, 2020.
Mikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based meta-learning
methods. Advances in Neural Information Processing Systems , 32:5917–5928, 2019.
Joseph B Kruskal. Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis.
Psychometrika , 29(1):1–27, 1964.
Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can
distort pretrained features and underperform out-of-distribution. In The Tenth International Conference
on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022. URL
https://openreview.net/forum?id=UYneFzXSJWh .
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
Optimization in Heterogeneous Networks. In Machine Learning and Systems , volume 2, pp. 429–450, 2020.
Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen, and Qiang Yang. A Secure Federated Transfer Learning
Framework. IEEE Intelligent Systems , 35(4):70–82, 2020.
Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and Jiashi Feng. No Fear of Heterogeneity: Classifier
Calibration for Federated Learning with Non-IID Data. In Advances in Neural Information Processing
Systems, volume 34, 2021.
Zhihan Lv, Liang Qiao, Qingjun Wang, and Francesco Piccialli. Advanced Machine-Learning Methods for
Brain-Computer Interfacing. IEEE/ACM Transactions on Computational Biology and Bioinformatics , 18
(5):1688–1698, 2021.
Disha Makhija, Xing Han, Nhat Ho, and Joydeep Ghosh. Architecture Agnostic Federated Learning for
Neural Networks. In International Conference on Machine Learning , volume 162, pp. 14860–14870, 2022.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-
Efficient Learning of Deep Networks from Decentralized Data. In International Conference on Artificial
Intelligence and Statistics , volume 54, pp. 1273–1282, 2017a.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-
efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics , pp.
1273–1282, 2017b.
Facundo Mémoli. Gromov–Wasserstein distances and the metric approach to object matching. Foundations
of computational mathematics , 11(4):417–487, 2011.
Fan Mo, Hamed Haddadi, Kleomenis Katevas, Eduard Marin, Diego Perino, and Nicolas Kourtellis. PPFL:
Privacy-Preserving Federated Learning with Trusted Execution Environments. In International Conference
on Mobile Systems, Applications, and Services , pp. 94–108, 2021.
15Under review as submission to TMLR
Junki Mori, Isamu Teranishi, and Ryo Furukawa. Continual Horizontal Federated Learning for Heterogeneous
Data.arXiv preprint arXiv:2203.02108 , 2022.
Boris Muzellec and Marco Cuturi. Generalizing Point Embeddings using the Wasserstein Space of Elliptical
Distributions. In Advances in Neural Information Processing Systems , volume 31, 2018.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading Digits
in Natural Images with Unsupervised Feature Learning. In NIPS Workshop on Deep Learning and
Unsupervised Feature Learning , 2011.
Jaehoon Oh, SangMook Kim, and Se-Young Yun. FedBABU: Toward Enhanced Representation for Federated
Image Classification. In International Conference on Learning Representations , 2022.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable
Visual Models From Natural Language Supervision. In International Conference on Machine Learning ,
volume 139, pp. 8748–8763, 2021a.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural
language supervision. In International conference on machine learning , pp. 8748–8763. PMLR, 2021b.
Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Gallinari, and
Matthieu Cord. Diverse weight averaging for out-of-distribution generalization. In S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems ,
volume 35, pp. 10821–10836. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/
paper_files/paper/2022/file/46108d807b50ad4144eb353b5d0e8851-Paper-Conference.pdf .
Thomas Rippl, Axel Munk, and Anja Sturm. Limit laws of the empirical Wasserstein distance: Gaussian
distributions. Journal of Multivariate Analysis , 151:90–109, 2016.
Daniele Romanini, Adam James Hall, Pavlos Papadopoulos, Tom Titcombe, Abbas Ismail, Tudor Cebere,
Robert Sandmann, Robin Roehm, and Michael A Hoeh. PyVertical: A Vertical Federated Learning
Framework for Multi-headed SplitNN. arXiv preprint arXiv:2104.00489 , 2021.
Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, NY , 55(58-63):94, 2015.
Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. Personalized Federated Learning using
Hypernetworks. In International Conference on Machine Learning , volume 139, pp. 9489–9502, 2021.
Shreya Sharma, Chaoping Xing, Yang Liu, and Yan Kang. Secure and Efficient Federated Transfer Learning.
InIEEE International Conference on Big Data (Big Data) , pp. 2569–2576, 2019.
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image
captioning with reading comprehension. In European Conference on Computer Vision , pp. 742–758, 2020.
Karan Singhal, Hakim Sidahmed, Zachary Garrett, Shanshan Wu, John Rush, and Sushant Prakash. Federated
Reconstruction: Partially Local Federated Learning. In Advances in Neural Information Processing Systems ,
volume 34, pp. 11220–11232, 2021.
Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards Personalized Federated Learning. IEEE
Transactions on Neural Networks and Learning Systems , pp. 1–17, 2022.
Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning
research, 9(11), 2008.
R. Veldhuis. The centroid of the symmetrical Kullback-Leibler distance. IEEE Signal Processing Letters , 9
(3):96–99, 2002.
16Under review as submission to TMLR
Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science .
Cambridge University Press, 2018.
Cedric Villani. Optimal Transport: Old and New . Springer Berlin Heidelberg, 2008.
Hongteng Xu, Dixin Luo, Ricardo Henao, Svati Shah, and Lawrence Carin. Learning Autoencoders with
Relational Regularization. In International Conference on Machine Learning , volume 119, pp. 10576–10586,
2020.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated Machine Learning: Concept and
Applications. Transactions on Intelligent Systems and Technology , 10(2), 2019.
Rui Ye, Zhenyang Ni, Chenxin Xu, Jianyu Wang, Siheng Chen, and Yonina C Eldar. Fedfm: Anchor-based
feature matching for data heterogeneity in federated learning. arXiv preprint arXiv:2210.07615 , 2022.
Florian Yger, Maxime Berar, and Fabien Lotte. Riemannian approaches in brain-computer interfaces: a
review.IEEE Transactions on Neural Systems and Rehabilitation Engineering , 25(10):1753–1762, 2016.
Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wenchao Xu, and Feijie Wu. Parameterized Knowledge
Transfer for Personalized Federated Learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman
Vaughan (eds.), Advances in Neural Information Processing Systems , 2021a.
Lin Zhang, Yong Luo, Yan Bai, Bo Du, and Ling-Yu Duan. Federated learning for non-iid data via unified
feature learning and optimization objective alignment. In Proceedings of the IEEE/CVF international
conference on computer vision , pp. 4420–4428, 2021b.
Tailin Zhou, Jun Zhang, and Danny Tsang. FedFA: Federated Learning with Feature Anchors to Align
Feature and Classifier for Heterogeneous Data. arXiv preprint arXiv: 22211.09299 , 2022.
17Under review as submission to TMLR
Appendices
Personalised Federated Learning On Heterogeneous Feature Spaces
Notations and conventions. We denote byB(Rd)the Borelσ-field of Rd,M(Rd)the set of all Borel
measurable functions fonRdand∥·∥the Euclidean norm on Rd. Forµa probability measure on (Rd,B(Rd))
andf∈M(Rd)aµ-integrable function, denote by µ(f)the integral of fwith respect to (w.r.t.) µ. Let
µandνbe two sigma-finite measures on (Rd,B(Rd)). Denote by µ≪νifµis absolutely continuous
w.r.t.νanddµ/dνthe associated density. We say that ζis a transference plan of µandνif it is a
probability measure on (Rd×Rd,B(Rd×Rd))such that for all measurable set AofRd,ζ(A×Rd) =µ(A)and
ζ(Rd×A) =ν(A). We denote byT(µ,ν)the set of transference plans of µandν. In addition, we say that a
couple of Rd-random variables (X,Y )is a coupling of µandνif there exists ζ∈T(µ,ν)such that (X,Y )are
distributed according to ζ. We denote byP1(Rd)the set of probability measures with finite 1-moment: for all
µ∈P1(Rd),/integraltext
Rd∥x∥dµ(x)<∞. We denote byP2(Rd)the set of probability measures with finite 2-moment:
for allµ∈P 2(Rd),/integraltext
Rd∥x∥2dµ(x)<∞. We define the squared Wasserstein distance of order 2associated
with∥·∥for any probability measures µ,ν∈P2(Rd)by
W2
2(µ,ν) = inf
ζ∈T(µ,ν)/integraldisplay
Rd×Rd∥x−y∥2dζ(x,y).
By (Villani, 2008, Theorem 4.1), for all µ,νprobability measures on Rd, there exists a transference plan
ζ⋆∈T(µ,ν)such that for any coupling (X,Y )distributed according to ζ⋆,W2(µ,ν) =E[∥x−y∥2]1/2. This
kind of transference plan (respectively coupling) will be called an optimal transference plan (respectively
optimalcoupling)associatedwith W2. By(Villani,2008, Theorem6.16), P2(Rd)equippedwiththeWasserstein
distance W2is a complete separable metric space. For the sake of simplicity, with little abuse, we shall use
the same notations for a probability distribution and its associated probability density function. For n≥1,
we refer to the set of integers between 1andnwith the notation [n]. Thed-multidimensional Gaussian
probability distribution with mean µ∈Rdand covariance matrix Σ∈Rd×dis denoted by N(µ,Σ). Given
two matrices M,N∈Rk×d, the principal angle distance between the subspaces spanned by the columns of
MandNis given by dist(M,N ) =∥ˆM†
⊥ˆN∥2=∥ˆN†
⊥ˆM∥2where ˆM,ˆNare orthonormal bases of Span (M)
andSpan (N), respectively. Similarly, ˆM⊥,ˆN⊥are orthonormal bases of orthogonal complements Span (M)⊥
andSpan (N)⊥, respectively. This principal angle distance is upper bounded by 1, see (Jain et al., 2013,
Definition 1).
Outline. This supplementary material aims at providing the interested reader with a further understanding
of the statements pointed out in the main paper. More precisely, in Appendix S1, we support the proposed
methodology FLICwith algorithmic and theoretical details. In Appendix S2, we prove the main results
stated in the main paper. Finally, in Appendix S3, we provide further experimental design choices and show
complementary numerical results.
S1 Algorithmic and Theoretical Insights
In this section, we highlight alternative but limited ways to cope with feature space heterogeneity; and justify
the usage, in the objective function (3)of the main paper, of Wasserstein distances with empirical probability
distributions instead of true ones. In addition, we detail the general steps depicted Algorithm 1.
S1.1 Some Limited but Common Alternatives to Cope with Feature Space Heterogeneity
Depending on the nature of the spaces {Xi}i∈[b], the feature transformation functions {ϕi}i∈[b]can be either
known beforehand or more difficult to find. As an example, if for any i∈[b],Xi⊆X, then we can set mask
functions as feature transformation functions in order to only consider features that are shared across all
18Under review as submission to TMLR
the clients. Besides, we could consider multimodal embedding models to perform feature transformation
on each client Duquenne et al. (2021). For instance, if clients own either pre-processed images or text of
titles, descriptions and tags, then we can use the Contrastive Language-Image Pre-Training (CLIP) model as
feature transformation function Radford et al. (2021a). These two examples lead to the solving of a classical
(personalised) FL problem which can be performed using existing state-of-the-art approaches. However, when
the feature transformation functions cannot be easily found beforehand, solving the FL problem at stake
becomes more challenging and has never been addressed in the federated learning literature so far, up to the
authors’ knowledge.
S1.2 Use of Wasserstein Losses Involving Empirical Probability Distributions
Since the true probability distributions {ν(c)
ϕi;c∈Yi}i∈[b]are unknown a priori, we propose in the main paper
to estimate the latter using {ˆν(c)
ϕi;c∈Yi}i∈[b]and to replace W2
2/parenleftig
µc,ν(c)
ϕi/parenrightig
byW2
2/parenleftig
µc,ˆν(c)
ϕi/parenrightig
in the objective
function (3)in the main paper. As shown in the following result, this assumption is theoretically grounded
when the marginal distributions of the input features are Gaussian.
Theorem S2. For anyi∈[b]andc∈[C], letn(c)
i=|D(c)
i|where D(c)
idenotes the subset of the local data
setDionly involving observations associated to the label c. Besides, assume that ν(c)
ϕiis Gaussian with mean
vectorm(c)
i∈Rkand full-rank covariance matrix Σ(c)
i∈Rk×k. Then, we have in the limiting case n(c)
i→∞,
/radicalig
n(c)
i/parenleftig
W2
2/parenleftig
µc,ˆν(c)
ϕi/parenrightig
−W2
2/parenleftig
µc,ν(c)
ϕi/parenrightig/parenrightigin distribution
−−−−−−−−−−→ Z(c)
i,
whereZ(c)
i∼N(0,s(c)
i)ands(c)
i= 4(m(c)
i−vc)⊤Σ(c)
i(m(c)
i−vc)+2Tr(Σ(c)
iΣc)−4/summationtextk
j=1κ1/2
jr⊤
jΣ−1/2
cΣ(c)
iΣ1/2
crj,
with{κj,rj}j∈[k]standing for (eigenvalue, eigenvector) pairs of the symmetric covariance matrix Σ(c)
i.
Proof.The proof follows from (Rippl et al., 2016, Theorem 2.1) with the specific choices µ1=ν(c)
ϕi,µ2=µc
andˆµ1= ˆν(c)
ϕiwhich are defined in Section 3 in the main paper.
S1.3 Detailed Pseudo-Code for Algorithm 1
In Algorithm S2, we provide algorithmic support to Algorithm 1 in the main paper by detailing how to perform
each step. Note that we use the decomposition Σ =LL⊤to enfore the positive semi-definite constraint for
the covariance matrix Σ.
19Under review as submission to TMLR
Algorithm S2 Detailed version of FLICwhen using FedRep
Require: initialisation α(0),µ(0)
1:C= [Σ(0)
1:C,v(0)
1:C]with Σ(0)
c=L(0)
c[L(0)
c]⊤,ϕ(0,0)
1:b,β(0,0)
1:band step-size η≤¯ηfor some ¯η>0.
1:fort= 0toT−1do
2: Sample a set of At+1of active clients.
3:fori∈At+1do
4: The central server sends α(t)andµ(t)
1:CtoAt+1.
5: // Update local parameters
6: form= 0toM−1do
7: Sample a fresh batch I(i,m)
t+1ofn′
isamples with n′
i∈[ni].
8: Sample Z(j,t,m )
c∼µ(t)
cforj∈I(i,m)
t+1andc∈YiviaZ(j,t,m )
c =v(t)
c+L(t)
cξ(t,m)
iwhereξ(t,m)
i∼N(0k,Ik).
9:ϕ(t,m+1)
i = ϕ(t,m)
i−ηni
|I(i,m)
t+1|/summationdisplay
j∈I(i,m)
t+1∇ϕiℓ/parenleftbigg
y(j)
i,g(i)
[α(t),β(t,m)
i]/bracketleftig
ϕ(t,m)
i/parenleftig
x(j)
i/parenrightig/bracketrightig/parenrightbigg
−
ηλ1/summationdisplay
c∈Yi∇ϕiW2
2/parenleftbigg
µ(t)
c,ν(c)
ϕ(t,m)
i/parenrightbigg
.
10:β(t,m+1)
i←β(t,m)
i−ηni
|I(i,m)
t+1|/summationdisplay
j∈I(i,m)
t+1Bj
11: withBj=/braceleftbigg
∇βiℓ/parenleftbigg
y(j)
i,g(i)
[α(t),β(t,m)
i]/bracketleftig
ϕ(t,m)
i/parenleftig
x(j)
i/parenrightig/bracketrightig/parenrightbigg
−ηλ2/summationtext
c∈Yi∇βiℓ/parenleftbigg
y(j)
i,g(i)
[α(t),β(t,m)
i]/bracketleftig
Z(j,t,m )
c/bracketrightig/parenrightbigg/bracerightbigg
.
12:ϕ(t+1,0)
i=ϕ(t,M)
i.
13:β(t+1,0)
i=β(t,M)
i.
14: // Update global parameters
15:α(t+1)
i←α(t)−ηni
|I(i,M)
t+1|/summationdisplay
j∈I(i,M)
t+1Aj
16: withAj=/braceleftbigg
∇αℓ/parenleftbigg
y(j)
i,g(i)
[α(t),β(t,M)
i]/bracketleftig
ϕ(t,M)
i/parenleftig
x(j)
i/parenrightig/bracketrightig/parenrightbigg
−ηλ2/summationtext
c∈Yi∇αℓ/parenleftbigg
y(j)
i,g(i)
[α(t),β(t,M)
i]/bracketleftig
Z(j,t,M )
c/bracketrightig/parenrightbigg/bracerightbigg
.
17: forc= 1toCdo
18: Update ˆm(c,t)
i,ˆΣ(c,t)
iusingϕ(t,M)
i.
19:v(t+1)
i,c =v(t)
c−ηλ1∇vc/vextenddouble/vextenddouble/vextenddoublev(t)
c−ˆm(c,t)
i/vextenddouble/vextenddouble/vextenddouble2
−ηλ2/summationtext
c∈Yini
|I(i,m)
t+1|/summationdisplay
j∈I(i,m)
t+1∇vcℓ/parenleftbigg
y(j)
i,g(i)
[α(t),β(t,M)
i]/bracketleftbig
Z(j,t,M )
c/bracketrightbig/parenrightbigg
.
20:L(t+1)
i,c =L(t)
c−ηλ1∇LcB2/parenleftig
L(t)
c[L(t)
c]⊤,ˆΣ(c,t)
i/parenrightig
−ηλ2/summationtext
c∈Yini
|I(i,m)
t+1|/summationdisplay
j∈I(i,m)
t+1∇Lcℓ/parenleftbigg
y(j)
i,g(i)
[α(t),β(t,M)
i]/bracketleftbig
Z(j,t,M )
c/bracketrightbig/parenrightbigg
.
21: // Communication with the server
22: Sendα(t+1)
i,v(t+1)
i,1:CandL(t+1)
i,1:Cto central server.
23: // Averaging global parameters
24:α(t+1)=b
|At+1|/summationtext
i∈At+1wiα(t+1)
i.
25: forc= 1toCdo
26:v(t+1)
c = (b/|At+1|)/summationtext
i∈At+1ωiv(t+1)
i,c.
27:L(t+1)
c = (b/|At+1|)/summationtext
i∈At+1ωiL(t+1)
i,cand set Σ(t+1)
c =L(t+1)
c [L(t+1)
c ]⊤.
Ensure: parameters α(T),µ(T)
1:C,ϕ(T,0)
1:b,β(T,0)
1:b.
S1.4 Additional Algorithmic Insights
Scalability. When the number of classes Cis large, both local computation and communication costs are
increased. In this setting, we propose to partition all the classes into Cmeta≪Cmeta-classes and consider
reference measures {µc}c∈[Cmeta]associated to these meta-classes. As an example, if we are considering a
dataset made of features associated to animals, the meta-class refers to an animal ( e.g.a dog) and the class
refers to a specific breed ( e.g.golden retriever).
Privacy Consideration. As other standard (personalised) FL algorithms, FLICsatisfies first-order privacy
guarantees by not allowing raw data exchanges but rather exchanges of local Gaussian statistics. Note that
FLICstands for a post-hoc approach and can be combined with other privacy/confidentiality techniques such
20Under review as submission to TMLR
as differential privacy Dwork & Roth (2014), secure aggregation via secure multi-party computation Chen
et al. (2022) or trusted execution environments Mo et al. (2021).
Inference on New Clients. When a client who has not participated to the training procedure appears,
there is no need to re-launch a potentially costly federated learning procedure. Instead, the server sends
the shared parameters {α(T),µ(T)
1:C}to the new client and the latter only needs to learn the local parameters
{ϕi,βi}.
S2 Proof of Theorem 1
This section aims at proving Theorem 1 in the main paper. To this end, we first provide in Appendix S2.1 a
closed-form expression for the estimated embedded features based on the features embedded by the oracle.
Then, in Appendix S2.3, we show technical lemmata that will be used in Appendix S2.2 to show Theorem 1.
To prove our results, we consider the following set of assumptions.
H1.(i)For anyi∈[b],j∈[ni], ground-truth embedded features ϕ⋆
i(x(j)
i)are distributed according to
N(0k,Ik).
(ii) Ground-truth model parameters satisfy ∥β⋆
i∥2=√
dfori∈[b]andA⋆has orthonormal columns.
(iii)For anyt∈ {0,...,T−1},|At+1|=⌊rb⌋with 1≤ ⌊rb⌋ ≤b, and if we select ⌊rb⌋clients, their
ground-truth head parameters {β⋆
i}i∈At+1spanRd.
(iv)In (2) in the main paper, ℓ(·,·)is theℓ2norm,ωi= 1/b,θi= [A,βi]andg(i)
θi(x) = (Aβi)⊤xforx∈Rk.
S2.1 Estimation of the Feature Transformation Functions
As in Section 4 in the main paper, we assume that x(j)
i∼N(mi,Σi)withmi∈RkiandΣi∈Rki×kifor
i∈[b],j∈[ni]. In addition, we consider that the continuous scalar labels are generated via the oracle
modely(j)
i= (A⋆β⋆
i)⊤ϕ⋆
i(x(j)
i)whereA⋆∈Rk×d,β⋆
i∈Rdandϕ⋆
i(·)are ground-truth parameters and feature
transformation function, respectively. Under H1-(i), the oracle feature transformation functions {ϕ⋆
i}i∈[b]
are assumed to map ki-dimensional Gaussian distributions N(mi,Σi)to a common k-dimension Gaussian
N(0k,Ik). As shown in (Delon et al., 2022, Theorem 4.1), there exist closed-form expressions for {ϕ⋆
i}i∈[b],
which can be shown to stand for solutions of a Gromov-Wasserstein problem restricted to Gaussian transport
plans. More precisely, these oracle feature transformation stand for affine maps and are of the form, for any
i∈[b],
ϕ⋆
i/parenleftig
x(j)
i/parenrightig
=/bracketleftig
˜I(i,⋆)
k(D(k)
i)−1/20k,ki−k/bracketrightig/parenleftig
x(j)
i−mi/parenrightig
,
where ˜I(i,⋆)
k=diagk(±1)is ak-dimensional diagonal matrix with diagonal elements in {−1,1},Σi=PiDiP⊤
i
is the diagonalisation of ΣiandD(k)
istands for the restriction of Dito the first kcomponents. In the
sequel, we assume that all oracle feature transformation functions share the same randomness, that is
˜I(i,⋆)
k=˜I⋆
k= diagk(±1).
For the sake of simplicity, we assume that we know the true latent distribution of ϕ⋆
i(x(j)
i)and as such consider
a pre-fixed reference latent distribution that equals the latter, that is µ= N(0k,Ik). Since we know from
(Delon et al., 2022, Theorem 4.1) that there exist mappings between Gaussian distributions with supports
associated to different metric spaces, we propose an estimate for the ground-truth feature transformation
functions defined by for any i∈[b],
ˆϕi/parenleftig
x(j)
i/parenrightig
=/bracketleftig
˜Ik(D(k)
i)−1/20k,ki−k/bracketrightig/parenleftig
x(j)
i−mi/parenrightig
,
where ˜Ik=diagk(±1). By noting that ˜Ik=Q˜I⋆
k, whereQ∈Rk×kis a diagonal matrix of the form diagk(±1),
it follows that
ˆϕi/parenleftig
x(j)
i/parenrightig
=Qϕ⋆
i/parenleftig
x(j)
i/parenrightig
. (S1)
21Under review as submission to TMLR
Algorithm S3 FLIC-FedRep for linear regression and Gaussian features
Require: step sizeη, number of outer iterations T, participation rate r∈(0,1), diagonalizations Σi=
PiDiP⊤
isorting eigenvalues in decreasing order.
1:// Estimation of embedded features
2:For each client i∈[b], set ˆϕi/parenleftig
x(j)
i/parenrightig
=/bracketleftig
˜Ik(D(k)
i)−1/20k,ki−k/bracketrightig/parenleftig
x(j)
i−mi/parenrightig
.
3:// Initialisation A(0)
4:Each client i∈[b]sendsZi= (1/ni)/summationtextni
j=1(y(j)
i)2ˆϕi/parenleftig
x(j)
i/parenrightig
[ˆϕi/parenleftig
x(j)
i/parenrightig
]⊤to the central server.
5:The central server computes UDU⊤←rank−dSVD/parenleftig
(1/b)/summationtextb
i=1Zi/parenrightig
.
6:The central server initialises A(0)=U.
7:fort= 0toT−1do
8:Sample a set of At+1of active clients such that |At+1|=⌊rb⌋.
9:fori∈At+1do
10:The central server sends A(t)toAt+1.
11:// Update local parameters
12:β(t+1)
i = arg minβi/summationtextni
j=1/parenleftig
y(j)
i−β⊤
i[A(t)]⊤ˆϕi/parenleftig
x(j)
i/parenrightig/parenrightig2
.
13:// Update global parameters
14:A(t+1)
i =A(t)−η∇A/summationtextni
j=1/parenleftig
y(j)
i−[β(t+1)
i]⊤A⊤ˆϕi/parenleftig
x(j)
i/parenrightig/parenrightig2
.
15:// Communication with the server
16:SendA(t+1)
ito the central server.
17:// Averaging and orthogonalisation of global parameter
18: ¯A(t+1)=1
⌊rb⌋/summationtext
i∈At+1A(t+1)
i.
19:A(t+1),R(t+1)←QR/parenleftbig¯A(t+1)/parenrightbig
.
Ensure: parameters A(T),β(T)
1:b.
In Appendix S2.2, the equation (S1)will allow us to relate the ground-truth labels y(j)
i= (A⋆β⋆
i)⊤ϕ⋆
i(x(j)
i)
with estimated predictions ˆy(j)
i= (A(T)β(T)
i)⊤ˆϕi(x(j)
i)via Algorithm S3 starting from the same embedded
features.
S2.2 Proof of Theorem 1
LetB∈Rb×dthe matrix having local model parameters {βi}i∈[b]as columns and denote by BAt+1∈R⌊rb⌋×d
its restriction to the row set defined by At+1where|At+1|=⌊rb⌋for somer∈(0,1]. For the sake of
simplicity, we assume in the sequel that all clients have the same number of data points that is for any
i∈[b],ni=n. For random batches of samples {(x(j)
i,y(j)
i),j∈[n]}i∈[⌊rb⌋], we define similarly to Collins
et al. (2021); Jain et al. (2013), the random linear operator A:R⌊rb⌋×d→R⌊rb⌋nfor anyM∈R⌊rb⌋×das
A(M) = [⟨ei(ϕ⋆
i(x(j)
i))⊤,M⟩]1≤i≤⌊rb⌋,1≤j∈n, whereeistands for the i-th standard vector of R⌊rb⌋. Using
these notations, it follows from Algorithm S3 that for any t∈ {0,...,T−1}, the model parameters
θ(t+1)
i = [A(t+1),β(t+1)
i]are computed as follows:
B(t+1)
At+1= arg min
BAt+11
⌊rb⌋n/vextenddouble/vextenddouble/vextenddoubleA(t+1)/parenleftig
B⋆
At+1[A⋆]⊤−BAt+1[A(t)]⊤Q/parenrightig/vextenddouble/vextenddouble/vextenddouble2
, (S2)
¯A(t+1)=¯A(t)−η
⌊rb⌋n/bracketleftig
(A(t+1))†A(t+1)/parenleftig
B⋆
At+1[A⋆]⊤−B(t+1)
At+1[A(t)]⊤Q/parenrightig/bracketrightig⊤
QB(t+1)
At+1,
A(t+1),R(t+1)←QR/parenleftig
¯A(t+1)/parenrightig
, (S3)
22Under review as submission to TMLR
whereA(t+1)stands for a specific instance of Adepending on the random subset of active
clients available at each round and A†is the adjoint operator of Adefined byA†(M) =/summationtext
i∈[⌊rb⌋]/summationtextn
i=1[⟨ei(ϕ⋆
i(x(j)
i))⊤,M⟩]ei(ϕ⋆
i(x(j)
i)).
The update in (S2) admits a closed-form expression as shown in the following lemma.
Lemma S1. For anyt∈...0,...,T−1, we have
B(t+1)
At+1=B⋆
At+1[A⋆]⊤QA(t)−F(t),
whereF(t)is defined in (S12),A(t)is defined in (S3)andB(t)
Atis defined in (S2).
Proof.The proof follows from the same steps as in (Collins et al., 2021, Proof of Lemma 1) using (S2).
UnderH1, we have the following non-asymptotic convergence result.
Theorem S3. Assume H1. Then, for any xi∈Rki, we have ˆϕi(xi) =Qϕ⋆
i(xi)whereQ∈Rk×kis of the
form diagk(±1). DefineE0=dist(A(0),QA⋆). Assume that n≥c(d3log(⌊rb⌋))/E2
0+d2k/(E2
0⌊rb⌋)for some
absolute constant c>0. Then, for any t∈{0,...,T−1},η≤1/(4¯σ2
max,⋆)and with high probability at least
1−e−110k−e−110d2log(⌊rb⌋), we have
dist(A(t+1),QA⋆)≤(1−κ)(t+1)/2dist(A(0),QA⋆),
whereA(t)is computed via Algorithm S3, distdenotes the principal angle distance and κ∈(0,1)is defined as
κ= 1−ηE0¯σ2
min,⋆/2.
Proof.The proof follows first by plugging Lemma S3, Lemma S8 and Lemma S9 into Lemma S2. Then, we
use the same technical arguments and steps as in (Collins et al., 2021, Proof of Lemma 6).
S2.3 Technical Lemmata
In this section, we provide a set of useful technical lemmata to prove our main result in Appendix S2.2.
Notations. We begin by defining some notations that will be used in the sequel. For any t∈{0,...,T−1},
we define
Z(t+1)=B(t+1)
At+1[A(t)]⊤Q−B⋆
At+1[A⋆]⊤. (S4)
In addition, let
G(t)=
G(t)
11···G(t)
1d.........
G(t)
d1···G(t)
dd
,C(t)=
C(t)
11···C(t)
1d.........
C(t)
d1···C(t)
dd
,D(t)=
D(t)
11···D(t)
1d.........
D(t)
d1···D(t)
dd
,
where forp,q∈[d],
G(t)
pq=1
n/summationdisplay
i∈At+1n/summationdisplay
j=1ei/parenleftig
ϕ⋆
i(x(j)
i)/parenrightig⊤
Qa(t)
p[a(t)
q]⊤Qϕ⋆
i(x(j)
i)e⊤
i, (S5)
C(t)
pq=1
n/summationdisplay
i∈At+1n/summationdisplay
j=1ei/parenleftig
ϕ⋆
i(x(j)
i)/parenrightig⊤
Qa(t)
p[a⋆
q]⊤Qϕ⋆
i(x(j)
i)e⊤
i, (S6)
D(t)
pq=⟨a(t)
p,a⋆
q⟩I⌊rb⌋, (S7)
23Under review as submission to TMLR
witha(t)
p∈Rkstanding for the p-th column of A(t)∈Rk×d; anda⋆
p∈Rkstanding for the p-th column of
A⋆∈Rk×d. Finally, we define for any i∈At+1,
Πi=1
nn/summationdisplay
j=1ϕ⋆
i(x(j)
i)[ϕ⋆
i(x(j)
i)]⊤, (S8)
(G(t))i= [A(t)]⊤QΠiQA(t), (S9)
(C(t))i= [A(t)]⊤QΠiQA⋆, (S10)
(D(t))i= [A(t)]⊤QA⋆. (S11)
Using these notations, we also define ˜β⋆= [(β⋆
1)⊤,..., (β⋆
d)⊤]⊤∈R⌊rb⌋dand
F(t)= [([G(t)]−1(G(t)D(t)−C(t))˜β⋆)1,..., ([G(t)]−1(G(t)D(t)−C(t))˜β⋆)d]. (S12)
Technical results. To prove our main result in Theorem S3, we begin by providing a first upper bound on
the quantity of interest namely dist/parenleftbig
A(t+1),QA⋆/parenrightbig
. This is the purpose of the next lemma.
Lemma S2. For anyt∈{0,...,T−1}andη>0, we have
dist/parenleftig
A(t+1),QA⋆/parenrightig
≤C1+C2, ,
where
C1=/vextenddouble/vextenddouble/vextenddouble/vextenddouble[A⋆
⊥]⊤QA(t)/parenleftbigg
Id−η
⌊rb⌋[B(t+1)
At+1]⊤B(t+1)
At+1/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftig
R(t+1)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2, (S13)
C2=η
⌊rb⌋/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg1
n[A⋆
⊥]⊤(QA(t+1))†A(t+1)/parenleftig
Z(t+1)/parenrightig
Q−Z(t+1)/parenrightbigg⊤
B(t+1)
At+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftig
R(t+1)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,(S14)
whereA(t)is defined in (S3),B(t)
Atis defined in (S2),Z(t)is defined in (S4)andR(t)comes from the QR
factorisation of ¯A(t), see step 20 in Algorithm S3.
Proof.The proof follows from the same steps as in (Collins et al., 2021, Proof of Lemma 6) and by noting
that dist(A(t),QA⋆) = dist(QA(t),A⋆)fort∈{0,...,T−1}.
We now have to control the terms C1andC2. For the sake of clarity, we split technical results aiming to
upper bound of C1andC2in two different paragraphs.
Control of C1.
Lemma S3. Assume H1. Letδd=cd3/2/radicalbig
log(⌊rb⌋)/n1/2for some absolute constant c>0. Then, for any
t∈{0,...,T−1}, with probability at least 1−e−111k2log(⌊rb⌋), we have for δd≤1/2andη≤1/(4¯σ2
max,⋆)
C1≤/bracketleftbigg
≤1−η/parenleftig
1−dist/parenleftig
A(0),QA⋆/parenrightig/parenrightig
¯σ2
min,⋆+ 2ηδd
1−δd¯σ2
max/bracketrightbigg
dist/parenleftig
A(t),QA⋆/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftig
R(t+1)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,
where ¯σ2
min,¯σ2
maxare defined in (S15)-(S16),C1is defined in (S13),A(t)is defined in (S3)andR(t)comes
from the QR factorisation of ¯A(t), see step 20 in Algorithm S3.
Proof.Using Cauchy-Schwarz inequality, we have
C1≤/vextenddouble/vextenddouble/vextenddouble(A⋆
⊥)⊤QA(t)/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddoubleId−η
⌊rb⌋[B(t+1)
At+1]⊤B(t+1)
At+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftig
R(t+1)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
= dist/parenleftig
A(t),QA⋆/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddoubleId−η
⌊rb⌋[B(t+1)
At+1]⊤B(t+1)
At+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftig
R(t+1)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2.
24Under review as submission to TMLR
Define the following minimum and maximum singular values:
¯σ2
min,⋆= min
A⊆[b],|A|=⌊rb⌋σmin/parenleftigg
1/radicalbig
⌊rb⌋B⋆
A/parenrightigg
(S15)
¯σ2
max,⋆= min
A⊆[b],|A|=⌊rb⌋σmax/parenleftigg
1/radicalbig
⌊rb⌋B⋆
A/parenrightigg
. (S16)
Using (Collins et al., 2021, Proof of Lemma 6, equations (67)-(68)), we have for δd≤1/2whereδdis defined
in Lemma S4 and η≤1/(4¯σ2
max,⋆),
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleId−η
⌊rb⌋[B(t+1)
At+1]⊤B(t+1)
At+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤1−η/parenleftig
1−dist/parenleftig
A(0),QA⋆/parenrightig/parenrightig
¯σ2
min,⋆+ 2ηδd
1−δd¯σ2
max,⋆,
with probability at least 1−e−111k2log(⌊rb⌋)The proof is concluded by combining the two previous bounds.
Control of C2.We begin by showing four intermediary results gathered in the next four lemmata.
Lemma S4. Assume H1. Letδd=cd3/2/radicalbig
log(⌊rb⌋)/n1/2for some absolute constant c>0. Then, for any
t∈{0,...,T−1}, with probability at least 1−e−111k3log(⌊rb⌋), we have
/vextenddouble/vextenddouble/vextenddouble[G(t)]−1/vextenddouble/vextenddouble/vextenddouble
2≤1
1−δd,
whereG(t)is defined in (S5).
Proof.The proof stands as a straightforward extension of (Collins et al., 2021, Proof of Lemma 2) by noting
that the random variable Qϕ⋆
i(x(j)
i) =ˆϕi(x(j)
i)is sub-Gaussian under H1-(i); and as such is omitted.
Lemma S5. Assume H1. Letδd=cd3/2/radicalbig
log(⌊rb⌋)/n1/2for some absolute constant c>0. Then, for any
t∈{0,...,T−1}, with probability at least 1−e−111k2log(⌊rb⌋), we have
/vextenddouble/vextenddouble/vextenddouble(G(t)D(t)−C(t))B⋆
At/vextenddouble/vextenddouble/vextenddouble
2≤δd/vextenddouble/vextenddoubleB⋆
At/vextenddouble/vextenddouble
2dist/parenleftig
A(t),QA⋆/parenrightig
,
whereG(t)is defined in (S5),D(t)is defined in (S7),C(t)is defined in (S6)andA(t)in(S3).
Proof.Without loss of generality and to ease notation, we remove the superscript (t)in the proof and re-index
the indexes of clients in At+1. LetH=GD−C. From(S8),(S9),(S10)and(S11), it follows, for any
i∈[⌊rb⌋], that
Hi=GiDi−Ci=A⊤QΠiQ(AA⊤−Ik)QA⋆.
Hence, by using the definition of H, we have
∥(GD−C)β⋆∥2
2=⌊rb⌋/summationdisplay
i=1/vextenddouble/vextenddoubleHiβ⋆
i/vextenddouble/vextenddouble2
2≤⌊rb⌋/summationdisplay
i=1/vextenddouble/vextenddoubleHi/vextenddouble/vextenddouble2
2∥β⋆
i∥2≤d
⌊rb⌋∥B⋆∥2
2⌊rb⌋/summationdisplay
i=1/vextenddouble/vextenddoubleHi/vextenddouble/vextenddouble2
2,
where the last inequality follows almost surely from H1-(iii). As in (Collins et al., 2021, Proof of Lemma 3),
we then define for any j∈[n], the vectors
u(j)
i=1√n[A⋆]⊤(AA⊤−Ik)Qϕ⋆
i(x(j)
i),
v(j)
i=1√nA⊤Qϕ⋆
i(x(j)
i).
25Under review as submission to TMLR
LetSd−1denotes the d-dimensional unit spheres. Then, by (Vershynin, 2018, Corollary 4.2.13), we can define
Nd, the 1/4-net overSd−1such that|Nd|≤9d. Therefore, by using (Vershynin, 2018, Equation (4.13)), we
have
/vextenddouble/vextenddoubleHi/vextenddouble/vextenddouble2
2≤2 max
z,y∈Ndn/summationdisplay
j=1⟨z,u(j)
i⟩⟨v(j)
i,y⟩.
Sinceϕ⋆
i(x(j)
i)is a standard Gaussian vector, it is sub-Gaussian and therefore ⟨z,u(j)
i⟩and⟨v(j)
i,y⟩are
sub-Gaussian with norms ∥1√n[A⋆]⊤(AA⊤−Ik)Q∥2= (1/√n)dist(A,QA⋆)and(1/√n), respectively. In
addition, we have
E/bracketleftig
⟨z,u(j)
i⟩⟨v(j)
i,y⟩/bracketrightig
=1
nE/bracketleftbigg
z⊤1√n[A⋆]⊤(AA⊤−Ik)Qϕ⋆
i(x(j)
i)[ϕ⋆
i(x(j)
i)]⊤QAy/bracketrightbigg
=1
nz⊤1√n[A⋆]⊤(AA⊤−Ik)Ay
= 0,
where we have used the fact that E[ϕ⋆
i(x(j)
i)[ϕ⋆
i(x(j)
i)]⊤] = 1,Q2= Ikand(AA⊤−Ik)A= 0. The rest of the
proof is concluded by using the Bernstein inequality by following directly the steps detailed in (Collins et al.,
2021, Proof of Lemma 3, see equations (35) to (39)).
Lemma S6. Assume H1. Letδd=cd3/2/radicalbig
log(⌊rb⌋)/n1/2for some absolute constant c>0. Then, for any
t∈[T], with probability at least 1−e−111k2log(⌊rb⌋), we have
/vextenddouble/vextenddouble/vextenddoubleF(t)/vextenddouble/vextenddouble/vextenddouble
F≤δd
1−δd/vextenddouble/vextenddoubleB⋆
At/vextenddouble/vextenddouble
2dist/parenleftig
A(t),QA⋆/parenrightig
,
whereF(t)is defined in (S12)andA(t)in(S3).
Proof.By the Cauchy-Schwarz inequality, we have/vextenddouble/vextenddoubleF(t)/vextenddouble/vextenddouble
F=/vextenddouble/vextenddouble[G(t)]−1(G(t)D(t)−C(t))B⋆
At/vextenddouble/vextenddouble
2≤
δd/vextenddouble/vextenddoubleB⋆
At/vextenddouble/vextenddouble
2≤/vextenddouble/vextenddouble[G(t)]−1/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble(G(t)D(t)−C(t))B⋆
At/vextenddouble/vextenddouble
2≤δd/vextenddouble/vextenddoubleB⋆
At/vextenddouble/vextenddouble
2. The proof is concluded by combining
the upper bounds given in Lemma S4 and Lemma S5.
Lemma S7. Assume H1 and letδ′
d=cd√
k//radicalbig
⌊rb⌋nfor some absolute positive constant c. For anyt∈[T]
and whenever δ′
d≤d, we have with probability at least 1−e−110k−e−110d2log(⌊rb⌋)
1
⌊rb⌋/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg1
nQ(A(t))†A(t)/parenleftig
Z(t)/parenrightig
Q−Z(t)/parenrightbigg⊤
B(t)
At/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤δ′
ddist/parenleftig
A(t),QA⋆/parenrightig
,
whereB(t)
Atis defined in (S2)andZ(t)is defined in (S4).
Proof.Lett∈[T]. Note that we have
/parenleftbigg1
nQ(A(t))†A(t)/parenleftig
Z(t)/parenrightig
Q−Z(t)/parenrightbigg⊤
B(t)
At=1
n/summationdisplay
i∈Atm/summationdisplay
j=1⟨Qϕ⋆
i(x(j)
i),z(t)
i⟩Qϕ⋆
i(x(j)
i)/bracketleftig
β(t)
i/bracketrightig⊤
−z(t)
i/bracketleftig
β(t)
i/bracketrightig⊤
.
LetSk−1andSd−1denote the k-dimensional and d-dimensional unit spheres, respectively. Then, by
(Vershynin, 2018, Corollary 4.2.13), we can define NkandNd,1/4-nets overSk−1andSd−1, respectively,
such that|Nk|≤9kand|Nd|≤9d. Therefore, by using (Vershynin, 2018, Equation (4.13)), we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg1
nQ(A(t))†A(t)/parenleftig
Z(t)/parenrightig
Q−Z(t)/parenrightbigg⊤
B(t)
At/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2
26Under review as submission to TMLR
= 2 max
u∈Nd,v∈Nku⊤
1
n/summationdisplay
i∈Atm/summationdisplay
j=1⟨Qϕ⋆
i(x(j)
i),z(t)
i⟩Qϕ⋆
i(x(j)
i)/bracketleftig
β(t)
i/bracketrightig⊤
−z(t)
i/bracketleftig
β(t)
i/bracketrightig⊤
v
= 2 max
u∈Nd,v∈Nk1
n/summationdisplay
i∈Atm/summationdisplay
j=1⟨Qϕ⋆
i(x(j)
i),z(t)
i⟩⟨u,Qϕ⋆
i(x(j)
i)⟩⟨β(t)
i,v⟩−⟨u,z(t)
i⟩⟨β(t)
i,v⟩. (S17)
In order to control (S17)using Bernstein inequality as in Lemma S5, we need to characterise, in particular,
the sub-Gaussianity of ⟨u,z(t)
i⟩and⟨β(t)
i,v⟩which require a bound on ∥z(t)
i∥and∥β(t)
i∥, respectively. From
Lemma S1, we have [β(t)
i]⊤= (β⋆
i)⊤(A⋆)⊤A(t)−(z(t)
i)⊤which leads to
/vextenddouble/vextenddouble/vextenddoublez(t)
i/vextenddouble/vextenddouble/vextenddouble2
=/vextenddouble/vextenddouble/vextenddoubleQA(t)(A(t))⊤QA⋆β⋆
i−QA(t)f(t)
i−A⋆β⋆
i/vextenddouble/vextenddouble/vextenddouble2
2
=/vextenddouble/vextenddouble/vextenddouble(QA(t)(A(t))⊤Q−Id)A⋆β⋆
i−QA(t)f(t)
i/vextenddouble/vextenddouble/vextenddouble2
2
≤2/vextenddouble/vextenddouble/vextenddouble(QA(t)(A(t))⊤Q−Id)A⋆/vextenddouble/vextenddouble/vextenddouble2
2∥β⋆
i∥2+ 2/vextenddouble/vextenddouble/vextenddoublef(t)
i/vextenddouble/vextenddouble/vextenddouble2
≤2ddist2(A(t),QA⋆) + 2/vextenddouble/vextenddouble/vextenddoublef(t)
i/vextenddouble/vextenddouble/vextenddouble2
.
Using (S12) and the Cauchy-Schwarz inequality, we have
/vextenddouble/vextenddouble/vextenddoublef(t)
i/vextenddouble/vextenddouble/vextenddouble2
=/vextenddouble/vextenddouble/vextenddouble[Gi,(t)]−1(Gi,(t)Di,(t)−Ci,(t))β⋆
i/vextenddouble/vextenddouble/vextenddouble2
≤/vextenddouble/vextenddouble/vextenddouble[Gi,(t)]−1/vextenddouble/vextenddouble/vextenddouble2
2/vextenddouble/vextenddouble/vextenddoubleGi,(t)Di,(t)−Ci,(t)/vextenddouble/vextenddouble/vextenddouble2
2∥β⋆
i∥2
≤d/vextenddouble/vextenddouble/vextenddouble[Gi,(t)]−1/vextenddouble/vextenddouble/vextenddouble2
2/vextenddouble/vextenddouble/vextenddoubleGi,(t)Di,(t)−Ci,(t)/vextenddouble/vextenddouble/vextenddouble2
2, (S18)
where the last inequality follows from H1-(ii).
Using Lemma S4 and Lemma S5 and similarly to (Collins et al., 2021, Equation (45)), it follows for any
i∈Atthat/vextenddouble/vextenddouble/vextenddoublez(t)
i/vextenddouble/vextenddouble/vextenddouble2
2≤4ddist(A(t),QA⋆),
with probability at least 1−e110d2log(⌊rb⌋).
Similarly, using Lemma S1 and (S18), we have with probability at least 1−e110d2log(⌊rb⌋)and for any i∈At,
that
/vextenddouble/vextenddouble/vextenddoubleβ(t)
i/vextenddouble/vextenddouble/vextenddouble2
≤2/vextenddouble/vextenddouble/vextenddouble[A(t)]⊤QA⋆β⋆
i/vextenddouble/vextenddouble/vextenddouble2
+ 2/vextenddouble/vextenddouble/vextenddoublef(t)
i/vextenddouble/vextenddouble/vextenddouble2
≤4d.
Besides, note we have
E/bracketleftig
⟨Qϕ⋆
i(x(j)
i),z(t)
i⟩⟨u,Qϕ⋆
i(x(j)
i)⟩⟨β(t)
i,v⟩/bracketrightig
=⟨u,z(t)
i⟩⟨β(t)
i,v⟩.
The proof is then concluded by applying the Bernstein inequality following the same steps as in the final
steps of (Collins et al., 2021, Proof of Lemma 5).
We are now ready to control C2.
Lemma S8. Assume H1 and letδ′
d=cd√
k//radicalbig
⌊rb⌋nfor some absolute positive constant c. For any
t∈{0,...,T−1},η>0and whenever δ′
d≤d, we have with probability at least 1−e−110k−e−110d2log(⌊rb⌋)
C2≤ηδ′
ddist/parenleftig
A(t),QA⋆/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftig
R(t+1)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,
whereC2is defined in (S14),A(t)is defined in (S3)andR(t)comes from the QR factorisation of ¯A(t), see
step 20 in Algorithm S3.
27Under review as submission to TMLR
Proof.Lett∈{0,...,T−1}andη > 0. Then, whenever δ′
d≤d, we have with probability at least
1−e−110k−e−110d2log(⌊rb⌋), we have
C2=η
⌊rb⌋/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg1
n[A⋆
⊥]⊤(QA(t+1))†A(t+1)/parenleftig
Z(t+1)/parenrightig
Q−Z(t+1)/parenrightbigg⊤
B(t+1)
At+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftig
R(t+1)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤η
⌊rb⌋/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg1
n(QA(t+1))†A(t+1)/parenleftig
Z(t+1)/parenrightig
Q−Z(t+1)/parenrightbigg⊤
B(t+1)
At+1/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftig
R(t+1)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2
≤ηδ′
ddist/parenleftig
A(t),QA⋆/parenrightig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftig
R(t+1)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,
where we used the Cauchy-Schwarz inequality in the second inequality and Lemma S7 for the last one.
Control of∥/parenleftbig
R(t+1)/parenrightbig−1∥2.To finalise our proof, it remains to bound ∥/parenleftbig
R(t+1)/parenrightbig−1∥2. The associated result
is depicted in the next lemma.
Lemma S9. Define ¯δd=δd+δ′
dwhereδdandδ′
dare defined in Lemma S4 and Lemma S5, respectively.
Assume H1. Then, we have with probability at least 1−e−110k−e−110d2log(⌊rb⌋),
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftig
R(t+1)/parenrightig−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2≤/parenleftbigg
1−4η¯δd
(1−¯δd)2¯σ2
max,⋆/parenrightbigg−1/2
.
Proof.The proof follows from (Collins et al., 2021, Proof of Lemma 6).
S3 Experimental Details
S3.1 Data Sets
We provide some details about the datasets we used for our numerical experiments
S3.1.1 Toy data sets
The first toy dataset, denoted as noisy features , is a 20-class classification problem in which the features
for a given class is obtained by sampling a Gaussian distribution of dimension 5, with random mean and
Identity covariance matrix. For building the training set, we sample 2000examples for each class and equally
share those examples among clients who hold that class. Then, in order to generate some class imbalances
on clients, we randomly subsample examples on all clients. For instance, with 100clients and 2 classes per
clients, this results in a problem with a total of about 16k samples with a minimal number of samples of
38and a maximal one of 400. In order to get different dimensionality, we randomly append on each client
dataset some Gaussian random noisy features with dimensionality varying from 1to10.
The second toy dataset, denoted as linear mapping , is a 20-class classification problem where each class-
conditional distribution is Gaussian distribution of dimension 5, with random mean and random diagonal
covariance matrix. As above, we generate 2000samples per class, distribute and subsample them across
clients in the similar way, leading to a total number of samples of about 15k. The dimensionality perturbation
is modelled by a random (Gaussian)linear transformation that maps the original samples to a space which
dimension goes up to 50.
S3.1.2 MNIST-USPS
We consider a digit classification problem with the original MNIST and USPS data sets which are respectively
of dimension 28×28and16×16and we assume that a client hosts either a subset of MNIST or USPS data
set. We use the natural train/test split of those datasets and randomly share them accross clients.
28Under review as submission to TMLR
Table S1: Summary of the Brain-Computer Interfaces dataset we used. We report the number of subjects
(#Subj), the number of channels (#Chan), the number of classes (#Classes), the number of trials per class
(#Trials class) and the number of features (#features) on the covariance representation has been vectorized.
Name #Subj #Chan #Classes #Trials class # features
AlexMI 8 16 3 20 136
BNCI2014001 9 22 4 144 253
BNCI2014002 14 15 2 80 120
BNCI2014004 9 3 2 360 6
Weibo2014 10 60 7 80 1830
Zhou2016 4 14 3 160 105
S3.1.3 TextCaps data set
The TextCaps data set Sidorov et al. (2020) is an Image captioning dataset for which goal is to develop a
model able to produce a text that captions the image. The dataset is composed of about 21k images and
110k captions and each image also comes with an object class. For our purpose, we have extracted pair of
14977images and captions from the following four classes Bottle,Car,FoodandBook. At each run, those
pairs are separated in 80%train and 20%test sets. Examples from the TextCaps datasets are presented
in Figure S5. Images and captions are represented by vectors by feeding them respectively to a pre-trained
ResNet18 and a pretrained Bert, leading to vectors of size 512and768.
Each client holds either the image or the text representation of subset of examples and the associated vectors
are randomly pruned of up to 10%coordinates. As such, all clients hold dataset with different dimensionality.
S3.2 Brain-Computer Interfaces data set
The Brain-Computer Interfaces dataset we used are summarized in Table S1. Each dataset description
can be obtained from the MOABB library Jayaram & Barachant (2018) and at the following URL: http:
//moabb.neurotechx.com/docs/datasets.html . For each subject, we select the predefined train/test splits
or used 75%of the trials for training and the remaining 25%for testing. We used a bandpass prefiltering
between 8and30Hz of the EEG signals and extracted a covariance matrix for each trial using all available
channels. These covariance matrices are vectorized and used as a feature. The classes that we used for the
classification problem are the following ones: [‘left hand’, ‘right hand’, ‘feet’, ‘tongue’,‘rest’] and a subset of
them as available for each dataset.
S3.3 Models and Learning Parameters
For the toy problems, the TextCaps data set and the BCI one, as a local transformation functions we used a
fully connected neural network with one input, one hidden layer and one output layers. The number of units
in hidden layer has been fixed to 64and the dimension of latent space as been fixed to 64. ReLU activation
has been applied after the input and hidden layers. For the digits dataset, we used a CNN model with 2
convolutional layers followed by a max-pooling layer and a sigmoid activation function. Once flattened, we
have a one fully-connected layer and ReLU activation. The latent dimension is fixed to 64.
For all datasets, as for the local model gθi, in order to be consistent with competitors, we first considered a
single layer linear model implementing the local classifier as well as a model with one input layer (linear units
followed by a LeakyReLU activation funcion) denoting the shared representation layer and an output linear
layer.
For training, all methods use Adam with a default learning rate of 0.001and a batch size of 100. Other
hyperparameters have been set as follows. Unless specified, the regularization strength λ1andλ2have been
fixed to 0.001. Local sample batch size is set to 100and the participation rate rto0.1. For all experiments,
we have set the number of communication round Tto50and the number of local epochs to respectively 10
and100for the real-world and toy datasets. For FLIC, as in FedRep those local epochs is followed by one
29Under review as submission to TMLR
Figure S1: Evolution of the local loss curve of three different clients for three different learning situations.
See text for details.
epoch for representation learning. We have trained the local embedding functions for 100local epochs and
a batch size of 10for toy datasets and TextCaps and while of 100for MNIST-USPS and BCI. Reported
accuracies are computed after local training for all clients.
S3.4 Ablating Loss Curves
In order to gain some understanding on the learning mechanism that involves local and global training
respectively due to the local embedding functions, the local classifier and the global representation learning,
we propose to look at local loss curves across different clients.
Here, we have considered the linear mapping toy dataset as used in the toy problem analysis. However, the
learning parameters we have chosen are different from those we have used to produce the results so as to
highlight some specific features. The number of epochs (communication rounds) is set to 100with a client
activation ration of 0.1. Local epochs are shared for either training the local parameters or the global ones
(note that in our reference Algorithm 1, the global parameter is updated only once for each client) Those
latter are trained starting after the 20-th communication round and in this case, the local epochs are equally
shared between local and global parameter updates. Note that because of the randomness in the client
selection at each epoch, the total number of local epochs is different from client to client. We have evaluated
three learning situations and plotted the loss curves for each client.
•the local embedding functions and the global models are kept fixed, and only the local classifier is
trained. Examples of loss curves for 3clients are presented in the left plot of Figure S1. For this
learning situation, there is no shared global parameters that are trained locally. Hence, the loss curve
is typical of those obtained by stochastic gradient descent with a smooth transition, at multiple of
100local epochs, when a given client is retrained after a communication round.
30Under review as submission to TMLR
Figure S2: Impact of epochs used for pretraining ϕion the model accuracy as well as updating those functions
during the training. Results for three different datasets are reported. Plain and dashed curves are respectively
related to local training with and without updates ϕi.
•the local embedding functions are kept fixed, while the classifier and global parameters are updated
using half of the local epochs each. This situation is interesting and reported in middle plot in
Figure S1. We can see that for some rounds of 100local epochs, a strong drop in the loss occurs at
starting at the 50th local epoch because the global parameters are being updated. Once the local
update of a client is finished the global parameter is sent back to the server and all updates of global
parameters are averaged by the server. When a client is selected again for local updates, it is served
with a new global parameter (hence a new loss value ) which causes the discontinuity in the loss
curve at the beggining of each local update.
•all the part (local embedding functions, global parameter and the classifier) of the models are trained.
Note at first that the loss value for those curves (bottom plot in Figure S1) is larger than for the two
first most left plots as the Wasserstein distance to the anchor distribution is now taken into account
and tends to dominate the loss. The loss curves are globally decreasing with larger drops in loss at
the beginning of local epochs.
S3.5 On Importance of Alignment Pre-Training and Updates.
We have analyzed the impact of pretraining the local transformation functions and their updates during
learning for fixed reference distribution. We have considered two learning situations : one in which they are
updated during local training (as usual) and another one in they are kept fixed all along the training. We have
chosen the setting with 100users and have kept the same experimental settings as for the performance figure
and made only varied the number of epochs considered for pretraining from 1to200. Results, averaged over
5runs are shown in Figure S2. We remark that for the three datasets, increasing the number of epochs up to
a certain number tends to increase performance, but overfitting may occur. The latter is mostly reflected in
thetoy linear mapping dataset for which 10to50epochs is sufficient for good pretraining. Examples of how
classes evolves during pretraining are illustrated in Figure 5, through t-sneprojection. We also illustrate
cases of how pretraining impact on the test set and may lead to overfitting as shown in Figure S4.
S3.6 On the Impact of the Participation Rate
We have analyzed the effect of the participation rate of each client into our federated learning approach.
Figure S3 reports the accuracies, averaged over 3runs, of our approach for the toy datasets and the TextCaps
problem with respect to the partication rate at each round. We can note that the proposed approach is
rather robust to the participation rate but may rather suffer from overfitting due to overtraining of local
models. On the left plot, performances, measured after the last communication round, for TextCaps is stable
over participation rate while those performances tend to decrease for the toyproblems. We associate these
31Under review as submission to TMLR
Figure S3: Evolution of the performance of our FLIC-Class algorithm with respects to the participation
rate of clients, using the same experimental setting as in Figure 4. (left) evaluating performance after last
communication rounds, (right) best performance across communication rounds.
decrease to overfitting since when we report (see right plot) the best performance over communication rounds
(and not the last one), they are stable for all problems. This suggests that number of local epochs may be
dependent to the task on each client and the client participation rate.
32Under review as submission to TMLR
Class 1
Class 3
Class 5
Class 6
Class 11
Class 1
Class 3
Class 5
Class 6
Class 11
Class 1
Class 3
Class 5
Class 6
Class 11
Class 1
Class 3
Class 5
Class 6
Class 11
Class 1
Class 3
Class 5
Class 6
Class 11
Class 1
Class 3
Class 5
Class 6
Class 11
Figure S4: . 2D t-sneprojection of 5classes partially shared by 3clients for the toy linear mapping
dataset after learning the local embedding functions for (left) 10 epochs, (middle) 50 epochs, (right) 100
epochs. Original dimensions on clients vary from 5to50. Top row shows the projection the training set while
bottom row plots show both training and test set. Star ⋆markers represent the projection of the mean of
each class-conditional. The three different marker styles represent the different clients. Classes are denoted
by colors and similar tones of color distinguish train and test sets. We see that each class from the training
set from each client converges towards the mean of its anchor distribution, represented by the star marker.
Interestingly, we also remark that unless convergence is reached, empirical class-conditional distributions on
each clients are not equal making necessary the learning of a joint representation. From the bottom plots,
we can understand that distribution alignment impacts mostly the training set but this alignment does not
always generalize properly to the test sets.
33Under review as submission to TMLR
Figure S5: Examples of some TextCaps pairs of image/caption from the 4classes we considered of (top-left)
Food, (top-right) Bottle, (bottom-left) Book (bottom-right) Car. We can see how difficult some examples can
be, especially from the caption point of view since few hint about the class is provided by the text.
34