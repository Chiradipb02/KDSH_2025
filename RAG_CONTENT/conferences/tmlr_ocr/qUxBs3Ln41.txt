Published in Transactions on Machine Learning Research (08/2023)
Structured Low-Rank Tensors for Generalized Linear Models
Batoul Taki batoul.taki@rutgers.edu
Department of Electrical and Computer Engineering
Rutgers University-New Brunswick
Anand D. Sarwate anand.sarwate@rutgers.edu
Rutgers University-New Brunswick
Waheed U. Bajwa waheed.bajwa@rutgers.edu
Rutgers University-New Brunswick
Reviewed on OpenReview: https: // openreview. net/ forum? id= qUxBs3Ln41
Abstract
Recent works have shown that imposing tensor structures on the coeﬃcient tensor in regres-
sion problems can lead to more reliable parameter estimation and lower sample complexity
compared to vector-based methods. This work investigates a new low-rank tensor model,
called Low Separation Rank (LSR), in Generalized Linear Model (GLM) problems. The
LSRmodel–whichgeneralizesthewell-knownTuckerandCANDECOMP/PARAFAC(CP)
models, and is a special case of the Block Tensor Decomposition (BTD) model – is imposed
ontothecoeﬃcienttensorintheGLMmodel. Thisworkproposesablockcoordinatedescent
algorithm for parameter estimation in LSR-structured tensor GLMs. Most importantly, it
derives a minimax lower bound on the error threshold on estimating the coeﬃcient tensor
in LSR tensor GLM problems. The minimax bound is proportional to the intrinsic degrees
of freedom in the LSR tensor GLM problem, suggesting that its sample complexity may be
signiﬁcantly lower than that of vectorized GLMs. This result can also be specialised to lower
bound the estimation error in CP and Tucker-structured GLMs. The derived bounds are
comparable to tight bounds in the literature for Tucker linear regression, and the tightness
of the minimax lower bound is further assessed numerically. Finally, numerical experiments
on synthetic datasets demonstrate the eﬃcacy of the proposed LSR tensor model for three
regression types (linear, logistic and Poisson). Experiments on a collection of medical imag-
ing datasets demonstrate the usefulness of the LSR model over other tensor models (Tucker
and CP) on real, imbalanced data with limited available samples.
1 Introduction
In machine learning, regression models are used to understand the relationship between a set of independent
variables (also known as covariates) and a dependent outcome. More formally, given a vector of covariates,
x, and outcome, y, jointly distributed according to Pxy, the goal is to produce a function that will predict y
when given x. Under the Mean Squared Error (MSE) criterion, the solution would be ﬁnding the conditional
mean Ey|x[y|x], which is obtained by modeling the conditional probability Py|xand estimating the model
class parameters. For example, in linear regression, the predictor is given by bTx, where the model class
parameters are denoted by a vector band estimated through a set of ntraining data samples {xi,yi}n
i=1,
where xiis theithsample vector of covariates. Diﬀerent regression models suit diﬀerent types of prediction
problems: some common examples are linear, logistic and Poisson regression, all of which fall under a
broader class of models called Generalized Linear Models (GLMs). GLMs were introduced to encompass
classes of models that cannot be appropriately modeled as a simple ‘linear-response model’ (McCullagh &
Nelder, 2019). In particular, GLMs refer to a parametric statistical framework that models the conditional
1Published in Transactions on Machine Learning Research (08/2023)
probability of a scalar response variable yithat follows an exponential family distribution. Because the
exponential family distribution encompasses a large range of widely-used distributions, GLMs allow one to
study a broader class of regression problems.
GLM regression models are used for wide array of datasets in a multitude of applications. However, modern-
day technologies are creating data-intensive environments and collecting increasingly high-dimensional data.
In particular, we often encounter data in the form of variegated and structured multi-dimensional arrays
(tensors), where the number of available data samples is far smaller than the number of variables (the
dimensionality of the data). Prominent examples of two-dimensional arrays include biological imaging data
such as electroencephalography (EEG) and ﬁber-bundle imaging (Dumas et al., 2019). Examples of three-
dimensional arrays include functional Magnetic Resonance Images (fMRI) (Bellec et al., 2017) and Magnetic
Resonance Angiography (MRA) (Yang et al., 2020). Though such data has been used in many instances
throughout the literature (Li et al., 2018; Hung & Wang, 2013; Zhou et al., 2013; Dumas et al., 2019),
classical parameter estimation methods assume vector-structured covariates and estimate a corresponding
coeﬃcient vector. There are two major concerns associated with multidimensional (a.k.a. tensor) data and
its vectorization. First, vectorizing data that was originally in tensor form destroys its underlying structure
(which often contains rich information valuable for regression analysis). Secondly, the resulting vector model
exhibits a very large number of parameters, in the sense that in the high-dimensional setting the GLM
regression model becomes ill-posed. This is an instance of ‘the curse of dimensionality’ (Hung & Wang,
2013; Zhang & Jiang, 2018).
A common solution to the curse of dimensionality in tensor GLM problems is to impose structure on the
modelparameters: thisisthefocusofthiswork. Ifthecovariatesare tensorstructured, weexpecttoestimate
coeﬃcient tensors whilst exploiting the multidimensional structure of the data and rich information lying in
the correlation between tensor modes. Common structures include: the addition of a sparsity regularizer or a
low-rank inducing regularizer to the regression problem (Abramovich & Grinshtein, 2018; Seber & Lee, 2003;
Zhang & Jiang, 2018; An & Zhang, 2020; Raskutti et al., 2019); the imposition of some tensor factorisation
on the model parameters (Ahmed et al., 2020; Li et al., 2018; Zhou et al., 2013; Zhang et al., 2020; Tan
et al., 2013; Zhang & Jiang, 2016; Wu et al., 2022; Taki et al., 2021); or both of the above. The imposition of
such structures should ultimately lead to the estimation of fewer parameters, improving the computational
complexity and performance of our parameter estimation problem.
Two commonly used tensor factorisations are the CANDECOMP/PARAFAC (CP) and Tucker decomposi-
tions (Kolda & Bader, 2009). These decompositions impose a compact structure on the coeﬃcient tensors,
thereby restricting the class of possible solutions in the parameter estimation problem. Compared to sim-
ple vector regression, these decompositions can decrease the number of training samples needed for reliable
coeﬃcient estimation (also referred to as ‘sample complexity’). A smaller sample complexity can lower the
variance of the model, yet the restrictive nature of these decompositions can also reduce the representation
power of the coeﬃcient tensors for many classes of tensors, causing a non-favourable bias-variance trad-eoﬀ.
A less studied decomposition – particularly in regression works – is the Block Tensor Decomposition (BTD)
(De Lathauwer, 2008), which can be expressed as a Tucker decomposition with block diagonal core tensor.
We further discuss BTD in relation to our work in Section 3.
In this work we impose a new decomposition on the coeﬃcient tensors that we will promptly refer to as the
Low Separation Rank (LSR) model. In fact we will show that the LSR decomposition is a generalization
of the Tucker decomposition, and a special case of the BTD model (De Lathauwer, 2008). The LSR model
maintains a lower sample complexity than vectorization-based tensor GLM regression but is less restrictive
than Tucker or CP models of same sized core tensor. Though estimating an LSR-structured tensor introduces
a greater sample complexity than the aforementioned decompositions, this increase is compensated by a
stronger representation power, leading to better parameter estimation performance. In other words, we
show that the increase in variance of the LSR model is conquered by its stronger representation power for a
larger class of tensors (ergo, its decrease in bias), leading to a more favourable bias-variance trade-oﬀ (model
compactness vs. representation power).
2Published in Transactions on Machine Learning Research (08/2023)
1.1 Contributions
In this paper we make the following contributions. We introduce the LSR-structured tensor problem under
the GLM framework that we appropriately denote as LSR-TGLM. GLMs encompass various ﬂavours of
regression including linear, logistic and Poisson. We focus on the high-dimensional setting and discuss the
various parameters of an LSR-structured tensor (such as ‘rank’ and ‘separation rank’, terms we will introduce
in Section 2) that reduce the sample complexity of parameter estimation in GLMs. We also compare sample
complexities between diﬀerent tensor decomposition models and the LSR model.
Additionally, we explore two problems at the core of this work. First, we propose a parameter estimation
algorithm(whichwenameLSRTR)fortheLSR-TGLMproblem. Themainideaisthatparameterestimation
in GLMs can be achieved through Maximum Likelihood Estimation (MLE) (McCullagh & Nelder, 2019).
However, for structured tensor settings, such as estimating CP or Tucker-structured tensors, the objective
function of the MLE problem is highly non-convex (Li et al., 2018; Zhang & Jiang, 2016; Zhou et al., 2013;
Tan et al., 2013). This is also true when the tensor is LSR-structured. To overcome this, we observe that
the problem can be partitioned into several convex sub-problems that can then be solved alternately. On
the basis thereof we propose a block coordinate descent algorithm to ﬁnd the MLE of the LSR-structured
coeﬃcient tensor. Secondly, and perhaps most importantly we investigate the fundamental error threshold
of the LSR-TGLM problem by deriving a minimax lower bound on the estimation error. This minimax
bound is useful for assessing the performance of the proposed algorithm and ascertaining the parameters
that may aﬀect the sample complexity of the parameter estimation problem. The bound is general and can
be specialised to previously introduced regression types under the GLM framework, such as CP and Tucker
tensor GLMs. Obtaining the bound requires a special construction of a packing set of LSR-structured
tensors. The methods we develop are systematic and can be appropriate in other works that consider similar
topological properties of structured tensors (i.e., LSR-structured tensors). We also assess the tightness of
our bounds in two ways: 1) Through a numerical study where we show that the ratio of the empirical error
through LSRTR and the minimax bound is approximately constant with increasing sample size, and 2) We
specialise our minimax bound to the Tucker linear regression case and show that our bound matches the
optimal error rates for Tucker linear regression found in recent works (Zhang et al., 2020).
Finally, we evaluate the performance of our algorithm through extensive numerical experiments on synthetic
data. We also test the performance of imposing the LSR structure on several classiﬁcation problems with
multidimensional medical imaging datasets. We show that while the LSR model outperforms the vector
model, its rich representation power also allows for enhanced performance over the Tucker (and CP) case.
1.2 Relation To Prior Work
Regression problems have been a major focus of high-dimensional statistics for many years (Giraud, 2021).
Some works on linear and logistic regression impose sparsity on the model parameter in order to reduce
the sample complexity of the vector-based regression problems (Abramovich & Grinshtein, 2018; Sun &
Zhang, 2012). However, in very high-dimensional regimes such as when the data is tensor-structured, i.e.,
we have{Xi}n
i=1, sparsity assumptions do not provide enough reduction in the sample complexity (Raskutti
et al., 2019; Lee & Courtade, 2020). Several works overcome the limitations of sparse vector regression by
extending regular regression to the high-dimensional and low-rank matrix settings. Low-rank assumptions
on data are common throughout the literature and are used to reduce the sample complexity of estimation
problems (Barnes & Özgür, 2019; Shi et al., 2014). Such works propose regularized matrix linear and logistic
regression models to obtain low-rank and/or sparse estimates of the coeﬃcient matrix in regression problems,
such as those on inference on images or graph data (Hung & Wang, 2013; Zhang & Jiang, 2018; Shi et al.,
2014; An & Zhang, 2020; Berthet & Baldin, 2020). Some works directly impose low-rank structures on
coeﬃcient matrices through the rank- rsingular value decomposition (SVD) (Taki et al., 2021).
Additionally, though tensors and their decompositions have long since been introduced in the literature
(Kolda & Bader, 2009), their applications in regression analysis have recently become established. Analogous
to the low-rank matrix regression works, a variety of works have introduced low-rank structures on coeﬃcient
tensors for tensor regression problems. For logistic regression, Tan et al. (2013) ﬁrst introduced using a low-
rank and/or sparse CANDECOMP/PARAFAC (CP) decomposition. A more ﬂexible generalization of this
3Published in Transactions on Machine Learning Research (08/2023)
work is imposing the Tucker decomposition on the coeﬃcient tensor in tensor logistic regression (Zhang &
Jiang, 2016; Wu et al., 2022). The Tucker structure has also been introduced for tensor linear regression
(Zhang et al., 2020; Ahmed et al., 2020; Wu et al., 2022). To the best of our knowledge, the BTD structure
has yet to be introduced for regression and GLMs.
More works to our interest generalize tensor linear and logistic regression works by imposing the CP and
Tucker decompositions in tensor GLMs (Li et al., 2018; Zhou et al., 2013). Both structures have been shown
to signiﬁcantly reduce the number of learnable parameters, leading to eﬃcient estimation and prediction in
a variety of regression problems, particularly with medical imaging data. The aforementioned works develop
eﬃcient estimation algorithms and provide empirical results on their performance. The proposed approaches
outperform vector-based methods (in terms of estimation and prediction accuracy) in the high-dimensional
setting when the number of available samples is limited. However, these matrix and tensor structures are
aimed at being compact (in the sense that they decrease the number of learnable parameters in a given
problem), and are therefore also quite restrictive in their representation power of the true coeﬃcient tensor.
A more general and ﬂexible tensor model is required to achieve accurate and eﬃcient estimation while
maintaining a useful level of compactness.
In terms of theoretical guarantees, various regression works provide local identiﬁability guarantees of the
proposed CP and Tucker tensor models for GLMs, and asymptotic consistency and normality results for the
MLE estimator of the model parameter (Li et al., 2018; Zhang et al., 2020; Zhou et al., 2013). Some works
on high-dimensional regression also provide sample complexity bounds of the proposed model in the form
of risk upper bounds (Zhang et al., 2020; Ahmed et al., 2020) or minimax lower bounds (Barnes & Özgür,
2019; Zhang et al., 2020; Raskutti et al., 2019; Foster et al., 2018; Abramovich & Grinshtein, 2018; Lee &
Courtade, 2020; Abramovich & Grinshtein, 2016; Raskutti et al., 2011); however, these works are speciﬁc
to vector-based logistic regression or Tucker linear regression. Current works in tensor logistic regression
or tensor-based GLMs do not provide any theoretical guarantees for sample complexity (upper or lower
bounds).
In terms of the LSR model, its motivational roots are two fold. First, a special case of the LSR model was
introduced by Tsiligkaridis & Hero (2013) for covariance estimation problems. An extension of this model
has only recently been used on tensor data for dictionary learning (Ghassemi et al., 2020). Secondly, the
LSR model can be rearranged into a specialised form of the BTD model, equipped with further constraints.
In terms of the GLM framework, to the best of our knowledge, our work is the ﬁrst to consider the LSR (or
BTD) model in regression problems.
1.3 Organization
The organization of this paper is as follows. In Section 2 we establish a background on various tensor
models, as well as the LSR tensor model. In Section 3 we formulate the LSR-TGLM model and introduce
two objectives: parameter estimation and minimax risk. In Section 4 we discuss the estimation problem of
LSR-structured coeﬃcient tensors in GLMs and propose an eﬃcient algorithm for parameter estimation. In
Section 5 we provide a numerical study of the LSRTR algorithm with synthetic data and experiments on real
data. In Section 6 we introduce a sample complexity bound in the form of a minimax lower bound on the
estimation error of the low-rank LSR-GLM model and provide a formal proof in Section 6.3. We conclude
our work in Section 7. Proofs of lemmas for the main theorem, and additional numerical results are provided
in the appendix.
2 Preliminaries
This work is based on structured tensor decompositions. For a more comprehensive tutorial on tensor
decompositions, see the survey of Kolda & Bader (2009). We will now list some necessary preliminaries
regarding tensors and tensor structures.
We use the following notation convention throughout the paper: x,x,XandXdenote scalars, vectors,
matrices and tensors, respectively. Speciﬁcally, Xis the tensor deﬁned as the aggregation of ntensors:
X= [X1,X2,...,Xn]. Given a ﬁxed tensor X,x,vec(X)is the column-wise vectorization of X. The
4Published in Transactions on Machine Learning Research (08/2023)
tensor Imis them×···×midentity tensor, such that for any tensor S, the product I·S=S·I=S.
GivennK-mode tensors{Xi}n
i=1of dimension m1×m2×···×mK,Xis the combined (aggregated) tensor
ofnsamples of dimension m1×m2×···×mK×n. For a positive integer K, the set [K] ={1,2,...,K}
so that (Xk)k∈[K]is the ordered set (X1,X2,...,XK). The symbol−[K]for the reverse order, so that
(Xk)k∈−[K]= (XK,XK−1,...,X1). For a matrix X, the vector x(j)is thejthcolumn of Xand the vector
xT(j)is itsjthrow. For a vector x,x(j)is thejthelement of x. Ifx∈Rthen⌊x⌋is the greatest integer
less than or equal to x. We use the standard notation /bardblx/bardblpfor thep-norm (p≥1)of a vector x. For
a matrix Xor tensor X, the Frobenius norms are /bardblX/bardblFand/bardblX/bardblF, respectively. For two vectors x1and
x2,x1◦x2denotes their outer product. Similarly, for two matrices X1andX2,X1⊗X2denotes their
Kronecker product. The inner product between two vectors, matrices or tensors is denoted as /angbracketleft·,·/angbracketright. For a
set ofKvectors{xi}K
i=1,x1◦x2◦···◦ xKproduces a k-dimensional rank- 1tensor. For Kmatrices{Xi}K
i=1,/circlemultiplytext
k∈[K]Xk,X1⊗X2⊗···⊗ XKproduces a ‘ K-order Kronecker-structured matrix’. We call a matrix a
K-order Kronecker-structured matrix if it is a product of K≥2matrices. The mode- kmatricization of
tensor XisX(k)(Kolda & Bader, 2009), and given a matrix B,X×kBdenotes the multiplication of Xby
Balong mode k. Finally, for all k∈[K]we have X×[K]Bk,X×1B1×2···×KBK.
We now formally deﬁne GLMs for vector-structured covariates, as discussed in the literature.
Deﬁnition 1 (Vector-structured Generalized Linear Models) .Consider an observation y, a vector of co-
variates x∈Rm, and a bias zand regression coeﬃcient vector b, both to be estimated. Let ybe a response
variable generated from a distribution in the exponential family with probability mass/density function as
follows:
P(y,η) =b(y) exp(ηT(y)−a(η)). (1)
Here,ηis called the natural parameter, T(y)is the suﬃcient statistic and a(η)is the log-partition (or
cumulant) function. Consider a given regression problem of estimating ygiven x. This requires minimising
the MSE as follows:
/hatwideyMMSE = arg min
/hatwideyEx,y[(/hatwidey−y)T(/hatwidey−y)] (2)
The Minimum MSE (MMSE) solution is the expected value of yconditioned on x, orE[y|x] =µ. Now,
letHbe the support of yand consider a strictly increasing and invertible link function g(·) :H→R. The
Generalized Linear Model is then deﬁned as
g(µ) =η,/angbracketleftb,x/angbracketright+z, (3)
where the natural parameter ηis the linear predictor, µ=g−1(η), andzis the bias.
The core idea behind GLMs is that though many regression problems are not linear, the distribution of the
observation yis only aﬀected by the linear combination /angbracketleftb,x/angbracketright+z. If the distribution of yfalls under the
exponential family (which it often does), then yis related to the linear predictor via g(·). In the case of linear
regression where y∈Ris assumed to be a continuous response variable with Gaussian distribution, we have
µ=/angbracketleftb,x/angbracketright+zandg(·)is just the identity function. In logistic regression yis Bernoulli distributed, taking
valuesy∈{0,1}, and we have µ=1
1+exp(−/angbracketleftb,x/angbracketright+z)andg(µ) = log/parenleftBig
µ
1−µ/parenrightBig
. In Poisson regression y∈Nis a
Poisson distributed random variable that expresses the count of an event occurring in a ﬁxed time interval,
whileµ= exp(/angbracketleftb,x/angbracketright+z)andg(µ) = log (µ).
Deﬁnition 2 (CP Decomposition (Kolda & Bader, 2009; Zhou et al., 2013)) .Consider a K-mode tensor
B∈Rm1×···×mK. The rank- rCANDECOMP/PARAFAC (CP) decomposition decomposes Binto a sum of
rrank- 1tensors as follows:
B=/summationdisplay
i∈[r]b1,i◦···◦ bK,i, (4)
5Published in Transactions on Machine Learning Research (08/2023)
where bk,i∈Rmk, k∈[K], i∈[r]is a column vector. Equivalently, (4)can be expressed in vector form as
follows:
vec(B),b=/summationdisplay
i∈[r]bK,i⊗···⊗ b1,i. (5)
Deﬁnition 3 (Tucker decomposition (Kolda & Bader, 2009; Li et al., 2018)) .Consider a K-mode tensor
B∈Rm1×···×mK. The rank- (r1,...,rk)Tucker decomposition decomposes Bas follows:
B=G×1B1×2···×KBK, (6)
where G∈Rr1×···×rKdenotes the core tensor and {Bk∈Rmk×rk}k∈[K]denote the factor matrices. Equiva-
lently, by deﬁning g,vec(G),(6)can be expressed in vector form as follows:
vec(B),b=/parenleftbig
BK⊗···⊗ B1/parenrightbig
g. (7)
(a)
 (b)
Figure 1: (a): A third-order tensor under the Tucker model. Tensor Bis decomposed into a core tensor
Gand factor matrices Bkmultiplied along the kthmode of Gfork∈[3]. The CP model appears as a
special case of the Tucker model where Gis a diagonal tensor of equal dimension along each mode. (b):
A third-order tensor under the LSR decomposition. Tensor Bis comprised of a sum of Tucker-structured
tensors, with core tensor Gﬁxed across all summands.
A visual depiction of the Tucker model is in Figure 1a. The Tucker model decomposes a tensor into a
core tensor Gof dimension r1×···×rKwhich is then multiplied by a factor matrix Bkalong each mode
k∈[K]. Assuming Gis small (i.e., rk/lessmuchmk∀k∈[K]), the factor matrices Bk∀k∈[K]are tall, rank rk
matrices. Additionally, the CP model appears as a specialised case of the Tucker model as follows: Fix the
number of basis vectors along all modes to some r∈R(i.e., ﬁx the rank of all factor matrices to r) and
impose G∈Rktimes/bracehtipdownleft/bracehtipupright/bracehtipupleft/bracehtipdownright
r×···×ras a diagonal tensor. The CP model lends a desirable compactness as the number
of learnable parameters in a CP-structured tensor can be far fewer than that of an unstructured tensor.
Despite this, however, the restrictive nature of the CP model (speciﬁcally its rank restriction, where each
factor matrix must have equal rank) renders it unfavourable against the more ‘rank-ﬂexible’ Tucker model.
Our second observation refers to (7). The vectorization of a Tucker-structured tensor shows the Kronecker-
structured matrix composed of the Kfactor matrices. However this ‘Kronecker’ structure – which implies
that the coeﬃcient vector bis composed of separable sub-matrices weighted by some vector g= vec( G)–
is also quite restrictive, when considering diﬀerent tensor structures having the same rank. To overcome
this, the Block Tensor Decomposition (BTD) was introduced and studied in recent works (De Lathauwer,
2008; Rontogiannis et al., 2021; Fu et al., 2020). An alternative way of viewing the BTD structure is as a
summation of STucker-structured tensors. The LSR decomposition we deﬁne next is a special case of a
BTD that uses the concept of matrix separation rank.
6Published in Transactions on Machine Learning Research (08/2023)
Deﬁnition 4 (Matrix Separation Rank (Tsiligkaridis & Hero, 2013)) .Fixm= (m1,m2,...,mK)∈NK
andr= (r1,r2,...,rK)∈NK, and set/tildewidem=/producttext
k∈[K]mkand/tildewider=/producttext
k∈[K]rk. Then for a matrix B∈R/tildewidem×/tildewider,
its separation rank SK
m,p(·)is the minimum number SofK-order Kronecker-structured matrices such that
B=/summationdisplay
s∈[S]B(1,s)⊗···⊗ B(K,s), (8)
where B(k,s)∈Rmk×rk.
The LSR model in (8) generalizes (7) by replacing the single Kronecker-structured matrix in (7) with a sum
of the form in (8). It poses that a matrix can be expressed as a sum of SKronecker-structured matrices, with
S= 1being a specialised case and what we observe in the Tucker model when the tensor is vectorized, as
shown in (7). Having introduced the concept of separation rank, we are now ready to deﬁne Low Separation
Rank (LSR) tensors.
Deﬁnition 5 (Low Separation Rank (LSR) Tensor Decomposition) .Consider a K-mode tensor B∈
Rm1×···×mK. The rank- (r1,...,rk)LSR decomposition with separation rank Sdecomposes Bas follows:
B=/summationdisplay
s∈[S]G×1B(1,s)×2···×KB(K,s), (9)
where G∈Rr1×···×rKdenotes the core tensor and B(k,s)∈Rmk×rk, k∈[K], s∈[S]denote the Kronecker-
structured factor matrices. Equivalently, by deﬁning g,vec(G),(9)can be expressed in vector form as
follows:
vec(B),b=/summationdisplay
s∈[S]/parenleftbig
B(K,s)⊗···⊗ B(1,s)/parenrightbig
g. (10)
A visual depiction of the LSR model is in Figure 1b. For the purposes of this work, we pose some constraints
on the LSR decomposition. First, we assume that the KSfactor matrices B(k,s)are ‘tall’ and rank rk
(i.e.,rk/lessmuchmk∀k∈[K]) and have orthonormal columns. Secondly, the LSR model corresponds to tensor
Bhaving a separation rank that is relatively small so that 1≤S < min/parenleftBig/producttext
k∈[K]mk,/producttext
k∈[K]rk/parenrightBig
. Thus,
deﬁning Om×ras them×rStiefel manifold, and for a ﬁxed tensor rank (r1,r2,...,rK)and separation rank
S, the LSR structured tensor Bbelongs to the following parameter space P{rk},S:
P{rk},S,/braceleftbigg
B/prime=/summationdisplay
s∈[S]G/prime×1B/prime
(1,s)×2···×KB/prime
(K,s)∈Rm1×···×mK:G/prime∈Rr1×···×rK,
rank(B/prime) = (r1,...,rK),B(k,s)∈Omk×rk, k∈[K], s∈[S]/bracerightbigg
(11)
The orthonormality assumption on the columns of B(k,s)is a common assumption made in the GLM litera-
ture (Zhang & Jiang, 2016). Similar assumptions such as unit-norm columns and columns with ﬁxed entries
are also common (Zhou et al., 2013; Li et al., 2018).
We have described how the LSR structure in (9) is a generalization of the Tucker (and therefore of CP)
decomposition. The form in (9) is also a special case of BTD, or a summation of STucker-structured
tensors, equipped with orthogonality constraints and a common core tensor among all summands. The
BTD can also be rearranged into a specialised Tucker-structured tensor with block-diagonal core tensor of
dimensions Kr1×···×KrKand factor matrices of size mk×Srk. In our work, however, we compare the
number of learnable parameters between tensor decompositions (CP, Tucker and LSR) of same-sized core
tensor (of same rank). When comparing tensor decompositions for a ﬁxed rank (r1,...,rK), LSR generalizes
the Tucker decomposition. We also remark that while the diﬀerences between LSR and a general BTD
might appear nuanced, they are signiﬁcant, particularly the critical requirement for a common core tensor
in the LSR structure, which allows for summing the Kronecker-structured factor matrices. This distinction
7Published in Transactions on Machine Learning Research (08/2023)
is fundamental for retaining the LSR matrix structure, thereby oﬀering a number of beneﬁts, on which we
will elaborate in Sections 3, 4, and 6.
Moving forward, Table 1 reviews the number of learnable parameters for the three models of CP, Tucker
and LSR, for a ﬁxed rank (r1,...,rK). The CP model contains the least number of parameters, especially
ifris small. The LSR model is the most complex of the three models in that it has more parameters than
the Tucker model. The working hypothesis throughout this work is that the price we are likely to pay for
an increase in sample complexity (by using the LSR model for tensor GLM problems) is worth the gain we
are likely to achieve in representation power and estimation accuracy.
CP Tucker LSR
Parameters/summationtextK
k=1(mkr) +r/summationtextK
k=1(mkrk) +/producttextK
k=1rkS/summationtextK
k=1(mkrk) +/producttextK
k=1rk
Table 1: Number of learnable parameters in the three tensor models (CP, Tucker and LSR).
3 Problem Statement
We are now ready to propose the Low Separation Rank Tensor Generalized Linear Model (LSR-TGLM).
Consider a response variable ywith probability distribution belonging to the exponential family with pa-
rameterη, as in (1). Consider also tensor-structured covariates X∈Rm1×···×mKand a coeﬃcient tensor
B∈Rm1×···×mKthat assumes a low-rank LSR structure as shown in (9). Given a link function g(·), the
LSR-TGLM model assumes ηis given by
g(µ) =η,/angbracketleftBigg/summationdisplay
s∈[S]G×1B(1,s)×2···×KB(K,s),X/angbracketrightBigg
+z. (12)
It is important to note here that we assume that the tensor rank (r1,r2,...,rK)and LSR rank Sare known
for reasons we will discuss in Section 3.1. Additionally for algebraic simplicity, from this point forward
we will consider the standard case where z= 0in (3) and without loss of generality we will express the
LSR-TGLM model as
g(µ) =/angbracketleftBigg/summationdisplay
s∈[S]G×1B(1,s)×2···×KB(K,s),X/angbracketrightBigg
. (13)
The LSR-TGLM model in (13) can be written as a standard GLM by vectorizing the parameters as follows
g(µ) =/angbracketleftvec(B),vec(X)/angbracketright=/angbracketleftBigg/summationdisplay
s∈[S]/parenleftbig
B(K,s)⊗···⊗ B(1,s)/parenrightbig
g,x/angbracketrightBigg
. (14)
The sum of Kronecker-structured matrices in (14) is due to the LSR-TGLM model in (13). We can now
formally deﬁne the two goals that are at the core of this work: 1) Parameter estimation for LSR-TGLM and
2) Minimax lower bound on the estimation error. One of the beneﬁts of the requirement of a common core
tensor is an intuitive reasoning pertaining to the role of the core tensor in the LSR-structured tensor. If one
was to view Gas a ‘weight tensor’, the same Gimplies that the Sgroups of factor matrices must be given
the same weight. We also emphasise here that the conceptual novelty of our work resides not in introducing
the concept of LSR-structured tensors, but in our unique application of the LSR decomposition to the GLM
model and our comprehensive analysis of the resulting GLM, which is non-trivial.
3.1 Parameter Estimation for LSR-TGLM
Theobjectiveinregressiontheoryistopredictanoutcome ybasedon X, whichisachievedbyﬁrstestimating
the model parameter. For LSR-TGLMs, we wish to ﬁnd an estimate of Bthat best ﬁts the model in (13).
8Published in Transactions on Machine Learning Research (08/2023)
The underlying (true) Bis a low-rank LSR-structured tensor belonging to a constraint set C:
C,/braceleftBigg
B∈Rm1×···×mK:B∈P{rk},S,rk≤RK,S≤min/braceleftBigg/productdisplay
kmk,/productdisplay
krk/bracerightBigg
,k∈[K]/bracerightBigg
.(15)
This is the set of all LSR-structured tensors with constrained tensor rank tuple (r1,r2,...,rK)and con-
strained separation rank S. Now consider the space of tensor-structured covariates X⊂Rm1×m2×···×mK,
and space of scalar observations Y⊂R. There exists a probability measure, denoted as Pxy, that allows
a learning procedure to randomly draw points {X∈X,y∈Y}from the product space X×Y. We do
not know Pxy, but we have access to nindependently sampled observations {Xi,yi}n
i=1. Therefore, we ﬁnd
an estimate of Bthrough Maximum Likelihood Estimation (MLE), making the the objective function the
negative log-likelihood:
Ln(B) =n/summationdisplay
i=1log(b(yi)) +n/summationdisplay
i=1(/angbracketleftB,Xi/angbracketrightT(yi)−a(/angbracketleftB,Xi/angbracketright)). (16)
To ﬁnd the MLE, we minimise (16) over the constraint set C:
arg min
B∈Cn/summationdisplay
i=1log(b(yi)) +n/summationdisplay
i=1(/angbracketleftB,Xi/angbracketrightT(yi)−a(/angbracketleftB,Xi/angbracketright)). (17)
The vectorized LSR-structured tensor uses a sum of SKronecker-structured matrices. Lemma 1from
Ghassemi et al. (2020) shows that there is a 1-to-1mapping between Kronecker-structured matrices with
separation rank Sand a set of rank Stensors. Finding the rank rof a tensor is NP-hard, (Håstad, 1990),
and thus so is ﬁnding the separation rank of a Kronecker-structured matrix. Therefore, in the context of our
work on LSR-TGLM, ﬁnding the LSR rank Sis NP-hard, and the problem in (17) is therefore intractable.
To mitigate this issue we ﬁrst assume that the tensor rank (r1,r2,...,rK)and separation rank Sare known
and we solve the following factorised problem for parameter estimation for LSR-TGLMs:
arg min
{B(k,s)},G=n/summationdisplay
i=1log(b(yi)) +n/summationdisplay
i=1/angbracketleftBiggS/summationdisplay
s=1G×[K]Bk,Xi/angbracketrightBigg
T(yi)−a/parenleftBigg/angbracketleftBiggS/summationdisplay
s=1G×[K]Bk,Xi/angbracketrightBigg/parenrightBigg
.(18)
subject to B(k,s)∈Omk×rk.
The expression in (18) provides a tractable relaxation for (17), where the coeﬃcient tensor is explicitly
written in terms of the core tensor Gand factor matrices B(k,s)of the low-rank LSR structure. In this work,
we will study the problem in (18).
3.2 Minimax Lower Bound for LSR-TGLM
Our second goal is to derive a lower bound on the minimax risk of estimating LSR-structured coeﬃcient
tensors for the LSR-TGLM problem in (13). Minimax bounds can be useful tools in developing an insight
into the parameters on which an achievable error of a given problem might depend and shed light on the
beneﬁts of imposing tensor structures in regression problems. They also provide a means of quantifying the
performance of existing algorithms. Previous studies of tensor-structured GLMs (Zhou et al., 2013; Li et al.,
2018) fall short of providing a sample complexity analysis. The analysis in this work is thus instrumental
in revealing the potential beneﬁts of imposing structure on the coeﬃcient tensor within the GLM context.
We will show that the LSR model exhibits a lower sample complexity as compared to the vector case, and
provides an expressive representation of tensor data.
We adopt a local analysis and assume that the LSR-TGLM’s underlying (true) Bresides within a neighbour-
hood with known radius around a ﬁxed point. However, for a suﬃciently large neighborhood, the minimax
lower bounds derived in this work eﬀectively become independent of the radius. Therefore we assume that
for a ﬁxed tensor rank (r1,r2,...,rK)and separation rank S, the underlying Bbelongs to the set
Bd(0),{B/prime∈P{rk},S:ρ(B/prime,0)<d}, (19)
9Published in Transactions on Machine Learning Research (08/2023)
the ball of radius dwith distance metric ρ=/bardbl·/bardblF, which resides in the parameter space P{rk},Sdeﬁned in
(11). ThusBd(0)⊂P{rk},SandBhas energy bounded by /bardblB/bardbl2
F<d2. Note that we ﬁx the reference point
as the tensor of all zero-elements 0without loss of generality. Indeed, any neighbourhood Bd(A)around
a point A∈P{rk},Sis just a translation from Bd(0)with known distance /bardblA/bardblF. Additionally, we note
that the point 0also belongs to the parameter space P{rk},S. The minimax risk is deﬁned as the minimum
worst-case behaviour for any estimator. Mathematically, it is expressed as follows.
ε∗= inf
/hatwideBsup
B∈Bd(0)Ey,X/braceleftbigg
φ(/hatwideB,B)/bracerightbigg
. (20)
Here,/hatwideBdenotes an estimator of B,y= [y1,y2,...,yn],X= [X1,X2,...,Xn], andφis a function with
φ(0) = 0. If we deﬁne φ=/bardbl·/bardbl2
F,R+→R+withφ(0) = 0then the minimax risk is simply the worst-case
Mean Squared Error (MSE) for the best estimator, i.e.,
ε∗= inf
/hatwideBsup
B∈Bd(0)Ey,X/braceleftbigg/vextenddouble/vextenddouble/vextenddouble/hatwideB−B/vextenddouble/vextenddouble/vextenddouble2
F/bracerightbigg
. (21)
Proving a lower bound ε∗> ε 0on the minimax risk shows that any estimator must have a risk lower
bounded by ε0. Existing minimax bounds on the parameter estimation problem in GLMs or regression
models provided in the literature (Abramovich & Grinshtein, 2016; Lee & Courtade, 2020; Raskutti et al.,
2011) cannot be applied here for two reasons. Primarily, these bounds do not account for the impact of the
structural assumptions we make on our model. In fact, we require bounds that accurately reﬂect the sample
complexity of estimation algorithms for LSR-structured coeﬃcient tensors. Secondly, the link function in
GLMs also involves the analysis of the space of LSR-structured coeﬃcients, making this part of our analysis
non-trivial and fundamentally diﬀerent to such works. We elaborate this point further in Section 6.3. The
derived minimax bound in this section conveniently generalizes the CP and Tucker tensor structures and
can be specialised to existing bounds in the literature such as that for Tucker-structured linear regression
(Zhang et al., 2020).
We now introduce some standard lemmas and assumptions used in this work.
Assumption 1 (Covariate Distribution) .ForX∈Rm1×···×mK, deﬁne vec(X),xand/tildewidem=/producttext
k∈[K]mk.
Then, x∼N(0,Σx), and thus E[x] =0andE[xxT] =Σx.
Lemma 1 ((McCullagh & Nelder, 2019)) .Any observation ygenerated according to a distribution from
the exponential family has mean a/prime(η), i.e., the ﬁrst derivative of a(η), and variance a/prime/prime(η), i.e., the second
derivative of a(η).
Assumption 2. The ﬁrst derivative of the cumulant function, a(η), with respect to ηis bounded uniformly
by a constant M≥0:a/prime(η)≤M.
Lemma 2. Consider the standard GLM problem in Deﬁnition 1 with negative log-likelihood function
n/summationdisplay
i=1log(b(yi)) +n/summationdisplay
i=1/parenleftbig
/angbracketleftb,xi/angbracketrightTT(yi)−a(/angbracketleftb,xi/angbracketright)/parenrightbig
. (22)
The gradient of (22)with respect to bis/summationtextn
i=1/parenleftbig
T(yi)−g−1(/angbracketleftb,xi/angbracketright)/parenrightbig
xi.
The proof of Lemma 2 follows the same steps explained in McCullagh & Nelder (2019). Assumption 1 states
that vec(X),xis a zero-mean Gaussian random variable with covariance matrix Σx. We do not place any
further assumptions on Σx. Assumption 2 implies that the mean of any GLM observation yis bounded.
This is a common assumption made in the literature (Lee & Courtade, 2020). For binary logistic regression,
e.g., this assumption is satisﬁed with M= 1. For linear or Poisson regression, this assumption implies that
the energy of yis bounded. Though the range of yisRandNfor linear and Poisson regression, respectively,
any outcome can be bounded by a positive constant Mwith high probability in most instances.
10Published in Transactions on Machine Learning Research (08/2023)
4 Estimation Problem and Algorithm
To solve (18) we propose an approach similar to those found in prior works (Li et al., 2018; Zhou et al., 2013;
Zhang & Jiang, 2016; Tan et al., 2013). The main idea behind this approach is to recognise that although
the objective function in (18) is non-convex in all elements Gand{B(k,s)}k∈[K],s∈[S]jointly, it is convex with
respect to each element separately. In this work, we propose a Block Coordinate Descent (BCD) algorithm
that estimates each element of the LSR structured tensor Balone while holding all other elements constant.
We note for the reader that BCD is simply Alternating Minimisation (AM) when there are only two factors
to be estimated. Additionally, one may use any solver to solve each convex sub-problem to estimate each
element; however, we choose to solve it via gradient steps in this work. First, for every k/prime∈[K]ands/prime∈[S],
we estimate the factor matrix Bk/prime,s/primewhile keeping all other factor matrices {B(k,s)}k∈[K]\k/prime,s∈[S]\s/primeand core
tensor Gﬁxed. Secondly, we estimate the core tensor Gwhile keeping all factor matrices {B(k,s}k∈[K],s∈[S]
ﬁxed. We also point out that though our BCD approach is similar to some existing works, in this work we
also show how each sub-problem reduces to a smaller scale GLM problem with fewer learnable parameters.
We also evaluate the computational complexity of each sub-problem. We derive the optimisation problem
for each step of the algorithm below.
4.1 Estimating Factor Matrices
To estimate the factor matrices we perform gradient descent over our objective function and project each
iterate onto the Stiefel manifold, with the objective function corresponding to the block B(k/prime,s/prime)given by:
Ln/parenleftbig
B(k/prime,s/prime)/parenrightbig
(23)
=n/summationdisplay
i=1
/angbracketleftBigg
B(k/prime,s/prime),Xi(k/prime)
/circlemultiplydisplay
−[K]\k/primeB(k,s/prime)
GT
(k/prime)/angbracketrightBigg
+/summationdisplay
s∈[S]\s/prime/angbracketleftbig
G×[K]B(k,s),Xi/angbracketrightbig
T(yi)
−a
/angbracketleftBigg
B(k/prime,s/prime),Xi(k/prime)
/circlemultiplydisplay
−[K]\k/primeB(k,s/prime)
GT
(k/prime)/angbracketrightBigg
+/summationdisplay
s∈[S]\s/prime/angbracketleftbig
G×[K]B(k,s),Xi/angbracketrightbig
.
In order to derive the expression in (23) for every B(k/prime,s/prime)fork/prime∈[K]ands/prime∈[S], notice that:
/angbracketleftB,X/angbracketright=/angbracketleftbig
G×1B(1,s/prime)×2···×KB(K,s/prime),X/angbracketrightbig
+/summationdisplay
s∈[S]\s/prime/angbracketleftbig
G×1B(1,s)×2···×KB(K,s),X/angbracketrightbig
=/angbracketleftBig
B(k/prime,s/prime),X(k/prime)/parenleftbig
B(K,s/prime)⊗···⊗ B(k/prime+1,s/prime)⊗B(k/prime−1,s/prime)⊗···⊗ B(1,s/prime)/parenrightbig
GT
(k/prime)/angbracketrightBig
(24)
+/summationdisplay
s∈[S]\s/prime/angbracketleftbig
G×1B(1,s)×2···×KB(K,s),X/angbracketrightbig
.
Now we deﬁne the following notations in order to keep the expressions in (24) concise. From the ﬁrst
summand in (24) we deﬁne
ω(k/prime,s/prime)= vec/parenleftBig
X(k/prime)/parenleftbig
B(K,s/prime)⊗···⊗ B(k/prime+1,s/prime)⊗B(k/prime−1,s/prime)⊗···⊗ B(1,s/prime)/parenrightbig
GT
(k/prime)/parenrightBig
, (25)
and from the second summand in (24) we deﬁne
γ(k/prime,s/prime)=/summationdisplay
s∈[S]\s/prime/angbracketleftbig
G×1B(1,s)×2···×KB(K,s),X/angbracketrightbig
. (26)
We can then express (24) as
/angbracketleftB,X/angbracketright=/angbracketleftbig
vec(B(k/prime,s/prime),ω(k/prime,s/prime))/angbracketrightbig
+γ(k/prime,s/prime)=/angbracketleftbig/bracketleftbig
vec(B(k/prime,s/prime)),1/bracketrightbig
,/bracketleftbig
ω(k/prime,s/prime),γ(k/prime,s/prime)/bracketrightbig/angbracketrightbig
. (27)
11Published in Transactions on Machine Learning Research (08/2023)
By deﬁning/tildewidex,/bracketleftbig
ω(k/prime,s/prime),γ(k/prime,s/prime)/bracketrightbig
, we rewrite (23) as
arg min
B(k/prime,s/prime)∈Omk/prime×rk/primen/summationdisplay
i=1/parenleftbig/angbracketleftbig/bracketleftbig
vec(B(k/prime,s/prime)),1/bracketrightbig
,/tildewidexi/angbracketrightbig/parenrightbig
T(yi)−a/parenleftbig/angbracketleftbig/bracketleftbig
vec(B(k/prime,s/prime)),1/bracketrightbig
,/tildewidexi/angbracketrightbig/parenrightbig
. (28)
The most important insight here is that the resulting problem can be viewed as a parameter estimation prob-
lem for GLMs with B(k/prime,s/prime)as the ‘parameter’ and /tildewidexias the ‘predictor’ or structured covariates. Estimating
B(k/prime,s/prime)alone in particular results in a low-dimensional problem with mk/primerk/primeparameters. We solve (23) via
projected gradient descent. Using the above deﬁned notations, the gradient of Ln(B(k/prime,s/prime))with respect to
B(k/prime,s/prime)is
∂Ln
∂vec(Bk/prime,s/prime)=n/summationdisplay
i=1/parenleftbig
T(yi)−g−1/parenleftbig/angbracketleftbig/bracketleftbig
vec(B(k/prime,s/prime)),1/bracketrightbig
,/tildewidexi/angbracketrightbig/parenrightbig/parenrightbig
/tildewidexi, (29)
and the projection operator H:Rmk/prime×rk/prime−→Omk/prime×rk/primethat projects the obtained iterate onto the manifold
of orthogonal matrices is deﬁned as
H(B(k/prime,s/prime)),arg min
/hatwideB∈Omk/prime×rk/prime/vextenddouble/vextenddouble/vextenddouble/hatwideB−B(k/prime,s/prime)/vextenddouble/vextenddouble/vextenddouble2
F. (30)
The solution to the problem in (30) is simply obtained using the QR decomposition of B(k/prime,s/prime).
4.2 Estimating the Core Tensor
For core tensor G, we are only required to perform gradient descent. The optimisation problem is
arg min
GLn(G),where (31)
Ln(G) =n/summationdisplay
i=1
/angbracketleftBigg
g,/summationdisplay
s∈[S]
/circlemultiplydisplay
−[K]B(k,s)
T
xi/angbracketrightBigg
T(yi)−a
/angbracketleftBigg
g,/summationdisplay
s∈[S]
/circlemultiplydisplay
−[K]B(k,s)
T
xi/angbracketrightBigg
.
In order to see the derivative of the expression in (31), notice that,
/angbracketleftB,X/angbracketright=/angbracketleftvec(B),vec(X)/angbracketright=/summationdisplay
s∈[S]/angbracketleftBigg
/circlemultiplydisplay
−[K]B(k,s)
g,x/angbracketrightBigg
=/angbracketleftBigg
g,/summationdisplay
s∈[S]
/circlemultiplydisplay
−[K]B(k,s)
T
x/angbracketrightBigg
.(32)
With a slight overload of notation we also deﬁne the following:
/tildewidex=/summationdisplay
s∈[S]/parenleftbig
B(K,s)⊗···⊗ B(1,s)/parenrightbigTx, (33)
and further express (32) and (31) as
/angbracketleftB,X/angbracketright=/angbracketleftg,/tildewidex/angbracketright (34)
and
arg min
Gn/summationdisplay
i=1(/angbracketleftg,/tildewidexi/angbracketright)T(yi)−a(/angbracketleftg,/tildewidexi/angbracketright), (35)
respectively. Once again, the resulting problem can be viewed as a parameter estimation problem for GLMs
withGas the ‘parameter’ and /tildewidexias the ‘predictor’ or structured covariates. Estimating Galone results in
a low-dimensional problem with/producttext
k∈[K]rkparameters. We solve (31) via gradient descent, where the gradient
ofLn(G)with respect to Gis
∂Ln
∂vec(G)=n/summationdisplay
i=1/parenleftbig
T(yi)−g−1(/angbracketleftg,/tildewidexi/angbracketright)/parenrightbig
/tildewidexi. (36)
12Published in Transactions on Machine Learning Research (08/2023)
4.3 Final Algorithm: LSRTR
We summarise the procedure discussed above in Algorithm 1 and we name our algorithm Low Separation
Rank Tensor Regression (LSRTR). We also show the prediction procedure performed using the estimated
coeﬃcient tensor in Algorithm 2, where we calculate the mean E[y|X]of the posterior distribution based
on the estimated coeﬃcient tensor Bfrom Algorithm 1. The posterior mean allows us to make predictions
for an observation yand report conﬁdence probabilities. Note that the convergence of BCD on non-convex
problems is a function of the initialisation. In this work, we initialise LSRTR randomly on the constraint
set. That is: an LSR-structured tensor with (KS)orthogonal factor matrices, randomly generated on the
Stiefel manifold, and a core tensor with random Gaussian entries. We also note that though BCD algorithms
such as LSRTR are popular amongst tensor-structured regression works and have been shown to be eﬀective
in practice (Zhou et al., 2013; Tan et al., 2013; Li et al., 2018; Zhang & Jiang, 2016), BCD algorithms in
prior tensor-structured GLM works do not explicitly exploit the coeﬃcient tensor’s Kronecker structure that
appears upon vectorization. Keeping a common core tensor and maintaining the LSR matrix structure in
(14) allows us to explore other parameter estimation algorithms that exploit the Kronecker matrix structure,
similar to those in existing dictionary learning works (Ghassemi et al., 2020). However, we leave this for
future work.
We next provide a brief discussion on the per-iteration computational complexity of each sub-problem of
LSRTR.Forfactormatrix B(k,s), eachgradientstephascomplexityoforder O(mkrk), andtheQRprojection
step has complexity of order O((mkrk)3). Thus the per-iteration computational complexity of estimating a
factor matrix isO((mkrk)3). Since there is no projection when estimating the core tensor G, its per-iteration
sample complexity is just O(/producttext
krk).
Algorithm 1 LSRTR: A block coordinate descent algorithm for LSR-TGLMs
1:Input:ntraining samples {Xi,yi}n
i=1, step sizeα, separation rank S, tensor rank (r1,r2,...,rK).
2:Initialise: Factor matrices B0
(k,s)∀k∈[K],s∈[S], core tensor G0andt←0.
3:repeat:
4:fors/prime∈[S]do
5:fork/prime∈[K]do
6:/tildewideB(t)
(k/prime,s/prime)←vec/parenleftBig
B(t)
(k/prime,s/prime)/parenrightBig
−α/summationtextn
i=1/parenleftbig
T(yi)−g−1/parenleftbig/angbracketleftbig/bracketleftbig
vec(B(k/prime,s/prime)),1/bracketrightbig
,/tildewidexi/angbracketrightbig/parenrightbig/parenrightbig
/tildewidexi
7: B(t+1)
(k/prime,s/prime)←H/parenleftBig
/tildewideB(t)
(k/prime,s/prime)/parenrightBig
8:end for
9:end for
10:/tildewideG(t+1)←vec(G(t))−α/summationtextn
i=1/parenleftbig
T(yi)−g−1(/angbracketleftg,/tildewidexi/angbracketright)/parenrightbig
/tildewidexi
11:t←t+ 1
12:untilconvergence
13:return/hatwideB←/summationtext
s∈[S]G(t)×1B(t)
(1,s)×2···×KB(t)
(K,s)
Algorithm 2 Posterior prediction for LSR-TGLMs
InputEstimate/hatwideB∈Rm1×···×mKandntetest data points{Xi}nte
i=1
Output Expectation/hatwideµ= [E[y1|X1],E[y2|X2]...E[ynte|Xnte]
1:Deﬁne:X,/bracketleftbig
X1,X2,...,Xnte/bracketrightbig
2:Compute/hatwideµfor inputXas:/hatwideµ=g−1(/angbracketleft/hatwideB,X/angbracketright)
3:return/hatwideµ
In this work, we do not provide convergence guarantees for LSRTR; this is a non-trivial task that we defer
to future work. The main challenge in proving convergence is the constraint set in LSRTR, which is a prod-
uct space of Stiefel manifolds and is neither closed nor convex. The non-convexity of the constraint space
makes a convergence analysis for LSRTR diﬃcult since we cannot apply general results for the convergence
of BCD (Bertsekas, 2016). Therefore, one can at best expect LSRTR to converge to a local minimum. The
13Published in Transactions on Machine Learning Research (08/2023)
non-convexity also presents challenges for analysing projections, in particular when using the QR decompo-
sition. More speciﬁcally, projection operators in projected gradient methods must be non-expansive in order
to prevent potential ampliﬁcation of the estimation error. Since the general method of proving the non-
expansiveness of a projection also assumes a closed and convex constraint set – which the Stiefel manifold is
not (Bertsekas, 2009) – demonstrating the non-expansiveness of QR projection in the general sense is also
non-trivial. In many works, however, the QR decomposition is a common tool for projecting onto and/or
optimising over the Stiefel manifold (Absil et al., 2008), yet proving stronger guarantees on optimisation
over Stiefel manifold is a larger and non-trivial problem. Therefore, we leave the theoretical analysis of
Algorithm 1 as a signiﬁcant direction for future works. However, we make some promising remarks on its
practical performance, convergence, and projection behaviour. Speciﬁcally in Section 5 we assess the perfor-
mance of our proposed algorithm on various GLM problems for 2-dimensional and 3-dimensional synthetic
data. Moreover, in Section 5.2, we use the LSR-TGLM model and the proposed LSRTR algorithm to solve
classiﬁcation problems on several real-world medical imaging datasets.
5 Numerical Study and Experiments on Medical Imaging Data
In this section we provide a comprehensive numerical study in order to assess the eﬃcacy of the LSR-
TGLM regression model and the corresponding proposed LSRTR algorithm. The objectives of this study
are three-fold. First, we investigate the performance of our algorithm, particularly: 1)The performance
of our proposed approach (parameter estimation and prediction) against a substantial growth in number
of parameters, and 2)The performance of our algorithm, particularly when faced with a large increase in
sample size. Secondly, we compare the estimation and prediction accuracy gains of the LSR model to current
regression models and algorithms in the literature on synthetic data (Zhou et al., 2013; Li et al., 2018; Zhang
et al., 2020; McCullagh & Nelder, 2019; Seber & Lee, 2003). Thirdly, we assess the performance of the
LSR-TGLM model on classiﬁcation problems for medical imaging datasets.
5.1 Experiments on Synthetic Data
We begin with experiments on synthetic data for three GLM problems: linear regression, logistic regression
and Poisson regression. The purpose of the experiments is to answer the following questions:
1. When the underlying coeﬃcient tensor has an LSR structure, does the proposed LSRTR algorithm
lead to reliable parameter estimation? Speciﬁcally, does our algorithm achieve a reduction in sample
complexity and estimation error compared to vector-based methods (for a ﬁxed sample size)?
2. Does our proposed algorithm: 1)provide reliable parameter estimation and prediction against a
substantial growth in number of parameters and 2)have per-iteration computation time comparable
to other tensor-based methods, when faced with a large increase in sample size?
3. Empirical convergence behaviour: does the proposed algorithm converge to a stationary point?
In our experiments we compute and report the following:
1. A coeﬃcient tensor /hatwideBestimated through the following learning methods: (i)The LSRTR method
(Algorithm 1), (ii)Low-rank Tucker model methods, speciﬁcally, a ‘block relaxation’ algorithm
for parameter estimation of Tucker-structured GLMs proposed by Li et al. (2018) (we will call
this procedure TTR), and a similar iterative procedure for logistic Tucker regression (LTuR) with
Frobenius norm regularization proposed by Zhang & Jiang (2016), (iii)unstructured (vector-based)
methods for regression. In the case of (iii)we use Least Squares (LS) for linear regression, a ﬁrst-
order method we name LR (Seber & Lee, 2003) for logistic regression, and a GLM ﬁtting algorithm
we name PR, with log link function from McCullagh & Nelder (2019) for Poisson regression. The
performance of each method is evaluated via the normalised estimation error deﬁned as/vextenddouble/vextenddoubleB−/hatwideB/vextenddouble/vextenddouble2
F
/bardblB/bardbl2
F.
14Published in Transactions on Machine Learning Research (08/2023)
2. A predicted response vector /hatwidey. The prediction accuracy for linear, logistic and Poisson regression
is evaluated via the normalised squared error deﬁned as/bardbl/hatwidey−y/bardbl2
2
/bardbly/bardbl2
2, the Mean Absolute Error (MAE)
deﬁned as/bardbl/hatwidey−y/bardbl1
nteforntetesting samples, and the normalised squared logarithmic error deﬁned as
/bardbllog(/hatwidey+1)−log(y+1)/bardbl2
2
/bardbllog(y+1)/bardbl2
2, respectively.
3. The magnitude of the computed gradient of the loss function, denoted as /bardblOL(·)/bardbl2, at the ﬁnal
iteration of every sub-problem in LSRTR.
Since we are reporting normalised estimation error, we only consider errors below 1. Indeed, the normalised
error compares the performance of an estimator relative to the ‘trivial estimator’ that always outputs 0. A
normalised error of 1occurs when the estimate is 0, yet an algorithm can potentially perform worse than the
trivial estimator, allowing an error greater than 1. Such an error is insubstantial and we disregard it when
analysing the performance of models and algorithms.
5.1.1 Experimental Setup
In our experiments we generate an (r1,r2,...,rK)-rank LSR-structured coeﬃcient tensor B∈Rm1×···×mK
with separation rank Sin (9) as follows. The entries of the core tensor Gare sampled from a
Gaussian (0,1
r1r2...rK)distribution. Each factor matrix B(k,s)is constructed by ﬁrst constructing matrix
A(k,s)∈Rmk×rkwith independent standard normal entries. The QR decomposition is then applied to
A(k,s)in order to obtain the orthonormal matrix /tildewideA(k,s)∈Omk×mk. The ﬁrst rkcolumns are then ex-
tracted from /tildewideA(k,s)in order to obtain B(k,s). With the above construction, we have /bardblB/bardbl2
F≤S2/tildewider/bardblG/bardbl2
F
(we derive a more precise bound on /bardblB/bardbl2
Fin Section 6.3). We also generate ndata samples{X,y}used
for training and estimation, and ntesamples used for testing and prediction. We generate independent
tensor-structured covariates {Xi}n
i=1and{Xi}nte
i=1with zero-mean Gaussian entries and covariance Σx, and
observations{yi}n
i=1and{yi}nte
i=1which are randomly generated according to the probabilistic model in (13)
and with appropriate link function g(µ). Additionally, zis assumed to follow a standard Normal distribution.
Thus, for linear regression, the observation yiconditioned on data Xifollows a Gaussian distribution, i.e.,
yi∼N (/angbracketleftB,Xi/angbracketright,1). In logistic regression, the observation yiconditioned on data Xifollows a Bernoulli
distribution, i.e., yi∼Bernoulli/parenleftbigg
1
1+exp(−/angbracketleftB,Xi/angbracketright)/parenrightbigg
. Finally, in Poisson regression, the observation yicon-
ditioned on data Xifollows a Poisson distribution, i.e., yi∼Poisson (exp(/angbracketleftB,Xi/angbracketright)). The experiment for
each GLM problem is performed for various model sizes (mk)k∈[K], tensor ranks (rk)k∈[K]and separation
rankS. The coeﬃcient tensor Band data samples {Xi,yi}n
i=1and{Xi,yi}nte
i=1are generated as described
above and each experiment is repeated for increasing value of n. We then compute the estimate /hatwideBand
prediction/hatwidey= [/hatwidey1,/hatwidey2,...,/hatwideynte] = [/hatwideµ1,/hatwideµ2,...,/hatwideµnte]where/hatwideµi=g(−1)/parenleftBig/angbracketleftBig
/hatwideB,Xi/angbracketrightBig/parenrightBig
. Finally, for each GLM
problem, unless otherwise stated, we conduct 50repetitions of each experiment (for a ﬁxed coeﬃcient tensor
B) and average the error over the repetitions.
5.1.2 2D Synthetic Data
We begin with the two-dimensional (matrix) case with K= 2as many medical imaging data fall under
this case (x-rays, EEG, ﬁber-bundle images.). In this set of experiments the underlying coeﬃcient matrix
Bhas dimensions m×mand rankr. The dimensions and rank of Bare thus represented as the tuple
(m,r). For all GLM problems we ﬁx the separation rank S= 2; thus the number of learnable parameters
isS(mr+mr) +r2= 4mr+r2. We now proceed with the speciﬁc experimental setups of the three GLM
problems under study.
For linear regression we consider various model sizes and ranks, i.e., m∈{64,128,256},r∈{4,8}. We range
sample size nfrom (roughly) the degrees of freedom (number of learnable parameters) of the smallest model
size to no smaller than the degrees of freedom of the largest model size. The smallest model size is for the
tuple (m= 64, r= 4)and the largest model size is for the tuple (m= 256, r= 8), which, under the LSR
15Published in Transactions on Machine Learning Research (08/2023)
structure, have 4(64×4) + 42= 1040and4(256×8) + 82= 8256learnable parameters, respectively. We
compare the performance of LSRTR to TTR and LS. Figure 2 reports the mean normalised estimation and
prediction accuracy, respectively, across 50repetitions. The shaded regions are one standard deviation of
mean estimation and prediction accuracy, based on 50 replications.
For logistic and Poisson regression we consider m∈{32,64,128}andr∈{4,8}. Since the models are non-
linear, we follow the heuristic rule of ranging sample size nfrom (roughly) 5times the degrees of freedom
of the smallest model size to no smaller than that of the largest model size (Li et al., 2018). The smallest
model size is for the tuple (m= 32, r= 4)and the largest model size is for the tuple (m= 128, r= 8),
which, under the LSR structure, have 4(32×4) + 42= 528and4(128×8) + 82= 4160learnable parameters,
respectively. We compare the performance of the LSRTR method to i)LTuR and LR and ii)TTR and PR
for logistic and Poisson regression, respectively. Figure 3 and Figure 10 in the appendix report the estimation
and prediction accuracy for logistic and Poisson regression, respectively, with shaded regions depicting one
standard deviation of mean estimation and prediction accuracy, based on 50 replications.
We pause to make a few remarks. Firstly, in the case of synthetic data, the rank rand separation rank Sare
assumed to be known and other algorithmic parameters (the step size α) are set using separate validation
experiments. Secondly, though the generated coeﬃcient Bis a matrix, the numerical study by Li et al.
(2018), shows that the low-rank Tucker model eﬀectively estimates two-dimensional parameters and the
TTR algorithm performs well in the matrix setting even with huge increases in model and sample sizes. The
study by Li et al. (2018) also shows how TTR generalizes CP tensor regression algorithms and improves
upon the performance of several other methods (such as PCA and Bayesian regression methods) in terms
of estimation and prediction accuracy in experiments with synthetic data and for several regression types
(linear and logistic) (Li et al., 2018). For these reasons we limit our scope of comparative algorithms for
matrix-structured regression to TTR.
We also note that since we report normalised errors, we do not investigate any errors greater than 1in
Figures 2, 3 and 10. The plots in Figures 2, 3 and 10 show that, for all algorithms in all GLM problems,
the estimation and prediction accuracy improves and the shaded regions (which characterise the standard
deviation of the error points over 50 repetitions) decay with an increase in observations. In particular, on
the whole, the tensor-based methods LSRTR, TTR and LTuR outperform the vector-based methods LR,
LS and PR, particularly with larger model size m. This is because the number of observations provided in
the experiments is typically much lower than the sample complexity of the vector methods. For example,
consider linear regression with m= 256. LS requires at least 256×256 = 65,536observations for reliable
parameter estimation, while if we set r= 4andS= 2, TTR and LSRTR require only 2064and4112
observations for reliable parameter estimation, respectively. Additionally, for TTR, LTuR and LSRTR, as
the core tensor dimensions grow (i.e., as we increase the Tucker rank of our tensor models), we require a
relatively larger sample size to achieve better accuracy. This is not surprising as a larger core tensor increases
the number of parameters to be learned. Figures 2, 3 and 10 also show that for all GLM problems under
study (namely linear, logistic and Poisson regression), the LSRTR algorithm shows ‘consistent’ performance
for estimation and prediction. What we mean by this is that, even with a substantial growth in model size,
the estimation error induced by LSRTR decreases with increasing sample size, and the LSRTR algorithm
can still achieve acceptable estimation and prediction errors as long as the number of observations provided
is large enough. Additionally, we can see that LSRTR algorithm can avoid over-ﬁtting in the prediction step
even with a very large number of observations.
We also gain some additional insights from the results. First, if the underlying parameter model Bis
LSR-structured, then LSRTR can recover Bwith a relatively low number of observations, in the sense that
the number of observations required is proportional to the intrinsic degrees of freedom in the LSR model
(rather than the extrinsic dimensionality of B). This implies that though we do not provide any theoretical
guarantees for Algorithm 1, it can be employed successfully in practice. Second and more interestingly, if the
underlying parameter model Bis LSR-structured, then imposing the Tucker model on Bin the parameter
estimation procedure – as in TTR and LTuR algorithms – results in less accurate estimation compared to
LSRTR. Particularly, we see that for relatively large model sizes, such as in Figures 2e and 2f, or for non-
linear models, such as in Figures 3e, and 10e, TTR and LTuR algorithms’ performance plateaus, even when
the number of observations has far exceeded the degrees of freedom of a Tucker-structured coeﬃcient matrix
16Published in Transactions on Machine Learning Research (08/2023)
(a)
 (b)
(c)
 (d)
(e)
 (f)
Figure 2: Comparison of LSRTR with LS and TTR for two-dimensional synthetic data when m∈
{64,128,256},r∈{4,8}andS= 2. Normalised estimation error for m= 64,128,and256is shown
in(a),(c),and(e), respectively. Normalised prediction error for m= 64,128,and256is shown in (b),(d),
and(f), respectively. Each marker represents the mean normalised estimation/prediction errors, over 50
repetitions. The shaded regions correspond to one standard deviation of the mean normalised errors.
17Published in Transactions on Machine Learning Research (08/2023)
(a)
 (b)
(c)
 (d)
(e)
 (f)
Figure 3: Comparison of LSRTR with LR and LTuR for two-dimensional synthetic data when m∈
{32,64,128},r= 4andS= 2. Normalised estimation error for m= 32,64,and 128is shown in
(a),(c),and(e), respectively. Normalised prediction error for m= 32,64,and128is shown in (b),(d),
and(f), respectively. Each marker represents the mean normalised estimation/prediction errors, over 50
repetitions. The shaded regions correspond to one standard deviation of the mean normalised errors.
18Published in Transactions on Machine Learning Research (08/2023)
B. In fact, even LS and LR outperform TTR and LTuR when the model size is small enough and enough
observations have been provided. Though we expect LSRTR to perform better than TTR or LTuR when
Bis originally LSR-structured, the results suggest that if regression problems with real data have model
parameters that are approximately-LSR structured, then TTR or LTuR may not be suitable algorithms and
we can motivate the use of algorithms such as LSRTR that allow for the consideration of a richer class of
tensors that may lead to more reliable estimation.
Notice also that in Figures 2e and 2f (as well as in Figures 3e and 3f), when r= 8, the normalised errors
are greater than 1. Here, LSRTR is operating in the under-sampling regime, where the model size is very
large and the number of available samples (9000)is just about the number of learnable parameters of the
model. In such a setting we expect the estimation error to be large and to decrease with increasing samples.
We also make speciﬁc remarks on Figures 3c and 3e. In the case of m= 64, we see that LSRTR shows
lower estimation error at n= 17500 (0.11vs0.29), yet has a larger standard deviation (shaded region) over
50experiment replications ( 0.04vs0.006). This is an acceptable standard deviation, especially considering
the reduction in estimation error compared to the Tucker-based LTuR algorithm. In the case of m= 128,
LTuR shows lower estimation error than LSRTR until about n= 7500(errors of 0.6vs0.4). This could
be due to the fact that LSRTR is operating in the undersampling regime for n= 7500(as LSR has more
learnable parameters than Tucker). Additionally, we see in Figure 3e that the estimation error for LTuR
increases after n= 7500, and in Figure 3f that LSRTR outperforms LTuR, which suggests that LTuR may
be overﬁtting the GLM model.
We also make a speciﬁc remark on Figure 10a in the appendix, where we witness a slight increase in
estimation error for (m,r) = (32,8)betweenn= 5000andn= 7500(0.025vs0.08). However, we see that
the estimation error continues to decrease after n= 7500, and we do not witness this phenomena in any
other experiment in Figure 10.
5.1.3 3D Synthetic Data
We have evaluated the performance of the LSRTR algorithm against an increase in model and sample
size forK= 2. Now, we wish to further explore the performance of our method compared to the other
state-of-the-art methods with 3-dimensional data ( K= 3). In these experiments, the underlying coeﬃcient
tensor Bhas dimensions m1×m2×m3and Tucker rank {r,r,r}. The dimensions and rank of Bare
thus represented as the tuple (m,r)where m= [m1,m2,m3]. We study the linear regression problem
and we ﬁx the separation rank S= 1, i.e., Bis Tucker structured. Thus Bis a3-dimensional Tucker
tensor of dimension m1×m2×m3and rankralong each mode, and the number of learnable parameters is
S(m1r+m2r+m3r) +r3=r(m1+m2+m3+r2). We consider the model size m= [16,32,64], r= 4.
The model size under the Tucker structure is 4(16 + 32 + 64 + 42) = 512. We range sample size nfrom
(roughly) two times the degrees of freedom to roughly eight times the degrees of freedom. We compare the
performance of LSRTR to TTR and LS. Figure 4 reports the estimation accuracy and prediction accuracy,
respectively, with shaded regions depicting one standard deviation of mean estimation accuracy, based on
50 replications. For LSRTR the rank ris assumed to be known and other parameters (the separation rank
Sand the step size α) are set using separate validation experiments. The separation rank was thus chosen
asS= 2.
We observe from Figure 4 the same trends as in the previous set of experiments. We also conclude that even
with the underlying coeﬃcient tensor being Tucker-structured, our proposed LSRTR method in this experi-
mentexhibitsbetterstatisticalperformancewithfewerobservationsthanthe‘Tuckerspeciﬁc’algorithmTTR
as well as LS. This suggests that the LSR model induces a richer class of solutions with greater representation
power that is able to capture meaningful information in the data that the more compact state-of-the-art
tensor models do not – even with increased sample size. The results also suggest that the projection step in
LSRTR (imposing orthonormal factor matrices) may contribute to the enhanced performance of LSRTR in
terms of estimation/prediction error (as TTR does not impose such a constraint).
Finally, we numerically investigate the per-iteration computational complexity of LSRTR vs TTR for
three-dimensional synthetic data, for varying sample size n. We repeat the previous experiment, i.e.,
m= [16,32,64],r= 4,S= 2, over varying sample size ( n= 100,n= 300,n= 500), and report the
19Published in Transactions on Machine Learning Research (08/2023)
(a)
 (b)
Figure 4: Comparison of LSRTR with LS and TTR for three-dimensional synthetic data when m=
[16,32,64],r= 4,S= 1. For LSRTR, S= 2was chosen. Normalised estimation error is shown in
(a). Normalised prediction error is shown in (b). Each marker represents the mean normalised estima-
tion/prediction errors, over 50 repetitions. The shaded regions correspond to one standard deviation of the
mean normalised errors.
n=100 n =300 n =500
LSRTR 0.1 (7e−5) 0.37 (2e−4) 0.65 (5e−4)
TTR 0.048 (3e−5) 0.18 (1e−4) 0.31 (2e−4)
Table 2: Per-iteration computation time (in seconds) of LSRTR and TTR over varying sample size ( n= 100,
n= 300,n= 500). We report the mean (and variance) over 50repetitions.
mean per-iteration computation time (in seconds) over 50repetitions of the experiment, for LSRTR and
TTR. Table 2 shows the results. We can see that LSRTR and TTR have comparable per-iteration compu-
tation times. In fact LSRTR has computation time of roughly S= 2times that of TTR.
5.1.4 Convergence Behaviour of LSRTR
We have previously discussed the challenges of providing theoretical guarantees for the proposed algorithm,
speciﬁcally an analysis of its convergence and the non-expansiveness of the QR projection. Nonetheless, we
are still interested in developing an understanding of the practical behaviour of LSRTR. For two-dimensional
synthetic data, for ﬁxed sample size n, we repeat the experiment from Section 5.1.2, i.e., for linear regression
of model sizes (64,4),(128,4)and(256,4), withn= 7000, and logistic regression of model sizes (32,4),
(64,4)and(128,4), withn= 7500, and with S= 2. We report the per-iteration normalised estimation
error for LSRTR and the magnitude of the computed gradient at the ﬁnal iteration of every sub-problem in
Algorithm 1. Figure 5 depicts the convergence behaviour of LSRTR. We see that the per-iteration normalised
estimation error decreases with increasing number of iterations. In regards to the non-expansiveness of the
QR projection, these results suggest that the error does not amplify, and the QR step does not prohibit
LSRTR from ﬁnding a ‘good estimate’ of the underlying coeﬃcient tensor. The decrease in the estimation
erroralsosuggeststhatLSRTRconverges. Moreover, Table3depictsthemagnitudeofthecomputedgradient
of the loss function at the ﬁnal iteration of every sub-problem in LSRTR. We can see in all cases that the
magnitudes of the gradients are very close to 0, suggesting the convergence of LSRTR to a stationary point.
20Published in Transactions on Machine Learning Research (08/2023)
(a)
 (b)
Figure 5: Convergence behaviour of LSRTR for two-dimensional synthetic data for linear regression when
m∈{64,128,256}, andr= 4, forn= 7000, and logistic regression when m∈{32,64,128}, andr= 4,
forn= 7500. For LSRTR, S= 2was chosen. Normalised estimation error for linear and logistic regression
is shown in (a)and(b), respectively.
Linear Regression (m,r) Logistic Regression (m,r)
gradient norm (64,4) (128 ,4) (256 ,4) (32,4) (64,4) (128 ,4)
/vextenddouble/vextenddoubleOL(B(1,1))/vextenddouble/vextenddouble
23.1×10−35.2×10−34.3×10−30.24 0.38 0 .21/vextenddouble/vextenddoubleOL(B(1,2))/vextenddouble/vextenddouble
27.8×10−31.6×10−39.3×10−30.2 0.32 0 .3/vextenddouble/vextenddoubleOL(B(2,1))/vextenddouble/vextenddouble
21.7×10−24.7×10−38.73×10−40.4 0.1 0 .18/vextenddouble/vextenddoubleOL(B(2,2))/vextenddouble/vextenddouble
23.5×10−22.1×10−34×10−30.29 0.15 0 .23
/bardblOL(G)/bardbl27.42×10−43.8×10−31.2×10−30.37 0.28 0 .19
Table 3: Magnitude of the ﬁnal iteration gradient, per sub-problem, in linear and logistic regression.
5.2 Experiments on Medical Imaging Data
We move on to investigating our approach on medical imaging data. Regression analysis of medical imaging
data can be a useful tool in medical decision making. We chose this type of data for several reasons.
Medical images are usually multi-dimensional, and since data acquisition in medical sciences is expensive,
the regression problems under study with such data is very high dimensional. Another major reason is
that medical images model complex biological structures (such as brain maps). One expects the model
parameter in the corresponding regression model to model this complexity. Lastly, the set of observations in
medicalimagingdataisusuallyseverely imbalanced. Inotherwords, positivemedicaldiagnosesarerelatively
rare, and thus a given set of observations will contain far fewer positive diagnoses than negative diagnoses.
The high-dimensionality, complexity and imbalanced observation set of medical imaging data serve as an
excellent assessment of the eﬃciency of our proposed model and algorithm. That is, we contend that due
to the LSR model’s favourable bias-variance trade-oﬀ and augmented representational power, LSR-TGLM
may be well-suited to handling such data and may perform more robustly than some other compact tensor-
structured GLMs. Here we study three datasets; ABIDE Autism (Craddock et al., 2013; Lodhi & Bajwa,
2020), ADHD200 (Bellec et al., 2017) and Vessel MNIST 3D (Yang et al., 2021b).
21Published in Transactions on Machine Learning Research (08/2023)
5.2.1 ABIDE Autism
The Autism Brain Imaging Data Exchange (ABIDE) dataset contains the resting state fMRI data collected
from 98subjects. The data has already been preprocessed for motion realignment and correction, slice
timing correction and image normalisation. Each data sample corresponds to 111cortical and sub-cortical
brain regions scanned over 116time periods (Craddock et al., 2013). Therefore each sample is a 111×116
matrix of fMRI data from one subject. Each observation is a binary response variable y∈{0,1}depicting
a subject’s diagnosis of either having autism (y= 1)or not (y= 0). The goal is to classify the subjects
as either autistic or not. We perform a single train-test split procedure of 80and14training and testing
samples respectively. As for the autistic-control split, we choose 40autistic and 40control samples uniformly
at random for training, and 14test subjects the same way. The case-control ratio is this dataset is thus 1 : 1.
ABIDE Autism is the only balanced dataset in this set of experiments.
5.2.2 ADHD200
ADHD200 is a repository of resting state fMRI images of subjects from 8research sites: Peking University,
Brown University, Kennedy Krieger Institute, Donders Institute, NYU Child Study Center, Oregon Health
and Science University, University of Pittsburgh and Washington University in St. Louis. The data includes
brain maps of fractional Amplitude of Low-Frequency Fluctuations (fALFF) of 959child subjects. fALFF
brain maps can be useful in predicting Attention Deﬁcit Hyperactivity Disorder (ADHD) in children. The
datahasbeenpreprocessed, (ADHD-200Consortium,2012;Bellecetal.,2017), andwealsoperformstandard
preprocessing of the data such as removing missing observations or images with poor quality (a list of images
with poor quality has been made public, (ADHD-200 Consortium, 2012)). Each data sample corresponds to
a3-dimensional patient fMRI brain scan (T1-weighted image) of size 121×145×121. Figure 6 shows the
axial, sagittal and coronal views (views across each tensor mode) of a fMRI data sample. Each observation is
a binary response variable y∈{0,1}depicting a child subject’s diagnosis of either having ADHD (y= 1)or
being typically developing (y= 0). The objective here is classiﬁcation of children subjects with ADHD. The
dataset has already undergone a train-test split procedure. The training dataset consists of 762samples,
280of which are labelled as ADHD (hyperactive/impulsive, inattentive and combined) and 482of which are
labelled as typical (control). The testing dataset consists of 197samples, where 76and93are labelled as
ADHD and control, respectively. The data is slightly imbalanced where the case-control ratio in the training
and testing sets is 3 : 5and4 : 5, respectively.
(a)
 (b)
 (c)
Figure 6: Axial (a), coronal (b), and sagittal (c)views of an fMRI data sample.
5.2.3 Vessel MNIST 3D
MedMNIST (Yang et al., 2021b) is a collection of medical imaging datasets suitable for various tasks such as
classiﬁcation and regression. One of the datasets is the Vessel 3D dataset (Yang et al., 2020) containing 1909
reconstructions of vessel segments from brain Magnetic Resonance Angiography (MRA) data of diﬀerent
22Published in Transactions on Machine Learning Research (08/2023)
Dataset Covariate Dimensions Train Size Test Size
ABIDE Autism (111,116) 80 14
ADHD200 (121,145,121) 762 169
Vessel MNIST 3D (28,28,28) 1335 382
Table 4: Description of three medical imaging datasets (ABIDE ADHD, ADHD 200, and Vessel MNIST
3D). Described are the model dimensions and train and test sizes for all three datasets.
subjects. Each data point corresponds to 3-dimensional vessel segment of dimension 28×28×28. Each
vessel segment may or may not exhibit an intracranial aneurysm and the dataset is diverse in the sense that
it includes a variety of shapes and scales of intracranial aneurysms. The data has also been preprocessed
to restore incomplete scans made by neurosurgeons and to remove duplicated data. Each observation is
a binary response variable y∈{0,1}depicting a vessel segment diagnosis of either having an aneurysm
(y= 1)or being healthy (y= 0). The goal is to therefore classify each vessel segment as either exhibiting
an aneurysm or being healthy. The dataset has already undergone a train-test split procedure. The training
dataset consists of 1335samples, 150of which are labelled as ‘aneurysm’ and 1185of which are labelled
as ‘healthy’. The testing dataset consists of 382samples, where 43and339are labelled as ‘aneurysm’ and
‘healthy’, respectively. The data is severely imbalanced where the case-control ratio in both the training and
testing sets is 3 : 25.
5.2.4 Results of Experiments on Medical Imaging Data
Allthreemedicalimagingdatasetssupportalogisticregressionproblem. Weestimatetheunknowncoeﬃcient
tensor Bthrough our LSRTR algorithm and use our estimate to predict the responses of the test data
through a logistic regression model, i.e., we calculate the posterior probability using the sigmoid function
s= (1/(1 + exp(/angbracketleftB,Xi/angbracketright+z)))for every test subject iand use a cut-oﬀ threshold of 0.5to classify each
response as 1or0, where the positive class is chosen if s > 0.5. Parameters including the Tucker rank
(rk)k∈[K], the separation rank Sand the step size αare set – for each dataset – using separate validation
experiments. On all three datasets we compare our method with four other methods, two of which have been
previously introduced, namely, LR and LTuR. The third method is a low-rank CP model method (which we
name LCPR), speciﬁcally a block coordinate descent approach for parameter estimation of CP structured
logistic regression models proposed by Tan et al. (2013). The fourth method is a Support Vector Machine
(SVM) with Gaussian Kernel (Hearst et al., 1998). In the case of the Vessel MNIST dataset, we also compare
our method with an established baseline in the literature for Medical MNIST datasets: ResNet 50, which
is residual neural network that is 50layers deep. This neural network has also been augmented with a 3D
convolutional layer designed to handle 3-dimensional data.
To evaluate the performance of each method we report the following scores: Speciﬁcity deﬁned as the true
negative rate, sensitivity deﬁned as the true positive rate, the F 1score, the average accuracy score deﬁned as
the MAE, and the Area Under the Curve (AUC). We note that we penalise false positives and false negatives
with the same severity and compute the MAE to be consistent with the current literature (Li et al., 2018;
Zhou et al., 2013; Zhang & Jiang, 2016). We also note that it is important to use the F 1score and AUC,
as they are more appropriate metrics in the case of class imbalance, compared to average accuracy. Table 4
summarises the description of each dataset and Tables 5a, 5b, and 5c summarise the results. The chosen rank
(rk)k∈[K]for ABIDE Autism, ADHD 200and Vessel MNIST 3D are (6,6),(6,6,6)and(3,3,3), respectively.
The chosen LSR rank for LSRTR for ABIDE Autism, ADHD 200and Vessel MNIST 3D areS= 2,S= 3
andS= 2, respectively. The results show that the LSRTR method performs well in terms of sensitivity and
average accuracy for all datasets and the vector based-method LR has poorer performance with all datasets,
where for some datasets all observations are predicted as positive. Particularly with the ADHD 200dataset,
we are faced with the challenge of eﬃcient estimation as the sample size is very small with respect to the
model size ( 762vs.2,097,152). The relatively good performance of LSRTR suggests that LSRTR performs
well in the high-dimensional setting (i.e., when the sample size is signiﬁcantly smaller than the number of
23Published in Transactions on Machine Learning Research (08/2023)
covariates). We notice that LR performs very poorly with ADHD 200, as there are over 2million parameters
to estimate. With the chosen rank, the LSR model decreases the dimensionality to 2322×S+ 216which
enables signiﬁcantly more eﬃcient classiﬁcation. LTuR and LTR decrease the dimensionality to 2322 + 216
and2322, respectively, but exhibit poorer performance than LSRTR (and even SVM), suggesting that CP
and Tucker models cause a loss of representation power that is essential to parameter estimation for some
datasets. With the Vessel MNIST dataset we have more training samples than ADHD 200and a lower-
dimensional problem (only 21,952). For this reason all methods perform relatively well, with LSRTR having
one of the best performances. When compared to ResNet 50, our method approaches its performance in
terms of F 1score and beats its performance in terms of average accuracy.
We make a remark on Table 5a. The sensitivity for LSRTR is 1, which is likely due to the fact that we use
only 14test samples from the ABIDE Autism dataset. In order to provide better interpretability, Figure 7
shows a histogram of the posterior probabilities for the 14 test data samples. We see that all of the samples
labelled as y= 1are classiﬁed as such since their posterior probabilities are all over 0.5. Most samples of
Class 1are classiﬁed with high conﬁdence (in the sense that posterior probabilities are well above the 0.5
threshold). Only one sample of Class 0was mis-classiﬁed.
To provide a more comprehensive comparison between LSRTR and other state-of-the-art neural networks,
we also compare the performance of LSRTR on vessel MNIST dataset to a benchmark analysis (Yang et al.,
2021b). We compare three ResNet 18methods, with various additional convolution layers (namely 2.5D,
3DandACS, or, ‘axial-coronal-sagittal’), two additional ResNet 50methods, with additional convolution
layers (namely 2.5D and ACS) (Yang et al., 2021a)), and AutoKeras. The results are shown in Table 6. The
results show that, although LSRTR may be outperformed by some complex methods, it exhibits comparable
and occasionally superior performance to several others. Additionally, we note that while neural networks
often outperform regression models in certain ﬁelds like computer vision due to their increased ﬂexibility,
their potential overﬁtting in a small sample regime (limited sample sizes are often associated with medical
datasets from clinical studies) remains a concern. Moreover, regression models are frequently termed ‘white-
box models’ owing to the transparency and interpretability of their parameters. This interpretability – an
indispensable attribute in numerous applications – allows for a detailed analysis of covariates’ statistical
signiﬁcance and the generation of conﬁdence intervals (not just accuracy scores) as posterior probabilities
– qualities that are challenging to obtain with neural networks (Dreiseitl & Ohno-Machado, 2002; Issitt
et al., 2022). Such conﬁdence probabilities are important information for medical professionals to consider
in medical studies, as they provide valuable insights into the classiﬁcation results. Additionally, unlike the
neural network methods, LSRTR is easily extendable beyond the 3D case, and is trivially applied to other
regression types (such as linear and Poisson), and does not require ﬁne tuning for each case.
Figure 7: Histogram of posterior probabilities for test data in ABIDE Autism Dataset for the case of LSRTR.
24Published in Transactions on Machine Learning Research (08/2023)
SVM LR LCPR LTuR LSRTR
Sensitivity 0.71 0.71 0.71 0.71 1
Speciﬁcity 0.14 0.71 0.85 0.85 0.85
F1score 0.55 0.71 0.77 0.77 0.93
AUC 0.42 0.51 0.84 0.84 0.9
Average Accuracy 0.43 0.71 0.78 0.78 0.92
(a)ABIDE Autism
SVM LR LCPR LTuR LSRTR
Sensitivity 0.41 1 0 .62 0.51 0.83
Speciﬁcity 0.78 0 0 .34 0.52 0 .4
F1score 0.5 0.62 0.35 0.48 0.65
AUC 0.62 0.5 0.56 0.62 0.67
Average Accuracy 0.62 0.45 0.47 0.51 0.59
(b)ADHD 200
SVM LR LCPR LTuR LSRTR ResNet 50 + 3D
Sensitivity 0.39 0.53 0.26 0.32 0.47 0 .85
Speciﬁcity 0.95 0.55 0.946 0.94 0.96 0 .86
F1score 0.44 0.21 0.3 0.37 0.55 0.57
AUC 0.84 0.52 0.6 0.66 0.81 0.9
Average Accuracy 0.89 0.55 0.869 0.87 0.91 0.85
(c)Vessel MNIST 3D
Table 5: Comparison of LSRTR with LR, LCPR and LTuR for diagnosis of test subjects in datasets:
(a)ABIDE Autism (b)ADHD 200and(c)Vessel MNIST 3D. In the case of Vessel MNIST 3D, LSRTR is also
compared to ResNet 50.
RN18+2.5D RN18+3D RN18+ACS RN50+2.5D RN50+ACS Autokeras
AUC 0.74 0 .87 0 .93 0 .75 0 .9 0 .773
Table 6: Neural network benchmark analysis for classiﬁcation diagnosis of test subjects in dataset Vessel
MNIST 3D. RN stands for ResNet.
6 Minimax Lower Bounds for Tensor-Structured GLMs
In this section we derive lower bounds on the minimax risk of the LSR-TGLM problem1. We numerically
assess the tightness of the derived bound in Section 6.1. In the results we will derive, the minimax risk
depends explicitly on the parameters of the LSR-structured tensor model, namely {rk}k∈[K],{mk}k∈[K],S,
the distribution of Xand the number of samples n. Speciﬁcally, we derive lower bounds on ε∗using an
argument for estimation problems based on Fano’s inequality as described by Yu (1997). This approach
relates the minimax risk of coeﬃcient tensor Bto a multiple hypothesis testing problem. It states that if
there exists an estimator (let us call it /hatwideB(y,X), and which can be any parameter estimation algorithm),
1Preliminary results appeared in a conference paper taki2021minimax.
25Published in Transactions on Machine Learning Research (08/2023)
with error matching the minimax risk ε∗(i.e.,/vextenddouble/vextenddouble/vextenddouble/hatwideB(y,X)−B/vextenddouble/vextenddouble/vextenddouble2
F=ε∗), then this estimator /hatwideB(y,X)can
be used to solve a multiple hypothesis testing problem (MHTP) (Khas’minskii, 1979). In this MHTP, the
hypothesis set consists of a collection, BL, ofLLSR-structured coeﬃcient tensors, where the goal of the
estimator,/hatwideB(y,X), is to detect the correct ‘generating’ LSR-structured tensor. Fano’s inequality provides
a fundamental limit/bound on the error probability for the multiple hypothesis testing problem. This limit
in turn provides a lower bound on the minimax risk ε∗. As mentioned in Section 3, our analysis is local and
our approach to deriving a lower bound on the minimax risk is information-theoretic and thus involves the
analysis of the mutual information (deﬁned as I(y;l)) between observations y= [y1...yn]and hypothesis
l∈[L];
I(y;l),Ey,l/bracketleftbigg
logf(y,l)
f(y)f(l)/bracketrightbigg
, (37)
wheref(·,·)andf(·)are joint and marginal probability distribution functions respectively. We are now
ready to state our main result.
Theorem 6. Consider the rank- (r1,···,rK)and separation rank SLSR-TGLM problem in (13)withn
i.i.d observations,/braceleftbig
Xi,yi/bracerightbign
i=1, where vec(Xi)∼N(0,Σx), and the true coeﬃcient tensor /bardblB/bardbl2
F<d2. The
minimax risk ε∗is then lower bounded by
ε∗≥/parenleftbig1
16/parenrightbig
S/summationtext
k∈[K](mk−1)rk+/producttext
k∈[K](rk−1)−1
128M/bardblΣx/bardbl2n, (38)
whereMis deﬁned in Assumption 2.
Theorem 6 can be specialised to the Tucker and CP decomposition regression problems in the existing
literature. That is, when S= 1we have the Tucker model, and when r1=r2=···=rK=randGis
constructed as a diagonal tensor, we have the CP model. With these specialisations we obtain the following
corollaries:
Corollary 1. Consider the rank- (r1,···,rK)Tucker-TGLM problem (Li et al., 2018; Zhang et al., 2020;
Zhang & Jiang, 2016) with ni.i.d observations,/braceleftbig
Xi,yi/bracerightbign
i=1, where vec(Xi)∼N(0,Σx)and the true coeﬃ-
cient tensor/bardblB/bardbl2
F<d2. The minimax risk for this problem is lower bounded by
ε∗≥/parenleftbig1
16/parenrightbig/summationtext
k∈[K](mk−1)rk+/producttext
k∈[K](rk−1)−1
128M/bardblΣx/bardbl2n. (39)
Corollary 2. Consider the rank- rCP-TGLM problem (Zhou et al., 2013; Tan et al., 2013) with ni.i.d
observations,/braceleftbig
Xi,yi/bracerightbign
i=1, where vec(Xi)∼ N (0,Σx)and the true coeﬃcient tensor /bardblB/bardbl2
F< d2. The
minimax risk for this problem is lower bounded by
ε∗≥/parenleftbig1
16/parenrightbig/summationtext
k∈[K](mk−1)r+ (r−1)−1
128M/bardblΣx/bardbl2n. (40)
Table 7 provides a summary of the minimax lower bounds from this work and the relevant current literature.
Speciﬁcally, it shows the order-wise lower bounds on the minimax risk for logistic regression, linear regression
and GLMs for several tensor structures on the model parameter. We make three remarks prior to our
discussion of Table 7. First, we deﬁne the terms /tildewidem=/producttext
k∈[K]mkand/tildewider=/producttext
k∈[K]rk. Additionally, σ2
yis
the variance of observation y,Dis a ﬁxed constant and an upper bound on the second derivative of the
cumulant function (a/prime/prime(η)), and we place a dash ( −) in the cells where there are no speciﬁc results in the
existing literature. However, note that our derived minimax bounds for tensor-structured GLMs cover these
gaps in the literature (namely the case of CP-structured linear and logistic regression, and Tucker-structured
logistic regression). Secondly, the order-wise lower bounds reported for the unstructured case are in fact
bounds on the minimax risk for prediction error, rather than estimation error. Nonetheless we include them
in our comparison as the sample complexities of estimation and prediction in a given problem tend to be
26Published in Transactions on Machine Learning Research (08/2023)
Structure of B
Regression Unstructured CP Tucker LSR
Linearσ2
y/tildewidem
n−σ2
y/parenleftBigg
/summationtext
k∈[K]mkrk−r2
k+/tildewider/parenrightBigg
n−
(Raskutti et al.,
2011)(Zhang et al., 2020)
Logistic/tildewidem
n− − −
(Abramovich &
Grinshtein, 2016)
GLMσ2
y/tildewidem
Dn/summationtext
k∈[K]mkr+r
M/bardblΣx/bardbl2n/summationtext
k∈[K]mkrk+/tildewider
M/bardblΣx/bardbl2nS/summationtext
k∈[K]mkrk+/tildewider
M/bardblΣx/bardbl2n
(Lee & Courtade,
2020)Corollary 2 Corollary 1 Theorem 6
Table 7: Summary of order-wise lower bounds on the minimax risk for linear regression, logistic regression
and GLM settings and for various tensor structures (unstructured, CP, Tucker and LSR). For unstructured
problems,/tildewidem=/producttext
k∈[K]mkis the total number of elements of the tensor. For Tucker and LSR models,
/tildewider=/producttext
k∈[K]rk.
proportional. Thirdly, here we have reported lower bounds on the minimax risk for non-sparse regression,
as we are not studying sparsity in the scope of this work.
In the ﬁrst two rows of the table, we report the order-wise minimax lower bounds for linear and logistic
regression. In all cells of the ﬁrst two rows, we see that the minimax risk decreases proportionally with
increasing sample size. In the unstructured case, the minimax risk is proportional to the number of learn-
able parameters in the model (/producttext
kmk). For linear regression, however, this only applies if the number of
parameters is less than n. This condition puts this result at a disadvantage for the high-dimensional setting
under study. We can see that the minimax risk of linear regression in the Tucker-structured tensor case is
proportional to the number of parameters in the Tucker model (/summationtext
kmkrk+/producttext
krk). The numerators (/producttext
kmk)
and(/summationtext
kmkrk+/producttext
krk)give insights into the parameters on which an achievable minimax risk might depend,
and thus give insights into the sample complexity of the parameter estimation problem. We can see that
imposing a low-rank Tucker structure may signiﬁcantly decrease the number of learnable parameters.
The third row is the row of interest as it summarises results for GLMs. Similar to the previous rows, the
minimax risk decreases with an increase in sample size. We also have a dependence on σ2
yand1
D. Intuitively
speaking, this means that the minimax risk increases with the variance of y. In the unstructured case the
sample complexity is (/producttext
kmk), and does not account for any tensor-structured GLM settings. Theorem 6
and Corollaries 1 and 2 make up the last three columns and address this gap in the literature. Therefore, our
proposed minimax lower bound is a uniﬁed bound, as it encompasses minimax lower bounds for the Tucker
and CP-structured GLM models that were introduced in prior works (Zhou et al., 2013; Li et al., 2018), a
feature that has yet to be examined in existing literature. The minimax risk is proportional to the number
of parameters for the CP, Tucker and LSR case. This is intuitively pleasing as it suggests that by imposing
either the CP, Tucker or LSR structure on B, one can signiﬁcantly reduce the sample complexity from the
unstructured case to/summationtext
kmkr+r,/summationtext
kmkrk+/producttext
krk, andS/summationtext
kmkrk+/producttext
krk, respectively, and that we
can develop algorithms (such as LSRTR in Algorithm 1) that meet these bounds. As previously mentioned,
the CP model induces the lowest number of parameters and the LSR model has (S−1)/summationtext
kmkrkmore
parameters than the Tucker model. Additionally, our results also imply an inverse relationship between the
27Published in Transactions on Machine Learning Research (08/2023)
minimax risk and /bardblΣx/bardbl2andM, which is an intuitively pleasing outcome of our analysis. The variance of
the covariates x(and thus/bardblΣx/bardbl2) symbolises the signal power held by the GLM, in the sense that the signal-
to-noise ratio increases with increasing variance of x. To illustrate perspicuously, we can take the speciﬁc
case of binary logistic regression: In this setting, an increased variance of xinduces a more varied natural
parameter, η, which in turn causes easier distinction between the two response classes. In a similar manner,
a smaller variance of xinduces a more diﬃcult classiﬁcation problem (the classes are harder to distinguish).
In the extreme case, a variance of 0causes all observations to collapse onto a single point, making the
classes indistinguishable. Therefore with increased variance of x(and thus/bardblΣx/bardbl2), the estimation error
should decrease. This argument is also consistent with various other minimax lower bounds in the literature
(Barnes & Özgür, 2019; Shakeri et al., 2016).
Lastly, though we do not provide any theoretical guarantees for the tightness of our bounds in Theorem 6
and Corollaries 1 and 2, we can see that our minimax bound for the Tucker-GLM meets that of the Tucker
linear regression works by Zhang et al. (2020), (we do not consider the r2
kterm in (Zhang et al., 2020) as
that only accounts for the non-singular transformation indeterminacy, which we do not discuss in the scope
of this work). Zhang et al. (2020) have shown that their bounds are indeed optimal, which suggests that
the minimax lower bounds derived in Theorem 6 and Corollaries 1 and 2 are also tight. Moreover, we now
provide a numerical analysis to assess the tightness of our bounds.
6.1 Tightness of Theorem 6
We utilise the results of our experiments on synthetic data in Section 5 to investigate the tightness of the
minimax lower bound in Theorem 6. We have shown that our result matches an existing bound in the
literature for the speciﬁc case of Tucker linear regression. Now, we perform an empirical assessment of
our result. Figure 8 shows the ratio of the mean empirical error (over 50repetitions) of LSRTR from our
experiments in Section 5.1.2 to our obtained lower bound in Theorem 6. We do this for linear and logistic
regression and we can see the ratio is approximately constant as a function of sample size for both regression
types. Such a result suggests that our bound may be achievable in the sense that we can develop algorithms
that meet the minimax lower bound and take advantage of the LSR tensor structure to lower the sample
complexity of GLM problems.
(a)
 (b)
Figure 8: Comparison of the empirical error in LSRTR to the minimax lower bound for linear and logistic
regression. (a)shows the ratio of the empirical error in linear regression to the minimax lower bound for
m= 64,128andr= 4,8.(b)shows the ratio of the empirical error in logistic regression to the minimax
bound form= 64,128andr= 4.
28Published in Transactions on Machine Learning Research (08/2023)
6.2 Roadmap for Proof of Theorem 6
We begin by introducing a few important concepts. First, in order to set up the multiple hypothesis testing
problem consider the constructed packing set
BL={Bl∈P{rk},S:l∈[L]}⊂Bd(0), L∈N, (41)
from which the true index lcorresponding to the true coeﬃcient tensor is generated uniformly at random.
In order to ensure a tight bound on our minimax risk, BLmust possess the following properties: (i)The
minimum packing distance between any two distinct hypotheses Bl,Bl/prime∈BL,l,l/prime∈[L], l/negationslash=l/primeis large.
Speciﬁcally, for a strictly positive parameter δwe require a construction such that /bardblBl−Bl/prime/bardblF≥√
8δ.
(ii)The hypothesis testing problem is hard in the sense that it is diﬃcult to detect the true index l(and
thus the true coeﬃcient tensor) based on an observation y.(iii)The construction of BLmust induce lower
bounds on the minimax risk that reﬂects the reduction in the sample complexity of the LSR tensor structure,
i.e., we require a bound that exhibits a relation between the intrinsic degrees of freedom of the LSR tensor
({mk}k∈[K],{rk}k∈[K],S)and the number of samples (n). Secondly, since the objective of the multiple
hypothesis testing problem is to detect the index l, it is solved through the minimum distance decoder
/hatwidel(y),arg min
l/prime∈[L]/vextenddouble/vextenddouble/vextenddouble/hatwideB−Bl/prime/vextenddouble/vextenddouble/vextenddouble
F, (42)
where/hatwideBis estimated through any learning algorithm, such as Algorithm 1 proposed in this work. In the
LSR-TGLM problem we have the following hypothesis detection criteria:
•If/vextenddouble/vextenddouble/vextenddouble/hatwideB−Bl/vextenddouble/vextenddouble/vextenddouble
F≤√
2δ:P(/hatwidel(y)/negationslash=l) = 0.
•If/vextenddouble/vextenddouble/vextenddouble/hatwideB−Bl/vextenddouble/vextenddouble/vextenddouble
F>√
2δ: A detection error mightoccur, P(/hatwidel(y)/negationslash=l)≥0.
The minimum distance decoder detects the true hypothesis if the estimate /hatwideBlies within the open ball
B(Bl,√
2δ), and a detection error can only occur if/vextenddouble/vextenddouble/vextenddouble/hatwideB−Bl/vextenddouble/vextenddouble/vextenddouble
F>√
2δ. Thus the probability of error is
bounded by
P(/hatwidel(y)/negationslash=l)≤P/parenleftBig/vextenddouble/vextenddouble/vextenddouble/hatwideB−Bl/vextenddouble/vextenddouble/vextenddouble
F≥√
2δ/parenrightBig
. (43)
In this approach we interpret the coeﬃcient estimation problem as a communication problem. From the set
in (41), the source selects the true hypothesis Bl(the true coeﬃcient tensor) uniformly at random. The
hypothesis Blthen generates the channel output according to a ‘channel model’ as in (13) with a chosen link
functiong(·). Speciﬁcally, the channel outputs yisuch that E[yi|Xi] =µi=g−1(/angbracketleftBl,Xi/angbracketright). The minimum
distance decoder then successfully recovers the true hypothesis if the estimator /hatwideBis within a ball of radius√
2δaround Bl. Thirdly, we recognise that the problem under study is supervised and the response variables
yi∀i∈[n]are conditioned on input data Xi∀i∈[n]. On the basis thereof we opt to evaluate the conditional
mutual information I(y;l|X)≥I(y;l).
Weusetheaforementionedconceptstoachieveourresultin (38)throughtheaccomplishmentofthefollowing
tasks: 1)We must construct the set BLas an exponentially large (with respect to the dimensions of an LSR-
structured tensor) family of Ldistinct LSR structured tensors, and satisfying the properties described above.
This involves constructing KS+ 1individual sets (for the KSfactor matrices{B(k,s)}k∈[k], s∈[S]and core
tensor G)andderivingconditionsunderwhichallsetscanexistsimultaneously(withhighprobability). 2)We
must ﬁnd tight bounds on the conditional mutual information I(y;l|X). As explained in Section 3, ‘Oﬀ-the-
shelf’ packing sets and bounds on the minimax risk for the unstructured (vector-based), low-rank matrix,
or low-rank Tucker-structured parameter estimation problems are not useful in our application as these
results do not capture the LSR tensor structure and would yield suboptimal lower bounds. Thus designing
the right geometric insight into LSR tensors – by constructing a packing that respects the factorisation
structure and constraints (e.g., orthogonality) on the factor matrices – is novel, as our goal is to understand
29Published in Transactions on Machine Learning Research (08/2023)
Figure 9: Information theoretic approach for deriving bounds on the minimax risk. Consider a packing set
BL={Bl:l∈[L]}⊂Bd(0)with minimum distance between any two elements as√
8δ. The true coeﬃcient
tensor is selected uniformly at random (u.a.r) and the LSR-TGLM model generates the channel output. We
detect the true coeﬃcient tensor using the minimum distance decoder. Here, the minimum distance decoder
will detect B1as the true hypothesis.
the beneﬁts of imposing the LSR structure on the problem’s sample complexity. The novelty in our work
is that we explicitly leverage this structure leading to a hypothesis set BLwith a structure that cannot
be achieved through current methods. Another challenge in using this technique is actually bounding the
relevant information-theoretic quantities. To that end, we derive a new tight bound on the Kullback-Leibler
divergence in tensor-structured GLMs. Our bound involves explicitly accounting for the link function with
an LSR-structured coeﬃcient tensor. This is a non-trivial derivation, as analysing the link function entails
the careful construction of a packing set and the derivation of tight bounds on the packing distance as in
Lemma 5. The results in Lemma 5 and the bounds on the KL-divergence can be used in other problems
involving LSR tensors and distinguish our work from previous studies. Lastly, though our analysis is local
and depends on a ﬁxed tensor in a neighbourhood with known radius, our minimax risk becomes independent
of this radius if it becomes suﬃciently large.
6.3 Proof of Theorem, 6
Four lemmas lay the foundation to the formal proof of Theorem 6. Through Lemmas 4, 5, 6 and 7 we
achieve the following: We construct the hypothesis set BLcontainingLdistinct tensors. Since each Bl∈BL
is LSR-structured, as deﬁned in (9), we construct BLby constructing KS+1individual sets that correspond
toKSsets for B(k,s)(k∈[K], s∈[S])and a set for G. Our construction of BLresults in tight upper
and lower bounds on the distance /bardblBl−Bl/prime/bardbl2
Ffor any two distinct l,l/prime∈[L], sampled uniformly at random
(u.a.r), which will be used to derive bounds on the mutual information I(y;l)and minimax risk ε∗. Complete
proofs of these lemmas are in the appendix. Lemma 4 (and Corollary 4) introduce the sets of ‘generating’
vectors (and matrices) from which we will later construct G(andB(k,s)for allk∈[k], s∈[S]), that are in
turn used to construct each Bl. Speciﬁcally, for some α∈N, our aim is to construct a set of (α)-dimensional
vectors with entries sampled uniformly at random from/braceleftBig
−1√α,1√α/bracerightBig
, with some designated minimum packing
distance in the Hamming metric between any two vectors. Topologically speaking, this can also be viewed as
the construction of a subset of an (α)-dimensional hypercube with a required minimum Hamming distance
between any two distinct elements in the set. Lemma 4 and its subsequent corollary utilise the Gilbert-
Varshamov bound on constructing binary codes with minimum distance in order to construct the ‘generating’
30Published in Transactions on Machine Learning Research (08/2023)
sets we have just described. The following can be found in the book by Tsybakov (Tsybakov, 2009, Lemma
2.9), where we restate the bound here in the interest of keeping this work self-contained.
Lemma 3 (Gilbert-Varshamov Bound, Lemma 2.9(Tsybakov, 2009)) .Letd≥8. Then there exists a subset
C⊂{ 0,1}dof size
|C|≥ 2d/8(44)
such that for any pair v,v/prime∈C, we have/bardblv−v/prime/bardbl1≥d
8.
Corollary 3. For anyd≥8, there exists a set of binary vectors C⊆{−α,α}dof size|C|≥ 2d/8such that
for any pair v,v/prime∈C, we have/bardblv−v/prime/bardbl0≥d
8.
Lemma 4 and Corollary 4 introduce a set of ‘generating’ binary vectors and matrices, respectively, with
minimum distance.
Lemma 4. Letrk>0∀k∈[K]and/tildewider=/producttext
k∈[K]rk. For any integers (r1,r2,···,rk)such that/tildewider≥8, there
exists a collection of F≥2(/tildewider−1)/8vectors{sf∈1√
/tildewider−1{−1,1}/tildewider−1:f∈[F]}such that for all f,f/prime∈[F]we
have/bardblsf−sf/prime/bardbl0≥(/tildewider−1)/8.
Corollary 4. Letmk>0andrk>0∀k∈[K]. For any integers (mk,rk)such that (mk−1)rk≥8there
exists a collection of P= 2(mk−1)rk/8matrices{Sp∈1√
(mk−1)rk{−1,1}(mk−1)rk:
p∈[P]}such that for all p,p/prime∈[P]we have/bardblSp−Sp/prime/bardbl0≥(mk−1)rk/8.
The sets generated in Lemma 4 and Corollary 4 can be used to construct the set BLofLtensors in (41).
We now introduce Lemma 5, which derives conditions on Lsuch that the sets in Lemma 4 and Corollary 4
exist simultaneously. In Lemma 5 we construct BLwith a certain set of properties. Note that every element
inBLmust have an LSR structure. That is, every element is comprised of KSlow-rank and orthogonal
factor matrices (of dimensions mk×rk,k∈[K]), and a core tensor (of dimensions r1×···×rK), that
we carefully construct from the set of ‘generating’ matrices and vectors deﬁned in Corollary 4 and Lemma
4, respectively, according to (9). Additionally, any two distinct elements Bl,Bl/prime∈BLmust be a suitable
distance apart. Lemma 5 derives an upper and lower bound on this distance; the lower bound determines
the minimum packing distance, while the upper bound is used to derive an upper bound on the conditional
mutual information in Lemma 6. The LSR tensor structure induced by having a common core tensor Gis
critical to such an analysis, speciﬁcally in deriving the lower bound in (46), which is a result of the algebraic
steps shown between (74) and (75), in the proof of Lemma 5 in the appendix. Not only would these steps be
considerably more challenging without the convenience of the LSR matrix structure of (14), but the eﬀect
of not achieving tight bounds on the packing distance may ultimately lead to minimax bounds that do not
reﬂect the intrinsic degrees of freedom of the chosen GLM model.
Lemma 5. Deﬁne/tildewider=/producttext
k∈[K]rk. LetS >0,mk>0,rk>0, and (mk−1)rk≥8∀k∈[K]. There exists
a collection of LtensorsBL,{Bl:l∈[L]}⊂Bd(0)for somed>0of cardinality
L≥21
8/bracketleftBig
S/summationtext
k∈[K](mk−1)rk+(/tildewider−1)/bracketrightBig
(45)
such that for any
1
S/radicalbigg
32(/tildewider−1)
/tildewider<ε≤d
S/radicalbigg
/tildewider−1
/tildewider, (46)
we have
S2/tildewider
/tildewider−1ε2≤/bardblBl−Bl/prime/bardbl2
F≤4S2/tildewider
/tildewider−1ε2. (47)
The bounds in (47) match (up to a constant) and are tight, which help ensure that the upper bound on
the conditional mutual information is also tight. Until now, we have completed the tasks of constructing
31Published in Transactions on Machine Learning Research (08/2023)
the setBLwith the packing distance 8δ=S2
4/tildewider
/tildewider−1ε2and with packing number (cardinality) parameterised
by the parameters {rk}k∈[K],{mk}k∈[K],S. We transition to the next task of deriving upper bounds on the
conditional mutual information, I(y;l|X). Upper bounds on I(y;l|X)involve the evaluation of the Kullback-
Leibler (KL) divergence. For this we employ well-established results in the literature (Cover & Thomas,
2012; Jung et al., 2016). Speciﬁcally, deﬁne fl(y|X)andfl/prime(y|X)as the conditional probability distribution
ofygivenXwith any two distinct coeﬃcient tensors BlandBl/primerespectively. Denote DKLas the relative
entropy, then
I(y;l|X)≤1
L2/summationdisplay
l,l/primeEXDKL(fl(y|X)||fl/prime(y|X)). (48)
We note here that we require the hypothesis test to be hard, which corresponds to a relatively small KL
divergence. Therefore, our aim is to derive a tight upper bound on the KL divergence in (48), by relying on
the tight bounds in (47). Lemma 6 thus derives an upper bound on I(y;l|X).
Lemma 6. Consider the LSR-TGLM problem given by the model in (13)such that B∈Bd(0)for some
d > 0, and consider the set BLconstructed in Lemma 5. Consider ni.i.d observations yifollowing a
probability distribution belonging to the exponential family when conditioned on vec (Xi)∼N(0,Σx). Then
we have:
I(y;l|X)≤4S2Mn/bardblΣx/bardbl2/tildewider
/tildewider−1ε2. (49)
The steps between (77) and (80) in the proof of Lemma 6 in the appendix elucidate how the analysis of
the KL-divergence involves both the link function and the bounds appearing in (47). Speciﬁcally, the KL-
divergence between two distributions fl,fl/primein the exponential family is a function of the link functions
ηl=/angbracketleftBl,X/angbracketrightandηl/prime=/angbracketleftBl/prime,X/angbracketright. Additionally, a tight upper bound on the KL-divergence requires a tight
upper bound on ηl−ηl/prime, or alternatively, /bardblBl−Bl/prime/bardblF. Since any Blis LSR-structured, we require the result
in (47). The ﬁnal step is to lower bound I(y;l|X)using Fano’s inequality (Yu, 1997), stated below:
I(y;l|X)≥I(y;l)≥/parenleftBig
1−P(/hatwidel(y)/negationslash=l)/parenrightBig
log2(L)−1. (50)
Evaluating (50) is simple. For this we refer the reader back to (43) in Section 6.2, which bounds the error
probability P(/hatwidel(y)/negationslash=l)for the recovery of hypothesis Blusing the minimum distance decoder. Lemma 7
bounds this probability, with proof that follows exactly that for Lemma 8by Jung et al. (2016), and thus is
omitted in this work.
Lemma 7. Consider the minimum distance decoder in (42). Consider also the LSR-TGLM regression
problem in (13)with minimax risk ε∗. Suppose ε∗≤δfor some non-negative scalar δ. If theLtensors
distributed in Lemma 5 satisfy /bardblB−Bl/prime/bardbl≥8δthen the detection error of the minimum distance decoder is
upper bounded by P(/hatwidel(y)/negationslash=l)≤1
2.
Proof of Theorem 6. Fix(r1,r2,...,rK). From Lemma 5 for any ε>0satisfying (46) there exists a packing
setBL⊂Bd(0)with cardinality Land packing distance /bardblBl−Bl/prime/bardbl2
Fsatisfying (45) and (47), respectively.
Also, from Lemma 6, with the set BL,I(y;l|X)satisﬁes (49). Now suppose there exists an estimator /hatwideB
which guarantees a risk ε∗=S2r
32(r−1)ε2. If we set 8δ=S2
4/tildewider
/tildewider−1ε2then from Lemma 7 for the set BLwe have
P(/hatwidel(y)/negationslash=l)≤1
2. Combining Lemmas 6 and 7 we have
1
2log2L−1≤I(y;l|X)≤128Mn/bardblΣx/bardbl2ε∗, (51)
which can be rearranged to achieve the result in (6).
32Published in Transactions on Machine Learning Research (08/2023)
7 Conclusion
In this work, we investigate the LSR model on tensor-structured GLM problems. Speciﬁcally, we imposed
a low-rank LSR structure on the coeﬃcient tensor in GLMs. The parameter estimation problem is highly
non-convex, and we propose a block coordinate descent algorithm, called LSRTR, for this purpose. Each
convex sub-problem in LSRTR estimates a separate element/component of the LSR-structured coeﬃcient
tensor. In our theoretical analysis, we provide a minimax lower bound on the estimation error of the
parameter estimation problem of LSR-structured tensors and specialise these bounds for Tucker-structured
and CP-structured tensors. These bounds show that the LSR structure reduces the sample complexity
compared to that of the vector case. We evaluate the tightness of these bounds numerically and through
the comparison of the speciﬁc case of Tucker regression to tight bounds in the literature. The methods
we use may be of interest to readers as they can also be utilised for deriving minimax bounds on other
LSR-structured estimation problems. Furthermore, we evaluate the LSRTR algorithm and the LSR-model
on several synthetic and real datasets. These experiments demonstrate that the LSR model is less restrictive
than other tensor models (such as Tucker or CP), and can be eﬀectively employed for analysis on balanced
and imbalanced medical imaging data. Some possible future work include theoretical analysis of the tightness
of the minimax bounds, and theoretical guarantees of the LSRTR algorithm.
Acknowledgments
This work was supported by the US National Science Foundation under awards CCF-1910110, CCF-1907658
andOAC-1940074, theUSNationalInstitutesofHealthunderaward2R01DA040487, andtheArmyResearch
Oﬃce under award W911NF-21-1-0301.
References
Felix Abramovich and Vadim Grinshtein. Model selection and minimax estimation in generalized linear mod-
els.IEEE Transactions on Information Theory , 62(6):3721–3730, 6 2016. doi:10.1109/TIT.2016.2555812.
Felix Abramovich and Vadim Grinshtein. High-dimensional classiﬁcation by sparse logistic regression. IEEE
Transactions on Information Theory , 65(5):3068–3079, 5 2018. doi:10.1109/TIT.2018.2884963.
P.-A. Absil, R. Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds . Princeton
University Press, 2008.
ADHD-200 Consortium. The ADHD-200 Consortium: A model to advance the translational po-
tential of neuroimaging in clinical neuroscience. Frontiers in Systems Neuroscience , 6:62, 2012.
doi:10.3389/fnsys.2012.00062.
Talal Ahmed, Haroon Raja, and Waheed U. Bajwa. Tensor regression using low-rank and sparse
Tucker decompositions. SIAM Journal on Mathematics of Data Science , 2(4):944–966, 2020.
doi:10.1137/19M1299335.
Baiguo An and Beibei Zhang. Logistic regression with image covariates via the combination of /lscript1and Sobolev
regularizations. PLOS One , 15(6):e0234975, 2020. doi:10.1371/journal.pone.0234975.
Leighton Pate Barnes and Ayfer Özgür. Minimax bounds for distributed logistic regression. Technical Report
arXiv:1910.01625 [cs.IT], ArXiV, 10 2019. URL https://arxiv.org/abs/1910.01625 .
Pierre Bellec, Carlton Chu, Francois Chouinard-Decorte, Yassine Benhajali, Daniel S. Margulies, and
R. Cameron Craddock. The Neuro Bureau ADHD-200 preprocessed repository. Neuroimage , 144(Part
B):275–286, 1 2017. doi:10.1016/j.neuroimage.2016.06.034.
Quentin Berthet and Nicolai Baldin. Statistical and computational rates in graph logistic regression. In
Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference
on Artiﬁcial Intelligence and Statistics , volume 108 of Proceedings of Machine Learning Research , pp.
2719–2730. PMLR, 8 2020. URL https://proceedings.mlr.press/v108/berthet20a.html .
33Published in Transactions on Machine Learning Research (08/2023)
Dimitri P. Bertsekas. Convex Optimization Theory , volume 1. Athena Scientiﬁc, 2009.
Dimitri P. Bertsekas. Nonlinear Programming: 3rd Edition . Athena Scientiﬁc, Nashua NH, USA, 2016. URL
http://www.athenasc.com/nonlinbook.html .
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory . John Wiley & Sons, 2012.
Cameron Craddock, Yassine Benhajali, Carlton Chu, Francois Chouinard, Alan Evans, András Jakab, Bud-
hachandra S. Khundrakpam, John D. Lewis, Qingyang Li, Yan Milham, Michael, Chaogan, and Pierre
Bellec. The Neuro Bureau preprocessing initiative: Open sharing of preprocessed neuroimaging data and
derivatives. In Frontiers in Neuroinformatics: Conference Abstract: Neuroinformatics 2013 , volume 7, pp.
27, Stockholm, Sweden, 8 2013. doi:10.3389/conf.fninf.2013.09.00041.
Lieven De Lathauwer. Decompositions of a higher-order tensor in block terms—Part I: Lemmas for
partitioned matrices. SIAM Journal on Matrix Analysis and Applications , 30(3):1022–1032, 2008.
doi:10.1137/060661685.
Stephan Dreiseitl and Lucila Ohno-Machado. Logistic regression and artiﬁcial neural network classi-
ﬁcation models: A methodology review. Journal of Biomedical Informatics , 35(5–6):352–359, 2002.
doi:10.1016/s1532-0464(03)00034-0.
John P. Dumas, Muhammad A. Lodhi, Batoul A. Taki, Waheed U. Bajwa, and Mark C. Pierce. Computa-
tional endoscopy—a framework for improving spatial resolution in ﬁber bundle imaging. Optics Letters ,
44(16):3968–3971, 2019. doi:10.1364/OL.44.003968.
Dylan J. Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan. Logistic regression:
The importance of being improper. In Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet (eds.),
Proceedings of the 31st Conference On Learning Theory , volume 75 of Proceedings of Machine Learning
Research , pp. 167–208. PMLR, 7 2018. URL https://proceedings.mlr.press/v75/foster18a.html .
Xiao Fu, Nico Vervliet, Lieven De Lathauwer, Kejun Huang, and Nicolas Gillis. Computing large-scale
matrix and tensor decomposition with structured factors: A uniﬁed nonconvex optimization perspective.
IEEE Signal Processing Magazine , 37(5):78–94, 9 2020. doi:10.1109/MSP.2020.3003544.
Mohsen Ghassemi, Zahra Shakeri, Anand D. Sarwate, and Waheed U. Bajwa. Learning mixtures of separable
dictionaries for tensor data: Analysis and algorithms. IEEE Transactions on Signal Processing , 68:33–48,
2020. doi:10.1109/TSP.2019.2952046.
Christophe Giraud. Introduction to High-dimensional Statistics . Number 139 in Monographs on Statistics
and Applied Probability. Chapman and Hall/CRC, New York, NY, USA, 2021.
Johan Håstad. Tensor rank is NP-complete. Journal of Algorithms , 11(4):644–654, 1990. doi:10.1016/0196-
6774(90)90014-6.
Marti A. Hearst, Susan T. Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector
machines. IEEE Intelligent Systems and their Applications , 13(4):18–28, 7 1998. doi:10.1109/5254.708428.
Hung Hung and Chen-Chien Wang. Matrix variate logistic regression model with application to EEG data.
Biostatistics , 14(1):189–202, 1 2013. doi:10.1093/biostatistics/kxs023.
Richard W. Issitt, Mario Cortina-Borja, William Bryant, Stuart Bowyer, Andrew M. Taylor, Neil Sebire,
and Stuart A. Bowyer. Classiﬁcation performance of neural networks versus logistic regression models:
Evidence from healthcare practice. Cureus, 14(2):e22443, 2022. doi:10.7759/cureus.22443.
Alexander Jung, Yonina C. Eldar, and Norbert Görtz. On the minimax risk of dictionary learning. IEEE
Transactions on Information Theory , 62(3):1501–1515, 3 2016. doi:10.1109/TIT.2016.2517006.
Rafail Z Khas’minskii. A lower bound on the risks of non-parametric estimates of densities in the uniform
metric.Theory of Probability & Its Applications , 23(4):794–798, 1979. doi:10.1137/1123095.
34Published in Transactions on Machine Learning Research (08/2023)
Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM Review , 51(3):
455–500, 2009. doi:10.1137/07070111X.
Kuan-Yun Lee and Thomas Courtade. Minimax bounds for generalized linear models. In H. Larochelle,
M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing
Systems, volume 33, pp. 9372–9382. Curran Associates, Inc., 2020.
Xiaoshan Li, Da Xu, Hua Zhou, and Lexin Li. Tucker tensor regression and neuroimaging analysis. Statistics
in Biosciences , 10(3):520–545, 3 2018. doi:10.1007/s12561-018-9215-6.
Muhammad Asad Lodhi and Waheed U. Bajwa. Learning product graphs underlying smooth graph sig-
nals. Technical Report arXiv:2002.11277 [eess.SP], ArXiV, 10 2020. URL https://arxiv.org/abs/
2002.11277 .
Peter McCullagh and John A. Nelder. Generalized Linear Models . Routledge, 2019.
Frank Nielsen. Statistical divergences between densities of truncated exponential families with nested sup-
ports: Duo Bregman and duo Jensen divergences. Entropy, 24(3):421, 2022. doi:10.3390/e24030421.
K. B. Petersen and M. S. Pedersen. The Matrix Cookbook, 11 2012. URL http://www2.compute.dtu.dk/
pubdb/pubs/3274-full.html . Version 20121115.
Garvesh Raskutti, Martin J. Wainwright, and Bin Yu. Minimax rates of estimation for high-dimensional
linear regression over /lscriptq-balls.IEEE Transactions on Information Theory , 57(10):6976–6994, 10 2011.
doi:10.1109/TIT.2011.2165799.
Garvesh Raskutti, Ming Yuan, and Han Chen. Convex regularization for high-dimensional multiresponse
tensor regression. The Annals of Statstics , 47(3):1554–1584, 2019. doi:10.1214/18-AOS1725.
AthanasiosA.Rontogiannis, EleftheriosKoﬁdis, andParisV.Giampouras. Block-termtensordecomposition:
Model selection and computation. IEEE Journal of Selected Topics in Signal Processing , 15(3):464–475,
4 2021. doi:10.1109/JSTSP.2021.3051488.
George A. F. Seber and Alan J. Lee. Linear Regression Analysis , volume 330. John Wiley & Sons, 2003.
Zahra Shakeri, Waheed U. Bajwa, and Anand D Sarwate. Minimax lower bounds for Kronecker-structured
dictionary learning. In Proceedings of the 2016 IEEE Int. Symposium on Information Theory (ISIT) , pp.
1148–1152, Barcelona, Spain, 7 2016. IEEE. doi:10.1109/ISIT.2016.7541479.
Jianing V Shi, Yangyang Xu, and Richard G. Baraniuk. Sparse bilinear logistic regression. Technical Report
arXiv:1404.4104 [math.OC], ArXiV, 4 2014. URL https://arxiv.org/abs/1404.4104 .
Tingni Sun and Cun-Hui Zhang. Scaled sparse linear regression. Biometrika , 99(4):879–898, 12 2012.
doi:10.1093/biomet/ass043.
Batoul Taki, Mohsen Ghassemi, Anand D. Sarwate, and Waheed U. Bajwa. A minimax lower bound
for low-rank matrix-variate logistic regression. In Proceedings of the 2021 55th Asilomar Confer-
ence on Signals, Systems, and Computers , pp. 477–484, Paciﬁc Grove, CA, USA, 10 2021. IEEE.
doi:10.1109/IEEECONF53345.2021.9723149.
Xu Tan, Yin Zhang, Siliang Tang, Jian Shao, Fei Wu, and Yueting Zhuang. Logistic tensor regression for
classiﬁcation. In Jian Yang, Fang Fang, and Changyin Sun (eds.), Intelligent Science and Intelligent Data
Engineering , pp. 573–581, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg. ISBN 978-3-642-36669-7.
doi:10.1007/978-3-642-36669-7_70.
Theodoros Tsiligkaridis and Alfred O. Hero. Covariance estimation in high dimensions via Kro-
necker product expansions. IEEE Transactions on Signal Processing , 61(21):5347–5360, 11 2013.
doi:10.1109/TSP.2013.2279355.
Alexandre B. Tsybakov. Introduction to Nonparametric Estimation . Springer New York, NY, 2009.
35Published in Transactions on Machine Learning Research (08/2023)
Martin J. Wainwright. Information-theoretic limits on sparsity recovery in the high-dimensional
and noisy setting. IEEE Transactions on Information Theory , 55(12):5728–5741, 12 2009.
doi:10.1109/TIT.2009.2032816.
Ying Wu, Dan Chen, Chaoqian Li, and Niansheng Tang. Bayesian tensor logistic regression with applications
to neuroimaging data analysis of Alzheimer’s disease. Statistical Methods in Medical Research , 31(12):
2368–2382, 9 2022. doi:10.1177/09622802221122409.
Jiancheng Yang, Xiaoyang Huang, Yi He, Jingwei Xu, Canqian Yang, Guozheng Xu, and Bingbing Ni.
Reinventing 2D convolutions for 3D images. IEEE Journal of Biomedical and Health Informatics , 25(8):
3009–3018, 8 2021a. doi:10.1109/JBHI.2021.3049452.
Jiancheng Yang, Rui Shi, and Bingbing Ni. MedMNIST classiﬁcation decathlon: A lightweight
AutoML benchmark for medical image analysis. In Proceedings of the 2021 IEEE 18th Inter-
national Symposium on Biomedical Imaging (ISBI) , pp. 191–195, Nice, France, 4 2021b. IEEE.
doi:10.1109/ISBI48211.2021.9434062.
Xi Yang, Ding Xia, Taichi Kin, and Takeo Igarashi. IntrA: 3D intracranial aneurysm dataset for deep
learning. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2656–2666, Seattle, WA, USA, 6 2020. doi:10.1109/CVPR42600.2020.00273.
Bin Yu. Assouad, Fano, and Le Cam. In Festschrift for Lucien Le Cam , pp. 423–435. Springer, 1997.
Anru R Zhang, Yuetian Luo, Garvesh Raskutti, and Ming Yuan. ISLET: Fast and optimal low-rank tensor
regression via importance sketching. SIAM Journal on Mathematics of Data Science , 2(2):444–479, 2020.
doi:10.1137/19M126476X.
Jianguang Zhang and Jianmin Jiang. Decomposition-based tensor learning regression for improved classi-
ﬁcation of multimedia. Journal of Visual Communication and Image Representation , 41:260–271, 2016.
doi:10.1016/j.jvcir.2016.10.006.
Jianguang Zhang and Jianmin Jiang. Rank-optimized logistic matrix regression toward improved matrix
data classiﬁcation. Neural Computation , 30(2):505–525, 2018. doi:10.1162/neco_a_01038.
Hua Zhou, Lexin Li, and Hongtu Zhu. Tensor regression with applications in neuroimag-
ing data analysis. Journal of the American Statistical Association , 108(502):540–552, 2013.
doi:10.1080/01621459.2013.776499.
A Supporting Results
A.1 Proofs
Proof of Corollary 3. ByLemma3thereexistsapackingwithminimumdistanced
8inthe/lscript1normcontaining
2d/8binary vectors. Mapping 0→−αand1→αfor all vectors in this packing shows that there exists a
packing of at least 2d/8vectors in{−α,α}dwith minimum distancedα
4in the/lscript1norm. For any pair v,v/primein
that packing we have /bardblv−v/prime/bardbl0≥d
8, since every entry of v−v/primeis in{−2α,0,2α}.
Proof of Lemma 4. This is a direct consequence of Corollary 3.
The proof of Lemma 5 contains the derivation of a tight upper bound on /bardblBl−Bl/prime/bardbl2
F. The following lemma
is a component needed to achieve such an upper bound; speciﬁcally, we will see that it proves useful in
deriving an upper bound on /bardblBl/bardbl2
F, for anyl∈[L].
Lemma 8. Letx∈Rmbe anym-dimensional real vector and O∈Om×mbe any orthogonal basis of
Rm. Deﬁneui=|cosθi|, whereθiis the angle between any possible xand basis vector oi. The function
f=m/summationtext
i=1(ui+ 1)2is minimized when ui=−1√m, or when xis equiangular to all basis vectors oi,∀i∈[m].
36Published in Transactions on Machine Learning Research (08/2023)
Proof of Lemma 8. Consider the function f=m/summationtext
i=1(ui+ 1)2. Additionally, for a basis O∈Om×mand
x∈Rm,/summationtextm
i=1(cosθi)2= 1, thus we have the equality constraint g=m/summationtext
i=1u2
i−1 = 0. Denoteλas the
Lagrange multiplier, and thus the Lagrange function is deﬁned as
Lg=m/summationdisplay
i=1(ui+ 1)2−λ(m/summationdisplay
i=1u2
i−1). (52)
The partial derivative of Lgwith respect to ui,∀i∈[m]is
∂Lg
∂ui= 2(ui+ 1)−2λui,∀i∈[m]. (53)
The partial derivative of Lgwith respect to λis
∂Lg
∂λ=−m/summationdisplay
i=1u2
i+ 1. (54)
By setting (53) and (54) to zero we get
ui=−1
1−λ∀i∈[m], (55)
and
−m/summationdisplay
i=1u2
i+ 1 = 0, (56)
respectively. What we have in (55) and (56) is a system of m+ 1equations and m+ 1unknowns. We solve
the system to ﬁnd the critical points of f(·), which are ui=−1√mandui=1√m. Sinceui≥0, the solution is
ui=1√m,∀i∈[m]minimizes the function f(·).
Proof of Lemma 5. Fix the following arbitrary real orthonormal bases: QofR/tildewider, andKsets ofrkbases,/braceleftbig
Uk,j/bracerightbigrk
j=1ofRmk,∀k∈[K].
Next, consider the following hypercubes or subsets thereof: 1) The set of Fvectors{sf}from Lemma 4:
sf∈/braceleftbigg−1√
/tildewider−1,+1√
/tildewider−1/bracerightbigg/tildewider−1
, (57)
wheref∈[F], and 2)KSsets ofP(k,s)matrices∀k∈[K],∀s∈[S], from Lemma 4:
Sp(k,s)∈/braceleftbigg−1/radicalbig
(mk−1)rk,+1/radicalbig
(mk−1)rk/bracerightbigg(mk−1)×rk
∀k∈[K], s∈[S], (58)
wherep(k,s)∈[P(k,s)].
We proceed with the following steps in order to construct the ﬁnal set BLof coeﬃcient tensors from the sets
in (57) and (58). Since BL⊂Bd(0), we know that the energy of any Blis upper bounded by d2. We will
construct Gf, and matrices with orthonormal columns, namely Bp(k,s)∀k∈[K], s∈[S], all of which will
be used to construct every Bl∈BL. Speciﬁcally, due to our LSR model, any tensor Blwill have a rank
(r1,r2,...,rK)LSR structure.
We use the notation (f,i)to denote the ithstep in constructing the fthelement of Gf. Hence in the ﬁrst
step, we construct vectors g(f,1)∈R/tildewiderforf∈[F], using Qandsf, as follows:
g(f,1)=Q/bracketleftBigg/radicalBig1√
/tildewider−1
sf/bracketrightBigg
,∀f∈[F]. (59)
37Published in Transactions on Machine Learning Research (08/2023)
From (59), since/bardblsf/bardbl2
2= 1we have:
/vextenddouble/vextenddoubleg(f,1)/vextenddouble/vextenddouble2
2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleQ/bracketleftBigg/radicalBig1√
/tildewider−1
sf/bracketrightBigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2=/tildewider
/tildewider−1.
Now, we deﬁne each vector gfas
gf=ε√
/tildewiderg(f,1),∀f∈[F], (60)
for some positive number ε.
Similarly, we use the notation (p(k,s),i)to denote the ithstep in constructing the pth
(k,s)element of Bp(k,s).
Hence, we construct matrices B(p(k,s),1)∈Rmk×rk, forp(k,s)∈[P(k,s)],∀k∈[K],s∈[S]. Deﬁne B(j)
(p(k,s),1)as
thejthcolumn of B(p(k,s),1),∀k∈[K],s∈[S], and S(j)
p(k,s)as thejthcolumn of Sp(k,s). Let the columns be
constructed as follows:
B(j)
(p(k,s),1)=Uk,j/bracketleftbigg1
S(j)
p(k,s)/bracketrightbigg
,∀p(k,s)∈[P(k,s)],k∈[K],s∈[S]. (61)
From (61) we have:
/vextenddouble/vextenddouble/vextenddoubleB(p(k,s),1))(j)/vextenddouble/vextenddouble/vextenddouble2
2= =rk+ 1
rk∀k∈[K].
We now construct matrices Bp(k,s)∈Rmk×rk, forp(k,s)∈[P(k,s)],k∈[K],s∈[S]. The construction of each
Bp(k,s)follows the same procedure for all k∈[K]ands∈[S]. Deﬁne B(j)
p(k,s)∈Rmkas thejthcolumn of
Bp(k,s), forj∈[rk]. We set
B(1)
p(k,s)=B(1)
(p(k,s),1)/vextenddouble/vextenddouble/vextenddoubleB(1)
(p(k,s),1)/vextenddouble/vextenddouble/vextenddouble
2, (62)
and deﬁne
a(j+1),B(j+1)
(p(k,s),1)−j/summationdisplay
j/prime=1/angbracketleftB(j+1)
(p(k,s),1),B(j/prime)
p(k,s)/angbracketrightB(j/prime)
p(k,s), (63)
and
B(j+1)
p(k,s),aj+1
/bardblaj+1/bardbl2. (64)
The steps in (62), (63) and (64) constitute the well-known Gram-Schmidt process. Thus, the set of vectors
B(j)
(p(k,s),1), forj∈[rk],p(k,s)∈[P(k,s)],k∈[K]ands∈[S]are orthonormal, i.e.,/vextenddouble/vextenddouble/vextenddoubleB(j)
p(k,s)/vextenddouble/vextenddouble/vextenddouble2
2= 1and
B(j)
p(k,s)⊥B(j/prime)
p(k,s), for any two distinct j,j/prime∈[rk]. Consequently,/parenleftbig
Bp(k,s)/parenrightbigT/parenleftbig
Bp(k,s)/parenrightbig
=Irk. Now by deﬁning
the set
L,/braceleftBig
(f,/parenleftbig
p(k,s)/parenrightbig
k∈[K], s∈[S]) :f∈[F],p(k,s)∈[P(k,s)],k∈[K],s∈[S]/bracerightBig
(65)
as the set of all tuples, (f,p(1,1),...,p (1,S),...,p (K,1),...,p (K,S)), we have
L=|L|(a)
≥2(1/8)/bracketleftbig
(/tildewider−1)+S/summationtextK
k=1(mk−1)rk/bracketrightbig
, (66)
38Published in Transactions on Machine Learning Research (08/2023)
where (a)follows from Lemma 4 and Corollary 4. We deﬁne the set of coeﬃcient tensors, BLas,
BL,/braceleftbigg
Bl=S/summationdisplay
s=1Gf×1Bp(1,s)×2Bp(2,s)···×KBp(K,s):l∈[L],f∈[F],p(k,s)∈[P(k,s)],k∈[K],s∈[S]/bracerightbigg
,
(67)
and we restrict εsuch that
1
S/radicalbigg
32(/tildewider−1)
/tildewider<ε<d
S/radicalbigg
/tildewider−1
/tildewider. (68)
We make the ﬁnal note that, due to the Kronecker product, we can express vec(Bl)as:
vec(Bl) =S/summationdisplay
s=1(Bp(K,s)⊗Bp(K−1,s)⊗···⊗ Bp(1,s))gf. (69)
We have the following remaining tasks at hand: 1) We must show that the energy of any Blis less than
d2. 2) We must derive an upper and lower bound on the distance/parenleftBig
/bardblBl−Bl/prime/bardbl2
F/parenrightBig
between any two distinct
tensors Bl,Bl/prime,∈BL. We begin by showing /bardblBl/bardbl2
F<d2:
/bardblBl/bardbl2
F=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleS/summationdisplay
s=1Gf×1Bp(1,s)×2Bp(2,s)×3···×KBp(K,s)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
F
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleS/summationdisplay
s=1/parenleftbig
Bp(K,s)⊗Bp(K−1,s)⊗···⊗ Bp(1,s)/parenrightbig
gf/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2
(b)
≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleS/summationdisplay
s=1Bp(K,s)⊗···⊗ Bp(1,s)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
F/bardblgf/bardbl2
2(70)
(c)
≤/parenleftBiggS/summationdisplay
s=1/vextenddouble/vextenddoubleBp(K,s)⊗···⊗ Bp(1,s)/vextenddouble/vextenddouble
F/parenrightBigg2
/bardblgf/bardbl2
2(71)
(d)=S2/productdisplay
k/bardblBk,pk/bardbl2
F/bardblgf/bardbl2
2=S2/tildewiderε2
/tildewider−1(e)
< d2, (72)
where (b)follows from the fact that for any matrix Aand any vector a,/bardblAa/bardbl2≤/bardblA/bardbl2/bardbla/bardbl2and the fact that
/bardblA/bardbl2≤/bardblA/bardblF(Petersen & Pedersen, 2012). Additionally, (c)follows the triangle inequality and (d)from
the fact that the matrix norm of the Kronecker product is the product of the matrix norms. Additionally,
(e)holds due to (68).
We proceed with deriving lower and upper bounds on /bardblBl−Bl/prime/bardbl2
Ffor any two distinct Bl,Bl/prime∈BL. We
ﬁrst denote the square matrix /tildewideBp(k,s)∈Rmk×mkas the completed orthonormal matrix of each low-rank
matrix Bp(k,s)∈Rmk×rk. Also,/tildewideGf∈Rm1×···×mKhas entries/tildewideGf(·)deﬁned as follows:
/braceleftBigg/tildewideGf(1:r1,..., 1:rK) =Gf(1:r1,..., 1:rK)
/tildewideGf(r1+ 1:m1,...,rK+ 1:mK) =Gf(1:r1,..., 1:rK).(73)
Also deﬁne/tildewideBl=/summationtextS
s=1/tildewideGf×1/tildewideBp(1,s)×2···×K/tildewideBp(K,s)for anyl∈[L]. With these deﬁnitions, we have the
equality/bardblBl−Bl/prime/bardbl2
F=/vextenddouble/vextenddouble/vextenddouble/tildewideBl−/tildewideBl/prime/vextenddouble/vextenddouble/vextenddouble2
F. Deﬁningk=1/circlemultiplytext
k=K/tildewideBp(k,s),/tildewideBp(K,s)⊗···⊗/tildewideBp(1,s)for anyl∈[L]and we
39Published in Transactions on Machine Learning Research (08/2023)
have the following:
/bardblBl−Bl/prime/bardbl2
F=ε2
/tildewider/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleS/summationdisplay
s=1/parenleftBiggk=1/circlemultiplydisplay
k=K/tildewideBp(k,s)/parenrightBigg
/tildewideg(f,1)−S/summationdisplay
s=1/parenleftBiggk=1/circlemultiplydisplay
k=K/tildewideBp/prime
(k,s)/parenrightBigg
/tildewideg(f/prime,1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2
=ε2
/tildewider
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleS/summationdisplay
s=1/parenleftBiggk=1/circlemultiplydisplay
k=K/tildewideBp(k,s)/parenrightBigg
/tildewideg(f,1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleS/summationdisplay
s=1/parenleftBiggk=1/circlemultiplydisplay
k=K/tildewideBp/prime
(k,s)/parenrightBigg
/tildewideg(f/prime,1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2
−2/angbracketleftBiggS/summationdisplay
s=1/parenleftBiggk=1/circlemultiplydisplay
k=K/tildewideBp(k,s)/parenrightBigg
/tildewideg(f,1),S/summationdisplay
s=1/parenleftBiggk=1/circlemultiplydisplay
k=K/tildewideBp/prime
(k,s)/parenrightBigg
/tildewideg(f/prime,1)/angbracketrightBigg/parenrightBigg
.
Deﬁne Ts=k=1/circlemultiplytext
k=K/tildewideBp(k,s),Vs=k=1/circlemultiplytext
k=K/tildewideBp/prime
(k,s)for anys∈[S], and T(j)
s,V(j)
sas thejthcolumn of TsandVs,
respectively, then we have
/bardblBl−Bl/prime/bardbl2
F=ε2
/tildewider
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleS/summationdisplay
s=1Ts/tildewideg(f,1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleS/summationdisplay
s=1Vs/tildewideg(f/prime,1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2−2/angbracketleftBiggS/summationdisplay
s=1Ts/tildewideg(f,1),S/summationdisplay
s=1Vsg(f/prime,1)/angbracketrightBigg

=ε2
/tildewider
/parenleftBiggS/summationdisplay
s=1TT(1)
s/tildewideg(f,1)/parenrightBigg2
+···+/parenleftBiggS/summationdisplay
s=1TT(m)
s/tildewideg(f,1)/parenrightBigg2
+/parenleftBiggS/summationdisplay
s=1VT(1)
s/tildewideg(f/prime,1)/parenrightBigg2
+···+/parenleftBiggS/summationdisplay
s=1VT(m)
s/tildewideg(f/prime,1)/parenrightBigg2
−2/parenleftBigg/parenleftBiggS/summationdisplay
s=1TT(1)
1/tildewideg(f,1)/parenrightBigg/parenleftBiggS/summationdisplay
s=1VT(1)
s/tildewideg(f/prime,1)/parenrightBigg
+···+/parenleftBiggS/summationdisplay
s=1TT(m)
s/tildewideg(f,1)/parenrightBigg/parenleftBiggS/summationdisplay
s=1VT(m)
s/tildewideg(f/prime,1)/parenrightBigg/parenrightBigg/parenrightBigg
We group every/parenleftBig/summationtextS
s=1TT(j)
s/tildewideg(f,1)/parenrightBig2
+/parenleftBig/summationtextS
s=1VT(j)
s/tildewideg(f/prime,1)/parenrightBig2
−2/parenleftBig/summationtextS
s=1TT(j)
1/tildewideg(f,1)/parenrightBig/parenleftBig/summationtextS
s=1VT(j)
s/tildewideg(f/prime,1)/parenrightBig
,
forj∈[m]. We get
/bardblBl−Bl/prime/bardbl2
F≥ε2
/tildewiderm/summationdisplay
i=1/parenleftBigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS/summationdisplay
s=1TT(i)
s/tildewideg(f,1)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS/summationdisplay
s=1VT(i)
s/tildewideg(f/prime,1)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightBigg2
. (74)
The expression in (74) contains inner products. Speciﬁcally,/vextendsingle/vextendsingle/vextendsingle/summationtextS
s=1TT(i)
s/tildewideg(f,1)/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/angbracketleftBig/summationtextS
s=1TT(i)
s,/tildewideg(f,1)/angbracketrightBig/vextendsingle/vextendsingle/vextendsingle.
Denoteλi=/vextendsingle/vextendsingle/vextendsinglecos\/parenleftBig/summationtextS
s=1TT(i)
s,/tildewideg(f,1)/parenrightBig/vextendsingle/vextendsingle/vextendsingle, then we have
/bardblBl−Bl/prime/bardbl2
F(f)
≥ε2
/tildewiderm/summationdisplay
i=1/parenleftBigg
λi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleS/summationdisplay
s=1TT(i)
s/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/tildewideg(f,1)/vextenddouble/vextenddouble
2−/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleS/summationdisplay
s=1VT(i)
s/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2/vextenddouble/vextenddouble/tildewideg(f/prime,1)/vextenddouble/vextenddouble
2/parenrightBigg2
(g)=/tildewiderε2
/tildewider(/tildewider−1)m/summationdisplay
i=1/parenleftBigg
λi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleS/summationdisplay
s=1TT(i)
s/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble−/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleS/summationdisplay
s=1VT(i)
s/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenrightBigg2
(h)
≥/tildewiderε2
/tildewider(/tildewider−1)m/summationdisplay
i=1S2(λi+ 1)2
(i)
≥/tildewiderε2
/tildewider(/tildewider−1)m/summationdisplay
i=1S2(1 +1√m)2≥S2/tildewider
/tildewider−1ε2, (75)
where (f)is due to applying Cauchy-Schwartz inequality to/vextendsingle/vextendsingle/vextendsingle/summationtextS
s=1VT(i)
s/tildewideg(f/prime,1)/vextendsingle/vextendsingle/vextendsingle,(g)is due to (60), (h)is due
to the fact that/vextenddouble/vextenddouble/vextenddouble/summationtextS
s=1TT(i)
s/vextenddouble/vextenddouble/vextenddoubleand/vextenddouble/vextenddouble/vextenddouble/summationtextS
s=1TV(i)
s/vextenddouble/vextenddouble/vextenddoubleare lower and upper bounded by −SandS, respectively,
40Published in Transactions on Machine Learning Research (08/2023)
and(i)is from the result in Lemma 8. Finally, for ﬁnding upper bounds on /bardblBl−Bl/prime/bardbl2
F, we have:
/bardblBl−Bl/prime/bardbl2
F
(j)
≤/parenleftBigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleS/summationdisplay
s=1/parenleftbig
Bp(K,s)⊗···⊗ Bp(1,s)/parenrightbig
gf/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleS/summationdisplay
s=1/parenleftBig
Bp/prime
(K,s)⊗···⊗ Bp/prime
(1,s)/parenrightBig
gf/prime/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/parenrightBigg2
≤/parenleftBiggS/summationdisplay
s=1/productdisplay
k/vextenddouble/vextenddoubleBp(k,s)/vextenddouble/vextenddouble
F/bardblgf/bardbl2+S/summationdisplay
s=1/productdisplay
k/vextenddouble/vextenddouble/vextenddoubleBp/prime
(k,s)/vextenddouble/vextenddouble/vextenddouble
F/bardblgf/prime/bardbl2/parenrightBigg2
(k)=/parenleftBigg
2S/summationdisplay
s=1/productdisplay
k/vextenddouble/vextenddoubleBp(k,s)/vextenddouble/vextenddouble
F/bardblgf/bardbl2/parenrightBigg2
=4S2/tildewider
/tildewider−1ε2, (76)
where (j)follows from the triangle inequality, and (k)follows from the fact that/vextenddouble/vextenddoubleBp(k,s)/vextenddouble/vextenddouble
F=/vextenddouble/vextenddouble/vextenddoubleBp/prime
(k,s)/vextenddouble/vextenddouble/vextenddouble
F
and that/bardblgf/bardbl2=/bardblgf/prime/bardbl2.
Proof of Lemma 6. Consider the set BLfrom Lemma 5, where the bounds in (75) and (76) hold. For the
LSR-TGLM model in (13), consider ni.i.d samples, with covariate tensors Xi∈Rm1×···×mK,∀i∈[n],
where vec(Xi)∼N (0,Σx). According to (13), observations yifollow an exponential family distribution
when conditioned on Xi,∀i∈[n]. Consider the vector of nobservations, y, and the tensor of nsamples,
X. Deﬁne also I(y;l|X)as the mutual information between observations yand index lconditioned on
side-informationX. From (Cover & Thomas, 2012; Wainwright, 2009), we have,
I(y;l|X)≤1
L2/summationdisplay
l,l/primeEXDKL(fl(y|X)||fl/prime(y|X), (77)
whereDKL(fl(y|X)||fl/prime(y|X)is the Kullback-Leibler (KL) divergence of probability distribution fl(y|X)
andfl/prime(y|X)ofygivenXfor some Bl,Bl/prime∈BL. Denoteηliandηl/primeias the link functions associated
withfl(yi|Xi)andfl/prime(yi|Xi), respectively. Also denote µlias the expectation of suﬃcient statistic T(yi)
conditioned on Xiunder model Bl(otherwise known as the canonical parameter). We evaluate the KL
divergence which is given as follows (Nielsen, 2022):
DKL(fl(y|X)||fl/prime(y|X)) =/summationdisplay
i∈[n](ηli−ηl/primei)µl−a(ηli) +a(ηl/primei). (78)
Now, we take the expectation of (78) with respect to the side-information X. We have
EX[(ηli−ηl/primei)µl−a(ηli) +a(ηl/primei)] =EX[(ηli−ηl/primei)µl], due to the fact that EX[a(ηli)] =EX[a(ηl/primei)]. We
now have:
EXDKL(fl(y|X)||fl/prime(y|X) =/summationdisplay
i∈[n]EX/bracketleftbig
(/angbracketleftBl,Xi/angbracketright−/angbracketleftBl/prime,Xi/angbracketright)E[T(yi)|Xi,l]]
(l)=/summationdisplay
i∈[n]EX/bracketleftbigg
(/angbracketleftBl−Bl/prime,Xi/angbracketright)∂a(ηli)
∂ηli/bracketrightbigg
≤/summationdisplay
i∈[n]/radicalBigg
EX/bracketleftbig
/angbracketleftBl−Bl/prime,Xi/angbracketright/bracketrightbig
EX/bracketleftbigg∂a(ηli)
∂ηli/bracketrightbigg2
(79)
≤n/bardblΣx/bardbl2/bardblBl−Bl/prime/bardbl2
FM, (80)
where (l)follows from the fact that µli=E[T(yi)|Xi,l] =∂a(ηli)
∂ηli. We achieve (79) through Cauchy-Schwartz
inequality. We make some remarks regarding (80): First, we replace the summation over nsamples with n
41Published in Transactions on Machine Learning Research (08/2023)
since each sample Xiis independent. Secondly, Assumption 2 allows us to bound∂a(ηli)
∂ηliwithM. Thirdly,
the conditions on εin (68) mean/bardblBl−Bl/prime/bardblF>1thus/bardblBl−Bl/prime/bardbl2
F>/bardblBl−Bl/prime/bardblF. Plugging in (80) into
(77) gives us
I(y;l|X)≤n/bardblΣx/bardbl2/bardblBl−Bl/prime/bardbl2
FM(m)
≤4S2Mn/bardblΣx/bardbl2/tildewider
/tildewider−1ε2,
where (m)follows from (76).
A.2 Numerical Results for Poisson Regression
Figure 10 reports the estimation and prediction accuracy for Poisson regression from the experiments on
synthetic data discussed in Section 5.1. The shaded regions depict one standard deviation of mean estimation
and prediction accuracy, based on 50 replications.
(a)
 (b)
(c)
 (d)
42Published in Transactions on Machine Learning Research (08/2023)
(e)
 (f)
Figure 10: Comparison of LSRTR with PR and TTR for two-dimensional synthetic data when m∈
{32,64,128},r∈{4,8}andS= 2. Normalised estimation error for m= 32,64,and128is shown
in(a),(c),and(e), respectively. Normalised prediction error for m= 32,64,and128is shown in (b),(d),
and(f), respectively. Each marker represents the mean normalised estimation/prediction errors, over 50
repetitions. The shaded regions correspond to one standard deviation of the mean normalised errors.
43