Published in Transactions on Machine Learning Research (01/2024)
A Multilinear Least-Squares Formulation for Sparse Tensor
Canonical Correlation Analysis
Jun Yu juy220@lehigh.edu
Department of Computer Science and Engineering
Lehigh University
Zhaoming Kong kong.zm@mail.scut.edu.cn
School of Software Engineering
South China University of Technology
Kun Chen kun.chen@uconn.edu
Department of Statistics
University of Connecticut
Xin Zhang henry@stat.fsu.edu
Department of Statistics
Florida State University
Yong Chen ychen123@pennmedicine.upenn.edu
Department of Biostatistics, Epidemiology and Informatics
University of Pennsylvania
Lifang He lih319@lehigh.edu
Department of Computer Science and Engineering
Lehigh University
Reviewed on OpenReview: https: // openreview. net/ forum? id= zc0Y0cAuTV
Abstract
Tensor data are becoming important recently in various applications, e.g., image and
video recognition, which pose new challenges for data modeling and analysis approaches,
such as high-order relations of large complexity, varying data scale and gross noise. In
this paper, we consider the problem of sparse canonical correlation analysis for arbitrary
tensor data. Although several methods have been proposed for this task, there are still
limitations hindering its practical applications. To this end, we present a generalSparse
TensorCanonicalCorrelation Analysis (gSTCCA) method from a multilinear least-squares
perspective. Specifically, we formulate the problem as a constrained multilinear least-squares
problem with tensor-structured sparsity regularization based on CANDECOMP/PARAFAC
(CP) decomposition. Then we present a divide-and-conquer deflation approach to tackle the
problem by successive rank-one tensor estimation of the residual tensors, where the overall
model is broken up into a set of unconstrained linear least-squares problems that can be
efficiently solved. Through extensive experiments conducted on five different datasets for
recognition tasks, we demonstrate that the proposed method achieves promising performance
compared to the SOTA vector- and tensor-based canonical correlation analysis methods in
terms of classification accuracy, model sparsity, and robustness to missing and noisy data.
The code is publicly available at https://github.com/junfish/gSTCCA.
1 Introduction
Canonical Correlation Analysis (CCA) (Hotelling, 1936) is a powerful tool for correlation analysis and
dimensionality reduction that has been successfully applied in a wide spectrum of fields, such as industrial
process (Zhu et al., 2023), neuroscience (Mihalik et al., 2022), speech processing (Choukri & Chollet, 1986),
natural language processing (Dhillon et al., 2011), computer vision (Lu et al., 2022), and graph learning (Zhang
et al., 2021). Suppose that two sets of measurements are available on the same set of observations, the classical
1Published in Transactions on Machine Learning Research (01/2024)
CCA seeks linear combinations or loadings of all variables in each set that are maximally correlated with each
other. Over the years, CCA has been extensively studied from multiple perspectives (Hardoon et al., 2004;
Yang et al., 2019; Lindenbaum et al., 2021; Sanghavi & Verma, 2022; Friedlander & Wolf, 2023). In particular,
CCA provides fully dense loadings, it can often lead to difficulty in interpretation of results and disturbance
from noisy features (Park & Konishi, 2020). When the number of variables far exceeds the number of subjects,
classical CCA methods are no longer appropriate (Chalise & Fridley, 2012). Therefore, a variety of models
have been proposed for sparse CCA (Chu et al., 2013; Wilms & Croux, 2015; Gao et al., 2017). Furthermore,
the sparsity of model weights provides essential interpretability in numerous applications (Witten et al., 2009;
Qiu et al., 2022). Nevertheless, these approaches have been limited to vector-type data and cannot effectively
deal with high order tensor data.
In many real-world applications, data are frequently organized in the form of tensors. Specifically, a grayscale
face image is transformed as a second-order tensor of height×widthand a gait sequence is arranged as
a third-order tensor of body height×body width×gait length . A common approach for applying CCA
methods to tensor data is to convert it into vectors by flattening the tensor element by element. However,
such an approach ignores the latent higher-order structure of pixel arrangement, such as spatial coherence.
Additionally, the high dimensionality of the resulting vectors can lead to computational challenges and
inefficiency (Zhou et al., 2013).
In recent years, there has been increasing interests in generalizing CCA methods to tensors. Several tensor-
based methods have been proposed to overcome the above limitations (Chen et al., 2021; Gang et al., 2011;
Kim et al., 2007b; Lee & Choi, 2007; Min et al., 2019; Wang et al., 2016). The objective is to discover
relationships between two tensors while simultaneously preserving multidimensional structure of the tensors
and utilizing substantially fewer parameters. While these methods have shown promising results in several
applications, most of them directly maximize the correlation between two tensor spaces and require input
tensors having the same order or even the same size, which are not practical for general use. Furthermore,
existing sparse tensor approaches (Min et al., 2019; Wang et al., 2016) focus on quantifying the sparsity of
factor vectors, where the high-order correlations between different modes of tensors are ignored. The use of
structured sparsity in tensors is still an issue under the constraint of unit variance in CCA criteria.
In this paper, we propose a general sparse tensor CCA (gSTCCA) method for analyzing paired tensor
data with arbitrary order, which exploits the tensor structure to encourage tensor-level sparsity. The main
contributions of this paper are threefold:
•We formulate the gSTCCA model as a constrained multilinear least-squares regression problem with
tensor-structured sparsity regularization based on the CANDECOMP/PARAFAC (CP) decomposi-
tion, which is both parsimonious and highly interpretable.
•We present an efficient divide-and-conquer approach to solve gSTCCA based on successive rank-one
tensor estimation of the residual tensors, and theoretically show that the whole problem boils down
to a set of unconstrained alternating least-squares problems.
•We conduct extensive experiments on five real-world classification datasets with varying tensor
sizes and orders. The results reveal that gSTCCA outperforms SOTA vector- and tensor-based
CCA methods in terms of accuracy, model sparsity, and robustness to missing and noisy data.
Moreover, gSTCCA demonstrates noteworthy self-supervised capabilities within the framework of
CCA, adapting effectively to downstream classification tasks.
2 Related Works
In this section, we briefly review the existing works that are relevant to our approach.
Vector-based CCA Approaches. CCA has been extensively studied in vector space from many aspects.
For example, a number of sparse CCA variants (Chu et al., 2013; Gao et al., 2017; Wilms & Croux, 2015;
Witten & Tibshirani, 2009) have been proposed to impose sparsity constraints on the canonical loading
vectors. In particular, the ℓ1-norm regularization is commonly used for its convexity and ease of optimization.
2Published in Transactions on Machine Learning Research (01/2024)
Various nonlinear methods have been proposed to capture nonlinear dependency in the data based on kernel
methods (Hardoon et al., 2004; Zheng et al., 2006) and deep learning methods (Andrew et al., 2013; Yang et al.,
2017). Some supervised approaches (Arandjelović, 2014; Kim et al., 2007a) have been proposed to exploit
label information by adding discriminative term to the classical CCA. Several multi-set CCA methods (Correa
et al., 2009; 2010; Parra, 2018) have been proposed to integrate multiple datasets. In addition, it has been
shown that under a mild condition, CCA can be formulated as a least-squares problem (Sun et al., 2008).
Tensor-based CCA Approaches. Several tensor-based CCA methods have recently been proposed for
tensor data. For example, Lee & Choi (2007) proposed two-dimensional CCA (2DCCA) via low-rank matrix
factorization. Gang et al. (2011) extended the 2DCCA to 3DCCA. Kim et al. (2007b) designed a shared-mode
tensor CCA (TCCA) model to capture the relations among tensor modes. Chen et al. (2021) applied CP
decomposition and devised a deflation-based model (dTCCA) using the higher-order power method. Yan
et al. (2011) proposed sparse 2DCCA (S2DCCA) with ℓ1-norm regularization. Wang et al. (2016) generalized
S2DCCA to higher-order tensors and proposed sparse tensor CCA (STCCA) by iteratively seeking CCA
components for each tensor mode. Min et al. (2019) proposed a population model via CP decomposition and
adapted it to the sparse case with the hard-thresholding technique called the spTCCA.
Generally, most of the above tensor methods focus on extending the classical CCA to maximize the correlation
coefficient between tensors, and require input tensors having the same order or size. Chen et al. (2021) solved
the problem from multilinear least-squares perspective, which is the most closely related work to us, however,
it is confined to tensor data of the same order and is not a sparse method. Furthermore, existing sparse
tensor models (Min et al., 2019; Wang et al., 2016) focus on inducing the sparsity of the factor vectors, there
still lacks a general and effective model that can directly learn tensor-level sparsity.
3 Preliminaries
Notation. Tensors are denoted by bold calligraphic letters, e.g., A∈RI1×···×IN. Matrices are denoted
by bold uppercase letters, e.g., A∈RI1×I2. Vectors are denoted by bold lowercase letters, e.g., a∈RI.
Indices are denoted by lowercase letters spanning the range from 1 to the uppercase letter of the index, e.g.,
n= 1,···,N. We denote the entries by ai,ai,j,ai,j,k, etc., depending on the number of dimensions. Thus,
each entry of an Nth-order tensorA∈RI1×···×INis indexed by Nindices{in}N
n=1, and eachinindexes the
n-mode ofA. Specifically,−ndenotes every mode except n,Vec(·)denotes the vectorization operator, and
Tr(·)denotes the trace of a matrix.
3.1 Tensor Algebra
We refer readers to Kolda & Bader (2009) for more details of tensor algebra.
Definition 3.1 (Inner product) .The inner product of two tensors A,B∈RI1×···×INis the sum of the
products of their entries, defined as ⟨A,B⟩=/summationtextI1
i1=1···/summationtextIN
i1=1ai1,···,iNbi1,···,iN.
It follows immediately that the ℓ2norm ofAis defined as∥A∥ 2=/radicalbig
⟨A,A⟩. Theℓ1norm of a tensor is
defined as∥A∥ 1=/summationtextI1
i1=1···/summationtextIN
iN=1|ai1,···,iN|.
Definition 3.2 (Tensor product) .Leta(n)∈RInbe a vector with dimension In. The tensor product of
Nvectors, denoted by A=a(1)◦···◦ a(N), is an (I1×···×IN)-tensor of which the entries are given by
Ai1,···,iN=a(1)
i1···a(N)
iN. We callAa rank-one tensor or a unit-rank tensor.
Definition 3.3 (n-mode product) .Then-mode product of a tensor A∈RI1×···×INby a vector u∈
RIn, denoted byA×nu, is an (I1×···×In−1×In+1···×IN)-tensor of which the entries are given by
(A×nu)i1,...,in−1in+1,...,iN=/summationtextIn
in=1ai1,...,iNuin.
Definition 3.4 (CP decomposition) .For any tensorA∈RI1×···×IN, the CP decomposition is defined as
A=/summationtextR
r=1σra(1)
r◦···◦ a(N)
r, whereσr∈R,a(n)
r∈RInand∥a(n)
r∥2= 1, and A(n)= [a(n)
1,···,a(n)
R]for
n= 1,···,Nare called factor matrices.
3Published in Transactions on Machine Learning Research (01/2024)
Definition 3.5 (Tensor rank1).The tensor rank of A, denoted by Rank (A), is the smallest number Rsuch
that the CP decomposition is exact.
Definition 3.6 (Orthogonality (Kolda, 2001)) .Two rank-one tensors AiandAjare orthogonal (Ai⊥Aj),
iff
⟨Ai,Aj⟩=/productdisplayN
n=1⟨a(n)
i,a(n)
j⟩= 0, (1)
where a(n)
ianda(n)
jare unit vectors based on CP.
3.2 Canonical Correlation Analysis
LetX∈RP×NandY∈RQ×Nbe two sets of Npaired observations. Without loss of generality, assume
both{xn}N
n=1and{yn}N
n=1have zero mean, i.e.,/summationtextN
n=1xn=0and/summationtextN
n=1yn=0. CCA aims to maximize
the correlation between the projections of two sets by
max
u,vuTXYTv,s.t.uTXXTu=vTYYTv=N, (2)
to get the first pair of canonical vectors u∈RPandv∈RQ, which are further used to obtain the first
pair ofcanonical variates uTXandvTY. To obtain multiple projections of CCA, we can recursively solve
the above optimization problem with additional constraint that the current canonical variates must be
orthogonal to all previous ones (Chu et al., 2013). Specifically, denoting U= [u1,···,uS]∈RP×Sand
V= [v1,···,vS]∈RQ×S, Eq. (2) can be reformulated to get multiple projections as
max
U,VTr(UTXYTV),s.t.UTXXTU=VTYYTV=IS. (3)
One appealing property of CCA is that it can be formulated from the view of least-squares problems as
follows (Brillinger, 1975; Chu et al., 2013):
min
U,V∥UTX−VTY∥2
2,s.t.UTXXTU=VTYYTV=IS. (4)
Once we obtain canonical matrices UandV, the projection of XandYinto the new space can be obtained
by(UTX,VTY).
4 Sparse Tensor Canonical Correlation Analysis
To illustrate our design, we start with the first pair of canonical rank-one tensors, and then present a deflation
method to find more than one canonical rank-one component.
4.1 Model Formulation
GivenNpaired samples of tensors {Xn,Yn}N
n=1, whereXn∈RP1×···×PSandYn∈RQ1×···×QT. Without
loss of generality, we assume that {Xn}N
n=1and{Yn}N
n=1are normalized to have mean zero and standard
deviation one on each feature. We consider the STCCA model of the form
min
U,V1
2NN/summationdisplay
n=1(⟨U,Xn⟩−⟨V,Yn⟩)2+λu∥U∥ 1+λv∥V∥ 1,
s.t.1
NN/summationdisplay
n=1⟨U,Xn⟩2= 1,1
N/summationdisplayN
n=1⟨V,Yn⟩2= 1,Rank (U) =Rank (V) = 1, (5)
where the low-rank constraints are enforced through the canonical tensors UandVin order to reduce
the complexity of the model and leverage the structural information, and the tensor-structured ℓ1norm is
employed to encourage sparsity in the multilinear combination of decision variables.
1There are multiple ways to define the tensor rank. In this paper, we define it based on the CP decomposition and call it
CP-rank.
4Published in Transactions on Machine Learning Research (01/2024)
Based on the definition of tensor rank, we assume that U=σuu(1)◦···◦ u(S),V=σvv(1)◦···◦ v(T), where
σu,σv>0,∥u(s)∥1= 1and∥v(t)∥1= 1. Note that this amounts to using an ℓ1normalization, instead of the
ℓ2norm preferred by mathematicians, and the trade-off is in the simplicity of formulae. σu≥0andσv>0
can always hold by flipping the signs of the factors.
It becomes evident that ∥U∥ 1=σu/producttextS
s=1∥u(s)∥1and∥V∥ 1=σv/producttextT
t=1∥v(t)∥1. In other words, the sparsity
of a rank-one tensor directly leads to the sparsity of its components. This allows us to kill multiple birds
with one stone: by simply pursuing the element-wise sparsity of the canonical rank-one tensor with only one
tunable parameter λin each view, solving (5) can produce a set of joint sparse factors u(s)fors= 1,···,S
simultaneously (same as v(t),t= 1,···,T).
Continuing with the unified representation, let X= [X1,···,XN]∈RP1×···×PS×NandY= [Y1,···,YN]∈
RQ1×···×QT×N. We denote byX×S
1U=X× 1σuu(1)···×Su(S)aN-dimension vector with n-th element
⟨U,Xn⟩, andY×T
1V=Y× 1σvv(1)···×Tv(T)aN-dimension vector with n-th element⟨V,Yn⟩. Eq. (5) can
be written in a unified compact form as
min
U,V1
2N∥X×S
1U−Y×T
1V∥2
2+λu∥U∥ 1+λv∥V∥ 1,
s.t.Var(X×S
1U) =Var(Y×T
1V) = 1,Rank (U) =Rank (V) = 1. (6)
4.2 Rank-One Estimation
The objective function in Eq. (6) is a non-convex multi-objective optimization problem, which makes it hard
to solve. Inspired by He et al. (2018), we show that this problem can be broken up into a set of unconstrained
alternating least-squares problems which can be efficiently solved.
Suppose we have an initial value V∗for the canonical tensor with respect to V. By fixing it, the optimization
problem in (6) reduces to
/hatwideU|V∗= arg min
U1
2N∥X×S
1U−Y×T
1V∗∥2
2+λu∥U∥ 1,
s.t.Var(X×S
1U) = 1, Rank (U) = 1. (7)
Similarly, for a fixed value U∗, the optimal value for Vcan also be reduced to
/hatwideV|U∗= arg min
V1
2N∥X×S
1U∗−Y×T
1V∥2
2+λv∥V∥ 1,
s.t.Var(Y×T
1V) = 1,Rank (V) = 1. (8)
Let us now for a moment assume that we will solve Eqs. (7) and (8) by the alternating least squares
(ALS) algorithm. We alternately optimize with respect to variables (σu,u(s))or(σv,v(t))with others
fixed. Let /hatwideu(s)=σuu(s),X(−s)=X× 1u(1)×2···×s−1u(s−1)×s+1···×Su(S), and let /hatwidev(t)=σvv(t),
Y(−t)=Y× 1v(1)×2···×t−1v(t−1)×t+1···×Tv(T). The subproblems in (7) with respect to (σu,u(s))and
(σv,v(t))boil down to
min
/hatwideu(s)1
2N∥/hatwideu(s)TX(−s)−v(t)∗TY(−t)∥2
2+λu∥/hatwideu(s)∥1,s.t.Var(/hatwideu(s)TX(−s)) = 1, (9)
min
/hatwidev(t)1
2N∥u(s)∗TX(−s)−/hatwidev(t)TY(−t)∥2
2+λv∥/hatwidev(t)∥1,s.t.Var(/hatwidev(t)TY(−t)) = 1. (10)
Once we obtain the solution /hatwideu(s), we can set σu=∥/hatwideu(s)∥1andu(s)=/hatwideu(s)/σuto satisfy the constraints
whenever /hatwideu(s)̸=0. If/hatwideu(s)=0,u(s)is no longer identifiable, we then set σu= 0and terminate the algorithm.
Analogously, σvandv(t)can be computed.
Inspired by the Lemma 3 in Mai & Zhang (2019), we present the following Lemma 4.1. We accordingly get
rid of the unit variance constraints in Eqs. (9) and (10).
5Published in Transactions on Machine Learning Research (01/2024)
Lemma 4.1. Consider the following minimization problem
min
/hatwideu1
2N∥/hatwideuTX−y∥2
2+λu∥/hatwideu∥1,s.t.Var(/hatwideuTX) =1
N/hatwideuTXXT/hatwideu= 1, (11)
where Xandyare fixed, and λuis a hyperparameter that controls sparsity. The solution to Eq. (11) is
/hatwideu∗= (Var(/tildewideu∗TX))−1/2/tildewideu∗, where /tildewideu∗is obtained via
/tildewideu∗= arg min
/tildewideu1
2N∥/tildewideuTX−y∥2
2+λu∥/tildewideu∥1 (12)
The proof of Lemma 4.1 above is shown in Mai & Zhang (2019). Specifically, the solution to Eq. (9) is
/hatwideu(s)= (Var(˜u(s)TX(−s)))−1/2˜u(s)Twith a fixed v(t)∗, where ˜u(s)is obtained by
min
/tildewideu(s)1
2N∥/tildewideu(s)TX(−s)−v(t)∗TY(−t)∥2
2+λu∥/tildewideu(s)∥1. (13)
Similarly, the solution to Eq. (10) is /hatwidev(t)= (Var(˜v(t)TY(−t)))−1/2˜v(t)Twith a fixed u(s)∗, where ˜v(t)is
obtained by
min
/tildewidev(t)1
2N∥u(s)∗TX(−s)−/tildewidev(t)TY(−t)∥2
2+λv∥/tildewidev(t)∥1. (14)
Notice that the problem boils down to alternatively solving Eqs. (13) and (14), which are the standard
least-squares lasso problems. Here we employ the fast SURF algorithm proposed in He et al. (2018) to solve
it, which can trace the solution paths for λuandλv, in the manner similar to that of stagewise solution for
sparse CCA (Sun et al., 2008). Algorithm 1 summarizes the procedure.
Algorithm 1 Rank-One Estimation
Input: Paired tensor dataset {Xn,Yn}N
n=1, step sizeϵ.
Output: Canonical rank-one tensor pair /hatwideUand/hatwideV.
1:InitializeU∗,V∗.
2:Setk= 0.
3:while not converged do
4: RunSURF (ϵ) algorithm to get /tildewideUkandλu,k
by using Eq. (13).
5: RunSURF (ϵ) algorithm to get /tildewideVkandλv,k
by using Eq. (14).
6:k←k+ 1.
7:end while
8:Compute /hatwideu(s)= (Var(˜u(s)T
kX(−s)))−1
2˜u(s)T
k,s= 1,···,S,
9:Compute /hatwidev(t)= (Var(˜v(t)T
kY(−t)))−1
2˜v(t)T
k,t= 1,···,T.Algorithm 2 Deflation for gSTCCA
Input: Paired tensor data {Xn,Yn}N
n=1, CP-rankRuandRv.
Output: Canonical tensor pair {/hatwideUr}Ru
r=1and{/hatwideVr}Rv
r=1.
1:InitializeU∗andV∗.
2:while not converged do
3: forr= 1,···,Rudo
4: Compute /hatwideUrin Eq. (15) by Algorithm 1,
5: Compute the residual /hatwideprby using Eq. (17).
6: end for
7: UpdateU∗=/summationtextRu
r=1/hatwideUr.
8: forr= 1,···,Rvdo
9: Compute /hatwideVrin Eq. (16) by Algorithm 1,
10: Compute the residual /hatwideqrby using Eq. (17).
11: end for
12: UpdateV∗=/summationtextRv
r=1/hatwideVr.
13:end while
4.3 Sequential Pursuit of Higher Rank
After the first pair of rank-one tensors is identified, it is desirable to pursue a higher-rank tensor canonical
pathway for better performance. In the following, we show how more tensor components can be derived
with a sequential rank-one tensor approximation algorithm based on the deflation technique (Chen et al.,
2021). To streamline the presentation, let us assume that we can solve the rank-one problem in Eq. (6) to get
/hatwideUrand/hatwideVrin eachr-th step and select the tuning parameters λr
uandλr
vsuitably, e.g., cross validation. In
general, the gSTCCA problem in the r-th step can be expressed alternatively as
/hatwideUr= arg min
Ur1
2N∥X×S
1Ur−/hatwidepr∥2
2+λr
u∥Ur∥1,
s.t.Var(X×S
1Ur) = 1,Rank (Ur) = 1,r= 1,···,Ru, (15)
6Published in Transactions on Machine Learning Research (01/2024)
/hatwideVr= arg min
Vr1
2N∥Y×T
1Vr−/hatwideqr∥2
2+λr
v∥Vr∥1,
s.t.Var(Y×T
1Vr) = 1,Rank (Vr) = 1,r= 1,···,Rv, (16)
where/hatwideprand/hatwideqrare the current residues of canonical variables with
/hatwidepr:=/braceleftigg
Y×T
1V∗, ifr= 1
/hatwidepr−1−X×S
1/hatwideUr−1,otherwise, (17)
and/hatwideqr:=/braceleftigg
X×S
1U∗, ifr= 1
/hatwideqr−1−Y×T
1/hatwideVr−1,otherwise, (18)
whereU∗andV∗are the initial values, /hatwidepr−1and/hatwideqr−1are the estimated values in the (r−1)-th step. Note
that, the values RuandRvare not necessarily the same, since input data XandYmay differ in tensor
structures.
It can be seen that the problems (15) and (16) are reduced to the same forms as given by Eqs. (7) and (8).
Based on Algorithm 1, a deflation method for solving the optimization problems of gSTCCA in (15) and (16)
that aims to find canonical tensor pairs {/hatwideUr}Ru
r=1and{/hatwideVr}Rv
r=1, furthermore, is summarized in Algorithm 2.
5 Discussions
Convergence Analysis. It is proved that the SURF algorithm converges to a coordinate-wise minimum
point (He et al., 2018). By construction of Steps 4 and 5 in Algorithm 1, we have f(/tildewideUk,λu,k)≥f(/tildewideVk,λv,k)≥
f(/tildewideUk+1,λu,k+1)≥f(/tildewideVk+1,λv,k+1). Thus, the iterative approach in Algorithm 1 (line 3 to line 7) monotonically
decreases the objective function (6) in each iteration. Steps 8 and 9 are equivalent to constrained optimization
problems in (9) and (10), which has been proved in Lemma 4.1 and is finite. Hence the number of
iterations is finite in Algorithm 1. Moreover, Algorithm 2 repeatedly calls Algorithm 1 to generate sequential
rank-one tensors based on the deflation, which has been proved that it leads to monotonically decreasing
sequence (da Silva et al., 2015). Thus, Algorithm 2 monotonically decreases during deflation and converges in
a finite number of steps.
Complexity Analysis. For simplicity, we assume that the size of the first view XP1×···×PS×Nis larger than
that of the second view YQ1×···×QT×N. For each iteration, the most time-consuming part of Algorithm 2
lies in iteratively performing the tensor-vector multiplication and solving the rank-one tensor estimation
by Algorithm 1. Based on the results in He et al. (2018), the computational complexity of gSTCCA per
iteration is O(NRu/summationtextS
s̸=ˆs(/producttextS
j̸=s,ˆsPj)). Since gSTCCA does not require explicit vectorization of input data,
thus yielding the space complexity of O(N/producttextS
s=1Ps).
Orthogonal Constraints. Unlike the standard CCA in Eqs. (3) and (4), for the gSTCCA model we do
not enforce orthogonality, as the concurrency of sparsity and orthogonality constraints makes the problem
much more difficult. On the other hand, sparsity and orthogonality have been two largely incompatible
goals. For instance, as mentioned in Uematsu et al. (2019), a standard orthogonalization process such as QR
factorization (Chan, 1987) will generally destroy the sparsity pattern of a matrix. Here we adopt the tensor
orthogonality given in Definition 3.6 to investigate the influence of orthogonality constraints on our gSTCCA
model. After solving gSTCCA in Algorithm 2, we can utilize the ‘CP-ORTHO’ framework (Afshar et al.,
2017) to orthogonalize rank-one tensors (dubbed gSTCCA ⊥).
FeatureProjection. Topreservethehigher-ordertensorstructureofinputdata, afterobtainingasequenceof
canonical rank-one tensors {Ur}Ru
r=1,{Vr}Rv
r=1, we can project each pair of {Xn,Yn}N
n=1onto the corresponding
feature spaces spanned by factor matrices {U(s)}S
s=1and{V(t)}T
t=1as
Xnew
n=Xn×1U(1)×···×SU(S),
Ynew
n=Yn×1V(1)×···×TV(T), (19)
where U(s)= [u(s)
1,···,u(s)
Ru]∈RPs×RuandV(t)= [v(t)
1,···,v(t)
Rv]∈RQt×Rv. The data pairXnew
nandYnew
n
areS-th order and T-th order tensors, respectively.
7Published in Transactions on Machine Learning Research (01/2024)
Table 1: Statistics of five real-world datasets in our experiments. We summarize the number of images,
classes and sizes for both the original and preprocessed ones. We can see from it that our methods have no
constraint on tensor input size.
Type Dataset # Images # Classes Image size Preprocessing Size( X) Size(Y)
Handwritten Digit MNIST 1000 3*28×28 Cropped 28 ×14 28×14
Human Face Yale 165 15 32 ×32 Wavelet 32 ×32 16×16
Brain Network BP 97 2 82 ×82 fMRI/DTI 82 ×82 82×82
Facial Expression JAFFE 213 7 200 ×180 Gabor 200 ×180 23×25×40
Gait Sequence Gait32 731 71 32 ×22×10 Optical flow 32 ×22×10 32×22×10
*For a fair comparison, we use 3 classes instead of 10 classes and follow the same settings as Chen et al. (2021) to conduct experiments on the
MNIST dataset.
Table 2: Visualization of original datasets with two views across five datasets.
6 Experiments
We evaluate the effectiveness and efficiency of gSTCCA through comprehensive experiments on diverse
real-world tensor datasets. Our evaluation encompasses comparison with state-of-the-art methods in terms of
classification accuracy, model sparsity, and training time. Furthermore, we delve into feature visualization,
perform in-depth hyperparameter analysis, and examine the robustness of our approach in handling missing
and noisy data. In all of our experiments, 80%of data are randomly selected for training, while the remaining
20%is used for testing. To mitigate randomness, we perform 10 independent cross-validation runs and report
the average results. All experiments are run with MATLAB R2022b on a machine equipped with a 3.6GHz
CPU and 16GB RAM.
6.1 Datasets
Table 1 summarizes the statistics of five real-world datasets used in our experiments. The original images with
two different views are visualized in Table 2. For reproducibility purpose, more details of data description
and preprocessing are provided as below.
Data Preprocessing. Handwritten Digit (MNIST). MNIST database (LeCun et al., 1998) contains
60000 training and 10000 testing images of size 28 ×28 with labels from ‘0’ to ‘9’. We choose a subset
consists of 1000 images with labels ‘0’, ‘1’, and ‘2’ and follow the same settings as Chen et al. (2021) to
conduct our experiments. The goal is to learn correlated representations between the upper and lower halves
(two views) of the original images.
Human Face (Yale). The face images are collected from the Yale database (Cai et al., 2007), which
contains 165 images of size 32×32. For each image, we apply wavelet transformation to generate the
corresponding encoded feature image using the dw2function in MATLAB. The original and encoded feature
tensors are used as two different views.
8Published in Transactions on Machine Learning Research (01/2024)
Table 3: Properties of all compared methods. λuandλvare the regularization parameters, Ris the
matrix/tensor rank, Kis the iteration number, Nis the sample size. For brevity we assume the size of
Xn∈RP1×···×PSis larger than that of Yn∈RQ1×···×QTwith tensor rank equal to Rfor each view.
Method Input Type Data Constraint Regularizer Hyperparameters Time Complexity Space Complexity
CCA (Hotelling, 1936) Vector — — R O (/producttextS
i=1P3
i) O(/producttextS
i=1P2
i+N/producttextS
i=1Pi)
SCCA (Avants et al., 2014) Vector — l1(u),l1(v)λu,λv,R O (KR/producttextS
i=1P2
i) O(/producttextS
i=1P2
i+N/producttextS
i=1Pi)
DCCA (Andrew et al., 2013) Vector — — Many⋆O(/producttextS
i=1P3
i) O(/producttextS
i=1P2
i+N/producttextS
i=1Pi)
3DCCA (Gang et al., 2011) Tensor S=T — R O (KN/summationtextS
i=1P3
i) O(N/producttextS
i=1Pi)
dTCCA (Chen et al., 2021) Tensor S=T — R O (KNR/summationtextS
i=1P3
i) O(N2+N/producttextS
i=1Pi)
STCCA (Wang et al., 2016) Tensor S=T l 1(u),l1(v)λu,λv,R O (K2R/summationtextS
i=1P2
i+KN/summationtextS
i=1P3
i)O(N/producttextS
i=1Pi)
spTCCA (Min et al., 2019) Tensor — l1(u),l1(v)λu,λv,R O (/producttextS
i=1P3
i+/summationtextS
i=1KP3
iR3)O(/producttextS
i=1P2
i+N/producttextS
i=1Pi)
gSTCCA (Ours) Tensor — l1(U),l1(V)λu,λv,R O (KNR/summationtextS
i̸=ˆi(/producttextS
j̸=i,ˆiPj)) O(N/producttextS
i=1Pi)
⋆“Many" means more than three hyperparameters.
Brain Network (BP). Brain networks play an important role in understanding brain functions. Bipolar
disorder (BP) dataset (Whitfield-Gabrieli & Nieto-Castanon, 2012) is collected from two modalities, e.g.,
functional magnetic resonance imaging (fMRI), and diffusion tensor imaging (DTI). We follow Liu et al.
(2018) to preprocess the imaging data, including realignment, co-registration, normalization and smoothing,
and then construct the brain networks from fMRI and DTI based on the Brodmann template, which are
treated as two views.
Facial Expression (JAFFE). The JAFFE database (Lyons et al., 2020) contains female facial expressions
of seven categories (neutral, happiness, sadness, surprise, anger, disgust and fear), and the number of images
for each category is almost the same. We first crop each image to the size of 200×180, and then construct
dataset of different sizes and orders. Specifically, we use the cropped image as the first view, and its 3D
Gabor features as the second view.
Gait Sequence (Gait32). The Gait32 dataset (Lu et al., 2008) contains 731 video sequences with 71
subjects designed for human identification. The size of each gait video is 32 ×22×10, which can be naturally
represented by a third-order tensor with the column, row, and time mode. We calculate optical flow and
obtain two 3rd-order tensors, which are used as two views (Wang et al., 2016).
6.2 Experimental Settings
Baselines. Table 3 summarizes the properties of all compared methods. Specifically, we investigate the
viability of the proposed gSTCCA as feature extractor for classification tasks, and compare it with the
following methods: standard CCA (Hotelling, 1936), sparse CCA (SCCA) (Avants et al., 2014), deep CCA
(DCCA) (Andrew et al., 2013), three-dimensional CCA (3DCCA) (Gang et al., 2011; Lee & Choi, 2007),
CP-based sparse tensor CCA (spTCCA) (Min et al., 2019), deflation-based tensor CCA (dTCCA) (Chen
et al., 2021) and sparse tensor CCA (STCCA) (Wang et al., 2016). For STCCA we implement it by
ourselves as the source code is not publicly available. For other compared methods, we use the source codes
provided by the original authors. To obtain the corresponding feature representation for classification, for
all tensor-based approaches, we use Eq. (19) to project each pair of input data (Xn,Yn)to get projected
tensors (Xnew
n,Ynew
n). Then (Xnew
n,Ynew
n)are vectorized and concatenated to form the final feature vector
fn= [Vec(Xnew
n),Vec (Ynew
n)], whichisfurtherfedtotheSVMclassifier(Chang&Lin,2011). Forvector-based
methods, we concatenate their extracted feature vectors as the input of SVM.
Hyperparameter Settings. For a fair comparison, the parameters of all compared methods are carefully
tuned using the same 5-fold cross validation strategy on the training set based on classification accuracy.
Specifically, rank Ris chosen from{5,10,···,60}, for methods that enforce sparsity constraints, λuandλv
are selected from {0.001,0.005,···,0.1}. For DCCA, we select the number of hidden layers from 2 to 4 and
learning rate lrfrom{0.001,0.005,0.01}.
9Published in Transactions on Machine Learning Research (01/2024)
Table 4: Performance comparison of different methods in terms of classification accuracy (mean ±std). ‘n/a’:
results are not available due to method constraints.
DatasetsMethodsCCA SCCA DCCA 3DCCA dTCCA STCCA spTCCA gSTCCA ⊥gSTCCA
MNIST 90.7 ±1.4 97.1±0.9 96.9±1.1 96.2±1.1 90.9±3.8 97.3±1.2 90.8±4.2 97.1±1.197.4±1.3
Yale 89.0±9.9 89.0±7.984.6±11.9 74.6±16.2 75.0±19.8 87.0±8.2 73.6±8.9 86.6±12.389.0±9.6
BP 61.1 ±9.1 58.9±10.8 51.1±12.2 55.8±11.9 55.0±12.6 50.5±10.6 52.1±9.8 59.5±13.663.2±7.0
JAFFE 86.1 ±7.2 86.3±6.9 84.8±6.8 n/a n/a n/a 81.5 ±12.1 85.9±12.290.0±5.7
Gait32 25.8 ±6.8 26.6±6.2 17.8±4.3 35.4±6.9 14.5±4.8 34.6±5.6 16.5±3.1 25.8±3.641.4±5.2
(a) Sparsity
 (b) Training Time
 (c) Sparsity vs. R
 (d) Accuracy vs. R
Figure 1: Comparisons among sparse models. (a) and (b): Histogram of SCCA, STCCA, spTCCA and
gSTCCA in terms of model sparsity and train time on five datasets. (c) and (d): Influence of Ron sparsity
and accuracy of gSTCCA.
Evaluation Metrics. To quantitatively evaluate the performance of compared methods, we mainly use
classification accuracy and model sparsity. The model sparsity is measured by the ratio of zero elements to
the total number of elements on factor matrices {U(s)}S
s=1and{V(t)}T
t=1.
6.3 Experimental Results
Table 4 presents the classification accuracy of all compared methods. The feature visualization of these
compared methods on MNIST is shown in Fig. 2. The model sparsity and training time of the sparse
approaches are illustrated in Figs. 1(a) and (b). Based on these results, we have the following observations:
Overall, the proposed gSTCCA demonstrates very competitive performance in terms of accuracy, sparsity
and efficiency. Specifically, gSTCCA produces the best results on all five datasets in terms of classification
accuracy. The state-of-the-art STCCA method also shows outstanding performance, which reflects the
advantage of pursuing sparse components. However, compared to STCCA, gSTCCA is more flexible since
STCCA can not handle paired tensor data with different orders. Meanwhile, gSTCCA is also able to achieve
higher sparsity, as can be seen from Fig. 1(a). In addition, we can see that gSTCCA outperforms gSTCCA ⊥
on all five datasets, indicating that the orthogonality constraints may introduce fitting noise and lead to poor
approximation of the observed tensors. It is noticed that some related works also discarded orthogonality
constraints of sparse components (Allen, 2012; Shen & Huang, 2008; Wang et al., 2016).
For efficiency, it is noticed from Fig. 1(b), gSTCCA is more computationally complex than STCCA, but it
is also easy and efficient to train in that we can select the optimal λforUandVfrom their corresponding
solution paths, thus eliminating the need to fine tune λuandλvfrom a pre-defined range. Besides, vector-
based sparse method ( i.e., SCCA) is very fast when the feature size is small, while its training time grows
significantly as the feature size increases. The reason is that the computational burden of SCCA lies mainly in
performing singular value decomposition (SVD), which is time-consuming for large matrices. To summarize,
the results above indicate that our gSTCCA is able to effectively and efficiently find high-quality sparse
tensor coefficients UandVthat capture the most useful components of input tensor data, and then boost
the classification performance.
10Published in Transactions on Machine Learning Research (01/2024)
(a) CCA
 (b) SCCA
 (c) DCCA
 (d) 3DCCA
(e) dTCCA
 (f) STCCA
 (g) spTCCA
 (h) gSTCCA
Figure 2: The t-SNE visualization of features obtained from compared methods on MNIST dataset.
(a) Missing data
 (b) Noisy data
 (c) Accuracy (Missing data)
 (d) Accuracy (Noisy data)
Figure 3: (a) and (b): Illustrations of MNIST images with missing ratio γ∈{0.1,0.5,0.9}and noise variance
ν∈{0.1,0.5,0.9}. (c) and (d): Classification accuracy of all compared methods on missing and noisy data.
6.3.1 Feature Visualization
To qualitatively assess the effectiveness of gSTCCA, we use the t-SNE algorithm (Van der Maaten & Hinton,
2008) to visualize the features {fi}N
i=1learned by different methods on MNIST dataset. As shown in Fig.
2, the results indicate that only STCCA and gSTCCA can produce well-separated and compact clusters.
However, from Table 4 in our paper, it is noticed that STCCA cannot handle paired tensor data with different
orders.
6.3.2 Hyperparameter Sensitivity Analysis
The tensor rank Ris essential for tensor methods, since it directly controls the reduced dimensionality of
feature space. Here we investigate the influence of tensor rank on the proposed gSTCCA over five datasets. For
simplicity we assume R=Ru=Rvand compare the sparsity and accuracy by varying Rfrom{5,10,···,30}.
From Fig. 1(c), we notice that the model becomes more sparse given a larger R, which can be explained
11Published in Transactions on Machine Learning Research (01/2024)
gSTCCA
ALS
(a)ϵ= 0.06
gSTCCA
ALS (b)ϵ= 0.06
gSTCCA
ALS (c)ϵ= 0.6
gSTCCA
ALS (d)ϵ= 0.6
Figure 4: Comparison of solution paths of gSTCCA and ALS for a subset of canonical tensors UandVwith
small and large step sizes ϵon MNIST dataset. The x-axis denotes the sparseness coefficient, and the y-axis
denotes the value of the coefficients.
by the fact that important information tends to be contained in the first few tensor components. However,
increasingRdoes not always guarantee higher accuracy in that it may introduce some unwanted noise. For
example, as can be seen from Fig. 1(d), the best results of gSTCCA on MNIST, Yale, BP, JAFFE and Gait32
are achieved with rank R= 5,20,55,10and35, respectively. In practice, this interesting observation may be
useful to narrow the range of parameter tuning for related datasets and applications.
6.3.3 Influence of Missing and Noisy Data
In many real-world applications, images are inevitably contaminated by outliers such as missing data and
noise during acquisition. Figs. 3(a) and (b) illustrate the MNIST data corrupted with certain missing ratio γ
and additive white gaussian noise (AWGN) of different variance ν, respectively. We evaluate the robustness of
all compared methods by varying γandν. From Figs. 3(c) and (d), it is noticed that the proposed gSTCCA
is more robust to high missing data ratio and noise level, because the SVD adopted by compared methods
may be more sensitive to extreme outliers.
6.3.4 Solution Path
Fig. 4 shows the collection of solution paths of gSTCCA for a subset of canonical tensors on MNIST dataset,
under both big and small step sizes. The path of estimates UandVfor eachλis treated as a function
oft=/summationtext∥U∥ 1andt=/summationtext∥V∥ 1, respectively. The x-axis denotes the sparseness coefficient, and the y-axis
denotes the value of the coefficients. The vertical lines denote (a subset of) the turning point of the path, as
the solution path for each of the coefficients is piece-wise linear. From Fig. 4, we can see that the gSTCCA
path is almost indiscernible from that of ALS when the step size is small, which matches well with the
theoretical analysis in He et al. (2018).
6.3.5 Application on ADNI
To show the significance and benefits of integrating tensor-structured sparsity into the CCA framework, we
perform the task of predicting the stage of Alzheimer’s Disease (AD) using real-world data sourced from the
Alzheimer’s Disease Neuroimaging Initiative (ADNI) database2. The dataset comprises 116 features extracted
from brain regions of interests (ROIs) across three medical imaging modalities: Voxel-Based Morphometry
Magnetic Resonance Imaging (VBM-MRI),18F-fluorodeoxyglucose Positron Emission Tomography (FDG-
PET), and 18-F florbetapir PET (AV45-PET). The feature grouping is performed using the MarsBaR
toolbox Tzourio-Mazoyer et al. (2002). For detailed information on data acquisition and preprocessing, please
refer to Yu et al. (2022).
2www.adni-info.org
12Published in Transactions on Machine Learning Research (01/2024)
Figure 5: Visualization of coefficient weights on the ADNI data. From top to bottom, three rows are
corresponding to three imaging modalities: (i.e., VBM-MRI, FDG-PET, and AV45-PET). From left to right,
there are 116 brain regions defined in Tzourio-Mazoyer et al. (2002). We can easily find that gSTCCA can
pay relatively balanced attention to all three modalities. The top 10 brain regions selected by our method
are: Cingulum-Post-L, Hippocampus-R, Hippocampus-L, Cingulum-Mid-R, Frontal-Med-Orb-L, Rectus-L,
Temporal-Pole-Sup-L, Rolandic-Oper-R, Angular-L, Parietal-Inf-L. Most of these selected regions are known
to be highly related to AD and Mild Cognitive Impairment (MCI) in previous studies Du et al. (2001);
Miklossy (2011); Vlassenko et al. (2012); Mattis et al. (2016); Bubb et al. (2018)
7 Conclusion
In this paper, we propose a novel sparse tensor CCA model for arbitrary tensor data. Specifically, we formulate
the problem as a constrained multilinear least-squares problem with tensor-structured sparse regularization,
and propose a divide-and-conquer strategy based on the deflation technique to sequentially solve the problem.
This solution results in a set of unconstrained linear least-squares problems which can be efficiently solved.
Extensive experiments conducted on five different datasets demonstrate the efficacy and robustness of our
approach in terms of classification accuracy, model sparsity and having missing and noise data. Our future
work will extend the proposed gSTCCA to multi-view learning and deep learning. It is also interesting to
explore how to better fuse gSTCCA with orthogonality constraints, which can help to discover distinct and
easily interpretable patterns from tensor data (Afshar et al., 2017).
Acknowledgments
This work is partially supported by the NSF grants (MRI-2215789, IIS-1909879, IIS-2319451, DMS-2053697),
NIH grants (1R01LM014344, 1R01AG077820, R01LM012607, R01AI130460, R01AG073435, R56AG074604,
R01LM013519, R56AG069880, U01TR003709, RF1G077820, R21AI167418, R21EY034179), and Lehigh’s
grants under Accelerator and CORE. We also would like to acknowledge and thank all the individuals who
shared their source code and datasets, or made them publicly available.
13Published in Transactions on Machine Learning Research (01/2024)
References
Ardavan Afshar, Joyce C Ho, Bistra Dilkina, Ioakeim Perros, Elias B Khalil, Li Xiong, and Vaidy Sunderam.
Cp-ortho: An orthogonal tensor factorization framework for spatio-temporal data. In ACM SIGSPATIAL ,
pp. 1–4, 2017.
Genevera Allen. Sparse higher-order principal components analysis. In Artificial Intelligence and Statistics ,
pp. 27–36. PMLR, 2012.
Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep canonical correlation analysis. In ICML,
pp. 1247–1255. PMLR, 2013.
Ognjen Arandjelović. Discriminative extended canonical correlation analysis for pattern set matching. Machine
Learning , 94(3):353–370, 2014.
Brian B Avants, David J Libon, Katya Rascovsky, Ashley Boller, Corey T McMillan, Lauren Massimo,
H Branch Coslett, Anjan Chatterjee, Rachel G Gross, and Murray Grossman. Sparse canonical correlation
analysis relates network-level atrophy to multivariate cognitive measures in a neurodegenerative population.
Neuroimage , 84:698–711, 2014.
David R. Brillinger. Time series data analysis and theory holt. Rinehart&amp Winston, New York , 1975.
Emma J Bubb, Claudia Metzler-Baddeley, and John P Aggleton. The cingulum bundle: anatomy, function,
and dysfunction. Neuroscience & Biobehavioral Reviews , 92:104–127, 2018.
Deng Cai, Xiaofei He, Yuxiao Hu, Jiawei Han, and Thomas Huang. Learning a spatially smooth subspace for
face recognition. In CVPR, 2007.
Prabhakar Chalise and Brooke L Fridley. Comparison of penalty functions for sparse canonical correlation
analysis. Computational statistics & data analysis , 56(2):245–254, 2012.
Tony F Chan. Rank revealing qr factorizations. Linear algebra and its applications , 88:67–82, 1987.
Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM Transactions on
Intelligent Systems and Technology , 2(3):1–27, 2011.
You-Lin Chen, Mladen Kolar, and Ruey S Tsay. Tensor canonical correlation analysis with convergence and
statistical guarantees. Journal of Computational and Graphical Statistics , pp. 1–17, 2021.
K Choukri and G Chollet. Adaptation of automatic speech recognizers to new speakers using canonical
correlation analysis techniques. Computer Speech & Language , 1(2):95–107, 1986.
Delin Chu, Li-Zhi Liao, Michael K Ng, and Xiaowei Zhang. Sparse canonical correlation analysis: New
formulation and algorithm. IEEE Transactions on Pattern Analysis and Machine Intelligence , 35(12):
3050–3065, 2013.
Nicolle M Correa, Yi-Ou Li, Tulay Adali, and Vince D Calhoun. Fusion of fmri, smri, and eeg data using
canonical correlation analysis. In ICASSP, pp. 385–388. IEEE, 2009.
Nicolle M Correa, Tom Eichele, Tülay Adalı, Yi-Ou Li, and Vince D Calhoun. Multi-set canonical correlation
analysis for the fusion of concurrent single trial erp and functional mri. Neuroimage , 50(4):1438–1445, 2010.
Alex P da Silva, Pierre Comon, and André LF de Almeida. An iterative deflation algorithm for exact cp
tensor decomposition. In ICASSP, pp. 3961–3965. IEEE, 2015.
Paramveer S Dhillon, Dean P Foster, and Lyle H Ungar. Multi-view learning of word embeddings via cca. In
NeurIPS , 2011.
AT Du, Nea Schuff, D Amend, MP Laakso, YY Hsu, WJ Jagust, K Yaffe, JH Kramer, B Reed, D Norman,
et al. Magnetic resonance imaging of the entorhinal cortex and hippocampus in mild cognitive impairment
and alzheimer’s disease. Journal of Neurology, Neurosurgery & Psychiatry , 71(4):441–447, 2001.
14Published in Transactions on Machine Learning Research (01/2024)
Tomer Friedlander and Lior Wolf. Dynamically-scaled deep canonical correlation analysis. In Pacific-Asia
Conference on Knowledge Discovery and Data Mining , pp. 232–244. Springer, 2023.
Lei Gang, Zhang Yong, Liu Yan-Lei, and Deng Jing. Three dimensional canonical correlation analysis and its
application to facial expression recognition. In ICICIS, pp. 56–61. Springer, 2011.
Chao Gao, Zongming Ma, Harrison H Zhou, et al. Sparse cca: Adaptive estimation and computational
barriers. The Annals of Statistics , 45(5):2074–2101, 2017.
David R Hardoon, Sandor Szedmak, and John Shawe-Taylor. Canonical correlation analysis: An overview
with application to learning methods. Neural Computation , 16(12):2639–2664, 2004.
Lifang He, Kun Chen, Wanwan Xu, Jiayu Zhou, and Fei Wang. Boosted sparse and low-rank tensor regression.
InNeurIPS , pp. 1009–1018, 2018.
Harold Hotelling. Relations between two sets of variates. Biometrika , 28(3/4):321–377, 1936.
Tae-Kyun Kim, Josef Kittler, and Roberto Cipolla. Discriminative learning and recognition of image set
classes using canonical correlations. IEEE Transactions on Pattern Analysis and Machine Intelligence , 29
(6):1005–1018, 2007a.
Tae-Kyun Kim, Shu-Fai Wong, and Roberto Cipolla. Tensor canonical correlation analysis for action
classification. In CVPR, pp. 1–8. IEEE, 2007b.
Tamara G Kolda. Orthogonal tensor decompositions. SIAM Journal on Matrix Analysis and Applications , 23
(1):243–255, 2001.
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM Review , 51(3):455–500,
2009.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
Sun Ho Lee and Seungjin Choi. Two-dimensional canonical correlation analysis. IEEE Signal Processing
Letters, 14(10):735–738, 2007.
Ofir Lindenbaum, Moshe Salhov, Amir Averbuch, and Yuval Kluger. L0-sparse canonical correlation analysis.
InInternational Conference on Learning Representations , 2021.
Ye Liu, Lifang He, Bokai Cao, S Yu Philip, Ann B Ragin, and Alex D Leow. Multi-view multi-graph
embedding for brain network clustering analysis. In AAAI, pp. 117–124, 2018.
Haiping Lu, Konstantinos N Plataniotis, and Anastasios N Venetsanopoulos. Mpca: Multilinear principal
component analysis of tensor objects. IEEE Transactions on Neural Networks , 19(1):18–39, 2008.
Y Lu, W Wang, B Zeng, Z Lai, L Shen, and X Li. Canonical correlation analysis with low-rank learning
for image representation. IEEE Transactions on Image Processing: a Publication of the IEEE Signal
Processing Society , 2022.
Michael J Lyons, Miyuki Kamachi, and Jiro Gyoba. Coding facial expressions with gabor wavelets (ivc special
issue).arXiv preprint arXiv:2009.05938 , 2020.
Qing Mai and Xin Zhang. An iterative penalized least squares approach to sparse canonical correlation
analysis. Biometrics , 75(3):734–744, 2019.
Paul J Mattis, Martin Niethammer, Wataru Sako, Chris C Tang, Amir Nazem, Marc L Gordon, Vicky Brandt,
Vijay Dhawan, and David Eidelberg. Distinct brain networks underlie cognitive dysfunction in parkinson
and alzheimer diseases. Neurology , 87(18):1925–1933, 2016.
15Published in Transactions on Machine Learning Research (01/2024)
Agoston Mihalik, James Chapman, Rick A Adams, Nils R Winter, Fabio S Ferreira, John Shawe-Taylor,
Janaina Mourão-Miranda, Alzheimer’s Disease Neuroimaging Initiative, et al. Canonical correlation analysis
and partial least squares for identifying brain-behaviour associations: a tutorial and a comparative study.
Biological Psychiatry: Cognitive Neuroscience and Neuroimaging , 2022.
Judith Miklossy. Alzheimer’s disease-a neurospirochetosis. analysis of the evidence following koch’s and hill’s
criteria. Journal of neuroinflammation , 8(1):1–16, 2011.
Eun Jeong Min, Eric C Chi, and Hua Zhou. Tensor canonical correlation analysis. Stat, 8(1):e253, 2019.
Heewon Park and Sadanori Konishi. Sparse common component analysis for multiple high-dimensional
datasets via noncentered principal component analysis. Statistical Papers , 61(6):2283–2311, 2020.
Lucas C Parra. Multi-set canonical correlation analysis simply explained. arXiv preprint arXiv:1802.03759 ,
2018.
Lin Qiu, Vernon M Chinchilli, and Lin Lin. Variational interpretable deep canonical correlation analysis. In
ICLR2022 Machine Learning for Drug Discovery , 2022.
Rushil Sanghavi and Yashaswi Verma. Multi-view multi-label canonical correlation analysis for cross-modal
matching and retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 4701–4710, 2022.
Haipeng Shen and Jianhua Z Huang. Sparse principal component analysis via regularized low rank matrix
approximation. Journal of Multivariate Analysis , 99(6):1015–1034, 2008.
Liang Sun, Shuiwang Ji, and Jieping Ye. A least squares formulation for canonical correlation analysis. In
ICML, pp. 1024–1031, 2008.
Nathalie Tzourio-Mazoyer, Brigitte Landeau, Dimitri Papathanassiou, Fabrice Crivello, Olivier Etard, Nicolas
Delcroix, Bernard Mazoyer, and Marc Joliot. Automated anatomical labeling of activations in spm using a
macroscopic anatomical parcellation of the mni mri single-subject brain. Neuroimage , 15(1):273–289, 2002.
Yoshimasa Uematsu, Yingying Fan, Kun Chen, Jinchi Lv, and Wei Lin. Sofar: Large-scale association
network learning. IEEE Transactions on Information Theory , 65(8):4924–4939, 2019.
Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning
research, 9(11), 2008.
Andrei G Vlassenko, Tammie LS Benzinger, and John C Morris. Pet amyloid-beta imaging in preclinical
alzheimer’s disease. Biochimica et Biophysica Acta (BBA)-Molecular Basis of Disease , 1822(3):370–379,
2012.
Su-Jing Wang, Wen-Jing Yan, Tingkai Sun, Guoying Zhao, and Xiaolan Fu. Sparse tensor canonical correlation
analysis for micro-expression recognition. Neurocomputing , 214:218–232, 2016.
Susan Whitfield-Gabrieli and Alfonso Nieto-Castanon. Conn: a functional connectivity toolbox for correlated
and anticorrelated brain networks. Brain Connectivity , 2(3):125–141, 2012.
Ines Wilms and Christophe Croux. Sparse canonical correlation analysis from a predictive point of view.
Biometrical Journal , 57(5):834–851, 2015.
Daniela M Witten and Robert J Tibshirani. Extensions of sparse canonical correlation analysis with
applications to genomic data. Statistical Applications in Genetics and Molecular Biology , 8(1), 2009.
Daniela M Witten, Robert Tibshirani, and Trevor Hastie. A penalized matrix decomposition, with applications
to sparse principal components and canonical correlation analysis. Biostatistics , 10(3):515–534, 2009.
Jingjie Yan, Wenming Zheng, Xiaoyan Zhou, and Zhijian Zhao. Sparse 2-d canonical correlation analysis via
low rank matrix approximation for feature extraction. IEEE Signal Processing Letters , 19(1):51–54, 2011.
16Published in Transactions on Machine Learning Research (01/2024)
Xinghao Yang, Weifeng Liu, Dapeng Tao, and Jun Cheng. Canonical correlation analysis networks for
two-view image recognition. Information Sciences , 385:338–352, 2017.
Xinghao Yang, Liu Weifeng, Wei Liu, and Dacheng Tao. A survey on canonical correlation analysis. IEEE
Transactions on Knowledge and Data Engineering , 2019.
Jun Yu, Benjamin Zalatan, Yong Chen, Li Shen, and Lifang He. Tensor-based multi-modal multi-target
regression for alzheimer’s disease prediction. In 2022 IEEE International Conference on Bioinformatics
and Biomedicine (BIBM) , pp. 639–646. IEEE, 2022.
Hengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S Yu. From canonical correlation analysis to
self-supervised graph neural networks. Advances in Neural Information Processing Systems , 34:76–89, 2021.
Wenming Zheng, Xiaoyan Zhou, Cairong Zou, and Li Zhao. Facial expression recognition using kernel
canonical correlation analysis (kcca). IEEE Transactions on Neural Networks , 17(1):233–238, 2006.
Hua Zhou, Lexin Li, and Hongtu Zhu. Tensor regression with applications in neuroimaging data analysis.
Journal of the American Statistical Association , 108(502):540–552, 2013.
Xiuli Zhu, Seshu Kumar Damarla, Kuangrong Hao, Biao Huang, Hongtian Chen, and Yicun Hua. Convlstm
and self-attention aided canonical correlation analysis for multi-output soft sensor modeling. IEEE
Transactions on Instrumentation and Measurement , 2023.
17