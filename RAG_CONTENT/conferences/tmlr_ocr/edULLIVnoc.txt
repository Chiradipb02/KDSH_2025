Published in Transactions on Machine Learning Research (01/2025)
Ask Your Distribution Shift if Pre-Training is Right for You
Benjamin Cohen-Wang bencw@mit.edu
Massachusetts Institute of Technology
Joshua Vendrow jvendrow@mit.edu
Massachusetts Institute of Technology
Aleksander Mądry madry@mit.edu
Massachusetts Institute of Technology
Reviewed on OpenReview: https: // openreview. net/ forum? id= edULLIVnoc
Abstract
Pre-training is a widely used approach to develop models that are robust to distribution
shifts. However, inpractice, itseffectivenessvaries: fine-tuningapre-trainedmodelimproves
robustness significantly in some cases but not at all in others (compared to training from
scratch). In this work, we seek to characterize the failure modes that pre-training can
andcannotaddress. In particular, we focus on two possible failure modes of models under
distributionshift: poorextrapolation(e.g., theycannotgeneralizetoadifferentdomain)and
biases in the training data (e.g., they rely on spurious features). Our study suggests that, as
a rule of thumb, pre-training can help mitigate poor extrapolation but not dataset biases.
Afterprovidingtheoreticalmotivationandempiricalevidenceforthisfinding, weexploretwo
of its implications for developing robust models: (1) pre-training and interventions designed
to prevent exploiting biases have complementary robustness benefits, and (2) fine-tuning
on a (very) small, non-diverse but de-biased dataset can result in significantly more robust
models than fine-tuning on a large and diverse but biased dataset.1
1 Introduction
A common paradigm for developing machine learning models is pre-training them on a large, diverse dataset
(e.g., ImageNet (Deng et al., 2009), JFT-300M (Sun et al., 2017), LAION-5B (Schuhmann et al., 2022)) and
then fine-tuning them on task-specific data. Indeed, compared to training from scratch, fine-tuning a pre-
trained model often significantly improves performance and reduces computational costs (Sharif Razavian
et al., 2014; Sun et al., 2017; Kornblith et al., 2019).
Yet another benefit that pre-training may offer is distribution shift robustness . Specifically, machine learning
models tend to suffer from distribution shifts, i.e., changes between the reference distribution used to develop
the model and the shifted distribution that the model actually encounters when deployed. For example, a
tumor identification model trained on tissue slide images from one hospital might perform poorly when
deployed at another hospital (Bandi et al., 2018; Koh et al., 2020). Notably, different models (with different
architectures, hyperparameters, etc.) tend to be similarly sensitive to a given distribution shift. However,
models pre-trained on auxiliary data and then fine-tuned on the reference distribution can break this trend,
exhibitingsubstantiallyhigherperformanceontheshifteddistributionthanmodelstrainedfromscratchwith
the same performance on the reference distribution (Taori et al., 2020; Miller et al., 2020; 2021; Andreassen
et al., 2021; Wortsman et al., 2021).
These robustness benefits of pre-training are promising, but they are notuniversal. In particular, fine-
tuning the same pre-trained model can yield significant robustness gains on some distribution shifts but not
1Code is available at https://github.com/MadryLab/pretraining-distribution-shift-robustness
1Published in Transactions on Machine Learning Research (01/2025)
on others (Section 3). Would a solution to attain robustness to the latter shifts then be to fine-tune a larger
model pre-trained on more data? Or are there fundamental limitations to the robustness that pre-training
can provide? To answer these questions, we would like to develop a more fine-grained understanding of when
pre-training can improve robustness. Specifically, we ask:
Can we identify and characterize the failure modes that pre-training can and cannot address?
Recall that under distribution shift, models can fail in a number of ways. One of them is their inability to
extrapolate effectively outside of the reference distribution (Gulrajani & Lopez-Paz, 2020; Koh et al., 2020).
If, for instance, a model is trained only on photos taken during the day, then it might fail when deployed on
photos taken at night.
Models can also underperform even when the shifted distribution does not contain anything “new.” In par-
ticular, they can fail due to biasesin the reference distribution. For example, if a certain feature is spuriously
correlated with the label in the reference distribution, a model might learn to exploit this relationship and
fail on examples encountered during deployment where it does not hold (Arjovsky et al., 2019; Geirhos et al.,
2020).
1.1 Our contributions
To identify the failure modes that pre-training can address, we study the robustness benefits of pre-training
under two types of distribution shifts: (1) shifts where extrapolation is necessary and (2) shifts where
extrapolation is not needed. We start by analyzing a simple logistic regression setting and illustrate why
pre-training might improve robustness to the former type of shift, but not the latter (Section 4). We
subsequently build on this intuition by measuring the robustness benefits of pre-training on synthetic and
natural distribution shifts of each type (Section 5). Our results suggest the following rule of thumb: pre-
training can help with extrapolation, but does not address other failures, for example, those stemming from
dataset biases.
Implications for developing robust models. Guided by this rule of thumb, we explore two related
avenues for harnessing pre-training to develop robust models.
1.Combining pre-training with interventions designed to handle bias (Section 6): There are a number
ofrobustnessinterventionsspecificallydesignedtomitigatebiasespresentinatrainingdataset(Byrd
& Lipton, 2019; Sagawa et al., 2020a; Liu et al., 2021; Kirichenko et al., 2022; Idrissi et al., 2022).
Our findings suggest that pre-training and this kind of intervention address two different sources of
failures (the former helping with extrapolating and the latter with avoiding dataset biases) and thus
may be viewed as complementary. We indeed find that combining them can yield models with both
sets of benefits.
2.Curating datasets for fine-tuning (Section 7): One possible intervention that aims to address dataset
biases is curating a de-biased dataset. In general, however, the de-biasing process might be pro-
hibitively expensive. That said, we find that if we leverage pre-training to help with extrapolation,
we might only need a small, non-diverse fine-tuning dataset; such a dataset might actually be fea-
sible to de-bias. For example, we demonstrate that fine-tuning on a carefully de-biased hair color
classification dataset with only 64 examples yields greater robustness than fine-tuning on the entire
CelebA dataset (Liu et al., 2015).
2 Background
Fine-tuning a pre-trained model. Methods for fine-tuning a pre-trained model vary: two common
strategies are full fine-tuning , in which one continues training the entire model, and linear probing , in which
one only fine-tunes the final layer. Some recent pre-trained models with natural language supervision (e.g.,
CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021)) can also be adapted to a downstream task in a
zero-shot context (i.e., without fine-tuning) by specifying the task through a text description. In this work,
2Published in Transactions on Machine Learning Research (01/2025)
70 75 80 85
ImageNet accuracy55606570758085ImageNet-V2 accuracy
ER = 0.49±0.15%
70 75 80 85
ImageNet accuracy2030405060708090ImageNet Sketch accuracy
ER = 12.26±1.06%
Baseline Baseline linear fit Pre-trained y=x
Figure 1: The robustness benefits of pre-training vary. On the ImageNet-V2 distribution shift (left),
different pre-trained models all exhibit very little effective robustness (ER), i.e., little improvement over the
linear trend of models trained from scratch (see Section 2). Meanwhile, on the ImageNet Sketch distribution
shift (right), some of these pre-trained models exhibit substantial effective robustness. We report average
effective robustness with a 95% confidence interval in the top left of each plot.
we focus on the full fine-tuning strategy, which typically outperforms linear probing and zero-shot models on
the reference distribution. We also will sometimes consider linear probing or zero-shot adaptation followed
by full fine-tuning; this can in some cases improve over full fine-tuning alone in terms of robustness and
performance (Kumar et al., 2022). We discuss other fine-tuning strategies in Appendix D.1.
Measuring robustness. For many distribution shifts, different models trained from scratch on the refer-
ence distribution exhibit similar degrees of robustness to the shift. Specifically, when varying architectures,
hyperparameters and training methods there is often a strong linearrelationship between the reference ac-
curacyandshifted accuracy2(i.e., the accuracies on the reference and shifted distributions, respectively)
(Taori et al., 2020; Miller et al., 2020; 2021). This relationship, dubbed accuracy on the line , can be visu-
alized by plotting shifted accuracies against reference accuracies and finding a linear fit. When this linear
trend is strong (i.e., shifted accuracies are highly correlated with reference accuracies), one can predict the
shifted accuracy of models trained from scratch from their reference accuracy. Furthermore, to quantify
the robustness of a model trained with a robustness intervention beyond the “baseline” of models trained
from scratch, one can measure the amount by which its shifted accuracy exceeds the linear fit’s prediction, a
metric known as effective robustness (ER) (Taori et al., 2020). In this work, we choose to study distribution
shifts for which accuracy on the line holds (i.e., the linear fit is strong) and quantify robustness by computing
effective robustness (see, e.g., Figure 1). See Appendix B.1.2 for additional details.
3 The Robustness Benefits of Pre-Training Vary
Our investigation is motivated by the following observation:
Pre-training can significantly improve robustness to some distribution shifts but not others.
To illustrate this, we consider two distribution shifts of ImageNet (Deng et al., 2009): ImageNet-V2 (Recht
et al., 2019) and ImageNet Sketch (Wang et al., 2019). For each of these shifts, we measure the effective
robustness (see Section 2) of various pre-trained models. Specifically, we first establish a baseline for robust-
ness by evaluating 78models trained from scratch on ImageNet (from PyTorch Image Models (Wightman,
2019)). We observe a strong linear relationship between their reference and shifted accuracies (see Figure
1). Next, we evaluate 55pre-trained models that are fine-tuned on ImageNet (also from PyTorch Image
2For a linear relationship, accuracies are probit-scaled (transformed by the inverse of the Gaussian CDF).
3Published in Transactions on Machine Learning Research (01/2025)
(a) Reference dataset 
in subspace Wref(b) Models trained from 
different initializations (c) Shift inside of Wref:
models equally robust (d) Shift outside of Wref:
robustness varies Positive example 
Negative example 
Model decision boundary 
Optimal decision boundary 
Figure 2: Illustration of logistic regression setting. (a) Consider a reference dataset that lies within a
subspaceWrefofRd. (b) Models trained from different initializations all learn the same (optimal) decision
boundary in Wref, but may behave differently outside of Wref. (c) Under shifts within Wref, models with dif-
ferent initializations are equally robust. (d) Under shifts outside of Wref, initialization can affect robustness.
Models) and measure the improvements in shifted accuracy over the linear trend. See Appendix B.2 for the
exact setup.
We find that while some of the pre-trained models exhibit substantial effective robustness on ImageNet
Sketch, they all exhibit very little effective robustness on ImageNet-V2. These pre-trained models represent
a wide variety of model architectures, pre-training datasets and pre-training algorithms–the largest model
has 1 billion parameters and is pre-trained on a dataset of 2 billion image-text pairs. Yet, the highest effective
robustness attained by anyof these models on ImageNet-V2 is just 1.80%. This suggests that pre-training
alone might not suffice to address certain types of failures that occur under distribution shift. We would like
to better understand this limitation; can we identify and characterize these types of failures?
4 Studying Pre-Training in a Logistic Regression Setting
Our central goal is to understand the failure modes that pre-training canandcannotaddress. To this end,
we first study the robustness benefits of pre-training in a simple logistic regression setting (see Figure 2).
Setup. Suppose that we are given access to a reference dataset Srefof input-label pairs, each consisting of
ad-dimensional input x∈Rdand a binary label y∈{− 1,1}. We are concerned with finding weights w∈Rd
that minimize the (standard) logistic loss on Sref:
Lref(w) =/summationdisplay
(x,y)∈Sreflog(1 +e−w⊤x·y). (1)
We assume that the reference dataset Srefsatisfies the following conditions:
1.Inputs in Sreflie within a k-dimensional (with k<d) subspace WrefofRd.Intuitively, this
condition corresponds to features lacking certain variation in the reference dataset.
2.The logistic loss Lrefhas a minimum value. This condition ensures that minimizing Lrefis
well-defined. Note that there may be multiple weights that attain this minimum value.
Startingwithinitialweights winit(which, inourcase, areeitherrandomortheresultofpre-training), suppose
that we use gradient descent to minimize Lref(w). We would like to understand how well the resulting model
performs under distribution shift. In particular, what role does pre-training play through winit? To answer
this question, we establish the following theorem (proof in Appendix A):
4Published in Transactions on Machine Learning Research (01/2025)
In-support shift 
Less None 
Mor e 
Al l Label Sett ing 
Label 
Out-of-support shift 
Label Sett ing 
Label 
Figure 3: Examples of in-support and out-of-support shifts. One example of an in-support shift
(left) is a shift in which the indoor/outdoor frequencies of animal appearances change, but the possible
combinations of animal and setting remain the same. An example of an out-of-support shift (right) is a shift
from day to night: the nighttime setting is entirely novel.
Theorem 4.1. Suppose that we start with initial weights winit∈Rdand run gradient descent to minimize
Lref(w). With an appropriately chosen learning rate, gradient descent converges to weights ˆwthat minimize
Lref. Furthermore, ˆwcan be written as
ˆw=w∗
ref+projW⊥
refwinit. (2)
Here,w∗
refis a property of the reference dataset Srefand lies within the reference subspace Wref. Meanwhile,
projW⊥
refwinitis the component of winitthat is orthogonal to Wref.
Theorem 4.1 implies that there are multiple weights that attain the minimum value of Lref, and that the
initial weights winitdetermine which of them we learn. Specifically, we can decompose the learned weights
ˆwinto two terms: w∗
refand projW⊥
refwinit. Notice that the first term is just a property of the reference
dataset and is in the reference subspace Wref, while the second term depends on winitand isorthogonal to
Wref. As a result, the reference dataset itself fully specifies the model’s behavior on inputs in Wref, while
the initialization determines how the model extends outside of Wref. Consequently, changing a model’s
initialization (e.g., with pre-training) can affect performance outside of Wref, but not within Wref.
This observation gives rise to an intuition that will guide our investigations in the remainder of this work:
pre-training can improve robustness to a distribution shift onlywhen the shifted distribution contains “out-
of-support” inputs, that is, inputs that could not be reasonably sampled from the reference distribution. In
other words, pre-training helps specifically with extrapolation outside of the reference distribution.
5 Exploring the Empirical Robustness Benefits of Pre-Training
In Section 4, we found that in a simple logistic regression setting, pre-training helps specifically with ex-
trapolation. We now want to assess whether this principle holds more broadly. To do so, we measure the
robustness benefits of pre-training under two types of shifts: in-support shifts , where models cannotfail due
to poor extrapolation (but might fail for other reasons, e.g., dataset biases), and out-of-support shifts , where
modelscanfail due to poor extrapolation (see Figure 3). We begin by describing these two types of shifts in
more detail and providing intuitions for why pre-training might improve robustness to out-of-support shifts,
but not in-support shifts.
In-support shift. A distribution shift is in-support if any input that could be sampled from the shifted
distribution could also be reasonably sampled from the reference distribution. In other words, the shifted
distribution does not contain anything “new”; however, an in-support shift can still cause failures if, for
example, the reference distribution is biased. To illustrate this failure mode, consider a cat vs. dog image
classification task in which photos are either taken indoors or outdoors. Suppose that in the reference
distribution 90% of cats appear indoors and 90% of dogs appear outdoors (i.e., the setting is spuriously
correlated with the animal). A model trained on this distribution would likely rely (at least in part) on
5Published in Transactions on Machine Learning Research (01/2025)
Baseline Baseline linear fit y=x CLIP (FT) CLIP (LP-FT) CLIP (ZS-FT) AugReg (FT) AugReg (LP-FT)
Spurious tint shift
 Label shift
60 70 80
Reference accuracy4050607080Shifted accuracy
ER = 0.34±0.87%
50 60 70 80
Reference accuracy4050607080
ER = 0.23±0.82%In-support shifts
Unseen tint shift
 Flip shift
50 60 70 80
Reference accuracy4050607080Shifted accuracy
ER = 5.93±0.80%
50 60 70 80
Reference accuracy304050607080
ER = 13.42±0.74%Out-of-support shifts
Figure 4: Robustness of pre-trained models to synthetic in-support and out-of-support shifts.
For each of two in-support shifts (left) and two out-of-support shifts (right) constructed by modifying Ima-
geNet, the reference and shifted accuracies of models trained from scratch (in blue) are linearly correlated.
Pre-trained models exhibit little effective robustness (ER), i.e., little improvement over the linear trend (see
Section 2), on the in-support shifts, but have significant effective robustness on the out-of-support shifts
(averages with 95% confidence intervals in the top left of each plot). Error bars denote 95% confidence
intervals over 4 random trials.
indoor vs. outdoor features (Xiao et al., 2020; Geirhos et al., 2020). Thus, under a shift in which the
setting/animal correlation is reversed (which would be in-support but out-of-distribution), the model would
likely underperform. If pre-training helps specifically with extrapolation, then it would not address this
failure mode and, more generally, could not improve robustness to in-support shifts.
Out-of-support shift. A distribution shift is out-of-support if there exists an input that could be sampled
from the shifted distribution but could not be reasonably sampled from the reference distribution. For
example, consider a cat vs. dog image classification task in which photos from the reference distribution are
taken during the day and photos from the shifted distribution are taken at night. In this case, the shifted
distribution contains images with previously unseen lighting conditions. Here, a model trained from scratch
might learn features that are sensitive to lighting and thus fail under the shift. Meanwhile, pre-training
could provide priors for extrapolating, e.g., by producing features that are agnostic to lighting conditions as
a starting point, leading to greater robustness.
5.1 Constructing synthetic in-support and out-of-support shifts
We now want to measure the robustness gains that pre-training provides on in-support and out-of-support
shifts. To this end, we explicity construct two shifts of each type by modifying ImageNet (Deng et al., 2009):
(1) a “spurious tint shift” in which we add a tint that is spuriously correlated with the label in the reference
dataset, but not in the shifted dataset, (2) a “label shift” in which the relative frequencies of classes change
between the reference and shifted datasets, (3) an “unseen tint shift” in add a random tint in the shifted
dataset, and (4) a “flip shift” in which we vertically flip images in the shifted dataset (see the top of Figure
4 for visualizations).
6Published in Transactions on Machine Learning Research (01/2025)
DobermannImageNet
rock beauty fish
 wooden spoon
 cowboy boot
 tennis ball
fountain penSketch
In-support
handkerchief
 scabbard
 aircraft carrier
 bulletproof vest
sarongSketch
Out-of-support
coffeemaker
 rugby ball
 slide rule
 torch
(a) Random samples from: ImageNet (top), the in-
support split of ImageNet Sketch (middle) and out-of-
support split of ImageNet Sketch (bottom).
ImageNet-V2 ImageNet Sketch ImageNet-R0.000.020.040.060.080.100.120.14Average effective robustnessOut-of-support
In-support(b) Average effective robustness of 55pre-trained models
on each split of each of the three shifts. Error bars denote
95% confidence intervals.
Figure 5:Dividing shifts of ImageNet into in-support and out-of-support splits. We divide each of
the ImageNet-V2, ImageNet Sketch and ImageNet-R datasets into an in-support split containing examples
that look like ImageNet examples and an out-of-support split containing examples that look unlike ImageNet
examples (see Appendix B.4 for a description of the splitting method). We display samples from each split of
ImageNet Sketch in Figure 5a and report the average effective robustnesses of pre-trained models in Figure
5b. See Appendix C.2.3 for scatterplots of reference vs. shifted accuracy.
For each shift, as a baseline, we train a ViT-B/32 (Dosovitskiy et al., 2021) model from scratch on the
reference dataset. We evaluate this model at different epochs and find a strong linear relationship between
reference and shifted accuracy, i.e., the accuracy on the line phenomenon occurs3(see Figure 4). Next,
we fine-tune pre-trained ViT-B/32 models and measure their effective robustness above this baseline. We
consider two pre-trained models: CLIP (Radford et al., 2021) and AugReg (Steiner et al., 2021), and three
(full) fine-tuning strategies: standard full fine-tuning (FT), linear probing followed by full fine-tuning (LP-
FT) and zero-shot initialization followed by full fine-tuning (ZS-FT). We select fine-tuning hyperparameters
that maximize accuracy on the reference distribution (in Appendix C.1.1, we find that other reasonable
hyperparameter choices yield similar robustness).
We observe that pre-trained models exhibit substantial effective robustness on out-of-support shifts, but
have close to zero effective robustness on in-support shifts (see Figure 4). In Appendix C.1.2, we vary the
strength of the biases in the in-support shifts and find that the effective robustness of pre-trained models
remains close to zero. See Appendix B.3 for a description of the exact setup.
5.2 Dividing natural shifts into in-support and out-of-support splits
So far, we have constructed synthetic in-support and out-of-support shifts and observed that pre-training can
significantly improve robustness to the latter but not the former. Now, we demonstrate that this principle
seems to extend to natural shifts as well. Note that it is hard to find natural shifts that are “purely” in-
support. After all, under natural shifts the shifted dataset may contain some inputs that are similar to
those in the reference dataset and some that are not. For example, in a shift from photos to sketches, some
sketches may look more photorealistic but most would probably be clearly distinguishable from photos. To
be able to measure robustness to each type of shift, we thus divideseveral natural shifted datasets each into
an “in-support split” containing inputs that look like they could have come from the reference dataset and
an “out-of-support split” containing the remaining inputs. We do so by training a classifier to distinguish
betweenthereferenceandshifteddatasetsandusingthisclassifiertoapproximatetheprobabilityofsampling
a given shifted example from the reference distribution (see Appendix B.4.1 for details).
3Typically, one evaluates different models to find this relationship. We use different epochs due to computational constraints.
7Published in Transactions on Machine Learning Research (01/2025)
Baseline Baseline linear fit y=x DFR Pre-trained Pre-trained and DFR40 45 50 55 60 65
Reference accuracy2025303540455055Shifted worst-group accuracy
ER=10.04%ER=1.99%ER=12.93%Effective robustnesses
3.4%2.4%
4.5%7.3% 8.8%3.3%Overlap of corrected examples
(% of shifted worst-performing group)
Figure 6: Combining pre-training and Deep Feature Reweighting (DFR) on the WILDS-FMoW
shift.Pre-training and DFR (an intervention designed to handle dataset biases (Kirichenko et al., 2022))
each yield some effective robustness (ER) and combining these two interventions yields the most effective
robustness (left). The examples corrected by applying pre-training and DFR have little overlap (right),
indicating that they largely improve performance on different subpopulations. Meanwhile, the examples
corrected by combining pre-training with DFR include most of the examples corrected by the individual
interventions (right), suggesting that combining pre-training with DFR improves performance on bothof
these subpopulations. Error bars denote 95% confidence intervals over 64 random trials.
Specifically, we consider three natural shifts of the ImageNet dataset: ImageNet-V2 (Recht et al., 2019),
which closely resembles ImageNet, ImageNet Sketch (Wang et al., 2019), which consists of sketches of Im-
ageNet classes, and ImageNet-R (Hendrycks et al., 2020a), which consists of “renditions” (e.g, paintings,
sculptures, cartoons) of a subset of ImageNet classes. We choose these shifted datasets because they include
many inputs that look like they could have come from ImageNet and many that do not (according to our
splitting method)4. In Figure 5a, we visualize examples from the in-support and out-of-support splits of
ImageNet Sketch.
Consistent with our hypothesis that pre-training helps specifically with extrapolation, on the out-of-support
splits of ImageNet Sketch and ImageNet-R pre-trained models have substantially higher effective robustness
than on the respective in-support splits (see Figure 5b). On both ImageNet-V2 splits, however, pre-trained
models have very little effective robustness. This may be because ImageNet-V2 is visually similar to Ima-
geNet, so poor extrapolation might not be a significant failure mode (instead, the performance drop may
be due to an increased presence of “harder” examples, as Recht et al. (2019) suggest). Thus, if pre-training
helps only with extrapolation, it would not be able to substantially improve robustness on the ImageNet-V2
out-of-support examples. See Appendix B.4.2 for a description of the exact setup.
6 Combining Pre-Training with Interventions for Handling Bias
Our observations in Section 5 suggest that pre-training indeed can help prevent failures caused by poor
extrapolation but not those stemming from biases in the reference dataset. How, then, can we develop
models that avoid bothfailure modes? In this section, we explore one possible strategy: combining pre-
training with interventions specifically designed to handle dataset biases.
In particular, we investigate the effectiveness of this strategy on WILDS-FMoW (Christie et al., 2018; Koh
et al., 2020), a distribution shift benchmark for classifying satellite images (in Appendix C.3.1, we provide a
4We also explored ObjectNet (Barbu et al., 2019) and ImageNet-Vid-Robust (Shankar et al., 2019) but our splitting method
marks fewer than 50examples from these shifted datasets as “in-support,” and thus we cannot reliably measure in-support
accuracy.
8Published in Transactions on Machine Learning Research (01/2025)
similaranalysisforasyntheticdistributionshift). InWILDS-FMoW,thereferencedatasetconsistsofsatellite
images taken between 2002 and 2012, while the shifted dataset consists of satellite images taken between
2016 and 2017. Additionally, the images depict different regions and models typically underperform on
underrepresented regions. Following Koh et al. (2020), we evaluate the worst-group accuracy (the minimum
accuracy across groups—in our case, regions) on the shifted dataset. Hence, robustness to this shift requires
being able to both extrapolate to later years andperform consistently across regions (e.g., by avoiding biases
that are harmful to performance on some regions).
Aiming to overcome these two challenges, we leverage two types of interventions. To extrapolate better to
later years, we initialize the model via pre-training; specifically, we obtain our model by fine-tuning a CLIP
ResNet-50 model. To handle potential biases in the reference dataset, we employ Deep Feature Reweighting
(DFR) (Kirichenko et al., 2022), an intervention intended to de-bias a model by re-training just the final
layer on group-balanced data. We measure the effective robustness of each intervention over a baseline
of ResNet-50 models trained from scratch. We find that pre-training and DFR each yield some effective
robustness and that combining the two yields greater effective robustness than applying either individually
(see the left side of Figure 6). See Appendix B.5 for a description of the exact setup.
Understanding robustness benefits. We observe that combining pre-training and DFR can be effective
for developing robust models, but is this actually because they address different failure modes, as we suggest?
To answer this question, we consider the corrected examples of each intervention, i.e., the set of test examples
that are often classified incorrectly by a baseline model but correctly by model with the intervention (on
average over 64trials). We observe that the corrected examples of pre-training and DFR have little overlap
(see the right side of Figure 6), suggesting that their benefits are indeed complementary. Meanwhile, the
corrected examples of combining pre-training with DFR include most of the corrected examples of the
individual interventions. This suggests that combining pre-training with DFR not only yields high effective
robustness but in fact leads to models with both sets of benefits.
7 Curating Datasets for Fine-Tuning
In Section 6, we explored pairing pre-training with interventions specifically designed to address dataset
biases. We observed that this strategy can be effective for developing models that both extrapolate effectively
andavoid undesirable biases present in the reference distribution.
In this section, we highlight one such intervention: training on a carefully curated (and, in particular, de-
biased) dataset insteadof the original reference dataset. In general, de-biasing a large and diverse dataset
may be prohibitively expensive. However, if we can rely on pre-training for extrapolation (as suggested in
Section 5), we might only need a small, non-diverse fine-tuning dataset, which would be more feasible to
de-bias. Thus, curating such a dataset and then fine-tuning a large pre-trained model on it might be a
relatively inexpensive method for developing robust and performant models.
As a case study, we consider the task of predicting hair color (blond vs. non-blond) in the CelebA dataset
(Liu et al., 2015). In this dataset, hair color is spuriously correlated with other attributes (especially gender).
For example, 24%of females are blond, while only 2%of males are blond. Following works studying group
robustness (Sagawa et al., 2020a; Liu et al., 2021; Kirichenko et al., 2022), we measure worst-group accuracy
to assess robustness rather than measuring accuracy on an explicit shifted dataset. In this case, the four
groups are blond females, non-blond females, blond males and non-blond males. A model exploiting the
spurious correlation between gender and hair color would likely perform poorly on the underrepresented
group of blond males.
Curatingade-biaseddataset. Tocurateade-biaseddatasetforhaircolorclassificationwith nexamples,
we construct a “counterfactual example” for each of n/2CelebA examples by changing the person’s hair to a
color corresponding to the opposite class (i.e., blond to non-blond and vice versa). We ensure that attributes
besides hair color remain unchanged and include both the original and edited images in our dataset. Hence,
attributes that are spuriously correlated with hair color in the CelebA dataset (e.g., gender, age) are equally
represented in the blond and non-blond populations of our curated dataset. To illustrate that this dataset
9Published in Transactions on Machine Learning Research (01/2025)
Non-blond male: 40% Blond male: 1%Non-blond female: 44% Blond female: 14%
CelebA dataset ( n = 162K) 
Non-blond female: 25% Blond female: 25%
[Counterfactual]  Blond female: 25%[Counterfactual]  Non-blond female: 25%
Curated dataset ( n = 64, …, 8192) 
Non-blond Blond Counterfactual (using image editing) 
(a)CelebA vs. our curated hair color classification
dataset. In the CelebA dataset (top), attributes such
as gender are spuriously correlated with the class (blond
vs. non-blond). In our much smaller curated dataset
(bottom), every real image is paired with a synthesized
“counterfactual example” of the other class. As a result,
the primary difference between the blond and non-blond
populations is hair color; other attributes such as gender,
age and hair style are not predictive. We include only
females in our dataset to illustrate that diversity might
not be necessary for robustness when fine-tuning.
75 80 85 90 95
Accuracy30405060708090Worst-group accuracy
ER=5.74%ER=68.35%
Avg. ER=60.85%Accuracy vs. worst-group accuracy
Baseline
Baseline linear fit
y=xPre-trained
Curated (n=64,,8192)
Pre-trained and Curated (n=64)(b)Fine-tuning on our curated dataset. Fine-tuning
apre-trainedmodelontheCelebAdataset(orange)yields
little effective robustness over a baseline of models trained
from scratch (blue). However, fine-tuning the same pre-
trained model on just 64examples from our curated
dataset (red) yields a model with both high effective ro-
bustness and high accuracy. Training from scratch on
our curated dataset (green) also yields high effective ro-
bustness, but results in substantially lower accuracy than
pre-trained models, even with many more examples. Er-
ror bars denote 95% confidence intervals over 64random
trials.
Figure 7: Fine-tuning a pre-trained model on a small, non-diverse but de-biased dataset (see Figure 7a)
yields a robust and performant model for hair color classification in CelebA (see Figure 7b).
doesnotneed to be diverse to yield high robustness and performance when fine-tuning, we restrict the
dataset to include onlyfemales. See Figure 7a for a visualization of the dataset and Appendix B.6 for the
image editing process. In Appendix C.4.2, we consider the simpler curation strategy of balancing the number
of samples from each group (Idrissi et al., 2022) and find that counterfactual image editing is more effective.
Fine-tuning on a de-biased dataset. As expected, models trained from scratch on the CelebA dataset
exhibit high accuracy but very low worst-group accuracy, likely because they rely on gender to predict
hair color (see Figure 7b). Furthermore, a pre-trained CLIP ViT-B/32 model fine-tuned on the CelebA
dataset exhibits very little effective robustness above these models trained from scratch, consistent with our
hypothesis that pre-training does not mitigate dataset biases. However, we observe that fine-tuning the same
pre-trained model on just64examples from our curated dataset yields a model with both high accuracy and
effective robustness. Finally, we also train models from scratch on our curated dataset and find that they
exhibit substantial effective robustness, but require many more examples to attain a comparable accuracy.
This suggests that the extrapolation benefits of pre-training are key to make effective use of our small, non-
diverse curated dataset. In particular, as we illustrate in Appendix C.4.1, pre-trained models extrapolate
from the female-only curated dataset to males better than models trained from scratch.
10Published in Transactions on Machine Learning Research (01/2025)
8 Related Work
Characterizing distribution shifts. There exists a plethora of definitions for characterizing distribution
shifts, many of which are aligned with the in-support and out-of-support chracterizations that we discuss in
this work. For example, domain generalization involves shifts in which the reference and shifted distributions
are from different domains (Koh et al., 2020; Gulrajani & Lopez-Paz, 2020). In a subpopulation shift ,
subpopulations appear with different frequencies in the reference and shifted distributions (Santurkar et al.,
2021; Koh et al., 2020; Yang et al., 2023). In shifts with spurious correlations , certain features are predictive
in the reference distribution but not in the shifted distribution (Arjovsky et al., 2019; Sagawa et al., 2020b).
Two more formal characterizations are covariate shift (Shimodaira, 2000), under which p(y|x)is fixed, and
label shift (Lipton et al., 2018), under which the label distribution may change but p(x|y)is fixed. We relate
these definitons to in-support and out-of-support shifts in Appendix D.4.
Robustness benefits of pre-training. Several works have suggested that pre-training can be an effective
strategy forimproving robustnessto distributionshifts (Hendryckset al., 2019;2020a;b; Tuet al.,2020; Taori
et al., 2020; Miller et al., 2021; Wiles et al., 2021; Andreassen et al., 2021; Bommasani et al., 2021; Liu et al.,
2022b; Ramanujan et al., 2023). In particular, Wiles et al. (2021) define different types of distribution shifts
and find that pre-training frequently improves performance under these shifts, while most other interventions
primarily help in specific settings. In the natural language processing setting, Tu et al. (2020) argue that
when pre-training helps with spurious correlations, it is because pre-trained models can generalize better
from the small number of counterexamples to these correlations; as we discuss in Appendix D.5, this is
consistent with our intuition that pre-training helps specifically with extrapolation. Lastly, Bommasani
et al. (2021) discuss failure modes that pre-training is unlikely to address including spurious correlations
(both in pre-training and fine-tuning datasets) and extrapolation across time.
9 Conclusion
In this work, we study the failure modes that pre-training alone canandcannotaddress. Our findings
suggest that pre-training can help mitigate failures caused by poor extrapolation (e.g., inability to generalize
to a new domain) but might not address other failures, such as those stemming from dataset biases. In
light of this observation, dataset biases present a fundamental limitation that cannot be overcome by simply
leveraging additional pre-training data or larger models. We thus encourage practitioners not to treat pre-
training as a panacea for robustness. Instead, they should consider the specific failures modes they might
encounter to determine if pre-training can help.
References
Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the limits of large scale
pre-training. arXiv preprint arXiv:2110.02095 , 2021.
Anders Andreassen, Yasaman Bahri, Behnam Neyshabur, and Rebecca Roelofs. The evolution of out-of-
distribution robustness throughout fine-tuning. arXiv preprint arXiv:2106.15831 , 2021.
Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv
preprint arXiv:1907.02893 , 2019.
Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen,
Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of
individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge.
IEEE Transactions on Medical Imaging , 2018.
Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenen-
baum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object
recognition models. In Neural Information Processing Systems (NeurIPS) , 2019.
11Published in Transactions on Machine Learning Research (01/2025)
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of
foundation models. arXiv preprint arXiv:2108.07258 , 2021.
Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing
instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 18392–18402, 2023.
Sébastien Bubeck. Theory of convex optimization for machine learning. arXiv preprint arXiv:1405.4980 , 15,
2014.
Jonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning? In
International conference on machine learning , pp. 872–881. PMLR, 2019.
Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 6 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In Computer Vision and Pattern Recognition (CVPR) , 2009.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. In International Conference on Learning Rep-
resentations (ICLR) , 2021.
Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias
Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. In Nature Machine In-
telligence , 2020.
Karan Goel, Albert Gu, Yixuan Li, and Christopher Ré. Model patching: Closing the subgroup performance
gap with data augmentation. arXiv preprint arXiv:2008.06775 , 2020.
Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint
arXiv:2007.01434 , 2020.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In
International conference on machine learning , pp. 1321–1330. PMLR, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition,
2015.
Kaiming He, Ross Girshick, and Piotr Dollár. Rethinking imagenet pre-training. In Proceedings of the
IEEE/CVF International Conference on Computer Vision , pp. 4918–4927, 2019.
Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness and
uncertainty. In International Conference on Machine Learning , pp. 2712–2721. PMLR, 2019.
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,
Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many
faces of robustness: A critical analysis of out-of-distribution generalization, 2020a.
Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained
transformers improve out-of-distribution robustness. arXiv preprint arXiv:2004.06100 , 2020b.
JohnHewitt,XiangLisaLi,SangMichaelXie,BenjaminNewman,andPercyLiang. Ensemblesandcocktails:
Robust finetuning for natural language generation. 2021.
Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing
achievescompetitiveworst-group-accuracy. In Conference on Causal Learning and Reasoning , pp.336–351.
PMLR, 2022.
12Published in Transactions on Machine Learning Research (01/2025)
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal
Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig
Schmidt. Openclip, 2021.
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,
Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text
supervision. In International Conference on Machine Learning , pp. 4904–4916. PMLR, 2021.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved
quality, stability, and variation. In International Conference on Learning Representations , 2018.
Alexander Ke, William Ellsworth, Oishi Banerjee, Andrew Y Ng, and Pranav Rajpurkar. Chextransfer:
performance and parameter efficiency of imagenet models for chest x-ray interpretation. In Proceedings of
the Conference on Health, Inference, and Learning , pp. 116–124, 2021.
Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for
robustness to spurious correlations. arXiv preprint arXiv:2204.02937 , 2022.
Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani,
Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Sara Beery, et al. Wilds: A benchmark of
in-the-wild distribution shifts. arXiv preprint arXiv:2012.07421 , 2020.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil
Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint arXiv:1912.11370 ,
2019.
Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In computer
vision and pattern recognition (CVPR) , 2019.
Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort
pretrained features and underperform out-of-distribution. arXiv preprint arXiv:2202.10054 , 2022.
Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander Madry.
ffcv. https://github.com/libffcv/ffcv/ , 2022.
Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facial
image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 5549–5558, 2020.
Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black
box predictors. In International conference on machine learning , pp. 3122–3130. PMLR, 2018.
EvanZLiu, BehzadHaghgoo, AnnieSChen, AditiRaghunathan, PangWeiKoh, ShioriSagawa, PercyLiang,
and Chelsea Finn. Just train twice: Improving group robustness without training group information. In
International Conference on Machine Learning , 2021.
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A
convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition , pp. 11976–11986, 2022a.
Ziquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Rong Jin, Xiangyang Ji, and Antoni B Chan. An em-
pirical study on distribution shift robustness from the perspective of pre-training and data augmentation.
arXiv preprint arXiv:2205.12753 , 2022b.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
International Conference on Computer Vision (ICCV) , 2015.
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin
Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In
Proceedings of the European Conference on Computer Vision (ECCV) , 2018.
13Published in Transactions on Machine Learning Research (01/2025)
John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. The effect of natural distribution shift
on question answering models. In International Conference on Machine Learning , pp. 6905–6916. PMLR,
2020.
John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy
Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-
of-distribution and in-distribution generalization. In International Conference on Machine Learning , pp.
7721–7735. PMLR, 2021.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In arXiv preprint arXiv:2103.00020 , 2021.
VivekRamanujan, ThaoNguyen, SewoongOh, LudwigSchmidt, andAliFarhadi. Ontheconnectionbetween
pre-training data diversity and fine-tuning robustness. arXiv preprint arXiv:2307.12532 , 2023.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize
to imagenet? In International Conference on Machine Learning (ICML) , 2019.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 10684–10695, 2022.
Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural net-
works for group shifts: On the importance of regularization for worst-case generalization. In International
Conference on Learning Representations , 2020a.
Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overpa-
rameterization exacerbates spurious correlations. In International Conference on Machine Learning , pp.
8346–8356. PMLR, 2020b.
Hadi Salman, Saachi Jain, Andrew Ilyas, Logan Engstrom, Eric Wong, and Aleksander Madry. When does
bias transfer in transfer learning? In arXiv preprint arXiv:2207.02842 , 2022.
Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation shift.
InInternational Conference on Learning Representations (ICLR) , 2021.
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,
Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale
dataset for training next generation image-text models. In arXiv preprint arXiv:2210.08402 , 2022.
Vaishaal Shankar, Achal Dave, Rebecca Roelofs, Deva Ramanan, Benjamin Recht, and Ludwig Schmidt. Do
image classifiers generalize across time? arXiv preprint arXiv:1906.02168 , 2019.
Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-the-shelf:
an astounding baseline for recognition. In conference on computer vision and pattern recognition (CVPR)
workshops , 2014.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood
function. Journal of statistical planning and inference , 2000.
Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer.
How to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint
arXiv:2106.10270 , 2021.
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness
of data in deep learning era. In Proceedings of the IEEE international conference on computer vision ,
2017.
14Published in Transactions on Machine Learning Research (01/2025)
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Mea-
suring robustness to natural distribution shifts in image classification. In Neural Information Processing
Systems (NeurIPS) , 2020.
Lifu Tu, Garima Lalwani, Spandana Gella, and He He. An empirical study on robustness to spurious corre-
lations using pre-trained language models. Transactions of the Association for Computational Linguistics ,
8:621–633, 2020.
Haohan Wang, Songwei Ge, Eric P Xing, and Zachary C Lipton. Learning robust global representations by
penalizing local predictive power. Neural Information Processing Systems (NeurIPS) , 2019.
Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models , 2019.
Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre Alvise-Rebuffi, Ira Ktena, Krishnamurthy Dvijotham,
and Taylan Cemgil. A fine-grained analysis on distribution shift. arXiv preprint arXiv:2110.11328 , 2021.
Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael
Gontijo-Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust
fine-tuning of zero-shot models. In arXiv preprint arXiv:2109.01903 , 2021.
Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of image
backgrounds in object recognition. arXiv preprint arXiv:2006.09994 , 2020.
Michael Xie, Neal Jean, Marshall Burke, David Lobell, and Stefano Ermon. Transfer learning from deep
features for remote sensing and poverty mapping. In Thirtieth AAAI Conference on Artificial Intelligence ,
2016.
Yuzhe Yang, Haoran Zhang, Dina Katabi, and Marzyeh Ghassemi. Change is hard: A closer look at
subpopulation shift. arXiv preprint arXiv:2302.12254 , 2023.
Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and Jun
Zhu. Ood-bench: Quantifying and understanding two dimensions of out-of-distribution generalization. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 7947–7958,
2022.
15Published in Transactions on Machine Learning Research (01/2025)
Appendices
A Theoretical Results 17
A.1 Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B Experiment Details 21
B.1 General . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.2 The robustness benefits of pre-training vary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.3 Constructing synthetic in-support and out-of-support shifts . . . . . . . . . . . . . . . . . . . 22
B.4 Dividing natural shifts into in-support and out-of-support splits . . . . . . . . . . . . . . . . . 23
B.5 Combining pre-training with interventions for handling bias . . . . . . . . . . . . . . . . . . . 25
B.6 Curating datasets for fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
C Additional Results 28
C.1 Constructing synthetic in-support and out-of-support shifts . . . . . . . . . . . . . . . . . . . 28
C.2 Dividing natural shifts into in-support and out-of-support splits . . . . . . . . . . . . . . . . . 31
C.3 Combining pre-training with interventions for handling bias . . . . . . . . . . . . . . . . . . . 34
C.4 Curating datasets for fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
D Additional Discussion 38
D.1 Alternative fine-tuning strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
D.2 Can pre-training hurt extrapolation? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
D.3 When does pre-training help with extrapolation? . . . . . . . . . . . . . . . . . . . . . . . . . 38
D.4 Relating in-support and out-of-support shifts to existing characterizations . . . . . . . . . . . 38
D.5 Understanding the robustness of pre-trained language models to spurious correlations . . . . 39
D.6 Additional related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
16Published in Transactions on Machine Learning Research (01/2025)
A Theoretical Results
A.1 Proof of Theorem 4.1
Setup. Suppose that we are given access to a reference dataset Srefof input-label pairs (x,y), withx∈Rd
andy∈{− 1,1}. We decide to learn a linear classifier for this task by finding a weight wthat minimizes the
(standard) logistic loss on Sref:
Lref(w) =/summationdisplay
(x,y)∈Sreflog(1 +e−w⊤x·y). (1)
We assume that the reference dataset Srefsatisfies the following conditions:
1.Inputs in Sreflie within a k-dimensional (with k<d) subspace WrefofRd.Intuitively, this
condition represents a lack of variation in certain features in the reference dataset.
2.The logistic loss Lrefhas a minimum value. This condition ensures that minimizing Lrefis
well-defined. Note that there may be multiple weights that attain this minimum value.
Theorem 4.1. Suppose that we start with initial weights winit∈Rdand run gradient descent to minimize
Lref(w). With an appropriately chosen learning rate, gradient descent converges to weights ˆwthat minimize
Lref. Furthermore, ˆwcan be written as
ˆw=w∗
ref+projW⊥
refwinit. (2)
Here,w∗
refis a property of the reference dataset Srefand lies within the reference subspace Wref. Meanwhile,
projW⊥
refwinitis the component of winitthat is orthogonal to Wref.
To prove Theorem 4.1, we will first show that running gradient descent starting from an initialization within
Wrefalways converges to the same weights w∗
ref. We will then show that running gradient descent starting
from an arbitrary initialization has the same convergence behavior except for an “offset” term projW⊥
refwinit
representing the component of the initialization that is orthogonal to Wref.
A.1.1 Convexity and smoothness of the loss
We begin by providing the gradient and hessian of Lrefand using these to establish convexity (Lemma A.1)
and smoothness (Lemma A.2) properties of Lref. The gradient of Lrefis
∇Lref(w) =/summationdisplay
(x,y)∈Srefx·y·1
1 +ew⊤x·y. (3)
The Hessian of Lrefis
∇2Lref(w) =/summationdisplay
(x,y)∈Srefxx⊤·1
2 +e−w⊤x·y+ew⊤x·y=X⊤D(w)X (4)
whereX∈R|Sref|×dis the matrix of inputs in SrefandD(w)∈R|Sref|×|Sref|is the diagonal matrix with
D(w)ii=1
2+e−w⊤x·y+ew⊤x·y. Note in particular that the non-zero elements of D(w)are in (0,1/4).
Lemma A.1. The lossLrefis (1) convex on Rd, (2) strictly convex on Wref, and (3) strongly convex on
any closed convex subset of Wref.
Proof.According to Taylor’s Theorem, for any u,v∈Rd, there exists a α∈[0,1]such that
Lref(v) =Lref(u) +∇Lref(u)⊤(v−u) +1
2·(v−u)⊤∇2Lref(v+α·(v−u))(v−u). (5)
17Published in Transactions on Machine Learning Research (01/2025)
1.Convexity on Rd.To show that Lrefis convex on Rd, we need to show that
Lref(v)≥Lref(u) +∇Lref(u)⊤(v−u)
for anyu,v∈Rd. Using (5), it suffices to show that a⊤∇2Lref(w)a≥0for anya∈Rdandw∈Rd.
Recall from (4) that ∇2Lref(w) =X⊤D(w)X. Thus, we have
a⊤∇2Lref(w)a=a⊤X⊤D(w)Xa
=∥D(w)1/2Xa∥2
2
≥0
2.Strict convexity on Wref.Next, to show that Lrefis strictly convex on Wref, we need to show
that
Lref(v)>L ref(u) +∇Lref(u)⊤(v−u)
for anyu,v∈Wref. Using (5), it suffices to show that a⊤∇2Lref(w)a>0for any non-zero a∈Wref
andw∈Wref. We know that a⊤∇2Lref(w)a=∥D(w)1/2Xa∥2
2. SinceD(w)is diagonal with positive
entries along the diagonal, ∥D(w)1/2Xa∥2
2>0if and only if Xa̸= 0. Recall that Wrefis the subspace
spanning the rows of X. Hence, since ais non-zero and is in Wref, we know that Xa̸= 0.
3.Strong convexity on any closed convex subset of Wref.Finally, to show that Lrefis strongly
convex on any closed convex subset TofWref, we need to show that there exists an m> 0such that
Lref(v)≥Lref(u) +∇Lref(u)⊤(v−u) +m
2∥v−u∥2
2
for anyu,v∈T. Using (5), it suffices to show that there exists an m> 0such thata⊤∇2Lref(w)a>
m
2·∥a∥2
2for anya∈Wrefandw∈T. Making use of the fact that Tis closed, let λminbe the
minimum diagonal entry of D(w)forw∈T, that is,
λmin= min
w∈Tmin
i∈{1,...,|Sref|}D(w)ii.
Next, letcminbe the minimum value of ∥Xa∥2
2over unit vectors ainWref, that is,
cmin= min
a∈Wref,∥a∥2=1∥Xa∥2
2.
We previously established that Xa̸= 0for any non-zero a∈Wref, which means that cmin>0.
Finally, we conclude that for m= 2·λmin·cmin,a⊤∇2Lref(w)a=∥D(w)1/2Xa∥2
2≥λmin·cmin·∥a∥2
2=
m
2·∥a∥2
2.
Lemma A.2. The gradient of the loss function ∇LrefisK-Lipschitz with K=∥X∥2
op/4.
Proof.To show that∇LrefisK-Lipschitz, we need to show that ∇2Lref(w)⪯KI. Recall from (4) that
∇2Lref(w) =X⊤D(w)X. Thus, we have
a⊤∇2Lref(w)a=a⊤X⊤D(w)Xa
=∥D(w)1/2Xa∥2
2
≤∥D(w)1/2∥2
op·∥X∥2
op·∥a∥2
2
≤(∥X∥2
op/4)·∥a∥2
2.
In the final step, we use the fact that D(w)is diagonal with non-zero elements in (0,1/4)to conclude that
∥D(w)1/2∥2
op≤1/4.
18Published in Transactions on Machine Learning Research (01/2025)
A.1.2 Convergence of gradient descent within the reference subspace
Next, we establish that there exists a unique minimizer of Lrefwithin the reference subspace Wref(Lemma
A.3) and that gradient descent converges to these weights (Lemma A.4).
Lemma A.3. There exists a unique w∗
ref∈Wrefsuch thatw∗
ref∈arg minwL(w).
Proof.We will first show that there exists a w∗
ref∈Wrefsuch thatw∗
ref∈arg minwLref(w). Letw∗∈
arg minwLref(w)be an arbitrary minimimum point of Lref. By definition, for every (x,y)∈Sref,x∈Wref.
Hence, for every such x,w⊤x=projWrefw⊤x. This means that Lref(w∗) =Lref(projWrefw∗), which implies
thatw∗
ref:=projWrefw∗∈arg minwLref(w), as desired. Next, because Lrefis strictly convex on Wref(Lemma
A.1),w∗
refis the only minimum point of LrefinWref.
Lemma A.4. If we start with winit∈Wrefand run gradient descent with η= 4/∥X∥2
opto minimize Lref(w),
the weights will converge to w∗
ref.
Proof.Suppose that we start with initial weights winit∈Wrefand run gradient descent to minimize Lref
with learning rate η. In particular, let w(0)=winitandw(t+1)=w(t)+η·∇Lref(w(t)). BecauseLrefis
convex (Lemma A.1), ∇LrefisK-Lipschitz with K=∥X∥2
op/4(Lemma A.2), and η= 4/∥X∥2
op≤1/K, we
know from Theorem 3.2 of Bubeck (2014) that
Lref(w(t))−Lref(w∗
ref)≤K·∥winit−w∗
ref∥
t−1. (6)
Hence, the loss attained by w(t)converges to the optimal loss attained by w∗
ref. To show that w(t)converges
tow∗
ref, we will show that Lrefis strongly convex on a set containing every w(t)fort≥0. In particular,
consider the set WGD={w∈Wref|∥w−w∗
ref∥2≤∥winit−w∗
ref∥2}containing weights in Wrefat least as close
tow∗
refaswinit. Clearly,WGDcontainsw(0)=winit. We know from Theorem 3.2 of Bubeck (2014) that with
each iteration of gradient descent we get closer to a minimum point, that is, ∥w(t+1)−w∗
ref∥≤∥w(t)−w∗
ref∥.
Additionally, because winitand∇Lrefare inWref, everyw(t)is inWref. Hence, every w(t)is inWGD. Because
WGDis closed and convex, from Lemma A.1 we know that Lrefis strongly convex on WGD. This means that
there exists an m> 0such that
Lref(w(t))≥Lref(w∗
ref) +∇Lref(w∗
ref)⊤(w(t)−w∗
ref) +m
2·∥w(t)−w∗
ref∥2
2.
Plugging in∇Lref(w∗
ref) = 0and rearranging, we get
∥w(t)−w∗
ref∥2
2≤2
m·(Lref(w(t))−Lref(w∗
ref)).
Finally, combining with (6) yields
∥w(t)−w∗
ref∥2
2≤2·K·∥winit−w∗
ref∥
m·(t−1)(7)
which completes our proof.
A.1.3 Proof of Theorem 4.1
We are now ready to prove Theorem 4.1. Suppose that we start with initial weights winitand run gradient
descent to minimize Lrefwith learning rate η= 4/∥X∥2
op. In particular, let w(0)=winitandw(t+1)=
w(t)+η·∇Lref(w(t))fort≥0. We will show that running gradient descent starting with an arbitrary
winithas the same behavior as running gradient descent with winitprojected onto Wref. To be more precise,
suppose that we instead start with initial weights projWrefwinitwhen running gradient descent. In particular,
with projWudenoting the projection of uonto a subspace W, letwproj(0)=projWrefwinitandwproj(t+1)=
wproj(t)+η·∇Lref(wproj(t))fort≥0. Then the trajectory of w(t)is the same as that of wproj(t)but with an
additional component projW⊥
refwinit= (winit−projWrefwinit). That is,
w(t)=projW⊥
refwinit+wproj(t).
19Published in Transactions on Machine Learning Research (01/2025)
To show that this is the case, we will proceed by induction. As a base case,
w(0)=winit
= (winit−projWrefwinit) +projWrefwinit
=projW⊥
refwinit+wproj(0).
For the inductive step, assume that the statement holds for t=k. Then,
w(k+1)=w(k)+η·∇Lref(w(k))
=projW⊥
refwinit+wproj(k)+η·∇Lref(projW⊥
refwinit+wproj(k))
=projW⊥
refwinit+wproj(k)+η·∇Lref(wproj(k))
=projW⊥
refwinit+wproj(k+1)
where in the third step we use the fact that ∇Lref(u+v) =∇Lref(u)ifv∈W⊥
ref. This completes the
induction. Because wproj(0)=projWrefwinit∈Wref, from Lemma A.4 (in particular, from equation 7), we
know that
∥wproj(t)−w∗
ref∥2
2≤2·K·∥projWrefwinit−w∗
ref∥
m·(t−1).
whereKandmare positive constants. Finally, we conclude that
∥w(t)−ˆw∥2
2=∥(projW⊥
refwinit+wproj(t))−(projW⊥
refwinit+w∗
ref)∥2
2
=∥wproj(t)−w∗
ref∥2
2
≤2·K·∥projWrefwinit−w∗
ref∥
m·(t−1)
Hence,w(t)converges to ˆw, completing our proof.
20Published in Transactions on Machine Learning Research (01/2025)
B Experiment Details
B.1 General
B.1.1 Model training
All models are trained using the FFCV data-loading library (Leclerc et al., 2022) on a cluster of A100 GPUs.
B.1.2 Measuring effective robustness
Effective robustness. In this work, we quantify the robustness of pre-trained models using effective
robustness (ER), a measure of the robustness a model above the “baseline” of models trained from scratch
(Taori et al., 2020). Computing this metric first involves establishing a relationship between the accuracies
of baseline models (in our case, models trained from scratch on a reference dataset). In particular, let
Accref(M)and Acc shift(M)denote the accuracies of a model Mon test datasets drawn from the reference
and shifted distributions, respectively. Given a set Mbaselineof baseline models, we compute a linear fit
relating Φ−1(Accref(M))andΦ−1(Accshift(M)), where Φ−1is the probit function (i.e., the inverse cumulative
distribution function of the standard normal distribution). We compute a linear fit relating probit-scaled
accuracies (instead of the accuracies themselves) because this has been empirically observed to improve the
strength of the linear relationship (Miller et al., 2021; Taori et al., 2020). Formally, we compute parameters
ˆaandˆbsuch that
ˆa,ˆb= arg min
a,b/summationdisplay
M∈M baseline∥(a·Φ−1(Accref(M)) +b)−Φ−1(Accshift(M))∥2
2.
Let/hatwidestAccshift(M)be the resulting function estimating shifted accuracy given reference accuracy, that is
/hatwidestAccshift(M) = Φ(ˆa·Φ−1(Accref(M)) +ˆb).
Then the effective robustness of a model Mis
ER(M) =Accshift(M)−/hatwidestAccshift(M)
Intuitively, effective robustness is the extent to which a model’s accuracy on the shifted distribution exceeds
the accuracy of a baseline model with the same accuracy on the reference distribution (see Figure 8).
60 65 70 75 80 85 90
Reference accuracy5060708090Shifted accuracy
Effective
robustness
(ER)y=x
Baseline
Baseline linear fit
Pre-trained
Figure 8: Visualization of effective robustness. To compute effective robustness (ER), we first estab-
lishes a linear relationship between the (probit-scaled) accuracies of baseline models (blue) on the reference
and shifted datasets. The effective robustness (green) of a pre-trained model (orange) is the amount by
which its actual accuracy on the shifted dataset exceeds the prediction of the linear trend.
21Published in Transactions on Machine Learning Research (01/2025)
Establishing a baseline for effective robustness. To establish a baseline with respect to which we can
measure effective robustness, we need a set of baseline models trained from scratch on the reference dataset.
The set of baseline models we consider varies by experiment. In each of the experiments in which we measure
effective robustness, we confirm that a strong linear relationship exists between the probit-scaled accuracies
of our baseline models on the reference and shifted datasets (see, e.g., Figure 4).
B.2 The robustness benefits of pre-training vary
In Section 3, we illustrate that pre-trained models exhibit substantial effective robustness on the ImageNet
Sketch distribution shift but very little effective robustness on the ImageNet-V2 distribution shift. We
consider 78models trained from scratch on ImageNet and 55pre-trained models fine-tuned on ImageNet, all
taken from PyTorch Image Models (Wightman, 2019). The pre-trained models represent a variety of model
architectures (e.g., ResNet (He et al., 2015), ConvNeXt (Liu et al., 2022a), ViT (Dosovitskiy et al., 2021)),
pre-training datasets (e.g., IG-1B (Mahajan et al., 2018), LAION-2B (Schuhmann et al., 2022), OpenAI’s
WIT (Radford et al., 2021)), and pre-training algorithms (e.g., supervised learning, CLIP (Radford et al.,
2021)). The complete list of models used is available with our code at https://github.com/MadryLab/
pretraining-distribution-shift-robustness .
B.3 Constructing synthetic in-support and out-of-support shifts
In Section 5.1, we measure the effective robustness of various pre-trained and fine-tuned models on two
in-support and two out-of-support shifts synthetically constructed by modifying ImageNet.
Specifications of synthetic shifts. Here, we provide detailed descriptions of the four synthetic distribu-
tion shifts (see Figure 4 for visualizations).
1.Spurious tint shift (in-support): We tint images (i.e., replace each pixel with a mix of the original
value, with weight 0.75and a specific color, with weight 0.25) such that the tint is correlated with the
label in the reference distribution but not in the shifted distribution (i.e., tint is a spurious feature).
Specifically, in the reference distribution we apply tint with a class-specific color to pspurious = 0.5of
examples and a tint with a random color to the remaining 1−pspurious = 0.5of examples. Meanwhile,
in the shifted distribution we apply a tint with a random color universally.
2.Label shift (in-support): Label shift is a commonly studied type of distribution shift in which the
relative frequencies of classes change, but p(x|y)is fixed. To construct a label shift, we sub-sample
ImageNet such that in the reference distribution, a randomly selected 500classes are less likely to
appear than the remaining 500classes. In particular, the selected classes appear with probability
pminority = 0.2, while the remaining classes appear with probability 1−pminority = 0.8. In the shifted
distribution, these relative frequencies are reversed.
3.Unseen tint shift (out-of-support): We randomly tint images in the shifted distribution (with the
same protocol as in the spurious tint shift).
4.Flip shift (out-of-support): We vertically flip images in the shifted distribution.
Shared model specifications. When training, we use the FFCV implementation of RandomResizedCro-
pRGBImageDecoder , resizing image crops to a resolution of 224×224. For data augmentation, we use the
FFCV implementations of RandomHorizontalFlip . When evaluating, we use the FFCV implementation of
CenterCropRGBImageDecoder with a ratio of 224/256, resizing image crops to a resolution of 224×224.
Specifications of baseline models. As a baseline, we train a ViT-B/32 model (the implementation
of Ilharco et al. (2021)) from scratch on ImageNet. We run AdamW for 100epochs, using a cosine learning
rate schedule with a peak learning rate of 0.003and10warmup epochs, a batch size of 512, a weight decay
of0.1, label smoothing of 0.1and gradient clipping at global norm 1. To establish a baseline for effective
robustness, we evaluate this model at epochs 50through 85(we stop at 85because the model’s accuracy at
22Published in Transactions on Machine Learning Research (01/2025)
later epochs becomes highly correlated). Miller et al. (2021) observe that evaluating a model trained from
scratchatdifferentepochsinthiswayoftenexhibitastronglinearrelationshipbetweentheiraccuraciesonthe
reference and shifted distributions (and the same relationship holds for models with different architectures,
hyperparameters, etc.).
Specifications of pre-trained models and fine-tuning strategies. We consider two different pre-
trained models: a CLIP (Radford et al., 2021) ViT-B/32 (the implementation of Ilharco et al. (2021))
and AugReg (Steiner et al., 2021) (the implementation of Wightman (2019)). For the AugReg model, we
consider full fine-tuning (FT) and linear probing followed by full fine-tuning (LP-FT) (Kumar et al., 2022).
We perform linear probing by running AdamW for 4epochs, using a cosine learning rate schedule, a peak
learning rate of 0.001, a batch size of 512, and without weight decay or gradient clipping. For the CLIP
model, we consider zero-shot initialization followed by full fine-tuning (ZS-FT) in addition to these two
strategies. We perform zero-shot initialization following Wortsman et al. (2021).
We fully fine-tune models by running AdamW for 8epochs, using a cosine learning rate schedule with 1
warmup epoch. We select the best peak learning rate (in terms of reference accuracy) among 3×10−4,1×
10−4,3×10−5,1×10−5,3×10−6,1×10−6. We use a batch size of 512, a weight decay of 0.1, and gradient
clipping at global norm 1.
B.4 Dividing natural shifts into in-support and out-of-support splits
B.4.1 Splitting a Shifted Dataset
To split a shifted dataset into an “in-support split” and an “out-of-support split”, we would ideally measure
the reference distribution probability density prefof inputs in the shifted dataset and assign inputs with
smallprefto the out-of-support split. Unfortunately, it is difficult to estimate prefdirectly when dealing with
high-dimensional inputs (in this case, images). Instead, we estimate the probability density ratiopref/pshift,
that is, how much more likely an input is under the reference distribution than under the shifted distribution.
We then assign examples in the shifted dataset with pref/pshift<0.2to the out-of-support split and examples
withpref/pshift≥0.2to the in-support split. We visualize examples in Figure 13.
Estimating pref/pshift.To estimate pref/pshift, we use a classifier trained to distinguish between examples
from the reference and shifted datasets. Specifically, let pbe a probability mass/density function over
examples that can either be drawn from DreforDshift(i.e.,prepresents the distribution of a dataset created
by joining a reference dataset and a shifted dataset). Next, let yrefbe the event that an example is drawn
fromDrefandyshiftbe the event that an example is drawn from Dshift. We can express the ratio pref/pshift
as follows:
pref(x)
pshift(x)=p(x|yref)
p(x|yshift)
=p(yref|x)·p(x)
p(yref)·p(yshift)
p(yshift|x)·p(x)
=p(yref|x)
p(yshift|x)·p(yshift)
p(yref).
The terms p(yref)andp(yshift)are easy to estimate since they are simply the proportions of reference and
shifted examples in p. Hence, to estimate pref/pshiftwe just need to estimate p(yref|x)andp(yshift|x).
To do so, we train a classifier to distinguish between reference and shifted examples on a dataset drawn
fromp. We construct such a dataset by combining 100Ksamples from ImageNet with each of the shifted
datasets (for ImageNet-R, which contains a subset of the classes of ImageNet, we restrict the 100Ksamples
to these classes). Next, we fine-tune a CLIP ViT-L/14 pre-trained on LAION-2B from OpenCLIP (Ilharco
et al., 2021) to distinguish between reference and shifted examples. We first fine-tune just the final layer
with a learning rate of 0.1and then fine-tune the entire model with the best learning rate selected from
2×10−4,1×10−4,5×10−5,2×10−5,1×10−5,5×10−6,2×10−6and1×10−6. After training the classifier,
we calibrate it through temperature scaling (Guo et al., 2017). We then estimate p(yref|x)andp(yshift|x)
23Published in Transactions on Machine Learning Research (01/2025)
by applying a sigmoid to its output, from which we can estimate pref/pshift. To estimate this ratio for the
entire shifted dataset, we split the dataset into 10folds and train a classifier to estimate pref/pshifton each
fold using the remaining 9folds.
Calibrating the classifiers used for splitting As discussed in Section B.4, our method for dividing a
shifteddatasetintoanin-supportsplitandanout-of-supportsplitrequiresa calibrated classifiertodistinguish
between examples from the reference and shifted datasets. Recall that to distinguish between examples from
the reference and shifted datasets, we fine-tune a CLIP Radford et al. (2021) ViT-L/14 Dosovitskiy et al.
(2021) pre-trained on LAION-2B from OpenCLIP (Ilharco et al., 2021). Such over-parameterized models
can be overconfident in their predictions (and thus uncalibrated), so we calibrate the classifier by rescaling
its (logit) output, a method known as temperature scaling (Guo et al., 2017).
In particular, let fbe a (potentially uncalibrated) classifier trained to distinguish between examples from
the reference and shifted datasets (where the output of fis a logit). We find the scaling parameter αthat
minimizes the standard logistic loss of fon a calibration set Scal:
α= arg min
α′/summationdisplay
(x,y)∈Scallog(1 +e−α′·f(x)·y). (8)
We then define a rescaled classifier fcal(x) =α·f(x)(which is used to estimate the ratio pref/pshift). We
produce calibration curves of the rescaled classifiers for each of the shifted datasets we split (see Figure 9)
and observe that they are indeed well-calibrated.
0.0 0.2 0.4 0.6 0.8 1.0
Average predicted probability0.00.20.40.60.81.0Positive rate
ImageNet-V2
0.0 0.2 0.4 0.6 0.8 1.0
Average predicted probability0.00.20.40.60.81.0
ImageNet Sketch
0.0 0.2 0.4 0.6 0.8 1.0
Average predicted probability0.00.20.40.60.81.0
ImageNet-R
y=x (perfectly calibrated) Our model
Figure 9: Calibration curves of classifiers used for splitting. We display calibration curves for the
classifiers used to divide ImageNet-V2, ImageNet-Sketch and ImageNet-R into in-support and out-of-support
splits. Specifically, we sort the outputs of each classifier on a combined dataset of reference and shifted
examples into 100bins (where bin edges are quantiles). For each bin, we compute the actual positive
rate (i.e., the proportion of examples from the shifted dataset) and the average predicted probability of an
example being from the shifted dataset. When we plot the actual positive rates against average predicted
probabilities, they are close to equal (close to y=x), suggesting that the classifiers are well-calibrated. Error
bars denote 95% Clopper-Pearson confidence intervals.
B.4.2 Specifications of ImageNet models
To measure the robustness benefits of pre-training on in-support and out-of-support splits of ImageNet
distribution shifts, we use the same suite of ImageNet models from PyTorch Image Models (Wightman,
2019) as detailed in Appendix B.2.
24Published in Transactions on Machine Learning Research (01/2025)
B.5 Combining pre-training with interventions for handling bias
Shared model specifications. When training on the WILDS-FMoW dataset, we use the FFCV imple-
mentation of RandomHorizontalFlip .
Specifications of models trained from scratch. We train models from scratch by running SGD for 64
epochs, using a triangular learning rate schedule with a peak learning rate of 0.2and8warmup epochs, a
batch size of 128, a weight decay of 5×10−4and a momentum of 0.9.
Baseline specifications. To establish a baseline, we train 100ResNet-50 models from scratch on ran-
dom subsets ranging from 25% of the reference dataset to the entire dataset. We increase the number of
epochs and warmup epochs inversely with the size of the subset. Miller et al. (2021) observe that models
trained from scratch in this way often exhibit a strong linear relationship between their accuracies on the
reference and shifted distributions (and the same relationship holds for models with different architectures,
hyperparameters, etc.).
Specifications of pre-trained models. The pre-trained model in this experiment is a CLIP ResNet-50
model (the implementation of Ilharco et al. (2021)), adapted using linear probing followed by full fine-tuning.
Note that the CLIP ResNet-50 architecture (Radford et al., 2021) deviates from the standard ResNet-50
architecture of He et al. (2015). We perform linear probing by running AdamW for 8epochs, using a cosine
learning rate schedule, a peak learning rate of 0.001, a batch size of 512, and without weight decay or gradient
clipping. We fine-tune models by running AdamW for 16epochs, using a cosine learning rate schedule with a
peak learning rate of 1×10−4and2warmup epochs, a batch size of 512, a weight decay of 0.1, and gradient
clipping at global norm 1.
Our implementation of Deep Feature Reweighting .TheDeep Feature Reweighting (DFR) interven-
tion proposed by Kirichenko et al. (2022) aims to improve the robustness of a model on difficult subpop-
ulations by using a validation dataset with group labels. The algorithm consists of two steps: (1) train
a standard model on the original training dataset, and (2) re-train only the final layer of the model (i.e.,
“re-weight” the features of the model) on the validation dataset to be more favorable to minority groups. To
re-train the final layer, Kirichenko et al. (2022) repeatedly sample group-balanced subsets of the validation
dataset, re-train the final layer on each subset, and then average the resulting re-trained final layers. Our im-
plementation differs slightly in that we assign sample weights to the validation dataset such that each group
has equal total weight and re-train the final layer on the weighted validation dataset. When applying Deep
Feature Reweighting to WILDS-FMoW, we use the out-of-distribution validation set following Kirichenko
et al. (2022).
B.6 Curating datasets for fine-tuning
Image editing to synthesize “counterfactual examples” In order to curate a “de-biased” dataset
for hair color classification, we edit images from CelebA-HQ (Karras et al., 2018), a subset of the CelebA
dataset with segmentation masks for each attribute provided by CelebAMask-HQ (Lee et al., 2020). To
change the hair color in a given image, we use InstructPix2Pix (Brooks et al., 2023), a recent image editing
model fine-tuned from Stable Diffusion (Rombach et al., 2022). This model accepts an input image to be
edited along with a prompt describing the desired change (e.g., “change the hair color to blond”). We find
that InstructPix2Pix is able to successfully edit the hair color; however, this model often makes undesired
changes to attributes such as skin tone and eye color (see, e.g., the left side of Figure 10). To ensure that we
only edit hair color, we use the attribute masks to isolate the pixels in a given image corresponding to the
hair region, and ignore any changes made outside of this area. When using a binary mask, this procedure
could cause unnatural “edges” along the border of the mask. Thus, we apply a Gaussian blur to the hair
mask to smooth the transition when “merging” the original and edited images.
To edit an image from non-blond to blond, we use the prompt “change the hair color to blond.” When
editing from blond to non-blond, however, we find that the prompt “change the hair color to non-blond”
gives inconsistent results, likely because the instruction is vague. We observe that most non-blond people
25Published in Transactions on Machine Learning Research (01/2025)
in the CelebA dataset have brown or black hair, so as a simple heuristic we randomly edit each image with
either the prompt “change the hair color to brown” or the prompt “change the hair color to black.” See
Figure 10 for a visualization of the image editing process.
InstructPix2Pix: “Change the hair color to blond” Counterfactual Example 
Unwanted Edits Accept changes 
within segmentation 
Reject changes outside 
of segmentation 
Figure 10: Synthesizing counterfactual examples. We edit hair color in CelebA-HQ images using
InstructPix2Pix (Brooks et al., 2023). However, this model can also make unwanted changes to attribute
other than hair color, e.g., changing eye color (left). To avoid such issues, in the final image we incorporate
only changes within the hair region of the image.
Shared model specifications. Accuracy and worst-group accuracy on the CelebA dataset are sensitive
to hyperparameter choices. As a result, we conduct a grid search to select hyperparameters for each type of
model. We use class-balanced accuracy as the metric for hyperparameter selection, which empirically better
correlates with worst-group accuracy than standard accuracy.
When selecting hyperparameters for a curated dataset of a given size, we randomly sample 32datasets of
that size from a pool of 16,000images (i.e., 8,000CelebA images and their corresponding counterfactual
synthesized images) and average the class-balanced accuracies of models trained on each dataset. When
evaluating the accuracy and worst-group accuracy of models trained on a curated dataset of a given size, we
similarly randomly sample 64datasets of that size and report average metrics.
For all models, we use the FFCV implementation of RandomHorizontalFlip for data augmentation.
Specifications of models trained from scratch. We train ResNet-18 models from scratch by running
SGD for 32epochs, using a triangular learning rate schedule with 4warmup epochs. We use a batch size of
128, a weight decay of 5×10−4and a momentum of 0.9. We select the best combination of batch size and
learning rate from batch sizes of 64,128,256,512and learning rates of 0.5,0.2,0.1,0.05,0.02,0.01.
When training models from scratch on our curated dataset, we run SGD for 512epochs and use a triangular
learning rate schedule with 64warmup epochs. We use a batch size equal to the total number of examples
when it is less than 512and a batch size of 512otherwise. We use a weight decay of 5×10−4and a momentum
of0.9. We select the best learning rate from 0.5,0.2,0.1,0.05,0.02,0.01.
Baseline specifications. To establish a baseline, we train 100ResNet-50 models from scratch on ran-
dom subsets ranging from 5% of the reference dataset to the entire dataset. We increase the number of
epochs and warmup epochs inversely with the size of the subset. Miller et al. (2021) observe that models
trained from scratch in this way often exhibit a strong linear relationship between their accuracies on the
reference and shifted distributions (and the same relationship holds for models with different architectures,
hyperparameters, etc.).
26Published in Transactions on Machine Learning Research (01/2025)
Specifications of pre-trained models. The pre-trained model in this experiment is a CLIP ViT-B/32
model initialized as a zero-shot classifier with “blond” and “non-blond” as the class names. We fine-tune
models by running AdamW for 16epochs, using a cosine learning rate schedule with 2warmup epochs, and
a weight decay of 0.1. We select the best combination of batch size and learning rate from batch sizes of
64,128,256,512and learning rates of 3×10−5,1×10−5,3×10−6,1×10−6.
When training on our curated dataset, we use a batch size of 64(the size of the dataset) and select the best
learning rate from 3×10−5,1×10−5,3×10−6,1×10−6.
27Published in Transactions on Machine Learning Research (01/2025)
C Additional Results
C.1 Constructing synthetic in-support and out-of-support shifts
C.1.1 How does the choice of fine-tuning hyperparameters affect robustness?
In Section 5.1, we select hyperparameters (in particular, learning rate) for fine-tuning that maximize accu-
racy on the reference distribution. This reasonably simulates hyperparameter selection in practice because
typically only samples from the reference distribution are available.
In this section, we investigate how the choice of hyperparameters affects the robustness of pre-trained models.
In particular, we would like to understand if pre-training yields little effective robustness to in-support shifts
and substantial effective robustness to out-of-support shifts across a wider range of hyperparameter choices.
We study the spurious tint shift (an in-support shift) and the flip shift (an out-of-support shift) from Section
5.1 and vary the learning rate, weight decay, number of epochs, and batch size of a CLIP ViT-B/32 initialized
with zero-shot weights (Figure 11). With zero-shot initialization, the starting point of fine-tuning is a robust
model that performs well on our task. Hence, even under an in-support shift, hyperparameter choices that
do not change the model substantially (e.g., low learning rate, small number of epochs) result in substantial
effective robustness. However, these hyperparameter choices generally result in lower absolute reference and
shifted accuracies, and are thus unreasonable. The hyperparameter choices that are relevant in practice are
those with high reference accuracy, and these are the hyperparameters that we use in our experiments.
28Published in Transactions on Machine Learning Research (01/2025)
60 70 80
Reference accuracy4050607080Shifted accuracy
Varying learning rate
Baseline
Baseline linear fit
CLIP zero-shot init.
y=x
0.0003
0.0001
3e-05 (used)
1e-05
3e-06
1e-06
60 70 80
Reference accuracy4050607080Shifted accuracy
Varying batch size
Baseline
Baseline linear fit
CLIP zero-shot init.
y=x
64
128
256
512 (used)
60 70 80
Reference accuracy4050607080Shifted accuracy
Varying number of epochs
Baseline
Baseline linear fit
CLIP zero-shot init.
y=x
1
2
4
8 (used)
16
32
60 70 80
Reference accuracy4050607080Shifted accuracy
Varying weight decay
Baseline
Baseline linear fit
CLIP zero-shot init.
y=x
0.1 (used)
0.01
0.001
0.0001
1e-05
0.0
(a)In-support shift. The in-support shift we consider is the “spurious tint shift” in which we introduce a tint
that is spuriously correlated with the label. On this in-support shift, learning rate and number of epochs influence
effective robustness, but the best hyperparameter choices result in a model with little effective robustness.
50 60 70 80
Reference accuracy304050607080Shifted accuracy
Varying learning rate
Baseline
Baseline linear fit
CLIP zero-shot init.
y=x
0.0003
0.0001
3e-05
1e-05 (used)
3e-06
1e-06
50 60 70 80
Reference accuracy304050607080Shifted accuracy
Varying batch size
Baseline
Baseline linear fit
CLIP zero-shot init.
y=x
64
128
256
512 (used)
50 60 70 80
Reference accuracy304050607080Shifted accuracy
Varying number of epochs
Baseline
Baseline linear fit
CLIP zero-shot init.
y=x
1
2
4
8 (used)
16
32
50 60 70 80
Reference accuracy304050607080Shifted accuracy
Varying weight decay
Baseline
Baseline linear fit
CLIP zero-shot init.
y=x
0.1 (used)
0.01
0.001
0.0001
1e-05
0.0
(b)Out-of-support shift. The out-of-support shift we consider is the “flip shift” in which we pad images in the
shifted distribution. On this out-of-support shift, batch size most significantly affects robustness, while learning rate
and number of epochs affect overall performance.
Figure 11: The effects of hyperparameter choices on robustness. We vary hyperparameters when
fine-tuning a CLIP ViT-B/32 initialized with zero-shot weights on synthetic ImageNet shifts from Section
5.1 (different shades of green). Varying certain hyperparameters (e.g., learning rate, number of epochs) can
affect the effective robustness of pre-trained models even on an in-support shift. In our experiments, we
choose hyperparameters which yield high reference accuracy (purple).
29Published in Transactions on Machine Learning Research (01/2025)
C.1.2 How does the strength of the bias affect robustness to in-support shifts?
In Section 5.1, we consider two in-support shifts under which models might fail due to dataset biases. In
particular, in the spurious tint shift, we introduce a tint that is spuriously correlated with the label in the
reference dataset, but not in the shifted dataset. The probability that an example in the reference dataset
has a class-specific tint (as opposed to a random tint) is determined by a parameter pspurious(set to 0.5for the
experiments in Figure 4). In the label shift, the relative frequencies of classes change between the reference
and shifted datasets. The classes are divided into “majority” and “minority” classes, with “minority” classes
appearing with probability pminorityin the reference dataset (set to 0.2for the experiments in Figure 4). In
the shifted distribution, the relative frequencies of the classes are reversed.
In this section, we investigate how the strength of the bias, i.e., pspuriousandpminority, affects the robustness
of pre-trained models to these in-support shifts. We observe the average effective robustness of pre-trained
models largely remains close to zero as we vary these parameters (see Figure 12).
60 70 80
Reference Accuracy4050607080Shifted Accuracy
ER = 0.34±0.87%pspurious=0.5
60 70 80
Reference Accuracy406080
ER = 0.50±1.11%pspurious=0.6
60 70 80 90
Reference Accuracy406080
ER = 0.84±1.51%pspurious=0.7
70 80 90
Reference Accuracy20406080
ER = 1.66±1.51%pspurious=0.8
70 80 90
Reference Accuracy20406080
ER = 7.23±2.62%pspurious=0.9
Baseline Baseline linear fit y=x CLIP (FT) CLIP (LP-FT) CLIP (ZS-FT) AugReg (FT) AugReg (LP-FT)
(a)Spurioustintshift. Acrossseveraldifferentprobabilitiesofthespuriousclass-specifictint( pspurious), theaverage
effective robustness of pre-trained models (top left of each plot) on the in-support “spurious tint shift” is close to zero.
The one exception is the shift with pspurious = 0.9, where the effective robustness is higher. This may be because
this shift is “close” to an out-of-support shift, since the probability of observing an example with a random tint (as
opposed to a class-specific tint) is low. Hence, pre-training might help by extrapolating better from the small number
of randomly tinted examples.
50 60 70 80
Reference Accuracy20406080Shifted Accuracy
ER = 3.63±2.73%
pminority=0.1
50 60 70 80
Reference Accuracy304050607080
ER = 2.53±1.61%
pminority=0.15
50 60 70 80
Reference Accuracy4050607080
ER = 0.23±0.82%pminority=0.2
50 60 70 80
Reference Accuracy4050607080
ER = 0.30±0.89%
pminority=0.25
50 60 70 80
Reference Accuracy4050607080
ER = 0.31±0.40%
pminority=0.3
Baseline Baseline linear fit y=x CLIP (FT) CLIP (LP-FT) CLIP (ZS-FT) AugReg (FT) AugReg (LP-FT)
(b)Label shift. Across several different probabilities of the minority classes ( pminority), the average effective robust-
ness of pre-trained models (top left of each plot) on the in-support “label shift” is close to zero. We note that in
the shifts with pminority = 0.1andpminority = 0.15, the effective robustness is slightly negative. However, the linear
correlation among baseline models is weak under these shifts, so these effective robustnesses are less meaningful.
Figure 12: The effects of the strength of the bias on robustness to in-support shifts. We vary the
strength of the bias of the two synthetic ImageNet in-support shifts from Section 5.1. Broadly, the effective
robustness of pre-trained models (top left of each plot) is close to zero across bias strengths.
30Published in Transactions on Machine Learning Research (01/2025)
C.2 Dividing natural shifts into in-support and out-of-support splits
C.2.1 Sizes of in-support and out-of-support splits
In Table 1, we report the sizes of the in-support and out-of-support splits we compute for ImageNet-V2,
ImageNet Sketch and ImageNet-R. The out-of-support splits are much larger than the in-support splits,
perhaps because the large majority of the examples from these shifted datasets look unlike examples from
ImageNet.
Table 1: Sizes of in-support and out-of-support splits.
Dataset In-support split size Out-of-support split size
ImageNet-V2 1920 8080
ImageNet Sketch 162 50727
ImageNet-R 588 29412
C.2.2 Examples from in-support and out-of-support splits
In Figure 13, we provide samples from the in-support and out-of-support splits we compute for ImageNet-V2,
ImageNet-Sketch and ImageNet-R.
31Published in Transactions on Machine Learning Research (01/2025)
pedestalImageNet
sulphur
butterfly
broom
 Wire Fox
T errier
Bernese
Mountain Dog
Saharan horned
viper
gyromitra
 kingsnake
 popsicle
 infant bed
great white
sharkV2
In-support
joystick
 cabbage
 English Setter
 wallet
 construction
crane
apron
 Dobermann
 geyser
 wild boar
celloV2
Out-of-support
fox squirrel
 pole
 freight car
 tape player
 frying pan
 tow truck
 plate rack
 eraser
 Bedlington
T errier
feather boaSketch
In-support
grey whale
 steam
locomotive
toilet paper
 couch
 Siamese cat
 aircraft
carrier
cliff dwelling
 through arch
bridge
rotary dial
telephone
football
helmetSketch
Out-of-support
washing
machine
Arctic fox
 snail
 tile roof
 screwdriver
 loggerhead sea
turtle
night snake
 water snake
 plectrum
pigR
In-support
West Highland
White T errier
Border Collie
 castle
 strawberry
 grasshopper
 scorpion
 assault rifle
 scuba diver
 lipstick
scorpionR
Out-of-support
Golden
Retriever
revolver
 clownfish
 violin
 snail
 tree frog
 banana
 soccer ball
 pufferfish
Figure 13: Random samples from ImageNet and from the in-support and out-of-support splits
of ImageNet-V2, ImageNet Sketch and ImageNet-R. In ImageNet-V2, it is difficult to distinguish
between examples from the in-support and out-of-support splits. In ImageNet Sketch and ImageNet-R,
examples from the in-support splits look more realistic (i.e., more like ImageNet examples) than examples
from the out-of-support splits.
C.2.3 Scatter plots of reference vs. shifted accuracy
In Figure 14, we provide scatter plots of accuracy on ImageNet vs. accuracy on the in-support and out-of-
support splits of ImageNet-V2, ImageNet Sketch and ImageNet-R.
32Published in Transactions on Machine Learning Research (01/2025)
70 75 80 85606570758085In-support accuracy
ER = 0.17%ImageNet-V2
70 75 80 855055606570758085
ER = 1.56%ImageNet Sketch
87.5 90 92.5 95 97.55060708090
ER = 1.78%ImageNet-R
70 75 80 85
ImageNet Accuracy55606570758085Out-of-support accuracy
ER = 0.56%
70 75 80 85
ImageNet Accuracy2030405060708090
ER = 12.30%
87.5 90 92.5 95 97.5
ImageNet (class-subsampled) Accuracy2030405060708090
ER = 13.04%
Baseline Baseline linear fit Pre-trained y=x
Figure 14: Reference vs. shifted accuracy for in-support and out-of-support splits of ImageNet
shifts.On each of the three ImageNet shifts we consider, the average effective robustness (ER) of pre-trained
models (orange) above the baseline of models trained from scratch (blue) on the in-support split (top) is
small. Meanwhile, their effective robustness can be very large on the out-of-support split (bottom).
C.2.4 Controlling for difficulty when measuring effective robustness
Thesignificanceofagiveneffectiverobustnessdependsonthe“difficulty”ofadistributionshift. Forexample,
if a shift causes an accuracy drop of 5%, an effective robustness of 4%might be considered large, but if a
shift that causes a drop of 25%, an effective robustness of 4%would probably be considered small. When we
divide a shifted dataset into an in-support and out-of-support split, the out-of-support split is typically more
difficult than the in-support split. If we compare the effective robustness of pre-trained models on examples
of similar difficulty in the in-support and out-of-support splits, do our findings from Section 5.2 still hold?
In particular, do pre-trained models still exhibit substantially higher robustness on out-of-support examples
than on in-support examples?
To answer this question, we re-weight examples in out-of-support splits such that the difficulty distribution
of the out-of-support split matches that of the in-support split. Specifically, we quantify the difficulty of
a given example in terms of the fraction of baseline models (of 77total baseline models) that classify it
incorrectly. Given an example of difficulty d, we re-weight it by a factor of pin-support (d)/pout-of-support (d)
wherepin-support is the difficulty probability density function of the in-support split and pout-of-support is
the difficulty probability density function of the out-of-support split. We then compute a “re-weighted”
accuracy, which in turn yields a re-weighted effective robustness, on the out-of-support split. Intuitively, this
re-weighted effective robustness represents the effective robustness of pre-trained models on out-of-support
examples of similar difficulty to in-support examples.
33Published in Transactions on Machine Learning Research (01/2025)
We report the re-weighted effective robustnesses in Figure 15. We observe that the re-weighted effective
robustnesses of pre-trained models on out-of-support splits are indeed lower than the original effective ro-
bustnesses. However, they are still substantially higher than the effective robustnesses on in-support splits.
ImageNet-V2 ImageNet Sketch ImageNet-R0.000.020.040.060.080.100.120.14Average effective robustness
Out-of-support Out-of-support (re-weighted) In-support
Figure 15: Re-weighed effective robustness of pre-trained models on in-support and out-of-
support splits of ImageNet shifts. When we re-weight examples in out-of-support splits to match the
difficulty distributions of their corresponding in-support splits, the average effective robustnesses of pre-
trained models (green) decrease relative to the original effective robustnesses (blue). However, they are still
very high on ImageNet Sketch and ImageNet-R. Meanwhile, the average effective robustnesses of pre-trained
models on in-support splits (orange) are consistently low.
C.3 Combining pre-training with interventions for handling bias
C.3.1 Studying a synthetic shift
In this section, we provide an additional experiment in a synthetic setting to further illustrate that pre-
training and interventions designed to handle dataset biases can be complementary. In Section 6, we dis-
cussed how robustness to the WILDS-FMoW distribution shift requires both extrapolating to later years
and performing consistently across regions. We construct a synthetic distribution shift using that similarly
requires both extrapolating well and avoiding reliance on spurious features. Specifically, we combine the tint
and pad shifts from Section 5.1. We modify CIFAR-10 such that in the reference distribution, we add a tint
that is spuriously correlated with the label: 80%of reference examples have a class-specific tint while the
remaining 20%are randomly tinted. Meanwhile, in the shifted distribution, examples are always randomly
tinted and are also padded (we add 6black pixels to each side of the original 32×32CIFAR-10 images).
To extrapolate to padded examples, we initialize a CLIP ResNet-50 and perform linear probing followed by
full fine-tuning on the reference distribution. To handle the spurious correlation between tint and label, we
consider the intervention of training on randomly tinted examples, which we refer to as balancing . This is an
“oracle” of sorts for handling dataset biases; it simply modifies the training distribution such that spurious
features are not useful.
As with WILDS-FMoW, we find that pre-training and balancing each yield some effective robustness (see
the left side of Figure 16). In this case, combining the two does not yield the greatest effective robustness,
but does have the highest shifted accuracy. We apply the same methodology as in Section 6 to understand
the robustness benefits of pre-training and balancing. Here, we observe a greater overlap between the
corrected examples of pre-training and balancing than we did for pre-training and DFR in the case of
WILDS-FMoW (see the right side of Figure 16). This may be due to the fact that every example requires
both extrapolation and avoiding reliance on the spurious bias. In other words, the failure modes that pre-
training and balancing are intended to address cooccur. However, we note that there are still many examples
thatarecorrectedbyoneofpre-trainingandbalancing, butnottheother, suggestingcomplementarybenefits.
34Published in Transactions on Machine Learning Research (01/2025)
Similarly to our observations with WILDS-FMoW, combining pre-training with balancing corrects most of
the examples corrected by the individual interventions. These results corroborate our finding that pre-
training and interventions designed to handle dataset biases can be complementary.
Baseline Baseline linear fit y=x Balanced Pre-trained Pre-trained and Balanced94 95 96 97
Reference accuracy7580859095Shifted accuracy
ER=15.23%ER=1.82%ER=11.50%Effective robustnesses
0.7%0.2%
1.6%2.3% 2.6%6.8%Overlap of corrected examples
(% of shifted dataset)
Figure 16: Combining pre-training and balancing on a synthetic CIFAR-10 distribution shift.
Pre-training and balancing (an “oracle” intervention for handling dataset biases) each yield some effective
robustness (ER) and combining these two interventions yields a high effective robustness and the highest
shifted accuracy (left). A substantial number of examples are corrected by one of pre-training and balancing,
but not the other (right), indicating that there are different subpopulations where they improve performance.
Meanwhile, the examples corrected by combining pre-training with balancing include most of the examples
corrected by the individual interventions (right), suggesting that combining pre-training with balancing
improves performance on bothof these subpopulations. Error bars denote 95% confidence intervals over 64
random trials.
C.4 Curating datasets for fine-tuning
C.4.1 Understanding the robustness benefits of pre-training when fine-tuning on a curated dataset
In Section 7, we find that fine-tuning on a curated dataset with only 64examples can yield a performant and
robust model for hair color classification. We observe that pre-training is necessary for effective use of the
small curated dataset; in particular, training a model from scratch on a curated dataset yields robustness
gains, but these gains are smaller and many more examples are required to attain comparable accuracy.
In this section, we shed additional light on how pre-training helps in this setting. Based on our intuition
from Sections 4 and 5 that pre-training helps specifically with extrapolation, we hypothesize that pre-training
provides two benefits when training on a small curated dataset. First, a pre-trained model may be able to
extrapolate better from a small number of examples. This would result in both higher accuracy on the
original CelebA distribution and higher worst-group accuracy, which we observe in Figure 7b. Second, recall
that our curated dataset consists entirely of females, but hair color classification models are expected to
perform well on males too. To compare different model’s ability to extrapolate along this axis, we plot the
balanced accuracy on males against the balanced accuracy on females. In Figure 17, we observe that the
pre-trained model indeed generalizes better to males than models trained from scratch.
35Published in Transactions on Machine Learning Research (01/2025)
78 80 82 84 86 88 90
Female balanced accuracy7075808590Male balanced accuracy
Female balanced vs. male balanced accuracy
y=x
Curated (n=64,,8192)
Curated (n=64,,8192) linear fit
Pre-trained and Curated (n=64)
Figure 17: Comparing extrapolation from females to males of pre-trained models and models
trained from scratch. We plot the balanced accuracy on males against the balanced accuracy of females
of a pre-trained model fine-tuned on the curated dataset from Section 7 (red) and models trained from
scratch on this dataset (green). Models trained from scratch establish a linear relationship between male
and female balanced accuracy; however, the pre-trained model outperforms this trend, suggesting that it
more effectively extrapolates to males from the female-only curated dataset.
C.4.2 Exploring balancing instead of counterfactual editing
In Section 7, we choose to curate a dataset by augmenting images from CelebA with “counterfactual ex-
amples” in which we edit the hair color to the opposite class. We do so in order to de-biasthis dataset as
much as possible. In this section, we explore a simpler approach to curating a dataset: balancing classes.
Similarly to our curated dataset, we constrain this balanced dataset to include only females. As with our
curated dataset, we observe that fine-tuning a pre-trained model on a class-balanced female-only dataset
yields a robust and performant model for hair color classification (see Figure 18a). We also observe again
that pre-training improves over training from scratch by helping with extrapolation from the female-only
reference dataset to males (see Figure 18b).
36Published in Transactions on Machine Learning Research (01/2025)
77.5 8082.5 85 87.5 90 92.5 95
Accuracy30405060708090Worst-group accuracy
ER=5.74%ER=58.04%
Avg. ER=55.58%Accuracy vs. worst-group accuracy
Baseline
Baseline linear fit
y=xPre-trained
Balanced (n=64,,16384)
Pre-trained and Balanced (n=64)
(a)Fine-tuning on a balanced female-only dataset.
Fine-tuning a pre-trained model on the CelebA dataset
(orange) yields little effective robustness over a baseline
of models trained from scratch (blue). However, fine-
tuning the same pre-trained model on just 64examples
from a balanced female-only dataset (red) yields a model
with both high effective robustness and high accuracy.
Training from scratch on a balanced female-only dataset
(green) also yields high effective robustness, but results
in substantially lower accuracy than pre-trained models,
even with many more examples. Error bars denote 95%
confidence intervals over 64random trials.
80 82 84 86 88 90 92
Female balanced accuracy7075808590Male balanced accuracy
Female balanced vs. male balanced accuracy
y=x
Balanced (n=64,,16384)
Balanced (n=64,,16384) linear fit
Pre-trained and Balanced (n=64)(b)Comparing extrapolation from females to
males of pre-trained models and models trained
from scratch. We plot the balanced accuracy on males
against the balanced accuracy of females of a pre-trained
model fine-tuned on a balanced female-only dataset (red)
and models trained from scratch on this dataset (green).
Modelstrainedfromscratchestablishalinearrelationship
between male and femalebalanced accuracy; however, the
pre-trained model outperforms this trend, suggesting that
it more effectively extrapolates to males from the female-
only reference dataset.
Figure 18: Fine-tuning a pre-trained model on a small, non-diverse but de-biased dataset (in this case,
a class-balanced female-only dataset) yields a robust and performant model for hair color classification in
CelebA (see Figure 7b).
37Published in Transactions on Machine Learning Research (01/2025)
D Additional Discussion
D.1 Alternative fine-tuning strategies
In this work, we focus on the common setting in which a pre-trained model is fully fine-tuned. It is important
to note that pre-trained models used in a zero-shot context (i.e., without fine-tuning) and partially fine-tuned
models (e.g., only the final classification layer is updated) are frequently more robust than fully fine-tuned
models (Radford et al., 2021; Miller et al., 2021; Kumar et al., 2022). Such models may have higher effective
robustness than fully fine-tuned models or in some cases may even outperform fully fine-tuned models on
the shifted distribution. However, such models are typically less performant on the reference distribution
than fully fine-tuned models.
Several works observe this tradeoff between performance on the reference distribution and robustness and
devise methods for mitigating it, i.e., methods for robust fine-tuning (Wortsman et al., 2021; Hewitt et al.,
2021; Kumar et al., 2022). For example, Kumar et al. (2022) argue that full fine-tuning “distorts” pre-
trained features and propose linear probing beforefull fine-tuning (LP-FT) to prevent distortion. They also
suggest that fine-tuning a model initialized as a zero-shot classifier may have a similar effect. In addition
to full fine-tuning, in Section 5.1 we thus consider LP-FT and zero-shot initialization for fine-tuning. On
in-support shifts, we observe that LP-FT and zero-shot initialization do not provide effective robustness
benefits compared to full fine-tuning (see Figure 4), suggesting that these strategies do not help mitigate
dataset biases.
Another strategy for robust fine-tuning is to ensemble a zero-shot model and a fully fine-tuned model. Both
weight-space ensembles (Wortsman et al., 2021) and output-space ensembles (Hewitt et al., 2021) have been
shown to improve robustness, sometimes even without sacrificing performance on the reference distribution.
In fact, this strategy can yield robustness benefits even when dataset biases are a primary failure mode
because the zero-shot model is independent of the biased reference dataset. Our work seeks to complement
such empirically effective strategies by providing an understanding of when they are necessary. In particular,
our findings suggest that ensembling is valuable precisely when dataset biases cause failures.
D.2 Can pre-training hurt extrapolation?
In this work, we discuss distribution shifts in which pre-training is beneficial to a model’s ability to ex-
trapolation outside of the reference distribution. A natural question to consider is whether pre-training can
insteadhurtit, yielding worse extrapolation than a model trained from scratch. A recent work by Salman
et al. (2022) suggests that this is indeed possible. Specifically, they show that biases of pre-trained models
can persist during fine-tuning. For example, a model pre-trained on ImageNet and fine-tuned on CIFAR-10
is highly sensitive to the presence of tennis balls (which are an ImageNet class but not a CIFAR-10 class).
Meanwhile, a model trained from scratch on CIFAR-10 is not particularly sensitive to tennis balls. Thus,
under a hypothetical “tennis ball shift” in which tennis balls appear in images in the shifted distribution,
a pre-trained model would be less robust than a model trained from scratch. In this instance, pre-training
provides a harmful prior for how to extrapolate.
D.3 When does pre-training help with extrapolation?
In this work, we provide evidence that pre-training canhelp with extrapolation, but not with other failure
modes. A natural question to consider is whether a particular pre-trained model and fine-tuning strategy
in factdoeshelp with a given extrapolation task. To this end, Ramanujan et al. (2023) explore how the
composition of the pre-training dataset affects robustness on the WILDS-iWildCam distribution shift (Koh
et al., 2020). We consider further exploration of this question to be a valuable direction for future work.
D.4 Relating in-support and out-of-support shifts to existing characterizations
The characterizations relevant in this work, in-support shift andout-of-support shift , overlap with many
existing definitions. Ye et al. (2022) introduce notions of correlation shift anddiversity shift (closely aligned
38Published in Transactions on Machine Learning Research (01/2025)
with in-support and out-of-support shifts, respectively) and provide a method for measuring the “amount”
of each type of shift in a given distribution shift (similar to our method for dividing a distribution shift
into in-support and out-of-support splits). Subpopulation shift (and its sub-types), shifts involving spurious
correlations, covariate shift, and label shift are typically in-support. However, there are exceptions; for
example, some works consider subpopulation shifts in which a subpopulation does not appear in the reference
distribution (Santurkar et al., 2021; Yang et al., 2023), which are out-of-support. Domain generalization
problems are nearly always out-of-support and extrapolating effectively outside of the reference distribution
is often a key challenge of these tasks.
D.5 Understanding the robustness of pre-trained language models to spurious correlations
Tu et al. (2020) study the robustness of pre-trained language models to distribution shifts with spurious
correlations. Their central finding is that pre-training canimprove performance on shifted datasets in which
spurious correlations do not hold. They illustrate that this is because pre-trained models can generalize
better from the small number of counterexamples to these correlations in the reference dataset. This is a
similar phenomenon to our observation from Figure 12a: pre-training can provide some effective robustness
on in-support shifts that are “close” to an out-of-support shift. In cases such as those discussed by Tu et al.
(2020), we hypothesize that pre-training can help to a limited extent by extrapolating better, but cannot
mitigating the underlying failure mode of dataset biases.
D.6 Additional related work
Pre-training. Pre-training a model (or taking an existing pre-trained model) and then fine-tuning it on
a task-specific dataset is a common practice when developing machine learning models, often significantly
improving performance over training a model from scratch (Sharif Razavian et al., 2014; Sun et al., 2017;
Kornblith et al., 2019; Kolesnikov et al., 2019). Pre-training can be effective even when the downstream task
is unrelated to the pre-training task, suggesting that pre-training yields useful general-purpose features; for
example, object classification models trained on ImageNet (Deng et al., 2009) are good initializations for
remote sensing (Xie et al., 2016) and medical imaging (Ke et al., 2021) tasks. Although greatly effective,
pre-training is not without limitations. In some settings, pre-training does not improve performance over a
randomly initialized model trained for long enough (He et al., 2019). Downstream performance can saturate
as performance on the pre-training task improves (Abnar et al., 2021). Finally, biases of pre-trained models
can persist after fine-tuning (Salman et al., 2022).
Distribution shift robustness. Machine learning models are often deployed in different environments
from those in which they are trained. Such distribution shifts can cause models to significantly underperform
(Koh et al., 2020; Gulrajani & Lopez-Paz, 2020; Hendrycks et al., 2020a). Numerous interventions have been
proposed to improve the robustness of models, often targeting particular types of shifts. These include
algorithmic interventions (Arjovsky et al., 2019; Byrd & Lipton, 2019; Sagawa et al., 2020a; Liu et al.,
2021; Kirichenko et al., 2022; Idrissi et al., 2022) (often requiring group information), data augmentations
(Hendrycks et al., 2020a; Goel et al., 2020) and pre-training (discussed below). However, interventions
proposed thus far have failed to provide consistent benefits across distribution shift benchmarks (Koh et al.,
2020; Gulrajani & Lopez-Paz, 2020; Hendrycks et al., 2020a; Wiles et al., 2021; Ye et al., 2022), rendering
distribution shift robustness a persistent challenge.
39