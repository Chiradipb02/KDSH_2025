Under review as submission to TMLR
Federated K-Means Clustering via Dual Decomposition-
based Distributed Optimization
Anonymous authors
Paper under double-blind review
Abstract
The use of distributed optimization in machine learning can be motivated either by the
resulting preservation of privacy or the increase in computational efficiency. On the one
hand, training data might be stored across multiple devices. Training a global model within
a network where each node only has access to its own confidential data requires the use of
distributed algorithms. Even if the data is not confidential, sharing it might be prohibitive
due to bandwidth limitations. On the other hand, the ever increasing amount of available
data leads to large-scale machine learning problems. By splitting the training process across
multiple nodes its efficiency can be significantly increased. This paper demonstrates the ap-
plication of dual decomposition to the distributed training of k-means clustering problems.
After an overview of distributed and federated machine learning, the mixed-integer quadrat-
ically constrained programming-based formulation of the k-means clustering training prob-
lem is presented. The training can be performed in a distributed manner by splitting the
data across different nodes and linking these nodes through consensus-constraints. Finally,
the performance of the subgradient method, the bundle trust method and the quasi-Newton
dual ascent algorithm are evaluated on a set of benchmark problems.
1 Introduction
Training a machine learning model of any kind on a large set of data usually involves the solution of a
challenging optimization problem. If the underlying data set becomes too large, it might not be possible to
solve the resulting optimization problem in a reasonable amount of time. Distributed optimization methods
can aid in rendering the optimization problem tractable through the use of multiple computational resources.
Peteiro-Barral & Guijarro-Berdiñas (2013) provide an overview of methods for distributed machine learning .
Inordertotrainaglobalmodelinadistributedmanneraconsensushastobeestablishedbetweentheinvolved
nodes and their underlying optimization problems. Forero et al. (2010; 2011) and Georgopoulos & Hasler
(2014) demonstrate the distributed training of machine learning models using consensus-based distributed
optimization. Tsianos et al. (2012) discuss practical issues with a consensus-based approach which arise from
the difference between synchronous and asynchronous communication. Nedić (2020) provides an overview
of distributed gradient methods for convex training problems while Verbraeken et al. (2020) give a general
survey of distributed machine learning.
While computational performance still remains an issue for many machine learning problems, the increase
in computing power and in the efficiency of optimization algorithms can render many challenging problems
tractable. However, the inability to share data due to confidentiality reasons still necessitates the use of
distributed algorithms. Fig. 1a shows a setting in which training data is stored across two different nodes.
Each node can use its local data to train an individual machine learning model. By including a coordination
layer the two training processes can be guided in a way that a global model is trained, without the need to
share confidential data. If the underlying optimization problems are still hard to solve, the training process
can be further divided into subproblems. Fig. 1b depicts the situation in which models of different node
clusters are trained in a distributed manner which in turn are again coordinated in order to obtain a global
model. Distributed training of a global model without sharing individual training data is often referred to as
federated optimization or federated learning (Konečn` y et al., 2016). Most algorithms for federated learning
1Under review as submission to TMLR
Data
Model
Data
Model
Coordinator
(a) Simple architecture for federated learning.
Data
Model
Data
Model
Model
Data
Model
Data
Model
Model
Coordinator
(b) Federated learning of node clusters.
Figure 1: Examples of federated learning architectures.
involve an averaging step of the model parameters of the individual nodes (McMahan et al., 2017). Yuan
et al. (2021) propose a dual averaging step in order to handle the nonsmoothness of federated composite
optimization problems. Federated learning methods have been applied in the context of manufacturing
(Hegiste et al., 2022), healthcare (Antunes et al., 2022), mobile devices (Lim et al., 2020) and smart city
sensing (Jiang et al., 2020). Li et al. (2020) and Liu et al. (2022) provide surveys on federated learning
while Chamikara et al. (2021) examine the privacy aspects related to external attacks. Applying federated
learning to heterogeneous data sets can lead to the deterioration of the model quality of individual nodes
in regards to their own training data, which might hinder their willingness to participate in such a setting.
This issue is addressed through personalized federated learning (Kulkarni et al., 2020; Tan et al., 2022).
2K-Means clustering
K-means clustering describes an unsupervised machine learning problem in which a set of observations/data
is divided into Kdisjoint clusters according to a similarity measure (Gambella et al., 2021). Clustering
problems can be found in many practical application such as image segmentation (Dhanachandra et al.,
2015), customer market segmentation (Kansal et al., 2018) or the identification of similar operating points in
2Under review as submission to TMLR
a production plant (Rahimi-Adli et al., 2019). This section presents the mixed-integer programming-based
formulation of the training problem. The formulation is subsequently extended to the case of distributedly
stored data, which gives rise to a federated learning problem. Consensus constraints are used to couple the
training problems of different nodes. These constraints can be dualized such that the federated learning
problem can be solved via dual decomposition-based distributed optimization. Since the underlying opti-
mization problem contains integrality constraints it is not convex and thus strong duality does not hold.
However, a feasible primal solution can be computed in each iteration through an averaging heuristic.
2.1 MIQCP formulation
(a) Clustering of the complete dataset.
(b) Clustering of data-subset 1.
 (c) Clustering of data-subset 2.
Figure 2: Illustration of K-means clustering both in a centralized and a decentralized setting.
The goal of K-means clustering is to assign a set of observations yj∈Rny, j∈J ={1,...,|J|}to a set
of clustersK={1,...,K}and to compute the centroids of each cluster. The number of clusters is a hyper-
parameter and is set a priori or in an iterative manner. This problem can be formulated as a mixed-integer
nonlinear programming (MINLP) problem (Aloise et al., 2012; Gambella et al., 2021),
min
wjk,mk/summationdisplay
j∈J/summationdisplay
k∈Kwjk·∥yj−mk∥2
2, (1a)
s.t./summationdisplay
k∈Kwjk= 1,∀j∈J, (1b)
wjk∈{0,1},∀j∈J,k∈K,mk∈Rny∀k∈K. (1c)
The binary variables wjkindicate if observation yjis assigned to cluster kandmkis the centroid of cluster
k. Constraint (1b) enforces that each observation is assigned to exactly one cluster, while the objective is
to minimize the sum of the squared Euclidean distances of all observations to the centroids of their assigned
clusters. Problem 1 is a nonconvex MINLP which is hard to solve. In practice it is more efficient to use
a linearized formulation by introducing the variable djk, which describes the squared distance between an
3Under review as submission to TMLR
observation jand the centroid of cluster k(Gambella et al., 2021),
min
wjk,djk,mk/summationdisplay
j∈J/summationdisplay
k∈Kdjk (2a)
s.t./summationdisplay
k∈Kwjk= 1,∀j∈J, (2b)
djk≥∥yj−mk∥2
2−Mj·(1−wjk),∀j∈J,k∈K (2c)
wjk∈{0,1},djk≥0,∀j∈J,k∈K,mk∈Rny∀k∈K. (2d)
2 is a mixed-integer quadratically constrained programming (MIQCP) problem with a convex integer re-
laxation. Constraint (2c) is an epigraph formulation of the squared Euclidean distance if observation jis
assigned to cluster k, i.e., when wjk= 1. Otherwise, the parameter Mjhas to be large enough so that
the constraint is trivially satisfied for wjk= 0. In theory a common big-M parameter can be used for all
constraints described by (2c). However, the parameter should be chosen as small as possible in order to
avoid weak integer relaxations. In the following the big-M parameter is set as
Mj= max
χ∈Y∥yj−χ∥2
2,∀j∈J, (3a)
Y={y∈Rny|min
j∈J[yj]l≤[y]l≤max
j∈J[yj]l, l= 1....,n y}. (3b)
Different approaches have been proposed to solve the clustering optimization problem. Bagirov & Yearwood
(2006) present a heuristic method based on nonsmooth optimization, Aloise et al. (2012) propose a column
generation algorithm and Karmitsa et al. (2017) use a diagonal bundle method. Fig. 2a illustrates the
concept ofK-means clustering. The unlabeled data (left) is split into 3 clusters according to their distance
to the computed cluster centroid (crosses).
2.2 Distributed consensus formulation
Problem 2 describes the case in which the entire data set is accessible from a single node. However, this might
not always be the case, especially if the underlying data is confidential. In the following it is assumed that
the data set is split across several nodes I={1,...,Ns}, with each node having access to the data-subset
Ji⊂J. The MIQCP problem 2 can be extended to the case of multiple nodes,
min
wijk,dijk,mk/summationdisplay
i∈I/summationdisplay
j∈Ji/summationdisplay
k∈Kdijk (4a)
s.t./summationdisplay
k∈Kwijk= 1,∀i∈I,j∈Ji, (4b)
dijk≥∥yj−mk∥2
2−Mj·(1−wijk),∀i∈I,j∈Ji,k∈K, (4c)
wijk∈{0,1},dijk≥0,∀i∈I,j∈Ji,k∈K,mk∈Rny∀,k∈K. (4d)
The goal of problem 4 is again to compute a set of cluster centroids mkand to assign the observations of
all nodes to these clusters. However, if the nodes cannot share their data, problem 4 cannot be solved in a
centralized manner. A simple distributed approach would be to solve a clustering problem in each node i.
This could lead to situation as depicted in Fig. 2b and Fig. 2c. If each the data-set is split across two nodes,
each one can solve a clustering problem. However, both nodes will compute different cluster centroids.
The goal of a federated learning approach is to train a global model, i.e., global cluster centroids in the
case ofK-means clustering, without sharing the local data between the nodes. To this end each node ican
compute individual cluster centroids mik,
min
wijk,dijk,mik/summationdisplay
i∈I/summationdisplay
j∈Ji/summationdisplay
k∈Kdijk (5a)
s.t./summationdisplay
k∈Kwijk= 1,∀i∈I,j∈Ji, (5b)
4Under review as submission to TMLR
dijk≥∥yj−mik∥2
2−Mj·(1−wijk),∀i∈I,j∈Ji,k∈K, (5c)
mik=mi′k,∀i∈I,i′∈Ni,k∈K, (5d)
wijk∈{0,1},dijk≥0,∀i∈I,j∈Ji,k∈K,mik∈Rny∀,i∈I,k∈K. (5e)
Since the goal is to obtain global cluster centroids, the individual cluster centroids are coupled through
consensus constraints (5d), where Nicontains the set of neighboring nodes of node i. Problem 5 describes a
set ofNssubproblems coupled through the consensus constraints. In the following subsection dual variables
are used in order to decouple the clustering problems of the different nodes.
3 Dual decomposition-based distributed clustering
This section presents how the consensus formulation (5) of the clustering problem can be decomposed by
introducing dual variables. Dual decomposition can be applied to constraint-coupled optimization problems
of the form
min
x/summationdisplay
i∈Ifi(xi), (6a)
s.t./summationdisplay
i∈IAixi=b, (6b)
xi∈X. (6c)
Equation (6) describes an optimization problem consisting of of a set of I={1,...,Ns}subproblems. The
subproblems are coupled through the constraints (6b) and each one is described by individual variables
xiand constraintsXi. Dual decomposition is based on the introduction of dual variables for the coupling
constraints (6b) and on the solution of the resulting dual optimization problem. The idea was first introduced
by Everett (1963) for problems involving shared limited resources. Problem 5 can also be rewritten as a
generalconstraint-coupledoptimizationproblembydefiningthematrix Adescribingtheconnectionsbetween
the different nodes. In the following only linear network topologies as depicted in Fig. 3 are considered. Note
that the discussion in the remainder of this paper can be easily extended to different network topologies.
1
2
3
Ns
ˆm1=ˆm2ˆm
2=
ˆm
3
Figure 3: Illustration of a linear network topology and the resulting consensus constraints.
By defining the vector of stacked cluster centroids of each node i,
ˆmi:=
mi,1
...
mi,k
∈RK·ny, (7)
the consensus constraints can be rewritten as
ˆm1−ˆm2=0, (8a)
ˆm2−ˆm3=0, (8b)
...
5Under review as submission to TMLR
ˆmNs−1−ˆmNs=0. (8c)
Constraints 8 can subsequently be rewritten in matrix form

I−I 0···0 0
0 I−I···0 0
..................
0 0 0···I−I

/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
=:A∈RK·ny·(Ns−1)×K·ny·Ns·
ˆm1
ˆm2
ˆm3
...
ˆmNs
=0, (9a)
or in a more compact way/summationdisplay
i∈IAiˆmi=0 (10)
withAi∈RK·ny·(Ns−1)×K·ny. By introducing dual variables λ∈RnK·ny·(Ns−1)for the consensus constraints
10 the Lagrange function of problem (5) can be defined,
L(wijk,dijk,mik,λ) =/summationdisplay
i∈I
/summationdisplay
j∈Ji/summationdisplay
k∈Kdijk+λTAiˆmi

/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
=:Li(wijk,dijk,mik,λ). (11)
The minimization of the Lagrange function for a fixed value of the dual variables λgives the corresponding
value of the dual function.
d(λ):= min
wijk,dijk,mik/summationdisplay
i∈ILi(wijk,dijk,mik,λ) (12a)
s.t./summationdisplay
k∈Kwijk= 1,∀i∈I,j∈Ji, (12b)
dijk≥∥yj−mik∥2
2−Mj·(1−wijk),∀i∈I,j∈Ji,k∈K, (12c)
wijk∈{0,1},dijk≥0,∀i∈I,j∈Ji,k∈K,
mik∈Rny∀,i∈I,k∈K. (12d)
The dual function has two important properties. First, the value of the dual function is always a lower
bound on the solution of its corresponding primal problem, in this case, problem (5) (Nocedal & Wright,
2006). The problem of finding the dual variables that result in the best lower bound is referred to as the
dual optimization problem,
max
λd(λ). (13)
The resulting dual problem can be solved in a distributed manner by solving the individual clustering
problems for the current value of the dual variables,
min
wijk,dijk,mikLi(wijk,dijk,mik,λ) (14a)
s.t./summationdisplay
k∈Kwijk= 1,∀j∈Ji, (14b)
dijk≥∥yj−mik∥2
2−Mj·(1−wijk),∀j∈Ji,k∈K, (14c)
wijk∈{0,1},dijk≥0,∀j∈Ji,k∈K,
mik∈Rny∀k∈K. (14d)
Second, the dual function (12) is always concave, regardless whether the primal problem is convex or not
(Nocedal & Wright, 2006). Therefore the dual problem (13) is a convex optimization problem. However, the
dual function is usually nondifferentiable due to a changing set of active individual constraints, which means
that problem (13) is a nonsmooth optimization problem (Yfantis et al., 2023). The following subsections
present some algorithms for the solution of the dual problem, namely the subgradient method, the bundle
trust method, and the quasi-Newton dual ascent algorithm.
6Under review as submission to TMLR
3.1 Subgradient method
Since the dual function is nondifferentiable a gradient cannot be defined for every value of the dual variables.
Instead, a subgradient can be used. A vector ξ∈Rnχis a subgradient of a concave function ϕ(χ)at a point
χ0if
ϕ(χ)≤ϕ(χ0) +ξT(χ−χ) (15)
for all χ∈domϕ. The set of all subgradients at a point χ0comprise the subdifferential ∂ϕ(χ0)Technically
equation (15) defines a supergradient. Nevertheless, the term subgradient is commonly used in the literature
for both convex and concave functions.
A subgradient of the dual function for a given value of the dual variables λ(t)can be computed by evaluating
the coupling constraints (10),
g(λ(t)) =/summationdisplay
i∈IAiˆmi(λ(t))∈∂d(λ(t)), (16)
where ˆmi(λ(t))are the cluster centroids obtained by solving the individual clustering problems (14).
In the subgradient method the dual variables are updated in each iteration talong the direction of the
subgradient (Shor, 2012)
λ(t+1)=λ(t)+α(t)g(λ(t)), (17)
whereα(t)is a step size parameter. The step size parameter plays an important role in the convergence of the
algorithm. If it is chosen too large the algorithm might diverge, while a too small choice might significantly
slow down its convergence. A common choice to adapt the step size over the course of the iterations is
α(t)=α(0)/√
t, (18)
with an initial step size α(0)(Bertsekas, 1999).
3.2 Bundle trust method
The subgradient method usually exhibits a slow rate of convergence, since only using information from the
current subgradient may not provide an ascent direction for the algorithm. Bundle methods are generally
more efficient by utilizing multiple subgradients from previous iterations (Mäkelä, 2002). To this end the
data
B(t)={(λ(l),g(λ(l)),d(λ(l)))∈Rnλ×Rnλ×R|l=t−τ+ 1,...,t} (19)
is stored in each iteration, where nλdenotes the number of dual variables. B(t)is referred to as a bundle and
it contains the dual variables, subgradients and values of the dual function from previous iterations. Since
storing all information from all previous iterations might cause memory issues, only data from the previous
τiterations is used.
The idea of bundle methods is to use the collected information to construct a piece wise linear over approx-
imation of the nonsmooth dual function d(λ), a so-called cutting plane model,
ˆd(t)(λ):= min
l∈{t−τ+1,...,t}{d(λ(l)) +gT(λ(l))(λ−λ(l))}. (20)
The approximation can be written in an equivalent form as
ˆd(t)(λ) = min
l∈{t−τ+1,...,t}{d(λ(t)) +gT(λ(l))(λ−λ(t))−β(l,t)}, (21)
with the linearization error
β(l,t)=d(λ(t))−d(λ(l))−gT(λ(l))(λ(t)−λ(l)),∀l∈{t−τ+ 1,...,t}. (22)
The update direction of the dual variables can then be computed by solving a direction finding problem
max
s∈Rnλˆd(t)(λ(t)+s), (23a)
s. t.∥s∥2
2≤α(t), (23b)
7Under review as submission to TMLR
(23c)
where constraint (23b) represents a trust region. Therefore, this variant of the bundle method is referred
to as bundle trust method (BTM). Other variants include proximal bundle methods, where the trust region
is replaced by a regularization term in the objective function (Bagirov et al., 2014). Problem (23) is still a
nonsmooth optimization problem and can be transformed into a smooth quadratic direction finding problem
by using an epigraph formulation,
max
v∈R,s∈Rnλv, (24a)
s. t.∥s∥2
2≤α(t), (24b)
gT(λ(l))s−β(l,t)≥v,∀l∈{t−τ+ 1,...,t}. (24c)
After computing a direction the dual variables are updated according to
λ(t+1)=λ(t)+s(t). (25)
Bundle methods are widely used in machine learning, as nonsmoothnes is encountered in many training
problems involving regularization terms (Le et al., 2007). Bundle methods can also be used to solve the
clustering problem (2) (Karmitsa et al., 2017). However, note that in this paper the BTM algorithm is used
to solve the nonsmooth dual problem (13).
3.3 Quasi-Newton dual ascent
Since the dual function is always concave it can be locally approximated by a quadratic function. Yfantis
& Ruskowski (2022) and Yfantis et al. (2023) recently proposed the quasi-Newton dual ascent (QNDA)
algorithm that approximates the dual function by a quadratic function,
d(t)
B(λ) =1
2(λ−λ(t))TB(t)(λ−λ(t)) +gT(λ(k))(λ−λ(t)) +d(λ(t)). (26)
This follows the idea of Newton methods, where the gradient and Hessian of the function are used within the
approximation. However, due to the nonsmoothness of the dual function, the gradient and Hessian are not
defined for each value of the dual variable. Instead, the gradient is replaced in eq. (26) by the subgradient
and the Hessian is approximated by the matrix B(t). The approximated Hessian can be updated in each
iteration using a Broyden-Fletcher-Goldfarb-Shanno (BFGS) update,
B(t)=B(t−1)+y(t)y(t),T
y(t),Ts(t)−B(t−1)s(t)s(t),TB(t−1),T
s(t),TB(t−1)s(t), (27)
where
s(t):=λ(t)−λ(t−1)(28)
is the variation of the dual variables and
y(t)=g(λ(t))−g(λ(t−1)) (29)
is the variation of the subgradients.
The approximated dual function dB(λ)is differentiable, while the actual dual function is nonsmooth. This
can lead to significant approximation errors and poor update directions. This issue can be addressed by
utilizing the same information as in the BTM algorithm. However, instead of using the bundle to construct
an over approximator of the dual function, it is used to further constrain the update of the dual variables,
d(t)
B(λ(t+1))≤d(λ(l)) +gT(λ(l))(λ(t+1)−λ(l)),∀l∈{t−τ+ 1,...,t}. (30)
Constraints (30) are derived from the definition of the subgradient (15). A violation of these constraints
would indicate that the updated dual variables λ(t+1)are outside the range of validity of the approximated
dual function. These constraints are referred to as bundle cuts and they can be summarized as
BC(t)={λ∈Rnb|d(t)
B(λ)≤d(λ(l)) +gT(λ(l))(λ−λ(l)),∀l∈{t−τ+ 1,...,t}}. (31)
8Under review as submission to TMLR
In the QNDA algorithm the dual variables are updated in each iteration by solving the optimization problem
λ(t+1)=argmax
λd(t)
B(λ), (32a)
s.t.∥λ−λ(t)∥2
2≤α(t), (32b)
λ∈BC(t). (32c)
(32d)
To avoid too aggressive update steps the same trust region (32b) as in the BTM algorithm is used.
3.4 Primal heuristics
The following sections provide some additional heuristics related to the primal optimization problem (5),
namelyanaveragingheuristicusedtoobtainfeasibleprimalsolutions, andtheadditionofsymmetrybreaking
constraints to the clustering problem.
3.4.1 Averaging heuristic
TheK-means clustering problem involves integrality constraints and is therefore nonconvex. While the
(optimal) value of the dual function 12 provides a lower bound on the optimal value of the primal problem 5,
feasibilityoftheprimalproblemisnotguaranteeduponconvergenceofadualdecomposition-basedalgorithm,
i.e., the consensus constraints may not be satisfied. Nevertheless, in the case of K-means clustering it is
straightforward to compute a feasible primal solution using an averaging step. In each iteration tof a dual
decomposition-based algorithm the coordinator communicates the dual variables λ(t)to the nodes. The
nodes in turn solve their individual clustering problems and communicate their computed cluster centroids
ˆmi(λ(t))to the coordinator. Based on this response the coordinator can compute the average of the primal
variables, i.e., the average cluster centroids,
mk(λ(t)) =1
Ns/summationdisplay
i∈Imik(λ(t)) (33)
which are then communicated back to the nodes. Using the mean cluster centroids the nodes can compute
their resulting primal objective value
zi(λ(t)) =/summationdisplay
j∈Jmin
k∈K∥yj−mk(λ(t))∥2
2. (34)
The primal objective value can be used to compute the relative duality gap in each iteration,
rel. DG = 100·/parenleftbigg
1−d(λ(t))/summationtext
i∈Izi(λ(t))/parenrightbigg
. (35)
Since the value of the dual function provides a lower bound on the optimal primal objective value the
relative duality gap can be used to assess the distance of a found solution to the global optimum. The entire
communication process between the coordinator and the nodes is illustrated in Fig. 4. Note that the average
cluster centroids are only used to compute the duality gap. They do not influence the update of the dual
variables.
3.4.2 Symmetry breaking constraints
The clustering problem 4 is highly symmetric, i.e., it contains solutions with the same objective values. This
is due to the fact that the index assigned to a cluster does not influence the objective function. Fig. 5
illustrates the situation of two symmetric solutions. This symmetry can lead to problems for the averaging
heuristic presented in the previous section, as the computed cluster centroids of a single node can switch
from one iteration to the next. For instance, while some points are assigned to cluster kin iteration t, they
9Under review as submission to TMLR
Coordinator
λ(t)λ(t)
(a) The coordinator sends the current dual variables to the
nodes.ˆm2(λ(t)) ˆm1(λ(t))
Coordinator
(b) The nodes compute their cluster centroids and send
them to the coordinator.
m(λ(t)) m(λ(t))
Coordinator
(c) The coordinator computes the average cluster centroids
and sends them to the nodes.z2(λ(t)) z1(λ(t))
Coordinator
(d) The nodes compute their objectives based on the re-
ceived average centroids.
Figure 4: Communication process between the coordinator and the nodes in iteration t.
Figure 5: Example of symmetric clustering solutions. In the two cases the data points are assigned to
different clusters without affecting the objective function.
could be assigned to cluster k′in iteration t+ 1by switching the centroids of clusters kandk′without
affecting the objective.
In order to prevent this behavior symmetry breaking constraints are added to the optimization problems of
the nodes. In the first iteration one of the nodes acts as the reference nodes, providing reference centroids
mref
k. In the subsequent iterations the quadratic constraint
∥mik−mref
k∥2
2≤∥mik′−mref
k∥2
2,∀k,k′∈K, (36)
is added to each node i. This ensures that cluster kof each node iwill be the one closest to the reference
centroid mref
k. The choice of the node which provides the reference centroid can be performed arbitrarily,
as it does not affect the optimization of the other nodes. Furthermore, the added constraint also does not
affect the optimal objective value while rendering all symmetric solutions, except for one, infeasible.
10Under review as submission to TMLR
4 Numerical analysis of distributed clustering problems
The dual decomposition-based distributed clustering approach was evaluated on a set of benchmark problems
ofvaryingsize. Thedataforeachbenchmarkproblemwasgeneratedrandomly. First, initialclustercentroids
m0
kwere generated, with [m0
k]l∈Uc(−1,1), l= 1,...,n y. Then, for each cluster kfive random data points
where added within a radius of 0.5 from the generated centroid. The parameters of the benchmark problems
were varied as follows:
Number of nodes: Ns∈{2,3,4},
Number of dimensions: ny∈{2,3,4},
Number of clusters: K∈{3,4}.
Fivebenchmarkproblemsweregeneratedforeachcombinationofnodes, dimensionsandclusters, resultingin
a total of 90 benchmark problems. A benchmark problem is characterized by its number of nodes, dimension
ofthe dataandnumber ofclusters. Forinstance, problem3N2D4K 5isthe 5thbenchmarkproblemcomprised
of 3 nodes with 2-dimensional data sorted into 4 clusters.
The benchmark problems were solved using the subgradient method, the bundle trust method and the
quasi-Newton dual ascent algorithm. The use of ADMM was omitted for several reasons. First, in each
communication round a feasible primal solution is obtained through the averaging heuristic (cf. Section
3.4.1). Thisprimalsolutiondoesnotcorrespondtothecurrentdualvariables. Duetothenonconvexityofthe
underlyingMIPproblemnoguaranteecanbemadethattheconsensusconstraintswillbesatisfiedatthedual
optimum. This in turn means that the regularization term in ADMM might not vanish, which would result
in different objective values of the Lagrangian and the augmented Lagrangian, leading to an overestimation
of the dual value and a subsequent underestimation of the duality gap, Second, the regularization of ADMM
introduces a bias towards the mean cluster centroids. Note that the averaging heuristic does not affect the
iterations of the other distributed optimization algorithms. It merely serves to compute a feasible primal
solution, i.e., an upper objective bound, in each iteration. Introducing a bias towards the mean centroids in
the solution of the clustering problems of the nodes would result in a stagnation of the algorithm. For badly
chosen regularization parameters all nodes would converge towards the initial mean centroids, which is not
the case in general for the other algorithms. The QADA algorithm could also be used to solve the clustering
problem. However, the numerical tests showed that the BTM and QNDA algorithms are already efficient
enough to converge within the sampling phase of QADA. Its inclusion in the results was therefore omitted.
Table 1: Parameter settings of the distributed optimization algorithms for the clustering benchmark prob-
lems.
Value Description Algorithms
λ(0)0 initial dual variables All
α(0)0.5initial step size/trust region parameter All
tmax 150 maximum number of iterations All
ϵp 10−2primal residual convergence tolerance All
ϵDG 0.25 %relative duality gap tolerance All
ϵb1 bundle cuts threshold QNDA
τ 50 allowed age of data points BTM, QNDA
B(0)−Iinitial approximated Hessian QNDA
The initial step size (SG)/ trust region (BTM, QNDA) parameter was set to α(0)= 0.5and varied according
to
α(t)=α(0)/√
t. (37)
The bundle cuts for QNDA were used in every iteration, i.e., ϵb= 1andτ= 50points were used to construct
the bundle in BTM and the bundle cuts in QNDA respectively. All algorithms were initialized with λ(0)=0
and the initial approximated Hessian of the QNDA algorithm was set to the negative identity matrix. The
11Under review as submission to TMLR
algorithms were terminated either when the Euclidean norm of the primal residual
∥wp∥2=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/summationdisplay
i∈IAiˆmi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2, (38)
i.e., the violation of the consensus constraints lied below a threshold of ϵp= 10−2or when the relative duality
gap 35 reached a value of ϵDG= 0.25 %. The used parameters for the different algorithms are summarized
in Tab. 1. The MIQCP clustering problems of all nodes were solved using the commercial solver Gurobi and
the total computation time was obtained through eq. ??, withTcomm = 800ms.
The results for the clustering benchmarks are summarized in Tab. 2.
Table 2: Summary of the results for the distributed optimization of the clustering benchmark problems, t:
mean number of iterations until termination, rel. DG: mean relative duality gap upon termination (in %),
Tcomp: mean computation time (in s).
Algorithm trel. DG Tcomp
SG 136.75 2.27 996.28
BTM 57.44 1.86 515.77
QNDA 54.48 1.81 483.22
Out of the examined algorithms, QNDA shows the best performance in terms of the required number of
iterations and computation time as well as in terms of the achieved relative duality gap. The BTM algorithm
shows a similar performance in terms of the number of iterations and the achieved duality gap. However, in
the case of distributed clustering each iteration is costly due to the underlying MIQCP problems. Therefore,
a slight performance increase in the number of iterations results in a more substantial performance increase
in terms of computation times. More detailed results for the clustering benchmarks are summarized in Tab. 3
in the appendix.
Iterations0 50 100
0510rel. DG [%]SG
BTM
QNDA
Figure 6: Evolution of the relative duality gap for benchmark problem 2N2D4K 3.
Fig. 6 shows the evolution of the relative duality gap for benchmark problem 2N2D4K 3. The subgradient
method converges rather slowly. In comparison, the BTM and QNDA algorithms exhibit a faster rate of
convergence. Between these two algorithms, BTM exhibits an oscillatory behavior before converging. In
contrast, the QNDA algorithm does not exhibit oscillations and therefore converges earlier. Additionally,
it should be noted that the QNDA algorithm achieves a relative duality gap of 0 %, i.e., it converges to a
proven global optimum.
12Under review as submission to TMLR
[y]1-0.5 0.0 0.5 1.0
-1.0-0.50.00.51.0[y]2
(a) Node 1, Iteration 1
[y]1-0.5 0.0 0.5 1.0
-1.0-0.50.00.51.0[y]2
(b) Node 2, Iteration 1
[y]1-0.5 0.0 0.5 1.0
-1.0-0.50.00.51.0[y]2
(c) Node 1, Iteration 4
[y]1-0.5 0.0 0.5 1.0
-1.0-0.50.00.51.0[y]2
(d) Node 2, Iteration 4
Figure 7: Exemplary clusters in different iterations of the QNDA algorithm for benchmark problem
2N2D4K 3.
Fig. 7 provides some further illustrations of the results. Fig. 7a and Fig. 7b show the results of the clustering
in the first iteration, i.e., the individual global optima. Fig. 7c and Fig. 7d depict the solutions upon
convergence of the QNDA algorithm. It can be seen that each node computes the same cluster centroids
corresponding to the globally optimal solution with respect to the entire data set, but not to the individual
data sets. It is therefore possible to compute a global model locally in each node while only accessing local
data.
5 Comparison to the central solution
As shown in the previous section, the solution of the MIQCP clustering problems is computationally ex-
pensive. This is due to the weak integer relaxation of problem 2, which means that the solution of the
relaxed problem within the branch-and-bound algorithm is far away from the integer solution. This results
in slow moving relative integrality gaps and to slow convergence of the solution algorithm. While the main
motivation of the distributed clustering approach is the training of a global model without exchange of local
data, it can also be used to efficiently solve larger clustering problems. Fig. 8 depicts the evolution of the
relative duality gap of the QNDA algorithm as well as the evolution of the relative integrality gap of Gurobi
for the complete data set of benchmark problem 4N4D4K 3. The clustering problems of the individual nodes
were solved in a sequential manner in the case of QNDA, also using Gurobi. While the relative gap of the
central solution improves very slowly, the QNDA algorithm quickly converges to a solution close to the global
13Under review as submission to TMLR
Time [s]0 200 400 600 800 1000 1200
10−1100101102rel. Gap [%]Central
QNDA
Figure 8: Evolution of the relative duality gap of QNDA compared to the relative integrality gap of the
central solution using Gurobi for benchmark problem 4N4D4K 3.
optimum. Note, that both relative gaps prove a worst-case distance to the global optimum. Hence, decom-
posing a large clustering problem into smaller subproblems and coordinating the solutions via a distributed
optimization algorithm can offer significant performance improvements compared to a central solution.
6 Conclusion
This paper demonstrated how dual decomposition-based distributed optimization can be applied to the
solution of clustering problems. The approach ensures privacy, i.e., enables federated learning, as each node
only has access to its local data. A global model can still be obtained by coordinating the solutions of
the individual clustering problems. Numerical tests on a large set of benchmark problems demonstrated
that the QNDA algorithm outperforms the subgradient method and the BTM algorithm. Furthermore, the
distributed optimization approach exhibited superior performance compared to a central solution approach.
In the future the developed algorithms can also be applied to other federated learning problems, like the
distributed training of support vector machines.
References
D. Aloise, P. Hansen, and L. Liberti. An improved column generation algorithm for minimum sum-of-squares
clustering. Mathematical Programming , 131:195–220, 2012.
R.S. Antunes, C. André da Costa, A. Küderle, I.A. Yari, and B. Eskofier. Federated learning for healthcare:
Systematic review and architecture proposal. ACM Transactions on Intelligent Systems and Technology
(TIST), 13(4):1–23, 2022.
A. Bagirov, N. Karmitsa, and M. Mäkelä. Introduction to Nonsmooth Optimization: Theory, Practice and
Software. Springer, 2014.
A.M Bagirov and J. Yearwood. A new nonsmooth optimization algorithm for minimum sum-of-squares
clustering problems. European Journal of Operational Research , 170(2):578–596, 2006.
D. P. Bertsekas. Nonlinear programming . Athena Scientific, 1999.
M.A.P. Chamikara, P. Bertok, I. Khalil, D. Liu, and S. Camtepe. Privacy preserving distributed machine
learning with federated learning. Computer Communications , 171:112–125, 2021.
N. Dhanachandra, K. Manglem, and Y.J. Chanu. Image segmentation using k-means clustering algorithm
and subtractive clustering algorithm. Procedia Computer Science , 54:764–771, 2015.
14Under review as submission to TMLR
H. Everett. Generalized Lagrange multiplier method for solving problems of optimum allocation of resources.
Operations Research , (11 (3)):399–417, 1963.
P.A. Forero, A. Cano, and G.B Giannakis. Consensus-based distributed support vector machines. Journal
of Machine Learning Research , 11(5), 2010.
P.A. Forero, A. Cano, and G.B. Giannakis. Distributed clustering using wireless sensor networks. IEEE
Journal of Selected Topics in Signal Processing , 5(4):707–724, 2011.
C. Gambella, B. Ghaddar, and J. Naoum-Sawaya. Optimization problems for machine learning: A survey.
European Journal of Operational Research , 290(3):807–828, 2021.
L. Georgopoulos and M. Hasler. Distributed machine learning in networks by consensus. Neurocomputing ,
124:2–12, 2014.
V. Hegiste, T. Legler, and M. Ruskowski. Application of federated machine learning in manufacturing. In
2022 International Conference on Industry 4.0 Technology (I4Tech) , pp. 1–8. IEEE, 2022.
J.C. Jiang, B. Kantarci, S. Oktug, and T. Soyata. Federated learning in smart city sensing: Challenges and
opportunities. Sensors, 20(21):6230, 2020.
T. Kansal, S. Bahuguna, V. Singh, and T. Choudhury. Customer segmentation using k-means clustering. In
International Conference on Computational Techniques, Electronics and Mechanical Systems (CTEMS) ,
pp. 135–139. IEEE, 2018.
N. Karmitsa, A.M. Bagirov, and S. Taheri. New diagonal bundle method for clustering problems in large
data sets. European Journal of Operational Research , 263(2):367–379, 2017.
J. Konečn` y, B. McMahan, D. Ramage, and P. Richtárik. Federated optimization: Distributed machine
learning for on-device intelligence. arXiv preprint arXiv:1610.02527 , 2016.
V. Kulkarni, M. Kulkarni, and A. Pant. Survey of personalization techniques for federated learning. In
4th World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4) , pp. 794–797.
IEEE, 2020.
Q. Le, A. Smola, and S. Vishwanathan. Bundle methods for machine learning. Advances in Neural Infor-
mation Processing Systems , 20, 2007.
L. Li, Y. Fan, M. Tse, and K.-Y. Lin. A review of applications in federated learning. Computers & Industrial
Engineering , 149:106854, 2020.
W.Y.B. Lim, N.C. Luong, D.T. Hoang, Y. Jiao, Y.-C. Liang, Q. Yang, D. Niyato, and C. Miao. Federated
learning in mobile edge networks: A comprehensive survey. IEEE Communications Surveys & Tutorials ,
22(3):2031–2063, 2020.
J. Liu, J. Huang, Y. Zhou, X. Li, S. Ji, H. Xiong, and D. Dou. From distributed machine learning to
federated learning: A survey. Knowledge and Information Systems , 64(4):885–917, 2022.
M. Mäkelä. Survey of bundle methods for nonsmooth optimization. Optimization Methods and Software , 17
(1):1–29, 2002. ISSN 1055-6788. doi: 10.1080/10556780290027828.
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of
deep networks from decentralized data. In Artificial Intelligence and Statistics , pp. 1273–1282. PMLR,
2017.
A. Nedić. Distributed gradient methods for convex machine learning problems in networks: Distributed
optimization. IEEE Signal Processing Magazine , 37(3):92–101, 2020.
J. Nocedal and S. Wright. Numerical optimization . Springer Science & Business Media, 2006.
15Under review as submission to TMLR
D. Peteiro-Barral and B. Guijarro-Berdiñas. A survey of methods for distributed machine learning. Progress
in Artificial Intelligence , 2:1–11, 2013.
K. Rahimi-Adli, P.D. Schiermoch, B. Beisheim, S. Wenzel, and S. Engell. A model identification approach
for the evaluation of plant efficiency. In Computer Aided Chemical Engineering , volume 46, pp. 913–918.
Elsevier, 2019.
N.Z. Shor. Minimization methods for non-differentiable functions , volume 3. Springer Science & Business
Media, 2012.
A.Z. Tan, H. Yu, L. Cui, and Q. Yang. Towards personalized federated learning. IEEE Transactions on
Neural Networks and Learning Systems , 2022.
K.I. Tsianos, S. Lawlor, and M.G. Rabbat. Consensus-based distributed optimization: Practical issues and
applications in large-scale machine learning. In 50th Annual Allerton Conference on Communication,
Control, and Computing , pp. 1543–1550. IEEE, 2012.
J. Verbraeken, M. Wolting, J. Katzy, J. Kloppenburg, T. Verbelen, and J.S. Rellermeyer. A survey on
distributed machine learning. ACM Computing Surveys , 53(2):1–33, 2020.
V. Yfantis and M. Ruskowski. A hierarchical dual decomposition-based distributed optimization algorithm
combining quasi-Newton steps and bundle methods. In 30th Mediterranean Conference on Control and
Automation (MED) , pp. 31–36. IEEE, 2022.
V. Yfantis, S. Wenzel, A. Wagner, M. Ruskowski, and S. Engell. Hierarchical distributed optimization of
constraint-coupled convex and mixed-integer programs using approximations of the dual function. EURO
Journal on Computational Optimization , 11:100058, 2023.
H. Yuan, M. Zaheer, and S. Reddi. Federated composite optimization. In International Conference on
Machine Learning , pp. 12253–12266. PMLR, 2021.
A Results for the clustering benchmark problems
Table 3: Results for the distributed optimization of the clustering benchmark problems, t: mean number of
performed iterations, rel. DG: mean relative duality gap (in %), Tcomp: mean computation time (in s).
SG BTM
Clustering trel. DG Tcomptrel. DG Tcomp
Mean 136.75 2.27 996.2857.44 1.86 515.77
2N2D3K 126.0 1.94 166.08 68.0 1.84 95.01
2N2D4K 113.2 0.89 431.02 64.4 0.8 348.56
2N3D3K 120.0 1.8 223.69 71.6 1.6 167.72
2N3D4K 115.6 0.27 782.18 13.6 0.1 93.92
2N4D3K 91.6 0.31 184.21 36.2 0.16 108 .0
2N4D4K 90.4 0.25 965.26 8.6 0.08 93.7
3N2D3K 138.4 7.01 404.59 123.2 6.14 424.47
3N2D4K 150.0 4.7 879.48 62.8 3.99 751.33
3N3D3K 150.0 2.33 301.63 67.0 1.76 160.05
3N3D4K 150.0 0.82 1469 .97 66.6 0.35 906.26
3N4D3K 150.0 2.07 354.48 37.2 1.63 110.82
3N4D4K 150.0 1.06 3295 .09 37.8 0.56 820.42
4N2D3K 150.0 5.01 311.78 103.2 3.66 262.33
4N2D4K 150.0 7.58 1319 .14 93.8 5.71 1346 .59
4N3D3K 150.0 1.32 317.69 6.6 0.15 16.75
Continued on next page
16Under review as submission to TMLR
SG BTM
Clustering trel. DG Tcomptrel. DG Tcomp
Mean 54.48 1.81 483.22
4N3D4K 150.0 1.55 2786 .47 65.8 0.53 2046 .2
4N4D3K 150.0 1.57 441.69 35.0 0.42 122.26
4N4D4K 150.0 1.7 2593.41 7.2 0.14 118.24
QNDA
Clustering trel. DG Tcomp
Mean 54.48 1.81 483.22
2N2D3K 63.2 1.82 92.57
2N2D4K 62.2 0.73 345.59
2N3D3K 62.2 1.58 150.11
2N3D4K 5.0 0.06 34.76
2N4D3K 32.8 0.12 97.89
2N4D4K 4.8 0.08 47.19
3N2D3K 121.0 6.21 412.26
3N2D4K 64.2 3.98 747 .3
3N3D3K 64.0 1.71 151.79
3N3D4K 63.8 0.28 858.76
3N4D3K 35.0 1.36 111.44
3N4D4K 37.2 0.46 731.33
4N2D3K 94.6 3.61 249.28
4N2D4K 93.8 5.68 1281 .33
4N3D3K 9.4 0.17 22.76
4N3D4K 66.0 0.49 1901 .69
4N4D3K 37.4 0.41 129.74
4N4D4K 9.8 0.17 178.24
17