Under review as submission to TMLR
Reward Collapse in Aligning Large Language Models
Anonymous authors
Paper under double-blind review
Abstract
The extraordinary capabilities of large language models (LLMs) such as ChatGPT and
GPT-4 are in part unleashed by aligning them with reward models that are trained on
human preferences represented as rankings of responses to prompts. In this paper, we doc-
ument the phenomenon of reward collapse , an empirical observation where the prevailing
ranking-based approach results in an identical reward distribution for diverse prompts dur-
ing the terminal phase of training. This outcome is undesirable as open-ended prompts like
“write a short story about your best friend” should yield a continuous range of rewards
for their completions, while specific prompts like “what is the capital city of New Zealand”
should generate either high or low rewards. Our theoretical investigation reveals that reward
collapse is primarily due to the insufficiency of the ranking-based objective function to in-
corporate prompt-related information during optimization. This insight allows us to derive
closed-form expressions for the reward distribution associated with a set of utility functions
in an asymptotic setting. To overcome reward collapse, we introduce a prompt-aware opti-
mization scheme that provably admits a prompt-dependent reward distribution within the
interpolating regime. Our experimental results suggest that our proposed prompt-aware
utility functions significantly alleviate reward collapse during the training of reward models.
1 Introduction
A cornerstone of the recent remarkable advancements in the capabilities of large language models (LLMs)
like ChatGPT and GPT-4 is the integration of human feedback (Ouyang et al. (2022); OpenAI (2023)). The
approach to leveraging human feedback often begins with the training of a reward model that encapsulates
human preferences, values, and ethical considerations (Christiano et al. (2017); Ibarz et al. (2018); Bahdanau
etal.(2018);Ziegleretal.(2019);Gangulietal.(2022)). Thisisfollowedbythefine-tuningoftheLLMsusing
reinforcement learning, guided by the reward model. This process, often referred to as reinforcement learning
from human feedback (RLHF), has proven effective in aligning LLMs with human intent, substantially
enriching the quality of human interaction.
However, developing an effective reward model based on human preferences is challenging (Bai et al. (2022b);
Liu et al. (2023); Sun et al. (2023)). A notable difficulty arises when a human labeler struggles to give a
quantitative score to a response/completion for a specific prompt. Instead, it is much easier for humans to
make pairwise comparisons between completions in terms of their quality, which is indeed employed in the
development of InstructGPT (Ouyang et al. (2022)). Explicitly, a human labeler is presented with several
completions generated by the LLMs for the same prompt and arranges the responses from the highest to
lowest perceived quality.1A neural network is then trained to obtain a reward model that assigns rewards
to the responses in an attempt to align as closely as possible with human preferences in the form of rankings.
Despitesomebenefits, suchaseliminatingcalibrationissues, rankingsfallshortinreflectingthevariedreward
distributions of different prompts. This is due to the fact that ranking one completion higher than another
doesnotindicatehow muchsuperiortheformeriscomparedtothelatter. Thisconcernisespeciallypertinent
in RLHF as some prompts are open-ended or, in other words, are dependent on the users’ backgrounds,
allowing the reward distribution to span a continuous range. Conversely, some prompts are closed-ended,
1In slightly more detail, Ouyang et al. (2022) required human labelers to utilize a drag-and-drop interface to construct
consistent rankings from pairwise comparisons.
1Under review as submission to TMLR
resulting in a response that should be either highly or lowly scored, thus generating a roughly two-point mass
distribution for the reward distribution. Instances of the first type of prompts include write a short story
about how AI will look like in 100 years andwhat is the best cuisine in the world , while examples of the second
type are prove the Pythagorean theorem andis chicken a dinosaur . An ideal reward model would assign a
reward of either low or high to close-ended prompts, ensuring that the completion accurately aligns with the
correct direction. Conversely, for open-ended prompts, the reward should avoid being either low or high to
encourage diverse responses. If the reward model cannot distinguish between open-ended and close-ended
prompts, it fails to assist language models in determining uncertainty when providing completions, whether
with high variability or low variability (Padmakumar & He (2023)). As a result, the reward model may
struggle to aid LLMs in accurately calibrating uncertainty without accounting for the nuances of different
prompts.2
Figure 1: Reward distribution of the five responses throughout the training process. The x-axis represents
the response index, sorted by reward from smallest to largest. The solid curve illustrates the mean across
several prompts, while the shadowed area represents the standard deviation. A clear observation from the
figure reveals the progressive convergence of the distribution towards a single value, thereby evidencing the
reward collapse phenomenon. Experiment details are elaborated in Section 5.
As our first main contribution, this paper documents a surprising phenomenon through a series of experi-
ments, demonstrating that training a reward model on preference rankings could result in the samereward
distribution regardless of the prompts. We call this phenomenon reward collapse , which occurs during the
terminal phase of training Papyan et al. (2020). Intriguingly, our theoretical analysis first predicted this
phenomenon prior to its experimental confirmation. Indeed, we show that the collapsed reward distribution
can be numerically deduced from a simple optimization program or, even simpler, admits a closed-form ex-
pression. As demonstrated in Figure 1, our prediction of reward collapse is in agreement with the empirical
results.
Reward collapse is clearly undesirable as it overlooks the subtle differences among various prompts, po-
tentially leading to the miscalibration of human preference during the training of LLMs via reinforcement
learning with the reward model. A rudimentary strategy to bypass this issue is to early stop the training of
2For instance, we suspect that this is partly accountable for the poor calibration of GPT-4 after RLHF (see page 12 of
OpenAI (2023)) and mode collapse (Casper et al. (2023a;b)).
2Under review as submission to TMLR
the reward model Ouyang et al. (2022), which, however, fails to address the fundamental limitation of using
a single utility function across all prompts.
Inoursecondmaincontribution, weintroduceaprincipledapproachtoalleviatingrewardcollapse, leveraging
insights derived from the same optimization program that was instrumental in predicting this phenomenon.
In essence, we propose to use distinct utility functions depending on prompts in training the reward model,
such that the resulting reward distribution can be either widely dispersed or tightly concentrated, contingent
on whether the prompt is open-ended or closed-ended. A notable advantage of this prompt-aware strategy is
that our analysis is analytical, enabling full control over the shape of the reward distribution as required. Our
experiments show that reward collapse can be substantially mitigated using this prompt-aware methodology.
2 What Is Reward Collapse
2.1 Reward modeling
We usexandyto denote prompts and completions. And we use R(x,y)to denote a reward model. In this
paper, we assume R(x,y)∈[0,1]. For a given prompt and ncompletions that are i.i.d. draws from an LLM,
a human labeler ranks the nresponses from the most preferred to the least preferred, and the ranking is
denoted as πx. The dataset is given by3
D={(x,y1,···,yn) :xis a prompt ,
y1,···,ynare its completions from the most prefered to the least prefered }
The reward model is expected to score each completion that is consistent with the human-provided ranking
πxas much as possible. To this end, we train a neural network that maximizes the following overall utility:
/summationdisplay
(x,y1,···,yn)∈D/summationdisplay
1≤i<j≤nU(Rθ(x,yi)−Rθ(x,yj)), (1)
whereUisan(increasing)utilityfunction, θistheweightsoftherewardneuralnetwork. Typically, Uissetto
U(z) = log sigmoid (cz)≡logecz
ecz+1, which is an increasing concave function (Ouyang et al. (2022); Rafailov
et al. (2024)). While maximizing Eq. 1, the reward model learns to not only align with the human-provided
ranking but also distinguish the rewards as much as possible.
2.2 Reward collapse
To illustrate what is reward collapse, we start with the overall utility (1). Let
S(r1,···,rn) =/summationdisplay
1≤i<j≤nU(ri−rj) (2)
then Eq.1 can be further written as
/summationdisplay
(x,y1,···,yn)∈DS(Rθ(x,y1),···,Rθ(x,yn)).
Consequently, if the maximum of S(r1,···,rn)isM, overall utility (1) is upper bound by |D|M. Further-
more, if ˆr1,···,ˆrnis the unique maximizer of S(r1,···,rn)withr1≥···≥rn, then overall utility can reach
|D|Mif and only if
Rθ(x,yi) = ˆri,i= 1,···,n. (3)
In fact, for any reward model that sufficiently optimize the overall utility, the reward Rθ(x,yi)is close to ˆri
for all prompts. We call this phenomenon Reward Collapse . (equation 3). Formally, we have the following
theorem:
3Here, we assume that each prompt has the same number of completions. However, our theory can be readily generalized
to cases where each prompt has a different number of completions.
3Under review as submission to TMLR
Theorem 1 (Reward collapse) .AssumeUis strongly concave with parameter µ>0and strictly increasing,
thenSdefined in (2) has some maximum Mobtained uniquely at ˆr1,···,ˆrn. For any neural network
parameterized by θ, such that
/summationdisplay
(x,y1,···,yn)∈D/summationdisplay
1≤i<j≤nU(Rθ(x,yi)−Rθ(x,yj))≥|D|M−µnϵ2
2,
we have
max
i|Rθ(x,yi)−ˆri−c(x)|≤ϵ
for all (x,y1,···,yn)∈Dand some constant c(x)depending on x.
That is, the empirical distribution of the rewards is approximately independent of the prompt itself in the
interpolating regime, thereby leading to reward collapse. The proof of this theorem can be found in Appendix
B
To further illustrate which neural network maximizes overall utility, consider the case where reward function
is parameterized as Rθ(x,y) =σ(⟨θ,ϕ(x,y)⟩), whereσis the sigmoid function, θ∈Rdrepresents the weights,
andϕ(x,y) :X×Y→ Rdis a known and fixed feature function. Such a reward parameterization is usually
derived by removing the last layer of the pre-trained model. A similar parametrization is also used in Zhu
et al. (2023). Note that we include a sigmoid function to ensure that the reward is in [0,1]. Then ifSdefined
in (2) attains its maximum Muniquely at ˆr1,···,ˆrnandd≥|D|n, there exist a θ∗, such that
Rθ∗(x,yi) = ˆri,i= 1,···,n.
Consequently, when training an over-parameterized neural network maximizing the overall utility, it is likely
to observe reward collapse. We validate this theoretical result through experiments on large language models,
as detailed in Section 5.
In practice, reward collapse is not what we want to observe in the reward model. Consider the following case
where two prompts are given: one open-ended, such as “write a short story about your best friend,” and one
close-ended, such as “what is the capital city of New Zealand.” We expect the rewards for different responses
to the open-ended prompt to be continuously distributed within [0,1]. However, for the close-ended prompt,
the rewards for different responses should be either 0or1. The reward model needs to provide different
reward distributions for different kinds of prompts.
3 Prompt-aware optimization
To avoid having the same reward distribution, one simple strategy is early stopping. While reward collapse
can be avoided via early stopping, early stopping might make the model neglect other important features.
A more principled approach is to change the objective. Our proposal is to let the utility function Unow
depend on the prompt. That is, now we consider training a neural network that maximizes
/summationdisplay
(x,y1,···,yn)∈D/summationdisplay
1≤i<j≤nUx(Rθ(x,yi)−Rθ(x,yj)), (4)
whereUxis a utility function that depends on the prompt x. Note that this reward modeling approach
is similar to traditional reward modeling in that it also aims to maximize the difference in rewards while
incorporating ranking information. However, the key difference here is that we allow for different utility
functions for different prompts x, making the reward model more sensitive to variations in prompts and,
hence, more accurately indicating the effect of different prompts.
In general, the choice of Uxshould reflect the open-endedness of the prompt x. Given the high flexibility in
choosingUx, it is generally recommended to let the practitioners choose these functions to meet their needs.
Nonetheless, below we introduce a family of such functions.
4Under review as submission to TMLR
For a strictly increasing utility function U, it can be easily demonstrated that the maximum can only be
attained when r1≥···≥rn(see Lemma C.1 in the Appendix). As a result, we consider the problem
max
0≤rn≤...≤r1≤1/summationdisplay
1≤i<j≤nU(ri−rj). (5)
We use the term “reward distribution” to refer to the empirical distribution of solutions to (5).
Class 1. LetU(z) =zγ,z∈[0,1]for some 0< γ < 1. This utility function encourages the reward to
take values either near 0 or 1 as γtends to be large. Some plots showing the reward distribution is given in
Figure 2(a) and 2(b).
Class 2. LetU(z) =−zγ,z∈(0,1]for0<γ≤1.We also define U(0) =∞for0≤γ≤1. In this case, the
reward distribution of Eq. 5 becomes more even as γincreases from 0 to 1. Some plots are shown in Figure
2(c) and 2(d).
Class 3. LetU(z) = log sigmoid (z/σ),z∈[0,1]forσ>0. The reward distribution becomes more spread
between 0 and 1 as σbecomes smaller. Some plots are shown in Figure 2(e) and 2(f).
0 0.2 0.4 0.6 0.8 100.20.40.60.81
0.40.4050.410.4150.420.4750.480.485
(a)U(z) =z0.8
0 0.2 0.4 0.6 0.8 100.20.40.60.81
0.40.4050.410.4150.420.440.4450.450.4550.46(b)U(z) =z0.2
00.2 0.4 0.6 0.8 100.20.40.60.81
0.40.4050.410.4150.420.430.4350.440.4450.45(c)U(z) = log z
0 0.2 0.4 0.6 0.8 100.20.40.60.81
0.40.4050.410.4150.420.40.410.420.43
(d)U(z) =−z−1
0 0.2 0.4 0.6 0.8 100.20.40.60.81
0.36 0.38 0.40.460.4650.470.4750.48(e)U(z) = log sigmoid (z)
0 0.2 0.4 0.6 0.8 100.20.40.60.81
0.36 0.38 0.40.380.40.420.44 (f)U(z) = log sigmoid (4z)
Figure 2: Empirical cumulative distribution function (e.c.d.f.) of rewards for different utility functions. As
the number of responses nincreases, the e.c.d.f. converges to a limiting distribution.
3.1 Asymptotics
In general, we can explicitly evaluate the reward distribution for any nby solving the optimization (5).
Nevertheless, it is helpful to get a handle on the empirical distribution of the solution to this optimization
program in the limit n→∞. The next result gives a closed-form expression of the reward distribution in
the case of a large number of completions.
Theorem 2. LetU(z) =zγ,z∈[0,1]for someγ∈(0,1). Then the reward distribution of (5) converges to
Beta/parenleftbig1−γ
2,1−γ
2/parenrightbig
asn→∞, which has probability density x−1+γ
2(1−x)−1+γ
2on(0,1).
5Under review as submission to TMLR
Theorem 3. ForU(z) =−z−γ,z∈(0,1]for0< γ≤1, the reward distribution of (5) converges in
distribution to Beta(1+γ
2,1+γ
2). ForU(z) = logz,z∈(0,1], the reward distribution of (5) converges in
distribution to Beta(1
2,1
2)
The proof of Theorem 3 can be found in Martinez-Finkelshtein et al. (2004); Landkof & Landkof (1972). In
the limitγ→1in Theorem 3, the Beta distribution tends to Beta(1,1), which is the uniform distribution on
[0,1]. This is indeed an example of the one-dimensional Thomson problem (Bowick et al. (2002)), which asks
the configuration of nelectrons constrained to a line that repel each other with a force given by Coulomb’s
law. This problem was first considered by Maxwell. Indeed, Martinez-Finkelshtein et al. (2004); Hardin et al.
(2004); Amore & Jacobo (2019) prove that the reward distribution will converge to the uniform distribution
forU(z) =−z−γwithγ≥1.
For the above two classes, the limiting distribution does not admit a probability mass. However, probability
mass can emerge in the case of a scaled log-sigmoid function.
Theorem 4. IfUis strictly increasing and concave, the derivative of the utility function satisfies U′(0)<
∞,U′(1)>0, then the reward distribution of (5) converges in distribution to a probability measure µ∗that
satisfies
µ∗({0}) =µ∗({1})≥U′(1)
U′(0)+U′(1)>0.
In general, the reward distribution can be characterized from a variational perspective. This gives the
following theorem.
Theorem 5. IfUis bounded, strongly concave, and increasing. There exists a probability measure µ∗
such that the reward distribution of (5) converges in distribution to µ∗, which is uniquely determined by the
following two properties:
(a)µ∗maximizes
E
X,X′iid∼µU(|X−X′|)
over all probability measures µon[0,1], and
(b) it is symmetric with respect to1
2in the sense that, for any measurable set A∈[0,1]and1−A=
{x: 1−x∈A},µ∗(A) =µ∗(1−A).
3.2 Prompt-aware optimization based on open-endedness
Based on the asymptotic properties discussed, we propose a prompt-aware optimization approach that lever-
ages the concept of open-endedness.
For a given prompt x, if it is close-ended (e.g., “What is the capital city of New Zealand?”), the reward for a
responseR(x,y)should be either high or low, indicating a clear right or wrong answer. In such cases, we set
Ux(z) =z, as its limiting reward distribution follows a Bernoulli distribution. Conversely, for an open-ended
prompt (e.g., “Write a short story about your best friend”), the reward should span a continuous range,
reflecting the diversity of possible responses. Here, we choose Ux(z) =−z−1to capture this variability.
Mixed-type prompts, such as “What is the capital city of New Zealand? Tell me some interesting stories
about it,” require a response that addresses both factual accuracy and creative content. For these prompts,
a natural choice is the log-sigmoid function, as its limiting distribution approximates a mixture of Bernoulli
and uniform distributions, effectively balancing the different types of responses required.
4 Proofs
In this section, we will briefly present the proofs of results in Section 2. However, we will deviate from the
previous order and start by proving Theorem 5. We also put the proof of Theorem 4 into Appendix D.3 due
to the length constraint. Let
S(r1,···,rn) :=/summationdisplay
1≤i<j≤nU(ri−rj)andˆr≡(ˆr1,..., ˆrn) := arg max
0≤r1,···,rn≤1S(r1,···,rn).
6Under review as submission to TMLR
In addition, for any vector (u1,···,un)∈Rn, we employ boldface notation uto represent the entire vector.
THis allows us to write S(r).
4.1 Proof of Theorem 5
First, when Uis concave and strictly increasing, ˆrexhibits the following properties:
Lemma 4.1. IfUis strictly concave and strictly increasing, the function S(r)is concave. Therefore, the
optimization problem uniquely determines ˆrn. Additionally, the following properties hold: (1) ˆr1≥···≥ ˆrn,
and (2) 1−ˆri= ˆrn−i+1for any 1≤i≤n.
The proof of Lemma 4.1 is straightforward and is provided in Appendix C.1. Upon further examination of
the function S(r), we discover that if Uis strongly concave with parameter µ>0, thenSalso exhibits some
kind of strongly concavity, except in the direction (1,1,···,1). This property is formulated in the following
lemma.
Lemma 4.2. IfUis strongly concave with parameter µ>0, and we consider another vector u= (u1,...,un),
the following inequality holds:
S(u)−S(ˆr)≤−nµ
2∥ProjVn(u−ˆr)∥2.
Here,Vn⊂Rnis the subspace orthogonal to (1,···,1), and∥·∥represents the Euclidean norm.
The proof of this lemma can be found in Appendix C.2. Our next lemma quantifies the difference between
two symmetric probability measures.
Lemma 4.3. For two different symmetric probability measure µ1andµ2on[0,1], letr(j)
i=1
2inf{t:
µj([0,t])≥n−i
n−1}+1
2sup{t:µj([0,t))<n−i
n−1}}),i= 1,2,···,n;j= 1,2. Then there exists positive constant
c0such that for all n,
∥ProjVn(r(1)−r(2))∥2
2≥c0n.
The proof of Lemma 4.3 is also provided in Appendix C.3. Now, we are ready to prove the uniqueness part
of Theorem 5. Due to the length constraint, we will present it as a separate lemma and defer the proof to
Appendix C.4. In short, we use Lemma 4.2 and 4.3 to demonstrate that for two distinct symmetric measures,
their distance is sufficiently large such that at least one of them is not optimal.
Lemma 4.4. Ifµ1andµ2are two symmetric probability measure which both maximize
E
X,X′iid∼µU(|X−X′|)
over all probability measures µon[0,1].Then we have µ1=µ2.
Now we are ready to prove the convergence part of Theorem 5.
Proof of Theorem 5. LetˆPn:=1
n/summationtextn
i=1δˆrndenote the empirical distribution of ˆrn. Note that{ˆPn}are
probability measures defined on [0,1], so they are tight. By Prohorov’s theorem, there exists a sub-sequence
{k(n)}n≥1such that ˆPk(n)d→ˆµ. LetXn,X′
niid∼ˆPnand ˆX,ˆX′iid∼ˆµ. By continuous mapping theorem, we
also have|Xn−X′
n|d→|ˆX−ˆX′|.Moreover, because Uis bounded and continuous, Portmanteau theorem
gives
E
X,X′iid∼ˆPk(n)U(|X−X′|)→E
X,X′iid∼ˆµU(|X−X′|).
Letµbe another probability measure on [0,1]. Let ˆQn=1
n/summationtextn
i=1δqn,isuch that ˆQnd→µ. By the same
argument before, we also have E
X,X′iid∼ˆQk(n)U(|X−X′|)→E
X,X′iid∼µU(|X−X′|).Then by the optimal
assumption of ˆrn,
E
X,X′iid∼ˆµU(|X−X′|) = lim
n→∞E
X,X′iid∼ˆPk(n)U(|X−X′|)
≥lim
n→∞E
X,X′iid∼ˆQk(n)U(|X−X′|) =E
X,X′iid∼µU(|X−X′|).
7Under review as submission to TMLR
This means ˆµmaximize E
X,X′iid∼µU(|X−X′|)over all probability measure µon[0,1]. From Lemma 4.1, we
know that 1−ˆri= ˆrn−i+1, soˆµis symmetric. If there is another sub-sequence m(n)such that ˆPm(n)d→ˆν.
By the same argument before, ˆνis also optimal and symmetric. From Lemma 4.4, ˆµ= ˆν. Thus for every
converging sub-sequence of {ˆPn}, the limit distribution must be the same. By the tightness of {ˆPn}, we have
ˆPnd→µ∗.
4.2 Proof of Theorem 2
For the utility function U(z) =zγ, having established Theorem 5, our objective is to identify a symmetric
probability measure µ∗that maximizes E
X,X′iid∼µU(|X−X′|). By employing the variational principle, we
can derive a condition that is necessary for optimality. Notably, this condition also suffices for optimality.
Lemma 4.5. LetU(z) =zγfor someγ∈(0,1). A probability measure µon[0,1]will maximize
E
X,X′iid∼µU(|X−X′|)if it satisfies the condition that EX∼µU(|X−c|)is independent of c∈[0,1].
The proof of Lemma 4.5 is provided in Appendix D.1. Therefore, proving Theorem 2 is reduced to verifying
the condition stated in Lemma 4.5. This verification process is tedious and will be deferred to Appendix D.2
for brevity.
5 Experiments
In this section, we conduct experiments to investigate the phenomenon of reward collapse and demonstrate
that prompt-aware training can prevent reward collapse.
5.1 Evidence of reward collapse in Large language model
We start our investigation by conducting experiments utilizing a LLM, specifically GPT-Neo-1.3B (Black
et al. (2021)). Guided by the methodologies outlined in the StackLlama project (Beeching et al. (2023)), we
trained the model on the StackExchange preference dataset (Lambert et al. (2023)), a robust resource that
provides rankings of responses for individual prompts.
Constrained by computational resources, we focused our training on a carefully selected subset of the dataset
containing only the prompts accompanied by exactly five responses. Our experimental setup comprised 128
distinct prompts, each of which contributed 10 pairs to the reward modeling process. By adopting the
codebase from StackLlama (Beeching et al. (2023)), and setting the learning rate to 3×10−5along with a
batch size of 20pairs, we carried out the training over 10epochs.
As demonstrated in Figure 1, our results highlight the emergence of the reward collapse phenomenon under
these realistic conditions. The evidence of this effect can be observed as the distribution becomes increasingly
concentrated over the course of the training.
5.2 Setup of our second experiment
The open-source datasets currently available for RLHF are rather limited. Most of these datasets (Nakano
et al. (2021); Bai et al. (2022a)) typically include only a handful of candidate responses (usually a single
pair) for each corresponding prompt question. Moreover, the ranking signals in those datasets are usually
noisy, either because they are sourced from the Internet (Ethayarajh et al. (2023)) or because of the inherent
subjectivity of the ranking process.
In order to conduct a carefully controlled experiment, we curated our own dataset, focusing on a single,
simplified feature – the length of the response, measured in terms of word count as the ground truth reward.
A subset of questions was selected from the LongForm dataset (Köksal et al. (2023)), a question-answer
dataset characterized by its lengthy answers. To simulate scenarios with open-ended and concrete problems,
we truncated the original answer according to two distinct length distributions, thereby generating eight
responses for each prompt: the first distribution is nearly uniform, ranging from 10 to 80 words, while the
8Under review as submission to TMLR
Figure 3: Reward collapse on the test set. Thex-axis represents the response index, sorted by reward
fromsmallesttolargest, consistentwiththefollowingfigure. Therewarddistributionsexhibitsimilarcollapse
phenomena on the test set, and employing a prompt-aware loss function can mitigate this collapse.
(a)logsigmoid as utility function
 (b) Prompt-aware utility function
Figure 4: (Left)The reward distribution of different prompts gradually converges into a single distribution
during training. (Right) When using the prompt-aware loss function, the reward distributions of the two
different prompts can be gradually separated during training.
second is a polarized distribution with response lengths primarily clustered around either 30 or 60 words.
Each question was randomly assigned as either open-ended or concrete.4Additionally, the phrases "Write
the answer in an open-ended way." and "Write either a short answer or a long answer." were added to the
open-ended and concrete questions, respectively, to distinguish the question type. Following this process, we
constructed a dataset comprising 8192 training questions and 16 test questions.
In our experiments, we focus on the following Ufunctions:z,logz,−1/z, as well as logsigmoid (z), which
is employed in Ouyang et al. (2022) and the prompt-aware U, which adaptively selects Ufromzand−1/z.
Given that the Ufunction operates on zin the range [−1,1], we adjust some Ufunctions with suitable
continuous extensions or scaling. We then train a DeBERTa V3 (He et al. (2021)) as the reward model. The
training details can be found in Appendix A.1.
5.3 Experimental results
Fixed loss function leads to reward collapse. As depicted in Figure 4(a), reward distributions corre-
spondingtodifferentpromptsgraduallyconvergetowardsasingle, prompt-independentdistributionthrough-
out the training process. Specifically, in the context of Figure 4(a), where the Ufunction is represented by
LogSigmoid , the reward distribution exhibits positive probability mass at reward scores of 0and1(illus-
4In practice, such assignments can be done by various methods. See Appendix A.2 for a short discussion.
9Under review as submission to TMLR
trated by the flat segments corresponding to the first two and last two scores). This observation validates
the prediction encapsulated in Theorem 4. Examining other Ufunctions, Figures 3 collectively indicates
the occurrence of loss collapse on the test datasets. Specifically, employing zas theUfunction results in a
polarized reward distribution, whereas utilizing −1/zas theUfunction yields a uniform reward distribution.
Prompt-aware training avoids reward collapse. Figures 3 shows the reward distribution at the end
of training with varying utility functions. The results along with Figure 4(b) reveal that using a prompt-
awareUfunction effectively prevents reward collapse across both training and test datasets. This strategy
yields a more uniform reward distribution for open-ended prompts while promoting a more polarized reward
distribution for concrete prompts.
6 Discussion
In this paper, we have introduced an empirical phenomenon known as reward collapse that arises during
reward model training for aligning LLMs using human preference rankings. This phenomenon results in the
same reward distribution regardless of the prompt type. The occurrence of reward collapse stems from neural
network interpolation during the final training phase. Although techniques that mitigate overfitting, such
as early stopping or regularization, can be employed to mitigate reward collapse, we propose a new method
that consider the nature of prompts. We provided an analytical framework that evaluates reward distribu-
tion, yielding closed-form reward expressions. Synthetic experiments substantiate our findings, presenting a
method superior to early stopping to tackle reward collapse.
While our experiments provide valuable insights, it is important to acknowledge their limitations, primarily
stemming from the constrained computational resources available. Given abundant resources, future re-
search can explore the use of a more diverse range of prompts, varying in terms of their open-endedness.
Additionally, it would be interesting to investigate the extent to which the trained reward model enhances
the capabilities of large language models, such as their ability to self-calibrate uncertainty (Lin et al. (2022);
Kadavath et al. (2022)). Theoretical investigations could focus on finding increasing, concave functions that
precisely match a given discrete reward distribution. On the practical side, developing a method to choose
a utility function based on prompts, perhaps using a parameter such as γin Section 3, poses an intriguing
avenue for further exploration (more discussion on choosing utility function is in Appendix A.2). Further-
more, exploring the potential benefits of truncated ranking by requiring human labelers to provide partial
rankings of acceptable completions and ignore unacceptable completions could offer valuable insights into
improving the training of reward models.
References
Paolo Amore and Martin Jacobo. Thomson problem in one dimension: Minimal energy configurations of n
charges on a curve. Physica A: Statistical Mechanics and its Applications , 519:256–266, 2019.
Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Arian Hosseini, Pushmeet Kohli, and Ed-
ward Grefenstette. Learning to understand goal specifications by modelling reward. arXiv preprint
arXiv:1806.01946 , 2018.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022a.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from
ai feedback. arXiv preprint arXiv:2212.08073 , 2022b.
Edward Beeching, Younes Belkada, Kashif Rasul, Lewis Tunstall, Leandro von Werra, Nazneen Rajani, and
Nathan Lambert. Stackllama: An rl fine-tuned llama model for stack exchange question and answering,
2023. URL https://huggingface.co/blog/stackllama .
10Under review as submission to TMLR
Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregres-
sive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/10.5281/zenodo.
5297715. If you use this software, please cite it using these metadata.
Mark Bowick, Angelo Cacciuto, David R Nelson, and Alex Travesset. Crystalline order on a sphere and the
generalized thomson problem. Physical Review Letters , 89(18):185502, 2002.
Stephen P Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.
Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando,
Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental
limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217 , 2023a.
Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. Explore, establish, exploit:
Red teaming language models from scratch. arXiv preprint arXiv:2306.09442 , 2023b.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforce-
ment learning from human preferences. Advances in neural information processing systems , 30, 2017.
Kawin Ethayarajh, Heidi Zhang, Yizhong Wang, and Dan Jurafsky. Stanford human preferences dataset,
2023. URL https://huggingface.co/datasets/stanfordnlp/SHP .
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann,
Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms:
Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 , 2022.
Doug P Hardin, Edward B Saff, et al. Discretizing manifolds via minimum energy points. Notices of the
AMS, 51(10):1186–1194, 2004.
Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pre-
training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543 , 2021.
Sangchul Lee (https://math.stackexchange.com/users/9340/sangchul lee). Expected absolute difference be-
tween two iid variables. Mathematics Stack Exchange. URL https://math.stackexchange.com/q/
2542224. URL:https://math.stackexchange.com/q/2542224 (version: 2017-11-29).
Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning
from human preferences and demonstrations in atari. Advances in neural information processing systems ,
31, 2018.
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know
what they know. arXiv preprint arXiv:2207.05221 , 2022.
Abdullatif Köksal, Timo Schick, Anna Korhonen, and Hinrich Schütze. Longform: Optimizing instruction
tuning for long text generation with corpus extraction, 2023.
Nathan Lambert, Lewis Tunstall, Nazneen Rajani, and Tristan Thrush. Huggingface h4 stack
exchange preference dataset, 2023. URL https://huggingface.co/datasets/HuggingFaceH4/
stack-exchange-preferences .
Naum Samo˘ ılovich Landkof and NS Landkof. Foundations of modern potential theory , volume 180. Springer,
1972.
Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words.
Transactions on Machine Learning Research , 2022.
H Liu, C Sferrazza, and P Abbeel. Chain of hindsight aligns language models with feedback. arXiv preprint
arXiv:2302.02676 , 2023.
11Under review as submission to TMLR
A Martinez-Finkelshtein, V Maymeskul, EA Rakhmanov, and EB Saff. Asymptotics for minimal discrete
riesz energy on curves in Rd.Canadian Journal of Mathematics , 56(3):529–552, 2004.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen
Krueger, KevinButton, MatthewKnight, BenjaminChess, andJohnSchulman. Webgpt: Browser-assisted
question-answering with human feedback. In arXiv, 2021.
OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. Advances in Neural Information Processing Systems , 35:27730–27744, 2022.
Vishakh Padmakumar and He He. Does writing with language models reduce content diversity? arXiv
preprint arXiv:2309.05196 , 2023.
Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of
deep learning training. Proceedings of the National Academy of Sciences , 117(40):24652–24663, 2020.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.
Direct preference optimization: Your language model is secretly a reward model. Advances in Neural
Information Processing Systems , 36, 2024.
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang,
and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human
supervision. arXiv preprint arXiv:2305.03047 , 2023.
Banghua Zhu, Jiantao Jiao, and Michael I Jordan. Principled reinforcement learning with human feedback
from pairwise or k-wise comparisons. arXiv preprint arXiv:2301.11270 , 2023.
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint
arXiv:1909.08593 , 2019.
12Under review as submission to TMLR
A Details about experiments
A.1 Training Details
We use the following extension of the utility functions during our training.
•logz:U(z) =/braceleftigg
log(z+ϵ)forz>0
z+ log(ϵ)forz≤0, whereϵis set to 0.1.
•−1/z:U(z) =/braceleftigg
−1/(z+ϵ)forz>0
z−1/ϵforz≤0, whereϵis also set to 0.1.
•logsigmoid (z):U(z) = log sigmoid (4z). Here, the scaling factor of 4ensures the output of
logsigmoid spans a sufficient range.
To train the reward model, we adopted the approach used in the OpenAssistant project, which utilizes the
DeBERTaV3 Base model He et al. (2021). To constrain the reward output between 0 and 1, a σfunction
was appended before the final output. The reward model was trained with a batch size of 224 (comprising
eight questions per batch, each with 28 pairs) for a total of 1000 steps, approximately equivalent to 1 epoch.
The maximum learning rate was configured to 1e-5, utilizing the Adam optimizer and a linear learning rate
schedule, inclusive of 10% warmup steps. The reward model was trained on a single A6000 GPU, with the
entire training process concluding in roughly 1 hour.
A.2 Discussion on assigning the prompt type
Determining the prompt type is a crucial aspect of our prompt-aware approach. In our experiments, we ran-
domly assigned prompts as either open-ended or close-ended. This sufficed to demonstrate the effectiveness
of our prompt-aware approach in shaping the reward distribution for different types of prompts. However,
in practice, there are various viable methods to accomplish this. While our primary focus is not on detailing
how to identify the prompt type, we intend to present some straightforward yet effective approaches.
One straightforward method is manually deciding the prompt type, similar to how human feedback is col-
lected in Instructgpt (Ouyang et al. (2022)), where preference rankings are collected from human labelers.
Typically, these labelers are asked to rank different responses. Moreover, we can ask them to evaluate the
extent of open-endedness in the prompt, using a scale that ranges from -1 to 1.
Automated annotation processes are also possible. For example, one could assess the diversity of responses to
a given prompt. If the responses exhibit significant diversity, the prompt could be categorized as open-ended.
Conversely, if the responses show limited diversity, the prompt might be classified as close-ended.
Determining the prompt type is indeed a complex and intriguing task, and it offers an interesting avenue for
future research.
B Proofs of Theorem 1
Proof.WhenUisµ-strong concave, by Lemma 4.2 and Lemma 4.4, Sdefined in (1) has a unique maximizer
ˆr1,···,ˆrn. Moreover, for any u1,···,un,
S(u1,···,un)≤S(ˆr1,···,ˆrn)−nµ
2∥ProjVn(u−ˆr)∥2.
Here,Vn⊂Rnis the subspace orthogonal to (1,···,1), and∥·∥represents the Euclidean norm. Back to
the Theorem 1, if a neural network Rθsatisfies
/summationdisplay
(x,y1,···,yn)∈DS(Rθ(x,y1),···,Rθ(x,yn))≥|D|M−µnϵ2
2,
13Under review as submission to TMLR
then for all (x,y1,···,yn)∈D,S(Rθ(x,y1),···,Rθ(x,yn))≥M−µnϵ2
2because the maximum of SisM.
As a result, letting µ= ((Rθ(x,y1),···,Rθ(x,yn))),
M−µnϵ2
2≤S(Rθ(x,y1),···,Rθ(x,yn))≤M−nµ
2∥ProjVn(u−ˆr)∥2.
This gives an upper bound ϵ2on∥ProjVn(u−ˆr)∥2. Finally, by the definition of ProjVn, there exists a
constantc(x), such that ProjVn(u−ˆr) =u−ˆr+c(x)·1. For this constant c(x),
max
i|Rθ(x,yi)−ˆri−c(x)|≤/radicalig
∥ProjVn(u−ˆr)∥2≤ϵ.
This finishes the proof.
C Missing Proofs in Section 2 and 4
C.1 Proof of Lemma 4.1
We break the proof of Lemma 4.1 into two different lemma.
Lemma C.1. If the utility function U(x)is strictly increasing, let ˆrbe the solution of optimization problem:
max
0≤r1,...,rn≤1/summationdisplay
1≤i<j≤nU(ri−rj)
Then ˆrsatisfies: ˆr1≥···≥ ˆrn.
Proof.LetS(r) =/summationtext
1≤i<j≤nU(ri−rj).Suppose the conclusion is not true, then there exists a k≥0, such
that ˆr1≥···≥ ˆrkandˆrk<ˆrk+1. Let us define
˜ri=

ˆriifi̸=k,k+ 1;
ˆrk+1ifi=k;
ˆrkifi=k+ 1.
Then
/summationdisplay
1≤i<j≤nU(ˆri−ˆrj)−/summationdisplay
1≤i<j≤nU(˜ri−˜rj) =U(ˆrk−ˆrk+1)−U(ˆrk+1−ˆrk)<0
becauseUis strictly increasing and ˆrk−ˆrk+1<0. This contracts with the fact that ˆris the solution of the
optimization problem, and thus the conclusion holds.
Lemma C.2. If the utility function U(x)is strictly increasing and strictly concave, then the function S(r) =/summationtext
1≤i<j≤nU(ri−rj)is concave. Moreover, the solution of optimization problem
max
0≤r1,...,rn≤1/summationdisplay
1≤i<j≤nU(ri−rj)
is unique and satisfies: 1−ˆri= ˆrn−i+1fori= 1,2,···,n.
Proof.The concavity of Sfollows directly from definition:
S(r) +S(r′) =/summationdisplay
1≤i<j≤nU(ri−rj) +U(r′
i−r′
j)
≤/summationdisplay
1≤i<j≤n2U(ri+r′
i−rj−r′
j
2) = 2S(r+r′
2).
14Under review as submission to TMLR
The above inequality is an equality if and only if ri−rj=r′
i−r′
jfor all 1≤i < j≤nwhenU(x)is
strictly concave. When Uis increasing, the solution ˆrof the optimization problem satisfies ˆr1= 1. Thus
the solution of the optimization problem max 1≤r1,···,rn≤1S(r)is unique, otherwise the vectorr1+r2
2makes
Slarger where r1andr2are two different solutions.
Finally, let ˆrbe the unique solution of the optimization problem. Let us define ˜ri= 1−ˆrn−i+1for all
i= 1,2,···,n. It follows that ˜ri−˜rj= ˆrn−j+1−ˆrn−i+1, and we have S(ˆr) =S(˜r).Consequently, the
uniqueness of the solution implies ˆr=˜r. This means that ˆri= 1−ˆrn−i+1fori= 1,···,n.
C.2 Proof of Lemma 4.2
Proof of Lemma 4.2. The definition of S(r)is
S(r) =/summationdisplay
1≤i<j≤nU(ri−rj).
The value of Sdoes not change if we increase all riby the same constant. Thus the value of S(r)only
depends on ProjVn(r)whereVn⊂Rndenotes the subspace orthogonal to (1,1,···,1). We can define a new
function on Vnby letting
F(ProjVn(r)) =S(r).
The domain of FisA={v∈Vn|∃r∈Rnsuch that 0≤ri≤1andv= ProjVn(r)}. First, we can show
thatFisnµ-strongly concave.
BecauseUisµ-strongly concave, U(x) +µ
2x2is concave. It follows that
S(r) +µ
2/summationdisplay
1≤i<j≤n(ri−rj)2
is also concave. We can write/summationtext
1≤i<j≤n(ri−rj)2as
/summationdisplay
1≤i<j≤n(ri−rj)2=nn/summationdisplay
i=1r2
i−(n/summationdisplay
i=1ri)2
by Lagrange identity. Then note that Vnis the subspace orthogonal to (1,1,···,1). The projection onto Vn
is given by
ProjVn(r) = (r1−1
nn/summationdisplay
i=1ri,···,rn−1
nn/summationdisplay
i=1ri).
As a result,
∥ProjVn(r)∥2=n/summationdisplay
i=1
ri−1
nn/summationdisplay
j=1rj
2
=n/summationdisplay
i=1r2
i−1
n(n/summationdisplay
i=1ri)2=1
n/summationdisplay
1≤i<j≤n(ri−rj)2.
From this equation and the concavity of S(r) +µ
2/summationtext
1≤i<j≤n(ri−rj)2, we know that
S(r) +nµ
2∥ProjVn(r)∥2
is also concave. Consequently, F(ProjVn(r))+nµ
2∥ProjVn(r)∥2is concave, which lead to the strong concavity
ofFbecause
F(v) +nµ
2∥v∥2
15Under review as submission to TMLR
is concave. Let ˆvbe the optimal vector that maximizes F(v), strong concavity implies (See e.g. Section
9.1.2 in Boyd & Vandenberghe (2004))
F(v)−F(ˆv)≤−nµ
2∥v−ˆv∥2.
Therefore, by the definition of F(ProjVn(r)) =S(r), we have
S(u)−S(ˆr)≤−nµ
2∥ProjVn(u−ˆr)∥2.
C.3 Proof of Lemma 4.3
Proof of Lemma 4.3. Becauseµj,j= 1,2are symmetric, we have
r(j)
n,n−i+1=1
2inf{t:µj([0,t])≥i−1
n−1}+1
2sup{t:µj([0,t))<i−1
n−1}})
=1
2(1−sup{t:µj([t,1])≥i−1
n−1}) +1
2(1−inf{t:µj((t,1])<i−1
n−1})
=1
2(1−sup{t:µj([0,t))<n−i
n−1}) +1
2(1−inf{t:µj([0,t])≥n−i
n−1}
= 1−r(j)
n,i.
So we have/summationtextn
i=1(r(1)
n,i−r(2))
n,i) = 0. Note that Vn⊂Rnis the subspace which is orthogonal to (1,1,···,1),
the projection of x= (x1,···,xn)ontoVnis given by
ProjVn(x) = (x1−1
nn/summationdisplay
i=1xi,···,xn−1
nn/summationdisplay
i=1xi).
Consequently,
∥ProjVn(r(1)
n−r(2)
n)∥2
2=n/summationdisplay
i=1(r(1)
n,i−r(2)
n,i)2.
Ifµ1andµ2are two different symmetric probability measure on [0,1], we can assume that there exists
q1<q2∈[0,1]andδ≥0, such that µ1([0,q2])<µ 2([0,q1])−δ. So wheni−1
n−1∈(µ1([0,q2]),µ2([0,q1])−δ),
we haver(1)
n,n−i+1≥q2becauseµ1([0,q2])<i−1
n−1. We also have r(2)
n,n−i+1≤q1becauseµ2([0,q1])>i−1
n−1.As
a result,r(1)
n,n−i+1−r(2)
n,n−i+1≥q2−q1whenever (i−1)/(n−1)∈(µ1([0,q2]),µ2([0,q1])−δ). Because the
length of the interval is positive, the number of such iis larger than c1nwherec1is a constant independent
ofn. Then we conclude that
∥ProjVn(r(1)
n−r(2)
n)∥2
2=n/summationdisplay
i=1(r(1)
n,i−r(2)
n,i)2
≥c1n(q1−q2)2.
Choosingc0=c1(q1−q2)2gives the inequality
∥ProjVn(r(1)
n−r(2)
n)∥2
2≥c0n.
16Under review as submission to TMLR
C.4 Proof of Lemma 4.4
Proof of Lemma 4.4. Suppose there exist two different symmetric probability measure µ1andµ2, they both
maximize E
X,X′iid∼µU(|X−X′|). LetM=E
X,X′iid∼µjU(|X−X′|),j= 1,2. Now let r(j)
n,i=1
2inf{t:
µj([0,t])≥i−1
n−1}+1
2sup{t:µj([0,t))<i−1
n−1}}),i= 1,2,···,n;j= 1,2as defined in Lemma 4.3. Accord-
ingly, let P(j)
n=1
n/summationtextn
i=1δr(j)
n,i. Then we have
P(j)
nd→µj,j= 1,2.
This can be proved easily by considering the definition of convergence in distribution. Since Gis bounded,
this lead to E
X,X′iid∼P(j)
nU(|X−X′|)→M,j = 1,2asn→∞.
The expectation E
X,X′iid∼P(j)
nU(|X−X′|)can be written more precisely as
E
X,X′iid∼P(j)
nU(|X−X′|) =1
n2/summationdisplay
1≤i,i′≤nU(|r(j)
n,i−r(j)
n,i′|).
By Lemma 4.2, we can bound the difference
1
n2/summationdisplay
1≤i,i′≤nU(|r(j)
n,i−r(j)
n,i′|)−1
n2/summationdisplay
1≤i≤i′≤nU(|ˆrn,i−ˆrn,i′|)
=2/parenleftbign
2/parenrightbig/summationdisplay
1≤i<i′≤nU(r(j)
n,i−r(j)
n,i′)−2/parenleftbign
2/parenrightbig/summationdisplay
1≤i<i′≤nU(ˆrn,i−ˆrn,i′)
≤ −2µ
n−1∥ProjVn(r(j)
n−ˆrn)∥2
2.
Then apply Lemma 4.3, there exist c0≥0such that
2∥ProjVn(r(1)
n−ˆrn)∥2+ 2∥ProjVn(r(2)
n−ˆrn)∥2≥∥ProjVn(r(1)
n−r(2)
n)∥2.
Here, we uses 2∥x∥2
2+ 2∥y∥2
2≥∥x−y∥2
2. So
min
j=1,21
n2
/summationdisplay
1≤i,i′≤nU(|r(j)
n,i−r(j)
n,i′|)−U(|ˆrn,i−ˆrn,i′|)

=−2µ
n−1max
j=1,2∥ProjVn(r(j)
n−ˆrn)∥2
2
≤ −2µ
n−1∥ProjVn(r(1)
n−r(2)
n)∥2
4
≤ −µc0n
2n−2≤−c0µ
2.
SinceM= max E
X,X′iid∼µU(|X−X′|), we know1
n2/summationtext
1≤i,i′≤nU(|ˆrn,i−ˆrn,i′|)≤M. As a result,
min
j=1,2E
X,X′iid∼P(j)
nU(|X−X′|)≤1
n2/summationdisplay
1≤i≤i′≤nU(|ˆrn,i−ˆrn,i′|)−µc0/2≤M−µc0/2.
This contradicts the assumption that E
X,X′iid∼P(j)
nU(|X−X′|)→M,j = 1,2, n→∞.
17Under review as submission to TMLR
D Proof of Theorem 2
Given Theorem 5, we only need to find a symmetric probability measure on [0,1], which maximizes
E
X,X′iid∼µU(|X−X′|).
The following proof in this section is adapted from (https://math.stackexchange.com/users/9340/sangchul
lee). LetM(B([0,1]))denote the sets of all finite signed measure on the Borel sigma algebra B([0,1]).
Apparently, P(B([0,1]))⊂M(B([0,1])). Then we define the following “inner product” in M(B([0,1])):
⟨µ,ν⟩=EX∼µ,X′∼ν,independentU(|X−X′|) =/integraldisplay
[0,1]2U(|x−y|)µ(dx)ν(dy).
We also define I(µ)asI(µ) :=⟨µ,µ⟩. With these notations, the problem becomes
max
µ∈P(B([0,1]))I(µ).
Lemma D.1. ForU(x) =xγwithγ∈(0,1). Ifµis a signed measure satisfying µ([0,1]) = 0, then we have
I(µ)≤0. Moreover, I(µ) = 0if and only if µ(E) = 0for allE⊂[0,1].
Proof.f(t) =1−cos(xt)
t1+γis integrable on (0,∞). As a result, using change of variables, we have
|x|γ=C/integraldisplay∞
01−cos(xt)
t1+γdt
for come constant C > 0. Then by Fubini’s theorem, we have
⟨µ,µ⟩=/integraldisplay
[0,1]2|x−y|γµ(dx)µ(dy)
=C/integraldisplay
[0,1]2/integraldisplay∞
01−cos((x−y)t)
t1+γdtµ(dx)µ(dy)
=C/integraldisplay∞
0/parenleftigg/integraldisplay
[0,1]21−cos((x−y)t)
t1+γµ(dx)µ(dy)/parenrightigg
dt.
Note that cos((x−y)t) =ℜ(eixt−iyt), we have
/integraldisplay
[0,1]21−cos((x−y)t)
t1+γµ(dx)µ(dy)
=−ℜ/parenleftigg/integraldisplay
[0,1]2eixte−iyt
t1+γµ(dx)µ(dy)/parenrightigg
=−ℜ/parenleftbig
|ˆµ(t)|2/parenrightbig
,
where ˆµ(t) =/integraltext
[0,1]eitxµ(dx)is the Fourier transform of µ. Then
I(µ) =−C/integraldisplay∞
0|ˆµ(t)|2
t1+γdt≤0.
Moreover,I(µ) = 0if and only if ˆµ(t) = 0for allt∈[0,∞)if and only if µ(E) = 0for allE∈B([0,1]).
D.1 Proof of Lemma 4.5
We first restate the lemma.
Lemma D.2. LetU(x) =xγfor someγ∈(0,1). If a probability measure µon[0,1]maximize
E
X,X′iid∼µU(|X−X′|)if it satisfies that EX∼µU(|X−c|)does not depend on c∈[0,1].
18Under review as submission to TMLR
Proof of Lemma 4.5. For two probability measure µandνon[0,1],(µ−ν)([0,1]) = 0. Supposeµsatisfies
EX∼µU(|X−c|) =Kdoes not depend on c∈[0,1]. Note that
⟨ν−µ,µ⟩=/integraldisplay
[0,1]/parenleftigg/integraldisplay
[0,1]|x−y|γµ(dx)/parenrightigg
(ν−µ)(dy) =/integraldisplay
[0,1]K(ν−µ)(dy) = 0.
And by lemma D.1, ⟨ν−µ,ν−µ⟩≤0. Therefore,
⟨ν,ν⟩=⟨µ,µ⟩+ 2⟨ν−µ,µ⟩+⟨ν−µ,ν−µ⟩≤⟨µ,µ⟩.
This means that µmaximize E
X,X′iid∼µU(|X−X′|).
D.2 Proof of Theorem 2
Proof of Theorem 2. Letµbe the probability measure induced by Beta (1−γ
2,1−γ
2). It has probability density
function
fγ(x) =1
B(1−γ
2,1−γ
2)x−1+γ
2(1−x)−1+γ
2.
For anyc∈[0,1],EX∼µU(|X−c|)can be expressed as
EX∼µU(|X−c|) =1
B(1−γ
2,1−γ
2)/integraldisplay1
0|x−c|γx−1+γ
2(1−x)−1+γ
2dx
=1
B(1−γ
2,1−γ
2)/integraldisplayπ
2
0|sin2θ−c|γ(sinθ)−1−γ(cosθ)−1−γdsin2θ
=2
B(1−γ
2,1−γ
2)/integraldisplayπ
2
0/vextendsingle/vextendsingle/vextendsingle/vextendsinglesin2θ−c
sinθcosθ/vextendsingle/vextendsingle/vextendsingle/vextendsingleγ
dθ
=2
B(1−γ
2,1−γ
2)/integraldisplay∞
0/parenleftigg/integraldisplayπ/2
01/braceleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglesin2θ−c
sinθcosθ/vextendsingle/vextendsingle/vextendsingle/vextendsingleγ
≥t/bracerightbigg
dθ/parenrightigg
dt.
Because
/integraldisplayπ/2
01/braceleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglesin2θ−c
sinθcosθ/vextendsingle/vextendsingle/vextendsingle/vextendsingleγ
≥t/bracerightbigg
dθ=1
2/integraldisplayπ
01/braceleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsinglecosθ+ 2c−1
sinθ/vextendsingle/vextendsingle/vextendsingle/vextendsingleγ
≥t/bracerightbigg
dθ
=π
2−1
2/integraldisplayπ
01/braceleftig
−cosθ−t1/γsinθ≤2c−1≤−cosθ+t1/γsinθ/bracerightig
dθ
=π
2−1
2/integraldisplayπ
01/braceleftbigg
−cos(θ−ϕ)≤2c−1√
1 +t2/γ≤−cos(θ+ϕ)/bracerightbigg
dθ
=π
2−ϕ,
where tanϕ=t1/γandϕ∈[0,π/2], and the last equation use the fact that c∈[0,1]. As a result,
EX∼µU(|X−c|)does not depend on c.
Note that Beta distribution is also symmetric. It follows from Theorem 5 that the reward distribution
converges to Beta/parenleftbig1−γ
2,1−γ
2/parenrightbig
.
D.3 Proof of Theorem 4
Theorem 4 can be intuitively understood as follows: If the function UsatisfiesU′(0)<∞andU′(1)>0,
we can show, by analyzing the first-order optimality condition, that a positive fraction of ˆris equal to 1.
Proof of Theorem 4. The derivative of −/summationtext
i<jU(ri−rj)with respect to rkis given by
−∂/summationtext
i<jU(ri−rj)
∂rk/vextendsingle/vextendsingle/vextendsingle/vextendsingle
ˆr1,···,ˆrn=k−1/summationdisplay
i=1U′(ˆri−ˆrk)−N/summationdisplay
i=k+1U′(ˆrk−ˆrj)≤(k−1)U′(0)−(n−k)U′(1).
19Under review as submission to TMLR
The inequality follows from the convexity of U. Letκ=U′(0)
U′(1). Ifk≤n/(κ+ 1), we have (k−1)U′(0)−(n−
k)U′(1)≤0. Hence, we can get ˆrk= 1. Otherwise, we could increase ˆrkto make/summationtext
i<jU(ˆri−ˆrj)larger. As
a result, ˆr1=···= ˆr[n/(κ+1)]= 1.This gives ˆPn({1})≥[n
κ+1]/n. By Theorem 5, we know that there exists
a limiting distribution µ∗such that ˆPd→µ∗andµ∗({1})≥1/(κ+ 1). Due to symmetry proved in Lemma
4.1, we also have µ∗({0})≥1/(κ+ 1).
20