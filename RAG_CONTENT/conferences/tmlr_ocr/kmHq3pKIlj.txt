Published in Transactions on Machine Learning Research (12/2024)
Lookahead Counterfactual Fairness
Zhiqun Zuo zuo.167@osu.edu
Department of Computer Science and Engineering
The Ohio State University
Tian Xie xie.1379@osu.edu
Department of Computer Science and Engineering
The Ohio State University
Xuwei Tan tan.1206@osu.edu
Department of Computer Science and Engineering
The Ohio State University
Xueru Zhang zhang.12807@osu.edu
Department of Computer Science and Engineering
The Ohio State University
Mohammad Mahdi Khalili khalili.17@osu.edu
Department of Computer Science and Engineering
The Ohio State University
Reviewed on OpenReview: https: // openreview. net/ forum? id= kmHq3pKIlj& referrer= %5Bthe% 20profile%
20of% 20Zhiqun% 20Zuo% 5D( %2Fprofile% 3Fid% 3D~Zhiqun_ Zuo1)
Abstract
As machine learning (ML) algorithms are used in applications that involve humans, concerns
have arisen that these algorithms may be biased against certain social groups. Counterfac-
tual fairness (CF) is a fairness notion proposed in Kusner et al. (2017) that measures the
unfairness of ML predictions; it requires that the prediction perceived by an individual in
the real world has the same marginal distribution as it would be in a counterfactual world, in
which the individual belongs to a different group. Although CF ensures fair ML predictions,
it fails to consider the downstream effects of ML predictions on individuals. Since humans
are strategic and often adapt their behaviors in response to the ML system, predictions
that satisfy CF may not lead to a fair future outcome for the individuals. In this paper,
we introduce lookahead counterfactual fairness (LCF), a fairness notion accounting for the
downstream effects of ML models which requires the individual future status to be counter-
factually fair. We theoretically identify conditions under which LCF can be satisfied and
propose an algorithm based on the theorems. We also extend the concept to path-dependent
fairness. Experiments on both synthetic and real data validate the proposed method1.
1 Introduction
The integration of machine learning (ML) into high-stakes domains (e.g., lending, hiring, college admissions,
healthcare) has the potential to enhance traditional human-driven processes. However, it may introduce
the risk of perpetuating biases and unfair treatment of protected groups. For instance, the violence risk
assessment tool SAVRY has been shown to discriminate against males and foreigners (Tolan et al., 2019);
Amazon’s previous hiring system exhibited gender bias (Dastin, 2018); the accuracy of a computer-aided
clinical diagnostic system varies significantly across patients from different racial groups (Daneshjou et al.,
1The code for this paper is available in https://github.com/osu-srml/LCF .
1Published in Transactions on Machine Learning Research (12/2024)
2021). Numerous fairness notions have been proposed in the literature to address unfairness issues, including
unawareness fairness that prevents the explicit use of demographic attributes in the decision-making process,
parity-based fairness that equalizes certain statistics (e.g., accuracy, true/false positive rate) across different
groups (Hardt et al., 2016b; Khalili et al., 2023; 2021b;a; Abroshan et al., 2024), preference-based fairness
that ensures a group of individuals, as a whole, regard the results or consequences they receive from the ML
system more favorably than those received by another group (Zafar et al., 2017; Do et al., 2022). Unlike
these notions that overlook the underlying causal structures among different variables Kusner et al. (2017)
introduced the concept of counterfactual fairness (CF), which requires that an individual should receive a
consistent treatment distribution in a counterfactual world where their sensitive attributes differs. Since
then many approaches have been developed to train ML models that satisfy CF (Chiappa, 2019; Zuo et al.,
2022; Wu et al., 2019; Xu et al., 2019; Ma et al., 2023; Zuo et al., 2023; Abroshan et al., 2022).
However, CF is primarily studied in static settings without considering the downstream impacts ML deci-
sions may have on individuals. Because humans in practice often adapt their behaviors in response to the
ML system, their future status may be significantly impacted by ML decisions (Miller et al., 2020; Shavit
et al., 2020; Hardt et al., 2016a). For example, individuals receiving approvals in loan applications may
have more resources and be better equipped to improve their future creditworthiness (Zhang et al., 2020).
Content recommended in digital platforms can steer consumer behavior and reshape their preferences (Dean
& Morgenstern, 2022; Carroll et al., 2022). As a result, a model that satisfies CF in a static setting without
accounting for such downstream effects may lead to unexpected adverse outcomes.
Although the downstream impacts of fair ML have also been studied in prior works (Henzinger et al., 2023a;
Xie & Zhang, 2024a; Ge et al., 2021; Henzinger et al., 2023b; Liu et al., 2018; Zhang et al., 2020; Xie et al.,
2024), the impact of counterfactually fair decisions remain relatively unexplored. The most related work
to this study is (Hu & Zhang, 2022), which considers sequential interactions between individuals and an
ML system over time and their goal is to ensure ML decisions satisfy path-specific counterfactual fairness
constraint throughout the sequential interactions. However, Hu & Zhang (2022) still focuses on the fairness
of ML decisions but not the fairness of the individual’s actual status. Indeed, it has been well-evidenced that
ML decisions satisfying certain fairness constraints during model deployment may reshape the population
and unintentionally exacerbate the group disparity (Liu et al., 2018; Zhang et al., 2019; 2020). A prime
example is Liu et al. (2018), which studied the lending problem and showed that the lending decisions
satisfying statistical parity or equal opportunity fairness (Hardt et al., 2016b) may actually cause harm to
disadvantaged groups by lowering their future credit scores, resulting in amplified group disparity. Tang
et al. (2023) considered sequential interactions between ML decisions and individuals, where they studied
the impact of counterfactual fair predictions on statistical fairness but their goal is still to ensure parity-based
fairness at the group level.
In this work, we focus on counterfactual fairness evaluated over individual futurestatus (label), which ac-
counts for the downstream effects of ML decisions on individuals. We aim to examine under what conditions
and by what algorithms the disparity between individual future status in factual and counterfactual worlds
can be mitigated after deploying ML decisions. To this end, we first introduce a new fairness notion called
“lookahead counterfactual fairness (LCF)." Unlike the original counterfactual fairness proposed by Kusner
et al. (2017) that requires the ML predictions received by individuals to be the same as those in the coun-
terfactual world, LCF takes one step further by enforcing the individual future status (after responding to
ML predictions) to be the same.
Given the definition of LCF, we then develop algorithms that learn ML models under LCF. To model the
effectsofMLdecisionsonindividuals, wefocusonscenarioswhereindividualssubjecttocertainMLdecisions
adapt their behaviors strategically by increasing their chances of receiving favorable decisions; this can be
mathematically formulated as modifying their features toward the direction of the gradient of the decision
function (Rosenfeld et al., 2020; Xie & Zhang, 2024b). We first theoretically identify conditions under which
an ML model can satisfy LCF, and then develop an algorithm for training ML models under LCF. We also
extend the algorithm and theorems to path-dependent LCF, which only considers unfairness incurred by the
causal effect from the sensitive attribute to the outcome along certain paths.
Our contributions can be summarized as follows:
2Published in Transactions on Machine Learning Research (12/2024)
•We propose lookahead counterfactual fairness (LCF), a novel fairness notion that evaluates coun-
terfactual fairness over individual future status (i.e., actual labels after responding to ML systems).
Unlike the original CF notion that focuses on current ML predictions, LCF accounts for the subse-
quent impacts of ML decisions and aims to ensure fairness over individual actual future status. We
also extend the definition to path-dependent LCF.
•For scenarios where individuals respond to ML models by changing features toward the direction
of the gradient of decision functions, we theoretically identify conditions under which an ML model
can satisfy LCF. We further develop an algorithm for training ML models under LCF.
•We conduct extensive experiments on both synthetic and real data to validate the proposed algo-
rithm. Results show that compared to conventional counterfactual fair predictors, our method can
improve disparity with respect to the individual actual future status.
2 Related Work
Causal fairness has been explored in many aspects in recent years’ research. Kilbertus et al. (2017) point out
that no observational criterion can distinguish scenarios determined by different causal mechanisms but have
the same observational distribution. They propose the definition of unresolved discrimination and proxy
discrimination based on the intuition that some of the paths from the sensitive attribute to the prediction
can be acceptable. Nabi & Shpitser (2018) argue that a fair causal inference on the outcome can be obtained
by solving a constrained optimization problem. These notions are based on defining constraints on the
interventional distributions.
Counterfactual fairness (Kusner et al., 2017) requires the prediction on the target variable to have the same
distribution in the factual world and counterfactual world. Many extensions of traditional statistical fairness
notions such as Fair on Average Causal Effect (FACE) (can be regarded as counterfactual demographic
parity) (Khademi et al., 2019), Fair on Average Causal Effect on the Treated (FACT) (can be regarded as
counterfactual equalized odds) (Khademi et al., 2019), and CAPI fairness (counterfactual individual fairness)
(Ehyaei et al., 2024) have been proposed. Path-specific counterfactual fairness (Chiappa, 2019) considered
the path-specific causal effect. However, the notions are focused on fairness in static settings and do not
consider the future effect. Most recent work about counterfactual fairness is about achieving counterfactual
fairness in different applications, such as graph data (Wang et al., 2024a;b), medical LLMs (Poulain et al.,
2024), or software debuging (Xiao et al., 2024). Some literature try to use counterfactual fairness for
explanations (Goethals et al., 2024). Connecting counterfactual fairness with group fairness notions (Anthis
& Veitch, 2024) or exploring counterfactual fairness with partial knowledge about the causal model (Shao
et al., 2024; Pinto et al., 2024; Duong et al., 2024; Zhou et al., 2024) are also receiving much attention.
Machado et al. (2024) propose an idea of interpretable counterfactual fairness by deriving counterfactuals
with optimal transports (De Lara et al., 2024). While extending the definition of counterfactual fairness to
include the downstream effects has been less focused on.
Several studies in the literature consider the downstream effect on fairness of ML predictions. There are
two kinds of objectives in the study of downstream effects: ensuring fair predictions in the future or fair
true status in the future. The two most related works to our paper are Hu & Zhang (2022) and Tang
et al. (2023). Hu & Zhang (2022) consider the problem of ensuring ML predictions satisfy path-specific
counterfactual fairness over time after interactions between individuals and an ML system. Tang et al.
(2023) study the impact on the future true status of the ML predictions. Even though they considered
the impact of a counterfactually fair predictor, their goal is to ensure parity-based fairness. Therefore, the
current works lack the consideration of ensuring counterfactual fairness on the true label after the individual
responds to the current ML prediction. Our paper is aimed at solving this problem.
3 Problem Formulation
Consider a supervised learning problem with a training dataset consisting of triples (A,X,Y ), where
A∈ Ais a sensitive attribute distinguishing individuals from multiple groups (e.g., race, gender),
3Published in Transactions on Machine Learning Research (12/2024)
X= [X1,X2,...,Xd]T∈Xis ad-dimensional feature vector, and Y∈Y ⊆ Ris the target variable in-
dicating individual’s underlying status (e.g., Yin lending identifies an applicant’s ability to repay the loan,
Yin healthcare may represent patients’ insulin spike level). The goal is to learn a predictor from training
data that can predict Ygiven inputs AandX. Let ˆYdenote the output of the predictor.
We assume (A,X,Y )is associated with a structural causal model (SCM) (Pearl et al., 2000) M= (V,U,F ),
whereV= (A,X,Y )represents observable variables, Uincludes unobservable (exogenous) variables that
are not caused by any variable in V, andF={f1,f2,...,fd+2}is a set ofd+ 2functions called structural
equations that determines how each observable variable is constructed. More precisely, we have the following
structural equations,
Xi=fi(pai,Upai),∀i∈{1,···,d},
A=fA(paA,UpaA),
Y=fY(paY,UpaY), (1)
wherepai⊆V,paA⊆VandpaY⊆Vare observable variables that are the parents of Xi,A, and
Y, respectively. Upai⊆Uare unobservable variables that are the parents of Xi. Similarly, we denote
unobservable variables UpaA⊆UandUpaY⊆Uas the parents of AandY, respectively.
3.1 Background: counterfactuals
If the probability density functions of unobserved variables are known, we can leverage the structural equa-
tions in SCM to find the marginal distribution of any observable variable Vi∈Vand even study how
intervening certain observable variables impacts other variables. Specifically, the intervention on vari-
ableViis equivalent to replacing structural equation Vi=fi(pai,Upai)with equation Vi=vfor somev.
Given new structural equation Vi=vand other unchanged structural equations, we can find out how the
distribution of other observable variables changes as we change value v.
In addition to understanding the impact of an intervention, SCM can further facilitate counterfactual
inference , which aims to answer the question “ what would be the value of YifZhad taken value zin the
presence of evidence O=o(bothYandZare two observable variables)? ” The answer to this question is
denoted by YZ←z(U)withUfollowing conditional distribution of Pr{U=u|O=o}. GivenU=uand
structural equations F, the counterfactual value of Ycan be computed by replacing the structural equation
ofZwithZ=zand replacing Uwithuin the rest of the structural equations. Such counterfactual is
typically denoted by YZ←z(u). Given evidence O=o, the distribution of counterfactual value YZ←z(U)can
be calculated as follows,2
Pr{YZ←z(U) =y|O=o}=/summationdisplay
uPr{YZ←z(u) =y}Pr{U=u|O=o}. (2)
Example 3.1 (Law school success ).Consider two groups of college students distinguished by gender
A∈{0,1}whose first-year average (FYA) in college is denoted by Y. The FYA of each student is causally
related to (observable) grade-point average (GPA) before entering college XG, entrance exam score (LSAT)
XL, and gender A. Suppose there are two unobservable variables U= (UA,UXY), e.g.,UXYmay be
interpreted as the student’s knowledge. Consider the following structural equations:
A=UA, X G=bG+wA
GA+UXY,
XL=bL+wA
LA+UXY, Y =bF+wA
FA+UXY,
where (bG,wA
G,bL,wA
L,bF,wA
F)are know parameters of the causal model. Given observation XG= 1,A= 0,
the counterfactual value can be calculated with an abduction-action-prediction procedure Glymour et al.
(2016): (i) abduction thatfindsposteriordistribution Pr{U=u|XG= 1,A= 0}. Here,wehave UXY= 1−bG
andUA= 0with probability 1; (ii)actionthat performs intervention A= 1by replacing structural equations
2Given structural equations equation 1 and the marginal distribution of U,Pr{U=u,O =o}can be calculated using the
Change-of-Variables Technique and the Jacobian factor. As a result, Pr{U=u|O=o}=Pr{U=u, O=o}
Pr{O=o}can also be calculated.
4Published in Transactions on Machine Learning Research (12/2024)
ofA; (iii)prediction that computes distribution of YA←1(U)givenXG= 1,A= 0using new structural
equations and the posterior. We have:
YA←1(U) =bf+wA
F+ 1−bGwith probability 1.
3.2 Counterfactual Fairness
Counterfactual Fairness (CF) was first proposed by Kusner et al. (2017); it requires that for an individual
with (X=x,A=a), the prediction ˆYin the factual world should be the same as that in the counterfactual
world in which the individual belongs to a different group. Mathematically, CF is defined as follows: ∀a,ˇa∈
A,X∈X,y∈Y,
Pr/parenleftig
ˆYA←a(U) =y|X=x,A=a/parenrightig
= Pr/parenleftig
ˆYA←ˇa(U) =y|X=x,A=a/parenrightig
,
While the CF notion has been widely used in the literature, it does not account for the downstream impacts
of ML prediction ˆYon individuals in factual and counterfactual worlds. To illustrate the importance of
considering such impacts, we provide an example below.
Example 3.2. Consider automatic lending where an ML model is used to decide whether to issue a loan
to an applicant based on credit score Xand sensitive attribute A. As highlighted in Liu et al. (2018),
issuing loans to unqualified people who cannot repay the loan may hurt them by worsening their future
credit scores. Assume an applicant in the factual world is qualified for the loan and does not default. But
in a counterfactual world where the applicant belongs to another group, he/she is not qualified. Under
counterfactually fair predictions, both individuals in the factual and counterfactual worlds should receive
the loan with the same probability. Suppose both are issued a loan, then the one in the counterfactual world
would have a worse credit score in the future. Thus, it is crucial to consider the downstream effects when
learning a fair ML model.
3.3 Characterize downstream effects
Motivated by Example 3.2, this work studies CF in a dynamic setting where the deployed ML decisions may
affect individual behavior and change their future features and statuses. Formally, let X′andY′denote an
individual’s future feature vector and status, respectively. We use an individual response rto capture the
impact of ML prediction ˆYon individuals, as defined below.
Definition 3.1 (Individual response) .An individual response r:U×V×Y ∝⇕⊣√∫⊔≀→U×V is a map from
the current exogenous variables U∈U, endogenous variables V∈V, and prediction ˆY∈Yto the future
exogenous variables U′and endogenous variables V′.
One way to tackle the issue in Example 3.2 is to explicitly consider the individual response and impose
a fairness constraint on future status Y′instead of the prediction ˆY. We call such a fairness notion the
Lookahead Counterfactual Fairness (LCF) and present it in Section 4.
4 Lookahead Counterfactual Fairness
Figure 1: Causal graph in
Example 4.1.Weconsiderthefairnessovertheindividual’sfutureoutcome Y′. Givenstruc-
tural causal model M= (U,V,F ), individual response r, and data (A,X,Y ),
we define lookahead counterfactual fairness below.
Definition 4.1. We say an ML model satisfies lookahead counterfactual
fairness (LCF) under a response rif the following holds ∀a,ˇa∈ A,X∈
X,y∈Y:
Pr (Y′
A←a(U) =y|X=x,A=a) = Pr (Y′
A←ˇa(U) =y|X=x,A=a),(3)
LCF implies that the subsequent consequence of ML decisions for a given
individual in the factual world should be the same as that in the counterfactual world where the individual
5Published in Transactions on Machine Learning Research (12/2024)
belongs to other demographic groups. Note that CF may contradict LCF: even under counterfactually fair
predictor, individuals in the factual and counterfactual worlds may end up with very different future statuses.
We show this with an example below.
Example 4.1. Consider the causal graph in Figure 1 and the structural functions as follows:
X=fX(U1) =U1, Y =fY(U2,X,A ) =U2+X+A,
U′
1=r(U1,ˆY) =U1+∇U1ˆY, U′
2=r(U2,ˆY) =U2+∇U2ˆY,
X′=fX(U′
1) =U′
1, Y′=fY(U′
2,X′,A) =U′
2+X′+A.
Based on Kusner et al. (2017), a predictor that only uses U1andU2as input is counterfactually fair.3
Therefore, ˆY=h(U1,U2)satisfies CF. Let U1andU2be uniformly distributed over [−1,1]. Note that
the response r(U1,ˆY)andr(U2,ˆY)imply that individuals make efforts to change feature vectors through
changing the unobservable variables, which results in higher ˆYin the future. It is easy to see that a CF
predictorh(U1,U2) =U1+U2minimizes the MSE loss E{(Y−ˆY)2}ifA∈{− 1,1}andPr{A= 1}= 0.5.
However, since∇U1ˆY=∇U2ˆY= 1, we have:
Pr (Y′
A←a(U) =y|X=x,A=a) =δ(y−a−x−2),
Pr (Y′
A←ˇa(U) =y|X=x,A=a) =δ(y−ˇa−x−2),
whereδ(y) = 1ify= 0andδ(y) = 0otherwise. It shows that although the decisions in the factual and
counterfactual worlds are the same, the future statuses Y′are still different and Definition 4.1 does not hold.
Theorem 4.1 below identifies more general scenarios under which LCF can be violated with a CF predictor.
Theorem 4.1 (Violation of LCF under CF predictors ).Consider a causal model M= (U,V,F )and
individual response rin the following form:
U′=r(U,ˆY).
If the response ris a function and the status Yin factual and counterfactual worlds have different distribu-
tions, i.e.,
Pr(YA←a(U) =y|X=x,A=a)̸= Pr(YA←ˇa(U) =y|X=x,A=a),
imposing any arbitrary model ˆYthat satisfies CF will violate LCF, i.e.,
Pr(Y′
A←a(U) =y|X=x,A=a)̸= Pr(Y′
A←ˇa(U) =y|X=x,A=a).
5 Learning under LCF
This section introduces an algorithm for learning a predictor under LCF. In particular, we focus on a special
case with the causal model and the individual response defined below.
Given sets of unobservable variables U={U1,...,Ud,UY}and observable variables {X1,...,Xd,A,Y}, we
consider causal model with the following structural functions:
Xi=fi(Ui,A), Y =fY(X1,...,Xd,UY), (4)
wherefiis an invertible function4, andfYis invertible w.r.t. UY. After receiving the ML prediction ˆY, the
individual’s future features X′and status Y′change accordingly. Specifically, we consider scenarios where
individual unobservable variables Uchange based on the following
U′
i=ri(Ui,ˆY) =Ui+η∇UiˆY,∀i∈{1,...,d}
U′
Y=rY(UY,ˆY) =UY+η∇UYˆY, (5)
3Note thatU1andU2can be generated for each sample (X,A). See Section 4.1 of (Kusner et al., 2017) for more details.
4Several works in causal inference also consider invertible structural function, e.g., bijective causal models introduced in
Nasr-Esfahany et al. (2023).
6Published in Transactions on Machine Learning Research (12/2024)
and the future attributes X′
iand status Y′also change accordingly, i.e.,
X′
i=fi(U′
i,A),
Y′=fY(X′
1,...,X′
d,U′
Y). (6)
The above scenario implies that individuals respond to ML model by strategically moving features toward
thedirection that increases their chances of receiving favorable decisions , step sizeη>0controls
the magnitude of data change and can be interpreted as the effort budget individuals have on changing their
data. Note that this type of response has been widely studied in strategic classification literature (Rosenfeld
et al., 2020; Hardt et al., 2016a). The above process with d= 2is visualized in Figure 2.
Figure 2: A causal graph and individual
responses with two features X1,X2. The
black arrows represent the connections de-
scribed in structural functions. The red ar-
rows represent the response process. The
green dash arrows are the potential connec-
tion to prediction ˆY.OurgoalistotrainanMLmodelunderLCFconstraint. Before
presenting our method, we first define the notion of counter-
factual random variables.
Definition 5.1 (Counterfactual random variable) .Letxand
abe realizations of random variables XandA, and ˇa̸=a. We
sayˇX:=XA←ˇa(U)and ˇY:=YA←ˇa(U)are the counterfactual
random variables associated with (x,a)ifUfollows the condi-
tional distribution Pr{U|X=x,A=a}as given by the causal
modelM. The realizations of ˇX,ˇYare denoted by ˇxandˇy.
The following theorem constructs a predictor gthat satisfies
LCF, i.e., deploying the predictor gin Theorem 5.1 ensures
the future status Y′is counterfactually fair.
Theorem 5.1 (Predictor with perfect LCF) .Consider causal
modelM = (U,V,F ), whereU={UX,UY},UX=
[U1,U2,...,Ud]T,V={A,X,Y},X= [X1,X2,...,Xd]T, and
the structural equations are given by,
X=α⊙UX+βA, Y =wTX+γUY, (7)
whereα= [α1,α2,...,αd]T,β= [β1,β2,...,βd]T,w= [w1,w2,..,wd]T, and⊙denotes the element wise
production. Then, the following predictor satisfies LCF,
g(ˇY,U) =p1ˇY2+p2ˇY+p3+h(U), (8)
wherep1=T
2withT:=1
η(||w⊙α||2
2+γ2), and ˇYis the counterfactual random variable associated with Y. Here,
p2andp3and function h(.)are arbitrary and can be trained to improve prediction performance.
Proof Sketch. For any given x,a, we can find the conditional distribution of U. For a sample udrawn from
the distribution, we can compute x,ˇx,yandˇy. Then we have
g(ˇy,u) =p1ˇy2+p2ˇy+p3+h(u).
From this we can compute the gradient of gw.r.t.uX,uY. With response function r, we can get u′
Xand
u′
Y. With the structural functions, we can know y′andˇy′. So, we have
|ˇy′−y′|=/vextendsingle/vextendsingle/vextendsingle/vextendsingleˇy−y+1
T/parenleftbigg∂g(y,uX,uY)
∂y−∂g(ˇy,uX,uY)
∂ˇy/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
Because we know that∂g(y,uX,uY)
∂y= 2p1y=Ty, we have
|ˇy′−y′|=|y−ˇy+ ˇy−y|= 0.
With law of total probability, we have LCF satisfied.
7Published in Transactions on Machine Learning Research (12/2024)
The above theorem implies that gshould be constructed based on the counterfactual random variable ˇY
andU. Even though Uis unobserved, it can be obtained from the inverse of structural equations. Quantity
Tin Theorem 5.1 depends on the step size ηin individual response, and parameters α,γ,win structural
functions. When p1=T
2, we can achieve perfect LCF.
It is worth noting that Definition 4.1 can be a very strong constraint and imposing Y′
A←a(U)andY′
A←ˇa(U)
to have the same distribution may degrade the performance of the predictor significantly. To tackle this, we
may consider a weaker version of LCF.
Definition 5.2 (Relaxed LCF) .We say Relaxed LCF holds if ∀(a,ˇa)∈A2,a̸= ˇa,X∈X,y∈Y,we have:
Pr/parenleftig/braceleftig
|Y′
A←a(U)−Y′
A←ˇa(U)|<|YA←a(U)−YA←ˇa(U)|/bracerightig/vextendsingle/vextendsingleX=x,A=a/parenrightig
= 1. (9)
Definition 5.2 implies that after individuals respond to ML model, the difference between the future status Y′
infactualandcounterfactualworldsshouldbesmallerthanthedifferencebetweenoriginalstatus Yinfactual
and counterfactual worlds. In other words, it means that the disparity between factual and counterfactual
worlds must decrease over time. In Section 7, we empirically show that constraint in equation 9 is weaker
than the constraint in equation 3 and can lead to a better prediction performance.
Corollary 5.1 (Relaxed LCF with predictor in equation 8) .Consider the same causal model defined in
Theorem 5.1 and the predictor defined in equation 8. Relaxed LCF holds if p1∈(0,T).
Proof Sketch. Whenp1∈(0,T), from
|ˇy′−y′|=/vextendsingle/vextendsingle/vextendsingle/vextendsingleˇy−y+1
T/parenleftbigg∂g(y,uX,uY)
∂y−∂g(ˇy,uX,uY)
∂ˇy/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle,
we have
|ˇy′−y′|=|y−ˇy+2p1
T(ˇy−y)|.
Therefore|ˇy′−y′|<|ˇy−y|. With law of total probability, the Relaxed LCF is satisfied.
Apart from relaxing p1in predictor as shown in equation 8, we can also relax the form of the predictor to
satisfy Relaxed LCF, as shown in Theorem 5.2.
Theorem 5.2 (Predictor under Relaxed LCF) .Consider the same causal model defined in Theorem 5.1. A
predictorg(ˇY,U)satisfies Relaxed LCF if ghas the following three properties:
(i)g(ˇy,u)is strictly convex in ˇy.
(ii)g(ˇy,u)can be expressed as g(ˇy,u) =g1(ˇy) +g2(u).
(iii) The derivative of g(ˇy,u)w.r.t. ˇyisK-Lipschitz continuous in ˇywithK <2
η(||w⊙α||2
2+γ2), i.e.,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂g(ˇy1,u)
∂ˇy−∂g(ˇy2,u)
∂ˇy/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤K|ˇy1−ˇy2|.
Proof Sketch. Wheng(ˇy,u)satisfies property (ii), we can prove that
|ˇy′−y′|=/vextendsingle/vextendsingle/vextendsingle/vextendsingleˇy−y+1
T/parenleftbigg∂g(y,uX,uY)
∂y−∂g(ˇy,uX,uY)
∂ˇy/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
still holds. Because of properties (i), we have
(ˇy−y)/parenleftbigg∂g(y,uX,uY)
∂y−∂g(ˇy,uX,uY)
∂y/parenrightbigg
<0.
Therefore, when/vextendsingle/vextendsingle/vextendsingle∂g(y,uX,uY)
∂y−∂g(ˇy,uX,uY)
∂y/vextendsingle/vextendsingle/vextendsingle<2|ˇy−y|,|y′−ˇy′|<|y−ˇy|, which is guaranteed by property
(iii).
8Published in Transactions on Machine Learning Research (12/2024)
Theorems 5.1 and 5.2 provide insights on designing algorithms to train a predictor with perfect or Relaxed
LCF. Specifically, given training data D={(x(i),y(i),a(i))}n
i=1, we first estimate the structural equations.
Then, we choose a parameterized predictor gthat satisfies the conditions in Theorem 5.1 or 5.2. An example
is shown in Algorithm 1, which finds an optimal predictor in the form of g(ˇy,u) =p1ˇy2+p2ˇy+p3+hθ(u)
under LCF, where p1=1
2η(||w⊙α||2
2+γ2),θis the training parameter for function h, andp2,p3are two other
training parameters. Under Algorithm 1, we can find the optimal values for p2,p3,θusing training data D.
If we only want to satisfy Relaxed LCF (Definition 5.2), p1can be a training parameter with 0<p1<T.
Algorithm 1 Training a predictor with perfect LCF
Input:Training dataD={(x(i),y(i),a(i))}n
i=1, response parameter η.
1:Estimate the structural equations 7 using Dto determine parameters α,β,w, andγ.
2:For each data point (x(i),y(i),a(i)), drawmsamples/braceleftbig
u(i)[j]/bracerightbigm
j=1from conditional distribution Pr{U|X=
x(i),A=a(i)}and generate counterfactual ˇy(i)[j]associated with u(i)[j]based on structural equations 7.
3:Computep1←1
2η(||w⊙α||2
2+γ2).
4:Solve the following optimization problem,
ˆp2,ˆp3,ˆθ= arg min
p2,p3,θ1
mnn/summationdisplay
i=1m/summationdisplay
j=1l/parenleftig
g/parenleftig
ˇy(i)[j],u(i)[j]/parenrightig
,y(i)/parenrightig
where
g/parenleftig
ˇy(i)[j],u(i)[j]/parenrightig
=p1/parenleftig
ˇy(i)[j]/parenrightig2
+p2ˇy(i)[j]+p3+hθ(u),
θis a parameter for function h, andlis a loss function.
Output: ˆp2,ˆp3,ˆθ
It is worth noting that the results in Theorems 5.1 and 5.2 are for linear causal models. When the causal
model is non-linear, it is hard to construct a model satisfying perfect LCF in Definition 4.1. Nonetheless, we
can still show that it is possible to satisfy Relaxed LCF (Definition 5.2) for certain non-linear causal models.
Theorem 5.3 below focuses on a special case when Xis not linearly dependent on AandUXand it identifies
the condition under which Relaxed LCF can be guaranteed.
Theorem 5.3. Consider a bijective causal model M= (U,V,F ), whereUis a scalar exogenous variable,
V={A,X,Y}, and the structural equations X=fX(A,U)∈Rd,Y=fY(X,U )∈Rcan be written in the
form of
Y=f(A,U) =fY(fX(A,U),U) =˜f(U+u0(A))
for some function ˜f, whereu0(A)is an arbitrary function of A. Define function Γ(s) = ˜f(s)d˜f(s)
dsas the
multiplication of ˜f(s)and its derivative. If ˜f(s)has the following properties:
•˜f(s)is monotonic and strictly concave;
•If˜f(s)≥˜f(s′)then ˜f(s)˜f′(s)≥˜f(s′)˜f′(s′),∀s,s′;
•Γ(s)≥0and there exists constant M > 0such that Γ(s)isM-Lipschitz continuous.
Then, the following predictor satisfies Relaxed LCF,
g(ˇY,U) =p1ˇY2+p2+h(U),
wherep1∈(0,1
ηM],p2are learnable parameters, ˇYis the counterfactual random variable associated with Y,
andh(U)can be an arbitrary monotonic function that is increasing (resp. decreasing) when ˜f(s)is increasing
(resp. decreasing).
9Published in Transactions on Machine Learning Research (12/2024)
Proof Sketch. For a sample udrawn from the conditional distribution of UgivenX=x,A =a, we can
computey′,ˇy′, and get
ˇy=˜f(ϕ1), y =˜f(ϕ2),ˇy′=˜f(ϕ3), y′=˜f(ϕ4).
The specific formulas of ϕ1,ϕ2,ϕ3andϕ4can be seen in the full proof. With the mean value theorem, we
know that
|y−ˇy|=˜f′(c1)|ϕ1−ϕ2|,
wherec1∈(ϕ1,ϕ2), and
|y′−ˇy′|=˜f′(c2)|ϕ3−ϕ4|,
wherec2∈(ϕ3,ϕ4). The three properties ensure that
˜f(c1)>˜f(c2),|ϕ1−ϕ2|≥|ϕ3−ϕ4|.
Therefore,|ˇy′−y′|<|ˇy−y|. With law of total probability, we have the Relaxed LCF satisfied.
Theorems 5.2 and 5.3 show that designing a predictor under Relaxed LCF highly depends on the form of
causal structure and structural equations. To wrap up this section, we would like to identify conditions
under which Relaxed LCF holds in a causal graph that Xis determined by the product of UXandA.
Theorem 5.4. Consider a non-linear causal model M= (U,V,F ), whereU={UX,UY},UX=
[U1,U2,...,Ud]T,V={A,X,Y},X= [X1,X2,...,Xd]T,A∈{a1,a2}is a binary sensitive attribute. As-
sume that the structural functions are given by,
X=A·(α⊙UX+β), Y =wTX+γUY, (10)
whereα= [α1,α2,...,αd]T,β= [β1,β2,...,βd]T, and⊙denotes the element wise production. A predictor
g(ˇY)satisfies Relaxed LCF if gand the causal model have the following three properties.
(i) The value domain of Asatisfiesa1a2>0.
(ii)g(ˇy)is strictly convex.
(iii) The derivate of g(ˇy)isK-Lipschitz continuous with K≤2
η(a1a2||w⊙α||2
2+γ2), i.e.,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂g(ˇy1)
∂ˇy−∂g(ˇy2)
∂ˇy/vextendsingle/vextendsingle/vextendsingle/vextendsingle<K|ˇy1−ˇy2|.
Proof Sketch. For a sample udrawn from the conditional distribution of UgivenX=x,A =a, we can
compute the y′andˇy′and get
|y′−ˇy′|=|y−ˇy+ ∆|.
The definition of ∆can be seen in the full proof. Because property (i) and property (ii),
(y−ˇy)∆<0.
Property (iii) ensures that |∆|<2|y−ˇy|. Therefore,|ˇy′−y′|<|ˇy−y|.
Although the structural equation associated with Yis still linear in XandUY, we emphasize that such a
linearassumptionhasbeenverycommonintheliteratureduetothecomplexnatureofstrategicclassification
Zhang et al. (2022); Liu et al. (2020); Bechavod et al. (2022). For instance, Bechavod et al. (2022) assumed
the actual status of individuals is Y=βX, a linear function of features X. Zhang et al. (2022) assumed
thatXitself may be non-linear in some underlying traits of the individuals, but the relationship between X
andP(Y= 1|X)is still linear. Indeed, due to the individual’s strategic response, conducting the theoretical
analysis accounting for such responses can be highly challenging. Nonetheless, it is worthwhile extending
LCF to non-linear settings and we leave this for future works.
10Published in Transactions on Machine Learning Research (12/2024)
6 Path-dependent LCF
An extension of counterfactual fairness called path-dependent fairness has been introduced in Kusner et al.
(2017). In this section, we also want to introduce an extension of LCF called path-dependent LCF. We will
also modify Algorithm 1 to satisfy path-dependent LCF.
We start by introducing the notion of path-dependent counterfactuals. In a causal model associated with a
causal graphG, we denotePGAas a set of unfair paths from sensitive attribute AtoY. We define XPc
GAas the set of features that are not present in any of the unfair paths. Under observation X=x,A=a, we
callYA←ˇa,XPc
GA←xPc
GA(U)path-dependent counterfactual random variable for Y, and its distribution can be
calculated as follows:
Pr{YA←ˇa,XPc
GA←xPc
GA(U) =y|X=x,A=a}=/summationdisplay
uPr{YA←ˇa,XPc
GA←xPc
GA(u) =y}Pr{U=u|X=x,A=a}.
For simplicity, we use ˇYPDandˇyPDto represent a path-dependent counterfactual and the corresponding
realization. That is, ˇYPD=YA←ˇa,XPc
GA←xPc
GA(U)whereUfollows Pr{U|X=x,A =a}. We consider the
same kind of causal model described in Section 5, the future attributes X′and outcome Y′are determined
by equation 5 and equation 6. We formally define the path-dependent LCF in the following definition.
Definition 6.1. We say an ML model satisfies path-dependent lookahead counterfactual fairness w.r.t. the
unfair path setPGAif the following holds ∀a,ˇa∈A,X∈X,y∈Y:
Pr/parenleftbigg
ˆY′
A←a,XPc
GA←xPc
GA(U) =y/vextendsingle/vextendsingle/vextendsingleX=x,A=a/parenrightbigg
= Pr/parenleftbigg
ˆY′
A←ˇa,XPc
GA←xPc
GA(U) =y/vextendsingle/vextendsingle/vextendsingleX=x,A=a/parenrightbigg
.
Then we have the following theorem.
Theorem 6.1. Consider a causal model and structural equations defined in Theorem 5.1. If we denote the
features on unfair path as XPGAand remaining features as XPc
GA, we can re-write structural equations as
XPGA=αPGA⊙UXPGA+βPGAA,
XPc
GA=αPc
GA⊙UXPc
GA+βPc
GAA,
Y=wT
PGAXPGA+wT
Pc
GAXPc
GA+γUY
Then, the following predictor satisfies path-dependent LCF,
g(ˇYPD,U) =p1ˇY2
PD+p2ˇYPD+p3+h(U),
wherep1=T
2with
T:=1
η(||wPGA⊙αPGA||2
2+||wPc
GA⊙αPc
GA||2
2+γ2),
p2andp3are learnable parameters to improve prediction performance and his an arbitary function.
Proof Sketch. Consider a sample u, we can compute xPc
GA,xPGA,ˇxPGA,yandˇyPD. Then we can get a similar
equation about|y′−ˇy′
PD|like Theorem 5.1. Then we can get |y′−ˇy′
PD|= 0from the form of g. With law
of total probability, we have the path-dependent LCF satisfied.
7 Experiment
We conduct experiments on both synthetic and real data to validate the proposed method.
11Published in Transactions on Machine Learning Research (12/2024)
7.1 Synthetic Data
We generate the synthetic data based on the causal model described in Theorem 5.1, where we set d= 10
and generated 1000data points. We assume UXandUYfollow the uniform distribution over [0,1]and the
sensitive attribute A∈{0,1}is a Bernoulli random variable with Pr{A= 0}= 0.5. Then, we generate X
andYusing the structural functions described in Theorem 5.1.5Based on the causal model, the conditional
distribution of UXandUYgivenX=x,A=aare as follows,
UX|X=x,A=a∼δ/parenleftigx−βa
α/parenrightig
, UY|X=x,A=a∼Uniform(0,1). (11)
Baselines. We used two baselines for comparison: (i) Unfair predictor (UF) is a linear model without
fairnessconstraintimposed. Ittakesfeature Xasinputandpredicts Y. (ii)Counterfactualfairpredictor
(CF)only takes the unobservable variables Uas the input and was proposed by Kusner et al. (2017).
Implementation Details. To find a predictor satisfying Definition 4.1, we train a predictor in the form
of Eq. 8. In our experiment, h(u)is a linear function. To train g(ˇy,u), we follows Algorithm 1 with m= 100.
We split the dataset into the training/validation/test set at 60%/20%/20%ratio randomly and repeat the
experiment 5 times. We use the validation set to find the optimal number of training epochs and the learning
rate. Based on our observation, Adam optimization with a learning rate equal to 10−3and2000epochs gives
us the best performance.
Metrics. We use three metrics to evaluate the methods. To evaluate the performance, we use the mean
squared error (MSE). Given a dataset {x(i),a(i),y(i)}n
i=1, for eachx(i)anda(i), we generate m= 100values
ofu(i)[j]from the posterior distribution. MSE can be estimated as follows,6
MSE =1
mnn/summationdisplay
i=1m/summationdisplay
j=1/vextenddouble/vextenddouble/vextenddoubley(i)−ˆy(i)[j]/vextenddouble/vextenddouble/vextenddouble2
, (12)
where ˆy(i)[j]is the prediction for data (x(i),a(i),u(i)[j]). Note that for the UF baseline, the prediction does
not depend on u(i)[j]. Therefore, ˆy(i)[j]does not change by jfor the UF predictor. To evaluate fairness, we
define a metric called average future causal effect (AFCE),
AFCE =1
mnn/summationdisplay
i=1m/summationdisplay
j=1/vextendsingle/vextendsingle/vextendsingley′(i)[j]−ˇy′(i)[j]/vextendsingle/vextendsingle/vextendsingle.
It is the average difference between the factual and counterfactual future outcomes. To compare |Y−ˇY|
with|Y′−ˇY′|under different algorithms, we use the unfairness improvement ratio (UIR) defined below.
The larger UIRimplies a higher improvement in disparity.
UIR =/parenleftigg
1−/summationtextn
i=1/summationtextm
j=1|y′(i)[j]−ˇy′(i)[j]|/summationtextn
i=1/summationtextm
j=1|y(i)[j]−ˇy(i)[j]|/parenrightigg
×100%.
Results. Table 1 illustrates the results when we set η= 10andp1=T
2. The results show that our
method can achieve perfect LCF with p1=T
2. Note that in our experiment, the range of Yis[0,3.73],
and our method and UF can achieve similar MSE. Moreover, our method achieves better performance than
the CF method because ˇYincludes useful predictive information and using it in our predictor can improve
performance and decrease the disparity simultaneously. Because both CF and UF do not take into account
future outcome Y′,|Y′−ˇY′|is similar to|Y−ˇY|, leading to UIR = 0. Based on Corollary 5.1, the value of
p1can impact the strength of fairness. We examine the tradeoff between accuracy and fairness by changing
5The exact values for parameters α,β,wandγcan be found in the Appendix B.
6Check Section 4.1 of Kusner et al. (2017) for details on why equation 12 is an empirical estimate of MSE.
12Published in Transactions on Machine Learning Research (12/2024)
Table 1: Results on Synthetic Data: comparison with two baselines, unfair predictor (UF) and counterfactual
fair predictor (CF), in terms of accuracy (MSE) and lookahead counterfactual fairness (AFCE, UIR).
Method MSE AFCE UIR
UF 0.036 ±0.003 1.296±0.000 0%±0
CF 0.520 ±0.045 1.296±0.000 0%±0
Ours (p1=T/2) 0.064±0.001 0.000±0.0016 100%±0
0.06 0.07 0.08 0.09 0.10 0.11
MSE0.00.20.40.60.81.01.2AFCE
=10
=5
=1
(a) Accuracy-fairness trade-off of predictor Eq. 8 on
synthetic data: we vary p1fromT
512toT
2under dif-
ferentη. Whenp1=T
2, we attain perfect LCF.
0.50 0.55 0.60 0.65
MSE0.0000.0050.0100.0150.020FCE
=15
=10
=5
(b) Accuracy-fairness trade-off on the law school
dataset: we vary p1fromT
512toT
2under different
η. Whenp1=T
2, we attain perfect LCF.
the value of p1fromT
512toT
2under different η. Figure 3a shows the MSE as a function of AFCE. The results
show that when η= 1we can easily control the accuracy-fairness trade-off in our algorithm by adjusting p1.
Whenηbecomes large, we can get a high LCF improvement while maintaining a low MSE. To show how
our method impacts a specific individual, we choose the first data point in our test dataset and plot the
distribution of factual future status Y′and counterfactual future status ˇY′for this specific data point under
different methods. Figure 4 illustrates such distributions. It can be seen in the most left plot that there is
an obvious gap between factual Yand counterfactual ˇY. Both UF and CF can not decrease this gap for
future outcome Y′. However, with our method, we can observe that the distributions of Y′and ˇY′become
closer to each other. When p1=T
2(the most right plot in Figure 4), the two distributions become the same
in the factual and counterfactual worlds.
7.2 Real Data: The Law School Success Dataset
Figure 5: Causal model for
the Law School Dataset.We further measure the performance of our proposed method using the Law
SchoolAdmissionDatasetWightman(1998). Inthisexperiment, theobjective
istoforecastthefirst-yearaveragegrades(FYA)ofstudentsinlawschoolusing
their undergraduate GPA and LSAT scores.
Dataset. The dataset consists of 21,791 records. Each record is character-
ized by 4 attributes: Sex ( S), Race (R), UGPA (G), LSAT (L), and FYA
(F). Both Sex and Race are categorical in nature. The Sex attribute can be
either male or female, while Race can be Amerindian, Asian, Black, Hispanic,
Mexican, Puerto Rican, White, or other. The UGPA is a continuous variable
ranging from 0to4. LSAT is an integer-based attribute with a range of [0,60].
FYA, which is the target variable for prediction, is a real number ranging from
−4to4(it has been normalized). In this study, we consider Sas the sensitive
attribute, while R,G, andLare treated as features.
13Published in Transactions on Machine Learning Research (12/2024)
0.0 2.5
Y0.00.51.01.5densityCurrent
4 6 8
Y/prime0.00.51.01.5UF
10 12
Y/prime0.00.51.01.52.0CF
0 2 4
Y/prime0.000.250.500.751.001.25LCF(p1=T
4)
0 2 4
Y/prime0.00.20.40.60.8LCF(p1=T
2)
factual
counter
factual
Figure 4: Density plot for Y′and ˇY′in synthetic data. For a chosen data point, we sampled a batch of U
under the conditional distribution of it and plot the distribution of Y′and ˇY′.
Causal Model. We adopt the causal model as presented in Kusner et al. (2017), which can be visualized in
Figure 5. In this causal graph, Krepresents an unobserved variable, which can be interpreted as knowledge .
Thus, the model suggests that students’ grades (UGPA, LSAT, FYA) are influenced by their sex, race, and
underlying knowledge. We assume that the prior distribution for Kfollows a normal distribution, denoted
asN(0,1). We adopt the same structural equations as Kusner et al. (2017):
G=N(wK
GK+wR
GR+wS
GS+bG,σG),
L=Poisson/parenleftbig
exp/braceleftbig
wK
LK+wR
LR+wS
LS+bL/bracerightbig/parenrightbig
,
F=N(wK
FK+wR
FR+wS
FS,1).
Implementation Details. Note that race is an immutable characteristic. Therefore, we assume that the
individuals only adjust their knowledge Kin response to the prediction model ˆY. That isK′=K+η∇KˆY.
In contrast to synthetic data, the parameters of structural equations are unknown, and we have to use the
training dataset to estimate them. Following the approach of Kusner et al. (2017), we assume that GandF
adhere to Gaussian distributions centered at wK
GK+wR
GR+wS
GS+bGandwK
FK+wR
FR+wS
FS, respectively.
Note thatLis an integer, and it follows a Poisson distribution with the parameter exp{wK
LK+wR
LR+wS
LS+
bL}. Using the Markov chain Monte Carlo (MCMC) method Geyer (1992), we can estimate the parameters
and the conditional distribution of Kgiven (R,S,G,L ). For each given data, we sampled m= 500different
k’s from this conditional distribution. We partitioned the data into training, validation, and test sets with
60%/20%/20%ratio.
Table 2: Results on Law School Data: comparison with two baselines, unfair predictor (UF) and counterfac-
tual fair predictor (CF), in terms of accuracy (MSE) and lookahead counterfactual fairness (AFCE, UIR).
Method MSE AFCE UIR
UF 0.393 ±0.046 0.026±0.003 0%±0
CF 0.496 ±0.051 0.026±0.003 0%±0
Ours (p1=T/4) 0.493±0.049 0.013±0.002 50%±0
Ours (p1=T/2) 0.529±0.049 0.000±0.000 100%±0
Results. Table 2 illustrates the results with η= 10andp1=T
4andp1=T
2whereT= 1/(wF
K)2. The
results show that our method achieves a similar MSE as the CF predictor. However, it can improve AFCE
significantly compared to the baselines. Figure 6 shows the distribution of YandY′for the first data point
in the test set in the factual and counterfactual worlds. Under the UF and CF predictor, the disparity
between factual and factual Y′remains similar to the disparity between factual and counterfactual Y. On
the other hand, the disparity between factual and counterfactual Y′under our algorithms gets better for both
p1=T/2andp1=T/4. Figure 3b demonstrates that for the law school dataset, the trade-off between MSE
and AFCE can be adjusted by changing hyperparameter p1. Figure 6 show the factual and counterfactual
distributions in real data experiment. It can be seen that our method is the only way that can decrease the
gap between Y′and ˇY′in an obvious way.
14Published in Transactions on Machine Learning Research (12/2024)
0.0 0.2
Y012345 densityCurrent
0.2 0.4
Y/prime012345 densityUF
1.0 1.2
Y/prime012345 densityCF
1.5 2.0
Y/prime0123 densityLCF(p1=T
4)
1.5 2.0
Y/prime0.00.51.01.52.02.5densityLCF(p1=T
2)
factual
counter
factual
Figure 6: Density plot for F′and ˇF′in law school data. For a chosen data point, we sampled Kfrom the
conditional distribution of Kand plot the distribution of F′and ˇF′.
8 Conclusion
This work studied the impact of ML decisions on individuals’ future status using a counterfactual inference
framework. We observed that imposing the CF predictor may not decrease the group disparity in individuals’
future status. We thus introduced the lookahead counterfactual fairness (LCF) notion, which takes into
account the downstream effects of ML models and requires the individual future status to be counterfactually
fair. We proposed a method to train an ML model under LCF and evaluated the method through empirical
studies on synthetic and real data.
Acknowledgements
This material is based upon work supported by the U.S. National Science Foundation under award IIS-
2202699, IIS-2416895, IIS-2301599, and CMMI-2301601, and by OSU President’s Research Excellence Ac-
celerator Grant, and grants from the Ohio State University’s Translational Data Analytics Institute and
College of Engineering Strategic Research Initiative.
References
Loan Prediction Problem Dataset — kaggle.com. https://www.kaggle.com/datasets/
altruistdelhite04/loan-prediction-problem-dataset . [Accessed 20-10-2024].
Mahed Abroshan, Mohammad Mahdi Khalili, and Andrew Elliott. Counterfactual fairness in synthetic data
generation. In NeurIPS Workshop on Synthetic Data for Empowering ML Research , 2022.
Mahed Abroshan, Andrew Elliott, and Mohammad Mahdi Khalili. Imposing fairness constraints in synthetic
data generation. In International Conference on Artificial Intelligence and Statistics , pp. 2269–2277.
PMLR, 2024.
Jacy Anthis and Victor Veitch. Causal context connects counterfactual fairness to robust prediction and
group fairness. Advances in Neural Information Processing Systems , 36, 2024.
YahavBechavod, CharaPodimata, StevenWu, andJubaZiani. Informationdiscrepancyinstrategiclearning.
InInternational Conference on Machine Learning , pp. 1691–1715, 2022.
Micah D Carroll, Anca Dragan, Stuart Russell, and Dylan Hadfield-Menell. Estimating and penalizing
induced preference shifts in recommender systems. In International Conference on Machine Learning , pp.
2686–2708. PMLR, 2022.
Silvia Chiappa. Path-specific counterfactual fairness. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 33, pp. 7801–7808, 2019.
Roxana Daneshjou, Kailas Vodrahalli, Weixin Liang, Roberto A Novoa, Melissa Jenkins, Veronica Rotem-
berg, Justin Ko, Susan M Swetter, Elizabeth E Bailey, Olivier Gevaert, et al. Disparities in dermatology
ai: Assessments using diverse clinical images. arXiv preprint arXiv:2111.08006 , 2021.
15Published in Transactions on Machine Learning Research (12/2024)
Jeffrey Dastin. Amazon scraps secret ai recruiting tool that showed bias against women. http://reut.rs/
2MXzkly, 2018.
Lucas De Lara, Alberto González-Sanz, Nicholas Asher, Laurent Risser, and Jean-Michel Loubes. Transport-
based counterfactual models. Journal of Machine Learning Research , 25(136):1–59, 2024.
Sarah Dean and Jamie Morgenstern. Preference dynamics under personalized recommendations. In Proceed-
ings of the 23rd ACM Conference on Economics and Computation , pp. 795–816, 2022.
Virginie Do, Sam Corbett-Davies, Jamal Atif, and Nicolas Usunier. Online certification of preference-based
fairness for personalized recommender systems. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 36, pp. 6532–6540, 2022.
Tri Dung Duong, Qian Li, and Guandong Xu. Achieving counterfactual fairness with imperfect structural
causal model. Expert Systems with Applications , 240:122411, 2024.
Ahmad-Reza Ehyaei, Kiarash Mohammadi, Amir-Hossein Karimi, Samira Samadi, and Golnoosh Farnadi.
Causal adversarial perturbations for individual fairness and robustness in heterogeneous data spaces. In
Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pp. 11847–11855, 2024.
Yingqiang Ge, Shuchang Liu, Ruoyuan Gao, Yikun Xian, Yunqi Li, Xiangyu Zhao, Changhua Pei, Fei Sun,
Junfeng Ge, Wenwu Ou, et al. Towards long-term fairness in recommendation. In Proceedings of the 14th
ACM international conference on web search and data mining , pp. 445–453, 2021.
Charles J Geyer. Practical markov chain monte carlo. Statistical science , pp. 473–483, 1992.
Madelyn Glymour, Judea Pearl, and Nicholas P Jewell. Causal inference in statistics: A primer . John Wiley
& Sons, 2016.
Sofie Goethals, David Martens, and Toon Calders. Precof: counterfactual explanations for fairness. Machine
Learning , 113(5):3111–3142, 2024.
Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classification. In
Proceedings of the 2016 ACM conference on innovations in theoretical computer science , pp. 111–122,
2016a.
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances in
neural information processing systems , 29:3315–3323, 2016b.
Thomas Henzinger, Mahyar Karimi, Konstantin Kueffner, and Kaushik Mallik. Runtime monitoring of
dynamic fairness properties. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and
Transparency , pp. 604–614, 2023a.
Thomas A Henzinger, Mahyar Karimi, Konstantin Kueffner, and Kaushik Mallik. Monitoring algorithmic
fairness. arXiv preprint arXiv:2305.15979 , 2023b.
Yaowei Hu and Lu Zhang. Achieving long-term fairness in sequential decision making. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 36, pp. 9549–9557, 2022.
Aria Khademi, Sanghack Lee, David Foley, and Vasant Honavar. Fairness in algorithmic decision making:
An excursion through the lens of causality. In The World Wide Web Conference , pp. 2907–2914, 2019.
Mohammad Mahdi Khalili, Xueru Zhang, and Mahed Abroshan. Fair sequential selection using supervised
learning models. Advances in Neural Information Processing Systems , 34:28144–28155, 2021a.
Mohammad Mahdi Khalili, Xueru Zhang, Mahed Abroshan, and Somayeh Sojoudi. Improving fairness and
privacy in selection problems. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35,
pp. 8092–8100, 2021b.
Mohammad Mahdi Khalili, Xueru Zhang, and Mahed Abroshan. Loss balancing for fair supervised learning.
InInternational Conference on Machine Learning , pp. 16271–16290. PMLR, 2023.
16Published in Transactions on Machine Learning Research (12/2024)
Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and
Bernhard Schölkopf. Avoiding discrimination through causal reasoning. Advances in neural information
processing systems , 30, 2017.
Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. Advances in neural
information processing systems , 30, 2017.
Lydia T Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine
learning. In International Conference on Machine Learning , pp. 3150–3158. PMLR, 2018.
Lydia T Liu, Ashia Wilson, Nika Haghtalab, Adam Tauman Kalai, Christian Borgs, and Jennifer Chayes.
The disparate equilibria of algorithmic decision making when individuals invest rationally. In Proceedings
of the 2020 Conference on Fairness, Accountability, and Transparency , pp. 381–391, 2020.
Jing Ma, Ruocheng Guo, Aidong Zhang, and Jundong Li. Learning for counterfactual fairness from obser-
vational data. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining, pp. 1620–1630, 2023.
Agathe Fernandes Machado, Arthur Charpentier, and Ewen Gallic. Sequential conditional transport on
probabilistic graphs for interpretable counterfactual fairness. arXiv preprint arXiv:2408.03425 , 2024.
John Miller, Smitha Milli, and Moritz Hardt. Strategic classification is causal modeling in disguise. In
International Conference on Machine Learning , pp. 6917–6926. PMLR, 2020.
Razieh Nabi and Ilya Shpitser. Fair inference on outcomes. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 32, 2018.
Arash Nasr-Esfahany, Mohammad Alizadeh, and Devavrat Shah. Counterfactual identifiability of bijective
causal models. arXiv preprint arXiv:2302.02228 , 2023.
Judea Pearl et al. Models, reasoning and inference. Cambridge, UK: CambridgeUniversityPress , 19(2), 2000.
Mariana Pinto, Andre V Carreiro, Pedro Madeira, Alberto Lopez, and Hugo Gamboa. The matrix reloaded:
Towards counterfactual group fairness in machine learning. Journal of Data-centric Machine Learning
Research , 2024.
Raphael Poulain, Hamed Fayyaz, and Rahmatollah Beheshti. Aligning (medical) llms for (counterfactual)
fairness. arXiv preprint arXiv:2408.12055 , 2024.
Nir Rosenfeld, Anna Hilgard, Sai Srivatsa Ravindranath, and David C Parkes. From predictions to decisions:
Using lookahead regularization. Advances in Neural Information Processing Systems , 33:4115–4126, 2020.
Pengyang Shao, Le Wu, Kun Zhang, Defu Lian, Richang Hong, Yong Li, and Meng Wang. Average user-side
counterfactual fairness for collaborative filtering. ACM Transactions on Information Systems , 42(5):1–26,
2024.
Yonadav Shavit, Benjamin Edelman, and Brian Axelrod. Causal strategic linear regression. In International
Conference on Machine Learning , pp. 8676–8686. PMLR, 2020.
Zeyu Tang, Yatong Chen, Yang Liu, and Kun Zhang. Tier balancing: Towards dynamic fairness over
underlying causal factors. arXiv preprint arXiv:2301.08987 , 2023.
Songül Tolan, Marius Miron, Emilia Gómez, and Carlos Castillo. Why machine learning may lead to
unfairness: Evidencefromriskassessmentforjuvenilejusticeincatalonia. In Proceedings of the Seventeenth
International Conference on Artificial Intelligence and Law , pp. 83–92, 2019.
Zichong Wang, Zhibo Chu, Ronald Blanco, Zhong Chen, Shu-Ching Chen, and Wenbin Zhang. Advanc-
ing graph counterfactual fairness through fair representation learning. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases , pp. 40–58. Springer, 2024a.
17Published in Transactions on Machine Learning Research (12/2024)
Zichong Wang, Meikang Qiu, Min Chen, Malek Ben Salem, Xin Yao, and Wenbin Zhang. Toward fair graph
neural networks via real counterfactual samples. Knowledge and Information Systems , 66(11):6617–6641,
2024b.
Linda F Wightman. Lsac national longitudinal bar passage study. lsac research report series. 1998.
Yongkai Wu, Lu Zhang, and Xintao Wu. Counterfactual fairness: Unidentification, bound and algorithm.
InProceedings of the twenty-eighth international joint conference on Artificial Intelligence , 2019.
Ying Xiao, Jie M Zhang, Yepang Liu, Mohammad Reza Mousavi, Sicen Liu, and Dingyuan Xue. Mirrorfair:
Fixing fairness bugs in machine learning software via counterfactual predictions. Proceedings of the ACM
on Software Engineering , 1(FSE):2121–2143, 2024.
Tian Xie and Xueru Zhang. Automating data annotation under strategic human agents: Risks and potential
solutions. arXiv preprint arXiv:2405.08027 , 2024a.
Tian Xie and Xueru Zhang. Non-linear welfare-aware strategic learning. arXiv preprint arXiv:2405.01810 ,
2024b.
Tian Xie, Zhiqun Zuo, Mohammad Mahdi Khalili, and Xueru Zhang. Learning under imitative strategic
behavior with unforeseeable outcomes. Transactions on Machine Learning Research , 2024. ISSN 2835-
8856. URL https://openreview.net/forum?id=82bNZGMNZa .
Depeng Xu, Shuhan Yuan, and Xintao Wu. Achieving differential privacy and fairness in logistic regression.
InCompanion Proceedings of The 2019 World Wide Web Conference , pp. 594–599, 2019.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness be-
yond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In
Proceedings of the 26th international conference on world wide web , pp. 1171–1180, 2017.
Xueru Zhang, Mohammadmahdi Khaliligarekani, Cem Tekin, et al. Group retention when using machine
learning in sequential decision making: the interplay between user dynamics and fairness. Advances in
Neural Information Processing Systems , 32:15269–15278, 2019.
Xueru Zhang, Ruibo Tu, Yang Liu, Mingyan Liu, Hedvig Kjellstrom, Kun Zhang, and Cheng Zhang. How
do fair decisions fare in long-term qualification? Advances in Neural Information Processing Systems , 33:
18457–18469, 2020.
Xueru Zhang, Mohammad Mahdi Khalili, Kun Jin, Parinaz Naghizadeh, and Mingyan Liu. Fairness inter-
ventions as (Dis)Incentives for strategic manipulation. In Proceedings of the 39th International Conference
on Machine Learning , pp. 26239–26264, 2022.
Zeyu Zhou, Ruqi Bai, and David I Inouye. Improving practical counterfactual fairness with limited causal
knowledge. In ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models ,
2024.
Aoqi Zuo, Susan Wei, Tongliang Liu, Bo Han, Kun Zhang, and Mingming Gong. Counterfactual fairness
with partially known causal graph. Advances in Neural Information Processing Systems , 35:1238–1252,
2022.
Zhiqun Zuo, Mohammad Mahdi Khalili, and Xueru Zhang. Counterfactually fair representation. Advances
in Neural Information Processing Systems , 36:12124–12140, 2023.
18Published in Transactions on Machine Learning Research (12/2024)
A Proofs
A.1 Proof of Theorem 5.1 and Theorem 5.2
Proof.For any given x,a, we can find the conditional distribution UX|X=x,A=aandUY|X=x,A=a
based on causal model M. Consider sample u= [uX,uY]drawn from this conditional distribution. For this
sample, we have,
ˇx=α⊙uX+βˇa,
ˇy=wTˇx+γuY.
Since ˇyis also a function of uX,uY, utilizing that
∂ˇy
∂uX=w⊙α,∂ˇy
∂uY=γ,
the gradient of g(ˇy,uX,uY)w.r.t.uX,uYare
∇uXg=∂g(ˇy,uX,uY)
∂uX+∂g(ˇy,uX,uY)
∂ˇyw⊙α,
∇uYg=∂g(ˇy,uX,uY)
∂uY+∂g(ˇy,uX,uY)
∂ˇyγ.
The response function ris defined as
u′
X=uX+∇uXg, u′
Y=uY+∇uYg.
y′can be calculated using response ras follows,
y′=y+ηwT/parenleftbigg
α⊙∂g(ˇy,uX,uY)
∂uX/parenrightbigg
+η||w⊙α||2
2∂g(ˇy,uX,uY)
∂ˇy+ηγ∂g(ˇy,uX,uY)
∂uY+ηγ2∂g(ˇy,uX,uY)
∂ˇy.(13)
Similarly, we can calculate counterfactual value ˇy′as follows,
ˇy′= ˇy+ηwT/parenleftbigg
α⊙∂g(y,uX,uY)
∂uX/parenrightbigg
+η||w⊙α||2
2∂g(y,uX,uY)
∂y+ηγ∂g(y,uX,uY)
∂uY+ηγ2∂g(y,uX,uY)
∂y.(14)
Note that the following hold for g,
∂g(ˇy,uX,uY)
∂uX=∂g(y,uX,uY)
∂uX, (15)
∂g(ˇy,uX,uY)
∂uY=∂g(y,uX,uY)
∂uY. (16)
Thus,
|ˇy′−y′|=/vextendsingle/vextendsingle/vextendsingle/vextendsingleˇy−y+η/parenleftbig
||w⊙α||2
2+γ2/parenrightbig/parenleftbigg∂g(y,uX,uY)
∂y−∂g(ˇy,uX,uY)
∂ˇy/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle. (17)
Given above equation, now we can prove Theorem 5.1, Corollary 5.1, and Theorem 5.2,
•Forgin Theorem 5.1 and Corollary 5.1, we have,
g(ˇy,uX,uY) =p1ˇy2+p2ˇy+p3+h(u). (18)
The partial derivative of gcan be computed as
∂g(ˇy,uX,uY)
∂ˇy= 2p1ˇy+p2. (19)
19Published in Transactions on Machine Learning Research (12/2024)
We denote T=η/parenleftbig
||w⊙α||2
2+γ2/parenrightbig
. From the theorem, we know that p1=T
2. Therefore, we have
|y′−ˇy′|=|ˇy−y+1
TT(y−ˇy)|= 0.
Since, for any realization of u, the above equation holds, we can conclude that the following holds,
Pr(ˆYA←a(U) =y|X=x,A=a) = Pr( ˆYA←ˇa(U) =y|X=x,A=a)
Whenp1∈(0,T), we have that
|y′−ˇy′|=|ˇy−y+2
Tp1(y−ˇy)|= 0.
Because
(ˇy−y)/parenleftbigg2p1
T(y−ˇy)/parenrightbigg
<0,
and
/vextendsingle/vextendsingle/vextendsingle/vextendsingle2p1
T(y−ˇy)/vextendsingle/vextendsingle/vextendsingle/vextendsingle<2|ˇy−y|,
we have|y′−ˇy′|<|y−ˇy|. With law of total probability, we have
Pr({|Y′
A←a(U)−Y′
A←ˇa(U)|<|YA←a(U)−YA←ˇa(U)|}|X=x,A=a) = 1.
•Forgin Theorem 5.2, since g(ˇy,uX,uY)is strictly convex in ˇy, we have,
(ˇy−y)/parenleftbigg∂g(y,uX,uY)
∂y−∂g(ˇy,uX,uY)
∂ˇy/parenrightbigg
>0.
Note that derivative of g(ˇy,uX,uY)with respect to ˇyisK-Lipschitz continuous in ˇy,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂g(y,uX,uY)
∂y−∂g(ˇy,uX,uY)
∂ˇy/vextendsingle/vextendsingle/vextendsingle/vextendsingle<2|y−ˇy|
η(||w⊙α||2
2+γ2),
we have that
/vextendsingle/vextendsingle/vextendsingle/vextendsingleη/parenleftbig
||w⊙α||2
2+γ2/parenrightbig/parenleftbigg∂g(y,uX,uY)
∂y−∂g(ˇy,uX,uY)
∂ˇy/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle<2|ˇy−y|.
Therefore,
|y′−ˇy′|<|y−ˇy|
So we have
Pr({|Y′
A←a(U)−Y′
A←ˇa(U)|<|YA←a(U)−YA←ˇa(U)|}|X=x,A=a) = 1
A.2 Theorem 5.2 for non-binary A
Let{a}∪{ ˇa[1],ˇa[2],...,ˇa[m]}be a set of all possible values for A. Let ˇY[j]be the counterfactual random
variable associated with ˇa[j]given observation X=xandA=a. Then,g/parenleftig
1
m(ˇY[1]+···ˇY[m]),U/parenrightig
satisfies
LCF, where gsatisfies the properties in Theorem 5.2.
20Published in Transactions on Machine Learning Research (12/2024)
Proof.For any given x,a, we assume the set of counterfactual ais{ˇa[1],ˇa[2],...,ˇa[m]}. Consider a sample
u= [uX,uY]drawn from the condition distribution of UX|X=x,A =aandUY|X=x,A =a, with a
predictorg/parenleftbig1
m(ˇy[1]+···ˇy[m]),u/parenrightbig
, use the same way in A.1, we can get
/vextendsingle/vextendsingle/vextendsingleˇy′[j]−y′/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingleˇy[j]−y+η(||w⊙α||2+γ2)/parenleftbigg∂g(ˇy[1]+···ˇy[m],u)
∂ˇy[1]+···ˇy[m]−∂g(y+ ˇy[1]+···+ ˇy[j−1]+ ˇy[j+1]···ˇy[m],u)
∂y+ ˇy[1]+···ˇy[j−1]+ ˇy[j+1]···ˇy[m]/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle,
withj∈{1,...,m}. Wheny>ˇy[j], we have
ˇy[1]+···ˇy[m]<y+ ˇy[1]+···ˇy[j−1]+ ˇy[j+1]···ˇy[m]
and wheny<ˇy[j],
ˇy[1]+···ˇy[m]>y+ ˇy[1]+···ˇy[j−1]+ ˇy[j+1]···ˇy[m]
Becausegis strictly convex and Lipschitz continuous, we have
(ˇy[j]−y)/parenleftbigg∂g(ˇy[1]+···ˇy[m],u)
∂ˇy[1]+···ˇy[m]−∂g(y+ ˇy[1]+···+ ˇy[j−1]+ ˇy[j+1]···ˇy[m],u)
∂y+ ˇy[1]+···ˇy[j−1]+ ˇy[j+1]···ˇy[m]/parenrightbigg
<0,
and
/vextendsingle/vextendsingle/vextendsingle/vextendsingleη(||w⊙α||2+γ2)/parenleftbigg∂g(ˇy[1]+···ˇy[m],u)
∂ˇy[1]+···ˇy[m]−∂g(y+ ˇy[1]+···+ ˇy[j−1]+ ˇy[j+1]···ˇy[m],u)
∂y+ ˇy[1]+···ˇy[j−1]+ ˇy[j+1]···ˇy[m]/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle<2|ˇy[j]−y|.
Therefore,
|ˇy′[j]−y′|<|ˇy[j]−y|
So we proved that, for any j∈{1,2,...,m}
Pr({|Y′
A←a(U)−Y′
A←ˇa[j](U)|<|YA←a(U)−YA←ˇa[j](U)|}|X=x,A=a) = 1
A.3 Proof of Theorem 5.3
Proof.Westartfromthecasewhen ˜fisincreasing. Foranygiven x,a,wecanfindtheconditionaldistribution
U|X=x,A=abasedonthecausalmodel M. Considerasample udrawnfromthisconditionaldistribution.
For this sample, we have
y=˜f(u+u0(a)),ˇy=˜f(u+u0(ˇa)).
So, the gradient of g(ˇy,u) =p1ˇy2+p2+h(u)w.r.t.uis
dg(ˇy,u)
du= 2p1ˇydˇy
du+dh(u)
du.
Therefore,y′can be calculated using response ras follows,
y′=˜f/parenleftbigg
u+u0(a) + 2ηp1ˇydˇy
du+ηdh(u)
du/parenrightbigg
.
Similarly, we have the future counterfactual status as
ˇy′=˜f/parenleftbigg
u+u0(ˇa) + 2ηp1ydy
du+ηdh(u)
du/parenrightbigg
.
Wheny= ˇy, it is obvious that y′= ˇy′since ˇydˇy
du=ydy
du. Wheny>ˇy, because ˜fis increasing, we have
u0(a)>u0(ˇa).
21Published in Transactions on Machine Learning Research (12/2024)
Becauseh(U)is increasing, we havedh(u)
du≥0and
2ηp1ˇydˇy
du+ηdh(u)
du≥0,2ηp1ydy
du+ηdh(u)
du≥0.
We further denote
ϕ1=u+u0(ˇa)
ϕ2=u+u0(a)
ϕ3=u+u0(ˇa) + 2ηp1ydy
du+ηdh(u)
du
ϕ4=u+u0(a) + 2ηp1ˇydˇy
du+ηdh(u)
du
We already have
ϕ1<ϕ2, ϕ 1≤ϕ3, ϕ 2≤ϕ4.
There are three cases for the relationship between ϕ1,ϕ2,ϕ3andϕ4.
Case 1:ϕ1<ϕ2≤ϕ3≤ϕ4. In this case, from the mean value theorem,
|y−ˇy|=/vextendsingle/vextendsingle˜f(ϕ2)−˜f(ϕ1)/vextendsingle/vextendsingle=/vextendsingle/vextendsingle˜f′(c1)/vextendsingle/vextendsingle|ϕ2−ϕ1|,
wherec1∈(ϕ1,ϕ2), and
|y′−ˇy′|=/vextendsingle/vextendsingle˜f(ϕ4)−˜f(ϕ3)/vextendsingle/vextendsingle=/vextendsingle/vextendsingle˜f′(c2)/vextendsingle/vextendsingle|ϕ4−ϕ3|,
wherec2∈(ϕ3,ϕ4). Because
|ϕ4−ϕ3|= (u0(a)−u0(ˇa)) + 2ηp1/parenleftbigg
ˇydˇy
du−ydy
du/parenrightbigg
,
and
ˇydˇy
du≤ydy
du,
we have
|ϕ4−ϕ3|≤u0(a)−u0(ˇa) =|ϕ2−ϕ1|.
Because ˜fis strictly concave, ˜f′is strictly decreasing, and we have,
/vextendsingle/vextendsingle˜f′(c1)/vextendsingle/vextendsingle>/vextendsingle/vextendsingle˜f′(c2)/vextendsingle/vextendsingle.
Therefore,
|y′−ˇy′|<|y−y′|.
Case 2:ϕ1≤ϕ3<ϕ2≤ϕ4. In this case,
|y−ˇy|=˜f(ϕ2)−˜f(ϕ1) =˜f(ϕ2)−˜f(ϕ3) +˜f(ϕ3)−˜f(ϕ1),
|y′−ˇy′|=˜f(ϕ4)−˜f(ϕ3) =˜f(ϕ4)−˜f(ϕ2) +˜f(ϕ2)−˜f(ϕ3).
From the mean value theorem,
/vextendsingle/vextendsingle˜f(ϕ3)−˜f(ϕ1)/vextendsingle/vextendsingle=/vextendsingle/vextendsingle˜f′(c1)/vextendsingle/vextendsingle|ϕ3−ϕ1|,
22Published in Transactions on Machine Learning Research (12/2024)
wherec1∈(ϕ1,ϕ3)and
/vextendsingle/vextendsingle˜f(ϕ4)−˜f(ϕ2)/vextendsingle/vextendsingle=/vextendsingle/vextendsingle˜f′(c2)/vextendsingle/vextendsingle|ϕ4−ϕ2|,
wherec2∈(ϕ2,ϕ4). Because ˜f′is decreasing, we have
/vextendsingle/vextendsingle˜f′(c1)/vextendsingle/vextendsingle>/vextendsingle/vextendsingle˜f′(c2)/vextendsingle/vextendsingle.
Because
|ϕ4−ϕ2|= 2ηp1ˇydˇy
du+ηdh(u)
du
|ϕ3−ϕ1|= 2ηp1ydy
du+ηdh(u)
du,
we have that|ϕ3−ϕ1|≤|ϕ4−ϕ2|. Therefore,|y′−ˇy′|<|y−y′|.
Case 3:ϕ1<ϕ2≤ϕ4≤ϕ3. In this case, we have
|y′−ˇy′|=˜f(ϕ3)−˜f(ϕ4).
From the mean value theorem,
|y−ˇy|=/vextendsingle/vextendsingle˜f′(c1)/vextendsingle/vextendsingle|ϕ2−ϕ1|,
wherec1∈(ϕ1,ϕ2), and
|y′−ˇy′|=/vextendsingle/vextendsingle˜f′(c2)/vextendsingle/vextendsingle|ϕ3−ϕ4|,
wherec2∈(ϕ4,ϕ3). Because
|ϕ2−ϕ1|=u0(a)−u0(ˇa);
|ϕ3−ϕ4|= (u0(ˇa)−u0(a)) + 2ηp1/parenleftbigg
ydy
du−ˇydˇy
du/parenrightbigg
,
and
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleydy
du−ˇydˇy
du
(u+u0(a))−(u+u0(ˇa))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤M,
whenp1≤1
ηM,
|ϕ3−ϕ4|≤(u0(ˇa)−u0(a)) + 2 (u0(a)−u0(ˇa))≤u0(a)−u0(ˇa) =|ϕ2−ϕ1|
Because ˜f′(c1)>˜f′(c2), we have|y′−ˇy′|<|y−ˇy|.
In conclusion, we prove that |y′−ˇy′|<|y−ˇy|for every sample u. With law of total probability, we have
Pr/parenleftig/braceleftig
|Y′
A←a(U)−Y′
A←ˇa(U)|<|YA←a(U)−YA←ˇa(U)|/bracerightig/vextendsingle/vextendsingleX=x,A=a/parenrightig
= 1.
For the case when ˜fis decreasing, we can consider −Y=−˜f(U+u0(A)), then we have
Pr/parenleftig/braceleftig
|−Y′
A←a(U)−(−Y′
A←ˇa(U))|<|−YA←a(U)−(−YA←ˇa(U))|/bracerightig/vextendsingle/vextendsingleX=x,A=a/parenrightig
= 1,
which is to say
Pr/parenleftig/braceleftig
|Y′
A←a(U)−Y′
A←ˇa(U)|<|YA←a(U)−YA←ˇa(U)|/bracerightig/vextendsingle/vextendsingleX=x,A=a/parenrightig
= 1.
23Published in Transactions on Machine Learning Research (12/2024)
A.4 Proof of Theorem 5.4
Proof.From the causal functions defined in Theorem 5.4, given any x,a, we can find the conditional distri-
butionUX|X=x,A=aandUY|X=x,A=a. Similar to the proof of Theorem 5.2, we have
ˇx= ˇa(α⊙uX+β),
ˇy=wTˇx+γuY.
Because
∂ˇy
∂uX=w⊙ˇaα,∂ˇy
∂uY=γ,
the gradient of g(ˇy)w.r.tuX,uYare
∇uXg=∂g(ˇy)
∂ˇyˇaw⊙α,
∇uYg=∂g(ˇy)
∂ˇyγ.
The response function ris defined as
u′
X=uX+∇uXg, u′
Y=uY+∇uYg.
y′can be calculated using the response ras follows,
y′=y+η/parenleftbig
aˇa||w⊙α||2+γ2/parenrightbig∂g(ˇy)
∂ˇy.
In the counterfactual world,
ˇy′= ˇy+η(ˇaa||w⊙α||2+γ2)∂g(y)
∂y.
So we have,
|y′−ˇy′|=/vextendsingle/vextendsingle/vextendsingle/vextendsingley−ˇy+η(aˇa||w⊙α||2+γ2)(∂g(ˇy)
∂ˇy−∂g(y)
∂y)/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
We denote that ∆ =η(aˇa||w⊙α||2+γ2)(∂g(ˇy)
∂ˇy−∂g(y)
∂y). BecauseAis a binary attribute, we have
aˇa=a1a2>0.
From the property of gthatg(ˇy)is strictly convex, we have
(y−ˇy)/parenleftbigg∂g(ˇy)
∂ˇy−∂g(y)
∂y/parenrightbigg
<0.
Note that the derivative of g(ˇy)isK-Lipschitz continuous,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂g(ˇy)
∂ˇy−∂g(y)
∂y/vextendsingle/vextendsingle/vextendsingle/vextendsingle<2|ˇy−y|
η(aˇa||w⊙α||2
2+γ2),
we have
/vextendsingle/vextendsingle/vextendsingle/vextendsingleη(aˇa||w⊙α||2+γ2)(∂g(ˇy)
∂ˇy−∂g(y)
∂y)/vextendsingle/vextendsingle/vextendsingle/vextendsingle<2|y−ˇy|.
Therefore, for every usampled from the conditional distribution, |ˇy′−y′|<|ˇy−y|. So we proved
Pr({|Y′
A←a(U)−Y′
A←ˇa(U)|<|YA←a(U)−YA←ˇa(U)|}|X=x,A=a) = 1.
24Published in Transactions on Machine Learning Research (12/2024)
A.5 Proof of Theorem 6.1
Proof.For any given x,awe can find the consitional distribution UX|X=x,A =aandUY|X=x,A =a
based on causal model M. Consider sample u= [uX,uY]drawn from this conditional distribution. For this
sample, we have,
ˇxPGA=αPGA⊙uXPGA+βPGAˇa,
ˇyPD=wT
PGAˇxPGA+wT
Pc
GAxPc
GA+γuY.
So, the gradient of g(ˇyPD,uXPGA,uXPc
GA,uY)w.r.t.uXPGA,uXPc
GA,uYare
∇uXPGAg=∂g(ˇyPD,uXPGA,uXPc
GA,uY)
∂uXPGA+∂g(ˇyPD,uXPGA,uXPc
GA,uY)
∂ˇy⊙wPGA⊙αPGA,
∇uXPc
GAg=∂g(ˇyPD,uXPGA,uXPc
GA,uY)
∂uXPc
GA+∂g(ˇyPD,uXPGA,uXPc
GA,uY)
∂ˇy⊙wPc
GA⊙αPc
GA,
∇uYg=∂g(ˇy,uXPGA,uXPc
GA,uY)
∂uY+∂g(ˇy,uXPGA,uXPc
GA,uY)
∂ˇyPDγ.
The response function ris defined as
u′
X=uX+∇uXg, u′
Y=uY+∇uYg.
y′can be calculated using response ras follows,
y′=y+ηwT
PGA
α⊙∂g(ˇyPD,uXPGA,uXPc
GA,uY)
∂uXPGA
+ηwT
Pc
GA
α⊙∂g(ˇyPD,uXPGA,uXPc
GA,uY)
∂uXPc
GA

+η/vextenddouble/vextenddouble/vextenddoublewPGA⊙αPGA/vextenddouble/vextenddouble/vextenddouble2
2∂g(ˇyPD,uXPGA,uXPc
GA,uY)
∂ˇyPD+η/vextenddouble/vextenddouble/vextenddoublewPc
GA⊙αPc
GA/vextenddouble/vextenddouble/vextenddouble2
2∂g(ˇyPD,uXPGA,uXPc
GA,uY)
∂ˇyPD
+ηγ∂g(ˇyPD,uXPGA,uY,uY)
∂uY+ηγ2∂g(ˇyPD,uXPGA,uY,uY)
∂ˇyPD.
Similarly, we can calculate path-dependent counterfactual value ˇy′
PDas follows,
ˇy′
PD=ˇyPD+ηwT
PGA
α⊙∂g(y,uXPGA,uXPc
GA,uY)
∂uXPGA
+ηwT
Pc
GA
α⊙∂g(y,uXPGA,uXPc
GA,uY)
∂uXPc
GA

+η/vextenddouble/vextenddouble/vextenddoublewPGA⊙αPGA/vextenddouble/vextenddouble/vextenddouble2
2∂g(y,uXPGA,uXPc
GA,uY)
∂y+η/vextenddouble/vextenddouble/vextenddoublewPc
GA⊙αPc
GA/vextenddouble/vextenddouble/vextenddouble2
2∂g(y,uXPGA,uXPc
GA,uY)
∂y
+ηγ∂g(y,uXPGA,uY,uY)
∂uY+ηγ2∂g(y,uXPGA,uY,uY)
∂y.
Thus,
|ˇy′
PD−y′|=
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleˇyPD−y+η/parenleftig
||wPGA⊙αPGA||2
2+||wPc
GA⊙αPc
GA||2
2+γ2/parenrightig
∂g(y,uXPGA,uXPc
GA,uY)
∂y−∂g(ˇyPD,uXPGA,uXPc
GA,uY)
∂ˇyPD
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
Denotep1=1
2η(||wPGA⊙αPGA||2
2+||wPc
GA⊙αPc
GA||2
2+γ2). Since the partial gradient of g(ˇyPD,uXPGA,uXPc
GA,uY)
w.r.t. ˇyPDis2p1ˇyPD+p2, we know that|y′−ˇy′
PD|= 0. Since for any realization of u, the equation holds,
we can conclude that the path-dependent LCF holds.
25Published in Transactions on Machine Learning Research (12/2024)
A.6 Proof of Theorem 4.1
Proof.SinceYis determined by U, we denote the causal function from UtoAas
Y=f(U,A).
Suppose the conditional distribution of UgivenX=x,A=acould denoted as Pc(U), we have
Pr(YA←a(U)|X=x,A=a) =/summationdisplay
u∈{u|f(u,a)}=yPc(u).
Because
Pr(YA←a(U) =y|X=x,A=a)̸= Pr(YA←ˇa(U) =y|X=x,A=a),
we have,
/summationdisplay
u∈{u|f(u,a)=y}Pc(u)̸=/summationdisplay
u∈{u|f(u,ˇa)=y}Pc(u). (20)
Because the predictor satisfies CF,
U′
A←a=r(U,ˆY),
U′
A←ˇa=r(U,ˆY),
the future outcome could be written as
Pr(Y′
A←a(U)|X=x,A=a) =/summationdisplay
u∈{u|f(r(u,ˆy),a)=y}Pc(u).
From Eq.20, we have
/summationdisplay
u|{f(r(u,ˆy),a)=y}Pc(u)̸=/summationdisplay
u|{f(r(u,ˆy),ˇa)=y}Pc(u),
which is to say
Pr(Y′
A←a(U)|X=x,A=a)̸= Pr(Y′
A←ˇa(U) =y|X=x,A=a).
B Parameters for Synthetic Data Simulation
When generating the synthetic data, we used
α=
0.37454012
0.95071431
0.73199394
0.59865848
0.15601864
0.15599452
0.05808361
0.86617615
0.60111501
0.70807258
;β=
0.02058449
0.96990985
0.83244264
0.21233911
0.18182497
0.18340451
0.30424224
0.52475643
0.43194502
0.29122914
;w=
0.61185289
0.13949386
0.29214465
0.36636184
0.45606998
0.78517596
0.19967378
0.51423444
0.59241457
0.04645041
;γ= 0.60754485
These values are generated randomly.
26Published in Transactions on Machine Learning Research (12/2024)
C Empirical Evaluation of Theorem 5.2
In this section, we use the same synthetic dataset generated in Section 7.1 to validate Theorem 5.2. We keep
all the experimental settings as the same as Section 7.1, but use a different predictor g(ˇy,u). The form of
g(ˇy,u)is
g(ˇy,u) =p1ˇy1.5+p2ˇy+h(u),
withp1=T
2. It is obvious that gsatisfies property (i) and (ii) in Theorem 5.2. Because in the synthetic
causal model, yis always larger than 0, property (iii) is also satisfied.
Table 3: Results on Synthetic Data for Theorem 5.2: comparison with two baselines, unfair predictor (UF)
and counterfactual fair predictor (CF), in terms of accuracy (MSE) and lookahead counterfactual fairness
(AFCE, UIR).
Method MSE AFCE UIR
UF 0.036±0.003 1.296±0.000 0%±0
CF 0.520±0.045 1.296±0.000 0%±0
Ours 0.064±0.001 0.930±0.001 28.2%±0
Table 3 displays the results of our method compared to the baselines. Except for our method, UF and
CF baselines cannot improve LCF. When the properties in Theorem 5.2 are all satisfied, our method can
guarantee an improvement of LCF.
D Empirical Evaluation of Theorem 5.3
We generate a synthetic dataset with the structural function:
Y= (αU+eA)2
3.
The domain of Uis(0,1).αis sampled from a uniform distribution and set as 0.5987 in this experiment.
In this case, ˜f(s) =s2
3. Therefore, property (i), (ii) are satisfied. Since s>e, we haveM=1
9e−2
3. We use
a predictor
g(ˇy) =p1ˇy2+p2+h(u),
and choose u=1
2ηM. Table 4 displays the results. Our method achieves a large improvement in LCF.
Table 4: Results on Synthetic Data for Theorem 5.3: comparison with two baselines, unfair predictor (UF)
and counterfactual fair predictor (CF), in terms of accuracy (MSE) and lookahead counterfactual fairness
(AFCE, UIR).
Method MSE AFCE UIR
UF 0.012±0.001 1.084±0.000 0%±0
CF 0.329±0.019 0.932±0.007 14.0%±0.65%
Ours 5.298±1.704 0.124±0.086 88.6%±7.93%
It should be noticed that although CF predictor improves AFCE, it is not contradictory to Theorem 4.1
because Theorem 4.1 is for LCF not relaxted LCF.
27Published in Transactions on Machine Learning Research (12/2024)
E Empirical Evaluation of Theorem 5.4
We generate a synthetic dataset following the structural functions 10. We choose a1= 1,a2= 2andd= 10.
We generated 1000 data samples. The parameters used in the structural functions are displayed as follows.
α=
0.37454012
0.95071431
0.73199394
0.59865848
0.15601864
0.15599452
0.05808361
0.86617615
0.60111501
0.70807258
;β=
0.02058449
0.96990985
0.83244264
0.21233911
0.18182497
0.18340451
0.30424224
0.52475643
0.43194502
0.29122914
;w=
0.61185289
0.13949386
0.29214465
0.36636184
0.45606998
0.78517596
0.19967378
0.51423444
0.59241457
0.04645041
;γ= 0.60754485
To construct a predictor g(ˇy)satisfies property (ii) and (iii) described in Theorem 5.4, we set
Table 5: Results on Synthetic Data for Theorem 5.4: comparison with two baselines, unfair predictor (UF)
and counterfactual fair predictor (CF), in terms of accuracy (MSE) and lookahead counterfactual fairness
(AFCE, UIR).
Method MSE AFCE UIR
UF 0.036±0.002 17.480±0.494 0%±0
CF 1.400±0.098 11.193±1.019 35.9%±11.6%
Ours 1.068±0.432 0.000±0.000 100% ±0
g(ˇy) =p1ˇy2+p2ˇy+p3,
withp1=1
2η(a1a2∥w⊙α∥2
2+γ2). Table 5 displays the experiment results. In this case, CF improved LCF.
However, we know that there is no guarantee that CF predictor will always improve LCF. And our method,
not only can provide the theoretical guarantee, but also achieves a better MSE-LCF trade-off.
F Empirical Evaluation On Real-world (Loan) Dataset
We measure the performance of our proposed method using Loan Prediction Problem Dataset (kag). In this
experiment, the objective is to forecast the Loan Amount ( Y) of individuals using their Gender ( A), income
(X1), co-applicant income ( X2), married status ( X3) and area of the owned property ( X4).
The causal model behind the dataset is that there exists an exogenous variable Uthat represents the hidden
financial status of the person. The structural functions are given as
X1=N(wA
1A+wU
1U+w3
1X3+ +w4
1X4b1,σ1),
X2=N(wA
2A+wU
2U+w3
2X3+ +w4
2X4b2,σ2),
Y=N(wA
YA+wU
YU+w3
YX3+ +w4
YX4bY,1).
X3andX4have no parent nodes. We use the same implementation as what we use in the experiments for
the Law School Success dataset. Table 6 shows the results of our method compared to the baselines. Again,
our method can achieve perfect LCF by setting p1asT
2. Compared to CF predictor, our method has only
a slightly larger MSE, but our LCF is greatly improved.
28Published in Transactions on Machine Learning Research (12/2024)
Table 6: Results on Loan Prediction Datset: comparison with two baselines, unfair predictor (UF) and
counterfactualfairpredictor(CF),intermsofaccuracy(MSE)andlookaheadcounterfactualfairness(AFCE,
UIR).
Method MSE ( ×104) AFCE UIR
UF 1.352 ±0.835 11.751±0.848 3.49%±7.19%
CF 2.596 ±0.255 11.792±0.790 0%±0
Ours (p1=T/4) 2.646±0.540 5.896±0.395 50%±3.35%
Ours (p1=T/2) 2.733±0.197 0.001±0.000 100% ±0
29