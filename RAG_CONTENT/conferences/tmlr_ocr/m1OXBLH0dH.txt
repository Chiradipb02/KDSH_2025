Published in Transactions on Machine Learning Research (04/2024)
Stochastic Direct Search Methods
for Blind Resource Allocation
Juliette Achddou juliette.achdou@gmail.com
Department of Computer Science
Università degli Studi di Milano
Olivier Cappé olivier.cappe@cnrs.fr
Department of Computer Science
ENS Paris
Aurélien Garivier aurelien.garivier@ens-lyon.fr
Department of Mathematics
ENS Lyon
Reviewed on OpenReview: https: // openreview. net/ forum? id= m1OXBLH0dH
Abstract
Motivated by programmatic advertising optimization, we consider the task of sequentially
allocating budget across a set of resources. At every time step, a feasible allocation is chosen
and only a corresponding random return is observed. The goal is to maximize the cumulative
expected sum of returns. This is a realistic model for budget allocation across subdivisions
of marketing campaigns, with the objective of maximizing the number of conversions. We
study direct search (also known as pattern search) methods for linearly constrained and
derivative-free optimization in the presence of noise, which apply in particular to sequential
budget allocation. These algorithms, which do not rely on hierarchical partitioning of the
resource space, are easy to implement; they respect the operational constraints of resource
allocation by avoiding evaluation outside of the feasible domain; and they are also compat-
ible with warm start by being (approximate) descent algorithms. However, they have not
yet been analyzed from the perspective of cumulative regret. We show that direct search
methods achieves ﬁnite regret in the deterministic and unconstrained case. In the presence
of evaluation noise and linear constraints, we propose a simple extension of direct search
that achieves a regret upper-bound of the order of T2/3. We also propose an accelerated
version of the algorithm, relying on repeated sequential testing, that signiﬁcantly improves
the practical behavior of the approach.
1 Introduction
1.1 Motivation: Blind Resource Allocation
In the ﬁeld of programmatic marketing, advertisers are given daily budgets that they are required to entirely
distribute across a number of predeﬁned subdivisions of a campaign. Their goal is to maximize some notion
of cumulative reward during the lifetime of the campaign, corresponding to the number of clicks or purchases
generated by the campaign. The expected reward generated by each subdivision every day is an unknown
function of the daily budget allocated to that campaign. We focus on the context in which, when choosing
a speciﬁc allocation, the advertiser only observes a noisy version of the total reward. The optimization
task faced by the advertiser can thus be formalized as a continuous resource allocation problem, under
zeroth order and noisy feedback. Indeed, a key operational constraint is the impossibility to directly access
higher-order (derivative) information. Furthermore, every noisy evaluation of the objective function has a
1Published in Transactions on Machine Learning Research (04/2024)
cost related to the value of the function, that needs to be accounted for in the performance criterion. The
cumulative reward is thus more relevant than alternative traditional measures of performance based, for
instance, on the distance to the optimum or the norm of the gradient of the objective function reached after
some iterations. Note that the resource allocation task that we consider is diﬀerent from that in which
the resource constraints are cumulative, i.e. where the budget spans the whole period instead of one day
or time-step. Cumulative and step-level constraints lead to distinct optimization problems, none of which
being a reduction of the other.
The blind resource allocation task may be seen as a speciﬁc instance of the more general model of
zeroth-order linearly constrained optimization considered in this paper. For resource allocation, we assume
that the learner has access to d+ 1∈Ndiﬀerent resources. To keep up with the dominant convention
in optimization, we consider a minimization problem for which the costs may be thought of as minus the
rewards. At each round t∈{1, . . . T}, the learner is allowed to choose her level of consumption of each
resource, on a continuous scale from 0to1. We impose that the consumption levels of all the resources sum
to1(corresponding to the constraint of spending all the daily budget in the advertising context). The use
of resource i∈{1, . . . , d + 1}to a level of x(i)
tgenerates an expected marginal cost wi(x(i)
t). Overall, the
expected one-step cost of the learner is given by/summationtextd+1
i=1wi(x(i)
t), where the set of all possible consumption
levels (x(1)
t, . . . , x(d+1)
t )corresponds to the ddimensional simplex. The goal of the learner is to sequentially
minimize the expected cumulative cost over Tevaluations of the function, having access only to a noisy
version of the expected cost associated to the allocation tried at step t. Not only are the cost functions w1
towd+1unknown, but one cannot observe their individual outputs.
1.2 Model
While the main application of interest to us is resource allocation, our results are valid for more general
linearly constrained optimization problems. We consider a generic optimization domain Dthat is a subset
ofRddeﬁned by linear constraints: D={x∈Rd, AIx≤u}with m∈N,AI∈Rm×d,andu∈Rm. At each
round t∈{1, . . . T}, the learner selects xt∈D and incurs a cost f(xt) +ϵt, where ϵtis assumed to be a
centered σ-subgaussian noise, with σknown to the learner.
The goal of the learner is to minimize the cumulative cost over Tevaluations of the function, or equivalently
tominimize the cumulative regret
RT=T/summationdisplay
t=1f(xt)−f(x⋆),
denoting by x⋆= arg minx∈Df(x)the optimal allocation. We make the following regularity assumption on f.
Assumption 1. fis continuously diﬀerentiable, β-smooth and a-strongly convex on D.
Note that under Assumption 1,fhas a unique minimum and is bounded from below. Assumption 1is com-
mon in online optimization due to the diﬃculty of controlling the cumulative cost without this assumption.
In programmatic advertising and economics, it is common to observe marginal returns that decrease as
the level of a resource increases (following the so-called law of diminishing returns). Assuming convexity
offonDis therefore reasonable, since Assumption 1implies that when fhas the form f(x(1)
t, . . . x(d)
t) =/summationtextd
i=1wi(x(i)
t) +wd+1/parenleftBig
1−/summationtextd
i=1x(i)
t/parenrightBig
, the marginal cost functions w1towd+1are also convex and hence
satisfy the law of diminishing return (viewing −wias the marginal utility associated to the i-th resource).
1.3 Related Works
The discrete counterpart of the resource allocation model in which the resources can only be used up to
discrete consumption levels, is a celebrated model of operations research with multiple applications. Its
properties have ﬁrst been discussed by Koopman (1953) who proposed the ﬁrst algorithmic solution for this
problem. Koopman’s works have further been extended by Gross (1956);Katoh et al. (1979) who propose
more eﬃcient algorithms under speciﬁc assumptions on the number of resources and the total consumption
2Published in Transactions on Machine Learning Research (04/2024)
budget. The range of applications is wide, including experimental design, load management in an industrial
context, computer scheduling and, more recently, the adwords problem introduced by Mehta et al. (2007).
Recently, Agrawal & Devanur (2015) studied online and oﬄine resource allocation, motivated by the latter
task. With the same motivation, Fontaine et al. (2020) focus on the online and continuous version of resource
allocation in which the learner accesses the derivatives. The method studied by Fontaine et al. (2020) extends
the bisection method in dimension d > 1.
The broader problem of derivative-free optimization in noisy environments has been considered by re-
searchers coming from diﬀerent horizons. A relevant stream of works originates from the bandit community,
which considered this task as an extension of the more traditional multi-armed bandit problem (see e.g
Auer et al. ,2002). The class ofX-armed bandits models focuses on the case where a learner can select
actions in a generic measurable space and the mean-payoﬀ function is regular. In ( Bubeck et al. ,2011),
for example, the mean payoﬀ function is supposed to be locally Lipschitz with respect to some dissimilarity
measure. Bubeck et al. (2011) and Munos (2014) adopt the approach of hierarchical optimization, in which
the optimization domain is iteratively partitioned, resulting in ﬁner and ﬁner partitions, that are required
to be balanced in some sense. The learner maintains an upper conﬁdence bound of the goal function that
is constant on each cell deﬁned by the ﬁnest partition. The algorithm proposed by Bubeck et al. (2011)
achieves a regret of the order of√
Twhen the learner knows the exact order of the smoothness at the
optimum. However, partitioning the domain in a hierarchical and balanced way is relatively easy when the
domain is an hypercube, but is a computational problem in itself when the domain has a more complex
form. We also mention that knowing the smoothness is considered a challenge most of the time in black-box
optimization, so that several methods have been introduced that are adaptive to the smoothness ( Locatelli
& Carpentier (2018);Valko et al. (2013);Shang et al. (2019)). We mention that concurrently to HOO
based on hierarchical partitioning, Agarwal et al. (2011) has also proposed a diﬀerent strategy for X-armed
bandits, but this time convex, with ellipsoid methods, that also result in O(√
T)regret.
The extension of more traditional ﬁrst-order optimization methods has also been considered. When the
function evaluation is not perturbed by any noise, Nesterov & Spokoiny (2017) consider random gradient
descent based on ﬁnite diﬀerences to estimate the gradient. Flaxman et al. (2004) consider a version of
stochastic gradient descent with a one-point estimate of the gradient for the adversarial setting introduced
byZinkevich (2003) in which at each time step, a new goal function is chosen by an adversary, making it
impossible to rely on a two-point estimate of the gradient. In this setting, Flaxman et al. (2004) show an
adversarial regret bound of the order of T5/6. Later on, Hazan & Levy (2014),Hazan & Li (2016),Bubeck
et al. (2017) propose new methods for the same setting, but with adversarial convex or strongly-convex
functions, showing improved regret bounds, as low as√
T. In a stochastic setting that is closer to ours,
Akhavan et al. (2020);Bach & Perchet (2016) consider a version of stochastic gradient descent with unbiased
estimates of the gradient, obtained by ﬁnite diﬀerences. While they provide an analysis in term of the
regret, they focus on a restricted notion of regret that is diﬀerent from the one considered in this work. The
algorithms that they propose rely on a number of samples used to estimate the gradient at each iteration
of the gradient descent algorithm. But the regret only accounts for the cost incurred by the iterates of
the gradient descent algorithm and ignores the regret incurred by the samples used for the estimation of
the gradient. Moreover, the constraints also do not apply to those samples, meaning that the algorithm is
allowed to get samples outside of the feasible domain in order to estimate the gradient. The authors prove
an upper bound on their version of the regret, which is of the order of√
Twhen Assumption 1is satisﬁed.
In Section 2.2below we will see how evaluations outside of the feasible domain can be avoided, using for
instance ideas of Bravo et al. (2018), at the price of an increased regret rate.
1.4 Contribution
In this paper, we focus instead on a class of simple but mathematically well grounded algorithms known
as direct or pattern search methods. Direct search ( Kolda et al. ,2003) makes use of the well-known fact
that if the objective function is continuously diﬀerentiable, then at least one of the directions of any positive
spanning set (a set that spans the space with non-negative coeﬃcients, abbreviated as PSS in what follows)
is a descent direction. It explores the space by evaluating the function at new points that are located in a
number of predeﬁned search-directions from the current iterate, at a distance from that iterate that varies
3Published in Transactions on Machine Learning Research (04/2024)
with time. The algorithm moves to a new iterate only if this iterate yields a suﬃcient improvement of the
value of the function (there exist other versions of the algorithm where the suﬃcient decrease condition is
replaced with a constraint on the choice of the trial directions). The sample-complexity of such an algorithm
has been analyzed in ( Vicente ,2013) in the deterministic and unconstrained setting. Lewis & Torczon (2000)
study direct search in linearly constrained domains. Handling the constraints in direct search is quite simple,
as it consists in testing only the directions in the set of search-directions that are feasible. Gratton et al.
(2015) analyzed direct search with random sets of search-directions instead of predeﬁned ones and later
extended the analysis to the case of linearly-constrained domains ( Gratton et al. ,2019).Dzahini (2022)
extended their work by analyzing a similar algorithm in the presence of noise, but without constraints.
Dzahini (2022) relies on an assumption on the decrease of the noise. In this paper, we will study direct
search algorithms that rely on a number of samples at each point to build tight estimates of the function
at the trial points, which can be understood as a way to decrease the noise. Dzahini (2022) analyzes some
notion of sample complexity of direct search, which only takes the iterates into account rather than the
number of function evaluations needed, which is not appropriate in our setting.
Our purpose is to study these methods that are suitable for the blind resource allocation model, i.e. in
particular, compatible with zeroth-order feedback, computationally tractable and that do not require to
sample points outside of the feasible domain. Besides satisfying the above requirements, these algorithms
have the advantage of being approximate descent algorithms with high probability, a guarantee that is
useful in practice, allowing, for instance, warm start from previously tested allocations. The adaptation
of direct search to the noisy case is achieved by performing enough sampling to ensure that the algorithm
moves to a new iterate only if it results in a suﬃcient improvement, with high probability. We propose two
ways of doing so: the ﬁrst method (termed FDS-Plan) simply computes the number of necessary evaluations
ahead of time, whereas the second one, FDS-Seq, uses a sequential testing strategy to interrupt sampling as
early as possible. The algorithms are speciﬁed in Section 2. An illustration of the behavior of the proposed
algorithms can be found in this same section, alongside an illustration of other baseline strategies, which
allows for understanding the speciﬁcs of direct search. We analyze the cumulative regret of these algorithms
in Section 3, providing an upper bound of their regret of the order of T2/3(up to logarithmic factors), when
the optimum is in the interior of the feasible domain. A signiﬁcant technical challenge for the analysis in
terms of regret is that, while in traditional analyses of direct-search, the number of rounds is ﬁxed and the
analysis proceeds by looking at the distance to the optimum at each round, here, the number of rounds
is random (the indexing of the regret is the actual number of function evaluations instead of the number
of rounds). We start Section 3by the simpler case in which there is neither noise nor constraints, showing
that in this basic setup the regret of direct search is bounded by a constant.
2 Algorithms
2.1 Description of the Algorithms
In Algorithm 1below, we start by describing the most common version of the direct search method used
for deterministic and unconstrained optimization. It requires the setting of an initial point x0and an initial
parameter α0. The learner also speciﬁes a PSS D, that is, a set of directions that spans Rdwith non-negative
coeﬃcients. At each iteration, the algorithm sequentially tests points at a distance αkfrom the current
iterate and in the directions deﬁned by D. If none of the test points results in a suﬃcient decrease of the
function’s value, the iteration is declared unsuccessful and the trial radius αkshrinks by a factor θ < 1,
otherwise, the iteration is declared successful and the iterate xkis moved to the ﬁrst trial point that results
in a suﬃcient improvement. A decrease is considered to be suﬃcient if it is larger than some predeﬁned
forcing function of αk, that we take here to be quadratic, with a coeﬃcient that can be set by the learner.
The analysis can also be adapted to the presence of a growth factor ϕ≥1by which the trial radius αkexpands
at successful iterations. For simplicity, we choose to focus on the case where ϕ= 1, as this parameter does
not modify the regret rates obtained in Section 3. Also note that Algorithm 1is a descent algorithm with
respect to the iterates xk, i.e., the sequence (f(xk))kis decreasing.
4Published in Transactions on Machine Learning Research (04/2024)
Choose x0∈Rd,α0>0,θ < 1,c >0,ρ(u) =cu2and a PSS D
fork= 0. . . K do
SetUnsuccessfulSearch ←True
forv∈Ddo
Evaluate f(xk+αkv)
iff(xk)−f(xk+αkv)≥ρ(αk)then
Setxk+1←xk+αkvandαk+1←αk
SetUnsuccessfulSearch ←False
Break
end
end
ifUnsuccessfulSearch then
Setxk+1←xkandαk+1←θαk
end
end
Algorithm 1: Direct Search with suﬃcient decrease
Obviously, diﬀerent choices of PSS result in diﬀerent trajectories of the algorithm. Setting Das the set of
2dvectors of the positive and negative coordinate directions results in the algorithm known as coordinate
or compass search. Other frequently considered choices include random directions, as in ( Gratton et al. ,
2015;2019;Dzahini ,2022).
Choose x0∈Rd,α0>0,θ < 1,c >0,ρ(u) =cu2
fork= 0. . . K do
SetUnsuccessfulSearch ←True
Select a set of directions Dk
SetNk=32σ2log(2 /δ)
ρ(αk)2.
Estimate f(xk)by making Nksamples at xk
and setting ˆf(xk) =1
Nk/summationtextNk
j=1f(xk) +ϵj
forv∈Dksuch that xk+αkv∈Ddo
Estimate f(xk+αkv)by making Nk
samples at xk+αkvand setting
ˆf(xk+αkv) =1
Nk/summationtextNk
j=1f(xk+αkv) +ϵ′
j
ifˆf(xk)−ˆf(xk+αkv)≥ρ(αk)then
Setxk+1←xk+αkvandαk+1←αk
SetUnsuccessfulSearch ←False
Break
end
end
ifUnsuccessfulSearch then
Setxk+1←xkandαk+1←θαk
end
end
Algorithm 2: FDS-PlanChoose x0∈Rd,α0>0,δ >0,θ < 1,c >0,
ρ(u) =cu2
SetUnsuccessfulSearch ←True
fork= 0. . . K do
Select a set of directions Dk
forvinDksuch that xk+αkv∈Ddo
while Condition 1is not satisﬁed do
ifnv,k≤n0,kthen
Sample at xk+αkvand update
nv,kand the empirical mean
ˆfnv,k(xk+αkv).
end
else
Sample at xkand update n0,kand
the empirical mean ˆfn0,k(xk)
end
end
ifˆfn0,k(xk)−ˆfnv,k(xk+αkv)≥ρ(αk)
then
Setxk+1←xk+αkv, and αk+1←αk.
UnsuccessfulSearch ←False
Break.
end
end
ifUnsuccessfulSearch then
Setxk+1←xkandαk+1←θαk
end
end
Algorithm 3: FDS-Seq
We apply three sorts of modiﬁcations to Algorithm 1in order to adapt it to the more general model introduced
in Section 1.2. The ﬁrst one consists in sampling a trial point only if it is feasible. The second consists in
5Published in Transactions on Machine Learning Research (04/2024)
allowing changes in the set of directions Dkconsidered. This is to account for the fact that the change in
search-radius at every round impacts the set of admissible directions, denoted Ak. We thus only need to
sample directions that span Akpositively, and not the whole optimization domain D. The third modiﬁcation
consists in introducing estimation stages that allow building reliable estimates of fat the trial points. We
propose two ways of doing so, that result in two diﬀerent algorithms. The ﬁrst algorithm that we study is
a plug-in version of Algorithm 1in which we replace f(xk)andf(xk+αkv)with their empirical estimates,
consisting of means computed from Nk=32σ2log(2 /δ)
ρ(αk)2samples. This number of samples guarantees that
with high probability, the estimation gap is smaller than ρ(αk)/4, which in turn ensures that an iteration is
declared successful only when it leads to a decrease of f(xk)by at least ρ(αk)/2and that an unsuccessful
iteration cannot occur if there exists a direction vinDksuch that the decrease achieved by moving to xk+αkv
is larger than 3ρ(αk)/2. The resulting algorithm is termed Feasible Direct Search with a planned number
of samples (FDS-Plan) and described in Algorithm 2. We also propose a faster algorithm, Feasible Direct
Search with Sequential Tests (FDS-Seq) described in Algorithm 3. For any v∈Dk, instead of planning the
number of samples at xk+αkvahead of time, it samples at xk+αkvandxkuntil either


/vextendsingle/vextendsingle/vextendsingleˆfn0,k(xk)−ˆfnv,k(xk+αkv)−ρ(αk)/vextendsingle/vextendsingle/vextendsingle≤/radicalbigg
2σ2log(1 /δ)/parenleftBig
1
n0,k+1
nv,k/parenrightBig
,
or(n0,k≥Nkandnv,k≥Nk),(1)
n0,kandnv,kdenoting the number of samples at xkandxk+αkvand ˆfn0,k(xk)and ˆfnv,k(xk+αkv)the
resulting empirical means. Successful and unsuccessful iterations are deﬁned as in FDS-Plan and trigger
the same actions.
The sequential stopping rule is designed to achieve early detection of suﬃcient decrease, but also to detect
as early as possible the cases in which the trial point cannot lead to a suﬃcient decrease. The ﬁrst test in
Condition 1is a consequence of the fact that the estimation gap at xk(respectively xk+αkv) isσ2/n0,k
subgaussian (respectively σ2/nv,ksubgaussian). The second test of Condition 1corresponds to a safeguard
preventing from waiting too long when the decrease induced by the trial point is very close to the suﬃciency
threshold. At worst, the number of evaluations needed is the same as in Algorithm 2. Essentially, the
sequential stopping rule reduces the number of evaluations needed for each iteration but maintains the
desirable property that with high probability, an iteration is declared successful only if it leads to a decrease
of at least ρ(αk)/2and that an iteration cannot be declared unsuccessful if there exists a direction vinDk
such that the decrease achieved by moving to xk+αkvis larger than 3ρ(αk)/2.
2.2 Illustration
In order to illustrate graphically the behavior of the proposed methods, we show on Figure 1(a)and1(d) their
trajectories in the case where there are 3 resources ( d= 2) and the loss functions associated to each resource
i∈{1, . . . , d + 1}are of the form wi(x) =−τilog(1+ γx)
log(1+ γ)with γ= 2,τ1= 1,τ2= 0.45, and τ3= 0.95. We set
the horizon to T= 100 ,000and use a Gaussian noise with standard deviation σ= 0.1, a realistic value for
budget allocation problems. In the symmetric representation of Figure 1, the three vertices correspond to the
points where one of the resource is fully saturated (equal to 1) and the edges correspond to linear paths along
which one of the resources is set to 0. The contour lines of the target function are materialized by orange
lines and the location of the minimum is marked by a black cross. The size of each point is a logarithmically
growing function of the number of samples made at this point, and its color is a function of the index of
the ﬁrst round at which it has been sampled. Finally, points at which a successful iteration of FDS-Plan
(and FDS-Seq) occurred are circled in blue. The parameters of both versions of feasible direct search are
α0= 0.2,c= 5andθ= 0.7, and the initial point corresponds to the allocation x(0) = (1 /3,1/3,1/3)(center
of the simplex). To make the ﬁgure more interpretable, we choose to set a ﬁxed D. The set of directions
chosen for these algorithms are the 6 directions that support the edges of the simplex (in both directions).
In a ﬁrst phase, the algorithm proceeds rapidly by testing directions until a suﬃcient descent direction is
found. Afterwards, when the iterates get closer to the minimizer, the search area iteratively shrinks as
ﬁnding descent directions becomes harder. In the ﬁrst phase, the trajectory is similar to a descent path
that would result from a gradient descent algorithm while the second phase is closer to the behavior of
6Published in Transactions on Machine Learning Research (04/2024)
-0.955 -0.866-0.777
-0.689
100102104
(a) FDS-Plan
-0.955 -0.866-0.777
-0.689
100102104
 (b) HOO
 (c) Gradient Descent with
two-points estimate
-0.955 -0.866-0.777
-0.689
100102104
(d) FDS-Seq
-0.955 -0.866-0.777
-0.689
100102104
 (e) UCB
 (f) Gradient Descent with
one-point estimate
Figure 1: Single trajectories
bandit algorithms based on hierarchical partitions, like HOO ( Bubeck et al. ,2011). In order to illustrate the
diﬀerences with such algorithms, we also plot the trajectories of baseline methods, either related to gradient
descent or bandits with hierarchical partitioning. Before turning to these other algorithms, it is important
to note the diﬀerence between FDS-Plan and FDS-Seq. Figure 1(d) shows that FDS-Seq is faster than FDS-
Plan, as it spends less time on the ﬁrst iterations, in which it is easy to determine whether the trial points
lead to a suﬃcient decrease. The FDS-Seq algorithm can thus perform more iterations than FDS-Plan. A
common point of these two algorithms that is illustrated on the ﬁgure is that they are approximate-descent
algorithms with high probability, an interesting quality for practitioners interested in interpretability.
Let us now comment on Figure 1(b), that represents the trajectory of a version of HOO. It is not straight-
forward to apply algorithms for X−armed bandits like HOO on the simplex, because it implies constructing
balanced hierarchical partitions of the simplex. We thus explain our implementation of HOO in Appendix
E. On Figure 1(b), we observe that this algorithm explores the partition tree in a way that favors the cells
close to the optimum while persistently visiting cells that are clearly far from the minimizer. The behavior
of algorithms based on direct search can thus be preferred because it makes warm-start possible, in the
sense that prior belief on the location of the minimizer can be used for setting the initial point, which is not
possible for HOO. The fact that suboptimal points will keep being sampled until the end of the experiment
can also be diﬃcult to accept for practitioners such as advertisers for example.
We also illustrate the behavior of UCB on a discretization of the space, which is an interesting strategy,
especially in dimension 2. The discretization used for UCB consists of points arranged in a regular grid of
[0,1]2, from which the points lying outside of the feasible domain have been removed. The step parameter
of the grid is taken as T−1/4as suggested by Combes & Proutiere (2014). Although simpler than HOO,
this algorithm results in similar sampling patterns, as shown on Figure 1(e), and hence shares some of its
drawbacks. The performance of UCB is good in dimension 2, as the regret can be proved to be of the order
of√
Twith this choice of step-size (see Combes & Proutiere (2014)) but it will worsen in higher dimension
due to the diﬃculty of simultaneously controlling the distance between grid points and the overall number of
7Published in Transactions on Machine Learning Research (04/2024)
points in the grid. In fact, the optimal step in this case is of the order of T−1/(d+2)and the resulting regret
is of the order of Td/d+2, which only works in favor of UCB for small values of d. Note that while Combes
& Proutiere (2014) also provide weaker regret guarantees under more general assumptions, the assumptions
required to obtain this order of regret are similar to Assumption 1. They are only less constraining than
Assumption 1in that they require a quadratic upper and lower bound on the function locally near the
optimum, whereas Assumption 1should hold uniformly on the domain.
Lastly, we illustrate the behavior of two methods related to stochastic gradient descent. The ﬁrst method
is related to that proposed by Akhavan et al. (2020). Without constraints, this method would estimate the
gradient of the function at xt, by evaluating the function at y+
t=xt+htZtandy−
t=xt−htZt, where Zt
is a random vector of the sphere of radius 1, and usef(y+
t)−f(y−
t)
htZtas an estimation of the gradient. The
method in itself does not take constraints into account, but a slight modiﬁcation results in an algorithm that
is feasible in the presence of constraints. This modiﬁcation consists in performing a homothetic perturbation
(Bravo et al. ,2018) on the evaluation points y+
tandy−
t: instead of using these points, the algorithm evaluates
the function at ˜yt=yt+ht/r(c−xt), where cis a point in the interior of the simplex such that B(c, r)⊂D.
We use ˜y+
t=y+
t+ht/r(c−xt)and ˜y−
t=y−
t−ht/r(c−xt), where cis a point in the interior of the simplex
such thatB(c, r)⊂D. This ensures that the evaluation point belongs to the constrained domain, provided
that xt∈D, but adds a bias which is proportional to ht, under suitable regularity assumptions on f. To
ensure that xt∈D, we also project the result of the gradient descent step on D. We use an estimation step
htequal to (t/2)−1/3and a learning rate decreasing as 1/(2.5t). This choice is justiﬁed by the following
reasoning: with this choice of value for the learning rate and htset to t−1/4,/summationtextT
t=1f(xt)−f(x∗)would be
bounded by O(T1/2), thanks to the analysis of Akhavan et al. (2020); but we have to add a term related to
the sum of evaluation steps/summationtextT/2
t=1(f(y+
t) +f(y−
t)−2f(x∗))to bound the actual regret, which under suitable
assumptions is of order/summationtextT
t=1ht; so that setting htto(t/2)−1/3allows to bound both terms by O(T2/3). On
Figure 1(c), we see one trajectory of this method when the starting iterate is on the center of the simplex. The
convergence speed is rather fast at the beginning but the speed is limited by the homothetic perturbation.
The second method is inspired by Flaxman et al. (2004). This paper proposes to use gradient descent with a
one-point gradient estimation. In order to evaluate the gradient of the function at xt, the algorithm evaluates
the function at yt=xt+htZt, where Ztis a random vector of the sphere of radius 1, and uses f(yt)/htzt
as an estimation of the gradient. As for the previous method, we apply an homothetic perturbation to yt.
We use an estimation step htequal to t−1/3and a learning rate decreasing as 1/(2.5t). The trajectory that
we see on Figure 1(f)is not very indicative of the average performance of the algorithm, since this method
comes with a very high variance. We see that the algorithm generates a trajectory that roughly gets closer
to the minimizer, but that is far from being a descent path because of the poor estimation of the gradient.
This method, which has been designed for adversarially evolving objective functions, is clearly not advisable
for static objectives with stochastic perturbations.
3 Regret Analysis
As discussed in the introduction, the regret criterion takes into account the number Tof function evaluations
instead of focusing on the number Kof iterations, as in the more traditional analysis. In the noiseless case
of Algorithm 1,TandKdiﬀer by a factor of at most |D|+1and this is not an issue. However, in the case
of Algorithms 2and3, the situation is very diﬀerent as the number of function evaluations per iteration is
stochastic and typically increases as the algorithm converges. In this case, it is not possible to predict in
advance the evolution of Tas a function of Kbecause it depends on the function and starting point. A
signiﬁcant part of the analysis is indeed devoted to quantifying this phenomenon. In practice, it means that
in order to comply with a number Tof function evaluations set in advance, the algorithms are run without
a ﬁxed number of rounds K, instead they are run until the number of function evaluations reaches T.
In the following, we analyze the proposed algorithms and show that in the constrained and noisy set-up of
interest, FDS-Plan and FDS-Seq have a regret of the order of (logT)2/3T2/3under some further assumptions
onfand on the chosen direction set Dk, provided that the optimal point lies in the interior of the feasible do-
8Published in Transactions on Machine Learning Research (04/2024)
main. To provide some intuition on the proofs, we start with the analysis of Algorithm 1in the unconstrained
and deterministic setting, thereby providing the ﬁrst regret bound of any direct search algorithm.
3.1 Warm-up: the Unconstrained and Deterministic Setting
The choice of Dis decisive for the performance of both FDS-Plan and FDS-Seq. In the sequel, we make the
following assumption on D.
Assumption 2. The vectors of Dhave unit norm and the cosine measure of Dis lower-bounded, i.e, there
exists κ > 0such that
cm(D) := min
u∈Rd,u̸=0max
v∈DuTv
∥u∥∥v∥> κ .
Assumption 2, common in direct search’s literature, guarantees that at each iteration k, the cosine similarity
between at least one direction in Dand−∇f(xk)is larger than κ. IfDis a PSS there exists a κsatisfying it.
Theorem 1. Under Assumptions 1and2, the cumulative regret of Algorithm 1satisﬁes
RT≤|D|+1
c/bracketleftBigg/parenleftbigg1
1−θ2(f(x0)−f(x⋆) +ρ(α0))/parenrightbigg/parenleftBig/parenleftBig
1 +η
a/parenrightBig
η+β/parenrightBig
+ (f(x0)−f(x⋆))/parenleftbiggβ
aα0∥∇f(x0)∥+β/parenrightbigg/bracketrightBigg
,
where η:=β
a1
κθ(c+β
2).
This result shows that under Assumption 1, the asymptotic behavior of the regret of direct search can be
compared to that of the more traditional gradient descent algorithm, whose regret is also bounded under
this assumption (see Theorem 3.6 of Bubeck et al. ,2008).
3.1.1 Elements of Proof
The proof of Theorem 1combines two well-known properties of direct search and the following lemma holding
for any descent algorithm.
Lemma 1. Iffsatisﬁes Assumption 1, then∀k′> k,∥∇f(xk′)∥≤β
a∥∇f(xk)∥.
In the proof of Theorem 1, Lemma 1is used in conjunction with the following well-known property (see e.g
Vicente ,2013) of direct search.
Lemma 2. Iffsatisﬁes Assumptions 1and2and iteration kcorresponds to an unsuccessful iteration, then
∥∇f(xk)∥≤1
κ/parenleftbiggβ
2αk+ρ(αk)
αk/parenrightbigg
=1
κ/parenleftbiggβ
2+c/parenrightbigg
αk.
The above lemma follows from the deﬁnition of the cosine measure of D, that results in a bound of
vT
k(−∇f(xk)), where vkis the direction in Dmaximizing the gap f(xk)−f(xk+αkv), and from the smooth-
ness assumption on f. Thanks to Lemma 1, this lemma also means that when iteration kis unsuccessful,
we can bound all subsequent gradients byβ
κa(β
2+c)αk. We can deduce that for any k′following the ﬁrst
unsuccessful iteration, ∥∇f(xk′)∥≤ηαk′, where η:=β
a1
κθ(c+β
2). Indeed, if k′is the index of an unsuccessful
iteration, Lemma 1suﬃces to prove ∥∇f(xk′)∥≤ηθα k′. In contrast, when k′is the index of a successful
iteration, one should consider the last unsuccessful iteration k. Since αk′≥θαk, there has been at most one
reduction of the step-size since iteration kand
∥∇f(xk′)∥≤β
κaθ/parenleftbiggβ
2+c/parenrightbigg
αk′=ηαk′. (2)
The following general argument on direct search is the ﬁnal key element of the proof of Theorem 1, that
links the sum of the squared search-radius to the initial sub-optimality gap f(x0)−f(x⋆).
9Published in Transactions on Machine Learning Research (04/2024)
Lemma 3. Iffsatisﬁes Assumptions 1and2,
∞/summationdisplay
k=0ρ(αk) =∞/summationdisplay
k=0cα2
k≤1
1−θ2(f(x0)−f(x⋆) +ρ(α0)).
Lemma 3can be explained by the fact that ρ(αk)decreases geometrically by a ratio θ2between two successive
successful iterations, so that the contribution to the sum of these iterations boils down to multiplying the
remainder of the sum by a factor of1
1−θ2. The sum on successful iterations cannot be too large, because by
deﬁnition of successful iterations, f(xk)−f(x⋆)is lower bounded by this sum plus the initial sub-optimality
gapf(x0)−f(x⋆). Bringing Lemma 3and the bound of Equation 2together results in a bound on the squared
norm of the gradients ∥∇f(xk)∥2after the ﬁrst unsuccessful iteration. Using the regularity conditions of
Assumption 1suﬃces to relate the regret to the squared norm of the gradients, which in turn yields Theorem
1. The complete proof can be found in Appendix B.
3.2 The Constrained and Noisy Setting
We now turn to the noisy and constrained case described in Section 1.2. We further impose the following
assumptions on the domain.
Assumption 3. Dis contained in a ball of radius b.
This assumption, together with assumption 1, implies that f(x)−f(x⋆)is bounded. It also follows from
these two assumptions that ∇fis bounded in norm by a constant, denoted by B, on the feasible set D.
While in the unconstrained case, the chosen PSS Donly needed to satisfy Assumption 2, a stronger as-
sumption is required in the presence of linear constraints. Indeed, Assumption 2was a way to ensure that
there was at least one trial direction vinDsatisfying−∇f(xk)Tv
∥v∥∥∇f(xk)∥≥κ. This property is not suﬃcient in
the constrained case, because in this case, the directions of interest at iteration kinDkare those that are
feasible. A problem that might arise for example, is that a suﬃcient descent direction is not detected even in
a situation where xk−αk∇f(xk)is feasible, because the set of feasible directions in Dkdoes not positively
span the feasible region. To avoid such cases, we impose a constraint on Dkthat involves the notion of
approximate tangent cones. Approximate tangent cones at a point xare the polar cones of the cones that
are generated by the α-binding constraints at xas deﬁned by Kolda et al. (2007) (see in particular Figure
2.1 of Kolda et al. (2007) for an illustration of the notion of approximate tangent cones).
LetaT
ibe the i-th row of the constraint matrix AIand letCi={y,s.taT
iy=ui}denote the sets where
thei-th constraint are binding. If there exists a point of Ciat a distance smaller than αfrom x, then the
i-th constraint is said to be α-binding. The indices of α-binding constraints at xare denoted I(x, α) =
{i,dist(x,Ci)≤α},where distis induced by the Euclidian distance. We deﬁne the approximate normal
cone N(x, α)to be the cone generated by the set {ai,s.t. i∈I(x, α)}∪{ 0}. The approximate tangent
cone T(x, α)is the polar of N(x, α), which means that T(x, α) ={v:yTv≤0,∀y∈N(x, α)}.Informally
T(x, α)is the cone inside of the boundaries generated by the α-binding constraints at x. We highlight that
since the number of constraints mis ﬁnite, there can only be a ﬁnite number, smaller than 2m, of tangent
cones. Consequently, Assumption 4is rather mild.
Assumption 4. Fork∈ {1. . . K},Dkcontains a setGkof positively generating directions of T(x, α)
included in T(x, α), for any x∈D andα∈R+.
In the following, we denote by Gksuch a set. Assumption 4was already necessary in ( Kolda et al. ,2003)
andLewis & Torczon (2000), while in ( Gratton et al. ,2019), the descent set at iteration kis assumed to be
contained in T(x, α)and to generate it. We explain the purpose of this assumption in the following. While
the purpose of Assumption 2was to ensure that the maximal cosine similarity of a vector in Dkwith−∇f(xk)
was bounded away from 0, we focus on a diﬀerent kind of measure of similarity to −∇f(xk)deﬁned as
/braceleftBigg
max v∈Gk−∇f(xk)Tv
∥PT(x,α)(−∇f(xk))∥∥v∥ifPT(x,α)(−∇f(xk))̸= 0,
1otherwise.
10Published in Transactions on Machine Learning Research (04/2024)
IfPT(x,α)(−∇f(xk))gets close to 0, this measure of similarity to −∇f(xk)does not necessarily become
small, although−∇f(xk)Tvis small for any vinGk. In order to bound this measure of similarity between
a vector ofGkand−∇f(xk)away from 0, we deﬁne the following approximate cosine measure:
cmT(xk,αk)(Gk) := inf
u∈Rd,PT(xk,αk)(u)̸=0max
v∈GkuTv
∥PT(xk,αk)(u)∥∥v∥.
As proved by Lewis & Torczon (2000) and recalled by Gratton et al. (2019), ifCis a set of cones cjthat
are respectively positively generated from a set of vectors G(cj), then
λ(C) := min
cj∈C/braceleftBigg
inf
u∈Rd,Pcj(u)̸=0max
v∈G(cj)uTv
∥Pcj(u)∥∥v∥/bracerightBigg
>0,
which guarantees that cmT(xk,αk)(Gk)is bounded by some constant κmin>0, under Assumption 4. Under
the above assumptions, we can bound the regret of FDS-Plan when the optimum lies in the interior of the
feasible set. Note that assuming optimal allocation in the interior of the feasible set is crucial for analysis,
but we believe the opposite would not be harmful in practice, as supported by simulations (see Appendix F).
Theorem 2. Under Assumptions 1,3, and 4, ifx⋆∈int(D)and if|Dk|is bounded by a constant SD, the
cumulative regret RTof FDS-Plan (respectively FDS-Seq) after the ﬁrst Tevaluations of fsatisﬁes
E[RT] =O( log( T)2/3T2/3)
for the choice δ=T−4/3(respectively δ=T−10/3for FDS-Seq).
In the absence of a lower bound, the optimality of such a regret rate is unsure. It is diﬃcult to compare it
to other known bounds, as the performance of related algorithms is often not evaluated in the same way. In
particular, the performance of the version of stochastic gradient descent proposed by Akhavan et al. (2020)
is analyzed with respect to a diﬀerent notion of regret, ˜RT, that does not take into account the samples
needed for the estimation of each gradient. Their analysis yields ˜RT=O(√
T). It is important to note that
the algorithm by Akhavan et al. (2020) takes advantage of the fact that in the setting of the latter paper,
sampling points outside of the feasible domain is possible. When using homothetic perturbation as we did for
the illustration in Figure 1(c), the regret of such a method is of the order of T2/3, as explained in Section 2.2.
Black-box algorithms such as HOO or StoOO ( Bubeck et al. ,2011;Munos ,2014) are other possible baselines.
When given a balanced hierarchical partition of X, and the smoothness of the function around its optimum,
these algorithms would incur a regret of the order of√
T. The regret rate of FDS-Plan appears to be larger
than that of HOO instantiated with the right parameters. However, HOO relies on a partition of the feasible
domain that is computationally diﬃcult to achieve with arbitrary linearly constrained domains.
The assumption that |Dk|be bounded by a constant SDis actually not constraining at all, since one way of
satisfying Assumption 4is to set Dkto the constant set of vectors corresponding to edges of optimization
domains, which amounts to 2m(m−1)directions, where mis the number of constraints. Depending on the
optimization domains, there may be smarter ways of choosing |Dk|that lead to smaller constants SD. The
motivational case of resource allocation, where the feasible domain is the simplex, is an example of that.
In that case, the above method for choosing Dkyields SD= 2d(d+ 1) whereas recomputing the direction
set at every round can spare us a factor d. An intuitive way to understand this is to consider the simplex
of dimension d= 2. When the iterate is in the interior of the simplex, and the step-size αkis such that
the admissible directions in T(xk, αk)formR2, we only need d+ 1 = 3 vectors (an angle of 2π/3apart).
When T(xk, αk)is smaller, then minimal sets Gkare formed by even fewer vectors. An eﬃcient method for
recomputing the set of directions at every round is described in Griﬃn et al. (2008). It is possible to verify
that for the simplex, this method provides less than 2ddirections at each round.
This is particularly important, because the regret bound is proportional to the number of directions contained
inDk(see the proof of Theorem 2, in Appendix D.2), so that the dependence of the regret with respect to
dis linear. For the sake of comparison, the regret of the algorithm by Akhavan et al. (2020) is quadratic in
d, whereas, when HOO is perfectly parameterized, the dependence on dof the regret of HOO disappears.
Recall however, that on arbitrary linearly constrained domain, or even on the simplex in high dimension,
HOO might be computationally intractable.
11Published in Transactions on Machine Learning Research (04/2024)
3.2.1 Elements of Proof
After some ﬁnite number of iterations that depends on ∆ := min i∈{1...m}dist(x⋆,Ci), the distance from xkto
the boundaries of Dis smaller than ∆/4with high probability, thanks to the analysis of Gratton et al. (2019).
Waiting for another number of iterations, αkgets small enough for the approximate tangent cone T(xk, αk)
to describe the whole space Rd. Then, the trajectory of the algorithm is the same as in the unconstrained
setting. In the unconstrained setting, the following elements provide an intuition of why the regret is of the
order of T2/3. With similar arguments to those of the proof of Theorem 1, i.e Lemmas 1,2and3, it is easy
to see that the instantaneous regret incurred at iteration kof the algorithm is proportional to the sum of α−2
k
(up to logarithmic factors), whereas it was proportional to α2
kin the deterministic case: indeed, iteration
know involves Nktimes more evaluations than in the noiseless case and Nkis proportional to α−4
k(up to
logarithmic factors). Thanks to Lemma 3, we know that α2
kis summable. Then, thanks to Hölder’s inequality
applied to the sum of α−2
kwritten as (αk)2/3(αk)−2/3−2, the regret is proportional to the total number of
evaluations to the power of 2/3, up to logarithmic factors. The complete proof can be found in Appendix D.
4 Experiments
In our experiments, we focus on the case in which there are seven resources ( d= 6), and the loss functions
are of the same form as in Section 2.2, and wi(x) =−τilog(1+ γx)
log(1+ γ)with γ= 2,τ1= 1,τ2=τ3=τ4= 0.75,
τ5= 0.89, and τ6=τ7= 0.95. On Figure 2, we compare FDS-Seq and FDS-Plan to UCB on a discretization
of the space, and gradient descent with an homothetic perturbation. Both methods are explained in Section
2.2. The comparison with HOO is made impossible by the numerical complexity of HOO. We set the horizon
toT= 500 ,000and use a Gaussian noise with standard deviation σ= 0.1. The set of directions used in
FDS-Seq and FDS-Plan are chosen with the method of Griﬃn et al. (2008). The step parameter of the grid
of UCB is set as T−1/(d+2)=T−1/8.
0 100000 200000 300000 400000 500000
Time020000400006000080000100000120000140000Regret
UCB
FDS-Seq
2-points GD
FDS-Plan
Figure 2: Regret plots of various strategies for resource allocation
The regrets of the algorithms strongly depend on the chosen function. In the case of UCB, the position of
the maximizer with respect to the grid that it relies on is important. To alleviate this issue, we plot the mean
regret of all the algorithms, when randomly shifting the loss function by a random vector whose coordinates
are in [0.05,0.05]. We use 1200 Monte Carlo repetitions. The shaded area represents the region between the
ﬁrst and third quartile.
Clearly, FDS-Seq, by reducing the number of samples needed at the beginning of the run (when moves cor-
responds to signiﬁcant drops of the target function), dominates FDS-Plan. The unsatisfying performance of
12Published in Transactions on Machine Learning Research (04/2024)
the gradient descent algorithm can be explained both by the homothetic perturbation that harms the conver-
gence to the optimizer, and by the bad dependence of this algorithm on the dimension. FDS-Seq also clearly
outperforms UCB. Note that the performance of UCB will worsen in higher dimension due to the diﬃculty
of simultaneously controlling the distance between grid points and the overall number of points in the grid.
5 Conclusion
We have studied extensions of direct search algorithms designed for linearly constrained zeroth-order opti-
mization in the stochastic setting. We have shown that these algorithms, though being fairly simple, suﬀer
a regret of the order of T2/3, which is quite satisfactory when compared to other options with comparable
implementation cost, like those inspired by ﬁnitely-armed bandit algorithms or by gradient descent schemes.
There is still a performance gap, in terms of regret rate, when compared to some continuously-armed bandit
approaches that are however computationally much more heavy, even in low-dimensional instances of the
resource allocation model, such as the one considered in Section 4. We do not believe that the analysis of the
algorithms proposed in this paper can be reﬁned so as to obtain the√
Tregret rate. However, an interesting
open question for future work is to know whether this bound could be achieved by other sampling allocation
schemes.
13Published in Transactions on Machine Learning Research (04/2024)
References
Alekh Agarwal, Dean P Foster, Daniel J Hsu, Sham M Kakade, and Alexander Rakhlin. Stochastic convex
optimization with bandit feedback. Advances in Neural Information Processing Systems , 24, 2011.
Shipra Agrawal and Nikhil R. Devanur. Fast algorithms for online stochastic convex programming. In
Proceedings of the Twenty-sixth Annual ACM-SIAM Symposium on Discrete Algorithms , SODA ’15, 2015.
Arya Akhavan, Massimiliano Pontil, and Alexandre Tsybakov. Exploiting higher order smoothness in
derivative-free optimization and continuous bandits. Advances in Neural Information Processing Systems ,
33:9017–9027, 2020.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine Learning , 47(2-3):235–256, 2002.
Francis Bach and Vianney Perchet. Highly-smooth zero-th order online optimization. In Conference on
Learning Theory , pp. 257–283. PMLR, 2016.
Mario Bravo, David Leslie, and Panayotis Mertikopoulos. Bandit learning in concave n-person games.
Advances in Neural Information Processing Systems , 31, 2018.
Sébastien Bubeck, Gilles Stoltz, Csaba Szepesvári, and Rémi Munos. Online optimization in x-armed bandits.
Advances in Neural Information Processing Systems , 21, 2008.
Sébastien Bubeck, Yin Tat Lee, and Ronen Eldan. Kernel-based methods for bandit convex optimization.
InProceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing , pp. 72–85, 2017.
Sébastien Bubeck, Rémi Munos, Gilles Stoltz, and Csaba Szepesvári. X-armed bandits. Journal of Machine
Learning Research , 12(5v), 2011.
Richard Combes and Alexandre Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms.
InInternational Conference on Machine Learning , pp. 521–529. PMLR, 2014.
Kwassi Joseph Dzahini. Expected complexity analysis of stochastic direct-search. Computational Optimiza-
tion and Applications , 81(1):179–200, 2022.
Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the
bandit setting: gradient descent without a gradient. arXiv preprint cs/0408007 , 2004.
Xavier Fontaine, Shie Mannor, and Vianney Perchet. An adaptive stochastic optimization algorithm for
resource allocation. In Algorithmic Learning Theory , pp. 319–363. PMLR, 2020.
Serge Gratton, Clément W Royer, Luís Nunes Vicente, and Zaikun Zhang. Direct search based on proba-
bilistic descent. SIAM Journal on Optimization , 25(3):1515–1541, 2015.
Serge Gratton, Clément W Royer, Luís Nunes Vicente, and Zaikun Zhang. Direct search based on prob-
abilistic feasible descent for bound and linearly constrained problems. Computational Optimization and
Applications , 72(3):525–559, 2019.
Joshua D Griﬃn, Tamara G Kolda, and Robert Michael Lewis. Asynchronous parallel generating set search
for linearly constrained optimization. SIAM Journal on Scientiﬁc Computing , 30(4):1892–1924, 2008.
O Gross. A class of discrete-type minimization problems. Technical report, 1956.
Elad Hazan and Kﬁr Levy. Bandit convex optimization: Towards tight bounds. Advances in Neural Infor-
mation Processing Systems , 27, 2014.
Elad Hazan and Yuanzhi Li. An optimal algorithm for bandit convex optimization. arXiv preprint
arXiv:1603.04350 , 2016.
14Published in Transactions on Machine Learning Research (04/2024)
Naoki Katoh, Toshihide Ibaraki, and Hisashi Mine. A polynomial time algorithm for the resource allocation
problem with a convex objective function. Journal of the Operational Research Society , 30(5):449–455,
1979.
Tamara G Kolda, Robert Michael Lewis, and Virginia Torczon. Optimization by direct search: New per-
spectives on some classical and modern methods. SIAM review , 45(3):385–482, 2003.
Tamara G Kolda, Robert Michael Lewis, and Virginia Torczon. Stationarity results for generating set search
for linearly constrained optimization. SIAM Journal on Optimization , 17(4):943–968, 2007.
Bernard O Koopman. The optimum distribution of eﬀort. Journal of the Operations Research Society of
America , 1(2):52–63, 1953.
Robert Michael Lewis and Virginia Torczon. Pattern search methods for linearly constrained minimization.
SIAM Journal on Optimization , 10(3):917–941, 2000.
Andrea Locatelli and Alexandra Carpentier. Adaptivity to smoothness in x-armed bandits. In Conference
on Learning Theory , pp. 1463–1492. PMLR, 2018.
Aranyak Mehta, Amin Saberi, Umesh Vazirani, and Vijay Vazirani. Adwords and generalized online match-
ing. Journal of the ACM (JACM) , 54(5):22, 2007.
Rémi Munos. From bandits to monte-carlo tree search: The optimistic principle applied to optimization and
planning. 2014.
Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Foundations
of Computational Mathematics , 17(2):527–566, 2017.
Xuedong Shang, Emilie Kaufmann, and Michal Valko. General parallel optimization a without metric. In
Algorithmic Learning Theory , pp. 762–788. PMLR, 2019.
Michal Valko, Alexandra Carpentier, and Rémi Munos. Stochastic simultaneous optimistic optimization. In
International Conference on Machine Learning , pp. 19–27. PMLR, 2013.
Luís Nunes Vicente. Worst case complexity of direct search. EURO Journal on Computational Optimization ,
1(1):143–153, 2013.
Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings
of the 20th international conference on machine learning (icml-03) , pp. 928–936, 2003.
15Published in Transactions on Machine Learning Research (04/2024)
Supplementary Material
Outline. Appendix A, contains general considerations about the potential repercutions of this (method-
ological) work. We prove in Appendix Ball the results pertaining to the noiseless, unconstrained case. In
Appendix C, we provide an additional result for the noisy but unconstrained case. The analysis of direct
search in the latter case paves the way for the proof of Theorem 2whose proof is deferred to Appendix
D. Appendix Econtains further explanations about simulations in Section 2.2and Appendix Fcontains
additional experiments.
A Broader Impact Statement
This paper is mostly a methodological paper that is unlikely to have a direct societal impact.
However, it explores the idea that direct search algorithms, akin to approximate descent algorithms, can
provide explicability in the context of budget allocation for advertising. Advertising practitioners who use
these algorithms can explain their actions to their clients by guaranteeing that with high probability, the
latter result in an increase of the desired performance indicator. This work is thus part of a collective eﬀort
to reach explicability in machine learning, which is crucial as it allows for more transparency.
From an even broader perspective, setting budgets for advertising campaigns is still a manual task in many
companies, which could be replaced by algorithms such as those we propose here. Note that this would still
leave the task of setting the scope of the campaign (which users to target, on which inventories, etc.) to
marketing professionals. It is not clear which impact on employment the automation of budget allocation
could have. However, for now, digital marketing is a ﬂourishing sector where employment seems to have
increased steadily in the last few years.
B Deterministic and unconstrained set-up
B.1 Preliminary Results
Lemma 1.Iffsatisﬁes Assumption 1,
∀k′> k,∥∇f(xk′)∥≤β
a∥∇f(xk)∥
Proof. First observe that because of strong convexity,
(∇f(xk)−∇f(x⋆))⊤(xk−x⋆)≥a∥xk−x⋆∥2,
and
a∥xk−x∗∥2≤∥∇ f(xk)∥∥xk−x∗∥,
which implies that
a∥xk−x∗∥≤∥∇ f(xk)∥. (3)
Hence,
∥∇f(xk′)∥≤β∥xk′−x⋆∥
≤β√a/radicalbig
f(xk′)−f(x⋆)≤β√a/radicalbig
f(xk)−f(x⋆)
≤β√a/radicalBig
∇f(xk)⊤(xk−x∗)≤β√a/radicalbig
∥∇f(xk)∥∥(xk−x∗)∥
≤β
a∇∥f(xk)∥,
16Published in Transactions on Machine Learning Research (04/2024)
where the ﬁrst inequality comes from the smoothness (in fact ∥∇f(xk)∥=∥∇f(xk)∥−∥∇ f(x∗)∥≤β∥xk−x⋆∥),
the second inequality is a result of the strong convexity, the third one ensues from the fact that the algorithm
is a descent algorithm, the fourth one arises as a resut of convexity and the ﬁfth one comes from the strong
convexity property of Equation 3.
Lemma 2.Iffsatisﬁes Assumption 1and iteration kcorresponds to an unsuccessful iteration then
∥∇f(xk)∥≤1
κ/parenleftbiggβ
2αk+ρ(αk)
αk/parenrightbigg
=1
κ/parenleftbiggβ
2+c/parenrightbigg
αk.
This lemma is already well-known (see e.g. Vicente ,2013), we only prove it here for completeness.
Proof. Since cm(Dk) := min u∈Rd,u̸=0max v∈DkuTv
∥u∥∥v∥> κ, there exists v∈Dksuch that
−∇f(xk)⊤v≥κ∥∇f(xk)∥.
Since the iteration is an unsuccessful iteration, we have f(xk)−f(xk+αkv)≤ρ(αk) =cα2
k. Then
καk∥∇f(xk)∥−ρ(αk)≤−∇ f(xk)⊤v+f(xk+αkv)−f(xk)
≤/integraldisplayαk
0∇f(xk+uv)⊤v−∇f(xk)⊤vdu
≤/integraldisplayαk
0∥∇f(xk+uv)−∇f(xk)∥∥v∥du
≤β/integraldisplayαk
0udu≤β
2α2
k,
which yields∥∇f(xk)∥≤1
κ/parenleftBig
β
2αk+ρ(αk)
αk/parenrightBig
=1
κ/parenleftBig
β
2αk+cαk/parenrightBig
.
Lemma 3.
∞/summationdisplay
k=0ρ(αk)≤1
1−θ2(f(x1)−f(x⋆) +ρ(α0)).
This lemma is also a common element of the analysis of direct search algorithms (see e.g. Gratton et al. ,
2019), we only prove it here for completeness.
We assume that there are inﬁnitely many successful iterations as it is trivial to adapt the argument otherwise.
Letkibe the index of the ith successful iteration ( i≥1). Deﬁne k0=−1andα−1=α0for convenience.
Let us rewrite/summationtext∞
k=0ρ(αk)as/summationtext∞
i=0/summationtextki+1
k=ki+1ρ(αk)and study ﬁrst/summationtextki+1
k=ki+1ρ(αk).Thanks to the deﬁnition
of the update on a successful iteration and on unsuccessful iterations,
ki+1/summationdisplay
k=ki+1ρ(αk) =ki+1/summationdisplay
k=ki+1ρ(θiαki) =ki+1/summationdisplay
k=ki+1cθ2iρ(αki)≤1
1−θ2ρ(αki).
Since on successes ρ(αki)≤f(xki)−f(xki+αki) =f(xki)−f(xki+1),
∞/summationdisplay
i=1ρ(αki)≤f(x1)−f(x⋆).
Hence
∞/summationdisplay
k=0ρ(αk)≤1
1−θ2(f(x1)−f(x⋆) +ρ(α0)).
17Published in Transactions on Machine Learning Research (04/2024)
Lemma 4. The index kfof the ﬁrst unsuccessful iteration satisﬁes:
kf≤f(x0)−f(x∗)
ρ(α0).
Another version of this lemma is due to Gratton et al. (2015).
Proof. Before the ﬁrst unsuccessful iteration, αk=α0. So by deﬁnition of a successful iteration,
∀k,such that 0< k≤kf
f(xk−1)−f(xk)≥ρ(α0).
By summing,
f(xk0)−f(xkf)≥kfρ(α0).
The left hand-side of this inequality is upper-bounded by f(x0)−f(x∗), which suﬃces to conclude the
proof.
B.2 Regret Bound
In this section, we prove in Theorem 3below a result involving the regret at iteration Kof the algorithm
instead of the regret after Tfunction evaluations. As T≤K(SD+ 1), Theorem 3directly implies Theorem
1.
Consider
˜RK=K/summationdisplay
k=0/parenleftBigg
f(xk)−f(x⋆) +/summationdisplay
v∈Dkf(xk+αkv)−f(x⋆)/parenrightBigg
which is an upper bound of the cumulative regret suﬀered by the algorithm at iteration K, since it accounts
for all directions in Dkat each round k, while not necessarily all of them will be tested. ˜RKcan be bounded
as follows.
Theorem 3. Under Assumptions 1and2,
˜RK≤(SD+ 1)/bracketleftBig/parenleftbigg1
c/parenleftbigg1
1−θ2(f(x0)−f(x⋆) +ρ(α0)/parenrightbigg/parenrightbigg/parenleftBig/parenleftBig
1 +η
a/parenrightBig
η+β/parenrightBig
+f(x0)−f(x⋆)
ρ(α0)/parenleftbiggβ
a∥∇f(x0)∥α0+βα2
0/parenrightbigg/bracketrightBig
,
where η:=β
a1
κθ(c+β
2).
Proof. We decompose the regret as
˜RK=K/summationdisplay
k=0/parenleftBigg
f(xk)−f(x⋆) +/summationdisplay
v∈Df(xk+αkv)−f(x⋆)/parenrightBigg
≤K/summationdisplay
k=0/parenleftBigg
f(xk)−f(x⋆) +/summationdisplay
v∈Df(xk+αkv)−f(xk) +f(xk)−f(x⋆)/parenrightBigg
≤K/summationdisplay
k=0((D|+1)(f(xk)−f(x⋆))) +K/summationdisplay
k=0/parenleftBigg/summationdisplay
v∈Df(xk+αkv)−f(xk)/parenrightBigg
≤kf/summationdisplay
k=0((D|+1)(f(xk)−f(x⋆))) +kf/summationdisplay
k=0/parenleftBigg/summationdisplay
v∈Df(xk+αkv)−f(xk)/parenrightBigg
+K/summationdisplay
k=kf((D|+1)(f(xk)−f(x⋆))) +K/summationdisplay
k=kf/parenleftBigg/summationdisplay
v∈Df(xk+αkv)−f(xk)/parenrightBigg
, (4)
18Published in Transactions on Machine Learning Research (04/2024)
where kfis the iteration of the ﬁrst unsuccessful iteration. The third inequality provides a decomposition of
the regret in a ﬁrst term that involves the suboptimality of the iterate, and a second term that involves the
diﬀerence between values of fat the iterate and at the trial points. As is usual for direct search algorithm, the
behavior of the algorithm before the ﬁrst unsuccessful iteration has to be studied separately, which explains
the use of the decomposition of the fourth inequality. We bound the regret due to the rounds preceding kf
by:
Lemma 5. The regret due to the rounds preceding kfis bounded by
kf/summationdisplay
k=0((D|+1)(f(xk)−f(x⋆))) +kf/summationdisplay
k=0/parenleftBigg/summationdisplay
v∈Df(xk+αkv)−f(xk)/parenrightBigg
≤C1,
where we denote by kfis the index of the ﬁrst unsuccessful iteration and by
C1=f(x0)−f(x⋆)
cα2
0/parenleftBig
(f(x0)−f(x⋆) +β
a∥∇f(x0)∥α0+βα2
0/parenrightBig
.
Proof. As until kf,f(xk)≤f(x0)andαk=α0, it holds that
kf/summationdisplay
k=0((D|+1)(f(xk)−f(x⋆))) +kf/summationdisplay
k=0/parenleftBigg/summationdisplay
v∈Df(xk+αkv)−f(xk)/parenrightBigg
≤kf/summationdisplay
k=0((D|+1)(f(x0)−f(x⋆))) +kf/summationdisplay
k=0/parenleftBigg/summationdisplay
v∈D∥∇f(xk)∥αk+βα2
k/parenrightBigg
≤kf((D|+1)(f(x0)−f(x⋆))) + ( D|+1)kf/parenleftbiggβ
a∥∇f(x0)∥α0+βα2
0/parenrightbigg
≤f(x0)−f(x⋆)
cα2
0/parenleftbigg
(D|+1)(f(x0)−f(x⋆) +β
a∥∇f(x0)∥α0+βα2
0)/parenrightbigg
=C1,
where Lemma 1is used for the second inequality and the third inequality comes from Lemma 4. The ﬁrst
inequality results from the following property of convex and β-smooth functions: f(y)−f(x)≤∇f(y)T(x−
y)≤∥∇ f(x)T(x−y)∥+β∥x−y∥2, applied to xk+αkvandxk.
Lemma 6. After kf,
K/summationdisplay
k=kff(xk+αkv)−f(xk)≤C2
where kfis the index of the ﬁrst unsuccessful iteration and
C2=1
c(η+β)/parenleftBig
1
1−θ2(f(x0)−f(x⋆) +ρ(α0)/parenrightBig
.
Proof. We take k > k f.Using the property of convex and β-smooth functions that f(y)−f(x)≤∥∇ f(x)T(x−
y)∥+β∥x−y∥2, applied to xk+αkvandxk, as in Lemma 5, we get
f(xk+αkv)−f(xk)≤αk∥∇f(xk)∥+βα2
k.
We note that if kis the index of an unsuccessful iteration,
∥∇f(xk)∥≤1
κ/parenleftbigg
c+β
2/parenrightbigg
αk=1
L′
1αk,
by Lemma 2, ifαk≤1. Ifkis the index of a successful iteration, we can come back to the last unsuccessful
iteration k′, since
∥∇f(xk)∥≤β
a∥∇f(xk′)∥≤β
a1
κ/parenleftbigg
c+β
2/parenrightbigg
αk′≤β
a1
κ/parenleftbigg
c+β
2/parenrightbiggαk
θ.
19Published in Transactions on Machine Learning Research (04/2024)
where the ﬁrst inequality comes from Lemma 1, and the third from the fact that αk≥θαk′. Hence for any
k > k f,
∥∇f(xk)∥≤ηαk,
and
f(xk+αkv)−f(xk)≤(η+β)αk.
Hence
K/summationdisplay
k=0f(xk+αkv)−f(xk)≤K/summationdisplay
k=0(η+β)α2
k.
Consequently,
K/summationdisplay
k=0f(xk+αkv)−f(xk)≤(η+β)1
c/parenleftBigg∞/summationdisplay
k=0ρ(αk)/parenrightBigg
≤1
c(η+β)/parenleftbigg1
1−θ2(f(x0)−f(x⋆) +ρ(α0))/parenrightbigg
.
Lemma 7.
K/summationdisplay
k=kf(f(xk)−f(x⋆))≤1
acη2/parenleftbigg1
1−θ2(f(x1)−f(x⋆) +ρ(α0))/parenrightbigg
:=C3
Proof. Take k > k f. Thanks to the convexity of f,
f(xk)−f(x⋆)≤∇f(xk)⊤(xk−x⋆)
≤1
a∥∇f(xk)∥2,
where the second inequality stems from Equation 3, which itself come from strong convexity.
As in the proof of Lemma 6we have for any k > k f,
∥∇f(xk)∥≤ηαk,
so that for any k > k f,
f(xk)−f(x⋆)≤1
a(η)2/parenleftBigαk
θ/parenrightBig2
.
Thanks to Lemma 3, we have/summationtext∞
k=0α2
k≤/parenleftBig
1
c1
1−θ2(f(x1)−f(x⋆) +ρ(α0)/parenrightBig
.
Eventually,
f(xk)−f(x⋆)≤1
aη2/parenleftbigg1
c1
1−θ2(f(x1)−f(x⋆) +ρ(α0))/parenrightbigg
=C3.
Using the regret decomposition of Equation 4together with Lemmas 4,6, and 7completes the proof of
Theorem 3.
20Published in Transactions on Machine Learning Research (04/2024)
C Noisy and unconstrained set-up
Before considering the constrained setting, we analyze the algorithms described in Section 2(Algorithms 2
and3) when there are no constraints, that is, D=Rd.
C.1 Presentation of the main Result
Theorem 4. Assume that fis lower bounded and upper bounded on Rd, so that there exits U,f(x)−f(x⋆)≤
U,∀x∈Rd. Also assume that the region X={x∈Rd:f(x)< f(x0)}is convex and that fisa-strongly
convex and β-smooth onX. Let RTbe the cumulative regret on the Tﬁrst evaluations of fmade by FDS-
Plan. Set δ=T−4/3. Then
E[RT] =O(log(T)2/3T2/3)
This regret bound is also valid for FDS-Seq under the same Assumptions, with δ=T−10/7/2. In the
following, we give a proof of the regret bound for FDS-Plan. Note that Sections C.3andC.2refer to FDS-
Plan, and Section C.4deals with FDS-Seq.
The regularity assumption in Theorem 4requires that fis bounded and satisﬁes a local version of Assumption
1. The initial point x0should not be chosen too far from x∗, nor should α0be too large. This assumption is
not unreasonable, since for every x0andα0, it is naturally satisﬁed by bounded and strictly convex functions
inC2for some choice of aandβ. We stress that under the alternative assumption 1, the same kind of regret
bound could still be proved, but with a smaller choice of δ, resulting in higher conﬁdence bonuses and the
multiplication of the regret by some constant factor. Indeed, in this case, estimating fincorrectly at each
round can lead to a trajectory that always deviates from x∗, which is highly detrimental to the regret rate;
meanwhile, under the assumption required by Theorem 4,fis bounded by U, so that deviating from x∗
contributes to the regret by at most UT.
In the following, we will use the following additional notation.
Notation. We deﬁne vkto be
vk:=/braceleftBigg
arg maxv∈Dkf(xk)−f(xk−αkv)if iteration kis unsuccessful
the chosen direction otherwise.
C.2 Intermediate results
Lemma 8. We callEkthe event
Ek={|f(xk+αkv)−ˆf(xk+αkv)|≤c/4(αk)2},∀v∈Dk∪{0}}.
The probability of Ekis lower bounded by
P(Ek|Fk−1)≥1−δ(SD+ 1)
whereFk−1is the σ-ﬁeld representing the history.
Proof. Letv∈D∪{0}f(xk+αkv)−ˆf(xk+αkv) =/summationtextNk
i=1ϵjwith ϵjindependent Gaussian variables with
variance σ2and we have
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleNj/summationdisplay
i=1ϵj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/radicalBigg
2σ2log(2 /δ)
Nj≤/radicalBigg
2σ2log(2 /δ)
32σ2log(2 /δ)/ρ(αk)2≤ρ(αk)
4
with probability 1−δ, when knowing Nk. By a union bound, P(Ek|Fk−1)≥1−δ(|D|+1)whereFk−1is the
σ-ﬁeld representing the history.
The following lemma characterizes unsuccessful iterations and successes when Ekoccurs.
21Published in Transactions on Machine Learning Research (04/2024)
Lemma 9. OnEk, ifkis an unsuccessful iteration then f(xk)−f(xk+αkvk)≤3c/2(αk)2and if kis a
successful iteration then f(xk)−f(xk+αkvk)≥c/2(αk)2.
Lemma 9implies that ifEkoccurs for all k, then each iteration of the algorithm results in a descent.
Lemma 10. On∩k≤KEk, the algorithm is a descent algorithm. In particular, xk∈X,∀k∈{1. . . K}.
Lemma 11. Iffsatisﬁes the assumptions of Theorem 4and on∩k≤KEkthen,
∀k′> k,∥∇f(xk′)∥≤β
a∥∇f(xk)∥.
Proof. The proof of Lemma 1applies verbatim thanks to Lemma 10.
Lemma 12. Iffsatisﬁes the assumptions of Theorem 4and the iteration kcorresponds to an unsuccessful
iteration then on Ek,
∥∇f(xk)∥≤1
κ/parenleftbiggβ
2αk+3ρ(αk)
2αk/parenrightbigg
=1
2κ/parenleftbig
βαk+ 3cα2
k/parenrightbig
.
Proof. We reproduce the proof of Lemma 2by using Lemma 9.
Since cm(D) := min v∈Rdmax v∈DvTv
∥v∥∥v∥> κ, there exists v∈Dsuch that
−f(xk)⊤v≥κ∥∇f(xk)∥.
Since the iteration is an unsuccessful iteration, we have f(xk)−f(xk+αkv)≤3
2ρ(αk) =3
2cα2
k, thanks to
Lemma 9. Then
καk∥∇f(xk)∥−ρ(αk)≤β
2α2
k,
exactly as in the proof of Lemma 2, which yields∥∇f(xk)∥≤1
κ/parenleftBig
β
2αk+3
2ρ(αk)
αk/parenrightBig
=1
κ/parenleftBig
β
2αk+3
2cαk/parenrightBig
.
Lemma 13. Iffsatisﬁes the assumptions of Theorem 4and on∩k≤KEk,
K/summationdisplay
k=0ρ(αk)≤2
1−θ2(f(x1)−f(x⋆) +ρ(α0)).
Assume that∩k≤KEkholds. Let kibe the index of the i-th successful iteration ( i≥1). Deﬁne k0=−1and
α−1=α0, and αk= 0,∀k > K for convenience. Deﬁne KIthe number of successes until K. We rewrite/summationtextKI
k=0ρ(αk)as/summationtextKI
i=0/summationtextki+1
k=ki+1ρ(αk)and study ﬁrst/summationtextki+1
k=ki+1ρ(αk).
Thanks to the deﬁnition of the update on a successful iteration and on unsuccessful iterations,
ki+1/summationdisplay
k=ki+1ρ(αk)≤1
1−θ2ρ(αki).
exactly as in the proof of Lemma 3. Since on successes,
1
2ρ(αki)≤f(xki)−f(xki+αki) =f(xki)−f(xki+1),
we have
1
2KI/summationdisplay
i=1ρ(αki)≤f(x1)−f(x⋆).
Hence
KI/summationdisplay
k=0ρ(αk)≤2
1−θ2(f(x1)−f(x⋆) +ρ(α0)).
22Published in Transactions on Machine Learning Research (04/2024)
Lemma 14. On∩k≤KEk, the ﬁrst iteration that results in an unsuccessful iteration occurs at round kf,
satisfying:
kf≤2f(x0)−f(x∗)
ρ(α0).
Before the ﬁrst unsuccessful iteration, αk=α0. So by Lemma 9,∀0< k≤kf
f(xk−1)−f(xk)≥1
2ρ(α0).
By summing,
f(xk0)−f(xkf)≥1
2kfρ(α0).
The left hand-side of this inequality is upper-bounded by f(x0)−f(x∗), which suﬃces to conclude the proof.
Lemma 15. Iffsatisﬁes the assumptions of Theorem 4and on∩k≤KEk, for any kafter the ﬁrst unsuccessful
iteration,
∥∇f(xk)∥≤η2αk,
where we denote by η2=β
2aκθ(3c+β).
Proof. For unsuccessful iterations,
∥∇f(xk)∥≤1
2κ(3c+β)αk,
thanks to Lemma 12. Ifkis the index of a successful iteration, we can come back to the last unsuccessful
iteration k′, since
∥∇f(xk)∥≤β
a∥∇f(xk′)∥≤β
a1
2κ(3c+β)αk′≤β
a1
2κ(3c+β)/parenleftBigαk
θ/parenrightBig
,
where the ﬁrst inequality comes from Lemma 11, the second from Lemma 12and the third from the fact
thatαk≥θαk′.
C.3 Regret Analysis of FDS-Plan
Lemma 16. Iffsatisﬁes the assumptions of Theorem 4and on∩k≤KEk,
˜RK≤C4log(2/δ) +C5log(2 /δ)/parenleftBiggK/summationdisplay
k=1Nk/parenrightBigg2/3
,
where

C4=32
c2C1α−4
0(SD+ 1)
C5=32
c2(SD+ 1)/parenleftBig
c
321
(1−θ2)(f(x1)−f(x⋆) +ρ(α0))/parenrightBig1/3/parenleftbig1
aη2
2+η2+β/parenrightbig
.
Proof. In the following we study the case where ∩k≤KEkholds true.
As in the deterministic case, we decompose the regret as
˜RK=kf/summationdisplay
k=0Nk/parenleftBigg
f(xk)−f(x⋆) +/summationdisplay
v∈Df(xk+αkv)−f(x⋆)/parenrightBigg
+K/summationdisplay
k=kfNk/parenleftBigg
f(xk)−f(x⋆) +/summationdisplay
v∈Df(xk+αkv)−f(x⋆)/parenrightBigg
.
23Published in Transactions on Machine Learning Research (04/2024)
We start by dealing with the cumulative regret before kf. We write
kf/summationdisplay
k=0Nk/parenleftBigg
f(xk)−f(x⋆) +/summationdisplay
v∈Df(xk+αkv)−f(x⋆)/parenrightBigg
≤N0kf/summationdisplay
k=0/parenleftBigg
f(xk)−f(x⋆) +/summationdisplay
v∈Df(xk+αkv)−f(x⋆)/parenrightBigg
≤32
c2α−4
0log(2 /δ)kf/summationdisplay
k=0/parenleftBigg
f(xk)−f(x⋆) +/summationdisplay
v∈Df(xk+αkv)−f(x⋆)/parenrightBigg
≤32
c2α−4
0log(2 /δ)×2C1
=C4log(2 /δ),
where the last inequality is obtained exactly as in the proof of Lemma 5with the help of Lemma 14instead
of Lemma 4. By using the above inequality and the decomposition of the regret, we get
˜RK−C4log(2 /δ)
≤K/summationdisplay
k=kfNk/parenleftBigg
f(xk)−f(x⋆) +/summationdisplay
v∈Df(xk+αkv)−f(x⋆)/parenrightBigg
≤K/summationdisplay
k=kfNk/parenleftBigg
f(xk)−f(x⋆) +/summationdisplay
v∈Df(Xk+αkv)−f(xk) +f(xk)−f(x⋆)/parenrightBigg
≤K/summationdisplay
k=kfNk/parenleftBigg
(SD+ 1)( f(xk)−f(x⋆)) +/summationdisplay
v∈Df(xk+αkv)−f(xk)/parenrightBigg
≤(SD+ 1)K/summationdisplay
k=kfNk/parenleftbigg1
a∥∇f(xk)∥2+∥∇f(xk)∥αk+βα2
k/parenrightbigg
,
where C4=32
c2C1α−4
0(SD+ 1). The fourth inequality comes from the regularity assumptions required for
Theorem 2together with Lemma 10. We use Lemma 15to get that for any k > k f,
∥∇f(xk)∥≤η2αk.
Then1
a∥∇f(xk)∥2+∥∇f(xk)∥αk+βα2
k≤1
aη2
2α2
k+η2α2
k+βα2
k.
We get
K/summationdisplay
k=kfNk/parenleftbigg1
a∥∇f(xk)∥2+∥∇f(xk)∥αk/parenrightbigg
≤C6log(2 /δ)K/summationdisplay
k=kf(αk)−2
≤C6log(2 /δ)K/summationdisplay
k=0(αk)−2.
where C6=/parenleftbig1
aη2
2+η2+β/parenrightbig32
c2. Consequently
˜RK≤C4log(2 /δ) +C6log(2 /δ)K/summationdisplay
k=0(αk)−2.
The number of function evaluations is deﬁned as
K/summationdisplay
k=0Nk=32 log(2 /δ)
cK/summationdisplay
k=0α−4
k.
24Published in Transactions on Machine Learning Research (04/2024)
Thanks to Lemma 13,
K/summationdisplay
k=0(αk)2≤2
c(1−θ2)(f(x1)−f(x⋆) +ρ(α0)).
By Hölder’s inequality, we get
K/summationdisplay
k=0(αk)−2=K/summationdisplay
k=0(αk)2/3(αk)−2/3−2
≤/parenleftBiggK/summationdisplay
k=0(αk)2/3×3/parenrightBigg1/3/parenleftBiggK/summationdisplay
k=0(αk)−8/3×3/2/parenrightBigg2/3
≤/parenleftBiggK/summationdisplay
k=0(αk)2/parenrightBigg1/3/parenleftBiggK/summationdisplay
k=0(αk)−4/parenrightBigg2/3
.
And thus
˜RK−C4log(2 /δ)
≤C6log(2 /δ)/parenleftbigg2
c(1−θ2)(f(x1)−f(x⋆) +ρ(α0)/parenrightbigg1/3/parenleftBigg
c
8 log(2 /δ)K/summationdisplay
k=1Nk/parenrightBigg2/3
≤C5log(2 /δ)/parenleftBigg
1
log(2 /δ)K/summationdisplay
k=1Nk/parenrightBigg2/3
≤C5log(2 /δ)1/3/parenleftBiggK/summationdisplay
k=1Nk/parenrightBigg2/3
,
where C5= (SD+ 1)C6/parenleftBig
c
321
(1−θ2)(f(x1)−f(x⋆) +ρ(α0))/parenrightBig1/3
.
Proof. of Theorem 4We note KTthe last round reached by the algorithm with Tevaluations. Lemma 16
proves that on∩k≤KTEk,
˜RKT≤C4log(2 /δ) +C5/parenleftbigg1
(SD+ 1)/parenrightbigg2/3
log(2 /δ)2/3(T)2/3.
Thanks to Lemma 8,
P/parenleftBig
∪KT
k=1EC
k/parenrightBig
≤T/summationdisplay
k=1P/parenleftbig
EC
k/parenrightbig
≤(SD+ 1)T/summationdisplay
t=1T−4/3/2≤(SD+ 1)T−1/3, (5)
when taking δ=T−4/3, since KT≤T. Hence,
E[RT]≤4
3C4log(2 T) +4
3/parenleftbigg1
(SD+ 1)/parenrightbigg2/3
C5log(2 T)2/3T2/3+ (SD+ 1)UT2/3
=O((log T)2/3T2/3).
25Published in Transactions on Machine Learning Research (04/2024)
C.4 Regret Analysis of FDS-Seq
Instead of considering Ekas in the previous section, we need to consider E′
k={(f(xk)−f(xk+αkvk)≤
3c/2(αk)2andkis an unsuccessful iteration )or(kis a successful iteration and f(xk)−f(xk+αkvk)≥
c/2(αk)2)}. Instead of Lemma 8we prove the following result.
Lemma 17. The probability of E′
kis lower bounded by
P(E′
k|Fk−1)≥1−δ×N2
k×(|D|+1),
whereFk−1is the σ-ﬁeld representing the history.
Proof. Fixv∈D. First assume that f(xk)> f(xk+αkv) + 3/2ρ(αk). In particular, f(xk)> f(xk+αkv) +
ρ(αk). We denote nτ
0,kandnτ
v,kthe values of n0,kandnv,kat the end of the while loop of FDS-Seq. Observe
that
P/parenleftbig
E′
k|Fk−1, nτ
0,k=nτ
v,k=Nk/parenrightbig
≥1−δ×(|D|+1).
Hence we only need to focus on the case when the ﬁrst part of Condition 1is ﬁrst satisﬁed. In this case,
knowing nτ
0,k, nτ
v,k, the probability that ˆfnτ
0,k(xk)<ˆfnτ
v,k(xk+αkv) +ρ(αk)when the ﬁrst row of Condition
1is ﬁrst satisﬁed is bounded as follows. We have
P/parenleftBigg
f(xk)−ˆfnτ
0,k(xk)−(f(xk+αkv)−ˆfnτ
v,k(xk+αkv))≥
/radicalbig
2σ2log(1 /δ)/radicalBigg
1
nτ
0,k+1
nτ
v,k/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleFk, nτ
0,k, nτ
v,k/parenrightBigg
≤δ.
Since f(xk)> f(xk+αkv) +ρ(αk),
f(xk)−ˆfnτ
0,k(xk)−(f(xk+αkv)−ˆfnτ
v,k(xk+αkv))
≥−ρ(αk)−ˆfnτ
0,k(xk) + ˆfnτ
v,k(xk+αkv).
So that the above deviation bound results in:
P/parenleftBigg
ˆfnτ
0,k(xk)−ˆfnτ
v,k(xk+αkv)−ρ(αk)≤
−/radicalbig
2σ2log(1 /δ)/radicalBigg
1
nτ
0,k+1
nτ
v,k/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleFk, nτ
0,k, nτ
v,k/parenrightBigg
Finally, we apply a union bound. Since nτ
0,kandnτ
v,kboth belong to [0, Nk]and cannot be simultaneously
equal to Nk:
P/parenleftBigg
ˆfnτ
0,k(xk)−ˆfnτ
v,k(xk+αkv)−ρ(αk)≤
−/radicalbig
2σ2log(1 /δ)/radicalBigg
1
nτ
0,k+1
nτ
v,k/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleFk,not(nτ
0,k=nτ
v,k=Nk)/parenrightBigg
≤(N2
k−1)δ .
This amounts to a bound of the probability of kbeing an unsuccessful iteration and thus of E′C
k, when
the ﬁrst part of condition 1is satisﬁed. Summing with the probability of E′
kCin the other case, we obtain
P(E′(k))≥N2
kδ.
26Published in Transactions on Machine Learning Research (04/2024)
The case f(xk)> f(xk+αkv) + 3/2ρ(αk)can be treated in the exact same way.
To adapt the proof of Theorem 4to FDS-Seq (with a diﬀerent choice of δ=T−10/3), it suﬃces to replace
Equation 5by
P/parenleftBig
∪KT
k=1E′C
k/parenrightBig
≤(SD+ 1)T/summationdisplay
k=1P/parenleftBig
E′C
k/parenrightBig
≤(SD+ 1)KT/summationdisplay
k=1N2
kT−10/3
≤(SD+ 1)T/summationdisplay
k=1T2T−10/3/2≤(SD+ 1)T−1/3.
The regret is hence
E[RT]≤10
3C4log(2 T) +10
3/parenleftbigg1
(SD+ 1)/parenrightbigg2/3
C5log(2 T)2/3T2/3+ (SD+ 1)UT2/3
=O((log T)2/3T2/3).
D Noisy and Constrained Set-Up
In this section we analyze the behavior of the algorithms in the presence of linear constraints. The complexity
of feasible direct search with linear constraints in the noiseless case has been studied by Gratton et al.
(2019). In this paper, instead of studying the speed at which the gradient converges to 0as is usual
in the unconstrained case, the authors study the convergence of a lower bound of the gradient χ(x) :=
max x+v∈D,∥v∥≤1−∇f(x)Tvto0. Indeed, the convergence of the gradient to 0might be unachievable when
the optimum lies on the boundaries, but χ(x)is equal to 0if and only if xis optimal. The paper proves that
the ﬁrst iteration Kϵat which χ(xk)is smaller than ϵis of the order of ϵ−2, like in the unconstrained case.
In the following, we denote by Uthe global upper bound of f(x)−f(x⋆)on the domain.
D.1 Intermediate Results
We recall thatEkdenotes the event
Ek={|f(xk+αkv)−ˆf(xk+αkv)|≤c/4(αk)2},∀v∈D∪{0}}.
Lemmas 8,9,10are left unchanged by the transition to constrained domains. A version of Lemma 3.4. of
(Gratton et al. ,2019) reads :
Lemma 18. On∩k≤KEk, and if fsatisﬁes Assumption 1, then the following holds: if the k-th iteration is
unsuccessful, then
χ(xk)≤/parenleftbiggβ
2κ+Bg
ηmin/parenrightbigg
αk+3ρ(αk)
2καk:=L2αk,
where ηmin:=λ(N)where Nis the set of all possible approximate normal cones N(x, α),∀∈D , α∈Rd.
Proof. It is straightforward to prove
∥PT(xk,αk)(−∇f(xk))∥≤1
κ/parenleftbiggβ
2αk+3ρ(αk)
2αk/parenrightbigg
=1
2κ/parenleftbig
βαk+ 3cα2
k/parenrightbig
,
with the same elements as in the proof Lemma 11, by noticing that DkcontainsGk, that generates T(xk, αk).
To prove the bound on χ(xk), we use the Moreau decomposition,stating that any vector v∈Rdcan be
27Published in Transactions on Machine Learning Research (04/2024)
decomposed as v=PTk[v] +PNk[v]with Nk=N(xk, αk)andPTk[v]> P Nk[v] = 0 , and write
χ(xk) = max
x+v∈D,∥v∥≤1(PTk[−∇f(xk)] + ( PTk[v]vT+PNk[v])TPNk[−∇f(xk)])
≤ max
x+v∈D,∥v∥≤1(vTPTk[−∇f(xk)] +PNk[v]TPNk[−∇f(xk)])
≤ max
x+v∈D,∥v∥≤1∥PTk[−∇f(xk)]∥+∥PNk[v]∥∥PNk[−∇f(xk)]∥. (6)
The ﬁrst term of the right hand side of Equation 6is bounded in the following way
∥PT(xk,αk)(−∇f(xk))∥≤1
2κ/parenleftbig
βαk+ 3cα2
k/parenrightbig
consequently.
Lemma 19 (Proposition B.1 of ( Lewis & Torczon ,2000)).Letx∈D andα > 0. Then, for any vector v
such that x+v∈D, one has
∥PN(x,α)[v]∥≤α
ηmin.
This in turn provides a bound of the second term of the right hand side of Equation 6:
∥PNk[v]∥∥PNk[−∇f(xk)]∥≤α
ηminBg,
which suﬃces to conclude the proof.
Lemma 20. Assume that∩k≤KEkholds, and fsatisﬁes Assumption 1. Set ϵ >0. Let hdenote the mapping
from ϵto
h(ϵ) =/parenleftbigg2L2
2U
cθ1/parenrightbigg
ϵ−2+log(α0L2
θ)
log(1 /θ)+2U
α2
0=E1ϵ−2+E2,
where E1=/parenleftBig
2L2
2U
cθ1/parenrightBig
andE2=log(α0L2
θ)
log(1 /θ)+2U
α2
0. Denote by k(ϵ)the ﬁrst iteration of the algorithm where
χ(xk)≤ϵ. Ifh(ϵ)≤K, then
k(ϵ)≤h(ϵ).
The proof is a mere adaptation of the proof of Theorem 1 of ( Gratton et al. ,2015), with diﬀerent constants
(we use Lemma 18and13).
Lemma 21. Iffsatisﬁes Assumption 1,
a∥xk−x⋆∥≤χ(xk).
Proof.
a∥xk−x⋆∥≤f(x)−f(x⋆)
∥xk−x⋆∥≤−∇f(x)(x⋆−xk)
∥xk−x⋆∥≤χ(xk),
by deﬁnition of χ(xk).
Lemma 22. Iffsatisﬁes Assumption 1and if x⋆is in the interior of Dand the algorithm achieves descent
at each iteration, then
∀k′> k,∥χ(xk′)∥≤/parenleftbiggβ
a/parenrightbigg3/2
∥χ(xk)∥.
Proof. Like in the proof of Lemma 1, we get
a∥xk−x∗∥≤∥∇ f(xk)∥.
28Published in Transactions on Machine Learning Research (04/2024)
Hence,
β∥xk′−x⋆∥≤β√a/radicalbig
f(xk′)−f(x⋆)≤β√a/radicalbig
f(xk)−f(x⋆)
≤β√a/radicalbig
β(xk−x⋆)2≤β3/2
√a/radicalbigg
χ(xk)2
a
≤/parenleftbiggβ
a/parenrightbigg3/2
χ(xk),
where the ﬁrst inequality comes from the strong convexity, and the second one comes from the fact that the
algorithm is a descent, the third one comes from Lemma 21.
Now let v= arg maxx+v∈D,∥v∥≤1−vT∇f(xk′).
−v⊤∇f(xk′) =−v⊤∇f(xk′) +v⊤∇f(x∗)≤∥∇ f(xk′)−∇f(x∗)∥≤β∥xk−x⋆∥,
because∇f(x∗) = 0 .This concludes the proof.
D.2 Regret Analysis when the Optimum is in the interior of D
Let us assume that x⋆is in the interior of D. Let us denote by ∆the distance from x⋆to the closest
boundary, and by r= ∆/4.
Lemma 23. Ifχ(x)≤arthen∥x−x⋆∥≤r.
If∥x−x⋆∥≥rthen r≤1
aχ(x)thanks to Lemma 21.
Lemma 24. For any k≥k/parenleftBig
(a/β)3/2ar/parenrightBig
,∥xk−x⋆∥≤r.
Thanks to Lemma 1, after k/parenleftBig
(a/β)3/2ar/parenrightBig
iterations, χ(xk)≤ar. And thanks to the previous lemma, we
thus have∥xk−x⋆∥≤r.
Lemma 25. Setksthe index of the ﬁrst successful iteration following
k/parenleftBig
(a/β)3/2ar/parenrightBig
where αk≤∆/2. After iteration ks,T(xk, αk)spans all directions in Rd, so that the
instantaneous regret is the same as that of the algorithm in the unconstrained case with initial point xks
and initial step-size αks. The iteration of this ﬁrst successful iteration comes before ki:=k/parenleftBig
(a/β)3/2ar/parenrightBig
+
log(α0/∆)
log 1/θ.
Proof. Ifkis a successful iteration αk≤2r= ∆/(2), since∥xk−x⋆∥≤rand∥xk+1−x⋆∥≤r. And if kis an
unsuccessful iteration, it comes after one of those successes and a sequence of unsuccessful iterations, which
yields αk≤∆/(2).
Lemma 26. On∩k≤KEk,
˜RK≤C7log(2 /δ)(1/θ)−4Cf+C5log(2 /δ)1/3/parenleftBiggK/summationdisplay
k=1Nk/parenrightBigg2/3
,
where C7= (SD+ 1)U32
c2α4
01
(1/θ)−1
and
Cf=E1/parenleftBigg/parenleftbigga
β/parenrightbigg3/2
ar/parenrightBigg−2
+E2+log(α0/∆)
log 1/θ+β∆2
α0.
29Published in Transactions on Machine Learning Research (04/2024)
Proof. In the proof of Lemma 16, we isolated the steps preceding the ﬁrst unsuccessful iteration. Similarly
here, we treat the iterations before the ﬁrst unsuccessful iteration after ki, denoted by k′
f, separately from
other iterations.
˜RK≤k′
f/summationdisplay
k=0Nk(f(xk)−f(x⋆)) +k′
f/summationdisplay
k=0/parenleftBigg
Nk/summationdisplay
v∈Dkf(xk+αkv)−f(x⋆)/parenrightBigg
+K/summationdisplay
k=k′
f(Nk(|Dk|+1)(f(xk)−f(x⋆))) +K/summationdisplay
k=k′
fNk/parenleftBigg/summationdisplay
v∈Dkf(xk+αkv)−f(xk)/parenrightBigg
.
Because f(x)−f(x⋆)is bounded by U,
k′
f/summationdisplay
k=0Nk(f(xk)−f(x⋆)) +k′
f/summationdisplay
k=0/parenleftBigg
Nk/summationdisplay
v∈Dkf(xk+αkv)−f(x⋆)/parenrightBigg
≤k′
f/summationdisplay
k=0(|Dk|+1)NkU.
By rewriting Nk,
k′
f/summationdisplay
k=0(|Dk|+1)NkU≤k′
f/summationdisplay
k=0(|Dk|+1)U32 log(1 /δ)
c2α4
k
≤k′
f/summationdisplay
k=0(|Dk|+1)U32 log(1 /δ)
c2α4
0θ4k
≤(SD+ 1)U32 log(1 /δ)
c2α4
0(1/θ)−4k′
f
(1/θ)−1.
Also on∩k≤KEk,
k′
f≤ki+f(xki)−f(x⋆)
α0≤ki+β∆2
α0≤k/parenleftBigg/parenleftbigga
β/parenrightbigg3/2
ar/parenrightBigg
+log(α0/∆)
log 1/θ+β∆2
αk,
where the ﬁrst inequality comes from the same argument used to prove Lemma 4, the second inequality
comes from the smoothness of fand the third one comes from the deﬁnition of ki.
On∩k≤KEk,
k/parenleftBigg/parenleftbigga
β/parenrightbigg3/2
ar/parenrightBigg
≤E1/parenleftBigg/parenleftbigga
β/parenrightbigg3/2
ar/parenrightBigg−2
+E2,
with E1,E2deﬁned in ( Gratton et al. ,2019). Finally, we focus on the part of the regret accumulated before
k′
f. On∩k≤KEk
K/summationdisplay
k=k′
f(Nk(|Dk|+1)(f(xk)−f(x⋆))) +K/summationdisplay
k=k′
fNk/parenleftBigg/summationdisplay
v∈Dkf(xk+αkv)−f(xk)/parenrightBigg
≤C5log(2 /δ)1/3/parenleftBiggK/summationdisplay
k=1Nk/parenrightBigg2/3
,
by following exactly the same steps as those needed to bound the regret in the unconstrained case.
30Published in Transactions on Machine Learning Research (04/2024)
Theorem 2.Under Assumptions 1,3and4, and if x⋆∈int(D), the cumulative regret RTof FDS-Plan
(respectively FDS-Seq) after the ﬁrst Tevaluations of f, satisﬁes
E[RT] =O( log( T)2/3T2/3)
for the choice δ=T−4/3(respectively δ=T−10/3for FDS-Seq).
Proof. for FDS-Plan. We denote by KTthe last round reached by the algorithm with Tevaluations..
Lemma 26proves that on the event ∩k≤KTEk,
˜RK≤C7log(2 /δ)(1/θ)−4Cf+C5log(2 /δ)1/3/parenleftBiggK/summationdisplay
k=1Nk/parenrightBigg2/3
.
Thanks to Lemma 8,
P/parenleftBig
∪KT
k=1EC
k/parenrightBig
≤(SD+ 1)P/parenleftbig
∪T
k=1EC
k/parenrightbig
≤(|Dk|+1)T/summationdisplay
t=1T−4/3/2≤(SD+ 1)T−1/3
since KT≤T. Hence,
E[RT]≤4
3C7(1/θ)−4Cflog(2 T) +4
3/parenleftbigg1
(|Dk|+1)/parenrightbigg2/3
C5log(2 T)2/3T2/3
+ ((SD+ 1)) UT2/3
=O((log T)2/3T2/3).
Adaptation of the proof for FDS-Seq The way of adapting the proof of FDS-Plan to the case of
FDS-Seq of Section C.4applies verbatim.
E Details on the implementation of HOO in Section 2.2
To implement HOO in the simulations of Section 2.2, the tree of partitions that we used is built in the
following way. We set the parameter ρof HOO as suggested by Bubeck et al. (2011) to 2−2/d. A binary tree
of depth H=log(1 /T)
2 log( ρ)of partitions of [0,1]dis obtained by recursively halving the cells at each depth hof
the tree along dimension h(mod 2) . At depth h, this approach yields a partition formed by rectangular cells
represented by their lower left corner [ai, bi]. Then, in order to remove unwanted cells, we traverse the tree,
starting from the leaves, and remove every cell having an empty intersection with the domain. Due to the
geometry of the simplex, knowing if a cell intersects the domain boils down to checking if its representation
[ai, bi]belongs to it. When the algorithm selects cell (h, i)at time t, the representation of that cell is chosen
as a sampling point. In the simulation, the smoothness parameter ν1of HOO is set to 16.
F Additional Experiments
Here, as in Section 2.2, we focus on the case in which there are three resources ( d= 2). The loss functions
for resources 1and 3are of the same form as in Section 2.2andwi(x) =−τilog(1+ γx)
log(1+ γ)with γ= 2,τ1= 1,
τ3= 0.3, but now the second resource is associated to w2(x) = 0 .1x. This choice of reward functions results
in an optimal choice whose second component is zero. We set the horizon to T= 100 ,000and use a Gaussian
noise with standard deviation σ= 0.1.
We show the trajectories of FDS-Plan and FDS-Seq in Figure 3. Notice that the trajectories do not change
drastically compared to those of Section 2.2, which seems to indicate that the location of the optimal
31Published in Transactions on Machine Learning Research (04/2024)
allocation on the border of the feasible set is not a problem in practice. We complement these plots with
regret plots (Figure 4) of all the algorithms detailed in Section 2.2run ﬁrst on the environment described in
this same section and second on the environment described above with the optimum on the border of the
simplex. Once again, this seems to show that the optimum lying in the border is not an issue in practice.
Incidentally, this last plot also shows that, as expected, UCB should be the preferred algorithm in dimension
d= 2, as it is simpler and gives excellent results.
 0:9542 0:8656
 0:7771 0:6885
100102104
(a) FDS-Plan
 0:9542 0:8656
 0:7771 0:6885
100102104
 (b) FDS-Seq
Figure 3: Single trajectories with the optimum on the border
0 20000 40000 60000 80000 100000
Time02500500075001000012500150001750020000Regret
FDS-Plan
FDS-Seq
HOO
UCB
GD
2-points GD
(a) Optimum in the interior
0 20000 40000 60000 80000 100000
Time0500010000150002000025000Regret
FDS-Plan
FDS-Seq
HOO
UCB
GD
2-points GD (b) Optimum on the border
Figure 4: Regret plots in dimension d= 2
32