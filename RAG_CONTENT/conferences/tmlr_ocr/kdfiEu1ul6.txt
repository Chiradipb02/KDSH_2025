Published in Transactions on Machine Learning Research (07/2023)
Learning from Time-dependent Streaming Data
with Online Stochastic Algorithms
Antoine Godichon-Baggioni antoine.godichon_baggioni@sorbonne-universite.fr
Laboratoire de Probabilités, Statistique et Modélisation
Sorbonne Université
Nicklas Werge nicklas.werge@sorbonne-universite.fr
Laboratoire de Probabilités, Statistique et Modélisation
Sorbonne Université
Olivier Wintenberger olivier.wintenberger@sorbonne-universite.fr
Laboratoire de Probabilités, Statistique et Modélisation
Sorbonne Université
Reviewed on OpenReview: https: // openreview. net/ forum? id= kdfiEu1ul6
Abstract
This paper addresses stochastic optimization in a streaming setting with time-dependent
and biased gradient estimates. We analyze several first-order methods, including Stochastic
Gradient Descent (SGD), mini-batch SGD, and time-varying mini-batch SGD, along with
their Polyak-Ruppert averages. Our non-asymptotic analysis establishes novel heuristics that
link dependence, biases, and convexity levels, enabling accelerated convergence. Specifically,
our findings demonstrate that (i) time-varying mini-batch SGD methods have the capability
to break long- and short-range dependence structures, (ii) biased SGD methods can achieve
comparable performance to their unbiased counterparts, and (iii) incorporating Polyak-
Ruppert averaging can accelerate the convergence of the stochastic optimization algorithms.
To validate our theoretical findings, we conduct a series of experiments using both simulated
and real-life time-dependent data.
1 Introduction
Machine learning has experienced remarkable growth and adoption across diverse domains, revolutionizing
various applications and unleashing the potential of data-driven decision-making (Bishop & Nasrabadi, 2006;
Goodfellow et al., 2016; Sutton & Barto, 2018; Hastie et al., 2009; Hazan et al., 2016; Shalev-Shwartz et al.,
2012). With this proliferation of machine learning, a significant volume of new data has emerged, including
streaming data that flows continuously and poses unique challenges for learning algorithms. Unlike traditional
static datasets, streaming data requires algorithms to adapt in real-time, continuously updating their models
to accurately predict new samples.
At the heart of machine learning lies optimization, the process of finding optimal model parameters that
minimize the objective function and extract meaningful insights from the data (Abu-Mostafa et al., 2012).
Stochastic Optimization (SO) methods have emerged as powerful tools for addressing optimization tasks in
machine learning (Kushner & Yin, 2003; Nemirovskij & Yudin, 1983; Shalev-Shwartz & Ben-David, 2014).
These SO methods overcome the scalability limitations of traditional batch learning techniques and allow for
efficient processing of streaming data (Bottou et al., 2018).
Among the various SO methods, Stochastic Gradient Descent (SGD) and its variants stand out as the
workhorses of the field (Robbins & Monro, 1951), extensively utilized in machine learning tasks (Hardt et al.,
2016; Shalev-Shwartz et al., 2011; Zhang, 2004; Xiao, 2009). SGD and its variants provide a practical and
1Published in Transactions on Machine Learning Research (07/2023)
efficient approach to optimize complex models by updating the model parameters using stochastic estimates
of the gradients. These methods have proven their effectiveness in a wide range of applications, including
deep learning, natural language processing, and computer vision (Goodfellow et al., 2016; Sutton & Barto,
2018; Hastie et al., 2009).
However, traditional analyses of SO problems often assume that gradient estimates are unbiased and drawn
independently and identically distributed (i.i.d.) from an unknown data generation process (Cesa-Bianchi
et al., 2004). Unfortunately, real-world data rarely adheres to this idealized assumption. Streaming data, in
particular, introduces additional complexity, as dependencies and biases can arise due to time-dependency
or other factors inherent in the data generation process. These dependencies and biases pose significant
challenges for conventional SO methods.
Notably, SGD-based methods have demonstrated the ability to converge even under biased gradient estimates
(Ajalloeian & Stich, 2020; Bertsekas, 2016; d’Aspremont, 2008; Devolder et al., 2011; Gorbunov et al., 2020a;b;
Schmidt et al., 2011). However, the theoretical understanding of the convergence properties of SGD in the
presence of biased gradients is not yet well-established. While empirical studies have shown promising results
in specific applications (Agarwal & Duchi, 2012; Chen & Luss, 2018; Karimi et al., 2019; Ma et al., 2022;
Schmidt et al., 2011), more rigorous investigations are necessary to generalize these findings and comprehend
the underlying mechanisms.
Contributions. In this paper, we explore SGD-based methods in the context of streaming data. We extend
the analysis of the unbiased i.i.d. case by Godichon-Baggioni et al. (2023) to include time-dependency and
biasedness. By leveraging their insights, we investigate the effectiveness of first-order SO methods in a
streaming setting, where the assumption of unbiased i.i.d. samples no longer holds. Our non-asymptotic
analysis establishes novel heuristics that bridge the gap between dependence, biases, and the convexity levels
of the SO problem. These heuristics enable accelerated convergence in complex problems, offering promising
opportunities for efficient optimization in streaming settings. Our contributions can be summarized as follows:
•Non-asymptotic Convergence Rates of Time-varying Mini-batch SGD-based methods.
We present non-asymptotic convergence rates specifically tailored for time-varying mini-batch SGD-
based methods under time-dependency and biasedness. These convergence rates offer valuable insights
into achieving and enhancing convergence in applications involving time-dependent and biased inputs,
providing a comprehensive understanding of the optimization process in streaming settings.
•Breaking Long- and Short-term Dependence. Our study demonstrates the effectiveness of
SGD-based methods in overcoming long- and short-range dependence by leveraging time-varying
mini-batches. These mini-batches are carefully designed to counteract the inherent dependency
structures present in streaming data, leading to improved performance and convergence. This
contribution expands the applicability of SGD-based methods to scenarios where dependence poses a
challenge.
•Robustness to Biased Gradients. We show that biased SGD-based methods can converge and
achieve accuracy comparable to unbiased ones, as long as the bias is not excessively large. This
finding highlights the robustness of SGD-based methods in the presence of bias, broadening their
applicability to scenarios where biased gradient estimates are encountered.
•Accelerated Convergence with Polyak-Ruppert Averaging. Our study reveals that incorporat-
ing Polyak-Ruppert averaging into SGD-based methods accelerates convergence (Polyak & Juditsky,
1992; Ruppert, 1988). Importantly, our findings emphasize the continued efficacy of this technique in
challenging streaming settings characterized by dependence structures and biases. Furthermore, by
combining Polyak-Ruppert averaging with the variance reduction capabilities offered by time-varying
mini-batches, we obtain the best of both worlds. This powerful combination not only enhances the
convergence rate but also increases robustness by reducing the variance.
Overall, our contributions extend the scope of SO by considering time-dependent and biased gradient estimates
in a streaming framework. We provide non-asymptotic convergence rates, demonstrate the effectiveness of
SGD-based methods in handling dependence and bias, and highlight the accelerated convergence achieved
2Published in Transactions on Machine Learning Research (07/2023)
through Polyak-Ruppert averaging. These findings enhance our understanding of SO in streaming settings and
offer practical strategies for researchers and practitioners to optimize their models in real-time applications.
Organization. In section 2, we introduce the streaming framework and lay the groundwork for our non-
asymptotic analysis. We provide an overview of key concepts, definitions, and assumptions, with a focus
on the dependency structures. In section 2.1, we discuss the assumptions related to the objective function,
while section 2.2 focuses on the assumptions concerning the gradient estimates, particularly the crucial role
ofα-mixing conditions in validating the dependency structures. We then present our streaming variants of
the SGD methods in section 2.3.
In section 3, we present our convergence results, considering both our streaming SGD variant with and
without Polyak-Ruppert averaging. We provide a detailed discussion of each result, highlighting connections
to previous work in the field. The proofs of our results are provided in appendix A.
Finally, in section 4, we present experimental results that validate our findings. We conduct experiments on
synthetic and real-life time-dependent streaming data, showcasing the practical implications of our work.
2 Problem Formulation, Assumptions, and Methods
We consider SO problems of the form
min
θ∈Rd{F(θ) =Eξ[f(θ;ξ)]}. (1)
In the formulation (1), θ∈Rdrepresents the parameter vector of interest. The objective function Fis defined
as the expected value of the random loss function f(θ;ξ), which depends on the parameter vector θand
the random variable ξ. To minimize F, we rely on estimates of its gradients. Specifically, we estimate Fby
evaluating the gradient estimates of fon a sequence of samples (ξt).
In our streaming setting, we have access to a sequence of time-varying mini-batches (ξt), rather than knowing
the true underlying distribution of ξ. Eachξtconsists ofnt∈Nindividual data points, represented by the
mini-batch{ξt,1,...,ξt,nt}. We extend this notion of time-varying mini-batches by defining ft(θ) =f(θ;ξt).
Consequently, (ft(θ))constitutes a sequence of differentiable (possible non-convex) random loss functions
(Nesterov et al., 2018; Nemirovskij & Yudin, 1983). Hence, each ft(θ)comprisesnt∈Nindividual losses,
represented by the mini-batch {ft,1,...,ft,nt}. For example, consider the scenario where ξtrepresents a
mini-batch of input-output pairs {(xt,i,yt,i)}nt
i=1. In this case, for a model class {hθ}θ∈Θ, we can express
ft,i(θ)as a combination of a loss function land a regularizer Ω:
ft,i(θ) =l(yt,i,hθ(xt,i)) + Ω(θ).
2.1 Assumptions on Objective Functions: Quasi-strong Convexity and Lipschitz Smoothness
In accordance with previous work by Bach & Moulines (2011); Gower et al. (2019); Nguyen et al. (2019), we
adopt certain assumptions regarding the objective function F. Firstly, we assume that Fpossesses a unique
global minimizer θ∗∈Θ, where Θis a closed convex set in Rd. These assumptions align with techniques
utilized in (online) convex optimization (Boyd & Vandenberghe, 2004; Nesterov et al., 2018; Hazan et al.,
2016; Shalev-Shwartz et al., 2012). In addition, we assume that the objective function Fisµ-quasi-strongly
convex (Karimi et al., 2016; Necoara et al., 2019):
Assumption 1 (µ-quasi-strong convexity) .The objective function Fis differentiable with ∇θF(θ∗) = 0and
there exists a constant µ>0such that∀θ∈Θ, we have
F(θ∗)≥F(θ) +⟨∇θF(θ),θ∗−θ⟩+µ
2∥θ∗−θ∥2. (2)
Theµ-quasi-strong convexity assumption serves as a relaxed version of strong convexity for the SO problem,
providing a more conservative notion. Various objective functions Femployed in machine learning applications
have been extensively investigated and documented by Teo et al. (2007). Researchers have also explored
3Published in Transactions on Machine Learning Research (07/2023)
milder degrees of convexity, such as the Polyak-Łojasiewicz condition (Polyak, 1963; Lojasiewicz, 1963) studied
by Karimi et al. (2016); Gower et al. (2021) for SGD methods, and their Ruppert-Polyak average investigated
by Gadat & Panloup (2023) under a Kurdyka-Łojasiewicz-type condition (Kurdyka, 1998; Lojasiewicz, 1963).
Relaxing the assumption of strict convexity is essential in practice to ensure the robustness and adaptiveness
of algorithms, particularly for non-strongly convex SO problems (Bach & Moulines, 2013; Nemirovski et al.,
2009; Necoara et al., 2019; Khaled & Richtárik, 2023).
Fbeingµ-quasi-strongly convex does not guarantee the µ-quasi-strong convexity of (ft).It is
crucial to understand that while the objective function Fsatisfies the µ-quasi-strong convexity assumption (2),
the individual loss functions (ft)may not exhibit the same property. This distinction plays an essential role in
our convergence analysis as it interacts with the time-dependency of the problem and the presence of biases.
In section 3, we will explore this relationship in detail and investigate its implications for the convergence
behavior. Specifically, we will examine how the level of dependence, biases, and the µ-quasi-strong convexity
conditions intertwine and affect the convergence properties of the optimization process.
To analyze the Polyak-Ruppert averaging estimate, we need to impose additional smoothness assumptions on
the objective function F, following the framework established in Bach & Moulines (2011); Godichon-Baggioni
et al. (2023). These smoothness assumptions ensure the necessary conditions for the convergence analysis of
the Polyak-Ruppert method (Ruppert, 1988; Polyak & Juditsky, 1992).
Assumption 2 (C∇- andC′
∇-smoothness) .The objective function FhaveC∇-Lipschitz continuous gradients
aroundθ∗, i.e., there exists a constant C∇>0such that∀θ∈Θ,
∥∇θF(θ)−∇θF(θ∗)∥≤C∇∥θ−θ∗∥. (3)
Next, the Hessian of FisC′
∇-Lipschitz-continuous around θ∗, that is, there exists a constant C′
∇≥0such
that∀θ∈Θ,
∥∇2
θF(θ)−∇2
θF(θ∗)∥≤C′
∇∥θ−θ∗∥. (4)
As highlighted in Bottou et al. (2018), the assumption stated in Assumption 2 guarantees that the gradient
∇θFdoes not exhibit arbitrary variations. This property makes ∇θFa valuable guide for reducing the
value ofF. In deterministic optimization, smooth optimization typically achieves faster convergence rates
compared to non-smooth optimization. However, in the context of SO, the benefits of smoothness are limited
to improvements in the associated constants (Nesterov et al., 2018).
2.2Assumptions on Gradient Estimates: Dependence, Bias, Expected Smoothness, and Gradient Noise
LetFt=σ(fi:i≤t)denote the natural filtration of the SO problem (1). The gradients of the time-varying
mini-batches (ft(θ))serve as estimates of ∇θF(θ). Unlike classical assumptions that typically demand
unbiased (and uniformly bounded) gradient estimates (Godichon-Baggioni et al., 2023), we adopt a more
flexible approach. We relax these constraints by allowing the gradients (∇θft(θ))to be time-dependent and
biased estimates of ∇θF(θ)in the following way:
Assumption 3-p (Dννt-dependence and Bννt-bias).For eacht≥1, the random function ∇θftis square-
integrable,Ft-measurable, and for a positive integer p, there exists some positive sequence (νt)t≥1and constants
Dν,Bν≥0such that
E[∥E[∇θft(θ)|Ft−1]−∇θF(θ)∥p]≤νp
t(Dp
νE[∥θ−θ∗∥p] +Bp
ν). (5)
Discussion of Assumption 3-p. Assumption 3-p follows the form of mixing conditions for weakly
dependent sequences, indicating that the dependence diminishes at a rate determined by νt. The verification
of Assumption 3-p can be accomplished using moment inequalities for partial sums of strongly mixing
sequences (Rio, 2017), which is commonly referred to as short-range dependence ; it is also known by other
aliases such as short memory orshort-range persistence .
Indeed, for any positive integer p, Assumption 3-p can be bounded above as follows:
E[∥E[∇θft(θ)|Ft−1]−∇θF(θ)∥p]≤E[∥∇θft(θ)−∇θF(θ)∥p] =n−p
tE[∥St∥p], (6)
4Published in Transactions on Machine Learning Research (07/2023)
using Jensen’s inequality. In (6), St=/summationtextnt
i=1(∇θft,i(θ)−∇θF(θ))represents a d-dimensional vector, and
∇θft(θ) =n−1
t/summationtextnt
i=1∇θft,i(θ). Let (∇θft,i)be a strictly stationary sequence, and assume the existence
of somer > psuch that supx>0(xrQ(x))1/r<∞, whereQ(x)denotes the quantile function of ∥∇θft,i∥.
Suppose (∇θft,i)is stronglyα-mixing according to Rosenblatt (1956), with strong mixing coefficients (αt)t≥1
satisfyingαt=O(t−pr/(2r−2p)). Then, by Rio (2017, Corollary 6.1), we find that E[∥St∥p] =O(np/2
t). This
implies that (6) is at most O(n−p/2
t). This encompasses several linear, non-linear, and Markovian time series,
e.g., see Bradley (2005); Doukhan (2012) for more examples of other mixing coefficients of weak dependence
and their relationships.
In relation to the form of Assumption 3-p, this indicates that Bν̸= 0in this case. However, it is possible
to haveBν= 0in unbiased examples, as we will demonstrate later in section 4. Another example is the
short-term dependent Markovian case, where νt=n−1/2
t, and the dependency constant Dνin Assumption 3-p
can be explicitly expressed using the mixing times of the Markov chain. Nagaraj et al. (2020) worked out
precise calculations for least squares regression. To summarize, we encounter short-range dependence when
(6) decays at most with O(n−p/2
t). Conversely, we encounter long-range dependence when (6) decays slower
thanO(n−p/2
t).
The classical convergence analysis for SGD-based methods relies on the assumption of (uniformly) bounded
gradient estimates, which is restrictive and applicable only to certain types of losses (Bottou et al., 2018;
Nguyen et al., 2018). In our work, we depart from this approach and instead adopt the assumptions introduced
by Bach & Moulines (2011); Gower et al. (2019), who analyzed SGD for quasi-strongly convex objectives as
expressed in (2). Specifically, we make assumptions regarding the expected smoothness of gradients (∇θft)
and the expected finiteness of (∇θft(θ∗)):
Assumption 4-p (Cκ-expected smoothness) .For a positive integer p, there exists some positive constant
Cκsuch that∀θ∈Θ,E[∥∇θft(θ)−∇θft(θ∗)∥p]≤Cp
κE[∥θ−θ∗∥p].
Assumption 5-p (σt-gradient noise) .For a positive integer p, there exists some positive sequence (σt)t≥1
such that E[∥∇θft(θ∗)∥p]≤σp
t.
As for Assumption 3-p, the verification of Assumption 5-p can be achieved using α-mixing conditions and
analogous arguments as those employed above, resulting in σp
tbeing of the order O(n−p/2
t). It is worth noting
that in the classical i.i.d. case with unbiased gradients, alternative relaxations of Assumptions 4-p and 5-p
exist, such as the ER or ABC assumptions (Gower et al., 2021; Khaled & Richtárik, 2023). However, since
our primary focus is on introducing time-dependent and biased gradient estimates, we do not delve into these
alternative assumptions.
In summary, the assumptions we have presented (Assumptions 3-p to 5-p) offer a more relaxed framework
compared to the standard assumptions found in the literature (Benveniste et al., 2012; Kushner & Yin, 2003;
Bach & Moulines, 2011; Godichon-Baggioni et al., 2023; Dieuleveut et al., 2017; Dieuleveut & Bach, 2016).
Our assumptions are designed to accommodate a wide range of scenarios, including both classical examples
and more intricate models that involve learning from time-dependent data with biases. By adopting these
assumptions, we provide a flexible and applicable framework for analyzing SO methods in such settings.
2.3 Stochastic Streaming Optimization Methods
To address the SO problem (1) within a streaming setting, we employ the Stochastic Streaming Gradient
(SSG) method proposed by Godichon-Baggioni et al. (2023). The SSG algorithm is defined as follows
(SSG) θt=θt−1−γt
ntnt/summationdisplay
i=1∇θft,i(θt−1). (7)
Here,γtdenotes the learning rate, which satisfies the conditions/summationtext∞
i=1γi=∞and/summationtext∞
i=1γ2
i<∞(Robbins &
Monro, 1951). It is worth noting that when nt= 1, the SSG algorithm reduces to the usual SGD method.
In many models, there are often constraints imposed on the parameter space, necessitating a projection of
the parameters. To address this, we introduce the Projected Stochastic Streaming Gradient (PSSG) estimate,
5Published in Transactions on Machine Learning Research (07/2023)
Algorithm 1: Stochastic streaming gradient estimates (SSG/PSSG/ASSG/APSSG)
Inputs :θ0∈Θ⊆Rd, project: TrueorFalse, average: TrueorFalse
Outputs:θt,¯θt(resulting estimates)
Initialization: ¯θ0∈Rd
foreacht≥1, a time-varying mini-batch of ntdata arrives, do
θt←θt−1−γt
nt/summationtextnt
i=1∇θft,i(θt−1) /* update */
ifprojectthen
θt←P Θ(θt) /* project */
ifaveragethen
¯θt←(Nt−1/Nt)¯θt−1+ (nt/Nt)θt−1 /* average */
defined as
(PSSG) θt=PΘ/parenleftigg
θt−1−γt
ntnt/summationdisplay
i=1∇θft,i(θt−1)/parenrightigg
, (8)
wherePΘrepresents the Euclidean projection onto Θ, given byPΘ(θ) = arg minθ′∈Θ∥θ−θ′∥2.
If one were to examine the trajectory of the stochastic gradients (∇θft,i), it would become apparent that
they exhibit high noise levels and lack robustness, which can lead to slow convergence or even prevent
convergence altogether. Therefore, it intuitively makes sense to use mini-batches of gradient estimates within
each iteration, as this reduces variance and facilitates the adjustment of the learning rate (γt), ultimately
improving the quality of each iteration.
Within this streaming framework, we are interested in incorporating acceleration techniques into the existing
algorithms presented in (7) and (8). One important extension is the Polyak-Ruppert procedure (Polyak &
Juditsky, 1992; Ruppert, 1988), which ensures optimal statistical efficiency without compromising computa-
tional costs. The Averaged Stochastic Streaming Gradient (ASSG) method, denoted as (ASSG)/(APSSG), is
defined by
(ASSG)/(APSSG) ¯θt=1
Ntt−1/summationdisplay
i=0ni+1θi. (9)
Here,Nt=/summationtextt
i=1nirepresents the accumulated sum of observations.1To ensure a fair comparison of our
streaming methods, it is crucial to evaluate them in terms of the number of observations used, namely Nt.
Similarly, we use APSSG to denote the (Polyak-Ruppert) averaged estimate of PSSG presented in (8). These
averaging methods sequentially aggregate past estimates, resulting in smoother curves (i.e., variance reduction
in the estimation trajectories), and accelerate convergence (Polyak & Juditsky, 1992).
The pseudo-code for these streaming estimates, including (7) to (9), is presented in algorithm 1. Additionally,
(9) can be modified to a weighted average version, giving more weight to the latest estimates. This modification
improves convergence while limiting the impact of poor initializations. Examples of such algorithms can be
found in Boyer & Godichon-Baggioni (2022); Mokkadem & Pelletier (2011).
The popularity of SGD methods has prompted efforts to improve their efficiency, robustness, and user-
friendliness. As a result, numerous variants, including second-order methods like Newton’s method and other
extensions, have been extensively explored and discussed in works such as Boyd & Vandenberghe (2004);
Nesterov et al. (2018); Byrd et al. (2016).
The choice of the learning rate (γt)significantly affects the convergence of SGD. A learning rate that is
too small leads to slow convergence, while an excessively high learning rate may prevent convergence due
to oscillations around the minimum of the loss function. Therefore, there is a strong motivation for the
1In practice, as we handle data sequentially, we employ the rewritten formula ¯θt= (Nt−1/Nt)¯θt−1+ (nt/Nt)θt−1, with
¯θ0= 0. This allows us to update the averaged estimate efficiently.
6Published in Transactions on Machine Learning Research (07/2023)
development of adaptive learning rate mechanisms that require less manual fine-tuning and offer improved
user-friendliness.2Moreover, the idea of having per-dimension learning rates that adjust individually as the
convergence progresses holds potential advantages (Godichon-Baggioni & Tarrago, 2023).
In Bottou et al. (2018), a comprehensive overview of various SO methods is provided, covering both convex and
non-convex optimization scenarios. The paper also delves into strategies for parallelization and distribution,
aiming to accelerate the SGD updates and improve overall performance.
3 Convergence Analysis
In this section, we analyze the convergence of the streaming methods presented in (7) to (9). Our objective is
to establish non-asymptotic bounds on δt=E[∥θt−θ∗∥2]and¯δt=E[∥¯θt−θ∗∥2], which solely depend on the
SO problem parameters.
To achieve this, we derive a recursive relation for δtthat allows us to non-asymptotically bound the estimates
in (7) and (8) for any given sequences of (γt),(νt),(σt), and (nt):
Lemma 1. Letδt=E[∥θt−θ∗∥2], where (θt)either follows the recursion in (7)or(8). Suppose Assumptions 1
and 3-p to 5-p hold for p= 2. Then, for any (γt),(νt),(σt), and (nt), we have
δt≤[1−(µ−2Dννt)γt+ 2C2
κγ2
t]δt−1+B2
ν
µν2
tγt+ 2σ2
tγ2
t. (10)
To prove this lemma, we adapt classical techniques from stochastic approximations (Benveniste et al.,
2012; Kushner & Yin, 2003), originally developed for the unbiased i.i.d. setting, to our specific streaming
setting. Our adaptation incorporates the time-dependence and biases introduced by the new assumptions,
Assumptions 3-p and 5-p. This enables us to gain novel insights into the interplay between dependence
and biases, leading to a deeper understanding of the convergence properties of SO methods. Notably, the
recursive relation for δtexplicitly depends on (γt),(νt),(σt), and (nt), which, to the best of our knowledge, is
a novel contribution. It highlights the connection between µ-quasi-strong convexity and the dependence term
Dνvt, as discussed in section 2.1. We will later refer to this connection as ensuringµ-quasi-strong convexity
through non-decreasing streaming batches when the dependence sequence (νt)is linked to the time-varying
mini-batches (nt).
In section 3.3, we will delve into the convergence analysis of the Polyak-Ruppert averaging estimate ¯θn.
Our goal is to establish a non-asymptotic bound on ¯δt, which quantifies the convergence properties of the
averaging method. This analysis builds upon the standard decomposition of the loss terms, enabling the
emergence of the Cramér-Rao term (Bach & Moulines, 2011; Gadat & Panloup, 2023; Godichon-Baggioni
et al., 2023). To facilitate this analysis, we also need to consider fourth-order moments. However, we will
provide a comprehensive exploration of this aspect in detail in section 3.3.
3.1 Learning Rate (γt), Uncertainty Terms (νt)and(σt), and Time-Varying Mini-batches (nt)
Before proceeding with the convergence analysis in sections 3.2 and 3.3, we first specify the functional forms
of the learning rate (γt), the uncertain terms (νt)and(σt), and the streaming batch (nt).
Learning Rate (γt).Following Godichon-Baggioni et al. (2023), we adopt the learning rate given by
γt=Cγnβ
tt−α,
whereCγ>0,β∈[0,1], andαis chosen based on the expected streaming batches nt. This learning rate
allows us to assign more weight to larger streaming batches through the hyperparameter β.
Uncertainty Terms (νt)and (σt)from Assumptions 3-p and 5-p. The uncertainty terms (νt)and(σt)
in our analysis play a crucial role in capturing the dependence and noise inherent in the SO problem. As
2E.g., see Momentum (Qian, 1999), Nesterov accelerated gradient (Nesterov, 1983), Adagrad (Duchi et al., 2011), Adadelta
(Zeiler, 2012), RMSprop (Tieleman et al., 2012), and Adam (Kingma & Ba, 2014).
7Published in Transactions on Machine Learning Research (07/2023)
discussed in section 2.2, these terms can be viewed as functions of the streaming batches (nt). We define
these terms as follows:
νt=n−ν
tandσt=Cσn−σ
t,
whereν∈(0,∞),σ∈[0,1/2], andCσ>0. By setting νt=n−ν
t, we introduce short-range dependence when
ν∈[1/2,∞)and long-range dependence when ν∈(0,1/2). Hence, the classical i.i.d. case (Godichon-Baggioni
et al., 2023) corresponds to ν→∞. The choice of σ∈[0,1/2]aligns with the framework proposed by
Godichon-Baggioni et al. (2023), where σ= 1/2represents the i.i.d. case. When σ<1/2, it allows for the
presence of noisier outputs.
Time-varying Mini-batches (nt).Inspired by the work of Godichon-Baggioni et al. (2023), we adopt a
formulation for time-varying mini-batches (nt)given by
nt=⌈Cρtρ⌉,
whereCρ∈Nandρ∈[0,1), ensuring that nt∈N. This formulation encompasses various scenarios, including
classical (online) SGD methods for Cρ= 1andρ= 0, (online) mini-batch SGD procedures with both constant
and time-varying sizes when Cρ∈Nandρ= 0orρ∈[0,1), respectively, as well as the Polyak-Ruppert
average of (online) time-varying mini-batches. For ease of reference, we use the term streaming batch size to
refer toCρand the term streaming rate forρ.
3.2 Stochastic Streaming Gradients
Theorem 1 (SSG/PSSG) .Letδt=E[∥θt−θ∗∥2], where (θt)either follows the recursion in (7)or(8).
Suppose Assumptions 1 and 3-p to 5-p hold for p= 2. Ifµν=µ− 1{ρ=0}2DνC−ν
ρ>0, then there exist
explicit constants Cδ,C′
δ,C′′
δ>0such that for α−ρβ∈(1/2,1), we have
δt≤O
exp
−µCγN1+ρβ−α
1+ρ
t
CδC1−β−α
1+ρρ

+C′
δB2
ν
µµνC2ν
1+ρρN2ρν
1+ρ
t+C′′
δC2
σCγ
µνC2σ−β−α
1+ρρNρ(2σ−β)+α
1+ρ
t. (11)
An explicit version of this bound is given in appendix A.
Sketch of proof. Under Assumptions 1 and 3-p to 5-p with p= 2, we can establish a recursive relation for
(δt)defined by
δt≤[1−(µ−2Dννt)γt+ 2C2
κγ2
t]δt−1+µ−1B2
νν2
tγt+ 2σ2
tγ2
t,
for any form of (γt),(νt),(σt), and (nt). The non-asymptotic upper bound of this recursive relation (10)
can be explicitly derived using classical techniques from stochastic approximations (Benveniste et al., 2012;
Kushner & Yin, 2003). Bounding the projected estimate in (8) can directly be obtained by noting that
E[∥PΘ(θ)−θ∗∥2]≤E[∥θ−θ∗∥2]. Alternatively, the projected estimate (8) can also be proved by assuming a
bounded gradient, which replaces Assumptions 4-p and 5-p. Detailed discussions and alternative proofs can
be found in works such as Bach & Moulines (2011); Godichon-Baggioni et al. (2023). The discontinuity in ρ
emerges from the introduction of an indicator that determines whether (νt)is constant or decreasing. When
(νt)is constant, it becomes challenging to obtain a bound that closely approaches µ. Conversely, when (νt)
decreases, there is a possibility of achieving a bound that can come as close as possible to µ. In the i.i.d.
case, this choice corresponds to the optimal strong convexity parameter (Godichon-Baggioni et al., 2023).
Alternatively, we can derive a better bound by considering a continuous trade-off where we minimize (νt)
in the bound while minimizing the other terms. This continuous trade-off has the potential to provide a
smoother transition between the cases of constant and decreasing (νt).
Related work. Our work extends and aligns with previous studies in the field. First, our results replicate
the outcomes of the unbiased i.i.d. case investigated in Godichon-Baggioni et al. (2023), specifically when
Bν= 0andσ= 1/2. This demonstrates the consistency of our findings with regard to the unbiased i.i.d.
setting. Additionally, our results are in line with the research conducted by Bach & Moulines (2011), which
focused on the unbiased i.i.d. case in a non-streaming setting, i.e., when Cρ= 1andρ= 0.
8Published in Transactions on Machine Learning Research (07/2023)
Bound on objective function. If the objective function Fhas gradients that are C∇-Lipschitz continuous
(Assumption 2), the inequality given by (11) provides a bound on the function values of F. Specifically, we
haveE[F(θt)−F(θ∗)]≤C∇δt/2by applying Cauchy-Schwarz’s inequality.
Ensuring µ-quasi-strong convexity through non-decreasing streaming batches. The positivity
of the dependence penalized convexity constant µν=µ− 1{ρ=0}2DνC−ν
ρis crucial for all terms of (11).
This constraint arises from the fact that while the objective function Fisµ-quasi-strongly convex, the loss
functions (ft)may not possess the same convexity properties, e.g., see discussion in section 2.1. This is
demonstrated in sections 4.1.2 and 4.1.3 for ARCH models (Werge & Wintenberger, 2022).
So, how should we interpret µν? When the streaming rate ρ= 0, the positivity of µνdepends on the convexity
constantµ, the streaming batch size Cρ, and the imposed dependencies specified in Assumption 3-p. If the
dependence quantity Dνis sufficiently large that µνbecomes non-positive, it becomes necessary to choose a
larger streaming batch size Cρto ensureµνremains positive. In other words, strong dependency structures
reduce convexity, but large mini-batches counteracts this effect.
An alternative approach to ensuring convexity is by employing increasing streaming batches, i.e., setting the
streaming rate ρ>0. This method provides greater robustness as we no longer need to determine a specific
value for the mini-batch size Cρto maintain the positivity of µν. However, combining both approaches is ideal
as it ensures convexity while a larger Cρeffectively reduces variance, resulting in a more favorable outcome.
In section 4, we delve into the challenge of calibrating Cρandρappropriately to achieve both stability and
convergence in the optimization process. The goal of this calibration is to strike a balance between taking
frequent gradient steps for faster convergence and ensuring the positivity of µν. To accomplish this, it is
crucial to choose a sufficiently large Cρandρthat can maintain the positivity of µνwhile allowing for as
many gradient steps as possible. This delicate balance is essential in achieving optimal performance in the
optimization process.
Variance reduction with larger streaming batches Cρ.Unsurprisingly, larger streaming batches Cρ
have a variance-reducing effect, as illustrated in section 4. This effect is explicitly demonstrated in each term
of (11). Interestingly, the variance reduction resulting from large mini-batches scales with batch size but does
not increase the decay rate of δt.
Decay of initial conditions. The initial conditions in the first term of (11) decay sub-exponentially. A
detailed expression for this term can be found in appendix A. The last term of (11) represents the noise term,
which is influenced by the gradient noise as described in Assumption 5-p. When α−ρβ∈(1/2,1), the noise
term decays at a rate of O(N−(ρ(2σ−β)+α)/(1+ρ)
t ). For instance, if we set α= 2/3,β= 1/3, andσ= 1/2,
the noise term decays as O(N−2/3
t)for any streaming rate ρ∈[0,1). This decay behavior is illustrated in
Godichon-Baggioni et al. (2023). Notably, in the unbiased cases ( Bν= 0), this noise term corresponds to the
asymptotic term (Godichon-Baggioni et al., 2023). Moreover, when α+β <2σ, the noise/asymptotic term is
positively influenced by larger streaming batches Cρ. This effect is demonstrated in section 4.
Behavior under biasedness, Bν>0.The second term in (11) represents a pure bias term determined
by the bias quantity Bν, the level of dependence ν, and the (dependence-penalized) convexity constant µν.
Importantly, the bias term is independent of the learning rate γt=Cγnβ
tt−α, but depends on the time-varying
mini-batches nt=Cρtρ, i.e., through the streaming batch size Cρand the streaming rate ρ.
The dependence term exhibits a scaling of O(N−2ρν/(1+ρ)
t ). For instance, to achieve a decay rate of O(N−1/2
t),
we would need ρ= 1andν= 1/2. It is remarkable that Theorem 1 accommodates both long- and short-range
dependence. While long-range dependence leads to slow convergence (slower than O(N−1/2
t)), a positive
streaming rate ρcan break long-range dependence. In summary, by increasing the streaming batch size, we
preserveµ-quasi-strong convexity and alleviate both long- and short-term dependence. This results in a
bound ofδt=O(max{ 1{Bν>0}N−2ρν/(1+ρ)
t,N−(ρ(2σ−β)+α)/(1+ρ)
t }).
9Published in Transactions on Machine Learning Research (07/2023)
3.3 Averaged Stochastic Streaming Gradients
In our subsequent analysis, our main focus is on the averaging estimate ¯θn, which is defined in (9). This
averaging estimate is obtained from either the SSG estimate in (7) or the PSSG estimate in (8). Our analysis
relies on the standard decomposition of the loss terms, which allows the Cramér-Rao term to emerge (Bach
& Moulines, 2011; Gadat & Panloup, 2023; Godichon-Baggioni et al., 2023). To facilitate this analysis, it is
necessary to consider the fourth-order moments, which require the assumptions Assumptions 3-p to 5-p to
hold forp= 4. Moreover, based on Assumption 5-p, where σt=Cσn−σ
twithσ∈[0,1/2], we introduce an
additional assumption regarding the covariance of the score vectors associated with the parameter vector θ∗.
Assumption 6 (Covariance of the scores) .There exists a non-negative self-adjoint operator Σsuch that
∀t≥1,n2σ
tE[∇θft(θ∗)∇θft(θ∗)⊤]⪯Σ + Σt, where Σtis a positive symmetric matrix with Tr(Σt) =C′
σn−2σ′
t,
C′
σ≥0, andσ′∈(0,1/2].
Insomecases, suchasthei.i.d. scenarioandcertainunbiasedsituationsdiscussedinsection4.1.1, Assumption6
holds with σ= 1/2andC′
σ= 0(Godichon-Baggioni et al., 2023). This condition applies to scenarios
characterized by short-range dependence, as shown in section 4.1.1 with σ= 1/2(andC′
σ>0). Conversely,
the long-range dependence case occurs when σ<1/2(andC′
σ>0). Under Assumption 6, we can derive the
dominant term Λ/Nt, where Λ =Tr(∇2
θF(θ)−1Σ∇2
θF(θ)−1). This assumption also enables the establishment
of the Cramer-Rao lower bound for the case of unbiased i.i.d. samples. Specifically, the bound can be
expressed as
E[∥¯θt−θ∗∥2]≤O(ΛN−1
t) +O(N−b
t)withb>1.
In order to analyze the projected averaged estimate (a.k.a. APSSG), an additional assumption is made to
avoid the computation of the sixth-order moment. We assume that (∇θft)is uniformly bounded on the
compact Θ. However, it is worth mentioning that the derivation of the sixth-order moment can be found in
Godichon-Baggioni (2016).
Assumption 7 (GΘ-bounded gradients) .LetDΘ=infθ∈∂Θ∥θ−θ∗∥>0with∂Θdenoting the boundary of
Θ. Moreover, there exists GΘ>0such that∀t≥1,supθ∈Θ∥∇θft(θ)∥2≤G2
Θa.s.
Theorem 2 (ASSG/PASSG) .Let¯δt=E[∥¯θt−θ∗∥2]with ¯θngiven by (9), where (θt)either follows the
recursion in (7)or(8). Suppose Assumptions 1 to 6 hold for p= 4. In addition, Assumption 7 must hold if
(θt)follows the recursion in (8). Ifµν=µ− 1{ρ=0}2DνC−ν
ρ>0, then forα−ρβ∈(1/2,1), we have
¯δ1
2
t≤Λ1
2
N1
2
t1{σ=1/2}+21
2Λ1
2C1−2σ
2(1+ρ)
ρ
N1+2ρσ
2(1+ρ)
t1{σ<1/2}+21
2C′1
2σC1−2(σ+σ′)
2(1+ρ)
ρ
µN1+2ρ(σ+σ′)
2(1+ρ)
t(12)
+˜O/parenleftbigg
max/braceleftbigg
N−2+ρ(2σ+β)−α
2(1+ρ)
t ,N−ρ(2σ−β)+α
1+ρ
t/bracerightbigg/parenrightbigg
+ 1{Bν̸=0}Ψt, (13)
with
Ψt=˜O/parenleftbigg
max/braceleftbigg
N−ρ(σ+ν)
2(1+ρ)
t,N−1+ρ(β+ν)−α
1+ρ
t ,N−1+2ρν
2(1+ρ)
t,N−δ/2+ρν
2(1+ρ)
t,N−2ρν
1+ρ
t/bracerightbigg/parenrightbigg
,
whereδ= 1{Bν=0}(ρ(2σ−β) +α) + 1{Bν̸=0}min{ρ(2σ−β) +α,2ρν}. An explicit version of this bound is
given in appendix A.
Related work. It is important to note that the bound presented in Theorem 2 is for the root mean square
error, while our discussions focus on ¯δtwithout taking the root. Similar to the unbiased i.i.d. case explored
in Godichon-Baggioni et al. (2023), the dominant term of ¯δtin (12) and (13) is Λ/Nt, which achieves the
asymptotically optimal Cramer-Rao lower bound (Murata & Amari, 1999). Each term in (12) directly stems
from Assumption 6. Importantly, these terms remain unaffected by the choice of learning rate (γt), but
depend on the time-varying mini-batches (nt). As discussed in Gadat & Panloup (2023), the bound of ¯δtcan
be interpreted as a bias-variance decomposition between the leading terms in (12) and the remaining terms
in (13).
Ensuringµ-quasi-strong convexity through non-decreasing streaming batches. The interpretation
ofµνin Theorem 2 is similar to that in Theorem 1. In both cases, the positivity of µνplays a crucial role in
10Published in Transactions on Machine Learning Research (07/2023)
all terms of (13), despite not being immediately apparent. Analogous to the insights gained from Theorem 1,
increasing the streaming batch size allows us to preserve µ-quasi-strong convexity and alleviate both long-
and short-term dependence. By choosing a larger streaming batch size, we ensure the positivity of µν, as
demonstrated in the experiments conducted for ARCH models in sections 4.1.2 and 4.1.3.
Accelerated decay through Polyak-Ruppert averaging. Through Polyak-Ruppert averaging, it is
possible to achieve the leading term Λ/Nt, which is known to attain the asymptotically optimal Cramer-Rao
bound in the i.i.d. case (Godichon-Baggioni et al., 2023). Consequently, we can achieve the optimal and
irreducible rate of ¯δt=O(N−1
t). This is always accomplished in the unbiased case ( Bν= 0) withσ= 1/2,
even under short-range dependence.
In the specific case of σ= 1/2, (12) and (13) simplify significantly. The last two terms of (12) become negligible
asσ′>0. The first term of (13) decays at the rate O(N−(2+ρ(2σ+β)−α)/(1+ρ)
t )orO(N−2(ρ(2σ−β)+α)/(1+ρ)
t ).
Choosingα,βsuch thatα+ρ(2σ/3−β) = 2/3(e.g.,α= 2/3,β= 1/3, andσ= 1/2) yields a decay of
O(N−4/3
t)for anyρ. Hence, by setting α= 2/3andβ= 1/3whenσ= 1/2, the first term of (13) robustly
achievesO(N−4/3
t)for any streaming rate ρ. Similarly, the last term of (13) is O(N−ρ(1/2+ν)/(1+ρ)
t )for any
ρwhenα= 2/3andβ= 1/3. In summary, Theorem 2 with α= 2/3,β= 1/3, andσ= 1/2simplifies to the
bound:
¯δ1
2
t≤Λ1
2
N1
2
t+˜O/parenleftig
N−2
3
t/parenrightig
+ 1{Bν̸=0}˜O/parenleftbigg
N−ρ(1/2+ν)
2(1+ρ)
t/parenrightbigg
. (14)
Variance reduction from larger streaming batches Cρ.Polyak-Ruppert averaging accelerates conver-
gence and, in particular, the decay rate. However, by taking large mini-batches, we can also achieve variance
reduction. As discussed earlier (for Theorem 1), larger mini-batches scale the error, which is particularly
beneficial in the initial stages. Therefore, combining averaging and mini-batches offers the best of both worlds,
resulting in a better slope (decay rate) and intercept.
Behavior under biasedness, Bν>0.The bias term Bνaffects only Ψt, except for the second term in
(13), which impacts the decay rate δ. Increasing the streaming rate ρdiminishes the negative influence of
the bias term. Surprisingly, Ψtapproaches zero as tincreases indefinitely for any ν. However, achieving the
desired decay rate of ¯δt=O(N−1
t)excludes long-range dependence. Nevertheless, in our experiments (see
section 4), the bias term does not seem to have a significant impact.
4 Experiments
In this section, we present the details and results of our experiments, aimed at evaluating the performance of
our streaming SGD-based methods on both synthetic and real-world data. Our objective is to demonstrate
findings that highlight the following: (i) time-varying mini-batch SGD methods have the capability to break
long- and short-range dependence structures, (ii) biased SGD methods can achieve comparable performance to
their unbiased counterparts, and (iii) incorporating Polyak-Ruppert averaging can accelerate the convergence.
These findings collectively showcase the effectiveness and potential of our streaming SGD-based methods.
4.1 Synthetic Data
A way to illustrate our findings is by use of classical methods that aim to model and predict an underlying
sequence of real-valued time-series (Xs); heresis short notation for indexing the sequence of observations,
(XNt,XNt−1,...,XNt−nt≡XNt−1,XNt−1−1,...)withNt=/summationtextt
i=1nt. The AutoRegressive (AR), Moving-
Average (MA), and AutoRegressive Moving-Average (ARMA) models are the most well-known models
for time-series (Brockwell & Davis, 2009; Box et al., 2015; Hamilton, 2020). The standard time-series
analysis often relies on independence and constant noise, but it can be relaxed by, e.g., the AutoRegressive
Conditional Heteroskedasticity (ARCH) model (Engle, 1982). Online learning algorithms of (both stationary
and non-stationary) dependent time-series have been studied in Agarwal & Duchi (2012); Anava et al. (2013);
Wintenberger (2021).
11Published in Transactions on Machine Learning Research (07/2023)
4.1.1 AR Model
A process (Xs)is called a (zero-mean) AR (1)process, if there exists real-valued parameter θsuch that
Xs=θXs−1+ϵs, where (ϵs)is weak white noise with zero mean and variance σ2
ϵ. To illustrate the versatility
of our results, we construct some (heavy-tailed) noise processes with long-range dependence: the noisiness is
integrated using a Student’s t-distribution with degrees of freedom above four, denoted by (zs). The long-range
dependence is incorporated by multiplying (zs)with the fractional Gaussian noise Gs(H) =Bs+1(H)−Bs(H),
where (Bs(H))is a fractional Brownian motion with Hurst index H∈(0,1).(Gs(H))can also be seen as a
(zero-mean) Gaussian process with stationary and self-similar increments (Nualart, 2006).
Well-specified case. Consider the well-specified case, in which, we estimate an AR (1)model from the
underlying stationary AR (1)processXs=θ∗Xs−1+ϵswith|θ∗|<1. The squared loss function ft(θ) =
n−1
t/summationtextnt
i=1(XNt−1+i−θXNt−1+i−1)2with gradient∇θft(θ) =−2n−1
t/summationtextnt
i=1XNt−1+i−1(XNt−1+i−θXNt−1+i−1).
Thus, the objective function is F(θ) = (σ2
ϵ(θ∗−θ)2)/(1−(θ∗)2)+σ2
ϵ, asE[Xs] = 0andE[X2
s] =σ2
ϵ/(1−(θ∗)2),
yielding∇θF(θ) = 2σ2
ϵ(θ−θ∗)/(1−(θ∗)2). Assumption 3-p with p= 2can be written as follows:
E[∥E[∇θft(θ)|Ft−1]−∇θF(θ)∥2] =4(θ−θ∗)2(1−(θ∗)2nt)2σ2
ϵ
(1−(θ∗)2)4n2
t/parenleftbigg
σ2
ϵ+1
1−(θ∗)2/parenrightbigg
.
This implies that Assumption 3-p holds when (Xs)has bounded moments, which is satisfied under the
natural constraint |θ∗|<1.3As a result, we can deduce that Dν>0,Bν= 0, andνtisO(n−1
t). Similarly,
Assumption 5-p can be verified in the same manner, with σtbeingO(n−1/2
t). Additionally, Assumption 2
holds with C∇= 2σ2
ϵ/(1−(θ∗)2)andC′
∇= 0, while Assumption 6 holds with Σ = 4σ4
ϵ/(1−(θ∗)2)and
Σt= 0. Furthermore, for an AR (1)process (Xs)constructed using the noise process ϵs=/radicalbig
Gs(H)zswith
Hurst index H≥1/2, one can verify that ν4
tandσ4
tareO(nH−1
t)using the self-similarity property (Nourdin,
2012).
Misspecified case. Now, assume that the underlying data generating process follows a MA (1)-process:
Xs=ϵs+ϕ∗ϵs−1, withϕ∗∈R. The misspecification error of fitting an AR (1)model to a MA (1)process
can be found by minimizing F(θ) =E[(Xs−θXs−1)2] =σ2
ϵ(1 + (ϕ∗−θ)2+θ2(ϕ∗)2), where∇θF(θ) =
2(θ−ϕ∗)σ2
ϵ+ 2θ(ϕ∗)2σ2
ϵ. Thus, as θ∗=arg minθF(θ)≡arg minθ(ϕ∗−θ)2+θ2(ϕ∗)2is a strictly convex
function in θ, we have∇θF(θ) = 0⇔2(θ−ϕ∗) + 2θ(ϕ∗)2= 0⇔2θ(1 + (ϕ∗)2) = 2ϕ∗⇔θ=ϕ∗/(1 + (ϕ∗)2).
Thus, for any ϕ∗∈R, we haveθ∈(−1/2,1/2). With this in mind, we can conduct our study of fitting
an AR(1) model to a MA(1) process with ϕ∗drawn randomly from R(figure 1b). Furthermore, this
reparametrization trick can be used to verify Assumption 3-p in the following way:
E[∥E[∇θft(θ)|Ft−1]−∇θF(θ)∥2] =4(θ−θ∗)2
n2
tfϕ∗(ϵNt−1),
wherefϕ∗(ϵNt−1)is finite function depending on the moments of (ϵNt−1)andϕ∗.4Consequently, we establish
Dν>0,Bν= 0, andνtbeingO(n−1
t). Similarly, using the reparametrization trick, we can verify that σtis
O(n−1/2
t)(Assumption 5-p).
4.1.2 ARCH Model
In time series analysis, modeling heteroscedasticity of the conditional variance is a crucial component,
especially in capturing phenomena like volatility clustering in financial time series. ARCH models are widely
recognized models that effectively incorporate this characteristic. A process (ϵs)is called an ARCH (1)process
with parameters α0andα1if it satisfies
/braceleftigg
ϵs=σszs,
σ2
s=α0+α1ϵ2
s−1,(15)
whereα0>0andα1≥0ensures the non-negativity of the conditional variance process (σ2
s), and the
innovations (zs)is white noise. We employ the Quasi-Maximum Likelihood (QML) procedure for the
3The verification is available in a longer version in appendix B.
4The verification is available in a longer version in appendix B.
12Published in Transactions on Machine Learning Research (07/2023)
statistical inference as outlined in Werge & Wintenberger (2022); the quasi likelihood losses is given by
fs(θ) = 2−1(ϵ2
s/σ2
s(θ) + log(σ2
s(θ))with first-order derivative
∇θfs(θ) =∇θσ2
s(θ)/parenleftbiggσ2
s(θ)−ϵ2
s
2σ4s(θ)/parenrightbigg
,
where∇θσ2
s(θ) = (1,ϵ2
s−1)T. Verification of Assumptions 3-p to 5-p can be done using mixing conditions;
Francq & Zakoian (2019, Theorem 3.5) showed that stationary ARCH processes are geometrically β-mixing,
which implies α-mixing as well. Observe that the loss function (fs)itself is not strongly convex but only
the objective function Fmay be strongly convex; convexity conditions of ARCH processes was investigated
in Wintenberger (2021). This makes the parameters challenging to estimate in empirical applications as
the optimization algorithms can quickly fail or converge to irregular solutions. There are different ways to
overcome lack of convexity: first, projecting the estimates such that the (conditional) variance process (σ2
s)
stays away from zero (and close to the unconditional variance). Second, in the specific example of ARCH
model, one could also recover convexity by implementing variance targeting techniques; an example using
Generalized ARCH (GARCH) models can be found in Werge & Wintenberger (2022). To simplify our analysis
we consider stationary ARCH(1) processes, where we fix α0at1and initialize it at 1/2.
4.1.3 AR-ARCH Model
We complete our experiments by considering an AR models with ARCH noise: the process (Xs)is called an
AR(1)-ARCH (1)process with parameters θ,α0, andα1, if it satisfies


Xs=θXs−1+ϵs,
ϵs=σszs,
σ2
s=α0+α1ϵ2
s−1,(16)
where the innovations (zs)is weak white noise. The statistical inference of this model is done using the
squared loss for the AR-part and the QML procedure for the ARCH part, e.g., see sections 4.1.1 and 4.1.2.
Assumptions 3-p and 5-p can be verified by Doukhan (1994, Proposition 6), which showed that ARMA-ARCH
processes are β-mixing.
4.1.4 Discussion
Our experimental evaluation assesses the performance using the mean quadratic error E[∥θNt−θ∗∥2]across
one thousand replications. The initial values θ0andθare randomly generated according to the specifications of
the models. We intentionally omit projecting our estimates to highlight the errors stemming from dependence
and lack of convexity. By averaging over multiple replications, we observe a reduction in variability, which
primarily benefits the SSG method. The experiments aim to demonstrate the impact of the choice of Cρ
andρon the measures of dependence Dν, biasBν, and the penalized convexity constant µνassociated
with dependence. To facilitate a comparison between different data streams, we set the parameters Cγ= 1,
α= 2/3, andβ= 0as fixed values.
The experiments described in sections 4.1.1 to 4.1.3 can be found in figure 1; here {Cρ= 1,ρ= 0}corresponds
to the classical SG descent and its (Polyak-Ruppert) average estimate, {Cρ= 64,ρ= 0}is a mini-batch
SSG/ASSG, and {Cρ= 64,ρ= 1/2}is an increasing SSG/ASSG with initial batch size of Cρ= 64.
Let’s examine the AR cases, both well-specified and misspecified, as shown in figures 1a and 1b. These figures
display the results for long-range dependent white noise processes with a Hurst index of H= 3/4. It is evident
that each pair of data streams converges, but the traditional SG method exhibits significant initial noise,
particularly affecting the average estimate period without affecting its decay rate. An improvement could be
achieved by modifying our average estimate to a weighted average version, which could mitigate the impact
of poor initializations (Mokkadem & Pelletier, 2011; Boyer & Godichon-Baggioni, 2022). Nonetheless, even
with this noise, the ASSG method still demonstrates better convergence. Both methods show a noticeable
reduction in variance as Cρincreases, which is particularly advantageous in the early stages. However,
excessively large streaming batch sizes Cρmay hinder convergence due to fewer iterations. Additionally,
13Published in Transactions on Machine Learning Research (07/2023)
Figure 1: Simulation of various data streams nt=Cρtρ. See section 4.1 for details.
(a) AR(1): well-specified case. See section 4.1.1 for
details.
(b) AR(1): misspecified case. See section 4.1.1 for
details.
(c) ARCH(1). See section 4.1.2 for details.
 (d) AR(1)-ARCH(1). See section 4.1.3 for details.
figures 1a and 1b indicate improved decay for the SSG methods as the streaming rate ρincreases. On the
other hand, improvements in the ASSG method are not observed, as we do not leverage the potential of using
more observations through the βparameter, which could accelerate convergence (refer to section 4.2). It
is surprising that we do not observe any effects from the long-range dependent white noise processes, but
this seems to be an artifact effect in the proof, as fourth-order moments are required (i.e., Assumptions 3-p
to 5-p with p= 4). Therefore, we conjecture that only second-order moment properties are responsible for
the behavior of our simulations and that σ= 1/2holds even for long-range dependent white noise processes,
as proven in section 4.1.1.
Moving on to figures 1c and 1d, we present the experiments for stationary ARCH(1) models, both with
and without an AR component, as outlined in sections 4.1.2 and 4.1.3, respectively. These figures illustrate
the lack of convexity when using small streaming batch sizes Cρ, such as the traditional SG descent and
its average estimate {Cρ= 1,ρ= 0}, which leads to divergence. It is important to note that the lack of
convexity is reflected in the absence of a positive µν, which can only be counteracted by larger streaming
batch sizes Cρ. Moreover, figure 1d excludes the traditional SG descent {Cρ= 1,ρ= 0}due to its lack of
14Published in Transactions on Machine Learning Research (07/2023)
convexity. The figure demonstrates that larger ( Cρ= 64) and non-decreasing ( ρ≥0) streaming batches can
converge even under challenging settings.
4.2 Historical Hourly Weather Data
To illustrate our methodology on real-life time-dependent streaming data, we consider some historical hourly
weather data.5This dataset comprises approximately five years (roughly 45000 data points) of high temporal
resolution hourly measurements encompassing various weather attributes, including temperature, humidity,
and air pressure. The dataset encompasses thirty-six cities, resulting in a dimensionality of d= 36. In our
analysis, we specifically focus on the hourly temperature measurements. To account for monthly and annual
seasonality, we preprocess the data by subtracting the corresponding monthly and annual averages. This
ensures that the analysis is centered around the deviations from the expected seasonal patterns.
4.2.1 Geometric Median
In the presence of noisy data, robust estimators such as the geometric median are preferred. The geometric
median, introduced by Haldane (Haldane, 1948), is a robust generalization of the real median. Its efficiency
makes it particularly well-suited for handling high-dimensional streaming data (Cardot et al., 2013; Godichon-
Baggioni, 2016). To estimate the geometric median of X∈Rd, we minimize the objective function F(θ) =
E[∥X−θ∥−∥X∥]using stochastic gradient estimates ∇θf(θ) = (X−θ)/∥X−θ∥. The existence, uniqueness,
and robustness (breakdown point) of the geometric median have been studied in Kemperman (1987); Gervini
(2008). It is important to note that this objective function only possesses locally strong convexity properties
(Cardot et al., 2013). However, by projecting the gradients, it is possible to adapt the proof of optimality in a
streaming setting, as demonstrated by Gadat & Panloup (2023). Alternatively, if Xis bounded, one can
adapt the approach of Cardot et al. (2012) to the streaming setting, which shows that the estimates remain
bounded and projection is unnecessary in such cases. In our analysis, we choose not to project our estimates,
as doing so would obscure the errors we intend to explore.
4.2.2 Discussion
Similar to previous evaluations, we assess the performance using the mean quadratic error of the parameter
estimates over one hundred replications, denoted as E[∥θNt−θ∗∥2]. In this comparison, we contrast our
estimates with the geometric median estimate obtained through Weiszfeld’s algorithm (Weiszfeld & Plastria,
2009). We suppose our data are standard Gaussian random variables centered at (θi)1≤i≤d, where each θiis
randomly selected from the range [−d,d]. To align with the reasoning of Cardot et al. (2013), we set Cγ=√
d
and choose α= 2/3.
Figure 2 presents the results of the geometric median estimation, following the procedure outlined in
section 4.2.1. Despite the robustness of the geometric median, noticeable fluctuations are observed in figure 2,
which arise from the time-dependency and noise present in the weather measurements. Figure 2a emphasizes
the importance of utilizing a mini-batch size Cρto stabilize the optimization process and ensure convexity
through larger streaming batches Cρ. However, to achieve reasonable convergence, it is crucial to employ
increasing streaming batches with positive streaming rates ρ>0, as illustrated in figures 2b and 2c. These
figures demonstrate an enhanced decay of the SSG when the streaming rate ρis increased. However, the
lack of convergence improvement in figure 2c can be attributed to the use of β= 0, which neglects the
potential benefits of leveraging additional observations to accelerate convergence. For further insights into
this matter, refer to Godichon-Baggioni et al. (2023). As discussed after Theorem 2, one way to achieve
this acceleration is by setting α= 2/3andβ= 1/3. Figure 2d demonstrates that simply selecting β= 1/3
enables this acceleration. Moreover, β= 1/3ensures optimal convergence robust to any streaming rate ρ.
Selecting an appropriate β >0is particularly crucial when Cρis large, as robustness is a fundamental aspect
of the geometric median method. Most surprisingly, excellent convergence with a final error as low as 10−5
can be achieved by combining increasing streaming batches with averaging. This is exemplified in figure 2d
withCρ= 64,ρ>0, andβ= 1/3. Based on these real-life experiments and the discussion in section 4.1.4,
5The historical hourly weather dataset can be found on https://www.kaggle.com/datasets/selfishgene/
historical-hourly-weather-data .
15Published in Transactions on Machine Learning Research (07/2023)
Figure 2: Geometric median for various data streams nt=Cρtρ. See section 4.2 for details.
(a) Varying Cρ,ρ= 0,β= 0
 (b) Varying ρ,Cρ= 1,β= 0
(c) Varying ρ,Cρ= 64,β= 0
 (d) Varying ρ,Cρ= 64,β= 1/3
we speculate that the sequence of scores (∇θft(θ∗))constitutes a martingale difference sequence that extends
beyond our specific examples. Notably, our findings indicate that σ= 1/2holds even in the presence of
long-range dependence. Thus, the complexity associated with Theorem 2 appears to be an artifact of the
proof, which relies on Assumptions 3-p to 5-p with p= 4.
5 Conclusions and Future Work
In this paper, we explored SGD-based methods in the context of streaming data. We extended the analysis
of the unbiased i.i.d. case by Godichon-Baggioni et al. (2023) to include time-dependency and biasedness. By
leveraging their insights, we investigated the effectiveness of first-order SO methods in a streaming setting,
where the assumption of unbiased i.i.d. samples no longer holds. Our non-asymptotic analysis established
novel heuristics that bridge the gap between dependence, biases, and the convexity levels of the SO problem.
These heuristics enabled accelerated convergence in complex problems, offering promising opportunities for
efficient optimization in streaming settings. Specifically, our findings demonstrated that (i) time-varying
16Published in Transactions on Machine Learning Research (07/2023)
mini-batch SGD methods can break long- and short-range dependence structures, (ii) biased SGD methods
can achieve comparable performance to their unbiased counterparts, and (iii) incorporating Polyak-Ruppert
averaging can accelerate the convergence. We validated our theoretical findings by conducting a series of
experiments using both simulated and real-life time-dependent data.
Future perspectives. There are several ways to expand our work about stochastic streaming algorithms:
(a) we can extend our analysis to include streaming batches of any size (and not as a function of streaming
batch sizeCρand streaming rates ρ), e.g., Godichon-Baggioni et al. (2023) discuss random streaming batches
with negative and positive drift. (b) an extension to non-strongly convex objectives could be advantageous as
it will provide more insight into how we should choose our learning rates (Bach & Moulines, 2013; Nemirovski
et al., 2009; Necoara et al., 2019; Gadat & Panloup, 2023). (c) learning rates should be made adaptive so they
are robust to poor initialization and require less tuning; an adaptive learning rate is essential for practitioners
as it builds a form of universality across applications, e.g., see Duchi et al. (2011); Kingma & Ba (2014);
Godichon-Baggioni & Tarrago (2023). (d) non-parametric analysis could improve our theoretical results
for large values of d. (e) we have focused on results in quadratic mean but another way to strengthen our
non-asymptotic guarantees could be high probability bounds (Durmus et al., 2021; 2022); for any δ∈(0,1),
we could obtain bounds on the sequence {∥θt−θ∗∥:t∈N}that holds with probability at least 1−δ.
References
Yaser S Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from data , volume 4. AMLBook
New York, 2012.
Alekh Agarwal and John C Duchi. The generalization ability of online algorithms for dependent data. IEEE
Transactions on Information Theory , 59(1):573–587, 2012.
Ahmad Ajalloeian and Sebastian U Stich. On the convergence of sgd with biased gradients. arXiv preprint
arXiv:2008.00051 , 2020.
Oren Anava, Elad Hazan, Shie Mannor, and Ohad Shamir. Online learning for time series prediction. In
Conference on learning theory , pp. 172–184. PMLR, 2013.
Francis Bach and Eric Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine
learning. Advances in neural information processing systems , 24, 2011.
Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with convergence
rate o (1/n). Advances in neural information processing systems , 26, 2013.
Albert Benveniste, Michel Métivier, and Pierre Priouret. Adaptive algorithms and stochastic approximations ,
volume 22. Springer Science & Business Media, 2012.
Dimitri Bertsekas. Nonlinear Programming , volume 3. Athena Scientific, 2016.
Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning , volume 4. Springer,
2006.
Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning.
Siam Review , 60(2):223–311, 2018.
George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series analysis: forecasting
and control . John Wiley & Sons, 2015.
Stephen Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.
Claire Boyer and Antoine Godichon-Baggioni. On the asymptotic rate of convergence of stochastic newton
algorithms and their weighted averaged versions. Computational Optimization and Applications , pp. 1–52,
2022.
Richard C Bradley. Basic properties of strong mixing conditions. a survey and some open questions. Probability
surveys, 2:107–144, 2005.
17Published in Transactions on Machine Learning Research (07/2023)
Peter J Brockwell and Richard A Davis. Time series: theory and methods . Springer Science & Business
Media, 2009.
Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-newton method
for large-scale optimization. SIAM Journal on Optimization , 26(2):1008–1031, 2016.
Hervé Cardot, Peggy Cénac, and Jean-Marie Monnez. A fast and recursive algorithm for clustering large
datasets with k-medians. Computational Statistics & Data Analysis , 56(6):1434–1449, 2012.
Hervé Cardot, Peggy Cénac, and Pierre-André Zitt. Efficient and fast estimation of the geometric median in
hilbert spaces with an averaged stochastic gradient algorithm. Bernoulli , 19(1):18–43, 2013.
Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning
algorithms. IEEE Transactions on Information Theory , 50(9):2050–2057, 2004.
Jie Chen and Ronny Luss. Stochastic gradient descent with biased but consistent gradient estimators. arXiv
preprint arXiv:1807.11880 , 2018.
Alexandre d’Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Optimization ,
19(3):1171–1183, 2008.
Olivier Devolder et al. Stochastic first order methods in smooth convex optimization. Technical report,
CORE, 2011.
Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-sizes. The
Annals of Statistics , 44(4):1363–1399, 2016.
Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger convergence
rates for least-squares regression. The Journal of Machine Learning Research , 18(1):3520–3570, 2017.
Paul Doukhan. Mixing. In Mixing, pp. 15–23. Springer, 1994.
Paul Doukhan. Mixing: properties and examples , volume 85. Springer Science & Business Media, 2012.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of machine learning research , 12(7), 2011.
Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, Kevin Scaman, and Hoi-To Wai. Tight high
probability bounds for linear stochastic approximation with fixed stepsize. Advances in Neural Information
Processing Systems , 34:30063–30074, 2021.
Alain Durmus, Eric Moulines, Alexey Naumov, and Sergey Samsonov. Finite-time high-probability bounds
for polyak-ruppert averaged iterates of linear stochastic approximation. arXiv preprint arXiv:2207.04475 ,
2022.
Robert F Engle. Autoregressive conditional heteroscedasticity with estimates of the variance of united
kingdom inflation. Econometrica: Journal of the econometric society , pp. 987–1007, 1982.
Christian Francq and Jean-Michel Zakoian. GARCH models: structure, statistical inference and financial
applications . John Wiley & Sons, 2019.
Sébastien Gadat and Fabien Panloup. Optimal non-asymptotic analysis of the ruppert–polyak averaging
stochastic algorithm. Stochastic Processes and their Applications , 156:312–348, 2023.
DanielGervini. Robustfunctionalestimationusingthemedianandsphericalprincipalcomponents. Biometrika ,
95(3):587–600, 2008.
Antoine Godichon-Baggioni. Estimating the geometric median in hilbert spaces with stochastic gradient
algorithms: Lp and almost sure rates of convergence. Journal of Multivariate Analysis , 146:209–222, 2016.
18Published in Transactions on Machine Learning Research (07/2023)
Antoine Godichon-Baggioni and Bruno Portier. An averaged projected robbins-monro algorithm for estimating
the parameters of a truncated spherical distribution. Electronic Journal of Statistics , 11(1):1890–1927,
2017.
Antoine Godichon-Baggioni and Pierre Tarrago. Non asymptotic analysis of adaptive stochastic gradient
algorithms and applications. arXiv preprint arXiv:2303.01370 , 2023.
Antoine Godichon-Baggioni, Nicklas Werge, and Olivier Wintenberger. Non-asymptotic analysis of stochastic
approximation algorithms for streaming data. ESAIM: PS , 27:482–514, 2023.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning . MIT press, 2016.
Eduard Gorbunov, Marina Danilova, and Alexander Gasnikov. Stochastic optimization with heavy-tailed noise
via accelerated gradient clipping. Advances in Neural Information Processing Systems , 33:15042–15053,
2020a.
Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richtárik. Linearly converging error
compensated sgd. Advances in Neural Information Processing Systems , 33:20889–20900, 2020b.
Robert Gower, Othmane Sebbouh, and Nicolas Loizou. Sgd for structured nonconvex functions: Learning
rates, minibatching and interpolation. In International Conference on Artificial Intelligence and Statistics ,
pp. 1315–1323. PMLR, 2021.
Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richtárik. Sgd:
General analysis and improved rates. In International Conference on Machine Learning , pp. 5200–5209.
PMLR, 2019.
JBS Haldane. Note on the median of a multivariate distribution. Biometrika , 35(3-4):414–417, 1948.
James Douglas Hamilton. Time series analysis . Princeton university press, 2020.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient
descent. In International conference on machine learning , pp. 1225–1234. PMLR, 2016.
Trevor Hastie, Robert Tibshirani, and Jerome H Friedman. The elements of statistical learning: data mining,
inference, and prediction , volume 2. Springer, 2009.
Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends ®in Optimization , 2
(3-4):157–325, 2016.
Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai. Non-asymptotic analysis of biased
stochastic approximation scheme. In Conference on Learning Theory , pp. 1944–1974. PMLR, 2019.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient
methods under the polyak-łojasiewicz condition. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases , pp. 795–811. Springer, 2016.
JHB Kemperman. The median of a finite measure on a banach space. Statistical data analysis based on the
L1-norm and related methods (Neuchâtel, 1987) , pp. 217–230, 1987.
Ahmed Khaled and Peter Richtárik. Better theory for SGD in the nonconvex world. Transactions on Machine
Learning Research , 2023. ISSN 2835-8856.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Krzysztof Kurdyka. On gradients of functions definable in o-minimal structures. In Annales de l’institut
Fourier, volume 48, pp. 769–783, 1998.
H. J. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms and Applications . Springer-
Verlag, 2003.
19Published in Transactions on Machine Learning Research (07/2023)
Stanislaw Lojasiewicz. A topological property of real analytic subsets. Coll. du CNRS, Les équations aux
dérivées partielles , 117(87-89):2, 1963.
Shaocong Ma, Ziyi Chen, Yi Zhou, Kaiyi Ji, and Yingbin Liang. Data sampling affects the complexity of
online sgd over dependent data. In Uncertainty in Artificial Intelligence , pp. 1296–1305. PMLR, 2022.
Abdelkader Mokkadem and Mariane Pelletier. A generalization of the averaging procedure: The use of
two-time-scale algorithms. SIAM Journal on Control and Optimization , 49(4):1523–1543, 2011.
Noboru Murata and Shun-ichi Amari. Statistical analysis of learning dynamics. Signal Processing , 74(1):
3–28, 1999. ISSN 0165-1684.
Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, and Praneeth Netrapalli. Least squares regression
with markovian data: Fundamental limits and algorithms. Advances in neural information processing
systems, 33:16666–16676, 2020.
Ion Necoara, Yu Nesterov, and Francois Glineur. Linear convergence of first order methods for non-strongly
convex optimization. Mathematical Programming , 175(1):69–107, 2019.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation
approach to stochastic programming. SIAM Journal on optimization , 19(4):1574–1609, 2009.
Arkadij Semenovič Nemirovskij and David Borisovich Yudin. Problem complexity and method efficiency in
optimization . Wiley-Interscience, 1983.
Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o
(1/kˆ 2). In Doklady an ussr , volume 269, pp. 543–547, 1983.
Yurii Nesterov et al. Lectures on convex optimization , volume 137. Springer, 2018.
Lam Nguyen, Phuong Ha Nguyen, Marten Dijk, Peter Richtárik, Katya Scheinberg, and Martin Takác. Sgd
and hogwild! convergence without the bounded gradients assumption. In International Conference on
Machine Learning , pp. 3750–3758. PMLR, 2018.
Lam M. Nguyen, Phuong Ha Nguyen, Peter Richtárik, Katya Scheinberg, Martin Takáč, and Marten van
Dijk. New convergence aspects of stochastic gradient algorithms. Journal of Machine Learning Research ,
20(176):1–49, 2019.
Ivan Nourdin. Selected aspects of fractional Brownian motion , volume 4. Springer, 2012.
David Nualart. The Malliavin calculus and related topics , volume 1995. Springer, 2006.
Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal
on control and optimization , 30(4):838–855, 1992.
Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi Matematiki
i Matematicheskoi Fiziki , 3(4):643–653, 1963.
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks , 12(1):145–151,
1999.
Emmanuel Rio. Asymptotic theory of weakly dependent random processes , volume 80. Springer, 2017.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics , pp. 400–407, 1951.
Murray Rosenblatt. A central limit theorem and a strong mixing condition. Proceedings of the National
Academy of Sciences of the United States of America , 42(1):43, 1956.
David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Technical report,
Cornell University Operations Research and Industrial Engineering, 1988.
20Published in Transactions on Machine Learning Research (07/2023)
Mark Schmidt, Nicolas Roux, and Francis Bach. Convergence rates of inexact proximal-gradient methods for
convex optimization. Advances in neural information processing systems , 24, 2011.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms .
Cambridge university press, 2014.
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. Pegasos: Primal estimated
sub-gradient solver for svm. Mathematical programming , 127(1):3–30, 2011.
Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and Trends ®in
Machine Learning , 4(2):107–194, 2012.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
Choon Hui Teo, Alex Smola, SVN Vishwanathan, and Quoc Viet Le. A scalable modular convex solver
for regularized risk minimization. In Proceedings of the 13th ACM SIGKDD international conference on
Knowledge discovery and data mining , pp. 727–736, 2007.
Tijmen Tieleman, Geoffrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of
its recent magnitude. COURSERA: Neural networks for machine learning , 4(2):26–31, 2012.
Endre Weiszfeld and Frank Plastria. On the point for which the sum of the distances to n given points is
minimum. Annals of Operations Research , 167(1):7–41, 2009.
Nicklas Werge and Olivier Wintenberger. Adavol: An adaptive recursive volatility prediction method.
Econometrics and Statistics , 23:19–35, 2022. ISSN 2452-3062.
OlivierWintenberger. Stochasticonlineconvexoptimization; applicationtoprobabilistictimeseriesforecasting.
arXiv preprint arXiv:2102.00729 , 2021.
Lin Xiao. Dual averaging method for regularized stochastic learning and online optimization. Advances in
Neural Information Processing Systems , 22, 2009.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 , 2012.
Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In
Proceedings of the twenty-first international conference on Machine learning , pp. 116, 2004.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings
of the 20th international conference on machine learning (icml-03) , pp. 928–936, 2003.
A Proofs
To begin, we establish recursive relationships for the desired quantities δt=E[∥θt−θ∗∥2]and¯δt=E[∥¯θt−θ∗∥2].
These relationships hold for any values of (γt),(νt),(σt), and (nt). Subsequently, we substitute the specific
functional forms of these parameters, which lead to the results presented in Theorem 1 and Theorem 2.
Before presenting the proofs, it is important to revisit a recurring argument employed to non-asymptotically
boundδtand¯δt:
Proposition 1 (Godichon-Baggioni et al. (2023)) .Suppose (ωt),(αt),(ηt), and (βt)to be some non-negative
sequences satisfying the recursive relation,
ωt≤[1−2λαt+ηtαt]ωt−1+βtαt, (17)
withω0≥0andλ >0. LetCω≥1be such that λαt≤1for allt≥tωwithtω=inf{t≥1 :Cωηt≤λ}.
Then, for (αt)and(ηt)decreasing, we have the upper bound on (ωt)given by
ωt≤τt+1
λmax
t/2≤i≤tβi, (18)
withτt= exp/parenleftig
−λ/summationtextt
i=t/2αi/parenrightig/bracketleftig
exp/parenleftig
Cω/summationtextt
i=1ηiαi/parenrightig/parenleftbig
ω0+1
λmax 1≤i≤tβi/parenrightbig
+/summationtextt/2−1
i=1βiαi/bracketrightig
.
21Published in Transactions on Machine Learning Research (07/2023)
Proposition 1 presents a straightforward method for bounding (ωt)in (17). The resulting bound in (18)
comprises a sub-exponential term τtand a noise term λ−1maxt/2≤i≤tβi. Hence, our primary objective is to
minimize the noise term without compromising the inherent decay of the sub-exponential term. It is worth
noting that the sub-exponential term τtdiminishes exponentially as t→∞.
During our proofs, specific functions will be introduced, resulting in various generalized harmonic numbers
that can be bounded using the integral test for convergence. Additionally, to express our findings in terms
ofNt=/summationtextt
i=1ni, we will utilize the inequality (Nt/2Cρ)1/(1+ρ)≤t≤(2Nt/Cρ)1/(1+ρ), as demonstrated in
Godichon-Baggioni et al. (2023). For the sake of simplicity in notation, we will employ the functions ψx(t)
andψy
x(t), defined as mappings from R+\0toR, given by
ψx(t) =

t1−x/(1−x)ifx<1,
1 + log(t)ifx= 1,
x/(x−1)ifx>1,andψy
x(t) =

t(1−x)/(1+y)/(1−x)ifx<1,
1 + log(t1/(1+y))ifx= 1,
x/(x−1) ifx>1,(19)
withy∈R+such thatψy
x(t) =ψx(t1/(1+y)). Consequently, for any x≥0, we have/summationtextt
i=1i−x≤ψx(t).
Additionally, when considering ψy
x(t)/t, we find that if x <1, thenψy
x(t)/t=O(t−(x+y)/(1+y)). Ifx= 1,
it isO(log(t)t−1). And ifx>1, it becomesO(t−1). Therefore, for any x0,x1,x2,y≥0, we can conclude
thatψy
x0(t)/t=˜O(t−(x0+y)/(1+y)), andψy
x1(t)ψy
x2(t)/t=˜O(t−(x1+x2+y−1)/(1+y)), where ˜O(·)denotes the
suppression of logarithmic factors.
In Lemma 1, we establish an explicit recursive relation for δt, which provides a non-asymptotic bound on the
t-th estimate of (7). We achieve this using classical techniques from stochastic approximations (Benveniste
et al., 2012; Kushner & Yin, 2003).
Proof of Lemma 1. By taking the quadratic norm on (7), expanding it, and taking the expectation, we can
derive the equation
δt=δt−1+γ2
tE[∥∇θft(θt−1)∥2]−2γtE[⟨∇θft(θt−1),θt−1−θ∗⟩], (20)
withδ0≥0. To bound the second term on the right-hand side of (20), we use Assumptions 4-p and 5-p for
p= 2. This allows us to derive the inequality,
E[∥∇θft(θt−1)∥2]≤2E[∥∇θft(θt−1)−∇θft(θ∗)∥2] + 2E[∥∇θft(θ∗)∥2]≤2C2
κδt−1+ 2σ2
t,(21)
using that∥x+y∥p≤2p−1(∥x∥p+∥y∥p). As mentioned in Bottou et al. (2018); Nesterov et al. (2018), (2)
implies that⟨∇θF(θ),θ−θ∗⟩≥µ∥θ−θ∗∥2for allθ∈Θ. Thus, asFisµ-quasi-strongly convex (2) and θt−1
isFt−1-measurable (Assumption 3-p), we can bound the third term on the right-hand side of (20) as follows:
E[⟨∇θft(θt−1),θt−1−θ∗⟩] =E[⟨∇θF(θt−1),θt−1−θ∗⟩] +E[⟨E[∇θft(θt−1)|Ft−1]−∇θF(θt−1),θt−1−θ∗⟩]
≥µδt−1−Dννtδt−1−Bννtδ1
2
t−1, (22)
since
E[⟨E[∇θft(θt−1)|Ft−1]−∇θF(θt−1),θt−1−θ∗⟩]≥−E[∥E[∇θft(θt−1)|Ft−1]−∇θF(θt−1)∥∥θt−1−θ∗∥]
≥−/radicalbig
E[∥E[∇θft(θt−1)|Ft−1]−∇θF(θt−1)∥2]/radicalbig
E[∥θt−1−θ∗∥2]
≥−/radicalig
ν2
t(D2νδt−1+B2ν)/radicalbig
δt−1≥−Dννtδt−1−Bννt/radicalbig
δt−1,
by Jensen’s inequality, Cauchy–Schwarz inequality, Hölder’s inequality, and Assumption 3-p with p= 2.
Hence, by applying the inequalities (21) and (22) to (20), we obtain that
δt≤[1−2µγt+ 2Dννtγt+ 2C2
κγ2
t]δt−1+ 2Bννtγtδ1
2
t−1+ 2σ2
tγ2
t
≤[1−(µ−2Dννt)γt+ 2C2
κγ2
t]δt−1+B2
ν
µν2
tγt+ 2σ2
tγ2
t,
22Published in Transactions on Machine Learning Research (07/2023)
using Young’s inequality6in the second line; 2Bννtγtδ1
2
t−1≤µγtδt−1+B2
νν2
tγt/µ. Bounding the projected
estimate (8) follows from the property that E[∥PΘ(θ)−θ∥2]≤E[∥θ−θ∥2], for allθ∈Rdandθ∗∈Θ, as
mentioned in Zinkevich (2003).
The following corollary directly follows by applying Proposition 1 to the recursive relation for δtderived in
Lemma 1.
Corollary 1. Letδt=E[∥θt−θ∗∥2], where (θt)either follows the recursion in (7)or(8). Suppose
Assumptions 1 and 3-p to 5-p hold for p= 2. Let 1{νt=C}and 1{νt¬C}indicate whether (νt)is constant or
not. Ifµν=µ− 1{νt=C}2Dννt>0, then, for any learning rate (γt), we have
δt≤πt+2B2
ν
µµνmax
t/2≤i≤tν2
i+4
µνmax
t/2≤i≤tσ2
iγi,
with
πt= exp
−µν
2t/summationdisplay
i=t/2γi
/bracketleftigg
exp/parenleftigg
1{νt¬C}2CδDνt/summationdisplay
i=1νiγi/parenrightigg
exp/parenleftigg
2CδC2
κt/summationdisplay
i=1γ2
i/parenrightigg
/parenleftbigg
δ0+2B2
ν
µµνmax
1≤i≤tν2
i+4
µνmax
1≤i≤tσ2
iγi/parenrightbigg
+B2
ν
µt/2−1/summationdisplay
i=1ν2
iγi+ 2t/2−1/summationdisplay
i=1σ2
iγ2
i
.
Proof of Corollary 1. First, we introduce the indicator function for whether (νt)is constant ( =C) or not
(¬C). With this notation, we can rewrite δtfrom Lemma 1 as follows:
δt≤[1−(µν− 1{νt¬C}2Dννt)γt+ 2C2
κγ2
t]δt−1+B2
ν
µν2
tγt+ 2σ2
tγ2
t, (23)
withµν=µ− 1{νt=C}2Dννt>0. Let us consider Cδ=max{1,2C2
κ,(µν/2)2,2 1{νt¬C}Dν}. By doing so, we
can rewrite (23) as follows:
δt≤[1−µνγt+Cδ(νt+γt)γt]δt−1+B2
ν
µν2
tγt+ 2σ2
tγ2
t. (24)
Lettδdenote inf{t:Cδ(νt+γt)≤µν/2}. Then, for any t≥tδ, we haveγt(µν/2)2≤Cδγt≤µν/2, i.e.,
µνγt/2≤1. Thus, the conditions of Proposition 1 are satisfied. Then, applying Proposition 1 to inequality
(24), we can conclude the proof.
Proof of Theorem 1. By substituting the functions γt=Cγnβ
tt−α,νt=n−ν
t,σt=Cσn−σ
t, andnt=⌈Cρtρ⌉≥
Cρtρinto the bound of Corollary 1, we obtain that
δt≤πt+21+2ρνB2
ν
µµνC2νρt2ρν+22+ρ(2σ−β)+αC2
σCγCβ
ρ
µνC2σρtρ(2σ−β)+α(25)
≤πt+2(2+6ρν)/(1+ρ)B2
ν
µµνC2ν/(1+ρ)
ρN2ρν/(1+ρ)
t+2(7+6ρσ)/(1+ρ)C2
σCγ
µνC(2σ−β−α)/(1+ρ)
ρ N(ρ(2σ−β)+α)/(1+ρ)
t, (26)
6Ifa,b,c> 0,p,q> 1such that 1/p+ 1/q= 1, thenab≤apcp/p+bq/qcq.
23Published in Transactions on Machine Learning Research (07/2023)
withµν=µ− 1{ρ=0}2DνC−ν
ρ>0, and
πt≤exp/parenleftigg
−µνCγCβ
ρt1+ρβ−α
22/parenrightigg/bracketleftigg
exp/parenleftigg
1{ρ̸=0}2CδDνCγCβ
ρψα−ρ(β−ν)(t)
Cνρ/parenrightigg
exp/parenleftigg
4(α−ρβ)CδC2
κC2
γC2β
ρ
(2α−2ρβ−1)/parenrightigg
/parenleftigg
δ0+2B2
ν
µµνC2νρ+4C2
σCγCβ
ρ
µνC2σρ/parenrightigg
+B2
νCγCβ
ρψα−ρ(β−2ν)(t/2)
µC2νρ+4(α−ρ(β−σ))C2
σC2
γC2β
ρ
(2α−2ρ(β−σ)−1)C2σρ/bracketrightigg
≤exp/parenleftigg
−µCγN(1+ρβ−α)/(1+ρ)
t
2(3+ρ(2+β)−α)/(1+ρ)C(1−β−α)/(1+ρ)
ρ/parenrightigg/bracketleftigg
exp/parenleftigg
1{ρ̸=0}2CδDνCγCβ
ρψρ
α−ρ(β−ν)(2Nt/Cρ)
Cνρ/parenrightigg
exp/parenleftigg
4(α−ρβ)CδC2
κC2
γC2β
ρ
(2α−2ρβ−1)/parenrightigg/parenleftigg
δ0+2B2
ν
µµνC2νρ+4C2
σCγCβ
ρ
µνC2σρ/parenrightigg
+B2
νCγCβ
ρψρ
α−ρ(β−2ν)(Nt/Cρ)
µC2νρ+4(α−ρ(β−σ))C2
σC2
γC2β
ρ
(2α−2ρ(β−σ)−1)C2σρ/bracketrightigg
, (27)
with help of an integral test for convergence7, the functions ψx(t)andψy
x(t)from (19), and by use of
(Nt/2Cρ)1/(1+ρ)≤t≤(2Nt/Cρ)1/(1+ρ).
Next, our focus turns to the analysis of the fourth-order rate, denoted as ∆t=E[∥θt−θ∗∥4], for the
recursive estimates given by equations (7) and (8). Similar to the approach in Corollary 1, we commence a
comprehensive investigation of the general case with (γt),(νt),(σt), and (nt).
Lemma 2. Let∆t=E[∥θt−θ∗∥4], where (θt)either follows the recursion in (7)or(8). Suppose Assumptions 1
and 3-p to 5-p hold for p= 4. Let 1{νt=C}and 1{νt¬C}indicate whether (νt)is constant or not. If
µ′
ν=µ− 1{νt=C}2D4
νν4
t/µ3>0, then for any learning rate (γt), we have
∆t≤Πt+4B4
ν
µ3µ′νmax
t/2≤i≤tν4
i+1024
µµ′νmax
t/2≤i≤tσ4
iγ2
i+96
µ′νmax
t/2≤i≤tσ4
iγ3
i,
with Πtgiven as
exp
−µ′
ν
4t/summationdisplay
i=t/2γi
/bracketleftigg
exp/parenleftigg
1{νt¬C}C∆D4
ν
µ3t/summationdisplay
i=1ν4
iγi/parenrightigg
exp/parenleftigg
256C∆C4
κ
µt/summationdisplay
i=1γ3
i/parenrightigg
exp/parenleftigg
24C∆C4
κt/summationdisplay
i=1γ4
i/parenrightigg
/parenleftbigg
∆0+4B4
ν
µ3µ′νmax
1≤i≤tν4
i+1024
µµ′νmax
1≤i≤tσ4
iγ2
i+96
µ′νmax
1≤i≤tσ4
iγ3
i/parenrightbigg
+B4
ν
µ3t/2−1/summationdisplay
i=1ν4
iγi+256
µt/2−1/summationdisplay
i=1σ4
iγ3
i+ 24t/2−1/summationdisplay
i=1σ4
iγ4
i
.
Proof of Lemma 2. The derivation of the recursive step sequence for the fourth-order moment, denoted by
∆t, in (7), follows a similar methodology as for the second-order moment in Corollary 1. By applying the
same approach used to derive (20), we can take the quadratic norm of (7), expand the norm, square both
sides of the equation, and take the conditional expectation on both sides. This leads us to the following
expression:
∆t=∆t−1+γ4
tE[∥∇θft(θt−1)∥4] + 4γ2
tE[⟨∇θft(θt−1),θt−1−θ∗⟩2] + 2γ2
tE[∥θt−1−θ∗∥2∥∇θft(θt−1)∥2]
−4γtE[∥θt−1−θ∗∥2⟨∇θft(θt−1),θt−1−θ∗⟩]−4γ3
tE[∥∇θft(θt−1)∥2⟨∇θft(θt−1),θt−1−θ∗⟩]
≤∆t−1+γ4
tE[∥∇θft(θt−1)∥4] + 6γ2
tE[∥θt−1−θ∗∥2∥∇θft(θt−1)∥2]
−4γtE[∥θt−1−θ∗∥2⟨∇θft(θt−1),θt−1−θ∗⟩] + 4γ3
tE[∥θt−1−θ∗∥∥∇θft(θt−1)∥3],
where we have made use of Cauchy-Schwarz inequality. Next, we can utilize Young’s inequality
to simplify the terms. By applying Young’s inequality, we have: 4γ3
t∥θt−1−θ∗∥∥∇θft(θt−1)∥3≤
7/summationtextt
i=1i2ρ(β−σ)−2α≤(2α−2ρ(β−σ))/(2α−2ρ(β−σ)−1)asν >0,σ∈[0,1/2],ρ∈[0,1),β∈[0,1], andα−ρβ∈(1/2,1).
24Published in Transactions on Machine Learning Research (07/2023)
2γ4
t∥∇θft(θt−1)∥4+ 2γ2
t∥θt−1−θ∗∥2∥∇θft(θt−1)∥2and 8γ2
t∥θt−1−θ∗∥2∥∇θft(θt−1)∥2≤(µγt/2)∥θt−1−
θ∗∥4+ 32µ−1γ3
t∥∇θft(θt−1)∥4. These inequalities allow us to obtain the simplified expression:
∆t≤[1 +µγt/2]∆t−1+ 3γ4
tE[∥∇θft(θt−1)∥4] + 32µ−1γ3
tE[∥∇θft(θt−1)∥4]
−4γtE[∥θt−1−θ∗∥2⟨∇θft(θt−1),θt−1−θ∗⟩].
In order to bound the fourth-order term E[∥∇θft(θt−1)∥4], we can utilize several assumptions. Firstly, we
make use of the Lipschitz continuity of ∇θft(as stated in Assumption 4-p). Additionally, we consider
the bounds on∇θft(θ∗)given in Assumption 5-p, and the fact that θt−1isFt−1-measurable (as stated in
Assumption 3-p). Combining these assumptions, we can show that E[∥∇θft(θt−1)∥4]≤8C4
κ∆t−1+ 8σ4
t. Thus,
∆t≤[1 +µγt/2 + 256µ−1C4
κγ3
t+ 24C4
κγ4
t]∆t−1+ 256µ−1σ4
tγ3
t+ 24σ4
tγ4
t
−4γtE[∥θt−1−θ∗∥2⟨∇θft(θt−1),θt−1−θ∗⟩]. (28)
Next, by employing similar arguments as in the proof of Corollary 1, along with Young’s inequality and
Assumption 3-p (with p= 4), we have that
4γtE[∥θt−1−θ∗∥2⟨E[∇θft(θt−1)|Ft−1]−∇θF(θt−1),θt−1−θ∗⟩]
≥−4γtE[∥θt−1−θ∗∥3∥E[∇θft(θt−1)|Ft−1]−∇θF(θt−1)∥]
≥−3µγt∆t−1−µ−3γtE[∥E[∇θft(θt−1)|Ft−1]−∇θF(θt−1)∥4]
≥−3µγt∆t−1−µ−3γtD4
νν4
t∆t−1−µ−3γtB4
νν4
t,
Hence, utilizing this inequality, we can bound the last term of (28) as follows,
4γtE[∥θt−1−θ∗∥2⟨∇θft(θt−1),θt−1−θ∗⟩] = 4γtE[∥θt−1−θ∗∥2⟨E[∇θft(θt−1)|Ft−1],θt−1−θ∗⟩]
= 4γtE[∥θt−1−θ∗∥2⟨∇θF(θt−1),θt−1−θ∗⟩] + 4γtE[∥θt−1−θ∗∥2⟨E[∇θft(θt−1)|Ft−1]−∇θF(θt−1),θt−1−θ∗⟩]
≥µγt∆t−1−µ−3γtD4
νν4
t∆t−1−µ−3γtB4
νν4
t.
To summarize, by inserting this into (28) and incorporating the indicator function that determines whether
(νt)is constant ( =C) or not (¬C), we obtain the following inequality:
∆t≤/bracketleftbigg
1−/parenleftbiggµν
2−1{νt¬C}D4
νν4
t
µ3/parenrightbigg
γt+256C4
κγ3
t
µ+ 24C4
κγ4
t/bracketrightbigg
∆t−1+B4
νν4
tγt
µ3+256σ4
tγ3
t
µ+ 24σ4
tγ4
t,(29)
withµ′
ν=µ− 1{νt=C}2D4
νν4
t/µ3>0. Note that µνfrom Corollary 1 is lower bounded by µ′
ν, and it is strictly
lower bounded when (νt)is constant, i.e., µν>µ′
ν>0. LetC∆≥1satisfy the conditions of Proposition 1.
The constant C∆is chosen such that C∆( 1{νt¬C}D4
νν4
t/µ3+ 256C4
κγ2
t/µ+ 24C4
κγ3
t)≤µ′
ν/2, which implies
µ′
νγt/2≤1. This condition is possible since the sequence (νt)is non-increasing and (γt)is decreasing. At
last, by applying Proposition 1 to (29), we obtain the desired bound for ∆t.
Corollary 2. Let∆t=E[∥θt−θ∗∥4], where (θt)either follows the recursion in (7)or(8). Suppose
Assumptions 1 and 3-p to 5-p hold for p= 4. Ifµ′
ν=µ− 1{ρ=0}2D4
ν/µ3C4ν
ρ>0, then forα−ρβ∈(1/2,1),
we have
∆t≤Πt+22+4ρνB4
ν
µ3µ′νC4νρt4ρν+22ρ(2σ−β)+2α(210µ−1+ 27CγCβ
ρ)C4
σC2
γC2β
ρ
µ′νC4σρt2ρ(2σ−β)+2α, (30)
with Πtgiven in (31)such that Πt=O(exp(−N(1+ρβ−α)/(1+ρ)
t )).
Proof of Corollary 2. By substituting the functions γt=Cγnβ
tt−α,νt=n−ν
t,σt=Cσn−σ
t, andnt=Cρtρ
into the bound of Lemma 2, and utilizing the inequality γ3
t≤CγCβ
ργ2
tdue toα−ρβ∈(1/2,1), we obtain
25Published in Transactions on Machine Learning Research (07/2023)
(30). In this inequality, we have µ′
ν=µ− 1{ρ=0}2D4
ν/µ3C4ν
ρ>0. Furthermore, Πtcan be bounded as follows:
Πt≤exp
−µ′
νCγCβ
ρ
4t/summationdisplay
i=t/2iρβ−α
/bracketleftigg
exp/parenleftigg
1{ρ̸=0}C∆D4
νCγCβ
ρ
µ3C4νρt/summationdisplay
i=1iρ(β−4ν)−α/parenrightigg
exp/parenleftigg
28C∆C4
κC3
γC3β
ρ
µt/summationdisplay
i=1i3ρβ−3α/parenrightigg
exp/parenleftigg
24C∆C4
κC4
γC4β
ρt/summationdisplay
i=1i4ρβ−4α/parenrightigg
/parenleftigg
∆0+4B4
ν
µ3µ′νC4νρ+1024C4
σC2
γC2β
ρ
µµ′νC4σρ+96C4
σC3
γC3β
ρ
µ′νC4σρ/parenrightigg
+B4
νCγCβ
ρ
µ3C4νρt/2−1/summationdisplay
i=1iρ(β−4ν)−α
+256C4
σC3
γC3β
ρ
µC4σρt/2−1/summationdisplay
i=1iρ(3β−4σ)−3α+24C4
σC4
γC4β
ρ
C4σρt/2−1/summationdisplay
i=1i4ρ(β−σ)−4α

≤exp/parenleftigg
−µ′
νCγCβ
ρt1+ρβ−α
23/parenrightigg/bracketleftigg
exp/parenleftigg
1{ρ̸=0}C∆D4
νCγCβ
ρψ0
α−ρ(β−4ν)(t)
µ3C4νρ/parenrightigg
exp/parenleftigg
210C∆C4
κC3
γC3β
ρ
µ/parenrightigg
exp/parenleftbig
26C∆C4
κC4
γC4β
ρ/parenrightbig/parenleftigg
∆0+22B4
ν
µ3µ′νC4νρ+210C4
σC2
γC2β
ρ
µµ′νC4σρ+27C4
σC3
γC3β
ρ
µ′νC4σρ/parenrightigg
+B4
νCγCβ
ρψ0
α−ρ(β−4ν)(t/2)
µ3C4νρ+210C4
σC3
γC3β
ρ
µC4σρ+26C4
σC4
γC4β
ρ
C4σρ/bracketrightigg
, (31)
with help of the integral test for convergence.8
Lemma 3. Let¯δt=E[∥¯θt−θ∗∥2]with ¯θngiven by (9), where (θt)either follows the recursion in (7)or(8).
Suppose Assumptions 1 to 6 hold for p= 4. In addition, Assumption 7 must hold true if (θt)follows the
recursion in (8), which is indicated by 1{DΘ<∞}. Then, for any learning rate (γt), we have
¯δ1/2
t≤Λ1/2
Nt/parenleftiggt/summationdisplay
i=1n2(1−σ)
i/parenrightigg1/2
+C′1/2
σ
µNt/parenleftiggt/summationdisplay
i=1n2(1−σ−σ′)
i/parenrightigg1/2
+21/2B1/2
ν
µNt
t/summationdisplay
j=2/parenleftigg
njνjj−1/summationdisplay
i=1niσi/parenrightigg
1/2
+1
µNtt−1/summationdisplay
i=1δ1/2
i/vextendsingle/vextendsingle/vextendsingle/vextendsingleni+1
γi+1−ni
γi/vextendsingle/vextendsingle/vextendsingle/vextendsingle+nt
µγtNtδ1/2
t+n1
µNt/parenleftbigg1
γ1+ 21/2(C∇+Cκ)/parenrightbigg
δ1/2
0
+21/2(C2
∇+C2
κ)1/2
µNt/parenleftiggt−1/summationdisplay
i=1n2
i+1δi/parenrightigg1/2
+C′′
∇
µNtt−1/summationdisplay
i=0ni+1∆1/2
i,
+23/4(C2
∇+C2
κ)1/2
µNt
t−1/summationdisplay
j=1/parenleftigg
(Dνδ1/2
j+ 21/2Bν)nj+1νj+1j−1/summationdisplay
i=0ni+1δ1/2
i/parenrightigg
1/2
,
with Λ = Tr(∇2
θF(θ∗)−1Σ∇2
θF(θ∗)−1)andC′′
∇=C′
∇/2 + 1{DΘ<∞}2GΘ/D2
Θ.
Proof of Lemma 3. The proof is presented in two parts. In the first part, we consider the case where (θt)
follows the recursion given by (7). In the second part, we examine the case of (8). Let’s assume that (θt)is
obtained using the recursion in (7). To proceed, we apply the observation made by Polyak & Juditsky (1992),
which observe that
∇2
θF(θ∗)(θt−1−θ∗) =−∇θft(θ∗) +∇θft(θt−1)−[∇θft(θt−1)−∇θft(θ∗)−∇θF(θt−1)]
−[∇θF(θt−1)−∇2
θF(θ∗)(θt−1−θ∗)],
8/summationtextt
i=1i3ρβ−3α≤3<22and/summationtextt
i=1i4ρ(β−x)−4α≤2for anyx≥0asα−ρβ∈(1/2,1).
26Published in Transactions on Machine Learning Research (07/2023)
where∇2
θF(θ∗)is invertible with lowest eigenvalue greater than µ, i.e.,∇2
θF(θ∗)≥µId. By summing the
individual parts, taking the quadratic norm and expectation, and applying Minkowski’s inequality, we obtain
the following inequality:
/parenleftig
E/bracketleftig/vextenddouble/vextenddouble¯θt−θ∗/vextenddouble/vextenddouble2/bracketrightig/parenrightig1
2≤
E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇2
θF(θ∗)−11
Ntt/summationdisplay
i=1ni∇θfi(θ∗)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

1
2
+
E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇2
θF(θ∗)−11
Ntt/summationdisplay
i=1ni∇θfi(θi−1)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

1
2
+
E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇2
θF(θ∗)−11
Ntt/summationdisplay
i=1ni[∇θfi(θi−1)−∇θfi(θ∗)−∇θF(θi−1)]/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

1
2
+
E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇2
θF(θ∗)−11
Ntt/summationdisplay
i=1ni/bracketleftbig
∇θF(θi−1)−∇2
θF(θ∗) (θi−1−θ∗)/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

1
2
.(32)
First term of (32): As (∇θft(θ∗))is a square-integrable sequences on Rd(Assumption 3-p), we have
E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇2
θF(θ∗)−11
Ntt/summationdisplay
i=1ni∇θfi(θ∗)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=1
N2
tt/summationdisplay
i=1n2
iE/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble∇2
θF(θ∗)−1∇θfi(θ∗)/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
+2
N2
t/summationdisplay
1≤i<j≤tninjE/bracketleftig/angbracketleftig
∇2
θF(θ∗)−1∇θfi(θ∗),∇2
θF(θ∗)−1∇θfj(θ∗)/angbracketrightig/bracketrightig
,
Here, the first term can be bounded using Assumption 6, as follows:
1
N2
tt/summationdisplay
i=1n2
iE/bracketleftbigg/vextenddouble/vextenddouble/vextenddouble∇2
θF(θ∗)−1∇θfi(θ∗)/vextenddouble/vextenddouble/vextenddouble2/bracketrightbigg
≤Λ
N2
tt/summationdisplay
i=1n2(1−σ)
i +C′
σ
µ2N2
tt/summationdisplay
i=1n2(1−σ−σ′)
i,
where Λdenotes Tr[∇2
θF(θ∗)−1Σ∇2
θF(θ∗)−1]. For the second term, we use Cauchy-Schwarz inequality,
Hölder’s inequality, and Assumptions 3-p and 5-p to show that
2
N2
t/summationdisplay
1≤i<j≤tninjE/bracketleftig/angbracketleftig
∇2
θF(θ∗)−1∇θfi(θ∗),∇2
θF(θ∗)−1∇θfj(θ∗)/angbracketrightig/bracketrightig
≤2
µ2N2
t/summationdisplay
1≤i<j≤tninjE[⟨∇θfi(θ∗),∇θfj(θ∗)−∇θF(θ∗)⟩]
≤2
µ2N2
t/summationdisplay
1≤i<j≤tninjE[∥∇θfi(θ∗)∥∥[E[∇θfj(θ∗)|Fj−1]−∇θF(θ∗)]∥]
≤2
µ2N2
t/summationdisplay
1≤i<j≤tninj/radicalbigg
E/bracketleftig
∥∇θfi(θ∗)∥2/bracketrightig/radicalbigg
E/bracketleftig
∥[E[∇θfj(θ∗)|Fj−1]−∇θF(θ∗)]∥2/bracketrightig
≤2Bν
µ2N2
t/summationdisplay
1≤i<j≤tninjσiνj=2Bν
µ2N2
tt/summationdisplay
j=2/parenleftigg
njνjj−1/summationdisplay
i=1niσi/parenrightigg
.
27Published in Transactions on Machine Learning Research (07/2023)
Thus, combining these finding gives us

E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇2
θF(θ∗)−11
Ntt/summationdisplay
i=1ni∇θfi(θ∗)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

1
2
≤Λ1
2
Nt/parenleftiggt/summationdisplay
i=1n2(1−σ)
i/parenrightigg1
2
+C′1/2
σ
µN1/2
t/parenleftiggt/summationdisplay
i=1n2(1−σ−σ′)
i/parenrightigg1
2
+21/2B1/2
ν
µNt
t/summationdisplay
j=2/parenleftigg
njνjj−1/summationdisplay
i=1niσi/parenrightigg
1
2
. (33)
Second term of (32): First, we use that1
Nt/summationtextt
i=1ni∇θfi(θi−1) =1
Nt/summationtextt
i=1ni
γi(θi−1−θi) =1
Nt/summationtextt−1
i=1(θi−
θ∗)(ni+1
γi+1−ni
γi)−1
Nt(θt−θ∗)nt
γt+1
Nt(θ0−θ∗)n1
γ1, which leads to an upper bound on normed quantity
∥∇2
θF(θ∗)−11
Nt/summationtextt
i=1ni∇θfi(θi−1)∥given by
1
µNtt−1/summationdisplay
i=1∥θi−θ∗∥/vextendsingle/vextendsingle/vextendsingle/vextendsingleni+1
γi+1−ni
γi/vextendsingle/vextendsingle/vextendsingle/vextendsingle+1
µNt∥θt−θ∗∥nt
γt+1
µNt∥θ0−θ∗∥n1
γ1.
Rewriting this in terms of δt=E[∥θt−θ∗∥2], gives us a bound for second term of (32);
1
µNtt−1/summationdisplay
i=1δ1
2
i/vextendsingle/vextendsingle/vextendsingle/vextendsingleni+1
γi+1−ni
γi/vextendsingle/vextendsingle/vextendsingle/vextendsingle+nt
µγtNtδ1
2
t+n1
µγ1Ntδ1
2
0. (34)
Third term of (32): Here, we use that E[∥∇2
θF(θ∗)−11
Nt/summationtextt
i=1ni[∇θfi(θi−1)−∇θfi(θ∗)−∇θF(θi−1)]∥2]can
be derived as
1
µ2N2
t/bracketleftiggt/summationdisplay
i=1n2
iE[∥∇θfi(θi−1)−∇θfi(θ∗)−∇θF(θi−1)∥2]
+ 2t/summationdisplay
i<jninjE[⟨∇θfi(θi−1)−∇θfi(θ∗)−∇θF(θi−1),∇θfj(θj−1)−∇θfj(θ∗)−∇θF(θj−1)⟩]
.
Next, we use Cauchy-Schwarz inequality, Assumption 4-p and (3) to show that
t/summationdisplay
i=1n2
iE[∥∇θfi(θi−1)−∇θfi(θ∗)−∇θF(θi−1)∥2]≤2(C2
∇+C2
κ)t/summationdisplay
i=1n2
iδi−1.
Similarly, for the other term, we note that
E[⟨∇θfi(θi−1)−∇θfi(θ∗)−∇θF(θi−1),∇θfj(θj−1)−∇θfj(θ∗)−∇θF(θj−1)⟩]
≤/radicalbig
E[∥∇θfi(θi−1)−∇θfi(θ∗)−[∇θF(θi−1)−∇θF(θ∗)]∥2]
/radicalig
E[∥E[∇θfj(θj−1)|Fj−1]−∇θF(θj−1)−[E[∇θfj(θ∗)|Fj−1]−∇θF(θ∗)]∥2]
≤/radicalbig
2E[∥∇θfi(θi−1)−∇θfi(θ∗)∥2] + 2E[∥∇θF(θi−1)−∇θF(θ∗)∥2]
/radicalig
2E[∥E[∇θfj(θj−1)|Fj−1]−∇θF(θj−1)∥2] + 2E[∥E[∇θfj(θ∗)|Fj−1]−∇θF(θ∗)∥2]
≤/radicalig
2(C2κ+C2
∇)δi−1/radicalig
2D2νν2
jδj−1+ 4B2νν2
j
≤21/2(C2
κ+C2
∇)1/2δ1/2
i−1(Dννjδ1/2
j−1+ 21/2Bννj),
28Published in Transactions on Machine Learning Research (07/2023)
usingFi−1⊂Fj−1sincei<j, Cauchy–Schwarz inequality, Hölder’s inequality, ∥a+b∥p≤2p−1(∥a∥p+∥b∥p)
withp∈N, Assumptions 3-p and 4-p, and (3). Thus, the third term of (32) can be upper bounded by
21/2(C2
κ+C2
∇)1/2
µNt/parenleftiggt/summationdisplay
i=1n2
iδi−1/parenrightigg1/2
+23/4(C2
∇+C2
κ)1/2
µNt
t/summationdisplay
j=2/parenleftigg
(Dνδ1/2
j−1+ 21/2Bν)njνjj−1/summationdisplay
i=1niδ1/2
i−1/parenrightigg
1/2
.
(35)
Fourth term of (32): Here, we use that (4) implies ∀θ,∥∇θF(θ)−∇2
θF(θ∗)(θ−θ∗)∥≤C′
∇∥θ−θ∗∥2/2
(Nesterov et al., 2018), which gives the upper boundC′
∇
2µNt/summationtextt
i=1ni∆1/2
i−1using the definition ∆t=E[∥θt−θ∗∥4].
Combining the terms (33) to (35) into (32), together with shifting the indices and collecting the δ0terms,
gives use the desired bound for when (θt)follows (7).
Now, assume that (θt)is derived from the recursion in (8). As above, we follow the steps of Polyak
& Juditsky (1992), in which, we can rewrite (8) to1
γt(θt−1−θt) =∇θft(θt−1)−1
γtΩt, where Ωt=
PΘ(θt−1−γt∇θft(θt−1))−(θt−1−γt∇θft(θt−1)). Thus, summing the parts, taking the norm and expectation,
and using the Minkowski’s inequality, yields the same terms as in (32), but with an additional term regarding
Ωt, namely

E
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble∇2
θF(θ∗)−11
Ntt/summationdisplay
i=1ni
γiΩi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2

1
2
≤1
µNtt/summationdisplay
i=1ni
γi/radicalbigg
E/bracketleftig
∥Ωi∥21{θi−1−γi∇θfi(θi−1)/∈Θ}/bracketrightig
,(36)
using (Godichon-Baggioni, 2016, Lemma 4.3). Next, we note that
∥Ωt∥2=∥PΘ(θt−1−γt∇θft(θt−1))−θt−1+γt∇θft(θt−1)∥2
≤2∥PΘ(θt−1−γt∇θft(θt−1))−θt−1∥2+ 2γ2
t∥∇θft(θt−1)∥2
=2∥PΘ(θt−1−γt∇θft(θt−1))−PΘ(θt−1)∥2+ 2γ2
t∥∇θft(θt−1)∥2
≤2∥θt−1−γt∇θft(θt−1)−θt−1∥2+ 2γ2
t∥∇θft(θt−1)∥2≤4γ2
tG2
Θ,
asPΘis Lipschitz and ∥∇θft(θ)∥2≤G2
Θfor anyθ∈Θ. This means that the inner expectation of (36),
E[∥Ωt∥21{θt−1−γt∇θft(θt−1)/∈Θ}] = 4γ2
tG2
ΘP[θt−1−γt∇θft(θt−1)/∈Θ]. Moreover, as in (Godichon-Baggioni
& Portier, 2017, Theorem 4.2) with use of Lemma 2, we know that P[θt−1−γt∇θft(θt−1)/∈Θ]≤∆t/D4
Θ,
whereDΘ= infθ∈∂Θ∥θ−θ∗∥with∂Θdenoting the frontier of Θ. Thus, (36) can then be bounded by
1
µNtt/summationdisplay
i=1ni
γi/radicalbigg
E/bracketleftig
∥Ωi∥21{θi−1−γi∇θfi(θi−1)/∈Θ}/bracketrightig
≤2GΘ
µD2
ΘNtt/summationdisplay
i=1ni+1∆1/2
i,
since the sequence (nt)is either constant or increasing, meaning ∀t,nt/nt+1≤1. At last, let C′′
∇=
C′
∇/2 + 1{DΘ<∞}2GΘ/D2
Θindicate whether (θt)follows (8) or not.
Proof of Theorem 2. This result can be obtained by simplifying and bounding each term of Lemma 3, using
the bounds provided by Theorem 1 and Lemma 2. By inserting the functions γt=Cγnβ
tt−α,νt=n−ν
t,
29Published in Transactions on Machine Learning Research (07/2023)
σt=Cσn−σ
t, andnt=Cρtρinto the bound of Lemma 3, we obtain that
¯δ1/2
t≤Λ1/2
N1/2
t1{σ=1/2}+Λ1/2C1−σ
ρ
Nt/parenleftiggt/summationdisplay
i=1i2ρ(1−σ)/parenrightigg1/2
1{σ̸=1/2}+C′1/2
σC1−σ−σ′
ρ
µNt/parenleftiggt/summationdisplay
i=1i2ρ(1−σ−σ′)/parenrightigg1/2
+(ρ(1−β) +α)Cρ
µCγCβ
ρNtt−1/summationdisplay
i=1iρ(1−β)+α−1δ1/2
i+21/2B1/2
νC1/2
σCρ
µC(σ+ν)/2
ρNt
t/summationdisplay
j=2/parenleftigg
jρ(1−ν)j−1/summationdisplay
i=1iρ(1−σ)/parenrightigg
1/2
+Cρtρ(1−β)+α
µCγCβ
ρNtδ1/2
t+Cρ
µNt/parenleftigg
1
CγCβ
ρ+ 21/2(Cκ+C∇)/parenrightigg
δ1/2
0+21/2+ρ(C2
κ+C2
∇)1/2Cρ
µNt/parenleftiggt−1/summationdisplay
i=1i2ρδi/parenrightigg1/2
+2ρC′′
∇Cρ
µNtt−1/summationdisplay
i=0iρ∆1/2
i+23/4+ρ(2−ν)/2(C2
∇+C2
κ)1/2Cρ
µCν/2
ρNt
t−1/summationdisplay
j=1/parenleftigg
(Dνδ1/2
j+ 21/2Bν)jρ(1−ν)j−1/summationdisplay
i=1iρδ1/2
i/parenrightigg
1/2
,
usingni+1/ni≤2ρand that|ni+1/γi+1−ni/γi|≤(ρ(1−β)+α)C1−β
ρ/Cγi1−ρ(1−β)−αasρ(1−β)+α≤1−ρ
withρ∈[0,1). Next, asσ∈[0,1/2]andσ′∈(0,1/2], we have/summationtextt
i=1i2ρ(1−σ−σ′)≤t1+2ρ(1−σ−σ′)/(1 + 2ρ(1−
σ−σ′)), wheret≤(2Nt/Cρ)1/(1+ρ). Similarly, as ν∈(0,∞), we have that
t−1/summationdisplay
j=2/parenleftigg
jρ(1−ν)j−1/summationdisplay
i=1iρ(1−σ)/parenrightigg
≤t−1/summationdisplay
j=1jρ(1−ν)t−1/summationdisplay
i=1iρ(1−σ)≤ψρ(ν−1)(t)ψρ(σ−1)(t)
≤ψρ
ρ(ν−1)(2Nt/Cρ)ψρ
ρ(σ−1)(2Nt/Cρ),
using theψ-function defined in (19). Hence,/radicalig
ψρ
ρ(σ−1)(2Nt/Cρ)ψρ
ρ(ν−1)(2Nt/Cρ)/Ntis˜O(N−ρ(σ+ν)/2(1+ρ)
t ).
From (25) we know that δt≤Dδ/tδwith
Dδ= sup
t∈Nπttδ+21+2ρνB2
ν
µµνC2νρ+22+ρ(2σ−β)+αC2
σCγCβ
ρ
µνC2σρ,
andδ= 1{Bν=0}(ρ(2σ−β) +α) + 1{Bν̸=0}min{ρ(2σ−β) +α,2ρν}, yielding
t−1/summationdisplay
j=1/parenleftigg
(Dνδ1/2
j+ 21/2Bν)jρ(1−ν)j−1/summationdisplay
i=1iρδ1/2
i/parenrightigg
≤D1/2
δt−1/summationdisplay
j=1/parenleftig
(DνD1/2
δj−δ/2+ 21/2Bν)jρ(1−ν)ψδ/2−ρ(t)/parenrightig
≤DνDδψδ/2−ρ(t)ψδ/2+ρ(ν−1)(t) + 21/2BνD1/2
δψδ/2−ρ(t)ψρ(ν−1)(t)
≤DνDδψρ
δ/2−ρ(2Nt/Cρ)ψρ
δ/2+ρ(ν−1)(2Nt/Cρ) + 21/2BνD1/2
δψρ
δ/2−ρ(2Nt/Cρ)ψρ
ρ(ν−1)(2Nt/Cρ),
ifδ/2−ρ≥0. Hence,/radicalig
ψρ
δ/2−ρ(2Nt/Cρ)ψρ
δ/2+ρ(ν−1)(2Nt/Cρ)/Ntis˜O(N−(δ+ρν)/2(1+ρ)
t ), and
/radicalig
ψρ
δ/2−ρ(2Nt/Cρ)ψρ
ρ(ν−1)(2Nt/Cρ)/Ntis˜O(N−(δ/2+ρν)/2(1+ρ)
t ). Next, we define ¯πt=/summationtextt
i=1i2πi≥/summationtextt
i=1πi
such thatπt≤t−1/summationtextt
i=1πi≤t−1¯πt≤t−1¯π∞sinceπtis decreasing. Similarly, let ¯Πt=/summationtextt
i=1iρΠi. Both ¯πt
and¯Πtconvergences to some finite constant depending on the model’s parameters. With use of these notions,
30Published in Transactions on Machine Learning Research (07/2023)
we have
¯δ1/2
t≤Λ1/2
N1/2
t1{σ=1/2}+21/2Λ1/2C(1−2σ)/2(1+ρ)
ρ
N(1+2ρσ)/2(1+ρ)
t1{σ̸=1/2}+21/2C′1/2
σC(1−2(σ+σ′))/2(1+ρ)
ρ
µN(1+2ρ(σ+σ′))/2(1+ρ)
t
+22+(7+2ρ(1+σ))/2(1+ρ)CσC(2−2σ−β−α)/2(1+ρ)
ρ
µµ1/2
νC1/2
γN(2+ρ(β+2σ)−α)/2(1+ρ)
t+ΓCρ
µNt+2(2+ρ)/(1+ρ)C(2+β−α)/(1+ρ)
ρ ¯π∞
µCγN(2+ρβ−α)/(1+ρ)
t
+2(1+ρ(1+2σ−β)+α)/(1+ρ)(25µ−1/2+ 24C1/2
γCβ/2
ρ)C′′
∇C2
σCγ
µ/radicalbig
µ′νC(1−2ρσ−α)/(1+ρ)
ρ N(ρ(2σ−β)+α)/(1+ρ)
t+ 1{Bν̸=0}Ψt
+2(5/2+ρ(5−2σ))/2(1+ρ)Dκ
∇CσC1/2
γC(1+β−2σ+α)/2(1+ρ)
ρ
µµ1/2
νN(1+ρ(2σ−β)+α)/(2(1+ρ))
t
+23/4+ρ(2−ν)/2/radicalbigDκ
∇D1/2
νD1/2
δCρ/radicalig
ψρ
δ/2−ρ(2Nt/Cρ)ψρ
δ/2+ρ(ν−1)(2Nt/Cρ)
µCν/2
ρNt,
asα−ρβ∈(1/2,1), whereµ′
ν=µ− 1{ρ=0}2D4
ν/µ3C4ν
ρ,Dκ
∇=C∇+Cκ,C′′
∇=C′
∇+ 1{DΘ<∞}2GΘ/D2
Θ,
Γ = 2 ¯π∞/CγCβ
ρ+ (1/CγCβ
ρ+ 21/2Dκ
∇)δ1/2
0+ 21/2+ρDκ
∇¯π1/2
∞+ 2ρC′′
∇¯Π∞,δ= 1{Bν=0}(ρ(2σ−β) +α) +
1{Bν̸=0}min{ρ(2σ−β) +α,2ρν}, and Ψtgiven as
21/2B1/2
νC1/2
σCρ/radicalig
ψρ
ρ(σ−1)(2Nt/Cρ)ψρ
ρ(ν−1)(2Nt/Cρ)
µC(σ+ν)/2
ρNt+23(1+ρν)BνC(1−β−ν−α)/(1+ρ)
ρ
µ3/2µ1/2
νCγN(1+ρ(β+ν)−α)/(1+ρ)
t
+21+ρ(2−ν)/2B1/2
ν/radicalbigDκ
∇D1/4
δCρ/radicalig
ψρ
δ/2−ρ(2Nt/Cρ)ψρ
ρ(ν−1)(2Nt/Cρ)
µCν/2
ρNt
+22(1+ρν)B2
νC′′
∇Cρψρ
ρ(2ν−1)(2Nt/Cρ)
µ5/2/radicalbig
µ′νC2νρNt+23/2+ρ(1+ν)BνDκ
∇Cρ/radicalig
ψρ
2ρ(ν−1)(2Nt/Cρ)
µ3/2µ1/2
νCνρNt
+23/2+ρνBνCρψρ
1+ρ(β+ν−1)−α(2Nt/Cρ)
µ3/2µ1/2
νCγCβ+ν
ρNt,
Furthermore, using the ˜O-notation one can show that
¯δ1/2
t≤Λ1/2
N1/2
t1{σ=1/2}+21/2Λ1/2C(1−2σ)/2(1+ρ)
ρ
N(1+2ρσ)/2(1+ρ)
t1{σ̸=1/2}+21/2C′1/2
σC(1−2(σ+σ′))/2(1+ρ)
ρ
µN(1+2ρ(σ+σ′))/2(1+ρ)
t
+26CσC(2−2σ−β−α)/2(1+ρ)
ρ
µµ1/2
νC1/2
γN(2+ρ(β+2σ)−α)/2(1+ρ)
t+27(µ−1/2+C1/2
γCβ/2
ρ)C′′
∇C2
σCγ
µ/radicalbig
µ′νC(1−2ρσ−α)/(1+ρ)
ρ N(ρ(2σ−β)+α)/(1+ρ)
t
+22Dκ
∇CσC1/2
γC(1+β−2σ+α)/2(1+ρ)
ρ
µµ1/2
νN(1+ρ(2σ−β)+α)/(2(1+ρ))
t+ΓCρ
µNt+22C(2+β−α)/(1+ρ)
ρ ¯π∞
µCγN(2+ρβ−α)/(1+ρ)
t
+˜O(N−(δ+ρν)/2(1+ρ)
t ) + 1{Bν̸=0}Ψt, (37)
where Ψt=˜O(N−ρ(σ+ν)/2(1+ρ)
t ) +˜O(N−(1+ρ(β+ν)−α)/(1+ρ)
t ) +˜O(N−(1+2ρν)/2(1+ρ)
t ) +˜O(N−(δ/2+ρν)/2(1+ρ)
t ) +
˜O(N−2ρν/(1+ρ)
t ), implying that ν >1/2to obtain the desired rate ¯δt=O(N−1)ifBν= 0.
B Verifications of Assumptions 3-p and 5-p for the AR model
Well-specified case. Consider the well-specified case, in which, we estimate an AR (1)model from the
underlying stationary AR (1)processXs=θ∗Xs−1+ϵswith|θ∗|<1. The squared loss function ft(θ) =
31Published in Transactions on Machine Learning Research (07/2023)
n−1
t/summationtextnt
i=1(XNt−1+i−θXNt−1+i−1)2with gradient∇θft(θ) =−2n−1
t/summationtextnt
i=1XNt−1+i−1(XNt−1+i−θXNt−1+i−1).
Thus, the objective function is
F(θ) =E/bracketleftigg
1
ntnt/summationdisplay
i=1(XNt−1+i−θXNt−1+i−1)2/bracketrightigg
=σ2
ϵ(θ∗−θ)2
1−(θ∗)2+σ2
ϵ,
using E[Xs] = 0andE[X2
s] =σ2
ϵ/(1−(θ∗)2), yielding∇θF(θ) = 2σ2
ϵ(θ−θ∗)/(1−(θ∗)2). Next, to verify
Assumption 3-p for p= 2, we first note that
E[∇θft(θ)|Ft−1] =2θ
ntnt/summationdisplay
i=1E/bracketleftig
X2
Nt−1+i−1/vextendsingle/vextendsingle/vextendsingleFt−1/bracketrightig
−2
ntnt/summationdisplay
i=1E/bracketleftbig
XNt−1+i−1XNt−1+i/vextendsingle/vextendsingleFt−1/bracketrightbig
=2(θ−θ∗)
ntnt/summationdisplay
i=1E/bracketleftig
X2
Nt−1+i−1/vextendsingle/vextendsingle/vextendsingleFt−1/bracketrightig
−2
ntnt/summationdisplay
i=1E/bracketleftbig
XNt−1+i−1ϵNt−1+i/vextendsingle/vextendsingleFt−1/bracketrightbig
,(38)
asXNt−1+i=θ∗XNt−1+i−1+ϵNt−1+i. For the first term of (38), we use that E[Xs+i|Fs] = (θ∗)iXsand
Var[Xs+i|Fs] =σ2
ϵ(1−(θ∗)2i)/(1−(θ∗)2), yielding
nt/summationdisplay
i=1E[X2
Nt−1+i−1|Ft−1] =X2
Nt−1nt/summationdisplay
i=1(θ∗)2(i−1)−σ2
ϵ
(1−(θ∗)2)nt/summationdisplay
i=1(θ∗)2(i−1)+σ2
ϵnt
1−(θ∗)2
=(1−(θ∗)2nt)X2
Nt−1
(1−(θ∗)2)−(1−(θ∗)2nt)σ2
ϵ
(1−(θ∗)2)2+σ2
ϵnt
1−(θ∗)2.
Next, the second term of (38) is zero by utilising that (ϵs)is a Martingale difference sequence, i.e., E[ϵs+i|Fs] =
0andE[ϵs+iϵs+j|Fs] = 0fori̸=j. Thus,
E[∥E[∇θft(θ)|Ft−1]−∇θF(θ)∥2] =4(θ−θ∗)2(1−(θ∗)2nt)2σ2
ϵ
(1−(θ∗)2)4n2
t/parenleftbigg
σ2
ϵ+1
1−(θ∗)2/parenrightbigg
,
meaning that Assumption 3-p is verified for p= 2if(Xs)has bounded moments; this is fulfilled by the
natural constraint that |θ∗|<1. Thus, we can deduce that Dν>0,Bν= 0, andνtisO(n−1
t). The remaining
assumption can be verified in the same way, in particular, Assumption 5-p is satisfied with σtisO(n−1/2
t),
Assumption 2 with C∇= 2σ2
ϵ/(1−(θ∗)2)andC′
∇= 0, and Assumption 6 with Σ = 4σ4
ϵ/(1−(θ∗)2)and
Σt= 0. Furthermore, for an AR (1)processXsconstructed using the noise process ϵs=/radicalbig
Gs(H)zswith
Hurst index H≥1/2, one can verify that ν4
tandσ4
tisO(nH−1
t)in Assumptions 3-p and 5-p using the
self-similarty property (Nourdin, 2012).
Misspecified case. Next, assume that the underlying data generating process follows the MA (1)-process,
Xs=ϵs+ϕ∗ϵs−1, withϕ∗∈R. The misspecification error of fitting an AR (1)model to a MA (1)process can
be found by minimizing
F(θ) =E[(Xs−θXs−1)2] =E[(ϵs+ϕ∗ϵs−1−θ(ϵs−1+ϕ∗ϵs−2))2]
=E[(ϵs+ (ϕ∗−θ)ϵs−1−θϕ∗ϵs−2)2] =σ2
ϵ(1 + (ϕ∗−θ)2+θ2(ϕ∗)2),
where∇θF(θ) = 2(θ−ϕ∗)σ2
ϵ+ 2θ(ϕ∗)2σ2
ϵ. Thus, asθ∗=arg minθF(θ)≡arg minθ(ϕ∗−θ)2+θ2(ϕ∗)2is a
strictly convex function in θ, we have∇θF(θ) = 0⇔2(θ−ϕ∗) + 2θ(ϕ∗)2= 0⇔2θ(1 + (ϕ∗)2) = 2ϕ∗⇔
θ=ϕ∗/(1 + (ϕ∗)2). This means for any ϕ∗∈Rthenθ∈(−1/2,1/2). With this in mind, we can conduct
our study of fitting an AR(1) model to the MA(1) process with ϕ∗drawn randomly from R(figure 1b).
Furthermore, this reparametrization trick can be used to verify Assumption 3-p: first, we can reparameterize
∇θF(θ) = 2σ2
ϵ(θ−θ∗)(1 + (ϕ∗)2)usingθ∗=ϕ∗/(1 + (ϕ∗)2). Next, for E[∇θft(θ)|Ft−1]one have that
E[∇θft(θ)|Ft−1] =2θ
ntnt/summationdisplay
i=1E[X2
Nt−1+i−1|Ft−1]−2
ntnt/summationdisplay
i=1E[XNt−1+i−1XNt−1+i|Ft−1],
32Published in Transactions on Machine Learning Research (07/2023)
where
nt/summationdisplay
i=1E[X2
Nt−1+i−1|Ft−1] =X2
Nt−1+E[X2
Nt−1+1|Ft−1] +···+E[X2
Nt−1+nt−1|Ft−1]
=X2
Nt−1+σ2
ϵ+ (ϕ∗)2ϵ2
Nt−1+···+σ2
ϵ+ (ϕ∗)2σ2
ϵ
=X2
Nt−1+ (ϕ∗)2ϵ2
Nt−1+σ2
ϵ(nt−1) + (ϕ∗)2σ2
ϵ(nt−2)
=X2
Nt−1+ (ϕ∗)2(ϵ2
Nt−1−σ2
ϵ) + (1 + (ϕ∗)2)σ2
ϵ(nt−1),
and
nt/summationdisplay
i=1E[XNt−1+i−1XNt−1+i|Ft−1] =ϕ∗XNt−1ϵNt−1+ϕ∗σ2
ϵ(nt−1)
=θ∗(1 + (ϕ∗)2)XNt−1ϵNt−1+θ∗(1 + (ϕ∗)2)σ2
ϵ(nt−1),
using the same white noise properties as for the well-specified case above. This yields,
E[∥E[∇θft(θ)|Ft−1]−∇θF(θ)∥2] =4(θ−θ∗)2
n2
tfϕ∗(ϵNt−1),
wherefϕ∗(ϵNt−1)is finite function depending on the moments of (ϵNt−1)andϕ∗. Hence, we have Dν>0and
Bν= 0withνtbeingO(n−1
t). Similarly, it can be verified that σtareO(n−1/2
t)by use of the reparametrization
trick.
33