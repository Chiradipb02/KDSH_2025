Under review as submission to TMLR
Scoring Rule Training for Simulation-Based Inference
Anonymous authors
Paper under double-blind review
Abstract
Bayesian Simulation-Based Inference (SBI) yields posterior approximations for simulator
models with intractable likelihood. Recent methods employ normalizing flows for SBI, based
on invertible neural networks parametrizing a flexible and tractable density approximation,
typically trained via maximum likelihood on simulated parameter-observation pairs. In con-
trast, GATSBI (Ramesh et al., 2022) approximated the posterior with generative networks,
which pose no constraints on the neural network, thus scaling better to high-dimensional
and structured data but losing access to the density. GATSBI relies on adversarial training,
which is unstable and can lead to a learned distribution underestimating the uncertainty.
Here, we introduce Scoring Rule training for SBI (ScoRuTSBI), applying for the first time
an overlooked adversarial-free training approach for generative networks to SBI. On our two
high-dimensionalexamples, we found ScoRuTSBI performsbetter withshorter training time
than GATSBI; moreover, ScoRuTSBI outperforms methods based on normalizing flows on
one of the high-dimensional examples, while performing equally on the other. Conversely,
ScoRuTSBI and GATSBI are considerably outperformed by normalizing-flow methods in
low-dimensional examples.
1 Introduction
Simulator models are statistical models for which it is impossible to evaluate the likelihood p(y|θ)for an
observation y, but from which it is easy to simulate for any parameter value θ. Given yand a prior π(θ),
the Bayesian posterior is π(θ|y)∝π(θ)p(y|θ). However, obtaining that explicitly or sampling from it with
Markov Chain Monte Carlo (MCMC) is impossible without having access to the likelihood.
Bayesian Simulation-Based Inference (SBI) techniques exploit model simulations to approximate the exact
posterior distribution when the likelihood is unavailable. A prototypical method is Approximate Bayesian
Computation (ABC) (Lintusaari et al., 2017; Bernton et al., 2019), which builds an implicit approximation
of the posterior by drawing parameter values from the prior and weighting them according to the distance
between observations and simulations obtained from those parameter values. Variants of ABC relying on
MCMC (Marjoram et al., 2003) and sequential Monte Carlo (Beaumont et al., 2009) also exist.
A recent strand of literature has performed SBI using neural networks representing probability densities,
such as generative networks trained adversarially (Goodfellow et al., 2014), normalizing flows (Papamakarios
et al., 2021) and score-based diffusion models (Song & Ermon, 2019; Song et al., 2020). Normalizing flows
represent a distribution with an invertible neural network transforming samples from a simple base measure,
thus allowing evaluation of the density of the distribution via the change-of-variables formula enabled by
invertibility. Using the latter, normalizing flows can be trained via maximum likelihood estimation on
parameter-simulation pairs. In SBI, they have been used to represent either the likelihood (Papamakarios
et al., 2019; Lueckmann et al., 2019) or the posterior (Papamakarios & Murray, 2016; Lueckmann et al.,
2017; Greenberg et al., 2019; Radev et al., 2020; Wildberger et al., 2023) or both (Wiqvist et al., 2021).
Despite being present on the deep learning scene for longer than normalizing flows or diffusion models,
generative networks have only been used for SBI in GATSBI (Ramesh et al., 2022), which adapted the
Generative Adversarial Network, or GAN, framework of Goodfellow et al. (2014) to learn the posterior.
Generative networks are more expressive and better to scale to larger sizes than normalizing flows but
1Under review as submission to TMLR
forgo density evaluation Empirically, Ramesh et al. (2022) showed how generative networks outperform
normalizing flows on high-dimensional and structured data, but lead to generally poor calibration of the
learnt distribution, which is a well-known consequence of unstable adversarial training (Arora et al., 2017;
Bellemare et al., 2017; Arora et al., 2018; Richardson & Weiss, 2018).
To leverage the power of generative networks while overcoming their poor calibration when trained adver-
sarially, we turn to Scoring Rule minimization (Bouchacourt et al., 2016; Gritsenko et al., 2020; Harakeh &
Waslander, 2021; Pacchiardi et al., 2022), which has been sparsely used before but never applied to SBI1. We
term our method Scoring Rule Training for Simulation-Based Inference (ScoRuTSBI). Being adversarial-free,
ScoRuTSBI leads to and faster convergence and training time, while empirically achieving better calibra-
tion than GATSBI. This allows ScoRuTSBI to outperform methods based on normalizing-flows on a high-
dimensional examples, while performing equally on another. Conversely, we find ScoRuTSBI and GATSBI
are considerably outperformed by normalizing-flow methods in low dimensional examples.
The rest of the paper is organized as follows. Section 2 discusses how to use a generative network to represent
an approximate posterior. Section 3 introduces Scoring Rule Training for SBI (ScoRuTSBI) and discusses
related approaches, including GATSBI (Ramesh et al., 2022) and normalizing flow methods. Section 4
reports simulation results and Section 5 gives concluding remarks.
Notation We will denote respectively by Y⊆RdandΘ⊆Rpthe data and parameter space. We will
useP(·|θ)andp(·|θ)to denote the distribution and likelihood (with respect to Lebesgue measure) of the
considered simulation-based model. Πandπwill denote prior distribution and prior density on Θ, and Π(·|y)
andπ(·|y)will denote corresponding posterior quantities for observation y. In general, we will use PorQto
denote distributions, while Swill denote a generic Scoring Rule. Other upper-case letters ( X,YandZ) will
denote random variables while lower-case ones will denote observed (fixed) values. We will denote by Yor
ythe observations (correspondingly random variables and realizations). Bold symbols denote vectors, and
subscripts to bold symbols denote sample index (for instance, yi). Instead, subscripts to normal symbols
denote component indices (for instance, yjis thej-th component of y, andyi,jis thej-th component of yi).
Finally,⊥ ⊥will denote independence between random variables, while Y∼Pindicates a random variable
distributed according to Pandy∼Pa sample from such random variable.
2 Approximate posterior via generative network
We use a generative network to represent an approximate posterior distribution Qϕ(·|y)on the parameter
space Θgiven an observation y∈Y. The generative network is defined via a neural network gϕ:Z×Y→ Θ
transforming samples from a probability distribution Pzover the spaceZconditionally on an observation
y∈Y;ϕrepresents neural network weights. Samples from Qϕ(·|y)are therefore obtained by sampling
z∼Pzand computing ˜θ=gϕ(z,y)∼Qϕ(·|y)2
In the following, as it is standard in the SBI setup, we assume to have access to parameter-simulations
pairs (θi,yi)n
i=1generated from the prior θi∼Πand the model yi∼P(·|θi); critically, these can also be
considered as samples from the data marginal yi∼Pand the posterior θi∼Π(·|yi). Using these samples,
we want to tune ϕsuch thatQϕ(·|y)≈Π(·|y)for all values of y; this is therefore an amortized setting
(Radev et al., 2020), namely the resulting posterior approximation is valid for multiple observations.
In the amortized setting, a single neural network has to map the observation into a posterior for all possible
observations; intuitively, we expect this to work well for those cases where such inversion process is in some
sense “generic”. In contrast, the amortized approach will perform poorly when the posterior distribution
depends on the data in a non-linear way. Additionally, the amortized approach may be wasteful in terms of
model simulations when inference for a single observation is needed, as the simulations from the simulation-
based model are drawn independently from it, so that many will be uninformative. In Appendix A, we
discuss strategies for tailoring simulations to a specific observation.
1Despite the name similarity, this is different from score-based diffusion networks (Song & Ermon, 2019; Song et al., 2020).
2Formally,Qϕ(·|y)is the push-forward of Pzthrough the map gϕ(·,y):Qϕ(·|y) =gϕ(·,y)♯Pz, which means that, for any
setAbelonging to the Borel σ-algebraσ(Θ),Qϕ(A|y) =Pz/parenleftbig
{z∈Z:gϕ(z,y)∈A}/parenrightbig
.
2Under review as submission to TMLR
3 Posterior inference via Scoring Rule training
We first review Scoring Rules and give examples of Scoring Rules that we’ll use in our framework (Sec. 3.1).
We then discuss in detail our proposed training method (Sec. 3.2) and related approaches (Sec. 3.2).
3.1 Scoring Rules
We first introduce Scoring Rules for a distribution Prelated to a generic random variable X. A Scoring Rule
(SR, Gneiting & Raftery, 2007) S(P,x)is a function of Pand of an observation xof the random variable
X. IfXis distributed according to Q, the expected Scoring Rule is defined as:
S(P,Q) :=EX∼QS(P,X),
The Scoring Rule Sisproperrelative to a set of distributions PoverXif
S(Q,Q)≤S(P,Q)∀P,Q∈P,
i.e., if the expected Scoring Rule is minimized in PwhenP=Q. Moreover, Sisstrictly proper relative to
PifP=Qis the unique minimum:
S(Q,Q)<S(P,Q)∀P,Q∈Ps.t.P̸=Q.
3.1.1 Examples of Scoring Rules
The following are strictly proper Scoring Rules that we will use in our experiments.
Energy score The energy score3is given by:
S(β)
E(P,x) = 2·E/bracketleftig
∥˜X−x∥β
2/bracketrightig
−E/bracketleftig
∥˜X−˜X′∥β
2/bracketrightig
,˜X⊥ ⊥˜X′∼P, (1)
whereβ∈(0,2). This is a strictly proper SR for the class of probability measures Psuch that E˜X∼P∥˜X∥β<
∞(Gneiting & Raftery, 2007). An unbiased estimate can be obtained by replacing the expectations in S(β)
E
with empirical means over draws from P(see Appendix D.1) We will fix β= 1in the rest of this work.
Kernel score Whenk(·,·)is a positive definite kernel, the kernel score for kcan be defined as (Gneiting
& Raftery, 2007):
Sk(P,x) =E[k(˜X,˜X′)]−2·E[k(˜X,x)],˜X⊥ ⊥˜X′∼P. (2)
The kernel score is proper for the class of probability distributions Pfor which E˜X,˜X′∼P[k(˜X,˜X′)]is finite
(by Theorem 4 in Gneiting & Raftery (2007)). It is closely related to the kernel Maximum Mean Discrepancy
(MMD, Gretton et al., 2012) and is strictly proper under conditions ensuring the MMD is a metric (Gretton
et al., 2012). These conditions are satisfied, among others, by the Gaussian kernel (which we will use in this
work):
k(˜x,x) = exp/parenleftbigg
−∥˜x−x∥2
2
2γ2/parenrightbigg
,
in whichγis a scalar bandwidth. As for the Energy Score, an unbiased estimate can be obtained by replacing
the expectations in Skwith empirical means over draws from P(see Appendix D.1).
3The probabilistic forecasting literature (Gneiting & Raftery, 2007) use a different convention for the energy score and the
subsequent kernel score, which amounts to multiplying our definitions by 1/2. We follow here the convention used in the
statistical inference literature (Rizzo & Székely, 2016; Chérief-Abdellatif & Alquier, 2020; Nguyen et al., 2020)
3Under review as submission to TMLR
Patched SR We now discuss a way to build a composite SR which preserves the structural information
inX. In fact, if Xrepresent values of a variable on a spatial grid, computing the SRs introduced above
discards this information as the components of Xcan be permuted without changing the resulting score.
The patched SR (Pacchiardi et al., 2022) is defined by sliding a window over the components of Xto obtain
patchesand summing the scores obtained for each patch. In practice, for a patch size δand a patch step s,
forx∈Rd, the patched SR is defined as:
˜Sp=⌈d−s
δ+1⌉/summationdisplay
j=1S(P|j·s:j·s+δ−1,xj·s:j·s+δ−1), (3)
where xj·s:j·s+δ−1= [xj·s,xj·s+1,xj·s+2,...,xj·s+δ−1]andP|j·s:j·s+δ−1is the marginal distribution induced
byPon the components j·stoj·s+δ−1ofX. In this way, the dependence between components of
the same patch is given importance, while those of different patches is not (except as mediated by other
components).
Note however that Spis not strictly proper; to make this strictly proper, we add the SR computed over the
fullx, which makes the overall SR strictly proper (see Lemma 3.4 in Pacchiardi et al. (2022)), thus obtaining:
Sp(P,x) =w1S(P,x) +w2˜Sp(P,x),
wherew1,w2>0.The formulation of patched SR can be generalised to X∈Rdn; see Appendix C for more
details.
3.2 ScoRuTSBI: Scoring Rule Training for Simulation-Based Inference
Let us now go back to the Bayesian SBI setting introduced at the start of the paper and let us denote by
Qϕ(·|y)the approximate posterior parametrized by the generative network.
For a strictly proper SR S, solving the following problem:
arg min
ϕEY∼PEθ∼Π(·|Y)S(Qϕ(·|Y),θ) = arg min
ϕEθ∼ΠEY∼P(·|θ)S(Qϕ(·|Y),θ) (4)
leads toQϕ(·|y) = Π(·|y)for all values of yfor whichp(y)>0.
An empirical analogue of the objective in Eq. (4) is obtained by replacing the expectations with empirical
means over the training dataset, leading to the following empirical minimization problem:
arg min
ϕ1
nn/summationdisplay
i=1S(Qϕ(·|yi),θi); (5)
computing the objective directly is intractable as, in general, we do not have access to S(Qϕ(·|y),θ). Notice,
however, that in order to solve Eq. (5) via Stochastic Gradient Descent (SGD) it is enough to obtain unbiased
estimates of∇ϕS(Qϕ(·|yi),θi), which can be easily done via automatic differentiation whenever Sadmits
an unbiased empirical estimator ˆSsuch that:
E/bracketleftig
ˆS({˜θ(y)
j}m
j=1,θ)/bracketrightig
=S(Qϕ(·|y),θ),
where the expectation is over ˜θ(y)
j∼Qϕ(·|y). More details can be found in Appendix D.2. If Sadmits such
an estimator, each step of SGD involves generating msimulations from the generative network Qϕ(·|yi)for
eachyiin the training batch. We term this approach ScoRuTSBI (Scoring Rule Training for Simulation-
Based Inference). Algorithm 1 shows a single epoch of the resulting training algorithm, with batch size
equal to 1 for simplicity. For the Energy and Kernel Scores introduced in Sec. 3.1.1, unbiased estimators are
available; as such, we will use these scoring rules in the following.
4Under review as submission to TMLR
Algorithm 1 ScoRuTSBI, single epoch (with batch size equal to 1).
Require: Generative network gϕ:Z×Y→ Θ, SRS, learning rate ϵ.
foreach training pair (θi,yi)do
Sample multiple z1,...,zm
Obtain ˜θϕ
i,j=gϕ(zj,yi)
Obtain unbiased estimate ˆS({˜θ(yi)
i,j}m
j=1,θi)ofS(Qϕ(·|yi),θi)
Setϕ←ϕ−ϵ·∇ϕˆS({˜θ(yi)
i,j}m
j=1,θi)
end for
3.3 Related approaches
3.3.1 Generative Adversarial Training for Simulation-Based Inference
In Ramesh et al. (2022), the posterior approximation Qϕwas trained in an adversarial framework in a
method termed GATSBI (Generative Adversarial Training for Simulation-Based Inference). This requires
introducing a discriminator orcriticneural network cψ: Θ×Y → Rwith weights ψwhose task is to
distinguish draws from the approximate and true posteriors. The loss employed in Ramesh et al. (2022) is
the conditional version of the original Generative Adversarial Network (GAN) loss from Goodfellow et al.
(2014), which was originally discussed in Mirza & Osindero (2014):
L(ϕ,ψ) =Eθ∼ΠEY∼P(·|θ)EZ∼Pz[logcψ(θ,Y) + log (1−cψ(gϕ(Z,Y),Y))]
=EY∼P/bracketleftig
Eθ∼Π(·|Y)(logcψ(θ,Y)) +E˜θ∼Qϕ(·|Y)/parenleftbig
log/parenleftbig
1−cψ(˜θ,Y)/parenrightbig/parenrightbig/bracketrightig
,(6)
whose saddle point solution
min
ϕmax
ψL(ϕ,ψ) (7)
leads toQϕ(·|y)being the exact posterior for all choices of yfor whichp(y)>0(providedgϕandcψ
have infinite expressive power; that in fact corresponds to minimizing the Jensen-Shannon divergence, see
Appendix B for more details).
As typically done in GANs, GATSBI trains Qϕalternating maximization steps over ψwith minimization
steps overϕ. At each step, the gradient is estimated by replacing the expectations in Eq. (6) with empirical
means over (a mini-batch of) the training dataset and draws from the generative network. This alternating
optimization is unstable and requires careful hyperparameters tuning and specialized training routines (Sali-
mans et al., 2016). Despite those, adversarial training often leads to underestimating the distribution width,
and even to collapse of the distribution on a single mode (Arora et al., 2018; Isola et al., 2017; Richardson
& Weiss, 2018). Arora et al. (2017) showed how this mode collapse can happen due to the finite capacity of
the discriminator, while Bellemare et al. (2017) theoretically linked it to the use of biased gradient estimates
forϕin optimizing Eq. (7) (in fact, estimates of gradients with respect to ϕrely on a value of ψobtained
by few optimization steps, rather than the value maximizing eq. 6).
This uncertainty underestimation may not be an issue in some applications of generative networks where
uncertainty quantification is not important, but it can be detrimental for approximate posterior inference.
In contrast, our Scoring Rule minimization formulation for SBI does not suffer from these theoretical issues
introduced by the use of alternating minimization. Indeed, we show in Sec. 4 how the approximate posterior
obtained with Scoring Rule minimization has better calibration than GATSBI (Section 4).
On the other hand, the unbiased empirical estimators of the Energy and Kernel Scores require multiple
draws from the generative network per observation value (Eq. 3.2). To train GATSBI, instead, a single draw
from the generative network is enough. In our experiments, however, 10or fewer draws lead to satisfactory
results with SR training. Additionally, as mentioned above, the SR approach does not require a discriminator
network and has a more stable training process, which implies convergence is generally reached with fewer
training epochs. These two factors lead to lower computational and memory cost with respect to adversarial
training (see Section 4 for details).
5Under review as submission to TMLR
3.3.2 Normalizing flows for SBI
Normalizing flows are generative networks which impose invertibility of the map gϕ(z,y)with respect to z.
As such, the density qϕwith respect to the Lebesgue measure exists and can be evaluated via the change-
of-variables formula, so that ϕis usually trained via maximum likelihood (Papamakarios et al., 2021). For
instance, in Radev et al. (2020), the following problem was considered, where DKLdenotes the Kullback-
Leibler divergence:
argmin
ϕEY∼P[DKL(Π(·|Y)∥Qϕ(·|Y))]
= argmin
ϕEY∼PEθ∼Π(·|Y)[−logqϕ(θ|Y)]
= argmin
ϕEθ∼ΠEY∼P(·|θ)[−logqϕ(θ|Y)],(8)
which corresponds to our SR-based approach in Eq. (4) by identifying S(Qϕ(·|y),θ) =−logqϕ(θ|y), which
is the strictly-proper logarithmic scoring rule (Gneiting & Raftery, 2007).
The Neural Posterior Estimation (NPE) methods presented in Papamakarios & Murray (2016); Lueckmann
et al. (2017); Greenberg et al. (2019) are closely related to the objective in Eq. 8, but they focus on inferring
a posterior distribution valid for a single observation and design sequential approaches to exploit simulations
more efficiently. A single turn of those methods correspond exactly to Eq. (8).
Differently, Neural Likelihood Estimation (NLE) Papamakarios et al. (2019); Lueckmann et al. (2019) targets
the likelihood instead of the posterior. Similarly, it is a sequential approach tailoring simulations to a single
observation, but it can also be used in a single-turn fashion. Sequential versions of our SR minimization
approach can also be designed, see Appendix A.
Finally, while all of the above methods relied discrete normalizing flows, Wildberger et al. (2023) exploited
instead continuous normalizing flows (Papamakarios et al., 2021) to parametrize the posterior distribution,
with a method termed “Flow Matching”.
3.3.3 Diffusion models for SBI
Score-based diffusion networks (Song & Ermon, 2019; Song et al., 2020) use a neural network to approximate
the gradient of the logarithm of a target density (termed score, but unrelated to the scoring rules focus of
our method) and generate samples by simulating a diffusion process starting from a base measure and
converging to the target distribution. A few works applied this approach to SBI: Sharrock et al. (2022)
focused on a sequential approach while and Geffner et al. (2023) shows how to leverage the approximate
score to produce posterior samples for any number of observations. Unfortunately, the unavailability of code
bases accompanying the works mentioned above at the time of the preparation of our manuscript prevented
our comparison with those methods.
3.4 Other uses of SR training
SR training has been used before for training generative networks: Bouchacourt et al. (2016), Gritsenko
et al. (2020) and Harakeh & Waslander (2021) all used the Energy Score, focusing respectively on the
tasks of hand pose estimation, speech synthesis and object estimation. With the exception of the latter,
these works only exploited SR training as it lead to better generated samples, but not for its probabilistic
performance. Pacchiardi et al. (2022) used SR training for the task of probabilistic forecasting, deriving
theoretical guarantees for a predictive-sequential (Dawid, 1984) training objective. Finally, subsequently
to the first version of this work, Bon et al. (2022) uses an objective similar to our Eq. (4) to calibrate an
approximate posterior obtained with other approaches.
4 Simulation studies
Wepresenthereresultsontwolow-dimensionalbenchmarkproblemsandtwohigh-dimensionalmodels, oneof
which has an implicitly defined prior, which were studied in Ramesh et al. (2022). Results on three additional
6Under review as submission to TMLR
low-dimensional benchmarks are reported in Appendix H. For all examples, we evaluate the performance of
the different methods as in Ramesh et al. (2022). Besides that, we assess the calibration of the approximate
posteriors by the discrepancy between credible intervals in the approximate posteriors and the frequency with
which the true parameter belongs to the credible interval itself (we call this metric calibration error ). We
also evaluate the match between the approximate posterior and the true parameter value via the Continuous
Ranked Probability Score (CRPS) averaged over multiple values for (θi,yi). The CRPS is a strictly proper
scoring rule and, as such, is minimized in expectation when the approximate posterior matches the true
posterior for the different observed values. As both metrics are for scalar variables, we compute their values
independently for each component of θand report their average. We provide more detail in Appendix E.
For the two low-dimensional benchmarks, besides the results for ScoRuTSBI, we report the results from
Ramesh et al. (2022), which compared GATSBI with NPE (Papamakarios & Murray, 2016; Lueckmann
et al., 2017; Greenberg et al., 2019), NLE (Papamakarios et al., 2019; Lueckmann et al., 2019) , Neural Ra-
tio Estimation (Hermans et al., 2020) and two versions of Approximate Bayesian Computation (REJ-ABC,
Tavaré et al., 1997, and SMC-ABC, Beaumont et al., 2009). We also report the results obtained by Wild-
berger et al. (2023) with the “Flow Matching” method to train continuous normalizing flows (Papamakarios
et al., 2021). Overall, we find that ScoRuTSBI performs comparably to GATSBI, with both of them being
overperformed by methods based on normalizing flows.
Generative neural networks have a competitive advantage in high-dimensional settings; indeed, Ramesh
et al. (2022) found that GATSBI was competitive with normalizing-flow methods on one of the two high-
dimensional cases (ABC methods, not being amortized, could not be run over the large number of ob-
servations in a reasonable amount of time). As such, we here report results of NPE, NLE, GATSBI and
ScoRuTSBI for that example (Sec. 4.2). Finally, GATSBI, ScoRuTSBI and NPE are the only methods that
can handle the implicit prior of the other high-dimensional example (Sec. 4.3).
Additional training details of our approach for all examples are reported in Appendix F. We refer to Ramesh
et al. (2022) for details of the other methods.
4.1 Benchmark models
We consider here the “Simple Likelihood Complex Posterior” (SLCP) and the “Two Moons” benchmarks; in
the former, a 5-dimensional θdefines the distribution of an 8-dimensional Gaussian yin a nonlinear manner.
In the Two Moons model, both yandθare 2-dimensional. We refer to Ramesh et al. (2022) and references
therein for more details4For both models, we train all methods on ntrain= 1000,10000and100000posterior
samples. We consider ScoRuTSBI with the Energy and Kernel Score trained with m= 3,5,10or20samples
from the generative network for each yiin a training batch. ScoRuTSBI is trained on a single CPU, while
GATSBI is trained on an NVIDIA Tesla-V100 GPU. For the Two Moons model, we do not use early stopping
forScoRuTSBI;additionally, weemploytheoptimalconfigurationfoundinRameshetal.(2022)forGATSBI.
For these two models, samples from reference posteriors are available (Lueckmann et al., 2021); therefore, as
doneinRameshetal.(2022), weassesstheperformanceofthedifferentmethodsviathediscriminationability
of a classifier trained to distinguish samples from the reference and approximate posteriors (classification-
based two-sample test, C2ST). If the classification accuracy is 0.5, the classifier is unable to distinguish
between the two sets of samples, implying perfect posterior approximation.
For ScoRuTSBI, we report here results with m= 20, as we found that to perform best. In Figure 1, we
report C2ST values for all considered methods for the different number of training simulations (Fig. 6 in
Appendix reports all values of mfor ScoRuTSBI). On these examples, NPE, NLE and Flow Matching
perform better than both GATSBI and ScoRuTSBI, with each of the latter slightly overperforming the
other on one of the two models. To better understand the difference between GATSBI and ScoRuTSBI,
in Tables 1 and 2, we report other performance metrics, together with the runtime and the epoch at which
training was early stopped, with ntrain= 100000 andm= 20. Notice how ScoRuTSBI was trained in much
shorter time (and on a single CPU). Additional results are reported in Appendices G.1 and G.2.
4These models are implemented in the sbibmPython package, whose accompanying paper (Lueckmann et al., 2021) provides
additional details.
7Under review as submission to TMLR
1000 10 000 100 000
Number of simulations0.500.751.00C2ST (accuracy)SLCP
1000 10 000 100 000
Number of simulations0.500.751.00C2ST (accuracy)T wo Moons
SMC-ABC
REJ-ABC
NPE
NRE
NLE
GATSBI
GATSBI-optim
Energy Score
Kernel Score
Flow MatchingA B
Figure 1: C2ST for the SLCP and Two Moons benchmarks for all considered methods (ScoRuTSBI with the
Energy and Kernel Score is reported with ntrain= 100000 andm= 20); larger values are worse. For both
SLCP and Two Moons, NPE and NLE methods perform better. Moreover, for SLCP, GATSBI performs
better than ScoRuTSBI, but both perform poorly on an absolute scale. For Two Moons, ScoRuTSBI with
the Energy Score perform better than GATSBI.
Table 1: SLCP: performance metrics, runtime and early stopping epoch for GATSBI and ScoRuTSBI (with
Energy and Kernel Score), with ntrain = 100000 andm= 20. Notice how ScoRuTSBI was trained on a
single CPU, while GATSBI was trained on a GPU. The maximum number of training epochs was 20000.
C2ST↓Cal. Err.↓CRPS↓Runtime (sec) Early stopping epoch
GATSBI 0.92±0.03 0.05±0.03 1.37±0.30 30963 20000
Energy 0.95±0.02 0.05±0.02 1.35±0.36 1645 2100
Kernel 0.98±0.01 0.08±0.05 1.46±0.40 1210 1200
4.2 Shallow water model
The shallow water model is obtained as the discretization of a PDE describing the propagation of an initial
disturbance across the surface of a 1D shallow basin; the parameter θ∈R100represents the depth of
the basin at equidistant points; the simulator outputs the evolution over 100 time-steps (producing a raw
observation of size 100×100 = 10000 ); then, a Fourier transform is computed and the real and imaginary
parts are concatenated and summed to Gaussian noise, leading to y∈R20k. More details are given in
Ramesh et al. (2022). We test here ScoRuTSBI with the Energy and Kernel score with m= 10computed in
three different configurations: 1) on the full parameter space, 2) with patch size 10 and step 5, and 3) with
patch size 20 and step 10. Training is done on 100k samples on a NVIDIA Tesla-V100 GPU; additional
details are discussed in Appendix F.1. Among the different instances of ScoRuTSBI, the Energy Score with
patch size 20 and step 10 performed better; therefore, we report only results for that method in the main
body of the paper; results for the other configurations are given in Appendix G.3. We compare with the
results obtained by GATSBI, NPE and NLE.
Table 2: Two Moons: performance metrics, runtime and early stopping epoch for GATSBI and ScoRuTSBI
(with Energy and Kernel Score), with ntrain = 100000 andm= 20. Notice how ScoRuTSBI was trained
on a single CPU, while GATSBI was trained on a GPU. Here, no early stopping was used (the maximum
number of training epochs was 20000).
C2ST↓Cal. Err.↓CRPS↓Runtime (sec) Early stopping epoch
GATSBI 0.82±0.07 0.05±0.01 0.36±0.00 30232 20000
Energy 0.73±0.04 0.03±0.00 0.35±0.00 10805 20000
Kernel 0.92±0.02 0.04±0.00 0.36±0.00 10902 20000
8Under review as submission to TMLR
01020Depth profileGround truth
Prior samples  Time1
22
50
69
940.03
0.000.03Amplitudet = 22 t = 69 t = 94
01020Depth profileENERGY SCORE
  Time1
22
50
69
940.03
0.000.03Amplitude
01020Depth profileGATSBI
  Time1
22
50
69
940.03
0.000.03Amplitude
01020Depth profileNPE
  Time1
22
50
69
940.03
0.000.03Amplitude
1 50 100
Position01020Depth profileNLE
  Position1 50 100  Time1
22
50
69
94
1 50 100
Position0.03
0.000.03Amplitude
1 50 100
Position1 50 100
PositionA
B
C
D
E
Figure 2: Shallow water model: inference results with GATSBI, NPE, NLE and ScoRuTSBI with Energy
Score with patch size 20 and step 10. The figure structure closely follows that in Ramesh et al. (2022).
Row A: Ground truth, observation and prior samples. Left: ground-truth depth profile and prior samples.
Middle: surface wave simulated from ground-truth profile as a function of position and time. Right: wave
amplitudes at three different fixed times for ground-truth depth profile (black), and waves simulated from
multiple prior samples (gray). The remaining rows refer to ScoRuTSBI with the Energy Score (with patch
size 20 and step 10), GATSBI, NPE AND NLE. For all methods, left represents posterior samples versus
ground-truth (black) depth profiles, from which it can be seen how posterior samples for ScoRuTSBI better
follow the truth with respect to GATSBI; middle represents surface wave simulated from a single posterior
sample; right represents wave amplitudes simulated from multiple posterior samples, at three different fixed
times, with black line denoting the actual observation; again, ScoRuTSBI better follows the observation,
except fort= 94.
In Figure 2, we report posterior and posterior predictive samples for all methods, together with prior samples
and the ground-truth depth profile. For ScoRuTSBI and NPE, posterior samples better follow the ground
truth profile and, similarly, posterior predictive samples better match the true observation.
In Table 3, we report the performance metrics, runtime and epoch of early stopping of the GATSBI and
ScoRuTSBI; notice how the calibration error is much smaller for the latter, whose training run faster. We
also assess calibration via Simulation Based Calibration (Talts et al., 2018, details in Appendix E.1.2) in
Figure 3. That as well highlights how the calibration of ScoRuTSBI is better than the one achieved by
GATSBI and comparable with that achieved by NPE.
9Under review as submission to TMLR
Table 3: Shallow Water model: performance metrics, runtime and early stopping epoch for GATSBI and
ScoRuTSBI with the Energy Score with patch size 20 and step 10. The latter method achieved better results
with shorter training time. We do not train GATSBI from scratch but rather relied on the trained network
obtained in Ramesh et al. (2022). The training time we report here corresponds to what is mentioned in
Ramesh et al. (2022), which used two GPUs for training (with respect to a single one for ScoRuTSBI). For
the same reason, we do not report the epoch at which GATSBI training was early stopped.
Cal. Err.↓CRPS↓Runtime (sec) Early stopping epoch
Energy 0.03±0.02 0.99±0.53 60017 12400
GATSBI 0.12±0.09 1.43±0.91≈345600 -
0 500 1000
Rank
Uniform CDF
GATSBI
0 500 1000
Rank
Uniform CDF
NPE
0.00.51.0empirical CDF
0 500 1000
RankUniform CDF
Energy Score
Figure 3: Shallow Water model: Simulation Based Calibration for ScoRuTSBI with Energy Score (patch
size 20 and step 10), GATSBI and NPE. Each line corresponds to a single dimension of θand represents
the CDF of the rank of the true parameter value with respect to a set of posterior samples. A calibrated
posterior implies uniform CDF (diagonal black line, with associated 99% confidence region for that number
of samples in gray).
4.3 Noisy Camera model
Here, we consider θ∈R28×28to be the images of the EMNIST dataset (Cohen et al., 2017), from which the
datay∈R28×28are generated by applying some blurring (see Ramesh et al., 2022 for details). Posterior
inference corresponds therefore to Bayesian denoising. In this model, the dimension of parameter space is
larger than in typical SBI applications; additionally, the prior is defined implicitly as we can only generate
samples from it. This prevents the application of many standard SBI methods such as NLE and ABC.
The only applicable methods are here GATSBI, NPE and ScoRuTSBI; we test the former two in their
default configuration and ScoRuTSBI with the Energy and Kernel score with m= 10in three different
configurations: 1) on the full parameter space, 2) with patch size 14 and step 7, and 3) with patch size 8 and
step 5. Training is done on 800 thousands samples on a NVIDIA Tesla-V100 GPU; additional details are
discussed in Appendix F.2. Among the different instances of ScoRuTSBI, those with patch size 8 and step 5
performed better; therefore, we report only results for ScoRuTSBI with the Kernel and Energy Score in that
configuration in the main body of the paper; results for the other configurations are given in Appendix G.4.
In Figure 4, we report posterior mean and standard deviation for a set of observations for the different
methods. The two ScoRuTSBI variations lead to cleaner image reconstruction and more meaningful
uncertainty quantification. NPE performs particularly poorly on this example; this shows how generative
networks have an advantage over normalizing flows for structured data such as images.
In Table 4, we report the performance metrics, runtime and epoch of early stopping of GATSBI and
ScoRuTSBI with the two choices of Scoring Rule; the latter leads to smaller calibration error, although
that is still quite poor in absolute terms. The R2values here are also poor. We believe these low metric
values are due to each pixel only taking a discrete set of values between 0 and 1, with white spaces assigned
0and darkest pixels being assigned 1. The generative network outputs is bounded in (0,1)as it is obtained
10Under review as submission to TMLR
yGATSBI
Figure 4: Noisy Camera model: ground truth and posterior inference with different methods, for a set
of observations (each observation corresponds to a column). The first two rows represent the ground-
truth values of θand the corresponding observation yo. The remaining rows represent mean and Standard
Deviation (SD) for GATSBI, ScoRuTSBI with Energy and Kernel Score with patch size 8 and step 5, and
NPE. Notice how NPE performs poorly and how the posterior mean for ScoRuTSBI are neater than those
obtained with GATSBI; additionally, the SD is larger close to the boundary of the reconstructed digit (notice
the different color scale in the SD for the various methods).
via a continuous transformation from R. For the calibration error (see Appendix E.1.1), that means that a
credible interval obtained from the generative network cannot contain the extreme values 0 or 1.
Table 4: Noisy Camera model: performance metrics, runtime and early stopping epoch for GATSBI and
ScoRuTSBIwithEnergyandKernelScore(patchsize8andstep5). ScoRuTSBIachievedbetterperformance
with shorter training time. All methods are trained on a single GPU.
Cal. Err.↓CRPS↓Runtime (sec) Early stopping epoch
GATSBI 0.50±0.01 0.28±0.26 45398 3600
Energy 0.37±0.12 0.04±0.03 22633 4000
Kernel 0.36±0.12 0.06±0.04 22545 3200
5 Conclusions
We considered using a generative network to represent posterior distributions for Bayesian Simulation-
Based Inference and investigated training it via Scoring Rule minimization rather than in the adversarial
setup of Ramesh et al. (2022). Our approach, termed Scoring Rule Training for Simulation-Based Inference
(ScoRuTSBI) is theoretically grounded and does not suffer from training instability and biased gradients, as
does the adversarial approach.
In low-dimensional benchmarks, ScoRuTSBI has comparable performance to the adversarial approach of
Ramesh et al. (2022) (termed GATSBI), but both fall short when compared to their counterparts based on
normalizing flows. The poor performance on low-dimensional benchmarks likely stems from the specifics
of the generative networks rather than from the training algorithms, as both ScoRuTSBI and GATSBI
11Under review as submission to TMLR
have good performance on the high-dimensional examples. Therefore, we recommend the utilization of
normalizing-flow-based techniques in such scenarios.
Furthermore, we are mainly interested in the performance of our method in high-dimensional settings,
a challenging terrain for traditional simulation-based inference methods and normalizing-flow-based
approaches alike. Here, we found that ScoRuTSBI improves upon GATSBI while requiring a lower
computational cost. Moreover, ScoRuTSBI is shown to be superior to methods based on normalizing
flows (such as Neural Posterior Estimation, Greenberg et al., 2019, and Neural Likelihood Estimation,
Papamakarios et al., 2019) on one high-dimensional example (noisy camera model) while comparable on the
other (shallow water model). Additionally, traditional sampling-based SBI methods are too expensive to
tackle these examples. Consequently, we believe that ScoRuTSBI holds substantial promise for addressing
Bayesian simulation-based inference for high-dimensional models.
For ScoRuTSBI, employing patched scores (Sec. 3.1.1) leads to a small performance improvement over the
vanilla ones on the high-dimensional examples (see Appendix G.3 and G.4). While we designed the patches
to capture the data structure, the improvement we observe could simply be due to computing the Energy
and Kernel scores on lower-dimensional objects. To disentangle these two effects, we could define scoring
rules using a random subset of components of θof the same size as the patches used above. We leave this
for future work.
Analogously to the patched scores, it may be that employing a patched discriminator (Isola et al., 2017)
improves results with GAN; however, we believe this would not completely close the performance gap,
which is mostly due to the harder optimization objective in GAN. To this point, more advanced adversarial
training algorithms than the original GAN objective (Goodfellow et al., 2014) may lead to better results;
however, for probabilistic forecasting, the results in Pacchiardi et al. (2022) show Scoring Rule minimization
to outperform state-of-the-art adversarial approach, while being cheaper and easier to train. We expect the
same to hold for simulation-based inference.
In the present work, we did not provide any theoretical guarantees for ScoRuTSBI; it could be of interest to
proveageneralizationboundbetweentheempirical(Eq.5)andpopulation(Eq.4)objectives,oraconsistency
result for the minimizer of Eq. (5), similarly to what done for probabilistic forecasting in Pacchiardi et al.
(2022) and for GATSBI in Wang & Ročková (2022). We leave these extensions for future work.
Finally, using a learned kernel to train a generative network via Kernel Score minimization could make the
method more flexible. This could be done by learning the kernel adversarially (as in MMD GAN, Bińkowski
et al., 2018), which however would break the ease of optimization which is the main advantage of the Scoring
Rule approach. We hope that future research will address achieving both these goals together.
References
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In
International conference on machine learning , pp. 214–223. PMLR, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in
generative adversarial nets (GANs). In International Conference on Machine Learning , pp. 224–232.
PMLR, 2017.
Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do GANs learn the distribution? Some theory and empirics.
InInternational Conference on Learning Representations , 2018. URL https://openreview.net/forum?
id=BJehNfW0- .
Mark A Beaumont, Jean-Marie Cornuet, Jean-Michel Marin, and Christian P Robert. Adaptive approximate
bayesian computation. Biometrika , 96(4):983–990, 2009.
MarcGBellemare, IvoDanihelka, WillDabney, ShakirMohamed, BalajiLakshminarayanan, StephanHoyer,
and Rémi Munos. The Cramer distance as a solution to biased Wasserstein gradients. arXiv preprint
arXiv:1705.10743 , 2017.
12Under review as submission to TMLR
Espen Bernton, Pierre E. Jacob, Mathieu Gerber, and Christian P. Robert. Approximate Bayesian computa-
tion with the Wasserstein distance. Journal of the Royal Statistical Society: Series B (Statistical Methodol-
ogy), 81(2):235–269, 2019. doi: https://doi.org/10.1111/rssb.12312. URL https://rss.onlinelibrary.
wiley.com/doi/abs/10.1111/rssb.12312 .
Mikołaj Bińkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs.
InInternational Conference on Learning Representations , 2018.
Joshua J Bon, David J Warne, David J Nott, and Christopher Drovandi. Bayesian score calibration for
approximate models. arXiv preprint arXiv:2211.05357 , 2022.
Diane Bouchacourt, Pawan K Mudigonda, and Sebastian Nowozin. DISCO nets: DISsimilarity COefficient
networks. Advances in Neural Information Processing Systems , 29:352–360, 2016.
Badr-Eddine Chérief-Abdellatif and Pierre Alquier. MMD-Bayes: Robust Bayesian estimation via maximum
mean discrepancy. In Symposium on Advances in Approximate Bayesian Inference , pp. 1–21. PMLR, 2020.
Jon Cockayne, Matthew M. Graham, Chris J. Oates, T. J. Sullivan, and Onur Teymur. Testing whether
a learning procedure is calibrated. Journal of Machine Learning Research , 23(203):1–36, 2022. URL
http://jmlr.org/papers/v23/21-1065.html .
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. EMNIST: Extending MNIST to
handwritten letters. In 2017 international joint conference on neural networks (IJCNN) , pp. 2921–2926.
IEEE, 2017.
A Philip Dawid. Present position and potential developments: Some personal views statistical theory the
prequential approach. Journal of the Royal Statistical Society: Series A (General) , 147(2):278–290, 1984.
Tomas Geffner, George Papamakarios, and Andriy Mnih. Compositional score modeling for simulation-based
inference, 2023.
Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal
of the American statistical Association , 102(477):359–378, 2007.
Tilmann Gneiting, Fadoua Balabdaoui, and Adrian E Raftery. Probabilistic forecasts, calibration and sharp-
ness.Journal of the Royal Statistical Society: Series B (Statistical Methodology) , 69(2):243–268, 2007.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing
systems, 27, 2014.
David Greenberg, Marcel Nonnenmacher, and Jakob Macke. Automatic posterior transformation for
likelihood-free inference. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research ,
pp. 2404–2414. PMLR, 09–15 Jun 2019. URL http://proceedings.mlr.press/v97/greenberg19a.
html.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A kernel
two-sample test. The Journal of Machine Learning Research , 13(1):723–773, 2012.
Alexey Gritsenko, Tim Salimans, Rianne van den Berg, Jasper Snoek, and Nal Kalchbrenner. A spectral
energy distance for parallel speech synthesis. Advances in Neural Information Processing Systems , 33:
13062–13072, 2020.
Ali Harakeh and Steven L. Waslander. Estimating and evaluating regression predictive uncertainty in
deep object detectors. In International Conference on Learning Representations , 2021. URL https:
//openreview.net/forum?id=YLewtnvKgR7 .
Joeri Hermans, Volodimir Begy, and Gilles Louppe. Likelihood-free MCMC with amortized approximate
ratio estimators. In International Conference on Machine Learning , pp. 4239–4248. PMLR, 2020.
13Under review as submission to TMLR
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional
adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pp. 1125–1134, 2017.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In Yoshua Bengio and Yann LeCun
(eds.),2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April
14-16, 2014, Conference Track Proceedings , 2014. URL http://arxiv.org/abs/1312.6114 .
Jarno Lintusaari, Michael U. Gutmann, Ritabrata Dutta, Samuel Kaski, and Jukka Corander. Fundamentals
and recent developments in approximate Bayesian computation. Systematic Biology , 66(1):e66–e82, 2017.
ISSN 1076836X. doi: 10.1093/sysbio/syw077. URL https://doi.org/10.1093/sysbio/syw077 .
Jan-Matthis Lueckmann, Pedro J Goncalves, Giacomo Bassetto, Kaan Öcal, Marcel Nonnenmacher, and
Jakob H Macke. Flexible statistical inference for mechanistic models of neural dynamics. In Advances in
Neural Information Processing Systems , pp. 1289–1299, 2017.
Jan-Matthis Lueckmann, Giacomo Bassetto, Theofanis Karaletsos, and Jakob H Macke. Likelihood-free
inference with emulator networks. In Symposium on Advances in Approximate Bayesian Inference , pp.
32–53. PMLR, 2019.
Jan-Matthis Lueckmann, Jan Boelts, David Greenberg, Pedro Goncalves, and Jakob Macke. Benchmarking
simulation-based inference. In Arindam Banerjee and Kenji Fukumizu (eds.), Proceedings of The 24th
International Conference on Artificial Intelligence and Statistics , volume 130 of Proceedings of Machine
Learning Research , pp. 343–351. PMLR, 13–15 Apr 2021.
Paul Marjoram, John Molitor, Vincent Plagnol, and Simon Tavaré. Markov chain Monte Carlo without
likelihoods. Proceedings of the National Academy of Sciences , 100(26):15324–15328, 2003.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784 ,
2014.
Hien Duy Nguyen, Julyan Arbel, Hongliang Lü, and Florence Forbes. Approximate Bayesian computation
via the energy statistic. IEEE Access , 8:131683–131698, 2020.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers us-
ing variational divergence minimization. In Proceedings of the 30th International Conference on Neural
Information Processing Systems , pp. 271–279, 2016.
Lorenzo Pacchiardi, Rilwan Adewoyin, Peter Dueben, and Ritabrata Dutta. Probabilistic forecasting with
conditional generative networks via scoring rule minimization. arXiv preprint arXiv:2112.08217 , 2022.
George Papamakarios and Iain Murray. Fast ε-free inference of simulation models with Bayesian conditional
density estimation. In Advances in Neural Information Processing Systems , pp. 1028–1036, 2016.
George Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: Fast likelihood-free
inference with autoregressive flows. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings of
Machine Learning Research , volume 89 of Proceedings of Machine Learning Research , pp. 837–848. PMLR,
16–18 Apr 2019. URL http://proceedings.mlr.press/v89/papamakarios19a.html .
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshmi-
narayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning
Research , 22(57):1–64, 2021. URL http://jmlr.org/papers/v22/19-1028.html .
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d’Alché Buc, E. Fox, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 32 , pp. 8024–8035. Curran Associates, Inc., 2019.
14Under review as submission to TMLR
Stefan T Radev, Ulf K Mertens, Andreas Voss, Lynton Ardizzone, and Ullrich Köthe. BayesFlow: Learning
complex stochastic models with invertible neural networks. IEEE Transactions on Neural Networks and
Learning Systems , 2020.
Poornima Ramesh, Jan-Matthis Lueckmann, Jan Boelts, Álvaro Tejero-Cantero, David S. Greenberg, Pe-
dro J. Goncalves, and Jakob H. Macke. GATSBI: Generative adversarial training for simulation-based
inference. In International Conference on Learning Representations , 2022. URL https://openreview.
net/forum?id=kR1hC6j48Tp .
EitanRichardsonandYairWeiss. OnGANsandGMMs. In Proceedings of the 32nd International Conference
on Neural Information Processing Systems , pp. 5852–5863, 2018.
Maria L Rizzo and Gábor J Székely. Energy distance. Wiley interdisciplinary reviews: Computational
statistics , 8(1):27–38, 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training GANs. Advances in neural information processing systems , 29, 2016.
LouisSharrock, JackSimons, SongLiu, andMarkBeaumont. Sequentialneuralscoreestimation: Likelihood-
free inference with conditional score based diffusion models, 2022.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Advances in neural information processing systems , 32, 2019.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint
arXiv:2011.13456 , 2020.
Gábor J Székely and Maria L Rizzo. A new test for multivariate normality. Journal of Multivariate Analysis ,
93(1):58–80, 2005.
Sean Talts, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. Validating Bayesian
inference algorithms with simulation-based calibration. arXiv preprint arXiv:1804.06788 , 2018.
Simon Tavaré, David J Balding, Robert C Griffiths, and Peter Donnelly. Inferring coalescence times from
DNA sequence data. Genetics , 145(2):505–518, 1997.
Yuexi Wang and Veronika Ročková. Adversarial Bayesian simulation. arXiv preprint arXiv:2208.12113 ,
2022.
Jonas Bernhard Wildberger, Maximilian Dax, Simon Buchholz, Stephen R Green, Jakob H. Macke, and
Bernhard Schölkopf. Flow matching for scalable simulation-based inference. In Thirty-seventh Conference
on Neural Information Processing Systems , 2023. URL https://openreview.net/forum?id=D2cS6SoYlP .
Samuel Wiqvist, Jes Frellsen, and Umberto Picchini. Sequential neural posterior and likelihood approxima-
tion, 2021.
A Sequential training
In the main text, we have considered the training data from the simulator model (θi,yi)n
i=1to be generated
independently from the observation on which inference is performed; under this assumption, we have dis-
cussed ways to learn posterior approximations valid for all values of ysuch thatp(y)>0. Once the neural
network is trained, therefore, inference can be performed for as many observations as we wish. This is a
so-called amortized setup (Radev et al., 2020).
However, practitioners may require posterior inference for a single observation yo. What they are interested
in, therefore, is the quality of the approximation for values of θwith large posterior density for the observed
yo. In this case, generating training samples independently from yomay be wasteful: a more efficient method
15Under review as submission to TMLR
(in terms of simulations from the model p(·|θ)) would generate more training samples θi’s close to the modes
of the true posterior, as those convey more information on the precise posterior shape. This can be done in a
sequential fashion: given a small amount of training data, a first approximation Qϕ1is obtained; from that,
additional training samples (θi,yi)are generated by θi∼Qϕ1(·|yo),yi∼P(·|θi)and used to (re-)train an
approximation Qϕ2. This procedure is iterated several times, allowing the training samples to progressively
focus around the posterior modes and thus refining the approximation (Lueckmann et al., 2017; Greenberg
et al., 2019).
However, naively following that strategy is incorrect. To see this, assume that, at the second round, we
just train on samples drawn from the approximate posterior ˜Π =Qϕ1(·|yo)obtained at the first round.
Such a sampled pair (θi,yi)was drawn from a joint density ˜π(θi)p(yi|θi) = ˜p(yi)˜π(θi|yi),where ˜πon the
left-hand side of the equality is the density of the proposal ˜Πand the quantities on the right-hand side are
univocally defined by the left-hand side. The optimal ϕ⋆obtained via SR-minimization thus corresponds to
qϕ⋆(·|y) = ˜π(·|y), which is not the correct target.
The traditional way to fix this entails introducing importance weights in the training objective (Eq. 4):
Eθ∼ΠEY∼P(·|θ)S(Qϕ(·|Y),θ) =Eθ∼˜Ππ(θ)
˜π(θ)EY∼P(·|θ)S(Qϕ(·|Y),θ).
As˜π(θ)cannot be evaluated, a solution is to fit a probabilistic classifier (at each round of the sequential
procedure) to samples from π(θ)and˜π(θ)and use it to estimate the ratioπ(θ)
˜π(θ). This classifier is not required
for the normalizing flows approaches, where the ratio can be evaluated explicitly (Lueckmann et al., 2017;
Greenberg et al., 2019) (unless the prior πis also defined implicitly, as in the camera model example in
Section 4). For GATSBI, a similar importance weights approach requires additionally to estimate the ratio
˜p(y)
p(y)(Ramesh et al., 2022).
An alternative approach, which was applied to GATSBI approach in Ramesh et al. (2022), involves correcting
the distribution of the variable Zwhich is transformed by the generative network. Specifically, Ramesh et al.
(2022) showed that π(θ|y) = ˜π(θ|y)w(θ,y)⇐⇒ ˜π(θ|y) =π(θ|y)(w(θ,y))−1, wherew(θ,y) =π(θ)
˜π(θ)˜p(y)
p(y).
Therefore you can consider a modified approximation ˜Qϕ(·|Y)and a new training objective:
EY∼˜PEθ∼˜Π(·|Y)S(˜Qϕ(·|Y),θ) =Eθ∼˜ΠEY∼P(·|θ)S(˜Qϕ(·|Y),θ) (9)
whose minimization leads to ˜Qϕ(·|Y) =˜Π(·|Y). By setting
˜Qϕ(·|Y) =Qϕ(·|Y)(w(θ,y))−1,
you ensure Qϕ(·|Y) = Π(·|Y). To trainϕusing the objective in Eq. (9), draws from ˜Qϕ(·|Y)are required;
those can be obtained by sampling z∼˜Pz, whose density is ˜pz(z) =pz(z)(w(gϕ(z,y),y))−1, and computing
θ=gϕ(z,y), which is thus a sample from ˜Qϕ(·|Y). Compared to using importance weights, the variance of
the training objective is here smaller. However, rejection sampling or MCMC are needed to sample from ˜Pz,
and two ratios have to be estimated via probabilistic classifiers (˜p(y)
p(y)andπ(θ)
˜π(θ)), making this strategy more
expensive than using importance weights
On the examples considered in Ramesh et al. (2022), the sequential approaches did not perform better than
the amortized one, mainly due to the additional computational cost associated to estimating the ratios. For
that reason, we did not investigate these methods for our approach.
B f-GAN
The problem in Eq. (7) can be obtained as a relaxation of the following one:
arg min
ϕEY∼P[DJS(Π(·|Y)∥Qϕ(·|Y))],
whereDJSis the Jensen-Shannon divergence. The objective in the above problem is 0 if and only if
Π(·|y) =Qϕ(·|y)for each y:p(y)>0. We report here a more general result by considering a class of
16Under review as submission to TMLR
divergences known as f-divergences, to which the Jensen-Shannon one belongs. We follow Nowozin et al.
(2016) in doing so5.
By temporarily discarding the dependence on Yand considering a reference distribution µwith resepct to
whichPandQϕare absolutely continuous, an f-divergence is defined as:
Df(P||Qϕ) =/integraldisplay
qϕ(θ)f/parenleftbiggp(θ)
qϕ(θ)/parenrightbigg
dµ(θ),
wheref:R+→Ris a convex, lower-semicontinuous function for which f(1) = 0, and where qϕandpare
densities of QϕandPwith respect to µ. We want now to fix ϕby:
arg min
ϕDf(P||Qϕ). (10)
Let now domfdenote the domain of f. By exploiting the Fenchel conjugate f∗(t) = supu∈domf{ut−f(u)},
Nowozin et al., 2016 obtain the following variational lower bound:
Df(P||Qϕ)≥sup
c∈C/parenleftig
Eθ∼Pc(θ)−E˜θ∼Qϕf∗(c(˜θ))/parenrightig
,
which holds for any set of functions CfromYtodomf∗. By considering a parametric set of functions
C={cψ:Y→ domf∗,ψ∈Ψ}, a surrogate to the problem in Eq. (10) becomes:
min
ϕmax
ψ/parenleftig
Eθ∼Pcψ(θ)−E˜θ∼Qϕf∗(cψ(˜θ))/parenrightig
.
By reintroducing the dependence on Y, the above generalizes to:
min
ϕmax
ψEY∼P/parenleftig
Eθ∼P(·|Y)cψ(θ,Y)−E˜θ∼Qϕ(·|Y)f∗(cψ(˜θ,Y))/parenrightig
, (12)
where now the function cψalso depends on the value of Y.
In practice, cψis parametrized by a Neural Network. To solve the problem in Eq. (12), people usually employ
alternating optimization over ϕandψby following stochastic gradients; this technique is called f-GAN. With
a finite number of steps over ψ, this leads to biased gradient estimates for ϕ. In Algorithm 2, we show a single
epoch (i.e. a loop on the full training dataset) of conditional f-GAN training; for simplicity, we consider
here using a single pair (θi,yi)to estimate the expectations in Eq. (12) (i.e., the batch size is 1), but using
a larger number of samples is possible. Notice how in Algorithm 2 we update the critic once every generator
update; however, multiple critic updates can be performed at each generator update.
Algorithm 2 Single epoch conditional f-GAN training.
Require: Parametric map gϕ, critic network cψ, learning rates ϵ,γ.
foreach training pair (θi,yi)do
Sample z∼Pz
Obtain ˜θϕ
i=gϕ(z,yi)
Setψ←ψ+γ·∇ψ/bracketleftig
cψ(θi,yi)−f∗(cψ(˜θϕ
i,yi))/bracketrightig
Setϕ←ϕ−ϵ·∇ϕ/bracketleftig
−f∗(cψ(˜θϕ
i,yi))/bracketrightig
end for
C Patched Scoring Rules
As mentioned in Section 3.1.1, the definition of patched SR given in Eq. (3) for Xrepresenting values on a
1D spatial grid can be generalised to n-dimensional spatial grids, with n>1, by considering sliding windows
5An analogous procedure allows to obtain a tractable training objective for the 1-Wasserstein distance as well (Arjovsky
et al., 2017)
17Under review as submission to TMLR
Stot = S1 + S2 + S3
Figure 5: Patched SR: a SR for multivariate data is computed on localized patches, and the resulting values
are summed. From Pacchiardi et al. (2022).
of dimension n. For instance, for X∈Rd1×Rd2, the definition can be generalised to:
˜Sp=⌈d1−s1
δ1+1⌉/summationdisplay
j=1⌈d2−s2
δ2+1⌉/summationdisplay
k=1S(P|j·s1:j·s1+δ1−1,k·s2:k·s2+δ2−1,xj·s1:j·s1+δ1−1,k·s2:k·s2+δ2−1),
wheres1ands2are the patch step for dimensions 1 and 2 respectively, and δ1andδ2are the patch sizes for
dimensions 1 and 2. A graphical representatio can be seen in Fig. 5
D Unbiased gradient estimates
We discuss here how we can obtain unbiased gradient estimates for the Scoring Rule training objective in
Eq. (5) with respect to the parameters of the generative network ϕ.
In order to do that, we first discuss how to obtain unbiased estimates of the SRs we use across this work.
Then, we show how those allow one to obtain unbiased gradient estimates. The steps we follow are the same
as in Pacchiardi et al. (2022) for the setting of probabilistic forecasting.
D.1 Unbiased scoring rule estimates
Assume we have draws ˜xj∼P,j= 1,...,m.
Energy Score An unbiased estimate of the energy score can be obtained by unbiasedly estimating the
expectations in S(β)
E(P,x)in Eq. (1):
ˆS(β)
E(P,x) =2
mm/summationdisplay
j=1∥˜xj−x∥β
2−1
m(m−1)m/summationdisplay
j,k=1
k̸=j∥˜xj−˜xk∥β
2.
18Under review as submission to TMLR
Kernel Score Similarly to the energy score, we obtain an unbiased estimate of Sk(P,x)in Eq. (2) by:
ˆSk(P,x) =1
m(m−1)m/summationdisplay
j,k=1
k̸=jk(˜xj,˜xk)−2
mm/summationdisplay
j=1k(˜xj,x).
Sum of SRs When adding multiple SRs, an unbiased gradient of the sum can be obtained by adding
unbiased estimates of the two addends.
D.2 Unbiased estimate of the training objective
Recall now that we want to solve:
ˆϕ:= arg min
ϕJ(ϕ), J (ϕ) =1
nn/summationdisplay
i=1S(Qϕ(·|yi),θi). (13)
To do this, we exploit Stochastic Gradient Descent (SGD), which requires unbiased estimates of J(ϕ). Notice
how, for all the Scoring Rules used across this work, as well as any weighted sum of those, we can write:
S(P,x) =E˜X,˜X′∼P/bracketleftbig
h(˜X,˜X′,x)/bracketrightbig
for some function h; namely, the SR is defined through an expectation over
(possibly multiple) samples from P. That is the form exploited in Appendix D.1 to obtain unbiased SR
estimates.
Now, we will use this fact to obtain unbiased estimates for the objective in Eq. (13).
J(ϕ) =1
nn/summationdisplay
i=1E˜θ,˜θ′∼Qϕ(·|yi)/bracketleftbig
h(˜θ,˜θ′,θi)/bracketrightbig
=1
nn/summationdisplay
i=1EZ,Z′∼Pz[h(gϕ(Z,yi),gϕ(Z′,yi),θi)],
where we used the fact that Qϕis the distribution induced by a generative network with transformation gϕ;
this is called the reparametrization trick (Kingma & Welling, 2014). Now:
∇ϕJ(ϕ) =∇ϕ1
nn/summationdisplay
i=1EZ,Z′∼Pz[h(gϕ(Z,yi),gϕ(Z′,yi),θi)]
=1
nn/summationdisplay
i=1EZ,Z′∼Pz[∇ϕh(gϕ(Z,yi),gϕ(Z′,yi),θi)].
In the latter equality, the exchange between expectation and gradient is not a trivial step, due to the non-
differentiability of functions (such as ReLU) used in gϕ. Fortunately, Theorem 5 in Bińkowski et al. (2018)
proved that to be valid almost surely with respect to a measure on the space Φto which the weights of the
neural network ϕbelong, under mild conditions on the NN architecture.
We can now easily obtain an unbiased estimate of the above using samples zi,j∼Pz,j= 1,...,m, for each
i∈{1,...,n}. Additionally, Stochastic Gradient Descent usually considers a small batch of training samples
at each step, obtained by taking a random subset (or batch) B⊆{ 1,2,...,n}. Therefore, the following
unbiased estimate of ∇ϕJ(ϕ)can be obtained:
\∇ϕJ(ϕ) =1
|B|/summationdisplay
i∈B1
m(m−1)m/summationdisplay
j,k=1
j̸=k∇ϕh(gϕ(zi,j;yi),gϕ(zi,k;yi),θi).
In practice, the above is obtained by computing the gradient of the following unbiased estimate of J(ϕ)via
autodifferentiation libraries (see for instance Paszke et al., 2019):
ˆJ(ϕ) =1
|B|/summationdisplay
i∈B1
m(m−1)m/summationdisplay
j,k=1
j̸=kh(gϕ(zi,j;yi),gϕ(zi,k;yi),θi).
19Under review as submission to TMLR
In Algorithm 1, we train a generative network for a single epoch using a scoring rule Sfor which unbiased
estimators can be obtained by using m > 1samples from Qϕ. Compare it with the adversarial approach
reported in Algorithm 2; in the SR approach, multiple samples from the generative networks are required at
each step (m> 1), while a unique one is enough for the adversarial approach. Conversely, the SR approach
does not require an additional critic network and learning rate γand is simpler and faster to train (see the
results in Sec. 4 and Pacchiardi et al., 2022 for more details). As in Algorithm 2 , we use a single pair (θi,yi)
to estimate the gradient.
E Details on performance measures
Here, wereviewthemeasuresofperformanceusedintheempiricalstudies. Allthesemetricsareforunivariate
θ; when handling multivariate θ, we therefore compute them on each dimension separately and report the
average.
E.1 Calibration measures
Here, we review two measures of calibration of a probabilistic forecast. Both measures consider the univariate
marginals of the approximate posterior distribution Qϕ(·|yi); for the component l, let us denote it by
Qϕ,l(·|yi). We follow Radev et al. (2020) in defining these measures and report them here for ease of
reference.
E.1.1 Calibration error
The calibration error (Radev et al., 2020) quantifies how well the credible intervals of the approximate
posteriorsQϕ,l(·|yi)for different yimatch the empirical distribution of θi,l. Specifically, let α(l)be the
proportion of times the verification θi,lfalls into an α-credible interval of Qϕ,l(·|yi), computed over all values
ofi. If the marginal forecast distribution is perfectly calibrated for component l,α(l) =αfor all values of
α∈(0,1).
Therefore, we define the calibration error as the median of |α(l)−α|over 100 equally spaced values of
α∈(0,1). Therefore, the calibration error is a value between 0and 1, where 0denotes perfect calibration.
In practice, the credible intervals of the predictive are estimated using a set of samples from Qϕ(·|yi).
The calibration error can be related to the strong calibration of Cockayne et al. (2022), which implies correct
coverage for credible sets (see their Remark 2.9).
E.1.2 Simulation-Based Calibration (SBC)
SBC (Talts et al., 2018) tests a self-consistency property of the Bayesian posterior in a posterior approxi-
mation. In fact, by assuming for simplicity that densities with respect to the Lebesgue measure exist, the
Bayesian posterior satisfies the following equality:
π(θ) =/integraldisplay
p(θ,˜θ,˜y)d˜yd˜θ=/integraldisplay
p(θ,˜y|˜θ)π(˜θ)d˜yd˜θ=/integraldisplay
π(θ|˜y)p(˜y|˜θ)π(˜θ)d˜yd˜θ (14)
in practice, this means that, if you sample from the prior ˜θ∼π, use that to generate a sample from the
likelihood ˜y∼p(·|θ)and use the latter in turn to generate a posterior sample θ∼π(·|˜y),θis distributed
according to the prior π(θ). If you repeat the same procedure by sampling θfrom anapproximate posterior,
sayθ∼Qϕ(·|˜y), then θ∼πis a necessary condition for qϕ(·|y) =π(·|y), i.e. for the approximate posterior
to be exact. Notice, however, how this is nota sufficient condition: the equality can be satisfied even if
qϕ(·|y)is different from the posterior (it is in fact trivially satisfied qϕ(·|y) =π, i.e., when the approximate
posterior corresponds to the prior).
A way to empirically test the above property involves, for a given prior sample ˜θ, drawing from the likelihood
multiple times yi∼p(·|˜θ),i= 1,...,Nand, for each of these, obtaining a single approximate posterior
sample θi∼qϕ(·|yi). Given these, you compute the rank of ˜θ:r=/summationtextN
i=11[θi<˜θ](this only makes sense
20Under review as submission to TMLR
ifθis univariate; otherwise, you compute the rank independently for each dimension of θ). If θi’s were
effectively distributed from the prior, ris a uniform random variable on {1,2,...,N}. Therefore, repeating
this procedure for different prior samples ˜θand visualizing the distribution of the resulting r’ s (for instance,
through a histogram or by plotting the CDF) gives an indication of whether an equivalence such as Eq. (14)
is satisfied for qϕ. See Algorithm 2 in Radev et al. (2020) for a precise description of this procedure, which
goes under the name of Simulation-Based Calibration (SBC). SBC tests the weak calibration of Cockayne
et al. (2022); additionally, it is closely related to the concept of probabilistic calibration and rank histogram
in the framework of probabilistic forecasting (Gneiting et al., 2007).
E.2 Continuous Ranked Probability Score (CRPS)
For a continuous scalar variable θand a distribution Qϕ, the CRPS (Gneiting & Raftery, 2007) is a strictly
properScoringRuledefinedbyconsideringthecumulativedistributionfunction FQϕofQϕandbycomputing:
SCRPS (Qϕ,θ) =/integraldisplay∞
−∞(FQϕ(˜θ)−1{˜θ≥θ})2d˜θ. (15)
In general, analytically computing the integral in Eq. (15) is undoable, except in simple cases such as
Gaussian distributions. However, the following alternative formulation (Eq. 17 in Székely & Rizzo, 2005)
can be estimated using samples from Qϕ:
SCRPS (Qϕ,θ) = 2·E/bracketleftbig
|˜θ−θ|2/bracketrightbig
−E/bracketleftbig
|˜θ−˜θ′|2/bracketrightbig
,˜θ⊥ ⊥˜θ′∼Qϕ. (16)
From Eq. (16), it is clear how the CRPS is a specific case of the Energy Score (Section 3.1.1) for scalar
variables.
In evaluating the approximate posterior distribution Qϕ(·|yi)obtained from one of the various SBI methods,
we draw a set of samples from Qϕ(·|yi)and estimate the CRPS for each dimension of the parameter space
using Eq. (16); we then compute the average and standard deviation over the various dimensions and then
average over various observations yi. Recall how the pairs (θi,yi)n
i=1are generated from the prior θi∼Π
and the model yi∼P(·|θi), which can also be considered as samples from the data marginal yi∼Pand
the posterior θi∼Π(·|yi). Hence, the marginal of the exact posterior minimizes the expected CRPS in
each dimension, of which the empirical average over various dimensions is a good estimate. This metric is
therefore smaller for methods which approximate better the marginals of the true posterior.
F Experimental details
Precise configuration details can be found in the code accompanying the paper <link removed for
anonymity>.
F.1 Shallow Water Model
We train all methods for at most 40k epochs on 100k training samples. For ScoRuTSBI, we tried both m= 3
andm= 10, with the latter resulting in improved performance; all the results reported in the paper refer to
m= 10.
GATSBI used a batch size of 125 (as in Ramesh et al., 2022), while ScoRuTSBI used a batch size of 60
(otherwise, GPU memory overflow occurs).
Recall that the parameters θ∈R100represent the depth of a 1-dimensional water basin at equidistant points.
When using the patched SR configuration, we consider patches of size patch_size disposed at a distance
patch_step from each other. Therefore, the number of patches is
n_patches = (100−patch_size )/patch_step + 1.
We used therefore the following patched SR configurations on the 1D grid:
21Under review as submission to TMLR
1.patch_size = 10andpatch_step = 5, resulting in n_patches = 19.
2.patch_size = 20andpatch_step = 10, resulting in n_patches = 9.
The patched SR is added to the overall score over the full parameter space.
The training time (per epoch) is roughly constant in the un-patched and the two different patched configu-
rations.
F.2 Camera Model
We train all methods for at most 10k epochs on 800k training samples. For ScoRuTSBI, we tried both m= 3
andm= 10, with the latter resulting in better performance.
Both ScoRuTSBI and GATSBI methods used a batch size of 800as in Ramesh et al. (2022).
Here, the parameters θare on a 28×28square grid. When using the patched SR configuration, we consider
patches of size patch_size×patch_size disposed at a distance patch_step from each other in both spatial
dimensions. The number of patches is obtained as
n_patches = [(28−patch_size )/patch_step + 1]2.
We used therefore the following patched SR configurations on the 2D grid:
1.patch_size = 14andpatch_step = 7, resulting in n_patches = 9.
2.patch_size = 8andpatch_step = 5, resulting in n_patches = 25.
The patched SR is added to the overall score over the full parameter space.
The training time (per epoch) is roughly constant in the un-patched and the two different patched configu-
rations.
G Additional experimental results on models considered in the main body
22Under review as submission to TMLR
1000 10 000 100 000
Number of simulations0.8750.9000.9250.9500.9751.000C2ST (accuracy)SLCP
1000 10 000 100 000
Number of simulations0.70.80.9C2ST (accuracy)T wo Moons
GATSBI
Energy 3
Energy 5
Energy 10
Energy 20
Kernel 3
Kernel 5
Kernel 10
Kernel 20A B
Figure 6: C2ST for the SLCP and Two Moons benchmarks for ScoRuTSBI with the Energy and Kernel
Score and GATSBI; larger values are worse. For ScoRuTSBI, we report results for different choices of the
number of generative network samples mused in training. SLCP: GATSBI performs better, but poorly on
an absolute scale. Two Moons: ScoRuTSBI with the Energy Score perform better. See Fig 1 for results on
all considered methods.
G.1 SLCP
The left panel of Fig. 6 reports the C2ST for multiple values of mandntrainfor ScoRuTSBI with the Energy
and the Kernel Score, together with values obtained with GATSBI.
InFigure7,wereporttheposteriorsamplesobtainedwiththeScoRuTSBIwiththeEnergyScorewith m= 20
and compare them with the samples from the reference posterior. The corresponding plot for GATSBI is
shown in Fig. 8. In Figure 9, we report Simulation-Based Calibration results (see Appendix E.1.2): for each
dimension of θ, the corresponding histogram represents the distribution of the rank of the true parameter
value in a set of samples from the approximate posterior. We show that for GATSBI and ScoRuTSBI with
the Energy Score with m= 20.
Tables 5, 6, 7, 8 and 9 report the different performance metrics, the runtime, and the early stopping epoch for
all methods (columns) and all number of training samples (rows); for Energy and Kernel Score, the number
in the column header denotes the number of draws from the generative network during training for each yi
in the training batch.
Table 5: SLCP: classification-based two-sample test (C2ST); lower is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 0.97±0.02 0.99±0.01 0.99±0.01 0.99±0.00 0.99±0.01 1.00±0.01 0.99±0.01 0.99±0.01 0.99±0.01
10000 0.94±0.03 0.98±0.01 0.97±0.01 0.98±0.01 0.98±0.01 0.99±0.01 0.99±0.01 0.99±0.01 0.99±0.01
100000 0.92±0.03 0.97±0.01 0.97±0.02 0.96±0.02 0.95±0.02 0.98±0.01 0.98±0.01 0.98±0.01 0.98±0.01
Table 6: SLCP: calibration error; lower is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 0.10±0.06 0.11±0.07 0.13±0.05 0.04±0.01 0.18±0.06 0.17±0.10 0.17±0.09 0.08±0.06 0.07±0.05
10000 0.07±0.06 0.05±0.04 0.05±0.04 0.07±0.05 0.06±0.04 0.09±0.07 0.08±0.06 0.09±0.07 0.09±0.07
100000 0.05±0.03 0.06±0.04 0.06±0.03 0.05±0.03 0.05±0.02 0.09±0.07 0.08±0.05 0.07±0.03 0.08±0.05
23Under review as submission to TMLR
Figure 7: SLCP: posterior samples for ScoRuTSBI with the Energy Score trained with m= 20and refer-
ence posterior samples. Diagonal panels represent univariate marginals, while off-diagonals panels represent
bivariate marginals.
24Under review as submission to TMLR
Figure 8: SLCP: posterior samples for GATSBI and reference posterior samples. Diagonal panels represent
univariate marginals, while off-diagonals panels represent bivariate marginals.
25Under review as submission to TMLR
0 250 500 750 1000
Rank statistic1
0 250 500 750 1000
Rank statistic2
0 250 500 750 1000
Rank statistic3
0 250 500 750 1000
Rank statistic4
0 250 500 750 1000
Rank statistic5
(a) GATSBI
0 250 500 750 1000
Rank statistic1
0 250 500 750 1000
Rank statistic2
0 250 500 750 1000
Rank statistic3
0 250 500 750 1000
Rank statistic4
0 250 500 750 1000
Rank statistic5
(b) ScoRuTSBI WITH Energy Score, m= 20
Figure 9: SLCP: Simulation-Based Calibration results represented as rank histograms; for each dimension
ofθ, the corresponding histogram represents the distribution of the rank of the true parameter value in a
set of samples from the approximate posterior. If the approximate posterior is calibrated, histogram bars
should be in the grey region with 99% probability.
Table 7: SLCP: CRPS; smaller is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 1.57±0.27 1.80±0.41 1.86±0.42 1.25±0.25 1.92±0.49 1.91±0.48 1.92±0.46 1.32±0.31 1.29±0.27
10000 1.37±0.31 1.50±0.35 1.48±0.35 1.51±0.36 1.49±0.36 1.55±0.39 1.55±0.39 1.56±0.39 1.57±0.40
100000 1.37±0.30 1.45±0.36 1.41±0.36 1.38±0.36 1.35±0.36 1.47±0.42 1.49±0.40 1.47±0.38 1.46±0.40
Table 8: SLCP: runtime in seconds; recall that GATSBI was trained on GPU while ScoRuTSBI were trained
on a single CPU.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 4796 654 692 620 885 515 531 682 1330
10000 9671 651 658 639 720 636 658 655 697
100000 30963 1060 1160 1305 1645 1245 1044 1057 1210
26Under review as submission to TMLR
Table 9: SLCP: epoch at which early stopping occurred; the max number of training epochs was 20000.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 20000 1000 1000 1000 1100 1100 1000 1000 1000
10000 20000 1100 1000 1100 1100 1100 1100 1000 1000
100000 20000 1000 1200 1500 2100 1600 1100 1000 1200
27Under review as submission to TMLR
G.2 Two Moons
The right panel of Fig. 6 reports the C2ST for multiple values of mandntrainfor ScoRuTSBI with the
Energy and the Kernel Score, together with values obtained with GATSBI.
In Figure 10, we report posterior samples obtained with ScoRuTSBI with the Energy Score with m= 20
and compare them with samples from the reference posterior. The corresponding plot for GATSBI is shown
in Fig. 11. In Figure 12, we report Simulation-Based Calibration results (see Appendix E.1.2): for each
dimension of θ, the corresponding histogram represents the distribution of the rank of the true parameter
value in a set of samples from the approximate posterior. We show that for GATSBI and for ScoRuTSBI
with the Energy Score with m= 20.
Tables 10, 11, 12, 13 and 14 report the different performance metrics, the runtime and the early stopping
epoch for all methods (columns) and all number of training samples (rows); for Energy and Kernel Score,
the number in the column header denotes the number of draws from the generative network during training
for each yiin the training batch.
Table 10: Two Moons: classification-based two-sample test (C2ST); lower is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 0.85±0.05 0.85±0.06 0.87±0.05 0.85±0.03 0.85±0.04 0.94±0.03 0.94±0.02 0.93±0.03 0.96±0.02
10000 0.81±0.03 0.79±0.04 0.76±0.05 0.76±0.04 0.74±0.07 0.92±0.03 0.93±0.01 0.91±0.03 0.93±0.01
100000 0.82±0.07 0.79±0.03 0.74±0.06 0.73±0.05 0.73±0.04 0.90±0.04 0.92±0.03 0.90±0.02 0.92±0.02
Table 11: Two Moons: calibration error; lower is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 0.06±0.01 0.04±0.01 0.08±0.02 0.06±0.01 0.05±0.00 0.04±0.00 0.05±0.01 0.07±0.01 0.04±0.01
10000 0.05±0.01 0.04±0.02 0.03±0.01 0.04±0.03 0.03±0.01 0.05±0.02 0.06±0.00 0.02±0.01 0.05±0.00
100000 0.05±0.01 0.04±0.01 0.03±0.00 0.03±0.02 0.03±0.00 0.08±0.01 0.05±0.01 0.04±0.02 0.04±0.00
Table 12: Two Moons: CRPS; smaller is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 0.36±0.00 0.36±0.00 0.36±0.00 0.36±0.00 0.36±0.00 0.39±0.00 0.39±0.00 0.40±0.01 0.40±0.01
10000 0.36±0.00 0.36±0.00 0.36±0.00 0.35±0.00 0.35±0.00 0.36±0.00 0.37±0.00 0.36±0.00 0.36±0.00
100000 0.36±0.00 0.36±0.00 0.36±0.00 0.35±0.00 0.35±0.00 0.36±0.00 0.36±0.00 0.36±0.00 0.36±0.00
28Under review as submission to TMLR
Figure 10: Two Moons: posterior samples for ScoRuTSBI with the Energy Score trained with m= 20
and reference posterior samples. Diagonal panels represent univariate marginals, while off-diagonal panels
represent bivariate marginals.
Figure 11: Two Moons: posterior samples for GATSBI and reference posterior samples. Diagonal panels
represent univariate marginals, while off-diagonal panels represent bivariate marginals.
29Under review as submission to TMLR
0 200 400 600 800 1000
Rank statistic1
0 200 400 600 800 1000
Rank statistic2
(a) GATSBI
0 200 400 600 800 1000
Rank statistic1
0 200 400 600 800 1000
Rank statistic2
(b) ScoRuTSBI with Energy Score, m= 20
Figure 12: Two Moons: Simulation-Based Calibration results represented as rank histograms; for each
dimension of θ, the corresponding histogram represents the distribution of the rank of the true parameter
valueinasetofsamplesfromtheapproximateposterior. Iftheapproximateposterioriscalibrated, histogram
bars should be in the grey region with 99% probability.
Table 13: Tow Moons: runtime in seconds; recall that GATSBI was trained on GPU while ScoRuTSBI were
trained on a single CPU.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 4799 578 690 759 896 585 613 651 852
10000 8163 1775 1917 2415 3228 1708 1883 2329 3267
100000 30232 9266 9388 9903 10805 9283 9479 9859 10902
Table 14: Two Moons: epoch at which early stopping occurred; the max number of training epochs was
20000.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 20000 20000 20000 20000 20000 20000 20000 20000 20000
10000 20000 20000 20000 20000 20000 20000 20000 20000 20000
100000 20000 20000 20000 20000 20000 20000 20000 20000 20000
30Under review as submission to TMLR
G.3 Shallow Water Model
In Figure 13, we show results, analogously to what done in Figure 2, for all methods. Table 15 reports the
different performance metrics, the runtime and the early stopping epoch for all methods. Finally, Figure 14
reports Simulation-Based Calibration results for all tested configurations of ScoRuTSBI.
Table 15: Shallow Water model: performance metrics, runtime and early stopping epoch for all methods.
We do not train GATSBI from scratch but rather relied on the trained network obtained in Ramesh et al.
(2022). The training time we report here is what is mentioned in Ramesh et al. (2022), which used two GPUs
for training (in contrast, we used a single GPU for ScoRuTSBI). For the same reason, we do not report the
epoch at which GATSBI training was early stopped.
Cal. Err.↓CRPS↓Runtime (sec) Early stopping epoch
Energy 0.03 ±0.02 1.46±0.34 51328 10400
Energy patched 10 20 0.03 ±0.02 0.99±0.53 60017 12400
Energy patched 5 10 0.03 ±0.02 1.54±0.37 49626 9600
Kernel 0.11 ±0.05 1.24±0.90 39608 7800
Kernel patched 10 20 0.09 ±0.04 1.40±0.86 47642 9000
Kernel patched 5 10 0.09 ±0.04 1.29±0.65 44590 9200
GATSBI 0.12 ±0.09 1.43±0.91≈345600 -
31Under review as submission to TMLR
01020Depth profileGround truth
Prior samples  Time1
22
50
69
940.03
0.000.03Amplitudet = 22 t = 69 t = 94
01020Depth profileGATSBI
  Time1
22
50
69
940.03
0.000.03Amplitude
01020Depth profileENERGY SCORE 
  Time1
22
50
69
940.03
0.000.03Amplitude
01020Depth profileENERGY SCORE 5 10
  Time1
22
50
69
940.03
0.000.03Amplitude
01020Depth profileENERGY SCORE 10 20
  Time1
22
50
69
940.03
0.000.03Amplitude
01020Depth profileKERNEL SCORE 
  Time1
22
50
69
940.03
0.000.03Amplitude
01020Depth profileKERNEL SCORE 5 10
  Time1
22
50
69
940.03
0.000.03Amplitude
1 50 100
Position01020Depth profileKERNEL SCORE 10 20
  Position1 50 100  Time1
22
50
69
94
1 50 100
Position0.03
0.000.03Amplitude
1 50 100
Position1 50 100
Position
Figure 13: Shallow water model: inference results with all methods. See Figure 2 for a description of the
different panels.
32Under review as submission to TMLR
0 500 1000
Rank0.00.51.0CDF
Uniform CDF
Energy Score
(a) Energy Score
0 500 1000
Rank0.00.51.0CDF
Uniform CDF
Kernel Score(b) Kernel Score
0 500 1000
Rank0.00.51.0CDF
Uniform CDF
Energy Score
(c) Energy Score patched 5, 10
0 500 1000
Rank0.00.51.0CDF
Uniform CDF
Kernel Score(d) Kernel Score patched 5, 10
0 500 1000
Rank0.00.51.0CDF
Uniform CDF
Energy Score
(e) Energy Score patched 10, 20
Kernel Score
0 500 1000
Rank0.00.51.0CDF
Uniform CDF
Kernel Score(f) Kernel Score patched 10, 20
Figure 14: Shallow Water model: Simulation Based Calibration for ScoRuTSBI using different scoring rules.
Each line corresponds to a single dimension of θand represents the CDF of the rank of the true parameter
value with respect to a set of posterior samples. A calibrated posterior implies uniform CDF (diagonal black
line, with associated 99% confidence region for the considered number of samples in gray).
G.4 Camera model
In Figure 15, we show results, analogously to what is done in Figure 4, for all methods. Table 16 reports
the different performance metrics, runtime, and early stopping epoch for all methods.
33Under review as submission to TMLR
yGATSBI
Figure 15: Noisy Camera model: ground truth and posterior inference with all methods, for a set of obser-
vations (each observation corresponds to a column). The first two rows represent the ground truth values of
θand the corresponding observation yo. The remaining rows represent the mean and Standard Deviation
(SD) for all methods.
34Under review as submission to TMLR
Table 16: Noisy Camera model: performance metrics, runtime and early stopping epoch for all methods.
Cal. Err.↓CRPS↓Runtime (sec) Early stopping epoch
GATSBI 0.50 ±0.01 0.28±0.26 45398 3600
Energy 0.36 ±0.12 0.05±0.04 24555 4200
Energy patched 5 8 0.37 ±0.12 0.04±0.03 22633 4000
Energy patched 7 14 0.37 ±0.12 0.05±0.03 24033 3600
Kernel 0.33 ±0.15 0.05±0.03 21862 3200
Kernel patched 5 8 0.36 ±0.12 0.06±0.04 22545 3200
Kernel patched 7 14 0.38 ±0.11 0.07±0.04 20605 3600
H Experiments on additional low-dimensional benchmarks
Here, wereportresultsonthreeadditionallow-dimensionalbenchmarkmodelsfromLueckmannet al. (2021):
the Gaussian Mixture, Gaussian Linear and Bernoulli GLM models. The setup is the same as considered
in Section 4.1 in the main body. We obtained here results with ScoRuTSBI with the Energy and Kernel
Score and with GATSBI; we also report the C2ST performance results for the Flow Matching method of
Wildberger et al. (2023). All methods are tested with 1000, 10000 and 100000 samples from the simulator
model. Moreover, we evaluate ScoRuTSBI wth the Energy and Kernel score with m= 3,5,10and20.
1000 10 000 100 000
Number of simulations0.800.850.900.95C2ST (accuracy)Gaussian Mixture
1000 10 000 100 000
Number of simulations0.8750.9000.9250.9500.9751.000C2ST (accuracy)Gaussian Linear
1000 10 000 100 000
Number of simulations0.920.940.960.981.00C2ST (accuracy)Bernoulli GLM
GATSBI
Energy 3
Energy 5
Energy 10
Energy 20
Kernel 3
Kernel 5
Kernel 10
Kernel 20A B C
(a) Results with ScoRuTSBI with the Energy and Kernel Score and GATSBI.
1000 10 000 100 000
Number of simulations0.50.60.70.80.91.0C2ST (accuracy)Gaussian Mixture
1000 10 000 100 000
Number of simulations0.50.60.70.80.91.0C2ST (accuracy)Gaussian Linear
1000 10 000 100 000
Number of simulations0.50.60.70.80.91.0C2ST (accuracy)Bernoulli GLM
GATSBI
Energy 3
Energy 5
Energy 10
Energy 20
Kernel 3
Kernel 5
Kernel 10
Kernel 20
Flow MatchingA B C
(b) Results with ScoRuTSBI with the Energy and Kernel Score, GATSBI and Flow Matching.
Figure 16: C2ST for the Gaussian Mixture, Gaussian Linear and Bernoulli GLM benchmarks for ScoRuTSBI
with the Energy and Kernel Score, GATSBI and Flow Matching; larger values are worse. For ScoRuTSBI,
we report results for different choices of the number of generative network samples mused in training.
H.1 Gaussian Mixture
The left panel of Fig. 16 reports the C2ST for multiple values of mandntrainfor ScoRuTSBI with the
Energy and the Kernel Score, together with values obtained with GATSBI and Flow Matching.
In Figure 17, we report the posterior samples obtained with the ScoRuTSBI with the Energy Score with
m= 20and compare them with the samples from the reference posterior. The corresponding plot for
GATSBI is shown in Fig. 18.
35Under review as submission to TMLR
Figure 17: Gaussian Mixture: posterior samples for ScoRuTSBI with the Energy Score trained with m= 20
and reference posterior samples. Diagonal panels represent univariate marginals, while off-diagonals panels
represent bivariate marginals.
Figure 18: Gaussian Mixture: posterior samples for GATSBI and reference posterior samples. Diagonal
panels represent univariate marginals, while off-diagonals panels represent bivariate marginals.
36Under review as submission to TMLR
Tables 17, 18, 19, 20 and 21 report the different performance metrics, the runtime, and the early stopping
epoch for all methods (columns) and all number of training samples (rows); for Energy and Kernel Score,
the number in the column header denotes the number of draws from the generative network during training
for each yiin the training batch.
Table 17: Gaussian Mixture: classification-based two-sample test (C2ST); lower is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 0.88±0.04 0.84±0.03 0.85±0.03 0.84±0.03 0.84±0.04 0.85±0.03 0.87±0.05 0.87±0.03 0.85±0.03
10000 0.86±0.03 0.83±0.02 0.83±0.03 0.83±0.03 0.82±0.03 0.85±0.04 0.86±0.04 0.86±0.04 0.85±0.04
100000 0.91±0.05 0.83±0.03 0.83±0.03 0.83±0.03 0.81±0.03 0.87±0.04 0.86±0.05 0.86±0.03 0.84±0.05
Table 18: Gaussian Mixture: calibration error; lower is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 0.05±0.00 0.29±0.02 0.28±0.03 0.30±0.00 0.31±0.03 0.25±0.04 0.22±0.02 0.22±0.03 0.19±0.01
10000 0.32±0.03 0.30±0.00 0.32±0.00 0.31±0.03 0.32±0.01 0.25±0.01 0.19±0.01 0.20±0.01 0.21±0.01
100000 0.07±0.03 0.31±0.03 0.33±0.00 0.32±0.03 0.35±0.00 0.18±0.02 0.22±0.04 0.19±0.02 0.19±0.02
Table 19: Gaussian Mixture: CRPS; smaller is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 0.73±0.10 0.48±0.01 0.53±0.03 0.47±0.04 0.48±0.01 0.57±0.06 0.51±0.02 0.51±0.00 0.51±0.00
10000 0.53±0.05 0.48±0.02 0.49±0.00 0.46±0.01 0.49±0.01 0.48±0.02 0.47±0.01 0.46±0.01 0.42±0.01
100000 0.94±0.04 0.49±0.01 0.47±0.01 0.47±0.00 0.50±0.00 0.49±0.00 0.47±0.06 0.43±0.00 0.42±0.00
37Under review as submission to TMLR
Table 20: Gaussian Mixture: runtime in seconds; recall that GATSBI was trained on GPU while ScoRuTSBI
were trained on a single CPU.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 4501.3 948.97 750.26 992.35 542.07 871.12 907.99 931.11 802.14
10000 10443.7 680.72 944.3 1323.25 1533.87 1136.34 800.03 846.62 1042.59
100000 33809.3 1435.79 1925.08 1616.09 2659.08 1416.08 1442.19 2235.21 1948.88
Table 21: Gaussian Mixture: epoch at which early stopping occurred; the max number of training epochs
was 20000.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 20000 1100 1000 1200 1000 1000 1000 1100 1100
10000 20000 1100 1100 1000 1200 1200 1000 1100 1200
100000 20000 1200 1100 1200 2000 1000 1100 1500 1000
H.2 Gaussian Linear
The middle panel of Fig. 16 reports the C2ST for multiple values of mandntrainfor ScoRuTSBI with the
Energy and the Kernel Score, together with values obtained with GATSBI and Flow Matching.
In Figure 19, we report the posterior samples obtained with the ScoRuTSBI with the Energy Score with
m= 20and compare them with the samples from the reference posterior. The corresponding plot for
GATSBI is shown in Fig. 20.
Tables 22, 23, 24, 25 and 26 report the different performance metrics, the runtime, and the early stopping
epoch for all methods (columns) and all number of training samples (rows); for Energy and Kernel Score,
the number in the column header denotes the number of draws from the generative network during training
for each yiin the training batch.
Table 22: Gaussian Linear: classification-based two-sample test (C2ST); lower is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 0.91±0.02 0.99±0.00 0.99±0.00 0.99±0.00 0.99±0.00 0.99±0.00 0.99±0.00 0.99±0.00 0.99±0.00
10000 0.88±0.01 0.97±0.01 0.97±0.01 0.94±0.01 0.95±0.01 0.98±0.00 0.97±0.01 0.97±0.00 0.96±0.01
100000 0.89±0.02 0.97±0.01 0.94±0.01 0.94±0.01 0.93±0.01 0.97±0.01 0.97±0.01 0.96±0.01 0.96±0.01
38Under review as submission to TMLR
Figure 19: Gaussian Linear: posterior samples for ScoRuTSBI with the Energy Score trained with m= 20
and reference posterior samples. Diagonal panels represent univariate marginals, while off-diagonals panels
represent bivariate marginals.
39Under review as submission to TMLR
Figure 20: Gaussian Linear: posterior samples for GATSBI and reference posterior samples. Diagonal panels
represent univariate marginals, while off-diagonals panels represent bivariate marginals.
40Under review as submission to TMLR
Table 23: Gaussian Linear: calibration error; lower is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 0.05±0.03 0.17±0.03 0.14±0.03 0.15±0.03 0.14±0.03 0.22±0.02 0.20±0.04 0.18±0.03 0.17±0.03
10000 0.04±0.02 0.11±0.01 0.10±0.02 0.07±0.03 0.07±0.02 0.13±0.02 0.11±0.02 0.11±0.03 0.10±0.03
100000 0.04±0.02 0.08±0.01 0.06±0.02 0.06±0.01 0.06±0.03 0.11±0.01 0.11±0.02 0.10±0.03 0.09±0.02
Table 24: Gaussian Linear: CRPS; smaller is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 0.28±0.01 0.32±0.01 0.31±0.01 0.31±0.01 0.31±0.01 0.33±0.01 0.32±0.01 0.31±0.01 0.31±0.01
10000 0.27±0.01 0.30±0.01 0.29±0.01 0.27±0.01 0.27±0.01 0.29±0.01 0.29±0.01 0.28±0.01 0.28±0.01
100000 0.26±0.00 0.29±0.01 0.27±0.00 0.27±0.01 0.27±0.01 0.28±0.01 0.28±0.01 0.28±0.01 0.27±0.01
Table 25: Gaussian Linear: runtime in seconds; recall that GATSBI was trained on GPU while ScoRuTSBI
were trained on a single CPU.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 13782.9 379.55 738.16 836.55 820.39 521.75 497.16 574.23 663.2
10000 21944.3 1537.55 1604 2294.72 1811.87 1408.64 1266.38 1194.58 2173.2
100000 48610.9 2380.17 4734.9 4801.3 5421.97 2484.7 2805.37 2858.23 3566.49
Table 26: Gaussian Linear: epoch at which early stopping occurred; the max number of training epochs was
20000.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 20000 1000 1400 1000 1000 1000 1000 1200 1100
10000 20000 1100 1100 1800 1700 1200 1300 1400 2000
100000 20000 1500 2600 1900 2500 1800 1700 2000 2100
41Under review as submission to TMLR
H.3 Bernoulli GLM
The right panel of Fig. 16 reports the C2ST for multiple values of mandntrainfor ScoRuTSBI with the
Energy and the Kernel Score, together with values obtained with GATSBI and Flow Matching.
In Figure 21, we report the posterior samples obtained with the ScoRuTSBI with the Energy Score with
m= 20and compare them with the samples from the reference posterior. The corresponding plot for
GATSBI is shown in Fig. 22.
Tables 27, 28, 29, 30 and 31 report the different performance metrics, the runtime, and the early stopping
epoch for all methods (columns) and all number of training samples (rows); for Energy and Kernel Score,
the number in the column header denotes the number of draws from the generative network during training
for each yiin the training batch.
Table 27: Bernoulli GLM: classification-based two-sample test (C2ST); lower is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 0.97±0.02 0.99±0.01 0.99±0.00 0.99±0.01 0.98±0.01 0.99±0.01 0.99±0.00 0.99±0.01 0.99±0.01
10000 0.93±0.01 0.97±0.02 0.97±0.02 0.95±0.02 0.95±0.03 0.98±0.01 0.98±0.01 0.97±0.01 0.97±0.01
100000 0.94±0.01 0.96±0.02 0.96±0.02 0.95±0.02 0.94±0.02 0.96±0.01 0.97±0.01 0.96±0.02 0.96±0.02
Table 28: Bernoulli GLM: calibration error; lower is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 0.22±0.02 0.03±0.03 0.04±0.04 0.03±0.04 0.02±0.01 0.05±0.03 0.04±0.04 0.04±0.03 0.03±0.01
10000 0.11±0.02 0.03±0.01 0.02±0.02 0.02±0.01 0.02±0.01 0.04±0.02 0.04±0.02 0.04±0.01 0.03±0.01
100000 0.09±0.03 0.01±0.01 0.01±0.00 0.02±0.01 0.02±0.01 0.04±0.02 0.04±0.02 0.03±0.01 0.03±0.01
Table 29: Bernoulli GLM: CRPS; smaller is better.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 0.54±0.08 0.67±0.21 0.67±0.20 0.62±0.18 0.57±0.10 0.69±0.20 0.68±0.20 0.64±0.19 0.64±0.17
10000 0.45±0.05 0.52±0.07 0.52±0.07 0.50±0.06 0.48±0.06 0.55±0.09 0.56±0.11 0.52±0.06 0.51±0.06
100000 0.45±0.05 0.49±0.06 0.51±0.07 0.49±0.06 0.47±0.05 0.50±0.06 0.52±0.07 0.49±0.06 0.49±0.06
42Under review as submission to TMLR
Figure 21: Bernoulli GLM: posterior samples for ScoRuTSBI with the Energy Score trained with m= 20
and reference posterior samples. Diagonal panels represent univariate marginals, while off-diagonals panels
represent bivariate marginals.
43Under review as submission to TMLR
Figure 22: Bernoulli GLM: posterior samples for GATSBI and reference posterior samples. Diagonal panels
represent univariate marginals, while off-diagonals panels represent bivariate marginals.
44Under review as submission to TMLR
Table 30: Bernoulli GLM: runtime in seconds; recall that GATSBI was trained on GPU while ScoRuTSBI
were trained on a single CPU.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 8737.34 891.35 649.68 1114.62 1456.01 640.54 660.62 739.97 722.27
10000 18790.1 1826.52 1390.01 1136.15 1396.37 746.6 692.29 910.15 1109.49
100000 42929.7 2323.19 1675.25 2043.82 2850.84 2280.55 1704.85 2576.69 2446.52
Table 31: Bernoulli GLM: epoch at which early stopping occurred; the max number of training epochs was
20000.
GATSBI Energy 3 Energy 5 Energy 10 Energy 20 Kernel3 Kernel5 Kernel10 Kernel20
1000 10900 1100 1000 1200 1800 1200 1000 1200 1100
10000 20000 1900 1900 2100 2700 1900 1500 2100 2100
100000 20000 3600 2000 2700 3700 3700 2300 3900 3200
45