Under review as submission to TMLR
Using the Krylov Subspace Formulation to Improve
Regularisation and Interpretation in Partial Least Squares
Regression
Anonymous authors
Paper under double-blind review
Abstract
Partial least squares regression (PLS-R) has been an important regression method in the
life sciences and many other ﬁelds for decades. However, PLS-R is typically solved using
an algorithmic approach, rather than through an optimisation formulation and procedure.
There is a clear optimisation formulation of the PLS-R problem based on a Krylov subspace
formulation, but it is only rarely considered. The popularity of PLS-R is attributed to the
ability to interpret the data through the model components, but the model components are
not available when solving the PLS-R problem using the Krylov subspace formulation. We
therefore highlight a simple reformulation of the PLS-R problem using the Krylov subspace
formulation as a promising modelling framework for PLS-R, and illustrate one of the main
beneﬁts of this reformulation—namely that it allows arbitrary penalty terms of the regres-
sion coeﬃcients to be included in the PLS-R model. Further, we propose an approach to
estimate the PLS-R model components for the solution found through the Krylov subspace
formulation, that are those we would have obtained had we been able to use the common al-
gorithms for estimating the PLS-R model. We illustrate the utility of the proposed method
on simulated and real data.
1 Introduction
Partial least squares regression (PLS-R) was proposed by Wold et al. (1983) as an alternative to principal
component regression (PCR, Massy, 1965) and ridge regression (Hoerl & Kennard, 1970) for the problem of
approximating concentrations of constituents in a chemical sample from spectroscopic data of the sample.
With spectroscopic data, the variables are typically strongly correlated and numerical instability problems
may therefore arise when using ordinary least squares regression. PLS-R solves this by projecting the data
onto an orthogonal set of basis vectors, derived from the data itself, and linear regression is then performed
within this subspace. This is very similar to how PCR works, but instead of using the singular value
decomposition (SVD) of the data, PLS-R is based on the partial least squares path modelling method in
mode A (Wold et al., 1983), where the linear subspace is derived using both the data and the target variables.
This leads to a non-linear shrinkage estimator (Krämer, 2007), with some unusual properties (Butler &
Denham, 2000).
Nevertheless, PLS-R has been widely used and is a very common choice of regression method in the chemical
and biological literature (Frank & Friedman, 1993; Wold et al., 2001; Boulesteix & Strimmer, 2007; Gromski
et al., 2015), but also in e.g.neuroimaging (Krishnana et al., 2011) and many other ﬁelds. PLS-R has been
a popular method within these ﬁelds not only for regression, but extensions have also been developed for
classiﬁcation (Sjöström et al., 1986; Ståhle & Wold, 1987; Barker & Rayens, 2003), outlier detection (Valder-
rama et al., 2007), and even for unsupervised problems such as e.g.clustering (Kloss et al., 2015). One of
the reasons for the popularity of PLS-R is likely because of the possibility to interpret the resulting regres-
sion vector in terms of the basis vectors of the solution subspace, the PLS-R score and loading vectors give
information about linear relationships between samples, but also about which variables correlate with those
1Under review as submission to TMLR
relationships. This is of course an important aspect, especially considering the recent growth in the ﬁeld
of interpretable and explainable machine learning (Linardatos et al., 2020). The fact that PLS-R allowed
better interpretations to be made, compared to PCR and ridge regression at the time, is likely to have added
to the popularity of the PLS-R method.
PLS-R has also been extended in many other ways, for instance to handle multiple target variables (Bisani
etal.,1983;Mateos-Aparicio,2011). Therearealsoseveralsimilarbutdiﬀerentformulationsoftheunderlying
optimisation problem, that make their own trade-oﬀs and typically give diﬀerent but related or similar
solutions (Helland, 1988; de Jong, 1993). There are also diﬀerent forms of preconditioning or preprocessing
methods, that are sometimes equivalent to the PLS-R problem, but allow diﬀerent interpretations of the
resulting subspace ( e.g., Trygg & Wold, 2002; Ergon, 2005; Kvalheim et al., 2009).
Regularised versions of PLS-R have also been proposed, such as sparse PLS-R (Lê Cao et al., 2008), elastic-
net PLS-R (Chun & Keleş, 2010), and non-negative PLS-R (Allen et al., 2013). However, those methods
regularise the score and/or loading vectors, which means that the resulting regression vector need not be
sparse.
While it is possible to solve the PLS-R optimisation problem using any general purpose solver, the most
common ones appear to be to use the SVD, or the non-linear iterative partial least squares (NIPALS)
algorithm—an instance of the power method (Abdi, 2010). While there are also accelerated versions of the
power method (Xu et al., 2018; Rabbani et al., 2021), the original NIPALS algorithm (Wold et al., 1983)
appears to still be one of the most common solvers for the PLS-R problem.
It has been shown that the PLS-R regression vector lies in a Krylov subspace (Helland, 1988; Rosipal &
Krämer, 2006; Krämer, 2007), and through that has connections to both Lanczos bidiagonalization (Eldén,
2004) and the conjugate gradient method (Wold et al., 1984). However, these relations do not seem to be
exploited in practice. A reason for this could be that while it is easy to solve the problem using the Krylov
subspace formulation, it is not immediately possible to obtain the scores and loadings from the obtained
regression vector, reducing the model interpretability.
In this work, we present solutions to two main problems: First, we show how to use the Krylov subspace
formulation to allow general-purpose regularisation terms to be added to the PLS-R problem. In particular,
we analyse a regularised version of the Krylov formulation of the PLS-R problem that results in a sparse
regression vector. The regularisation we used was the elastic net, i.e.a linear combination of /lscript1and squared
/lscript2penalties (Zou & Hastie, 2005). Second, we propose a means to estimate the score and loading vectors
for the found regression vector that we would have gotten, had we been able to use the traditional NIPALS
solver for the regularised PLS-R problem. This procedure thus allows the same interpretation of the model,
data, and target values as in classical PLS-R but now also when using the Krylov subspace formulation.
2 Method
2.1 Partial Least Squares Regression
We consider the standard linear regression problem, i.e.,
yi=xT
iw+εi,
fori= 1,...,n, whereyi∈Ris a continuous target variable, xi∈Rpis a data sample of pmeasured
variables, and εi∼N (0,σ2)is zero-mean additive Gaussian noise with variance σ2. The data samples, xi,
and target variables, yi, are assumed to be zero-mean.
In this setting, the PLS-R problem is typically formulated as (Höskuldsson, 1988),
maximise
w1∈RpyTX0w1 (1)
subject to/bardblw1/bardbl2
2= 1,
where y∈Rnis the vector of all target variables (all vectors are assumed to be column vectors), X0:=
X∈Rn×pcontains the ndata samples in the rows, and w1∈Rpis a weight vector. Once the weight vector
2Under review as submission to TMLR
is found, a score vector is computed as t1=X0w1and a loading vector as p1=XT
0t1/(tT
1t1). We also
compute a y-loading,c1=yTt1/(tT
1t1). Once all score and loading vectors are found, the data matrix, X0,
isdeﬂated, by anti-projecting on the found score vector,
X1=X0−t1pT
1=X0−t1tT
1X0
tT
1t1=/parenleftbigg
I−t1tT
1
tT
1t1/parenrightbigg
X0.
After deﬂation, the optimisation program in Equation 1 is run again, using X1, to ﬁnd a second set of
weights, w2, scores, t2, and loadings, p2andc2. A sequence of K≤rank(X)≤min(n,p)such vectors are
thus constructed, and we collect them as the columns in the matrices
W= [w1,...,wK],T= [t1,...,tK],
P= [p1,...,pK],and C= [c1,...,cK].
This procedure leads to mutually orthogonal weight and score vectors. A ﬁnal regression vector is computed
as
βPLS=W(PTW)−1(TTT)−1TTy=W(PTW)−1CT, (2)
and new samples are predicted as
ˆynew=xT
newβPLS.
The PLS-R method is thus a complicated procedure, and the steps leading to Equation 2 are fairly opaque,
andtypicallyinneedofcarefulindividualstudytofullyunderstand. Further, thedeﬂationprocedureissensi-
tive to numerical precision in the solution, and any errors are propagated to higher order components (Björck
& Indahl, 2017). It would be very diﬃcult to include regularisation terms in Equation 1, that penalises the
regression vector, βPLS, (through Equation 2) since the problem would become highly non-linear and a very
complicated function of the weight vectors, w, and especially so with more elaborate regularisers. In the
next section, we present an alternative, but equivalent formulation of the PLS-R problem in which we can
trivially incorporate penalties of the regression vector.
2.2 Partial Least Squares and Krylov Subspaces
Helland (1988) showed that an alternative basis for the weight vectors is the sequence, XTy,(XTX)XTy,
...,(XTX)K−1XTy, generating a Krylov subspace (Watkins, 2007). We have the following deﬁnition.
Deﬁnition 1. A Krylov subspace of order K, generated by a matrix A∈Rm×mand a vector v∈Rm, is
the linear subspace spanned by the ﬁrst Kpowers of A, and is denoted by
KK(A,v) = span{v,Av,A2v,...,AK−1v}.
Now, since the PLS-R weight vectors all lie in span{XTy,(XTX)XTy,..., (XTX)K−1XTy}(Helland, 1988),
we immediately obtain the following result. This result is known, but we have failed to ﬁnd a direct proof
of it; we therefore provide a simple proof, for completeness.
Lemma 1. IfKK(XTX,XTy)is a basis for the weight vectors, W= [w1,...,wK], then the PLS-R regres-
sion vector,βPLS, lie in the Krylov subspace of order Kgenerated by XTXandXTy, i.e.
βPLS∈KK(XTX,XTy).
Proof.It is well-known that the Krylov subspace KK(XTX,XTy)is a basis for the weight vectors (see e.g.,
Helland, 1988). Hence, if we let K∈Rp×Kbe some basis for KK(XTX,XTy), then
W=KA,
for some matrix A∈RK×K. From Equation 2, we have
βPLS=W(PTW)−1(TTT)−1TTy=Wv,
3Under review as submission to TMLR
withv= (PTW)−1(TTT)−1TTy. Thus
βPLS=Wv=KAv =Kα,
whereα=Avis a vector. This concludes the proof.
Hence, we see that the PLS-R problem can be cast in the form of a linear least squares problem, as
minimise
β∈Rp1
2n/bardbly−Xβ/bardbl2
2
subject toβ∈KK(XTX,XTy),
and by Lemma 1 an equivalent reformulation is thus
minimise
α∈RK1
2n/bardbly−XKα/bardbl2
2, (3)
where K∈Rp×Kagain is a basis for the Krylov subspace KK(XTX,XTy)andα∈RK. We assume that
Kis an orthonormal basis. An analytical solution is thus
α= (KTXTXK)−1KTXTy,
assuming that KTXTXKis invertible, but any numerical optimisation algorithm that solves Equation 3 can
of course also be used. Finally, the PLS-R regression coeﬃcient vector is retrieved as
βPLS=Kα.
Note that this is the sameregression vector as that found in Equation 2.
2.3 Regularising the Regression Vector in Partial Least Squares Regression
With Equation 3, the PLS-R problem is in a familiar form, and we can apply any regularisation we want
to the least squares objective. E.g., we can add a square /lscript2norm penalty, and obtain a ridge PLS-R
hybrid model, where the /lscript2regularisation parameter and Kwould control the trade-oﬀ between linear least
squares regression, ridge regression, and PLS-R. Equivalently, we can add an /lscript1norm penalty, for a Lasso
PLS-R hybrid model, with a trade-oﬀ between linear least squares regression, the Lasso, and PLS-R. This
is particularly interesting, since the /lscript1norm penalty performs variable selection, and thus a truly sparse
PLS-R model.
We chose to add both the /lscript1and squared /lscript2norm penalties, to obtain an elastic net (Zou & Hastie, 2005)
PLS-R hybrid model, where we thus can ﬁnd an optimal trade-oﬀ between /lscript1, squared/lscript2, and PLS-R
regularisation of the regression coeﬃcient vector, i.e.theKα. This model thus also performs variable
selection in the regression coeﬃcient vector, i.e.a sparse PLS-R model, where the sparsity is with respect
to the regression coeﬃcients instead of the weights and/or scores. The optimisation problem thus becomes
minimise
α∈RK1
2n/bardbly−XKα/bardbl2
2+γ
2/bardblKα/bardbl2
2+λ/bardblKα/bardbl1, (4)
whereλ>0andγ >0are regularisation parameters (or rather, Lagrange multipliers) controlling the trade-
oﬀ between the main objective and the regularisation terms. We obtain our ﬁnal sparse regression vector as
/hatwideβPLS=Kα.
The optimisation problem in Equation 4 was the main object of our attention in this work, and in the
examples that follow, we used the alternating direction method of multipliers (ADMM, Gabay & Mercier,
1976; Boyd et al., 2010) to solve it.
4Under review as submission to TMLR
2.3.1 The Steps of the Alternating Direction Method of Multipliers
We cast the program in Equation 4 in the following form,
minimise
α∈RK1
2n/bardbly−XKα/bardbl2
2+γ
2/bardblKα/bardbl2
2+λ/bardblz/bardbl1 (5)
subject to Kx=z.
We formulate the augmented Lagrangian of Equation 5,
Lρ(x,z,υ) =1
2n/bardbly−XKx/bardbl2
2+γ
2/bardblKx/bardbl2
2+λ/bardblz/bardbl1+υT(Kx−z) +ρ
2/bardblKx−z/bardbl2
2, (6)
withρ>0the penalty parameter and υ∈Rpa vector of Lagrange multipliers. For the ADMM algorithm,
we must minimise Equation 6 with respect to xand with respect to z. We see that, by setting the gradient
ofLρwith respect to xto zero and solving for x, we obtain
arg min
x∈RKLρ(x,z,υ) =/parenleftbigg1
nKTXTXK+ (γ+ρ)KTK/parenrightbigg−1/parenleftbigg1
nKTXTy−KTυ+ρKTz/parenrightbigg
.
To minimise Lρwith respect to z, we ﬁrst see that
Lρ(x,z,υ) =λ/bardblz/bardbl1−υTz+ρ
2/bardblKx−z/bardbl2
2+C1(x,υ)
=λ/bardblz/bardbl1+ρ
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftBig
Kx+υ
ρ/parenrightBig
−z/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2+C2(x,υ)
whereC1andC2are constant wrt.z. We recognise this as the proximal operator of the /lscript1norm, and thus
arrive at
arg min
z∈RpLρ(x,z,υ) = prox λ
ρ/bardbl·/bardbl1/parenleftBig
Kx+υ
ρ/parenrightBig
.
Finally, the steps of ADMM are,
x(s+1)= arg min
x∈RKLρ(x,z(s),υ(s))
z(s+1)= arg min
z∈RpLρ(x(s+1),z,υ(s))
υ(s+1)=υ(s)+ρ(Kx(s+1)−z(s+1)).
2.4 Reconstructing the Components of Partial Least Squares Regression
The regression coeﬃcient vector we obtain by solving Equation 4 will not coincide with the one obtained by
PLS-R in Equation 2, nor with the equivalent one we obtain by solving Equation 3. While we still select it
from the Krylov subspace KK(XTX,XTy), the found vector, α, will be diﬀerent in Equation 4 from that
found in Equation 3, and so the corresponding regression vectors will also be diﬀerent.
What we can do is to set up an optimisation problem, searching for weights, scores, and loadings fulﬁlling
the properties of the components of PLS-R, at least approximately, and that gives a regression vector that
5Under review as submission to TMLR
iscloseto/hatwideβPLS. We therefore want to solve the following optimisation problem,
maximise
/tildewidewk+1∈RpyTX/tildewidewk+1 (7)
subject to/bardbl/tildewidewk+1/bardbl2
2= 1, (8)
/tildewiderWT
k/tildewidewk+1=0, (9)
/tildewideTT
k/tildewidetk+1=0, (10)
/tildewiderWT
k/tildewidepk+1=0, (11)
/bardblwk+1−/tildewidewk+1/bardbl2
2≤α, (12)
/bardbltk+1−/tildewidetk+1/bardbl2
2≤β, (13)
/bardblpk+1−/tildewidepk+1/bardbl2
2≤γ, (14)
/bardblck+1−/tildewideck+1/bardbl2
2≤δ, (15)
/bardbl/hatwideβPLS−/tildewiderWk+1(/tildewidePT
k+1/tildewiderWk+1)−1/tildewideCT
k+1/bardbl2
2≤ε, (16)
where parameters with a tilde, such as /tildewidewk+1, are the component k+ 1parameters that we want to ﬁnd.
Parameters without tilde, such as w, are those found using unregularised PLS-R (from Section 2.1), and
/hatwideβPLS=Kαis the regularised PLS-R regression vector found by Equation 5. A matrix with an index has
a number of columns containing the components already found in order, e.g./tildewiderWk= [/tildewidew1,...,/tildewidewk]. Hence,
/tildewiderWk+1also includes the current sought parameter vector.
The objective, Equation 7, is the original PLS-R objective (from Equation 1), which still is what we want to
maximise. Equation 8 is the unit norm constraint. Equation 9 is an orthogonality constraint on the weight
vector,/tildewidewk+1, to thekalready found weight vectors, /tildewiderWk. Equation 10 is a corresponding orthogonality
constraint for the score vectors. The loadings, /tildewidepk+1, should be orthogonal to the already found weight
vectors, encoded in Equation 11 (see e.g., Manne, 1987). Further, we may want the found weights, scores,
and loadings to be close to the PLS-R weights, scores, and loadings, which is encoded in Equations 12–15.
Finally, Equation 16 forces the regression coeﬃcient vector, computed using Equation 2, to be near to the
one obtained from the regularised Krylov formulation of the PLS-R problem in Equation 5.
This formulation, in Equation 7, poses the exact problem that we want to solve, but it becomes a very
diﬃcult problem in practice. We have multiple constraints of which several are non-linear, non-convex, and
there may not even be a non-empty feasible set. We therefore propose the following slightly relaxed problem,
employing the method of Lagrange multipliers,
minimise
/tildewideωk+1∈Rpf(/tildewideωk+1) (17)
=−yTXK/tildewideωk+1 (18)
+λ/bardbl/hatwideβPLS−/tildewiderWk+1(/tildewidePT
k+1/tildewiderWk+1)−1/tildewideCT
k+1/bardbl2
2 (19)
+µ/bardblwk+1−K/tildewideωk+1/bardbl2
2 (20)
+ν/bardbltk+1−XK/tildewideωk+1/bardbl2
2 (21)
+ξ/bardbl(tT
k+1tk+1)pk+1−XTXK/tildewideωk+1/bardbl2
2 (22)
+π/bardbl(tT
k+1tk+1)ck+1−yTXK/tildewideωk+1/bardbl2
2 (23)
subject to/bardblK/tildewideωk+1/bardbl2
2≤1, (24)
/tildewiderWT
kK/tildewideωk+1=0, (25)
/tildewideTT
kXK/tildewideωk+1=0, (26)
/tildewiderWT
kXTXK/tildewideωk+1=0, (27)
where/tildewidewk+1=K/tildewideωk+1, theλ,µ,ν,ξ, andπare regularisation parameters (Lagrange multipliers), and where
the constraints from Equations 8–16, for which we the projection operators are easy to compute, have been
6Under review as submission to TMLR
kept as constraints in Equations 24–27, and those that are more diﬃcult, or alternatively, very easy to
optimise over in their penalty form, have been put as penalty terms instead.
We now give the derivation and interpretation of the terms in Equations 19–27 in order:
•Equation 19: Note that /tildewiderWk+1= [/tildewidew1,...,/tildewidewk,K/tildewideωk+1], and similar for /tildewidePk+1and/tildewideCk+1. Correspond-
ing to Equation 16, this is a non-linear function in /tildewideωk+1, for which we don’t know the projection
operator nor the proximal operator. It is easier to minimise in penalty form.
•Equation 20: Corresponds to Equation 12. This projection operator is known and easy to compute,
and a smooth function in /tildewideωk+1, why we make it into a penalty instead.
•Equation 21: Corresponds to Equation 13. We express this as a function of /tildewideωk+1, with/tildewidetk+1=
XK/tildewideωk+1, and since it is smooth and convex, we put it as a penalty.
•Equation 22: Corresponds to Equation 14. Note that PLS-R deﬁnes the loadings as
/tildewidepk+1=XT/tildewidetk+1
/tildewidetT
k+1/tildewidetk+1=XTX/tildewidewk+1
/bardblX/tildewidewk+1/bardbl2
2=XTXK/tildewideωk+1
/bardblXK/tildewideωk+1/bardbl2
2,
which is non-linear in /tildewideωk+1. To get rid of the denominator we make an approximation in that we
multiply both terms by the squared norm of their corresponding score vector, and instead ask that
XTXK/tildewideωk+1be close to (tT
k+1tk+1)pk+1. Hence, another smooth convex function put as a penalty.
•Equation 23: Corresponds to Equation 15. Recall that PLS-R deﬁnes the y-loadings as
/tildewideck+1=yT/tildewidetk+1
/tildewidetT
k+1/tildewidetk+1=yTX/tildewidewk+1
/bardblX/tildewidewk+1/bardbl2
2=yTXK/tildewideωk+1
/bardblXK/tildewideωk+1/bardbl2
2,
which is non-linear in /tildewideωk+1. To get rid of the denominator, we again make an approximation in
that we multiply both terms by the squared norm of their corresponding score vector, and ask that
yTXK/tildewideωk+1be close to (tT
k+1tk+1)ck+1. Hence, a smooth convex function in /tildewideωk+1, put as a penalty.
•Equation 24: This is a convex relaxation of Equation 8.
•Equations 25–26: Same as Equations 9–10, with the diﬀerence that Equation 26 is expressed as a
function of/tildewideωk+1.
•Equation 27: Corresponds to Equation 11. We know that the PLS-R loadings satisfy (Höskuldsson,
2003),
/tildewiderWT
k/tildewidepk+1=/tildewiderWT
kXT/tildewidetk+1
/tildewidetT
k+1/tildewidetk+1=/tildewiderWT
kXTXK/tildewideωk+1
/bardblXK/tildewideωk+1/bardbl2
2=0,
which clearly is equivalent to
/tildewiderWT
k/tildewidepk+1=/tildewiderWT
kXTXK/tildewideωk+1=0,
assuming/tildewideωT
k+1KTXTXK/tildewideωk+1>0, but since K≤rank(X), this is achieved.
With these changes, consisting of several reformulations, one convex relaxation, and two approximations,
we have an objective function that is the sum of a number of functions that all but one are convex, with
four convex constraints (of which three are linear). We can solve this problem using e.g.projected gradient
descent (Bertsekas, 1999), or any other optimisation algorithm of choice.
In order to apply projected gradient descent, we need to know the gradient of fand the projection operator
correspondingtothefourconstraintsinEquations24–27. Thesearestraight-forward, butneedtobeoutlined
in more detail.
7Under review as submission to TMLR
2.4.1 Projection Operators
Each constraint in Equations 24–27 correspond to a set of feasible points, denoted S1,...,S4, respectively.
In order for all four constraints to be satisﬁed, the solution must lie in all of them, i.e.we seek a point that
lie in their intersection,
S={x:x∈S1∧···∧ x∈S4}=/braceleftbigg
x:x∈4/intersectiondisplay
i=1Si/bracerightbigg
.
We see from Equations 24–27 that for all Si, fori= 1,..., 4, we have that at least 0∈Si, and soS/negationslash=∅We
note that eachSi, fori= 1,..., 4, is a convex set, and since the intersection of convex sets is convex, Sis
also a convex set.
The single projection operator corresponding to the four constraints in Equations 24–27 is the projection
onto their intersection, i.e.the projection onto S. The projection of a point w∈Rponto a convex set,
S⊆Rp, is deﬁned as,
projS(w) = arg min
x∈Rp/bardblx−w/bardbl2
2+χS(x), (28)
whereχSis the characteristic function over S,i.e.,
χS(x) =/braceleftBigg
0ifx∈S,
∞ifx/∈S.
We can numerically compute the projection onto the intersection of the four sets, S1,...S4,i.e.ontoS, by
using a parallel Dykstra-like proximal algorithm, as outlined by e.g.Combettes & Pesquet (2011).
We give the two projection operators, and start with Equation 24. The proximal operator for λ/bardblKx/bardbl2
2is
trivially
proxλ/bardblK·/bardbl2
2(x) = (I+ 2λKTK)−1x=1
1 + 2λx,
since Kis assumed orthonormal (and thus KTK=I), and we seek the smallest λsuch that Equation 24
is fulﬁlled, i.e., such that x∈S 1={x∈Rp:/bardblKproxλ/bardblK·/bardbl2
2(x)/bardbl2
2≤1}, which we achieve by ﬁnding the
smallestλ∗such that
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleK/parenleftbigg1
1 + 2λ∗x/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2≤1⇐⇒λ∗≥/bardblKx/bardbl2
2−1
2.
Hence, using this λ∗, the projection operator becomes
projS1(x) =/braceleftBigg
x if/bardblKx/bardbl2
2≤1,
1
1+2λ∗xotherwise.
The constraints in Equations 25–27 all have the general form Aix=0withA2=/tildewiderWT
kK,A3=/tildewideTT
kXK,
orA4=/tildewiderWT
kXTXK, respectively. The projection operator onto the sets Si={x∈Rp:Aix=0}, for
i∈{2,3,4}, has the analytic solution (Bauschke & Kruk, 2004),
projSi(x) =/braceleftBigg
x, ifAix=0,
x−A†
iAix,otherwise,
where A†is the Moore-Penrose pseudo-inverse of A.
2.4.2 Gradient of the Objective
We need to compute the gradient of fin Equation 17. Now, Equation 18 and Equations 20–23 are straight-
forward linear and quadratic functions, with trivial gradients, but Equation 19 is more diﬃcult to ﬁnd, why
we present it here.
8Under review as submission to TMLR
We have that (Höskuldsson, 2003),
/tildewidew∗
k+1=/tildewidewk+1−/bracketleftBiggk/summationdisplay
i=1/tildewidew∗
i/tildewidepT
i/bracketrightBigg
/tildewidewk+1
=K/tildewideωk+1−/bracketleftBiggk/summationdisplay
i=1/tildewidew∗
i/tildewidepT
i/bracketrightBigg
K/tildewideωk+1
= (I−D)K/tildewideωk+1,
where Iis an identity matrix and Dis a constant matrix.
Let/hatwideβPLS,kbe the regression coeﬃcient vector approximated using the kpreviously found components, and
further let a=KTXTy,b=/hatwideβPLS−/hatwideβPLS,k,A= (I−D)K, and B=KTXTXK. Then the gradient
becomes,
∇/tildewideωk+1/vextenddouble/vextenddouble/hatwideβPLS−/tildewiderWk+1(/tildewidePT
k+1/tildewiderWk+1)−1/tildewideCT
k+1/vextenddouble/vextenddouble2
2(29)
=−2(ATbaT+abTA)/tildewideωk+1
/tildewideωT
k+1B/tildewideωk+1
+4bTA/tildewideωk+1/tildewideωT
k+1aB/tildewideωk+1
(/tildewideωT
k+1B/tildewideωk+1)2
+2(ATA/tildewideωk+1/tildewideωT
k+1a+/tildewideωT
k+1ATA/tildewideωk+1a)aT/tildewideωk+1
(/tildewideωT
k+1B/tildewideωk+1)2
−4aT/tildewideωk+1/tildewideωT
k+1ATA/tildewideωk+1/tildewideωT
k+1aB/tildewideωk+1
(/tildewideωT
k+1B/tildewideωk+1)3.
2.4.3 Minimising the Objective Function
We used projected gradient descent (Bertsekas, 1999) to solve the non-linear program in Equation 17, which
amounts to iterating the weight update scheme,
/tildewideω(s+1)
k+1←projS/parenleftbig/tildewideω(s)
k+1−η∇f(/tildewideω(s)
k+1)/parenrightbig
wheresis a sequence index, η > 0is a step size, the gradient of fis Equation 29 plus the gradients of
the rest of the terms, i.e.from Equation 18 and Equations 20–23, and the projection operator, projS, was
computed numerically by solving Equation 28 using a parallel Dykstra-like proximal algorithm (Combettes
& Pesquet, 2011).
2.4.4 The Found Regression Coeﬃcient Vector
We have the following immediate result about the regression coeﬃcient vector, /hatwideβPLS,K =
/tildewiderWK(/tildewidePT
K/tildewiderWK)−1/tildewideCT
K, found by using the program in Equation 17.
Theorem 1. The PLS-R regression vector, /hatwideβPLS,K, found through Equation 17, lie in the Krylov subspace
of orderKgenerated by XTXandXTy, i.e.
/hatwideβPLS,K∈KK(XTX,XTy).
Proof.All weight vectors found through Equation 17 are written in the form /tildewidewk=K/tildewideωk, where Kis a basis
for the Krylov subspace KK(XTX,XTy). We can then write
/tildewiderWK=KΩK,
withΩK= [/tildewideω1,...,/tildewideωK].I.e., all weight vectors lie in the Krylov subspace. Hence, by Lemma 1, the PLS-R
regression vector, /hatwideβPLS,K, lie in that Krylov subspace of order Kgenerated by XTXandXTy.
9Under review as submission to TMLR
Hence, the solution found using the Krylov formulation has the same property as the PLS-R solution, namely
that both the weights, /tildewiderW, and the regression vector, /hatwideβPLS, lie in a Krylov subspace. This property may
lead to better future algorithms for solving the PLS-R problems.
3 Examples
To illustrate the utility of the proposed Krylov-based PLS-R formulation, we present two examples. The
ﬁrst example is based on simulated data and the second example is based on near infrared reﬂectance (NIR)
scans of soil samples. Both examples are analysed withoutregularisation, to show that the proposed method
is able to reconstruct the scores and loadings of a standard PLS-R model computed using the NIPALS
algorithm (Helland, 1988; Abdi, 2010), and both are also analysed withelastic-net ( /lscript1and squared /lscript2)
regularisation, to illustrate the extended regularisation, the variable selection, and the interpretation of the
regression coeﬃcient vector and the reconstructed scores and loadings.
3.1 Example 1: Simulated Data
The ﬁrst example illustrates how the proposed regularised PLS-R method works in comparison to standard
PLS-R. We will illustrate that without regularisation, the proposed method gives the same result as standard
PLS-R, and further how the regularised PLS-R diﬀers from the regular PLS-R in terms of the regression
vector, and the weights and scores.
−4 −2 0 2 4
Variables0.00.10.20.30.40.50.60.70.8Loadings(a)p1
p2
p3
−4 −2 0 2 4
Variables−0.10.00.10.20.3Normalised Regression Coeﬃcients .(b)βPLS
/hatwideβPLS(No Reg.)
/hatwideβPLS(Reg.)
−4 −2 0 2 4
Variables−0.10.00.10.20.3Weights,wj(c)PLS-R
Krylov PLS-R (No Reg.)
Krylov PLS-R (Reg.)
1 11 21 31 41
Samples,i−3−2−101Scores,ti(d)PLS-R
Krylov PLS-R (No Reg.)
Krylov PLS-R (Reg.)
Figure 1: (a) The loading proﬁles that make up the data matrix. (b) The regression coeﬃcient vectors found
using regular PLS-R. Note that the green and blue curves are indistinguishable. (c) The ﬁrst weight vectors
for each method. (d) The ﬁrst score vectors for each method.
The data were collected in a matrix, X∈Rn×p, withn= 50andp= 101, and was composed of three
spectra, each with a Gaussian proﬁle as seen in Figure 1 (a). The data were constructed as
X=t1pT
1+t2pT
2+t3pT
3+E,
where y=t1∼N (0n,In)were sampled from a standard normal, t2=1
2|z2|+ 0.35withz2∼N (0n,In),
andt3=z3−1.25withz3∼N(0n,In), and Eare independent zero-mean normal with variance 0.01. There
was thus a perfect correlation between yandt1, the correlation between yandt2was about 0.213, and the
correlation between yandt3was about 0.005.
10Under review as submission to TMLR
We ﬁt one regular PLS-R model, one PLS-R model based on the Krylov subspace formulation without
regularisation, and one PLS-R model based on the Krylov subspace formulation with elastic net regulari-
sation. The number of components extracted were K= 20, and for the elastic net regularisation, we used
γ= 0.00125andλ= 0.02375.
The PLS-R regression vectors are illustrated in Figure 1 (b). We see that the regular PLS-R regression
vector,βPLS, picked up much noise in the data. We further see the unregularised PLS-R vector, /hatwideβPLS, found
using the unregularised Krylov subspace formulation, and that it is almost indistinguishable from the regular
PLS-R regression vector (the green and blue curves in Figure 1 (b)). In fact, the diﬀerences are attributed to
numerical instability in the noise dimensions, because the diﬀerences disappeared for few components, and
when the number of components were near the rank of the data matrix. The regression vector found when
using the unregularised Krylov subspace formulation had much less noise, and had seven variables (about
7 %) that were smaller than 5·10−7(considered as zero). Note that the regression vector for the PLS-R
model computed using the NIPALS algorithm did not have any coeﬃcients that were near zero.
The found and reconstructed weight and score vectors of the models are illustrated in Figure 1 (c) and
Figure 1 (d), respectively. They are all highly correlated, implying that it would be possible to interpret
them in a similar way.
500 750 1000 1250 1500 1750 2000 2250 2500
Wavelength [nm]−0.20−0.15−0.10−0.050.000.050.100.15Normalised Regression Coeﬃcients ..(a)
βOLS
βPLS
/hatwideβPLS(No Reg.)
/hatwideβPLS(Reg.)
0.02 0.03 0.04 0.05 0.06
First Weight Vector, w1−0.04−0.020.000.020.040.06Second Weight Vector, w2
400500
600
700 80090010001100
1200
130014001500
160017001800190020002100
2200230024002498(b)
W
/tildewiderW(Reg.)
18.0 18.5 19.0 19.5 20.0 20.5 21.0
First Score Vector, t1−1.00−0.75−0.50−0.250.000.250.50Second Score Vector, t2
1
23
456
78910
11121314 15161718
1920 2122
232425
26272829
3031
32
33 3435
363738
39
40
414243
4445 4647 484950
5152
535455
5657
5859606162
636465 66676869
7071
727374
75
767778798081
82
838485
8687 88 89909192
9394
9596
97(c)
T
/tildewideT(Reg.)
Figure2: (a)Theregressionvectorsfromtheordinaryleastsquaresmodel, βOLS, thePLS-Rmodelcomputed
using the NIPALS algorithm, βPLS, the PLS-R model computed using the Krylov formulation without
regularisation, /hatwideβPLS(No Reg.), and the PLS-R model computed using the Krylov formulation with elastic
net regularisation, /hatwideβPLS(Reg.). Note that the purple and blue lines are indistinguishable. (b) The ﬁrst
and second weight vectors for the PLS-R model computed using the NIPALS algorithm (red curve), and
for the PLS-R model computed using the Krylov formulation with elastic net regularisation (blue curve).
The numbers indicate the wavelengths (in nm). (c) The ﬁrst and second score vectors for the PLS-R model
computed using the NIPALS algorithm (red points), and for the PLS-R model computed using the Krylov
formulation with elastic net regularisation (blue points). The numbers indicate the sample index.
3.2 Example 2: Soil Samples Measured with NIR
The second example contains soil samples originating from a long-term ﬁeld experiment in Abisko, Sweden,
described by Rinnan & Rinnan (2007)1. Each of 36 samples were collected from the 5 to 10 cm depth
1Obtained from http://www.models.life.ku.dk/NIRsoil.
11Under review as submission to TMLR
with three repetitions, yielding a total of n= 108samples. The samples were scanned using NIR, in the
wavelength range of 400–2498 nm at p= 1050wavelengths. The target variable was soil organic matter
(SOM,e.g.plant residues), that was measured as loss on ignition at 550◦C.
Again, we ﬁt one regular PLS-R model and one PLS-R model based on the Krylov subspace formulation
with elastic net regularisation. The number of components extracted were K= 13for the regular PLS-R
model, and it was K= 50withγ= 1.0·10−5andλ= 1.0·10−3for the elastic net regularised PLS-R model
based on the Krylov formulation. We also ﬁt an ordinary least squares (OLS) regression model, and one
PLS-R model using the Krylov formulation without the elastic net regularisation.
The PLS-R regression vectors are illustrated in Figure 2 (a). We see that the OLS vector is very noisy (green
curve), and that the PLS-R regression vectors are less so (purple and blue curves). Further, we see that the
regression vector from the regularised PLS-R model (red curve) has many values close to zero, especially in
the range of about 900–1750 nm. The regression coeﬃcient values are close to zero, and it has a sparsity
strucure with 27 coeﬃcients being zero (smaller than 5·10−7), or about 2.5 %. Note that the regression
vector for the PLS-R model computed using the NIPALS algorithm did not have any coeﬃcients that were
near zero.
The reconstructed ﬁrst and second weight and score vectors, from the regularised PLS-R model, are illus-
trated in Figure 2 (b) and (c), respectively, together with the weight and score vectors from the PLS-R
model computed using the NIPALS algorithm. We see that they are very close, implying that it would be
possible to interpret them in a similar way.
4 Discussion and Conclusions
We have presented a simple way to use the Krylov formulation to solve the PLS-R problem, which allows
additional regularisation terms to be added to the model. We illustrated the use of elastic net regularisation
(/lscript1and squared /lscript2terms) for additional regularisation and variable selection, and demonstrated that the
found regression vectors were sparse.
Note, however, that while we illustrated that the proposed formulation allow sparse regression vectors, the
proposed formulation allows an analyst to impose any conceivable problem-relevant penalties in the PLS-R
model. Further, using the Krylov formulation allows other solvers, for instance more eﬃcient Krylov-based
solvers to be used for the PLS-R problem.
Further, we proposed an approach to approximate the scores and loadings for the PLS-R regression vector
found using the Krylov formulation, which allows interpretations of the model in the same way as PLS-R
models are interpreted when they are computed using e.g.the NIPALS algorithm.
We illustrated the utility of the model on simulated data, and on a real data set with soil sample data. Both
examples showed that the Krylov PLS-R method gave regression coeﬃcient vectors that had coeﬃcients that
were zero or close to zero, meaning that variable selection was performed. Further, both examples showed
that it was possible to approximate weight and score vectors for the Krylov PLS-R model, that were close
to the PLS-R equivalents, and that thus can be used for model and data interpretation.
The proposed PLS-R model formulation opens the door to more elaborate regularisation in a PLS-R model,
while still allowing corresponding scores, weights, and loadings to be approximated. Follow-up research
could also focus on computational aspects, to e.g.speed up the computations required for the component
reconstructions.
References
Hervé Abdi. Partial least squares regression and projection on latent structure regression (PLS regression).
WIREs Computational Statistics , 2(1):97–106, 2010.
Genevera I. Allen, Christine Peterson, Marina Vannucci, and Mirjana Maletić-Savatić. Regularized partial
least squares with an application to NMR spectroscopy. Statistical Analysis and Data Mining , 6(4):302–
314, 2013.
12Under review as submission to TMLR
Matthew Barker and William Rayens. Partial least squares for discrimination. Journal of Chemometrics ,
17:166–173, 2003.
Heinz H. Bauschke and Serge G. Kruk. Reﬂection-projection method for convex feasibility problems with
an obtuse cone. Journal of Optimization Theory and Applications , 120(3):503–531, 2004.
Dimitri P. Bertsekas. Nonlinear Programming . Athena Scientiﬁc, Belmont, Ma., U.S.A., 1999.
M. Laura Bisani, Domenico Faraone, Sergio Clementi, Kim H. Esbensen, and Svante Wold. Principal
components and partial least-squares analysis of the geochemistry of volcanic rocks from the aeolian
archipelago. Analytica Chimica Acta , 150:129–143, 1983.
Åke Björck and Ulf G. Indahl. Fast and stable partial least squares modelling: A benchmark study with
theoretical comments. Journal of Chemometrics , 31, 2017.
Anne-Laure Boulesteix and Korbinian Strimmer. Partial least squares: a versatile tool for the analysis of
high-dimensional genomic data. Brieﬁngs in Bioinformatics , 8(1):32–44, 2007.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization
and statistical learning via the alternating direction method of multipliers. Foundations and Trends in
Machine Learning , 3(1):1–122, 2010.
Neil A. Butler and Michael C. Denham. The peculiar shrinkage properties of partial least squares regression.
Journal of the Royal Statistical Society , 62(3):585–593, 2000.
HyonhoChunandSündüzKeleş. Sparsepartialleastsquaresregressionforsimultaneousdimensionreduction
and variable selection. Journal of the Royal Statistical Society , 72(1):3–25, 2010.
Patrick L. Combettes and Jean-Christophe Pesquet. Proximal splitting methods in signal processing. In
H. H. Bauschke, R. S. Burachik, P. L. Combettes, V. Elser, D. R. Luke, and H. Wolkowicz (eds.), Fixed-
Point Algorithms for Inverse Problems in Science and Engineering , pp. 185–212. New York: Springer,
2011.
Sijmen de Jong. SIMPLS: an alternative approach to partial least squares regression. Chemometrics and
Intelligent Laboratory Systems , 18:251–263, 1993.
Lars Eldén. Partial least-squares vs. Lanczos bidiagonalization—I: analysis of a projection method for
multiple regression. Computational Statistics & Data Analysis , 46:11–31, 2004.
Rolf Ergon. PLS post-processing by similarity transformation (PLS+ST): a simple alternative to OPLS.
Journal of Chemometrics , 19(1):1–4, 2005.
Ildiko E. Frank and Jerome H. Friedman. A statistical view of some chemometrics regression tools. Techno-
metrics, 35(2):109–135, 1993.
Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational problems via
ﬁnite element approximation. Computers & Mathematics with Applications , 2(1):17–40, 1976.
Piotr S. Gromski, Howbeer Muhamadali, David I. Ellis, Yun Xu, Elon Correa, Michael L. Turner, and
Royston Goodacre. A tutorial review: Metabolomics and partial least squares-discriminant analysis – a
marriage of convenience or a shotgun wedding. Analytica Chimica Acta , 879(16):10–23, 2015.
Inge S. Helland. On the structure of partial least squares regression. Communications in Statistics—
Simulation and Computation , 17(2):581–607, 1988.
Arthur E. Hoerl and Robert W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems.
Technometrics , 8:27–51, 1970.
Agnar Höskuldsson. PLS regression methods. Journal of Chemometrics , 2:211–228, 1988.
13Under review as submission to TMLR
Agnar Höskuldsson. Analysis of latent structures in linear models. Journal of Chemometrics , 17:630–645,
2003.
Ricardo Barbosa Kloss, Marcos Vinicius Mussel Cirne, Samira Silva, Helio Pedrini, and William Robson
Schwartz. Partial least squares image clustering. In L. R. Oliveira, A. L. Apolinário Junior, and R. P.
Lemes (eds.), Proceedings of the 28th Conference on Graphics, Patterns and Images (SIBGRAPI 2015) ,
pp. 41–48, 2015.
Nicole Krämer. An overview on the shrinkage properties of partial least squares regression. Computational
Statistics , 22:249–273, 2007.
Anjali Krishnana, Lynne J. Williams, Anthony Randal McIntosh, and Hervé Abdi. Partial least squares
(PLS) methods for neuroimaging: A tutorial and review. NeuroImage , 56(2):455–475, 2011.
Olav M. Kvalheim, Tarja Rajalahti, and Reidar Arneberg. X-tended target projection (XTP)—comparison
with orthogonal partial least squares (OPLS) and PLS post-processing by similarity transformation
(PLS+ST). Journal of Chemometrics , 23(1):49–55, 2009.
Kim-Anh Lê Cao, Debra Rossouw, Christèle Robert-Granié, and Philippe Besse. A sparse PLS for variable
selection when integrating omics data. Statistical Applications in Genetics and Molecular Biology , 7(1),
2008.
Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. Explainable AI: A review of machine
learning interpretability methods. Entropy (Basel) , 23:1, 2020.
Rolf Manne. Analysis of two partial-least-squares algorithms for multivariate calibration. Chemometrics and
Intelligent Laboratory Systems , 2:187–197, 1987.
William F. Massy. Principal components regression in exploratory statistical research. Journal of the
American Statistical Association , 60:234–246, 1965.
Gregoria Mateos-Aparicio. Partial least squares (PLS) methods: Origins, evolution, and application to social
sciences. Communications in Statistics—Theory and Methods , 40:2305–2317, 2011.
Tahseen Rabbani, Apollo Jain, Arjun Rajkumar, and Furong Huang. Practical and fast momentum-based
power methods. In Proceedings of the 2nd Annual Conference on Mathematical and Scientiﬁc Machine
Learning , volume 145 of Proceedings of Machine Learning Research , pp. 1–36. PMLR, 2021.
Riikka Rinnan and Åmund Rinnan. Application of near infrared reﬂectance (NIR) and ﬂuorescence spec-
troscopytoanalysisofmicrobiologicalandchemicalpropertiesofarcticsoil. Soil Biology and Biochemistry ,
39(7):1664–1673, 2007.
RomanRosipalandNicoleKrämer. Overviewandrecentadvancesinpartialleastsquares. InCraigSaunders,
Marko Grobelnik, Steve Gunn, and John Shawe-Taylor (eds.), Subspace, Latent Structure and Feature
Selection , pp. 34–51. Springer Berlin Heidelberg, 2006.
Michael Sjöström, Svante Wold, and Bengt Söderström. PLS discriminant plots. In E. S. Gelsema and L. N.
Kanal(eds.), Pattern Recognition in Practice , pp.461–470.ElsevierSciencePublishersBV,North-Holland,
1986.
Lars Ståhle and Svante Wold. Partial least squares analysis with cross-validation for the two-class problem:
a Monte Carlo study. Journal of Chemometrics , 1:185–196, 1987.
Johan Trygg and Svante Wold. Orthogonal projections to latent structures (O-PLS). Journal of Chemomet-
rics, 16:119–128, 2002.
Patrícia Valderrama, Jez Willian B. Braga, and Ronei Jesus Poppi. Variable selection, outlier detection,
and ﬁgures of merit estimation in a partial least-squares regression multivariate calibration model. A case
study for the determination of quality parameters in the alcohol industry by near-infrared spectroscopy.
Journal of Agricultural Food Chemistry , 55(21):8331–8338, 2007.
14Under review as submission to TMLR
David S. Watkins. The Matrix Eigenvalue Problem . Society for Industrial and Applied Mathematics, 2007.
Philadelphia, Pa., U.S.A.
Svante Wold, Harald Martens, and Herman Wold. The multivariate calibration problem in chemistry solved
by the PLS method. Lecture Notes in Mathematics , 973:286–293, 1983.
Svante Wold, A. Ruhe, Herman Wold, and William J. III Dunn. The collinearity problem in linear regres-
sion. The partial least squares (PLS) approach to generalized inverses. SIAM Journal on Scientiﬁc and
Statistical Computing , 5(3):735–743, 1984.
Svante Wold, Michael Sjöström, and Lennart Eriksson. PLS-regression: a basic tool of chemometrics.
Chemometrics and Intelligent Laboratory Systems , 58(2):109–130, 2001.
Peng Xu, Bryan He, Christopher De Sa, Ioannis Mitliagkas, and Christopher Re. Accelerated stochastic
power iteration. In Amos Storkey and Fernando Perez-Cruz (eds.), Proceedings of the Twenty-First Inter-
national Conference on Artiﬁcial Intelligence and Statistics , volume 84 of Proceedings of Machine Learning
Research , pp. 58–67. PMLR, 2018.
Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal
Statistical Society , 67(2):301–320, 2005.
15