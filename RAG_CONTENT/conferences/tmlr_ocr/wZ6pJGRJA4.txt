Under review as submission to TMLR
UGAE: A Novel Approach to Non-exponential Discounting
Anonymous authors
Paper under double-blind review
Abstract
The discounting mechanism in Reinforcement Learning determines the relative importance
of future and present rewards. While exponential discounting is widely used in practice,
non-exponential discounting methods that align with human behavior are often desirable
for creating human-like agents. However, non-exponential discounting methods cannot be
directly applied in modern on-policy actor-critic algorithms like PPO. To address this issue,
we propose Universal Generalized Advantage Estimation (UGAE), which allows for the
computation of GAE advantages with arbitrary discounting. Additionally, we introduce
Beta-weighted discounting, a continuous interpolation between exponential and hyperbolic
discounting, to increase flexibility in choosing a discounting method. To showcase the utility
of UGAE, we provide an analysis of the properties of various discounting methods. We
also show experimentally that agents with non-exponential discounting trained via UGAE
outperform variants trained with Monte Carlo advantage estimation. Through analysis of
various discounting methods and experiments, we demonstrate the superior performance
of UGAE with Beta-weighted discounting over the Monte Carlo baseline on standard RL
benchmarks. UGAE is simple and easily integrated into any advantage-based algorithm as
a replacement for the standard recursive GAE.
1 Introduction
Building a Reinforcement Learning (RL) algorithm for time-dependent problems requires specifying a dis-
counting mechanism that defines how important the future is relative to the present. Typically, this is done
by setting a discount rate γ∈[0,1]and decaying the future rewards exponentially by a factor γt(Bellman,
1957). This induces a characteristic planning horizon, which should match the properties of the environ-
ment. In practice, γis one of the most important hyper-parameters to tune. Small deviations may massively
decrease the algorithm’s performance, which makes training an RL agent less robust, and more difficult for
users to perform a hyper-parameter search with new algorithms and environments.
The choice of a discounting is an example of the bias-variance trade-off. If the discount factor is too low (or
correspondingly, the general discounting decreases too rapidly), the value estimate is too biased, and in an
extreme case, the agent cannot plan sufficiently far into the future. Conversely, with a discount factor too
high, the variance of the value estimation is very large due to the high impact of the distant future, often
irrelevant to the decision at hand.
In the literature, a variety of discounting mechanisms have been proposed, from the widely used exponential
discounting (Strotz, 1955) to hyperbolic discounting, first introduced by psychologists to describe human
behavior (Ainslie & Haslam, 1992), which we show to be two special cases of our Beta-weighted discounting.
Other options include fixed-horizon discounting (Lattimore & Hutter, 2011), where all rewards beyond a
certain horizon are ignored, or not using any discounting (Naik et al., 2019). Any discounting method can
also be truncated by setting it to zero for all timesteps after a certain point.
The Generalized Advantage Estimation (Schulman et al., 2018) (GAE) algorithm, which can be seen as an
extension of the TD( λ) algorithm, is widely used in training RL agents, but it can only use exponential
discounting. This limits the behaviors that we can observe in trained agents with respect to balancing
rewards at multiple timescales. To enable using arbitrary discounting methods, we introduce Universal
1Under review as submission to TMLR
GeneralAdvantageEstimation(UGAE)–amodifiedformulationofGAEthatacceptsanyarbitrarydiscount
vectors. We also define a novel discounting method, named Beta-weighted discounting, which is obtained by
continuously weighing all exponential discount factors according to a Beta distribution. We show that this
method captures both exponential and hyperbolic discounting, depending on its parameters.
Moreover, we offer an analysis of several exponential and non-exponential discounting methods and their
properties. While these methods (except for Beta-weighted discounting) are not new, they can be used in
practical RL experiments thanks to UGAE; therefore, it is worthwhile to understand their differences.
As pointed out by Pitis (2019), exponential discounting with a constant discount factor fails to model
all possible preferences that one may have. While our beta-weighted discounting only introduces a time-
dependent discount factor and thus does not solve this problem in its entirety, it enables using more complex
discounting mechanisms. Furthermore, our UGAE can serve as a step towards a practical implementation
of state-action-dependent discounting.
Finally, we experimentally evaluate the performance of UGAE on a set of RL environments, and compare
it to the unbiased Monte Carlo advantage estimation method. We show that UGAE can match or surpass
the performance of exponential discounting, without a noticeable increase in computation time. Since it can
be seamlessly used with existing codebases (usually by replacing one function), it offers a good alternative
to the conventional approach, and enables a large range of future empirical research into the properties of
non-exponential discounting.
While currently research into non-exponential discounting is largely limited to toy problems and simple
tabularalgorithms, ourUGAEmakesitpossibletousearbitrarydiscountingwithstate-of-the-artalgorithms.
It can be used to solve a wide range of problems, including ones with continuous observation and action
spaces, and multiagent scenarios, by combining it with algorithms like PPO (Schulman et al., 2017).
In summary, our contributions are twofold: we introduce UGAE, a modification of GAE that accepts arbi-
trary discounting methods, offering greater flexibility in the choice of a discounting; and we introduce a novel
discounting method, named Beta-weighted discounting, which is a practical way of using non-exponential
discounting.
2 Related Work
Discounted utility is a concept commonly used in psychology (Ainslie & Haslam, 1992) and economics
(Lazaro et al., 2002) to understand how humans and other animals (Hayden, 2016) choose between a small
immediate reward or a larger delayed reward. The simplest model is exponential discounting, where fu-
ture rewards are considered with exponentially decaying importance. Hyperbolic discounting is sometimes
proposed as a more empirically accurate alternative (Ainslie & Haslam, 1992), however it is nonsummable,
leading to problems in continuing environments. Other works (Hutter, 2006; Lattimore & Hutter, 2011)
consider arbitrary discounting matrices that could vary over time. Schultheis et al. (2022) propose a formal
method for non-exponential discounting in continuous-time reinforcement learning under the framework of
continuous control.
The same mechanism of discounting future rewards is used in Reinforcement Learning to ensure computa-
tional stability and proper handling of hazard in the environment (Sozou, 1998; Fedus et al., 2019). It has
been also shown to work as a regularizer (Amit et al., 2020) for Temporal Difference methods, especially
when the value of the discount factor is low. The choice to discount future rewards has been criticized from
a theoretical standpoint (Naik et al., 2019) as it may favor policies suboptimal with respect to the undis-
counted reward. The alternative is using the undiscounted reward for episodic tasks, and the average reward
for continuing tasks (Siddique et al., 2020). While this idea is interesting, it has not gained wide acceptance
in practice, seeing as setting the discount factor to a nontrivial value often improves the performance of RL
algorithms.
In contrast to the hyperbolic discounting of Fedus et al. (2019), we propose Beta-weighted discounting
that uses a more general weighing distribution which can be reduced to both exponential and hyperbolic
discounting. Furthermore, through our proposed UGAE it is applied to stochastic Actor-Critic algorithms
2Under review as submission to TMLR
in discrete-time RL problems, as opposed to the Temporal Difference-based algorithms common in the non-
exponential discounting literature (Maia, 2009; Alexander & Brown, 2010), and the continuous-time setting
of Schultheis et al. (2022).
Policy gradient (PG) algorithms are derived from REINFORCE (Williams, 1992). They directly optimize
the expected reward by following its gradient with respect to the parameters of the (stochastic) policy. Many
modern algorithms use this approach, such as TRPO (Schulman et al., 2015) as PPO (Schulman et al., 2017).
In our work, we use PPO for the experiments. Their central idea is the policy gradient understood as the
gradient of the agent’s expected returns w.r.t. the policy parameters, with which the policy can be optimized
using Gradient Ascent or an algorithm derived from it like Adam (Kingma & Ba, 2015). A practical way of
computing the gradient, equivalent in expectation, uses the advantages based on returns-to-go obtained by
disregarding past rewards and subtracting a value baseline.
Advantage estimation is a useful tool for reducing the variance of the policy gradient estimation without
changing the expectation, hence without increasing the bias (Sutton et al., 1999; Mnih et al., 2016). Its key
idea is that, instead of using the raw returns, it is more valuable to know how much better an action is than
expected. This is measured by the Advantage, equal to the difference between the expected value of taking
a certain action in a given state, and the expected value of that state following the policy.
Pitis (2019) introduces a formally justified method of performing discounting with the value of the discount
factor being dependent on the current state of the environment and the action taken by the agent. This
approach makes it possible to model a wide range of human preferences, some of which cannot be modelled
as Markov Decision Processes with a constant exponential discount factor. A similar idea is present in
Temporal Difference algorithms (Sutton & Barto, 2018), where their (typically constant) parameters can be
state and action-dependent.
Generalized Advantage Estimation (GAE) (Schulman et al., 2018) computes the Advantage by con-
sidering many ways of bootstrapping the value estimation, weighted exponentially, analogously to TD( λ).
At the extreme points of its λparameter, it is equivalent to Temporal Difference or Monte Carlo estimation.
GAE has become the standard method of computing advantages in Actor-Critic algorithms (Schulman et al.,
2017) due to its simple implementation and the performance improvements.
In contrast, we propose a new, vectorized formulation of GAE that allows using arbitrary discounting mech-
anisms, including our Beta-weighted discounting, in advantage-based Actor-Critic algorithms. This enables
greater flexibility in choosing the discounting method and gives a practical way of doing non-exponential
discounting by setting the parameters of Beta-weighted discounting.
3 UGAE – Universal Generalized Advantage Estimation
In this section, we introduce the main contribution of this paper, UGAE, which is a way of combining the
GAE algorithm (Schulman et al., 2018) with non-exponential discounting methods. Then, we define several
discounting methods that will be further explored in this work.
Problem Setting We formulate the RL problem as a Markov Decision Process (MDP) (Bellman, 1957).
An MDP is defined as a tuple M= (S,A,P,R,µ ), whereSis the set of states, Ais the set of actions,
P:S×A→∆Sis the transition function, R:S×A→Ris the reward function and µ∈∆Sis the initial
state distribution. Note that ∆Xrepresents the set of probability distributions on a given set X. An agent
is characterized by a stochastic policy π:S→ ∆A, at each step tsampling an action at∼π(st), observing
the environment’s new state st+1∼P(st,at), and receiving a reward rt=R(st,at). Over time, the agent
collects a trajectory τ=⟨s0,a0,r0,s1,a1,r1,...⟩, which may be finite (episodic tasks) or infinite (continuing
tasks).
ThetypicalgoalofanRLagentismaximizingthetotalreward/summationtextT
t=0rt, whereTisthedurationoftheepisode
(potentially infinite). A commonly used direct objective for the agent to optimize is the total discounted
reward/summationtextT
t=0γtrtunder a given discount factor γ(Sutton & Barto, 2018). Using a discount factor can serve
as a regularizer for the agent (Amit et al., 2020), and is needed for continuing tasks ( T=∞) to ensure that
the total reward remains finite.
3Under review as submission to TMLR
Inthiswork, weconsideramoregeneralscenariothatallowsnon-exponentialdiscountingmechanismsdefined
by a function Γ(·):N→[0,1]. The optimization objective is then expressed as RΓ=/summationtext∞
t=0Γ(t)rt.
3.1 UGAE
The original derivation of GAE relies on the assumption that the rewards are discounted exponentially.
While the main idea remains valid, the transformations that follow and the resulting implementation cannot
be used with a different discounting scheme.
Recall that GAE considers multiple k-step advantages, each defined as:
ˆA(k)
t=−V(st) +k−1/summationdisplay
l=0γlrt+l+γkV(st+k). (1)
Given a weighing parameter λ, the GAE advantage is then:
ˆAGAE (γ,λ)
t := (1−λ)(ˆA(1)
t+λˆA(2)
t+...) =∞/summationdisplay
l=0(γλ)lδV
t+l
whereδV
t=rt+γV(st+1)−V(st). While this formulation makes it possible to compute all the advantages
in a dataset with an efficient, single-pass algorithm, it cannot be used with a general discounting method.
In particular, δV
tcannot be used as its value depends on which timestep’s advantage we are computing.
To tackle this, we propose an alternative expression using an arbitrary discount vector Γ(t). To this end, we
redefine the k-step advantage using this concept, as a replacement for Equation 1:
˜A(k)
t=−V(st) +k−1/summationdisplay
l=0Γ(l)rt+l+ Γ(k)V(st+k) (2)
We then expand it to obtain the equivalent to Equation 2:
˜AUGAE (Γ,λ)
t := (1−λ)(˜A(1)
t+λ˜A(2)
t+...) (3)
=−V(st) +∞/summationdisplay
l=0λlΓ(l)rt+l+ (1−λ)∞/summationdisplay
l=0Γ(l+1)λlV(st+l+1)
Note that the second and third terms are both sums of products, and can therefore be interpreted as scalar
productsofappropriatevectors. Bydefining rrrt= [rt+i]i∈N,VVVt= [V(st+i)]i∈N,ΓΓΓ = [Γ(i)]i∈N,ΓΓΓ′= [Γ(i+1)]i∈N,
λλλ= [λi]i∈N, we rewrite Equation 3 in a vectorized form in Equation 4. We use the notation that xxx⊙yyy
represents the Hadamard (element-wise) product, and xxx·yyy– the scalar product.
Theorem 1. UGAE: GAE with arbitrary discounting
Considerrrrt,VVVt,ΓΓΓ,ΓΓΓ′,λλλ,λdefined as above. We can compute GAE with arbitrary discounting as:
˜AUGAE (Γ,λ)
t :=−V(st) + (λλλ⊙ΓΓΓ)·rrrt+ (1−λ)(λλλ⊙ΓΓΓ′)·VVVt+1
IfΓ(t)=γt, this is equivalent to the standard GAE advantage. Proofis in the supplemental material.
Discussion. Theorem 1 gives a vectorized formulation of GAE. This makes it possible to use GAE with
arbitrary discounting methods with little computational overhead, by leveraging optimized vector computa-
tions.
Note that while the complexity of exponential GAE computation for an entire episode is O(T)whereTis the
episode length, the vectorized formulation increases it to O(T2)due to the need for multiplying large vectors.
Fortunately, truncating the future rewards is trivial using the vectorized formulation, and that can be used
4Under review as submission to TMLR
through truncating the discounting horizon, by setting a maximum length Lof the vectors in Theorem 1.
The complexity in that case is O(LT), so again linear in Tas long asLstays constant. In practice, as we
show in this paper, the computational cost is not of significant concern, and the truncation is not necessary,
as the asymptotic complexity only becomes noticeable with unusually long episodes.
3.2 Added estimation bias
An important aspect of our method is the additional bias it introduces to the value estimation. To compute
a k-step advantage, we must evaluate the tail of the reward sequence using the discounting itself (the
V(st+k)term in Equation 1). This is impossible with any non-exponential discounting, as the property
Γ(k+t)= Γ(k)Γ(t)implies Γ(·)being an exponential function. Seeing as we are performing an estimation of
the value of those last steps, this results in an increase in the estimation bias compared to Monte Carlo
estimation.
This ties into the general bias-variance trade-off when using GAE or TD-lambda estimation. In its original
form, it performs interpolation between high-variance (Monte Carlo) and high-bias (TD) estimates for ex-
ponential discounting. In the case of non-exponential discounting, using UGAE as opposed to Monte Carlo
estimates has the same effect of an increase in bias, but decreasing the variance in return.
The difference between the non-exponential discounting and its tail contributes to an additional increase of
bias beyond that caused by using GAE, but we show that this bias remains finite in the infinite time horizon
for summable discountings (including our Beta-weighted discounting, as well as any truncated discounting).
Theorem 2. UGAE added bias
Consider an arbitrary summable discounting Γ(t). The additional bias, defined as the discrepancy between the
UGAE and Monte Carlo value estimations, is finite in the infinite time horizon. Proofis in the supplemental
material.
In practice, as we show in our experiments, the decreased variance enabled by UGAE effectively counteracts
the added bias, resulting in an overall better performance over Monte Carlo estimation.
4 Beta-weighted discounting
In this section, we present our second contribution, i.e. Beta-weighted discounting. It uses the Beta distri-
bution as weights for all values of γ∈[0,1]. We use their expected value as the effective discount factors Γ(t),
which we show to be equal to the distribution’s raw moments.
4.1 Beta-weighted discounting
As a simple illustrative example, let us use two discount factors γ1,γ2with weights pand(1−p)respectively,
wherep∈[0,1]. This can be treated as a multiple-reward problem (Shelton, 2000) where the total reward
is a weighted sum of individual rewards Rγ=/summationtext∞
t=0γtrt. Therefore, we have:
R(γ1,γ2)=p/summationdisplay
γt
1rt+ (1−p)/summationdisplay
γt
2rt (4)
=/summationdisplay
rt(pγt
1+ (1−p)γt
2)
We extend this reasoning to any countable number of exponential discount factors with arbitrary weights,
that sum up to 1. Taking this to the continuous limit, we also consider continuous distributions of discount
factorsw∈∆([0,1]), leading to the equation:
Rw=/summationdisplay
rt/integraldisplay1
0w(γ)γtdt=/summationdisplay
rtΓ(t)(5)
An important observation is that as long as supp (w)⊆[0,1], the integral is by definition equal to the t-th raw
moment of the distribution w. Hence, with an appropriately chosen distribution. an analytical expression is
obtained for all its moments, and therefore, all the individual discount factors Γ(t).
5Under review as submission to TMLR
We choosethe Beta distribution due to the simple analytical formula for its moments, as well as its relation to
othercommondiscountingmethods. Itsprobabilitydensityfunctionisdefinedas f(x;α,β)∝xα−1(1−x)β−1.
Note that with β= 1, it is equivalent to the exponential distribution which induces a hyperbolic discounting.
Its moments are known in analytical form (Johnson et al., 1994), which leads to our proposed discounting
mechanism in Theorem 3.
Theorem 3. Beta-weighted discounting
Considerα,β∈[0,∞). The following equations hold for the Beta-weighted discount vector parametrized by
α,β.Proofis in the supplementary material.
Γ(t)=t−1/productdisplay
k=0α+k
α+β+k(6)
Γ(t+1)=α+t
α+β+tΓ(t)(7)
4.2 Beta distribution properties
Here, we investigate the Beta distribution’s parameter space and consider an alternative parametrization
that eases its tuning. We also analyze important properties of the Beta-weighted discounting based on those
parameters, and compare them to exponential and hyperbolic baselines.
Canonically, the Beta distribution is defined by parameters α,β∈(0,∞). It is worth noting certain spe-
cial cases and how this approach generalizes other discounting methods. When α,β→∞such that its
meanµ:=α
α+β=const., the beta distribution asymptotically approaches the Dirac delta distribution
δ(x−µ), resulting in the usual exponential discounting Γ(t)=µt. Alternatively, when β= 1, we get
Γ(t)=/producttextt−1
k=0α+k
α+k+1=α
α+t=1
1+t/α, i.e. hyperbolic discounting.
Meanµand dispersion ηA key property is that we would like the effective discount rate to be comparable
with existing exponential discount factors. To do so, we define a more intuitive parameter to directly control
the distribution’s mean as µ=α
α+β∈(0,1).µdefines the center of the distribution and should therefore be
close to typically used γvalues in exponential discounting.
A second intuitive parameter should control the dispersion of the distribution. Depending on the context,
two choices seem natural: βitself, or its inverse η=1
β. As stated earlier, βcan take any positive real value.
By discarding values of β < 1which correspond to a local maximum of the probability density function
around 0, we obtain η∈(0,1]. That way we obtain an easy-to-interpret discounting strategy. as we show in
Lemma 4,η→0andη= 1correspond to exponential discounting, and hyperbolic discounting, respectively,
which allows us to finally define the range of ηas[0,1]. Other values smoothly interpolate between both of
these methods, similar to how GAE interpolates between Monte Carlo and Temporal Difference estimation.
Given the values of µandη, the original distribution parameters can be recovered as α=µ
η(1−µ)andβ=1
η.
The raw moments parametrized by µandηaremt=/producttextt−1
k=0µ+kη(1−µ)
1+kη(1−µ).
Lemma 4. Special cases of Beta-weighted discounting
We explore the relation of Beta-weighted discounting to exponential and hyperbolic discountings. Consider
the Beta-weighted discounting Γ(t)parametrized by µ∈(0,1),η∈(0,1]. The following is true:
•ifη→0, then Γ(t)=µt, i.e. it is equal to exponential discounting
•ifη= 1, then Γ(t)=µ
µ+(1−µ)t=1
1+t/α, i.e. it is equal to hyperbolic discounting
Proofis in the supplemental material.
Discussion. Beta-weighted discounting is controlled by two parameters (u,η), which includes the classic
exponential discounting, but also enables more flexibility in designing agent behaviors .
6Under review as submission to TMLR
Lemma 5. Beta-weighted discounting summability
Given the Beta-weighted discount vector Γ(t)=/producttextt−1
k=0α+k
α+β+k,α∈[0,∞),β∈[0,∞), the following property
holds. Proofis in the supplemental material.
∞/summationdisplay
t=0Γ(t)=/braceleftiggα+β−1
β−1ifβ >1
∞ otherwise(8)
Discussion. Lemma 5 describes the conditions under which the Beta-weighted discounting is summable
depending on its parameters. While less critical for episodic tasks, summability of the discount function is
important for continuing tasks. Otherwise, the discounted reward can grow arbitrarily high over time.
5 Analysis of non-exponential discounting methods
Here, our goal is to justify the usage, and enable deep understanding of different discounting methods. To
this end, we first analyze some of their main properties: the importance of future rewards, the variance
of the discounted rewards, the effective planning horizon, and the total sum of the discounting. Then, we
compare those properties among the previously described discounting methods.
Since not all discounting methods are summable (particularly the cases of hyperbolic and no discounting),
we consider the maximum (“infinite”) episode length to be 10000 steps. We focus on a characteristic time
scale of the environment around 100 steps.
5.1 Properties of discounting
Importance of future rewards Properly describing the influence of the future under a specific discounting
is challenging. On one hand, individual rewards are typically counted with a smaller weight, as discount
vectors Γ(t)are usually monotonically decreasing. On the other hand, the longer the considered time horizon
is, the more timesteps it includes, increasing its overall importance. Furthermore, a long time horizon (e.g.
100 steps) directly includes a shorter horizon (e.g. 10 steps), and therefore, the partial sums are not directly
comparable. To balance these aspects, we focus on the importance of the first 100 steps using the following
expressions:
Γt2
t1=/summationtextt2
t=t1Γ(t)
/summationtext∞
t=0Γ(t)(9)
Variance of the discounted rewards The overall objective of the RL agent is maximizing the total
(discounted) reward it obtains in the environment. Since both the policy and the environment can be
stochastic, the total reward will also be subject to some uncertainty. While the exact rewards, as well as
their variances, depend heavily on the exact environment, we make the simplifying assumption that the
instantaneous rewards rtare sampled according to a distribution Dwith a constant variance of σ2, e.g.
rt∼D=N(µ,σ2)with an arbitrary, possibly varying, µ. We also assume all rewards to be uncorrelated,
which leads to the following expression:
Var[T/summationdisplay
t=0Γ(t)rt] =T/summationdisplay
t=0Γ(t)2Var[rt] =T/summationdisplay
t=0Γ(t)2σ2=σ2T/summationdisplay
t=0Γ(t)2
Equation 10 shows that the variance of the total discounted reward is proportional to the sum of all the
squares of discount factors. While in some cases it is easy to obtain analytically, quite often the expression
can be complex and difficult to obtain and analyze; hence, we consider the numerical values, as well as
analytical expressions where applicable.
Effective planning horizon For any discounting Γ(t), our goal is to have a measure of its effective planning
horizon. However, in most cases, we cannot have a clear point beyond which the rewards do not matter and
there is not a unique notion of a time horizon that could be specified. Thus, to maintain consistency with
7Under review as submission to TMLR
the standard notion of a time horizon from exponential discounting, we define the effective time horizon as
the timestep Teffafter which approximately1
e(≈37%)of the weight of the discounting remains.
Total sum of the discounting Depending on the RL algorithm and the reward normalization method
(if any), the magnitude of the total discounted reward might impact the stability of the training, as neural
networks typically cannot deal with very large numbers. For this reason, we consider the sum of all rewards
under a given discounting.
Consistency It is worth keeping in mind that, as pointed out by Lattimore & Hutter (2011), the only time
consistent discounting is the exponential discounting. This means that it is possible that other methods
cause the agent to change its plans over time. While this has the potential to significantly degrade the
performance in some specific environments, that is often not the case in typical RL tasks, as we show in
Section 6.
5.2 Discounting methods
Beta-weightedDiscounting isdescribedinthefollowingsection. Exponentialandhyperbolicdiscountings
are equivalent to Beta-weighted discounting with η=0andη=1, respectively. The former is given by Γ(t)=µt
withµ∈[0,1], and the latter by Γ(t)=1
1+kt=1
1+1−µ
µtparametrized by k∈[0,∞)orµ∈(0,1].
No Discounting implies that the discount vector takes the form ∀t∈NΓ(t)=1. This is nonsummable, but it
is trivial to compute any partial sums to estimate the importance of future rewards or the variance. The
effective planning horizon depends on the episode length T, and is equal to (1−1
e)T.
Fixed-HorizonDiscounting Here, thereisasingleparameter Tmaxwhichdefineshowmanyfuturerewards
are considered with a constant weight. The discount vector is then Γ(t)=1t<Tmax, and the planning horizon,
according to our definition, is (1−1
e)Tmax.
Truncated Discounting All discounting methods can be truncated by adding an additional parameter
Tmax, and setting the discount vector to 0 for all timesteps t > Tmax. Truncating a discounting decreases
the importance of future rewards and the effective planning horizon, but also decreases the variance of the
total rewards.
5.3 Experimental Analysis
0.0 0.2 0.4 0.6 0.8 1.0
102
101
100Importance of first 100 steps
(a)0.0 0.2 0.4 0.6 0.8 1.0
100101102103104Variance
(b)0.0 0.2 0.4 0.6 0.8 1.0
100101102103Effective horizon
(c)0.0 0.2 0.4 0.6 0.8 1.0
100101102103104Total sum
(d)(0.9, )
 (0.97, )
 (0.99, )
 (0.999, )
 (0.25, )
 (1.0, )
 (0.99, 500)
Figure 1: Different properties of a discounting, as a function of η, with given ( µ,Tmax) parameters listed
in the legend. (a) Importance of the near future (b) Variance measure (c) Effective time horizon (d) Total
discounting sum
To analyze the properties of different discounting methods, we compute them for a set of relevant discounting
methods and their parameters. The impact of Beta discounting’s ηis illustrated in Figure 1(a-d), showing:
(a) the importance of first 100 steps Γ100
0, (b) the variance measure, (c) the effective time horizon, (d) the
total sum of the discounting – full results are in the supplement. Note that the choice of the discounting is
an example of the common bias-variance trade-off. If the real objective is maximizing the total undiscounted
8Under review as submission to TMLR
reward, decreasing the weights of future rewards inevitably biases the algorithm’s value estimation, while
simultaneously decreasing its variance.
Usingno discounting , the rewards from the distant future have a dominant contribution to the total reward
estimate since more steps are included. Exponential discounting places more emphasis on the short term
rewards, according to its γparameter, while simultaneously decreasing the variance; when γ= 0.99, it
effectively disregards the distant future of t>1000.
InBeta-weighted discounting withη>0, the future rewards importance, the variance and the effective
time horizon increase with η. Ifµis adjusted to make Teffsimilar to exponential discounting’s value,
the variance decreases significantly, and the balance of different time horizons shifts towards the future,
maintaining some weight on the distant future.
Withhyperbolic discounting (Beta-weighted with η= 1) the distant future becomes very important,
and the effective time horizon becomes very large (in fact, infinite in the limit of an infinitely long episode).
To reduce the time horizon to a value close to 100, its µparameter has to be very small, near µ= 0.25,
putting most of the weight on rewards that are close in time, but also including the distant rewards unlike
any exponential discounting.
The behavior of fixed-horizon discounting is simple – it acts like no discounting, but ignores any rewards
beyondTmax. Truncating another discounting method results in ignoring the rewards beyond Tmax, and
decreasing the variance and the effective time horizon (see supplemental material).
In summary, by modifying ηin Beta-weighted discounting, and Tmaxin the truncated scenario, we can
successfully change various aspects of the resulting discounting in a more flexible way than with exponential
discounting. In general, changing the discounting method can bias the agent to favor certain timescales and
focus on maximizing the rewards that occur within them.
5.4 Discussion
Whenηis increased, the importance shifts towards the future (Figure 1a), variance (Figure 1b) and the
effective horizon (Figure 1c) increase. Introducing a truncation (pink line) decreases the effective horizon
and shifts the reward importance towards the near rewards. When truncated at 100 steps, Beta-weighted
discounting with η= 0.5and hyperbolic discounting ( η= 1) have very similar properties, indicating their
main difference lies in how they deal with the distant future. This is confirmed by comparing the non-
truncated versions, where hyperbolic discounting puts a very large weight on the distant future, unlike the
Beta-weighted discounting.
5.5 Why non-exponential discounting?
A natural question that arises during this discussion is – why do we even want to train agents with non-
exponential discounting? As described by Naik et al. (2019), optimizing a discounted reward is not equivalent
to optimizing the reward itself. The choice of a discounting method affects the optimal policy, or even its
existence in the first place. While we do not tackle this problem in this work, we enable larger flexibility
in designing the discounting mechanism. This in turn allows researchers to generate more diverse emergent
behaviors through the choice of an appropriate discounting – in case that exponential discounting leads to
undesirable results.
As mentioned earlier, any discounting other than exponential has the potential for inconsistent behavior.
This means that the agent may change its mind on a decision as time passes, without any additional changes
to the situation. While this behavior is admittedly irrational, it is commonly exhibited by humans (Ainslie
& Haslam, 1992). Therefore, it is important to take this into consideration when creating agents meant to
mimichumanbehaviorinapplicationslikevideogamesorsocialrobotics, wherehuman-likenessisimportant,
and potentially not appropriately reflected in the reward function.
6 DRL Experiments
9Under review as submission to TMLR
0 250 500 750 1000 1250 1500 1750 2000
Training step02000400060008000 Mean episode reward=0.8
=0.9
=1
(a) Inverted Double Pendulum, η= 0.8.
0 250 500 750 1000 1250 1500 1750 2000
Training step400006000080000100000120000140000 Mean episode reward=0.95
=1
 (b) Humanoid Standup, η= 0.5.
0 200 400 600 800 1000
Training step200
150
100
50
0Mean episode reward=0.891
=1
(c) Crowd Crossing, η= 0.8.
0 200 400 600 800 1000
Training step250
200
150
100
50
0Mean episode reward=0.891
=1
 (d) Crowd Corridor, η= 0.5.
Figure 3: Training curves in DRL experiments using non-exponential discounting. All curves are averaged
across 8 independent training runs. Shading indicates the standard error of the mean. In all experiments,
usingλvalues that were tuned for optimality with exponential discounting, UGAE significantly outperforms
the MC baseline ( λ= 1). This indicates that UGAE enables translating the benefits of GAE to non-
exponential discounting.
(a) Corridor
 (b) Crossway
Figure 2: Visualizations of the two crowd simulation
scenarios used in the experiments. In both cases, each
agent needs to reach the opposite end of their respec-
tive route, and is then removed from the simulation.In this section, we evaluate our UGAE for train-
ing DRL agents with non-exponential discounting,
in both single-agent and multiagent environments.
As the baseline, we use non-exponential discount-
ing with regular Monte Carlo (MC) advantage esti-
mation, equivalent to UGAE with λ= 1. We use
Beta-weighted discounting to parametrize the non-
exponential discounting with its ηvalue. The code
withhyperparametersandotherimplementationde-
tails will be released upon publication.
We use four test environments to evaluate our
discounting method: InvertedDoublePendulum-
v4 and HumanoidStandup-v4 from MuJoCo via
Gym (Todorov et al., 2012; Brockman et al., 2016);
Crossway and Corridor crowd simulation scenarios
with 50 homogeneous agents each, introduced by
Kwiatkowski et al. (2023). The crowd scenarios are
displayed in Figure 2. We chose these environments because their optimal λvalues with exponential GAE
are relatively far from λ= 1based on prior work. In environments where GAE does not provide benefit over
MC estimation, we similarly do not expect an improvement with non-exponential discounting.
Inverted Double Pendulum and Humanoid Standup are both skeletal control tasks with low- and high-
dimensional controls, respectively. The former has an 11-dimensional observation space and 1-dimensional
action space, whereas the latter has a 376-dimensional observation space, and a 17-dimensional action space.
The crowd simulation scenarios use a hybrid perception model combining raycasting to perceive walls, and
direct agent perception for neighboring agents for a total of 177-dimensional vector observation and 4-
dimensionalembeddingofeachneighbor, asdescribedinKwiatkowskietal.(2023). Theyusea2-dimensional
10Under review as submission to TMLR
action space with polar velocity dynamics. The episode length is 1000 for MuJoCo experiments, and 200 for
crowd simulation experiments.
We train the agents with the PPO algorithm, with hyperparameters based on the RL Baselines Zoo (Raffin,
2020) for the MuJoCo environments, and from Kwiatkowski et al. (2023) for the crowd environments. It is
worth noting that the MuJoCo hyperparameters have been tuned for a prior version of the environments
(v3), and thus the results can be different. We use the optimal value of λin the exponential discounting
paradigm, and apply it analogously with UGAE. A single training takes 2-5 hours with a consumer GPU.
6.1 Results
We show the results in Figure 3. In all tested environments, training with UGAE leads to a higher perfor-
mance compared to the MC baseline, with the largest effect being present in the Inverted Double Pendulum
where UGAE achieves a mean episode reward of 8213±1067, while MC only achieves 3364±1078. The
effect is smaller in the Humanoid Standup task, but still significant, with the final rewards being 137300
±3400and129000±2520respectively. In the crowd scenarios, a more detailed analysis of the emergent
behaviors indicates that agents trained with MC fail to maintain a comfortable speed (which is part of the
reward function), while UGAE agents are able to efficiently navigate to their goals. This results in rewards
of11.2±1.457for UGAE and -0.156±2.745for MC in the Corridor scenario; and -1.46±1.50for
UGAE and -35.15±8.81for MC in the Crossway scenario.
6.2 Computation time
100
101
102
103
104
105
Episode length103
101
101103105107Time (ms)Vectorized UGAE
Recursive GAE
Training step duration
Figure 4: Time needed to compute GAE (orange) and
UGAE (blue) with a single consumer CPU, on log-log
scale. The green line is a reference duration of 10 sec-
onds representing a typical training iteration. While
UGAE is more expensive, with typical training step
durations, the time to compute its values is negligible.To estimate the computational impact of our vec-
torized UGAE formulation as compared to the
standard recursive GAE, we generate 16 random
episodes with a length between 1 and 100,000 steps,
and plot the time needed to compute the advantages
as a function of the episode length. The results are
in Figure 4. For a reference duration of a full train-
ing step, we use 10 seconds. It shows that while
the computational cost of UGAE (blue) is larger
than that of GAE (orange line), it remains insignif-
icant compared to a full training step with episodes
shorter than 104steps. For longer episodes, it be-
comes noticeable, however, this is rarely the case in
practice.
6.3 Discussion
As our experiments show, using UGAE with
episodes of length up to ca. 103steps carries a neg-
ligible computational cost, allowing its seamless in-
tegration into a PPO training pipeline. At the same
time, it enables a performance improvement mirroring that of GAE, but for non-exponential discounting.
In conjunction with Beta-weighted discounting, it enables practical and efficient training of agents with
non-exponential discounting.
The main limitation of our work lies in the asymptotic complexity of advantage computation. The time
needed to compute the UGAE advantage is negligible with episodes up to around 103steps, and becomes
noticeable (although still not overwhelmingly so) at around 104steps. In the rare scenario one needs to
compute the advantages for episodes over 104steps, the computation may become too expensive and require
truncating the discounting, or otherwise optimizing it.
Our beta-weighted discounting adds a new hyperparameter, which is a potential challenge, as RL algorithms
typically already have a large number of hyperparameters that must be optimized. However, due to the
11Under review as submission to TMLR
interpretation of η, there is a natural default value of η= 0which corresponds to exponential discounting.
With the other extreme being η= 1, this yields a compact range of possible values that can be easily included
in a hyperparameter optimization procedure. This also opens the door to further research on automatically
tuning the discount factor from a wider family of possibilities as opposed to just exponential methods.
7 Conclusions
Our work follows the exciting trend of rethinking the discounting mechanism in RL. In typical applications,
our UGAE can be used with negligible overhead, and together with Beta-weighted discounting they provide
anelegantwaytoperformefficientnon-exponentialdiscounting. Toourknowledge, UGAEisthefirstmethod
that enables using arbitrary discounting mechanisms in Actor-Critic algorithms. Our experiments show that
using non-exponential discounting gives more flexibility in the temporal properties of the RL agent, and thus
enables more diverse, potentially human-like, emergent behaviors.
Importantly, this work makes it possible for researchers to empirically investigate different methods of dis-
counting and their relation with various RL problems, including state-dependent discounting. A challenging
but valuable contribution would be developing a method to analyze the properties of an environment, and
relating them to the ideal discounting method. Finally, developing an analogous method for value-based al-
gorithms like DQN or DDPG would further broaden the applicability of non-exponential discounting across
a more comprehensive selection of cutting-edge RL algorithms. In doing so, we anticipate that our work will
inspire and inform the ongoing evolution of reinforcement learning and its diverse applications.
References
George Ainslie and Nick Haslam. Hyperbolic discounting. In Choice over time , pp. 57–92. Russell Sage
Foundation, New York, NY, US, 1992. ISBN 978-0-87154-558-9.
William H. Alexander and Joshua W. Brown. Hyperbolically Discounted Temporal Difference Learning.
Neural computation , 22(6):1511–1527, June 2010. ISSN 0899-7667. doi: 10.1162/neco.2010.08-09-1080.
Sayel A. Ali. The mth Ratio Test: New Convergence Tests for Series. The American Mathematical Monthly ,
115(6):514–524, June 2008. ISSN 0002-9890. doi: 10.1080/00029890.2008.11920558. Publisher: Taylor &
Francis _eprint: https://doi.org/10.1080/00029890.2008.11920558.
Ron Amit, Ron Meir, and Kamil Ciosek. Discount Factor as a Regularizer in Reinforcement Learning. In
International Conference on Machine Learning , pp. 269–278. PMLR, November 2020. ISSN: 2640-3498.
Richard Bellman. A Markovian Decision Process. Journal of Mathematics and Mechanics , 6(5):679–684,
1957. ISSN 0095-9057. Publisher: Indiana University Mathematics Department.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. OpenAI Gym. arXiv:1606.01540 [cs] , June 2016.
William Fedus, Carles Gelada, Yoshua Bengio, Marc G. Bellemare, and Hugo Larochelle. Hyperbolic
Discounting and Learning over Multiple Horizons. arXiv:1902.06865 [cs, stat] , February 2019. arXiv:
1902.06865.
Benjamin Y. Hayden. Time discounting and time preference in animals: A critical review. Psychonomic
Bulletin & Review , 23(1):39–53, February 2016. ISSN 1531-5320. doi: 10.3758/s13423-015-0879-3.
Marcus Hutter. General Discounting Versus Average Reward. In José L. Balcázar, Philip M. Long, and
Frank Stephan (eds.), Algorithmic Learning Theory , Lecture Notes in Computer Science, pp. 244–258,
Berlin, Heidelberg, 2006. Springer. ISBN 978-3-540-46650-5. doi: 10.1007/11894841_21.
Norman Lloyd Johnson, Samuel Kotz, and N. Balakrishnan. Continuous univariate distributions . Wiley
series in probability and mathematical statistics. Wiley, New York, 2nd ed edition, 1994. ISBN 978-0-471-
58495-7 978-0-471-58494-0.
12Under review as submission to TMLR
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Yoshua Bengio and
Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL http://arxiv.org/abs/1412.6980 .
Ariel Kwiatkowski, Vicky Kalogeiton, Julien Pettré, and Marie-Paule Cani. Understanding reinforcement
learned crowds. Computers & Graphics , 110:28–37, February 2023. ISSN 00978493. doi: 10.1016/j.cag.
2022.11.007. URL https://linkinghub.elsevier.com/retrieve/pii/S0097849322002035 .
Tor Lattimore and Marcus Hutter. Time Consistent Discounting. In Jyrki Kivinen, Csaba Szepesvári,
Esko Ukkonen, and Thomas Zeugmann (eds.), Algorithmic Learning Theory , Lecture Notes in Com-
puter Science, pp. 383–397, Berlin, Heidelberg, 2011. Springer. ISBN 978-3-642-24412-4. doi: 10.1007/
978-3-642-24412-4_30.
Angelina Lazaro, Ramon Barberan, and Encarnacion Rubio. The discounted utility model and social pref-
erences:: Some alternative formulations to conventional discounting. Journal of Economic Psychology , 23
(3):317–337, June 2002. ISSN 0167-4870. doi: 10.1016/S0167-4870(02)00079-X.
Tiago V. Maia. Reinforcement learning, conditioning, and the brain: Successes and challenges. Cognitive,
Affective, & Behavioral Neuroscience , 9(4):343–364, December 2009. ISSN 1531-135X. doi: 10.3758/
CABN.9.4.343.
Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Tim Harley, Timothy P. Lillicrap,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Pro-
ceedings of the 33rd International Conference on International Conference on Machine Learning - Volume
48, ICML’16, pp. 1928–1937, New York, NY, USA, June 2016. JMLR.org.
Abhishek Naik, Roshan Shariff, Niko Yasui, Hengshuai Yao, and Richard S. Sutton. Discounted Rein-
forcement Learning Is Not an Optimization Problem. arXiv:1910.02140 [cs] , November 2019. arXiv:
1910.02140.
Silviu Pitis. Rethinking the Discount Factor in Reinforcement Learning: A Decision Theoretic Approach.
Proceedings of the AAAI Conference on Artificial Intelligence , 33:7949–7956, July 2019. ISSN 2374-3468,
2159-5399. doi: 10.1609/aaai.v33i01.33017949. URL https://aaai.org/ojs/index.php/AAAI/article/
view/4795 .
Antonin Raffin. RL Baselines3 Zoo . GitHub, 2020. URL https://github.com/DLR-RM/
rl-baselines3-zoo . Publication Title: GitHub repository.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust Region Policy
Optimization. In International Conference on Machine Learning , pp. 1889–1897. PMLR, June 2015. ISSN:
1938-7228.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Opti-
mization Algorithms. arXiv:1707.06347 [cs] , August 2017.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-Dimensional
Continuous Control Using Generalized Advantage Estimation. arXiv:1506.02438 [cs] , October 2018.
Matthias Schultheis, Constantin A. Rothkopf, and Heinz Koeppl. Reinforcement Learning with Non-
Exponential Discounting, December 2022. URL http://arxiv.org/abs/2209.13413 . arXiv:2209.13413
[cs, eess, q-bio, stat].
Christian R. Shelton. Balancing multiple sources of reward in reinforcement learning. In Proceedings of
the 13th International Conference on Neural Information Processing Systems , NIPS’00, pp. 1038–1044,
Cambridge, MA, USA, January 2000. MIT Press.
Umer Siddique, Paul Weng, and Matthieu Zimmer. Learning Fair Policies in Multi-Objective (Deep) Re-
inforcement Learning with Average and Discounted Rewards. In International Conference on Machine
Learning , pp. 8905–8915. PMLR, November 2020. ISSN: 2640-3498.
13Under review as submission to TMLR
P. D. Sozou. On hyperbolic discounting and uncertain hazard rates. Proceedings of the Royal Society B:
Biological Sciences , 265(1409):2015–2020, October 1998. ISSN 0962-8452. doi: 10.1098/rspb.1998.0534.
R. H. Strotz. Myopia and Inconsistency in Dynamic Utility Maximization. The Review of Economic Studies ,
23(3):165–180, 1955. ISSN 0034-6527. doi: 10.2307/2295722. Publisher: [Oxford University Press, Review
of Economic Studies, Ltd.].
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction . A Bradford Book,
Cambridge, MA, USA, 2018. ISBN 978-0-262-03924-6.
Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for
reinforcement learning with function approximation. In Proceedings of the 12th International Conference
on Neural Information Processing Systems , NIPS’99, pp. 1057–1063, Cambridge, MA, USA, November
1999. MIT Press.
E. Todorov, T. Erez, and Y. Tassa. MuJoCo: A physics engine for model-based control. In 2012 IEEE/RSJ
International Conference on Intelligent Robots and Systems , pp. 5026–5033, October 2012. doi: 10.1109/
IROS.2012.6386109. ISSN: 2153-0866.
Ronald J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement
Learning. Machine Language , 8(3-4):229–256, May 1992. ISSN 0885-6125. doi: 10.1007/BF00992696.
14Under review as submission to TMLR
Supplemental Material
A Proofs
Notation
•Γ(t)– general discount factor at step t
•rt– reward at step t
•V(s)– value estimate of a state s
•Γ(z) =/integraltext∞
0xz−1e−xdx– the usual Gamma function
•B(α,β) =/integraltext1
0tα−1(1−t)β−1dt=Γ(α)Γ(β)
Γ(α+β)
•∆(X)– set of probability distributions on the set X
Theorem 1. UGAE: GAE with arbitrary discounting
Considerrrrt= [rt+i]i∈N,VVVt= [V(st+i)]i∈N,ΓΓΓ = [Γ(i)]i∈N,ΓΓΓ′= [Γ(i+1)]i∈N,λλλ= [λi]i∈N. We define the GAE
with arbitrary discounting as:
˜AUGAE (Γ,λ)
t :=−V(st) + (λλλ⊙ΓΓΓ)·rrrt+ (1−λ)(λλλ⊙ΓΓΓ′)·VVVt+1 (10)
IfΓ(t)=γt, this is equivalent to the standard GAE advantage.
Proof.Recall that we defined the k-step advantage as
˜A(k)
t:=−V(st) +k−1/summationdisplay
l=0Γ(l)rt+l+ Γ(k)V(st+k).
With this, we expand the expression for UGAE as
˜AUGAE (Γ,λ)
t = (1−λ)(˜A(1)
t+λ˜A(2)
t+λ2˜A(3)
t+...)
= (1−λ)/bracketleftbig
−V(st) +rt+ Γ(1)V(st+1)−λV(st) +λrt+λΓ(1)rt+1+λΓ(2)V(st+2) +.../bracketrightbig
= (1−λ)/bracketleftigg
−∞/summationdisplay
l=0(λl)V(st) +∞/summationdisplay
l=0(λl)rt+∞/summationdisplay
l=1(λl)Γ(1)r1+...+ Γ(1)V(st+1) +λΓ(2)V(st+2) +.../bracketrightigg
= (1−λ)/bracketleftbigg
−V(st)
1−λ+rt
1−λ+λΓ(1)rt+1
1−λ+...+ Γ(1)V(st+1) +λΓ(2)V(st+2) +.../bracketrightbigg
=−V(st) +∞/summationdisplay
l=0λlΓ(l)rt+l+ (1−λ)∞/summationdisplay
l=0λlΓ(l+1)V(st+l+1)
=−V(st) + (λλλ⊙ΓΓΓ)·rrrt+ (1−λ)(λλλ⊙ΓΓΓ′)·VVVt+1 (11)
showing the validity of Equation 5 in the main manuscript. To reduce it to standard GAE with exponential
discounting, it is sufficient to replace Γ(···)withγ···in the second line of Equation 11 and follow the proof from
Schulman et al. (2018).
Theorem 2. UGAE added bias
Consider an arbitrary summable discounting Γ(t)in an environment where the reward is bounded by R∈R.
The additional bias, defined as the discrepancy between the UGAE and Monte Carlo value estimations, is
finite.
15Under review as submission to TMLR
Proof.The goal is to find a finite bound on the difference between the empirical (Monte Carlo) value estimate
used for bootstrapping the advantage estimation, and the value estimation used in UGAE, in the infinite
time limit. This can be expressed as follows:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∞/summationdisplay
l=0Γ(l+1)λlˆV(st+l+1)−∞/summationdisplay
l=0λlVΓ
l+1(st+l+1)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(12)
whereVΓ
l+1is the true value as discounted with Γ,lsteps after the step for which we compute the advantage,
and ˆVis the UGAE estimate:
VΓ
k(st) =E/summationdisplay
t′Γ(k+t′)rt′ (13)
ˆVk(st) =E/summationdisplay
t′Γ(t′)rt′ (14)
With this we can expand the expression in Equation 12 as follows:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∞/summationdisplay
l=0Γ(l+1)λlˆV(st+l+1)−∞/summationdisplay
l=0λlVΓ
l+1(st+l+1)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∞/summationdisplay
l=0λl/bracketleftig
Γ(l+1)ˆV(st+l+1)−VΓ
l+1(st+l+1)/bracketrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∞/summationdisplay
l=0λl/bracketleftigg
Γ(l+1)E/summationdisplay
t′Γ(t′)rt′−E/summationdisplay
t′Γ(l+1+t′)rt′/bracketrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=
=E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∞/summationdisplay
l=0λl/summationdisplay
t′/bracketleftig
Γ(l+1)Γ(t′)−Γ(l+1+t′)/bracketrightig
rt′/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤
≤E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∞/summationdisplay
l=1λ(l−1)/summationdisplay
t′/bracketleftig
Γ(l)Γ(t′)−Γ(l+t′)/bracketrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleR (15)
We now focus on the key expression of the last line, which we denote as δΓ
l:
δΓ
l=/summationdisplay
t′Γ(l)Γ(t′)−Γ(l+t′)≤
≤/summationdisplay
t′Γ(l)Γ(t′)≤
≤max
tΓ(t)/summationdisplay
t′Γ(t′)≤∞ (16)
This shows that δΓ
lis finite for any summable discounting Γand for every value of l. Because the δΓ
lterms
are summed with an exponentially decreasing factor λ(l−1)in Equation 15, the total difference in Equation
12 must also be finite, completing the proof.
Theorem 3. Beta-weighted discounting
16Under review as submission to TMLR
Considerα,β∈[0,∞). The following equations hold for the Beta-weighted discount vector parametrized by
α,β:
Γ(t)=t−1/productdisplay
k=0α+k
α+β+k(17)
Γ(t+1)=α+t
α+β+tΓ(t)(18)
Proof.As mentioned in the paper, if we use an effective discount factor obtained by weighing individual
values according to some probability distribution w∈∆([0,1]), the effective discount factor at step tis given
by the distribution’s raw moment Γ(t)=mt, where
mt=/integraldisplay1
0w(γ)γtdγ (19)
Consider the Beta distribution. Its probability distribution function (Johnson et al., 1994) is given by the
following expression:
f(x;α,β) =1
B(α,β)xα−1(1−x)β−1(20)
The raw moments can be obtained as follows:
Γ(t)=mt=/integraldisplay1
0xtf(x;α,β)
=/integraldisplay1
0xt1
B(α,β)xα−1(1−x)β−1
=1
B(α,β)/integraldisplay1
0x(α+t)−1(1−x)β−1
=1
B(α+β)B(α+t,β)
=Γ(α+β)
Γ(α)Γ(β)×
×Γ(α)·α·(α+ 1)·...·(α+t−1)·Γ(β)
Γ(α+β)·(α+β)·(α+β+ 1)·...·(α+β+t−1)
=α·(α+ 1)·...·(α+t−1)
(α+β)·(α+β+ 1)·...·(α+β+t−1)
=t−1/productdisplay
k=0α+k
α+β+k(21)
which proves Equation 17. We then consider the recurrence between consecutive Γ(·)values
Γ(t+1)=t/productdisplay
k=0α+k
α+β+k
=α+t
α+β+tt−1/productdisplay
k=0α+k
α+β+k
=α+t
α+β+tΓ(t)(22)
proving Equation 18 and completing our proof of Theorem 1.
17Under review as submission to TMLR
Lemma 4. Special cases of Beta-weighted discounting
Consider a discounting Γ(t)given by the Beta-weighted discounting parametrized by µ∈(0,1),η∈(0,1]. The
following is true:
•ifη→0, then Γ(t)=µt, i.e. it is equal to exponential discounting
•ifη= 1, then Γ(t)=µ
µ+(1−µ)t=1
1+t/α, i.e. it is equal to hyperbolic discounting
Proof.Remember that µ=α+β
β,η=1
β. Let us first consider η→0, i.e.β→∞so thatα
α+β=const.Note
that this also implies α→∞. Consider the expression for Γ(t):
Γ(t)=t−1/productdisplay
k=0α+k
α+β+k(23)
Asαandβgrow arbitrarily high, the bounded values of kbecome negligible, and the expression can be
reduced to
Γ(t)=t−1/productdisplay
k=0α
α+β=t−1/productdisplay
k=0µ=µt. (24)
Forη= 1, we can reuse the expression obtained in the proof of Lemma 5. As shown there, with β=η= 1,
the effective discount factor is
Γ(t)=1
1 +t/α(25)
which with k=1
α, becomes the usual hyperbolic discounting:
Γ(t)=1
1 +kt(26)
thus completing the proof.
Lemma 5. Beta-weighted discounting summability
Given the Beta-weighted discount vector Γ(t)=/producttextt−1
k=0α+k
α+β+k,α∈[0,∞),β∈[0,∞), the following property
holds:
∞/summationdisplay
t=0Γ(t)=/braceleftiggα+β−1
β−1ifβ >1
∞ otherwise(27)
Thus, Beta-weighted discounting is summable iff β >1.
Proof.First, we analyze the convergence of Beta-weighted Γ(t)depending on α,β.
In particular, we consider the series:
S=∞/summationdisplay
t=0at=∞/summationdisplay
t=0/parenleftiggt−1/productdisplay
k=0α+k
α+β+k/parenrightigg
(28)
We then use the Raabe’s convergence test (Ali, 2008). Given a series (at)consider the series of terms
bt=t/parenleftig
at
at+1−1/parenrightig
and its limit L= limt→∞bt. There are three possibilities:
•ifL>1, the original series converges
•ifL<1, the original series diverges
18Under review as submission to TMLR
•ifL= 1, the test is inconclusive
In the case of Beta-weighted discounting, we have:
lim
t→∞bt=t
t−1/producttext
k=0α+k
α+β+k
t/producttext
k=0α+k
α+β+k−1

= lim
t→∞t
α+t
α+β+t−t
= lim
t→∞αt+βt+t2−αt−t2
α+t
= lim
t→∞βt
α+t
=β (29)
Thus, we show that Beta-weighted discounting is summable with β >1and nonsummable with β <1. For
β= 1, we can rewrite the effective discount factor as:
Γ(t)=t−1/productdisplay
k=0α+k
α+β+k
=t−1/productdisplay
k=0α+k
α+k+ 1
=α·(α+ 1)·...·(α+t−2)·(α+t−1)
(α+ 1)·(α+ 2)·...·(α+t−1)·(α+t)
=α
α+t
=1
1 +t/α(30)
The series/summationtext∞
t=01
1+t/αis a general harmonic series and therefore divergent, completing the proof of conver-
gence.
To obtain the exact value, we use the following Taylor expansion
1
1−x= 1 +x+x2+... (31)
By evaluating the expected value of this expression with the Beta distribution’s probability density function
w(x), we obtain the desired sum of all discount factors:
E(1
1−X) =/integraldisplay1
01
1−xf(x;α,β)dx
=/integraldisplay1
0w(x) +xw(x) +x2w(x) +...dx
=/integraldisplay1
0x0w(x)dx+/integraldisplay1
0x1w(x)dx+...
= Γ(0)+ Γ(1)+...=∞/summationdisplay
t=0Γ(t)(32)
19Under review as submission to TMLR
This expression can be expanded as follows:
E(1
1−X) =/integraldisplay1
01
1−xf(x;α,β)dx
=/integraldisplay1
0(1−x)−11
B(α,β)xα−1(1−x)β−1
=1
B(α,β)/integraldisplay1
0xα−1(1−x)(β−1)−1
=B(α,β−1)
B(α,β)
=Γ(α)Γ(β−1)
Γ(α+β−1)Γ(α+β)
Γ(α)Γ(β)
=Γ(α)Γ(β−1)
((((((Γ(α+β−1)(α+β−1)((((((Γ(α+β−1)
(β−1)Γ(α)Γ(β−1)
=α+β−1
β−1(33)
completing the proof.
B Beta-weighted Discounting Properties
In Table 1 we present the values of the properties described in Section 5 for a set of discounting methods.
For each of them, we list their normalized partial sums Γ10
0,Γ100
10,Γ1000
100,Γ10000
1000, the variance measure, the
effective time horizon, and the total sum of the first 1000 steps.
20Under review as submission to TMLR
Table 1: The values of different metrics for a chosen set of discounting method and their parameters.
Discounting
methodΓ10
0 Γ100
10 Γ1000
100 Γ10000
1000Variance
10000/summationtext
t=0Γ(t)2TeffTotal
1000/summationtext
t=0Γ(t)
No discounting 0.001 0.009 0.090 0.900 10000 6322 1000
Exponential
γ= 0.990.096 0.538 0.366 0.000 50.25 100 100
Exponential
γ= 0.9990.010 0.085 0.537 0.368 500.25 1000 632.3
Exponential
γ= 0.970.263 0.690 0.0480 0.000 16.92 33 33.3
Beta-weighted
µ= 0.99,η= 0.50.049 0.293 0.509 0.149 66.67 323 166.1
Beta-weighted
µ= 0.97,η= 0.50.135 0.476 0.334 0.055 22.23 110 61.7
Hyperbolic
µ= 0.990.021 0.130 0.370 0.479 98.53 1741 238.8
Hyperbolic
µ= 0.250.439 0.188 0.187 0.187 1.12 107 3.3
Fixed-horizon
Tmax= 1000.100 0.900 0.000 0.000 100 64 100
Fixed-horizon
Tmax= 1600.062 0.562 0.375 0.000 160 102 160
Truncated Exponential
γ= 0.99,Tmax= 1000.151 0.849 0.000 0.000 43.52 51 63.4
Truncated Exponential
γ= 0.99,Tmax= 5000.096 0.542 0.362 0.000 50.25 99 99.3
Truncated Beta-weighted
µ= 0.99,η= 0.5,Tmax= 1000.143 0.857 0.000 0.000 47.11 54 69.4
Truncated Hyperbolic
µ= 0.99,Tmax= 1000.138 0.862 0.000 0.000 50.13 55 69.4
Truncated Hyperbolic
µ= 0.99,Tmax= 5000.054 0.335 0.612 0.000 83.13 210 178.6
21Under review as submission to TMLR
2 4 6 8 10 12 14
Path index i012345Path returnempirical
hyperbolic, k=0.05
exponential, =0.9
exponential, =0.95
exponential, =0.975
exponential, =0.99
beta-weighted, =0.95, =0.5
Figure5: Pathworldenvironmentresultsunderdifferentdiscountingschemes. Hyperbolic(Fedusetal.,2019)
and exponential (various curves) discountings fail to approximate the empirical (dashed) value. Instead, the
proposed Beta-weighted discounting approximates it much better, despite its different functional form.
C Pathworld experiments
Table 2: Values of the Mean Square Error for different discounting methods on the Pathworld environment,
summed across the first 14 paths i∈1,14. Lower is better.
Discounting methoddiscountη MSEfactor†
Exponential 0.990 0 3.962
Exponential 0.950 0 0.446
Exponential 0.975 0 0.242
Hyperbolic ‡ 0.05 1 0.250
Beta-Weighted 0.95 0.5 0.032
†: factor is γfor exponential
kfor hyperbolic
µfor Beta-weighted
‡: Results obtained by our re-implementation
of Fedus et al. (2019)
We showcase the utility of our method on a simple toy environment called the Pathworld introduced by Fedus
et al. (2019). Our goal is to show that Beta-weighted discounting can accurately model the presence of
unknown risk in an environment, even without being designed to a priori match the functional form of the
risk distribution.
C.1 Setup
In Pathworld, the agent takes a single action and observes a single reward after some delay. The actions
(i.e. paths) are indexed by natural numbers. When taking the i-th path, the agent receives a reward r=i
afterd=i2steps. Each path may also be subject to some hazard. In each episode, a risk λis sampled from
the uniform distribution U([0,2k])for a given parameter k. Given the risk λ, at each timestep on the path,
the agent has a chance (1−e−λ)of dying, and thus not collecting any reward in that episode. The task of
the agent is to use experience gathered without risk, and use the discounting to accurately predict a path’s
value when evaluated with an unknown risk.
22Under review as submission to TMLR
C.2 Pathworld results
If the risk is sampled from the Dirac Delta distribution H=δ(λ−λ0), the optimal discounting method is
exponential discounting, i.e. Γt=γtwithγ=e−λ. As shown by Fedus et al. (2019), if the risk is sampled
from an exponential distribution H=1
kexp(−λ/k), the optimal discounting scheme is their hyperbolic
discounting Γt=1
1+kt.
As shown in Lemma 4, Beta-weighted discounting subsumes both exponential ( µ=γ,η→0) and hyperbolic
discounting ( α=1
k,η=1). Thus, it directly models scenarios whose optimal discounting is exponential or
hyperbolic.
A more interesting scenario is when the functional form is different, e.g. when the risk is sampled from
a uniform distribution H∼U([0,2λµ]). Figure 5 and Table 2 report the results. We observe that Beta-
weighted discounting (with µchosen to fit the mean of the true risk distribution, and ηchosen heuristically
to decrease the variance of the Beta distribution) successfully outperforms all baselines, indicating that
using Beta-weighted discounting enabled by the proposed UGAE allows better modelling of unknown risk
distributions in environments where the risk phenomenon makes discounting necessary.
23