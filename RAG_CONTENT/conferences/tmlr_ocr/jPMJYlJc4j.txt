Under review as submission to TMLR
On the Convergence Rates of Federated Q-Learning across
Heterogeneous Environments
Anonymous authors
Paper under double-blind review
Abstract
Large-scale multi-agent systems are often deployed across wide geographic areas, where agents
interact with heterogeneous environments. There is an emerging interest in understanding
the role of heterogeneity in the performance of the federated versions of classic reinforcement
learning algorithms. In this paper, we study synchronous federated Q-learning, which aims
to learn an optimal Q-function by having Kagents average their local Q-estimates per E
iterations. We observe an interesting phenomenon on the convergence speeds in terms of
KandE. Similar to the homogeneous environment settings, there is a linear speed-up
concerning Kin reducing the errors that arise from sampling randomness. Yet, in sharp
contrast to the homogeneous settings, E > 1leads to significant performance degradation.
Specifically, we provide a fine-grained characterization of the error evolution in the presence
of environmental heterogeneity, which decay to zero as the number of iterations Tincreases.
The slow convergence of having E > 1turns out to be fundamental rather than an artifact
of our analysis. We prove that, for a wide range of stepsizes, the ℓ∞norm of the error
cannot decay faster than Θ(E/T). In addition, our experiments demonstrate that the
convergence exhibits an interesting two-phase phenomenon. For any given stepsize, there is
a sharp phase-transition of the convergence: the error decays rapidly in the beginning yet
later bounces up and stabilizes. Provided that the phase-transition time can be estimated,
choosing different stepsizes for the two phases leads to faster overall convergence.
1 Introduction
Advancements in unmanned capabilities are rapidly transforming industries and national security by enabling
fast-paced and versatile operations across domains such as advanced manufacturing (Park et al., 2019),
autonomous driving (Kiran et al., 2021), and battlefields (Möhlenhof et al., 2021). Reinforcement learning
(RL) – a cornerstone for unmanned capabilities – is a powerful machine learning method that aims to enable
an agent to learn an optimal policy via interacting with its operating environment to solve sequential decision-
making problems (Bertsekas & Tsitsiklis, 1996; Bertsekas, 2019). However, the ever-increasing complexity of
the environment results in a high-dimensional state-action space, often imposing overwhelmingly high sample
collection requirements on individual agents. This limited-data challenge becomes a significant hurdle that
must be addressed to realize the potential of reinforcement learning.
In this paper, we study reinforcement learning within a federated learning framework (also known as
Federated Reinforcement Learning (Qi et al., 2021; Jin et al., 2022; Woo et al., 2023)), wherein multiple
agents independently collect samples and collaboratively train a common policy under the orchestration of a
parameter server without disclosing the local data trajectories. A simple illustration can be found in Fig.1.
When the environments of all agents are homogeneous, it has been shown that the federated version of classic
reinforcement learning algorithms can significantly alleviate the data collection burden on individual agents
(Woo et al., 2023; Khodadadian et al., 2022) – error bounds derived therein exhibit a linear speedup in terms
of number of agents.
Moreover, by tuning the synchronization period E(i.e., the number of iterations between agent synchroniza-
tion), the communication cost can be significantly reduced compared with E= 1yet without significant
1Under review as submission to TMLR
performance degradation. However, many large-scale multi-agent systems are often deployed across wide
geographic areas, resulting in agents interacting with heterogeneous environments. For instance, connected
and autonomous vehicles (CAVs) operating in various regions of a metropolitan area encounter diverse
conditions such as varying traffic patterns, road infrastructure, and local regulations. The clients’ federa-
tion must be managed in a way that ensures the learned policy is robust to environmental heterogeneity.
Figure 1: An illustration of a federated learning system.There is an emerging interest in mathematically un-
derstanding the role of heterogeneity in the perfor-
mance of the federated versions of classic reinforce-
ment learning algorithms (Jin et al., 2022; Woo et al.,
2023; Doan et al., 2019; Wang et al., 2023; Xie &
Song, 2023) such as Q-learning, policy gradient meth-
ods, and temporal difference (TD) methods. In this
paper, we study synchronous federated Q-learning in
the presence of environmental heterogeneity, which
aims to learn an optimal Q-function by averaging
local Q-estimates per E(whereE≥1) update itera-
tions on their local data. We leave the exploration of
asynchronous Q-learning for future work. Federated
Q-learning is a natural integration of FedAvg and Q-learning (Jin et al., 2022; Woo et al., 2023). The former
is the most widely adopted classic federated learning algorithm (Kairouz et al., 2021; McMahan et al., 2017),
and the latter is one of the most fundamental model-free reinforcement learning algorithms (Watkins &
Dayan, 1992). Despite intensive study, the tight sample complexity of Q-learning in the single-agent setting
was open until recently (Li et al., 2024). Similarly, the understanding of FedAvg is far from complete; a
detailed discussion can be found in Section 2.
Contributions. In this paper, we study synchronous federated Q-learning in the presence of environment
heterogeneity.
•We provide a fine-grained characterization of the error evolution, which decays to zero as the number
of iterations Tincreases. We observe an interesting phenomenon on the convergence speeds in
terms ofKandE. Similar to the homogeneous environment settings, there is a linear speed-up
concerning Kin reducing the errors that arise from sampling randomness. Yet, in sharp contrast to
the homogeneous settings, E > 1leads to significant performance degradation.
•We prove that the convergence slowing down for E > 1is fundamental. We show that the ℓ∞norm
of the error cannot decay faster than Θ(E/T). A practical implication of this impossibility result is
that, eventually, having multiple local updates (i.e., E > 1) ends up consuming more samples (i.e.,
E×more) than using E= 1.
•Our numerical results illustrate that when the environments are heterogeneous and E > 1, and there
exists a sharp phase-transition of the error convergence: The error decays rapidly in the beginning
yet later bounces up and stabilizes. In addition, provided that the phase-transition time can be
estimated, choosing different stepsizes for the two phases can lead to faster overall convergence.
2 Related Work
Federated Learning. Federated learning is a communication-efficient distributed machine learning approach
that enables training global models without sharing raw local data (McMahan et al., 2017; Kairouz et al.,
2021). Federated learning has been adopted in commercial applications that involve diverse edge devices
such as autonomous vehicles (Du et al., 2020; Chen et al., 2021; Zeng et al., 2022; Posner et al., 2021; Peng
et al., 2023), internet of things (Nguyen et al., 2019; Yu et al., 2020), industrial automation (Liu et al.,
2020), healthcare (Yan et al., 2021; Sheller et al., 2019), and natural language processing (Yang et al., 2018;
Ramaswamy et al., 2019). Multiple open-source frameworks and libraries are available such as FATE, Flower,
OpenMinded-PySyft, OpenFL, TensorFlow Federated, and NVIDIA Clara.
2Under review as submission to TMLR
FedAvg was proposed in the seminal work (McMahan et al., 2017), and has been one of the most widely
implemented federated learning algorithms. It also has inspired many follow-up algorithms such as FedProx
(Li et al., 2020b), FedNova (Wang et al., 2020), SCAFFOLD (Karimireddy et al., 2020), and adaptive
federated methods (Deng et al., 2020). Despite intensive efforts, the theoretical understanding of FedAvg is
far from complete. Most existing theoretical work on FedAvg overlooks the underlying data statistics at the
agents, which often leads to misalignment of the pessimistic theoretical predictions and empirical success (Su
et al., 2023; Pathak & Wainwright, 2020; Wang et al., 2022a;b). This theory and practice gap is studied in a
recent work (Su et al., 2023) in the context of solving general non-parametric regression problems. It shows
that the limiting points of the global model under FedAvg is one unbiased estimator of the underlying model
that generates the data.
Reinforcement Learning. There has been extensive research on the convergence guarantees of reinforcement
learning algorithms. A recent surge of work focuses on non-asymptotic convergence and the corresponding
sample complexity for the single-agent setup. Bhandari et al. (2018) analyses non-asymptotic TD learning
with linear function approximation (LFA) considering a variety of noise conditions, including noiseless,
independent noise and Markovian noise. The results were extended to TD( λ) and Q-learning. Li et al. (2020a)
investigates the sample complexity of asynchronous Q-learning with different families of learning rates. They
also provide an extension of using variance reduction methods inspired by the seminal SVRG algorithm. Li
et al. (2024) shows the sample complexity of Q-learning. Let Abe the set of actions. When |A|= 1, the
sample complexity of synchronous Q-learning is sharp and minimax optimal, however, when |A|≥ 2, it is
shown that synchronous Q- learning has a lower bound which is not minimax optimal.
Federated Reinforcement Learning. Woo et al. (2023) provides sample complexity guarantees for
both synchronous and asynchronous distributed Q-learning and reveals that given the same transition
probability (i.e., homogeneous environment) for all agents, they can speed up the convergence process linearly
by collaboratively learning the optimal Q-function. Doan et al. (2019) investigates distributed Temporal
Difference (TD) algorithm TD(0) with LFA under the setting of multi-agent MDP, where multiple agents act
in a shared environment and each agent has its own reward function. They provide a finite-time analysis of
this algorithm that with constant stepsize, the estimates of agents can converge to a neighborhood around
optimal solutions at the rate of O(1/T)and asymptotically converge to the optimal solutions at the rate of
O(1/√
T+ 1), whereTis the timestep. Khodadadian et al. (2022) studies on-policy federated TD learning,
off-policy federated TD learning, and federated Q-learning of homogeneous environment and reward with
Markovian noise. The sample complexity derived exhibits linear speedup with respect to the number of
agents. Heterogeneous environments are considered in Jin et al. (2022); Wang et al. (2023); Xie & Song
(2023); Zhang et al. (2023b). Jin et al. (2022) studies federated Q-learning and policy gradient methods under
the setting of different known transition probabilities for each agent. Yet, no state sampling is considered.
Wang et al. (2023) proposes FedTD(0) with LFA dealing with the environmental and reward heterogeneity of
MDPs. They rigorously prove that in a low-heterogeneity regime, there is a linear convergence speedup in
the number of agents. Xie & Song (2023) uses KL-divergence to penalize the deviation of local update from
the global policy, and they prove that under the setting of heterogeneous environments, the local update
is beneficial for global convergence using their method. Zhang et al. (2023a) proposes FedSARSA using
the classic on-policy RL algorithm SARSA with linear function approximation (LFA) under the setting of
heterogeneous environments and rewards. They theoretically prove that the algorithm can converge to the
near-optimal solution. Neither Xie & Song (2023) nor Zhang et al. (2023a) characterize sample complexity.
3 Preliminary on Q-Learning
Markov decision process. A Markov decision process (MDP) is defined by the tuple ⟨S,A,P,γ,R⟩, whereS
representsthesetofstates, Arepresentsthesetofactions, thetransitionprobability P:S×A→ [0,1]provides
the probability distribution over the next states given a current state sand action a, the reward function
R:S×A→ [0,1]assigns a reward value to each state-action pair, and the discount factor γ∈(0,1)models
the preference for immediate rewards over future rewards. It is worth noting that P={P(·|s,a)}s∈S,a∈Ais
a collection of|S|×|A| probability distributions over S, one for each state-action pair (s,a).
Policy, value function, Q-Function, and optimality. A policyπspecifies the action-selection strategy
and is defined by the mapping π:S→ ∆(A), whereπ(a|s)denotes the probability of choosing action a
3Under review as submission to TMLR
when in state s. For a given policy π, the value function Vπ:S→Rmeasures the expected total discounted
reward starting from state s:
Vπ(s) =Eat∼π(·|st),st+1∼P(·|st,at)/bracketleftigg/summationdisplay
tγtR(st,at)|s0=s/bracketrightigg
,∀s∈S.
The state-action value function, or Q-function Qπ:S×A→ R, evaluates the expected total discounted
reward from taking action ain statesand then following policy π:
Qπ(s,a) =R(s,a) +Eat∼π(·|st),st+1∼P(·|st,at)/bracketleftigg/summationdisplay
tγtR(st,at)|s0=s,a0=a/bracketrightigg
,∀(s,a)∈S×A.
An optimal policy π∗is one that maximizes the value function for every state, that is ∀s∈S,Vπ∗(s)≥Vπ(s)
for any other π̸=π∗. Such a policy ensures the highest possible cumulative reward. The optimal value
functionV∗(shorthand for Vπ∗) and the optimal Q-function Q∗(shorthand for Qπ∗) are defined under the
optimal policy π∗.
The Bellman optimality equation for the value function and state-value function are:
V∗(s) = max
a[R(s,a) +γ/summationdisplay
s′∈SP(s′|s,a)V∗(s′)]
Q∗(s,a) =R(s,a) +γ/summationdisplay
s′∈SP(s′|s,a) max
a′∈AQ∗(s′,a′).
Q-learning. Q-learning (Watkins & Dayan, 1992) is a model-free reinforcement learning algorithm that
aims to learn the value of actions of all states by updating Q-values through iterative exploration of the
environment, ultimately converging to the optimal state-action function. Based on the Bellman optimality
equation for the state-action function, the update rule for Q-Learning is formulated as:
Qt+1(s,a) = (1−λ)Qt(s,a) +λ[R(s,a) +γmax
a′∈AQt(s′,a′)],∀(s,a)∈S×A,
wheres′is sampled from the environment or the transition probability and λis the stepsize.
4 Federated Q-learning
The federated learning system consists of one parameter server (PS) and Kagents. The Kagents are deployed
in possibly heterogeneous yet independent environments. The Kagents are modeled as Markov Decision
Processes (MDPs) with Mk=⟨S,A,Pk,γ,R⟩fork= 1,···,K, wherePk={Pk(·|s,a)}s∈S,a∈Aare the
collection of probability distributions that can be heterogeneous across agents. In the synchronous setting,
each agent khas access to a generative model, and generates a new state sample for each (s,a)via
sk
t(s,a)∼Pk(·|s,a)
i.e.,P/braceleftbig
sk
t(s,a) =s′/bracerightbig
=Pk(s′|s,a)for alls′∈S, independently across state-action pairs (s,a). For each
(s,a), the global environment ¯P(·|s,a)(Jin et al., 2022) is defined as
¯P(s′|s,a) =1
KK/summationdisplay
k=1Pk(s′|s,a),∀s′. (1)
with the corresponding global MDP defined as Mg=⟨S,A,¯P,γ,R⟩. Define transition heterogeneity κas
sup
k,s,a/vextenddouble/vextenddouble¯P(·|s,a)−Pk(·|s,a)/vextenddouble/vextenddouble
∞:=κ. (2)
LetQ∗denote the optimal Q-function of the global MDP. By the Bellman optimality equation, we have for
all(s,a),
Q∗(s,a) =R(s,a) +γ/summationdisplay
s′∈S¯P(s′|s,a)V∗(s′), (3)
whereV∗(s) = maxa∈AQ∗(s,a)is the optimal value function.
4Under review as submission to TMLR
Algorithm 1 Synchronous Federated Q-Learning
Inputs: discount factor γ,E, total iteration T, step-
sizeλ, initial estimate Q0
1:fork∈[K]do
2:Qk
0=Q0
3:end for
4:fort= 0toT−1do
5: fork∈[K]and(s,a)∈S×A do
6:Qk
t+1
2(s,a) = (1−λ)Qk
t(s,a) +
λ/parenleftbig
R(s,a) +γmaxa′∈AQk
t(st(s,a),a′)/parenrightbig
.
7: if(t+ 1) modE= 0then
8: Qk
t+1=1
K/summationtextK
k=1Qk
t+1
2
9: else
10: Qk
t+1=Qk
t+1
2
11: end if
12: end for
13:end for
14:returnQT=1
K/summationtextK
k=1Qk
TThe goal of the federated Q-learning is to have the
Kagents collaboratively learn Q∗. We consider syn-
chronous federated Q-learning, which is a natural
integration of FedAvg and Q-learning (Woo et al.,
2023; Jin et al., 2022) – described in Algorithm 1.
Every agent initializes its local Qkestimate as Q0
andperformsstandardsynchronousQ-learningbased
on the locally collected samples sk
t(s,a). Whenever
t+ 1 modE= 0, through the parameter server,
theKagents average their local estimate of Q; that
is, all agents report their Qk
t+1
2to the parameter
server, which computes the average and sends back
to agents.
5 Main Results
With a little abuse of notation, let the matrix
Pk∈R|S||A|×|S|represent the transition kernel of
the MDP of agent kwith the (s,a)-th row being
Pk(·|s,a)∈R|S|– the transition probability of the
state-action pair (s,a). For ease of exposition, we
writePk(·|s,a) =Pk(s,a)as the state transition probability at the state-action pair (s,a)when its meaning
is clear from the context.
5.1 Main Convergence Results.
Let/tildewidePk
t∈{0,1}|S||A|×|S|denote the local empirical transition matrix at the t-th iteration, defined as
/tildewidePk
t(s′|s,a) =1{s′=sk
t(s,a)}.
Denoting/tildewidePk
iV∗∈R|S||A|× 1with the (s,a)-th entry as/tildewidePk
i(s,a)V∗=/summationtext
s′∈S/tildewidePk
i(s′|s,a)V∗(s′). Let ¯Qt+1:=
1
K/summationtextK
k=1Qk
t+1. From lines 6, 8, and 10 of Algorithm 1, it follows that
¯Qt+1=1
KK/summationdisplay
k=1/parenleftig
(1−λt)Qk
t+λt(R+γ/tildewidePk
tVk
t)/parenrightig
,
whereVk
t(s) := maxa∈AQk
t(s,a)for alls∈S. Define
∆t+1:=Q∗−¯Qt+1,and ∆0:=Q∗−Q0. (4)
The error iteration ∆tis captured in the following lemma.
Lemma 1 (Error iteration) .For anyt≥0,
∆t+1= (1−λ)t+1∆0+γλt/summationdisplay
i=0(1−λ)t−i1
KK/summationdisplay
k=1(¯P−/tildewidePk
i)V∗
+γλt/summationdisplay
i=0(1−λ)t−i1
KK/summationdisplay
k=1/tildewidePk
i(V∗−Vk
i). (5)
To show the convergence of ∥∆t+1∥∞, we bound each of the three terms in the right-hand-side of equation 5.
The following lemma is a coarse upper bound of errors.
5Under review as submission to TMLR
Lemma 2. ChoosingR(s,a)∈[0,1]for each state-action pair (s,a), and choose 0≤Q0(s,a)≤1
1−γfor any
(s,a)∈S×A, then 0≤Qk
t(s,a)≤1
1−γ,0≤Q∗(s,a)≤1
1−γ,
/vextenddouble/vextenddoubleQ∗−Qk
t/vextenddouble/vextenddouble
∞≤1
1−γ,and/vextenddouble/vextenddoubleV∗−Vk
t/vextenddouble/vextenddouble
∞≤1
1−γ,∀t≥0,andk∈[K]. (6)
With the choice of Q0in Lemma 2, the first term in equation 5 can be bounded as/vextenddouble/vextenddouble(1−λ)t+1∆0/vextenddouble/vextenddouble
∞≤
(1−λ)t+1 1
1−γ. In addition, as detailed in the proof of Lemma 4 and Theorem 1, the boundedness in Lemma
2 enables us to bound the second term in equation 5 via invoking the Hoeffding’s inequality. It remains to
bound the third term in equation 5, for which we follow the analysis roadmap of Woo et al. (2023) by a
two-step procedure that is described in Lemma 3 and Lemma 4. Let
∆k
t=Q∗−Qk
t,andχ(t) =t−(tmodE), (7)
i.e.,∆k
tis the local error of agent k, andχ(t)is the most recent synchronization iteration of t.
Lemma 3. IftmodE= 0, then maxs,a/vextenddouble/vextenddouble/vextenddouble1
K/summationtextK
k=1/tildewidePk
t(V∗−Vt)/vextenddouble/vextenddouble/vextenddouble
∞≤∥∆t∥∞. Otherwise,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
t(V∗−Vk
t)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤/vextenddouble/vextenddouble∆χ(t)/vextenddouble/vextenddouble
∞+ 2λ1
KK/summationdisplay
k=1t−1/summationdisplay
t′=χ(t)/vextenddouble/vextenddouble∆k
t′/vextenddouble/vextenddouble
∞
+γλ1
KK/summationdisplay
k=1max
s,a/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglet−1/summationdisplay
t′=χ(t)/parenleftig
/tildewidePk
t′(s,a)−¯P(s,a)/parenrightig
V∗/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
where we use the convention that/summationtextχ(t)−1
t′=χ(t)/vextenddouble/vextenddouble∆k
t′/vextenddouble/vextenddouble
∞= 0.
Lemma 4. Chooseλ≤1
E. For anyδ∈(0,1), with probability at least (1−δ),
/vextenddouble/vextenddouble∆k
i/vextenddouble/vextenddouble
∞≤/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞+3γ
1−γλ(E−1)κ+3γ
1−γ/radicalbigg
λlog|S||A|KT
δ,∀i≤T,k∈[K]. (8)
Both Lemma 3 and Lemma 4 are non-trivial adaptations of the characterization in the analysis of Woo et al.
(2023) due to lack of common optimal action for any given state when environments are heterogeneous.
To bound the ℓ∞norm of the third term in equation 5, we first invoke Lemma 3, followed by Lemma 4. It is
worth noting that directly applying Lemma 4 can also lead to a valid error bound yet the resulting bound
will not decay as Tincreases with proper choice of stepsizes.
Theorem 1 (Convergence) .ChooseE−1≤1−γ
4γλandλ≤1
E. For anyδ∈(0,1
3), with probability at least
1−3δ, it holds that
∥∆T∥∞≤4
(1−γ)2exp/braceleftbigg
−1
2/radicalbig
(1−γ)λT/bracerightbigg
+2γ2
(1−γ)2(6λ2(E−1)2+λ(E−1))κ
+/parenleftigg
12γ2λ
(1−γ)2√
E−1 +2γ2√
λ
(1−γ)2/parenrightigg/radicalbigg
λ(E−1) log|S||A|KT
δ
+2γ
(1−γ)2/radicalbigg
1
Kλlog|S||A|TK
δ.
The first term of Theorem 1 is the standard error bound in the absence of environmental heterogeneity and
sampling noises. The second term arises from environmental heterogeneity. It is clear that when E= 1, the
environmental heterogeneity does not negatively impact the convergence. The last two terms result from the
randomness in sampling.
6Under review as submission to TMLR
Remark 1 (Eventual zero error) .It is common to choose the stepsize λbased on the time horizon T. Let
λ=g(T)be a non-increasing function of T. As long as λ=g(T)decay inT, terms 2-4 in Theorem 1 will go
to 0 asTincreases. In addition, when λ=ω(1/T), the first term will decay to 0.
There is a tradeoff in the convergence rates of the first term and the remaining terms – the slower λdecay in
Tleads to faster decay in the first term but slower in the remaining terms. Forcing these terms to decay
around the same speed lead to slow overall convergence. Corollary 1 follows immediately from Theorem 1 via
carefully choosing λto balance the decay rates of different terms.
Corollary 1. Choose (E−1)≤min1
λ{γ
1−γ,1
K}, andλ=4 log2(TK)
T(1−γ). LetT≥E. For anyδ∈(0,1
3), with
probability at least 1−3δ,
∥∆T∥∞≤4
(1−γ)2TK+36
(1−γ)3log(TK)√
TK/radicalbigg
log|S||A|TK
δ+56 log2(TK)
(1−γ)3E−1
Tκ.
Remark 2 (Partial linear speedup and the negative impacts of E > 1).Intuitively, both terms 1 and 2 decay
as if there are TKiterations – a linear speedup. In fact, the decay rate of the sampling noises in Corollary
1, with respect to TK, is the minimax optimal up to polylog factors (Vershynin, 2018). The decay of the
third term is controlled by environmental heterogeneity when E > 1. In sharp contrast to the homogeneous
settings, larger Esignificantly slows down the convergence of this term.
We show in the next subsection that this slow convergence is fundamental.
5.2 On the fundamentals of Convergence Slowing Down for E > 1.
Theorem 2. LetQ0=0. For any even K≥2, there exist a collection of {(S,A,Pk,R,γ ) :k∈[K]}such
that, for any synchronization period Eand time-invariant stepsize λ≤1
1+γ,
∥∆T∥∞= Ω (E/T),
whenT/E∈NandT≳E
1−γlog1
1−γ.
Proof Sketch. Below we discuss the key ideas and provide the proof sketch of Theorem 2. The full proof is
deferred to Appendix F.
The eventual slow rate convergence is due to the heterogeneous environments Pkregardless of the cardinality
of the action space. In particular, we prove the slow rate when the action space is a singleton, in which
case the Q-function coincides with the V-function. The process is also known as the Markov reward process.
According to Algorithm 1, when (t+ 1)modE̸= 0, we have
Qk
t+1=/parenleftbig
(1−λ)I+λγPk/parenrightbig
Qk
t+λR.
Following Algorithm 1, we obtain the following recursion between two synchronization rounds:
∆(r+1)E=¯A(E)∆rE+/parenleftig/parenleftig
I−¯A(E)/parenrightig
−/parenleftig
I+¯A(1)+...¯A(E−1)/parenrightig/parenleftig
I−¯A(1)/parenrightig/parenrightig
Q∗, (9)
where ¯A(ℓ)≜1
K/summationtextK
k=1(Ak)ℓandAk≜(1−λ)I+λγPk. While the first term on the right-hand side
of equation 9 decays rapidly to zero, the second term is non-vanishing due to environment heterogeneity for
E≥2. Specifically, to ensure the rapid decay of the first term, it is necessary to select a stepsize λ=/tildewideΩ(1
rE).
However, this choice results in the dominating residual error from the second term, which increases linearly
withλE=/tildewideΩ(1
r).
Next, we instantiate the analyses by constructing the set Pkover a pair of states and an even number of
clients with
P2k−1=/bracketleftbigg1 0
0 1/bracketrightbigg
, P2k=/bracketleftbigg0 1
1 0/bracketrightbigg
,fork∈N. (10)
Applying the formula of ¯A(ℓ)yields the following eigen-decomposition:
¯A(ℓ)=αℓ(I−¯P) +βℓ¯P,
7Under review as submission to TMLR
where ¯P=1
211⊤,αℓ≜1
2(νℓ
1+νℓ
2),βℓ≜νℓ
2,ν1≜1−(1 +γ)λ, andν2≜1−(1−γ)λ. For this instance
ofPk, the error evolution equation 9 reduces to ∆(r+1)E=/parenleftbig
αE(I−¯P) +βE¯P/parenrightbig
∆rE+κE(I−¯P)Q∗with
κE≜−γ
2/parenleftig
1−νE
2
1−γ−1−νE
1
1+γ/parenrightig
, which further yields the following full error recursion:
∆rE=/parenleftbig
αr
E(I−¯P) +βr
E¯P/parenrightbig
∆0+1−αr
E
1−αEκE(I−¯P)Q∗.
Starting from Q0= 0, the error can be decomposed into
∆rE=βr
E¯PQ∗+/parenleftbigg
αr
E+1−αr
E
1−αEκE/parenrightbigg
(I−¯P)Q∗. (11)
The two terms of the error are orthogonal and both non-vanishing. Therefore, it remains to lower bound the
maximum magnitude of two coefficients irrespective of the stepsize λ. To this end, we analyze two regimes of
λseparated by a threshold λ0≜logr
(1−γ)rE:
•Slow rate due to small stepsize when λ≤λ0. Sinceβr
Edecreases as λincreases,
βr
E≥(1−(1−γ)λ0)rE=/parenleftbigg
1−logr
rE/parenrightbiggrE
≳1
r.
•Slow rate due to environment heterogeneity when λ≥λ0. We show that
/vextendsingle/vextendsingle/vextendsingle/vextendsingleκE
1−αE/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥γ2λ(E−1)
4≳γ2logr
(1−γ)r,/parenleftbigg
1 +/vextendsingle/vextendsingle/vextendsingle/vextendsingleκE
1−αE/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
αr
E≤1
(1−γ2)r.
We conclude that at least one component of the error in equation 11 must be slower than the rate Ω(1/r).
Remark 3.The explicit calculations are based on a set Pkover a pair of states. Nevertheless, the evolu-
tion equation 9 is generally applicable. Similar analyses can be extended to scenarios involving more than
two states, provided that the sequence of matrices ¯A(ℓ)is simultaneously diagonalizable. For instance, the
construction of the transition kernels in equation 10 can be readily extended to multiple states if the set S
can be partitioned into two different classes. The key insight is the non-vanishing residual on the right-hand
side of equation 9 when E≥2due to the environment heterogeneity.
6 Experiments
Description of the setup. In our experiments, we consider K= 5agents (Jin et al., 2022), each
interacting with an independently and randomly generated 5×5maze environment ⟨S,A,Pk,R,γ⟩for
k∈{1,2,···,5}. The state setScontains 25cells that the agent is currently in. The action set contains
4 actionsA={left,up,right,down}. Thus,|S|×|A| = 100. We choose γ= 0.99. For ease of verifying our
theory, each entry of the reward R∈R100is sampled from Bern (p= 0.05), which slightly departs from
a typical maze environment wherein only two state-action pairs have nonzero rewards. We choose this
reward so that∥∆0∥∞≈100 =1
1−γ, which is the coarse upper bound of ∥∆t∥∞for allt. For each agent k,
its state transition probability vectors Pkare constructed on top of standard state transition probability
vectors of maze environments incorporated with a drifting probability 0.1in each non-intentional action
as in WindyCliff (Jin et al., 2022; Paul et al., 2019). In this way, the environment heterogeneity lies not
only in the differences of the non-zero probability values (Jin et al., 2022; Paul et al., 2019) but also in the
probability supports (i.e., the locations of non-zero entries). Our construction is more challenging: The
environment heterogeneity κas per (2) of our environment construction was calculated to be 1.2. Yet, the
largest environment heterogeneity of the WindyCliff construction in Jin et al. (2022) is about 0.31.
We choose Q0=0∈R100. All numerical results are based on 5 independent runs to capture the variability.
The dark lines represent the mean of the runs, while the shaded areas around each line illustrate the range
obtained by adding and subtracting one standard deviation from the mean. The maximum time duration is
T= 20,000in our experiment since it is sufficient to capture the characteristics of the training process.
8Under review as submission to TMLR
Two-phase phenomenon. We plot the evolutions of ∥∆t∥∞for synchronous federated Q-learning under
heterogeneous and homogeneous environments, respectively. Our results show that the sharp two-phase
phenomenon mainly arises from environmental heterogeneity rather than sampling noise.
From Figure 2a, it is clear that under the heterogeneous setting, for a given set of constant stepsizes
λ∈{0.9,0.5,0.2,0.1,0.05}, theℓ∞-norm of ∆t=¯Qt−Q∗decreases to a minimum point and then bounces
back rapidly before stabilizing around some fixed errors. Moreover, we can see that different stepsizes give
different minimum error. The smaller the stepsize, the smaller the minimum error; however, it takes longer
to reach such minimum errors. In sharp contrast, as shown in Figure 2b, there is no drastic bounce when the
environments are homogeneous.
(a) Heterogeneous environments E= 10.
 (b) Homogeneous environments E= 10.
Figure 2: The ℓ∞error of different constant stepsizes under the heterogeneous and homogenous settings.
A useful practice implication of our results is that: While constant stepsizes are often used in reinforcement
learning problems because of the great performance in applications as described in Sutton & Barto (2018),
they suffer significant performance degradation in the presence of environmental heterogeneity.
Impacts of the synchronization period E.Furthermore, we test the impacts of the synchronization
periodE. As shown in Figure 3 and Figure 2a, with λ∈{0.9,0.5,0.2,0.1,0.05}, asEincreases, the final error
increases and saturates around 62 in the presence of environmental heterogeneity. For a homogeneous setting
(results deferred to Appendix G.1), Edoes not have a significant impact, which aligns with the observations
in the existing literature on the homogeneous settings (Woo et al., 2023; Khodadadian et al., 2022).
Potential utilization of the two-phase phenomenon.
As shown in Figures 2a and 3, in the presence of environmental heterogeneity, the smaller the stepsizes, the
smaller error∥∆t∥∞can reach and less significant of the error bouncing in the second phase.
In our preliminary experiments, we tested small stepsizes λ= 1/Tαforα∈{0.4,0.5,···,1}, which eventually
lead to small errors yet at the cost of being extremely slow. Among these choices, λ= 1/√
Thas the fastest
convergence performance yet is still ≈24up to iteration 20,000.
Lett0be the iteration at which the error trajectory ∥∆t∥∞switches from phase 1 to phase 2. Provided that
t0can be estimated, choosing different stepsizes for the two phases can lead to faster overall convergence,
compared with using the same stepsize throughout.
Figure 4 illustrates two-phase training with different phase 1 stepsizes and phase 2 stepsize λ= 1/√
T
compared with using λ= 1/√
Tthroughout. Overall, using λ= 1/√
Tthroughout leads to the slowest
convergence, highlighting the benefits of the two-phase training strategy. Among all two-phase stepsize
choices, the stepsize of 0.05 in the first phase results in a longer phase 1 duration ( t0= 5550) but the lowest
9Under review as submission to TMLR
(a) E=1
 (b) E=20
(c) E=40
 (d) E= ∞
Figure 3: Heterogeneous environments with varying E.
Figure 4: Choosing different stepsizes for phases 1 and 2 leads to faster overall convergence. E= 10.
10Under review as submission to TMLR
final error (2.75327), suggesting a better convergence. We further test the convergence performance with
respect to different target error levels, details can be found in Appendix G.2.
We leave the estimation and characterization of t0for future work.
11Under review as submission to TMLR
References
Dimitri Bertsekas. Reinforcement learning and optimal control , volume 1. Athena Scientific, 2019.
Dimitri Bertsekas and John N Tsitsiklis. Neuro-dynamic programming . Athena Scientific, 1996.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with
linear function approximation. In Conference on learning theory , pp. 1691–1692. PMLR, 2018.
Jin-Hua Chen, Min-Rong Chen, Guo-Qiang Zeng, and Jia-Si Weng. Bdfl: a byzantine-fault-tolerance decen-
tralized federated learning method for autonomous vehicle. IEEE Transactions on Vehicular Technology ,
70(9):8639–8652, 2021.
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated learning.
arXiv preprint arXiv:2003.13461 , 2020.
Thinh Doan, Siva Maguluri, and Justin Romberg. Finite-time analysis of distributed td (0) with linear
function approximation on multi-agent reinforcement learning. In International Conference on Machine
Learning , pp. 1626–1635. PMLR, 2019.
Zhaoyang Du, Celimuge Wu, Tsutomu Yoshinaga, Kok-Lim Alvin Yau, Yusheng Ji, and Jie Li. Federated
learning for vehicular internet of things: Recent advances and open issues. IEEE Open Journal of the
Computer Society , 1:45–61, 2020.
Hao Jin, Yang Peng, Wenhao Yang, Shusen Wang, and Zhihua Zhang. Federated reinforcement learning with
environment heterogeneity. In International Conference on Artificial Intelligence and Statistics , pp. 18–37.
PMLR, 2022.
Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Hubert
Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi,
Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson,
Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecný, Aleksandra Korolova,
Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard
Nock, Ayfer Özgür, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn
Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth
Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances
and open problems in federated learning. Foundations and Trends ®in Machine Learning , 14(1–2):1–210,
2021.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In Inter-
national Conference on Machine Learning , pp. 5132–5143. PMLR, 2020.
Sajad Khodadadian, Pranay Sharma, Gauri Joshi, and Siva Theja Maguluri. Federated reinforcement
learning: Linear speedup under markovian sampling. In International Conference on Machine Learning ,
pp. 10997–11057. PMLR, 2022.
B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani, and
Patrick Pérez. Deep reinforcement learning for autonomous driving: A survey. IEEE Transactions on
Intelligent Transportation Systems , 23(6):4909–4926, 2021.
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Sample complexity of asynchronous q-learning:
Sharper analysis and variance reduction. Advances in neural information processing systems , 33:7031–7043,
2020a.
Gen Li, Changxiao Cai, Yuxin Chen, Yuting Wei, and Yuejie Chi. Is q-learning minimax optimal? a tight
sample complexity analysis. Operations Research , 72(1):222–236, 2024.
12Under review as submission to TMLR
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. Proceedings of Machine Learning and Systems , 2:429–450, 2020b.
Yang Liu, Anbu Huang, Yun Luo, He Huang, Youzhi Liu, Yuanyuan Chen, Lican Feng, Tianjian Chen, Han
Yu, and Qiang Yang. Fedvision: An online visual object detection platform powered by federated learning.
InProceedings of the AAAI Conference on Artificial Intelligence , volume 34, pp. 13172–13179, 2020.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-
efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics , pp.
1273–1282. PMLR, 2017.
Thies Möhlenhof, Norman Jansen, and Wiam Rachid. Reinforcement learning environment for tactical
networks. In 2021 International Conference on Military Communication and Information Systems (ICMCIS) ,
pp. 1–8. IEEE, 2021.
Thien Duc Nguyen, Samuel Marchal, Markus Miettinen, Hossein Fereidooni, N Asokan, and Ahmad-Reza
Sadeghi. Dïot: A federated self-learning anomaly detection system for iot. In 2019 IEEE 39th International
conference on distributed computing systems (ICDCS) , pp. 756–767. IEEE, 2019.
In-Beom Park, Jaeseok Huh, Joongkyun Kim, and Jonghun Park. A reinforcement learning approach to
robust scheduling of semiconductor manufacturing facilities. IEEE Transactions on Automation Science
and Engineering , 17(3):1420–1431, 2019.
Reese Pathak and Martin J Wainwright. Fedsplit: An algorithmic framework for fast federated optimization.
Advances in neural information processing systems , 33:7057–7066, 2020.
Supratik Paul, Michael A Osborne, and Shimon Whiteson. Fingerprint policy optimisation for robust
reinforcement learning. In International Conference on Machine Learning , pp. 5082–5091. PMLR, 2019.
Muzi Peng, Jiangwei Wang, Dongjin Song, Fei Miao, and Lili Su. Privacy-preserving and uncertainty-aware
federated trajectory prediction for connected autonomous vehicles. In 2023 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS) , pp. 11141–11147, 2023. doi: 10.1109/IROS55552.
2023.10341638.
Jason Posner, Lewis Tseng, Moayad Aloqaily, and Yaser Jararweh. Federated learning in vehicular networks:
Opportunities and solutions. IEEE Network , 35(2):152–159, 2021.
Jiaju Qi, Qihao Zhou, Lei Lei, and Kan Zheng. Federated reinforcement learning: techniques, applications,
and open challenges. Intelligence &amp Robotics , 2021. doi: 10.20517/ir.2021.02. URL https://doi.org/
10.20517%2Fir.2021.02 .
Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Françoise Beaufays. Federated learning for emoji
prediction in a mobile keyboard. arXiv preprint arXiv:1906.04329 , 2019.
Micah J Sheller, G Anthony Reina, Brandon Edwards, Jason Martin, and Spyridon Bakas. Multi-institutional
deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation. In
Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop,
BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Revised
Selected Papers, Part I 4 , pp. 92–104. Springer, 2019.
Lili Su, Jiaming Xu, and Pengkun Yang. A non-parametric view of fedavg and fedprox: Beyond stationary
points.Journal of Machine Learning Research , 24(203):1–48, 2023.
Richard S. Sutton and Andrew G. Barto. Chapter 2.5 Tracking a Nonstationary Problem, Reinforcement
Learning: An Introduction , chapter 8, pp. 33. The MIT Press, 2018.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science , volume 47.
Cambridge university press, 2018.
13Under review as submission to TMLR
Chunnan Wang, Xiang Chen, Junzhe Wang, and Hongzhi Wang. Atpfl: Automatic trajectory prediction model
design under federated learning framework. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pp. 6563–6572, June 2022a.
Han Wang, Aritra Mitra, Hamed Hassani, George J Pappas, and James Anderson. Federated temporal
difference learning with linear function approximation under environmental heterogeneity. arXiv preprint
arXiv:2302.02212 , 2023.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency
problem in heterogeneous federated optimization. Advances in neural information processing systems , 33:
7611–7623, 2020.
Jianyu Wang, Rudrajit Das, Gauri Joshi, Satyen Kale, Zheng Xu, and Tong Zhang. On the unreasonable
effectiveness of federated averaging with heterogeneous data. arXiv preprint arXiv:2206.04723 , 2022b.
Christopher Watkins and Peter Dayan. Q-learning. Machine Learning , 8:279–292, 1992. URL https:
//api.semanticscholar.org/CorpusID:208910339 .
Jiin Woo, Gauri Joshi, and Yuejie Chi. The blessing of heterogeneity in federated q-learning: Linear speedup
and beyond. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,
and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning ,
volume 202 of Proceedings of Machine Learning Research , pp. 37157–37216. PMLR, 23–29 Jul 2023. URL
https://proceedings.mlr.press/v202/woo23a.html .
Zhijie Xie and Shenghui Song. FedKL: Tackling Data Heterogeneity in Federated Reinforce-
ment Learning by Penalizing KL Divergence. IEEE Journal on Selected Areas in Commu-
nications , 41(4):1227–1242, April 2023. ISSN 1558-0008. doi: 10.1109/JSAC.2023.3242734.
URL https://ieeexplore.ieee.org/abstract/document/10038492?casa_token=yGyMDlnL_FsAAAAA:
hqNvzWEb6yVKwTZVdHKLVnorDg07AWx4uujDsLLTTY_7unjr1ew8Yv4_UUAWfCx3X1b9wHNySP8 .
Bingjie Yan, Jun Wang, Jieren Cheng, Yize Zhou, Yixian Zhang, Yifan Yang, Li Liu, Haojiang Zhao,
Chunjuan Wang, and Boyi Liu. Experiments of federated learning for covid-19 chest x-ray images. In
Advances in Artificial Intelligence and Security: 7th International Conference, ICAIS 2021, Dublin, Ireland,
July 19-23, 2021, Proceedings, Part II 7 , pp. 41–53. Springer, 2021.
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, and
Françoise Beaufays. Applied federated learning: Improving google keyboard query suggestions. arXiv
preprint arXiv:1812.02903 , 2018.
Tianlong Yu, Tian Li, Yuqiong Sun, Susanta Nanda, Virginia Smith, Vyas Sekar, and Srinivasan Seshan.
Learning context-aware policies from multiple smart homes via federated multi-task learning. In 2020
IEEE/ACM Fifth International Conference on Internet-of-Things Design and Implementation (IoTDI) , pp.
104–115. IEEE, 2020.
Tengchan Zeng, Omid Semiari, Mingzhe Chen, Walid Saad, and Mehdi Bennis. Federated learning on the
road autonomous controller design for connected and autonomous vehicles. IEEE Transactions on Wireless
Communications , 21(12):10407–10423, 2022.
Chenyu Zhang, Han Wang, Aritra Mitra, and James Anderson. Finite-time analysis of on-policy heterogeneous
federated reinforcement learning. In The Twelfth International Conference on Learning Representations ,
2023a.
Shangtong Zhang, Remi Tachet Des Combes, and Romain Laroche. On the convergence of sarsa with linear
function approximation. In International Conference on Machine Learning , pp. 41613–41646. PMLR,
2023b.
14Under review as submission to TMLR
Appendices
A Proof of Lemma 1
The update of ∆t+1is as follows:
∆t+1=Q∗−¯Qt+1
=1
KK/summationdisplay
k=1(Q∗−((1−λ)Qk
t+λ(R+γ/tildewidePk
tQk
t)))
=1
KK/summationdisplay
k=1((1−λ)(Q∗−Qk
t) +λ(Q∗−R−γ/tildewidePk
tVk
t))
= (1−λ)∆t+γλ1
KK/summationdisplay
k=1(¯PV∗−/tildewidePk
tVk
t)
= (1−λ)∆t+γλ
KK/summationdisplay
k=1(¯P−/tildewidePk
t)V∗+γλ
KK/summationdisplay
k=1/tildewidePk
t(V∗−Vk
t)
= (1−λ)t+1∆0+γλt/summationdisplay
i=0(1−λ)t−i1
KK/summationdisplay
k=1(¯P−/tildewidePk
i)V∗
+γλt/summationdisplay
i=0(1−λ)t−i1
KK/summationdisplay
k=1/tildewidePk
i(V∗−Vk
i),
recalling that ∆0=Q∗−Q0.
B Proof of Lemma 2
We first show 0≤Qk
t(s,a)≤1
1−γby inducting on t. Whent= 0, this is true by the choice of Q0. Suppose
that 0≤Qk
t−1(s,a)≤1
1−γfor any state-action pair (s,a)and any client k. Let’s focus on time t. Whentis
not a synchronization iteration (i.e., t+ 1 mod̸= 0), we have
Qk
t(s,a) = (1−λ)Qk
t−1(s,a) +λ(R(s,a) +γ/tildewidePk
tVk
t−1(s))
≤1−λ
1−γ+λ(R(s,a) +γ/tildewidePk
tVk
t−1(s))
(a)
≤1−λ
1−γ+λ(1 +γ
1−γ)
≤1
1−γ−λ
1−γ+λ
1−γ
=1
1−γ,
where inequality (a) holds because for any s,Vk
t−1(s) =maxa∈AQk
t−1(s,a)≤1
1−γby the inductive hypothesis,
and/tildewidePk
t∥1= 1. Similarly, we can show the case when tis a synchronization iteration.
With the above argument, we can also show that 0≤Q∗(s,a)≤1
1−γfor any state-action pair (s,a). Therefore,
we have that/vextenddouble/vextenddoubleQ∗−Qk
t/vextenddouble/vextenddouble
∞≤1
1−γ.
15Under review as submission to TMLR
Next, we show that bound on/vextenddouble/vextenddoubleV∗−Vk
t/vextenddouble/vextenddouble
∞.
/vextenddouble/vextenddoubleV∗−Vk
t/vextenddouble/vextenddouble
∞= max
s∈S/vextendsingle/vextendsingleV∗(s)−Vk
t(s)/vextendsingle/vextendsingle
= max
s∈S/vextendsingle/vextendsingle/vextendsingle/vextendsinglemax
a∈AQ∗(s,a)−max
a′∈AQk
t(s,a′)/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤max
s∈S,a∈A/vextendsingle/vextendsingleQ∗(s,a)−Qk
t(s,a)/vextendsingle/vextendsingle
=/vextenddouble/vextenddoubleQ∗−Qk
t/vextenddouble/vextenddouble
∞
≤1
1−γ.
C Proof of Lemma 3
WhentmodE= 0, i.e.,iis a synchronization round, Qk
t=Qk′
tfor any pair of agents k,k′∈[K]. Hence,
1
KK/summationdisplay
k=1/tildewidePk
t(s,a)(V∗−Vt) =/parenleftigg
1
KK/summationdisplay
k=1/tildewidePk
t(s,a)/parenrightigg
(V∗−Vt)
≤∥1
KK/summationdisplay
k=1/tildewidePk
t(s,a)∥1∥V∗−Vt∥∞
≤∥Q∗−Qt∥∞
=∥∆t∥∞. (12)
For general t, we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
t(V∗−Vk
t)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
t(V∗−Vk
χ(t)+Vk
χ(t)−Vk
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞
≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
t(V∗−Vk
χ(t))/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
t(Vk
χ(t)−Vk
t)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞
≤/vextenddouble/vextenddouble∆χ(t)/vextenddouble/vextenddouble
∞+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
t(Vk
χ(t)−Vk
t)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞by equation 12
≤/vextenddouble/vextenddouble∆χ(t)/vextenddouble/vextenddouble
∞+1
KK/summationdisplay
k=1/vextenddouble/vextenddouble/vextenddoubleVk
χ(t)−Vk
t/vextenddouble/vextenddouble/vextenddouble
∞. (13)
For any state s∈S, we have
Vk
t(s)−Vk
χ(t)(s)
=Qk
t(s,ak
t(s))−Qk
χ(t)(s,ak
χ(t)(s))
(a)
≤Qk
t(s,ak
t(s))−Qk
χ(t)(s,ak
t(s))
=Qk
t(s,ak
t(s))−Qk
t−1(s,ak
t(s)) +Qk
t−1(s,ak
t(s))−Qk
t−2(s,ak
t(s))
+···+Qk
χ(t)+1(s,ak
t(s))−Qk
χ(t)(s,ak
t(s)). (14)
where inequality (a) holds because Qk
χ(t)(s,ak
t(s))≤Qk
χ(t)(s,ak
χ(t)(s)).
16Under review as submission to TMLR
For eacht′such thatχ(t)≤t′≤t, it holds that,
Qk
t′+1(s,ak
t(s))−Qk
t′(s,ak
t(s))
= (1−λ)Qk
t′(s,ak
t(s)) +λ(R(s,ak
t(s)) +γ/tildewidePk
t′(s,ak
t(s))Vk
t′)−Qk
t′(s,ak
t(s))
(a)=−λQk
t′(s,ak
t(s)) +λ/parenleftig
Q∗(s,ak
t(s))−R(s,ak
t(s))−γ¯P(s,ak
t(s))V∗+R(s,ak
t(s)) +γ/tildewidePk
t′(s,ak
t(s))Vk
t′/parenrightig
=λ∆k
t′(s,ak
t(s)) +γλ/parenleftig
(/tildewidePk
t′(s,ak
t(s))−¯P(s,ak
t(s)))V∗+/tildewidePk
t′(s,ak
t(s))(Vk
t′−V∗)/parenrightig
≤2λ/vextenddouble/vextenddouble∆k
t′/vextenddouble/vextenddouble
∞+γλ/parenleftig
/tildewidePk
t′(s,ak
t(s))−¯P(s,ak
t(s))/parenrightig
V∗,
where equality (a) follows from the Bellman equation equation 3. Thus,
Vk
t(s)−Vk
χ(t)(s)≤t−1/summationdisplay
t′=χ(t)Qk
t′+1(s,ak
t(s))−Qk
t′(s,ak
t(s))
= 2λt−1/summationdisplay
t′=χ(t)/vextenddouble/vextenddouble∆k
t′/vextenddouble/vextenddouble
∞+γλt−1/summationdisplay
t′=χ(t)/parenleftig
/tildewidePk
t′(s,ak
t(s))−¯P(s,ak
t(s))/parenrightig
V∗. (15)
Similarly, we have
Vk
t(s)−Vk
χ(t)(s)≥t−1/summationdisplay
t′=χ(t)Qk
t′+1(s,ak
χ(t)(s))−Qk
t′(s,ak
χ(t)(s))
≥−2λt−1/summationdisplay
t′=χ(t)/vextenddouble/vextenddouble∆k
t′/vextenddouble/vextenddouble
∞+γλt−1/summationdisplay
t′=χ(t)/parenleftig
/tildewidePk
t′(s,ak
χ(t)(s))−¯P(s,ak
χ(t)(s))/parenrightig
V∗.(16)
Plugging the bounds in equation 15 and in equation 16 back into equation 13, we get
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
t(V∗−Vk
t)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤/vextenddouble/vextenddouble∆χ(t)/vextenddouble/vextenddouble
∞+1
KK/summationdisplay
k=1/vextenddouble/vextenddouble/vextenddoubleVk
χ(t)−Vk
t/vextenddouble/vextenddouble/vextenddouble
∞
≤/vextenddouble/vextenddouble∆χ(t)/vextenddouble/vextenddouble
∞+ 2λ1
KK/summationdisplay
k=1t−1/summationdisplay
t′=χ(t)/vextenddouble/vextenddouble∆k
t′/vextenddouble/vextenddouble
∞
+γλ1
KK/summationdisplay
k=1max
s,a/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublet−1/summationdisplay
t′=χ(t)/parenleftig
/tildewidePk
t′(s,a)−¯P(s,a)/parenrightig
V∗/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞.
D Proof of Lemma 4
WhenimodE= 0, then ∆k
i= ∆χ(i). WhenimodE̸= 0, we have
Qk
i= (1−λ)Qk
i−1+λ/parenleftig
R+γ/tildewidePk
i−1Vk
i−1/parenrightig
= (1−λ)Qk
i−1+λ/parenleftig
Q∗−R−γ¯PV∗+R+γ/tildewidePk
i−1Vk
i−1/parenrightig
.
17Under review as submission to TMLR
So,
∆k
i= (1−λ)∆k
i−1+λγ/parenleftig
¯PV∗−/tildewidePk
i−1Vk
i−1/parenrightig
= (1−λ)∆k
i−1+λγ(¯P−/tildewidePk
i−1)V∗+λγ/tildewidePk
i−1(V∗−Vk
i−1)
≤(1−λ)i−χ(i)∆χ(i)+γλi−1/summationdisplay
j=χ(i)(1−λ)i−j−1(¯P−/tildewidePk
j)V∗
+γλi−1/summationdisplay
j=χ(i)(1−λ)i−j−1/tildewidePk
j(V∗−Vk
j). (17)
For any state-action pair (s,a),
|(1−λ)i−χ(i)∆χ(i)(s,a)|≤(1−λ)i−χ(i)/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞. (18)
By invoking Hoeffding’s inequality, for any given δ∈δ∈(0,1), with probability at least 1−δ, it holds that
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleγλi−1/summationdisplay
j=χ(i)(1−λ)i−j−1(¯P−/tildewidePk
j)V∗/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤γ
1−γλi−1/summationdisplay
j=χ(i)(1−λ)i−1−jκ+γ
1−γ/radicalbigg
λlog|S||A|KT
δ
≤γ
1−γλ(E−1)κ+γ
1−γ/radicalbigg
λlog|S||A|KT
δ, (19)
for all (s,a)∈S×A,i∈[T],k∈[K]. In addition, we have
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleγλi−1/summationdisplay
j=χ(i)(1−λ)i−j−1/tildewidePk
j(V∗−Vk
j)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤γλi−1/summationdisplay
j=χ(i)(1−λ)i−j−1/vextenddouble/vextenddouble∆k
j/vextenddouble/vextenddouble
∞. (20)
Combining the bounds in equation 18, equation 19, and equation 20, we get
/vextenddouble/vextenddouble∆k
i/vextenddouble/vextenddouble
∞≤(1−λ)i−χ(i)/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞+γ
1−γλ(E−1)κ+γ
1−γ/radicalbigg
λlog|S||A|KT
δ
+γλi−1/summationdisplay
j=χ(i)(1−λ)i−j−1/vextenddouble/vextenddouble∆k
j/vextenddouble/vextenddouble
∞
≤(1−(1−γ)λ)i−χ(i)/vextenddouble/vextenddouble∆χ(t)/vextenddouble/vextenddouble
∞
+ (1 +γλ)i−χ(i)/parenleftigg
γ
1−γλ(E−1)κ+γ
1−γ/radicalbigg
λlog|S||A|KT
δ/parenrightigg
, (21)
where the last inequality can be shown via inducting on i−χ(i)∈{0,···,E−1}. Whenλ≤1
E,
(1 +γλ)i−χ(i)≤(1 +λ)E≤(1 + 1/E)E≤e≤3.
We get
/vextenddouble/vextenddouble∆k
i/vextenddouble/vextenddouble
∞≤/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞+ 3γ
1−γλ(E−1)κ+ 3γ
1−γ/radicalbigg
λlog|S||A|KT
δ.
E Proof of Theorem 1
By Lemma 1,
∆t+1= (1−λ)t+1∆0+t/summationdisplay
i=0(1−λ)iγλ
KK/summationdisplay
k=1(¯P−/tildewidePk
t−i)V∗+t/summationdisplay
i=0(1−λ)iγλ
KK/summationdisplay
k=1/tildewidePk
t−i(V∗−Vk
t−i).
18Under review as submission to TMLR
Taking the ℓ∞norm on both sides, we get
∥∆t+1∥∞≤(1−λ)t+1∥∆0∥∞+/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublet/summationdisplay
i=0(1−λ)iλγ1
KK/summationdisplay
k=1(¯P−/tildewidePk
t−i)V∗/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞
+t/summationdisplay
i=0(1−λ)iλγ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
t−i(V∗−Vk
t−i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞.
We bound the three terms in the right-hand-side of the above-displayed equation separately.
Since 0≤Q0(s,a)≤1
1−γ, the first term can be bounded as
(1−λ)t+1∥∆0∥∞≤(1−λ)t+11
1−γ. (22)
To bound the second term/vextenddouble/vextenddouble/vextenddouble/summationtextt
i=0(1−λ)iλγ1
K/summationtextK
k=1(¯P−/tildewidePk
t−i)V∗/vextenddouble/vextenddouble/vextenddouble
∞, we have
t/summationdisplay
i=0(1−λ)iλγ1
KK/summationdisplay
k=1(¯P−/tildewidePk
t−i)V∗=t/summationdisplay
i=0(1−λ)iλγ1
KK/summationdisplay
k=1(Pk−/tildewidePk
t−i)V∗
=1
KK/summationdisplay
k=1t/summationdisplay
i=0(1−λ)iλγ(Pk−/tildewidePk
t−i)V∗.
LetXi,k(s,a)) =1
Kγλ(1−λ)i(Pk−/tildewidePk
t−i)V∗. It is easy to see that E[Xi,k(s,a)]= 0for all (s,a). By Lemma
2, we have|Xi,k|≤2
K(1−γ)γλ(1−λ)ifor all (s,a). Since the sampling across clients and across iterations are
independent, via invoking Hoeffding’s inequality, for any given δ∈(0,1), with probability at least 1−δ,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublet/summationdisplay
i=0(1−λ)iλγ1
KK/summationdisplay
k=1(¯P−/tildewidePk
t−i)V∗/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞≤γ
1−γ/radicalbigg
1
Kλlog|S||A|TK
δ. (23)
To bound the third term/summationtextt
i=0(1−λ)iλγ/vextenddouble/vextenddouble/vextenddouble1
K/summationtextK
k=1/tildewidePk
t−i(V∗−Vk
t−i)/vextenddouble/vextenddouble/vextenddouble
∞, following the roadmap of Woo et al.
(2023), we divide the summation into two parts as follows. For any βE≤t≤T, we have
t/summationdisplay
i=0(1−λ)iλγ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
t−i(V∗−Vk
t−i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞
=t/summationdisplay
i=0(1−λ)t−iλγ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
i(V∗−Vk
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞
=χ(t)−βE/summationdisplay
i=0(1−λ)t−iλγ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
i(V∗−Vk
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞+t/summationdisplay
i=χ(t)−βE+1(1−λ)t−iλγ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
i(V∗−Vk
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞
≤γ
1−γ(1−λ)t−χ(t)+βE+t/summationdisplay
i=χ(t)−βE+1(1−λ)t−iλγ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
i(V∗−Vk
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞.
19Under review as submission to TMLR
By Lemma 3,
t/summationdisplay
i=χ(t)−βE+1(1−λ)t−iλγ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
i(V∗−Vk
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞
≤t/summationdisplay
i=χ(t)−βE+1(1−λ)t−iλγ
/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞+ 2λ1
KK/summationdisplay
k=1i−1/summationdisplay
j=χ(i)/vextenddouble/vextenddouble∆k
t′/vextenddouble/vextenddouble
∞
+γλ1
KK/summationdisplay
k=1max
s,a/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglei−1/summationdisplay
j=χ(i)/parenleftig
/tildewidePk
j(s,a)−¯P(s,a)/parenrightig
V∗/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
.
Since/tildewidePk
j(s,a)’s are independent across time jand across state action pair (s,a), and|/tildewidePk
j(s,a)−¯P(s,a)V∗|≤
1
1−γ(from Lemma 2), with Hoeffding’s inequality and union bound, we get for any δ∈(0,1), with probability
at least 1−δ,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglei−1/summationdisplay
j=χ(i)/parenleftig
/tildewidePk
j(s,a)−¯P(s,a)/parenrightig
V∗/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤(E−1)1
1−γκ+1
1−γ/radicalbigg
(E−1) log|S|A|KT
δ(24)
for all (s,a)∈S×A,k∈K, andi. By Lemma 4, with probability at least (1−δ), we have
t/summationdisplay
i=χ(t)−βE+1(1−λ)t−iλγ2λ1
KK/summationdisplay
k=1i−1/summationdisplay
j=χ(i)/vextenddouble/vextenddouble∆k
j/vextenddouble/vextenddouble
∞
≤2λ2γt/summationdisplay
i=χ(t)−βE+1(1−λ)t−i1
KK/summationdisplay
k=1i−1/summationdisplay
j=χ(i)/parenleftigg
/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞+ 3γ
1−γλ(E−1)κ+ 3γ
1−γ/radicalbigg
λlog|S||A|KT
δ/parenrightigg
≤2λγ(E−1) max
χ(t)−βE≤i≤t/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞+6γ2λ2
1−γ(E−1)2κ+6γ2λ
1−γ(E−1)/radicalbigg
λlog|S||A|KT
δ.
Thus, with probability at least (1−2δ),
t/summationdisplay
i=χ(t)−βE+1(1−λ)t−iλγ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
i(V∗−Vk
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞
≤γ max
χ(t)−βE≤i≤t/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞+ 2λγ(E−1) max
χ(t)−βE≤i≤t/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞+6γ2λ2
1−γ(E−1)2κ
+6γ2λ
1−γ(E−1)/radicalbigg
λlog|S||A|KT
δ
+t/summationdisplay
i=χ(t)−βE+1(1−λ)t−iλγ/parenleftigg
γλ
1−γ(E−1)κ+γλ
1−γ/radicalbigg
(E−1) log|S||A|KT
δ/parenrightigg
=γ(1 + 2λ(E−1)) max
χ(t)−βE≤i≤t/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞+γ2
1−γ(6λ2(E−1)2+λ(E−1))κ
+γ2λ
1−γ/radicalbigg
(E−1) log|S||A|KT
δ+6γ2λ
1−γ(E−1)/radicalbigg
λlog|S||A|KT
δ.
20Under review as submission to TMLR
The third term can be bounded as
t/summationdisplay
i=0(1−λ)iλγ/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
KK/summationdisplay
k=1/tildewidePk
i(V∗−Vk
i)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞
≤γ
1−γ(1−λ)t−χ(t)+βE+γ(1 + 2λ(E−1)) max
χ(t)−βE≤i≤t/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞+γ2
1−γ(6λ2(E−1)2+λ(E−1))κ
+γ2λ
1−γ/radicalbigg
(E−1) log|S||A|KT
δ+6γ2λ
1−γ(E−1)/radicalbigg
λlog|S||A|KT
δ. (25)
Combing the bounds for terms 1, 2, and 3, we get the following recursion holds for all rounds Twith
probability at least (1−3δ):
∥∆t+1∥∞≤(1−λ)t+11
1−γ+γ
1−γ/radicalbigg
1
Kλlog|S||A|TK
δ+γ
1−γ(1−λ)t−χ(t)+βE
+γ(1 + 2λ(E−1)) max
χ(t)−βE≤i≤t/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞+γ2
1−γ(6λ2(E−1)2+λ(E−1))κ
+γ2λ
1−γ/radicalbigg
(E−1) log|S||A|KT
δ+6γ2λ
1−γ(E−1)/radicalbigg
λlog|S||A|KT
δ
≤γ(1 + 2λ(E−1)) max
χ(t)−βE≤i≤t/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞+2
1−γ(1−λ)βE+γ2
1−γ(6λ2(E−1)2+λ(E−1))κ
+γ2λ
1−γ/radicalbigg
(E−1) log|S||A|KT
δ+6γ2λ
1−γ(E−1)/radicalbigg
λlog|S||A|KT
δ
+γ
1−γ/radicalbigg
1
Kλlog|S||A|TK
δ.
Let
ρ:=2
1−γ(1−λ)βE+γ2
1−γ(6λ2(E−1)2+λ(E−1))κ
+γ2λ
1−γ/radicalbigg
(E−1) log|S||A|KT
δ+6γ2λ
1−γ(E−1)/radicalbigg
λlog|S||A|KT
δ
+γ
1−γ/radicalbigg
1
Kλlog|S||A|TK
δ. (26)
With the assumption that λ≤1−γ
4γ(E−1), the above recursion can be written as
∥∆t+1∥∞≤1 +γ
2max
χ(t)−βE≤i≤t/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞+ρ.
Unrolling the above recursion Ltimes where LβE≤t≤T, we obtain that
∥∆t+1∥∞≤(1 +γ
2)Lmax
χ(t)−LβE≤i≤t/vextenddouble/vextenddouble∆χ(i)/vextenddouble/vextenddouble
∞+L−1/summationdisplay
i=0(1 +γ
2)iρ
≤(1 +γ
2)L1
1−γ+2
1−γρ.
21Under review as submission to TMLR
Choosingβ=1
E/radicalig
(1−γ)T
2λ,L=/radicalig
λT
1−γ,t+ 1 =T, we get
∥∆T∥∞≤1
1−γ(1 +γ
2)/radicalbig
λT
1−γ+2
1−γ/parenleftbigg2
1−γ(1−λ)βE+γ2
1−γ(6λ2(E−1)2+λ(E−1))κ
+/parenleftigg
6γ2λ
1−γ√
E−1 +γ2√
λ
1−γ/parenrightigg/radicalbigg
λ(E−1) log|S||A|KT
δ+γ
1−γ/radicalbigg
1
Kλlog|S||A|TK
δ/parenrightigg
≤1
1−γ(1 +γ
2)/radicalbig
λT
1−γ+4
(1−γ)2(1−λ)/radicalbig
(1−γ)T
2λ+2γ2
(1−γ)2(6λ2(E−1)2+λ(E−1))κ
+/parenleftigg
12γ2λ
(1−γ)2√
E−1 +2γ2√
λ
(1−γ)2/parenrightigg/radicalbigg
λ(E−1) log|S||A|KT
δ+2γ
(1−γ)2/radicalbigg
1
Kλlog|S||A|TK
δ
≤1
1−γexp/braceleftbigg
−1
2/radicalbig
(1−γ)λT/bracerightbigg
+4
(1−γ)2exp/braceleftig
−/radicalbig
(1−γ)λT/bracerightig
+2γ2
(1−γ)2(6λ2(E−1)2+λ(E−1))κ
+/parenleftigg
12γ2λ
(1−γ)2√
E−1 +2γ2√
λ
(1−γ)2/parenrightigg/radicalbigg
λ(E−1) log|S||A|KT
δ+2γ
(1−γ)2/radicalbigg
1
Kλlog|S||A|TK
δ
≤4
(1−γ)2exp/braceleftbigg
−1
2/radicalbig
(1−γ)λT/bracerightbigg
+2γ2
(1−γ)2(6λ2(E−1)2+λ(E−1))κ
+/parenleftigg
12γ2λ
(1−γ)2√
E−1 +2γ2√
λ
(1−γ)2/parenrightigg/radicalbigg
λ(E−1) log|S||A|KT
δ+2γ
(1−γ)2/radicalbigg
1
Kλlog|S||A|TK
δ.
22Under review as submission to TMLR
F Proof of Theorem 2
Let|A|= 1, in which case Q-function coincides with the V-function. According to Algorithm 1, when
(t+ 1)modE̸= 0, we have
Qk
t+1=/parenleftbig
(1−λ)I+λγPk/parenrightbig
Qk
t+λR.
DefineAk≜(1−λ)I+λγPk. We obtain the following recursion between two synchronization rounds:
Qk
(r+1)E= (Ak)EQk
rE+/parenleftbig
(Ak)0+...(Ak)E−1/parenrightbig
λR.
Define
¯A(ℓ)≜1
KK/summationdisplay
k=1(Ak)ℓ. (27)
Note thatQ∗is the fixed point under the transition kernel ¯P, we haveλR=λ(I−γ¯P)Q∗= (I−¯A(1))Q∗
since ¯A(1)=I−λ(I−γ¯P). Furthermore, since Q1
tE,...,QK
tEare identical due to synchronization, we get
¯Q(r+1)E=¯A(E)¯QrE+/parenleftig
I+¯A(1)+...¯A(E−1)/parenrightig/parenleftig
I−¯A(1)/parenrightig
Q∗.
Consequently,
∆(r+1)E=Q∗−¯Q(r+1)E
=¯A(E)∆rE+/parenleftig/parenleftig
I−¯A(E)/parenrightig
−/parenleftig
I+¯A(1)+...¯A(E−1)/parenrightig/parenleftig
I−¯A(1)/parenrightig/parenrightig
Q∗. (28)
Next, consider|S|= 2and evenKwith
P2k−1=/bracketleftbigg1 0
0 1/bracketrightbigg
, P2k=/bracketleftbigg0 1
1 0/bracketrightbigg
,fork∈N.
Then ¯P=1
211⊤, where 1denotes the all ones vector. For the above transition kernels, we have
1
kK/summationdisplay
k=1(Pk)ℓ=/braceleftigg
I, ℓeven,
¯P, ℓodd.
Applying the definition of ¯A(ℓ)in equation 27 yields that
¯A(ℓ)=1
KK/summationdisplay
k=1(Ak)ℓ
=1
KK/summationdisplay
k=1((1−λ)I+λγPk)ℓ
=1
KK/summationdisplay
k=1ℓ/summationdisplay
j=0/parenleftbiggℓ
j/parenrightbigg
(λγPk)j((1−λ)I)ℓ−j
=/summationdisplay
jeven/parenleftbiggℓ
j/parenrightbigg
(1−λ)ℓ−j(λγ)j(I−¯P+¯P) +/summationdisplay
jodd/parenleftbiggℓ
j/parenrightbigg
(1−λ)ℓ−j(λγ)j¯P
=1
2((1−λ−λγ)ℓ+ (1−λ+λγ)ℓ)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜αℓ(I−¯P) + (1−λ+λγ)ℓ
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≜βℓ¯P
=αℓ(I−¯P) +βℓ¯P,
which is the eigen-decomposition of ¯A(ℓ). Let
λ1≜(1 +γ)λ,λ2≜(1−γ)λ, ν 1= 1−λ1,ν2= 1−λ2.
23Under review as submission to TMLR
Then
αℓ=1
2(νℓ
1+νℓ
2), βℓ=νℓ
2. (29)
Note that 0≤α≤β≤1andI−¯Pand ¯Pare orthogonal projection matrices satisfying (I−¯P)¯P= 0. The
matrices for the second term of the error on the right-hand side of 28 reduce to
/parenleftig
I+¯A(1)+...¯A(E−1)/parenrightig/parenleftig
I−¯A(1)/parenrightig
=/parenleftiggE−1/summationdisplay
ℓ=0αℓ(I−¯P) +E−1/summationdisplay
ℓ=0βℓ¯P/parenrightigg
/parenleftbig
(α0−α1)(I−¯P) + (β0−β1)¯P/parenrightbig
=/parenleftigg
(1−α1)E−1/summationdisplay
ℓ=0αℓ(I−¯P)2+ (1−β1)E−1/summationdisplay
ℓ=0βℓ¯P2/parenrightigg
sinceα0=β0= 1
=/parenleftigg
(1−α1)E−1/summationdisplay
ℓ=0αℓ(I−¯P) + (1−β1)E−1/summationdisplay
ℓ=0βℓ¯P/parenrightigg
since (I−¯P)and ¯Pare idempotent .
It follow that
/parenleftig
I−¯A(E)/parenrightig
−/parenleftig
I+¯A(1)+...¯A(E−1)/parenrightig/parenleftig
I−¯A(1)/parenrightig
=/parenleftigg
(1−αE)−(1−α1)/parenleftiggE−1/summationdisplay
i=0αi/parenrightigg/parenrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
≜κE(I−¯P) +/parenleftigg
(1−βE)−(1−β1)/parenleftiggE−1/summationdisplay
i=0βi/parenrightigg/parenrightigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
=0¯P.
Applying equation 29 yields that
κE=−γ
2/parenleftbigg1−νE
2
1−γ−1−νE
1
1 +γ/parenrightbigg
. (30)
It follows from equation 28 that the error evolves as
∆(r+1)E=/parenleftbig
αE(I−¯P) +βE¯P/parenrightbig
∆rE+κE(I−¯P)Q∗,
which further yields the following full recursion of the error:
∆rE=/parenleftbig
αE(I−¯P) +βE¯P/parenrightbigr∆0+r−1/summationdisplay
ℓ=0/parenleftbig
αE(I−¯P) +βE¯P/parenrightbigℓκE(I−¯P)Q∗
=/parenleftbig
αr
E(I−¯P) +βr
E¯P/parenrightbig
∆0+r−1/summationdisplay
ℓ=0/parenleftbig
αℓ
E(I−¯P) +βℓ
E¯P/parenrightbig
κE(I−¯P)Q∗
since/parenleftbig
αE(I−¯P) +βE¯P/parenrightbigℓ=αℓ
E(I−¯P) +βℓ
E¯P,∀ℓ∈N
=/parenleftbig
αr
E(I−¯P) +βr
E¯P/parenrightbig
∆0+1−αr
E
1−αEκE(I−¯P)Q∗
=/parenleftbigg
αr
E+1−αr
E
1−αEκE/parenrightbigg
(I−¯P)Q∗+βr
E¯PQ∗,
where the last equality applied the zero initialization condition.
Note that (I−¯P)Q∗and ¯PQ∗are orthogonal vectors. Since |S|= 2, we have
∥∆rE∥∞≥1√
2∥∆rE∥2≥min{∥(I−¯P)Q∗∥2,∥¯PQ∗∥2}√
2·max/braceleftbigg
|αr
E+1−αr
E
1−αEκE|,βr
E/bracerightbigg
.
24Under review as submission to TMLR
SinceQ∗= (I−γ¯P)−1R= (I−¯P)R+1
1−γ¯PR, we obtain that
∥(I−¯P)Q∗∥2=∥(I−¯P)R∥2,∥¯PQ∗∥2=1
1−γ∥¯PR∥2.
Therefore, for the reward Rin general position, we have min{∥(I−¯P)Q∗∥2,∥¯PQ∗∥2}≥cRfor some constant
cRdepending on the reward function. It remain to analyze the coefficients as functions of λ. To this end, we
introduce the following lemma:
Lemma 5. The following properties hold:
1. Negativity: κE<0;
2. Monotonicity:κE
1−αEis monotonically decreasing for λ∈(0,1
1+γ);
3. Upper bound:|κE
1−αE|≤γ2
1−γ2forλ∈(0,1
1+γ);
4. Lower bound: if (1 +γ)λ≤1
2E, then|κE
1−αE|≥λγ2(E−1)
4.
Proof.We prove the properties separately.
1.Note thatν1<ν2,1−ν1= (1 +γ)λ, and 1−ν2= (1−γ)λ. Then it follows from equation 30 that
κE=−λγ
2E−1/summationdisplay
i=1(νi
2−νi
1)<0.
2. For the monotonicity, it suffices to show thatd
dλκE
1−αE≤0. We calculate the derivative as
d
dλκE
1−αE=γE(1−νE
1)(1−νE
2)
2(1−γ2)(1−αE)2/parenleftigg
(1 +γ)νE−1
1
1−νE
1−(1−γ)νE−1
2
1−νE
2/parenrightigg
.
Note that
(1 +γ)νE−1
1
1−νE
1−(1−γ)νE−1
2
1−νE
2=1
λ/parenleftigg
νE−1
1
1 +ν1+···+νE−1
1−νE−1
2
1 +ν2+···+νE−1
2/parenrightigg
≤0.
3.For the upper bound, it suffices to show the result at λ=1
1+γdue to the negativity and monotonicity.
Atλ=1
1+γ, we have
/vextendsingle/vextendsingle/vextendsingle/vextendsingleκE
1−αE/vextendsingle/vextendsingle/vextendsingle/vextendsingle=γ
1−γ2/parenleftigg
γ−(2γ
1+γ)E
2−(2γ
1+γ)E/parenrightigg
≤γ2
1−γ2.
4. For the lower bound, the case E= 1trivially holds. Next, consider E≥2. We have
κE
1−αE=−γ
1−γ2(1 +γ)(1−νE
2)−(1−γ)(1−νE
1)
(1−νE
1) + (1−νE
2)
=−λγ/summationtextE−1
ℓ=1(νℓ
2−νℓ
1)
(1−νE
1) + (1−νE
2).
Note that 1−nx≤(1−x)n≤1−1
2nxforn≥1and0≤x≤1
n. Then, for (1 +γ)λ≤1
2E, we have
νE
1= (1−(1 +γ)λ)E≥1−(1 +γ)λE≥1
2,
νE
2= (1−(1−γ)λ)E≥1−(1−γ)λE.
25Under review as submission to TMLR
Moreover, for all x∈[ν1,ν2]⊆[0,1]andℓ−1≤E, we have
xℓ−1≥xE≥νE
1≥1
2.
We obtain that
/summationtextE−1
ℓ=1(νℓ
2−νℓ
1)
(1−νE
1) + (1−νE
2)≥/summationtextE−1
ℓ=1/integraltextν2
ν1ℓ·xℓ−1dx
2λE≥/summationtextE−1
ℓ=1ℓ1
2(ν2−ν1)
2λE=1
4γ(E−1).
The proof is completed.
We consider two regimes of the stepsize separated by λ0≜logr
(1−γ)rE<1
1+γ, where the dominating error is due
to the small stepsize and the environment heterogeneity, respectively:
Slow rate due to small stepsize when λ≤λ0.Sinceβr
Emonotonically decreases as λincreases,
βr
E= (1−(1−γ)λ)rE≥(1−(1−γ)λ0)rE=/parenleftbigg
1−logt
rE/parenrightbiggrE
.
Note thatlogr
rE∈(0,1
2), applying the fact log(1−x) +x≥−x2forx∈[0,1
2]yields that
log/parenleftbigg
1−logr
rE/parenrightbigg
+logr
rE≥−/parenleftbigglogr
rE/parenrightbigg2
≥−1
rE.
Then we get
βr
E≥/parenleftbigg
1−logr
rE/parenrightbiggrE
≥1
er.
Slow rate due to environment heterogeneity when λ≥λ0.Recall that λ <1
1+γ. Applying the
triangle inequality yields that
/vextendsingle/vextendsingle/vextendsingle/vextendsingleαr
E+1−αr
E
1−αEκE/vextendsingle/vextendsingle/vextendsingle/vextendsingle≥/vextendsingle/vextendsingle/vextendsingle/vextendsingleκE
1−αE/vextendsingle/vextendsingle/vextendsingle/vextendsingle−/parenleftbigg
1 +/vextendsingle/vextendsingle/vextendsingle/vextendsingleκE
1−αE/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
αr
E.
For the first term, by the negativety and monotonicity in Lemma 5, it suffices to show the lower bound
atλ=λ0. Sinceλ<1
1+γ, thenαE=1
2/parenleftbig
(1−(1−γ)λ)E+ (1−(1 +γ)λ)E/parenrightbig
decreases as λincreases. For
t≳1
1−γlog1
1−γsuch that (1 +γ)λ0≤1
2E, we apply the lower bound in Lemma 5 and obtain that
/vextendsingle/vextendsingle/vextendsingle/vextendsingleκE
1−αE/vextendsingle/vextendsingle/vextendsingle/vextendsingle≳γ2logr
(1−γ)r.
Additionally, applying the upper bound in Lemma 5 yields
/parenleftbigg
1 +/vextendsingle/vextendsingle/vextendsingle/vextendsingleκE
1−αE/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
αr
E≤νrE
2
1−γ2=(1−(1−γ)λ)rE
1−γ2≤1
(1−γ2)r.
The conclusion follows.
26Under review as submission to TMLR
G Additional experiments
G.1 Impacts of Eon homogeneous settings.
For the homogeneous settings, in addition to E= 10, we also consider E={1,20,40,∞}, whereE=∞
means no communication among the agents throughout the entire learning process. Similar to Figure 2b,
there is no obvious two-phase phenomenon even in the extreme case when E=∞. Also, though there is
indeed performance degradation caused by larger E, the overall performance degradation is nearly negligible
compared with the heterogeneous settings shown in Figures 2a and 3.
(a) E=1
 (b) E=20
(c) E=40
 (d) E= ∞
Figure 5: Homogeneous FQL with varying E.
G.2 Different target error levels.
In Figure 6, we show the error levels that these training strategies can achieve within a time horizon
T= 20,000. The tolerance levels are 10%,5%,3%,and1%of the initial error ∥∆0∥∞, respectively. At a
high level, choosing different stepsizes for phases 1 and 2 can speed up convergence.
27Under review as submission to TMLR
(a) One common λ=1√
Tthroughout. ∥∆t∥∞does meet
any of the tolerance levels within 20000 iterations
(b) With a phase 1 stepsize of 0.9, it meets the 10%
tolerance level at iteration 16502.
(c) With a phase 1 stepsize of 0.5, it meets the 10%
tolerance level at iteration 15250.
(d) With a phase 1 stepsize of 0.2, it meets the 10%
and 5% tolerance level at iterations 9669 and 19597,
respectively.
(e) With a phase 1 stepsize of 0.1, it meets the 10%
and 5% tolerance level at iterations 3901 and 14008,
respectively.
(f) With a phase 1 stepsize of 0.05, it meets the 10%, 5%,
and 3% tolerance levels at iterations 4610, 8795, and
16687, respectively.
Figure 6: Convergence performance of different tolerance levels of different stepsize choices. The horizontal dashed lines
represent the tolerance levels not met, while the vertical dashed lines indicate the iterations at which the training processes meet
the corresponding tolerance levels.
28