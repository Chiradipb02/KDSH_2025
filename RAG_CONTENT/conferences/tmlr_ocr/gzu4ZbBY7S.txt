Published in Transactions on Machine Learning Research (11/2022)
Incorporating Sum Constraints into
Multitask Gaussian Processes
Philipp Pilar philipp.pilar@it.uu.se
Department of Information Technology
Uppsala University
Carl Jidling carl.jidling@it.uu.se
Department of Information Technology
Uppsala University
Thomas B. Schön thomas.schon@it.uu.se
Department of Information Technology
Uppsala University
Niklas Wahlström niklas.wahlstrom@it.uu.se
Department of Information Technology
Uppsala University
Reviewed on OpenReview: https: // openreview. net/ forum? id= gzu4ZbBY7S
Abstract
Machine learning models can be improved by adapting them to respect existing background
knowledge. In this paper we consider multitask Gaussian processes, with background
knowledge in the form of constraints that require a specific sum of the outputs to be
constant. This is achieved by conditioning the prior distribution on the constraint fulfillment.
The approach allows for both linear and nonlinear constraints. We demonstrate that the
constraints are fulfilled with high precision and that the construction can improve the overall
prediction accuracy as compared to the standard Gaussian process.
1 Introduction
Many real world problems come with background knowledge known a priori, for instance that the outputs
must be positive at all times or fulfill a certain differential equation. The constraints are often known to
near perfect precision. Any model would certainly benefit from having such knowledge hardcoded in advance
instead of having to rediscover it, as the additional information would allow for the exclusion of the majority
of possible outputs.
In this work we consider the Gaussian process (GP) (Rasmussen & Williams, 2006), which is a popular
and powerful machine learning model. Some assumptions about the underlying function, e.g. regarding its
smoothness, can be encoded in a relatively straightforward way into the kernel of the GP. However, it is
usually trickier to include more specific prior knowledge and constrained GPs (or, for that matter, constrained
machine learning methods) constitute a relevant and active area of research (Willard et al., 2021; Swiler
et al., 2020).
In this work, we focus on constraints that take the form of a sum over the outputs of a multitask GP.
Constraints of this form arise, for example, when considering conserved quantities in physics such as energy
and momentum, where the sum over the energies or momenta of all subcomponents of a closed system must
remain constant. As a toy example, we consider the harmonic oscillator, which is ubiquitous in physics; the
1Published in Transactions on Machine Learning Research (11/2022)
expression for the energy takes the form
E=Epot(t) +Ekin(t) =kz(t)2/2 +mv(t)2/2, (1)
whereEpotandEkindenote potential and kinetic energy, respectively. We assume that the displacement
from the rest position zand the velocity vare the outputs of a multitask GP, whereas the time tserves as
input. While the input in this example is one-dimensional, the results we derive in this paper also apply to
higher dimensional inputs.
We have developed a method that allows nonlinear constraints like (1)to be incorporated into the GP. First,
we show how nonlinear constraints can be reduced to linear ones via a suitable transformation of the outputs
of the GP. Then we proceed to condition the joint prior of the GP on the constraints, which in turn results in
a constrained predictive distribution. In the next section, we start by providing a formal definition of the
problem.
2 Problem Formulation
2.1 Background on the GP
A GP is formally defined as “a collection of random variables, any finite number of which have a joint
Gaussian distribution” (Rasmussen & Williams, 2006). Formally, we write f(x)∼GP (m(x),k(x,x′)), where
m(x) =E[f(x)]andk(x,x′) =E[(f(x)−m(x))(f(x′)−m(x′))]are the mean and the covariance function of
the GP, respectively. The dataset available for training the GP consists of inputs X={xk}N
k=1and noisy
outputsyk=f(xk) +ϵk, where we assume Gaussian noise ϵk∼N(0,σ2
n). We use yto denote a vector storing
allNoutputs.
In the following we consider the multitask setting (Bonilla et al., 2008; Skolidis & Sanguinetti, 2011), where a
vector f(x)ofNfoutputs is learned. The overall GP framework remains unchanged but the output vector f
(and observation vector y) has to be interpreted as an extended vector consisting of the concatenated multitask
outputs fk=f(xk)— that is f= [fT
1,fT
2,...,fT
N]Tfor which it holds that f∼N (mf(X),Kf,f′(X,X′)).
When constructing the mean and covariance function, the different tasks need to be taken into account
(Alvarez et al., 2012). We write the mean as
mf(X) = [md(x1)mt(x1)T,...,m d(xN)mt(xN)T]T, (2)
wheremd(·)is the data mean and mt(·)is the task mean. The task mean returns a column vector of length
Nf. The covariance matrix becomes
Kf,f′(X,X) =
kd11kt(x1,x1)kd12kt(x1,x2)...
kd21kt(x2,x1)kd22kt(x2,x2)...
.........
, (3)
wherekdij=kd(xi,xj), and where kd(·,·)andkt(·,·)denote the data and task kernels, respectively. Note
that the task kernel returns a matrix of size (Nf,Nf).
The task mean and kernel are often assumed to be position independent (although this assumption is not
necessary for our method to work); then mfandKf,f′can be written as Kronecker products
mf(X) =md(X)⊗mt, (4a)
Kf,f′(X,X′) =kd(X,X′)⊗Σt. (4b)
Given the expressions for the mean and the kernel, the predictive distribution is formed through the standard
procedure; see Section B.1 in the supplementary material for details. See also Section B.2, for details on how
to deal with the case of incomplete measurements, i.e. when there are data points ykfor which only some of
the output tasks have been measured.
2Published in Transactions on Machine Learning Research (11/2022)
2.2 Sum Constraint
The main concern of this work is to show how constraints on the sum of some (nonlinear) transformations
hi(·)of the outputs fican be incorporated into the GP. Formally, we define this class of constraints as
F[f(x)] =/summationdisplay
iai(x)hi(fi(x)) =C(x), (5)
where the functions ai(x)serve as prefactors to the various terms in the sum, iindexes the outputs of the
GP, andC(x)specifies what value the sum over the outputs should equal at position xin the input space. In
the following we refer to constraints of this form as sum constraint .
In the general case (5), we consider input-dependent constraints C(x)andai(x). This requires knowledge of
the functions C(x)andai(x), which could be practically infeasible. Hence, an important special case of (5)
is theconstant sum constraint
F[f(x)] =/summationdisplay
iaihi(fi(x)) =C, (6)
with constant prefactors aiand constant sum C.
One example of a constant sum constraint is the previously mentioned energy conservation for the harmonic
oscillator (1). There we have a1=k/2,a2=m/2,h1(z) =z2,h2(v) =v2andC=E. Other situations
where sum constraints arise include learning of probabilities that must sum to one, and the case of mechanical
equilibrium where the sum of acting forces must be zero at each point.
3 Method
Let us now develop the methodology required to incorporate sum constraints as defined in Section 2.2 into
the GP. In Section 3.1.1, we consider the case where all the outputs of the GP enter the sum constraint via a
monotonic (invertible) nonlinearity and show how to reduce it to a linear sum constraint. In Section 3.1.2
we extend the procedure to sum constraints with non-monotonic nonlinearities. Finally, we show in Section
3.2 how to include linear sum constraints into the GP and hence, via the aforementioned reductions, also
nonlinear sum constraints.
3.1 Reduction to Linear Constraint
3.1.1 Monotonically Increasing Nonlinearity
Consider the sum constraint (5)— while the constraint is nonlinear in terms of the outputs, it is linear in
terms of the transformed outputs hi(fi); definingf′
i=hi(fi)and substituting it into (5) yields
F[f′(x)] =/summationdisplay
ai(x)f′
i(x) =C(x), (7)
which is linear in the transformed outputs f′
i. Hence, we can train a GP to predict the transformed outputs
obeying the linear constraint (7)and backtransform to the original outputs via fi=h−1
i(f′
i). Note that this
GP needs to be trained on transformed data y′, wherey′
i=hi(yi). This approach requires that the nonlinear
functionshi(·)are invertible, otherwise it is not possible to unambiguously recover the fi. See also Snelson
et al. (2004).
However, it is not necessary for hito be invertible on its entire domain. Consider the case where it is known
that the output fiis restricted to an invertible subregion of the domain of hi; then we solve the problem by
choosing the backtransformation h−1
icorresponding to this subregion. For example, in case of the square
function, we can consider the case where fiis known to be always positive (or always negative). Then we can
just restrict the domain of the nonlinearity hito the positive (negative) half-axis where the function is in fact
invertible.
When employing the transformation (7), it is important to keep in mind that the GP prior now has to be
chosen in a way suitable for the transformed outputs f′instead off; depending on the transformations h(·)
3Published in Transactions on Machine Learning Research (11/2022)
Algorithm 1 The Constrained GP: High-level Procedure
Step 1: train an unconstrained GP on the data yto obtain the auxiliary outputs faux
- (optional) use the posterior mean of fauxto create virtual measurements
Step 2: train the constrained GP on the transformed data y′(for details, see Algorithm 2) to obtain f′
- (optional) (re)learn the auxiliary outputs together with the constrained outputs
Step 3: backtransform the transformed outputs f′using the posterior mean of fauxfrom Step 1
involved, this could prove to be more challenging. We recover credible intervals for fin the same way as we
recoverf, by backtransforming them; for more details, see Section B.7 in the Supplementary material.
Furthermore, the noise corresponding to the transformed data y′will in general not be normally distributed
anymore, which means that GP regression loses its analytical tractability due to the resulting non-Gaussian
likelihood. Methods to deal with non-Gaussian likelihoods include the Laplace approximation (Williams
& Barber, 1998; Vanhatalo et al., 2009), variational inference (Blei et al., 2017; Tran et al., 2016), and
expectation propagation (Minka, 2001). Due to its simplicity, in this work we use the Laplace approximation
to deal with this issue, where applicable. It enables us to approximate non-Gaussian distributions with a
Gaussian; see Appendix B.3 for details.
3.1.2 Non-monotonically Increasing Nonlinearity
In the previous section we showed how to reduce nonlinear sum constraints to linear ones, as long as the
nonlinearities are monotonic. However, this is a rather limiting assumption as it would exclude e.g. the
square function h(f) =f2from the admissible transformations. Here we describe a way of circumventing this
problem.
The idea underlying our solution is to introduce one (or multiple) auxiliary variables that allow for a
unique backtransformation. Typically, the auxiliary variables will keep track of where in the domain of
h(·)it is thatf′lies, such that the correct local inverse can be chosen when backtransforming. In case
of the square function, we can add the auxiliary output faux=fand retrieve the initial output fvia
f=sign(faux)h−1(f′) =sign(faux)/radicalbig
h(f). While the initial output fis a practical choice here, this is in
general not necessary and fauxcan be chosen arbitrarily.
There is no guarantee that learned values f′will always fall within the domain of the backtransformation. If
it happens that a predicted value lies outside, a pragmatic solution is to approximate f′with the closest
valid value; for example zero in case of negative valued predictions for square values.
Sometimes more information can be extracted from fauxand used to ameliorate the transformed data y′, for
instance when the backtransformation switches from one local inverse to another; then we can add virtual
measurements for f′at those points and force the constrained GP towards values consistent with faux, which
can significantly reduce artefacts in the backtransformed outputs f. Note that this can come at the cost of
overconfident credible intervals in the vicinity of the virtual measurements.
In Algorithm 1, we summarize this procedure. In most cases, it is advantageous to learn the auxiliary outputs
in a separate GP in Step 1, independently of the constrained outputs; when virtual measurements are to be
created, this is required. Optionally, auxiliary outputs can be (re)learned in Step 2; for some examples, this
can stabilize the hyperparameter learning of the constrained GP. However, when virtual measurements are
involved, the prediction fauxfrom Step 1 should also be used for the backtransformation.
We illustrate the approach by returning to the harmonic oscillator (1), with the transformed outputs f′
1=z2
andf′
2=v2(see also the last paragraph in Section 2.2). We choose the auxiliary outputs as f1
aux=zand
f2
aux=v, which we use to extract the sign when backtransforming f′
1andf′
2; furthermore, we use the auxiliary
outputs to create virtual measurements for f′
1andf′
2at the zero crossings of the posterior mean of f1
aux
andf2
aux. In order to fit the transformed outputs of the GP, the observations yk= [zk,vk]Tare transformed
analogously to obtain y′
k= [z2
k,v2
k,zk,vk]T;zkandvkare part of y′
ksince we chose to relearn them together
with the constrained outputs to improve the performance. The virtual measurements are also included in
the transformed data y′. In terms of the transformed outputs the constraint can be written compactly as
4Published in Transactions on Machine Learning Research (11/2022)
Algorithm 2 Constraining the GP (Section B.1 refers to the Supplementary material)
Input:mean mf(·);kernel Kf,f′(·,·);constraints (F,S);(transformed) data X,y′;points of prediction X∗
Output: constrained predictive distribution f′
∗|X,y′,X∗
Note:During hyperparameter optimization X∗={}and hence f′
∗={}
Step 1: Construct the joint prior distribution for [f′,f′
∗]T∼N(µ0,Σ0)according to (B.1)
-omit noise term σ2
nI
Step 2: Construct Ftot,Stotaccording to (9b)
Step 3: UseFtot,Stotto calculate constrained µ′,Σ′according to (8b)
Step 4: Remove entries in µ′,Σ′corresponding to incomplete measurements as detailed in Section B.2
ifHyperparameter optimization then
Step 5: Calculate the log marginal likelihood according to (B.7c)
Step 6: Perform optimization step
else ifPrediction then
Step 5: Calculate the predictive distribution f′
∗|X,y′,X∗according to (B.7a)
end if
Ff′=C, where F= [a1,a2,0,0]. For more details on the harmonic oscillator dataset, see Section C.1 in the
supplementary material.
3.2 Solving with Linear Constraints
Having shown how to reduce nonlinear sum constraints to linear ones, we proceed to describe how to
incorporate linear sum constraints into the GP. The idea is to make use of the fact that sampling from a
GP is equivalent to sampling from a multivariate Gaussian distribution, where the mean and covariance are
obtained by evaluating the mean and the kernel of the GP at the points of interest.
Let the random vector f′∼N(µ,Σ); we are interested in the conditional distribution f′|/summationtext
iaif′
i=C. More
generally, to include multiple sum constraints, we want to find the distribution f′|Ff′=S, where the rows of
the matrix Fcontain the coefficients for each of the NFsum constraints to be included, and the elements of
the vector Scontain the corresponding sums; compare equation (9a) below.
The required conditional distribution can be calculated analytically (Majumdar & Majumdar, 2019) as
(f′|Ff′=S)∼N (µ′,Σ′), (8a)
where
µ′=Aµ+DTS, Σ′=ATΣA,
D= (FΣFT)−1FΣT,A=In−DTF.(8b)
Of course, we need to enforce the constraint at all Ntotdata points — to that end, we construct the
blockdiagonal matrix Ftotand the vector Stotaccording to
F(x) =
a1(x)a2(x)...
b1(x)b2(x)...
......
, S(x) =
Ca(x)
Cb(x)
...
, (9a)
Ftot=diag(F(x1),F(x2),...), Stot= [S(x1)T,...]T. (9b)
We useNtotin two different contexts: during the hyperparameter optimization, Ntotdenotes the number of
data points; whereas during prediction, Ntotdenotes the number of both data and predictive points.
Algorithm 2 summarizes the practical procedure of constructing the covariance and the mean, both during
hyperparameter optimization and when forming the constrained predictive distribution of the GP. In case of
a position dependent constraint, it is important to note that the values of the functions C(x)andai(x)must
be known at all points for which the constraint should be enforced; in our case, this means all Ntotpoints.
5Published in Transactions on Machine Learning Research (11/2022)
Note that in Step 1 of the algorithm we first omit the noise term, since the constraints only hold exactly for
noiseless data; the noise then enters in Step 4, after the constraints have been taken into account.
Mathematically, the constraint is enforced by conditioning the Gaussian distribution on it. While the method
is not strictly global in the sense of providing a constrained kernel for the GP, it is global for practical
purposes as the constraint is enforced at all points of prediction of the GP.
Due to the matrix inversion in (8b), the computational complexity of the algorithm is cubic with leading
order term∼O(N3
FN3
tot), during both hyperparameter optimization and prediction.
3.2.1 Special Case of Constant Constraints
In the special case of constant constraints and constant inter-task dependencies of the GP mean and kernel,
the constraints can be incorporated more efficiently. Here, the kernel of the GP factorizes into data and task
kernel as in (4)and the procedure above simplifies: it now suffices to enforce the constraints (F,S)on the
task mean and covariance matrix and to subsequently perform the Kronecker product with the data mean
and covariance matrix to obtain the constrained distribution.
Formally, this can be written as follows: let µtandΣtbe the task mean and covariance matrix, respectively;
then the constrained quantities µ′
tandΣ′
tare calculated via (8b), using FandS(since the task mean and
covariance matrix are constrained directly, it is not necessary to construct FtotandStot). Finally, the full
constrained mean and covariance matrix are constructed via µ′=m⊗µ′
tandΣ′=K⊗Σ′
t, where mandK
are the data mean and covariance matrix, respectively. Due to the constant constraint, the data mean is
also required to be constant. Without loss of generality, we choose it as m=1Ntot(compare B.6.1). This
procedure is summarized in Algorithm 3 in the Supplementary material. Furthermore, we provide proof that
this approach is indeed equivalent to the more general approach from Section 3.2 in Appendix B.6.1.
Now, the complexity of the matrix inversion involved in (8b)is reduced to∼O(N3
F); since µtandΣtare
constraineddirectlyitnolongerdependson Ntot(comparealso (4)). Thisconstitutesasignificantimprovement
over the general algorithm as usually NF≤Nf≪Ntot, whereNFis the number of constraints and Nfthe
number of tasks. Whenever applicable, it is preferable to use this way of incorporating the constraint, since it
is more efficient and numerically more stable than the general procedure given in Algorithm 2.
4 Experimental Results
In this section, we demonstrate our method at the hand of two simulation experiments and one real data
experiment1. They have in common that the constraints involved are constant (see Section 3.2.1); for
examples of the non-constant case, see Sections A.2 and A.3 in the Supplementary material.
4.1 Toy Problem Revisited
We gave a formulation of the auxiliary variables approach for the harmonic oscillator in Section 3.1.2 and
detailed information on the dataset can be found in Section C.1 in the Supplementary material. Figure 1
illustrates this approach. The constrained GP achieves higher overall accuracy around extremal points, where
the prediction is more robust with regard to the influence of random noise. In addition, the constrained GP
manages to mitigate the negative effect of incomplete measurements, i.e. data points where only one of the
two output dimensions has been measured, better than the unconstrained one (compare left part of vauxin
the figure). This is natural, since the constrained GP has implicitly added a correlation between the two
outputs, which the unconstrained GP is lacking.
The credible intervals in Figure 1 clarify another advantage of the constrained GP: when multiple outputs
are learned to a different degree of certainty, information can be transferred from high- to low-credibility
outputs, thereby narrowing the credible intervals also for the latter. This is clearly visible in areas with
incomplete measurements. On the other hand, credible intervals tend to be overconfident in the vicinity of
1The code used for the experiments is available at https://github.com/ppilar/SumConstraint.
6Published in Transactions on Machine Learning Research (11/2022)
zaux vaux z2v2
2Ez v
0 2 4 6 8 101.5
1.0
0.5
0.00.51.01.52.0
0 2 4 6 8 100.00.51.01.52.02.5
0 2 4 6 8 101.5
1.0
0.5
0.00.51.01.52.0
t t t
Figure 1: Demonstration of the auxiliary variables approach for the harmonic oscillator. The quantities
zaux,zandvaux,vrefer to the position and velocity of the harmonic oscillator, respectively. We distinguish
betweenzaux,zandvaux,vto emphasize that, while they aim to approximate the same curve, they are
learned by different GPs. The posterior means of the GPs are depicted, together with the 2σcredible intervals.
The dotted lines represent the true curves and the big dots/crosses correspond to the data available to the
GPs. Left:Results for the unconstrained GP are shown. For this example, these outputs coincide with
the auxiliary outputs required for the constrained GP. Middle: The transformed outputs learned by the
constrained GP are depicted, together with the constraint 2E=kz2+mv2(wherek=m= 1). The results
for the auxiliary outputs have been employed to create virtual measurements at zero crossings (differently
colored squares) in order to force the quadratic functions towards zero. Right:The backtransformed outputs
of the constrained GP are shown, where the auxiliary outputs zauxandvauxhave been used to recover the
signs.
σn= 0.05 σn= 0.1 σn= 0.3
GP-c GP-u GP-c GP-u GP-c GP-u
fd= 0RMSE 2.3±0.6 3.2±0.54.4±1.2 6.4±1.113.7±3.7 18.5±3.3 (e-2)
|∆C|0.0±0.0 3.1±0.70.0±0.0 6.5±1.4 0.1±0.1 18.7±4.4 (e-2)
fd= 0.2RMSE 3.4±3.0 4.8±2.85.4±1.9 8.0±2.517.0±5.9 23.0±5.7 (e-2)
|∆C|0.0±0.1 4.3±1.50.1±0.2 7.8±2.2 0.2±0.4 22.6±6.3 (e-2)
Table 1: Comparison of the performance of the constrained GP (GP-c) and the unconstrained GP (GP-u) for
the harmonic oscillator. Shown are the root mean squared error (RMSE) of the prediction as well as the
mean absolute violation of the constraint, |∆C|. The standard deviation of the noise is given by σnwhereas
fdis the probability with which output components have been omitted at random from the data. The values
have been obtained by averaging over 50 datasets and are given plus-or-minus one standard deviation. Bold
font highlights best performance.
virtual measurements. Due to the nonlinear, piecewise backtransformation, some discontinuities have been
introduced in the credible intervals of the constrained GP near the zero crossings.
In Table 1, values for both the root mean squared error (RMSE) and the average absolute violation of the
constraint|∆C|are given for various noise levels σn, both with complete and incomplete measurements; in
case of incomplete measurements, the output components have been omitted at random with probability
fd= 0.2. The values have been obtained by averaging over 50 datasets. We observe that the constrained GP
fulfills the constraint with up to two orders of magnitude higher accuracy and also performs slightly better in
terms of RMSE.
The reason why the constraint is not fulfilled with yet higher accuracy for the constrained GP is that around
zero crossings it can occur that invalid values are predicted by the constrained GP (that is, negative values
forz2andv2), which we pragmatically put to zero. This is also the origin of the small artefacts visible in
that region of the mean curves in the right plot of Fig. 1.
7Published in Transactions on Machine Learning Research (11/2022)
10
5
0 5 10024681012
xy
Figure2: Visualizationofthetriangleintheplane.
The task for the GP is to give the location of the
corners of the triangle (blue dots) when given the
parameterα, which parameterizes different poses
of the triangle.σn GP-c GP-u GP-tr
1e-4RMSE 3.3±0.24.8±0.3 14±30 (e-3)
|∆C|0.3±0.01.9±0.1 1.8±2.7 (e-3)
1e-3RMSE 5.5±1.05.0±0.39.2±16 (e-3)
|∆C|0.8±0.12.0±0.2 1.6±1.3 (e-3)
1e-2RMSE 4.2±0.81.6±0.23.9±0.9 (e-2)
|∆C|6.2±1.06.4±1.4 8.6±1.4 (e-3)
Table 2: Results for the length constraint applied to
the triangle in the plane. We compare results for the
constrained GP (GP-c), the unconstrained GP (GP-u)
and the unconstrained GP trained on the transformed
outputs (GP-tr) (Salzmann & Urtasun, 2010a). For
small values of noise σn, the sum constraint improves
the performance of the GP. The values have been ob-
tained by averaging over 50 datasets and are given
plus-or-minus one standard deviation.
4.2 Pose Estimation
Here we demonstrate how our approach can incorporate length constraints (Perriollat et al., 2011), inspired by
applications such as pose estimation. In essence, the length constraint states that the distance Llmbetween
two adjacent points (indexed by landm) in a rigid body is constant, irrespective of position and orientation
of the body. When the position is given in terms of Cartesian coordinates zi, the length constraint takes the
following form
3/summationdisplay
i=1z2
li−2zlizmi+z2
mi=L2
lm. (10)
This constraint is no longer an instance of the sum constraint as defined in (5), since the middle term depends
on multiple outputs. However, with a more elaborate transformation procedure, the sum constraint can still
be applied.
To make this more concrete, we consider the example of a triangle in the plane. Here, the outputs of interest
are the coordinates of the triangle corners, f= [z1x,z1y,z2x,z2y,z3x,z3y]. The input αis a continuous
parametrization of different poses of the triangle in the plane. Although αis one-dimensional in this example,
the approach generalizes to higher dimensional inputs. In our choice of transformed outputs, we follow the
approach by Salzmann & Urtasun (2010a), where pairwise products of the original outputs are learned and
subsequently transformed back via a singular value decomposition (SVD); for more details on the technicalities
we refer to Section C.5 in the Supplementary material.
A visualization of the problem is provided in Figure 2 where different poses αof the triangle are depicted;
the blue points represent the corners of the triangle, the positions of which are learned by the GP. As can be
seen from the data in Table 2, our approach here performs best for low noise levels. When the noise is very
small,σ≲1e-3, the constrained approach achieves about the same overall accuracy in terms of RMSE as the
unconstrained GP, whereas the error in the constraint is reduced by factors of 2-6.
This reduction is not simply a result of the particular parameterization of the problem, which enforces the
constraint implicitly for noiseless observations, as shown by Salzmann & Urtasun (2010a). To see that, we
included the results for a GP that is trained on the transformed outputs, but where the constraint is not
enforced explicitly. Table 2 shows that the result is improved when enforcing the constraint in addition to
using the transformed outputs.
8Published in Transactions on Machine Learning Research (11/2022)
xy
0.00 1.00 2.006
4
2
02
t
t t0.0 0.5 1.0 1.5 2.07.5
5.0
2.5
0.02.55.07.510.0
0.0 0.5 1.0 1.5 2.0
Figure 3: Left:Trajectory of the double pendulum. Note that the trajectory shown here is longer than the
sequences of motion considered in the plots to the right. Middle: Kinetic energy Ekin, potential energy Epot
and total energy Eof the double pendulum are shown. It is apparent that for the considered segment of the
motion the energy is constant for practical purposes, except for fluctuations in the contribution of the kinetic
energy due to measurement error. An estimate ˆEof the energy is obtained by averaging over E.Right:
Positionsz0,zand velocities v0,vof the masses (four components each) as learned by the unconstrained
(left inset) and the constrained GP (right inset), respectively; the posterior means of the GPs are depicted
together with the 2σcredible intervals. The dotted lines represent the available data, where the subset of big
dots has been used for training.
4.3 Real Data Experiment: Double Pendulum
In this section we consider the ‘Double Pendulum Chaotic’ dataset (Asseman et al., 2018); this dataset
consists of 21 different two dimensional trajectories of a double pendulum and contains annotated positions of
the masses attached at the ends of the two pendula. Each trajectory consists of about 17000 measurements,
taken at a frequency of 500Hz. For more information on the parameters of the double pendulum, see Section
D.1 in the Supplementary material. We attempt to construct a GP that models both positions zx,zyand
velocitiesvx,vyof the two masses (i.e. 8 outputs), while at the same time respecting the law of energy
conservation; the time tserves as input. As friction is present, we consider a limited section of the trajectory
during the second half of the motion where we can assume constant energy (compare Figure 3); energy
conservation here takes the form
E=mbgzby+mggzgy+mb
2/parenleftbig
v2
bx+v2
by/parenrightbig
+mg
2/parenleftbig
v2
gx+v2
gy/parenrightbig
, (11)
wheregdenotes the gravitational acceleration on earth, and where the indices bandgrefer to the blue and the
green pendulum, respectively. The constraint is incorporated into the GP in analogy to the harmonic oscillator.
In terms of (6), we identify a1= 0,a2=mbg,a3= 0,a4=mgg,a5=mb/2,a6=mb/2,a7=mg/2,
a8=mg/2,h2(zby) =zby,h4(zgy) =zgy,h5(vbx) =v2
bx,h6(vby) =v2
by,h7(vgx) =v2
gx,h8(vgy) =v2
gyand
C=E; note that the coefficients a1,a3correspond to the outputs zbx,zgx, which are not part of the
constraint (11).
We pick a sequence of 200 data points (which are fairly close together) from one of the trajectories; 15 of
these points are used during hyperparameter optimization, and to receive an estimate ˆEof the energy. The
remaining 185 points are used as test data to compare the performance of constrained and unconstrained GP,
both in terms of constraint fulfillment and in terms of RMSE with respect to the data.
Results for one individual sequence are shown in the rightmost plot of Figure 3. We observe that the
constrained GP is better at learning the precise shapes of the extrema of the velocity curves, although some
artefacts arise close to zero crossings due to inaccurately learned square values. For values close to zero, the
credible intervals of the unconstrained GP are often smoother and thinner than those of the constrained GP.
Averaging the results over 50 sequences chosen at random from the second half of the trajectories (with less
friction), the RMSE for the constrained GP is 0.31±0.14, whereas for the unconstrained GP it is 0.33±0.16.
9Published in Transactions on Machine Learning Research (11/2022)
In terms of constraint fulfillment, the constrained GP clearly performs better with |∆C|= 0.17±0.14as
compared to|∆C|= 0.91±0.61for the unconstrained GP. The values here are given plus-or-minus one
standard deviation.
5 Related Work
Several research projects have considered incorporating constraints into the GP; examples include boundary
conditions (Solin & Kok, 2019), inequality constraints (Veiga & A.Marrel, 2012; Maatouk & Bay, 2017) and
differential equation constraints (Jidling et al., 2017; Raissi et al., 2017; 2018). The recent review by Swiler
et al. (2020) provides a good overview of the existing literature on constrained GPs. So far, most of the efforts
have been concentrated on the single-task GP. The sum constraint, however, is qualitatively very different
from constraints on single-task GPs, in that it explicitly enforces a relationship between different outputs
instead of acting on individual outputs. Hence, in this section, we focus on works that consider constraints
on the outputs of multitask GPs.
Prior knowledge about vector fields have been imposed into GPs through special divergence-free and curl-free
kernels (Wahlström et al., 2013). Jidling et al. (2017) developed a more general method to include linear
operator constraints into the kernel of the GP; this is possible by using the property that GPs are closed
under linear transformations (Papoulis & Pillai, 2001) and relating the GP to a suitable latent GP, resembling
the use of potential functions in physics. See also Lange-Hegermann (2018) for a discussion of this approach
from a more mathematical perspective. Practical applications include modelling of electromagnetic fields
(Solin et al., 2018) and reconstruction of strain fields (Jidling et al., 2018; Hendriks et al., 2019b;a; 2020b).
Geist & Trimpe (2020) consider affine constraints on the dynamics of mechanical systems and construct a
GP satisfying Gauss’ principle of least constraint.
There is a connection between our method and the method by Jidling et al. (2017): while they do not consider
affine constraints, their approach can be extended to include those in the context of the constant linear sum
constraint (compare also Hendriks et al. (2020a), where the same idea is applied to neural networks). The two
works attack the problem from different angles: whereas Jidling et al. (2017) start by directly constructing
a covariance matrix out of vectors spanning the nullspace of the constraining operator, we start with the
covariance matrix and subsequently constrain it. More details on these parallels are given in Appendix E. An
advantage of our approach is that it is straightforward to include additional structure in the task kernel, such
as in (B.12). Furthermore, we consider the general case of nonconstant, nonlinear sum constraints.
Constructing kernels that are invariant with respect to certain symmetry transformations has proven fruitful
in the fields of atomic and molecular physics. Glielmo et al. (2017) consider GPs to model interatomic force
fields; they construct a ‘covariant kernel’ by including symmetries of the force, such as rotation and reflection.
Methods for constructing invariant kernels are given by Haasdonk & Burkhardt (2007), whereas Chmiela
et al. (2020) use a similar approach to construct a kernel that allows for simultaneous prediction of energies
and forces in molecules.
Pose estimation constitutes another area where constrained multitask GPs are of importance; in the case of
rigid pose estimation, the lengths are required to be constant. A method to explicitly enforce the constraints
during inference is given by Salzmann & Urtasun (2010b), whereas Salzmann & Urtasun (2010a) propose
a method to implicitly enforce the fixed-length constraint by learning transformed outputs in which the
constraint is linear. We followed this latter approach in the example with the rotated triangle in Section 4.2;
in addition to using the transformed outputs we also imposed the length constraint explicitly, which (at least
in principle) should allow for training points that do not fulfill the constraint exactly.
6 Conclusions and Future Work
We have derived a way of incorporating both linear and nonlinear sum constraints into multitask GPs. This
is achieved by learning transformed outputs and by conditioning the prior distribution of the GP on the
constraint. The toy problem of the harmonic oscillator demonstrated the potential of the method; it showed
that the constraint is fulfilled with high accuracy and that the constrained GP can mitigate detrimental
10Published in Transactions on Machine Learning Research (11/2022)
effects of noise or of incomplete measurements. Our experiment with the triangle in the plane showed that
the sum constraint improved the method by Salzmann & Urtasun (2010a) of including the length constraint
into pose estimation problems; so far, these results are particularly promising in the low-noise setting. The
results for the double pendulum dataset showed that our method also works well in case of real-world, noisy
data, given a way of estimating the constraint with sufficient accuracy.
In light of the results received for the triangle in the plane in Section 4.2, it appears as if it would be worth
investigating the applicability of this approach to pose estimation problems further; especially, in cases where
the approach by Salzmann & Urtasun (2010a) gives good results, our constrained GP could potentially
improve the performance. To increase the suitability of the approach for big datasets, combining the sum
constraint framework with methods such as sparse variational inference (Hensman et al., 2013) appears to
be a fruitful direction of inquiry. Finding general methods to incorporate constraints similar to the length
constraint (10)into the GP, where nonlinearities may depend on more than one of the outputs at once,
constitutes another interesting avenue of future research and would widen the range of possible applications.
Acknowledgements
The work is financially supported by the Swedish Research Council (VR) via the project Physics-informed
machine learning (registration number: 2021-04321) and by the Kjell och Märta Beijer Foundation .
References
Mauricio A. Alvarez, Lorenzo Rosasco, and Neil D. Lawrence. Kernels for vector-valued functions: a review.
arXiv:1106.6251 , 2012.
A. Asseman, T. Kornuta, and A. Ozcan. Learning beyond simulated physics. In Neural Information Processing
Systems, Modeling and Decision-making in the Spatiotemporal Domain Workshop , 2018.
David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians.
Journal of the American Statistical Association , 112(518):859–877, 2017.
E. V. Bonilla, K. Ming, A. Chai, and C. Williams. Multi-task Gaussian process prediction. In Advances in
Neural Information Processing Systems 20 , pp. 153–160, 2008.
S. Chmiela, H. E. Sauceda, A. Tkatchenko, and K. R. Müller. Accurate Molecular Dynamics Enabled by
Efficient Physically Constrained Machine Learning Approaches . Springer International Publishing, 2020.
J. R. Gardner, G. Pleiss, D. Bindel, K. Q. Weinberger, and A. G. Wilson. Gpytorch: Blackbox matrix-matrix
Gaussian process inference with GPU acceleration. In Advances in Neural Information Processing Systems
31, pp. 7576–7586, 2018.
A. R. Geist and S. Trimpe. Learning constrained dynamics with Gauss principle adhering Gaussian processes.
CoRR, abs/2004.11238, 2020.
A. Glielmo, P. Sollich, and A. De Vita. Accurate interatomic force fields via machine learning with covariant
kernels.Physical Review B , 95 (21), 2017.
B. Haasdonk and H. Burkhardt. Invariant kernel functions for pattern analysis and machine learning. Machine
Learning , 68:35–61, 2007.
J. Hendriks, Alex Gregg, Chris Wensrich, and Adrian Wills. Implementation of traction constraints in
Bragg-edge neutron transmission strain tomography. Strain, 55 (5), 2019a.
J. N. Hendriks, C. M. Wensrich, A. Wills, V. Luzin, and A. W. T Gregg. Robust inference of two-dimensional
strain fields from diffraction-based measurements. Nuclear Instruments and Methods in Physics Research
Section B: Beam Interactions with Materials and Atoms , 444:80–90, 2019b.
J. N. Hendriks, C. Jidling, A. Wills, and T. B. Schön. Linearly constrained neural networks. arXiv:2002.01600 ,
2020a.
11Published in Transactions on Machine Learning Research (11/2022)
J. N. Hendriks, C. M. Wensrich, and A. Wills. A Bayesian approach to triaxial strain tomography from
high-energy x-ray diffraction. Strain, 56 (3), 2020b.
James Hensman, Nicolò Fusi, and Neil D. Lawrence. Gaussian processes for big data. In Proceedings of the
Twenty-Ninth Conference on Uncertainty in Artificial Intelligence , pp. 282–290, 2013.
C. Jidling, N. Wahlström, A. Wills, and T. B. Schön. Linearly constrained Gaussian processes. In Advances
in Neural Information Processing Systems 31 , pp. 1216–1224, 2017.
C. Jidling, J. Hendriks, N. Wahlström, A. Gregg, T.B. Schön, C. Wensrich, and A. Wills. Probabilistic
modelling and reconstruction of strain. Nuclear Instruments and Methods in Physics Research Section B:
Beam Interactions with Materials and Atoms , 436:141–155, 2018.
M. Lange-Hegermann. Algorithmic linearly constrained Gaussian processes. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information
Processing Systems , volume 31. Curran Associates, Inc., 2018.
A. Lindholm, N. Wahlström, F. Lindsten, and T. B. Schön. Machine Learning - A First Course for Engineers
and Scientists . Cambridge University Press, 2021.
H. Maatouk and X. Bay. Gaussian process emulators for computer experiments with inequality constraints.
Mathematical Geosciences , 49:557–582, 2017.
D. J. C. MacKay. Introduction to Gaussian processes. In C. M. Bishop (ed.), Neural Networks and Machine
Learning , pp. 133–165. Springer, 1998.
R.MajumdarandS.Majumdar. Ontheconditionaldistributionofamultivariatenormalgivenatransformation
- the linear case. Heliyon, 5(e01136), 2019.
A. G. G. Matthews, J. Hron, R. E. Turner, and Z. Ghahramani. Sample-then-optimize posterior sampling for
Bayesian linear models. In NeurIPS Workshop on Advances in Approximate Bayesian Inference , 2017.
Thomas Minka. A family of algorithms for approximate Bayesian inference . PhD thesis, Masachusetts
Institute of Technology, 2001.
A. Papoulis and S. U. Pillai. Probability, Random Variables and Stochastic Processes . McGraw-Hill Education,
New York, 2001.
M. Perriollat, R. Hartley, and A. Bartoli. Monocular template-based reconstruction of inextensible surfaces.
International Journal of Computer Vision2 , 95:124–137, 2011.
M. Raissi, P. Perdikaris, and G. E. Karniadakis. Machine learning of linear differential equations using
Gaussian processes. Journal of Computational Physics , 348:683–693, 2017.
M. Raissi, P. Perdikaris, and G. E. Karniadakis. Numerical Gaussian processes for time-dependent and
nonlinear partial differential equations. SIAM Journal on Scientific Computing , 40(1):A172–A198, 2018.
C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning . MIT Press, 2006.
M. Salzmann and R. Urtasun. Implicitly constrained Gaussian process regression for monocular non-rigid
pose estimation. In Advances in Neural Information Processing Systems 23 , pp. 2065–2073, 2010a.
M. Salzmann and R. Urtasun. Combining discriminative and generative methods for 3D deformable surface
and articulated pose reconstruction. IEEE Conference on Computer Vision and Pattern Recognition , 2010b.
G. Skolidis and G. Sanguinetti. Bayesian multitask classification with Gaussian process priors. IEEE
Transactions on Neural Networks , 22(12), 2011.
E. Snelson, C.E. Rasmussen, and Z. Ghahramani. Warped Gaussian processes. In Advances in Neural
Information Processing Systems 16 , pp. 337–344, 2004.
12Published in Transactions on Machine Learning Research (11/2022)
A. Solin and M. Kok. Know your boundaries: Constraining Gaussian processes by variational harmonic
features. In Proceedings of Machine Learning Research2 , volume 89, pp. 2193–2202, 2019.
A. Solin, M. Kok, N. Wahlström, T. B. Schön, and S. Särkkä. Modeling and interpolation of the ambient
magnetic field by Gaussian processes. IEEE Transactions on Robotics , 34 (4):1112–1127, 2018.
Laura P. Swiler, Mamikon Gulian, Ari Frankel, Cosmin Safta, and John D. Jakeman. A survey of constrained
Gaussian process regression: Approaches and implementation challenges. Journal of Machine Learning for
Modeling and Computing , 1:119–156, 2020.
Michalis Titsias and Neil D. Lawrence. Bayesian Gaussian process latent variable model. In Yee Whye
Teh and Mike Titterington (eds.), Proceedings of the Thirteenth International Conference on Artificial
Intelligence and Statistics , volume 9 of Proceedings of Machine Learning Research , pp. 844–851, 2010.
Dustin Tran, Rajesh Ranganath, and David M. Blei. The variational Gaussian process. In 4th International
Conference on Learning Representations , 2016.
Jarno Vanhatalo, Pasi Jylänki, and Aki Vehtari. Gaussian process regression with Student-t likelihood. In
Advances in Neural Information Processing Systems , volume 22, 2009.
S. Da Veiga and A.Marrel. Gaussian process modeling with inequality constraints. In In Annales de la Faculté
des Sciences de Toulouse , volume 21, pp. 529– 555, 2012.
N. Wahlström, M. Kok, T. B. Schön, and F. Gustafsson. Modeling magnetic fields using Gaussian processes.
IEEE International Conference on Acoustics, Speech and Signal Processin , pp. 3522–3526, 2013.
Jared Willard, Xiaowei Jia, Shaoming Xu, Michael Steinbach, and Vipin Kumar. Integrating scientific
knowledge with machine learning for engineering and environmental systems. arXiv:2003.04919 , 2021.
C.K.I. Williams and D. Barber. Bayesian classification with Gaussian processes. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 20(12):1342–1351, 1998. doi: 10.1109/34.735807.
13Published in Transactions on Machine Learning Research (11/2022)
z0
vauxz v2 z
v
0 1 2 3 4 5 63
2
1
01
0 1 2 3 4 5 63
2
1
01234
0 1 2 3 4 5 63
2
1
01
t t t
Figure 4: Demonstration of the auxiliary variables approach for the free fall. The quantities z0,zandvaux,
vrefer to the position and velocity of the mass, respectively. We distinguish between z0,zandvaux,vto
emphasize that, while they aim to approximate the same curve, they are learned by different GPs. The
posterior means of the GPs are depicted, together with the 2-sigma credible intervals. The dotted lines
represent the true curves and the big dots/crosses correspond to the data available to the GPs. Left:Results
for the unconstrained GP are shown. For this example, the v-curve coincides with the auxiliary variable
vauxrequired for the constrained GP. Middle: The transformed outputs learned by the constrained GP are
depicted. The results for the auxiliary variable have been employed to create a virtual measurement for v2at
the zero crossing of vaux(differently colored square) in order to force the quadratic function towards zero.
Right:The backtransformed outputs of the constrained GP are shown, where the auxiliary output vauxhas
been used to recover the sign of v.
σn= 0.05 σn= 0.1 σn= 0.3
GP-c GP-u GP-c GP-u GP-c GP-u
fd= 0RMSE 1.9±0.5 2.3±0.4 3.2±1.1 4.7±1.2 10.1±3.6 13.2±3.3 (e-2)
|∆C|0.7±2.6 35.3±13.6 0.1±0.1 79.0±31.8 0.0±0.1 216.1±84.0 (e-2)
fd= 0.3RMSE 2.5±0.9 3.4±1.2 3.9±1.6 6.0±2.0 15.3±7.9 20.0±7.8 (e-2)
|∆C|0.4±0.8 47.5±26.8 0.1±0.1 93.7±35.1 0.7±2.1 286.9±123.8 (e-2)
Table 3: Comparison of the performance of the constrained GP (GP-c) and the unconstrained GP (GP-u)
for the free fall. Shown are the root mean squared error (RMSE) of the prediction as well as the mean
absolute violation of the constraint, |∆C|. The standard deviation of the noise is given by σnwhereasfdis
the probability with which output components have been omitted at random from the data. The values have
been obtained by averaging over 50 datasets and are given plus-or-minus one standard deviation. Bold font
highlights best performance.
A Additional Examples
In this Section, we take a look at some additional examples where the sum constraint can be applied. The
free fall dataset in Section A.1 is another example from physics, where one of the outputs enters linearly
into the constraint, instead of quadratically. The damped harmonic oscillator in Section A.2 constitutes a
variation of the harmonic oscillator toy example and demonstrates the case of a non-constant constraint. In
Section A.3, we investigate an example where the constraint includes different nonlinearities.
A.1 Free Fall
In addition to the harmonic oscillator (see Section 4.1), we investigated the simple example of a mass in
free fall as a second toy problem. Here, the output of the GP consists in position and velocity of the mass,
14Published in Transactions on Machine Learning Research (11/2022)
fT= [z,v], whereas the time tserves as input. Then the constraint takes the following form
F[f(t)] =mgz(t) +m
2v(t)2=Epot(t) +Ekin(t) =E. (A.1)
In terms of (6), we identify a1=mg,a2=m/2,h1(z) =z,h2(v) =v2andC=E. Hence, we receive for the
transformed outputs f′
1=f1=zandf′
2=v2. We choose the auxiliary output as f1
aux=v, which we use to
extract the sign when backtransforming f′
2and to create virtual measurements for f′
2at the zero crossings of
the posterior mean of f1
aux. In order to fit the transformed outputs of the GP, the observations yk= [zk,vk]T
are transformed analogously to obtain y′
k= [zk,v2
k,vk]T;vkis part of the constrained outputs, as this improves
the performance for this example. The virtual measurements are also included in the transformed data y′. In
terms of the transformed outputs the constraint can be written compactly as Ff′=C, where F= [a1,a2,0].
For more details on the free fall dataset, see Section C.3.
In Figure 4, results for both constrained and unconstrained GP, applied to the free fall dataset, are depicted.
When comparing the left and the right plot, it is apparent, that the constrained GP manages to mitigate
detrimental effects of both noise and incomplete measurements, where some of the observed output components
have been omitted at random, better than the unconstrained GP (compare area around peak of z0in the
figure). When looking at the 2σcredible intervals, we get the same picture as before for the harmonic
oscillator: the constrained GP can utilize higher certainty in one output and transfer it to the other one,
resulting in overall slimmer intervals. Close to the zero crossing of v, however, some artefacts are present due
to the piecewise, nonlinear backtransformation, which are absent for the unconstrained GP; furthermore,
confidence intervals are stretched a bit due to the backtransformation via the square root.
In Table 3, values for both the root mean squared error (RMSE) and the average absolute violation of the
constraint|∆C|are given for various noise levels σn, both with complete and incomplete measurements; in
case of incomplete measurements, the output components have been omitted at random with probability
fd= 0.3. The values have been obtained by averaging over 50 datasets. We observe that the constrained GP
fulfills the constraint with up to two orders of magnitude higher accuracy and also performs better in terms
of RMSE.
A.2 Damped Harmonic Oscillator
Next, we investigate a slight variation of the harmonic oscillator, the damped harmonic oscillator. The formal
treatment remains mostly unchanged and details can be found in Section 3.1.2 in the main paper; the main
difference is that damping has been added to the model of the oscillator. As a consequence, the energy is no
longer constant and the amplitude of the oscillation decays over time; see Section C.2 for more details. Hence,
this example constitutes an instance of the non-constant sum constraint F[f(t)] =E(t), where Algorithm 2
applies.
In Figure 5, results for both constrained and unconstrained GP are depicted. The findings are similar to
the undamped harmonic oscillator, and it is apparent that the constrained GP can mitigate the detrimental
effects of noisy or incomplete measurements better than the unconstrained GP. In Table 4, the performance
on 50 random datasets is evaluated. The outputs of the constrained GP fulfill the constraint with up to two
orders of magnitude higher accuracy than the unconstrained one, and also perform slightly better in terms
of RMSE. This example demonstrates that, given similar datasets, the performance of our method is very
similar, both in case of constant and non-constant constraints (compare Section 4.1).
A.3 Non-square Nonlinearity
Finally, we investigate an example where nonlinearities other than the square-nonlinearity are involved in the
constraint. We consider the outputs f=/bracketleftbigf1,f2/bracketrightbigT, on which we want to enforce the constraint
F[f(x)] = log(f1(x)) + sin(f2(x)) =C(x). (A.2)
In terms of (6), we identify a1= 1,a2= 1,h1(f1) =log(f1),h2(f2) =sin(f2)andC=C(x). Here, we assume
that the true value C(x)is known. Hence, we receive for the transformed outputs f′
1=log(f1)andf′
2=sin(f2).
15Published in Transactions on Machine Learning Research (11/2022)
0 2 4 6 8 101.0
0.5
0.00.51.01.5
z2v22E z v
t t tzaux vaux
0 2 4 6 8 100.25
0.000.250.500.751.001.251.501.75
0 2 4 6 8 101.0
0.5
0.00.51.0
Figure 5: Demonstration of the auxiliary variables approach for the damped harmonic oscillator. The
quantitieszaux,zandvaux,vrefer to the position and velocity of the damped harmonic oscillator, respectively.
We distinguish between zaux,zandvaux,vto emphasize that, while they aim to approximate the same
curve, they are learned by different GPs. The posterior means of the GPs are depicted, together with the 2σ
credible intervals. The dotted lines represent the true curves and the big dots/crosses correspond to the data
available to the GPs. Left:Results for the unconstrained GP are shown. For this example, these outputs
coincide with the auxiliary outputs required for the constrained GP. Middle: The transformed outputs
learned by the constrained GP are depicted, together with the constraint 2E(t) =kz2+mv2(wherek= 1
andm= 1). The results for the auxiliary outputs have been employed to create virtual measurements at zero
crossings (differently colored squares) in order to force the quadratic functions towards zero. Right:The
backtransformed outputs of the constrained GP are shown, where the auxiliary outputs zauxandvauxhave
been used to recover the signs.
σn= 0.05 σn= 0.1 σn= 0.3
GP-c GP-u GP-c GP-u GP-c GP-u
fd= 0RMSE 3.1±1.3 3.2±0.65.6±2.3 6.5±1.2 13.4±4.1 17.7±3.8 (e-2)
|∆C|0.0±0.0 2.4±0.60.0±0.0 5.1±1.1 0.1±0.1 13.3±3.3 (e-2)
fd= 0.2RMSE 4.1±2.3 4.8±2.26.2±2.6 7.8±2.120.7±11.4 23.5±7.1 (e-2)
|∆C|0.1±0.1 3.3±0.90.1±0.1 5.8±1.4 0.2±0.3 16.3±4.8 (e-2)
Table 4: Comparison of the performance of the constrained GP (GP-c) and the unconstrained GP (GP-u)
for the damped harmonic oscillator. Shown are the root mean squared error (RMSE) of the prediction as
well as the mean absolute violation of the constraint, |∆C|. The standard deviation of the noise is given
byσnwhereasfdis the probability with which output components have been omitted at random from the
data. The values have been obtained by averaging over 50 datasets and are given plus-or-minus one standard
deviation. Bold font highlights best performance.
We choose the auxiliary output as f1
aux=f2, which we use to disambiguate the backtransformation via the
arcsine, that is we keep track of how many multiples of ±π/2the output f1
auxhas crossed. We also use the
auxiliary output to create virtual measurements for f′
2at points where the posterior mean of f1
auxcrosses
multiples of±π/2, in order to reduce artefacts caused by the discontinuity in the backtransformation.
In Figure 6, results for both constrained and unconstrained GP are depicted. It is apparent that, while not
perfect, the constrained GP outperforms the unconstrained one. In Table 5, the results averaged over 50
datasets are given. We see, that the constrained GP outperforms the unconstrained one in terms of RMSE,
and it fulfills the constraint with up to 30 times higher accuracy.
16Published in Transactions on Machine Learning Research (11/2022)
f1
f2log(f 1) sin(f 2)
2Cf0
1
f0
2
1.0
0.5
0.0 0.5 1.0 1.5 2.04
3
2
1
012
1.0
0.5
0.0 0.5 1.0 1.5 2.03
2
1
01
1.0
0.5
0.0 0.5 1.0 1.5 2.04
3
2
1
012
x x x
Figure 6: Demonstration of the auxiliary variables approach for the example with non-square nonlinearity.
The quantities f0
1,f1andf0
2,f2refer to the same respective outputs of the GPs. We distinguish between f0
1,
f1andf0
2,f2to emphasize that, while they aim to approximate the same curve, they are learned by different
GPs. The posterior means of the GPs are depicted, together with the 2-sigma credible intervals. The dotted
lines represent the true curves and the big dots/crosses correspond to the data available to the GPs. Left:
Results for the unconstrained GP are shown. For this example, the f0
2-curve coincides with the auxiliary
outputfauxrequired for the constrained GP. Middle: The transformed outputs learned by the constrained
GP are depicted, together with the constraint C=log(f1) +sin(f2). The result for the auxiliary output has
been employed to create a virtual measurement at the point where fauxcrosses−π/2(differently colored
square). Right:The backtransformed outputs of the constrained GP are shown, where the auxiliary output
fauxhas been used to disambiguate the backtransformation via the arcsine.
σn= 0.05 σn= 0.1 σn= 0.15
GP-c GP-u GP-c GP-u GP-c GP-u
fd= 0RMSE 3.0±1.7 3.5±0.5 4.6±1.0 7.3±1.1 7.0±1.0 10.8±2.2 (e-2)
|∆C|1.3±2.3 5.9±1.4 0.9±0.5 13.9±4.8 0.5±0.5 18.0±6.3 (e-2)
fd= 0.2RMSE 3.1±1.0 8.6±9.9 5.6±1.8 11.7±8.8 8.4±2.1 16.2±7.3 (e-2)
|∆C|1.1±0.8 12.2±14.2 0.7±0.6 17.2±11.4 0.4±0.4 23.5±10.8 (e-2)
Table 5: Comparison of the performance of the constrained GP (GP-c) and the unconstrained GP (GP-u) for
the example with non-square nonlinearity. Shown are the root mean squared error (RMSE) of the prediction
as well as the mean absolute violation of the constraint, |∆C|. The standard deviation of the noise is given
byσnwhereasfdis the probability with which output components have been omitted at random from the
data. The values have been obtained by averaging over 50 datasets and are given plus-or-minus one standard
deviation. Bold font highlights best performance.
A.4 Comparison of approximation methods
In this section, we give a brief comparison of different methods of approximate inference at the example of
the harmonic oscillator. The approximation methods under consideration are the Laplace approximation B.3
and variational inference B.4.
Fig. 7 shows the predictive performance of the the unconstrained GP, variational inference and the Laplace
approximation. While it is apparent that both approximate GPs fulfill the constraint with high precision, the
variational approach tends to overfit to the data. On the other hand, overconfident credible intervals seem to
be less of an issue for the variational approach than for the Laplace approximation.
In Table 6, results obtained when averaging over 20 runs are given for different noise settings. It is apparent
that the constrained GP with Laplace approximation performs best. While the constrained GP utilizing
variational inference performs worst in terms of root-mean-square error, the constraint is still fulfilled with
high precision. In case of the variational approach, it might be possible to improve upon these results by
17Published in Transactions on Machine Learning Research (11/2022)
0 2 4 6 8 10
t1.5
1.0
0.5
0.00.51.01.5
0 2 4 6 8 10
t1.0
0.5
0.00.51.01.5
0 2 4 6 8 10
t1.0
0.5
0.00.51.01.5
zaux vaux z v z v
Figure 7: Comparison of the performance of the unconstrained GP ( Left), the variational approach ( Middle),
and the Laplace approximation ( Right) for the example of the harmonic oscillator. The quantities zaux,z
andvaux,vrefer to the position and velocity of the harmonic oscillator, respectively. The posterior means of
the GPs are depicted, together with the 2σcredible intervals. The dotted lines represent the true curves and
the big dots/crosses correspond to the data available to the GPs.
σn= 0.05 σn= 0.1
GP-c L GP-c var GP-u GP-c L GP-c var GP-u
fd= 0RMSE 2.4±0.7 3.8±0.6 3.2±0.44.2±1.1 7.0±1.6 6.3±0.9 (e-2)
|∆C|0.0±0.00.0±0.0 3.3±0.70.0±0.0 0.1±0.1 6.4±1.4 (e-2)
fd= 0.2RMSE 3.0±1.2 5.5±3.0 4.5±1.66.7±4.0 9.1±3.1 8.3±2.2 (e-2)
|∆C|0.0±0.0 0.2±0.3 4.2±0.90.1±0.2 0.4±0.6 8.1±1.7 (e-2)
Table 6: Comparison of the performance of the constrained GP with Laplace approximation (GP-c L), the
constrained GP using variational inference (GP-c var), and the unconstrained GP (GP-u) for the harmonic
oscillator. Shown are the root mean squared error (RMSE) of the prediction as well as the mean absolute
violation of the constraint, |∆C|. The standard deviation of the noise is given by σnwhereasfdis the
probability with which output components have been omitted at random from the data. The values have
been obtained by averaging over 20 datasets and are given plus-or-minus one standard deviation. Bold font
highlights best performance.
trying different parameterizations of the variational distribution, or by finding a better suited optimization
scheme.
B Technicalities
B.1 Background on GP Regression
In this section we give a very brief overview of some important GP regression formulas. For more detailed
accounts see e.g. Rasmussen & Williams (2006); Lindholm et al. (2021). Given the mean function m(·)and
kernelK(·,·)of the GP, the predictive distribution of the GP can be calculated by first constructing the joint
distribution between observations yand function values at test locations f∗,
/bracketleftbigg
y
f∗/bracketrightbigg
∼N/parenleftbigg/bracketleftbigg
m
m∗/bracketrightbigg
,/bracketleftbiggK+σ2
nI K∗
KT
∗ K∗∗/bracketrightbigg/parenrightbigg
, (B.1)
where m=m(X),m∗=m(X∗),K=K(X,X),K∗=K(X,X∗)andK∗∗=K(X∗,X∗).
18Published in Transactions on Machine Learning Research (11/2022)
Then, the conditional distribution f∗|X,y,X∗is constructed as follows:
f∗|X,y,X∗∼N/parenleftbig¯f∗,cov(f∗)/parenrightbig
,where (B.2a)
¯f∗∆=E[f∗|X,y,X∗]
=m∗+KT
∗[K+σ2
nI]−1(y−m), (B.2b)
cov(f∗) =K∗∗−KT
∗[K+σ2
nI]−1K∗. (B.2c)
The log-marginal likelihood, which is used for hyperparameter optimization, is given by
logp(y|X) =−1
2(y−m)T(K+σ2
nI)−1(y−m)
−1
2log|K+σ2
nI|−n
2log2π. (B.3)
B.2 Accommodating incomplete measurements
Throughout the paper, we often consider the case of incomplete measurements, i.e. data points where
measurements are available only for a subset of the tasks. This can be taken into account by considering
equation (B.1)and removing the the rows and columns on the right-hand side corresponding to missing
entries in y.
To make this more concrete, let us assume that the j-th entry of yis missing on the left-hand side of (B.1).
Then we also delete the j-th row of m,(K+σ2
nI)andK∗, as well as the j-th column of (K+σ2
nI)andKT
∗,
before explicitly constructing the joint distribution. We proceed analogously when training the constrained
GP on the transformed data y′.
B.3 Laplace approximation
The Laplace approximation can be employed when the noise distribution corresponding to the (transformed)
observations y′is non-Gaussian in order to obtain analytical expressions for the predictive equations and for
the log-marginal likelihood. Following again Rasmussen & Williams (2006), we approximate the posterior
p(f′|y′)∝p(y′|f′)p(f′)viap(f′|y′)≈q(f′|y′) =N/parenleftig
f′|ˆf′,(K−1+W)−1/parenrightig
, where
ˆf′=K(∇f′logp(y′|f′))|f′=ˆf′, (B.4a)
W=−∇f′∇f′logp(y′|f′)|f′=ˆf′. (B.4b)
Newton’s method is employed to iteratively determine ˆf′from (B.4a) via the update rule
f′new=f′−γ(∇f′∇f′Ψ(f′))−1∇f′Ψ(f′) (B.5)
=γm+ (1−γ)f′+γ/parenleftbig
(K−1+W)−1(∇f′logp(y′|f′) +W(f′−m)/parenrightbig
, (B.6)
where Ψ(f′) = logp(y′|f′) +logp(f′|X)and where γis the step size. In terms of these quantities, the
expressions (B.2) from the previous section become
¯f′
∗=m+KT
∗K−1(ˆf′−m), (B.7a)
cov(f′
∗) =K∗∗−KT
∗[K+W−1]−1K∗, (B.7b)
logp(y′|X) =−1
2(ˆf′−m)TK−1(ˆf′−m) + logp(y′|ˆf′)−1
2log(|K||K−1+W|). (B.7c)
For details on the derivation of these formulas, see Section 3.4 in Rasmussen & Williams (2006).
For the Laplace approximation, the likelihood py′(y′|f′)of the transformed data y′=h(y)has to be known.
We assume the original data yto be contaminated by Gaussian noise. In case of the square nonlinearity
19Published in Transactions on Machine Learning Research (11/2022)
where y′=y2, the likelihood is then given by the pdf of a noncentral chi-squared distribution. In the case of
arbitray nonlinearities hj, the likelihood can be obtained via
py′(y′|f′) =/productdisplay
ijpy′
ij(y′
ij|f′
ij) =/productdisplay
ijpyij/parenleftbig
h−1
j(y′
ij)|h−1
j(f′
ij)/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingledh−1
j(y′
ij)
dy′
ij/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle, (B.8)
where the indices iandjdenote data points and tasks, respectively.
There is no guarantee that Newton’s method will determine the correct maximum ˆf′in case of multimodal
distributions, or that the resulting Gaussian distribution will constitute a good approximation of the true
posterior. For these reasons, it has to be decided on a case by case basis whether the Laplace approximation
should be employed or not. Visual inspection of the GP predictions often gives a good idea on whether the
Laplace approximation performs well or not. In cases where it does not perform well, standard GP regression
might still produce reasonable results. Alternatively, different methods such as variational inference ( ?)
or expectation propagation (Minka, 2001) could be employed; the equations in (B.7)will then need to be
replaced by expressions corresponding to these techniques. A brief discussion on variational inference is given
in Appendix B.4, as well as a comparison with the Laplace approximation in Appendix A.4.
Throughout the paper, we used the Laplace approximation for the harmonic oscillator 4.1, the free fall A.1,
the damped harmonic oscillator A.2, and the example with non-square nonlinearity A.3. In case of the
double pendulum 4.3 and the triangle in the plane 4.2, we chose standard GP regression over the Laplace
approximation.
B.4 Variational Inference
As an alternative to the Laplace approximation (see previous section), variational inference (Blei et al., 2017;
Titsias & Lawrence, 2010) can be employed to approximate the posterior p(f′|y′)∝p(y′|f′)p(f′)when the
likelihoodp(y′|f′)is non-Gaussian. The idea is to approximate the posterior with the variational distribution
q(f′)∼N (f′|µq,Σq))and to learn the parameters µq,Σqby minimizing the Kullback-Leibler divergence
KL(q(f′)||p(f′|y′))between variational distribution and posterior. In order to ensure positive definiteness,
the entries of the covariance matrix Σqare not learned directly, but instead the entries of its Cholesky factor
Lq, where it holds that Σq=LqLT
q. Since an exact minimization of the KL divergence is intractable, the
evidence lower bound ELBO = logp(y′)−KL(q(f′)||p(f′|y′))≤logp(y′)is maximized in its stead.
The predictive equations in terms of the variational parameters are given by
¯f′
∗=m+KT
∗K−1(µq−m), (B.9a)
cov(f′
∗) =K∗∗+KT
∗K−1/parenleftbig
ΣqK−1T−I/parenrightbig
K∗, (B.9b)
and the ELBO can be rewritten in terms of numerically tractable, one-dimensional integrals
ELBO =Eq[log(p(y′|f′))]−KL(q(f′)||p(f′)) (B.10)
=/integraldisplay
log(p(y′|f′))q(f′)df′−KL(q(f′)||p(f′))
=/summationdisplay
i/integraldisplay
log(p(y′
i|f′
i))q(f′
i)df′
i−KL(q(f′)||p(f′)).
Equations B.9 are obtained analogously to (B.7), for a derviation of (B.10), see Titsias & Lawrence (2010).
The parameters µq,Σqof the variational distribution and the GP hyperparameters are determined jointly by
maximizing the ELBO, which we do by employing gradient descent. Same as for the Laplace approximation,
the likelihood for the transformed data p(y′|f′)is required when calculating the ELBO and can be obtained
via (B.8).
Inourexperiments, thevariationalapproachtendedtooverfittothedatamorethantheLaplaceapproximation.
It is possible that a different parameterization of the variational covariance matrix, or a different optimization
20Published in Transactions on Machine Learning Research (11/2022)
Algorithm 3 Constraining the GP - Special Case of Constant Task Interdependencies
Input:data mean md(·) = 1;data kernel kd(·,·); task mean µt; task covariance matrix Σt;
constraints (F,S);(transformed) data X,y′; points of prediction X∗
Output: constrained predictive distribution f′
∗|X,y′,X∗
Note:During hyperparameter optimization X∗={}and hence f′
∗={}
Step 1: UseF,Sto calculate constrained µ′
t,Σ′
taccording to (8b)
Step 2: Construct parameters m,Kof the (single task) joint prior distribution according to (B.1)
-omit noise term σ2
nI
Step 3: Useµ′
t,Σ′
t,m,Kto construct constrained (multi task) µ′
tot,Σ′
totaccording to (4)
Step 4: Remove entries in µ′
tot,Σ′
totcorresponding to incomplete measurements as detailed in Section B.2
ifHyperparameter optimization then
Step 5: Calculate the log marginal likelihood according to (B.7c)
Step 6: Perform optimization step
else ifPrediction then
Step 5: Calculate the predictive distribution f′
∗|X,y′,X∗according to (B.7a)
end if
scheme would manage to yield better results. In the paper, we went with the Laplace approximation over
variational inference; for a brief comparison of the two approaches at the example of the harmonic oscillator,
see Appendix A.4.
B.5 Kernel and Mean
Throughout the paper we use a radial basis function (RBF) kernel (also: squared exponential kernel) as data
kernel,
kRBF(x,x′) =σ2
fexp/parenleftbigg
−||x−x′||2
2l2/parenrightbigg
, (B.11)
whereσfis a scale factor and lis the length scale. We use the position independent index kernel provided by
gpytorch (Gardner et al., 2018),
kt=BBT+diag(v), (B.12)
where Bis a low-rank matrix and vis a non-negative vector; we chose the rank of Bto be equal to the
number of tasks of the GP in question. The parameters σf,l,Bandvare to be learned during the training
process. For more examples of possible kernels see e.g. Rasmussen & Williams (2006); MacKay (1998). The
Gram matrix is then constructed via the Kronecker product
Kf,f′(X,X′) =kRBF(X,X′)⊗kt. (B.13)
We chose constant mean functions for all outputs of the multitask GP. All the models have been implemented
in python with the library gpytorch (Gardner et al., 2018).
B.6 Special Case of Constant Constraints
In Section 3.2 in the main paper, we detailed the method for incorporating the sum constraint into the GP in
the general, non-constant case. Subsequently, in Section 3.2.1, we pointed out the possibility of implementing
the sum constraint in a more efficient way for the case, where all of the constraints are constant and where
the kernel of the GP factorizes into data and task kernel, as in (4)and(B.13). The main ideas are discussed
in the main paper, here we summarize the modified procedure in Algorithm 3. A proof that the factorization
holds also for the constrained GP is given in the next section.
B.6.1 Proof of factorization
In this section we provide formal proof that the claims made in Section 3.2.1 hold, i.e. that directly
constraining the mean and task covariance matrix and subsequently performing the Kronecker product with
the data mean and covariance matrix does indeed lead to the constrained GP from Section 3.2.
21Published in Transactions on Machine Learning Research (11/2022)
To start, let us summarize the objects involved:
Σtot=K⊗Σt (C.1)
µtot=m⊗µt
Ftot=INtot⊗F
Stot=1Ntot⊗S
Here, KandΣtdenote data and task covariance matrix, whereas mandµtdenote data and task mean,
respectively. FandSdefine the constraint at a single point. INtotdenotes identity matrix and 1Ntota vector
of only ones of dimension Ntot. The quantities with the totsubscript give the quantities that correspond to
the general approach from Section 3.2.
Due to the requirement of constant constraint and inter-task dependencies, we also need to pick a constant
data mean m, with entries a=const.We introduce the new quantity S′=S
a, which is used when constraining
the task mean and covariance matrix. With equation (8), we find the following:
Dtot= (FtotΣtotFT
tot)−1FtotΣT
tot (C.2)
=/parenleftbig
(INtot⊗F)(K⊗Σt)(INtot⊗F)T/parenrightbig−1(IN⊗F)(K⊗Σt)T
=/parenleftbig
K⊗(FΣtFT)/parenrightbig−1(KT⊗FΣT
t)
=/parenleftbig
K−1⊗(FΣtFT)−1/parenrightbig
(KT⊗FΣT
t)
=K−1KT⊗(FΣtFT)−1FΣtT
=INtot⊗D
Atot=INtot⊗INf−DT
totFtot (C.3)
=INtot⊗INf−(INtot⊗D)T(INtot⊗F)
=INtot⊗INf−INtot⊗DTF
=INtot⊗(INf−DTF)
=INtot⊗A
µ′
tot=Atotµtot+DT
totStot (C.4)
= (INtot⊗A)(m⊗µt) + (INtot⊗D)T(1Ntot⊗S)
=m⊗Aµt+1Ntot⊗DTS
=m⊗(Aµt+DTS′)
=m⊗µ′
t
Σ′
tot=AtotΣtotAT
tot (C.5)
= (INtot⊗A)(K⊗Σt)(INtot⊗A)T
=K⊗(AΣtAT)
=K⊗Σ′
t
Hence we have shown that µ′
totandΣ′
totof the constrained GP factorize into Kronecker products between
the data mean and covariance matrix and the constrained task mean and covariance matrix, respectively.
22Published in Transactions on Machine Learning Research (11/2022)
B.7 Credible Intervals
While standard deviation and variance for the backtransformed outputs fcannot be recovered via a simple
backtransformation of the corresponding quantities of f′, due to the potentially nonlinear and piecewise
backtransformation, it is possible to recover credible intervals in this way: for the transformed outputs f′,
we generate the upper and lower bounds of the 2σcredible interval; subsequently those bounds can be
backtransformed in the same way as we do for the mean of the GP. That means that the posterior of the
constrained fcan be a bit skewed, i.e. the mean may not lie exactly in the middle between upper and lower
credible interval. When auxiliary outputs are involved in the backtransformation, their respective means
should be used also when recovering the credible intervals, for the results to be consistent with the posterior
means.
B.8 Training Procedure
The models have been trained using the Adam optimizer provided by gpytorch. For each experiment, the
corresponding learning rate (lr), number of iterations (iter) and (if applicable) scheduler settings are given in
Table 7. The scheduler multiplies the learning rate with s-factor after s-steps iterations. The two different
scheduler parameters given for the double pendulum correspond to the constrained and the unconstrained
GP, respectively.
During the training of all datasets, we checked for errors in the Cholesky decomposition, which can happen
when a matrix becomes singular due to numerical errors; when that happened, hyperparameter optimization
was restarted with a new random initialization. For the non-square nonlinearity (logsin) experiment, training
of the constrained GP proved to be less stable than for the other datasets. To counteract the issue, we
tested for two further failure modes of the GP. First, we checked the learned lengthscale of the GP; if it was
unreasonably small (smaller than 0.1), the training was repeated. Very small lengthscales typically correspond
to the case where the GP learns an almost constant function with spikes towards all of the training points.
Secondly, we confirmed that gradient descent had actually converged during training: to this end, we took
the loss values over the last 40 iterations and checked, whether the standard deviation was smaller than 0.1.
If either of the two checks failed, the training was repeated with newly initialized hyperparameters.
lr iter s-steps s-factor
HO (Sec. 4.1) 0.1 200 100 0.5
Triangle (Sec. 4.2) 0.1 2000 800 0.2
DP (Sec. 4.3) 0.1 2000 800/500 0.2/0.5
Free fall (A.1) 0.1 200 100 0.5
Damped HO (A.2) 0.1 200 100 0.5
Non-square (A.3) 0.1 200 100 0.5
Table 7: Training parameters for the experiments.
B.9 Computing Power Available for the Experiments
All experiments have been conducted on a system with NVIDIA GTX 1060, 6GB GPU, Intel Core i7 7700-K
@ 4.2GHz CPU and 16GB RAM. The creation of average values for one set of parameters as displayed in
Tables 1-4 typically took between 15 minutes and three hours.
C Details on Simulated Datasets
In this section we provide information on how the data used in the different simulation experiments was
generated.
23Published in Transactions on Machine Learning Research (11/2022)
C.1 Harmonic Oscillator
The data for the harmonic oscillator toy problem was generated from
z(t) =z0sin(ω0t), (C.1a)
v(t) =z0ω0cos(ω0t). (C.1b)
The energy is given by
E=k
2z(t)2+m
2v(t)2= (C.2)
=k
2z2
0sin2(ω0t) +m
2z2
0ω2
0cos2(ω0t) =k
2z2
0. (C.3)
We have chosen E= 0.8 J,m= 1 kg,ω0= 1 s−1and it holds that k=mω2
0andz0=/radicalbig
2E/k.
Training data has been generated by evaluating the function on the equally spaced grid t∈linspace(0,10,20) [ s].
Subsequently, random noise ϵ∼N(0,σ2
n)was added to the data and output components were omitted at
random with probability fd; the values for σnandfdare given in Table 1 in the main text. Test data has
been generated on the grid t∈linspace(-0.1,10,100) [ s].
C.2 Damped Harmonic Oscillator
The data for the damped harmonic oscillator was generated from
z(t) =z′
0(t) sin(ωt), (C.4a)
v(t) =z′
0(t)ωcos(ωt)−z′
0(t)b
2msin(ωt), (C.4b)
wherez′
0(t) =z0exp/parenleftbig−bt
2m/parenrightbig
andω=/radicalig
ω2
0−/parenleftbigb
2m/parenrightbig2. The energy is given by
E(t) =k
2z(t)2+m
2v(t)2, (C.5)
which is now time dependent and no longer yields a constant expression.
We have chosen E= 0.8 J,m= 1kg,ω0= 1 s−1,b= 0.1kgs−1and it holds that k=mω2
0andz0=/radicalbig
2E/k.
Training data has been generated by evaluating the function on the equally spaced grid t∈linspace(0,10,20) [ s].
Subsequently, random noise ϵ∼N(0,σ2
n)was added to the data and output components were omitted at
random with probability fd; the values for σnandfdare given in Table 4. Test data has been generated on
the gridt∈linspace(-0.1,10,100) [ s].
C.3 Free Fall
The data for the free fall was generated from
z(t) =v0t−g
2t2, (C.6a)
v(t) =v0−gt. (C.6b)
The energy is given by
E=mgz(t) +m
2v(t)2=m
2v2
0. (C.7)
We have chosen E= 200 J,m= 1kgand it holds that v0=/radicalbig
2E/m, and the gravitational acceleration
on earth is g= 9.81 m s−2. Training data has been generated by evaluating the function on the equally
spaced grid t∈linspace(0,6,20) [ s]. Subsequently, random noise ϵ∼N(0,σ2
n)was added to the data and
output components were omitted at random with probability fd; the values for σnandfdare given in Table
3. To ensure good visibility and learnability, we scaled the data ywith a factor a= 20:y→y/a. Both in
Figure 4 and in Table 3, the results are given in terms of the rescaled data (and noise values); the results in
terms of the original scale can be obtained by multiplying with a. Test data has been generated on the grid
t∈linspace(-0.1,6,100) [ s].
24Published in Transactions on Machine Learning Research (11/2022)
C.4 Non-square Nonlinearity
The data for the experiment with non-square nonlinearities was generated from
f1(x) = 2e−5(x−1)2+e−5(x+1)2+ 0.2, (C.8)
f2(x) =−x3
2. (C.9)
Training data has been generated on the equally spaced grid x∈linspace(-1.2,2,20) . Subsequently, random
noiseϵ∼N(0,σ2
n)was added to the data and output components were omitted at random with probability fd;
the values for σnandfdare given in Table 5. Test data has been generated on the grid t∈linspace(-1.2,2,100) .
C.5 Triangle in the Plane
In terms of the parameter α, the trajectory that we used for the triangle in the plane in Section 4.2 is given
by
Z0=/bracketleftbigg4 8 8.4
4 4 6/bracketrightbigg
, (C.10a)
Z1=Z0+d(α), (C.10b)
Z=R(α)Z1+d(α), (C.10c)
where each column of the matrix Zcontains the coordinates of one corner point of the triangle, and where
d(α) =1
2cos(2α)andR(α)is a rotation matrix. Subsequently, random noise ϵ∼N(0,σ2
n)was added to Z;
the values for σnare given in Table 2 in the main text. We then added an auxiliary point of known position
(4,4)to each datapoint, which will be important for the backtransformation:
Z=/bracketleftbigg
z1xz2xz3x4
z1yz2yz3y4/bracketrightbigg
. (C.11)
Following the approach from Salzmann & Urtasun (2010a), we constructed the matrix Q=ZTZand used
the upper triangular elements of Qas transformed outputs for the constrained GP:
y′= [Q11,Q12,Q13,Q14,Q22,Q23,Q24,Q33,Q34,Q44]. (C.12)
Then the matrix Fand the corresponding vector Sencoding the length constraints for all the edges of the
triangle become
F=
1−2 0 0 1 0 0 0 0 0
1 0−2 0 0 0 0 1 0 0
0 0 0 0 1 −2 0 1 0 0
0 0 0 0 0 0 0 0 0 1
, (C.13)
S=/bracketleftbigL2
12L2
13L2
23L2
04/bracketrightbigT, (C.14)
whereLijdenote the distances between the points iandj. The the last row of Fcorresponds to the constraint
on the distance between the auxiliary point and the origin of the coordinate system. Note, that Q14,Q24and
Q34could in principle be learned separately from the remaining transformed outputs, as they do not enter
into any of the constraints and the corresponding columns in (C.13)are zero. Furthermore, Q44could be
omitted from the learning process entirely, as the value is known.
After training the constrained GP, the predicted values f′are rearranged into the (symmetric) matrix /tildewideQ,
analogously to (C.12). Then the matrix /tildewideZis recovered via a singular value decomposition (SVD) of /tildewideQ. This
decomposition is not unique and the auxiliary point comes into play: we compare the learned with the known
position and determine the angle between them, which enables us to rotate the learned coordinates to their
true positions.
Training data was generated on the grid α∈[0,5], consisting of 20uniformly spaced points. Subsequently,
random noise ϵ∼N(0,σ2
n)was added to the data; the values for σnare given in the main text. Test data
was generated over the same range [0,5], although this time with the grid divided into 100points.
25Published in Transactions on Machine Learning Research (11/2022)
0 250 500 750 1000 1250 1500 1750 20005
051015202530
0 250 500 750 1000 1250 1500 1750 20005
05101520
E Ekin Epot400 Hz 500 Hz
Figure 8: Comparison of the energy of the double pendulum for different frame rates of the camera. It
is apparent that a frame rate of 400Hzis incompatible with the principle of energy conservation; while
the energy is decreasing in the long term due to friction, it should never increase. No choice of mass ratio
mb/mgwas able to resolve this issue. On the other hand, a frame rate of 500Hztogether with the mass ratio
mb/mg= 6.5is compatible with energy conservation, within the bounds of error.
D Details on the Double Pendulum Dataset
D.1 Parameters
In Section 4.3 we demonstrated the applicability of our approach to the ‘Double Pendulum Chaotic’ dataset.
A description of the dataset can be found in Asseman et al. (2018); to prevent confusion, we should mention
that the blue and the green marker in our paper correspond to the green and the blue marker in Asseman
et al. (2018), respectively (i.e. the colors have been exchanged). The lengths of the two pendula are given as
lb= 91 mmandlg= 70 mm, where the subfix brefers to the pendulum with blue marker and gto the one
with green marker. However, in order to calculate the energy (up to a constant factor), knowledge of the
masses, or at least of the ratio mb/mgis required. From information given by the authors of the paper and
the manufacturer of the double pendulum, together with some experimentation of our own we estimated this
ratio asmb/mg≈6.5. Note that in our description of the double pendulum (11), we made the assumption
that it consists of two point masses, which is only approximately true.
Another quantity of interest is the frame rate of the camera that was used to create the dataset; it enters into
the model when calculating the velocities of the masses. In their paper (Asseman et al., 2018), the authors
state a frame rate of 400Hz. However, our experiments with the dataset and keeping the energy constraint in
mind strongly indicate a frame rate of 500Hz; for 400Hzthere are segments of the motion where the total
energyEclearly increases which violates the principle of energy conservation (see Figure 8).
The ‘Double Pendulum Chaotic’ dataset was published under the “Community Data License Agreement -
Sharing - Version 1.0”.
D.2 Implementation Details
The ‘Double Pendulum Chaotic’ dataset provides data in the form of annotated positions of the masses
attached to the ends of the two pendula (together with the position of the top of the apparature holding the
pendulum which does not change and which we therefore omitted). We now have the positions as points on
an equally spaced grid; in terms of the camera frame rate rthe spacing between two adjacent points is given
by1/r. To obtain the velocities we numerically take the gradient of the positions on the grid and we receive
the data which we use for our GP, with outputs
f= [zbx,zby,zgx,zgy,vbx,vby,vgx,vgy]T. (D.1)
To obtain positions and velocities with comparable absolute values, which enhances the performance of the
GP and which makes the quantities easier to compare in plots, we scaled positions by a factor of 20and
velocities by a factor of√
10; the timetwas scaled by a factor of 5.
As outlined in Section 4.3, we obtain training data, to be used during hyperparameter optimization, and
test data, to evaluate the quality of predictions, by picking a random interval of 200 datapoints from the
26Published in Transactions on Machine Learning Research (11/2022)
second half of the trajectories provided by the dataset; out of those we use 15 points as training data and
the rest as test data. Note that the value ˆEreceived by evaluating (11)and averaging over the training
data will in general be a less accurate estimate than the value of the energy ˆE0received when averaging over
all datapoints in the interval, since the average is taken over fewer points in the former case. Hence, when
determining the accuracy of the constraint fulfillment, the results in Section 4.3 have been compared to ˆE0.
For the double pendulum, we receive the transformed outputs
f′= [zby,zgy,v2
bx,v2
by,v2
gx,v2
gy]T, (D.2)
with corresponding
F= [mbg,m gg,mb
2,mb
2,mg
2,mg
2]. (D.3)
The auxiliary outputs are
faux= [zbx,zgx,vbx,vby,vgx,vgy]T; (D.4)
note, that the outputs zbxandzgxare not actually auxiliary outputs, but since they are not involved in the
constraint (11)(i.e. the corresponding entry in Fwould be zero), they can be learned separately from the
constrained outputs, together with the auxiliary outputs. Same as for the harmonic oscillator, we created
virtual measurements for v2
bx,v2
by,v2
gx,v2
gyat zero crossings of the auxiliary outputs vbx,vby,vgx,vgy.
E Comparison to Jidling et al. (2017)
In this section we will investigate the parallels between the method of Jidling et al. (2017) and our own. While
they only consider homogeneous constraints in the paper, in the context of constant linear sum constraints it
is simple to extend the method to affine constraints, as we will see below.
In the approach of Jidling et al. (2017), vectors spanning the nullspace of the constraint Fare used to
construct the task covariance matrix. Given a sum constraint F(f) =Ff=S, and vectors hispanning the
nullspace (i.e. Fhi= 0), we can define the matrix
G= [h1h2...hn], (E.1)
wherendenotes the dimension of the nullspace. A suitable task covariance matrix can then be constructed
viakt=GGT, and in order to accommodate the non-zero right hand side of the sum constraint, the task
mean mtis chosen such that Fmt=S. Then we obtain the multivariate Gaussian N(mt,kt), samples of
which obey the constraint F. Note that ktis the projector on the nullspace of the constraint; in Matthews
et al. (2017), the relationship between constrained multivariate Gaussian distributions and the nullspace of
the corresponding linear operator is discussed. Subsequently, the full mean and covariance matrix of the GP
can be constructed according to (4).
To make this more concrete, we consider again the example of the harmonic oscillator from Section 3.1.2.
Here, the constraint is given by F= [k/2,m/2,0,0]andS=E. Then the corresponding matrix Gcan be
constructed as (the choice of null vectors is not unique)
G=
m√
m2+k20 0
−k√
m2+k20 0
0 1 0
0 0 1
. (E.2)
The task mean and task covariance matrix become
kt=GGT=
m2
m2+k2−mk
m2+k20 0
−mk
m2+k2k2
m2+k20 0
0 0 1 0
0 0 0 1
, mt=
E
kE
m
0
0
. (E.3)
27Published in Transactions on Machine Learning Research (11/2022)
Now if we approach the problem from the other side, as it turns out, starting with the identity matrix as task
covariance matrix and then conditioning it according to (8)leads to the same ktobtained in (E.3). Here we
note one advantage of our approach: it is straightforward to include additional correlations into the task
covariance matrix, e.g. correlations between constrained outputs and those not involved in the constraint.
For example, when introducing an additional correlation between tasks one and three we obtain (after setting
m=k= 1)
k0
t=
1 0 0.5 0
0 1 0 0
0.5 0 1 0
0 0 0 1
−→kt=
0.5−0.5 0.25 0
−0.5 0.5−0.25 0
0.25−0.25 0.875 0
0 0 0 1
, (E.4)
where k0
tandktare the unconstrained and the constrained task covariance matrix, respectively. So the
constraint alters correlations between outputs involved in the constraint and other outputs. In the approach
of Jidling et al. (2017), correlations between the nullspace dimensions could be added by introducing a
non-diagonal matrix between GTandGin(E.3). However, that method would not allow us to introduce
arbitrary correlations between the tasks as demonstrated in (E.4). Throughout the work, we have used the
structure given in (B.12) as the starting point for our task covariance matrices.
28