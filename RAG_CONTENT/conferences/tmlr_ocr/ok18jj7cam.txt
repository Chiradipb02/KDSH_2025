Published in Transactions on Machine Learning Research (11/2023)
GraphPNAS: Learning Probabilistic Graph Generators for
Neural Architecture Search
Muchen Li muchenli@cs.ubc.ca
University of British Columbia
Vector Institute for AI
Jeﬀrey Liu jeﬀrey.yunfan.liu@uwaterloo.ca
University of Waterloo
Leonid Sigal lsigal@cs.ubc.ca
University of British Columbia
Vector Institute for AI
Canada CIFAR AI Chair
NSERC CRC Chair
Renjie Liao rjliao@ece.ubc.ca
University of British Columbia
Vector Institute for AI
Canada CIFAR AI Chair
Reviewed on OpenReview: https: // openreview. net/ forum? id= ok18jj7cam
Abstract
Neural architectures can be naturally viewed as computational graphs. Motivated by this
perspective, we, in this paper, study neural architecture search (NAS) through the lens of
learning random graph models. In contrast to existing NAS methods which largely focus
on searching for a single best architecture, i.e., point estimation, we propose GraphPNAS ,
a deep graph generative model that learns a distribution of well-performing architectures.
Relying on graph neural networks (GNNs), our GraphPNAS can better capture topologies
of good neural architectures and relations between operators therein. Moreover, our graph
generator leads to a learnable probabilistic search method that is more ﬂexible and eﬃcient
than the commonly used RNN generator and random search methods. Finally, we learn
our generator via an eﬃcient reinforcement learning formulation for NAS. To assess the
eﬀectiveness of our GraphPNAS, we conduct extensive experiments on three search spaces,
including the challenging RandWire on Tiny-ImageNet, ENAS on CIFAR10, and NAS-
Bench-101/201. The complexity of RandWire is signiﬁcantly larger than other search spaces
in the literature. We show that our proposed graph generator consistently outperforms
RNN based one and achieves better or comparable performances than state-of-the-art NAS
methods. We open source our code here: https://github.com/DSL-Lab/GraphPNAS .
1 Introduction
In recent years, we have witnessed a rapidly growing list of successful neural architectures that underpin deep
learning, e.g., VGG, LeNet, ResNets ( He et al. ,2016), Transformers ( Dosovitskiy et al. ,2020). Designing
these architectures requires researchers to go through time-consuming trial and errors. Neural architecture
search (NAS) ( Zoph & Le ,2016;Elsken et al. ,2018b ) has emerged as an increasingly popular research area
which aims to automatically ﬁnd state-of-the-art neural architectures without human-in-the-loop.
NAS methods typically have two components: a search module and an evaluation module. The search
module is expressed by a machine learning model, such as a deep neural network, designed to operate in a
high dimensional search space. The search space, of all admissible architectures, is often designed by hand in
1Published in Transactions on Machine Learning Research (11/2023)
advance. The evaluation module takes an architecture as input and outputs the reward, e.g., performance of
this architecture trained and then evaluated with a metric. The learning process of NAS methods typically
iterates between the following two steps. 1) The search module produces candidate architectures and sends
them to the evaluation module; 2) The evaluation module evaluates these architectures to get the reward
and sends the reward back to the search module. Ideally, based on the feedback from the evaluation module,
the search module should learn to produce better and better architectures. Unsurprisingly, this learning
paradigm of NAS methods ﬁts well to reinforcement learning (RL).
Most NAS methods ( Liu et al. ,2018b ;White et al. ,2020;Cai et al. ,2019) only return a single best archi-
tecture ( i.e., a point estimate) after the learning process. This point estimate could be very biased as it
typically underexplores the search space. Further, a given search space may contain multiple (equally) good
architectures, a feature that a point estimate cannot capture. Even worse, since the learning problem of
NAS is essentially a discrete optimization where multiple local minima exist, many local search style NAS
methods ( Ottelander et al. ,2020) tend to get stuck in local minima. From the Bayesian perspective, mod-
elling the distribution of architectures is inherently better than point estimation, e.g., leading to the ability
to form ensemble methods that work better in practice. Moreover, modelling the distribution of architec-
tures naturally caters to probabilistic search methods which are better suited for avoiding local optima, e.g.,
simulated annealing. Finally, modeling the distribution of architectures allows to capture complex structural
dependencies between operations that characterize good architectures capable of more eﬃcient learning and
generalization.
Motivated by the above observations and the fact that neural architectures can be naturally viewed as
attributed graphs, we propose a probabilistic graph generator which models the distribution over good
architectures using graph neural networks (GNNs). Our generator excels at generating topologies with com-
plicated structural dependencies between operations. From the Bayesian inference perspective, our generator
returns a distribution over good architectures, rather than a single point estimate, allowing to capture the
multi-modal nature of the posterior distribution of good architectures and to eﬀectively average or ensemble
architecture (sample) estimates. Diﬀerent from the Bayesian deep learning ( Neal,2012;Blundell et al. ,2015;
Gal & Ghahramani ,2016) that models distributions of weights/hidden units, we model distributions of neu-
ral architectures. Lastly, our probabilistic generator is less prone to the issue of local minima, since multiple
random architectures are generated at each step during learning. In summary, our key contributions are as
below.
•We propose a GNN-based graph generator for neural architectures which empowers a learnable
probabilistic search method. To the best of our knowledge, we are the ﬁrst to explore learning deep
graph generative models as generators in NAS.
•We explore a signiﬁcantly larger search space ( e.g., graphs with 32 operators) than the literature
(e.g., garphs with up to 12 operators) and propose to evaluate architectures under low-data regime,
which altogether boost eﬀectiveness and eﬃciency of our NAS system.
•Extensive experiments on three diﬀerent search spaces show that our method consistently out-
performs RNN-based generators and is slightly better or comparable to the state-of-the-art NAS
methods. Also, it can generalize well across diﬀerent NAS system setups.
2 Related Works
Neural Architecture Search. The main challenges in NAS are 1) the hardness of discrete optimization,
2) the high cost for evaluating neural networks, and 3) the lack of principles in the search space design.
First, to tackle the discrete optimization, evolution strategies (ES) ( Elsken et al. ,2019;Real et al. ,2019),
reinforcement learning (RL) ( Baker et al. ,2017;Zhong et al. ,2018;Pham et al. ,2018b ;Liu et al. ,2018a ),
Bayesian optimization ( Bergstra et al. ,2013;White et al. ,2019) and continuous relaxations ( Liu et al. ,2018b )
have been explored in the literature. We follow the RL path as it is principled, ﬂexible in injecting prior
knowledge, achieves the state-of-the-art performances ( Tan & Le ,2019), and can be naturally applied to our
graph generator. Second, the evaluation requires training individual neural architectures which is notoriously
time consuming( Zoph & Le ,2016).Pham et al. (2018b );Liu et al. (2018b ) propose a weight-sharing supernet
2Published in Transactions on Machine Learning Research (11/2023)
to reduce the training time. Baker et al. (2018) use a machine learning model to predict the performance of
fully-trained architectures conditioned on early-stage performances. Brock et al. (2018);Zhang et al. (2018)
directly predict weights from the search architectures via hypernetworks. Since our graph generator do not
relies on speciﬁc choice of evaluation method, we choose to experiment on both oracle training(training from
scratch) and supernet settings for completeness. Third, the search space of NAS largely determines the
optimization landscape and bounds the best-possible performance. It is obvious that the larger the search
space is, the better the best-possible performance and the higher the search cost would likely be. Besides this
trade-oﬀ, few principles are known about designing the search space. Previous work ( Pham et al. ,2018b ;Liu
et al. ,2018b ;Ying et al. ,2019;Li et al. ,2020) mostly focuses on cell-based search space. A cell is deﬁned
as a small ( e.g., up to 8 operators) computational graph where nodes ( i.e., operators like 3 ×3 convolution)
are connected following some topology. Once the search is done, one often stacks up multiple cells with the
same topology but diﬀerent weights to build the ﬁnal neural network. Other works ( Tan et al. ,2019;Cai
et al. ,2019;Tan & Le ,2019) typically ﬁx the topology, e.g., a sequential backbone, and search for layer-wise
conﬁgurations ( e.g., operator types like 3 ×3 vs. 5 ×5 convolution and number of ﬁlters). In our method,
to demonstrate our graph generator’s ability in exploring large topology search space, we ﬁrst explore on a
challenging large cell space (32 operators), after which we experiment on ENAS Macro ( Pham et al. ,2018b )
and NAS-Benchmark-101( Ying et al. ,2019) for more comparison with previous methods.
Neural Architecture as Graph for NAS. Recently, a line of NAS research works propose to view neural
architectures as graphs and encode them using graph neural networks (GNNs). In ( Zhang et al. ,2020;Luo
et al. ,2018a ), graph auto-encoders are used to map neural architectures to and back from a continuous space
for gradient-based optimization. Shi et al. (2020) use bayesian optimization (BO), where GNNs are used to
get embedding from neural architectures. Despite the extensive use of GNNs as encoders, few works focus
on building graph generative models for NAS. Closely related to our work, Xie et al. (2019) explore diﬀerent
topologies of the similar cell space using non-learnable random graph models. You et al. (2020) subsequently
investigate the relationship between topologies and performances. Following this, Ru et al. (2020) propose
a hierarchical search space modeled by random graph generators and optimize hyper-parameters using BO.
They are diﬀerent from our work as we learn the graph generator to automatically explore the cell space.
Deep Graph Generative Models. Graph generative models date back to the ErdsRényi model ( Erdös
& Rényi ,1959), of which the probability of generating individual edges is the same. Other well-known
graph generative models include the stochastic block model ( Holland et al. ,1983), the small-world model
(Watts & Strogatz ,1998), and the preferential attachment model ( Barabási & Albert ,1999). Recently,
deep graph generative models instead parameterize the probability of generating edges and nodes using deep
neural networks in, e.g., the auto-regressive fashion ( Li et al. ,2018;You et al. ,2018;Liao et al. ,2019) or
variational autoencoder fashion ( Kipf & Welling ,2016;Grover et al. ,2018;Liu et al. ,2019). These models
are highly ﬂexible and can model complicated distributions of real-world graphs, e.g., molecules ( Jin et al. ,
2018), road networks ( Chu et al. ,2019), and program structures ( Brockschmidt et al. ,2018). Our graph
generator builds on top of the state-of-the-art deep graph generative model in ( Liao et al. ,2019) with several
important distinctions. First, instead of only generating nodes and edges, we also generate node attributes
(e.g., operator types in neural architectures). Second, since good neural architectures are actually latent,
our learning objective maximizes the expected reward ( e.g., validation accuracies) rather than the simple
log likelihood, thus being more challenging.
Graph Generative models for Neural Architectural Search Closely related to our work, there is a line
of research that learns to generate graph architecture for neural architecture search.In the prior paradigm,
graph generative networks largely relied upon Variational Auto Encoders (VAEs). NAO( Luo et al. ,2018b )
employed an LSTM-based VAE, synchronized with performance prediction for gradient-based architectural
optimization. In contrast, Arch2Vec( Yan et al. ,2020) was designed to translate the graph representation of
neural architecture into an implicit vector using GraphVAE, aiming for its optimization through Bayesian
Optimization. AG-Net( Lukasik et al. ,2022), a more recent advent, originated a generative network from the
VAE decoder, allied with a surrogate model to expedite eﬃcient learning. To extract a sample from AG-Net,
an initial random sample is obtained from the architectural representation space, after which a decoder is
utilized to translate the sample into a graph representation. Our methodology diﬀers signiﬁcantly, utilizing
an auto-regressive graph generator, wherein the generation of the graph mirrors a Markov decision process.
3Published in Transactions on Machine Learning Research (11/2023)
Figure 1: Figure (a) is the pipeline of our NAS system. The core part is a GNN-based graph generator from
which we sample graph representations of neural network G. The corresponding model for each Gis then
sent to the evaluator for evaluation. The evaluation result is ﬁrst stored in a replay buﬀer and then used for
learning the graph generator through Reinforcement Learning.Figure(b) shows one generation step in the
proposed probabilistic graph generator.
3 Methods
The architecture of any feedforward neural network can be naturally represented as a directed acyclic graph
(DAG), a.k.a., computational graph . There exist two equivalent ways to deﬁne the computational graph.
First, we denote operations ( e.g., convolutions) as nodes and denote operands ( e.g., tensors) as edges which
indicate how the computation ﬂows. Second, we denote operands as nodes and denote operators as edges.
We adopt the ﬁrst view. In particular, a neural network Gwith Noperations is deﬁned as a tuple (A, X )
where A∈ {0,1}N×Nis an N×Nadjacent matrix with Aij= 1 indicates that the output of the j-th
operator is used as the input of the i-th operator. For operator with multiple inputs, inputs are combined
together ( e.g., using sumoraverage operator) before sending into the operator. Xis aN-size attribute
vector encoding operation types. For any operation i, its operation type Xican only choose from a pre-
deﬁned list with length D,e.g.,1×1,3×3or5×5convolutions. Note that for any valid feedforward
architecture, Gcan not have loops. One suﬃcient condition to satisfy the requirement is to constrain Ato
be a lower triangular matrix with zero diagonal ( i.e., excluding self-loops). This formalism creates a search
space of DN2N(N−1)/2possible architectures, which is huge even for moderately large number of operators
Nand number of operation types D. The goal of NAS is to ﬁnd an architecture or a set of architectures
within this search space that would perform well. For practical consideration, we search for cell graphs ( e.g.,
N= 32) and then replicate this cell several times to build a deep neural architecture. We also experiment on
the ENAS Macro search space where Gdeﬁnes a entire network. More details for the corresponding search
spaces can be found in Section 4.
4Published in Transactions on Machine Learning Research (11/2023)
3.1 Neural Architecture Search System
Before delving into details, we ﬁrst give an overview of our NAS system, which consists of two parts: a
generator and an evaluator. The system diagram is shown in Fig. 1. At each step, the probabilistic graph
generator samples a set of cell graphs, which are further translated to neural architectures by replicating the
cell graph multiple times and stacking them up. Then the evaluator evaluates these architectures, obtains
rewards, and sends architecture-reward pairs to the replay buﬀer. The replay buﬀer is then used to improve
the generator, eﬀectively forming a reinforcement learning loop.
3.1.1 Probabilistic Generators for Neural Architectures
Now we introduce our probabilistic graph generator which is based on a state-of-the-art deep auto-regressive
graph generative model in ( Liao et al. ,2019).
Auto-Regressive Generation. Speciﬁcally, we decompose the distribution of a cell graph along with
attributes (operation types) in an auto-regressive fashion,
P(A, X ) =N∏
i=1P(Ai,:|Ai−1,:, Xi−1,· · ·, A1,:, X1)P(Xi|Ai−1,:, Xi−1,· · ·, A1,:, X1), (1)
where Ai,:andXidenote the i-th row of the adjacency matrix Aand the i-th operation type respectively.
To ensure the generated graphs are DAGs, we constrain Ato be lower triangular by adding a binary mask,
i.e., the i-th node can only be reached from the ﬁrst i−1nodes. We omit the masks in the equations for
better readability. We further model the conditional distributions as follows,
P(Ai,:|Ai−1,:, Xi−1,· · ·, A1,:, X1) =K∑
k=1αk∏
1≤j<iθk,i,j (2)
P(Xi|Ai−1,:, Xi−1,· · ·, A1,:, X1) =Categorical (β1,· · ·, βD) (3)
α1, . . . , α K=Softmax(∑
1≤j<iMLP α(hS
i−hS
j))
(4)
β1, . . . , β D=Softmax(
MLP β(hS
i))
(5)
θ1,i,j, . . . , θ K,i,j =Sigmoid(
MLP θ(hS
i−hS
j))
, (6)
where the distributions of the operation type and edges are categorical and K-mixture of Bernoulli respec-
tively. Dis again the number of operation types. MLP α, MLP β, and MLP θare diﬀerent instances of
two-layer MLPs with ReLU activations. Here hS
iis the representation of i-th node returned by a GNN
which has been executed Ssteps of message passing at each generation step. This auto-regressive construc-
tion breaks down the nice property of permutation invariance for graph generation. However, we do not ﬁnd
it as an issue in practice, partly due to the fact that the graph isomorphism becomes less likely to happen
while considering both topology and operation types.
Message Passing GNNs. Each generation step n≤Nin auto-regressive generation above relies on
representations of nodes up to and including nitself (see Eq. ( 4)–(6)). To obtain these node representations
{hS
i}, we exploit message passing GNNs ( Gilmer et al. ,2017) with an attention mechanism similar to
(Liao et al. ,2019). In particular, the s-th message passing step involves executing the following equations
successively,
ms
ij=f([hs
i−hs
j,1ij]) (7)
˜hs
i= [hs
i, ui] (8)as
ij=Sigmoid (g(˜hs
i−˜hs
j)) (9)
hs+1
i=GRU (hs
i,∑
j∈N(i)as
ijms
ij). (10)
where N(i)is the set of node ialong with its neighboring nodes. ms
ijis the message sent from node itoj
at the s-th message passing step. The connectivity for the propagation in GNN is given by A1:i−1,1:i−1with
the last node (for which Ai,:has not been generated yet) being fully connected. Note that message passing
step is diﬀerent from the generation step and we run multiple message passing steps per generation step
in order to capture the structural dependency among nodes and edges. The fandgare two-layer MLPs.
5Published in Transactions on Machine Learning Research (11/2023)
Since graphs are DAGs in our case rather than undirected ones as in ( Liao et al. ,2019), we add 1ijin Eq.
(7), a one-hot vector for indicating the direction of the edge. We initialize the node representations h0
i(for
i < n ) as the corresponding one-hot encoded operation type vectors; h0
nis initialized to a special one-hot
vector. Here uiis an additional feature vector that helps distinguish i-th node from others. We found using
one-hot-encoded incoming neighbors of i-th node and a positional encoding of the node index iwork well in
practice. We encourage readers to reference Fig. 4for a detailed visualization of graph generation process.
Sampling. To sample from our generator, we ﬁrst draw architectures following the standard ancestral
sampling where each step involves drawing random samples from a categorical distribution and a mixture
of Bernoulli distribution. At each step, this sampling process adds a new operator with a certain operation
type and wire it to previously sampled operators.
3.1.2 Evaluator
Our design of generator and NAS pipeline do not rely on a speciﬁc choice of evaluator. Motivated by ( Mnih
et al. ,2013), we use a replay buﬀer for storing the evaluated architectures. In our paper, based on speciﬁc
datasets, we explore three types of evaluators, namely, oracle evaluator, supernet evaluator and benchmark
evaluator, which are brieﬂy introduced as follows.
Oracle evaluator. Given a sample from the generator, an oracle evaluator trains the corresponding network
from scratch and tests it to get the validation performances. To reduce computation overhead, a common
approach is to use early stopping (training with fewer epochs) as in ( Tan et al. ,2019;Tan & Le ,2019). In
our experiment, we instead use a low-data evaluator similar to few-shot learning where we keep the same
number of classes but use fewer samples per class to train.
SuperNet evaluator. Aiming at further reducing the amount of compute, this evaluator uses a weight-
sharing strategy where each graph is a sub-graph of the supernet. We followed the single-path supernet
setup used in ( Pham et al. ,2018b ) to compare with previous methods.
Benchmark evaluator. NAS benchmarks, e.g., (Ying et al. ,2019), provide accurate evaluation for archi-
tectures within the search space, which can be seen as oracle evaluators with full training budgets on target
datasets.
3.2 Learning Method
Since we are dealing with discrete latent variables, i.e., good architectures in our case, we train our NAS
system using REINFORCE ( Williams ,1992) algorithm with the control variate (a.k.a. baseline) to reduce
the variance. In particular, the gradient of the loss or negative expected reward Lw.r.t. the generator
parameters ϕis,
∇L(ϕ) =EP(G)[
−∂logP(G)
∂ϕ¯R(G)]
, (11)
where the reward ¯Ris standardized as ¯R(G) = (R(G)−C)/σ. Here the baseline Cis the average reward of
architectures in the replay buﬀer and σis standard deviation of rewards in the replay buﬀer. The expectation
in Eq. ( 11) is approximated by the Monte Carlo estimation. However, the score function ( i.e., the gradient of
log likelihood w.r.t. parameters) in the above equation may numerically diﬀer a lot for diﬀerent architectures.
For example, if a negative sample, i.e., an architecture with a reward lower than the baseline, has a low
probability P(G), it would highly likely to have an extremely large absolute score function value, thus leading
to a negative reward with an extremely large magnitude. Therefore, in order to balance positive and negative
rewards, we propose to use the reweighted log likelihood as follows,
logP(G) =β1¯R(G)≤0log(1 −P(G)) +1¯R(G)>0log(P(G)) (12)
where βis a hyperparameter that controls the weighting between negative and positive rewards. P(G)is the
original probability given by our generator.
Exploration vs. Exploitation Similar to many RL approaches, our NAS system faces the exploration vs.
exploitation dilemma. We found that our NAS system may quickly collapse ( i.e., overly exploit) to a few
6Published in Transactions on Machine Learning Research (11/2023)
MethodsCost Low Data (Search) Full Data (Final)
(GPU Days) Val Avg Acc Std Val Avg Acc Std
ER-TopK 15.2 23.12 0.34 61.76 0.04
WS-TopK 15.6 22.39 0.91 62.24 0.34
ER-BEST 15.2 20.07 1.62 62.10 0.25
WS-BEST 15.6 18.68 1.41 62.16 0.92
RNN ( Zoph et al. ,2018) 17.2 18.46 0.99 61.73 0.77
Ours 16.7 20.32 1.12 62.57 0.40
Table 1: Comparisons on Tiny-ImageNet. The top and bottom blocks include random search and learning-
to-search methods respectively. ER-TopK and WS-TopK refers to top ( K=4) architectures found by all WS
and ER models during search. ER-BEST and WS-BEST refer to the best ER and WS models found during
search, i.e., WS( k=4,p=0.75) and ER( p=0.1). Here Avg and Std of accuracies are computed based on 4
architectures sampled from generators.
good architectures due to the powerful graph generative model, thus losing the diversity and reducing to
point estimate. Inspired by the epsilon greedy algorithm ( Sutton & Barto ,2018) used in multi-armed bandit
problems, we design a random explorer to encourage more exploration in the early stage. Speciﬁcally, at
each search step, our generator samples from either itself or a prior graph distribution like the WattsStrogatz
model with a probability ϵ. As the search goes on, ϵis gradually annealed to 0 so that the generator gradually
has more exploitation over exploration. Whats more, we design our replay buﬀer to keep a small portion of
candidates. As training goes on, bad samples will be gradually be replaced by good samples for training our
generators, which encourage the model to exploit more.
4 Experiments
In this section, we extensively investigate our NAS system on three diﬀerent search spaces to verify its
eﬀectiveness. First, we adopt the challenging RandWire search space ( Xie et al. ,2019) which is signiﬁcantly
larger than common ones. To the best of our knowledge, we are the ﬁrst to explore learning NAS systems in
this space. Then we search on the ENAS Macro ( Pham et al. ,2018b ) and NAS-Bench-101( Ying et al. ,2019)
search spaces to further compare with previous literature. For all experiments, we set the number of mixture
Bernoulli Kto be 10, the number of message passing steps Sto 7, hidden sizes of node representation hs
i
and message ms
ijto 128. For RNN-based baselines, we follow the design in ( Zoph et al. ,2018) if not other
speciﬁed.
4.1 RandWire Search Space on Tiny-ImageNet
RandWire Search Space. Originally proposed in ( Xie et al. ,2019), a randomly wired neural network
is a ResNet-like four-stage network with the cell graph Gdeﬁnes the connectivity of Nconvolution layers
within each stage. At the end of each stage, the resolution is downsampled by 3 ×3 convolution with stride 2
whereas the number of channels is doubled. While following the RandWire small regime in ( Xie et al. ,2019),
we share the cell graph Gamong last three stages for simpliﬁcation. To keep the number of parameters
roughly the same, we ﬁx the node type to be separable 3 ×3 convolution. The number of nodes Nwithin
the cell graph Gis set to 32 excluding the input and output nodes. This yields a search space of 2.1×10149
valid adjacency matrices, which is extremely large and renders the neural architecture search challenging.
More details of the RandWire search space can be found in the Appendix C.1.
Tiny-ImageNet w. Oracle Evaluator. To enable search on the RandWire space, we exploit the oracle
evaluator on the Tiny-ImageNet dataset ( Chrabaszcz et al. ,2017). To save computation, we employ a low-
data oracle evaluator where we sample 1/10of Tiny-ImageNet training set for training and use the rest for
validation at each search step. Similar to the few-shot learning, we keep the number of classes unchanged
but reduce the number of samples per class. After the search, we retrain our found architectures on the full
training set and evaluate it on the original validation set. Speciﬁcally, for each model, the oracle evaluator
trains for 300 epochs and uses the average validation accuracy of the last 3 epochs as the reward. Our
total search budget is around 16 GPU days, which approximately amounts to 320 model evaluations, e.g.,
40 search steps and 8 samples evaluated per step. For random search baselines, we choose ErdsRényi (ER)
7Published in Transactions on Machine Learning Research (11/2023)
Model Param (M) Top1 / Top5 Acc
Resnet18 11.68 59.71±0.09 80.32±0.10
Resnet50 25.56 63.42±0.30 82.61±0.15
Resnext50 27.56 63.62±0.07 82.73±0.08
FC 3.49 60.82±0.24 82.29±0.09
ER-Top1 3.23 61.82±0.09 82.30±0.18
RS-Top1 3.22 62.55±0.15 82.64±0.21
RNN 3.32 62.29±0.39 82.16±0.24
Ours 3.27 63.23 ±0.1883.06 ±0.05
WS-Top1 Large 19.38 63.84±0.13 82.61±0.16
RNN Large 19.78 63.69±0.28 82.74±0.21
Ours Large 19.18 64.45 ±0.2683.23 ±0.26
Table 2: Comparisons of best-searched architectures (averaged over 3 runs per architecture) on Tiny Ima-
geNet.
and WattsStrogatz (WS) models. Speciﬁcally, we ﬁrst randomly draw hyperparameters from certain ranges,
i.e.,0.1≤p≤0.5for ER and (2,0.2)≤(k, p)≤(6,0.8)for WS, and then sample Gfrom individual models.
We set the reweight coeﬃcient βto 0.05. For the random explorer, we choose WS model with the same
hyperparameter range as a prior distribution and set ϵ= 0.6in the beginning and decay it by a factor
of 0.2 every 10 search steps. We also ﬁnd that gradually shrinking replay buﬀer size to keep 30% to10%
of top-performing architectures helps stabilize the training of the generator. At the search time, we reject
samples that already appear in the replay buﬀer to avoid duplications. We apply the same setting to the
RNN generator for a fair comparison.
Results. As shown in Table 1, we compare our NAS system with other random search methods and learning-
to-search methods. We can see that our method outperforms the RNN-based generator and other random
search methods in terms of average validation accuracy on the full dataset. Our generator also has a lower
variance compared to the RNN-based one. Moreover, we observed that RNN-based generator sometimes
degenerates so that it frequently samples densely-connected graphs. This is probably due to the fact that
RNN based generator does not eﬀectively utilize the topology information. We can see that a high search
reward ( i.e., a low-data validation accuracy) does not necessarily lead to better performances in full data
training, which indicates a bias of the oracle evaluator within the low-data regime. Random search methods
are prone to be biased as they select architectures solely based on the search reward. This suggests that if a
model is selected purely based on the proxy measurement, there may be bias when using proxy measurements
such as low-data compared to Full data. Instead, learning a probabilistic distribution with the proxy and
then sampling from this distribution may oﬀer a more robust search method than deterministic selection
with a proxy.
We also show results of the best architectures found within 4 samples in Table 2. Here, ER-top-1 and WS-
top-1 refer to the best model found from the corresponding random search. FC refers to the fully-connected
graph, which takes three times longer to train compared to our model. It is clear that the best model found
by our method outperforms those discovered by other methods by a considerable margin. Moreover, we
scale up the best models (denoted as large) by adding more channels and one more computation stage (more
details are in Appendix C.1). We can see that our searched architectures perform favorably against manually
designed architectures like ResNet ( He et al. ,2016) and ResNeXt ( Xie et al. ,2017).
4.2 ENAS Macro Search Space on CIFAR10
ENAS Macro Search Space , originally proposed by Pham et al. (2018b ), is a search space which focuses
on the entire network. Ghere deﬁnes the entire network with N= 12 nodes. The operation type ( D=6)1
per node is also searchable. Gis guaranteed to contain a length-11 path, i.e.,∀i >1,Ai,i−1= 1. The goal
is to search oﬀ-diagonal entries, i.e., skip connections. This gives a search space of 1.6×1029valid networks
in total.
11×1,5×5convolution, 1×1,5×5separable convolution, max pooling, avg pooling
8Published in Transactions on Machine Learning Research (11/2023)
MethodsSearch Cost Params Best Top Samples
(days) (M) Error Rate Avg Std
Net Transform ( Cai et al. ,2018) 10 19.7 5.7 - -
NAS ( Zoph & Le ,2016) 22400 7.1 4.47 - -
PNAS ( Liu et al. ,2018a ) 225 3.2 3.41 - -
Lemonade ( Elsken et al. ,2018a ) 56 3.4 3.6 - -
EPNAS-Macro ( Perez-Rua et al. ,2018) 1.2 38.8 4.01 - -
RNN* ( Pham et al. ,2018b ) 0.9 19.64 4.18 4.47 0.282
RNN* Large 0.9 36.92 4.00 4.16 0.089
Ours 0.5 20.47 3.73 3.93 0.098
Ours Large 0.5 37.71 3.55 3.62 0.050
Table 3: Comparisons on CIFAR10 dataset. The bottom and top blocks include NAS methods with ENAS
Macro and other search spaces respectively. *: our re-implementation. -: inapplicable.
Figure 2: Performances (average over 10 runs) of
best architectures vs. the number of architecture
evaluations (search step).Method Avg Error #Queries
GCN Pred†6.331 150
Evolution†6.109 150
Ours 5.930 ±0.143 150
NAO†6.51 192
AG-NET†5.82 192
Arch2Vec†5.95 400
Random Search 6.413 ±0.422 300
Local Search 5.879 ±0.371 300
BANANAS 5.906 ±0.296 300
RNN (RL) 6.028 ±0.228 300
Ours (RL) 5.807 ±0.072 300
Table 4: Best model performances on NAS-
Bench-101.†indicates numbers are taken from
(White et al. ,2019;Luo et al. ,2018b ;Yan et al. ,
2020;Lukasik et al. ,2022)
SuperNet Evaluator. For ENAS Macro search space, we experiment on CIFAR10 ( Krizhevsky et al. ,
2009) dataset. For our generator, we use the ER model with p= 0.4as our explorer, where ϵdecays from
1 to 0 in the ﬁrst 100 search steps. For RNN based generator, we follow the setup in ( Pham et al. ,2018b ).
We also adopt the weight-sharing mechanism in ( Pham et al. ,2018b ) to obtain a SuperNet evaluator that
eﬃciently evaluates a model’s performance. We use a budget of 300 search steps with around 100 architectures
evaluated per step for all methods. After the search, we use a short-training of 100 epochs to evaluate the
performances of 8 sampled architectures, after which top-4 performing ones are chosen for a 600-epoch full
training. The best validation error rate among these 4 architectures is reported. For simplicity and a fair
comparison, we do not use additional tricks ( e.g., adding entropy regularizer) in ( Pham et al. ,2018b ). More
details are provided in Appendix D.
In Table 3, we compare the error rates and variances for diﬀerent NAS methods. Note that this variance
reﬂects the uncertainty of the distribution of architectures as it is computed based on sampled architectures.
It is clear that our GraphPNAS achieves both lower error rates and lower variances compared to RNN
based generator and is on par with the state-of-the-art NAS methods on other search spaces. We also see
that the best architecture performance of our generator outperforms RNN based generator by a signiﬁcant
margin. This veriﬁes that our GraphPNAS is able to learn a distribution of well-performing neural archi-
tectures. Given that we only sample 8 architectures, the performances could be further improved with more
computational budgets.
4.3 NAS Benchmarks
NAS-Bench-101 (Ying et al. ,2019) is a tabulate benchmark containing 423K cell graphs, each of which is
a DAG with up to 7nodes and 9edges including input and output nodes.
9Published in Transactions on Machine Learning Research (11/2023)
MethodsCIFAR-10 CIFAR-100 ImageNet-16-120GeneratorVal Test Val Test Val Test
Optimum 91.61 94.37 73.49 73.51 46.73 47.31 -
Weight Sharing NAS:
GDAS ( Dong & Yang ,2019) 89.68 ±0.72 93.23 ±0.58 68.35 ±2.71 68.17 ±2.50 39.55 ±0.00 39.40 ±0.00 -
ENAS ( Pham et al. ,2018a ) 90.20 ±0.00 93.76 ±0.00 70.21 ±0.71 70.67 ±0.62 40.78 ±0.00 41.44 ±0.00 -
SGNAS ( Huang & Chu ,2021) 90.18 ±0.31 93.53 ±0.12 70.28 ±1.20 70.31 ±1.09 44.65 ±2.32 44.98 ±2.10 -
DrNet ( Chen et al. ,2021) 91.55 ±0.0094.36 ±0.0073.49 ±0.0073.51 ±0.0046.37 ±0.0046.34 ±0.00 -
Multi-trial NAS
RANDOM ( Dong & Yang ,2020) 91.07 ±0.26 93.86 ±0.23 71.46 ±0.97 71.55 ±0.97 45.03 ±0.91 45.28 ±0.97 RANDOM
BOHB ( Falkner et al. ,2018) 91.17 ±0.27 93.94 ±0.28 72.04 ±0.93 72.00 ±0.86 45.55 ±0.79 45.70 ±0.86 HyperBand
REINFORCE ( Ying et al. ,2019) 91.12 ±0.25 93.90 ±0.26 71.80 ±0.94 71.86 ±0.89 45.37 ±0.74 45.64 ±0.78 LSTM
GANAS ( Changiz Rezaei et al. ,2021) - 94.34 ±0.05 - 73.28 ±0.17 - 46.80 ±0.29 Graph GAN
Arch2VEC ( Yan et al. ,2020) 91.41 ±0.22 94.18 ±0.24 73.35 ±0.3273.37 ±0.3046.34 ±0.1846.27 ±0.37 Graph VAE
AG-Net ( Lukasik et al. ,2022) 91.55 ±0.0894.24 ±0.19 73.2±0.34 73.12 ±0.40 46.31 ±0.33 46.20 ±0.47 Graph VAE Decoder
GraphPNAS 91.50 ±0.0994.34 ±0.0473.41 ±0.0973.35 ±0.21 46.29 ±0.2746.50 ±0.20 Auto-regressive GRAN
Table 5: Searched best architecture performance on NAS-Bench-201. We run our methods 10 times to obtain
mean and standard deviation.
We compare the performances of our GraphPNAS to open-source implementations of random search methods,
local search methods, and BANANAS ( White et al. ,2019). The latter two are the best algorithms found
byWhite et al. (2020) on NAS-Bench-101. For GCN prediction and evolution methods, we use the score
reported in ( White et al. ,2020).
We give each NAS method the same budget of 300 queries and plot the curve of lowest test error as a function
of the number of evaluated architectures.
As shown in Fig. 2, our GraphPNAS is able to quickly ﬁnd well-performing architectures. We also report
the avg error rate over 10 runs in Table 4. Our GraphPNAS again outperforms RNN based generator by
a signiﬁcant margin and beats strong baselines like local search methods and BANANAS. Notably, our
GraphPNAS has a much lower variance than other methods, thus being more stable across multiple runs.
NAS-Bench-201 (Dong & Yang ,2020) is deﬁned on a smaller search space where up to 4nodes and
6edges are allowed. Here we compare our method on NAS-Bench-201( Dong & Yang ,2020) with random
search (RANDOM) Bergstra & Bengio (2012), ENAS Pham et al. (2018b ), GDAS Dong & Yang (2019),
BOHB Falkner et al. (2018). Closely related to our methods, we compare with 3 more NAS methods with
graph genrator: Arch2Vec Yan et al. (2020), AG-Net Lukasik et al. (2022), GANNAS Changiz Rezaei et al.
(2021). Here Arch2VEC, AG-Net and our methods are ﬁxed to 100 queries, while GANNAS is reported to
use 400 qureis. Note that Arch2VEC is coupled with a surrogate model for preselecting arch before queries.
Our model can also potentially beneﬁt from such setting. As shown in table 5, our method is either on-
par-with or have outperformed previous graph generator based model on all three splits of NAS-Bench-201.
However, it’s worth noting that the NAS-Bench-201 is less challenging in the size of search space. Gradient
based methods like DrNAS Chen et al. (2021) can well explore the entire search space given limited compute
budget and ﬁnd near optimal solution. In table 5, DrNAS is able to ﬁnd the optimal model with a validation
accuracy of 73.49 on cifar100 validation set while our method ﬁnd a sub-optimal model with 73.41(-0.08)
accuracy.
We also experiment with the eﬀect of budget on ImageNet-16-120 split. We start with a baseline of 45.00
and 45.39 accuracy on validation and test set over 40 queries. By extending the budget with 20 more queries,
we observe signiﬁcantly improvement of performance to 45.57 (+0.57) and 45.79 (+0.4) on validation and
test sets respectively. Extending it to 100 queries gives us the state-of-the-art result showed in table 5. This
indicates that our model can not well explore the search space and converge with limited search steps. This
is probably due to the fact that the auto-regressive model requires enough samples in the replay buﬀer to be
able to train well, so a reasonable number of search steps is needed for our model to reach its full potential.
5 Discussion & Conclusion
Qualitative Comparisons between RNN and GraphPNAS. In (You et al. ,2020), the clustering
coeﬃcient and the average path length have been used to investigate distributions of graphs. Here we adopt
10Published in Transactions on Machine Learning Research (11/2023)
OursRNN Generator
Average Path lengthClustering Coefficient
Figure 3: Visualization of architecutre explore sapce of GraphPNAS vs RNN. Each point in the ﬁgure
denotes a model evaluation. Colors of each node denotes its validation accuracy returned by the low-data
Oracle evaluator.
the same metrics to visualize architectures (graphs) sampled by RNN based and our generators in RandWire
experiments. Points in Fig. 3refer to architectures sampled from both generators in the last 15 search steps
where random explorers are disabled. The validation performances are color-coded. We can see that our
GraphPNAS samples a set of graphs that have better validation accuracies while the ones of RNN generator
have large variances in performances. Moreover, the graphs in our case concentrate to those with smaller
clustering coeﬃcients, thus less likely being densely-connected. On the contrary, RNN generator tends to
sample graphs that are more likely to be densely-connected. While RNN has been widely used for NAS, we
show in our experiments that our graph generator consistently outperforms RNN over three search spaces
on two diﬀerent datasets. This is likely due to the fact that our graph generator better leverages graph
topologies, thus being more expressive in learning the distribution of graphs.
Bias in Evaluator. In our experiments, we use SuperNet evaluator, low-data, and full-data Oracle evaluator
to eﬃciently evaluate the model. From the computational eﬃciency perspective, one would prefer the
SuperNet evaluator. However, it tends to give high rewards to those architectures used for training SuperNet.
Although the low-data evaluator is more eﬃcient than the full-data one, its reward is biased as discussed in
Section 4.1. This bias is caused by the discrepancy between the data distributions in low-data and full-data
regimes. We also tried to follow ( Tan et al. ,2019) to use early stopping to reduce the time cost of the full-
data evaluator. However, we found that it assigns higher rewards to those shallow networks which converge
much faster in the early stage of training. We show detailed results in Appendix C.4.
Search Space Design. The design of search space largely aﬀects the performances of NAS methods. Our
GraphPNAS successfully learns good architectures on the challenging RandWire search space. However, the
search space is still limited as the cell graph across diﬀerent stages is shared. A promising direction is to
learn to generate graphs in a hierarchical fashion. For example, one can ﬁrst generate a macro graph and
then generate individual cell graphs (each cell is a node in the macro graph) conditioned on the macro graph.
This will signiﬁcantly enrich the search space by including the macro graph and untying cell graphs.
Limitation to Scale up. Our method, as with all multi-trial generator-based NAS methods, does face
a notable limitation: scaling up with an Oracle Evaluator proves challenging. One can resort to proxy
evaluators, employing strategies such as weight-sharing, early-stopping, zero-cost-proxy, etc. However, these
search results could be heavily inﬂuenced by the bias of the evaluator. On the upside, the decoupling of the
architecture generator from the evaluator (unlike pruning-based DrNAS and weight-sharing methods like
ENAS) implies that any improvement in evaluation can consistently result in superior NAS methods across
all search spaces
Conclusion. In this paper, we propose a GNN-based graph generator for NAS, called GraphPNAS. Our
graph generator naturally captures topologies and dependencies of operations of well-performing neural archi-
tectures. It can be learned eﬃciently through reinforcement learning. We extensively study its performances
on the challenging RandWire as well as two widely used search spaces. Experimental results show that
our GraphPNAS consistently outperforms the RNN-based generator on all datasets. Future works include
11Published in Transactions on Machine Learning Research (11/2023)
exploring ensemble methods based on our GraphPNAS and hierarchical graph generation on even larger
search spaces.
Acknowledgments
This work was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chair, NSERC CRC,
NSERC DG and Discovery Accelerator Grants, and Oracle Cloud credits. Resources used in preparing
this research were provided, in part, by the Province of Ontario, the Government of Canada through the
Digital Research Alliance of Canada alliance.can.ca , and companies sponsoring the Vector Institute www.
vectorinstitute.ai/#partners , Advanced Research Computing at the University of British Columbia,
and the Oracle for Research program. Additional hardware support was provided by John R. Evans Leaders
Fund CFI grant and Compute Canada under the Resource Allocation Competition award. We would like to
thank Raquel Urtasun, Wenyuan Zeng, Yuwen Xiong, Ethan Fetaya, and Thomas Kipf for supporting the
exploration along this research direction before this work.
References
Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures
using reinforcement learning. In International Conference on Learning Representations ICLR , 2017.
Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search
using performance prediction. In International Conference on Learning Representations ICLR , 2018.
Albert-László Barabási and Réka Albert. Emergence of scaling in random networks. Science , 286(5439),
1999.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of machine
learning research , 13(2), 2012.
James Bergstra, Daniel Yamins, and David D. Cox. Making a science of model search: Hyperparameter opti-
mization in hundreds of dimensions for vision architectures. In Proceedings of the International Conference
on Machine Learning, ICML , 2013.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural
networks. arXiv preprint arXiv:1505.05424 , 2015.
Andrew Brock, Theodore Lim, James M. Ritchie, and Nick Weston. SMASH: one-shot model architecture
search through hypernetworks. In International Conference on Learning Representations ICLR , 2018.
Marc Brockschmidt, Miltiadis Allamanis, Alexander L Gaunt, and Oleksandr Polozov. Generative code
modeling with graphs. arXiv preprint arXiv:1805.08490 , 2018.
Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. Path-level network transformation for
eﬃcient architecture search. In International Conference on Machine Learning , pp. 678–687. PMLR, 2018.
Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task and
hardware. In International Conference on Learning Representations ICLR . OpenReview.net, 2019.
Seyed Saeed Changiz Rezaei, Fred X. Han, Di Niu, Mohammad Salameh, Keith Mills, Shuo Lian, Wei
Lu, and Shangling Jui. Generative adversarial neural architecture search. pp. 2227–2234, 8 2021. doi:
10.24963/ijcai.2021/307. URL https://doi.org/10.24963/ijcai.2021/307 . Main Track.
Xiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, and Cho-Jui Hsieh. Dr{nas}: Dirichlet
neural architecture search. In International Conference on Learning Representations , 2021. URL https:
//openreview.net/forum?id=9FWas6YbmB3 .
Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an alternative
to the cifar datasets. arXiv preprint arXiv:1707.08819 , 2017.
12Published in Transactions on Machine Learning Research (11/2023)
Hang Chu, Daiqing Li, David Acuna, Amlan Kar, Maria Shugrina, Xinkai Wei, Ming-Yu Liu, Antonio
Torralba, and Sanja Fidler. Neural turtle graphics for modeling city road layouts. In Proceedings of the
IEEE International Conference on Computer Vision , pp. 4522–4530, 2019.
Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four gpu hours. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 1761–1770, 2019.
Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search.
arXiv preprint arXiv:2001.00326 , 2020.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Eﬃcient multi-objective neural architecture search
via lamarckian evolution. arXiv preprint arXiv:1804.09081 , 2018a.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. arXiv
preprint arXiv:1808.05377 , 2018b.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Eﬃcient multi-objective neural architecture search
via lamarckian evolution. In International Conference on Learning Representations ICLR , 2019.
Paul Erdös and Alfréd Rényi. On random graphs i. Publicationes Mathematicae Debrecen , 6, 1959.
Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and eﬃcient hyperparameter optimization
at scale. In International Conference on Machine Learning , pp. 1437–1446. PMLR, 2018.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty
in deep learning. In international conference on machine learning , pp. 1050–1059, 2016.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message
passing for quantum chemistry. arXiv preprint arXiv:1704.01212 , 2017.
Aditya Grover, Aaron Zweig, and Stefano Ermon. Graphite: Iterative generative modeling of graphs. arXiv
preprint arXiv:1803.10459 , 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016.
Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First steps.
Social Networks , 5(2), 1983.
Sian-Yao Huang and Wei-Ta Chu. Searching by generating: Flexible and eﬃcient one-shot nas with architec-
ture generator. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 983–992, 2021.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular
graph generation. arXiv preprint arXiv:1802.04364 , 2018.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308 , 2016.
Alex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Wei Li, Shaogang Gong, and Xiatian Zhu. Neural graph embedding for neural architecture search. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 34, pp. 4707–4714, 2020.
Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative models
of graphs. arXiv preprint arXiv:1803.03324 , 2018.
13Published in Transactions on Machine Learning Research (11/2023)
Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K Duvenaud, Raquel Urtasun,
and Richard Zemel. Eﬃcient graph generation with graph recurrent attention networks. In Advances in
Neural Information Processing Systems , pp. 4255–4265, 2019.
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille,
Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Proceedings of the Euro-
pean Conference on Computer Vision (ECCV) , pp. 19–34, 2018a.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Diﬀerentiable architecture search. arXiv preprint
arXiv:1806.09055 , 2018b.
Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing ﬂows. In Advances
in Neural Information Processing Systems , pp. 13578–13588, 2019.
Jovita Lukasik, Steﬀen Jung, and Margret Keuper. Learning where to look–generative nas is surprisingly
eﬃcient. In European Conference on Computer Vision , pp. 257–273. Springer, 2022.
Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization. Advances
in neural information processing systems , 31, 2018a.
Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization. Advances
in neural information processing systems , 31, 2018b.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 , 2013.
Radford M Neal. Bayesian learning for neural networks , volume 118. Springer Science & Business Media,
2012.
T Den Ottelander, Arkadiy Dushatskiy, Marco Virgolin, and Peter AN Bosman. Local search is a remarkably
strong baseline for neural architecture search. arXiv preprint arXiv:2004.08996 , 2020.
Juan-Manuel Perez-Rua, Moez Baccouche, and Stéphane Pateux. Eﬃcient progressive neural architecture
search. In BMVC , 2018.
Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeﬀ Dean. Eﬃcient neural architecture search
via parameters sharing. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pp. 4095–4104.
PMLR, 10–15 Jul 2018a. URL https://proceedings.mlr.press/v80/pham18a.html .
Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeﬀ Dean. Eﬃcient neural architecture search
via parameter sharing. arXiv preprint arXiv:1802.03268 , 2018b.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized evolution for image classiﬁer
architecture search. In AAAI Conference on Artiﬁcial Intelligence , 2019.
Robin Ru, Pedro Esperanca, and Fabio Maria Carlucci. Neural architecture generator optimization. Advances
in Neural Information Processing Systems , 33:12057–12069, 2020.
Han Shi, Renjie Pi, Hang Xu, Zhenguo Li, James Kwok, and Tong Zhang. Bridging the gap between sample-
based and one-shot neural architecture search with bonas. Advances in Neural Information Processing
Systems , 33:1808–1819, 2020.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
Mingxing Tan and Quoc Le. Eﬃcientnet: Rethinking model scaling for convolutional neural networks. In
International conference on machine learning , pp. 6105–6114. PMLR, 2019.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V
Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 2820–2828, 2019.
14Published in Transactions on Machine Learning Research (11/2023)
Duncan J Watts and Steven H Strogatz. Collective dynamics of small-world networks. Nature , 393(6684),
1998.
Colin White, Willie Neiswanger, and Yash Savani. Bananas: Bayesian optimization with neural architectures
for neural architecture search. arXiv preprint arXiv:1910.11858 , 2019.
Colin White, Sam Nolen, and Yash Savani. Local search is state of the art for neural architecture search
benchmarks. arXiv preprint arXiv:2005.02960 , 2020.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Machine learning , 8(3-4):229–256, 1992.
Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transforma-
tions for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pp. 1492–1500, 2017.
Saining Xie, Alexander Kirillov, Ross Girshick, and Kaiming He. Exploring randomly wired neural networks
for image recognition. In Proceedings of the IEEE International Conference on Computer Vision , pp.
1284–1293, 2019.
Shen Yan, Yu Zheng, Wei Ao, Xiao Zeng, and Mi Zhang. Does unsupervised architecture representation
learning help neural architecture search? Advances in neural information processing systems , 33:12486–
12498, 2020.
Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy, and Frank Hutter. Nas-bench-101:
Towards reproducible neural architecture search. In International Conference on Machine Learning , pp.
7105–7114, 2019.
Jiaxuan You, Rex Ying, Xiang Ren, William L Hamilton, and Jure Leskovec. Graphrnn: Generating realistic
graphs with deep auto-regressive models. arXiv preprint arXiv:1802.08773 , 2018.
Jiaxuan You, Jure Leskovec, Kaiming He, and Saining Xie. Graph structure of neural networks. arXiv
preprint arXiv:2007.06559 , 2020.
Chris Zhang, Mengye Ren, and Raquel Urtasun. Graph hypernetworks for neural architecture search. arXiv
preprint arXiv:1810.05749 , 2018.
Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, Zongyuan Ge, and Steven Su. Diﬀerentiable neural
architecture search in equivalent space with exploration enhancement. Advances in Neural Information
Processing Systems , 33:13341–13351, 2020.
Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and Cheng-Lin Liu. Practical block-wise neural network
architecture generation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR , 2018.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578 , 2016.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scal-
able image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pp. 8697–8710, 2018.
15Published in Transactions on Machine Learning Research (11/2023)
A More details on Graph Generator
To more clearly illustrate the sampling process of our generator, we detailed probabilistic sampling process
of our generator in Fig. 4.
Figure 4: Detailed steps of auto-regressive generation with our graph generator.
B Comparison with NAGO
Following Xie et al. (2019)’s work on random graph models, Ru et al. (2020) propose to learn parameters of
random graph models using bayesian optimization. We compare with the randwire search space (refers to as
RANG) in ( Ru et al. ,2020). Since the original search space in ( Xie et al. ,2019) do not reuse cell graphs for
diﬀerent stages, we train conditionally independent graph generators for diﬀerent stages respectively. That
is 3 conditionally independent generators for conv 3, conv 4, and conv 5stage in Table 7. We perform a search
on the CIFAR10 dataset, where each model is evaluated for 100 epochs. We restrict the search budget to 600
oracle evaluations. We align with settings in ( Ru et al. ,2020) for retraining and report sampled architecture’s
test accuracy and standard deviation in the Table 6. We can see that our method learns a distribution of
Methods Reference Avg. Test Accuracy (%) Std.
RANG-D Xie et al. (2019) 94.1 0.16
RANG-BOHB Ru et al. (2020) 94.0 0.26
RANG-MOBO Ru et al. (2020) 94.3 0.13
Ours - 94.6 0.18
Table 6: Comparison of the searched results on CIFAR10. Mean test accuracy and the standard deviation
are calculated over 8 samples from the searched generator. We align the search space design and retraining
setting for a fair comparison.
graphs that outperforms previous methods.
C More details on the Randwire experiments
C.1 Details of RandWire search space
Here we provided more details on the RandWire search space shown in Table 7and Fig. 5.
16Published in Transactions on Machine Learning Research (11/2023)
Stage OutputBase Large
Cell Channels Cell Channels
conv 1 112×112 conv 3×3 32 conv 3×3 48
conv 2 56×56 conv 3×3 64 conv 3×3 96
conv 3 28×28 G 64 G 192
conv 4 14×14 G 128G 288
G 384
conv 5 7×7 G 256 G 586
classiﬁer 1×11×1conv 1×1, 1280-d
global average pool, 200-d fc, softmax
Table 7: Randwire search space with base and large settings. Base is the default setting for search while
Large refers to the architecture of scaled up models in Table 2.conv denote a ReLU-SepConv-BN triplet .
The input size is 224 ×224 pixels. The change of the output size implies a stride of 2 (omitted in table) in
the convolutions that are placed at the end of each block. Gis the shared cell graph that has N= 32 node.
Figure 5: Visualization of RandWire search base space used in this paper. Diﬀerent from ( Xie et al. ,2019),
Ghere is shared across three stages.
17Published in Transactions on Machine Learning Research (11/2023)
C.2 Details for RandWire experiments
For experiment on Tiny-Imagenet, we resize image to 224×224as showed in Table 7. We apply the basic
data augmentation of horizontal random ﬂip and random cropping with padding size 4. We provide detailed
hyper-parameters for oracle evaluator training and learning for GraphPNAS in Table 8
Oracle Evaluator Graph Controller
batch size 256 graph batch size 16
optimizer SGD generator optimizer Adam
learning rate 0.1 generator Learning rate 1e-4
learning rate deacy consine lr decay generator learning rate decay none
weight decay 1e-4 generator weight decay 0.
grad clip 0. generator gradient clip 1.0
training epochs 300 replay buﬀer ﬁtting epochs 2000
Table 8: Hyperparameter setting for oracle evaluator and training our graph generator.
C.3 Visualization of architectures from our generator
Here we visualize the top candidate architectures in Fig. 6
C.4 Bias for early stopping
As discussed in Section 5, using early stopping will lead to local minimal where the generator learns to
generate shallow cell structure. We quantify this phenomenon in table 9, where we can see that with early
stopping training, the generator will generate more shallow architectures with a shorter path from input to
output. The corresponding average ﬁnal validation accuracy also dropped by a large margin compared to
the low data evaluator counter part.
evaluator Final Val Acc Average Path Longest Path
early stopping 61.86 2.595 6.125
low data regime 62.57 3.046 8.75
Table 9: In the table, we show ablation on the choice of oracle evaluator with our graph generator. The
average Path and Longest path are computed as the average path length and longest path length from input
to output over 8 samples from the corresponding generator.
D More details on ENAS Macro experiments
For ENAS Macro search space, we use a pytorch-based open source implementation2and follow the detailed
parameters provided in ( Pham et al. ,2018b ) for RNN generator. Speciﬁcally, we follow ( Pham et al. ,2018b )
to train the SuperNet and update the generator in an iterative style. At each search step, two sets of
samples GtrainandGevalare sampled from the generator. Gtrainis used to learn the SuperNet’s weights by
back-propagating the training loss. The updated SuperNet is used for evaluating Geval, which is then used
for updating the generator.
For our generator, we evaluate 100 architectures per step and update our generator every 5 epochs of
SuperNet training. Instead of evaluating on a single batch, we reduce the number of models evaluated per
step and evaluate on the full test set. We found this stables the training of our generator while keeping
evaluation costs the same. In the replay buﬀer, the top 20% of architectures is kept.
2https://github.com/microsoft/nni/tree/v1.6/examples/nas/enas
18Published in Transactions on Machine Learning Research (11/2023)
Figure 6: Visualization of Top 3 architectures sampled by each method. We observe that around 50% of
samples from RNN generators are densely connected graphs or even fully connected graphs.
19Published in Transactions on Machine Learning Research (11/2023)
Figure 7: The best architecture found by GraphPNAS and RNN generator ( Pham et al. ,2018b ). Correspond
to scores report in table 3. To get this architecture, we pre-evaluate 8 samples for both methods and select
top-performing architecture.
For training SuperNet and RNN generator, we follow the same hyper-parameter setting in ( Pham et al. ,
2018b ) except the learning rate decay policy is changed to cosine learning rate decay. For training our
generator we use the same hyperparameter as in Table 8with graph batch size changed to 32. For retraining
the found the best architecture, we use a budget of 600 epoch training with a learning rate of 0.1, batch
size 256, and weight decay 2e-4. We also apply a cutout with a probability of 0.4 to all the models when
retraining the model.
D.1 Visualization of best architecture found
Here we visualize the best architecture found by GraphPNAS and RNN generator for Enas Macro search
space in Fig. 7.
E More details on NAS Benchmark
For sampling on NAS-Bench-101 ( Ying et al. ,2019), we ﬁrst sample a 7-node DAG, then we remove any
node that is not connected to the input or the output node. We reject samples that have more than 9 edges
or don’t have a path from input to output.
To train our generator on Nas-Bench-101, we use ErdsRényi with p= 0.25,ϵis set to 1 in the beginning and
decreased to 0 after 30 search steps.
20Published in Transactions on Machine Learning Research (11/2023)
For the replay buﬀer, we keep the top 30 architectures. Our model is updated every 10 model evaluations,
where we train 70 epochs on the replay buﬀer at each update time. The learning rate is set to 1e-3 for with
a batch size of 2 on Nas-bench-101.
21