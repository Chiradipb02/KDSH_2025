Under review as submission to TMLR
Relationship between Batch Size and Number of Steps
Needed for Nonconvex Optimization of Stochastic Gradient
Descent using Armijo Line Search
Anonymous authors
Paper under double-blind review
Abstract
While stochastic gradient descent (SGD) can use various learning rates, such as constant
or diminishing rates, the previous numerical results showed that SGD performs better than
other deep learning optimizers using when it uses learning rates given by line search meth-
ods. In this paper, we perform a convergence analysis on SGD with a learning rate given
by an Armijo line search for nonconvex optimization indicating that the upper bound of
the expectation of the squared norm of the full gradient becomes small when the number
of steps and the batch size are large. Next, we show that, for SGD with the Armijo-line-
search learning rate, the number of steps needed for nonconvex optimization is a monotone
decreasing convex function of the batch size; that is, the number of steps needed for non-
convex optimization decreases as the batch size increases. Furthermore, we show that the
stochastic ﬁrst-order oracle (SFO) complexity, which is the stochastic gradient computation
cost, is a convex function of the batch size; that is, there exists a critical batch size that
minimizes the SFO complexity. Finally, we provide numerical results that support our the-
oretical results. The numerical results indicate that the number of steps needed for training
deep neural networks decreases as the batch size increases and the SFO complexity is convex
with respect to the batch size.
1 Introduction
1.1 Background
Nonconvex optimization is useful for training deep neural networks, since the loss functions called the
expected risk and empirical risk are nonconvex and they need only be minimized in order to ﬁnd
the model parameters. Deep-learning optimizers have been presented for minimizing the loss func-
tions. The simplest one is stochastic gradient descent (SGD) ( Robbins & Monro ,1951;Zinkevich ,2003;
Nemirovski et al. ,2009;Ghadimi & Lan ,2012;2013) and there are numerous theoretical analyses on using
SGD for nonconvex optimization ( Jain et al. ,2018;Vaswani et al. ,2019;Fehrman et al. ,2020;Chen et al. ,
2020;Scaman & Malherbe ,2020;Loizou et al. ,2021). Variants have also been presented, such as momen-
tum methods ( Polyak ,1964;Nesterov ,1983) and adaptive methods including Adaptive Gradient (AdaGrad)
(Duchi et al. ,2011), Root Mean Square Propagation (RMSProp) ( Tieleman & Hinton ,2012), Adaptive Mo-
ment Estimation (Adam) ( Kingma & Ba ,2015), Adaptive Mean Square Gradient (AMSGrad) ( Reddi et al. ,
2018), and Adam with decoupled weight decay (AdamW) ( Loshchilov & Hutter ,2019). SGD and its variants
are useful for training not only deep neural networks but also generative adversarial networks ( Heusel et al. ,
2017;Naganuma & Iiduka ,2023;Sato & Iiduka ,2023).
The performance of deep-learning optimizers for nonconvex optimization depends on the batch size. The
previous numerical results in ( Shallue et al. ,2019) and ( Zhang et al. ,2019) have shown that the number of
steps Kneeded to train a deep neural network halves for each doubling of the batch size band that there is
a region of diminishing returns beyond the critical batch size b⋆. This fact can be expressed as follows: there
is a positive number Csuch that N:=Kb≈Cforb≤b⋆andN:=Kb≥Cforb≥b⋆. The deep neural
1Under review as submission to TMLR
network model uses bgradients of the loss functions per step. Hence, when Kis the number of steps required
to train a deep neural network, the model has a stochastic gradient computation cost of Kb. We will deﬁne
thestochastic ﬁrst-order oracle (SFO) complexity (Iiduka ,2022;Sato & Iiduka ,2023) of a deep-learning
optimizer to be N:=Kb. From the previous numerical results in ( Shallue et al. ,2019) and ( Zhang et al. ,
2019), the SFO complexity is minimized at a critical batch size b⋆and there are diminishing returns once
the batch size exceeds b⋆. Therefore, it is desirable to use the critical batch size when minimizing the SFO
complexity of the deep-learning optimizer.
Not only a batch size but also a learning rate aﬀects the performance of deep-learning optimizers for noncon-
vex optimization. A performance measure of a deep-learning optimizer generating a sequence (θk)k∈Nis the
expectation of the squared norm of the gradient of a nonconvex loss function f, denoted by E[∥∇f(θk)∥2].
If this performance measure becomes small when the number of steps kis large, the deep-learning optimizer
approximates a local minimizer of f. For example, let us consider the problem of minimizing a smooth
function f(see Section 2.1for the deﬁnition of smoothness). Here, SGD using a constant learning rate
α=O(1
L)satisﬁes min k∈[K]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
=O(1
K+α
b), where Lis the Lipschitz constant of ∇f,bis the
batch size, and [K] :={1,2, . . . , K}(see also Table 1). Moreover, SGD using a learning rate satisfying the
Armijo condition was presented in ( Vaswani et al. ,2019). The Armijo line search (Nocedal & Wright ,2006,
Chapter 3.1) is a standard method for ﬁnding an appropriate learning rate αkgiving a suﬃcient decrease in
f, i.e., f(θk+1)< f(θk)(see Section 2.3.1 for the deﬁnition of the Armijo condition).
1.2 Motivation
The numerical results in ( Vaswani et al. ,2019) indicated that the Armijo-line-search learning rate is superior
to using a constant learning rate when using SGD to train deep neural networks in the sense of minimiz-
ing the training loss functions and improving test accuracy. Motivated by the useful numerical results in
(Vaswani et al. ,2019), we decided to perform convergence analyses on SGD with the Armijo-line-search
learning rate for nonconvex optimization in deep neural networks.
Theorem 3 in ( Vaswani et al. ,2019) is a convergence analysis of SGD with the Armijo-line-search learning
rate for nonconvex optimization under a strong growth condition that implies the interpolation property.
Here, let f:Rd→Rbe an empirical risk deﬁned by f(θ) :=1
n/summationtext
i∈[n]fi(θ), where nis the number of
training data and fi:Rd→Ris a loss function corresponding to the i-th training data zi. We say that fhas
the interpolation property if ∇f(θ) =0implies∇fi(θ) =0(i∈[n]). The interpolation property holds for
optimization of a linear model with the squared hinge loss for binary classiﬁcation on linearly separable data
(Vaswani et al. ,2019, Section 2). However, the strong growth condition would be unrealistic for deep neural
networks, since their loss functions are nonconvex. The motivation behind this work is thus to show that
SGD with the Armijo-line-search learning rate can solve nonconvex optimization problems in deep neural
networks.
As indicated the second paragraph in Section 1.1, the batch size has a signiﬁcant eﬀect on the performance
of SGD. Hence, in accordance with the ﬁrst motivation stated above, we decided to investigate appropriate
batch sizes for SGD with the Armijo-line-search learning rate. In particular, we were interested in verifying
whether a critical batch size b⋆minimizing the SFO complexity Nexists for training deep neural networks
with SGD using the Armijo condition in theory and in practice. This is because the previous studies in
(Shallue et al. ,2019;Zhang et al. ,2019;Iiduka ,2022;Sato & Iiduka ,2023) showed the existence of critical
batch sizes for training deep neural networks or generative adversarial networks with optimizers with constant
or diminishing learning rates and without Armijo-line-search learning rates.
1.3 Contribution
1.3.1 Convergence analysis of SGD with Armijo-line-search learning rates
The ﬁrst contribution of this paper is to present a convergence analysis of SGD with Armijo-line-search
learning rates for general nonconvex optimization (Theorem 3.1); in particular, it is shown that SGD with
2Under review as submission to TMLR
this rate αksatisﬁes that, for all K≥1,
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤C1/bracehtipdownleft /bracehtipupright/bracehtipupleft /bracehtipdownright
2(f(θ0)−f∗)
˜α−(Lnα−1)α1
K/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B(θ0,K)+C2/bracehtipdownleft /bracehtipupright/bracehtipupleft /bracehtipdownright
Lnα2σ2
˜α−(Lnα−1)α1
b/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
V(σ2,b), (1)
where the parameters are deﬁned in Table 1(see also Theorem 3.1). The inequality ( 1) indicates that the
upper bound of the performance measure min k∈[0:K−1]E[∥∇f(θk)∥2]that consists of a bias term B(θ0, K)
and variance term V(σ2, b)becomes small when the number of steps Kis large and the batch size bis large.
Therefore, it is desirable to set Klarge and blarge so that Algorithm 1will approximate a local minimizer
off.
The essential lemma to proving ( 1) is the guarantee of the existence of a lower bound on the learning rates
satisfying the Armijo condition (Lemma 2.1). Although, in general, learning rates satisfying the Armijo
condition do not have any lower bound (Lemma 2.1(i)), the corresponding learning rates computed by a
backtracking line search (Algorithm 2) have a lower bound (Lemma 2.1(ii)). In addition, the descent lemma
(i.e., f(y)≤f(x) +⟨∇f(x),y−x⟩+Ln
2∥y−x∥2(x,y∈Rd)) holds from the smoothness condition on f.
Thus, we can prove ( 1) by using the existence of a lower bound on the learning rates satisfying the Armijo
condition and the descent lemma (see Appendix A.2for details of the proof of Theorem 3.1).
Table 1: Relationship between batch size band number of steps Kto achieve an ϵ–approximation deﬁned
bymin k∈[0:K−1]E[∥∇f(θk)∥2]≤C1
K+C2
b=ϵ2for SGD with a constant learning rate α∈(0,2
Ln)and for
SGD with the Armijo-line-search learning rate αk∈[α,α]([0 :K−1] :={0,1, . . . , K−1},f:=1
n/summationtext
i∈[n]fi
is bounded below by f∗,Liis the Lipschitz constant of ∇fi,Ln:=1
n/summationtext
i∈[n]Li,˜α:=2δ(1−c)
Ln,δ∈(1
4,1),
c∈(0,1−1
4δ), and σ2is the upper bound of the variance of the stochastic gradient)
Learning Rate Upper BoundC1
K+C2
bSteps K SFO N Critical Batch b⋆
Constant α∈/parenleftbigg
0,2
Ln/parenrightbiggC12(f(θ0)−f∗)
(2−Lnα)α K=C1b
ϵ2b−C2N=C1b2
ϵ2b−C2b⋆=2C2
ϵ2
C2Lnασ2
2−Lnα
Armijo α∈/parenleftbigg1
Ln,2
Ln/parenrightbiggC12(f(θ0)−f∗)
˜α−(Lnα−1)α K=C1b
ϵ2b−C2N=C1b2
ϵ2b−C2b⋆=2C2
ϵ2
C2Lnα2σ2
˜α−(Lnα−1)α
1.3.2 Steps needed for ϵ–approximation of SGD with Armijo line-search-learning rates
The previous results in ( Shallue et al. ,2019;Zhang et al. ,2019;Iiduka ,2022;Sato & Iiduka ,2023) indicated
that, for optimizers, the number of steps Kneeded to train a deep neural network or generative adversarial
networks decreases as the batch size increases. The second contribution of this paper is to show that, for
SGD with the Armijo-line-search learning rate, the number of steps Kneeded for nonconvex optimization
decreases as the batch size increases. Let us consider the case in which the right-hand side of ( 1) is equal to
ϵ2, where ϵ >0is the precision. Then, Kis a rational function deﬁned for a batch size bby
K=K(b) =C1b
ϵ2b−C2, (2)
where C1andC2are the positive constants deﬁned in ( 1) (see also Table 1). We can easily show that K
deﬁned above is a monotone decreasing and convex function with respect to b(Theorem 3.2). Accordingly,
the number of steps needed for nonconvex optimization decreases as the batch size increases.
3Under review as submission to TMLR
1.3.3 Critical batch size minimizing SFO complexity of SGD with Armijo-line-search learning rates
Using Kdeﬁned by ( 2) above, we can further deﬁne the SFO complexity Nof SGD with Armijo-line-search
learning rates (see also Table 1):
N=Kb=K(b)b=C1b2
ϵ2b−C2. (3)
We can easily show that Nis convex with respect to band that a global minimizer
b⋆=2C2
ϵ2=2Lnα2σ2
{˜α−(Lnα−1)α}ϵ2(4)
exists for it (Theorem 3.3). Accordingly, there is a critical batch size b⋆at which Nis minimized.
Here, we compare the number of steps KCand the SFO complexity NCfor SGD using a constant learning
rateαwith KAandNAfor SGD using the Armijo-line-search learning rate αk(∈[α,α]). Let C1,C(resp.
C2,C) be C1(resp. C2) in Table 1for SGD using a constant learning rate and let C1,A(resp. C2,A) be C1
(resp. C2) in Table 1for SGD using the Armijo-line-search learning rate. We have that
C1,A< C 1,Ciﬀ(2−Lnα)α < ˜α−(Lnα−1)α,
C2,A< C 2,Ciﬀα2σ2
A
˜α−(Lnα−1)α<ασ2
C
2−Lnα,(5)
where σ2
C(resp. σ2
A) denotes the upper bound of the variance of the stochastic gradient for SGD using
a constant learning rate α(resp. the Armijo-line-search learning rate). If ( 5) holds, then SGD using the
Armijo-line-search learning rate converges faster than SGD using a constant learning rate in the sense that
C1,Ab
ϵ2b−C2,A=KA< K C=C1,Cb
ϵ2b−C2,CandC1,Ab2
ϵ2b−C2,A=NA< N C=C1,Cb2
ϵ2b−C2,C.It would be diﬃcult to check exactly
that ( 5) holds before implementing SGD, since ( 5) involves unknown parameters, such as Ln=1
n/summationtext
i∈[n]Li,
σ2
C, and σ2
A. However, it can be expected that ( 5) holds, since it is known empirically ( Vaswani et al. ,2019,
Figure 5) that the relationship between the Armijo-line-search learning rate αkand a constant learning rate
αisα < α k<α.
1.3.4 Numerical results supporting our theoretical results
The numerical results in ( Vaswani et al. ,2019) showed that SGD with the Armijo-line-search learning rate
performs better than other optimizers in training deep neural networks. Hence, we sought to verify whether
the numerical results match our theoretical results (Sections 1.3.1,1.3.2, and 1.3.3). We trained residual
networks (ResNets) on the CIFAR-10 and MNIST datasets. We numerically found that increasing the
batch size bdecreases the number of steps Kneeded to train the DNN and that there are critical batch
sizes minimizing the SFO complexities. To verify whether SGD using the Armijo-line-search learning rate
performs better than SGD using a constant learning rate (see the discussion in condition ( 5)), we numerically
compared SGD using the Armijo-line-search learning rate with not only SGD using a constant learning rate
but also variants of SGD, such as the momentum method, Adam, AdamW, and RMSProp. We found
that SGD using the Armijo-line-search learning rate and the critical batch size performs better than other
optimizers in the sense of minimizing the number of steps and the SFO complexities needed to train the
DNN (Section 4).
2 Mathematical Preliminaries
2.1 Deﬁnitions
LetNbe the set of nonnegative integers, [n] :={1,2, . . . , n}forn≥1, and [0 :n] :={0,1, . . . , n}forn≥0.
LetRdbe a d–dimensional Euclidean space with inner product ⟨·,·⟩inducing the norm ∥·∥.
Letf:Rd→Rbe continuously diﬀerentiable. We denote the gradient of fby∇f:Rd→Rd. Let L > 0.
f:Rd→Ris said to be L–smooth if∇f:Rd→RdisL–Lipschitz continuous, i.e., for all x,y∈Rd,
4Under review as submission to TMLR
∥∇f(x)−∇f(y)∥≤L∥x−y∥. When f:Rd→RisL–smooth, the following inequality, called the descent
lemma ( Beck ,2017, Lemma 5.7), holds: for all x,y∈Rd,f(y)≤f(x) +⟨∇f(x),y−x⟩+L
2∥y−x∥2. Let
f∗∈R.f:Rd→Ris said to be bounded below by f∗if, for all x∈Rd,f(x)≥f∗.
2.2 Assumptions and problem
Given a parameter θ∈Rdand a data point zin a data domain Z, a machine learning model provides a
prediction whose quality can be measured by a diﬀerentiable nonconvex loss function f(θ;z). We aim to
minimize the empirical average loss deﬁned for all θ∈Rdbyf(θ) =1
n/summationtext
i∈[n]f(θ;zi) =1
n/summationtext
i∈[n]fi(θ),where
S= (z1, z2, . . . , z n)denotes the training set and fi(·) :=f(·;zi)denotes the loss function corresponding to
thei-th training data zi.
This paper considers the following smooth nonconvex optimization problem.
Problem 2.1 Suppose that fi:Rd→R(i∈[n])isLi–smooth and bounded below by fi,∗. Then,
minimize f(θ) :=1
n/summationdisplay
i∈[n]fi(θ)subject to θ∈Rd.
We assume that a stochastic ﬁrst-order oracle (SFO) exists such that, for a given θ∈Rd, it returns a
stochastic gradient Gξ(θ)of the function f, where a random variable ξis supported on a ﬁnite/an inﬁnite
setΞindependently of θ. We make the following standard assumptions.
Assumption 2.1
(A1) Let(θk)k∈N⊂Rdbe the sequence generated by SGD. For each iteration k,
Eξk[Gξk(θk)] =∇f(θk), (6)
where ξ0, ξ1, . . .are independent samples, the random variable ξkis independent of (θl)k
l=0, andEξk[·]
stands for the expectation with respect to ξk. There exists a nonnegative constant σ2such that
Eξk/bracketleftbig
∥Gξk(θk)−∇f(θk)∥2/bracketrightbig
≤σ2. (7)
(A2) For each iteration k, SGD samples a batch Bkof size bindependently of kand estimates the full
gradient∇fas
∇fBk(θk) :=1
b/summationdisplay
i∈[b]Gξk,i(θk) =1
b/summationdisplay
i∈[b]∇fξk,i(θk),
where ξk,iis a random variable generated by the i-th sampling in the k-th iteration.
From the independence of ξ0, ξ1, . . ., we can deﬁne the total expectation EbyE=Eξ0Eξ1···Eξk
2.3 Stochastic gradient descent using Armijo line search
2.3.1 Armijo condition
Suppose that f:Rd→Ris continuously diﬀerentiable. We would like to ﬁnd a stationary point θ⋆∈Rd
such that∇f(θ⋆) =0by using an iterative method deﬁned by
θk+1:=θk+αkdk, (8)
where αk>0is the step size (called a learning rate in the machine learning ﬁeld) and dk∈Rdis the search
direction. Various methods can be used depending on the search direction dk. For example, the method
(8) with dk:=−∇f(θk)is gradient descent, while the method ( 8) with dk:=−∇f(θk) +βk−1dk−1, where
5Under review as submission to TMLR
βk≥0, is the conjugate gradient method. If we deﬁne dk(e.g., dk:=−∇f(θk)), it is desirable to set α⋆
k
satisfying
f(θk+α⋆
kdk) = min
α>0f(θk+αdk). (9)
The step size α⋆
kdeﬁned by ( 9) can be easily computed when fis quadratic and convex. However, for a
general nonconvex function f, it is diﬃcult to compute the step size α⋆
kin (9) exactly. Here, we can use the
Armijo condition for ﬁnding an appropriate step size αk: Let c∈(0,1). We would like to ﬁnd αk>0such
that
f(θk+αkdk)≤f(θk) +cαk⟨∇f(θk),dk⟩. (10)
When dksatisﬁes the descent property deﬁned by ⟨∇f(θk),dk⟩<0(e.g., gradient descent using dk:=
−∇f(θk)has the property such that ⟨∇f(θk),dk⟩=−∥∇f(θk)∥2<0), the Armijo condition ensures that
f(θk+1) =f(θk+αkdk)< f(θk). Accordingly, αksatisfying the Armijo condition ( 10) is appropriate in the
sense of minimizing f.
The existence of step sizes satisfying the Armijo condition ( 10) is guaranteed.
Proposition 2.1 (Nocedal & Wright ,2006, Lemma 3.1) Letf:Rd→Rbe continuously diﬀerentiable. Let
θk∈Rdand let dk(̸=0)have the descent property deﬁned by ⟨∇f(θk),dk⟩<0. Let c∈(0,1). Then, there
exists γk>0such that, for all αk∈(0, γk], the Armijo condition ( 10) holds.
2.3.2 Stochastic gradient descent under Armijo condition
The objective of this paper is to solve Problem 2.1using mini-batch SGD under Assumption 2.1deﬁned by
θk+1=θk+αkdk=θk−αk∇fBk(θk)
=θk−αk
b/summationdisplay
i∈[b]Gξk,i(θk),
where b >0is the batch size and αk>0is the learning rate. For each iteration k, we can use θk,fBk, and
∇fBk. Hence, the Armijo condition ( Vaswani et al. ,2019, (1)) at the k-th iteration for SGD can be obtained
by replacing fin (10) with fBkand using dk=−∇fBk(θk):
fBk(θk−αk∇fBk(θk))≤fBk(θk)−cαk∥∇fBk(θk)∥2. (11)
The Armijo condition ( 11) ensures that fBk(θk+1) =fBk(θk−αk∇fBk(θk))< f Bk(θk); i.e., the Armijo
condition ( 11) is appropriate in the sense of minimizing the estimated objective function fBkfrom the full
objective function f. In fact, the numerical results in ( Vaswani et al. ,2019, Section 7) indicate that SGD
using the Armijo condition ( 11) is superior to using other deep-learning optimizers to train deep neural
networks.
Algorithm 1is the SGD algorithm using the Armijo condition ( 11).
Algorithm 1 Stochastic gradient descent using Armijo line search
Require: c∈(0,1)(hyperparameter), b >0(batch size), θ0∈Rd(initial point), K≥1(steps)
Ensure: θK∈Rd
k←0
fork= 0,1, . . . , K−1do
Compute αk>0satisfying fBk(θk−αk∇fBk(θk))≤fBk(θk)−cαk∥∇fBk(θk)∥2◁Algorithm 2
Compute θk+1=θk−αk∇fBk(θk)
end for
The search direction of Algorithm 1isdk=−∇fBk(θk) (̸=0)which has the descent property deﬁned by
⟨∇fBk(θk),dk⟩=−∥∇fBk(θk)∥2<0. Hence, from Proposition 2.1, there exists a learning rate αk∈(0, γk]
6Under review as submission to TMLR
satisfying the Armijo condition ( 11). Moreover, the proposition guarantees that the learning rate can be
chosen to be suﬃciently small, e.g., lim inf k→+∞αk= 0.
The convergence analyses of Algorithm 1use a lower bound of αk∈(0, γk]satisfying the Armijo condition
(11). To guarantee the existence of such a lower bound, we use the backtracking method (( Nocedal & Wright ,
2006, Algorithm 3.1) and ( Vaswani et al. ,2019, Algorithm 2)) described in Algorithm 2.
Algorithm 2 Backtracking Armijo-line-search method ( Nocedal & Wright ,2006, Algorithm 3.1)
Require: c, δ,1
γ∈(0,1)(hyperparameters), α(Initialization, see Algorithm 3),θk∈Rd,fBk:Rd→R
Ensure: αksatisfying fBk(θk−αk∇fBk(θk))≤fBk(θk)−cαk∥∇fBk(θk)∥2
repeat
α←δα
until fBk(θk−α∇fBk(θk))≤fBk(θk)−cα∥∇fBk(θk)∥2
The following lemma guarantees the existence of a lower bound on the learning rates computed by Algorithm
2. The proof is given in Appendix A.1.
Lemma 2.1 Consider Algorithm 1under Assumption 2.1for solving Problem 2.1. Let αkbe a learning
rate satisfying the Armijo condition ( 11) (whose existence is guaranteed by Proposition 2.1), let LBkbe the
Lipschitz constant of ∇fBk. Then, the following hold.
(i)[Counter-example of ( Vaswani et al. ,2019, Lemma 1)] There exists Problem 2.1such that αkdoes
not satisfy min{2(1−c)
LBk,α}≤αk, where αis an upper bound of αk.
(ii)[Lower bound on learning rate determined by backtracking line search method] Ifαkcan be computed
by Algorithm 2, then there exists a lower bound of αksuch that 0< α :=2δ(1−c)
L≤αk, where Lis
the maximum value of Li.
3 Analysis of SGD using Armijo Line Search
3.1 Convergence analysis of Algorithm 1
Here, we present a convergence analysis of Algorithm 1. The proof of Theorem 3.1is given in Appendix A.2.
Theorem 3.1 (Upper bound of the squared norm of the full gradient) Consider the sequence
(θk)k∈Ngenerated by Algorithm 1under Assumption 2.1for solving Problem 2.1and suppose that the
learning rate αk∈[α,α]is computed by Algorithm 2. Then, for all K≥1, the following hold:
(i)In the case of1
Ln≥α,
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤C1/bracehtipdownleft /bracehtipupright/bracehtipupleft /bracehtipdownright
2(f(θ0)−f∗)
(2−Lnα)α1
K/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B(θ0,K)+C2/bracehtipdownleft/bracehtipupright/bracehtipupleft /bracehtipdownright
Lnα2σ2
(2−Lnα)α1
b/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
V(σ2,b),
where Ln:=1
n/summationtext
i∈[n]Li,f∗:=1
n/summationtext
i∈[n]fi,∗,α:=2δ(1−c)
L,δ∈(0,1), and c∈(0,1).
(ii)Suppose that the random variable ξkfollows a discrete uniform distribution DUb(n). In the case of
1
Ln<α,
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤C1/bracehtipdownleft /bracehtipupright/bracehtipupleft /bracehtipdownright
2(f(θ0)−f∗)
˜α−(Lnα−1)α1
K/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B(θ0,K)+C2/bracehtipdownleft /bracehtipupright/bracehtipupleft /bracehtipdownright
Lnα2σ2
˜α−(Lnα−1)α1
b/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
V(σ2,b),
where Ln:=1
n/summationtext
i∈[n]Li,f∗:=1
n/summationtext
i∈[n]fi,∗,˜α:=2δ(1−c)
Ln,δ∈(1
4,1), and c∈(0,1−1
4δ).
7Under review as submission to TMLR
Theorem 3.1indicates that the upper bound of the minimum value of E[∥∇f(θk)∥2]consists of a bias term
B(θ0, K)and variance term V(σ2, b). When the number of steps Kis large and the batch size bis large,
B(θ0, K)andV(σ2, b)become small. Therefore, we need to set Klarge and blarge so that Algorithm 1will
approximate a local minimizer of f.
Here, we compare Theorem 3.1with the convergence analysis of SGD using a constant learning rate. SGD
using a constant learning rate α∈(0,2
Ln)satisﬁes
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤2(f(θ0)−f∗)
(2−Lnα)α1
K+Lnασ2
2−Lnα1
b(12)
(The proof of ( 12) is given in Appendix A.5). We need to set a constant learning rate α∈(0,2
Ln)depending
on the Lipschitz constant Lnof∇f. However, since computing Lnis NP-hard ( Virmaux & Scaman ,2018),
it is diﬃcult to set α∈(0,2
Ln). Meanwhile, from Theorem 3.1(ii), we need to set c, δ∈(0,1)in Algorithm
2such that δ∈(1
4,1)andc∈(0,1−1
4δ)(see Section 4for the performance of Algorithms 1and2using
δ= 0.9and small parameters c).
We also compare Theorem 3.1with Theorem 3 in ( Vaswani et al. ,2019). Theorem 3 in ( Vaswani et al. ,
2019) indicates that, under a strong growth condition with a constant ρ(i.e.,Ei[∥∇fi(θ)∥2]≤ρ∥∇f(θ)∥2
(θ∈Rd)) and the Armijo condition, SGD satisﬁes that
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤f(θ0)−f(θ⋆)
∆K,
where Lis the maximum value of the Lipschitz constant Liof∇fi,c > 1−L
ρLn,α <2
ρLn,∆ := ( α+
2(1−c)
L)−ρ(α−2(1−c)
L+Lnα2), and θ⋆is a local minimizer of f. Theorem 3.1is a convergence analysis of
Algorithm 1without assuming the strong growth condition. Moreover, Theorem 3.1shows that using large
batch size is appropriate for SGD using the Armijo line search (Algorithm 1).
3.2 Steps needed for ϵ–approximation
To investigate the relationship between the number of steps Kneeded for nonconvex optimization and the
batch size b, we consider an ϵ–approximation of Algorithm 1deﬁned as follows:
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤ϵ2, (13)
where ϵ >0is the precision.
Theorem 3.1leads to the following theorem indicating the relationship between band the values of Kthat
achieves an ϵ–approximation. The proof of Theorem 3.2is given in Appendix A.3.
Theorem 3.2 (Steps needed for nonconvex optimization of SGD using Armijo line search)
Suppose that the assumptions in Theorem 3.1hold. Deﬁne K:R→Rfor all b >C2
ϵ2by
K(b) =C1b
ϵ2b−C2, (14)
where the positive constants C1andC2are deﬁned as in Theorem 3.1. Then, the following hold:
(i)[Steps needed for nonconvex optimization] Kdeﬁned by ( 14) achieves an ϵ–approximation ( 13).
(ii)[Properties of the steps] Kdeﬁned by ( 14) is monotone decreasing and convex for b >C2
ϵ2.
Theorem 3.2ensures that the number of steps Kneeded for SGD using the Armijo line search to be an
ϵ–approximation is small when the batch size bis large. Therefore, it is useful to set a suﬃciently large
batch size in the sense of minimizing the steps needed for an ϵ–approximation of SGD using the Armijo line
search.
8Under review as submission to TMLR
3.3 Critical batch size minimizing SFO complexity
The following theorem shows the existence of a critical batch size for SGD using the Armijo line search. The
proof of Theorem 3.3is given in Appendix A.4.
Theorem 3.3 (Existence of critical batch size for SGD using Armijo line search) Suppose that
the assumptions in Theorem 3.1hold. Deﬁne SFO complexity N:R→Rfor the number of steps K, deﬁned
by (14), needed for an ϵ–approximation ( 13) and for a batch size b >C2
ϵ2by
N(b) =K(b)b=C1b2
ϵ2b−C2, (15)
where the positive constants C1andC2are deﬁned as in Theorem 3.1(ii). Then, the following hold:
(i)[SFO complexity] Ndeﬁned by ( 15) is convex for b >C2
ϵ2.
(ii)[Critical batch size] There exists a critical batch size
b⋆=2C2
ϵ2=σ2
ϵ2L2
nα2
{2(1−c)δ−(Lnα−1)Lnα}(16)
such that b⋆minimizes the SFO complexity ( 15).
The previous results in ( Shallue et al. ,2019;Zhang et al. ,2019;Iiduka ,2022) show that, for deep-learning
optimizers, there are critical batch sizes at which the SFO complexities are minimized. We are interested in
verifying whether a critical batch size exists for SGD using the Armijo line search. Theorem 3.3(ii) indicates
that the critical batch size can be obtained from the hyperparameters. The next section numerically examines
the relationship between the batch size band the number of steps Kneeded for nonconvex optimization and
the relationship between band the SFO complexity Nto check if there is a critical batch size b⋆minimizing
N.
4 Numerical Results
We veriﬁed whether numerical results match our theoretical results (Theorems 3.2and3.3). We also com-
pared the performance of Algorithm 1with the performances of other optimizers, such as SGD with a constant
learning rate (SGD), momentum method (Momentum), Adam, AdamW, and RMSProp. The learning rate
and hyperparameters of the ﬁve optimizers used in each experiment were determined on the basis of a grid
search.
The metrics were the number of steps Kand the SFO complexity N=Kbindicating for diﬀerent batch
sizes b, at time k, the number of steps Kneeded for the average gradient norm over the past ksteps to be
less than ϵ= 0.5. We used Algorithm 1with the Armijo-line-search learning rate computed by Algorithm
2with δ= 0.9,α= 10 (see https://github.com/IssamLaradji/sls for the setting of parameters), and
various values of c. We reset the step size each epoch by using Algorithm 3with J=⌈n
b⌉.
Algorithm 3 Reset step size
Require: αinit,J,αk−1, . . . , α k−J+1
ifk≤Jthen
αk=αinit
else
αk=2
J/summationtextJ+1
j=1αk−j
end if
return αk
We trained ResNet-18 on the CIFAR-10 dataset ( n= 50000 ). Figure 1plots the number of steps needed
to train ResNet-18 on the CIFAR-10 dataset for Algorithm 1versus the batch size. It can be seen that
9Under review as submission to TMLR
Algorithm 1decreases the number of steps as the batch size increases. Figure 2plots the SFO complexities
of Algorithm 1versus the batch size. It indicates that there are critical batch sizes that minimize the SFO
complexities.
Figure 3and Figure 4compare the performance of Algorithm 1with c= 0.1with those of SGD variants.
The ﬁgures indicate that, when the batch sizes are from 25to211, SGD+Armijo (Algorithm 1) performs
better than the other optimizers. In particular, the SFO complexity of SGD+Armijo (Algorithm 1) using
c= 0.1and the critical batch size ( b⋆= 28) is the smallest of the optimizers for any batch size.
2526272829210211
Batch Size104105Steps
c=0.1 c=0.01 c=0.001
Figure 1: Number of steps for Algorithm 1versus
batch size needed to train ResNet-18 on CIFAR-
10
2526272829210211
Batch Size107SFO
c=0.1 c=0.01 c=0.001Figure 2: SFO complexity for Algorithm 1versus
batch size needed to train ResNet-18 on CIFAR-
10 (The double-circle symbol denotes the mea-
sured critical batch size)
2526272829210211
Batch Size104105Steps
SGD+Armijo
SGDMomentum
AdamAdamW
RMSProp
Figure 3: Number of steps for Algorithm 1with
c= 0.1and variants of SGD versus batch size
needed to train ResNet-18 on CIFAR-10
2526272829210211
Batch Size107SFO
SGD+Armijo
SGDMomentum
AdamAdamW
RMSPropFigure 4: SFO complexity for Algorithm 1with
c= 0.1and variants of SGD versus batch size
needed to train ResNet-18 on CIFAR-10 (The
double-circle symbol denotes the measured criti-
cal batch size)
Therefore, we can conclude that Algorithm 1using the critical batch size b⋆(= 28) performs better than other
optimizers using any batch size in the sense of minimizing the SFO complexities needed to train ResNet18
on the CIFAR-10 dataset.
We also considered the case of training ResNet-18 on the MNIST dataset ( n= 60000 ). Figure 5plots the
number of steps needed to train ResNet-18 on the MNIST dataset for Algorithm 1versus the batch size, and
Figure 6plots the SFO complexities of Algorithm 1versus the batch size. Here, there are critical batch sizes
that minimize the SFO complexities. As in Figures 5and6, these ﬁgures show that Algorithm 1 decreases
the number of steps as the batch size increases and there are critical batch sizes that minimize the SFO
complexities.
10Under review as submission to TMLR
Figure 7and Figure 8compare the performance of Algorithm 1with c= 0.1with those of SGD variants.
The ﬁgures indicate that, when the batch sizes are from 25to211, SGD+Armijo (Algorithm 1) performs as
well as the other optimizers.
2526272829210211
Batch Size103104Steps
c=0.10 c=0.01 c=0.001
Figure 5: Number of steps for Algorithm 1versus
batch size needed to train ResNet-18 on MNIST
2526272829210211
Batch Size106
4×1056×105SFO
c=0.10 c=0.01 c=0.001Figure 6: SFO complexity for Algorithm 1versus
batch size needed to train ResNet-18 on MNIST
(The double-circle symbol denotes the measured
critical batch size)
2526272829210211
Batch Size103104Steps
SGD+Armijo
SGDMomentum
AdamAdamW
RMSProp
Figure 7: Number of steps for Algorithm 1with
c= 0.1and variants of SGD versus batch size
needed to train ResNet-18 on MNIST
2526272829210211
Batch Size105106SFO
SGD+Armijo
SGDMomentum
AdamAdamW
RMSPropFigure 8: SFO complexity for Algorithm 1with
c= 0.1and variants of SGD versus batch
size needed to train ResNet-18 on MNIST (The
double-circle symbol denotes the measured criti-
cal batch size)
Therefore, we can conclude that Algorithm 1using the critical batch size b⋆(= 29) performs as well as other
optimizers using any batch size in the sense of minimizing the SFO complexities needed to train ResNet18
on the MNIST dataset.
5 Conclusion
This paper presented a convergence analysis of SGD using the Armijo line search for nonconvex optimization.
We showed that the number of steps needed for nonconvex optimization is monotone decreasing and convex
with respect to the batch size; i.e., the steps decrease in number as the batch size increases. We also showed
that the SFO complexity needed for nonconvex optimization is convex with respect to the batch size and
that there exists a critical batch size at which the SFO complexity is minimized.
References
Amir Beck. First-Order Methods in Optimization . Society for Industrial and Applied Mathematics, Philadel-
phia, PA, 2017.
11Under review as submission to TMLR
Hao Chen, Lili Zheng, Raed AL Kontar, and Garvesh Raskutti. Stochastic gradient descent in correlated set-
tings: A study on Gaussian processes. In Advances in Neural Information Processing Systems , volume 33,
2020.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research , 12:2121–2159, 2011.
Benjamin Fehrman, Benjamin Gess, and Arnulf Jentzen. Convergence rates for the stochastic gradient
descent method for non-convex objective functions. Journal of Machine Learning Research , 21:1–48, 2020.
Leonardo Galli, Holger Rauhut, and Mark Schmidt. Don’t be so monotone: Relaxing stochastic line search
in over-parameterized models. arXiv preprint arXiv:2306.12747 , 2023.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochas-
tic composite optimization I: A generic algorithmic framework. SIAM Journal on Optimization , 22:1469–
1492, 2012.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochas-
tic composite optimization II: Shrinking procedures and optimal algorithms. SIAM Journal on Optimiza-
tion, 23:2061–2089, 2013.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems , volume 30, 2017.
Hideaki Iiduka. Critical bach size minimizes stochastic ﬁrst-order oracle complexity of deep learning optimizer
using hyperparameters close to one. arXiv: 2208.09814, 2022.
Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Parallelizing
stochastic gradient descent for least squares regression: Mini-batching, averaging, and model misspeciﬁ-
cation. Journal of Machine Learning Research , 18(223):1–42, 2018.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of The
International Conference on Learning Representations , 2015.
Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien. Stochastic polyak step-size for
SGD: An adaptive learning rate for fast convergence. In Proceedings of the 24th International Conference
on Artiﬁcial Intelligence and Statistics , volume 130, 2021.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of The International
Conference on Learning Representations , 2019.
Hiroki Naganuma and Hideaki Iiduka. Conjugate gradient method for generative adversarial networks. In
Proceedings of The 26th International Conference on Artiﬁcial Intelligence and Statistics , volume 206 of
Proceedings of Machine Learning Research , pp. 4381–4408. PMLR, 2023.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approxima-
tion approach to stochastic programming. SIAM Journal on Optimization , 19:1574–1609, 2009.
Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of convergence
O(1/k2).Doklady AN USSR , 269:543–547, 1983.
J. Nocedal and S. J. Wright. Numerical Optimization . Springer Series in Operations Research and Financial
Engineering. Springer, New York, 2nd edition, 2006.
Boris T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational
Mathematics and Mathematical Physics , 4:1–17, 1964.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In Proceedings
of The International Conference on Learning Representations , 2018.
12Under review as submission to TMLR
Herbert Robbins and Herbert Monro. A stochastic approximation method. The Annals of Mathematical
Statistics , 22:400–407, 1951.
Naoki Sato and Hideaki Iiduka. Existence and estimation of critical batch size for training generative
adversarial networks with two time-scale update rule. In Proceedings of the 40th International Conference
on Machine Learning , volume 202 of Proceedings of Machine Learning Research , pp. 30080–30104. PMLR,
23–29 Jul 2023.
Kevin Scaman and Cédric Malherbe. Robustness analysis of non-convex stochastic gradient descent using
biased expectations. In Advances in Neural Information Processing Systems , volume 33, 2020.
Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E.
Dahl. Measuring the eﬀects of data parallelism on neural network training. Journal of Machine Learning
Research , 20:1–49, 2019.
Tijmen Tieleman and Geoﬀrey Hinton. RMSProp: Divide the gradient by a running average of its recent
magnitude. COURSERA: Neural networks for machine learning , 4:26–31, 2012.
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-Julien.
Painless stochastic gradient: Interpolation, line-search, and convergence rates. In Advances in Neural
Information Processing Systems , volume 32, 2019.
Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: analysis and eﬃcient
estimation. In Advances in Neural Information Processing Systems , volume 31, 2018.
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl, Christopher J.
Shallue, and Roger Grosse. Which algorithmic choices matter at which batch sizes? Insights from a noisy
quadratic model. In Advances in Neural Information Processing Systems , volume 32, 2019.
Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings
of the 20th International Conference on Machine Learning , pp. 928–936, 2003.
13Under review as submission to TMLR
A Appendix
A.1 Proof of Lemma 2.1
(i) Let k∈Nand let LBkbe the Lipschitz constant of ∇fBk. Lemma 1 in ( Vaswani et al. ,2019) is as follows:
∀fBk:Rd→R∀c∈(0,1)∀θk∈Rd∀α > 0
∃αk∈(0,α] (fBk(θk−αk∇fBk(θk))≤fBk(θk)−cαk∥∇fBk(θk)∥2)
⇒min/braceleftbigg2(1−c)
LBk,α/bracerightbigg
≤αk.(17)
The negative proposition of ( 17) is as follows:
∃fBk:Rd→R∃c∈(0,1)∃θk∈Rd∃α > 0
∃αk∈(0,α] (fBk(θk−αk∇fBk(θk))≤fBk(θk)−cαk∥∇fBk(θk)∥2)
∧min/braceleftbigg2(1−c)
LBk,α/bracerightbigg
> α k.(18)
We will prove that ( 18) holds. Let n=b= 1,d= 1,c= 0.1,α= 1, and f(θ) =fBk(θ) =θ2. From
∇f(θ) = 2 θ, we have that LBk= 2. Since θ∗= 0 is the global minimizer of f, we set θk∈Rsuch that
θk̸=θ∗. The Armijo condition in this case is such that (θk−2αkθk)2≤θ2
k−cαk(2θk)2, which is equivalent
toαk≤1−c= 0.9. Hence,
∃αk∈(0,1] (αk≤0.9)∧(min{0.9,1}> α k)
⇔∃αk∈(0,α] (αk≤1−c)∧/parenleftbigg
min/braceleftbigg2(1−c)
LBk,α/bracerightbigg
> α k/parenrightbigg
⇔∃αk∈(0,α] (fBk(θk−αk∇fBk(θk))≤fBk(θk)−cαk∥∇fBk(θk)∥2)
∧min/braceleftbigg2(1−c)
LBk,α/bracerightbigg
> α k,
which implies that ( 18) holds.
(ii) We can show Lemma 2.1(ii) using the proof (Case 2) of Lemma 1 in ( Galli et al. ,2023). Sinceαk
δdoes
not satisfy the Armijo condition ( 11), we have that
fBk/parenleftBig
θk−αk
δ∇fBk(θk)/parenrightBig
> fBk(θk)−cαk
δ∥∇fBk(θk)∥2. (19)
TheLBk–smoothness of fBkensures that the descent lemma is true, i.e.,
fBk/parenleftBig
θk−αk
δ∇fBk(θk)/parenrightBig
≤fBk(θk) +/angbracketleftBig
∇fBk(θk),/parenleftBig
θk−αk
δ∇fBk(θk)/parenrightBig
−θk/angbracketrightBig
+LBk
2/vextenddouble/vextenddouble/vextenddouble/parenleftBig
θk−αk
δ∇fBk(θk)/parenrightBig
−θk/vextenddouble/vextenddouble/vextenddouble2
,
which implies that
fBk/parenleftBig
θk−αk
δ∇fBk(θk)/parenrightBig
≤fBk(θk) +αk
δ/parenleftbiggLBkαk
2δ−1/parenrightbigg
∥∇fBk(θk)∥2. (20)
Hence, ( 19) and ( 20) imply that
−cαk
δ∥∇fBk(θk)∥2≤αk
δ/parenleftbiggLBkαk
2δ−1/parenrightbigg
∥∇fBk(θk)∥2,
which in turn implies that
αk
δ/parenleftbiggLBkαk
2δ−(1−c)/parenrightbigg
∥∇fBk(θk)∥2≥0.
14Under review as submission to TMLR
Accordingly,
LBkαk
2δ−(1−c)≥0,i.e.,αk≥2δ(1−c)
LBk≥2δ(1−c)
L=:α.
A.2 Proof of Theorem 3.1
The deﬁnition of f(θ) :=1
n/summationtext
i∈[n]fi(θ)and the Li–smoothness of fi(i∈[n])imply that, for all θ1,θ2∈Rd,
∥∇f(θ1)−∇f(θ2)∥≤1
n/summationdisplay
i∈[n]∥∇fi(θ1)−∇fi(θ2)∥≤/summationtext
i∈[n]Li
n∥θ1−θ2∥,
which in turn implies that ∇fis Lipschitz continuous with Lipschitz constant Ln:=1
n/summationtext
i∈[n]Li. Hence,
the descent lemma ensures that, for all k∈N,
f(θk+1)≤f(θk) +⟨∇f(θk),θk+1−θk⟩+Ln
2∥θk+1−θk∥2,
which, together with θk+1:=θk−αk∇fBk(θk), implies that
f(θk+1)≤f(θk)−αk⟨∇f(θk),∇fBk(θk)⟩+Lnα2
k
2∥∇fBk(θk)∥2. (21)
From⟨x,y⟩=1
2(∥x∥2+∥y∥2−∥x−y∥2) (x,y∈Rd), we have that, for all k∈N,
⟨∇f(θk),∇fBk(θk)⟩=1
2/parenleftbig
∥∇f(θk)∥2+∥∇fBk(θk)∥2−∥∇ f(θk)−∇fBk(θk)∥2/parenrightbig
.
Accordingly, ( 21) implies that, for all k∈N,
f(θk+1)≤f(θk)−αk
2/parenleftbig
∥∇f(θk)∥2+∥∇fBk(θk)∥2−∥∇ f(θk)−∇fBk(θk)∥2/parenrightbig
+Lnα2
k
2∥∇fBk(θk)∥2
=f(θk)−αk
2∥∇f(θk)∥2+1
2(Lnαk−1)αk∥∇fBk(θk)∥2+αk
2∥∇f(θk)−∇fBk(θk)∥2.
(i) We consider the case of1
Ln≥α. The condition 0< α k≤αimplies that Lnαk−1≤Lnα−1and
0≥Lnα−1. From 0< α =2δ(1−c)
L≤αk, we have that, for all k∈N,
f(θk+1)≤f(θk)−α
2∥∇f(θk)∥2+1
2(Lnα−1)α∥∇fBk(θk)∥2+α
2∥∇f(θk)−∇fBk(θk)∥2. (22)
Assumption 2.1guarantees that
Eξk[∇fBk(θk)|θk] =∇f(θk)andEξk/bracketleftbig
∥∇fBk(θk)−∇f(θk)∥2|θk/bracketrightbig
≤σ2
b. (23)
Hence, we have
Eξk/bracketleftbig
∥∇fBk(θk)∥2|θk/bracketrightbig
=Eξk/bracketleftbig
∥∇fBk(θk)−∇f(θk) +∇f(θk)∥2|θk/bracketrightbig
=Eξk/bracketleftbig
∥∇fBk(θk)−∇f(θk)∥2|θk/bracketrightbig
+ 2Eξk[⟨∇fBk(θk)−∇f(θk),∇f(θk)⟩|θk] +Eξk/bracketleftbig
∥∇f(θk)∥2|θk/bracketrightbig
≤∥∇ f(θk)∥2+σ2
b.(24)
Inequalities ( 22), (23), and ( 24) guarantee that, for all k∈N,
Eξk[f(θk+1)|θk]≤f(θk)−α
2∥∇f(θk)∥2+1
2(Lnα−1)α/parenleftbigg
∥∇f(θk)∥2+σ2
b/parenrightbigg
+ασ2
2b
=f(θk)−α
2∥∇f(θk)∥2−/braceleftbigg(Lnα−1)α
2/bracerightbigg
∥∇f(θk)∥2+Lnα2σ2
2b.(25)
15Under review as submission to TMLR
Taking the total expectation on both sides of ( 25) thus ensures that, for all k∈N,
α−(Lnα−1)α
2≤E[f(θk)−f(θk+1)] +Lnα2σ2
2b. (26)
LetK≥1. Summing ( 26) from k= 0tok=K−1ensures that
α−(Lnα−1)α
2K−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤E[f(θ0)−f(θK)] +Lnα2σ2K
2b,
which, together with the boundedness of f, i.e., f∗≤f(θk), implies that
α−(Lnα−1)α
2K−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤E[f(θ0)−f∗] +Lnα2σ2K
2b.
Accordingly,
1
KK−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤2(f(θ0)−f∗)
{α−(Lnα−1)α}K+Lnα2σ2
{α−(Lnα−1)α}b.
Moreover, since we have
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤1
KK−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
,
the assertion in Theorem 3.1(i) holds.
(ii) Let us consider the case of1
Ln<α. From 0< α k≤α, we have that, for all k∈N,Lnαk−1≤Lnα−1
and 0< L nα−1. For all k∈N,
f(θk+1)≤f(θk)−αk
2∥∇f(θk)∥2+1
2(Lnα−1)α∥∇fBk(θk)∥2+α
2∥∇f(θk)−∇fBk(θk)∥2. (27)
Inequalities ( 23), (24), and ( 27) guarantee that, for all k∈N,
Eξk[f(θk+1)|θk]≤f(θk)−Eξk/bracketleftBigαk
2∥∇f(θk)∥2/vextendsingle/vextendsingle/vextendsingleθk/bracketrightBig
+1
2(Lnα−1)α/parenleftbigg
∥∇f(θk)∥2+σ2
b/parenrightbigg
+ασ2
2b. (28)
Since ξkandθk(ξk−1)are independent, we have that
Eξk/bracketleftBigαk
2∥∇f(θk)∥2/vextendsingle/vextendsingle/vextendsingleθk/bracketrightBig
=Eξk/bracketleftBigαk
2∥∇f(θk)∥2/bracketrightBig
=1
2∥∇f(θk)∥2Eξk[αk].
Hence, ( 28) implies that
Eξk[f(θk+1)]≤f(θk)−1
2Eξk[αk]∥∇f(θk)∥2+(Lnα−1)α
2∥∇f(θk)∥2+Lnα2σ2
2b. (29)
Here, let us assume that ξk∼DUb(n). Then, we have that
Eξk[LBk] =Eξk/bracketleftBigg
1
bb/summationdisplay
i=1Lξk,i/bracketrightBigg
=1
bb/summationdisplay
i=1Eξk,i/bracketleftbig
Lξk,i/bracketrightbig
=1
bb/summationdisplay
i=1n/summationdisplay
j=1LjP(ξk,i=j) =1
bb/summationdisplay
i=11
nn/summationdisplay
j=1Lj=Ln.
Moreover, Jensen’s inequality implies that
Eξk[αk]≥Eξk/bracketleftbigg2δ(1−c)
LBk/bracketrightbigg
≥2δ(1−c)
Eξk[LBk]=2δ(1−c)
Ln.
16Under review as submission to TMLR
Hence, ( 29) ensures that, for all k∈N,
Eξk[f(θk+1)]≤f(θk)−1
22δ(1−c)
Ln/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
˜α∥∇f(θk)∥2+(Lnα−1)α
2∥∇f(θk)∥2+Lnα2σ2
2b. (30)
Taking the total expectation on both sides of ( 30) thus ensures that, for all k∈N,
˜α−(Lnα−1)α
2E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤E[f(θk)−f(θk+1)] +Lnα2σ2
2b. (31)
LetK≥1. Summing ( 31) from k= 0tok=K−1ensures that
˜α−(Lnα−1)α
2K−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤E[f(θ0)−f(θK)] +Lnα2σ2K
2b,
which, together with the boundedness of f, i.e., f∗≤f(θk), implies that
˜α−(Lnα−1)α
2K−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤E[f(θ0)−f∗] +Lnα2σ2K
2b.
Letδ∈(1
4,1),c∈(0,1−1
4δ), and
ˆα:=1 +/radicalbig
1 + 8(1−c)δ
2Ln<
[1−c<1
δ]2
Ln.
Then, we have that 4(1−c)δ >1,1−1
δ<0< c, and 1−c <1
δ. Hence,
2(1−c)δ <2⇔2(1−c)δ−1<1⇔8(1−c)δ{2(1−c)δ−1}+ 1<8(1−c)δ+ 1
⇔{4(1−c)δ−1}2<8(1−c)δ+ 1⇔4(1−c)δ <1 +/radicalbig
8(1−c)δ+ 1
⇔ˆα:=1 +/radicalbig
1 + 8(1−c)δ
2Ln>˜α:=2δ(1−c)
Ln.
Since ˜α <α < ˆα, we have that ˜α−(Lnα−1)α > 0. Therefore, we have
1
KK−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤2(f(θ0)−f∗)
{α−(Lnα−1)α}K+Lnα2σ2
{α−(Lnα−1)α}b.
A.3 Proof of Theorem 3.2
(i) We have
C1
K+C2
b=ϵ2
is equivalent to
K=K(b) =C1b
ϵ2b−C2.
Hence, Theorem 3.1leads to an ϵ–approximation.
(ii) We have
dK(b)
db=−C1C2
(ϵ2b−C2)2≤0andd2K(b)
db2=2C1C2ϵ2
(ϵ2b−C2)3≥0,
which implies that Kis monotone decreasing and convex with respect to b.
17Under review as submission to TMLR
A.4 Proof of Theorem 3.3
(i) From
N(b) =C1b2
ϵ2b−C2,
we have
dN(b)
db=C1b(ϵ2b−2C2)
(ϵ2b−C2)2andd2N(b)
db2=2C1C2
2
(ϵ2b−C2)3≥0,
which implies that Nis convex with respect to b.
(ii) We have
dN(b)
db

<0ifb < b⋆,
= 0 ifb=b⋆=2C2
ϵ2,
>0ifb > b⋆.
Hence, the point b⋆minimizes N.
From Theorem 3.1(ii), we have that
Ln:=1
n/summationdisplay
i∈[n]Li=2δ(1−c)
˜α.
Hence,
b⋆=2C2
ϵ2=2Lnα2σ2
{˜α−(Lnα−1)α}ϵ2=2Lnα2σ2
{(2δ(1−c)/Ln)−(Lnα−1)α}ϵ2
=σ2
ϵ2L2
nα2
{2(1−c)δ−(Lnα−1)Lnα}
A.5 Proof of ( 12)
LetK≥1. From ( 21) and αk:=α > 0, we have that, for all k∈N,
f(θk+1)≤f(θk)−α⟨∇f(θk),∇fBk(θk)⟩+Lnα2
2∥∇fBk(θk)∥2.
Hence, ( 23) and ( 24) ensure that, for all k∈N,
E[f(θk+1)]≤E[f(θk)]−αE/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
+Lnα2
2/parenleftbigg
E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
+σ2
b/parenrightbigg
,
which implies that, for all k∈N,
α/parenleftbigg
1−Lnα
2/parenrightbigg
E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤E[f(θk)−f(θk+1)] +Lnα2σ2
2b.
Summing the above inequalities from k= 0tok=K−1ensures that
α/parenleftbigg
1−Lnα
2/parenrightbiggK−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤E[f(θ0)−f(θK)] +Lnα2σ2K
2b.
Since fis bounded below by f∗:=1
n/summationtext
i∈[n]fi,∗, we have
min
k∈[0:K−1]E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤1
KK−1/summationdisplay
k=0E/bracketleftbig
∥∇f(θk)∥2/bracketrightbig
≤2E[f(θ0)−f∗]
α(2−Lnα)K+Lnασ2
(2−Lnα)b.
18Under review as submission to TMLR
A.6 Estimation of critical batch size
We estimated the critical batch size by using Theorem 3.3(iii) and the ideas presented in ( Iiduka ,2022) and
(Sato & Iiduka ,2023). We used Algorithm 1with c= 0.001for training ResNet-18 on the CIFAR-10 dataset
(Figures 2). Theorem 3.3(iii) indicates that the equation of the critical batch size involves the unknown value
σ2. We checked that the Armijo-line-search learning rates for Algorithm 1with c= 0.001are about 10(see
also ( Vaswani et al. ,2019, Figure 5 (Left))). Hence, we used α≈10. We estimated the unknown value
X=σ2
ϵ2in equation ( 16) of the critical batch size by using δ= 0.9,b⋆= 28(see Figure 2) and α≈10as
follows:
b⋆=σ2
ϵ2L2
nα2
{2(1−c)δ−(Lnα−1)Lnα}
We consider case of1
Ln≥α, since b⋆is monotonically increasing when Ln≥0. We have
b⋆≤X1
2(1−c)δ
Setting c= 0.001andb⋆= 28(see Figure 2) gives
28≥X1
2(1−0.001)0 .9
Let us estimate the critical batch size using X≈460, and Theorem 3.3(iii). For example, when Algorithm 1
with c= 0.01is used to train ResNet-18 on the CIFAR-10 dataset, the equation of the critical batch size is
X1
2(1−c)δ≈258≈28=b⋆
which implies that the estimated critical batch size 258is close to the measured critical batch size b⋆= 28=
256in Figure 2.
19