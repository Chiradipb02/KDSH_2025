Published in Transactions on Machine Learning Research (09/2024)
Learning multi-modal generative models with permutation-
invariant encoders and tighter variational objectives
Marcel Hirt marcelandre.hirt@ntu.edu.sg
School of Social Sciences
Nanyang Technological University
Singapore
Domenico Campolo d.campolo@ntu.edu.sg
School of Mechanical and Aerospace Engineering
Nanyang Technological University
Singapore
Victoria Leong VictoriaLeong@ntu.edu.sg
School of Social Sciences
Nanyang Technological University
Singapore
Juan-Pablo Ortega juan-pablo.ortega@ntu.edu.sg
School of Physical and Mathematical Sciences
Nanyang Technological University
Singapore
Reviewed on OpenReview: https: // openreview. net/ forum? id= lM4nHnxGfL
Abstract
Devisingdeeplatentvariablemodelsformulti-modaldatahasbeenalong-standingthemein
machine learning research. Multi-modal Variational Autoencoders (VAEs) have been a pop-
ular generative model class that learns latent representations that jointly explain multiple
modalities. Various objective functions for such models have been suggested, often moti-
vated as lower bounds on the multi-modal data log-likelihood or from information-theoretic
considerations. To encode latent variables from different modality subsets, Product-of-
Experts (PoE) or Mixture-of-Experts (MoE) aggregation schemes have been routinely used
and shown to yield different trade-offs, for instance, regarding their generative quality or
consistency across multiple modalities. In this work, we consider a variational objective
that can tightly approximate the data log-likelihood. We develop more flexible aggregation
schemes that avoid the inductive biases in PoE or MoE approaches by combining encoded
features from different modalities based on permutation-invariant neural networks. Our nu-
merical experiments illustrate trade-offs for multi-modal variational objectives and various
aggregation schemes. We show that our variational objective and more flexible aggregation
models can become beneficial when one wants to approximate the true joint distribution
over observed modalities and latent variables in identifiable models.
1 Introduction
Multi-modal data sets where each sample has features from distinct sources have grown in recent years. For
example, multi-omics data such as genomics, epigenomics, transcriptomics, and metabolomics can provide a
1Published in Transactions on Machine Learning Research (09/2024)
(a)
 (b)
 (c)
 (d)
Figure 1: Reconstruction or cross-prediction of modalities in 1a and 1b for a mixture-based bound and
our objective, respectively. The mixture-based bound resorts to a single latent variable Z∼qϕ(·|xS)that
encodes information from a modality subset xSand is trained to reconstruct the conditioning modalities xS,
as well as to predict the masked modalities x\S. Our objective relies on two latent variables ZS∼qϕ(·|xS)
andZM∼qϕ(·|xS,x\S), whereZSis learned to reconstruct all its conditioning modalities, with ZMlearned
to reconstruct the remaining modalities. KL regularization terms in 1c and 1d for a mixture-based bound
and our objective, respectively. The mixture-based bound aims to minimize the KL divergence between the
encoding distribution given a modality subset xSand a prior distribution. Our objective additionally aims to
minimize the KL divergence between the encoding distribution given all modalities relative to the encoding
distribution of a modality subset xS.
more comprehensive understanding of biological systems if multiple modalities are analyzed in an integrative
framework (Argelaguet et al., 2018; Lee and van der Schaar, 2021; Minoura et al., 2021). In neuroscience,
multi-modal integration of neural activity and behavioral data can help to learn latent neural dynamics
(Zhou and Wei, 2020; Schneider et al., 2023). However, annotations or labels in such data sets are often rare,
makingunsupervisedorsemi-supervisedgenerativeapproachesparticularlyattractiveassuchmethodscanbe
used in these settings to (i) generate data, such as missing modalities, and (ii) learn latent representations
that are useful for down-stream analyzes or that are of scientific interest themselves. The availability of
heterogeneous data for different modalities promises to learn generalizable representations that can capture
shared content across multiple modalities in addition to modality-specific information. A promising class
of weakly-supervised generative models is multi-modal VAEs (Suzuki et al., 2016; Wu and Goodman, 2019;
Shi et al., 2019; Sutter et al., 2021) that combine information across modalities in an often-shared low-
dimensional latent representation. A common route for learning the parameters of latent variable models is
via maximization of the marginal data likelihood with various lower bounds thereof, as suggested in previous
work.
Setup. We consider a set of Mrandom variables {X1,...,XM}with empirical density pd, where each
random variable Xs,s∈M ={1,...,M}, can be used to model a different data modality taking values
inXs. With some abuse of notation, we write X={X1,...,XM}and for any subset S ⊂M, we set
X= (XS,X\S)for two partitions of the random variables into XS={Xs}s∈SandX\S={Xs}s∈M\S.
We pursue a latent variable model setup, analogous to uni-modal VAEs (Kingma and Ba, 2014; Rezende
et al., 2014). For a latent variable Z∈Zwith prior density pθ(z), we posit a joint generative model1
pθ(z,x) =pθ(z)/producttextM
s=1pθ(xs|z), wherepθ(xs|z)is commonly referred to as the decoding distribution for
modalitys. Observe that all modalities are independent given the latent variable zshared across all modal-
ities. However, one can introduce modality-specific latent variables by making sparsity assumptions for the
decoding distribution. Intuitively, this conditional independence assumption means that the latent variable
Zcaptures all unobserved factors shared by the modalities.
1We usually denote random variables using upper-case letters, and their realizations by the corresponding lower-case letter.
We assume throughout that Z=RD, and that pθ(z)is a Lebesgue density, although the results can be extended to more general
settings such as discrete random variables Zwith appropriate adjustments, for instance, regarding the gradient estimators.
2Published in Transactions on Machine Learning Research (09/2024)
Multi-modal variational bounds and mutual information. Popular approaches to train multi-modal
models are based on a mixture-based variational bound (Daunhawer et al., 2022; Shi et al., 2019) given by
LMix(θ,ϕ,β ) =/integraltext
ρ(S)LMix
S(x,θ,ϕ,β )dS, where
LMix
S(x,θ,ϕ,β ) =/integraldisplay
qϕ(z|xS) [logpθ(x|z)] dz−βKL(qϕ(z|xS)|pθ(z)) (1)
andρis some distribution on the power set P(M)ofMandβ > 0. Forβ= 1, one obtains the bound
LMix
S(x,θ,ϕ,β )≤logpθ(x). However, as shown in Daunhawer et al. (2022), there is a gap between the
variational bound and the log-likelihood given by the conditional entropies that cannot be reduced even for
flexible encoding distributions. More precisely, it holds that
/integraldisplay
pd(x) logpθ(x)dx≥/integraldisplay
pd(x)LMix(x,θ,ϕ, 1)dx+/integraldisplay
ρ(S)H(pd(X\S|XS))dS,
whereH(pd(X\S|XS))is the entropy of the conditional data distributions. Intuitively, in (1), one tries to
reconstruct or predict all modalities from incomplete information using only the modalities S, which leads
to learning an inexact, average prediction (Daunhawer et al., 2022). In particular, it cannot reliably predict
modality-specific information that is not shared with other modality subsets, as measured by the conditional
entropiesH(pd(X\S|XS)). For an illustration, see Figure 1a, where the latent variable Zencodes information
from a text and audio modality and is tasked to both reconstruct the text and audio modalities and to predict
an unobserved image. Information that is specific to the image modality only thus cannot be recovered.
A related observation has been made for Masked AutoEncoders (He et al., 2022) that correspond to the
limiting case β→0, where the cross-reconstruction objective leads to learning features that are invariant
to the masking of modalities (Kong and Zhang, 2023) and allows for the recovery of latent variables that
represent maximally shared information between the unmasked and masked modality (Kong et al., 2023).
We will illustrate that maximizing LMix
Scan be interpreted as the information-theoretic objective of
maximizing/braceleftig
ˆIlb
qϕ(X,ZS)−βˆIub
qϕ(XS,ZS)/bracerightig
, (2)
where ˆIub
qandˆIlb
qare variational upper, respectively, lower bounds of the corresponding mutual information
Iq(X,Y ) =/integraltext
q(x,y) logq(x,y)
q(x)q(y)dxdyof random variables XandYhaving marginal and joint densities q.
In order to emphasize that the latent variable Zis conditional on XSunder the encoding density qϕ, we
writeZSinstead ofZ. Variations of (1) have been suggested (Sutter et al., 2020), such as by replacing
the prior density pθin the KL-term by a weighted product of the prior density pθand the uni-modal
encoding distributions qϕ(z|xs), for alls∈M. Likewise, the multi-view variational information bottleneck
approach developed in Lee and van der Schaar (2021) for predicting X\SgivenXScan be interpreted as
maximizing ˆIlb
qϕ(X\S,ZS)−βˆIub
qϕ(XS,ZS). Hwang et al. (2021) suggested a related bound that aims to
maximize the reduction of total correlation of Xwhen conditioned on a latent variable. Similar bounds have
been suggested in Sutter et al. (2020) and Suzuki et al. (2016) by considering different KL-regularisation
terms; see also Suzuki and Matsuo (2022). Shi et al. (2020) add a contrastive estimate ˆIpθof the point-
wise mutual information to the maximum likelihood objective and minimize −logpθ(x)−βˆIpθ(xS,x\S).
Optimizing variational bounds of different mutual information terms such as (2) yield latent representations
that have different trade-offs in terms of either (i) reconstruction or (ii) cross-prediction of multi-modal data
from a rate-distortion viewpoint (Alemi et al., 2018).
Multi-modal aggregation schemes. To optimize the variational bounds above or to allow for flexible
conditioning at test time, we need to learn encoding distributions qϕ(z|xS)for anyS∈P (M). The typical
aggregation schemes that are scalable to a large number of modalities are based on a choice of uni-modal
encoding distributions qϕs(z|xs)for anys∈M, which are then used to define the multi-modal encoding
distributions as follows:
•Mixture of Experts (MoE), see Shi et al. (2019),
qMoE
ϕ(z|xS) =1
|S|/summationdisplay
s∈Sqϕs(z|xs).
3Published in Transactions on Machine Learning Research (09/2024)
(a) PoE
 (b) MoE
 (c) Sum-Pooling
 (d) Self-Attention
Figure 2: Illustration of multi-modal aggregation schemes. All encoding schemes first apply modality-specific
encoders to each individual modality. A PoE model (2a) aggregates the outputs from the modality-specific
encoders into a single Gaussian distribution that results from a multiplication of the corresponding uni-
modal Gaussian densities. An MoE model (2b) assumes an equally weighted Gaussian mixture distribution
comprisedoftheuni-modalGaussiandensities. Ournewaggregationschemesallowforlearningpermutation-
invariant fusion models: A Sum-Pooling or Deep Set model (2c) applies the same function gto the encoded
featureshs,s∈{T,A,I}, before summing them up and using a non-linear projection ρto the parameters
of a Gaussian distribution. A Self-Attention model (2d) differs from the Sum-Pooling approach by applying
self-attention layers or transformer layers before summing up the features, thereby accounting for pairwise
interactions between the encoded modalities. Our newly introduced schemes allow for encoding only a
modality subset by using standard masking operations.
•Product of Experts (PoE), see Wu and Goodman (2018),
qPoE
ϕ(z|xS)∝pθ(z)/productdisplay
s∈Sqϕs(z|xs).
Figure 2a-2b illustrates these previously considered aggregation schemes. While these schemes do not require
learning the aggregation function, we introduce aggregation schemes that, as illustrated in 2c-2d, involve
learning additional neural network parameters that specify a learnable aggregation function.
Contributions. This paper contributes (i) a new variational objective as an approximation of a lower
bound on the multi-modal log-likelihood (LLH). We avoid a limitation of mixture-based bounds (1), which
may not provide tight lower bounds on the joint LLH if there is considerable modality-specific variation
(Daunhawer et al., 2022), even for flexible encoding distributions. The novel variational objective contains
a lower bound of the marginal LLH logpθ(xS)and a term approximating the conditional logpθ(x\S|xS)for
any choice ofS∈P (M), provided that we can learn a flexible multi-modal encoding distribution. This
paper then contributes (ii) new multi-modal aggregation schemes that yield more expressive multi-modal
encoding distributions when compared to MoEs or PoEs. These schemes are motivated by the flexibility of
permutation-invariant (PI) architectures such as DeepSets (Zaheer et al., 2017) or attention models (Vaswani
etal.,2017;Leeetal.,2019). Weillustratethattheseinnovations(iii)arebeneficialwhenlearningidentifiable
models, aided by using flexible prior and encoding distributions consisting of mixtures, and (iv) yield higher
LLH in experiments.
Further related work. Canonical Correlation Analysis (Hotelling, 1936; Bach and Jordan, 2005) is a clas-
sical approach for multi-modal data that aims to find projections of two modalities by maximally correlating
4Published in Transactions on Machine Learning Research (09/2024)
and has been extended to include more than two modalities (Archambeau and Bach, 2008; Tenenhaus and
Tenenhaus, 2011) or to allow for non-linear transformations (Akaho, 2001; Hardoon et al., 2004; Wang et al.,
2015; Karami and Schuurmans, 2021). Probabilistic CCA can also be seen as multi-battery factor analysis
(MBFA) (Browne, 1980; Klami et al., 2013), wherein a shared latent variable models the variation common to
all modalities with modality-specific latent variables capturing the remaining variation. Likewise, latent fac-
tor regression or classification models (Stock and Watson, 2002) assume that observed features and response
are driven jointly by a latent variable. Vedantam et al. (2018) considered a tiple-ELBO for two modalities,
while Sutter et al. (2021) introduced a generalized variational bound that involves a summation over all
modality subsets. A series of work has developed multi-modal VAEs based on shared and private latent
variables (Wang et al., 2016; Lee and Pavlovic, 2021; Lyu and Fu, 2022; Lyu et al., 2021; Vasco et al., 2022;
Palumbo et al., 2023). Tsai et al. (2019a) proposed a hybrid generative-discriminative objective and mini-
mized an approximation of the Wasserstein distance between the generated and observed multi-modal data.
Joy et al. (2021) consider a semi-supervised setup of two modalities that requires no explicit multi-modal
aggregation function. Extending the Info-Max principle (Linsker, 1988), maximizing mutual information
Iq(g1(X1),g(X2))≤Iq((X1,X2),(Z1,Z2))based on representations Zs=gs(Xs)for modality-specific en-
codersgsfrom two modalities has been a motivation for approaches based on (symmetrized) contrastive
objectives (Tian et al., 2020; Zhang et al., 2022c; Daunhawer et al., 2023) such as InfoNCE (Oord et al.,
2018; Poole et al., 2019; Wang and Isola, 2020) as a variational lower bound on the mutual information
betweenZ1andZ2. Recent work (Bounoua et al., 2023; Bao et al., 2023) considered score-based diffusion
models on auto-encoded private latent variables.
2 A tighter variational objective with arbitrary modality masking
ForS⊂Mandβ >0, we define
LS(xS,θ,ϕ,β ) =/integraldisplay
qϕ(z|xS) [logpθ(xS|z)] dz−βKL(qϕ(z|xS)|pθ(z)). (3)
This is simply a standard variational lower bound (Jordan et al., 1999; Blei et al., 2017) restricted to the
subsetSforβ= 1, and thereforeLS(xS,θ,ϕ, 1)≤logpθ(xS). One can express the variational bound in
information-theoretic (Alemi et al., 2018) terms as
/integraldisplay
pd(xS)L(xS)dxS=−DS−βRS
for the rate
RS=/integraldisplay
pd(xS)KL(qϕ(z|xS)|pθ(z))dxS
measuring the information content that is encoded from the observed modalities in Sbyqϕinto the latent
representation, and the distortion
DS=−/integraldisplay
pd(xS)qϕ(z|xS) logpθ(xS|z)dzdxS
given as the negative reconstruction log-likelihood of the modalities in S. While the latent variable ZS
that is encoded via qϕ(z|xS)fromXScan be tuned via the choice of β > 0to tradeoff compression and
reconstruction of all modalities in Sjointly, it does not explicitly optimize for cross-modal prediction of
modalities not in S. Indeed, the mixture-based variational bound differs from the above decomposition
exactly by an additional cross-modal prediction or cross-distortion term
Dc
\S=−/integraldisplay
pd(x)qϕ(zS|xS) logpθ(x\S|zS)dzSdx,
thereby explicitly optimizing for both self-reconstruction of a modality subset and cross-modal prediction
within a single objective: /integraldisplay
pd(dx)LMix
S(x) =−DS−Dc
\S−βRS. (4)
5Published in Transactions on Machine Learning Research (09/2024)
Instead of adding an explicit cross-modal prediction term, we consider an additional variational objective
with a second latent variable ZMthat encodes all observed modalities X=XMand tries to reconstruct the
remaining modality subset. Unlike the latent variable ZSin (1) and (3) that can only encode incomplete
information using the modalities S, this second latent variable ZMcan encode modality-specific information
from all observed modalities, thereby avoiding the averaging prediction in the mixture-based bound, see
Figure 1b.
Ideally, we may like to consider an additional variational objective that lower bounds the conditional log-
likelihood logpθ(x\S|xS)so that maximizing the sum of both bounds maximizes a lower bound of the
multi-modal log-likelihood logpθ(xS,x\S) = logpθ(xS) + logpθ(x\S|xS). To motivate such a conditional
variational objective, note that maximizing the variational bound (3) with infinite capacity encoders yields
qϕ(z|xS) =pθ(z|xS). This suggests replacing the intractable posterior pθ(z|xS)with the encoder qϕ(z|xS)
for the probabilistic model when conditioned on xS. A variational objective under this replacement then
becomes
L\S(x,θ,ϕ,β ) =/integraldisplay
qϕ(z|x)/bracketleftbig
logpθ(x\S|z)/bracketrightbig
dz−βKL(qϕ(z|x)|qϕ(z|xS)). (5)
However, the above only approximates a lower bound on the conditional log-likelihood logpθ(x\S|xS), and
L\Sis a lower bound only under idealized conditions. We will make these approximations more precise
in Section 2.1, where we illustrate how these bounds yield to a matching of different distributions in the
latent or data space, while Section 2.2 provides an information-theoretic interpretation of these variational
objectives. The introduction of a second latent variable leads to two KL regularization terms, see Figure 1d,
that do not satisfy a triangle inequality KL(qϕ(z|x)|qϕ(z|xS)) +KL(qϕ(z|xS)|pθ(z))≱KL(qϕ(z|x)|pθ(z)).
In summary, for some fixed density ρonP(M), we suggest to maximize the overall bound
L(x,θ,ϕ,β ) =/integraldisplay
ρ(S)/bracketleftbig
LS(xS,θ,ϕ,β ) +L\S(x,θ,ϕ,β )/bracketrightbig
dS,
with respect to θandϕ, which is a generalization of the bound suggested in Wu and Goodman (2019) to an
arbitrary number of modalities. This bound can be optimized using standard Monte Carlo techniques, for
example, by computing unbiased pathwise gradients (Kingma and Ba, 2014; Rezende et al., 2014; Titsias
and Lázaro-Gredilla, 2014) using the reparameterization trick. For variational families such as Gaussian
mixtures2, one can employ implicit reparameterization (Figurnov et al., 2018). It is straightforward to adapt
variance reduction techniques such as ignoring the scoring term of the multi-modal encoding densities for
pathwise gradients (Roeder et al., 2017), see Algorithm 1 in Appendix H for pseudo-code. Nevertheless, a
scalable approach requires an encoding technique that allows to condition on any masked modalities with
a computational complexity that does not increase exponentially in M. We will analyse scalable model
architectures in Section 3.
2.1 Multi-modal distribution matching
Likelihood-based learning approaches aim to match the model distribution pθ(x)to the true data distribution
pd(x). Variational approaches achieve this by matching in the latent space the encoding distribution to the
true posterior as well as maximizing a tight lower bound on logpθ(x), see Rosca et al. (2018). These types of
analyses have proved useful for uni-modal VAEs as they can provide some insights as to why VAEs may lead
to worse generative sample quality compared to other generative models such as GANs (Goodfellow et al.,
2014) or may fail to learn useful latent representations (Zhao et al., 2019; Dieng et al., 2019). We show similar
results for the multi-modal variational objectives. This suggests that limitations from uni-modal VAEs also
affect multi-modal VAEs, but also that previous attempts to address these shortcomings in uni-modal VAEs
may benefit multi-modal VAEs. In particular, mismatches between the prior and the aggregated prior for
uni-modal VAEs that result in poor unconditional generation have a natural counterpart for cross-modal
generations with multi-modal VAEs that may potentially be reduced using more flexible conditional prior
distributions, see Remark 3, or via adding additional mutual information regularising terms (Zhao et al.,
2For MoE aggregation schemes, Shi et al. (2019) considered a stratified ELBO estimator as well as a tighter bound based
on importance sampling, see also Morningstar et al. (2021), that we do not pursue here for consistency with other aggregation
schemes that can likewise be optimized based on importance sampling ideas.
6Published in Transactions on Machine Learning Research (09/2024)
2019), see Remark 4. Given these results, it is neither surprising that multi-modal diffusion models such
as Bounoua et al. (2023); Bao et al. (2023) yield improved sample quality, nor that sample quality can be
improved by augmenting multi-modal VAEs with diffusion models (Pandey et al., 2022; Palumbo et al.,
2024).
We consider the densities
pθ(z,x) =pθ(z)pθ(xS|z)pθ(x\S|z)
and
qϕ(zS,x) =pd(xS)qϕ(zS|xS).
The latter is the encoding path comprising the encoding density qϕconditioned on xSand the empirical
densitypd. We write
qagg
ϕ,S(z) =/integraldisplay
pd(xS)qϕ(z|xS)dxS
for the aggregated prior (Makhzani et al., 2016; Hoffman and Johnson, 2016; Tomczak and Welling, 2017)
restrictedonmodalitiesfrom Sandq⋆(xS|z) =qϕ(xS,z)/qagg
ϕ(z)andlikewiseconsideritsconditionalversion,
qagg
ϕ,\S(z|xS) =/integraldisplay
pd(x\S|xS)qϕ(z|x)dx\S
for an aggregated encoder conditioned on xS. We provide a multi-modal ELBO surgery, summarized in
Proposition 1 below. It implies that maximizing/integraltext
pd(xS)LS(xS,θ,ϕ)dxSdrives
1. the joint inference distribution qϕ(z,xS) =pd(xS)qϕ(z|xS)of theSsubmodalities to the joint
generative distribution pθ(z,xS) =pθ(z)pθ(xS|z)and
2. the generative marginal pθ(xS)to its empirical counterpart pd(xS).
Analogously, maximizing/integraltext
pd(x\S|xS)L\S(x,θ,ϕ )dx\Sdrives, for fixed xS,
1. the distribution pd(x\S|xS)qϕ(z|x)to the distribution pθ(x\S|z)qϕ(z|xS)and
2. the conditional pθ(x\S|xS)to its empirical counterpart pd(x\S|xS).
Furthermore, it shows that maximizing L\S(x,θ,ϕ )minimizes a Bayes-consistency matching term
KL(qagg
ϕ,\S(z|xS)|qϕ(z|xS))for the multi-modal encoders where a mismatch can yield poor cross-generation,
as an analog of the prior not matching the aggregated posterior leading to poor unconditional generation,
see Remark 4.
Proposition 1 (Marginal and conditional distribution matching) For anyS∈P (M), we have
/integraldisplay
pd(xS)LS(xS,θ,ϕ)dxS+H(pd(xS))
=−KL(qϕ(z,xS)|pθ(z,xS)) (ZXmarginal)
=−KL(pd(xS)|pθ(xS))−/integraldisplay
pd(xS)KL(qϕ(z|xS)|pθ(z|xS))dxS (Xmarginal)
=−KL(qagg
ϕ,S(z)|pθ(z))−/integraldisplay
qagg
ϕ,S(z)KL(q⋆(xS|z)|pθ(xS|z))dxS, (Zmarginal)
whereq⋆(xS|z) =qϕ(xS,z)/qagg
ϕ(z). Moreover, for fixed xS,
/integraldisplay
pd(x\S|xS)L\S(x,θ,ϕ )dx\S+H(pd(x\S|xS))
=−KL/parenleftbig
qϕ(z|x)pd(x\S|xS)/vextendsingle/vextendsinglepθ(x\S|z)qϕ(z|xS)/parenrightbig
(ZXconditional )
=−KL(pd(x\S|xS)|pθ(x\S|xS)) (Xconditional )
−/integraldisplay
pd(x\S|xS)/parenleftbigg
KL(qϕ(z|x)|pθ(z|x))−/integraldisplay
qϕ(z|x) logqϕ(z|xS)
pθ(z|xS)dz/parenrightbigg
dx\S
=−KL(qagg
ϕ,\S(z|xS)|qϕ(z|xS))−/integraldisplay
qagg
ϕ,\S(z|xS)/parenleftbig
KL(q⋆(x\S|z,xS)|pθ(x\S|z))/parenrightbig
dz, (Zconditional )
7Published in Transactions on Machine Learning Research (09/2024)
whereq⋆(x\S|z,xS) =qϕ(z,x\S|xS)/qagg
ϕ,\S(z|xS) =pd(x\S|xS)qϕ(z|x)/qagg
ϕ,\S(z|xS).
Ifqϕ(z|xS)approximates pθ(z|xS)exactly, Proposition 1 implies that L\S(x,θ,ϕ )is a lower bound of
logpθ(x\S|xS). More precisely, we obtain the following log-likelihood approximation.
Corollary 2 (Multi-modal log-likelihood approximation) For any modality mask S, we have
/integraldisplay
pd(x)/bracketleftbig
LS(xS,θ,ϕ, 1) +L\S(x,θ,ϕ, 1)/bracketrightbig
dx−/integraldisplay
pd(x) [logpθ(x)] dx
=−/integraldisplay
pd(xS) [KL(qϕ(z|xS)|pθ(z|xS))] dx−/integraldisplay
pd(x) [KL(qϕ(z|x)|pθ(z|x))] dx
+/integraldisplay
pd(x)qϕ(z|x)/bracketleftbigg
logqϕ(z|xS)
pθ(z|xS)/bracketrightbigg
dzdx.
ProofThis follows from ( Xmarginal) and ( Xconditional ).
Our approach recovers meta-learning with (latent) Neural processes (Garnelo et al., 2018b) when one op-
timizes onlyL\SwithSdetermined by context-target splits, cf. Appendix B. Our analysis implies that
LS+L\Sis an approximation of a lower bound on the multi-modal log-likelihood that becomes tight for
infinite-capacity encoders so that qϕ(z|xS) =pθ(z|xS)andqϕ(z|x) =pθ(z|x), see Remarks 3 and 5 for details.
Remark 3 (Log-Likelihood approximation and Empirical Bayes) The term
/integraldisplay
pd(x)qϕ(z|x)/bracketleftbigg
logqϕ(z|xS)
pθ(z|xS)/bracketrightbigg
dzdx
arising in Corollary 2 and in ( Xconditional ) is not necessarily negative. Analogous to other variational ap-
proaches for learning conditional distributions such as latent Neural processes, our bound becomes an ap-
proximation of a lower bound. Note that LSis maximized when qϕ(z|xS) =pθ(z|xS), see ( Xmarginal), which
implies a lower bound in Corollary 2 of
/integraldisplay
pd(x)/bracketleftbig
LS(xS,θ,ϕ, 1) +L\S(x,θ,ϕ, 1)/bracketrightbig
dx=/integraldisplay
pd(x) [logpθ(x)−KL(qϕ(z|x)|pθ(z|x))] dx.
We can re-write the conditional expectation of L\Sfor any fixed xSas
/integraldisplay
pd(x\S|xS)L\S(x,θ,ϕ, 1)dx\S=/integraldisplay
pd(x\S|xS)qϕ(z|x) logpθ(x\S|z)dzdx\S+pd(x\S|xS)H(qϕ(z|x))dx\S
+/integraldisplay
qagg
ϕ,\S(z|xS) logqϕ(z|xS)dz.
Wheneverqϕ(z|xS)can be learned independently from qϕ(z|x), the above is maximized for
qϕ(z|xS) =/integraldisplay
pd(x\S|xS)qϕ(z|x)dx\S=qagg
ϕ,\S(z|xS).
From a different perspective, we can consider an Empirical Bayes viewpoint (Robbins, 1992; Wang et al.,
2019b) wherein one chooses the hyperparameters of the (conditional) prior so that it maximizes an approxi-
mation of the conditional log-likelihood logpθ(x\S|xS). The conditional prior p⋆
ϑ(z|xS)in the corresponding
conditional ELBO term
L⋆
\S(x,θ,ϕ,ϑ,β ) =/integraldisplay
qϕ(z|x)/bracketleftbig
logpθ(x\S|z)/bracketrightbig
dz−βKL(qϕ(z|x)|p⋆
ϑ(z|xS)). (6)
can thus be seen as a learned prior having the parameter ϑ=ϑ(D)that is learned by maximizing the
above variational approximation of logpθ(x\S|xS)overx∼pdforβ= 1, and as such depends on the
8Published in Transactions on Machine Learning Research (09/2024)
empirical multi-modal dataset D. While the aggregated prior qagg
ϕ,\S(z|xS)is the optimal learned prior when
maximizingL\S, this choice can lead to overfitting. Moreover, computation of the aggregated prior, or
sparse approximations thereof, such as variational mixture of posteriors prior (VampPrior) in Tomczak and
Welling (2017), are challenging in the conditional setup. Previous constructions in this direction (Joy et al.,
2021) for learning priors for bi-modal data only considered unconditional versions, wherein pseudo-samples
are not dependent on some condition xS. While our permutation-invariant architectures introduced below
may be used for flexibly parameterizing conditional prior distributions p⋆
ϑ(z|xS)as a function of xSwith
a model that is different from the encoding distributions qϕ(z|xS), we contend ourselves with choosing the
same model for both the conditional prior and the encoding distribution, p⋆
ϑ(z|xS) =qϕ(z|xS). Note that
the encoding distribution then features both bounds, which encourages learning encoding distributions that
perform well as conditional priors and as encoding distributions. In the ideal scenario where both the
generative and inference models have the flexibility to satisfy pd(x\S|xS) =pθ(x\S|xS),qϕ(z|xS) =pθ(z|xS)
andqϕ(z|xS) =pθ(z|xS), then the optimal conditional prior distribution is
qagg
ϕ,\S(z|xS) =/integraldisplay
pθ(x\S|xS)pθ(z|x)dx\S=/integraldisplay
pθ(x\S|xS)pθ(z|xS)pθ(x\S|xS)
pθ(x\S|xS)dx\S=pθ(z|xS) =qϕ(z|xS).
Remark 4 (Prior-hole problem and Bayes or conditional consistency) In the uni-modal setting,
the mismatch between the prior and the aggregated prior can be large and can lead to poor uncondi-
tional generative performance because this would lead to high-probability regions under the prior that have
not been trained due to their small mass under the aggregated prior (Hoffman and Johnson, 2016; Rosca
et al., 2018). Equation ( Zmarginal) extends this to the multi-modal case, and we expect that unconditional
generation can be poor if this mismatch is large. Moreover, ( Zconditional ) extends this conditioned on some
modality subset, and we expect that cross-generation for x\Sconditional on xScan be poor if the mismatch
betweenqagg
ϕ,\S(z|xS)andqϕ(z|xS)is large for xS∼pd, because high-probability regions under qϕ(z|xS)will
not have been trained - via optimizing L\S(x)- to model x\Sconditional on xS, due to their small mass
underqagg
ϕ,\S(z|xS). The mismatch will vanish when the encoders are consistent and correspond to a single
Bayesian model where they approximate the true posterior distributions. A potential approach to reduce
this mismatch may be to include as a regulariser the divergence between them that can be optimized by
likelihood-free techniques, such as the Maximum-Mean Discrepancy (Gretton et al., 2006), as in Zhao et al.
(2019) for uni-modal or unconditional models. For the mixture-based bound, the same distribution mismatch
affects unconditional generation, while both the training and generative sampling distribution is qϕ(z|xS)
for cross-generation.
Remark 5 (Variational gap for mixture-based bounds) Corollary 2 shows that the variational objec-
tive can become a tight bound in the limiting case where the encoding distributions approximate the true
posterior distributions. A similar result does not hold for the mixture-based multi-modal bound. More-
over, our bound can be tight for an arbitrary number of modalities in the limiting case of infinite-capacity
encoders. In contrast, Daunhawer et al. (2022) show that for mixture-based bounds, this variational gap
increases with each additional modality if the new modality is ’sufficiently diverse’, even for infinite-capacity
encoders.
Remark 6 (Optimization, multi-task learning and the choice of ρ)For simplicity, we have chosen
to sampleS∼ρin our experiments via the hierarchical construction γ∼U(0,1),mj∼Bern (γ)iid for
allj∈[M]and settingS={s∈[M]:mj= 1}. The distribution ρfor masking the modalities can be
adjusted to accommodate various weights for different modality subsets. Indeed, (2) can be seen as a linear
scalarization of a multi-task learning problem (Fliege and Svaiter, 2000; Sener and Koltun, 2018). We aim to
optimize a loss vector (LS+L\S)S⊂M, where the gradients for each S⊂Mcan point in different directions,
making it challenging to minimize the loss for all modalities simultaneously. Consequently, Javaloy et al.
(2022) used multi-task learning techniques (e.g., as suggested in Chen et al. (2018); Yu et al. (2020)) for
adjusting the gradients in mixture-based VAEs. Such improved optimization routines are orthogonal to our
approach. Similarly, we do not analyze optimization issues such as initializations and training dynamics that
have been found challenging for multi-modal learning (Wang et al., 2020; Huang et al., 2022).
9Published in Transactions on Machine Learning Research (09/2024)
2.2 Information-theoretic perspective
Beyond generative modeling, β-VAEs (Higgins et al., 2017) have been popular for representation learning
and data reconstruction. Alemi et al. (2018) suggest learning a latent representation that achieves certain
mutual information with the data based on upper and lower variational bounds of the mutual information.
A Legendre transformation thereof recovers the β-VAE objective and allows a trade-off between information
content or rate versus reconstruction quality or distortion. We show that the proposed variational objective
gives rise to an analogous perspective for multiple modalities. Recall that the mutual information on the
inference path3is given by
Iqϕ(XS,ZS) =/integraldisplay
qϕ(xS,zS) logqϕ(xS,zS)
pd(xS)qagg
ϕ,S(zS)dzSdxS,
can be bounded by standard (Barber and Agakov, 2004; Alemi et al., 2016; 2018) lower and upper bounds:
HS−DS≤HS−DS+ ∆ 1= Iqϕ(XS,ZS) =RS−∆2≤RS, (7)
with ∆1,∆2≥0andHS≤RS+DS. For details, see Appendix C. Consequently, by tuning β, we can vary
upper and lower bounds of Iqϕ(XS,ZS)to tradeoff between compressing and reconstructing XS.
To arrive at a similar interpretation for the conditional bound L\Sthat involves the conditional mutual
information
Iqϕ(X\S,ZM|XS) =/integraldisplay
pd(xS)KL(pd(x\S,zM|xS))|pd(x\S|xS)qagg
ϕ,\S(zM|xS))dxS
recalling that qagg
ϕ,\S(zM|xS) =/integraltext
pd(x\S|xS)qϕ(zM|x)dx\S, we set
R\S=/integraldisplay
pd(x)KL(qϕ(z|x)|qϕ(z|xS))dx
for a conditional or cross rate. Similarly, set
D\S=−/integraldisplay
pd(x)qϕ(z|x) logpθ(x\S|z)dzdx.
One obtains the following bounds, see Appendix C.
Lemma 7 (Variational bounds on the conditional mutual information) It holds that
−/integraldisplay
L\S(x,θ,ϕ,β )pd(dx) =D\S+βR\S
and for ∆\S,1,∆\S,2≥0,
H\S−D\S+ ∆\S,1= Iqϕ(X\S,ZM|XS) =R\S−∆\S,2.
Consequently, by tuning β, we can vary upper and lower bounds of Iqϕ(X\S,ZM|XS)to tradeoff between
compressing relative to qϕ(·|xS)and reconstructing X\S. Using the chain rules for entropy, we obtain that
the suggested bound can be seen as a relaxation of bounds on marginal and conditional mutual information.
Corollary 8 (Lagrangian relaxation) It holds that
H−DS−D\S≤Iqϕ(XS,ZS) + Iqϕ(X\S,ZM|XS)≤RS+R\S
and maximizingLfor fixedβ=∂(DS+D\S)
∂(RS+R\S)minimizes the rates RS+R\Sand distortions DS+D\S.
3We include the conditioning modalities as an index for the latent variable Zwhen the conditioning set is unclear.
10Published in Transactions on Machine Learning Research (09/2024)
Remark 9 (Mixture-based variational bound) We show in Appendix C, see also Daunhawer et al.
(2022), that
HM−DS−Dc
S≤HM−DS−Dc
S+ ∆′
1= Iqϕ(XM,ZS),
where ∆′
1=/integraltext
qagg
ϕ(z)KL(q⋆(x|z)|pθ(x|z))dz>0. Consequently,HM−DS−Dc
Sis a variational lower bound,
whileRSis a variational upper bound on Iqϕ(XM,ZS), which establishes (2). Maximizing the mixture-
based bound thus corresponds to encoding a single latent variable ZSthat maximizes the reconstruction of
all modalities while at the same time being maximally compressive relative to the prior.
Remark 10 (Optimal variational distributions) Consider the annealed likelihood ˜pβ,θ(xS|z)∝
pθ(xS|z)1/βas well as the adjusted posterior ˜pβ,θ(z|xS)∝˜pβ,θ(xS|z)pθ(z). The minimum of the bound/integraltext
pd(dx)LS(x)is attained at any xSfor the variational density
q⋆(z|xS)∝exp/parenleftbigg1
β[logpθ(xS|z) +βlogpθ(z)]/parenrightbigg
∝˜pβ,θ(z|xS), (8)
see also Huang et al. (2020) and Remark 18. Similarly, if (8) holds, then it is readily seen that the minimum
of the bound/integraltext
pd(dx)L\S(x)is attained at any xfor the variational density q⋆(z|x) = ˜pβ,θ(z|x). In contrast,
as shown in Appendix 19, the optimal variational density for the mixture-based (1) multi-modal bound is
attained at
q⋆(z|xS)∝˜pβ,θ(z|xS) exp/parenleftbigg/integraldisplay
pd(x\S|xS) log ˜pβ,θ(x\S|z)dx\S/parenrightbigg
.
The optimal variational density for the mixture-based bound thus tilts the posterior distribution to points
that achieve higher cross-modal predictions.
3 Permutation-invariant modality encoding
Optimizing the above multi-modal bounds requires learning variational densities with different conditioning
sets. We write hs,φ:Xs∝⇕⊣√∫⊔≀→RDEfor some modality-specific feature function. We recall the following multi-
modal encoding functions suggested in previous work where usually hs,φ(xs) =/bracketleftbig
µs,φ(xs)⊤,vec(Σs,φ(xs))⊤/bracketrightbig⊤
withµs,φandΣs,φbeing the mean, respectively the (often diagonal) covariance, of a uni-modal encoder of
modalitys. Accommodating more complex variational families, such as mixture distributions for the uni-
modal encoding distributions, can be more challenging for these approaches.
•MoE:qMoE
φ(z|xS) =1
|S|/summationtext
s∈SqN(z|µs,φ(xs),Σs,φ(xs)),whereqN(z|µ,Σ)is a Gaussian density with
meanµand covariance Σ.
•PoE:qPoE
φ(z|xS) =1
Zpθ(z)/producttext
s∈SqN(z|µs,φ(xs),Σs,φ(xs)), for someZ ∈R. For Gaussian priors
pθ(z) =qN(z|µθ,Σθ)with mean µθand covariance Σθ, the multi-modal distribution qPoE
φ(z|xS)is
Gaussian with mean
(µθΣθ+/summationdisplay
s∈Sµs,φ(xs)Σs,φ(xs))(Σ−1
1,θ+/summationdisplay
s∈SΣs,φ(xs)−1)−1
and covariance
(Σ−1
1,θ+/summationdisplay
s∈SΣs,φ(xs)−1)−1.
•Mixture of Product of Experts (MoPoE), see Sutter et al. (2021),
qMoPoE
ϕ (z|xM) =1
2M/summationdisplay
xS∈P(xM)qPoE
ϕ(z|xS).
11Published in Transactions on Machine Learning Research (09/2024)
3.1 Learnable permutation-invariant aggregation schemes
We aim to learn a more flexible aggregation scheme under the constraint that the encoding distribution is
invariant (Bloem-Reddy and Teh, 2020) with respect to the ordering of encoded features of each modality.
Put differently, for all (Hs)s∈S∈R|S|×DEand all permutations π∈SSofS, we assume that the conditional
distribution is SS-invariant, i.e. q′
ϑ(z|h) =q′
ϑ(z|π·h)for allz∈RD, whereπacts onH= (Hs)s∈Sviaπ·H=
(Hπ(s))s∈S. We setqϕ(z|xS) =q′
ϑ(z|hs,φ(xs)s∈S),ϕ= (φ,ϑ)and remark that the encoding distribution is
not invariant with respect to the modalities, but becomes only invariant after applying modality-specific
encoder functions hs,φ. Observe that such a constraint is satisfied by the aggregation schemes above for hs,φ
being the uni-modal encoders.
A variety of invariant (or equivariant) functions along with their approximation properties have been consid-
ered previously, see for instance Santoro et al. (2017); Zaheer et al. (2017); Qi et al. (2017); Lee et al. (2019);
Segol and Lipman (2019); Murphy et al. (2019); Maron et al. (2019); Sannai et al. (2019); Yun et al. (2019);
Bruno et al. (2021); Wagstaff et al. (2022); Zhang et al. (2022b); Li et al. (2022); Bartunov et al. (2022), and
applied in different contexts such as meta-learning (Edwards and Storkey, 2016; Garnelo et al., 2018b; Kim
et al., 2018; Hewitt et al., 2018; Giannone and Winther, 2022), reinforcement learning (Tang and Ha, 2021;
Zhang et al., 2022a) or generative modeling of (uni-modal) sets (Li et al., 2018; 2020; Kim et al., 2021; Biloš
and Günnemann, 2021; Li and Oliva, 2021). We can use such constructions to parameterize more flexible
encoding distributions. Indeed, the results from Bloem-Reddy and Teh (2020) imply that for an exchangable
sequenceHS= (Hs)s∈S∈R|S|×DEand random variable Z, the distribution q′(z|hS)isSS-invariant if and
only if there is a measurable function4f⋆: [0,1]×M (RDE)→RDsuch that
(HS,Z)a.s.= (HS,f⋆(Ξ,MHS)), where Ξ∼U[0,1]andΞ⊥ ⊥HS
withMHS(·) =/summationtext
s∈SδHs(·)being the empirical measure of hS, which retains the values of hS, but discards
their order. For variational densities from a location-scale family such as a Gaussian or Laplace distribution,
we find it more practical to consider a different reparameterization in the form Z=µ(hS) +σ(hS)⊙Ξ,
where Ξis a sample from a parameter-free density psuch as a standard Gaussian and Laplace distribution,
while/bracketleftbig
µ(hS),logσ(hS)/bracketrightbig
=f(hS)for a PI function f:R|S|×DE→R2D. Likewise, for mixture distributions
thereof, assume that for a PI function f,
/bracketleftbigµ1(hS),logσ1(hS),...,µK(hS),logσK(hS),logω(hS)/bracketrightbig
=f(hS)∈R2DK+K
andZ=µL(hS) +σL(hS)⊙ΞwithL∼Cat(ω(hS))denoting the sampled mixture component out of K
mixtures. For simplicity, we consider here only two examples of PI functions fthat have representations
with parameter ϑin the form
fϑ(hS) =ρϑ/parenleftigg/summationdisplay
s∈Sgϑ(hS)s/parenrightigg
for a function ρϑ:RDP→RDOand permutation-equivariant function gϑ:RN×DE→RN×DP.
Example 1 (Sum Pooling Encoders) The Deep Set (Zaheer et al., 2017) construction fϑ(hS) =
ρϑ/parenleftbig/summationtext
s∈Sχϑ(hs)/parenrightbig
applies the same neural network χϑ:RDE→RDPto each encoded feature hs. We
assume that χϑis a feed-forward neural network and remark that pre-activation ResNets (He et al., 2016)
have been advocated for deeper χϑ. For exponential family models, the optimal natural parameters of
the posterior solve an optimization problem where the dependence on the generative parameters from the
different modalities decomposes as a sum, see Appendix F.
Example 2 (Set Transformer Encoders) Let MTB ϑbe a multi-head pre-layer-norm transformer block
(Wang et al., 2019a; Xiong et al., 2020), see Appendix D for precise definitions. For some neural network
χϑ:RDE→RDP, setg0
S=χϑ(hS)and fork∈{1,...,L}, setgk
S=MTBϑ(gk−1
S). We then consider
fϑ(hS) =ρϑ/parenleftbig/summationtext
s∈SgL
s/parenrightbig
. This can be seen as a Set Transformer (Lee et al., 2019; Zhang et al., 2022a) model
4The function f⋆generally depends on the cardinality of S. Finite-length exchangeable sequences imply a de Finetti latent
variable representation only up to approximation errors (Diaconis and Freedman, 1980).
12Published in Transactions on Machine Learning Research (09/2024)
without any inducing points as for most applications, a computational complexity that scales quadratically
in the number of modalities can be acceptable. In our experiments, we use layer normalization (Ba et al.,
2016) within the transformer model, although, for example, set normalization (Zhang et al., 2022a) could be
used alternatively.
Note that the PoE’s aggregation mechanism involves taking inverses, which can only be approximated by
the learned aggregation models. The considered permutation-invariant models can thus only recover a PoE
scheme under universal approximation assumptions.
Remark 11 (Mixture-of-Product-of-Experts or MoPoEs) Sutter et al. (2021) introduced a MoPoE
aggregation scheme that extends MoE or PoE schemes by considering a mixture distribution of all 2M
modality subsets, where each mixture component consists of a PoE model, i.e.,
qMoPoE
ϕ (z|xM) =1
2M/summationdisplay
xS∈P(xM)qPoE
ϕ(z|xS).
This can also be seen as another PI model. While it does not require learning separate encoding models for
all modality subsets, it, however, becomes computationally expensive to evaluate for large M. Our mixture
models using components with a SumPooling or SelfAttention aggregation can be seen as an alternative
that allows one to choose the number of mixture components Kto be smaller than 2M, with non-uniform
weights, while the individual mixture components are not constrained to have a PoE form.
Remark 12 (Pooling expert opinions) Combining expert distributions has a long tradition in decision
theoryandBayesianinference; seeGenestandZidek(1986)forearlyworks, withpopularschemesbeinglinear
pooling (i.e., MoE) or log-linear pooling (i.e., PoE with tempered densities). These are optimal schemes for
minimizing different objectives, namely a weighted (forward or reverse) KL-divergence between the pooled
distribution and the individual experts (Abbas, 2009). Log-linear pooling operators are externally Bayesian,
allowing for consistent Bayesian belief updates when each expert updates her belief with the same likelihood
function (Genest et al., 1986).
3.2 Permutation-equivariance and private latent variables
In principle, the general permutation invariant aggregation schemes that have been introduced could also be
used for learning multi-modal models with private latent variables. For example, suppose that the generative
model factorizes as
pθ(z,x) =p(z)/productdisplay
s∈Mpθ(xs|z′,˜zs) (9)
forz= (z′,˜z1,..., ˜zM)∈Z, for shared latent variables Z′and private latent variable ˜Zsfor eachs∈M.
Note that for s̸=t∈[M],
Xs⊥ ⊥˜Zt|Z′,˜Zs. (10)
Consequently,
pθ(z′,˜zS,˜z\S|xS) =pθ(z′,˜zS,|xS)pθ(˜z\S|z′,˜zS,xS) =pθ(z′,˜zS,|xS)pθ(˜z\S|z′,˜zS). (11)
An encoding distribution qϕ(z|xS)that approximates pθ(z|xS)should thus be unaffected by the inputs xS
when encoding ˜zsfors /∈S, provided that, a priori, all private and shared latent variables are independent.
Observe that for fϑwith the representation (3.1) where ρϑhas aggregated inputs y, and that parameterizes
the encoding distribution of z= (z′,˜zS,˜z\S), the gradients of its i-th dimension with respect to the modality
valuesxsis
∂
∂xs[fϑ(hS(xS))i] =∂ρϑ,i
∂y/parenleftigg/summationdisplay
t∈Sgϑ(hS(xS)t)/parenrightigg
∂
∂xs/parenleftigg/summationdisplay
t∈Sgϑ(hS(xS))t/parenrightigg
.
In the case of a SumPooling aggregation, the gradient simplifies to
∂ρϑ,i
∂y/parenleftigg/summationdisplay
t∈Sχϑ(ht(xt))/parenrightigg
∂χϑ
∂h(hs(xs))∂hs(xs)
∂xs.
13Published in Transactions on Machine Learning Research (09/2024)
Suppose that the i-th component of ρϑmaps to the mean or log-standard deviation of some component of
˜Zsfor somes∈M\S. Notice that only the first factor depends on iso that for this gradient to be zero, ρϑ,i
has to be locally constant around y=/summationtext
s∈Sχϑ(hs(xs))if some other components have a non-zero gradient
with respect to Xs. It it thus very likely that inputs Xsfors∈Scan impact the distribution of the private
latent variables ˜z\S.
However, the specific generative model also lends itself to an alternative parameterization that guarantees
thatcross-modallikelihoodsfrom X\Sdonotaffecttheencodingdistributionof ˜ZSunderournewvariational
objective. The assumption of private latent variables suggests an additional permutation-equivariance into
the encoding distribution that approximates the posterior in (11), in the sense that for any permutation
π∈SS, it holds that
q′
ϕ(˜zS|π·hφ(xS),z′) =q′
ϕ(π·˜zS|hφ(xS),z′),
assuming that all private latent variables are of the same dimension D.5Indeed, suppose we have modality-
specific feature functions hφ,ssuch that{Hs=hφ,s(Xs)}s∈Sis exchangeable. Clearly, (10) implies for any
s̸=tthat
hφ,s(Xs)⊥ ⊥˜Zt|Z′,˜Zs.
The results from Bloem-Reddy and Teh (2020) then imply, for fixed |S|, the existence of a function f⋆such
that for all s∈S, almost surely,
(HS,˜Zs) = (HS,f⋆(Ξs,Z′,Hs,MHS)), where Ξs∼U[0,1]iid and Ξs⊥ ⊥HS. (12)
This fact suggests an alternative route to approximate the posterior distribution in (11): First, pθ(˜z\S|z′,˜zS)
can often be computed analytically based on the learned or fixed prior distribution. Second, a permutation-
invariant scheme can be used to approximate pθ(z′|xS). Finally, a permutation-equivariant scheme can
be employed to approximate pθ(˜zS|xS,z′)with a reparameterization in the form of (12). The variational
objective that explicitly uses private latent variables is detailed in Appendix E. Three examples of such
permutation-equivariant schemes are given below with pseudocode for optimizing the variational objective
given in Algorithm 2. Note that the assumption qϕ(˜zS|z′,˜z\S,xS) =qϕ(˜zS|z′,xS)is an inductive bias that
generally decreases the variational objective as it imposes a restriction on the encoding distribution that
only approximates the posterior where this independence assumption holds. However, this independence
assumption allows us to respect the modality-specific nature of the private latent variables during encod-
ing. In particular, for some permutation-invariant encoder qϕ(z′|xS)for the private latent variables and
permutation-equivariant encoder qϕ(˜zS|z′,xS)for the private latent variables of the observed modalities, we
can encode via
qϕ(z′,˜zM|xS) =qϕ(z′|xS)pθ(˜z\S|z′)qϕ(˜zS|z′,xS)
so that the modality-specific information of xSas encoded via ˜zSis not impacted by the realisation ˜Z\Sof
modality-specific variation from the other modalities.
Example 3 (Permutation-equivariant PoE) Similar to previous work Wang et al. (2016); Lee and
Pavlovic (2021); Sutter et al. (2020), we consider an encoding density of the form
qϕ(z′,˜zM|xS) =qPoE
φ(z′|xS)/productdisplay
s∈SqN(˜zs|˜µs,φ(xs),˜Σs,φ(xs))/productdisplay
s∈M\Spθ(˜zs),
where
qPoE
φ(z′|xS) =1
Zpθ(z′)/productdisplay
s∈SqN(z′|µ′
s,φ(xs),Σ′
s,φ(xs))
is a (permutation-invariant) PoE aggregation, and we assumed that the prior density factorizes over the
shared and different private variables. For each modality s, we encode different features h′
s,φ= (µ′
s,φ,Σ′
s,φ)
and˜hs,φ= (˜µs,φ,˜Σs,φ)for the shared, respectively, private, latent variables. We followed previous works
(Tsai et al., 2019b; Lee and Pavlovic, 2021; Sutter et al., 2020) in that the encodings and prior distributions
for the modality-specific latent variables are independent of the shared latent variables. However, this
assumption can be relaxed, as long as the distributions remain Gaussian.
5The effective dimension can vary across modalities in practice if the decoders are set to mask redundant latent dimensions.
14Published in Transactions on Machine Learning Research (09/2024)
Example 4 (Permutation-equivariant Sum-Pooling) We consider an encoding density that is written
as
qϕ(z′,˜zM|xS) =qSumP
ϕ (z′|xS)qEquiv-SumP
ϕ(˜zS|z′,xS)/productdisplay
s∈M\Spθ(˜zs|z′).
Here, we use a (permutation-invariant) Sum-Pooling aggregation scheme for constructing the shared latent
variableZ′=µ′(hS)+σ′(hS)⊙Ξ′∼qSumP
ϕ (z′|xS), where Ξ′∼pandfϑ:R|S|×DE→RDgiven as in Example
(1) with/bracketleftbigµ′(h),logσ′(h)/bracketrightbig
=fϑ(h). To sample ˜ZS∼qEquiv-SumP
ϕ(˜zS|z′,xS), consider functions χj,ϑ:RDE→
RDP,j∈[3], andρϑ:RDP→RDO, e.g., fully-connected neural networks. We define fEquiv-SumP
ϑ:Z×
R|S|×DE→R|S|×DOvia
fEquiv-SumP
ϑ(z′,hS)s=ρϑ/parenleftigg/bracketleftigg/summationdisplay
t∈Sχ0,ϑ(ht)/bracketrightigg
+χ1,ϑ(z′) +χ2,ϑ(hs)/parenrightigg
.
With/bracketleftbig
˜µ(hS)⊤,log ˜σ(hS)⊤/bracketrightbig⊤=fEquiv-SumP
ϑ(z′,hS), we then set ˜Zs= ˜µ(hS)s+ ˜σ(hS)s⊙˜Ξsfor˜Ξs∼piid,
hs=hφ,s(xs)for modality-specific feature functions hφ,s:Xs→RDE.
Example 5 (Permutation-equivariant Self-Attention) Similar to a Sum-Pooling approach, we con-
sider an encoding density that is written as
qϕ(z′,˜zM|xS) =qSA
ϕ(z′|xS)qEquiv-SA
ϕ(˜zS|z′,xS)/productdisplay
s∈M\Spθ(˜zs|z′).
Here, the shared latent variable Z′is sampled via the permutation-invariant aggregation above by summing
the elements of a permutation-equivariant transformer model of depth L′. For encoding the private latent
variables, we follow the example above but set
/bracketleftbig
˜µ(hS)⊤,log ˜σ(hS)⊤/bracketrightbig⊤=fEquiv-SA
ϑ(z′,hS)s=gL
S,
withgk
S=MTBϑ(gk−1
S)ang0= (χ1,ϑ(hs) +χ2,ϑ(z′))s∈S.
4 Identifiability and model extensions
4.1 Identifiability
Identifiability of parameters and latent variables in latent structure models is a classic problem (Koopmans
and Reiersol, 1950; Kruskal, 1976; Allman et al., 2009), that has been studied increasingly for non-linear
latent variable models, e.g., for ICA (Hyvarinen and Morioka, 2016; Hälvä and Hyvarinen, 2020; Hälvä et al.,
2021), VAEs (Khemakhem et al., 2020a; Zhou and Wei, 2020; Wang et al., 2021; Moran et al., 2021; Lu et al.,
2022; Kim et al., 2023), EBMs (Khemakhem et al., 2020b), flow-based (Sorrenson et al., 2020) or mixture
models (Kivva et al., 2022).
Non-linear generative models are generally unidentifiable without imposing some structure (Hyvärinen and
Pajunen, 1999; Xi and Bloem-Reddy, 2022). Yet, identifiability up to some ambiguity can be achieved in
some conditional models based on observed auxiliary variables and injective decoder functions wherein the
prior density is conditional on auxiliary variables. Observations from different modalities can act as auxiliary
variables to obtain identifiability of conditional distributions given some modality subset under analogous
assumptions.
Example 6 (Auxiliary variable as a modality) In the iVAE model (Khemakhem et al., 2020a), the
latent variable distribution pθ(z|x1)is independently modulated via an auxiliary variable X1=U. Instead
of interpreting this distribution as a (conditional) prior density, we view it as a posterior density given
the first modality X1. Khemakhem et al. (2020a) estimate a model for another modality X2by lower
bounding logpθ(x2|x1)viaL\{1}under the assumption that qϕ(z|x1)is given by the prior density pθ(z|x1).
Similarly, Mita et al. (2021) optimize logpθ(x1,x2)by a double VAE bound that reduces to Lfor a masking
15Published in Transactions on Machine Learning Research (09/2024)
distribution ρ(s1,s2) = (δ1⊗δ0)(s1,s2)that always masks the modality X2and choosing to parameterize
separate encoding functions for different conditioning sets. Our bound thus generalizes these procedures to
multiple modalities in a scalable way.
We are interested in identifiability, conditional on having observed some non-empty modality subset S⊂M.
For illustration, we translate an identifiability result from the uni-modal iVAE setting in Lu et al. (2022),
whichdoesnotrequiretheconditionalindependenceassumptionfromKhemakhemetal.(2020a). Weassume
that the encoding distribution qϕ(z|xS)approximates the true posterior pθ(z|xS)and belongs to a strongly
exponential family, i.e.,
pθ(z|xS) =qϕ(z|xS) =pEF
Vϕ,S,λϕ,S(z|xS), (13)
with
pEF
VS,λS(z|xS) =µ(z) exp [⟨VS(z),λ(xS)⟩−log ΓS(λS(xS))],
whereµis a base measure, VS:Z→Rkis the sufficient statistics, λS(xS)∈Rkthe natural parameters and
ΓSa normalizing term. Furthermore, one can only reduce the exponential component to the base measure
on sets having measure zero. In this section, we assume that
pθ(xs|z) =ps,ϵ(xs−fθ,s(z)) (14)
for some fixed noise distribution ps,ϵwith a Lebesgue density, which excludes observation models for discrete
modalities. Let ΘSbe the domain of the parameters θS= (f\S,VS,λS)withf\S:Z∋z∝⇕⊣√∫⊔≀→(fs(z))s∈M\S∈
×s∈M\S Xs=X\S. Assuming (13), note that
pθS(x\S|xS) =/integraldisplay
pVS,λS(z|xS)p\S,ϵ(x\S−f\S(z))dz,
withp\S,ϵ=⊗s∈M\Sps,ϵ. We define an equivalence relation on ΘSby(f\S,VS,λS)∼AS(˜f\S,˜VS,˜λS)iff
there exist invertible AS∈Rk×kandcS∈Rksuch that
VS(f−1
\S(x\S)) =AS˜VS(˜f−1
\S(x\S)) +cS
for allx\S∈X\S.
Proposition 13 (Weak identifiability) Consider the data generation mechanism pθ(z,x) =
pθ(z)/producttext
s∈Mpθ(xs|z)where the observation model satisfies (14)for an injective f\S. Suppose further
thatpθ(z|xS)is strongly exponential and (13)holds. Assume that the set {x\S∈X\S|φ\S,ϵ(x\S) = 0}has
measure zero, where φ\S,ϵis the characteristic function of the density p\S,ϵ. Furthermore, suppose that
there existk+ 1pointsx0
S,...,xk
S∈XSsuch that
L=/bracketleftbig
λS(x1
S)−λS(x0
S),...,λS(xk
S)−λS(x0
S)/bracketrightbig
∈Rk×k
is invertible. Then pθS(x\S|xS) =p˜θS(x\S|xS)for allx∈Ximpliesθ∼AS˜θ.
This result follows from Theorem 4 in Lu et al. (2022). Note that pθS(x\S|xS) =p˜θS(x\S|xS)for all
x∈Ximplies with the regularity assumption on φ\S,ϵthat the transformed variables Z=f−1
\S(X\S)and
˜Z=˜f−1
\S(X\S)have the same density function conditional on XS.
Remark 14 (Conditional identifiability) The identifiability result above is about conditional models
and does not contradict the un-identifiability of VAEs: When S=∅and we view x=xMas one modality,
then the parameters of pθ∅(x)characterized by the parameters V∅andλ∅of the prior pθ∅(z|x∅)and the
encodersfMwill not be identifiable as the invertibility condition will not be satisfied.
Remark 15 (Private latent variables) For models with private latent variables, we might not expect
that conditioning on XShelps to identify ˜Z\Sas
pθ(z′,˜zS,˜z\S|xS) =pθ(z′,˜zS|xS)pθ(˜z\S|z′,˜z\S).
Indeed, Proposition 13 will not apply in such models as f\Swill not be injective.
16Published in Transactions on Machine Learning Research (09/2024)
Remark 16 (Data supported on low-dimensional manifolds) Note that (14) and (13) imply that
each modality has a Lebesgue density under the generative model. This assumption may not hold for some
modalities, such as imaging data that can be supported (closely) on a lower-dimensional manifold (Roweis
and Saul, 2000), causing issues in likelihood-based methods such as VAEs (Dai and Wipf, 2018; Loaiza-
Ganem et al., 2022). Moreover, different conditioning sets or modalities may result in different dimensions
of the underlying manifold for conditional data (Zheng et al., 2022). Some two-step approaches (Dai and
Wipf, 2018; Zheng et al., 2022) first estimate the dimension rof the ground-truth manifold as a function
of the encoder variance relative to the variance under the (conditional) prior for each latent dimension i,
i∈[D], withr≤D. It would, therefore, be interesting to analyze in future work if more flexible aggregation
schemes that do not impose strong biases on the variance components of the encoder can better learn the
manifold dimensions in conditional or multi-modal models following an analogous two-step approach.
Recall that the identifiability considered here concerns parameters of the multi-modal posterior distribution
and the conditional generative distribution. It is thus preliminary to estimation and only concerns the gen-
erative model and not the inference approach. However, both the multi-modal posterior distribution and
the conditional generative distribution are intractable. In practice, we thus replace them with approxima-
tions. We believe that our inference approach is beneficial for this type of identifiability when making these
variational approximations because (a) unlike some other variational bounds, the posterior is the optimal
variational distribution with L\S(x)being an approximation of a lower bound on logpθ(x\S|xS), see Remark
10, and (b) the trainable aggregation schemes can be more flexible for approximating the optimal encoding
distribution.
4.2 Mixture models
An alternative to the choice of uni-modal prior densities pθhas been to use Gaussian mixture priors (Johnson
et al., 2016; Jiang et al., 2017; Dilokthanakul et al., 2016) or more flexible mixture models (Falck et al.,
2021). Following previous work, we include a latent cluster indicator variable c∈[K]that indicates the
mixture component out of Kpossible mixtures with augmented prior pθ(c,z) =pθ(c)pθ(z|c). The classic
example ispθ(c)being a categorical distribution and pθ(z|c)a Gaussian with mean µcand covariance matrix
Σc. Similar to Falck et al. (2021) that use an optimal variational factor in a mean-field model, we use an
optimal factor of the cluster indicator in a structured variational density qϕ(c,z|xS) =qϕ(z|xS)qϕ(c|z,xS)
withqϕ(c|z,xS) =pθ(c|z). Appendix G details how one can optimize an augmented multi-modal bound.
Concurrent work (Palumbo et al., 2024) considered a similar optimal variational factor for a discrete mixture
model under a MoE aggregation.
4.3 Missing modalities
In practical applications, modalities can be missing for different data points. We describe this missing-
ness pattern by missingness mask variables ms∈ {0,1}wherems= 1indicates that observe modal-
itys, whilems= 0means it is missing. The joint generative model with missing modalities will be of
the formpθ(z,x,m ) =pθ(z)/producttext
s∈Mpθ(xs|z)pθ(m|x)for some distribution pθ(m|x)over the mask variables
m= (ms)s∈M. ForS⊂M, wedenoteby xo
S={xs:ms= 1,s∈S}andxm
S={xs:ms= 0,s∈S}thesetof
observed, respectivelymissing, modalities. Thefulllikelihoodoftheobservedandmissingnessmasksbecomes
thenpθ(xo
S,m) =/integraltext
pθ(z)/producttext
s∈Spθ(xs|z)pθ(m|x)dxm
sdz. Ifpθ(m|x)does not depend on the observations, that
is, observations are missing completely at random (Rubin, 1976), then the missingness mechanisms pθ(m|x)
for inference approaches maximizing pθ(xo,m)can be ignored. Consequently, one can instead concentrate
on maximizing logpθ(xo)only, based on the joint generative model pθ(z,xo) =pθ(z)/producttext
{s∈M:ms=1}pθ(xs|z).
In particular, one can employ the variational objectives above by considering only the observed modalities.
Since masking operations are readily supported for the considered permutation-invariant models, appropri-
ate imputation strategies (Nazabal et al., 2020; Ma et al., 2019) for the encoded features of the missing
modalities are not necessarily required. Settings allowing for not (completely) at random missingness have
been considered in the uni-modal case, for instance, in Ipsen et al. (2021); Ghalebikesabi et al. (2021); Gong
et al. (2021), and we leave multi-modal extensions thereof for future work for a given aggregation approach.
17Published in Transactions on Machine Learning Research (09/2024)
5 Experiments
We conduct a series of numerical experiments to illustrate the effects of different variational objectives and
aggregation schemes. Recall that the full reconstruction log-likelihood is the negative full distortion −DM
based on all modalities, while the full rate RMis the averaged KL between the encoding distribution of all
modalities and the prior. Note that mixture-based bounds maximize directly the cross-modal log-likelihood
−Dc
\S, see (4), and do not contain a cross-rate term R\S, i.e. the KL between the encoding distribution
for all modalities relative to a modality subset, as a regulariser, in contrast to our objective (Lemma 7 and
Corollary 8). The log-likelihood should be higher if a generative model is able to capture modality-specific
information for models trained with β= 1. For arbitrary β, we can take a rate-distortion perspective
and look at how different generative models self-reconstruct all modalities, i.e., the full reconstruction term
−DM, relative to the KL-divergence between the multi-modal encoding distribution and the prior, i.e. RM.
This corresponds to a rate-distortion analysis of a VAE that merges all modalities into a single modality.
A high full-reconstruction term is thus indicative of the encoder and decoder being able to reconstruct all
modalities precisely so that they do not produce an average prediction. Note that neither our objective nor
the mixture-based bound optimize for the full-reconstruction term directly.
5.1 Linear multi-modal VAEs
The relationship between uni-modal VAEs and probabilistic PCA (Tipping and Bishop, 1999) has been
studied in previous work (Dai et al., 2018; Lucas et al., 2019; Rolinek et al., 2019; Huang et al., 2020;
Mathieu et al., 2019). We analyze how different multi-modal fusion schemes and multi-modal variational
objectives affect (a) the learned generative model in terms of its true marginal log-likelihood (LLH) and (b)
the latent representations in terms of information-theoretic quantities and identifiability. To evaluate the
(weak) identifiability of the method, we follow Khemakhem et al. (2020a;b) to compute the mean correlation
coefficient (MCC) between the true latent variables Zand samples from the variational distribution qϕ(·|xM)
after an affine transformation using CCA.
Generative model. Suppose that a latent variable Ztaking values in RDis sampled from a standard
Gaussian prior pθ(z) =N(0,I)generatesMdata modalities Xs∈RDs,D≤Ds, based on a linear decoding
modelpθ(xs|z) =N(Wsz+bs,σ2I)for a factor loading matrix Ws∈RDs×D, biasbs∈RDsand observation
scaleσ > 0. Note that the annealed likelihood function ˜pβ,θ(xs|z) =N(Wsz+bs,βσ2I)corresponds to a
scaling of the observation noise, so that we consider only the choice σ= 1, setσβ=σβ1/2and varyβ >0.
It is obvious that for any S⊂M, it holds that ˜pβ,θ(xS|z) =N(WSz+bS,σ2
βIS), whereWSandbSare
given by concatenating row-wise the emission or bias matrices for modalities in S, whileσ2
βISis the diagonal
matrix of the variances of the corresponding observations. By standard properties of Gaussian distributions,
it follows that ˜pβ,θ(xS) =N(bS,CS)whereCS=WSW⊤
S+σ2
βISis the data covariance matrix. Furthermore,
withKS=W⊤
SWS+σ2
βId, the adjusted posterior is ˜pβ,θ(z|xS) =N(K−1
SW⊤
S(xS−bS),σ2
βIdK−1
S). If we
sample orthogonal rows of W, the posterior covariance becomes diagonal so that it can - in principle - be
well approximated by an encoding distribution with a diagonal covariance matrix. Indeed, the inverse of
the posterior covariance matrix is only a function of the generative parameters of the modalities within S
and can be written as the sum σ2
βI +W⊤
SWS=σ2
βI +/summationtext
s∈SW⊤
sWs, while the posterior mean function is
xS∝⇕⊣√∫⊔≀→(σ2
βI +/summationtext
s∈SW⊤
sWs)−1/summationtext
s∈SWs(xs−bs).
Illustrative example. We consider a bi-modal setup comprising a less noisy and more noisy modality.
Concretely, for a latent variable Z= (Z1,Z2,Z3)∈R3, assume that the observed modalities can be repre-
sented as
X1=Z0+Z1+U1
X2=Z0+ 10Z2+U2,
for a standard Gaussian prior Z∼N(0,I)and independent noise variables U1,U2∼N(0,1). Note that the
secondmodalityismorenoisycomparedtothefirstone. TheresultsinTable1fortheobtainedlog-likelihood
18Published in Transactions on Machine Learning Research (09/2024)
Table 1: Gaussian model with a noisy and less noisy modality. Relative difference of the true MLE vs
the (analytical) LLH from the learned model in the first two columns, followed by multi-modal information
theoretic quantities.
Relative LLH gap Full Reconstruction Full Rates Cross Prediction Cross Rates
Aggregation our obj. mixture bound our obj. mixture bound our obj. mixture bound our ob. mixture bound our obj. mixture bound
PoE 1.29 7.11 −2.30·1035−2.2·10352.1·10352.0·1035−2.4·1034−1.9·10351.4·10351.7·1035
MoE 0.11 0.6 -32.07 -30.09 1.02 2.84 -33.27 -28.52 2.37 19.33
SumPooling 3.6·10−50.06 -2.84 -3.23 2.88 2.82 -52.58 -27.26 1.42 27.35
SelfAttention 3.4·10−50.06 -2.85 -3.23 2.87 2.82 -52.59 -27.25 1.42 27.41
Table 2: Gaussian model with five modalities: Relative difference of true LLH to the learned LLH. MCC to
true latent. The generative model for the invariant aggregation schemes uses dense decoders, whereas the
ground truth model for the permutation-equivariant encoders uses sparse decoders to account for private
latent variables. We report mean values with standard deviations in parentheses over five independent runs.
Invariant aggregation Equivariant aggregation
Proposed objective Mixture bound Proposed objective Mixture bound
Aggregation LLH Gap MCC LLH Gap MCC
PoE 0.03 (0.058) 0.75 (0.20) 0.04 (0.074) 0.77 (0.21) 0.00 (0.000) 0.91 (0.016) 0.01 (0.001) 0.88 (0.011)
MoE 0.01 (0.005) 0.82 (0.04) 0.02 (0.006) 0.67 (0.03)
SumPooling 0.00 (0.000) 0.84 (0.00) 0.00 (0.002) 0.84 (0.02) 0.00 (0.000) 0.85 (0.004) 0.00 (0.000) 0.82 (0.003)
SelfAttention 0.00 (0.003) 0.84 (0.00) 0.02 (0.007) 0.83 (0.00) 0.00 (0.000) 0.83 (0.006) 0.00 (0.000) 0.83 (0.003)
values show first that learnable aggregation models yield higher log-likelihoods6, and second that our bound
yields higher log-likelihood values compared to mixture-based bounds for any given fixed aggregation model.
We also compute various information theoretic quantities, confirming that our bound leads to higher full
reconstructions at higher full rates and lower cross predictions at lower cross rates compared to mixture-
based bounds. More flexible aggregation schemes increase the full and cross predictions for any given bound
while not necessarily increasing the full or cross rates, i.e., they can result in an improved point within a
rate-distortion curve for some configurations.
Simulation study. We consider M= 5modalities following multi-variate Gaussian laws. We consider
generative models, where all latent variables are shared across all modalities, as well as generative models,
where only parts of the latent variables are shared across all modalities, while the remaining latent vari-
ables are modality-specific. The setting of private latent variables can be incorporated by imposing sparsity
structures on the decoding matrices and allows us to analyze scenarios with considerable modality-specific
variation described through private latent variables. We provide more details about the data generation
mechanisms in Appendix J. For illustration, we use multi-modal encoders with shared latent variables using
invariant aggregations in the first case and multi-modal encoders that utilize additional equivariant aggre-
gations for the private latent variables in the second case. Results in Table 2 suggest that more flexible
aggregation schemes improve the LLH and the identifiability for both variational objectives. Furthermore,
our new bound yields higher LLH for a given aggregation scheme.
5.2 Non-linear identifiable models
Auxiliary labels as modalities. We construct artificial data following Khemakhem et al. (2020a), with
the latent variables Z∈RDbeing conditionally Gaussian having means and variances that depend on
an observed index value X2∈[K]. More precisely, pθ(z|x2) =N(µx2,Σx2), whereµc∼⊗U (−5,5)and
Σc=diag(Λc),Λc∼⊗U (0.5,3)iid forc∈[K]. The marginal distribution over the labels is uniform
U([K])so that the prior density pθ(z) =/integraltext
[K]pθ(z|x2)pθ(x2)dx2becomes a Gaussian mixture. We choose
an injective decoding function f1:RD→RD1,D≤D1, as a composition of MLPs with LeakyReLUs and
full rank weight matrices having monotonically increasing row dimensions (Khemakhem et al., 2020b), with
iid randomly sampled entries. We assume X1|Z∼N (f1(Z),σ2I)and setσ= 0.1,D=D1= 2.f1has a
6We found that a PoE model can have numerical issues here.
19Published in Transactions on Machine Learning Research (09/2024)
(a) DataX
(b) Proposed objec-
tive +SumPool
(c) Proposed objec-
tive +PoE
(d) Mixture bound
+SumPool
(e) Mixture bound
+PoE
(f) TrueZ
(g) Proposed objec-
tive +SumPool
(h) Proposed objec-
tive +PoE
(i) Mixture bound
+SumPool
(j) Mixture bound
+PoE
Figure 3: Continuous data modality in (a) and reconstructions using different bounds and fusion models in
(b)-(e). The true latent variables are shown in (f), with the inferred latent variables in (g)-(j) with a linear
transformation indeterminacy. Labels are color-coded.
single hidden layer of size D1= 2. One realization of bi-modal data X, the true latent variable Z, as well as
inferred latent variables and reconstructed data for a selection of different bounds and aggregation schemes,
are shown in Figure 3, with more examples given in Figures 6 and 7. We find that learning the aggregation
model through a SumPooling model improves the data reconstruction and better recovers the ground truth
latents, up to rotations, in contrast to a PoE model. Simulating five different such datasets, the results
in Table 3 indicate first that our bound obtains better log-likelihood estimates for different fusion schemes.
Second, it demonstrates the advantages of our new fusion schemes that achieve better log-likelihoods for both
bounds. Third, it shows the benefit of using aggregation schemes that have the capacity to accommodate
prior distributions different from a single Gaussian. Also, MoE schemes lead to low MCC values, while PoE
schemes have high MCC values.
Table3: Non-linear identifiable model with one real-valued modalityand an auxiliary label acting as a second
modality: The first four rows use a fixed standard Gaussian prior, while the last four rows use a Gaussian
mixture prior with 5components. Mean and standard deviation over 4 repetitions. Log-likelihoods are
estimated using importance sampling with 64particles.
Proposed objective Mixture bound
Aggregation LLH ( β= 1) MCC (β= 1) MCC (β= 0.1) LLH (β= 1) MCC (β= 1) MCC (β= 0.1)
PoE -43.4 (10.74) 0.98 (0.006) 0.99 (0.003) -318 (361.2) 0.97 (0.012) 0.98 (0.007)
MoE -20.5 (6.18) 0.94 (0.013) 0.93 (0.022) -57.9 (6.23) 0.93 (0.017) 0.93 (0.025)
SumPooling -17.9 (3.92) 0.99 (0.004) 0.99 (0.002) -18.9 (4.09) 0.99 (0.005) 0.99 (0.008)
SelfAttention -18.2 (4.17) 0.99 (0.004) 0.99 (0.003) -18.6 (3.73) 0.99 (0.004) 0.99 (0.007)
SumPooling -15.4 (2.12) 1.00 (0.001) 0.99 (0.004) -18.6 (2.36) 0.98 (0.008) 0.99 (0.006)
SelfAttention -15.2 (2.05) 1.00 (0.001) 1.00 (0.004) -18.6 (2.27) 0.98 (0.014) 0.98 (0.006)
SumPoolingMixture -15.1 (2.15) 1.00 (0.001) 0.99 (0.012) -18.2 (2.80) 0.98 (0.010) 0.99 (0.005)
SelfAttentionMixture -15.3 (2.35) 0.99 (0.005) 0.99 (0.004) -18.4 (2.63) 0.99 (0.007) 0.99 (0.007)
Multiple modalities. Considering the same generative model for Zwith a Gaussian mixture prior,
suppose now that instead of observing the auxiliary label, we observe multiple modalities Xs∈RDs,
Xs|Z∼ N (fs(Z),σ2I), for injective MLPs fsconstructed as above, with D= 10,Ds= 25,σ= 0.5
20Published in Transactions on Machine Learning Research (09/2024)
Table 4: Partially observed ( η= 0.5) and fully observed ( η= 0)non-linear identifiable model with 5
modalities: The first four rows use a fixed standard Gaussian prior, while the last four rows use a Gaussian
mixture prior.
Partially observed Fully observed
Proposed objective Mixture bound Proposed objective Mixture bound
Aggregation LLH MCC LLH MCC LLH MCC LLH MCC
PoE -250.9 (5.19) 0.94 (0.015) -288.4 (8.53) 0.93 (0.018) -473.6 (9.04) 0.98 (0.005) -497.7 (11.26) 0.97 (0.008)
MoE -250.1 (4.77) 0.92 (0.022) -286.2 (7.63) 0.90 (0.019) -477.9 (8.50) 0.91 (0.014) -494.6 (9.20) 0.92 (0.004)
SumPooling -249.6 (4.85) 0.95 (0.016) -275.6 (7.35) 0.92 (0.031) -471.4 (8.29) 0.99 (0.004) -480.5 (8.84) 0.98 (0.005)
SelfAttention -249.7 (4.83) 0.95 (0.014) -275.5 (7.45) 0.93 (0.022) -471.4 (8.97) 0.99 (0.002) -482.8 (10.51) 0.98 (0.004)
SumPooling -247.3 (4.23) 0.95 (0.009) -269.6 (7.42) 0.94 (0.018) -465.4 (8.16) 0.98 (0.002) -475.1 (7.54) 0.98 (0.003)
SelfAttention -247.5 (4.22) 0.95 (0.013) -269.9 (6.06) 0.93 (0.022) -469.3 (4.76) 0.98 (0.003) -474.7 (8.20) 0.98 (0.002)
SumPoolingMixture -244.8 (4.44) 0.95 (0.011) -271.9 (6.54) 0.93 (0.021) -464.5 (8.16) 0.99 (0.003) -474.2 (7.61) 0.98 (0.004)
SelfAttentionMixture -245.4 (4.55) 0.96 (0.010) -270.3 (5.96) 0.94 (0.016) -464.4 (8.50) 0.99 (0.003) -473.6 (8.24) 0.98 (0.002)
Table 5: Test LLH estimates for the joint data (M+S+T) and marginal data (importance sampling with
512 particles). The first part of the table is based on the same generative model with shared latent variable
Z∈R40, while the second part of the table is based on a restrictive generative model with a shared latent
variableZ′∈R10and modality-specific latent variables ˜Zs∈R10.
Proposed objective Mixture bound
Aggregation M+S+T M S T M+S+T M S T
PoE+ 6872 (9.62) 2599 (5.6) 4317 (1.1) -9 (0.2) 5900 (10) 2449 (10.4) 3443 (11.7) -19 (0.4)
PoE 6775 (54.9) 2585 (18.7) 4250 (8.1) -10 (2.2) 5813 (1.2) 2432 (11.6) 3390 (17.5) -19 (0.1)
MoE+ 5428 (73.5) 2391 (104) 3378 (92.9) -74 (88.7) 5420 (60.1) 2364 (33.5) 3350 (58.1) -112 (133.4)
MoE 5597 (26.7) 2449 (7.6) 3557 (26.4) -11 (0.1) 5485 (4.6) 2343 (1.8) 3415 (5.0) -17 (0.4)
SumPooling 7056 (124) 2478 (9.3) 4640 (114) -6 (0.0) 6130 (4.4) 2470 (10.3) 3660 (1.5) -16 (1.6)
SelfAttention 7011 (57.9) 2508 (18.2) 4555 (38.1) -7 (0.5) 6127 (26.1) 2510 (12.7) 3621 (8.5) -13 (0.2)
PoE+ 6549 (33.2) 2509 (7.8) 4095 (37.2) -7 (0.2) 5869 (29.6) 2465 (4.3) 3431 (8.3) -19 (1.7)
SumPooling 6337 (24.0) 2483 (9.8) 3965 (16.9) -6 (0.2) 5930 (23.8) 2468 (16.8) 3491 (18.3) -7 (0.1)
SelfAttention 6662 (20.0) 2516 (8.8) 4247 (31.2) -6 (0.4) 6716 (21.8) 2430 (26.9) 4282 (49.7) -27 (1.1)
andK=M= 5. We consider a semi-supervised setting where modalities are missing completely at random,
as in Zhang et al. (2019), with a missing rate ηas the sample average of1
|M|/summationtext
s∈M(1−Ms). Table 4 shows
that using the new variational objective improves the LLH and the identifiability of the latent representation.
Furthermore, using learnable aggregation schemes benefits both variational objectives.
5.3 MNIST-SVHN-Text
Followingpreviouswork(Sutteretal.,2020;2021;Javaloyetal.,2022), weconsideratri-modaldatasetbased
on augmenting the MNIST-SVHN dataset (Shi et al., 2019) with a text-based modality. Herein, SVHN con-
sists of relatively noisy images, whilst MNIST and text are clearer modalities. Multi-modal VAEs have
been shown to exhibit differing performances relative to their multi-modal coherence, latent classification
accuracy or test LLH, see Appendix I for definitions. Previous works often differ in their hyperparameters,
from neural network architectures, latent space dimensions, priors and likelihood families, likelihood weight-
ings, decoder variances, etc. We have chosen the same hyperparameters for all models, thereby providing
a clearer disentanglement of how either the variational objective or the aggregation scheme affects different
multi-modal evaluation measures. In particular, we consider multi-modal generative models with (i) shared
latent variables and (ii) private and shared latent variables. We also consider PoE or MoE schemes (denoted
PoE+, resp., MoE+) with additional neural network layers in their modality-specific encoding functions so
that the number of parameters matches or exceeds those of the introduced PI models, see Appendix M.5
for details. For models without private latent variables, estimates of the test LLHs in Table 5 suggest that
our bound improves the LLH across different aggregation schemes for all modalities and different βs (Table
7), with similar results for PE schemes, except for a Self-Attention model. More flexible fusion schemes
yield higher LLHs for both bounds. Qualitative results for the reconstructed modalities are given in Figures
21Published in Transactions on Machine Learning Research (09/2024)
4 with shared latent variables, in Figure 10 for different β-hyperparameters and in Figure 11 for models
with private latent variables. Cross-generation of the SVHN modality is challenging for the mixture-based
bound with all aggregation schemes. In contrast, our bound, particularly when combined with learnable
aggregation schemes, leads to more realistic samples of the cross-generated SVHN modality. No variational
objective or aggregation scheme performs best across all modalities by the generative coherence measures
(see Table 6 for uni-modal inputs, Table 8 for bi-modal ones and Tables 9- 12 for models with private latent
variables and different βs), along with reported results from external baselines (MVAE, MMVAE, MoPoE,
MMJSD, MVTCAE). Overall, our objective is slightly more coherent for cross-generating SVHN or Text, but
less coherent for MNIST. The mixture-based bound tends to improve the unsupervised latent classification
accuracy across different fusion approaches and modalities, see Table 13. To provide complementary insights
into the trade-offs for the different objectives and fusion schemes, we consider a multi-modal rate-distortion
evaluation in Figure 5. Ignoring MoE where reconstructions are similar, our bound improves the full recon-
struction with higher full rates and across various fusion schemes. The mixture-based bound yields improved
cross-predictions for all aggregation models, with increased cross-rate terms. Flexible PI architectures for
our bound improve the full reconstruction, even at lower full rates.
(a) Proposed objective
 (b) Mixture-based bound
Figure 4: Conditional generation for different aggregation schemes and bounds and shared latent variables.
The first column is the conditioned modality. The next three columns are the generated modalities using a
SumPooling aggregation, followed by the three columns for a SelfAttention aggregation, followed by PoE+,
and lastly MoE+.
5.4 Summary of experimental results
We presented a series of numerical experiments that illustrate the benefits of learning more flexible aggre-
gation models and that optimizing our variational objective leads to higher log-likelihood values. Overall,
we find that for a given choice of aggregation scheme, our objective achieves a higher log-likelihood across
the different experiments. Likewise, fixing the variational objective, we observe that Sum-Pooling or Self-
Attention encoders achieve higher multi-modal log-likelihoods compared to MoE or PoE schemes. Moreover,
we demonstrate that our variational objective results in models that differ in their information theoretic
quantities compared to those models trained with a mixture-based bound. In particular, our variational ob-
jective achieves higher full-reconstruction terms with higher full rates across different data sets, aggregation
schemes, and beta values. Conversely, the mixture-based bound improves the cross-prediction while having
higher cross-rate terms.
22Published in Transactions on Machine Learning Research (09/2024)
Table 6: Conditional coherence with shared latent variables and uni-modal inputs. The letters on the second
line represent the generated modality based on the input modalities on the line below it.
Proposed objective Mixture bound
M S T M S T
Aggregation M S T M S T M S T M S T M S T M S T
PoE 0.970.22 0.56 0.290.60 0.36 0.78 0.43 1.000.96 0.83 0.990.11 0.57 0.10 0.44 0.39 1.00
PoE+ 0.970.15 0.63 0.24 0.63 0.42 0.79 0.351.000.96 0.83 0.990.11 0.59 0.11 0.45 0.39 1.00
MoE 0.96 0.80 0.990.11 0.59 0.11 0.44 0.37 1.000.94 0.81 0.97 0.10 0.54 0.10 0.45 0.39 1.00
MoE+ 0.93 0.77 0.95 0.11 0.54 0.10 0.44 0.37 0.98 0.94 0.80 0.98 0.10 0.53 0.10 0.45 0.39 1.00
SumPooling 0.970.48 0.87 0.25 0.720.36 0.73 0.48 1.00 0.97 0.86 0.99 0.10 0.63 0.10 0.45 0.40 1.00
SelfAttention 0.970.44 0.79 0.20 0.71 0.36 0.61 0.43 1.00 0.97 0.86 0.99 0.10 0.63 0.11 0.45 0.40 1.00
Results from Sutter et al. (2021), Sutter et al. (2020) and Hwang et al. (2021)
MVAE NA 0.24 0.20 0.43 NA 0.30 0.28 0.17 NA
MMVAE NA 0.75 0.990.31 NA 0.30 0.96 0.76 NA
MoPoE NA 0.74 0.990.36 NA 0.34 0.96 0.76 NA
MMJSD NA 0.82 0.99 0.37 NA 0.36 0.97 0.83 NA
MVTCAE (w/o T) NA 0.60 NA 0.82NA NA NA NA NA
(a) Full Reconstruction −DM
 (b) Cross Prediction −Dc
\S
(c) Full Rates RM
 (d) Cross Rates R\S
Figure 5: Rate and distortion terms for MNIST-SVHN-Text with shared latent variables ( β= 1) for our
proposed objective (’Masked’) and the ’Mixture’ based bound.
6 Conclusion
Limitations. A drawback of our bound is that computing a gradient step is more expensive as it requires
drawing samples from two encoding distributions. Similarly, learning aggregation functions are more compu-
23Published in Transactions on Machine Learning Research (09/2024)
tationally expensive compared to fixed schemes. Mixture-based bounds might be preferred if one is interested
primarily in cross-modal reconstructions.
Outlook. Using modality-specific encoders to learn features and aggregating them with a PI function is
clearly not the only choice for building multi-modal encoding distributions. However, it allows us to utilize
modality-specific architectures for the encoding functions. Alternatively, our bounds could also be used, e.g.,
when multi-modal transformer architectures (Xu et al., 2022) encode multiple modalities with modality-
specific tokenization and embeddings onto a shared latent space. Our approach applies to general prior
densities if we can compute its cross-entropy relative to the multi-modal encoding distributions. An example
would be to apply it with more flexible prior distributions, e.g., as specified via score-based diffusion models
(Vahdat et al., 2021). Likewise, diffusion models could be utilized to specify PI conditional prior distribution
in the conditional bound by utilizing permutation-equivariant score models (Dutordoir et al., 2023; Yim
et al., 2023; Mathieu et al., 2023).
Acknowledgments
This work is supported by funding from the Wellcome Leap 1kD Program and by the RIE2025 Human Po-
tential Programme Prenatal/Early Childhood Grant (H22P0M0002), administered by A*STAR. The com-
putational work for this article was partially performed on resources of the National Supercomputing Centre,
Singapore ( https://www.nscc.sg ).
References
A. E. Abbas. A Kullback-Leibler view of linear and log-linear pools. Decision Analysis , 6(1):25–37, 2009.
S. Akaho. A kernel method for Canonical Correlation Analysis. In International Meeting of Psychometric
Society, 2001 , 2001.
A. Alemi, B. Poole, I. Fischer, J. Dillon, R. A. Saurous, and K. Murphy. Fixing a broken ELBO. In
International conference on machine learning , pages 159–168. PMLR, 2018.
A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep Variational Information Bottleneck. arXiv
preprint arXiv:1612.00410 , 2016.
E. S. Allman, C. Matias, and J. A. Rhodes. Identifiability of parameters in latent structure models with
many observed variables. The Annals of Statistics , 37(6A):3099–3132, 2009.
C. Archambeau and F. Bach. Sparse probabilistic projections. Advances in neural information processing
systems, 21, 2008.
R. Argelaguet, B. Velten, D. Arnol, S. Dietrich, T. Zenz, J. C. Marioni, F. Buettner, W. Huber, and
O. Stegle. Multi-Omics Factor Analysis—a framework for unsupervised integration of multi-omics data
sets.Molecular systems biology , 14(6):e8124, 2018.
J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016.
F. R. Bach and M. I. Jordan. A Probabilistic Interpretation of Canonical Correlation Analysis. 2005.
D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.
arXiv preprint arXiv:1409.0473 , 2014.
F. Bao, S. Nie, K. Xue, C. Li, S. Pu, Y. Wang, G. Yue, Y. Cao, H. Su, and J. Zhu. One transformer fits all
distributions in multi-modal diffusion at scale. In International Conference on Machine Learning , pages
1692–1717. PMLR, 2023.
D.BarberandF.Agakov. TheIMAlgorithm: avariationalapproachtoInformationMaximization. Advances
in neural information processing systems , 16(320):201, 2004.
24Published in Transactions on Machine Learning Research (09/2024)
S. Bartunov, F. B. Fuchs, and T. P. Lillicrap. Equilibrium aggregation: Encoding sets via optimization. In
Uncertainty in Artificial Intelligence , pages 139–149. PMLR, 2022.
M. Biloš and S. Günnemann. Scalable normalizing flows for permutation invariant densities. In International
Conference on Machine Learning , pages 957–967. PMLR, 2021.
D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. Journal of
the American Statistical Association , 112(518):859–877, 2017.
B. Bloem-Reddy and Y. W. Teh. Probabilistic symmetries and invariant neural networks. J. Mach. Learn.
Res., 21:90–1, 2020.
M. Bounoua, G. Franzese, and P. Michiardi. Multi-modal latent diffusion. arXiv preprint arXiv:2306.04445 ,
2023.
J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. Van-
derPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy
programs, 2018. URL http://github.com/google/jax .
M. Browne. Factor analysis of multiple batteries by maximum likelihood. British Journal of Mathematical
and Statistical Psychology , 1980.
A. Bruno, J. Willette, J. Lee, and S. J. Hwang. Mini-batch consistent slot set encoder for scalable set
encoding. Advances in Neural Information Processing Systems , 34:21365–21374, 2021.
S. Cao. Choose a transformer: Fourier or Galerkin. Advances in neural information processing systems , 34:
24924–24940, 2021.
S. Chatterjee, P. Diaconis, et al. The sample size required in importance sampling. The Annals of Applied
Probability , 28(2):1099–1135, 2018.
Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich. Gradnorm: Gradient normalization for adaptive
lossbalancingindeepmultitasknetworks. In International conference on machine learning , pages794–803.
PMLR, 2018.
J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio. A recurrent latent variable model
for sequential data. In Advances in neural information processing systems , pages 2980–2988, 2015.
B. Dai and D. Wipf. Diagnosing and enhancing vae models. In International Conference on Learning
Representations , 2018.
B. Dai, Y. Wang, J. Aston, G. Hua, and D. Wipf. Connections with robust PCA and the role of emergent
sparsity in variational autoencoder models. The Journal of Machine Learning Research , 19(1):1573–1614,
2018.
I. Daunhawer, T. M. Sutter, K. Chin-Cheong, E. Palumbo, and J. E. Vogt. On the Limitations of Multimodal
VAEs. In International Conference on Learning Representations , 2022.
I. Daunhawer, A. Bizeul, E. Palumbo, A. Marx, and J. E. Vogt. Identifiability results for multimodal
contrastive learning. arXiv preprint arXiv:2303.09166 , 2023.
P. Diaconis and D. Freedman. Finite exchangeable sequences. The Annals of Probability , pages 745–764,
1980.
A. B. Dieng, Y. Kim, A. M. Rush, and D. M. Blei. Avoiding latent variable collapse with generative skip
models. In The 22nd International Conference on Artificial Intelligence and Statistics , pages 2397–2405.
PMLR, 2019.
N. Dilokthanakul, P. A. Mediano, M. Garnelo, M. C. Lee, H. Salimbeni, K. Arulkumaran, and M. Shana-
han. Deep unsupervised clustering with Gaussian Mixture Variational Autoencoders. arXiv preprint
arXiv:1611.02648 , 2016.
25Published in Transactions on Machine Learning Research (09/2024)
V. Dutordoir, A. Saul, Z. Ghahramani, and F. Simpson. Neural diffusion processes. In International Con-
ference on Machine Learning , pages 8990–9012. PMLR, 2023.
H. Edwards and A. Storkey. Towards a neural statistician. arXiv preprint arXiv:1606.02185 , 2016.
F. Falck, H. Zhang, M. Willetts, G. Nicholson, C. Yau, and C. C. Holmes. Multi-facet clustering Variational
Autoencoders. Advances in Neural Information Processing Systems , 34:8676–8690, 2021.
M. Figurnov, S. Mohamed, and A. Mnih. Implicit reparameterization gradients. In Advances in Neural
Information Processing Systems , pages 441–452, 2018.
J. Fliege and B. F. Svaiter. Steepest descent methods for multicriteria optimization. Mathematical methods
of operations research , 51:479–494, 2000.
A. Foong, W. Bruinsma, J. Gordon, Y. Dubois, J. Requeima, and R. Turner. Meta-learning stationary
stochastic process prediction with convolutional neural processes. Advances in Neural Information Pro-
cessing Systems , 33:8284–8295, 2020.
S. Gao, R. Brekelmans, G. Ver Steeg, and A. Galstyan. Auto-encoding total correlation explanation. In The
22nd International Conference on Artificial Intelligence and Statistics , pages 1157–1166. PMLR, 2019.
M. Garnelo, D. Rosenbaum, C. Maddison, T. Ramalho, D. Saxton, M. Shanahan, Y. W. Teh, D. Rezende,
and S. A. Eslami. Conditional neural processes. In International conference on machine learning , pages
1704–1713. PMLR, 2018a.
M. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. Eslami, and Y. W. Teh. Neural processes.
arXiv preprint arXiv:1807.01622 , 2018b.
C. Genest and J. V. Zidek. Combining probability distributions: A critique and an annotated bibliography.
Statistical Science , 1(1):114–135, 1986.
C. Genest, K. J. McConway, and M. J. Schervish. Characterization of externally Bayesian pooling operators.
The Annals of Statistics , pages 487–501, 1986.
S. Ghalebikesabi, R. Cornish, L. J. Kelly, and C. Holmes. Deep generative pattern-set mixture models for
nonignorable missingness. arXiv preprint arXiv:2103.03532 , 2021.
G. Giannone and O. Winther. Scha-vae: Hierarchical context aggregation for few-shot generation. In
International Conference on Machine Learning , pages 7550–7569. PMLR, 2022.
Y. Gong, H. Hajimirsadeghi, J. He, T. Durand, and G. Mori. Variational selective autoencoder: Learning
from partially-observed heterogeneous data. In International Conference on Artificial Intelligence and
Statistics , pages 2377–2385. PMLR, 2021.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.
Generative adversarial nets. In Advances in neural information processing systems , pages 2672–2680, 2014.
A. Gretton, K. Borgwardt, M. Rasch, B. Schölkopf, and A. Smola. A Kernel Method for the Two-Sample-
Problem. Advances in neural information processing systems , 19, 2006.
H. Hälvä and A. Hyvarinen. Hidden markov nonlinear ica: Unsupervised learning from nonstationary time
series. In Conference on Uncertainty in Artificial Intelligence , pages 939–948. PMLR, 2020.
H. Hälvä, S. Le Corff, L. Lehéricy, J. So, Y. Zhu, E. Gassiat, and A. Hyvarinen. Disentangling identifiable
features from noisy data with structured nonlinear ICA. Advances in Neural Information Processing
Systems, 34:1624–1633, 2021.
D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation analysis: An overview with appli-
cation to learning methods. Neural computation , 16(12):2639–2664, 2004.
26Published in Transactions on Machine Learning Research (09/2024)
K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In Computer Vision–
ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings,
Part IV 14 , pages 630–645. Springer, 2016.
K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick. Masked Autoencoders are Scalable Vision
Learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages
16000–16009, 2022.
J. Heek, A. Levskaya, A. Oliver, M. Ritter, B. Rondepierre, A. Steiner, and M. van Zee. Flax: A neural
network library and ecosystem for JAX, 2023. URL http://github.com/google/flax .
L.B.Hewitt, M.I.Nye, A.Gane, T.Jaakkola, andJ.B.Tenenbaum. Thevariationalhomoencoder: Learning
to learn high capacity generative models from few examples. arXiv preprint arXiv:1807.08919 , 2018.
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. β-VAE:
Learning basic visual concepts with a constrained variational framework. In International conference on
learning representations , 2017.
M. D. Hoffman and M. J. Johnson. ELBO surgery: yet another way to carve up the variational evidence
lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS , 2016.
P. Holderrieth, M. J. Hutchinson, and Y. W. Teh. Equivariant learning of stochastic fields: Gaussian
processes and steerable conditional neural processes. In International Conference on Machine Learning ,
pages 4297–4307. PMLR, 2021.
H. Hotelling. Relations between two sets of variates. Biometrika , 28(3/4):321–377, 1936.
S. Huang, A. Makhzani, Y. Cao, and R. Grosse. Evaluating lossy compression rates of deep generative
models.arXiv preprint arXiv:2008.06653 , 2020.
Y. Huang, J. Lin, C. Zhou, H. Yang, and L. Huang. Modality competition: What makes joint training of
multi-modal network fail in deep learning?(provably). arXiv preprint arXiv:2203.12221 , 2022.
H. Hwang, G.-H. Kim, S. Hong, and K.-E. Kim. Multi-view representation learning via total correlation
objective. Advances in Neural Information Processing Systems , 34:12194–12207, 2021.
A. Hyvarinen and H. Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear
ICA.Advances in neural information processing systems , 29, 2016.
A. Hyvärinen and P. Pajunen. Nonlinear Independent Component Analysis: Existence and uniqueness
results.Neural networks , 12(3):429–439, 1999.
N. B. Ipsen, P.-A. Mattei, and J. Frellsen. not-MIWAE: Deep Generative Modelling with Missing not at
Random Data. In ICLR 2021-International Conference on Learning Representations , 2021.
E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint
arXiv:1611.01144 , 2016.
A. Javaloy, M. Meghdadi, and I. Valera. Mitigating Modality Collapse in Multimodal VAEs via Impartial
Optimization. arXiv preprint arXiv:2206.04496 , 2022.
Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou. Variational deep embedding: an unsupervised and
generative approach to clustering. In Proceedings of the 26th International Joint Conference on Artificial
Intelligence , pages 1965–1972, 2017.
M. J. Johnson, D. Duvenaud, A. B. Wiltschko, S. R. Datta, and R. P. Adams. Structured vaes: Composing
probabilistic graphical models and variational autoencoders. arXiv preprint arXiv:1603.06277 , 2016.
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for
graphical models. Machine learning , 37(2):183–233, 1999.
27Published in Transactions on Machine Learning Research (09/2024)
T. Joy, Y. Shi, P. H. Torr, T. Rainforth, S. M. Schmon, and N. Siddharth. Learning multimodal VAEs
through mutual supervision. arXiv preprint arXiv:2106.12570 , 2021.
M. Karami and D. Schuurmans. Deep probabilistic canonical correlation analysis. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 35, pages 8055–8063, 2021.
I. Khemakhem, D. Kingma, R. Monti, and A. Hyvarinen. Variational Autoencoders and nonlinear ICA: A
unifying framework. In International Conference on Artificial Intelligence and Statistics , pages 2207–2217.
PMLR, 2020a.
I. Khemakhem, R. Monti, D. Kingma, and A. Hyvarinen. ICE-BeeM: Identifiable Conditional Energy-
Based Deep Models Based on Nonlinear ICA. Advances in Neural Information Processing Systems , 33:
12768–12778, 2020b.
H. Kim, A. Mnih, J. Schwarz, M. Garnelo, A. Eslami, D. Rosenbaum, O. Vinyals, and Y. W. Teh. Attentive
neural processes. In International Conference on Learning Representations , 2018.
J. Kim, J. Yoo, J. Lee, and S. Hong. Setvae: Learning hierarchical composition for generative modeling
of set-structured data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 15059–15068, 2021.
Y.-g.Kim, Y.Liu, andX.-X.Wei. Covariate-informedRepresentationLearningtoPreventPosteriorCollapse
of iVAE. In International Conference on Artificial Intelligence and Statistics , pages 2641–2660. PMLR,
2023.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014.
B. Kivva, G. Rajendran, P. K. Ravikumar, and B. Aragam. Identifiability of deep generative models without
auxiliary information. In Advances in Neural Information Processing Systems , 2022.
A. Klami, S. Virtanen, and S. Kaski. Bayesian canonical correlation analysis. Journal of Machine Learning
Research , 14(4), 2013.
L. Kong, M. Q. Ma, G. Chen, E. P. Xing, Y. Chi, L.-P. Morency, and K. Zhang. Understanding masked
autoencoders via hierarchical latent variable models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 7918–7928, 2023.
X. Kong and X. Zhang. Understanding masked image modeling via learning occlusion invariant feature. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6241–6251,
2023.
W. Kool, H. van Hoof, and M. Welling. Buy 4 reinforce samples, get a baseline for free! 2019.
T.C.KoopmansandO.Reiersol. Theidentificationofstructuralcharacteristics. The Annals of Mathematical
Statistics , 21(2):165–181, 1950.
D. Kramer, P. L. Bommer, D. Durstewitz, C. Tombolini, and G. Koppe. Reconstructing nonlinear dynamical
systems from multi-modal time series. In International Conference on Machine Learning , pages 11613–
11633. PMLR, 2022.
J. B. Kruskal. More factors than subjects, tests and treatments: An indeterminacy theorem for canonical
decomposition and individual differences scaling. Psychometrika , 41(3):281–293, 1976.
T. A. Le, H. Kim, M. Garnelo, D. Rosenbaum, J. Schwarz, and Y. W. Teh. Empirical evaluation of neural
process objectives. In NeurIPS workshop on Bayesian Deep Learning , volume 4, 2018.
C.LeeandM.vanderSchaar. Avariationalinformationbottleneckapproachtomulti-omicsdataintegration.
InInternational Conference on Artificial Intelligence and Statistics , pages 1513–1521. PMLR, 2021.
28Published in Transactions on Machine Learning Research (09/2024)
J. Lee, Y. Lee, J. Kim, A. Kosiorek, S. Choi, and Y. W. Teh. Set Transformer: A framework for attention-
based permutation-invariant neural networks. In International conference on machine learning , pages
3744–3753. PMLR, 2019.
M. Lee and V. Pavlovic. Private-shared disentangled multimodal vae for learning of latent representations. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1692–1700,
2021.
C.-L. Li, M. Zaheer, Y. Zhang, B. Poczos, and R. Salakhutdinov. Point cloud GAN. arXiv preprint
arXiv:1810.05795 , 2018.
Q. Li, T. Lin, and Z. Shen. Deep neural network approximation of invariant functions through dynamical
systems. arXiv preprint arXiv:2208.08707 , 2022.
Y. Li and J. Oliva. Partially observed exchangeable modeling. In International Conference on Machine
Learning , pages 6460–6470. PMLR, 2021.
Y. Li, H. Yi, C. Bender, S. Shan, and J. B. Oliva. Exchangeable neural ode for set modeling. Advances in
Neural Information Processing Systems , 33:6936–6946, 2020.
R. Linsker. Self-organization in a perceptual network. Computer , 21(3):105–117, 1988.
G.Loaiza-Ganem,B.L.Ross,J.C.Cresswell, andA.L.Caterini. DiagnosingandFixingManifoldOverfitting
in Deep Generative Models. Transactions on Machine Learning Research , 2022.
C. Lu, Y. Wu, J. M. Hernández-Lobato, and B. Schölkopf. Invariant causal representation learning for
out-of-distribution generalization. In International Conference on Learning Representations , 2022.
J. Lucas, G. Tucker, R. B. Grosse, and M. Norouzi. Don’t Blame the ELBO! A Linear VAE Perspective on
Posterior Collapse. In Advances in Neural Information Processing Systems , pages 9408–9418, 2019.
Q. Lyu and X. Fu. Finite-sample analysis of deep CCA-based unsupervised post-nonlinear multimodal
learning. IEEE Transactions on Neural Networks and Learning Systems , 2022.
Q. Lyu, X. Fu, W. Wang, and S. Lu. Understanding latent correlation-based multiview learning and self-
supervision: An identifiability perspective. arXiv preprint arXiv:2106.07115 , 2021.
C. Ma, S. Tschiatschek, K. Palla, J. M. Hernandez-Lobato, S. Nowozin, and C. Zhang. EDDI: Efficient
Dynamic Discovery of High-Value Information with Partial VAE. In International Conference on Machine
Learning , pages 4234–4243. PMLR, 2019.
C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation of discrete
random variables. arXiv preprint arXiv:1611.00712 , 2016.
A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial Autoencoders. In ICLR, 2016.
H. Maron, E. Fetaya, N. Segol, and Y. Lipman. On the universality of invariant networks. In International
conference on machine learning , pages 4363–4371. PMLR, 2019.
E. Mathieu, T. Rainforth, N. Siddharth, and Y. W. Teh. Disentangling disentanglement in Variational
Autoencoders. In International Conference on Machine Learning , pages 4402–4412. PMLR, 2019.
E. Mathieu, V. Dutordoir, M. Hutchinson, V. De Bortoli, Y. W. Teh, and R. Turner. Geometric Neural
Diffusion Processes. Advances in Neural Information Processing Systems , 37, 2023.
K. Minoura, K. Abe, H. Nam, H. Nishikawa, and T. Shimamura. A mixture-of-experts deep generative
model for integrated analysis of single-cell multiomics data. Cell reports methods , 1(5):100071, 2021.
G. Mita, M. Filippone, and P. Michiardi. An identifiable double VAE for disentangled representations. In
International Conference on Machine Learning , pages 7769–7779. PMLR, 2021.
29Published in Transactions on Machine Learning Research (09/2024)
G. E. Moran, D. Sridhar, Y. Wang, and D. M. Blei. Identifiable deep generative models via sparse decoding.
arXiv preprint arXiv:2110.10804 , 2021.
W. Morningstar, S. Vikram, C. Ham, A. Gallagher, and J. Dillon. Automatic differentiation variational
inference with mixtures. In International Conference on Artificial Intelligence and Statistics , pages 3250–
3258. PMLR, 2021.
R. Murphy, B. Srinivasan, V. Rao, and B. Riberio. Janossy pooling: Learning deep permutation-invariant
functions for variable-size inputs. In International Conference on Learning Representations (ICLR 2019) ,
2019.
A. Nazabal, P. M. Olmos, Z. Ghahramani, and I. Valera. Handling incomplete heterogeneous data using
VAEs.Pattern Recognition , 107:107501, 2020.
A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748 , 2018.
E. Palumbo, I. Daunhawer, and J. E. Vogt. Mmvae+: Enhancing the generative quality of multimodal vaes
without compromises. In The Eleventh International Conference on Learning Representations , 2023.
E. Palumbo, L. Manduchi, S. Laguna, D. Chopard, and J. E. Vogt. Deep Generative Clustering with
Multimodal Diffusion Variational Autoencoders. In The Twelfth International Conference on Learning
Representations , 2024.
K. Pandey, A. Mukherjee, P. Rai, and A. Kumar. Diffusevae: Efficient, controllable and high-fidelity gener-
ation from low-dimensional latents. Transactions on Machine Learning Research , 2022.
B. Poole, S. Ozair, A. Van Den Oord, A. Alemi, and G. Tucker. On variational bounds of mutual information.
InInternational Conference on Machine Learning , pages 5171–5180. PMLR, 2019.
C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and
segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages
652–660, 2017.
R. Ranganath, S. Gerrish, and D. M. Blei. Black box variational inference. In AISTATS , pages 814–822,
2014.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep
generative models. In Proceedings of the 31st International Conference on Machine Learning (ICML-14) ,
pages 1278–1286, 2014.
H. E. Robbins. An empirical Bayes approach to statistics. In Breakthroughs in Statistics: Foundations and
basic theory , pages 388–394. Springer, 1992.
G. Roeder, Y. Wu, and D. Duvenaud. Sticking the landing: An asymptotically zero-variance gradient
estimator for variational inference. arXiv preprint arXiv:1703.09194 , 2017.
M. Rolinek, D. Zietlow, and G. Martius. Variational Autoencoders pursue PCA directions (by accident).
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12406–
12415, 2019.
M. Rosca, B. Lakshminarayanan, and S. Mohamed. Distribution matching in variational inference. arXiv
preprint arXiv:1802.06847 , 2018.
S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. science, 290
(5500):2323–2326, 2000.
D. B. Rubin. Inference and missing data. Biometrika , 63(3):581–592, 1976.
A. Sannai, Y. Takai, and M. Cordonnier. Universal approximations of permutation invariant/equivariant
functions by deep neural networks. arXiv preprint arXiv:1903.01939 , 2019.
30Published in Transactions on Machine Learning Research (09/2024)
A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lillicrap. A simple
neural network module for relational reasoning. Advances in neural information processing systems , 30,
2017.
S. Schneider, J. H. Lee, and M. W. Mathis. Learnable latent embeddings for joint behavioural and neural
analysis. Nature, pages 1–9, 2023.
N. Segol and Y. Lipman. On universal equivariant set networks. In International Conference on Learning
Representations , 2019.
O. Sener and V. Koltun. Multi-task learning as multi-objective optimization. Advances in neural information
processing systems , 31, 2018.
Y. Shi, B. Paige, P. Torr, et al. Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Gener-
ative Models. Advances in Neural Information Processing Systems , 32, 2019.
Y. Shi, B. Paige, P. Torr, and N. Siddharth. Relating by Contrasting: A Data-efficient Framework for
Multimodal Generative Models. In International Conference on Learning Representations , 2020.
P. Sorrenson, C. Rother, and U. Köthe. Disentanglement by nonlinear ICA with general incompressible-flow
networks (GIN). arXiv preprint arXiv:2001.04872 , 2020.
J. H. Stock and M. W. Watson. Forecasting using principal components from a large number of predictors.
Journal of the American statistical association , 97(460):1167–1179, 2002.
T. Sutter, I. Daunhawer, and J. Vogt. Multimodal generative learning utilizing Jensen-Shannon-divergence.
Advances in Neural Information Processing Systems , 33:6100–6110, 2020.
T. M. Sutter, I. Daunhawer, and J. E. Vogt. Generalized multimodal elbo. In 9th International Conference
on Learning Representations (ICLR 2021) , 2021.
M. Suzuki and Y. Matsuo. Mitigating the Limitations of Multimodal VAEs with Coordination-based Ap-
proach. 2022.
M. Suzuki, K. Nakayama, and Y. Matsuo. Joint multimodal learning with deep generative models. arXiv
preprint arXiv:1611.01891 , 2016.
Y. Tang and D. Ha. The sensory neuron as a transformer: Permutation-invariant neural networks for
reinforcement learning. Advances in Neural Information Processing Systems , 34:22574–22587, 2021.
A. Tenenhaus and M. Tenenhaus. Regularized generalized Canonical Correlation Analysis. Psychometrika ,
76:257–284, 2011.
Y. Tian, D. Krishnan, and P. Isola. Contrastive multiview coding. In Computer Vision–ECCV 2020:
16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16 , pages 776–794.
Springer, 2020.
M.E.TippingandC.M.Bishop. ProbabilisticPrincipalComponentAnalysis. Journal of the Royal Statistical
Society: Series B (Statistical Methodology) , 61(3):611–622, 1999.
M. Titsias and M. Lázaro-Gredilla. Doubly stochastic variational bayes for non-conjugate inference. In
Proceedings of the 31st International Conference on Machine Learning (ICML-14) , pages 1971–1979, 2014.
M. K. Titsias, F. J. Ruiz, S. Nikoloutsopoulos, and A. Galashov. Information theoretic meta learning with
gaussian processes. In Uncertainty in Artificial Intelligence , pages 1597–1606. PMLR, 2021.
J. M. Tomczak and M. Welling. Vae with a vampprior. arXiv preprint arXiv:1705.07120 , 2017.
Y.-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency, and R. Salakhutdinov. Multimodal trans-
former for unaligned multimodal language sequences. In Proceedings of the conference. Association for
Computational Linguistics. Meeting , volume 2019, page 6558. NIH Public Access, 2019a.
31Published in Transactions on Machine Learning Research (09/2024)
Y.-H. H. Tsai, P. P. Liang, A. Zadeh, L.-P. Morency, and R. Salakhutdinov. Learning factorized multimodal
representations. In International Conference on Representation Learning , 2019b.
A. Vahdat, K. Kreis, and J. Kautz. Score-based generative modeling in latent space. Advances in Neural
Information Processing Systems , 34, 2021.
M. Vasco, H. Yin, F. S. Melo, and A. Paiva. Leveraging hierarchy in multimodal generative models for
effective cross-modality inference. Neural Networks , 146:238–255, 2022.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.
Attention is all you need. Advances in neural information processing systems , 30, 2017.
R. Vedantam, I. Fischer, J. Huang, and K. Murphy. Generative models of visually grounded imagination.
InInternational Conference on Learning Representations , 2018.
G. Ver Steeg and A. Galstyan. Maximally informative hierarchical representations of high-dimensional data.
InArtificial Intelligence and Statistics , pages 1004–1012. PMLR, 2015.
S. Virtanen, A. Klami, S. Khan, and S. Kaski. Bayesian group factor analysis. In Artificial Intelligence and
Statistics , pages 1269–1277. PMLR, 2012.
E. Wagstaff, F. B. Fuchs, M. Engelcke, M. A. Osborne, and I. Posner. Universal approximation of functions
on sets.Journal of Machine Learning Research , 23(151):1–56, 2022.
Q. Wang and H. Van Hoof. Doubly stochastic variational inference for neural processes with hierarchical
latent variables. In International Conference on Machine Learning , pages 10018–10028. PMLR, 2020.
Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao. Learning deep transformer models
for machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics , pages 1810–1822, 2019a.
T. Wang and P. Isola. Understanding contrastive representation learning through alignment and uniformity
on the hypersphere. In International Conference on Machine Learning , pages 9929–9939. PMLR, 2020.
W. Wang, R. Arora, K. Livescu, and J. Bilmes. On deep multi-view representation learning. In International
conference on machine learning , pages 1083–1092. PMLR, 2015.
W. Wang, X. Yan, H. Lee, and K. Livescu. Deep Variational Canonical Correlation Analysis. arXiv preprint
arXiv:1610.03454 , 2016.
W. Wang, D. Tran, and M. Feiszli. What makes training multi-modal classification networks hard? In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12695–
12705, 2020.
Y. Wang, A. C. Miller, and D. M. Blei. Comment: Variational Autoencoders as Empirical Bayes. 2019b.
Y. Wang, D. Blei, and J. P. Cunningham. Posterior collapse and latent variable non-identifiability. Advances
in Neural Information Processing Systems , 34:5443–5455, 2021.
S. Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of research and
development , 4(1):66–82, 1960.
M. A. Wright and J. E. Gonzalez. Transformers are deep infinite-dimensional non-mercer binary kernel
machines. arXiv preprint arXiv:2106.01506 , 2021.
M. Wu and N. Goodman. Multimodal generative models for scalable weakly-supervised learning. Advances
in Neural Information Processing Systems , 31, 2018.
M. Wu and N. Goodman. Multimodal generative models for compositional representation learning. arXiv
preprint arXiv:1912.05075 , 2019.
32Published in Transactions on Machine Learning Research (09/2024)
Q. Xi and B. Bloem-Reddy. Indeterminacy in latent variable models: Characterization and strong identifi-
ability.arXiv preprint arXiv:2206.00801 , 2022.
R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. On layer
normalization in the transformer architecture. In International Conference on Machine Learning , pages
10524–10533. PMLR, 2020.
J. Xu, E. Dupont, K. Märtens, T. Rainforth, and Y. W. Teh. Deep Stochastic Processes via Functional
Markov Transition Operators. Advances in Neural Information Processing Systems , 37, 2023.
P. Xu, X. Zhu, and D. A. Clifton. Multimodal learning with transformers: A survey. arXiv preprint
arXiv:2206.06488 , 2022.
J. Yim, B. L. Trippe, V. De Bortoli, E. Mathieu, A. Doucet, R. Barzilay, and T. Jaakkola. SE (3) diffusion
model with application to protein backbone generation. In International Conference on Machine Learning ,
pages 40001–40039. PMLR, 2023.
T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn. Gradient surgery for multi-task learning.
Advances in Neural Information Processing Systems , 33:5824–5836, 2020.
C. Yun, S. Bhojanapalli, A. S. Rawat, S. Reddi, and S. Kumar. Are transformers universal approximators
of sequence-to-sequence functions? In International Conference on Learning Representations , 2019.
M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola. Deep Sets.
Advances in neural information processing systems , 30, 2017.
C. Zhang, Z. Han, H. Fu, J. T. Zhou, Q. Hu, et al. CPM-Nets: Cross partial multi-view networks. Advances
in Neural Information Processing Systems , 32, 2019.
F. Zhang, B. Liu, K. Wang, V. Y. Tan, Z. Yang, and Z. Wang. Relational Reasoning via Set Transformers:
Provable Efficiency and Applications to MARL. arXiv preprint arXiv:2209.09845 , 2022a.
L. Zhang, V. Tozzo, J. Higgins, and R. Ranganath. Set Norm and Equivariant Skip Connections: Putting
the Deep in Deep Sets. In International Conference on Machine Learning , pages 26559–26574. PMLR,
2022b.
Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz. Contrastive learning of medical visual
representations from paired images and text. In Machine Learning for Healthcare Conference , pages 2–25.
PMLR, 2022c.
S. Zhao, C. Gao, S. Mukherjee, and B. E. Engelhardt. Bayesian group factor analysis with structured
sparsity. The Journal of Machine Learning Research , 2016.
S. Zhao, J. Song, and S. Ermon. InfovVAE: Balancing Learning and Inference in Variational Autoencoders.
InProceedings of the aaai conference on artificial intelligence , volume 33, pages 5885–5892, 2019.
Y. Zheng, T. He, Y. Qiu, and D. P. Wipf. Learning manifold dimensions with conditional Variational
Autoencoders. Advances in Neural Information Processing Systems , 35:34709–34721, 2022.
D. Zhou and X.-X. Wei. Learning identifiable and interpretable latent models of high-dimensional neural
activity using pi-VAE. Advances in Neural Information Processing Systems , 33:7234–7247, 2020.
33Published in Transactions on Machine Learning Research (09/2024)
A Multi-modal distribution matching
Proof[Proofof Proposition 1]Theequations for LS(xS)arewellknown foruni-modalVAEs, see forexample
Zhao et al. (2019). To derive similar representations for the conditional bound, note that the first equation
(ZXconditional ) for matching the joint distribution of the latent and the missing modalities conditional on a
modality subset follows from the definition of L\S,
/integraldisplay
pd(x\S|xS)L\S(x,θ,ϕ )dx\S
=/integraldisplay
pd(x\S|xS)/integraldisplay
qϕ(z|x)/bracketleftbig
logpθ(x\S|z)−logqϕ(z|x) + logqϕ(z|xS))/bracketrightbig
dzdx\S
=/integraldisplay
pd(x\S|xS) logpd(x\S|xS)dx\S+/integraldisplay
pd(x\S|xS)/integraldisplay
qϕ(z|x)/bracketleftbigg
logpθ(x\S|z)qϕ(z|xS))
qϕ(z|x)pd(x\S|xS)/bracketrightbigg
dzdx\S
=−H(pd(x\S|xS))−KL/parenleftbig
qϕ(z|x)pd(x\S|xS)/vextendsingle/vextendsinglepθ(x\S|z)qϕ(z|xS)/parenrightbig
.
Toobtainthesecondrepresentation ( Xconditional )formatchingtheconditionaldistributionsinthedataspace,
observe that pθ(x\S|xS,z) =pθ(x\S|z)and consequently,
−/integraldisplay
pd(x\S|xS)L\S(x,θ,ϕ )dx\S−H(pd(x\S|xS))
=/integraldisplay
pd(x\S|xS)qϕ(z|x) logpd(x\S|xS)qϕ(z|x)
pθ(x\S|z)qϕ(z|xS)dzdx\S
=/integraldisplay
pd(x\S|xS)qϕ(z|x) logpd(x\S|xS)qϕ(z|x)pθ(z|xS)
pθ(x\S|z)pθ(z|xS)qϕ(z|xS)dzdx\S
=/integraldisplay
pd(x\S|xS)qϕ(z|x) logpd(x\S|xS)qϕ(z|x)pθ(z|xS)
pθ(x\S|z,xS)pθ(z|xS)qϕ(z|xS)dzdx\S
=/integraldisplay
pd(x\S|xS)qϕ(z|x) logpd(x\S|xS)qϕ(z|x)pθ(z|xS)
pθ(x\S|xS)pθ(z|xS,x\S)qϕ(z|xS)dzdx\S
=KL(pd(x\S|xS)|pθ(x\S|xS)) +/integraldisplay
pd(x\S|xS)/integraldisplay
qϕ(z|x)/bracketleftbigg
logqϕ(z|x)
pθ(z|x)+ logpθ(z|xS)
qϕ(z|xS)/bracketrightbigg
dzdx\S.
Lastly, the representation ( Zconditional ) for matching the distributions in the latent space given a modality
subset follows by recalling that
pd(x\S|xS)qϕ(z|x) =qagg
ϕ,\S(z|xS)q⋆(x\S|z,xS)
and consequently,
−/integraldisplay
pd(x\S|xS)L\S(x,θ,ϕ )dx\S−H(pd(x\S|xS))
=/integraldisplay
pd(x\S|xS)qϕ(z|x) logpd(x\S|xS)qϕ(z|x)
pθ(x\S|z)qϕ(z|xS)dzdx\S
=/integraldisplay
qagg
ϕ,\S(z|xS)q⋆(x\S|z,xS) logqagg
ϕ,\S(z|xS)q⋆(x\S|z,xS)
pθ(x\S|z)qϕ(z|xS)dzdx\S
=KL(qagg
ϕ,\S(z|xS)|qϕ(z|xS))−/integraldisplay
qagg
ϕ,\S(z|xS)/parenleftbig
KL(q⋆(x\S|z,xS)|pθ(x\S|z))/parenrightbig
dz.
34Published in Transactions on Machine Learning Research (09/2024)
B Meta-learning and Neural processes
Meta-learning. We consider a standard meta-learning setup but use slightly non-standard notations to
remain consistent with notations used in other parts of this work. We consider a compact input or covariate
spaceAand output space X. LetD=∪∞
M=1(A×X )Mbe the collection of all input-output pairs. In
meta-learning, we are given a meta-dataset, i.e., a collection of elements from D. Each individual data
setD= (a,x) =Dc∪Dt∈Dis called a task and split into a context set Dc= (ac,xc), and target set
Dt= (at,xt). We aim to predict the target set from the context set. Consider, therefore, the prediction map
π:Dc= (ac,xc)∝⇕⊣√∫⊔≀→p(xt|at,Dc) =p(xt,xc|at,ac)/p(xc|ac),
mapping each context data set to the predictive stochastic process conditioned on Dc.
VariationallowerboundsforNeuralprocesses. LatentNeuralprocesses(Garneloetal.,2018b;Foong
et al., 2020) approximate this prediction map by using a latent variable model with parameters θin the form
of
z∼pθ, pθ(xt|at,z) =/productdisplay
(a,x)∈Dtpϵ(x−fθ(a,z))
for a prior pθ, decoderfθand a parameter free density pϵ. The model is then trained by (approximately)
maximizing a lower bound on logpθ(xt|at,ac,xc). Note that for an encoding density qϕ, we have that
logpθ(xt|at,ac,xx) =/integraldisplay
qϕ(z|x,a) logpθ(xt|at,z)dz−KL(qϕ(z|a,x)|pθ(z|ac,xc)).
Since the posterior distribution pθ(z|ac,xc)is generally intractable, one instead replaces it with a variational
approximation or learned conditional prior qϕ(z|ac,xc), and optimizes the following objective
LLNP
\C(x,a) =/integraldisplay
qϕ(z|x,a) logpθ(xt|at,z)dz−KL(qϕ(z|a,x)|qϕ(z|ac,xc)).
Note that this objective coincides with L\Cconditioned on the covariate values aand whereCcomprises
the indices of the data points that are part of the context set. Using the variational lower bound LLNP
\Ccan
yield subpar performance compared to another biased log-likelihood objective (Kim et al., 2018; Foong et al.,
2020),
log ˆpθ(xt|at,ac,xc) = log
1
LL/summationdisplay
l=1exp
/summationdisplay
(xt,at)∈Dtlogpθ(xt|at,zl
c)


forLimportance samples zl
c∼qϕ(zc|xc,ac)drawn from the conditional prior as the proposal distribution.
The required number of importance samples Lfor accurate estimation scales exponentially in the forward
KL(qϕ(z|x,a)|qϕ(z|xc,ac)), see Chatterjee et al. (2018). Unlike a variational approach, such an estimator
does not enforce a Bayes-consistency term for the encoders and may be beneficial in the setting of finite
data and model capacity. Note that the Bayes consistency term for including the target set (xt,at)into the
context set (xc,ac)writes as
KL(qagg
ϕ,\C(z|xc,ac)|qϕ(z|xc,ac)) =KL/parenleftigg/integraldisplay
pd(xt|at,xc,ac)qϕ(z|x,a)dxt/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleqϕ(z|xc,ac)/parenrightigg
.
Moreover, if one wants to optimize not only the conditional but also the marginal distributions, one may
additionally optimize the variational objective corresponding to LC, i.e.,
LLNP
C(xc,ac) =/integraldisplay
qϕ(z|xc,ac) logpθ(xc|ac,z)dz−KL(qϕ(z|ac,xc)|pθ(z)),
as we do in this work for multi-modal generative models. Note that the objective LLNP
Calone can be seen
as a form of a Neural Statistician model (Edwards and Storkey, 2016) where Ccoincides with the indices of
35Published in Transactions on Machine Learning Research (09/2024)
the target set, while a form of the mixture-based bound corresponds to a Neural process bound similar to
variational Homoencoders (Hewitt et al., 2018), see also the discussion in Le et al. (2018). The multi-view
variational information bottleneck approach developed in Lee and van der Schaar (2021) for predicting X\S
givenXSinvolves the joint variational objective
LIB
S(x,θ,ϕ,β ) =/integraldisplay
qϕ(z|xS) logpθ(x\S|z)dz−βKL(qϕ(z|xS)|pθ(z))
which can be interpreted as maximizing ˆIlb
qϕ(X\S,ZS)−βˆIub
qϕ(XS,ZS)and corresponds to the variational
information bottleneck for meta-learning in Titsias et al. (2021).
C Information-theoretic perspective
We recall first that the mutual information is given by
Iqϕ(XS,ZS) =/integraldisplay
qϕ(xS,zS) logqϕ(xS,zS)
pd(xS)qagg
ϕ,S(zS)dzSdxS,
whereqagg
ϕ,S(z) =/integraltext
pd(xS)qϕ(z|xS)dxSis the aggregated prior (Makhzani et al., 2016). It can be bounded by
standard (Barber and Agakov, 2004; Alemi et al., 2016; 2018) lower and upper bounds using the rate and
distortion:
HS−DS≤HS−DS+ ∆ 1= Iqϕ(XS,ZS) =RS−∆2≤RS, (15)
with ∆1=/integraltext
qagg
ϕ(z)KL(q⋆(xS|z)|pθ(xS|z))dz > 0,∆2=KL(qagg
ϕ,S(z)|pθ(z))>0andq⋆(xS|z) =
qϕ(xS,z)/qagg
ϕ(z).
Moreover, if the bounds in (7) become tight with ∆1= ∆ 2= 0in the hypothetical scenario of infinite-
capacity decoders and encoders, one obtains/integraltext
pdLS= (1−β) Iqϕ(XS,ZS) +HS. Forβ >1, maximizingLS
yields an auto-decoding limit that minimizes Iqϕ(xS,z)for which the latent representations do not encode
any information about the data, whilst β <1yields an auto-encoding limit that maximizes Iqϕ(XS,ZS)and
for which the data is perfectly encoded and decoded.
Themixture-basedboundcanbeinterpretedasthemaximizationofavariationallowerboundof Iqϕ(XM,ZS)
and the minimization of a variational upper bound of Iqϕ(XS,ZS). Indeed, see also Daunhawer et al. (2022),
HM−DS−Dc
S≤HM−DS−Dc
S+ ∆′
1= Iqϕ(XM,ZS),
where ∆′
1=/integraltext
qagg
ϕ(z)KL(q⋆(x|z)|pθ(x|z))dz>0, due to
Iqϕ(XM,ZS) =HM−Hqϕ(X|ZS) =HM+/integraldisplay
pd(x)qϕ(z|xS) [logq⋆(x|z)] dzdx
=HM+/integraldisplay
pd(x)qϕ(z|xS)/bracketleftbigg
logpθ(xS|z) + logpθ(x\S|z) + logq⋆(x|z)
pθ(x|z)/bracketrightbigg
dzdx.
Recalling that/integraldisplay
pd(dx)LMix
S(x) =−DS−Dc
\S−βRS,
one can see that maximizing the first part of the mixture-based variational bound corresponds to maximizing
−DS−Dc
\Sas a variational lower bound of Iqϕ(XM,ZS), when ignoring the fixed entropy of the multi-modal
data. Maximizing the second part of the mixture-based variational bound corresponds to minimizing RSas
a variational upper bound of Iqϕ(XS,ZS), see (15).
Proof[Proof of Lemma 7] The proof follows by adapting the arguments in Alemi et al. (2018). The law of
X\SandZconditional on XSon the encoder path can be written as
qϕ(z,x\S|xS) =pd(x\S|xS)qϕ(z|x) =qagg
ϕ,\S(z|xS)q⋆(x\S|z,xS)
36Published in Transactions on Machine Learning Research (09/2024)
withq⋆(x\S|z,xS) =qϕ(z,x\S|xS)/qagg
ϕ,\S(z|xS). To prove a lower bound on the conditional mutual informa-
tion, note that
Iqϕ(X\S,ZM|XS)
=/integraldisplay
pd(xS)/integraldisplay
qagg
ϕ,\S(z|xS)/integraldisplay
q⋆(x\S|z,xS) logqagg
ϕ,\S(z|xS)q⋆(x\S|z,xS)
qagg
ϕ,\S(z|xS)pd(x\S|x\S)dzdx\SdxS
=/integraldisplay
pd(xS)/integraldisplay
qagg
ϕ,\S(z|xS)/bracketleftbigg/integraldisplay
q⋆(x\S|z,xS) logpθ(x\S|z))dx\S+KL(q⋆(x\S|z,xS)|pθ(x\S|z))/bracketrightbigg
dzdxS
−/integraldisplay
pd(xS)/integraldisplay
pd(x\S|xS) logpd(x\S|xS)dx
=/integraldisplay
pd(x)/integraldisplay
qϕ(z|x) logpθ(x\S|z)dzdx−/integraldisplay
pd(xS)/integraldisplay
pd(x\S|xS) logpd(x\S|xS)dx
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
=−H\S=−H(X\S|XS)
+/integraldisplay
pd(xS)/integraldisplay
qagg
ϕ,\S(z|xS)KL(q⋆(x\S|z,xS)|pθ(x\S|z))dxS
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
=∆\S,1≥0
=∆\S,1+D\S+H\S.
The upper bound follows by observing that
Iqϕ(X\S,ZM|XS)
=/integraldisplay
pd(xS)/integraldisplay
pd(x\S|xS)qϕ(z|x) logqϕ(z|x)pd(x\S|xS)
qagg
ϕ,\S(z|xS)pd(x\S|xS)dzdx
=/integraldisplay
pd(x)KL(qϕ(z|x)|qϕ(z|xS))dx−/integraldisplay
pd(xS)KL(qagg
ϕ,\S(z|xS)|qϕ(z|xS))dxS
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
=∆\S,2≥0
=R\S−∆\S,2.
Remark 17 (Total correlation based objectives) The objective suggested in Hwang et al. (2021) is
motivated by a conditional variational bottleneck perspective that aims to maximize the reduction of total
correlation of Xwhen conditioned on Z, as measured by the conditional total correlation, see Watanabe
(1960); Ver Steeg and Galstyan (2015); Gao et al. (2019), i.e.,
minimizing/braceleftig
TC(X|Z) =TC(X)−TC(X,Z) =TC(X) + Iqϕ(X,Z)−M/summationdisplay
s=1Iqϕ(Xs,Z)/bracerightig
,(16)
where TC (X) =KL(p(x)|/producttextd
i=1p(xi))ford-dimensional X. Resorting to variational lower bounds and using
a constant β > 0that weights the contributions of the mutual information terms, approximations of (16)
can be optimized by maximizing
LTC(θ,ϕ,β ) =/integraldisplay
ρ(S)/integraldisplay
{qϕ(z|x) [logpθ(x|z)] dz−βKL(qϕ(z|x)|qϕ(z|xS))}dS,
whereρis concentrated on the uni-modal subsets of M.
Remark 18 (Entropy regularised optimization) Letqbe a density over C,exp(g)be integrable with
respect toqandτ >0. The maximum of
f(q) =/integraldisplay
Cq(c) [g(c)−τlogq(c)] dc
37Published in Transactions on Machine Learning Research (09/2024)
that is attained at q⋆(c) =1
Zeg(c)/τwith normalizing constant Z=/integraltext
Ceg(c)/τdcis
f⋆=f(q⋆) =τlog/integraldisplay
Ceg(c)/τdc.
Remark 19 (Optimal variational distribution) The optimal variational density for the mixture-based
(1) multi-modal objective,
/integraldisplay
pd(dx)LMix
S(x) =/integraldisplay
pd(xS)/integraldisplay
qϕ(z|xS)/integraldisplay
pd(x\S|xS)
/bracketleftbig
logpθ(xS|z) + logpθ(x\S|z) +βlogpθ(z)−βlogqϕ(z|xS)/bracketrightbig
dx\SdzdxS,
using Remark 18, is attained at
q⋆(z|xS)∝exp/parenleftbigg1
β/integraldisplay
pd(x\S|xS)/bracketleftbig
logpθ(xS|z) + logpθ(x\S|z) +βlogpθ(z)/bracketrightbig
dx\S/parenrightbigg
∝˜pβ,θ(z|xS) exp/parenleftbigg/integraldisplay
pd(x\S|xS) log ˜pβ,θ(x\S|z)dx\S/parenrightbigg
.
D Permutation-invariant architectures
Multi-head attention and masking. We introduce here a standard multi-head attention (Bahdanau
et al., 2014; Vaswani et al., 2017) mapping MHA ϑ:RI×DX×RS×DY→RI×DYgiven by
MHAϑ(X,Y ) =WO/bracketleftbig
Head1(X,Y,Y ),...,HeadH(X,Y,Y )/bracketrightbig
, ϑ = (WQ,WK,WV,WO),
with output matrix WO∈RDA×DY, projection matrices WQ∈RDX×DAWK,WV∈RDY×DAand
Headh(Q,K,V ) =Att(QWh
Q,KWh
K,VWh
V)∈RI×D(17)
where we assume that D=DA/H∈Nis the head size. Here, the dot-product attention function is
Att(Q,K,V ) =σ(QK⊤)V,
whereσis the softmax function applied to each column of QK⊤.
Masked multi-head attention. In practice, it is convenient to consider masked multi-head attention
models MMHA ϑ,M:RI×DX×RT×DY→RI×DYfor mask matrix M∈{0,1}I×Tthat operate on key or
value sequences of fixed length Twhere theh-th head (17) is given by
Headh(Q,K,V ) =/bracketleftbig
M⊙σ(QWh
Q(KWh
K)⊤)/bracketrightbig
Vt′Wh
V∈RT×D.
Using the softmax kernel function SM D(q,k) = exp(q⊤k/√
D), we set
MMHAϑ,M(X,Y )i=T/summationdisplay
t=1H/summationdisplay
h=1MitSMD(WQ
hXi,WK
hYt)
/summationtextT
t′=1Mit′SMD(XiWQ
h,Yt′WK
h)YtWV
hWO
h (18)
which does not depend on YtifM·t= 0.
Masked self-attention. For mask matrix M=mm⊤withm= (1{s∈S})s∈M, we write
MHAϑ(YS,YS) =MMHAϑ,M(i(YS),i(YS))S.
where MMHA ϑ,Moperates on sequences with fixed length and i(YS))t=Ytift∈Sand0otherwise.
38Published in Transactions on Machine Learning Research (09/2024)
LayerNorm and SetNorm. Leth∈RT×Dand consider the normalization
N(h) =h−µ(h)
σ(h)⊙γ+β
whereµandσstandardize the input hby computing the mean, and the variance, respectively, over some
axis ofh, whilstγandβdefine a transformation. LayerNorm (Ba et al., 2016) standardises inputs over the
last axis, e.g., µ(h) =1
D/summationtextD
d=1µ·,d, i.e., separately for each element. In contrast, SetNorm (Zhang et al.,
2022b) standardises inputs over both axes, e.g., µ(h) =1
TD/summationtextT
t=1/summationtextD
d=1µt,d, thereby losing the global mean
and variance only. In both cases, γandβshare their values across the first axis. Both normalizations are
permutation-equivariant.
Transformer. We consider a masked pre-layer-norm (Wang et al., 2019a; Xiong et al., 2020) multi-head
transformer block
(MMTBϑ,M(iS(YS)))S= (Z+σReLU(LN(Z)))S
withσReLUbeing a ReLU non-linearity and
Z= iS(YS) +MMHAϑ,M(LN(iS(YS)),LN(iS(YS)))
whereM=mm⊤form= (1{s∈S})s∈M.
Set-Attention Encoders. Setg0= iS(χϑ(hS))and fork∈{1,...,L}, let
gk=MMTBϑ,M(gk−1
S).
Then, we can express the self-attention multi-modal aggregation mapping via fϑ(hS) =ρϑ/parenleftbig/summationtext
s∈SgL
s/parenrightbig
.
Remark 20 (Context-aware pooling) Assuming a single head for the transformer encoder in Example
2 with head size Dand projection matrices WQ,WK,WV∈RDP×D, the attention scores for the initial input
sequencegS=g0
S=χϑ(hS)∈R|S|×DParea(gs,gt) =⟨W⊤
Qgs,W⊤
Kgt⟩/√
D. The attention outputs os∈RD
fors∈Scan then be written as
os=1
Z/summationdisplay
t∈Sκ(gs,gt)v(gt),
whereZ=/summationtext
t∈Sκ(gs,gt)>0,v(gt) =W⊤
Vgtand
κ(gs,gt) = exp(a(gs,gt)) = exp/parenleftig
⟨W⊤
Qgs,W⊤
Kgt⟩/√
D/parenrightig
can be seen as a learnable non-symmetric kernel (Wright and Gonzalez, 2021; Cao, 2021). Conceptually, the
attention encoder pools a learnable D-dimensional function vusing a learnable context-dependent weighting
function. While such attention models directly account for the interaction between the different encodings,
a DeepSet aggregation approach may require a sufficiently high-dimensional latent space DPto achieve
universal approximation properties (Wagstaff et al., 2022).
Remark 21 (Multi-modal time series models) We have introduced a multi-modal generative model
in a general form that also applies to the time-series setup, such as when a latent Markov process
drives multiple time series. For example, consider a latent Markov process Z= (Zt)t∈Nwith prior dy-
namicspθ(z1,...,zT) =pθ(z1)/producttextT
t=2pθ(zt|zt−1)for an initial density pθ(z1)and homogeneous Markov
kernelspθ(zt|zt−1). Conditional on Z, suppose that the time-series (Xs,t)t∈Nfollows the dynamics
pθ(xs,1,...,xs,T|z1,...,zT) =/producttextT
t=2pθ(xs,t|zt)for decoding densities pθ(xs,t|zt). A common choice (Chung
et al., 2015) for modeling the encoding distribution for such sequential (uni-modal) VAEs is to assume the
factorization qϕ(z1,...zT|x1,...xT) =qϕ(z1|x1)/producttextT
t=2qϕ(zt|zt−1,xt)forxt= (xs,t)s∈M, with initial encod-
ing densities qϕ(z1|x1)and encoding Markov kernels qϕ(zt|zt−1,xt). One can again consider modality-specific
encodingshs= (hs,1,...,hs,T),hs,t=hs,φ(xs,t), now applied separately at each time step that are then
used to construct Markov kernels that are permutation-invariant in the form of q′
ϕ(zt|zt−1,πhφ(xt,S)) =
39Published in Transactions on Machine Learning Research (09/2024)
q′
ϕ(zt|zt−1,hφ(xt,S))for permutations π∈SS. Alternatively, in the absence of the auto-regressive encoding
structure with Markov kernels, one could also use transformer models that use absolute or relative positional
embeddings across the last temporal axis but no positional embeddings across the first modality axis, fol-
lowed by a sum-pooling operation across the modality axis. Note that previous works using multi-modal
time series such as Kramer et al. (2022) use a non-amortized encoding distribution for the full multi-modal
posterior only. A numerical evaluation of permutation-invariant schemes for time series models is, however,
outside the scope of this work.
Remark 22 (Alternative multi-modal encoding models) Learning different encoders for each modal-
itysubsetcanbeanalternativewhenthenumberofmodalitiesissmall. Forexample, onecouldlearndifferent
MLP heads for each modality subset Sthat aggregates the encoded features hSfrom modality-specific en-
coders that are shared for any modality mask. Our initial experiments with such encoders for the simulated
linear modalities in Section 5.1 for M= 5modalities did not improve on the permutation-invariant models,
while also being more computationally demanding.
E Permutation-equivariance and private latent variables
Remark 23 (Variational bounds with private latent variables) To compute the multi-modal varia-
tional bounds, notice that the required KL divergences can be written as follows:
KL(qϕ(z′,˜z|xS)|pθ(z′,˜z)) =KL(qϕ(z′|xS)|pθ(z′)) +/integraldisplay
qϕ(z′|xS)KL(qϕ(˜zS|z′,xS)|pθ(˜zS|z′))dz′
and
KL(qϕ(z′,˜z|xM)|qϕ(z′,˜z|xS))
=KL(qϕ(z′|xM)|(qϕ(z′|xS)) +/integraldisplay
qϕ(z′|xM)KL(qϕ(PS˜z|z′,xM)|qϕ(PS˜z|z′,xS))dz′
+/integraldisplay
qϕ(z′|xM)KL(qϕ(P\S˜z|z′,xS)|pθ(P\S˜z|z′))dz′
where PS: (˜z1,...˜zM)∝⇕⊣√∫⊔≀→(˜zs)s∈Sprojects all private latent variables to those contained in S.
These expressions can be used to compute our overall variational bound LS+L\Svia
/integraldisplay
qϕ(z′|xS)qϕ(˜zS|z′,xS)] logpθ(xS|z′,˜zS)dz′d˜zS
−KL/parenleftig
qϕ(z′|xS)qϕ(˜zS|z′,xS)/vextendsingle/vextendsingle/vextendsinglepθ(z′)pθ(˜zS|z′)/parenrightig
+/integraldisplay
qϕ(z′|xM)qϕ(˜z\S|z′,xM)] logpθ(xS|z′,˜z\S)dz′d˜zS
−KL/parenleftig
qϕ(z′,˜zS,˜z\S|xM)/vextendsingle/vextendsingle/vextendsingleqϕ(z′,˜zS,˜z\S|xS)/parenrightig
.
Remark 24 (Comparison with MMVAE+ variational bound) It is instructive to compare our
bound with the MMVAE+ approach suggested in Palumbo et al. (2023). Assuming a uniform masking
distribution restricted to uni-modal sets so that S={s}for somes∈M, we can write the bound from
Palumbo et al. (2023) as1
M/summationtextM
s=1LMMVAE+
{s}(x)with
LMMVAE+
{s}(x) =/integraldisplay
qϕ(z′|x{s})qϕ(˜z{s}|x{s})/bracketleftig
logpθ(x{s}|z′,˜z{s})/bracketrightig
dz′d˜z{s}
+/integraldisplay
qϕ(z′|x{s})rϕ(˜z\{s})/bracketleftig
logpθ(x\{s}|z′,˜z\{s})/bracketrightig
dz′d˜z\{s}
−KL/parenleftig
qMoE
ϕ(z′,˜zM|xM)/vextendsingle/vextendsingle/vextendsinglepθ(z′)pθ(˜zM)/parenrightig
.
40Published in Transactions on Machine Learning Research (09/2024)
Here, it is assumed that the multi-modal encoding distribution for computing the KL-divergence is of the
form
qMoE
ϕ(z′,˜zM|xM) =1
M/summationdisplay
s∈M(qϕ(z′|xs)qϕ(˜zs|xs))
andrϕ(˜zA) =/producttext
s∈Arϕ(˜zs)are additional trainable priordistributions.
Remark 25 (Cross-modal context variables and permutation-equivariant models) In contrast to
thePoEmodel,wheretheprivateencodingsareindependent,theprivateencodingsaredependentintheSum-
Poolingmodelbyconditioningonasamplefromthesharedlatentspace. Thesharedlatentvariable Z′canbe
seen as a shared cross-modal context variable, and similar probabilistic constructions to encode such context
variables via permutation-invariant models have been suggested in few-shot learning algorithms (Edwards
and Storkey, 2016; Giannone and Winther, 2022) or, particularly, for neural process models (Garnelo et al.,
2018b;a; Kim et al., 2018). Permutation-equivariant models have been studied for stochastic processes where
invariant priors correspond to equivariant posteriors (Holderrieth et al., 2021), such as Gaussian processes
or Neural processes with private latent variables, wherein dependencies in the private latent variables can
be constructed hierarchically (Wang and Van Hoof, 2020; Xu et al., 2023).
F Multi-modal posterior in exponential family models
Consider the setting where the decoding and encoding distributions are of the exponential family form, that
is
pθ(xs|z) =µs(xs) exp [⟨Ts(xs),fs,θ(z)⟩−logZs(fs,θ(z))]
for alls∈M, while for allS⊂M,
qϕ(z|xS) =µ(z) exp [⟨V(z),λϕ,S(xS)⟩−log ΓS(λϕ,S(xS))]
whereµsandµare base measures, Ts(xs)andV(z)are sufficient statistics, while the natural parameters
λϕ,S(xS)andfs,θ(z)are parameterized by the decoder or encoder networks, respectively, with ZsandΓS
being normalizing functions. Note that we made a standard assumption that the multi-modal encoding
distribution has a fixed base measure and sufficient statistics for any modality subset. For fixed generative
parameters θ, we want to learn a multi-modal encoding distribution that minimizes over xS∼pd,
KL(qϕ(z|xS)|pθ(z|xS))
=/integraldisplay
qϕ(z|xS)/bracketleftig
logqϕ(z|xS)−logpθ(z)−/summationdisplay
s∈Slogpθ(xs|z)/bracketrightig
dz−logpθ(xS)
=/integraldisplay
qϕ(z|xS)/bracketleftig
⟨V(z),λϕ,S(xS)⟩−log ΓS(λϕ,S(xS))−/summationdisplay
s∈Slogµs(xs)
−/braceleftbig/summationdisplay
s∈S⟨Ts,θ(xs),fs,θ(z)⟩+ logpθ(z)−/summationdisplay
s∈SZs(fs,θ(z))/bracerightbig/bracketrightig
dz−logpθ(xS)
=/integraldisplay
qϕ,ϑ(z|xS)/bracketleftig/angbracketleftig/bracketleftbiggV(z)
1/bracketrightbigg
,/bracketleftbiggλϕ,ϑ,S(xS)
−log ΓS(λϕ,ϑ,S(xS))/bracketrightbigg/angbracketrightig
−/summationdisplay
s∈S/angbracketleftig/bracketleftbiggTs(xs)
1/bracketrightbigg
,/bracketleftbiggfθ,s(z)
bθ,s(z)/bracketrightbigg/angbracketrightig/bracketrightig
dz,
withbθ,s(z) =1
|S|pθ(z)−logZs(fs,θ(z)).
G Mixture model extensions for different variational bounds
We consider the optimization of an augmented variational bound
L(x,θ,ϕ ) =/integraldisplay
ρ(S)/bracketleftig/integraldisplay
qϕ(c,z|xS) [logpθ(c,xS|z)] dzdc−KL(qϕ(c,z|xS)|pθ(c,z))
+/integraldisplay
qϕ(c,z|xS)/bracketleftbig
logpθ(x\S|z)/bracketrightbig
dzdc−KL(qϕ(c,z|x)|qϕ(c,z|xS))/bracketrightig
dS.
41Published in Transactions on Machine Learning Research (09/2024)
We will pursue here an encoding approach that does not require modeling the encoding distribution over
the discrete latent variables explicitly, thus avoiding large variances in score-based Monte Carlo estimators
(Ranganath et al., 2014) or resorting to advanced variance reduction techniques (Kool et al., 2019) or
alternatives such as continuous relaxation approaches (Jang et al., 2016; Maddison et al., 2016).
Assuming a structured variational density of the form
qϕ(c,z|xS) =qϕ(z|xS)qϕ(c|z,xS),
we can express the augmented version of (3) via
LS(xS,θ,ϕ) =/integraldisplay
qϕ(c,z|xS) [logpθ(c,xS|z)] dz−βKL(qϕ(c,z|xS)|pθ(c,z))
=/integraldisplay
qϕ(z|xS) [fx(z,xS) +fc(z,xS)] dz,
wherefx(z,xS) = logpθ(xS|z)−βlogqϕ(z|xS))and
fc(z,xS) =/integraldisplay
qϕ(c|z,xS) [−βlogqϕ(c|z,xS) +βlogpθ(c,z)] dc. (19)
We can also write the augmented version of (5) in the form of
L\S(x,θ,ϕ ) =/integraldisplay
qϕ(c,z|xS)/bracketleftbig
logpθ(x\S|z)/bracketrightbig
dz−βKL(qϕ(c,z|x)|qϕ(c,z|xS))
=/integraldisplay
qϕ(z|x)gx(z,x)dz
where
gx(z,x) = logpθ(x\S|z)−βlogqϕ(z|x) +βlogqϕ(z|xS)
which does not depend on the encoding density of the cluster variable. To optimize the variational bound
with respect to the cluster density, we can thus optimize (19), which attains its maximum value of
f⋆
c(z,xS) =βlog/integraldisplay
pθ(c)pθ(z|c)dc=βlogpθ(z)
atqϕ(c|z,xS) =pθ(c|z)due to Remark 18 below with g(c) =βlogpθ(c,z).
We can derive an analogous optimal structured variational density for the mixture-based and total-
correlation-based variational bounds. First, we can write the mixture-based bound (1) as
LMix
S(x,θ,ϕ ) =/integraldisplay
qϕ(z|xS) [logpθ(c,x|z)] dz−βKL(qϕ(c,z|xS)|pθ(c,z))
=/integraldisplay
qϕ(z|xS)/bracketleftbig
fMix
x(z,x) +fc(z,x)/bracketrightbig
dz,
wherefMix
x(z,x) = logpθ(x|z)−βlogqϕ(z|xS)andfc(z,x)has a maximum value of f⋆
c(z,x) =βlogpθ(z).
Second, we can express the corresponding terms from the total-correlation-based bound as
LTC
S(θ,ϕ) =/integraldisplay
qϕ(z|x) [logpθ(x|z)] dz−βKL(qϕ(c,z|x)|qϕ(c,z|xS))
=/integraldisplay
qϕ(z|x)/bracketleftbig
fTC
x(z,x)/bracketrightbig
dz,
wherefTC
x(z,x) = logpθ(x|z)−βlogqϕ(z|x) +βlogqϕ(z|xS).
42Published in Transactions on Machine Learning Research (09/2024)
H Algorithm and STL-gradient estimators
We consider a multi-modal extension of the sticking-the-landing (STL) gradient estimator (Roeder et al.,
2017) that has also been used in previous multi-modal bounds (Shi et al., 2019). The gradient estima-
tor ignores the score function terms when sampling qϕ(z|xS)for variance reduction purposes because it
has a zero expectation. For the bounds (2) that involves sampling from qϕ(z|xS)andqϕ(z|xM), we
thus ignore the score terms for both integrals. Consider the reparameterization with noise variables ϵS,
ϵM∼pand transformations zS=tS(ϕ,ϵS,xS) =finvariant-agg (ϑ,ϵS,S,hS), forhS=hφ,s(xs)s∈Sand
zM=tM(ϕ,ϵM,xM) =finvariant-agg (ϑ,ϵM,M,hM), forhM=hφ,s(xs)s∈M. We need to learn only a
single aggregation function that applies and masks the modalities appropriately. Pseudo-code for computing
the gradients are given in Algorithm 1. If the encoding distribution is a mixture distribution, we apply
the stop-gradient operation also to the mixture weights. Notice that in the case of a mixture prior and an
encoding distribution that includes the mixture component, the optimal encoding density over the mixture
variable has no variational parameters and is given as the posterior density of the mixture component under
the generative parameters of the prior.
Algorithm 1 Single training step for computing unbiased gradients of L(x).
Input:Multi-modal data point x, generative parameter θ, variational parameters ϕ= (φ,ϑ).
SampleS∼ρ.
SampleϵS,ϵM∼p.
SetzS=tS(ϕ,ϵS,xM)andzM=tM(ϕ,ϵM,xM).
Stop gradients of variational parameters ϕ′=stop_grad (ϕ).
Set/hatwideLS(θ,ϕ) = logpθ(xS|zS) +βlogpθ(zS)−βlogqϕ′(zS|xS).
Set/hatwideL\S(θ,ϕ) = logpθ(x\S|zM) +βlogqϕ(zM|xS)−βlogqϕ′(zM|xM).
Output:∇θ,ϕ/bracketleftig
/hatwideLS(θ,ϕ) +/hatwideL\S(θ,ϕ)/bracketrightig
In the case of private latent variables, we proceed analogously and rely on reparameterizations z′
S=
t′
S(ϕ,ϵ′
S,xS)for the shared latent variable z′
S∼qϕ(z′|xS)as above and ˜zS=˜tS(ϕ,z′,ϵS,xS) =
fequivariant-agg (ϑ,˜ϵS,z′,S,hS)for the private latent variables ˜zS∼qϕ(˜zS|z′,xS). Moreover, we write PS
for a projection on the S-coordinates. Pseudo-code for computing unbiased gradient estimates for our bound
is given in Algorithm 2.
Algorithm 2 Single training step for computing unbiased gradients of L(x)with private latent variables.
Input:Multi-modal data point x, generative parameter θ, variational parameters ϕ= (φ,ϑ).
SampleS∼ρ.
Sampleϵ′
S,ϵS,ϵ\S,ϵ′
M,ϵM,ϵ\M∼p.
Setz′
S=t′
S(ϕ,ϵ′
S,xS),˜zS=˜tS(ϕ,z′
S,ϵS,xS).
Setz′
M=t′
M(ϕ,ϵ′
M,xM),˜zM=˜tM(ϕ,z′
M,ϵM,xM).
Stop gradients of variational parameters ϕ′=stop_grad (ϕ).
Set/hatwideLS(θ,ϕ) = logpθ(xS|z′
S,˜zS) +βlogpθ(z′
S)−βlogqϕ′(z′
S|xS) +βlogpθ(˜zS|z′
S)−βlogqϕ′(˜zS|z′
S,xS).
Set/hatwideL\S(θ,ϕ) = logpθ(x\S|z′
M) +βlogqϕ(z′
M|xS)−βlogqϕ′(˜zM|z′
M,xM) +βlogqϕ(PS(˜zM)|z′
M,xS) +
βlogpθ(P\S(˜zM)|z′
M,˜zM)−βlogqϕ′(˜zM|z′
M,xM).
Output:∇θ,ϕ/bracketleftig
/hatwideLS(θ,ϕ) +/hatwideL\S(θ,ϕ)/bracketrightig
I Evaluation of multi-modal generative models
We evaluate models using different metrics suggested previously for multi-modal learning, see for example
Shi et al. (2019); Wu and Goodman (2019); Sutter et al. (2021).
43Published in Transactions on Machine Learning Research (09/2024)
Marginal, conditional and joint log-likelihoods. We can estimate the marginal log-likelihood using
classic importance sampling
logpθ(xS)≈log1
KK/summationdisplay
k=1pθ(zk,xS)
qϕ(zk|xS)
forzk∼qϕ(·|xS). This also allows to approximate the joint log-likelihood logpθ(x), and consequently also
the conditional logpθ(x\S|xS) = logpθ(x)−logpθ(xS).
Generative coherence with joint auxiliary labels. Following previous work (Shi et al., 2019; Sutter
et al., 2021; Daunhawer et al., 2022; Javaloy et al., 2022), we assess whether the generated data share the
same information in the form of the class labels across different modalities. To do so, we use pre-trained
classifiers clf s:Xs→[K]that classify values from modality stoKpossible classes. More precisely, for
S⊂M andm∈M, we compute the self- ( m∈S) or cross- ( m /∈S) coherence C S→mas the empirical
average of
1{clfm(ˆxm)=y},
over test samples xwith label ywhere ˆzS∼qϕ(z|xS)and ˆxm∼pθ(xm|ˆzS). The caseS=M\{m}
corresponds to a leave-one-out conditional coherence.
Linear classification accuracy of latent representations. To evaluate how the latent representation
can be used to predict the shared information contained in the modality subset Sbased on a linear model,
we consider the accuracy Acc Sof a linear classifier clf z:Z→[K]that is trained to predict the label
based on latent samples zS∼qϕ(zS|xtrain
S)from the training values xtrain
Sand evaluated on latent samples
zS∼qϕ(z|xtest
S)from the test values xtest
S.
J Linear models
Data generation. We generate 5data sets of N= 5000samples, each with M= 5modalities. We set
the latent dimension to D= 30, while the dimension Dsof modality sis drawn fromU(30,60). We set the
observation noise to σ= 1, shared across all modalities, as is standard for a PCA model. We sample the
components of bsindependently from N(0,1). For the setting without modality-specific latent variables, Ws
is the orthonormal matrix from a QR algorithm applied to a matrix with elements sampled iid from U(−1,1).
The bias coefficients Wbare sampled independently from N(0,1/d). Conversely, the setting with private
latent variables in the ground truth model allows us to describe modality-specific variation by considering
the sparse loading matrix
WM=
W′
1˜W10... 0
W′
2 0 ˜W2... 0
...............
W′
M 0... 0 ˜WM
.
Here,W′
s,˜Ws∈RDs×D′withD′=D/(M+ 1) = 5 , Furthermore, the latent variable Zcan be written as
Z= (Z′,˜Z1,..., ˜ZM)for private and shared latent variables ˜Zs, resp.Z′. We similarly generate orthonormal/bracketleftbig
W′
s,˜Ws/bracketrightbig
from a QR decomposition. Observe that the general generative model with latent variable Z
corresponds to the generative model (9) with shared Z′and private latent variables ˜Zwith straightforward
adjustments for the decoding functions. Similar models have been considered previously, particularly from
a Bayesian standpoint with different sparsity assumptions on the generative parameters (Archambeau and
Bach, 2008; Virtanen et al., 2012; Zhao et al., 2016).
Maximum likelihood estimation. Assume now that we observe Ndata points{xn}n∈[N], consisting of
stacking the views xn= (xs,n)s∈Sfor each modality in Sand letS=1
N/summationtextN
n=1(xn−b)(xn−b)⊤∈RDx×Dx,
Dx=/summationtextM
s=1Ds, be the sample covariance matrix across all modalities. Let Ud∈RDx×Dbe the matrix of the
firstDeigenvectors of Swith corresponding eigenvalues λ1,...λDstored in the diagonal matrix ΛD∈RD×D.
44Published in Transactions on Machine Learning Research (09/2024)
The maximum likelihood estimates are then given by bML=1
N/summationtextN
n=1xn,σ2
ML=1
N−D/summationtextN
j=D+1λjand
WML=UD(ΛD−σ2
MLI)1/2with the loading matrix identifiable up to rotations.
Model architectures. We estimate the observation noise scale σbased on the maximum likelihood esti-
mateσML. We assume linear decoder functions pθ(xs|z) =N(Wθ
sz+bθ,σ2
ML), fixed standard Gaussian prior
p(z) =N(0,I)and generative parameters θ= (Wθ
1,bθ
1,...,Wθ
M,bθ
M). Details about the various encoding
architectures are given in Table 15. The modality-specific encoding functions for the PoE and MoE schemes
have a hidden size of 512, whilst they are of size 256for the learnable aggregation schemes having additional
aggregation parameters φ.
K Non-linear identifiable models
We also show in Figure 6 the reconstructed modality values and inferred latent variables for one realization
with our bound, with the corresponding results for a mixture-based bound in Figure 7.
(a) Observed data x
 (b) True latents z
 (c) PoE (x)
 (d) PoE (z)
(e) MoE (x)
 (f) MoE (z)
 (g) SumP,K= 1(x)
(h) SumP,K= 1(z)
(i) SumP,K= 5(x)
 (j) SumP,K= 5(z)
(k) SumPM, K= 5(z)
(l) SumPM, K= 5(z)
Figure 6: Bi-modal non-linear model with label and continuous modality based on our proposed objective.
SumP: SumPooling, SumPM: SumPoolingMixture.
45Published in Transactions on Machine Learning Research (09/2024)
(a) Observed data x
 (b) True latents z
 (c) PoE (x)
 (d) PoE (z)
(e) MoE (x)
 (f) MoE (z)
 (g) SumP,K= 1(x)
(h) SumP,K= 1(z)
(i) SumP,K= 5(x)
 (j) SumP,K= 5(z)
(k) SumPM, K= 5(z)
(l) SumPM, K= 5(z)
Figure 7: Bi-modal non-linear model with label and continuous modality based on mixture bound. SumP:
SumPooling, SumPM: SumPoolingMixture.
46Published in Transactions on Machine Learning Research (09/2024)
L MNIST-SVHN-Text
L.1 Training hyperparamters
The MNIST-SVHN-Text data set is taken from the code accompanying Sutter et al. (2021) with around 1.1
million train and 200k test samples. All models are trained for 100 epochs with a batch size of 250 using
Adam (Kingma and Ba, 2014) and a cosine decay schedule from 0.0005 to 0.0001.
L.2 Multi-modal rates and distortions
(a) Full Reconstruction −DM
 (b) Cross Prediction −Dc
\S
(c) Full Rates RM
 (d) Cross Rates R\S
Figure 8: Rate and distortion terms for MNIST-SVHN-Text with shared and private latent variables.
47Published in Transactions on Machine Learning Research (09/2024)
(a) Full Reconstruction −DM
 (b) Cross Prediction −Dc
\S
(c) Full Rates RM
 (d) Cross Rates R\S
Figure 9: Rate and distortion terms for MNIST-SVHN-Text with shared latent variables and different β.
48Published in Transactions on Machine Learning Research (09/2024)
L.3 Log-likelihood estimates
Table 7: Test log-likelihood estimates for varying βchoices for the joint data (M+S+T) as well as for
the marginal data of each modality based on importance sampling (512 particles). Multi-modal generative
model with a 40-dimensional shared latent variable. The second part of the Table contains reported log-
likelihood values from baseline methods that, however, impose more restrictive assumptions on the decoder
variances, whichlikelycontributestomuchlowerlog-likelihoodvaluesreportedinpreviousworks, irrespective
of variational objectives and aggregation schemes.
Proposed objective Mixture bound
(β, Aggregation) M+S+T M S T M+S+T M S T
(0.1, PoE+) 5433 (24.5) 1786 (41.6) 3578 (63.5) -29 (2.4) 5481 (18.4) 2207 (19.8) 3180 (33.7) -39 (1.0)
(0.1, SumPooling) 7067 (78.0) 2455 (3.3) 4701 (83.5) -9 (0.4) 6061 (15.7) 2398 (9.3) 3552 (7.4) -50 (1.9)
(1.0, PoE+) 6872 (9.6) 2599 (5.6) 4317 (1.1) -9 (0.2) 5900 (10.0) 2449 (10.4) 3443 (11.7) -19 (0.4)
(1.0, SumPooling) 7056 (124.4) 2478 (9.3) 4640 (113.9) -6 (0.0) 6130 (4.4) 2470 (10.3) 3660 (1.5) -16 (1.6)
(4.0, PoE+) 7021 (13.3) 2673 (13.2) 4413 (30.5) -5 (0.1) 5895 (6.2) 2484 (5.5) 3434 (2.2) -13 (0.4)
(4.0, SumPooling) 6690 (113.4) 2483 (9.9) 4259 (117.2) -5 (0.0) 5659 (48.3) 2448 (10.5) 3233 (27.7) -10 (0.2)
Results from Sutter et al. (2021) and Sutter et al. (2020)
MVAE -1790 (3.3) NA NA NA
MMVAE -1941 (5.7) NA NA NA
MoPoE -1819 (5.7) NA NA NA
MMJSD -1961 (NA) NA NA NA
L.4 Generated modalities
(a) Proposed objective, β=
0.1
(b) Proposed objective, β=
4
(c) Mixture-based bound,
β= 0.1
(d) Mixture-based bound,
β= 4
Figure 10: Conditional generation for different βparameters. The first column is the conditioned modality.
The next three columns are the generated modalities using a SumPooling aggregation, followed by the three
columns for a PoE+ scheme.
49Published in Transactions on Machine Learning Research (09/2024)
(a) Our bound
 (b) Mixture-based bound
Figure 11: Conditional generation for permutation-equivariant schemes and private latent variable con-
straints. The first column is the conditioned modality. The next three columns are the generated modalities
using a SumPooling aggregation, followed by the three columns for a SelfAttention scheme and a PoE model.
L.5 Conditional coherence
Table 8: Conditional coherence for models with shared latent variables and bi-modal conditionals. The
letters on the second line represent the modality which is generated based on the sets of modalities on the
line below it.
Proposed objective Mixture bound
M S T M S T
Aggregation M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T
PoE 0.98 0.98 0.60 0.75 0.580.77 0.82 1.00 1.00 0.96 0.97 0.95 0.61 0.11 0.61 0.45 0.99 0.98
PoE+ 0.97 0.980.55 0.73 0.52 0.75 0.83 1.00 0.99 0.97 0.97 0.960.64 0.11 0.63 0.45 0.99 0.97
MoE 0.88 0.97 0.90 0.35 0.11 0.35 0.41 0.72 0.69 0.88 0.96 0.89 0.32 0.10 0.33 0.42 0.72 0.69
MoE+ 0.85 0.94 0.86 0.32 0.10 0.32 0.40 0.71 0.67 0.87 0.96 0.89 0.32 0.10 0.32 0.42 0.72 0.69
SumPooling 0.97 0.97 0.86 0.78 0.30 0.800.76 0.99 1.000.97 0.97 0.95 0.65 0.10 0.65 0.45 0.99 0.97
SelfAttention 0.97 0.97 0.82 0.760.30 0.78 0.69 1.00 1.00 0.97 0.97 0.99 0.66 0.10 0.65 0.45 0.99 1.00
Results from Sutter et al. (2021), Sutter et al. (2020) and Hwang et al. (2021)
MVAE NA NA 0.32 NA 0.43 NA 0.29 NA NA
MMVAE NA NA 0.87 NA 0.31 NA 0.84 NA NA
MoPoE NA NA 0.94 NA 0.36 NA 0.93 NA NA
MMJSD NA NA 0.95 NA 0.48 NA 0.92 NA NA
MVTCAE (w/o T) NA NA NA NA NA NA NA NA NA
Table 9: Conditional coherence for models with private latent variables and uni-modal conditionals. The
letters on the second line represent the modality which is generated based on the sets of modalities on the
line below it.
Proposed objective Mixture bound
M S T M S T
Aggregation M S T M S T M S T M S T M S T M S T
PoE+ 0.97 0.12 0.13 0.20 0.62 0.24 0.16 0.15 1.000.96 0.83 0.990.11 0.58 0.11 0.44 0.39 1.00
SumPooling 0.97 0.42 0.59 0.440.670.40 0.65 0.45 1.00 0.970.86 0.99 0.11 0.62 0.11 0.45 0.40 1.00
SelfAttention 0.97 0.12 0.12 0.27 0.710.28 0.46 0.40 1.00 0.96 0.09 0.08 0.12 0.67 0.12 0.15 0.17 1.00
50Published in Transactions on Machine Learning Research (09/2024)
Table 10: Conditional coherence for models with private latent variables and bi-modal conditionals. The
letters on the second line represent the modality, which is generated based on the sets of modalities on the
line below it.
Proposed objective Mixture bound
M S T M S T
Aggregation M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T
PoE+ 0.97 0.97 0.14 0.66 0.33 0.67 0.18 1.00 1.00 0.97 0.97 0.94 0.63 0.11 0.63 0.45 0.99 0.96
SumPooling 0.97 0.97 0.54 0.79 0.430.800.57 1.00 1.00 0.97 0.97 0.93 0.64 0.11 0.63 0.45 0.99 0.97
SelfAttention 0.97 0.97 0.120.800.290.810.491.00 1.00 0.96 0.96 0.08 0.70 0.12 0.70 0.15 1.00 1.00
Table 11: Conditional coherence for models with shared latent variables for different βs and uni-modal
conditionals. The letters on the second line represent the modality which is generated based on the sets of
modalities on the line below it.
Proposed objective Mixture bound
M S T M S T
(β, Aggregation) M S T M S T M S T M S T M S T M S T
(0.1, PoE+) 0.980.11 0.12 0.12 0.62 0.14 0.61 0.25 1.000.96 0.83 0.990.11 0.58 0.11 0.45 0.39 1.00
(0.1, SumPooling) 0.97 0.48 0.81 0.30 0.720.330.86 0.55 1.00 0.97 0.86 0.990.11 0.64 0.11 0.45 0.40 1.00
(1.0, PoE+) 0.97 0.15 0.63 0.24 0.63 0.42 0.79 0.35 1.000.96 0.83 0.990.11 0.59 0.11 0.45 0.39 1.00
(1.0, SumPooling) 0.97 0.48 0.87 0.25 0.720.36 0.73 0.48 1.000.970.86 0.99 0.10 0.63 0.10 0.45 0.40 1.00
(4.0, PoE+) 0.97 0.29 0.83 0.410.600.580.76 0.38 1.000.96 0.82 0.990.10 0.57 0.10 0.44 0.38 1.00
(4.0, SumPooling) 0.97 0.48 0.88 0.35 0.66 0.44 0.83 0.53 1.000.96 0.85 0.990.11 0.57 0.10 0.45 0.39 1.00
Table 12: Conditional coherence for models with shared latent variables for different βs and bi-modal con-
ditionals. The letters on the second line represent the modality, which is generated based on the sets of
modalities on the line below it.
Proposed objective Mixture bound
M S T M S T
(β, Aggregation) M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T
(0.1, PoE+) 0.98 0.98 0.15 0.70 0.14 0.72 0.66 1.00 1.00 0.96 0.96 0.93 0.62 0.11 0.62 0.45 0.99 0.95
(0.1, SumPooling) 0.97 0.97 0.86 0.830.310.840.85 0.99 1.000.97 0.97 0.94 0.66 0.11 0.65 0.45 0.99 0.96
(1.0, PoE+) 0.97 0.980.55 0.73 0.52 0.75 0.83 1.000.99 0.97 0.97 0.960.64 0.11 0.63 0.45 0.99 0.97
(1.0, SumPooling) 0.97 0.97 0.86 0.78 0.30 0.80 0.76 0.99 1.000.97 0.97 0.95 0.65 0.10 0.65 0.45 0.99 0.97
(4.0, PoE+) 0.97 0.980.84 0.76 0.660.78 0.82 1.00 1.00 0.97 0.97 0.960.62 0.10 0.62 0.45 0.99 0.98
(4.0, SumPooling) 0.97 0.97 0.89 0.77 0.40 0.78 0.860.991.000.97 0.97 0.960.61 0.10 0.60 0.45 0.99 0.97
51Published in Transactions on Machine Learning Research (09/2024)
L.6 Latent classification accuracy
Table 13: Unsupervised latent classification for β= 1and models with shared latent variables only (top
half) and shared plus private latent variables (bottom half). Accuracy is computed with a linear classifier
(logistic regression) trained on multi-modal inputs (M+S+T) or uni-modal inputs (M, S or T).
Proposed objective Mixture bound
Aggregation M+S+T M S T M+S+T M S T
PoE 0.988 (0.000) 0.940 (0.009) 0.649 (0.039) 0.998 (0.001) 0.991 (0.004) 0.977 (0.002) 0.845 (0.000) 1.000 (0.000)
PoE+ 0.978 (0.002) 0.934 (0.001) 0.624 (0.040) 0.999 (0.001) 0.998 (0.000) 0.981 (0.000) 0.851 (0.000) 1.000 (0.000)
MoE 0.841 (0.008) 0.974 (0.000) 0.609 (0.032) 1.000 (0.000) 0.940 (0.001) 0.980 (0.001) 0.843 (0.001) 1.000 (0.000)
MoE+ 0.850 (0.039) 0.967 (0.014) 0.708 (0.167) 0.983 (0.023) 0.928 (0.017) 0.983 (0.002) 0.846 (0.001) 1.000 (0.000)
SelfAttention 0.985 (0.001) 0.954 (0.002) 0.693 (0.037) 0.986 (0.006) 0.991 (0.000) 0.981 (0.001) 0.864 (0.003) 1.000 (0.000)
SumPooling 0.981 (0.000) 0.962 (0.000) 0.704 (0.014) 0.992 (0.008) 0.994 (0.000) 0.983 (0.000) 0.866 (0.002) 1.000 (0.000)
PoE+ 0.979 (0.009) 0.944 (0.000) 0.538 (0.032) 0.887 (0.07) 0.995 (0.002) 0.980 (0.002) 0.848 (0.006) 1.000 (0.000)
SumPooling 0.987 (0.004) 0.966 (0.004) 0.370 (0.348) 0.992 (0.002) 0.994 (0.001) 0.982 (0.000) 0.870 (0.001) 1.000 (0.000)
SelfAttention 0.990 (0.003) 0.968 (0.002) 0.744 (0.008) 0.985 (0.000) 0.997 (0.001) 0.974 (0.000) 0.681 (0.031) 1.000 (0.000)
Results from Sutter et al. (2021), Sutter et al. (2020) and Hwang et al. (2021)
MVAE 0.96 (0.02) 0.90 (0.01) 0.44 (0.01) 0.85 (0.10)
MMVAE 0.86 (0.03) 0.95 (0.01) 0.79 (0.05) 0.99 (0.01)
MoPoE 0.98 (0.01) 0.95 (0.01) 0.80 (0.03) 0.99 (0.01)
MMJSD 0.98 (NA) 0.97 (NA) 0.82 (NA) 0.99 (NA)
MVTCAE (w/o T) NA 0.93 (NA) 0.78 (NA) NA
Table 14: Unsupervised latent classification for different βs and models with shared latent variables only.
Accuracy is computed with a linear classifier (logistic regression) trained on multi-modal inputs (M+S+T)
or uni-modal inputs (M, S or T).
Proposed objective Mixture bound
(β, Aggregation) M+S+T M S T M+S+T M S T
(0.1, PoE+) 0.983 (0.006) 0.919 (0.001) 0.561 (0.048) 0.988 (0.014) 0.992 (0.002) 0.979 (0.002) 0.846 (0.004) 1.000 (0.000)
(0.1, SumPooling) 0.982 (0.004) 0.965 (0.002) 0.692 (0.047) 0.999 (0.001) 0.994 (0.000) 0.981 (0.002) 0.863 (0.005) 1.000 (0.000)
(1.0, PoE+) 0.978 (0.002) 0.934 (0.001) 0.624 (0.040) 0.999 (0.001) 0.998 (0.000) 0.981 (0.000) 0.851 (0.000) 1.000 (0.000)
(1.0, SumPooling) 0.981 (0.000) 0.962 (0.000) 0.704 (0.014) 0.992 (0.008) 0.994 (0.000) 0.983 (0.000) 0.866 (0.002) 1.000 (0.000)
(4.0, PoE+) 0.981 (0.006) 0.943 (0.007) 0.630 (0.008) 0.993 (0.001) 0.998 (0.000) 0.981 (0.000) 0.846 (0.001) 1.000 (0.000)
(4.0, SumPooling) 0.984 (0.004) 0.963 (0.001) 0.681 (0.009) 0.995 (0.000) 0.992 (0.002) 0.980 (0.001) 0.856 (0.001) 1.000 (0.000)
52Published in Transactions on Machine Learning Research (09/2024)
M Encoder Model architectures
M.1 Linear models
Table 15: Encoder architectures for Gaussian models.
(a) Modality-specific encoding functions hs(xs). Latent dimen-
sionD= 30, modality dimension Ds∼U (30,60).
MoE/PoE SumPooling/SelfAttention
Input:Ds Input:Ds
DenseDs×512, ReLU Dense Ds×256, ReLU
Dense 512×512, ReLU Dense 256×256, ReLU
Dense 512×60 Dense 256×60(b) Model for outer aggregation function ρϑfor
SumPooling and SelfAttention schemes.
Outer Aggregation
Input: 256
Dense 256×256, ReLU
Dense 256×256, ReLU
Dense 256×60
(c) Inner aggregation function χϑ.
SumPooling SelfAttention
Input: 256 Input: 256
Dense 256×256, ReLU Dense 256×256, ReLU
Dense 256×256, ReLU Dense 256×256
Dense 256×256(d) Transformer parameters.
SelfAttention (1 Layer)
Input: 256
Heads: 4
Attention size: 256
Hidden size FFN: 256
M.2 Linear models with private latent variables
Table 16: Encoder architectures for Gaussian models with private latent variables.
(a) Modality-specific encoding functions hs(xs). All private and
shared latent variables are of dimension 10. Modality dimension
Ds∼U (30,60).
PoE (hshared
sandhprivate
s) SumPooling/SelfAttention
Input:Ds Input:Ds
DenseDs×512, ReLU Dense Ds×128, ReLU
Dense 512×512, ReLU Dense 128×128, ReLU
Dense 512×10 Dense 128×10(b) Model for outer aggregation function
ρϑfor SumPooling scheme.
Outer Aggregation ( ρϑ)
Input: 128
Dense 128×128, ReLU
Dense 128×128, ReLU
Dense 128×10
(c) Inner aggregation functions.
SumPooling ( χ0,ϑ,χ1,ϑ,χ2,ϑ) SelfAttention (χ1,ϑ,χ2,ϑ)
Input: 128 Input: 128
Dense 128×128, ReLU Dense 128×128, ReLU
Dense 128×128, ReLU Dense 128×128
Dense 128×128(d) Transformer parameters.
SelfAttention (1 Layer)
Input: 128
Heads: 4
Attention size: 128
Hidden size FFN: 128
53Published in Transactions on Machine Learning Research (09/2024)
M.3 Nonlinear model with auxiliary label
Table 17: Encoder architectures for nonlinear model with auxiliary label.
(a) Modality-specific encoding functions hs(xs). Modality di-
mensionD1= 2(continuous modality) and D2= 5(label).
Embedding dimension DE= 4for PoE and MoE and DE= 128
otherwise.
Modality-specific encoders
Input:Ds
DenseDs×128, ReLU
Dense 128×128, ReLU
Dense 128×DE(b) Model for outer aggregation function ρϑ
for SumPooling and SelfAttention schemes and
mixtures thereof. Output dimension is D0=
25for mixture densities and DO= 4otherwise.
Outer Aggregation
Input: 128
Dense 128×128, ReLU
Dense 128×128, ReLU
Dense 128×DO
(c) Inner aggregation function χϑ.
SumPooling SelfAttention
Input: 128 Input: 128
Dense 128×128, ReLU Dense 128×128, ReLU
Dense 128×128, ReLU Dense 128×128
Dense 128×128(d) Transformer parameters.
SelfAttention
Input: 128
Heads: 4
Attention size: 128
Hidden size FFN: 128
M.4 Nonlinear model with five modalities
Table 18: Encoder architectures for a nonlinear model with five modalities.
(a) Modality-specific encoding functions hs(xs). Modality di-
mensionsDs= 25. Latent dimension D= 25
MoE/PoE SumPooling/SelfAttention
Input:Ds Input:Ds
DenseDs×512, ReLU Dense Ds×256, ReLU
Dense 512×512, ReLU Dense 256×256, ReLU
Dense 512×50 Dense 256×256(b) Model for outer aggregation function ρϑ
for SumPooling and SelfAttention schemes and
mixtures thereof. Output dimension is D0=
50for mixture densities and DO= 25other-
wise.
Outer Aggregation
Input: 256
Dense 256×256, ReLU
Dense 256×256, ReLU
Dense 256×DO
(c) Inner aggregation function χϑ.
SumPooling SelfAttention
Input: 256 Input: 256
Dense 256×256, ReLU Dense 256×256, ReLU
Dense 256×256, ReLU Dense×256
Dense 256×256(d) Transformer parameters.
SelfAttention
Input: 256
Heads: 4
Attention size: 256
Hidden size FFN: 256
M.5 MNIST-SVHN-Text
For SVHN and Text, we use 2d- or 1d-convolutional layers, respectively, denoted as Conv( f,k,s) for feature
dimensionf, kernel-size k, and stride s. We denote transposed convolutions as tConv. We use the neural
network architectures as implemented in Flax Heek et al. (2023).
54Published in Transactions on Machine Learning Research (09/2024)
Table 19: Encoder architectures for MNIST-SVHN-Text.
(a) MNIST-specific encoding functions hs(xs).
Modality dimensions Ds= 28×28. The embedding
dimension is DE= 2Dfor PoE/MoE and DE= 256
for SumPooling/SelfAttention. For PoE+/MoE+,
weaddfourtimesaDenselayerofsize 256withReLU
layer before the last linear layer.
MoE/PoE/SumPooling/SelfAttention
Input:Ds,
DenseDs×400, ReLU
Dense 400×400, ReLU
Dense 400×DE(b) SVHN-specific encoding functions hs(xs).
Modality dimensions Ds= 3×32×32. The
embedding dimension is DE= 2Dfor PoE/MoE
andDE= 256for SumPooling/SelfAttention. For
PoE+/MoE+, we add four times a Dense layer of
size 256with ReLU layer before the last linear layer.
MoE/PoE/SumPooling/SelfAttention
Input:Ds
Conv(32, 4, 2), ReLU
Conv(64, 4, 2), ReLU
Conv(64, 4, 2), ReLU
Conv(128, 4, 2), ReLU, Flatten
Dense 2048×DE
(c) Text-specific encoding functions hs(xs). Modal-
ity dimensions Ds= 8×71. Embedding di-
mension is DE= 2Dfor PoE/MoE and DE=
256for permutation-invariant models (SumPool-
ing/SelfAttention) and DE= 128for permutation-
equivariant models (SumPooling/SelfAttention). For
PoE+/MoE+, weaddfourtimesaDenselayerofsize
256with ReLU layer before the last linear layer.
MoE/PoE/SumPooling/SelfAttention
Input:Ds
Conv(128, 1, 1), ReLU
Conv(128, 4, 2), ReLU
Conv(128, 4, 2), ReLU, Flatten
Dense 128×DE(d) Model for outer aggregation function ρϑfor
SumPooling and SelfAttention schemes. Output di-
mension is D0= 2D= 80for models with shared
latent variables only and D0= 10 + 10 for mod-
els with private and shared latent variables. DE=
256for permutation-invariant and DI= 128for
permutation-invariant models.
Outer Aggregation
Input:DE
DenseDE×DE, LReLU
DenseDE×DE, LReLU
DenseDE×DO
(e) Inner aggregation function χϑfor permutation-
invariant models ( DE= 256) and permutaion-
equivariant models ( DE= 128).
SumPooling SelfAttention
Input:DE Input:DE
DenseDE×DE, LReLU Dense DE×DE, LReLU
DenseDE×DE, LReLU Dense ×DE
DenseDE×DE(f) Transformer parameters for permutation-
invariant models. DE= 256for permutation-
invariant and DI= 128for permutation-invariant
models.
SelfAttention ( 2Layers)
Input:DE
Heads: 4
Attention size: DE
Hidden size FFN: DE
N MNIST-SVHN-Text Decoder Model architectures
For models with private latent variables, we concatenate the shared and private latent variables. We use a
Laplace likelihood as the decoding distribution for MNIST and SVHN, where the decoder function learns
both its mean as a function of the latent and a constant log-standard deviation at each pixel. Following
previous works (Shi et al., 2019; Sutter et al., 2021), we re-weight the log-likelihoods for different modalities
relative to their dimensions.
55Published in Transactions on Machine Learning Research (09/2024)
Table 20: Decoder architectures for MNIST-SVHN-Text.
(a) MNIST decoder. DI= 40for models with shared
latent variables only, and DI= 10 + 10 otherwise.
MNIST
Input:DI
Dense 40×400, ReLU
Dense 400×400, ReLU
Dense 400×Ds, Sigmoid(b) SVHN decoder. DI= 40for models with shared
latent variables only, and DI= 10 + 10 otherwise.
SVHN
Input:DI
DenseDI×128, ReLU
tConv(64, 4, 3), ReLU
tConv(64, 4, 2), ReLU
tConv(32, 4, 2), ReLU
tConv(3, 4, 2)
(c) Text decoder. DI= 40for models with shared
latent variables only, and DI= 10 + 10 otherwise.
Text
Input:DI
DenseDI×128, ReLU
tConv(128, 4, 3), ReLU
tConv(128, 4, 2), ReLU
tConv(71, 1, 1)
O Compute resources and existing assets
A reference implementation is available at https://github.com/marcelah/MaskedMultimodalVAE . Our
computations were performed on shared HPC systems. All experiments except Section 5.3 were run on a
CPU server using one or two CPU cores. The experiments in Section 5.3 were run on a GPU server using
one NVIDIA A100.
Our implementation is based on JAX (Bradbury et al., 2018) and Flax (Heek et al., 2023). We com-
pute the mean correlation coefficient (MCC) between true and inferred latent variables following Khe-
makhem et al. (2020b), as in https://github.com/ilkhem/icebeem and follow the data and model
generation from Khemakhem et al. (2020a), https://github.com/ilkhem/iVAE in Section 5.2, as well
ashttps://github.com/hanmenghan/CPM_Nets from Zhang et al. (2019) for generating the missingness
mechanism. In our MNIST-SVHN-Text experiments, we use code from Sutter et al. (2021), https:
//github.com/thomassutter/MoPoE .
56