Under review as submission to TMLR
Gaussian Processes with Bayesian Inference of Covariate
Couplings
Anonymous authors
Paper under double-blind review
Abstract
Gaussian processes are powerful probabilistic models that are often coupled with Auto-
matic Relevance Determination ( ard) capable of uncovering the importance of individual
covariates. We develop covariances characterized by aﬃne transformations of the inputs,
formalized via a precision matrix between covariates, which can uncover covariate couplings
for enhanced interpretability. We study a range of couplings priors from Wishart to Horse-
shoe and present fully Bayesian inference of such precision matrices within sparse Gaussian
process. We demonstrate empirically the eﬃcacy and interpretability of this approach.
1 Introduction
Statistical models based on Gaussian Processes ( gps) oﬀer attractive modeling choices for various quantita-
tive sciences due to their ability to impose functional priors with certain desired characteristics and to carry
out principled uncertainty quantiﬁcation (Rasmussen & Williams, 2006). Modeling and inference of gps has
evolved signiﬁcantly in the directions of scalability for large data (Cutajar et al., 2017; Hensman et al., 2013;
Wilson & Nickisch, 2015), deep learning (Damianou & Lawrence, 2013; Wilson et al., 2016; Salimbeni &
Deisenroth, 2017), and generality with autodiﬀ frameworks (Krauth et al., 2017; Matthews et al., 2017).
The choice of the covariance (kernel) function plays a crucial role in specifying the function space induced
bygps. This choice is often overlooked by opting for the reputable “default” exponential ardcovariances
(Neal, 1996), which capture the importance of each covariate, but also assumes an axis-aligned anisotropic
data structure, blind to covariate couplings (Matérn, 1960).
In contrast, aﬃneanisotropic covariances are able to explicitly consider the linear dependencies between
covariates (Matérn, 1960; Poggio & Girosi, 1990), which is a common feature of real-world data, via the
precision matrix Λof the Mahalanobis distance (x−x/prime)/latticetopΛ(x−x/prime). A seminal work of Vivarelli & Williams
(1998) proposes a parameterization of the precision based on Principal Component Analysis ( pca), while
Titsias & Lazaro-Gredilla (2013) apply mean-ﬁeld variational inference over factors of such a precision matrix
Λ. Relevant works on aﬃne-covariances gps include non-stationary extensions (Paciorek & Schervish, 2003),
and applications to imaging (Kalaitzis, 2009) and material sciences (Noack et al., 2020).
In this paper, our goal is to revitalise Mahalanobis distance-based covariances as a more interpretable and
general alternative to “diagonal” ardcovariances, whereby we are able to uncover covariate couplings. This
is illustrated in Fig. 1, where we refer to these more general types of covariance functions as Automatic
Coupling Determination ( acd) covariances. We study a fully Bayesian scalable formulation of gps, where
we carry out inference over the matrix Λ, thus obtaining posterior distributions over covariate couplings.
Our contributions are as follows: (i) a gpmodel that determines covariate couplings through the analysis
of the matrix Λ; (ii) an analysis of sparsity-inducing priors for the matrix Λfrom Wishart, Laplace and
Horseshoe families; (iii) a demonstration of the enhanced explainability of acdcovariances compared to ard
covariances; (iv) the development of a fully Bayesian Markov chain Monte Carlo ( mcmc) inference scheme
of the couplings; and (v) an empirical demonstration of the usefulness of acdcovariances.
1Under review as submission to TMLR
Figure 1: The Automatic Coupling Determination ( acd) covariance reveals rich predictive covariate
couplings. Comparison between arddiagonal precisions Σ−1=diag(/lscript−2)(a) and acdprecision matrix Λmean
(b) and variance ( c) with graph illustrations. We assume an element-wise Normal prior on Λ. The acdcovariance
detects that the covariates (5,6,7) are close to redundant on the kin8nmdataset.
2 Background
We consider supervised learning problems with Ninput-label pairs {X,y}={(xn,yn)}N
n=1, with xn∈RD
andyn∈Rusing Gaussian processes (Rasmussen & Williams, 2006). Let f= (f(x1),...,f (xN))∈RNbe
unknown latent variables of inputs X= (x1,...,xN), and/producttextN
i=1p(yn|fn)be an i.i.d likelihood.
2.1 Gaussian process priors
By imposing a gpprior
f(x)∼GP (0,k) (1)
on the latent variables f, we are assuming that they are jointly Gaussian (Rasmussen & Williams, 2006).
The covariance of the multivariate Gaussian over f, which determines the properties of the functions that
can be drawn from the prior, is the kernel function k(x,x/prime;θ), whereθare hyper-parameters. The prior
overfis thenp(f|θ) =N(0,Kxx|θ), where Kxx|θis theN×Ncovariance matrix obtained by evaluating
k(x,x/prime;θ)at all input pairs {x,x/prime}. For simplicity, we assume zero-mean gps and omit the conditioning on
X.
The posteriors over fat inputs x∗, and inference or optimization over θis based on the analysis of the joint
p(y,f,θ) =p(y|f)p(f|θ)p(θ). (2)
With Gaussian likelihoods it is possible to marginalize out fleading to a Gaussian p(y,θ) =p(y|θ)p(θ).
With non-Gaussian likelihoods further complications arise due to the lack of conjugacy (Williams & Barber,
1998; Opper & Winther, 2000).
An overarching issue with gpmodels is scalability, as these models generally require costly O(N3)operations
involving Kxx|θinverses. Linearization techniques based on random features (Rahimi & Recht, 2008) were
proposed in Lázaro-Gredilla et al. (2010), and they were later developed to operate with mini-batches within
stochastic gradient optimization and to deep gps (Cutajar et al., 2017). Sparsiﬁcation techniques based
on inducing points (Williams & Seeger, 2000; Snelson & Ghahramani, 2005) were later embedded within
a variational formulation (Titsias, 2009), and they were extended to mini-batching (Hensman et al., 2013;
Krauth et al., 2017). In this paper we consider sparse gps, and in particular their fully Bayesian version
presented in Rossi et al. (2021), where all variables are treated in a Bayesian way and inference is carried
out using stochastic gradient mcmc(Chen & Zhang, 2004).
2Under review as submission to TMLR
2.2 Fully Bayesian sparse GP s
We focus on the Bayesian sparse Gaussian process ( bsgp) framework (Rossi et al., 2021), but the acd
covariance speciﬁcations apply in general to any gpimplementations. In sparse gps, we introduce a set of
Minducing variables u= (u1,...,uM)at inducing inputs Z={z1,...,zM}, such that um=f(zm)(Candela
& Rasmussen, 2005). The inducing variables are assumed to follow the original gp, yielding a joint gpprior
p(f,u|X,Z,θ) =p(f|u,X,Z,θ)p(u|Z,θ) (3)
p(u|Z,θ)∼N(0,Kzz|θ) (4)
p(f|u,X,Z,θ)∼N(Au,Kxx|θ−AK/latticetop
xz|θ), (5)
where A=Kxz|θK−1
zz|θ. This augmented model can be used for modeling tasks by introducing a likelihood
p(y|f). We assign priors over all remaining variables pψ(θ)andpξ(Z), notably including inducing locations
Zand kernel hyperparameters θ(Rossi et al., 2021). The joint becomes
p(θ,Z,u,f,y|X) =pψ(θ)pξ(Z)p(f,u|X,Z,θ)p(y|f). (6)
We can use variational formulations to integrate out fto obtain an objective that factorizes across data.
This allows parameter inference over Ψdef={u,Z,θ}with scalable mcmcbased on stochastic gradients (Chen
& Zhang, 2004).
3 Bayesian inference of covariate couplings
Inthissection, afterbrieﬂydiscussingcovarianceswith AutomaticRelevanceDetermination( ard)(MacKay,
1995; Neal, 1996), which induce some scaling of individual covariates, we present an extension involving an
aﬃne transformation of the covariates revealing couplings among these. We discuss how this is achieved by
introducing a Mahalanobis distance among inputs with a precision matrix Λ, and we show how to treat this
in a Bayesian way by imposing matrix-variate and sparsity-inducing element-wise priors. We term this type
of covariance Automatic Coupling Determination ( acd).
3.1 Automatic relevance determination
The design of covariance functions for gpmodels is an important part of the modeling process. Considering
the space of functions f:RD/mapsto→R, the choice of a covariance cov[f(x),f(x/prime)] =k(x,x/prime;θ)determines
the prior distribution over fbefore observing data. A common choice is the Gaussian covariance function
(Radial Basis Function ( rbf)):
kRBF(x,x/prime;θ)∝exp/parenleftbigg
−1
2d2(x,x/prime;θ)/parenrightbigg
, (7)
whered(x,x/prime;θ)is a parametric distance function between inputs xandx/prime. This covariance imposes a prior
over inﬁnitely diﬀerentiable (smooth) functions. Other common covariance functions based on the distance
d(x,x/prime;θ)includetheMatérncovariance, exponential, arc-cosine; see, e.g., Shawe-Taylor&Cristianini(2004)
for an in-depth treatment.
The simplest distance form
d2
ISOTROPIC (x,x/prime;θ) =1
/lscript2(x−x/prime)/latticetop(x−x/prime) (8)
induces an isotropic covariance, as all input features are scaled by the same length-scale parameter /lscriptand
contribute equally to the distance, which assumes spherical data.
Another choice increasing model ﬂexibility introduces covariate-speciﬁc length-scales parameters,
d2
ARD(x,x/prime;θ) = (x−x/prime)/latticetopΣ−1(x−x/prime) (9)
3Under review as submission to TMLR
withΣ=diag(/lscript2
1,...,/lscript2
D). This choice gives rise to covariances suitable for ard(MacKay, 1995; Neal, 1996).
Intuitively, this deﬁnition is built on the assumption that if a dimension dhas a small value of the associated
length-scale /lscriptdsmall changes in the covariate would lead to large responses in the target. The covariance
induced by this choice is anisotropic with an axis-aligned metric acting as a scaling of individual covariates.
3.2 Automatic coupling determination
The family of ardcovariances allows gpmodels to yield non-parametric and probabilistic mappings from
inputs to labels, while simultaneously determining the importance of each covariate if /lscriptd’s are optimized
or inferred. In this paper, we do not limit ourselves to assessing the relevance of each input covariate,
but to automatically discover couplings among these in a general way which can be readily applied to any
distance-based covariance function.
We replace the diagonal matrix Σ−1containing the inverse length-scales with a full Positive Semi-Deﬁnite
(psd) precision matrix Λ=Σ−1in the calculation of distances,
d2
ACD(x,x/prime;θ) = (x−x/prime)/latticetopΛ(x−x/prime) (10)
=D/summationdisplay
i,jΛij(xi−x/prime
j)2, (11)
yielding the so-called Mahalanobis distance (Titsias & Lazaro-Gredilla, 2013), which can be interpreted as
a distance obtained after an aﬃne transformation (rotation and stretching) of the inputs by the identity
(Matérn, 1960; Vivarelli & Williams, 1998; Kalaitzis, 2009)
d(Λ1
2x,Λ1
2x/prime;I) =d(˜x,˜x/prime;Λ). (12)
If the underlying distribution of the inputs xis Gaussian, this operation produces an implicit whitening of
the input data yielding ˜x. While the quadratic form in Eq. 10 has an additive form (Eq. 11), the induced
functionsdonotlendthemselvestoanadditivefunctioninterpretation(Vivarelli&Williams,1998;Duvenaud
et al., 2011).
We notice that if the precision matrix has zero elements Λij= 0, the distance function ignores the coupling
between covariates iandjin the calculation of pairwise distances among inputs.
Discriminative vs Generative modeling The parameterization of acdcovariances has apparent con-
nections with Markov Random Fields ( mrfs) (cf. Murphy (2023)), whereby the matrix Λis used to specify
an adjacency structure for a set of Drandom variables {X1,...,XD}.mrfs oﬀer the possibility to ver-
ify conditional independence properties of groups of random variables based on the analysis of Λ, while
placing no other assumptions on their underlying distribution. While it is tempting to think of the acd
parameterization of the covariance function as something to be used to draw conclusions on conditional
independence among covariates, we are eﬀectively not modeling the distribution of these. Instead, we are
pushing Λdirectly in the deﬁnition of the gppriorp(f|Λ). Therefore Λassumes the interpretation of a
precision matrix inducing an aﬃne transformation of the input, which is optimized or inferred based on the
marginal likelihood (or a lower bound thereof). Thus the focus is on performing optimization or inference of
Λto accurately modeling the labels, with the intention of obtaining some indication of the predictive power
of couplings of covariates. We leave the modeling of the input through mrfs as an interesting avenue for
future work.
3.3 Precision parameterizations
The precision matrix Λin the acdcovariance needs to be symmetric and psd. The psdconstraint in the ard
covariance is easy to satisfy, since working with a diagonal version of Λonly requires to have non-negative
elements on its diagonal and consequently, a log-transformation of the length-scales is suﬃcient.
4Under review as submission to TMLR
Table 1: Summary of precision priors and the range of hyperparameters studied.
Prior Deﬁnition Parameters Log pdf
Wishart p(Λ) =W(V,K)K=D,V=K−1ID logC−/summationtext
dlog/bardblLdd/bardbl−1
2Tr[KΛ]
Inverse Wishart p(Λ) =IW(V,K)K=D,V=ID logC−(2K+ 1)/summationtext
dlog|Ldd|−1
2Tr[VΛ−1]
Laplace p(Λij) =L(m,b)m= 0,b∈{0.01,0.1,1}logC−1
b||Λij−m||1
Horseshoe p(Λij) =HS(τ)τ∈{0.01,0.1,1} logC+1
2τ2Λ2
ij+ logE1(1
2τ2Λ2
ij)
3.3.1 Lower triangular factorization
In the case of the acdcovariance, optimization or inference of Λneeds to be performed while preserving the
psdconstraint, so that it is straightforward to operate with unconstrained optimization/ mcmcsampling.
Among all factorizations that can be used to express Λ, following (Kalaitzis, 2009), a natural parameteriza-
tion is via the lower-trangular matrix L,
Λ=LL/latticetop. (13)
Thisallowsustodirectlyoptimizeorsample Lelement-wiseandrecover Λ. Inaddition,thisparameterization
has some computational advantages in calculating Jacobians which are useful within mcmc, as discussed
shortly.
3.3.2 Low-rank factorizations
The increased ﬂexibility oﬀered by the acdformulation comes at a computational cost, which we need to
deal with: going from learning /lscript∈RDlength-scales in the ardcovariance to learning a full Λ∈RD×D
matrix. This is why, for problems where the dimensionality Dis high but we are at the same time interested
in obtaining an informative precision matrix recovering the underlying structure among the Dfeatures, we
tackle this problem with pca, similarly to Vivarelli & Williams (1998) and Paciorek & Schervish (2003).
Focusing on the acddistance, by applying a projection to the diﬀerence between data samples, we obtain:
(x−x/prime)/latticetopPdΛdP/latticetop
d(x−x/prime) (14)
where Pdis the RD×dmatrix obtained from the eigendecomposition of the empirical covariance matrix
Σ=1
NX/latticetop
cXc=PSP/latticetop, (15)
andXcis the centered RN×Dinput matrix. To obtain Pdwe select the d<Dcolumns of Pcorresponding
to thedhighest eigenvalues from S. A sample x∈RDcan be projected down to Rdthrough P/latticetop
dx. As a
result, we learn a projected version Λdin this latent representation of the full precision matrix. By applying
the projection in Eq. 14 we recover the precision matrix in the original space.
4 Priors over Λ
As a consequence of adopting the bsgpframework we need to specify a prior pψ(θ)over covariance hyper-
parametersθ. Dealingwiththe acdcovariance, thepriorisseparatelyplacedoverboththemarginalvariance
parameterσ2
fand on the precision matrix Λ. While the ﬁrst is simply a LogNormal distribution with a
ﬁxed mean and variance, the prior distribution over the precision matrix Λrequires a deeper understanding.
First of all, the Cholesky parameterization Λ=LLTin the context of mcmcsampling introduces a change
of variable. We impose a prior probability over a non-linear transformation of L, while this is the variable
that is actually sampled together with U,Zandσ2
f.
The change of measure induced by the change of variables, requires the determinant of the Jacobian J:
p(vecL) =p(vecΛ)/vextendsingle/vextendsingleJ(vecΛ,vecL)/vextendsingle/vextendsingle (16)
5Under review as submission to TMLR
Figure 2: The acdcovariances signiﬁcantly outperform ardones on select datasets, while being
competetive throughout. Test mean negative loglikelihood ( mnll) on both UCI regression benchmarks ( top) and
classiﬁcation ( bottom) benchmarks with 20%−80%error quantiles (lower is better), and rank summaries ( bottom
right).
For the lower-triangular parameterization, the determinant of the Jacobian takes a particularly convenient
form, which can be computed linearly in D(Magnus & Neudecker, 1980):
log/vextendsingle/vextendsingleJ(vecΛ,vecL)/vextendsingle/vextendsingle= log 2D/productdisplay
d(Ldd)D−d+1. (17)
We have identiﬁed two diﬀerent families of priors p(Λ): (1) matrix-variate distributions over Λand (2)
factorized scalar distributions over the single entries of the precision Λdeﬁned asp(Λ) =/producttext
ijp(Λij).
4.1 Matrix-variate priors
Wishart prior When dealing with matrix-valued distributions over psdmatrices, a natural probability
distribution to consider is the Wishart distribution. Beside being deﬁned over symmetric psdmatrices,
the Wishart prior is a conjugate distribution of precision matrices. Considering Λ∈RD×Dthe probability
density function can be expressed as:
p(Λ) =W(Λ|V,K)
=C|Λ|−1
2(K−D−1)exp/parenleftbigg
−1
2Tr(V−1Λ)/parenrightbigg
, (18)
whereC= (2KD|V|K/2ΓD(K/2))−1is a constant term, Vis the scale matrix and K≥Dis the degrees
of freedom parameter. The Bartlett decomposition proves that imposing independent Gaussian priors on
the columns of the lower-triangular matrix L= (l1,...,lD)asp(li) =N(0,V)is equivalent to a Wishart
distribution over LLTasW(λID,K). We choose K=Ddegrees of freedom and V=D−1ID, such that the
expected precision E[Λ] =IDis identity.
Inverse Wishart Another prior over psdmatrices related to the Wishart distribution is the inverse
Wishart. An interesting interpretation stems from the interpretation of Λas a covariance matrix in the
Fourier domain when Bochner’s theorem is applied:
kRBF-ACD (xi,xj;σ2
f,Λ) (19)
=Eµ,b√
2σfcos(µTxi+b)·√
2σfcos(µTxj+b),
µ∼N(0,Λ),b∼Unif[0,2π]. (20)
6Under review as submission to TMLR
Therefore, apart from viewing Λas the precision matrix of the kernel, it can also be seen as a covariance
matrix in the frequency domain, which oﬀers a motivation for using such a prior
p(Λ) =IW(Λ|V,K)
=C|Λ|−1
2(K+D+1)exp/parenleftbigg
−1
2Tr(VΛ−1)/parenrightbigg
, (21)
whereC= (2KD/ 2|V|−(K/2)ΓD(K/2))−1. We setK=DandV=ID. The inverse Wishart view can
translate into more eﬃcient random Fourier approximations.
4.2 Sparsity-inducing priors
Moving away from matrix-variate distributions, it is possible to encourage sparsity in Λwith an element-wise
prior. Since we might be interested in promoting sparsity in recovering covariance couplings to a diﬀerent
degree than in the contribution of individual covariates, we separate the prior over the elements of Λas
follows:
p(Λ) =p(Λx)·p(diag Λ)
=/productdisplay
i,j|i/negationslash=jp(Λij)/productdisplay
ip(Λii), (22)
where ΛxanddiagΛare the oﬀ-diagonal elements and the RDarray of the diagonal elements of Λ, respec-
tively. In this work, we assume a weakly informative Gaussian prior on the diagonal of Λ, while we study
diﬀerent sparsity-promoting prior distributions for Λx, as discussed next.
Laplace. A natural way to promote sparse solutions is L1-regularization (cf. graphical lasso in Friedman
et al. (2008)), which is equivalent to a Laplace prior. The expression in (22) becomes:
p(Λ) =/productdisplay
i,j|i/negationslash=jL(Λij|m,b)/productdisplay
iN(Λii|µ,σ2), (23)
where
L(Λij|m,b) =C1exp/parenleftbigg
−1
b||Λij−m||1/parenrightbigg
(24)
N(Λii|µ,σ2) =C2exp/parenleftbigg
−1
2σ2(Λii−µ)2/parenrightbigg
, (25)
whereC1andC2are the normalizing constants. We ﬁx m=µ= 0,σ2= 1, and analyze the resulting
posteriors for diﬀerent sparsity coeﬃcients b(lowerbincreases sparsity).
Horseshoe. The Horseshoe prior has become a popular probabilistic sparsity-inducing prior (Carvalho
et al., 2009),
Λij|σ,τ∼N(0,σ2τ2), σ∼C+(0,1) (26)
whereC+(0,1)is a Half-Cauchy distribution for the local shrinkage σ, whileτis the global shrinkage
parameter. The Horseshoe density of a single entry Λijis
πτ(Λij) =1√
2π3τ2exp/parenleftBigg
Λ2
ij
2τ2/parenrightBigg
E1/parenleftBigg
Λ2
ij
2τ2/parenrightBigg
, (27)
whereE1(·)is the exponential integral function that can be approximated by elementary functions.
7Under review as submission to TMLR
Figure 3: The precision matrices reveal couplings, redundancies and separabilities. The posterior mean
(a) and variance ( b) of precision matrices Λof UCI benchmark datasets with Horseshoe prior ( τ= 0.1).
5 Experiments
We consider eight UCI datasets as a benchmark to assess performance of gpmodels for regression and
classiﬁcation tasks. We standardize all datasets to zero mean and unit variance, and report all results with
ﬁve-fold cross-validation. Following previous works (e.g., Rossi et al. (2021)), we report test mnllfor all
data, and normalized root mean square error ( rmse) for regression and error for classiﬁcation tasks.
In all experiments, we chose to approximate gps with 500inducing points. We ran bsgpfor10,000iterations
with a step-size of 0.01and mini-batch of 1,000data points. We evaluate performance on test data from
50samples collected during training after 1,500burn-in iterations and using a thinning of 180. We adopt
gradientclippingfornumericalstabilityandtoavoidexplodinggradients,whichweexperiencedwhenworking
with the Horseshoe priors.
5.1 UCI benchmarks
With the above setup, we report results on UCI benchmarks by considering various choices of priors (See
Table 1). For the kernel variance σ2
fwe placed a Lognormal prior with unit variance and mean 0.05as
in Rossi et al. (2021). The proposed mcmcscheme yields good convergence and sampling eﬃciency, as
illustrated in Appendix C in the supplement; see also Fig. 19 and Fig. 20 for insights on the multimodality
of the posterior. Fig. 2 shows the comparative performance for the UCI benchmark datasets, including the
range between the 20th and 80th percentiles over the diﬀerent folds, together with a rank summary. For the
small data sets, we could also run full gps and we observed a similar trend; we refer the reader to Fig. 15
and Fig. 16 for a direct comparison between full gps and bsgps.
Interestingly, diﬀerent choices of prior and prior hyper-parameters yield comparable performance. A closer
inspection indicates that the element-wise Laplace prior performs worst overall, and this might be due to
the heavy sparsity promoted by this prior (or the lack thereof) for some hyper-parameter settings (Fig. 5).
The element-wise Horseshoe prior, while promoting sparsity, fares slightly better than the Laplace prior. It
is interesting how the inverse Wishart prior, which operates directly on Λ, promotes some sparsity after all,
while oﬀering relatively competitive performance.
5.2 Sparse couplings
Next, we study the precision matrices themselves. Fig. 3 shows the posterior precisions of all benchmark
datasets. Notablyweseestrongdependenciesemergingin kin8nm,eegandwiltdatasets, while powerplant ,
concrete ,breast, and diabetes are sparsely diagonal. We notice that the standard deviation of the
elements on Λis larger for covariate pairs with large positive/negative partial covariance, while it is generally
small for pairs that have small partial covariance. This indicates both the relative scaling of uncertainty,
and the ﬂexibility in coupling magnitudes.
We provide a more in-depth look into the dependencies in Fig. 1 (page 2) that contrasts the precision matrix
of the ardcovariance of kin8nmandbreastdatasets to the posterior mean and standard deviations of the
8Under review as submission to TMLR
precision matrix of the acdcovariances. The acddetects that 5th and 6th covariates of kin8nmare close
to redundant, and negatively coupled to 7th covariate. Less evindently, in breastwe detect coupling chains
over covariates such as (0,3,6,8) and (1,6,7), indicating predictive dependencies in the data. Fig. 9 shows
for this dataset how a diﬀerent choice of the prior distribution over Λcan reveal a diﬀerent and sparser
structure of the couplings. We visualize these as circular graphs along with the standard deviations of the
elements of the precision matrices.
Fig. 4 shows an ablation of comparing the posterior mean precision structures from Horseshoe prior with
τ={0.01,0.1,1}on the concrete dataset. The Horseshoe is able to sparsify the entire structure into an
ard-like structure, while higher τ= 1reveals oﬀ-diagonal dependencies. To obtain more intuition into the
couplings, we also visualize the covariate graphs in the bottom panel of Fig. 4 that indicate for instance the
strong dependence between the 3rd and the 4th covariate. Further illustrations on all UCI data sets for the
Wishart prior Fig. 9, inverse Wishart Fig. 10, and Laplace prior with b= 0.1Fig. 11 can be found in the
supplement.
Figure 4: The sparsity control of Horseshoe prior. The posterior mean precision matrices of Horseshoe priors
onconcrete dataset with high ( left) to low ( right) sparsity.
5.3 Sparsiﬁcation eﬀect
Fig.5showsthesparsityoftheposteriorprecisionmatrices Λinthe bostondataset. Surprisingly, theInverse
Wishart prior has an intrinsic sparsifying eﬀect. The Laplace prior sparsiﬁes according to its hyperparameter
bwhile, for this dataset, the Horseshoe prior with τ= 0.1achieves slightly more sparsity than the Horseshoe
prior withτ= 0.01.
As a conclusion of these experiments on UCI, we observe that the Horseshoe prior obtains better performance
compared to the Laplace prior and it is competitive with matrix-variate priors. Also, these generally outper-
form the ardcovariance. Interestingly, there seems to be some data-dependent eﬀect connecting sparsity
and performance; in data sets such as boston, high sparsity seems to be associated with good performance,
while for others such as eegit is the opposite. This indicates that sparsity should perhaps be treated as
a hyper-parameter and learned together with the model. We leave this interesting development for future
work.
5.4 Low-rank precision matrices
We also look at the eﬀect of low-rank precision matrices. Fig. 6 shows the posterior precision patterns learned
by the Wishart prior using a PCA with rank 11,7or3in contrast to the full rank 13. The performance
9Under review as submission to TMLR
Figure 5: The sparsiﬁcation of bostondataset. Left: Relationship between precision sparsity and hyperparam-
eters. Right: Posterior mean sparsity from diﬀerent priors.
degrades strongly at ranks lower than 11, which is likely indicative of the intrinsic rank of the dataset for
this task.
Figure 6: Overly low rank degrades performance. Posterior precisions with Wishart prior of varying rank d
of Eq. (14) ( top) and corresponding performances ( bottom) on the bostondataset.
5.5 Dependencies of motion capture data
We illustrate the capability of acdcovariances to reveal dependencies in a motion capture task, where the
subjects internal connectivity is known (Fig. 7). We observe a trajectory Y= (y1,y2,...yN)T∈RN×Dover
10Under review as submission to TMLR
Table 2: MoCap results on subject 09 using gp-odewith ARD and ACD kernels.
Metric Method Subject 09 (short)
MNLLgp-ode-vanilla ard−2.25±0.18
gp-ode-vanilla acd−2.13±0.17
MSEgp-ode-vanilla ard 34.49±5.60
gp-ode-vanilla acd 51.17±12.97
Ntimepoints, where yi∈RDrepresents the noisy observation of subject state x(ti)∈RDat timeti. The
state consists of a total of D= 50measurements across 21 body parts (Fig. 7). We follow the gp-ode
model (Heinonen et al., 2018; Hegde et al., 2022), where the state follows an ordinary diﬀerential equation
˙x(t) =f(x(t))with a vector-valued gpprior on the diﬀerential f:RD/mapsto→RD,
f(x)∼GP (0,Kθ(x,x/prime)), (28)
whereKθ∈RD×Dis an operator-valued kernel. The most straightforward covariance function is a separable
oneK(x,x/prime;θ) =k(x,x/prime;θ)ID, where we learn a shared precision matrix for all outputs. As an alternative,
we also consider a variant K(x,x/prime;θ) = diag{k(x,x/prime;θ1),...,k (z,z/prime;θD)}, where each diagonal entry has
its own kernel k(z,z/prime;θi)and its own precision matrix Λiassociated with output ˙xi.
Fig. 7 shows the posterior shared precision mean pooled over the body parts in a human walk cycle. A rich
pattern of dependencies emerges. For instance, right and left wrists are strongly coupled across the body,
while being negatively coupled to each other. The wrists move in large, cyclic and synchronised patterns,
while the back and root have little relevance, indicating their smaller movement ranges during walking.
Finally, many adjacent body parts are coupled, such as foot and tibia, and wrist and radius. Table 2 shows
the performance between ardandacdon subject 09, where the likelihoods are similar, but acddoes
perform worse in mean square error. The purpose of the experiment was to demonstrate structure learning
with standard inference runs, and we did not focus on performance tuning, which odemodels are known to
be ﬁnicky about (Hegde et al., 2022).
Figure 7: The acdcovariance reveals a highly regular coupling structure from human motion. gp-ode
model trained with shared acdcovariance in a latent space of 15 dimensions. Panel ( a) shows the precision matrix
Λreporting just the average value for each group of sensors. Panel ( b) shows the reference skeleton connectivity.
11Under review as submission to TMLR
6 Conclusions
In the literature of gps, covariances equipped with ardare popular. These materialize with the deﬁnition
of a set of length-scale parameters scaling the inputs, which are then optimized or inferred based on the
marginal likelihood (or an approximation/bound). In this work, we revisited a more general deﬁnition of
anisotropic covariances, where the distance metric among inputs is determined by a psdprecision matrix.
We showed that this extension provides a framework for metric learning and we discussed some interesting
insights on the determination of couplings among covariates. Crucially, thanks to a fully Bayesian scalable
formulation of gps, we can operate with virtually any number of data points and obtain samples from the
posterior distribution over such covariate couplings, which can be used to determine the level of conﬁdence
in their predictive power.
We also studied priors for the precision matrix Λdetermining the input metric. We showed that element-
wise Laplace and Horseshoe priors provide the highest level of sparsity, while Horseshoe priors seem to oﬀer
better performance. Interestingly, the inverse Wishart prior oﬀers higher sparsity than the Wishart prior
with overall comparable performance.
In order to address the quadratic scalability with respect to the number of covariates, we also revisited the
work by Vivarelli & Williams (1998), which proposes a low-dimensional projection of the inputs through
pca, in light of modern scalable gps and inference.
We are currently investigating an extension of our approach whereby the conclusions we can draw from the
analysis of Λare in terms of conditional independence statements. In order to do this, we plan to extend our
model to target the modeling of both labels and inputs, including a prior over the inputs p({xn}|Λ)in the
form of a Markov Random Field, where Λnow determines the conditional independence among covariates.
References
Joaquin Q. Candela and Carl E. Rasmussen. A Unifying View of Sparse Approximate Gaussian Process
Regression. Journal of Machine Learning Research , 6:1939–1959, 2005.
Carlos Carvalho, Nicholas Polson, and James Scott. Handling sparsity via the Horseshoe. In AISTATS ,
2009.
Songcan Chen and Daoqiang Zhang. Robust image segmentation using FCM with spatial constraints based
on new kernel-induced distance measure. IEEE Transactions on Systems, Man and Cybernetics, Part B ,
34(4):1907–1916, 2004.
Kurt Cutajar, Edwin V. Bonilla, Pietro Michiardi, and Maurizio Filippone. Random feature expansions for
deep Gaussian processes. In ICML, 2017.
Andreas C. Damianou and Neil D. Lawrence. Deep Gaussian Processes. In AISTATS , 2013.
David Duvenaud, Hannes Nickisch, and Carl Rasmussen. Additive Gaussian processes. In NeurIPS , 2011.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation with the
graphical lasso. Biostatistics , 2008.
Pashupati Hegde, Çağatay Yıldız, Harri Lähdesmäki, Samuel Kaski, and Markus Heinonen. Variational
multiple shooting for Bayesian ODEs with Gaussian processes. In UAI, 2022.
Markus Heinonen, Cagatay Yildiz, Henrik Mannerström, Jukka Intosalmi, and Harri Lähdesmäki. Learning
unknown ODE models with Gaussian processes. In ICML, 2018.
James Hensman, Nicolo Fusi, and Neil D. Lawrence. Gaussian Processes for Big Data. In UAI, 2013.
Alfredo Kalaitzis. Image inpainting with Gaussian processes. Technical report, University of Edinburgh,
2009.
12Under review as submission to TMLR
Karl Krauth, Edwin V. Bonilla, Kurt Cutajar, and Maurizio Filippone. AutoGP: Exploring the capabilities
and limitations of Gaussian process models. In UAI, 2017.
M. Lázaro-Gredilla, J. Quinonero-Candela, C. E. Rasmussen, and A. R. Figueiras-Vidal. Sparse Spectrum
Gaussian Process Regression. Journal of Machine Learning Research , 11:1865–1881, 2010.
D J C MacKay. Probable networks and plausible predictions-a review of practical bayesian methods for
supervised neural networks. Network: Computation in Neural Systems , 6(3):469, aug 1995. doi: 10.1088/
0954-898X/6/3/011.
Jan R. Magnus and H. Neudecker. The elimination matrix: Some lemmas and applications. SIAM Journal
on Matrix Analysis and Applications , 1(4):422–449, 1980.
Bertil Matérn. Spatial Variation . Springer, 1960.
Alexander G. Matthews, Mark van der Wilk, Tom Nickson, Keisuke Fujii, Alexis Boukouvalas, Pablo León-
Villagrá,ZoubinGhahramani,andJamesHensman. GPﬂow: AGaussianprocesslibraryusingTensorFlow.
Journal of Machine Learning Research , 18(40):1–6, April 2017.
Kevin Murphy. Probabilistic Machine learning: Advanced topic . The MIT Press, 2023.
Radford M. Neal. Bayesian Learning for Neural Networks (Lecture Notes in Statistics) . Springer, 1 edition,
August 1996. ISBN 0387947248.
Marcus Noack, Gregory Doerk, Ruipeng Li, Jason Streit, Richard Vaia, Kevin Yager, and Masafumi Fukuto.
Autonomous materials discovery driven by gaussian process regression with inhomogeneous measurement
noise and anisotropic kernels. Scientiﬁc reports , 2020.
M.OpperandO.Winther. Gaussianprocessesforclassiﬁcation: Mean-ﬁeldalgorithms. Neural Computation ,
12(11):2655–2684, 2000.
Christopher Paciorek and Mark Schervish. Nonstationary covariance functions for Gaussian process regres-
sion. InNIPS, 2003.
Tomaso Poggio and Federico Girosi. Networks for approximation and learning. In Proceedings of the IEEE ,
1990.
Ali Rahimi and Benjamin Recht. Random Features for Large-Scale Kernel Machines. In NIPS, 2008.
Carl E. Rasmussen and Christopher Williams. Gaussian Processes for Machine Learning . MIT Press, 2006.
Simone Rossi, Markus Heinonen, Edwin Bonilla, Zheyang Shen, and Maurizio Filippone. Sparse Gaussian
processes revisited: Bayesian approaches to inducing-variable approximations. In AISTATS , 2021.
Hugh Salimbeni and Marc Deisenroth. Doubly Stochastic Variational Inference for Deep Gaussian Processes.
InNeurIPS , 2017.
John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis . Cambridge University Press,
New York, NY, USA, 2004.
Edward Snelson and Zoubin Ghahramani. Sparse Gaussian Processes using Pseudo-inputs. In NIPS, 2005.
Michalis Titsias and Miguel Lazaro-Gredilla. Variational inference for mahalanobis distance metrics in
gaussian process regression. In NIPS, 2013.
Michalis K. Titsias. Variational Learning of Inducing Variables in Sparse Gaussian Processes. In AISTATS ,
2009.
FrancescoVivarelliandChristopherWilliams. Discoveringhiddenfeatureswithgaussianprocessesregression.
InNIPS, 1998.
13Under review as submission to TMLR
Christopher K. I. Williams and David Barber. Bayesian classiﬁcation with Gaussian processes. IEEE
Transactions on Pattern Analysis and Machine Intelligence , 20:1342–1351, 1998.
Christopher K. I. Williams and Matthias Seeger. Using the Nyström method to speed up kernel machines.
InNIPS, 2000.
Andrew Wilson and Hannes Nickisch. Kernel Interpolation for Scalable Structured Gaussian Processes
(KISS-GP). In ICML, 2015.
AndrewG.Wilson, ZhitingHu, RuslanSalakhutdinov, andEricP.Xing. DeepKernelLearning. In AISTATS ,
2016.
14Under review as submission to TMLR
A Experimental details
In this section, we present details to reproduce our experimental campaign. All the experiments were
conducted on Google Colab.
BSGP model We useM= 500inducing points initialized by a k-means algorithm as commonly used
in practice and we place a Normal prior pξ(Z)over the inducing locations Z. For inference, we use an
adaptive version of Stochastic Gradient Hamiltonian Monte Carlo ( sghmc) in which the hyperparameters
are automatically tuned during a burn-in phase. We set the default hyperparameter of the number of sghmc
steps toK= 10. Exclusively for regression datasets with Gaussian likelihood, we employ an Adam optimizer
with a learning rate set at 0.01 for optimizing the variance of the likelihood.
ARD kernel We use the Radial Basis Function ( rbf) kernel with Automatic Relevance Determination
(ard) placing a LogNormal prior with unit variance and means equal to 1 and 0.05 for the lengthscales and
variance, respectively.
ACD kernel We place a LogNormal prior with unit variance and mean 0.05 over the kernel variance σ2
f
while over the precision matrix Λwe explore a wide range of priors.
Table 3: Parameter settings for the UCI experiments.
parameter value
num. of inducing points 500
mini-batch size 1000
num. iterations 10500
step size 0.01
momentum 0.05
num. of burn-in steps 1500
num. of samples 50
thinning interval 180
B Simulated dataset
Inthissectionwecarryoutanexperimentusingsimulateddatasetswithknownunderlyingprecisionmatrices.
In particular we assess the ability of the bsgpmodel using a acdkernel to recover the true precision Λwhile
ﬁtting simple regression problems. Here it’s described how the simulated regression datasets are constructed
and some experiments conduced to show the behaviour of the model. We consider Ninput-label pairs
{X,y}={(xn,yn)}N
n=1withxn∈RDandyn∈Rdeﬁned as follows:
xn∼N(0,I)
Kxx:Kxx[i,j] =σ2
f/parenleftbig
(xi−xj)/latticetopΛ(xi−xj)/parenrightbig
y∼N(0,Kxx+σnI)(29)
Once that the underlying precision Λhas been constructed, specifying a value for the kernel variance σ2
fand
another one for the Gaussian noise in observations via σnis suﬃcient. The regression dataset {X,y}can be
used to train a bsgpmodel by means of the acdkernel. Through acquiring samples of the precision matrix
Λ, we aim to recover the original underlying precision used to generate the data. A visual insight into this
experiment is given in Fig. 8.
15Under review as submission to TMLR
Figure 8: Underlying sparse precision (top left) compared with mean and standard deviation of the Λsamples
obtained with diﬀerent priors. The dataset is made of N= 1000samples and the labels are obtained according to
Eq. 29 setting σ2
f= 1andσn= 0.1
.
C Additional results
Figure 9: Posterior precision matrix mean ( a) and variance ( b) with Wishart prior.
Figure 10: Posterior precision matrix mean ( a) and variance ( b) with Inverse Wishart prior.
16Under review as submission to TMLR
Figure 11: Posterior precision matrix mean ( a) and variance ( b) with Laplace prior b= 0.1.
Figure 12: The precision matrices of the wiltdataset using Laplace prior show a progressive level sparsity.
Figure 13: The full motion capture precision matrix ( a), a pooled part-wise ( b) and reference skeleton connectivity
(c).
17Under review as submission to TMLR
sample ˆREﬀective Sample Size ( ess)
0 1.01 526 .99
1 1.00 703 .17
2 1.01 664 .86
Figure 14: Traces of the mean of the predictive distribu-
tion for three test points on bostondataset with Inverse
Wishart prior ( 4chains, 200samples represented); the ta-
ble reports ˆRand Eﬀective Sample Size ( ess) statistics
for each set of 4chains.Table 4: UCI datasets used, including number of data-
points and dimensionalities.
Dataset N D
boston 506 13
breast 683 9
diabetes 783 8
concrete 1,030 8
wilt 4,839 5
kin8nm 8,192 8
powerplant 9,568 4
eeg 45,730 14
(a)
 (b)
Figure 15: Comparison of full gps () vsbsgps (#) with 200inducing points on two UCI regression data sets. The
metrics are mnllin(a)and normalized rmsein(b).
18Under review as submission to TMLR
(a)
 (b)
Figure 16: Comparison of full gps () vsbsgps (#) with 500inducing points on two UCI classiﬁcation data sets.
The metrics are mnllin(a)and Error-Rate in (b).
Figure 17: Comparison of posterior mean of the precision matrix Λon the bostondataset with Wishart prior for
fullgpvsbsgpwith 500inducing points.
Figure 18: mnllvs iterations for bsgpwith 500inducing points and for full gponconcrete dataset. The plots
show one value every 10of the 10,000iterations.
19Under review as submission to TMLR
Figure 19: Posterior samples distribution of precision matrix entries for kin8nmdataset with Horseshoe ( τ= 0.1)
prior.
Figure 20: PCA representation of vectorized posterior precision matrices. Each point in the 2D space represents a
posterior sample (precision matrix). (a)kin8nmdataset, (b)eegdataset.
20