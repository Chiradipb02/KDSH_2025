Published in Transactions on Machine Learning Research (04/2024)
Indexed Minimum Empirical Divergence-Based Algorithms
for Linear Bandits
Jie Bian jiebian@u.nus.edu
Department of Electrical and Computer Engineering
National University of Singapore
Vincent Y. F. Tan vtan@nus.edu.sg
Department of Mathematics
Department of Electrical and Computer Engineering
National University of Singapore
Reviewed on OpenReview: https: // openreview. net/ forum? id= wE9kpJSemv
Abstract
The Indexed Minimum Empirical Divergence (IMED) algorithm is a highly effective ap-
proach that offers a stronger theoretical guarantee of the asymptotic optimality compared
to the Kullback–Leibler Upper Confidence Bound (KL-UCB) algorithm for the multi-armed
bandit problem. Additionally, it has been observed to empirically outperform UCB-based
algorithms and Thompson Sampling. Despite its effectiveness, the generalization of this
algorithm to contextual bandits with linear payoffs has remained elusive. In this paper, we
present novel linear versions of the IMED algorithm, which we call the family of LinIMED
algorithms. We demonstrate that LinIMED provides a /tildewideO(d√
T)upper regret bound where
dis the dimension of the context and Tis the time horizon. Furthermore, extensive em-
pirical studies reveal that LinIMED and its variants outperform widely-used linear bandit
algorithms such as LinUCB and Linear Thompson Sampling in some regimes.
1 Introduction
The multi-armed bandit (MAB) problem (Lattimore & Szepesvári (2020)) is a classical topic in decision
theory and reinforcement learning. Among the various subfields of bandit problems, the stochastic linear
bandit is the most popular area due to its wide applicability in large-scale, real-world applications such
as personalized recommendation systems (Li et al. (2010)), online advertising, and clinical trials. In the
stochastic linear bandit model, at each time step t, the learner has to choose one arm Atfrom the time-
varying action set At. Each arm a∈Athas a corresponding context xt,a∈Rd, which is a d-dimensional
vector. By pulling the arm a∈Atat time step t, under the linear bandit setting, the learner will receive the
rewardYt,a, whose expected value satisfies E[Yt,a|xt,a] =⟨θ∗,xt,a⟩, whereθ∗∈Rdis an unknown parameter.
The goal of the learner is to maximize his cumulative reward over a time horizon T, which also means
minimizing the cumulative regret, defined as RT:=E/bracketleftig/summationtextT
t=1maxa∈At⟨θ∗,xt,a⟩−Yt,At/bracketrightig
. The learner needs
to balance the trade-off between the exploration of different arms (to learn their expected rewards) and the
exploitation of the arm with the highest expected reward based on the available data.
1.1 Motivation and Related Work
TheK-armed bandit setting is a special case of the linear bandit. There exist several good algorithms such
as UCB1 (Auer et al. (2002)), Thompson Sampling (Agrawal & Goyal (2012)), and the Indexed Minimum
Empirical Divergence (IMED) algorithm (Honda & Takemura (2015)) for this setting. There are three main
families of asymptotically optimal multi-armed bandit algorithms based on different principles (Baudry et al.
1Published in Transactions on Machine Learning Research (04/2024)
Problem independent regret bound Regret bound inde-
pendent of K?Principle that the algo-
rithm is based on
OFUL (Abbasi-Yadkori et al.
(2011))O(d√
Tlog(T)) ✓ Optimism
LinUCB (Li et al. (2010)) Hard to analyze Unknown Optimism
LinTS (Agrawal & Goyal (2013)) O(d3
2√
T)∧O(d/radicalbig
Tlog(K)) ✓ Posterior sampling
SupLinUCB (Chu et al. (2011)) O(/radicalbig
dTlog3(KT)) ✗ Optimism
LinUCB with OFUL’s confidence
boundO(d√
Tlog(T)) ✓ Optimism
Asymptotically Optimal IDS
(Kirschner et al. (2021))O(d√
Tlog(T)) ✓ Information directed
sampling
LinIMED-3 (this paper) O(d√
Tlog(T)) ✓ Min. emp. divergence
SupLinIMED (this paper) O(/radicalbig
dTlog3(KT)) ✗ Min. emp. divergence
Table 1: Comparison of algorithms for linear bandits with varying arm sets
(2023)). However, among these algorithms, only IMED lacks an extension for contextual bandits with linear
payoff. In the context of the varying arm setting of the linear bandit problem, the LinUCB algorithm in
Li et al. (2010) is frequently employed in practice. It has a theoretical guarantee on the regret in the order
ofO(d√
Tlog(T))using the confidence width as in OFUL (Abbasi-Yadkori et al. (2011)). Although the
SupLinUCB algorithm introduced by Chu et al. (2011) uses phases to decompose the reward dependence
of each time step and achieves an /tildewideO(√
dT)(the/tildewideO(·)notation omits logarithmic factors in T) regret upper
bound, itsempiricalperformancefallsshortofboththealgorithminLietal.(2010)andtheLinearThompson
Sampling algorithm (Agrawal & Goyal (2013)) as mentioned in Lattimore & Szepesvári (2020, Chapter 22).
On the other hand, the O ptimism in the F ace of Uncertainty L inear (OFUL) bandit algorithm in Abbasi-
Yadkorietal.(2011)achievesaregretupperboundof /tildewideO(d√
T)throughanimprovedanalysisoftheconfidence
bound using a martingale technique. However, it involves a bilinear optimization problem over the action
set and the confidence ellipsoid when choosing the arm at each time. This is computationally expensive,
unless the confidence ellipsoid is a convex hull of a finite set.
For randomized algorithms designed for the linear bandit problem, Agrawal & Goyal (2013) proposed the
LinTS algorithm, which is in the spirit of Thompson Sampling (Thompson (1933)) and the confidence
ellipsoid similar to that of LinUCB-like algorithms. This algorithm performs efficiently and achieves a regret
upper bound of O(d3
2√
T∧d√TlogK), whereKis the number of arms at each time step such that |At|=K
for allt. Compared to LinUCB with OFUL’s confidence width, it has an extra O(√
d∧√logK)term for
the minimax regret upper bound.
Recently, MED-like(minimumempiricaldivergence)algorithmshavecometotheforesincetheserandomized
algorithms have the property that the probability of selecting each arm is in closed form, which benefits
downstream work such as offline evaluation with the inverse propensity score. Both MED in the sub-
Gaussian environment and its deterministic version IMED have demonstrated superior performances over
Thompson Sampling (Bian & Jun (2021), Honda & Takemura (2015)). Baudry et al. (2023) also shows
MED has a close relation to Thompson Sampling. In particular, it is argued that MED and TS can be
interpreted as two variants of the same exploration strategy. Bian & Jun (2021) also shows that probability
of selecting each arm of MED in the sub-Gaussian case can be viewed as a closed-form approximation of the
same probability as in Thompson Sampling. We take inspiration from the extension of Thompson Sampling
to linear bandits and thus are motivated to extend MED-like algorithms to the linear bandit setting and
prove regret bounds that are competitive vis-à-vis the state-of-the-art bounds.
Thus, this paper aims to answer the question of whether it is possible to devise an extension of the IMED
algorithm for the linear bandit problem the varying arm set setting (for both infinite and finite arm sets)
with a regret upper bound of O(d√
TlogT)which matches LinUCB with OFUL’s confidence bound while
being as efficient as LinUCB. The proposed family of algorithms, called LinIMED as well as SupLinIMED,
can be viewed as generalizations of the IMED algorithm (Honda & Takemura (2015)) to the linear bandit
setting. We prove that LinIMED and its variants achieve a regret upper bound of /tildewideO(d√
T)and they perform
2Published in Transactions on Machine Learning Research (04/2024)
efficiently, no worse than LinUCB. SupLinIMED has a regret bound of /tildewideO(√
dT), but works only for instances
with finite arm sets. In our empirical study, we found that the different variants of LinIMED perform better
than LinUCB and LinTS for various synthetic and real-world instances under consideration.
Compared to OFUL, LinIMED works more efficiently. Compared to SupLinUCB, our LinIMED algorithm is
significantly simpler, and compared to LinUCB with OFUL’s confidence bound, our empirical performance
is better. This is because in our algorithm, the exploitation term and exploration term are decoupling and
this leads to a finer control while tuning the hyperparameters in the empirical study.
Compared to LinTS, our algorithm’s (specifically LinIMED-3) regret bound is superior, by an order of
O(√
d∧√logK). Since fixed arm setting is a special case of finite varying arm setting, our result is more
general than other fixed-arm linear bandit algorithms like Spectral Eliminator (Valko et al. (2014)) and
PEGOE (Lattimore & Szepesvári (2020, Chapter 22)). Finally, we observe that since the index used in
LinIMED has a similar form to the index used in the Information Directed Sampling (IDS) procedure
in Kirschner et al. (2021) (which is known to be asymptotically optimal but more difficult to compute),
LinIMED performs significantly better on the “End of Optimism” example in Lattimore & Szepesvari (2017).
We summarize the comparisons of LinIMED to other linear bandit algorithms in Table 1. We discussion
comparisons to other linear bandit algorithms in Sections 3.2, 3.3, and Appendix B.
2 Problem Statement
Notations: For anyddimensional vector x∈Rdand ad×dpositive definite matrix A, we use∥x∥Ato
denote the Mahalanobis norm√
x⊤Ax. We usea∧b(resp.a∨b) to represent the minimum (resp. maximum)
of two real numbers aandb.
The Stochastic Linear Bandit Model: In the stochastic linear bandit model, the learner chooses an
armAtat each round tfrom the arm set At={at,1,at,2,...}⊆R, where we assume the cardinality of each
arm setAtcan be potentially infinite such that |At|=∞for allt≥1. Each arm a∈Atat timethas a
corresponding context (arm vector) xt,a∈Rd, which is known to the learner. After choosing arm At, the
environment reveals the reward
Yt=⟨θ∗,Xt⟩+ηt
tothelearnerwhere Xt:=xt,Atisthecorrespondingcontextofthearm At,θ∗∈Rdisanunknowncoefficient
of the linear model, ηtis anR-sub-Gaussian noise conditioned on {A1,A2,...,At,Y1,Y2,...,Yt−1}such that
for anyλ∈R, almost surely,
E[exp(ληt)|A1,A2,...,At,Y1,Y2,...,Yt−1]≤exp/parenleftigλ2R2
2/parenrightig
.
Denotea∗
t:= arg maxa∈At⟨θ∗,xt,a⟩as the arm with the largest reward at time t. The goal is to minimize
the expected cumulative regret over the horizon T. The (expected) cumulative regret is defined as
RT=E/bracketleftiggT/summationdisplay
t=1⟨θ∗,xt,a∗
t⟩−⟨θ∗,Xt⟩/bracketrightigg
.
Assumption 1. For each time t, we assume that ∥Xt∥≤L, and∥θ∗∥≤Sfor some fixed L,S > 0. We
also assume that ∆t,b:= maxa∈At⟨θ∗,xt,a⟩−⟨θ∗,xt,b⟩≤1for each arm b∈Atand timet.
3 Description of LinIMED Algorithms
In the pseudocode of Algorithm 1, for each time step t, in Line 4, we use the improved confidence bound of
θ∗as in Abbasi-Yadkori et al. (2011) to calculate the confidence bound βt−1(γ). After that, for each arm
a∈At, in Lines 6 and 7, the empirical gap between the highest empirical reward and the empirical reward
of armais estimated as
ˆ∆t,a=/braceleftbigg
maxj∈At⟨ˆθt−1,xt,j⟩−⟨ˆθt−1,xt,a⟩if LinIMED-1,2
maxj∈AtUCBt(j)−UCBt(a)if LinIMED-3
3Published in Transactions on Machine Learning Research (04/2024)
Algorithm 1 LinIMED-xforx∈{1,2,3}
1:Input:LinIMED mode x, Dimension d, Regularization parameter λ, BoundSon∥θ∗∥, Sub-Gaussian
parameterR, Concentration parameter γofθ∗, BoundLon∥xt,a∥for allt≥1anda∈At, Constant
C≥1.
2:Initialize: V0=λId×d,W0=0d×1(all zeros vector with ddimensions ),ˆθ0=V−1
0W0
3:fort= 1,2,...T do
4:Receive the arm set Atand compute βt−1(γ) =/parenleftig
R/radicalig
dlog(1+(t−1)L2/λ
γ) +√
λS/parenrightig2
.
5:fora∈Atdo
6:Compute: ˆµt,a=⟨ˆθt−1,xt,a⟩andUCBt(a) =⟨ˆθt−1,xt,a⟩+/radicalbig
βt−1(γ)∥xt,a∥V−1
t−1
7:Compute: ˆ∆t,a= (maxj∈Atˆµt,j−ˆµt,a)· 1{x= 1,2}+ (maxj∈AtUCBt(j)−UCBt(a))· 1{x= 3}
8: ifa= arg maxj∈At(ˆµt,j· 1{x= 1,2}+ UCBt(j)· 1{x= 3})then
9:It,a=−log(βt−1(γ)∥xt,a∥2
V−1
t−1)· 1{x= 1} (LinIMED-1)
10: +/bracketleftig
logT∧/parenleftig
−log(βt−1(γ)∥xt,a∥2
V−1
t−1)/parenrightig/bracketrightig
· 1{x= 2} (LinIMED-2)
11: +/bracketleftig
logC
maxa∈Atˆ∆2
t,a∧/parenleftig
−log(βt−1(γ)∥xt,a∥2
V−1
t−1)/parenrightig/bracketrightig
· 1{x= 3} (LinIMED-3)
12: else
13:It,a=ˆ∆2
t,a
βt−1(γ)∥xt,a∥2
V−1
t−1−log(βt−1(γ)∥xt,a∥2
V−1
t−1)
14: end if
15: end for
16:Pull the arm At= arg mina∈AtIt,a(ties are broken arbitrarily) and receive its reward Yt.
17: Update:
18:Vt=Vt−1+XtX⊤
t,Wt=Wt−1+YtXt, and ˆθt=V−1
tWt.
19:end for
Then, in Lines 9 to 11, with the use of the confidence width of βt−1(γ), we can compute the in-
dexIt,afor the empirical best arm a= arg maxj∈Atˆµt,a(for LinIMED-1,2) or the highest UCB arm
a= arg maxj∈AtUCBj(a)(for LinIMED-3). The different versions of LinIMED encourage different amounts
of exploitation. For the other arms, in Line 13, the index is defined and computed as
It,a=ˆ∆2
t,a
βt−1(γ)∥xt,a∥2
V−1
t−1+ log1
βt−1(γ)∥xt,a∥2
V−1
t−1.
Then with all the indices of the arms calculated, in Line 16, we choose the arm Atwith the minimum index
such thatAt= arg mina∈AtIt,a(where ties are broken arbitrarily) and the agent receives its reward. Finally,
in Line 18, we use ridge regression to estimate the unknown θ∗asˆθtand update the matrix Vtand the vector
Wt. After that, the algorithm iterates to the next time step until the time horizon T. From the pseudo-code,
we observe that the only differences between the three algorithms are the way that the square gap, which
plays the role of the empirical divergence, is estimated and the index of the empirically best arm. The
latter point implies that we encourage the empirically best arm to be selected more often in LinIMED-2 and
LinIMED-3 compared to LinIMED-1; in other words, we encourage more exploitation in LinIMED-2 and
LinIMED-3. Similar to the core spirit of IMED algorithm Honda & Takemura (2015), the first term of our
indexIt,afor LinIMED-1 algorithm is ˆ∆2
t,a/(βt−1(γ)∥xt,a∥2
V−1
t−1), this is the term controls the exploitation,
while the second term −log(βt−1(γ)∥xt,a∥2
V−1
t−1)controls the exploration in our algorithm.
3.1 Description of the SupLinIMED Algorithm
Now we consider the case in which the arm set Atat each time tis finite but still time-varying. In particular,
At={at,1,at,2,...,at,K}⊆Rare sets of constant size Ksuch that|At|=K <∞. In the pseudocode of
Algorithm2, weapplytheSupLinUCBframework(Chuetal.,2011), leveragingAlgorithm3(inAppendixA)
4Published in Transactions on Machine Learning Research (04/2024)
Algorithm 2 SupLinIMED
1:Input:T∈N,S′=⌈logT⌉,Ψs
t=∅for alls∈[S′],t∈[T]
2:fort= 1,2,...,T do
3:s←1and ˆA1←[K]
4:repeat
5:Use BaseLinUCB (stated in Algorithm 3 in Appendix A) with Ψs
tto calculate the width ws
t,aand
sample mean ˆYs
t,afor alla∈ˆAs.
6: ifws
t,a≤1√
Tfor alla∈ˆAsthen
7: chooseAt= arg mina∈ˆAsIt,awhereIt,ais the same index function as in LinIMED algorithm:
8: Calculate the index
It,a=

log(2T)∧/parenleftbig
−log((ws
t,a)2)/parenrightbig
Ifa= arg maxb∈ˆAsˆYs
t,b
(ˆ∆s
t,a
ws
t,a)2−log((ws
t,a)2) otherwisewhere ˆ∆s
t,a:= max
b∈ˆAsˆYs
t,b−ˆYs
t,a.
9: Keep the same index sets at all levels: Ψs′
t+1←Ψs′
tfor alls′∈[S]. ←Case 1
10: else ifws
t,a≤2−sfor alla∈ˆAsthen
11: ˆAs+1←/braceleftig
a∈ˆAs:ˆYs
t,a+ws
t,a≥maxa′∈ˆAs(ˆYs
t,a′+ws
t,a′)−21−s/bracerightig
12:s←s+ 1 ←Case 2
13: else
14: ChooseAt∈ˆAssuch thatws
t,At>2−s
15: Updatetheindexsetsatalllevels: Ψs′
t+1←Ψs′
t∪{t}ifs=s′;Ψs′
t+1←Ψs′
tifs̸=s′←Case 3
16: end if
17: untilan actionAtis found
18:end for
as a subroutine within each phase. This ensures the independence of the choice of the arm from past
observations of rewards, thereby yielding a concentration inequality in the estimated reward (see Lemma
1 in Chu et al. (2011)) that converges to within√
dproximity of the unknown expected reward in a finite
arm setting. As a result, the regret yields an improvement of√
dignoring the logarithmic factor. At each
time steptand phases, in Line 5, we utilize the BaseLinUCB Algorithm as a subroutine to calculate the
sample mean and confidence width since we also need these terms to calculate the IMED-style indices of each
arm. In Lines 6–9 (Case 1), if the width of each arm is less than1√
T, we choose the arm with the smaller
IMED-style index. In Lines 10–12 (Case 2), the framework is the same as in SupLinUCB (Chu et al. (2011)),
if the width of each arm is smaller than 2−sbut there exist arms with widths larger than1√
T, then in Line
11 the “unpromising” arms will be eliminated until the width of each arm is smaller enough to satisfy the
condition in Line 6. Otherwise, if there exist any arms with widths that are larger than 2−s, in Lines 14–15
(Case 3), we choose one such arm and record the context and reward of this arm to the next layer Ψs
t+1.
3.2 Relation to the IMED algorithm of Honda & Takemura (2015)
The IMED algorithm is a deterministic algorithm for the K-armed bandit problem. At each time step t, it
chooses the arm awith the minimum index, i.e.,
a= arg min
i∈[K]/braceleftbig
Ti(t)Dinf(ˆFi(t),ˆµ∗(t)) + logTi(t)/bracerightbig
, (1)
whereTi(t) =/summationtextt−1
s=11{At=a}is the total arm pulls of the arm iuntil timetandDinf(ˆFi(t),ˆµ∗(t))is
some divergence measure between the empirical distribution of the sample mean for arm iand the arm
with the highest sample mean. More precisely, Dinf(F,µ) := infG∈G:E(G)≤µD(F∥G)andGis the family of
distributions supported on (−∞,1]. As shown in Honda & Takemura (2015), its asymptotic bound is even
better than KL-UCB (Garivier & Cappé (2011)) algorithm and can be extended to semi-bounded support
models such asG. Also, this algorithm empirically outperforms the Thompson Sampling algorithm as shown
5Published in Transactions on Machine Learning Research (04/2024)
in Honda & Takemura (2015). However, an extension of IMED algorithm with minimax regret bound of
/tildewideO(d√
T)has not been derived. In our design of LinIMED algorithm, we replace the optimized KL-divergence
measure in IMED in Eqn. (1) with the squared gap between the sample mean of the arm iand the arm
with the maximum sample mean. This choice simplifies our analysis and does not adversely affect the regret
bound. On the other hand, we view the term 1/Ti(t)as the variance of the sample mean of arm iat timet;
then in this spirit, we use βt−1(γ)∥xt,a∥2
V−1
t−1as the variance of the sample mean (which is ⟨ˆθt−1,xt,a⟩) of arm
aat timet. We choose ˆ∆2
t,a/(βt−1(γ)∥xt,a∥2
V−1
t−1)instead of the KL-divergence approximation for the index
sinceintheclassicallinearbanditsetting, thenoiseissub-GaussiananditisknownthattheKL-divergenceof
two Gaussian random variables with the same variance ( KL(N(µ1,σ2),N(µ2,σ2)) =(µ1−µ2)2
2σ2) has a closed
form expression similar to ˆ∆2
t,a/(βt−1(γ)∥xt,a∥2
V−1
t−1)ignoring the constant1
2.
3.3 Relation to Information Directed Sampling (IDS) for Linear Bandits
Information Directed Sampling (IDS), introduced by Russo & Van Roy (2014), serves as a good principle
for regret minimization in linear bandits to achieve the asymptotic optimality. The intuition behind IDS is
to balance between the information gain on the best arm and the expected reward at each time step. This
goal is realized by optimizing the distribution of selecting each arm π∈D(A)(whereAis the fixed finite
arm set) with the minimum information ratio such that:
πIDS
t:= arg min
π∈D(A)ˆ∆2
t(π)
gt(π),
where ˆ∆tis the empirical gap and gtis the so-called information gain (defined later). Kirschner & Krause
(2018), Kirschner et al. (2020) and Kirschner et al. (2021) apply the IDS principle to the linear bandit
setting, The first two works propose both randomized and deterministic versions of IDS for linear bandit.
They showed a near-optimal minimax regret bound of the order of /tildewideO(d√
T). Kirschner et al. (2021) designed
an asymptotically optimal linear bandit algorithm retaining its near-optimal minimax regret properties.
Comparing these algorithms with our LinIMED algorithms, we observe that the first term of the index of
non-greedy actions in our algorithms is ˆ∆2
t,a/(βt−1(γ)∥xt,a∥2
V−1
t−1), which is similar to the choice of informa-
tion ratio in IDS with the estimated gap ∆t(a) := ˆ∆t,aas defined in Algorithm 1 and the information ratio
gt(a) :=βt−1(γ)∥xt,a∥2
V−1
t−1. As mentioned in Kirschner & Krause (2018), when the noise is 1-subgaussian
and∥xt,a∥2
V−1
t−1≪1, the information gain in deterministic IDS algorithm is approximately ∥xt,a∥2
V−1
t−1, which
is similar to our choice βt−1(γ)∥xt,a∥2
V−1
t−1. However, our LinIMED algorithms are different from the deter-
ministic IDS algorithm in Kirschner & Krause (2018) since the estimated gap defined in our algorithm ˆ∆t,ais
different from that in deterministic IDS. Furthermore, as discussed in Kirschner et al. (2020), when the noise
is 1-subgaussian and ∥xt,a∥2
V−1
t−1≪1, the action chosen by UCB minimizes the deterministic information
ratio. However, this is not the case for our algorithm since we have the second term −log(βt−1(γ)∥xt,a∥2
V−1
t−1)
in LinIMED-1 which balances information and optimism. Compared to IDS in Kirschner et al. (2021), their
algorithm is a randomized version of the deterministic IDS algorithm, which is more computationally expen-
sive than our algorithm since our LinIMED algorithms are fully deterministic (the support of the allocation
in Kirschner et al. (2021) is two). IDS defines a more complicated version of information gain to achieve
asymptotically optimality. Finally, to the best of our knowledge, all these IDS algorithms are designed for
linear bandits under the setting that the arm set is fixed and finite, while in our setting we assume the arm
set is finite and can change over time. We discuss comparisons to other related work in Appendix B.
4 Theorem Statements
Theorem 1. Under Assumption 1, the assumption that ⟨θ∗,xt,a⟩≥0for allt≥1anda∈At, and the
assumption that√
λS≥1, the regret of the LinIMED-1 algorithm is upper bounded as follows:
RT≤O/parenleftbig
d√
Tlog3
2(T)/parenrightbig
.
6Published in Transactions on Machine Learning Research (04/2024)
A proof sketch of Theorem 1 is provided in Section 5.
Theorem 2. Under Assumption 1, and the assumption that√
λS≥1, the regret of the LinIMED-2 algorithm
is upper bounded as follows:
RT≤O/parenleftbig
d√
Tlog3
2(T)/parenrightbig
.
Theorem 3. Under Assumption 1, the assumption that√
λS≥1, and thatCin Line 11 is a constant, the
regret of the LinIMED-3 algorithm is upper bounded as follows:
RT≤O/parenleftbig
d√
Tlog(T)/parenrightbig
.
Theorem 4. Under Assumption 1, the assumption that L=S= 1, the regret of the SupLinIMED algorithm
(which is applicable to linear bandit problems with K <∞arms) is upper bounded as follows:
RT≤O/parenleftbigg/radicalig
dTlog3(KT)/parenrightbigg
.
The upper bounds on the regret of LinIMED and its variants are all of the form /tildewideO(d√
T), which, ignoring the
logarithmic term, is the same as OFUL algorithm (Abbasi-Yadkori et al. (2011)). Compared to LinTS, it has
an advantage of O(√
d∧√logK). Also, these upper bounds do not depend on the number of arms K, which
means it can be applied to linear bandit problems with a large arm set (including infinite arm sets). One
observes that LinIMED-2 and LinIMED-3 do not require the additional assumption that ⟨θ∗,xt,a⟩≥0for all
t≥1anda∈Atto achieve the /tildewideO(d√
T)upper regret bound. It is difficult to prove the regret bound for the
LinIMED-1 algorithm without this assumption since in our proof we need to use that ⟨θ∗,Xt⟩≥0for any
timetto bound the F1term. On the other hand, LinIMED-2 and LinIMED-3 encourage more exploitations
in terms of the index of the empirically best arm at each time without adversely influencing the regret
bound; this will accelerate the learning with well-preprocessed datasets. The regret bound of LinIMED-3,
in fact, matches that of LinUCB with OFUL’s confidence bound. In the proof, we will extensively use
a technique known as the “peeling device” (Lattimore & Szepesvári, 2020, Chapter 9). This analytical
technique, commonly used in the theory of bandit algorithms, involves the partitioning of the range of some
random variable into several pieces, then using the basic fact that P(A∩(∪∞
i=1Bi))≤/summationtext∞
i=1P(A∩Bi), we
can utilize the more refined range of the random variable to derive desired bounds.
Finally, Theorem 4 says that when the arm set is finite, we can use the framework of SupLinUCB (Chu et al.,
2011) with our LinIMED index It,ato achieve a regret bound of the order of /tildewideO(√
dT), which is√
dbetter
than the regret bounds yielded by the family of LinIMED algorithms (ignoring the logarithmic terms). The
proof is provided in Appendix F.
5 Proof Sketch of Theorem 1
We choose to present the proof sketch of Theorem 1 since it contains the main ingredients for all the theorems
in the preceding section. Before presenting the proof, we introduce the following lemma and corollary.
Lemma 1. (Abbasi-Yadkori et al. (2011, Theorem 2)) Under Assumption 1, for any time step t≥1and
anyγ >0, we have
P/parenleftbig
∥ˆθt−1−θ∗∥Vt−1≤/radicalbig
βt−1(γ)/parenrightbig
≥1−γ.
This lemma illustrates that the true parameter θ∗lies in the ellipsoid centered at ˆθt−1with high probability,
which also states the width of the confidence bound.
The second is a corollary of the elliptical potential count lemma in Abbasi-Yadkori et al. (2011):
Corollary 1. (Corollary of Lattimore & Szepesvári (2020, Exercise 19.3)) Assume that V0=λIand∥Xt∥≤
Lfort∈[T], for any constant 0<m≤2, the following holds:
T/summationdisplay
t=11/braceleftig
∥Xt∥2
V−1
t−1≥m/bracerightig
≤6d
mlog/parenleftig
1 +2L2
λm/parenrightig
.
7Published in Transactions on Machine Learning Research (04/2024)
We remark that this lemma is slightly stronger than the classical elliptical potential lemma since it reveals
information about the upper bound of frequency that ∥Xt∥2
V−1
t−1exceeds some value m. Equipped with this
lemma, we can perform the peeling device on ∥Xt∥2
V−1
t−1in our proof of the regret bound, which is a novel
technique to the best of our knowledge.
Proof.First we define a∗
tas the best arm in time step tsuch thata∗
t= arg maxa∈At⟨θ∗,xt,a⟩, and use
x∗
t:=xt,a∗
tdenote its corresponding context. Let ∆t:=⟨θ∗,x∗
t⟩−⟨θ∗,Xt⟩denote the regret in time t. Define
the following events:
Bt:=/braceleftbig
∥ˆθt−1−θ∗∥Vt−1≤/radicalbig
βt−1(γ)/bracerightbig
, Ct:=/braceleftbig
max
b∈At⟨ˆθt−1,xt,b⟩>⟨θ∗,x∗
t⟩−δ/bracerightbig
, Dt:=/braceleftbigˆ∆t,At≥ε/bracerightbig
.
whereδandεare free parameters set to be δ=∆t√
logTandε= (1−2√
logT)∆tin this proof sketch.
Then the expected regret RT=E/summationtextT
t=1∆tcan be partitioned by events Bt,Ct,Dtsuch that:
RT=ET/summationdisplay
t=1∆t· 1{Bt,Ct,Dt}
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:F1+ET/summationdisplay
t=1∆t· 1/braceleftbig
Bt,Ct,Dt/bracerightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:F2+ET/summationdisplay
t=1∆t· 1/braceleftbig
Bt,Ct/bracerightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:F3+ET/summationdisplay
t=1∆t· 1/braceleftbig
Bt/bracerightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:F4.
ForF1, from the event Ctand the fact that ⟨θ∗,x∗
t⟩= ∆t+⟨θ∗,Xt⟩≥ ∆t(here is where we use that
⟨θ∗,xt,a⟩≥ 0for alltanda), we obtain maxb∈At⟨ˆθt−1,xt,b⟩>(1−1√
logT)∆t. For convenience, define
ˆAt:= arg maxb∈At⟨ˆθt−1,xt,b⟩as the empirically best arm at time step t, where ties are broken arbitrarily,
then use ˆXtto denote the corresponding context of the arm ˆAt. Therefore from the Cauchy–Schwarz
inequality, we have ∥ˆθt−1∥Vt−1∥ˆXt∥V−1
t−1≥⟨ˆθt−1,ˆXt⟩>(1−1√
logT)∆t. This implies that
∥ˆXt∥V−1
t−1≥(1−1√
logT)∆t
∥ˆθt−1∥Vt−1. (2)
On the other hand, we claim that ∥ˆθt−1∥Vt−1can be upper bounded as O(√
T). This can be seen from the
fact that∥ˆθt−1∥Vt−1=∥ˆθt−1−θ∗+θ∗∥Vt−1≤∥ˆθt−1−θ∗∥Vt−1+∥θ∗∥Vt−1. Since the event Btholds, we know
the first term is upper bounded by/radicalbig
βt−1(γ), and since the largest eigenvalue of the matrix Vt−1is upper
bounded by λ+TL2and∥θ∗∥≤S, the second term is upper bounded by S√
λ+TL2. Hence,∥ˆθt−1∥Vt−1
is upper bounded by O(√
T). Then one can substitute this bound back into Eqn. (2), and this yields
∥ˆXt∥V−1
t−1≥Ω/parenleftig1√
T/parenleftig
1−1√logT/parenrightig
∆t/parenrightig
. (3)
Furthermore, by our design of the algorithm, the index of Atis not larger than the index of the arm with
the largest empirical reward at time t. Hence,
It,At=ˆ∆2
t,At
βt−1(γ)∥Xt∥2
V−1
t−1+ log1
βt−1(γ)∥Xt∥2
V−1
t−1≤log1
βt−1(γ)∥ˆXt∥2
V−1
t−1. (4)
In the following, we set γas well as another free parameter Γas follows:
Γ =dlog3
2T√
Tandγ=1
t2,. (5)
If∥Xt∥2
V−1
t−1≥∆2
t
βt−1(γ), by using Corollary 1 with the choice in Eqn. (5), the upper bound of F1in this case
isO/parenleftbig
d√TlogT/parenrightbig
. Otherwise, using the event Dtand the bound in (3), we deduce that for all Tsufficiently
8Published in Transactions on Machine Learning Research (04/2024)
large, we have∥Xt∥2
V−1
t−1≥Ω/parenleftbig∆2
t
βt−1(γ) log(T/∆2
t)/parenrightbig
. Therefore by using Corollary 1 and the “peeling device”
(Lattimore & Szepesvári, 2020, Chapter 9) on ∆tsuch that 2−l<∆t≤2−l+1forl= 1,2,...,⌈Q⌉where
Q=−log2ΓandΓis chosen as in Eqn. (5). Now consider,
F1≤O(1) +ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥Ω/parenleftig∆2
t
βt−1(γ) log(T/∆2
t)/parenrightig/bracerightbigg
≤O(1)+TΓ+ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥Ω/parenleftig∆2
t
βt−1(γ) log(T/∆2
t)/parenrightig/bracerightbigg
1/braceleftbig
2−l<∆t≤2−l+1/bracerightbig
≤O(1) +TΓ +ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=12−l+1· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥Ω/parenleftig2−2l
βt−1(γ) log(T·22l)/parenrightig/bracerightbigg
≤O(1)+TΓ+E⌈Q⌉/summationdisplay
l=12−l+1O/parenleftbigg
22ldβT(γ) log(22lT) log/parenleftig
1+2L2·22lβT(γ) log(T·22l)
λ/parenrightig/parenrightbigg
(6)
≤O(1) +TΓ +⌈Q⌉/summationdisplay
l=12l+1·O/parenleftbigg
dβT(γ) log(T
Γ2) log/parenleftig
1 +L2βT(γ) log(T
Γ2)
λΓ2/parenrightig/parenrightbigg
≤O(1) +TΓ +O/parenleftbiggdβT(γ) log(T
Γ2)
Γlog/parenleftig
1 +L2βT(γ) log(T
Γ2)
λΓ2/parenrightig/parenrightbigg
, (7)
where in Inequality (6) we used Corollary 1. Substituting the choices of Γandγin (5) into (7) yields the
upper bound on E/summationtextT
t=1∆t· 1{Bt,Ct,Dt}· 1/braceleftbig
∥Xt∥2
V−1
t−1<∆2
t
βt−1(γ)/bracerightbig
of the order O(d√
Tlog3
2T). Hence
F1≤O(d√
Tlog3
2T). Other details are fleshed out in Appendix C.2.
ForF2, sinceCtandDttogether imply that ⟨θ∗,x∗
t⟩−δ < ε +⟨ˆθt−1,Xt⟩, then using the choices of δand
ε, we have⟨ˆθt−1−θ∗,Xt⟩>∆t√
logT. Substituting this into the event Btand using the Cauchy–Schwarz
inequality, we have
∥Xt∥2
V−1
t−1≥∆2
t
βt−1(γ) logT.
Again applying the “peeling device” on ∆tand Corollary 1, we can upper bound F2as follows:
F2≤TΓ +O/parenleftbiggdβT(γ) logT
Γ/parenrightbigg
log/parenleftig
1 +L2βT(γ) logT
λΓ2/parenrightig
. (8)
Then with the choice of Γandγas stated in (5), the upper bound of the F2is also of order O(d√
Tlog3
2T).
More details of the calculation leading to Eqn. (8) are in Appendix C.3.
ForF3, this is the case when the best arm at time tdoes not perform sufficiently well so that the empirically
largest reward at time tis far from the highest expected reward. One observes that minimizing F3results in a
tradeoff with respect to F1. On the event Ct, we can again apply the “peeling device” on ⟨θ∗,x∗
t⟩−⟨ˆθt−1,x∗
t⟩
such thatq+1
2δ≤⟨θ∗,x∗
t⟩−⟨ˆθt−1,x∗
t⟩<q+2
2δwhereq∈N. Then using the fact that It,At≤It,a∗
t, we have
log1
βt−1(γ)∥Xt∥2
V−1
t−1<q2δ2
4βt−1(γ)∥x∗
t∥2
V−1
t−1+ log1
βt−1(γ)∥x∗
t∥2
V−1
t−1. (9)
On the other hand, using the event Btand the Cauchy–Schwarz inequality, it holds that
∥x∗
t∥V−1
t−1≥(q+ 1)δ
2/radicalbig
βt−1(γ). (10)
9Published in Transactions on Machine Learning Research (04/2024)
If∥Xt∥2
V−1
t−1≥∆2
t
βt−1(γ), the regret in this case is bounded by O(d√TlogT). Otherwise, combining Eqn. (9)
and Eqn. (10) implies that
∥Xt∥2
V−1
t−1≥(q+ 1)2δ2
4βt−1(γ)exp/parenleftbigg
−q2
(q+ 1)2/parenrightbigg
.
Using Corollary 1, we can now conclude that F3is upper bounded as
F3≤TΓ +O/parenleftbiggdβT(γ) logT
Γ/parenrightbigg
log/parenleftig
1 +L2βT(γ) logT
λΓ2/parenrightig
. (11)
Substituting Γandγin Eqn. (5) into Eqn. (11), we can upper bound F3byO(d√
Tlog3
2T). Complete
details are provided in Appendix C.4.
ForF4, using Lemma 1 with the choice of γ= 1/t2andQ=−log Γ, we have
F4=ET/summationdisplay
t=1∆t· 1/braceleftbig
Bt/bracerightbig
≤TΓ +ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=1∆t· 1/braceleftbig
2−l<∆t≤2−l+1/bracerightbig
1/braceleftbig
Bt/bracerightbig
≤TΓ +T/summationdisplay
t=1⌈Q⌉/summationdisplay
l=12−l+1P/parenleftbig
Bt/parenrightbig
≤TΓ +T/summationdisplay
t=1⌈Q⌉/summationdisplay
l=12−l+1γ <T Γ +π2
3.
Thus,F4≤O(d√
Tlog3
2T). In conclusion, with the choice of Γandγin Eqn. (5), we have shown that the
expected regret of LinIMED-1 RT=/summationtext4
i=1Fiis upper bounded by O(d√
Tlog3
2T).
For LinIMED-2, the proof is similar but the assumption that ⟨θ∗,xt,a⟩≥0is not required. For LinIMED-3,
bydirectlyusingtheUCBinLine6ofAlgorithm1, weimprovetheregretboundtomatchthestate-of-the-art
O(d√
TlogT), which matches that of LinUCB with OFUL’s confidence bound.
6 Empirical Studies
This section aims to justify the utility of the family of LinIMED algorithms we developed and to demonstrate
their effectiveness through quantitative evaluations in simulated environments and real-world datasets such
as the MovieLens dataset. We compare our LinIMED algorithms with LinTS and LinUCB with the choice
λ=L2. We setβt(γ) = (R/radicalbig
3dlog(1 +t) +√
2)2(hereγ=1
(1+t)2andL=√
2) for the synthetic dataset
with varying and finite arm set and βt(γ) = (R/radicalbig
dlog((1 +t)t2) +√
20)2(hereγ=1
t2andL=√
20)
for the MovieLens dataset respectively. The confidence widths/radicalbig
βt(γ)for each algorithm are multiplied
by a factor αand we tune αby searching over the grid {0.05,0.1,0.15,0.2,..., 0.95,1.0}and report the
best performance for each algorithm; see Appendix G. Both γ’s are of order O(1
t2)as suggested by our
proof sketch in Eqn. (5). We set C= 30in LinIMED-3 throughout. The sub-Gaussian noise level is
R= 0.1. We choose LinUCB and LinTS as competing algorithms since they are paradigmatic examples of
deterministic and randomized contextual linear bandit algorithms respectively. We also included IDS in our
comparisons for the fixed and finite arm set settings. Finally, we only show the performances of SupLinUCB
and SupLinIMED algorithms but only in Figs. 1 and 2 since it is well known that there is a substantial
performance degradation compared to established methodologies like LinUCB or LinTS (as mentioned in
Lattimore & Szepesvári (2020, Chapter 22) and also seen in Figs. 1 and 2.
6.1 Experiments on a Synthetic Dataset in the Varying Arm Set Setting
We perform an empirical study on a varying arm setting. We evaluate the performance with different
dimensions dand different number of arms K. We set the unknown parameter vector and the best context
vector asθ∗=x∗
t= [1√d−1,...,1√d−1,0]⊤∈Rd. There are K−2suboptimal arms vectors, which are all
the same (i.e., repeated) and share the context [(1−1
7+zt,i)1√d−1,..., (1−1
7+zt,i)1√d−1,(1−1
7+zt,i)]⊤∈Rd
10Published in Transactions on Machine Learning Research (04/2024)
(a)K= 10
 (b)K= 100
 (c)K= 500
Figure 1: Simulation results (expected regrets) on the synthetic dataset with different K’s
(a)d= 2
 (b)d= 20
 (c)d= 50
Figure 2: Simulation results (expected regrets) on the synthetic dataset with different d’s
wherezt,i∼Uniform [0,0.1]is iid noise for each t∈[T]andi∈[K−2]. Finally, there is also one “worst”
arm vector with context [0,0,..., 0,1]⊤. First we fix d= 2. The results for different numbers of arms such
asK= 10,100,500are shown in Fig. 1. Note that each plot is repeated 50times to obtain the mean and
standard deviation of the regret. From Fig. 1, we observe that LinIMED-1 and LinIMED-2 are comparable
to LinUCB and LinTS, while LinIMED-3 outperforms LinTS and LinUCB regardless of the number of the
armsK. Second, we set K= 10with the dimensions d= 2,20,50. Each trial is again repeated 50times and
the regret over time is shown in Fig. 2. Again, we see that LinIMED-1 and LinIMED-2 are comparable to
LinUCB and LinTS, LinIMED-3 clearly perform better than LinUCB and LinTS.
The experimental results on synthetic data demonstrate that the performances of LinIMED-1 and LinIMED-
2 are largely similar but LinIMED-3 is slightly superior (corroborating our theoretical findings). More
importantly, LinIMED-3 outperforms both the LinTS and LinUCB algorithms in a statistically significant
manner, regardless of the number of arms Kor the dimension dof the data.
6.2 Experiments on the “End of Optimism” instance
Algorithms based on the optimism principle such as LinUCB and LinTS have been shown to be not asymp-
totically optimal. A paradigmatic example is known as the “End of Optimism” (Lattimore & Szepesvari,
2017; Kirschner et al., 2021)). In this two-dimensional case in which the true parameter vector θ∗= [1; 0],
there are three arms represented by the arm vectors: [1; 0],[0; 1]and[1−ε; 2ε], whereε >0is small. In
this example, it is observed that even pulling a highly suboptimal arm (the second one) provides a lot of
information about the best arm (the first one). We perform experiments with the same confidence parameter
βt(γ) = (R/radicalbig
3dlog(1 +t)+√
2)2as in Section 6.1 (where the noise level R= 0.1, dimension d= 2). We also
include the asymptotically optimal IDS algorithm (Kirschner et al. (2021) with the choice of ηs=βs(δ)−1;
11Published in Transactions on Machine Learning Research (04/2024)
(a)ε= 0.005
 (b)ε= 0.01
 (c)ε= 0.02
Figure 3: Simulation results (expected regrets) on the “End of Optimism” instance with different ε’s
(a)K= 20
 (b)K= 50
 (c)K= 100
Figure 4: Simulation results (CTRs) of the MovieLens dataset with different K’s
this is suggested in Kirschner et al. (2021). Each algorithm is run over 10independent trials. The regrets of
all competing algorithms are shown in Fig. 3 with ε= 0.05,0.01,0.02and for a fixed horizon T= 106.
From Fig. 3 we observe that the LinIMED algorithms perform much better than LinUCB and LinTS and
LinIMED-3 is comparable to IDS in this “End of Optimism” instance. In particular, LinIMED-3 performs
significantly better than LinUCB and LinTS even when εis of a moderate value such as ε= 0.02. We surmise
that the reason behind the superior performance of our LinIMED algorithms on the "End of Optimism"
instance is that the first term of our LinIMED index is ˆ∆2
t,a/(βt−1(γ)∥xt,a∥2
V−1
t−1), which can be viewed as an
approximate and simpler version of the information ratio that movtivates the design the IDS) algorithm.
6.3 Experiments on the MovieLens Dataset
The MovieLens dataset (Cantador et al. (2011)) is a widely-used benchmark dataset for research in recom-
mendation systems. We specifically choose to use the MovieLens 10M dataset, which contains 10 million
ratings (from 0 to 5) and 100,000 tag applications applied to 10,000 movies by 72,000 users. To preprocess
the dataset, we choose the best K∈{20,50,100}movies for consideration. At each time t, one random user
visits the website and is recommended one of the best Kmovies. We assume that the user will click on the
recommendedmovieifandonlyiftheuser’sratingofthismovieisatleast 3. Weimplementthethreeversions
of LinIMED, LinUCB, LinTS and IDS on this dataset. Each trial is repeated over 100runs and the averages
and standard deviations of the click-through rates (CTRs) as functions of time are reported in Fig. 4. One
observes that LinIMED variants significantly outperform LinUCB and LinTS for all K∈{20,50,100}when
12Published in Transactions on Machine Learning Research (04/2024)
time horizon Tis sufficiently large. LinIMED-1 and LinIMED-2 perform significantly better than IDS when
k= 20,50, LinIMED-3 perform significantly better than IDS when k= 50,100. Furthermore, by virtue of
the fact that IDS is randomized, the variance of IDS is higher than that of LinIMED.
7 Future Work
In the future, a fruitful direction of research is to further modify the LinIMED algorithm to make it also
asymptotically optimal; we believe that in this case, the analysis would be more challenging, but the theo-
retical and empirical performances might be superior to our three LinIMED algorithms. In addition, one can
generalize the family of IMED-style algorithms to generalized linear bandits or neural contextual bandits.
Acknowledgements
This work is supported by funding from a Ministry of Education Academic Research Fund (AcRF) Tier 2
grant under grant number A-8000423-00-00 and AcRF Tier 1 grants under grant numbers A-8000189-01-00
and A-8000980-00-00. This research is supported by the National Research Foundation, Singapore under its
AI Singapore Programme (AISG Award No: AISG2-PhD-2023-08-044T-J), and is part of the programme
DesCartes which is supported by the National Research Foundation, Prime Minister’s Office, Singapore
under its Campus for Research Excellence and Technological Enterprise (CREATE) programme.
References
Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits.
Advances in Neural Information Processing Systems , 24, 2011.
Shipra Agrawal and Navin Goyal. Analysis of Thompson sampling for the multi-armed bandit problem. In
Conference on Learning Theory , pp. 39.1–39.26. JMLR Workshop and Conference Proceedings, 2012.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In Inter-
national Conference on Machine Learning , pp. 127–135. PMLR, 2013.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine Learning , 47:235–256, 2002.
Dorian Baudry, Kazuya Suzuki, and Junya Honda. A general recipe for the analysis of randomized multi-
armed bandit algorithms. arXiv preprint arXiv:2303.06058 , 2023.
Jie Bian and Kwang-Sung Jun. Maillard sampling: Boltzmann exploration done optimally. arXiv preprint
arXiv:2111.03290 , 2021.
IvánCantador, PeterBrusilovsky, andTsviKuflik. Secondworkshoponinformationheterogeneityandfusion
in recommender systems (HetRec2011). In Proceedings of the Fifth ACM Conference on Recommender
Systems, pp. 387–388, 2011.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions.
InProceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics , pp.
208–214. JMLR Workshop and Conference Proceedings, 2011.
Aurélien Garivier and Olivier Cappé. The KL-UCB algorithm for bounded stochastic bandits and beyond.
InProceedings of the 24th Annual Conference on Learning Theory , pp. 359–376. JMLR Workshop and
Conference Proceedings, 2011.
Junya Honda and Akimichi Takemura. Non-asymptotic analysis of a new bandit algorithm for semi-bounded
rewards. J. Mach. Learn. Res. , 16:3721–3756, 2015.
Johannes Kirschner and Andreas Krause. Information directed sampling and bandits with heteroscedastic
noise. In Conference On Learning Theory , pp. 358–384. PMLR, 2018.
13Published in Transactions on Machine Learning Research (04/2024)
Johannes Kirschner, Tor Lattimore, and Andreas Krause. Information directed sampling for linear partial
monitoring. In Conference on Learning Theory , pp. 2328–2369. PMLR, 2020.
Johannes Kirschner, Tor Lattimore, Claire Vernade, and Csaba Szepesvári. Asymptotically optimal
information-directed sampling. In Conference on Learning Theory , pp. 2777–2821. PMLR, 2021.
Tor Lattimore and Csaba Szepesvari. The end of optimism? an asymptotic analysis of finite-armed linear
bandits. In Artificial Intelligence and Statistics , pp. 728–737. PMLR, 2017.
Tor Lattimore and Csaba Szepesvári. Bandit Algorithms . Cambridge University Press, 2020.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized
news article recommendation. In Proceedings of the 19th International Conference on World Wide Web ,
pp. 661–670, 2010.
Haolin Liu, Chen-Yu Wei, and Julian Zimmert. Bypassing the simulator: Near-optimal adversarial linear
contextual bandits. Advances in Neural Information Processing Systems , 36, 2024.
Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. Advances in
Neural Information Processing Systems , 27, 2014.
Hassan Saber, Pierre Ménard, and Odalric-Ambrym Maillard. Indexed minimum empirical divergence for
unimodal bandits. Advances in Neural Information Processing Systems , 34:7346–7356, 2021.
William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika , 25(3/4):285–294, 1933. ISSN 00063444. URL http://www.jstor.
org/stable/2332286 .
Michal Valko, Rémi Munos, Branislav Kveton, and Tomáš Kocák. Spectral bandits for smooth graph func-
tions. In International Conference on Machine Learning , pp. 46–54. PMLR, 2014.
14Published in Transactions on Machine Learning Research (04/2024)
Supplementary Materials for the TMLR submission
“Linear Indexed Minimum Empirical Divergence Algorithms”
A BaseLinUCB Algorithm
Here, we present the BaseLinUCB algorithm used as a subroutine in SubLinIMED (Algorithm 2).
Algorithm 3 BaseLinUCB
1:Input:γ=1
2t2,α=/radicalig
1
2ln2TK
γ,Ψt⊆{1,2,...,t−1}
2:Vt=Id+/summationtext
τ∈ΨtxT
τ,Aτxτ,Aτ
3:bt=/summationtext
τ∈ΨtYτ,Aτxτ,Aτ
4:ˆθt=V−1
tbt
5:ObserveKarm features xt,1,xt,2,...,xt,K∈Rd
6:fora∈[K]do
7:wt,a=α/radicalig
xT
t,aV−1
txt,a
8: ˆYt,a=⟨ˆθt,xt,a⟩
9:end for
B Comparison to other related work
Saberetal.(2021)adoptstheIMEDalgorithmtounimodalbanditswhichachievesasymptoticallyoptimality
for one-dimensional exponential family distributions. In their algorithm IMED-UB, they narrow down the
search region to the neighboring regions of the empirically best arm and then implement the IMED algorithm
forK-armed bandit as in Honda & Takemura (2015). This design is inspired by the lower bound and only
involves the neighboring arms of the best arm. The settings in which the algorithm in Saber et al. (2021) is
applied to is different from our proposed LinIMED algorithms as we focus on linear bandits, not unimodal
bandits.
Liu et al. (2024) proposes an algorithm that achieves /tildewideO(√
T)regret for adversarial linear bandits with
stochastic action sets in the absence of a simulator or prior knowledge on the distribution. Although their
setting is different from ours, they also use a bonus term −αtˆΣ−1
tin the lifted covariance matrix to encourage
exploration. This is similar to our choice of the second term log(1/βt−1(γ)∥xt,a∥2
V−1
t−1)in LinIMED-1.
C Proof of the regret bound for LinIMED-1 (Complete proof of Theorem 1)
Here and in the following, we abbreviate βt(γ)asβt, i.e., we drop the dependence of βtonγ, which is taken
to be1
t2per Eqn. (5).
C.1 Statement of Lemmas for LinIMED-1
We first state the following lemmas which respectively show the upper bound of F1toF4:
Lemma 2. Under Assumption 1, the assumption that ⟨θ∗,xt,a⟩≥0for allt≥1anda∈At, and the
assumption that√
λS≥1, then for the free parameter 0<Γ<1, the termF1for LinIMED-1 satisfies:
F1≤O(1) +TΓ +O/parenleftbiggdβTlog(T
Γ2)
Γlog/parenleftig
1 +L2βTlog(T
Γ2)
λΓ2/parenrightig/parenrightbigg
. (12)
With the choice of Γas in Eqn. (5),
F1≤O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
.
15Published in Transactions on Machine Learning Research (04/2024)
Lemma 3. Under Assumption 1, and the assumption that√
λS≥1, for the free parameter 0<Γ<1, the
termF2for LinIMED-1 satisfies:
F2≤2TΓ +O/parenleftbiggdβTlogT
Γ/parenrightbigg
log/parenleftbigg
1 +L2βTlogT
λΓ2/parenrightbigg
. (13)
With the choice of Γas in Eqn. (5),
F2≤O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
.
Lemma 4. Under Assumption 1, and the assumption that√
λS≥1, for the free parameter 0<Γ<1, the
termF3for LinIMED-1 satisfies:
F3≤2TΓ +O/parenleftbiggdβTlog(T)
Γlog/parenleftig
1 +L2βTlog(T)
λΓ2/parenrightig/parenrightbigg
. (14)
With the choice of Γas in Eqn. (5),
F3≤O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
.
Lemma 5. Under Assumption 1, for the free parameter 0<Γ<1, the termF4for LinIMED-1 satisfies:
F4≤TΓ +O(1).
With the choice of Γas in Eqn. (5),
F4≤O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
.
C.2 Proof of Lemma 2
Proof.From the event Ctand the fact that ⟨θ∗,x∗
t⟩= ∆t+⟨θ∗,Xt⟩ ≥ ∆t(here is where we use that
⟨θ∗,xt,a⟩≥ 0for alltanda), we obtain maxb∈At⟨ˆθt−1,xt,b⟩>(1−1√
logT)∆t. For convenience, define
ˆAt:= arg maxb∈At⟨ˆθt−1,xt,b⟩as the empirically best arm at time step t, where ties are broken arbitrarily,
then use ˆXtto denote the corresponding context of the arm ˆAt. Therefore from the Cauchy–Schwarz
inequality, we have ∥ˆθt−1∥Vt−1∥ˆXt∥V−1
t−1≥⟨ˆθt−1,ˆXt⟩>(1−1√
logT)∆t. This implies that
∥ˆXt∥V−1
t−1≥(1−1√
logT)∆t
∥ˆθt−1∥Vt−1.
On the other hand, we claim that ∥ˆθt−1∥Vt−1can be upper bounded as O(√
T). This can be seen from the
fact that∥ˆθt−1∥Vt−1=∥ˆθt−1−θ∗+θ∗∥Vt−1≤∥ˆθt−1−θ∗∥Vt−1+∥θ∗∥Vt−1. Since the event Btholds, we know
the first term is upper bounded by/radicalbig
βt−1(γ), and since the maximum eigenvalue of the matrix Vt−1is upper
bounded by λ+TL2and∥θ∗∥≤S, the second term is upper bounded by S√
λ+TL2. Hence,∥ˆθt−1∥Vt−1
is upper bounded by O(√
T). Then one can substitute this bound back into Eqn. (2), and this yields
∥ˆXt∥V−1
t−1≥Ω/parenleftbigg1√
T/parenleftig
1−1√logT/parenrightig
∆t/parenrightbigg
.
Furthermore, by our design of the algorithm, the index of Atis not larger than the index of the arm with
the largest empirical reward at time t. Hence,
It,At=ˆ∆2
t,At
βt−1(γ)∥Xt∥2
V−1
t−1+ log1
βt−1(γ)∥Xt∥2
V−1
t−1≤log1
βt−1(γ)∥ˆXt∥2
V−1
t−1.
16Published in Transactions on Machine Learning Research (04/2024)
If∥Xt∥2
V−1
t−1≥∆2
t
βt−1, by using Corollary 1 and the “peeling device” (Lattimore & Szepesvári, 2020, Chapter 9)
on∆tsuch that 2−l<∆t≤2−l+1forl= 1,2,...,⌈Q⌉whereQ=−log2Γ,
ET/summationdisplay
t=1∆t· 1{Bt,Ct,Dt}· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
≤ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
(15)
=ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
· 1{∆t≤Γ}+ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
· 1{∆t>Γ}
≤TΓ +ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
· 1/braceleftbig
2−l<∆t≤2−l+1/bracerightbig
≤TΓ +ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=12−l+1· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥2−2l
βT/bracerightbigg
≤TΓ +E⌈Q⌉/summationdisplay
l=12−l+16dβT
2−2llog/parenleftbigg
1 +2L2βT
λ·2−2l/parenrightbigg
=TΓ +E⌈Q⌉/summationdisplay
l=12l·12dβTlog/parenleftbigg
1 +22l+1L2βT
λ/parenrightbigg
<TΓ +E⌈Q⌉/summationdisplay
l=12l·12dβTlog/parenleftbigg
1 +22Q+3L2βT
λ/parenrightbigg
=TΓ + (2⌈Q⌉−1)·24dβTlog/parenleftbigg
1 +22Q+3L2βT
λ/parenrightbigg
<TΓ +48dβT
Γlog/parenleftbigg
1 +8L2βT
λΓ2/parenrightbigg
Then with the choice of Γas in Eqn. (5),
ET/summationdisplay
t=1∆t· 1{Bt,Ct,Dt}· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
<d√
Tlog3
2T+48βT√
T
log3
2Tlog/parenleftbigg
1 +8L2βTT
λd2log3T/parenrightbigg
≤O/parenleftig
d√
Tlog3
2T/parenrightig
. (16)
Otherwise we have ∥Xt∥2
V−1
t−1<∆2
t
βt−1, then log1
βt−1∥Xt∥2
V−1
t−1>0since ∆t≤1. Substituting this into
Eqn. (4), then using the event Dtand the bound in (3), we deduce that for all Tsufficiently large, we
have∥Xt∥2
V−1
t−1≥Ω/parenleftbig∆2
t
βt−1log(T/∆2
t)/parenrightbig
. Therefore by using Corollary 1 and the “peeling device” (Lattimore &
Szepesvári, 2020, Chapter 9) on ∆tsuch that 2−l<∆t≤2−l+1forl= 1,2,...,⌈Q⌉where Γ := 2−Qis a
free parameter that we can choose. Consider,
ET/summationdisplay
t=1∆t· 1{Bt,Ct,Dt}· 1/braceleftbigg
∥Xt∥2
V−1
t−1<∆2
t
βt−1/bracerightbigg
≤ET/summationdisplay
t=1∆t· 1{Bt,Ct,Dt}· 1/braceleftig
∆t≤2−⌈Q⌉/bracerightig
· 1/braceleftbigg
∥Xt∥2
V−1
t−1<∆2
t
βt−1/bracerightbigg
+ET/summationdisplay
t=1∆t· 1{Bt,Ct,Dt}· 1/braceleftig
∆t>2−⌈Q⌉/bracerightig
· 1/braceleftbigg
∥Xt∥2
V−1
t−1<∆2
t
βt−1/bracerightbigg
17Published in Transactions on Machine Learning Research (04/2024)
≤O(1) +TΓ +ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥Ω/parenleftig∆2
t
βt−1log(T/∆2
t)/parenrightig/bracerightbigg
1/braceleftig
∆t>2−⌈Q⌉/bracerightig
≤O(1)+TΓ+ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥Ω/parenleftig∆2
t
βt−1log(T/∆2
t)/parenrightig/bracerightbigg
1/braceleftbig
2−l<∆t≤2−l+1/bracerightbig
≤O(1) +TΓ +ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=12−l+1· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥Ω/parenleftig2−2l
βt−1log(T·22l)/parenrightig/bracerightbigg
=O(1) +TΓ +E⌈Q⌉/summationdisplay
l=12−l+1T/summationdisplay
t=11/braceleftbigg
∥Xt∥2
V−1
t−1≥Ω/parenleftbig 2−2l
βt−1log(T·22l)/parenrightbig/bracerightbigg
≤O(1)+TΓ+E⌈Q⌉/summationdisplay
l=12−l+1O/parenleftbigg
22ldβTlog(T·22l) log/parenleftig
1+2L2·22lβTlog(T·22l)
λ/parenrightig/parenrightbigg
<O(1) +TΓ +E⌈Q⌉/summationdisplay
l=12l+1·O/parenleftbigg
dβTlog(T
Γ2) log/parenleftig
1 +L2βTlog(T
Γ2)
λΓ2/parenrightig/parenrightbigg
≤O(1) +TΓ +O/parenleftbiggdβTlog(T
Γ2)
Γlog/parenleftig
1 +L2βTlog(T
Γ2)
λΓ2/parenrightig/parenrightbigg
,
This proves Eqn. (12). Then with the choice of the parameters as in Eqn. (5),
ET/summationdisplay
t=1∆t· 1{Bt,Ct,Dt}· 1/braceleftbigg
∥Xt∥2
V−1
t−1<∆2
t
βt−1/bracerightbigg
<O(1) +d√
Tlog3
2T+O/parenleftbigg
dβTlog/parenleftigT2
d2log3T/parenrightig√
T
dlog3
2Tlog/parenleftbigg
1 +L2βTT
λd2log3T·log/parenleftigT2
d2log3T/parenrightig/parenrightbigg/parenrightbigg
≤O/parenleftig
d√
Tlog3
2T/parenrightig
.
Hence, we can upper bound F1as
F1=ET/summationdisplay
t=1∆t· 1{Bt,Ct,Dt}· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
+ET/summationdisplay
t=1∆t· 1{Bt,Ct,Dt}· 1/braceleftbigg
∥Xt∥2
V−1
t−1<∆2
t
βt−1/bracerightbigg
≤O/parenleftig
d√
Tlog3
2T/parenrightig
+O/parenleftig
d√
Tlog3
2T/parenrightig
≤O/parenleftig
d√
Tlog3
2T/parenrightig
,
which concludes the proof.
C.3 Proof of Lemma 3
Proof.SinceCtandDttogether imply that ⟨θ∗,x∗
t⟩−δ < ε +⟨ˆθt−1,Xt⟩, then using the choices of δand
ε, we have⟨ˆθt−1−θ∗,Xt⟩>∆t√
logT. Substituting this into the event Btand using the Cauchy–Schwarz
inequality, we have
∥Xt∥2
V−1
t−1≥∆2
t
βt−1(γ) logT.
Again applying the “peeling device” on ∆tand Corollary 1, we can upper bound F2as follows:
F2≤ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1logT/bracerightbigg
18Published in Transactions on Machine Learning Research (04/2024)
≤TΓ +ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1logT/bracerightbigg
· 1/braceleftbig
2−l<∆t≤2−l+1/bracerightbig
≤TΓ +ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=12−l+1· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥2−2l
βTlogT/bracerightbigg
≤TΓ +E⌈Q⌉/summationdisplay
l=12−l+1·22l·6dβT(logT) log/parenleftbigg
1 +22l+1·L2βTlogT
λ/parenrightbigg
≤TΓ +E⌈Q⌉/summationdisplay
l=12l·12dβT(logT) log/parenleftbigg
1 +22⌈Q⌉+1·L2βTlogT
λ/parenrightbigg
=TΓ + (2⌈Q⌉−1)·24dβT(logT) log/parenleftbigg
1 +22⌈Q⌉+1·L2βTlogT
λ/parenrightbigg
<TΓ +48dβTlogT
Γlog/parenleftbigg
1 +8L2βTlogT
λΓ2/parenrightbigg
=TΓ +O/parenleftbiggdβTlogT
Γlog/parenleftbigg
1 +L2βTlogT
λΓ2/parenrightbigg/parenrightbigg
This proves Eqn. (13). Hence with the choice of the parameter Γas in Eqn. (5),
F2≤d√
Tlog3
2T+O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
≤O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
.
C.4 Proof of Lemma 4
Proof.ForF3, this is the case when the best arm at time tdoes not perform sufficiently well so that the
empirically largest reward at time tis far from the highest expected reward. One observes that minimizing
F3results in a tradeoff with respect to F1. On the event Ct, we can apply the “peeling device” on ⟨θ∗,x∗
t⟩−
⟨ˆθt−1,x∗
t⟩such thatq+1
2δ≤⟨θ∗,x∗
t⟩−⟨ˆθt−1,x∗
t⟩<q+2
2δwhereq∈N. Then using the fact that It,At≤It,a∗
t,
we have
log1
βt−1∥Xt∥2
V−1
t−1<q2δ2
4βt−1∥x∗
t∥2
V−1
t−1+ log1
βt−1∥x∗
t∥2
V−1
t−1. (17)
On the other hand, using the event Btand the Cauchy–Schwarz inequality, it holds that
∥x∗
t∥V−1
t−1≥(q+ 1)δ
2/radicalbig
βt−1. (18)
If∥Xt∥2
V−1
t−1≥∆2
t
βt−1, the regret in this case is bounded by O(d√TlogT)(similar to the procedure to get from
Eqn. (15) to Eqn. (16)). Otherwise log1
βt−1∥Xt∥2
V−1
t−1>log1
∆2
t≥0, then combining Eqn. (17) and Eqn. (18)
implies that
∥Xt∥2
V−1
t−1≥(q+ 1)2δ2
4βt−1exp/parenleftbigg
−q2
(q+ 1)2/parenrightbigg
.
Notice here with√
λS≥1,∥Xt∥2
V−1
t−1<∆2
t
βt−1≤1
βt−1≤1, it holds that for all q∈N,
(q+ 1)2δ2
4βt−1exp/parenleftbigg
−q2
(q+ 1)2/parenrightbigg
<1. (19)
19Published in Transactions on Machine Learning Research (04/2024)
Using Corollary 1, one can show that:
T/summationdisplay
t=1∆t· 1/braceleftbig
Bt,Ct/bracerightbig
· 1/braceleftbigg
∥Xt∥2
V−1
t−1<∆2
t
βt−1/bracerightbigg
≤TΓ +T/summationdisplay
t=1⌈Q⌉/summationdisplay
l=1∆t· 1/braceleftbig
Bt,Ct/bracerightbig
· 1/braceleftbigg
∥Xt∥2
V−1
t−1<∆2
t
βt−1/bracerightbigg
· 1/braceleftbig
2−l<∆t≤2−l+1/bracerightbig
≤TΓ +T/summationdisplay
t=1⌈Q⌉/summationdisplay
l=1∞/summationdisplay
q=1∆t· 1{Bt}· 1/braceleftbiggq+ 1
2δ≤⟨θ∗,x∗
t⟩−⟨ˆθt−1,x∗
t⟩<q+ 2
2δ/bracerightbigg
· 1/braceleftbigg
∥Xt∥2
V−1
t−1<∆2
t
βt−1/bracerightbigg
· 1/braceleftbig
2−l<∆t≤2−l+1/bracerightbig
≤TΓ +T/summationdisplay
t=1⌈Q⌉/summationdisplay
l=1∞/summationdisplay
q=1∆t· 1/braceleftbigg
1≥∥Xt∥2
V−1
t−1≥(q+ 1)2δ2
4βt−1exp/parenleftbigg
−q2
(q+ 1)2/parenrightbigg/bracerightbigg
· 1/braceleftbig
2−l<∆t≤2−l+1/bracerightbig
=TΓ +T/summationdisplay
t=1⌈Q⌉/summationdisplay
l=1∞/summationdisplay
q=1∆t· 1/braceleftbigg
1≥∥Xt∥2
V−1
t−1≥(q+ 1)2∆2
t
4βt−1logTexp/parenleftbigg
−q2
(q+ 1)2/parenrightbigg/bracerightbigg
· 1/braceleftbig
2−l<∆t≤2−l+1/bracerightbig
≤TΓ +T/summationdisplay
t=1⌈Q⌉/summationdisplay
l=1∞/summationdisplay
q=12−l+1· 1/braceleftbigg
1≥∥Xt∥2
V−1
t−1>(q+ 1)2·2−2l
4βTlogTexp/parenleftbigg
−q2
(q+ 1)2/parenrightbigg/bracerightbigg
≤TΓ +⌈Q⌉/summationdisplay
l=1∞/summationdisplay
q=12−l+1·22l·24dβT(logT)·exp/parenleftbigg
q2
(q+1)2/parenrightbigg
(q+ 1)2·log/parenleftbigg
1 +22l·8L2βTlogT
λ·exp/parenleftbigg
q2
(q+1)2/parenrightbigg
(q+ 1)2/parenrightbigg
<TΓ +⌈Q⌉/summationdisplay
l=1∞/summationdisplay
q=12l+1·24dβT(logT)·exp/parenleftbigg
q2
(q+1)2/parenrightbigg
(q+ 1)2·log/parenleftbigg
1 +22l+1·L2βTlogT
λ/parenrightbigg
=TΓ +⌈Q⌉/summationdisplay
l=12l+1·24dβT(logT)·log/parenleftbigg
1 +22l+1·L2βTlogT
λ/parenrightbigg∞/summationdisplay
q=1exp/parenleftbigg
q2
(q+1)2/parenrightbigg
(q+ 1)2
≤TΓ +⌈Q⌉/summationdisplay
l=12l+1·24dβT(logT)·log/parenleftbigg
1 +22l+1·L2βTlogT
λ/parenrightbigg
·(1.09)
≤TΓ +⌈Q⌉/summationdisplay
l=12l+1·27dβT(logT)·log/parenleftbigg
1 +22l+1·L2βTlogT
λ/parenrightbigg
≤TΓ +⌈Q⌉/summationdisplay
l=12l+1·27dβT(logT)·log/parenleftbigg
1 +22⌈Q⌉+1·L2βTlogT
λ/parenrightbigg
<TΓ +⌈Q⌉/summationdisplay
l=1216dβTlogT
Γ·log/parenleftbigg
1 +8L2βTlogT
λΓ2/parenrightbigg
=TΓ +O/parenleftbiggdβTlogT
Γlog/parenleftig
1 +L2βTlogT
λΓ2/parenrightig/parenrightbigg
. (20)
Hence
F3=T/summationdisplay
t=1∆t· 1/braceleftbig
Bt,Ct/bracerightbig
· 1/braceleftbigg
∥Xt∥2
V−1
t−1<∆2
t
βt−1/bracerightbigg
+T/summationdisplay
t=1∆t· 1/braceleftbig
Bt,Ct/bracerightbig
· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
<O/parenleftbiggdβT
Γlog/parenleftbigg
1 +L2βT
λΓ2/parenrightbigg/parenrightbigg
+ 2TΓ +O/parenleftbiggdβTlogT
Γlog/parenleftig
1 +L2βTlogT
λΓ2/parenrightig/parenrightbigg
20Published in Transactions on Machine Learning Research (04/2024)
≤2TΓ +O/parenleftbiggdβTlog(T)
Γlog/parenleftig
1 +L2βTlog(T)
λΓ2/parenrightig/parenrightbigg
.
This proves Eqn. (14). With the choice of Γas in Eqn. (5),
F3≤2d√
Tlog3
2T+O/parenleftbiggd√
TβTlogT
dlog3
2Tlog/parenleftbigg
1 +TL2βTlogT
λd2log3T/parenrightbigg/parenrightbigg
<2d√
Tlog3
2T+O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
=O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
.
C.5 Proof of Lemma 5
Proof.ForF4, the proof is straightforward by using Lemma 1 with the choice of γ. Indeed, one has
F4=ET/summationdisplay
t=1∆t· 1/braceleftbig
Bt/bracerightbig
≤TΓ +ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=1∆t· 1/braceleftbig
2−l<∆t≤2−l+1/bracerightbig
1/braceleftbig
Bt/bracerightbig
≤TΓ +ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=12−l+11/braceleftbig
Bt/bracerightbig
≤TΓ +T/summationdisplay
t=1⌈Q⌉/summationdisplay
l=12−l+1P/parenleftbig
Bt/parenrightbig
≤TΓ +T/summationdisplay
t=1⌈Q⌉/summationdisplay
l=12−l+1γ
=TΓ +T/summationdisplay
t=11
t2⌈Q⌉/summationdisplay
l=12−l+1=TΓ +T/summationdisplay
t=12−Γ
t2<TΓ +π2
3=TΓ +O(1).
With the choice of Γas in Eqn. (5),
F4<d√
Tlog3
2T+O(1)
≤O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
.
C.6 Proof of Theorem 1
Proof.Combining Lemmas 2, 3, 4 and 5,
RT=F1+F2+F3+F4
≤O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
+O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
+O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
+O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
=O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
.
D Proof of the regret bound for LinIMED-2 (Proof of Theorem 2)
We choose γandΓas follows:
γ=1
t2Γ =√dβTlogT√
T. (21)
21Published in Transactions on Machine Learning Research (04/2024)
D.1 Statement of Lemmas for LinIMED-2
We first state the following lemmas which respectively show the upper bound of F1toF4:
Lemma 6. Under Assumption 1, and the assumption that√
λS≥1, for the free parameter 0<Γ<1, the
termF1for LinIMED-3 satisfies:
F1≤TΓ +O/parenleftbiggdβTlogT
Γ/parenrightbigg
log/parenleftbigg
1 +L2βTlogT
λΓ2/parenrightbigg
.
Lemma 7. Under Assumption 1, and the assumption that√
λS≥1, for the free parameter 0<Γ<1, the
termF2for LinIMED-3 satisfies:
F2≤TΓ +O/parenleftbiggdβTlogT
Γ/parenrightbigg
log/parenleftbigg
1 +L2βTlogT
λΓ2/parenrightbigg
.
Lemma 8. Under Assumption 1, and the assumption that√
λS≥1, for the free parameter 0<Γ<1, the
termF3for LinIMED-3 satisfies:
F3≤5TΓ +O/parenleftbiggdβTlogT
Γlog/parenleftig
1 +L2βTlogT
λΓ2/parenrightig/parenrightbigg
+O/parenleftbigg/radicalbig
TlogTlog/parenleftbiggL2βTlogT
λΓ2/parenrightbigg/parenrightbigg
.
Lemma 9. Under Assumption 1, with the choice of γ=1
t2as in Eqn. (21), for the free parameter 0<Γ<1,
the termF4for LinIMED-3 satisfies:
F4≤TΓ +O(1).
D.2 Proof of Lemma 6
Proof.We first partition the analysis into the cases ˆAt̸=Atand ˆAt=Atas follows:
F1=ET/summationdisplay
t=1∆t· 1{Bt,Ct,Dt}
=ET/summationdisplay
t=1∆t· 1{Bt,Ct,Dt}· 1/braceleftig
ˆAt̸=At/bracerightig
+ET/summationdisplay
t=1∆t· 1{Bt,Ct,Dt}· 1/braceleftig
ˆAt=At/bracerightig
Case 1: IfˆAt̸=At, this means that the index of AtisIt,At=ˆ∆2
t,At
βt−1∥Xt∥2
V−1
t−1+ log1
βt−1∥Xt∥2
V−1
t−1. Using the
fact thatIt,At≤It,ˆAtwe have:
It,At=ˆ∆2
t,At
βt−1∥Xt∥2
V−1
t−1+ log1
βt−1∥Xt∥2
V−1
t−1
≤logT∧log1
βt−1∥ˆXt∥2
V−1
t−1
≤logT.
Therefore
ˆ∆2
t,At
βt−1∥Xt∥2
V−1
t−1+ log1
βt−1∥Xt∥2
V−1
t−1≤logT . (22)
22Published in Transactions on Machine Learning Research (04/2024)
If∥Xt∥2
V−1
t−1≥∆2
t
βt−1, using the same procedure to get from Eqn. (15) to Eqn. (16), one has:
ET/summationdisplay
t=1∆t· 1{Bt,Ct,Dt}· 1/braceleftig
ˆAt̸=At/bracerightig
· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
≤ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
<TΓ +48dβT
Γlog/parenleftbigg
1 +8L2βT
λΓ2/parenrightbigg
=TΓ +O/parenleftbiggdβT
Γlog/parenleftbigg
1 +L2βT
λΓ2/parenrightbigg/parenrightbigg
.
Else if∥Xt∥2
V−1
t−1<∆2
t
βt−1, this implies that log1
βt−1∥Xt∥2
V−1
t−1>log1
∆2
t≥0. Then substituting the event
Dt:={ˆ∆t,At≥ε}into Eqn. (22), we obtain
ε2
βt−1∥Xt∥2
V−1
t−1≤logT .
With√
λS≥1we haveβt−1≥1, then one has
∥Xt∥2
V−1
t−1≥ε2
βt−1logT.
Hence
ET/summationdisplay
t=1∆t· 1/braceleftbigg
Bt,Ct,Dt,ˆAt̸=At,∥Xt∥2
V−1
t−1<∆2
t
βt−1/bracerightbigg
≤ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥ε2
βt−1logT/bracerightbigg
.
With the choice of ε= (1−2√
logT)∆t, whenT≥149>exp(5),ε >∆t
10, then performing the “peeling
device” on ∆tyields
ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥ε2
βt−1logT/bracerightbigg
· 1{∆t≥Γ}
≤149 + ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=1∆t· 1/braceleftbigg
2−l<∆t≤2−l+1,∥Xt∥2
V−1
t−1≥ε2
βt−1logT/bracerightbigg
≤O(1) +E⌈Q⌉/summationdisplay
l=12−l+1T/summationdisplay
t=11/braceleftbigg
∥Xt∥2
V−1
t−1≥ε2
βt−1logT/bracerightbigg
≤O(1) +E⌈Q⌉/summationdisplay
l=12−l+1T/summationdisplay
t=11/braceleftbigg
∥Xt∥2
V−1
t−1≥2−2l
100βTlogT/bracerightbigg
≤O(1) +E⌈Q⌉/summationdisplay
l=12−l+1·22l·600dβT(logT) log/parenleftbigg
1 +22l·200L2βTlogT
λ/parenrightbigg
23Published in Transactions on Machine Learning Research (04/2024)
≤O(1) +E⌈Q⌉/summationdisplay
l=12l+1·600dβT(logT) log/parenleftbigg
1 +22⌈Q⌉·200L2βTlogT
λ/parenrightbigg
<O(1) +4800dβTlogT
Γlog/parenleftbigg
1 +800L2βTlogT
λΓ2/parenrightbigg
.
Considering the event {∆t<Γ}, we can upper bound the corresponding expectation as follows
ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥ε2
βt−1logT/bracerightbigg
· 1{∆t<Γ}≤ET/summationdisplay
t=1∆t· 1{∆t<Γ}<TΓ.
Then
ET/summationdisplay
t=1∆t· 1/braceleftbigg
Bt,Ct,Dt,ˆAt̸=At,∥Xt∥2
V−1
t−1<∆2
t
βt−1/bracerightbigg
≤ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥ε2
βt−1logT/bracerightbigg
=ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥ε2
βt−1logT/bracerightbigg
· 1{∆t≥Γ}
+ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥ε2
βt−1logT/bracerightbigg
· 1{∆t<Γ}
≤O(1) +TΓ +4800dβTlogT
Γlog/parenleftbigg
1 +800L2βTlogT
λΓ2/parenrightbigg
.
Hence
ET/summationdisplay
t=1∆t· 1/braceleftig
Bt,Ct,Dt,ˆAt̸=At/bracerightig
=ET/summationdisplay
t=1∆t· 1/braceleftbigg
Bt,Ct,Dt,ˆAt̸=At,∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
+ET/summationdisplay
t=1∆t· 1/braceleftbigg
Bt,Ct,Dt,ˆAt̸=At,∥Xt∥2
V−1
t−1<∆2
t
βt−1/bracerightbigg
≤TΓ +O/parenleftbiggdβT
Γlog/parenleftbigg
1 +L2βT
λΓ2/parenrightbigg/parenrightbigg
+O(1) +TΓ +4800dβTlogT
Γlog/parenleftbigg
1 +800L2βTlogT
λΓ2/parenrightbigg
≤TΓ +O/parenleftbiggdβTlogT
Γlog/parenleftbigg
1 +L2βTlogT
λΓ2/parenrightbigg/parenrightbigg
.
Case 2: IfˆAt=At, then from the event Ctand the choice δ=∆t√
logTwe have
⟨ˆθt−1−θ∗,Xt⟩>/parenleftig
1−1√logT/parenrightig
∆t.
Furthermore, using the definition of the event Bt, that implies that
∥Xt∥2
V−1
t−1>(1−1√
logT)2∆2
t
βt−1.
WhenT >8>exp(2),(1−1√
logT)2>1
16, then similarily, we can bound this term by O(dβT
Γ) log(1 +L2βT
λΓ2)
24Published in Transactions on Machine Learning Research (04/2024)
Summarizing the two cases,
F1≤O(1) +TΓ +O/parenleftbiggdβTlogT
Γ/parenrightbigg
log/parenleftbigg
1 +L2βTlogT
λΓ2/parenrightbigg
≤TΓ +O/parenleftbiggdβTlogT
Γ/parenrightbigg
log/parenleftbigg
1 +L2βTlogT
λΓ2/parenrightbigg
.
D.3 Proof of Lemma 7
Proof.Recall that
F2=ET/summationdisplay
t=1∆t· 1/braceleftbig
Bt,Ct,Dt/bracerightbig
.
FromCtandDt, we derive that:
⟨θ∗,a∗
t⟩−δ<ε +⟨ˆθt−1,Xt⟩.
With the choice δ=∆t√
logT,ε= (1−2√
logT)∆t, we have
⟨ˆθt−1−θ∗,Xt⟩>∆t√logT. (23)
Then using the definition of the event Btin Eqn. (23) yields
∥Xt∥2
V−1
t−1≥∆2
t
βt−1logT.
Using a similar procedure as in that from Eqn. (15) to Eqn. (16), we can upper bound F2by
F2≤TΓ +O/parenleftbiggdβTlogT
Γ/parenrightbigg
log/parenleftbigg
1 +L2βTlogT
λΓ2/parenrightbigg
.
D.4 Proof of Lemma 8
Proof.From the event Ct, which is maxb∈At⟨ˆθt−1,b⟩≤⟨θ∗,x∗
t⟩−δ, the index of the best arm at time tcan
be upper bounded as:
It,a∗
t≤(⟨θ∗,x∗
t⟩−δ−⟨ˆθt−1,x∗
t⟩)2
βt−1∥x∗
t∥2
V−1
t−1+ log1
βt−1∥x∗
t∥2
V−1
t−1.
Case 1: IfˆAt̸=At, then we have
It,a∗
t≥It,At≥log1
βt−1∥Xt∥2
V−1
t−1.
Supposeq+1
2δ≤⟨θ∗,x∗
t⟩−⟨ˆθt−1,x∗
t⟩<q+2
2δforq∈N, then one has
log1
βt−1∥Xt∥2
V−1
t−1≤q2δ2
4βt−1∥x∗
t∥2
V−1
t−1+ log1
βt−1∥x∗
t∥2
V−1
t−1. (24)
25Published in Transactions on Machine Learning Research (04/2024)
On the other hand, on the event Bt,
∥x∗
t∥V−1
t−1≥(q+ 1)δ
2/radicalbig
βt−1. (25)
If∥Xt∥2
V−1
t−1≥∆2
t
βt−1, using the same procedure from Eqn. (15) to Eqn. (16), one has:
ET/summationdisplay
t=1∆t· 1/braceleftbig
Bt,Ct/bracerightbig
· 1/braceleftig
ˆAt̸=At/bracerightig
· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
≤ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
<TΓ +48dβT
Γlog/parenleftbigg
1 +8L2βT
λΓ2/parenrightbigg
=TΓ +O/parenleftbiggdβT
Γlog/parenleftbigg
1 +L2βT
λΓ2/parenrightbigg/parenrightbigg
.
Else if∥Xt∥2
V−1
t−1<∆2
t
βt−1, this implies that log1
βt−1∥Xt∥2
V−1
t−1>log1
∆2
t≥0. Then combining Eqn. (24) and
Eqn. (25) implies that
∥Xt∥2
V−1
t−1≥(q+ 1)2δ2
4βt−1exp/parenleftbigg
−q2
(q+ 1)2/parenrightbigg
.
Then using the same procedure to get from Eqn. (19) to Eqn. (20), we have
T/summationdisplay
t=1∆t· 1/braceleftbig
Bt,Ct/bracerightbig
· 1/braceleftbigg
∥Xt∥2
V−1
t−1<∆2
t
βt−1,ˆAt̸=At/bracerightbigg
<TΓ +O/parenleftbiggdβTlogT
Γlog/parenleftig
1 +L2βTlogT
λΓ2/parenrightig/parenrightbigg
. (26)
Case 2: ˆAt=At. If∥Xt∥2
V−1
t−1≥∆2
t
βt−1, using the same procedure to get from Eqn. (15) to Eqn. (16), one
has:
ET/summationdisplay
t=1∆t· 1/braceleftbig
Bt,Ct/bracerightbig
· 1/braceleftig
ˆAt=At/bracerightig
· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
≤ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
<TΓ +48dβT
Γlog/parenleftbigg
1 +8L2βT
λΓ2/parenrightbigg
=TΓ +O/parenleftbiggdβT
Γlog/parenleftbigg
1 +L2βT
λΓ2/parenrightbigg/parenrightbigg
.
Else∥Xt∥2
V−1
t−1<∆2
t
βt−1implies that log1
βt−1∥Xt∥2
V−1
t−1>log1
∆2
t≥0.
26Published in Transactions on Machine Learning Research (04/2024)
Iflog1
βt−1∥Xt∥2
V−1
t−1<logT, then using the same procedure to get from Eqn. (24) to Eqn. (26), we have
T/summationdisplay
t=1∆t· 1/braceleftbig
Bt,Ct/bracerightbig
· 1

∥Xt∥2
V−1
t−1<∆2
t
βt−1,ˆAt=At,log1
βt−1∥Xt∥2
V−1
t−1<logT
βt−1


<TΓ +O/parenleftbiggdβTlogT
Γlog/parenleftig
1 +L2βTlogT
λΓ2/parenrightig/parenrightbigg
.
Iflog1
βt−1∥Xt∥2
V−1
t−1≥logT, this means now the index of AtisIt,At= logT, by performing the “peeling
device” such thatq+1
2δ≤⟨θ∗,x∗
t⟩−⟨ˆθt−1,x∗
t⟩<q+2
2δforq∈N, we have
logT≤q2δ2
4βt−1∥x∗
t∥2
V−1
t−1+ log1
βt−1∥x∗
t∥2
V−1
t−1. (27)
On the other hand, using the definition of the event Bt,
∥x∗
t∥V−1
t−1≥(q+ 1)δ
2/radicalbig
βt−1. (28)
Combining Eqn. (27) and (28), we have
δ≤2 exp(q2
2(q+1)2)
(q+ 1)√
T.
Then with δ=∆t√
logT, this implies that
∆t≤2√logTexp(q2
2(q+1)2)
(q+ 1)√
T.
On the other hand, fromq+1
2δ≤/radicalbig
βt−1∥x∗
t∥V−1
t−1≤/radicalbig
βt−1·L√
λ, we haveq+ 1≤2L√
βt−1logT√
λ∆t. Hence,
T/summationdisplay
t=1∆t· 1/braceleftbig
Bt,Ct/bracerightbig
· 1

∥Xt∥2
V−1
t−1<∆2
t
βt−1,ˆAt=At,log1
βt−1∥Xt∥2
V−1
t−1≥logT,∆t≥Γ


≤E⌊2L√
βTlogT√
λΓ−1⌋/summationdisplay
q=1T/summationdisplay
t=1∆t· 1

∆t≤2√logTexp(q2
2(q+1)2)
(q+ 1)√
T


≤E⌊2L√
βTlogT√
λΓ−1⌋/summationdisplay
q=1T/summationdisplay
t=12√logTexp(q2
2(q+1)2)
(q+ 1)√
T
=E⌊2L√
βTlogT√
λΓ−1⌋/summationdisplay
q=12√TlogTexp(q2
2(q+1)2)
q+ 1
<E⌊2L√
βTlogT√
λΓ−1⌋/summationdisplay
q=12√e√TlogT
q+ 1
27Published in Transactions on Machine Learning Research (04/2024)
<2√e/radicalbig
TlogTlog/parenleftbigg2L√logT√
λΓ−1/parenrightbigg
≤O/parenleftbigg/radicalbig
TlogTlog/parenleftbiggL2βTlogT
λΓ2/parenrightbigg/parenrightbigg
.
Summarizing the two cases ( ˆAt̸=Atand ˆAt=At), we see that F3is upper bounded by:
F3<TΓ +O/parenleftbiggdβT
Γlog/parenleftbigg
1 +L2βT
λΓ2/parenrightbigg/parenrightbigg
+TΓ +O/parenleftbiggdβTlogT
Γlog/parenleftig
1 +L2βTlogT
λΓ2/parenrightig/parenrightbigg
+TΓ +O/parenleftbiggdβT
Γlog/parenleftbigg
1 +L2βT
λΓ2/parenrightbigg/parenrightbigg
+TΓ +O/parenleftbiggdβTlogT
Γlog/parenleftig
1 +L2βTlogT
λΓ2/parenrightig/parenrightbigg
+TΓ +O/parenleftbigg/radicalbig
TβTlogTlog/parenleftbiggL2βTlogT
λΓ2/parenrightbigg/parenrightbigg
≤5TΓ +O/parenleftbiggdβTlogT
Γlog/parenleftig
1 +L2βTlogT
λΓ2/parenrightig/parenrightbigg
+O/parenleftbigg/radicalbig
TlogTlog/parenleftbiggL2βTlogT
λΓ2/parenrightbigg/parenrightbigg
.
D.5 Proof of Lemma 9
Proof.The proof of this case is straightforward by using Lemma 1 with the choice γ=1
t2:
F4=ET/summationdisplay
t=1∆t· 1/braceleftbig
Bt/bracerightbig
=ET/summationdisplay
t=1∆t· 1/braceleftbig
Bt,∆t<Γ/bracerightbig
+ET/summationdisplay
t=1∆t· 1/braceleftbig
Bt,∆t≥Γ/bracerightbig
<TΓ +ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=1∆t· 1/braceleftbig
Bt,2−l<∆t≤2−l+1/bracerightbig
≤TΓ +ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=12−l+1· 1/braceleftbig
Bt/bracerightbig
≤TΓ +⌈Q⌉/summationdisplay
l=12−l+1T/summationdisplay
t=1P/braceleftbig
Bt/bracerightbig
=TΓ +⌈Q⌉/summationdisplay
l=12−l+1·π2
6
<TΓ + (2−Γ)·π2
6
<TΓ +π2
3
=TΓ +O(1).
D.6 Proof of Theorem 2
Proof.Combining Lemmas 6, 7, 8 and 9, with the choices of γandΓas in Eqn. (21), the regret of LinIMED-2
is bounded as follows:
RT=F1+F2+F3+F4
28Published in Transactions on Machine Learning Research (04/2024)
≤TΓ +O/parenleftbiggdβTlogT
Γ/parenrightbigg
log/parenleftbigg
1 +L2βTlogT
λΓ2/parenrightbigg
+TΓ +O/parenleftbiggdβTlogT
Γ/parenrightbigg
log/parenleftbigg
1 +L2βTlogT
λΓ2/parenrightbigg
+ 5TΓ +O/parenleftbiggdβTlogT
Γlog/parenleftbigg
1 +L2βTlogT
λΓ2/parenrightbigg/parenrightbigg
+O/parenleftbigg/radicalbig
TlogTlog/parenleftbiggL2βTlogT
λΓ2/parenrightbigg/parenrightbigg
+TΓ +O(1)
≤8TΓ +O/parenleftbiggdβTlogT
Γlog/parenleftbigg
1 +L2βTlogT
λΓ2/parenrightbigg/parenrightbigg
+O/parenleftbigg/radicalbig
TlogTlog/parenleftbiggL2βTlogT
λΓ2/parenrightbigg/parenrightbigg
= 8/radicalbig
dTβTlogT+O/parenleftbigg/radicalbig
dTβTlog/parenleftbigg
1 +TL2
λdlogT/parenrightbigg/parenrightbigg
+O/parenleftbigg/radicalbig
TlogTlog/parenleftbiggTL2
λdlogT/parenrightbigg/parenrightbigg
= 8d√
Tlog3
2T+O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
+O/parenleftbigg√
Tlog3
2T/parenrightbigg
≤O/parenleftbigg
d√
Tlog3
2T/parenrightbigg
.
E Proof of the regret bound for LinIMED-3 (Proof of Theorem 3)
First we define a∗
tas the best arm in time step tsuch thata∗
t= arg maxa∈At⟨θ∗,xt,a⟩, and usex∗
t:=xt,a∗
t
denote its corresponding context. Define ˆAt:= arg maxa∈AtUCBt(a). Let ∆t:=⟨θ∗,x∗
t⟩−⟨θ∗,Xt⟩denote
the regret in time t. Define the following events:
B′
t:=/braceleftbig
∥ˆθt−1−θ∗∥Vt−1≤/radicalbig
βt−1(γ)/bracerightbig
, D′
t:=/braceleftbigˆ∆t,At>ε/bracerightbig
.
whereεis a free parameter set to be ε=∆t
3in this proof sketch.
Then the expected regret RT=E/summationtextT
t=1∆tcan be partitioned by events B′
t,D′
tsuch that:
RT=ET/summationdisplay
t=1∆t· 1{B′
t,D′
t}
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:F1+ET/summationdisplay
t=1∆t· 1/braceleftig
Bt,D′
t/bracerightig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:F2+ET/summationdisplay
t=1∆t· 1/braceleftig
B′
t/bracerightig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:F3.
For theF1case:
FromD′
twe knowAt̸=ˆAt, therefore
It,At=ˆ∆2
t,At
βt−1∥Xt∥2
V−1
t−1+ log1
βt−1∥Xt∥2
V−1
t−1. (29)
FromD′
tandIt,At≤It,ˆAt≤logC
maxa∈Atˆ∆2
t,a, we have
It,At<logC
ε2. (30)
Combining Eqn. (29) and Eqn. (30),
ˆ∆2
t,At
βt−1∥Xt∥2
V−1
t−1+ log1
βt−1∥Xt∥2
V−1
t−1<logC
ε2.
Then
ˆ∆2
t,At
βt−1∥Xt∥2
V−1
t−1<log/parenleftbigg
βt−1∥Xt∥2
V−1
t−1·C
ε2/parenrightbigg
. (31)
29Published in Transactions on Machine Learning Research (04/2024)
If∥Xt∥2
V−1
t−1≥∆2
t
βt−1, using the same procedure from Eqn. (15) to Eqn. (16), one has:
ET/summationdisplay
t=1∆t· 1{B′
t,D′
t}· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
≤ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1≥∆2
t
βt−1/bracerightbigg
<TΓ +48dβT
Γlog/parenleftbigg
1 +8L2βT
λΓ2/parenrightbigg
=TΓ +O/parenleftbiggdβT
Γlog/parenleftbigg
1 +L2βT
λΓ2/parenrightbigg/parenrightbigg
.
Else∥Xt∥2
V−1
t−1<∆2
t
βt−1,this implies that βt−1∥Xt∥2
V−1
t−1<∆2
t, plug this into Eqn. (31) and with the choice of
ε=∆t
3andD′
t, we have
∆2
t
9βt−1∥Xt∥2
V−1
t−1<log(9C).
SinceC≥1is a constant, then
∥Xt∥2
V−1
t−1>∆2
t
9βt−1log(9C).
Using the same procedure from Eqn. (15) to Eqn. (16), one has:
ET/summationdisplay
t=1∆t· 1{B′
t,D′
t}· 1/braceleftbigg
∥Xt∥2
V−1
t−1<∆2
t
βt−1/bracerightbigg
≤ET/summationdisplay
t=1∆t· 1/braceleftbigg
∥Xt∥2
V−1
t−1>∆2
t
9βt−1log(9C)/bracerightbigg
<TΓ +O/parenleftbiggdβTlogC
Γlog/parenleftbigg
1 +L2βTlogC
λΓ2/parenrightbigg/parenrightbigg
.
Hence
F1<2TΓ +O/parenleftbiggdβTlogC
Γlog/parenleftbigg
1 +L2βTlogC
λΓ2/parenrightbigg/parenrightbigg
. (32)
For theF2case:Since the event B′
tholds,
max
a∈AtUCBt(a)≥UCBt(a∗
t) =⟨ˆθt−1,x∗
t⟩+/radicalbig
βt−1∥x∗
t∥V−1
t−1≥⟨θ∗,x∗
t⟩ (33)
On the other hand, from D′
twe have
max
a∈AtUCBt(a)≤UCBt(At) +ε=⟨ˆθt−1,Xt⟩+/radicalbig
βt−1∥Xt∥V−1
t−1+ε . (34)
Combining Eqn. (33) and Eqn. (34),
⟨θ∗,x∗
t⟩≤⟨ ˆθt−1,Xt⟩+/radicalbig
βt−1∥Xt∥V−1
t−1+ε .
Hence
∆t−ε≤⟨ˆθt−1−θ∗,Xt⟩+/radicalbig
βt−1∥Xt∥V−1
t−1.
30Published in Transactions on Machine Learning Research (04/2024)
Then with ε=∆t
3andB′
t, we have
2
3∆t≤2/radicalbig
βt−1∥Xt∥V−1
t−1,
therefore
∥Xt∥2
V−1
t−1>∆2
t
9βt−1.
Using the same procedure from Eqn. (15) to Eqn. (16), one has:
F2<TΓ +O/parenleftbiggdβT
Γlog/parenleftbigg
1 +L2βT
λΓ2/parenrightbigg/parenrightbigg
. (35)
For theF3case:
using Lemma 1 with the choice γ=1
t2:
F3=ET/summationdisplay
t=1∆t· 1/braceleftig
B′
t/bracerightig
=ET/summationdisplay
t=1∆t· 1/braceleftig
B′
t,∆t<Γ/bracerightig
+ET/summationdisplay
t=1∆t· 1/braceleftig
B′
t,∆t≥Γ/bracerightig
<TΓ +ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=1∆t· 1/braceleftig
B′
t,2−l<∆t≤2−l+1/bracerightig
≤TΓ +ET/summationdisplay
t=1⌈Q⌉/summationdisplay
l=12−l+1· 1/braceleftig
B′
t/bracerightig
≤TΓ +⌈Q⌉/summationdisplay
l=12−l+1T/summationdisplay
t=1P/braceleftig
B′
t/bracerightig
=TΓ +⌈Q⌉/summationdisplay
l=12−l+1·π2
6
<TΓ + (2−Γ)·π2
6
<TΓ +π2
3
=TΓ +O(1). (36)
E.1 Proof of Theorem 3
Proof.Combining Eqn. (32), (35), (36) with the choices of γ=1
t2andΓ =βT√
TandC≥1is a constant, the
regret of LinIMED-3 is bounded as follows:
RT=F1+F2+F3+F4
<4TΓ +O/parenleftbiggdβTlogC
Γlog/parenleftbigg
1 +L2βTlogC
λΓ2/parenrightbigg/parenrightbigg
+O/parenleftbiggdβT
Γlog/parenleftbigg
1 +L2βT
λΓ2/parenrightbigg/parenrightbigg
+O(1)
<O/parenleftbigg
d√
TlogClog/parenleftbigg
1 +L2TlogC
λ/parenrightbigg/parenrightbigg
=O/parenleftbigg
d√
Tlog(T)/parenrightbigg
.
This completes the proof.
31Published in Transactions on Machine Learning Research (04/2024)
F Proof of the regret bound for SupLinIMED (Proof of Theorem 4)
Definest∈[⌈logT⌉]as the index of swhen the arm is chosen at time t. For the SupLinIMED, the index
of arms except the empirically best arm is defined by It,a=/parenleftbigg
ˆ∆st
t,a
wst
t,a/parenrightbigg2
−2 log(wst
t,a), whereas the index of
the empirically best arm is defined by It,ˆA∗
t= log(2T)∧(−2 log(wst
t,ˆA∗
t))where ˆA∗
t= arg maxa∈ˆAst⟨ˆθst
t,xt,a⟩.
Define the index of the best arm at time tasa∗
t:= arg maxa∈[K]⟨θ∗,xt,a⟩.
Remark 1. Here the upper bound we set for the index of the empirically best arm is log(2T), which is
slightly larger than our previous logT(Line 10 in the LinIMED algorithm) since in the first step of the of
the SupLinIMED algorithm or, more generally, the SupLinUCB-type algorithms, the width of each arm is
less than1√
T, as a result, the index of each arm is larger than logT.
Let the set of time indices such that the chosen arm is from Step 1 (Lines 6–9 in Algorithm 2) be Ψ0. Then
the cumulative expected regret of the SupLinIMED algorithm over time horizon Tcan be defined by the
following equation:
RT=E/bracketleftigg/summationdisplay
t∈Ψ0⟨θ∗,xt,a∗
t−Xt⟩/bracketrightigg
+E
/summationdisplay
t̸∈Ψ0⟨θ∗,xt,a∗
t−Xt⟩
 (37)
Since the index set has not changed in Step 1 (see Line 9 in Algorithm 2), the second term of the regret is
the same as in the original SupLinUCB algorithm of Chu et al. (2011). For the first term, we partitioned it
by the following events:
Bt:=/intersectiondisplay
t∈[T],s∈[logT],a∈[K]/braceleftbigg
|⟨θ∗−ˆθs
t,xt,a⟩|≤α+ 1
αws
t,a/bracerightbigg
,and
Dt:=/braceleftig
ˆ∆st
t,At≥ε/bracerightig
,
whereα=/radicalig
1
2ln2TK
γas in the SupLinUCB (Chu et al., 2011). We choose γ=1
2t2throughout. Furthermore,
ˆθs
tis the ˆθtobtained from Algorithm 3 with Ψs
tas the input, i.e.,
ˆθs
t:=/parenleftbigg
Id+/summationdisplay
τ∈Ψs
txτ,AτxT
τ,Aτ/parenrightbigg−1/summationdisplay
τ∈Ψs
tYτ,Aτxτ,Aτ.
Define ∆t:=⟨θ∗,xt,a∗
t−Xt⟩as the instantaneous regret at each time step t. In addition, choose ε=∆t
3in
the definition of Dt. Then the first term of the expected regret in (37) can be partitioned by the events Bt
andDtas follows:
E/bracketleftigg/summationdisplay
t∈Ψ0⟨θ∗,xt,a∗
t−Xt⟩/bracketrightigg
=E/bracketleftigg/summationdisplay
t∈Ψ0∆t· 1{Bt,Dt}/bracketrightigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:F1+E/bracketleftigg/summationdisplay
t∈Ψ0∆t· 1/braceleftbig
Bt,Dt/bracerightbig/bracketrightigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:F2+E/bracketleftigg/summationdisplay
t∈Ψ0∆t· 1/braceleftbig
Bt/bracerightbig/bracketrightigg
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
=:F3
We recall that when t∈Ψ0,wst
t,a≤1√
Tfor alla∈ˆAst.
To boundF1, we note that since Btoccurs, the actual best arm a∗
t∈ˆAstwith high probability ( 1−γlog2T)
by Chu et al. (2011, Lemma 5). As such,
max
a∈ˆAst⟨ˆθst
t,xt,a⟩≥⟨ ˆθst
t,xt,a∗
t⟩≥⟨θ∗,xt,a∗
t⟩−α+ 1
αws
t,a∗
t≥⟨θ∗,xt,a∗
t⟩−2√
T
32Published in Transactions on Machine Learning Research (04/2024)
where the last inequality is from the fact that γ=1
2t2andα=/radicalig
1
2ln2TK
γ≥1.Else if, in fact, the best
arma∗
t/∈ˆAst, the corresponding regret in this case is bounded by:
E/summationdisplay
t∈Ψ0∆t· 1/braceleftig
a∗
t/∈ˆAst/bracerightig
≤ET/summationdisplay
t=1∆t· 1/braceleftig
a∗
t/∈ˆAst/bracerightig
≤ET/summationdisplay
t=11/braceleftig
a∗
t/∈ˆAst/bracerightig
=T/summationdisplay
t=1P(a∗
t/∈ˆAst)
≤T/summationdisplay
t=1γlog2T
=T/summationdisplay
t=1log2T
2t2
<π2
12log2T .
Case 1: IfˆA∗
t̸=At, this means that the index of AtisIt,At=(ˆ∆st
t,At)2
α2∥Xt∥2
V−1
t+ log1
α2∥Xt∥2
V−1
t. Using the fact
thatIt,At≤It,ˆA∗
twe have
(ˆ∆st
t,At)2
α2∥Xt∥2
V−1
t+ log1
α2∥Xt∥2
V−1
t≤2 logT .
Then using the definition of the event Dtand the fact that (wst
t,a)2=α2∥Xt∥2
V−1
t≤1
T, we have
∆2
t≤9α2∥Xt∥2
V−1
t−1logT≤9 logT
T.
Hence, ∆t≤3√
logT√
T. Therefore F1in this case is upper bounded as follows:
E/bracketleftigg/summationdisplay
t∈Ψ0∆t· 1{Bt,Dt}· 1/braceleftig
ˆA∗
t̸=At/bracerightig
· 1/braceleftig
a∗
t∈ˆAst/bracerightig/bracketrightigg
≤E/bracketleftigg/summationdisplay
t∈Ψ03√logT√
T/bracketrightigg
≤3/radicalbig
TlogT .
Case 2: IfˆA∗
t=At, then using the definition of the event Bt, we have
⟨ˆθst
t,Xt⟩= max
a∈ˆAst⟨ˆθst
t,xt,a⟩≥⟨ ˆθst
t,xt,a∗
t⟩≥⟨θ∗,xt,a∗
t⟩−2√
T=⟨θ∗,Xt⟩+ ∆t−2√
T
therefore since event Btoccurs,
∆t≤⟨ˆθst
t−θ∗,Xt⟩+2√
T≤3√
T.
HenceF1in this case is bounded as 2√
T. Combining the above cases,
F1≤3/radicalbig
TlogT+ 3√
T+π2
12log2T≤O(/radicalbig
TlogT).
33Published in Transactions on Machine Learning Research (04/2024)
To boundF2, we note from the definition of Btthat
max
a∈ˆAst⟨ˆθst
t,xt,a⟩≥⟨ ˆθst
t,xt,a∗
t⟩≥⟨θ∗,xt,a∗
t⟩−2√
T
then on the event Dt,
⟨θ∗,xt,a∗
t⟩−2√
T≤max
a∈ˆAst⟨ˆθst
t,xt,a⟩<ε+⟨ˆθst
t,Xt⟩=∆t
3+⟨ˆθst
t,Xt⟩,
therefore
∆t<3
2/parenleftbigg
⟨ˆθst
t−θ∗,Xt⟩+2√
T/parenrightbigg
≤9
2√
T
Hence
F2=E/bracketleftigg/summationdisplay
t∈Ψ0∆t· 1/braceleftbig
Bt,Dt/bracerightbig
· 1/braceleftig
a∗
t∈ˆAst/bracerightig/bracketrightigg
+E/bracketleftigg/summationdisplay
t∈Ψ0∆t· 1/braceleftbig
Bt,Dt/bracerightbig
· 1/braceleftig
a∗
t/∈ˆAst/bracerightig/bracketrightigg
≤E/bracketleftiggT/summationdisplay
t=1∆t· 1/braceleftbig
Bt,Dt/bracerightbig/bracketrightigg
+π2
12log2T
<E/bracketleftiggT/summationdisplay
t=19
2√
T· 1/braceleftbig
Bt,Dt/bracerightbig/bracketrightigg
+π2
12log2T
<T·9
2√
T+π2
12log2T
=9
2√
T+π2
12log2T
≤O(√
T).
To boundF3, we use the proof as in of Chu et al. (2011, Lemma 1), which is restated as follows.
Lemma 10. For anya∈[K],s∈[⌈logT⌉],t∈[T],
P/bracketleftbigg
|⟨θ∗−ˆθs
t,xt,a⟩|>α+ 1
αws
t,a/bracketrightbigg
≤γ
TK
whereα=/radicalig
1
2ln2TK
γ.
Then using the union bound, we have for all t∈[T],s∈[⌈logT⌉], for alla∈[K],
P/bracketleftbig
Bt/bracketrightbig
=P
/uniondisplay
t∈[T],s∈[⌈logT⌉],a∈[K]/braceleftbigg
|⟨θ∗−ˆθs
t,xt,a⟩|>α+ 1
αws
t,a/bracerightbigg

≤/summationdisplay
t∈[T]/summationdisplay
s∈[⌈logT⌉]/summationdisplay
a∈[K]P/bracketleftbigg
|⟨θ∗−ˆθs
t,xt,a⟩|>α+ 1
αws
t,a/bracketrightbigg
</parenleftbig
TK(1 + logT)/parenrightbigγ
TK
=γ(1 + logT).
34Published in Transactions on Machine Learning Research (04/2024)
With the choice γ=1
2t2and the assumption ∆t≤1,
F3=E/bracketleftigg/summationdisplay
t∈Ψ0∆t· 1/braceleftbig
Bt/bracerightbig/bracketrightigg
≤T/summationdisplay
t=1P/bracketleftbig
Bt/bracketrightbig
<T/summationdisplay
t=11 + logT
2t2
<π2
12(1 + logT)
≤O(logT).
Hence the first term in RTin (37) is upper bounded by:
E/bracketleftigg/summationdisplay
t∈Ψ0⟨θ∗,xt,a∗
t−Xt⟩/bracketrightigg
≤O(√
T) +O(logT) +O(/radicalbig
TlogT)
≤O(/radicalbig
TlogT).
On the other hand, by Chu et al. (2011, Theorem 1), the second term in RTin (37) is upper bounded as
follows:
E
/summationdisplay
t̸∈Ψ0⟨θ∗,xt,a∗
t−Xt⟩
≤O/parenleftig/radicalig
dTlog3(KT)/parenrightig
.
Hence the regret of our algorithm SupLinIMED is upper bounded as follows:
RT≤O/parenleftig/radicalig
dTlog3(KT)/parenrightig
.
This completes the proof of Theorem 4.
G Hyperparameter tuning in our empirical study
G.1 Synthetic Dataset
The below tables are the empirical results while tuning the hyperparameter α(scale of the confidence width)
for fixedT= 1000.
Method LinUCB LinTS LinIMED-1 LinIMED-2 LinIMED-3 (C= 30)
α0.50.550.60.20.25 0.30.15 0.20.250.20.250.30.15 0.20.25
Regret 7.7806.6956.8569.7699.20112.068 24.086 5.4826.1084.9994.9987.32925.588 2.0752.760
Table 2: Tuning αwhenK= 10,d= 2
Method LinUCB LinTS LinIMED-1 LinIMED-2 LinIMED-3 (C= 30)
α0.50.550.60.10.150.20.20.250.30.20.250.30.20.250.3
Regret 7.2036.8327.42354.221 7.0427.3526.7076.0538.4586.2544.9187.0134.4072.5623.041
Table 3: Tuning αwhenK= 100,d= 2
35Published in Transactions on Machine Learning Research (04/2024)
Method LinUCB LinTS LinIMED-1 LinIMED-2 LinIMED-3 (C= 30)
α0.50.550.60.10.150.20.15 0.20.250.20.250.30.15 0.20.25
Regret 7.9195.6797.06369.955 6.9257.03724.393 5.6256.3356.3354.8317.04041.355 1.9362.250
Table 4: Tuning αwhenK= 500,d= 2
Method LinUCB LinTS LinIMED-1 LinIMED-2 LinIMED-3 (C= 30)
α0.450.50.55 0.10.15 0.2 0.10.15 0.2 0.10.150.20.10.150.2
Regret 9.1649.09414.183 14.252 9.88614.680 19.663 6.46310.643 15.685 5.3998.3738.0242.0623.342
Table 5: Tuning αwhenK= 10,d= 20
Method LinUCB LinTS LinIMED-1 LinIMED-2 LinIMED-3 (C= 30)
α0.250.30.35 0.10.15 0.20.05 0.10.150.10.15 0.2 0.05 0.10.15
Regret 7.9237.08510.981 14.983 9.56519.300 58.278 6.1659.2258.9168.57513.483 142.704 2.8163.497
Table 6: Tuning αwhenK= 10,d= 50
We run these algorithms on the same dataset with different choices of α, we choose the best αwith the
corresponding least regret.
G.2 MovieLens Dataset
The below tables are the empirical results while tuning the hyperparameter α(scale of the confidence width)
for fixedT= 1000.
Method LinUCB LinTS LinIMED-1 LinIMED-2 LinIMED-3 (C= 30) IDS
α0.70.750.80.050.10.150.150.20.250.150.20.250.20.250.30.250.30.35
CTR 0.6080.6750.6680.6150.7050.6790.7400.8230.7660.7400.8230.7660.7130.7420.6900.6550.7280.714
Table 7: Tuning αwhenK= 20
Method LinUCB LinTS LinIMED-1 LinIMED-2 LinIMED-3 (C= 30) IDS
α0.750.80.85 00.050.10.10.150.20.050.10.150.050.10.150.30.350.4
CTR 0.7080.7540.7130.5170.7110.6460.6480.6680.5950.6580.6680.6510.6970.7170.6490.6430.6880.606
Table 8: Tuning αwhenK= 50
Method LinUCB LinTS LinIMED-1 LinIMED-2 LinIMED-3 (C= 30) IDS
α0.850.90.95 00.050.10.050.10.150.050.10.150.050.10.150.30.350.4
CTR 0.7210.7540.7450.4870.6740.5880.6820.7290.5940.6870.7290.5940.6890.7050.5940.6840.7390.695
Table 9: Tuning αwhenK= 100
We run these algorithms on the same dataset with different choices of αand we choose the best αwith the
corresponding largest reward.
36