Published in Transactions on Machine Learning Research (11/2024)
Risk Bounds for Mixture Density Estimation on Compact
Domains via the h-Lifted Kullback–Leibler Divergence
Mark Chiu Chong mark.chiuchong@gmail.com
School of Mathematics and Physics
The University of Queensland
St Lucia, QLD 4072, Australia
Hien Duy Nguyen hien@imi.kyushu-u.ac.jp
Institute of Mathematics for Industry
Kyushu University
Nishi Ward, Fukuoka 819-0395, Japan;
School of Computing, Engineering, and Mathematical Sciences
La Trobe University
Bundoora, VIC 3086, Australia;
TrungTin Nguyen trungtin.nguyen@uq.edu.au
School of Mathematics and Physics
The University of Queensland
St Lucia, QLD 4072, Australia
Reviewed on OpenReview: https: // openreview. net/ forum? id= lAKvQO4vHj
Abstract
We consider the problem of estimating probability density functions based on sample data,
using a finite mixture of densities from some component class. To this end, we introduce the
h-lifted Kullback–Leibler (KL) divergence as a generalization of the standard KL divergence
and a criterion for conducting risk minimization. Under a compact support assumption, we
prove anO(1/√n)bound on the expected estimation error when using the h-lifted KL
divergence, which extends the results of Rakhlin et al. (2005, ESAIM: Probability and
Statistics, Vol. 9) and Li & Barron (1999, Advances in Neural Information Processing
Systems, Vol. 12) to permit the risk bounding of density functions that are not strictly
positive. We develop a procedure for the computation of the corresponding maximum h-
lifted likelihood estimators ( h-MLLEs) using the Majorization-Maximization framework and
provide experimental results in support of our theoretical bounds.
1 Introduction
Let(Ω,A,P)be an abstract probability space and let X: Ω→Xbe a random variable taking values
in the measurable space (X,F), whereXis a compact metric space equipped with its Borel σ-algebra F.
Suppose that we observe an independent and identically distributed (i.i.d.) sample of random variables
Xn= (Xi)i∈[n], where [n] ={1,...,n}, and that each Xiarises from the same data generating process as
X, characterized by the probability measure F≪µon(X,F), with density function f= dF/dµ, for some
σ-finiteµ. In this work, we are concerned with the estimating fvia a data dependent double-index sequence
of estimators (fk,n)k,n∈N, where
fk,n∈Ck= cok(P) =

fk(·;ψk) =k/summationdisplay
j=1πjφ(·;θj)|φ(·;θj)∈P, πj≥0, j∈[k],k/summationdisplay
j=1πj= 1

,
1Published in Transactions on Machine Learning Research (11/2024)
for eachk,n∈N, and where
P=/braceleftbig
φ(·;θ) :X→R≥0|θ∈Θ⊂Rd/bracerightbig
, (1)
ψk= (π1,...,πk,θ1,...,θk), andd∈N. To ensure the measurability and existence of various optima, we
shall assume that φisCarathéodory in the sense that φ(·;θ)is(X,F)-measurable for each θ∈Θ, andφ(X;·)
is continuous for each X∈X.
In the definition above, we can identify the set Ck= cok(P)as the set of density functions that can be
written as a convex combination of kelements ofP, wherePis often called the space of component density
functions. We then interpret Ckas the class of k-component finite mixtures of densities of class P, as studied,
for example, by McLachlan & Peel (2004); Nguyen et al. (2020; 2022b).
1.1 Risk bounds for mixture density estimation
We are particularly interested in oracle bounds of the form
E{ℓ(f,fk,n)}−ℓ(f,C)≤ρ(k,n), (2)
where (p,q)∝⇕⊣√∫⊔≀→ℓ(p,q)∈R≥0is a loss function on pairs of density functions. We define the density-to-class
loss
ℓ(f,C) = inf
q∈Cℓ(f,q),C= cl/parenleftigg/uniondisplay
k∈Ncok(P)/parenrightigg
,
where cl(·)is the closure. Here, we identify (k,n)∝⇕⊣√∫⊔≀→ρ(k,n)as a characterization of the rate at which
the left-hand side of (2) converges to zero as kandnincrease. Our present work follows the research of
Li & Barron (1999), Rakhlin et al. (2005) and Klemelä (2007) (see also Klemelä 2009, Ch. 19). In Li &
Barron (1999) and Rakhlin et al. (2005), the authors consider the case where ℓ(p,q)is taken to be the
Kullback–Leibler (KL) divergence
KL (p||q) =/integraldisplay
plogp
qdµ
andfk,n=fk(·;ψk,n)is a maximum likelihood estimator (MLE), where
ψk,n∈arg max
ψk∈Sk×Θk1
nn/summationdisplay
i=1logfk(Xi;ψk),
is a function of Xn, withSkdenoting the probability simplex in Rk.
Under the assumption that f,fk≥a, for somea>0and eachk∈[n](i.e., strict positivity), Li & Barron
(1999) obtained the bound
E{KL (f||fk,n)}−KL (f||C)≤c11
k+c2klog (c3n)
n,
for constants c1,c2,c3>0, which was then improved by Rakhlin et al. (2005) who obtained the bound
E{KL (f||fk,n)}−KL (f||C)≤c11
k+c21√n,
for constants c1,c2>0(constants (cj)j∈Nare typically different between expressions).
Alternatively, Klemelä (2007) takes ℓ(p,q)to be the squared L2(µ)norm distance (i.e., the least-squares
loss):
ℓ(p,q) =∥p−q∥2
2,µ,
where∥p∥2
2,µ=/integraltext
X|p|2dµ, for eachp∈L2(µ), and choose fk,nas minimizers of the L2(µ)empirical risk,
i.e.,fk,n=fk(·;ψk,n), where
ψk,n∈arg min
ψk∈Sk×Θk−2
nn/summationdisplay
i=1fk(·;ψk) +∥fk(·;ψk)∥2
2,µ. (3)
2Published in Transactions on Machine Learning Research (11/2024)
Here, Klemelä (2007) establish the bound
E∥f−fk,n∥2
2,µ−inf
q∈C∥f−q∥2
2,µ≤c11
k+c21√n,
c1,c2>0, without the lower bound assumption on f,fkabove, even permitting Xto be unbounded. Via
the main results of Naito & Eguchi (2013), the bound above can be generalized to the U-divergences, which
includes the special L2(µ)norm distance as a special case.
On the one hand, the sequence of MLEs required for the results of Li & Barron (1999) and Rakhlin et al.
(2005)aretypicallycomputable, forexample, viatheusualexpectation–maximizationapproach(cf. McLach-
lan & Peel 2004, Ch. 2). This contrasts with the computation of least-squares density estimators of form
(3), which requires evaluations of the typically intractable integral expressions ∥fk(·;ψk)∥2
2. However, the
least-squares approach of Klemelä (2007) permits the analysis using families Pof usual interest, such as
normal distributions and beta distributions, the latter of which being compactly supported but having den-
sities that cannot be bounded away from zero without restrictions, and thus do not satisfy the regularity
conditions of Li & Barron (1999) and Rakhlin et al. (2005).
1.2 Main contributions
Weproposethefollowing h-liftedKLdivergence, asageneralizationofthestandardKLdivergencetoaddress
the computationally tractable estimation of density functions which do not satisfy the regularity conditions
of Li & Barron (1999) and Rakhlin et al. (2005). The use of the h-lifted KL divergence has the possibility
to advance theories based on the standard KL divergence in statistical machine learning. To this end, let
h:X→R≥0be a function in L1(µ), and define the h-lifted KL divergence by:
KLh(p||q) =/integraldisplay
X{p+h}logp+h
q+hdµ. (4)
In the sequel, we shall show that KLhis a Bregman divergence on the space of probability density functions,
as per Csiszár (1995).
Assume that his a probability density function, and let Yn= (Yi)i∈[n]be a an i.i.d. sample, independent of
Xn, where each Yi: Ω→Xis a random variable with probability measure on (X,F), characterized by the
densityhwith respect to µ. Then, for each kandn, letfk,nbe defined via the maximum h-lifted likelihood
estimator (h-MLLE; see Appendix B for further discussion) fk,n=fk(·;ψk,n), where
ψk,n∈arg max
ψk∈Sk×Θk1
nn/summationdisplay
i=1(log{fk(Xi;ψk) +h(Xi)}+ log{fk(Yi;ψk) +h(Yi)}). (5)
The primary aim of this work is to show that
E{KLh(f||fk,n)}−KLh(f||C)≤c11
k+c21√n(6)
for some constants c1,c2>0, without requiring the strict positivity assumption that f,fk≥a>0.
This result is a compromise between the works of Li & Barron (1999) and Rakhlin et al. (2005), and Klemelä
(2007), as it applies to a broader space of component densities P, and because the required h-MLLEs (5) can
be efficiently computed via minorization–maximization (MM) algorithms (see e.g., Lange 2016). We shall
discuss this assertion in Section 4.
1.3 Relevant literature
Our work largely follows the approach of Li & Barron (1999), which was extended upon by Rakhlin et al.
(2005) and Klemelä (2007). All three texts use approaches based on the availability of greedy algorithms
for maximizing convex functions with convex functional domains. In this work, we shall make use of the
3Published in Transactions on Machine Learning Research (11/2024)
proof techniques of Zhang (2003). Related results in this direction can be found in DeVore & Temlyakov
(2016) and Temlyakov (2016). Making the same boundedness assumption as Rakhlin et al. (2005), Dalalyan
& Sebbar (2018) obtain refined oracle inequalities under the additional assumption that the class Pis finite.
Numerical implementations of greedy algorithms for estimating finite mixtures of Gaussian densities were
studied by Vlassis & Likas (2002) and Verbeek et al. (2003).
Theh-MLLE as an optimization objective can be compared to other similar modified likelihood estimators,
such as the Lqlikelihood of Ferrari & Yang (2010) and Qin & Priebe (2013), the β-likelihood of Basu
et al. (1998) and Fujisawa & Eguchi (2006), penalized likelihood estimators, such as maximum a posteriori
estimators of Bayesian models, or f-separable Bregman distortion measures of Kobayashi & Watanabe (2024;
2021).
The practical computation of the h-MLLEs, (5), is made possible via the MM algorithm framework of Lange
(2016), see also Hunter & Lange (2004), Wu & Lange (2010), and Nguyen (2017) for further details. Such
algorithms have well-studied global convergence properties and can be modified for mini-batch and stochastic
settings (see, e.g., Razaviyayn et al., 2013 and Nguyen et al., 2022a).
A related and popular setting of investigations is that of model selection, where the objects of interest are
single-index sequences (fkn,n)n∈N, and where the aim is to obtain finite-sample bounds for losses of the form
ℓ(fkn,n,f), where each kn∈Nis a data dependent function, often obtained by optimizing some penalized
loss criterion, as described in Massart (2007), Koltchinskii (2011, Ch. 6), and Giraud (2021, Ch. 2). In the
context of finite mixtures, examples of such analyses can be found in the works of Maugis & Michel (2011)
and Maugis-Rabusseau & Michel (2013). A comprehensive bibliography of model selection results for finite
mixtures and related statistical models can be found in Nguyen et al. (2022c).
1.4 Organization of paper
The manuscript is organized as follows. In Section 2, we formally define the h-lifted KL divergence as a
Bregman divergence and establish several of its properties. In Section 3, we present new risk bounds for the
h-lifted KL divergence of the form (2). In Section 4, we discuss the computation of the h-lifted likelihood
estimator in the form of (5), followed by empirical results illustrating the convergence of (2) with respect
to bothkandn. Additional insights and technical results are provided in the Appendices at the end of the
manuscript.
2 The h-lifted KL divergence and its properties
In this section we formally define the h-lifted KL divergence on the space of density functions and establish
some of its properties.
Definition 1 (h-lifted KL divergence) .Letf,g, andhbe probability density functions on the space X, where
h>0. Theh-liftedKLdivergence from gtofis defined as follows:
KLh(f||g) =/integraldisplay
X{f+h}logf+h
g+hdµ=Ef/braceleftbigg
logf+h
g+h/bracerightbigg
+Eh/braceleftbigg
logf+h
g+h/bracerightbigg
.
2.1KLhas a Bregman divergence
Letϕ:I→R,I= (0,∞)be a strictly convex function that is continuously differentiable. The Bregman
divergence between scalars dϕ:I×I→ R≥0generated by the function ϕis given by:
dϕ(p,q) =ϕ(p)−ϕ(q)−ϕ′(q)(p−q),
whereϕ′(q)denotes the derivative of ϕatq.
Bregman divergences possess several useful properties, including the following list:
1. Non-negativity: dϕ(p,q)≥0for allp,q∈Iwith equality if and only if p=q;
4Published in Transactions on Machine Learning Research (11/2024)
2. Asymmetry: dϕ(p,q)̸=dϕ(q,p)in general;
3. Convexity: dϕ(p,q)is convex in pfor every fixed q∈I.
4. Linearity: dc1ϕ1+c2ϕ2(p,q) =c1dϕ1(p,q) +c2dϕ2(p,q)forc1,c2≥0.
The properties for Bregman divergences between scalars can be extended to density functions and other
functional spaces, as established in Frigyik et al. (2008) and Stummer & Vajda (2012), for example. We also
direct the interested reader to the works of Pardo (2006), Basu et al. (2011), and Amari (2016).
The class of h-lifted KL divergences constitute a generalization of the usual KL divergence and are a subset of
the Bregman divergences over the space of density functions that are considered by Csiszár (1995). Namely,
letPbe a convex set of probability densities with respect to the measure µonX. The Bregman divergence
Dϕ:P×P→ [0,∞)between densities p,q∈Pcan be constructed as follows:
Dϕ(p||q) =/integraldisplay
Xdϕ(p(x),q(x)) dµ(x).
Theh-lifted KL divergence KLhas a Bregman divergence is generated by the function ϕ(u) = (u+h) log(u+
h)−(u+h) + 1. This assertion is demonstrated in Appendix C.1.
2.2 Advantages of the h-lifted KL divergence
When the standard KL divergence is employed in the density estimation problem, it is common to restrict
consideration of density functions to those bounded away from zero by some positive constant. That is, one
typically considers the smaller class of so-called admissible target densitiesPα⊂P(cf. Meir & Zeevi, 1997),
where
Pα={φ(·;θ)∈P|φ(·;θ)≥α>0}.
Without this restriction, the standard KL divergence can be unbounded, even for functions with bounded
L1norms. For example, let pandqbe densities of beta distributions on the support X= [0,1]. That is,
suppose that p,q∈Pbeta, respectively characterized by parameters θp= (ap,bp)andθq= (aq,bq), where
Pbeta=/braceleftbigg
x∝⇕⊣√∫⊔≀→β(x;θ) =Γ (a+b)
Γ (a) Γ (b)xa−1(1−x)b−1,θ= (a,b)∈R2
>0/bracerightbigg
. (7)
Then, from Gil et al. (2013), the KL divergence between pandqis given by:
KL (p||q) = log/braceleftbiggΓ (aq) Γ (bq)
Γ (aq+bq)/bracerightbigg
−log/braceleftbiggΓ (ap) Γ (bp)
Γ (ap+bp)/bracerightbigg
+ (ap−aq){ψ(ap)−ψ(ap+bp)}+ (bp−bq){ψ(bp)−ψ(ap+bp)},
whereψ:R>0→Ris the digamma function. Next, suppose that ap=bqandaq=bp= 1, which leads to
the simplification
KL (p||q) = (ap−1){ψ(ap)−ψ(1)}.
Sinceψis strictly increasing, we observe that the right-hand side diverges as ap→∞. Thus, the KL
divergence between beta distributions is unbounded. The h-lifted KL divergence in contrast does not suffer
from this problem, and does not require the restriction to Pα. This allows us to consider cases where p,q∈P
are not bounded away from 0, as per the following result.
Proposition 2. LetPbe defined as in (1).KLh(f||g)is bounded for all continuous densities f,g∈P.
Proof.See Appendix C.2.
LetLp(f,g)denote the standard Lp-norm,Lp(f,g) =/braceleftbig/integraltext
X|f(x)−g(x)|pdµ(x)/bracerightbig1/p. As remarked previously,
Klemelä (2007) established empirical risk bounds in terms of the L2-norm distance. Following results from
Meir & Zeevi (1997) characterizing the relationship between the KL divergence in terms of the L2-norm
5Published in Transactions on Machine Learning Research (11/2024)
distance, in Proposition 3 we establish the corresponding relationship between the h-lifted KL divergence
and theL2-norm distance, along with a relationship between the h-lifted KL divergence and the L1-norm
distance.
Proposition 3. For probability density functions f, g,andh, wherehis such that h(x)≥γ > 0for all
x∈X, the following inequalities hold:
1
4L2
1(f,g)≤KLh(f||g)≤γ−1L2
2(f,g).
Proof.See Appendix C.3.
Remark 4. Proposition 2 highlights the benefit of the h-lifted KL divergence being bounded for all continuous
densities, unlike the standard KL divergence, while satisfying a relationship similar to that between the
KL divergence and the L2norm distance. Moreover, the first inequality of Proposition 3 is a Pinsker-like
relationship between the h-lifted KL divergence and the total variation distance TV(f,g) =1
2L1(f,g).
3 Main results
Here we provide explicit statements regarding the convergence rates claimed in (6) via Theorem 5 and
Corollary 6, which are proved in Appendix A.2. We assume that fis bounded above by some constant c
and that the lifting function his bounded above and below by constants aandb, respectively.
Theorem 5. Lethbe a positive density satisfying 0<a≤h(x)≤b, for allx∈X. For any target density
fsatisfying 0≤f(x)≤c, for allx∈Xand wherefk,nis the minimizer of KLhoverk-component mixtures,
the following inequality holds:
E{KLh(f||fk,n)}−KLh(f||C)≤u1
k+ 2+u2√n/integraldisplayc
0log1/2N(P,ε/2,∥·∥∞)dε+u3√n,
whereu1,u2, andu3are positive constants that depend on some or all of a,b, andc.
Corollary 6. LetXandΘbe compact and assume the following Lipschitz condition holds: for each x∈X,
and for each θ,τ∈Θ,
|φ(x;θ)−φ(x;τ)|≤Φ(x)∥θ−τ∥1, (8)
for some function Φ:X→R≥0, where∥Φ∥∞= supx∈X|Φ(x)|<∞. Then the bound in Theorem 5 becomes
E{KLh(f||fk,n)}−KLh(f||C)≤c1
k+ 2+c2√n,
wherec1andc2are positive constants.
Remark 7. Our results are applicable to any compact metric space X, with [0,1]used in the experimental
setup in Section 4.2 as a simple and tractable example to illustrate key aspects of our theory. There is
no issue in generalizing to X= [−m,m ]dform> 0andd∈N, or more abstractly, to any compact subset
X⊂Rd. Additionally,Xcould even be taken as a functional compact space, though establishing compactness
and constructing appropriate component classes Pover such spaces to achieve small approximation errors
KLh(f||C)is an approximation theoretic task that falls outside the scope of our work.
From the proof of Theorem 5 in Appendix A.2, it is clear that the dimensionality of Xonly influences our
bound through the complexity of the class P, specifically, the constant/integraltextc
0log1/2N(P,ε/2,∥·∥∞)dε, which
remains independent of both nandk. Here,N(P,ε,∥·∥)is theε-covering number of P. In fact, the
constant with respect to k(u1in Theorem 5) is entirely unaffected by the dimensionality of X. Thus, the
rates of our bound on the expected h-lifted KL divergence are dimension-independent and hold even when X
is infinite-dimensional, as long as there exists a class Psuch that/integraltextc
0log1/2N(P,ε/2,∥·∥∞)dεis finite.
Corollary 6 provides a method for obtaining such a bound when the elements of Psatisfy a Lipschitz condition.
6Published in Transactions on Machine Learning Research (11/2024)
4 Numerical experiments
Here we discuss the computability and computation of KLhestimation problems and provide empirical
evidence towards the rates obtained in Theorem 5. Specifically, we seek to develop a methodology for
computing h-MLLEs, and to use numerical experiments to demonstrate that the sequence of expected h-
lifted KL divergences between some density fand a sequence of k-component mixture densities from a
suitable classP, estimated using nobservations does indeed decrease at rates proportional to 1/kand1/√n,
askandnincrease.
The code for all simulations and analyses in Experiments 1 and 2 is available in both the Rand
Pythonprogramming languages. The code repository is available here: https://github.com/hiendn/
LiftedLikelihood .
4.1 Minorization–Maximization algorithm
One solution for computing (5) is to employ an MM algorithm. To do so, we first write the objective of (5)
as
Lh,n(ψk) =1
nn/summationdisplay
i=1
log

k/summationdisplay
j=1πjφ(Xi;θj) +h(Xi)

+ log

k/summationdisplay
j=1πjφ(Yi;θj) +h(Yi)


,
whereψk∈Ψk=Sk×Θk. We then require the definition of a minorizer QnforLh,non the space Ψk, where
Qn: Ψk×Ψk→Ris a function with the properties:
(i)Qn(ψk,ψk) =Lh,n(ψk), and
(ii)Qn(ψk,χk)≤Lh,n(ψk),
for eachψk,χk∈Ψk. In this context, given a fixed χk, the minorizer Qn(·,χk)should possess properties
that simplify it compared to the original objective Lh,n. These properties should make the minorizer more
tractable and might include features such as parametric separability, differentiability, convexity, among
others.
Inordertobuildanappropriateminorizerfor Lh,n, wemakeuseoftheso-calledJensen’sinequalityminorizer,
as detailed in Lange (2016, Sec. 4.3), applied to the logarithm function. This construction results in a
minorizer of the form
Qn(ψk,χk) =1
nn/summationdisplay
i=1k/summationdisplay
j=1{τj(Xi;χk) logπj+τj(Xi;χk) logφ(Xi;θj)}
+1
nn/summationdisplay
i=1k/summationdisplay
j=1{τj(Yi;χk) logπj+τj(Yi;χk) logφ(Yi;θj)}
+1
nn/summationdisplay
i=1{γ(Xi;χk) logh(Xi) +γ(Yi;χk) logh(Yi)}
−1
nn/summationdisplay
i=1k/summationdisplay
j=1{τj(Xi;χk) logτj(Xi;χk) +τj(Yi;χk) logτj(Yi;χk)}
−1
nn/summationdisplay
i=1{γ(Xi;χk) logγ(Xi;χk) +γ(Yi;χk) logγ(Yi;χk)}
where
γ(Xi;ψk) =h(Xi)/

k/summationdisplay
j=1πjφ(Xi;θj) +h(Xi)


7Published in Transactions on Machine Learning Research (11/2024)
and
τj(Xi;ψk) =πjφ(Xi;θj)/

k/summationdisplay
j=1πjφ(Xi;θj) +h(Xi)

.
Observe that Qn(·,χk)now takes the form of a sum-of-logarithms, as opposed to the more challenging
log-of-sum form of Lh,n. This change produces a functional separation of the elements of ψk.
UsingQn, we then define the MM algorithm via the parameter sequence/parenleftig
ψ(s)
k/parenrightig
s∈N, where
ψ(s)
k= arg max
ψk∈ΨkQn/parenleftig
ψk,ψ(s−1)
k/parenrightig
, (9)
for eachs > 0, and where ψ(0)
kis user chosen and is typically referred to as the initialization of the
algorithm. Notice that for each s, (9) is a simpler optimization problem than (5). Writing ψ(s)
k=/parenleftig
π(s)
1,...,π(s)
k,θ(s)
1,...,θ(s)
k/parenrightig
, we observe that (9) simplifies to the separated expressions:
π(s)
j=/summationtextn
i=1/braceleftig
τj/parenleftig
Xi;ψ(s−1)
k/parenrightig
+τj/parenleftig
Yi;ψ(s−1)
k/parenrightig/bracerightig
/summationtextn
i=1/summationtextk
l=1/braceleftig
τl/parenleftig
Xi;ψ(s−1)
k/parenrightig
+τl/parenleftig
Yi;ψ(s−1)
k/parenrightig/bracerightig
and
θ(s)
j= arg max
θj∈Θ1
nn/summationdisplay
i=1/braceleftig
τj/parenleftig
Xi;ψ(s−1)
k/parenrightig
logφ(Xi;θj) +τj/parenleftig
Yi;ψ(s−1)
k/parenrightig
logφ(Yi;θj)/bracerightig
,
for eachj∈[k].
A noteworthy property of the MM sequence/parenleftig
ψ(s)
k/parenrightig
s∈Nis that it generates an increasing sequence of objective
values, due to the chain of inequalities
Lh,n/parenleftig
ψ(s−1)
k/parenrightig
=Qn/parenleftig
ψ(s−1)
k,ψ(s−1)
k/parenrightig
≤Qn/parenleftig
ψ(s)
k,ψ(s−1)
k/parenrightig
≤Lh,n/parenleftig
ψ(s)
k/parenrightig
,
where the equality is due to property (i) of Qn, the first in equality is due to the definition of ψ(s)
k, and
the second inequality is due to property (ii) of Qn. This provides a kind of stability and regularity to the
sequence/parenleftig
Lh,n/parenleftig
ψ(s)
k/parenrightig/parenrightig
s∈N.
Of course, we can provide stronger guarantees under additional assumptions. Namely, assume that (iii)
Ψk⊂Ψk, whereΨkis an open set in a finite dimensional Euclidean space on which Lh,nandQn(·,χk)is
differentiable, for each χk∈Ψk. Then, under assumptions (i)–(iii) regarding Lh,nandQn, and due to the
compactness of Ψkand the continuity of QnonΨk×Ψk, Razaviyayn et al. (2013, Cor. 1) implies that/parenleftig
ψ(s)
k/parenrightig
s∈Nconverges to the set of stationary points of Lh,nin the sense that
lim
s→∞inf
ψ∗
k∈Ψ∗
k/vextenddouble/vextenddouble/vextenddoubleψ(s)
k−ψ∗
k/vextenddouble/vextenddouble/vextenddouble
2= 0,where Ψ∗
k=/braceleftigg
ψ∗
k∈Ψk:∂Lh,n
∂ψk/vextendsingle/vextendsingle/vextendsingle/vextendsingle
ψk=ψ∗
k= 0/bracerightigg
.
More concisely, we say that the sequence/parenleftig
ψ(s)
k/parenrightig
s∈Nglobally converges to the set of stationary points Ψ∗
k.
4.2 Experimental setup
Towards the task of demonstrating empirical evidence of the rates in Theorem 5, we consider the family of
beta distributions on the unit interval X= [0,1]as our base class (i.e., (7)) to estimate a pair of target
densities
f1(x) =1
2χ[0,2/5](x) +1
2χ[3/5,1](x),
8Published in Transactions on Machine Learning Research (11/2024)
and
f2(x) =χ[0,1](x)/braceleftigg
2−4xifx≤1/2,
−2 + 4xifx>1/2,
whereχAis the characteristic function that takes value 1ifx∈Aand0, otherwise. Note that neither f1
norf2are inC. In particular, f1(x) = 0whenx∈/parenleftbig2
5,3
5/parenrightbig
, andf2(x) = 0whenx= 1/2, and hence neither
densities are bounded away from 0, on X. Thus, the theory of Rakhlin et al. (2005) cannot be applied to
provide bounds for the expected KL divergence between MLEs of beta mixtures and the pair of targets. We
visualizef1andf2in Figure 1.
0.0 0.2 0.4 0.6 0.8 1.00.0 0.5 1.0 1.5 2.0
xdensity
Figure 1: Simulation target densities f1(solid line) and f2(dashed line).
To observe the rate of decrease of the h-lifted KL divergence between the targets and respective sequences
ofh-MLLEs, we conduct two experiments E1andE2. InE1, our target density is set to f1andh1=
β(·; 1/2,1/2). For eachn∈/braceleftbig
210,..., 215/bracerightbig
andk∈{2,..., 8}, we independently simulate XnandYnwith
eachXiandYi(i∈[n]), i.i.d., from the distributions characterized by f1andh1, respectively. In E2, we
targetf2withh-MLLEs over the same ranges of kandn, but withh2=β(·; 1,1)–the density of the uniform
distribution. For each kandn, we simulate XnandYn, respectively, from distributions characterized by f2
andh2.
In both experiments, we simulate r= 50replicates of each (k,n)-scenario and compute the corresponding
h-MLLEs, (fk,n,l)l∈[r], using the previously described MM algorithm. For each l∈[r], we compute the
corresponding negative log h-lifted likelihood between the target fandfk,n,l:
Kk,n,l=−/integraldisplay
X(f+h) log (fk,n,l+h) dµ
to assess the rates, and note that
KLh(f||fk,n,l) =/integraldisplay
X(f+h) log (f+h) dµ+Kk,n,l,
where the prior term is a constant with respect to kandn.
9Published in Transactions on Machine Learning Research (11/2024)
To analyze the sample of 7×6×50 = 2100 observations of relationship between the values (Kk,n,l)l∈[r]and
the corresponding values of kandn, we use non-linear least squares (Amemiya, 1985, Sec. 4.3) to fit the
regression relationship
E[Kk,n,l] =a0+a1
(k+ 2)b1+a2
nb2. (10)
We obtain 95%asymptotic confidence intervals for the estimates of the regression parameters a0,a1,a2,b1,
b2∈R, under the assumption of potential mis-specification of (10), by using the sandwich estimator for the
asymptotic covariance matrix (cf. White 1982).
4.3 Results
We report the estimates along with 95%asymptotic confidence intervals for the parameters of (10) for E1
andE2in Table 1. Plots of the average negative log h-lifted likelihood values by sample sizes nand numbers
of components kare provided in Figure 2.
Table 1: Estimates of parameters for fitted relationships (with 95%confidence intervals) between negative
logh-lifted likelihood values, sample size and number of mixture components for experiments E1andE2.
E1 a0 a1 a2 b1 b2
Est.−1.68 0 .73 6 .80 1 .87 0 .99
95% CI (−1.68,−1.67) (0.68,0.78) (1.24,12.36) (1.81,1.93) (0.87,1.11)
E2 a0 a1 a2 b1 b2
Est.−1.47 1 .49 6 .75 4 .36 1 .07
95% CI (−1.48,−1.47) (0.58,2.41) (2.17,11.32) (3.91,4.81) (0.97,1.16)
From Table 1, we observe that E[Kk,n,l]decreases with both nandkin both simulations, and that the rates
at which the averages decrease are faster than anticipated by Theorem 5, with respect to both nandk. We
can visually confirm the decreases in the estimate of E[Kk,n,l]via Figure 2. In both E1andE2, the rate
of decrease over the assessed range of nis approximately proportional to 1/n, as opposed to the anticipated
rate of 1/√n, whereas the rate of decrease in kis far larger, at approximately 1/k1.87forE1and1/k4.36for
E2.
These observations provide empirical evidence towards the fact that the rate of decrease of E[Kk,n,l]is at
least 1/kand1/√n, respectively, for kandn, at least over the simulation scenarios. These fast rates of fit
over small values of nandkmay be indicative of a diminishing returns of fit phenomenon, as discussed in
Cadez & Smyth (2000) or the so-called elbow phenomenon (see, e.g., Ritter 2014, Sec. 4.2), whereupon the
rate of decrease in average loss for small values of kis fast and becomes slower as kincreases, converging
to some asymptotic rate. This is also the reason why we do not include the outcomes when k= 1, as the
drop in E[Kk,n,l]betweenk= 1andk= 2is so dramatic that it makes our simulated data ill-fitted by any
model of form (10). As such, we do not view Theorem 5 as being pessimistic in light of these phenomena,
as it applies uniformly over all values of kandn.
10Published in Transactions on Machine Learning Research (11/2024)
1024 2048 4096 8192 16384 327688
7
6
5
4
3
2-1.658 -1.664 -1.666 -1.667 -1.668 -1.668
-1.658 -1.662 -1.664 -1.665 -1.666 -1.666
-1.656 -1.660 -1.662 -1.663 -1.664 -1.664
-1.652 -1.656 -1.658 -1.659 -1.659 -1.659
-1.648 -1.651 -1.653 -1.653 -1.654 -1.654
-1.633 -1.636 -1.637 -1.638 -1.639 -1.639
-1.620 -1.621 -1.622 -1.622 -1.623 -1.623E1
1024 2048 4096 8192 16384 327688
7
6
5
4
3
2-1.466 -1.469 -1.470 -1.471 -1.471 -1.471
-1.466 -1.469 -1.470 -1.471 -1.471 -1.471
-1.466 -1.469 -1.470 -1.471 -1.471 -1.471
-1.466 -1.469 -1.470 -1.470 -1.471 -1.471
-1.466 -1.469 -1.470 -1.470 -1.471 -1.471
-1.467 -1.468 -1.469 -1.469 -1.469 -1.469
-1.465 -1.466 -1.466 -1.467 -1.467 -1.467E2
1.66
1.65
1.64
1.63
1.62
             average
1.471
1.470
1.469
1.468
1.467
1.466
1.465
             average
Sample size nNum. components k
Figure 2: Average negative log h-lifted likelihood values by sample sizes nand numbers of components kfor
experiments E1andE2.
11Published in Transactions on Machine Learning Research (11/2024)
5 Conclusion
The estimation of probability densities using finite mixtures from some base class Pappears often in machine
learning and statistical inference as a natural method for modelling underlying data generating processes.
In this work, we pursue novel generalization bounds for such mixture estimators. To this end, we introduce
the family of h-lifted KL divergences for densities on compact supports, within the family of Bregman
divergences, which correspond to risk functions that can be bounded, even when densities in the class Pare
not bounded away from zero, unlike the standard KL divergence.
Unlike the least-squares loss, the corresponding maximum h-likelihood estimation problem can be computed
via an MM algorithm, mirroring the availability of EM algorithms for the maximum likelihood problem
corresponding to the KL divergence. Along with our derivations of generalization bounds that achieve the
same rates as the best-known bounds for the KL divergence and least square loss, we also provide numerical
evidence towards the correctness of these bounds in the case when Pcorresponds to beta densities.
Aside from beta distributions, mixture densities on compact supports that can be analysed under our frame-
work appear frequently in the literature. For supports on compact Euclidean subset, examples include
mixtures of Dirichlet distributions (Fan et al., 2012) and bivariate binomial distributions (Papageorgiou &
David, 1994). Alternatively, one can consider distributions on compact Euclidean manifolds, such as mix-
tures of Kent (Peel et al., 2001) distributions and von Mises–Fisher distributions (Banerjee et al., 2005,
Ng & Kwong, 2022). We defer investigating the practical performance of the maximum h-lifted likelihood
estimators and accompanying theory for such models to future work.
Acknowledgments
We express sincere gratitude to the Reviewers and Action Editor for their valuable feedback, which has
helped to improve the quality of this paper. Hien Duy Nguyen and TrungTin Nguyen acknowledge funding
from the Australian Research Council grant DP230100905.
References
Shun-ichi Amari. Information Geometry and Its Applications , volume 194. Springer, New York, 2016.
Takeshi Amemiya. Advanced econometrics . Harvard University Press, 1985.
Arindam Banerjee, Inderjit S Dhillon, Joydeep Ghosh, Suvrit Sra, and Greg Ridgeway. Clustering on the
unit hypersphere using von Mises-Fisher distributions. Journal of Machine Learning Research , 6(9), 2005.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural
results.Journal of Machine Learning Research , 3(Nov):463–482, 2002.
Ayanendranath Basu, Ian R Harris, Nils L Hjort, and MC Jones. Robust and efficient estimation by min-
imising a density power divergence. Biometrika , 85(3):549–559, 1998.
Ayanendranath Basu, Hiroyuki Shioya, and Chanseok Park. Statistical Inference: The Minimum Distance
Approach . CRC press, Boca Raton, 2011.
Igor Cadez and Padhraic Smyth. Model complexity, goodness of fit and diminishing returns. Advances in
Neural Information Processing Systems , 13, 2000.
Imre Csiszár. Generalized projections for non-negative functions. In Proceedings of 1995 IEEE International
Symposium on Information Theory , pp. 6. IEEE, 1995.
Arnak S Dalalyan and Mehdi Sebbar. Optimal kullback–leibler aggregation in mixture density estimation
by maximum likelihood. Mathematical Statistics and Learning , 1(1):1–35, 2018.
Ronald A DeVore and Vladimir N Temlyakov. Convex optimization on banach spaces. Foundations of
Computational Mathematics , 16(2):369–394, 2016.
12Published in Transactions on Machine Learning Research (11/2024)
Luc Devroye and László Györfi. Nonparametric Density Estimation: The L1 View . Wiley Interscience Series
in Discrete Mathematics. Wiley, 1985.
Luc Devroye and Gabor Lugosi. Combinatorial Methods in Density Estimation . Springer Science & Business
Media, 2001.
Wentao Fan, Nizar Bouguila, and Djemel Ziou. Variational learning for finite dirichlet mixture models and
applications. IEEE transactions on neural networks and learning systems , 23:762–774, 2012.
Davide Ferrari and Yuhong Yang. Maximum lq-likelihood method. Annals of Statistics , 38:573–583, 2010.
Béla A Frigyik, Santosh Srivastava, and Maya R Gupta. Functional bregman divergence and bayesian
estimation of distributions. IEEE Transactions on Information Theory , 54(11):5130–5139, 2008.
Hironori Fujisawa and Shinto Eguchi. Robust estimation in the normal mixture model. Journal of Statistical
Planning and Inference , 136(11):3989–4011, 2006.
Subhashis Ghosal. Convergence rates for density estimation with Bernstein polynomials. The Annals of
Statistics , 29(5):1264 – 1280, 2001. Publisher: Institute of Mathematical Statistics.
Manuel Gil, Fady Alajaji, and Tamas Linder. Rényi divergence measures for commonly used univariate
continuous distributions. Information Sciences , 249:124–131, 2013.
Christophe Giraud. Introduction to high-dimensional statistics . CRC Press, 2021.
Uffe Haagerup. The best constants in the Khintchine inequality. Studia Mathematica , 70(3):231–283, 1981.
David R Hunter and Kenneth Lange. A tutorial on mm algorithms. The American Statistician , 58(1):30–37,
2004.
Jussi S Klemelä. Density estimation with stagewise optimization of the empirical risk. Machine Learning ,
67:169–195, 2007.
Jussi S Klemelä. Smoothing of multivariate data: density estimation and visualization . John Wiley & Sons,
New York, 2009.
Masahiro Kobayashi and Kazuho Watanabe. Generalized Dirichlet-process-means for f-separable distortion
measures. Neurocomputing , 458:667–689, 2021.
Masahiro Kobayashi and Kazuho Watanabe. Unbiased Estimating Equation and Latent Bias under f-
Separable Bregman Distortion Measures. IEEE Transactions on Information Theory , 2024.
Vladimir Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems:
École D’Été de Probabilités de Saint-Flour XXXVIII-2008 . Lecture Notes in Mathematics. Springer, 2011.
Vladimir Koltchinskii and Dmitry Panchenko. Rademacher processes and bounding the risk of function
learning. arXiv: Probability , pp. 443–457, 2004.
Michael R. Kosorok. Introduction to Empirical Processes and Semiparametric Inference . Springer Series in
Statistics. Springer New York, 2007.
Kenneth Lange. Optimization , volume 95. Springer Science & Business Media, New York, 2013.
Kenneth Lange. MM optimization algorithms . SIAM, Philadelphia, 2016.
Jonathan Li and Andrew Barron. Mixture Density Estimation. In S. Solla, T. Leen, and K. Müller (eds.),
Advances in Neural Information Processing Systems , volume 12. MIT Press, 1999.
Pascal Massart. Concentration inequalities and model selection: Ecole d’Eté de Probabilités de Saint-Flour
XXXIII-2003 . Springer, 2007.
13Published in Transactions on Machine Learning Research (11/2024)
Cathy Maugis and Bertrand Michel. A non asymptotic penalized criterion for gaussian mixture model
selection. ESAIM: Probability and Statistics , 15:41–68, 2011.
Cathy Maugis-Rabusseau and Bertrand Michel. Adaptive density estimation for clustering with gaussian
mixtures. ESAIM: Probability and Statistics , 17:698–724, 2013.
Colin McDiarmid. On the method of bounded differences. In J.Editor Siemons (ed.), Surveys in Combi-
natorics, 1989: Invited Papers at the Twelfth British Combinatorial Conference , London Mathematical
Society Lecture Note Series, pp. 148–188. Cambridge University Press, 1989.
Colin McDiarmid. Concentration. In Michel Habib, Colin McDiarmid, Jorge Ramirez-Alfonsin, and Bruce
Reed (eds.), Probabilistic Methods for Algorithmic Discrete Mathematics , pp. 195–248. Springer Berlin
Heidelberg, Berlin, Heidelberg, 1998.
Geoffrey J McLachlan and David Peel. Finite Mixture Models . John Wiley & Sons, 2004.
Ronny Meir and Assaf Zeevi. Density estimation through convex combinations of densities: Approximation
and estimation bounds. Neural Networks , 10:99–109, 02 1997.
Kanta Naito and Shinto Eguchi. Density estimation with minimization of U-divergence. Machine Learning ,
90(1):29–57, January 2013.
Tin Lok James Ng and Kwok-Kun Kwong. Universal approximation on the hypersphere. Communications
in Statistics-Theory and Methods , 51:8694–8704, 2022.
HienDNguyen. Anintroductiontomajorization-minimizationalgorithmsformachinelearningandstatistical
estimation. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery , 7(2):e1198, 2017.
Hien D Nguyen, TrungTin Nguyen, Faicel Chamroukhi, and Geoffrey J McLachlan. Approximations of
conditional probability density functions in Lebesgue spaces via mixture of experts models. Journal of
Statistical Distributions and Applications , 8(1):13, August 2021.
Hien D Nguyen, Florence Forbes, Gersende Fort, and Olivier Cappé. An online minorization-maximization
algorithm. In Proceedings of the 17th Conference of the International Federation of Classification Societies ,
2022a.
TrungTin Nguyen, Hien D Nguyen, Faicel Chamroukhi, and Geoffrey J McLachlan. Approximation by finite
mixtures of continuous density functions that vanish at infinity. Cogent Mathematics & Statistics , 7:
1750861, 2020.
TrungTin Nguyen, Faicel Chamroukhi, Hien D Nguyen, and Geoffrey J McLachlan. Approximation of
probability density functions via location-scale finite mixtures in Lebesgue spaces. Communications in
Statistics - Theory and Methods , pp. 1–12, 2022b.
TrungTin Nguyen, Hien D Nguyen, Faicel Chamroukhi, and Florence Forbes. A non-asymptotic approach
for model selection via penalization in high-dimensional mixture of experts models. Electronic Journal of
Statistics , 16(2):4742–4822, 2022c.
Haris Papageorgiou and Katerina M David. On countable mixtures of bivariate binomial distributions.
Biometrical journal , 36(5):581–601, 1994.
Leandro Pardo. Statistical Inference Based on Divergence Measures . CRC Press, Boca Raton, 2006.
David Peel, William J Whiten, and Geoffrey J McLachlan. Fitting mixtures of kent distributions to aid in
joint set identification. Journal of the American Statistical Association , 96:56–63, 2001.
Sonia Petrone. Random Bernstein Polynomials. Scandinavian Journal of Statistics , 26(3):373–393, 1999.
Sonia Petrone and Larry Wasserman. Consistency of Bernstein Polynomial Posteriors. Journal of the Royal
Statistical Society Series B: Statistical Methodology , 64(1):79–100, March 2002.
14Published in Transactions on Machine Learning Research (11/2024)
Yichen Qin and Carey E Priebe. Maximum Lq-likelihood estimation via the expectation-maximization
algorithm: a robust estimation of mixture models. Journal of the American Statistical Association , 108
(503):914–928, 2013.
Alexander Rakhlin, Dmitry Panchenko, and Sayan Mukherjee. Risk bounds for mixture density estimation.
ESAIM: PS , 9:220–229, 2005.
Meisam Razaviyayn, Mingyi Hong, and Zhi-Quan Luo. A Unified Convergence Analysis of Block Successive
Minimization Methods for Nonsmooth Optimization. SIAM Journal on Optimization , 23(2):1126–1153,
2013.
Gunter Ritter. Robust cluster analysis and variable selection . CRC Press, Boca Raton, 2014.
Ralph Tyrrell Rockafellar. Convex analysis , volume 11. Princeton University Press, Princeton, 1997.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: from Theory to Algorithms .
Cambridge University Press, Cambridge, 2014.
Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on Stochastic Programming:
Modeling and Theory, Third Edition . Society for Industrial and Applied Mathematics, Philadelphia, PA,
July 2021.
Wolfgang Stummer and Igor Vajda. On bregman distances and divergences of probability measures. IEEE
Transactions on Information Theory , 58(3):1277–1288, 2012.
Rolf Sundberg. Statistical Modelling by Exponential Families . Cambridge University Press, Cambridge, 2019.
Vladimir N Temlyakov. Convergence and rate of convergence of some greedy algorithms in convex optimiza-
tion.Proceedings of the Steklov Institute of Mathematics , 293:325–337, 2016.
Sara van de Geer. Estimation and Testing Under Sparsity: École d’Été de Probabilités de Saint-Flour XLV
– 2015. Springer, 2016.
Aad W van der Vaart and Jon A Wellner. Weak Convergence and Empirical Processes: With Applications
to Statistics . Springer, 1996.
Jakob J Verbeek, Nikos Vlassis, and Ben Kröse. Efficient greedy learning of Gaussian mixture models. Neural
computation , 15(2):469–485, 2003.
Nikos Vlassis and Aristidis Likas. A greedy EM algorithm for gaussian mixture learning. Neural Processing
Letters, 15:77–87, 2002.
Halbert White. Maximum likelihood estimation of misspecified models. Econometrica , 50(1):1–25, 1982.
Tong Tong Wu and Kenneth Lange. The MM alternative to EM. Statistical Science , 25(4):492–505, 2010.
Tong Zhang. Sequential greedy approximation for certain convex optimization problems. IEEE Transactions
on Information Theory , 49(3):682–691, 2003.
15Published in Transactions on Machine Learning Research (11/2024)
A Proofs of main results
The following section is devoted to establishing some technical definitions and instrumental results which
are used to prove Theorem 5 and Corollary 6, and also includes the proofs of these results themselves.
A.1 Preliminaries
Recall that we are interested in bounds of the form (2). Note that Pis a subset of the linear space
V= cl
/uniondisplay
k∈N

k/summationdisplay
j=1ϖjφ(·;θj)|φ(·;θj)∈P,ϖj∈R,j∈[k]


,
and hence we can apply the following result, paraphrased from Zhang (2003, Thm. II.1).
Lemma 8. Letκbe a differentiable and convex function on V, and let/parenleftbig¯fk/parenrightbig
k∈Nbe a sequence of functions
obtained by Algorithm 1. If
sup
p,q∈C,π∈(0,1)d2
dπ2κ((1−π)p+πq)≤M<∞,
then, for each k∈N,
κ/parenleftbig¯fk/parenrightbig
−inf
p∈Cκ(p)≤2M
k+ 2.
Algorithm 1 Algorithm for computing a greedy approximation sequence.
Require: ¯f0∈P
1:fork∈Ndo
2:Compute/parenleftbig
¯πk,¯θk/parenrightbig
= arg min
(π,θ)∈[0,1]×Θκ/parenleftbig
(1−π)¯fk−1+πφ(·;θ)/parenrightbig
3:Define ¯fk= (1−¯πk)¯fk−1+ ¯πkφ/parenleftbig
·;¯θk/parenrightbig
4:end for
We are interested in two choices for κ:
κ(p) = KLh(f||p) (11)
and its sample counterpart,
κn(p) =1
nn/summationdisplay
i=1logf(Xi) +h(Xi)
p(Xi) +h(Xi)+1
nn/summationdisplay
i=1logf(Yi) +h(Yi)
p(Yi) +h(Yi), (12)
where (Xi)i∈[n]and(Yi)i∈[n]are realisations of XandY, respectively. We obtain the following important
results.
Proposition 9. Letκdenote either κ, the KLhdivergence (11), orκn, the sample KLhdivergence (12),
and assume that h≥aandφ(·;θ)≤c, for eachθ∈Θ. Then,
κ/parenleftbig¯fk/parenrightbig
−inf
p∈Cκ(p)≤4a−2c2
k+ 2,
for eachk∈N, where/parenleftbig¯fk/parenrightbig
k∈Nis obtained as per Algorithm 1.
Proof.See Appendix C.4.
Notice that sequences/parenleftbig¯fk/parenrightbig
k∈Nobtained via Algorithm 1 are greedy approximation sequences, and that
¯fk∈Ck, for eachk∈N. Let (fk)k∈Nbe the sequence of minimizers defined by
fk= arg min
ψk∈Sk×ΘkKLh(f||fk(·;ψk)), (13)
16Published in Transactions on Machine Learning Research (11/2024)
and let (fk,n)k∈Nbe the sequence of h-MLLEs, as per (5). Then, by definition, we have the fact that
κ(fk)≤κ/parenleftbig¯fk/parenrightbig
andκ(fk,n)≤κ/parenleftbig¯fk/parenrightbig
, forκset as (11) or (12), respectively. Thus, we have the following
result.
Proposition 10. For the KLhdivergence (11), under the assumption that h≥aandφ(·;θ)≤c, for each
θ∈Θ, we have
κ(fk)−inf
p∈Cκ(p)≤4a−2c2
k+ 2(14)
for eachk∈N, where (fk)k∈Nis the sequence of minimizers defined via (13). Furthermore, for the sample
KLhdivergence (12), under the same assumptions as above, we have
κn(fk,n)−inf
p∈Cκn(p)≤4a−2c2
k+ 2, (15)
for eachk∈N, where (fk,n)k∈Nareh-MLLEs defined via (5).
As is common in many statistical learning/uniform convergence results (e.g., Bartlett & Mendelson, 2002,
Koltchinskii & Panchenko, 2004), we employ the use of Rademacher processes and associated bounds. Let
(εi)i∈[n]be i.i.d. Rademacher random variables, that is P(εi=−1) =P(εi= 1) = 1/2, that are independent
of(Xi)i∈[n]. The Rademacher process, indexed by a class of real measurable functions S, is defined as the
quantity
Rn(s) =1
nn/summationdisplay
i=1s(Xi)εi,
fors∈S. The Rademacher complexity of the class Sis given byRn(S) =Esups∈S|Rn(s)|.
In the subsequent section, we make use of the following result regarding the supremum of convex functions:
Lemma 11 (Rockafellar, 1997, Thm. 32.2) .Letηbe a convex function on a linear space T, and letS⊂T
be an arbitrary subset. Then,
sup
p∈Sη(p) = sup
p∈co(S)η(p).
In particular, we use the fact that since a linear functional of convex combinations achieves its maximum
value at vertices, the Rademacher complexity of Sis equal to the Rademacher complexity of co(S)(see
Lemma 21). We consequently obtain the following result.
Lemma12. Let(εi)i∈[n]be i.i.d. Rademacher random variables, independent of (Xi)i∈[n]andPbe defined as
above. The setsCandPwill have equal complexity, Rn(C) =Rn(P), and the supremum of the Rademacher
process indexed by Cis equal to the supremum on the basis functions of P:
Eεsup
g∈C/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1g(Xi)εi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=Eεsup
θ∈Θ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1φ(Xi;θ)εi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
Proof.Follows immediately from Lemma 11.
A.2 Proofs
We first present a result establishing a uniform concentration bound for the h-lifted log-likelihood ratios,
which is instrumental in the proof of Theorem 5. Our proofs broadly follow the structure of Rakhlin et al.
(2005), modified as needed for the use of KLh.
Assume that 0≤φ(·;θ)<cfor somec∈R>0. For brevity, we adopt the notation: ∥T(g)∥C= supg∈C|T(g)|.
Theorem 13. LetX1,...,Xnbe an i.i.d. sample of size ndrawn from a fixed density fsuch that 0≤
f(x)≤cfor allx∈X, and lethbe a positive density with 0<a≤h(x)≤bfor allx∈X. Then, for each
t>0, with probability at least 1−e−t,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1logg(Xi) +h(Xi)
f(Xi) +h(Xi)−Eflogg+h
f+h/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
C≤w1√nE/integraldisplayc
0log1/2N(P,ε,dn,x)dε+w2√n+w3/radicalbigg
t
n,
17Published in Transactions on Machine Learning Research (11/2024)
wherew1,w2, andw3are constants that each depend on some or all of a,b, andc, andN(P,ε,dn,x)is the
ε-covering number of Pwith respect to the following empirical L2metric
d2
n,x(φ1,φ2) =1
nn/summationdisplay
i=1(φ1(Xi)−φ2(Xi))2.
Remark 14. The bound on the term
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1logg(Yi) +h(Yi)
f(Yi) +h(Yi)−Ehlogg+h
f+h/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
C
is the same as the above, except where the empirical distance dn,xis replaced by dn,y, defined in the same
way asdn,xbut withYireplacingXi.
Proof of Theorem 13. Fixhand define the following quantities: ˜g=g+h,˜f=f+h,˜C=C+h,
mi= log˜g(Xi)
˜f(Xi), m′
i= log˜g(X′
i)
˜f(X′
i), Z (x1,...,xn) =/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1log˜g(Xi)
˜f(Xi)−Elog˜g
˜f/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C.
We first apply McDiarmid’s inequality (Lemma 23) to the random variable Z. The bound on the martingale
difference is given by
|Z(X1,...,Xi,...,Xn)−Z(X1,...,X′
i,...,Xn)|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextenddouble/vextenddouble/vextenddouble/vextenddoubleElog˜g
˜f−1
n(m1+...+mi+...+mn)/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C
−/vextenddouble/vextenddouble/vextenddouble/vextenddoubleElog˜g
˜f−1
n(m1+...+m′
i+...+mn)/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤1
n/vextenddouble/vextenddouble/vextenddouble/vextenddoublelog˜g(X′
i)
˜f(X′
i)−log˜g(Xi)
˜f(Xi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
˜C
≤1
n/parenleftbigg
logc+b
a−loga
c+b/parenrightbigg
=1
n2 logc+b
a=ci.
The chain of inequalities holds because of the triangle inequality and the properties of the supremum. By
Lemma 23, we have
P(Z−EZ >ε )≤exp/braceleftigg
−nε2
(√
2 logc+b
a)2/bracerightigg
,
so
P(Z≤ε+EZ)≥1−exp/braceleftigg
−nε2
(√
2 logc+b
a)2/bracerightigg
,
where it follows from t=nε2/(√
2 logc+b
a)2thatε=√
2 log/parenleftbigc+b
a/parenrightbig/radicalig
t
n. Therefore with probability at least
1−e−t,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1log˜g(Xi)
˜f(Xi)−Eflog˜g
˜f/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C≤E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1log˜g(Xi)
˜f(Xi)−Eflog˜g
˜f/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C+√
2 log/parenleftbiggc+b
a/parenrightbigg/radicalbigg
t
n.
Let(εi)i∈[n]be i.i.d. Rademacher random variables, independent of (Xi)i∈[n]. By Lemma 24,
E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1log˜g(Xi)
˜f(Xi)−Eflog˜g
˜f/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C≤2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1log˜g(Xi)
˜f(Xi)εi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C.
By combining the results above, the following inequality holds with probability at least 1−e−t
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1log˜g(Xi)
˜f(Xi)−Eflog˜g
˜f/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C≤2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1log˜g(Xi)
˜f(Xi)εi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C+√
2 log/parenleftbiggc+b
a/parenrightbigg/radicalbigg
t
n.
18Published in Transactions on Machine Learning Research (11/2024)
Now letpi=˜g(Xi)
˜f(Xi)−1, such thata
c+b≤pi+ 1≤c+b
aholds for all i∈[n]. Additionally, let η(p) = log(p+ 1)
so thatη(0) = 0and note that for p∈/bracketleftig
a
c+b−1,c+b
a−1/bracketrightig
, the derivative of η(p)is maximal at p∗=a
c+b−1,
and equal to η′(p∗) = (c+b)/a. Therefore,a
b+clog(p+ 1)is1-Lipschitz. By Lemma 22 applied to η(p),
2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1log˜g(Xi)
˜f(Xi)εi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C= 2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1η(pi)εi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C≤4(c+b)
aE/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1˜g(Xi)
˜f(Xi)εi−1
nn/summationdisplay
i=1εi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C
≤4(c+b)
aE/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1˜g(Xi)
˜f(Xi)εi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C+4(c+b)
aEε/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1εi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤4(c+b)
aE/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1˜g(Xi)
˜f(Xi)εi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C+4(c+b)
a1√n,
where the final inequality follows from the following result, proved in Haagerup (1981):
Eε/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1εi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤
Eε/braceleftigg
1
nn/summationdisplay
i=1εi/bracerightigg2
1/2
=1√n.
Now, letξi(˜gi) =a·˜g(Xi)/˜f(Xi), and note that
|ξi(ui)−ξi(vi)|=a
|˜f(Xi)||u(Xi)−v(Xi)|≤|u(Xi)−v(Xi)|.
By again applying Lemma 22, we have
4(c+b)
aE/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1˜g(Xi)
˜f(Xi)εi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C≤8(c+b)
a2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1˜g(Xi)εi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble˜C
≤8(c+b)
a2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1g(Xi)εi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
C+8(c+b)
a2E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1h(Xi)εi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤8(c+b)
a2E/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1g(Xi)εi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
C+8(c+b)
a2b√n.
Applying Lemmas 12 and 25, the following inequality holds for some constant K > 0:
Eεsup
g∈C/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1g(Xi)εi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle=Eεsup
θ∈Θ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1φ(Xi;θ)εi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤K√nE/integraldisplayc
0log1/2N(P,ε,dn,x)dε, (16)
and combining the results together, the following inequality holds with probability at least 1−e−t:
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1log˜g(Xi)
˜f(Xi)−Eflog˜g
˜f/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤8(c+b)K
a2√nE/integraldisplayc
0log1/2N(P,ε,dn,x)dε+(8b+ 4a)(c+b)
a2√n+√
2 log/parenleftbiggc+b
a/parenrightbigg/radicalbigg
t
n
=w1√nE/integraldisplayc
0log1/2N(P,ε,dn,x)dε+w2√n+w3/radicalbigg
t
n, (17)
wherew1,w2, andw3are constants that each depend on some or all of a,b, andc.
Remark 15. From Lemma 25 we have that σ2
n:= supf∈FPnf2. To make explicit why 2σn=/parenleftbig
supg∈CPng2/parenrightbig1/2= 2c, letF=Cand observe
σ2
n= sup
g∈CPng2= sup
g∈C1
nn/summationdisplay
i=1g(Xi)2≤1
nn/summationdisplay
i=1c2=c2.
19Published in Transactions on Machine Learning Research (11/2024)
Since our basis functions φ(·,θ)are bounded by c, everything greater than cwill have value 0and hence the
change from 2ctocis inconsequential. However, it can also be motivated by the fact that φ(·,θ)are positive
functions.
As highlighted in Remark 14, the full result of Theorem 13 relies on the empirical L2distancesdn,xand
dn,y. In the result of Theorem 5, we make use of the following result to bound dn,xanddn,y.
Proposition 16. By combining Lemmas 18 and 19, the following inequalities holds:
logN(P,ε,∥·∥)≤logN[](P,ε,∥·∥)≤logN(P,ε/2,∥·∥∞),
whereN[](P,ε,∥·∥)is theε-bracketing number of P. Therefore, we have that
logN(P,ε,dn,x)≤logN(P,ε/2,∥·∥∞),and logN(P,ε,dn,y)≤logN(P,ε/2,∥·∥∞).
With this result, we can now prove Theorem 5.
Proof (of Theorem 5). The notation is the same as in the proof of Theorem 13. The values of the constants
may change from line to line.
KLh(f||fk,n)−KLh(f||fk) =Eflog˜f
˜fk,n+Ehlog˜f
˜fk,n−Eflog˜f
˜fk−Ehlog˜f
˜fk
=Eflog˜f
˜fk,n−1
nn/summationdisplay
i=1log˜f(Xi)
˜fk,n(Xi)+1
nn/summationdisplay
i=1log˜f(Xi)
˜fk,n(Xi)+Ehlog˜f
˜fk,n−1
nn/summationdisplay
i=1log˜f(Yi)
˜fk,n(Yi)+1
nn/summationdisplay
i=1log˜f(Yi)
˜fk,n(Yi)
−Eflog˜f
˜fk+1
nn/summationdisplay
i=1log˜f(Xi)
˜fk(Xi)−1
nn/summationdisplay
i=1log˜f(Xi)
˜fk(Xi)−Ehlog˜f
˜fk+1
nn/summationdisplay
i=1log˜f(Yi)
˜fk(Yi)−1
nn/summationdisplay
i=1log˜f(Yi)
˜fk(Yi)
=/parenleftigg
Eflog˜f
˜fk,n−1
nn/summationdisplay
i=1log˜f(Xi)
˜fk,n(Xi)/parenrightigg
+/parenleftigg
Ehlog˜f
˜fk,n−1
nn/summationdisplay
i=1log˜f(Yi)
˜fk,n(Yi)/parenrightigg
+/parenleftigg
1
nn/summationdisplay
i=1log˜f(Xi)
˜fk(Xi)−Eflog˜f
˜fk/parenrightigg
+/parenleftigg
1
nn/summationdisplay
i=1log˜f(Yi)
˜fk(Yi)−Ehlog˜f
˜fk/parenrightigg
+/parenleftigg
1
nn/summationdisplay
i=1log˜f(Xi)
˜fk,n(Xi)−1
nn/summationdisplay
i=1log˜f(Xi)
˜fk(Xi)/parenrightigg
+/parenleftigg
1
nn/summationdisplay
i=1log˜f(Yi)
˜fk,n(Yi)−1
nn/summationdisplay
i=1log˜f(Yi)
˜fk(Yi)/parenrightigg
≤2 sup
˜g∈˜C/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1log˜g(Xi)
˜f(Xi)−Eflog˜g
˜f/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+ 2 sup
˜g∈˜C/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1log˜g(Yi)
˜f(Yi)−Ehlog˜g
˜f/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+/parenleftigg
1
nn/summationdisplay
i=1log˜f(Xi)
˜fk,n(Xi)−1
nn/summationdisplay
i=1log˜f(Xi)
˜fk(Xi)/parenrightigg
+/parenleftigg
1
nn/summationdisplay
i=1log˜f(Yi)
˜fk,n(Yi)−1
nn/summationdisplay
i=1log˜f(Yi)
˜fk(Yi)/parenrightigg
≤2E/braceleftbiggwx
1√n/integraldisplayc
0log1/2N(P,ε,dn,x)dε/bracerightbigg
+wx
2√n+wx
3/radicalbigg
t
n+1
nn/summationdisplay
i=1log˜fk(Xi)
˜fk,n(Xi)
+ 2E/braceleftbiggwy
1√n/integraldisplayc
0log1/2N(P,ε,dn,y)dε/bracerightbigg
+wy
2√n+wy
3/radicalbigg
t
n+1
nn/summationdisplay
i=1log˜fk(Yi)
˜fk,n(Yi)
≤w1√n/integraldisplayc
0log1/2N(P,ε/2,∥·∥∞)dε+w2√n+w3/radicalbigg
t
n+1
nn/summationdisplay
i=1log˜fk(Xi)
˜fk,n(Xi)+1
nn/summationdisplay
i=1log˜fk(Yi)
˜fk,n(Yi),
with probability at least 1−e−t, by Theorem 13. Now, we can use (15) from Proposition 10 applied to the
target density fk, obtaining the following:
KLh(fk||fk,n) =1
nn/summationdisplay
i=1log˜fk(Xi)
˜fk,n(Xi)+1
nn/summationdisplay
i=1log˜fk(Yi)
˜fk,n(Yi)≤4a−2c2
k+ 2+ inf
p∈CKLh(fk||p).
20Published in Transactions on Machine Learning Research (11/2024)
Since by definition we have that fk∈C,infp∈CKLh(fk||p) = 0, and so with probability at least 1−e−twe
have:
KLh(f||fk,n)−KLh(f||fk)≤w1√n/integraldisplayc
0log1/2N(P,ε/2,∥·∥∞)dε+w2√n+w3/radicalbigg
t
n+w4
k+ 2.(18)
We can write the overall error as the sum of the approximation and estimation errors as follows. The former
is bounded by (14), and the latter is bounded as above in (18). Therefore, with probability at least 1−e−t,
KLh(f||fk,n)−KLh(f||C) = [KLh(f||fk)−KLh(f||C)] + [KLh(f||fk,n)−KLh(f||fk)]
≤w4
k+ 2+w1√n/integraldisplayc
0log1/2N(P,ε/2,∥·∥∞)dε+w2√n+w3/radicalbigg
t
n.(19)
As in Rakhlin et al. (2005), we rewrite the above probabilistic statement as a statement in terms of expec-
tations. To this end, let
A:=w4
k+ 2+w1√n/integraldisplayc
0log1/2N(P,ε/2,∥·∥∞)dε+w2√n,
andZ:= KLh(f||fk,n)−KLh(f||C). We have shown P/parenleftig
Z≥A +w3/radicalig
t
n/parenrightig
≤e−t. SinceZ≥ 0,
E{Z}=/integraldisplayA
0P(Z>s)ds+/integraldisplay∞
AP(Z>s)ds≤A+/integraldisplay∞
0P(Z>A+s)ds.
Settings=w3/radicalig
t
n, we havet=w5ns2andE{Z}≤A +/integraltext∞
0e−w5ns2ds≤A+w√n. Hence,
E{KLh(f||fk,n)}−KLh(f||C)≤c1
k+ 2+c2√n/integraldisplayc
0log1/2N(P,ε/2,∥·∥∞) dε+c3√n,
wherec1,c2, andc3are constants that depend on some or all of a,b, andc.
Remark 17. The approximation error characterises the suitability of the class C, i.e., how well functions
inCare able to estimate a target fwhich does not necessarily lie in C. The estimation error characterises
the error arising from the estimation of the target fon the basis of the finite sample of size n.
Proof of Corollary 6. LetXandΘbe compact and assume the Lipshitz condition given in (8). If φ(x;·)is
continuously differentiable, then
|φ(x;θ)−φ(x;τ)|≤d/summationdisplay
k=1/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂φ(x;·)
∂θk(θ∗
k)/vextendsingle/vextendsingle/vextendsingle/vextendsingle|θk−τk|≤sup
θ∗∈Θ/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂φ(x;·)
∂θ(θ∗)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
1∥θ−τ∥1.
Setting
Φ(x) = sup
θ∗∈Θ/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂φ(x;·)
∂θ(θ∗)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
1,
we have∥Φ∥∞<∞. From Lemma 20, we obtain the fact that
logN[](P,2ε∥Φ∥∞,∥·∥∞)≤logN(Θ,ε,∥·∥∞),
which by the change of variable δ= 2ε∥Φ∥∞=⇒ε=δ/2∥Φ∥∞implies
logN[](P,ε/2,∥·∥∞)≤logN/parenleftbigg
Θ,ε
4∥Φ∥∞,∥·∥1/parenrightbigg
.
Since Θ⊂Rd, using the fact that a Euclidean set of radius rhas covering number N(r,ε)≤/parenleftbig3r
ε/parenrightbigd,we have
logN/parenleftbigg
Θ,ε
4∥Φ∥∞,∥·∥1/parenrightbigg
≤dlog/bracketleftbigg12∥Φ∥∞diam (Θ)
ε/bracketrightbigg
.
21Published in Transactions on Machine Learning Research (11/2024)
So/integraldisplayc
0/radicaligg
logN/parenleftbigg
Θ,ε
4∥Φ∥∞,∥·∥1/parenrightbigg
dε≤/integraldisplayc
0/radicaligg
dlog/bracketleftbigg12∥Φ∥∞diam (Θ)
ε/bracketrightbigg
dε,
and sincec<∞, the integral is finite, as required.
B Discussions and remarks regarding h-MLLEs
In this section, we share some commentary on the derivation of the h-lifted KL divergence, its advantages
and drawbacks, and some thoughts on the selection of the lifting function h. We also discuss the suitability
of the MM algorithms in contrast to other approaches (e.g., EM algorithms).
B.1 Elementary derivation
From Equation 4, we observe that if Xarises from a measure with density f, and if we aim to approximate
fwith a density g∈cok(P)that minimizes the h-lifted KL divergence KLhwith respect to f, then we can
define an approximator (referred to as the minimum h-lifted KL approximator) as
fk= arg min
g∈cok(P)/bracketleftbigg/integraldisplay
X{f+h}log{f+h}dµ−/integraldisplay
X{f+h}log{g+h}dµ/bracketrightbigg
= arg min
g∈cok(P)−/integraldisplay
X{f+h}log{g+h}dµ= arg max
g∈cok(P)/integraldisplay
X{f+h}log{g+h}dµ,
noting that/integraltext
X{f+h}log{f+h}dµis a constant that does not depend on the argument g. Now, observe
that
/integraldisplay
Xflog{g+h}dµ=Eflog{g+h}and/integraldisplay
Xflog{g+h}dµ=Eflog{g+h},
since bothfandhare densities onXwith respect to the dominating measure µ. If a sample Xn= (Xi)i∈[n]
is available, we can estimate the expectation Eflog{g+h}by the sample average functional
1
nn/summationdisplay
i=1log{g(Xi) +h(Xi)},
resulting in the sample estimator for fk:
f′
k,n= arg max
g∈cok(P)/bracketleftigg
1
nn/summationdisplay
i=1log{g(Xi) +h(Xi)}+Ehlog{g+h}/bracketrightigg
,
which serves as an alternative to Equation 5. However, the expectation Ehlog{g+h}is intractable, making
the optimization problem computationally infeasible, especially when Xis multivariate (i.e., X⊂Rdford>
1), as integral evaluations may be challenging to compute accurately. Thus, we approximate the intractable
integral Ehlog{g+h}using the sample average approximation (SAA) from stochastic programming (cf.
Shapiro et al., 2021, Chapter 5), yielding the Monte Carlo approximation
1
n1n1/summationdisplay
i=1log{g(Yi) +h(Yi)}
for a sufficiently large n1∈N, where each Yiis an independent and identically distributed random variable
from the measure on Xwith density h. This approach provides an estimator for fkof the form
fk,n,n 1= arg max
g∈cok(P)/bracketleftigg
1
nn/summationdisplay
i=1log{g(Xi) +h(Xi)}+1
n1n1/summationdisplay
i=1log{g(Yi) +h(Yi)}/bracketrightigg
,
22Published in Transactions on Machine Learning Research (11/2024)
which is exactly the h-MLLE defined by Equation (5) when we take n1=n. Notably, the additional
samples Yn= (Yi)i∈[n]provide no information regarding Eflog{g+h}, which is the component of the
objective function coupling the estimator gwith the target f. However, it offers a feasible mechanism for
approximating the otherwise intractable integral Ehlog{g+h}.
By settingn1=nfor the SAA approximation of Ehlog{g+h}, the convergence rate in Theorem 5 remains
unaffected. Specifically, for any t>0,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1logg(Xi) +h(Xi)
f(Xi) +h(Xi)−Eflogg+h
f+h/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
C≤w1√nElog1/2N(P,ε,dn,x)dε+w2√n+w3/radicalbigg
t
n
and/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1logg(Yi) +h(Yi)
f(Yi) +h(Yi)−Ehlogg+h
f+h/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
C≤w1√nElog1/2N(P,ε,dn,y)dε+w2√n+w3/radicalbigg
t
n,
with probability at least 1−e−t, as noted in Remark 14. Given that both upper bounds are of order
O(1/√n) +O(/radicalbig
t/n), the combined bound in the proof of Theorem 5 in Appendix A.2 is also of this order,
as required.
Finally, to obtain the additional samples Yn= (Yi)i∈[n], we simply simulate Ynfrom the data-generating
process defined by h. Since we can choose hfreely, selecting an hthat facilitates easy simulation (e.g., h
uniform overX, which remains bounded away from zero on a compact set) is advisable for satisfying the
requirements of our theorems.
B.2 Advantages and limitations
As discussed extensively in Sections 1 and 2, our two primary benchmarks are the MLE and the least L2
estimator. Indeed, the MLE is simpler than the h-MLLE estimator, as it takes the reduced form
ˆfk,n= arg max
g∈cok(P)1
nn/summationdisplay
i=1logg(Xi),
and does not require a sample average approximation for intractable integrals. It is well established that the
MLE estimates the minimum KL divergence approximation to the target f
fk= arg min
g∈cok(P)/integraldisplay
Xflogf
gdµ= KLh(f||g).
However, ashighlightedinthefoundationalworksofLi&Barron(1999)andRakhlinetal.(2005), controlling
the expected risk
E/braceleftig
KL/parenleftig
f||ˆfk,n/parenrightig/bracerightig
−KL (f||C).
requires that f≥γfor some strictly positive constant γ >0. This requirement excludes many interesting
density functions as targets, including those that vanish at the boundaries of X, such as the β(·; 1/2,1/2)
distribution,orthosethatvanishintheinteriorof X,suchasexamples f1andf2inSection4.2. Consequently,
the condition f≥γis restrictive and often impractical in many data analysis settings.
Alternatively, one could consider targeting the minimum L2estimator:
fk= arg min
g∈cok(P)/integraldisplay
X(f−g)2dµ= arg min
g∈cok(P)/integraldisplay
Xf2−2fg+g2dµ= arg min
g∈cok(P)/bracketleftbigg
−2/integraldisplay
Xfgdµ+/integraldisplay
Xg2dµ/bracketrightbigg
.
Using a sample Xngenerated from the distribution given by f, the first term of the objective can be
approximatedby −1
n/summationtextn
i=1g(Xi),whichisrelativelysimple. However,thesecondterminvolvesanintractable
integral that cannot be approximated by Monte Carlo sampling from a fixed generative distribution, as it
dependsontheoptimizationargument g. Thus,unlikethe h-MLLE,itisnotfeasibletoreducethisintractable
integral to a sample average, which implies the need for a numerical approximation in practice. This can be
23Published in Transactions on Machine Learning Research (11/2024)
computationally complex, particularly when gis intricate andXis high-dimensional. Hence, the minimum
L2-norm estimator of the form
ˆfk,n= arg min
g∈cok(P)/bracketleftigg
−2
nn/summationdisplay
i=1g(Xi) +/integraldisplay
Xg2dµ/bracketrightigg
is often computationally infeasible, though its risk
E∥f−ˆfk,n∥2−inf
g∈C∥f−g∥2
can be bounded, as shown in the works of Klemelä (2007) and Klemelä (2009), even when minXf= 0. In
comparison with the minimum L2estimator and the MLE, we observe that the h-MLLE allows risk bounding
for targetsfnot bounded from below (i.e., minXf= 0), without requiring intractable integral expressions.
Theh-MLLE achieves the beneficial properties of both the MLE and minimum L2estimators, which is the
focus of our work.
Other divergences and risk minimization schemes for estimating f, such asβ-likelihoods and Lqlikelihood,
could also be considered. The Lqlikelihood, for instance, provides a maximizing estimator with a simple
sample average expression, similar to the MLE and h-MLLE. However, it lacks a characterization in terms of
a proper divergence function, such as the KL divergence, h-lifted KL divergence, or L2norm. Consequently,
this estimator is often inconsistent, as observed in Ferrari & Yang (2010) and Qin & Priebe (2013). These
studies show that the Lqlikelihood estimator may not converge meaningfully to f, even when f∈cok(P),
for any fixed q∈R>0\{1}, unless a sequence of maximum Lqlikelihood estimators is constructed with q
depending on nand approaching 1to approximate the MLE. Thus, the maximum Lqlikelihood estimator
does not yield the type of risk bound we require.
Similarly, with the β-likelihood (or density power divergence), the situation is comparable to that of the
minimumL2norm estimator, where the sample-based estimator involves an intractable integral that cannot
be approximated through SAA. Specifically, the minimum β-likelihood estimator is defined as (cf. Basu
et al., 1998):
ˆfk,n= arg min
g∈cok(P)/bracketleftigg
−1
n/parenleftbigg
1 +1
β/parenrightbiggn/summationdisplay
i=1gβ(Xi) +/integraldisplay
Xg1+βdµ/bracketrightigg
forβ > 0, which closely resembles the form of the minimum L2estimator. Hence, the limitations of the
minimumL2estimator apply here as well, although a risk bound with respect to the β-likelihood divergence
could theoretically be obtained if the computational challenges are disregarded. In Section 1.3, we cite
additional estimators based on various divergences and modified likelihoods. Nevertheless, in each case, one
of the limitations discussed here will apply.
B.3 Selection of the lifting density function h
The choice of his entirely independent of the data. In fact, hcan be any density with respect to µ, satisfying
0< a≤h(x)≤b <∞for everyx∈X. Beyond this requirement, our theoretical framework remains
unaffected by the specific choice of h. In Section 4, we explore cases where his uniform and non-uniform,
demonstrating convergence in both kandnthat aligns with the predictions of Theorem 5. For practical
implementation, as discussed in Appendix B.1, hserves as the sampling distribution for the sample average
approximation (SAA) of the intractable integral Ehlog{g+h}. Given its role as a sampling distribution, it
is advantageous to select a form for hthat is easy to sample from. In many applications, we find that the
uniform distribution over Xis an optimal choice for h, as it meets the bounding conditions.
We observe that although calibrating hdoes not improve the rate, it does influence the constants in the
upper bound of the final equation of Equation 19. Specifically, for each t >0, with probability at least
1−e−t,
KLh(f||fk,n)−KLh(f||C)≤w1√n/integraldisplayc
0log1/2N(P,ε/2,∥·∥∞)dε+w2√n+w3/radicalbigg
t
n+w4
k+ 2,
24Published in Transactions on Machine Learning Research (11/2024)
which contributes to the constants in Theorem 5. Letting cdenote the upper bound of the target f(i.e.,
f(x)≤c<∞for everyx∈X), we make the following observations regarding the constants:
w1∝c+b
a2, w 2∝(8b+ 4a)(c+b)
a2, w 3∝log/parenleftbiggc+b
a/parenrightbigg
, w 4∝c2
a2.
Here,w1, w2,andw3are per the final bound in Equation 17 in the proof of Theorem 13, while w4arises
from the bound in Equation 15.
Whenhis uniform, it takes the form h(x) =z, wherez= 1//integraltext
Xdµ, makinga=z=b. Ifhis non-uniform,
then necessarily a < z < b , as there would exist a region Zof positive measure where h(x)> z, which
implies that h(x)<zfor somex∈X\Z; otherwise,
/integraldisplay
Xhdµ=/integraldisplay
Zhdµ+/integraldisplay
X\Zhdµ>µ(Z)
µ(X)+µ(X\Z )
µ(X)= 1,
contradicting hbeing a density function. Although we cannot control c, we can choose hto controlaandb.
Settingh=zminimizes the numerators in w1, as deviations from uniformity increase the numerators of w1
andw4while decreasing the denominators. The same reasoning applies to w2:
w2∝(8b+ 4a)(c+b)
a2=/braceleftbigg8bc
a2+4c
a+8b2
a2+4b
a/bracerightbigg
.
Sincec>0, any deviation from uniformity in heither increases or maintains the numerators while decreasing
thedenominators, minimizing w2whenhisuniform. Thesamelogicappliesto w3, asthelogarithmicfunction
is increasing, so w3is minimized when his uniform.
Consequently, we conclude that the smallest constants in Theorem 5 are achieved when his chosen as the
uniform distribution on X. This suggests that a uniform his optimal from both practical and theoretical
perspectives.
B.4 Discussions regarding the sharpness of the obtained risk bound
Similar to the role of Gaussian mixtures as the archetypal class of mixture models for Euclidean spaces, beta
mixtures represent the archetypal class of mixture models on the compact interval [0,1], as established in
the studies by Ghosal (2001); Petrone (1999). Just as Gaussian mixtures can approximate any continuous
density onX=Rdto an arbitrary level of accuracy in the Lp-norm (Nguyen et al., 2020; 2021; 2022b),
mixtures of beta distributions can similarly approximate any continuous density on X= [0,1]with respect
to the supremum norm (Ghosal, 2001; Petrone, 1999; Petrone & Wasserman, 2002). We will leverage this
property in the following discussion.
Assuming the target fis within the closure of our mixture class C(i.e., KLh(f||C) = 0), settingkn=O(√n)
achieves a convergence rate in expected KLhofO(1/√n)for the mixture maximum h-lifted likelihood
estimator (h-MLLE)fkn,n. An interesting question is whether this rate is tight and not overly conservative,
given the observed rates in Table 1. We aim to investigate this question by discussing a lower bound for the
estimation problem.
To approach this, we use Proposition 3 to observe that KLhsatisfies a Pinsker-like inequality:
/radicalbig
KLh(f||g)≥TV(f,g),
where TV(f,g) =1
2/integraltext
X|f−g|dµ. Using this inequality along with Corollary 6 and the convexity of f∝⇕⊣√∫⊔≀→f2,
we find that the h-MLLE satisfies the following total variation bound:
E{TV(f,fkn,n)}≤/radicalbiggw1,f
kn+w2,f√n≤w1,f
k1/2
n+w2,f
n1/4≤wf
n1/4,
for some positive constants w1,f, w2,f, wfdepending on f, by taking kn=√n. Now, consider the specific
case whenX= [0,1], and the component class Pconsists of beta distributions. In this case, we have (cf.
25Published in Transactions on Machine Learning Research (11/2024)
Petrone & Wasserman, 2002, Eq. 5), for any continuous density function f: [0,1]→R≥0:
inf
g∈Csup
x∈[0,1]|f(x)−g(x)|= 0,which implies that inf
g∈CKLh(f||g) = 0,
since
sup
x∈[0,1]|f(x)−g(x)|≥L2(f,g)≥√γKLh(f||g),
for any 0< γ≤h, with the second inequality due to Proposition 3. Thus, for a compact parameter space
ΘdefiningP, we assume KLh(f||C) = 0. Consequently, the rate of O(n−1/4)for expected total variation
distance is achievable in the beta mixture model setting. This convergence is uniform in the sense that
E{TV(f,fkn,n)}≤w
n1/4,
wherewdepends only on the maximum c≥f, the diameter of Θ, and the condition KLh(f||C) = 0, with
component distributions in Prestricted to parameter values in Θ.
In the context of minimum total variation density estimation on [0,1], Exercise 15.14 of Devroye & Lugosi
(2001) states that for every estimator ˆfand every Lipschitz continuous density f(with a sufficiently large
Lipschitz constant),
sup
f∈LipEf/braceleftig
TV( ˆf,f)/bracerightig
≥W
n1/3,
for some universal constant Wdepending only on the Lipschitz constant. This lower bound is faster than
our achieved rate of O(n−1/4), but it applies only to the smaller class of Lipschitz targets, a subset of the
continuous targets satisfying KLh(f||C) = 0.
The target f2from our simulations in Section 4 belongs to the class of Lipschitz targets, and thus the
improved lower bound rate of O(n−1/3)from Devroye & Lugosi (2001) applies. We can compare this with√
n−b2for Experiment 2 in Table 1, yielding an empirical rate in nofO(n−1.03), with an exponent between
−1.07and−0.98(95% confidence), over the range n∈{210,..., 215}. Clearly, this observed rate is faster
than the lower bound rate of O(n−1/3), indicating that the faster rates observed in Table 1 are due to small
values ofnandk. Asnincreases, the rate must eventually decelerate to at least O(n−1/3)when the target
fis Lipschitz onX, which is only marginally faster than our guaranteed rate of O(n−1/4). Demonstrating
thatO(n−1/4)is minimax optimal for certain target classes fis a complex task, left for future exploration.
Lastly, we note that our discussions in this section implies that the h-MLLE provides an effective and genetic
method for obtaining estimators with total variation guarantees, which complements the comprehensive
studies on the topic presented in Devroye & Györfi (1985) and Devroye & Lugosi (2001).
B.5 The KL divergence and the MLE
For any probability densities fandgwith respect to a dominant measure µonX, theh-lifted KL divergence
is defined as
KLh(f||g) =/integraldisplay
X{f+h}log/parenleftbiggf+h
g+h/parenrightbigg
dµ,
which we establish as a Bregman divergence on the space of probability densities dominated by µonXin
Appendix C.1.
We previously demonstrated a relationship between KLhand theL2distance (Proposition 3), showing that
ifh(x)≥γ >0for allx∈X, then
KLh(f||g)≤1
γL2
2(f,g),whereL2
2(f,g) =∥f−g∥2
2=/integraldisplay
X(f−g)2dµ
is the square of the L2distance between the densities. Given that we can always select h(x)≥γ, this bound
is always enforceable. This relationship is stronger than that between the standard KL divergence and the
L2distance, which similarly satisfies
KL (f||g)≤1
γL2
2(f,g),
26Published in Transactions on Machine Learning Research (11/2024)
but with the more restrictive requirement that f(x)≥γ >0for everyx∈X, limiting its applicability to
densities that do not vanish. In the proof of Proposition 3 in Appendix C.2, we show that one can write
KLh(f||g) = 2KL/parenleftbiggf+h
2,g+h
2/parenrightbigg
,
which allows the application of the theory from Rakhlin et al. (2005) by considering the mixture density
(f+h)/2as the target and using g+has the approximand, where g∈cok(P). Under this framework, the
maximum likelihood estimator can be formulated as
fk,n∈arg min
g∈cok(P)−1
nn/summationdisplay
i=1log/parenleftbiggg(Zi) +h(Zi)
2/parenrightbigg
,
where (Zi)i∈[n]areindependentandidenticallydistributedsamplesfromadistributionwithdensity (f+h)/2.
This sampling can be performed by choosing Xiwith probability 1/2orYiwith probability 1/2for each
i∈[n], whereXiis an observation from the generative model fandYiis an independent sample from
the auxiliary density h. Although the modified estimator, based on the bound from Rakhlin et al. (2005),
attains equivalent convergence rates, it inefficiently utilizes observed data, as 50% of the data is replaced
by simulated samples Yi. In contrast, our h-MLLE estimator maximally utilizes all available data while
achieving the same bound.
B.6 Comparison of the MM algorithm and the EM algorithm
Since the risk functional is not a log-likelihood, a straightforward EM approach cannot be used to compute
theh-MLLE. However, by interpreting KLhas a loss between the target mixture (f+h)/2and the estimator
(fk,n+h)/2, an EM algorithm can be constructed using the standard admixture framework (see Lange,
2013, Section 9.5). Remarkably, the EM algorithm for estimating (fk,n+h)/2, has the same form as our
MM algorithm, which leverages Jensen’s inequality (cf. Lange, 2013, Section 8.3). In fact, the majorizer in
any EM algorithm results directly from Jensen’s inequality (see Lange, 2013, Section 9.2), making our MM
algorithm in Section 4.1 no more complex than an EM approach for mixture models.
Beyond the EM and MM methods, no other standard algorithms typically address the generic estimation of
ak-component mixture model in cok(P)for a given parametric class P. Since our MM algorithm follows
a structure nearly identical to the EM algorithm for the MLE of this problem, it has comparable iterative
complexity. Notably, per iteration, the MM approach requires additional evaluations for both XnandYn,
and forg(Xi)andh(Xi), so it requires a constant multiple of evaluations compared to EM, depending on
whetherhis a uniform distribution or otherwise (typically by a factor of 2 or 4).
B.7 Non-convex optimization
We note that the h-MLLE problem (and the corresponding MLE) are non-convex optimization problems.
This implies that, aside from global optimization methods, no iterative algorithm–whether gradient-based
methods like gradient descent, coordinate descent, mirror descent, or momentum-based variants–can be
guaranteed to find a global optimum. Likewise, second-order techniques such as Newton and quasi-Newton
methodsalsocannotbeexpectedtolocatetheglobalsolution. Innon-convexscenarios,theprimaryassurance
that can be offered is convergence to a critical point of the objective function. In our case, this assurance is
achieved by applying Corollary 1 from Razaviyayn et al. (2013), as discussed in Section 4.1. Notably, this
convergence guarantee is consistent with that provided by other iterative approaches, such as EM, gradient
descent, or Newton’s method.
Additionally, it may be valuable to examine whether specific convergence rates can be ensured when the
algorithm’siteratesapproachaneighborhoodaroundacriticalvalue. InthecontextoftheMMalgorithm, we
can affirmatively answer this question: since the h-MLLE objective is twice continuously differentiable with
respect to the parameter ψk, it satisfies the local convergence conditions outlined in Lange (2016, Proposition
7.2.2). This result implies that if ψ(s)
klies within a sufficiently close neighborhood of a local minimizer ψ∗
k,
the MM algorithm converges linearly to ψ∗
k. This behavior aligns with the convergence guarantees offered
27Published in Transactions on Machine Learning Research (11/2024)
by other iterative methods, such as gradient descent or line-search based quasi-Newton methods. Quadratic
convergence rates near ψ∗
kcan be achieved with a Newton method, though this forfeits the monotonicity
(or stability) of the MM algorithm, as it is well-known that even in convex settings, Newton’s method can
diverge if the initialization is not properly handled.
An additional advantage of the MM algorithm over Newton’s method is its capacity to decompose the
original objective into a sum of functions where each component of ψk= (π1,...,πk,θ1,...,θk)is separable
within the summation. In other words, we can independently optimize functions that depend only on
subsets of parameters, either (π1,...,πk)or eachθjforj= 1,...,k, thereby simplifying the iterative
computation. This characteristic is noted after Equation 9 in the main text. Such decomposition can lead
to computational efficiency by avoiding the need to compute the Hessian matrix for Newton’s method or
approximations required by quasi-Newton methods. Specifically, in cases involving mixtures of exponential
familydistributionssuchasthebetadistributionsdiscussedinSection4.2, eachparameter-separatedproblem
becomes a strictly concave maximization problem, which can be efficiently solved (see Proposition 3.10 in
Sundberg, 2019).
C Auxiliary proofs
In this section, we include other proofs of claims made in the main text that are not included in Appendix A.
C.1 Theh-lifted KL divergence as a Bregman divergence
Let˜u=u+h, so thatϕ(u) = ˜ulog(˜u)−˜u+ 1. Thenϕ′(u) = log(˜u), and
Dϕ(f||g) =/integraldisplay
X{˜flog(˜f)−˜f−1}−{ ˜glog(˜g)−˜g−1}−log(˜g)(f−g)dµ
=/integraldisplay
X˜flog(˜f)−˜glog(˜g)−flog(˜g) +glog(˜g)dµ
=/integraldisplay
X{f+h}log(˜f)−{g+h}log(˜g)−flog(˜g) +glog(˜g)dµ
=/integraldisplay
X{f+h}logf+h
g+hdµ= KLh(f||g).
C.2 Proof of Proposition 2
Let˜f=f+hand˜g=g+h. Sincehispositive, thereexistssome ˜g∗suchthat ˜g∗= infx∈X{g(x) +h(x)}>0.
Similarly, sinceXis compact, there exists some positive ˜f∗such that 0<˜f∗= supx∈X{f(x) +h(x)}<∞.
DefineM= supx∈Xlog{˜f(x)/˜g(x)}. ThenM <∞, and
KLh(f||g) =/integraldisplay
X˜flog˜f
˜gdµ≤sup
x∈Xlog˜f
˜g/integraldisplay
X˜fdµ= 2M <∞.
C.3 Proof of Proposition 3
Defining ˜fand˜gas above, we have
KLh(f||g) =/integraldisplay
X˜flog˜f
˜gdµ≤/integraldisplay
X˜f/parenleftbigg˜f
˜g−1/parenrightbigg
dµ=/integraldisplay
X(f−g)2
˜gdµ≤γ−1L2
2(f,g),
The first inequality comes from the fundamental inequalities on logarithm log(x)≤x−1for allx≥0.
Indeed, let f(x) = log(x)−x+ 1. We obtain f′(x) =1
x−1 =1−x
x. Thenf′(x)<0ifx>1andf′(x)≥0
ifx≤1. Therefore, fis strictly decreasing on (1,∞)andfis strictly increasing on (0,1]. This leads to the
desired inequality f(x)≤f(1) = 0for allx≥0.
28Published in Transactions on Machine Learning Research (11/2024)
The next equality comes from the following identities:
/integraldisplay
X˜f(˜f
˜g−1)dµ=/integraldisplay
X˜f2−˜f˜g
˜gdµ=/integraldisplay
X˜f2−˜f˜g−˜f˜g+ ˜g˜g
˜gdµ=/integraldisplay
X(˜f−˜g)2
˜gdµ=/integraldisplay
X(f−g)2
˜gdµ.
The last equality is followed from
/integraldisplay
X−˜f˜g+ ˜g˜g
˜gdµ=−/integraldisplay
X˜fdµ+/integraldisplay
X˜gdµ=−/integraldisplay
X(f+h)dµ+/integraldisplay
X(g+h)dµ=−/integraldisplay
Xhdµ+/integraldisplay
Xhdµ= 0.
In fact, the proof of Proposition 3 follows the standard technique in the derivation of the estimation error,
see for example Meir & Zeevi (1997).
Additionally, we can show that the h-lifted KL divergence satisfies a Pinsker-like inequality, in the sense that
/radicalbig
KLh(f||g)≥TV(f,g),
where TVrepresents the total variation distance between the densities fandg. Indeed, this is easy to
observe since
KLh(f||g) =/integraldisplay
X{f+h}logf+h
g+hdµ= 2/integraldisplayf+h
2log/braceleftig
f+h
2/bracerightig
/braceleftig
g+h
2/bracerightigdµ= 2KL/parenleftbiggf+h
2,g+h
2/parenrightbigg
≥4/braceleftbigg1
2/integraldisplay
X/vextendsingle/vextendsingle/vextendsingle/vextendsinglef+h
2−g+h
2/vextendsingle/vextendsingle/vextendsingle/vextendsingledµ/bracerightbigg2
=/braceleftbigg/integraldisplay
X/vextendsingle/vextendsingle/vextendsingle/vextendsinglef+h
2−g+h
2/vextendsingle/vextendsingle/vextendsingle/vextendsingledµ/bracerightbigg2
=/braceleftbigg1
2/integraldisplay
X|f−g|dµ/bracerightbigg2
= TV2(f,g),
where the inequality is due to Pinsker’s inequality:
/radicalbigg
1
2KL (f||g)≥TV (f,g).
C.4 Proof of Proposition 9
For choice (11), by the dominated convergence theorem, we observe that
d2
dπ2κ((1−π)p+πq) =Ef/braceleftbiggd2
dπ2logf+h
(1−π)p+πq+h/bracerightbigg
+Eh/braceleftbiggd2
dπ2logf+h
(1−π)p+πq+h/bracerightbigg
=Ef/braceleftigg
(p−q)2
[(1−π)p+πq+h]2/bracerightigg
+Eh/braceleftigg
(p−q)2
[(1−π)p+πq+h]2/bracerightigg
.
Suppose that each φ(·;θ)∈ Pis bounded from above by c <∞. Then, since p,q∈ Care non-
negative functions, we have the fact that (p−q)2≤c2. If we further have a≤hfor somea > 0, then
[(1−π)p+πq+h]2≥a2, which implies that
d2
dπ2κ((1−π)p+πq)≤2×c2
a2
for everyp,q∈Candπ∈(0,1), and thus
sup
p,q∈C,π∈(0,1)d2
dπ2κ((1−π)p+πq)≤2c2
a2<∞.
Similarly, for case (12), we have
d2
dπ2κn((1−π)p+πq) =1
nn/summationdisplay
i=1d2
dπ2logf(xi) +h(xi)
(1−π)p(xi) +πq(xi) +h(xi)
29Published in Transactions on Machine Learning Research (11/2024)
+1
nn/summationdisplay
i=1d2
dπ2logf(yi) +h(yi)
(1−π)p(yi) +πq(yi) +h(yi)
=1
nn/summationdisplay
i=1(p(xi)−q(xi))2
[(1−π)p(xi) +πq(xi) +h(xi)]2+1
nn/summationdisplay
i=1(p(yi)−q(yi))2
[(1−π)p(yi) +πq(yi) +h(yi)]2.
By the same argument, as for κ, we have the fact that (p(x)−q(x))2≤c2, for every p,q∈Cand every
x∈X, and furthermore [(1−π)p(x) +πq(x) +h(x)]2≥a2, for anyπ∈(0,1). Thus,
sup
p,q∈C,π∈(0,1)d2
dπ2κ((1−π)p+πq)≤2c2
a2<∞,as required.
D Technical results
Here we collect some technical results that are required in our proofs but appear elsewhere in the literature.
In some places, notation may be modified from the original text to keep with the established conventions
herein.
Lemma 18 (Kosorok,2007. Lem9.18) .LetN(F,ε,∥·∥)denote theε-covering number of F,N[](F,ε,∥·∥)
theε-bracketing number of F, and∥·∥be any norm on F. Then, for all ε>0
N(F,ε,∥·∥)≤N[](F,ε,∥·∥)
Lemma 19 (Kosorok, 2007. Lem 9.22) .For any norm∥·∥dominated by∥·∥∞and any class of functions
F,
logN[](F,2ε,∥·∥)≤logN(F,ε,∥·∥∞),for allε>0.
Lemma 20 (Kosorok, 2007. Thm 9.23) .For some metric donT, letF={ft:t∈T}be a function class:
|fs(x)−ft(x)|≤d(s,t)F(x),
some fixed function FonX, and for all x∈Xands,t∈T. Then, for any norm ∥·∥,
N[](F,2ε∥F∥,∥·∥)≤N(T,ε,d ).
Lemma 21 (Shalev-Shwartz & Ben-David (2014), Lem 26.7) .LetAbe a subset of Rmand let
A′=

n/summationdisplay
j=1αjaj|n∈N,aj∈A,αj≥0,∥α∥1= 1

.
Then,Rn(A′) =Rn(A), i.e., bothAandA′have the same Rademacher complexity.
Lemma 22 (van de Geer, 2016, Thm. 16.2) .Let(Xi)i∈[n]be non-random elements of Xand let Fbe a
class of real-valued functions on X. Ifφi:R→R,i∈[n], are functions vanishing at zero that satisfy for
allu,v∈R,|φi(u)−φi(v)|≤|u−v|,then we have
E/braceleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1φi(f(Xi))εi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/bracerightigg
≤2E/braceleftigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoublen/summationdisplay
i=1f(Xi)εi/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F/bracerightigg
.
Lemma 23 (McDiarmid, 1998, Thm. 3.1 or McDiarmid, 1989) .Suppose (Xi)i∈[n]are independent random
variables and let Z=g(X1,...,Xn), for some function g. Ifgsatisfies the bounded difference condition,
that is there exists constant cjsuch that for all j∈[n]and allx1,...,xj,x′
j,...,xn,
|g(x1,...,xj−1,xj,xj+1,...,xn)−g(x1,...,xj−1,x′
j,xj+1,...,xn)|≤cj,
then
P(Z−EZ≥t)≤exp/braceleftigg
−2t2
/summationtextn
j=1c2
j/bracerightigg
.
30Published in Transactions on Machine Learning Research (11/2024)
Lemma 24 (van der Vaart & Wellner, 1996, Lem. 2.3.1) .LetR(f) =EfandRn(f) =n−1/summationtextn
i=1f(Xi). If
Φ:R>0→R>0is a convex function, then the following inequality holds for any class of measurable functions
F:
EΦ(∥R(f)−Rn(f)∥F)≤EΦ(2∥Rn(f)∥F),
whereRn(f)is the Rademacher process indexed by F. In particular, since the identity map is convex,
E{∥R(f)−Rn(f)∥F}≤2E{∥Rn(f)∥F}.
Lemma 25 (Koltchinskii, 2011, Thm. 3.11) .Letdnbe the empirical distance
d2
n(f1,f2) =1
nn/summationdisplay
i=1(f1(Xi)−f2(Xi))2
and denote by N(F,ε,dn)theε-covering number of F. Letσ2
n:= supf∈FPnf2. Then the following
inequality holds
E{∥Rn(f)∥F}≤K√nE/integraldisplay2σn
0log1/2N(F,ε,dn)dε
for some constant K > 0.
31