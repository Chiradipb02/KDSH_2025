Published in Transactions on Machine Learning Research (June/2024)
Growing Tiny Networks: Spotting Expressivity Bottlenecks
and Fixing Them Optimally
Manon Verbockhaven, Théo Rudkiewicz, Sylvain Chevallier, Guillaume Charpiat
TAU team, LISN, Université Paris-Saclay, CNRS, Inria, 91405, Orsay, France
firstname.name@inria.fr
Reviewed on OpenReview: https: // openreview. net/ forum? id= hbtG6s6e7r
Abstract
Machine learning tasks are generally formulated as optimization problems, where one
searches for an optimal function within a certain functional space. In practice, parame-
terized functional spaces are considered, in order to be able to perform gradient descent.
Typically, a neural network architecture is chosen and fixed, and its parameters (connection
weights) are optimized, yielding an architecture-dependent result. This way of proceeding
however forces the evolution of the function during training to lie within the realm of what
is expressible with the chosen architecture, and prevents any optimization across architec-
tures. Costly architectural hyper-parameter optimization is often performed to compensate
for this. Instead, we propose to adapt the architecture on the fly during training. We show
that the information about desirable architectural changes, due to expressivity bottlenecks
when attempting to follow the functional gradient, can be extracted from backpropagation.
To do this, we propose a mathematical definition of expressivity bottlenecks, which enables
us to detect, quantify and solve them while training, by adding suitable neurons. Thus,
while the standard approach requires large networks, in terms of number of neurons per
layer, for expressivity and optimization reasons, we provide tools and properties to develop
anarchitecturestartingwithaverysmallnumberofneurons. Asaproofofconcept, weshow
results on the CIFAR dataset, matching large neural network accuracy, with competitive
training time, while removing the need for standard architectural hyper-parameter search.
1 Introduction
Issues with the fixed-architecture paradigm. Universal approximation theorems such as (Hornik
et al., 1989; Cybenko, 1989) are historically among the first theoretical results obtained on neural networks,
stating the family of neural networks with arbitrary width as a good candidate for a parameterized space of
functions to be used in machine learning. However the current common practice in neural network training
consists in choosing a fixed architecture, and training it, without any possible architecture modification
meanwhile. This inconveniently prevents the direct application of these universal approximation theorems,
as expressivity bottlenecks that might arise in a given layer during training will not be able to be fixed. There
are two approaches to circumvent this in daily practice. Either one chooses a (very) large width, to be sure to
avoid expressivity and optimization issues (Hanin & Rolnick, 2019b; Raghu et al., 2017), to the cost of extra
computational power consumption for training and applying such big models; to mitigate this cost, model
reduction techniques are often used afterwards, using pruning, tensor factorization, quantization (Louizos
et al., 2017) or distillation (Hinton et al., 2015). Or one tries different architectures and keeps the most
suitable one (in terms of performance-size compromise for instance), which multiplies the computational
burden by the number of trials. This latter approach relates to the Auto-DeepLearning field (Liu et al.,
2020), where different exploration strategies over the space of architecture hyper-parameters (among other
ones) have been tested, including reinforcement learning (Baker et al., 2017; Zoph & Le, 2016), Bayesian
optimization techniques (Mendoza et al., 2016), and evolutionary approaches (Miller et al., 1989; Stanley
et al., 2009; Miikkulainen et al., 2017; Bennet et al., 2021), that all rely on random tries and consequently
1Published in Transactions on Machine Learning Research (June/2024)
take time for exploration. Within that line, Net2Net (Chen et al., 2015), AdaptNet (Yang et al., 2018)
and MorphNet (Gordon et al., 2018) propose different strategies to explore possible variations of a given
architecture, possiblyguidedbymodelsizeconstraints. Instead, weaimatprovidingawaytolocateprecisely
expressivity bottlenecks in a trained network, which might speed up neural architecture search significantly.
Moreover, based on such observations, we aim at modifying the architecture on the fly during training, in
a single run (no re-training), using first-order derivatives only, while avoiding neuron redundancy. Related
work on architecture adaptation while training includes probabilistic edges (Liu et al., 2019) or sparsifying
priors (Wolinski et al., 2020). Yet the training is done on the largest architecture allowed, which is resource-
consuming. On the opposite we aim at starting from the simplest architecture possible.
Optimization properties. An important reason for common practice to choose wide architectures is the
associated optimization properties: sufficiently larger networks are proved theoretically and shown empiri-
cally to be better optimized than small ones (Jacot et al., 2018). Typically, small networks exhibit issues
with spurious local minima, while wide ones find good nearly-global minima. One of our goals is to train
small networks without suffering from such optimization difficulties.
Neural architecture growth. A related line of work consists in growing networks neuron by neuron,
by iteratively estimating the best possible neurons to add, according to a certain criterion. For instance,
approaches such as (Wu et al., 2019) or Firefly (Wu et al., 2020) aim at escaping local minima by adding
neurons that minimize the loss under neighborhood constraints. These neurons are found by gradient descent
or by solving quadratic problems involving second-order derivatives. Other approaches (Causse et al., 2019;
Bashtova et al., 2022), including GradMax (Evci et al., 2022), seek to minimize the loss as fast as possible and
involve another quadratic problem. However the neurons added by these approaches are possibly redundant
with existing neurons, especially if one does not wait for training convergence to a local minimum (which is
time consuming) before adding neurons, therefore producing larger-than-needed architectures.
Redundancy. To our knowledge, the only approach tackling redundancy in neural architecture growth
adds random neurons that are orthogonal in some sense to the ones already present (Maile et al., 2022).
More precisely, the new neurons are picked within the kernel(preimage of{0}) of an application describing
already existing neurons. Two such applications are proposed, respectively the matrix of fan-in weights
and the pre-activation matrix, yielding two different notions of orthogonality. The latter formulation is
close to the one of GradMax, in that both study first-order loss variations and use the same pre-activation
matrix, with an important difference though: GradMax optimally decreases the loss without caring about
redundancy, while the other one avoids redundancy but picks random directions instead of optimal ones. In
this paper we bridge the gap between these two approaches, picking optimal directions that avoid redundancy
in the pre-activation space.
Notions of expressivity. Several concepts of expressivity or complexity exist in the Machine Learning
literature, ranging from Vapnik-Chervonenkis dimension (Vapnik & Chervonenkis, 1971) and Rademacher
complexity (Koltchinskii, 2001) to the number of pieces in a piecewise affine function (as networks with ReLU
activations are) (Serra et al., 2018; Hanin & Rolnick, 2019a). Bottlenecks have been also studied from the
point of view of Information Theory, through mutual information between the activities of different layers
(Tishby & Zaslavsky, 2015; Dai et al., 2018); this quantity is difficult to estimate though. Also relevant and
from Information Theory, the Minimum Description Length paradigm (Rissanen, 1978; Grünwald & Roos,
2019) and Kolmogorov complexity (Kolmogorov, 1965; Li et al., 2008) enable to define trade-offs between
performance and model complexity.
In this article, we aim at measuring lacks of expressivity as the difference between what the backpropagation
asks for and what can be done by a small parameter update (such as a gradient step), that is, between the
desired variation for each activation in each layer (for each sample) and the best one that can be realized by
a parameter update. Intuitively, differences arise when a layer does not have sufficient expressive power to
realize the desired variation. Our main contributions are that we:
•adopt a functional analysis perspective on gradient descent in neural networks, advocating to follow
the functional gradient. We not only optimize the weights of the current architecture but also
2Published in Transactions on Machine Learning Research (June/2024)
dynamically adjust the architecture itself to progress towards a suitable parameterized functional
space. This approach mitigates optimization challenges like local minima that are due to thin
architectures;
•properly define and quantify the concept of expressivity bottlenecks, both globally at the neural
network output and locally at individual layers, in a computationally accessible manner. This
methodology enables to localize expressivity bottlenecks within a neural network;
•formally define as a quadratic problem the best possible neurons to add to a given layer to decrease
lacks of expressivity ; solve it and compute the associated expressivity gain;
•use a naive strategy on the basis of our approach to expand a neural network while maintaining
competitive computational complexity and performance compared to large and well-known model
or other NAS search methods.
2 Main concepts
2.1 Notations
LetFbe a functional space, e.g. L2(Rp→Rd), and a loss function L:F→R+defined on it, of the form
L(f) =E(x,y)∼D/bracketleftig
ℓ(f(x),y)/bracketrightig
, whereℓis the per-sample loss, assumed to be differentiable, and where D
is the sample distribution, from which the dataset {(x1,y1),...,(xN,yN)}is sampled, with xi∈Rpand
yi∈Rd.
For the sake of simplicity we consider a feedforward neural network fθ:Rp→RdwithLhidden layers,
each of which consisting of an affine layer with weights Wlfollowed by a differentiable activation function
σlwhich satisfies σl(0) = 0. The network parameters are then θ:= (Wl)l=1...L. The network iteratively
computes:
b0(x) =/parenleftbiggx
1/parenrightbigg
∀l∈[1,L],

al(x) =Wlbl−1(x)
bl(x) =/parenleftigg
σl(al(x))
1/parenrightigg
fθ(x) =σL(aL(x))
Figure 1: Notations
To any vector-valued function noted t(x)and any batch of inputs X:= [x1,...,xn], we associate the
concatenated matrix T(X) :=/parenleftbig
t(x1)...t(xn)/parenrightbig
∈R|t(.)|×n. The matrices of pre-activation and post-
activation activities at layer lover a minibatch Xare thus respectively: Al(X) =/parenleftbigal(x1)...al(xn)/parenrightbig
andBl(X) =/parenleftbig
bl(x1)...bl(xn)/parenrightbig
.
NB: convolutions can also be considered, with appropriate representations (cf matrix Bc
l(x)in C.1).
2.2 Approach
Functional gradient descent. We take a functional perspective on the use of neural networks. Ideally
in a machine learning task, one would search for a function f:Rp→Rdthat minimizes the loss Lby
gradient descent:∂f
∂t=−∇fL(f)for some metric on the functional space F(typically,L2(Rp→Rd)), where
∇fdenotes the functional gradient and tdenotes the evolution time of the gradient descent. The descent
directionvgoal:=−∇fL(f)is a function of the same type as fand whose value at xis easily computable
asvgoal(x) =−(∇fL(f)) (x) =−∇uℓ(u,y(x))/vextendsingle/vextendsingle
u=f(x)(see Appendix A.1 for more details). This direction
vgoalis the best infinitesimal variation in Fto add to the output ( u=f(x)) to decrease the loss L, regardless
of the parametrization of f.
3Published in Transactions on Machine Learning Research (June/2024)
Parametric gradient descent reminder. However in practice, to represent functions and to compute
gradients,theinfinite-dimensionedfunctionalspace Fhastobereplacedwithafinite-dimensionedparametric
space of functions, which is usually done by choosing a particular neural network architecture Awith weights
θ∈ΘA. The associated parametric search space FAthen consists of all possible functions fθthat can be
represented with such a network for any parameter value θ. Under standard weak assumptions (see Appendix
A.2), the gradient descent is of the form:
∂θ
∂t=−∇θL(fθ) =−E
(x,y)∼D/bracketleftig
∇θℓ(fθ(x),y)/bracketrightig
. (1)
Using the chain rule (on∂fθ
∂tthen on∇θℓ(fθ(x),y)), these parameter updates yield a functional evolution:
vGD:=∂fθ
∂t=∂fθ
∂θ∂θ
∂t=∂fθ
∂θE
(x,y)∼D/bracketleftigg
∂fθ
∂θT
(x)vgoal(x)/bracketrightigg
(2)
which significantly differs from the original functional gradient descent. We will aim to augment the neural
network architecture so that the parametric gradient descent can get closer to the functional one.
Figure 2: Expressivity bottleneckOptimal move direction. We nameTfθ
A, or justTA,
thetangentspaceof FAatfθ, thatis, thesetofallpossible
infinitesimal variations around fθunder small parameter
variations:
Tfθ
A:=/braceleftbigg∂fθ
∂θδθ/vextendsingle/vextendsingle/vextendsingle/vextendsingles.t.δθ∈ΘA/bracerightbigg
This linear functional1space is a first-order approxima-
tion of the neighborhood of fθwithinFA. The direction
vGDobtained above by gradient descent is actually not
the best one to consider within TA. Indeed, the best move
v∗would be the orthogonal projection of the desired di-
rectionvgoal:=−∇fθL(fθ)ontoTA. This projection is
what a (generalization of the notion of) natural gradient
would compute (Ollivier, 2017).
Indeed, the parameter variation δθ∗associated to the functional variation v∗=∂fθ
∂θδθ∗is the gradi-
ent−∇TA
θL(fθ)ofL◦fθw.r.t. parameters θwhen considering the L2metric on functional variations
∥∂fθ
∂θδθ∥L2(TA), not to be confused with the usual gradient ∇θL(fθ), based on the L2metric on parame-
tervariations∥δθ∥L2(R|ΘA|). This can be seen in a proximal formulation as:
v∗= arg min
v∈TA∥v−vgoal∥2= arg min
v∈TA/braceleftbigg
DfL(f)(v) +1
2∥v∥2/bracerightbigg
(3)
whereDis the directional derivative (see details in Appendix A.3), or equivalently as:
δθ∗= arg min
δθ∈ΘA/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂fθ
∂θδθ−vgoal/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
= arg min
δθ∈ΘA/braceleftigg
DθL(fθ)(δθ) +1
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂fθ
∂θδθ/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracerightigg
=:−∇TA
θL(fθ).
Lack of expressivity. Whenvgoaldoes not belong to the reachable subspace TA, there is a lack of
expressivity, that is, the parametric space Ais not rich enough to follow the ideal functional gradient
descent. This happens frequently with small neural networks (see Appendix A.4 for an example). The
expressivity bottleneck is then quantified as the distance ∥v∗−vgoal∥between the functional gradient vgoal
and the optimal functional move v∗given the architecture A(in the sense of Eq. 3).
1∂fθ
∂θδθ :Rp→Rd,x∝⇕⊣√∫⊔≀→∂fθ(x)
∂θδθ
4Published in Transactions on Machine Learning Research (June/2024)
2.3 Generalizing to all layers
Ideal updates. The same reasoning can be applied to the pre-activations alat each layer l, seen as
functionsal:x∈Rp∝⇕⊣√∫⊔≀→al(x)∈Rdldefined over the input space of the neural network. The optimal
parameter update for a given layer lthen follows the projection of the desired update −∇alL(fθ)of the
pre-activation functions alonto the linear subspace Tal
Aof pre-activation variations that are possible with
the architecture, as we will detail now.
Given a sample (x,y)∈ D, standard backpropagation already iteratively computes vl
goal(x) :=
−(∇alL(fθ)) (x) =−∇uℓ(σL(WLσL−1(WL−1...σl(u))),y)|u=al(x), which is the derivative of the loss
ℓ(fθ(x),y)with respect to the pre-activations u=al(x)of each layer. This is usually performed in order
to compute the gradients w.r.t. model parameters Wl, as∇Wlℓ(fθ(x),y) =∂al(x)
∂Wl∇alℓ(fθ(x),y).
vl
goal(x) :=−(∇alL(fθ)) (x)indicates the direction in which one would like to change the layer pre-
activationsal(x)in order to decrease the loss at point x. However, given a minibatch of points (xi),
most of the time no parameter move δθis able to induce this progression for each xisimultaneously, because
theθ-parameterized family of functions alis not expressive enough.
Activityupdateresultingfromaparameterchange. Givenasubsetofparameters ˜θ(suchastheones
specifictoalayer: ˜θ=Wl),andanincrementaldirection δ˜θtoupdatetheseparameters(e.g.theoneresulting
from a gradient descent: δ˜θ=−/summationtext
(x,y)∈minibatch∇˜θℓ(fθ(x),y)), the impact of the parameter update γδ˜θon
the pre-activations alat layerlat order 1 in γ, whereγan amplitude factor, is as vl(x,γδ˜θ) :=γ∂al(x)
∂˜θδ˜θ.
The factor γis the analogue of the learning rate and is considered to be small.
3 Expressivity bottlenecks
We now quantify expressivity bottlenecks at any layer las the distance between the desired activity update
vl
goal(.)and the best realizable one vl(.)(cf. Figure 2):
Definition 3.1 (Lack of expressivity) .For a neural network fθand a minibatch of points X=
{(xi,yi)}n
i=1, we define the lack of expressivity at layer las how far the desired activity update
Vl
goal= (vl
goal(x1),vl
goal(x2),...)is from the closest possible activity update Vl= (vl(x1),vl(x2),...)realiz-
able by a parameter change δθ:
Ψl:= min
vl∈Tal
A1
nn/summationdisplay
i=1/vextenddouble/vextenddoublevl(xi)−vl
goal(xi)/vextenddouble/vextenddouble2= min
δθ1
n/vextenddouble/vextenddoubleVl(X,δθ)−Vl
goal(X)/vextenddouble/vextenddouble2
Tr(4)
where||.||stands for the L2norm,||.||Trfor the Frobenius norm, and Vl(X,δθ)is the activity update
resulting from parameter change δθas defined in previous section. In the two following parts we fix the
minibatchXand simplify notations accordingly by removing the dependency on X. Proofs of this section
are deferred to Appendix C.
3.1 Best move without modifying the architecture of the network
LetδW∗
lbe the solution of Equation (4) when the parameter variation δθis restricted to involve only
layerlparameters, i.e. Wl. This move is sub-optimal in that it does not result from an update of all
architecture parameters but only of the current layer ones. In this case the activity update simplifies as
Vl(Wl) =δWlBl−1and the problem becomes:
δW∗
l= arg min
δW1
n/vextenddouble/vextenddoubleVl(δW)−Vl
goal/vextenddouble/vextenddouble2
Tr. (5)
Proposition 3.1. The solution of Problem equation 5 is: δW∗
l=1
nVl
goalBT
l−1(1
nBl−1BT
l−1)+whereP+
denotes the pseudoinverse of matrix P.
This update δW∗
lis not equivalent to the usual gradient descent update, whose form is δWGD
l∝Vl
goalBT
l−1.
In fact the associated activity variation, δW∗
lBl−1, is the projection of Vl
goalon the post-activation matrix
5Published in Transactions on Machine Learning Research (June/2024)
Figure 3: Adding one neuron to layer lin cyan (K=
1), with connections in cyan. Here, α∈R5andω∈R3.
Figure 4: Sum of functional moves
of layerl−1, that is to say onto the span of all possible post-activation directions, through the projector
1
nBT
l−1/parenleftbig1
nBl−1BT
l−1/parenrightbig+Bl−1. To increase expressivity if needed, we will aim at increasing this span with the
most useful directions to close the gap between this best update and the desired one. Note that the update
δW∗
lconsists of a standard gradient ( Vl
goalBT
l−1) followed by a change of metric in the pre-activation space
as an application of/parenleftbig1
nBl−1BT
l−1/parenrightbig+, a structure which is reminiscent of the natural gradient.
3.2 Reducing expressivity bottleneck by modifying the architecture
To get as close as possible to Vl
goaland to increase the expressive power of the current neural network,
we modify each layer of its structure. At layer l−1, we addKneuronsn1,...,nKwith input weights
α1,...,αkand output weights ω1,...,ωK(cf. Figure 3). We have the following expansions by concatenation:
WT
l−1←/parenleftbigWT
l−1α1...αK/parenrightbig
andWl←/parenleftbigWlω1...ωK/parenrightbig
. We note this architecture modification
θ←θ⊕θK
↔where⊕is the concatenation sign and θK
↔:= (αk,ωk)K
k=1are theKadded neurons.
The added neurons could be chosen randomly, as in usual neural network initialization, but this would not
yield any guarantee regarding the impact on the system loss. Another possibility would be to set either input
weights (αk)K
k=1or output weights (ωk)K
k=1to 0, so that the function fθ(.)would not be modified, while its
gradient w.r.t. θwould be enriched from the new parameters. Another option is to solve an optimization
problem as in the previous section with the modified structure θ←θ⊕θK
↔and jointly search for both the
optimal new parameters θK
↔and the optimal variation δWof the old ones:
arg min
θK↔,δW1
n/vextenddouble/vextenddoubleVl(δW⊕θK
↔)−Vl
goal/vextenddouble/vextenddouble2
Tr. (6)
As shown in Figure 4, the displacement Vlat layerlis actually a sum of the moves induced by the neurons
already present ( δW) and by the added neurons ( θK
↔). Our problem rewrites as :
arg min
θK
↔,δW1
n/vextenddouble/vextenddoubleVl(θK
↔) +Vl(δW)−Vl
goal/vextenddouble/vextenddouble2
Tr(7)
withvl(x,θK
↔) :=/summationtextK
k=1ωk(bl−2(x)Tαk)(see A.5). We choose δWas the best move of already-existing
parameters as defined in Proposition 3.1 and we note Vl
goalproj:=Vl
goal−Vl(δW∗). We are looking for the
solution/parenleftbig
K∗,θK∗
↔/parenrightbig
of the optimization problem :
arg min
K,θK
↔1
n/vextenddouble/vextenddouble/vextenddoubleVl(θK
↔)−Vl
goalproj/vextenddouble/vextenddouble/vextenddouble2
Tr. (8)
One should note that Problems (7) and (8) are generally not equivalent, though similar (cf. C.4).
This quadratic optimization problem can be solved thanks to the low-rank matrix approximation theorem
6Published in Transactions on Machine Learning Research (June/2024)
(Eckart & Young, 1936), using matrices N:=1
nBl−2/parenleftbig
Vl
goalproj/parenrightbigTandS:=1
nBl−2BT
l−2. AsSis positive
semi-definite, let its SVD be S=OΣOT, and define S−1
2:=O√
Σ−1OT, with the convention that the
inverse of 0 eigenvalues is 0. Finally, consider the SVD of matrix S−1
2N=/summationtextR
k=1λkukvT
k, whereRis the
rank of the matrix S−1
2N. Then:
Proposition 3.2. The solution of Problem (8) is:
•optimal number of neurons: K∗=R
•their optimal weights: θK∗
↔= (α∗
k,ω∗
k)K∗
k=1=/parenleftig√λkS−1
2uk,√λkvk/parenrightigK∗
k=1
Moreover for any number of neurons K⩽R, and associated optimal weights θK,∗
↔consisting of the first K
neurons of θK∗
↔, the expressivity gain can be quantified very simply as a function of the singular values λk:
Ψl
θ⊕θK,∗
↔= Ψl
θ−K/summationdisplay
k=1λ2
k (9)
where Ψl
θis the expressivity bottleneck (defined in Eq. (4)). For convolutional layers instead of fully-connected
ones, Equation (9) becomes an inequality ( ⩽).
In practice before adding new neurons (α,ω), we multiply them by an amplitude factor γfound by a simple
line search (see Appendix E.3), i.e. we add (√γα,√γω). The addition of each neuron khas an impact on
the bottleneck of the order of γλ2
kprovidedγis small. We observe the same phenomenon with the loss as
stated in the next proposition :
Proposition 3.3. Forγ >0, solving (8) using Vgoalproj=Vgoal−V(γδW∗)is equivalent to minimizing the
lossLat order one in γVl. Furthermore, performing an architecture update with γδW∗(5) and a neuron
addition with γθK,∗
↔(3.2) has an impact on the loss at first order in γas :
L(fθ⊕γθK,∗
↔) :=1
nn/summationdisplay
i=1ℓ(fθ⊕γθK,∗
↔(xi),yi) =L(fθ)−γ/parenleftig
σ′
l−1(0)∆θK,∗
↔+ ∆δW∗/parenrightig
+o(γ)(10)
with
∆θK,∗
↔:=1
n/angbracketleftig
Vl
goalproj,Vl(θK,∗
↔)/angbracketrightig
Tr=K/summationdisplay
k=1λ2
k (11)
∆δW∗:=1
n/angbracketleftbig
Vl
goal,Vl(δW∗)/angbracketrightbig
Tr⩾0. (12)
Theλkcould be used in a selection criterion realizing a trade-off with computational complexity. A selec-
tion based on statistical significance of singular values can also be performed. The full algorithm and its
complexity are detailed in Appendices E.4 and E.5. We finish this section by some additional propositions
and remarks.
Proposition 3.4. IfSis positive definite, then solving (8) is equivalent to taking ωk=Nαkand finding
theKfirst eigenvectors αkassociated to the Klargest eigenvalues λof the generalized eigenvalue problem:
NNTαk=λSαk.
Corollary 1. For all integers m,m′such thatm+m′⩽R, at order one in V, addingm+m′neurons
simultaneously according to the previous method is equivalent to adding mneurons then m′neurons by
applying successively the previous method twice.
One should also note that the family {Vl+1((αk,ωk))}K
k=1of pre-activity variations induced by adding the
neuronsθK,∗
↔is orthogonal for the trace scalar product. We could say that the added neurons are orthogonal
to each other (and to the already-present ones) in that sense. Interestingly, the GradMax method (Evci
et al., 2022) also aims at minimizing the loss 10, but without avoiding redundancy (see Appendix B.1 for
more details).
7Published in Transactions on Machine Learning Research (June/2024)
4 About greedy growth sufficiency and TINY convergence
One might wonder whether a greedy approach on layer growth might get stuck in a non-optimal state. By
greedywe mean that every neuron added has to decrease the loss. Since in this work we add neurons layer
per layer independently, we study here the case of a single hidden layer network, to spot potential layer
growth issues. For the sake of simplicity, we consider the task of least square regression towards an explicit
continuous target f∗, defined on a compact set. That is, we aim at minimizing the loss:
inf
f/summationdisplay
x∈D||f(x)−f∗(x)||2(13)
wheref(x)is the output of the neural network and Dis the training set. Proofs and supplementary
propositions are deferred to Appendix D, in particular D.4 and D.7.
First, if one allows only adding neurons but no modification of already existing ones:
Proposition 4.1 (Exponential convergence to 0 training error by ReLU neuron additions) .It is possible to
decrease the loss exponentially fast with the number tof added neurons, i.e. as γtL(f), towards 0 training
loss, and this in a greedy way, that is, such that each added neuron decreases the loss. The factor γis
γ= 1−1
n3d′/parenleftig
dm
dM/parenrightig2
, wheredmanddMare quantities solely dependent on the dataset geometry, d′is the
output dimension of the network, and nis the dataset size.
In particular, there exists no situation where one would need to add many neurons simultaneously to decrease
the loss: it is always feasible with a single neuron.
TINY might get stuck when no correlation between inputs xiand desired output variations f∗(xi)−f(xi)
can be found anymore. To prevent this, one can choose an auxiliary method to add neurons in such cases,
for instance random neurons (with a line search over their amplitude, cf. Appendix D.3), or locally-optimal
neurons found by gradient descent, or solutions of higher-order expressivity bottleneck formulations using
further developments of the activation function. We will name completed-TINY the completion of TINY by
any such auxiliary method.
Now, if we also update already existing weights when adding new neurons, we get a stronger result:
Proposition 4.2 (Completed-TINY reaches 0 training error in at most nneuron additions) .Under certain
assumptions (full batch optimization, updating already existing parameters, and, more technically: polynomial
activation function of order ⩾n2), completed-TINY reaches 0 training error in at most nneuron additions
almost surely.
Hence we see the importance of updating existing parameters on the convergence speed. This optimization
protocol is actually the one we follow in practice when training neural networks with TINY (except when
comparing with other methods using their protocol).
Note that our approach shares similarity with gradient boosting Friedman (2001) somehow, as we grow the
architecture based on the gradient of the loss. Note also that finding the optimal neuron to add is actually
NP-hard (Bach, 2017), but that we do not need new neuron optimality to converge to 0 training error.
5 Results
5.1 Comparison with GradMax on CIFAR-100
The closest growing method toTINY is GradMax (Evci et al. (2022)), asit solves a quadraticproblem similar
to (8). By construction, the objective of GradMax is to decrease the loss as fast as possible considering an
infinitesimal increment of new neurons. The main difference is that GradMax does not take into account
the expressivity of the current architecture as TINY does in (8) by projecting vgoal. In-depth details about
the difference between the GradMax and TINY are provided in Appendix B.1.
In this section, we show on the CIFAR-100 dataset that solving (8) instead of (23) (defined by GradMax)
to grow a network using a naive strategy allows better final performance and almost full expressive power.
8Published in Transactions on Machine Learning Research (June/2024)
Figure 5: Test accuracy as a function of the number of parameters during architecture growth from ResNet s
to ResNet18. The starting architecture in the left (resp. right) column is ResNet 1/4(resp. ResNet 1/64).
The amount of training time ∆tbetween consecutive neuron additions in the upper (resp. lower) row is 0.25
(resp. 1) epoch. Statistics over 4 runs are performed for each setting.
To do so, we have re-implemented the GradMax method and mimicked its growing process which consists
in increasing the architecture of a thin ResNet18 until it reaches the architecture of the usual ResNet18.
This process is described in the pseudo code 1, where two parameters can be chosen : the relative thinness
sof the starting architecture, w.r.t. the usual ResNet18 architecture ( s= 1/4ors= 1/64, cf. Table 3), and
the amount of training time between consecutive neuron additions ( ∆t= 1or∆t= 0.25epochs). Then
the number of parameters and the performance of the growing network are evaluated at regular intervals
to plot Figure 5. We note that both methods reach their final accuracy within less than one GPU day,
outperforming by far other NAS search methods in their category (cf. Table 5).
Once the models have reached the final architecture ResNet18, they are trained for 250 more epochs (or
500 epochs if they have not converged yet) on the training set. We have summarized the final performance
in Table 1. We also added the column Reference , which gives the performance of a ResNet18 trained from
scratch by usual gradient descent with all its neurons. We do not expect TINY or GradMax to achieve the
performance of the reference as its architecture and optimisation process have been optimised for years. The
columnSmall references corresponds to the training of the architecture ResNet18 sfors= 1/4ands= 1/64
using standard gradient descent without any increase in architecture.
The details of the protocol can be found in the annexes F.1, as well as other technical details such as the
dynamic of the training batch size E.2, the normalization of the new neurons E.3, the number of examples
used to estimate and solve the expressivity bottleneck E.1 and the complexity of the algorithms E.5. For both
methods, all the latter apply so that the main differences between GradMax and TINY in this experiment
is the mathematical definition of the new neurons.
Fors= 1/64, we observe a significant difference in performance between TINY and GradMax methods.
While TINY models almost achieve the reference’s performance, GradMax remains stuck 10 points below.
This suggests that the framework proposed by GradMax is not sufficient to be able to start with an archi-
9Published in Transactions on Machine Learning Research (June/2024)
Figure 6: Evolution of accuracy and number of parameters as a function of gradient steps for the setting
∆t= 1,s= 1/64, for TINY and GradMax. Mean and standard deviations are estimated over four runs.
Results with other settings can be found in Fig. 13 in the appendix.
0 50 100 150 200 250
epochs0.60.70.80.91.0AccuracyTINY
train
test
0 100 200 300 400 500
epochs0.60.70.80.91.0GradMaxStarting architecture : 1/64
t=1
Figure 7: Evolution of accuracy as a function of epochs for the setting ∆t= 1,s= 1/64during extra training
for TINY and GradMax. Mean and standard deviations are estimated over four runs. Results with other
settings can be found in Figures 14 and 15 in the appendix.
10Published in Transactions on Machine Learning Research (June/2024)
TINY GradMaxSmall Refe-
∆t0.25 1 0.25 1 references rence
s1/4 67.2±0.1 70.4±0.2 65.1±0.2 68.6±0.168.6±0.45∗72.81/4 70.3±0.25∗70.9±0.25∗67.0±0.25∗69.0±0.25∗
±
1/64 65.8±0.1 68.1±0.5 45.0±0.4 56.8±0.263.4±0.35∗0.35∗
1/64 69.5±0.25∗68.7±0.65∗57.0±0.410∗58.4±0.210∗
Table 1: Final accuracy on test set of ResNet18 after architecture growth ( grey) and after convergence ( blue).
The number of stars indicates the multiple of 50 epochs needed to achieve convergence. With the starting
architecture ResNet 1/64and∆t= 0.25, the method TINY achieves 65.8±0.1on test set after its growth and
it reaches 69.5±0.25∗after 5∗:= 5×50additional epochs (examples of training curves for the extra training
can be found in Figure 14). Means and standard deviations are performed on 4 runs for each setting.
tecture far from full expressivity, i.e. ResNet 1/64, while TINY is able to handle it. As for the setting s= 1/4,
both methods seem equivalent in terms of final performance and achieve full expressivity.
The curves on Figure 6, which are extracted from Figure 13 in the appendix, show that TINY models have
converged at the end of the growing process, while GradMax ones have not. This latter effect contrasts
with GradMax formulation which is to accelerate the gradient descent as fast as possible by adding neurons.
Furthermore GradMax needs extra training to achieve full expressivity: for the particular setting s=
1/64,∆t= 1, the extra training time required by GradMax is twice as high as TINY’s, as shown in Figure 7.
This need for extra training also appears for all settings in Table 1. In particular for s= 1/64,∆t= 0.25,
the difference in performance after and before extra training goes up to 20 % above the initial performance
while it is only of 6% for TINY.
5.2 Comparison with Random on CIFAR-100 : initialisation impact
In this section, we focus on the impact of the new neurons’ initialization. To do so, we consider as a baseline
the Random method, which initializes the new neurons according to a Gaussian distribution: (α∗
k,ω∗
k)K
k=1∼
N(0,Id)or a uniform distribution U[−1,1]. Also, when adding new neurons, we now search for the best
scaling using a line-search on the loss. Thus, we perform the operation θK
↔←γ∗θK
↔, with the amplitude
factorγ∗∈Rdefined as :
γ∗:= arg min
γ∈[−L,L]/summationdisplay
iℓ(fθ⊕γθK↔(xi),yi) withγθK∗
↔= (γα∗
k,γω∗
k)K
k=1 (14)
withLa positive constant. More details can be found in Appendix E.3.2 and in Algorithm 1. With such
an amplitude factor, one can measure the quality of the directions generated by TINY and Random by
quantifying the maximal decrease of loss in these directions.
Tobettermeasuretheimpactoftheinitialisationmethod, andtodistinguishitfromtheoptimizationprocess,
we do not perform any gradient descent. This contrasts with the previous section where long training time
after architecture growth was modifying the direction of the added neurons, dampening initialization impact
with training time, especially as they were added with a small amplitude factor (cf Section E.3.1).
With these two modifications to the protocol of previous section, we obtain Figure 8. We see the crucial
impact of TINY initialization compared to the Random one. Indeed, TINY reaches more than 17% accuracy
just by adding neurons (without any further update), which accounts for about one quarter of the total
accuracy with the full method (69% in Table 1 using in plus gradient descent). On the opposite, the
Random initialization does not contribute to the accuracy through the growing process (just about 1%); this
can be explained and quantified as follows. To study the random setting, we can model v(X)andvgoal(X)
as independent variables where vgoal∼N/parenleftbig
0d,1
dId/parenrightbig
andveither∼N/parenleftbig
0d,1
dId/parenrightbig
or∼U[−1
d,1
d],dbeing the
dimension of vgoalandv. From Equation (10), the scalar product ⟨V(X),Vgoal(X)⟩:=1
n/summationtext
ivgoal(xi)Tv(xi)
isaproxyfortheexpecteddecreaseoflossaftereacharchitecturegrowth. Thisquantitycanbeapproximated
11Published in Transactions on Machine Learning Research (June/2024)
Figure 8: Test accuracy as a function of the number of parameters during pure architecture growth (no
gradient steps performed, only neuron addition with a given initialization) from ResNet 1/64to ResNet 18,
averaged over four independent runs.
by its standard deviation, ie1√
nd, which makes the expected relative gain of loss (for a gradient step) of the
order of magnitude of1√
64for the first layer and1√
512for the last layer when compared to the true gradient,
and consequently when compared to TINY. Furthermore, one can take into account the effect of a line search
over the random direction: in that case the expected relative loss gain is quadratic in the angle between the
directions and therefore of the order of magnitude of1
64or1
512respectively (see Appendix D.3).
Note that the search interval of Equation (14) can be shrunk to [0,L]with TINY initialization, as the first
order development of the loss in Equation (10) is positive. This property is the direct consequence of the
definition of V∗as the minimizer of the expressivity bottleneck (Eq. (8)). One can also note that we do
not include GradMax in Figure 8, because its protocol initializes the on-going weights to zero ( αk←0) and
imposes a small norm on its out-going weights ( ||ωk||=ε). Those two aspects make the amplitude factor γ∗
meaningless and the impact of the new neurons initialization invisible without gradient descent.
The code is available at https://gitlab.inria.fr/mverbock/tinypub .
6 Conclusion
We provided the theoretical principles of TINY, a method to grow the architecture of a neural net while
training it; starting from a very thin architecture, TINY adds neurons where needed and yields a fully
trained architecture at the end. Our method relies on the functional gradient to find new directions that
tackle the expressivity bottleneck, even for small networks, by expanding their space of parameters. This
way, we combine in the same framework gradient descent and neural architecture search, that is, optimizing
the network parameters and its architecture at the same time, and this, in a way that guarantees convergence
to 0 training error, thus escaping expressivity bottlenecks indeed.
While transfer learning works well on ordinary tasks, for it to succeed, it needs to fine-tune and use large
architectures at deployment in order to extract and manipulate common knowledge. Our method has the
advantage of being generic and could also produce smaller models as it adapts the architecture to a single
task.
12Published in Transactions on Machine Learning Research (June/2024)
It is already instantiated for linear and convolutional layers ; extension to self-attention mechanisms (trans-
formers) is part of future works. Although common architectures consist of a succession of layers, a research
direction is to develop tools handling general computational graphs (such as U-net, Inception, Dense-Net),
which offers the possibility to let the architecture graph grow and bypass manual design.
Another possible development would be to study the statistical reliability of the TINY method, for instance
using tools borrowed from random matrix theory. Indeed statistical tests can be applied to the intermediate
computations from which the new neurons are obtained. An interesting byproduct of this approach would
be to define a threshold to select neurons found by Prop. 3.2, based on statistical significance.
7 Acknowledgments
The authors address their deepest thanks to Stella Douka, Andrei Pantea, Stéphane Rivaud & François
Landes for the exchanges and discussions on this project.
This work was supported by grants ANR-22-CE33-0015-01 and ANR-17-CONV-0003 operated by LISN to
Sylvain Chevallier and by ANR-20-CE23-0025 operated by Inria to Guillaume Charpiat. This work was also
funded by the European Union under GA no. 101135782 (MANOLO project).
Numerical computation was enabled by the scientific Python ecosystem: Matplotlib Hunter (2007),
Numpy Harris et al. (2020), Scipy Virtanen et al. (2020), pandas pandas development team (2020), Py-
Torch Paszke et al. (2019).
References
Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of Machine
Learning Research , 18(1):629–681, 2017.
Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures
usingreinforcementlearning. In International Conference on Learning Representations , 2017. URL https:
//openreview.net/forum?id=S1c2cvqee .
Kateryna Bashtova, Mathieu Causse, Cameron James, Florent Masmoudi, Mohamed Masmoudi, Houcine
Turki, and Joshua Wolff. Application of the topological gradient to parsimonious neural networks. In
Computational Sciences and Artificial Intelligence in Industry , pp. 47–61. Springer, 2022.
Pauline Bennet, Carola Doerr, Antoine Moreau, Jeremy Rapin, Fabien Teytaud, and Olivier Teytaud. Nev-
ergrad: black-box optimization platform. ACM SIGEVOlution , 14(1):8–15, 2021.
Mathieu Causse, Cameron James, Mohamed Slim Masmoudi, and Houcine Turki. Parsimonious neural
networks. In Cesar Conference ,2019. URL https://www.cesar-conference.org/wp-content/uploads/
2019/10/s5_p1_21_1330.pdf .
Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer.
arXiv preprint arXiv:1511.05641 , 2015.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and
systems, 2(4):303–314, 1989.
Bin Dai, Chen Zhu, Baining Guo, and David Wipf. Compressing neural networks using the variational
information bottleneck. In International Conference on Machine Learning , pp. 1135–1144. PMLR, 2018.
Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika ,
1(3):211–218, September 1936. ISSN 1860-0980. doi: 10.1007/BF02288367. URL https://doi.org/10.
1007/BF02288367 .
Utku Evci, Bart van Merrienboer, Thomas Unterthiner, Fabian Pedregosa, and Max Vladymyrov. Gradmax:
Growing neural networks using gradient information. In International Conference on Learning Represen-
tations, 2022. URL https://openreview.net/forum?id=qjN4h_wwUO .
13Published in Transactions on Machine Learning Research (June/2024)
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Training pruned neural networks.
CoRR, abs/1803.03635, 2018. URL http://arxiv.org/abs/1803.03635 .
Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics , pp.
1189–1232, 2001.
ArielGordon, EladEban, OfirNachum, BoChen, HaoWu, Tien-JuYang, andEdwardChoi. Morphnet: Fast
& simple resource-constrained structure learning of deep networks. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pp. 1586–1595, 2018.
Peter Grünwald and Teemu Roos. Minimum description length revisited. International Journal of Math-
ematics for Industry , 11(01):1930001, 2019. doi: 10.1142/S2661335219300018. URL https://doi.org/
10.1142/S2661335219300018 .
Boris Hanin and David Rolnick. Complexity of linear regions in deep networks. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning ,
volume 97 of Proceedings of Machine Learning Research , pp. 2596–2604. PMLR, 09–15 Jun 2019a. URL
https://proceedings.mlr.press/v97/hanin19a.html .
Boris Hanin and David Rolnick. Deep relu networks have surprisingly few activation patterns. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances
in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019b. URL https:
//proceedings.neurips.cc/paper/2019/file/9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf .
C.R.Harris, K.J.Millman, S.J.vanderWalt, R.Gommers, P.Virtanen, D.Cournapeau, E.Wieser, J.Taylor,
S. Berg, N.J. Smith, R. Kern, M. Picus, S. Hoyer, M.H. van Kerkwijk, M. Brett, A. Haldane, J. Fernández
del Río, M. Wiebe, P. Peterson, P. G’erard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi,
C. Gohlke, and T.E. Oliphant. Array programming with NumPy. Nature, 585(7825):357–362, 2020.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015.
K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators.
Neural Networks , 2(5):359–366, 1989.
J.D. Hunter. Matplotlib: A 2D graphics environment. Computing in science & engineering , 9(3):90–95,
2007.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems , vol-
ume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf .
Andrei N Kolmogorov. Three approaches to the quantitative definition ofinformation’. Problems of informa-
tion transmission , 1(1):1–7, 1965.
Vladimir Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions on
Information Theory , 47(5):1902–1914, 2001.
Jean-François Le Gall. Intégration, Probabilités et Processus Aléatoires . 2006.
Ming Li, Paul Vitányi, et al. An introduction to Kolmogorov complexity and its applications , volume 3.
Springer, 2008.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In In-
ternational Conference on Learning Representations , 2019. URL https://openreview.net/forum?id=
S1eYHoC5FX .
14Published in Transactions on Machine Learning Research (June/2024)
ZhengyingLiu, ZhenXu, ShangethRajaa, MeysamMadadi, JulioCSJacquesJunior, SergioEscalera, Adrien
Pavao, Sebastien Treguer, Wei-Wei Tu, and Isabelle Guyon. Towards automated deep learning: Analysis
of the autodl challenge series 2019. In NeurIPS 2019 Competition and Demonstration Track , pp. 242–252.
PMLR, 2020.
Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. Advances in
neural information processing systems , 30, 2017.
Kaitlin Maile, Emmanuel Rachelson, Hervé Luga, and Dennis George Wilson. When, where, and how to add
new neurons to ANNs. In First Conference on Automated Machine Learning (Main Track) , 2022. URL
https://openreview.net/forum?id=SWOg-arIg9 .
Hector Mendoza, Aaron Klein, Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. To-
wards automatically-tuned neural networks. In Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren
(eds.),Proceedings of the Workshop on Automatic Machine Learning , volume 64 of Proceedings of Ma-
chine Learning Research , pp. 58–65, New York, New York, USA, 24 Jun 2016. PMLR. URL https:
//proceedings.mlr.press/v64/mendoza_towards_2016.html .
Risto Miikkulainen, Jason Zhi Liang, Elliot Meyerson, Aditya Rawal, Daniel Fink, Olivier Francon, Bala
Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. Evolving deep neural
networks. CoRR, abs/1703.00548, 2017. URL http://arxiv.org/abs/1703.00548 .
Geoffrey F. Miller, Peter M. Todd, and Shailesh U. Hegde. Designing neural networks using genetic algo-
rithms. In ICGA, 1989.
Yann Ollivier. True asymptotic natural gradient optimization, 2017.
The pandas development team. pandas-dev/pandas: Pandas, February 2020. URL https://doi.org/10.
5281/zenodo.3509134 .
A.Paszke,S.Gross,F.Massa,A.Lerer,J.Bradbury,G.Chanan,T.Killeen,Z.Lin,N.Gimelshein,L.Antiga,
A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang,
J. Bai, and S. Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In
Advances in Neural Information Processing Systems (NeurIPS) , pp. 8024–8035. Curran Associates, Inc.,
2019.
Allan Pinkus. Approximation theory of the mlp model in neural networks. ACTA NUMERICA , 8:143–195,
1999.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive
power of deep neural networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research ,
pp. 2847–2854. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/raghu17a.html .
J. Rissanen. Modeling by shortest data description. Automatica , 14(5):465–471, 1978. ISSN 0005-1098.
doi: https://doi.org/10.1016/0005-1098(78)90005-5. URL https://www.sciencedirect.com/science/
article/pii/0005109878900055 .
Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam. Bounding and counting linear regions
of deep neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pp. 4558–4566.
PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/serra18b.html .
Kenneth O Stanley, David B D’Ambrosio, and Jason Gauci. A hypercube-based encoding for evolving
large-scale neural networks. Artificial life , 15(2):185–212, 2009.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle.
CoRR, abs/1503.02406, 2015. URL http://dblp.uni-trier.de/db/journals/corr/corr1503.html#
TishbyZ15 .
15Published in Transactions on Machine Learning Research (June/2024)
V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their
probabilities. Theory of Probability and its Applications , 16(2):264–280, 1971. doi: 10.1137/1116025. URL
http://link.aip.org/link/?TPR/16/264/1 .
P. Virtanen, R. Gommers, T.E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Pe-
terson, W. Weckesser, J. Bright, S.J. van der Walt, M. Brett, J. Wilson, J.K. Millman, N. Mayorov,
A.R.J. Nelson, E. Jones, R. Kern, E. Larson, C.J. Carey, I. Polat, Y. Feng, E.W. Moore, J. VanderPlas,
D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E.A. Quintero, C.R. Harris, A.M. Archibald, A.H.
Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms
for Scientific Computing in Python. Nature Methods , 17:261–272, 2020.
P. Wolinski, G. Charpiat, and O. Ollivier. Asymmetrical scaling layers for stable network pruning. OpenRe-
view Archive , 2020.
Lemeng Wu, Dilin Wang, and Qiang Liu. Splitting steepest descent for growing neural architectures. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances
in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. URL https://
proceedings.neurips.cc/paper/2019/file/3a01fc0853ebeba94fde4d1cc6fb842a-Paper.pdf .
Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Firefly neural architecture descent: a gen-
eral approach for growing neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Bal-
can, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 22373–
22383. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
fdbe012e2e11314b96402b32c0df26b7-Paper.pdf .
Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig
Adam. Netadapt: Platform-aware neural network adaptation for mobile applications. In Proceedings of
the European Conference on Computer Vision (ECCV) , pp. 285–300, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. In International Conference on Learning Representations ,
2017. URL https://openreview.net/forum?id=Sy8gdB9xx .
Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning, 2016. URL http:
//arxiv.org/abs/1611.01578 . cite arxiv:1611.01578.
Appendix outline
•Appendix A details the theoretical approach of TINY.
•Appendix B reformulates related works with a common theoretical framework and compares those
approaches.
•Appendix C proves the propositions of Section 3.
•Appendix D details the proofs regarding the convergence (related to Section 4).
•Appendix E provides the hyper parameters for learning.
•Appendix F gives additional graphics associated to the result part.
ForAppendicesBandC,weusethetracescalarproductanditsassociatednorm. Wenote ⟨., .⟩:=⟨., .⟩Tr
and||.||:=||.||Tr. One should remark that ||.||=||.||Tr=||.||2.
16Published in Transactions on Machine Learning Research (June/2024)
A Theoretical details for part 2
A.1 Functional gradient
The functional loss Lis a functional that takes as input a function f∈Fand outputs a real score:
L:f∈F ∝⇕⊣√∫⊔≀→ L (f) = E
(x,y)∼D[ℓ(f(x),y)]∈R.
The function space Fcan typically be chosen to be L2(Rp→Rd), which is a Hilbert space. The directional
derivative (or Gateaux derivative, or Fréchet derivative) of functional Lat function fin direction vis defined
as:
DL(f)(v) = lim
ε→0L(f+εv)−L(f)
ε
if it exists. Here vdenotes any function in the Hilbert space Fand stands for the direction in which we
would like to update f, following an infinitesimal step (of size ε), resulting in a function f+εv.
If this directional derivative exists in all possible directions v∈Fand moreover is continuous in v, then the
Riesz representation theorem implies that there exists a unique direction v∗∈Fsuch that:
∀v∈F, DL(f)(v) =⟨v∗,v⟩.
This direction v∗is named the gradient of the functional Lat function fand is denoted by ∇fL(f).
Note that while the inner product ⟨·,·⟩considered is usually the L2one, it is possible to consider other ones,
such as Sobolev ones (e.g., H1). The gradient ∇fL(F)depends on the chosen inner product and should
consequently rather be denoted by ∇L2
fL(f)for instance.
Note that continuous functions from RptoRd, as well as C∞functions, are dense in L2(Rp→Rd).
Let us now study properties specific to our loss design: L(f) =E(x,y)∼D[ℓ(f(x),y)]. Assuming sufficient
ℓ-loss differentiability and integrability, we get, for any function update direction v∈Fand infinitesimal
step sizeε∈R:
L(f+εv)−L(f) = E
(x,y)∼D[ℓ(f(x) +εv(x),y)−ℓ(f(x),y)]
=E
(x,y)∼D/bracketleftig
∇uℓ(u,y)/vextendsingle/vextendsingle
u=f(x)·εv(x) +O/parenleftbig
ε2∥v(x)∥2/parenrightbig/bracketrightig
using the usual gradient of function ℓat point (u=f(x),y)w.r.t. its first argument u, with the standard
Euclidean dot product ·inRp. Then the directional derivative is:
DL(f)(v) = E
(x,y)∼D/bracketleftig
∇uℓ(u,y)/vextendsingle/vextendsingle
u=f(x)·v(x)/bracketrightig
=E
x∼D/bracketleftbigg
E
y∼D|x/bracketleftig
∇uℓ(u,y)/vextendsingle/vextendsingle
u=f(x)/bracketrightig
·v(x)/bracketrightbigg
and thus the functional gradient for the inner product ⟨v,v′⟩E:=Ex∼D[v(x)·v′(x)]is the function:
∇E
fL(f) :x∝⇕⊣√∫⊔≀→E
y∼D|x/bracketleftig
∇uℓ(u,y)/vextendsingle/vextendsingle
u=f(x)/bracketrightig
which simplifies into:
∇E
fL(f) :x∝⇕⊣√∫⊔≀→∇uℓ(u,y(x))/vextendsingle/vextendsingle
u=f(x)
if there is no ambiguity in the dataset, i.e. if for each xthere is a unique y(x).
Note that by considering the L2(Rp→Rd)inner product/integraltext
v·v′instead, one would respectively get:
∇L2
fL(f) :x∝⇕⊣√∫⊔≀→pD(x)E
y∼D|x/bracketleftig
∇uℓ(u,y)/vextendsingle/vextendsingle
u=f(x)/bracketrightig
and
∇L2
fL(f) :x∝⇕⊣√∫⊔≀→pD(x)∇uℓ(u,y(x))/vextendsingle/vextendsingle
u=f(x)
instead, where pD(x)is the density of the dataset distribution at point x. In practice one estimates such
gradients using a minibatch of samples (x,y), obtained by picking uniformly at random within a finite
dataset, and thus the formulas for the two inner products coincide (up to a constant factor).
17Published in Transactions on Machine Learning Research (June/2024)
A.2 Differentiation under the integral sign
LetXbe an open subset of R, and Ωbe a measure space. Suppose f:X×Ω− →Rsatisfies the following
conditions:
•f(x,ω)is a Lebesgue-integrable function of ωfor eachx∈X.
•For almost all ω∈Ω, the partial derivative∂
∂xfoffaccording to xexists for all x∈X.
•There is an integrable function θ: Ω− →Rsuch that/vextendsingle/vextendsingle∂
∂x(x,ω)/vextendsingle/vextendsingle⩽θ(ω)for allx∈Xand almost
everyω∈Ω.
Then, for all x∈X,
∂
∂x/integraldisplay
Ωf(x,ω)dω=/integraldisplay
Ω∂
∂xf(x,ω)dω (15)
See proof and details: Le Gall (2006).
A.3 Gradients and proximal point of view
Gradients with respect to standard variables such as vectors are defined in the same way as functional
gradients above: given a sufficiently smooth loss /tildewideL:θ∈ΘA∝⇕⊣√∫⊔≀→/tildewideL(θ) =L(fθ)∈R, and an inner product ·in
the space ΘAof parameters θ, the gradient∇θ/tildewideL(θ)is the unique vector τ∈ΘAsuch that:
∀δθ∈ΘA,τ·δθ=Dθ/tildewideL(θ)(δθ)
whereDθ/tildewideL(θ)(δθ)is the directional derivative of /tildewideLat pointθin the direction δθ, defined as in the previous
section. This gradient depends on the inner product chosen, which can be highlighted by the following
property. The opposite −∇θ/tildewideL(θ)of the gradient is the unique solution of the problem:
arg min
δθ∈ΘA/braceleftbigg
Dθ/tildewideL(θ)(δθ) +1
2∥δθ∥2
P/bracerightbigg
where∥ ∥Pis the norm associated to the chosen inner product. Changing the inner product changes the
way candidate directions δθare penalized, leading to different gradients. This proximal formulation can be
obtained as follows. For any δθ, its distance to the gradient descent direction is:
/vextenddouble/vextenddouble/vextenddoubleδθ−/parenleftig
−∇θ/tildewideL(θ)/parenrightig/vextenddouble/vextenddouble/vextenddouble2
=∥δθ∥2+ 2δθ·∇θ/tildewideL(θ) +/vextenddouble/vextenddouble/vextenddouble∇θ/tildewideL(θ)/vextenddouble/vextenddouble/vextenddouble2
= 2/parenleftbigg1
2∥δθ∥2+Dθ/tildewideL(θ)(δθ)/parenrightbigg
+K
whereKdoes not depend on δθ. For the above to hold, the inner product used has to be the one from which
the norm is derived. By minimizing this expression with respect to δθ, one obtains the desired property.
In our case of study, for the norm over the space ΘAof parameter variations, we consider a norm in the
space of associated functional variations, i.e.:
∥δθ∥P:=/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂fθ
∂θδθ/vextenddouble/vextenddouble/vextenddouble/vextenddouble
which makes more sense from a physical point of view, as it is more intrinsic to the task to solve and depends
as little as possible on the parameterization (i.e. on the architecture chosen). This results in a functional
move that is the projection of the functional one to the set of possible moves given the architecture. On
the opposite, the standard gradient (using Euclidean parameter norm ∥δθ∥in parameter space) yields a
18Published in Transactions on Machine Learning Research (June/2024)
functional move obtained not only by projecting the functional gradient but also by multiplying it by a
matrix∂fθ
∂θ∂fθ
∂θTwhich can be seen as a strong architecture bias over optimization directions.
We consider here that the loss Lto be minimized is the real loss that the user wants to optimize, possibly
including regularizers to avoid overfitting, and since the architecture is evolving during training, possibly
to architectures far from usual manual design and never tested before, one cannot assume architecture bias
to be desirable. We aim at getting rid of it in order to follow the functional gradient descent as closely as
possible.
Searching for
v∗= arg min
v∈TA∥v−vgoal∥2= arg min
v∈TA/braceleftbigg
DL(f)(v) +1
2∥v∥2/bracerightbigg
(16)
or equivalently for:
δθ∗= arg min
δθ∈ΘA/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂fθ
∂θδθ−vgoal/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
= arg min
δθ∈ΘA/braceleftigg
DθL(fθ)(δθ) +1
2/vextenddouble/vextenddouble/vextenddouble/vextenddouble∂fθ
∂θδθ/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracerightigg
=:−∇TA
θL(fθ)(17)
then appears as a natural goal.
A.4 Example of expressivity bottleneck
Example. Suppose one tries to estimate the function y=ftrue(x) =
2 sin(x) +xwith a linear model fpredict (x) =ax+b. Consider
(a,b) = (1,0)and the square loss L. For the dataset of inputs
(x0,x1,x2,x3) = (0,π
2,π,3π
2), there exists no parameter update ( δa,δb)
thatwouldimprovepredictionat x0,x1,x2andx3simultaneously, asthe
space of linear functions {f:x→ax+b|a,b∈R}is not expressive
enough. To improve the prediction at x0,x1,x2andx3, one should look
for another, more expressive functional space such that for i= 0,1,2,3
the functional update ∆f(xi) :=ft+1(xi)−ft(xi)goes into the same
direction as the functional gradient vgoal(xi) :=−∇f(xi)L(f(xi),yi) =
−2(f(xi)−yi)whereyi=ftrue(xi).
0 2 4 6
x0123456ytarget
predictionFigure 9: Linear interpolation
A.5 Problem formulation and choice of pre-activities
There are several ways to design the problem of adding neurons, which we discuss now, in order to explain
our choice of the pre-activities to express expressivity bottlenecks.
Suppose one wishes to add KneuronsθK
↔:= (αk,ωk)K
k=1to layerl−1, which impacts the activities alat the
next layer, in order to improve its expressivity. These neurons could be chosen to have only null weights, or
null input weights αkand non-null output weights ωk, or the opposite, or both non-null weights. Searching
for the best neurons to add for each of these cases will produce different optimization problems.
Let us remind first that adding such Kneurons with weights θK
↔:= (αk,ωk)K
k=1changes the activities alof
the (next) layer by
δal=K/summationdisplay
k=1ωkσ(αT
kbl−2(x)) (18)
Small weight approximation Under the hypothesis of small input weights αk, and assuming smooth
activation function σat0, the activity variation 18 can be approximated by:
σ′(0)K/summationdisplay
k=1ωkαT
kbl−2(x) (19)
19Published in Transactions on Machine Learning Research (June/2024)
at first order in∥αk∥. We will drop the constant σ′(0)in the sequel.
This quantity is linear both in αkandωk, therefore the first-order parameter-induced activity variations are
easy to compute:
vl/parenleftbig
x,(αk)K
k=1/parenrightbig
=∂al(x)
∂( (αk)K
k=1)|(αk)K
k=1=0(αk)K
k=1=K/summationdisplay
k=1ωkbl−2(x)Tαk
vl/parenleftbig
x,(ωk)K
k=1/parenrightbig
=∂al(x)
∂( (ωk)K
k=1)|(ωk)K
k=1=0(ωk)K
k=1=K/summationdisplay
k=1ωkbl−2(x)Tαk
so with a slight abuse of notation we have:
vl/parenleftbig
x,θK
↔/parenrightbig
=K/summationdisplay
k=1ωkαT
kbl−2(x)
Note also that technically the quantity above is first-order in αkand inωkbut second-order in the joint
variableθK
↔= (αk,ωk).
Adding neurons with 0 weights (both input and output weights). In that case, one increases the
number of neurons in the layer, but without changing the function (since only null quantities are added) and
also without changing the gradient with respect to the parameters, thus not improving expressivity. Indeed,
the added quantity (Eq. 18) involves 0×0multiplications, and consequently the derivative∂al(x)
∂θK↔/vextendsingle/vextendsingle/vextendsingle
θK↔=0w.r.t.
these new parameters, that is, bl−2(x)Tαkw.r.t.ωkandωkbl−2(x)Tw.r.t.akis 0, as bothakandωkare
0.
Adding neurons with non-0 input weights and 0 output weights or the opposite. In these cases,
the addition of neurons will not change the function (because of multiplications by 0), but just the gradient.
One of the 2 gradients (w.r.t. akor w.r.tωk) will be non-0, as the variable that is 0 has non-0 derivatives.
The question is then how to pick the best non-null variable, ( akorωk) such that the added gradient will be
the most useful. The problem can then be formulated similarly as what is done in the paper.
Adding neurons with small yet non-0 weights. In this case, both the function and its gradient will
change when adding the neurons. Fortunately, Proposition 3.2 states that the best neurons to add in terms
of expressivity (to get the gradient closer to the variation desired by the backpropagation) are also the best
neurons to add to decrease the loss, i.e. the function change they will imply goes into the right direction.
For each family (ωk)K
k=1, the tangent space in alrestricted to the family (αk)K
k=1,i.e.Tal
A:=/braceleftbigg
∂al
∂(αk)K
k=1|(αk)K
k=1=0(.)(αk)K
k=1|(αk)K
k=1∈/parenleftbig
R|bl−2(x)|/parenrightbigK/bracerightbigg
varies with the family (ωk)K
k=1,i.e.Tal
A:=
Tal
A((ωk)K
k=1). Optimizing w.r.t. the ωkis equivalent to search for the best tangent space for the αk,
while symmetrically optimizing w.r.t. the αkis equivalent to find the best projection on the tangent space
defined by the ωk(Figure 10).
Pre-activities vs. post-activities. The space of pre-activities alis a natural space for this framework,
as they are formed with linear operations and we compute first-order variation quantities. Considering
the space of post-activities bl=σ(al)is also possible, though computing variations will be more complex.
Indeed, without first-order approximation, the obtained problem is not manageable, because of the non-
linear activation function σadded in front of all quantities (while in the cases of pre-activations, quantity 18
is linear inωkand thus does not require approximation in ωk, which allow considering large ωk), and, with
first-order approximation, it would add the derivative of the activation function, taken at various locations
σ′(al)(while in the previous case the derivatives of the activation function were always taken at 0).
20Published in Transactions on Machine Learning Research (June/2024)
Figure 10: Changing the tangent space with different values of (ωk)K
k=1.
A.6 Adding convolutional neurons
To add a convolutional neuron at layer l−1, one should add a kernel at layer l−1and expand one dimension
to all the kernels in layer lto match the new dimension of the post-activity, see Figure 11.
Figure 11: Adding one convolutional neuron at layer one for an input with tree channels.
B Theoretical comparison with other approaches
B.1 GradMax method
To facilitate reading we remove the layer index of each quantity, i.e.b:=bl−2,B:=Bl−2,Vgoal:=Vgoall
andVgoalproj:=Vgoall
proj.
The theoretical approach of GradMax is to add neurons with zero fan-in weights and choose the fan-out
weights that would decrease the loss as much as possible after one gradient step. We note Ωthe fan-out
weightsofsuchneuronsandperformtheadditionatlayer l−1attimet. Afteronegradientstep, i.e.t→t+1,
21Published in Transactions on Machine Learning Research (June/2024)
and considering a learning rate equal to 1, the decrease of loss is :
Lt+1≈Lt+⟨∇θL,δθ⟩+⟨∇ΩL,δΩ⟩ (20)
Taking the direction of the usual gradient descent, ie δθ=−∇θLandδΩ =−∇ΩL, we have :
Lt+1≈Lt−||∇θL||2−||∇ ΩL||2(21)
The output weights of the new neurons as formulated in the original paper Evci et al. (2022) at eq (11) are
the solution of :
(ω∗
1,...,ω∗
K) :=Ω∗= arg max
Ω||∇ΩL||2s.t.||Ω||2⩽c (22)
we remark that :
||∇ΩL||2=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
ib(xi)vgoalT(xi)Ω/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleBVgoalTΩ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle˜NΩ/vextendsingle/vextendsingle/vextendsingle/vextendsingle2˜N:=BVgoalT
It follows that the fan-out of the neurons computed by GradMax are the solution of the problem :
Ω∗:= arg max
Ω/vextendsingle/vextendsingle/vextendsingle/vextendsingle˜NΩ/vextendsingle/vextendsingle/vextendsingle/vextendsingles.t.||Ω||2⩽c (23)
To compare this optimization problem with TINY, we use the following proposition:
Proposition B.1. ∀D∈Rp,q,B∈Rk,q,
∃c∈Rs.t. arg min
H∈Rp,k||D−HB||2= arg max
H,||HB||2⩽c⟨D,HB⟩
The proof can be found in B.1.
TakingVgoalasDandV=ΩATBasHB, we can reformulate TINY optimization problem 8 as :
A∗,Ω∗= arg max
A,Ω⟨V(A,Ω),Vgoalproj⟩ s.t.||V(A,Ω)||2⩽c (24)
We remark that/angbracketleftig
V(A,Ω),Vgoalproj/angbracketrightig
=/angbracketleftig
ΩATB,Vgoalproj/angbracketrightig
= Tr(BTAΩTVgoalproj)
= Tr(AΩTVgoalprojBT)
=/angbracketleftig
ΩAT,VgoalprojBT/angbracketrightig
With the definition N:=BVgoalT
proj,
/angbracketleftig
V(A,Ω),Vgoalproj/angbracketrightig
=/angbracketleftbig
AΩT,N/angbracketrightbig
(25)
For the constrain on V(A,Ω), we perform the change of variable ˜A:=S1
2A, it follows that :
||V(A,Ω)||2=/vextendsingle/vextendsingle/vextendsingle/vextendsingleΩATB/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
= Tr(AΩTΩATS) S=BBT
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleΩ(S1
2A)T/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingleΩ˜AT/vextendsingle/vextendsingle/vextendsingle/vextendsingle
22Published in Transactions on Machine Learning Research (June/2024)
With the same change of variable, the initial scalar product Equation (25) is :
⟨V(A,Ω),Vgoalproj⟩=⟨Ω˜ATS−1
2,NT⟩=⟨Ω˜AT,NTS−1
2⟩=⟨˜AΩT,S−1
2N⟩ (26)
TINY optimization problem is now equivalent to :
ˆA∗,Ω∗= arg max
ˆA,Ω⟨˜AΩT,S−1
2N⟩ s.t./vextendsingle/vextendsingle/vextendsingle/vextendsingleΩ˜AT/vextendsingle/vextendsingle/vextendsingle/vextendsingle2⩽c (27)
To maximize the scalar product, we choose ˜AΩT=S−1
2N. A solution for (˜A,Ω)is the (left, right)
eigenvectors of the matrix S−1
2N. It implies that :
Ω∗:= arg max
Ω/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS−1
2NΩ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
s.t.||Ω||⩽˜c (28)
One can note three differences between GradMax optimization problem and the last formulation of TINY
(Equation (28)):
•First, the matrix ˜Nis not defined using the projection of the desired update Vgoall+1
proj. As a
consequence, GradMax does not take into account redundancy, and on the opposite will actually
try to add new neurons that are as redundant as possible with the part of the goal update that is
already feasible with already-existing neurons.
•Second, the constraint lies in the weight space for GradMax method while it lies in the pre-activation
space in our case. The difference is that GradMax relies on the Euclidean metric in the space of
parameters, which arguably offers less meaning that the Euclidean metric in the space of activities.
Essentially this is the same difference as between the standard L2 gradient w.r.t. parameters and
the natural gradient, which takes care of parameter redundancy and measures all quantities in the
output space in order to be independent from the parameterization. In practice we do observe that
the "natural" gradient direction improves the loss better than the usual L2 gradient.
•Third, our fan-in weights are not set to 0 but directly to their optimal values (at first order).
We now prove the proposition B.1.
Proof.Indeed,
arg min
H||D−HB||2= arg min
H||HB||2−2⟨D,HB⟩ (29)
= arg min
H||HB||2−2||HB||/angbracketleftbigg
D,HB
||HB||/angbracketrightbigg
(30)
= arg min
hU,||HB||=h,U=HB
||HB||h2−2h⟨D,U⟩ (31)
(32)
We noteU∗:= arg max
U=HB
||HB||⟨D,U⟩which depends on BandDbut does not depend on h. Then :
arg min
H||D−HB||2= arg min
hU∗,h⩾0h2−2h⟨D,U∗⟩ (33)
=h∗U∗(34)
With the convention that0
||0||= 0.
23Published in Transactions on Machine Learning Research (June/2024)
B.2 NORTH Preactivation
In paper Maile et al. (2022), fan-out weights are initialized to 0 while fan-in weights are initialized as
αi=S−1Bl−2VAl−1riwhereriis a random vector and VZl−1∈Rn,|ker(AT
l−1)|is a matrix consisting of
orthogonal vectors of the kernel of pre-activations Al−1. In our paper fan-in weights are initialized as
αi=S−1
2Bl−2VgoalT
projvi=S−1
2Bl−2Pker(Bl−1)VgoalTvi, wherePKer(Bl−1)∈Rn,nis the projector on the
kernel of the linear application Bl−1andviare the right eigenvectors of the matrix S−1
2N.
The main difference is thus that we use the backpropagation to find the best viorridirectly, while the
NORTH approach tries random directions rito explore the space of possible neuron additions.
C Proofs of Part 3
C.1 Proposition 3.1
Denoting by M+the generalized (pseudo-)inverse of M, we have:
δW∗
l=1
nVgoallBT
l−1/parenleftbigg1
nBl−1BT
l−1/parenrightbigg+
andVl
0=1
nVgoallBT
l−1/parenleftbigg1
nBl−1BT
l−1/parenrightbigg+
Bl−1.
Proof.Fully connected layers
Consider the function
g(δW) :=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoall−δWBl−1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
F(35)
then:
g(δW+H) =||Vgoall−δWBl−1−HBl−1||2(36)
=g(δW)−2/angbracketleftig
Vgoall−δWBl−1,HBl−1/angbracketrightig
+o(||H||) (37)
=g(δW)−2/angbracketleftig/parenleftig
Vgoall−δWBl−1/parenrightig
BT
l−1,H/angbracketrightig
+o(||H||) (38)
By identification ∇δWg(δW) =−2/parenleftig
Vgoall−δWBl−1/parenrightig
BT
l−1, and thus:
∇δWg(δW) = 0 =⇒VgoallBT
l−1=δWBl−1BT
l−1
Using that gis convex and the definition of the generalized inverse, we get:
δW∗
l=1
nVgoallBT
l−1/parenleftbigg1
nBl−1BT
l−1/parenrightbigg+
as one solution.
Convolutional layers
For convolutional layers we aim to solve (where we dropped the index l−1for readability):
arg min
δW||Vgoal−ConvδW(B)||F(39)
We convert this in a linear regression by transforming the convolution in a matrix multiplication. First we
reshape and permute:
24Published in Transactions on Machine Learning Research (June/2024)
•δW∈(C[+1],C,d [+1],d[+1])is transformed in δWF∈(C[+1],Cd[+1]d[+1])
•Vgoal∈(n,C[+1],H[+1],W[+1])is transformed in VgoalF∈(nH[+1]W[+1],C[+1])
Then we can define Bc∈(n,Cd [+1]d[+1],H[+1]W[+1])and its reshaped version Bc
F∈
(nH[+1]W[+1],Cd[+1]d[+1])that satisfies that Bc
FδWT
F∈(nH[+1]W[+1],C[+1])is a reshaped version
ofConvδW(B). (Bccan be easily computed using torch.nn.Unfold .)
Hence Equation (39) becomes:
arg min
δWF/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoalF−Bc
FδWT
F/vextendsingle/vextendsingle/vextendsingle/vextendsingle
F(40)
Using the same reasoning that for fully connected layers, we get an optimal solution:
(δW∗
F)T=/parenleftbig
(Bc
F)TBc
F/parenrightbig+(Bc
F)TVgoalF(41)
C.2 Proposition 3.2
We define the matrices N:=1
nBl−2/parenleftig
Vgoall
proj/parenrightigT
andS:=1
nBl−2BT
l−2. Let us denote its SVD by S=
OΣOT, and noteS−1
2:=O√
Σ−1OTand consider the SVD of the matrix S−1
2N=/summationtextR
k=1λkukvT
kwith
λ1⩾...⩾λR⩾0, whereRis the rank of the matrix N. Then:
Proposition C.1 (3.2).The solution of equation 8 can be written as:
•optimal number of neurons: K∗=R
•their optimal weights: (α∗
k,ω∗
k) = (√λkS−1
2uk,√λkvk)fork= 1,...,R.
Moreover for any number of neurons K⩽R, and associated scaled weights θK,∗
↔, the expressivity gain and
the first order in ηof the loss improvement due to the addition of these Kneurons are equal and can be
quantified very simply as a function of the eigenvalues λk:
Ψl
θ⊕θK,∗
↔= Ψl
θ−K/summationdisplay
k=1λ2
k
for fully-connected layers, with an inequality instead ( ⩽) for convolutional layers.
Proof.
To facilitate reading we remove the layer index of each quantity, ie B:=Bl−2,Vl(A,Ω) :=V(A,Ω)and
Vgoall
proj:=Vgoalproj. We fixnandx1,....,xnon which we solve the expressivity bottleneck formula.
To solve this problem, we consider the input of the incoming connections Band the desired change in the
output of the outgoing connections Vgoalproj. Hence if we note L(A)andL(Ω)the additional connections
of the expanded representation and σthe non linearity, we optimize the following proxy problem:
arg min
A,Ω1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle(L(Ω)◦σ◦L(A))(B)−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
Tr(42)
We solve this problem at first order by linearizing the non-linearity σ. We denote Lin(a,b)(W)the fully
connected layer with input size a, output size band weight matrix W. We also note C[+1]andC[−1]the
25Published in Transactions on Machine Learning Research (June/2024)
layer width at layer l+ 1andl−1with the convention that C[0] is the dimension of the input x. With those
notations, for fully connected layers, we have for the additions of Kneurons:
arg min
A,Ω1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleLin(C[+1],K)(Ω)(Lin(K,C[−1])(A)(B))−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
2(43)
With the same notations, for convolutional layers, we have for the additions of Kintermediate channels:
arg min
A,Ω1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleConv (C[+1],K)(Ω)(Conv (K,C[−1])(A)(B))−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
2(44)
If we noteV(A,Ω)the result of Bafter applying the layers parametrized by AandΩ, in both cases we
aim to optimize:
arg min
A,Ω1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleV(A,Ω)−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
Tr(45)
First we will transform the resolution of the problem in solving the following optimization problem:
arg min
A,Ω/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS1
2AΩT−S−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
2(46)
whereSdepends ofBandNofBandVgoalproj.
If we noteS=OΛOTthe SVD ofS, we define the square root of SasS1
2:=O√
ΛOTandS−1
2:=
O√
Λ−1OTwith the convention 0−1= 0.
C.2.1 Fully connected layers
For a fully connected layer, we have
V(A,Ω) =Lin(C[+1],K)(Ω)(Lin(K,C[−1])(A)(B)) =ΩATB (47)
Lemma C.1. LetY∈R(t,n),X∈R(s,n),C∈R(t,s)
We define:
S:=1
nXXT∈R(s,s) (48)
N:=1
nXYT∈R(s,t) (49)
1
n||CX−Y||2=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleCS1
2−NTS−1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
+1
n||Y||2(50)
Proof in C.2.2
Hence using Lemma C.5 with C←ΩAT,Y←VgoalprojandB←X, we have:
1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleLin(C[+1],K)(Ω)(Lin(K,C[−1])(A)(B))−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
=
1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleΩATB−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
=
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleΩATS1
2−NTS−1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
+1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2(51)
26Published in Transactions on Machine Learning Research (June/2024)
With:
S:=1
nBBT∈R(C[−1],C[−1]) (52)
N:=1
nBVgoalT
proj∈R(C[−1],C[+1]) (53)
C.2.2 Convolutional connected layers
We haveA∈R(K,C [−1],d,d)andΩ∈R(C[+1],K,d [+1],d[+1])whered,d[+1]is the kernel size at land
l+ 1. We noteAFthe flatten and transposed version of Aof shape (C[−1]dd,K )andαk:=AF[:,k]∈
(C[−1]dd,1). We will now consider Ωwith the last order flatten i.e.Ω∈R(C[+1],K,d [+1]d[+1]). We also
noteωk,m:=Ω[m,k]∈(d[+1]d[+1],1). Using this we define Ω[m]F:=
ωT
1,m
...
ωT
K,m
∈(K,d[+1]d[+1])and
ΩF:=/parenleftbig
Ω[1]F···Ω[m]F/parenrightbig
∈(K,C [+1]d[+1]d[+1]).
We define the tensor Tsuch that for a pixel jof the output of the convolutional layer, Tjis a linear
application that select the pixels of the input of the convolutional layer that are used to compute the
pixeljof the output in a flatten version image (flatten only on the space not on the channels). T∈
R(H[+1]W[+1],d[+1]d[+1],HW )whereHandWare the height and width of the intermediate image and
H[+1]andW[+1]are the height and width of the output image.
Aspreviously,wehave Bctheunfoldedversionof BsuchthatBc∈R(n,C[−1]dd,HW )satisfying Conv (Bi)
is equal with the correct reshape to ABc
i.
In addition, we use jas an index on the space of pixels instead of having a couple h,wfor height and width.
With those notations we have:
V(A,Ω)[i,m,j ] =Conv (C[+1],K)(Ω)(Conv (K,C[−1])(A)(Bi))[m,j] (54)
=K/summationdisplay
kωT
m,kTj(Bc
i)Tαk (55)
In the following for simplicity, we note Bt
i,j:=Tj(Bc
i)T. To find the best neurons to add we solve the
expressivity bottleneck as :
arg min
A,Ω1
n/summationdisplay
i/summationdisplay
j/summationdisplay
m/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoalproj(j,m)
i−K/summationdisplay
k=1ωT
m,kBt
i,jαk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(56)
(57)
27Published in Transactions on Machine Learning Research (June/2024)
Using the properties of the trace, it follows that :
/summationdisplay
i,j,m/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoalproj(j,m)
i−K/summationdisplay
k=1ωT
m,kBt
i,jαk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
=/summationdisplay
i,j,m/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoalproj(j,m)
i−/summationdisplay
kTr/parenleftbig
Bt
i,jαkωT
m,k/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(58)
=/summationdisplay
i,j,m/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoalproj(j,m)
i−Tr
Bt
i,jFm/bracehtipdownleft/bracehtipupright/bracehtipupleft/bracehtipdownright/summationdisplay
kαkωT
m,k
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(59)
=/summationdisplay
i,j,m/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoalproj(j,m)
i−flat(Bt
i,j)Tflat(Fm)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(60)
=/summationdisplay
i,j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoalproj(j)
i−flat(Bt
i,j)TF/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(61)
(62)
WithF:=/parenleftbig
flat(F1)... flat (FC[+1])/parenrightbig
.
We remark that V(A,Ω)is a linear function of the matrix Fwhich implies that the solution of 56 is the
same as the one for linear layer. Replacing ΩAbyFin 47 and following the same reasoning as for linear
layer, it follows that 56 is equivalent to :
arg min
F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS1
2F−S−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
2(63)
withS:=/summationtext
i,jflat(Bt
i,j)flat(Bt
i,j)TandN:=/summationtext
i,jVgoalprojjflat(Bt
i,j)T.
However, we remark that the dimension of S∈R(C[−1]d[+1]2d2,C[−1]d[+1]2d2)is quite large and that
computing the SVD of such matrix is costly. To avoid expensive computation, we approximate 56 by defining
the matrixSandNas 98 and 100. We now prove that 3.2, 3.4 and Equation (10) still hold with such
new definitions of SandN.
Lemma C.2. Letr:= min(Bt
1,1.shape ), we define:
S:=r
nn/summationdisplay
i=1H[+1]W[+1]/summationdisplay
j=1(Bt
i,j)T(Bt
i,j)∈(C[−1]dd,C [−1]dd) (64)
Nm:=1
nn,H[+1]W[+1]/summationdisplay
i,jVgoal proji,j,m(Bt
i,j)T∈(C[−1]dd,d[+1]d[+1]) (65)
N:=/parenleftbig
N1···NC[+1]/parenrightbig
∈(C[−1]dd,C [+1]d[+1]d[+1]) (66)
We have:
1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleV(A,Ω)−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
⩽/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS1
2AFΩF−S−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
+1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoal proj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(67)
Proof in C.2.2
Lemma C.3. ForS∈R(s,s),N∈R(s,t),A∈R(s,K),Ω∈R(K,t).
We noteUΛVthe singular value decomposition of S−1
2NandUKthe firstKcolumns ofU,VKthe first
Klines ofV,ΛKthe firstKsingular values of ΛandΛK+1:the other singular values of Λ.
We define:
A∗:=S−1
2UK/radicalbig
ΛK (68)
Ω∗:=/radicalbig
ΛKVK (69)
28Published in Transactions on Machine Learning Research (June/2024)
Then:
min
A,Ω1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleV(A,Ω)−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
⩽1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleV(A∗,Ω∗)−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
=−||ΛK||2+1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(70)
with equality in the linear case. Using expressivity bottlenecks notations, this rewrites:
Ψl
θ⊕θK↔∗⩽Ψl
θ−K/summationdisplay
k=1λ2
k. (71)
Proof.Using Lemma C.5 and Lemma C.7:
1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleV(A,Ω)−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
⩽/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS1
2AΩ−S−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
+1
nΨl
θ/bracehtipdownleft/bracehtipupright/bracehtipupleft/bracehtipdownright/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(72)
Hence we minimize the second term of the right term, that is:
arg min
A,Ω/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS1
2AΩ−S−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle (73)
As we suppose that Sis invertible, we can use the change of variable /tildewideA=S1
2Athus we have:
min
A,Ω/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS1
2AΩ−S−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle= min
/tildewideA,Ω/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/tildewideAΩ−S−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle (74)
The solution of such problems is given by the paper Eckart & Young (1936) and is:
/tildewideA∗=UK/radicalbig
ΛK (75)
Ω∗=/radicalbig
ΛKVK (76)
To recoverA∗we simply have to multiply by S−1
2on the left side of /tildewideA∗. By definition of the SVD and the
construction of (A∗,Ω∗)we have:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS1
2A∗Ω∗−S−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
=||ΛK+1:||2−||Λ||2=−||ΛK||2(77)
Using this and Equation (72) we immediately get the desired Equation (70). To conclude, we can also rewrite
this with the bottleneck expression:
Ψθ⊕θK↔:= min
A,Ω1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleV(A,Ω)−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
⩽Ψl
θ−K/summationdisplay
k=1λ2
k (78)
We now prove all the lemmas.
Lemma C.4. ForS∈R(s,s),N∈R(s,t),C∈R(t,s).
IfN=S1
2S−1
2N, we have:
/angbracketleftbig
CT,SCT/angbracketrightbig
−2/angbracketleftbig
N,CT/angbracketrightbig
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS1
2CT−S−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(79)
29Published in Transactions on Machine Learning Research (June/2024)
Proof. •For the first term we have:
/angbracketleftbig
CT,SCT/angbracketrightbig
=/angbracketleftig
CT,S1
2S1
2CT/angbracketrightig
(80)
=/angbracketleftig
S1
2CT,S1
2CT/angbracketrightig
(81)
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS1
2CT/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(82)
•For the second term we have:
/angbracketleftbig
N,CT/angbracketrightbig
=/angbracketleftig
S1
2S−1
2N,CT/angbracketrightig
(83)
=/angbracketleftig
S−1
2N,S1
2CT/angbracketrightig
(84)
Hence we have that:
/angbracketleftbig
CT,SCT/angbracketrightbig
−2/angbracketleftbig
N,CT/angbracketrightbig
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS1
2CT/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
−2/angbracketleftig
S−1
2N,S1
2CT/angbracketrightig
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(85)
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS1
2CT−S−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(86)
Lemma C.5. LetY∈R(t,n),X∈R(s,n),C∈R(t,s)
We define:
S:=1
nXXT∈R(s,s) (87)
N:=1
nXYT∈R(s,t) (88)
1
n||CX−Y||2=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleCS1
2−NTS−1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
+1
n||Y||2(89)
Proof.By developing the scalar product we get:
1
n||CX−Y||2=1
n||Y||2−2/angbracketleftbigg
Y,1
nCX/angbracketrightbigg
+1
n||CX||2(90)
=1
n||Y||2−2/angbracketleftbigg
Y,1
nCX/angbracketrightbigg
+1
n⟨CX,CX⟩ (91)
=1
n||Y||2−2/angbracketleftbigg
YT,1
n(CX)T/angbracketrightbigg
+1
n/angbracketleftig
(CX)T,(CX)T/angbracketrightig
(92)
=1
n||Y||2−2/angbracketleftbigg1
nXYT,CT/angbracketrightbigg
+/angbracketleftbigg
CT,1
nXXTCT/angbracketrightbigg
(93)
We now use the two following lemma:
Lemma C.6. LetY∈R(t,n),X∈R(s,n)andS:=XXT∈R(s,s).
S1
2S−1
2XYT=XYT(94)
Proof.Let decompose YonIm(XT)⊕⊥ker(X):Y=XTI+K.
XYT=XXTI+XK=XXTI=SI
HenceXYT∈Im(S), hence asS|Im(S)is invertible, we have: S−1
2S1
2XYT=XYT.
30Published in Transactions on Machine Learning Research (June/2024)
Continuing the demonstration from Equation (93), by applying Lemma C.4, we have:
1
n||CX−Y||2=1
n||Y||2−2/angbracketleftbig
N,CT/angbracketrightbig
+/angbracketleftbig
CT,SCT/angbracketrightbig
(95)
=1
n||Y||2+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS1
2CT−S−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(96)
=1
n||Y||2+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleCS1
2−NTS−1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(97)
Lemma C.7. Letr:= min(Bt
1,1.shape ), we define:
S:=r
nn/summationdisplay
i=1H[+1]W[+1]/summationdisplay
j=1(Bt
i,j)T(Bt
i,j)∈(C[−1]dd,C [−1]dd) (98)
Nm:=1
nn,H[+1]W[+1]/summationdisplay
i,jVgoal proji,j,m(Bt
i,j)T∈(C[−1]dd,d[+1]d[+1]) (99)
N:=/parenleftbig
N1···NC[+1]/parenrightbig
∈(C[−1]dd,C [+1]d[+1]d[+1]) (100)
We have:
1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleV(A,Ω)−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
⩽/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS1
2AFΩF−S−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
−/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleS−1
2N/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
+1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoal proj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(101)
Proof.We have:
1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleV(A,Ω)−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
=1
n||V(A,Ω)||2−2
n/angbracketleftig
V(A,Ω),Vgoalproj/angbracketrightig2
+1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(102)
We will now simplify the first two terms separately.
Norm simplification
Lemma C.8. For any square matrix A∈R(n,n),Tr(A)2⩽rank(A)||A||2.
Proof.Using the truncated SVD we have A=UΣVwith Σa diagonal and U∈R(n,rank(A)),V∈
R(rank(A),n)truncated orthonormal matrices.
We have:
Tr(A)2= Tr(UΣV)2(103)
= Tr(VUΣ)2(104)
=⟨VU,Σ⟩2(105)
(Cauchy-Swarz )⩽||VU||2||Σ||2(106)
AsU,Vare truncated orthonormal matrices, we have:
||VU||2= Tr(UTVTVU) = Tr(UTU) = Tr(Irank(A)) = rank(A)
Hence:
Tr(A)2⩽rank(A)||Σ||2(107)
31Published in Transactions on Machine Learning Research (June/2024)
AsU,Vare truncated orthonormal matrices, we have:
||Σ||2= Tr(ΣTΣ) = Tr((VΣU)UΣV) = Tr(ATA) =||A||2
We conclude that:
Tr(A)2⩽rank(A)||A||2(108)
Lemma C.9. ForM∈(m,n),(uk)k∈[[K]]∈(m)K,(vk)k∈[[K]]∈(n)Kand withW:=/summationtext
k∈JKKvkuT
k∈
(n,m)we have:/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
k∈JKKuT
kMvk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
= Tr (MW )2(109)
Proof.Leti∈I:
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
k∈JKKvuT
kMvk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
=
/summationdisplay
k∈JKKuT
kMvk
2
(110)
= Tr
/summationdisplay
k∈JKKuT
kMvk
2
(111)
= Tr
M/summationdisplay
k∈JKKvkuT
k
2
(112)
= Tr (MW )2(113)
Lemma C.10. For(Mi)i∈I∈(m,n)Isuch that∀i∈I,rank(MiW)⩽Hand withW∈(n,m)we have:
/summationdisplay
i∈ITr (MiW)2⩽/angbracketleftigg
W,H/summationdisplay
i∈IMT
iMiW/angbracketrightigg
(114)
Proof.Leti∈I:
Using Lemma C.8 with A←MiW
Tr (MiW)2⩽rank(MiW)||MiW||2(115)
⩽H||MiW||2H:= min(Mi.shape ) (116)
Hence we have:
/summationdisplay
i∈I/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
k∈JKKuT
kMivk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
⩽H/summationdisplay
i∈I||MiW||2(117)
=H/summationdisplay
i∈I⟨MiW,MiW⟩ (118)
=H/summationdisplay
i∈I/angbracketleftbig
W,MT
iMiW/angbracketrightbig
(119)
=H/angbracketleftigg
W,/summationdisplay
i∈IMT
iMiW/angbracketrightigg
(120)
32Published in Transactions on Machine Learning Research (June/2024)
Using first Lemma C.9 with uk←ωk,m,vk←αkandM←Bt
i,j, we have:
1
n||V(A,Ω)||2=1
nC[+1]/summationdisplay
mn,H[+1]W[+1]/summationdisplay
i,j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleK/summationdisplay
kωT
k,m(Bt
i,j)αk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(121)
=1
nC[+1]/summationdisplay
mn,H[+1]W[+1]/summationdisplay
i,jTr/parenleftigg
Bt
i,jK/summationdisplay
kαkωT
k,m/parenrightigg2
(122)
=1
nC[+1]/summationdisplay
mn,H[+1]W[+1]/summationdisplay
i,jTr/parenleftbig
Bt
i,jAFΩ[m]F/parenrightbig2(123)
Using Lemma C.10 with i←(i,j),Mi←Bt
i,jandW←AFΩ[m]F:
⩽C[+1]/summationdisplay
m/angbracketleftigg
AFΩ[m]F,r
nn,H[+1]W[+1]/summationdisplay
i,j(Bt
i,j)T(Bt
i,j)AFΩ[m]F/angbracketrightigg
(124)
=C[+1]/summationdisplay
m⟨AFΩ[m]F,SAFΩ[m]F⟩ (125)
=⟨AFΩF,SAFΩF⟩ (126)
Scalar product simplification
Lemma C.11. ForM∈(m,n),u∈(m),v∈(n)we have:
uTMv =/angbracketleftbig
vuT,MT/angbracketrightbig
(127)
Proof.
uTMv = (uTMv)T(128)
=vTMTu (129)
=/angbracketleftbig
v,MTu/angbracketrightbig
(130)
=/angbracketleftbig
vuT,MT/angbracketrightbig
(131)
We have:
1
n/angbracketleftig
V(A,Ω),Vgoalproj/angbracketrightig2
=1
nC[+1]/summationdisplay
mn,H[+1]W[+1]/summationdisplay
i,jK/summationdisplay
kωT
k,mBt
i,jαkVgoalproji,j,m(132)
(Vgoalproji,j,m∈(1)) =C[+1]/summationdisplay
mK/summationdisplay
kωT
k,m1
nn,H[+1]W[+1]/summationdisplay
i,j(Bt
i,jVgoalproji,j,m)αk (133)
33Published in Transactions on Machine Learning Research (June/2024)
Using Lemma C.11 with M←/summationtextn,H[+1]W[+1]
i,jVgoalproji,j,m(Bt
i,j)T
=C[+1]/summationdisplay
m/angbracketleftiggK/summationdisplay
kαkωT
k,m,1
nn,H[+1]W[+1]/summationdisplay
i,jVgoalproji,j,m(Bt
i,j)T/angbracketrightigg
(134)
=C[+1]/summationdisplay
m⟨AFΩ[m]F,Nm⟩ (135)
=⟨AFΩF,N⟩ (136)
Conclusion In total, we get:
1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleV(A,Ω)−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
⩽⟨AFΩF,SAFΩF⟩−2⟨AFΩF,N⟩+1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(137)
If we suppose that Sis invertible, we can apply Lemma C.4 and get the result.
Proposition C.2. Solving 8 is equivalent to minimizing the loss Lat order one in Vl. Furthermore
performing an update of architecture with γδW∗(5) and a neuron addition with γθK
↔∗(3.2), has an impact
on the loss at first order in γas :
L(fθ⊕θK↔) =L(fθ)−γ/parenleftig
σ′
l−1(0)∆θK,∗
↔+ ∆δW∗/parenrightig
+o(γ) (138)
with
∆θK,∗
↔:=1
n/angbracketleftig
Vl
goalproj,Vl(θK,∗
↔)/angbracketrightig
Tr=K/summationdisplay
k=1λ2
k (139)
∆δW∗:=1
n/angbracketleftbig
Vl
goal,Vl(δW∗)/angbracketrightbig
Tr⩾0 (140)
To prove such proposition we use the following lemma :
Lemma C.12. We noteV(A,Ω)the result ofBafter applying the layers parameterized by AandΩ. Let
us considerV(A∗,Ω∗)whereA∗andΩ∗are defined in 3.2. Then:
1
n/angbracketleftig
Vgoal proj,V(A∗,Ω∗)/angbracketrightig
=||ΛK||2(141)
Proof.Starting from Lemma C.3
1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleV(A∗,Ω∗)−Vgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
=−||ΛK||2+1
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVgoalproj/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
(142)
Hence by developing the norm, we have:
1
n||V(A∗,Ω∗)||2−2
n/angbracketleftig
V(A∗,Ω∗),Vgoalproj/angbracketrightig
=−||ΛK||2(143)
Moreover by construction we have1
n||V(A∗,Ω∗)||2=||ΛK||2and therefore we get:
−2
n/angbracketleftig
V(A∗,Ω∗),Vgoalproj/angbracketrightig
=−2||ΛK||2(144)
which concludes the proof.
34Published in Transactions on Machine Learning Research (June/2024)
We now prove the main proposition. Suppose that each quantity is added to the architecture with an
amplitude factor γi.e.the best update is then γδW∗and the new neurons are {√γα∗
i,√γω∗
i}i.
Using the Fréchet derivative on γ, we have the following:
L(al+γδal) =L(al) +/angbracketleftbig
∇alL,γδal/angbracketrightbig
+o(γ) (145)
On one hand, performing an update of architecture, ie W∗← −W+γδW∗, changes the activation function
albyγδal
u:=V(γδW∗). Then, as explained in Appendix A.5, adding neurons (A∗,Ω∗)at layerl−1
changes the activation function alby :
γ δal
a=σ′(0)γV(A∗,Ω∗) +o(γ). (146)
We now suppose δal
u̸=−δal
aand perform a first order development in γ. Then combining Equations (145)
and (146), we have :
L(A∗,Ω∗) =L+/angbracketleftbig
∇alL,γ/parenleftbig
δal
u+δal
a/parenrightbig/angbracketrightbig
+o(γ). (147)
Using thatvgoal(xi) :=−∇al(xi)ℓ(xi)and thatL=1
n/summationtext
iℓ(xi), it follows that :
L(A∗,Ω∗) =L−1
n/angbracketleftbig
Vgoal,γ/parenleftbig
δal
u+δal
a/parenrightbig/angbracketrightbig
+o(γ) (148)
=L−γ
n/parenleftig/angbracketleftbig
Vgoal,δal
u/angbracketrightbig
+/angbracketleftbig
Vgoal−V(γδW∗),δal
a/angbracketrightbig
+γ/angbracketleftbig
δal
u,δal
a/angbracketrightbig/parenrightig
+o(γ).(149)
Using C.12 we have :
L(A∗,Ω∗) =L−γ/parenleftigg
σ′(0)K/summationdisplay
k=1λ2
k+1
n/angbracketleftbig
Vgoal,δal
u/angbracketrightbig/parenrightigg
+o(γ). (150)
Noteontheapproximationforconvolutionallayer. Bydevelopingtheexpression ||V−Vgoalproj||2, we
remark that minimizing ||V−Vgoalproj||2overVis equivalent to maximizing ⟨V,Vgoalproj⟩with a constraint
on the norm of V. This constraint lies in the functional space of the activities and can be reformulated
in the parameter space with the matrix Sas||AΩT||S=||V||. By changing the matrix Sfor another
positive semi-definite matrix Spseudo, we modify the metric on Vand obtain a pseudo-solution S−1
pseudoN.
C.3 Proposition 3.4 and Corollary 3.2
Proposition C.3. 3.4 SupposeSis semi definite, we note S=S1
2S1
2. Solving equation 8 is equivalent
to find the K first eigenvectors αkassociated to the K largest eigenvalues λof the generalized eigenvalue
problem:
NNTαk=λSαk (151)
Corollary 2. (3.2)For all integers m,m′such thatm+m′⩽R, at order one in η, addingm+m′neurons
simultaneously according to the previous method is equivalent to adding mneurons then m′neurons by
applying successively the previous method twice.
Proof
To prove 3.4, we show that the solution of 151 and the formula of 3.2 are collinear.
35Published in Transactions on Machine Learning Research (June/2024)
Solving 151 is equivalent to maximizing the following generalized Rayleigh quotient (which is solvable by the
LOBPCG technique):
α∗= arg max
ααTNNTα
αTSα(152)
p∗= arg max
p=S1/2αpTS−1
2NNTS−1
2p
pTp(153)
p∗= arg max
||p||=1||NTS−1
2p|| (154)
α∗=S−1
2p∗(155)
Considering the SVD of NTS−1
2=/summationtextR
r=1λrerfT
r, thenS−1
2NNTS−1
2=/summationtextR
r=1λ2
rfrfT
r, because
j̸=i=⇒eT
iej= 0andfT
ifj= 0. Hence maximizing the first quantity is equivalent to p∗
k=fk, then
αk=S−1
2fk, which matches the formula of Proposition 3.2. The same reasoning can be applied to ωk.
We prove second corollary 3.2 by induction. Note that v(θK,∗
↔,x) =o(η), then form=m′= 1:
al(x)t+1=al(x)t+v(θ1,∗
↔,x) +o(η) (156)
Remark that vgoal(x)is a function of al(x),i.e.vgoal(x) :=g(al(x)). Then suppose that L(f(x),y))is
twice differentiable in al(x). It follows that g(al(x))is differentiable and :
vgoalt+1(x) =g(at
l(x) +v(θ1,∗
↔,x)) (157)
=g(at
l(x)) +∇at
l(x)g(at
l(x))Tv(θ1,∗
↔,x) +o(η2) (158)
=vgoalt(x) +η∂2L(fθ(x),y)
∂al(x)2v(θ1,∗
↔,x) +o(η2) (159)
=vgoalt(x) +o(η) (160)
Adding the second neuron we obtain the minimization problem:
arg min
α2,ω2||Vgoalt−V(α2,ω2)||+o(η) (161)
C.4 About equivalence of quadratic problems
Problems 8 and 7 are generally not equivalent, but might be very close, depending on layer sizes and number
ofsamples. Thedifferencebetweenthetwoproblemsisthatinonecaseoneminimizesthequadraticquantity:
/vextenddouble/vextenddouble/vextenddoubleVl(θK
↔) +Vl(M)−Vgoall/vextenddouble/vextenddouble/vextenddouble2
w.r.t.MandθK
↔jointly, while in the other case the problem is first minimized w.r.t. Mand then w.r.t.
θK
↔. The latter process, being greedy, might thus provide a solution that is not as optimal as the joint
optimization.
We chose this two-step process as it intuitively relates to the spirit of improving upon a standard gradient
descent: we aim at adding neurons that complement what the other ones have already done. This choice is
debatable and one could solve the joint problem instead, with the same techniques.
The topic of this section is to check how close the two problems are. To study this further, note that
Vl(M) =δWlBl−1whileVl(θK
↔) =/summationtextK
k=1ωkBT
l−2αk. The rank of Bl−1ismin(nS,nl−1)wherenSis the
number of samples and nl−1the number of neurons (post-activities) in layer l−1, while the rank of Bl−2is
min(nS,nl−2)wherenl−2is the number of neurons (post-activities) in layer l−2. Note also that the number
of degrees of freedom in the optimization variables δWlandθK
↔= (ωk,αk)is much larger than these ranks.
36Published in Transactions on Machine Learning Research (June/2024)
Small sample case. If the number nSof samples is lower than the number of neurons nl−1andnl−2
(which is potentially problematic, see Section E.1), then it is possible to find suitable variables δWlandθK
↔
to form any desired Vl(M)andVl(θK
↔). In particular, if nS⩽nl−1⩽nl−2, one can choose Vl(θK
↔)to be
Vgoall−Vl(M)and thus cancel any effect due to the greedy process in two steps. The two problems are
then equivalent.
Large sample case. On the opposite, if the number of samples is very large (compared to the number of
neuronsnl−1andnl−2), then the lines of matrices Bl−1andBl−2become asymptotically uncorrelated, under
the assumption of their independence (which is debatable, depending on the type of layers and activation
functions). Thus the optimization directions available to Vl(M)andVl(θK
↔)become orthogonal, and
proceeding greedily does not affect the result, the two problems are asymptotically equivalent.
In the general case, matrices Bl−1andBl−2are not independent, though not fully correlated, and the
number of samples (in the minibatch) is typically larger than the number of neurons; the problems are then
different.
Note that technically the ranks could be lower, in the improbable case where some neurons are perfectly
redundant, or, e.g., if some samples yield exactly the same activities.
D Section About greedy growth sufficiency and TINY convergence with more
details and proofs
One might wonder whether a greedy approach on layer growth might get stuck in a non-optimal state.
Bygreedywe mean that every neuron added has to decrease the loss. We derive the following series of
propositions in this regard. Since in this work, we add neurons layer per layer independently, we study here
the case of a single hidden layer network, to spot potential layer growth issues. For the sake of simplicity, we
consider the task of least square regression towards an explicit continuous target f∗, defined on a compact
set. That is, we aim at minimizing the loss:
inf/summationdisplay
x∈D∥f(x)−f∗(x)∥2(162)
wheref(x)is the output of the neural network and Dis the training set.
We start with an optional introductory section D.1 about greedy growth possibilities, then prepare lemmas
in Sections D.2 and D.3 that will be used in Section D.4 to show that one can keep on adding neurons to
a network (without modifying already existing weights) to make it converge exponentially fast towards the
optimal function. Then in Section D.6 we present a growth method that explicitly overfits each dataset
sample one by one, thus requiring only nneurons, thanks to existing weights modification. Finally, more
importantly, in Section D.7, we show that actually any reasonable growth method that follows a certain
optimization protocol (this includes TINY completed by random neuron additions if necessary) will reach 0
training error in at most nneuron additions.
D.1 Possibility of greedy growth
Proposition D.1 (Greedy completion of an existing network) .Iffis notf∗yet, there exists a set of
neurons to add to the hidden layer such that the new function f′will have a lower loss than f.
One can even choose the added neurons such that the loss is arbitrarily well minimized.
Proof.The classic universal approximation theorem about neural networks with one hidden layer Pinkus
(1999) states that for any continuous function g∗defined on a compact set ω, for any desired precision γ,
and for any activation function σprovided it is not a polynomial, then there exists a neural network gwith
one hidden layer (possibly quite large when γis small) and with this activation function σ, such that
∀x,∥g(x)−g∗(x)∥⩽γ (163)
37Published in Transactions on Machine Learning Research (June/2024)
We apply this theorem to the case where g∗=f∗−f, which is continuous as f∗is continuous, and
fis a shallow neural network and as such is a composition of linear functions and of the function σ,
that we will suppose to be continuous for the sake of simplicity. We will suppose that fis real-valued
for the sake of simplicity as well, but the result is trivially extendable to vector-valued functions (just
concatenate the networks obtained for each output independently). We choose γ=1
10∥f∗−f∥L2, where
⟨a|b⟩L2=1
|ω|/integraltext
x∈ωa(x)b(x)dx. This way we obtain a one-hidden-layer neural network gwith activation
functionσ, and we write a(x) =g(x)−g∗(x)the error term, it follows that:
∀x∈ω,−γ⩽g(x)−g∗(x)⩽γ (164)
∀x∈ω, g(x) =f∗(x)−f(x) +a(x) (165)
with∀x∈ω,|a(x)|⩽γ.
Then:
∀x∈ω, f∗(x)−(f(x) +g(x)) =−a(x) (166)
∀x∈ω,(f∗(x)−h(x))2=a2(x) (167)
withhbeing the function corresponding to a neural network consisting of concatenating the hidden layer
neurons offandg, and consequently summing their outputs.
∥f∗−h∥2
L2=∥a∥2
L2 (168)
∥f∗−h∥2
L2⩽γ2=1
100∥f∗−f∥2
L2 (169)
and consequently the loss is reduced indeed (by a factor of 100 in this construction).
The same holds in expectation or sum over a training set, by choosing γ=1
10/radicalig
1
|D|/summationtext
x∈D∥f(x)−f∗(x)∥2,
as Equation (167) then yields:
/summationdisplay
x∈D(f∗(x)−h(x))2=/summationdisplay
x∈Da2(x)⩽1
100/summationdisplay
x∈D(f∗(x)−f(x))2(170)
which proves the proposition as stated.
For more general losses, one can consider order-1 (linear) development of the loss and ask for a network g
that is close to (the opposite of) the gradient of the loss.
Proof of the additional remark. The proof in Pinkus (1999) relies on the existence of real values cnsuch that
then-th order derivatives σ(n)(cn)are not 0. Then, by considering appropriate values arbitrarily close to
cn, one can approximate the n-th derivative of σatcnand consequently the polynomial cnof ordern. This
standard proof then concludes by density of polynomials in continuous functions.
Provided the activation function σis not a polynomial, these values cncan actually be chosen arbitrarily,
in particular arbitrarily close to 0. This corresponds to choosing neuron input weights arbitrarily close to
0.
Proposition D.2 (Greedy completion by one single neuron) .Iffis notf∗yet, there exists a neuron to
add to the hidden layer such that the new function f′will have a lower loss than f.
Proof.From the previous proposition, there exists a finite set of neurons to add such that the loss will be
decreased. In this particular setting of L2regression, or for more general losses if considering small function
moves, this means that the function represented by this set of neurons has a strictly negative component over
the gradient gof the loss ( g=−2(f∗−f)in the case of the L2regression). That is, denoting by aiσ(Wi·x)
theseNneurons: /angbracketleftiggN/summationdisplay
i=1aiσ(wi·x)/vextendsingle/vextendsingleg/angbracketrightigg
L2=K < 0 (171)
38Published in Transactions on Machine Learning Research (June/2024)
i.e.
N/summationdisplay
i=1⟨aiσ(wi·x)|g⟩L2=K < 0 (172)
We have:
0>1
NK=1
NN/summationdisplay
i=1⟨aiσ(wi·x)|g⟩L2⩾N
min
i=1⟨aiσ(wi·x)|g⟩L2 (173)
Then necessarily at least one of the Nneurons satisfies
⟨aiσ(wi·x)|g⟩L2⩽1
NK < 0 (174)
and thus decreases the loss when added to the hidden layer of the neural network representing f. Moreover
this decrease is at least1
Nof the loss decrease resulting from the addition of all neurons.
As a consequence, there exists no situation where one would need to add many neurons simultaneously to
decrease the loss: it is always feasible with a single neuron. Note that finding the optimal neuron to add is
actually NP-hard (Bach, 2017), so we will not necessarily search for the optimal one. A constructive lower
bound on how much the loss can be improved will be given later in this section.
Proposition D.3 (Greedy completion by one infinitesimal neuron) .The neuron in the previous proposition
can be chosen to have arbitrarily small input weights.
Proof.This is straightforward, as, following a previous remark, the neurons found to collectively decrease
the loss can be supposed to all have arbitrarily small input weights.
This detail is important in that our approach is based on the tangent space of the function fand thus
manipulates infinitesimal quantities. Our optimization problem indeed relies on the linearization of the
activation function by requiring the added neuron to have infinitely small input weights, to make the problem
easier to solve. This proposition confirms that such neuron exists indeed.
Correlations and higher orders. Note that, as a matter of fact, our approach exploits linear correlations
between inputs of a layer and desired output variations. It might happen that the loss is not minimized
yet but there is no such correlation to exploit anymore. In that case the optimization problem (8) will
not find neurons to add. Yet following Prop. D.3 there does exist a neuron with arbitrarily small input
weights that can reduce the loss. This paradox can be explained by pushing further the Taylor expansion
of that neuron output in terms of weight amplitude (single factor εon all of its input weights), for instance
σ(εα·x)≃σ(0)+σ′(0)εα·x+1
2σ′′(0)ε2(α·x)2+O(ε3). Though the linear term α·xmight be uncorrelated
over the dataset with desired output variation v(x), i.e.Ex∼D[xv(x)] = 0, the quadratic term (α·x)2,
or higher-order ones otherwise, might be correlated with v(x). Finding neurons with such higher-order
correlations can be done by increasing accordingly the power of (α·x)in the optimization problem (7).
Note that one could consider other function bases than the polynomials from Taylor expansion, such as
Hermite or Legendre polynomials, for their orthogonality properties. In all cases, one does not need to solve
such problems exactly but just to find an approximate solution, i.e. a neuron improving the loss.
Adding random neurons. Another possibility to suggest additional neurons, when expressivity bottle-
necks are detected but no correlation (up to order p) can be exploited anymore, is to add random neurons.
The firstporder Taylor expansions will show 0 correlation with desired output variation, hence no loss
improvement nor worsening, but the correlation of the p+ 1-th order will be non-0, with probability 1, in
the spirit of random projections. Furthermore, in the spirit of common neural network training practice, one
could consider brute force combinatorics by adding many random neurons and hoping some will be close
enough to the desired direction (Frankle & Carbin, 2018). The difference with the usual training is that
we would perform such computationally costly searches only when and where relevant, exploiting all simple
information first (linear correlations in each layer).
39Published in Transactions on Machine Learning Research (June/2024)
D.2 Loss decreases with a line search on a quadratic energy
LetLbe a quadratic loss over Rdandgbe a vector in Rd. The lossLcan be written as:
L(g) =gTQg+vTg+K (175)
whereQis a matrix that we will suppose to be symmetric positive definite. This is to ensure that all
eigenvalues of Qare positive, hence modeling a local minimum without a saddle point. vis a vector in Rd
andKis a real constant.
For instance, the mean square loss Ex∈D/bracketleftig
∥f(x)−f∗(x)∥2
S/bracketrightig
, whereDis a finite dataset of Nsamples,f∗
a target function, and Sis a symmetric positive definite matrix used as a metric, fits these hypotheses,
considering g= (f(x1),f(x2),...)as a vector. Indeed this loss rewrites as
N/summationdisplay
i=1f(xi)TSf(xi)−2/summationdisplay
if∗T(xi)Sf(xi) +K=gTQg+vTg+K (176)
by flattening and concatenating the vectors f(xi)and considering Q=S⊗S⊗S⊗...the tensor product
ofNtimes the same matrix S, i.e. a diagonal-block matrix with Nidentical blocks S. Note that for the
standard regression with the L2metric, this matrix Qis just the Identity.
Starting from point g, and given a direction h∈Rd, the question is to perform a line search in that direction,
i.e. to optimize the factor λ∈Rin order to minimize L(g+λh).
Developing that expression, we get:
L(g+λh) = (g+λh)TQ(g+λh) +vT(g+λh) +K=λ2hTQh+λ(2hTQg+vTh) +L(g)(177)
which is a second-order polynomial in λwith a positive quadratic coefficient. Note that the linear coefficient
ishT∇gL(g), where∇gL(g) = 2Qg+visthegradientofLatpointg. Theuniqueminimumofthepolynomial
inλis then:
λ∗=−1
2hT∇gL(g)
hTQh(178)
which leads to
min
λL(g+λh) =λ∗2hTQh+λ∗hT∇gL(g) +L(g) (179)
=L(g)−1
4/parenleftbig
hT∇gL(g)/parenrightbig2
hTQh(180)
=L(g)−1
4/angbracketleftbiggh
∥h∥Q/vextendsingle/vextendsingle/vextendsingle/vextendsingle∇Q
gL(g)/angbracketrightbigg2
Q. (181)
Thus the loss gain obtained by a line search in a direction his quadratic in the angle between that direction
and the gradient of the loss, in the sense of the Qnorm (and it is also quadratic in the norm of the
gradient). Note that inner products with the gradient do not depend on the metric, in the sense that
⟨h|∇gL(g)⟩L2=/angbracketleftbig
h|∇S
gL(g)/angbracketrightbig
S∀hfor any metric S, i.e. any symmetric definite positive matrix S,
associated to the norm ∥h∥2
S=hTShand to the gradient ∇S
gL(g) =S−1∇L2
gL(g).
In the case of a standard L2regression this boils down to:
min
λ∥g+λh∥2
L2=∥g∥2−/angbracketleftbiggh
∥h∥/vextendsingle/vextendsingle/vextendsingle/vextendsingleg/angbracketrightbigg2
L2(182)
i.e. consideringL(f) :=Ex∈D/bracketleftig
∥f(x)−f∗(x)∥2/bracketrightig
:
min
λL(f+λh) =L(f)−/angbracketleftbiggh
∥h∥/vextendsingle/vextendsingle/vextendsingle/vextendsinglef∗−f/angbracketrightbigg2
L2=L(f)−E
x∈D[ (f∗−f)h]2
E
x∈D/bracketleftbig
∥h∥2/bracketrightbig. (183)
A result that is useful in the next sections.
40Published in Transactions on Machine Learning Research (June/2024)
D.3 Expected loss gain with a line search in a random direction
Using Appendix D.2 above, the loss gain when performing a line search on a quadratic loss is quadratic in the
angleα=/angbracketleftig
V(X)
∥V(X)∥/vextendsingle/vextendsingle/vextendsingleVgoal(X)
∥Vgoal(X)∥/angbracketrightig
L2between the random search direction V(X)and the gradient Vgoal(X).
This angle has average 0 and is of standard deviation1
nd, as described in Section 5.2. The loss gain is thus
of the order of magnitude of1
din the best case (single-sample minibatch).
D.4 Exponential convergence to 0 training error
Considering a regression to a target f∗with the quadratic loss, the function frepresented by the current
neural network (fully-connected, one hidden layer, with ReLU activation function) can be improved to
reach 0 loss by an addition of nneurons (hi)1⩽i⩽n, withnis the dataset size, using Zhang et al. (2017).
Unfortunately there is no guarantee that if one adds each of these neurons one by one, the loss decreases
each time. We will prove that one of these neurons does decrease the loss, and we will quantify by how
much, relying on the explicit construction in Zhang et al. (2017). This decrease will actually be a constant
factor of the loss, thus leading to exponential convergence towards the target f∗on the training set.
As in the proof of Proposition D.2 in Appendix D, at least one of the added neurons satisfies that its inner
product with the gradient direction is at least 1/n. While one could consequently hope for a loss gain in
O(1
n), one has to see that this decrease would be the one of a gradient step, which is multiplied by a step
sizeη, and asks for multiple steps to be done. Instead in our approach we actually perform a line search
over the direction of the new neuron. In both cases (line search or multiple small gradient steps), one has
to take into account at least order-2 changes of the loss to compute the line search or estimate suitable η
and/or its associated number of steps. Luckily in our case of least square regression, the loss is exactly equal
to its second order Taylor development, and all following computations are exact.
We consider the mean square regression loss L(f) =Ex∈D/bracketleftig
∥f(x)−f∗(x)∥2
S/bracketrightig
, whereDis a finite training
dataset ofNsamples. Its functional gradient ∇L(f)at pointfis2(f−f∗), which is proportional to the
optimal change to add to f, that is,f∗−f. Thenneurons (hi)1⩽i⩽nto be added to ffollowing Zhang et al.
(2017) satisfy/summationtext
ihi=f∗−f=−1
2∇L(f). Thus
/angbracketleftigg/summationdisplay
ihi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglef∗−f/angbracketrightigg
L2=∥f∗−f∥2
L2=L(f). (184)
Then like in the proof of D.2 we use that the maximum is greater or equal to the mean to get that there
exists a neuron hithat satisfies:
⟨hi|f∗−f⟩L2⩾L(f)/n. (185)
By applying Appendix D.2 one obtains that the new loss after line search into the direction of hiyields:
min
λL(f+λhi) =L(f)−⟨hi|f∗−f⟩2
L2
∥hi∥2⩽L(f)×/parenleftbigg
1−L(f)
n2∥hi∥2/parenrightbigg
. (186)
From the particular construction in Zhang et al. (2017) it is possible to bound the square norm of the neuron
∥hi∥2bynd′/parenleftig
dM
dm/parenrightig2
L(f), wheredMis related to the maximum distance between 2 points in the dataset,
dmis another geometric quantity related to the minimum distance, and d′is the network output dimension.
To ease the reading of this proof, we defer the construction of this bound to the next section, Appendix D.5.
Then the loss at each neuron addition decreases by a factor which is at least γ= 1−1
n3d′/parenleftig
dm
dM/parenrightig2
<1. This
factor is a constant, as it is a bound that depends only on the geometry of the dataset (not on f).
41Published in Transactions on Machine Learning Research (June/2024)
Thus it is possible to decrease the loss exponentially fast with the number tof added neurons, i.e. L(ft)⩽
γtL(f), towards 0 training loss, and this in a greedy way, that is, by adding neuron one by one, with the
property that each neuron addition decreases the loss.
Note that, in the proof of Zhang et al. (2017), the added neurons could be chosen to have arbitrarily small
input weights. This corresponds to choosing awith small norm instead of unit norm in Equation 187.
The number of neuron additions expected to reach good performance according to this bound is in the
order of magnitude of n3, which is to be compared to n(number of neurons needed to overfit the dataset,
without the constraint that each addition decreases the loss). This bound might be improved using other
constructionsthanZhangetal.(2017), thoughwiththisprooftheboundcannotbebetterthan n2(supposing
∥hi∥can be made not to depend on n).
Note also that with ReLU activation functions, all points that are on the convex hull of the dataset (which
is necessarily the case of all points if the input dimension is higher that the number of points) can easily in
turn be perfectly predicted (0 loss) by just one neuron addition each (without changing the outputs for the
other points), by choosing an hyperplane that separates the current convex hull point from the rest of the
dataset, and setting a ReLU neuron in that direction.
D.5 Bound on the norm of the neurons
Here we prove that the neurons obtained by Zhang et al. (2017) can be chosen so as to bound the square
norm of any neuron ∥hi∥2bynd′/parenleftig
dM
dm/parenrightig2
L(f), wheredMis related to the maximum distance between 2
points in the dataset, and dmis another geometric quantity related to the minimum distance. For the sake
of simplicity, we first consider the case where the output dimension is d′= 1.
In Zhang et al. (2017), the nneurons are obtained by solving y=Aw, wherey= (y1,y2...)is the target
function (here (f∗−f)at eachxj),Ais the matrix given by Ajk=ReLU (a·xj−bk), representing neuron
activations, and ais any vector that separates the dataset points, i.e. a·xj̸=a·xj′∀j̸=j′, that is,acould
be almost any vector in Rd(in the sense of random projections, that is, the set of vectors that do not satisfy
this is of measure 0).
Here we will pick a particular unit direction a, one that maximizes the distance between any two samples
after projection:
a∈arg max
∥a∥=1min
j,j′|a·(xj−xj′)| (187)
and let us denote d′
mthe associated value: d′
m= minj,j′|a·(xj−xj′)|for thata. Note that d′
m⩽
minj,j′∥xj−xj′∥and that it depends only on the training set. The quantity d′
mis likely to be also
lower-bounded (over all possible datasets) by minj,j′∥xj−xj′∥times a factor depending on the embed-
ding dimension dand the number of points n.
Now, let us sort the samples according to increasing a·xj, that is, let us re-index the samples such that
(a·xj)now grows with j. By definition of a, the difference between any two consecutive a·xjis at leastd′
m.
We now choose biases bj=a·xj−d′
m+εfor some very small ε. The neurons are then defined as hk(x) =
wkReLU (a·x−bk). The induced activation matrix Ajk=ReLU (a·xj−bk)then satisfies∀j <k ;Ajk= 0
and∀j⩾k;Ajk⩾d′
m−ε. The matrix Ais lower triangular with diagonal elements above dm:=d′
m−ε,
hence invertible. Recall that y=Aw.
Consequently, w=A−1y, and hence∥w∥2⩽|||A−1|||2∥y∥2, that is,
∥w∥2⩽1
d2mL(f) (188)
as the target yis the vector f∗−fin our case. Consequently, for any neuron hi, one has:
w2
i⩽1
d2mL(f). (189)
42Published in Transactions on Machine Learning Research (June/2024)
As the norm of the neuron is ∥hi∥2=w2
i/summationtext
jA2
ji, one still has to bound the activities Aji=ReLU (a·xj−bi).
Asawas chosen a unit direction, the values a·xjspan a domain smaller than the diameter of the dataset D:
|a·(xj−xj′)|⩽∥xj−xj′∥⩽diam (D)∀j,j′. Hence all values ∀i,j,|Aij|=|a·xi−bj|=|a·xi−a·xj+dm|<
dM:=diam (D) +dm. Note that dMdepends only the dataset geometry, as for dm.
We now have:
∥hi∥2=w2
i/summationdisplay
jA2
ji⩽nd2
M
d2mL(f) (190)
which ends the proof.
For higher output dimensions d′, one vector wof output weights is estimated per dimension, independently,
leading to the same bound for each dimension. The square norms of neurons are summed over all dimensions
and thus multiplied by at most d′.
D.6 Reaching 0 training error in nneuron additions by overfitting each dataset sample in turn
If one allows updating already existing output weights at the same time as one adds new neurons, then it
is possible to reach 0 training error in only nsteps (where nis the size of the dataset) while decreasing the
loss at each addition.
This scenario is closer to the one we consider with TINY, as we compute the optimal update of existing
weights inside the layer, as a byproduct of new neuron estimation, and apply them.
However the existence proof here follows a very different way to create new neurons, tailored to obtain a
constructive proof, and inspired by the previous section. See Appendix D.7 for another, more generic proof,
applicable to a wide range of growth methods.
Here we consider the same approach as in Appendix D.5 above, but introducing neurons one by one instead
ofnneurons at once. After computing aand the biases bj, thus forming the activity matrix A, we add only
the last neuron hn. The activity of this neuron is 0for all input samples xjexcept for the last one, for which
it isAnn>0. Thus, the neuron hnseparates the sample xnfrom the rest of the dataset, and it is easy to
findwnso that the loss gets to 0 on that training sample, without changing the outputs for other samples.
Similarly, one can then add neuron hn−1, which is active only for samples xn−1andxn. However designing
wn−1so that the loss becomes 0 at point xn−1disturbs the output for point xn(and for that point only).
Luckily if one allows updating wnthen there exists a (unique) solution (wn−1,wn)to achieve 0 loss at both
points. This is done exactly as previously, by solving y=Aw, but considering only the last 2 lines and rows
ofA, leading to a smaller 2×2system which is also lower-triangular with positive diagonal.
Proceeding iteratively this way adds neuron one by one in a way that sends each time one more sample to
0 loss. Thus adding nneurons is sufficient to achieve 0 loss on the full training set, and this in a way that
each time decreases the loss.
Note that updating existing output weights wiwhile adding a new neuron, to decrease optimally the loss,
is actually what TINY does. However, the construction in this Appendix completely overfits each sample
in turn, by design, without being able to generalize to new test points. On the opposite, TINY exploits
correlations over the whole dataset to extract the main tendencies.
D.7 TINY reaches 0 training error in nneuron additions
We will now show that the TINY approach, as well as any other suitable greedy growth method, implemented
within the right optimization procedure, reaches 0 training error in at most nsteps (where nis the size of
the dataset), almost surely.
Before stating it formally, we need to introduce the optimization protocol, growth completion and a proba-
bility measure over activation functions.
43Published in Transactions on Machine Learning Research (June/2024)
Optimization protocol. For this we consider the following optimization protocol conditions, that has to
be applied at least during the last, n-th addition step:
•afull batch approach,
•when adding new neurons, also compute and add the optimal moves of already existing pa-
rameters (i.e. of output weights w).
The first point is to ensure that all dataset samples will be taken into account in the loss during the n-th
update. Otherwise, for instance if using minibatches instead, the optimization of output weights wwill not
be able to overfit the training loss.
The second point is to make sure that, after update, the output weights wwill be optimal for the training
loss. Note that in the mean square regression case, this is easy to do, as the loss is quadratic in w: the
optimal move (leading to the global optimum f∗) can be obtained by line search over the natural gradient
(which is obtained for free as a by-product of TINY’s projection of Vgoal, and is proportional to f∗−f).
This is precisely what we do in practice with TINY when training networks (except when comparing with
other methods and using their own protocol).
Growth completion. For this proof to make sense, we will need the growth method to actually be able to
performnneuronadditions, ifithasnotreached0traininglossbefore. Acounter-examplewouldbeagrowth
method that gets stuck at a place where the training loss is not 0 while being unable to propose new neuron
to add. In the case of TINY, this can happen when no correlation between inputs xiand desired output
variationsf∗(xi)−f(xi)can be found anymore. To prevent this, one can choose any auxiliary method to
add neurons in such cases, for instance random directions, solutions of higher-order expressivity bottleneck
formulations using further developments of the activation function, or locally optimal neurons found by
gradient descent. Some auxiliary methods are guaranteed to further decrease the loss by a neuron addition
(cf. Appendices D.2, D.3, D.4), while any other one is guaranteed not to increase the loss if combined with
a line search along that neuron direction.
We will name completed-TINY the completion of TINY by any such auxiliary method.
Activation function. For technical reasons, the result will stand almost surely only, depending on the
invertibility of a certain matrix, namely, the activation matrix A, defined as Aij=σ(vj·xi+bj), indexed
by samples iand neurons j.
Generally speaking, kernels induced by neurons kj:x∝⇕⊣√∫⊔≀→σ(vj·x+bj)form free families, in the sense
that they are linearly independent (to the notable exception of the linear kernel). This linear independency
means that a linear combination of kernels cannot be equal, as a function, to another kernel with different
parameters. Equality is to be understood as for all possible points xever. However here we will evaluate
the functions only at a finite number nof points (the dataset samples), therefore linear independence will
be considered among the rows of the activation matrix A. This notion of linear dependence is much weaker:
kernels might form a free family as functions but be linearly dependent once restricted to the dataset samples,
by mere chance. While this is not likely (over dataset samples), this is not impossible in general (though of
measure 0), and it is difficult to express an explicit, simple condition on the activation function to be sure
that the activation matrix Aisalwaysinvertible (up to slight changes of parameters). Thus instead we will
express results almost surely over activation functions and neuron parameters.
For most activation functions in the space of smooth functions, the activation matrix Awill be invertible
almostsurelyoverallpossibledatasets. Intheunluckycasewherethematrixisnotinvertible, aninfinitesimal
move of the neurons’ parameters will be sufficient to make it invertible. For some activation functions,
however, such as linear or piecewise-linear ones (e.g., ReLU), the matrix might remain non-invertible over
a wide range of parameter variations (unless further assumptions are made on the neurons added by the
growth process). Yet, in such cases, slight perturbations of the activation function (i.e., choosing another,
smooth, activation function, arbitrarily close to the original one) will yield invertibility.
44Published in Transactions on Machine Learning Research (June/2024)
To properly define " almost surely " regarding activation functions, let us restrict the activation function σto
belong to the space Pof polynomials of order at least n2, that is:
σ(x) =K/summationdisplay
k=0γkxk(191)
withn2⩽K < +∞, and non-0 highest-order amplitude γK̸= 0. This set Pis dense in the set of all
continuous functions over the set Ω = [−rM,rM]dwhich is a hypercube of sufficient radius rMto cover
all samples from the given dataset. One can define probability distributions over P, for instance consider
the density p(σ) =α
K2/producttextK
k=0e−γ2
k√
2πwith a factor α=/parenleftig
π2
6−/summationtext
k<n21
k2/parenrightig−1
to normalize the distribution, and
whereKis the order of the polynomial and thus depends on σ. This density is continuous in the space of
parameters γk(though not continuous in the usual functional metric spaces). Note that the decomposition of
anyσ∈Pas a finite-order polynomial is unique, as monomials of different orders are linearly independent.
We can now state the following lemma (that we will prove later):
Lemma D.1 (Invertibility of the activation matrix) .LetD={xi,1⩽i⩽n}be a dataset of ndistinct
points, and let σ:R→Rbe a function in P, that is, a polynomial of order at least n2. Then with probability 1
over function and neuron parameters (γk),(vj)and(bj), the activity matrix Adefined byAij=σ(vj·xi+bj)
is full rank.
and the following proposition:
Proposition D.4 (Reaching 0 training error in at most nneuron additions) .Under the assumptions above
(polynomial activation function of order ⩾n2, full-batch optimization and computation of the optimal moves
of already existing parameters), completed-TINY reaches 0 training error in at most nneuron additions
almost surely.
Proof.If the growth method reaches 0 training error before nneuron additions, the proof is done. Otherwise,
let us consider the n-th neuron addition. We will show in Lemma D.1 that the activity matrix A, defined by
Aij=σ(vj·xi+bj), indexed by samples iand neurons j, is invertible. Then there exists a unique w∈Rn
such thatAw=f∗, i.e./summationtext
jwjσ(vj·xi+bj) =f∗(xi)for each point xiof the dataset. This vector of
output parameters wrealizes the global minimum of the loss over already existing weights: infwL(fv,w) =
infw∥Aw−f∗∥2. They are also the ones found by a natural gradient step over the loss (up to a factor 2,
that can easily be found by line search as the loss is convex). Then after that update the training loss is
exactly 0.
Note: piecewise-linear activation functions such as ReLU are not covered by this proposition. However the
result might still hold with further assumptions over the growth process. For instance, with the method in
Zhangetal.(2017), theReLUneuronsarechoseninsuchawaythatthematrix Aisfullrankbyconstruction.
Proof of Lemma D.1. Let us first show that if, unluckily, for a given activation function σand given param-
eters (vj,bj), the matrix Ais not full rank, then upon infinitesimal variation of the parameters, the matrix
Abecomes full rank.
Indeed, if all pre-activities ai,j:=vj·xi+bjare not distinct for all i,j, then an infinitesimal variation of
the vectorsvjcan make them distinct. For this, one can see that the set of directions vjon which any two
dataset points xiandxi′have the same projection is finite (since it has to be the direction of xi−xi′, for a
given pair of dataset samples (i,i′)) and thus of measure 0. As a consequence with probability 1 over neuron
parametersvjandbj, all pre-activities are distinct.
Now, if the matrix Ais not invertible, as invertible matrices are dense in the space of matrices, one can
easily find an infinitesimal change δAto apply to Ato make it invertible. This corresponds to changing the
activation function σaccordingly at each of the n2distinct pre-activity values. Since σhas more than n2
parameters, this is doable. For instance, one can select the n2first parameters and search for a suitable
variationg:= (δγk)0⩽k<n2of them by solving the linear system Sg=δAwhere then2×n2matrixSis
45Published in Transactions on Machine Learning Research (June/2024)
defined bySij,k=ak
i,j= (vj·xi+bj)k. This matrix Sis invertible because any gsuch thatSg= 0would
induce:
∀i,j,n2−1/summationdisplay
k=0δγkak
i,j= 0 (192)
and thus the polynomial P(x) =/summationtextn2−1
k=0δγkxkhas at least n2roots while being of order at most n2−1.
ThusSg= 0 =⇒g= 0andSis invertible. Note that as δAis infinitesimal, g=S−1δAwill be
infinitesimal as well, and so is the change brought to the activation function σ.
Consequently we have that the set of activation functions σand neuron parameters (vj,bj)for which the
matrixAis full rank is dense in the set of polynomials Pof order at least n2and of neuron parameters N.
Now, the function det :P×N→R,((γk)k,(vj,bj)j)∝⇕⊣√∫⊔≀→detA= det (σγ(vj·xi+bj))is smooth as a function
of its input parameters (the determinant being a polynomial function of the matrix coefficients). As this
continuous function is non-0 on a dense set of its inputs, the pre-image det−1{0}is closed and contains no
open subset. This is not yet sufficient to prove that this pre-image is of measure 0 (e.g., fat Cantor set).
For a fixed order K, one can see this function as a polynomial of its inputs γkandvj,bj, and conclude2that
the set of its roots is of measure 0. As a consequence, the probability, over coefficients γkor equivalently over
polynomials σof orderK, that detAis non-0, is 1. As this stands for all K, we have that the probability
that the matrix Ais invertible is at least the mass of polynomials of all orders K, i.e./summationtext
k⩾n2α
k2= 1. Thus
Ais invertible with probability 1.
E Technical details
E.1 Batch size to estimate the new neuron and the best update
In this section we study the variance of the matrices M∗andS−1/2Ncomputed using a minibatch of n
samples, seeing the samples as random variables, and the matrices computed as estimators of the true
matrices one would obtain by considering the full distribution of samples. Those two matrices are the
solutions of the multiple linear regression problems defined in (35) and in (46), as we are trying to regress
the desired update noted Yonto the span of the activities noted X. We suppose we have the following
setting :
Y∼AX+ε, ε∼N(0,σ2),E[ε|X] = 0 (193)
where the (Xi,Yi)arei.i.d.andAis the oracle for M∗or matrixS−1/2N. IfYis multidimensional, the
total variance of our estimator can be seen as the sum of the variances of the estimator on each dimension
ofY.
We now suppose that Y∈Rand note ˆA:=YXT(XXT)+the solution of 193. We first remark that ˆAX =
YPwithP=XT(XXT)+X∈R(n,n). It follows that when n⩽p, almost surely we have rk(P) =n
andYP =Y, resulting in a zero expressivity bottleneck for that specific mini-batch, i.e.Y=ˆAX. In
practice, we do not consider n < pas the solution ( δWorA,Ω) would overfit a specific mini-batch and
would increase the expressivity bottleneck for the rest of the dataset.
We now suppose that n>p, we now have almost surely that rk(P) =pandYP̸=Y. We now study the
variance of the estimator ˆA∈Rp. We have almost surely that XXTis invertible and note (XX)−1its
inverse. Taking the expectation on variable ε, we have cov (ˆA) =σ2(XXT)−1. Ifnis large, and if matrix
1
nXXT→Q, withQnon-singular, then, asymptotically, we have ˆA∼N(A,σ2Q−1
n), which is equivalent to
(ˆA−A)√n
σQ1/2∼N (0,I). Then||(ˆA−A)√n
σQ1/2||2∼χ2(k)wherekis the dimension of X. It follows
thatE/bracketleftig
||(ˆA−A)Q1/2||2/bracketrightig
=kσ2
nand asQ1/2Q1/2Tis positive definite, we conclude that var( ˆA)⩽kσ2
nλmin(Q).
2See for instance a proof by recurrence that roots of a polynomial are always of measure 0: https://math.stackexchange.
com/questions/1920302/the-lebesgue-measure-of-zero-set-of-a-polynomial-function-is-zero .
46Published in Transactions on Machine Learning Research (June/2024)
In practice we aim to keep the variance of our estimators stable during architecture growth. To ensure this
we can choose the batch size nto make the bound constant. With the notations defined in Figure 12, we
estimate a matrix of size k←(SW)2. Fornimages, as each input sample contains Pquantities, and that
each is a realization of the random variable X(totalnPvariables), we have in total n←nPdata points
for the estimation of the best neuron. Hence to add new neurons with a (asymptotically) fixed variance, we
use batch size
n∝(SW)2
P.
For convolutional layers, we take n= 0.001×(SW)2
P×2k(Figure 5) and n= 0.01×(SW)2
P×2k(Figure 8),
wherekis equal to/radicalig
32×32
P, and this 2kfactor is found empirically to somehow account for the variances of
the estimators even when the same input is used multiple times, as are the {Bt
i,j}j∈Pin Equation (55).
E.2 Batch size for learning
We adjust the batch size for gradient descent as follows: the batch size is set to bt=0= 32at the beginning of
eachexperiment, anditisscheduledtoincreaseasthesquarerootofthecomplexityofthemodel( i.e.number
of parameters). If at time tthe network has complexity Ctparameters, then at time t+ 1the training batch
size is equal to bt+1=bt×/radicalig
Ct+1
Ct.
E.3 Normalization
E.3.1 Figures 5, 6 and 16 : Usual normalization
For the GradMax method of Figures 5 and 16, before adding the new neurons to the architecture, we
normalize the outgoing weight of the new neurons according to Evci et al. (2022), i.e.:
α∗
k←0 (194)
for Figures 5 and 6 ω∗
k←ω∗
k×10−3
/radicalig
||(ω∗
j)nd
j=1||2
2/nd(195)
for Figure 16 ω∗
k←ω∗
k×/radicaligg
10−3
||(ω∗
j)nd
j=1||2
2/nd(196)
For TINY method of both figures, the previous normalization process is mimicked by normalizing the in and
out going weights by their norms and multiplying them by√
10−3,i.e.:
αk←α∗
k×/radicaligg
10−3
||(α∗
j)nd
j=1||2
2/nd(197)
ωk←ω∗
k×/radicaligg
10−3
||(ω∗
j)nd
j=1||2
2/nd(198)
E.3.2 Figure 8 : Amplitude Factor
For the Random and the TINY methods of Figure 8, we first normalize the parameters as :
For the new neurons
α∗
k←α∗
k×1/radicalig
||(α∗
j)nd
j=1||2
2/nd
ω∗
k←ω∗
k×1/radicalig
||(ω∗
j)nd
j=1||2
2/ndFor the best update
W∗←W∗×1/radicalbig
||W∗||2
2/nd
47Published in Transactions on Machine Learning Research (June/2024)
Then, we multiply them by the amplitude factor γ∗:
For the new neurons :
α∗
k, ω∗
k←α∗
kγ∗,ω∗
kγ∗
γ∗:= arg min
γ∈[−L,L]/summationdisplay
iL(fθ⊕γθK↔(xi),yi)For the best update :
W∗
l←γ∗Wl
γ∗:= arg min
γ∈[−L,L]/summationdisplay
iL(fθ+γW∗(xi),yi)
where the operation γθK∗
↔= (γα∗
k,γω∗
k)K
kis the concatenation of the neural network with the new neurons
andθ+γW∗is the update of one layer with its best update. The batch on which γ∗is computed is different
from the one used to estimate the new parameters and its size is fixed to 1000 for all experiments.
E.4 Full algorithm
In this section we describe in detail the pseudo-code to plot Figures 5, 6 and 8. The function NewNeurons (l),
in Algorithm 2, computes the new neurons defined at Proposition 3.2 for layer lsorted by decreasing eigen-
values. The function BestUpdate (l), in Algorithm 4 computes the best update at Proposition 3.1 for layer l.
Algorithm 1: Algorithm to plot Figures 5 and 8.
1foreach method [TINY, MethodToCompareWith] do
2Start from neural network Nwith initial structure s∈{1/4,1/64};
3whileN architecture does not match ResNet18 width do
4ford in {depths to grow} do
5 θK∗
↔=NewNeurons (d,method );
6 NormalizeθK∗
↔according to E.3;
7 Add the neurons at layer d;
8 TrainNfor∆tepochs ;
9 Save model Nand its performance ;
10 end
11end
12end
48Published in Transactions on Machine Learning Research (June/2024)
Algorithm 2: NewNeurons
Data:l,method =TINY
Result: Best neurons at l
1ifmethod =TINYthen
2M=BestUpdate (l+ 1);
3S,N=MatrixSN (l−1,l+ 1,M=M);
4Compute the SVD of S:=OΣOT;
5Compute the SVD of
O√
Σ−1ON :=AΛΩ;
6Use the columns of A, the lines of Ωand
the diagonal of Λto construct the new
neurons of Prop. 3.2;
7else ifmethod =GradMax then
8M=None;
9_,N=MatrixSN (l−1,l+ 1,M=M);
10Compute the SVD of NTN;
11Use the eigenvectors to define the new
out-going weights ;
12Set the new in-going weight to 0;
13else ifmethod =Randomthen
14 (αk,ωk)nd
k=1∼N(0,Id);
15endAlgorithm 3: MatrixSN
Data:p1,p2(layer indexes), M=None
Result: Construct matrices SandN
1Take a minibatch Xof size∝(SW)2
P;
2Propagate and backpropagate X;
3Compute Vgoalatp2,i.e.−∂Ltot
∂Ap2;
4ifM̸=Nonethen
5 Vgoal−=MBp1
6end
7S,N=Bp1Bp1T,Bp1VgoalT;
Algorithm 4: BestUpdate
Data:l, index of a layer
Result: Best update at l
1Take a minibatch Xof size∝(SW)2
P;
2Compute (S,N)with MatrixSN( l,l);
3M=NTS−1;
E.5 Computational complexity
We estimate here the computational complexity of the above algorithm for architecture growth.
Theoretical estimate. We use the following notations:
•number of layers: L
•layer width, or number of kernels if convolutions: W(assuming for simplicity that all layers have
same width or kernels)
•number of pixels in the image: P(P= 1for fully-connected)
•kernel filter size: S(S= 1if fully-connected)
•minibatch size used for standard gradient descent: M
•minibatch size used for new neuron estimation: M′
•minibatch size used in the line-search to estimate amplitude factor: M′′
•number of classical gradients steps performed between 2 addition tentatives: T
Complexity, estimated as the number of basic operations, cumulated over all calls of the functions:
•of the standard training part: TMLSW2P
•of the computation of matrices of interest (function MatrixSN): LM′(SW)2P
•of SVD computations (function NewNeurons): L(SW)3
•of line-searches (function AmplitudeFactor): L2M′′SW2P
49Published in Transactions on Machine Learning Research (June/2024)
Figure 12: Notation and size for convolutional and linear layers
•of weight updates (function BestUpdate): LSW
The relative added complexity w.r.t. the standard training part is thus:
M′S/TM +S2W/TMP +M′′L/TM + 1/WTMP.
SVD cost is negligible. The relative cost of the SVD w.r.t. the standard training part is S2W/TMP .
In the fully-connected network case, S= 1,P= 1, and the relative cost of the SVD is then W/TM. It is
then negligible, as layer width Wis usually much smaller than TM, which is typically 10×100for instance.
In the convolutional case, S= 9for3×3kernels, and P≈1000for CIFAR, P≈100000for ImageNet, so
the SVD cost is negligible as long as layer width W < < 10000or 1 000 000 respectively. So one needs no
worrying about SVD cost.
Likewise, the update of existing weights using the “optimal move" (already computed as a by-product) is
computationally negligible, and the relative cost of the line searches is limited as long as the network is not
extremely deep ( L<TM/M ”).
On the opposite, the estimation of the matrices (to which SVD is applied) can be more ressource-demanding.
ThefactorM′S/TMcanbelargeiftheminibatchsize M′needstobelargeforstatisticalsignificancereasons.
One can show that an upper bound to the value required for M′to ensure estimator precision (see Appendix
E.1) is (SW)2/P. In that case, if W >/radicalbig
TMP/S3, these matrix estimations will get costly. In the fully-
connected network case, this means W >√
TM≈30forT= 10andM= 100. In the convolutional case,
this means W >/radicalbig
TMP/S3≈30for CIFAR and≈300for ImageNet. We are working on finer variance
estimation and on other types of estimators to decrease M′and consequently this cost. Actually (SW)2/P
is just an upper bound on the value required for M′, which might be much lower, depending on the rank of
computed matrices.
In practice. In practice the cost of a full training with our architecture growth approach is similar (some-
times a bit faster, sometimes a bit slower) than a standard gradient descent training using the final ar-
chitecture from scratch. This is great as the right comparison should take into account the number of
different architectures to try in the classical neural architecture search approach. Therefore we get layer
width hyper-optimization for free.
F Additional experimental results and remarks
F.1 ResNet18 on CIFAR-100
Figures. In all plots the black line represents the average performance over two independent runs, and the
colored regions indicate the confidence interval.
Technical details of Figures 5 and 8 The experiments were performed on 1 GPU. The optimizer is
SGD(lr= 1e−2) with the starting batch size 32 E.2. At each depth lwe set the number nlof neurons to be
50Published in Transactions on Machine Learning Research (June/2024)
added at this depth 2. These numbers do not depend on the starting architecture and have been chosen such
that each depth will reach its final width with the same number of layer extensions. For the initial structure
s= 1/4, resp. 1/64, we set the number of layer extensions to 16, resp. 21, such that at depth 2 (named
Conv2 in Table 3), n2= (Sizefinal
2−Sizestart
2)/nb of layer extensions = (64−16)/16 = (64−1)/21 = 3. The
initial architecture is described in Table 3.
depthlConv2 Conv3 Conv5 Conv6 Conv8 Conv9 Conv11 Conv12
nl 3 3 6 6 12 12 24 24
Table 2: Number of neurons to add per layer. The depth is identified by its name on Table 3.
F.1.1 Performance for gradient-based methods
We present in Table 5 two indicators of performance on classical visual datasets for four gradient-based
methods: DART (Liu et al. (2019)), NORTH Maile et al. (2022), GradMax (Evci et al. (2022)), and TINY.
We use the adjective gradient-based when the method uses the information from the propagation of the loss
to search for an architecture. While GradMax, NORTH, and TINY increase the size of existing architectures
(VGG and ResNet), DARTS uses cells to create the architecture, starting from a large graph of cells and
removing what is considered unnecessary connections. We make the following remarks for the indicator
Time, which is the time in GPU days to search for an architecture and train it, and the indicator Acc., which
is the accuracy on the test set:
•For NORTH, GradMax, and TINY, the accuracy on the test set is comparable, as all methods
share a lot in terms of methodology, growth process, and neuron initialization (cf. B). Nonetheless,
NORTH search time complexity is much larger as for each increase of architecture its strategy
relies on trial-and-error methodology (going up to 1000 generations before actually increasing the
architecture).
•The time search complexity of TINY will always be slightly higher than GradMax as it needs to
project the desired update and compute the matrix S(cf B).
•Although DARTS reaches good performance, its time search complexity is always greater than one
GPU as it starts from a large architecture and decreases its size by removing connections between
cells. For the ImageNet dataset, the indicator Timedoes not take into account the time spent
searching for the basic cells, which are then used to create the graph.
51Published in Transactions on Machine Learning Research (June/2024)
Figure 13: Accuracy and number of parameters during architecture growth for methods TINY and GradMax
as a function of the gradient step. Mean and standard deviation for four independent runs.
52Published in Transactions on Machine Learning Research (June/2024)
Figure14: AccuracyasafunctionofthenumberofepochsduringextratrainingforTINYonfourindependent
runs.
Figure 15: Accuracy curves as a function of the number of epochs during extra training for GradMax on
four independent runs.
53Published in Transactions on Machine Learning Research (June/2024)
Table 3: Initial and final architecture for the models of Figure 5. Numbers in color indicate where the
methods were allowed to add neurons (middle of ResNet blocks). In blue the initial structure for the model
1/64and in green the initial structure for the model 1/4, i.e., 1|16indicates that the model 1/64started
with 1neuron at this layer while the model 1/4started with 16neurons at the same layer.
ResNet18
Layer name Output size Initial layers (kernel=(3,3), padd.=1) Final layers (end of Fig 5)
Conv 1 32×32×64/bracketleftig
3×3,/bracketrightig /bracketleftig
3×3,64/bracketrightig
Conv 2 32×32×64
3×3,64
3×3,1|16

3×3,1|16
3×3,64

3×3,64
3×3,64

3×3,64
3×3,64

Conv 3 32×32×64
3×3,64
3×3,1|16

3×3,1|16
3×3,64

3×3,64
3×3,64

3×3,64
3×3,64

Conv 4 16×16×64/bracketleftig
3×3,128/bracketrightig /bracketleftig
3×3,128/bracketrightig
Conv 5 16×16×128
3×3,128
3×3,2|32

3×3,2|32
3×3,128

3×3,128
3×3,128

3×3,128
3×3,128

Conv 6 16×16×128
3×3,128
3×3,2|32

3×3,2|32
3×3,128

3×3,128
3×3,128

3×3,128
3×3,128

Conv 7 8×8×256/bracketleftig
3×3,256/bracketrightig /bracketleftig
3×3,256/bracketrightig
Conv 8 8×8×256
3×3,256
3×3,4|64

3×3,4|64
3×3,256

3×3,256
3×3,256

3×3,256
3×3,256

Conv 9 8×8×256
3×3,256
3×3,4|64

3×3,4|64
3×3,256

3×3,256
3×3,256

3×3,256
3×3,256

Conv 10 4×4×512/bracketleftig
3×3,512/bracketrightig /bracketleftig
3×3,512/bracketrightig
Conv 11 4×4×512
3×3,512
3×3,8|128

3×3,8|128
3×3,512

3×3,512
3×3,512

3×3,512
3×3,512

Conv 12 4×4×512
3×3,512
3×3,8|128

3×3,8|128
3×3,512

3×3,512
3×3,512

3×3,512
3×3,512

AvgPool2d 1×1×512
FC 1 100 512×100 512×100
SoftMax 100
54Published in Transactions on Machine Learning Research (June/2024)
Figure 16: Accuracy on test split of as a function of the number of parameters during architecture growth
from ResNet 1/64to ResNet18. The normalization for GradMax is√
10−3.
∆t= 1BaselineTINY GradMax
s = 1/6468.1±0.5 57.2±0.372.8±0.35∗
68.7±0.65∗57.7±0.33∗
Table 4: Final accuracy on test split of ResNet18 of 16 after the architecture growth ( grey) and after
convergence ( blue). The number of stars indicates the multiple of 50 epochs needed to achieve convergence.
With the starting architecture ResNet 1/64and∆t= 1the method TINY achieves 68.1±0.5on test split
after its growth and it reaches 68.7±0.65∗after∗:= 5×50epochs.
MethodIndicatorsDatasetArch. Time Acc.
DARTS× 4 97.0±0.1CIFAR-10× 6.5 97.8±0.1
× 4 73.3 ImageNet
NORTHVGG-11 3.3∼76.4CIFAR-10ResNet-28 0.4∼83.3
VGG-11 1.6∼55.7CIFAR-100
GradMax∔∆t= 0.25,s= 1/64ResNet-18 0.12 45.0±0.4
CIFAR-100∆t= 1,s= 1/64ResNet-18 0.26 56.8±0.2
TINY∔ ∆t= 0.25,s= 1/64ResNet-18 0.13 65.8±0.1
∆t= 0.1,s= 1/64ResNet-18 0.28 68.1±0.5
Table 5: Time: GPU days spent to search for the architecture and to train it. Acc.: accuracy on test set (%).
∔: estimated with our implementation. For the NORTH method, we computed the average performance
over their different initializations and strategies based on their Figure 4.
55