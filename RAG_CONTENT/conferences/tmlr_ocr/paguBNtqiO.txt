Published in Transactions on Machine Learning Research (02/2023)
Improved Differentially Private Riemannian Optimization:
Fast Sampling and Variance Reduction
Saiteja Utpala saitejautpala@gmail.com
Independent
Andi Han andi.han@sydney.edu.au
University of Sydney
Pratik Jawanpuria pratik.jawanpuria@microsoft.com
Microsoft India
Bamdev Mishra bamdevm@microsoft.com
Microsoft India
Reviewed on OpenReview: https: // openreview. net/ forum? id= paguBNtqiO
Abstract
A common step in differentially private (DP) Riemannian optimization is sampling from
the (tangent) Gaussian distribution as noise needs to be generated in the tangent space to
perturbthegradient. Inthisregard, existingworks eitheruse the MarkovchainMonteCarlo
(MCMC) sampling or explicit basis construction based sampling methods on the tangent
space. This becomes a computational bottleneck in the practical use of DP Riemannian
optimization, especially when performing stochastic optimization. In this paper, we discuss
different sampling strategies and develop efficient sampling procedures by exploiting linear
isometry between tangent spaces and show them to be orders of magnitude faster than
both the MCMC and sampling using explicit basis construction. Furthermore, we develop
the DP Riemannian stochastic variance reduced gradient algorithm and compare it with
DP Riemannian gradient descent and stochastic gradient descent algorithms on various
problems.
1 Introduction
Differentialprivacy(DP)providesarigoroustreatmentforthenotionofdataprivacybypreciselyquantifying
the deviation in the model’s output distribution under modification of a small number of data points (Dwork
et al., 2006b). Provable guarantees of DP coupled with properties like immunity to arbitrary post-processing,
and graceful composability have made it a de-facto standard of privacy with steadfast adoption in the real
world (Erlingsson et al., 2014; Apple, 2017; Near, 2018; Abowd, 2018). Furthermore, it has been shown
empiricallythatDPmodelsresistvariouskindsofleakageattacksthatmaycauseprivacyviolations(Rahman
et al., 2018; Carlini et al., 2019; Sablayrolles et al., 2019; Zhu et al., 2019; Balle et al., 2022; Carlini et al.,
2022).
Various approaches have been explored in the literature to ensure differential privacy in machine learning
models. These include output perturbation (Chaudhuri et al., 2011; Zhang et al., 2017) and objective
perturbation (Chaudhuri et al., 2011; Iyengar et al., 2019), in which a perturbation term is added to the
output of a non-DP algorithm or the optimization objective, respectively. Another approach, gradient
perturbation, involves perturbing the gradient information at every iteration of gradient based approaches
and has received significant interest in the context of deep learning and stochastic optimization (Song et al.,
2013; Bassily et al., 2014; Abadi et al., 2016; Wang et al., 2017; Bassily et al., 2019; Wang et al., 2019a;
Bassily et al., 2021).
1Published in Transactions on Machine Learning Research (02/2023)
Recently, achieving differential privacy over Riemannian manifolds has also been explored in the context of
obtaining Fréchet mean (Reimherr et al., 2021; Utpala et al., 2022b; Soto et al., 2022) and, more generally,
solving empirical risk minimization problems (Han et al., 2022a). Riemannian geometry is a generalization of
the Euclidean geometry (Lee, 2006) and includes several non-linear spaces such as the set of positive definite
matrices (Bhatia, 2009), set of orthogonal matrices (Edelman et al., 1998), and hyperbolic space (Beltrami,
1868; Gromov, 1987), among others. Many machine learning tasks such as principal component analysis
(Absil et al., 2007), low-rank matrix/tensor modeling (Boumal & Absil, 2011; Kasai et al., 2019; Jawanpuria
& Mishra, 2018; Nimishakavi et al., 2018), metric learning (Bhutani et al., 2018; Han et al., 2021), natural
language processing (Jawanpuria et al., 2019a; 2020a;b), learning embeddings (Nickel & Kiela, 2017; 2018;
Suzuki et al., 2019; Jawanpuria et al., 2019b; Qi et al., 2021), optimal transport (Mishra et al., 2021;
Jawanpuria et al., 2021; Han et al., 2022b), etc., may be viewed as problem instances on Riemannian
manifolds.
In differentially private Riemannian optimization (Han et al., 2022a), a key step is to use tangent Gaussian
sampling at every iteration to perturb the gradient direction in the tangent space. Han et al. (2022a)
proposed to use the Markov Chain Monte Carlo (MCMC) method (Robert & Casella, 1999), which is
computationally expensive especially on matrix manifolds with large dimensions. When the underlying
Riemannian metric is induced from the Euclidean metric, such as for sphere, Han et al. (2022a) showed
one can avoid MCMC via basis construction for the tangent space. For general manifolds of interest,
however, a discussion on basis construction and computationally efficient sampling is missing. The sampling
step is computationally prohibitive, especially when performing differentially private stochastic optimization
over Riemannian manifolds, where the number of sampling calls is relatively high compared to the case
of deterministic optimization. It should also be noted that generalizing more sophisticated differentially
private Euclidean stochastic algorithms like differentially private stochastic variance reduced gradient (Wang
et al., 2017) to Riemannian geometry is non-trivial and is an active area of research. The benefits of (non-
private) Riemannian stochastic variance reduction gradient (RSVRG) methods over Riemannian stochastic
gradient (Bonnabel, 2013) has been studied in existing works (Zhang et al., 2016; Zhou et al., 2019; Han &
Gao, 2021; Sato et al., 2019).
Our main contributions on improving differentially private Riemannian optimization framework are summa-
rized below.
1.Sampling. We propose generic fast sampling methods on the tangent space for various matrix
manifolds of interest. This makes differentially private Riemannian optimization more practically
appealing for real-world applications. The proposed sampling strategy is based on linear isometry
between tangent spaces. We show that it is computationally efficient and orders of magnitude faster
than other sampling schemes, MCMC, and explicit basis construction, presented in (Han et al.,
2022a).
2.DP-SVRG. We propose a differentially private Riemannian stochastic variance reduced gradient
(DP-RSVRG), enriching the suite of differentially private stochastic Riemannian optimization meth-
ods.
OurfirstcontributionallowstoscaledifferentiallyprivateRiemannianoptimizationalgorithmssincesampling
is now faster. Our second contribution is on developing DP-RSVRG and together with faster sampling, DP-
RSVRG is scalable to large datasets. In the experiments section, we show the empirical benefit of both of
these contributions.
Organization. The rest of the paper is organized as follows. Section 2 gives a background on Riemannian
geometry, Riemannian optimization, and differential privacy. We then use various properties of tangent
Gaussian distribution and discuss different possible sampling strategies in Section 3. Section 4 presents our
proposed sampling procedure and gives exact details about how to implement it in practice for several man-
ifolds of interest. In Section 5, we develop a differentially private Riemannian stochastic variance reduction
gradient algorithm (DP-RSVRG). Section 6 discusses the empirical results. Section 7 concludes the paper.
2Published in Transactions on Machine Learning Research (02/2023)
2 Preliminaries and related work
Riemannian Geometry. A Riemannian Manifold Mof dimension dis smooth manifold with an inner
product structure ⟨.,.⟩w(i.e., having a Riemannian metric) on every tangent space TwM.Given a basis
B= (β1,...,βd)forTwMatw∈M, the Riemannian metric can be represented as a symmetric positive
definite matrix Gwand the inner product can be written as
⟨ν1,ν2⟩w=− →ν1TGw− →ν2,
where− →ν1,− →ν2are coordinates of the tangent vectors ν1,ν2∈TwMin the coordinate system given by B.
An induced norm is defined as ∥ν∥w=p
⟨ν,ν⟩wforν∈TwM.Letγ: [0,1]→Mdenote any smooth
curve andγ′(t)∈Tγ(t)Mits derivative, then distance between w1,w2∈Mis defined as dist (w1,w2) =
infγ:γ(0)=w1,γ(1)=w2/integraltext1
0∥γ′(t)∥γ(t)dt.
A smooth curve γ: [0,1]→Mis called the geodesic if it locally minimizes the distance between γ(0)and
γ(t). For anyν∈TwM, the exponential map is defined as Expw(ν) =γ(1)whereγ(0) =wandγ′(0) =ν.
If between any two points w,w′∈W⊆M there is a unique geodesic connecting them, then the exponential
map has an inverse map Exp−1
w:W→TwM,which maps a point on the manifold to the tangent space
TwM. Transporting the vectors on the manifold requires the notion of parallel transport. In particular, the
parallel transport from w1∈Mtow2∈Mdenoted as PTw1→w2:Tw1M→Tw2Mis a linear isometry (i.e.,
inner product preserving) along a geodesic. In this work, the curvature of a manifold refers to the sectional
curvature, which provides a local measure of curvature at each point on the manifold.
The Riemannian gradient of a real valued function f:M→ R, denoted as gradf(w), is a tangent vector
such that for any ν∈TwM,
⟨gradf(w),ν⟩w=Df[w](ν),
where Df[w](ν)denotes the directional derivative of fatwalongν. We refer the readers to (Do Carmo &
Flaherty Francis, 1992; Lee, 2006) for a detailed exposition of Riemannian geometry and (Absil et al., 2009;
Boumal, 2022) for Riemannian optimization.
Function classes on Riemannian Manifolds. We call a neighbourhood W⊆M totally normal if for
any two points, the exponential map is invertible. Let W⊆M be a totally normal neighborhood and DW
be its diameter and κminbe the lower bound on the sectional curvature of W.
A function f:W → Ris calledL0-geodesically Lipschitz continuous (L-g-lipschitz) if for any w1,w2∈
M,|f(w1)−f(w2)|≤L0dist(w1,w2).Under the assumption of continuous gradient, a function fisL0
geodesically Lipschitz continuous if and only if
∥gradf(w)∥≤L0,
for allw∈M(Boumal, 2022). A differentiable function f:M→ Ris geodesically L-smooth (L-g-smooth)
if its gradient is L-Lipschitz, i.e.,∥gradf(w1)−PTw2→w1gradf(w2)∥w1≤Ldist(w1,w2). Additionally, it
can be shown that if fis geodesically L-smooth, then
f(w1)≤f(w2) +⟨gradf(w2),Exp−1
w2(w1)⟩w2+L
2∥Exp−1
w2(w1)∥2
w2,
for allw1,w2∈M. A function fis called geodesically µ-strongly convex ( µ-strongly g-convex) (Zhang et al.,
2016) if for all w1,w2∈W, it satisfies
f(w1)≥f(w2) +⟨gradf(w2),Exp−1
w2(w1)⟩w2+µ
2∥Exp−1
w2(w1)∥2
w2.
Letw∗be a global minimizer of f. Thenf:W→ Ris said to satisfy the Riemannian Polyak–Łojasiewicz
(PL) condition if there exists τ >0,such that,
f(w)−f(w∗)≤τ∥gradf(w)∥2
w,
3Published in Transactions on Machine Learning Research (02/2023)
for anyw∈M(Zhang et al., 2016). The Riemannian PL condition is a strictly weaker notion than the
geodesic strong convexity, i.e., every geodesic µ-strongly convex function satisfies the Riemannian PL con-
dition (with τ= 1/(2µ)) and there exist functions that satisfy the Riemannian PL condition but are not
geodesically strongly convex. The trigonometric distance bound from Zhang & Sra (2016) (see Lemma 10),
which is crucial for deriving convergence analysis of Riemannian optimization algorithms makes use of the
curvature constant, is defined as
ζ=

√
|κmin|DW
tanhÄ√
|κmin|DWäκmin<0
1 κmin≥0.
Differential privacy. LetZbe an input data space and two datasets of size Z,Z′∈Znof sizenare
calledadjacent if they differ by at most one element. We represent the adjacent datasets Z,Z′by the
notationZ∼Z′.A manifold-valued randomized mechanism R:Zn→Mis said to be (ϵ,δ)-approximately
differentially private (ADP) (Dwork et al., 2006a; Wasserman & Zhou, 2010) if for any two adjacent datasets
Z∼Z′and for all measurable sets S⊆M, we have
P[R(Z)∈S]≤exp (ϵ)P[R(Z′)∈S] +δ.
Rényi differential privacy (RDP) by Mironov (2017) is a refinement of DP which gives tight privacy bounds
under composition of mechanisms. The λ-th moment of a mechanismRis defined as
KR(λ) = sup
Z∼Z′logÇ
E
o∼R(Z)ñÅp(R(Z) =o)
p(R(Z′) =o)ãλôå
,
and mechanismRis said to satisfy (λ,ρ)-RDP if1
λ−1KR(λ−1)≤ρ. If the mechanism R:Z→M is an
(adaptive) composition of kmechanisms{Ri}k
i=1, i.e.,Ri:/producttexti−1
j=1Mj×Zn→Mi, then
KR(λ)≤k/summationdisplay
i=1KRi(λ).
Usingthemomentsaccountanttechnique(Abadietal.,2016), (λ,ρ)-RDPmechanismcanbegiven (ϵ,δ)-ADP
certificate. We refer the interested readers to (Dwork et al., 2014; Vadhan, 2017) for more details.
Differential privacy on Riemannian manifolds. Reimherr et al. (2021) are the first to consider differen-
tial privacy in the Riemannian setting and derived the Riemannian Laplace mechanism based on distribution
from (Hajri et al., 2016). Utpala et al. (2022b) derive output perturbation for manifold of symmetric posi-
tive definite matrices (SPD) with the Log-Euclidean metric based on distribution from (Schwartzman, 2016).
While (Reimherr et al., 2021; Utpala et al., 2022b) focus on output perturbation, Han et al. (2022a) propose
a unified differentially private Riemannian optimization framework through gradient perturbation.
Han et al. (2022a) consider the following problem (1) where the parameter of interest lies on a Riemannian
manifoldMand{zi},i= 1,...,n, represent the set of data samples, i.e.,
min
w∈M(
F(w) =1
nn/summationdisplay
i=1fi(w) =1
nn/summationdisplay
i=1f(w;zi))
. (1)
The aim of differentially private Riemannian optimization is to privatize the solution from a Riemannian
optimizationsolverbyinjectingnoisetotheRiemanniangradientsimilartotheEuclideancase. TheRieman-
nian gradient gradF(w)belongs to the tangent space (TwM,⟨,⟩w)and to perturb the Riemannian gradient,
Han et al. (2022a) define an intrinsic Gaussian distribution on the tangent space TwMwith density
p(ν)∝exp(−∥ν−µ∥2
w/2σ2), ν∈TwM,
and refer to it as the tangent Gaussian distribution. They propose differentially private Riemannian gradient
and Riemannian stochastic gradient descent algorithms.
4Published in Transactions on Machine Learning Research (02/2023)
Algorithm 1: Sampling from tangent Gaussian: a general algorithm
Input : ManifoldMof dimension d, base point w∈M, Riemannian metric ⟨.,.⟩w, meanµ∈M
and standard deviation σ>0.
Output:ξsuch thatξ∼Nw(0,σ2).
1Construct a basis BofTwMorthonormal wrt to ⟨.,.⟩w.
2Generated-dimensional coordinates a∼N(0,σ2Id).
3Generate the sample ξ∈TwMasξ=/summationtextd
i=1aiβi.
3 Sampling from tangent Gaussian
In this section, we derive various properties of the tangent Gaussian distribution to discuss different sam-
pling strategies for different manifolds. The proofs of the claims discussed in this section are provided in
Appendix B.
We begin with the definitions of the Lebesgue measure on the tangent space and the tangent Gaussian distri-
bution. We then show that the tangent Gaussian reduces to the multivariate Gaussian when an appropriate
basis is constructed. This allows sampling to be performed in the intrinsic coordinates of the tangent space
and then generate a tangent Gaussian sample by a linear combination of the basis vectors with the sampled
coordinates.
Definition 1 (Lebesgue measure on tangent space) .Consider a Riemannian manifold Mwith the intrinsic
dimension d. Forw∈ M, letB={β1,...,βd}be an orthonormal basis of TwMwith respect to the
Riemannian metric ⟨.,.⟩w. DefineϕB:Rd→TpMasϕB(c1,...,cd) =/summationtextd
i=1ciβi.Letλdenote the standard
Lebesgue measure on Rd. Then, we define the Lebesgue measure on TwMas the pushforward measure ϕB
∗
given by
(ϕB
∗λ)(S)≜λ ϕ−1
B(S).
Remark1.LetB1,B2be two orthonormal bases of TwM, thenϕB1∗λ=ϕB2∗λbecause the Lebesgue measure
is invariant under orthogonal transformations (with respect to the Riemannian metric). Hence, in the rest
of this draft, we drop the superscript Bfor clarity and denote the pushforward measure as ϕ∗λ.
We now define the tangent space Gaussian distribution (Han et al., 2022a) under the measure in Definition 1.
Definition 2 (Tangent Gaussian (Han et al., 2022a)) .Letw∈M, a random tangent vector ξ∈TwM
follows a tangent space Gaussian distribution at w, denoted as ξ∼Nw(µ,σ2)with mean µ∈TwMand
standard deviation σ>0if its density is given by
pw(ν) =Cw,σexpÇ
−∥ν−µ∥2
w
2σ2å
,
under the pushforward measure given in Definition 1.
Lemma 1. Letw∈MandBbe any orthonormal basis of TwM.Also, letξ∈TwMdenote a random
tangent vector. Then, the following holds:
1. Ifξ∼Nw(µ,σ2)for someµ∈TwMand forσ>0, thenCw,σ= (2πσ2)1/2.
2.ξ∼Nw(µ,σ2)⇐⇒⃗ξ∼N(⃗ µ,σ2Id)where⃗ξ,⃗ µ∈Rddenote coordinates in basis B.
3. Ifξ∼Nw(0,σ2), then E∥ξ∥2
w=dσ2, wheredis the dimension of the manifold.
Remark 2.Statement 3 of Lemma 1 improves the bound on variance from (Han et al., 2022a, Lemma 4) by
removing the dependency on the metric tensor Gw.
Statement 2 of Lemma 1 implies that a random tangent vector follows tangent Gaussian if and only if its
random coordinates in any orthonormal basis follow from the Euclidean Gaussian distribution of the intrinsic
dimension. This allows to avoid the computationally expensive MCMC based sampling, which is suggested
5Published in Transactions on Machine Learning Research (02/2023)
Algorithm 2: Sampling from tangent Gaussian using isometric transportation
Input : ManifoldMof dimension d, base point w∈M, Riemannian metric ⟨.,.⟩p, meanµ∈
standard deviation σ>0, reference point bw.
Output:ξsuch thatξ∼Nw(0,σ2).
1Sampledcoordinates a∼N(0,σ2Id).
2Generate the sample ζ∈T“wMatbw.
3Generate the sample ξ∈TwMby isometric transportation of ζfromT“wMtoTwM:ξ=LI“w→w(ζ).
in (Han et al., 2022a) for manifolds with non-Euclidean Riemannian metrics, and instead apply the basis
constructionapproachforanymanifold. WesummarizetheprocedureinAlgorithm1. However, thepractical
efficiency depends on how Steps 1, 2, and 3 of Algorithm 1 are implemented for different manifolds.
One natural approach for sampling from tangent Gaussian is to perform an explicit basis construction (Step
1) in which we fully enumerate the basis elements in B. Steps 2 and 3 can subsequently be performed in
a straightforward manner. The other approach is to combine Steps 1, 2, and 3 implicitly. We discuss these
approaches in the following sections.
3.1 Sampling with explicit basis enumeration
Here, we construct a basis explicitly, i.e., either analytically or by using Gram-Schmidt orthogonalization.
Gram-Schmidt orthogonalization. The tangent space at a point on a manifold is parameterized by a
system of linear equations. One approach to perform sampling is to first solve the underlying linear equations
to get the basis BofTwMthat is orthonormal in the sense of the Euclidean metric. Depending on the
Riemannian metric, we now have two further scenarios.
•When the Riemannian metric ⟨.,.⟩wis a scaled Euclidean metric, then the orthonormal basis with
respect to the Riemannian metric ⟨.,.⟩wcan be obtained by appropriate scaling of B.
•If the Riemannian metric ⟨.,.⟩wis a more general metric, we employ the Gram-Schmidt (GS) orthog-
onalization process on Bto generate a new basis that is orthonormal with respect to the Riemannian
metric⟨.,.⟩w. This is computationally expensive because if dis the dimension of manifold, then we
have to evaluateO(d2)inner products⟨.,.⟩w.
Analytic basis construction. One way to avoid the computationally prohibitive GS orthogonalization
strategy is to analytically construct bases for different manifolds. This can be done for various manifolds
by exploiting the geometry of the space. We construct the full orthonormal basis with respect to the metric
⟨.,.⟩wexplicitly by full enumeration. We empirically observe (refer Section 6) that sampling with the explicit
basis construction strategy is computationally expensive even if the basis is known analytically.
3.2 Sampling implicitly using isometric transportation
Since our end goal is to efficiently generate tangent Gaussian samples, instead of first fully constructing
the orthonormal basis and then performing linear combinations, we aim to combine Steps 1, 2, and 3 of
Algorithm 1. Hence, we do not fully enumerate the basis but rather create a basis implicitly.
Given a manifold with a Riemannian metric and depending on the basis chosen, there are many ways of
implementing the implicit basis strategy. We propose a unified way that is both computationally efficient
and easy to implement using linear isometric transportation between tangent spaces.
The key observation of this strategy is the following claim which states that to sample from the tangent
Gaussian on TwMforw∈M, one can simply sample from the tangent Gaussian from any other base
(reference) point bwand then transport the sample using any linear isometry operator from the reference
pointbwto the required base point w.
6Published in Transactions on Machine Learning Research (02/2023)
Table 1: Reference points bwfor Algorithm 2. I∈Rm×mdenotes the identity matrix. (e1,...,er)denotes
the standard basis vectors of Rmando∈Rmdenotes the zero vector. ⟨,⟩Fand⟨,⟩2denote the standard
Euclidean inner product on matrices and vectors, respectively. We observe that at specific reference points
both the Riemannian metric and tangent space expressions simply.
Manifold Metric Reference point bwTangent space T“wMMetric⟨,⟩“wAlgorithm Cost
SPDAffine-Invariant I∈Rm×mSYM (m)⟨,⟩F Alg 3O(m3)
Bures-Wasserstein I∈Rm×mSYM (m)⟨,⟩F/4 Alg 4O(m3)
Log-Euclidean I∈Rm×mSYM (m)⟨,⟩F Alg 5O(m3)
HyperoblicPoincaré ball o∈RmRm⟨,⟩2 Alg 6O(m)
Lorentz hyperboloid e1∈Rm{0}×Rm−1⟨,⟩2 Alg 7O(m)
Sphere Euclidean e1∈Rm{0}×Rm−1⟨,⟩2 Alg 8O(m)
Stiefel Euclidean [e1,...,er]∈Rm×rSKEW (r)×R(m−r)×r⟨,⟩F Alg 9O(mr2)
Grassmann Euclidean [e1,...,er]∈Rm×r{0}r×r×R(m−r)×r⟨,⟩F Alg 10O(mr2)
Claim 2. Letbw∈Mand letLI“w→w:T“wM→TwMbe any linear isometric transportation. If ξ∼
N“w(µ,σ2)for someµ∈TwMandσ>0, then
LI“w→w(ξ)∼Nw(LI“w→w(µ),σ2).
Remark 3.Linear isometry, as defined above, encompasses the parallel transport and more generally some
classes of vector transport on manifolds (Absil et al., 2009; Boumal, 2022; Huang et al., 2015; 2017). We
denote the parallel transport operation as PT and the vector transport operation as VT.
We choose the reference point bwsuch that it is relatively easy to sample from the tangent Gaussian at T“wM,
and then, isometrically transport from bwto the required point w. To be precise, we choose a reference
point where two things happen: (i)the tangent space T“wMis parametrized freely and (ii)the underlying
Riemannian metric ⟨.,.⟩“wbecomes a scaled Euclidean metric. We term this procedure as isometric trans-
portation and summarize it in Algorithm 2. The isometric transportation strategy can be seen as performing
implicit basis construction, i.e., transporting the samples from the bwto the required point wis equivalent
to transporting the tangent space basis from bwtowasξ=LI“w→wÄ/summationtextd
i=1aiβiä
=/summationtextd
i=1aiLI“w→w(βi).
Efficient implementations of these isometric transportation procedures (parallel transport and vector trans-
port) are extensively studied in the literature (Absil et al., 2009; Xie et al., 2013; Huang et al., 2015; 2017;
Thanwerdas & Pennec, 2023; Guigui & Pennec, 2022) and are readily available in many of the existing
Riemannian optimization libraries (Boumal et al., 2014; Townsend et al., 2016; Miolane et al., 2020; Utpala
et al., 2022a). Hence, a benefit of the isometric transportation strategy (Algorithm 2) is that one only needs
to take care of the sampling at bwand the rest follows through. As we see later that implementing tangent
Gaussian sampling at a properly chosen reference point bwcan be made computationally efficient. For all
the manifolds, sampling at bwamounts to simply reshaping samples from the standard normal distribution
to certain a size and is readily implementable.
4 Isometric transportation based sampling for different manifolds
In this section, we discuss the proposed sampling strategy and provide details about how to implement it for
several interesting manifolds. The rest of the section deals with how to concretely implement Algorithm 2
for several manifolds of interest. For each manifold, we include a summary of the reference points, the
metric at the points, and the concrete algorithm for sampling in Table 1. For the expressions of the parallel
transport and vector transport operations on different manifolds, see Appendix A. We illustrate through
the experiments that Algorithm 2 is significantly better than other discussed procedures in computational
efficiencyandrendersimplementationofdifferentiallyprivateoptimizationcomputationallyviable, especially
for high dimensional matrix manifolds.
7Published in Transactions on Machine Learning Research (02/2023)
4.1 SPD manifold
Let SPD (m)denote the set of symmetric positive definite matrices of size m×m. AtW∈SPD(m), the
tangent space at WisTWSPD(m) =SYM (m), where SYM (m)denotes the set of symmetric matrices of
sizem×m. (Bhatia, 2009). We consider three Riemannian metrices: the Affine-Invariant (AI) metric (Pen-
nec, 2006; Bhatia, 2009), Bures-Wasserstein (BW) metric (Bhatia et al., 2019), and Log-Euclidean (LE)
metric (Arsigny et al., 2007) to endow SPD (m)with a Riemannian structure.
LetW,cW∈SPD(m),U,V∈SYM (m)and denote C∈SYM (m)such that Cij= 1ifi=jandCij=1√
2
fori̸=jandcij=Cij.W=PDPTis the eigenvalue decomposition of W, whereP,D∈Rm×mand
Dis diagonal matrix of eigenvalues [λ1,...,λm]andPis an orthogonal matrix. We denote the Hadamard
between product two square matrices with ⊙.
SPD with Affine-Invariant metric. The AI metric defined as ⟨U,V⟩AI
W:= Tr W−1UW−1V. The
reference point for Algorithm 2 is cW=Iand the AI metric at Isimplifies as⟨U,V⟩AI
”W=Tr(UV). As
the parallel transport operation is well-known for the AI metric, we choose it as the linear isometric trans-
portation operation in Algorithm 2. The concrete implementation of Algorithm 2 for the SPD manifold with
the AI metric is shown in Algorithm 3. The computational cost of implementing Algorithm 3 is O(m3).
Furthermore, the implicit basis that is being used by Algorithm 3 at Wis
BAI
W=¶
cij.W1/2eieT
j+ejeT
iW1/2:i= 1,...,m,j =i+ 1,...,m©
,
whereW1/2=PD1/2Pdenotes the principal square root of W.
Algorithm 3: Sampling on SPD with Affine-Invariant metric
Input : Base point W.
Output: Tangent Gaussian sample U∼NW(0,σ2).
1Generate normal random vector a∼N(0,σ2Im(m+1)
2).
2Reshapea∈Rm(m+1)/2intoA∈SYM (m).
3U=PTIm→W(C⊙A).
SPD with Bures-Wasserstein metric. The BW metric is defined as ⟨U,V⟩BW
W:=Tr(LW[U]V), where
LW[U]is the solution to the matrix equation LW[U]U+ULW[U] =U. The reference point for Algorithm 2
iscW=Iand the BW metric at Isimplifies as⟨U,V⟩BW
”W=Tr(UV)/4. We choose the parallel transport
as the preferred isometric transportation procedure. The concrete implementation is shown in Algorithm 4
and the cost of implementation is O(m3). The implicit basis at Wthat is being used by Algorithm 4 is
BBW
W=¶
cijPî
K⊙Ä
PTeieT
j+ejeT
iPäó
PT:i= 1,...,m,j =i+ 1,...,m©
,
whereK∈Rm×msuch that Krs=»
λr+λs
2.
Algorithm 4: Sampling on SPD with Bures-Wasserstein metric
Input : Base point W.
Output: Tangent Gaussian sample U∼NW(0,σ2).
1Generate a normal random vector a∼N(0,σ2Im(m+1)
2).
2Reshapea∈Rm(m+1)/2intoA∈SYM (m).
3U=PTIm→W(4C⊙A).
SPD with Log-Euclidean metric. The LE metric is defined as ⟨U,V⟩LE
W:=
Tr(DLogm [W](U)DLogm [W](V)), where DLogm [W](U)is directional derivative of matrix logarithm of
Wevaluated at U.
The reference point for Algorithm 2 is cW=Iand the LE metric at Isimplifies as⟨U,V⟩LE
”W=Tr(UV). With
the parallel transport as the preferred isometric transportation procedure, the sampling implementation is
8Published in Transactions on Machine Learning Research (02/2023)
shown in Algorithm 5. The computational cost of this implementation is O(m3). The implicit basis that is
being used by Algorithm 5 at Wis
BLE
W=¶
cijPî
K⊙Ä
PTeieT
j+ejeT
iPäó
PT:i= 1,...,m,j =i+ 1,...,m©
,
whereK∈Rm×msuch that Krs=f(λr,λs), wheref(x,y) =x−y
exp (x)−exp (y)ifx̸=yelsef(x,y) =1
exp (x)
whenx=y.
Algorithm 5: Sampling on SPD with Log-Euclidean metric
Input : Base point W.
Output: Tangent Gaussian sample U∼NW(0,σ2).
1Generate a normal random vector a∼N(0,σ2Im(m+1)
2).
2Reshapea∈Rm(m+1)/2intoA∈SYM (m).
3U=PTIm→X(C⊙A).
4.2 Hyperbolic
We consider the two popular geometric models of the hyperbolic space: the Poincaré ball and the Lorentz
hyperboloid model (Nickel & Kiela, 2017; 2018).
Poincaré ball. The Poincaré ball model is defined as PB (m) ={w∈Rm:∥w∥2<1}with the metric
given by⟨u,v⟩PB
w= 4⟨u,v⟩2/(1−∥w∥2
2). The tangent space at any w∈PB(m)isTwPB(m) =Rm. The
reference point for Algorithm 2 is “w=o∈Rm, whereodenotes the zero vector. The PB metric at ois
⟨u,v⟩PB
“w=⟨u,v⟩2. The sampling algorithm is concretely shown in Algorithm 6 whose implementation cost
isO(m). The implicit basis that is being used by Algorithm 6 is
BPB
x={ei(1−∥w∥2
2)/4 :i= 1,...,m},
whereei∈Rm,i= 1,...,mdenotes the standard basis vector.
Algorithm 6: Sampling on Poincaré ball
Input : Base point w∈PB(m).
Output: Tangent Gaussian sample u∼Nw(0,σ2).
1Generate a normal random vector a∼N(0,σ2Im).
2u=PTom→x(a/4).
Lorentz hyperboloid. The Lorentizian inner product for x,w∈Rnis given by⟨x,w⟩L=−x1y1+/summationtextk
i=2xiyi.The Loretnz hyperboloid model is defined as LH (k) ={w∈Rk|⟨w,w⟩L=−1}with the
Lorentizian inner product as the Riemannian metric. The tangent space at w∈LH(k)is given by
TwLH(k) ={u∈Rk|⟨w,u⟩L= 0}. The reference point for Algorithm 2 is “w=e1∈Rm, the LH metric at e1
simplifies as⟨u,v⟩LH
“w=⟨u,v⟩2foru,v∈T“wPB(m)and tangent space simplifies as T“wPB(m) ={0}×Rm−1.
The sampling algorithm is concretely shown in Algorithm 7. The implementation cost is O(m). The implicit
basis that is being used by Algorithm 7 at wis
BLH
w=ß
¯ei−wi+1
1 +w1(ei+w) :i= 1,...,m−1™
,
where ¯ei= (0,eei)andei∈Rm,eei∈Rm−1denotes standard basis vectors for i= 1,...,m−1.
Algorithm 7: Sampling on Lorentz hyperboloid
Input : Base point w∈PB(m).
Output: Tangent Gaussian sample u∼Nw(0,σ2).
1Generate a normal random vector a∼N(0,σ2Im−1).
2Perform zero padding a= [0,a]∈Rm.
3u=PTe1→w(a).
9Published in Transactions on Machine Learning Research (02/2023)
4.3 Sphere manifold
The sphere manifold is denoted as the set SP (m) ={w∈Rm|∥w∥2= 1}and the tangent space at wis given
byTwSP(m) ={u∈Rm|⟨w,u⟩2= 0}.The Riemannian metric is the induced by the Euclidean metric, i.e.,
⟨u,v⟩w=⟨u,v⟩2.The reference point for Algorithm 2 is “w=e1∈Rmand SP metric at e1simplifies as
⟨u,v⟩SP
“w=⟨u,v⟩2. With parallel transport as the preferred isometric transportation procedure, the sampling
implementation is shown in Algorithm 8 with implementation cost O(m). The implicit basis being used by
Algorithm 8 is
BSP
w={¯ei−wi+1·w:i= 1,...,m−1},
where ¯ei= (0,eei)andeei∈Rm−1denotes the standard basis vector for i= 1,...,m−1.
Algorithm 8: Sampling on sphere
Input : Base point w∈SP(m).
Output: Tangent Gaussian sample u∼Nw(0,σ2).
1Generate a normal random vector a∼N(0,σ2Im−1).
2Perform zero padding a= [0,a]∈Rm.
3u=PTe1→w(a).
4.4 Stiefel manifold
The Stiefel manifold is the set of column orthonormal matrices, i.e., ST (m,r) ={W∈Rm×r|WTW=I}
and its tangent space at WisTWST(m,r) ={U∈Rm×r|UTW+WTU=O}, whereO∈Rr×ris
the zero matrix. The Riemannian metric is the induced by the Euclidean metric ⟨U,V⟩ST
W=Tr(UTV)
(Edelman et al., 1998). The reference point for Algorithm 2 is cW= [e1,...er]∈Rm×r, where the metric
is⟨U,V⟩ST
“w=Tr(UTV)and the tangent space is T ”WST(m,r) =SKEW (r)×R(m−r)×r, where we denote
SKEW (r)as the set of skew-symmetric matrices of size r×r. Huang et al. (2017) have proposed an efficient
isometric vector transport procedure, which we choose for implementing Algorithm 2. The concrete sampling
procedure is shown in Algorithm 9 with an implementation cost O(mr2). The implicit basis that is being
used by Algorithm 9 is
BST
W={1√
2W(eieT
j−ejeT
i) :i= 1...r,j =i+ 1,...,r}∪{W⊥eeieT
j:i= 1,...,m−r,j= 1,...,r},
whereei∈Rr,eei∈Rm−rdenotes the standard basis vectors for i= 1,...,m−1andW⊥∈Rm×(m−r)
denotes a matrix such that the columns form an orthonormal basis of the orthogonal complement of the
columns of W.
Algorithm 9: Sampling on Stiefel manifold
Input : Base point W∈ST(m).
Output: Tangent Gaussian sample, U∼NW(0,σ2).
1Generate a normal random vector a1∼N(0,σ2Ir(r−1)
2)and reshape into A1∈SKEW (r).
2Generate a normal random vector a2∼N(0,σ2I(m−r)×r)and reshape into A2∈R(m−r)×r.
3cW= [e1,...,er],A=ïA1/√
2
A2ò
∈Rm×r.
4U=VT”W→W(A).
4.5 Grassmann manifold
The Grassmann manifold GR (m,r)consists of r-dimensional linear subspaces of Rm(r≤m) and is rep-
resented as GR (m,r) ={colspan (W)|W∈Rm×r,WTW=Ir}, where colspan denotes the column space.
10Published in Transactions on Machine Learning Research (02/2023)
The tangent space at WisTWGR(m,r) ={U∈Rm×r|U∈Rm×r,WTU=Or}whereOr∈Rr×r
is zero matrix (Edelman et al., 1998). The Riemannian metric is induced by the Euclidean metric
⟨U,V⟩GR
W=Tr[UTV]forU,V∈TWGR(m,r). (Edelman et al., 1998). The reference point for Algo-
rithm 2 is cW= [e1,...er]∈Rm×r, the metric at cWis⟨U,V⟩GR
“w=Tr(UTV)and tangent space at cW
simplifies as T ”WGR(m,r) ={0}r×r×R(m−r)×r, where we denote {0}r×ras the singleton set of zero ma-
trix of size r×r. Similar to the Stiefel case, we use the vector transport may as the preferred isometric
transportation procedure (Huang et al., 2017). The sampling implementation is shown in Algorithm 10 with
computational cost O(mr2). The implicit basis that is being used by Algorithm 10 is
BGR
W={W⊥eeieT
j:i= 1,...,m−r,j= 1,...,r},
whereei∈Rr,eei∈Rm−1denote the standard basis vectors for i= 1,...,m−1andW⊥∈Rm×(m−r)
denotes a matrix such that the columns form an orthonormal basis of the orthogonal complement of the
column space of W.
Algorithm 10: Sampling on Grassmann manifold
Input : Base point W∈GR(m).
Output: Tangent Gaussian sample, U∼NW(0,σ2).
1Generate a normal random vector a∼N(0,σ2I(m−r)×r)and reshape into A∈R(m−r)×r.
2cW= [e1,...,er],A=ïOr
Aò
∈Rm×r.
3U=VT”W→W(A).
5 Private Riemannian variance reduced stochastic optimization
Variance reduced stochastic optimization methods (Roux et al., 2012; Johnson & Zhang, 2013; Defazio
et al., 2014; Reddi et al., 2016) employ a hybrid update rule that uses both full gradient and stochastic
gradientinformationsimultaneously. Bydoingso, variancereducedmethodsimprovethegradientcomplexity
compared to the stochastic and the full gradient descent methods by requiring less gradient calls to achieve
the same convergence rates than the full gradient descent method. Many variance reduction strategies that
work in the Euclidean space have also been generalized to manifolds (Zhang et al., 2016; Sato et al., 2019;
Zhou et al., 2019; Han & Gao, 2021).
Inthissection, weprivatizetheRiemannianstochasticvariancereducedgradient(RSVRG)algorithm(Zhang
et al., 2016) for solving (1) and develop a differentially private RSVRG algorithm, henceforth denoted by DP-
RSVRG. Our proposed DP-RSVRG is summarized in Algorithm 11. DP-RSVRG with restart is presented
as Algorithm 12.
DP-RSVRG has two loops. In the inner loop, an unbiased variance reduced stochastic gradient is constructed
by correcting the Riemannian stochastic gradient with the full gradient calculated at the outer loop. We
add noise from the tangent Gaussian distribution to the variance reduced gradient. The clipping operation
clipτ:TwM→TwMis defined as clipτ(ν) = min¶∥ν∥w
τ,1©
νand it ensures that the norm of νis at mostτ.
The norm of the full gradient is clipped with parameter C0and the variance reduced gradient with parameter
C1, respectively. PT refers to the parallel transport operation.
5.1 Privacy guarantee
In this section, we analyze the privacy guarantees of DP-RSVRG. We begin by noting that the variance
reduced stochastic gradient has a deterministic and a subsampled component. Hence, Step 7of Algorithm
11 can be equivalently re-written as
vs+1
t= clipC1(gradf(ws+1
t;zit))−PT‹ws→ws+1
t clipC1(gradf(ews;zit))−(gs+1+ξs
t1)+ξs
t2,(2)
11Published in Transactions on Machine Learning Research (02/2023)
Algorithm 11: DP-RSVRG
Input : update frequency m, learning rate η, number of epochs S, clipping parameters C0,C1, and
initial iterate w0.
1initialize ew=w0.
2fors= 0,1,...,S−1do
3ws+1
0=ews.
4gs+1=1
n/summationtextn
i=1clipC0(gradf(ews;zi)).
5fort= 0,1,...,m−1do
6 Randomly pick it∈{1,...,n}.
7vs+1
t= clipC1(gradf(ws+1
t;zit))−PT‹ws→ws+1
t clipC1(gradf(ews;zit))−gs+1+ϵs+1
t, where
ϵs+1
t∼Nws+1
t(0,σ2).
8ws+1
t+1=Expws+1
t(−ηvs+1
t).
9Setewa=ws+1
m.
10Output I :wpriv=ewS.
11Output II :wprivis choosen uniformly randomly from {{ws+1
t}m−1
t=0}S−1
s=0.
Algorithm 12: DP-RSVRG with restarts
Input : update frequency m, learning rate η, number of epochs S, and initial iterate w0.
1fork= 0,1,...,K−1do
2wk+1=DP-RSVRG (m,η,S,wk)with output option II.
whereξs
t1∼N‹ws(0,σ2
1)andξs
t2∼Nws+1
t(0,σ2
2). Specifically, the noise variance σ2is split into into σ2
1for
the full gradient query and σ2
2for the variance reduced stochastic gradient query such that σ2
1+σ2
2=σ2.
Claim 2 ensures that PT‹ws→ws+1
tξs
t1+ξs+1
t2=ϵs+1
t∼Nws+1
t(0,σ2). Hence, (2) can be viewed as a composition
of a full gradient tangent Gaussian mechanism
Rs(Z) =rs+1=1
nn/summationdisplay
i=1clipC0(gradf(ews;zi)) +ξs
t1,
whereξs
t1∼N‹ws(0,σ2
1)and a variance reduced Gaussian mechanism
Rs+1
t(Z) = clipC1(gradf(ws+1
t;zit))−PT‹ws→ws+1
t clipC1(gradf(ews;zit))−rs+1+ξs+1
t2,
whereξs+1
t2∼Nws+1
t(0,σ2
2). We now prove the moments bounds on the full gradient mechanism KRsand
variance reduced mechanism KRs+1
tin the following claims and the proofs are given in Section B.3.1.
Claim 3. The moments bounds satisfy
KRs(λ)≤2λ(λ+ 1)C2
0
n2σ2
1andKRs+1
t(λ)≤8λ(λ+ 1)C2
1
σ2
2.
Now we derive the moments bound on subsampled version of Rs+1
tusing the results given in (Wang et al.,
2019b;c) and the proof is given in Section B.3.2.
Claim 4. Definesubsample :Zn→Zas the process of sampling a single data point from ndata points
uniformly randomly. Define the subsampled mechanism for Rs+1
tassubRs+1
t=Rs+1
t◦subsample . Suppose
σ2≥12C2
1andλ≤2/3σ2
2log n(λ+ 1)(1 + (σ2
2/16C2
1)), we have
KsubRs+1
t(λ)≤28λ(λ+ 1)C2
1
n2σ2
2.
12Published in Transactions on Machine Learning Research (02/2023)
The full mechanism Rcan be seen as an adaptive composition of {{K subRs+1
t}m−1
t=0}S−1
s=0and{{KRs}m−1
t=0}S−1
s=0.
Sinceσ2
1+σ2
2=σ2, we can rewrite σ2
1=ασ2,σ2
2= (1−α)σ2for someα∈(0,1).Using this claim, minimizing
overα, and settingC= max{C0,C1}, we have
KR(λ)≤m/summationdisplay
t=0S−1/summationdisplay
s=0KsubRs+1
t(λ) +m/summationdisplay
t=0S−1/summationdisplay
s=0KRs+1(λ)≤2mSλ (λ+ 1)C2
0
n2σ2
1+28mSλ (λ+ 1)C2
1
n2σ2
2
⇒KR(λ)≤min
α∈(0,1)mSλ (λ+ 1)C2
n2σ2ï2
α+28
1−αò
. (3)
It should be noted that for a given λ, the minimization over αhas a closed-form solution.
The moments bound KRgiven in (3) can be converted to (ϵ,δ)guarantee using conversion rules, e.g., based
on (Mironov, 2017, Proposition 3): Given 0< δ < 1,ϵ= minλ≥1KR(λ−1)+log 1/δ
λ−1.Recently, however, the
optimal conversion rule has been given in (Asoodeh et al., 2020, Theorem 3) for which there exists no closed-
form expression but can be solved numerically to get ϵ. The solver is available in the autodplibrary (Wang
et al., 2019c). The above result connecting the moment bound KRwithαin (3) implies that tighter (ϵ,δ)
guarantees can be obtained by optimizing over α, i.e., by exploiting the inter-play between the the noise
added to the full gradient and that to the variance reduced gradient.
It should be emphasized that in the Euclidean setting, Wang et al. (2017) have not considered optimization
ofαas in (3). We empirically show that such an optimization of αobtains significant improvement in privacy
in Section 6.2. We end this section with the following privacy result for Algorithms 11 and 12.
Claim 5. Algorithms 11 and 12 are (ϵ,δ)-differentially private with σ2≥c1mSlog(1/δ)C2
n2ϵ2andσ2≥
c2mSK log(1/δ)C2
n2ϵ2, respectively, for some positive constants c1,c2andC= max{C0,C1}.
5.2 Utility guarantee
In this section, we prove the utility guarantees of DP-RSVRG under various function classes on manifolds
including geodesic strong convex functions, general nonconvex functions, and functions that satisfy the
RiemannianPolyak–Łojasiewicz(PL)condition. Inparticular, thegeodesicstrongconvexityandRiemannian
PL conditions generalize the notions of strong convexity and PL condition from the Euclidean space to
manifolds, allowing fast convergence (for problems satisfying these conditions) to global optimality when
optimizing on manifolds. The proofs of the results discussed in this section are included in Sections B.4.1-
B.4.3.
LetW⊆M be a totally normal neighborhood and DWdenotes its diameter and κminis the lower bound
on curvature ofW(discussed in details in Section 2). Following (Zhang & Sra, 2016; Han & Gao, 2021; Han
et al., 2022a), we make the below standard assumption.
Assumption 1. Eachfiin (1) is L-geodesically smooth and L0-geodesically Lipschitz over W.
The gradient complexity of an algorithm is measured in the number of incremental first-order oracle
(IFO) calls needed. An IFO (Agarwal & Bottou, 2015) takes an index i∈[n], w∈ Wand returns
(fi(w),gradfi(w))∈R×TwM. Also, for readability we hide the logfactors through notation ‹Oin the
utility bounds and gradient complexities. The exact expressions are in (11), (12) for µ-strongly convex func-
tions; (16), (17) for non-convex functions; and (18), (19) for functions with the Riemannian PL condition in
the appendix section.
Theorem 6 (Utility under geodesic strong convexity) .Suppose that Assumption 1 holds and Fisµ-
strongly geodesic convex over W. If we run the Algorithm 11 with learning rate η=O(µ
ζL2), fre-
quencym=‹O(ζL2
µ2)forS=O(log(nϵµ
log (1/δ)ζL2
0d))outer loops with output I, then E[F(wpriv)−F(w∗)] =
‹O
dζLL2
0log(1/δ)E[dist2(w0,w∗)]
µ2n2ϵ2
.Furthermore, the gradient complexity is given by ‹O(n+ζL2
µ2).
Theorem 7 (Utility under nonconvex functions) .Suppose that Assumption 1 holds. If we run the Algo-
rithm 11 with output II, learning rate η=O(1
Ln2/3ζ1/2), frequency m= Θ(n)and forS=q
Lζ
dlog(1/δ)n2/3ϵ
L0
13Published in Transactions on Machine Learning Research (02/2023)
outer loops, then E∥gradF(wpriv)∥2≤L0√
dLlog(1/δ)E[F(w0)−F(w∗)]
nϵ.The gradient complexity is given by
O(q
Lζ
dlog(1/δ)n5/3ϵ
L0).
We now use Algorithm 12 to achieve utility guarantee under the Riemannian PL condition.
Theorem 8 (Utility under Riemannian PL condition) .Suppose that Assumption 1 holds and F=
1
n/summationtextn
i=1fi(w)satisfies the Riemannian PL condition with parameter τ. If we run Algorithm 12 with
learning rate η=O(1
Ln2/3ζ1/2), frequency m= Θ(n),S=O(1), andK= log(n2ϵ2
dLτ2log(1/δ)L2
0), then
E[F(wpriv)−F(w∗)]≤‹O(dLτ2log(1/δ)L2
0
n2ϵ2 ).Furthermore, the gradient complexity is given by ‹O(Lτζ1/2n2/3).
5.3 Discussion: DP-RGD vs DP-RSGD vs DP-RSVRG
In this section, we compare our proposed DP-RSVRG with DP-RGD (Han et al., 2022a) and DP-RSGD
(Han et al., 2022a).
1.Strongly geodesic convex : DP-RSGD and DP-RGD both assume fiin (1) to be µ−strongly
g-convex. In contrast, DP-RSVRG in Theorem 6 just assumes that F=/summationtextn
i=1fito beµ-strongly
g-convex, which is a much weaker assumption. Furthermore, DP-RSVRG assumes fito beL-g-
smooth, while DP-RGD, DP-RSGD do not make any smoothness assumption.
Forµ-strongly g-convex functions, DP-RGD and DP-RSGD obtain the utility bound
O
dζL2
0log (1/δ)E[dist2(w0,w∗)]
µn2ϵ2
with gradient complexities n2andn3, respectively (Han et al., 2022a,
Theorem 3). On the other hand, DP-RSVRG obtains a utility bound ‹O
dζLL2
0log(1/δ)E[dist2(w0
S,w∗)]
µ2n2ϵ2
in‹O(n+ζL2
µ2)IFO calls. DP-RSVRG bounds are worse in terms of condition number L/µdue to
the weaker assumption as discussed above.
2.Riemannian PL condition : DP-RSGD and DP-RGD both assume fiin (1) to satisfy the Rie-
mannian PL condition with parameter τ. On the other hand, DP-RSVRG in Theorem 8 assumes
thatF=/summationtextn
i=1fisatisfies the same condition, which is weaker.
DP-RGD and DP-RSGD obtain utility bound of ‹O
τ−1dlog(1/δ)L2
0E[F(w0)−F(w∗)]
n2ϵ2
in
nlog
n2ϵ2
dL2
0log(1/δ)
and log
n2ϵ2
dL2
0log(1/δ)
IFO calls, respectively. DP-RSVRG obtains a utility
bound‹O
dLτ2log(1/δ)L2
0
n2ϵ2
in‹O(Lτζ1/2n2/3)IFO calls. DP-RSVRG bounds are worse in terms of
PL parameter τbecause of weaker assumption as mentioned above.
3.Nonconvex : In the nonconvex setting, only a bound on the gradient norm can be obtained instead
of a bound on the excess risk. Both DP-RGD and DP-RSGD obtain bound on gradient norm as
O(L0√
dLlog(1/δ)
nϵ)inO(√
Ln2ϵ
L0√
dlog(1/δ))andO(√
Lnϵ
L0√
dlog(1/δ))iterations respectively (Han et al., 2022a,
Theorem 5). From Theorem 7, DP-RSVRG obtains bound on gradient as O(L0√
dLlog(1/δ)
nϵ)in
O(q
Lζ
dlog(1/δ)n5/3ϵ
L0)iterations. Hence, in this case, DP-RGD, DP-RSGD, and DP-RSVRG have the
same matching utility bounds.
6 Experiments
In this section, we illustrate the efficacy of the proposed sampling procedures and the proposed DP-RSVRG
algorithm. We also show the benefit of αoptimization (Section 5.1) in terms of the gain in privacy guarantee.
6.1 Benchmarking of different sampling procedures
We benchmark our proposed isometric transportation (Algorithm 2) based sampling, denoted as ‘Trans-
portation’, with the following three baselines.
14Published in Transactions on Machine Learning Research (02/2023)
20 40
m103
102
101
100101102Time in Secs
Gram-Schmidt
Explicit
Transportation
(a) SPD with AI.
20 40
m103
101
101Time in Secs
Gram-Schmidt
Explicit
Transportation (b) SPD with BW.
20 40
m103
102
101
100101102Time in Secs
Gram-Schmidt
Explicit
Transportation (c) SPD with LE.
2000 4000
m104
103
102
101
Time in Secs
Explicit
Transportation (d) Sphere manifold.
250 500 750 1000
m104
103
102
101
100Time in Secs
Explicit
Explicit-Sparse
Transportation
(e) Stiefel r= 5.
250 500 750 1000
m103
102
101
100101Time in Secs
Explicit
Explicit-Sparse
Transportation (f) Stiefel r= 20.
250 500 750 1000
m103
102
101
100Time in Secs
Explicit
Explicit-Sparse
Transportation (g) Grassmann r= 5.
250 500 750 1000
m103
102
101
100101Time in Secs
Explicit
Explicit-Sparse
Transportation (h) Grassmann r= 20.
500 1000 1500 2000
m105
104
103
102
Time in Secs
Explicit
Explicit-Sparse
Transportation
(i) Poincaré ball.
500 1000 1500 2000
m104
103
102
101
100101Time in Secs
Gram-Schmidt
Explicit
Transportation (j) Lorentz hyperboloid.
Figure 1: Benchmarking of different sampling strategies. As can be seen, our proposed method ‘Transporta-
tion’ consistently outperforms the other baselines on the manifolds.
1.Sampling using Gram-Schmidt. We perform Gram-Schmidt orthogonalization on the basis B
thatisorthonormalwrttheEuclideanmetric. ThisisabaselinefortheSPDandLorentzhyperboloid
manifolds because for other manifolds, the orthonormal basis with respect to the underlying metric
⟨.,.⟩wcan be simply obtained by scaling B. This is denoted as ‘Gram-Schmidt’.
2.Sampling using explicit basis. We take the implicit bases generated by the isometric trans-
portation strategy (Algorithm 2) and generate them explicitly, i.e., construct the full basis and then
perform linear combinations. This is denoted as ‘Explicit’.
3.Sampling using explicit basis by exploiting sparsity. As an additional baseline, we implement
sampling with explicit basis construction using sparse operations. Sparsity is present in Stiefel,
Grassmann, and Poincaré ball bases, and is therefore a baseline only for these three manifolds. This
is denoted as ‘Explicit-Sparse’.
In Figure 1, we benchmark the sampling time for generating a single sample from the tangent Gaussian
distribution on various manifolds discussed in Section 4. For SPD (m), we consider m={5,10,20,30,50}.
15Published in Transactions on Machine Learning Research (02/2023)
Table 2: Overhead of privatizations for DP-RSGD (with 3×105epochs) for the SPD Fréchet mean and
the principal eigenvector problems. Our proposed isometric transportation based sampling strategy lead to
orders of magnitude improvements than those of Han et al. (2022a).
Manifold Size Han et al. (2022a) This work
SPD 11×11 660 hrs 41seconds (∼104improvement)
Sphere 786 668 seconds 24seconds (∼10improvement)
For PB (m), LH(m), and SP (m), we consider m={250,500,1000,1500,2000}. For GR (m,r)and ST (m,r),
we consider m={100,250,500,750,1000}andr={10,20}.
Figure 1 shows the average sampling time over five different base points chosen at random. From the figure,
we see that the transportation sampling strategy is faster by two to four orders of magnitude than all the
consideredbaselines. Italsoshowsthebenefitofthetransportationstrategyasaunifiedsamplingframework.
We study the benefits of the proposed sampling procedures in two problems: private estimation of the SPD
Fréchet mean and the principal eigenvector (discussed in Section 6.3). We use DP-RSGD algorithm for both
problems and compare our sampling strategy with that developed in (Han et al., 2022a). The results are
shown in Table 2. We observe that the proposed sampling strategy offers significant improvements leading
to minimal overhead due to privatization.
6.2 Optimizing αin moments bound for better (ϵ,δ)guarantees
0 20 40
Epochs(S)0.10.20.30.40.50.6
opt(=0.1)
half(=0.1)
opt(=0.05)
half(=0.05)
Figure 2: Improving pri-
vacy withα.We now show that better privacy guarantees can be empirically achieved by op-
timizingαin moments bound (Section 5.1). To this end, we use the autodp
library (Wang et al., 2019c) and set σ1=√ασ,σ 2=p
(1−α)σinstead of the
standard setting σ1=σ2=σ/√
2. We fixC1= 0.1,C2= 0.01and frequency
tom= 10000 andn= 100000 . The results are shown in Figure 2 for epochs
S={1,5,10,25,50,100}and noiseσ={0.1,0.05}. We observe that our pro-
posal to optimize over αsignificantly improves the privacy guarantees than the
standard setting. For noise level σ= 0.05, we obtain ϵ= 0.47, while the standard
setting achieves ϵ= 0.64leading to a 1.6×improvement in privacy guarantee.
6.3 Benchmarking DP-RSVRG
In this section, we compare our proposed DP-SVRG with DP-RGD and DP-RSGD (Han et al., 2022a) for
the task of computing the Fréchet mean and leading eigenvector with privacy configuration ϵ={0.1,0.3,0.5}
andδ= 10−6. The parameter details for all the algorithms are in Section C.
Private Fréchet mean on SPD manifold. We consider the problem of privately estimating the Fréchet
mean of SPD matrices under the Affine-Invariant metric. We select images from PATHMNIST medical
imaging dataset (Yang et al., 2021) and pass them through the covariance descriptor pipeline to generate
images, each represented as a SPD matrix of size 11×11. Please refer to Section C.1 for more details
on the problem formulation and covariance descriptors. We consider the two sets consisting of 10704and
10356images from two different classes. For each set, we compute the optimal Fréchet mean by running
the (non-private) RGD for 1000epochs with learning rate set to 0.5. For both the sets, we plot excess risk
against the IFO calls in Figure 3a averaged over five randomized runs. The plots corresponding to the two
sets are shown in the two rows of Figure 3a.
Private principal eigenvector computation on sphere. We also consider the problem of computing the
leading eigenvector a symmetric matrix, details in Section C.2. We take images from two classes of MNIST
and generate 784vectors to form two sets of 6903and7877images. For each set, we compute the covariance
matrix and compute its leading eigenvector by using eigen-decomposition of matrix 1/n/summationtextn
i=1zizT
ito find
the optimal solution. We plot the excess risk against the IFO calls in Figure 3b averaged over five randomized
runs. The plots corresponding to the two sets are shown in the two rows of Figure 3b.
16Published in Transactions on Machine Learning Research (02/2023)
0123
IFO calls ×105101
100101102Excess Risk=0.1
0123
IFO calls ×105102
101
100101102=0.3
0123
IFO calls ×105102
101
100101102=0.5
0.00.51.01.52.0
IFO calls ×105100101102103Excess Risk=0.1
0.00.51.01.52.0
IFO calls ×105101
100101102103=0.3
0123
IFO calls ×105101
100101102103=0.5
DP-RGD
DP-RSGD
DP-RSVRG
(a) Private Fréchet mean of the medical imaging data on the SPD manifold.
0.00.51.01.52.0
IFO calls ×1051012×1013×1014×1016×101Excess Risk=0.1
0123
IFO calls ×105101=0.3
0123
IFO calls ×105100101=0.5
012
IFO calls ×105101Excess Risk=0.1
01234
IFO calls ×105100101=0.3
01234
IFO calls ×105100101=0.5
DP-RGD
DP-RSGD
DP-RSVRG
(b) Private principal eigenvector on MNIST dataset.
0123
IFO calls ×104102
101
100Excess Risk=0.1
0 1 2
IFO calls ×104102
101
100=0.3
0.00.51.0
IFO calls ×104103
102
101
100=0.5
DP-RGD
DP-RSGD
DP-RSVRG
(c) Private Fréchet mean of Poincaré embeddings.
Figure 3: Comparison between DP-RGD, DP-RSGD, and DP-RSVRG. Each row in (a), (b), and (c) corre-
sponds to consistent dataset. We see the proposed DP-SVRG achieves a comparable excess risk compared
to the baselines with lower number of IFO calls.
17Published in Transactions on Machine Learning Research (02/2023)
Private Fréchet mean of the Poincaré word embeddings. We generate the hierarchy tree of transitive
closureofmammalsubtreeoftheWordNetdataset(Miller,1995), alexicaldatabase, andcomputetheprivate
Fréchet mean of the Poincaré word embeddings (Nickel & Kiela, 2017). WordNet provides relationship
between pairs of concepts. For instance, the ‘mammal’ subtree of WordNet has the concept ‘mammal’ as the
root node and the ‘is-a’ (hypernymy) relationship defines its edges: ‘tiger’ is-a ‘mammal’, ‘lion’ is-a ‘rodent’,
etc. The mammal subtree consists of 1180nodes and 6540edges. The results are shown in Figure 3c.
Results. In Figure 3a, we observe that the proposed DP-RSVRG obtains an overall better excess risk
in computing the private Fréchet mean of the two classes (corresponding to the two rows in Figure 3a)
on medical imaging data. In Figure 3a) first row, DP-RSVRG performs consistently better than both the
baselines. In Figure 3a) second row, DP-RSVRG performs better than DP-RGD and is similar to DP-SRGD.
In Figure 3b, the benefit of variance reduction is clearly observed. In both rows of Figure 3b, the proposed
DP-SVRG consistently outperforms DP-RGD and DP-RSGD in the gradient calls and achieves a good excess
risk. On the Fréchet mean computation of the Poincaré embeddings (Figure 3c), we see that the benefit
of variance reduction as well as the proposed DP-RSVRG performs better than DP-RSGD especially in
lowϵregime (more stringent private setting). In all cases, DP-RGD performs the best and our proposed
DP-RSVRG matches the performance in the initial iterations.
Overall, we observe that the proposed DP-RSVRG obtains better or comparable excess risk against DP-GD
and DP-SGD with generally fewer IFO calls across different levels of noise injection.
7 Conclusion
In this work, we have improved the framework of differentially private Riemannian optimization via efficient
sampling and variance reduction. We have proposed a linear isometry based sampling strategy to generate
tangentGaussiansamples. ThislargelyreducesthecostofprivatizingRiemannianoptimization. Inaddition,
we have shown how variance reduction improves the gradient complexity in practice. We believe this work
allows Riemannian optimization to be privatized efficiently for large-scale applications.
References
MartinAbadi, AndyChu, IanGoodfellow, HBrendanMcMahan, IlyaMironov, KunalTalwar, andLiZhang.
Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer
and communications security , pp. 308–318, 2016. 1, 4
John M Abowd. The US census bureau adopts differential privacy. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining , pp. 2867–2867, 2018. 1
P-A Absil, Christopher G Baker, and Kyle A Gallivan. Trust-region methods on Riemannian manifolds.
Foundations of Computational Mathematics , 7(3):303–330, 2007. 2
P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. In
Optimization Algorithms on Matrix Manifolds . Princeton University Press, 2009. 3, 7, 24
Alekh Agarwal and Leon Bottou. A lower bound for the optimization of finite sums. In International
conference on machine learning , pp. 78–86. PMLR, 2015. 13
D Apple. Learning with privacy at scale. Apple Machine Learning Journal , 1(8), 2017. 1
VincentArsigny,PierreFillard,XavierPennec,andNicholasAyache. Geometricmeansinanovelvectorspace
structure on symmetric positive-definite matrices. SIAM Journal on Matrix Analysis and Applications , 29
(1):328–347, 2007. doi: 10.1137/050637996. 8
Shahab Asoodeh, Jiachun Liao, Flavio P Calmon, Oliver Kosut, and Lalitha Sankar. A better bound gives a
hundred rounds: Enhanced privacy guarantees via f-divergences. In 2020 IEEE International Symposium
on Information Theory (ISIT) , pp. 920–925. IEEE, 2020. 13
18Published in Transactions on Machine Learning Research (02/2023)
Borja Balle, Giovanni Cherubin, and Jamie Hayes. Reconstructing training data with informed adversaries.
arXiv preprint arXiv:2201.04845 , 2022. 1
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algo-
rithms and tight error bounds. In 2014 IEEE 55th annual symposium on foundations of computer science ,
pp. 464–473. IEEE, 2014. 1
Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic convex
optimization with optimal rates. In Advances in Neural Information Processing Systems , volume 32, 2019.
1
RaefBassily, Cristóbal Guzmán, andAnupama Nandi. Non-Euclideandifferentially private stochastic convex
optimization. In Conference on Learning Theory , pp. 474–499. PMLR, 2021. 1
Eugenio Beltrami. Teoria fondamentale degli spazii di curvatura costante: memoria . F. Zanetti, 1868. 2
Rajendra Bhatia. Positive definite matrices. In Positive Definite Matrices . Princeton university press, 2009.
2, 8, 24
Rajendra Bhatia, Tanvi Jain, and Yongdo Lim. On the Bures–Wasserstein distance between positive definite
matrices. Expositiones Mathematicae , 37(2):165–191, 2019. 8
Mukul Bhutani, Pratik Jawanpuria, Hiroyuki Kasai, and Bamdev Mishra. Low-rank geometric mean metric
learning. ICML workshop on Geometry in Machine Learning (GiMLi), 2018. 2
Silvere Bonnabel. Stochastic gradient descent on Riemannian manifolds. IEEE Transactions on Automatic
Control, 58(9):2217–2229, 2013. 2
NicolasBoumal. Anintroductiontooptimizationonsmoothmanifolds. ToappearwithCambridgeUniversity
Press, Apr 2022. 3, 7, 24
Nicolas Boumal and Pierre-antoine Absil. RTRMC: A Riemannian trust-region method for low-rank matrix
completion. Advances in neural information processing systems , 24, 2011. 2
Nicolas Boumal, Bamdev Mishra, P-A Absil, and Rodolphe Sepulchre. Manopt, a matlab toolbox for
optimization on manifolds. The Journal of Machine Learning Research , 15(1):1455–1459, 2014. 7
Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating
and testing unintended memorization in neural networks. In Proceedings of the 28th USENIX Conference
on Security Symposium , SEC’19, pp. 267–284, USA, 2019. USENIX Association. ISBN 9781939133069. 1
Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership
inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy (SP) , pp.
1897–1914. IEEE, 2022. 1
Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical risk mini-
mization. Journal of Machine Learning Research , 12(3), 2011. 1
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: a fast incremental gradient method with
support for non-strongly convex composite objectives. Advances in neural information processing systems ,
27, 2014. 11
Manfredo Perdigao Do Carmo and J Flaherty Francis. Riemannian geometry , volume 6. Springer, 1992. 3
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, our-
selves: Privacy via distributed noise generation. In Annual international conference on the theory and
applications of cryptographic techniques , pp. 486–503. Springer, 2006a. 4
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private
data analysis. In Theory of cryptography conference , pp. 265–284. Springer, 2006b. 1
19Published in Transactions on Machine Learning Research (02/2023)
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found. Trends Theor.
Comput. Sci. , 9(3-4):211–407, 2014. 4
Alan Edelman, Tomás A Arias, and Steven T Smith. The geometry of algorithms with orthogonality
constraints. SIAM journal on Matrix Analysis and Applications , 20(2):303–353, 1998. 2, 10, 11
Úlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. Rappor: Randomized aggregatable privacy-
preserving ordinal response. In Proceedings of the 2014 ACM SIGSAC conference on computer and com-
munications security , pp. 1054–1067, 2014. 1
Mikhael Gromov. Hyperbolic groups . Springer, 1987. 2
Nicolas Guigui and Xavier Pennec. Numerical accuracy of ladder schemes for parallel transport on manifolds.
Foundations of Computational Mathematics , 22(3):757–790, 2022. 7
Hatem Hajri, Ioana Ilea, Salem Said, Lionel Bombrun, and Yannick Berthoumieu. Riemannian Laplace
distribution on the space of symmetric positive definite matrices. Entropy, 18(3):98, 2016. 4
Andi Han and Junbin Gao. Improved variance reduction methods for Riemannian non-convex optimization.
IEEE Transactions on Pattern Analysis and Machine Intelligence , 2021. 2, 11, 13
Andi Han, Bamdev Mishra, Pratik Kumar Jawanpuria, and Junbin Gao. On riemannian optimization over
positive definite matrices with the bures-wasserstein geometry. Advances in Neural Information Processing
Systems, 34:8940–8953, 2021. 2
Andi Han, Bamdev Mishra, Pratik Jawanpuria, and Junbin Gao. Differentially private Riemannian opti-
mization. arXiv preprint arXiv:2205.09494 , 2022a. 2, 4, 5, 6, 13, 14, 16, 26
Andi Han, Bamdev Mishra, Pratik Jawanpuria, and Junbin Gao. Riemannian block SPD coupling manifold
and its application to optimal transport. Machine Learning Journal , 2022b. 2
Wen Huang, Kyle A Gallivan, and P-A Absil. A Broyden class of quasi-Newton methods for Riemannian
optimization. SIAM Journal on Optimization , 25(3):1660–1685, 2015. 7, 24
WenHuang,P-AAbsil,andKyleAGallivan. Intrinsicrepresentationoftangentvectorsandvectortransports
on matrix manifolds. Numerische Mathematik , 136(2):523–543, 2017. 7, 10, 11, 24
Roger Iyengar, Joseph P Near, Dawn Song, Om Thakkar, Abhradeep Thakurta, and Lun Wang. Towards
practical differentially private convex optimization. In 2019 IEEE Symposium on Security and Privacy
(SP), pp. 299–316. IEEE, 2019. 1
Pratik Jawanpuria and Bamdev Mishra. A unified framework for structured low-rank matrix learning. In
ICML, 2018. 2
Pratik Jawanpuria, Arjun Balgovind, Anoop Kunchukuttan, and Bamdev Mishra. Learning multilingual
word embeddings in latent metric space: A geometric approach. Transactions of the Association for
Computational Linguistics , 7:107–120, 2019a. 2
Pratik Jawanpuria, Mayank Meghwanshi, and Bamdev Mishra. Low-rank approximations of hyperbolic
embeddings. In IEEE Conference on Decision and Control , 2019b. 2
Pratik Jawanpuria, Mayank Meghwanshi, and Bamdev Mishra. Geometry-aware domain adaptation for
unsupervised alignment of word embeddings. In Annual Meeting of the Association for Computational
Linguistics , 2020a. 2
Pratik Jawanpuria, Mayank Meghwanshi, and Bamdev Mishra. A simple approach to learning unsupervised
multilingual embeddings. In Conference on Empirical Methods in Natural Language Processing , 2020b. 2
PratikJawanpuria,N.T.V.SatyaDev,andBamdevMishra. Efficientrobustoptimaltransport: formulations
and algorithms. In IEEE Conference on Decision and Control , 2021. 2
20Published in Transactions on Machine Learning Research (02/2023)
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction.
Advances in neural information processing systems , 26, 2013. 11
Hiroyuki Kasai, Pratik Jawanpuria, and Bamdev Mishra. Riemannian adaptive stochastic gradient algo-
rithms on matrix manifolds. In International Conference on Machine Learning (ICML) , 2019. 2
Jakob Nikolas Kather, Johannes Krisam, Pornpimol Charoentong, Tom Luedde, Esther Herpel, Cleo-Aron
Weis, Timo Gaiser, Alexander Marx, Nektarios A Valous, Dyke Ferber, et al. Predicting survival from
colorectal cancer histology slides using deep learning: A retrospective multicenter study. PLoS medicine ,
16(1):e1002730, 2019. 33
John M Lee. Riemannian manifolds: an introduction to curvature , volume 176. Springer Science & Business
Media, 2006. 2, 3
Aaron Lou, Isay Katsman, Qingxuan Jiang, Serge Belongie, Ser-Nam Lim, and Christopher De Sa. Differ-
entiating through the Fréchet mean. In International Conference on Machine Learning , pp. 6393–6403.
PMLR, 2020. 24
George A Miller. Wordnet: a lexical database for english. Communications of the ACM , 38(11):39–41, 1995.
18
Nina Miolane, Nicolas Guigui, Alice Le Brigant, Johan Mathe, Benjamin Hou, Yann Thanwerdas, Stefan
Heyder, Olivier Peltre, Niklas Koep, Hadi Zaatiti, et al. Geomstats: a Python package for riemannian
geometry in machine learning. Journal of Machine Learning Research , 21(223):1–9, 2020. 7, 32
NinaMiolane, MatteoCaorsi, UmbertoLupo, MariusGuerard, NicolasGuigui, JohanMathe, YannCabanes,
Wojciech Reise, Thomas Davies, António Leitão, et al. Iclr 2021 challenge for computational geometry &
topology: Design and results. arXiv preprint arXiv:2108.09810 , 2021. 32
Ilya Mironov. Rényi differential privacy. In 2017 IEEE 30th computer security foundations symposium
(CSF), pp. 263–275. IEEE, 2017. 4, 13
Bamdev Mishra, NTV Satyadev, Hiroyuki Kasai, and Pratik Jawanpuria. Manifold optimization for non-
linear optimal transport problems. arXiv:2103.00902 , 2021. 2
Adele Myers, Saiteja Utpala, Shubham Talbar, Sophia Sanborn, Christian Shewmake, Claire Donnat, Johan
Mathe, Rishi Sonthalia, Xinyue Cui, Tom Szwagier, et al. Iclr 2022 challenge for computational geometry
& topology: Design and results. In Topological, Algebraic and Geometric Learning Workshops 2022 , pp.
269–276. PMLR, 2022. 32
Joe Near. Differential privacy at scale: Uber and Berkeley collaboration. In Enigma 2018 (Enigma 2018) ,
2018. 1
Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical representations. Ad-
vances in neural information processing systems , 30, 2017. 2, 9, 18
Maximillian Nickel and Douwe Kiela. Learning continuous hierarchies in the Lorentz model of hyperbolic
geometry. In International Conference on Machine Learning , pp. 3779–3788. PMLR, 2018. 2, 9
Madhav Nimishakavi, Pratik Jawanpuria, and Bamdev Mishra. A dual framework for low-rank tensor
completion. In NeurIPS , 2018. 2
XavierPennec. Intrinsicstatisticsonriemannianmanifolds: Basictoolsforgeometricmeasurements. Journal
of Mathematical Imaging and Vision , 25(1):127–154, 2006. 8
Xavier Pennec, Pierre Fillard, and Nicholas Ayache. A Riemannian framework for tensor computing. Inter-
national Journal of computer vision , 66(1):41–66, 2006. 24
Guodong Qi, Huimin Yu, Zhaohui Lu, and Shuzhao Li. Transductive few-shot classification on the oblique
manifold. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 8412–8422,
2021. 2
21Published in Transactions on Machine Learning Research (02/2023)
Md Atiqur Rahman, Tanzila Rahman, Robert Laganière, Noman Mohammed, and Yang Wang. Membership
inference attack against differentially private deep learning model. Trans. Data Priv. , 11(1):61–79, 2018.
1
Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabás Póczos, and Alex Smola. Stochastic variance reduction
for nonconvex optimization. In International conference on machine learning , pp. 314–323. PMLR, 2016.
11
Matthew Reimherr, Karthik Bharath, and Carlos Soto. Differential privacy over Riemannian manifolds.
Advances in Neural Information Processing Systems , 34:12292–12303, 2021. 2, 4
Christian P Robert and George Casella. Monte Carlo statistical methods , volume 2. Springer, 1999. 2
Nicolas Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential conver-
gence rate for finite training sets. Advances in neural information processing systems , 25, 2012. 11
Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and Hervé Jégou. White-box vs
black-box: Bayes optimal strategies for membership inference. In International Conference on Machine
Learning , pp. 5558–5567. PMLR, 2019. 1
Hiroyuki Sato, Hiroyuki Kasai, and Bamdev Mishra. Riemannian stochastic variance reduced gradient
algorithm with retraction and vector transport. SIAM Journal on Optimization , 29(2):1444–1472, 2019.
2, 11
ArminSchwartzman. Lognormaldistributionsandgeometricaveragesofsymmetricpositivedefinitematrices.
International Statistical Review , 84(3):456–486, 2016. 4
Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially
private updates. In 2013 IEEE global conference on signal and information processing , pp. 245–248. IEEE,
2013. 1
Carlos J Soto, Karthik Bharath, Matthew Reimherr, and Aleksandra Slavkovic. Shape and structure pre-
serving differential privacy. In Advances in Neural Information Processing Systems , 2022. 2
RyotaSuzuki, RyusukeTakahama, andShunOnoda. Hyperbolicdiskembeddingsfordirectedacyclicgraphs.
InInternational Conference on Machine Learning , pp. 6066–6075. PMLR, 2019. 2
Yann Thanwerdas and Xavier Pennec. O(n)-invariant Riemannian metrics on SPD matrices. Linear Algebra
and its Applications , 661:163–201, 2023. 7, 24
James Townsend, Niklas Koep, and Sebastian Weichwald. Pymanopt: A python toolbox for optimization
on manifolds using automatic differentiation. arXiv preprint arXiv:1603.03236 , 2016. 7
Saiteja Utpala, Andi Han, Pratik Jawanpuria, and Bamdev Mishra. Rieoptax: Riemannian optimization in
JAX. In OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop) , 2022a. 7
SaitejaUtpala, PraneethVepakomma, andNinaMiolane. DifferentiallyprivateFréchetmeanonthemanifold
of symmetric positive definite (SPD) matrices. arXiv preprint arXiv:2208.04245 , 2022b. 2, 4
Salil Vadhan. The complexity of differential privacy. In Tutorials on the Foundations of Cryptography , pp.
347–450. Springer, 2017. 4
Di Wang, Minwei Ye, and Jinhui Xu. Differentially private empirical risk minimization revisited: Faster and
more general. Advances in Neural Information Processing Systems , 30, 2017. 1, 2, 13
Di Wang, Changyou Chen, and Jinhui Xu. Differentially private empirical risk minimization with non-convex
loss functions. In International Conference on Machine Learning , pp. 6526–6535, 2019a. 1
Lingxiao Wang, Bargav Jayaraman, David Evans, and Quanquan Gu. Efficient privacy-preserving stochastic
nonconvex optimization. arXiv preprint arXiv:1910.13659 , 2019b. 12, 26
22Published in Transactions on Machine Learning Research (02/2023)
Yu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled Rényi differential privacy
and analytical moments accountant. In The 22nd International Conference on Artificial Intelligence and
Statistics , pp. 1226–1235. PMLR, 2019c. 12, 13, 16
LarryWassermanandShuhengZhou. Astatisticalframeworkfordifferentialprivacy. Journal of the American
Statistical Association , 105(489):375–389, 2010. 4
Qian Xie, Sebastian Kurtek, Huiling Le, and Anuj Srivastava. Parallel transport of deformations in shape
space of elastic surfaces. In Proceedings of the IEEE International Conference on Computer Vision , pp.
865–872, 2013. 7
Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing
Ni. Medmnist v2: A large-scale lightweight benchmark for 2d and 3d biomedical image classification.
arXiv preprint arXiv:2110.14795 , 2021. 16
Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In Conference on
Learning Theory , pp. 1617–1638. PMLR, 2016. 4, 13, 27
Hongyi Zhang, Sashank J Reddi, and Suvrit Sra. Riemannian SVRG: Fast stochastic optimization on
Riemannian manifolds. Advances in Neural Information Processing Systems , 29, 2016. 2, 3, 4, 11, 27, 29,
31, 33
Jiaqi Zhang, Kai Zheng, Wenlong Mou, and Liwei Wang. Efficient private ERM for smooth objectives. In
International Joint Conference on Artificial Intelligence , pp. 3922–3928, 2017. 1
Pan Zhou, Xiao-Tong Yuan, and Jiashi Feng. Faster first-order methods for stochastic non-convex opti-
mization on Riemannian manifolds. In The 22nd International Conference on Artificial Intelligence and
Statistics , pp. 138–147. PMLR, 2019. 2, 11
Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. Advances in neural information
processing systems , 32, 2019. 1
23Published in Transactions on Machine Learning Research (02/2023)
A Details about parallel transport and vector transport
A.1 Parallel transport expressions for SPD, hyperbolic, and sphere manifolds
SPD manifold. For the Affine-Invariant and the Log-Euclidean metrics, the parallel transport operation
of a tangent vector U∈SYM (m)fromcWtoW,cW,W∈SPD(m)is available is closed form (Bhatia, 2009;
Pennec et al., 2006; Thanwerdas & Pennec, 2023).
For the Bures-Wasserstein metric, there is no closed-form expression for the parallel transport operation for
general cW,W. However, when cWandWcommute, there exists a closed-form expression (Thanwerdas &
Pennec, 2023). We exploit this for our case as cW=I(Algorithm 4), i.e., any base point always commutes
with the reference point. Below, we list the parallel transport expressions for all the three metrics, i.e.,
Affine-Invariant: PT”W→W(U) = (WcW−1)1
2U(cW−1W)1
2,
Bures-Wasserstein: PT”W→W(U) =Pî
KBW⊙Ä
PTUPäó
PT,
Log-Euclidean: PT”W→W(U) =Pî
KLE⊙Ä
PTUPäó
PT,
whereKBW∈Rm×msuch that (KBW)rs=λr+λs
δr+δs,KLE∈Rm×msuch that (KLE)rs=f(λr,λs). Here,
f(x,y) =x−y
exp (x)−exp (y)ifx̸=yelsef(x,y) =1
exp (x)whenx=yand(δ1,...,δm)and (λ1,...,λm)∈Rm
denotes the eigenvalues of cWandW, respectively.
Hyperbolic manifold. The parallel transport expressions can be found in (Lou et al., 2020),
Poincaré ball: PT“w→w(u) =1−∥w∥2
2
1−∥“w∥2
2gyr[w,−“w](u),gyr[“w,w](u) = (o⊖(“w⊕w))⊕(“w⊕(w⊕u)),
where“w⊕w=[(1 + 2⟨“w,w⟩2+∥w∥2
2)“w+ (1−∥“w∥2
2)w]
[1 + 2⟨“w,w⟩2+∥“w∥2
2∥w∥2
2],“w⊖w=“w⊕−w.
Lorentz hyperboloid: PT“w→w(u) =u−⟨w,u⟩L
1−⟨“w,w⟩L(“w+w).
Sphere manifold. The parallel transport expression can be found in (Absil et al., 2009; Boumal, 2022),
PT“w→w(u) =Å
I+ (cos∥v∥2−1)vvT
∥v∥2−sin∥v∥2“wvT
∥v∥2ã
u,
wherev=Exp−1
“ww= arccos⟨“w,w⟩2(I−“w“wT)(w−“w)
∥(I−“w“wT)(w−“w)∥2.
A.2 Vector transport for Stiefel and Grassmann manifolds
Efficient vector transport on the Stiefel and Grassmann manifolds are provided in (Huang et al., 2017) which
proposes a strategy called transportation by parallelization (Huang et al., 2015). For the exact algorithms,
see (Huang et al., 2017, Algorithms 3, 4, and 5).
B Proofs
B.1 Proof of Lemma 1
Theorem 9 (Change of variable formula) .LetX,Ybe measurable space and ϕ:X→Yandf:Y→Ris
measurable mapping and let λbe measure on Xandϕ∗λdenote the pushforward measure of λthroughϕon
Ythen/integraltext
Yfd(ϕ∗λ) =/integraltext
Xf◦ϕdλ.
24Published in Transactions on Machine Learning Research (02/2023)
Proof.
1.Let⃗ µ∈Rddenote the coordinates of µand consider the normalizing constant
Cw,σ=/integraldisplay
TwMexpÇ
−∥ν−µ∥2
w
2σ2å
d(ϕ∗λ)(ν)(∗)=/integraldisplay
RdexpÖ
−/vextenddouble/vextenddouble/vextenddouble/summationtextd
i=1ciβi−/summationtextd
i=1⃗ µiβi/vextenddouble/vextenddouble/vextenddouble2
w
2σ2è
dλ(c)
=/integraldisplay
Rdexp 
−/summationtextd
i=1/summationtextd
j=1⟨(ci−⃗ µi)βi,(cj−⃗ µj)βj⟩w
2σ2!
dλ(c)(∗∗)=/integraldisplay
RdexpÇ
−/summationtextd
i=1(ci−⃗ µi)2
2σ2å
dλ(c)
(†)= (2πσ2)d/2, (4)
where we use the change of variable rule (Theorem 9) under transformation ϕin(∗), that (β1,...,βd)
is orthonormal tangent vectors in (∗∗), and that/integraltext
Rdexp(−/summationtextd
i=1(ci−⃗ µi)2
2σ2 )dλis the normalizing constant of
N(⃗ µi,σ2.I)in(†).
2.
Now, letξ∼Nw(µ,σ2), we show that ⃗ξ∼N(⃗ µ,σ2Id).LetA⊆Rdbe a measurable set, then consider
Pr[⃗ξ∈A] =Pr[ξ∈ϕB(A)] =/integraldisplay
ϕB(A)1
(2πσ2)d/2expÇ
−∥ν−µ∥2
w
2σ2å
d(ϕ∗λ)(ν)
=/integraldisplay
A1
(2πσ2)d/2expÇ
−/summationtextd
i=1(ci−⃗ µi)2
2σ2å
dλ(c).
The last equality is obtained similarly as in (4). Since the last expression is exactly probability that a random
vector distributed as N(⃗ µ,σ2Id)belongs to set A, we are done. The converse is shown in a similar way.
3. This simply follows Statement 2of Lemma 1 and the variance bound from the standard Gaussian
distribution.
B.2 Proof of Claim 2
Proof.Given that LI“w→wis a linear isometric mapping, one can show that it is invertible and its inverse is
again isometry, which we will denote by LIw→“w.Ifϕ∗λis Lebesuge measure on Tw1Mthen LIw1→w
∗(ϕ∗λ)
is the Lebesgue measure on TwM. This can be seen by observation that, if B={β1,...,βd}is orthonormal
basis forT“wMthen{LI“w→wβ1,...,LI“w→wβd}is orthonormal basis for TwM. Letξ∼N“w(µ,σ2), we will
show that LI“w→wξ∼Nw(LI“w→wµ,σ2). consider measurable set S⊆TwM
Prî
LI“w→w(ξ1)∈Só
=Prî
ξ1∈LIw→“w(S)ó
=/integraldisplay
LIw→cw(S)1
(2πσ2)d/2expÇ
−∥ν−µ∥2
“w
2σ2å
d(ϕ∗λ)(ν)
(∗)=/integraldisplay
S1
(2πσ2)d/2expÖ
−/vextenddouble/vextenddouble/vextenddoubleLIw→“w(ν)−µ)/vextenddouble/vextenddouble/vextenddouble2
“w
2σ2è
dÄ
LI“w→w
∗(ϕ∗λ)ä
(ν)
(∗∗)=/integraldisplay
S1
(2πσ2)d/2expÖ
−/vextenddouble/vextenddouble/vextenddoubleν−LI“w→w(µ)/vextenddouble/vextenddouble/vextenddouble2
w
2σ2è
dÄ
LI“w→w
∗(ϕ∗λ)ä
(ν),
where we used change of variables formula Theorem 9 (with X=LIw→“w(S),Y=Sandϕ=LI“w→w)
and that LI is isometry in (∗∗). Since LI“w→w
∗(ϕ∗λ)is the Lebesuge measure on TwM, we have that
LI“w→wξ∼Nw(LI“w→wµ,σ2).
25Published in Transactions on Machine Learning Research (02/2023)
B.3 Proofs of Section 5
B.3.1 Proof of Claim 3
Proof.LetQs+1denote the full gradient query given by Qs+1(Z) =1
n/summationtextn
i=1gradf(ews;zi).LetZ,Z′∈Zn
denote adjacent datasets, consider the sensitivity, denoted as ∆s,
∆s+1= sup
Z∼Z′∥Qs+1(Z)−Qs+1(Z′)∥≤1
n[∥gradf(ews;zn)∥‹ws+∥gradf(ews;z′
n)∥‹ws]≤2C0
n.(5)
Following (Han et al., 2022a, Lemma 2), the moments bound of the full gradient mechanism Rsis given by
KRs(λ)≤λ(λ+ 1)
2σ2
1(∆s)2(5
)≤2λ(λ+ 1)C2
0
n2σ2
1.
LetQs+1
tdenote the variance reduced stochastic gradient query given by Qs+1
t(Z) = gradf(ws+1
t;z)−
PT‹ws→ws+1
t(gradf(ews;z)−gs+1).LetZ,Z′∈Zdenote adjacent datasets, consider its sensitivity, denoted
at∆s
t+1,
∆s+1
t
= sup
Z∼Z′/vextenddouble/vextenddoubleQs+1
t2(Z)−Qs+1
t2(Z′)/vextenddouble/vextenddouble
ws+1
t
(∗)
≤sup
Z∼Z′ï/vextenddouble/vextenddoublegradf(ws+1
t;z)−gradf(ws+1
t;z′)/vextenddouble/vextenddouble
ws+1
t+/vextenddouble/vextenddouble/vextenddoublePT‹ws→ws+1
t(gradf(ews;z)−gradf(ews;z′))/vextenddouble/vextenddouble/vextenddouble
ws+1
tò
(†)= sup
Z∼Z′h/vextenddouble/vextenddoublegradf(ws+1
t;z)−gradf(ws+1
t;z′)/vextenddouble/vextenddouble
ws+1
t+∥gradf(ews;z)−gradf(ews;z′)∥‹wsi
≤sup
Z∼Z′h/vextenddouble/vextenddoublegradf(ws+1
t;z)/vextenddouble/vextenddouble
ws+1
t+/vextenddouble/vextenddoublegradf(ws+1
t;z′)/vextenddouble/vextenddouble
ws+1
t+∥gradf(ews;z)∥‹ws+∥gradf(ews;z′)∥‹wsi
(‡)
≤4C1, (6)
where we used linearity of parallel transport and triangle’s inequality in (∗)and that parallel transport is
isometric in (†)and triangle inequality and assumption of lipschitz in (‡).Now moments bound of Rs+1
tis
given by,
KRs+1
t(λ)≤λ(λ+ 1)
2σ2
2(∆s+1
t)2(6)
≤8λ(λ+ 1)C2
1
σ2
2. (7)
B.3.2 Proof of Claim 4
Proof.By using (Wang et al., 2019b, Lemma 3.7) and by choice of parameters σ2,λwe have
KsubRs+1
t(λ)≤3.5
n2KRs+1
t(λ)(7)
≤28λ(λ+ 1)C2
1
σ2n2.
B.3.3 Proof of Claim 5
Proof.ForRcan be show (ϵ,δ)-differentially private by solving for ϵandδas follows, i.e.,
min
α∈(0,1)mSλ (λ+ 1)C2
n2σ2ï2
α+28
1−αò
=mSλ (λ+ 1)C2
n2σ2ï2
α∗+28
1−α∗ò
≤λϵ
2,expÅ
−λϵ
2ã
≤δ,(8)
26Published in Transactions on Machine Learning Research (02/2023)
whereα∗= (√
14−1)/13and there exists constant c1>0such thatσ2≥c1mSlog(1/δ)C2
n2ϵ2satisfies (8). Hence,
Algorithm 11 satisfies (ϵ,δ)-DP. For Algorithm 12 using similar arguments there exists constant c2>0such
thatσ2≥c2mSK log(1/δ)C2
n2ϵ2guarantees (ϵ,δ)-DP .
B.4 Proofs of Section 5.2
Lemma 10 (Trigonometric distance bound (Zhang & Sra, 2016)) .Letw0,w1,w2∈W⊆M lie in totally
normal neighborhood of Riemannian manifold with curvature lower bounded by κminandℓ0=dist(w0,w1)
andℓ1=dist(w1,w2)andℓ2=dist(w0,w2).Denoteθas the angle on Tw0Msuch that cos(θ) =
1
ℓ0ℓ1⟨Exp−1
w0(w1),Exp−1
w0(w2)⟩w0.LetDWbe the diameter of Wi.e.,DW:= maxw,w′dist(w,w′).Define curva-
ture constant ζ=√κmin
tanh√κminifκmin<0andζ= 1ifκmin≥0.Then, we have that ℓ2
1≤ζℓ2
0+ℓ2
2−2ℓ0ℓ2cosθ.
Lemma 11.
Eit,ϵt∥vs+1
t∥2
ws+1
t≤Eit∥gradf(ws+1
t;zit)−PT‹ws→ws+1
t(gradf(ews;zit)−gs+1)∥2
ws+1
t+dσ2.(9)
Proof.
Eit,ϵt∥vs+1
t∥2
ws+1
t=Eit,ϵt∥gradf(ws+1
t;zit)−PT‹ws→ws+1
t(gradf(ews;zit)−gs+1) +ϵt∥2
ws+1
t
=Eit,ϵt∥gradf(ws+1
t;zit)−PT‹ws→ws+1
t(gradf(ews;zit)−gs+1)∥2
ws+1
t+Eϵt∥ϵt∥2
ws+1
t
+⟨Eitgradf(ws+1
t;zit)−PT‹ws→ws+1
t(gradf(ews;zit)−gs+1),Eϵt[ϵt]⟩ws+1
t
≤Eit∥gradf(ws+1
t;zit)−PT‹ws→ws+1
t(gradf(ews;zit)−gs+1)∥2
ws+1
t+dσ2,
where we used that Eϵt[ϵt] = 0andEϵt∥ϵt∥2
ws+1
t≤dσ2in last inequality.
B.4.1 Proof of Theorem 6
Proof.We bound first term Eit∥gradf(ws+1
t;zit)−PT‹ws→ws+1
t(gradf(ews;zit)−gs+1)∥2
ws+1
tas in (Zhang
et al., 2016)
Eit/vextenddouble/vextenddouble/vextenddoublegradf(ws+1
t;zit)−PT‹ws→ws+1
t(gradf(ews;zit)−gs+1)/vextenddouble/vextenddouble/vextenddouble2
ws+1
t
≤Eit/vextenddouble/vextenddouble/vextenddoublegradf(ws+1
t;zit)−PT‹ws→ws+1
tgradf(ews;zit) +PT‹ws→ws+1
tÄ
gradF(ews)−PT‹w∗→‹wsgradF(w∗)ä/vextenddouble/vextenddouble/vextenddouble2
ws+1
t
≤2Eit/vextenddouble/vextenddouble/vextenddoublegradf(ws+1
t;zit)−PT‹ws→ws+1
tgradf(ews;zit)/vextenddouble/vextenddouble/vextenddouble2
ws+1
t
+ 2Eit/vextenddouble/vextenddouble/vextenddoublePT‹ws→ws+1
tÄ
gradF(ews)−PT‹w∗→‹wsgradF(w∗)ä/vextenddouble/vextenddouble/vextenddouble2
ws+1
t
= 2Eit/vextenddouble/vextenddouble/vextenddoublegradf(ws+1
t;zit)−PT‹ws→ws+1
tgradf(ews;zit)/vextenddouble/vextenddouble/vextenddouble2
ws+1
t+ 2Eit/vextenddouble/vextenddouble/vextenddoublegradF(ews)−PT‹w∗→‹wsgradF(w∗)/vextenddouble/vextenddouble/vextenddouble2
‹ws
≤4L2∥Exp−1
ws+1
t(w∗)∥2
ws+1
t+ 6L2/vextenddouble/vextenddoubleExp−1
‹wsw∗/vextenddouble/vextenddouble2
‹ws
= 4L2dist2(ws+1
t,w∗) + 6L2dist2(ews,w∗). (10)
Using the trigonometric distance bound in Lemma 10 with w0=xs+1
t,w1=ws+1
t+1,w2=w∗,
dist2(ws+1
t+1,w∗)≤ζdist2(ws+1
t+1,ws+1
t) +dist2(ws+1
t,w∗)−2⟨Exp−1
xs+1
t(ws+1
t+1),Exp−1
ws+1
t(w∗)⟩ws+1
t
=ζ/vextenddouble/vextenddouble/vextenddoubleExp−1
ws+1
tws+1
t+1/vextenddouble/vextenddouble/vextenddouble2
ws+1
t+dist2(ws+1
t,w∗)−2⟨−ηvs+1
t,Exp−1
ws+1
t(w∗)⟩ws+1
t
=ζη2/vextenddouble/vextenddoublevs+1
t/vextenddouble/vextenddouble2
ws+1
t+dist2(ws+1
t,w∗) + 2η⟨vs+1
t,Exp−1
ws+1
t(w∗)⟩ws+1
t.
27Published in Transactions on Machine Learning Research (02/2023)
Applying expectation we have
dist2(ws+1
t+1,w∗)
≤ζη2Eit,ϵt/vextenddouble/vextenddoublevs+1
t/vextenddouble/vextenddouble2
ws+1
t+dist2(ws+1
t,w∗) + 2η⟨Eit,ϵtvs+1
t,Exp−1
ws+1
t(w∗)⟩ws+1
t
=ζη2L24dist2(ws+1
t,w∗) + 6dist2(ews,w∗)+ 2η⟨gradF(ws+1
t),Exp−1
ws+1
t(w∗)⟩ws+1
t+dζη2σ2
≤ζη2L24dist2(ws+1
t,w∗) + 6dist2(ews,w∗)+ 2η[F(w∗)−F(ws+1
t)−µ
2dist2(ws+1
t,w∗)] +dζη2σ2
≤(1 + 4ζη2L2−ηµ)dist2(ws+1
t,w∗) + 6ζη2L2dist2(ews,w∗) +dζη2σ2.
Definingut=dist2(ws+1
t+1,w∗),q= (1 + 4ζη2L2−ηµ),p= 6ζη2L2,c=dζη2σ2we have following recurrence
ut+1−pu0≤q(ut−pu0) +cfrom which we have that um≤(p+qm(1−p))u0+/summationtextm−1
i=1qic.Now choosing
η=µ
17ζL2andm≥10ζL2
µ2. we getq= 1−µ2
10ζL2andp= 1/5. Note that 0<µ2
10ζL2<1(L>µ,ζ≥1) and
hence 0<q< 1and from which we have that (p+qm(1−p)) = 1/2.
E[d2(ws+1
m,w∗)]≤E[dist2(ws
m,w∗)] +dζµ2σ2
289ζ2L4m−1/summationdisplay
i=1Å
1−µ2
10ζL2ãi
≤E[dist2(ws
m,w∗)] +dζµ2σ2
289ζ2L4∞/summationdisplay
i=1Å
1−µ2
10ζL2ãi
=E[dist2(ws
m,w∗)] +dζµ2σ2
289ζ2L410ζL2
µ2=E[dist2(ws
m,w∗)] +d10σ2
289L2,
from which we have
E[dist2(wS
m,w∗)] = 2−SE[dist2(w0
m,w∗)] +d10σ2
289L2S/summationdisplay
i=01
2i≤2−SE[dist2(w0
m,w∗)] + 2dc−110
289L2mSlog(1/δ)L2
0
n2ϵ2
≤2−SE[dist2(w0
m,w∗)] +d200ζ
289µ2Slog(1/δ)L2
0
n2ϵ2.
E[f(xa)−f(w∗)]≤1
2ELdist2(xa,w∗)≤2−SLE[dist2(w0,w∗)] +Ldζ
µ2Slog(1/δ)L2
0
n2ϵ2.
Now, setting 2−S=dζ
µ2log(1/δ)L2
0
n2ϵ2E[dist2(w0,w∗)]=⇒ 2S=n2ϵ2289µ2E[dist2(w0,w∗)]
d100ζlog(1/δ)L2
0=⇒S=
O
log
nϵµE[dist2(w0,w∗)]
log (1/δ)ζL0d
, substituting this we have that, and now for S=O
log
nϵµE[dist2(w0,w∗)
log (1/δ)ζL2
0d
E[f(xa)−f(w∗)]≤OÇ
dζLL2
0log(1/δ)E[dist2(w0,w∗)]
µ2n2ϵ2logÅnϵµ
ζL2
0dlog(1/δ)ãå
. (11)
Gradient complexity: S×nplusm×2 IFO calls = 2nS+ 2mS,
OÇÅ
n+ζL2
µ2ã
logÇ
nϵµE[dist2(w0,w∗)]
log (1/δ)ζL0dåå
. (12)
This completes the proof.
B.4.2 Proof of Theorem 7
Before proving Theorem 7, we state and prove following lemma that we will be using later.
28Published in Transactions on Machine Learning Research (02/2023)
Lemma12. Assume that each fiisL-g-smooth, the sectional curvature in Xis lower bounded by κminand we
run Algorithm 11 with Option II. For ct,ct+1,β,η> 0and suppose we have ct=ct+1(1+βη+2ζL2η2)+L3η2
andδ(t) =η−ct+1η
β−Lη2−2ct+1ζη2>0, then the iterate ws+1
tsatisfies the bound
E∥gradf(ws+1
t)∥2≤Rs+1
t−Rs+1
t+1
δt+ 1
2dLη2+ct+1ζdη2
δtσ2,
whereRs+1
t:=E[F(ws+1
t) +ct/vextenddouble/vextenddoubleExp‹wsws+1
t/vextenddouble/vextenddouble]for0≤s≤S−1.
Proof.The proof is adapted from (Zhang et al., 2016, Lemma 2). Denoting ∆s+1
t= gradf(ws+1
t;zit)−
PT‹ws→ws+1
tgradf(ews;zit)it can be seen that Eit|exs,ws+1
t[∆s+1
t] = gradF(ws+1
t)−PT‹ws→ws+1
tgradF(ews)
Eit,ϵt/vextenddouble/vextenddoublevs+1
t/vextenddouble/vextenddouble2
ws+1
t(9)
≤Eit/vextenddouble/vextenddouble/vextenddoublegradf(ws+1
t;zit)−PT‹ws→ws+1
t(gradf(ews;zit)−gs+1)/vextenddouble/vextenddouble/vextenddouble2
ws+1
t+dσ2
=Eit/vextenddouble/vextenddouble∆s+1
t−Eit∆s+1
t+ gradF(ws+1
t)/vextenddouble/vextenddouble2
ws+1
t+dσ2
(∗)
≤2Eit/vextenddouble/vextenddouble∆s+1
t−Eit∆s+1
t/vextenddouble/vextenddouble2+ 2/vextenddouble/vextenddoublegradF(ws+1
t)/vextenddouble/vextenddouble2
ws+1
t+dσ2
(∗∗)
≤2Eit/vextenddouble/vextenddouble∆s+1
t/vextenddouble/vextenddouble2
ws+1
t+ 2/vextenddouble/vextenddoublegradF(ws+1
t)/vextenddouble/vextenddouble2
ws+1
t+dσ2
(†)
≤2L2/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t)/vextenddouble/vextenddouble2
‹ws+ 2/vextenddouble/vextenddoublegradF(ws+1
t)/vextenddouble/vextenddouble2
ws+1
t+dσ2,
where∥a+b∥2≤2∥a∥2+ 2∥b∥2in(∗)andEit/vextenddouble/vextenddouble∆s+1
t−Eit∆s+1
t/vextenddouble/vextenddouble2=Eit/vextenddouble/vextenddouble∆s+1
t/vextenddouble/vextenddouble2−/vextenddouble/vextenddoubleE∆s+1
t/vextenddouble/vextenddouble2≤
Eit/vextenddouble/vextenddouble∆s+1
t/vextenddouble/vextenddouble2in(∗∗)and assumption that fiisL-g-smooth in (†).Taking full expectation we have
E/vextenddouble/vextenddoublevs+1
t/vextenddouble/vextenddouble2
ws+1
t≤2L2/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t)/vextenddouble/vextenddouble2
‹ws+ 2/vextenddouble/vextenddoublegradF(ws+1
t)/vextenddouble/vextenddouble2
ws+1
t+dσ2. (13)
For bounding the Lyapunov function Rs+1
t+1:=Eî
F(ws+1
t+1) +ct+1/vextenddouble/vextenddoubleExp‹ws(ws+1
t+1)/vextenddouble/vextenddouble2ó
, we need to bound on
E[F(ws+1
t+1)],E[/vextenddouble/vextenddoubleExp‹ws(ws+1
t+1)/vextenddouble/vextenddouble2], First consider
EF(ws+1
t+1)
(∗)
≤Eï
F(ws+1
t) +D
gradF(ws+1
t),Exp−1
ws+1
t(ws+1
t+1)E
ws+1
t+L
2/vextenddouble/vextenddouble/vextenddoubleExp−1
ws+1
t(ws+1
t+1)/vextenddouble/vextenddouble/vextenddouble2
ws+1
tò
(∗∗)
≤Eï
F(ws+1
t)−η/vextenddouble/vextenddoublegradF(ws+1
t)/vextenddouble/vextenddouble2
ws+1
t+Lη2
2/vextenddouble/vextenddoublevs+1
t/vextenddouble/vextenddouble2
ws+1
tò
(13)
≤Eï
F(ws+1
t)−η/vextenddouble/vextenddoublegradF(ws+1
t)/vextenddouble/vextenddouble2
ws+1
t+Lη2
2 2L2∥Exp−1
‹ws(ws+1
t)∥2+ 2∥gradF(ws+1
t)∥2+σ2dò
= (Lη2−η)∥gradF(ws+1
t)∥2+F(ws+1
t) +L3η2∥Exp−1
‹ws(ws+1
t)∥2+1
2dLη2σ2, (14)
where we used the assumption that fiisL-g-smooth implies that FisL-g-smooth in (∗)and Exp−1
ws+1
t=vs+1
t
andEvs+1
t= gradF(ws+1
t)in(∗∗). Using the trigonometric distance bound on ws+1
t,ws+1
t+1,ewswe have,
/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t+1)/vextenddouble/vextenddouble2
‹ws≤/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t)/vextenddouble/vextenddouble2
‹ws+ζ/vextenddouble/vextenddouble/vextenddoubleExp−1
ws+1
t(ws+1
t+1)/vextenddouble/vextenddouble/vextenddouble2
ws+1
t−D
Exp−1
ws+1
t(ws+1
t+1),Exp−1
ws+1
t(ews)E
ws+1
t
=/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t)/vextenddouble/vextenddouble2+ζη2/vextenddouble/vextenddoublevs+1
t/vextenddouble/vextenddouble2+ 2η⟨gradF(ws+1
t),Exp−1
ws+1
t(ews)⟩.
29Published in Transactions on Machine Learning Research (02/2023)
Taking the expectation we have
E/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t+1)/vextenddouble/vextenddouble2
‹ws
≤Eh/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t)/vextenddouble/vextenddouble2+ζη2∥vs+1
t∥2+ 2η⟨gradF(ws+1
t),Exp−1
ws+1
t(ews)⟩i
≤Eï/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t)/vextenddouble/vextenddouble2+ζη2/vextenddouble/vextenddoublevs+1
t/vextenddouble/vextenddouble2+ 2ηï1
2β/vextenddouble/vextenddoublegradf(ws+1
t)/vextenddouble/vextenddouble2+β
2/vextenddouble/vextenddouble/vextenddoubleExp−1
ws+1
t(ews)/vextenddouble/vextenddouble/vextenddouble2òò
≤Eî
(1 +βη)/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t)/vextenddouble/vextenddouble2+ζη2î
2L2/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t)/vextenddouble/vextenddouble2+ 2/vextenddouble/vextenddoublegradF(ws+1
t)/vextenddouble/vextenddouble2+σ2dóó
+Eïη
β/vextenddouble/vextenddoublegradf(ws+1
t)/vextenddouble/vextenddouble2ò
= 1 + 2ζη2L2+ηβ/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t)/vextenddouble/vextenddouble2+Å
2ζη2+η
βã/vextenddouble/vextenddoublegradF(ws+1
t)/vextenddouble/vextenddouble2+ζdη2σ2. (15)
Putting (14) and (15) into Rs+1
t+1, we have
Rs+1
t+1:=E[f(ws+1
t+1) +ct+1/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t+1)/vextenddouble/vextenddouble2]
=ct+1 1 + 2ζη2L2+ηβ/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t)/vextenddouble/vextenddouble2+ct+1Å
2ζη2+η
βã/vextenddouble/vextenddoublegradF(ws+1
t)/vextenddouble/vextenddouble2+ct+1ζdη2σ2
+ (Lη2−η)/vextenddouble/vextenddoublegradF(ws+1
t)/vextenddouble/vextenddouble2+F(ws+1
t) +L3η2/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t)/vextenddouble/vextenddouble2+1
2dLη2σ2
=F(ws+1
t) + (ct+1 1 + 2ζη2L2+ηβ+L3η2)/vextenddouble/vextenddoubleExp−1
‹ws(ws+1
t)/vextenddouble/vextenddouble2
+Å
Lη2−η+ct+1Å
2ζη2+η
βãã/vextenddouble/vextenddoublegradF(ws+1
t)/vextenddouble/vextenddouble2+Å1
2dLη2+ct+1ζdη2ã
σ2
=Rs+1
t−Å
−Lη2+η−ct+1Å
2ζη2+η
βãã
∥gradF(ws+1
t)∥2+Å1
2dLη2+ct+1ζdη2ã
σ2.
Rearranging, we get
Å
η−Lη2−ct+1Å
2ζη2+η
βãã
E∥gradF(ws+1
t)∥2≤Rs+1
t−Rs+1
t+1+Å1
2dLη2+ct+1ζdη2ã
σ2
from which we have
E∥gradF(ws+1
t)∥2≤Rs+1
t−Rs+1
t+1Ä
η−Lη2−ct+1Ä
2ζη2+η
βää+ 1
2L+ct+1ζdη2
Ä
Lη2−η−ct+1Ä
2ζη2+η
βääσ2.
We now give the proof of Theorem 7.
30Published in Transactions on Machine Learning Research (02/2023)
Proof.The proof is adapted from (Zhang et al., 2016, Theorems 2, 6 and Corollary 6). Let δn= mintδtand
T=mS
m−1/summationdisplay
t=0E/vextenddouble/vextenddoublegradf(ws+1
t)/vextenddouble/vextenddouble2
≤m−1/summationdisplay
t=0Rs+1
t−Rs+1
t+1
δt+ 1
2L+ct+1ζdη2
δtσ2
(∗)
≤Rs+1
0−Rs+1
m
δn+ 1
2L+ct+1ζmdη2
δnσ2
=Eî
F(ws+1
0)−F(ws+1
m) +c0/vextenddouble/vextenddoubleExpwes(ws+1
0)/vextenddouble/vextenddouble2−cm/vextenddouble/vextenddoubleExpwes(ws+1
m)/vextenddouble/vextenddouble2ó
δn+ 1
2L+c0ζmdη2
δnσ2
(∗∗)
≤EF(ews)−F(ews+1)
δn+ 1
2L+c0ζmdη2
δnσ2,
whereδt≥δn,ct≤c0is used in (∗)and thatws+1
0=ews,ws+1
m=ews+1and thatcm= 0,c0≥0in(∗∗).
Now, summing the gradient norm square over all the epochs and using F(w∗)≤F(ewm), we get
1
TS−1/summationdisplay
s=0m−1/summationdisplay
t=0E/vextenddouble/vextenddoublegradf(ws+1
t)/vextenddouble/vextenddouble2≤EF(ew0)−F(w∗)
Tδn+ 1
2L+c0ζdη2
δnσ2.
Choosingβ=Lζ1−α2/nα1/2and solving recurrence relation ctusingη,mgiven by theorem as (Zhang et al.,
2016, Theorem 2) one can get c0=µ0L
nα1/2ζ(e−1). Substituting that in δn≥ν
Lnα1ζα2and finally using this
we have
1
TS−1/summationdisplay
s=0m−1/summationdisplay
t=0E/vextenddouble/vextenddoublegradf(ws+1
t)/vextenddouble/vextenddouble2
≤cµ0Lnα1ζα2
νnSEF(ew0)−F(w∗)+Lnα1ζα2Ä
1
2L+µ0L
nα1/2ζ(e−1)ζäµ2
0
L2n2α1ζ2α2
νdσ2.
Finally, puttingthevaluesof α1= 2/3,α2= 1/2µ0= 1/10,ν= 1/2, andσ2=c2mSlog(1/δ)L2
0
n2ϵ2 =c3Slog(1/δ)L2
0
nϵ2
one can get that
E∥gradf(wa)∥2≤c4Ç
Lζ1/2
n1/3SEF(ew0)−F(w∗)+ï1
n2/3ζ1/2+1
nζ1/2òdSlog(1/δ)L2
0
nϵ2å
≤c4Ç
Lζ1/2
n1/3SEF(ew0)−F(w∗)+dSlog(1/δ)L2
0
n5/3ζ1/2ϵ2å
.
SettingS=q
LζE[F(‹w0)−F(w∗)]
dlog(1/δ)n2/3ϵ
L0, we have
E∥gradf(wa)∥2≤c4L0p
dLlog(1/δ)E[F(ew0)−F(w∗)]
nϵ. (16)
The gradient complexity is given by
S(n+ 2m) = 
LζE[F(ew0)−F(w∗)]
dlog(1/δ)n2/3ϵ
L0
n+n
30
= 
LζE[F(ew0)−F(w∗)]
dlog(1/δ)n5/3ϵ
L0.(17)
This completes the proof.
31Published in Transactions on Machine Learning Research (02/2023)
B.4.3 Proof of Theorem 8
Proof.With the values given in the theorem statement, σ2=mSK log(1/δ)L2
0
n2ϵ2 =
Kn⌈6+18
n−3⌉Lτζ1/2µ0
νn1/3log(1/δ)L2
0
3µ0n2ϵ2 =K⌈6+18
n−3⌉Lτζ1/2log(1/δ)L2
0
νn1/3
3nϵ2 . This implies that
E[/vextenddouble/vextenddoublegradf(wk+1)/vextenddouble/vextenddouble2]≤1
2τEF(ew0)−F(w∗)+ï1
n2/3ζ1/2+1
nζ1/2òdK⌈6 +18
n−3⌉Lτζ1/2log(1/δ)L2
0
νn1/3
3nϵ2
≤1
2τEF(ew0)−F(w∗)+24dKLτ log(1/δ)L2
0
3n2ϵ2.
Using the Riemannian PL condition, we have
Ef(wk+1)−f(w∗)≤τE[/vextenddouble/vextenddoublegradf(wk+1)/vextenddouble/vextenddouble2]≤1
2EF(wk)−F(w∗)+24dKLτ2log(1/δ)L2
0
3n2ϵ2.
Recursively applying the above for k= 0toK−1, we have
Ef(wK)−f(w∗)≤1
2KEF(w0)−F(w∗)+8dKLτ2log(1/δ)L2
0
n2ϵ2K−1/summationdisplay
i=01
2i
≤1
2KEF(w0)−F(w∗)+8dKLτ2log(1/δ)L2
0
n2ϵ2∞/summationdisplay
i=01
2i
=1
2KEF(w0)−F(w∗)+16dKLτ2log(1/δ)L2
0
n2ϵ2.
PuttingK= logÅ
n2ϵ2E[F(w0)−F(w∗)]
dLτ2log(1/δ)L2
0ã
there is a constant csuch that
Ef(wK)−f(w∗)≤cdLτ2log(1/δ)L2
0
n2ϵ2logÇ
n2ϵ2EF(w0)−F(w∗)
dLτ2log(1/δ)L2
0å
.
Ignoring the log factors,
Ef(wK)−f(w∗)=OÅdLτ2log(1/δ)L2
0
n2ϵ2ã
. (18)
Finally, the gradient complexity is given by,
KS(n+ 2m) = logÇ
n2ϵ2EF(w0)−F(w∗)
dLτ2log(1/δ)L2
0åÅ
⌈6 +18
n−3⌉Lτζ1/2µ0
νn1/3ãÅ
n+⌊n
3µ⌋ã
≤Lτζ1/2n2/3logÇ
n2ϵ2EF(w0)−F(w∗)
dLτ2log(1/δ)L2
0å
. (19)
C Additional experiments and more experimental details for Section 6
Details on the parameter configurations of DP-RSVRG, DP-RSGD, and DP-RGD. For DP-
RGD, we tune the clipping parameters from the set C={1,0.1,0.01}and the number of epochs from
{10,20,30}. For DP-RSGD, clipping parameter is chosen from C={1,0.1,0.01}and number of epochs
from{n,n∗5,n∗10,n∗20,n∗30}. For DP-RSVRG number of epochs is chosen from {5,10}and set the
frequency as m=n/10and full gradient clipping parameter Cis tuned from{1,0.1}and variance reduced
gradient clipping parameter C2from{1,0.1,0.01}. For all three algorithms, we tune the learning rate from
η={5e−5,1e−55e−4,1e−4,..., 5e−1,1e−1,1,2,..., 5}. In all our experiments, we use geomstats (Miolane
et al., 2020; 2021; Myers et al., 2022)
32Published in Transactions on Machine Learning Research (02/2023)
C.1 Details on the the Fréchet mean of SPD matrices computation and the covariance descriptors
The Riemannian distance induced by the metric is given by dist (Z1,Z2) =∥Logm (Z−1/2
2Z1Z−1/2
2)∥F,
where Logm denotes matrix logarithm. Given points {Z1,...,Zn} ∈ SPD(m),
the Fréchet mean is defined as the solution to following optimization problem:
minW∈SPD(m)¶
F(W) =1
n/summationtextn
i=1f(W;Zi) =1
n/summationtextn
i=1∥logm (W−1/2ZiW−1/2)∥2
F©
.Riemannian gra-
dient offis given in terms inverse Exponential map gradf(W,Xi) =−2Exp−1
W(Xi) =
−2W1/2Logm (W−1/2XiW−1/2)W1/2. We take first two classes from PATHMNIST (Kather et al.,
2019) (ADI, adipose tissue; BACK, background).
Covariance descriptors. LetI ∈Rh×w×3denote a RGB image with height hand width w. Letϕ:
Rh×w×3→Rhw×kbe a feature extractor of dimension k, i.e.ϕ(I)(x)is ak-dimensional vector at each spatial
coordinate xintheimage’sdomain S. Givenasmall η>0, thecovariancedescriptor Rη:Rh×w×3→SPD(k)
associated with ϕis defined as
Rη(I) ="
1
|S|/summationdisplay
x∈S(ϕ(I)(x)−µ)(ϕ(I)(x)−µ)T#
+η.I,
whereµ=|S|−1/summationtext
x∈Sϕ(I)(x), andη.IensuresRη(I)∈SPD(k). Our experiments on the private Fréchet
mean computation problem (Section 6.3) use the covariance descriptors with following feature vector:
ϕ(I)(x) =ï
x,y,I,|Ix|,|Iy|,|Ixx|,|Iyy|,»
|Ix|2+|Iy|2,arctanÅ|I|x
|I|yãò
,
wherex= (x,y),intensitiesderivativesaredenotedby Ix,Iy,Ixx,Iyyandη= 10−6.Let⋆denoteconvolution
operation, then first and second order intensity derivatives are computed as below,
Ix=I⋆1
4
+1 0−1
+2 0−2
+6 0−12
,Ix=I⋆1
4
+1 0−1
+2 0−2
+6 0−12
,
Ixx=I⋆1
32
+1 0−2 0 1
+4 0−8 0 4
+6 0−12 0 6
+4 0−8 0 4
+1 0−2 0 1
,Iyy=I⋆1
32
+1 +4 +6 +4 +1
0 0 0 0 0
−2−8−12−8−2
0 0 0 0 0
+1 +4 +6 +4 +1
.
For RGB images, ϕ(I)(x)is a11-dimensional vector that makes Rη(I)a11×11SPD matrix.
C.2 Details on the private leading eigenvector computation problem
The problem of computing the leading eigenvector of sample covariance matrix is
minw∈SmF(w) =1
n/summationtextn
i=1f(w;zi) =−1
n/summationtextn
i=1wT(zizT
i)w	.It has been shown that above problem
satisfies Riemannian PL condition (Zhang et al., 2016) while the problem is nonconvex in the Euclidean
setting. Riemannian gradient of fis given by gradf(w;zi) =−2(Id+1−wwT)zizT
iw.
33