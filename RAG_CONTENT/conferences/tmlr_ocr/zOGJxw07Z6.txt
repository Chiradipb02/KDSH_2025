Published in Transactions on Machine Learning Research (MM/YYYY)
Asynchronous Training Schemes in Distributed Learning with
Time Delay
Haoxiang Wang whx22@mails.tsinghua.edu.cn
Department of Automation
Tsinghua University
Zhanhong Jiang zhjiang@iastate.edu
Translational AI Center
Iowa State University
Chao Liu cliu5@tsinghua.edu.cn
Department of Energy and Power Engineering
Tsinghua University
Soumik Sarkar soumiks@iastate.edu
Department of Mechanical Engineering
Iowa State University
Dongxiang Jiang jiangdx@tsinghua.edu.cn
Department of Energy and Power Engineering
Tsinghua University
Young M. Lee young.m.lee@jci.com
Johnson Controls
Reviewed on OpenReview: https: // openreview. net/ forum? id= zOGJxw07Z6
Abstract
In the context of distributed deep learning, the issue of stale weights or gradients could
result in poor algorithmic performance. This issue is usually tackled by delay tolerant algo-
rithms with some mild assumptions on the objective functions and step sizes. In this paper,
we propose a diﬀerent approach to develop a new algorithm, called Predicting Clipping
Asynchronous Stochastic Gradient Descent (aka, PC-ASGD). Speciﬁcally, PC-ASGD has
two steps - the predicting step leverages the gradient prediction using Taylor expansion to
reduce the staleness of the outdated weights while the clipping step selectively drops the
outdated weights to alleviate their negative eﬀects. A tradeoﬀ parameter is introduced to
balance the eﬀects between these two steps. Theoretically, we present the convergence rate
considering the eﬀects of delay of the proposed algorithm with constant step size when the
smooth objective functions are weakly strongly-convex, general convex, and nonconvex. One
practical variant of PC-ASGD is also proposed by adopting a condition to help with the
determination of the tradeoﬀ parameter. For empirical validation, we demonstrate the per-
formance of the algorithm with four deep neural network architectures on three benchmark
datasets.
1 Introduction
The availability of large datasets and powerful computing led to the emergence of deep learning that is
revolutionizing many application sectors from the internet industry and healthcare to transportation and
energy Gijzen (2013);Wiedemann et al. (2019);Gao et al. (2022);Liu & Liu (2023). As the applications are
1Published in Transactions on Machine Learning Research (MM/YYYY)
scaling up, the learning process of large deep learning models is looking to leverage emerging resources such
as edge computing and distributed data centers privacy preserving. In this regard, distributed deep learning
algorithms are being explored by the community that leverage synchronous and asynchronous computations
with multiple computing agents that exchange information over communication networks Lian et al. (2017);
Cao et al. (2023);Qian et al. (2022). We consider an example setting involving an industrial IoT framework
where the data is geographically distributed as well as the computing resources. While the computing
resources within a local cluster can operate in a (loosely) synchronous manner, multiple (geographically
distributed) clusters may need to operate in an asynchronous manner. Furthermore, communications among
the computing resources may not be reliable and prone to delay and loss of information.
The master-slave and peer-to-peer are two categories of distributed learning architectures. On one hand,
Federated Averaging and its variants are considered to be the state-of-the-art for training deep learning
models with data distributed among the edge computing resources such as smart phones and idle computers
Hard et al. (2018);Sattler et al. (2019). PySyft Ryﬀel et al. (2018) and its robust version Deng et al. (2020),
the scalable distributed DNN training algorithms Strom (2015) and more recent distributed SVRG Cen et al.
(2020) and clustered FL Sattler et al. (2021) are examples of the master-slave architecture. On the other
hand, examples of the peer-to-peer architecture include the gossip algorithms Blot et al. (2016);Even et al.
(2020);Li et al. (2021);Tu et al. (2022), and the collaborative learning frameworks Jiang et al. (2017);Liu
et al. (2019).
However, as mentioned earlier, communication delay remains a critical challenge for achieving convergence
in an asynchronous learning setting Chen et al. (2016);Tsianos et al. (2012) and aﬀects the performances
of the frameworks above. Furthermore, the amount of delay could be varying widely due to artifacts of
wireless communication and diﬀerent devices. To eliminate the negative impact of varying delays on the
convergence characteristics of distributed learning algorithms, this work proposes a novel algorithm, called
Predicting Clipping Asynchronous Stochastic Gradient Descent (aka, PC-ASGD). The goal is to solve the
distributed learning problems involving multiple computing or edge devices such as GPUs and CPUs with
varying communication delays among them. Diﬀerent from traditional distributed learning scenarios where
synchronous and asynchronous algorithms are considered separately, we take both into account together in
a networked setting.
Table 1: Comparisons between asynchronous algorithms
Methods f ∇f Delay Ass. Con.Rate D.C. G.C. A.S.
ASGD Dean et al. (2013) Non-convex Lip. Bou. O(1√
T) 7 7 7
DC-ASGD Zheng et al. (2017)Str-con Lip. Bou. O(1
T) 7 3 7
Non-convex Lip. Bou. O(1√
T) 7 3 7
D-ASGD Lian et al. (2017) Non-convex Lip.&Bou. Bou. O(1√
T) 3 7 7
DC-s3dg Rigazzi (2019) Non-convex Lip. Unbou. N/A 3 3 7
AGP Assran & Rabbat (2020) Str-con Lip. Bou. O(1
T+1
Tζ+1
T1−ζ)3 7 7
Praque Luo et al. (2020) Non-convex Lip. Bou. N/A 3 7 7
DSGD-AAU Xiong et al. (2023) Non-convex Lip. Bou. O(1√
T) 3 7 7
DGD-ATC Wu et al. (2023) Str-con Lip. Unbou. O(ρT) 3 7 7
AD-APD Abolfazli et al. (2023) Convex Lip. Bou. O(1
T) 3 7 7
PC-ASGD (This paper)Weakly Str-con Lip. Bou. O(ρT+1
T+1√
T) 3 3 3
Convex Lip. Bou. O(1√
T+1
T1.5) 3 3 3
Non-convex Lip. Bou. O(1√
T) 3 3 3
Con.Rate: convergence rate, Str-con: strongly convex. Lip.& Bou.: Lipschitz continuous and bounded. Delay Ass.: Delay Assumption.
Unbou.: Unbounded. T: Total iterations. D.C.: decentralized computation. G.C.: Gradient Compensation. A.S.: Alternant Step,
ρ∈(0,1)is a positive constant. Note that the convergence rate of PC-ASGD is obtained by using the constant step size. ζ∈(0,1).
Related work . In the early works on distributed learning with master-slave architecture, Asynchronous
Stochastic Gradient Descent (ASGD) algorithm has been proposed Dean et al. (2013), where each local
worker continues its training process right after its gradient is added to the global model. The algorithm
could tolerate the delay in communication. Later works Agarwal & Duchi (2011);Feyzmahdavian et al.
(2015);Recht et al. (2011);Zhuang et al. (2021) extend ASGD to more realistic scenarios and implement the
algorithms with a central server and other parallel workers. Typically, since asynchronous algorithms suﬀer
from stale gradients, researchers have proposed algorithms such as DC-ASGD Zheng et al. (2017), adopting
2Published in Transactions on Machine Learning Research (MM/YYYY)
the concept of delay compensation to reduce the impact of staleness and improve the performance of ASGD.
For the distributed learning with peer-to-peer architecture, Lian et al. (2017) proposes an algorithm termed
AD-PSGD (decentralized ASGD algorithm, aka D-ASGD) that deals with the problem of the stale parameter
exchange, as well as presents theoretical analysis for the algorithm performance under bounded delay. Liang
et al. (2020) also proposes a similar algorithm with slightly diﬀerent assumptions. However, these algorithms
do not provide empirical or theoretical analysis regarding the impact of delay in detail. Additional works such
as using a central agent for control Nair & Gupta (2017), requiring prolonged communication Tsianos & Rab-
bat(2016), utilizing stochastic primal-dual method Lan et al. (2020), and adopting importance sampling Du
et al. (2020), have also been done to address the communication delay in the decentralized setting. More
recently, Rigazzi (2019) proposes the DC-s3gd algorithm to enable large-scale decentralized neural network
training with the consideration of delay. Zakharov (2020),Venigalla et al. (2020),Chen et al. (2019) and
Abbasloo & Chao (2019) also develop algorithms of asynchronous decentralized training for neural networks,
while theoretical guarantee is still missing. Asynchronous version of stochastic gradient push ( AGP )Assran
& Rabbat (2020) is developed to address the asynchronous training in multi-agent framework. The authors
claim that AGP is more robust to failing or stalling agents, than the synchronous ﬁrst-order methods. While
the proposed algorithm is only applicable to the strongly convex objectives. To further advance this area, the
most recent schemes such as Praque Luo et al. (2020) adopting a partial all-reduce communication primitive,
DSGD-AAU Xiong et al. (2023) utilizing an adaptive asynchronous updates, DGD-ATC Wu et al. (2023)
extending the Adapt-then-Combine technique from synchronous algorithms, and AD-APD Abolfazli et al.
(2023) leveraging accelerated primal-dual algorithm, are developed, but most of them are limited to only
(strongly) convex cases. Another line of work based on Federated Learning Dun et al. (2023);Gamboa-
Montero et al. (2023);Miao et al. (2023);Xu et al. (2023);Zhang et al. (2023) has also recently received
considerable attention, while all proposed approaches essentially rely on a center server, which may threat
the privacy of local workers. Diﬀerent from the aforementioned works, in this study, we speciﬁcally present
analysis of the impact of the communication delay on convergence error bounds.
Contributions . The contributions of this work are speciﬁcally as follows:
•Algorithm Design . A novel algorithm, called PC-ASGD for distributed learning is proposed to tackle
the convergence issues due to the varying communication delays. Built upon ASGD, the PC-ASGD
algorithm consists of two steps. While the predicting step leverages the gradient prediction using
Taylor expansion to reduce the staleness of the outdated weights, the clipping step selectively drops
the outdated weights to alleviate their negative eﬀects. To balance the eﬀects, a tradeoﬀ parameter
is introduced to combine these two steps.
•Convergence guarantee .We show that with a proper constant step size, PC-ASGD can converge to
theneighborhood of the optimal solution at a linear rate for weakly strongly-convex functions while
at a sublinear rate for both generally convex and nonconvex functions (speciﬁc comparisons with
other related existing approaches are listed in Table 1).We also model the delay and take it into
consideration in the convergence analysis.
•Veriﬁcation studies .PC-ASGD is deployed on distributed GPUs with three datasets CIFAR-10,
CIFAR-100, and TinyImageNet by using PreResNet110, DenseNet, ResNet20, and EﬃcientNet ar-
chitectures. Our proposed algorithm outperforms the existing delay tolerant algorithms as well as
the variants of the proposed algorithm using only the predicting step or the clipping step.
2 Formulation and Preliminaries
Consider Nagents in a networked system such that their interactions are driven by a graph G, where
G={V,E}, where V={1,2, .., N}indicates the node or agent set, E ⊆ V × V is the edge set. Throughout
the paper, we assume that the graph is undirected and connected. The connection between any two agents
iandjcan be determined by their physical connections, leading to the communication between them.
Traditionally, if agent jis in the neighborhood of agent i, they can communicate with each other. Thus, we
deﬁne the neighborhood for any agent iasNb(i) :={j∈ V| (i, j)∈ Eor j =i}. Rather than considering
3Published in Transactions on Machine Learning Research (MM/YYYY)
synchronization and asynchronization separately, this paper considers both scenarios together by deﬁning
the following terminologies.
Deﬁnition 1. At a time step t, an agent jis called a reliable neighbor of the agent iif agent ihas the
state information of agent jup to t−1.
Deﬁnition 2. At a time step t, an agent jis called an unreliable neighbor of the agent iif agent ihas
the state information of agent jonly up to t−τ, where τis the so-called delay and 1< τ < ∞.
Remark: Deﬁnitions 1and2allow us to perceive the delay problem in the decentralized learning with a
new perspective that depends on the amount of delay. One agent can selectively make use of the outdated
information from unreliable neighbors or completely drop such information. The ﬁrst scenario is related to
most previous works on asynchronous delay tolerant approaches as it involves a gradient prediction technique
to reduce the negative eﬀects of stale parameters. The second scenario corresponds to most synchronous
schemes since the agent only collects information from the reliable neighbors. In this paper, we mainly focus
on the ﬁxed delay with a connected graph to better characterize the inﬂuence of delay. Real cases with
complex topology and time-varying delay modeling are essential but future extensions of the work.
Thus, inside the neighborhood of an agent, there are reliable and unreliable neighbors respectively. This
work aims at studying how to eﬀectively tackle issues such as negative impacts that delays may bring on the
performance. We deﬁne a set for reliable neighbors of agent ias:R:={j∈Nb(i)|Pr(xj=xj
t−1|t) = 1},
implying that agent jhas the state information xup to the time t−1, i.e., xj
t−1. We can directly have the
set for unreliable neighbors such that Rc=Nb\ R1.
Then we can consider the decentralized empirical risk minimization problems, which can be expressed as the
summation of all local losses incurred by each agent:
min F(x) :=N∑
i=1∑
s∈Difs
i(x) (1)
where x= [x1;x2;. . .;xN],xiis the local copy of x∈Rd,Diis a local data set uniquely known by agent
i,fs
i:Rd→Ris the incurred local loss of agent igiven a sample s. Based on the above formulation, we
then assume everywhere that our objective function is bounded from below and denote the minimum by
F∗:=F(x∗)where x∗:=argmin F(x). Hence F∗>−∞. Moreover, all vector norms refer to the Euclidean
norm while matrix norms refer to the Frobenius norm. Some necessary deﬁnitions and assumptions are given
below for characterizing the main results.
Assumption 1. Each objective function fiis assumed to satisfy the following conditions: a) fiisγi−smooth ;
b)fiis proper (not everywhere inﬁnite) and coercive.
Assumption 2. A mixing matrix W∈RN×Nsatisﬁes a) 1⊤W=1⊤, W1⊤=1⊤; b) Null {I−W}=
Span{1}; c)I⪰W≻ −I.
Assumption 3. The stochastic gradient of Fat any xis denoted by g(x), such that a) g(x)is the unbiased
estimate of gradient ∇F(x); b) The variance is uniformly bounded by σ2, i.e.,E[∥g(x)− ∇F(x)∥2]≤σ2; c)
The second moment of g(x)is bounded, i.e., E[∥g(x)∥2]≤G2.
Remark: Given Assumption 1, one immediate consequence is that Fisγm:=max{γ1, γ2, . . . , γ N}-smooth
at all x∈RdN.The main outcome of Assumption 2is that the mixing matrix Wis doubly stochastic matrix
and that we have e1(W) = 1 > e2(W)≥..≥eN(W)>−1, where ez(W)denotes the z-th largest eigenvalue
ofWZeng & Yin (2018). In Assumption 3, the ﬁrst two are quite generic. While the third part is much
weaker than the bounded gradient that is not necessarily applicable to quadratic-like objectives.
1Note that the delay varies in the asynchronous learning scheme, and there are two types of asynchronization, (i) ﬁxed value
of delays Zheng et al. (2017);Rigazzi (2019) and (ii) time-varying delays Dean et al. (2013);Lian et al. (2017) along the learning
process. We follow the ﬁrst setting in this work to implement the experiments.
4Published in Transactions on Machine Learning Research (MM/YYYY)
3 PC-ASGD
3.1 Algorithm Design
We present the speciﬁc update law for our proposed method, PC-ASGD. In Algorithm 1, for the predicting
step (line 6), any agent kthat is unreliable has a delay when communicating its weights with agent i. To
compensate for the delay, we adopt the Taylor expansion to approximate the gradient for each time step. The
predicted gradient (or delay compensated gradient) is denoted by gdc
k(xk
t−τ), which is expressed as follows
gdc
k(xk
t−τ) =τ−1∑
r=0gk(xk
t−τ) +λgk(xk
t−τ)⊙gk(xk
t−τ)⊙(xi
t−τ+r−xi
t−τ), (2)
where λis a positive constant in (0,1]and the term λgk(xk
t−τ)⊙gk(xk
t−τ)is an estimate of the Hessian matrix,
∇gk(xk
t−τ). Throughout the rest of analysis, we deﬁne gdc,r
k(xk
t−τ) := gk(xk
t−τ) +λgk(xk
t−τ)⊙gk(xk
t−τ)⊙
(xi
t−τ+r−xi
t−τ). We brieﬂy provide explanation for the ease of understanding, while referring interested
readers to Appendix A.2for the details of the derivation of Eq. 2.For agent k, at the current time step t,
since it did not get updated over the past τtime steps, it is known that xk
t:=xk
t−τ. By abuse of notation, we
usegdc
k(xk
t−τ)instead of gdc
k(xk
t)for the predicted gradient, as the former reasonably justiﬁes Eq. 2. While
we still keep the parameter of xk
tin the predicting step since it will be convenient for us to derive the
compact step in the following. However, one can also replace xk
twith xk
t−τif necessary. Additionally, the
communication scheme for the predicting step among agents remains similar as in Jiang et al. (2017) but our
method needs extra communication overhead in the predicting step for the gradient information of gk(xk
t−τ).
This burden can be further reduced by gradient quantization Alistarh et al. (2017) but beyond the scope of
our work.
Remark: Also, the term (xi
t−τ+r−xi
t−τ)is from agent idue to the inaccessible outdated information
of agent k, which intuitively illustrates that the compensation is driven by the agent iwhen agent kis
in its neighborhood and deemed an unreliable one. We remark that the replacement by using agent iis
reasonably feasible due to the following three reasons. First, the individual model diﬀerence decays along
with time for all agents in the network. Particularly, as agent kis in the neighborhood of agent i, the
decaying trend between (xi
t−τ+r−xi
t−τ)and (xk
t−τ+r−xk
t−τ)should be similar. Second, such a replacement
may cause an extra error term in the error bound if imposing an assumption to bound the diﬀerence, e.g.,
∥(xi
t−τ+r−xi
t−τ)−(xk
t−τ+r−xk
t−τ)∥ ≤c, where c≥0, but the overall convergence rate remains the same
as the step size plays a role to determine it. Finally, the empirical results show the feasibility of such a
replacement. If the replacement caused the divergence of the gradient norm, the training loss and testing
accuracy would be poor. In our practice, we carefully examine their diﬀerence to make sure the replacement
proceeds well. However, due to the replacement, the predicted gradient is approximate.
On the contrary, at the time instant t, when the clipping step is taken, intuitively, we have to clip the agents
that possess outdated information, resulting in the change of the mixing matrix W. Essentially, we can
manipulate the corresponding weight values wij, j∈ RcinWsuch that at the clipping step, wij= 0, j∈ Rc.
For the convenience of analysis, we introduce ˜Wto represent the mixing matrix at this step. This seems a
bit unnecessary in terms of formulation as the ﬁrst part in the predicting step can be used for the clipping
step, while for analysis it would help clarify, particularly when we show the convex combination in line 8.
Additionally, though the ﬁrst part in the predicting step is essentially the same as the clipping step, the
separation between them in Algorithm 1would make the presentation clear about that these two steps take
place in a single update.
Diﬀerent from the DC-ASGD, which signiﬁcantly relies on a central server to receive information from each
agent, our work removes the dependence on the central server, and instead constructs a graph for all of
agents. The clipping step (line 7) essentially rejects information from all the unreliable neighbors in the
neighborhood of one agent. Subsequently, the equality in line 8 balances the tradeoﬀ between the predicting
and clipping steps. In practice, the determination of θtresults in some practical variants. In the empirical
study presented in Section 5, one can see that θtis either 0 or 1 by leveraging one condition, which implies
that in each epoch, only one step is adopted, yielding two other variants shown in the experiments, C-ASGD
5Published in Transactions on Machine Learning Research (MM/YYYY)
Algorithm 1: PC-ASGD
Input: number of agents N, learning rate η >0, agent interaction matrices W,˜W, number of epochs T,
the tradeoﬀ parameter 0≤θt≤1, t∈ {0,1, . . . , T −1}
Output: the models’ parameters in agents xi
T,i= 1,2,···N
1:Initialize all the agents’ parameters xi
0,i= 1,2,···N
2:Do broadcast to identify the clusters of reliable agents and the delay τ
3:t= 0
4:while epoch t < T do
5:foreach agent ido
6: Predicting Step: xi
t+1,pre=∑
j∈Rwijxj
t−ηgi(xi
t) +∑
k∈Rcwik(xk
t−ηgdc
k(xk
t−τ))
7: Clipping Step: xi
t+1,cli=∑
j∈Nb(i)˜wijxj
t−ηgi(xi
t)
8: xi
t+1=θtxi
t+1,pre+ (1−θt)xi
t+1,cli
9:end for
10: t=t+ 1
11:end while
or P-ASGD. However, for the sake of generalization, we provide the analysis for the combined steps (line 8).
In practice, we try a practical strategy for adaptive θchoices and we also show the eﬀectiveness empirically.
Since the term∑
k∈Rcwikgdc
k(xk
t−τ)applies to unreliable neighbors only, for the convenience of analysis, we
expand it to the whole graph. It means that we establish an expanded graph to cover all of agents by setting
some elements in the mixing matrix W′∈RN×Nequal to 0, but keeping the same connections as in W.
Namely, we have w′
ik= 0, k∈ R andw′
ik=wik, k∈ Rc. By setting the current time as t+τ, the compact
form in line 8 can be rewritten as:
xt+τ+1=Wt+τxt+τ−η(g(xt+τ) +θt+ττ−1∑
r=0W′gdc,r(xt)) (3)
Wt+τis denoted by θt+τW+ (1−θt+τ)˜W, where W=W⊗Id×d,˜W=˜W⊗Id×d, and W′=W′⊗Id×d.
We have deferred the derivation of Eq. 3to the Appendix.
4 Convergence Analysis
This section presents convergence results for the PC-ASGD. We show the consensus estimate and the optimal-
ity for both weakly strongly-convex (Polyak-Łojasiewicz Condition Karimi et al. (2016)), generally convex,
and nonconvex smooth objectives. The consensus among agents (aka, disagreement estimate) can be thought
of as the norms ∥xi
t−xj
t∥, the diﬀerences between the iterates xi
tandxj
t. Alternatively, the consensus can
be measured with respect to a reference sequence, i.e., yt=1
N∑N
i=1xi
t. In particular, we discuss ∥xi
t−yt∥
for any time tas the metrics with respect to the delay τ.
Lemma 1. (Consensus ) Let Assumptions 2 and 3 hold. Assume that the delay compensated gradients are
uniformly bounded, i.e., there exists a scalar B > 0, such that
∥gdc,r(xt)∥ ≤B,∀t≥0and 0≤r≤τ−1,
Then for all i∈Vandt≥0,∃η >0, we have
E[∥xi
t−yt∥]≤ηG+ (τ−1)Bθm
1−δ2, (4)
where θm=max{θs+1}t+τ−1
s=t,δ2=max{θse2+ (1−θs)˜e2}t+τ−1
s=0 <1, where e2:=e2(W)<1and ˜e2:=
e2(˜W)<1.
The detailed proof is shown in the Appendix. Lemma 1 states the consensus bound among agents, which
is proportional to the step size ηand inversely proportional to the gap between the largest and the second-
largest magnitude eigenvalues of the equivalent graph W.
6Published in Transactions on Machine Learning Research (MM/YYYY)
Remark: One implication that can be made from Lemma 1 is when τ= 1, the consensus bound becomes the
smallest, which can be obtained asηG
1−δ2. This bound is the same as obtained already by most decentralized
learning (or optimization) algorithms. This accordingly implies that the delay compensated gradient or
predicted gradient does not necessarily require many time steps. Otherwise, more compounding error could
be included. Alternatively, θm= 0 can also result in such a bound, suggesting that the clipping step
dominates in the update. On the other hand, once τ≫1andθm̸= 0, the consensus bound becomes worse,
which will be validated by the empirical results. Additionally, if the network is sparse, which suggests e2→1
and ˜e2→1, the consensus among agents may not be achieved well and correspondingly the optimality would
be negatively aﬀected, which has been justiﬁed in existing works Jiang et al. (2017).
Most previous works have typically explored the convergence rate on the strongly convex objectives. However,
the assumption of strong convexity can be quite strong in most models such that the results obtained may
be theoretically instructive and useful. Hence, we introduce a condition that is able to relax the strong
convexity, but still maintain the similar theoretical property, i.e., Polyak-Łojasiewicz (PL) condition Karimi
et al. (2016). The condition is expressed as follows: A diﬀerentiable function Fsatisﬁes the PL condition
such that there exists a constant µ > 0
1
2∥∇F(x)∥2≥µ(F(x)−F∗). (5)
When F(x)is strongly convex, it also implies the PL condition. However, this is not vice versa. We now
state the ﬁrst main result.
Theorem 1. Let Assumptions 1,2 and 3 hold. Assume that the delay compensated gradients are uniformly
bounded, i.e., there exists a scalar B > 0such that
∥gdc,r(xt)∥ ≤B,∀t≥0and 0≤r≤τ−1, (6)
and that ∇F(xt)isξm-smooth for all t≥0. Then for the iterates generated by PC-ASGD, when 0< η≤1
2µτ
and the objective satisﬁes the PL condition, they satisfy
E[F(xt)−F∗]≤(1−2µητ)t−1(F(x1)−F∗−Q
2µητ) +Q
2µητ, (7)
where
Q= 2(1 −2µητ)GηC 1+η3ξmG
2τ−1∑
r=1Cr+ 2η2GγmC1
+Gητσ +η2G(γm+ϵD+ϵ+ (1−λ)G2)τ−1∑
r=1Cr+ηG2+η2γmGτC 2(8)
andC1=G+(τ−1)Bθm
1−δ2, Cr=2G+(r−1)Bθm
1−δ2, C2=2G+(τ−1)Bθm
1−δ2.ϵD>0andϵ >0are upper bounds for the
approximation errors of the Hessian matrix that can be obtained as we describe in the Appendix2.
Remark: One implication from Theorem 1is that PC-ASGD enables the iterates {xt}to converge to the
neighborhood of x∗, which isQ
2ηµτ, matching the results by Jiang et al. (2017);Bottou et al. (2018);Patrascu
& Necoara (2017). In addition, Theorem 1shows that the error bound is signiﬁcantly attributed to network
errors caused by the disagreement among agents with respect to the delay and the variance of stochastic
gradients. Another implication can be made from Theorem 1is that the convergence rate is closely related
to the delay and the step size such that when the delay is large it may reduce the coeﬃcient, 1−2µητ, to
speed up the convergence. However, correspondingly the upper bound of the step size is also reduced. Hence,
there is a tradeoﬀ between the step size and the delay in PC-ASGD. Theorem 1also suggests that when
the objective function only satisﬁes the PL condition and is smooth, the convergence to the neighborhood of
x∗in a linear rate can still be achieved. The PL condition may not necessarily imply convexity and hence
the conclusion can even apply to some nonconvex functions. To further analyze the error bound, we deﬁne
2The proof for this theorem is fairly non-trivial and technical. We refer readers to the Appendix for more details. To simplify
the proof, this main result will be divided into several lemmas.
7Published in Transactions on Machine Learning Research (MM/YYYY)
η=O(1√
t), PC-ASGD enjoys a convergence rate of O(ρt+1
t+1√
t)to the neighborhood of x∗, which becomes
G(2(1−2µητ)C1+τσ+G).
Studying the convergence behavior of PC-ASGD for generally convex functions is critically vital as many
objectives in machine learning fall into this category of interest. However, the proof techniques for showing
the convergence are diﬀerent from those shown for the above weakly strongly convex objectives. In the sequel,
a well-known result regarding convexity is ﬁrst introduced and then the theoretical claim is presented, while
its associated proof is deferred to the Appendix.
Lemma 2. IfF:Rd→Ris convex and diﬀerentiable, then for all x, y∈Rd, the following holds:
F(x)≥f(y) +⟨∇F(y), x−y⟩. (9)
Theorem 2. Let Assumptions 1, 2 and 3 hold. Assume that the delay compensated gradients are uniformly
bounded, i.e., there exists a scalar B > 0such that for all T≥1
∥gdc(xt)∥ ≤B,∀t≥0and 0≤r≤τ−1, (10)
and there exists C > 0,
E[∥xt−x∗∥]≤C, (11)
where x∗∈argmin F(x). Then for the iterations generated by PC-ASGD, there exists 0< η <1
20γm, such
that
E[F(¯xT)−F∗]≤∥x1−x∗∥2
Tη+A
η, (12)
where A= 10η2σ2
∗+10η2σ2+20η4G2C2
1+5η2θ2
mτ2B2+2ηCθ mτB+2Gη2C1(2C+1), C1=G+(τ−1)Bθm
1−δ2, σ2
∗:=
E∥g(x∗)− ∇F(x∗)∥2,¯xT:=1
T∑T
t=1xt.
Remark: As Theorem 2suggests, when Fis generally convex, asymptotically, PC-ASGD yields the con-
vergence of the iterates {xt}to the neighborhood of x∗, which isA
η. Analogously, Theorem 2shows that
the error bound is highly correlated with the consensus estimates among agents and the stochastic gradient
variances as well as the time delay. To further analyze the error bound, we still deﬁne the η=O(1√
T),
PC-ASGD exhibits a convergence rate of O(1√
T+1
T1.5)to the neighborhood of x∗, which is 2C(τ−1)θmB.
This implies that if there is no time delay, PC-ASGD converges to the optimal solution exactly. Also, the
convergence rate matches the state-of-the-art in both centralized Garrigos & Gower (2023);Khaled et al.
(2023) and distributed Nedic (2020);Sun et al. (2023);Choi & Kim (2023) settings. In Theorem 2, we also
impose a constraint for the distance between xtandx∗. Immediately, we know that the error bound in
Eq.12can becomeC2
Tη+A
η, which is relatively looser. On the other hand, without such a constraint, we can
probably just use ∥xt−x∗∥ ≤ ∥ x1−x∗∥to replace C, which enables a slightly tighter bound. This also
illustrates that Eq. 11is not a strong constraint.
We next investigate the convergence for the non-convex objectives. For PC-ASGD, we show that it converges
to a ﬁrst-order stationary point at a sublinear rate. It should be noted that such a result may not absolutely
guarantee a feasible minimizer due to the lack of some necessary second-order information. However, for most
nonconvex optimization problems, this is generic, though some existing works have discussed the second-order
stationary points Carmon et al. (2018), which is out of our investigation scope.
Theorem 3. Let Assumptions 1, 2 and 3 hold. Assume that the delay compensated gradients are uniformly
bounded, i.e., there exists a scalar B > 0such that for all T≥1
∥gdc,r(xt)∥ ≤B,∀t≥0and 0≤r≤τ−1, (13)
and there exists M > 0,
E[∥gdc(xt)∥2]≤M. (14)
Then for the iterations generated by PC-ASGD, there exists 0< η <1
γm, such that
1
TT∑
t=1E[∥∇F(xt)∥2]≤2(F(x1)−F∗)
Tη+R
η, (15)
8Published in Transactions on Machine Learning Research (MM/YYYY)
where, R= 2Gη2C1+τ2η2γmM
2+ησ2
2+ηστB + 2η2γm(τB+G)C1, C1=G+(τ−1)Bθm
1−δ2.
Remark: Theorem 3states that with a properly chosen constant step size, PC-ASGD is able to converge
the iterates {xT}to the noisy neighborhood of a stationary point x∗in a rate of O(1√
T), whose radius is
determined byσ2
2+στB, if we deﬁne η=O(1√
T). Additionally, based onσ2
2+στB, we can know that the
error bound is mainly caused by the variance of stochastic gradients and the time delay. As the length of
the delay can have an impact on the predicting steps used in the delay compensated gradient, a short term
prediction may help alleviate the negative eﬀect caused by the stale agents. Otherwise, the compounding
error in the delay compensated gradient could deteriorate the performance of the algorithm.
5 Experiments
5.1 Practical Variant
So far, we have analyzed theoretically in detail how the proposed PC-ASGD converges with some mild
assumptions. In practical implementation, we need to choose a suitable θtto enable the training fast with
clipping steps and allow the unreliable neighbors to be involved in training with predicting steps. In this
context, we develop a heuristic practical variant with a criterion for determining the tradeoﬀ parameter value.
Intuitively, if the delay messages from the unreliable neighbors do not inﬂuence the training negatively, they
should be included in the prediction. This can be determined by the comparison with the algorithm without
making use of these messages. The criterion is shown as follows:
xt+1
i={
xi
t+1,pre⟨xi
t+1,pre−xi
t,gi(xi
t)⟩
∥xi
t+1,pre−xi
t∥≥⟨xi
t+1,cli−xi
t,gi(xi
t)⟩
∥xi
t+1,cli−xi
t∥
xi
t+1,cli o.w.(16)
where we choose the cosine distance to compare the distances for predicting and clipping steps. The prediction
step is selected if it has the larger cosine distance, which implies that the update due to the predicting
step yields the larger loss descent. Otherwise, the clipping step should be chosen by only trusting reliable
neighbors. Our practical variant with this criterion still converges since we just set θtas0or1for each
iteration and the previous analysis in our paper still holds. To facilitate the understanding of predicting and
clipping steps, in the following experiments, we also have two other variants P-ASGD and C-ASGD. While
the former corresponds to an “optimistic" scenario to only rely on the predicting step, the latter presents a
“pessimistic" scenario by dropping all outdated agents. Both of variants follow the same convergence rates
induced by PC-ASGD. The speciﬁc algorithm is shown as Algorithm 2.
5.2 Distributed Network and Learning Setting
Models and Data sets . D-ASGD is adopted as the baseline algorithm. Two deep learning structures,
PreResNet110 He et al. (2016b ), DenseNet Huang et al. (2017), ResNet20 He et al. (2016a ) and EﬃcientNet
Tan & Le (2019) (noted as model 1 ,model 2 ,model 3 andmodel 4 ), are employed. The detailed training
settings are illustrated in Appendix C.CIFAR-10, CIFAR-100 and TinyImageNet are used in the experiments
following the settings in Krizhevsky (2012). The training data is randomly assigned to each agent, and the
parameters of the deep learning structure are maintained within each agent and communicated with the
predeﬁned delays. The testing set is utilized for each agent to verify the performance, where our metric is
the average accuracy among the agents. 6 runs are carried out for each case and the mean and variance are
obtained and listed in Table 3.
Delay setting . The delay is set as τas discussed before, which means the parameters received from the
agents outside of the reliable cluster are the ones that were obtained τiterations before. τis both ﬁxed at
20 to test the performances of diﬀerent algorithms including our diﬀerent variants (P-ASGD, C-ASGD, and
PC-ASGD) and baseline algorithms in Section 5.3and5.5. We also try to exploit its impact in Section 5.4.
Distributed network setting . A distributed network (noted as distributed network 1 ) with 8agents
(nodes) in a fully connected graph is ﬁrst applied with model 1-4 , and 2 clusters of reliable agents are deﬁned
within the graph consisting of 3 agents and 5 agents, respectively. Then two distributed networks (with
9Published in Transactions on Machine Learning Research (MM/YYYY)
Algorithm 2: PC-ASGD-PV
Input: number of agents N, learning rate η >0, agent interaction matrices W,˜W, number of epochs T
Output: the models’ parameters in agents xi
T,i= 1,2, . . . , N
1:Initialize all the agents’ parameters xi
0,i= 1,2, . . . , N
2:Do broadcast to identify the clusters of reliable agents and the delay τ
3:t= 0
4:while epoch t < T do
5:foreach agent ido
6: Predicting Step: xi
t+1,pre=∑
j∈Rwijxj
t−ηgi(xi
t) +∑
k∈Rcwik(xk
t−ηgdc
k(xk
t−τ))
7: Clipping Step: xi
t+1,cli=∑
j∈R˜wijxj
t−ηgi(xi
t)
8: ∆pre=xi
t+1,pre−xi
t; ∆cli=xi
t+1,cli−xi
t
9: if⟨∆pre,gi(xi
t)⟩
∥∆pre∥≥⟨∆cli,gi(xi
t)⟩
∥∆cli∥then
10: xi
t+1=xi
t+1,pre
11: else
12: xi
t+1=xi
t+1,cli
13: end if
14: end for
15: t=t+ 1
16:end while
5-agent and 20-agent, respectively) are used for scalability analysis, noted as distributed network 2 and
distributed network 3 , individually. For distributed network 2 , we construct 2 clusters of reliable agents with
3 and 2 agents. In distributed network 3 , four clusters are formed and 3 clusters consist of 6agents while
each of the rest has 2agents.
5.3 Performance Evaluation
The testing accuracies on the CIFAR-10 and CIFAR-100 data sets with model 1 andmodel 2 indistributed
network 1 are shown in Fig. 1. It shows that the proposed PC-ASGD outperforms the other single variants
and it presents an accuracy increment greater than 2.3%(nearly 4%for DenseNet with CIFAR-10) compared
to the baseline algorithm. For other variants P-ASGD or C-ASGD, the testing accuracies are also higher
than that of the baseline algorithm. Moreover, PC-ASGD shows faster convergence than P-ASGD as the
updating rule overcomes the staleness, and achieves better accuracy than the C-ASGD as it includes the
messages from the unreliable neighbors. This is consistent with the analysis in this work. We also show the
detailed results of both distributed network 1 anddistributed network 3 in Table 2.
We then compare our proposed algorithm with other delay-tolerant algorithms in distributed network 1
with model 1-4 , including the baseline algorithm D-ASGD , DC-s3gd Rigazzi (2019),D-ASGD with IS Du
et al. (2020), and Adaptive Braking Venigalla et al. (2020). The distributed network 1 is applied for the
comparisons. From Table 3, the proposed PC-ASGD obtains the best results in all the cases. It should
be noted that some of above-listed algorithms are not designed speciﬁcally for this kind of peer-to-peer
application (e.g., Adaptive Braking) or may not consider the modeling of severe delays in their works (e.g.,
D-ASGD with IS and DC-s3gd). In this context, they may not perform well in the test cases. The results
also demonstrate our proposed framework can be employed by diﬀerential types of models, such as simple
ResNet20 and complex EﬃcientNet. We also conduct numerical studies on TinyImageNet and time-series
dataset in Appendix D, the results also verify the eﬀectiveness of our method. Before concluding this section,
we remark on the diﬀerence between PC-ASGD and DC-s3gd as the latter also leverages the predicting step
for the gradient estimates. In our PC-ASGD approach, we diﬀerentiate between reliable and unreliable agents,
allowing reliable agents to proceed with their updates. For unreliable agents, our method incorporates both
predicting and clipping steps, establishing choice criteria to enhance overall performance. In contrast, DC-
s3gd exclusively employs prediction (delay compensation) for all agents. Regarding the delay compensation
aspect, for agent irequiring an update, our approach employs delay compensation with the delayed gradient
10Published in Transactions on Machine Learning Research (MM/YYYY)
0 25 50 75 100 125 150 175 200
epochs4050607080Accuracy
PC-ASGD
P-ASGD
C-ASGD
Baseline
(a) DenseNet CIFAR-10
0 25 50 75 100 125 150 175 200
epochs50:052:555:057:560:062:565:067:5Accuracy
PC-ASGD
P-ASGD
C-ASGD
Baseline (b) DenseNet CIFAR-100
0 20 40 60 80 100 120 140 160
epochs30405060708090Accuracy
PC-ASGD
P-ASGD
C-ASGD
Baseline
(c) PreResNet110 CIFAR-10
0 25 50 75 100 125 150 175
epochs50525456586062646668Accuracy
PC-ASGD
P-ASGD
C-ASGD
Baseline (d) PreResNet110 CIFAR-100
Figure 1: Testing accuracy on CIFAR-10 and CIFAR-100 with distributed network 1 .
Table 2: Performance evaluation of PC-ASGD on CIFAR-10 and CIFAR-100
5 agents
Model & datasetPC-ASGD P-ASGD C-ASGD Baseline
acc. (%) o.p. (%) acc. (%) o.p. (%) acc.(%) o.p. (%) acc. (%)
Pre110, CIFAR-10 87.3±1.1 3.3±1.184.9±0.9 0.9±0.9 86 .0±1.0 2.0±1.0 84 .0±0.3
Pre110, CIFAR-100 67.4±1.4 3.1±1.964.8±1.3 1.3±1.5 66 .4±1.2 1.9±1.6 64 .5±1.5
Des, CIFAR-10 86.9±0.9 3.6±1.884.4±0.6 1.0±1.5 85 .9±0.9 2.7±1.7 83 .3±0.9
Des, CIFAR-100 68.6±0.6 2.3±1.766.8±1.5 1.6±1.6 66 .8±1.6 1.8±1.6 66 .1±1.9
20 agents
Model & datasetPC-ASGD P-ASGD C-ASGD Baseline
acc. (%) o.p. (%) acc. (%) o.p. (%) acc.(%) o.p. (%) acc. (%)
Pre110, CIFAR-10 84.7±0.9 4.2±1.083.3±0.9 2.7±0.9 82 .5±1.0 1.9±1.4 80 .4±0.7
Pre110, CIFAR-100 62.4±0.8 3.3±2.061.7±1.0 2.0±1.6 61 .5±1.0 2.5±2.3 59 .3±1.7
Des, CIFAR-10 82.9±0.9 2.4±0.982.0±0.7 1.4±1.3 81 .8±0.6 1.8±1.0 80 .1±0.9
Des, CIFAR-100 64.5±0.7 3.8±1.762.5±1.3 2.9±2.0 62 .0±1.5 1.3±1.4 60 .4±1.7
acc.–accuracy, o.p.–outperformed comparing to baseline.
of agent kin the unreliable cluster, denoted as gdc
k, utilizing the term (xi
t−xi
t−τ). Conversely, DC-s3gd
utilizes (xi
t−xk
t−τ)without any theoretical guarantee. We hypothetically claim that though it is feasible
for us to leverage the same predicting step employed in DC-s3gd, while in practice, (xi
t−xk
t−τ)may cause
larger error bound when models between agents iandkare signiﬁcantly diﬀerent. We can also compare the
algorithm employing only P-step (P-ASGD), as presented in Table 2(with 5 agents) with DC-s3gd in Table
3. The results also reveal that in certain scenarios (Pre110/Des, CIFAR100), the performance of P-step is
better than that of DC-s3gd, which supports our approximation of gdc
kas well.
11Published in Transactions on Machine Learning Research (MM/YYYY)
Table 3: Performance comparison for diﬀerent algorithms
Model & datasetPre110
CIFAR-10Des
CIFAR-10ResNet20
CIFAR-10Pre110
CIFAR-100Des
CIFAR-100EﬃcientNet
CIFAR-100
PC-ASGD (Ours) 87.3±1.186.9±0.684.9±0.667.4±1.468.6±0.678.5±1.3
D-ASGD
Lian et al. (2017)84.0±0.3 82.5±0.1 83.3±0.9 64.5±1.5 66.1±1.9 74.7±0.4
DC-s3gd
Rigazzi (2019)86.3±0.8 85.7±0.8 83.1±0.7 63.5±1.7 66.2±1.3 76.0±1.1
D-ASGD with IS
Du et al. (2020)85.0±0.3 84.6±0.4 83.1±0.5 64.6±1.2 66.2±0.8 75.5±1.4
Adaptive Braking
Venigalla et al. (2020)86.8±0.9 85.3±1.0 84.3±0.4 66.5±1.2 67.3±1.1 77.3±0.8
5.4 Impacts of Diﬀerent Delay Settings
To further show our algorithm’s eﬀectiveness, we also implement experiments with diﬀerent delays. As
discussed above, a more severe delay could cause a signiﬁcant drop in the accuracy. More numerical studies
with diﬀerent steps of delay are presented here. The delays are set as 5,20,60with our PreResNet110 ( model
1) of 8 agents (synchronous network without delay is also tested). We use CIFAR-10 in the studies and the
topology is distributed network 1 . The results are shown in Fig. 2.
/uni00000036/uni0000005c/uni00000051/uni00000046/uni0000004b/uni00000055/uni00000052/uni00000051/uni00000052/uni00000058/uni00000056 /uni00000018 /uni00000015/uni00000013 /uni00000019/uni00000013/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni0000001a/uni00000018/uni0000001b/uni00000013/uni0000001b/uni00000018/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni0000001c/uni00000014/uni00000011/uni00000015
/uni0000001b/uni0000001b/uni00000011/uni00000013/uni0000001b/uni0000001a/uni00000011/uni00000016/uni0000001b/uni00000018/uni00000011/uni0000001b/uni0000001b/uni0000001a/uni00000011/uni00000016/uni0000001b/uni00000019/uni00000011/uni00000016
/uni0000001b/uni00000017/uni00000011/uni00000015/uni0000001b/uni0000001a/uni00000011/uni00000019/uni0000001b/uni00000019/uni00000011/uni0000001b/uni0000001b/uni00000018/uni00000011/uni00000017/uni0000001b/uni00000019/uni00000011/uni0000001b
/uni0000001b/uni00000017/uni00000011/uni00000013
/uni0000001b/uni00000015/uni00000011/uni00000016
/uni00000036/uni0000005c/uni00000051/uni00000046/uni0000004b/uni00000055/uni00000052/uni00000051/uni00000052/uni00000058/uni00000056
/uni00000033/uni00000026/uni00000010/uni00000024/uni00000036/uni0000002a/uni00000027
/uni00000027/uni00000026/uni00000056/uni00000016/uni0000004a/uni00000047
/uni00000024/uni00000025
/uni00000027/uni00000010/uni00000024/uni00000036/uni0000002a/uni00000027
Figure 2: Performance evaluation for diﬀerent steps of delay.
We can ﬁnd out as the delay increases, the accuracy decreases. For the synchronous setting, the testing
accuracy is close to that in the centralized scenario Yang (2019) but with a higher batch size. When the
delay is 60, the accuracy for the D-ASGD reduces signiﬁcantly, and this validates that the large delay
signiﬁcantly inﬂuences the performance and causes diﬃculties in the training process. However, the delays
are practical in real implementations such as industrial IoT platforms. Our proposed PC-ASGD outperforms
other algorithms in all cases with diﬀerent delays. Moreover, the accuracy drop is relatively smaller in cases
with larger delays, which suggests that PC-ASGD is more robust to diﬀerent communication delays.
5.5 Impacts of Network Size
For evaluating the performance in diﬀerent structure sizes of distributed networks, distributed network 2
anddistributed network 3 follow the same setting as in the distributed network 1 (delay τ= 20 ,model 1 ,
CIFAR-10). The results are shown in Fig. 3. According to both Table 2and Fig. 3, as the number of agents
increases, the accuracy decreases. It shows that the large size of the network has a negative impact on the
training. Our proposed PC-ASGD outperforms all other approaches, which further validates the eﬃcacy and
scalability of the proposed algorithm.
12Published in Transactions on Machine Learning Research (MM/YYYY)
/uni00000018/uni00000003/uni00000044/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000056 /uni0000001b/uni00000003/uni00000044/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000056 /uni00000015/uni00000013/uni00000003/uni00000044/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000056/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni0000001a/uni00000018/uni0000001b/uni00000013/uni0000001b/uni00000018/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni0000001b/uni0000001b/uni00000011/uni00000013/uni0000001b/uni0000001a/uni00000011/uni00000016
/uni0000001b/uni00000017/uni00000011/uni0000001a /uni0000001b/uni00000018/uni00000011/uni00000015 /uni0000001b/uni00000017/uni00000011/uni0000001c
/uni0000001b/uni00000016/uni00000011/uni00000016/uni0000001b/uni00000019/uni00000011/uni00000015 /uni0000001b/uni00000019/uni00000011/uni00000013
/uni0000001b/uni00000015/uni00000011/uni00000018/uni0000001b/uni00000019/uni00000011/uni0000001a /uni0000001b/uni00000019/uni00000011/uni00000016
/uni0000001b/uni00000016/uni00000011/uni00000014/uni0000001b/uni0000001a/uni00000011/uni00000014 /uni0000001b/uni00000019/uni00000011/uni0000001b
/uni0000001b/uni00000015/uni00000011/uni0000001c/uni0000001b/uni00000018/uni00000011/uni00000013/uni0000001b/uni00000017/uni00000011/uni00000013
/uni0000001b/uni00000013/uni00000011/uni00000017
/uni00000033/uni00000026/uni00000010/uni00000024/uni00000036/uni0000002a/uni00000027
/uni00000033/uni00000010/uni00000024/uni00000036/uni0000002a/uni00000027
/uni00000026/uni00000010/uni00000024/uni00000036/uni0000002a/uni00000027
/uni00000027/uni00000026/uni00000056/uni00000016/uni0000004a/uni00000047
/uni00000024/uni00000025
/uni00000027/uni00000010/uni00000024/uni00000036/uni0000002a/uni00000027
Figure 3: Performance evaluation for diﬀerent numbers of agents.
5.6 Numerical Studies on θAssignments
We also conduct empirical studies about the diﬀerent choices for θ. As we mentioned above, a practical variant
is applied for θ, where we intend to form a strategy to determine if the received information (parameters of
the deep learning models) is outdated or not. Here, diﬀerent assignment rules for θare tested and compared.
Model 1 is applied, by using CIFAR-10 and the 8 agents system with 3 and 5 agents ( distributed network 1 ).
First, θis ﬁxed as 0.3,0.5,0.7(denoted as f1, f2, f3 ), respectively. Then we determine the θas0,1randomly
with ﬁxed probability in each round with 0.3,0.5,0.7(denoted as p1, p2, p3 ). We also try the fully uniformly
random assigned θin each round (denoted as r1). The results are listed in Table 4. The PC-ASGD-PV
Table 4: Mean Performance for Diﬀerent θassignment for Pre110, CIFAR-10
Method\Parameters f1/p1 f2/p2 f3/p3
θFixed 86.3 85.0 84.5
θBool randomly 85.6 85.0 84.1
θrandomly (r1) 85.2
PC-ASGD-PV 87.3
D-ASGD(Baseline) 84.0
obtains the best performance which implies that the trade-oﬀ between the predicting step and the clipping
step in Algorithm 2is proper and plays an important role in the convergence process. With the ﬁxed θ(ﬁrst
row ‘ θﬁxed’), the experimental results show that the optimal ratio between the predicting step and clipping
step is 0.3 in this case. And this suggests that more clipping steps are better. For the p1, p2, p3 cases
(second row θBool randomly, i.e. either 0 or 1), the experimental results show that the optimal probability
between the predicting step and clipping step is 0.3. This is consistent with the ﬁxed θcase. Compared with
the ﬁx θsetting, picking 0,1for the θin a predeﬁned probability performs worse. The randomness still helps
the convergence process but is not as good as the ﬁx θsetting. For the random θ, the randomness helps the
convergence process. However, there exists an optimal θfor every case and the randomness is not able to get
the best performance. The baseline D-ASGD gets the worst performance, which shows the predicting and
clipping steps are helpful for the scenarios with delays in the distributed network. This also provides us with
the necessity of the additional time cost for the predicting and clipping steps. Note also that optimizing the
selection of θis beneﬁcial and we can set θas binary or non-binary (continuous). The binary setting with
the strategy in Algorithm 2is straightforward and performs well in this work.
To further explore the connection between the θselection and the binary strategy in our algorithm, the
occurrence of choosing the predicting step or clipping step in PC-ASGD-PV is collected and shown in Fig.
4. The frequencies for the clipping and predicting step choices tend to stabilize with the epochs when the
values are around 0.625and 0.375respectively. This is consistent with the ﬁxed θexperiments (where the
optimal ratio between the predicting step and clipping step is 0.3, compared to 0.5and0.7.)The ﬁnal choice
frequency appears to be empirically determined by the proportion of unreliable clusters. It indicates that
the proportion of delayed information included by these unreliable clusters will determine the likelihood of
rejecting the P-step. Additionally, we also observe θwill increase as the time delay diminishes to be smaller
13Published in Transactions on Machine Learning Research (MM/YYYY)
empirically. However, we cannot construct a proportional relationship since we adopt some approximations
when dealing with P-step and more theoretical and empirical analysis can be an interesting future direction.
0 20 40 60 80 100 120 140 160
epochs0:20:30:40:50:60:70:8frequencyC choice
P choice
Figure 4: Predicting and clipping steps choices changing with epochs.
5.7 Time Cost Comparison
The time cost for the presented algorithm is compared with the baseline algorithm (D-ASGD), P-ASGD,
and C-ASGD. The average time costs for model 1 with CIFAR-10 in distributed network 1 are collected and
shown in Fig. 5.The hardware we adopt is shown in Appendix C.
/uni00000033/uni00000026/uni00000010/uni00000024/uni00000036/uni0000002a/uni00000027 /uni00000033/uni00000010/uni00000024/uni00000036/uni0000002a/uni00000027 /uni00000026/uni00000010/uni00000024/uni00000036/uni0000002a/uni00000027 /uni00000027/uni00000010/uni00000024/uni00000036/uni0000002a/uni00000027/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000014/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000014/uni00000011/uni00000014/uni0000001a/uni0000005b
/uni00000014/uni00000011/uni00000013/uni00000017/uni0000005b
/uni00000013/uni00000011/uni0000001c/uni0000001c/uni0000005b /uni00000014/uni00000011/uni00000013/uni0000005b
Figure 5: Average time costs for diﬀerent methods (per epoch).
We observe that the extra time costs for the predicting and clipping steps and additional criterion are not
large, although there are still 17% more costs compared to D-ASGD. Therefore, we need to consider the trade-
oﬀ before implementing the proposed algorithm. However, with the improvement of the local computing
resources and the architecture design, the extra time cost might be acceptable with the gains in performance.
Moreover, the extra time cost is not changed with the delay, while the boosting in the performance is more
signiﬁcant in large delays (as shown in Fig. 2). It means that our algorithm could be more applicable in the
distributed network with various delays, and this is realistic in industrial IoT systems where the computing
resources vary remarkably among the agents and the data in each agent also diﬀers signiﬁcantly.
5.8 Validation for Theoretical Analysis
Finally, we present two examples to verify our constructed theoretical analysis. We establish a network
involving three agents. We also set two reliable clusters with 1 and 2 agents, respectively. We leverage
three nonconvex functions, i.e., Rastrigin, Rosenbrock Liang et al. (2006) and three three-hump camel
function Horst et al. (2000) to test the performance of our proposed framework. Though these functions
are simple nonconvex problems, they have been used widely to test the performance of many numerical
optimizers Mishra (2006). We randomly sample batches during local training in each agent. We set a ﬁxed
step size according to our Theorem 2 as 0.008. The number of iterations is set to 500 for each case.
14Published in Transactions on Machine Learning Research (MM/YYYY)
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.02
1
0123
(a) Convergence trajectories for Rosen-
brock function
4
 2
 0 2 44
2
024(b) Convergence trajectories for Rastrigin
function
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.02
1
0123(c) Convergence trajectories for three-
hump camel function
0 10 20 30 40 50
iteration10203040gradient normdelay = 20
bound:delay=20
delay = 125
bound:delay=125
delay = 350
bound:delay=350
(d) Average gradients bound veriﬁcation
in Rosenbrock function
0 25 50 75 100 125 150 175 200
iteration6080100120140160gradient normdelay = 20
bound:delay=20
delay = 125
bound:delay=125
delay = 350
bound:delay=350(e) Average gradients bound veriﬁcation
in Rastrigin function
0 10 20 30 40 50
iteration1015202530354045gradient normdelay = 20
bound:delay=20
delay = 125
bound:delay=125
delay = 350
bound:delay=350(f) Average gradients bound veriﬁcation in
three three-hump camel function
Figure 6: The results of simple functions.
From Fig. 6(a),6(b) and6(c), we can view the convergence of our proposed PC-ASGD algorithms. For
the bound veriﬁcation, we take diﬀerent values of the delay to observe the performances of our theoretical
framework. We ﬁrst ﬁnd that when the delay is large, the squared norm of the gradient is large, which is
consistent with our theoretical analysis. In all three cases, the quantitative results verify the correctness
of our proposed theoretical framework in Fig. 6(d),6(e), and 6(f).In the Rosenbrock function case, our
established theory could describe the tendency of the average gradients square norm and the results are nearly
tight asymptotically. But in Rastrigin function and three three-hump camel function cases, we observe that
the diﬀerences between diﬀerent delays are not large such that the bound is not so tight. However, when
calculating bounds, we ﬁnd that the bounds for diﬀerent delays diﬀer mildly, which is consistent with all the
empirical results. It also shows the eﬀectiveness of our proposed theoretical analysis.
6 Limitations
In practical applications, our proposed algorithm still faces challenges. In comparison with the classical
D-ASGD, our methods do not excel in communication eﬃciency. Notably, our methods introduce additional
computational overhead, as illustrated in Fig. 5(approximately 0.17 times). This is attributed to increased
communication overhead (doubled for transmitting gradients to compute gdc
k) and heightened memory over-
head, necessitating agents to store xi
t−τ, . . . , xi
t−1.
These challenges become more pronounced when computational resources, communication bandwidth, and
storage capacity are limited. For the real-world implementation of our algorithm, it is necessary to ensure the
availability of adequate resources. It’s also a promising direction to alleviate these burdens by algorithmic
improvements, such as quantization and compression.
In addition to the conventional practice of tuning the learning rate, ﬁne-tuning the parameter λingdc
kis
also essential for optimal results (as detailed in Appendix C). If opting to determine the P/C choice θ, it
also requires tuning in accordance with the discussions in Sec. 5.6.
15Published in Transactions on Machine Learning Research (MM/YYYY)
7 Conclusion
This paper presents a novel learning algorithm for distributed deep learning with heterogeneous delay charac-
teristics in agent-communication-network systems. We propose PC-ASGD algorithm consisting of a predict-
ing step, a clipping step, and the corresponding update law for reducing the staleness and negative eﬀects
caused by the outdated weights. We present theoretical analysis for the convergence rate of the proposed
algorithm with constant step size when the objective functions are weakly strongly-convex and nonconvex.
The numerical studies show the eﬀectiveness of our proposed algorithms in diﬀerent distributed systems with
delays, by comparing it to multiple baselines. In future work, the cases for distributed networks with diverse
delays and dynamic topology will be further studied and tested.
References
Soheil Abbasloo and H. Jonathan Chao. SharpEdge: An Asynchronous and Core-Agnostic Solution to
Guarantee Bounded-Delays. arXiv e-prints , art. arXiv:2001.00112, Dec 2019.
Nazanin Abolfazli, Afrooz Jalilzadeh, and Erfan Yazdandoost Hamedani. An accelerated asynchronous
distributed method for convex constrained optimization problems. In 2023 57th Annual Conference on
Information Sciences and Systems (CISS) , pp. 1–6. IEEE, 2023.
Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. arXiv: Optimization and
Control , 2011.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communication-eﬃcient
sgd via gradient quantization and encoding. Advances in neural information processing systems , 30, 2017.
Mahmoud S Assran and Michael G Rabbat. Asynchronous gradient push. IEEE Transactions on Automatic
Control , 66(1):168–183, 2020.
S. Becker and Yann Lecun. Improving the convergence of back-propagation learning with second-order
methods. In D. Touretzky, G. Hinton, and T. Sejnowski (eds.), Proceedings of the 1988 Connectionist
Models Summer School, San Mateo , pp. 29–37. Morgan Kaufmann, 1989.
Michael Blot, David Picard, Matthieu Cord, and Nicolas Thome. Gossip training for deep learning. arXiv:
Computer Vision and Pattern Recognition , 2016.
Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning.
SIAM review , 60(2):223–311, 2018.
Xuanyu Cao, Tamer Başar, Suhas Diggavi, Yonina C Eldar, Khaled B Letaief, H Vincent Poor, and Junshan
Zhang. Communication-eﬃcient distributed learning: An overview. IEEE journal on selected areas in
communications , 2023.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for nonconvex opti-
mization. SIAM Journal on Optimization , 28(2):1751–1772, 2018.
Shicong Cen, Huishuai Zhang, Yuejie Chi, Wei Chen, and Tie-Yan Liu. Convergence of distributed stochastic
variance reduced methods without sampling extra data. IEEE Transactions on Signal Processing , 68:3976–
3989, 2020.
Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. Revisiting distributed
synchronous sgd. arXiv preprint arXiv:1604.00981 , 2016.
Yang Chen, Xiaoyan Sun, and Yaochu Jin. Communication-eﬃcient federated deep learning with layerwise
asynchronous model update and temporally weighted aggregation. IEEE transactions on neural networks
and learning systems , 31(10):4229–4238, 2019.
Woocheol Choi and Jimyeong Kim. On the convergence analysis of the decentralized projected gradient
descent. arXiv preprint arXiv:2303.08412 , 2023.
16Published in Transactions on Machine Learning Research (MM/YYYY)
Jeﬀrey Dean, Greg S Corrado, Rajat Monga, Kai Chen, and Andrew Y Ng. Large scale distributed deep
networks. Advances in neural information processing systems , 2013.
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Distributionally robust federated aver-
aging. Advances in Neural Information Processing Systems , 33, 2020.
Yubo Du, Keyou You, and Yilin Mo. Asynchronous stochastic gradient descent over decentralized datasets.
In2020 IEEE 16th International Conference on Control & Automation (ICCA) , pp. 216–221. IEEE, 2020.
Chen Dun, Mirian Hipolito, Chris Jermaine, Dimitrios Dimitriadis, and Anastasios Kyrillidis. Eﬃcient and
light-weight federated learning via asynchronous distributed dropout. In International Conference on
Artiﬁcial Intelligence and Statistics , pp. 6630–6660. PMLR, 2023.
Mathieu Even, Hadrien Hendrikx, and Laurent Massoulié. Asynchrony and acceleration in gossip algorithms.
arXiv preprint arXiv:2011.02379 , 2020.
Hamid Reza Feyzmahdavian, Arda Aytekin, and Mikael Johansson. An asynchronous mini-batch algorithm
for regularized stochastic optimization. conference on decision and control , 61(12):1384–1389, 2015.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning , volume 1.
Springer series in statistics New York, 2001.
Juan Jose Gamboa-Montero, Fernando Alonso-Martin, Sara Marques-Villarroya, Joao Sequeira, and
Miguel A Salichs. Asynchronous federated learning system for human–robot touch interaction. Expert
Systems with Applications , 211:118510, 2023.
Ning Gao, Le Liang, Donghong Cai, Xiao Li, and Shi Jin. Coverage control for uav swarm communication
networks: A distributed learning approach. IEEE Internet of Things Journal , 9(20):19854–19867, 2022.
Guillaume Garrigos and Robert M Gower. Handbook of convergence theorems for (stochastic) gradient
methods. arXiv preprint arXiv:2301.11235 , 2023.
Hubert Gijzen. Big data for a sustainable future. Nature , 502(7469):38–38, 2013.
Andrew Hard, Chloe Kiddon, Daniel Ramage, Francoise Beaufays, Hubert Eichner, Kanishka Rao, Rajiv
Mathews, and Sean Augenstein. Federated learning for mobile keyboard prediction. arXiv: Computation
and Language , 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In
Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14,
2016, Proceedings, Part IV 14 , pp. 630–645. Springer, 2016b.
Reiner Horst, Panos M Pardalos, and Nguyen Van Thoai. Introduction to global optimization . Springer
Science & Business Media, 2000.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolu-
tional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.
4700–4708, 2017.
Zhanhong Jiang, Aditya Balu, Chinmay Hegde, and Soumik Sarkar. Collaborative deep learning in ﬁxed
topology networks. In Advances in Neural Information Processing Systems , pp. 5904–5914, 2017.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient
methods under the polyak-łojasiewicz condition. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases , pp. 795–811. Springer, 2016.
17Published in Transactions on Machine Learning Research (MM/YYYY)
Ahmed Khaled, Othmane Sebbouh, Nicolas Loizou, Robert M Gower, and Peter Richtárik. Uniﬁed analysis
of stochastic gradient methods for composite convex and smooth optimization. Journal of Optimization
Theory and Applications , 199(2):499–540, 2023.
Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto , 05 2012.
Guanghui Lan, Soomin Lee, and Yi Zhou. Communication-eﬃcient algorithms for decentralized and stochas-
tic optimization. Mathematical Programming , 180(1):237–284, 2020.
Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N , 7(7):3, 2015.
Jinhao Lei, Chao Liu, and Dongxiang Jiang. Fault diagnosis of wind turbine based on long short-term
memory networks. Renewable energy , 133:422–432, 2019.
Zhongguo Li, Bo Liu, and Zhengtao Ding. Consensus-based cooperative algorithms for training over dis-
tributed data sets using stochastic gradients. IEEE Transactions on Neural Networks and Learning Sys-
tems, 2021.
Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic gradient
descent. arXiv: Optimization and Control , 2017.
Jing J Liang, A Kai Qin, Ponnuthurai N Suganthan, and S Baskar. Comprehensive learning particle swarm
optimizer for global optimization of multimodal functions. IEEE transactions on evolutionary computation ,
10(3):281–295, 2006.
Xinyue Liang, Alireza M Javid, Mikael Skoglund, and Saikat Chatterjee. Asynchrounous decentralized
learning of a neural network. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) , pp. 3947–3951. IEEE, 2020.
Bo Liu, Zhengtao Ding, and Chen Lv. Distributed training for multi-layer neural networks by consensus.
IEEE transactions on neural networks and learning systems , 31(5):1771–1778, 2019.
Chao Liu, Dongxiang Jiang, and Wenguang Yang. Global geometric similarity scheme for feature selection
in fault diagnosis. Expert Systems with Applications , 41(8):3585–3595, 2014.
Xiaolan Liu and Yuanwei Liu. Distributed learning for metaverse over wireless networks. IEEE Communi-
cations Magazine , 2023.
Qinyi Luo, Jiaao He, Youwei Zhuo, and Xuehai Qian. Prague: High-performance heterogeneity-aware asyn-
chronous decentralized training. In Proceedings of the Twenty-Fifth International Conference on Architec-
tural Support for Programming Languages and Operating Systems , pp. 401–416, 2020.
Yinbin Miao, Ziteng Liu, Xinghua Li, Meng Li, Hongwei Li, Kim-Kwang Raymond Choo, and Robert H
Deng. Robust asynchronous federated learning with time-weighted and stale model aggregation. IEEE
Transactions on Dependable and Secure Computing , 2023.
Sudhanshu K Mishra. Some new test functions for global optimization and performance of repulsive particle
swarm method. Available at SSRN 926132 , 2006.
Ravi Nair and S Gupta. Wildﬁre: approximate synchronization of parameters in distributed deep learning.
Ibm Journal of Research and Development , 61(4):7, 2017.
Angelia Nedic. Distributed gradient methods for convex machine learning problems in networks: Distributed
optimization. IEEE Signal Processing Magazine , 37(3):92–101, 2020.
Andrei Patrascu and Ion Necoara. Nonasymptotic convergence of stochastic proximal point methods for
constrained convex optimization. The Journal of Machine Learning Research , 18(1):7204–7245, 2017.
Liangxin Qian, Ping Yang, Ming Xiao, Octavia A Dobre, Marco Di Renzo, Jun Li, Zhu Han, Qin Yi, and
Jiarong Zhao. Distributed learning for wireless communications: Methods, applications and challenges.
IEEE Journal of Selected Topics in Signal Processing , 16(3):326–342, 2022.
18Published in Transactions on Machine Learning Research (MM/YYYY)
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: A lock-free approach to par-
allelizing stochastic gradient descent. Advances in neural information processing systems , 24:693–701,
2011.
Alessandro Rigazzi. Dc-s3gd: Delay-compensated stale-synchronous sgd for large-scale decentralized neural
network training. arXiv: Learning , 2019.
Theo Ryﬀel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel Rueckert, and Jonathan
Passeratpalmbach. A generic framework for privacy preserving deep learning. arXiv: Learning , 2018.
Felix Sattler, Simon Wiedemann, Klaus-Robert Müller, and Wojciech Samek. Robust and communication-
eﬃcient federated learning from non-iid data. IEEE transactions on neural networks and learning systems ,
31(9):3400–3413, 2019.
Felix Sattler, Klaus-Robert Müller, and Wojciech Samek. Clustered federated learning: Model-agnostic
distributed multitask optimization under privacy constraints. IEEE Transactions on Neural Networks and
Learning Systems , 32(8):3710–3722, 2021. doi: 10.1109/TNNLS.2020.3015958.
Nikko Strom. Scalable distributed dnn training using commodity gpu cloud computing. In Sixteenth Annual
Conference of the International Speech Communication Association , 2015.
Tao Sun, Dongsheng Li, and Bao Wang. On the decentralized stochastic gradient descent with markov chain
sampling. IEEE Transactions on Signal Processing , 2023.
S Sundhar Ram, Angelia Nedić, and Venugopal V Veeravalli. Distributed stochastic subgradient projection
algorithms for convex optimization. Journal of optimization theory and applications , 147:516–545, 2010.
Mingxing Tan and Quoc Le. Eﬃcientnet: Rethinking model scaling for convolutional neural networks. In
International conference on machine learning , pp. 6105–6114. PMLR, 2019.
Konstantinos Tsianos, Sean Lawlor, and Michael G Rabbat. Communication/computation tradeoﬀs in
consensus-based distributed optimization. In Advances in neural information processing systems , pp. 1943–
1951, 2012.
Konstantinos I Tsianos and Michael G Rabbat. Eﬃcient distributed online prediction and stochastic optimiza-
tion with approximate distributed averaging. IEEE Transactions on Signal and Information Processing
over Networks , 2(4):489–506, 2016.
Jun Tu, Jia Zhou, and Donglin Ren. An asynchronous distributed training algorithm based on gossip
communication and stochastic gradient descent. Computer Communications , 195:416–423, 2022.
Abhinav Venigalla, Atli Kosson, Vitaliy Chiley, and Urs Köster. Adaptive braking for mitigating gradient
delay. arXiv preprint arXiv:2007.01397 , 2020.
Simon Wiedemann, Klaus-Robert Müller, and Wojciech Samek. Compact and computationally eﬃcient
representation of deep neural networks. IEEE transactions on neural networks and learning systems , 31
(3):772–785, 2019.
Xuyang Wu, Changxin Liu, Sindri Magnusson, and Mikael Johansson. Delay-agnostic asynchronous dis-
tributed optimization. arXiv preprint arXiv:2303.18034 , 2023.
Guojun Xiong, Gang Yan, Shiqiang Wang, and Jian Li. Straggler-resilient decentralized learning via adaptive
asynchronous updates. arXiv preprint arXiv:2306.06559 , 2023.
Yang Xu, Zhenguo Ma, Hongli Xu, Suo Chen, Jianchun Liu, and Yinxing Xue. Fedlc: Accelerating asyn-
chronous federated learning in edge computing. IEEE Transactions on Mobile Computing , 2023.
Wei Yang. pytorch-classiﬁcation. https://github.com/bearpaw/pytorch-classification , 2019. Ac-
cessed: 2019-01-24.
Maxim Zakharov. Asynchronous Consensus Algorithm. arXiv e-prints , art. arXiv:2001.07704, Jan 2020.
19Published in Transactions on Machine Learning Research (MM/YYYY)
Jinshan Zeng and Wotao Yin. On nonconvex decentralized gradient descent. IEEE Transactions on signal
processing , 66(11):2834–2848, 2018.
Tuo Zhang, Lei Gao, Sunwoo Lee, Mi Zhang, and Salman Avestimehr. Timelyﬂ: Heterogeneity-aware asyn-
chronous federated learning with adaptive partial training. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 5063–5072, 2023.
Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma, and Tie-Yan Liu. Asyn-
chronous stochastic gradient descent with delay compensation. In International Conference on Machine
Learning , pp. 4120–4129. PMLR, 2017.
Huiping Zhuang, Yi Wang, Qinglai Liu, and Zhiping Lin. Fully decoupled neural network learning using
delayed gradients. IEEE Transactions on Neural Networks and Learning Systems , 2021.
20Published in Transactions on Machine Learning Research (MM/YYYY)
A Additional Analysis
Before presenting the main results, we introduce some necessary background on the delay compensated
gradients.
A.1 Connection Between PC Steps
As discussed above, PC-ASGD relies upon the two steps to determine the updates for each agent at every
time step, as displayed in Fig. 7. We ﬁrst turn to the clipping step (line 7 of Algorithm 1) where all stale
Figure 7: Predicting-Clipping Steps: in the predicting step, blue lines indicate no delay transmission; green
lines represent delayed transmission that requires gradient prediction to reduce the stale eﬀect; in the clipping
step, the agent selectively drops the delayed information while only receiving information without delay.
information is dropped, which is equivalent to ‘clipping’ the original graph to become a smaller scale graph.
Therefore, between the predicting step and the clipping step, we can observe two static graphs switching
alternatively. This also suggests that element values of the mixing matrix ˜Win the clipping step are diﬀerent
from those in the predicting step. In the predicting step (line 6 of Algorithm 1), the agent still requires all the
information from its neighbors while asking for gradient prediction from the unreliable neighbors. However,
the update is determined by the combination of these two steps in Algorithm 1, which relies on the θvalue
to balance the tradeoﬀ. For simplicity, we set the initialization of each agent 0.
We now turn to the practical variant of PC-ASGD in Algorithm 2in the Appendix. The condition (line 9)
adopted for PC-ASGD is based on the approximate cosine value of the angle between gi(xi
t)and ∆pre(or
∆clip). When the angle between gi(xi
t)and ∆pre(or∆clip) is smaller, leading to a larger cosine value, the
corresponding step should be chosen as it enables a larger descent amount along with the direction of gi(xi
t).
Hence, with a sequence of graphs and the properly set condition, these two alternating steps are connected
to each other, allowing for convergence.
A.2 Delay compensated gradient
We detail how to arrive at Eq. 2. Speciﬁcally, given the outdated weights of agent k,xk
t−τ, due to the delay
equal to τ, by induction, we can obtain for agent k
xk
t−τ+1=xk
t−τ−ηgk(xk
t−τ)
=xk
t−τ−η0∑
r=0[gk(xk
t−τ) +λgk(xk
t−τ)⊙gk(xk
t−τ)⊙(xi
t−τ+r−xi
t−τ)](17)
xk
t−τ+2=xk
t−τ+1−ηgk(xk
t−τ+1) =xk
t−τ−ηgk(xk
t−τ)−ηgk(xk
t−τ+1)
≈xk
t−τ−η1∑
r=0[gk(xk
t−τ) +λgk(xk
t−τ)⊙gk(xk
t−τ)⊙(xi
t−τ+r−xi
t−τ)]
···(18)
xk
t≈xk
t−τ−ητ−1∑
r=0[gk(xk
t−τ) +λgk(xk
t−τ)⊙gk(xk
t−τ)⊙(xi
t−τ+r−xi
t−τ)] (19)
21Published in Transactions on Machine Learning Research (MM/YYYY)
As we mentioned in the main contents, the term (xi
t−τ+r−xi
t−τ)is from agent idue to the outdated
information of agent k, which intuitively illustrates that the compensation is driven by the agent iwhen
agent kis in its neighborhood and deemed an unreliable one.
A.3 Compact Form of PC Steps
We next brieﬂy discuss how to arrive at the compact form of the predicting and clipping steps for the analysis.
For the convenience of analysis, we set the current time step as t+τsuch that line 6 in Algorithm 1 shifts
τtime steps ahead. Let us start with the predicting step and discuss its associated term∑
j∈Rwijxj
t+τ+∑
k∈Rcwikxk
t+τ, where for the time being, it essentially holds that xk
t+τ:=xk
t. Note that Rincludes
the agents iitself. Although unreliable neighbors are outdated, in the context, the update for agent istill
requires such outdated information, which suggests that the whole graph applies. Additionally, the consensus
is performed in parallel with the local computation, so this term boils down to a similar term in the existing
consensus-based optimization algorithms in the literature. Thus, one can convert the current consensus term
for weights to∑
pwipxp
t+τ, p∈V. To show the evolution of predicting gradient over the past steps ranging
from 0 to τ−1, we use gdc,r
k(xk
t)to represent.
Hence, the update law for the predicting step can be rewritten as:
xi
t+τ+1=∑
pwipxp
t+τ−η(gk(xi
t+τ) +∑
k∈Rcwikτ−1∑
r=0gdc,r
k(xk
t)) (20)
One may argue that for those outdated agent k∈ Rc, they have no information ahead of time t, which is
τtime steps back from the current time. As the graph is undirected and connected, the time scale will not
change the connections among agents. Also, for agent i, it receives always information from other agents,
either the current or the outdated to update its weights. Thus, we have,
xp
t+τ={
xj
t+τ p=j, j∈ R
xk
t p=k, k∈ Rc (21)
Since the term∑
k∈Rcwik∑τ−1
r=0gdc,r
k(xk
t)applies to unreliable neighbors only, for the convenience of analysis,
we expand it to the whole graph. It means that we establish an expanded graph to cover all of agents by
setting some elements in the mixing matrix W′∈RN×Nequal to 0, but keeping the same connections as in
W. Then Eq. 20can be modiﬁed as
xi
t+τ+1=∑
pwipxp
t+τ−η(gk(xi
t+τ) +∑
qw′
iqτ−1∑
r=0gdc,r
k(xq
t)) (22)
where
w′
iq={wik if q =k, k∈ Rc
0 if q∈ R(23)
Thus, we know via the above setting that W′is at least a row stochastic matrix. We rewrite the update law
into a compact form such that
xt+τ+1=Wxt+τ−η(g(xt+τ) +τ−1∑
r=0W′gdc,r(xt)). (24)
where W=W⊗Id×dandW′=W′⊗Id×d. Similarly, we rewrite the clipping steps in a vector form as
follows:
xt+τ+1=˜Wxt+τ−ηg(xt+τ) (25)
where ˜W=˜W⊗Id×d. We are now ready to give the generalized step
xt+τ+1=Wt+τxt+τ−η(g(xt+τ) +θt+ττ−1∑
r=0W′gdc,r(xt)), (26)
22Published in Transactions on Machine Learning Research (MM/YYYY)
where Wt+τis denoted as θt+τW+ (1−θt+τ)˜Wthroughout the rest of the analysis. Though the original
graphs corresponding to the predicting and clipping steps are static, the equivalent graph Wt+τhas become
time-varying due to the time-varying θvalue.
A.4 Approximate Hessian Matrix
Based on the update law, we know that the key part of PC-ASGD is the delay compensated gradients using
Taylor expansion and Hessian approximation. Therefore, the Taylor expansion of the stochastic gradient
g(xt+τ)atxtcan be written as follows:
g(xt+τ) =g(xt) +∇g(xt)(xt+τ−xt) +O((xt+τ−xt)2)I, (27)
where ∇gdenotes the matrix with the element ∇gij=∂F
∂xi∂xjfor all i, j∈V.
In most asynchronous SGD works, they used the zero-order item in Taylor expansion as its approximation
tog(xt+τ)by ignoring the higher order term. Following from Zheng et al. (2017), we have
g(xt+τ)≈g(xt) +∇g(xt)(xt+τ−xt), (28)
Directly adopting the above equation would be diﬃcult in practice since ∇g(xt)is generically computationally
intractable when the model is very large, such as deep neural networks. To make the delay compensated
gradients in PC-ASGD technically feasible, we apply approximation techniques for the Hessian matrix. We
ﬁrst use O(xt)to denote the outer product matrix of the gradient at xt, i.e.,
O(xt) = (∂
∂xtF(xt))(∂
∂xtF(xt))T(29)
When the objective functions take the form of the cross-entropy loss or negative log-likelihood, the outer
product of the gradient is an asymptotically unbiased estimation of the Hessian, according to the two
equivalent methods to calculate the Fisher information matrix Friedman et al. (2001). That is,
ϵt=E[∥O(xt)−H(xt)∥]→0, t→0 (30)
where H(xt)is the Hessian matrix of Fat point xt.
The above equivalence relies on assumptions that the underlying distribution equals the model distribution
with parameter x∗and that the training model xtasymptotically converges to the (globally or locally)
optimal model x∗. According to the universal approximation theorem for DNN and some recent results on
the optimality of the local optimal, such assumptions are technically reasonable. As the above equivalence
was only developed by the negative log-likelihood form, that may not be applicable when we use PC-ASGD
for the mean square error form, such as some time-series predictions with LSTM networks. Therefore, we
introduce one assumption on the top of such an equivalence as follows,
E[∥O(xt)−H(xt)∥]≤ϵ∃ϵ >0 (31)
which primarily eliminates the computational complexity when directly calculating H(xt). Another concern
would be the large variance probably caused by O(xt), though it is an unbiased estimation of H(xt). Similar
toZheng et al. (2017), we introduce a new approximator λO(xt)≜λ(∂
∂xtF(xt))(∂
∂xtF(xt))T. The authors
inZheng et al. (2017) have proved that λO(xt)is able to lead to smaller variance during training. Thus we
refer interested readers to Zheng et al. (2017) for more details.
To reduce the storage of the approximator λO(xt), one widely-used diagonalization trick is adopted Becker
& Lecun (1989). Hence, in the update law for PC-ASGD, we can see in the delay compensated gradient
involving λg(xt)⊙λg(xt). By denoting the diagonalized approximator as Diag (λO(xt)), the following
relationship is obtained:
Diag (λO(xt)) =λg(xt)⊙λg(xt) (32)
23Published in Transactions on Machine Learning Research (MM/YYYY)
However, for analysis, when we apply diagonalization to H(xt), it could cause diagonalization error such
that we assume that the error is upper bounded by a constant ϵD>0, i.e.,
∥Diag (H(xt))−H(xt)∥ ≤ϵD (33)
B Additional Proof
For completeness, when presenting proof, we re-present statements for all lemmas and theorems.
Lemma 3 : The iterates generated by PC-ASGD satisfy ∀t≥0, and τ≥2:
xt+τ=t+τ−1∏
v=0Wt+τ−1−vx0−ηt+τ−1∑
s=0t+τ−1∏
v=s+1Wt+τ+s−vg(xs)−ηt+τ−1∑
s=tt+τ−1∏
v=s+1θs+1Wt+τ+s−vτ−2∑
r=0W′g(xs+1).
(34)
Proof. Based on the vector form of the update law, we obtain
xt+τ=Wt+τ−1xt+τ−1−η(g(xt+τ−1) +θt+τ−1τ−2∑
r=0W′gdc,r(xt)) (35)
With the above equation, it can be observed that xt+τis a function with respect to xt, which contains all of
agents. This suggests that by xt, there were no delay compensated gradients, while after xt+1, the unreliable
neighbors need the delay compensated gradients due to delay. Hence, applying the above equation from 0
tot+τ−1yields the desired result.
Bounded (stochastic) gradient assumption : AsE[∥g(x)∥2]≤G2andE[g(x)] =∇F(x), one can get
that∥∇F(x)∥=∥E[g(x)]∥ ≤E[∥g(x)∥] =√
(E[∥g(x)∥])2≤√
E[∥g(x)∥2] =G.
Lemma 1 : Let Assumptions 2 and 3 hold. Assume that the delay compensated gradients are uniformly
bounded, i.e., there exists a scalar B > 0, such that
∥gdc,r(xt)∥ ≤B,∀t≥0and 0≤r≤τ−1, (36)
Then for all i∈Vandt≥0,∃η >0, we have
E[∥xi
t−yt∥]≤ηG+ (τ−1)Bθm
1−δ2, (37)
where θm=max{θs+1}t+τ−1
s=t,δ2=max{θse2+ (1−θs)˜e2}t+τ−1
s=0 <1, where e2:=e2(W)<1and ˜e2:=
e2(˜W)<1.
Proof. Since
∥xi
t+τ−yt+τ∥ ≤ ∥ xt+τ−yt+τ1∥=∥xt+τ−1
N1Txt+τ1∥
=∥xt+τ−1
N11Txt+τ∥=∥(I−1
N11T)xt+τ∥,(38)
24Published in Transactions on Machine Learning Research (MM/YYYY)
where 1is the column vector with entries all being 1. According to Assumption 2, we have1
N11TW=1
N11T.
Hence, by induction, setting x0= 0, and Lemma 3, the following relationship can be obtained
∥xt+τ−yt+τ1∥
=η∥t+τ−1∑
s=0(t+τ−1∏
v=s+1Wt+τ+s−v−1
N11T)g(xs) +t+τ−1∑
s=t(t+τ−1∏
v=s+1Wt+τ+s−v−1
N11T)θs+1τ−2∑
r=0W′gdc,r(xt)∥
≤ηt+τ−1∑
s=0∥t+τ−1∏
v=s+1Wt+τ+s−v−1
N11T∥∥g(xs)∥+ηt+τ−1∑
s=t∥t+τ−1∏
v=s+1Wt+τ+s−v−1
N11T∥∥θs+1τ−2∑
r=0W′gdc,r(xt)∥
≤ηGt+τ−1∑
s=0δt+τ−1−s
2 +ηt+τ−1∑
s=tδt+τ−1−s
2 θs+1(τ−1)B
≤ηG1
1−δ2+η(τ−1)Bθmδt
2−δt+τ−1
2
1−δ2
≤ηG+ (τ−1)Bθm
1−δ2.
(39)
The second inequality follows from the Triangle inequality and Cauthy-Schwartz inequality and the third
inequality follows from Assumption 2and that the matrix1
N11Tis the projection of Wonto the eigenspace
associated with the eigenvalue equal to 1. The last inequality follows from the property of geometric sequence.
The proof is completed by replacing t+τwith ton the left hand side.
To prove the main results, we present several auxiliary lemmas ﬁrst. We deﬁne
Gh(xt) =τ−1∑
r=0g(xt+r) +H(xt)(vt+r−xt)
∇Fh(xt) =τ−1∑
r=0∇F(xt+r) +E[H(xt)(vt+r−xt)](40)
which are the incrementally delay compensated gradient and its expectation, respectively. It can be ob-
served that Gh(xt)is the unbiased estimator of ∇Fh(xt). It should be noted that H(xt) =∇g(xt).
Letvt+τ=Wt+τxt+τ. We next present a lemma to upper bound ∥∇F(vt+r)− ∇Fh,r(xt)∥, where
∇Fh,r(xt) =∇F(xt+r) +E[H(xt)(vt+r−xt)].
Lemma 4 : Let Assumptions 1,2 and 3 hold. Assume that ∇F(xt)isξm-smooth. For t≥0, the iterates
generated by PC-ASGD satisfy the following relationship, when r≥1
∥∇F(vt+r)− ∇Fh,r(xt)∥ ≤ξm
2η2(2G+ (r−1)Bθm
1−δ2)2; (41)
when r= 0, we have
∥∇F(vt)− ∇F(xt)∥ ≤2γmη(G+ (τ−1)Bθm)
1−δ2. (42)
Proof. By the smoothness condition for ∇F(x), we have
∥∇F(vt+r)− ∇Fh,r(xt)∥ ≤ξm
2∥vt+r−xt∥2≤ξm
2∥xt+r−xt∥2(43)
Let∆t+r=xt+r−xt. Thus, based on Lemma 1, we have
xt+r=t+r−1∏
v=tWt+r−1−vxt−ηt+r−1∑
s=tt+r−1∏
v=s+1Wt+r+s−vg(xs)−ηt+r−1∑
s=tt+r−1∏
v=s+1Wt+s+r−vr−2∑
z=0θs+1W′gdc,z(xs+1−r)
(44)
25Published in Transactions on Machine Learning Research (MM/YYYY)
Hence, we can obtain
∥∆t+r∥2=∥(t+r−1∏
v=tWt+r−1−v−I)xt−ηt+r−1∑
s=tt+r−1∏
v=s+1Wt+r+s−vg(xs)−ηt+r−1∑
s=tt+r−1∏
v=s+1Wt+s+r−vr−2∑
z=0θs+1W′gdc,z(xs+1−r)∥2
(45)
Due to x0= 0and no delay compensated gradients before time step t, we can obtain
∥∆t+r∥2
=∥ −ηt+r−1∑
s=0t+r−1∏
v=s+1Wt+r+s−vg(xs)−ηt+r−1∑
s=tt+r−1∏
v=s+1Wt+s+r−vr−2∑
z=0θs+1W′gdc,z(xs+1−r) +ηt∑
s=0t∏
v=sWt+s−vg(xs)∥2
≤η2(∥t+r−1∑
s=0t+r−1∏
v=s+1Wt+r+s−vg(xs)∥+∥t+r−1∑
s=tt+r−1∏
v=s+1Wt+s+r−vr−2∑
z=0θs+1W′gdc,z(xs+1−r)∥+∥t∑
s=0t∏
v=sWt+s−vg(xs)∥)2
≤η2(t+r−1∑
s=0∥t+r−1∏
v=s+1Wt+r+s−vg(xs)∥+t+r−1∑
s=t∥t+r−1∏
v=s+1Wt+s+r−vr−2∑
z=0θs+1W′gdc,z(xs+1−r)∥+t∑
s=0∥t∏
v=sWt+s−vg(xs)∥)2
≤η2(t+r−1∑
s=0t+r−1∏
v=s+1∥Wt+r+s−v∥∥g(xs)∥+t+r−1∑
s=tt+r−1∏
v=s+1∥Wt+s+r−v∥∥r−2∑
z=0θs+1W′gdc,z(xs+1−r)∥
+t∑
s=0t∏
v=s∥Wt+s−v∥∥g(xs)∥)2
≤η2(2G
1−δ2+1
1−δ2B(r−1)θm)2
≤η2(2G+θm(r−1)B
1−δ2)2
(46)
The ﬁrst inequality follows from the Triangle inequality. The second inequality follows from the Jensen
inequality. The third inequality follows from the Cauthy-Schwartz inequality and the submultiplicative
matrix norm applied to stochastic matrices. The fourth inequality follows from the Assumption 2and
bounded gradient. We have observed that this holds when r≥1. While r= 0enables ∥∇F(vt+r)−Fh,r(xt)∥
to degenerate to ∥∇F(vt)− ∇F(xt)∥based on the deﬁnition of Fh(xt). Using the smoothness condition of
F(x), we can immediately obtain
∥∇F(vt)− ∇F(xt)∥ ≤2γmηG+ (τ−1)Bθm
1−δ2. (47)
The proof is completed.
Lemma 5 : Let Assumptions 1, 2 and 3 hold. Assume that the delay compensated gradients are uniformly
bounded, i.e., there exists a scalar B > 0such that
∥gdc,r(xt)∥ ≤B,∀t≥0and 0≤r≤τ−1, (48)
Then for the iterates generated by PC-ASGD, ∃η >0, they satisfy
∥E[Gh(xt)]−τ−1∑
r=0W′gdc,r(xt)∥
≤τ−1∑
r=1(γm+ϵD+ϵ+ (1−λ)G2)η2G+ (r−1)Bθm
1−δ2+τσ(49)
26Published in Transactions on Machine Learning Research (MM/YYYY)
Proof. Based on the deﬁnition of EGh(xt), we have
∥E[Gh(xt)]−τ−1∑
r=0W′gdc,r(xt)∥=∥E[τ−1∑
r=0g(xt+r) +τ−1∑
r=0H(xt)(xt+r−xt)]−τ−1∑
r=0W′gdc,r(xt)∥
=∥E[Gh,r=0(xt)]−W′gdc,r=0(xt) +E[Gh,r=1(xt)]−W′gdc,r=1(xt) +···+E[Gh,r=τ−1(xt)]−W′gdc,r=τ−1(xt)∥
≤∥E[Gh,r=0(xt)]−W′gdc,r=0(xt)∥+∥E[Gh,r=1(xt)]−W′gdc,r=1(xt)∥+···+∥E[Gh,r=τ−1(xt)]−W′gdc,r=τ−1(xt)∥
(50)
The last inequality follows from the Triangle inequality. Now let us discuss ∥EGh,r(xt)−W′gdc,r(xt)∥. The
following analysis is for cases where r≥1. We give a brief analysis for the case in which r= 0subsequently.
∥E[Gh(xt)]−W′gdc,r(xt)∥
=∥E[g(xt+r) +H(xt)(xt+r−xt)]W′[g(xt) +λg(xt)⊙g(xt)⊙(xt+r−xt)]∥
=∥∇F(xt+r)−W′g(xt) + [H(xt)−λW′g(xt)⊙g(xt)](xt+r−xt)∥
≤∥∇F(xt+r)−W′g(xt)∥+∥[H(xt)−λW′g(xt)⊙g(xt)](xt+r−xt)∥
≤∥∇F(xt+r)−W′g(xt)∥+∥[H(xt)−λW′g(xt)⊙g(xt) +g(xt)⊙g(xt)−g(xt)⊙g(xt)
−Diag (H(xt)) +Diag (H(xt))](xt+r−xt)∥
≤∥∇F(xt+r)−W′g(xt)∥+∥xt+r−xt∥∥(λW′g(xt)⊙g(xt)−g(xt)⊙g(xt)) + ( g(xt)⊙g(xt)
−Diag (H(xt))) + ( Diag (H(xt))−H(xt))∥
≤∥∇F(xt+r)−W′g(xt)∥+∥xt+r−xt∥(∥λW′g(xt)⊙g(xt)−g(xt)⊙g(xt)∥+∥g(xt)⊙g(xt)
−Diag (H(xt))∥+∥Diag (H(xt))−H(xt)∥)
The third inequality follows from Cauthy-Schwarz inequality while the last one follows from the Triangle
inequality. It should be noted that when we combine H(xt)(xt+r−xt)andλW′g(xt)⊙g(xt)⊙(xt+r−xt),
we follow the update law. Since in a rigorously mathematical sense, g(xt)⊙g(xt)should be g(xt)g(xt)T.
However, for reducing the computational complexity when implementing the algorithm, as discussed above,
we have made the approximation and diagonalization trick. Hence, we assume that H(xt)−λW′g(xt)⊙g(xt)
can hold for simplicity and convenience.
Then we discuss E[∥∇F(xt+r)−W′g(xt)∥].
E[∥∇F(xt+r)−W′g(xt)∥]≤E[∥∇F(xt+r)−g(xt)∥]
=E[∥∇F(xt+r)− ∇F(xt) +∇F(xt)−g(xt)∥]
≤E[∥∇F(xt+r)− ∇F(xt)∥] +E[∥∇F(xt)−g(xt)∥]
≤γm∥xt+r−xt∥+√
(E[∥∇F(xt)−g(xt)∥])2
≤γmη2G+ (r−1)Bθm
1−δ2+√
E[∥∇F(xt)−g(xt)∥]2
≤γmη2G+ (r−1)Bθm
1−δ2+σ(51)
Hence, we have
∥E[Gh(xt)]−τ−1∑
r=0W′gdc,r(xt)∥ ≤γmη2G+ (r−1)Bθm
1−δ2+ [(1−λ)G2+ϵD+ϵ]η2G+ (r−1)Bθm
1−δ2+σ
=(γm+ϵD+ϵ+ (1−λ)G2)η2G+ (r−1)Bθm
1−δ2+σ(52)
The above relationship is obtained for cases where r≥1. There still is r= 0left. For r= 0,
∥∇F(xt)−W′g(xt)∥ ≤σ (53)
27Published in Transactions on Machine Learning Research (MM/YYYY)
Thus, combining each upper bound for ∥E[Gh,r(xt)]−W′gdc,r(xt)∥, we can obtain
∥E[Gh(xt)]−τ−1∑
r=0W′gdc,r(xt)∥ ≤τ−1∑
r=1(γm+ϵD+ϵ+ (1−λ)G2)η2G+ (r−1)Bθm
1−δ2+τσ, (54)
which completes the proof.
Lemma 6 : Let Assumptions 1, 2 and 3 hold. Assume that the delay compensated gradients are uniformly
bounded, i.e., there exists a scalar B > 0such that
∥gdc,r(xt)∥ ≤B,∀t≥0and 0≤r≤τ−1, (55)
Then for the iterates generated by PC-ASGD, ∃η >0, they satisfy
F(xt+τ)≥F(vt+τ)−2GηG+ (τ−1)Bθm
1−δ2(56)
Proof. Due to the convexity, we have
F(xt+τ)≥F(vt+τ) +∇F(vt+τ)(xt+τ−vt+τ)
≥F(vt+τ)− ∥∇ F(vt+τ)∥∥vt+τ−xt+τ∥
≥F(vt+τ)−G∥vt+τ−xt+τ∥
≥F(vt+τ)−G∥vt+τ−yt+τ1+yt+τ1−xt+τ∥
≥F(vt+τ)−G(∥vt+τ−yt+τ1∥+∥yt+τ1−xt+τ∥)
≥F(vt+τ)−2GηG+ (τ−1)Bθm
1−δ2(57)
The second inequality follows from the Cauthy-Schwarz inequality. The proof is completed.
Theorem 1: Let Assumptions 1,2 and 3 hold. Assume that the delay compensated gradients are uniformly
bounded, i.e., there exists a scalar B > 0such that
∥gdc,r(xt)∥ ≤B,∀t≥0and 0≤r≤τ−1, (58)
and that ∇F(xt)isξm-smooth for all t≥0. Then for the iterates generated by PC-ASGD, when 0< η≤1
2µτ
and the objective satisﬁes the PL condition, they satisfy
E[F(xt)−F∗]≤(1−2µητ)t−1(F(x1)−F∗−Q
2µητ) +Q
2µητ, (59)
Q= 2(1 −2µητ)GηC 1+η3ξmG
2τ−1∑
r=1Cr+ 2η2GγmC1
+Gητσ +η2G(γm+ϵD+ϵ+ (1−λ)G2)τ−1∑
r=1Cr+ηG2+η2γmGτC 2(60)
and,
C1=G+ (τ−1)Bθm
1−δ2
Cr=2G+ (r−1)Bθm
1−δ2
C2=2G+ (τ−1)Bθm
1−δ2,(61)
ϵD>0andϵ >0are upper bounds for the approximation errors of the Hessian matrix.
28Published in Transactions on Machine Learning Research (MM/YYYY)
Proof. According to the smoothness condition of F(x). We have
E[F(xt+τ+1)−F(x∗)]≤E[F(vt+τ)−F(x∗)] +E[⟨∇F(vt+τ),(xt+τ+1−vt+τ)⟩] +γm
2E[∥xt+τ+1−vt+τ∥2]
(62)
Based on the update law, we can obtain
E[F(xt+τ+1)−F(x∗)]
≤E[F(vt+τ)−F∗]−ηE[⟨∇F(vt+τ),g(xt+τ)⟩]−ηE[⟨∇F(vt+τ),τ−1∑
r=0W′gdc,r(xt)⟩]
+γmη2
2E[∥g(xt+τ) +τ−1∑
r=0W′gdc,r(xt)∥2]
≤E[F(vt+τ)−F∗]−ηE[⟨∇F(vt+τ),g(xt+τ)⟩]−ηE[⟨∇F(vt+τ), τ∇F(vt+τ)⟩]
+ηE[⟨∇F(vt+τ), τ∇F(vt+τ)−τ−1∑
r=0∇F(vt+r)⟩] +ηE[⟨∇F(vt+τ),τ−1∑
r=0∇F(vt+r)− Fh(xt)⟩]
+ηE[⟨∇F(vt+τ),E[Gh]−τ−1∑
r=0W′gdc,r(xt)⟩] +γmη2
2E[∥g(xt+τ) +τ−1∑
r=0W′gdc,r(xt)∥2](63)
We next investigate each term on the right hand side. Based on Lemma 6, we can obtain
F(xt+τ)≥F(vt+τ)−2GηG+ (τ−1)Bθm
1−δ2(64)
such that
F(xt+τ)−F∗≥F(vt+τ)−F∗−2GηG+ (τ−1)Bθm
1−δ2(65)
For the term −ηE[⟨∇F(vt+τ), g(xt+τ)⟩], we can quickly get that is is bounded above by ηG2due to the
Cauthy-Schwarz inequality. Then for term −ηE[⟨∇F(vt+τ), τ∇F(vt+τ)⟩], one can get the following relation-
ship due to the PL condition.
−ηE[⟨∇F(vt+τ), τ∇F(vt+τ)⟩]≤ −2ητµ(F(vt+τ)−F∗) (66)
Combining F(vt+τ)−F∗, we have
(1−2ητµ)(F(vt+τ)−F∗)
≤(1−2ητµ)[(F(xt+τ)−F∗) + 2GηG+ (τ−1)Bθm
1−δ2](67)
Based on Lemma 4, we have known that
∥∇F(vt+r)− ∇Fh,r(xt)∥ ≤ξm
2η2[2G+ (r−1)Bθm
1−δ2]2; (68)
forr≥1, while for r= 0, it can be obtained that
∥∇F(vt)− ∇F(xt)∥ ≤2γmηG+ (τ−1)Bθm
1−δ2. (69)
Since
ηE[⟨∇F(vt+τ),τ−1∑
r=0∇F(vt+r)− Fh(xt)⟩]≤ηE[∥∇F(vt+τ)∥∥τ−1∑
r=0∇F(vt+r)− Fh(xt)∥]
≤E[∥∇F(vt+τ)∥τ−1∑
r=0∥∇F(vt+r)− Fh(xt)∥](70)
29Published in Transactions on Machine Learning Research (MM/YYYY)
The ﬁrst inequality follows from Cauthy-Schwarz inequality and the second one follows from Triangle in-
equality. Hence, we can have
ηE[⟨∇F(vt+τ),τ−1∑
r=0∇F(vt+r)− Fh(xt)⟩]≤η3ξmG
2(1−δ2)τ−1∑
r=1[2G+B(r−1)θm] + 2η2GγmG+ (τ−1)Bθm
1−δ2
(71)
According to Lemma 4, the following relationship can be obtained,
E[⟨∇F(vt+τ),E[Gh(xt)]−τ−1∑
r=0W′gdc,r(xt)⟩]≤η2G
1−δ2(γm+ϵD+ϵ+ (1−λ)G2)τ−1∑
r=1[2G+ (r−1)Bθm] +Gητσ
(72)
The last term is ηE[⟨∇F(vt+τ), τ∇F(vt+τ)−∑τ−1
r=0∇F(vt+r)⟩], which can be rewritten such that
ηE[⟨∇F(vt+τ), τ∇F(vt+τ)−τ−1∑
r=0∇F(vt+r)⟩]
≤ηE[∥∇F(vt+τ)∥∥∇F(vt+τ)− ∇F(vt) +···+∇F(vt+τ)− ∇F(vt+τ−1)∥]
≤ηE[∥∇F(vt+τ)∥∥∇F(vt+τ)− ∇F(vt)∥+···+∥∇F(vt+τ)− ∇F(xt+τ−1)∥](73)
Using the smoothness condition, we then can bound the term by deriving the following relationship with
Lemma 1 and Lemma 3,
ηE[⟨∇F(vt+τ), τ∇F(vt+τ)−τ−1∑
r=0∇F(vt+r)⟩]≤η2γmGτ2G+ (τ−1)Bθm
1−δ2(74)
We combine the upper bounds of each term on the right hand side to produce the following relationship.
E[F(xt+τ+1)−F(x∗)]≤(1−2ηµτ)(F(xt+τ)−F∗) + 2(1 −2ηµτ)GηG+ (τ−1)Bθm
1−δ2
+η3ξmG
2(1−δ2)τ−1∑
r=1[2G+ (r−1)Bθm] + 2η2GγmG+ (τ−1)Bθm
1−δ2+Gητσ +ηG2
+η2G
1−δ2(γm+ϵD+ϵ+ (1−λ)G2)τ−1∑
r=1[2G+ (r−1)Bθm] +η2γmGτ2G+ (τ−1)Bθm
1−δ2.
(75)
We now know that
E[F(xt+1)−F∗]≤(1−2ητµ)E[F(xt)−F∗] +Q, (76)
subtracting the constantQ
2µτηfrom both sides, one obtains
E[F(xt+1)−F∗]−Q
2µτη≤(1−2ηµτ)E[F(xt)−F∗] +Q−Q
2µτη
= (1−2ηµτ)(E[F(xt)−F∗]−Q
2µτη)(77)
Observe that the above inequality is a contraction inequality since 0<2ηµτ≤1due to 0< η≤1
2µτ. The
result thus follows by applying the inequality repeatedly through iteration t∈N.
Another scenario that could be of interest is the strongly convex objective. As Theorem 1has shown with a
properly set constant step size, PC-ASGD is able to converge to the neighborhood of the optimal solution
30Published in Transactions on Machine Learning Research (MM/YYYY)
with a linear rate. This also applies to the strongly convex objective in which the strong convexity implies
the PL condition, while the constants are subject to changes. We now proceed to give the proof for the
generally convex case.
Theorem 2: Let Assumptions 1, 2 and 3 hold. Assume that the delay compensated gradients are uniformly
bounded, i.e., there exists a scalar B > 0such that for all T≥1
∥gdc,r(xt)∥ ≤B,∀t≥0and 0≤r≤τ−1, (78)
and there exists C > 0,
E[∥xt−x∗∥]≤C, (79)
where x∗∈argmin F(x). Then for the iterations generated by PC-ASGD, there exists 0< η <1
20γm, such
that
E[F(¯xT)−F∗]≤∥x1−x∗∥2
Tη+A
η, (80)
where A= 10η2σ2
∗+10η2σ2+20η4G2C2
1+5η2θ2
mτ2B2+2ηCθ mτB+2Gη2C1(2C+1), C1=G+(τ−1)Bθm
1−δ2, σ2
∗:=
E∥g(x∗)− ∇F(x∗)∥2,¯xT:=1
T∑T
t=1xt.
Proof. According to the compact update law, we have
∥xt+τ+1−x∗∥2=∥Wt+τxt+τ−η(g(xt+τ) +θt+ττ−1∑
r=0W′gdc,r(xt))−x∗∥2. (81)
Asvt+τ=Wt+τxt+τ, we can obtain
∥xt+τ+1−x∗∥2=∥vt+τ−x∗∥2−2η⟨vt+τ−x∗,g(xt+τ) +θt+ττ−1∑
r=0W′gdc,r(xt)⟩
+η2∥g(xt+τ) +θt+ττ−1∑
r=0W′gdc,r(xt)∥2.(82)
For convenience, we deﬁne that Γt+τ=g(xt+τ) +θt+τ∑τ−1
r=0W′gdc,r(xt). Hence, the above equation can be
rewritten as
∥xt+τ+1−x∗∥2=∥vt+τ−x∗∥2+η2∥Γt+τ∥2
+ 2η⟨x∗−vt+τ,g(vt+τ)⟩+ 2η⟨x∗−vt+τ,g(xt+τ)−g(vt+τ)⟩
+ 2η⟨x∗−vt+τ, θt+ττ−1∑
r=0W′gdc,r(xt)⟩.(83)
Taking expectation on both sides leads to the following relationship:
E[∥xt+τ+1−x∗∥2]≤E[∥xt+τ−x∗∥2] +η2E[∥Γt+τ∥2]
+ 2ηE[⟨x∗−vt+τ,∇F(vt+τ)⟩] + 2ηE[⟨x∗−vt+τ,∇F(xt+τ)− ∇F(vt+τ)⟩]
+ 2ηE[⟨x∗−vt+τ, θt+ττ−1∑
r=0W′gdc,r(xt)⟩].(84)
The inequality holds due to the basic property for the projection Sundhar Ram et al. (2010). For the last two
terms on the right hand side of the above inequality, we can leverage Cauchy-Schwartz inequality to obtain
the upper bounds. For 2ηE[⟨x∗−vt+τ,∇F(vt+τ)⟩], we will use Lemma 2 to reformulate. We next investigate
η2E[∥Γt+τ∥2]. Before that, we introduce a theoretical fact for the generally convex smooth functions.
Variance transfer: gradient noise (Lemma 4.20) in Garrigos & Gower (2023). IfFis smooth and convex,
then for all xwe have that
E[∥g(x)∥2]≤4γm(F(x)−F∗) + 2σ2
∗, (85)
31Published in Transactions on Machine Learning Research (MM/YYYY)
where g(x)is the stochastic gradient, σ2
∗is the variance of stochastic gradient at x∗. Rewrite ∥Γt+τ∥2=
∥g(xt+τ) +∇F(xt+τ)−∇F(xt+τ) +g(vt+τ)−g(vt+τ) +∇F(vt+τ)−∇F(vt+τ) +θt+τ∑τ−1
r=0W′gdc,r(xt)∥2.
We then have the following relationship:
E[∥xt+τ+1−x∗∥2]≤E[∥xt+τ−x∗∥2] + 5η2E[∥g(vt+τ)∥2] + 5η2E[∥g(xt+τ)− ∇F(xt+τ)∥2]
+ 5η2E[∥g(vt+τ)− ∇F(vt+τ)∥2] + 5η2E[∥∇F(xt+τ)− ∇F(vt+τ)∥2]
+ 5η2E[∥θt+ττ−1∑
r=0W′gdc,r(xt)∥2] + 2ηE[F∗−F(vt+τ)]
+ 2ηE[∥x∗−vt+τ∥∥∇F(xt+τ)− ∇F(vt+τ)∥] + 2ηE[∥x∗−vt+τ∥∥θt+ττ−1∑
r=0W′gdc,r(xt)∥].
(86)
The last inequality holds due to the basic inequality ∥∑N
i=1ai∥2≤N∑N
i=1∥ai∥2, the convexity property,
and Cauchy-Schwartz inequality. By substituting Eq. 85into Eq. 86, the following relationship can be
obtained
E[∥xt+τ+1−x∗∥2]≤E[∥xt+τ−x∗∥2] + 20 η2γmE[F(vt+τ)−F∗] + 10 η2σ2
∗
+ 5η2E[∥g(xt+τ)− ∇F(xt+τ)∥2]
+ 5η2E[∥g(vt+τ)− ∇F(vt+τ)∥2] + 5η2E[∥∇F(xt+τ)− ∇F(vt+τ)∥2]
+ 5η2E[∥θt+ττ−1∑
r=0W′gdc,r(xt)∥2] + 2ηE[F∗−F(vt+τ)]
+ 2ηE[∥x∗−vt+τ∥∥∇F(xt+τ)− ∇F(vt+τ)∥] + 2ηE[∥x∗−vt+τ∥∥θt+ττ−1∑
r=0W′gdc,r(xt)∥]
≤E[∥xt+τ−x∗∥2] + 2η(10γmη−1)E[F(xt+τ)−F∗] + 10 η2σ2
∗
+ 10η2σ2+ 20η4G2C2
1+ 5η2θ2
mτ2B2+ 2ηC(2GηC 1+θmτB).
(87)
The second inequality follows from Assumption 3, Eq. 47and bounds for the predicted gradients. With
mathematical manipulation, the above inequality can be written as
2η(1−10γmη)E[F(vt+τ)−F∗]≤E[∥xt+τ−x∗∥2]−E[∥xt+τ+1−x∗∥2]
+ 10η2σ2
∗+ 10η2σ2+ 20η4G2C2
1+ 5η2θ2
mτ2B2+ 2ηC(2GηC 1+θmτB)
(88)
Due to η≤1
20γm,1−10γmη≥1
2such that ηE[F(vt+τ)−F∗]≤2η(1−10γmη)E[F(vt+τ)−F∗]. Dividing
both sides of Eq. 88byηyields the following
E[F(vt+τ)−F∗]≤1
η(E[∥xt+τ−x∗∥2]−E[∥xt+τ+1−x∗∥2])
+1
η(10η2σ2
∗+ 10η2σ2+ 20η4G2C2
1+ 5η2θ2
mτ2B2+ 2ηC(2GηC 1+θmτB)).(89)
Similar to Lemma 6, we can obtain that F(vt+τ)≥F(xt+τ)−2GηG+(τ−1)Bθm
1−δ2. Then it is immediately
obtained that F(vt+τ)−F∗≥F(xt+τ)−F∗−2GηG+(τ−1)Bθm
1−δ2. With this, the following relationship can
be obtained
E[F(xt+τ)−F∗]≤1
η(E[∥xt+τ−x∗∥2]−E[∥xt+τ+1−x∗∥2]) +A
η, (90)
where A= 10η2σ2
∗+ 10η2σ2+ 20η4G2C2
1+ 5η2θ2
mτ2B2+ 2ηCθ mτB+ 2Gη2C1(2C+ 1). Recursively summing
overtfrom 1 to Tand replacing t+τwith tgrants us the following relationship
T∑
t=1E[F(xt)−F∗]≤∥x1−x∗∥2
η+AT
η. (91)
32Published in Transactions on Machine Learning Research (MM/YYYY)
Dividing both sides by Tin the last relationship attains the following
1
TT∑
t=1E[F(xt)−F∗]≤∥x1−x∗∥2
Tη+A
η. (92)
Using that Fis convex with Jensen inequality gives the desirable result.
In the sequel, we provide the details for the smooth nonconvex functions.
Theorem 3: Let Assumptions 1,2 and 3 hold. Assume that the delay compensated gradients are uniformly
bounded, i.e., there exists a scalar B > 0such that
∥gdc,r(xt)∥ ≤B,∀t≥0and 0≤r≤τ−1, (93)
and that
E[∥gdc(xt)∥2]≤M. (94)
Then for the iterates generated by PC-ASGD, there exists 0< η <1
γm, such that for all T≥1,
1
TT∑
t=1E[∥∇F(xt)∥2]≤2(F(x1)−F∗)
Tη+R
η, (95)
where
R= 2Gη2C1+τη2γmM
2+ησ2
2+ηστB + 2η2γm(τB+G)C1.
Proof. According to the smoothness condition of F(x), we have
F(xt+τ+1)−F(vt+τ)
≤⟨∇F(vt+τ),xt+τ+1−vt+τ⟩+γm
2+∥xt+τ+1−vt+τ∥2
=⟨∇F(vt+τ),−η(τ−1∑
r=0W′gdc,r(xt) +g(xt+τ))⟩+η2γm
2∥τ−1∑
r=0W′gdc,r+g(xt+τ)∥2
=⟨∇F(vt+τ)− ∇F(xt+τ) +∇F(xt+τ), η(τ−1∑
r=0W′gdc,r(xt) +g(xt+τ))⟩+η2γm
2∥τ−1∑
r=0W′gdc,r+g(xt+τ)∥2
=−η⟨∇F(xt+τ),τ−1∑
r=0W′gdc,r(xt) +g(xt+τ)⟩+η⟨(∇F(vt+τ)− ∇F(xt+τ),τ−1∑
r=0W′gdc,r(xt) +g(xt+τ))⟩
+η2γm
2∥τ−1∑
r=0W′gdc,r+g(xt+τ)∥2
=−η
2[∥∇F(xt+τ)∥2+∥τ−1∑
r=0W′gdc,r(xt) +g(xt+τ)∥2− ∥∇ F(xt+τ)−(τ−1∑
r=0W′gdc,r(xt) +g(xt+τ))∥2]
+η⟨∇F(xt+τ)− ∇F(vt+τ),τ−1∑
r=0W′gdc,r(xt) +g(xt+τ)⟩+η2γm
2∥τ−1∑
r=0W′gdc,r+g(xt+τ)∥2
=−η
2∥∇F(xt+τ)∥2−η
2∥τ−1∑
r=0W′gdc,r(xt) +g(xt+τ)∥2+η
2(∥∇F(xt+τ)−g(xt+τ)∥2+∥τ−1∑
r=0W′gdc,r(xt)∥2
−2⟨∇F(xt+τ)−g(xt+τ),τ−1∑
r=0W′gdc,r(xt)⟩) +η⟨∇F(xt+τ)− ∇F(vt+τ),τ−1∑
r=0W′gdc,r(xt) +g(xt+τ)⟩
33Published in Transactions on Machine Learning Research (MM/YYYY)
+η2γm
2∥τ−1∑
r=0W′gdc,r+g(xt+τ)∥2
=−η
2∥∇F(xt+τ)∥2−(η
2−η2γm
2)∥τ−1∑
r=0W′gdc,r(xt) +g(xt+τ)∥2+η
2∥∇F(xt+τ)−g(xt+τ)∥2+η
2∥τ−1∑
r=1W′gdc,r(xt)∥2
−η⟨∇F(xt+τ)−g(xt+τ),τ−1∑
r=1W′gdc,r(xt)⟩+η⟨∇F(xt+τ)− ∇F(vt+τ),τ−1∑
r=0W′gdc,r(xt) +g(xt+τ)⟩
=−η
2∥∇F(xt+τ)∥2+ (η2γm
2−η
2)∥τ−1∑
r=0W′gdc,r(xt)∥2+ (η2γm
2−η
2)∥g(xt+τ)∥2
+ (η2γm
2−η
2)⟨g(xt+τ),τ−1∑
r=0W′gdc,r(xt)⟩+η
2∥∇F(xt+τ)−g(xt+τ)∥2+η
2∥τ−1∑
r=1W′gdc,r(xt)∥2
−η⟨∇F(xt+τ)−g(xt+τ),τ−1∑
r=1W′gdc,r(xt)⟩+η⟨∇F(xt+τ)− ∇F(vt+τ),τ−1∑
r=0W′gdc,r(xt) +g(xt+τ)⟩
≤ −η
2∥∇F(xt+τ)∥2+ (η2γm
2−η
2)∥τ−1∑
r=0W′gdc,r(xt)∥2+ (η2γm
2−η
2)∥g(xt+τ)∥2
+ (η2γm
2−η
2)∥g(xt+τ)∥∥τ−1∑
r=0W′gdc,r(xt)∥+η
2∥∇F(xt+τ)−g(xt+τ)∥2+η
2∥τ−1∑
r=1W′gdc,r(xt)∥2
+η∥∇F(xt+τ)−g(xt+τ)∥∥τ−1∑
r=1W′gdc,r(xt)∥+η∥∇F(xt+τ)− ∇F(vt+τ)∥∥τ−1∑
r=0W′gdc,r(xt) +g(xt+τ)∥.
The ﬁrst inequality follows from the smooth property of the objective. The last inequality follows from
Cauthy-Schwarz inequality. The left hand side of the above inequality can be rewritten:
F(xt+τ+1)−F(xt+τ) +F(xt+τ)−F(vt+τ)
Taking expectations for both sides, with the last inequality, we have
E[F(xt+τ+1)−F(xt+τ)]
≤E[F(vt+τ)−F(xt+τ)]−η
2E[∥∇F(xt+τ)∥2] +η2γm−η
2E[∥τ−1∑
r=0W′gdc,r(xt)∥2] +η2γm−η
2E[∥g(xt+τ)∥2]
+η2γm−η
2E[∥g(xt+τ)∥∥τ−1∑
r=0W′gdc,r(xt)∥] +η
2E[∥∇F(xt+τ)−g(xt+τ)∥2] +η
2E[∥τ−1∑
r=1W′gdc,r(xt)∥2]
+ηE[∥∇F(xt+τ)−g(xt+τ)∥∥τ−1∑
r=1W′gdc,r(xt)∥] +ηE[∥∇F(xt+τ)− ∇F(vt+τ)∥∥τ−1∑
r=0W′gdc,r(xt) +g(xt+τ)∥]
≤GE[∥vt+τ−xt+τ∥]−η
2E[∥∇F(xt+τ)∥2] +η2γm−η
2ττ−1∑
r=0E[∥W′gdc,r(xt)∥2] +η2γm−η
2E[∥g(xt+τ)∥2]
+η2γm−η
2E[∥g(xt+τ)∥∥τ−1∑
r=0W′gdc,r(xt)∥] +η
2E[∥∇F(xt+τ)−g(xt+τ)∥2] +η
2E[∥τ−1∑
r=1W′gdc,r(xt)∥2]
+ηE[∥∇F(xt+τ)−g(xt+τ)∥∥τ−1∑
r=1W′gdc,r(xt)∥] +ηE[∥∇F(xt+τ)− ∇F(vt+τ)∥∥τ−1∑
r=0W′gdc,r(xt) +g(xt+τ)∥]
≤ −η
2E[∥∇F(xt+τ)∥2] +τ2η2γmM
2+ησ2
2+ηστB + 2η2γm(τB+G+G
ηγm)G+ (τ−1)Bθm
1−δ2
(96)
34Published in Transactions on Machine Learning Research (MM/YYYY)
The last inequality follows from the smoothness condition of F(x)and the bounded gradient, respectively,
as well as η <1
γm. Hence, by replacing t+τwith t, one can obtain
E[F(xt+1)−F(xt)]≤ −η
2E[∥∇F(xt)∥2] +R (97)
where Rindicates the constant term on the right hand side of the inequality. As we assume that F(x)is
bounded from below, applying the last inequality from 1 to T, one can get
F∗−F(x1)≤E[F(xt+1)]−F(x1)≤ −η
2T∑
t=1E[∥∇F(xt)∥2] +TR (98)
which results in
T∑
t=1E[∥∇F(xt)∥2]≤2[(F(x1)−F∗) +TR]
η(99)
Dividing both sides by T, the desirable results are obtained.
C Detailed Settings of Deep Learning Models
Model Settings For the PreResNet110 ( model 1 ), DenseNet ( model 2 ), ResNet20 ( model 3 ) and Eﬃcient-
Net ( model 4 ), models’ architectures are shown in He et al. (2016b ),Huang et al. (2017),He et al. (2016a )
andTan & Le (2019) respectively. The batch size is selected as 128. After hyperparameter searching in
(0.1,0.01,0.001), the learning rate is set as 0.01 for the ﬁrst 160 epochs and changed to 0.001. The decays
are applied in epochs (80,120,160,200). The approximation coeﬃcient λis set as 1. λ= 0.001is ﬁrst tried
as suggested by DC-ASGD Zheng et al. (2017) and the results show that the predicting step doesn’t aﬀect
the training process. By considering the upper bound of 1, a set of values (0.001,0.1,1)are tried, and λ= 1
is applied according to the performance.
Hardware environment. Our experiments are implemented and evaluated at GTX-1080 ti with Intel
Xenon 2.55GHz processor with 32GB RAM.
Table 5: Performance comparison in TinyImageNet and Time Series dataset
Model & datasetPre110
TinyImageNetDesNet
TinyImageNetEﬃcientNet
TinyImageNetLSTM
Wind Turbine Data
PC-ASGD (Ours) 58.0±1.4 61.4±0.7 74.8±0.9 71.2±0.5
D-ASGD
Lian et al. (2017)52.1±0.3 57.5±0.2 70.4±0.5 66.2±0.1
DC-s3gd
Rigazzi (2019)55.1±0.8 58.5±1.4 73.2±1.2 61.4±1.1
D-ASGD with IS
Du et al. (2020)53.2±0.9 58.1±1.2 73.4±0.7 69.2±0.2
Adaptive Braking
Venigalla et al. (2020)55.2±1.2 60.2±1.1 67.3±1.5 66.5±1.2
Table 6: Performance evaluation of ResNet20 on CIFAR-10
20 agents
Model & datasetPC-ASGD P-ASGD C-ASGD Baseline
acc. (%) o.p. (%) acc. (%) o.p. (%) acc.(%) o.p. (%) acc. (%)
ResNet 20, CIFAR-10 84.9±0.62.4±0.782.9±0.70.4±0.883.8±0.81.3±0.982.5±0.1
acc.–accuracy, o.p.–outperformed comparing to baseline.
35Published in Transactions on Machine Learning Research (MM/YYYY)
D More Empirical Results with diﬀerent datasets
We also adopt our numerical studies on TinyImageNet Le & Yang (2015) and Wind turbine data set Liu
et al. (2014). For TinyImageNet, we adopt PreResNet110 He et al. (2016b ), DenseNet Huang et al. (2017),
and EﬃcientNet Tan & Le (2019). For the wind turbine data set, we use LSTM3inLei et al. (2019) to
classify the fault in the wind turbine.
Results in Tab. 5shows the eﬀectiveness of our proposed methods in diﬀerent models, datasets, and even
diﬀerent tasks (time series classiﬁcation). It further demonstrates the generality of our proposed framework.
We also supplement the experiment with ResNet20 on CIFAR-10 to ablate the functions of the P-step and
C-step in Tab. 6. The quantitative results are consistent with Tab. 2, showing the beneﬁts of our PC steps
design.
3Actually, we use SGD-based optimizer for better analysis instead of Adam in Lei et al. (2019), hence we do not achieve the
best results in Lei et al. (2019). But our framework shows the best performances among other framework handling delay.
36