Published in Transactions on Machine Learning Research (01/2024)
Online Reference Tracking For Linear Systems with Un-
known Dynamics and Unknown Disturbances
Nariman Niknejad niknejad@msu.edu
Department of Mechanical Engineering
Michigan State University
Michigan, USA
Farnaz Adib Yaghmaie farnaz.adib.yaghmaie@liu.se
Faculty of Electrical Engineering
Linköping University
Linköping, Sweden
Hamidreza Modares modaresh@msu.edu
Faculty of of Mechanical Engineering
Michigan State University
Michigan, USA
Reviewed on OpenReview: https: // openreview. net/ forum? id= pfbVayaUMc
Abstract
This paper presents an online learning mechanism to address the challenge of state tracking
for unknown linear systems under general adversarial disturbances. The reference trajectory
is assumed to be generated by unknown exosystem dynamics, which relaxes the common
assumption of known dynamics for exosystems. Learning a tracking control policy for un-
known systems with unknown exosystem dynamics under general disturbances is challenging
and surprisingly unsettled. To face this challenge, the presented online learning algorithm
has two stages: In the first stage, an algorithm identifies the dynamics of the uncertain
system, and in the second stage, an online parametrized memory-augmented controller ac-
counts for the identification error, unknown exosystem dynamics as well as disturbances.
The controller’s parameters are learned to optimize a convex cost function, which is not
necessarily quadratic, and learning the control parameters is formulated as an online con-
vex optimization problem. This approach uses the memory of previous disturbances and
reference values to capture their effects on performance over time. Besides, it implicitly
learns the dynamics of the exosystems. The algorithm enables online tuning of controller
parameters to achieve state tracking and disturbance rejection. It is shown that the algo-
rithm achieves a policy regret of O(T2/3). In the simulation results, the performance of
the presented tracking algorithm is compared with the certainty equivalent H∞-control and
linear quadratic regulator.
1 Introduction
Reference tracking is a fundamental problem in control theory (Isidori, 1985; Huang, 2004; Dixon et al.,
2004; Vamvoudakis et al., 2017) and it has many applications (Zare et al., 2022), where the goal is to design
a control policy that steers the closed-loop system towards a reference trajectory or set-point. Significant
progress has been made toward developing reference tracking controllers for both linear and nonlinear sys-
tems. Most of the existing results, however, have made all or some of the following assumptions: 1) the
system dynamics are known; 2) the exosystem dynamics generating the reference trajectories are known; 3)
the system is deterministic and might be under bounded energy disturbances or the system is stochastic and
1Published in Transactions on Machine Learning Research (01/2024)
under independent and identically distributed (i.i.d.) noise, mostly Gaussian noise; and 4) only asymptotic
tracking is of concern, and no optimality of performance is concerned.
Several learning-based or adaptive controllers have been presented to deal with epistemic uncertainties (i.e.,
uncertainties that can be reduced by collecting data) in system dynamics and exosystem dynamics (i.e.,
to relax numbers 1 and 2 of the above-mentioned assumptions). Traditional adaptive controllers do not
provide performance guarantees, as they only optimize an instantaneous cost function (Lewis, 1986; Sutton
& Barto, 2018; Bertsekas, 2019; Zhang & Lewis, 2012). In contrast, reinforcement learning (RL) (Yao &
Yao, 2022; Wang et al., 2022b; Chen et al., 2019; Gao & Jiang, 2022; Deng et al., 2021; Mei et al., 2022;
Modares et al., 2016; Rabiee & Safari, 2023) leads to learning adaptive optimal control policies by optimizing
a long-horizon cost function using collected data. Despite this advantage, existing RL-based control solutions
for continuous state-actions are limited to deterministic systems with no disturbances (Li & Wu, 2020; Hao
et al., 2021) or bounded disturbances (Li et al., 2020; Mohammadi et al., 2021) and stochastic systems
with Gaussian noise (Cheng et al., 2019). For the case of bounded disturbances, RL algorithms typically
reformulate the H2-optimal control design into a robust min-max or H∞-optimal control problem (Khalil,
2002; Modares et al., 2015), which can be overly conservative. To this end, some works proposed combining
controllers such as resonant control with H∞for a better performance (Dadkhah & Moheimani, 2023). For
stochastic systems, existing RL algorithms are either based on policy gradient under which the controller
is parametrized and its parameters are learned through the gradient descent method or based on policy
iteration method under which the policies are iteratively evaluated until convergence. The former requires
approximating the expected gradient of the cost function, and the latter requires approximating the expected
cost itself. These results are typically limited to systems with Gaussian noise. However, in practical control
systems, adversaries aiming to degrade the control performance can act as adversarial disturbances that are
unpredictable and do not follow any distribution.
In this paper, the main focus is on constructing tracking controllers for linear systems when the underlying
system dynamics and exosystem dynamics are unknown, and disturbances are adversarial. Adversarial
disturbances can take any arbitrary form and are not restricted to those that are bounded with a known
bound or follow a probability distribution. A control policy in the form of (disturbance, reference)-action
is developed that leverages a fixed-size history of disturbances and reference values in its actions. The
presented online learning algorithm extends the results of Yaghmaie & Modares (2023); Hazan et al. (2020)
to systems with unknown dynamics and has two stages: In the first stage, a system identifier algorithm
estimates the unknown dynamics and uncertainty, and in the second stage, an online parameterized memory-
augmented controller is learned to optimize a convex cost function while accounting for the identification
error, unknown exosystem dynamics, and adversarial disturbances. Yaghmaie & Modares (2023) provides
a concise parameterization of the control policy resulting in O(√
T)regret bound benchmarking against
the best linear control policy when the system dynamics is known. In this extension to their work, our
algorithm achieves a regret bound of O(T2/3)for linear systems with unknown dynamics. Besides the
theoretical guarantees, in the simulation results, we compare the performance of the presented algorithm
with a couple of relevant solutions including the H∞and Linear Quadratic Regulator (LQR) control to
highlight its superior performance.
2 Related works
In this section, the related works to the problem of optimal tracking are summarized. These works are
focused on scenarios with an induced disturbance on the states and also where the dynamics of the system
are unknown.
System identification: Linear dynamic system identification is the process of determining the mathemat-
ical model that describes the input-output behavior of a linear dynamic system and it has been studied in
Ljung (1998). The least-squares method and its variants, such as total least squares and recursive least
squares, are widely used to identify the system’s parameters (Tatari et al., 2021; Faradonbeh et al., 2017;
Sarkar et al., 2019). System identification using machine learning techniques, such as artificial neural net-
works and support vector machines, has also gained popularity in recent years (Nagumo & Noda, 1967;
Weber et al., 2019; Chiuso & Pillonetto, 2019; Mehrzad et al., 2023). The choice of method depends on
2Published in Transactions on Machine Learning Research (01/2024)
the system’s characteristics, the available data, and the required accuracy of the identified model. A prop-
erly identified model can facilitate the design of robust controllers and the prediction of system behavior
under different operating conditions. In situations where there is an adversarial disturbance, the use of the
least-squares method may produce unreliable estimates. Thus, this paper exploits the method introduced
by (Theorem 19 in (Hazan et al., 2020)).
Output regulation theory: The output regulation theory, as introduced by Isidori and Huang in their
works(Isidori,1985;Huang,2004), hasbeenwidelyutilizedinthedesignofmodel-freereinforcementlearning
(RL) algorithms for solving optimal tracking problems, as well as in attenuating the effects of disturbances
(Gao et al., 2017; Chen et al., 2022; Jiang et al., 2020b; Chen et al., 2019; Jiang et al., 2020a; Gao & Jiang,
2016; 2015). However, a limitation of RL and adaptive dynamic programming (ADP) approaches based on
the output regulation theory is that they assume the disturbance is generated by a dynamical system, which
is not always the case in many real-world applications. This constraint restricts the applicability of the
output regulation theory in practical scenarios. Additionally, ADP methods typically optimize risk-neutral
(expected) or risk-aware measures of the cost function under the assumption of i.i.d and Gaussian noise.
This assumption is made because either the value function is learned directly based on collected data to
estimate expected or risk-aware accumulated rewards in policy interaction or value iteration methods, or the
expected or risk-aware cost function or its derivative with respect to control parameters is learned from data
in policy gradient methods. For general disturbances, usually a robust control approach is utilized which is
discussed below.
Robust control design: To handle general disturbances with limited energy, the H∞-control theory is
often employed to guarantee an L2-gain performance bound (Doyle, 1995; Khalil, 2002; Modares et al.,
2015). That said, the H∞-approach is known to be overly conservative, as the resulting robust controller
is designed to hedge against the worst-case disturbance sequence, which is rarely encountered in reality.
To that end, some works proposed online compensation for unknown stochastic disturbances for motion
planning and control (Faust et al., 2015).
Gaussiandisturbance: TheLinearQuadraticRegulator(LQR)canbeusedtodesignanoptimalcontroller
for linear systems subject to Gaussian process disturbance (noise) (Bertsekas, 2012). It is also the optimal
controller for noise-free linear systems. However, in many practical control systems, the disturbance does
not follow a Gaussian distribution or the cost function is not quadratic. The provided guarantees so far in
the literature are for the Gaussian case.
3 Optimal Reference Tracking Problem
Notations and preliminaries: LetIrepresent an identity matrix with the appropriate size. Let 1and0
denote matrices with appropriate sizes consisting of all ones and all zeros, respectively. The gradient of a
functionf(x)with respect to xis denoted by∇xf. TheL2-norm ofxis denoted by∥x∥L2= (/summationtext+∞
k=0∥xk∥2)1
2
where∥xk∥is the instantaneous Euclidean norm of the vector xk. For matrix A, the spectral norm is
denoted by∥A∥, and the Frobenius norm is denoted by ∥A∥F. Let IEbe an indicator function on set E.
For a time-dependent variable xk, the notation xi:j, j≥iis defined as xi:j={xi,xi+1,..,xj}. The notation
O()is leveraged throughout the paper to express the regret upper bound as a function of T.
3.1 Tracking Problem
Consider the following linear dynamical system
xk+1=Axk+Buk+wk, (1)
where the variables xk∈Rnanduk∈Rmrepresent the state and control input of the system, respectively.
In equation 1, wk∈Rndenotes the adversarial (unknown and arbitrary) disturbance. Only a bound, which
also does not need to be known a priori, is assumed on the disturbance for theoretical reasons. It can be
assumed that x0=0without loss of generality and incorporate the initial condition into w0.
The objective of this paper is to choose the input variable ukin such a way that the state of the system xk
follows an arbitrary reference signal rkthat is not known beforehand. This reference signal is only revealed
3Published in Transactions on Machine Learning Research (01/2024)
sequentially after the control input has been applied.
zk+1=Szk,
rk=Fzk,(2)
wherezk∈Rp, rk∈Rnrepresent the state and the output of the reference generator.
In the sequel, a list of a few definitions and results are brought that are related to equation 1-equation 2 and
the tracking problem.
Definition 1 (Agarwal et al., 2019) Consider
xk+1=Axk+Buk,
andγ∈[0,1), κ > 1. A linear controller Kis(κ,γ)-stable if∥K∥≤κand∥˜At
K∥2≤κ2(1−γ)t∀t≥0
where ˜AK=A+BK. Equivalently, a linear controller Kis also (κ,γ)-stable if there exist a decomposition
of˜AK=QLQ−1, such that∥L∥≤(1−γ), and∥A∥,∥B∥,∥Q∥,∥Q−1∥,∥K∥≤κ.
Definition 2 (Strong Controllability)(Definition 7 in (Hazan et al., 2020)) A linear dynamical system
(A,B)is said to have controllability index λif the matrix Gλis full-row rank, and
Gλ= [B,AB,A2B...Aλ−1B],
whereGλis defined as the matrix associated with (A,B)forλ≥1. In addition, such a system is also defined
(λ,κ)strongly controllable if ∥(GλGT
λ)−1∥≤κ.
Inacontrollablesystem, thecontrollabilityindex λhasanupperboundofthenumberofstatesinthesystem.
Itisworthnotingthat, duetotheCayley-Hamiltontheorem, thecontrollabilityindexofacontrollablesystem
is never greater than the dimension of the state space. Assuming that the system (A+BK,B )is(λ,κ)
strongly controllable, similar to the concept of stability, a measurable counterpart of controllability is initially
presented by (Cohen et al., 2018).
Lemma 1 (Maintaining Stability) (Lemma 15 in (Hazan et al., 2020)) Consider an identified dynamical
system with (ˆA,ˆB). Assume that the original system is (κ,γ)-strongly stable. It can be shown that control gain
Kis(κ+ϵA,B,γ−2κ3ϵA,B)- strongly stable for (ˆA,ˆB), as long as∥A−ˆA∥,∥B−ˆB∥≤ϵA,B. For the system
with estimated matrices, one has ∥ˆA∥,∥ˆB∥≤κ+ϵA,B. Assuming∥Q∥,∥Q−1∥,∥K∥,∥A∥,∥B∥≤κ, we define
ˆA+ˆBK=QˆLQ−1where one can show ∥ˆL∥≤1−γ+ 2κ3ϵA,B, where ˆL:=L+Q−1((ˆA−A)−(ˆB−B)K).
Moreover, the Qmatrix coincide in both the actual system (A,B)and the one with estimated system matrices
(ˆA,ˆB).
Theorem 1 is typically leveraged to present a fundamental discovery outlining the necessary and sufficient
conditionfortheexistenceofalinearfeedbackstrategythatcansolvethestatetrackingproblem. Specifically,
the problem concerns ensuring that xk→rkin the absence of disturbances. In this context, the term "linear
feedback policy" denotes a particular approach utilized to address the problem.
Theorem 1 (Isidori, 1985) Consider the dynamical system in equation 1 and the reference signal in equa-
tion 2. Assume that wk≡0,(A,B)is stabilizable. Assume that the learner has previous knowledge on Kfb
such thatA+BKfbis strongly stable. Then, the controller
ulin
k(Kf) =Kfbxk+Kffzk. (3)
solves the classical state tracking problem xk→rk, if and only if there exist matrices Π∈Rn×pandΓ∈Rm×p
such that
ΠS=AΠ +BΓ,Π−F=0 (4)
andKff= Γ−KfbΠ.
4Published in Transactions on Machine Learning Research (01/2024)
3.2 Performance index
Themaingoalofthispaperistodesignacontrolpolicy π: (x1:k,w1:k−1,r1:k)→ukthatoptimizesanaverage
cost function, reflecting the designer’s intentions. The total cost linked to a given policy πis determined as
follows
JT(π) =T/summationdisplay
k=1ck(xk,uk), (5)
whereckis the rolling cost. Also, the average cost of a policy πis defined as the below equation
¯JT(π) =1
TT/summationdisplay
k=1ck(xk,uk). (6)
3.3 The presented policy
The conventional linear controller in the form of equation 3 aims to mitigate the impact of adversarial or
arbitrary disturbances on the cost function by determining gains KfbandKffusingH∞-control design.
However, this approach is overly conservative and encounters difficulties in online control design due to the
non-convexity of the cost function ck(xk,uk)with respect to KfbandKff. To tackle this issue, we work
with the class of memory augmented policies which is capable of handling adversarial disturbances and
unknown dynamics (Yaghmaie & Modares, 2023; Agarwal et al., 2019).
Definition 3 A Memory-augmented Control Policy is denoted by π(K,M,P )
uπ
k(K,M,P ) =Kxk+mw/summationdisplay
t=1M[t−1]wk−t+mr−1/summationdisplay
s=0P[s]rk−s, (7)
whereK∈Kis a fixed matrix and Y= [M, P ] = [M[0],...,M[mw−1],[P[0],...,P[mr−1]]∈Yare parameters
to be learned. The domains K,Yare defined as
K={K:A+BKis(κ,γ)−stable}, (8)
Y={Y= [M[0],...,M[mw−1], P[0],...,P[mr−1]]|∥M[t]∥,∥P[t]∥≤κbκ3(1−γ)t}.
Since the policy parameters are learned, which are changing over time, Mk= [M[0]
k,...,M[mw−1]
k]andPk=
[P[0]
k,...,P[mr−1]
k]are representing as the policy parameters at step k.
Observer that the class of memory-augmented policies is more general than the class of linear controller
policies. Indeed, a linear control policy is a special case of the memory-augmented policy.
Letxπ
kbe the state attained upon execution of the policy π(K,M 0:k−1,P0:k−1)that generates the control
input in equation 7 at time k. One can show that the state attained upon execution of a memory-augmented
control policy is linear in M. This is established in the next lemma. Consequently, this implies linearity
of the memory-augmented control policy in M. Since the cost function ck(xk,uk)is convex in xk,ukand
xπ
k, uπ
kare linear in M, one can conclude that ckis convex in M. Let ˜AK=A+BKand define
ΨK,h
k,y(Mk−h−1:k−1) :=˜Ay
KIy≤h−1+h−1/summationdisplay
j=0˜Aj
KBM[y−j−1]
k−j−1I1≤y−j≤mw, (9)
ψK,h
k,z(Pk−h−1:k−1) :=h−1/summationdisplay
j=0˜Aj
KBP[z−j−1]
k−j−1I1≤z−j≤mr. (10)
5Published in Transactions on Machine Learning Research (01/2024)
Lemma 2 ((Yaghmaie & Modares, 2023)) Letxπ
kbe the state attained upon execution of the policy
π(K,M 0:k−1,P0:k−1)that generates the control input in equation 7 at time k. Then
xπ
k=xK
k(M0:k−1,P0:k−1) =˜Ah
Kxπ
k−h+mw+h−1/summationdisplay
y=0ΨK,h
k,y(Mk−h−1:k−1)wk−y−1
+mr+h−1/summationdisplay
z=0ψK,h
k,z(Pk−h−1:k−1)rk−z.(11)
or equivalently
xπ
k=xK
k(M0:k−1,P0:k−1) =k−1/summationdisplay
y=0ΨK,k
k,y(M0:k−1)wk−y−1+k−1/summationdisplay
z=0ψK,k
k,z(P0:k−1)rk−z. (12)
3.4 Assumptions and the optimal tracking problem
In this subsection, a list of the assumptions is brought to be used throughout the paper and define the
optimal tracking problem in the presence of adversarial disturbances.
Assumption 1 (dynamical system) The pair (A,B)is unknown but stabilizable. The actual system
matrices (A,B)and the identified system dynamics matrices (ˆA,ˆB)are bounded, i.e., ∥A∥,∥B∥≤κand
∥ˆA∥,∥ˆB∥,≤κ+ϵA,B, whereϵA,Bdenotes the distance between the actual and identified system matrices.
Assumption 2 (disturbance) The disturbance sequence wkis bounded, i.e.,∥wk∥≤κwfor someκw>0.
Moreover, the disturbance wkdoes not depend on the control input uk.
Assumption 3 (reference signal) The dynamics of the reference signal generator are unknown but de-
tectable. The state of the reference signal zkis not measurable but the output rkis measurable and rk, zk
are bounded, i.e., ∥rk∥≤κrand∥zk∥≤κz.
Assumption 4 (Known Linear Controller) A control gain Kin equation 7 that makes the unknown
system (A,B),(κ,γ)−stable is available to the learner. In other words, the set in equation 8 is known.
Remark 1 Partial knowledge of the system model can be typically extracted for many practical systems using
the physical information available, which can be leveraged to design robust controllers satisfying Assumption
4. That is, even though no knowledge of the system models is used during learning, partial knowledge of the
system models (e.g., the matrices AandBbelonging to sets of possible system models) is required to find the
starting control policy. This is a standard assumption in most control systems since, without a stabilizing
control policy to start with, the system’s state can quickly become large and useless to learn from, and the
system can also fail before any learning occurs.
Assumption 3 stipulates that the reference signal must be bounded, as an unbounded reference signal may
lead to an unbounded average cost. This assumption is commonly employed in analyzing average cost, as
observed in works like Abbasi-Yadkori et al. (2014) and Adib Yaghmaie et al. (2019). Nevertheless, the
issue of tracking unbounded reference signals can be tackled by exploring discounted cost settings, where an
appropriate discounting factor can ensure the boundedness of the discounted cost. This approach has been
demonstrated in other studies, including Kiumarsi et al. (2014).
Assumption 5 (cost function) The costck(xk,uk)is convex in xk, uk. Moreover, when ∥x∥,∥u∥≤D,
it holds that|ck(xk,uk)|≤βD2and∥∇xck(x,u)∥,∥∇uck(x,u)∥≤GcDfor someβ >0andGc>0.
Assumption 5 broadens the scope of applicable cost functions beyond quadratic forms, thereby enhancing
the inclusiveness of the assumption.
6Published in Transactions on Machine Learning Research (01/2024)
Problem 1 (Optimal Tracking Against Adversarial Disturbances with Unknown Dynamics)
Consider the dynamical system in equation 1, the reference generator in equation 1-equation 2 and the cost
function in equation 5. Let Assumptions 1-5 hold. Design a policy in the form of equation 7
uπ
k(K,M,P ) =Kxk+mw/summationdisplay
t=1M[t−1]wk−t+mr−1/summationdisplay
s=0P[s]rk−s,
from the class of memory-augmented policies in Definition 3 to optimize the total cost in equation 5
JT(π) =T/summationdisplay
k=1ck(xk,uπ
k).
4 Memory-augmented online state-tracking algorithm
We propose Algorithm 1 to solve Problem 1. The algorithm uses the concept of truncated state and cost
which will be defined in the sequel.
4.1 Truncated state, input and cost
Similar to (Yaghmaie & Modares, 2023), we limit everything to a fixed memory length of H. Let ˜xπ
k,˜uπ
k,:fk
represent the truncated state, input, and cost if the system had started at ˜xπ
k−H=0. The expressions for
˜xπ
k,˜uπ
kare
˜xπ
k(Mk−H−1:k−1,Pk−H−1:k−1) = (13)
mw+H−1/summationdisplay
y=0ΨK,H
k,y(Mk−H−1:k−1)wk−y−1+mr+H−1/summationdisplay
z=0ψK,H
k,z(Pk−H−1:k−1)rk−z,
˜uπ
k(Mk−H−1:k,Pk−H−1:k) = (14)
K˜xK
k(Mk−H−1:k−1,Pk−H−1:k−1) +mw/summationdisplay
t=1M[t−1]
kwk−t+mr−1/summationdisplay
s=0P[s]
krk−s,
and the truncated cost fkreads
fk(Mk−H−1,...,Mk−1,Pk−H−1,...,Pk−1)
=ck(˜xπ
k(Mk−H−1:k−1,Pk−H−1:k−1)−rk,˜uπ
k(Mk−H−1:k,Pk−H−1:k)).(15)
In Appendix B, some theoretical results are provided regarding the memory-augmented controllers and the
associated states and costs which are essential in obtaining the main result in Theorem 4.
4.2 The overall online learning algorithm (Algorithm 1)
Algorithm 1 involves two stages. In the first stage, the system dynamics are identified, then, in the second
stage, theonlinecontrollerislearned. Thisapproachiscommonlyknownastheexplore-then-commitpipeline
and will be explained in the sequel.
The algorithm starts in Line 1with the selection of a stabilizing controller gain K, as well as other necessary
parameters.
System identification (Algorithm SysId): Following the initiation of the algorithm, the identification
stage begins by executing Algorithm SysId. As detailed in Algorithm SysId, the controller in equation 18
is used to collect T0samples. In this algorithm, it is assumed that the learner has access to a stabilizing
7Published in Transactions on Machine Learning Research (01/2024)
control gain K. System identification using binary inputs with values of -1 and 1 (also called bipolar inputs)
is a method to analyze and model the behavior of a system. By systematically applying binary input
sequences consisting of -1 and 1, the system’s response is measured and recorded in Line 3.InLines 4,5,6 ,
dummy variables GandQare computed. Then, in Line 7, a deterministic-equivalent matrix pair (ˆA,ˆB)
is identified through an iterative procedure that determines matrices of the form (A+BK)iB, followed
by solving a linear system of equations to recover the original matrix A. Note that during this stage, the
trajectory to be followed is disregarded. The estimated dynamics (ˆA,ˆB)is fed to the presented robust
tracking algorithm. This identification procedure is inspired by (Hazan et al., 2020).
Remark 2 (Ljung, 1995) Generating a stochastic binary signal often involves the introduction of white
Gaussian noise, which is subsequently filtered using a carefully selected linear filter. The resulting signal’s sign
is then extracted, conforming it to a desired binary level. A signal is deemed favorable for the identification
of linear systems when it exhibits a small crest factor. The crest factor, with a minimum value of 1, is
achieved when employing a binary waveform. However, binary signals prove less useful for nonlinear system
identification due to their limited information content and lack of varied excitation levels (Novak et al.,
2009).
Robust tracking (Algorithm RobTrack): Upon identification of the system, the domain set Yis ini-
tialized and a loop starts. In Line 7, the reference signal rkis recorded. uπ
kin equation 7 is calculated
and applied to the system. Next, in Line 8, the next state xk+1is observed, and the disturbance ˆwkis
estimated by equation 16. Selection of ˆwkaccording to equation 16 ensures that the state, action, and cost
produced by Algorithm 1 coincide with those of the actual system. In Line 9, the algorithm suffers the cost
ck(ek,uk). ThenLine 10, the truncated state and inputs are computed from equation 13-equation 14 using
the latest values of M,P, and the truncated cost fk(M[0],...,M[mw−1],P[0],...,P[mr−1])is calculated from
equation 15. In Line 11, the weights M, Pare adjusted with projected gradient descent on the truncated
costfk(M[0],...,M[mw−1],P[0],...,P[mr−1])based on equation 17.
It should be noted that during each iteration kof the algorithm outlined in Algorithm 1, the values of
ˆwT0+1:k−1andrT0+1:kare already known and accessible. Additionally, for all k<T 0,ˆwkandrkare defined
to be equal to the zero vector. Thus, it is possible to compute the expressions in equation 13-equation 15 for
any given iteration of the algorithm. The projection operator ΠMandΠPare matrix projection operators
withL2norm ofκbκ3(1−γ).
The properties related to Algorithms SysId and 1 are given in Appendices C-D.
4.3 Regret Analysis
The standard measure for online control based on the gradient descent is the policy regret (Agarwal et al.,
2019), which is defined here as the difference between cumulative cost of the designed parameterized control
policyπlearned by Algorithm 1 and that of the optimal linear control policy in the form of equation 3.
Definition 4 Consider the system in equation 1. Let the control policy be designed to generate the control
actionukin equation 7 at time k. Let Algorithm 1 be used to update the parameters of uk. Then, its regret
is defined as
Regret =T/summationdisplay
k=1ck(xk,uk)−min
Kf∈KJT(Kf),
whereJT(Kf)is the total cost in equation 5 of the linear feedback controller in equation 3.
TheregretcomparestheperformanceofAlgorithm1generatingcontrollersfromtheclassoffeasiblememory-
augmented control policies with the best linear control policy in hindsight.
8Published in Transactions on Machine Learning Research (01/2024)
Algorithm 1 Online state tracking algorithm
1:Initialize: Set a stabilizing controller gain K, perturbation horizon mw, reference horizon mw, rounds of
system identification T0, number of iterations after system identification T, and horizon of identification
λ.
2:Stage 1: System Identification
(ˆA,ˆB) =SysId (T0,λ).
3:Stage 2: Robust Tracking (Algorithm RobTrack)
4:InitializeY={Y= [M[0],...,M[mw−1], P[0],...,P[mr−1]]|∥M[t]∥,∥P[t]∥≤κbκ3(1−γ)t}.
5:Setˆwk= 0for allk≤T0and ˆwk=xT0.
6:fork=T0+ 1,..,T 0+Tdo
7:Recordrkand execute
uπ
k(K,M,P ) =Kxk+mw/summationdisplay
t=1M[t−1]ˆwk−t+mr−1/summationdisplay
s=0P[s]rk−s
8:Observexk+1and record an estimate
ˆwk=xk+1−ˆAxk−ˆBuk. (16)
9:Sufferck(ek,uk).
10:Computefk(M[0],...,M[mw−1],P[0],...,P[mr−1])in equation 13-equation 15 for ˆA,ˆB, and ˆw.
11:UpdateM, P.
M= ΠM(M−η∇Mfk(M[0],...,M[mw−1],P[0],...,P[mr−1])),
P= ΠP(P−η∇Pfk(M[0],...,M[mw−1],P[0],...,P[mr−1])).(17)
Algorithm SysId System identification by inducing random inputs
1:Inputs:T0, λ.
2:fork= 1,...,T 0do
3:Induce the control
uk=Kxk+ηk,ηk∼i.i.d.{±1}m. (18)
4:Observe and record the resulting state xk.
5:CalculateQj=1
T0−λ/summationtextT0−λ−1
k=0xk+j+1ηT
k,∀j∈{λ}.
6:FormG0= (Q0,...,Qλ−1),G1= (Q1,...,Qλ).
7:Outputs: ˆAand ˆB
ˆB=Q0,ˆA′=G1GT
0(G1G0)−1,ˆA=ˆA′−ˆBK.
In the sequel, we give the regret analysis of Algorithm 1. The main technical difficulty in the regret analysis
lies in combining the system identification and the online control approach where both the estimated and
true dynamics are present. Another aspect is to find the right balance between the identification and control
horizons to guarantee a sublinear regret bound.
9Published in Transactions on Machine Learning Research (01/2024)
Theorem 2 Suppose Algorithm 1 is executed under Assumptions 1-5. Let H=mw=mr. Select the
learning rate ηand the memory size Hto satisfyη=O(1
Gcκw√
T),H=O(logκ2T
γ), andT0=T2/3. Then,
Regret =O(T2/3).
Proof:LetK∗= arg min K∈K/summationtextT
k=1ck(xk,uk). We decompose the regret as
Regret =T/summationdisplay
k=1(ck(xk,uk)−ck
minK∈K(xk,uk))
=T0/summationdisplay
k=1(ck(xk,uk)−ck
minK∈K(xk,uk)) +T/summationdisplay
k=T0+1(ck(xk,uk)−ck
minK∈K(xk,uk))
≤T0/summationdisplay
k=1(ck(xk,uk)−ck
minK∈K(xk,uk)) +T/summationdisplay
k=1(ck(xk,uk)−ck
minK∈K(xk,uk))
=T0/summationdisplay
k=1(ck(xk,uk)−ck
minK∈K(xk,uk))
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
J0+J(A|ˆA,ˆB,{ˆw},{r})−J(K∗|ˆA,ˆB,{ˆw},{r})/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
R1
+J(K∗|ˆA,ˆB,{ˆw},{r})−J(K∗|A,B,{w},{r})/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
R2.
The termJ0contains the regret for the system identification stage in Algorithm SysId. The regret analysis
is given in Lemma 8 where we show that R1=O(T0). The term R1compares the total cost by our
algorithm using the estimated dynamics with that of the best linear controller using the estimated dynamics.
In Theorem 4, we prove that R2=O(√
T). The term R2compares the total cost by the best linear
controller using the estimated dynamics with that of using the original dynamics. In Lemma 11, we show
thatR2=O(TT−1/2
0). Selecting T0=T2/3, the regret is concluded.
5 Simulation results
In this section, the simulation results are given.
5.1 The dynamical system, reference, and cost function
Consider the following tracking problem where the dynamics of the system is considered as
xk+1=/bracketleftbigg1 1
0 1/bracketrightbigg
xk+/bracketleftbigg1 0
0 1/bracketrightbigg
uk+wk, (19)
and the reference signal is generated by
zk+1=
0 1 0
−1 1.5 0
0 0 1
zk, z0= [1,−2,0.5]T,
rk=/bracketleftbigg
1 0 0
0 0 1/bracketrightbigg
zk.(20)
where
xk=/bracketleftbiggx1k
x2k/bracketrightbigg
, wk=/bracketleftbiggw1k
w2k/bracketrightbigg
, rk=/bracketleftbiggr1k
r2k/bracketrightbigg
, ek=/bracketleftbigge1k
e2k/bracketrightbigg
=/bracketleftbiggx1k−r1k
x2k−r2k/bracketrightbigg
.
10Published in Transactions on Machine Learning Research (01/2024)
A quadratic cost with Q= 20I2, R=I2is considered; that is
ck=eT
kQek+uT
kRuk.
Note that the presented algorithm is designed to handle any convex cost function, but a quadratic cost
is chosen for comparison with classical control approaches such as Linear Quadratic Regulator (LQR) and
H∞-controllers.
5.2 Disturbances
We consider 7 different cases of disturbance. In each case, the disturbance is introduced at the start of the
simulation, and as a result, the sequence of disturbances is consistent across all algorithms. The first three
cases involve randomly generated disturbances, while the remaining cases involve continuous disturbances,
which allows us to study how the algorithms perform when the disturbances are not stochastic. Also, a
worst-case disturbance case is considered to assess the performance of the tracking algorithm against an
adversary.
•Uniformly sampled disturbance It is considered the disturbance to be uniformly sampled from
the interval [0,1].
•Constant disturbance The constant disturbance is considered as w1k=w2k= 1.
•Amplitude modulation disturbance The disturbance is considered as w1k=w2k=
sin(6πk/500) sin(8πk/500).
•Sinusoidal disturbance A sinusoidal disturbance is considered as w1k=w2k= sin(8πk/100).
•Gaussian disturbance Gaussian disturbances is utilized where w1k∼ N (0,0.01)andw2k∼
N(0,0.01)in this study. If the system’s dynamic is known, the optimal controller for an LQR cost is
a linear one (Bertsekas, 2012). The support for the Gaussian noise is not finite for the LQR method.
That said, the theoretical results require the disturbance to be bounded. However, the actual bound
does not necessarily need to be known and could be large, in contrast to robust control methods
such asH∞. In this paper’s simulation, the Gaussian noise generator (numpy.random.normal) is
utilized which is provided by Numpy in Python that generates bounded samples.
•Random walk disturbance It is assumed that the disturbance follows a random walk and is
generated by wk= 0.999wk−1+ηk−1, whereηk−1∼N (0,0.01). The internal dynamics of the
random walk is chosen to be 0.999instead of 1in order to ensure the boundedness of the disturbance.
When the noise follows a random walk, the optimal LQR controller is linear. To illustrate this, the
random walk disturbance in equation 1 is replaced with
xk+1=Axk+Buk+ 0.999wk−1+ηk−1.
Here, in each time step k, the statexkis measured, and according to Assumption 1, wk−1is known.
A new state variable ¯xk= [xT
k,wT
k−1]is introduced, then one can obtain
¯xk+1=/bracketleftbiggA0.999I
00.999I/bracketrightbigg
¯xk+/bracketleftbiggB
0/bracketrightbigg
u+/bracketleftbiggI0
0I/bracketrightbigg
ηk−1. (21)
Thus, equation 1 with a random walk disturbance can be viewed as an extended system described
by equation 21, where the noise ηk−1is Gaussian. Consequently, the optimal controller for this
extended system is the LQR.
•Worst-case disturbance (Adversary) Tracking an unknown reference signal with an adversary
agent can be formulated as a two-player zero-sum game in which the control policy seeks to minimize
the value function, while the disturbance policy wkdesires to maximize it. The goal is to find the
11Published in Transactions on Machine Learning Research (01/2024)
feedback saddle point (u∗
k,w∗
k)such that if we take the rolling cost ci=xT
iQxi+uT
iRui−γ2wT
iwi,
one has
J∗(xk) = min
ukmax
wkT/summationdisplay
i=k[eT
iQei+uT
iRui−γ2wT
iwi]. (22)
The worst disturbance based on the formulation of the zero-sum game in (Kiumarsi et al., 2017) can
be computed at each state as w∗
k=−Kw(xk−rk)whereKwcan be computed as
Kw=(ITPI−γ2I−DTPB(R+BTPB)−1)−1(ITPA−ITPB(R+BTPB)−1BTPA)
wherePsatisfies the game algebraic Riccati equation (GARE)
P=ATPA+Q−[ATPB ATPI]/bracketleftbiggR+BTPBBTPD
DTPB DTPD−γ2I/bracketrightbigg−1/bracketleftbiggBTPA
DTPA/bracketrightbigg
.
5.3 The compared control approaches
The effectiveness of the presented online tracking algorithm is shown by comparing it to other linear control
methods, including the LQRandH∞approaches. These approaches optimize a quadratic performance index
and are considered optimal for Gaussian and worst-case disturbances, making them the best performers in
scenarios where these types of disturbances are present.
In the LQR approach, we consider two cases where the actual model of the system is known, as well as when
an estimated model is used for the design. We set T0= 464, λ= 5when we identify the dynamics of the
system in an approach. The details are given in the description of each algorithm. In the case of Adversarial
disturbance, the identified model from the Gaussian noise is used since the adversarial disturbance makes
the open-loop system unstable. Learning a system model despite an adversarial disturbance is a daunting
challenge and an open problem.
•Online state tracking in Algorithm 1: During the execution of the algorithm, the value of
Kis maintained unchanged, which can be obtained based on a priori knowledge of the systems’
dynamics. Note that this Kcan be any stabilizing controller that the algorithm is assumed to have
access to. The other parameters are chosen as H= 5,mr= 5,mw= 5, andη= 0.0001, andM
andPare initialized as zero matrices. In this algorithm, no information about the dynamics of the
reference signal is needed. This algorithm only relies on the measured outputs of the reference signal
rk. Similarly, no information about the disturbance is Incorporated into this algorithm.
•LQR:Thefeedbackcontrollergain Kfbischosenas−(R+BTPrB)−1BTPrA, wherePriscalculated
usingARE (A,B,Q,R ), assuming knowledge of the dynamics of the reference signal. Subsequently,
Kffis calculated using the approach mentioned in Theorem 1. The control law uk=Kfbxk+Kffzk
requires knowledge of the state of the reference signal, denoted as zk, which is constructed from rk
using the dynamics of the reference signal as described in Lemma 1 of (Yaghmaie & Modares, 2023).
•Certainty equivalent (C.E.) LQR and LQR for random walk: The feedback controller gain
Kfbis chosen as−(R+ˆBTPrˆB)−1ˆBTPrˆA, wherePris calculated using ARE (ˆA,ˆB,Q,R ), with
(ˆA,ˆB)being the identified system. Subsequently, Kffis computed with the method mentioned
in Theorem 1. Also, the control law uk=Kfbxk+Kffzkrequires knowledge of the state of the
reference signal, denoted as zk, which is constructed from rkusing the dynamics of the reference
signal as described in Lemma 1 of (Yaghmaie & Modares, 2023).
It is observed in Subsection 5.2 that when the disturbance is a random walk, the system dynamics
can be extended according to equation 21. The extended dynamics involve a Gaussian disturbance,
and consequently, LQR for the extended dynamics is used as the optimal controller. In this case,
the algorithm is referred to as “ LQR for random walk ”.
12Published in Transactions on Machine Learning Research (01/2024)
Table 1: The final maximum and normal difference between the identified dynamics of the system (ˆA,ˆB)
and the actual one (A,B). "Maximum difference" refers to the maximum difference in the identification of
entries of matrices A and B
Disturbance Max Difference for A Norm of Difference for A Max Difference for B Norm of Difference for B
Constant 0.14 0.27 0.03 0.04
Amplitude mod. 0.11 0.17 0.06 0.08
Sinusoidal 0.16 0.27 0.03 0.04
Gaussian 0.07 0.07 0.03 0.03
Random walk 0.22 0.28 0.07 0.10
Uniformly sam. 0.06 0.08 0.02 0.03
•Certainty equivalent (C.E.) H∞-control: The Certainty equivalent (C.E.) H∞-control ap-
proach aims to design a controller, denoted as Kfb, for the system described by equation 1, such
that theL2-norm of the system’s output, scaled by√Q, divided by theL2-norm of the worst-case
disturbance input, denoted as w, is less than or equal to a threshold. For the sake of comparison,
it is assumed that the dynamics of the reference input, described by equation 20, are known. This
knowledge is utilized to construct zkfromrkas described in Lemma 1 of (Yaghmaie & Modares,
2023). Then, the controlinput ukis calculated as uk=Kfbxk+Kffzk, whereKffis thefeedforward
controller computed with the method mentioned in Theorem 1. Certainty equivalent H∞-control
approach is conservative, as it ensures a finite L2-gain for the worst-case disturbance.
5.4 Evaluation of the identification algorithm
In this subsection, the performance of the identification algorithm is discussed for the 6 cases of the distur-
bance in Subsection 5.2. In Table 1, the maximum difference between the actual system and the identified
one is summarized, as well as the norm of their difference. When the noise is non-Gaussian, system iden-
tification using random binary inputs and the least squares method have different implications. Random
binary inputs can provide diverse frequency content for analysis, but they may be more sensitive to outliers
and non-linear distortions, resulting in potentially less accurate parameter estimates. Meanwhile, the least
squares method assumes Gaussian noise, and when this assumption is violated, the parameter estimates ob-
tained may be biased or less accurate due to the influence of non-Gaussian noise. Thus, the choice between
these methods should consider the specific non-Gaussian characteristics of the noise to ensure reliable system
identification results. In table 1 the results gained from the presented system identification show that even
though the disturbances are not Gaussian except for one case, the identification errors are very close to it,
with a maximum L2norm error of 0.28 for random walk noise and a minimum L2norm error of 0.03 for
Gaussian noise.
5.5 Evaluation of the Tracking Algorithm
In Fig. 1, the reference signal is plotted over t= [9900,10000]for the representation purpose. The
algorithms in Subsection 5.3 are run for T= 10000 steps and the final average costs are brought in Table 2.
The performance of the algorithms over t= [9900,10000]is depicted in Fig. 2 - 8.
When the disturbance follows a non-Gaussian or non-random walk distribution, there is no analytical ap-
proach to determine the optimal linear feedback policy. In such cases, the H∞-controller is commonly
employed to design a linear feedback policy that ensures a finite L2-gain for the worst-case disturbance, al-
beit with a conservative approach. If the actual disturbance is not the worst-case scenario, the H∞-controller
may not yield the best performance. According to Fig. 2 - 7, the presented algorithm has an even better
performance in constant, amplitude modulation, sinusoidal, random walk, uniformly distributed, and adver-
sarial disturbances. Additionally, the performance of the presented algorithm is comparable to the actual
LQR and certainty-equivalent LQR when Gaussian noise is present.
In the case where the disturbance is Gaussian and the dynamics of the reference signal and the actual
system are known, the optimal linear feedback policy can be determined by selecting Kfb=−(R+
BTPrB)−1BTPrA, wherePr=ARE (A,B,Q,R ), and subsequently calculating Kff. For the sake of the
13Published in Transactions on Machine Learning Research (01/2024)
Table 2: The final average cost, as introduced in equation 6, incurred by the different algorithms over a
duration of T = 10000 steps is presented. Notably, the most competitive average cost values, indicated in
bold, are reported for each respective disturbance case. It is noteworthy that the evaluation of LQR for
random walk is solely applicable to scenarios involving random walk disturbances, and thus its performance
is only assessed in such instances. C.E. refers to certainty equivalent and R.W. refers to random walk.
Disturbance Algorithm 1 C.E. LQR C.E.H∞C.E./Actual LQR R.W. LQR
Constant 8.04 40.07 29.18 N.A. 57.76
Amplitude mod. 7.83 16.74 12.72 N.A. 17.58
Sinusoidal 15.21 27.71 20.08 N.A. 30.21
Gaussian 5.62 5.32 5.29 N.A. 5.25
Random walk 17.75 236.11 163.02 19.22 / 15.48 236.68
Uniformly sam. 10.09 19.51 16.09 N.A. 21.99
Adversarial 13.76 17.21 14.75 N.A. 17.17
experiment, one can take Kfbcomputed this way as the initial stabilizing control gain for the presented
algorithm as well. The results can be seen in Table 2. The average cost of the presented tracking approach
is considerably close to the optimal control when the noise is Gaussian (LQR). This can be seen in Figure 9.
A similar discourse is applicable to the scenario of a random-walk disturbance, as elucidated in Subsection
5.2, where it is demonstrated that the optimal controller for the system in the presence of a random-walk
disturbance can be obtained by solving an LQR problem for the extended system. That said, the uncer-
tainty that is present in the model identification of the system resulted in poorer performance of the certainty
equivalents as compared to the presented algorithm.
6 Conclusion
In this paper, the challenge of state tracking in the presence of general disturbances is addressed, even
when the dynamics of the actual system are unknown. An algorithm that combines an identification period
with a memory-augmented robust tracking algorithm is introduced. This presented algorithm enables online
tuning of the controller parameters to achieve state tracking and disturbance rejection while minimizing
convex costs. It is shown that the presented online algorithm achieves a policy regret of O(T2/3). In our
future research, we plan to extend our approach to partially observable dynamical systems and eliminate the
bounded assumption on the reference signal.
Figure 1: The reference signals used for the evaluation and comparison of various control algorithms under
different disturbances.
14Published in Transactions on Machine Learning Research (01/2024)
Figure 2: Tracking error for constant disturbance for the presented Algorithm 1, versus certainty equivalent
H∞-control, certainty equivalent LQR control, and LQR control knowing the dynamics of the system using
the reference signals in Fig. 1.
Figure 3: Tracking error for amplitude modulation disturbance for the presented Algorithm 1, versus cer-
tainty equivalent H∞-control, certainty equivalent LQR control, and LQR control knowing the dynamics of
the system using the reference signals in Fig. 1.
Acknowledgment
Nariman Niknejad and Hamidreza Modares are supported by the Department of Navy award N00014-22-1-
2159 issued by the Office of Naval Research, USA. Farnaz Adib Yaghmaie is supported by the Excellence
Center at Linköping–Lund in Information Technology (ELLIIT), ZENITH, and partially by Sensor infor-
matics and Decision-making for the Digital Transformation (SEDDIT).
References
Yasin Abbasi-Yadkori, Peter Bartlett, and Varun Kanade. Tracking adversarial targets. In International
Conference on Machine Learning , pp. 369–377. PMLR, 2014.
Farnaz Adib Yaghmaie, Svante Gunnarsson, and Frank L. Lewis. Output regulation of unknown linear
systems using average cost reinforcement learning. Automatica , 110:108549, 2019. ISSN 00051098. doi:
10.1016/j.automatica.2019.108549. URL https://doi.org/10.1016/j.automatica.2019.108549 .
Naman Agarwal, Brian Bullins, Elad Hazan, Sham M. Kakade, and Karan Singh. Online control with
adversarial disturbances. 36th International Conference on Machine Learning, ICML 2019 , 2019-June:
15Published in Transactions on Machine Learning Research (01/2024)
Figure 4: Tracking error for sinusoidal disturbance for the presented Algorithm 1, versus certainty equivalent
H∞-control, certainty equivalent LQR control, and LQR control knowing the dynamics of the system using
the reference signals in Fig. 1.
Figure 5: Tracking error for Gaussian disturbance for the presented Algorithm 1, versus the certainty
equivalentH∞-control, certainty equivalent LQR control, and LQR control knowing the dynamics of the
system using the reference signals in Fig. 1.
154–165, 2019.
Dimitri Bertsekas. Dynamic programming and optimal control: Volume I . Athena scientific, 2012.
Dimitri Bertsekas. Reinforcement learning and optimal control . Athena Scientific, 2019.
Ci Chen, Hamidreza Modares, Kan Xie, Frank L. Lewis, Yan Wan, and Shengli Xie. Reinforcement Learning-
Based Adaptive Optimal Exponential Tracking Control of Linear Systems with Unknown Dynamics. IEEE
Transactions on Automatic Control , 64(11):4423–4438, 2019. ISSN 15582523. doi: 10.1109/TAC.2019.
2905215.
Ci Chen, Lihua Xie, Yi Jiang, Kan Xie, and Shengli Xie. Robust output regulation and reinforcement
learning-based output tracking design for unknown linear discrete-time systems. IEEE Transactions on
Automatic Control , pp. 1–1, 2022.
Richard Cheng, Gábor Orosz, Richard M Murray, and Joel W Burdick. End-to-end safe reinforcement
learning through barrier functions for safety-critical continuous control tasks. In Proceedings of the AAAI
conference on artificial intelligence , volume 33, pp. 3387–3395, 2019.
16Published in Transactions on Machine Learning Research (01/2024)
Figure 6: Tracking error for the random walk disturbance for the presented Algorithm 1, versus certainty
equivalentH∞-control, certainty equivalent LQR control, certainty equivalent LQR for random walk, LQR
control, and LQR control for random walk disturbance knowing the dynamics of the system using the
reference signals in Fig. 1.
Figure 7: Tracking error for the uniformly sampled disturbance for the presented Algorithm 1, versus cer-
tainty equivalent H∞-control, certainty equivalent LQR control, and the LQR control knowing the dynamics
of the system using the reference signals in Fig. 1.
A. Chiuso and G. Pillonetto. System identification: A machine learning perspective. An-
nual Review of Control, Robotics, and Autonomous Systems , 2(1):281–304, 2019. doi: 10.1146/
annurev-control-053018-023744. URL https://doi.org/10.1146/annurev-control-053018-023744 .
Alon Cohen, Avinatan Hasidim, Tomer Koren, Nevena Lazic, Yishay Mansour, and Kunal Talwar. Online
linear quadratic control. In International Conference on Machine Learning , pp. 1029–1038. PMLR, 2018.
Diyako Dadkhah and SO Reza Moheimani. Combining H∞and resonant control to enable high-bandwidth
measurements with a MEMS force sensor. Mechatronics , 96:103086, 2023.
Chao Deng, Xiao-Zheng Jin, Wei-Wei Che, and Hai Wang. Learning-based distributed resilient fault-tolerant
control method for heterogeneous mass under unknown leader dynamic. IEEE Transactions on Neural
Networks and Learning Systems , 33(10):5504–5513, 2021.
Warren E Dixon, Marcio S de Queiroz, Darren M Dawson, and Terrance J Flynn. Adaptive tracking and
regulation of a wheeled mobile robot with controller/update law modularity. IEEE Transactions on control
systems technology , 12(1):138–147, 2004.
17Published in Transactions on Machine Learning Research (01/2024)
Figure 8: Tracking error for the adversary disturbance for the presented Algorithm 1, versus the H∞-control,
certainty equivalent H∞-control, certainty equivalent LQR control, and LQR control knowing the dynamics
of the system using the reference signals in Fig. 1.
John Doyle. Robust and optimal control. Proceedings of 35th IEEE Conference on Decision and Control , 2:
1595–1598 vol.2, 1995.
Quan-Yong Fan, Yucong Sun, and Bin Xu. Improved data-driven control design based on LMI and its
applications in lithium-ion batteries. IEEE Transactions on Circuits and Systems II: Express Briefs ,
2023.
Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite time identification in
unstable linear systems, 2017. URL https://arxiv.org/abs/1710.01852 .
Aleksandra Faust, Nick Malone, and Lydia Tapia. Preference-balancing motion planning under stochastic
disturbances. In 2015 IEEE International Conference on Robotics and Automation (ICRA) ,pp.3555–3562.
IEEE, 2015.
Weinan Gao and Zhong Ping Jiang. Global Optimal Output Regulation of Partially Linear Systems via
Robust Adaptive Dynamic Programming. IFAC-PapersOnLine , 48(11 11):742–747, 2015. ISSN 24058963.
doi: 10.1016/j.ifacol.2015.09.278. URL http://dx.doi.org/10.1016/j.ifacol.2015.09.278 .
Weinan Gao and Zhong Ping Jiang. Adaptive Dynamic Programming and Adaptive Optimal Output Reg-
ulation of Linear Systems. IEEE Transactions on Automatic Control , 61(12):4164–4169, 2016. ISSN
00189286. doi: 10.1109/TAC.2016.2548662.
Weinan Gao and Zhong-Ping Jiang. Learning-based adaptive optimal output regulation of linear and non-
linear systems: an overview. Control Theory and Technology , 20(1):1–19, 2022.
Weinan Gao, Zhong-Ping Jiang, Frank L. Lewis, and Yebin Wang. Cooperative optimal output regulation of
multi-agent systems using adaptive dynamic programming. In 2017 American Control Conference (ACC) ,
pp. 2674–2679, 2017.
Gaofeng Hao, Zhuang Fu, Xin Feng, Zening Gong, Peng Chen, Dan Wang, Weibin Wang, and Yang Si. A
deep deterministic policy gradient approach for vehicle speed tracking control with a robotic driver. IEEE
Transactions on Automation Science and Engineering , 19(3):2514–2525, 2021.
Elad Hazan, Sham M. Kakade, and Karan Singh. The Nonstochastic Control Problem. In Algorithmic
Learning Theory , pp. 408—-421, 2020. URL http://arxiv.org/abs/1911.12178 .
Jie Huang. Nonlinear output regulation: theory and applications . SIAM, 2004.
Alberto Isidori. Nonlinear control systems: an introduction . Springer, 1985.
18Published in Transactions on Machine Learning Research (01/2024)
Yi Jiang, Bahare Kiumarsi, Jialu Fan, Tianyou Chai, Jinna Li, and Frank L. Lewis. Optimal Output
Regulation of Linear Discrete-Time Systems with Unknown Dynamics Using Reinforcement Learning.
IEEE Transactions on Cybernetics , 50(7):3147–3156, 2020a. ISSN 21682275. doi: 10.1109/TCYB.2018.
2890046.
Yi Jiang, Bahare Kiumarsi, Jialu Fan, Tianyou Chai, Jinna Li, and Frank L. Lewis. Optimal output
regulation of linear discrete-time systems with unknown dynamics using reinforcement learning. IEEE
Transactions on Cybernetics , 50(7):3147–3156, 2020b.
Hassan K. Khalil. Nonlinear Systems . Prentice Hall, second edition, 2002.
Bahare Kiumarsi, Frank L. Lewis, Hamidreza Modares, Ali Karimpour, and Mohammad Bagher Naghibi-
Sistani. Reinforcement Q-learning for optimal tracking control of linear discrete-time systems with un-
known dynamics. Automatica , 50(4):1167–1175, 2014. ISSN 00051098. doi: 10.1016/j.automatica.2014.
02.015. URL http://dx.doi.org/10.1016/j.automatica.2014.02.015 .
Bahare Kiumarsi, Frank L Lewis, and Zhong-Ping Jiang. H∞control of linear discrete-time systems: Off-
policy reinforcement learning. Automatica , 78:144–152, 2017.
Frank. L. Lewis. Optimal control((book)). New York, Wiley-Interscience, 1986, 371 , 1986.
Bohao Li and Yunjie Wu. Path planning for uav ground target tracking via deep reinforcement learning.
IEEE access , 8:29064–29074, 2020.
Hongyi Li, Ying Wu, and Mou Chen. Adaptive fault-tolerant tracking control for discrete-time multiagent
systems via reinforcement learning algorithm. IEEE Transactions on Cybernetics , 51(3):1163–1174, 2020.
Lennart Ljung. System identification toolbox: User’s guide . Citeseer, 1995.
Lennart Ljung. System identification . Springer, 1998.
Alireza Mehrzad, Milad Darmiani, Yashar Mousavi, Miadreza Shafie-Khah, and Mohammadreza Aghamo-
hammadi. A review on data-driven security assessment of power systems: Trends and applications of
artificial intelligence. IEEE Access , 2023.
Di Mei, Jian Sun, Lihua Dou, and Yong Xu. Learning-based distributed adaptive control of heterogeneous
multi-agent systems with unknown leader dynamics. IET Cyber-Physical Systems: Theory & Applications ,
2022.
Hamidreza Modares, Frank L. Lewis, and Zhong Ping Jiang. H∞Tracking Control of Completely Unknown
Continuous-Time Systems via Off-Policy Reinforcement Learning. IEEE Transactions on Neural Networks
and Learning Systems , 26(10):2550–2562, 2015. ISSN 21622388. doi: 10.1109/TNNLS.2015.2441749.
Hamidreza Modares, Subramanya P Nageshrao, Gabriel A Delgado Lopes, Robert Babuška, and Frank L
Lewis. Optimal model-free output synchronization of heterogeneous systems using off-policy reinforcement
learning. Automatica , 71:334–341, 2016.
Mehdi Mohammadi, Mohammad Mehdi Arefi, Peyman Setoodeh, and Okyay Kaynak. Optimal tracking
control based on reinforcement learning value iteration algorithm for time-delayed nonlinear systems with
external disturbances and input constraints. Information Sciences , 554:84–98, 2021.
J. Nagumo and A. Noda. A learning method for system identification. IEEE Transactions on Automatic
Control, 12(3):282–287, 1967. doi: 10.1109/TAC.1967.1098599.
Antonin Novak, Laurent Simon, František Kadlec, and Pierrick Lotton. Nonlinear system identification using
exponential swept-sine signal. IEEE Transactions on Instrumentation and Measurement , 59(8):2220–2229,
2009.
Pedram Rabiee and Amirsaeid Safari. Safe exploration in reinforcement learning: Training backup control
barrier functions with zero training time safety violations, 2023.
19Published in Transactions on Machine Learning Research (01/2024)
Tuhin Sarkar, Alexander Rakhlin, and Munther A. Dahleh. Nonparametric finite time lti system identifica-
tion, 2019. URL https://arxiv.org/abs/1902.01848 .
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
Farzaneh Tatari, Majid Mazouchi, and Hamidreza Modares. Fixed-time system identification using con-
current learning. IEEE Transactions on Neural Networks and Learning Systems , pp. 1–11, 2021. doi:
10.1109/TNNLS.2021.3125145.
Kyriakos G Vamvoudakis, Arman Mojoodi, and Henrique Ferraz. Event-triggered optimal tracking control
of nonlinear systems. International Journal of Robust and Nonlinear Control , 27(4):598–619, 2017.
Licheng Wang, Engang Tian, Changsong Wang, and Shuai Liu. Secure estimation against malicious attacks
for lithium-ion batteries under cloud environments. IEEE Transactions on Circuits and Systems I: Regular
Papers, 69(10):4237–4247, 2022a.
Ning Wang, Ying Gao, Chen Yang, and Xuefeng Zhang. Reinforcement learning-based finite-time tracking
control of an unknown unmanned surface vehicle with input constraints. Neurocomputing , 484:26–37,
2022b.
Thomas Weber, Johannes Sossenheimer, Steffen Schäfer, Moritz Ott, Jessica Walther, and Eberhard Abele.
Machine learning based system identification tool for data-based energy and resource modeling and simula-
tion.Procedia CIRP ,80:683–688,2019. ISSN2212-8271. doi: https://doi.org/10.1016/j.procir.2018.12.021.
URL https://www.sciencedirect.com/science/article/pii/S2212827118313003 . 26th CIRP Con-
ference on Life Cycle Engineering (LCE) Purdue University, West Lafayette, IN, USA May 7-9, 2019.
Farnaz Adib Yaghmaie and Hamidreza Modares. Online optimal tracking of linear systems with ad-
versarial disturbances. Transactions on Machine Learning Research , 2023. ISSN 2835-8856. URL
https://openreview.net/forum?id=5nVJlKgmxp .
Zhikai Yao and Jianyong Yao. Toward reliable designs of data-driven reinforcement learning tracking control
for euler–lagrange systems. Neural Networks , 153:564–575, 2022.
SoroushZare, MohammadRezaHairiYazdi, MehdiTaleMasouleh, DanZhang, SahandAjami, andAmirhos-
sein Afkhami Ardekani. Experimental study on the control of a suspended cable-driven parallel robot for
object tracking purpose. Robotica, 40(11):3863–3877, 2022.
Hongwei Zhang and Frank L Lewis. Adaptive cooperative tracking control of higher-order nonlinear systems
with unknown dynamics. Automatica , 48(7):1432–1439, 2012.
20Published in Transactions on Machine Learning Research (01/2024)
A Real-world Application Example
In the main body of the paper, we compared the performance of the presented algorithm with LQR, H∞,
and their certainty equivalent versions. In this subsection, the implementation of the algorithm is brought
on a real-world application. The operational efficiency of lithium-ion batteries is inherently influenced by
various factors in real-world situations, such as ambient temperature, battery aging, and operational status.
These factors pose significant risks to the secure functioning of lithium batteries and may potentially lead to
undesirable damage to the connected equipment. Consider the following tracking a constant signal problem
where the dynamics of the system of a Lithium-ion battery from (Fan et al., 2023; Wang et al., 2022a) as
shown in Figure 10 is
xk+1=
1−1
R1C10 0
0 1−1
R2C20
0 0 1
xk+
1
C11
C2
−1
Cb
uk+wk, (23)
where nominal values are (C1,C2,Cb) = (39.91F,3.50F,40F)and(R1,R2) = (0.0269 Ω,7.3 Ω)and the
reference signal is generated by
zk+1=
1 0 0
0 1 0
0 0 1
zk, z0= [−0.02,−5.20,0.89]T,
rk=
1 0 0
0 1 0
0 0 1
zk.(24)
where
xk=
x1k
x2k
x3k
, wk=
w1k
w2k
w3k
, rk=
r1k
r2k
r3k
, ek=
e1k
e2k
e3k
=
x1k−r1k
x2k−r2k
x3k−r3k
.
A quadratic cost with Q=I3, R= 1is considered; that is
ck=eT
kQek+uT
kRuk.
A.1 Disturbances
Same as the previous simulation, in this section we brought the comparison of the different control algorithms
implemented on the system under different disturbances.
•Uniformly sampled disturbance It is considered the disturbance to be uniformly sampled from
the interval [0,0.5].
•Constant disturbance The constant disturbance is considered as w1k=w2k=w3k= 0.05.
•Amplitude modulation disturbance The disturbance is considered as w1k=w2k=w3k=
0.05×sin(6πk/500) sin(8πk/500).
•Sinusoidal disturbance A sinusoidal disturbance is considered as w1k=w2k=w3k= 0.05×
sin(8πk/100).
•Gaussian disturbance Gaussian disturbances is utilized where w1k∼ N (0,0.05)andw2k∼
N(0,0.05)in this case.
•Worst-case disturbance (Adversary) Same as explained in Section 5, the worst-case disturbance
is generated using the zero-sum game formulation.
21Published in Transactions on Machine Learning Research (01/2024)
Table 3: The final maximum and normal difference between the identified dynamics of the system (ˆA,ˆB)
and the actual one (A,B). "Maximum difference" refers to the maximum difference in the identification of
entries of matrices A and B
Disturbance Max Difference for A Norm of Difference for A Max Difference for B Norm of Difference for B
Constant 0.89 0.89 0.00 0.00
Amplitude mod. 0.09 0.11 0.00 0.00
Sinusoidal 0.19 0.25 0.00 0.00
Gaussian 0.32 0.45 0.00 0.00
Uniformly sam. 0.65 0.68 0.27 0.27
A.2 The compared control approaches
same as the previous simulation example, the below control algorithms are employed to assess their perfor-
mance against the previously mentioned disturbances.
•Online state tracking in Algorithm 1:During the execution of the algorithm, the value of Kis
maintained unchanged, which can be obtained based on a priori knowledge of the systems’ dynamics.
Note that this Kcan be any stabilizing controller that the algorithm is assumed to have access to.
The other parameters are chosen as H= 5,mr= 5,mw= 5, andη= 0.0001, andMandPare
initialized as zero matrices.
•LQR with the actual system matrices
•Certainty equivalent (C.E.) LQR
•Certainty equivalent (C.E.) H∞-control
The settings of the last three algorithms are the same as defined before.
A.3 Evaluation of the identification algorithm
In this subsection, the performance of the identification algorithm is discussed for the 5 cases of the dis-
turbance mentioned in the previous section. The identification period for this system is 293. In Table 3,
the maximum difference between the actual system and the identified one is summarized, as well as the L2
norm of their difference. As can be seen in Table 3, the performance of the system identification is the worst
for the "Constant disturbance" case with L2(A−ˆA) = 0.89which is similar to the results of the previous
example. The best identification accuracy happened for the "Amplitude modulation" case.
A.4 Evaluation of the Tracking Algorithm
for this example, the algorithms are run for T= 5000steps and the final average costs are brought in Table
4. Tracking the constant signals for the three states of the system t= [4900,5000]is depicted in Fig. 11 -
16. As it can be seen, due to the inherent limitations in computing the feedforward term during tracking,
particularly when matrix B lacks square dimensions, the accuracy of the feedforward term is compromised.
Consequently, across all algorithms reliant on this computation such as C.E. LQRandH∞, a discernible
decline in tracking performance becomes evident, emphasizing the superiority of the presented algorithm. It
can be seen in Table 4, that the presented algorithm outperforms all other methods in the tracking of the
defined reference signal.
22Published in Transactions on Machine Learning Research (01/2024)
Table 4: The final average cost, as introduced in equation 6, incurred by the different algorithms over a
duration of T= 5000steps is presented. Notably, the most competitive average cost values, indicated in
bold, are reported for each respective disturbance case.
Disturbance Algorithm 1 C.E. LQR C.E.H∞LQR
Constant 2.35 28.28 51.15 30.77
Amplitude mod. 1.34 23.32 2.02 22.80
Sinusoidal 1.17 27.36 20.94 22.63
Gaussian 1.19 30.51 70.02 22.63
Uniformly sam. 1.94 31.45 26.01 23.61
Adversarial 1.77 12.66 2.60 7.58
Appendix B: Theoretical Results
B Results related to the memory augmented controller
In this section, a list of results related to the memory-augmented controllers is brought.
•Lemma 3 provides bounds for ΨK,h
k,yandψK,h
k,z.
•Lemma 4 gives bounds on the states and inputs.
•Lemma 5 provides a bound on the tracking error.
•Lemma 6 defines the Lipschitz condition on the truncated cost.
•Lemma 7 gives a bound on the gradient of the truncated cost for the tracking algorithm.
Lemmas 3, 6-7 are given in (Yaghmaie & Modares, 2023) and Lemmas 4-5 are inspired by (Yaghmaie &
Modares, 2023) and are essential in proving the main results in Theorem 4.
Lemma 3 (Lemma 4 in (Yaghmaie & Modares, 2023)) Let Assumptions 1-5 hold. Suppose that K
is(κ,γ)-strongly stable. Then,
∥ΨK,h
k,y∥≤κ2(1−γ)yIy≤h−1+mwκ5κ2
b(1−γ)y−1,
∥ψK,h
k,z∥≤mrκ5κ2
b(1−γ)z−1.(25)
Lemma 4 Let Assumptions 1-5 hold. Define
Y0:k:= [M0:k,P0:k],
YH,k:= [Mk−H:k,Pk−H:k].
D:=γ−1κwκ3+ (κrmr+κwmw)(1−γ)−1κ6κ2
b
1−κ2(1−γ)H+(κw+κbκκz)κbκ3
γ,
Suppose that KandK∗
fbare(κ,γ)-strongly stable. Define xlin
k(K∗
fb,K∗
ff)as the system state corresponding
to an optimal linear feedback controller. Then, one has
max(∥xπ
k(Y0:k−1)∥,∥˜xπ
k(YH,k−1)∥,∥xlin
k(K∗
fb,K∗
ff)∥)≤D, (26)
max(∥uπ
k(Y0:k)∥,∥˜uπ
k(YH,k)∥)≤D,
∥xπ
k(Y0:k−1)−˜xπ
k(YH,k−1)∥≤κ2(1−γ)HD,
∥uπ
k(Y0:k)−˜uπ
k(YH,k)∥≤κ3(1−γ)HD.
23Published in Transactions on Machine Learning Research (01/2024)
Proof:Using equation 11, one has
∥xπ
k∥≤∥ ˜AH
K∥∥xπ
k−H∥+κwmw+H−1/summationdisplay
y=0∥ΨK,H
k,y(Mk−H−1:k−1)∥+κrmr+H−1/summationdisplay
z=0∥ψK,H
k,z(Pk−H−1:k−1)∥
≤κ2(1−γ)H∥xπ
k−H∥+κwγ−1(κ2+mwκ5κ2
b(1−γ)−1) +κrγ−1(mrκ5κ2
b)(1−γ)−1.
The above recursion satisfies
∥xπ
k∥≤γ−1κwκ2+ (κrmr+κwmw)(1−γ)−1κ5κ2
b
1−κ2(1−γ)H.
Similarly, from equation 13, one has
∥˜xπ
k(YH,k−1)∥≤mw+H−1/summationdisplay
y=0∥ΨK,H
k,y(Mk−H−1:k−1)wk−y−1∥+mr+H−1/summationdisplay
z=0∥ψK,H
k,z(Pk−H−1:k−1)rk−z∥
≤γ−1κwκ2+γ−1(κwmw+κrmr)κ5κ2
b(1−γ)−1≤D.
where the last inequality is obtained because 0≤1−κ2(1−γ)H≤1. Moreover,
∥xlin
k(K∗
fb,K∗
ff)∥=∥k−1/summationdisplay
y=0˜Ay
K∗
fbwk−y−1+k−1/summationdisplay
i=0˜Ai
K∗
fbBK∗
ffzk−i∥
≤γ−1κ2(κw+κκbκz)≤D.
Besides, one has
∥uπ
k(Y0:k)∥=∥Kxπ
k(Y0:k−1) +mw/summationdisplay
t=1M[t−1]wk−t+mr−1/summationdisplay
s=0P[s]rk−s∥
≤κ∥xπ
k(Y0:k−1)∥+κwmw/summationdisplay
t=1κbκ3(1−γ)(t−1)+κrmr−1/summationdisplay
s=0κbκ3(1−γ)s
≤γ−1κwκ3+ (κrmr+κwmw)(1−γ)−1κ6κ2
b
1−κ2(1−γ)H+(κw+κr)κbκ3
γ≤D.
Similarly,
∥˜uπ
k(YH,k)∥=∥K˜xπ
k(YH,k−1) +mw/summationdisplay
t=1M[t−1]wk−t+mr−1/summationdisplay
s=0P[s]rk−s∥
≤κ∥˜xπ
k(YH,k−1)∥+κwmw/summationdisplay
t=1κbκ3(1−γ)(t−1)+κrmr−1/summationdisplay
s=0κbκ3(1−γ)s
≤γ−1κwκ3+γ−1(κwmw+κrmr)κ6κ2
b(1−γ)−1+(κw+κr)κbκ3
γ≤D.
To bound the difference between the actual and truncated state, from equation 13 and equation 11, one has
∥xπ
k(Y0:k−1)−˜xπ
k(YH,k−1)∥=∥˜AH
Kxπ
k−H(Y0:k−H−1)∥≤κ2(1−γ)HD,
which gives
∥uπ
k(Y0:k)−˜uπ
k(YH,k)∥≤∥K∥∥˜AH
Kxπ
k−H(Y0:k−H−1)∥≤κ3(1−γ)HD.
This completes the proof.
24Published in Transactions on Machine Learning Research (01/2024)
Lemma 5 Let Assumptions 1-5 hold. Suppose that Kis(κ,γ)-strongly stable. Define the tracking error as
ek:=xπ
k(Y0:k−1)−Fzk.
Then,
∥ek∥≤κwγ−1(κ2+mwκ5κ2
b(1−γ)) +κrγ−1(1−γ)l−1mrκ5κ2
b+κrl−1/summationdisplay
z=0κz.
Proof:Without loss of generality and for simplicity, assume that ∥F∥=∥F−1∥≤κ. The tracking error
reads
ek=k−1/summationdisplay
y=0ΨK,k
k,y(M0:k−1)wk−y−1+k−1/summationdisplay
z=0ψK,k
k,z(P0:k−1)rk−z−Fzk.
Using the bounds in equation 25
∥ek∥≤k−1/summationdisplay
y=0(κ2(1−γ)yIy≤k−1+mwκ5κ2
b(1−γ)y−1)κw+κκz+k−1/summationdisplay
z=0mrκ5κ2
b(1−γ)z−1κr
≤κwγ−1(κ2+mwκ5κ2
b(1−γ)−1) +κκz+κrγ−1mrκ5κ2
b,
which is based on the fact that/summationtextN
n=0(1−γ)n≤1
γin the last inequality.
Lemma 6 (Lemma 7 in (Yaghmaie & Modares, 2023)) Let Assumptions 1-5 hold. Define YH,k=
[Y1,...,Yt,...,Y 2H] = [Mk−H:kPk−H:k]and ˜YH,k= [Y1,...,˜Yt,...,Y 2H]where ˜YH,khas all its elements the
same asYH,k, except one element. Then, the truncated cost function in equation 15 satisfies the following
Lipschitz condition
|fk(Y1,,...,Yt,...,Y 2H)−fk(Y1,,..., ˜Yt,...,Y 2H)|≤Lf∥Yt−˜Yt∥
where
Lf:= 3GcDκbκ3(κr+κw).
Lemma 7 (Lemma 8 in (Yaghmaie & Modares, 2023)) Let Assumptions 1-5 hold. The following gra-
dient bound is satisfied
∥∇YH,kfk(YH,k)∥F≤6Hd2Gc(κr+κw)κbκ3γ−1=:Gf
whered= max(n,m).
C Results related to Algorithm SysId
In this subsection, a list of the properties related to Algorithm SysId is brought. More specifically:
•Theorem 3 gives the bounds on the estimated dynamics.
•Lemma 8 gives bounds on the state and the input while Algorithm SysId is running.
To this end, Some additional notations will be required along with the proofs. Let
•J(A|A,B,{w},{r})be representing the total cost associated with executing the algorithm Aover
theTtime steps. With some abuse of notations, one can say J(K|A,B,{w},{r})shows the total
cost associated with executing the linear controller K,
25Published in Transactions on Machine Learning Research (01/2024)
•xk(A|A,B,{w},{r})be the state visited at time k, and
•uk(A|A,B,{w},{r})be the control input at time k.
Also, if instead of (A,B)in the above notations, (ˆA,ˆB)are used, it means that they are associated with the
identified system instead of the actual one.
Theorem 3 (Theorem 19 in (Hazan et al., 2020)) If the system identification algorithm is run for T0
steps, the output pair (ˆA,ˆB)satisfies, with probability 1−δ, that∥ˆA−A∥T0,∥ˆB−B∥T0≤ϵA,B, where
T0= 103λmn2κ10κ2
w
γ2ϵ2
A,Blogκmn
δ.
Lemma 8 Assume that Algorithm SysId is run for T0steps. Select the input as uk=Kxk+ηk, where
ηk= [ηk1,...,ηkm]T, ηkj∼{± 1}, j= 1,...,m. Define
Did:=κ3
γ(κw+κb√m) +√m.
Then,
∥xk∥≤Did,∥uk∥≤Did, (27)
J0=T0/summationdisplay
k=1∥ck(xk,uk)−ck
minK∈K(xk,uk)∥≤4T0GcD2
id. (28)
Proof.From the strong stability of K, forxk, one has
∥xk+1∥≤∥k/summationdisplay
i=0(A+BK)k−i(wi+Bηi)∥
≤k/summationdisplay
i=0κ2(1−γ)k−i(κw+κb∥ηi∥).
Based onηkj∼{± 1}m, one has∥ηk∥≤√m. As a result, using the fact that/summationtextk
i=0(1−γ)i≤1
γ
∥xk∥≤κ2
γ(κw+κb√m). (29)
For∥uk∥, one can derive the following
∥uk∥≤∥Kxk+ηk∥≤κκ2
γ(κw+κb√m) +√m=:Did.
Next, an upper bound for ∥ck(xk,uk)−minK∈Kck(xk,uk)∥is computed. Based on Assumption 5, one has
∥ck(xk,uk)−min
K∈Kck(xk,uk)∥
≤GcDid∥xk−xk(KOpt|A,B,{w},{r})∥+GcDid∥uk−uk(KOpt|A,B,{w},{r})∥
≤2GcD2
id+ 2GcD2
id≤4GcD2
id.
The result in equation 28 is concluded by summing the above inequality over T0steps.
26Published in Transactions on Machine Learning Research (01/2024)
D Result related to Algorithm 1
Thefollowinglemmaprovidesanestimationoftheupperboundsforthestate,controlinput,andperturbation
during the tracking stage.
Lemma 9 In the tracking step of Algorithm 1 for k≥T0+ 1subsequently,
∥xπ
k∥≤κ2
γ(κw+κ2
bκ3
γ(κw+Ew,T 0+κ2
γ(κw+κb√m)) +κ2
bκ3
γκr) =:Dx, (30)
∥uπ
k∥≤κDx+κbκ3
γ(κw+Ew,T 0+κ2
γ(κw+κb√m)) +κbκ3
γκr=:Du. (31)
and
∥ˆwk−wk∥≤ϵA,B(Dx+ (κDx+κ3κb
γ(κw+Ew,T 0+κ2
γ(κw+κb√m)) +κ3κb
γκr)) =:Ew,(32)
ϵA,B(κDx,T0+κbκ3
γ(κ2
γ(κw+κb√m)) +κbκ3
γκr) =:Ew,T 0
whereDx, DuandEware the upper bounds to the state, control input and perturbation estimation error,
respectively. Ew,T 0is the upper bound on the perturbation error at k=T0.
Proof.We prove the result by induction. First note that for equation 1 if the input is chosen as
uπ
k=Kxk+mw/summationdisplay
j=1M[j−1]ˆwk−j+mr/summationdisplay
s=0P[s]rk−s, (33)
the statexπ
k+1reads
xπ
k+1=k/summationdisplay
i=0(A+BK)k−i(wi+Bmw/summationdisplay
j=1M[j−1]ˆwk−j+Bmr/summationdisplay
s=0P[s]rk−s).
As a result,
∥xπ
k+1∥≤∥k/summationdisplay
i=0(A+BK)k−i(wi+Bmw/summationdisplay
j=1M[j−1]ˆwk−j+Bmr/summationdisplay
s=0P[s]rk−s)∥ (34)
≤k/summationdisplay
i=0κ2(1−γ)k−i(κw+κb∥ˆwk∥mw/summationdisplay
j=1κbκ3(1−γ)t−1+κb∥rk∥mr/summationdisplay
s=0κbκ3(1−γ)s).
At time step k=T0, one defines ˆwk=xT0whose upper bound is computed in equation 29. Thus, for k=T0,
one has
∥ˆwT0∥≤κ2
γ(κw+κb√m).
Based on the fact that/summationtextk
i=0(1−γ)i≤1
γfork=T0, one gets
∥xπ
T0+1∥≤κ2
γ(κw+κ2
bκ3
γ(κ2
γ(κw+κb√m)) +κ2
bκ3
γκr) =:Dx,T0+1.
27Published in Transactions on Machine Learning Research (01/2024)
Then,
∥uπ
T0+1∥≤∥Kxπ
k+1∥+mw/summationdisplay
j=1∥M[j−1]ˆwk−j∥+mr/summationdisplay
s=0∥P[s]rk−s∥
≤κDx,T0+1+∥ˆwk∥mw/summationdisplay
j=1κbκ3(1−γ)t−1+∥rk∥mr/summationdisplay
s=0κbκ3(1−γ)s
≤κDx,T0+1+κbκ3
γ(κ2
γ(κw+κb√m)) +κbκ3
γκr:=Du,T0+1.
Assuming that∥A−ˆA∥,∥B−ˆB∥≤ϵA,B, atk>T 0
∥ˆwk−wk∥≤∥ ((A−ˆA)xk+ (B−ˆB)uk)∥≤ϵA,B∥xk∥+ϵA,B∥uk∥.
Thus,
∥ˆwk−wk∥≤ϵA,B(κ2
γ(κw+κ2
bκ3
γ(κ2
γ(κw+κb√m) +κ2
bκ3
γκr))+
ϵA,B(κDx,T0+κbκ3
γ(κ2
γ(κw+κb√m)) +κbκ3
γκr) :=Ew,T 0,
and
∥ˆwk∥≤max(Ew,T 0+κw,∥ˆwT0∥).
Then, if the upper bound for ∥ˆwk∥is replaced with Ew,T 0+κw+∥ˆwT0∥in equation 34, the bounds in
equation 30-equation 31 are concluded.
E Helper results for the proof of the regret bound
To prove the regret bound, a few results are needed.
•Lemma 10 is a technical result to be used in Lemma 11.
•Lemma 11 gives an upper bound for the difference in the costs for the real and estimated systems
using a linear controller.
•Lemma 12 provides an upper bound for the difference between the cost of using the tracking algo-
rithm and the minimum cost that can be achieved by the same class of controller.
Lemma 10 For any matrix pair P,∆P, such that∥P∥,∥P+ ∆P∥≤1−γ, it holds
∞/summationdisplay
i=0∥(P+ ∆P)i−Pi∥≤∥∆P∥
γ2.
Proof.This proof is based on an inductive argument. First,we prove that the inequality ∥(P+∆P)i−Pi∥≤
∥∆P∥i(1−γ)i−1holds true. Then, the validity of this claim can be easily verified for i= 0andi= 1. Next,
it is shown that this claim is valid for the case i+ 1. Observe that
∥(P+ ∆P)i+1−Pi+1∥≤∥ (P+ ∆P)(P+ ∆P)i−(P)(P)i∥
≤∥P((P+ ∆P)i−Pi) + ∆P(P+ ∆P)i∥
≤∥P((P+ ∆P)i−Pi)∥+∥∆P(P+ ∆P)i∥
28Published in Transactions on Machine Learning Research (01/2024)
Itisknownfromtheclaimthat ∥(P+∆P)i−Pi∥≤i∥∆P∥(1−γ)i−1anditisassumedthat ∥P+∆P∥≤1−γ,
thus,
∥P((P+ ∆P)i−Pi)∥+∥∆P(P+ ∆P)i∥≤∥P∥∥((P+ ∆P)i−Pi)∥+∥∆P(P+ ∆P)i∥
≤(1−γ)i∥∆P∥(1−γ)i−1+∥∆P∥(1−γ)i
≤(i+ 1)(1−γ)i∥∆P∥.
Then,
∞/summationdisplay
i=0∥(P+ ∆P)i−Pi∥=∞/summationdisplay
i=−1∥(P+ ∆P)(i+1)−P(i+1)∥≤
∞/summationdisplay
i=−1(i+ 1)(1−γ)i∥∆P∥=∞/summationdisplay
i=0(i)(1−γ)i−1∥∆P∥.
(Hazan et al., 2020) in Lemma 17 showed that/summationtext∞
i=0i(1−γ)i−1≤1
γ2. Thus, the proof is concluded.
Lemma 11 Assuming that∥A−ˆA∥≤ϵA,B,∥B−ˆB∥≤ϵA,B, whereϵA,B≤0.25k−3γ, and thatKis(κ,γ)-
strongly stable with respect to the pair (A,B). Then, from Lemma 9, for any perturbation sequence satisfying
∥wk−ˆwk∥≤Ew, and it is assumed that ∥ˆw0∥≤W0, the following statement holds
|J(K|ˆA,ˆB,{ˆw},{r})−J(K|A,B,{w},{r})|≤poly(κ,1
γ,λ,m,n,κ w,κz)GcTT−1/2
0.
Proof.One has∥L∥≤1−γfor(A,B)and∥ˆL∥≤1−γ+ 2κ3ϵA,Bfrom Lemma 1. It can be said that
ˆL=L+∆Land∥∆L∥≤2κ3ϵA,B. Using Lemma 10 for LandˆLit can be stated that if one take ϵA,B=γ
4κ3,
one will have∥L∥and∥ˆL∥≤1−γ
2. The linear controller K is as equation 3, and is (κ,γ)-strongly stable
for (A, B). It holds
xk(K|A,B,{w},{r}) =k/summationdisplay
i=0(A+Bkfb)i(BKffzk−i+wk−i).
Thus, with the assumption that κb≤κwithout any loss of generality,
∥xk(K|A,B,{w},{r})∥≤κ4κz
γ+κ2κw
γ.
Similarly it can be said that by Lemma 1, since ϵA,B=γ
4κ3,κ > 1, and 0≤γ≤1one has∥ˆA∥ ≤
∥A∥+ϵA,Band∥ˆB∥≤∥B∥+ϵA,B, hence
∥ˆA∥≤κ+γ
4κ3≤κ+1
4κ3≤2κ,
∥ˆB∥≤κ+γ
4κ3≤κ+1
4κ3≤2κ.
Thus, K is (2κ,γ
2)-strongly stable for (ˆA,ˆB)and
∥xk(K|ˆA,ˆB,{ˆw},{r})∥≤(2κ)4κz
γ/2+(2κ)2κz
γ/2W0≤32κ4κz
γ+8κ2W0
γ.
29Published in Transactions on Machine Learning Research (01/2024)
Subsequently:
∥xk+1(K|A,B,{w},{r})−xk+1(K|ˆA,ˆB,{ˆw},{r})∥
≤k/summationdisplay
i=0∥(A+BKfb)iwk−i−(ˆA+ˆBkfb)iˆwk−i+ (A+BKfb)iBKffzk−i−(ˆA+ˆBKfb)iˆBKffzk−i∥
≤k/summationdisplay
i=0(∥(A+BKfb)iwk−i−(A+BKfb)iˆwk−i∥+∥(A+BKfb)iˆwk−i−(ˆA+ˆBkfb)iˆwk−i∥
+∥(A+BKfb)iBKffzk−i−(ˆA+ˆBKfb)iˆBKffzk−i∥).
Starting from the first term, one has
k/summationdisplay
i=0∥(A+BKfb)iwk−i−(A+Bkfb)iˆwk−i∥)≤κ2Ew
γ+κ2(1−γ)kW0.
For the second term, one has ∥ˆA+ˆBkfb∥≤Q−1ˆLQand∥A+Bkfb∥≤Q−1LQ. Since∥L∥,∥ˆL∥≤1−γ
2,
then∥ˆA+ˆBkfb∥,∥A+Bkfb∥≤Q−1(1−γ
2)Q. Additionally,∥Q∥,∥Q−1∥≤κ. Using Lemma 10 and knowing
∥∆L∥≤2κ3ϵA,B,
k/summationdisplay
i=0(∥(A+BKfb)iˆwk−i−(ˆA+ˆBkfb)iˆwk−i∥)
≤k/summationdisplay
i=0(∥(Q−1LQ)iˆwk−i−(Q−1ˆLQ)iˆwk−i∥)
≤κ2W0k/summationdisplay
i=0∥(L)i−(ˆL)i∥
≤8κ5ϵA,B
γ2W0.
For the third term, following the same steps from the last part,
k/summationdisplay
i=0∥((A+BKfb)iBKffzk−i−(ˆA+ˆBKfb)iˆBKffzk−i)∥
≤4κ4(2κ3ϵA,Bκz)
γ2.
Thus,
∥xk+1(K|A,B,{w},{r})−xk+1(K|ˆA,ˆB,{ˆw},{r})∥
≤κ2Ew
γ+κ2(1−γ)kW0+8W0κ5ϵA,B
γ2+8κ7ϵA,Bκz
γ2.
From assumption 5, it can be shown that ∥xk∥,∥uk∥≤Dand∥∇xck(x,u)∥,∥∇uck(x,u)∥≤GcD. With
abuse of the use of the notation, let ˆDdenote the bound related to the identified system. It holds:
uk+1(K|ˆA,ˆB,{ˆw},{r}) =Kfbxk+1(K|ˆA,ˆB,{ˆw}) +Kffzk+1.
30Published in Transactions on Machine Learning Research (01/2024)
Thus,
∥uk+1(K|ˆA,ˆB,{ˆw},{r})∥≤8κ3W0
γ+32κ5κz
γ+κκz=:ˆD.
As a result, we have
|J(K|ˆA,ˆB,{ˆw},{r})−J(K|A,B,{w},{r})|≤TGcˆD∥xk+1(K|A,B,{w},{r})−xk+1(K|ˆA,ˆB,{ˆw},{r})∥
≤TGc(8κ3W0
γ+32κ5κz
γ+κκz)(8κ5W0ϵA,B
γ2+8κ7ϵA,Bκz
γ2+κ2(1−γ)kW0+κ2Ew
γ).
From equation 32, one can derive Ew≤poly(κ,κb,m,κw,γ−1,κr)ϵA,B, andW0≤Ew. Also, from Theorem
3, it is known that T0=ϵ−2
A,Bpoly(κ,κw,m,n ). Thus, one can write the above inequality as
|J(K|ˆA,ˆB,{ˆw},{r})−J(K|A,B,{w},{r})|≤poly(κ,1
γ,λ,m,n,κ w,κz)GcTT−1/2
0.
Lemma 12 Let Assumptions 1-5 hold. Let xπ∗
kdenote the state using the optimal memory-augmented
controlleruπ∗
k. SetH=mw=mr. LetY∗
H,k:= [M∗,..,M∗,P∗,...,P∗]denote the optimal weights learned
by Algorithm 1, each one of the weights repeated for Htimes, and ˜xπ
k(Y∗
H,k),˜uπ
k(Y∗
H,k)denote the truncated
states and control using these optimal weights according to equation 13-equation 14. Then
|ck(˜xπ
k(Y∗
H,k)−rk,˜uπ
k(Y∗
H,k))−ck/parenleftbig
xπ∗
k−rk,uπ∗
k/parenrightbig
|≤2GcD2κ3(1−γ)H.
Proof of Lemma 12: Stacking optimal learned weights for k times makes Y∗
0:k:= [M∗,...,M∗,P∗,...,P∗], and
then stacking them for Htimes defines Y∗
H,k:= [M∗,...,M∗,P∗,...,P∗]. Based on Lemma 4, one has
|ck(˜xπ
k(Y∗)−rk,˜uπ
k(Y∗))−ck/parenleftbig
xπ∗
k−rk,uπ∗
k/parenrightbig
|≤GcD∥(xK
k(Y∗
0:k−1)−rk)−(˜xπ
k(Y∗
H,k)−rk)∥
+GcD∥uK
k(Y∗
0:k−1)−˜uπ
k(Y∗
H,k)∥≤2GcD2κ3(1−γ)H.
This completes the proof.
Theorem 4 SupposeA:=Algorithm RobTrack is executed under Assumptions 1-5. Let H=mw=mr.
Select the learning rate ηand the memory size Hto satisfyη=O(1
Gcκw√
T),H=O(logκ2T
γ), andT0=T2/3.
The regret of Algorithm RobTrack on the identified system (ˆA,ˆB)and the perturbation {ˆw}is
J(A|ˆA,ˆB,{ˆw},{r})−J(Aopt|ˆA,ˆB,{ˆw},{r}) =O(√
T),
whereJ(Aopt|ˆA,ˆB,{ˆw},{r})denotes the total cost associated with the optimal memory-augmented policy
uπ∗
k.
31Published in Transactions on Machine Learning Research (01/2024)
Proof:To begin, one has
J(A|A,B,{w},{r})−J(Aopt|A,B,{w},{r}) =T/summationdisplay
k=1ck(ek,uk)−T/summationdisplay
k=1ck/parenleftbig
xπ∗
k−rk,uπ∗
k)
=T/summationdisplay
k=1ck(ek(Y0:k−1),uk(Y0:k−1))−T/summationdisplay
k=1fk(YH,k)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
αT
+T/summationdisplay
k=1fk(YH,k)−T/summationdisplay
k=1fk(Y∗)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
βT
+T/summationdisplay
k=1fk(Y∗)−T/summationdisplay
k=1ck/parenleftbig
xπ∗
k−rk,uπ∗
k)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
ζT,
whereY∗= [M[0]∗,...,M[H−1]∗,P[0]∗,...,P[H−1]∗]∈(Rm×n)2Hdenote the optimal weights learned by Algo-
rithm 1, satisfying the conditions in 3.
The regret analysis is split into three parts: αTdenotes the difference between the cost of Algorithm 1 and
the truncated cost. βTdenotes the difference between the truncated and optimal truncated costs. ζTdenotes
the difference between the optimal truncated cost and the optimal memory-augmented control policy.
The bound of the first term αTis given by
|ck(ek,uk)−fk(YH,k)|≤GcD∥(xK
k(Y0:k−1)−rk)−(˜xπ
k(YH,k)−rk)∥+GcD∥uK
k(Y0:k−1)−˜uπ
k(YH,k)∥
≤2GcD2κ3(1−γ)H,
where one can use Lemma 4 to get the above result. Therefore,
∥αT∥=∥T/summationdisplay
k=1ck(ek,uk)−T/summationdisplay
k=1fk(YH,k)∥≤2TGcD2κ3(1−γ)H=O(√
T), (35)
where the last equality is obtained based on H=O(logT ).
The termβTcan be bounded by Theorem 4.6 of Agarwal et al. (2019) and the results of Lemmas 6 and 7 as
T/summationdisplay
k=1fk(YH,k)−T/summationdisplay
k=1fk(Y∗)≤1
ηM2
b+TG2
fη+LfH2ηGfT,
whereMb:= 2√
dκbκ3γ−1, d= max(n,m). By selecting η=O(1
Gcκw√
T), H =O(log(T)), thenβT=
O(√
T).
The last term is the difference between the truncated cost of the algorithm and the cost by the optimal
memory-augmented controller. For the third term, using Lemma 12,
T/summationdisplay
k=1fk(Y∗
H,k)−T/summationdisplay
k=1ck/parenleftbig
xπ∗
k−rk,uπ∗
k/parenrightbig
≤2TGcD2κ3(1−γ)H=O(√
T),
where the last equality is obtained based on H=O(log(T)). Observe that
•IfϵA,B≤γ
4κ3, Lemma 1 guarantees that kis(2κ,γ
2)-strongly stable on (ˆA,ˆB),
•IfϵA,B≤poly(κ,1
γ), the iterates obtained by running Algorithm A(trajectory tracking algorithm)
satisfy∥xk∥,∥ˆwk∥≤poly(κ,1
γ,n)(1 +κw), as guaranteed by Lemma 9.
32Published in Transactions on Machine Learning Research (01/2024)
Given the aforementioned observations and the proof before that, it is ensured that
J(A|ˆA,ˆB,{ˆw},{r})−J(Aopt|ˆA,ˆB,{ˆw},{r})≤O(√
T).
Also, for the sake of completeness and self-containment of this article, it can be mentioned that from (Yagh-
maie & Modares, 2023), it is known that
J(A|A,B,{w},{r})−J(K|A,B,{w},{r})≤O(√
T),
and following the same steps as before one can conclude
J(A|ˆA,ˆB,{ˆw},{r})−J(K|ˆA,ˆB,{ˆw},{r})≤O(√
T).
33Published in Transactions on Machine Learning Research (01/2024)
(a) Gaussian
 (b) Constant
(c) Random walk
 (d) Amplitude modulation
(e) Uniformly sampled
 (f) Sinusoidal
(g) Adversary
Figure 9: The changes in the average cost ¯JT, as given by equation 6 over time Tis compared between
Algorithm 1 and other control methods, namely certainty equivalent H∞-control, certainty equivalent LQR
control, and LQR control with knowledge of system dynamics, for different types of disturbances such as
Gaussian, random walk, uniformly sampled, constant, amplitude modulation, sinusoidal, and adversary
disturbances.
34Published in Transactions on Machine Learning Research (01/2024)
Figure 10: Circuit of a Lithium-ion battery.
Figure 11: Tracking performance for constant disturbance for the presented Algorithm 1, versus certainty
equivalentH∞-control, certainty equivalent LQR control, and LQR control knowing the dynamics of the
system.
35Published in Transactions on Machine Learning Research (01/2024)
Figure12: TrackingperformanceforamplitudemodulationdisturbanceforthepresentedAlgorithm1, versus
certainty equivalent H∞-control, certainty equivalent LQR control, and LQR control knowing the dynamics
of the system.
Figure 13: Tracking performance for sinusoidal disturbance for the presented Algorithm 1, versus certainty
equivalentH∞-control, certainty equivalent LQR control, and LQR control knowing the dynamics of the
system.
36Published in Transactions on Machine Learning Research (01/2024)
Figure14: TrackingperformanceforGaussiandisturbanceforthepresentedAlgorithm1, versusthecertainty
equivalentH∞-control, certainty equivalent LQR control, and LQR control knowing the dynamics of the
system.
Figure 15: Tracking performance for the uniformly sampled disturbance for the presented Algorithm 1,
versus certainty equivalent H∞-control, certainty equivalent LQR control, and the LQR control knowing the
dynamics of the system.
37Published in Transactions on Machine Learning Research (01/2024)
Figure 16: Tracking performance for the adversary disturbance for the presented Algorithm 1, versus the
H∞-control, certainty equivalent H∞-control, certainty equivalent LQR control, and LQR control knowing
the dynamics of the system.
38Published in Transactions on Machine Learning Research (01/2024)
(a) Gaussian
 (b) Constant
(c) Amplitude modulation
 (d) Uniformly sampled
(e) Sinusoidal
 (f) Adversary
Figure 17: The changes in the average cost ¯JT, as given by equation 6, over varying values of T, is com-
pared between Algorithm 1 and other control methods, namely certainty equivalent H∞-control, certainty
equivalent LQR control, and LQR control with knowledge of system dynamics, for different types of dis-
turbances such as Gaussian, uniformly sampled, constant, amplitude modulation, sinusoidal, and adversary
disturbances.
39