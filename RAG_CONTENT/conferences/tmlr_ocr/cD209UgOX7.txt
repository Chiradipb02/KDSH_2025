Published in Transactions on Machine Learning Research (09/2024)
Scaling Up Bayesian Neural Networks with Neural Networks
Zahra Moslemi zmoslemi@uci.edu
Department of Statistics
University of California
Irvine, CA, USA
Yang Meng mengy13@uci.edu
Department of Statistics
University of California
Irvine, CA, USA
Shiwei Lan slan7@asu.edu
School of Mathematical and Statistical Sciences
Arizona State University
Tempe, AZ, USA
Babak Shahbaba babaks@uci.edu
Department of Statistics
University of California
Irvine, CA, USA
Reviewed on OpenReview: https: // openreview. net/ forum? id= cD209UgOX7
Abstract
Bayesian Neural Networks (BNNs) offer a principled and natural framework for proper un-
certainty quantification in the context of deep learning. They address the typical challenges
associated with conventional deep learning methods, such as data insatiability, ad-hoc na-
ture, and susceptibility to overfitting. However, their implementation typically either relies
on Markov chain Monte Carlo (MCMC) methods, which are characterized by their com-
putational intensity and inefficiency in a high-dimensional space, or variational inference
methods, which tend to underestimate uncertainty. To address this issue, we propose a
novel Calibration-Emulation-Sampling (CES) strategy to significantly enhance the compu-
tational efficiency of BNN. In this framework, during the initial calibration stage, we collect
a small set of samples from the parameter space. These samples serve as training data
for the emulator, which approximates the map between parameters and posterior proba-
bility. The trained emulator is then used for sampling from the posterior distribution at
substantially higher speed compared to the standard BNN. Using simulated and real data,
we demonstrate that our proposed method improves computational efficiency of BNN, while
maintaining similar performance in terms of prediction accuracy and uncertainty quantifi-
cation.
1 Introduction
In recent years, Deep Neural Networks (DNN) have emerged as the predominant driving force in the field
of machine learning and are regarded as fundamental tools for many intelligent systems (Cheng et al.,
2018; LeCun et al., 1998; Sze et al., 2017). While DNN have demonstrated significant success in prediction
tasks, they often struggle with accurately quantifying uncertainty. Additionally, due to their vulnerability
to overfitting, they can generate highly confident yet erroneous predictions (Su et al., 2019; Kwon et al.,
2022). In recent years, there have been some attempts to address this issue. For example, the Ensemble
1Published in Transactions on Machine Learning Research (09/2024)
Deep Learning method (Lakshminarayanan et al., 2017) aggregates predictions from multiple models to
improve reliability and uncertainty estimates. While these methods represent important progress in the right
direction, developing a principled and computationally efficient framework for Uncertainty Quantification
(UQ) within the context of deep learning remains a significant challenge and active area of research. This
is especially important in domains where critical decisions, such as medical diagnostics, are involved. To
address these issues, Bayesian Neural Networks (BNNs) (MacKay, 1992; Neal, 2012; Jospin et al., 2022)
have emerged as an alternative to standard DNN, providing a more reliable framework within the field of
machine learning. Their intrinsic ability to capture and quantify uncertainties in predictions establishes a
robust foundation for decision-making under uncertainty. However, Bayesian inference in high-dimensional
BNN poses significant computational challenges due to the inefficiency of traditional Markov Chain Monte
Carlo (MCMC) methods. In fact, not only BNN, but almost all traditional Bayesian inference methods
relying on MCMC techniques are known for their computational intensity and inefficiency when dealing with
high-dimensional problems. Variational inference (Jordan et al., 1999) methods have been proposed to speed
up computation by approximating the posterior distribution, but they tend to underestimate uncertainty
(Minka, 2005). Consequently, researchers have proposed various approaches to expedite the inference process
(Welling & Teh, 2011; Shahbaba et al., 2014; Ahn et al., 2014; Hoffman & Gelman, 2014; Beskos et al., 2017;
Cui et al., 2016; Zhang et al., 2017; 2018; Li et al., 2019). Here, we focus on a state-of-the-art approach,
called Calibration-Emulation-Sampling (CES) (Cleary et al., 2021), which has shown promising results in
large-dimensional UQ problems such as inverse problems (Lan et al., 2022). CES involves the following three
steps:
(i)Calibrate models to collect sample parameters and their corresponding expensive evaluation of posterior
probability for the emulation step;
(ii)Emulate the parameter-to-posterior map using the samples from Step (i); and
(iii)Generate posterior samples using MCMC based on the trained emulator at substantially lower cost.
This framework allows for reusing expensive forward evaluations from parameters to posterior probability
and offers a computationally efficient alternative to existing MCMC procedures.
The standard CES method (Cleary et al., 2021) focuses on UQ in inverse problems and uses Gaussian
Process (GP) models for the emulation component. GP models have a well-established history of application
in emulating computer models (Currin et al., 1988), conducting uncertainty analyses (Oakley & O’Hagan,
2002), sensitivity assessments (Oakley & O’Hagan, 2004), and calibrating computer codes (Kennedy &
O’Hagan, 2002; Higdon et al., 2004). Despite their versatility, GP-based emulators are computationally
intensive, with a complexity of O( N3) using the squared-exponential kernel, where Nis the sample size.
Lower computational complexity can be achieved using alternative kernels (Lan et al., 2015) or various
computational techniques (Liu et al., 2020; Bonilla et al., 2007; Gardner et al., 2018; Seeger et al., 2003).
Nevertheless, scaling up GP emulators to high-dimensional problems remains a limiting factor. Furthermore,
the prediction accuracy of GP emulators highly depends on the quality of the training data, emphasizing the
importanceofrigorousexperimentaldesign. Toaddresstheseissues, Lanetal.(2022)proposedanalternative
CES scheme called Dimension-Reduced Emulative Autoencoder Monte Carlo (DREAMC) method, which
uses Convolutional Neural Networks (CNN) as emulator. DREAMC improves and scales up the application
oftheCESframeworkforBayesianUQininverseproblemsfromhundredsofdimensions(withGPemulation)
to thousands of dimensions (with CNN emulation). Here, we adopt a similar approach and propose a new
method, called Fast BNN (FBNN), for Bayesian inference in neural networks. We use DNN for the emulation
component of our CES scheme. DNN has proven to be a powerful tool in a variety of applications and
offers several advantages over GP emulation (Lan et al., 2022; Dargan et al., 2020). It is computationally
more efficient and suitable for high-dimensional problems. The choice of DNN as an emulator enhances
computational efficiency and flexibility.
Besides the computational challenges of building emulators, efficiently sampling from posterior distributions
using these emulators also presents a significant challenge due to the high dimensionality of the target
distribution. Traditional Metropolis-Hastings algorithms, typically defined on finite-dimensional spaces,
encounterdiminishingmixingefficiencyasthedimensionsincrease(Gelmanetal.,1997;Roberts&Rosenthal,
2Published in Transactions on Machine Learning Research (09/2024)
1998; Beskos et al., 2009). To overcome this inherent drawback, a novel class of dimension-independent
MCMC methods has emerged, operating within infinite-dimensional spaces (Beskos, 2014; Beskos et al.,
2009; 2011; Cotter et al., 2013; Law, 2014; Beskos, 2014; Beskos et al., 2017). More specifically, we use
the Preconditioned Crank-Nicolson (pCN) algorithm. The most significant feature of pCN is its dimension
robustness, which makes it well-suited for high-dimensional sampling problems. The pCN algorithm is well-
defined, with non-degenerate acceptance probability, even for target distributions on infinite-dimensional
spaces. As a result, when pCN is implemented on a real-world computer in large but finite dimension N,
the convergence properties of the algorithm are independent of N. This is in strong contrast to schemes
such as Gaussian random-walk Metropolis-Hastings and the Metropolis-adjusted Langevin algorithm, whose
acceptance probability degenerates to zero as Ngoes to infinity.
In summary, this paper addresses the critical challenges of UQ in high-dimensional BNN. By incorporat-
ing deep neural networks for emulation and leveraging the dimension-robust pCN algorithm for sampling,
this research significantly enhances computational efficiency and scalability in Bayesian uncertainty quan-
tification, offering a robust counterpart to DNN, and a scalable counterpart to BNN. Through extensive
experiments, we demonstrate the feasibility and effectiveness of utilizing FBNN to accelerate Bayesian UQ
in high-dimensional neural networks.
2 Related Methods
Various MCMC methods have been employed to explore complex probability distributions for Bayesian in-
ference. In this section, we discuss some of the main MCMC algorithms related to our work. Additionally,
we discuss a variety of state-of-the-art methods utilized in our numerical experiments, which extend be-
yond MCMC frameworks. These include Ensemble Deep Learning for Neural Networks (Perrone & Cooper,
1995), BNNs with Variational Inference (Jaakkola & Jordan, 2000), BNNs leveraging Lasso Approxima-
tion (MacKay, 1992), Monte Carlo Dropout (MC-Dropout) (Gal & Ghahramani, 2016), Stochastic Weight
Averaging-Gaussian (SWAG) (Maddox et al., 2019), and Accelerated Hamiltonian Monte Carlo (HMC)
(Zhang et al., 2017). These techniques offer a comprehensive spectrum for evaluating our FBNN model.
2.1 Hamiltonian Monte Carlo (HMC)
MCMC methods are designed for sampling from intractable probability distributions. The fundamental
principle involves constructing a Markov chain whose equilibrium distribution coincides with the target
distribution. Various algorithms exist for constructing such Markov chains, the simplest of which is the
Metropolis-Hastings (MH) algorithm (Metropolis et al., 1953; Hastings, 1970), which relies on a random walk
to explore the parameter space. Hamiltonian Monte Carlo (HMC) is a special case of the MH algorithm that
incorporates Hamiltonian dynamics evolution and auxiliary momentum variables (Neal, 2011). Compared
to using a Gaussian random walk proposal distribution in the MH algorithm, HMC reduces the correlation
between successive sampled states by proposing moves to distant states that maintain a high probability of
acceptance due to the approximate energy conserving properties of the simulated Hamiltonian dynamic. The
reduced correlation means fewer Markov chain samples are needed to approximate integrals with respect to
the target probability distribution for a given Monte Carlo error.
2.2 Stochastic Gradient Hamiltonian Monte Carlo (SGHMC)
As discussed earlier, HMC sampling methods provide a mechanism for defining distant proposals with high
acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state
space than standard random-walk proposals. However, a limitation of HMC methods is the required gradient
computation for simulation of the Hamiltonian dynamical system; such computation is infeasible in problems
involving a large sample size. Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) (Chen et al., 2014)
addresses computational inefficiency by using a noisy but unbiased estimate of the gradient computed from
a mini-batch of the data. SGHMC is an effective method for Bayesian inference, particularly when dealing
with large datasets, as it leverages stochastic gradients and hyperparameter adaptation to efficiently explore
high-dimensional target distributions.
3Published in Transactions on Machine Learning Research (09/2024)
Algorithm 1 Preconditioned Crank-Nicolson (pCN) Algorithm
Notation:
k: iteration index.
u(k): state of the algorithm at iteration k.
ξ(k)∼N(0,C): Gaussian noise with covariance matrix Cat iteration k.
β: parameter controlling the proposal step size.
Φ: potential function related to the target probability distribution.
a(u(k),v(k)): acceptance probability for the proposed state v(k).
Algorithm Steps:
Initialize iteration counter k←0.
Choose an initial state u(0).
repeat
Generate noise ξ(k)withξ(k)∼N(0,C).
Proposev(k)←/radicalbig
1−β2u(k)+βξ(k).
Calculate acceptance probability a(u(k),v(k))←min{1,exp(Φ(u(k))−Φ(v(k)))}.
ifrandom number≤a(u(k),v(k))then
Setu(k+1)←v(k).
else
Setu(k+1)←u(k).
end if
Increment iteration counter k←k+ 1.
untila stopping criterion (fixed number of iterations or convergence) is met.
2.3 Random Network Surrogate–HMC (RNS–HMC)
Alternatively, we can reduce the computational cost of HMC by constructing surrogate Hamiltonians. As
an example, the random network surrogate–HMC (RNS–HMC) method (Zhang et al., 2017) uses random
non-linear bases to approximate posterior distributions. The goal is to explore and exploit the structure and
regularity in parameter space for the underlying probabilistic model and construct an effective approximation
of its geometric properties. To achieve this, RNS–HMC starts by identifying suitable bases that can capture
the complex geometric properties of the parameter space. Through an optimization process, these bases
are then used to form a surrogate function to approximate the expensive Hamiltonian dynamics. Unlike
traditional HMC, which requires repeated evaluation of the model and its derivatives, RNS-HMC leverages
the surrogate function to perform leapfrog integration steps, leading to a substantially lower computational
cost. Later, Li et al. (2019) extended this idea by using a neural network to directly approximate the gradient
of the Hamiltonian.
2.4 Preconditioned Crank-Nicolson (pCN)
Preconditioned Crank-Nicolson (Da Prato & Zabczyk, 2014) is a variant of MCMC that incorporates a
preconditioning matrix for adaptive scaling (Cotter et al., 2013). It involves re-scaling and random pertur-
bation of the current state, incorporating prior information. Despite the Gaussian prior assumption, the
approach adapts to cases where the posterior distribution may not be Gaussian but is absolutely continuous
with respect to an appropriate Gaussian density. This adaptation is achieved through the Radon-Nikodym
derivative, connecting the posterior distribution with the dominating Gaussian measure, often chosen as the
prior. The algorithmic foundation of pCN lies in using stochastic processes that preserve either the posterior
or prior distribution. These processes serve as proposals for Metropolis-Hastings methods with specific dis-
cretizations, ensuring preservation of the Gaussian reference measure. The key steps of the pCN algorithm
are outlined in Algorithm 1.
4Published in Transactions on Machine Learning Research (09/2024)
Algorithm 2 Variational Inference in Bayesian Neural Networks (BNNs)
Initialization:
Choose an initial variational distribution qθ(W)for the weights Wof BNN, parameterized by θ.
Define the prior distribution p(W)over the weights.
whilenot converged do
E-Step: Estimate the Expectation of the log-likelihood over the variational distribution.
Compute the gradient of the ELBO (Evidence Lower BOund) with respect to θ, where
ELBO (θ) =Eqθ(W)[logp(Y|X,W )]−KL[qθ(W)||p(W)]
Here,XandYare the inputs and outputs of the dataset, respectively, and KL denotes the Kullback-
Leibler divergence between the variational distribution and the prior.
M-Step: Maximize the ELBO with respect to θusing gradient ascent:
θ←θ+η∇θELBO (θ)
whereηis the learning rate.
end while
Output: Variational distribution qθ(W)approximating the posterior distribution p(W|X,Y ).
2.5 Variational Inference
The concept of variational inference has been applied in various forms to probabilistic models. The technique
offersawaytoapproximateposteriordistributionsinBayesianmodels(Jordanetal.,1999). Theapproximate
distribution allows for a more feasible inference, especially for complex models like neural networks. In the
context of BNNs, variational inference was brought into focus by Hinton & Van Camp (1993), which, while
not explicitly termed as such in the modern sense, laid the groundwork for later developments. A more
direct application of variational inference to BNNs was detailed in later works (e.g., Graves, 2011). More
recently, Kingma & Welling (2014), Rezende et al. (2014) and Blundell et al. (2015) significantly contributed
to popularizing and advancing the use of variational inference in deep learning and Bayesian neural networks
through the introduction of efficient gradient-based optimization techniques.
Algorithm2succinctlyshowstheiterativeprocessofoptimizingtheparametersofavariationaldistributionto
approximate the posterior distribution of a BNN’s weights. Through the alternation of expectation (E-Step)
and maximization (M-Step) phases, it aims to minimize the difference between the variational distribution
and the true posterior, leveraging the Evidence Lower Bound (ELBO) (Jordan et al., 1999) as a tractable
surrogate objective function. This approach enables the practical application of Bayesian inference to neural
networks, facilitating the quantification of uncertainty in predictions and model parameters.
2.6 Laplace Approximation
Previous studies have shown that in the context of BNNs, the Laplace approximation serves as an efficient
method for approximating the posterior distribution over the network’s weights (Arbel et al., 2023; Blundell
et al., 2015). At the core of the Laplace approximation is the assumption that, around the loss function’s
minimum,theposteriordistributionofthenetwork’sweightscanbeapproximatedbyaGaussiandistribution.
This is achieved by finding the mode of the posterior, called the Maximum A Posteriori (MAP; equivalent to
the minimum of the loss function in the Bayesian framework), and then approximating the curvature of the
loss surface at this point using the Hessian matrix (Liang et al., 2018). The inverse of this Hessian is used
to define the covariance of the Gaussian posterior, thus simplifying the representation of uncertainty in the
predictions. More specifically in BNNs, this approach can be used to approximate the posterior distribution
of the weights given the observed data.
5Published in Transactions on Machine Learning Research (09/2024)
2.7 Monte Carlo Dropout
Monte Carlo (MC) Dropout (Gal & Ghahramani, 2016) was introduced as a Bayesian approximation method
to quantify model uncertainty in deep learning. The core idea behind this method is to interpret dropout,
a technique commonly used to prevent overfitting in neural networks, from a Bayesian perspective. Nor-
mally, dropout randomly disables a fraction of neurons during the training phase to improve generalization.
However, when viewed through the Bayesian lens, dropout can be seen as a practical way to approximate
Bayesian inference in deep networks. This approximation allows the network to estimate not just a single set
of weights, but a distribution over them, enabling the model to express uncertainty in its predictions. The
MC Dropout technique involves running multiple forward passes through the network with dropout enabled.
Each forward pass generates a different set of predictions due to the random omission of neurons, leading to
a distribution of outputs for a given input.
2.8 Stochastic Weight Averaging-Gaussian (SWAG)
Building on the idea of Stochastic Weight Averaging (SWA) (Izmailov et al., 2018; Maddox et al., 2019),
SWAG approximates the distribution of model weights by a Gaussian distribution, leveraging the empirical
weight samples collected from training. This approach allows for a more nuanced understanding of the
model’s uncertainty compared to SWA, which simply averages weights over the latter part of the training
process. SWAG involves collecting a set of weights {Wi}N
i=1over the last Nepochs of training, where Wi
represents the weight vector at epoch i. The mean µof the Gaussian distribution is then computed as the
simpleaverageoftheseweights: µ=1
N/summationtextN
i=1Wi. Tocapturethecovarianceoftheweightdistribution,SWAG
calculates the empirical covariance matrix: Σ =1
N−1/summationtextN
i=1(Wi−µ)(Wi−µ)T. This formulation assumes
a diagonal, or low-rank plus diagonal, approximation of the covariance matrix to maintain computational
efficiency. The resulting Gaussian distribution, characterized by µandΣ, can then be used for uncertainty
estimation and prediction by sampling weights from this distribution and averaging the predictions of the
resulting models.
3 Bayesian UQ for Neural Networks: Calibration-Emulation-Sampling
Standard neural networks (NN) typically consist of multiple layers, starting with an input layer, denoted
asl0, followed by a series of hidden layers llforl= 1,...,m−1, and ending with an output layer lm. In
this architectural framework, comprising a total of m+ 1layers, each layer lis characterized by a linear
transformation, which is subsequently subjected to a nonlinear operation g, commonly referred to as an
activation function (Jospin et al., 2022):
l0=X,
ll=gl(Wlll−1+bl)for alll∈{1,···,m−1}, (1)
lm=Y.
Here,θ= (W,b)are the parameters of the network, where Ware the weights of the network connections
andbare the biases. A given NN architecture represents a set of functions isomorphic to the set of possible
parametersθ. Deep learning is the process of estimating the parameters θfrom the training set (X,Y) :=
{(xn,yn)}N
n=1composed of a series of input Xand their corresponding labels Y. Based on the training set,
a neural network is trained to optimize network parameters θin order to map X→Ywith the objective of
obtaining the maximal accuracy (under certain loss function L(·)). Considering the error, we can write NN
as a forward mapping, denoted as G, that maps each parameter vector θto a function that further connects
XtoYwith small errors εn:
G: Θ→YX,θ∝⇕⊣√∫⊔≀→G(θ) (2)
More specifically,
G(θ) :X→Y,yn=ˆyn+εn,ˆyn=G(xn;θ),ε∼N(0,Γ) (3)
6Published in Transactions on Machine Learning Research (09/2024)
whereεrepresents random noise capturing disparity between the predicted and actual observed values in
the training data. Here, Yis a continuous random variable in regression problems, or a continuous latent
variable in classification problems.
To train NN, stochastic gradient algorithms could be used to solve the following optimization problem:
θ∗= arg min
θ∈ΘL(θ;X,Y) = arg min
θ∈ΘL(Y−G(X;θ))
For example, the loss function L(θ;X,Y)can be defined in terms of the negative log-likelihood function Φ
as follows:
Φ(θ;X,Y) =1
2∥Y−G(X;θ)∥2
Γ (4)
The point estimate approach, which is the traditional approach in deep learning, is relatively straightfor-
ward to implement with modern algorithms and software packages, but tends to lack proper uncertainty
quantification (Guo et al., 2017; Nixon et al., 2019). To address this issue, stochastic neural networks, which
incorporate stochastic components in the network, have emerged as a standard solution. This is performed
by giving the network either stochastic activation functions or stochastic weights to simulate random samples
forθ. The integration of stochastic components into neural networks allows for an extensive exploration of
model uncertainty, which can be approached through Bayesian methods among others. It should be noted
that not all neural networks that represent uncertainty are Bayesian or even stochastic; some employ deter-
ministic methods to estimate uncertainty without relying on stochastic components or Bayesian inference
(Lakshminarayanan et al., 2017; Sensoy et al., 2018). Bayesian neural networks (BNN) represent a subset
of stochastic neural networks where Bayesian inference is specifically used for training, offering a rigorous
probabilistic interpretation of model parameters (MacKay, 1992; Neal, 2012). The primary objective is to
gain a deeper understanding of the uncertainty that underlies the specific process the network is modeling.
To design a BNN, we put a prior distribution over the model parameters, p(θ). By applying Bayes’ theorem,
the posterior probability can be written as:
p(θ|X,Y ) =p(Y|X,θ)p(θ)/integraltext
θp(Y|X,θ′)p(θ′)dθ′(5)
∝p(Y|X,θ)p(θ). (6)
BNNisusuallytrainedusingMCMCalgorithms. Becausewetypicallyhavebigamountofdata,thelikelihood
evaluation tends to be expensive. One common approach to address this issue is subsampling, which restricts
the computation to a subset of the data (see for example, Hoffman et al., 2010; Welling & Teh, 2011; Chen
et al., 2014). The assumption is that there is redundancy in the data and an appropriate subset of the data
can provide a good enough approximation of the information provided by the full data set. In practice, it is a
challenge to find good criteria and strategies for an effective subsampling in many applications. Additionally,
subsampling could lead to a significant loss of accuracy (Betancourt, 2015).
3.1 Fast Bayesian Neural Network (FBNN).
We propose an alternative approach that explores smoothness or regularity in parameter space, a charac-
teristic common to most statistical models. Therefore, one would expect to find good and compact forms
of approximation of functions (e.g., likelihood function) in parameter space. Sampling algorithms can use
these approximate functions, also known as “surrogate” functions, to reduce their computational cost. More
specifically, we propose using the CES scheme for high-dimensional BNN problems in order to bypass the
expensive evaluation of original forward models and reduce the cost of sampling to a small computational
overhead. Compared with MCMC methods, which require repeatedly evaluating the original (large) NN for
the likelihood given the data, the proposed method builds a smaller NN emulator that bypasses the data
(i.e., cuts out the middleman) by mapping the parameters directly to the likelihood function, thus avoid-
ing costly evaluations. That is, the emulator is trained based on the parameter-likelihood pairs, which are
collected through few iterations of the original BNN. In contrast to subsampling methods, this approach
7Published in Transactions on Machine Learning Research (09/2024)
Algorithm 3 Fast Bayesian Neural Network (FBNN)
Input:Training set{(Xn,Yn)}N
n=1, Priorp(θ)
Output: Posterior samples for model parameters
procedure FBNN({(Xn,Yn)}N
n=1,p(θ))
Calibration Step:
Initialize model parameters θusing SGHMC
Save posterior samples {θ(j)
n}J
j=1and the corresponding {Gθ(j)
n(Xn)}J
j=1after a few iterations
Emulation Step:
Build an emulator of the forward mapping Gebased on{θ(j)
n,Gθ(j)
n(Xn)}J
j=1using a DNN as the
emulator
Sampling Step: Run approximate MCMC, particularly pCN, based on the emulator to propose θ′
fromθ.
end procedure
can handle computationally intensive likelihood functions, whether the computational cost is due to high-
dimensional data or complex likelihood function (e.g., models based on differential equations). Additionally,
the calibration process increases the efficiency of MCMC algorithms by providing a robust initial point in
the high-density region. Algorithm 3 shows how our proposed method, called Fast Bayesian Neural Net-
wor (FBNN), combines the strengths of BNN in uncertainty quantification, SGHMC for efficient parameter
calibration, and the pCN method for sampling. More details are provided in the following sections.
3.2 Calibration – Early stopping in Bayesian Neural Network
By “calibration” we mean collecting an optimal sample of parameters to build an emulator with a reasonable
level of accuracy. This is aligned with traditional calibration goals of balancing accuracy and reliability, but
within a new context. Here, the calibration step involves an early stopping strategy, aimed at collecting a
targeted set of posterior samples without fully converging to the target distribution. More specifically, we
use the Stochastic Gradient Hamilton Monte Carlo (SGHMC) algorithm for a limited number of iterations
to collect a small set of samples. These samples include both the model parameters ( θ(j)) and the outputs
predicted by the model ( G(X;θ(j))) for each sample jout of a total of Jsamples. The key focus of this
training phase is not to obtain a precise approximation of the target posterior distribution, but rather
collecting a small number of posterior samples as the “training data” for the subsequent emulation step. The
SGHMC algorithm plays a crucial role in efficiently handling large datasets and collecting essential samples
during the calibration step of the FBNN. Its ability to introduce controlled stochasticity in updates proves
instrumental in preventing local minima entrapment, thereby providing a comprehensive set of posterior
samples that capture the variability in the parameter space.
3.3 Emulation – Deep Neural Network (DNN)
The original forward mapping in BNN involves mapping input dataset Xto the response variable Y. For the
likelihood evaluation using original forward mapping, it is necessary to calculate the likelihood L(θ;X,Y )for
each sample of model parameters. This means that with each iteration, when a new set of model parameters
is introduced, the original forward mapping needs to be applied to generate output predictions, followed by
the calculation of the likelihood. In general, this process can be very time-consuming. If, however, we have a
small set of estimated model parameters along with their corresponding predicted outputs collected during
thecalibrationstep, anemulatorcanbetrainedtoeliminatetheintermediarystep(passingthrougheachdata
point), allowing us to map the parameters directly to the likelihood function. This leads to a computationally
efficientlikelihoodevaluation. Therefore, toaddressthecomputationalchallengesofevaluatingthelikelihood
with large datasets, we build an emulator Geusing the recorded pairs {θ(j),ˆy(j)=G(X;θ(j))}J
j=1obtained
during the calibration step. More specifically, these input-output pairs are used to train a DNN model as
8Published in Transactions on Machine Learning Research (09/2024)
an emulatorGeof the forward mapping G:
Ge(X;θ) =DNN (θ,G(X;θ)) =FK−1◦···◦F0(θ), (7)
Fk(·) =gk(Wk·+bk)∈C/parenleftbig
Rdk,Rdk+1/parenrightbig
(8)
Given a DNN model where θrepresents the input and G(X;θ)denotes the output, we set the dimensions
asd0=danddK=D, wheredrepresents the dimension of the input parameter vector θ, andDrepresents
the dimension of the output G(X;θ). Here, the matrices Wkare defined in the space Rdk+1×dkand the
vectorsbkinRdk+1. The functions gkact as (continuous) activation mechanisms. In the context of our
numerical examples, the activation functions for the DNN emulator are selected to ensure that both the
function approximations and their derived gradients have minimized errors. This involves a grid search
over a predefined set of activation functions to ensure that the network efficiently approximates the target
functions and their gradients.
After the emulator is trained, the log-likelihood can be approximated as follows:
L(θ;X,Y )≈Le(θ;X,Y ) =L(Y−Ge(X;θ)) (9)
By combining the approximate likelihood Le(θ;X,Y )with the prior probability p(θ), an approximate poste-
riordistributioncanbeobtained. Similarly,wecouldapproximatethepotentialfunctionusingthepredictions
from DNN:
Φ(θ;X,Y)≈Φe(θ;X,Y) =1
2∥Y−Ge(X;θ)∥2
Γ (10)
BuildinguponthefoundationalconceptsofusingaDNNemulator Geforapproximatingtheforwardmapping
functionG, we further elaborate on the implications and advantages of this approach for Bayesian inference,
particularly in the context of handling large datasets and/or complex likelihood functions. The emulation
step, which involves training the DNN emulator with input-output pairs {θ(j),G(X;θ(j))}, serves as a
critical phase where the emulator learns to mimic the behavior of the original model with high accuracy.
The utilization of DNN emulator to approximate the likelihood function in Bayesian inference presents a
significant computational advantage over the direct use of the original BNN likelihood. This advantage stems
primarily from the inherent differences in computational complexity between evaluating the the likelihood
with a DNN emulator – which takes a set of model parameters as input and yields predicted responses—and
the original BNN model – which processes Xas input to produce the response variable.
In the sampling stage, the computational complexity could be significantly reduced if we use Φeinstead of
Φin the accept/reject step of MCMC. If the emulator is a good representation of the forward mapping,
the difference between Φeand Φwould be small and negligible. Then, the samples by such emulative
MCMC have the stationary distribution that closely follows the true posterior distribution. This approach
not only ensures that the sampling process is computationally feasible, but also maintains the integrity of
the stationary distribution, closely approximating the true posterior distribution with minimal discrepancy.
The integration of DNN emulators into the Bayesian inference workflow thus presents a compelling solution
to the computational challenges associated with evaluating likelihood functions in complex models.
3.4 Sampling – Preconditioned Crank-Nicolson (pCN)
In the context of the FBNN method, the sampling step is crucial for approximating the posterior distribution
efficiently. The method employs MCMC algorithms based on a trained emulator to achieve full exploration
and exploitation. However, challenges arise, especially in high-dimensional parameter spaces, where classical
MCMC algorithms often exhibit increasing correlations among samples. To address this issue, the pCN
method presented in Algorithm 1 has been used in our proposed framework as a potential solution. Un-
like classical methods, pCN avoids dimensional dependence challenges, making it particularly suitable for
scenarios like BNN models with a high number of weights to be inferred (Hairer et al., 2009).
As explained in section 2.4, the pCN approach minimizes correlations between successive samples, a critical
feature for ensuring the representativeness of the samples collected. This characteristic is vital for FBNNs, as
9Published in Transactions on Machine Learning Research (09/2024)
Figure 1: Sampling from a mixture of 25 Gaussians shown in (a) with 200k samples. SGHMC in (b) broadly
explores the space, while pCN in (c) hones in on the high-density regions for precise mode capture.
it directly impacts the network’s ability to learn from data and make robust predictions. The pCN method
excels in traversing the parameter space with controlled perturbations, enhancing the algorithm’s ability to
capturethemostprobableconfigurationsofmodelparameters. Thisfocusoneffectiveexplorationaroundthe
mode contributes to a more accurate representation of the underlying neural network, ultimately improving
model performance. In other words, the choice of pCN as the sampling method in FBNN is motivated by
its tailored capacity to navigate and characterize the most probable regions of the parameter space. This
choice reinforces the methodology’s robustness and reliability, as pCN facilitates efficient sampling, leading
to a more accurate and representative approximation of the posterior distribution.
To illustrate this, Figure 1 displays a simulation that contrasts the sampling mechanisms of SGHMC and
pCN within a multimodal probability distribution. The task is to sample from a mixture of 25 Gaussian
distributions, represented in panel (a), using a total of 200,000 samples. Here, the target distribution
is multimodal with several distinct peaks (modes). Middle figure shows that SGHMC has explored the
parameter space, although with a less concentrated sampling around the modes compared to the target
distribution. This indicates that while SGHMC is effective at exploring the space, it may not capture the
modes as tightly as the target distribution. In the right panel related to pCN sampler, the concentration of
samples around the modes is much higher compared to SGHMC, which indicates that pCN is more effective
at exploring around the modes of the distribution. Thus, we believe the combination of SGHMC and pCN in
our proposed framework can complement each other for a more effective exploration of the parameter space.
3.5 Theoretical Foundations
In this section, we aim to quantify the error between the true potential function and its emulation in
the context of the FBNN method. Let Ω = (0,1)dand consider forward mappings in the Sobolev
spaceWn,p(Ω) :={f∈Lp(Ω) :Dαf∈Lp(Ω)for allα∈(N∪{0})dwith|α| ≤n}with∥f∥n,p:=/parenleftig/summationtext
0≤|α|≤n∥Dαf∥p
p/parenrightig1
p, and∥f∥n,∞:= max 0≤|α|≤n∥Dαf∥∞. Define the Sobolev-Slobodeckij norm for
0<s< 1:∥f∥s,p:=/parenleftig
∥f∥p
p+/integraltext
Ω/integraltext
Ω|f(x)−f(y)|p
|x−y|sp+ddxdy/parenrightig1
pand∥f∥s,∞:= max/braceleftig
∥f∥∞,ess supx,y∈Ωf(x)−f(y)
|x−y|s/bracerightig
.
Theorem 3.1. Let1≤p≤∞and0≤s<1. AssumeGj(X;·)∈Wn,p(Ω)∩L∞(Ω)forj= 1,···,D. For
anyϵ∈(0,1/2), there is a standard NN, Ge, with ReLU activation functions such that
∥Φ−Φe∥s,p≤ϵ. (11)
and the depth K≤clog(ϵ−n/(n−s)), the number of weights and units N≤cϵ−d/(n−s)log2(ϵ−n/(n−s))with
constantc=c(d,n,p,s )>0.
10Published in Transactions on Machine Learning Research (09/2024)
Proof.Note that we have
Φ(θ)−Φe(θ) =1
2[⟨G(X;θ)−Ge(X;θ),y−G(X;θ)⟩Γ+⟨y−Ge(X;θ),G(X;θ)−Ge(X;θ)⟩Γ]
BecauseGj(X;·)∈L∞((0,1)d), there exists a constant M > 0such that max 1≤j≤D∥Gj(X;·)∥∞,∥y∥≤M.
Forϵ/(MD)>0, by Theorem 4.1 of Gühring et al. (2020), there exists a standard NN with ReLU activation
functions and the depth Kand the number of weights and units as in the condition such that
∥Gj(X;·)−Ge
j(X;·)∥s,p≤ϵ/(MD), j = 1,···,D.
Therefore we have
∥Φ−Φe∥s,p≤MD/summationdisplay
j=1∥Gj(X;·)−Ge
j(X;·)∥s,p≤ϵ.
Denote the Hellinger distance between densities as dH(π,πe) =/integraltext
(√π−√
πe)edµ. Then we describe how the
emulation error propogates into the Hellinger error in the likelihood.
Theorem 3.2. Letπ(·;θ)∝exp(−Φ(·;θ))andπe(·;θ)∝exp(−Φe(·;θ))denote the likelihood and its
emulation, respectively. Suppose the conditions of Theorem 3.1 holds for p=∞. Then we have
dH(π,πe)≲ϵ. (12)
whereϵsatisfies the constraints in Theorem 3.1.
Proof.Compute the Hellinger distance
2d2
H(π,πe) =/integraldisplay
(√π−√
πe)2dµ=/integraldisplay/bracketleftbigg
1−exp/parenleftbigg1
2Φ(y;θ)−1
2Φe(y;θ)/parenrightbigg/bracketrightbigg2
π(y;θ)dy
≤/integraldisplayC
4|Φ(y;θ)−Φe(y;θ)|2π(y;θ)dy≤C′/integraldisplay
∥Φ−Φe∥2
∞π(y;θ)dy≲ϵ2.
where the last inequality is by Theorem 3.1. Taking square-root on both sides yields the conclusion.
Furthermore, given Gaussian prior for θ,π0, we can characterize the discrepancy of posteriors with the
original and emulated likelihoods in the following theorem.
Theorem 3.3. Letˆπ(θ)∝exp(−Φ(y;θ))π0(θ)andˆπe(θ)∝exp(−Φe(y;θ))π0(θ)denote the posterior and
the one with emulated likelihood, respectively. Suppose the conditions of Theorem 3.1 holds for p=∞. Then
we have
dH(ˆπ,ˆπe)≲ϵ. (13)
whereϵsatisfies the constraints in Theorem 3.1.
Proof.By Bayes’ theorem, we have
ˆπ(θ) =1
Z(y)exp(−Φ(y;θ))π0(θ),0<Z(y) =/integraldisplay
Ωexp(−Φ(y;θ))π0(θ)dθ)<+∞
ˆπe(θ) =1
Z(y)exp(−Φe(y;θ))π0(θ),0<Ze(y) =/integraldisplay
Ωexp(−Φe(y;θ))π0(θ)dθ)<+∞.
11Published in Transactions on Machine Learning Research (09/2024)
Therefore we compute the Hellinger distance
dH(ˆπ,ˆπe) =/integraldisplay
Ω/bracketleftbigg
Z(y)−1
2exp/parenleftbigg
−1
2Φ(y;θ)/parenrightbigg
−Ze(y)−1
2exp/parenleftbigg
−1
2Φe(y;θ)/parenrightbigg/bracketrightbigg2
π0(dθ)
≤2
Z(y)/integraldisplay
Ω/bracketleftbigg
exp/parenleftbigg
−1
2Φ(y;θ)/parenrightbigg
−exp/parenleftbigg
−1
2Φe(y;θ)/parenrightbigg/bracketrightbigg2
π0(dθ) + 2|Z(y)−1
2−Ze(y)−1
2|2Ze(y)
≤2/integraldisplay
Ω/bracketleftbigg
1−exp/parenleftbigg1
2Φ(y;θ)−1
2Φe(y;θ)/parenrightbigg/bracketrightbigg2
ˆπ(dθ) +C|Z(y)−Ze(y)|2
≤/integraldisplayC′
2|Φ(y;θ)−Φe(y;θ)|2ˆπ(dθ) +C/integraldisplay
Ω|exp(−Φ(y;θ))−exp(−Φe(y;θ))|2π0(dθ)
≤C′
2∥Φ−Φe∥2
∞+C′′∥Φ−Φe∥2
∞≲ϵ2
where the last inequality is by Theorem 3.1. Taking square-root on both sides yields the conclusion.
4 Numerical Experiments
4.1 Setup
We demonstrate the effectiveness of our method on eleven synthetic and real-world datasets, comparing it
against a comprehensive selection of baseline approaches.
Datasets. Weexperimentonaseriesofregressionandclassificationproblems. Detailedinformationregard-
ing these datasets, including the number of features and datapoints in each, and the number of parameters
used in the main FBNN model for each dataset, is outlined in Table 1. We have also included the details of
the DNN Emulator architecture for each dataset in Table 2.
Baseline Methods. We present empirical evidence comparing our CES method against a broad array of
baseline approaches including two baseline BNN methods equipped with the SGHMC and pCN samplers
(shown as BNN-SGHMC and BNN-pCN), and BNN architectures incorporating Variational Inference, Lasso
Approximation, MC-Dropout, SWAG, and RNS-HMC. Detailed information about these methods was pro-
vided in Section 2. We also include the results from DNN, which does not provide uncertainty quantification,
butservesasareferencepoint. Moreover, weprovidetheresultsofDeepEnsembles, whichconsistofmultiple
DNNs, each initialized with different random seeds. We refer to this method as Ensemble-DNN. Although
the Ensemble-DNN approach allows for parallelization, it falls short in providing a probabilistic framework
for analysis, a significant advantage offered by our CES method.
As discussed earlier, one of the distinctive features of our main FBNN model, more specifically shown as
FBNN (SGHMC-pCN), lies in its strategic integration of the SGHMC sampler during the calibration step
and the pCN algorithm during the sampling step. This combination is carefully chosen to harness the
complementary strengths of these two sampling methods. Further expanding our exploration, we introduce
three additional FBNN models: FBNN (pCN-SGHMC), where pCN is employed in the calibration step and
SGHMC in the sampling step; FBNN (pCN-pCN), where pCN is used in both steps; and FBNN (SGHMC-
SGHMC), where SGHMC is used in both calibration and sampling steps.
Throughout these experiments, we collect 2000 posterior samples for the BNN-SGHMC and BNN-pCN, with
samples being collected at each iteration. In contrast, for the FBNN methods, we use a small number (200)
of samples from either BNN-SGHMC or BNN-pCN (depending on the specific FBNN model) along with
the corresponding predicted outputs during the calibration step. These 200 samples serve as the “training
data” for the emulator. Moreover, we evaluate the efficacy of utilizing only the initial 200 samples from the
BNN-SGHMC model across all the datasets. This was done to demonstrate the necessity of collecting more
samples, either using the original BNN or employing the FBNN method, rather than relying our inference
on a limited number of initial samples.
12Published in Transactions on Machine Learning Research (09/2024)
Table 1: Description of various datasets used to evaluate the overall performance of our proposed approach
against state-of-the-art baseline methods.
Task Dataset # Datapoints # Features # FBNN parameters
Regression Boston Housing 506 13 3,009
Wine Quality 1,599 11 241
Alzheimer 185,831 56 33,345
Year Prediction MDS 515,345 90 81,901
Simulation 5,000,000 1,000 52,832
Classification Adult 40,434 14 2,761
Mnist 70,000 784 3,961
Alzheimer 185,831 56 33,345
celebA 202,599 39 1,521
SVHN 630,420 3072 171,313
Simulation 5,000,000 1,000 52,832
Table 2: Description of various datasets and their corresponding DNN Emulator architectures (Droupout
layers have been used on input layer and first hidden layer)
Task Dataset #Hidden
Layers# Neurons
per LayerActivation
Functions#Epochs Dropout
Rate
Regression Boston Housing 2 3,32 ReLU 1000 0.7
Wine Quality 2 3,32 ReLU 1000 0.5
Alzheimer 2 4,64 ReLU 1000 0.5
Year Prediction 2 4,64 ReLU 1000 0.5
Simulation 3 8,64,32 ReLU 1000 0.5
Classification Adult 2 4,32 ReLU 1000 0.5
Mnist 2 3,64 ReLU 1000 0.5
Alzheimer 2 4,64 ReLU 1000 0.5
celebA 2 3,32 ReLU 1000 0.5
SVHN 3 4,64,32 ReLU 1000 0.7
Simulation 3 8,64,32 ReLU 1000 0.5
It is also crucial to highlight that the BNN-SGHMC and BNN-pCN models are trained from a randomly
chosen initial point for the MCMC sampling process. On the other hand, in the FBNN methods, we employ
the set of posterior samples collected during the last iteration of the calibration step as the starting point
for the subsequent MCMC sampling.
Metrics. To thoroughly assess the performance and effectiveness of each method, we use a range of key
metrics. These include Mean Squared Error (MSE) for regression tasks (Figure 2) and Accuracy for clas-
sification tasks (Figure 3). We also evaluate the models based on their computational cost, and various
statistics related to the Effective Sample Size (ESS) of model parameters. These statistics include the mini-
mum, maximum, and median ESS, as well as the minimum ESS per second. We also quantify the amount
of speedup, denoted as “spdup”, a metric that compares the minimum ESS per second of each model with
that of BNN-SGHMC as the benchmark (Figure 4). Analysing spdup is crucial as it provides a comparative
13Published in Transactions on Machine Learning Research (09/2024)
measure of efficiency, highlighting the model’s capability to achieve high-quality parameter sampling with
lower computational resource utilization relative to the benchmark BNN-SGHMC.
The effective sample size takes the autocorrelation among the consecutive samples into account. While we
can reduce autocorrelation using the thinning strategy, this leads to a higher computational time for the
same number of samples. Our spdup metric allows for a fair comparison of sampling algorithms (regardless
of what thinning strategy used) by taking both autocorrelation and computational cost into account.
For UQ in regression cases, we evaluate the Coverage Probability (CP) set at 95%. In addition, we construct
95% Credible Intervals (CI) by the prediction results of Bayesian models, along with the average true output,
toillustrateUQinregressionproblems. Forclassificationproblems,weuseExpectedCalibrationError(ECE)
and Reliability Diagrams to evaluate UQ. ECE addresses model calibration, aiming for accurate uncertainty
estimates, while reliability diagrams offer a visual summary of probabilistic forecasts.
Figures 2, 3, and 4 summarize our results. More detailed results are provided in Tables A1 and A2.
4.2 Regression Tasks
We first evaluate our proposed method using a set of simulated and real regression problems.
Simulated Data. We begin our empirical evaluation by using simulated data. To this end, we utilize
themake_regression function from the sklearn.datasets package to generate a dataset consisting of
5,000,000 observations and 1,000 predictors.
Figure 5 compares the true and emulated log likelihood functions associated with posterior samples collected
using BNN-SGHMC. The emulated values are based on the FBNN (SGHMC-pCN) model. As we can see,
the two functions are similar, indicating that the emulator provides a reasonable approximation of the true
target distribution.
Figure 2 compares the MSE among all models, showing that while the DNN method achieves the lowest MSE
at 0.71, the FBNN (SGHMC-pCN) model provides a similar performance. Notably, among all the FBNN
variants, FBNN (SGHMC-pCN) provides the highest CP at 92.2%, demonstrating a level of calibration
comparable to that of the BNN model. The Ensemble-DNN demonstrates comparable performance to
FBNN (SGHMC-pCN) in terms of CP, yet it operates at a pace three times slower.
Examining the efficiency of sample generation, all FBNN variants have relatively higher ESS per second
compared to all the other BNN models, except for BNN-RNS-HMC. Among all the models, FBNN (SGHMC-
pCN) has the highest min ESS per second at 0.043. Figure 4 indicates our model provides the highest
speedup (16.33) compared to BNN-SGHMC as the baseline model, highlighting our method’s computational
efficiency. Considering these results, FBNN (SGHMC-pCN) emerges as a strong approach with a good
balance between predictive accuracy, computational efficiency, and uncertainty quantification, making it the
overall best option for Bayesian deep learning.
Figure6ashowstheestimatedmeanandpredictionuncertaintyforbothBNN-SGHMCandFBNN(SGHMC-
pCN) models, alongside the smoothed average and 95% interval for the true output. For clarity and concise-
ness within our figures, we have employed Principal Component Analysis (PCA) and used the first principal
component to transform the original data into a one-dimensional representative feature (x-axis in Figure 6).
As we can see, BNN and FBNN have very similar credible intervals. This consistency in credible interval
bounds is significant for UQ, indicating that both models effectively and almost equally quantify uncertainty
in their predictions.
Wine Quality Data. As the first real dataset for the regression task, we use the Wine Quality data
(Cortez et al., 2009). This dataset contains various physicochemical properties of different wines, while the
target variable is the quality rating. The performance of FBNN (SGHMC-pCN) indicates a well-balanced
approach, making it superior to the other models for several reasons. Firstly, it achieves a competitively low
MSE of 0.52, comparable to other high-performing models like BNN-SGHMC and SWAG, but it surpasses
them in terms of speedup. Moreover, FBNN (SGHMC-pCN) exhibits a robust predictive performance,
14Published in Transactions on Machine Learning Research (09/2024)
Figure 2: A comprehensive comparison of the MSE for various BNN methods across five regression datasets.
Each subplot corresponds to a dataset and the color-coded bars represent the distinct methodologies evalu-
ated. The main FBNN method, called FBNN (SGHMC-pCN), highlighted in bold, shows among the lowest
values for MSE among all datasets.
Figure 3: A comprehensive comparison of the Accuracy for various BNN methods across five classification
datasets. Eachsubplotcorrespondstoadatasetandthecolor-codedbarsrepresentthedistinctmethodologies
evaluated. The main FBNN method, FBNN (SGHMC-pCN), highlighted in bold, demonstrates superior
performance by achieving the highest accuracy in four out of the five datasets examined and the second
highest in one dataset.
with a competitively high CP. Figure 6b shows the prediction mean and 95% CI BNN-SGHMC and FBNN
(SGHMC-pCN), as well as the smoothed average and 95% interval for the true output.
Boston Housing Data. The Boston housing dataset was collected in 1978 (Harrison Jr & Rubinfeld,
1978). Each of the entries present aggregated data for homes from various suburbs in Boston. For this
dataset, FBNN (SGHMC-pCN) stands out with a notable balance between MSE (3.82), CP (81.1%), and
computational efficiency, completing the task in just 91 seconds. This model significantly outperforms all the
other models in terms of speedup (11.94), showcasing its effectiveness in sampling. Figure 6c shows the 95%
CIs and mean predictions of both BNN-SGHMC and FBNN (SGHMC-pCN). The FBNN (SGHMC-pCN)
15Published in Transactions on Machine Learning Research (09/2024)
Figure 4: Comparative analysis of speedup (spdup) for various Bayesian Neural Network (BNN) methods
across tested datasets. Methods are ordered by efficiency within each dataset, highlighting the impact
of model characteristics on sampling performance. Notably, "FBNN (SGHMC-pCN)" achieves the highest
speedup among all datasets except for the Wine Quality dataset, where it ranks as the second highest,
underscoring its exceptional efficiency in diverse analytical contexts.
model, in particular, displays well-calibrated uncertainty quantification, mirroring the performance of the
BNN models, implying that its probabilistic predictions capture the model uncertainty.
Alzheimer Data. Next, we analyzethe data fromthe NationalAlzheimer’sCoordinatingCenter (NACC),
which is responsible for developing and maintaining a database of patient information collected from the
Alzheimerdiseasecenters(ADCs)fundedbytheNationalInstituteonAging(NIA)(Beeklyetal.,2004). The
NIA appointed the ADC Clinical Task Force to determine and define an expanded, standardized clinical data
set, called the Uniform Data Set (UDS). The goal of the UDS is to provide ADC researchers a standard set
of assessment procedures to identify Alzheimer’s disease (Beekly et al., 2007). We have used 56 key features
for our analysis. These features were carefully selected to represent a wide spectrum of variables relevant
to Alzheimer’s disease diagnosis, including functional abilities, brain morphometrics, living situations, and
demographic information (Ren et al., 2022). For the regression case, the goal is to predict Left Hippocampus
Volume, a critical marker in the progression of the disease (van der Flier & Scheltens, 2009), as a function
of other variables. For this dataset, Figure 2 shows that the FBNN (SGHMC-pCN) model stands out for
its balanced performance, recording the second lowest MSE at 0.48 and a relatively high CP at 91.6%. It
shows a considerable improvement in computational efficiency, evidenced by a speedup factor of 22 times
compared to BNN-SGHMC as the baseline BNN model.
Year Prediction MSD Data. For this data, the goal is to predict the release year of a song from audio
features. Songs are mostly western, commercial tracks ranging from 1922 to 2011, with a peak in the year
2000s (Bertin-Mahieux, 2011). In the context of the YearPredictionMSD dataset, FBNN (SGHMC-pCN)
showcases its superiority over other models by achieving a good balance between accuracy, computational
efficiency, and effective uncertainty quantification. With an MSE of 73.41, close to that of Ensemble-DNN,
and CP of 92.23% it outperforms most other models. Moreover, the computational efficiency of FBNN
(SGHMC-pCN) is highlighted by its speedup factor of 11.98 (Figure 4) over the baseline model BNN-
SGHMC.
16Published in Transactions on Machine Learning Research (09/2024)
Figure 5: Evaluating the performance of the emulator. The plot contrasts log likelihood values obtained
using the true likelihood function against those derived using the emulated likelihood function. The x-axis
and y-axis represent the first and second principal components of the model parameters based on the MCMC
samples obtained in our simulation study.
(a) Simulated data
 (b) Wine Quality data
 (c) Boston Housing data
Figure 6: Comparative analysis of predictive credible intervals and mean predictions for regression tasks.
For each dataset, the 95% CI for BNN predictions and FBNN predictions are shown as shaded areas. The
average predictions from BNN and FBNN are represented with dashed lines. Additionally, the 95% CI for
the true output as ground truth and the smoothed average true output are plotted as solid lines. The x-axis
shows the first principal component of the predictors.
Figure 7: Reliability Diagrams for Simulated dataset in classification task. These diagrams incorporate equal
frequency binning.
4.3 Classification Tasks
Next, we evaluate our method based on a set of simulated and real classification problems. The results are
summarized in Figures 3 and 4. More details results are provided in Table A2.
Simulated Data. As before, we start with a simulated dataset with a binary outcome. For this, we use
themake_classification function from the sklearn.datasets package to generate a dataset consisting of
17Published in Transactions on Machine Learning Research (09/2024)
5,000,000 observations and 1000 predictors. Accuracy comparison in Figure 3 shows FBNN (SGHMC-pCN),
DNN, and Ensemble-DNN exhibit comparable performance and outperform other models. While BNN-
RNS-HMC achieves the highest speedup, it significantly underperforms in terms of accuracy. In contrast,
FBNN (SGHMC-pCN) provides the second-highest speedup at 32.00, showcasing its computational efficiency
relative to BNN-SGHMC. Furthermore, it maintains the second highest accuracy rate of 96%, indicating an
optimal balance between computational efficiency and accuracy.
For this example, BNN-MC-Dropout has the lowest ECE value compared to other methods (see Appendix),
but it also have a lower accuracy rate. Among the FBNN variants, FBNN (SGHMC-pCN) presents a low
ECE, closely aligning with the ECE value of BNN-MC-Dropout, while providing an accuracy rate similar to
DNN. Moreover, as illustrated in Figure 7, the FBNN variants, particularly FBNN (SGHMC-pCN), appear
to be better calibrated across most probability ranges except at the highest probabilities, suggesting a more
reliable UQ. Note that in these figures, a model with a reliability curve that closely follows the diagonal line
is considered better calibrated, meaning its predicted probabilities are more aligned with actual outcomes.
Adult Data. Next, weusetheAdultdataset(Becker&Kohavi,1996), wheretheclassificationtaskinvolves
predicting whether an individual will earn more or less than $50,000 per year. Figure 3 demonstrates that
all the methods achieve comparable accuracy rates, although DNN, Ensemble-DNN, and FBNN(SGHMC-
pCN) have the best performance. FBNN (SGHMC-pCN) also stands out as the most computationally
efficient method for uncertainty quantification, with a speedup value of 54.91 relative to the baseline BNN
approach. A low ECE value for the FBNN (SGHMC-pCN) model signifies its superior performance in terms
of uncertainty quantification.
Alzheimer Data. We have used the same NACC dataset we previously discussed, but this time as a
classification problem. Here, our objective is to predict cognitive status, classifying individuals as either
cognitively unimpaired (healthy controls, HC), labeled as class 0, or as having mild cognitive impairment
(MCI) due to Alzheimer’s disease (AD) or dementia due to AD, labeled as class 1.
Achieving the highest accuracy of 84% at a relatively low computational cost, FBNN (SGHMC-pCN) sur-
passes all other models in correctly identifying Alzheimer’s disease, making it the most reliable model among
those tested. This model not only surpasses the accuracy of the standard DNN and Ensemble-DNN mod-
els but also offers a balance between computational efficiency and high accuracy. FBNN (SGHMC-pCN)
demonstrates the highest sampling efficiency (speedup of 11), indicating it can achieve high accuracy with a
lower computational cost compared to other Bayesian models. Moreover, for the FBNN model implemented
on the Alzheimer dataset, the ECE value is low, and the reliability curve closely tracks the diagonal line.
MNIST Dataset. The MNIST dataset is commonly used as a benchmark dataset for the hand-written
digit classification task (Deng, 2012). Among the various models evaluated on the MNIST dataset, FBNN
(SGHMC-pCN) stands out as the optimal choice, demonstrating exceptional performance across multiple
metrics. It achieves the highest accuracy of 94%, matching the top performance of Ensemble-DNN but
with significantly improved efficiency and effectiveness in uncertainty quantification. It exhibits a substan-
tial speedup of 16.77, and the lowest ECE at 0.241, suggesting that it not only provides highly accurate
predictions but also reliably estimates the uncertainty associated with these predictions.
CelebA Dataset. CelebA (Liu et al., 2015) is an image dataset of celebrity faces annotated with 40
attributes including gender, hair color, age, smiling, etc. The task is to predict hair color, which is either
blondY= 1or non-blond Y= 0. The FBNN (SGHMC-pCN) model stands out among the alternative
methods,showcasingitssuperioritythroughseveralkeyperformancemetrics. Itachievesthehighestaccuracy
of 85%, and the highest speedup factor of 61.84, indicating an exceptional balance between computational
efficiencyandperformance. ThemodelachievesthelowestECEof0.493, indicatingreliabilityinitspredictive
uncertainty.
SVHN dataset. The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) includes labelled
real-world images of house numbers taken from Google Street View. The images are 32x32 pixels in size and
have three color channels (RGB). The goal is to classify digit images into 10 classes. The results demonstrate
18Published in Transactions on Machine Learning Research (09/2024)
thesuperiorityofFBNN(SGHMC-pCN)intermsofaccuracyandcomputationalefficiency. FBNN(SGHMC-
pCN) achieved an accuracy of 96%, the second highest among all models. The speedup compared to the
baseline BNN-SGHMC method was 14.99 times, the highest speedup value recorded. Additionally, the low
ECE of 0.203 demonstrates better uncertainty quantification than most other methods, including BNN-VI,
BNN-MC-Dropout, and SWAG.
5 Conclusion
In this paper, we have proposed an innovative CES framework called FBNN, specifically designed to enhance
thecomputationalefficiencyandscalabilityofBNNforhigh-dimensionaldata. Ourprimarygoalistoprovide
a robust solution for uncertainty quantification in high-dimensional spaces, leveraging the power of Bayesian
inference while mitigating the computational bottlenecks traditionally associated with BNN.
In our numerical experiments, we have successfully applied several variants of FBNN, including different
configurations with BNN, to regression and classification tasks on both synthetic and real datasets. Remark-
ably, the FBNN variant incorporating SGHMC for calibration and pCN for sampling, denoted as FBNN
(SGHMC-pCN), not only matches the predictive accuracy of traditional BNN but also offers substantial
computational advantages. More specifically, our numerical experiments across various regression and classi-
fication tasks consistently demonstrate the superiority of the FBNN (SGHMC-pCN) method over traditional
BNNs and DNNs. In regression tasks, FBNN (SGHMC-pCN) demonstrates a competitive MSE while signif-
icantly enhancing computational efficiency, achieving notable speedups compared to baseline models. This
efficiency does not come at the expense of accuracy, as evidenced by the competitive MSE values and robust
uncertainty quantification metrics. In classification tasks, FBNN (SGHMC-pCN) stands out by achieving
high accuracy rates and low ECE values, which indicate reliable uncertainty quantification.
The superior performance of FBNN (SGHMC-pCN) can be attributed to the complementary strengths of
SGHMC and pCN. SGHMC excels at broad exploration of the parameter space, providing an effective
means for understanding the global structure during the calibration step. On the other hand, pCN is adept
at efficient sampling around modes, offering a valuable tool for capturing local intricacies in the distribution
during the final sampling step. By combining these samplers within the FBNN framework, we achieve a
balanced approach between exploration (calibration with SGHMC) and exploitation (final sampling with
pCN).
Future work could involve extending our method to more complex problems (e.g., spatiotemporal data)
and complex network structures (e.g., graph neural networks). Additionally, future research could focus on
improving the emulation step by optimizing the DNN architecture. Finally, our method could be further
improved by embedding the sampling algorithm in an adaptive framework similar to the method of Zhang
et al. (2018).
Acknowledgements
The authors thank the Editor, Action Editor, and anonymous reviewers for their insightful suggestions and
constructive feedback, which significantly improved the article. This work was supported by NSF grants
NCS-FR-2319618 and DMS-2134256.
References
Sungjin Ahn, Babak Shahbaba, and Max Welling. Distributed stochastic gradient MCMC. In International
conference onmachine learning, pp. 1044–1052. PMLR, 2014.
Julyan Arbel, Konstantinos Pitas, Mariia Vladimirova, and Vincent Fortuin. A primer on Bayesian neural
networks: review and debates. arXivpreprint arXiv:2309.16314, 2023.
Barry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 1996. doi: 10.24432/C5XW20.
19Published in Transactions on Machine Learning Research (09/2024)
Duane L Beekly, Erin M Ramos, Gerald van Belle, Woodrow Deitrich, Amber D Clark, Mary E Jacka,
Walter A Kukull, et al. The national Alzheimer’s coordinating center (NACC) database: an Alzheimer
disease database. Alzheimer Disease&Associated Disorders, 18(4):270–277, 2004.
Duane L Beekly, Erin M Ramos, William W Lee, Woodrow D Deitrich, Mary E Jacka, Joylee Wu, Janene L
Hubbard, Thomas D Koepsell, John C Morris, Walter A Kukull, et al. The National Alzheimer’s Coor-
dinating Center (NACC) database: the uniform data set. Alzheimer Disease&Associated Disorders, 21
(3):249–258, 2007.
Thierry Bertin-Mahieux. Year Prediction MSD. UCI Machine Learning Repository, 2011. DOI:
https://doi.org/10.24432/C50K61.
Alexandros Beskos. A stable manifold MCMC method for high dimensions. Statistics &Probability Letters,
90:46–52, 2014.
Alexandros Beskos, Gareth Roberts, and Andrew Stuart. Optimal scalings for local Metropolis–Hastings
chains on nonproduct targets in high dimensions. TheAnnalsofAppliedProbability, 19(3):863–898, 2009.
doi: 10.1214/08-AAP563.
Alexandros Beskos, Frank J Pinski, Jesús Marıa Sanz-Serna, and Andrew M Stuart. Hybrid Monte Carlo
on Hilbert spaces. Stochastic Processes andtheirApplications, 121(10):2201–2230, 2011.
Alexandros Beskos, Mark Girolami, Shiwei Lan, Patrick E Farrell, and Andrew M Stuart. Geometric mcmc
for infinite-dimensional inverse problems. JournalofComputational Physics, 335:327–351, 2017.
Michael Betancourt. The fundamental incompatibility of scalable Hamiltonian Monte Carlo and naive data
subsampling. In International Conference onMachine Learning, pp. 533–540. PMLR, 2015.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural
network. In International conference onmachine learning, pp. 1613–1622. PMLR, 2015.
Edwin V Bonilla, Kian Chai, and Christopher Williams. Multi-task gaussian process prediction. Advances
inNeuralInformation Processing Systems, 20, 2007.
TianqiChen, EmilyFox, andCarlosGuestrin. StochasticgradientHamiltonianMonteCarlo. In International
Conference onMachine Learning, pp. 1683–1691, 2014.
Jian Cheng, Pei-song Wang, Gang Li, Qing-hao Hu, and Han-qing Lu. Recent advances in efficient computa-
tion of deep convolutional neural networks. Frontiers ofInformation Technology &Electronic Engineering,
19:64–77, 2018.
Emmet Cleary, Alfredo Garbuno-Inigo, Shiwei Lan, Tapio Schneider, and Andrew M. Stuart. Calibrate,
emulate, sample. JournalofComputational Physics, 424:109716, 2021. doi: 10.1016/j.jcp.2020.109716.
PauloCortez, AntonioCerdeira, FernandoAlmeida, TelmoMatos, andJoseReis. Winequality. UCIMachine
Learning Repository, 2009. DOI: https://doi.org/10.24432/C56S3T.
Simon L Cotter, Gareth O Roberts, Andrew M Stuart, and David White. Mcmc methods for functions:
modifying old algorithms to make them faster. 2013.
Tiangang Cui, Kody JH Law, and Youssef M Marzouk. Dimension-independent likelihood-informed MCMC.
JournalofComputational Physics, 304:109–137, 2016.
Carla Currin, Toby Mitchell, Max Morris, and Don Ylvisaker. A Bayesian approach to the design and
analysis of computer experiments. Technical report, Oak Ridge National Lab., TN (USA), 1988.
Giuseppe Da Prato and Jerzy Zabczyk. Stochastic equations ininfinitedimensions. Cambridge University
Press, 2014.
20Published in Transactions on Machine Learning Research (09/2024)
Shaveta Dargan, Munish Kumar, Maruthi Rohit Ayyagari, and Gulshan Kumar. A survey of deep learn-
ing and its applications: a new paradigm to machine learning. Archives ofComputational Methods in
Engineering, 27:1071–1092, 2020.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEESignal
Processing Magazine, 29(6):141–142, 2012.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty
in deep learning. In International conference onmachine learning, pp. 1050–1059. PMLR, 2016.
Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, and Andrew G Wilson. GPyTorch: Black-
box matrix-matrix gaussian process inference with GPU acceleration. Advances inNeuralInformation
Processing Systems, 31, 2018.
Andrew Gelman, Walter R. Gilks, and Gareth O. Roberts. Weak convergence and optimal scaling of random
walk Metropolis algorithms. TheAnnalsofAppliedProbability, 7(1):110–120, 1997.
Alex Graves. Practical variational inference for neural networks. In Proceedings ofthe24thInternational
Conference onNeuralInformation Processing Systems, NIPS’11, pp. 2348–2356, Red Hook, NY, USA,
2011. Curran Associates Inc. ISBN 9781618395993.
Ingo Gühring, Gitta Kutyniok, and Philipp Petersen. Error bounds for approximations with deep ReLU
neural networks in ws,pnorms.Analysis andApplications, 18(05):803–859, 2020.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In
Proceedings ofthe34thInternational Conference onMachine Learning, pp. 1321–1330. PMLR, August
2017.
Martin Hairer, Andrew M Stuart, and Jochen Voss. Sampling conditioned diffusions. TrendsinStochastic
Analysis, 353:159–186, 2009.
David Harrison Jr and Daniel L Rubinfeld. Hedonic housing prices and the demand for clean air. Journal
ofenvironmental economics andmanagement, 5(1):81–102, 1978.
WilfredK.Hastings. MonteCarlosamplingmethodsusingMarkovchainsandtheirapplications. Biometrika,
57(1):97–109, 1970.
Dave Higdon, Marc Kennedy, James C. Cavendish, John A. Cafeo, and Robert D. Ryne. Combining field
data and computer simulations for calibration and prediction. SIAMJournalonScientific Computing, 26
(2):448–466, 2004. doi: 10.1137/S1064827503426693.
Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description
length of the weights. In Proceedings ofthesixthannualconference onComputational learning theory,
pp. 5–13, 1993.
Matthew D. Hoffman and Andrew Gelman. The no-u-turn sampler: Adaptively setting path lengths in
hamiltonian monte carlo. JournalofMachine Learning Research, 15(47):1593–1623, 2014.
Matthew D. Hoffman, David M. Blei, and Francis R. Bach. Online learning for latent Dirichlet allocation.
In John D. Lafferty, Christopher K. I. Williams, John Shawe-Taylor, Richard S. Zemel, and Aron Culotta
(eds.),Advances inNeuralInformation Processing Systems (NurIPS), pp. 856–864. Curran Associates,
Inc., 2010.
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Av-
eraging weights leads to wider optima and better generalization. In Amir Globerson and Ricardo Silva
(eds.),Proceedings oftheThirty-Fourth Conference onUncertainty inArtificial Intelligence, UAI2018,
Monterey, California, USA,August6-10,2018, pp. 876–885. AUAI Press, 2018.
Tommi S Jaakkola and Michael I Jordan. Bayesian parameter estimation via variational methods. Statistics
andComputing, 10:25–37, 2000.
21Published in Transactions on Machine Learning Research (09/2024)
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to
variational methods for graphical models. Machine learning, 37, 1999.
Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine, and Mohammed Bennamoun. Hands-
on Bayesian neural networks—a tutorial for deep learning users. IEEEComputational Intelligence
Magazine, 17(2):29–48, 2022.
Marc C. Kennedy and Anthony O’Hagan. Bayesian calibration of computer models. JournaloftheRoyal
Statistical SocietySeriesB:Statistical Methodology, 63(3):425–464, 2002. doi: 10.1111/1467-9868.00294.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In Yoshua Bengio and Yann LeCun
(eds.),2ndInternational Conference onLearning Representations, ICLR2014,Banff,AB,Canada, April
14-16,2014,Conference TrackProceedings, 2014.
Yongchan Kwon, Joong-Ho Won, Beom Joon Kim, and Myunghee Cho Paik. Uncertainty quantification
using Bayesian neural networks in classification: Application to ischemic stroke lesion segmentation. In
Medical Imaging withDeepLearning, 2022.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncer-
tainty estimation using deep ensembles. Advances inneuralinformation processing systems, 30, 2017.
Shiwei Lan, Julia A Palacios, Michael Karcher, Vladimir N Minin, and Babak Shahbaba. An efficient
Bayesian inference framework for coalescent-based nonparametric phylodynamics. Bioinformatics, 31(20):
3282–3289, 2015.
Shiwei Lan, Shuyi Li, and Babak Shahbaba. Scaling up Bayesian uncertainty quantification for inverse
problems using deep neural networks. SIAM/ASA JournalonUncertainty Quantification, 10(4):1684–
1713, 2022. doi: 10.1137/21M1439456.
Kody JH Law. Proposals which speed up function-space MCMC. JournalofComputational andApplied
Mathematics, 262:127–138, 2014.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-
ment recognition. Proceedings oftheIEEE, 86(11):2278–2324, 1998.
Lingge Li, Andrew Holbrook, Babak Shahbaba, and Pierre Baldi. Neural network gradient Hamiltonian
Monte Carlo. Computational Statistics, 34:281–299, 2019.
Faming Liang, Qizhai Li, and Lei Zhou. Bayesian neural networks for selection of drug sensitive genes.
JournaloftheAmerican Statistical Association, 113(523):955–972, 2018.
Haitao Liu, Yew-Soon Ong, Xiaobo Shen, and Jianfei Cai. When gaussian process meets big data: A review
of scalable gps. IEEETransactions onNeuralNetworks andLearning Systems, 31(11):4405–4423, 2020.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings oftheIEEEInternational Conference onComputer Vision, pp. 3730–3738, 2015.
David JC MacKay. A practical bayesian framework for backpropagation networks. NeuralComputation, 4
(3):448–472, 1992.
Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple
baseline for Bayesian uncertainty in deep learning. Advances inneuralinformation processing systems,
32, 2019.
Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller.
Equation of state calculations by fast computing machines. TheJournal ofChemical Physics, 21(6):
1087–1092, 1953.
Tom Minka. Divergence measures and message passing. Technical Report MSR-TR-2005-173, January 2005.
22Published in Transactions on Machine Learning Research (09/2024)
Radford M Neal. MCMC using Hamiltonian dynamics. Handbook ofMarkovChainMonteCarlo, 2(11):2,
2011.
Radford M Neal. Bayesian learning forneuralnetworks, volume 118. Springer Science & Business Media,
2012.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. Reading
digits in natural images with unsupervised feature learning. In NIPSworkshop ondeeplearning and
unsupervised featurelearning, volume 2011, pp. 4. Granada, 2011.
Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring
calibration in deep learning. In Proceedings oftheIEEE/CVF Conference onComputer Visionand
PatternRecognition (CVPR) Workshops, 2019.
Jeremy Oakley and Anthony O’Hagan. Bayesian inference for the uncertainty distribution of computer model
outputs. Biometrika, 89(4):769–784, 2002. doi: 10.1093/biomet/89.4.769.
Jeremy E. Oakley and Anthony O’Hagan. Probabilistic sensitivity analysis of complex models: A Bayesian
approach. JournaloftheRoyalStatistical Society. SeriesB(Statistical Methodology), 66(3):751–769,
2004.
Michael P Perrone and Leon N Cooper. When networks disagree: Ensemble methods for hybrid neural
networks. In HowWeLearn;HowWeRemember: TowardAnUnderstanding OfBrainAndNeural
Systems: Selected PapersofLeonNCooper, pp. 342–358. World Scientific, 1995.
Yueqi Ren, Babak Shahbaba, and Craig E Stark. Hierarchical, multiclass prediction of Alzheimer’s clinical
diagnosis using imputed, multimodal NACC data. Alzheimer’s &Dementia, 18:e066698, 2022.
DaniloJimenezRezende,ShakirMohamed,andDaanWierstra. Stochasticbackpropagationandapproximate
inference in deep generative models. In International conference onmachine learning, pp. 1278–1286.
PMLR, 2014.
Gareth O. Roberts and Jeffrey S. Rosenthal. Optimal scaling of discrete approximations to Langevin diffu-
sions.JournaloftheRoyalStatistical Society: SeriesB(Statistical Methodology), 60(1):255–268, 1998.
Matthias W. Seeger, Christopher K. I. Williams, and Neil D. Lawrence. Fast forward selection to speed up
sparse gaussian process regression. In International Workshop onArtificial Intelligence andStatistics, pp.
254–261. PMLR, 2003.
Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classification
uncertainty. Advances inneuralinformation processing systems, 31, 2018.
Babak Shahbaba, Shiwei Lan, Wesley O Johnson, and Radford M Neal. Split Hamiltonian Monte Carlo.
Statistics andComputing, 24(3):339–349, 2014.
Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep neural
networks. IEEETransactions onEvolutionary Computation, 23(5):828–841, 2019.
Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S. Emer. Efficient processing of deep neural networks:
A tutorial and survey. Proceedings oftheIEEE, 105(12):2295–2329, 2017.
Wiesje M. van der Flier and Philip Scheltens. Hippocampal volume loss and alzheimer disease progression.
NatureReviews Neurology, 5(7):361–362, Jul 2009.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings
ofthe28thInternational Conference onMachine Learning (ICML-11), pp. 681–688, 2011.
Cheng Zhang, Babak Shahbaba, and Hongkai Zhao. Hamiltonian Monte Carlo acceleration using surrogate
functions with random bases. Statistics andComputing, 27(6), 2017. ISSN 15731375. doi: 10.1007/
s11222-016-9699-1.
ChengZhang,BabakShahbaba,andHongkaiZhao. VariationalHamiltonianMonteCarloviascorematching.
Bayesian Analysis, 13(2), 2018. ISSN 19316690. doi: 10.1214/17-BA1060.
23Published in Transactions on Machine Learning Research (09/2024)
Appendix A: Comparing Various Deep Learning Techniques for Regression and
Classification Problems
Table A1: Performance of various deep learning methods based on regression problems. For ESS, minimum,
median, and maximum values are provided in parenthesis.
Dataset Method MSE CP Time (s) ESS minESS/s spdup
Simulated DNN 0.71 - 3531 - - -
Dataset Ensemble-DNN 0.74 91.3% 14633 - - -
BNN-VI 1.14 88.8% 8408 - - -
BNN-Lasso 0.98 87.4% 6941 - - -
BNN-MC-Dropout 0.83 93.4% 2861 - - -
BNN-SGHMC 0.79 91.6% 41281 (109, 829, 1642) 0.002 1.00
BNN-SGHMC(first 200) 4.48 92.8% 3970 (21, 66, 162) 0.005 2.01
SWAG 0.73 90.0% 13488 (93, 1163, 1542) 0.006 2.61
BNN-RNS-HMC 1.62 87.5% 6046 (126, 948, 1510) 0.021 7.89
BNN-pCN 0.81 89.3% 42523 (107, 844, 1533) 0.002 0.95
FBNN (pCN-SGHMC) 0.96 79.6% 4512 (132, 1241, 1615) 0.029 11.08
FBNN (pCN-pCN) 0.86 85.2% 4631 (94, 975, 1621) 0.020 7.69
FBNN (SGHMC-SGHMC) 0.82 90.1% 4497 (137, 1236, 1638) 0.030 11.53
FBNN (SGHMC-pCN) 0.74 92.2% 4312 (186, 912, 1606) 0.043 16.33
Wine DNN 0.43 - 26 - - -
Quality Ensemble-DNN 0.41 47.4% 137 - - -
BNN-VI 0.66 39.5% 28 - - -
BNN-Lasso 0.63 40.9% 42 - - -
BNN-MC-Dropout 0.67 32.3% 12 - - -
BNN-SGHMC 0.53 51.3% 505 (111, 837, 1538) 0.219 1.00
BNN-SGHMC(first 200) 2.75 54.6% 61 (13, 111, 150) 0.213 0.91
SWAG 0.53 48.2% 97 (98, 1021, 1489) 1.010 4.39
BNN-RNS-HMC 0.62 44.7% 38 (107, 925, 1520) 2.815 12.24
BNN-pCN 0.65 51.1% 620 (99, 1003, 1532) 0.159 0.69
FBNN (pCN-SGHMC) 0.52 32.2% 68 (91, 912, 1533) 1.338 5.95
FBNN (pCN-pCN) 0.65 24.5% 67 (105, 1087, 1540) 1.567 6.86
FBNN (SGHMC-SGHMC) 0.50 40.0% 70 (77, 806, 1536) 1.100 4.78
FBNN (SGHMC-pCN) 0.52 48.1% 57 (92, 897, 1536) 1.614 7.33
Boston DNN 3.21 - 14 - - -
Housing Ensemble-DNN 3.17 72.1% 74 - - -
BNN-VI 7.60 83.7% 85 - - -
BNN-Lasso 6.20 79.2% 68 - - -
BNN-MC-Dropout 10.12 81.2% 91 - - -
BNN-SGHMC 3.83 75.3% 888 (76, 649, 1536) 0.085 1.00
BNN-SGHMC(first 200) 7.40 66.9% 86 (9, 87, 150) 0.104 1.22
SWAG 5.81 71.9% 104 (68, 724, 1532) 0.653 7.22
BNN-RNS-HMC 9.42 73.4% 76 (73, 1032, 1504) 0.960 10.6
BNN-pCN 3.25 79.3% 901 (76, 649, 1536) 0.084 0.88
FBNN (pCN-SGHMC) 4.16 41.7% 186 (71, 965, 1543) 0.381 4.22
FBNN (pCN-pCN) 3.81 47.1% 186 (80, 966, 1541) 0.430 4.78
FBNN (SGHMC-SGHMC) 4.15 48.9% 94 (69, 979, 1542) 0.734 8.22
FBNN (SGHMC-pCN) 3.82 81.1% 91 (93, 938, 1543) 1.021 11.94
Alzheimer DNN 0.49 - 326 - - -
Dataset Ensemble-DNN 0.42 89.3% 1597 - - -
BNN-VI 0.53 87.6% 341 - - -
BNN-Lasso 0.52 83.5% 561 - - -
BNN-MC-Dropout 0.60 92.8% 268 - - -
BNN-SGHMC 0.49 91.6% 6524 (102, 973, 1376) 0.015 1.00
BNN-SGHMC(first 200) 3.17 72.7% 641 (7, 82, 150) 0.011 0.69
SWAG 0.74 89.3% 1214 (106, 1002, 1542) 0.087 5.58
BNN-RNS-HMC 0.61 92.4% 7324 (96, 892, 1531) 0.013 0.83
BNN-pCN 0.51 89.9% 6212 (120, 1092, 1448) 0.019 1.23
FBNN (pCN-SGHMC) 0.50 90.2% 643 (116, 994, 1504) 0.180 11.53
FBNN (pCN-pCN) 0.56 91.4% 682 (108, 998, 1498) 0.171 10.93
FBNN (SGHMC-SGHMC) 0.55 88.4% 671 (118, 1012, 1541) 0.176 11.24
FBNN (SGHMC-pCN) 0.48 91.6% 632 (149, 984, 1497) 0.218 13.97
YearPredictionMSD DNN 74.54 - 1569 - - -
Dataset Ensemble-DNN 72.89 90.47% 7929 - - -
BNN-VI 78.21 88.43% 2146 - - -
BNN-Lasso 79.44 89.04% 3243 - - -
BNN-MC-Dropout 83.08 87.89% 1287 - - -
BNN-SGHMC 81.67 83.81% 25533 (122, 1005, 1540) 0.004 1.00
BNN-SGHMC(first 200) 105.92 94.13% 2613 (7, 84, 149) 0.002 0.56
SWAG 93.87 86.33% 3841 (113, 987, 1537) 0.029 7.35
BNN-RNS-HMC 92.82 80.19% 17289 (109, 925, 1563) 0.006 1.57
BNN-pCN 82.45 85.74% 25655 (124, 873, 1631) 0.004 1.20
FBNN (pCN-SGHMC) 74.92 88.73% 2815 (143, 1049, 1676) 0.050 10.63
FBNN (pCN-pCN) 74.03 90.45% 3046 (138, 992, 1618) 0.045 9.48
FBNN (SGHMC-SGHMC) 76.69 90.40% 2974 (146, 973, 1599) 0.049 10.27
FBNN (SGHMC-pCN) 73.41 92.23% 2724 (156, 934, 1608) 0.057 11.98
24Published in Transactions on Machine Learning Research (09/2024)
Table A2: Performance of various deep learning methods based on classification problems.
Dataset Method Acc Time(s) ESS(min,med,max) minESS/s spdup ECE
Simulated DNN 96% 4257 - - - -
Dataset Ensemble-DNN 97% 17415 - - - 0.382
BNN-VI 90% 4275 - - - 0.399
BNN-Lasso 92% 3189 - - - 0.363
BNN-MC-Dropout 87% 2912 - - - 0.277
BNN-SGHMC 95% 43841 (47, 212, 1459) 0.001 1.00 0.471
BNN-SGHMC(first 200) 83% 4218 (21, 59, 156) 0.004 4.64 0.498
SWAG 81% 4731 (81, 773, 1368) 0.017 15.97 0.482
BNN-RNS-HMC 69% 1309 (135, 1190, 1493) 0.103 96.20 0.513
BNN-pCN 91% 49774 (36, 207, 1417) 0.001 0.67 0.475
FBNN (pCN-SGHMC) 93% 5179 (134, 959, 1419) 0.051 24.13 0.409
FBNN (pCN-pCN) 91% 4858 (146, 921, 1412) 0.058 28.03 0.423
FBNN (SGHMC-SGHMC) 95% 4517 (149, 891, 1540) 0.032 30.76 0.406
FBNN (SGHMC-pCN) 96% 4489 (154, 911, 1602) 0.070 32.00 0.396
Adult DNN 85% 426 - - - -
Dataset Ensemble-DNN 84% 2153 - - - 0.556
BNN-VI 80% 562 - - - 0.642
BNN-Lasso 83% 256 - - - 0.631
BNN-MC-Dropout 82% 187 - - - 0.540
BNN-SGHMC 83% 5979 (16, 202, 1520) 0.002 1.00 0.574
BNN-SGHMC(first 200) 78% 581 (1, 41, 148) 0.002 0.95 0.594
SWAG 79% 1641 (47, 912, 1532) 0.028 10.70 0.668
BNN-RNS-HMC 72% 6110 (89, 960, 1530) 0.014 5.44 0.658
BNN-pCN 83% 6227 (9, 117, 1518) 0.001 0.54 0.616
FBNN (pCN-SGHMC) 82% 642 (87, 892, 1539) 0.135 50.63 0.580
FBNN (pCN-pCN) 82% 639 (88, 890, 1540) 0.137 51.46 0.592
FBNN (SGHMC-SGHMC) 83% 612 (68, 941, 1541) 0.111 41.52 0.583
FBNN (SGHMC-pCN) 84% 609 (89, 875, 1539) 0.146 54.91 0.576
Alzheimer DNN 82% 51 - - - -
Dataset Ensemble-DNN 83% 262 - - - 0.542
BNN-VI 72% 61 - - - 0.546
BNN-Lasso 76% 256 - - - 0.524
BNN-MC-Dropout 76% 12 - - - 0.429
BNN-SGHMC 81% 2736 (81, 588, 1526) 0.029 1.00 0.499
BNN-SGHMC(first 200) 69% 282 (8, 84, 149) 0.028 0.96 0.523
SWAG 81% 312 (72, 913, 1562) 0.231 7.69 0.508
BNN-RNS-HMC 58% 293 (84, 915, 1540) 0.286 9.55 0.521
BNN-pCN 73% 2660 (71, 424, 1534) 0.026 0.90 0.469
FBNN (pCN-SGHMC) 76% 277 (76, 947, 1542) 0.274 9.26 0.568
FBNN (pCN-pCN) 77% 274 (70, 931, 1542) 0.255 8.33 0.377
FBNN (SGHMC-SGHMC) 80% 278 (81, 973, 1538) 0.291 8.63 0.448
FBNN (SGHMC-pCN) 84% 280 (92, 914, 1535) 0.328 11.09 0.376
Mnist DNN 92% 231 - - - -
Dataset Ensemble-DNN 94% 1129 - - - 0.312
BNN-VI 86% 273 - - - 0.417
BNN-Lasso 87% 184 - - - 0.445
BNN-MC-Dropout 89% 212 - - - 0.328
BNN-SGHMC 90% 8641 (15, 364, 1456) 0.001 1.00 0.280
BNN-SGHMC(first 200) 78% 916 (1, 34, 149) 0.001 0.62 0.271
SWAG 83% 1294 (15, 431, 1376) 0.011 7.72 0.327
BNN-RNS-HMC 69% 4541 (14, 372, 1394) 0.003 2.05 0.349
BNN-pCN 88% 8912 (17, 398, 1471) 0.001 1.26 0.321
FBNN (pCN-SGHMC) 88% 927 (15, 412, 1383) 0.016 10.78 0.352
FBNN (pCN-pCN) 90% 931 (16, 393, 1421) 0.017 11.45 0.312
FBNN (SGHMC-SGHMC) 91% 923 (18, 409, 1461) 0.019 13.01 0.283
FBNN (SGHMC-pCN) 94% 914 (23, 474, 1521) 0.025 16.77 0.241
celebA DNN 80% 3689 - - - -
Dataset Ensemble-DNN 82% 15445 - - - 0.569
BNN-VI 80% 1132 - - - 0.622
BNN-Lasso 79% 2159 - - - 0.561
BNN-MC-Dropout 78% 1641 - - - 0.512
BNN-SGHMC 81% 17234 (19, 383, 1537) 0.001 1.00 0.567
BNN-SGHMC(first 200) 69% 1849 (1, 85, 149) 0.001 0.63 0.642
SWAG 70% 8913 (85, 1014, 1467) 0.009 8.65 0.534
BNN-RNS-HMC 72% 1835 (72, 951, 1494) 0.039 35.59 0.612
BNN-pCN 76% 19676 (19, 331, 1538) 0.001 0.95 0.529
FBNN (pCN-SGHMC) 80% 1972 (99, 1155, 1542) 0.050 45.53 0.565
FBNN (pCN-pCN) 79% 1951 (116, 1155, 1542) 0.059 53.93 0.542
FBNN (SGHMC-SGHMC) 81% 1904 (93, 978, 1544) 0.048 44.30 0.568
FBNN (SGHMC-pCN) 85% 1892 (129, 785, 1517) 0.068 61.84 0.493
SVHN DNN 96% 3748 - - - -
Dataset Ensemble-DNN 97% 17415 - - - 0.382
BNN-VI 90% 4275 - - - 0.399
BNN-Lasso 92% 3189 - - - 0.363
BNN-MC-Dropout 87% 2912 - - - 0.277
BNN-SGHMC 91% 18639 (31, 379, 1528) 0.001 1.00 0.221
BNN-SGHMC(first 200) 74% 2515 (5, 67, 262) 0.001 1.19 0.246
SWAG 83% 6294 (78, 499, 1383) 0.012 7.45 0.282
BNN-RNS-HMC 71% 9083 (19, 410, 1384) 0.002 1.25 0.299
BNN-pCN 93% 17812 (27, 398, 1317) 0.001 0.91 0.275
FBNN (pCN-SGHMC) 93% 1931 (36, 391, 1403) 0.018 11.21 0.263
FBNN (pCN-pCN) 91% 1927 (29, 372, 1280) 0.015 9.04 0.318
FBNN (SGHMC-SGHMC) 96% 1891 (34, 412, 1464) 0.017 10.81 0.248
FBNN (SGHMC-pCN) 96% 1884 (47, 486, 1533) 0.024 14.99 0.223
25