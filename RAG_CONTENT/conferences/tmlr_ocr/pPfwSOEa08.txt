Under review as submission to TMLR
Momentum via Primal Averaging: Theoretical Insights and
Learning Rate Schedules for Non-Convex Optimization
Anonymous authors
Paper under double-blind review
Abstract
Momentum methods are now used pervasively within the machine learning community for
training non-convex models such as deep neural networks. Empirically, they outperform
traditional stochastic gradient descent (SGD) approaches. In this work we develop an Lya-
punov analysis of SGD with momentum (SGD+M), by utilizing an equivalent rewriting of
the method known as the stochastic primal averaging (SPA) form. This analysis is tight
enough to give precise insights into when SGD+M may outperform SGD, and what hyper-
parameter schedules will work and why. Surprisingly, we show that the commonly used
stage-wise schedule doesn’t make sense in SPA form, and discuss how to ﬁx it. Our theory
suggests that momentum is only useful at the early stages of training, and we verify this
empirically by showing that dropping momentum after one epoch results in no loss of ﬁnal
test accuracy on CIFAR-10 and ImageNet training.
1 Introduction
Heavy ball methods have a long history dating back to the work of Polyak (1964). More recently, the
stochastic heavy ball method, also known as stochastic gradient descent with momentum (SGD+M), has
become a standard for deep learning practitioners since it was observed that momentum signiﬁcantly helps
on common computer vision problems (Sutskever et al., 2013).
In this work we provide an analysis of SGD+M for non-convex problems that is much tighter than past
approaches. The form of this analysis is tight enough to provide several insights into the practical behavior
of SGD+M, including suggesting hyper-parameter schemes and indicating why SGD+M is faster than SGD
at the early stages of optimization. We believe our analysis technique is also useful in it’s own right, and
may be a good starting point for analyzing other methods that involve momentum.
There is a substantial body of prior work on the SGD+M method. Non-asymptotic convergence in the
non-stochastic convex setting was ﬁrst established by Ghadimi et al. (2015), where it is shown that for
parameters of the form βk=k/(k+2)andαk∝1/(k+2), the method obtains last iterate convergence rates
comparable to gradient descent. They also show that when βkis constant the best convergence rate they are
able to obtain is worse than gradient descent by a constant factor β. Unfortunately their proof technique
does not extend readily to the stochastic setting. Flammarion and Bach (2015) consider both momentum
and accelerated methods for convex quadratic problems, where they are able to establish bounds using the
technique of diﬀerence equations, even with noisy (but not stochastic) gradients.
Yuan et al. (2016) analyze momentum methods under the assumption of strong convexity and small step
sizes in the online setting, and show no actual advantage to momentum methods in this setting. Can et al.
(2019) establish strong results in another special case, where gradient noise is bounded and the objective is
either strongly convex or quadratic. Needell et al. 2014 also consider the strongly-convex case, using proof
techniques developed for the randomized Kaczmarz algorithm. Also under a quadratic assumption, Jain
et al. (2018) analyzed an accelerated scheme related to Nesterov’s accelerated method in the stochastic case.
While the heavy ball method is known to provide accelerated convergence rates for quadratic problems, these
rates provably do not extend to the non-quadratic case (Kidambi et al., 2018).
1Under review as submission to TMLR
Yanetal.(2018)providetheﬁrstanalysisofmomentum(withanearlierpreprintYangetal.,2016), including
Nesterov’s scheme, in the non-convex case, establishing a bound of the form:
min
k=0,...,tE/bracketleftBig
/bardbl∇f(xk)/bardbl2/bracketrightBig
≤2 [f(x0)−f∗] (1−β)
t+ 1max/braceleftbigg2L
1−β,√t+ 1
C/bracerightbigg
+CLβ2/parenleftBig
G2+σ2+Lσ2(1−β)2/parenrightBig
√t+ 1 (1−β)3,
where/bardbl∇f(x)/bardbl≤G,E/bracketleftBig
/bardbl∇f(x,ξ)−∇f(x)/bardbl2/bracketrightBig
≤σ2,Cis a positive constant, and f is L-Lipschitz smooth,
for method Eq. 1. This rate is much looser than the rate we establish in this work, and our rate includes no
unspeciﬁed constants. Yu et al. (2020) consider the distributed non-convex setting, where they establish a
rate that is also looser than our own. A general result of almost-sure convergence is shown by Gadat et al.
(2018) in the non-convex setting.
Recently, Sebbouh et al. (2020) establish rates for the convex and strongly convex settings in the stochastic
case that mirror the tight rates in the deterministic case of Ghadimi et al. (2015), using a Lyapunov function
analysis. Along with Tao et al., 2020 and Defazio and Gower, 2020, this line of work shows that the primary
advantage of the heavy ball method over SGD is that it it is possible to show tight convergence of the last-
iterate, rather than an average of iterates (as for SGD). Last-iterate convergence rates for SGD are weaker
than the average iterate convergence unless very careful parameter schemes are used (Jain et al., 2019), and
even then only when the stopping time is known in advance.
For the non-convex setting, the closest work to ours is that of Liu et al. (2020), who use a Lyapunov analysis
and make use of the same zkquantity that we use in this work, as an ancillary point. In our view zkshould
be a key part of the algorithm, rather than a derived quantity. They give the following bound on their
Lyapunov function Λk:
E[Λk+1]−Λk
≤/parenleftbigg
−α+−β+β2
2(1−β)Lα2+ 4c1α2/parenrightbigg
E/bracketleftBig
/bardblgk/bardbl2/bracketrightBig
+β2
2(1 +β)Lα2σ2+1
2Lα2σ2+ 2c11−β
1 +βα2σ2
where Λk=f(zk)−f∗+/summationtextk−1
i=1ci/vextenddouble/vextenddoublexk+1−i−xk−i/vextenddouble/vextenddouble2.We refer the reader to their paper for details in the
values ofc,αand the settings in which this bound holds. This bound is looser than the one we derive, and
provides less insight into the practical behavior of SGD+M than the bound we derive in this work. In other
work on the non-convex case, Cutkosky and Mehta (2020) analyze a form of SGD+M with normalized steps.
The recent work of Mai and Johansson (2020) analyze SGD+M under a weak convexity assumption as well
as in the smooth case, using diﬀerent proof techniques than we explore in this work, resulting in a looser
bound.
2 The averaging form of momentum
The stochastic gradient method with momentum (SGD+M) is commonly written in the following form:
mk+1=βkmk+∇f(xk,ξk),
xk+1=xk−αkmk+1, (1)
wherexkis the iterate sequence, and mkis the momentum buﬀer, and ∇f(xk,ξk)the stochastic gradient
at stepk. For our analysis we will not use this form, instead, we will make use of the recently discovered
averaging form of the momentum method (Defazio, 2019; Sebbouh et al., 2020), also discovered as a separate
method (without relating to SGD+M) under the name SPA (stochastic primal averaging) by Tao et al.
(2020):
zk+1=zk−ηk∇f(xk,ξk),
xk+1= (1−ck+1)xk+ck+1zk+1.
2Under review as submission to TMLR
Mapping SGD+M parameters to SPA parameters
0 50 100101
100ValueSGD+M
k
 k
0 50 100105
103
101
101103105ValueSPA
k
 ck
(a)αkdecreased
0 50 100100
8×101
9×101
ValueSGD+M
k
 k
0 50 100106
102
102106ValueSPA
k
 ck (b)βkdecreased
Figure 1: The behavior of the hyper-parameters of the SPA form when they are set so as to maintain an
identical iterate sequence as the SGD+M form
For speciﬁc choices of values for the hyper-parameters, the xksequence generated by this method will be
identical to that of SGD+M. The quantity zkis actually used in some early analysis of momentum methods,
but without this explicit transformation (Ghadimi et al., 2015). A continuous time version of this update is
analyzed in Krichene et al. (2016), but without relating it to the heavy ball method.
The averaging form, compared to the standard form, appears to be easier to analyze theoretically, as the z
sequence arises naturally when performing a Lyapunov-style analysis of the method. The mapping between
the two forms is described in the following theorem.
Theorem 1. Thexksequences of the SPA method and SGD+M are equal when z0=x0and for all k≥0:
ηk+1=ηk−αk
βk+1, ck+1=αk
ηk,
conversely, αk=ηkck+1,andβk=ηk−1
ηk(1−ck).
This correspondence results in surprising dynamics when otherwise reasonable hyper-parameter schedules
are mapped from one form to another. For illustration, we will consider the case where one or both of the
parameters are changed by a ﬁxed factor, as is commonly done when using a stage-wise schedule. We apply
this change at step 20 of 100 steps, with β= 0.9andα= 1.0. Each case is shown in Figure 1.
(a)When the learning rate αof the SGD+M form is decreased by a ﬁxed factor while βis kept constant,
the learning rate in the SPA form begins to grow geometrically, and cshrinks geometrically. This is
the most common schedule used in practice for the SGD+M method, and the fact that it causes such
odd behavior in the SPA form is a cause for concern. This schedule in SPA form is NOT supported
by our Lyapunov analysis.
(b)When the momentum constant βis changed (in our example from 0.9 to 0.8), while keeping αconstant,
a similar geometric increase/decrease behavior occurs as in case 1.
Both behaviors above are unsatisfying when viewed from the perspective of the SPA method. We may also
perform the reverse operation, and consider the behavior of the hyper-parameters of the SGD+M method
when step-wise schedules are used for the SPA form (Figure 2).
(a)Whenηkis decreased 10 fold, a spike occurs in βk, after which αkdrops 10 fold and βkdrops back to
it’s earlier value.
3Under review as submission to TMLR
Mapping SPA parameters to SGD+M parameters
0 50 100101
100101ValueSGD+M
k
 k
0 50 100101
100101ValueSPA
k
 ck
(a)ηkdecreased
0 50 100100101ValueSGD+M
k
 k
0 50 100101
100101ValueSPA
k
 ck (b)ckincreased
Figure 2: The behavior of the hyper-parameters of the SGD+M form when they are set so as to maintain
an identical iterate sequence as the SGD+M form
(b)Whenckis increased 10 fold, then the SGD+M form is better behaved, as αkincreases 10 fold and βk
drops to 0. This is reasonable behavior as this change corresponds to removing the momentum in
both forms, while attempting to keep the eﬀective step size the same.
(c)As we show in Section 5, the most theoretically motivated choice is to actually change both ηkandck.
This unfortunately also results in a spike in αk
(d)Replacing the sudden change in ηkandckby a gradual change removes the spike and keeps βkbelow
1. We show in Section 6 that a gradual change is actually required by our Lyapunov theory.
3 Lyapunov analysis
In the Lyapunov analysis technique, a non-negative function Λk= Λ(x0:k,z0:k,...)is deﬁned in terms
of all indexed quantities in the algorithm up to the current time-step, for the purposes of controlling the
convergence of the optimization method under analysis. In the convex case, the standard approach is to show
thatE[f(xk)−f∗]≤Λk−E[Λk+1] +noise, after which we can apply a telescoping argument to complete
the proof. In the non-convex case we instead attempt to control the norm of the gradient of f, through a
bound of the form:
dk/bardbl∇f(xk)/bardbl2≤Λk−E[Λk+1] +noise
wheredkis some constant, and with expectations over randomness in the current step k, conditional on prior
steps (we use this convention in the remainder of this work). We call an equation of this form a Lyapunov
step equation. In the case of SGD it is straight-forward to show that the Lyapunov step takes the following
form, assuming E/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
≤G2) and thatfisL-Lipschitz smooth:
1
ηkE/bracketleftBig
/bardbl∇f(xk)/bardbl2/bracketrightBig
≤Λk−E[Λk+1] +1
2LG2+Rk, (2)
where Λk=η−2
kE[f(xk)−f∗]andRk= (η−2
k−η−2
k−1) [f(zk)−f∗]. From this Lyapunov step equation,
a standard telescoping argument (we give details in the appendix) completes the convergence rate proof,
yielding a bound on E/bracketleftBig
/bardbl∇f(xi)/bardbl2/bracketrightBig
for a randomly sampled i.
4Under review as submission to TMLR
3.1 Momentum case
In the appendix, we construct the following Lyapunov function Λfor the SGD+M method in SPA form:
Λk+1=1
η2
k[f(zk+1)−f∗] +L
ηk/parenleftbigg1
ck−1/parenrightbigg
[f(xk)−f∗] +L
2η2
kc2
k+1/bardblxk+1−xk/bardbl2(3)
Theorem 2. The SPA method obeys the following Lyapunov step equation for k≥1, with expectations
conditioning on xkand prior gradients ∇f(xi) fori≤k:
1
2ηk/bardbl∇f(xk)/bardbl2+1
2ηk/bardbl∇f(zk)/bardbl2
≤Λk−E[Λk+1] +LE/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
+Rk
+1
2/bracketleftBigg
1
η2
k/parenleftbigg1
ck−1 +ηkL/parenrightbigg/parenleftbigg1
ck−1/parenrightbigg
+1
ηkL/parenleftbigg1
ck−1/parenrightbigg2
−1
η2
k−1c2
k/bracketrightBigg
L/bardblxk−xk−1/bardbl2. (4)
where the remainder term Rk(active on steps where ηorcchanges) is deﬁned as:
Rk=/bracketleftBig
L
ηk/parenleftBig
1
ck−1/parenrightBig
−L
ηk−1/parenleftBig
1
ck−1−1/parenrightBig/bracketrightBig
[f(xk−1)−f∗] +/bracketleftBig
1
η2
k−1
η2
k−1/bracketrightBig
[f(zk)−f∗]
This bound is our key theoretical result. We give the full telescoped proof using this bound in the appendix
yielding aO(k−1/2)rate. The key diﬀerences between this bound and the bound for SGD (Equation 2) are:
1. The convergence rate is in terms of1
2ηk/bardbl∇f(zk)/bardbl2+1
2ηk/bardbl∇f(xk)/bardbl2for SGD+M compared to
1
ηk/bardbl∇f(xk)/bardbl2for SGD. When we telescope to give a convergence rate bound, the bound is on a
randomly sampled iterate from a weighted set of xkandzkrather than just xk.
2. There is an extra /bardblxk−xk−1/bardbl2term on the right which will be negative and hence beneﬁcial for
typical choices of the hyper-parameters, as we show in Section 6.
3. The noise term E/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
is weighted by Lfor SGD+M and1
2Lfor SGD. Although this
noise term is twice as large for SGD+M, we show in Section 4, that almost half of it is canceled by
the negative/bardblxk−xk−1/bardbl2term when additional assumptions are made, meaning that the noise is
actually essentially the same as SGD.
4. TheLyapunovfunctionofSGDisjust η−2
kf(xk), whereastheLyapunovfunctionofSGD+Minvolves
η−2
kf(zk)plus two other terms. After telescoping for Tsteps (as we show in the appendix), the
/bardblxk+1−xk/bardbl2term drops out, and the [f(xk)−f∗]term decays at a rate√
Tfaster than the other
terms, making it negligible at the end of optimization for typical values of ck, i.e. when/parenleftBig
1
c1−1/parenrightBig
/lessmuch
√
T. These terms appear to be the main limiting factor for how small ckcan be chosen (i.e. how
much momentum is used).
5. TheRkterm is 0 when ηk=ηk−1andck=ck−1, otherwise it contains an “error” accumulated from
changing the hyper-parameters. In a stage-wise hyper-parameter scheme this error accumulation
happens only at the end of each stage, and it’s contribution to the ﬁnal convergence rate bound will
be weighted with 1/T, signiﬁcantly smaller than the 1/√
Tweight of the primary terms. This is
similar behavior to the Rkterm in the SGD step equation.
4 Insight #1: Momentum may cancel out noise during early iterations
The noise term in the Lyapunov step of SGD+M is twice as large as the noise term1
2LE/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
in
SGD. Although typically such small diﬀerences are disregarded in the analysis of optimization methods, in
5Under review as submission to TMLR
this case we believe that this term gives substantial insight into the practical behavior of the two methods.
The diﬀerence between the bounds on the convergence rate of the two methods will depend crucially on the
magnitude of the negative /bardblxk−xk−1/bardbl2term in comparison to this noise term. When this negative iterate
diﬀerence term is suﬃciently large, SGD+M can be expected to converge faster than SGD. In this section
we analyze this term in detail. We will assume in this section that ck=candηk=ηare independent of k,
we consider in Section 6 what happens to /bardblxk−xk−1/bardbl2when they change in a step-wise scheme.
Firstly note that the the weight of /bardblxk−xk−1/bardbl2in the Lyapunov step (4) can be written in the following
form after expanding and simplifying when using constant hyper-parameters:
L
2/bracketleftbigg
−2
η2c+1
η2+L
ηc2−L
ηc/bracketrightbigg
.
To understand the magnitude of /bardblxk−xk−1/bardbl2, we may consider it’s recursive expansion:
/bardblxk−xk−1/bardbl2= (1−c)2/bardblxk−1−xk−2/bardbl2
+c2η2/bardbl∇f(xk−1,ξk−1)/bardbl2−2ηc2/parenleftbigg1
c−1/parenrightbigg
/angbracketleft∇f(xk−1),xk−1−xk−2/angbracketright. (5)
This recursive expression may be further unwound, giving a geometrically decreasing weighted sequence. We
consider the inner-product term in the next section, for the moment we assume that it has expectation zero.
The gradient term /bardbl∇f(xk−1,ξk−1)/bardbl2here gives some insight into why we may expect cancelation against
the noise term in the Lyapunov step. When this expression is unwound, it contains a contribution from all
past gradients:
k/summationdisplay
i=0(1−c)2ic2η2E/bracketleftBig
/bardbl∇f(xk−i,ξk−i)/bardbl2/bracketrightBig
,
So the noise term1
2LE/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
is not canceled immediately by the negative iterate distance
/bardblxk−xk−1/bardbl2, instead, it cancels part of the noise from past iterations. In fact, we can see that after
some stepi, the noise term introduced by that step over and above SGD, namely1
2LE/bracketleftBig
/bardbl∇f(xi,ξi)/bardbl2/bracketrightBig
will
be partially negated at every successive step, in a geometrically decaying fashion. Considering it as an
inﬁnite sum, we have:
∞/summationdisplay
i=0(1−c)2i=1
1−(1−c)2=1
c(2−c)
Is this suﬃcient for the negative terms to cancel the additional noise over SGD? Let’s consider the weight
heuristically before providing a more precise argument. Firstly, consider the weight in front of /bardblxk−xk−1/bardbl2
. The dominating term in this expression for small ηandcis−L/η2c. The/bardbl∇f(xi,ξi)/bardbl2term is multiplied
byc2η2in the geometric sum. The inﬁnite sum is above is 1/2cfor smallc, so we ﬁnd that we have:
−L
η2c·c2η2·1
2c=L
2,
which is exactly large enough to cancel the additional noise. We can make this argument precise using the
tools of Lyapunov analysis, without requiring the above simpliﬁcations. In particular, we can augment the
Lyapunov function with an additional term:
L
2ηc2/bracketleftbiggL(1−c)
c(2−c)−1
η/bracketrightbigg
/bardblxk+1−xk/bardbl2.
As we shown in the appendix, as long as η≤2c(2−c)
L(1−c),this term captures the additional noise introduced at
each step (k=i), and how it decays geometrically overtime. With the addition of this term in the Lyapunov
function, the noise term reduces to
/parenleftbigg
1 +ηL(1−c)
c(2−c)/parenrightbiggL
2E/bracketleftBig
/bardbl∇f(xi,ξi)/bardbl2/bracketrightBig
,
6Under review as submission to TMLR
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of first epoch213
29
25
21
2327Squared norm1
2Ef(xk)2
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of first epoch213
28
23
2227212Squared norm1
2cExkxk12
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of first epoch20232629212Squared normRatio
0.00 0.05 0.10 0.15 0.20
Fraction of first epoch25
22
212427210213Squared norm1
2Ef(xk)2
0.00 0.05 0.10 0.15 0.20
Fraction of first epoch22
212427210Squared norm1
2cExkxk12
0.00 0.05 0.10 0.15 0.20
Fraction of first epoch23
21
2123252729211Squared normRatio
Figure 3: Quantities shown are during CIFAR10 (top row) and ImageNet (bottom row) training with
momentum 0.9. Full details of the experimental setup are in the appendix. The extra negative xk−xk−1
term cancels out the large gradient norm squared term when the shown ratio (right) is above 1. Here this
occurs for the initial steps during the ﬁrst epoch of training.
almost matching SGD except for the termηL(1−c)
c(2−c),which is very small for the η∝1/(L/radicalbig
T)values that
the theory supports. Note however that by expanding /bardblxk−xk−1/bardbl2we must also consider the additional
inner-product terms introduced in Eq. 5, which we do in the next section.
When momentum helps By expanding the recursive deﬁnition of /bardblxk−xk−1/bardbl2, we have halved the
noise term, but at the expense of introducing an inner-product term proportional to:
−2ηc2/parenleftbigg1
c−1/parenrightbiggk/summationdisplay
i=0(1−c)2i/angbracketleft∇f(xi),xi−xi−1/angbracketright.
This term gives a precise characterization of when the convergence rate bound for SGD+M will be tighter
than SGD; when for a particular weighted average, each ∇f(xi−1)is on average positively aligned or at
worst orthogonal to the momentum buﬀer: mi−1∝−(xi−1−xi−2). If on average they are highly positively
correlated, then we can expect momentum methods to signiﬁcantly outperform non-momentum methods.
The correlation between the momentum buﬀer and the next gradient is not assured during optimization.
Intuitively, a high correlation can be expected when the optimization path is heading in a steady direction,
rather than oscillating around a minima or valley. This is particularly the case in the early stages of
optimization, where there is a clear descent direction, in contrast to the later stages of optimization, where
the optimization path will typically bounce around a minima or valley due to the noise introduced by
using stochastic gradients. When the optimization path bounces around signiﬁcantly, we would expect this
inner-product term to be close to zero in expectation. So although the worst case behavior of SGD+M the
convergence rate bound has double the noise of SGD, in practice we expect a behavior where at the early
stages of optimization it may be faster, and at the later stages of optimization it will converge at the same
rate as it enters a more noise dominated regime.
7Under review as submission to TMLR
0 50 100 150 200 250 300
Epoch30405060708090Test Accuracy (%)CIFAR-10 Momentum Change
Constant   (95.20% ± 0.106)
Drop @ 1   (95.11% ± 0.072)
0 20 40 60 80 100
Epoch203040506070Test Accuracy (%)ImageNet Momentum Change
Constant   (76.17% ± 0.091)
Drop @ 1   (76.23% ± 0.062)
Figure 4: Removing momentum by setting ck= 0after the ﬁrst epoch has no negative consequences on the
ﬁnal test accuracy for either problem. Experimental setup detailed in Section H.
An empirical study This result also suggests that momentum may ONLY be useful during the very
earliest iterations. In the case of the CIFAR10 (Krizhevsky, 2009) problem shown, it appears to only
provide a positive beneﬁt for less than half of the ﬁrst epoch, and the beneﬁt is even shorter for ImageNet
(Russakovsky et al., 2015). To test this hypothesis, we did a comparison where we turned oﬀ momentum
after the ﬁrst epoch. As shown in Figure 4, this gives the same test error curve and ﬁnal test error as for
when momentum is used for the whole run (two-sided t-test on the diﬀerence of means yields a p value of
0.61 for CIFAR10 and 0.705 for ImageNet).
Our theory suggests that we may directly measure when momentum is having a positive eﬀect on convergence
by comparing the expectations of the quantities1
η2c/bardblxk−xk−1/bardbl2to1
2/bardbl∇f(xk)/bardbl2. Figure 3 shows the
magnitudes of these two quantities (smoothed using an exponential moving average to approximate the
expectation), as well as the ratio on two test problems. When considering the ratio, the /bardblxk−xk−1/bardbl2term
is signiﬁcantly bigger at the earliest stages of optimization, and then quickly approaches the “noise” level of
1, corresponding to the inner-product discussed above being on average 0. Interestingly, the gradient norm
is also very large during these early iterations, which may explain why momentum helps so much: It negates
the contribution of the noise term to the convergence rate bound during the iterations when it is largest.
5 Insight #2: Reduce ckwhen you decrease ηk
Consider the remainder term Rk:
Rk=/bracketleftBig
L
ηk/parenleftBig
1
ck−1/parenrightBig
−L
ηk−1/parenleftBig
1
ck−1−1/parenrightBig/bracketrightBig
[f(xk−1)−f∗] +/bracketleftBig
1
η2
k−1
η2
k−1/bracketrightBig
[f(zk)−f∗].
This term contains the additional error accumulated when the step size is changed. Our hyper-parameter
choices should aim to keep this term small if possible. The second term involving [f(zk)−f∗]is exactly the
remainder term that appears in SGD theory, and so we would not expect to be able to control it further. The
ﬁrst line involves both candη, and so we have a degree of control over it. We are particularly interested in
stage-wise schemes, where at a certain time-step Tthe step-size ηis divided by a factor φ(typically 10), i.e.
ηT=ηT−1/φ. In that case, we may keep the ﬁrst term’s coeﬃcient at 0 if we choose parameters satisfying:
1
cT= 1 +1
φ/parenleftbigg1
cT−1−1/parenrightbigg
.
For smallc, this is approximately cT=φcT−1. I.e. when the step size is decreased by a factor φ, we should
increasecby that same φfactor. Using the equivalence in Theorem 1, we can see that when constant step
8Under review as submission to TMLR
2950 3000 3050 3100 3150 3200
Step0.000.020.040.060.080.100.120.14Train lossCIFAR-10 (PreResNet152)
Standard  
Gradual   
Abrupt    
k only
0 50 100 150 200 250 300
Epoch406080Test Accuracy (%)CIFAR-10 Learning Rate Schedule
Standard   (95.18% ± 0.056)
Gradual    (95.14% ± 0.091)
Abrupt     (95.20% ± 0.040)
k only (95.09% ± 0.077)
0 20 40 60 80 100
Epoch203040506070Test Accuracy (%)ImageNet Learning Rate Schedule
Standard   (76.17% ± 0.091)
Gradual    (76.17% ± 0.058)
k only (76.24% ± 0.060)
Abrupt     (76.12% ± 0.073)
Figure 5: Top: training loss before and after then annealing point where the learning rate is decreased by
a factor 10. Bottom: A comparison of the standard SGD+M schedule against the primal averaging abrupt
and gradual schedules (Figure 6) and against an ηkdecrease only schedule. In each case the schedule is
applied at the usual 10 fold LR decrease points, 150 and 225 epochs for CIFAR10, 30,60 & 90 epochs for
ImageNet.
9Under review as submission to TMLR
Mapping SPA parameters to SGD+M parameters under suggested scheme
0 50 100100101ValueSGD+M
k
 k
0 50 100101
100101ValueSPA
k
 ck
(a)αkdecreased and ckincreased
0 50 100101
100ValueSGD+M
k
 k
0 50 100101
100101ValueSPA
k
 ck (b)αkdecreased and ckincreased gradually
Figure 6: The behavior of the hyper-parameters of the SGD+M form when they are set so as to maintain
an identical iterate sequence as the SGD+M form, under a number of standard decrease schemes
sizes are used, the equivalence:
β= (1−c), α =ηc,
suggests that decreasing ηand increasing cproportionally actually leaves the step size αthe same, but
decreases the amount of momentum βin the SGD+M form. This suggests an alternative approach to the
learning rate schedule, when working in SGD+M form: Decrease βrather than decrease α, up to the point
whereβ= 0, corresponding to SGD without momentum.
Unfortunately, this scaling still presents problems, as we see in Figure 6 (left ﬁgure), there is an instantaneous
spike inαkwhen using this approach. Changing the learning rate by a large factor suddenly also eﬀects
the constants in front of the /bardblxk−xk−1/bardbl2term in the Lyapunov step, resulting in this term being positive,
rather than negative. We explore this diﬃculty and a potential solution in the next section (right ﬁgure).
6 Insight #3: Change hyper-parameters gradually
When constant momentum and step sizes are used, the weight of the term /bardblxk−xk−1/bardbl2in the Lyapunov step
is non-positive for values of ηlarger than the typical 2/Lmaximum required for non-momentum methods:
η≤2−c
Lc(1−c). (6)
However, when ηchanges abruptly by large amounts between steps, this expression can not be satisﬁed.
Instead, lets determine the largest multiplicative change in ηallowed between steps. Let ηk=ηk−1/r, where
we expectrto be larger than 1. We use ηto denoteηk−1to simplify the notation. We also apply ηL≤1
to simplify. This gives:
r2
η2/parenleftbigg1
c2−1
c/parenrightbigg
+r
ηL/parenleftbigg1
c−1/parenrightbigg2
≤1
η2c2,
Thereforer2−r2c+rηL(1−c)2−1≤0. Solving this quadratic equation gives two roots, one of which is
always negative, the other root is:
r=−ηL(1−c)2+/radicalBig
η2L2(1−c)4+ 4(1−c)c
2(1−c).
For instance with c= 0.1ηL= 0.1, a value of r= 1.01satisﬁes the inequality. Note that when the learning
rate is decreased further, the allowable values of rincrease. This suggests that at the point in which the
10Under review as submission to TMLR
learning rate would normally decrease by a large factor such as 10 in a stage-wise schedule, instead the
learning rate should be decreased geometrically, by a factor αeach step, until it reaches the 10x lower value.
Figure 6 shows that the αkandβkvalues stay within reasonable ranges under this gradual scheme compared
to the sudden spikes that are seen under other schemes. This provides further motivation for the use of a
gradual reduction procedure.
An empirical study Theviolationoftheinequalitythatoccurswhenthelearningrateischangedsuddenly
isnot just an artifact of the analysis used, a spike in the training loss is readily observed in practice, An
example that occurs during CIFAR-10 training is shown in Figure 5. Full details of the experimental setup
are available in the Appendix. The gradual approach avoids the spike seen when the learning rate is changed
suddenly. Although the training loss recovers rapidly after the spike, the gradual approach quickly obtains
a lower training loss. The gradual approach modiﬁes the standard scheme by increasing cby 10-fold (up to
a maximum of 1.0 for c) whenever ηis decreased 10-fold. Instead of an instantaneous change we changed
both with a 1.0005geometric factor each step until they reached their new value. As can be seen in Figure
5, there is also no loss of ﬁnal test accuracy at all from using the gradual schedule for both CIFAR-10 or
ImageNet. The result of applying two-sided t-tests for the diﬀerence of means pairwise between schedules
shows no statistically signiﬁcant diﬀerences at the 90% conﬁdence level.
Conclusion
Our analysis provides a better understanding of momentum methods for non-convex optimization through
the lens of the primal averaging form. We characterize the extra terms introduced introduced into the
Lyapunov analysis from the use of momentum, and show when these terms are beneﬁcial and when they
are harmful. We also analyze the behavior of the primal averaging form under changing step size schemes,
and show the surprising result that standard schemes do not make sense in the averaging form, and suggest
alternatives that are better behaved.
Summary of Contributions
•We show that common learning rate schedules deﬁned in the SGD+M form do not make sense when
mapped to the SPA form.
•We provide a Lyapunov analysis of momentum in the non-convex setting which is tight enough to
provide actionable insights into the behavior of momentum in practice.
•We provide insight into why SGD with momentum is typically no worse than SGD without momen-
tum: the bound we derive for momentum has twice the noise term of SGD’s bound, however when
consecutive gradients are roughly uncorrelated or positively correlated the noise is almost halved,
bringing it in line with SGD without momentum. We show this situation occurs in practice through
an empirical investigation.
•We suggest ﬁxes to the common stage-wise learning rate scheme that results in a schedule that makes
sense in both SPA form and SGD+M form. We validate this schedule on CIFAR10 and ImageNet
test problems. These ﬁxes are directly motivated by minimizing our upper bound on the Lyapunov
function.
11Under review as submission to TMLR
References
Bugra Can, Mert Gürbüzbalaban, and Lingjiong Zhu. Accelerated linear convergence of stochastic momen-
tum methods in wasserstein distances. In Proceedings of the 36th International Conference on Machine
Learning (ICML 2019) , 2019.
Ashok Cutkosky and Harsh Mehta. Momentum improves normalized sgd. Proceedings of the 37th Interna-
tional Conference on Machine Learning , 2020.
Aaron Defazio. On the curved geometry of accelerated optimization. Advances in Neural Information
Processing Systems 33 (NIPS 2019) , 2019.
Aaron Defazio and Robert M. Gower. Factorial powers for stochastic optimization. arXiv, 2020.
Nicolas Flammarion and Francis Bach. From averaging to acceleration, there is only a step-size. Proceedings
of Machine Learning Research. PMLR, 2015.
Sébastien Gadat, Fabien Panloup, and Soﬁane Saadane. Stochastic heavy ball. Electronic Journal of Statis-
tics, 2018.
Euhanna Ghadimi, Hamid Feyzmahdavian, and Mikael Johansson. Global convergence of the heavy-ball
method for convex optimization. In 2015 European Control Conference (ECC) , pages 310–315, 2015.
Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating
stochasticgradientdescentforleastsquaresregression. ProceedingsofMachineLearningResearch.PMLR,
2018.
Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Making the last iterate of sgd information the-
oretically optimal. In Conference on Learning Theory, COLT 2019 , Proceedings of Machine Learning
Research. PMLR, 2019.
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham M. Kakade. On the insuﬃciency of existing
momentum schemes for stochastic optimization. In International Conference on Learning Representations ,
2018.
WalidKrichene, AlexandreBayen, andPeterLBartlett. Adaptiveaveraginginaccelerateddescentdynamics.
InAdvances in Neural Information Processing Systems 29 . 2016.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent with momentum.
arXiv, 2020.
Vien V. Mai and Mikael Johansson. Convergence of a stochastic gradient method with momentum for non-
smooth non-convex optimization. Proceedings of the 37th International Conference on Machine Learning ,
2020.
Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent, weighted sampling, and the
randomized kaczmarz algorithm. In Advances in Neural Information Processing Systems 27 . 2014.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course , volume 87. Springer, 2013.
B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational
Mathematics and Mathematical Physics , 1964.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale
Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015.
doi: 10.1007/s11263-015-0816-y.
12Under review as submission to TMLR
Othmane Sebbouh, Robert M. Gower, and Aaron Defazio. On the convergence of the stochastic heavy ball
method. arXiv, 2020.
Ilya Sutskever, James Martens, George Dahl, and Geoﬀrey Hinton. On the importance of initialization
and momentum in deep learning. In Proceedings of the 30th International Conference on International
Conference on Machine Learning (ICML2013) , 2013.
W. Tao, Z. Pan, G. Wu, and Q. Tao. Primal averaging: A new gradient evaluation step to attain the optimal
individual convergence. IEEE Transactions on Cybernetics , 2020.
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi Yang. A uniﬁed analysis of stochastic momentum
methods for deep learning. Proceedings of the Twenty-Seventh International Joint Conference on Artiﬁcial
Intelligence (IJCAI-18) , 2018.
Tianbao Yang, Qihang Lin, and Zhe Li. Uniﬁed convergence analysis of stochastic momentum methods for
convex and non-convex optimization. arXiv, 2016.
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication eﬃcient momentum
sgd for distributed non-convex optimization. Proceedings of the 36th International Conference on Machine
Learning , 2020.
Kun Yuan, Bicheng Ying, and Ali H. Sayed. On the inﬂuence of momentum acceleration on online learning.
Journal of Machine Learning Research , 2016.
13Under review as submission to TMLR
A SGD+M and SPA equivalence
Theorem 3. Deﬁne the SGD+M method by the two sequences:
mk+1=βkmk+∇f(xk,ξk),
xk+1=xk−αkmk+1,
and the SPA sequences as:
zk+1=zk−ηk∇f(xk,ξk),
xk+1= (1−ck+1)xk+ck+1zk+1.
Consider the case where m0= 0for SGD+M and and z0= 0for SPA. Then if c1=α0/η0and fork≥0
ηk+1=ηk−αk
βk+1, ck+1=αk
ηk,
Thexsequence produced by the SPAmethod is identical to the xsequence produced by the SGD+M method.
Proof.Consider the base case where x0=z0. Then for SGD+M:
m1=∇f(x0,ξ0)
)x1=x0−α0∇f(x0,ξ0) (7)
and for the SPA form:
z1=x0−η0∇f(x0,ξ0)
x1= (1−c0)x0+c0(x0−η0∇f(x0,ξ0))
=x0−c0η0∇f(x0,ξ0) (8)
Clearly Equation 7 is equivalent to Equation 8 when α0=c0η0.
Now consider k >0. We will deﬁne zkin term of quantities in the SGD+M method, then show that with
this deﬁnition, the step-to-step changes in zcorrespond exactly to the SPA method. In particular, let:
zk=xk−/parenleftbigg1
ck−1/parenrightbigg
αk−1mk. (9)
Then
zk+1=xk+1−/parenleftbigg1
ck+1−1/parenrightbigg
αkmk+1
=xk−αkmk+1−/parenleftbigg1
ck+1−1/parenrightbigg
αkmk+1
=zk+/parenleftbigg1
ck−1/parenrightbigg
αk−1mk−αk
ck+1(βkmk+∇f(xk,ξk))
=zk+/bracketleftbigg/parenleftbigg1
ck−1/parenrightbigg
αk−1−αk
ck+1βk/bracketrightbigg
mk−αk
ck+1∇f(xk,ξk).
This is equivalent to the SPA step
zk+1=zk−ηk∇f(xk,ξk),
as long asαk
ck+1=ηkand
0 =/parenleftbigg1
ck−1/parenrightbigg
αk−1−αk
ck+1βk
= (ηk−1−αk−1)−ηkβk,
14Under review as submission to TMLR
i.e.ηk=ηk−1−αk−1
βk.
Using this deﬁnition of the zsequence, we can rewrite the SGD+M xsequence using a rearrangement of
Equation 9:
mk+1=/parenleftbigg1
ck+1−1/parenrightbigg−1
α−1
k(xk+1−zk+1),
=ck+1
1−ck+1α−1
k(xk+1−zk+1),
as
xk+1=xk−αkmk+1
=xk−ck+1
1−ck+1(xk+1−zk+1)
=xk−ck+1
1−ck+1xk+1+ck+1
1−ck+1zk+1
= (1−ck+1)xk+ck+1zk+1,
matching the SPA update.
B Lemmas
Lemma 1. (LEMMA 1.2.3, Nesterov (2013)) Suppose that fis diﬀerentiable and has L-Lipschitz gradient:
/bardbl∇f(x)−∇f(y)/bardbl≤L/bardblx−y/bardbl, (10)
then:
|f(y)−f(x)−/angbracketleft∇f(x),y−x/angbracketright|≤L
2/bardblx−y/bardbl2
2,∀x,y∈Rn. (11)
in particular,
f(y)≤f(x) +/angbracketleft∇f(x),y−x/angbracketright+L
2/bardblx−y/bardbl2
2, (12)
andf(y)≥f(x) +/angbracketleft∇f(x),y−x/angbracketright−L
2/bardblx−y/bardbl2
2. (13)
We will make heavy use of the fact that the xk+1update can be rearranged to give:
zk=xk−/parenleftbigg1
ck−1/parenrightbigg
(xk−1−xk).
Lemma 2. Suppose that fis diﬀerentiable and has L-Lipschitz gradient, then the updates of the SPA form
obey fork≥1:
L
c2
k+1E/bardblxk+1−xk/bardbl2≤L/parenleftbigg1
ck−1 +ηkL/parenrightbigg/parenleftbigg1
ck−1/parenrightbigg
/bardblxk−xk−1/bardbl2+η2
kLE/bardbl∇f(xk,ξk)/bardbl2
+ 2ηkL/parenleftbigg1
ck−1/parenrightbigg
[f(xk−1)−f(xk)].
Proof.We may write the diﬀerence of the xkupdates between steps as:
xk+1−xk=ck+1(zk−xk)−ηkck+1∇f(xk,ξk)
Recall that:
zk−xk=/parenleftbigg1
ck−1/parenrightbigg
(xk−xk−1).
15Under review as submission to TMLR
So:
xk+1−xk=ck+1/parenleftbigg1
ck−1/parenrightbigg
(xk−xk−1)−ηkck+1∇f(xk,ξk)
Taking the squared norm and expanding, then taking expectations with respect to ξkgives:
E/bardblxk+1−xk/bardbl2=c2
k+1/parenleftbigg1
ck−1/parenrightbigg2
/bardblxk−xk−1/bardbl2+c2
k+1η2
kE/bardbl∇f(xk,ξk)/bardbl2
−2ηkc2
k+1/parenleftbigg1
ck−1/parenrightbigg
/angbracketleft∇f(xk),xk−xk−1/angbracketright
Now we apply the smoothness lower bound (Eq. 13):
f(xk−1)≥f(xk) +/angbracketleft∇f(xk),xk−1−xk/angbracketright−L
2/bardblxk−xk−1/bardbl2
Rearranged into the form:
−/angbracketleft∇f(xk),xk−xk−1/angbracketright≤f(xk−1)−f(xk) +L
2/bardblxk−xk−1/bardbl2
to give:
E/bardblxk+1−xk/bardbl2≤c2
k+1/parenleftbigg1
ck−1/parenrightbigg2
/bardblxk−xk−1/bardbl2+c2
k+1η2
kE/bardbl∇f(xk,ξk)/bardbl2
+ 2ηkc2
k+1/parenleftbigg1
ck−1/parenrightbigg
[f(xk−1)−f(xk)] +ηkLc2
k+1/parenleftbigg1
ck−1/parenrightbigg
/bardblxk−xk−1/bardbl2
Now group terms and multiply by L/c2
k+1:
L
c2
k+1E/bardblxk+1−xk/bardbl2≤L/parenleftbigg1
ck−1 +ηkL/parenrightbigg/parenleftbigg1
ck−1/parenrightbigg
/bardblxk−xk−1/bardbl2+η2
kLE/bardbl∇f(xk,ξk)/bardbl2
+ 2ηkL/parenleftbigg1
ck−1/parenrightbigg
[f(xk−1)−f(xk)]
Lemma 3. Suppose that fis diﬀerentiable and has L-Lipschitz gradients, then the updates of the SPA form
obey fork≥1:
E[f(zk+1)] +ηk
2/bardbl∇f(zk)/bardbl2≤f(zk)−ηk
2/bardbl∇f(xk)/bardbl2+1
2ηkL2/parenleftbigg1
ck−1/parenrightbigg2
/bardblxk−xk−1/bardbl2
+1
2η2
kLE/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
,
where the expectation is with respect to ξk, and is conditional on the iterates and gradients from prior steps.
Proof.Usingzk+1=zk−ηk∇f(xk,ξk)and the smoothness upper bound (Equation 12):
E[f(zk+1)]≤f(zk)−ηkE/angbracketleft∇f(zk),∇f(xk,ξk)/angbracketright+1
2η2
kLE/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
=f(zk)−ηk/angbracketleft∇f(zk),∇f(xk)/angbracketright+1
2η2
kLE/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
=f(zk) +ηk
2/bardbl∇f(zk)−∇f(xk)/bardbl2−ηk
2/bardbl∇f(zk)/bardbl2−ηk
2/bardbl∇f(xk)/bardbl2
+1
2η2
kLE/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
16Under review as submission to TMLR
Now we use our assumption that the gradients are Lipschitz (Eq. 10):
/bardbl∇f(zk)−∇f(xk)/bardbl2≤L2/bardblzk−xk/bardbl2=L2/parenleftbigg1
ck−1/parenrightbigg2
/bardblxk−xk−1/bardbl2
to give:
E[f(zk+1)] +ηk
2/bardbl∇f(zk)/bardbl2≤f(zk)−ηk
2/bardbl∇f(xk)/bardbl2+1
2ηkL2/parenleftbigg1
ck−1/parenrightbigg2
/bardblxk−xk−1/bardbl2
+1
2η2
kLE/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
C Proof of theorem 2
Proof.Consider Lemma 2 after taking expectations and dividing by 2η2
k:
L
2η2
kc2
k+1E/bracketleftBig
/bardblxk+1−xk/bardbl2/bracketrightBig
≤1
2η2
kL/parenleftbigg1
ck−1 +ηkL/parenrightbigg/parenleftbigg1
ck−1/parenrightbigg
/bardblxk−xk−1/bardbl2
+1
ηkL/parenleftbigg1
ck−1/parenrightbigg
[f(xk−1)−f(xk)]
+1
2LE/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
and Lemma 3 divided by η2
k:
1
η2
kE[f(zk+1)] +1
2ηk/bardbl∇f(zk)/bardbl2
≤1
η2
kf(zk)−1
2ηk/bardbl∇f(xk)/bardbl2
+1
21
ηkL2/parenleftbigg1
ck−1/parenrightbigg2
/bardblxk−xk−1/bardbl2
+1
2LE/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
Combining those bounds results in the following natural choice of Lyapunov function Λ:
Λk+1=1
η2
k[f(zk+1)−f∗]
+L
ηk/parenleftbigg1
ck−1/parenrightbigg
[f(xk)−f∗]
+1
2L1
η2
kc2
k+1/bardblxk+1−xk/bardbl2(14)
17Under review as submission to TMLR
and yields the bound for k≥1:
1
2ηk/bardbl∇f(xk)/bardbl2+1
2ηk/bardbl∇f(zk)/bardbl2
≤Λk−E[Λk+1] +LE/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
+L
2/bracketleftBigg
1
η2
k/parenleftbigg1
ck−1 +ηkL/parenrightbigg/parenleftbigg1
ck−1/parenrightbigg
+1
ηkL/parenleftbigg1
ck−1/parenrightbigg2
−1
η2
k−1c2
k/bracketrightBigg
/bardblxk−xk−1/bardbl2
+/bracketleftbigg1
ηkL/parenleftbigg1
ck−1/parenrightbigg
−1
ηk−1L/parenleftbigg1
ck−1−1/parenrightbigg/bracketrightbigg
[f(xk−1)−f∗]
+/bracketleftbigg1
η2
k−1
η2
k−1/bracketrightbigg
[f(zk)−f∗] (15)
D Telescoping
In order to complete a convergence rate proof, we must consider the behavior of the method at step 0. The
above two lemmas are simpliﬁed in this case, yielding the following bound replacing Lemma 2:
E/bracketleftBig
/bardblx1−x0/bardbl2/bracketrightBig
=c2
1η2
0E/bracketleftBig
/bardbl∇f(x0,ξ0)/bardbl2/bracketrightBig
,
and replacing Lemma 3
E[f(z1)] +1
2η0/bardbl∇f(z0)/bardbl2≤f(z0)−1
2η0/bardbl∇f(x0)/bardbl2+1
2η2
0LE/bracketleftBig
/bardbl∇f(x0,ξ0)/bardbl2/bracketrightBig
.
Multiplying the ﬁrst result by L/(2c2
1η2
0)and dividing the second result by η0, we may sum these equations
to give:
1
2η0/bardbl∇f(x0)/bardbl2+1
2η0/bardbl∇f(z0)/bardbl2≤1
η2
0[f(z0)−f∗]−1
η2
0E[f(z1)−f∗]
−L
2η2
0c2
1E/bracketleftBig
/bardblx1−x0/bardbl2/bracketrightBig
+LE/bardbl∇f(x0,ξ0)/bardbl2.
Now consider the behavior of the SGD+M method when we use a ﬁxed step size η. As long as
η≤2−c
Lc(1−c),
andE/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
≤G2, we may telescope from this base case to step T, yielding:
1
ηT/summationdisplay
kE/bracketleftbigg1
2/bardbl∇f(xk)/bardbl2+1
2/bardbl∇f(zk)/bardbl2/bracketrightbigg
≤1
η2[f(z0)−f∗] +L
η/parenleftbigg1
c1−1/parenrightbigg
[f(x0)−f∗] +TLG2. (16)
Multiplying by η/Tgives a bound on the average iterate:
1
TT/summationdisplay
kE/bracketleftbigg1
2/bardbl∇f(xk)/bardbl2+1
2/bardbl∇f(zk)/bardbl2/bracketrightbigg
≤1
ηT[f(z0)−f∗] +L
T/parenleftbigg1
c1−1/parenrightbigg
[f(x0)−f∗] +ηLG2.
18Under review as submission to TMLR
Using the optimal step size η2=T−1L−1G−2[f(z0)−f∗]gives:
1
TT/summationdisplay
kE/bracketleftbigg1
2/bardbl∇f(xk)/bardbl2+1
2/bardbl∇f(zk)/bardbl2/bracketrightbigg
≤2G/radicalbig
L[f(z0)−f∗]√
T+L
T/parenleftbigg1
c1−1/parenrightbigg
[f(x0)−f∗],
whereas the more realistic step size η2=T−1L−2gives
1
TT/summationdisplay
kE/bracketleftbigg1
2/bardbl∇f(xk)/bardbl2+1
2/bardbl∇f(zk)/bardbl2/bracketrightbigg
≤L√
T[f(z0)−f∗] +L
T/parenleftbigg1
c1−1/parenrightbigg
[f(x0)−f∗] +G2
√
T.
In each case, the extra term [f(x0)−f∗]that diﬀers from the standard SGD Lyapunov function decays at a
1/T rate, and so becomes negligible for large T.
E Removing the bounded gradients assumption
The above argument uses a bounded gradients assumption, however this assumption is easily replaced by a
bound on the gradient variance:
σ2=E/bracketleftBig
/bardbl∇f(xk,ξk)−∇f(xk)/bardbl2/bracketrightBig
.
Using E/bracketleftBig
/bardbl∇f(xk,ξk)/bardbl2/bracketrightBig
−/bardbl∇f(xk)/bardbl2=E/bracketleftBig
/bardbl∇f(xk,ξk)−∇f(xk)/bardbl2/bracketrightBig
Equation 16 is replaced by
1
ηT/summationdisplay
kE/bracketleftbigg1−2Lη
2/bardbl∇f(xk)/bardbl2+1
2/bardbl∇f(zk)/bardbl2/bracketrightbigg
≤1
η2[f(z0)−f∗] +L
η/parenleftbigg1
c1−1/parenrightbigg
[f(x0)−f∗]
+TLσ2.
The key diﬀerence is that the factor of1
2multiplying the squared gradient term is replaced by1−2Lη
2. As
long asη≤1/(2L)this yields a comparable convergence bound as under the bounded gradient assumption,
up to a factor of 2.
F SGD reference proof
We reproduce the standard argument for non-convex SGD convergence here using our notation for easy
comparison to our SGD+M proof above. Consider the step xk+1=xk−ηk∇f(xk,ξk).Then:
f(xk+1)≤f(xk) +/angbracketleft∇f(xk),xk+1−xk/angbracketright+1
2L/bardblxk+1−xk/bardbl2
=f(xk)−ηk/angbracketleft∇f(xk),∇f(xk,ξk)/angbracketright+1
2Lη2
k/bardbl∇f(xk,ξk)/bardbl2.
Taking expectations and using the bounded gradients assumption gives:
E[f(xk+1)]≤f(xk)−ηk/bardbl∇f(xk)/bardbl2+1
2Lη2
kG2.
Deﬁne Λk=η−2
kE[f(xk)−f∗]: Then rearranging gives:
1
ηkE/bracketleftBig
/bardbl∇f(xk)/bardbl2/bracketrightBig
≤Λk−E[Λk+1] +1
2LG2+ (η−2
k−η−2
k−1) [f(zk)−f∗].
19Under review as submission to TMLR
Assuming a ﬁxed step size, we telescope from 0toTafter taking total expectations:
1
ηT/summationdisplay
k=0E/bracketleftBig
/bardbl∇f(xk)/bardbl2/bracketrightBig
≤Λ0−E[ΛT+1] +1
2LG2T.
So:
1
TT/summationdisplay
k=0E/bracketleftBig
/bardbl∇f(xk)/bardbl2/bracketrightBig
≤1
ηT[f(x0)−f∗] +1
2LηG2,
using the optimal step size
η=/radicalbigg
2 [f(x0)−f∗]
TLG2
gives:
1
TT/summationdisplay
k=0E/bracketleftBig
/bardbl∇f(xk)/bardbl2/bracketrightBig
≤G/radicalbig
2L[f(x0)−f∗]√
T,
which for large T, only diﬀers from the SGD+M rate by a factor√
2.
G Augmented Lyapunov
In Section 4, we consider the case of constant ηandc, and we introduce the additional assumption that
/angbracketleft∇f(xk−1),xk−1−xk−2/angbracketright= 0, so that:
/bardblxk−xk−1/bardbl2= (1−c)2/bardblxk−1−xk−2/bardbl2+c2η2/bardbl∇f(xk−1,ξk−1)/bardbl2, (17)
We want to modify the Lyapunov function so that we have:
ρΓk+1≤ρΓk+ρkc2η2/bardbl∇f(xk,ξk)/bardbl2,
whereρis a negative, and Γk+1=/bardblxk+1−xk/bardbl2. Consider the constants in front of the /bardblxk−xk−1/bardbl2term
in the Lyapunov step:
L
2cη/bracketleftbigg
−2−c
η+L−Lc
c/bracketrightbigg
/bardblxk+1−xk/bardbl2.
Using this expression, clearly our requirement on ρwill be satisﬁed if:
ρ(1−c)2+L
2cη/bracketleftbigg
−2−c
η+L−Lc
c/bracketrightbigg
=ρ,
solving for ρgives:
ρ=L
2ηc2/bracketleftbiggL(1−c)
c(2−c)−1
η/bracketrightbigg
.
ρwill be negative when:
η≤c(2−c)
L(1−c),
which covers all reasonable choices of hyper-parameters as considered in the convergence rate theory above.
Using this ρ, we have an additional term in the Lyapunov step equation given by weighting the gradient
noise term in Eq. 17 by ρ:
ρc2η2/bardbl∇f(xk,ξk)/bardbl2=/bracketleftbiggηL(1−c)
c(2−c)−1/bracketrightbiggL
2/bardbl∇f(xk,ξk)/bardbl2.
This value is very close to −L
2/bardbl∇f(xk,ξk)/bardbl2for sensible hyper-parameter values. For instance, for a typical
η=T−1/2L−1choice you get for the inner term:
ηL(1−c)
c(2−c)−1 =1−c√
Tc(2−c)−1,
which forc= 0.1andT= 10,000, yields1−c√
Tc(2−c)−1 = 0.047−1.
20Under review as submission to TMLR
H Details of experiments
In both cases below, when expressed in SPA form, the initial LR 0.1 corresponds to an initial learning rate
of 1.0 andc= 0.1.
CIFAR10
Our data augmentation pipeline consisted of random horizontal ﬂipping, then random crop to 32x32, then
normalization by centering around (0.5, 0.5, 0.5). We used the standard learning rate schedule for this
problem, consisting of a 10-fold decrease at epochs 150 and 225. Test/train/validate splits are standard.
Total running time is < 24 hours per run. Our results are averaged over 20 seeds for each variant.
Hyper-parameter Value
Architecture PreAct ResNet152
Epochs 300
GPUs 1xV100
Batch Size per GPU 128
Decay 0.0001
ImageNet
Data augmentation consisted of the RandomResizedCrop(224) operation in PyTorch, followed by Ran-
domHorizontalFlip then normalization to mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]. We
used the standard learning rate schedule for this problem, where the learning rate is decreased 10 fold every
30 epochs. Test/train/validate splits are standard. Total running time is < 24 hours per run. Our results
are averaged over 5 seeds for each variant.
Hyper-parameter Value
Architecture ResNet50
Epochs 100
GPUs 8xV100
Batch size per GPU 32
Decay 0.0001
21