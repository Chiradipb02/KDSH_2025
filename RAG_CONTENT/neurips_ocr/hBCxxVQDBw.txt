Towards Scalable and Stable Parallelization of
Nonlinear RNNs
Xavier Gonzalez1,2, Andrew Warrington1,2,3, Jimmy T.H. Smith2,4,5, Scott W. Linderman1, 2
1Department of Statistics, Stanford University.
2Wu Tsai Neurosciences Institute, Stanford University.
3GE Healthcare.4ICME, Stanford University.5Liquid AI.
{xavier18,scott.linderman}@stanford.edu
Abstract
Conventional nonlinear RNNs are not naturally parallelizable across the sequence
length, unlike transformers and linear RNNs. Lim et al. [36] therefore tackle par-
allelized evaluation of nonlinear RNNs, posing it as a fixed point problem solved
with Newton’s method. By deriving and applying a parallelized form of New-
ton’s method, they achieve large speedups over sequential evaluation. However,
their approach inherits cubic computational complexity and numerical instability.
We tackle these weaknesses. To reduce the computational complexity, we ap-
ply quasi-Newton approximations and show they converge comparably, use less
memory, and are faster, compared to full-Newton. To stabilize Newton’s method,
we leverage a connection between Newton’s method damped with trust regions
and Kalman smoothing. This connection allows us to stabilize the iteration, per
the trust region, and use efficient parallelized Kalman algorithms to retain per-
formance. We compare these methods empirically and highlight use cases where
each algorithm excels.
1 Introduction
Parallel computation has helped fuel the rise of deep learning [30]. Architectures such as transform-
ers [73] and linear RNNs [31, 65, 53, 27, 21] are specifically designed to allow parallelization over
the length of the input sequence. However, most conventional nonlinear RNNs (e.g. Elman RNNs,
GRUs [15], LSTMS [29] etc.) are not readily parallelizable over the sequence length due to their
sequential architecture. Thus, they do not benefit as much from parallel hardware. Nonetheless,
these nonlinear RNN architectures are still used widely across the scientific community [44, 62, 34].
Furthermore, recent work has suggested that linear RNNs (and transformers) are fundamentally lim-
ited in their expressivity compared to nonlinear RNNs [43]. Finally, nonlinear RNNs continue to
be of significant interest in computational and theoretical neuroscience as models of neural sys-
tems [74, 69, 54, 64, 60, 19, 18, 17]. Therefore, scalable and stable parallelization methods for
nonlinear RNNs offer significant benefits across many fields.
Towards this goal, Lim et al. [36] proposed DEER, a method for evaluating a nonlinear RNN in
parallel. DEER casts inference as finding the solution of a fixed point equation designed specifically
to capture the nonlinear dynamics of the RNN. Newton’s method is used to solve the resulting fixed
point equation. With good initialization, Newton’s method enjoys quadratic convergence rates [49,
Chapter 11]. Lim et al. [36] also show that the inversion of the structured Jacobian matrix required
by Newton’s method can be cast as an associative parallel scan [7]. DEER therefore reduces the
evaluation runtime over sequential evaluation by as much as factor of twenty.
However, DEER also inherits the weaknesses of Newton’s method and parallel scans. The first
weakness is scalability . Let Ddenote the state dimension and Tdenote sequence length. From
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Scalability: 
+ Diagonal Jacobian
DEER [36] Quasi -DEER
ELK Quasi -ELKStability:
+ Trust region
+ Kalman filterFigure 1: Overview of the paralleliz-
able methods we consider in this paper.
We introduce diagonal approximations
to improve complexity (quasi-DEER,
Section 4.1) and link to Kalman filter-
ing and trust regions to improve stability
(ELK, Section 4.2). We combine these
ideas in quasi-ELK (Section 4.2).Table 1: Description of the relative strengths and
weaknesses of the five evaluation methods we consider.
We include a discussion of this in Section 7.
MethodDesiderata
Parallel Work Memory Stability
Sequential No O(TD2)O(D) Very high
DEER [36] Yes O(TD3)O(TD2)Low
Quasi-DEER Yes O(TD)O(TD) Low
ELK Yes O(TD3)O(TD2)High
Quasi-ELK Yes O(TD)O(TD) Moderate
using a parallel scan to evaluate updates from Newton’s method, DEER inherits O(TD2)memory
complexity and O(TD3)computational work [7]. These costs can be prohibitive in practical deep
learning settings. The second limitation of DEER is numerical stability , inherited from Newton’s
method. In general, undamped Newton’s method does not provide global convergence guarantees,
and in practice often diverges [49]. We seek to ameliorate both these weaknesses.
To do this, we leverage two techniques: quasi approximations and trust regions. Quasi approxima-
tions are a common adaptation of Newtons method, where approximate, but faster and less mem-
ory intensive updates, are used in-place of exact “full” Newton steps. Empirically, these are often
observed to expedite convergence in terms of wallclock time, even though more Newton iterates
are used. We apply quasi-approximations to remove the memory and compute scaling inherited
by DEER, also finding accelerated convergence and reduced memory consumption. Secondly, we
leverage a connection between Newton’s method with a trust region and Kalman smoothing in se-
quential models [71]. This allows us to stabilize the Newton iteration by limiting the step size (to
the radius of the trust region), preventing large and numerically unstable steps, while still being able
to use parallelized Kalman smoothers [59, 12], achieving a parallel runtime that is logarithmic in
the sequence length. We refer to DEER accelerated with a quasi approximation as quasi-DEER, and
DEER stabilized with trust regions as “ Evaluating Levenberg-Marquardt via Kalman” (ELK). We
then combine these yielding a fast and stable algorithm, which we term quasi-ELK.
Crucially, DEER, ELK, and their quasi-variants are algorithms for parallelizing anydiscrete-time
nonlinear dynamical system, including stateful architectures such as RNNs, that may or may not in-
clude stochasticity. We use “parallel” to refer to the fact that each iteration of our iterative algorithm
operates on the entire T-length sequence (and not on each sequence element one at a time).
We outline the key contributions and organization of the paper here: We first introduce background
material, particularly focusing on DEER [36], in Sections 2 and 3. We then present three short
novel proofs: that DEER is globally convergent; that this convergence is robust to modifications of
the linearized dynamics (Proposition 1); and that there is a unique solution with no local minima
(Appendices A.1 and A.2). We then introduce quasi-approximations to DEER to improve efficiency
(quasi-DEER, Section 4.1), and trust regions to stabilize DEER (ELK, Section 4.2) We also provide
an interpretation of how trust regions stabilize the dynamics by damping the eigenvalues of the
Jacobians (Section 4.2 and Appendix A.3). We show empirically that quasi-DEER remains accurate,
with reduced runtime and memory consumption (Section 6). In regimes where DEER is numerically
unstable or convergences slowly, we show ELK and quasi-ELK can enjoy fast, numerically stable
convergence. We conclude by discussing the relative strengths and weaknesses of each method,
providing guidance on how to select and tune them, and highlighting avenues for future research
(Section 7). We provide our code at https://github.com/lindermanlab/elk .
2 Problem Statement
We consider nonlinear Markovian state space models, with the state at time tdenoted st∈RD
and nonlinear transition dynamics f:RD→RD. We denote the full sequence of Tstates as
s1:T∈RT×D. Note that we will be mainly considering the transition dynamics in this paper, and
2so we suppress any (possibly random) input dependence of the model in the notation. Note however
the algorithms in this paper extend trivially to these situations.
For any collection of candidate states {st}T
t=1and an initial state s0we can define the residual
r(s1:T) := [s1−f(s0),s2−f(s1),s3−f(s2), . . . ,sT−f(sT−1)]∈RT×D. (1)
This residual can be interpreted as the one-step error incurred by assuming the tthstate is stin-
stead of f(st−1). The solution of the state space model, s∗
1:T, is the only trace with zero residual.
Equivalently, it is the unique solution to the fixed point equation
r(s∗
1:T) =0. (2)
The conventional way of obtaining s∗
1:Tis to apply fsequentially Ttimes. Sequential evaluation
always yields a valid trace, but it requires O(T)sequential operations (i.e. computational depth or
span), and hence does not fully leverage the capabilities of parallel hardware. We aim to compute
s∗
1:Tin sublinear time using parallel computation.
Jacobian of the Residual For notational brevity, we overload sandrto also denote vectors in
RTD, representing flattened versions of s1:Tandr1:T. We can therefore write the Jacobian of the
residual for the whole sequence, J(s), as a TD×TDmatrix with block bidiagonal structure of the
form
J(s) :=∂r
∂s(s) =
ID 0. . . 0 0
−∂f
∂s(s1)ID. . . 0 0
...............
0 0 . . . I D 0
0 0 . . .−∂f
∂s(sT−1)ID
. (3)
3 DEER: Newton’s Method for Parallel Evaluation of Sequential Models
Lim et al. [36] propose DEER, an algorithm using Newton’s method for parallel evaluation of non-
linear sequential models, including both discrete-time nonlinear RNNs (GRUs, LSTMs, etc.) and
neural ODEs [13, 32]. In this paper, we focus on the discrete-time setting, and address questions that
arise from Lim et al. [36]: how to scale Newton’s method, and how to make it numerically stable .
In this section we introduce DEER. We begin with a simplified derivation that emphasizes the link
between Newton’s method on vector spaces and parallelizable linear recurrences. We then present
a new proof that DEER theoretically always converges globally. This proof also highlights why
global convergence can be numerically unstable and/or slow in practice. We conclude by using these
insights to discuss the weaknesses of DEER, and to motivate the methods we develop in Section 4.
3.1 Derivation of DEER from Newton’s Method
The original derivation of DEER used Newton’s method on Banach spaces and the Frécht derivative
for continuous-time systems to derive the update and show convergence [36]. We specialize to the
setting of discrete-time RNNs, and present a streamlined derivation that more directly connects the
structure of the Jacobian in (3) to the linear recurrence relation in (6). This connection highlights
why DEER incurs cubic work in Dand may encounter numerical instabilities. We will also use this
form to prove DEER’s global convergence in Section 3.2.
TheithNewton iterate for (2), starting at s(i), is given by
s(i+1)←s(i)−J(s(i))−1r(s(i)), (4)
or equivalently,
∆s(i+1):=s(i+1)−s(i)=−J(s(i))−1r(s(i)). (5)
Note this uses the root-finding view of Newton’s method, see Appendix C.2.
Provided all of {∂f/∂s}T
t=2are finite, then the Jacobian defined in (3) is invertible and all of the
eigenvalues are equal to one.1Storing and naively inverting the Jacobian is infeasible for large D
1To see this fact, note that the characteristic polynomial of J(s)in (3) is (λ−1)TD.
3orT. However, since J(s)is block bidiagonal, we can solve for ∆sin (5) by forward substitution.
This reduces to a simple recursion with the initial condition ∆s(i+1)
1 =−r1(s(i)), and for t >1,
∆s(i+1)
t =∂f
∂s(s(i)
t−1)
∆s(i+1)
t−1−rt(s(i)). (6)
DEER uses the linearity of this recursion, soling it in parallel with a parallel associative scan [36, 7,
65]. Therefore, with O(T)processors, each Newton iteration can be performed in O(logT)time.
We emphasize that the computation of the Newton step ∆sin (5) is being parallelized. Jwould, in
general, be a TD×TDmatrix that is prohibitive to store or invert. But by formulating this solve as
an LDS in (6), we are able to parallelize the computation of ∆s(which consists of Tstate updates,
each of dimension D) over the sequence length. With sufficient processors, each update in (5) can
be computed in O(logT)time. We use the parallel scan from JAX [9] (see Appendix B.6).
3.2 Global Convergence of DEER
We present a proof that DEER converges globally for discrete-time RNNs to the solution s∗
1:Tof (2)
in at most Tsteps.
Proposition 1. Undamped Newton’s method will converge to the true solution, s∗
1:T, of the fixed
point (2)in at most TNewton iterations, for any initial s(0)
1:T.
Proof sketch. For the full proof by induction, see Appendix A.1. The structure of J(s)determines
the recurrence in (6). The update applied at time t,∆s(i+1)
t , from (6) is the summation of a linearized
fapplied to the update at time t−1, and the residual one-step error at time t. Therefore, if the
previous timestep is correct (i.e. ∆s(i+1)
t−1=0), then the update at time tis just the one-step residual,
which is defined exactly as the error. Therefore, if the previous value is correct, the updated current
value will be correct. Given that fands0are fixed and known, the result follows that all Ttimesteps
will have zero residual after Titerations.
It is not immediately obvious from (4) or the proof given by Lim et al. [36] that DEER converges
globally, but Proposition 1 shows that it in fact does, at least theoretically. This result has two crucial
corollaries. First, after iNewton iterations, s(i)
1:Twill have zero error for all t≤i. Therefore, if the
iteration encounters numerical instabilities, as Newton is prone to, we can simply use a heuristic of
resetting s(i)
tto a finite value for all t > i . This preserves the solution for time indices t≤iand
allows the optimization to continue, but it is equivalent to running Newton’s method from scratch
onsi:T. This process is repeated until the entire trace has zero residual. A second corollary is
thatanyset of finite matrices can replace {∂f/∂s}T
t=2in (3) or (6), and the resulting quasi-Newton
method will still converge globally in at most Titerations. This preservation of global convergence
provides further motivation for exploring quasi-Newton methods, as we discuss in the next section.
3.3 Weaknesses of DEER
Despite the theoretical convergence of DEER, its formulation as a linear recurrence relation in (6)
highlights its limited scalability and stability. Scalability is limited because, in general, ∂f/∂sis a
dense D×Dmatrix. Therefore the parallel associative scan, which uses matrix-matrix multipli-
cations, has O(TD3)computational work and O(TD2)memory complexity. Stability is limited
because we often have no control over the eigenvalues of ∂f/∂s. If sufficiently many of these eigen-
values over the sequence length are larger in magnitude than one, then the linear recurrence relation
will be numerically unstable. The heuristic approach of resetting unstable values is sufficient to
ensure global convergence, but as we show in Section 6.3, it comes at the cost of runtime, as conver-
gence is dramatically slower. These weaknesses motivate the development of two new techniques
for parallelized evaluation of RNNs, quasi-DEER and ELK, which we discuss in the next section.
4 Scaling and Stabilizing Newton’s Method for Parallel Evaluation
In Section 4.1 we introduce quasi-DEER, a quasi-Newton method that addresses the intractability
of DEER for large state sizes. In Section 4.2 we introduce Evaluating Levenberg-Marquardt with
4Kalman (ELK), a damped Newton method for numerically stable, parallel evaluation of nonlinear
RNNs. We also introduce quasi-ELK, which combines quasi-DEER and ELK to create a damped
Newton’s method for parallel sequential evaluation that is scalable and numerically stable.
4.1 Quasi-DEER: Scaling DEER with Diagonal Jacobian Approximations
As a consequence of our Proposition 1, replacing the Jacobians {∂f/∂s}T
t=2with an arbitrary ma-
trix will still result in global convergence of the resulting DEER-like algorithm in at most Tit-
erations. A straightforward way to reduce the computational cost is to replace {∂f/∂s}T
t=2with
{diag ( ∂f/∂s)}T
t=2, i.e. take the diagonal entries of the Jacobians of the dynamics functions. The
resulting linear recursion requires only O(DT)memory because it only needs to store diagonal ma-
trices, and O(DT)work, because the parallelized associative scan only uses element-wise vector
multiplies. Position-wise matrix-vector multiplies are still required to obtain the residuals, but this
computation can be embarrassingly parallelized across the sequence.
Quasi-Newton methods approximate the Jacobian for computational reasons, so we refer to this
algorithm as quasi-DEER. In Section 6, we show that quasi-DEER outperforms DEER on wall-clock
time and memory usage on the tests from Lim et al. [36]. Quasi-DEER improves the scalability of
DEER, but it does not address stability concerns. We propose a more stable solution below.
4.2 ELK: Stabilizing DEER with Trust Regions
Rather than treating RNN evaluation as a fixed point finding problem, let us instead consider it as an
optimization problem. First, we define the merit function
L(s):=1
2∥r(s)∥2
2. (7)
As in the fixed point formulation, the unique minimizer of this objective is s∗. In fact, the only
local minimum of the merit function (7) is s∗, as proved in Proposition 3 in Appendix A.2. One
way of minimizing this nonlinear sum of squares objective is via the Gauss-Newton algorithm [49],
which alternates between linearizing the terms in the merit function and solving the resulting linear
sum-of-squares problem. The linearized objective at iteration iis
eLs(i)(∆s) =1
2r(s(i)) +J(s(i))∆s2
2. (8)
The solution is ∆s(i+1)=−J(s(i))−1r(s(i)), which is exactly the DEER update from (5).
Formulating evaluation as nonlinear least squares also suggests more stable algorithms. The
Levenberg-Marquardt algorithm [49] uses updates that solve a constrained optimization problem
min
∆seLs(i)(∆s)subject to ∥∆s∥2≤Di+1, (9)
where Di+1is an upper bound on the step size. We recognize this constraint as a trust region, which
is often used in conjunction with Newton’s method to improve numerical stability and convergence.
Finally, minimizing this constrained optimization is equivalent to minimizing the Lagrangian
bL(∆s, λi+1) =eLs(i)(∆s) +λi+1
2∥∆s∥2
2 (10)
for some λi+1≥0. As noted by Särkkä and Svensson [71], the minimizer of this Lagrangian can
be obtained by a Kalman smoother. We emphasize this connection in the following proposition.
Proposition 2. Solving for the Levenberg-Marquardt update that minimizes (10) with fixed λi+1is
equivalent to finding the maximum a posteriori (MAP) estimate of s1:Tin a linear Gaussian state
space model, which can be done in O(logT)time on a sufficiently large parallel machine.
Proof. Expanding the residual and Jacobian functions in (8), we see that up to an additive constant,
the negative Lagrangian can be rewritten as,
5−bL(∆s, λi+1)·= logN(s1|f(s0), ID) +TX
t=1logN
s(i)
tst,1
λi+1ID
+TX
t=2logN
stf(s(i)
t−1) +∂f
∂s(s(i)
t−1)
(st−1−s(i)
t−1), ID
,(11)
where N(x|µ,Σ)denotes the probability density function of the multivariate normal distribution.
We recognize (11) as the log joint probability of a linear Gaussian state space model (LGSSM) [70]
on(s1, . . . ,sT). The means of the dynamics distributions are given by the linearization of f, and
the emissions are the previous iteration’s states, s(i). The parameter λi+1sets the precision of the
emissions, governing how far the posterior mode deviates from the previous states.
The minimizer of (10) is the posterior mode of the LGSSM (11), and can be obtained by Kalman
smoothing [70]. As with the linear recursions in DEER, the Kalman smoother can be implemented
as a parallel scan that scales as O(logT)in time on a machine with O(T)processors [59, 12].
Therefore, we can evaluate an RNN by minimizing the merit function with the Levenberg-Marquardt
algorithm. Since each step of the algorithm is performed by parallel Kalman smoothing, we call this
approach Evaluating Levenberg-Marquardt with Kalman (ELK). Note that DEER is a special case
of ELK, where λ= 0, which can be seen as minimizing the unpenalized linearized objective (8),
or, alternatively, taking a Newton step with an infinitely large trust region. Moreover, under certain
conditions, ELK also enjoys global convergence guarantees [49, Thms. 11.7, 11.8].
Quasi-ELK: Scalability and Stability As with DEER, we can substitute an approximate Jacobian
into the Lagrangian to obtain the quasi-ELK algorithm. Quasi-ELK enjoys the compute and memory
scaling of quasi-DEER, as well as stability from the trust region damping from ELK. We show
empirically in Section 6.3 that while quasi-ELK takes more iterates to converge than ELK, each
quasi-ELK iterate is faster, giving overall runtime speedups.
Implementation Details The convergence rate of (quasi-)ELK depends on the trust region ra-
diusDi(or alternatively λi). While there exist methods to analytically set λi[49, Algorithm 4.3],
these approaches require factorizing ∂r/∂s, which is intractable at scale. Therefore, we treat λas a
hyperparameter set by a sweep over log-spaced values (cf. Appendix B.4).
We also use Kalman filtering instead of smoothing. We do so for two main reasons: filtering requires
less work and memory; and we also found it to converge in fewer Newton iterations than smoothing.
We believe this is a result of Proposition 1, where the early part of the trace converges first. In
Appendix A.3 we also discuss a connection that reinterprets ELK and the Kalman filter as defining
a linear recurrence where the trust region attenuates the eigenvalues used in the parallel scan.
Limitations The quasi-methods lose the local quadratic convergence properties of Newton (but
remain globally convergent, cf. Proposition 1). Our implementation of quasi-DEER for training
uses approximate gradients (cf. Section 6.2). The heuristic of resetting to zeros when unstable is
also motivated by Proposition 1, but does slow convergence in (quasi-)DEER methods. As a result,
we develop ELK to stabilize evaluation, but, like DEER, ELK has cubic complexity in D(quasi-
ELK then combats this). However, quasi-ELK adds an additional hyperparameter. Note that all four
parallelized methods discussed in this paper, as well as sequential evaluation of RNNs, have different
regimes where they are fastest. For example, in our evaluation of autoregressive RNNs (Section 6.3),
the ELK methods are faster than the DEER methods on wallclock time, but they are slower than
sequential. In our evaluation of the Lorenz96 system (Appendix B.5), ELK is more stable than
DEER, but DEER is faster on wallclock time. An area for future research is characterizing the
properties of dynamical systems and hardware where each method is fastest. Finally, at the core of
the implementation of the parallelized methods is the parallel associative scan (cf. Appendix B.6),
which at time of writing is most easily implemented in JAX [9].
5 Related Work
RNNs and Parallelism Nonlinear RNNs are a natural choice for modeling sequential data be-
cause of their inductive biases and memory efficiency. However, most nonlinear RNNs are not
6parallelizable over the sequence length, and architectures that can exploit parallel computational
hardware have been core to the success of deep learning. Therefore, a range of sequence architec-
tures that inherently admit parallelism have been proposed, including transformers [73], deep linear
RNNs [41, 31, 28, 65, 53, 27, 4, 21], and convolutions [51, 57, 56, 55]. These obtain parallelism
by developing new architectures, and do not consider parallelizing existing nonlinear architectures.
DEER [36] is notable as it considers parallel evaluation and training of arbitrary nonlinear RNNs.
Root Finding in Deep Learning Beyond DEER, there has been broad interest in using root
finders/fixed-point iterations in deep learning and sequence modeling. Deep implicit layers [33, 1–
3] and neural ODEs [13, 42] replace conventional feed forward network layers [26] with an implicit
layer whose output is the root of an equation. Moreover, Song et al. [66] parallelizes the evalua-
tion of feedforward nets using Jacobi and Gauss-Siedel iterations. In sequence modeling, parallel
decoding methods [45, 58, 23] adapt ideas from Jacobi and Gauss-Siedel iterations to evaluate au-
toregressive sequence models in parallel. These approaches iteratively refine inputs by repeatedly
feeding in previous outputs back into a parallelized sequence model. However, these methods pre-
suppose the existence of a parallelized forward pass for the sequence model and do not leverage
additional gradient information to obtain sublinear convergence.
Parallelizing Dynamical Systems over Time Other work has investigated evaluating other non-
linear dynamical systems over time. ParaDIGMS [63] parallelizes sampling from diffusion models,
but uses Picard iterations instead of Newton’s method. In the numerical ODE and PDE communities
there has been great interest in Parallel in Time methods, see Gander [24], Ong and Schroder [50]
for surveys. Vargas et al. [72] parallelized the evaluation of chaotic dynamical systems over time,
but instead of casting Newton’s method as a parallel scan resorts to multigrid methods to evaluate at
different hierarchies. Moreover, these methods have not yet been applied to parallelizing RNNs.
Scaling and Stabilizing Newton’s Method Quasi-Newton methods are efficient algorithms that
use an approximation of the Jacobian or Hessian in Newton’s method, and include approaches like
BFGS [11, 22, 25, 61] and L-BFGS [37]. Other approaches use Newton’s method to optimize deep
nets [40]. However, these quasi-Newton algorithms do not admit efficient parallel scans. There are
also conjugate gradients methods for exploiting structured Jacobians or Hessians [68], though they
often do not attain the fast convergence rates of Newton or quasi-Newton methods [49]. Methods for
stabilizing and ensuring Newtons method converges globally include regularization approaches [48,
20], backtracking line search [52], and trust regions [16]. All these stabilization methods have
strengths and weaknesses, but as noted by Nocedal and Wright [49]: “the trust-region Newton
method has proved to be highly effective in practice, ” leading us to apply it to evaluating RNNs.
Nonlinear Least Squares and Kalman Smoothing Bell and Cathey [6] and Bell [5] draw connec-
tions between the Gauss-Newton method and the iterated extended Kalman filter and smoother [67,
70]. Because Gauss-Newton is unstable, it is natural to use Levenberg-Marquardt [35, 39] to stabi-
lize the filtering/smoothing problem [14, 38, 71]. These approaches start with a smoothing problem,
and stabilize it using approaches from nonlinear equations; whereas we start with a nonlinear equa-
tion to solve, and make the connection with Kalman filtering to leverage parallelized algorithms [59].
We also address the practicalities of operationalizing this connection for modern deep networks.
6 Experiments
We now experimentally examine the relative performance of these methods. Specifically, we eval-
uate whether quasi-DEER can provide memory savings over DEER and runtime savings over se-
quential evaluation, while retaining the accuracy of training and evaluation; and whether ELK and
quasi-ELK can be used to stabilize evaluation in regimes where DEER is unstable. In Sections 6.1
and 6.2 we use experimental designs from Lim et al. [36] and show quasi-DEER retains the fast run-
time and accuracy of DEER, and can reduce memory consumption by up to an order of magnitude.
In Section 6.3 we show that (quasi-)ELK remains stable when DEER becomes unstable, and that
quasi-ELK is the fastest of all parallelized methods. We provide further details in Appendix B.
710−1100101Wallclock (s)
D= 8
 D= 16
 D= 32
 D= 64Sequential DEER Quasi-DEER
30K 100K 300K 1M100101Memory (GB)
30K 100K 300K 1M
 30K 100K 300K 1M
 30K 100K 300K 1M
Sequence Length (T)Figure 2: Evaluating an untrained GRU. Relative performance of sequential, DEER and quasi-
DEER for evaluating a randomly initialized (and untrained) GRU on ( Top Row ) wall-clock time,
averaged over 20 random seeds and ( Bottom Row ) memory, averaged over 3 random seeds. All
experiments use a 16GB V100 SMX2 (memory capacity indicated by the black dashed line) and
Newton methods were run to convergence. Missing points in each series indicate the GPU ran
out of memory. Quasi-DEER has a runtime commensurate with DEER, but with lower memory
consumption, allowing quasi-DEER to work at scales where DEER cannot. The accuracy of the
final converged solution is similar for all methods (see Figure 5 in Appendix B.1).
6.1 Quasi-DEER for Evaluation
We first use an experimental design from Lim et al. [36]. The task is to evaluate an untrained GRU
across a range of hidden state sizes ( D) and sequence lengths ( T) on a 16GB V100 GPU; the inputs
to the RNN also have dimension D. We compare the wall-clock time and memory usage of three
methods: sequential evaluation, DEER, and quasi-DEER. Results are shown Figure 2.
Both DEER and quasi-DEER are up to twenty times faster than sequential evaluation. The runtimes
are similar between DEER and quasi-DEER for small networks, because although quasi-DEER
steps are faster, quasi-DEER takes more iterations to converge. For larger networks, the difference
in runtime is more pronounced. We also see that quasi-DEER requires as much as an order of mag-
nitude less memory than DEER, thus allowing the application to architectural regimes previously
infeasible with DEER. In Figure 6 of Appendix B.1.1 we show that in smaller TandDregimes we
observe the expected sublinear time scaling with sequence length. This experiment confirms that
quasi-DEER can replicate the performance of DEER, but with a smaller memory footprint.
6.2 Quasi-DEER for Training
We verify that quasi-DEER expedites training nonlinear RNN models. We replicate the third exper-
iment from Lim et al. [36], where a GRU is trained to classify C. elegans phenotypes from the time
series of principal components of the worms’ body posture [10].
We show results in Figure 3. We see that the training dynamics under quasi-DEER leads to the sim-
ilar validation accuracy trajectories. However, every quasi-DEER training step is faster by a factor
of2.5, despite performing 2-2.5times more Newton iterations per training step. This highlights
how quasi-DEER can replace DEER when training nonlinear RNNs, bringing both time and mem-
ory savings. In our experiment, we use the quasi- approximation for the backwards pass as well,
leading to gradients that are different from DEER in this setting, but we show empirically that there
is negligible degradation in performance (Figure 3, Left).
DEER is prone to “spikes”, where orders of magnitude more steps are required for convergence
(Figure 3, Middle). While quasi-DEER is not as susceptible to these spikes (never more than half an
order of magnitude), these instabilities motivated the study of stabilizing methods.
80 50K 100K
Training Step050100Validation
Accuracy (%)Sequential
DEER
Quasi-DEER
0 50K 100K
Training Step0.00.20.4Wallclock time
per param
update (s)
0 50K 100K
Training Step01020Newton iters
per updateFigure 3: Training a GRU with DEER. Comparison of DEER and quasi-DEER during GRU
training for the C. elegans time-series classification task (Section 6.2). Each time series has length
T= 17 ,984. We show the median, and 5-95% interval across a rolling window of 20 training
steps. (Left) DEER and quasi-DEER have the similar validation accuracy trajectories, indicating
similar training dynamics. The sequential trace shown is for 24 hours of training (compared to 11
and 4 hours for the whole DEER and quasi-DEER traces). (Center) Each quasi training iteration
is 2.5 times faster than each DEER training iteration. Sequential training steps took more than 6
seconds each (not pictured). (Right) Each quasi training iteration requires (approximately) 2-2.5
times more Newton iterations to converge, indicating that each quasi Newton step is approximately
6 times faster than the corresponding DEER Newton step.
6.3 ELK and Quasi-ELK for Evaluating Autoregressive RNNs
We conclude by studying an application where these numerical instabilities in DEER are critical.
We use a small autoregressive GRU (hidden dimension Nh= 3), where the previous sampled value
is input into the GRU at the next step. Such autoregressive architectures were not examined by Lim
et al. [36], but are an important class of models. We describe the precise details of the AR GRU we
use in Appendix B.3. Crucially, the Markovian state stused by all four parallelized methods must
be expanded to include the current sampled output value, as well as the current GRU state.
Initialized AR GRU We first repeat the analysis in Section 6.1 (and similar to the evaluation in
Lim et al. [36]) for evaluating a randomly initialized autoregressive GRU. We see in the top left
panel of Figure 4 that all four parallelized methods converge rapidly and stably to the correct trace,
indicated by a low mean absolute discrepancy (MAD) between the true trace and the generated trace.
Trained AR GRU We then study a pre-trained GRU that generates a noisy sine wave (see Figure 4,
Bottom). The linear recurrence relation (6) was numerically unstable in DEER and quasi-DEER. To
remedy these instabilities, we take the approach described earlier of setting the unstable parts of the
trace to a fixed value (here zero). Doing so ensures convergence; but at the cost of “resetting” the
optimization for large swathes of the trace (Figure 4, Bottom), slowing convergence (see Figure 4,
Top Right). This finding highlights how the instabilities of DEER — which are inherited from both
pathologies of Newton’s method and the parallel recurrence — can be crippling in even very simple
scenarios. While resetting allows for convergence, the resulting convergence is very slow.
We then apply ELK and quasi-ELK. We show the results in the top right and bottom panels of Fig-
ure 4. We select the trust region size with a one-dimensional search over log-spaced values between
100and107. We see ELK has stabilized convergence, with the evaluation never incurring numerical
instabilities or requiring heuristics. Crucially, by taking more stable steps (and not needing stabiliz-
ing heuristics) ELK and quasi-ELK converge faster than DEER and quasi-DEER. ELK can stabilize
and expedite the convergence of DEER, with quasi-ELK faster still (by wall-clock time).
However, on this task, all parallelized methods (including DEER) are slower than sequential gen-
eration. Quasi-ELK is the fastest parallel method, taking 221 milliseconds, compared to sequential
evaluation, taking 96 milliseconds. For comparison, DEER took 1,225 milliseconds. Quasi-ELK
therefore still represents a large improvement in runtime over previous parallel methods. We pro-
vide timing details and further discussion in Appendix B.3.2.
90 5 10 15
Newton Iterations10−1010−710−4Untrained
ARGRU MAD
0 2000 4000 6000 8000 10000
Newton Iterations10−610−3100103Trained
ARGRU MADDEER Quasi-DEER ELK Quasi-ELK
-200201
-20020100
-200201000
0 2000 4000 6000 8000 10000
Timestep,t-200202000Time series after
Newton iteration:Figure 4: ELK stabilizes parallel evaluation of an AR GRU. (Top Left) The mean absolute differ-
ence (MAD) evaluated on the outputs converges rapidly for all four methods on a sequence gener-
ated by an untrained AR GRU. (Top Right) The MAD for evaluating a trained AR GRU. Undamped
DEER variants are unstable and converge slowly (using the reset heuristic). ELK stabilizes and ac-
celerates convergence. (Bottom) The output after 1, 100, 1000, and 2000 Newton iterations. The
black dotted line is the true trace. ELK and quasi-ELK converge rapidly, but DEER and quasi-DEER
are unstable. The lines where DEER and quasi-DEER are zero depict the zeroing heuristic.
7 Conclusion
In this paper we proposed methods for scalable and stable parallel evaluation of nonlinear RNNs.
DEER [36] achieved speedups over sequential evaluation, but incurred quadratic memory, cubic
work, and numerical instabilities. We therefore extended DEER to use quasi-Newton approxima-
tions, reduing computational complexity; and provided a novel proof that both DEER and quasi-
DEER converge globally. To stabilize DEER, we leveraged a connection between the Levenberg-
Marquardt method and Kalman smoothing to enable parallel evaluation of RNNs, allowing us to sta-
bilize DEER while still leveraging fast parallel filtering. We verified empirically that quasi-DEER,
ELK, and quasi-ELK improve convergence across a range of metrics and examples. This result
allows parallel evaluation of nonlinear RNNs to be scaled beyond what is possible with DEER.
When selecting an approach, we offer the following advice: If rapid convergence is reliably ob-
served, our experiments show that quasi-DEER provides the fastest convergence in terms of wall-
clock time. However, if the dynamics are on the edge of stability, then of the parallelized methods,
ELK offers the most stable performance, but quasi-ELK could be faster in wall-clock time and just
as stable. In such settings, it is worth sweeping the hyperparameter to choose the best version of
ELK (note that for λ= 0, ELK specializes to DEER). However, in the setting of chaotic dynamics,
standard sequential evaluation may ultimately be faster.
Our experiments and these observations also highlight avenues for future research. While we found
success with a diagonal approximation, structured approximations of the Jacobian that still admit
fast parallelism but are more faithful approximations may allow for more accurate quasi steps to
be taken. Secondly, quantifying the convergence rates of quasi-ELK would allow us to provide
tighter bounds than those derived in Proposition 1. Finally, theoretically investigating whether fur-
ther improvements to parallelized methods can prove faster than sequential evaluation for dynamical
systems on the edge of stability, or whether there are fundamental limitations to the computational
benefit of parallelization, are interesting questions for future work.
10Acknowledgements
We thank John Duchi, David Zoltowski, and the members of the Linderman Lab for their helpful
feedback. This work was supported by grants from the NIH BRAIN Initiative (U19NS113201,
R01NS131987, & RF1MH133778), the NSF/NIH CRCNS Program (R01NS130789). X.G. would
also like to acknowledge support from the Walter Byers Graduate Scholarship from the NCAA.
S.W.L. is supported by fellowships from the Simons Collaboration on the Global Brain, the Alfred
P. Sloan Foundation, and the McKnight Foundation. The authors have no competing interests to
declare.
Some of the experiments were performed on the Sherlock cluster. We would like to thank Stanford
University and the Stanford Research Computing Center for providing computational resources and
support that contributed to these research results.
References
[1] S. Bai, J. Z. Kolter, and V . Koltun. Deep equilibrium models. In Advances in Neural Informa-
tion Processing Systems , volume 32, 2019.
[2] S. Bai, V . Koltun, and J. Z. Kolter. Multiscale deep equilibrium models. In Advances in Neural
Information Processing Systems , volume 33, pages 5238–5250, 2020.
[3] S. Bai, V . Koltun, and J. Z. Kolter. Neural deep equilibrium solvers. In International Confer-
ence on Learning Representations , 2021.
[4] M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brand-
stetter, and S. Hochreiter. xLSTM: Extended long short-term memory. arXiv preprint
arXiv:2405.04517 , 2024.
[5] B. M. Bell. The iterated Kalman smoother as a Gauss–Newton method. SIAM Journal on
Optimization , 4(3):626–636, 1994. doi: 10.1137/0804035.
[6] S. M. Bell and F. W. Cathey. The iterated Kalman filter update as a Gauss-Newton method.
IEEE Transactions on Automatic Control , 38(2):294–297, 1993.
[7] G. E. Blelloch. Prefix sums and their applications. Technical Report CMU-CS-90-190,
Carnegie Mellon University, School of Computer Science, 1990.
[8] S. Boyd and L. Vandenberghe. Convex Optimization . Cambridge University Press, Cambridge,
UK, 2004. ISBN 9780521833783.
[9] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, and S. Wanderman-
Milne. JAX: composable transformations of Python+NumPy programs, 2018. URL http:
//github.com/google/jax .
[10] A. E. Brown, E. I. Yemini, L. J. Grundy, T. Jucikas, and W. R. Schafer. A dictionary of behav-
ioral motifs reveals clusters of genes affecting caenorhabditis elegans locomotion. Proceedings
of the National Academy of Sciences , 110(2):791–796, 2013.
[11] C. Broyden. The convergence of a class of double-rank minimization algorithms. IMA Journal
of Applied Mathematics , 6(1):76–90, 1970.
[12] P. Chang, G. Harper-Donnelly, A. Kara, X. Li, S. Linderman, and K. Murphy. Dynamax: State
space models library in JAX, 2023. URL https://github.com/probml/dynamax .
[13] R. T. Q. Chen, Y . Rubanova, J. Bettencourt, and D. Duvenaud. Neural ordinary differential
equations. In Advances in Neural Information Processing Systems , volume 31, pages 6571–
6583, 2018.
[14] Y . Chen and D. S. Oliver. Levenberg–Marquardt forms of the iterative ensemble smoother for
efficient history matching and uncertainty quantification. Computational Geosciences , 17(4):
689–703, 2013. doi: 10.1007/s10596-013-9351-5.
11[15] K. Cho, B. van Merriënboer, Ç. Gülçehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y . Ben-
gio. Learning phrase representations using RNN encoder-decoder for statistical machine trans-
lation. arXiv preprint arXiv:1406.1078 , 2014.
[16] A. Conn, N. Gould, and P. Toint. Trust Region Methods , volume 1. Society for Industrial and
Applied Mathematics, 2000.
[17] J. C. Costacurta, S. Bhandarkar, D. M. Zoltowski, and S. W. Linderman. Structured flexibility
in recurrent neural networks via neuromodulation. NeurIPS , 2024. doi: 10.1101/2024.07.26.
605315.
[18] C. Curto and K. Morrison. Graph rules for recurrent neural network dynamics. Notices of
the American Mathematical Society , 70(4), April 2023. doi: 10.1090/noti2661. URL https:
//doi.org/10.1090/noti2661 .
[19] F. Dinc, A. Shai, M. Schnitzer, and H. Tanaka. CORNN: Convex optimization of recurrent
neural networks for rapid inference of neural dynamics. Adv. Neural Inf. Process. Syst. , Nov.
2023.
[20] N. Doikov and Y . Nesterov. Gradient regularization of Newton method with Bregman dis-
tances. Mathematical Programming , pages 1–25, 2023.
[21] L. Feng, F. Tung, M. O. Ahmed, Y . Bengio, and H. Hajimirsadeghi. Were rnns all we needed?
arXiv , 2024. URL https://doi.org/10.48550/arXiv.2410.01201 .
[22] R. Fletcher. A new approach to variable metric algorithms. The Computer Journal , 13(3):
317–322, 1970.
[23] Y . Fu, P. Bailis, I. Stoica, and H. Zhang. Break the sequential dependency of LLM inference
using lookahead decoding. In Forty-first International Conference on Machine Learning , 2024.
[24] M. J. Gander. 50 Years of Time Parallel Time Integration , volume 9 of Contributions in
Mathematical and Computational Sciences . Springer International Publishing, 2015. ISBN
978-3-319-23320-5. doi: 10.1007/978-3-319-23321-2. URL https://doi.org/10.1007/
978-3-319-23321-2 .
[25] D. Goldfarb. A family of variable-metric methods derived by variational means. Mathematics
of Computation , 24(109):23–26, 1970.
[26] I. Goodfellow, Y . Bengio, and A. Courville. Deep Learning . MIT Press, 2016. ISBN 978-
0262035613.
[27] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. In
First Conference on Language Modeling , 2024.
[28] A. Gu, K. Goel, and C. Ré. Efficiently modeling long sequences with structured state spaces.
InInternational Conference on Learning Representations (ICLR) , 2021.
[29] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–
1780, 1997.
[30] S. Hooker. The hardware lottery. Communications of the ACM , 64(12):58–65, 2021.
[31] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are RNNs: Fast autore-
gressive transformers with linear attention. In International Conference on Machine Learning ,
pages 5156–5165. PMLR, 2020.
[32] P. Kidger. On Neural Differential Equations . PhD thesis, University of Oxford, 2021.
[33] Z. Kolter, D. Duvenaud, and M. Johnson. Deep implicit layers - Neural ODEs, Deep Equilib-
rium Models, and Beyond, 2020. NeurIPS 2020 Tutorial.
[34] D. Lawson, A. Raventós, A. Warrington, and S. Linderman. SIXO: Smoothing inference with
twisted objectives. In Advances in Neural Information Processing Systems , volume 35, 2022.
12[35] K. Levenberg. A method for the solution of certain non-linear problems in least squares.
Quarterly of Applied Mathematics , 2:164–168, 1944.
[36] Y . H. Lim, Q. Zhu, J. Selfridge, and M. F. Kasim. Parallelizing non-linear sequential models
over the sequence length. In International Conference on Learning Representations , 2024.
[37] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization.
Mathematical Programming , 45(1-3):503–528, 1989.
[38] J. Mandel, E. Bergou, S. Gürol, S. Gratton, and I. Kasanick `y. Hybrid Levenberg–Marquardt
and weak-constraint ensemble Kalman smoother method. Nonlinear Processes in Geophysics ,
23(2):59–73, 2016.
[39] D. W. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal
of the Society for Industrial and Applied Mathematics , 11(2):431–441, 1963.
[40] J. Martens and R. Grosse. Optimizing neural networks with Kronecker-factored approximate
curvature. In International conference on machine learning , pages 2408–2417. PMLR, 2015.
[41] E. Martin and C. Cundy. Parallelizing linear recurrent neural nets over sequence length. In
International Conference on Learning Representations , 2018.
[42] S. Massaroli, M. Poli, S. Sonoda, T. Suzuki, J. Park, A. Yamashita, and H. Asama. Differ-
entiable multiple shooting layers. Advances in Neural Information Processing Systems , 34:
16532–16544, 2021.
[43] W. Merrill, J. Petty, and A. Sabharwal. The illusion of state in state-space models. In Forty-first
International Conference on Machine Learning , 2024.
[44] I. D. Mienye, T. G. Swart, and G. Obaido. Recurrent neural networks: A comprehensive
review of architectures, variants, and applications. Information , 15:517, 2024. doi: 10.3390/
info15090517. URL https://doi.org/10.3390/info15090517 . Academic Editor: María
N. Moreno García.
[45] T. Mihaylova and A. F. T. Martins. Scheduled sampling for transformers. In F. Alva-Manchego,
E. Choi, and D. Khashabi, editors, Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics: Student Research Workshop , pages 351–356, Florence, Italy,
July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-2049.
[46] K. Murphy. Probabilistic Machine Learning . Cambridge, 2022.
[47] Y . Nesterov. Lectures on Convex Optimization , volume 137 of Springer Optimization and
Its Applications . Springer, 2nd edition, 2018. ISBN 978-3-319-91577-4. doi: 10.1007/
978-3-319-91578-1. URL https://doi.org/10.1007/978-3-319-91578-1 .
[48] Y . Nesterov and B. T. Polyak. Cubic regularization of newton method and its global perfor-
mance. Mathematical Programming , 108(1):177–205, 2006.
[49] J. Nocedal and S. J. Wright. Numerical Optimization . Springer, 2 edition, 2006.
[50] B. W. Ong and J. B. Schroder. Applications of time parallelization. Computing and Vi-
sualization in Science , 23:1–10, 2020. doi: 10.1007/s00791-020-00323-3. URL https:
//doi.org/10.1007/s00791-020-00323-3 .
[51] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner,
A. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint
arXiv:1609.03499 , 2016.
[52] J. M. Ortega and W. C. Rheinboldt. Iterative Solution of Nonlinear Equations in Several
Variables . SIAM, 2000.
[53] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De. Resur-
recting recurrent neural networks for long sequences. In International Conference on Machine
Learning , pages 26670–26698. PMLR, 2023.
13[54] C. Pandarinath, D. J. O’Shea, J. Collins, R. Jozefowicz, S. D. Stavisky, J. C. Kao, E. M.
Trautmann, M. T. Kaufman, S. I. Ryu, L. R. Hochberg, J. M. Henderson, K. V . Shenoy, L. F.
Abbott, and D. Sussillo. Inferring single-trial neural population dynamics using sequential
auto-encoders. Nature Methods , 15:805–815, 2018.
[55] R. Parnichkun, S. Massaroli, A. Moro, J. T. Smith, R. Hasani, M. Lechner, Q. An, C. Re,
H. Asama, S. Ermon, T. Suzuki, M. Poli, and A. Yamashita. State-free inference of state-space
models: The transfer function approach. In Forty-first International Conference on Machine
Learning , 2024.
[56] M. Poli, S. Massaroli, E. Nguyen, D. Y . Fu, T. Dao, S. Baccus, Y . Bengio, S. Ermon, and C. Ré.
Hyena hierarchy: Towards larger convolutional language models. In International Conference
on Machine Learning , pages 28043–28078. PMLR, 2023.
[57] D. W. Romero, A. Kuzina, E. J. Bekkers, J. M. Tomczak, and M. Hoogendoorn. CKConv:
Continuous kernel convolution for sequential data. In International Conference on Learning
Representations , 2022.
[58] A. Santilli, S. Severino, E. Postolache, V . Maiorca, M. Mancusi, R. Marin, and E. Rodolà.
Accelerating transformer inference for translation via parallel decoding. In Proceedings of the
61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-
pers) , pages 12336–12355, Toronto, Canada, July 2023. Association for Computational Lin-
guistics. doi: 10.18653/v1/2023.acl-long.689.
[59] S. Särkkä and Á. F. García-Fernández. Temporal parallelization of bayesian smoothers. IEEE
Transactions on Automatic Control , 66(1):299–306, 2021. doi: 10.1109/TAC.2020.2976316.
[60] M. Schimel, T.-C. Kao, K. T. Jensen, and G. Hennequin. ILQR-V AE: Control-Based Learning
of Input-Driven Dynamics with Applications to Neural Data. In Proceedings of the Interna-
tional Conference on Learning Representations (ICLR) . ICLR, 2022.
[61] D. Shanno. Conditioning of quasi-newton methods for function minimization. Mathematics of
Computation , 24(111):647–656, 1970.
[62] O. Shchur, M. Bilos, and S. Gunnemann. Intensity-free learning of temporal point processes.
InInternatinal Conference on Learnng Representations , 2020.
[63] A. Shih, S. Belkhale, S. Ermon, D. Sadigh, and N. Anari. Parallel sampling of diffu-
sion models. 37th Conference on Neural Information Processing Systems , 2023. URL
https://doi.org/10.48550/arXiv.2305.16317 . 37th Conference on Neural Informa-
tion Processing Systems.
[64] J. Smith, S. Linderman, and D. Sussillo. Reverse engineering recurrent neural networks with
Jacobian switching linear dynamical systems. Advances in Neural Information Processing
Systems , 34:16700–16713, 2021.
[65] J. T. Smith, A. Warrington, and S. W. Linderman. Simplified state space layers for sequence
modeling. In International Conference on Learning Representations (ICLR) , 2023.
[66] Y . Song, C. Meng, R. Liao, and S. Ermon. Accelerating feedforward computation via parallel
nonlinear equation solving. In International Conference on Machine Learning , 2021.
[67] H. W. Sorenson. Kalman filtering techniques. In H. W. Sorenson, editor, Kalman Filtering:
Theory and Application , page 90. IEEE Press, New York, 1966.
[68] T. Steihaug. The conjugate gradient method and trust regions in large scale optimization. SIAM
Journal on Numerical Analysis , 20(3):626–637, 1983.
[69] D. Sussillo and O. Barak. Opening the black box: Low-dimensional dynamics in high-
dimensional recurrent neural networks. Neural Computation , 25(3):626–649, 2013.
[70] S. Särkkä. Bayesian Filtering and Smoothing . Cambridge University Press, Cambridge, UK,
2013. ISBN 978-1-107-03385-3.
14[71] S. Särkkä and L. Svensson. Levenberg-Marquardt and line-search extended Kalman smoothers.
InICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP) , pages 5875–5879. IEEE, 2020. doi: 10.1109/ICASSP40776.2020.9054764.
[72] D. A. Vargas, R. D. Falgout, S. Günther, and J. B. Schroder. Multigrid reduction in time
for chaotic dynamical systems. SIAM Journal on Scientific Computing , 45(4):A2019–A2042,
2023. doi: 10.1137/22M1519605.
[73] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polo-
sukhin. Attention is all you need. In Proceedings of the 31st International Conference on
Neural Information Processing Systems , pages 6000–6010, 2017.
[74] S. Vyas, M. D. Golub, D. Sussillo, and K. V . Shenoy. Computation through neu-
ral population dynamics. Annual Review of Neuroscience , 43:249–275, 2020.
doi: 10.1146/annurev-neuro-092619-094115. URL https://doi.org/10.1146/
annurev-neuro-092619-094115 .
[75] C. K. Yap. Fundamental Problems in Algorithmic Algebra . Oxford University Press, Oxford,
UK, 1993. ISBN 978-0-19-512537-3.
15A Theoretical Results
A.1 Proof of Proposition 1
Proposition 1 Undamped Newton’s method will converge to the true solution, s∗, of the fixed point
(2)in at most TNewton iterations, for any initial s(0).
Proof. We prove this result by induction on the sequence length.
In general, the guess s(0)need not equal the solution s∗anywhere. However, the initial state s0and
the dynamics functions fare fixed. Therefore, s∗
1=f(s0)and in general s∗
t=f(s∗
t−1). Thus, it
follows from the initial condition of the DEER recurrence relation that s(i)
1=s∗
1for all i≥1.
Furthermore, we observe that if s(i)
t=s∗
tfor all tless than some t(i), then rt(s(i)) =0for all
t < t(i)by the definition of the residual in (1). Therefore, the DEER linear recurrence relation
necessarily gives ∆s(i+1)
t =0for all t < t(i). Furthermore, because s∗
t(i)=f(s(i)
t), it follows that
∆s(i+1)
t(i)=−rt(i)(s(i)) =s∗
t(i)−s(i)
t(i). Thus, it follows that after applying another Newton iteration
thats(i+1)
t =s∗
tfor all t < t(i)+ 1. The global convergence result and bound on Newton iterates
follows by induction.
We note that this proof technique (induction) is very similar to that used to prove Proposition 1
of Shih et al. [63]. However, Shih et al. [63] proves a result about Picard iterations (zeroth order
method). Our proof about the global convergence of Newton iterations contains the additional com-
plication of dealing with a LDS (as a consequence of using a first order method). Note, that the
global convergence comes from the zeroth order update; however, the first order gradient informa-
tion can accelerate convergence.
Discussion of corollaries of Proposition 1 Corollaries of this proposition include that the method
will converge (in at most Titerations) to s∗even if the Jacobians ∂f/∂sare replaced by arbitrary
matrices, and that we can reset the values of s(i)
tfort > i arbitrarily and still enjoy convergence.
In more detail, the elements in the sub-block-diagonal of J:=∂r/∂scan be replaced with arbitrary
values – but the main block diagonal must remain as the identity and all other entries must be
zero. Retaining convergence under modifications to the sub-block-diagonal portion is a corollary of
Proposition 1, and can be seen from (6): If all the states up to and including position t−1at the
(i)th Newton iteration are correct, then the update in (6) at Newton iteration (i+ 1) for position t
will use ∆s(i+1)
t−1=0(no update is required at position t−1), and so the update to s(i+1)
t no longer
depends on the Jacobian.
We exploit this to develop quasi-DEER, retaining only the diagonal of the Jacobians. This reduces
the parallel scan from O(D3)toO(D)making each iteration faster (while still admitting global
convergence as above), but needs more Newton iterations to converge due to approximate updates.
We find that this trade-off often yields a faster wallclock time (cf. Figure 7).
Explicitly, the global convergence of quasi-DEER is a theoretical result (a corollary of Proposi-
tion 1), but the fast runtime of quasi-DEER in practice is an empirical result (cf. Figure 3).
A.2 The Merit Function Has No Local Minima or Saddle Points
Proposition 3. The merit function L(s)defined in (7)has a global minimum at the true trace s∗,
satisfying L(s∗) = 0 . It has no other critical points, i.e. no ssuch that ∇L(s) =0other than at the
unique s∗for which r(s∗) =0.
Proof. First, we observe that ∇L(s) =J(s)Tr(s), where J(s)is defined as in (3). Because J(s)is a
lower triangular matrix with all entries on its diagonal equal to 1, it follows that all of its eigenvalues
are equal to 1. Therefore, J(s)is nonsingular for all s. Thus, J(s)has trivial nullspace for all s, i.e.
J(s)Tr(s) =0⇐⇒r(s) =0. But only s∗satisfies r(s∗) =0.
16Since there are no critical points other than s∗, the merit function cannot have local minima or saddle
points.
We also discuss further the uniqueness of the global minimizer s∗of the merit function L.
For a deterministic forward function fand fixed inputs there is a fixed sequence of states and outputs
(note that any stochastic dynamics function can be reparameterized as deterministic by conditioning
on the random inputs). Thus, s∗is the only sequence with zero residual (i.e. there is a unique
sequence generated by the deterministic dynamics).
Furthermore, DEER cannot get stuck at any point that is not this sequence. We prove this in Propo-
sition 1. Another way to see this however is that each update step (4) is equal to J−1r. But, Jis
always invertible and so has trivial nullspace. Furthermore, the residual rcan only be zero at the
unique solution s∗. Thus J−1ris nonzero everywhere except at the true solution, where it is zero.
Thus, DEER cannot get stuck en route to finding the true and unique solution.
A.3 Kalman Filtering Damps the Eigenvalues of the Dynamics Matrices
A complementary perspective on how ELK results in more stable evaluation of nonlinear RNNs is to
see how the Kalman filter damps the eigenvalues of the Jacobian matrices of the transition dynamics.
We first provide a high-level overview, and then provide a more detailed derivation.
Overview LetAtbe the Jacobians ∂f/∂sused in the linear recurrence relations and btbe the
offsets. Then the prediction step of the Kalman filter (ELK) is the same as DEER. However, after
applying the update step in ELK (which imposes the trust region), we obtain a second linear recur-
rence relation where the linear operator is given by ΓtAT. Note that Γtis a symmetric positive
definite matrix with eigenvalues bounded above by 1/1+λ. Thus, by the Spectral Theorem, it fol-
lows that the norms of the eigenvalues of ΓtAtare bounded above by the max of the norms of the
eigenvalues of At, scaled by 1/1+λ. Note that larger λcorresponds to more regularization/smaller
trust region; and therefore correspondingly results in smaller effective eigenvalues in the scan. We
recover DEER exactly if λ= 0. Thus, while large eigenvalues in Atare the cause of the insta-
bility of DEER when evaluating unstable dynamical systems, ELK directly attenuates these large
eigenvalues, explaining why the intermediate iterations using ELK remain stable.
Derivation We define our dynamics used in Newton iteration (i+ 1) as
At=∂ft+1
∂s(s(i)
t)
bt=ft+1(s(i)
t)−∂ft+1
∂s(s(i)
t)s(i)
t.
NowAt∈RD×Dandbt∈RD.
In line with considering the system as the LDS in (11), we set the process noise to be ID, and with
the emissions governed by
s(i+1)
t∼ N(s(i)
t, σ2ID),
where σ2controls the size of our trust region (note that in the notation of developed in Section 4.2
we have λ= 1/σ2).
In the notation of Murphy [46], we see that the predict step is
µ(t+1)|t=Jtµt|t+bt
Σ(t+1)|t=AtΣt|tAT
t+ID.
Meanwhile, the update step is
µ(t+1)|(t+1)=µ(t+1)|t+Σ(t+1)|t(Σ(t+1)|t+σ2ID)−1(yt+1−µ(t+1)|t)
Σ(t+1)|(t+1)=Σ(t+1)|t−Σ(t+1|t)(Σ(t+1|t)+σ2ID)ΣT
(t+1|t).
To unpack this further, we first define the attenuation matrix
Γt=σ2
AtΣt|tAT
t+ (σ2+ 1)ID−1
.
17Because Σt|tis a covariance matrix, it is also symmetric positive definite, and so AtΣt|tAT
tis
symmetric positive definite, and so all of its eigenvalues are greater than zero. Therefore, all the
eigenvalues of AtΣt|tAT
t+ (σ2+ 1)IDare greater than σ2+ 1.
We note that Γtis also symmetric and positive definite. Thus, by the Spectral Theorem, all eigen-
values of Γtare positive. By the above argument, the eigenvalues of Γtare all less thanσ2
1+σ2<1.
Thus, we observe that the resulting filtering is given by the recurrence relation
µ(t+1)|(t+1)=linearz}|{
ΓtAtµt|t+offsetz }| {
Γtbt+ (AtΣt|tAT
t+ID)
AtΣt|tAT
t+ (σ2+ 1)ID−1
yt+1
Σ(t+1)|(t+1)=Γt(AtΣt|tAT
t+ID).
Givenn
Σt|toT−1
t=0, we see that the filtered means (the updates for ELK) come from a linear recur-
rence relation with linear term ΓtAt.
We therefore compare the eigenvalues of ΓtAtto eigenvalues of At. Because Γtis symmetric
positive definite, by the Spectral Theorem we can write Γt=QΛtQT, where Qis an orthogonal
(and therefore unitary) matrix, and Λtis a diagonal matrix where every entry is in (0,1)(the entries
ofΛtare the eigenvalues of Γt, which are greater than 0by the Spectral Theorem and less than
σ2
1+σ2<1by the argument above).
Now, let’s consider any arbitrary unit vector v∈CD, and let Λmax
tdenote the maximum of the
norms of all eigenvalues of At. Then ∥Atv∥2≤Λmax
tby the definition of Λmax
t. However, we want
to know ∥QΛtQTAtv∥2for any arbitrary unit vector v∈RD. However, we know that the action
of a unitary matrix cannot change the 2-norm of a vector, so ∥QΛtQTAtv∥2=∥ΛtQTAtv∥2.
Moreover, multiplying a vector by a diagonal matrix cannot increase the 2-norm of a vector by more
than the absolute value of the diagonal matrix, which in the case of Λtis bounded above by σ2/σ2+1.
Thus,∥QΛtQTAtv∥2≤σ2
σ2+1∥Atv∥, or
∥QΛtQTAtv∥2≤σ2
1 +σ2Λmax
t
for any unit vector v∈CD. This highlights that we can interpret reducing σ2(reducing the size
of the trust region and increasing stabilization) as directly attenuating the eigenvalues in the linear
recurrence, helping to combat eigenvalues with large magnitude.
B Experimental Details
B.1 Quasi-DEER for Evaluation
In this section we elaborate on our Experiment 1, discussed in Section 6.1. We closely follow the
experimental design in Section 4.1 of Lim et al. [36], including 5 warm-up steps for all timing
experiments and a batch size of 16. However, instead of running 5 seeds for 20 repetitions each, we
run 20 seeds for 5 repetitions each, to get more coverage over different evaluation (as the timing for
each evaluation are observed to be low variance). We also include memory profiling experiments
not present in Lim et al. [36]. For these experiments we use 3 random seeds and only one repetition,
because the memory usage is extremely stable in between runs. We discus the memory profiling
experiments in more detail in Section B.1.3.
For runs with the same specifications (sequence length T, hidden state size D, and algorithm), we
observe that sometimes runs with memory profiling ran out of memory whereas runs with timing
profiling did not run out of memory. A difference between our time profiling runs and memory
profiling runs was how we handled preallocation of memory. For time profiling, we allowed JAX
to preallocate memory because that is how JAX usually runs, and so gives a better indication of
wallclock time in practice. For memory profiling, we did not allow JAX to preallocate memory so
we could get a more fine-grained measure of memory usage.
However, JAX provides this following discussion of memory preallocation (see https://jax.
readthedocs.io/en/latest/gpu_memory_allocation.html# ):
189800 9820 9840 9860 9880 9900 9920 9940 9960 99800.5
0.00.5GRU outputs for last 200 indices, DEER vs Sequential
DEER
Sequential
0 2000 4000 6000 8000 100002
021e7
Difference between sequential and DEER outputs
9800 9825 9850 9875 9900 9925 9950 9975 10000
Sequence index
(a)0.5
0.00.5GRU outputs for last 200 indices, quasi-DEER vs Sequential
Quasi-DEER
Sequential
0 2000 4000 6000 8000 10000
Sequence index
(b)1
01e5
Difference between sequential and quasi-DEER outputsFigure 5: The accuracy of evaluating with parallelized methods (DEER and quasi-DEER) as op-
posed to sequential evaluation. The parallelized methods converge to the correct trace within nu-
merical precision. The hidden state size is D= 4and the sequence length is T= 10,000.
However, [not preallocating memory] is more prone to GPU memory fragmenta-
tion, meaning a JAX program that uses most of the available GPU memory may
OOM with preallocation disabled.
Because the specifications where the time profile runs stay within memory but the memory runs run
out of memory are likely very close to the 16GB threshold, our hypothesis is that this phenomenon
is a manifestation of this documented memory fragmentation.
B.1.1 Numerical Precision of DEER and Quasi-DEER
In Figure 5 we qualitatively show that for the same example used in Figure 3 of Lim et al. [36]
that quasi-DEER converges within numerical precision to the correct trace in the untrained GRU
benchmarking task discussed in Section 6.1. Similar results for DEER can be found in Section 4.1
of Lim et al. [36].
B.1.2 Different Scaling Regimes Depending on GPU Saturation
In Figure 6, we run the timing benchmarks of Section 6.1 on a wider range of sequence lengths T
and hidden state sizes D, on a larger GPU (a V100 with 32 GB) and with a smaller batch size of
1. In doing so, we highlight that the parallel nature of DEER and quasi-DEER, as their wall-clock
time scales sublinearly in the sequence length Tin smaller ( D,T) regimes. However, we note that
in the larger regimes considered in our main text and in Lim et al. [36], we often observe linear
scaling in the sequence length Tfor the wall-clock time of DEER and quasi-DEER, even though
these algorithms are still faster than sequential evaluation. Figure 6 shows good evidence that these
parallel algorithms are suffering from saturation of the GPU, and would benefit from even more
optimized parallel hardware
The parallel scan, given sufficiently many processors, scales as O(logT). As we show in Figure 6,
we see this speedup at low model sizes and sequence lengths. Once the processors are saturated, we
see a linear increase in the runtime (since the amount of work done is linear), but it is making much
more effective use of the GPU, resulting in a constant factor speedup over sequential application at
larger model sizes/sequence lengths.
B.1.3 Memory Profiling Details
As we discussed in Section 4.1, quasi-DEER is O(TD)in memory while DEER is O(TD2)in
memory because DEER uses dense Jacobians ∂f/∂swhile quasi-DEER uses a diagonal approxi-
mation, diag( ∂f/∂s). However, to implement quasi-DEER with automatic differentiation, the most
standard approach would be to compute the dense Jacobian, and then to take the diagonal; how-
ever, such an approach would still be O(TD2)in memory required. There are two implementation
1910−410−2100Wallclock (s)
D= 1
 D= 2Sequential DEER Quasi-DEER
D= 4
 D= 8
 D= 16
 D= 32
 D= 641K
3K
10K
30K
100K
300K
1M100101Memory (GB)
1K
3K
10K
30K
100K
300K
1M
1K
3K
10K
30K
100K
300K
1M
1K
3K
10K
30K
100K
300K
1M
1K
3K
10K
30K
100K
300K
1M
1K
3K
10K
30K
100K
300K
1M
1K
3K
10K
30K
100K
300K
1M
Sequence Length (T)Figure 6: Evaluating an untrained GRU. Sublinear and linear timing regimes for parallelized
algorithms. The above experiments were run on a 32 GB V100 with a batch size of 1. As in
Figure 2, we use 20 seeds for timing, 3 seeds for memory, and the dashed black line indicates the
memory capacity of the GPU (32 GB). We observe that in smaller regimes in DandTthat the
wall-clock time shows sublinear scaling indicative of the use of parallel algorithms. However, when
the GPU becomes saturated, the benefits of parallelization are reduced and we begin to see linear
scaling in wall-clock time with T.
workarounds. One is to loop over computing partial derivatives, effectively trading time for mem-
ory. The second is simply derive the diagonal entries of the Jacobian for the architecture of interest.
For the purpose of showcasing the O(TD)memory usage of quasi-DEER in Section 6.1, we take
this second approach, deriving the diagonal entries of the Jacobian of the GRU nonlinear dynamics
and implementing them in JAX. However, for our other experiments, where memory capacity is not
a problem, we simply use the less memory efficient version of quasi-DEER.
We also see linear memory scaling in evaluating the RNN sequentially. This behavior occurs because
we JIT compile a lax.scan in JAX, and we track the maximum memory used on the GPU at any
point in the computation. Because the inputs and the hidden states of the RNN scales are both
of length T, the memory usage of O(T). While there may be more memory efficient ways to
sequentially evaluate an RNN, we keep the same benchmarking structure as Lim et al. [36] for to
make comparison easier.
20B.2 Quasi-DEER for Training
Here we discuss the experimental details for Experiment 2 in Section 6.2. We follow the same
experimental set up as in Section 4.3 and Appendix B.3 of Lim et al. [36]. As an aside, we note
that the choice of hardware can impact behavior of the algorithms dramatically. For replicability, we
run on the same hardware as Lim et al. [36], using a 16GB V100 SXM2. However, we note that if
we try to run these same experiments on A100, DEER struggles to converge numerically, although
quasi-DEER shows no such difficulty. If we run on a CPU, both DEER and quasi-DEER converge
numerically. On balance, on the eigenworms time series classification task, both DEER and quasi-
DEER are numerically stable for the most part; the numerical instabilities we have observed for
DEER on an A100 are likely specific to some particular detail of JAX/hardware interaction.
B.3 ELK and Quasi-ELK for Evaluating Autoregressive RNNs
Here we discuss our experimental details for our Experiment 3, discussed in Section 6.3.
B.3.1 AR GRU Architecture
The architecture is a GRU with hidden states ht∈R3and scalar inputs xt∈R. However, at every
point in the sequence t,we readout the hidden state ht∈R3and use it to parameterize a mean
µt+1∈Rand a variance σ2
t+1∈R+. We then sample xt+1according to xt+1∼ N(µt+1, σ2
t+1);
thisoutput xt+1is then fed into as the input to the AR GRU at time step t+ 1 to make the new
hidden step ht+1.
This AR GRU is trained using standard sequential evaluation and backpropagation-through-time to
produce a noisy sine wave of length 10,000. We train the AR GRU on 1024 traces x1:Tgenerated
from a sine wave with amplitude 10 and white noise applied to each time step, and the training
objective is to minimize the the negative log probability of the x1:T.
Once the AR GRU has been trained, it can generate its own trace ˜x1:Tgiven an initial hidden state
h0and noises ϵ1:T.
We note that such a system is Markovian with dimension D= dim( h) + dim( x), as together the
hidden state htand output xt+1determine the next hidden state ht+1and output xt+2. Thus, in
the notation of Section 2, a hidden state stof the Markovian state space model is st= (xt+1,ht).
Therefore, we can apply fixed point methods to try to find the correct trace s∗in a parallelized
manner instead of autoregressively.
We note that one distinction of this set-up with respect to the notation developed in Section 2 is that
the dynamics functions fare effectively time-varying because the way in which xt+2is generated
from (xt+1, ht)depends on the noise ϵt+2, the value of which varies across the sequence. However,
all the results in the paper still apply after subsuming the input dependence into a time-varying
dynamics function ft.
B.3.2 Wall-clock Time Benchmark
The timing experiments were carried out as follows on an Nvidia A100 GPU. We ran sequential
evaluation of the trained AR GRU to produce noisy sine waves of length T= 10,000, as well as the
four parallelized method we consider in this paper (DEER, quasi-DEER, ELK, and quasi-ELK).
We ran 20 different random seeds (which lead to different values of the ϵ1:Tand therefore different
nonlinear dynamics), and timed each for a total of 4 repetitions (i.e. 80 timing runs per method). We
record the wall-clock time needed to evaluate the length Tsequence sequentially, as well as wall-
clock time, divided by Tneeded to run TNewton iterations of each of the parallelized methods
(thus, we obtain the time per Newton iteration for each of the parallelized methods).
Over these 80 timing runs, the sequential evaluation took an average of 96milliseconds, with stan-
dard deviation of 1.55ms. We report the average time per Newton iteration, the total number of
iterations needed for convergence, and the total wall-clock time to convergence in Table 2. Note that
the third column of Table 2 is the product of the first two columns.
21We effectively read the number of Newton iteration to convergence off of the graphs in Figure 4,
but find the number of Newton iterations to convergence to be quite stable across random seeds (see
Figure 7).
Table 2: Time to evaluate a length T= 10,000trained AR GRU using sequential vs parallelized
methods. We note the dynamax package [12] we used for the parallel Kalman filter implementation
in ELK is not optimized for speed, and hence these run times could be further improved.
Algorithm Time per New-
ton step (ms,
mean±std)Newton steps to
convergenceTotal time to
convergence
(ms)
Sequential Evaluation
Sequential N/A N/A 96
Parallelized Methods
DEER 0.282±0.0005 4449 1255
Quasi-DEER 0.087±0.0002 7383 642
ELK 3.600±0.0670 172 619
Quasi-ELK 0.141±0.0004 1566 221
These timing results are illustrative of multiple themes of our paper. We see that while the undamped
Newton steps are individually faster because they are carrying out fewer computations (they are
just computing a linear recurrence relation, or equivalently an undamped Newton step, instead of
computing a filtering pass, or equivalently solving a trust region problem). However, because the
undamped Newton methods are numerically unstable, they take dramatically more Newton steps to
convergence.
Similarly, we see that the quasi methods are dramatically faster than their dense counterparts as they
are replace O(D3)matrix-matrix multiplication with O(D)diagonal matrix multiplication. The
O(D3)work required by a parallel scan on a dense linear recurrence likely saturates the GPU). We
see in Table 2 that individual steps in the dense DEER/ELK are (approximately) a factor of between
3.5 and 30 times slower per step than their quasi (diagonal) variants. However, they take a factor of
between 2 and 10 fewer iterations.
Thus, we find that our fastest parallelized method on wall-clock time is quasi-ELK, but even so it is
approximately two times slower than sequentially evaluating this AR GRU. Therefore, an interest-
ing direction for future work would be to characterize regimes where parallel methods can outper-
form sequential methods, and to investigate whether this autoregressive setting is such a regime, or
whether parallelized methods can benefit from further speed-ups by leveraging adaptive trust region
sizes, clever initialization strategies, or even more modern parallelized hardware.
B.4 Setting the Hyperparameter for the AR GRU
We provide more details on how to set the hyperparameters for ELK. Figure 7 shows how to set the
hyperparameter for ELK in the context of the evaluating the AR GRU that generates a noisy sine
wave (Figure 4).
We sweep over the hyperparamter for 15 different input sequences, and plot the median and quartiles
of the cost to convergence in terms of Newton iterates and runtime (left column of Figure 7). We
see a bathtub curve: large λtakes needlessly small steps, slowing progress; small λresults in many
resets, slowing convergence. Crucially, we see there is little variance across individual sequences.
These results show that there is a well-behaved dependence that can be optimized on a validation set
with a simple 1-d grid search.
We also chart the approximation error against cost for the AR GRU (center and right column of
Figure 7). We see that the approximation error reduces in fewer Newton steps with full DEER as
opposed to quasi-DEER, but, crucially, the wallclock time (the more important of the two metrics)
is notably lower across all accuracies for quasi-DEER. This indicates that our more efficient – but
approximate – quasi-DEER is broadly preferable to the more expensive – but exact – DEER updates.
Furthermore, the stabilized ELK and quasi-ELK are better still. We also show the steps/time to
22convergence for a range of accuracy thresholds, and see that our methods outperform DEER across
the full range of thresholds and metrics.
These experiments were run on a single Nvidia A100 with 80GB of onboard memory.
10−310−1
λ102103104Newton steps
for MAD<0.001
0 5000 10000
Newton steps1091025MAD
10−310−210−1100
/epsilon1102103Newton steps
for MAD</epsilon1
10−310−1
λ100101Wallclock time (s)
for MAD<0.001
0.0 0.5 1.0 1.5
Wallclock time (s)1091025MAD
10−310−210−1100
/epsilon10.11.0Wallclock time
for MAD</epsilon1(s)DEER q-DEER ELK q-ELK
Figure 7: Experiment to show how to set the hyperparameters for (quasi)-ELK on the AR GRU pre-
trained to generate a noisy sine wave (Figure 4 in the main text). Top row plots Newton steps; bottom
row plots wallclock time. Lower is better for all plots. ( Left) median steps/time to convergence over
λover15sequences. Quartiles are shaded but are very small. DEER methods are independent of λ.
(Center ) Updated version of Figure 4 instead plotting MAD as a function of wallclock time. ( Right )
Time to convergence is robust as a function of convergence threshold ϵ. Median and quartiles across
15 sequences are shown. DEER methods are nearly constant at the thresholds considered (very
slight positive slope). Note we plot for increasing λcorresponding to a smaller trust region, and
reducing ϵcorresponding to a tighter convergence threshold.
B.5 Additional Experiment: Evaluating Chaotic Lorenz96 Systems
We include an extra experiment where we tackle the parallel evaluation of the classic non-linear 5-
dimensional Lorenz-96 system, with F= 8which results in chaotic dynamics. We seek to evaluate
this system (for T= 1000 timesteps) using (quasi)-DEER and (quasi)-ELK. We directly use the
Lorenz-96 dynamics as our nonlinear dynamics function f, i.e. the architecture/time evolution is
the Lorenz-96 ODE system. The state is the five-dimensional Lorenz system state. The input is
therefore the initial condition of the ODE; and the outputs are the T×5subsequent system states.
We demonstrate that all the parallelized methods converge to the correct trace, but that (quasi)-ELK
is dramatically more stable at intermediate Newton iterations prior to convergence. We see that
DEER and ELK methods converge in a comparable number of steps (this makes sense as DEER is
a special case of ELK for λ→0). DEER is faster (in terms of wallclock time) because of the extra
work done per ELK iteration. However, ELK has stabilized convergence, whereas DEER relies
heavily on resetting. Interestingly we see that quasi is slower by all metrics, suggesting that the
chaotic dynamics may require the more accurate updates. Quasi methods can be implemented to
consume notably lower memory, however, and so may be preferable in certain circumstances.
In Figure 8, we report mean absolute deviation (MAD) of the time series at Newton iteration (i)
against the true state sequence. “Iteration” then refers to the number of Newton iterations, i.e. the
number of updates applied to the entire state sequence. We set hyperparameters using 10 different
evaluations of the Lorenz96 (i.e. starting from 10 different initial points).
These experiments were run on a single Nvidia A100 with 80GB of onboard memory.
2310−310−210−1100
λ102103Newton steps
for MAD<0.1
0 500 1000
Newton steps1091025MAD
10−210−1100
/epsilon11001000Newton steps
for MAD</epsilon1
10−310−210−1100
λ100Wallclock time (s)
for MAD<0.1
0.0 0.5 1.0 1.5
Wallclock time (s)1091025MAD
10−210−1100
/epsilon10.11.0Wallclock time
for MAD</epsilon1(s)DEER q-DEER ELK q-ELK
Iteration 50
True trace DEER q-DEER ELK q-ELKIteration 100 Iteration 200 Iteration 500Figure 8: Evaluating the Lorenz96 system in parallel. (Top two rows) : Same format as Figure 7.
(Bottom row) : Plot of Lorenz96 trajectory during optimization. DEER methods are noticeably
more unstable than ELK methods.
B.6 Background on Parallel Scans
For a more detailed reference on parallel scans, the interested reader should refer to Appendix H of
Smith et al. [65] or to Blelloch [7].
In our codebase, we leverage jax.lax.associative_scan with the correct binary associative
operator. The binary associative operator for DEER and quasi-DEER is simply the composition of
affine maps, while the binary associative operation for Kalman filtering can be found in Särkkä and
García-Fernández [59] and in dynamax [12].
C Additional Background on Newton’s Method
In this appendix, we provide additional background on Newton’s method, and why it is of use for
parallelizing nonlinear RNNs.
Newton’s method provably enjoys quadratic (very fast) convergence in a basin near the true solution.
Moreover, as exhibited by the widespread usage of Newton’s method across many domains, New-
ton’s method can exhibit fast convergence in practice. However, a major motivation for this paper
is that globally, Newton’s method can be unstable and converge slowly. This instability is a major
motivation for our development of ELK.
A core insight from Lim et al. [36] is that in the setting of evaluating RNNs, Newton’s method can
be cast as a parallel scan (called DEER). At each “Newton iteration,” DEER linearizes the nonlinear
dynamics of the RNN it is evaluating. To the extent that linear approximations are a very powerful
tool across a wide variety of domains (e.g. Taylor expansions), this linear approximation can be
24a good approximation, leading to rapid convergence. For example, if we were dealing with linear
RNNs, DEER would converge in one Newton iteration. In this paper, we are instead dealing with
nonlinear RNNs, so more Newton iterations are required.
C.1 Newton’s Method for Root-Finding
We provide a brief discussion of Newton’s method for root finding. A great resource for further
study is Nocedal and Wright [49].
Let’s say we want to find the solution s∗to the nonlinear equation r(s) = 0 , and we have a guess
s(i)at iteration i. Newton’s method linearizes r(s)at the guess s(i), i.e.
ˆr(s) :=r(s(i)) +∂r
∂s(s(i))(s−s(i)).
Thus, we get our new guess s(i+1)as the solution to ˆr(s) = 0 . Therefore, s(i+1)satisfies
s(i+1)−s(i)=−J−1r(s(i)),
where we define J:=∂r
∂s.
C.2 Newton, Gauss-Newton, Root-Finding, and Optimization
In this paper, we seek to find the root of a nonlinear equation r(s) = 0 . In Appendix C.1 we discuss
how to use Newton’s method for root finding to obtain the update
s(i+1)←s(i)−J−1r.
However, another approach is to consider minimizing the merit function L(s) :=∥r(s)∥2
2/2. The
roots∗ofrwill also minimize L(s), so the goal of root-finding to solve r(s) = 0 is the same as
trying to find the minimize of L(s). However, if one applies Newton’s method for optimization to
try to minimize L(s)(see Boyd and Vandenberghe [8] for a great reference on Newton’s method and
optimization), the update obtained is actually
s(i+1)←s(i)−H(s(i))−1g(s(i)),
where His the Hessian of Landg=JTris the gradient of Lwith respect to s. The Gauss-Newton
method approximates this optimization update for minimizing the merit function by making the
approximation H≈JTJ, and so the Gauss-Newton update for minimizing the merit function ends
up being the same as the Newton update for the finding the root of r.
C.3 Convergence of Newton’s Method
Newton’s method only converges within a suitable basin [47, §1.2.4, p. 37], but establishing best
practices for initialization is an open problem. For instance, Yap provides a bound on the norm of the
basin [75, Lecture IV , §10, p. 174]. However, this definition requires bounding the derivative of the
objective function, which is harder than the original problem. Nesterov derives a basin for quadratic
convergence around the true solution [47, Thm 1.2.5, §1.2.4, p. 39], but does not provide information
on how to locate this basin a priori . Indeed, Nesterov defaults to taking standard gradient steps early
in optimization until you assume you are in the basin, and then using Newton steps [47, §1.2.4,
p. 39].
25NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The main claim in the abstract and introduction is that we improve the lim-
itations of parallelizing the evaluation of nonlinear RNNs with Newton’s method by ad-
dressing scalability and stability concerns. We develop quasi-DEER to address scalability
concerns and ELK to address stability concerns. We combine them in quasi-ELK.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims made
in the paper.
• The abstract and/or introduction should clearly state the claims made, including the con-
tributions made in the paper and important assumptions and limitations. A No or NA
answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See limitations.
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to vi-
olations of these assumptions (e.g., independence assumptions, noiseless settings, model
well-specification, asymptotic approximations only holding locally). The authors should
reflect on how these assumptions might be violated in practice and what the implications
would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was only
tested on a few datasets or with a few runs. In general, empirical results often depend on
implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be used
reliably to provide closed captions for online lectures because it fails to handle technical
jargon.
• The authors should discuss the computational efficiency of the proposed algorithms and
how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to address
problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an important
role in developing norms that preserve the integrity of the community. Reviewers will be
specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
26Justification: We prove Proposition 1.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if they
appear in the supplemental material, the authors are encouraged to provide a short proof
sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide experimental details to allow for reproducibility, including hard-
ware used, in Section 6 and Appendix B. We also provide our code at https://github.
com/lindermanlab/elk
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived well
by the reviewers: Making the paper reproducible is important, regardless of whether the
code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct the
dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case authors
are welcome to describe the particular way they provide for reproducibility. In the
case of closed-source models, it may be that access to the model is limited in some
way (e.g., to registered users), but it should be possible for other researchers to have
some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
27Answer: [Yes]
Justification: We provide our code at https://github.com/lindermanlab/elk
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
• The authors should provide instructions on data access and preparation, including how to
access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new pro-
posed method and baselines. If only a subset of experiments are reproducible, they should
state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized ver-
sions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide experimental details to allow for reproducibility, including hard-
ware used, in Section 6 and Appendix B. We provide out code https://github.com/
lindermanlab/elk .
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropri-
ate information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Yes, we provide standard deviations in Table 2. Sometimes we don’t provide
error bars when the runs are extremely similar (low variance).
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer "Yes" if the results are accompanied by error bars, confidence
intervals, or statistical significance tests, at least for the experiments that support the main
claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall run
with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula, call
to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error of
the mean.
28• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or fig-
ures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Yes, we are very careful to specify the type of hardware, including memory
capacity.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or
cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute than
the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t
make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have read and followd the NeurIPS code of ethics. We do not use human
subjects; we use publicly available datasets; and our research is in ways to accelerate stan-
dard machine learning algorithms so their broader impacts are to allow current machine
learning techniques to be more scalable and stable.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consider-
ation due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We discuss how we make machine learning more scalable. Such scalability
can lead to more energy efficient usage. Any negative impacts would occur from more
efficient machine learning being used for pernicious ends.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g.,
deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
29• The conference expects that many papers will be foundational research and not tied to
particular applications, let alone deployments. However, if there is a direct path to any
negative applications, the authors should point it out. For example, it is legitimate to point
out that an improvement in the quality of generative models could be used to generate
deepfakes for disinformation. On the other hand, it is not needed to point out that a
generic algorithm for optimizing neural networks could enable people to train models
that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is being
used as intended and functioning correctly, harms that could arise when the technology is
being used as intended but gives incorrect results, and harms following from (intentional
or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks, mecha-
nisms for monitoring misuse, mechanisms to monitor how a system learns from feedback
over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We have written an algorithms paper. We do not produce a pretrained lan-
guage model, an image generator, or a scraped dataset.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do not
require this, but we encourage authors to take this into account and make a best faith
effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We cite Lim et al. [36], whose work and code was an inspiration for this
paper, and Chang et al. [12], which we used to scaffold our implementation of ELK.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of the
derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to the
asset’s creators.
3013.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [Yes]
Justification: We provide our code at https://github.com/lindermanlab/elk
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license, limi-
tations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data col-
lector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: We do not use crowdsourcing nor research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
31