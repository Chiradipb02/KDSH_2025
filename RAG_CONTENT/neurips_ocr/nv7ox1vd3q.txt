Precise asymptotics of reweighted least-squares
algorithms for linear diagonal networks
Chiraag Kaushik
Electrical and Computer Engineering
Georgia Institute of Technology
Atlanta, GA 30308
ckaushik7@gatech.eduJustin Romberg
Electrical and Computer Engineering
Georgia Institute of Technology
Atlanta, GA 30308
jrom@ece.gatech.edu
Vidya Muthukumar
Electrical and Computer Engineering,
Industrial & Systems Engineering
Atlanta, GA 30308
vmuthukumar8@gatech.edu
Abstract
The classical iteratively reweighted least-squares (IRLS) algorithm aims to re-
cover an unknown signal from linear measurements by performing a sequence of
weighted least squares problems, where the weights are recursively updated at each
step. Varieties of this algorithm have been shown to achieve favorable empirical
performance and theoretical guarantees for sparse recovery and ℓp-norm mini-
mization. Recently, some preliminary connections have also been made between
IRLS and certain types of non-convex linear neural network architectures that are
observed to exploit low-dimensional structure in high-dimensional linear models.
In this work, we provide a unified asymptotic analysis for a family of algorithms
that encompasses IRLS, the recently proposed lin-RFM algorithm (which was
motivated by feature learning in neural networks), and the alternating minimization
algorithm on linear diagonal neural networks. Our analysis operates in a “batched”
setting with i.i.d. Gaussian covariates and shows that, with appropriately chosen
reweighting policy, the algorithm can achieve favorable performance in only a
handful of iterations. We also extend our results to the case of group-sparse re-
covery and show that leveraging this structure in the reweighting scheme provably
improves test error compared to coordinate-wise reweighting.
1 Introduction
Many high-dimensional machine learning and signal processing tasks rely on solving optimization
problems with regularizers that explicitly enforce certain structure on the learned parameters. The
traditional formulation for such tasks involves a regularized empirical risk minimization (ERM)
problem of the form
min
θ∈RdL(θ) +λR(θ), (1)
where L(·)is a loss function that encourages fidelity to the observed training data and R(·)encodes
desirable structural properties. In many important applications, it is desirable to obtain a sparsity-
seeking solution; in such cases, the regularizer is typically non-smooth, as in the LASSO, group
LASSO, and nuclear norm regularizers. As an alternative approach to this non-smooth optimization,
several recent works have proposed the “Hadamard over-parameterization” of θinto the entry-
wise product of two factors u⊙v. While the resulting minimization problem is non-convex, this
38th Conference on Neural Information Processing Systems (NeurIPS 2024).parameterization, coupled with a smooth regularizer, has been shown to achieve competitive empirical
performance (in terms of numerical stability, robustness, and convergence rate) when compared to
traditional sparse recovery algorithms [ 13,25]. For example, rather than solving the convex, but non-
smooth LASSO (where Lis the squared loss and Ris the ℓ1norm), the Hadamard reparameterization
yields the following non-convex and smooth formulation:
min
u,v∈RdL(u⊙v) +λ
2(∥u∥2
2+∥v∥2
2). (2)
In the case where the regression function is linear in θ, solving (2) is equivalent to learning a function
of the form
fu,v(x) =⟨x,u⊙v⟩=⟨diag(v)x,u⟩,
which can be thought of as a one hidden layer neural network with linear activation function and inner
weight matrix diag(v). In this context, this linear diagonal neural network (LDNN) architecture has
also been studied as an illustrative case study to improve our understanding of how neural networks
perform iterative “feature learning” to leverage low-dimensional structure in high-dimensional
settings [ 34,23]. We note here that, in the linear model case, feature learning is equivalent to learning
which subset of the input’s coordinates are relevant for the true predictor (i.e., feature selection).
One way to understand the connection between classical sparse recovery algorithms and the Hadamard
product/LDNN form in (2) is to consider the change of variable vi→√ηiandui→θi√ηi[25]. This
yields the following optimization problem, which is jointly convex in ηandθ:
min
θ∈Rdmin
η∈Rd
+L(θ) +λ
2dX
i=1θ2
i
ηi+ηi
. (3)
After solving the minimum over ηexplicitly, the second term becomes exactly λ∥θ∥1, and we recover
the Lasso objective. This is a special case of the so-called “eta-trick” [ 1], which can be used to write
many common sparsity-inducing penalties as the minimization of a quadratic functional of θ.
A variety of algorithms for learning Hadamard product parameterizations have recently been studied,
including alternating minimization [ 13], bi-level optimization [ 25], and joint gradient descent on
(u,v)[34]. The connection to the (θ,η)optimization in (3) can also be leveraged to construct
algorithms based on classical sparse recovery techniques. In particular, alternating minimization
overθandηin (3) yields the popular iteratively reweighted least-squares (IRLS) algorithm [ 11,9].
Translating these updates to the equivalent updates on (u,v), we obtain an iterative least-squares
algorithm for LDNNs, which alternately sets v(t+1)=p
|u(t)⊙v(t)|and performs a weighted least
squares update on u. This particular form of reparameterized IRLS was generalized in [ 28] to a
larger family of iterative least-squares algorithms under the name of linear recursive feature machines
(lin-RFM) .
While several methods for learning Hadamard/LDNN parameterizations have been introduced in the
literature, there remain many open questions about how they each perform and how they compare.
Theoretical analyses of these algorithms typically assume fixed, possibly worst-case training data,
and aim to characterize the properties of the fixed-points [ 28,34] or give convergence guarantees
to second-order stationary points [ 25]. However, these worst-case analyses do not readily yield
guarantees on the estimation error, which is the principal metric of interest. Indeed, many works have
shown that studying the average-case , or typical, behavior of non-convex optimization algorithms
can allow for estimation guarantees that are more precise and reflective of practice [14, 18, 7].
In this paper, we provide a precise analysis of a general family of iterative algorithms for learning
LDNNs that take the form
u(t+1)= arg min
u∈Rd1
ny(t)−1√
dX(t)(u⊙v(t))2
2+λ
d∥u∥2
2
v(t+1)=ψ(u(t+1),v(t)),
for some reweighting function ψand batches of training data (X(t),y(t)). As we show in Section 2,
this formulation encompasses multiple existing algorithms, including reparameterized IRLS, lin-RFM,
and alternating minimization over uandv. We consider the common scenario where training is
performed with batches of data and characterize the exact distribution of the parameters after each
iteration in the high-dimensional limit (n, d)→ ∞ . This allows us to address questions such as
2•How do different algorithm choices compare (in terms of convergence and signal recovery)
in the high-dimensional regime?
•How many iterations does it take common algorithms to find statistically favorable solutions?
•What is the effect of model architecture in LDNNs? Does leveraging group structure
provably improve sample complexity when the ground-truth signal is group-sparse?
Contributions: We define a general class of algorithms which learns LDNNs by alternately
performing least-squares and reweighting steps in a sample-split/batched setting, and we show the
following.
(1)Under mild assumptions on the target signal, initialization, and reweighting function, we provide
an exact characterization of the distribution of the entries of the parameters at each iteration in the
limit as n, dapproach infinity (Theorem 1).
(2)We show that this asymptotic result aligns well with numerical simulations and allows for accurate
prediction of the test error at each iteration. This enables rigorous comparison between different
algorithms and demonstrates that, with appropriate reweighting schemes, a statistically favorable
solution can be obtained in only a handful of iterations.
(3)Lastly, we extend our asymptotic framework to a setting of structured sparsity , where θ∗has group-
sparse structure (Theorem 2). Our results show that using a grouped Hadamard parameterization (i.e.,
tying together groups of weights in the LDNN) effectively learns such signals, with performance
scaling with the number of non-zero groups, rather than the total sparsity level.
1.1 Related work
IRLS and the η-trick: The reformulation of non-smooth regularizers in terms of quadratic variational
forms (the “ η-trick”) has been studied in various early works in computer vision and robust statistics
[12,4]. Further analysis and examples of sparsity-promoting norms are provided in [ 20,2], and
[25] provides a characterization of when a regularizer admits a variational form of this type. The
resultant optimization algorithm is iteratively-reweighted least-squares (IRLS), a popular technique
for compressive sensing and sparse recovery [ 11,9]. These works also consider IRLS algorithms
corresponding to ℓp-norm regularization for 0< p < 1; in this case, the minimization is no longer
convex, but [ 11] shows that such methods can find sparse solutions with fast local convergence rate.
The family of algorithms we consider includes a reparameterized version of each of these IRLS
algorithms, but unlike these prior works, we consider a batched setting and the high-dimensional
asymptotic regime. Moreover, our results apply to other algorithms which may not be easily expressed
as resulting from the η-trick.
Hadamard parameterization and linear diagonal networks: The reparameterization of θinto
the product of factors u⊙vhas been considered in a variety of recent works. The authors of
[31,35] show that early-stopped joint gradient descent over the two factors can lead to optimal
sample complexity for sparse linear regression. The equivalence of this parameterization to LDNNs
has also led to a surge of interest in the implicit bias of gradient descent/flow on this parameterization,
i.e., a characterization of which solution gradient descent will reach without explicit regularization
(corresponding to λ= 0). These works typically consider gradient flow run until completion and
characterize the solution as a minimizer of a certain sparsity-inducing functional that depends on the
initialization [34, 10, 23].
The connection between the LASSO (as well as some non-convex ℓqpenalties) and the Hadamard
parameterization was studied in [ 13], where alternating minimization over the two factors is used
instead of first-order methods. More recently, [ 25] extends these observations by making explicit the
connection to the η-trick and showing that saddle points are strict (escapable). These insights lead to
global convergence guarantees and a smooth bi-level optimization scheme [ 25,26] for non-smooth
structured optimization problems that was shown to perform competitively with state-of-the-art
solvers. The non-convex landscape of such formulations is further explored in [ 15], where it is
shown that for a large class of parameterizations (including grouped, deep, and fractional Hadamard
products), the non-convex problem has no spurious local minima. Motivated by the type of feature
learning observed in neural networks, the authors of [ 28] propose lin-RFM, which updates one of the
parameters via weighted least-squares while iteratively updating the other parameter via a reweighting
scheme based on the average gradient outer product of the learned function. The authors characterize
3properties of the fixed-points and show that, for certain reweighting schemes, lin-RFM is equivalent
to a reparameterization of IRLS. The family of algorithms we consider is similar, consisting of a
weighted least-squares step and a reweighting step; however, it is more general and doesn’t require
the reweighting function to have the particular form required by lin-RFM. Moreover, our asymptotic
characterization of the iterates allows for a precise understanding of how the test error evolves. On
the other hand, our analysis relies on batching/sample-splitting of training data while all of the above
works reuse the entire batch of training data at each iteration.
We make particular note here of the few works which explicitly consider a “grouped” Hadamard
parameterization, which we consider in Section 4. This corresponds to a LDNN with groups of tied
weights in the hidden layer. Early stopped gradient flow/descent for this type of architecture was
shown in [ 17] to achieve sample-complexity scaling with the number of non-zero groups (rather than
the overall sparsity). The non-convex landscape for this grouped architecture is studied in [ 36] and
[15]. Our results complement these works by studying group-reweighted least-squares algorithms
(rather than gradient methods) for learning functions of this form.
Precise characterization of higher-order non-convex optimization problems: On a technical level,
our work provides a precise deterministic characterization of a family of higher-order optimization
algorithms. In this sense, our results are of a similar flavor to [ 7], where Gaussian comparison
inequalities are used to obtain a precise characterization of non-convex optimization problems.
However, since the Hadamard parameterization is a re-parameterization of the actual estimator of
interest ( θ:=u⊙v), the results of [ 7] are not directly applicable. While our results are asymptotic
and do not provide finite-sample guarantees, we provide a distributional characterization of vafter
each reweighting step, which allows us to characterize the behavior of more complicated functions of
the iterates. Precise characterizations of alternating minimization and lin-prox methods for rank-1
matrix sensing are studied in the works [ 6,19]. While these works obtain non-asymptotic guarantees,
the estimation model and resulting optimization objective are quite different, with each unknown
parameter interacting with independent sensing vectors (rather than a single sensing vector interacting
with the product of the two parameters).
2 Background and formulation
Notation: The ones vector of dimension dis denoted as 1d. We denote the element-wise multiplica-
tion (Hadamard product) of two vectors xandyasx⊙y. Element-wise division of two vectors is
denoted asx
y. We say a function f:Rp→Rispseudo-Lipschitz of order 2if, for all x,y∈Rp,
|f(x)−f(y)| ≤C(1 +∥x∥2+∥y∥2)∥x−y∥2
for some constant C >0. The set of such functions is denoted by PL (2).
Convergence in probability of a sequence of random variables Xdto a random variable Xis denoted
byXdP→X. Convergence in Wasserstein-2 distance of a sequence of probability distributions
νdto a limiting distribution νis denoted as νdW2→ν, and this fact is equivalent to the statement
EX∼νdg(X)→EX∼νg(X)for all g∈PL(2) [3]. If the νdarerandom probability measures, we
say that νdW2→νif the same convergence holds in probability, i.e., EX∼νdg(X)P→EX∼νg(X)for
allg∈PL(2) . The empirical distribution of a vector z∈Rdis defined as1
dPd
i=1δ(zi), where δ(zi)
is the Dirac delta distribution centered at zi.
Formulation: We consider a batched noisy linear model where, at each time t= 0,1, . . . , T , a user
has access to an independent batch of data (X(t),y(t))∈Rn×d×Rnsatisfying
y(t)=1√
dX(t)θ∗+ϵ(t).
Above, θ∗∈Rdis an unknown signal, X(t)has i.i.d. standard Gaussian entries, and ϵ(t)∼
N(0, σ2In)is i.i.d. noise in the measurements. Given an initial weight vector v(0)∈Rd, we are
interested in the behavior of iterative algorithms of the form
u(t+1)= arg min
u∈Rd1
ny(t)−1√
dX(t)(u⊙v(t))2
2+λ
d∥u∥2
2
v(t+1)=ψ(u(t+1),v(t)),(4)
4Table 1: Some algorithms taking the form (4)
Algorithm Reweighting function
Alternating minimization (AM) [13] ψ(u, v) =u
Reparameterized IRLS [9, 11, 28] ψ(u, v) = (u2v2+ϵ)α
Linear recursive feature machines (lin-RFM) [28] ψ(u, v) =ϕ(u2v2)
where ψ:R×R→Ris a “reweighting” function that acts entry-wise on (u(t),v(t))andλ >0is a
hyperparameter governing the strength of the regularization. We we will study the behavior of the
iterates u(t),v(t)in the high-dimensional limit where nanddboth approach infinity with fixed ratio
d
n=κ. Since our primary interest is to reveal the feature learning capabilities of such algorithms
whenθ∗is a high-dimensional signal with low-dimensional structure, we will typically focus on the
regime where κ > T , where Tis the number of total iterations. This ensures that the total number of
observed samples nTis smaller than the ambient dimension d.
Before proceeding to our main results, we note that this formulation encompasses a wide variety of
classical and modern algorithms (summarized in Table 1):
•Alternating minimization: One perspective on this algorithm is to consider it as a way to
perform alternating minimization on the non-convex loss function
L(u,v) =1
ny−1√
dX(u⊙v)2
2+λ
d∥u∥2
2+λ
d∥v∥2
2.
Using the fact that the loss function is symmetric in uandv, choosing ψ(u, v) =urecovers
the mini-batched alternating minimization algorithm for this loss. In other words, ψsimply
switches the two parameters uandv.
•IRLS algorithms for sparse recovery: As shown in [ 28], classical IRLS reweighting
schemes used for sparse recovery and compressed sensing [ 11,21] can be reparameterized
in the form of (4) ψ(u, v) = ( u2v2+ϵ)α, where different choices of αcorrespond to
different ℓppenalties. In particular, the choice p= 2−4αcorresponds to the IRLS-p
algorithm of [21].
•Lin-RFM [ 28]:Generalizing the reparameterized IRLS update, the authors of [ 28] propose
the choice ψ(u, v) =ϕ(u2v2)for some continuous function ϕ:R→R+. Here, the quantity
u2v2arises from the average outer product of the gradient of the learned regression function,
which was shown empirically in [ 27] to correlate with the features learned in the weight
matrices of various common neural network architectures.
Our goal is to understand statistical properties of the iterates for different choices of ψ, and in
particular how the test error evolves from iteration to iteration. In the following section, we develop
an asymptotic characterization of the iterates that can be used to gain insight into these questions for
a large class of reweighting functions and problem settings.
3 A precise characterization of the iterates
In this section, we provide a precise characterization of the iterates of the algorithm in Equation 4
with i.i.d. Gaussian covariates. First, we introduce and discuss the two main assumptions needed for
our main result. The first assumption is concerned with the distribution of the initialization v0and
the target signal θ∗:
Assumption 1. The empirical distribution of the entries of v(0)andθ∗converges in W2distance to
some joint distribution Π0, i.e.,1
dPd
i=1δ(v(0)
i, θ∗
i)W2→Π0. Additionally, v(0)
i̸= 0for all iandθ∗
has bounded entries almost surely.
Here, the requirement of empirical distribution convergence is easily satisfied by common choices of
v(0), including the ones vector and i.i.d. Gaussian entries. For a typical sparse regression setup, we
might, for example, consider the Π0induced by choosing v(0)=1dand letting θ∗have i.i.d. entries
5that equal 0with certain probability. The requirement that θ∗has bounded entries appears to be an
artifact of the proof, and is used only in the proof of one technical lemma. In our simulations, we find
that our asymptotic predictions often remain accurate when θ∗has entries from distributions which
are not bounded almost surely (e.g., Gaussian entries).
Secondly, we define the set of reweighting functions ψfor which our result will apply.
Assumption 2. The reweighting function ψ:R×R→Rsatisfies the following:
1.IfU, V are random variables such that U, V̸= 0with probability 1, then ψ(U, V)̸= 0with
probability 1.
2.ψis continuous and bounded or ψ2is pseudo-Lipschitz of order 2.
This family allows us to consider many of the choices of ψdiscussed in the previous section,
including ψ(u, v) =u(AM on linear diagonal networks), ψ(u, v) =p
|uv|(lin-RFM and IRLS),
ψ(u, v) =ϕ(u2v2)for bounded ϕ(lin-RFM). We note that this does notinclude some choices of ψ
which apply more “aggressive” weighting, such as ψ(u, v) =|uv|. Nevertheless, we can apply our
theoretical predictions for these choices of ψafter passing the weights through a bounded activation
(such as a sigmoid). In Appendix D, we show that our predictions often still show excellent agreement
with empirical simulation even when the boundedness assumption is violated.
Our results are stated in terms of the following iteration, for t≥0:
τt+1, βt+1= arg max
τ≥0min
β≥0τσ2
β+τβ(1−κ)−τ2+τλE(V,Θ)∼ΠtΘ2+β2κ
τV2+βλ
Qt+1=τt+1V(Θ + βt+1√κGt)
τt+1V2+βt+1λ,
Πt+1=Law(ψ(Qt+1, V),Θ),(5)
where Gti.i.d.∼ N(0,1). In words, given a probability distribution ΠtoverR×R,τt+1andβt+1are
scalars computed as the unique1solutions to a deterministic optimization problem (this can be solved
easily by studying the optimality conditions, as shown in Appendix C). Then, Qt+1is defined as a
random variable that is a function of (V,Θ)∼ΠtandGt∼ N(0,1). Lastly, Πt+1is defined as the
joint distribution of ψ(Qt+1, V)andΘ.
Given this iteration, we obtain the following result, which is proved in Appendix A:
Theorem 1. Suppose Assumptions 1 and 2 are satisfied. Then, for any t≥0and any function
g:R3→Rsuch that g∈PL(2)orgis bounded and continuous, we have
1
ddX
i=1g(u(t+1)
i, v(t)
i, θ∗
i)P→E[g(Qt+1, V,Θ)],
where the expectation is over the independent random variables (V,Θ)∼ΠtandGt∼ N(0,1).
The limit in this theorem should be interpreted as being the limit as n, d→ ∞ with their ratio κ=d
nheld as a constant. Applying the above theorem for each t≥0, we can get precise asymptotic
predictions for a wide variety of test functions of the iterates. One example of particular interest
is the test error, which we measure as the normalized ℓ1distance between u(t+1)⊙v(t)andθ∗,
corresponding to g(u, v, θ ) =|uv−θ|(we provide a proof that this is PL(2) in Proposition 1
in Appendix B). We note that the limiting expectation can be computed via simple Monte Carlo
simulation of a scalar random variable.
From a technical standpoint, our result is obtained by applying the Convex Gaussian Min-Max
Theorem (CGMT) [ 30,29] to the weighted least-squares objective function in (4). Previous works
have obtained a similar distributional characterization for the solution to least-squares with anisotropic
covariates (where the “weights” vare the square root of the eigenvalues of the data covariance) [ 8].
However, while [ 8] assume that the eigenvalues are uniformly bounded by constants, this is not a
reasonable assumption in our setting, since many common choices of ψare not bounded and hence
1The uniqueness of the solution is shown in the proof of Theorem 1.
61 2 3 4 5 6 7 8
Iterations10−2ℓ1Test Errorψ=|uv|1
2(theory)
ψ=|uv|1
2(sim)
ψ= tanh |uv|(theory)
ψ= tanh |uv|(sim)
ψ=u(theory)
ψ=u(sim)
ψ= tanh u2(theory)
ψ= tanh u2(sim)
(a)σ= 0.11 2 3 4 5 6 7 8
Iterations10−410−310−2ℓ1Test Errorψ=|uv|1
2(theory)
ψ=|uv|1
2(sim)
ψ= tanh |uv|(theory)
ψ= tanh |uv|(sim)
ψ=u(theory)
ψ=u(sim)
ψ= tanh u2(theory)
ψ= tanh u2(sim)
(b)σ= 0.001
Figure 1: Theoretical predictions and simulations of the test error1
d∥u⊙v−θ∗∥1(log scale,
pluses denote the median over 100 trials and the shaded region indicates the interquartile range)
for two different noise levels, where n= 250 , d= 2000 , andθ∗hasBernoulli (0.01)entries. Here,
ψ=|uv|1
2corresponds to the classical IRLS weighting from [ 11],ψ= tanh |uv|is a version of
lin-RFM, ψ=ucorresponds to AM, and ψ= tanh u2is a new reweighting scheme we introduce.
We note that the ψwhich depend only on ucan lead to oscillatory behavior in the test risk.
v(t)is not necessarily bounded uniformly for t >1. A second key difference is that we need to
obtain a distributional characterization which can be applied recursively for all t≥0. The analysis in
[8] assumes convergence of the initialization in W4distance and proves convergence of the estimator
inW3distance. However, to apply the result recursively in our setting, if assume that the empirical
distribution of (v(0),θ∗)converges in Wkdistance, then we need to show that after one iteration, the
iterates also converge in Wkdistance (and not in any weaker sense).
To overcome these differences, we use a different technique to show distributional convergence of the
iterates. Similar to the approach in [ 5], we apply the CGMT to a perturbed optimization problem,
which ultimately allows us to show convergence of test functions of the solutions to the unperturbed
problem. While this approach necessitates the additional assumption that θ∗has bounded entries
and we obtain results for a slightly smaller family of test functions g(note, for example, that the
squared loss g(u, v, θ ) = ( uv−θ)2is not PL(2)), we obtain a distributional convergence result
that can be applied to a sequence of recursively defined least-squares problems which define the
trajectory of an algorithm, rather than to a single optimization problem. Moreover, our simulations
in Appendix D suggest that the predictions of Theorem 1 still often apply without these additional
assumptions, including in the case of the squared loss, indicating that these additional assumptions
could potentially be weakened with a more complicated analysis.
3.1 Application to sparse linear regression
In this subsection, we apply Theorem 1 to a sparse recovery setting and compare the asymptotic
predictions to numerical simulations on high-dimensional Gaussian data. First, we consider a setting
where n= 250 , d= 2000 , andθ∗hasBernoulli (0.01)entries (so the expected sparsity level is
E[s] = 20 ). We run Algorithm 4 with initialization v(0)=1dfor four different choices of reweighting
function and display the test error at each iteration (median over 100 trials) in Figure 1. For each
choice of reweighting function ψ, we choose the regularization parameter λthat minimizes the
asymptotic test loss achieved within 8 iterations, and we plot the corresponding trajectory. As shown
in the figure, the numerical simulations show excellent alignment with the asymptotic predictions
even for this moderate choice of nandd.
The asymptotic predictions show that this family of algorithms can find solutions with low test error
within only a few iterations. Our results also reveal fine-grained differences in the convergence
7behavior of the different algorithms. For instance, more aggressive weightings ψ= tanh |uv|
andψ= tanh u2seem to find better solutions after several iterations. Interestingly, the weighting
functions which depend only on u(like alternating minimization) sometimes display a non-monotonic,
oscillatory decay of the test loss, particularly in the low-noise regime. However, we do see a steady
decrease in test error after every pair of iterations (e.g., in AM, after both parameters have been
updated). Finally, we note that our framework allows for analysis of new algorithms for training
LDNNs. In particular, to our knowledge, weighting functions of the form ϕ(u2)have not been
previously considered for this task, but our results indicate that this small modification to AM is
competitive with many existing algorithms in this setting.
4 Grouped IRLS and the benefits of structured feature learning
In many scenarios, the unknown signal θ∗is known to possess additional structure that can be
leveraged during training. One commonly studied example of this is structured sparsity , or group
sparsity, where θ∗has many blocks which are zero. In this section, we generalize the results of
Theorem 1 to the case where the reweighting function respects this additional structure in the signal,
i.e.,ψacts on blocks of v, rather than on individual coordinates.
Concretely, we consider the following modification to our formulation. Let b≥1be a constant and
writeRdas a product space over M=d
bfactors: Rb× ··· × Rb. Then, θ∗,u(t),v(t)∈Rdcan all
be represented as Mstacked blocks, each in Rb. Under the same linear measurement model, we now
letψ:Rb×Rb→Rbact on each of the factors of(u(t),θ∗), and consider the same Algorithm 4.
Here, the case b= 1 recovers the results of the previous section, but the case b >1allows us to
study the interplay between signal structure and reweighting scheme in a more fine-grained way. For
example, suppose θ∗is known to be group-sparse, meaning that many of the factors {θ∗
i}M
i=1are
zero. In this case, it might make sense for ψto return a vector of the form
ψ(u(t)
i,v(t)
i) =αi1b,
for some αi∈Rthat is chosen as a function u(t)
iandv(t)
i. This corresponds to a reweighting
scheme which acts on blocks, rather than individual entries. Another way to motivate this “grouped
reweighting” is to leverage the connection to the η-trick, as in (3). In particular, the group Lasso
problem can be written in the following variational form [2]:
min
θ∈Rdmin
η∈RM
+L(θ) +λ
2MX
i=1∥θi∥2
2
ηi+ηi
, (6)
where the closed-form solution to the ηminimization yields the classical group norm regularizerPM
i=1∥θi∥2. Here, reparameterizing as αi→√ηiandui→θi√ηigives rise naturally to the grouped
Hadamard parameterization, with vi=αi1b.
From the perspective of linear diagonal networks, such an approach is equivalent to “tying” together
the weights of the hidden layer that correspond to each block. Rather than studying gradient
descent/flow for this parameterization (as in [ 16,17]), we consider an optimization approach that
relies on alternate updates of u(t)andv(t).
We make the same technical assumptions as in Assumptions 1 and 2, with the natural modifications
to accommodate b≥1:
1.Π0is the limit of the empirical distribution of factors of (v(0),θ∗)and hence is a distribution
overRb×Rb.
2. Each factor θ∗
i∈Rbfori= 1, . . . , M has bounded ℓ2-norm almost surely.
3.ψis bounded and continuous or each of its coordinate projections satisfies ψ2
j∈PL(2)for
j= 1, . . . , b .
81 2 3 4 5 6 7 8
Iterations10−2ℓ1Test ErrorGroup-blind (theory)
Group-blind (sim)
Group-aware (theory)
Group-aware (sim)
(a) Comparison of trajectory12 4 8 10
Ground truth group size0.0020.0040.0060.008ℓ1Test ErrorGroup-blind (theory)
Group-blind (sim)
Group-aware (theory)
Group-aware (sim)
(b) Effect of group size b
Figure 2: Group-blind ( ψgb) vs. group-aware ( ψga) reweighting when θ∗has group-sparse structure.
We set n= 500 , d= 4000 , σ= 0.1, andθ∗
ii.i.d.∼Bernoulli (0.01)1b. For each curve, λis set to
minimize the asymptotic test error achieved. Simulation results are the median/IQR over 100 trials.
Left: Comparison of the test error trajectory (log scale) for a fixed block size b= 8. Right: ℓ1test
error after T= 4iterations, for varying group sizes.
A straightforward extension of Theorem 1 yields the following generalization for the grouped
algorithm, where V,Θ,Gt,Qt+1∈Rbare now vector-valued random variables: For t≥0, let
τt+1, βt+1= arg max
τ≥0min
β≥0

τσ2
β+τβ(1−κ)−τ2+τλE(V,Θ)∼Πt
1
bbX
j=1Θ2
j+β2κ
τV2
j+βλ



Qt+1=τt+1V⊙(Θ+βt+1Gt√κ)
τt+1V⊙2+βt+1λ1b,(entry-wise division)
Πt+1=Law(ψ(Qt+1,V),Θ).(7)
Here,Gti.i.d.∼ N(0,Ib). Then, we have the following result, which is proved in Appendix A.
Theorem 2. [Generalization of Theorem 1 for b≥1] Under the assumptions above, for any t≥0
and any function g: (Rb)3→Rsuch that g∈PL(2)orgis bounded and continuous, we have
1
MMX
i=1g(u(t+1)
i,v(t)
i,θ∗
i)P→E[g(Qt+1,V,Θ)].
Given a reweighting function ψ, Theorem 2 characterizes the distribution of the factors (blocks) of
the iterates. Hence, by choosing g(u,v,θ) =|u⊙v−θ|, we can predict the exact limiting test
error for this family of algorithms.
Computing these theoretical predictions reveals that choosing ψin a group-aware way can lead to
significant performance improvements compared to coordinate-wise reweighting. In Figure 2, we
fixσ= 0.1, n= 500 , d= 4000 , and set the overall expected sparsity level of θ∗as in Figure 1. We
compare the performance of Algorithm 4 for a “group-blind” ( ψgb) and “group-aware” ( ψga) choice
of reweighting function:
•ψgb(u,v) = tanh |u⊙v|— note this is identical to one of the reweightings considered in
Section 3.1.
•ψga(u,v) =
1
bPb
j=1tanh|ujvj|
1b
The theoretical predictions align with simulations and show a notable improvement in performance
when using the group-aware scheme with b >1. Moreover, as the group size bincreases, the
performance of ψgbremains approximately the same, indicating that it is not able to take adapt to the
9group-structure. By contrast, using ψgaleads to a consistent improvement in test error as bgets larger.
Hence, the test error when using the group-aware scheme scales with the size/number of groups,
rather than the overall sparsity level.
5 Conclusion
In this paper, we derived a precise asymptotic characterization of the iterates of a family of algorithms
for learning high-dimensional linear models with linear diagonal networks. We used these predictions
to obtain fine-grained predictions of the test error at each iteration for various existing algorithms
for this task, and we showed that our framework can also be used as a test bed for new variations on
these algorithms that take a similar form. Lastly, we demonstrated the advantage of embedding more
structure into the model by tying together groups of weights when the ground-truth has structured
sparsity. Several interesting open questions about these types of algorithms remain. While our
simulations align very well with the predicted asymptotic trajectory, it would be interesting to obtain
finite-sample guarantees that hold even for batch sizes that are much smaller than d(as in the “mini-
batch” case studied by [ 19]). Moreover, seeing as our analysis depends crucially on the independence
the covariates at every iteration, developing precise predictions of the trajectory in the non-batched
setting remains an interesting direction for future work.
Acknowledgments and Disclosure of Funding
We thank the anonymous reviewers for their helpful feedback. This work was supported by an
NSF Graduate Research Fellowship (DGE-2039655), the NSF AI Institute AI4OPT, NSF grants
IIS-2212182, CCF-223915 and 2112533, and gifts from Amazon and Adobe.
References
[1]Francis Bach. The “ η-trick” or the effectiveness of reweighted least-squares.
https://francisbach.com/the-%CE%B7-trick-or-the-effectiveness-of-
reweighted-least-squares/ , 2019. Accessed: April 2024.
[2]Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, et al. Optimization with
sparsity-inducing penalties. Foundations and Trends ®in Machine Learning , 4(1):1–106, 2012.
[3]Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with
applications to compressed sensing. IEEE Transactions on Information Theory , 57(2):764–785,
2011.
[4]Michael J Black and Anand Rangarajan. On the unification of line processes, outlier rejection,
and robust statistics with applications in early vision. International Journal of Computer Vision ,
19(1):57–91, 1996.
[5]David Bosch, Ashkan Panahi, Ayca Ozcelikkale, and Devdatt Dubhashi. Random features
model with general convex regularization: A fine grained analysis with precise asymptotic
learning curves. In International Conference on Artificial Intelligence and Statistics , pages
11371–11414. PMLR, 2023.
[6]Kabir Aladin Chandrasekher, Mengqi Lou, and Ashwin Pananjady. Alternating minimization
for generalized rank one matrix sensing: Sharp predictions from a random initialization. arXiv
preprint arXiv:2207.09660 , 2022.
[7]Kabir Aladin Chandrasekher, Ashwin Pananjady, and Christos Thrampoulidis. Sharp global
convergence guarantees for iterative nonconvex optimization with random data. The Annals of
Statistics , 51(1):179–210, 2023.
[8]Xiangyu Chang, Yingcong Li, Samet Oymak, and Christos Thrampoulidis. Provable benefits of
overparameterization in model compression: From double descent to pruning neural networks.
InProceedings of the AAAI Conference on Artificial Intelligence , volume 35, pages 6974–6983,
2021.
10[9]Rick Chartrand and Wotao Yin. Iteratively reweighted algorithms for compressive sensing.
In2008 IEEE International Conference on Acoustics, Speech and Signal Processing , pages
3869–3872. IEEE, 2008.
[10] Hung-Hsu Chou, Johannes Maly, and Holger Rauhut. More is less: inducing sparsity via
overparameterization. Information and Inference: A Journal of the IMA , 12(3):1437–1460,
2023.
[11] Ingrid Daubechies, Ronald DeV ore, Massimo Fornasier, and C Sinan Güntürk. Iteratively
reweighted least squares minimization for sparse recovery. Communications on Pure and
Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences ,
63(1):1–38, 2010.
[12] Donald Geman and George Reynolds. Constrained restoration and the recovery of discon-
tinuities. IEEE Transactions on Pattern Analysis & Machine Intelligence , 14(03):367–383,
1992.
[13] Peter D Hoff. Lasso, fractional norm and structured sparse estimation using a Hadamard product
parametrization. Computational Statistics & Data Analysis , 115:186–198, 2017.
[14] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using
alternating minimization. In Proceedings of the 45th Annual ACM Symposium on Theory of
Computing , pages 665–674, 2013.
[15] Chris Kolb, Christian L Müller, Bernd Bischl, and David Rügamer. Smoothing the edges:
A general framework for smooth optimization in sparse regularization using Hadamard over-
parametrization. arXiv preprint arXiv:2307.03571 , 2023.
[16] Akshay Kumar, Akshay Malhotra, and Shahab Hamidi-Rad. Group sparsity via implicit
regularization for MIMO channel estimation. In 2023 IEEE Wireless Communications and
Networking Conference (WCNC) , pages 1–6. IEEE, 2023.
[17] Jiangyuan Li, Thanh V Nguyen, Chinmay Hegde, and Raymond KW Wong. Implicit regular-
ization for group sparsity. arXiv preprint arXiv:2301.12540 , 2023.
[18] Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing
data: Provable guarantees with non-convexity. Advances in Neural Information Processing
Systems , 24, 2011.
[19] Mengqi Lou, Kabir Aladin Verchand, and Ashwin Pananjady. Hyperparameter tuning via
trajectory predictions: Stochastic prox-linear methods in matrix sensing. arXiv preprint
arXiv:2402.01599 , 2024.
[20] Charles A Micchelli, Jean M Morales, and Massimiliano Pontil. Regularizers for structured
sparsity. Advances in Computational Mathematics , 38:455–489, 2013.
[21] Karthik Mohan and Maryam Fazel. Iterative reweighted algorithms for matrix rank minimization.
The Journal of Machine Learning Research , 13(1):3441–3473, 2012.
[22] Whitney K Newey and Daniel McFadden. Large sample estimation and hypothesis testing.
Handbook of Econometrics , 4:2111–2245, 1994.
[23] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal
linear networks: a provable benefit of stochasticity. Advances in Neural Information Processing
Systems , 34:29218–29230, 2021.
[24] David Pollard. Asymptotics for least absolute deviation regression estimators. Econometric
Theory , 7(2):186–199, 1991.
[25] Clarice Poon and Gabriel Peyré. Smooth bilevel programming for sparse regularization. Ad-
vances in Neural Information Processing Systems , 34:1543–1555, 2021.
[26] Clarice Poon and Gabriel Peyré. Smooth over-parameterized solvers for non-smooth structured
optimization. Mathematical Programming , 201(1):897–952, 2023.
11[27] Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin. Mecha-
nism for feature learning in neural networks and backpropagation-free machine learning models.
Science , 383(6690):1461–1467, Mar 2024.
[28] Adityanarayanan Radhakrishnan, Mikhail Belkin, and Dmitriy Drusvyatskiy. Linear recursive
feature machines provably recover low-rank matrices. arXiv preprint arXiv:2401.04553 , 2024.
[29] Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi. Precise error analysis of regularized
m-estimators in high dimensions. IEEE Transactions on Information Theory , 64(8):5592–5628,
2018.
[30] Christos Thrampoulidis, Samet Oymak, and Babak Hassibi. Regularized linear regression: A
precise analysis of the estimation error. In Conference on Learning Theory , pages 1683–1709.
PMLR, 2015.
[31] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal
sparse recovery. Advances in Neural Information Processing Systems , 32, 2019.
[32] Roman Vershynin. High-dimensional Probability: An Introduction with Applications in Data
Science , volume 47. Cambridge university press, 2018.
[33] Cédric Villani et al. Optimal Transport: Old and New , volume 338. Springer, 2009.
[34] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay
Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models.
InConference on Learning Theory , pages 3635–3673. PMLR, 2020.
[35] Peng Zhao, Yun Yang, and Qiao-Chu He. High-dimensional linear regression via implicit
regularization. Biometrika , 109(4):1033–1046, 2022.
[36] Liu Ziyin and Zihao Wang. spred: Solving L1penalty with SGD. In International Conference
on Machine Learning , pages 43407–43422. PMLR, 2023.
12A Proof of main results
In this section, we provide the proofs of our main results.
A.1 Notation and background
For convenience, we first restate the main notation that is used in our proofs.
Notation The ones vector of dimension dis denoted 1d. We write a≲bwhen a≤Cbfor
some sufficiently large constant C > 0which does not depend on d. We denote the element-wise
multiplication (Hadamard product) of two vectors xandyasx⊙y. Element-wise division of two
vectors is denoted asx
y. We use the shorthand (·)+= max( ·,0). A function f:Rp→Ris called
pseudo-Lipschitz of order 2if, for all x,y∈Rp,
|f(x)−f(y)| ≤C(1 +∥x∥2+∥y∥2)∥x−y∥2
for some constant C >0. The set of such functions is denoted PL(2).
Convergence in probability of a sequence of random variables Xdto a random variable Xis denoted
XdP→X. Convergence in Wasserstein-2 distance of a sequence of probability distributions νd
to a limiting distribution νis denoted as νdW2→ν, and this fact is equivalent to the statement
EX∼νdg(X)→EX∼νg(X)for all g∈PL(2) [3]. If the νdarerandom probability measures, we
say that νdW2→νif the same convergence holds in probability, i.e., EX∼νdg(X)P→EX∼νg(X)for
allg∈PL(2) . The empirical distribution of a vector z∈Rdis defined as1
dPd
i=1δ(zi), where δ(zi)
is the Dirac delta distribution centered at zi. For any random variable (or group of random variables)
X, we use the notation Law (X)to denote the probability distribution of X.
We also define two key quantities which appear in the analysis.
Definition 1. The Moreau envelope function of a lower semi-continuous, proper convex function
ℓ:Rp→Rwith step size τis defined as
Mℓ(x;τ) = min
y∈Rpℓ(y) +1
2τ∥y−x∥2
2.
The proximal (prox) operator of ℓwith step size τ, denoted proxℓ(x, τ)is defined as the arg min of
the above optimization problem.
Lastly, we restate the version of the Convex Gaussian Min-Max Theorem (CGMT) that we will use
in our proofs.
Theorem 3 (Convex Gaussian Min-Max Theorem [ 29]).LetG∈Rm×n,g∈Rm,h∈Rnhave
i.i.d.N(0,1)entries. Let Sw⊂RnandSu⊂Rmbe compact, convex sets, and f:Rn×Rm→R
be convex-concave on Sw× Su. Define the following two min-max problems:
Φ(G):= min
w∈Swmax
u∈Suu⊤Gw+f(w,u)
ϕ(g,h):= min
w∈Swmax
u∈Su∥w∥2u⊤g+∥u∥2w⊤h+f(w,u)
Then, for all c∈Randt >0,
P{|Φ(G)−c|> t} ≤2P{|ϕ(g,h)−c|> t}
A.2 Proof of Theorems 1 and 2
Proof of Theorem 1. Assume that1
dPd
i=1δ(v(t)
i, θ∗
i)W2→Πt(note that this holds by assumption at
t= 0; we will show later that it holds at time t+ 1).
First observe that convergence of the joint empirical distribution of (u(t+1),v(t),θ∗)to the joint
distribution of (Qt+1, V,Θ)in Wasserstein-2 distance implies that
1
ddX
i=1g(u(t+1)
i, v(t)
i, θ∗
i)P→E[g(Qt+1, V,Θ)],
13for any g∈PL(2)or which is bounded and continuous. This is because W2convergence implies
convergence in expectation of any pseudo-Lipschitz function of order 2 [ 3, Lemma 5] and of any
bounded continuous function (since W2convergence is stronger than weak convergence [ 33, Theorem
6.9]). Hence, it suffices to show that
1
ddX
i=1δ(u(t+1)
i, v(t)
i, θ∗
i)W2→Law(Qt+1, V,Θ), (8)
where (V,Θ)∼ΠtandQt+1is defined as in Eq. 5.
Recall that the objective function for the update on uis given by
u(t+1)= arg min
u∈Rd1
ny(t)−1√
dX(t)(u⊙v(t))2
2+λ
d∥u∥2
2.
Rather than study this update directly, we first analyze a slightly more general problem (following the
approach in [ 5]). Let h:R3→Rbe a continuous test function with ∥∇2h∥2≤Cand that satisfies
one of the following:
1.his uniformly bounded.
2.h(u, v, θ ) =u2.
Then we consider the following problem (the dependence of X,y,ϵ, andvontis dropped to
simplify the notation):
P1(µ) = min
u∈Rd1
ny−1√
dX(u⊙v)2
2+λ
d∥u∥2
2+µ
ddX
i=1h(ui, vi, θ∗
i), (9)
where µ∈[−µ∗, µ∗]andµ∗=λ
Cis chosen sufficiently small so that the objective function (scaled
byd) isλ-strongly convex for all µin this range. The case µ= 0recovers the original problem of
interest.
Step 1: Convergence of the loss Rewriting this in terms of the error vector ∆:=1√
d(u⊙v−θ∗),
we have
P1(µ) = min
∆∈Rd1
n∥ϵ−X∆∥2
2+λ
d√
d∆+θ∗
v2
2+µ
ddX
i=1h √
d∆i+θ∗
i
vi, vi, θ∗
i!
.
In writing this, we use the fact that vi̸= 0for all iwith probability 1(and the notation in the second-
to-last term indicates entry-wise division). Now, using the identity ∥·∥2
2= max q2q⊤(·)− ∥q∥2
2, we
can write this as
P1(µ) = min
∆∈Rdmax
q∈Rn2√nq⊤ϵ−2√nq⊤X∆− ∥q∥2
2+λ
d√
d∆+θ∗
v2
2+µ
ddX
i=1h √
d∆i+θ∗
i
vi, vi, θ∗
i!
.
(10)
Next, in Lemma 1, we show that there exist Euclidean balls B∆andBq, each of radius C1∥v∥∞
such that, with probability approaching 1, we can constrain the feasible set to lie with these balls
without changing the value of P1(µ), so we can study
˜P1(µ) = min
∆∈B∆max
q∈Bq2√nq⊤ϵ−2√nq⊤X∆− ∥q∥2
2+λ
d√
d∆+θ∗
v2
2+µ
ddX
i=1h √
d∆i+θ∗
i
vi, vi, θ∗
i!
,
(11)
where P1(µ) =˜P1(µ)with probability tending to 1. We can therefore condition on this event for the
remainder of the analysis without changing our asymptotic conclusions.
14Now, noting that this is in the correct form to apply Theorem 3, we define the auxiliary optimization
problem
P2(µ) = min
∆∈B∆max
q∈Bq2√nq⊤ϵ−2√n∥q∥2g⊤∆−2√n∥∆∥2h⊤q− ∥q∥2
2+λ
d√
d∆+θ∗
v2
2
(12)
+µ
ddX
i=1h √
d∆i+θ∗
i
vi, vi, θ∗
i!
,
(13)
where g∈Rdandh∈Rnhave i.i.d. standard normal entries. By Theorem 3, for all δ >0and fixed
¯P(µ)∈R,
P{|P1(µ)−¯P(µ)|> δ} ≤2P{|P2(µ)−¯P(µ)|> δ}.
In particular, if we can find some ¯P(µ)such that P2(µ)P→¯P(µ), then we can conclude also that
P1(µ)P→¯P(µ).
To accomplish this, we next perform a series of simplifications to P2(µ)which will later help us
characterize its asymptotic behavior. First, we can decouple the optimization over qinto its norm and
direction, and the latter can be solved explicitly. Letting τ=∥q∥2, this yields
P2(µ) = min
∆∈B∆max
0≤τ≤R2τ√n∥ϵ− ∥∆∥2h∥2−2τ√ng⊤∆−τ2+λ
d√
d∆+θ∗
v2
2
+µ
ddX
i=1h √
d∆i+θ∗
i
vi, vi, θ∗
i!
,(14)
where R:=C1∥v∥∞. Next, note that ϵandhare independent Gaussian vectors and hence ϵ−
∥∆∥2hd=p
σ2+∥∆∥2
2h. So, we have
P2(µ)d= min
∆∈B∆max
0≤τ≤R2τ∥h∥2√nq
σ2+∥∆∥2
2−2τ√ng⊤∆−τ2+λ
d√
d∆+θ∗
v2
2
+µ
ddX
i=1h √
d∆i+θ∗
i
vi, vi, θ∗
i!
.
(15)
Before proceeding further, we rewrite this in terms of a minimization over the variable u=√
d∆+θ∗
v.
P2(µ)d= min
u∈Bumax
0≤τ≤R2τ∥h∥2√nr
σ2+1
d∥u⊙v−θ∗∥2
2−2τ√
ndg⊤(u⊙v−θ∗)
−τ2+λ
d∥u∥2
2+µ
ddX
i=1h(ui, vi, θ∗
i)
(16)
Here, Bu:=n
u∈Rd:1√
d(u⊙v−θ∗)
2≤Ro
. After this step, observe that the objective
function is strongly concave in τand strongly convex in u(the sum of the last two terms is strongly
convex in ubased on the assumption that hhas bounded Hessian and µis sufficiently small). Since
the objective function is convex-concave over convex and compact sets, we can invoke Sion’s minimax
theorem to switch the min andmax. Furthermore, we use the fact that√x= min β>0x
2β+β
2to
write this as
P2(µ)d= max
0≤τ≤Rmin
u∈Bu,
σ≤β≤σ+Rτ∥h∥2√n 
σ2
β+∥u⊙v−θ∗∥2
2
βd+β!
−2τ√
ndg⊤(u⊙v−θ∗)
−τ2+λ
d∥u∥2
2+µ
ddX
i=1h(ui, vi, θ∗
i).
(17)
15Here, note that we can add the constraint on βwithout changing the solution since the optimal value
ofβwill be obtained atq
σ2+1
d∥u⊙v−θ∗∥2
2∈[σ, σ+R]for all feasible u.
Next, we can explicitly solve the inner minimization over u. To do this, we first show in Lemma 2
that the optimal solution to the unconstrained minimization is strictly feasible for large enough C1,
and hence, the unconstrained and constrained minimizations over uare equivalent. Next, observe that
the unconstrained problem is separable over the indices, so we need only to solve the scalar problem
min
ui∈Rτ∥h∥2(uivi−θ∗
i)2
βd√n−2τ√
ndgi(uivi−θ∗
i) +λ
du2
i+µ
dh(ui, vi, θ∗
i) (18)
Completing the squares, we obtain that the above problem can be written in terms of the Moreau
envelope (Definition 1) of the function ℓ(u) =λu2+µh:
1
d"
τξθ∗2
i
β−τ
βξ 
βgi√κ−ξθ∗
i2+Mλ(·)2+µh(·,vi,θ∗
i) 
ξθ∗
i−βgi√κ
ξvi;β
2ξτv(t)2
i!#
,
where we have introduced the shorthand notation ξ=∥h∥2√n. Substituting this into the expression for
P2above, we obtain
P2(µ)d= max
0≤τ≤Rmin
σ≤β≤σ+Rτσ2ξ
β+τβξ−τ2+1
ddX
i=1τξθ∗2
i
β−τ
βξ 
βgi√κ−ξθ∗
i2
+1
ddX
i=1"
Mλ(·)2+µh(·,vi,θ∗
i) 
ξθ∗
i−βgi√κ
ξvi;β
2ξτv(t)2
i!#
:= max
0≤τ≤Rmin
σ≤β≤σ+Rfd(τ, β)
(19)
Now that the optimization has been fully “scalarized”, we proceed by considering its asymptotic
behavior. First, note that the partial minimization over upreserves the concavity/convexity in (τ, β).
In Lemma 3, we prove that for any fixed τandβ, the objective function fd(τ, β)converges in
probability to
f(τ, β) =τσ2
β+τβ(1−κ)−τ2+E
Mλ(·)2+µh(·,V,Θ)Θ−βG√κ
V;β
2τV2
, (20)
where the expectation is over (V,Θ)∼Πtand an independent G∼ N(0,1). We note here that
Lemma 3 is the only place in our proof which requires the boundedness of the entries of θ∗.
Since fd(τ, β)is strongly concave in τwith parameter 1for all feasible β, we can conclude that
f(τ, β)is also strongly concave in τwith parameter 1. Directly taking a derivative with respect to β,
we also find that fhas a single non-negative critical point, at the point ˆβ=p
σ2+E[(ˆuV−Θ)2],
where
ˆu= ˆu(V,Θ) = proxλ(·)2+µh(·,V,Θ)Θ−βG√κ
V;β
2τV2
.
So, we can conclude that fhas unique saddle point (ˆτ,ˆβ). Note fis a deterministic function that
does not depend on dand hence (ˆτ,ˆβ)are also deterministic and independent of d.
Now let C2:= max {ˆτ,ˆβ}+ 1. By the “convexity lemma” (as stated in [ 24]), pointwise convergence
(in probability) of a convex function is uniform over compact sets. So, this result implies that the
convergence is uniform over (τ, β)∈[0, C2]×[0, C2],2so
max
0≤τ≤C2min
0≤β≤C2fd(τ, β)P→max
0≤τ≤C2min
0≤β≤C2f(τ, β)
Let(ˆτd,ˆβd)denote the optimnal solution for the problem on the left. We can also conclude that
(ˆτd,ˆβd)P→(ˆτ,ˆβ)by [22, Theorem 2.1], which states that uniform convergence in probability of
2Note we could not have directly applied this to the feasible sets of P2(µ), since Rmay have a dependence
ond.
16a convex function over a compact set implies convergence of the optimal minimizer. So, with
probability approaching 1, (ˆτd,ˆβd)are strictly smaller than C2, and the same solution is also optimal
forP2(µ). We can therefore conclude
P2(µ)P→max
0≤τ≤C2min
0≤β≤C2f(τ, β) = max
τ≥0min
β≥0f(τ, β) =: ¯P(µ)
Therefore, by Theorem 3, for any fixed µ∈[−µ∗, µ∗], we have the convergence
P1(µ)P→¯P(µ).
In the special case µ= 0, we can further simplify the Moreau envelope term to obtain
¯P(0):= max
τ≥0min
β≥0τσ2
β+τβ(1−κ)−τ2+τλEΘ2+β2κ
τV2+βλ
. (21)
Step 2: Convergence of the optimal solution We next need to extend this result to the desired
Wasserstein-2 convergence result (8). Recall here that u(t+1)is the solution of P1(0).
First, let h:R3→Rbe any bounded, Lipschitz function, and let h(k)be a sequence of bounded,
twice-differentiable functions that converge uniformly to hask→ ∞ (e.g., the convolution of h
with a sequence of mollifiers). Let P(k)
1(µ),¯P(k)(µ)be the optimal cost of P1and¯Pwhen using
test function h(k)and for µ∈[−µ∗, µ∗]. Note the convergence P(k)
1(µ)P→¯P(k)(µ)for any µin a
sufficiently small neighborhood around zero holds by Step 1.
By the uniform convergence of the h(k)toh,
lim
k→∞P(k)
1(µ) =P1(µ)
lim
k→∞¯P(k)(µ) =¯P(µ).
Now, fix δ >0and choose klarge enough that |P(k)
1(µ)−P1(µ)|<δ
3and|¯P(k)(µ)−¯P(µ)|<δ
3.
Then,
P
|P1(µ)−¯P(µ)|> δ	
≤Pn
|P(k)
1(µ)−¯P(k)(µ)|> δ/3o
→0,
since P(k)
1P→¯P2(k)for all k. Hence, we can also apply the result of Step 1 to any bounded Lipschitz
function h.
Since the convergence result of Step 1 holds for any µin a neighborhood around zero, we can
conclude that
1
ddX
i=1h(u(t+1)
i, vi, θ∗
i)P→d¯P(µ)
dµ
µ=0,
where the derivative is well-defined since ¯Phas a unique solution in a neighborhood around zero.
The proof of this fact is identical to that of Lemma 7 of [ 5], so we omit it here. Moreover, using
the Dominated Convergence Theorem to differentiate inside the expectation, we can compute this
exactly:
d¯P(µ)
dµ
µ=0=Eh 
proxλ(·)2 
Θ−ˆβG√κ
V;ˆβ
2ˆτV2!
, V,Θ!
=Eh 
ˆτV(Θ−ˆβG√κ)
ˆτV2+ˆβλ, V,Θ!
,
where (ˆβ,ˆτ)are found in the optimal solution to ¯P(0).
Hence, for all bounded, Lipschitz h, we have
1
ddX
i=1h(u(t+1)
i, vi, θ∗
i)P→Eh 
ˆτV(Θ−ˆβG√κ)
ˆτV2+ˆβλ, V,Θ!
,
so the empirical distribution of the triple (u(t+1)
i, vi, θ∗
i)converges weakly to the distribution of
the random variable (ˆτV(Θ−ˆβG√κ)
ˆτV2+ˆβλ, V,Θ), where G∼ N (0,1)and(V,Θ)∼Πt. By choosing
h(u, v, θ ) = u2, we also know that second moments of the empirical distribution converge in
probability. Hence, the convergence can be strengthened from weak convergence to convergence in
W2distance (see, e.g. [33, Theorem 6.9]).
17Step 3: Verifying the inductive hypothesis Lastly, we need to show that
1
ddX
i=1δ(v(t+1)
i, θ∗
i)W2→Law(ψ(Qt+1, V),Θ):= Π t+1.
where (V,Θ)∼Πt. Here, weak convergence follows from the result of Step 2 since ψis a continuous
map. To show convergence of second moments, we need to show
1
ddX
i=1v(t+1),2
i =1
ddX
i=1ψ(u(t+1)
i, v(t)
i)2P→E[ψ(Qt+1, V)2].
Forψthat satisfies Assumption 2, this convergence is immediate from the result of Step 2 (since
W2convergence implies convergence in expectation of bounded continuous and PL(2) functions).
Therefore, the initial inductive assumption made at the beginning of this proof holds at time t+ 1,
and we can apply the result inductively to conclude Theorem 1.
Proof of Theorem 2. The proof is an extension of the proof of Theorem 1 to the case where the test
function hacts on blocks rather than individual entries. Much of the proof is identical, so we only
sketch the argument and highlight the major differences here. We begin with the inductive hypothesis
that1
MPM
i=1δ(v(t)
i,θ∗
i)W2→Πt, for a known distribution ΠtoverRb×Rb. Recall here that M
denotes the number of blocks/factors of size b(soM=d
b).
Then, let h: (Rb)3→Rbe a test function with ∥∇2h∥2≤Cand such that either his bounded or
h(ui,vi,θi) =∥ui∥2
2. We consider a similar perturbed optimization problem:
P1(µ) = min
u∈Rd1
ny−1√
dX(u⊙v)2
2+λ
d∥u∥2
2+µ
MMX
i=1h(ui,vi,θ∗
i). (22)
Again, we consider this for |µ| ≤λ
bC, so that the optimization problem isλ
dstrongly convex in u.
Noting that the proof of Lemma 1 still holds in this grouped case, we can constrain P1(µ)to be over
compact sets and apply the CGMT to obtain the auxiliary problem
P2(µ) = min
∆∈B∆max
q∈Bq2√nq⊤ϵ−2√n∥q∥2g⊤∆−2√n∥∆∥2h⊤q− ∥q∥2
2+λ
d√
d∆+θ∗
v2
2
(23)
+µ
MMX
i=1h(ui,v(t)
i,θ∗
i).(24)
The sequence of “scalarization” steps on P2is identical to in Theorem 1, until we arrive at
P2(µ)d= max
0≤τ≤Rmin
u∈Bu,
σ≤β≤σ+Rτ∥h∥2√n 
σ2
β+∥u⊙v−θ∗∥2
2
βd+β!
−2τ√
ndg⊤(u⊙v−θ∗)
−τ2+λ
d∥u∥2
2+µ
MMX
i=1h(ui,v(t)
i,θ∗
i).
(25)
Here, since the sum of the last two terms in the objective function isλ
dstrongly convex by our
choice of µ, the proof of Lemma 2 holds without change and we can consider the unconstrained
minimization over u. In this case, the minimization is block-separable over each of the Mfactors of
u, so it can be expressed as
1
dMX
i=1min
ui∈Rbτξ
β∥ui⊙vi−θ∗
i∥2
2−2τ√κg⊤
i(ui⊙vi−θ) +λ∥u∥2
2+µbh(ui,vi,θ∗
i)
=1
bMMX
i=1min
ui∈Rbτξ
β∥ui⊙vi−θ∗
i∥2
2−2τ√κg⊤
i(ui⊙vi−θ) +λ∥u∥2
2+µbh(ui,vi,θ∗
i)
:=1
bMMX
i=1q(vi,θ∗
i,gi),
18where gi∈Rbdenotes the ith block of gandq:= (Rb)3→Ris defined as a shorthand for the
quantity inside the summation.
Next, we consider the asymptotic behavior of P2(µ). Here, the only term which is different than in
Theorem 1 is the term1
bMPM
i=1q(vi,θ∗
i,gi). By the same argument as in Lemma 3, we can write q
as the Moreau envelope of a convex function and show that
1
bMMX
i=1q(vi,θ∗
i,gi)P→E1
bq(V,Θ,G),
where the expectation is over (V,Θ)∼ΠtandG∼ N(0,Ib). After the same uniform convergence
argument as in the proof of Theorem 1, we can conclude that for all µ∈[−µ∗, µ∗],P1(µ)P→¯P(µ),
where
¯P(µ) = max
τ≥0min
β≥0τσ2
β+τβ−τ2+E1
bq(V,Θ,G).
In particular, when µ= 0, the minimization implicit in the definition of qcan be solved exactly;
this yields exactly the optimization problem in 7. Step 2 of the proof (convergence of test functions
of the optimal minimizer) is identical to that of Theorem 1, and for the final step (showing the
inductive hypothesis holds at the next iteration), we need to argue that the second moment of
v(t+1)=ψ(u(t+1)
i,vi)converges to its expectation under Πt+1:
1
MMX
i=1∥ψ(u(t+1)
i,vi)∥2
2P→E∥ψ(Qt+1,V)∥2
2
Ifψis bounded and continuous or has PL(2)coordinate projections (as we have assumed), then
the above convergence holds based on the Wasserstein-2 convergence of the joint distribution of
(u(t+1),v).
B Technical lemmas
Proposition 1. The function g(u, v, θ ) =|uv−θ|is pseudo-Lipschitz of order 2.
Proof. The result follows from the following series of inequalities:
||uv−θ| − |u′v′−θ′|| ≤ |uv−θ−(u′v′−θ′)|
≤ |uv−u′v′|+|θ−θ′|
≤ |u||v−v′|+|v′||u−u′|+|θ−θ′|
≤(|u|+|v′|+ 1)(|u−u′|+|v−v′|+|θ−θ′|)
≤(1 +∥x∥1+∥x′∥1)∥x−x′∥1
≤3(1 +∥x∥2+∥x′∥2)∥x−x′∥2,
where x,x′∈R3denote (u, v, θ )and(u′, v′, θ′), respectively.
Lemma 1. Let∆∗,q∗be the optimal solution to (10). Then, there exists universal constant C1>0
such that
lim
d→∞P{∥∆∗∥2≤C1∥v∥∞}= lim
d→∞P{∥q∗∥2≤C1∥v∥∞}= 1.
Proof. We proceed via a similar argument to Lemma 2 in [ 5]. First, consider the original expression
forP1(µ)from (9) :
P1(µ) = min
u∈RdF(u) +R(u),
where F(u):=1
ny−1√
dX(u⊙v)2
2andR(u):=λ
d∥u∥2
2+µ
dPd
i=1h(ui, v(t)
i, θ∗
i). Recall here
thatRisλ
d-strongly convex for all µ∈[−µ∗, µ∗], and denote the unique optimal minimizer to this
19problem as u∗. Then, the following chain of inequalities holds, by the optimality of u∗and the
non-negativity of F.
1
n∥y∥2
2+R(0) =F(0) +R(0)≥F(u∗) +R(u∗)≥R(u∗).
Moreover, by the strong convexity of R, we have
R(u∗)≥R(0) +∇R(0)⊤u∗+λ
2d∥u∗∥2
2.
Combining the above two series of inequalities, we obtain (recall κ=d
n)
∥u∗∥2
2+2d
λ∇R(0)⊤u∗≤2κ
λ∥y∥2
2.
After completing the square, this yields
u∗+d
λ∇R(0)2
2≤2κ
λ∥y∥2
2+d2
λ2∥∇R(0)∥2
2,
whence, by the triangle inequality,
∥u∗∥2≤d
λ∥∇R(0)∥2+r
2κ
λ∥y∥2
2+d2
λ2∥∇R(0)∥2
2
≤2d
λ∥∇R(0)∥2+r
2κ
λ∥y∥2.
Here, standard concentration inequalities for Gaussian random variables (e.g., [ 32, Theorem 5.2.2,
Corollary 7.3.3]) imply that, with probability approaching 1, ∥X∥2≲√
dand∥ϵ∥2≲√
d. And
Assumption 1 implies that ∥θ∗∥2≲√
dwith probability tending to 1. So,
∥y∥2≤µ√
d∥X∥∥θ∗∥2+∥ϵ∥2≲√
d
with probability approaching 1. Next, we bound ∥∇R(0)∥2. Recalling the definition of R,
∥∇R(0)∥2=µ
dvuutdX
i=1∂
∂uh(u, vi, θ∗
i)
u=02
=1√
dvuut1
ddX
i=1∂
∂uh(u, vi, θ∗
i)
u=02
.
Since the function g(v, θ) =∂
∂uh(u, v, θ )
u=0is Lipschitz (by the fact that hhas bounded sec-
ond derivatives), g2is pseudo-Lipschitz of order 2. So, the quantity under the square root con-
verges in probability to E(V,Θ)∼Πtg2by Assumption 1, and, with probability tending to 1, we have
∥∇R(0)∥2≲1/√
d.
Combining the above bounds on ∥y∥2and∥∇R(0)∥2, we can conclude that ∥u∗∥2≲√
d. The first
part of the lemma follows by noting that
∥∆∗∥2=1√
d∥u∗⊙v−θ∗∥2≤1√
d∥u∗⊙v∥2+1√
d∥θ∗∥2
≤1√
d∥v∥∞∥u∗∥2+1√
d∥θ∗∥2
≲∥v∥∞,
where the last inequality holds with probability approaching 1. Lastly, the optimal qfor any ∆has
closed-form q=1√nϵ−1√nX∆. By the triangle inequality, we then obtain
∥q∗∥2≤1√n∥ϵ∥2+1√n∥X∥∥∆∗∥2≲∥v∥∞,
with the last inequality holding with probability approaching 1, by the concentration of norms of ϵ
andXas discussed above, and the bound on ∥∆∗∥2.
20Lemma 2. Consider the following unconstrained minimization problem over u∈Rd:
min
u∈Rdmax
0≤τ≤R2τ∥h∥2√nr
σ2+1
d∥u⊙v−θ∗∥2
2−2τ√
ndg⊤(u⊙v−θ∗)
−τ2+λ
d∥u∥2
2+µ
ddX
i=1h(ui, v(t)
i, θ∗
i).
With probability approaching 1, the solution u∗satisfies1√
d(u∗⊙v−θ∗)
2≲∥v∥∞.
Proof. First note1√
d(u∗⊙v−θ∗)
2≤1√
d∥u∗∥2∥v∥∞+1√
d∥θ∗∥2, and1√
d∥θ∗∥2is bounded
by a constant with probability approaching 1by the assumed W2convergence of θ∗to a fixed limit.
Hence, it suffices to show that ∥u∗∥2≲√
dwith high probability. We begin by noting that the inner
maximization over τadmits a closed form solution, so the problem becomes
min
u∈Rd 
∥h∥2
2√nr
σ2+1
d∥u⊙v−θ∗∥2
2−1
2√
ndg⊤(u⊙v−θ∗)!2
++λ
d∥u∥2
2+µ
ddX
i=1h(ui, v(t)
i, θ∗
i).
Now, we can proceed similarly to in the proof of Lemma 1. Let
F(u):= 
∥h∥2
2√nr
σ2+1
d∥u⊙v−θ∗∥2
2−1
2√
ndg⊤(u⊙v−θ∗)!2
+,
R(u):=λ
d∥u∥2
2+µ
ddX
i=1h(ui, v(t)
i, θ∗
i).
Then, noting Fis always non-negative and Risλ
dstrongly-convex, we use the same argument as in
Lemma 1 to obtain the inequality
∥u∗∥2
2+2d
λ∇R(0)⊤u∗≤2d
λF(0).
After completing the squares, we obtain
u∗+d
λ∇R(0)2
2≤2d
λF(0) +d2
λ2∥∇R(0)∥2
2,
so we can conclude
∥u∗∥2≤d
λ∥∇R(0)∥2+r
2d
λF(0) +d2
λ2∥∇R(0)∥2
2≤2d
λ∥∇R(0)∥2+r
2d
λF(0).
As shown in Lemma 1, ∥∇R(0)∥2≲1√
dwith probability approaching 1. It remains to show thatp
F(0)≲1with probability approaching 1. To see this, observe that
p
F(0) = 
∥h∥2
2√nr
σ2+1
d∥θ∗∥2
2−1
2√
ndg⊤θ∗!
+
≤∥h∥2
2√nr
σ2+1
d∥θ∗∥2
2−1
2√
ndg⊤θ∗
≤∥h∥2
2√nr
σ2+1
d∥θ∗∥2
2+1
2√
nd∥g∥2∥θ∗∥2,
where the last line uses the triangle and Cauchy-Schwarz inequalities. By concentration of the norm
for Gaussian vectors, there exists a universal constant csuch that∥h∥2√n≤cand∥g∥2√n≤cwith
probability approaching 1. Moreover, by Assumption 1,∥θ∗∥2√
d≤cwith probability approaching 1.
Hence,p
F(0)≲1with probability approaching 1. Substituting this into the bound for ∥u∗∥2from
above completes the proof.
21Lemma 3. Under Assumption 1, the function fd(τ, β)(Eq. 19) converges pointwise in probability to
f(τ, β)(Eq. 20) as d→ ∞ .
Proof. We consider the limit of each term in fdseparately. The limit of the termsτσ2ξ
βandτβξ is
found by noting that ξ→1in probability by Gaussian Lipschitz concentration.
The first summation term simplifies as follows:
1
ddX
i=1τξθ∗2
i
β−τ
βξ 
βgi√κ−ξθ∗
i2
=1
ddX
i=1
−τ
βξ 
β2g2
iκ−2βgi√κξθ∗
i
=−τ
βξ1
ddX
i=1
β2g2
iκ−2βgi√κξθ∗
iP→ −τβκ,
where the last line follows from the weak law of large numbers since the giare i.i.d. standard
Gaussian variables.
For the last term, after again using the fact that ξP→1, we need to consider
1
ddX
i=1"
Mλ(·)2+µh(·,v(t)
i,θ∗
i) 
θ∗
i−βgi√κ
v(t)
i;β
2τv(t)2
i!#
=:1
ddX
i=1q(vi, θ∗
i, gi)
To show convergence in probability of this term, first fix δ >0. Then, we want to show
P"1
ddX
i=1q(vi, θ∗
i, gi)−Eq(V,Θ, G)> δ#
→0,
where the expectation is over (V,Θ)∼ΠtandG∼ N(0,1). It suffices to show the following two
statements:
P"1
ddX
i=1q(vi, θ∗
i, gi)−Eg1
ddX
i=1q(vi, θ∗
i, gi)>δ
2#
→0, (26)
P"Eg1
ddX
i=1q(vi, θ∗
i, gi)−Eq(V,Θ, G)>δ
2#
→0. (27)
To show (26), we rely on a concentration inequality for the Moreau envelope of a Gaussian vector
plus a bounded vector from [5]. First note that
1
ddX
i=1q(vi, θ∗
i, gi) =1
dmin
u∈Rd(
λ∥u∥2
2+µdX
i=1h(ui, vi, θ∗
i) +τ
βu⊙v−θ∗+β√κg2
2)
=1
dmin
u∈Rd(
λ∥u∥2
2+µdX
i=1h(ui, vi, θ∗
i) +τβκu⊙v
β√κ−θ∗
β√κ+g2
2)
=1
dmin
θ∈Rd(
λβ√κθ
v2
2+µdX
i=1hβ√κθi
vi, vi, θ∗
i
+τβκθ−θ∗
β√κ+g2
2)
=1
dMℓθ∗
β√κ−g;1
2τβκ
,
where the second to last line follows from the change of variable θ=u⊙v
β√κ, andℓ(θ):=λβ√κθ
v2
2+
µPd
i=1h
β√κθi
vi, vi, θ∗
i
. Here, for fixed v,ℓis a proper convex function of θ. Moreover, by
Assumption 1,θ∗
β√κhas norm of order√
dwith high probability. Hence, by [ 5, Lemma 8], this
quantity concentrates around its expectation (with respect to g), and we can conclude that there is
some c >0such that
P"1
ddX
i=1q(vi, θ∗
i, gi)−Eg1
ddX
i=1q(vi, θ∗
i, gi)>δ
2#
≤cτ2β2κ2
dδ2→0.
22To show (27), note that
Eg1
ddX
i=1q(vi, θ∗
i, gi) =1
ddX
i=1EGq(vi, θ∗
i, G).
Observe that this quantity is an expectation with respect to the joint empirical distribution of (v,θ∗).
By the assumption of W2convergence of the empirical distribution of (v,θ∗)(Assumption 1), if
we can show that mapping (v, θ)→EGq(v, θ, G )is bounded, then we can conclude that the above
quantity converges in probability to Eq(V,Θ, G), with (V,Θ)∼Πt. To see this, recall that for all
G∈R,qis bounded below as
q(v, θ, G )≥min
uλu2+µh(u, v, θ ),
which is always bounded below since his bounded (or, in the case h=u2, the lower bound is zero).
Next, for a given G, we can bound qabove as
q(v, θ, G )≤µh(0, v, θ) +τ
β(θ−β√κG)2.
Hence,
EGq(v, θ, G )≤µh(0, v, θ) +τ
βθ2+τβκ < C
for some universal constant C >0, since θis bounded by assumption and h(0, v, θ)is either bounded
above or equal to 0(in the case where h=u2). Combining (26) and (27) yields the desired result.
C Solving the min-max problem
Below, we show that the max-min problems (5) and (7) have easily computable solutions. Note it
suffices to consider the grouped case (7), since we can apply it with b= 1to recover the ungrouped
case. The following derivation closely follows the analysis of the scalar max-min problem in [ 8],
who study a similar scalar problem (albeit in the case of λ= 0, which we do not consider).
First recall that, as shown in the proof of Theorem 1, there exists a unique saddle point, due to the
strong convexity in τand strict convexity in β. Now, taking derivatives with respect to τandβ, we
obtain the following saddle point conditions:
0 =σ2
β+β(1−κ)−2τ+βλ2E"
1
bbX
i=1Θ2
i+β2κ
(τV2
i+βλ)2#
, (28)
0 =−τσ2
β2+τ(1−κ) +τλE"
1
bbX
i=11
τV2
i+βλ#
−τλ2E"
1
bbX
i=1Θ2
i+β2κ
(τV2
i+βλ)2#
. (29)
Solving each of these equations for the quantity β2λ2Eh
1
bPb
i=1Θ2
i+β2κ
(τV2
i+βλ)2i
and then equating the
two, we arrive at
2τβ−β2(1−κ)−σ2=β2(1−κ)−σ2+ 2λβ3κE"
1
bbX
i=11
τV2
i+βλ#
,
which implies
τ=β(1−κ) +λβ2κE"
1
bbX
i=11
τV2
i+βλ#
.
Defining the auxiliary variable γ=τ/β, this yields the fixed point equation
γ= 1−κ+λκE"
1
bbX
i=11
γV2
i+λ#
. (30)
23Substituting this γback into the first optimality condition in (28), we can express the optimal βin
closed-form, in terms of γ:
β=vuuutσ2+λ2Eh
1
bPb
i=1Θ2
i
(γV2
i+λ)2i
2γ+κ−1−λ2κEh
1
bPb
i=11
(γV2
i+λ)2i.
Finally, the optimal τcan be found simply as τ=γβ.
This yields a simple recipe for solving the min-max problem. First, compute the positive solution ˆγ
to the fixed point equation (30) (this can be found easily using standard numerical solvers). Then,
(ˆβ,ˆτ)are both given in closed-form as functions of ˆγ(where the required expectations can all be
approximated via Monte Carlo simulation).
D Further simulations
In this section, we demonstrate that our asymptotic predictions can provide accurate estimates of the
test error, even when some of our technical assumptions are not satisfied.
First, we compare the two “heavier” weightings considered in Section 3.1, ψ(u, v) = tanh |uv|and
ψ(u, v) = tanh u2, to the same weightings without the bounded tanh activation: ψ(u, v) =|uv|
andψ(u, v) =u2. We note that the reweighting choice |uv|is considered in [ 21,28] as a limit as
p→0of the classical IRLS update for ℓpminimization. In Figure 3a, we consider the same sparse
regression as in Section 3.1, i.e., with n= 250 , d= 2000 , σ= 0.1, θ∗
ii.i.d.∼Bernoulli (0.01)andλ
chosen to minimize the predicted asymptotic loss.
For each choice of ψ, we apply the theoretical predictions of Theorem 1, even if ψviolates Assumption
2. We find that our predictions remain accurate for all these choices of ψ. The choice tanh|uv|
performs almost identically without the tanh activation. Interestingly, the choice ψ= tanh u2
outperforms the variant without the tanh and has a more regular decay of the test loss.
In Figure 3b, we apply Theorem 1to predict the asymptotic squared test loss:1
d∥u⊙v−θ∗∥2
2at each
iteration. While this function is not PL(2), as required by the theorem, the asymptotic predictions
still align well with simulations. Extending our technical results to hold formally in such scenarios is
an interesting direction for future work.
241 2 3 4 5 6 7 8
Iterations10−2ℓ1Test Errorψ=|uv|(theory)
ψ=|uv|(sim)
ψ= tanh |uv|(theory)
ψ= tanh |uv|(sim)
ψ=u2(theory)
ψ=u2(sim)
ψ= tanh u2(theory)
ψ= tanh u2(sim)
(a) Predictions for ψwhich are
not uniformly bounded1 2 3 4 5 6 7 8
Iterations0.0020.0040.0060.0080.010Mean Squared Errorψ=|uv|1
2(theory)
ψ=|uv|1
2(sim)
ψ= tanh |uv|(theory)
ψ= tanh |uv|(sim)
ψ=u(theory)
ψ=u(sim)
ψ= tanh u2(theory)
ψ= tanh u2(sim)
(b) Predictions for the squared
test loss
Figure 3: Here, we fix n= 250 , d= 2000 , σ= 0.1, θ∗
ii.i.d.∼Bernoulli (0.01)and select λto minimize
the predicted asymptotic loss. Plus marks denote the median over 100 trials, and the shaded region
indicates the interquartile range. Left: Predictions and simulations for weighting functions which are
not uniformly bounded. Right: Predictions and simulations for the squared error1
d∥u⊙v−θ∗∥2
2.
25NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
• You should answer [Yes] , [No] , or [NA] .
•[NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a
proper justification is given (e.g., "error bars are not reported because it would be too computationally
expensive" or "we were unable to find the license for the dataset we used"). In general, answering
"[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found.
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Theorems 1 and 2 directly reflect the claims from the abstract/introduction.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We provide detailed discussion of the strength of our technical assumptions
after the statement of Theorem 1 and in Appendix D, and we further elaborate on these areas
for improvement in the conclusion.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
26• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We formally state and discuss Assumptions 1 and 2, and we provide a complete
proof of our results in the Appendix A of the supplemental material.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide all details of hyperparameters used to run simulations in the corre-
sponding figure caption. We also detail the exact method used to compute our asymptotic
predictions in Appendix C and provide accompanying code for the simulations.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
27•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We provide an accompanying file with code for computing the asymptotic
predictions and running simulations with high-dimensional Gaussian data.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
28Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: For each simulation, hyperparameter choices (and the method for choosing the
ridge parameter λ) are specified either in the caption or in the accompanying discussion in
the text.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We report the median and interquartile range for all simulations over 100
independent trials. These metrics are chosen due to the asymmetry of the distribution across
trials (e.g., the mean minus the standard deviation might be negative in low-noise settings).
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: Due to the small-scale of our included simulations, we do not include this
information.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
29•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research presented in this work abides by the Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: Our work provides statistical analysis for a generic family of algorithms in an
abstract setup, and we do not believe our work has immediate social impacts or an immediate
path toward such impacts.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [Yes]
Justification: The paper poses no such risks.
30Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The provided code includes all necessary information for running simulations.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
31Answer: [NA]
Justification: The paper does not involve research with human subjects or crowdsourcing.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve research with human subjects or crowdsourcing.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
32